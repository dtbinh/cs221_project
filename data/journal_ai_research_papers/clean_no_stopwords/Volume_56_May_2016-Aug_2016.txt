Journal Artificial Intelligence Research 56 (2016) 517-545

Submitted 4/16; published 7/16

Time-Sensitive Bayesian Information Aggregation
Crowdsourcing Systems
Matteo Venanzi

mavena@microsoft.com

Microsoft, 2 Waterhouse Square,
London EC1N 2ST UK

John Guiver

joguiver@microsoft.com

Microsoft Research, 21 Station Road,
Cambridge CB1 2FB UK

Pushmeet Kohli

pkohli@microsoft.com

Microsoft Research, One Microsoft Way,
Redmond WA 98052-6399 US

Nicholas R. Jennings

n.jennings@imperial.ac.uk

Imperial College, South Kensington,
London SW7 2AZ UK

Abstract
Many aspects design efficient crowdsourcing processes, defining workers
bonuses, fair prices time limits tasks, involve knowledge likely duration
task hand. work introduce new timesensitive Bayesian aggregation
method simultaneously estimates tasks duration obtains reliable aggregations
crowdsourced judgments. method, called BCCTime, uses latent variables represent
uncertainty workers completion time, tasks duration workers
accuracy. relate quality judgment time worker spends task,
model assumes task completed within latent time window within
workers propensity genuinely attempt labelling task (i.e., spammers)
expected submit judgments. contrast, workers lower propensity
valid labelling, spammers, bots lazy labellers, assumed perform tasks
considerably faster slower time required normal workers. Specifically, use
efficient message-passing Bayesian inference learn approximate posterior probabilities
(i) confusion matrix worker, (ii) propensity valid labelling worker,
(iii) unbiased duration task (iv) true label task. Using two realworld public datasets entity linking tasks, show BCCTime produces
11% accurate classifications 100% informative estimates tasks
duration compared stateoftheart methods.

1. Introduction
Crowdsourcing emerged effective way acquire large amounts data enables
development variety applications driven machine learning, human computation participatory sensing systems (Kamar, Hacker, & Horvitz, 2012; Bernstein, Little,
Miller, Hartmann, Ackerman, Karger, Crowell, & Panovich, 2010; Zilli, Parson, Merrett,
c
2016
AI Access Foundation. rights reserved.

fiVenanzi, Guiver, Kohli & Jennings

& Rogers, 2014). Services Amazon Mechanical Turk1 (AMT), oDesk2 CrowdFlower3 enabled number applications hire pools human workers provide
data serve training image annotation (Whitehill, Ruvolo, Wu, Bergsma, & Movellan,
2009; Welinder, Branson, Belongie, & Perona, 2010), galaxy classification4 (Kamar et al.,
2012) information retrieval systems (Alonso, Rose, & Stewart, 2008). applications, central problem deal diversity accuracy speed workers
exhibit performing crowdsourcing tasks. result, due uncertainty
reliability individual crowd responses, many systems collect many judgments different workers achieve high confidence quality labels. However, incur
high cost either time money, particularly workers paid per judgment,
delay completion entire crowdsourcing project introduced
workers intentionally delay submissions follow work schedule. example, typical crowdsourcing scenario, requester must specify number requested
assignments (i.e., individual responses different workers), well time limit
completion assignment. must also set price paid response5 ,
usually includes participation fee bonus based quality submission
actual effort required task. However, nontrivial problem set time
limit gives workers sufficient time perform task correctly without leading
task starvation (i.e., one working task assigned). Generally speaking,
knowledge actual duration assignment (task instance) useful
requesters various reasons. First, tasks duration used proxy estimate
difficulty, difficult tasks usually take longer complete (Faradani, Hartmann,
& Ipeirotis, 2011). Second, information useful set time limit task
reduce overall time task completion. Third, task requestor use task
duration pay fair bonuses workers based difficulty task complete.
seeking estimate information, however, important consider
workers might perform task immediately might delay submissions
accepting task or, extreme, might submit poor annotation rapid
time (Kazai, 2011). result, common heuristic estimates tasks duration (such
workers average median completion time) account aspects
likely inaccurate.
Given above, number challenges addressed various steps
designing efficient crowdsourcing workflows. First, judgments
collected, uncertainty unknown reliability individual workers must
taken account compute final labels. aggregated labels often estimated
settings true answer task never revealed, quantity
crowdsourcing process trying discover (Kamar et al., 2012). Second,
estimating tasks duration, uncertainty completion time deriving
private work schedule worker must taken account (Huff & Tingley, 2015).
1.
2.
3.
4.
5.

www.mturk.com
www.odesk.com
www.crowdflower.com
www.galaxyzoo.org
common guideline task requesters consider $0.10 per minute minimum wage
ethical crowdsourcing experiments (www.wearedynamo.org).

518

fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems

Third, two challenges must addressed simultaneously due interdependencies
workers reliability, time required complete task, final labels
estimated tasks.
attempt address challenges, growing interest developing
algorithms techniques compute accurate labels minimising set of, possibly
unreliable, crowd judgements (Sheng, Provost, & Ipeirotis, 2008). detail, simple solutions typically use heuristic methods majority voting weighted majority voting
(Tran-Thanh, Venanzi, Rogers, & Jennings, 2013). However, methods consider
reliability different workers treat judgments equally reliable.
sophisticated methods onecoin model (Karger, Oh, & Shah, 2011), GLAD
(Whitehill et al., 2009), CUBAM (Welinder et al., 2010), DS (Dawid & Skene, 1979)
Bayesian Classifier Combination (BCC) (Kim & Ghahramani, 2012) use probabilistic
models take reliabilities account, potential labelling biases workers, e.g., tendency worker consistently underrate items. particular
DS represents workers skills based confusion matrix expressing reliability
worker possible class objects. BCC works similarly DS, also considers uncertainty confusion matrices aggregated labels using principled
Bayesian learning framework. representational power enabled BCC successfully applied number crowdsourcing applications including galaxy classification
(Simpson, Roberts, Psorakis, & Smith, 2013), disaster response (Ramchurn, Huynh, Ikuno,
Flann, Wu, Moreau, Jennings, Fischer, Jiang, Rodden, et al., 2015) sentiment analysis
(Simpson, Venanzi, Reece, Kohli, Guiver, Roberts, & Jennings, 2015). recently, Venanzi, Guiver, Kazai, Kohli, Shokouhi (2014) proposed communitybased extension
BCC (i.e., CBCC) improve predictions leveraging groups workers similar confusion matrices. Similarly, Simpson et al. combined BCC language modelling
techniques automated text sentiment analysis using crowd judgments. degree
applicability performance BCC-based methods promising point departure
developing new data aggregation methods crowdsourcing systems. However, none
existing methods reason workers completion time learn duration
task outsourced crowd. Moreover, methods learn probabilistic models information contained judgment set. Unfortunately,
strategy challenged datasets arbitrarily sparse, i.e., workers provide judgments small sub-set tasks, therefore judgments provide weak
evidence accuracy worker. contexts, hypothesis wider set
features must leveraged learn reliable crowdsourcing models. work,
focus time takes worker complete task considered key indicator
quality work. Importantly, information workers completion time
made available popular crowdsourcing platforms including AMT, Microsoft Universal Human Relevance System (UHRS) CrowdFlower. Therefore, seek
efficiently combine features data aggregation algorithm naturally
integrated output data produced platforms. detail, present
novel timesensitive data aggregation method simultaneously estimates tasks
duration obtains reliable aggregations crowdsourced judgments. characteristic
timesensitivity method relates ability jointly reason workers
completion time together judgments data aggregation process. detail,
519

fiVenanzi, Guiver, Kohli & Jennings

method extension BCC, term BCCTime. Specifically, incorporates
newly developed time model enables method leverage observations time
spent worker task best inform inference final labels. BCC,
use confusion matrices represent labelling accuracy individual workers.
model granularity workers time profiles, use latent variables represent
propensity worker submit valid judgments. Further, model uncertainty
duration task, use latent thresholds define time interval within
task expected completed workers high propensity valid labelling. Then, using Bayesian message-passing inference, method simultaneously infers
posterior probabilities (i) confusion matrix worker, (ii) propensity
valid labelling worker, (iii) true label task (iv) upper
lower bound duration task. particular, latter represents reliable
estimate likely duration task obtained automatically filtering
contributions workers low propensity valid labelling. demonstrate
efficacy method using two commonlyused public datasets relate important Natural Language Processing (NLP) application crowdsourcing entity linking tasks.
datasets, method achieves 11% accurate classifications compared
seven state-of-the-art methods. Further, show tasks duration estimates
100% informative common heuristics consider workers
completion time correlated quality judgments.
background, make following contributions state art.
analysis two real-world datasets crowdsourcing entity-linking tasks,
show existence different types taskspecific qualitytime trends, e.g.,
increasing, decreasing invariant trends, quality judgments
time spent workers produce them. also re-confirm existing results
showing workers submit judgments quickly slowly
entire task set typically provide lower quality judgments.
develop BCCTime: first time-sensitive Bayesian aggregation model leverages observations workers completion time simultaneously aggregate crowd
judgments infer duration task well reliability worker.
show BCCTime outperforms seven competitive stateoftheart
data aggregation methods crowdsourcing, including BCC, CBCC, one coin
majority voting, providing 11% accurate classifications 100%
informative estimates tasks duration.
rest paper unfolds follows. Section 2 describes notation preliminaries Bayesian aggregation crowd judgments. Section 3 details time analysis
real-world datasets. Then, Section 4 formally introduces BCCTime details probabilistic inference. Section 5 presents evaluation state art. Section 6
summarises rest related work areas data aggregation time analysis
crowd generated content Section 7 concludes.
520

fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems

Table 1: List symbols.
Symbol
N
K
C

J
ti

(k)
ci
(k)

(k)
p
k


vik


0
0
0
0
0
0
s0
p0
(k)
0

Definition
Number tasks
Number workers
Number true label values
Set observed workers completion time
Set observed judgments
True label task
Vector ti
Judgment k task
Time spent k judging task
Confusion matrix k
Class proportions tasks
Propensity k making valid labelling attempts
Labelling probabilities general low-propensity worker
Vector k , k = 1, . . . , K
(k)
Boolean variable signalling ci valid labelling attempt
Lower-bound threshold duration task
Upper-bound threshold duration task
Mean Gaussian prior
Precision hyperparameter Gaussian prior
Mean hyperparameter Gaussian prior
Precision hyperparameter Gaussian prior
True count hyperparameter Beta prior k
False count hyperparameter Beta prior k
Hyperparameter Dirichlet prior
Hyperparameter Dirichlet prior p
Hyperparameter Dirichlet prior (k)

2. Preliminaries
Consider crowd K workers labelling N objects C possible classes symbols
(k)
listed Table 1. Assume k submits judgment ci {1, . . . , C} classifying
(k)
object i. Let ti unobserved true label i. Then, suppose R+
(k)
(k)
time taken k produce ci . Let J = {ci |i = 1, . . . , N, k = 1, . . . , K}
(k)
= {i |i = 1, . . . , N, k = 1, . . . , K} set containing judgments
time spent workers, respectively.
introduce key features BCC model relevant method.
First introduced Kim Ghahramani (2012), BCC method combines multiple
judgments produced independent classifiers (i.e., crowd workers) unknown accuracy. Specifically, model assumes that, task i, ti drawn categorical
distribution parameters p:

ti |p Cat(ti |p)
521

(1)

fiVenanzi, Guiver, Kohli & Jennings

p denotes class proportions objects. Then, workers accuracy
represented confusion matrix (k) comprising labelling probabilities k
(k)
(k)
(k)
possible true label value. Specifically, row matrix c = {c,1 , . . . , c,C }
(k)

vector c,j probability k producing judgment j object class
c. Importantly, confusion matrix expresses accuracy (diagonal values)
biases (off-diagonal values) worker. recognise workers particularly accurate (inaccurate) bias specific class objects. fact, accurate (inaccurate)
workers represented high (low) probabilities diagonal confusion
matrix, whilst workers bias towards particular class high probabilities
corresponding column matrix. example, galaxy zoo domain
workers classify images celestial galaxies, confusion matrices detect workers
low accuracy classifying spiral galaxies systematically classify
every object elliptical galaxies (Simpson et al., 2013).
relate workers confusion matrix quality judgment, BCC assumes
(k)
ci drawn categorical distribution parameters corresponding ti -th row
(k) :
(k)

(k)

(k)

ci | (k) , ti Cat ci | ti

(2)
(k)

equivalent categorical mixture model ci ti mixture
parameter c parameter c-th categorical component. Then, assuming
judgments independent identically distributed (i.i.d.), joint likelihood
expressed as:
p(C, t|, p) =

N


Cat(ti |p)

i=1

K


(k)

(k)

Cat ci | ti

k=1

Using conjugate Dirichlet prior distributions parameters p applying
Bayes rule, joint posterior distribution derived as:
p(, p|C, t) Dir(p|p0 )

N n

Cat(ti |p)
i=1

K


(k)

(k)

Cat ci | ti


(k) (k)
Dir( ti | ti ,0 )

(3)

k=1

expression, possible derive predictive posterior distributions
unobserved (latent) variable using standard integration rules Bayesian inference (Bishop,
2006). Unfortunately, exact derivation posterior distributions intractable
BCC due non-conjugate form model (Kim & Ghahramani, 2012). However,
shown that, particularly BCC models, possible compute efficient approximations distributions using standard techniques Gibbs sampling (Kim
Ghahramani), variational Bayes (Simpson, 2014) Expectation-Propagation (Venanzi
et al., 2014). Building this, several extensions BCC proposed various
522

fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems

crowdsourcing domains (Venanzi et al., 2014; Simpson et al., 2015, 2013). particular,
CBCC applies communitybased techniques represent groups workers similar
confusion matrices classifier combination process (Venanzi et al.). mechanism
enables model transfer learning workers reliability communities
improve quality inference.
However, drawback BCC based models learn tasks
duration consider extra features workers judgments.
result, perform full learning confusion matrices task labels using
judgments produced workers. But, mentioned earlier, strategy challenged
sparse datasets worker labels tasks. case, instance,
Crowdflower dataset used 2013 CrowdScale Shared Task challenge6
sentiment 98,980 tweets classified 1,960 workers five sentiment classes.
dataset, 30% workers judged 15 tweets, i.e., 0.015% total samples,
long tail workers less 3 judgments.

3. Analysis Workers Time Spent Judgments
discussed basic concepts non-time based data aggregation, turn
analysis relationship time workers spend task
quality judgments produce. contrast previous works area (Demartini, Difallah, & Cudre-Mauroux, 2012; Wang, Faridani, & Ipeirotis, 2011), extend
analysis qualitytime responses specific task instances, well entire
task set. doing, provide key insights inform design timesensitive
aggregation model. end, consider two public datasets generated widely
used NLP application crowdsourcing entity linking tasks.
3.1 Datasets
ZenCrowd - India (ZC-IN): contains set links names entities
extracted news articles uniform resource identifiers (URIs) describing entity
Freebase7 DBpedia8 (Demartini et al., 2012). dataset collected using AMT,
worker asked classify whether single URI either irrelevant (0)
relevant (1) single entity. contains timestamps acceptance
submission judgment. Moreover, gold standard labels collected expert
editors tasks. information released regarding restrictions
worker pool, although workers known living India, worker
paid $0.01 per judgment. total 11,205 judgements collected small pool
25 workers, giving dataset moderately high number judgements per worker,
detailed Table 2. particular, Figure 1a shows vast majority tasks receive
5 judgements, Figure 1c shows skewed distribution gold labels, 78%
links entities URIs classified workers irrelevant (0). such,
worth noting binary classifiers bias towards unrelated classification
correctly classify majority tasks thus receive high accuracy. Therefore,
6. www.crowdscale.org/shared-task
7. www.freebase.com
8. www.dbpedia.org

523

fiVenanzi, Guiver, Kohli & Jennings

Table 2: Crowdsourcing datasets entity linking tasks.
Dataset:

Judgements

Workers

Tasks

Labels

11205
12190
6000

25
74
110

2040
2040
300

2
2
5

ZC-IN
ZC-US
WS-AMT

Judgement
accuracy
0.678
0.770
0.704

1800

1000

1400

800

# tasks

# tasks

1200
1000
800
600

600
400

400

200

200
2

3

4

5

7

9

10

# judgements

14

15

19

0

20

2 3 4 5 6 7 8 9 101112131516192225
# judgements

(a) ZC -

(b) ZC - US

1600

100

1400

80

1200
1000

# tasks

# tasks

Judgements
per worker
448.200
164.730
54.545

1200

1600

0

Judgements
per task
5.493
5.975
20.000

800
600
400

60
40
20

200
0

0
0

Gold label

1

0

1

2

3

4

Gold label

(c) ZC

(d) WS - AMT

Figure 1: Histograms number judgments per task ZC-IN (a) ZC-US (b)
WS - AMT shown tasks received exactly 20 judgments number
tasks per gold label ZC (c) WS - AMT (d).

detail Section 5, important select accuracy metrics evaluate classifier
across whole spectrum possible discriminant thresholds.
ZenCrowd - USA (ZC-US): dataset also provided Demartini et al. (2012)
contains judgements set tasks ZC-IN, although judgements
collected AMT workers US. payment $0.01 per judgement
used. However, larger pool 74 workers involved, lower number
judgements collected worker, shown Table 2. Furthermore, Figure 1b
shows similar distribution judgements per task India dataset, although slightly
fewer tasks received 5 judgements, remaining tasks receiving 3-4 judgements
524

fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems

9-11 judgements. judgement accuracy US dataset higher India
dataset despite identical crowdsourcing system reward mechanism used.
Weather Sentiment - AMT (WS-AMT): Weather Sentiment dataset provided CrowdFlower 2013 Crowdsourcing Scale shared task challenge.9
includes 300 tweets 1,720 judgements 461 workers used several
experimental evaluations crowdsourcing models (Simpson et al., 2015; Venanzi et al.,
2014; Venanzi, Teacy, Rogers, & Jennings, 2015b). detail, workers asked
classify sentiment tweets respect weather following categories:
negative (0), neutral (1), positive (2), tweet related weather (3) cant tell (4).
result, dataset pertains multi-class classification problem. However, original
dataset used Share task challenge contain time information
collected judgments. Therefore, new dataset (WS-AMT), recollected
tasks CrowdFlower shared task dataset using AMT platform, acquiring exactly 20 judgements recording elapsed time judgment (Venanzi, Rogers,
& Jennings, 2015a). result, WS-AMT contains 6,000 judgements 110 workers,
shown Table 2. restrictions placed worker pool worker
paid $0.03 per judgement. Furthermore, Figure 1d shows that, per original dataset,
common gold label unrelated, five tasks assigned gold label
cant tell.
3.2 Time Spent Task versus Judgment Accuracy
wish analyse distribution workers completion time judgments
accuracy. so, focus two datasets, ZC-US ZC-IN binary labels.
fact, binary nature two datasets allow us analyse accuracy higher level
detail, i.e., terms precision recall workers judgments time spent
produce them. Specifically, Figure 2 shows cumulative distribution precision
recall set judgments selected specific time threshold (x-axis) respect
gold standard labels. Here, precision fraction true positive classifications
returned positive classifications (true positives + false positives) recall
number true positive classifications divided number positive samples.
Similarly Demartini et al. (2012), find accuracy lower extremes
time distributions. ZC-US, precision recall higher sub-set
judgments produced 80 seconds less 1500 seconds.
ZC-IN, precision recall higher judgments produced 80 seconds
less 600 seconds.
addition, Figure 3 shows distribution recall execution time sample
set six positive task instances (i.e., entities positive gold standard labels)
least ten judgments. example, Figure 3b shows time distribution judgments
URI: freebase.com/united states associated entity American.
graphs, samples increasing quality-time curve, i.e., workers spending
time produce better judgments, (Figure 3a Figure 3b). samples decreasing
quality-time curve, i.e., workers spending time produce worse judgments (Figure 3c
Figure 3d). Finally, last two samples approximately constant quality-time
9. www.kaggle.com/c/crowdflower-weather-twitter

525

fi0.9

0.8

0.8

0.7

0.7

0.6

0.6

Recall

Precision

Venanzi, Guiver, Kohli & Jennings

0.5
0.4

0.5
0.4
0.3

0.3
0.2

0.2

0.1

0.1

0

5

80

620

320

0

1520

5

80

620

320

1520

Time spent(sec)

Time spent (sec)

(b) ZC - US

(a) ZC - US
0.5

0.7
0.6

0.4

Recall

Precision

0.5
0.3
0.2

0.4
0.3
0.2

0.1
0

0.1
5

85

350

Time spent (sec)

600

0

1400

5

85

350

Time spent (sec)

600

1400

(d) ZC -

(c) ZC -

Figure 2: Histograms precision recall binned time spent US workers
(a, b) Indian workers (c, d) ZenCrowd datasets.
curve, i.e., workers quality invariant time spent (Figure 3e Figure 3f).
also seen trends naturally correlate difficulty task instance.
instance, URI: freebase.com/m/03hkhgs linked entity Southern Avenue
difficult judge URI: dbpedia.org/page/Switzerland linked entity
Switzerland. fact, Southern Avenue ambiguous entity name,
may lead worker open URI check content able issue correct
judgment. Instead, relevance second entity Switzerland judged
easily visual inspection URI. addition, task specific time interval
includes sub-set judgments highest precision. example, ZC-IN,
judgments highest precision URI: dbpedia.org/page/Switzerland
entity Switzerland submitted 5 sec. 20 sec. (Figure 2d).
Instead, ZC-US, best judgments URI: dbpedia.org/page/European linked
entity European submitted interval 2 sec. 16 sec. (Figure 2c).
result, clear task instance specific qualitytime profile relates
difficulty labelling instance.
better analyse trends, Figure 4 shows Pearsons correlation coefficient ()
(i.e., standard measure degree linear correlation two variables)
13 entities positive links ten judgments across two datasets.
time spent worker always (linearly) correlated quality judgment
526

fi1

1

0.8

0.8

Recall

Recall

Time-Sensitive Bayesian Information Aggregation Crowdsourcing Systems

0.6
0.4
0.2
3

10

18

0

41

Time spent (sec)

11

23

48

Time spent (sec)

(a) Time-increasing task

(b) Time-increasing task

ZC - US
Entity: SouthernAvenue - Link:freebase.com/m/03hkhgs

ZC -
Entity: American - Link: freebase.com/united_states

1

1

0.8

0.8

0.6
0.4

0

0.6
0.4
0.2

0.2
2

12

Time spent (sec)

0

25

19

36

Execution time (sec)

45

(d) Time-decreasing task

ZC - US
Entity: European - Link: dbpedia.org/page/European

ZC -
Entity: Swiss - Link: dbpedia.org/page/Switzerland

1

1

0.8

0.8

0.6

0.6

0.4
0.2
0

5

(c) Time-decreasing task

Recall

Recall

0.4
0.2

Recall

Recall

0

0.6

0.4
0.2

2

8

14

Time spent (sec)

0

71

2

7

10

Time spent(sec)

20

60

(e) Time-decreasing task

(f) Time-constant task

ZC - US
Entity: Switzerland - Link: dbpedia.org/page/Switzerland

ZC - US
Entity: GMT - Link: dbpedia.org/page/Greenwich_Mean_Time

Figure 3: Histograms recall six entity linking tasks positive gold standard
labels least ten judgments ZenCrowd datasets. show different trends
recall-time curves various tasks.

across task instances. tasks significantly positive correlation (i.e., task
index = 6, 8, 13 > 0.7, p < 0.05), others significantly negative correlation
(i.e., task index = 9, 12 < 0.7, p < 0.05), whilst tasks less significant
correlation accuracy judgments time spent workers.
confirms different task instances substantially different quality-time responses
based difficulty sample. Thus, insight significantly extends previous
findings reported Demartini et al. (2013) qualitytime trend
observed across entire task set. Moreover, empirically supports theory several
existing data aggregation models (Kamar, Kapoor, & Horvitz, 2015; Whitehill et al., 2009;
Bachrach, Graepel, Minka, & Guiver, 2012) make use taskspecific features
527

fiVenanzi, Guiver, Kohli & Jennings

0.45

1
0.8

Strong positive
correlation

0.4

0.6

0.35
0.3

0.2

p value

Pearson's

0.4
0
-0.2

0.2
0.15

-0.4

0.1

-0.6
-0.8
-1

0.25

Significant correlation

0.05

Strong negative
correlation

0

1 2 3 4 5 6 7 8 9 10 11 12 13
Task index
(a)

1 2 3 4 5 6 7 8 9 10 11 12 13
Task index
(b)

Figure 4: Pearsons correlation coefficient (a) p-value (b) linear correlation workers completion time judgments accuracy 13 entity
linking tasks positive gold standard labels judgments ZenCrowd datasets.
achieve accurate classifications number crowdsourcing applications concerning,
among others, galaxy classification (Kamar et al.), image labelling (Whitehill et al., 2009)
problem solving (Bachrach et al., 2012).

4. BCCTime Model
Based results time analysis workers judgments, observed
different types qualitytime trends occur specific task instances. However, standard BCC, well existing aggregation models consider
information, unable perform inference likely duration task. rectify
this, need extend BCC able include trends aggregation
crowd judgments. end, model must flexible enough identify workers
who, addition imperfect skills, may also intention make valid
attempt complete task. increases uncertainty data reliability.
section, describe Bayesian Classifier Combination model Time (BCCTime). particular, describe three components model concerning (i)
representation unknown workers propensity valid labelling, (ii) reliability
workers judgments (iii) uncertainty workers completion time, followed
details probabilistic inference.
4.1 Modelling Workers Propensity Valid Labelling
Given uncertainty intention worker submit valid judgments, introduce latent variable k [0, 1] representing propensity k towards making
valid labelling attempt given task. way, model able naturally
explain unreliability worker based imperfect skills also
attitude towards approaching task correctly. particular, k close one means
528

fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems

worker tendency exert best effort provide valid judgments, even though
judgments might still noisy consequence imperfect skills possesses.
contrast, k close zero means worker tends provide valid judgments
tasks, means behaves similarly spammer. Specifically,
workers high propensity valid labelling provide inputs meaningful
tasks true label tasks duration. capture this, define per-judgment
(k)
(k)
boolean variable vi {0, 1} vi = 1 meaning k made valid labelling
(k)
(k)
(k)
attempt submitting ci vi = 0 meaning ci invalid annotation.
setting, number valid labelling attempts made worker derives
(k)
propensity valid labelling. Thus, model assuming vi random
draw Bernoulli distribution parametrised k :
(k)

vi

Bernoulli(k )

(4)

is, workers high propensity valid labelling likely make valid
labelling attempts, whilst workers low propensity likely submit spam
annotations.
4.2 Modelling Workers Judgments
describe part model concerned generative process crowd
judgments confusion matrix propensity workers. Intuitively,
judgments associated valid labelling attempts considered estimate
final labels. means judgment may generated two different
processes depending whether comes valid labelling attempt. capture
generative model BCCTime, mixture model used switch
(k)
(k)
two cases conditioned vi . first case valid labelling attempt, i.e., vi = 1,
judgment generated workers confusion matrix per standard BCC
(k)
model. Therefore, assume ci generated model described BCC
(k)
(Eq. 2), including vi conditional variables. Formally:
(k)

(k)

ci | (k) , ti , vi

(k)

(k)

= 1 Cat ci | ti

(5)
(k)

second case judgment produced invalid labelling attempt, i.e., vi = 0,
natural assume judgment contribute estimation true
label. Formally, assumption represented general random vote model
(k)
ci drawn categorical distribution vector parameter s:
(k)

(k)

ci |s, vi

(k)
= 0 Cat ci |s

(6)

vector labelling probabilities general worker low propensity
make valid labelling attempts. Notice equation depend ti ,
means judgments coming invalid labelling attempts treated
noisy responses uncorrelated ti .
529

fiVenanzi, Guiver, Kohli & Jennings

4.3 Modelling Workers Completion Time
shown Section 3, duration task may defined interval
workers likely submit high-quality judgments. However, due dependency
duration tasks characteristics, requirement interval must
non-constant across tasks. model this, define lower-bound threshold, ,
upper-bound threshold, , time interval representing duration i.
pertask thresholds latent variables must learnt training time. Then,
tasks lower higher variability duration represented based
values time thresholds. setting, valid labelling attempts made
workers expected completed within tasks duration interval detailed
(k)
thresholds. Formally, represent probability greater using
standard greaterThan probabilistic factor introduced Herbrich, Minka, Graepel
(2007) TrueSkill Bayesian ranking model:
(k)

I(i

(k)

> |vi

= 1)

(7)

factor defines non-conjugate relationship posterior distribution
(k)
form prior distribution . Therefore posterior
(k)
distribution p(i ) needs approximated. via moment matching
(k)
Gaussian distribution p(i ) matching precision precision adjusted mean
(k)
(i.e., mean multiplied precision) posterior distribution p(i ), shown
(k)
Table 1 Herbrich et al. (2007). similar way, model probability
greater as:
(k)

(k)

I(i > |vi

= 1)

(8)

Drawing together, upon observing set i.i.d. pairs judgments workers
completion times contained J respectively, express joint likelihood
BCCTime as:

p(J , , t|, p, s, ) =

N

i=1

Cat(ti |p)


K

(k)

I(i

(k)

(k)


(k) k

> )I(i > )Cat ci | ti

k=1

(k) (1k )
Cat ci |s


(9)

factor graph BCCTime illustrated Figure 5. Specifically, two shaded vari(k)
(k)
ables ci observed inputs, unobserved random variables
unshaded. graph uses gate notation (dashed box) introduced Minka Winn
(2008) represent two mixture models BCCTime. Specifically, outer gate represents workers judgments (see Section 4.2) completion times (see Section 4.3)
(k)
generated either BCC random vote model using vi gating variable.
inner gate mixture model generating workers judgments rows
confusion matrix using ti gating variable.
530

fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems

Dir.

Dir.

p



K workers

Beta

Gaussian

Gaussian



Cat.

k

ti



Bernoulli

C true label values
Cat.

Dir.

(k)

greater

smaller

(k)

(k)

N tasks

vi

Cat.

(k)
c

ci



Figure 5: factor graph BCCTime.
4.4 Probabilistic Inference
perform Bayesian inference unknown quantities, must provide prior
distributions latent parameters BCCTime. Following structure model,
select conjugate distributions parameters enable tractable
inference posterior probabilities. Therefore, prior p Dirichlet distributed
hyperparameter p0 :
(true label prior)
(k)

priors c
respectively:

p Dir(p|p0 )

(10)
(k)

also Dirichlet distributed hyperparameter s0 c,0

(spammer label prior)
(confusion matrix prior)

Dir(s|s0 )
(k)
c

(k)
Dir( (k)
c | c,0 )

(11)
(12)

Then, k Beta prior true count 0 false count 0 :
(workers propensity prior) k Beta(k |0 , 0 )

(13)

two time thresholds Gaussian priors mean 0 0 precision
0 0 respectively:
(lower-bound tasks duration threshold prior) N (i |0 , 0 )

(14)

(upper-bound tasks duration threshold prior) N (i |0 , 0 )

(15)

531

fiVenanzi, Guiver, Kohli & Jennings

Collecting hyperparameters set = {p0 , s0 , 0 , 0 , 0 , 0 , 0 , 0 }, find
applying Bayes theorem joint posterior distribution proportional to:

p(, p, s, t, |J , , ) Dir(s|s0 )Dir(p|p0 )

N


Cat(ti |p)N (i |0 , 0 )N (i |0 , 0 )

i=1
K


(k)

I(i

(k)

(k)

(k)

> )I(i < )Cat ci | ti

(k)

(k)

Dir( ti | ti ,0 )

k

k=1



Cat

(k) (1k )
ci |s
Beta(k |0 , 0 )

(16)

expression, compute marginal posterior distributions latent
variable integrating remaining variables. Unfortunately, integrations
intractable due nonconjugate form model. However, still compute
approximations posterior distributions using standard techniques family approximate Bayesian inference methods (Minka, 2001). particular, use
well-known EP algorithm (Minka, 2001) shown provide good quality approximations BCC models (Venanzi et al., 2014)10 . method leverages factorised
distribution joint probability approximate marginal posterior distributions
iterative message passing scheme implemented factor graph. Specifically,
use EP implementation provided Infer.NET (Minka, Winn, Guiver, & Knowles,
2014), standard framework running Bayesian inference probabilistic models.
Using Infer.NET, able train BCCTime largest dataset 12,190 judgments
within seconds using approximately 80MB RAM standard laptop.

5. Experimental Evaluation
described model, test performance terms classification accuracy
ability learn tasks duration real crowdsourcing experiments. Using datasets
described Section 3, conduct experiments following experimental setup.
5.1 Benchmarks
consider set benchmarks consisting three popular baselines (Majority voting,
Vote distribution Random) three stateoftheart aggregation methods (One coin,
BCC CBCC) commonly employed crowdsourcing applications.
detail:
One coin: method represents accuracy worker single reliability
parameter (or workers coin) assuming worker return correct answer
probability specified coin, incorrect answer inverse probability. result, method applicable binary datasets. Crucially,
model represents core mechanism several existing methods including (Whitehill
10. Alternative inference methods Gibbs sampling Variational Bayes trivially applied
model Infer.NET framework.

532

fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems

et al., 2009; Demartini et al., 2012; Liu, Peng, & Ihler, 2012; Karger et al., 2011; Li,
Zhao, & Fuxman, 2014)11 .
BCC: closest benchmark method described Section 2.
learns confusion matrices aggregated labels without considering
workers completion time input feature. used several crowdsourcing contexts including galaxy classification (Simpson et al., 2013), image annotation
(Kim & Ghahramani, 2012) disaster response (Ramchurn et al., 2015).
BCCPropensity: equivalent BCCTime workers propensity
learnt. benchmark used assess contribution inferring
workers propensity, versus joint learning tasks time thresholds,
quality final labels. Note BCCPropensity easy obtain BCCTime
setting time thresholds static observations = 0.0 = max.value.
CBCC: extension BCC learns communities workers similar
confusion matrices described Section 2. Given judgment set, CBCC able
learn confusion matrix community worker, well task
label. method also used number crowdsourcing applications
including web search evaluation sentiment analysis (Venanzi et al., 2014).
experiments, ran CBCC number worker types set two communities
order infer two groups reliable workers less reliable workers
similar results observed higher number communities.
Majority Voting: simple yet popular algorithm estimates
aggregated label one receives votes (Littlestone & Warmuth,
1989; Tran-Thanh et al., 2013). assigns point mass label highest
consensus among set judgments. Thus, algorithm represent
uncertainty around classification considers judgments coming
reliable workers.
Vote Distribution: method estimates true label based empirical probabilities class observed judgment set (Simpson et al., 2015). Specifically,
assigns probability label fraction judgments corresponding
label.
Random: baseline method assigns random class labels tasks,
i.e., assigns uniform probabilities labels.
Note alternative variant BCCTime captures time spent redundant. fact, workers propensity modelled together time spent,
workers accuracy captured confusion matrices. means
model equivalent BCC, already included benchmarks. benchmarks also implemented Infer.NET trained using EP algorithm.
11. particular, refer One coin unconstrained version ZenCrowd (Demartini et al., 2012)
without two unicity SameAs constraints defined original method. suggests
version suitable fair comparison methods.

533

fiVenanzi, Guiver, Kohli & Jennings

experiments, set hyperparameters BCCTime reproduce typical situation
task requester prior knowledge true labels labelling probabilities workers, basic prior knowledge accuracy workers
representing that, priori, assumed better random annotators (Kim
& Ghahramani, 2012). Therefore, workers confusion matrices initialised
slightly higher value diagonal (0.6) lower values rest matrix. Then,
Dirichlet priors p set uninformatively uniform counts12 . priors
confusion matrices initialised higher diagonal value (0.7) meaning
priori workers assumed better random. Gaussian priors tasks
time durations set means 0 = 10 0 = 50 precisions 0 = 0 = 101 ,
meaning priori entity linking task expected completed within 10 50
seconds. Furthermore, initialise Beta prior k function number tasks
0 = 0.7N 0 = 0.3N represent fact priori worker considered
reliable makes valid labelling attempts 70% tasks. Importantly, given
shape distribution workers time completion data observed datasets (see
(k)
Figure 2), apply logarithmic transformation order obtain uniform
distribution workers completion time training data. Finally, priors
benchmarks set equivalently BCCTime.
5.2 Accuracy Metrics
evaluate classification accuracy tested methods measured Area
ROC Curve (AUC) ZC-US ZC-IN average recall WS-AMT.
particular, former standard accuracy metric evaluate performance
binary classifiers range discriminant thresholds applied predictive class
probabilities (Hanley & McNeil, 1982), well suited two ZenCrowd binary
datasets. latter recall averaged class categories (Rosenberg, 2012),
main metric used score probabilistic methods competed 2013
CrowdFlower shared task challenge dataset equivalent WS-AMT (see Section 3.1).
5.3 Results
Table 3 reports AUC seven algorithms ZenCrowd datasets. Specifically,
shows BCCTime BCCPropensity highest accuracy datasets:
AUC 11% higher ZC-IN 8% higher ZC-US, respectively, compared
methods. Among two, BCCTime best method improvement 13%
ZC-IN 1% ZC-US. Similarly, Table 4 reports average recall methods
WS-AMT showing BCCTime highest average recall, 2% higher
second best benchmark (Vote distribution) 4% higher BCCPropensity13 .
means inference time thresholds, already provides valuable information
tasks extracted judgments, also adds extra quality improvement
aggregated labels addition modelling workers propensities.
12. noted cases different type knowledge available workers,
information plugged method selecting appropriate prior distributions.
13. dataset, majority vote performs similarly BCCTime due higher quality
workers

534

fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems

Table 3: AUC tested methods measured ZenCrowd datasets.
highest AUC dataset highlighted bold.
Dataset:
Majority vote
Vote distribution
One coin
Random
BCC
CBCC
BCCPropensity
BCCTime

ZC-US
0.3820
0.2101
0.7204
0.5000
0.6418
0.6730
0.7740
0.7800

Table 4: average recall tested
methods measured WS-AMT.
highest average recall highlighted
bold.

ZC-IN
0.3862
0.3080
0.6263
0.5000
0.5407
0.5544
0.6177
0.6925

Dataset:
Majority vote
Vote distribution
One coin
Random
BCC
CBCC
BCCPropensity
BCCTime

WS-AMT
0.727
0.728
N/A
0.183
0.705
0.711
0.703
0.730

important observation proves information workers completion time
effectively data aggregation. Altogether, information allows model
correctly filter unreliable judgments consequently provide accurate classifications.
Figure 6 shows ROC curve methods ZenCrowd (binary) datasets,
namely plot false positive rate true positive rate obtained different
discriminant thresholds. graph shows true positive rate BCCTime generally higher benchmarks false positive rate. detail, Majority
vote, Vote distribution perform worse Random datasets methods
clearly penalised presence less reliable workers treat workers
equally reliable. Interestingly, One coin performs better BCC CBCC meaning
confusion matrix better approximated single (one coin) parameter
two datasets. Also, looking percentages workers propensities inferred BCCTime reported Table 5, found 93.2% workers ZC-US, 60% workers
ZC-IN 97.3% workers WS-AMT propensity greater 0.5.
means that, ZC-US WS-AMT, workers identified suspected spammers majority estimated reliable different propensity
values. ZC-IN, percentage suspected spammers higher also reflected
lower accuracy judgments respect gold standard labels.
Figure 7 shows mean value inferred upper-bound time threshold (blue
cross points) workers maximum completion time (green asterisked points)
Table 5: propensity workers learnt BCCTime dataset.
Dataset:
ZC-US
ZC-IN
WS-AMT

% high propensity
workers (p(k ) > 0.5)
93.2%
60%
97.3%

535

% low propensity
workers (p(k ) 0.5)
6.8%
30%
2.7%

fiVenanzi, Guiver, Kohli & Jennings

Majority vote

Vote distribution

One coin

Random

BCC

BCCPropensity

CBCC

BCCTime

1

1
0.9

0.8

0.7

True positive rate

True positive rate

0.8
0.6
0.5
0.4
0.3

0.6
0.4
0.2

0.2
0.1
0

0

0.2

0.4

0.6

False positive rate

0.8

0

1

(a) ZC - US

0

0.2

0.4
0.6
False positive rate

0.8

1

(b) ZC -

Figure 6: ROC curve aggregation methods ZC-US (a) ZC-IN (b).

task three datasets. Looking raw data ZenCrowd datasets, average
maximum time spent US workers higher (approx. 1.7 minutes)
Indian workers (approx. 1 minute). also seen datasets
significant portion outliers reach 50 minutes. However, discussed Section
3, know many entity linking tasks fairly simple
easily solved visual inspection candidate URI. imply
normal worker completes task single session (i.e., interrupts)
take long time issue judgment. Interestingly, BCCTime efficiently removes
outliers recovers realistic estimates maximum duration entity
linking task. fact, estimated upper-bound time thresholds lie within smaller time
band, i.e., around 10 seconds ZC-US 40 seconds ZC-IN. Similar results also
observed WS-AMT average observed maximum time significantly higher
average inferred maximum time, thus suggesting BCCTime estimates
also realistic dataset. addition, Figure 7 shows plot
average duration estimated BCCTime (i.e, (E[i ] E[i ])/2 i) average
workers completion time task. graphs show BCCTime estimates
similar micro-tasks three datasets, i.e., 3 5,
estimates obtained workers completion time data much higher: 53 seconds
ZC-US, 45 seconds ZC-IN 80 seconds WS-AMT. Again, due
presence outliers original data significantly bias empirical average times
towards high values. Moreover, measuring variability two sets estimates,
BCCTime estimates much smaller standard deviation 100% lower
empirical averages. means estimates informative
compared normal average times obtained raw workers completion time
data.
536

fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems

10

104
Inferred max. time
Observed max. time

3

Time spent (sec)

Time spent (sec)

104

102
101
10

500

1000 1500
Task index

10

2

100

0

0

Inferred avg. time
Observed avg. time

2000

0

500

(a) ZC - US

Time spent (sec)

Time spent (sec)

10
Inferred max. time
Observed max. time

3

102

10

4
Inferred avg. time
Observed avg. time

102

100

1

0

500

1000 1500
Task index

2000

0

500

1000 1500
Task index

4

10

4
Inferred avg. time
Observed avg. time

Time spent (sec)

Time spent (sec)

Inferred max. time
Observed max. time

103

102

10

2000

(d) ZC -

(c) ZC -

10

2000

(b) ZC - US

104

10

1000 1500
Task index

102

100

1

0

100

200

300

Task index

0

100

200

300

Task index

(e) WS - AMT

(f) WS - AMT

Figure 7: plot inferred (+) observed (*) maximum time spent tasks
ZC-US (a), ZC-IN (c) WS-AMT (e), average time spent tasks
ZC-US (b), ZC-IN (d) WS-AMT (f).

537

fiVenanzi, Guiver, Kohli & Jennings

0

0

10

10

1

AUC

AUC

10

2

10
1

10

3

0

2000

4000

6000

8000

10

10000

0

2000

4000

6000

Num. judgments

Num. judgments

(a) ZC US

(b) ZC

0.8

8000

Majority vote

Average recall

Vote distribution
0.6

BCC
One coin

0.4

CBCC
BCCPropensity

0.2

BCCTime
0

Random
1000 2000 3000 4000 5000 6000
Num. judgments
(c) WS AMT

Figure 8: AUC ZC-US (a) ZC-IN (b) average recall WS-AMT (c)
methods trained increasingly large sub-sets judgments.

evaluate performance methods data sparsity, Figure 8 shows
accuracy measured sub-samples judgments dataset. detail, one coin
accurate sparse judgments ZC-IN ZC-US, WS-AMT
clear winner since methods except Random similar average recall trained
sparse judgments. shows BCCTime current form necessarily
outperform methods sparse data. explained fact
extra latent variables (i.e., workers propensity time thresholds) used improve
quality final labels also require larger set judgments accurately learnt.
However, address issue, possible draw community-based models (e.g.,
CBCC) design hierarchical extension BCCTime over, example, workers
confusion matrices improve robustness sparse data. Here, simplicity, BCCTime presented based simpler instance Bayesian classifier combination framework
(i.e., BCC model), community-based version considered trivial extension.
538

fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems

Table 6: Comparison 21 existing methods computing aggregated labels crowdsourced judgments classified according classification models (binary class multiclass) learning features (worker accuracy, worker confusion matrix, task difficulty, task
duration workers type).

Majority voting
DS - Dawid & Skene (1979)
GLAD - Whitehill et al. (2009)
RY - Raykar et al. (2010)
CUBAM - Welinder et al. (2010)
YU - Yan et al. (2010)
LDA - Wang et al. (2011)
KJ - Kajino et al. (2012)
ZenCrowd - Demartini et al. (2012)
DARE - Bachrach et al. (2012)
MinMaxEntropy - Zhou et al. (2012)
BCC - Kim & Ghahramani (2012)
MSS - Qi et al. (2013)
MLNB - Bragg et al. (2013)
BM - Bi et al. (2014)
GP - Rodriguez et al. (2014)
LU - Liu et al. (2014)
WM -Li et al. (2014)
CBCC - Venanzi et al. (2014)
APM - Nushi et al. (2015)
BCCTime - Proposed method

binary
class
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

multi
class
X
X
X
X
X
X
X
X
X
X
X
X

worker
acc.
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

worker
CF
X
X
X
X
X

task
diff.
X
X
X
X

task
duration
X
X

worker
type
X
X
X
X
X
X

6. Related Work
review rest previous work relating aggregation models time analysis
crowdsourcing contexts extending background methods already considered
experimental evaluation. recent years, large body literature focussed
development smart data aggregation methods aid requesters combining judgments
multiple workers. general, existing methods vary assumptions complexity
modelling different aspects labelling noise. interested reader may refer
survey Sheshadri Lease (2013), well summary Table 6 lists
popular methods comparison approach.
particular, methods able handle binary classification problems, i.e., workers vote objects two possible classes, multi-class
classification problems, i.e., workers vote objects two
classes. Among these, many approaches use one coin model introduced benchmarks. detail, model represents workers reliability single parameter
defined within range [0, 1] (0 = unreliable worker, 1 = reliable worker) (Karger et al.,
2011; Liu et al., 2012; Demartini et al., 2012; Li et al., 2014; Nushi, Singla, Gruenheid, Zamanian, Krause, & Kossmann, 2015). Specifically, Karger et al. combines model
budgetlimited task allocation framework provides strong theoretical guarantees
539

fiVenanzi, Guiver, Kohli & Jennings

asymptotical optimality inference workers reliability worker-task
matching. Liu et al. uses general variational inference model reduces Karger
et al.s method, well algorithms special conditions. methods use
two coin model represents bias worker towards positive labelling class
(specificity) towards negative class (sensitivity) (Raykar, Yu, Zhao, Valadez, Florin,
Bogoni, & Moy, 2010; Rodrigues, Pereira, & Ribeiro, 2014; Bragg, Mausam, & Weld, 2013).
Then, quantities may inferred using logistic regression work Raykar
et al. maximumaposteriori approaches work Bragg et al. Alternatively,
Rodrigues et al. uses two coin model embedded Gaussian process classification
framework compute predictive probabilities aggregated labels workers
reliability using EP. Along lines, models reason difficulty task
affects quality judgment improve reliability aggregated labels (Whitehill et al., 2009; Bachrach et al., 2012; Kajino & Kashima, 2012). area, Whitehill
et al. use logistic regression model incorporate tasks difficulty, together
expertise worker labelling images. contrast, Bachrach et al. use difference
two quantities quantify advantage worker may classifying object within joint difficulty-ability-response model. similar setting, Kajino
Kashima exploit convex problem formulation model improve efficiency
inferring quantities numerical optimisation method. Additional factors,
workers motivation propensity particular task, taken account
sophisticated models (Welinder et al., 2010; Yan, Rosales, Fung, Schmidt, Valadez,
Bogoni, Moy, & Dy, 2010; Bi, Wang, Kwok, & Tu, 2014). recently, Nushi et al. (2015)
devised method leverage fact error rates workers directly affected access path follow, access path represents several contextual
features task (e.g., task design, information sources task composition). However,
unlike work, none methods learn confusion matrix worker.
result, represent reliability considering accuracy potential biases
worker single data structure.
Alternative models learn confusion matrices workers presented, among others, works Dawid Skene (1979), Zhou, Basu, Mao, Platt
(2012), Kim Ghahramani (2012) Venanzi et al. (2014). particular, Dawid
Skene introduced first confusion matrix-based model confusion matrices
inferred using expectation-maximisation unsupervised manner. Then, Zhou et al.
extended work include taskspecific latent matrix representing confusability
task perceived workers. However, neither methods consider uncertainty workers reliability parameters models. example,
one label obtained worker, methods may infer worker
perfectly reliable totally incompetent when, reality, worker neither. overcome limitation, methods BCC CBCC capture uncertainty
workers expertise true labels using Bayesian learning framework. two
methods extensively discussed earlier (see Sections 2 5) included benchmarks experiments. Similarly CBCC, methods leverage groups workers
equivalent reliability improve quality aggregated labels limited data
(Li et al., 2014; Bi et al., 2014; Kajino & Kashima, 2012; Yan et al., 2010). However,
already noted, methods use extra information workers
540

fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems

judgments learn probabilistic models. result, unlike approach, cannot take full advantage time information provided crowdsourcing platform
improve quality inference results.
turn problem time analysis crowd generated content. Recently
introduced metric measuring effort required complete crowdsourced task
based area error-time curve (ETA). such, metric supports
idea considering time important factor crowdsourcing effort. regard,
closely related work analysis ZenCrowd datasets (see Section 3) presented
work Difallah, Demartini, Cudre-Mauroux (2012). work showed
workers complete tasks fast slow typically less accurate
others. findings also confirmed work. However, addition, extended
analysis showing judgments quality correlated time spent
workers different ways specific task instances. intuition method
exploits efficiently combine workers completion time features data aggregation
process. Furthermore, earlier work introducing method predicts duration
task based number available features (including tasks price, creation time
number assignments) using survival analysis model presented paper
Wang et al. (2011). However, method deal aggregating labels,
learning accuracy workers, approach.

7. Conclusions
presented evaluated BCCTime, new timesensitive aggregation method simultaneously merges crowd labels estimates duration individual task instances
using principled Bayesian inference. key innovation method leverage extended set features comprising workers completion time judgment set.
appropriately correlated together, features become important indicators reliability worker that, turn, allow us estimate final labels, tasks duration
workers reliability accurately. Specifically, introduced new representation
accuracy profile worker consisting workers confusion matrix,
accounts workers labelling probabilities class, workers propensity
valid labelling, represents workers intention meaningfully participate
labelling process. Furthermore, used latent variables represent duration
task using pairs latent thresholds capture time interval best judgments task likely submitted honest workers. way, model
deal differences time length task instance relating different
type correlation quality received judgments time spent
workers. fact, taskspecific correlations observed experimental
analysis crowdsourced datasets various task instances showed different types
qualitytime trends. Thus, main idea behind BCCTime model trends
aggregation crowd judgments make reliable inference quantities
interest. extensive experimental validation real-world datasets, showed
BCCTime produces significantly accurate classifications estimates
tasks duration considerably informative common heuristics obtained
raw workers completion time data.
541

fiVenanzi, Guiver, Kohli & Jennings

background, several implications work concerning various
aspects reliable crowdsourcing systems. Firstly, process designing task
take exploit unbiased tasks duration estimated BCCTime. shown,
information valid proxy assess difficulty task therefore supports number
decisionmaking problems fair pricing difficult tasks defining fair
bonuses honest workers. Secondly, workers propensity valid labelling uncovers
additional dimension workers reliability enables us score attitude
towards correctly approaching given task. information useful select different task
designs engaging tasks workers systematically approach task incorrectly.
Thirdly, method uses features readily available common crowdsourcing
systems, allows faster take technology real applications.
Building advances, several aspects current model indicate
promising directions improvements. example, consider time
dependencies accuracy profile worker capture fact workers typically
improve skills time performing sequence tasks. doing, possible
take advantage temporal dynamics potentially improve quality final
labels. addition, crowdsourcing settings involve continuous-valued judgments
currently supported method. deal cases, number non
trivial extensions generative model and, turn, new treatment probabilistic
inference required.

8. Acknowledgments
authors gratefully acknowledge funding bodies, Microsoft UK Research
Council ORCHID project, grant EP/I011587/1ORCHID, Bhaskar Mitra (Microsoft) proofreading manuscript.

References
Alonso, O., Rose, D. E., & Stewart, B. (2008). Crowdsourcing relevance evaluation.
ACM SigIR Forum, Vol. 42, pp. 915, New York, NY, USA. ACM.
Bachrach, Y., Graepel, T., Minka, T., & Guiver, J. (2012). grade test without
knowing answersa Bayesian graphical model adaptive crowdsourcing
aptitude testing. Proceedings 29th International Conference Machine
Learning (ICML), pp. 11831190.
Bernstein, M., Little, G., Miller, R., Hartmann, B., Ackerman, M., Karger, D., Crowell, D.,
& Panovich, K. (2010). Soylent: word processor crowd inside. Proceedings
23nd annual ACM symposium User interface software technology, pp.
313322. ACM.
Bi, W., Wang, L., Kwok, J. T., & Tu, Z. (2014). Learning predict crowdsourced
data. Proceedings 30th International Conference Uncertainty Artificial
Intelligence (UAI).
Bishop, C. (2006). Pattern recognition machine learning, Vol. 4. Springer New York.
542

fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems

Bragg, J., Mausam, & Weld, D. (2013). Crowdsourcing multi-label classification taxonomy creation. First AAAI Conference Human Computation Crowdsourcing,
pp. 2533.
Dawid, A., & Skene, A. (1979). Maximum likelihood estimation observer error-rates using
em algorithm. Applied statistics, 2028.
Demartini, G., Difallah, D. E., & Cudre-Mauroux, P. (2012). Zencrowd: Leveraging probabilistic reasoning crowdsourcing techniques large-scale entity linking.
Proceedings 21st international conference World Wide Web (WWW), pp.
469478.
Difallah, D. E., Demartini, G., & Cudre-Mauroux, P. (2012). Mechanical cheat: Spamming
schemes adversarial techniques crowdsourcing platforms. CrowdSearch, pp.
2630.
Faradani, S., Hartmann, B., & Ipeirotis, P. G. (2011). Whats right price? pricing tasks
finishing time. Human Computation, Vol. WS-11-11 AAAI Workshops,
pp. 2631. AAAI.
Hanley, J. A., & McNeil, B. J. (1982). meaning use area receiver
operating characteristic (roc) curve. Radiology, 143 (1), 2936.
Herbrich, R., Minka, T., & Graepel, T. (2007). Trueskill(tm): Bayesian skill rating
system. Advances Neural Information Processing Systems (NIPS), pp. 569576.
MIT Press.
Huff, C., & Tingley, D. (2015). people? evaluating demographic characteristics political preferences mturk survey respondents. Research & Politics,
2 (3), 2053168015604648.
Kajino, H., & Kashima, H. (2012). Convex formulations learning crowds. Transactions Japanese Society Artificial Intelligence, 27, 133142.
Kamar, E., Hacker, S., & Horvitz, E. (2012). Combining human machine intelligence
large-scale crowdsourcing. Proceedings 11th International Conference
Autonomous Agents Multiagent Systems (AAMAS), pp. 467474.
Kamar, E., Kapoor, A., & Horvitz, E. (2015). Identifying accounting task-dependent
bias crowdsourcing. Third AAAI Conference Human Computation
Crowdsourcing.
Karger, D., Oh, S., & Shah, D. (2011). Iterative learning reliable crowdsourcing systems.
Advances Neural Information Processing Systems (NIPS), pp. 19531961. MIT
Press.
Kazai, G. (2011). search quality crowdsourcing search engine evaluation.
Advances information retrieval, pp. 165176. Springer.
Kim, H., & Ghahramani, Z. (2012). Bayesian classifier combination. International
Conference Artificial Intelligence Statistics, pp. 619627.
Li, H., Zhao, B., & Fuxman, A. (2014). wisdom minority: discovering targeting
right group workers crowdsourcing. Proceedings 23rd International
Conference World Wide Web (WWW), pp. 165176.
543

fiVenanzi, Guiver, Kohli & Jennings

Littlestone, N., & Warmuth, M. K. (1989). weighted majority algorithm. 30th
Annual Symposium Foundations Computer Science, pp. 256261. IEEE.
Liu, Q., Peng, J., & Ihler, A. (2012). Variational inference crowdsourcing. Advances
Neural Information Processing Systems (NIPS), pp. 692700. MIT Press.
Minka, T. (2001). Expectation propagation approximate Bayesian inference. Proceedings 17th Conference Uncertainty Artificial Intelligence (UAI), pp.
362369.
Minka, T., & Winn, J. (2008). Gates. Advances Neural Information Processing Systems
(NIPS), pp. 10731080. MIT Press.
Minka, T., Winn, J., Guiver, J., & Knowles, D. (2014). Infer.NET 2.6. Microsoft Research
Cambridge.
Minka, T. P. (2001). family algorithms approximate Bayesian inference. Ph.D.
thesis, Massachusetts Institute Technology.
Nushi, B., Singla, A., Gruenheid, A., Zamanian, E., Krause, A., & Kossmann, D. (2015).
Crowd access path optimization: Diversity matters. Third AAAI Conference
Human Computation Crowdsourcing, pp. 130139.
Ramchurn, S. D., Huynh, T. D., Ikuno, Y., Flann, J., Wu, F., Moreau, L., Jennings, N. R.,
Fischer, J. E., Jiang, W., Rodden, T., et al. (2015). Hac-er: disaster response system
based human-agent collectives. 2015 International Conference Autonomous
Agents Multiagent Systems, pp. 533541.
Raykar, V., Yu, S., Zhao, L., Valadez, G., Florin, C., Bogoni, L., & Moy, L. (2010). Learning
crowds. Journal Machine Learning Research, 11, 12971322.
Rodrigues, F., Pereira, F., & Ribeiro, B. (2014). Gaussian process classification active
learning multiple annotators. Proceedings 31st International Conference
Machine Learning (ICML), pp. 433441.
Rosenberg, A. (2012). Classifying skewed data: Importance weighting optimize average
recall. INTERSPEECH, pp. 22422245.
Sheng, V., Provost, F., & Ipeirotis, P. (2008). Get another label? Improving data quality
data mining using multiple, noisy labelers. Proceedings 14th International
Conference Knowledge Discovery Data Mining (SIGKDD), pp. 614622. ACM.
Sheshadri, A., & Lease, M. (2013). Square: benchmark research computing crowd
consensus. Proceedings 1st AAAI Conference Human Computation
Crowdsourcing (HCOMP), pp. 156164.
Simpson, E., Roberts, S., Psorakis, I., & Smith, A. (2013). Dynamic bayesian combination
multiple imperfect classifiers. Decision Making Imperfection, pp. 135.
Springer.
Simpson, E., Venanzi, M., Reece, S., Kohli, P., Guiver, J., Roberts, S., & Jennings, N. R.
(2015). Language understanding wild: Combining crowdsourcing machine
learning. 24th International World Wide Web Conference (WWW), pp. 9921002.
ACM.
544

fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems

Simpson, E. (2014). Combined Decision Making Multiple Agents. Ph.D. thesis, University Oxford.
Tran-Thanh, L., Venanzi, M., Rogers, A., & Jennings, N. R. (2013). Efficient Budget Allocation Accuracy Guarantees Crowdsourcing Classification Tasks.
12th International Conference Autonomous Agents Multi-Agent Systems (AAMAS), pp. 901908.
Venanzi, M., Guiver, J., Kazai, G., Kohli, P., & Shokouhi, M. (2014). Community-based
bayesian aggregation models crowdsourcing. 23rd International Conference
World Wide Web (WWW), pp. 155164. ACM.
Venanzi, M., Rogers, A., & Jennings, N. R. (2015a). Weather Sentiment - Amazon Mechanical Turk dataset. University Southampton.
Venanzi, M., Teacy, W., Rogers, A., & Jennings, N. R. (2015b). Bayesian modelling
community-based multidimensional trust participatory sensing data sparsity.
Twenty-Fourth International Joint Conference Artificial Intelligence (IJCAI15), pp. 717724.
Wang, J., Faridani, S., & Ipeirotis, P. (2011). Estimating completion time crowdsourced tasks using survival analysis models. Crowdsourcing Search Data
Mining (CSDM), Vol. 31, pp. 3134.
Welinder, P., Branson, S., Belongie, S., & Perona, P. (2010). multidimensional wisdom
crowds. Advances Neural Information Processing Systems (NIPS), Vol. 10,
pp. 24242432. MIT Press.
Whitehill, J., Ruvolo, P., Wu, T., Bergsma, J., & Movellan, J. R. (2009). Whose vote
count more: Optimal integration labels labelers unknown expertise.
Advances Neural Information Processing Systems (NIPS), Vol. 22, pp. 2035
2043. MIT Press.
Yan, Y., Rosales, R., Fung, G., Schmidt, M., Valadez, G. H., Bogoni, L., Moy, L., & Dy,
J. (2010). Modeling annotator expertise: Learning everybody knows bit
something. International Conference Artificial Intelligence Statistics, pp.
932939.
Zhou, D., Basu, S., Mao, Y., & Platt, J. (2012). Learning wisdom crowds
minimax entropy. Advances Neural Information Processing Systems (NIPS), pp.
21952203. MIT Press.
Zilli, D., Parson, O., Merrett, G. V., & Rogers, A. (2014). hidden markov model-based
acoustic cicada detector crowdsourced smartphone biodiversity monitoring. Journal Artificial Intelligence Research, 805827.

545

fiJournal Artificial Intelligence Research 56 (2016) 613-656

Submitted 03/16; published 08/16

Datalog Ontology Consolidation
Cristhian Ariel D. Deagustini
Mara Vanina Martnez
Marcelo A. Falappa
Guillermo R. Simari

cadd@cs.uns.edu.ar
mvm@cs.uns.edu.ar
mfalappa@cs.uns.edu.ar
grs@cs.uns.edu.ar

AI R&D Lab., Institute Computer Science Engineering (ICIC)
Consejo Nacional de Investigaciones Cientficas Tecnicas (CONICET)
Universidad Nacional del Sur (UNS), Alem 1253,
(B8000CPB) Baha Blanca, Argentina.

Abstract
Knowledge bases form ontologies receiving increasing attention
allow clearly represent available knowledge, includes knowledge constraints imposed domain users. particular, Datalog
ontologies attractive property decidability possibility
dealing massive amounts data real world environments; however,
case many ontological languages, application collaborative environments
often lead inconsistency related issues. paper introduce notion incoherence regarding Datalog ontologies, terms satisfiability sets constraints,
show specific conditions incoherence leads inconsistent Datalog ontologies.
main contribution work novel approach restore consistency
coherence Datalog ontologies. proposed approach based kernel contraction
restoration performed application incision functions select formulas
delete. Nevertheless, instead working minimal incoherent/inconsistent sets encountered ontologies, operators produce incisions non-minimal structures called
clusters. present construction consolidation operators, along properties
expected satisfied them. Finally, establish relation construction properties means representation theorem. Although proposal
presented Datalog ontologies consolidation, operators applied
types ontological languages, Description Logics, making apt used
collaborative environments like Semantic Web.

1. Introduction
integration different systems, interaction resulting integration, led
host pervasive practical problems challenging research opportunities;
interesting ones occurs Webs collaborative environments, e.g., e-commerce,
arrival Semantic Web, ontology engineering. However,
collaboration among systems brings along problem conflicting pieces information
likely appear knowledge repositories evolve. Admittedly, management
conflicting information important challenging issue faced (Gomez,
Chesnevar, & Simari, 2010; Haase, van Harmelen, Huang, Stuckenschmidt, & Sure, 2005;
Huang, van Harmelen, & ten Teije, 2005; Bell, Qi, & Liu, 2007), specially integrating
c
2016
AI Access Foundation. rights reserved.

fiDeagustini, Martinez, Falappa & Simari

knowledge coming different sources (Black, Hunter, & Pan, 2009; Baral, Kraus, &
Minker, 1991; Amgoud & Kaci, 2005), knowledge expected exploited
reasoning process. context, knowledge bases form ontologies becoming useful device provide convenient way represent intensional
extensional knowledge application domain. Moreover, expressive power ontologies allows perform important tasks data integration (Lenzerini, 2002), also plays
role great importance aforementioned Semantic Web (Berners-Lee, Hendler, &
Lassila, 2001). work adopt Datalog ontologies, family rule-based ontology
languages (Cal, Gottlob, & Lukasiewicz, 2012). Datalog enables modular rule-based
style knowledge representation, represent syntactical fragments first-order
logic (FOL) answering Boolean Conjunctive Query (BCQs) Q set
Datalog rules input database equivalent classical entailment check
|= Q. Tractable fragments Datalog guarantee termination query answering
procedures polynomial time data complexity first-order rewritability. Moreover, ontologies described using existential rules generalize several well-known Description
Logics (DLs); particular, linear guarded Datalog (two basic tractable fragments
family) strictly expressive whole DL-Lite family (Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2005), guarded Datalog strictly expressive
EL (Brandt, 2004; Baader, Brandt, & Lutz, 2005). Therefore, results presented
paper extend directly DLs well. properties Datalog together
expressive power, fact keeps syntax closer used relational
databases greater readability, make useful modeling real applications,
ontology querying, Web data extraction, data exchange, ontology-based data access,
data integration.
focus two particular problems arise integration and/or evolution
information systems: inconsistency incoherence. Inconsistency refers lack
models theory. hand, ontological settings, incoherence refers set
ontological rules cannot applied without leading violations constraints
imposed knowledge, making unsatisfiable. Incoherence inconsistency,
arise automated procedures data integration ontology matching, may
serious issues real world applications. Since standard ontology languages adhere
classical FOL semantics, classical inference semantics fails presence kind
problems. Thus, important focus formalization methods address
inconsistency incoherence ontologies able cope users expectations
terms effectiveness procedures query answering meaning
answers potential conflict exists.
paper addresses problem handling inconsistencies incoherences
may appear Datalog ontologies. regard, propose general framework
aims consolidation Datalog ontologies (i.e., solving every conflict coherence
consistency them). is, consolidation operator takes input (possibly)
incoherent inconsistent Datalog ontology returns another Datalog ontology
conflicts amended, thus ensuring coherent consistent.
usual setting, assumption minimal change made, say,
expected consolidation process changes original ontology little possible.
approach presented based use incision functions (Hansson, 1993, 1994, 1997,
614

fiDatalog Ontology Consolidation

2001) Belief Revision literature. Instead operators account
information included conflicts knowledge base, work aim capture
consolidation operators consider information included KB solving
conflicts. main contributions work following:
introduce notion incoherence tailored Datalog . achieve adapt
setting similar notions Description Logics. Also, look relationship incoherence inconsistency impacts consolidation process.
provide set properties expected satisfied consolidation operators
Datalog ontologies means postulates. postulates provide formal
characterization consolidation operator without focusing consolidation
process actually performed, thus providing formal comparison framework
consolidation operators. postulates consider intuitions classic Belief
Revision; nevertheless, adapted Datalog ontological setting (and
could also adapted suit ontological languages), meaning
two versions (one addressing incoherence another one inconsistency).
present complete construction consolidation operators take (possibly)
incoherent inconsistent Datalog ontology gives result consistent
coherent one. noteworthy characteristic operators involves two
steps approach, first considering incoherence conflicts, solving inconsistency
conflicts latter step, helping improve final result terms information
needs deleted solve conflicts.
study relationship formal properties operator
construction propose, demonstrating equivalent; thus, shows
consolidation operator satisfying properties corresponds construction
introduced work.
paper organized follows: Section 2 introduce necessary notions
Datalog Belief Revision. Next, though inconsistency incoherence related,
also two distinct problems setting ontological knowledge bases
particular, clear separation intensional extensional knowledge.
Therefore, Section 3, discuss two notions Datalog ontologies, relate
other, reasons need treated combination separately.
Then, Section 4 present properties ontology consolidation operator must
satisfy, Section 5 introduce process used restore consistency coherence
Datalog ontologies, relate presented process given properties means
representation theorem. Next, present complete example depicting entire
consolidation process. Finally, Sections 7 8 discuss related work different
areas Artificial Intelligence Database Theory, provide conclusions future
lines research, respectively.

2. Preliminaries Background
facilitate reading, begin introducing notions Datalog Belief
Revision needed rest paper.
615

fiDeagustini, Martinez, Falappa & Simari

2.1 Preliminaries Datalog
First, recall basic notions Datalog ontologies used paper
(see Cal et al., 2012 details). Datalog extends Datalog allowing existential
quantification rule heads, together extensions enumerate below,
limiting interaction elements order achieve tractability.
assume domain discourse Datalog ontology consists
countable set data constants , countable set nulls N (as place holders unknown
values), countable set variables V. also assume different constants represent
different values (unique names assumption). distinguish constants variables,
adopt standard notation logic programming, variable names begin
uppercase letters, constants predicate symbols begin lowercase letters.
assume relational schema R finite set predicate symbols (or simply predicates). term constant, null, variable. atom form p(t1 , . . . , tn ),
p n-ary predicate t1 , . . . , tn terms; atom ground iff terms
constants. Let L first-order language R L; LR denotes
sublanguage generated R. database (instance) R finite set atoms predicates R terms N . homomorphism constants, nulls variables
mapping h : N V N V (i) c implies h(c) = c, (ii)
c N implies h(c) N , (iii) h naturally extended atoms, sets atoms,
conjunctions atoms.
Given relational schema R, tuple-generating dependency (TGD) first-order
formula form XY(X, Y) Z(X, Z) (X, Y) (X, Z) conjunctions atoms R called body (denoted body()) head (denoted head()),
respectively. Consider database relational schema R, TGD R
form (X, Y) Z (X, Z). Then, applicable exists homomorphism
h maps atoms (X, Y) atoms D. Let applicable D, h0
homomorphism extends h follows: Xi X, h0 (Xi ) = h(Xi );
Zj Z, h0 (Zj ) = zj , zj fresh null, i.e., zj N , zj occur D, zj
lexicographically follows nulls already introduced. application adds
atom h0 ((X, Z)) already D. application say
satisfied D. Chase database set TGDs , denoted chase(D, ),
exhaustive application TGDs (Cal et al., 2012) breadth-first (level-saturating) fashion, leads (possibly infinite) chase . important
remark BCQs Q evaluated chase , i.e.,
|= Q equivalent chase(D, ) |= Q (Cal et al., 2012).
Negative constraints (NCs) first-order formulas form X(X) ,
(X) conjunction atoms (without nulls) head truth constant false,
denoted . NC satisfied database set TGDs iff
exist homomorphism h maps atoms (X) D, every
TGD satisfied, i.e., atoms body cannot true together.
Equality-generating dependencies (EGDs) first-order formulas form
X(X) Xi = Xj , (X) conjunction atoms, Xi Xj variables X. EGD satisfied database R iff, whenever exists
homomorphism h h((X)) D, holds h(Xi ) = h(Xj ). work
616

fiDatalog Ontology Consolidation

focus particular class EGDs, called separable (Cal et al., 2012); intuitively,
separability EGDs w.r.t. set TGDs states that, EGD violated, atoms
contained reason violation (and application TGDs); i.e.,
EGD E violated apply TGDs database D, EGD
also violated D. Separability standard assumption Datalog ontology, one
important features family languages focus decidable (Cal,
Lembo, & Rosati, 2003) (actually tractable) fragments Datalog .
NCs EGDs play important role matter conflicts Datalog ontologies.
fact, approach present work ensure neither NCs EGDs
violated resulting ontology. Also, important remark, note restriction
using separable EGDs makes certain cases conflicts considered
proposal. treatment cases, though interesting technical point view,
outside scope work since focus tractable fragments Datalog .
usual case literature, general universal quantifiers TGDs,
negative constraints EGDs omitted, sets dependencies constraints
assumed finite. presented different ways expressing
knowledge Datalog , ready formally define Datalog ontologies.
Definition 1 (Datalog Ontology) Datalog ontology KB = (D, ), =
E NC , consists database instance finite set ground atoms (without
nulls), set TGDs , set separable EGDs E set NCs NC .
Otherwise explicitly said, paper clear context refer
component KB set constraints ontology, without distinguishing
dependencies constraints. Given database R set constraints
= E NC , set models , denoted mods(D, ), set
databases B B every formula satisfied. following
example shows simple Datalog ontology; ontology describes knowledge
therapy/psychology domain.
Example 1 (Datalog Ontology)

D: {a1 : therapy(charlie), a2 : dating(kate, charlie),




a3 : therapist(kate), a4 : belongs to(g1 , charlie),






5 : therapy(patrick ), a6 : belongs to(g2 , ed ),





7 : belongs to(g1 , kate)}







NC : {1 : treating(T , P ) dating(T , P ) }
KB =


E : {1 : treating(T , P ) treating(T 0 , P ) = 0 }








: {1 : therapy(P ) patient(P ),




2 : therapist(T ) belongs to(G, ) leads(T , G),




3 : leads(T , G) belongs to(G, P ) treating(T , P ),



4 : treating(T , P ) therapist(T )}













































set TGDs expresses dependencies as: TGD 1 states person P
therapy P patient, 2 establishes therapist belongs group
617

fiDeagustini, Martinez, Falappa & Simari

G leader group. NC 1 states patient cannot dating
therapist, EGD 1 states every patient treatment one therapist.
Following classical notion consistency, say consistent Datalog ontology
non-empty set models.
Definition 2 (Consistency) Datalog ontology KB = (D, ) consistent iff
mods(D, ) 6= . say KB inconsistent otherwise.
Example 2 Consider Datalog ontology example above; ontology clearly

inconsistent. Database instance clearly model since least TGD 2
applicable D, superset satisfies TGDs constraints time. instance TGDs 2 applicable creating atom
leads(kate, g1 ) making 3 applicable resulting new atom treating(kate, charlie),
together dating(kate, charlie) (that already D) violate NC 1 ,
therapist dating one patients.
rest paper, otherwise explicitly stated KB = (D, ) denote Datalog
ontology = E NC , database instance, set
TGDs, E set separable EGDs NC set NCs .
2.2 Background Belief Revision
Establishing origins scientific ideas difficult task sometimes controversial; nevertheless, could argued origins belief change theory go back
work Isaac Levi (1977), discussed problems concerning field research,
William Harpers proposal rational way interrelate belief change operators (Harper, 1975). However, main advances belief change theory came
1980s Carlos Alchourron David Makinson studied changes legal codes (Alchourron & Makinson, 1981), Peter Gardenforss introduced rational postulates
change operators (Gardenfors, 1982). that, three authors produced foundational paper containing became known AGM model (Alchourron, Gardenfors,
& Makinson, 1985). core contribution AGM model presentation
new general formal framework study belief change; today, work
considered cornerstone belief change theory evolved.
Since introduction AGM model, different frameworks belief dynamics
respective epistemic models proposed. epistemic model corresponds
formalism beliefs represented, providing framework different
kinds operators defined. AGM model conceived idealistic theory
rational change epistemic states represented belief sets (sets sentences
closed logical consequence, commonly denoted boldface), epistemic input
represented sentence. AGM model, three basic change operators defined:
expansion, contraction, revision. rest section, whenever use term
consistent inconsistent, refer traditional notion inconsistency knowledge
base models. Let K belief set, change operations follows:
618

fiDatalog Ontology Consolidation

Expansions: result expanding K sentence possibly larger set
infers ; intuitively, belief , hopefully consistent given epistemic state,
directly added K.
Contractions: result contracting K possibly smaller set
infer , unless tautology;
Revisions: result revising K set neither extends part
set K. general, fallacy consistently inferred
revision K .
great importance AGM comes providing axiomatic characterizations contraction revision terms rationality postulates. rationality postulates regard
operators black boxes, characterizing do, explaining
it. words, behavior constrained regard inputs basic cases,
without describing internal mechanisms used achieving behavior, crucial
say contraction revision operators also obtained via constructive
approaches. AGM contractions realized partial meet contractions,
based selection among (maximal) subsets K imply . Via Levis
identity (Gardenfors, 1988), associated revision operations called partial meet revisions
obtained. Another possible approach contraction based selection among
(minimal) subsets K contribute make K imply , safe contraction (Alchourron & Makinson, 1985). general variant approach, known
kernel contraction, introduced later (Hansson, 1994). shown safe
contractions kernel contractions equivalent partial meet contractions, hence
AGM approach contraction (Hansson, 1994, 2001).
particularly interesting characteristic kernel contraction may concerned
changes symbolic level since suitable applied belief bases (set
sentences closed consequence relation) well belief sets. Thus, matters
beliefs actually represented. happen AGM approach,
studies changes knowledge level since uses belief sets. distinction
knowledge symbolic level proposed Allen Newell (1982). According Newell,
knowledge level lies symbolic level, latter used somehow represent
former. this, belief bases different symbolic content may represent
knowledge. importance that, although statically equivalent
(they represent beliefs), equivalent belief bases could dynamically different
choose use approach working directly them, kernel contraction.
Besides three basic operations mentioned, years additional operations
developed Belief Revision achieve different behaviors. instance,
belief base inconsistent, removal enough sentences lead consistent
state. additional operation called consolidation, consolidation belief base
K denoted K ! (see Hansson, 1991, 2001). focus last operation,
inherently different contraction revision, since ultimate goal obtain
consistent belief base possibly inconsistent one (without given epistemic
input), rather revising knowledge base specific formula removing
particular formula it. consolidation K obtained natural way belief
619

fiDeagustini, Martinez, Falappa & Simari

bases contracting falsum, i.e., K ! = K , represents contraction
operator; process restores consistency attending every conflict K (Hansson, 1991).

3. Incoherence Inconsistency Problems Related Datalog
Ontology Consolidation
problem obtaining consistent knowledge inconsistent knowledge base
natural many computer science fields. knowledge evolves, contradictions likely
appear, inconsistencies handled way affect
quality information obtained database.
setting Consistent Query Answering (CQA), repairing relational databases,
inconsistency-tolerant query answering ontological languages (Arenas, Bertossi, &
Chomicki, 1999; Lembo, Lenzerini, Rosati, Ruzzi, & Savo, 2010; Lukasiewicz, Martinez, &
Simari, 2012), often assumption made set expresses semantics
data component D, internal conflict set constraints
constraints subject changes time. means first, set
always satisfiable, sense application inevitably yield consistency
problem. Second, result assumption, must case conflicts come
data contained database instance, part ontology
must modified order restore consistency. Although reasonable assumption
make, specially case single ontology, work focus
general setting, consider data constraints change time
become conflicting. general scenario, knowledge evolves (and ontology
represents it) data related issues appear, also constraint related ones.
argue also important identify separate sources conflicts
Datalog ontologies. previous section defined inconsistency Datalog
ontology based lack models. operational point view, conflicts appear
Datalog ontology whenever NC EGD violated, is, whenever body
one constraint mapped either atoms atoms obtained
application TGDs . Beside conflicts, also
focus relationship set TGDs set NCs EGDs,
could happen (a subset of) TGDs cannot applied without leading
always violation NCs EGDs. Note case clearly data
database instance problem, database TGDs applicable
inevitable produce inconsistent ontology. issue related unsatisfiability
problem concept ontology, known Description Logics community
incoherence (Flouris, Huang, Pan, Plexousakis, & Wache, 2006; Schlobach & Cornet, 2003;
Borgida, 1995; Beneventano & Bergamaschi, 1997; Kalyanpur, Parsia, Sirin, & Hendler,
2005; Schlobach, Huang, Cornet, & van Harmelen, 2007; Qi & Hunter, 2007). Incoherence
particularly important combining multiple ontologies since constraints
imposed one data could (possibly) represent conflicting models
application hand. Clearly, notions incoherence inconsistency highly
related; fact, Flouris et al.s (2006) work establish relation incoherence
inconsistency, considering incoherence particular form inconsistency.
620

fiDatalog Ontology Consolidation

Later section present complete definition incoherence Datalog , based
concept unsatisfiability sets TGDs. Nevertheless, sufficient
know proposed notion incoherence states given set unsatisfiable
constraints possible find set atoms KB = (D, ) consistent
ontology time TGDs applicable D. means
Datalog ontology consistent even set constraints incoherent, long
database instance make dependencies applicable. hand,
Datalog ontology inconsistent even set constraints satisfiable,
e.g., KB = ({tall(peter), small(peter)}, {tall(X) small(X) }), (empty)
set dependencies trivially satisfiable thus ontology coherent; ontology is,
nevertheless, inconsistent.
formalizing notion incoherence use Datalog setting
need identify set atoms relevant given set TGDs. Intuitively, say
set atoms relevant set TGDs atoms set
application generates atoms needed apply TGDs , i.e.,
triggers application every TGD .
Definition 3 (Relevant Set Atoms Set TGDs) Let R relational
schema, set TGDs, (possibly existentially closed) non-empty set
atoms, R. say relevant iff form
XY(X, Y) Z(X, Z) holds chase(A, ) |= XY(X, Y).
clear context, singleton set = {a} relevant
say atom relevant .
Example 3 (Relevant Set Atoms) Consider following constraints:

= {1 : supervises(X , ) supervisor (X ),
2 : supervisor (X ) makes decisions(X ) leads department(X , D),
3 : employee(X ) works in(X , D)}
Consider set A1 = {supervises(walter , jesse), makes decisions(walter ), employee(jesse)}.
set relevant set atoms set constraints = {1 , 2 , 3 }, since 1
3 directly applicable A1 2 becomes applicable apply 1 (i.e.,
chase entails atom supervisor (walter ), together makes decisions(walter )
triggers 2 ).
However, set A2 = {supervises(walter , jesse), makes decisions(gus)} relevant
. Note even though 1 applicable A2 , TGDs 2 3 never applied
chase(A2 , ), since atoms bodies never generated chase(A2 , ).
instance, consider TGD 2 . chase create atom
supervisor(walter), nevertheless still cannot trigger 2 since
cannot generate atom makes decisions(walter ), atom makes decisions(gus)
already A2 match constant value.
present notion coherence Datalog , adapts efforts made
DLs Schlobach Cornets (2003) Flouris et al.s (2006). conception
621

fiDeagustini, Martinez, Falappa & Simari

(in)coherence based notion satisfiability set TGDs w.r.t. set
constraints. Intuitively, set dependencies satisfiable relevant set
atoms triggers application dependencies set produce
violation constraint NC E , i.e., TGDs satisfied along NCs
EGDs KB .
Definition 4 (Satisfiability Set TGDs w.r.t. Set Constraints) Let R
relational schema, set TGDs, N NC E , R. set
satisfiable w.r.t. N iff set (possibly existentially closed) atoms R
relevant mods(A, N ) 6= . say unsatisfiable w.r.t.
N iff satisfiable w.r.t. N . Furthermore, satisfiable w.r.t. NC E iff
unsatisfiable w.r.t. N N NC E .
rest paper sometimes write set TGDs (un)satisfiable omitting
set constraints, context particular ontology
fixed set constraints NC E since set TGDs satisfiable w.r.t. NC E
satisfiable w.r.t. subset and, hand, set TGDs unsatisfiable
w.r.t. subset NC E also unsatisfiable w.r.t. whole set constraints.
Example 4 (Unsatisfiable Sets Dependencies) Consider following constraints.

1NC = { : risky job(P ) unstable(P ) }
1T = {1 : dangerous work (W ) works in(W, P ) risky job(P ),
2 : therapy(P ) unstable(P )}
set 1T satisfiable set TGDs, even though simultaneous application
1 2 may violate formula 1NC 1E , hold every relevant
set atoms. Consider example relevant set D1 = {dangerous work (police),
works in(police, marty), therapy(rust)}; D1 relevant set 1T , however,
mods(D1 , 1T 1NC 1E ) 6= 1T satisfiable.
hand, example unsatisfiability consider following constraints:
2NC = {1 : sore throat(X) sing(X) }
2T = {1 : rock singer (X) sing loud (X), 2 : sing loud (X) sore throat(X),
3 : rock singer (X) sing(X)}
set 2T unsatisfiable set dependencies, application TGDs {1 , 2 , 3 }
relevant set atoms cause violation 1 . instance, consider
relevant atom rock singer (axl): application 2T {rock singer (axl)}
causes violation 1 considered together 2T , therefore
mods({rock singer (axl)}, 2T 2NC 2E ) = . Note set relevant atoms
cause violation 1 .
ready formally define coherence Datalog ontology. Intuitively,
ontology coherent subset TGDs unsatisfiable w.r.t.
constraints ontology.
622

fiDatalog Ontology Consolidation

Definition 5 (Coherence) ontology KB coherent satisfiable
w.r.t. NC E . Also, KB said incoherent iff coherent.
Example 5 (Coherence) Consider sets dependencies constraints defined Ex-

ample 4 arbitrary database instance D. see Datalog ontology
KB 1 = (D, 1T 1NC 1E ) coherent, KB 2 = (D, 2T 2NC 2E ) incoherent.
Considering incoherence set TGDs important consolidation process
Datalog ontologies, since treated appropriately within consolidation process,
incoherent set TGDs may lead trivial solution removing every single relevant
atom (in worst case, entire database instance). may adequate
particular domains, seem desirable outcome general case.
Looking Definitions 4 5 see close relationship
concepts incoherence inconsistency. fact, inferred definitions
incoherent KB induce inconsistent KB database instance contains
set atoms relevant unsatisfiable sets TGDs. result captured
following proposition (proofs results presented Appendix A).
Proposition 1 KB incoherent exists relevant
unsatisfiable set U KB = (D, ) inconsistent.
instance relationship, consider following representative example.
Example 6 (Relating Incoherence Inconsistency) Consider following ontology.

KB =


: {a1 : sing(simone), a2 : rock singer (axl ), a3 : sing loud (ronnie),




a4 : fans(ronnie), a5 : rock singer (ronnie), a6 : rock singer (roy),




a7 : manage(band1 , richard )}








NC : {1 : sore throat(X) sing(X) ,




2 : private life(X) famous(X) }



E :








:
















{1 : manage(X, ) manage(X, Z) = Z}
{1
2
3
4
5

: rock singer (X) sing loud (X),
: sing loud (X) sore throat(X),
: fans(X) famous(X),
: rock singer (X) sing(X),
: fans(X) private life(X)}

















































hinted previously Example 4, set = {rock singer (axl)}
unsatisfiable set TGDs U = {1 : rock singer (X) sing loud (X), 2 :
sing loud (X) sore throat(X), 4 : rock singer (X) sing(X)}. Since relevant
U conditions Proposition 1 fulfilled, indeed ontology KB = (D, )
inconsistent since 1 violated.
set constraints one presented Example 6 may appear consider scenarios components ontology evolve (perhaps collaboratively
623

fiDeagustini, Martinez, Falappa & Simari

maintained pool users). long new constraints added, incoherence problems
may arise. particular scenario would seem sensible identify modify,
somehow, set incoherent constraints make satisfiable, instead deleting
information ontology; proceed solve remaining inconsistencies, any. is, could beneficial define consolidation processes
changes performed achieve coherence given higher priority changes needed
consistency possible. address present twofold proposal consolidation Datalog ontologies: is, obtain new KB 0 begin addressing issues
component w.r.t. components E NC original ontology obtain
new coherent set constraints, giving TGDs necessary. Then, address
problems arising component D, obtaining new one D0 consistent
0T E NC . next section characterize, means set postulates,
consolidation operator takes account considerations.

4. Characterizing Consolidation: Postulates Datalog Ontology
Consolidation Operators
Belief Revision one main areas deals defined principled methods solve
incoherences inconsistencies; explained Section 2, common characterize
change operators means postulates, properties operators must satisfy. section introduce set postulates objective characterizing
consolidation operators Datalog ontologies. start briefly defining scenario
underlying consolidation process introducing characteristics sets formulas focus (Friedman & Halpern, 2001).

4.1 Defining Consolidation Environment
Depending type knowledge base, find two main streams work Belief
Revision. one hand, works based sets formulas closed
consequence relation, called belief sets (Alchourron et al., 1985). known
Belief Revision literature coherence model. hand, option
choose belief bases (Katsuno & Mendelzon, 1991, 1992; Fuhrmann, 1991; Hansson, 1994,
1997, 2001; Falappa, Kern-Isberner, & Simari, 2002), i.e., non-closed sets formulas;
referred foundational model.
Opposite traditional closed world assumption found established areas
like relational databases, one important characteristic Datalog open world
assumption, unknown data represented means null values. consequence,
generation new information language application rules susceptible
infinite (Cal, Gottlob, & Kifer, 2008, 2013), seems make foundational
model appealing choice working setting. Therefore, consolidation
Datalog ontologies chosen follow foundational model. model,
epistemic state (possibly incoherent inconsistent) Datalog ontology.
624

fiDatalog Ontology Consolidation

4.2 Expected Properties Consolidation Operator: Postulates
present set properties consolidation operator Datalog ontologies
must satisfy. use following notation rest paper. Let KB = (D, )
original Datalog ontology consolidated, = E NC . Also,
KB ! denotes Datalog ontology KB ! = (D!, !) resulting consolidation KB ,
D! ! consolidated components KB !, respectively.
necessary differentiate KBs using subscripts. cases, given KB denote
consolidation KB ! = (Di !, !).
ready introduce Ontology Consolidation Postulates (OCP) expected
satisfied consolidation operators. Let set Datalog ontologies.
Then, Datalog ontology consolidation operator ! : function must
satisfy following properties:
OCP 1. (Inclusion) ! D! D.
consolidation process includes resulting ontology formulas belonging original ontology.
OCP 2. (Consistency) KB ! consistent.
ontology obtained consolidation process must consistent, i.e.,
negative constraints equality-generating dependencies
violated apply TGDs ! atoms D!, therefore
mods({D!, !}) 6= .
OCP 3. (Coherence) KB ! coherent.
ontology obtained consolidation process must coherent, i.e.,
! must satisfiable respect NC E !.
OCP 4. (Minimality): KB 0 KB coherent consistent, holds
KB ! 6 KB 0 .
coherent consistent ontology obtained original ontology strictly contains consolidated ontology.
postulates presented inspired properties proposed Hansson (1994) Konieczny Pino-Perez (2002). Nevertheless, adapted
suit particularities ontological setting Datalog ; particular, take
account distinction incoherence inconsistency. instance, Inclusion
direct adaptation Hanssons homonymous postulate (Hansson, 1994), states
contraction knowledge base (not necessarily proper) subset
original one. Consistency Coherence, hand, result adapting
setting Konieczny Pino-Perezs postulate IC1 (2002), intuitively ask
resulting merging must consistent; ask resulting consolidation
consistent also coherent. Minimality postulate added ensure
quality consolidation (w.r.t. loss information aspect), adapted
particular work, rather general notion Belief Revision, noted
625

fiDeagustini, Martinez, Falappa & Simari

Hansson (2001) given many names conservatism (Harman, 2008), conservativity (Gardenfors, 1988), minimum mutilation (Quine, 1986) minimal change (Rott,
1992).
proposed postulates capture notion changes made respect
original ontology necessary, resulting ontology is, expected,
coherent consistent. is, given original ontology consolidation process
removes constraints (TGDs) atoms somehow involved making
original ontology incoherent/inconsistent, makes way unnecessary
removal made.

5. Datalog Ontology Consolidation Operator
previous sections presented examples incoherences inconsistencies
arise Datalog ontologies. Additionally, stated properties consolidation
operator satisfy order make adequate changes original ontology regaining
coherence consistency. Now, propose construction consolidation operator
addresses incoherence inconsistency problems Datalog ontologies.
5.1 Possible Construction Consolidation Operator
literature Belief Revision several constructions revision contraction operators studied. Hansson (1994) presents contraction operation belief
bases modeled means application incision functions. functions
contract belief base formula taking minimal sets entail (called -kernels)
producing incisions sets longer entail . resulting belief base
conformed union formulas removed function.
approach known kernel contraction; task restoring consistency also known
belief revision literature contraction falsum (Hansson, 1991). work,
define consolidation process application incision functions. Nevertheless,
instead directly considering minimal inconsistent subsets formulas different
components ontology (which equivalent -kernels), work perform incisions structures called clusters (Martinez, Pugliese, Simari, Subrahmanian, & Prade,
2007; Lukasiewicz et al., 2012) groups together related kernels. specifically,
solve incoherence begin establishing dependency kernels; analogous way,
define data kernels solve inconsistencies w.r.t. , then, based them,
obtain dependency clusters data clusters exploiting overlapping relation.
5.1.1 Identifying Relation among Conflicts
first step towards conflict resolution framework calculate minimal coherence consistency conflicts, identify possible relations among conflicts, any.
Dependency kernels sets TGDs unsatisfiable w.r.t. set NCs EGDs
Datalog ontology minimal set inclusion. sets known Minimal unsatisfiability-preserving sub-TBoxes (MUPS) Minimal incoherence-preserving
sub-TBoxes (MIPS) (Schlobach & Cornet, 2003) DL community.

626

fiDatalog Ontology Consolidation

Definition
6 (Dependency Kernels) set dependency kernels KB , denoted
Q
KB , set X X unsatisfiable set dependencies
w.r.t. NC E every proper subset X 0 X (X 0 ( X) satisfiable w.r.t. NC E .
Example 7 (Dependency Kernels) Consider following sets constraints

Datalog ontology KB :

NC : {1 : counselor (X ) regent(X ) ,




2 : cannot rule(X ) heir (X ) }








E : {1 : advise(X , K ) advise(X , K 0 ) K = K 0 }







: {1 : advise(X , K ) counselor (X ),
KB =
2 : propose law (X , K ) regent(X ),




3 : prince(P ) heir (P ),




4 : son(P , K ) king(K ) prince(P ),




5 : counselor (C ) regent(C ),




6 : bastard son(X , ) son(X , ),



7 : bastard son(X , K ) king(K ) cannot rule(X )}









































KB exist two dependency kernels, i.e.,
Q
KB

= {{3 , 4 , 6 , 7 }, {5 }}.

easy show dependency kernels Datalog ontology independent
particular component ontology, thus obtained looking
component . is, even replace component ontology
empty set atoms, dependency kernels ontology empty database
original one.


Lemma 1 Let KB
Q 1 = (D
Q1 , 1 ) KB 2 = (, 2 ) two Datalog ontologies
1 = 2 . Then, KB 1 = KB 2 .

addition removal TGDs make set unsatisfiable (thus making
ontology incoherent), solve inconsistencies may need remove atoms components order address data inconsistency well. Analogously definition
dependency kernels, define data kernels minimal subset atoms
makes KB = (D, ) inconsistent.

`
Definition 7 (Data Kernels) set data kernels KB , denoted KB , set
X mods(X, ) = every X 0 ( X holds mods(X 0 , ) 6= .
627

fiDeagustini, Martinez, Falappa & Simari

Example 8 (Data Kernels) Consider following coherent inconsistent KB , pro-

posed Lukasiewicz et al. (2012).

KB =


: {directs(john, d1 ), directs(tom, d1 ), directs(tom, d2 ),




supervises(tom, john), works in(john, d1 ), works in(tom, d1 )}








NC : {supervises(X , ) manager (Y ) ,




supervises(X , ) works in(X , D) directs(Y , D) }



E :








:








{directs(X , D) directs(X , 0 ) = 0 }
{1 : works in(X , D) employee(X ),
2 : directs(X , D) employee(X ),
3 : directs(X , D) works in(X , D) manager (X )}





































KB , set data kernels
`
KB



{supervises(tom, john), directs(john, d1 ), works in(john, d1 )},
{supervises(tom, john), directs(john, d1 ), works in(tom, d1 )},
=


{directs(tom, d1 ), directs(tom, d2 )}

know minimal conflicts ontology identify relations among them,
relation exists. this, group related kernels together new structure called
cluster, makes possible achieve optimal solution related kernels. Clusters
obtained overlapping relation defined follows.
Definition 8 (Overlapping, Equivalence) Let L first order language, R L
relational schema, LR sublanguage
generated R. Given LR B LR ,
say overlap, denoted B, iff B 6= . Furthermore, given multi-set first
equivalence relation obtained
order formulas 2LR denote
reflexive transitive closure .
exploiting overlapping among dependency kernels data kernels define
dependency clusters data clusters, respectively.
Q
Definition 9 (Dependency Clusters) Let KB
set Dependency Kernels
Q

KB . Let overlapping relation, K = KB /Q
quotient set equivKB
Q

alence relation obtainedQ
Qover KB . Constraint Cluster set C = [] ,
[] K. denote KB set Constraint Clusters KB .
`
Definition 10 (Data Clusters) `
Let KB set Data Kernels KB . Let

overlapping relation, K = KB /`
quotient set equivalence relation
KB
`

obtained
KB . Data Cluster set C = [] , [] K. denote
`
`
set Data Clusters KB .
KB
Intuitively, dependency cluster groups dependency kernels TGD
common, transitive fashion; data clusters groups data kernels analogous way.
628

fiDatalog Ontology Consolidation

Example 9 (Dependency Clusters Data Clusters) Assume KB

Q

`
= {{1 , 2 }, {1 , 3 }, {4 , 5 }} KB = {{a1 , a2 }, {a1 , a3 }, {a2 , a4 , a5 }}. Then,
two dependency clusters based kernels, grouping first two kernels (due
1 ) remaining kernel another cluster; i.e.,
KB

Q
Q
KB

= {{1 , 2 , 3 }, {4 , 5 }}.

hand, case data clusters
`
`
KB

= {{a1 , a2 , a3 , a4 , a5 }}.

following proposition states that, since clusters based equivalence classes, every
kernel included one one cluster.
Q
Q
Q
PropositionQ
2 KB X some`X KB * X 0
Q
0

X 0 . Analogously,
KB X
`
`
`
`X KB X 6=
0
0
X KB * X X KB X 6= X 0 .
corollary Proposition 2 formula kernel included
one cluster.
Q
Corollary 1 (Corollary

Proposition
2)
Let








KB
`
Q
Q
0
0
forQ
X X KB
/ X 0
`
`all
Qsome KB . Then,
0
0
0








X


X

X KB X 6= X . Analogously,
KB
`
`

/ X 0 X 0 KB X 6= X 0 .
following lemma shall use paper shows example how,
ontological setting Datalog , Leibnizs indiscernibility identicals (von Leibniz,
1976) holds w.r.t. clusters Datalog ontologies, two KBs equivalent
set clusters.
ontologies KB = KB . Then,
Lemma
2 Let KB 1Q
KBQ
2 two Datalog
1
2
`
`
`
`
Q
Q
=

=
.
KB 1
KB 2
KB 1
KB 2

5.1.2 Solving Conflicts: Incision Functions
identified clusters, establish incoherences inconsistencies solved. incision function selects formulas deleted
data dependency clusters.
Definition 11 (General Incision Function) General Incision Function KB
function : (2LR , 2LR ) 2LR following conditions holds:
SQ
Q
S`
`
1. (KB ) ( KB ) ( KB ).
Q
Q
Q
2. X KB KB X holds (Y (KB )) 6= .
`
`
`
3. X KB KB X holds (Y (KB )) 6= .
629

fiDeagustini, Martinez, Falappa & Simari

Q
Q
4. X KB holds = (X (KB )) exists R X
R satisfies conditions 1 2, R ( .
`
`
5. X KB holds = (X (KB )) exists R X
R satisfies conditions 1 3, R ( .
Definition 11 states general incision function selects dependency (data,
respectively) cluster TGDs (atoms, respectively) deletion order restore coherence
(consistency). incision function complies Definition 11 used base
consolidation operator. However, note operator may differentiate
restoring coherence consistency. problem classic literature
Belief Revision since notion incoherence, distinction
rules facts languages like propositional logic; thus, consistency conflicts
appear, avoiding need treat incoherences. Nevertheless, ontological setting
Datalog opportunity exploiting fact two different although
related kinds conflicts address separately goal finding solution
better suits needs applications rely kind knowledge bases.
point paper trying make that, knowledge bases form
Datalog ontologies important differentiate, adequately handle, incoherence
inconsistency quality consolidated ontology heavily depends
assuming strive minimal loss process. This, complex setting, needs
careful definition constitutes kernel. see could happen done
properly, consider following example.
Example 10 (Influence Incoherence Consolidation) Consider KB Exam-

ple 6. = 2T 2E 2NC 2T unsatisfiable. explained Example 6, singleton set {rock singer (axl)} NC 1 : sore throat(X)
sing(X) violated, making {rock singer (axl)} inconsistent . Then,
{rock singer (axl)} data kernel (and data cluster, since cannot overlap
kernel) verifiable every singleton set relevant dependency cluster. Thus,

`
`
KB


{rock singer (axl)},



{rock singer (ronnie)},
=
{rock singer (roy)},



{has fans(ronnie)}









Consider cluster {rock singer (axl)}; cluster
({rock singer (axl)}) = rock singer (axl).
`
`
`
`
S`
`
situation holds every cluster KB , thus ( KB ) = ( KB ).
problem example data kernels (and hence data clusters)
computed w.r.t. original component, which, case, contain unsatisfiable sets
constraints. seen Example 10, becomes utter importance
atoms relevant unsatisfiable sets: case, general incision function (and
inconsistency management technique based deletion treat incoherence
conflicts) necessarily delete atoms.
630

fiDatalog Ontology Consolidation

Proposition 3 Let general incision function. relevant X
(KB ).

Q
KB

Clearly, corollary Proposition 3 every atom relevant
unsatisfiable set remove every atom restore consistency.
Corollary 2 (Corollary Proposition 3) LetQ general incision function.
holds relevant X KB (KB ).
seen, incoherence great influence consolidation treated properly
(that is, previously consistency restoration). would seem better compute
data clusters based retained satisfiable part components. Lemma 1
show dependency kernels obtained independently component
original ontology, unsatisfiable sets violate negative
constraint equality-generating
dependency relevant set atoms. Therefore,
Q
Q
first obtain
,

use

incision function
KB
`
` dependency clusters select
TGDs deleted.Q
QThen, calculate KB based result application
incision function KB , way paying attention constraints
prevail consolidation process.
Next, define constraint incision functions data incision functions
used select candidates deletion (from original ontology) restore coherence
consistency, respectively. First, define incision function dependency clusters
helps solve incoherence constraints.
Definition 12 (Constraints Incision Function) Constraint Incision Function KB
function : (2LR , 2LR ) 2LR following conditions hold:
SQ
Q
1. (KB ) ( KB ).
Q
Q
Q
2. X KB KB X holds (Y (KB )) 6= .
Q
Q
3. X KB holds = (X (KB )) exists R X
R satisfies conditions 1 2, R ( .
Intuitively, constraint incision function takes dependency clusters removes TGDs
way resulting KB coherent. Analogously
constraints
incision functions, define data incision functions solve inconsistencies
`
`
.
KB
Definition 13 (Data Incision Function) Data Incision Function function
% : (2LR , 2LR ) 2LR following conditions hold:
S`
`
%(KB ) ( KB ).
`
`
`
X KB KB X holds (Y %(KB )) 6= .
`
`
X KB holds = (X %(KB )) exists R X
R satisfies conditions 13 13, R ( .
631

fiDeagustini, Martinez, Falappa & Simari

Finally, necessary make significant remark regarding usage incision
functions. that, let us first consider following excerpt quoted Hanssons (2001,
cf. p. 122) regarding possible parameters passed selection functions (which
case incision functions) choice affects possible outcomes.
[. . . ] proof uniformity makes essential use fact selection functions defined remainder sets form A, pairs
form hA, i. instead defined selection functions follows:
(A, ) non-empty subset (A, ) non-empty.
(A, ) = {A} empty.

(A, ) would operation similar partial meet contraction respects, would possible (A, ) 6= (A, )
hold = A, standard definition allow [. . . ]
Thus, extending Hanssons observation incision functions use consolidation,
take sets conflicts arguments incisions formulas
removed two different ontologies set conflicts operator
using incision function identical. reason operator could
tell difference ontologies since parameter conflicts,
exactly same. However, chosen restrict family operators
behaviors; instead, model operators whose behavior could select removal
formula equal conflicts, restricted choice. achieve
this, chosen take ontologies parameters; so, fits application domain
operators exploited, formulas conflict could affect
outcome consolidation.
approach presented here, incision function consider TGDs
effect cluster, global effect whole knowledge base. reason
requirement unlike classic models belief revision, language used
greater expressivity fact TGD generates multiple inferences. instance,
framework TGD form XY(X, Y) Z(X, Z) possible
infer multiple instances (X, Z).
see reason behind choice clearly consider following example.
Example 11 Consider following ontologies.


: {p(a), q(a)}





NC : {p(X ) r (X ) }
KB 1 =





: {1 : q(X ) r (X )}








KB 2 =








: {p(a), q(a)}








NC : {p(X ) r (X ) }












{1 : q(X ) r (X ),
2 : p(X ) s(X ),
3 : p(X ) t(X )}













:








KB , set data clusters equal,
632

fiDatalog Ontology Consolidation

`
`
KB 1

=

`
`
KB 2

=



{p(a), q(a)}



use standard approach take clusters arguments incisions, must
remove formula ontologies, explained incision
function therefore cannot choose differently argument.
Nevertheless, suppose particular scenario want remove atoms based
information help infer. case, KB 1 remove
p(a), KB 2 take q(a), since KB 2 formula p(a) triggers
TGDs, thus inferring atoms. achieve type behavior, necessary pass
ontologies parameters, since provides adequate context.
5.1.3 Cluster Contraction-Based Consolidation Operator
Lastly, define consolidation operator Datalog ontologies represents two
different parts consolidation. First, coherence restoration component
obtained based dependency clusters component original ontology.
Second, restoration consistency component obtained based data
clusters w.r.t. ! component obtained applying constraint incision function
original . way achieve behavior stated earlier paper; sense,
give incoherence resolution higher priority, since retain atoms addressing
unsatisfiable sets TGDs instead, choose follow path. cluster contractionbased consolidation operator formally defined follows:
Definition 14 (Cluster Contraction-based Consolidation Operator)
Let KB Datalog ontology, Constraint Incision Function % Data Incision
Function. Also, let KB ? = (D, \ (KB )) Datalog ontology resulting deleting
KB TGDs selected . Cluster contraction-based consolidation operator
KB !, defined follows:
KB ! = (D \ %(KB ? ), \ (KB ))
result KBQ
! Datalog ontology obtained removing,
first, TGDs
`
`
Q
(selected
) atoms (selected %
) original
KB ?
KB
ontology KB . important note that, one hand TGDs removed
, dependency clusters contain EGDs NCs. hand, Data
Incision Function uses KB ? instead KB atoms conflicts
\ (KB ) removed; data clusters calculated based constrains
obtained consolidation .
5.2 Relation Postulates Construction: Representation Theorem
Section 4 introduced properties Datalog consolidation operator
must satisfy. means following representation theorem establish
relationship set postulates Datalog ontology consolidation operator
cluster contraction-based consolidation operator proposed previous
section. follows denote ! consolidation operator defined Definition 14
% correspond arbitrary constraint data incision functions, respectively.
633

fiDeagustini, Martinez, Falappa & Simari

Theorem 1 (Representation Theorem) operator consolidation ! Cluster
Contraction-based Datalog Ontology Consolidation Operator Datalog ontology KB
iff satisfies Inclusion, Coherence, Consistency, Minimality.

6. Complete Example Datalog Ontologies Consolidation
introduced operator allows us consolidate Datalog ontologies
satisfies set expected properties expressed postulates Section 4.
section, complete process consolidation Datalog ontologies depicted
following example.
Example 12 (Consolidation Datalog Ontologies) Suppose (in-

coherent inconsistent) ontology KB shown Figure 1, expresses information
collected certain company.

D:




























NC :













E :
KB =






:




































{a1
a3
a5
a7
a8
a9

: boss(walter ), a2 : supervises(walter , jesse),
: makes decisions(walter ),a4 : makes decisions(jesse),
: supervises(skyler , walter ), a6 : employee(walter ),
: charge (jesse, distribution),
: charge (walter , cooking),
: strike(mike)}

{1 : follows orders(X ) makes decisions(X ) ,
2 : supervises(Y , X ) supervisor (X ) ,
3 : absent(X ) strike(X ) }
{1 : charge (X , ) charge (X , 0 ) = 0 }
{1 : employee(X ) supervised (X ),
2 : supervised (X ) follows orders(X ),
3 : boss(X ) makes profit(X ),
4 : supervises(Y , X ) supervisor (Y ),
5 : supervises(Y , X ) employee(X ),
6 : supervised (X ) makes decisions(X ),
7 : supervised (X ) work (X ),
8 : work (X ) get paid (X ),
9 : work (X ) charge (X , ),
10 : strike(X ) absent(X )}





















































































Figure 1: original ontology consolidated.
Now, begin first part consolidation process (i.e., solving incoherences making set satisfiable) obtain, first step towards obtaining
dependency clusters, dependency kernels KB :
Q
KB

= {{2 , 6 }, {10 }},

based kernels, calculate set dependency clusters KB
Q
Q
= {{2 , 6 }, {10 }}.
KB
634

fiDatalog Ontology Consolidation

Q
Q
Q
Note that, overlap among dependency kernels, KB = KB . Next,
use cluster incision function solve incoherency problems. sake example
assume guide contraction process means quantitative criterion, i.e.,
choosing among possible incisions ones removes fewer formulas, using
plausibility among formulas cardinality incisions same.
following show possible incisions, i.e., satisfying conditions Definition 12.
sets
cluster {2 , 6 } could either remove 2 6 . Since two incisions remove
number atoms assume example 2 plausible 6 ,
thus prefer retain former.
cluster {10 } remove 10 .
Then, particular incision example follows:
({2 , 6 }) = {6 }
({10 }) = {10 }
Now, move next part consolidation process: consistency recovery.
explained before, part operator considers TGDs effectively
included consolidation. particular example ! = \ {6 , 10 }.
let KB ? = (D, !); based KB ? calculate data kernels.
Q
= {{a2 , a4 }, {a3 , a5 }, {a3 , a6 }, {a2 , a5 }}
KB ?
Then, obtain data clusters, are:
Q
Q
= {{a2 , a3 , a4 , a5 , a6 }}
KB ?
Now, solve inconsistencies need consider sets intersection
kernels included clusters empty, using ! instead
this. again, analyze possible incisions (the sets respecting conditions
Definition 13) light number atoms deleted plausibility formulas
them. different possible incisions cluster are:
- remove {a2 , a3 }.
- remove {a2 , a3 , a6 }.
- remove {a2 , a5 , a6 }.
- remove {a4 , a5 , a6 }.
again, sets presented removal induce operator
satisfies postulates, thus captured framework. Nonetheless, explained
example choose remove atoms possible. is,
choose remove {a2 , a3 }),
%({{a2 , a3 , a4 , a5 , a6 }}) = {a2 , a3 })
Then, using Datalog ontology consolidation operator based contraction
clusters like one introduced Definition 14 obtain coherent consistent
ontology shown Figure 2.
635

fiDeagustini, Martinez, Falappa & Simari

KB ! =


D! : {boss(walter ), makes decisions(jesse),




supervises(skyler , walter ), employee(walter ),




charge (jesse, distribution),




charge (walter , cooking),




strike(mike)}








NC ! : {follows orders(X ) makes decisions(X ) ,




supervises(Y , X ) supervisor (X ) ,




absent(X ) strike(X ) }



E ! :








! :

































































{in charge (X , ) charge (X , 0 ) = 0 }







{employee(X ) supervised (X ),




supervised (X ) follows orders(X ),




boss(X ) makes profit(X ),




supervises(Y , X ) supervisor (Y ),




supervises(Y , X ) employee(X ),




supervised (X ) work (X ),




work (X ) get paid (X ),



work (X ) charge (X , )}

Figure 2: ontology resulting consolidation.

7. Related Work
closely related work work Croitoru Rodriguez (2015).
work authors present consolidation operators used basis definition
semantics inconsistency tolerant ontology query answering Datalog+ (a
expressive language Datalog , Cal et al., 2012). case work,
work Croitoru Rodriguez (2015) based use Hanssons incision functions
(Hansson, 1994) solve conflicts. Nevertheless, remarkable differences
works well. Among important ones operators presented
Croitoru Rodriguez deal inconsistent ontologies, acknowledgment
incoherence problem made. shown work,
significant impact quality consolidation analysed respect minimal
loss information. Moreover, fact makes that, even though set postulates
works similar spirit, family operators characterized Croitoru
Rodriguez subset ones characterized here. due fact setting
consider (i.e., inconsistent incoherent ontologies) general,
since instance operators remove facts also TGDs, Croitoru
Rodriguezs operators since focus inconsistency.
Another closely related work one Lukasiewicz et al. (2012). There, authors define general framework inconsistency-tolerant query answering Datalog
ontologies based notion incision functions. Nevertheless, work focused
enforcing consistency query time obtaining (lazy) consistent answers inconsistent ontology instead consolidating one. Clearly, process must carried
every query posed system, approach obtain new knowledge base
636

fiDatalog Ontology Consolidation

offline manner, knowledge base queried without considering inconsistency issues; approaches prove useful, depending application domain.
Additionally, one KB used rational assumption conflicts
constraints also made, therefore notions unsatisfiability
incoherence. stated before, order gain generality chosen drop
assumption, treat incoherence problems well inconsistency ones. addition
works Croitoru Rodriguez Lukasiewicz et al., several works
solve inconsistency incoherence means adapting approaches based Belief
Revision techniques knowledge representation formalism.
7.1 Propositional Knowledge Bases
numerous works revision merging propositional knowledge bases (see,
instance, Konieczny & Perez, 2002; Katsuno & Mendelzon, 1992; Lin & Mendelzon, 1999;
Liberatore & Schaerf, 1998; Everaere, Konieczny, & Marquis, 2008; Konieczny & Perez,
2011; Delgrande, Dubois, & Lang, 2006; Booth, Meyer, Varzinczak, & Wassermann, 2010;
Delgrande, 2011; Delgrande & Jin, 2012; Falappa, Kern-Isberner, Reis, & Simari, 2012),
provided foundations work (fragments of) first order logics.
expected, works deep connections ours, also remarkable
differences, shall see.
mentioned throughout paper, work Sven Ove Hansson (1994)
provides inspiration foundations work: follow approach akin Kernel
Contraction several intuitions it, adapted ontological language, Datalog .
consequence, besides treating incoherence also provide complete inconsistency
resolution process takes advantage ontological setting, exploiting relation
components ontology define coherence consistency
restored. Also, classic incision functions introduced Hansson produce incision
minimal conflicts. approach, however, work clusters, groupings
kernels, thus always minimal. Then, propose particularization
Hanssons incision functions, focusing incision functions successfully work
clusters.
Konieczny Pino-Perez (2002) made one main contributions merging
conflicting information. work follow intuitions proposed them.
Nevertheless, main difference approach (besides obvious one
aims works, merging vs. consolidation) state final merging
consistent presence consistent (or, terminology, coherent) set
integrity constraints, analyze alternative case.
respect work Lin Mendelzon (1999), besides difference
focus (once merging vs. consolidation), main difference inconsistency
management strategy chosen work conflict solving strategy relies
votes majority establish formulas retained merging. Instead,
chosen introduce particular strategy. Nevertheless, possible adapt
framework use preference relations choose possible incisions (in
similar way shown Example 12). relations indeed designed
comply majority intuition (providing votes,
637

fiDeagustini, Martinez, Falappa & Simari

apply ontology consolidation environment since one ontology), thus
obtaining similar strategy.
work Katsuno Mendelzon (1992) problem knowledge base revision
propositional case addressed. approach, language used
express facts world constraints imposed KB .
Nevertheless, difference (in case) update KB
consolidation KB arises treatment integrity constraints: work
integrity constraints considered invariant updates restore consistency
restricted facts.
works Delgrande (2011), Delgrande Jin (2012) authors present
approach revising propositional knowledge base set sentences, every
sentence set independently accepted inconsistencies
considering whole set. main idea follows AGM theory, differs
that, necessary alter Success postulate suits intuition every
sentence set final revision (since set inconsistent).
Guided principle informational economy, characterize revision
plausible worlds among various maximally consistent subsets set sentences.
parallel Datalog ontology environment, revising component
ontology solve inconsistencies. set sentences inconsistent, union
original KB inconsistent. Nonetheless, important difference
works ours. works authors first solve inconsistencies
set sentences, decide subset characterize revision.
approach different, directly consider inconsistent KB . Then, order solve
problem setting, necessary consider union KB
entire set sentences, apply consolidation operator.
7.2 Knowledge Expressed Description Logics Ontologies, Logic Programs
Relational Databases
focus knowledge representation formalisms closely related
Datalog , mainly family Description Logics (Baader, Calvanese, McGuinness,
Nardi, & Patel-Schneider, 2003) Logic Programming (Lloyd, 1987; Nilsson & Maluszynski, 1995; Gelfond, 2008). remarkable work using belief revision solve conflicts DLs
one Qi, Liu, Bell (2006), based AGM theory (Alchourron
et al., 1985; Gardenfors, 1988). makes work stand
introduce generalizations AGM postulates case DLs, also define two
operators knowledge bases, based formulas weakening, satisfy postulates.
main difference approach take account consistency problems ontologies, incoherence treatment provided. pointed earlier,
incoherence lead extreme weakening information, may
take every individual name general concept inclusion.
previously mentioned, notion incoherence inspired Schlobach
Cornets work (2003), among others. paper authors focus definition
processes capable detecting unsatisfiabilities incoherences DLs ontologies, introducing complete algorithms along empirical analysis approach. Nevertheless,
638

fiDatalog Ontology Consolidation

main focus work, authors set aside issue recover
coherence conflict detected, also consider inconsistencies.
work presented consolidation process treats incoherence inconsistency,
based use Belief Revision techniques. Thus, approach presented Schlobach
Cornet could potentially useful regarding implementation operators presented work, providing effective way obtaining set kernels
set clusters based.
Black et al. (2009) propose approach capable using information coming
several DL ontologies order answer queries, taking care process incoherence inconsistency. approach based agents argumentative capabilities,
one personal knowledge base form DL ontology. agents use
dialogue games interchange arguments reach agreement answer
certain query. Thus, agents use (possible incoherent/inconsistent) union
ontologies without merging them, still obtain answer influenced every
ontology play. Moreover, approach advantage information lost,
formula deleted ontologies, result inferences obtained
approach superset obtained ontology resulting
consolidation union DL ontologies. Even though authors argue one
advantage proposed approach need waste time effort
performing consolidation KB , one disadvantage computational complexity
associated argumentative reasoning (Parsons, Wooldridge, & Amgoud, 2003; Dunne &
Wooldridge, 2009; Cecchi, Fillottrani, & Simari, 2006) process conducted
query issued online manner. Even though consolidation process
also computationally expensive, necessary perform
done offline query answering system becomes available. choice one approach depends highly environment used, i.e.,
size ontologies used, often updates issued KB
critical time consumption system, among considerations; course
set inferences obtained every approach may differ
also taken account. consolidation-based approach could suitable
time-dependant systems like real-time systems query intensive systems data
tractability associated (a consolidated) Datalog ontology may proven handy.
Another work worth mentioning Kalyanpur, Parsia, Horridge, Sirins
(2007). work verses find justifications entailments Description
Logics ontology. justification simply precise set axioms ontology responsible
particular entailment (Kalyanpur et al., 2007). words, minimal set
axioms sufficient produce entailment, related use kernels mean
obtain clusters part consolidation strategy used. Moreover, Horridge, Parsia,
Sattler (2009) state justifications important repairing inconsistent ontologies.
Thus, could important definition consolidation processes similar
cluster-based consolidation, least one axioms justifications
entailment removed ontology, corresponding entailment longer
holds.(Kalyanpur et al., 2007, p. 269). One main contributions work
definition practical black-box (i.e.,, reasoner independent) techniques allows us
find justifications entailments ontology efficient way. such, evident
639

fiDeagustini, Martinez, Falappa & Simari

work verses different direction still benefit findings.
particular, may possible use developed algorithms part implementation
strategy consolidation operators, adapting used Datalog
dual incoherence/inconsistency setting.
Regarding Logic Programming, also several works address problem
merging knowledge bases expressed logic programs, solving inconsistency issues
process. instance, Hue, Papini, Wurbel (2009) introduce merging process
based stable model semantics, using logic Here-and-There (Turner, 2003). Hue
et al. consider merging strategy based pre-orders among deletion candidates called
potential removed sets establish particular way obtain preorders. Instead, assume strategy P given pre-order defines
P . case Lin Mendelzons work (1999), although falls scope
present work certainly adapt framework use similar techniques
choosing incision prevails.
Another notorious work Logic Programming field one Delgrande, Schaub,
Tompits, Woltran (2009). work two different approaches proposed. first
one follows arbitration approach, selecting models program differs least
w.r.t. models programs. work case unsatisfiable programs
studied, similar way consider incoherence leaded unsatisfiable sets TGDs.
Nevertheless, consider unsatisfiability certain program, concept
union programs. Furthermore, strategy solve unsatisfiability simply
leaving unsatisfiable program consideration merging, instead trying
solve conflict somehow. second approach based selection models
special program P0 , thought constraints guiding merging process,
least variations w.r.t. programs merging. approach
seen particular instance approach proposed Konieczny Perez (2002).
area databases, one influential works one Arenas et al.
(1999) Consistent Query Answering, authors propose model theoretic definition consistent answers query relational database potentially inconsistent
set integrity constraints. Intuitively, consistent answers query set atoms
(classical) answers query every repair inconsistent database;
repair set atoms satisfy set constraints close possible
original database. Different notions repairs studied literature, well
different notions means set atoms close possible original
database. proposals based repairing inserting and/or deleting tuples
to/from database (actually, possible actions depend form integrity
constraints expressiveness) notion closeness defined via set inclusion
cardinality. work Arieli, Denecker, Bruynooghe (2007), however, proposes
uniform framework representing implementing different approaches database
repairing based minimizing domain dependent distances. main idea work
show thinking terms (different) distances express preferences among repairs
leads different preferences applied different scenarios. authors show
set repairs obtained using proposed distance functions deviate
obtained using set-inclusion. Furthermore, besides insertion deletion entire tuples several domain independent approaches, e.g., based cardinality
640

fiDatalog Ontology Consolidation

complex objective functions. approach proposed Wijsen (2005) updates
considered primitive theoretical framework; Bohannon et al. (2005) present
cost-based framework allows finding good repairs databases exhibit inconsistencies form violations either functional inclusion dependencies, allowing
also updates attribute values. work, two heuristics defining constructing repairs based equivalence classes attribute values; algorithms presented
based greedy selection least repair cost, number performance optimizations also explored. quite different semantics repairing proposed Caroprese,
Greco, Zumpano (2009), Caroprese Truszczynski (2011) Active Integrity
Constraints (AICs short); AIC production rule body conjunction
literals, false database consistent, whereas head
disjunction update atoms performed body true (that constraint violated). Repairs defined minimal sets (under set inclusion) update
actions (tuple deletions/insertions) AICs specify set update actions used
restore data consistency. Hence, among set possible repairs, subset
founded repairs consisting update actions supported AICs considered.
works area propose different semantics repairing either explicitly implicitly
considering preference relation among set repairs (cf. Andritsos, Fuxman, & Miller,
2006; Staworko, Chomicki, & Marcinkowski, 2012; Greco & Molinaro, 2012).
recently, area ontology-based data access (OBDA), Lembo et al. (2010)
study adaptation CQA DL-Lite ontologies, called AR (ABox semantics).
work, also intersection (IAR) semantics presented sound approximation consistent answers; semantics consists computing intersection repairs answers
obtained there, though (possibly many) AR answers cannot obtained IAR semantics, latter computationally easy obtain DL-Lite family,
i.e., necessary compute whole set repairs order compute intersection. data combined complexity semantics studied (Rosati,
2011) wider spectrum DLs. Also, Rosati (2011) presents intractability results
query answering EL intersection semantics, non-recursive segment
language proved computable polynomial time. recently, Bienvenu
Rosati (2013) propose another family approximations CQA, also DL-Lite
family. k-support semantics allows (soundly) approximate set queries entailed
CQA semantics, based k subsets database consistently entail q;
hand, k-defeater semantics approximates complete approximations seeking
sets contradict supporters q. semantics FO-rewritable ontological language standard CQ answering FO-rewritable well, used
conjunction over- under-approximate consistent answers.
Much like Black et al. (2009), treatment inconsistencies proposed
semantics related particular queries instead inconsistency whole database.
Thus, attempt obtain final consistent database queried without
considering restrictions. Furthermore, address issues incoherence
inconsistency together; instead approaches either assume set integrity
constraint correctly defines semantics database instance, room
incoherence, treat constraints data alike moment removing ignoring
information, leads type problems discuss Example 10.
641

fiDeagustini, Martinez, Falappa & Simari

techniques may suitable case one single database, presence
incoherence set ICs, case consider several databases together,
approach would lead meaningless empty answers, since subset database
could satisfy constraints would also case approach Lukasiewicz
et al. (2012).
Also related databases field work Lin Mendelzon (1998). There,
database viewed first-order theory without rules, ICs used ensure
consistency final result work Konieczny Perez (2002), presenting ways
solve known database merging problems like synonyms homonyms. Nonetheless, like
Konieczny Pino-Perezs work, consider problems related set ICs.
Instead, set ICs used merging process unique, choice set
expected performed merge designer. Unlike Lin Mendelzon, made
assumption consolidation environment set ICs conflict-free.
Cholvy (1998) introduces another approach used reason contradictory information. framework represented set axioms inference
rules. Additionally, paper several applications framework introduced,
e.g., solving conflicts among beliefs represented first order databases, facts
ground literals rules integrity constraints deduction rules.
scenario, contradiction obtained application constraints considering several databases together. establishes certain parallel case
inconsistency Datalog ontology. However, main difference work lies
strategy inconsistency management process defined. work,
preference order databases assumed. Instead, chosen restrict
achieve consolidation, thus presenting general approach. Nevertheless, stated
adapt incision functions suit intuition every formula equally
desirable, choosing instance preferences ontologies guideline (if using
approach tasks rather consolidation single ontology), obtaining
inconsistency management strategy akin one introduced Cholvy.
Finally, Meyer, Lee, Booth (2005) use two well-known techniques knowledge
integration propositional case, adapted refined expressiveness DLs.
proposed approach takes knowledge bases produces disjunctive knowledge
base (DKB) result integration. One disadvantage DKBs state
possible options take conflicting knowledge expected exploited
reasoning process rather choosing one them. Thus, contrary approach
final consolidated ontology given, definitive final merging; moreover,
set aside research problems related incoherence integration process.

8. Conclusions Future Work
Collaborative work information exchange becoming key aspects almost system; thus, uttermost importance automatic adequate ways solve
conflicts: knowledge evolves collaborative environment incoherence inconsistency prone arise. knowledge often represented ontologies
collaboratively built, often shared among entities use modify them. One particular way deal conflicts appear application environments
642

fiDatalog Ontology Consolidation

try modify information contained ontology order regain coherence
consistency. paper shown achieve consolidation Datalog
ontologies. introduced concept incoherence Datalog ontologies terms
unsatisfiability sets TGDs, showed relationship classical notion
inconsistency logical theory lacks models.
also proposed construction consolidation operators. construction inspired kernel contraction, uses incision functions groupings minimal unsatisfiable/inconsistent sets called clusters solve conflicts. Finally, stated properties
Datalog ontology consolidation operator expected satisfy. showed
operators satisfy respective properties, obtaining result consolidation
new Datalog ontology always coherent consistent minimizing changes
made conflict resolution.
final remark, notice operators take care incoherences ontology. However, rare cases ontology designer introduce unsatisfiable
concepts ontology purpose, model particular feature application domain. case remove incoherence, rather
delete atoms triggering it, any. Clearly, since defined setting
mind behavior cannot achieved operators presented here. Nevertheless,
modify present approach suit setting almost straightforward, provide
identify whether unsatisfiable set TGDs made purpose not.
future work, intend study new constructions Datalog consolidation
operators. this, first plan change general approach, i.e., operators based
formalisms kernel contraction, mainly AGM theory (Alchourron
& Makinson, 1985; Alchourron et al., 1985); then, proposed framework
cluster contraction based consolidation operators fully constructive, depending
application domain may certainly difficult asses effect incisions, i.e.,
may hard decide among family possible incisions one select.
design point view, may easier select perform consolidation
additional information formulas knowledge base,
preference relation can, example, elicited domain experts. general,
could easier expert provide guidelines information application
domain hand could modeled preference relation formulas
ontology rather trying single desired incisions. direction want
explore constructions based exploiting preference relations among formulas
ontologies define different strategies choose formulas delete, possibly tailored
particular scenarios. Mainly, plan analyze two different aspects: relation
operators based preference relations respect ones presented
work, different strategies affect behavior.
Also, work make point differentiating concept inconsistency
incoherence; therefore, need focus languages separate extensional
intensional knowledge, otherwise two notions indistinguishable (as
case propositional logic). sense, choice Datalog due desirable
property generalizing several popular languages classical Datalog, DL-Lite, ELH,
F-Logic-Lite, etc. Even though paper perform particular analysis
effects nulls proposed solutions consolidation, Datalog family languages
643

fiDeagustini, Martinez, Falappa & Simari

chosen offers wide variety languages high computational tractability
(some FO rewritable others PTIME inference algorithms). results
work pave way continue research line next natural step, show
(or whether) different syntactic semantic properties yield tractability
query answering allow us obtain tractability results also consolidation problem,
much way happened already area consistent query answering
(where repairs extensional part KB considered). is, example,
rewriting algorithms capability value invention plays important role:
value invention process controlled (in general syntactic restrictions) order
keep low complexity reasoning tasks. mind, future
look role processes like value invention consolidation Datalog
ontologies, impact conflicts solved computational
efficiency.
currently working implementation operators; plan study different techniques used order produce efficient implementation, possibly
tailored specific fragments Datalog . explained before, algorithms introduced
Schlobach Cornet (2003) proven useful regarding aspect since may
provide way calculate kernels Datalog ontology, thus providing first step
towards incoherence resolution. Another important work regarding implementation
consolidation operators one Wassermann (2000), author shows
minimal incision functions knowledge base obtained kernels KB
using algorithm finding minimal hitting sets (Reiter, 1987). Several works
area ontology debugging repairs, (e.g., Halaschek-Wiener & Katz, 2006, Horridge
et al., 2009 way find justifications inconsistency) exploited Reiters
algorithms order implement frameworks. Among others, plan study
adequate techniques operators, almost direct relation
minimal incision functions Reiters minimal hitting sets; way, may possible adapt Reiter techniques attend incoherences inconsistencies, moreover,
already discussed, plan analyze relation cluster incision functions
preference relations. Regarding implementation, hold conjecture relations
exploited refine implementation operators: Reiters algorithm
based expansion directed acyclic graph, expansion made
breadth first fashion, end generates possible values minimal incision
functions. acknowledged Wassermann, kind ordering among formulas
present, ordering used choose branch expand; words,
may possible implement construction operators proposed work
means exploiting Reiters hitting sets algorithm, also use preference
relation equivalent incision (if any) guide consolidation process. is,
may possible adapt algorithm chooses expand branch less
preferred set formulas, thus guiding graph expansion process.

Appendix A. Proofs
Proof Proposition 1
644

fiDatalog Ontology Consolidation

Proof Consider U U unsatisfiable set dependencies w.r.t.
NC E , set atoms relevant U .
follows definition satisfiability set dependencies w.r.t. set
constraints U unsatisfiable exist relevant set atoms A0
makes mods(A0 , U E NC ) 6= , otherwise U satisfiable.
Then, mods(A, U E NC ) = . Moreover, since U
chase(A, U ) chase(D, ), thus NC EGD violated chase(A, U )
also violated chase(D, ). Thus, mods(D, E NC ) = , i.e., KB inconsistent.
Proof Lemma 1
Proof Let KB 1 = (D1 , 1 ) 1 = 1T 1E 1NC , KB 2 = Q
(, 2 )

2 = 2T 2E 2NC twoQDatalog ontologies 1 = 2 , KB 1
dependency kernels KB 1 , KB 2 dependency kernels KB 2 , respectively.
Q
Consider X KB 1 . Then, Definition 6 X 1T unsatisfiable
set dependencies w.r.t. 1E 1NC every X 0 ( X satisfiable w.r.t. 1E 1NC .
Since 1 = 2 , 1T = 2T , 1E = 2E 1NC = 2NC , thus holds
X 2T unsatisfiable set dependencies w.r.t. 2E 2NC every X 0 ( X
satisfiable w.r.t. 2E 2NC .
Q
Then,Q
Definition 6 Q

Q X KB 2 , since holds arbitrary
kernel KB 1 KB 1 = KB 2 .
Proof Proposition 2
Proof focus case dependency clusters, omitting theQproof data
clusters, analogous other. Consider arbitrary KB .
) begin showing kernel

Q
Q part
Q
Qis part cluster
0
0
cluster, i.e., X X KB * X X KB
X 6= X 0 .

obtained directly definition clusters: X Q
= []
[] equivalence class equivalence relation obtained KB . Then,
clearly X []. Therefore, since definition two
equivalence
classes either equal disjoint holds
/ [0 ] [0 ]. Let

0
0
0
0
X = 0 [0 ] . holds X 6= X * X . Since holds
Q
Q
arbitrary equivalence
class [0 ] holds X X KB * X 0
Q
Q
X 0 KB X 6= X 0 .
) show
cluster, i.e.,
Q
Q exist any0 kernel belong
Q
Q
0
* X X KB X 6= X X X KB . Again,Q
arise
Q
use equivalence classes Definitions 9 10. * X 0 X 0 KB
X 6= X 0 , holds
/ [0 ] [0 ] 6= []. So,
Ssince equivalence classes form0
partition must holds []. Therefore, X = [] * X
Q
Q
X 0 KB X 6= X 0 X.
Proof Corollary 1
645

fiDeagustini, Martinez, Falappa & Simari

Q
Proof Consider
KB . Q
Proposition
2 X
Q
Q
Q
0 X 0
X KB onlyQ


*
X


X 6= X 0 . Thus,
KB
Q
Q
Q
X X KB
/ X 0 X 0 KB X 6= X 0 .
`
0
0
Analogously,


show







X
KB
`
`
`
`
0
0
X KB
/ X X KB X 6= X 0 .
Proof Lemma 2
Q
Proof Consider X KB 1 . Then, X minimal unsatisfiable set TGDs w.r.t.
1NC 1E . Since KB 1 = KB 2 , holds X KB 2 , 1E = 2E , 1NC = 2NC
X unsatisfiable set TGDs w.r.t. 2NC 2E . Also, exist X 0 ( X
X 0 unsatisfiable set
contradict
Q TGDs w.r.t. 2NC 2E , since otherwise wouldQ
hypothesis X KB 1 , 1NC
E . Then,
Q = 2NC 1E = 2Q
QX KB 2 ;
since holds arbitrary X KB 1 , KB 1 = KB 2 .
Q
Q
Q
Consider
arbitrary X, KB 1 XY . Since
= KB 2 ,
KB
1
Q
Q
Q
Q
Q


X, KB 2 . Thus, Q
equivalent Q
, KB 1 = KB 2 .
KB 1
KB 2
`
`
`
0 0 . Since
0, 0
=


X
Likewise, consider

arbitrary
X
KB
KB
1 `
1
` KB 2 ,
`
`
`


0
0
`
`
, thus KB 1 = KB 2 .
equivalent
X , KB 2 . Therefore,
KB 2

KB 1

Proof Proposition 3
Q
Proof Consider X KB relevant X. Definition 6
X unsatisfiable w.r.t. N E NC , Definition 4
fact relevant X mods({}, X N ) = (1). Also, since {}
singleton ( {} = , clearly
mods(, X N ) 6= (2). Then,
`
(1), (2)`
`and Definition 7 follows {} KB . Also, Definition 9
{} KB , since {} cannot overlap kernel, singleton.
Consider incision {}. Definition 11 follows (KB ) {} 6= .
Then, (KB ) {} = , thus (KB ).
Proof Corollary 2
Q
Proof Consider arbitrary D. Since relevant X KB ,
Proposition 3 holds (KB ). Thus, since holds arbitrary
(KB ).
Proof Theorem 1
Proof Let KB 1 = (D1 , 1 ) KB 2 = (D2 , 2 ) two Datalog ontologies
KB 1 = KB 2 .
) Construction postulates
Consider operator ! defined Definition 14; prove ! satisfies every
postulate Theorem 1. Let KB 1 ! = (D1 !, 1 !) KB 2 ! = (D2 !, 2 !) two Datalog
ontologies resulting consolidation KB 1 KB 2 means !, respectively.
ontology
Furthermore, let KB ?1 = (D1 , 1 \ (KB 1 )) KB ?2 = (D2 , 2 \ (KB 2 )) Q
`
resulting removing TGDs selected KB 1 KB 2 . Let KB 1 KB ?
1

646

fiDatalog Ontology Consolidation

Q
`
set dependency data kernels KB 1 KB ?1 respectively, KB 2 KB ?
Q
Q
`
` 2
sets dependency data kernels KB 2 KB ?2 . Finally, let KB 1 KB ?
Q
Q
`
` 1
set dependency data clusters KB 1 KB ?1 respectively, KB 2 KB ?
2
sets dependency data clusters KB 2 KB ?2 .
Inclusion: 1 ! 1 D1 ! D1 .
definition KB 1 ! D1 ! = D1 \ %(KB ?1 ), thus D1 ! D1 .
similar way, definition KB 1 ! 1 ! = 1 \ (KB 1 ), thus
1 ! 1 .
Coherence: KB 1 ! coherent.
prove KB 1 ! coherent show 1 ! satisfiable
E NC 1 !. sufficient show minimal conflicts
attended operator, i.e., dependency kernel included 1 !,
Q
exists
Consider
Q
Q arbitrary X KB 1 . Proposition 2 Q
Q

X . definition holds
KBQ
KB 1
1
X KB 1 X holds ((KB 1 ) X) 6= . Then, exists
X ((KB 1 ) X), thus
/ 1 !. Therefore,
X * 1 !,
Q
i.e., conflict solved. Since holds arbitrary X KB 1 every
unsatisfiable set 1 included 1 !, thus 1 ! satisfiable
E NC 1 !, i.e., KB 1 ! coherent.
Consistency: Proof analogous Coherence.
Minimality: KB 0 KB 1 coherent consistent, holds KB 1 ! 6
KB 0 .
0
LetQ
KB 0 coherent consistent, let CF 1 = 1 \
S`
`

QKB KB 1
( KB ) CF D1 = D1 \ ( KB ) set formulas belong
kernel 1 D1 , respectively.
0
Suppose
reductio KB 1 ! SKB
`
` . definition KB 1 ! (KB 1 )

Q
Q
( KB ) , %(KB 1 ) ( KB ). Then, CF 1 KB 1 ! CF D1 KB 1 !.
Therefore, CF 1 KB 0 CF D1 KB 0 .
Q
Q
`
`
Then, since KB 1 ! KB 0 must exist KB KB KB 0

/ KB 1 !, KB 0 coherent consistent same. is, exists
dependency cluster data cluster removal optimal, since could
included consolidation. rest proof simplicity reasons,
consider case belongs dependency cluster. made without
loss generality, since proof case included data cluster
analogous one presented here.
Q
Q
Let us consider
KB 0 . Corollary 1
KB
Q
Q
X X KB . Let = (X (KB )) incision performed
cluster, let R = (X {KB \ KB 0 }) formulas removed XQwhen
obtaining KB 0 . Clearly, since KB 0 coherent X KB

647

fiDeagustini, Martinez, Falappa & Simari

holds R 6= , otherwise
KB 0 , make KB 0 incoherent.
Q
Q
Besides, since R R KB , thus R satisfies first two conditions
Definition 12.
Definition 12 exists set TGDs
satisfies first two conditions definition time holds
(1) R.
Since
/ KB 1 ! X (KB ), thus . However, know
X KB 0 , thus
/ R. Therefore (2) 6 R.
(1) (2) R 6 R, absurd coming
original assumption KB 1 ! KB 0 , holds KB 0 KB 1 coherent
consistent KB 1 ! 6 KB 0 .
) Postulates Construction
second part proof, consider operator ! satisfies postulates
Theorem 1. Let (!) function based ! defined follows:
Q
Q
(!) (KB 1 ) = {x | x X X KB 1 x
/ {1 KB 1 !}}
Let KB ?1 = (D1 , 1 \ (!) (KB 1 )) ontology resulting removing KB 1
TGDs selected (!) . Then, let %(!)D another function based ! defined follows:
`
`
/ {D1 KB 1 !}}
%(!)D (KB ?1 ) = {x | x X X KB ? x
1

Based %(!)D (!) define new operator follows:
KB 1 !0 = (D1 \ %(!)D (KB ?1 ), 1 \ (!) (KB 1 ))
show !0 Datalog ontology consolidation operator based Cluster
Contraction. this, first prove %(!)D well-defined data incision function
(!) well-defined constraint incision function. is, given (!)
prove that:
- (!) well-defined, i.e., KB 1 = KB 2 , (!) (KB 1 ) = (!) (KB 2 ).
Q
Q
definition (!) (!) (KB 1 ) = {x | x X X
KB 1
x
/ 1 KB 1 !}.
Consider
arbitrary
x (!) (KB 1 ). Since KB 1 = KBQ
, Lemma 2
2Q
Q
Q Q
Q
KBQ
=
.
Since
x


(KB
),

x

X

, thus holds
1
(!)
KB 2
KB 1

1
Q
x X KB 2 (1).
Q
Q
Besides, since x X KB 1 x 1 . Thus, since x
/ 1 KB 1 !, x
/ KB 1 !.
Since KB 1 = KB 2 , fact ! function KB 1 ! = KB 2 !,
also holds x
/ KB 2 !. Thus, x
/ 2 KB 2 !(2).
(1) (2)Q
Qit follows x (!) (KB 1 ) holds x {y |


/ 2 KB 2 !}. definition (!) (!) (KB 2 ),
KB 2
thus KB 1 = KB 2 , (!) (KB 1 ) = (!) (KB 2 ).
648

fiDatalog Ontology Consolidation

- (!) (KB 1 )

SQ
Q
( KB 1 ).

follows directly Q
definition (!) , since every x (!) (KB 1 ) holds
Q
x X X KB 1 first condition definition.
- X

Q
Q

eY

Q

6= X, (Y (!) (KB 1 )) 6= .
Q
Q
Q
Suppose reductio
exists X KB 1 KB 1 6= ,

X (Y (!) (KB 1 )) = .
Q
Q
Then, holds
/ (!) (KB
),
i.e.,


/
X

{1 KB 1 !}.
1
KB 1
Q
hypothesis KB 1 X. Thus X, therefore
must hold {1 KB 1 !}, extension KB 1 !.
KB 1

KB 1

Since holds arbitrary 1 !. Definition 6
holds minimal unsatisfiable set TGDs w.r.t. E NC 1 . Then,
relevant set atoms holds mods(A, E NC ) = . Then, since
1 ! relevant set A0 holds mods(A0 , 1 ! E NC ) = ,
TGDs triggered A0 . Then, 1 ! unsatisfiable set TGDs w.r.t.
E NC 1 .
However, Coherence KB 1 ! coherent, thus 1 ! satisfiable
w.r.t. E NC 1 .
1 ! satisfiable w.r.t. E NC 1 1 ! unsatisfiable
w.r.t. E Q
NC 1 , absurd
coming initial supposition
Q
Q
exists
X KB 1 KB



=
6
,


X

(Y
(!) (KB 1 )) = ,
1
Q
Q
Q
andTit holds X KB 1 KB 1 X, 6=
(Y (!) (KB 1 )) 6= .
Q
Q
holds = (X (!) (KB 1 )) exists
- X
KB 1
R X R satisfies two previous conditions R ( .
prove sufficient show that, clusters disjoint sets, election
cluster optimal, otherwise exists cluster incision
function choose optimal way
Q
Q Minimality would satisfied.
So, suppose reductio exists X KB 1 = (X (!) (KB 1 ))
exist R X R satisfies two previous conditions R ( .
Q
Q
Let us consider KB 0 = (0 , D0 ) KB 1 6= X holds
0 = (Y (!) (KB 1 )) R0 = (Y {KB \ KB 0 }) (those formulas removed
obtaining KB 0 ) 0 = R0 . Since 0 = R0 R0 Q

Q
two conditions

Definition
12

satisfied.
Besides,
let
CF
=

\

1
1
KB
`
`
CF D1 = D1 \ KB set formulas belong kernel 1
D1 , respectively; let KB 0 CF 1 0 CF D1 D0 .
fact every formula conflict belongs KB 0 KB 0
built way election cluster different X
KB 0 (!) (KB 1 ) makes KB 0 \ (X {KB \ KB 0 }) = KB 1 ! \ (X (!) (KB 1 )).
is, difference KB 0 KB 1 ! difference arise
election formulas remove X.
649

fiDeagustini, Martinez, Falappa & Simari

Finally, supposition exists R X R satisfies two
previous conditions R ( . Let KB 0 R ( R = (X {KB \KB 0 })
set formulas removed X obtaining KB 0 . Then, KB 0
coherent consistent, since every conflict clusters KB 1 solved, whether
removing R (for cluster X) sets R0 (for every cluster different X). Besides,
since KB 0 \ (X {KB \ KB 0 }) = KB 1 ! \ (X S(!) (KB 1 )) R ( ,
Q
KB 1 ! = KB 1 \ (!) (KB 1 ) KB 0 = KB 1 \ { {Q
R0 R}
\X}
KB 1

(R0 = {KB \ KB 0 }) R = (X {KB \ KB 0 }) holds KB 1 ! KB 0 (1).
is, formulas involved conflicts belong KB 1 ! KB 0 ,
cluster different X formulas removed, set formulas
removed X obtain KB 0 strict subset removed (!) (KB 1 )
obtain KB 1 !, KB 1 ! strict subset KB 0 , i.e., removed formulas
deleting deleting R.
hand, since KB 0 coherent consistent, Minimality
KB 1 ! 6 KB 0 (2).
0
Therefore, (1) (2) KB 1 ! KB 0 KB
Q
Q 1 ! 6 KB , absurd
= (X
coming initial supposition exists X
KB 1
(!) (KB 1 )) exists R X
R
satisfies

two
previous
conditions
Q
Q
R ( , X

holds


=
(X


(!) (KB 1 ))
KB 1
exists R X R satisfies two previous conditions
R ( T.
omit proof %(!)D well-defined data incision function using Consistency
Minimality since analogous proof (!) well-defined constraint
incision function using Coherence Minimality.
shown %(!)D (!) well-defined data incision functions
constraint incision functions, respectively, conclude second part proof
show !0 coincides !. Inclusion follows D1 ! D1
1 ! 1 (1). Also, definition %(!)D follows %(!)D (KB ?1 ) = D1 \ D1 !,
definition (!) follows (!) (KB 1 ) = 1 \ 1 ! (2). Then,
(1) (2) D1 ! = D1 \ %(!)D (KB ?1 ) 1 ! = 1 \ (!) (KB 1 ). Thus, ! =
(D1 \ %(!)D (KB ?1 ), 1 \ (!) (KB 1 )), therefore !0 coincides !.

References
Alchourron, C., Gardenfors, P., & Makinson, D. (1985). logic theory change:
Partial meet contraction revision functions. Journal Symbolic Logic, 50 (2),
510530.
Alchourron, C., & Makinson, D. (1981). Hierarchies Regulation Logic. New
Studies Deontic Logic, 125148.
Alchourron, C., & Makinson, D. (1985). Logic Theory Change: Safe Contraction.
Studia Logica, 44, 405422.
Amgoud, L., & Kaci, S. (2005). argumentation framework merging conflicting knowledge bases: prioritized case. Proc. 8th European Conferences Symbolic
650

fiDatalog Ontology Consolidation

Quantitative Approaches Reasoning Uncertainty (ECSQUARU 05), pp.
527538.
Andritsos, P., Fuxman, A., & Miller, R. J. (2006). Clean answers dirty databases:
probabilistic approach. Proc. 22nd International Conference Data Engineering (ICDE 06), p. 30.
Arenas, M., Bertossi, L. E., & Chomicki, J. (1999). Consistent query answers inconsistent databases. Proc. 18th ACM SIGACT-SIGMOD-SIGART Symposium
Principles Database Systems (PODS 99), pp. 6879.
Arieli, O., Denecker, M., & Bruynooghe, M. (2007). Distance semantics database repair.
Annals Mathematics Artificial Intelligence, 50 (3-4), 389415.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. Proc. 19th
International Joint Conference Artificial Intelligence (IJCAI 05), pp. 364369.
Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2003). Description Logic Handbook: Theory, Implementation, Applications.
Cambridge University Press.
Baral, C., Kraus, S., & Minker, J. (1991). Combining multiple knowledge bases. Transactions Knowledge Data Engineering, 3 (2), 208220.
Bell, D. A., Qi, G., & Liu, W. (2007). Approaches inconsistency handling descriptionlogic based ontologies. Proc. Move Meaningful Internet Systems
(OTM) Workshops (2), pp. 13031311.
Beneventano, D., & Bergamaschi, S. (1997). Incoherence subsumption recursive
views queries object-oriented data models. Data Knowledge Engineering,
21 (3), 217252.
Berners-Lee, T., Hendler, J., & Lassila, O. (2001). semantic web. Scientific American,
284(5):3443.
Bienvenu, M., & Rosati, R. (2013). Tractable approximations consistent query answering
robust ontology-based data access. Proc. 23rd International Joint Conference
Artificial Intelligence (IJCAI 13), pp. 775781.
Black, E., Hunter, A., & Pan, J. Z. (2009). argument-based approach using multiple ontologies. Proc. 3rd International Conference Scalable Uncertainty
Management (SUM 09), pp. 6879.
Bohannon, P., Flaster, M., Fan, W., & Rastogi, R. (2005). cost-based model effective heuristic repairing constraints value modification. Proc. 24th ACM
SIGMOD International Conference Management Data / Principles Database
Systems (PODS 05), pp. 143154.
Booth, R., Meyer, T. A., Varzinczak, I. J., & Wassermann, R. (2010). Horn belief change:
contraction core. Proc. 19th European Conference Artificial Intelligence
(ECAI 10), pp. 10651066.
Borgida, A. (1995). Description logics data management. Transactions Knowledge
Data Engineering, 7 (5), 671682.
651

fiDeagustini, Martinez, Falappa & Simari

Brandt, S. (2004). Polynomial time reasoning description logic existential restrictions, GCI axioms, - else?. Proc. 16th European Conference
Artificial Intelligence (ECAI 04), pp. 298302.
Cal, A., Gottlob, G., & Kifer, M. (2008). Taming infinite chase: Query answering
expressive relational constraints. Brewka, G., & Lang, J. (Eds.), Proc. 11th
International Conference Principles Knowledge Representation Reasoning
(KR 08), pp. 7080. AAAI Press.
Cal, A., Gottlob, G., & Kifer, M. (2013). Taming infinite chase: Query answering
expressive relational constraints. Journal Artificial Intelligence Research, 48,
115174.
Cal, A., Gottlob, G., & Lukasiewicz, T. (2012). general Datalog-based framework
tractable query answering ontologies. Journal Web Semantic, 14, 5783.
Cal, A., Lembo, D., & Rosati, R. (2003). decidability complexity query answering inconsistent incomplete databases. Proc. 22nd ACM SIGMOD
Symposium Principles database systems (PODS 03), pp. 260271. ACM.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2005). DL-Lite:
Tractable description logics ontologies. AAAI, pp. 602607.
Caroprese, L., Greco, S., & Zumpano, E. (2009). Active integrity constraints database
consistency maintenance. Transactions Knowledge Data Engineering, 21 (7),
10421058.
Caroprese, L., & Truszczynski, M. (2011). Active integrity constraints revision programming. Theory Practice Logic Programming, 11 (6), 905952.
Cecchi, L., Fillottrani, P., & Simari, G. R. (2006). Complexity DeLP
Game Semantics. Dix, J., & Hunter, A. (Eds.), Proc. 11th International Workshop Non-Monotonic Reasoning (NMR 06), pp. 386394.
Cholvy, L. (1998). Reasoning merged information. Belief Change, Vol. 3, pp.
233263. Springer Netherlands.
Croitoru, M., & Rodriguez, R. O. (2015). Using kernel consolidation query answering
inconsistent OBDA. Proc. Joint Ontology Workshops 2015 Episode 1:
Argentine Winter Ontology.
Delgrande, J. P. (2011). Revising inconsistent set formulas. Proc. 22nd
International Joint Conference Artificial Intelligence (IJCAI 11), pp. 833838.
Delgrande, J. P., Dubois, D., & Lang, J. (2006). Iterated revision prioritized merging.
Proc. 10th International Conference Principles Knowledge Representation
Reasoning (KR 06), pp. 210220.
Delgrande, J. P., & Jin, Y. (2012). Parallel belief revision: Revising sets formulas.
Artificial Intelligence, 176 (1), 22232245.
Delgrande, J. P., Schaub, T., Tompits, H., & Woltran, S. (2009). Merging logic programs
answer set semantics. Proc. 25th International Conference Logic Programming (ICLP 09), pp. 160174.
652

fiDatalog Ontology Consolidation

Dunne, P., & Wooldridge, M. (2009). Argumentation Artificial Intelligence, chap. Complexity Abstract Argumentation, pp. 85104. Springer.
Everaere, P., Konieczny, S., & Marquis, P. (2008). Conflict-based merging operators.
Proc. 11th International Conference Principles Knowledge Representation
Reasoning (KR 08), pp. 348357.
Falappa, M. A., Kern-Isberner, G., Reis, M. D. L., & Simari, G. R. (2012). Prioritized
non-prioritized multiple change belief bases. Journal Philosophical Logic, 41 (1),
77113.
Falappa, M. A., Kern-Isberner, G., & Simari, G. R. (2002). Belief Revision, Explanations
Defeasible Reasoning. Artificial Intelligence, 141, 128.
Flouris, G., Huang, Z., Pan, J. Z., Plexousakis, D., & Wache, H. (2006). Inconsistencies,
negations changes ontologies. Proc. 21st National Conference Artificial
Intelligence (AAAI 06), pp. 12951300.
Friedman, N., & Halpern, J. Y. (2001). Belief revision: critique. Computer Research
Repository (CoRR), cs.AI/0103020.
Fuhrmann, A. (1991). Theory contraction base contraction. Journal Philosophical Logic, 20, 175203.
Gardenfors, P. (1982). Rule rational changes belief. Philosophical Essay Dediccated
Lennart Aqvist Fiftieth Birthday, 88101.
Gardenfors, P. (1988). Knowledge Flux: Modeling dynamics epistemic states. MIT
Press.
Gelfond, M. (2008). Answer sets. Handbook Knowledge Representation, chap. 7, pp.
285316. Elsevier.
Gomez, S. A., Chesnevar, C. I., & Simari, G. R. (2010). Reasoning inconsistent
ontologies argumentation. Applied Artificial Intelligence, 24 (1&2), 102148.
Greco, S., & Molinaro, C. (2012). Probabilistic query answering inconsistent databases.
Annals Mathematics Artificial Intelligence (AMAI), 64 (2-3), 185207.
Haase, P., van Harmelen, F., Huang, Z., Stuckenschmidt, H., & Sure, Y. (2005). framework handling inconsistency changing ontologies. Proc. 4th International
Semantic Web Conference (ISWC 05), pp. 353367.
Halaschek-Wiener, C., & Katz, Y. (2006). Belief base revision expressive description
logics. Proc. International Workshop OWL: Experiences Directions
(OWLED 06).
Hansson, S. O. (1991). Belief Base Dynamics. Ph.D. thesis, Uppsala University, Department
Philosophy, Uppsala, Sweden.
Hansson, S. O. (1993). Theory contraction base contraction unified. Journal Symbolic
Logic, 58 (2), 602625.
Hansson, S. O. (1994). Kernel contraction. Journal Symbolic Logic, 59 (3), 845859.
Hansson, S. O. (1997). Semi-revision. Journal Applied Non-Classical Logics, 7 (1-2),
151175.
653

fiDeagustini, Martinez, Falappa & Simari

Hansson, S. O. (2001). Textbook Belief Dynamics: Solutions Exercises. Kluwer
Academic Publishers, Norwell, MA, USA.
Harman, G. (2008). Change view: Principles reasoning. Cambridge University Press.
Harper, W. (1975). Rational Belief Change, Popper Functions Counterfactuals. Synthese, 30, 221262.
Horridge, M., Parsia, B., & Sattler, U. (2009). Explaining inconsistencies OWL ontologies.
Scalable Uncertainty Management, pp. 124137. Springer.
Huang, Z., van Harmelen, F., & ten Teije, A. (2005). Reasoning inconsistent ontologies.
Proc. 19th International Joint Conference Artificial Intelligence (IJCAI 05),
pp. 454459.
Hue, J., Papini, O., & Wurbel, E. (2009). Merging belief bases represented logic programs. Proc. 10th European Conference Symbolic Quantitative Approaches Reasoning Uncertainty (ECSQARU 09), pp. 371382.
Kalyanpur, A., Parsia, B., Horridge, M., & Sirin, E. (2007). Finding justifications
OWL DL entailments. Springer.
Kalyanpur, A., Parsia, B., Sirin, E., & Hendler, J. A. (2005). Debugging unsatisfiable classes
owl ontologies. Web Semantics: Science, Services Agents World Wide
Web, 3 (4), 268293.
Katsuno, H., & Mendelzon, A. O. (1991). difference updating knowledge base revising it. Proc. 2nd International Conference Principles
Knowledge Representation Reasoning (KR91), pp. 387394.
Katsuno, H., & Mendelzon, A. O. (1992). Propositional knowledge base revision minimal change. Artificial Intelligence, 52 (3), 263294.
Konieczny, S., & Perez, R. P. (2002). Merging information constraints: logical
framework. Journal Logic Computation, 12 (5), 773808.
Konieczny, S., & Perez, R. P. (2011). Logic based merging. Journal Philosophical Logic,
40 (2), 239270.
Lembo, D., Lenzerini, M., Rosati, R., Ruzzi, M., & Savo, D. F. (2010). Inconsistencytolerant semantics description logics. Proc. 4th International Conference
Web Reasoning Rule Systems (RR 10), pp. 103117.
Lenzerini, M. (2002). Data integration: theoretical perspective. Proc, 21st ACM
SIGMOD Symposium Principles Database Systems (PODS 02), pp. 233246.
Levi, I. (1977). Subjunctives, Dispositions Chances. Synthese, 34 (4), 423455.
Liberatore, P., & Schaerf, M. (1998). Arbitration (or merge knowledge bases).
Knowledge Data Engineering, 10 (1), 7690.
Lin, J., & Mendelzon, A. O. (1998). Merging databases constraints. International
Journal Cooperative Information Systems, 7 (1), 5576.
Lin, J., & Mendelzon, A. O. (1999). Knowledge base merging majority. Applied Logic
Series, 18, 195218.
654

fiDatalog Ontology Consolidation

Lloyd, J. W. (1987). Foundations Logic Programmming. Springer-Verlag.
Lukasiewicz, T., Martinez, M. V., & Simari, G. I. (2012). Inconsistency handling
datalog+/- ontologies. Proc. 20th European Conference Artificial Intelligence (ECAI 12), pp. 558563.
Martinez, M., Pugliese, A., Simari, G., Subrahmanian, V., & Prade, H. (2007). dirty
relational database? axiomatic approach. Mellouli, K. (Ed.), Proc.
9th European Conference Symbolic Quantitative Approaches Reasoning
Uncertainty (ECSQARU 07), Vol. 4724 Lecture Notes Computer Science, pp.
103114. Springer.
Meyer, T., Lee, K., & Booth, R. (2005). Knowledge integration description logics.
Veloso, M., & Kambhampati, S. (Eds.), Proceedings AAAI05, Twentieth National
Conference Artificial Intelligence, pp. 645650. AAAI Press.
Newell, A. (1982). Knowledge Level. Artificial Intelligence, 18, 87127.
Nilsson, U., & Maluszynski, J. (1995). Logic, Programming Prolog (2ed). John Wiley
& Sons Ltd.
Parsons, S., Wooldridge, M., & Amgoud, L. (2003). Properties complexity
formal inter-agent dialogues. Journal Logic Computation, 13 (3), 347376.
Qi, G., & Hunter, A. (2007). Measuring incoherence description logic-based ontologies.
Proc. 6th International Semantic Web Conference 2nd Asian Semantic
Web Conference (ISWC/ASWC 07), pp. 381394.
Qi, G., Liu, W., & Bell, D. A. (2006). Knowledge base revision description logics.
Proc. 10th European Conference Logics Artificial Intelligence (JELIA 06),
pp. 386398.
Quine, W. V. O. (1986). Philosophy logic. Harvard University Press.
Reiter, R. (1987). theory diagnosis first principles. Artificial Intelligence, 32(1),
5795.
Rosati, R. (2011). complexity dealing inconsistency description logic
ontologies. Proc. International Joint Conference Artificial Intelligence (IJCAI
11), pp. 10571062.
Rott, H. (1992). Modellings belief change: Prioritization entrenchment. Theoria,
58 (1), 2157.
Schlobach, S., & Cornet, R. (2003). Non-standard reasoning services debugging
description logic terminologies.. Proceedings Eighteenth International Joint
Conference Artificial Intelligence (IJCAI 03), pp. 355362.
Schlobach, S., Huang, Z., Cornet, R., & van Harmelen, F. (2007). Debugging incoherent
terminologies. Journal Automated Reasoning, 39 (3), 317349.
Staworko, S., Chomicki, J., & Marcinkowski, J. (2012). Prioritized repairing consistent query answering relational databases. Annals Mathematics Artificial
Intelligence, 64 (2-3), 209246.
655

fiDeagustini, Martinez, Falappa & Simari

Turner, H. (2003). Strong equivalence made easy: nested expressions weight constraints.
Theory Practice Logic Programming, 3 (4-5), 609622.
von Leibniz, G. W. F. (1976). Philosophical Papers Letters: selection, Vol. 1. Springer.
Wassermann, R. (2000). algorithm belief revision. Proc. International Conference Principles Knowledge Representation Reasoning (KR 00), pp. 345
352.
Wijsen, J. (2005). Database repairing using updates. ACM Transaction Database Systems, 30 (3), 722768.

656

fiJournal Artificial Intelligence Research 56 (2016) 153-195

Submitted 03/15; published 06/16

Global Continuous Optimization Error Bound
Fast Convergence
Kenji Kawaguchi

kawaguch@mit.edu

Massachusetts Institute Technology
Cambridge, MA, USA

Yu Maruyama

maruyama.yu@jaea.go.jp

Nuclear Safety Research Center
Japan Atomic Energy Agency
Tokai, Japan

Xiaoyu Zheng

zheng.xiaoyu@jaea.go.jp

Nuclear Safety Research Center
Japan Atomic Energy Agency
Tokai, Japan

Abstract
paper considers global optimization black-box unknown objective function
non-convex non-differentiable. difficult optimization problem arises
many real-world applications, parameter tuning machine learning, engineering
design problem, planning complex physics simulator. paper proposes new
global optimization algorithm, called Locally Oriented Global Optimization (LOGO), aim
fast convergence practice finite-time error bound theory. advantage
usage new algorithm illustrated via theoretical analysis experiment
conducted 11 benchmark test functions. Further, modify LOGO algorithm
specifically solve planning problem via policy search continuous state/action
space long time horizon maintaining finite-time error bound. apply
proposed planning method accident management nuclear power plant. result
application study demonstrates practical utility method.

1. Introduction
Optimization problems prevalent held great importance throughout history
engineering applications scientific endeavors. instance, many problems
field artificial intelligence (AI) viewed optimization problems. Accordingly,
generic local optimization methods, hill climbing gradient method,
successfully adopted solve AI problems since early research topic (Kirk, 1970;
Gullapalli, Franklin, & Benbrahim, 1994; Deisenroth & Rasmussen, 2011).
hand, application global optimization AI problems studied much less
despite practical importance. mainly due lack necessary computational
power past absence practical global optimization method strong
theoretical basis. two obstacles, former becoming less serious today,
evidenced number studies global optimization past two decades (Horst
& Tuy, 1990; Ryoo & Sahinidis, 1996; He, Verstak, Watson, Stinson, et al., 2004; Rios &
Sahinidis, 2013). aim paper partially address latter obstacle.
c
2016
AI Access Foundation. rights reserved.

fiKawaguchi, Maruyama, & Zheng

inherent difficulty global optimization problem led two distinct research
directions: development heuristics without theoretically guaranteed performance
advancement theoretically supported methods regardless difficulty. degree
practical success resulted heuristic approaches simulated annealing, genetic
algorithms (for brief introduction context AI, see Russell & Norvig, 2009),
swarm-based optimization (for interesting example recent study, see Daly & Shen,
2009). Although methods heuristics without strong theoretical supports,
became popular partly optimization mechanisms aesthetically mimic
natures physical biological optimization mechanism.
hand, Lipschitzian approach global optimization aims accomplish global optimization task theoretically supported manner. Despite early
successes theoretical viewpoints (Shubert, 1972; Mladineo, 1986; Pinter, 1986; Hansen,
Jaumard, & Lu, 1991), early studies based assumption impractical
applications: Lipschitz constant, bound slope objective
function, known. relaxation crucial assumption resulted well-known
DIRECT algorithm (Jones, Perttunen, & Stuckman, 1993) worked well practice,
yet guarantees consistency property. Recently, Simultaneous Optimistic Optimization (SOO) algorithm (Munos, 2011) achieved guarantee finite-time error bound
without knowledge Lipschitz constant. However, practical performance
algorithm unclear.
paper, propose generic global optimization algorithm aimed achieve
satisfactory performance practice finite-loss bound theoretical basis
without strong additional assumption1 (Section 2), apply AI planning problem
(Section 6). AI planning problem, aim solving real-world engineering problem
long planning horizon continuous state/action space. illustration
advantage method, present preliminary results application study
conducted accident management nuclear power plant well. Note optimization problems discussed paper practically relevant yet inherently difficult
scale higher dimensions, i.e., NP-complete (Murty & Kabadi, 1987). Accordingly,
discuss possible extensions algorithm higher dimensional problems,
experimental illustration 1000-dimensional problem.

2. Global Optimization Black-Box Function
goal global optimization solve following general problem:
maxx f (x)
subject x
f objective function defined domain RD . Since performance
proposed algorithm independent scale , consider problem
rescaled domain 0 = [0, 1]D . Further, paper, focus deterministic function f .
1. paper, use term strong additional assumption indicate assumption tight
Lipschitz constant known and/or main assumption many Bayesian optimization methods
objective function sample Gaussian process known kernel hyperparameters.

154

fiGlobal Continuous Optimization Error Bound Fast Convergence

global optimization, performance algorithm assessed loss rn ,
given
rn = max0 f (x) f (x+ (n)).
x

x+ (n)

Here,
best input vector found algorithm n trials (more precisely,
define n denote total number divisions next section).
minimal assumption allows us solve problem objective function f evaluated points 0 arbitrary order. applications,
assumption easily satisfied, example, simulator world dynamics
experimental procedure defines f itself. former case, x corresponds
input vector simulator f , ability arbitrarily change input
run simulator satisfies assumption. possible additional assumption
gradient function f evaluated. Although assumption may produce
effective methods, limits applicability terms real-world applications. Therefore,
assume existence simulator method evaluate f , access
gradient f . methods scope often said derivative-free
objective function said black-box function.
However, assumption made, general problem proven
intractable. specifically, number function evaluations cannot guarantee getting
close optimal (maximum) value f (Dixon, 1978). solution may
exist arbitrary high narrow peak, makes impossible relate optimal
solution evaluations f points.
One simplest additional assumptions restore tractability would
slope f bounded. form assumption studied Lipschitz continuity
f :
|f (x1 ) f (x2 )| bkx1 x2 k, x1 , x2 0 ,
(1)
b > 0 constant, called Lipschitz constant, k k denotes Euclidean
norm. global optimization assumption referred Lipschitz optimization, studied long time. best-known algorithm early days
history Shubert algorithm (Shubert, 1972), equivalently Piyavskii algorithm (Piyavskii, 1967) algorithm independently developed. Based
assumption Lipschitz constant known, creates upper bound function
objective function chooses point 0 highest upper bound
iteration. problems higher dimension 2, finding point highest upper bound becomes difficult many algorithms proposed tackle
problem (Mayne & Polak, 1984; Mladineo, 1986). algorithms successfully provided
finite-loss bounds.
However appealing theoretical point view, practical concern soon raised
regarding assumption Lipschitz constant known. many applications,
complex physics simulator objective function f , Lipschitz constant
indeed unknown. researchers aimed relax somewhat impractical assumption
proposing procedures estimate Lipschitz constant optimization process
(Strongin, 1973; Kvasov, Pizzuti, & Sergeyev, 2003). Similarly, Bayesian optimization
method upper confidence bounds (Brochu, Cora, & de Freitas, 2009) estimates objective function upper confidence bounds certain model assumption, avoiding
155

fiKawaguchi, Maruyama, & Zheng

prior knowledge Lipschitz constant. Unfortunately, approach, including
Bayesian optimization method, results mere heuristics unless several additional
assumptions hold. notable assumptions algorithm maintain overestimate upper bound finding point highest upper
bound done timely manner. noted Hansen Jaumard (1995),
unclear approach provides advantage, considering successful heuristics
already available. argument still applies day relatively recent algorithms
Kvasov et al. (2003) Bubeck, Stoltz, Yu (2011).
Instead trying estimate unknown Lipschitz constant, well-known DIRECT
algorithm (Jones et al., 1993) deals unknowns simultaneously considering
possible Lipschitz constants, b: 0 < b < . past decade, many
successful applications DIRECT algorithm, even large-scale engineering problems
(Carter, Gablonsky, Patrick, Kelly, & Eslinger, 2001; et al., 2004; Zwolak, Tyson, &
Watson, 2005). Although works well many practical problems, DIRECT algorithm
guarantees consistency property, limn rn = 0 (Jones et al., 1993; Munos, 2013).
SOO algorithm (Munos, 2011) expands DIRECT algorithm solves major
issues, including weak theoretical basis. is, SOO algorithm guarantees
finite-time loss bound without knowledge slopes bound, also employs
weaker assumption. contrast Lipschitz continuity assumption used DIRECT
algorithm (Equation (1)), SOO algorithm requires local smoothness assumption
described below.

Assumption 1 (Local smoothness). decreasing rate objective function f around
least one global optimal solution {x 0 : f (x ) = supx0 f (x)} bounded semimetric `, x 0
f (x ) f (x) `(x, x ).

Here, semi-metric generalization metric satisfy
triangle inequality. instance, `(x, x ) = bkx xk metric semi-metric.
hand, whenever > 1 p < 1, `(x, x ) = bkx xkp metric
semi-metric since satisfy triangle inequality. assumption much
weaker assumption described Equation (1) two reasons. First, Assumption 1
requires smoothness (or continuity) global optima, Equation (1)
points whole input domain, 0 . Second, Lipschitz continuity assumption
Equation (1) requires smoothness defined metric, Assumption 1 allows
semi-metric used. best knowledge, SOO algorithm
algorithm provides finite-loss bound weak assumption.
Summarizing above, DIRECT algorithm successful practice,
concern weak theoretical basis led recent development generalized
version, SOO algorithm. generalize SOO algorithm increase
practicality strengthen theoretical basis time. paper adopts
weak assumption, Assumption 1, maintain generality wide applicability.
156

fiGlobal Continuous Optimization Error Bound Fast Convergence

Iteration 1

Iteration 2

Iteration 4

(N = 9)

w=1

(N = 3)

Iteration 3

w=4

(N = 9)

Figure 1: Illustration SOO (w = 1) LOGO (w = 1 4) end iteration

3. Locally Oriented Global Optimization (LOGO) Algorithm
section, modify SOO algorithm (Munos, 2011) accelerate convergence
guaranteeing theoretical loss bounds. new algorithm modification,
LOGO (Locally Oriented Global Optimization) algorithm, requires additional assumption. use LOGO algorithm, one needs prior knowledge objective
function f ; may leverage prior knowledge available. algorithm uses two parameters, hmax (n) w, inputs hmax (n) [1, ) w Z+ . hmax (n) w
act part balance local global search. hmax (n) biases search towards
global search whereas w orients search toward local area.
case w = 1 4 (top bottom diagrams) Figure 1 illustrates functionality LOGO algorithm simple 2-dimensional objective function. view,
LOGO algorithm generalization SOO algorithm local orientation
parameter w SOO special case LOGO fixed parameter w = 1.
3.1 Predecessor: SOO Algorithm
discuss algorithm detail, briefly describe direct predecessor,
SOO algorithm2 (Munos, 2011). top diagrams Figure 1 (the scenario w =
1) illustrates functionality SOO algorithm simple 2-dimensional objective
function. illustrated Figure 1, SOO algorithm employs hierarchical partitioning
maintain hyperintervals, center evaluation point objective
function f . is, Figure 1, rectangle represents hyperintervals end
iteration algorithm. Let h set rectangles size
divided h times. algorithm uses parameter, hmax (n), limit size rectangle
overly small (and hence restrict greediness search). order select
refine intervals likely contain global optimizer, algorithm executes
following procedure:
2. describe SOO simple division procedure LOGO uses. SOO specify
division procedure.

157

fiKawaguchi, Maruyama, & Zheng

(i)
(ii)
(iii)
(iv)

Initialize 0 = {0 } > 0, = {}
Set h = 0
Select interval maximum center value among intervals set h
interval selected (iii) center value greater larger
interval (i.e., intervals l l < h), divide adds new intervals
h+1 . Otherwise, reject interval skip step.

(v) Set h = h + 1
(vi) Repeat (iii)(v) smaller interval exists (i.e., l = {} l h)
h > hmax (n)
(vii) Delete intervals already divided (iv) repeat (ii)(vi)
explain procedure using example Figure 1. brevity, use
term, iteration, refer iteration step (ii)(vii). Figure 1, center point
shown (black) dot rectangle rectangle (red) circle around
(black) dot one divided (into three smaller rectangles) iteration.
beginning first iteration, one rectangle 0 , entire
search domain 0 . Thus, step (iii) selects rectangle step (iv) divides it, resulting
leftmost diagram N = 3 (the rectangle center point red circle
one divided first iteration two created result).
beginning second iteration, three rectangles 1 (i.e., three rectangles
leftmost diagram N = 3) none 0 (because step (vii) previous
iteration deleted interval 0 ). Hence, steps (iii)(iv) executed 0
begin 1 . Step (iii) selects top rectangle three rectangles
maximum center point among these. Step (iv) divides
larger interval, resulting second diagram top (labeled w = 1). Iteration
2 continues conducting steps (iii)(iv) 2 three smaller rectangles
2 . Step (iii) selects center rectangle top (in second diagram top
labeled w = 1). However, step (iv) rejects center value greater
larger rectangle l l < h = 2. smaller rectangle
iteration 2 ends. beginning iteration 3, two rectangles 1
three rectangles 2 (as shown second diagram top labeled w = 1).
Iteration 3 begins conducting steps (iii)(iv) 1 . Steps (iii)(iv) select divide
top rectangle. rectangles 2 , steps (iii)(iv) select divides middle rectangle.
Here, middle rectangle rejected iteration 2 larger rectangle
larger center value existed iteration 2. However, larger rectangle longer
exists iteration 3 due step (vii) end iteration 2, hence rejected.
result third diagram (on top labeled w = 1). Iteration 3 continues
newly created rectangles 3 . halts, however, reason iteration 2.
3.2 Description LOGO
Let k superset union w sets k = kw kw+1 kw+w1
k = 0, 1, 2, . . . . Then, similar SOO algorithm, LOGO algorithm conducts
following procedure select refine intervals likely contain global
optimizer:
158

fiGlobal Continuous Optimization Error Bound Fast Convergence

(i) Initialize 0 = {0 } > 0, = {}
(ii) Set k = 0
(iii) Select interval maximum center value among intervals superset k
(iv) interval selected (iii) center value greater larger interval
(i.e., intervals l l < k), divide adds new intervals . Otherwise,
reject interval skip step.
(v) Set k = k + 1
(vi) Repeat (iii)(v) smaller interval exists (i.e., l = {} l k)
k > bhmax (n)/wc.
(vii) Delete intervals already divided (iv) repeat (ii)(vi)
compared SOO algorithm, steps identical except
LOGO processes superset k instead set h . superset k reduced h
k = h w = 1 thus LOGO reduced SOO.
explain procedure using example Figure 1. w = 1, LOGO
algorithm functions fashion SOO algorithm. See last paragraph
previous section explanation SOO LOGO algorithms function
example. case w = 4, difference arises iteration 3
compared case w = 1. beginning iteration 3, two sets 1
2 (i.e., two sizes rectangles second diagram bottom w = 4).
However, one superset consisting two sets 0 = 0 0+1 0+41 .
Therefore, step (iii)(iv) conducted k = 0 LOGO algorithm divides
one rectangle highest center value among 0 . Consequently,
algorithm one additional iteration (iteration 4) using number function
evaluations (N = 9) case w = 1. seen w increases,
algorithm biased local search and, example, strategy turns
beneficial algorithm divides rectangle near global optima
w = 4 w = 1.
pseudocode LOGO algorithm provided Algorithm 1. Steps (ii), (v),
(vi) correspond for-loop lines 1019. Steps (iii)(iv) correspond line 11 line
1214, respectively. use following notation. hyperrectangle, 0 , coupled
function value center point f (c ), c indicates center point
rectangle. explained earlier, use h denote number divisions index
set h . define h,i ith element set h (i.e., h,i h ). Let
xh,i ch,i arbitrary point center point rectangle h,i , respectively.
denote val[h,i ] indicate stored function value center point rectangle
h,i . seen line 14, paper considers simple division procedure
rescaled domain 0 . prior knowledge domain function,
leverage information. example, could map original input space
another obtain better ` based theoretical results Section 4,
could employ elaborate division procedure based prior knowledge.
discussed LOGO algorithm functions, consider reason
algorithm might work well. key mechanism DIRECT SOO algorithms
159

fiKawaguchi, Maruyama, & Zheng

Algorithm 1: LOGO algorithm
0:
1:
2:
3:
4:
5:

6:
7:
8:
9:
10:
11:
12:
13:
14:

15:

16:

17:
18:
19:
20:

Inputs (problem): objective function f : x RD R, search domain : x
Inputs (parameter): search depth function hmax : Z+ [1, ), local weight
w Z+ , stopping condition
Define set h set hyperrectangles divided h times
Define superset k union w sets: k = kw kw+1 kw+w1
Normalize domain 0 = [0, 1]D
Initialize variables: set hyperrectangles: h = {}, h = 0, 1, 2, . . . ,
current maximum index set: hupper = 0
number total divisions: n = 1
Adds initial hyperrectangle 0 set: 0 0 {0 } (i.e., 0,0 = 0 )
Evaluate function f center point 0 , c0,0 : val [0,0 ] f (c0,0 )
iteration = 1, 2, 3, . . .
val max , hplus hupper
k = 0, 1, 2, . . . , max(bmin(hmax (n), hupper )/wc, hplus )
Select hyperrectangle divided: (h, i) arg maxh,i val [h,i ] h, : h,i k
val [h,i ] > val max
val max val [h,i ], hplus 0, hupper max(hupper , h + 1), n n + 1
Divide hyperrectangle h,i along longest coordinate direction
- three smaller hyperrectangles created left , center , right
- val [center ] val [h,i ]
Evaluate function f center points two new hyperrectangles:
val [left ] f (cleft ), val [right ] f (cright )
Group new hyperrectangles set h+1 remove original rectangle:
h+1 h+1 {center , left , right }, h h \ h,i
end
stopping condition met Return (h, i) = arg maxh,i val [h,i ]
end
end

divide hyperintervals potentially highest upper bounds w.r.t. unknown
smoothness iteration. idea behind LOGO algorithm reduce number
divisions per iteration biasing search toward local area concept
supersets. Intuitively, beneficial two reasons. First, reducing
number divisions per iteration, information utilized selecting intervals
divide. example, one may simultaneously divide five ten intervals per iteration.
former, selecting sixth tenth interval divide, one leverage
information gathered previous five divisions (evaluations), whereas latter makes
impossible. selection intervals depends information, turn
provides new information next selection, minor difference availability
information may make two sequences search different long run.
Second, biasing search toward local area, algorithm likely converges faster
certain type problem. many practical problems, aim find position
160

fiGlobal Continuous Optimization Error Bound Fast Convergence

global optima, position function value close global optima. case,
local bias likely beneficial unless many local optima, value far
global optima. Even though local bias strategy motivated solve
problem impractically slow convergence rate global optimization methods,
algorithm maintains guaranteed loss bounds w.r.t. global optima, discussed below.

4. Theoretical Results: Finite-Time Loss Analysis
first derive loss bounds LOGO algorithm uses division strategy
satisfies certain assumptions. Then, provide loss bounds algorithm
concrete division strategy provided Algorithm 1 parameter values
use rest paper. motivation first part extend existing
framework theoretical analysis thus produce basis future work.
second part prove LOGO algorithm maintains finite-time loss bounds
parameter settings actually use experiments.
4.1 Analysis General Division Method
section, generalize result obtained Munos (2013) previous result
seen special case new result w = 1. previous work provided
loss bound SOO algorithm division process satisfied following
two assumptions, adopt section.
Assumption A1 (Decreasing diameter). exists function (h) > 0
hyperinterval h,i 0 , (h) supxh,i `(xh,i , ch,i ), (h 1) (h) holds
h 1.
Assumption A2 (Well-shaped cell). exists constant > 0 hyperinterval h,i contains `-ball radius (h) centered h,i .
Intuitively, Assumption A1 states unknown local smoothness ` upper-bounded
monotonically decreasing function h. assumption ensures division
increase upper bound, (h). Assumption A2 ensures every interval covers
least certain amount space order relate number intervals unknown
smoothness ` (because ` defined terms space). present analysis, need
define relevant terms variables. define -optimal set X
X := {x 0 : f (x) + f (x )}.
is, set -optimal set X set input vectors whose function value least
-close value global optima. order bound number hyperintervals
relevant -optimal set X , define near-optimality dimension follows.
Definition 1 (Near-optimality dimension). near-optimality dimension smallest
0 exists C > 0, > 0, maximum number disjoint `-balls
radius centered -optimal set X less equal Cd .
near-optimality dimension introduced Munos (2011) closely related
previous measure used Kleinberg, Slivkins, Upfal (2008). value
161

fiKawaguchi, Maruyama, & Zheng

near-optimality dimension depends objective function f , semi-metric `
division strategy (i.e., constant Assumption A2). consider semi-metric `
satisfies Assumptions 1, A1, A2, value depends semimetric ` division strategy. Theorem 2, show division strategy
LOGO algorithm let = 0 general class semi-metric `.
defined relevant terms variables used previous work,
introduce new concepts advance analysis. First, define set -optimal
hyperinterval h (w)
h (w) := {h,i 0 : f (ch,i ) + (h w + 1) f (x )}.
-optimal hyperinterval h (w) used relate hyperintervals -optimal set X .
Indeed, -optimal hyperinterval h (w) almost identical (h w + 1)-optimal
set X(hw+1) (-optimal set X (hw+1)), except h (w) considers
hyperintervals values center points X(hw+1) whole
input vector space. order relate h (w) h (1) , define `-ball ratio follows.
Definition 2 (`-ball ratio). every h w, `-ball ratio smallest h (w) > 0
volume `-ball radius (h w + 1) volume h (w)
disjoint `-balls radius (h).
following lemma, bound maximum cardinality h (w) . use |h (w) |
denote cardinality.
Lemma 1. Let near-optimality dimension C denote corresponding constant Definition 1. Let h (w) `-ball ratio Definition 2. Then, -optimal
hyperinterval bounded
|h (w) | Ch (w)(h w + 1)d .
Proof. proof follows definition -optimal set X , Definition 1, Definition 2,
Assumption A2. definition -optimal space X , write (hw+1)-optimal
set
X(hw+1) = {x 0 : f (x) + (h w + 1) f (x )}.
definition near-optimality dimension (Definition 1) implies C(h
w + 1)d centers disjoint `-balls radius (h w + 1) exist within space X(hw+1) .
Then, definition `-ball ratio (Definition 2), space C(h w + 1)d
disjoint `-balls radius (h w + 1) covered Ch (w)(h w + 1)d disjoint
`-balls radius (h). Notice set space covered C(h w + 1)d disjoint
`-balls radius (h w + 1) superset X(hw+1) . Therefore, deduce
Ch (w)(h w + 1)d centers disjoint `-balls radius (h) within
X(hw+1) . Now, recall definition h-w-optimal interval,
h (w) := {h,i 0 : f (ch,i ) + (h w + 1) f (x )}
notice number intervals equal number centers ch,i satisfy
condition f (ch,i ) + (h w + 1) f (x ). Assumption A2 causes number
equivalent number centers disjoint `-balls radius (h), showed
upper bounded Ch (w)(h w + 1)d .
162

fiGlobal Continuous Optimization Error Bound Fast Convergence

Next, bound maximum size optimal hyperinterval, contains global
optimizer x . following analysis, use concept set superset
hyperintervals. Recall set h contains hyperintervals divided h
times thus far, superset k union w sets, given k = kw kw+1
kw+w1 k = 0, 1, 2, . . . . say hyperinterval dominated intervals
hyperinterval divided center value
hyperintervals set.
Lemma 2. Let kn highest integer optimal hyperinterval, contains
global optimizer x , belongs superset kn n total divisions (i.e., kn n
determines size optimal hyperinterval, hence loss algorithm). Then,
kn lower bounded kn K K satisfies 0 K bhmax (n)/wc
K

w1
X
hmax (n) + w X


n
|kw (1) | +
|kw+l (l + 1) | .
w


k=0

l=1

Proof. Let (k ) number divisions, algorithm divides
optimal hyperinterval superset k places k+1 . example Figure 1
w = 1, optimal hyperinterval initially whole domain 0 0 . divided
first division optimal hyperinterval placed 1 . Therefore, (0 ) = 1.
Similarly, (1 ) = 2. division non-optimal interval occurs optimal
one (2 ) hence (2 ) = 4. words, (k ) time optimal
hyperinterval superset k divided escapes superset k , entering
k+1 . Let ckw+l,i center point optimal hyperinterval set kw+l k .
prove statement showing quantity (k ) (k1 ) bounded
number -optimal hyperintervals h (w) . so, let us consider possible
hyperintervals divided time [ (k1 ), (k ) 1]. hyperintervals
set kw , ones possibly divided time must satisfy f (ckw,i ) f (ckw,i ) f (x ) (kw). first inequality due fact
algorithm divide interval center value less maximum center value existing interval set, exists f (ckw,i ) time
[ (k1 ), (k ) 1]. second inequality follows Assumption 1 definition
optimal interval. Then, definition h (w) , hyperintervals possibly
divided time belong kw (1) k .
addition set kw , superset k , sets kw+l l : w 1 l 1.
sets, f (ckw+l,i ) f (clw+l,i ) f (x ) (kw) similar deductions.
Here, notice time [ (k1 ), (k ) 1], sure center value
superset lower bounded f (ckw,i ) instead f (ckw+l,i ). addition,
(kw) = (kw + l l). Thus, conclude hyperintervals set kw+l
divided time [ (k1 ), (k ) 1] belongs kw+l (l + 1) (w 1) l 1.
hyperinterval superset k may divided iteration since intervals
dominated supersets. case, f (cjw+l,i ) f (ckw,i )
f (x ) (kw) j < k l 0. similar deductions, easy see
f (x ) (kw) f (x ) (jw + l). Thus, hyperintervals superset j j < k
dominate superset k [ (k1 ), (k ) 1] belongs jw+l (1) .
163

fiKawaguchi, Maruyama, & Zheng

Putting results together noting algorithm divides
bhmax (n)/wc + 1 intervals iteration (hplus plays role algorithm divides one interval),

(k ) (k1 )



w1
k1 w1
X
X
X
hmax (n)



+ 1 |kw (1) | +
|kw+l (l + 1) | +
|jw+l (1) | .
w
j=1 l=0

l=1

Then,


kn
X
k=1

kn

w1
X
hmax (n) + w X


(k ) (k1 )
|kw (1) | +
|kw+l (l + 1) |
w


k=1

l=1

since last term superset j j k 1 previous inequality contains
optimal
Pkn intervals subsets optimal intervals covered new
summation k=1 .
kn bhmax (n)/wc, statement always holds true 0 K bhmax (n)/wc
since kn bhmax (n)/wc K. Accordingly, assume kn < bhmax (n)/wc following.
Since (0 ) upper bounded term previous summation right hand
inequality k = 0,


hmax (n) + w
(kn +1 )
w



kX
w1
n +1
X


|kw+l (l + 1) | .
|kw (1) | +

l=1

k=0

definition kn , n < (kn +1 ). Therefore, K bhmax (n)/wc



K

w1
X
hmax (n) + w X


|kw (1) | +
|kw+l (l + 1) |
w
k=0

l=1



hmax (n) + w
n<
w


kX
n +1



|kw (1) | +

w1
X

k=0


|kw+l (l + 1) | ,


l=1

kn K.
Lemmas 1 2, present main result section provides
finite-time loss bound LOGO algorithm.
Theorem 1. Let ` semi-metric Assumptions 1, A1, A2 satisfied.
Let h(n) smallest integer h


hmax (n) + w
nC
w

bh/wc

w1
X
X


(kw) +
kw+l (l + 1)(kw)
.
k=0

l=1

Then, loss LOGO algorithm bounded

rn min(wbh(n)/wc w, wbhmax (n)/wc) .
164

fiGlobal Continuous Optimization Error Bound Fast Convergence

Proof. Lemma 1 definition h(n),


hmax (n) + w
n>C
w


hmax (n) + w

w


bh(n)/wc1
w1
X
X


(kw) +
kw+l (l + 1)(kw + l l)
k=0

l=1

bh(n)/wc1
X



|kw (1) | +

k=0

w1
X


|kw+l (l + 1) | .


l=1

Therefore, set K K = bh(n)/wc 1 following apply result Lemma 2.
Then, follows kn K(n) K < bhmax (n)/wc. Here, number divisions
interval superset K least Kw = wbh(n)/wc w. Therefore,
Assumptions 1, A1, A2, deduce rn (wbh(n)/wc w).
K bhmax (n)/wc, b(hupper )/wc kn bhmax (n)/wc. Thus,
case, kn equal least bhmax (n)/wc. Assumptions 1, A1, A2,
similarly deduce rn (wbhmax (n)/wc).
loss bound stated Theorem 1 applies LOGO algorithm division
strategy satisfies Assumptions A1 A2. add following assumption
division process derive concrete forms loss bound.
Assumption A3 (Decreasing diameter revisit). decreasing diameter defined Assumption 1 written (h) = c h/D c > 0 < 1, accordingly
corresponding `-ball ratio h (w) = ((h w + 1)/(h))D .
Assumption A3 similar assumption made Munos (2013),
(h) = c h . contrast previous assumption, assumption explicitly reflects
fact size hyperinterval decreases slower rate higher dimensional
problems. LOGO algorithm, validity Assumptions A1, A2, A3 confirmed next section.
present finite-loss bound LOGO algorithm case general
division strategy additional assumption = 0.
Corollary 1. Let ` semi-metric Assumptions 1, A1, A2, A3 satisfied.

near-optimality dimension = 0 hmax (n) set n w, loss
LOGO algorithm bounded n
!




w w 1 1

w
1
rn c exp min
n
2, n w
ln
.
C 1 1

Proof. Based definition h(n) Theorem 1, first relate h(n) n
hmax (n) + w
nC
w
hmax (n) + w
=C
w

bh(n)/wc

X

(kw)



+

k=0

k=0





kw+l (l + 1)(kw + l l)

l=1

bh(n)/wc

X

w1
X

1+

w1
X
l=1



w



hmax (n) + w
C
w

165




w1
X
h(n)
+1
w .
w
l=0

fiKawaguchi, Maruyama, & Zheng

0.9
0.8
0.7
0.6



0.5
0.4
0.3
0.2
0.1
1

2

3

4

5

w
Figure 2: Effect local bias w loss bound case = 0 := w2 ( 1 1)/( w 1)
first line follows definition h(n), second line due = 0
Assumption A3. algebraic manipulation,


Here, use hmax (n) =


w

h(n)
n
w

1 1

1.
w
C hmax (n) + w 1 1


n w, hence





w w 1 1
h(n)
n
1.
w
C 1 1

substituting results statement Theorem 1,



!
w2 w 1 1

2
n
2w, w n w
.
rn min
C 1 1
Assumption A3, (h) = c h/D . using (h) = c h/D inequality,
statement corollary.
Corollary 1 shows LOGO algorithm guarantees exponential bound

loss terms n (a stretched exponential bound terms n). loss bound
Corollary 1 becomes almost identical SOO algorithm w = 1. Accordingly,

illustrate effect w, n large enough let us focus coefficient n,
Figure 2. (red) bold line label 1 indicates area w effect
bound. area lines labels greater one w improves bound,
area labels less one w diminishes bound. concretely,

figure, consider ratio coefficient n loss bound various
value w w = 1. ratio w2 ( 1 1)/( w 1) w, depending
element min bound smaller. Since w2 ( 1 1)/( w 1) w (in
domain consider), plotted w2 ( 1 1)/( w 1) avoid overestimating benefit
w. Thus, rather pessimistic illustration advantage generalization
166

fiGlobal Continuous Optimization Error Bound Fast Convergence

regarding w. instance, second element min bound smaller n
large enough, increasing w always improves bound, regardless values Figure 2.
next corollary presents finite-loss bound LOGO algorithm case
6= 0.
Corollary 2. Let ` semi-metric Assumptions 1, A1, A2, A3 satisfied.
near-optimality dimension > 0 hmax (n) set ((ln n)c1 ) w
c1 > 1, loss LOGO algorithm bounded
!

w


1 1 1/d
2 wd/D
2wd/D
1/d
w (

)
rn n
.
1 1
Proof. way first step proof Corollary 1, except > 0,
n Ccd

hmax (n) + w
w

bh(n)/wc w1

X

X

k=0

l=0

lkwd/D .

reason couldP
bound loss similar rate case = 0
last summation term w1
l=0 longer independent k. Since
w1
X



lkwd/D

=

w

1
,
1
1

kwd/D

l=0

bh(n)/wc

X
k=0

kwd/D =

(bh(n)/wc+1)wd/D 1
,
wd/D 1

algebraic manipulation,


c

(

(bh(n)/wc+1)wd/D

w

w

1 1 wd/D
n
(
1).
1)
C hmax (n) + w 1 1

Therefore,
c (wbh(n)/wcw)/D

!1/d
w

n
w

1 1 wd/D
.
(
1) 2wd/D
C hmax (n) + w 1 1

Theorem 1 Assumption A3,

rn max

!
w

1/d
n
w

1 1 wd/D
(
2wd/D )
, c (wbhmax (n)/wcw)/D .
C hmax (n) + w 1 1

hmax (n) = ((ln n)c1 ) w sufficiently large n, first element previous
max becomes larger second one, order equivalent one
statement.
derived loss bound SOO algorithm Assumption A3 case
6= 0 well. SOO version loss bound rn O(n1/d ( d/D 2d/D )1/d ),
equivalent loss bound LOGO algorithm w = 1 Corollary 2.
Figure 3, thereby illustrate effect w loss bound form.
figure, plotted ratio elements inside loss bounds. Figure 2
167

fiKawaguchi, Maruyama, & Zheng



1

1

1

0.8

0.8

0.8

0.6



0.6



0.6

0.4

0.4

0.4

0.2

0.2

0.2

1

2

3

4

5

w
(a) = 0.01

1

2

3

w
(b) = 0.5

4

5

4

1

2

3

4

5

w
(c) = 1.0

Figure 3: Effect local bias w loss bound case 6= 0 := (w2 ( wd/D
w 1
)1 )1/d /( d/D 2d/D )1/d
2wd/D ( 1 1
Figure 3, infer loss bound improved w > 1 large
small (when n sufficiently large). Intuitively, makes sense, since
different yet similar sizes hyperintervals w.r.t. ` larger smaller.
case, dividing hyperintervals marginally different sizes would redundant
waste computational resources. Note discussion limited loss
bound now, may tightened future work. would see
different effects w tightened bounds.
4.2 Basis Practical Usage
section, derive loss bound LOGO algorithm concrete division
strategy presented Section 3.1. purpose section analyze LOGO
algorithm division process parameter settings actually used
rest paper. results section directly applicable experiments.
section, discard Assumptions A1, A2, A3. consider following assumption
present loss bound concrete form.
Assumption B1. exists semi-metric ` satisfies Assumption 1
following conditions hold:
exist b > 0, > 0 p 1 x, 0 , `(x, y) = bkx ykp
exist (0, 1) x 0 , f (x ) f (x) + ` (x, x ).
First, state loss bound algorithm practical division process
parameter settings decreases stretched exponential rate.
Theorem 2 (worst-case analysis). Let ` semi-metric Assumptions 1 B1
satisfied. loss LOGO algorithm bounded
!

w


w

1 1
w
1
0
rn c exp min
n 0
2, w n w
ln
w C 1 1

168

fiGlobal Continuous Optimization Error Bound Fast Convergence

= 3 c = b3 D/p . Here, w0 = 1 set parameter hmax (n) =

hand, w0 = w set parameter hmax (n) = w n w.



nw.

Proof. Assumption B1 division strategy,
supxh,i `(xh,i , ch,i ) b(3bh/Dc D1/p ) = bD/p 3bh/Dc
corresponds diagonal length rectangle, 3bh/Dc corresponds
length longest side. quantity upper bounded 3h/D+ . Thus,
consider case (h) = b3 D/p 3h/D , satisfies Assumption A1. Also,
Assumption A3 satisfied (h) = 3 c = b3 D/p .
Every rectangle contains least `-ball radius corresponding length
shortest side rectangle. Consequently, least `-ball radius (h) =
b3 3h/D rectangle = 32 D/p , satisfies Assumption A2.
Assumption B1, volume V `-ball radius (h) proportional ((h))D
following: VDp ((h)) = (2(h)(1 + 1/p))D /(1 + D/p). Therefore, Assumption A3
satisfied `-ball ratio h (w). addition, (h)-optimal set X(h) covered
`-ball radius (h) Assumption B1, thereby contains ((h)/(h))D =
disjoint `-balls radius (h). Hence, number `-balls depend (h),
means = 0.
satisfied Assumptions A1, A2, A3 = 3 , c = b3 D/p ,
= 0, obtain statement following proof Corollary 1.
Regarding effect local orientation w, Theorem 2 presents worst-case analysis.
Recall w introduced paper restore practicality global optimization
methods. Thus, focusing worst case likely pessimistic. mitigate
problem, present following optimistic analysis.
Theorem 3 (best-case analysis terms w). Let ` semi-metric Assumptions 1 B1 satisfied. 1 l w, let h+l1,i0 hyperinterval
may dominate intervals set h algorithms execution. Assume
h+l1,i0 h (1) . Then, loss LOGO algorithm bounded




1
w
1
0
rn c exp min
n 0 2, w n w
ln
wC


= 3 c = b3 D/p . Here, w0 = 1 set parameter hmax (n) = nw.

hand, w0 = w set parameter hmax (n) = w n w.
Proof. statement Lemma 2 modified

K

w1
X
hmax (n) + w X


n
|kw (1) | +
|kw (1) | .
w
k=0

l=1

statement Theorem 1 modified

bh/wc
hmax (n) + w X
nC
(w(kw)d ),
w
k=0


rn min(wbh(n)/wc w, wbhmax (n)/wc) .
169

fiKawaguchi, Maruyama, & Zheng

25
25

4

5

1

w
(a) Pessimistic w2 ( 1 1)/( w 1)

5

3

1

2

25

0.5

2.5
1

20

0.5

20

1

15

1

15

1.5

10

1.5

10

2

10

2

5

2.5

5

2.5

1

3
1

3

2

w

3

4

5

(b) Optimistic w2

Figure 4: Effect local bias w loss bound practical setting. real effect would
exist somewhere in-between.
Then, follow proof Theorem 2 Corollary 1, obtaining


n
1
h(n)

1.
w
C hmax (n) + w
hmax (n) =
theorem.



nw, modified statement Theorem 1, obtain statement

Theorem 3 makes strong assumption eliminate negative effect local
orientation bound, increasing w always improves loss bound theorem
n sufficiently large. may seem overly optimistic, show instance
case experiment.
realistically, effect w large n would exist somewhere left
right diagrams Figure 4. previous figures, (red) bold line
label 1 w effect bound, area labels greater one
w improves bound, area labels less one w diminishes
bound. left diagram shows effect w worst case Theorem 2 plotting
w2 ( 1 1)/( w 1) = 3 . reason plotting w2 ( 1 1)/( w 1)
represents worst case discussed previous section. right diagram presents
effect w best case Theorem 2 Theorem 3 simply plotting w2 . Notice
Theorem 2 Theorem 3, best scenario effect w use

hmax (n) = w n w second element min dominates bound. case,

coefficient n w2 , effect w bound n large enough
ignore term.
conclusion, showed LOGO algorithm provides stretched exponential
bound loss algorithms division strategy, likely practical
one used analysis SOO algorithm, parameter setting


hmax (n) = n w hmax (n) = w n w. also discussed local bias w
170

fiGlobal Continuous Optimization Error Bound Fast Convergence

affects loss bound. Based results, use LOGO algorithm following
experiments.

5. Experimental Results
section, test LOGO algorithm series experiments. main
part experiments, compared LOGO algorithm direct predecessor,
SOO algorithm (Munos, 2011) latest powerful variant, Bayesian Multi-Scale
Optimistic Optimization (BaMSOO) algorithm (Wang, Shakibi, Jin, & de Freitas, 2014).
BaMSOO algorithm combines SOO algorithm Gaussian Process (GP)
leverage GPs estimation upper confidence bound. shown outperform
traditional Bayesian Optimization method uses GP DIRECT algorithm
(Wang et al., 2014). Accordingly, omitted comparison traditional Bayesian
Optimization method. also compare LOGO popular heuristics, simulated annealing
(SA) genetic algorithm (GA) (see Russell & Norvig, 2009 brief introduction).
experiments, rescaled domains [0, 1]D hypercube. used
division process SOO, BaMSOO LOGO, one presented Section 3.2
proven provide stretched exponential bounds loss Section 4.2. Previous
algorithms also used division process experiments (Jones et al., 1993;
Gablonsky, 2001; Munos, 2013; Wang et al., 2014). SOO LOGO algorithms,

set hmax (n) = w n w. setting guarantees stretched exponential bound LOGO,
proven Section 4.2, SOO (Munos, 2013). LOGO algorithm, used
simple adaptive procedure set parameter w. Let f (x+
) best value found
thus far end iteration i. Let W = {3, 4, 5, 6, 8, 30}. algorithm begins
w = W1 = 3. end iteration i, algorithm set w = Wk k = min(j + 1, 6)
+
f (x+
) f (xi1 ), k = max(j 1, 1) otherwise, Wj previous parameter
value w adjustment occurs. Intuitively, adaptive procedure encourage
algorithm locally biased seems making progress, forcing explore
global region seem case. Although values
set W = {3, 4, 5, 6, 8, 30} arbitrary, simple setting used experiments
paper, including real-world application Section 6.4. results demonstrate
robustness setting. discussed later, future work would replace
simple adaptive mechanism improve performance proposed algorithm.
BaMSOO algorithm, previous work Wang et al. (2014) used pair good kernel
hyperparameters handpicked test function. experiments,
assumed handpicking procedure unavailable, typically case
practice. tested several pairs kernel hyperparameters; however, none
pairs performed robustly well test functions (e.g., one pair performed well
one test function, although others). Thus, used empirical Bayes method
3
adaptively update hyperparameters
p . selected isotropic Matern kernel
0
= 5/2, given (x, x ) = g( 5kx x0 k2 /l), function g defined
g(z) = 2 (1 + z + z 3 /3). hyperparameters initialized = 1 l = 0.25.
updated hyperparameters every iteration 1,000 function evaluations executed
3. implemented BaMSOO use empirical Bayes method, done
original implementation. original implementation BaMSOO available us well.

171

fiKawaguchi, Maruyama, & Zheng

f



SOO


N

Time (s)

BaMSOO
Error

N

Time (s)

LOGO
Error

N

Time (s)

Error

Sin 1

1

[0, 1]

57

5.3 E02 2.3 E06

30

2.0 E+00 2.3 E06 17 3.9 E02 2.3 E06

Sin 2

2

[0, 1]2

271

1.7 E01 4.6 E06

181

7.5 E+00 4.6 E06 45 5.4 E02 4.6 E06

Peaks

2

[3, 3]2

141

1.0 E01 9.0 E05

37

3.5 E+00 9.0 E05 35 6.1 E02 9.0 E05

Branin

2 [5, 10] [0, 15]

339

2.1 E01 9.0 E05

121

8.1 E+00 9.0 E05 85 7.0 E02 8.7 E05

Rosenbrock 2

2

[5, 10]2

491

3.1 E01 9.7 E06 >4000 5.8 E+04 5.5 E03 137 1.3 E01 9.7 E06

Hartman 3

3

[0, 1]3

359

2.3 E01 7.91 E05

126

8.9 E+00 7.9 E05 65 7.1 E02 5.1 E05

Shekel 5

4

[0, 10]4

1101 6.6 E01 8.4 E05

316

3.1 E+01 8.4 E05 157 1.2 E01 8.4 E05

Shekel 7

4

[0, 10]4

1117 7.1 E01 9.4 E05

95

1.2 E+01 9.4 E05 157 1.2 E01 9.4 E05

Shekel 10

4

[0, 10]4

1117 6.4 E0.1 9.68 E05 >4000 4.5 E+04 8.1 E+00 197 1.5 E01 9.7 E05

Hartman 6

6

[0, 1]6

1759 1.2 E+00 7.51 E05 >4000 4.0 E+04 2.3 E03 161 1.3 E01 6.8 E05

[5, 10]10

>8000 7.8 E+00 3.83 E03 >8000 5.8 E+04 9.6 E+00 1793 1.7 E+00 4.8 E05

Rosenbrock 10 10

Table 1: Performance comparison terms number evaluations (N ) CPU time
(Time) achieve Error < 104 . grayed cells indicate experiments could
achieve Error < 104 even large number function evaluations (4000 8000).
per 1,000 iterations afterward (to reduce computational cost). SA GA,
used settings Matlab standard subroutines simulannealbnd
ga, except specified domain bounds.
Table 1 shows results comparison 11 test functions terms
number evaluations CPU time achieve small error. first two test functions,
Sin 1 Sin 2, used test SOO algorithm (Munos, 2013), form
f (x) = (sin(13x) sin(27x) + 1)/2 f (x1 , x2 ) = f (x1 )f (x2 ) respectively. form
third function, Peaks, given Equation (16) illustrated Figure 2 McDonald,
Grantham, Tabor, Murphys paper (2007). rest test functions common
benchmarks global optimization literature; Surjanovic Bingham present detailed
information functions (2013). table, Time (s) indicates CPU time
second Error defined
(
|(f (x ) f (x+ ))/f (x )| f (x ) 6= 0,
Error =
|f (x ) f (x+ )|
otherwise.
table, N = 2n number function evaluations needed achieve Error < 104 ,
n total number divisions one used main measure analyses previous sections. Here, N equal 2n adopted division process.
Thus, lower value N becomes, better algorithms performance is.
continued iterations 4000 function evaluations functions dimensionality
less 10, 8000 function dimensionality equal 10.
seen Table 1, LOGO algorithm outperformed algorithms.
superior performance LOGO algorithm small number function evaluations attributable focusing promising area discovered search.
Conversely, SOO algorithm continues search global domain tends similar uniform grid search. BaMSOO algorithm also follows tendency toward
172

fiGlobal Continuous Optimization Error Bound Fast Convergence

grid search based SOO algorithm. BaMSOO algorithm chooses
divide based SOO algorithm; however, omits function evaluations upper
coincidence bound estimated GP indicates evaluation likely
beneficial. Although mechanism BaMSOO algorithm seems beneficial
reduce number function evaluations cases, two serious disadvantages.
first disadvantage computational time due use GP. Notice requires
O(N 3 ) every time re-compute upper confidence bound.4 serious disadvantage possibility determining solution all. Table 1, see
BaMSOO improves performance SOO 7/11 cases; however, severely degrades
performance 4/11 cases. Moreover, may BaMSOO reduce performance
also may guarantee convergence even limit practice. BaMSOO
algorithm reduces number function evaluations relying estimation
upper confidence bound. However, estimation wrong, wrong, may
never explore region global optimizer exists. Notice limitations
unique BaMSOO also apply many GP-based optimization methods.
terms first limitation (computational time), BaMSOO significant improvement
compared traditional GP-based optimization methods (Wang et al., 2014).
Although LOGO algorithm bias toward local search, maintains strong
theoretical guarantee, similar SOO algorithm, proven previous sections.
terms theoretical guarantee, SOO algorithm LOGO algorithm share
similar rate loss bound base analyses set assumptions
hold practice. hand, BaMSOO algorithm worse rate loss
bound (an asymptotic loss order n(1)/d ) bound applies restricted
class metric ` (the Euclidean norm power = {1, 2}). also requires several
additional assumptions guarantee bound. additional assumptions would
impractical, particularly assumption objective function always wellcaptured GP chosen kernel hyperparameters. discussed above,
assumption would cause BaMSOO lose loss bound also consistency
guarantee (i.e., convergence limit) practice.
Figure 5 presents performance comparison number function evaluations
Figure 6 plots corresponding computational time. figures, lower plotted
value along vertical axis indicates improved algorithm performance. SA GA,
figure shows mean 10 runs. report mean standard deviation
time following. SA, 1.19 (Sin 1), 1.32 (Sin 2), 0.854 (Peaks), 0.077
(Branin), 1.06 (Rosenbrock 2), 0.956 (Hartman 3), 0.412 (Shekel 5), 0.721 (Shekel 7), 1.38
(Shekel 10), 0.520 (Hartman 6), 0.489 (Rosenbrock 10). GA, 0.921 (Sin 1),
0.399 (Sin 2), 0.526 (Peaks), 0.045 (Branin), 1.27 (Rosenbrock 2), 0.493 (Hartman 3), 0.216
(Shekel 5), 0.242 (Shekel 7), 1.19 (Shekel 10), 0.994 (Hartman 6), 0.181 (Rosenbrock
10).
illustrated Figure 5, LOGO algorithm generally delivered improved performance compared algorithms. particularly impressive result LOGO
algorithm robustness challenging functions, Shekel 10 Rosenbrok 10.
4. Although several methods mitigate computational burden approximation, effect
approximation performance BaMSOO algorithm unknown left future
work.

173

fiKawaguchi, Maruyama, & Zheng

0

4
6

SA
GA
SOO
BaMSOO
LOGO

8
10
12
14
16

1

2
4
6

SA
GA
SOO
BaMSOO
LOGO

8
10
12
14
16

1

N

10

1

10

7
9

N

100

N

5

SA
GA
SOO
BaMSOO
LOGO

15
20
25
1

10

100

N

5
6
1

10

100

N

3

SA
GA
SOO
BaMSOO
LOGO

5
7
9
11
13

LogDistancetoOptima

6
4
2
0
2
4
6
8
10
12

3

SA
GA
SOO
BaMSOO
LOGO

7
9

11
13
1

10

N

100

(j) Hartman 6

7
9

1000

10

N

100

1000

0
2

SA
GA
SOO
BaMSOO
LOGO

4
6
8
10
12

1

10

100

N

1000

(h) Shekel 7

1

5

5

2

(g) Shekel 5
1

1000

(f) Hartman 3

1

1000

100

SA
GA
SOO
BaMSOO
LOGO

3

1

15

7

N

1

1000

LogDistancetoOptima

LogDistancetoOptima

SA
GA
SOO
BaMSOO
LOGO

10

11

1

4

13

(e) Rosenbrock 2

0

3

11

1

10

1000

1

2

9

(c) Peaks

0

(d) Branin

1

SA
GA
SOO
BaMSOO
LOGO

7

1

30

11
10

5

1000

LogDistancetoOptima

LogDistancetoOptima

LogDistancetoOptima

SA
GA
SOO
BaMSOO
LOGO
1

LogDistancetoOptima

100

5

1

5

3

(b) Sin 2

1
3

1

15

100

(a) Sin 1

LogDistancetoOptima

LogDistancetoOptima

LogDistancetoOptima

LogDistancetoOptima

0
2

1

10

N

100

1000

(i) Shekel 10

SA
GA
SOO
BaMSOO
LOGO
1

10

N

100

1000

(k) Rosenbrock 10

Figure 5: Performance comparison: number evaluations N vs. log error computed
log10 |f (x ) f (x+ )|. f (x ) indicates true optimal value objective function
f (x+ ) best value determined algorithm.

function Shekel local optimizers slope surface generally becomes
larger increases. Therefore, Shekel 10 Rosenbrok 10, 10-dimensionality,
generally difficult functions compared others experiment.
Indeed, LOGO algorithm achieved acceptable performance these. Figure 6, see LOGO algorithm SOO algorithm fast. LOGO
algorithm often marginally slower SOO algorithm owing additional
computation required maintain supersets. reason BaMSOO algorithm
required large computational cost horizontal axis points continued skipping conduct function evaluations (because evaluations judged
174

fiGlobal Continuous Optimization Error Bound Fast Convergence

1000

1000

100

CPUtime(s)

1
0.1

0.01

SOO
SA

0.001
0

20

BaMSOO
GA
40

60

N

CPUtime(s)

100
10
1
0.1

SA
BaMSOO

0.01

LOGO

0.001

80

0

100

200

(a) Sin 1

400

600

N

SOO

1
0.1

800

0

1000

0.01
0.001
0

200

SA
SOO
LOGO
400 N 600

GA
BaMSOO

100
10

GA
BaMSOO

1

1000

N

2000

3000

4000

0

1000

SA
SOO
LOGO

0.001
0

200

400

600

N

GA
BaMSOO

1
0.1

SA
SOO
LOGO

0.01
0.001

800

(g) Shekel 5

200

400

600

N

800

1000

(h) Shekel 7

400

600

N

800

1000

SA
SOO
LOGO

100
10

GA
BaMSOO

1
0.1
0

1000

N

2000

3000

4000

(i) Shekel 10

100000

100000

10000

1000

SA
SOO
LOGO

100
10

GA
BaMSOO

1
0.1

1000

CPUtime(s)

10000

CPUtime(s)

200

GA
BaMSOO

0.01

0

1000

GA
BaMSOO

CPUtime(s)

10000

10

CPUtime(s)

100000

100

CPUtime(s)

1000

100

1

1000

(f) Hartman 3

1000

0.1

SA
SOO
LOGO

(e) Rosenbrock 2

10

800

1
0.1

0.001

0

1000

600

N

0.01

0.01

800

400

10

0.1

(d) Branin

0.01

200

100

SA
SOO
LOGO

1000

CPUtime(s)

1

SOO

1000

10000

10

GA
LOGO

(c) Peaks

100000

100

0.1

SA
BaMSOO

0.01
0.001

(b) Sin 2

1000

CPUtime(s)

GA
LOGO

10

CPUtime(s)

CPUtime(s)

10

SA
SOO
LOGO

100
10

GA
BaMSOO

1
0.1

0.01

0.01
0

1000

N 2000

(j) Hartman 6

3000

4000

0

2000

N 4000

6000

8000

(k) Rosenbrock 10

Figure 6: CPU time comparison: CPU time required achieve performance indicated
Figure 5

beneficial based GP). effective mechanism BaMSOO avoid wasteful function evaluations; however, one must careful make sure function evaluations
costly, relative mechanism.
summary, compared BaMSOO algorithm, LOGO algorithm faster
considerably simpler (in implementation parameter selection) stronger
theoretical bases delivering superior performance experiments. compared
SOO algorithm, LOGO algorithm decreased theoretical convergence rate
worst case analysis, exhibited significant improvements experiments.
confirmed advantages LOGO algorithm, discuss possible limitations: scalability parameter sensitivity. scalability high dimensions
175

fiKawaguchi, Maruyama, & Zheng

Log Distance Optimal

LogDistancetoOptima

0
1
2
3

SA
GA
SOO
BaMSOO
REMBOLOGO

4
5
6
7
8

1
-1
-3
-5

w=1
w=2
w = 20
adaptive w

-7
-9

-11

1

10

N

100

1000

1

10

N

100

1000

(b) Sensitivity local bias parameter w

(a) Scalability: 1000-dimensional function

Figure 7: current possible limitations LOGO
challenge non-convex optimization general search space grows exponentially
space. However, may achieve scalability leveraging additional structures
objective function present applications. example, Kawaguchi (2016b)
showed instance deep learning models, objective function
additional structure: nonexistence poor local minima. illustration, combine
LOGO random embedding method, REMBO (Wang, Zoghi, Hutter, Matheson, &
De Freitas, 2013), account another structure: low effective dimensionality. Figure 7 (a), report algorithms performances 1000 dimensional function: Sin 2
embedded 1000 dimensions manner described Section 4.1 previous
study (Wang et al., 2013).
Another possible limitation LOGO sensitivity performance free
parameter w. Even though provided theoretical analysis insight effect
parameter value previous section, yet unclear set w principle
manner. illustrate current limitation Figure 7 (b). result labeled
adaptive w indicates result fixed adaptive mechanisms w use
experiments except ones Figure 7 (b) 8. illustration, use
Branin function experiment conducted clearly illustrated limitation.
seen figure, performance early stage always improved w
increases algorithm finds local optimum faster higher w. However, w
large, w = 20 figure, algorithm gets stuck local optimum
long time. Thus, best value (or sweet spot) exists large small
values w. results experiment, seen choice w = 2
best, finds global optima high precision within 200 function evaluations.
However, limitation would serious problem practice following four
reasons. First, similar limitation exists, best knowledge, algorithms
successfully used practice (e.g., simulated annealing, genetic algorithm, swarmbased optimization, DIRECT algorithm, Bayesian optimization). Second, unlike
previous algorithm, finite-time loss bound always applies even bad choice
w. Third, demonstrated previous experiments simple adaptive
rule may suffice produce good result. Also, future work may mitigate
limitation developing different methods adaptively determine value w. Also,
176

fiGlobal Continuous Optimization Error Bound Fast Convergence

another possibility would conduct optimization w cheaper surrogate model.
Finally, limitation may apply target objective functions all.
fourth final reason, recall speculated algorithms analysis
increasing w would always beneficial effects problems, illustrated
Figure 4. Clearly, problems within scope local optimization fall category.
Figure 8, show rather unobvious instance problems, thus example,
limitation parameter sensitivity apply. seen
diagram left Figure 8, test function many local optima, one
global optimum. Nevertheless, diagram right, performance
LOGO algorithm improves w increases, harmful effect.

Log Distance Optimal

0

f

x1

-2

N = 50

-4

N = 100

-6
-8
-10
-12
-14
-16

x2

1

10

100

Local Bias Parameter: w

Figure 8: example problems increasing w always better. diagram
left shows objective function, diagram right presents performance
N = 50 100 w.

6. Planning LOGO via Policy Search
apply LOGO algorithm planning, important area field
AI. goal planning problem find action sequence maximizes
total return infinite discounted horizon finite horizon (unlike classical planning
problem, consider constraints specify goal state). paper, discuss
formulations case infinite discounted horizon, arguments
applicable case finite horizon straightforward modifications. consider
case state/action space continuous, planning horizon long,
transition reward functions known deterministic.
planning problem formulated follows. Let RDS set states,
RDA set actions, : RDS RDS transition function, R : RDS RDA R
return reward function, 1 discount factor. planner considers
take action state S, triggers transition another state based
transition function , receiving return based reward function R.
discount factor discounts future rewards fulfill either following
two roles: accounting relative importance immediate rewards compared future
177

fiKawaguchi, Maruyama, & Zheng

rewards, obviating need think ahead toward infinite horizon. action
sequence represented policy maps state space action space:
: RDS RDA .
value action sequence policy , V , sum rewards
infinite discounted horizon,


V (s0 ) =


X

R(sj , (sj )).

j=0

value policy also written recursive form

V (s) = R(s, (s)) + V (s, (s)) .

(2)

Here, interested finding optimal policy . dynamic programing approach, compute optimal policy, solving following Bellmans optimality
equation:
V (s) = max R(s, a) + V (T (s, a))
(3)




V value optimal policy. Equation (3), optimal policy set
actions defined max. major problem approach efficiency
computation depends size state space. real-world application,
state space usually large continuous, often makes impractical solve
Equation (3).
successful approach avoid state size dependency focus state
space reachable current state within planning time horizon.
way, even infinitely large state space, planner needs consider finitely
sized subset space. approach called local planning. Unlike local optimization
vs. global optimization, optimal solution local planning indeed globally optimal,
given initial state. called local planning cover states
solution changes different initial states. Accordingly, initial state changes,
planner may need conduct re-planning.
natural way solve local planning use tree search methods, construct
tree rooted initial state toward future possible states depth planning
horizon. tree search conducted using traditional search method, including
uninformed search (e.g., breadth-first depth-first search) informed (heuristic)
search (e.g., search). Also, recent studies developed several tree-based algorithms
specialized local planning. Among those, SOO algorithm, direct predecessor LOGO algorithm, applied local planning tree search approach
(Busoniu, Daniels, Munos, & Babuska, 2013). new algorithms, example,
HOLOP (Bubeck & Munos, 2010; Weinstein & Littman, 2012), operate stochastic
transition functions.
However efficient proposed algorithms are, search space tree search
approach grows exponentially planning time horizon, H. Therefore, local planning
tree search approach would work well long time horizon.
applications, small H justified, applications, not. application
problem requires long time tradeoff immediate future rewards, tree
178

fiGlobal Continuous Optimization Error Bound Fast Convergence

search approach would impractical. Here, motivated solve real-world
application, therefore need another approach.
paper, consider policy search (Deisenroth, Neumann, & Peters, 2013)
effective alternative solve planning problem continuous state/action space
long time horizon. Policy search form local planning. Thus, like tree search
approach, operates even infinitely large continuous state space. addition, unlike
tree search approach, policy search significantly reduces search space naturally
integrating domain-specific expert knowledge structure policy.
concretely, search space policy search set policies {x : x },
parameterized vector x RD . Therefore, search space longer dependent
planning time horizon H, state space S, action space A,
parameter space . Here, parameter space determined expert knowledge,
significantly reduce search space.
use regret rm measure policy search algorithms performance:
+



rm = V x (s0 ) V x (m) (s0 )
x optimal policy given set policies {x : x }, x+ (m)
best policy found algorithm steps planning. evaluation
policy takes mH steps consider fixed planning horizon H. Here, x may differ
optimal policy covered set {x : x }.
policy search approach usually adopted gradient methods (Baxter & Bartlett,
2001; Kober, Bagnell, & Peters, 2013; Weinstein, 2014). gradient method fast,
converges local optima (Sutton, McAllester, Singh, & Mansour, 1999). Further,
observed may result mere random walk large plateaus exist
surface policy space (Heidrich-Meisner & Igel, 2008). Clearly, problems
resolved using global optimization methods cost scalability (Brochu et al.,
2009; Azar, Lazaric, & Brunskill, 2014). Unlike previous policy search methods, method
guarantees finite-time regret bounds w.r.t. global optima {x : x } without strong
additional assumption, provides practically useful convergence speed.
6.1 LOGO-OP Algorithm: Leverage (Unknown) Smoothness Policy
Space Planning Horizon
section, present simple modification LOGO algorithm leverage
unknown smoothness policy space also known smoothness planning
horizon. former accomplished direct application LOGO algorithm
policy search, latter modification section aims without
losing advantage original LOGO algorithm. call modified version, Locally
Oriented Global Optimization Optimism Planning horizon (LOGO-OP). result
modification, add new free parameter L.
pseudocode LOGO-OP algorithm provided Algorithm 2. comparing
Algorithms 1 2, seen LOGO-OP algorithm functions
manner LOGO algorithm, except line 15 (the function evaluation or, equivalently,
policy evaluation policy search) line 20. Notice LOGO algorithm
directly applicable policy search considering V f Algorithm 1.
179

fiKawaguchi, Maruyama, & Zheng

LOGO algorithm assume structure function f , LOGO-OP algorithm
functions exploits given structure value function V (i.e., MDP model).
algorithm functions follows. policy evaluation performed policy
x parameter x specified two new hyperrectangles (from line 15-1
15-11). Given initial condition s0 S, transition function , reward function R,
discount factor 1, policy x , algorithm computes value policy
Equation (2) (from line 15-2 line 15-10, except line 15-6).
main modification appears line 15-6 algorithm leverages known
smoothness planning horizon. Remember unknown smoothness policy
space (or input space x) specified f (x )f (x) `(x, x ) (from Assumption 1) thus
infers upper bound value policy yet evaluated similar (close
policy space w.r.t. `) already evaluated polices. Conversely, known smoothness
planning horizon renders upper bound value policy particular
policy evaluated. is, known smoothness planning horizon
written


X
X
t+1
j R(sj , x (sj ))
j R(sj , x (sj ))
Rmax
1
j=0

j=0

0 arbitrary point planning horizon line 15-3 Rmax
maximum reward. known smoothness due definition Rmax sum
geometric series. case finite horizon H, formula
( /(1 ))Rmax replaced (H t)Rmax . line 15-6, unlike original
LOGO algorithm, LOGO-OP algorithm terminates evaluation policy
continuation evaluating policy judged misuse computational resources
based known smoothness planning horizon. Concretely, terminates
evaluation policy upper bound value policy becomes less
(V + L), V + value best policy found thus far L algorithms
parameter.
upper bound value policy becomes less V + , planner
know policy best policy. Thus, tempting simply terminate
policy evaluation criterion. However, essence LOGO algorithm
utilization unknown smoothness embedded surface value function
policy space. words, algorithm makes use result policy evaluation,
whether policy best one not. interruption policy evaluation changes
shape surface value function, interferes mechanism
LOGO algorithm. Nevertheless, degree interruption likely beneficial
since goal find optimal policy instead surface analysis.
LOGO-OP algorithm uses L determine degree interruption.
+
V monotonically increasing along execution, value policy fully
evaluated owing line 15-6 early iterations tends greater value policy
fully evaluated later iterations. algorithm resolves problem
line 20 biased divide interval evaluated early iteration.
smaller L, LOGO-OP algorithm stop evaluation non-optimal
policy earlier, cost accuracy evaluation value functions surface.
larger L, algorithm needs spend time evaluation non-optimal policy,
180

fiGlobal Continuous Optimization Error Bound Fast Convergence

Algorithm 2: LOGO-OP algorithm
Inputs (problem): initial condition s0 S, transition function , reward
function R, discount factor 1 convergence criteria (or finite horizon H),
policy space x : x RD .
1: Inputs (parameter): search depth function hmax : Z+ [1, ), local weight
w Z+ , stopping condition, maximum reward Rmax , parameter L.
25: lines 25 exactly lines 25 Algorithm 1
6: Adds initial hyperrectangle 0 set: 0 0 {0 } (i.e., 0,0 = 0 )
7: Evaluate value function V center point 0 , c0,0 : val [0,0 ] V (c0,0 ),
V + val [0,0 ]
8: iteration = 1, 2, 3, . . .
9: val max , hplus hupper
10: k = 0, 1, 2, . . . , max(bmin(hmax (n), hupper )/wc, hplus )
11:
Select hyperrectangle divided: (h, i) arg maxh,i val [h,i ] h, : h,i k
12:
val [h,i ] > val max
13:
val max val [h,i ], hplus 0, hupper max(hupper , h + 1), n n + 1
14:
Divide hyperrectangle h,i along longest coordinate direction
- three smaller hyperrectangles created left , center , right
- val [center ] val [h,i ]
15:
Evaluate value function V center points two new hyperrectangles:
151:
policy x corresponding cleft cright
152:
z1 0, z2 1, s0
153:
= 0, 1, 2, . . . ,
154:
z1 z1 + z2 R(s, x (s))
155:
z2 z2 , (s, x (s)) ,
156:
z1 + ( t+1 /(1 ))Rmax < (V + L) Exit loop
157:
convergence criteria met Exit loop
158:
end
159:
save z1 value corresponding rectangle
1510:
val [left ] z1 val [right ] z1
1511:
end
1512:
V + max(V + , val [left ], val [center ], val [right ])
16:
Group new hyperrectangles set h+1 remove original rectangle:
h+1 h+1 {center , left , right }, h h \ h,i
17:
end
18:
stopping condition met Return (h, i) = arg maxh,i val [h,i ]
19: end
20: intervals val [] < (V + L) val [] (V + L)
21: end
0:

obtain accurate estimate value functions surface. regret
analysis, show certain choice L ensures tighter regret bound compared
direct application LOGO algorithm.
181

fiKawaguchi, Maruyama, & Zheng

6.2 Parallel Version LOGO-OP Algorithm
LOGO-OP algorithm presented Algorithm 2 four main procedures: Select (line
11), Divide (line 14), Evaluate (line 15), Group (line 16). natural way parallelize algorithm decouple Select three procedures. is, let
algorithm first Select z hyperrectangles divided, allocate z number
Divide, Evaluate, Group z parallel workers. However, natural parallelization
data dependency one Select another Select. words, procedure
next Select cannot start Divide, Evaluate, Group previous Select
finalized. result, parallel overhead tends non-negligible. addition,
Select chooses less hyperrectangles parallel workers, available resources
parallel workers wasted. Indeed, latter problem tackled creating multiple
initial rectangles recent parallelization study DIRECT algorithm (He, Verstak,
Sosonkina, & Watson, 2009). use multiple initial rectangles certainly
mitigate problem, still allows occasional occurrence resource wastage,
addition requiring user specify arrangement initial rectangles.
solve problems, instead decouple Evaluate procedure
three procedures allocate Evaluate task parallel worker. call
parallel version, pLOGO-OP algorithm. algorithm uses one master process
conduct Select, Divide, Group operations arbitrary number parallel workers
execute Evaluate. main idea temporarily use artificial value assignment
center point hyperrectangle master process, overwritten
true value parallel worker finishes evaluating center point. strategy,
data dependency parallel workers occupied tasks almost
time. paper, use center value original hyperrectangle
division temporary artificial value, artificial value may computed using
advanced method (e.g., methods surface analysis) future work. center
point initial hyperrectangle, simply assign worst possible value (if
knowledge regarding worst value, use ).
master process keeps selecting new hyperrectangles unless parallel workers
occupied tasks. logic ensures parallel workers always tasks
assigned master process, master process select many hyperrectangles based artificial information. Note parallelization makes sense
Evaluate time consuming procedure, likely true policy
evaluation.
6.3 Regret Analysis
certain condition, finite-loss bounds LOGO algorithm directly
translated regret bound LOGO-OP algorithm. condition must
met (V + L) less center value optimal hyperinterval
algorithms execution. state regret bound concretely below. simplicity,
use notion planning horizon H, effective (non-negligible) planning
horizon LOGO accordance discount factor, . Let H 0 effective
planning horizon LOGO-OP algorithm. Then, planning horizon LOGO-OP,
H 0 , becomes smaller LOGO, H, algorithm finds improved function
182

fiGlobal Continuous Optimization Error Bound Fast Convergence

values. LOGO-OP algorithm terminates policy evaluation line
15-6 upper bound policy value determined lower (V + L).
Corollary 3. Let H 0 H planning horizon used LOGO-OP algorithm
policy evaluation. Let V + value best policy found algorithm
iteration. Assume value function policy satisfies Assumptions 1 B1.
(V + L) maintained less center value optimal hyperinterval,
algorithm holds finite-time loss bound Theorem 2



n
.
2H 0
Proof. policy search special case optimization problem, trivial
loss bound Theorem 2 holds LOGO algorithm applied policy
search. every function evaluation takes H steps planning horizon,
n bm/2Hc case. LOGO-OP algorithm, effect new parameter
L loss analysis takes place proof Lemma 2. (V + L) maintained
less center value optimal interval, statements proof
hold true LOGO-OP algorithm well. Here, due effect L, function evaluation
may take less H steps planning horizon. Therefore, statement
corollary.
tighten regret bound LOGO-OP algorithm decreasing L, since
algorithm terminate evaluations unpromising policies earlier, means
value H 0 bound reduced. However, using small value L
violates condition Corollary 3 leads us discard theoretical guarantee. Even
case, small value L results global search,
consistency property, limn rn = 0, still trivially maintained. hand,
set L = , LOGO-OP algorithm becomes equivalent direct application
LOGO algorithm policy search, thus, regret bound Corollary 3
H 0 = H.
pLOGO-OP algorithm also maintains regret bound n = np
np counts number total divisions devoted set -optimal hyperinterval kw+l (l + 1) , (w 1) l 0. non-parallel versions ensure
devotion kw+l (l + 1) , parallelization makes possible conduct division
hyperintervals. Thus, considering worst case, pLOGO-OP may improve
bound proof procedure, although parallelization likely beneficial practice.
6.4 Application Study Nuclear Accident Management
management risk potentially hazardous complex systems, nuclear
power plants, major challenge modern society. section, apply proposed
method accident management nuclear power plants demonstrate potential
utility usage method real-world application. focus assessing
efficiency containment venting accident management measure obtaining
knowledge effective operational procedure (i.e., policy ). problem requires
planning continuous state space long planning horizon (H 86400),
183

fiKawaguchi, Maruyama, & Zheng

dynamic programming (e.g., value iteration), tree-based planning (e.g., search
variants) would work well (dynamic programming suffers curse
dimensionality state space, search space tree-based methods grows exponentially planning horizon).
Containment venting operation used maintain integrity containment vessel mitigate accident consequences releasing gases containment vessel atmosphere. accident Fukushima Daiichi nuclear power
plant 2011, containment venting activated essential accident management
measure. result, 2012, United States Nuclear Regulatory Commission (USNRC) issued order 31 nuclear power plants install containment vent system
(USNRC, 2013). Currently, many countries considering improvement containment venting system operational procedures (OECD/NEA/CSNI, 2014).
difficulty determining actual benefit effective operation comes fact
containment venting also releases fission products (radioactive materials) atmosphere. words, effective containment venting must trade future risk
containment failure immediate release fission products (radioactive materials). experiments, use release amount main fission product compound,
cesium iodide (CsI), measure effectiveness containment venting.
nuclear accident management literature, integrated physics simulator used
model world dynamics transition function state space S.
simulator adopt paper THALES2 (Thermal Hydraulics radionuclide
behavior Analysis Light water reactor Estimate Source terms severe accident
conditions) (Ishikawa, Muramatsu, & Sakamoto, 2002). Thus, transition function
state space fully specified THALES2. initial condition s0 designed
approximately simulate accident Fukushima Daiichi nuclear power plant.
experiment, focus single initial condition deterministic simulator,
relaxation discussed next section. reward function R negative
amount CsI released atmosphere result state-action pair.
use finite-time horizon H = 86400 seconds (24 hours), traditional first phase
time-window considered risk analysis nuclear power plant simulations (owing
assumption 24 hours, many highly uncertain human operations expected).
use following policy structure based engineering judgment.
(
1 ((FP x1 ) (Press x2 )) (Press > 100490),
x =
0 otherwise,
x = 1 indicates implementation containment venting, FP (g) represents
amount CsI gas phase suppression chamber, Press (kgf/m2 )
pressure suppression chamber. Here, suppression chamber volume
containment vessel connected atmosphere via containment venting
system. policy structure reflects engineering knowledge venting
done fission products exist certain amount suppression chamber, operated pressure gets larger specific value.
consider x1 = [0, 3000] x2 = [10810, 100490]. let x = 1 whenever pressure
exceeds 100490 kgf/m2 , since containment failure considered probably occur af184

fiCsI release Computed Policy (g)

Global Continuous Optimization Error Bound Fast Convergence

10000

1000

100
SOO
LOGO
LOGO-OP
pLOGO-OP

10

1
0

10000

20000

30000

40000

Wall time (s)

Figure 9: Performance computed policy (CsI release) vs. Wall Time.

ter pressure exceeds point. detail experimental setting outlined
Appendix A.
first compare performance various algorithms problem.
algorithms, used parameter settings benchmark tests Section 5.

is, used hmax (n) = w n w simple adaptive procedure parameter w
W = {3, 4, 5, 6, 8, 30}. LOGO-OP algorithm pLOGO-OP algorithm,
blindly set L = 1000 (i.e., likely better parameter setting L). used
eight parallel workers pLOGO-OP algorithm.
Figure 9 shows result comparison wall time 12 hours. vertical axis
total amount CsI released atmosphere (g), want minimize.
Since conducted containment venting whenever pressure exceeded 100490 kgf/m2 ,
containment failure prevented simulation experiments. Thus, lower
value along vertical axis gets, better algorithms performances is. seen,
new algorithms performed well compared SOO algorithm. also clear
two modified versions LOGO algorithm improved performance original.
LOGO-OP algorithm, effect L computational efficiency becomes greater
found best policy improves. Indeed, LOGO algorithm required 10798 seconds
ten policy evaluations 52329 seconds 48 evaluations. LOGO-OP algorithm
required 9297 seconds ten policy evaluations, 44678 seconds 48 evaluations.
data conjunction Figure 9 illustrates property LOGO-OP algorithm
policy evaluation becomes faster found best policy improves. pLOGOOP algorithm, number function evaluations performed algorithm increased
factor approximately eight (the number parallel workers) compared nonparallel versions. Notice parallel version tends allocate extra resources
global search (as opposed local search). focus local search
utilizing previous results policy evaluations; however, parallel version must
initiate several policy evaluations without waiting previous evaluations, resulting
tendency global search. tendency forced improvement, terms reducing
185

fiKawaguchi, Maruyama, & Zheng

(1)

(2)

(4)

(3)

(5)

(6)

Venting (-) / CsI (g)

Venting (-) / CsI (g)

1
Venting = 1: yes, 0:

0.8

CsI atmosphere

0.6
0.4
0.2
0
0

10000

20000

30000
40000
50000
60000
70000
Time along Accident Progression (s)

80000

Time along Accident Progression (s)

Figure 10: Action sequence generated found policy CsI release

Figure 10: Action sequence generated found policy CsI release
amount CsI, moderate relative number policy evaluations
particular experiment. However, tendency may positive effect different
problems increased global search beneficial. CPU time per policy evaluation
varied significantly different policies owing different phenomenon computed
simulator. average, LOGO-OP algorithm, took approximately 930 seconds
per policy evaluation.
partially confirmed validity pLOGO-OP algorithm, attempt
use provide meaningful information application field. Based examination results comparison, narrowed range parameter values
x1 = [0, 1.2] x2 = [10330, 10910]. computation CPU time 86400 (s)
eight workers parallelization, pLOGO-OP algorithm found policy
x1 0.195 (g) x2 10880 (kgf/m2 ). policy determined, containment
failure prevented total amount CsI released atmosphere limited
approximately 0.5 (g) (approximately 0.002% total CsI) 24 hours
initiation accident. major improvement scenario
experimental setting considered result containment failure best, large
amount CsI release, 2000 (g) (about 10% total CsI) setting.
computational cost CPU time 86400 (s) likely acceptable application field.
terms computational cost, must consider two factors: offline computation
variation scenarios. computational cost CPU time 86400 (s) phenomenon requires 86400 (s) acceptable online computation (i.e., determining
satisfactory policy accident progressing). However, computational cost
likely acceptable consider preparing acceptable policies various scenarios
offline manner (i.e., determining satisfactory polices accident). information
regarding polices utilized accident first identifying accident
scenario heuristics machine learning methods (Park & Ahn, 2010). offline preparation, must determine policies various major scenarios thus computation
takes, example, one month, may acceptable.
Note policy found method novel nontrivial literature,
yet worked well. Accordingly, explain policy performed well
did. Figure 10 shows action sequence generated policy found amount
186

fiGlobal Continuous Optimization Error Bound Fast Convergence

CsI (g) released versus accident progression time (s). analyze action sequence
dividing six phases, indicated Figure 10, six numbers inside
parentheses. first phase (1), venting conducted intermittently order
keep pressure around x2 10880 (kgf/m2 ). phase, fission product yet
released nuclear fuels. Reducing pressure heat done
preferably without releasing fission products, actions phase accomplish this.
One may wonder venting done intermittently, instead continuing
conduct venting reduce pressure much possible, done without
release fission products phase. reducing pressure much
leads large difference pressures suppression chamber reactor
pressure vessel, turn results large mass flow fission product transportation
reactor pressure vessel suppression chamber (see Figure 11 Appendix
information mass flow paths). increase amount fission products
suppression chamber likely result large release fission products
atmosphere venting conducted. Therefore, specific value x2 generates
intermittent venting works well first phase. second phase (2), containment
venting executed time since pressure suppression chamber increases
rapidly phase (due operation depressurizing reactor pressure vessel via
SRV line), thus, criterion (Press x2 ) policy satisfied time
point. beginning third phase (3), amount CsI suppression
chamber exceeds x1 0.195 (g) thereby venting conducted. fourth phase
(4), pressure reaches 100490 (kgf/m2 ) containment venting intermittently done
order keep pressure point avoid catastrophic containment failure.
fifth phase (5), containment vent kept open amount CsI gas
phase suppression chamber decreases x1 (due phenomenon illustrated
Figure 12 Appendix A). continuous containment venting decreases pressure
venting required terms pressure final phase (6), venting
conducted also amount CsI becomes larger x1 .
Thus, clear policy found AI-related method also basis
terms physical phenomenon. addition, generated action sequence likely
simple enough engineer discover several sensitivity analyses. particular,
method solve known tradeoff immediate CsI release
risk future containment failure, method also discovered existence new tradeoff
immediate reduction pressure without CsI release future increase
mass flow. Although consensus operate containment venting
system moment, tendency use pressure exceeds certain
point order prevent immediate sever damage containment vessel, corresponds
fourth phase (4) Figure 10. experiment, myopic operation
resulted containment failure, significantly large amount CsI released
atmosphere (at least 4800 (g)).
summary, successfully applied proposed method investigate containment
venting policy nuclear power plant accidents. preliminary application study, several
topics left future work. theoretical viewpoint, future work consider
way mitigate simulation bias due model error model uncertainty.
model error, robotics community already cognizant small error simulator
187

fiKawaguchi, Maruyama, & Zheng

result poor performance derived policy (i.e., simulation bias) (Kober et al.,
2013). mitigate problem adding small noise model, since
noise works regularization prevent over-fitting demonstrated Atkeson (1998).
model uncertainty, recent studies field nuclear accident analysis provide
possible directions treatment uncertainty accident phenomena (Zheng, Itoh,
Kawaguchi, Tamaki, & Maruyama, 2015) accident scenarios (Kawaguchi, Uchiyama, &
Muramatsu, 2012). result either countermeasures, objective
function becomes stochastic, thereby may first expand pLOGO-OP algorithm
stochastic case. hand, phenomenological point view, future work
consider fission products well CsI. fission products include,
limited to, Xe, Cs, I, Te, Sr, Ru. particular, noble gas element, Xe,
major concern accident (it tends released lot easily diffused
atmosphere), property different CsI (its half-life much smaller). Thus,
Xe identified major concern, one may consider significantly different policy
(considering half-life, one may delay conducting containment venting).

7. Conclusions
paper, proposed LOGO algorithm, global optimization algorithm
designed operate well practice maintaining finite-loss bound strong
additional assumption. analysis LOGO algorithm generalized previous finite-loss
bound analysis. Importantly, analysis also provided several insights regarding practical
usage type algorithm showing relationship among loss bound,
division strategy, algorithms parameters.
applied LOGO algorithm AI planning problem policy search
framework, showed performance algorithm improved leveraging unknown smoothness policy space, also known smoothness
planning horizon. study motivated solve real-world engineering applications, also discussed parallelization design utilizes property AI planning
order minimize overhead. resulting algorithm, pLOGO-OP algorithm,
successfully applied complex engineering problem, namely, policy derivation nuclear
accident management.
Aside planning problem considered, LOGO algorithm also
used, example, optimize parameters algorithms (i.e., algorithm configuration). AI community, algorithm configuration problem addressed
several methods, including genetic algorithm (Ansotegui, Sellmann, & Tierney, 2009), discrete optimization convergence guarantee limit (Hutter, Hoos, Leyton-Brown, &
Stutzle, 2009), racing approach originated machine learning community (Hoeffding Races) (Birattari, Yuan, Balaprakash, & Stutzle, 2010), model-based optimization
convergence guarantee limit (Hutter, Hoos, & Leyton-Brown, 2011), simultaneous use several randomized local optimization methods (Gyorgy & Kocsis, 2011),
Bayesian optimization (Snoek, Larochelle, & Adams, 2012). Compared previous
parameter tuning methods, LOGO algorithm limited optimizing continuous
deterministic functions. apply stochastic functions, future work would modify
LOGO algorithm done SOO algorithm previous study (Valko, Car188

fiGlobal Continuous Optimization Error Bound Fast Convergence

pentier, & Munos, 2013). consider categorical and/or discrete parameters addition
continuous parameters, possibility could use LOGO algorithm subroutine
deal continuous variables one previous methods.
promising results presented paper suggest several interesting directions
future research. important direction leverage additional assumptions. Since LOGO
based weak set assumptions, would natural use LOGO main subroutine add mechanisms account additional assumptions. example,
illustrated LOGO would able scale higher dimension additional
assumptions Section 5. Another possibility add GP assumption based
idea presented recent paper (Kawaguchi, Kaelbling, & Lozano-Perez, 2015). Future
work also would design autonomous agent integrating planning algorithm
learning/exploration algorithm (Kawaguchi, 2016a). One remaining challenge LOGO
derive series methods adaptively determine algorithms free parameter
w. illustrated experiment, achievement topic mitigates
problem parameter sensitivity, also would improve algorithms performance.

Acknowledgments
work carried first author Japan Atomic Energy Agency.
authors would like thank Dr. Hiroto Itoh Mr. Jun Ishikawa JAEA several
discussions related topics. authors would like thank Mr. Lawson Wong
MIT insightful comments. authors would like thank anonymous reviewers
insightful constructive comments.

Appendix A. Experimental Design Application Study Nuclear
Accident Management
appendix, present experimental setting Application Study Nuclear
Accident Management Section 6.4. THALES2, consider volume nodalization shown Figure 11. reactor pressure vessel divided seven volumes,
consisting core, upper plenum, lower plenum, steam dome, downcomer, recirculation
loops B. containment vessel consists drywell, suppression chamber, pedestal
vent pipes. atmosphere suppression chamber connected via containment venting system (S/C venting). plant data initial conditions determined
based data Unit 1 Browns Ferry nuclear power plant (BWR4/Mark-I)
construction permit application forms BWR plants Japan. failure
containment vessel assumed occur pressure vessel becomes 2.5
times greater design pressure. Here, design pressure 3.92 (kgf/cm2 g)
criterion containment failure 108330 kgf/m2 . degree opening
containment venting fixed 25% filtering considered.
consider TQUV sequence accident scenario. TQUV sequence,
Emergency Core Cooling Systems (ECCSs) functions, similar case accident
Fukushima Daiichi nuclear power plant. TQUV sequence one major
scenarios considered Probabilistic Risk Assessment (PRA) nuclear plants. Therefore,
189

fiFigure 11: Nodalization physical space
Kawaguchi, Maruyama, & Zheng

Reactor building
Containment

CST
Steam
dome

RCIC
HPCI

Plenum

SRV

DC

Core

LPCI

Lower
plenum
Loop

Loop B

Vacuum
breaker

Pedestal

Vent
pipe
Suppression chamber

Containm
ent Vent

Figure 11: Nodalization physical space

FP: Fission Products

FP: Fission Products

12: Phenomenon
considered
fission
producttransportation
transportation
Figure 1:Figure
Phenomenon
considered
fission
product
190



fiGlobal Continuous Optimization Error Bound Fast Convergence

results show promising benefit containment venting, long use
good policy.
simulator developed Japan Atomic Energy Agency adopted
experiment (THALES2) computes transportation fission products well thermal
hydraulics volume Figure 11 core melt progression Core volume.
transportation fission products considered experiment shown Figure 12.
details computation THALES2 code found paper Ishikawa et al.
(2002). Figure 11 Figure 12 modified versions graphs used previous
presentation ongoing development THALES2 code, given
USNRCs 25th Regulatory Information Conference (Maruyama, 2013).

References
Ansotegui, C., Sellmann, M., & Tierney, K. (2009). gender-based genetic algorithm
automatic configuration algorithms. Proceedings 15th International
Conference Principles Practice Constraint Programing (CP 2009).
Atkeson, C. G. (1998). Nonparametric model-based reinforcement learning. Advances
Neural Information Processing Systems (NIPS), pp. 10081014.
Azar, M. G., Lazaric, A., & Brunskill, E. (2014). Stochastic optimization locally
smooth function correlated bandit feedback. 31st International Conference
Machine Learning (ICML).
Baxter, J., & Bartlett, P. L. (2001). Infinite-horizon policy-gradient estimation. Journal
Artificial Intelligence Research (JAIR), 15, 319350.
Birattari, M., Yuan, Z., Balaprakash, P., & Stutzle, T. (2010). F-race iterated F-race:
overview. Experimental Methods Analysis Optimization Algorithms,
pp. 311336. Springer-Verlag.
Brochu, E., Cora, V. M., & de Freitas, N. (2009). tutorial Bayesian optimization
expensive cost functions, application active user modeling hierarchical reinforcement learning. Technical report No. UBC TR-2009-23 arXiv:1012.2599v1,
Dept. Computer Science, University British Columbia.
Bubeck, S., & Munos, R. (2010). Open loop optimistic planning. Conference Learning
Theory.
Bubeck, S., Stoltz, G., & Yu, J. Y. (2011). Lipschitz bandits without Lipschitz constant.
Proceedings 22nd International Conference Algorithmic Learning Theory.
Busoniu, L., Daniels, A., Munos, R., & Babuska, R. (2013). Optimistic planning
continuous-action deterministic systems. 2013 Symposium Adaptive Dynamic
Programming Reinforcement Learning.
Carter, R. G., Gablonsky, J. M., Patrick, A., Kelly, C. T., & Eslinger, O. J. (2001). Algorithms noisy problems gas transmission pipeline optimization. Optimization
Engineering, 2 (2), 139157.
Daly, R., & Shen, Q. (2009). Learning Bayesian network equivalence classes ant colony
optimization. Journal Articial Intelligence Research, 35 (1), 391447.
191

fiKawaguchi, Maruyama, & Zheng

Deisenroth, M. P., Neumann, G., & Peters, J. (2013). survey policy search robotics.
Foundations Trends Robotics, 2, 1142.
Deisenroth, M. P., & Rasmussen, C. E. (2011). PILCO: model-based data-efficient
approach policy search. 28th International Conference Machine Learning
(ICML).
Dixon, L. C. W. (1978). Global Optima Without Convexity. Hatfield, England: Numerical
Optimization Centre, Hatfield Polytechnic.
Gablonsky, J. M. (2001). Modifications direct algorithm. Ph.D. thesis, North Carolina
State University, Raleigh, North Carolina.
Gullapalli, V., Franklin, J., & Benbrahim, H. (1994). Acquiring robot skills via reinforcement
learning. Control Systems Magazine, IEEE, 14 (1), 1324.
Gyorgy, A., & Kocsis, L. (2011). Efficient multi-start strategies local search algorithms.
Journal Artificial Intelligence Research (JAIR), 41, 407444.
Hansen, P., & Jaumard, B. (1995). Lipschitz optimization. Horst, R., & Pardalos,
P. M. (Eds.), Handbook Global Optimization, pp. 407493. Netherlands: Kluwer
Academic Publishers.
Hansen, P., Jaumard, B., & Lu, S. H. (1991). number iterations Piyavskiis
global optimization algorithm. Mathematics Operations Research, 16, 334350.
He, J., Verstak, A., Sosonkina, M., & Watson, L. (2009). Performance modeling analysis
massively parallel DIRECT, Part 1. Journal High Performance Computing
Applications, 23, 1428.
He, J., Verstak, A., Watson, L. T., Stinson, C. A., et al. (2004). Globally optimal transmitter
placement indoor wireless communication systems. IEEE Transactions Wireless
Communications, 3 (6), 19061911.
Heidrich-Meisner, V., & Igel, C. (2008). Evolution strategies direct policy search.
Proceedings 10th International Conference Parallel Problem Solving
Nature: PPSN X, pp. 428437. Springer-Verlag.
Horst, R., & Tuy, H. (1990). Global Optimization: Deterministic Approaches. Berlin:
Springer.
Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2011). Sequential model-based optimization
general algorithm configuration. Learning Intelligent Optimization, 5, 507
523.
Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: automatic
algorithm configuration framework. Journal Artificial Intelligence Research (JAIR),
36, 267306.
Ishikawa, J., Muramatsu, K., & Sakamoto, T. (2002). Systematic source term analysis
level 3 PSA BWR Mark-II containment THALES-2 code. Proceedings
10th International Conference Nuclear Engineering, ICONE-10-22080.
Jones, D. R., Perttunen, C. D., & Stuckman, B. E. (1993). Lipschitzian optimization without
Lipschitz constant. Journal Optimization Theory Applications, 79 (1), 157
181.
192

fiGlobal Continuous Optimization Error Bound Fast Convergence

Kawaguchi, K., Uchiyama, T., & Muramatsu, K. (2012). Efficiency analytical methodologies uncertainty analysis seismic core damage frequency. Journal Power
Energy Systems, 6 (3), 378393.
Kawaguchi, K. (2016a). Bounded optimal exploration MDP. Proceedings 30th
AAAI Conference Artificial Intelligence (AAAI).
Kawaguchi, K. (2016b). Deep learning without poor local minima. Massachusetts Institute
Technology, Technical Report, MIT-CSAIL-TR-2016-005.
Kawaguchi, K., Kaelbling, L. P., & Lozano-Perez, T. (2015). Bayesian optimization
exponential convergence. Advances Neural Information Processing (NIPS).
Kirk, D. E. (1970). Optimal Control Theory. Englewood Cliffs, NJ: Prentice-Hall.
Kleinberg, R. D., Slivkins, A., & Upfal, E. (2008). Multi-armed bandit problems metric
spaces. Proceedings 40th ACM Symposium Theory Computing, pp.
681690.
Kober, J., Bagnell, J. A. D., & Peters, J. (2013). Reinforcement learning robotics:
survey. International Journal Robotics Research, 32.
Kvasov, D. E., Pizzuti, C., & Sergeyev, Y. D. (2003). Local tuning partition strategies
diagonal GO methods. Numerische Mathematik, 94 (1), 93106.
Maruyama, Y. (2013). Development THALES2 code application analysis
accident Fukushima Daiichi Nuclear Power Plant. NRCs 25th Regulatory
Information Conference.
Mayne, D. Q., & Polak, E. (1984). Outer approximation algorithm nondifferentiable
optimization problems. Journal Optimization Theory Applications, 42 (1), 19
30.
McDonald, D. B., Grantham, W. J., Tabor, W. L., & Murphy, M. J. (2007). Global
local optimization using radial basis function response surface models. Applied Mathematical Modelling, 31 (10), 20952110.
Mladineo, R. H. (1986). algorithm finding global maximum multimodal,
multivariate function. Mathematical Programming, 34, 188200.
Munos, R. (2011). Optimistic optimization deterministic functions without knowledge
smoothness. Advances Neural Information Processing Systems (NIPS), pp.
783791.
Munos, R. (2013). bandits Monte-Carlo tree search: optimistic principle applied
optimization planning. Foundations Trends Machine Learning, 7 (1),
1130.
Murty, K. G., & Kabadi, S. N. (1987). np-complete problems quadratic
nonlinear programming. Mathematical programming, 39 (2), 117129.
OECD/NEA/CSNI (2014). Status report filtered containment venting. Technical report
NEA/CSNI/R(2014)7, JT03360082.
Park, Y., & Ahn, I. (2010). SAMEX: severe accident management support expert. Annals
Nuclear Energy, 37 (8), 10671075.
193

fiKawaguchi, Maruyama, & Zheng

Pinter, J. (1986). Globally convergent methods n-dimensional multiextremal optimization. Optimization, 17, 187202.
Piyavskii, S. A. (1967). algorithm finding absolute minimum function.
Theory Optimal Solutions, 2, 1324. Kiev, IK USSR.
Rios, L. M., & Sahinidis, N. V. (2013). Derivative-free optimization: review algorithms
comparison software implementations. Journal Global Optimization, 56,
12471293.
Russell, S. J., & Norvig, P. (2009). Articial intelligence: modern approach (3rd edition).
Prentice-Hall.
Ryoo, H. S., & Sahinidis, N. V. (1996). branch-and-reduce approach global optimization. Journal Global Optimization, 8 (2), 107138.
Shubert, B. O. (1972). sequential method seeking global maximum function.
SIAM Journal Numerical Analysis, 9, 379388.
Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian optimization machine learning algorithms. Advances Neural Information Processing Systems
(NIPS).
Strongin, R. G. (1973). convergence algorithm finding global extremum.
Engineering Cybernetics, 11, 549555.
Surjanovic, S., & Bingham, D. (2013). Virtual library simulation experiments: Test
functions datasets. Retrieved July 2, 2014, http://www.sfu.ca/~ssurjano.
Sutton, R. S., McAllester, D., Singh, S., & Mansour, Y. (1999). Policy gradient methods reinforcement learning function approximation. Advances Neural
Information Processing Systems, 12, 10571063.
USNRC (2013). Hardened Vents Filtration Boiling Water Reactors Mark
Mark II containment designs . Retrieved August 2015 http://www.nrc.gov/
reactors/operating/ops-experience/japan-dashboard/hardened-vents.html .
Valko, M., Carpentier, A., & Munos, R. (2013). Stochastic simultaneous optimistic optimization. Proceedings 30th International Conference Machine Learning
(ICML).
Wang, Z., Shakibi, B., Jin, L., & de Freitas, N. (2014). Bayesian multi-scale optimistic
optimization. AI Statistics, pp. 10051014.
Wang, Z., Zoghi, M., Hutter, F., Matheson, D., & De Freitas, N. (2013). Bayesian optimization high dimensions via random embeddings. Proceedings Twenty-Third
international joint conference Artificial Intelligence, pp. 17781784. AAAI Press.
Weinstein, A. (2014). Local planning continuous Markov decision processes. Ph.D.
thesis, Rutgers, State University New Jersey.
Weinstein, A., & Littman, M. L. (2012). Bandit-based planning learning continuousaction Markov decision processes. International Conference Automated Planning
Scheduling, pp. 306314.
194

fiGlobal Continuous Optimization Error Bound Fast Convergence

Zheng, X., Itoh, H., Kawaguchi, K., Tamaki, H., & Maruyama, Y. (2015). Application
Bayesian nonparametric models uncertainty sensitivity analysis source
term BWR severe accident. Journal Reliability Engineering & System Safety,
138, 253262.
Zwolak, J. W., Tyson, J. J., & Watson, L. T. (2005). Globally optimized parameters
model mitotic control frog egg extracts. IEE Systems Biology, 152 (2), 8192.

195

fiJournal Artificial Intelligence Research 56 (2016) 403428

Submitted 11/15; published 07/16

Satisfiability Problem SPARQL Patterns
Xiaowang Zhang

xiaowangzhang@tju.edu.cn

School Computer Science Technology,
Tianjin University, China
Tianjin Key Laboratory
Cognitive Computing Application,
Tianjin, China

Jan Van den Bussche

jan.vandenbussche@uhasselt.be

Hasselt University, Belgium

Francois Picalausa

fpicalausa@gmail.com

Abstract
satisfiability problem SPARQL 1.0 patterns undecidable general, since
relational algebra emulated using patterns. goal paper delineate
boundary decidability satisfiability terms constraints allowed filter
conditions. classes constraints considered bound-constraints, negated boundconstraints, equalities, nonequalities, constant-equalities, constant-nonequalities.
main result paper summarized saying that, soon inconsistent filter
conditions formed, satisfiability undecidable. key insight case find
way emulate set difference operation. Undecidability obtained
known undecidability result algebra binary relations union, composition,
set difference. inconsistent filter conditions formed, satisfiability
decidable syntactic checks bound variables use literals. Although
problem shown NP-complete, experimentally shown checks
implemented efficiently practice. paper also points satisfiability
so-called well-designed patterns decided check bound variables check
inconsistent filter conditions.

1. Introduction
Resource Description Framework popular data model information Web.
RDF represents information form directed, labeled graphs. standard query
language RDF data SPARQL (Harris & Seaborne, 2013). current version 1.1
SPARQL extends SPARQL 1.0 (Prudhommeaux & Seaborne, 2008) important features aggregation regular path expressions (Arenas, Conca, & Perez, 2012).
features, negation subqueries, also added, mainly efficiency reasons, already expressible, involved manner, version 1.0.
Hence, still relevant study fundamental properties SPARQL 1.0. paper,
follow elegant formalization SPARQL 1.0 Arenas, Gutierrez, & Perez (2009)
eminently suited theoretical investigations.
fundamental problem investigate satisfiability SPARQL patterns. pattern called satisfiable exists RDF graph pattern
evaluates nonempty set mappings. query language, satisfiability clearly one
c
2016
AI Access Foundation. rights reserved.

fiZhang, Van den Bussche, & Picalausa

essential properties one needs understand one wants automated reasoning.
Since SPARQL patterns emulate relational algebra expressions (Angles & Gutierrez,
2008; Polleres, 2007; Arenas & Perez, 2011), satisfiability relational algebra undecidable (Abiteboul, Hull, & Vianu, 1995), general satisfiability problem SPARQL
undecidable well.
Whether pattern satisfiable depends mainly filter operations appearing
pattern; without filter operations, pattern always satisfiable except trivial cases
literal occurs wrong place. goal paper precisely delineate
decidability SPARQL fragments defined terms constraints
used filter conditions. six basic classes constraints consider boundconstraints; equalities; constant-equalities; negations. way, fragments
SPARQL constructed specifying kinds constraints allowed filter
conditions. example, fragment SPARQL(bound, 6=, 6=c ), filter conditions
bound constraints, nonequalities, constant-nonequalities.
main result states fragments satisfiability decidable
two fragments SPARQL(bound, =, 6=c ) SPARQL(bound, 6=, 6=c ) subfragments. Consequently, soon either negated bound-constraints, constant-equalities,
combinations equalities nonequalities allowed, satisfiability problem becomes
undecidable. undecidable case established showing set difference operation emulated. already known using negated bound-constraints (Angles
& Gutierrez, 2008; Arenas & Perez, 2011); show also possible using constantequalities, using combinations equalities nonequalities, way.
Undecidability obtained known undecidability result algebra
binary relations union, composition, set difference (Tan, Van den Bussche, &
Zhang, 2014).
decidable cases, satisfiability decided syntactic checks bound variables use literals. Although problem shown NP-complete,
experimentally shown checks implemented efficiently practice.
end paper look well-behaved class patterns known
well-designed patterns (Perez et al., 2009). observe satisfiability well-designed
patterns decided combining check bound variables check inconsistent filter conditions.
paper organized follows. next section, introduce syntax
semantics SPARQL patterns introduce different fragments consideration. Section 3 introduces satisfiability problem shows satisfiability checking
fragments SPARQL(bound, =, 6=c ) SPARQL(bound, 6=, 6=c ). Section 4 shows undecidability fragments SPARQL(bound), SPARQL(=c ), SPARQL(=, 6=). Section 5
considers well-designed patterns.
Section 6 reports experiments test decision methods practice. Section 7
briefly discuss results extend new operators added
SPARQL 1.1. conclude Section 8.
404

fiSatisfiability Problem SPARQL

2. SPARQL Fragments
section recall syntax semantics SPARQL patterns, closely following
core SPARQL formalization given Arenas, Gutierrez, & Perez (2009).1 semantics
use set-based, whereas semantics real SPARQL bag-based. However,
satisfiability (the main topic paper), makes difference whether use set
bag semantics (Schmidt, Meier, & Lausen, 2010, Lemma 1).
section also define language fragments defined terms allowed
filter conditions, form object paper.
2.1 RDF Graphs
Let I, B, L infinite sets IRIs, blank nodes literals, respectively. three
sets pairwise disjoint. denote union B L U , elements L
referred constants. Note blank nodes constants.
triple (s, p, o) (I B) U called RDF triple. RDF graph finite set
RDF triples.
2.2 Syntax SPARQL Patterns
Assume furthermore infinite set V variables, disjoint U . convention
SPARQL variables written beginning question mark, distinguish
constants. follow convention paper.
SPARQL patterns inductively defined follows.
triple (I L V ) (I V ) (I L V ) pattern (called triple pattern).
P1 P2 patterns, following:
P1 UNION P2 ;
P1 P2 ;
P1 OPT P2 .
P pattern C constraint (defined next), P FILTER C pattern;
call C filter condition.
Here, constraint one six following forms:
1. bound-constraint: bound(?x)
2. negated bound-constraint: bound(?x)
3. equality: ?x = ?y
4. nonequality: ?x 6= ?y ?x ?y distinct variables
5. constant-equality: ?x = c c constant
6. constant-nonequality: ?x 6= c
1. Arenas, Perez, Guttierez (2009) discuss minor deviations formalization real
SPARQL, differences inessential purpose formal investigation.

405

fiZhang, Van den Bussche, & Picalausa

need consider conjunctions disjunctions filter conditions, since
conjunctions expressed repeated application filter, disjunctions
expressed using UNION. Hence, going disjunctive normal form, predicate built
using negation, conjunction, disjunction indirectly supported language.
Moreover, real SPARQL also allows blank nodes triple patterns. feature
omitted formalization blank nodes triple patterns equivalently
replaced variables.
2.3 Semantics SPARQL Patterns
semantics patterns defined terms sets so-called solution mappings, hereinafter simply called mappings. solution mapping total function : U
finite set variables. denote domain dom().
make use following convention.
Convention. mapping constant c L, agree (c) equals c
itself.
words, mappings default extended constants according identity
mapping.
given graph G pattern P , define semantics P G, denoted
JP KG , set mappings, following manner.
P triple pattern (u, v, w),

JP KG := { : {u, v, w} V U | ((u), (v), (w)) G}.
definition relies Convention 2.3 formulated above.
P form P1 UNION P2 ,
JP KG := JP1 KG JP2 KG .
P form P1 P2 ,
JP KG := JP1 KG
n JP2 KG ,
where, two sets mappings 1 2 , define
1
n 2 = {1 2 | 1 1 2 2 1 2 }.
Here, two mappings 1 2 called compatible, denoted 1 2 , agree
intersection domains, i.e., every variable ?x dom(1 )dom(2 ),
1 (?x) = 2 (?x). Note 1 2 compatible, union 1 2
well-defined mapping; property used formal definition above.
P form P1 OPT P2 ,
JP KG := (JP1 KG
n JP2 KG ) (JP1 KG r JP2 KG ),
where, two sets mappings 1 2 , define
1 r 2 = {1 1 | 2 2 : 1 2 }.
406

fiSatisfiability Problem SPARQL

Finally, P form P1 FILTER C,
JP KG := { JP1 KG | |= C}
satisfaction constraint C mapping , denoted |= C, defined
follows:
1. |= bound(?x) ?x dom();
2. |= bound(?x) ?x
/ dom();
3. |= ?x = ?y ?x, ?y dom() (?x) = (?y);
4. |= ?x 6= ?y ?x, ?y dom() (?x) 6= (?y);
5. |= ?x = c ?x dom() (?x) = c;
6. |= ?x 6= c ?x dom() (?x) 6= c.
Note |= ?x 6= ?y 6|= ?x = ?y, similarly |= ?x 6= c.
line three-valued logic semantics filter conditions used official
semantics (Arenas et al., 2009). example, ?x
/ dom(), three-valued logic
?x = c evaluates error ; consequently, also ?x = c evaluates error .
Accordingly, semantics above, 6|= ?x = c 6|= ?x 6= c.
2.4 SPARQL Fragments
form fragments SPARQL specifying six classes constraints
allowed filter conditions. denote class bound-constraints bound, negated
bound-constraints bound, equalities =, nonequalities 6=, constant-equalities
=c , constant-nonequalities 6=c . subset F {bound, bound, =
, 6=, =c , 6=c } form fragment SPARQL(F ). example, SPARQL(bound, =,
6=c ), filter conditions bound constraints, equalities, constant-nonequalities.

3. Satisfiability: Decidable Fragments
pattern P called satisfiable exists graph G JP KG nonempty.
general, checking satisfiability complicated, indeed undecidable, problem.
two fragments SPARQL(bound, =, 6=c ) SPARQL(bound, 6=, 6=c ), turn
essentially two possible reasons unsatisfiability.
first possible reason pattern specifies literal value first position
RDF triple, whereas RDF triples literals third position.
example, using literal 42, triple pattern (42, ?x, ?y) unsatisfiable. Note literals
middle position triple pattern already disallowed definition triple
pattern, need worry first position.
discrepancy triple patterns RDF triples easy sidestep, however.
Appendix show how, without loss generality, may assume
patterns contain triple pattern (u, v, w) u literal.
second main possible reason unsatisfiability filter conditions require
variables bound together way cannot satisfied subpattern
407

fiZhang, Van den Bussche, & Picalausa

filter applies. example, pattern
((?x, a, ?y) UNION (?x, b, ?z)) FILTER (bound(?y) bound(?z))
unsatisfiable. Note bound constraints strictly necessary illustrate
phenomenon: example replace filter condition ?y = ?z resulting
pattern still unsatisfiable.
next prove formally satisfiability patterns SPARQL(bound, =, 6=c )
SPARQL(bound, 6=, 6=c ) effectively decidable, catching reason unsatisfiability
described above. Note also two fragments combined, since satisfiability
SPARQL(=, 6=) undecidable see next Section.
3.1 Checking Bound Variables
perform bound checks variables, associate every pattern P set (P ) schemes,
scheme simply set variables, following way.2
P triple pattern (u, v, w), (P ) := {{u, v, w} V }.
(P1 UNION P2 ) := (P1 ) (P2 ).
(P1 P2 ) := {S1 S2 | S1 (P1 ) S2 (P2 )}.
(P1 OPT P2 ) := (P1 P2 ) (P1 ).
(P1 FILTER C) := {S (P1 ) | ` C}, ` C defined follows:
C form bound(?x) ?x = c ?x 6= c, ` C ?x S;
C form ?x = ?y ?x 6= ?y, ` C ?x, ?y S;
` bound(?x) ?x
/ S.
Example 1. Consider pattern
P = (?x, p, ?y) OPT ((?x, q, ?z) UNION (?x, r, ?u)).
subpattern P1 = (?x, q, ?z) UNION (?x, r, ?u) (P1 ) = {{?x, ?z}, {?x, ?u}}.
Hence, ((?x, p, ?y) P1 ) = {{?x, ?y, ?z}, {?x, ?y, ?u}}. conclude (P ) =
{{?x, ?y}, {?x, ?y, ?z}, {?x, ?y, ?u}}.
Example 2. another example, consider pattern
P = ((?x, p, ?y) OPT ((?x, q, ?z) FILTER ?y = ?z)) FILTER ?x 6= c.
(?x, q, ?z) = {{?x, ?z}}. Note {?x, ?z} 6` ?y = ?z, ?y
/ {?x, ?z}.
Hence, subpattern P1 = (?x, q, ?z) FILTER ?y = ?z (P1 ) = .
subpattern P2 = (?x, p, ?y) OPT P1 (P2 ) = (?x, p, ?y) = {{?x, ?y}}. Since
{?x, ?y} ` ?x 6= c, conclude (p) = {{?x, ?y}}.
2. define (P ) general patterns, belonging fragments considered
Section, make another use (P ) Section 5.

408

fiSatisfiability Problem SPARQL

establish main result Section.
Theorem 3. Let P SPARQL(bound, =, 6=c ) SPARQL(bound, 6=, 6=c ) pattern.
P satisfiable (P ) nonempty.
only-if direction Theorem 3 easy direction given following
Lemma 4. Note lemma holds general patterns; straightforwardly
proven induction structure P .
Lemma 4. Let P pattern G graph. JP KG exists (P )
dom() = S.
direction Theorem 3 SPARQL(bound, =, 6=c ) given following
Lemma 5.
following use var(P ) denote set variables occurring pattern
P .3
Lemma 5. Let P pattern SPARQL(bound, =, 6=c ). Let c constant
appear constant-nonequality filter condition P . constant mapping
: var(P ) {c}, let G RDF graph consisting possible triples ((u), (v), (w))
(u, v, w) triple pattern P .
every (P ) exists 0 |S 0 belongs JP KG .
Proof. induction structure P . P triple pattern (u, v, w) =
{u, v, w} V . Since (|S (u), |S (v), |S (w)) = ((u), (v), (w)) G, |S JP KG
take 0 = S.
P form P1 UNION P2 , claim follows readily induction.
P form P1 P2 , = S1 S2 Si (Pi ) = 1, 2.
induction, exists Si0 Si |Si0 JPi KG . Clearly |S10 |S20 since
restrictions mapping. Hence |S10 |S20 = S10 S20 JP KG take
0 = S10 S20 .
P form P1 OPT P2 , two possibilities.
(P1 P2 ) reason previous case.
(P1 ) induction exists S10 |S10 JP1 KG .
two possibilities:
(P2 ) nonempty induction exists S20 |S20 JP2 KG .
reason case P1 P2 .
Otherwise, Lemma 4 know JP2 KG empty. JP KG = JP1 KG
take 0 = S10 .
Finally, P form P1 FILTER C, know (P1 ) ` C.
induction, exists 0 |S 0 JP1 KG . show |S 0 JP KG
showing |S 0 |= C. three possibilities C.
3. also use following standard notion restriction mapping. f : X total function
Z X, restriction f |Z f Z total function Z defined f |Z (z) = f (z)
every z Z. is, f |Z f defined subdomain Z.

409

fiZhang, Van den Bussche, & Picalausa

C form bound(?x), know ` C ?x 0 . Hence |S 0 |= C.
C form ?x = ?y, know ?x, ?y 0 , certainly |S 0 |= C
since maps everything c.
C form ?x 6= d, 6= c choice c, |S 0 |= C since
(?x) = c.
Example 6. illustrate Lemma, consider pattern
P = ((?x, p, ?y) FILTER ?x 6= a) OPT ((?x, q, ?z) UNION (?x, r, ?u))
variant pattern Example 1. example, (P ) =
{{?x, ?y}, {?x, ?y, ?z}, {?x, ?y, ?u}}. case, mapping Lemma maps ?x,
?y, ?z ?u c. graph G Lemma equals {(c, p, c), (c, q, c), (c, r, c)},
JP KG = {1 , 2 } 1 = |{?x,?y,?z} 2 = |{?x,?y,?u} . consider = {?x, ?y}
(P ). 0 = {?x, ?y, ?z} indeed 0 |S 0 = 1 JP KG . Note
example could also chosen {?x, ?y, ?u} 0 .
counterpart Lemma 5 fragment SPARQL(bound, 6=, 6=c ) given
following Lemma, thus settling Theorem 3 fragment.
Lemma 7. Let P pattern SPARQL(bound, 6=, 6=c ). Let W set constants
appearing constant-nonequality filter condition P . Let Z finite set
constants cardinality var(P ), disjoint W . : var(P ) Z
arbitrary fixed injective mapping, let G RDF graph consisting possible
triples ((u), (v), (w)) (u, v, w) triple pattern P .
every (P ) exists 0 |S 0 belongs JP KG .
Proof. prove every subpattern Q P every (Q) exists 0
|S 0 JQKG . proof induction height Q. reasoning
largely proof Lemma 5. difference case Q
form Q1 FILTER C. showing 0 |= C, argue follows last
two cases:
C form ?x 6= ?y, |S 0 |= C since injective.
C form ?x 6= c, |S 0 |= C since Z W disjoint.
3.2 Computational Complexity
section show satisfiability decidable fragments NP-complete. Note
immediately follow NP-completeness SAT, since boolean
formulas part syntax decidable fragments.
Theorem 3 implies following complexity upper bound:
Corollary 8. satisfiability problem SPARQL(bound, =, 6=c ) patterns, well
SPARQL(bound, 6=, 6=c ) patterns, belongs complexity class NP.
410

fiSatisfiability Problem SPARQL

Proof. Theorem 3, SPARQL(bound, =, 6=c ) SPARQL(bound, 6=, 6=c ) pattern P
satisfiable exists scheme (P ). Following definition (P ),
clear polynomial-time nondeterministic algorithm that, input
P , accepting possible run computes scheme (P ), every scheme
(P ) computed accepting possible run.
Specifically, algorithm works bottom-up syntax tree P computes
scheme every subpattern. every leaf Q, corresponding triple pattern P ,
compute unique scheme (Q). every UNION operator nondeterministically
choose continuing scheme left right child. every
operator continue union left right child schemes. every
OPT operator, nondeterministically choose treating AND, simply
continuing scheme left. every FILTER operation constraint C
check child scheme whether ` C. check succeeds, continue
S; check fails, run rejected. computation reached root
syntax tree compute scheme root, run accepting
computed scheme output.
Remark 9. presentation syntax SPARQL, consider conjunction
disjunction filter conditions. Extending syntax allow would ruin
NP upper bound. Allowing conjunctions disjunctions, would need extend
definition (P ) obvious manner, defining ` C1 C2 ` C1 ` C2 ,
similarly definition ` C1 C2 . results would carry through.
next show satisfiability actually NP-hard, even patterns using
OPT operators using bound constraints filter conditions.
Proposition 10. satisfiability problem OPT-free patterns SPARQL(bound)
NP-hard.
Proof. define problem Nested Set Cover follows:
Input: finite set finite set E sets subsets . (So, every element E
set subsets .)

Decide: Whether element e E choose subset Se e, eE Se =
T.
show later problem NP-hard; let us first describe
reduced polynomial time satisfiability problem hand. Consider input (T, E)
Nested Set Cover. Without loss generality may assume set variables
{?x1 , ?x2 , . . . , ?xn }. Fix constant c. subset , make pattern PS
taking (x, c, c) x S. set e subsets , form
pattern Pe taking UNION PS e. Finally, form pattern PE
taking Pe e E.
consider following pattern denote P(T,E) :
PE FILTER bound(?x1 ) FILTER bound(?x2 ) . . . FILTER bound(?xn )
411

fiZhang, Van den Bussche, & Picalausa

claim P(T,E) satisfiable (T, E) yes-instance Nested Set
Cover. see only-if direction, let G graph JP(T,E) KG nonempty, i.e.,
element solution mapping .
particular JPE KG . Hence, every
e E exists e JPe KG = eE e . Since Pe UNION PS
e, e E exists Se e e JPSe KG . Since PSeSis
(x, c, c) x SSe , follows dom(e ) = Se . Hence, since dom() = eE dom(e ),
dom() = eE Se . However, bound constraints filters
applied P(T,E) ,

also dom() = {?x1 , . . . , ?xn } = . conclude = eE Se desired.
if-direction, assume e E exists Se e =
eE Se . Consider singleton graph G = {(c, c, c)}. subset , let :
{c} constant solution mapping domain S. Clearly, JPS KG , Se JPe KG
everySe E. map constant,
Hence,
compatible.

= eE Se , JPE KG . Since dom() = eE dom(Se ) = eE Se = =
{?x1 , . . . , ?xn }, mapping satisfies every constraint bound(?xi ) = 1, . . . , n.
conclude JP(E,T ) KG desired.
remains show Nested Set Cover NP-hard. Thereto reduce classical
CNF-SAT problem. Assume given boolean formula CNF, conjunction
clauses, clauses disjunction literals (variables negated variables).
construct input (T, E) Nested Set Cover follows. Denote set variables used
W .
take set clauses . variable x W , consider set Posx
consisting clauses contain positive occurrence x, set Negx consisting
clauses contain negative occurrence x. define ex pair
{Posx , Negx }.
E defined set {ex | x W }. clear satisfiable
constructed input yes-instance Nested Set Cover. Indeed, truth assignments
variables correspond selecting either Posx Negx ex x W .

4. Undecidable Fragments
Section show two decidable fragments SPARQL(bound, =, 6=c )
SPARQL(bound, 6=, 6=c ) are, sense, maximal. Specifically, three minimal fragments
subsumed one two fragments SPARQL(bound), SPARQL(=, 6=),
SPARQL(=c ). main result Section is:
Theorem 11. Satisfiability undecidable SPARQL(bound) patterns, SPARQL(=,
6=) patterns, SPARQL(=c ) patterns.
prove theorem reducing satisfiability problem algebra
finite binary relations union, composition, difference (Tan et al., 2014).
algebra also called Downward Algebra denoted DA. expressions DA
defined follows. Let R arbitrary fixed binary relation symbol.
symbol R DA-expression.
e1 e2 DA-expressions, e1 e2 , e1 e2 , e1 e2 .
412

fiSatisfiability Problem SPARQL

Semantically, DA-expressions represent binary queries binary relations, i.e., mappings
binary relations binary relations. Let J binary relation. DA-expression e,
define binary relation e(J) inductively follows:
R(J) = J;
(e1 e2 )(J) = e1 (J) e2 (J);
(e1 e2 )(J) = e1 (J) e2 (J) (set difference);
(e1 e2 )(J) = {(x, z) | : (x, y) e1 (J) (y, z) e2 (J)}.
DA-expression called satisfiable exists finite binary relation J
e(J) nonempty.
Example 12. example DA-expression e = (R R) R. J binary relation
{(a, b), (b, c), (a, c), (c, d)} e(J) = {(b, d), (a, d)}. example unsatisfiable DA
expression ((R R R) R) (R R R).
recall following result. actually well known (Andreka, Givant, & Nemeti,
1997) relational composition together union complementation leads
undecidable algebra; following result simplifies matters showing undecidability
already holds expressions single relation symbol using set difference instead
complementation. following result proven reduction universality
problem context-free grammars.
Theorem 13 (Tan et al., 2014). satisfiability problem DA-expressions undecidable.
4.1 Expressing MINUS
main problem face reducing DA SPARQL fragments stated Theorem 11, emulate difference operator. review generally emulate
MINUS operator, meaningful counterpart relational difference
operator SPARQL context.
MINUS operator defined follows. two patterns P1 P2 graph G,
define
JP1 MINUS P2 KG = JP1 KG r JP2 KG ,
reuse r operation sets mappings, already seen definition OPT
Section 2.3.
fragment SPARQL(bound), expressibility MINUS already known:
Lemma 14 (Arenas & Perez, 2011). MINUS expressible SPARQL(bound).
precisely, two patterns P1 P2 graph G, JP1 MINUSP2 KG = JP KG
P pattern

P1 OPT (P2 (?u, ?v, ?w)) FILTER bound(?u).
Here, ?u, ?v ?w fresh variables used P1 P2 .
413

fiZhang, Van den Bussche, & Picalausa

task find similar expressions two fragments SPARQL(=, 6=)
SPARQL(=c ). actually able express MINUS projection,
mild assumptions graph G.
projection, counterpart SPARQL operation SELECT, defined
follows. Let P pattern let finite set variables. SELECTS P
restricts solution mappings coming P variables listed S. Formally,
graph G, define
JSELECTS P KG = {|Sdom() | JP KG }.
assumptions graph G need make active domain.
Intuitively, active domain graph set entries triples graph.
Formally, define
adom(G) = {s | p, : (s, p, o) G} {p | s, : (s, p, o) G} {o | s, p : (s, p, o) G}.
easily express active domain SPARQL, following sense. Using three
variables ?u, ?v, ?w, consider pattern
adom = (?u, ?v, ?w) UNION (?w, ?u, ?v) UNION (?v, ?w, ?u).
graph,
adom(G) = {(?u) | JadomKG }
= {(?v) | JadomKG }

= {(?w) | JadomKG }.
ready state counterpart Lemma 14 SPARQL(=, 6=).
Lemma 15. MINUS expressible SPARQL(=, 6=), projection graphs
least two distinct elements. precisely, two patterns P1 P2
graph G adom(G) least two distinct elements, equality
JP1 MINUS P2 KG = JSELECTvar(P1 ) P KG , P pattern



P1 OPT ((P2 adom adom 0 ) FILTER ?u 6= ?u0 )

adom adom 0 FILTER ?u = ?u0 .

Here, adom 0 copy adom pattern different variables ?u0 , ?v 0 ?w0 .
variables, variables ?u, ?v ?w used adom, fresh variables used P1
P2 .
Proof. prove equality stated Theorem going consider inclusions.
easy reference name subpatterns P follows.
P20 denotes (P2 adom adom 0 ) FILTER ?u 6= ?u0 ;
P3 denotes P1 OPT P20 .
414

fiSatisfiability Problem SPARQL

Thus, P (P3 adom adom 0 ) FILTER ?u = ?u0 .
prove inclusion right left, let JP KG . = 3 ,
3 JP3 KG mapping defined {?u, ?v, ?w, ?u0 , ?v 0 , ?w0 } (?u) = (?u0 ).
particular, 3 . Since P3 = P1 OPT P20 , two possibilities 3 :
3 JP1 KG 02 JP20 KG 3 02 . 3 = |var(P1 ) ,
remains show exist 2 JP2 KG 3 2 . Assume
contrary. Since adom(G) least two distinct elements, 2 extended
mapping 02 JP20 KG . 2 02 3 , contradiction.

3 = 1 02 1 JP1 KG 02 JP20 KG . particular, 3 defined ?u
?u0 3 (?u) 6= 3 (?u0 ). hand, since 3 , (?u) = (?u0 ),
also 3 (?u) = 3 (?u0 ). contradiction, possibility consideration
cannot happen.

prove inclusion left right, let 1 JP1 MINUS P2 KG . Assume, sake
argument, would exist 02 JP20 KG 1 02 . Mapping 02 contains
mapping 2 JP2 KG , definition P20 . Since 1 02 , also 1 2 possible.
So, know exist 02 JP20 KG 1 02 . Hence,
1 JP3 KG . Note six variables ?u, ?u0 , ?v, ?v 0 , ?w, ?w0 belong
var(P1 ). Since G nonempty, 1 thus extended mapping JP KG .
conclude 1 JSELECTvar(P1 ) P KG desired.
analogous result fragment SPARQL(=c ) follows. Fix two distinct
constants b arbitrarily.
Lemma 16. MINUS expressible SPARQL(=c ), projection graphs
b appear. precisely, two patterns P1 P2 graph G
b belong adom(G), equality JP1 MINUS P2 KG = JSELECTvar(P1 ) P KG ,
P pattern



Pe1 OPT ((Pe2 adom ?u ) FILTER ?u = a) adom ?u FILTER ?u = b.
always, expression, variables ?u, ?v ?w used adom taken
fresh variables used P1 P2 .
correctness proof Lemma analogous proof given Lemma 15;
instead exploiting inconsistency ?u 6= ?u0 ?u = ?u0 done proof,
exploit inconsistency ?u = ?u = b.
4.2 Reduction Downward Algebra
ready formulate reduction satisfiability problem DA
satisfiability problem three fragments mentioned Theorem 11. precisely
formulate reduction prove Theorem fragment SPARQL(bound) first.
discuss reduction must adapted two fragments.
say RDF graph G represents binary relation J J = {(s, o) | p : (s, p, o)
G}. Intuitively, view RDF graph binary relation ignoring middle column.
415

fiZhang, Van den Bussche, & Picalausa

Lemma 17. every DA-expression e exists SPARQL(bound) pattern Pe
following properties:
1. exist two distinct fixed variables ?x ?y every RDF graph G
every JPe KG , ?x ?y belong dom();
2. every binary relation J RDF graph G represents J,
e(J) = {((?x), (?y)) | JPe KG };
Proof. induction structure e. e R Pe triple pattern (?x, ?z, ?y).
e form e1 e2 , Pe Pe1 UNION Pe2 .
e form e1 e2 , Pe Pe01 Pe02 , Pe01 Pe02 obtained
follows. First, renaming variables, may assume without loss generality Pe1
Pe2 variables common ?x ?y. Let ?z fresh variable.
Pe1 , rename ?y ?z, yielding Pe01 , Pe2 , rename ?x ?z, yielding Pe02 .
Finally, e form e1 e2 , use expression P Lemma 14 applied
Pe1 Pe2 . may assume without loss generality Pe1 Pe2
variables common ?x ?y.
lemma clearly e satisfiable Pe satisfiable.
thus reduction satisfiability DA satisfiability SPARQL(bound),
showing undecidability latter problem.
discuss two remaining fragments.
4.3 SPARQL(=, 6=)
fragment consider minor variant satisfiability DA-expressions
restrict attention binary relations least two elements. Formally, active domain
binary relation J set entries pairs belonging J, adom(J) := {x |
: (x, y) J (y, x) J}. DA-expression e called two-satisfiable e(J)
nonempty J adom(J) least two distinct elements.
Clearly, two-satisfiability undecidable well, decidable, satisfiability
would decidable too. Indeed, e satisfiable two-satisfiable, satisfiable
binary relation J single element. isomorphism one J
(the singleton {(x, x)}), case could checked separately.
Lemma 17 adapted claiming second property binary relations
J least two distinct elements. proof case e e1 e2 ,
use Lemma 15.
Using adapted lemma, reduce two-satisfiability DA satisfiability
SPARQL(=, 6=). need extra test whether graph represents binary
relation least two distinct elements. use following pattern test (using
fresh variables ?u, . . . , ?w0 usual):
(((?u, ?v, ?w) (?u0 , ?v 0 , ?w0 )) FILTER ?u 6= ?u0 )
UNION (((?u, ?v, ?w) (?u0 , ?v 0 , ?w0 )) FILTER ?w 6= ?w0 )
UNION (((?u, ?v, ?w) (?u0 , ?v 0 , ?w0 )) FILTER ?u 6= ?w0 )
Then, e two-satisfiable Pe test satisfiable.
416

fiSatisfiability Problem SPARQL

4.4 SPARQL(=c )
fragment consider variant two-satisfiability, called ab-satisfiability,
two arbitrary fixed constants a, b I. DA-expression called ab-satisfiable e(J)
nonempty binary relation J a, b adom(J).
DA-expressions distinguish isomorphic binary relations. Hence, absatisfiability equivalent two-satisfiability, thus still undecidable.
adapt Lemma 17, follows. second property claimed
binary relations J a, b adom(J). proof case e = e1 e2 ,
use Lemma 16.
obtain e ab-satisfiable Pe test ab satisfiable,
test ab following pattern tests whether graph represents binary relation
b active domain:
(((?u, ?v, ?w) UNION (?w, ?v, ?u)) ((?u0 , ?v 0 , ?w0 ) UNION (?w0 , ?v 0 , ?u0 )))
FILTER ?u = FILTER ?u0 = b
Remark 18. Recall literals cannot appear first second position RDF triple.
Patterns using constant-equality predicates unsatisfiable reason.
example, using literal 42, pattern (?x, ?y, ?z) FILTER ?y = 42 unsatisfiable. However, seen use constant-equality predicates leads undecidability
satisfiability much fundamental reason, nothing literals,
namely, ability emulate set difference.

5. Satisfiability Well-Designed Patterns
well-designed patterns (Perez et al., 2009) identified well-behaved class
SPARQL patterns, properties similar conjunctive queries relational databases
(Abiteboul et al., 1995). Standard conjunctive queries always satisfiable, conjunctive
queries extended equality nonequality constraints, possibly involving constants,
unsatisfiable constraints inconsistent. analogous behavior present
call AF-patterns: patterns use FILTER operators.
formalize Proposition 19. show Theorem 21 well-designed
pattern satisfiable reduction AF-pattern satisfiable.
words, far satisfiability concerned, well-designed patterns treated like AFpatterns.
5.1 Satisfiability AF-Patterns
Section 3.1 associated set schemes (P ) every pattern P . (P )
empty, P unsatisfiable (Lemma 4).
P AF-pattern (P ) nonempty, satisfiability P turn
depend solely equalities, nonequalities, constant-equalities, constantnonequalities occurring filter conditions P . denote set constraints
C(P ).
set constraints called consistent exists mapping satisfies every
constraint .
417

fiZhang, Van den Bussche, & Picalausa

establish:
Proposition 19. AF-pattern P satisfiable (P ) non-empty
C(P ) consistent.
Proof. only-if direction proposition given Lemma 4 together
observation JP KG , satisfies every constraint C(P ). Since P satisfiable,
G exist, C(P ) consistent.
direction, since P UNION OPT operators, (P )
singleton {S}. Since C(P ) consistent, exists mapping : U satisfying every
constraint C(P ). Let G graph consisting triples ((u), (v), (w))
(u, v, w) triple pattern P . straightforward show induction height
Q every subpattern Q P , |S 0 JQKG , (Q) = {S 0 }. Hence
JP KG P satisfiable.
Note (P ) blow possible UNION OPT operators,
missing AF-pattern. Hence, AF-pattern P , efficiently compute
(P ) single bottom-up pass P . Morever, C(P ) conjunction possibly negated
equalities constant equalities. well known consistency conjunctions
decided polynomial time (Kroening & Strichman, 2008). Hence, conclude:
Corollary 20. Satisfiability AF-patterns checked polynomial time.
5.2 AF-Reduction Well-Designed Patterns
well-designed pattern defined union union-free well-designed patterns. Since
union satisfiable one terms is, focus union-free patterns
follows. Formally, union-free pattern P called well-designed (Perez et al., 2009)
1. every subpattern P form Q FILTER C, variables mentioned C also
occur Q;
2. every subpattern Q P form Q1 OPT Q2 , every ?x var(Q2 ), ?x
also occurs P outside Q, ?x var(Q1 ).
associate every union-free pattern P AF-pattern (P ) obtained removing
applications OPT right operands; left operand remains place. Formally,
define following:
P triple pattern, (P ) equals P .
P form P1 P2 , (P ) = (P1 ) (P2 ).
P form P1 FILTER C, (P ) = (P1 ) FILTER C.
P form P1 OPT P2 , (P ) = (P1 ).
announced result given following theorem, proved directly
results Perez et al. (2009).4
4. thank anonymous referee offering given proof only-if direction.

418

fiSatisfiability Problem SPARQL

Theorem 21. Let P union-free well-designed pattern. P satisfiable
(P ) is.
Proof. going refer Lemma 4.3 Proposition 4.5 Perez et al. (2009).
Indeed, Lemma 4.3 gives us if-direction Theorem 21. cited paper introduced
notion reduction P 0 E P . Whenever P 0 E P , also (P ) E P 0 (P ) = (P 0 ).
only-if direction, assume P satisfiable, exists G
JP KG . exists P 0 E P Jand(P 0 )KG (Proposition 4.5). Here,
and(P 0 ) denotes pattern obtained P 0 replacing every OPT AND.
(P ) = (P 0 ).
following claim easy verify every union-free pattern P 0 :
Jand(P 0 )KG |var((P 0 )) J(P 0 )KG . claim, obtain J(P 0 )KG nonempty
(P 0 ) = (P ) satisfiable, desired.
Since (P ) efficiently computed P , Theorem Corollary 20
imply:
Corollary 22. Satisfiability union-free well-designed patterns tested polynomial
time.

6. Experimental Evaluation
want evaluate experimentally positive results presented far:
1. Wrong literal reduction (Proposition 24);
2. Satisfiability checking SPARQL(bound, =, 6=c ) SPARQL(bound, 6=, 6=c ) computing (P ) (Theorem 3);
3. Satisifiability checking well-designed patterns, reduction AF-patterns (Proposition 19 Theorem 21).
experiments follow reported earlier Picalausa Vansummeren
(2011). test datasets real-life SPARQL queries, use logs SPARQL endpoint
DBpedia.5 data source contains query dumps year 2012, divided
14 logfiles. chose three logs 20120913, 20120929 20121031
obtain span roughly three months; took sample 100 000 queries
them. typical query log size 75 125 (size measured number
nodes syntax tree). 10% queries log usable
syntax errors use features covered analysis.
implementation tests done Java 7 Windows 7, Intel Core 2
Duo SU94000 processor (1.40GHz, 800MHz, 3MB) 3GB memory (SDRAM DDR3
1067MHz).
tests measure time needed perform analyses SPARQL queries presented
above. timings averaged queries log, experiment repeated
five times smooth accidental quirks operating system. Although give
5. ftp://download.openlinksw.com/support/dbpedia/

419

fiZhang, Van den Bussche, & Picalausa

Table 1: Timings experiments (averaged five repeats). Times ms. Baseline
time read parse 1000 000 queries; WL stands baseline plus time
wrong-literal reduction. (P ) stands WL plus time computing (P ). AF
stands WL, plus testing well-designedness, plus AF-reduction testing
satisfiability (Proposition 19). percentages show increases relative
baseline.
logfile
20120913
20120929
20121031

baseline
39 422
34 281
32 286

WL
41 254
35 868
33 186

5%
5%
3%

(P )
44 395
38 102
34 419

8%
7%
4%

AF
48 329
41 087
36 993

10%
9%
8%

absolute timings, main emphasis percentage time needed analyse
query, respect time needed simply read parse query.
percentage small demonstrates efficient, linear time complexity practice.
turn indeed achieved experiments, shown Table 1.
following subsections discuss results detail.
6.1 Wrong Literal Reduction
Testing removing triple patterns wrong literals pattern P performed
reduction (P ) defined Appendix. definition (P ) clear
computed single bottom-up traversal P indeed borne
experiments. Table 1 shows average, wrong-literal reduction takes 3
5% time needed read parse input.
Interestingly, real-life queries literals wrong position indeed found;
one example following:
SELECT DISTINCT *
{ 49 dbpedia-owl:wikiPageRedirects

?redirectLink .}

6.2 Computing (P )
Section 3 seen satisfiability decidable fragments tested
computing (P ), problem NP-complete. Intuitively, problem
intractable (P ) may size exponential size P . actually occurs
real life; common SPARQL query pattern use many nested OPTIONAL operators
gather additional information strictly required query may may
present. found experiments queries 50 nested OPT operators,
naively would lead (P ) size 250 . shortened example query
shown Figure 1.
practice, however, blowup (P ) avoided follows. Recall Theorem 3 states P satisfiable (P ) nonempty. elements (P )
sets variables. Looking definition (P ), set may removed (P )
420

fiSatisfiability Problem SPARQL

SELECT DISTINCT *
{
?s <http://dbpedia.org/ontology/EducationalInstitution>,
<http://dbpedia.org/ontology/University> .
?s <http://dbpedia.org/ontology/country> <http://dbpedia.org/resource/Brazil> .
OPTIONAL {?s <http://dbpedia.org/ontology/affiliation> ?ontology_affiliation .}
OPTIONAL {?s <http://dbpedia.org/ontology/abstract> ?ontology_abstract .}
OPTIONAL {?s <http://dbpedia.org/ontology/campus> ?ontology_campus .}
OPTIONAL {?s <http://dbpedia.org/ontology/chairman> ?ontology_chairman .}
OPTIONAL {?s <http://dbpedia.org/ontology/city> ?ontology_city .}
OPTIONAL {?s <http://dbpedia.org/ontology/country> ?ontology_country .}
OPTIONAL {?s <http://dbpedia.org/ontology/dean> ?ontology_dean .}
OPTIONAL {?s <http://dbpedia.org/ontology/endowment> ?ontology_endowment .}
OPTIONAL {?s <http://dbpedia.org/ontology/facultySize> ?ontology_facultySize .}
OPTIONAL {?s <http://dbpedia.org/ontology/formerName> ?ontology_formerName .}
OPTIONAL {?s <http://dbpedia.org/ontology/head> ?ontology_head .}
OPTIONAL {?s <http://dbpedia.org/ontology/mascot> ?ontology_mascot .}
OPTIONAL {?s <http://dbpedia.org/ontology/motto> ?ontology_motto .}
OPTIONAL {?s <http://dbpedia.org/ontology/president> ?ontology_president .}
OPTIONAL {?s <http://dbpedia.org/ontology/principal> ?ontology_principal .}
OPTIONAL {?s <http://dbpedia.org/ontology/province> ?ontology_province .}
OPTIONAL {?s <http://dbpedia.org/ontology/rector> ?ontology_rector .}
OPTIONAL {?s <http://dbpedia.org/ontology/sport> ?ontology_sport .}
OPTIONAL {?s <http://dbpedia.org/ontology/state> ?ontology_state .}
OPTIONAL {?s <http://dbpedia.org/property/acronym> ?property_acronym .}
OPTIONAL {?s <http://dbpedia.org/property/address> ?property_address .}
OPTIONAL {?s <http://www.w3.org/2003/01/geo/wgs84_pos#lat> ?property_lat .}
OPTIONAL {?s <http://www.w3.org/2003/01/geo/wgs84_pos#long> ?property_long .}
OPTIONAL {?s <http://dbpedia.org/property/established> ?property_established .}
OPTIONAL {?s <http://dbpedia.org/ontology/logo> ?ontology_logo .}
OPTIONAL {?s <http://dbpedia.org/property/website> ?property_website .}
OPTIONAL {?s <http://dbpedia.org/property/location> ?property_location .}
FILTER ( langMatches(lang(?ontology_abstract), "es") ||
langMatches(lang(?ontology_abstract), "en") )
FILTER ( langMatches(lang(?ontology_motto), "es") ||
langMatches(lang(?ontology_motto), "en") )
}

Figure 1: real-life query many nested OPTIONAL operators, retrieving much
information possible universities Brazil.

421

fiZhang, Van den Bussche, & Picalausa

application FILTER. Hence, variables mentioned FILTER conditions influence emptiness (P ); variables ignored. example,
query Figure 1, two variables appear filter, namely ?ontology abstract
?ontology motto, maximal size (P ) reduced 22 .
experiments, turns typically variables involved filter conditions. Hence, strategy works well practice.
Another practical issue that, paper, considered filter conditions bound checks, equalities, constant-equalities, possibly negated. practice, filter conditions typically apply built-in SPARQL predicates predicate
langMatches Figure 1. experimental purpose testing practicality computing (P ), however, predicates simply treated bound checks. way
apply experiments 70% queries testfiles.
practical adaptations, experiments show computing (P )
efficient: Table 1 shows requires, average, 4 8% time needed
read parse input, timings even include wrong-literal reduction
all, experiments encountered unsatisfiable queries. observation
corrobated findings recent new statistical analysis practical SPARQL usage
(Han, Feng, Zhang, Wang, Rao, & Jiang, 2016). course, users practice
write unsatisfiable expressions good news. Satisfiability remains basic problem
need understand, many problems reduced it.
6.3 Satisfiability Testing Well-Designed Patterns
Section 5 seen testing satisfiability well-designed pattern done
testing satisfiability AF-reduction (Theorem 21). latter done testing
nonemptiness (P ) testing consistency filter conditions (Proposition 19).
Computing AF-reduction done simple bottom-up traversal pattern.
Moreover, AF-pattern P , computing (P ) poses problems since either empty
singleton. far testing consistency filter conditions concerned, experiments
yield rather baffling observation: almost well-designed patterns test sets
filters all. cannot explain phenomenon, implies
able test performance consistency checks real-life SPARQL queries.
Anyhow, Table 1 shows entire analysis wrong-literal reduction, testing
well-designedness, AF-reduction, computing (P ), consistency checking (in
cases latter necessary), incurs 10% increase relative reading
parsing input.
6.4 Scalability
experiments described run sets 100 000 queries each. also
modest scaling experiment varied number queries 5 000 200 000.
Table 2 shows performance scales linearly.
422

fiSatisfiability Problem SPARQL

Table 2: Scalability experiment (times ms). Timings clearly scale linearly increasing
input size.
input size
baseline
WL
(P )
AF

200 000
74 168
77 800
81 730
91 470

100 000
39 422
41 253
44 395
48 329

50 000
21 315
21 876
23 552
26 023

10 000
3 596
3 762
4 016
4 463

5 000
1 851
1 942
2 036
2 254

Pearson coeficient
0.999924005
0.999989454
0.999900948
0.999044542

7. Extension SPARQL 1.1
already mentioned Introduction, SPARQL 1.0 extended SPARQL 1.1
number new operators building patterns. main new features property
paths; grouping aggregates; BIND; VALUES; MINUS; EXISTS EXISTSsubqueries; SELECT. complete analysis SPARQL 1.1 goes beyond scope
present paper. Nevertheless, section, briefly discuss results may
extended new setting.
Property paths provide form regular path querying graphs. aspect graph
querying already extensively investigated, including questions satisfiability
kinds static analysis query containment (Kostylev, Reutter, & Vrgoc, 2014;
Kostylev, Reutter, Romero, & Vrgoc, 2015). Therefore discuss property paths
here.
SPARQL 1.1 features discuss grouped two categories:
cause undecidability, harmless far satisfiability concerned.
begin harmless category.
7.1 SELECT Operator EXISTS-Subqueries
SPARQL 1.1 allows patterns form SELECTS P , finite set variables
P pattern. novelty compared 1.0 applied subexpressions.
semantics projection; already seen Section 4.1.
feature influence satisfiability patterns. Indeed, patterns
extended SELECT operators reduced patterns without said operators.
reduction amounts simply rename variables projected fresh variables
used anywhere else pattern; SELECT operators
removed. resulting, SELECT-free, pattern equivalent original one
omit fresly introduced variables solution mappings final result.
particular, two patterns equisatisfiable.
Example 23. Rather giving formal definition SELECT-reduction formally
stating proving equivalence, give example. Consider pattern P :
(c, p, ?x) OPT ((?x, p, ?y) SELECT?y (?y, q, ?z) SELECT?y (?y, r, ?z))
423

fiZhang, Van den Bussche, & Picalausa

Renaming projected-out variables fresh variables omitting SELECT operators
yields following pattern P 0 :
(c, p, ?x) OPT ((?x, p, ?y) (?y, q, ?z1 ) (?y, r, ?z2 ))
Pattern P 0 equivalent P sense graph G, JP KG = { |
JP 0 KG }, denotes mapping obtained omitting values ?z1
?z2 (if present dom()).
know handle SELECT operators, also handle EXISTSsubqueries. Indeed, pattern P FILTER EXISTS(Q) (with obvious SQL-like semantics)
equivalent SELECTvar(P ) (P Q).
7.2 Features Leading Undecidability
Section 4 seen soon one express union, composition
difference binary relations, satisfiability problem becomes undecidable. Since union
composition readily expressed basic SPARQL (UNION AND), key lies
expressibility difference operator. subsection see various
new features SPARQL 1.1 indeed allow expressing difference.
7.2.1 MINUS Operator EXISTS Subqueries
two features quite obviously used express difference,
dwell further.
7.2.2 Grouping Aggregates
known trick expressing difference using grouping counting (Celko, 2005)
emulated extension SPARQL 1.0 grouping. illustrate technique
example.
Consider query (?x, p, ?y) MINUS (?x, q, ?y) asking pairs (a, b)
(a, p, b) holds (a, q, b) not. express query (with obvious SQL-like
semantics) follows:

SELECT?x,?y (?x, p, ?y) OPT ((?x, q, ?y) (?xx, p, ?yy))
GROUP ?x, ?y
count(?xx) = 0
Note technique looking (?x, ?y) groups zero count ?xx
similar technique used express difference using negated bound constraint (seen
proof Lemma 17).
7.2.3 BIND VALUES
seen Section 4.4 allowing constant equalities filter constraints allows us
emulate difference operator. Two mechanisms introduced SPARQL 1.1, BIND
VALUES, allow introduction constants solution mappings. Together equality
constraints allows us express constant equalities, hence, difference.
424

fiSatisfiability Problem SPARQL

Specifically, using VALUES, express P FILTER ?x = c
SELECTvar(P ) (P VALUES?x (c)).
Using BIND, expressed
SELECTvar(P ) ((P BIND?x0 (c)) FILTER ?x = ?x0 )
?x0 fresh variable. Note use SELECT, which, however, influence
satisfiability discussed above. conclude SPARQL(=) extended BIND,
SPARQL(=) extended VALUES, undecidable satisfiability problem.

8. Conclusion
results paper may summarized saying that, long kinds
constraints allowed filter conditions cannot combined yield inconsistent sets
constraints, satisfiability SPARQL patterns decidable; otherwise, problem undecidable. Moreover, well-designed patterns, satisfiability decidable well.
positive results yield straightforward bottom-up syntactic checks implemented
practice.
thus attempted paint rather complete picture satisfiability problem
SPARQL 1.0. course, satisfiability basic automated reasoning
task. One may move complex tasks equivalence, implication,
containment, query answering ontologies. Indeed, investigations along line
limited fragments SPARQL already happening (Letelier, Perez, Pichler, & Skritek,
2013; Wudage, Euzenat, Geneves, & Layada, 2012; Kollia & Glimm, 2013; Cuenca Grau,
Motik, Stoilos, & Horrocks, 2012) hope work may serve provide
additional grounding investigations.
also note query optimization standard check satisfiability
subexpressions, avoid executing useless code. specific works SPARQL query
optimization (Sequeda & Miranker, 2013; Groppe, Groppe, & Kolbaum, 2009) mention
inconsistent constraints cause unsatisfiability, provided sound
complete characterizations satisfiability, like offered paper. Thus,
results useful direction well.

Acknowledgment
thank anonymous referees, original submission revised
submission, critical comments, encouraged us significantly improve
paper. work funded grant G.0489.10 Research Foundation Flanders
(FWO).

Appendix A.
Literals wrong place triple patterns easily dealt following manner.
define wrong-literal reduction pattern P , denoted (P ), set either
empty singleton containing single pattern P 0 :
425

fiZhang, Van den Bussche, & Picalausa

P triple pattern (u, v, w) u literal, (P ) := ; else (P ) := {P }.
(P1 UNION P2 ) := (P1 ) (P2 ) (P1 ) (P2 ) empty;
(P1 UNION P2 ) := {P10 UNION P20 | P10 (P1 ) P20 (P2 )} otherwise.
(P1 P2 ) := {P10 P20 | P10 (P1 ) P20 (P2 )}.
(P1 OPT P2 ) := (P1 ) empty;
(P1 OPT P2 ) := (P1 ) (P2 ) empty (P1 ) nonempty;
(P1 OPT P2 ) := {P10 OPT P20 | P10 (P1 ) P20 (P2 )} otherwise.
(P1 FILTER C) := {P10 FILTER C | P10 (P1 )}.
Note wrong-literal reduction never literal subject position triple
pattern. next proposition shows that, far satisfiability checking concerned,
may always perform wrong-literal reduction.
Proposition 24. Let P pattern. (P ) empty P unsatisfiable; (P ) =
{P 0 } P P 0 equivalent, i.e., JP KG = JP 0 KG every RDF graph G. Moreover,
(P ) = {P 0 } P 0 contain triple pattern (u, v, w) u literal.
Proof. Assume P triple pattern (u, v, w) u literal, (P ) = . Since u
constant, (u) equals literal u every solution mapping . Since triple
RDF graph literal first position, JP KG empty every RDF graph G,
i.e., P unsatisfiable. u literal, (P ) = {P } claims Proposition
trivial.
P form P1 UNION P2 , P1 P2 , P1 FILTER C, claims
Proposition follow straightforwardly induction.
P form P1 OPT P2 , three cases consider.
(P1 ) empty (P ). case, induction, P1 unsatisfiable,
whence P .
(P1 ) = {P10 } nonempty (P2 ) empty, (P ) = {P10 }. induction,
P2 unsatisfiable. Hence, P equivalent P1 , turn equivalent P10
induction. P10 contain triple pattern literal first position
follows induction.
(P1 ) = {P10 } (P2 ) = {P20 } nonempty, (P ) = P10 OPT P20 .
induction, P1 equivalent P10 P2 P20 . Hence, P equivalent
P10 OPT P20 desired. induction, neither P10 P20 contain triple pattern
literal first position, neither P10 OPT P20 .

426

fiSatisfiability Problem SPARQL

References
Abiteboul, S., Hull, R., & Vianu, V. (1995). Foundations Databases. Addison-Wesley.
Andreka, H., Givant, S., & Nemeti, I. (1997). Decision problems equational theories
relational algebras, Vol. 126 Memoirs. AMS.
Angles, R., & Gutierrez, C. (2008). expressive power SPARQL. Sheth, A., Staab,
S., et al. (Eds.), Proceedings 7th International Semantic Web Conference, Vol. 5318
Lecture Notes Computer Science, pp. 114129. Springer.
Arenas, M., Conca, S., & Perez, J. (2012). Counting beyond Yottabyte, SPARQL
1.1 property paths prevent adoption standard. Mille, A., et al. (Eds.),
Proceedings 21st World Wide Web Conference, pp. 629638. ACM.
Arenas, M., & Perez, J. (2011). Querying semantic web data SPARQL. Proceedings
30st ACM Symposium Principles Databases, pp. 305316. ACM.
Arenas, M., Perez, J., & Gutierrez, C. (2009). semantics SPARQL. De Virgilio,
R., Giunchiglia, F., & Tanca, L. (Eds.), Semantic Web Information ManagementA
Model-Based Perspective, pp. 281307. Springer.
Celko, J. (2005). SQL Smarties: Advanced SQL Programming (Third edition). Elsevier.
Cuenca Grau, B., Motik, B., Stoilos, G., & Horrocks, I. (2012). Completeness guarantees
incomplete ontology reasoners: Theory practice. Journal Artificial Intelligence
Research, 43, 419476.
Groppe, J., Groppe, S., & Kolbaum, J. (2009). Optimization SPARQL using coreSPARQL. Cordeiro, J., & Filipe, J. (Eds.), Proceedings 11th International Conference
Enterprise Information Systems, pp. 107112.
Han, X., Feng, Z., Zhang, X., Wang, X., Rao, G., & Jiang, S. (2016). statistical
analysis practical SPARQL patterns. Proceedings 19th International Workshop
Web Databases.
Harris, S., & Seaborne, A. (2013). SPARQL 1.1 query language. W3C Recommendation.
Kollia, I., & Glimm, B. (2013). Optimizing SPARQL query answering OWL ontologies.
Journal Artificial Intelligence Research, 48, 253303.
Kostylev, E., Reutter, J., Romero, M., & Vrgoc, D. (2015). SPARQL property paths.
Arenas, M., Corcho, O., Simperl, E., Strohmaier, M., et al. (Eds.), Proceedings
14th International Semantic Web Conference, Vol. 9366 Lecture Notes Computer
Science, pp. 318. Springer.
Kostylev, E., Reutter, J., & Vrgoc, D. (2014). Containment data graph queries.
Proceedings 17th International Conference Database Theory. ACM.
Kroening, D., & Strichman, O. (2008). Decision Procedures. Springer.
Letelier, A., Perez, J., Pichler, R., & Skritek, S. (2013). Static analysis optimization
semantic web queries. ACM Transactions Database Systems, 38 (4), article 25.
Perez, J., Arenas, M., & Gutierrez, C. (2009). Semantics complexity SPARQL. ACM
Transactions Database Systems, 34 (3), article 16.
427

fiZhang, Van den Bussche, & Picalausa

Picalausa, F., & Vansummeren, S. (2011). real SPARQL queries like?. De Virgilio, R., Giunchiglia, F., & Tanca, L. (Eds.), Proceedings International Workshop
Semantic Web Information Management, No. 7. ACM Press.
Polleres, A. (2007). SPARQL rules (and back). Williamson, C., Zurko, M., et al.
(Eds.), Proceedings 16th World Wide Web Conference, pp. 787796. ACM.
Prudhommeaux, E., & Seaborne, A. (2008). SPARQL query language RDF. W3C
Recommendation.
Schmidt, M., Meier, M., & Lausen, G. (2010). Foundations SPARQL query optimization.
Proceedings 13th International Conference Database Theory, pp. 433. ACM.
Sequeda, J., & Miranker, D. (2013). Ultrawrap: SPARQL execution relational data. Web
Semantics, 22, 1939.
Tan, T., Van den Bussche, J., & Zhang, X. (2014). Undecidability satisfiability algebra finite binary relations union, composition, difference. arXiv:1406.0349.
Wudage, M., Euzenat, J., Geneves, P., & Layada, N. (2012). SPARQL query containment
SHI axioms. Proceedings 26th AAAI Conference Artificial Intelligence,
pp. 1016.

428

fiJournal Artificial Intelligence Research 56 (2016) 547-571

Submitted 01/16; published 08/16

Research Note
Time-Bounded Best-First Search Reversible Non-reversible
Search Graphs
Carlos Hernandez

CARLOS . HERNANDEZ . U @ UNAB . CL

Departamento de Ciencias de la Ingeniera,
Universidad Andres Bello,
Santiago, Chile

Jorge A. Baier

JABAIER @ ING . PUC . CL

Departamento de Ciencia de la Computacion
Pontificia Universidad Catolica de Chile
Santiago, Chile

Roberto Asn

RASIN @ UCSC . CL

Departamento de Ingeniera Informatica
Universidad Catolica de la Santsima Concepcion
Concepcion, Chile

Abstract
Time-Bounded A* real-time, single-agent, deterministic search algorithm expands
states graph order A* does, unlike A* interleaves search action execution. Known outperform state-of-the-art real-time search algorithms based Korfs Learning
Real-Time A* (LRTA*) benchmarks, studied detail sometimes
considered true real-time search algorithm since fails non-reversible problems even
goal still reachable current state. paper propose study Time-Bounded
Best-First Search (TB(BFS)) straightforward generalization time-bounded approach
best-first search algorithm. Furthermore, propose Restarting Time-Bounded Weighted A* (TBR
(WA*)), algorithm deals adequately non-reversible search graphs, eliminating
backtracking moves incorporating search restarts heuristic learning. non-reversible
problems prove TB(BFS) terminates deduce cost bounds solutions returned
Time-Bounded Weighted A* (TB(WA*)), instance TB(BFS). Furthermore, prove TBR
(WA*), reasonable conditions, terminates. evaluate TB(WA) grid pathfinding
15-puzzle. addition, evaluate TBR (WA*) racetrack problem. compare
algorithms LSS-LRTWA*, variant LRTA* exploit lookahead search weighted
heuristic. general observation performance TB(WA*) TBR (WA*) improves weight parameter increased. addition, time-bounded algorithms almost
always outperform LSS-LRTWA* significant margin.

1. Introduction
many search applications, time scarce resource. Examples range video game path
finding, handful milliseconds given search algorithm controlling automated
characters (Bulitko, Bjornsson, Sturtevant, & Lawrence, 2011), highly dynamic robotics (Schmid,
Tomic, Ruess, Hirschmuller, & Suppa, 2013). settings, usually assumed standard
search algorithm able compute complete solution action required,
thus execution search must interleaved.
c
2016
AI Access Foundation. rights reserved.

fiH ERN ANDEZ , BAIER , &

Time-Bounded A* (Bjornsson, Bulitko, & Sturtevant, 2009) algorithm suitable searching tight time constraints. nutshell, given parameter k, runs standard A* search
towards goal rooted initial state, k expansions completed, move performed search, still needed, resumed. move computed follows. agent
path found A* root node best node b search frontier
agent moved towards b following path . Otherwise, performs backtracking move, returning agent previous state. algorithm always terminates agent goal
state, problem solution.
Time-Bounded A* algorithm relevant real-time search community.
significantly superior well-known real-time heuristic search algorithms applications.
Indeed Hernandez, Baier, Uras, Koenig (2012) showed significantly outperforms state-of-theart real-time heuristic search algorithms RTAA* (Koenig & Likhachev, 2006) daRTAA*
(Hernandez & Baier, 2012) pathfinding.
relatively new algorithm, Time-Bounded A* studied deeply literature. One reasons perhaps inability adequately deal non-reversible
problems. Indeed, non-reversible problems real-time search algorithm fail soon
algorithm led agent dead-end state; i.e., one goal unreachable. TimeBounded A*, however, additional failure condition: always fail soon backtrack
move required unreversible action. Thus class problems cannot solve limited compared real-time search algorithms, like, example, well-known LRTA* (Korf,
1990). reason, Time-Bounded A* sometimes excluded experimental comparisons
real-time search algorithms (see e.g. Burns, Ruml, & Do, 2013, p. 725).
paper extend time-bounded search approach two directions. already noted
authors (Bjornsson et al., 2009), time-bounded approach limited A*. first
contribution paper study implications using search algorithms
instead A*. Specifically, generalize Time-Bounded A* Time-Bounded Best-First Search.
general, instance Best-First Search, call TB(A) algorithm results
applying time-bounded approach A. second contribution paper extension
time-bounded search approach allows algorithm deal adequately non-reversible
problems. algorithm propose here, Restarting Time-Bounded Weighted A*which call
TBR (WA*), seen lying middle ground time-bounded algorithms
learning-based real-time search algorithms like Korfs Learning Real-Time A* (LRTA*) (1990).
fact, TBR (WA*) restarts search current state backtracking move available
updates heuristic function.
carry theoretical analysis Time-Bounded Weighted A* (TB(WA*)), instance
TB(BFS), TBR (WA*). TB(WA*) establish upper lower bounds
solution cost. cost bound establishes that, domains, solution cost may reduced
significantly increasing w without increasing search time; hence, contrast wellknown Weighted A* solving offline search problems, might obtain better solutions
increasing weight. result important since suggests TB(WA*) (with w > 1)
preferred TB(A*) domains WA* runs faster A*. WA*
always run faster A* (see e.g., Wilt & Ruml, 2012), known many situations.
Experimentally, evaluate TB(WA*) pathfinding benchmarks 15-puzzle,
TBR (WA*) racetrack problem. three benchmarks observe performance improvement w increased. addition, observe TB(WA*) significantly superior TB(A*)
548

fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHS

LSS-LRTWA* (Rivera, Baier, & Hernandez, 2015), real-time search algorithm use
weighted heuristics.
paper extends work appears conference proceedings (Hernandez, Asn, & Baier,
2014), including empirical analysis new benchmarks (Counter Strike Maps, racetrack,
15-puzzle), extending pathfinding experiments 16-neighbor connectivity, providing lower bound cost solution returned TB(WA*) (Theorem 2, below),
introducing, analyzing, evaluating TBR (WA*).
rest paper organized follows. start describing background needed
rest paper. describe TB(BFS) TBR (BFS), including formal analysis
properties. describe experimental results, finish summary
perspectives future research.

2. Background
describe background rest paper.
2.1 Search Reversible Non-reversible Environments
search graph tuple G = (S, A), finite set states, set edges
represent actions available agent state. path graph (S, A)
sequence states = s0 s1 sn , (si , si+1 ) A, {0, . . . , n 1}, s0 = s,
sn = t. say successor (s, t) edge A. Moreover, every
define Succ(s) = {t | (s, t) A}.
cost function c search graph (S, A) c : P
R+ ; i.e., associates action
positive cost. cost path = s0 s1 sn c() = n1
i=0 c(si , si+1 ), i.e. sum
costs edge considered path. cost-optimal path one
lowest cost among paths t; denote cost c (s, t). addition, denote
cT (s, t) cost cost-optimal path visits states , is,
cost-optimal path = s1 s2 . . . sn = s1 , sn = t, si , {2, . . . , n 1}.
search problem tuple (S, A, c, sstart , sgoal ) G = (S, A) search graph, sstart
sgoal states S, c cost function G. search graph G = (S, A) reversible
symmetric; is, whenever (s, t) (t, s) A. search problem reversible
search graph reversible. Consequently, problem non-reversible search graph
contains action (s, t) contain action (t, s).
solution search problem path sstart sgoal .
2.2 Best-First Search
Best-First Search (BFS) (Pearl, 1984) encompasses family search algorithms static environments associate evaluation function f (s) every state s. priority
f (s) < f (t) viewed promising node t. BFS starts initializing
priority states search space infinity, except sstart , priority set
f (sstart ). priority queue Open initialized containing sstart . iteration, algorithm
extracts Open state lowest priority, s. successor computes evaluation fs (t), considering path found s. fs (t) lower f (t),
549

fiH ERN ANDEZ , BAIER , &

added Open f (t) set fs (t). algorithm repeats process sgoal Open
lowest priority.
pseudo code presented Algorithm 1. f -value state usually implemented
attribute s, Open list implemented priority list. Furthermore, assume
cost fs (t) computed Line 13 function path via s. Thus fs (t) take finite
number values execution BFS, depends (finite) number simple
paths connect initial state s.
Algorithm 1: Best-First Search

16
17

sroot scurrent
Open
foreach
f (s)
f (sroot ) evaluation sroot
Insert sroot Open
Open 6=
Let state minimum f -value Open
= sgoal
return
Remove Open
foreach Succ(s)
fs (t) evaluation function considering discovered
fs (t) < f (t)
f (t) fs (t)
parent(t)
Insert Open

18

return solution

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

instance Best-First Search Weighted A* (WA*) (Pohl, 1970). WA* computes evaluation function terms two functions, g h. g-value corresponds cost
lowest-cost path found far towards s, implemented attribute s. WA*s evaluation function defined f (s) = g(s) + wh(s), g(s) cost lowest-cost path
found sstart s. addition, h non-negative, user-given heuristic function h(s)
estimates cost path sgoal . Finally, w real number greater equal 1.
pseudo-code WA* obtained Algorithm 1 storing g-value attribute state, h value computed external function. resulting pseudo-code
appears Algorithm 2.
heuristic function h admissible h(s) c (s, sgoal ), S. Function
h consistent h(sgoal ) = 0, h(s) c(s, t) + h(t) every edge (s, t) search graph.
Consistency implies path h(s) c() + h(t), which, turn, implies
admissibility.
BFSs closed listdenoted henceforth Closed defined set states
Open g(s) infinity.1 words, contains states
path known considered re-expansion.
1. BFS initially sets f (s) infinity every start node. WA* translates setting g(s) infinity
except sstart .

550

fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHS

Algorithm 2: Weighted A*

19
20

sroot scurrent
Open
foreach
g(s)
f (s)
g(sroot ) 0
f (sroot ) wh(sroot )
Insert sroot Open
Open 6=
Let state minimum f -value Open
= sgoal
return
Remove Open
foreach Succ(s)
gs,t = min{g(t), g(s) + c(s, t)}
gs,t < g(t)
g(t) gs,t
f (t) g(t) + wh(t)
parent(t)
Insert Open

21

return solution

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

h admissible, WA* known find solution whose cost cannot exceed wc (sstart , sgoal ).
such, WA* may return increasingly worse solutions w increased. advantage increasing w search time usually decreased fewer states expanded. w = 1,
WA* equivalent A* (Hart, Nilsson, & Raphael, 1968). Another interesting result generalizes
well-known property consistent heuristics A* algorithm. formally stated follows:
Lemma 1 (Ebendt & Drechsler, 2009) every moment execution Weighted A*
state sroot , h consistent, upon expansion state (Line 14 Algorithm 2), holds
g(s) wc (sroot , s).
Another instance Best-First Search Greedy Best-First Search (GBFS). f equal
user-given heuristic function h. WA* used sufficiently large value w,
WA* GBFS rank nodes similar way. Indeed, let fGBFS fWA* denote, respectively, f
function GBFS WA*. w exceeds g-value every node ever generated
two nodes s1 s2 generated g-value algorithms
fGBFS (s1 ) = h(s1 ) > h(s2 ) = fGBFS (s2 ), hold fWA* (s1 ) > fWA* (s2 ). However,
even w sufficiently large, reverse always true since fWA* (s1 ) > fWA* (s2 ) hold
true h(s1 ) = h(s2 ), g-value fWA* acts practice tie breaker.
2.3 Real-Time Heuristic Search
real-time search objective solve search problem subject additional real-time
constraint. constraint, constant amount time (independent problem size) given
search algorithm, end expected perform one actions
sequence. constant small relation time would required offline
551

fiH ERN ANDEZ , BAIER , &

search algorithm solve search problem. performing actions agent reached
goal, process repeats. iteration algorithm understood two consecutive
episodes: (1) search episode, path computed, (2) execution episode,
actions path performed.
Rather receiving time limit seconds, real-time search algorithms receive parameter, say k, guarantee computational time taken search episode bounded
non-decreasing function k. example real-time search algorithm Local Search-Space,
Learning Real-Time A* (LSS-LRTA*; Algorithm 3) (Koenig & Sun, 2009). receives search
problem P parameter k. search episode, runs bounded execution A* rooted
current state expands k states. Following, updates heuristic values
states closed list A* run. update, usually referred learning step, makes h
informed, guarantees following holds every A*s closed list:
h(s) = min {cClosed (s, t) + h(t)}.
tOpen

(1)

execution episode performs actions appear path found A* current
state towards state lowest f -value open list. reversible search spaces h
Algorithm 3: LSS-LRTA*
1
2
3
4
5
6
7
8

Input: search problem P natural number k
sstart
goal state
run A* k states expanded goal node best state Open
best state A*s closed list lowest f -value
Closed
update h-value Equation 1 holds
move along path found A* best
best,

initially consistent shown LSS-LRTA* terminates search problem
solution (Koenig & Sun, 2009). search space non-reversible, however, termination cannot
guaranteed. see later, time-bounded algorithms (without restarts) prove solution
exist well. property hold algorithms whose search expands nodes
whose distance current state bounded, like LSS-LRTA*.
2.4 Comparing Two Real-Time Search Algorithms
One way frequently used literature compare two real-time search algorithms B
comparing cost returned paths algorithms configured way
search episodes approximately duration. Assume real-time search algorithm
requires n search episodes solve search problem runtime . say
average time per search episode run /n.
evaluate relative performance two algorithms B use set benchmark
problems P set algorithm parameters. parameter algorithm A, obtain
record average solution cost problems P average time per episode.
likewise B plot average solution cost versus average time per episode
algorithm. curve algorithm always top curve algorithm B clearly
552

fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHS

state B superior A, B returns better-quality solutions comparable search time
per episode.
Another approach used compare real-time search algorithms Game Time
Model (Hernandez et al., 2012). model, time partitioned uniform time intervals.
agent execute one movement time interval, search movements done
parallel. objective move agent start location goal location
time intervals possible. game time model motivated video games. Video games often
partition time game cycles, couple milliseconds long (Bulitko et al.,
2011). using Game Time Model, implementation real-time search algorithm
modified stop search soon units timewhere parameterhave passed.

3. Time-Bounded Best-First Search
Time-Bounded A* (TB(A*), Bjornsson et al., 2009) real-time search algorithm based A*.
Intuitively, TB(A*) understood algorithm runs A* search sstart sgoal
alternates search phase execution phase goal reached. search phase
bounded number states expanded using A*. execution phase two cases.
agent path sstart best state Open, forward movement
path performed. Otherwise, algorithm performs backtracking moves agent
moved state came from. search phase execute path connecting
sstart sgoal already found. algorithm terminates agent reached
goal.
generalization TB(A*) Time-Bounded Best-First Search, simply replaces A*
TB(A*) Best-First Search. pseudo code shown Algorithm 4. parameters
search problem (S, A, c, sstart , sgoal ), integer k refer lookahead
parameter.
TB(BFS) uses variable scurrent store current state agent. MoveToGoal procedure (called Main) implements loop alternates search execution. initialization
(Lines 2728) scurrent initialized sstart , and, among things, BFSs Open list set
contain sstart only. goal state reached (represented fact variable
goalF ound false), bounded version BFS called (Line 31) expands k states,
computes path sstart state Open minimizes evaluation function f .
path built quickly following parent pointers, stored variable path. execution phase (Lines 3236), current position agent, scurrent , path, agent
performs action determined state immediately following scurrent path. Otherwise,
backtracking move implemented moving agent parent search tree BFS,
parent(scurrent ). use backtracking moves mechanism guarantees agent
eventually reach state variable path because, worst case, agent eventually reach
sstart . soon state reached agent start moving towards state believed
closest goal.
Algorithm 4 equivalent TB(A*) BFS replaced A*. Finally, call TimeBounded Greedy Best-First Search (TB(GBFS)) algorithm results use Greedy
Best-First Search instead BFS.
Note length path cannot general bounded constant size
problem. bound computation search episode use technique described
553

fiH ERN ANDEZ , BAIER , &

Algorithm 4: Time-Bounded Best-First Search
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

procedure InitializeSearch()
sroot scurrent
Open
foreach
f (s)
f (sroot ) evaluation sroot
Insert sroot Open
goalFound false
function Bounded-Best-First-Search()
expansions 0
Open 6= expansions < k f (sgoal ) > mintOpen f (t)
Let state minimum f -value Open
Remove Open
foreach Succ(s)
Compute fs (t) considering discovered s.
fs (t) < f (t)
f (t) fs (t)
parent(t)
Insert Open
expansions expansions + 1
Open = return false
Let sbest state minimum priority Open.
sbest = sgoal goalFound true
path path sroot sbest
return true
function MoveToGoal()
scurrent sstart
InitializeSearch()
scurrent 6= sgoal
goalFound = false
Bounded-Best-First-Search() = false return false
scurrent path
scurrent state scurrent path
else
scurrent parent(scurrent );
Execute movement scurrent
return true
procedure Main
MoveToGoal() = true
print(the agent goal state)
else
print(no solution)

554

fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHS

Bjornsson et al. (2009), whereby additional counter (analogous k used measure
effort path extraction). omitted pseudocode clarity.
3.1 Properties
analyze interesting properties algorithms proposed. First,
like TB(A*), TB(BFS) always terminates finds solution one exists. important
property since many real-time heuristic search algorithms (e.g., LSS-LRTA*) enter infinite loop
unsolvable problems. Second, prove upper lower bound cost solutions
returned TB(WA*). bound interesting since suggests increasing w one might
obtain better solutions rather worse.
Theorem 1 TB(BFS) move agent goal state given reversible search problem P
solution P exists. Otherwise, eventually print solution.
Proof: Follows fact Best-First Search eventually finds path towards goal.
fact search space finite state inserted Open
finite number times. addition, moves carried algorithm (including moving
parent(s)) executable reversible search space.

important note reason TB(BFS) eventually print solution
unsolvable problem dependent fact Open list used. LSS-LRTA* cannot always
detect unsolvable problems search expand locality around current state.
characteristic agent-centered search algorithms (Koenig, 2001), class algorithms
TB(BFS) member of.
following two lemmas intermediate results allow us prove upper bound
cost solutions obtained TB(WA*). results apply TB(A*) knowledge
Lemma 2 Theorem 2 proven TB(A*).
results below, assume P = (S, A, c, sstart , sgoal ) reversible search problem,
TB(WA*) run parameter w 1 h admissible heuristic. Furthermore,
assume c+ = max(u,v)A c(u, v), c = min(u,v)A c(u, v), N (w) number
expansions needed WA* solve P . Finally, assume k N (w) reasonable
assumption given real-time setting.
Lemma 2 cost moves incurred agent controlled TB(WA*) goalFound
becomes true bounded b N (w)1
cc bounded b N (w)1
cc+ .
k
k
Proof: N (w) 1 states expanded goalFound becomes true. k states expanded
per call search procedure, clearly b N (w)1
c number calls Best-Firstk
Search terminates without setting goalFound true. move costs least c c+ ,
result follows.

focus cost incurred complete path found. following Lemma
related property enjoyed TB(A*) stated Theorem 2 Hernandez et al. (2012).
Lemma 3 cost moves incurred agent controlled TB(WA*) goalFound
become true cannot exceed 2wc (sstart , sgoal ).
555

fiH ERN ANDEZ , BAIER , &

Proof: Assume goalFound become true. Let path starts sstart , ends
scurrent defined following parent pointers back sstart . Path prefix
path lowest f -value state previous run WA* therefore, Lemma 1,
c() < wc (sstart , sgoal ). worst case terms number movements necessary reach
goal path coincide sstart . case, agent backtrack
way back sstart . sstart reached, agent move goal path cost
wc (sstart , sgoal ). Thus agent may incur cost higher 2wc (sstart , sgoal ) reach
goal.

obtain lower bound upper bound solution cost TB(WA*)
follows straightforwardly two previous lemmas.
Theorem 2 Let C solution cost obtained TB(WA*). Then,
b

N (w) 1
N (w) 1 +
cc C b
cc + 2wc (sstart , sgoal ).
k
k

Proof: put together inequalities implied Lemmas 2 3.



first observation result shown empirically domains,
w increased, N (w) may decrease substantially. Gaschnig (1977), example, reports
8-puzzle N (1) exponential depth solution whereas N (w), large w
subexponential d. domains like grid pathfinding, well known using high values
w results substantial reductions expanded nodes (see e.g., Likhachev, Gordon, & Thrun,
2003). Thus, increasing w, lower bound first term upper bound may
decrease substantially. second term upper bound, 2wc (sstart , sgoal ), increasing
w, may increase linearly w. suggests situations better- rather
worse-quality solutions may found w increased. see later, confirmed
experimental evaluation.
second observation bounds factor b(N (w) 1)/kc decreases k increases. suggests k large (i.e., close N (w)), increasing w may actually lead
decreased performance.
Putting observations together, Theorem 2 suggests TB(WA*) produce better solutions TBA* k relatively small problems WA* expands fewer nodes
A* offline mode. Problems WA* expand fewer nodes A* exist (Wilt &
Ruml, 2012).
Finally, hard see Theorem 2 generalized algorithms provide
optimality guarantees. Given two search algorithms B provide bounds whose
relative performance known, theorem used predictor relative performance
TB(A) versus TB(B).
3.2 Non-reversible Search Problems via Restarting
non-reversible problems, well-known real-time heuristic search algorithms LSS-LRTA*
fail when, execution episode, state path goal visited.
Time-bounded algorithms like TB(BFS) fail condition also
fail soon physical backtrack required non-reversible action. second condition
556

fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHS

failure reason sometimes time-bounded algorithms discarded use nonreversible domains. objective section propose time-bounded algorithm that,
used non-reversible problems, fail due latter condition, due former.
modification TB(WA*) non-reversible problems comes incorporating
two key characteristics real-time search algorithms like LSS-LRTA*: search restarts heuristic
updates. Indeed, whenever physical backtracking available, or, generally,
predefined restart condition holds, algorithm restarts search. addition, avoid getting
trapped infinite loops, algorithm updates heuristic using update rule LSSLRTA*. call resulting algorithm Restarting Time-Bounded Weighted A* (TBR (WA*)).
Algorithm 5 shows details TBR (WA*). Lines 1012 relevant difference
previous algorithm. algorithm restarts search agent path certain
restart condition, must become true action leading current state
(scurrent ) parent (parent(scurrent )).
Algorithm 5: Restarting Time-Bounded Weighted A*
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

function MoveToGoal()
scurrent sstart
InitializeSearch()
scurrent 6= sgoal
goalFound = false
Bounded-WA*() = false return false;
scurrent path
scurrent state scurrent path
Execute movement scurrent
else restart condition holds
Update heuristic function h using LSS-LRTA* update rule (Equation 1)
InitializeSearch()
else
scurrent parent(scurrent );
Execute movement scurrent
return true
procedure Main
MoveToGoal() = true
print(the agent goal state)
else
print(no solution)

Note prior restarting algorithm updates heuristic LSS-LRTA* would.
implemented version Dijkstras algorithm. Note number states may
need updated may bounded constant. needed, compute update
incremental manner, across several episodes. refer reader analysis Koenig
Sun (2009), Hernandez Baier (2012) details implementation proofs
correctness.
3.2.1 ERMINATION TBR (WA*)
TBR (WA*) used reversible non-reversible domains. heuristic function h
initially consistent search graph strongly connected, algorithm terminates.
557

fiH ERN ANDEZ , BAIER , &

Theorem 3 Let P search problem strongly connected search graph. TBR (WA*),
run consistent heuristic h, finds solution P .
proof Theorem 3 depends intermediate results, proofs
appear elsewhere. following result establishes h consistent, remains consistent
updated.
Lemma 4 (Koenig & Sun, 2009) h consistent remains consistent h updated
Equation 1.
Another intermediate results says h cannot decrease update following Equation 1.
Lemma 5 (Koenig & Sun, 2009) h initially consistent h(s), every s, cannot decrease
h updated following Equation 1.
Another intermediate result says h(s) finitely converges, intuitively means even
wanted apply infinite number updates h, point on, h change
anymore.
Definition 1 (Finite Convergence) series functions {fi }i0 finitely converges function f
exists n every n, holds fm = f . addition, say series
functions {fi }i0 finitely converges exists function f finitely converges.
Lemma 6 Let h0 consistent heuristic function P strongly connected graph. Let
= {hi }i0 hk+1 function results (1) assigning hk hk+1 (2)
updating hk+1 using Equation 1, set Closed Open generated bounded Weighted
A* run rooted arbitrary state. finitely converges.
Proof: first observation hk (s) bounded positive number every
every k. Indeed, Lemma 4, hk consistent, thus admissible, every k.
addition, problem solution, hk (s) c (s, sgoal ), every every k.
second observation set h-values state take finite, even
infinite. Formally prove H(s) = {hk (s) | k 0} finite set. Indeed, hard verify
induction (we leave exercise reader) using Equation 1, every k 0,
holds hk (s) = c(ks ) + h0 (s0 ) some, possibly empty path ks originating finishing
s0 . recall hk (s) bounded observe finitely many paths
graph whose cost bounded. conclude H(s) finite set, every s.
proof follows contradiction, assuming finitely converge.
non-decreasing (Lemma 5), possibility increases infinitely often. implies
least one state H(s) infinite: contradiction. conclude finitely
converges.

Note previous lemma saying anything function converges to;
need know function rest proof. last intermediate result
related result Ebendt Drechsler (2009) stated Section 2.2 (Lemma 1).
Lemma 7 every moment execution Weighted A* state sroot , h consistent,
every state open list, holds g(s) wcClosed (sroot , s).
558

fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHS

Proof: Let cost-optimal path sroot visits states Closed. Let s0
state precedes . s0 part optimal path have:
cClosed (sroot , s0 ) + c(s0 , s) = cClosed (sroot , s).

(2)

successor s0 , holds that:
g(s) g(s0 ) + c(s0 , s).

(3)

g(s0 ) wc (sroot , s0 ),

(4)

g(s) wc (sroot , s0 ) + c(s0 , s).

(5)

g(s) wcClosed (sroot , s0 ) + wc(s0 , s) = w(cClosed (sroot , s0 ) + c(s0 , s)).

(6)

Lemma 1, that:

Inequalities 3 4 imply:
w > 0 cClosed c :

Substituting Equation 2 that:
g(s) wcClosed (sroot , s),

(7)


finishes proof.
provide proof main result section.

Proof (of Theorem 3) : Let us assume algorithm terminate thus enters infinite
loop. Note means algorithm restarts infinite number times (otherwise, Weighted A*
would eventually find goal state, allowing agent reach goal). Assume moment
infinite execution h converged (we know Lemma 6), let s1 s2 . . .
infinite sequence states si state search restarted. prove
every i, h(si ) > h(si+1 ).
Let denote contents open list exactly algorithm expanded si+1 , Closed
denote contents closed list immediately heuristic updated. Equation 1,
following holds:
h(si ) = cClosed (si , ) + h(sO ),



(8)

rewrite Equation 8 as:
wh(si ) = wcClosed (si , ) + wh(sO ),

(9)

Let g(sO ) denote g-value exactly si+1 preferred expansion . Now,
prove wcClosed (si , ) g(sO ). Indeed, Closed follows Lemma 1
fact cClosed c w 1. hand, Open, obtain
wcClosed (si , ) g(sO ) Lemma 7. use fact write:
wh(si ) g(sO ) + wh(sO ).
559

(10)

fiH ERN ANDEZ , BAIER , &

algorithm preferred expand si+1 instead , g(sO ) + wh(sO ) g(si+1 ) +
wh(si+1 ), hence:
wh(si ) g(si+1 ) + wh(si+1 ).
(11)
Finally, w > 0 g(si+1 ) > 0 obtain h(si ) > h(si+1 ).
implies sequence states s1 s2 . . . strictly decreasing h-values.
state space finite, must case si = sj , j 6= j, would
lead conclude h(si ) > h(si ), contradiction.


4. Experimental Results
section presents experimental results. objective experimental evaluation
understand effect weight configuration performance TB(WA*) TBR
(WA*). end, evaluate TB(WA*) reversible search problems (grid pathfinding
15-puzzle), TBR (WA*) non-reversible problem (the racetrack). reference, compare
LSS-LRTWA* (Rivera et al., 2015), version LSS-LRTA* uses Weighted A* rather
A* search phase. used algorithm since among real-time search
algorithms able exploit weights search. LSS-LRTWA* configured perform
single action execution phase.
decided include results WLSS-LRTA* (Rivera et al., 2015), another real-time
search algorithm exploits weights, two reasons. First, new results focused relatively large lookahead values (over 128). lookahead values, Rivera et al. (2015),
grid-like terrain, observe improvements significant. Second, observed that,
15-puzzle, WLSS-LRTA* yields worse performance w increased.
Section 4.1 report results 8- 16-neighbor grids similar manner reported
earlier publication (Hernandez et al., 2014). Section 4.2 reports results 8- 16-neighbor
grids using Game Time Model (cf. Section 2.4). Section 4.3 reports results non-reversible
maps deterministic version setting used evaluate algorithms Stochastic ShortestPath problem (Bonet & Geffner, 2003). Finally, Subsection 4.4 reports results 15-puzzle.
path-finding tasks Section 4.1 Section 4.2 evaluated using 8-neighbor (Bulitko
et al., 2011; Koenig & Likhachev, 2005) and16-neighbor
grids (Aine & Likhachev, 2013) (see
Figure 5). costs movements 1, 2, 5 for, respectively, orthogonal, diagonal,
chess-knight movements. implementation agent cannot jump obstacles. addition, diagonal movement (d, d) (for {1, 1}) illegal (x, y) either (x+d, y) (x, +d)
obstacle. 8-neighbor 16-neighbor grids use octile distance Euclidean
distance heuristic values, respectively. experiments run Intel(R) Core(TM) i72600 @ 3.4Ghz machine, 8Gbytes RAM running Linux. algorithms common
code base use standard binary heap Open. Ties Open broken favor larger
g-values; rule breaking ties.
4.1 Results 8-Neighbor 16-Neighbor Grid Maps
evaluated algorithms considering solution cost runtime, measures solution quality
efficiency, respectively, several lookahead weight values.
used 512 512 maps video game Baldurs Gate (BG), Room maps
(ROOMS), maps different size Starcraft (SC) available N. Sturtevants
560

fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHS

8-neighbor BG Maps

6000
4000
2000

Algorithm

Cost (log scale)

Cost (log scale)

)
)(3
.0

)
TB

(W
A*

)(2
.6

)
(W
A*

8-neighbor Counter Strike Maps

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

20000
10000

TB

Algorithm

8-neighbor Starcraft Maps

100000

)(2
.2

)
TB

(W
A*

)(1
.8

)
(W
A*
TB

TB

(W
A*

)(1
.0

)
)(3
.0

)
TB

(W
A*

)(2
.6

)
)(2
.2

(W
A*
TB

)(1
.8

(W
A*
TB

TB

(W
A*

)(1
.4

(W
A*

)(1
.0
TB

(W
A*
TB

)

300
)

500

300
)

500

)

1000

(W
A*

1000

10000

)(1
.4

2000

Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

TB

6000
4000

Cost (log scale)

Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

10000
Cost (log scale)

8-neighbor Room Maps
30000

2000
1000
500

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

1e+06

100000
20000
10000

Algorithm

0)
A*
)

(3
.

6)
TB

(W

A*
)

(2
.

2)
TB

(W

A*
)

(2
.

8)
(W
TB

(W

A*
)

(1
.

4)
TB

(W

A*
)

(1
.

0)
TB

TB

(W

A*
)

(1
.

0)
A*
)

(3
.

6)
TB

(W

A*
)

(2
.

2)
TB

(W

A*
)

(2
.

8)
TB

(W

A*
)

(1
.

4)
(1
.
TB

(W

A*
)
(W

TB

TB

(W

A*
)

(1
.

0)

2000

Algorithm

Figure 1: 8-neighbor results, solution cost tends decrease w lookahead parameter
increased.

path-finding repository (Sturtevant, 2012). addition, used 7 large maps Counter Strike
(CS), whose sizes range 4259 4097 4096 5462.
evaluated six lookahead values (1, 4, 16, 64, 128, 256) 512 512 maps six
lookahead values (1, 32, 128, 256, 512, 1024) SC CS maps. used six weight values
(1.0, 1.4, 1.8, 2.2, 2.6, 3.0). map generated 50 random solvable search problems, resulting 1800 problems BG, 2000 problems ROOMS, 3250 problems SC, 350
problems CS.
Figures 1 2 show performance measures 8-neighbor grid maps. Note
average search time per episode across algorithms using lookahead
parameter. search time per episode proportional lookahead parameter
depends variable (in particular, depend weight). Thus fair conclusions
drawn comparing two configurations lookahead parameter set
value.
561

fiH ERN ANDEZ , BAIER , &

8-neighbor BG Maps

)
)(3
.0

)
TB

(W
A*

)(2
.6

)
(W
A*

8-neighbor Counter Strike Maps

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

Runtime (ms)

Runtime (ms)

)(2
.2

)
)(1
.8

Algorithm

8-neighbor Starcraft Maps

50

(W
A*

TB

TB

(W
A*

)(1
.0
(W
A*

A*
)(

Algorithm

TB

)

0)
3.

6)
TB
(

W

A*
)(

2.

2)
TB
(

W

A*
)(

2.

8)
W
TB
(

TB
(

W

A*
)(

1.

4)
1.
A*
)(
W

TB
(

TB
(

W

A*
)(

1.

0)

0

)

1

10
8
6
4
2
0
(W
A*

2

)(1
.4

3

20

TB

Runtime (ms)

4

Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

25

TB

Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

5
Runtime (ms)

8-neighbor Room Maps

5
3
1

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

20000
10000

1000
200

Algorithm

)

)

(3.0
A*)
TB
(W

TB
(W

A*)

(2.6

)
(2.2
A*)
TB
(W

)
(1.8
A*)
TB
(W

)
(1.4

(1.0

A*)
TB
(W

(W

TB
(W

A*)

0)
A*
)

(3
.

6)
TB

TB

(W

A*
)

(2
.

2)

(W

A*
)

(2
.

8)
TB

TB

(W

A*
)

(1
.

4)
(1
.
A*
)

(W
TB

TB

(W

A*
)

(1
.

0)

)

60

Algorithm

Figure 2: 8-neighbor results, search time typically decreases w lookahead parameter
increased.

observe following relations hold maps regarding solution cost search time.

Solution Cost lookahead values, solution cost decreases w increased. significant improvements observed lower lookahead values. surprising light
cost bound (Theorem 2) . large lookahead parameters ( 256), value w
affect solution cost significantly. lookahead parameter increases, fewer search
episodes needed less physical backtracks (back moves) needed (Hernandez et al.,
2014). Back moves strongly influence performance algorithms. TB(WA*),
w increased number back moves decreases, explains improvement solution quality. example, BG maps, using lookahead 1, average reduction
back moves 1,960.5, comparing w = 1 w = 3, whereas lookahead 512
reduction 2.4, comparing w = 1 w = 3.
562

fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHS

16-neighbor BG Maps
Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

6000
4000
2000
1000

10000
6000
4000
2000
1000

500

Algorithm

Cost (log scale)

Cost (log scale)

)
)(3
.0

)
TB

(W
A*

)(2
.6

)
(W
A*

16-neighbor Counter Strike Maps

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

20000
10000

TB

Algorithm

16-neighbor Starcraft Maps

100000

)(2
.2

)
TB

(W
A*

)(1
.8

(W
A*

)(1
.4
TB

)
(W
A*
TB

TB

(W
A*

)(1
.0

)
)(3
.0

)
TB

(W
A*

)(2
.6

)
TB

(W
A*

)(2
.2

)
)(1
.8

(W
A*
TB

)
(W
A*
TB

)

)(1
.4

(W
A*

)(1
.0
TB

(W
A*

)

500
300

300

TB

Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

30000
Cost (log scale)

10000
Cost (log scale)

16-neighbor Room Maps

2000
1000
500

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

1e+06

100000
20000
10000

Algorithm

0)
A*
)

(3
.

6)
TB

(W

A*
)

(2
.

2)
TB

(W

A*
)

(2
.

8)
(W
TB

(W

A*
)

(1
.

4)
TB

(W

A*
)

(1
.

0)
TB

TB

(W

A*
)

(1
.

0)
A*
)

(3
.

6)
TB

(W

A*
)

(2
.

2)
TB

(W

A*
)

(2
.

8)
TB

(W

A*
)

(1
.

4)
(1
.
TB

(W

A*
)
(W

TB

TB

(W

A*
)

(1
.

0)

2000

Algorithm

Figure 3: 16-neighbor results, solution cost tends decrease w lookahead parameter
increased.

Search Time w increased, search time decreases significantly lower lookahead values
decreases moderately higher lookahead values. ROOMS observe largest
improvements w increased. behavior ROOMS explained WA*
performs well type map w > 1.
Figures 3 4 show performance measures 16-neighbor grid maps. observe
relations observed 8-neighbor grid maps regarding solution cost search time.
4.1.1 8-N EIGHBOR VERSUS 16-N EIGHBOR G RID APS
Lower cost solutions obtained 8-neighbor grids 16-neighbor grids lookahead values 1, 4, 16 BG. Note exist 16-neighbor movements
expensive 8-neighbor moves, small lookaheads, 16-neighbor solutions may
similar number moves, worse quality 8-neighbor solutions. hand,
563

fiH ERN ANDEZ , BAIER , &

16-neighbor BG Maps

)
)(3
.0

)
TB

(W
A*

)(2
.6

)

16-neighbor Counter Strike Maps

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

Runtime (ms)

Runtime (ms)

(W
A*

Algorithm

16-neighbor Starcraft Maps

50

)(2
.2

)

Algorithm

(W
A*

TB

TB

(W
A*

)(1
.0
(W
A*

A*
)(

)(1
.8

)

0)
3.

6)
TB
(W

A*
)(

2.

2)
TB
(W

A*
)(

2.

8)
TB
(W

A*
)(

1.

4)
1.
TB
(W

A*
)(
TB
(W

TB
(W

A*
)(

1.

0)

0

10
8
6
4
2
0

TB

1

)

2

(W
A*

3

20

)(1
.4

4

25

TB

Runtime (ms)

5

Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

30

TB

Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

7
Runtime (ms)

16-neighbor Room Maps

5
3
1

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

20000
10000

1000
200

Algorithm

)

)

(3.0
A*)
TB
(W

TB
(W

A*)

(2.6

)
(2.2
A*)
TB
(W

)
(1.8
A*)
TB
(W

)
(1.4

(1.0

A*)
TB
(W

(W

TB
(W

A*)

0)
A*
)

(3
.

6)
TB

TB

(W

A*
)

(2
.

2)

(W

A*
)

(2
.

8)
TB

TB

(W

A*
)

(1
.

4)
(1
.
A*
)

(W
TB

TB

(W

A*
)

(1
.

0)

)

60

Algorithm

Figure 4: 16-neighbor results, search time typically decreases w lookahead parameter increased.

(a)

(b)

Figure 5: 8-neighborhoods (a) 16-neighborhoods (b).

564

fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHS

8-neighbor Counter Strike Maps
10000

TB(WA*)(1.0)
TB(WA*)(1.4)
TB(WA*)(1.8)
TB(WA*)(2.2)
TB(WA*)(2.6)
TB(WA*)(3.0)

9000
8000
7000
6000
5000
4000
3000
2000
1000

TB(WA*)(1.0)
TB(WA*)(1.4)
TB(WA*)(1.8)
TB(WA*)(2.2)
TB(WA*)(2.6)
TB(WA*)(3.0)

9000
Number Time Intervals

Number Time Intervals

10000

16-neighbor Counter Strike Maps

8000
7000
6000
5000
4000
3000
2000

0.1

0.3

0.5

0.7

0.9

1.1

1000

Duration Time Interval (ms)

0.1

0.3

0.5

0.7

0.9

1.1

Duration Time Interval (ms)

Figure 6: Results Game Time Model.
similar quality observed lookahead values. TB(WA*), almost values w
lookahead configurations, 16-neighbor grids performs fewer moves 8-neighbor grids.
example, SC w = 2.6 lookahead parameter 1024, 8-neighbor grids need
factor 1.6 moves 16-neighbor grids. Note however 16-neighbor moves
higher cost 8-neighbor moves. Regarding runtime, TB(WA*) 8-neighbor connectivity runs faster TB(WA*) 16-neighbor connectivity. happens expansion
state 16-neighbor connectivity takes time expanding state 8-neighbor
connectivity.
4.2 Results Game Time Model
report results TB(WA*) using Game Time Model Counter Strike maps 8and 16-neighbor grids. use 0.1, 0.3, 0.5, 0.7, 0.9, 1.1 milliseconds duration time
intervals. setting, quality solution measured number time intervals
required solve problem, fewer intervals used, better solution quality
is.
Figure 6 shows average performance. observe length time interval increases, TB(WA*) yields solutions better quality. hand, w increased, TB(WA*)
obtains better solutions. observed clearly duration intervals
small (e.g., 0.1ms). also observe better-quality solutions 16- rather 8-neighbor
connectivity. 16-neighbor connectivity agent perform knight move
single interval.
4.3 Results Non-reversible Search Graphs: Racetrack
section compare TBR(WA*) LSS-LRTWA* deterministic version racetrack problem (Barto, Bradtke, & Singh, 1995; Bonet & Geffner, 2003). problem race565

fiH ERN ANDEZ , BAIER , &

Extended Hansen Racetrack
400

TBR(WA*)(1.0)
TBR(WA*)(3.0)
TBR(WA*)(5.0)
TBR(WA*)(7.0)
LSS-LRT(WA*)(1.0)
LSS-LRT(WA*)(3.0)
LSS-LRT(WA*)(5.0)
LSS-LRT(WA*)(7.0)

300
250

TBR(WA*)(1.0)
TBR(WA*)(3.0)
TBR(WA*)(5.0)
TBR(WA*)(7.0)
LSS-LRT(WA*)(1.0)
LSS-LRT(WA*)(3.0)
LSS-LRT(WA*)(5.0)
LSS-LRT(WA*)(7.0)

450
400
Number Actions

350
Number Actions

Game Map Racetrack
500

200
150

350
300
250
200
150

100

100

Average Time per Search (ms)

2

8

6

1.

4

1.

1.

1

2
1.

8

6

0.

4

0.

0.

2
0.

2

2
2.

8

6

1.

4

1.

1

2

1.

1.

8

6

0.

0.

4

50
0.

0.

2

50

Average Time per Search (ms)

Figure 7: Results Racetrack Grids.
track represented grid cells marked obstacles. Similar grid pathfinding,
problem move agent set initial positions cells marked final
position. Nevertheless, problem agent associated velocity, set actions
involve accelerating (vertically horizontally), performing no-op action maintains
current velocity.
state racetrack tuple (x, y, vx , vy ), (x, y) position vehicle,
(vx , vy ) velocity vector. actions represented tuples form (ax , ay ),
ax , ay {1, 0, 1}, correspond acceleration vector. Unlike original version (Barto
et al., 1995), actions deterministic one initial one destination cell.
actions deterministic, (ax , ay ) performed (x, y, vx , vy ), new state given
(x0 , 0 , vx0 , vy0 ), vx0 = vx +ax vy0 = vy +ay , (x0 , 0 ) computed considering
vehicle changes velocity (vx0 , vy0 ) moving. movement towards (x0 , 0 )
would lead crashing obstacle, like Bonet Geffner (2003) do, leave vehicle next
obstacle velocity (0, 0).
experiments, used two racetracks. firstwhich refer HRTis 33
207 grid corresponds extended version racetrack used Hansen Zilberstein (2001) (which 33 69 grid). also use game map AR0205SR Baldurs Gate,
whose size 214x212. refer map GRT.
generated 50 random test cases HRT GRT Manhattan distance
initial state goal state greater half width map. absolute
value components velocity vector restricted 3. heuristic
use Euclidean distance divided maximum speed.
evaluated TBR (WA*) LSS-LRTAWA* four weight values (1.0, 3.0, 5.0, 7.0). Figure 7 shows plot number actions versus average time per search episode. TBR (WA*)
number actions corresponds sum number moves plus number times
566

fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHS

TBWA*(2)
TBWA*(3)
TBWA*(4)
TBWA*(5)
LSS-LRTWA*(2)
LSS-LRTWA*(3)
LSS-LRTWA*(4)
LSS-LRTWA*(5)

0
50
10
0
15
0
20
0
25
0
30
0
35
0
40
0
45
0
50
0
55
0

100

Lookahead

100000

TBWA*(2)
TBWA*(3)
TBWA*(4)
TBWA*(5)
LSS-LRTWA*(2)
LSS-LRTWA*(3)
LSS-LRTWA*(4)
LSS-LRTWA*(5)

10000

0
50
10
0
15
0
20
0
25
0
30
0
35
0
40
0
45
0
50
0
55
0

1000

15-puzzle
Number Expansions (log scale)

Cost (log scale)

15-puzzle

Lookahead

Figure 8: Cost time comparison TB-WA LSS-LRTWA*

vehicle move. TBR (WA*) make movements search
restarted.
important note time spent updating heuristic proportional number
states updated. update TBR (WA*) may take time update
LSS-LRTWA* Closed list may contain states former algorithm.
reason use comparison average time per search, considers search
update time.
HRT (Figure 7) observe worst behavior one obtained TBR (WA*)(1.0).
algorithms improve performance increasing w, TBR (WA*), used weight
greater 1.0, algorithm clearly yields best performance. GRT, worst algorithms TBR (WA*)(1.0) LSS-LRTA(1.0). Here, algorithms improve increasing
weight.
benchmark used fewer problems game maps, carried
95% confidence analysis cost solutions. HRT, showed costs
best configuration TBR (WA*)(5.0) could 10% away true mean,
LSS-LRTA*(3.0) costs could 11% away true mean. GRT, hand,
difference performance two best configurations TBR (7) LSS-LRTWA*(7)
statistically significant.
Finally, experiments showed computational cost learning phase TB(WA*)
higher LSS-LRTA(WA*). Indeed, number updates carried TB(WA*)
3.4 times less number updates carried LSS-LRTA(WA*) HRT 1.6 time
less GRT. explains better performance terms runtime.
567

fiH ERN ANDEZ , BAIER , &

4.4 Results 15-Puzzle
chose 15-puzzle another domain evaluating time-bounded algorithms.
build 15-puzzle implementation extending Richard Korfs implementation available Carlos Linaress homepage.2 present results TB(WA*), LSS-LRT(WA*) algorithms.
use 100 test cases presented Korf (1993), uses Manhattan distance heuristic.
domain report results slightly different way. First, omit results TB(A*)
(TB(WA*) w = 1) terminate reasonable time. due fact
A* needs many expansions solving hardest test cases. Second, use number
expansions instead runtime efficiency measure. domain, found measure
stable since, general, solving 100 problems take much time
w > 1 (0.3s w = 2; 0.08s w = 3), thus time prone affected external factors
controlled operating system.
Figure 8 shows performance TB(WA*) LSS-LRT(WA*). use lookahead values
{16, 32, 64, 128, 256, 512} weights {2, 3, 4, 5}. observe following relations.
Solution Cost solution cost TB(WA*) decreases w increased almost lookahead
values. TB(WA*) obtains better results LSS-LRTWA* lookahead values
w > 2. w < 2 performance TB(WA*) worse performence LSSLRTA*. hand, TB(WA*) w = 5 obtains solution 2.0 times better
average solution obtained LSS-LRTA* (LSS-LRTWA* w = 1).
Number Expansions number expansions TB(WA*) decreases w increased.
TB(WA*) efficient LSS-LRTWA* lookahead values w > 2.
worst performing configuration TB(WA*) w = 1.
Note curve remains flat several configurations. small
number expansions needed solve problem.
conclusion, considering solution cost number expansions, 15-puzzle TB(WA*)
better algorithm. instance, average solution cost TB(WA*) 1.6 times better
average average solution cost LSS-LRTA*.
compare greedy algorithm (Parberry, 2015), real-time domainspecific, unlike our.

5. Summary Conclusions
paper introduced Time-Bounded Best-First Search, generalization real-time search
algorithm Time-Bounded A*. addition, introduced restarting version time-bounded
approach, TBR (WA*), unlike TB(BFS), better coverage non-reversible domains.
carried theoretical analysis TB(WA*) TBR (WA*), including termination
results cost bound TB(WA*). Given weight w, bound suggests TB(WA*)
significantly superior TB(A*) precisely search problems WA* expands significantly
fewer states A*. addition, bound suggests TB(WA*) may yield benefits
domains WA*, run offline, yield improvements A*. theoretical
bounds easily adapted instances Best-First Search offer guarantees solution
2. http://scalab.uc3m.es/clinares/download/source/ida/ida.html

568

fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHS

quality. TBR (WA*), proved termination strongly connected graphs, even contain
non-reversible actions. property also enjoyed real-time search algorithms LRTA*
family enjoyed TB(BFS).
experimental evaluation, focused pathfinding, 15-puzzle, racetrack
problem, found TB(WA*) TBR (WA*) significantly superior real-time
search algorithms LRTA* family. addition, found performance tends improve
weight parameter increased, without increasing time per search episode. finding
interesting although quality also improved increasing lookahead parameter,
increases time spent search episode.
well known many search benchmarks, WA* may expand significantly fewer nodes
A*. Consistent this, experiments, time-bounded versions suboptimal algorithms
like Weighted A* produce significantly better solutions obtained TB(A*). Improvements less noticeable lookahead parameter large, also predicted theory.
first observe performance gains using weights real-time setting.
Indeed, findings consistent Rivera et al. (2015), also obtain better solutions
using weighted heuristics. work adds another piece evidence justifies studying
incorporation weights real-time algorithms (e.g., RIBS EDA;* Sturtevant, Bulitko,
& Bjornsson, 2010; Sharon, Felner, & Sturtevant, 2014). Finally, SLA* (Shue & Zamani, 1993)
LRTS (Bulitko & Lee, 2006) two algorithms also perform backtracking moves.
investigation whether restarts could provide benefits algorithms left future
work.

Acknowledgements
thank Vadim Bulitko providing Counter Strike maps. research partly funded
Fondecyt grant number 1150328.

References
Aine, S., & Likhachev, M. (2013). Truncated incremental search: Faster replanning exploiting
suboptimality. Proceedings 27th AAAI Conference Artificial Intelligence (AAAI),
Bellvue, Washington, USA.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic programming. Artificial Intelligence, 72(1-2), 81138.
Bjornsson, Y., Bulitko, V., & Sturtevant, N. R. (2009). TBA*: Time-bounded A*. Proceedings
21st International Joint Conference Artificial Intelligence (IJCAI), pp. 431436.
Bonet, B., & Geffner, H. (2003). Labeled rtdp: Improving convergence real-time dynamic
programming.. ICAPS, Vol. 3, pp. 1221.
Bulitko, V., & Lee, G. (2006). Learning real time search: unifying framework. Journal
Artificial Intelligence Research, 25, 119157.
Bulitko, V., Bjornsson, Y., Sturtevant, N., & Lawrence, R. (2011). Real-time Heuristic Search
Game Pathfinding. Applied Research Artificial Intelligence Computer Games. Springer.
Burns, E., Ruml, W., & Do, M. B. (2013). Heuristic search time matters. Journal Artificial
Intelligence Research, 47, 697740.
569

fiH ERN ANDEZ , BAIER , &

Ebendt, R., & Drechsler, R. (2009). Weighted A* search - unifying view application. Artificial
Intelligence, 173(14), 13101342.
Gaschnig, J. (1977). Exactly good heuristics?: Toward realistic predictive theory bestfirst search. Reddy, R. (Ed.), Proceedings 5th International Joint Conference
Artificial Intelligence (IJCAI), pp. 434441. William Kaufmann.
Hansen, E. A., & Zilberstein, S. (2001). Lao: heuristic search algorithm finds solutions
loops. Artificial Intelligence, 129(1), 3562.
Hart, P. E., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determination
minimal cost paths. IEEE Transactions Systems Science Cybernetics, 4(2).
Hernandez, C., Asn, R., & Baier, J. A. (2014). Time-bounded best-first search. Proceedings
7th Symposium Combinatorial Search (SoCS).
Hernandez, C., & Baier, J. A. (2012). Avoiding escaping depressions real-time heuristic
search. Journal Artificial Intelligence Research, 43, 523570.
Hernandez, C., Baier, J. A., Uras, T., & Koenig, S. (2012). TBAA*: Time-Bounded Adaptive A*.
Proceedings 10th International Joint Conference Autonomous Agents Multi
Agent Systems (AAMAS), pp. 9971006, Valencia, Spain.
Koenig, S. (2001). Agent-centered search. Artificial Intelligence Magazine, 22(4), 109131.
Koenig, S., & Likhachev, M. (2005). Fast replanning navigation unknown terrain. IEEE
Transactions Robotics, 21(3), 354363.
Koenig, S., & Likhachev, M. (2006). Real-time adaptive A*. Proceedings 5th International
Joint Conference Autonomous Agents Multi Agent Systems (AAMAS), pp. 281288.
Koenig, S., & Sun, X. (2009). Comparing real-time incremental heuristic search real-time
situated agents. Autonomous Agents Multi-Agent Systems, 18(3), 313341.
Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42(2-3), 189211.
Korf, R. E. (1993). Linear-space best-first search. Artificial Intelligence, 62(1), 4178.
Likhachev, M., Gordon, G. J., & Thrun, S. (2003). ARA*: Anytime A* Provable Bounds
Sub-Optimality. Proceedings 16th Conference Advances Neural Information
Processing Systems (NIPS), Vancouver, Canada.
Parberry, I. (2015). Memory-efficient method fast computation short 15-puzzle solutions.
IEEE Trans. Comput. Intellig. AI Games, 7(2), 200203.
Pearl, J. (1984). Heuristics: Preintelligent Search Strategies Computer Problem Solving.
Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.
Pohl, I. (1970). Heuristic search viewed path finding graph. Artificial Intelligence, 1(3),
193204.
Rivera, N., Baier, J. A., & Hernandez, C. (2015). Incorporating weights real-time heuristic
search. Artificial Intelligence, 225, 123.
Schmid, K., Tomic, T., Ruess, F., Hirschmuller, H., & Suppa, M. (2013). Stereo vision based indoor/outdoor navigation flying robots. IEEE/RSJ International Conference Intelligent Robots Systems (IROS), pp. 39553962.
570

fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHS

Sharon, G., Felner, A., & Sturtevant, N. R. (2014). Exponential deepening a* real-time agentcentered search. Proceedings 7th Symposium Combinatorial Search (SoCS), pp.
871877.
Shue, L., & Zamani, R. (1993). admissible heuristic search algorithm. Komorowski, H. J.,
& Ras, Z. W. (Eds.), Proceedings 7th International Symposium Methodologies
Intelligent Systems (ISMIS), Vol. 689 LNCS, pp. 6975. Springer.
Sturtevant, N. (2012). Benchmarks grid-based pathfinding. Transactions Computational
Intelligence AI Games, 4(2), 144 148.
Sturtevant, N. R., Bulitko, V., & Bjornsson, Y. (2010). learning agent-centered search.
Proceedings 9th International Joint Conference Autonomous Agents Multi Agent
Systems (AAMAS), pp. 333340, Toronto, Ontario.
Wilt, C. M., & Ruml, W. (2012). weighted A* fail?. Proceedings 5th Symposium Combinatorial Search (SoCS), Niagara Falls, Ontario, Canada.

571

fiJournal Artificial Intelligence Research 56 (2016) 379-402

Submitted 09/15; published 06/16

Generating Models Matched Formula
Polynomial Delay
Petr Savicky

savicky@cs.cas.cz

Institute Computer Science, Czech Academy Sciences
Pod Vodarenskou Vez 2, 182 07 Praha 8, Czech Republic

Petr Kucera

kucerap@ktiml.mff.cuni.cz
Department Theoretical Computer Science Mathematical Logic
Faculty Mathematics Physics, Charles University Prague,
Malostranske nam. 25, 118 00 Praha 1, Czech Republic

Abstract
matched formula CNF formula whose incidence graph admits matching
matches distinct variable every clause. formula always satisfiable. Matched
formulas used, example, area parametrized complexity. prove
problem counting number models (satisfying assignments) matched
formula #P-complete. hand, define class formulas generalizing
matched formulas prove formula class one choose polynomial
time variable suitable splitting tree search models formula.
consequence, models formula class, particular matched
formula, generated sequentially delay polynomial size input.
hand, prove task cannot performed efficiently linearly
satisfiable formulas, generalization matched formulas containing class
considered above.

1. Introduction
paper, consider problem counting models (satisfying assignments)
generating subsets models given formula conjunctive normal form (CNF).
well known problem counting models general CNF #P-complete (Sipser,
2006). problem generating models general CNF formula clearly also hard,
checking whether least one satisfying assignment formula,
SAT problem, NP-complete (Garey & Johnson, 1979).
paper, mostly deal problem enumerating models formula.
problem important areas research applications, unbounded model
checking (Kang & Park, 2005; McMillan, 2002) data mining (Coquery, Jabbour, Sais,
Salhi, et al., 2012). success modern SAT solvers inspired design model counting
enumeration algorithms well (see e.g. Jabbour, Lonlac, Sais, & Salhi, 2014; Morgado
& Marques-Silva, 2005a, 2005b). addition basic enumeration problem
require models generated prescribed order, versions
considered, e.g. generating models non-decreasing weight (Creignou, Olive, & Schmidt,
2011).
Another line research concentrated studying special classes boolean formulas
enumeration algorithm guaranteed complexity could devised. One
c
2016
AI Access Foundation. rights reserved.

fiSavicky & Kucera

easily find example formula set models exponentially larger
size formula itself. case reasonable include size output
bound running time enumeration algorithm. specifically say
algorithm enumerates models formula runs output polynomial time
running time bounded polynomial two variables, size input
(i.e. input formula ) size output (i.e. number models ).
paper, consider restrictive setting follows. algorithm receives input
formula generates sequence models way time needed
generating first model time generating two consecutive models
sequence polynomial length formula. type complexity bound
called polynomial delay. clear enumerate models formula
polynomial delay, construct output polynomial algorithm
task well. hand, much harder get enumeration algorithm
polynomial delay output polynomial algorithm. overview various
notions enumeration complexity (see Johnson, Yannakakis, & Papadimitriou, 1988).
special classes formulas polynomial delay enumeration algorithms
described, includes 2-CNF formulas, Horn formulas, generalized satisfiability
problems others (see e.g. Aceto, Monica, Ingolfsdottir, Montanari, & Sciavicco, 2013;
Creignou & Hebrard, 1997; Dechter & Itai, 1992; Kavvadias, Sideri, & Stavropoulos, 2000).
paper, describe another class formulas polynomial delay enumeration algorithm based backtrack-free search described. contrary
algorithms known 2-CNF Horn formulas, splitting variable step cannot
chosen arbitrarily, however, existence suitable variable guaranteed
efficiently identified.
particular consider class matched formulas introduced Franco
Van Gelder (2003). Given CNF formula , consider incidence graph I() defined follows. I() bipartite graph one part consisting clauses
part containing variables . edge {x, C} variable x clause C
I() x x appears C. observed Aharoni Linial (1986) Tovey
(1984) I() admits matching (i.e. set pairwise disjoint edges) size (where
number clauses ), satisfiable. Later formulas satisfying
condition called matched formulas Franco Van Gelder. Since matching
maximum size bipartite graph found polynomial time (see e.g. Lovasz &
Plummer, 1986), one check efficiently whether given formula matched.
Given general formula , measure far matched considering maximum deficiency (), number clauses remain unmatched
maximum matching I(). formula thus matched iff () = 0. weaker notion
deficiency () = n, number clauses n number
variables , also often considered.
Matched formulas play significant role theory satisfiability solving. Since
introduction matched formulas considered base class parameterized
algorithms satisfiability, see e.g. book Flum Grohe (2006) overview
parameterized algorithms theory. particular, Fleischner, Kullmann, Szeider (2002)
show satisfiability formulas maximum deficiency bounded constant k
decided time O(kknO(k) ) kk length input formula n
380

fiGenerating Models Matched Formula

denotes number variables. result later improved Szeider (2003)
algorithm satisfiability parameterized maximum deficiency formula
complexity O(2k n3 ). Parameterization based backdoor sets respect matched
formulas considered Szeider (2007).
Since matched formulas trivially satisfiable, ask stronger question:
hard count enumerate models matched formula? prove counting
models matched formula #P-complete problem, turn attention
generating models matched formula. main result paper algorithm
generates models matched formula polynomial delay. algorithm
constructs splitting tree whose nodes correspond either matched unsatisfiable
formula. However, cases strategy sufficient since nodes tree
cannot split way. prove node corresponds formula
satisfied iterated elimination pure literals. Formulas property
called pure literal satisfiable. formulas studied Kullmann (2000)
subclass linearly satisfiable formulas. node pure literal satisfiable formula
reached, algorithm switches simpler strategy. prove models pure
literal satisfiable formula generated delay linear length formula.
hand, #SAT problem pure literal satisfiable formulas #P-complete,
problem #P-complete monotone 2CNFs (Valiant, 1979a, 1979b),
pure literal satisfiable.
Several generalizations matched formulas also considered literature. Kullmann (2000) generalized matched formulas class linearly satisfiable
formulas. Autarkies based matchings studied Kullmann (2003). Szeider (2005)
considered another generalization matched formulas, classes biclique satisfiable
var-satisfiable formulas. Unfortunately, biclique satisfiable var-satisfiable
formulas hard check formula falls one classes (Szeider, 2005).
show paper result transfer class linearly satisfiable
formulas demonstrating possible generate models linearly satisfiable
formula polynomial delay unless P=NP.
paper organized follows. giving basic definitions Section 2, describe
Section 3 specific simple splitting property class formulas, allows
generate models formula class efficiently. Section 4, consider pure
literal satisfiable formulas prove class required splitting property.
Section 5, consider matched formulas prove required splitting property
class formulas, generalizes matched pure literal satisfiable formulas
natural way. implies algorithm generating models matched formula
formula general class polynomial delay. Section 6, present
complexity bounds efficient versions algorithms previous sections.
Section 7, show negative result concerning linearly satisfiable formulas. Section 8
contains concluding remarks directions research.

2. Definitions
section, give necessary definitions summarize results use
paper.
381

fiSavicky & Kucera

2.1 Boolean Functions
Boolean function n variables mapping f : {0, 1}n {0, 1}. literal either
variable, called positive literal, negation, called negative literal. negation
variable x denoted x x. clause disjunction set literals,
contains one literal variable. Formula conjunctive normal form
(CNF) or, equivalently, CNF formula, conjuction clauses. often treat
clause set literals CNF formula set clauses. well known
fact every Boolean function represented CNF formula (see e.g. Genesereth
& Nilsson, 1987). size formula number clauses
denoted ||. length formula total number occurrences literals ,
i.e. sum sizes clauses , denoted kk. Given variable x
value {0, 1}, [x = a] denotes formula originating substituting x
value obvious simplifications consisting removing falsified literals satisfied
clauses. extend notation negative literals well setting [x = a] = [x = a].
formula obtained assigning values a1 , . . . , ak {0, 1} variables
x1 , . . . , xk denoted [x1 = a1 , x2 = a2 , . . . , xk = ak ]. say literal l pure
CNF formula, occurs formula negated literal l not. literal
irrelevant formula, neither literal negation occurs formula.
variable pure, appears positively, negatively , i.e. appears
literal, pure .
Let formula defining Boolean function f n variables. assignment
values v {0, 1}n model (also satisfying assignment, true point ),
satisfies f , i.e. f (v) = 1. set models denoted (). models
() defined variables occurrence . set variables
function defined formula larger, however, introduce special
notation general case. algorithmic purposes, also necessary,
since adding irrelevant variable formula changes set models adding
variable possible values element original set models.
partial assignment assigns values subset variables. formula
variables x1 , . . . , xn , represented ternary vector v {0, 1, }n ,
vi = denotes fact xi assigned value v.
Note empty clause admit satisfying assignment empty CNF
satisfied assignment.
2.2 Matched Formulas
paper use standard graph terminology, (see e.g. Bollobas, 1998). Given
undirected graph G = (V, E), subset edges E matching G edges
pairwise disjoint. bipartite graph G = (A, B, E) undirected graph
disjoint sets vertices B, set edges E satisfying E B. set
W vertices G, let (W ) denote neighborhood W G, i.e. set vertices
adjacent element W . shall use following well-known result matchings
bipartite graphs:
Theorem 2.1 (Halls Theorem). Let G = (A, B, E) bipartite graph. matching
size |M | = |A| exists every subset |S| |(S)|.
382

fiGenerating Models Matched Formula

Let = C1 . . . Cm CNF formula n variables X = {x1 , . . . , xn }. associate
bipartite graph I() = (, X, E) (also called incidence graph ),
vertices correspond clauses variables X. clause Ci connected
variable xj (i.e. {Ci , xj } E) Ci contains xj xj . CNF formula matched I()
matching size m, i.e. matching pairs clause unique
variable, shall call matching clause saturated matching. Note matched
CNF trivially satisfiable, since clause satisfied literal containing
variable matched given clause. variable, matched clause
given matching , called matched , free otherwise.
2.3 Generating Models Polynomial Delay
main goal paper describe algorithm which, given matched formula ,
generates set () models polynomial delay. Let us state formally
require algorithm.
say algorithm generates models Boolean formula polynomial
delay, polynomial p, algorithm, given formula input,
satisfies following properties.
1. works steps, takes time O(p(kk)).
2. step, either finds model different models obtained
previous steps (in particular, model first step) determines
model, previous steps already found models .
algorithm properties exists, follows construct
set () models time O((|T ()| + 1) p(kk)), means algorithm
output polynomial. Note since () may exponential size respect kk,
efficiency respect size input output best hope
constructing ().

3. Efficient Splitting Tree Algorithm
idea algorithm construct decision tree function represented
given satisfiable CNF, every subtree larger single leaf contains 1-leaf.
depth tree number variables. tree searched
DFS order, time needed arbitrary moment reach 1-leaf n
times time needed split node. following, show classes
formulas including matched formulas possible find splitting procedure
yields tree described above.
decision tree Boolean function f labeled binary tree, inner node
labeled variable, leaves edges labels 0 1. decision tree computes
f (x) given assignment x process starts root visited
node follows edge labeled value variable, label node.
output label leaf reached process. computation path tests
variable, tested previous part path, test redundant.
consider trees without redundant tests.
383

fiSavicky & Kucera

decision tree representing function given CNF formula constructed top follows. root tree assigned . non-leaf node
tree assigned formula , choose arbitrary split variable x
occurrence assign restricted formulas [x = 0] [x = 1] successors.
node assigned empty formula becomes 1-leaf node assigned formula,
contains empty clause, becomes 0-leaf. resulting decision tree represents
function given , although large practical purposes. path
root inner node u tree corresponds partial assignment changes
formula representing function computed subtree whose root u.
depth tree function n variables n.
leaf node labeled 1 represents set models , precisely, leaf
depth represents 2nd models . Moreover, different leaves tree represent
disjoint sets models. Given decision tree function represented , can,
traversing it, generate models time proportional size. process leads
large delay generating successive models, tree contains large subtrees
0-leaves. following condition class formulas describes situation
avoided.
Definition 3.1. Let U class formulas, let U let x variable
occurrence . say x splitting variable relative U , every
{0, 1}, [x = a] satisfiable, [x = a] U .
class formulas U splitting property, every formula U containing
variable contains splitting variable relative U .
shall associate splitting problem class formulas U splitting property.
Definition 3.2. Let U class formulas splitting property. splitting problem
relative U following problem: Given formula U , find splitting variable
relative U results satisfiability tests formulas [x = 0] [x = 1].
Note complexity splitting problem relative U also upper bound
time satisfiability test formulas U . formula satisfiable,
variable x least one formulas [x = 0]
[x = 1] satisfiable. result satisfiability checks splitting variable x
required part solution splitting problem.
Theorem 3.3. class formulas U splitting property splitting problem
relative U solved time c(), c() kk formula U ,
models formula U n variables generated delay O(n c()).
Proof. Construct tree DFS order using splitting variable every formula
assigned non-leaf node. non-leaf node labeled x splitting
variable, successors labeled [x = 0] [x = 1]. formulas
unsatisfiable, corresponding successor becomes 0 leaf. formulas
empty, corresponding successor becomes 1 leaf. root tree split even
unsatisfiable, however, nodes labeled unsatisfiable formula split.
384

fiGenerating Models Matched Formula

Hence, except possibly root, node two 0-leaves successors.
Since length every formula tree kk, node, time O(c())
sufficient choose splitting variable, determine successors leaf,
construct formulas successors node.
Let us assume u non-leaf node constructed tree different root.
One successors u labeled unsatisfiable formula. recognized
splitting algorithm successor 0-leaf. Consequently, time
O(c()), construction tree continues satisfiable successor u. Hence,
n splitting steps time O(n c()), 1-leaf reached. 2
Remark 3.4. contains unit clause U closed unit propagation,
variable x contained unit clause splitting variable identified efficiently.
reason known satisfiable, one formulas [x = a] contains
empty clause and, hence, satisfiable.
Remark 3.5. class U satisfies
1. satisfiability formulas U tested polynomial time,
2. U closed partial assignments,
splitting problem relative U polynomial complexity. Indeed, case
variable formula U splitting variable satisfiability tests
corresponding restrictions obtained polynomial time. Class U property
sometimes also conservative. also say property particular form
self-reducibility (in sense considered e.g. Khuller & Vazirani, 1991). classes
generalized satisfiability problem described Creignou Hebrard (1997)
property addition classes, consider, instance, Horn formulas, SLUR formulas,
2CNFs, q-Horn formulas, etc. immediate corollary Theorem 3.3, possible
generate models formulas classes polynomial delay.
main result paper splitting problem relative slight generalization matched formulas also polynomial complexity although class matched
formulas closed partial assignments.

4. Pure Literal Satisfiable Formulas
considering matched formulas, let us make small detour class formulas
satisfiable iterated elimination pure literals, call pure literal
satisfiable. formulas already considered Kullmann (2000) special
case linearly satisfiable formulas.
set literals called consistent, contain contradictory literals. l
literal, let assign(l) assignment variable contained l, satisfies
l. consistent set sequence literals L, let assign(L) partial assignment
variables satisfying literals L. formula , [L] abbreviation
[assign(L)].
385

fiSavicky & Kucera

Definition 4.1. pure literal sequence formula consistent sequence literals
(l1 , . . . , lk ), every = 1, . . . , k, literal li either pure irrelevant
formula [l1 , . . . , li1 ]. particular, l1 pure irrelevant . pure literal sequence
called strict, literals li pure [l1 , . . . , li1 ].
L pure literal sequence , formula [L] called reduced formula
corresponding L. [L] contain pure literal, L called maximal
pure literal sequence .
Definition 4.2. formula pure literal satisfiable, pure literal sequence L
, reduced formula [L] empty or, equivalently, assign(L) satisfying
assignment .
autarky formula partial assignment v variables, every
clause either satisfied unchanged v. Autarkies studied e.g. Kullmann
(2000). Note every initial segment pure literal sequence defines assignment
variables, autarky. Moreover, one easily verify property
characterizes pure literal sequences.
Let us note pure literal satisfiable formulas closed partial assignments.
Consider formula , contain pure literal. Let formula obtained
adding new variable x positive literal every clause. Formula
pure literal satisfiable, [x = 0] = pure literal satisfiable. follows
pure literal satisfiable formulas satisfy second property required Remark 3.5
put effort showing pure literal satisfiable formulas
splitting property splitting problem relative pure literal satisfiable formulas
polynomial complexity.
every CNF formula, may tested polynomial time, whether pure literal
satisfiable. order find pure literal sequence witnessing fact, procedure
FindPLS Algorithm 1 uses greedy approach, step chooses satisfies
pure literal current formula. approach meaningful, since literal
pure stage procedure, either remains pure becomes irrelevant
following stages. pure literal sequence obtained procedure depends
nondeterministic choices made procedure, however, Corollary 4.4, resulting
reduced formula uniquely determined input.
Lemma 4.3. clause C CNF removed run FindPLS,
removed every run FindPLS input .
Proof. Let L K pure literal sequences produced different runs FindPLS
. formulas [L] [K] corresponding reduced formulas let C
clause contained [L]. Hence, L contains literals C. Since [K]
subset , L pure literal sequence [K]. literal L contained
[K], first literals pure [K]. Since [K] contain pure
literal, literal L contained [K]. particular, C contained [K]. 2
following immediate corollary.
386

fiGenerating Models Matched Formula

Algorithm 1 Constructing pure literal sequence
Require: CNF formula .
Ensure: maximal strict pure literal sequence L corresponding reduced
formula.
1: procedure FindPLS()
2:

3:
Initialize new empty list literals L.
4:
Initialize Pure() set pure literals .
5:
Pure() 6=
6:
Choose literal l Pure().
7:
Add l L.
8:
[l].
9:
Update Pure() consist pure literals .
10:
end
11: end procedure

Corollary 4.4. Let CNF formula let L pure literal sequence obtained
FindPLS .
1. formula [L] uniquely determined .
2. formula pure literal satisfiable, [L] empty.
Since running time procedure FindPLS polynomial length input
formula, maximal pure literal sequence formula constructed polynomial
time. complexity constructing maximal pure literal sequence formula is,
fact, O(kk) Lemma 6.1.
Lemma 4.5. Let L = (l1 , . . . , ln ) pure literal sequence formula , contains
literal variable . = 1, . . . , n, denote xi variable contained li .
xi variable largest index among variables, occurence
, xi splitting variable relative pure literal satisfiable formulas
formulas [xi = 0] [xi = 1] satisfiable, contain
empty clause.
Proof. Let one formulas [xi = 0] [xi = 1] let L = (l1 , . . . , li1 ).
Clearly, L pure literal sequence . Moreover, contain empty clause,
L assigns value literals every clause hence, satisfies it. 2
sufficient show splitting problem relative class pure literal
satisfiable formulas polynomial complexity. Later Theorem 6.2 shall show
splitting problem case solved time O(kk).
Lemma 4.6. splitting problem relative class pure literal satisfiable formulas
polynomial complexity.
387

fiSavicky & Kucera

Proof. pure literal satisfiable, pure literal sequence, satisfies it,
obtained FindPLS polynomial time. sequence contain literals
variables, extended polynomial time appending arbitrary literals missing
variables obtain pure literal sequence satisfying assumption Lemma 4.5. Then,
lemma implies method select splitting variable obtain results
satisfiability test corresponding restrictions polynomial time. 2
pure literal sequence satisfies assumption Lemma 4.5 formula ,
sequence used find splitting variable formulas splitting tree
. Using this, models pure literal satisfiable formula generated
delay smaller general bound Theorem 3.3, see Corollary 6.2.
Remark 4.7. sign literal given variable, occurs strict pure literal
sequence, uniquely determined. variables y1 y2 occur
positively negatively strict pure literal sequence formula
(x1 y1 ) (x2 y1 ) (x3 y2 ) (x4 y2 ) (y1 y2 ) .
example, (x2 , y1 , x3 , y2 ) (x4 , y2 , x1 , y1 ) strict pure literal sequences formula.

5. Matched Formulas
section concentrate matched formulas. Let us start showing
problem determining number models matched formula , i.e. size |T ()|,
hard general #SAT problem.
Theorem 5.1. problem determining |T ()| given matched formula #Pcomplete.
Proof. Let = C1 C2 . . . Cm arbitrary CNF formula n variables. Let
y1 , . . . , ym new variables appearing let = (y1 y2 . . . ym ) clause.
Let us define CNF formula n + variables equivalent
= (C1 D) (C2 D) . . . (Cm D) .
Clearly, matched formula one also observe |T ()| = |T ()| 2n (2m 1).
thus reduced problem counting models general CNF formula
(i.e. general #SAT problem) problem counting models matched CNF
formula (i.e. #SAT problem restricted matched formulas). 2
goal show generate models matched formula
polynomial delay. Theorem 3.3 cannot used directly, since class
matched formulas splitting property seen following
example. Consider formula
(x1 x2 ) (x1 x3 ) (x2 x3 ) .
388

fiGenerating Models Matched Formula

formula matched, splitting variable. Indeed, setting x1 0 leads
satisfiable, yet matched formula (x2 )(x3 )(x2 x3 ) symmetry true
variables x2 x3 well. order achieve objective, consider richer
class formulas. class consider generalizes matched pure literal satisfiable
formulas follows. Note empty formula matched, since corresponds
empty graph formally assume empty graph possesses required
matching.
Definition 5.2. formula called pure literal matched, reduced formula obtained
procedure FindPLS matched.
Elimination pure literal preserves property matched, since pure literal
autarky. Hence, matched formula pure literal matched. Clearly, every pure literal
satisfiable formula pure literal matched, since reduced formula empty and, hence,
matched.
basic idea efficient splitting algorithm matched formulas presented
following theorem. Later shall show Corollary 6.4 splitting problem
relative pure literal matched formulas solved time O(n2 kk).
Theorem 5.3. class pure literal matched formulas splitting property
splitting problem relative pure literal matched formulas polynomial complexity.
order prove Theorem 5.3, show several statements concerning
structure matched formula. V set variables, say clause limited
V , contains literals variables V .
Definition 5.4. Let V subset variables matched formula let C denote
set clauses limited V . set V called critical block ,
|C| = |V |. Formally, V empty, also critical block.
Note matched formula, V subset variables, C set
clauses limited V , Halls theorem (Theorem 2.1 above)
|C| (C) |V |. Critical blocks achieving equality. blocks
following property.
Lemma 5.5. Let V critical block matched formula . Then, every clause
saturated matching I(), variables V matched clauses limited V .
Proof. Let matched formula fixed clause saturated matching
variables clauses . V critical block, |V | clauses limited
V clauses matched variables V . Since variables matched
clauses different, variables V matched one clauses. 2
Another useful property set critical blocks follows.
Lemma 5.6. set critical blocks matched formula closed intersection.
Proof. Let matched formula let V1 , V2 critical blocks. intersection
V1 V2 empty, conclusion lemma satisfied. variable x V1 V2 ,
389

fiSavicky & Kucera

Lemma 5.5, every clause saturated matching I(), variable matched
clause, limited V1 also V2 . Hence, number clauses,
limited V1 V2 , least |V1 V2 |. Since matched, number clauses
equal |V1 V2 | Halls theorem. Hence, V1 V2 critical block required. 2
formula x variable contained least one critical block,
Lemma 5.6 implies unique inclusion minimal critical block containing
x, equal intersection critical blocks containing x. matched
formula number clauses variables, every variable contained
critical block, since set variables formula critical block.
Definition 5.7. matched formula number clauses variables x one variables, let Bx denote inclusion minimal critical block
containing x.
notation Bx specify formula, since always clear
context. aim show formula matched, either find
splitting variable relative matched formulas, actually pure literal satisfiable.
order show property basis algorithm, shall first investigate
structure critical blocks respect matchings.
Lemma 5.8. Let matched formula number clauses variables.
Let l literal containing variable x let us assume formula [l]
matched.
1. literal l pure irrelevant clauses limited Bx ,
2. clause C contains l, every matching , C matched
variable y, Bx (where denotes strict inclusion).
Proof. symmetry, shall consider case l = x. Hence, assumptions,
[x = 0] matched formula.
1. critical block Bx subset every critical block containing x. Hence, order
prove first part lemma, sufficient show least one
critical block B containing x, x occur negatively clauses
limited B. Let C set clauses Halls condition
formula [x = 0] satisfied |(C)| < |C|. Let V = (C) set
variables, occurrence clauses C, let k = |V |.
least k + 1 clauses C. Since every clause C limited V ,
least k + 1 clauses [x = 0] limited V . clauses either
clause obtained clause removing literal x. Consider
set clauses limited V {x}. Since matched, Halls condition
satisfied set. Hence, contains k + 1 clauses limited V {x}.
Setting x = 0 leads least k + 1 clauses limited V . Hence, contains precisely
k + 1 clauses limited V {x} none contains literal x. Hence,
V {x} critical block required property proof first part
lemma finished.
390

fiGenerating Models Matched Formula

2. Let us fix clause saturated matching clauses variables I() let
clause matched x. Since [x = 0] matched, follows
contains positive literal x, otherwise matching would work
[x = 0] well. Let C clause containing x let variable C
matched . Since C different D, 6= x. assumptions,
set variables critical block and, hence, critical block
well-defined. Since C matched y, C limited Lemma 5.5.
implies x , x C. Since critical block containing x
Bx inclusion minimal critical block containing x, Bx . first part
lemma, clause limited Bx contains x implies C limited
Bx thus Bx 6= . Together get Bx .
2
structure critical blocks used show following proposition needed
prove Theorem 5.3.
Theorem 5.9. Let matched formula. every variable x, occurence
, {0, 1}, [x = a] matched, pure literal satisfiable.
Proof. Let matched formula satisfying assumptions let us fix clause
saturated matching I(). variable x matched
clause, assigning value x yields matched formula. assumption
therefore suppose variable exist variable matched
clause. case, numbers clauses variables equal
variable x , Bx well-defined.
Let n number variables clauses . = 1, . . . , n, let li
literal containing variable xi clause matched variable. every
= 1, . . . , n, formula [li ] matched formula [li ] matched. Consider
strict partial order variables defined
x < Bx

(1)

means strict inclusion. Lemma 5.8, variables maximal
partial order pure . Let us consider total ordering variables,
consistent strict partial order (1). Using appropriate renaming variables,
may assume ordering x1 , . . . , xn , every i, j, xi < xj , < j.
Let us verify using ordering, sequence ln , ln1 , . . . , l1 satisfying pure literal
sequence . Let us show induction = n, . . . , 1 xi pure irrelevant
formula [ln , . . . , li+1 ]. true = n Lemma 5.8, xn maximal
order variables induced inclusion critical blocks. Let us fix
consider partial assignment assign(ln , . . . , li+1 ). Lemma 5.8, clause containing
li matched variable xj satisfying xi < xj . Hence, clauses eliminated
considered partial assignment variable xi pure irrelevant formula
[ln , . . . , li+1 ]. 2
Proof Theorem 5.3. Assume, pure literal matched formula. Let L pure
literal sequence obtained FindPLS procedure let = [L], is,
391

fiSavicky & Kucera

assumption, matched formula. Since L maximal, contain pure literal.
empty, pure literal satisfiable formula find splitting variable
method Lemma 4.6. empty, matched pure
literal satisfiable. Hence, Theorem 5.9, variable x , [x = 0]
[x = 1] matched. Since L contain literal variable x,
application assign(L) x = commute {0, 1}. Hence, L pure literal
sequence formula [x = a] application assign(L) [x = a] leads
[x = a], matched. Hence, {0, 1}, formula [x = a] pure literal
matched variable x splitting variable formula .
time polynomial length formula sufficient select splitting variable
x proof above. nonempty, satisfiability [x = 0] [x = 1]
guaranteed choice x. empty, pure literal satisfiable method
Lemma 4.6 used. Hence, splitting variable results required satisfiability
tests obtained polynomial time. 2
Similarly class matched formulas, also class pure literal matched formulas
closed unit propagation. implies unit propagation used part
construction splitting tree, particular Remark 3.4 always select
variable unit clause splitting variable.
Proposition 5.10. class pure literal matched formulas closed unit propagation.
Proof. Assume, pure literal matched formula containing unit clause C = (l)
l literal. Let us prove [l] pure literal matched formula.
Let L pure literal sequence . Observe l cannot contained L,
[l] unsatisfiable. rest proof, distinguish, whether l contained L
not.
l contained L, let L1 denote sequence literals L l let L2
sequence literals L l. simplicity, written L = (L1 , l, L2 ).
clauses missing [l] changed removing l. Since l
contained L1 , sequence L1 pure literal sequence [l]. Since assignments
disjoint sets variables commute, [L1 , l] = [l, L1 ] and, hence, sequence
L2 pure literal sequence formulas. Hence, sequence L = (L1 , L2 )
pure literal sequence [l]. Since, moreover, [L1 , l, L2 ] = [l, L1 , L2 ], application
L [l] leads matched formula. Consequently, [l] pure literal matched.
Let us consider case l contained L. case [L] matched
formula contains unit clause C = (l), since clause cannot eliminated
satisfying literals L. every maximum matching [L], clause C matched
l. Thus satisfying l gives matched formula [L, l]. Since [L, l] = [l, L] L
pure literal sequence [l], formula pure literal matched. 2

6. Algorithms Complexity
section, prove specific complexity bounds algorithms presented
previous sections. complexity bounds derived RAM model unit cost
392

fiGenerating Models Matched Formula

measure word size O(log kk), input formula. data structures
used algorithms similar described Minoux (1988) Murakami
Uno (2014). Let us first concentrate pure literal satisfiable formulas.
Lemma 6.1. maximal pure literal sequence L CNF formula constructed
time O(kk).
Proof. use approach presented linear time algorithm unit propagation
Minoux (1988) obtain efficient version procedure FindPLS Algorithm 1.
addition initializations Algorithm 1, initialize auxiliary data structures.
data structures similar described Murakami Uno (2014).
particular, occurences literals formula represented nodes arranged
sparse matrix, whose rows correspond literals columns correspond clauses.
node contains identification clause literal, whose occurence represents.
auxiliary data structures names follows:
literal l denote cl(l) row matrix, doubly-linked list
nodes representing occurences l .
clause C denote lit(C) column matrix, doublylinked list nodes corresponding occurences literals C.
literal l denote cnt(l) counter, contains size list lit(C)
number clauses l appears.
initialize set Pure() queue always contains pure literals .
literals l cnt(l) > 0 cnt(l) = 0.
data structures initialized traversing linear time. important
note node represents occurence literal l clause C.
structure representing node contains four pointers, two doubly-linked list lit(C)
two double-linked list cl(l). Thus removing node lists
performed constant time.
procedure FindPLS repeat following steps find pure literal l , add l
L apply assign(l) . Finding pure literal amounts dequeueing Pure().
applying assign(l) remove clauses containing l (these satisfied)
remove l remaining clauses. Let 1 consist clauses contain l
let 0 consist clauses contain l. claim assign(l) applied
time O(k1 k + |0 |).
1. Removing clauses 1 means going list cl(l) clause C
list literal l lit(C) (including l), remove corresponding node
cl(l ) make list lit(C) inaccessible. requires time O(1) literal l .
operation also decrement counters cnt(l ) literals lit(C)
negated counterparts becomes pure, add queue Pure().
2. Removing occurrences l means going list cl(l) clause
C list, remove corresponding node cl(l) lit(C).
done time O(1) occurrence l.
393

fiSavicky & Kucera

Repeating steps literals included L requires constant number
operations occurrence literal input formula implies total
time O(kk). 2

Theorem 6.2. splitting problem relative pure literal satisfiable formulas
solved time O(kk) input pure literal satisfiable formula. Moreover,
set () models pure literal satisfiable formula generated delay
O(kk).
Proof. Using efficient version FindPLS guaranteed Lemma 6.1, operations
used proof Lemma 4.6 done time O(kk). implies first statement
theorem. procedure used preprocessing step algorithm
proving second statement. time O(kk), preprocessing produces pure literal
sequence L = (l1 , . . . , ln ), contains literal variable . auxiliary data
structures cl(l), lit(C) cnt(l) used preprocessing used also later,
stored reconstructed needed.
construction L, assumption Lemma 4.5 satisfied L. method
Lemma 4.5 used find splitting variable formula,
corresponding restrictions either contains empty clause also satisfies assumption
Lemma 4.5 L. Hence, sequence L used selecting splitting variable
nodes splitting tree .
DFS search controlled stack postponed nodes, initialized
root search starts. search split sequence descending branches.
descending branches starts removing node stack resuming
search node. visited node two satisfiable successors, DFS continues
one put onto stack. node single satisfiable successor,
stack modified. descending branch ends 1-leaf found.
estimate delay, estimate total time needed construct nodes one
descending branches follows.
indices L splitting variables chosen descending branch monotonically decreasing. Hence, total time needed search splitting variables
one descending branch O(n) and, hence, O(kk).
time needed manipulations formula one descending branch
follows. node removed stack, auxiliary data structures cl(l), lit(C)
cnt(l) computed original formula modified according
sequence settings variables along path root current node.
done time O(kk). Then, node descending branch, assignments
chosen variable computed satisfiable successor selected. one node,
done time O(k), k number occurrences chosen variable
. satisfiable successor selected, auxiliary structures updated according
it. total time needed operations one descending branch O(kk) using
similar argument proof Corollary 6.1.
combining estimates, total time constructing descending branch
and, hence, delay generating two consecutive models, O(kk). 2
394

fiGenerating Models Matched Formula

let us concentrate time complexity selecting splitting variable pure
literal matched formula.
Lemma 6.3. splitting problem relative pure literal matched formulas solved
time O(n kk) input formula n variables.
Proof. Following proof Theorem 5.3, first find pure literal sequence L
done time O(kk) Lemma 6.1. = [L] empty formula,
last variable L splitting variable. Otherwise matched find maximum

matching . step performed time O(kk n) (see Hopcroft & Karp,
1973). Then, search variable x , [x = 0] [x = 1]
matched. variable exists Theorem 5.9. number clauses less
n, variable used matching property. Otherwise, check
every variable, whether [x = a] matched {0, 1}. assignment x = satisfies
matched literal containing x, [x = a] matched. rest proof,
estimate complexity n checks assignments falsifying
matched literal.
Partial assignment performed time O(kk) = O(kk). partial assignment satisfied clauses removed occurences variable x removed
remaining clauses. modify matching matching N [x = a] accordingly, remove pairs containing satisfied clause pair containing x.
|N | = (where number clauses [x = a]), done. Otherwise know
|N | = 1, since one pair containing clause [x = a], specifically,
pair containing literal x, removed forming N . remains check
whether N already maximum matching whether better matching.
tested looking single augmentating path I([x = a]) matching N .
augmentating path found using breadth first search time linear size
graph I([x = a]) (see e.g. Hopcroft & Karp, 1973; Lovasz & Plummer, 1986). Hence,
test, whether [x = a] matched done time O(kk) = O(kk). 2
corollary Lemma 6.3 general bound Theorem 3.3, get
following.
Corollary 6.4. Models pure literal matched formula n variables generated
delay O(n2 kk).
Proof. Lemma 6.3 find splitting variable pure literal matched formula
time O(n kk), time determine satisfiability formulas [x = 0]
[x = 1] well. Theorem 3.3 thus get delay O(n2 kk). 2

7. Linearly Satisfiable Formulas
section consider class linearly satisfiable formulas. results Kullmann
(2000), class generalizes matched formulas pure literal satisfiable
formulas and, combining proofs, also class pure literal matched formulas.
section, show possible generate models linearly satisfiable formulas
polynomial delay unless P=NP.
395

fiSavicky & Kucera

consequence, splitting problem relative linearly satisfiable formulas
polynomial complexity unless P=NP. consequence follows also unconditionally
Example 7.10, presents linearly satisfiable formula 4 variables,
splitting variable respect class linearly satisfiable formulas.
Let us recall notation introduced Kullmann, used present
definition basic facts concerning linearly satisfiable formulas. l literal,
var(l) variable literal. v partial assignment, v(l) value
assignment literal l.
Definition 7.1 (Kullmann, 2000). Let CNF formula let v non-empty
partial assignment variables . say v simple linear autarky,
associated weight function w assigns variable x evaluated v positive
real number w(x) clauses C
X
X
w(var(l))
w(var(l)) .
(2)
lC,v(l)=1

lC,v(l)=0

Clearly, literal C falsified v, must literal satisfied v
well. Therefore simple linear autarky autarky. Kullmann showed check
whether simple linear autarky v CNF formula find one, exists,
solving several linear programs.
literal l pure formula, partial assignment v(l) = 1 weight
w(var(l)) = 1 simple linear autarky formula. another example, consider
satisfying assignment satisfiable 2-CNFs. assignment weight
variables forms simple linear autarky. Similarly, pure Horn CNFs without unit
clauses satisfiable simple linear autarky assigns value 0 equal weight
variables. hand, pure Horn CNF formula contains unit clause,
satisfiable simple linear autarky. example formula
(x1 ) (x1 x2 ) (x1 x3 ) (x1 x2 x3 ) ,
simple linear autarky Theorem 7.8 Lemma 7.9 below.
considering iterative application simple linear autarkies formula get
class linearly satisfiable formulas defined follows.
Definition 7.2 (Kullmann, 2000). class linearly satisfiable formulas defined
smallest class satisfying following two properties:
1. empty CNF linearly satisfiable.
2. Let CNF, simple linear autarky v [v] linearly
satisfiable. .
words, CNF formula linearly satisfiable subsequent applications
linear autarkies obtain empty formula. composition simple linear autarkies
called linear autarky Kullmann (2000) class linearly satisfiable formulas
therefore consists formulas satisfiable linear autarky. Kullmann showed
matched formulas linearly satisfiable. Since pure literal simple linear
396

fiGenerating Models Matched Formula

autarky, pure literal satisfiable formula linearly satisfiable. Similarly, pure literal
matched formula defined Section 5 linearly satisfiable simple linear autarkies
pure literals concatenated linear autarky resulting matched formula.
matched pure literal satisfiable formulas presented algorithms
generate models formulas polynomial delay, possible extend
result linearly satisfiable formulas unless P=NP. Let us first present construction,
used reduction argument.
Let arbitrary 3-CNF formula variables x1 , . . . , xn clauses c1 , . . . , cm .
Consider new variables y1 , y2 , y3 let formula consisting clauses
(y1 y2 ), (y2 y3 ), (y3 y1 ),
(cj y1 y2 y3 ),
j = 1, . . . ,
(xi y1 )
= 1, . . . , n .
Recall number models formula number satisfying assignments
variables, occurrence it. Hence, next lemma, () ()
defined different sets variables.
Lemma 7.3. Formula linearly satisfiable number models |T ()| =
|T ()| + 1.

Proof. clause ci contains three literals. Hence, clause , number
positive literals least number negative literals. follows
assignment variables 1 equal weight variables defines simple linear
autarky, satisfies . Hence, formula linearly satisfiable.
model satisfies y1 = y2 = y3 . assignment containing y1 = y2 = y3 = 1
model xi = 1 = 1, . . . , n. assignment containing y1 = y2 = y3 = 0
model assignment variables xi model . implies
second part statement lemma. 2
Since formula constructed every 3-CNF formula , lemma implies
following immediate corollary.

Corollary 7.4. NP-complete problem determine, whether general linearly
satisfiable formula least 2 models.
Note implies NP-hardness #SAT problem restricted linearly satisfiable formulas. problem is, fact, also #P-complete, since #P-complete
count models monotone formulas, pure literal satisfiable and, hence, linearly
satisfiable.
Example 7.10 below, present linearly satisfiable formula, splitting
variable relative class linearly satisfiable formulas. analysis example,
use characterization simple linear autarkies obtained using clause-variable matrix.
Definition 7.5. Let CNF formula clauses c1 , . . . , cm variables x1 , . . . , xn .
clause-variable matrix formula matrix = {aj,i } dimension n
defined

1 xi cj
1 xi cj
aj,i =

0 otherwise .
397

fiSavicky & Kucera

u Rm , u 0 means uj 0 j = 1, . . . , m. Kullmann showed following
proposition.
Lemma 7.6 (Kullmann, 2000). formula clause-variable matrix simple
linear autarky, nonzero z Rn , Az 0. Moreover,
linear autarky obtained vector z using assignment

1 zi > 0
v(xi ) =
0 zi < 0

zi = 0
weight function w(xi ) = |zi |.
Let us present well-known Farkas lemma form used proof Theorem
7.8.
Theorem 7.7 (Farkas lemma). Let n real matrix b Rn . Then, exactly
one following statements true.
1. vector Rm , 0 = bt .
2. vector z Rn , Az 0 bt z < 0.
linear combination real vectors non-negative coefficients called,
simplicity, non-negative combination.
Theorem 7.8. Assume, formula n variables clauses clausevariable matrix dimension n. Then, exactly one following statements
true:
(a) linear autarky,
(b) every vector Rn non-negative combination rows A.
Proof. First, assume, (a) (b) satisfied. Lemma 7.6 implies
non-zero z Rn , Az 0. (b), non-negative vector Rm ,
= z . Multiplying z right, get
Az = z z < 0 .
contradiction, since Az non-negative.
Assume, (b) satisfied. Hence, vector b Rn , non-negative
combination rows A. Farkas lemma, vector z Rn , Az 0
bt z < 0. Since latter condition implies z non-zero, simple linear
autarky Lemma 7.6 means (a) satisfied. 2
Lemma 7.9. Assume, matrix dimension n, rank(A) = n
vector u Rm components positive, ut = 0. Then, every vector
Rn non-negative combination rows A.
398

fiGenerating Models Matched Formula

Proof. assumption, linear space generated rows Rn . Hence,
every z Rn , v Rm , v = z. sufficiently large real number s,
vector v + su components non-negative (v + su)t = z. 2
Note every linearly satisfiable CNF formula 3 variables splitting
variable relative class linearly satisfiable CNF formulas, since setting variable
constant leads formula 2 variables, quadratic and,
hence, satisfiable linearly satisfiable.
Example 7.10. Denote E = {a {0, 1}4 | 2 a1 + a2 + a3 + a4 3} every Boolean
variable x, let x1 = x x0 = x. formula
(x1 , x2 , x3 , x4 ) =

4
^ _

xai

aE i=1

linearly satisfiable, splitting variable relative class linearly satisfiable
formulas.
Proof. every clause, number positive literals least number negative
literals. Hence, formula linearly satisfiable Lemma 7.6 z = (1, 1, 1, 1).
Since invariant permutation variables, sufficient prove
x4 splitting variable. Since every clause contains negative literal,
(0, 0, 0, 0) = 1. follows formula [x4 = 0] satisfiable. One verify
[x4 = 0] =

3
^ _

xai ,

aE i=1

E = {a {0, 1}3 | 1 a1 + a2 + a3 2}. order prove [x4 = 0]
linearly satisfiable, consider clause-variable matrix columns corresponding
x1 , x2 , x3 ,


1
1 1
1 1
1


1

1
1


1 1 1 .


1
1 1
1 1

1

matrix rank 3, since vectors (2, 0, 0), (0, 2, 0), (0, 0, 2) sum two
rows first three. Moreover, sum rows matrix zero
vector. Hence, formula [x4 = 0] linear autarky Lemma 7.9
Theorem 7.8. 2

8. Conclusion Directions Research
paper, shown possible generate models matched formula
n variables delay O(n2 kk). byproduct shown models
399

fiSavicky & Kucera

pure literal satisfiable formula (i.e. formula satisfiable iterated pure literal
elimination) generated delay O(kk). also shown result
cannot generalized class linearly satisfiable formulas since possible
generate models linearly satisfiable formulas polynomial delay unless P=NP.
Let us mention procedure generating models bounded delay
extended formulas small strong backdoor set respect class
matched formulas empty clause detection found. Let us assume B
backdoor set formula , i.e. B set variables satisfying partial
assignment variables B leads matched formula, formula containing
empty clause. generate decision tree (and thus generate models)
time O(2|B| kk + (f ) n2 kk). Unfortunately, searching strong backdoor sets
respect class matched formulas hard (Szeider, 2007).
algorithms described paper cases pure literal satisfiable pure
literal matched formulas used general algorithm model enumeration
based splitting tree. This, turn, DPLL based enumeration algorithm.
end, similar approach one described Stefan Szeider (2003) used.
Together formula would keep maximum matching I(). maximum
matching maintained reduction assignment steps performed
enumeration algorithm. algorithm arrives matched formula,
select splitting variables way described paper guaranteed
polynomial delay.
interesting question whether approach could used parameterized
satisfiability algorithm based maximum deficiency (see Szeider, 2003) order get
parameterized algorithm generating models general formula.

Acknowledgments
Petr Savicky supported CE-ITI GACR grant number GBP202/12/G061
institutional research plan RVO:67985807. Petr Kucera supported
Czech Science Foundation (grant GA15-15511S).

References
Aceto, L., Monica, D., Ingolfsdottir, A., Montanari, A., & Sciavicco, G. (2013). Logic
Programming, Artificial Intelligence, Reasoning: 19th International Conference,
LPAR-19, Stellenbosch, South Africa, December 14-19, 2013. Proceedings, chap.
Algorithm Enumerating Maximal Models Horn Theories Application
Modal Logics, pp. 117. Springer Berlin Heidelberg, Berlin, Heidelberg.
Aharoni, R., & Linial, N. (1986). Minimal non-two-colorable hypergraphs minimal
unsatisfiable formulas. Journal Combinatorial Theory, Series A, 43 (2), 196 204.
Bollobas, B. (1998). Modern Graph Theory, Vol. 184 Graduate Texts Mathematics.
Springer.
400

fiGenerating Models Matched Formula

Coquery, E., Jabbour, S., Sais, L., Salhi, Y., et al. (2012). SAT-based approach
discovering frequent, closed maximal patterns sequence. Proceedings
ECAI.
Creignou, N., & Hebrard, J.-J. (1997). generating solutions generalized satisfiability
problems. Informatique theorique et applications, 31 (6), 499511.
Creignou, N., Olive, F., & Schmidt, J. (2011). Theory Applications Satisfiability
Testing - SAT 2011: 14th International Conference, SAT 2011, Ann Arbor, MI, USA,
June 19-22, 2011. Proceedings, chap. Enumerating Solutions Boolean CSP
Non-decreasing Weight, pp. 120133. Springer Berlin Heidelberg, Berlin, Heidelberg.
Dechter, R., & Itai, A. (1992). Finding solutions find one. AAAI-92
Workshop Tractable Reasoning, pp. 3539.
Fleischner, H., Kullmann, O., & Szeider, S. (2002). Polynomial-time recognition minimal unsatisfiable formulas fixed clause-variable difference. Theoretical Computer
Science, 289 (1), 503 516.
Flum, J., & Grohe, M. (2006). Parameterized complexity theory (1st edition)., Vol. 3
Texts Theoretical Computer Science. EATCS Series. Springer-Verlag Berlin
Heidelberg.
Franco, J., & Van Gelder, A. (2003). perspective certain polynomial-time solvable
classes satisfiability. Discrete Appl. Math., 125 (2-3), 177214.
Garey, M., & Johnson, D. (1979). Computers Intractability: Guide Theory
NP-Completeness. W.H. Freeman Company, San Francisco.
Genesereth, M., & Nilsson, N. (1987). Logical Foundations Artificial Intelligence. Morgan
Kaufmann, Los Altos, CA.
Hopcroft, J. E., & Karp, R. M. (1973). n5/2 algorithm maximum matchings
bipartite graphs. SIAM Journal computing, 2 (4), 225231.
Jabbour, S., Lonlac, J., Sais, L., & Salhi, Y. (2014). Extending modern sat solvers
models enumeration. IEEE 15th International Conference Information Reuse
Integration (IRI), 2014, pp. 803810. IEEE.
Johnson, D. S., Yannakakis, M., & Papadimitriou, C. H. (1988). generating maximal
independent sets. Information Processing Letters, 27 (3), 119 123.
Kang, H.-J., & Park, I.-C. (2005). Sat-based unbounded symbolic model checking.
Computer-Aided Design Integrated Circuits Systems, IEEE Transactions on,
24 (2), 129140.
Kavvadias, D. J., Sideri, M., & Stavropoulos, E. C. (2000). Generating maximal models
Boolean expression. Information Processing Letters, 74 (34), 157162.
Khuller, S., & Vazirani, V. V. (1991). Planar graph coloring self-reducible, assuming
P 6= N P . Theoretical Computer Science, 88 (1), 183 189.
Kullmann, O. (2000). Investigations autark assignments. Discrete Applied Mathematics,
107 (13), 99 137.
401

fiSavicky & Kucera

Kullmann, O. (2003). Lean clause-sets: generalizations minimally unsatisfiable clausesets. Discrete Applied Mathematics, 130 (2), 209 249. Renesse Issue Satisfiability.
Lovasz, L., & Plummer, M. D. (1986). Matching Theory. North-Holland.
McMillan, K. L. (2002). Computer Aided Verification: 14th International Conference,
CAV 2002 Copenhagen, Denmark, July 2731, 2002 Proceedings, chap. Applying SAT
Methods Unbounded Symbolic Model Checking, pp. 250264. Springer Berlin Heidelberg, Berlin, Heidelberg.
Minoux, M. (1988). LTUR: simplified linear time unit resolution algorithm Horn
formulae computer implementation. Information Processing Letters, 29, 1 12.
Morgado, A., & Marques-Silva, J. (2005a). Algorithms propositional model enumeration counting. Tech. rep., Instituto de Engenharia de Sistemas e Computadores,
Investigacao e Desenvolvimento, Lisboa.
Morgado, A., & Marques-Silva, J. (2005b). Good learning implicit model enumeration. Tools Artificial Intelligence, 2005. ICTAI 05. 17th IEEE International
Conference on, pp. 6 pp.136.
Murakami, K., & Uno, T. (2014). Efficient algorithms dualizing large-scale hypergraphs.
Discrete Applied Mathematics, 170, 8394.
Sipser, M. (2006). Introduction Theory Computation, Vol. 2. Thomson Course
Technology Boston.
Szeider, S. (2003). Minimal unsatisfiable formulas bounded clause-variable difference
fixed-parameter tractable. Warnow, T., & Zhu, B. (Eds.), Computing
Combinatorics, Vol. 2697 Lecture Notes Computer Science, pp. 548558. Springer
Berlin Heidelberg.
Szeider, S. (2005). Generalizations matched CNF formulas. Annals Mathematics
Artificial Intelligence, 43 (1-4), 223238.
Szeider, S. (2007). Matched formulas backdoor sets. Marques-Silva, J., & Sakallah,
K. A. (Eds.), Theory Applications Satisfiability Testing SAT 2007, Vol. 4501
Lecture Notes Computer Science, pp. 9499. Springer Berlin Heidelberg.
Tovey, C. A. (1984). simplified NP-complete satisfiability problem. Discrete Applied
Mathematics, 8 (1), 85 89.
Valiant, L. (1979a). complexity computing permanent. Theoretical Computer
Science, 8 (2), 189 201.
Valiant, L. (1979b). complexity enumeration reliability problems. SIAM Journal
Computing, 8 (3), 410421.

402

fiJournal Artificial Intelligence Research 56 (2016) 247-268

Submitted 12/15; published 06/16

Association Discovery Diagnosis Alzheimers Disease
Bayesian Multiview Learning
Zenglin Xu

zlxu@uestc.edu.cn

Big Data Research Center
School Computer Science & Engineering
University Electronic Science & Technology China
Chengdu, Sichuan, 611731 China

Shandian Zhe

szhe@purdue.edu

Department Computer Science, Purdue University
West Lafayette, 47906 USA

Yuan(Alan) Qi

alanqi@cs.purdue.edu

Department Computer Science & Department Statistics
Purdue University
West Lafayette, 47906 USA

Peng Yu

yu peng py@lilly.com

Eli Lilly Company, Indianapolis, 46225, USA

Abstract
analysis diagnosis Alzheimers disease (AD) based genetic variations, e.g., single nucleotide polymorphisms (SNPs) phenotypic traits, e.g., Magnetic
Resonance Imaging (MRI) features. consider two important related tasks: i)
select genetic phenotypical markers AD diagnosis ii) identify associations
genetic phenotypical data. previous studies treat two tasks separately, tightly coupled underlying associations genetic variations
phenotypical features contain biological basis disease. present new
sparse Bayesian approach joint association study disease diagnosis. approach, common latent features extracted different data sources based sparse
projection matrices used predict multiple disease severity levels; return,
disease status guide discovery relationships data sources. sparse
projection matrices reveal interactions data sources also select groups
biomarkers related disease. Moreover, take advantage linkage disequilibrium (LD) measuring non-random association alleles, incorporate graph
Laplacian type prior model. learn model data, develop efficient
variational inference algorithm. Analysis imaging genetics dataset study
Alzheimers Disease (AD) indicates model identifies biologically meaningful associations genetic variations MRI features, achieves significantly higher
accuracy predicting ordinal AD stages competing methods.

1. Introduction
Alzheimers disease (AD) common neurodegenerative disorder (Khachaturian,
1985). order predict onset progression AD, NIH funded Alzheimers
Disease Neuroimaging Initiative (ADNI) facilitate evaluation genetic variations,
e.g., Single Nucleotide Polymorphisms (SNPs) phenotypical traits, e.g., Magnetic Resoc
2016
AI Access Foundation. rights reserved.

fiXu, Zhe, Qi, & Yu

nance Imaging (MRI). addition progression study, becoming important medical
studies identify relevant pathological genotypes phenotypic traits, discover
associations. Although found many bioinformatics applications (Consoli, Lefevre,
Zivy, de Vienne, & Damerval, 2002; Hunter, 2012; Gandhi & Wood, 2010; Liu, Pearlson,
Windemuth, Ruano, Perrone-Bizzozero, & Calhoun, 2009), association studies scarce
especially need AD study.
Many statistical approaches developed discover associations select features (or variables) prediction high dimensional problem. association studies, representative approaches canonical correlation analysis (CCA) extensions (Harold, 1936; Bach & Jordan, 2005). approaches widely used
expression quantitative trait locus (eQTL) analysis (Parkhomenko, Tritchler, & Beyene,
2007; Daniela & Tibshirani, 2009; Chen, Liu, & Carbonell, 2012). disease diagnosis based high dimensional biomarkers, popular approaches include lasso (Tibshirani,
1994), elastic net (Zou & Hastie, 2005), group lasso (Yuan & Lin, 2007), Bayesian
automatic relevance determination (MacKay, 1991; Neal, 1996). Despite wide success
many applications, approaches limited following reasons:
association studies neglect supervision disease status.
many diseases, AD, direct result genetic variations often highly
correlated clinical traits, disease status provides useful yet currently unutilized
information finding relationships genetic variations clinical traits.
disease diagnosis, sparse approaches use classification models
consider order disease severity. subjects AD studies, natural
severity order normal mild cognitive impairment (MCI)
MCI AD. Classification models cannot capture order ADs severity levels.
previous methods designed handle heterogeneous data types.
SNPs values discrete (and ordinal based additive genetic model),
imaging features continuous. Popular CCA lasso-type methods simply treat
continuous data overlook heterogeneous nature data.
previous methods ignore cannot utilize valuable prior knowledge.
example, occurrence combinations alleles genetic markers population often less often would expected random
formation haplotypes alleles based frequencies, known
Linkage Disequilibrium (LD) (Falconer & Mackay, 1996). knowledge,
structure utilized association discovery.
address problems, propose new Bayesian approach unifies multiview
learning sparse ordinal regression joint association study disease diagnosis.
also conduct nonlinear classification latent variables (Zhe, Xu, Qi, & Yu, 2014)
find associations incorporating LD information additional prior SNPs
data (Zhe, Xu, Qi, & Yu, 2015) . detail, genetic variations phenotypical traits
generated common latent features based separate sparse projection matrices
suitable link functions, common latent features used predict disease status
(See Section 2). enforce sparsity projection matrices, assign spike slab priors
248

fiSimultaneous Association Discovery Diagnosis

(George & McCulloch, 1997) them; priors shown effective
l1 penalty learn sparse projection matrices (Goodfellow, Couville, & Bengio, 2012;
Mohamed et al., 2012). order take advantage linkage disequilibrium,
describes non-random association alleles different loci, employ additional
graph Laplacian type prior SNPs view. sparse projection matrices
reveal critical interactions different data sources also identify biomarkers
data relevant disease status. Meanwhile, via direct connection latent features,
disease status influences estimation projection matrices guide
discovery associations heterogeneous data sources relevant disease.
learn model data, develop variational inference approach (See Section
3). iteratively minimizes Kullback-Leibler divergence tractable approximation exact Bayesian posterior distributions. extend proposed sparse multiview
learning model incorporating linkage disequilibrium information SNPs Section 4. employ model real study AD Section 5. results show
model achieves highest prediction accuracy among competing methods. Furthermore, model finds biologically meaningful predictive relationships
SNPs, MRI features, AD status.

2. Sparse Heterogeneous Multiview Learning Models
section, first present notations assumptions, present sparse
heterogeneous multiview learning model.
2.1 Notations Assumptions
First, let us describe data. assume two heterogeneous data sources: one
contains continuous data example, MRI features contains discrete
ordinal data instance, SNPs. Note easily generalize model
handle views data types adopting suitable link functions (e.g., Poisson
model count data). Given data n subjects, p continuous features q discrete
features, denote continuous data p n matrix X = [x1 , . . . , xn ], discrete
ordinal data q n matrix Z = [z1 , . . . , zn ] labels (i.e., disease status)
n 1 vector = [y1 , . . . , yn ]> . AD study, let yi = 0, 1, 2 i-th subject
normal, MCI AD condition, respectively.
2.2 Spare Heterogeneous Multiview Learning Model
link two data sources X Z together, introduce common latent features
U = [u1 , . . . , un ] assume X Z generated U sparse projection.
common latent feature assumption sensible association studies SNPs
MRI features biological measurements subjects. Note ui latent
feature i-th subject dimension k. denote proposed Spare Heterogeneous
Multiview Learning
Q Model SHML. Bayesian framework, assign Gaussian prior
U, p(U) = N (ui |0, I), specify rest model (see Figure 1) follows.
249

fiXu, Zhe, Qi, & Yu

Sh

H



w

Sw

U

G

Sg



X

Z

Figure 1: graphical representation SHML, X continuous view, Z ordinal
view, labels.

2.2.1 Continuous Data Distribution
Given U, X generated
p(X|U, G, ) =

n


N (xi |Gui , 1 I)

i=1

G = [g1 , g2 , ...gp ]> p k projection matrix, identity matrix, 1
precision matrix Gaussian distribution. precision parameter ,
assign conjugate prior Gamma prior, p(|r1 , r2 ) = Gamma(|r1 , r2 ) r1 r2
hyperparameters set 103 experiments.
2.2.2 Ordinal Data Distribution
ordinal variable z {0, 1, . . . , R1}, value decided region auxiliary
variable c falls
= b0 < b1 < . . . < bR = .
c falls [br , br+1 ), z set r. AD study, SNPs Z take values {0, 1, 2}
therefore R = 3. Given q k projection matrix H = [h1 , h2 , ...hq ]> , auxiliary
variables C = {cij } ordinal data Z generated
p(Z, C|U, H) =

q
n


p(cij |hi , uj )p(zij |cij )

i=1 j=1


p(cij |hi , uj ) = N (cij |h>
uj , 1)
p(zij |cij ) =

2
X

(zij = r)(br cij < br+1 ).

r=0

(a) = 1 true (a) = 0 otherwise.
250

fiSimultaneous Association Discovery Diagnosis

2.2.3 Label Distribution
disease status labels ordinal variables too. generate y, use ordinal
regression model based latent representation U,
p(y, f |U, w) = p(y|f )p(f |U, w),
f latent continuous values corresponding y, w weight vector
latent features
p(fi |ui , w) = N (fi |u>
w, 1),
p(yi |fi ) =

2
X

(yi = r)(br fi < br+1 ).

r=0

Note labels linked data X Z via latent features U
projection matrices H G. Due sparsity H G, groups
variables X Z selected predict y.
2.2.4 Sparse Priors Projection Matrices Weights Vector
want identify critical interactions different data sources,
use spike slab prior (George & McCulloch, 1997) sparsify projection matrices G
H. spike slab priors continuous bimodal priors model hypervariance parameters, controls selection variable effective scale choosing
variable. apply spike slab prior weight vector w. Specifically,
use p k matrix Sg represent selection elements G: sgij = 1, gij selected
follows Gaussian prior distribution variance 12 ; sgij = 0, gij selected
forced almost zero (i.e., sampled Gaussian small variance 22 ).
following prior G:
p(G|Sg , g ) =

p
k


ij ij
p(gij |sij
g )p(sg |g )

i=1 j=1


ij
2
ij
2
p(gij |sij
g ) = sg N (gij |0, 1 ) + (1 sg )N (gij |0, 2 ),
ij
ij
p(sij
g |g ) = g

sij
g

ij

(1 gij )1sg ,

2
2
gij g probability sij
g = 1, 1 2 (in experiment, set
2
2
6
1 = 1 2 = 1o ). reflect uncertainty g , assign Beta hyperprior
distribution:
p
k

p(g |l1 , l2 ) =
Beta(gij |l1 , l2 ),
i=1 j=1

l1 l2 hyperparameters. set diffuse non-informative hyperprior, i.e.,
l1 = l2 = 1 experiments. Similarly, H sampled
p(H|Sh , h ) =

q
k

i=1 j=1

251

ij ij
p(hij |sij
h )p(sh |h ),

fiXu, Zhe, Qi, & Yu

sij

ij

ij
ij
ij ij
ij h
2
2
p(hij |sij
(1hij )1sh .
h ) = sh N (hij |0, 1 )+(1sh )N (hij |0, 2 ) p(sh |h ) = h
ij
ij
Sh binary selection variables h h probability sh = 1. assign Beta
hyperpriors h :
q
k

p(h |d1 , d2 ) =
Beta(hij |d1 , d2 ),
i=1 j=1

d1 d2 hyperparameters. set d1 = d2 = 1 experiments since
found sensitive final performance. Similarly weights vector w,
p(w|sw , w ) =

k


j
p(wj |sjw )p(sjw |w
)

j=1
sjw

j

j
j
j 1sw
p(wj |sjw ) = sjw N (wj |0, 12 ) + (1 sjw )N (wj |0, 22 ) p(sjw |w
) = w
(1 w
)
.
j
j
sw binary selection variables w w probability sw = 1. assign
Beta hyperpriors w :

p( w ) =

k



Beta(w
|e1 , e2 ),

i=1

e1 e2 hyperparameters. similarly set e1 = e2 = 1 experiments.
2.2.5 Joint Distribution
Based specifications, joint distribution model
p(X, Z, y, U, G, Sg , g , , C, H, H, Sh , h , Sw , w , f )
= p(X|U, G, )p(G|Sg )p(Sg |g )p(g |l1 , l2 )p(|r1 , r2 )
p(Z, C|U, H)p(H|Sh )p(Sh |h )p(h |d1 , d2 )
p(y|f )p(f |U, w)p(w|Sw )p(Sw |w )p(U).

(1)

Different Figure 1, put conjugate prior Sw Sg joint distribution. next step estimate distributions latent variables
hyperparemeters.

3. Model Inference
Given model specified previous section, present efficient method
estimate latent features U, projection matrices H G, selection indicators Sg
Sh , selection probabilities g h , variance , auxiliary variables C
generating ordinal data Z, auxiliary variables f generating labels y, weights
vector w generating f corresponding selection indicators probabilities sw
w . Bayesian framework, estimation task amounts computing posterior
distributions.
However, computing exact posteriors turns infeasible since cannot
calculate normalization constant posteriors based Equation (1). Thus,
252

fiSimultaneous Association Discovery Diagnosis

resort mean-field variational approach. Specifically, approximate posterior
distributions U, H, G, Sg , Sh , g , h , , w, C f factorized distribution
Q() = Q(U)Q(H)Q(G)Q(Sg )Q(Sh )Q(g )Q(h )Q()Q(w)Q(C)Q(f )

(2)

denotes latent variables.
Variational inference minimizes Kullback-Leibler (KL) divergence approximate exact posteriors
min KL (Q()kp(|X, Z, y))

(3)

Q()

specifically, using coordinate descent algorithm, variational approach updates
one approximate distribution, e.g, q(H), Equation (2) time
others fixed. detailed updates given following paragraphs.
3.1 Updating Variational Distributions Continuous Data
continuous data X, approximate distributions projection matrix G,
noise variance , selection indicators Sg selection probabilities g
Q(G) =

Q(Sg ) =

p


N (gi ; , ),

i=1
p
k


sij

(4)
ij

ijg (1 ij )1sg ,

(5)

Beta(gij |l1ij , l2ij ),

(6)

i=1 j=1

Q(g ) =

p
k

i=1 j=1

Q() = Gamma(|r1 , r2 ).

(7)

mean covariance gi calculated follows:
= hihUU> +

1
1
1
diag(hsig i) + 2 diag(1 hsig i) ,
2
1
2

= (hihUixi ),
hi means expectation distribution, xi sig transpose i-th
2 j-th diagonal element .
rows X Sg , hsig = [i1 , . . . , ik ]> , hgij

computation parameters ij Q(gij ) found Appendices A.
3.2 Updating Variational Distributions Ordinal Data
ordinal data Z, update approximate distributions projection matrix H,
auxiliary variables C, sparse selection indicators Sh selection probabilities
h . make variational distributions tractable, update Q(H) column-wise
253

fiXu, Zhe, Qi, & Yu

way re-denote H = [h1 , h2 , ...hk ], Sh = [s1h , s2h , ...skh ] U = [u1 , u2 , ...uk ]> .
variational distributions C H
Q(C) =

q
k


Q(cij ),

(8)

i=1 j=1

Q(cij ) (bzij cij < bzij +1 )N (cij |cij , 1),
Q(H) =

k


N (hi ; , ),

(9)
(10)

i=1

1
cij = (hHihUi)ij , = hui > ui iI+ 12 diag(hsih i)+ 12 diag(h1sih i) , = Ci hui
1
2
P
Ci = C j6=i j huj i> . computation parameters distributions Sh
h given Appendices B.
3.3 Updating Variational Distributions Labels
ordinal labels y, update approximation distributions auxiliary variables
f , weights vector w, sparse selection indicators sw selection probabilities
w . variational distributions f w
Q(f ) =

n


Q(fi ),

(11)

i=1

Q(fi ) (byi fi < byi +1 )N (fi |fi , f2i ),

(12)

Q(w) = N (w; m, w ),

(13)

1
= w hUihf i.
fi = (hUi> m)i , w = hUU> i+ 12 diag(sw )+ 12 diag(1sw )
1
2
computation parameters variational distributions sw w found
Appendices C.
3.4 Updating Variational Distributions Latent Representation U
variational distribution U given

Q(U) =
N (ui |i , )

(14)




1

(15)

= (hwihfi + hihGi xi + hHi hci i).

(16)

= hww> + hihG> Gi + hH> Hi +
>

>

required moments given Appendices D.
3.5 Label Prediction
Let us denote training data Dtrain = {Xtrain , Ztrain , ytrain } test data
Dtest = {Xtest , Ztest }. prediction task needs latent representation Utest Dtest .
254

fiSimultaneous Association Discovery Diagnosis

carry variational inference simultaneously Dtrain Dtest . Q(Utest )
Q(Utrain ) obtained, predict labels test data follows:
ftest = hUtest i> m,

ytest
=

R1
X


r (br ftest
< br+1 ),

(17)
(18)

r=0

ytest
prediction i-th test sample.

4. Sparse Heterogeneous Multiview Learning Model Linkage
Disequilibrium Priors
population genetics, lLinkage Disequilibrium (LD) refers non-random association
alleles different loci, i.e., presence statistical associations alleles
different loci different would expected alleles independently,
randomly sampled based individual allele frequencies (Slatkin, 2008).
linkage disequilibrium alleles different loci said linkage
equilibrium.
Linkage Disequilibrium also appears SNPs, measure pairs
SNPs regarded natural indicator correlation SNPs.
information publicly retrieved www.ncbi.nlm.nih.gov/books/NBK44495/.
incorporate correlation prior model, first introduce latent q k matrix
H, tightly linked H explained later. column hj H regularized
graph Laplacian LD structure, i.e.,

p(H|L) =
N (hj |0, L1 )
j

=



N (0|hj , L1 )

j

= p(0|H, L),
L graph Laplacian matrix LD structure. shown above, prior
p(H|L) form p(0|H, L), viewed generative model
words, observation 0 sampled H. view enables us combine
generative model graph Laplacian regularization sparse projection model via
principled hybrid Bayesian framework (Lasserre et al., 2006).
link two models together, introduce prior H:

p(H|H) =
N (hj |hj , I)
j

variance controls similar H H model. simplicity,
set = 0 p(H|H) = Dirac(H H) Dirac(a) = 1 = 1 Dirac(a) = 0
= 0.
Adopting additional information, new graphical model designed shown
Fig. 2.
255

fiXu, Zhe, Qi, & Yu

L

Sh



w

Sw

hj

H

U

G

Sg

0

Z



X

Figure 2: graphical representation model, X continuous view, Z
ordinal view, labels L graph laplacian generated LD structure.

Based specifications, joint distribution model
p(X, Z, y, U, G, Sg , g , , C, H, H, Sh , h , Sw , w , f )
= p(X|U, G, )p(G|Sg )p(Sg |g )p(g |l1 , l2 )p(|r1 , r2 )
p(Z, C|U, H)p(H|Sh )p(Sh |h )p(h |d1 , d2 )p(H|H)
p(0|H, L)p(y|f )p(f |U, w)p(w|Sw )p(Sw |w )p(U).

(19)

inference almost original model described Section 2, except
updating sparse projection matrix H. Given ordinal data Z updates
variables, update approximate distributions projection matrix H,
auxiliary variables C, sparse selection indicators Sh selection probabilities h .
variational distributions C H
Q(C) =

q
k


Q(cij ),

(20)

i=1 j=1

Q(cij ) (bzij cij < bzij +1 )N (cij |cij , 1),
Q(H) =

k


N (hi ; , ),

(21)
(22)

i=1

1
cij = (hHihUi)ij , = hui > ui iI + L + 12 diag(hsih i) + 12 diag(h1 sih i) , =
1
2
P
Ci hui Ci = C j6=i j huj i> . updating variables remains same.

5. Experimental Results Discussion
order examine performance proposed method , design simulation study
realworld study Alzheimers Disease.
256

fiSimultaneous Association Discovery Diagnosis

5.1 Simulation Study
first design simulation study examine basic model, i.e., model, terms
(i) estimation accuracy finding associations two views (ii) prediction
accuracy ordinal labels. Note similar study conducted model
LD priors.
5.1.1 Simulation Data
generate ground truth, set n = 200 (200 instances), p = q = 40, k = 5.
designed G, 40 5 projection matrix continuous data X, block diagonal
matrix; column G 8 elements ones rest zeros,
ensuring row one nonzero element. designed H, 40 5 projection
matrix ordinal data Z, block diagonal matrix; first four columns
H 10 elements ones rest zeros, fifth column
contained zeros. randomly generated latent representations U Rkn
column ui N (0, I). generate Z, first sampled auxiliary variables C
column ci N (Hui , 1), P
decided value element zij region
cij fell inin words, zij = 2r=0 r(br cij < br+1 ). Similarly, generate y,
sampled auxiliary variables f N (0, U> U + I) yi generated
p(yi |fi ) = (yi = 0)(fi 0) + (yi = 1)(fi > 0).
5.1.2 Comparative Methods
compared model several state-of-the-art methods including (1) CCA (Bach &
Jordan, 2005), finds projection direction maximizes correlation
two views, (2) sparse CCA (Sun, Ji, & Ye, 2011; Daniela & Tibshirani, 2009),
sparse priors put CCA directions, (3) multiple-response regression lasso
(MRLasso) (Kim, Sohn, & Xing, 2009) column second view (Z) regarded
output first view (X). include results sparse probabilistic
projection approach (Archambeau & Bach, 2009) performed unstably
experiments. Regarding software implementation, used built-in Matlab routine
CCA code (Sun et al., 2011) sparse CCA. implemented MRLasso
based Glmnet package (cran.r-project.org/web/packages/glmnet/index.html).
test prediction accuracy, compared proposed SHML model based
Gaussian process prior following ordinal multinomial regression methods: (1)
lasso multinomial regression (Tibshirani, 1994), (2) elastic net multinomial regression
(Zou & Hastie, 2005), (3) sparse ordinal regression spike slab prior, (4) CCA
+ lasso, first ran CCA obtain latent features H applied lasso
predict y, (5) CCA + elastic net, first ran CCA obtain projection
matrices applied elastic net projected data, (6) Gaussian Process Ordinal
Regression (GPOR) (Chu & Ghahramani, 2005), (7) Laplacian Support Vector Machine
(LapSVM) (Melacci & Mikhail, 2011), semi-supervised SVM classification method.
used published code lasso, elastic net, GPOR LapSVM. methods,
used 10-fold cross validation training data run choose kernel form
(Gaussian linear Polynomials) parameters (the kernel width polynomial
orders) model, GPOR, LapSVM.
257

fiXu, Zhe, Qi, & Yu

alternative methods cannot learn dimension automatically simple comparison, provided dimension latent representation methods tested
simulations. partitioned data 10 subsets used 9 training
1 subset testing; repeated procedure 10 times generate averaged test
results.
5.1.3 Results
estimate linkage (i.e., interactions) X Z, calculated cross covariance
matrix GH> . computed precision recall based ground truth.
precision-recall curves shown Figure 3. Clearly, method successfully recov1

SHML
0.9
0.8

Precision

0.7

Sparse CCA

0.6

MRLasso

0.5
0.4
0.3

CCA

0.2
0.1
0

0

0.2

0.4

0.6

0.8

1

Recall

Figure 3: precision-recall curves association discovery.

ered almost links significantly outperformed competing methods.
improvement may come i) use spike slab priors, remove
irrelevant elements projection matrices also avoid over-penalizing active association structures (the Laplace prior used sparse CCA penalize relevant
ones) ii) importantly, supervision labels y, probably
biggest difference methods association study. failing
CCA sparse CCA may due insufficient representation sources data
caused using one projection direction. prediction accuracies unknown
standard errors shown Figure 4a AUC standard errors
shown Figure 4b. proposed SHML model achieves significant improvement
methods. reduces prediction error elastic net (which ranks second
best) 25%, reduces error LapSVM 48%.
5.2 Real-World Study Alzheimers Disease
Alzheimers Disease common form dementia 30 million patients
worldwide payments care estimated $200 billion 2012 (Alzheimers
258

fiSimultaneous Association Discovery Diagnosis

0.95

Area Curve

0.9

Precision

0.85

0.8

LapSVM
Lasso
ElasticNet
SparseOR
GPOR
CCA + Lasso
CCA + ElasticNet
SHML

0.9

0.85

0.75
0.8

(a) Precision simulation

(b) AUC simulation

Figure 4: prediction results simulated real datasets. results averaged 10
runs. error bars represent standard errors.

Association, 2012). conducted association analysis diagnosis AD based
dataset Alzheimers Disease Neuroimaging Initiative(ADNI) 1 . ADNI study
longitudinal multisite observational study elderly individuals normal cognition, mild
cognitive impairment, AD. applied proposed method study associations
genotypes brain atrophy measured MRI predict subject status
(normal vs MCI vs AD). Note statuses ordinal since represent increasing
severity levels.
removing missing values, data set consists 625 subjects including183 normal,
308 MCI 134 AD cases, subject contains 924 SNPs 328 MRI features.
selected SNPs top SNPs separating normal subjects AD ADNI.
MRI features measure brain atrophies different brain regions based cortical
thickness, surface areas volumes, obtained FreeSurfer software 2 . test
diagnosis accuracy, compared method previously mentioned ordinal
multinomial regression methods. employ extended model linkage disequilibrium
priors, denoted SHML-LD, discover associations.
compare SHML SHML-LD state-of-the-art classification methods.
used 10-fold cross validation run tune free parameters training
data. determine dimension k latent features U method, computed
variational lower bounds approximation model marginal likelihood (i.e.,
evidence), various k values {10, 20, 40, 60}. chose value largest approximate evidence, led k = 20 (see Figure 5). experiments confirmed
k = 20, model achieved highest prediction accuracy, demonstrating benefit
evidence maximization.
shown Figure 6, method achieved highest prediction accuracy, higher
second best method, GP ordinal Regression, 10% worst
method, CCA+lasso, 22%. two-sample test shows model outperforms
alternative methods significantly (p < 0.05).
1. http://adni.loni.ucla.edu/
2. http://surfer.nmr.mgh.harvard.edu

259

fiXu, Zhe, Qi, & Yu

5

x 10

Evidence Lower Bound

7.4

7.42

7.44

7.46

7.48

10

20
40
Dimensions

60

Figure 5: variational lower bound model marginal likelihood.

0.64

Precision

0.6

0.55

0.5



L





SH

C

C



C

+

SH

et
N
ic

El




La




R
+
C


R

PO
G

se


ic

N
et

Sp
ar






La


El

La
pS

VM

0.46

Figure 6: prediction accuracy standard errors real data.
also examined strongest associations discovered model. Firstly, ranking MRI features terms prediction power three different disease populations
(normal, MCI AD) demonstrate top ranked features based
cortical thickness measurement. hand, features based volume
260

fiSimultaneous Association Discovery Diagnosis

CT std R. CaudalAnteriorCingulate
CT std L. SuperiorParietal
CT std R. Postcentral
CT std R. SuperiorParietal
CT std L. Precentral
Vol (WMP) CorpusCallosumMidPosterior
Vol (WMP) CorpusCallosumCentral
Vol (WMP) CorpusCallosumPosterior
Vol (WMP) CorpusCallosumMidAnterior
Vol (WMP) L. CerebellumWM
Vol (WMP) R. CerebellumWM
Vol (WMP) FifthVentricle
Vol (WMP) NonWMHypoIntensities
Surf Area L. Unknown
Vol (WMP) ThirdVentricle
Vol (WMP) R. LateralVentricle
Vol (WMP) L. LateralVentricle
Vol (WMP) R. Caudate

2

1

0

1

CAP
CAPZB(rs7
CA ZB(rs 04415
CAPPZB(rs43692 0)
52
8
Z
CAP B(rs1605023 )
2
CAPZB(rs1936880)
CAPZB(rs732472 )
7
BCAZB(rs914550 )
9
BCAR3(rs188785 )
9)
5
R
5

P3K3(rs38 3833)
NCO 1(rs1 5803
NCO A2(r 1318 8)
s80
77)

NC 2(rs12 14818
TRAOA2(rs588339)
TRA F3(rs 76130 )
7
8
F
TR 3(rs1252178 )
TRAAF3(rs 896382)
F3( 2533 1)
rs13
0
260 59)
060
)

2

(a)

CAP
Z
CAP B(rs70
44
Z
CAP B(rs80 150)
5
CAP ZB(rs4 0232)
369
ZB
CAP (rs169 252)
368
ZB(
80)
rs7
CAP
ZB( 14550
rs13
9)
CAP
24
Z
TRA B(rs98 727)
878
F3(
TRA rs1326 59)
0
F
TRA 3(rs25 060)
330
F3(
TRA rs1289 59)
6
F
BCA 3(rs75 381)
21
R
BCA 3(rs15 782)
5
R3(rs3 3833)
P3K
858
NCO 1(rs11 038)
3
NCO A2(rs8 1877)
014
A2(
8
r
NCO s1258 18)
83
A2(
rs76 39)
130
8)

Vol (WMP) R. Hippocampus
Vol (WMP) L. Hippocampus
Vol (CP) R. Parahippocampal
CT std L. Unknown
CT std R. Unknown
Vol (CP) L. Entorhinal
Vol (CP) R. Entorhinal
CT Avg R. Entorhinal
CT Avg L. Entorhinal
Vol (CP) L. Parahippocampal
CT Avg R. Parahippocampal
CT Avg L. Parahippocampal
CT Avg R. Unknown
CT Avg L. Unknown
Vol (WMP) L. Amygdala
Vol (CP) R. Unknown
Vol (WMP) R. Amygdala
Vol (CP) L. Unknown

(b)

Figure 7: estimated associations MRI features SNPs. sub-figure,
MRI features listed right SNP names given
bottom.

surface area estimation less predictive. Particularly, thickness measurements middle
temporal lobe, precuneus, fusiform found predictive compared
brain regions. findings consistent memory-related function
regions findings literature prediction power AD. also found
measurements structure left right sides similar weights,
indicating algorithm automatically select correlated features groups, since
asymmetrical relationship found brain regions involved AD.
Secondly, analysis associating genotype AD prediction also generated interesting results. Similar MRI features, SNPs vicinity often
selected together, indicating group selection characteristics algorithm. example, top ranked SNPs associated genes including CAPZB (F-actin-capping
261

fiXu, Zhe, Qi, & Yu

protein subunit beta), NCOA2 (The nuclear receptor coactivator 2) BCAR3(Breast
cancer anti-estrogen resistance protein 3).
last, biclustering gene-MRI associations, shown Figure 7, reveals interesting patterns terms relationship genetic variations brain atrophy
measured structural MRI. example, top ranked SNPs associated
genes including BCAR3 (Breast cancer anti-estrogen resistance protein 3) NCOA2,
MAP3K1 (mitogen-activated protein kinase kinase kinase 1) studied
carefully cancer research. set SNPs associated cingulate negative
directions, part limbic system involves emotion formation processing. Compared structures temporal lobe, plays important
role formation long-term memory. example, association MAP3K1
caudate anterior cingulate cortex identified. Literature shown
MAP3K1 associated biological processes apoptosis, cell cycle, chromatin
binding DNA binding3 , cingulate cortex shown severely affected
AD (Jones et al., 2006). strong association discovered work might indicate
potential genetic effects atrophy pattern observed cingulate subregion.

6. Related Work
proposed model model related broad family probabilistic latent variable
models, including probabilistic principle component analysis (Tipping & Bishop, 1999),
probabilistic canonical correlation analysis (Bach & Jordan, 2005) extensions (Yu,
Yu, Tresp, Kriegel, & Wu, 2006; Archambeau & Bach, 2009; Guan & Dy, 2009; Virtanen,
Klami, & Kaski, 2011). learn latent representation whose projection leads
observed data. Recent studies probabilistic factor analysis methods put focus
sparsity-inducing priors projection matrix. Among them, Guan Dy (2009)
used Laplace prior, Jeffreys prior, inverse-Gaussian prior; Archambeau
Bach (2009) employed inverse-Gamma prior; Virtanen et al. (2011) used Automatic Relevance Determination(ARD) prior. Despite success, sparsity-inducing
priors disadvantages confound degree sparsity degree
regularization relevant irrelevant variables, practical settings
little reason two types complexity control tightly bounded
together. Although inverse-Gaussian prior inverse-Gamma prior provide
flexibility controlling sparsity, suffer highly sensitive controlling parameters thus lead unstable solutions. contrast, model adopts spike
slab prior, recently used multi-task multiple kernel learning (Titsias
& Lazaro-Gredilla, 2011), sparse coding (Goodfellow et al., 2012), latent factor analysis (Carvalho, Chang, Lucas, Nevins, Wang, & West, 2008). Note Beta priors
selection indicators lead simple yet effective variational updates, hierarchical prior work Carvalho et al.(2008) better handle selection uncertainty.
Regardless priors assigned spike slab models, generally avoid
confounding issue separately controlling projection sparsity regularization
effect selected elements.
3. https://portal.genego.com/

262

fiSimultaneous Association Discovery Diagnosis

SHML also connected many methods learning multiple sources
views (Hardoon, Leen, Kaski, & Shawe-Taylor, 2008). Multiview learning methods
often used learn better classifier multi-label classification usually text mining
image classification domains based correlation structures among training data
labels (Yu et al., 2006; Virtanen et al., 2011; Rish, Grabarnik, Cecchi, Pereira, &
Gordon, 2008). However, medical analysis diagnosis, meet two separate tasks
association discovery genetic variations clinical traits, diagnosis
patients. proposed SHML conducts two tasks simultaneously: employs
diagnosis labels guide association discovery, leveraging association structures
improve diagnosis. particular, diagnosis procedure SHML leads ordinal
regression model based latent Gaussian process models. latent Gaussian process
treatment differentiates multiview CCA models (Rupnik & Shawe-Taylor, 2010).
Moreover, multiview learning methods model heterogeneous data types
different views, simply treat continuous data. simplification
degrate predictive performance. Instead, based probabilistic framework , SHML
uses suitable link functions fit different types data.

7. Conclusions
presented new Bayesian multiview learning framework simultaneously find
key associations data sources (i.e., genetic variations phenotypic traits)
predict unknown ordinal labels. shown model also employ background
information, e.g., Linkage Disequilibrium information, via additional graph Laplacian
type prior. proposed approach follows generative model: extracts common
latent representation encodes structural information within data views,
generates data via sparse projections. encoding knowledge multiple
views via latent representation makes possible effectively detect associations
high sensitivity specificity.
Experimental results ADNI data indicate model found biologically meaningful associations SNPs MRI features led significant improvement
predicting ordinal AD stages alternative classification ordinal regression
methods. Despite drawbacks proposed framework slow training speed requirement careful tuning parameters, strong modeling power due Bayesian
nature. Although focused AD study, expect model, powerful extension CCA, applied wide range applications biomedical research
example, eQTL analysis supervised additional labeling information.

Acknowledgments
Data used preparation article obtained Alzheimers Disease Neuroimaging Initiative (ADNI) database (adni.loni.ucla.edu). such, investigators within
ADNI contributed design implementation ADNI and/or provided data
participate analysis writing report. complete listing ADNI investi263

fiXu, Zhe, Qi, & Yu

gators found at: http://adni.loni.ucla.edu/wp-content/uploads/how apply/ADNI
Acknowledgement List.pdf.
work supported NSF IIS-0916443, IIS-1054903, CCF-0939370, NSF China
(Nos. 61572111, 61433014, 61440036), 973 project china (No.2014CB340401), 985
Project UESTC (No.A1098531023601041) Basic Research Project China Central
University ( No. ZYGX2014J058).
Zenglin Xu Shandian Zhe equal contributions article. Yuan Qi
Principle corresponding author.

Appendix A. Parameter Update Continuous Data
parameter ij Q(sij
g ) introduced Section 3.1 calculated ij = 1/ 1 +

2
ij
ij
2 i( 1 1 )) . parameters Beta
exp(hlog(1 g )i hlog(g )i + 21 log( 12 ) + 12 hgij
2
2


2
1
2
distribution Q(gij ) given lij = ij + l1 lij = 1 ij + l2 . parameters
1

2

1
>
Gamma distribution Q() updated r1 = r1 + np
2 r2 = r2 + 2 tr(XX )
1
>
>
>
tr(hGihUiX ) + 2 tr(hUU ihG Gi).
moments required distributions calculated hi = rr12

hlog(gij )i = (l1ij ) (l1ij + l2ij ),
hlog(1 gij )i = (l2ij ) (l1ij + l2ij ),
>

hG Gi =

p
X

+ >
,

i=1

hGi = [1 , . . . , p ]> ,
(x) =


dx

(23)

ln (x).

Appendix B. Parameter Update Ordinal Data
variational distributions Sh h introduced Section 3.2 given
Q(Sh ) =

q
k


sij

ij

ijh (1 ij )1sh ,

(24)

Beta(hij |dij1 , dij2 ),

(25)

i=1 j=1

Q(h ) =

q
k

i=1 j=1


2
ij = 1/ 1+exp(hlog(1hij )ihlog(hij )i+ 12 log( 12 )+ 12 hh2ij i( 12 12 )) , dij
1 = ij +d1 ,
2
1
2
ij

>
2
d2 = 1 ij + d2 , hsh = [1i , . . . , qi ] , hhij i-th diagonal element j .
required moments updating distributions calculated follows:
hlog( ij )i = (dij ) (dij + dij ),
h
hij )i

1

1

2

ij ij
hlog(1
= (dij
2 ) (d1 + d2 ),
N (bzij +1 |cij , 1) N (bzij |cij , 1)
,
hcij = cij
(bzij +1 cij ) (bzij cij )
264

fiSimultaneous Association Discovery Diagnosis

() cumulative distribution function standard Gaussian distribution. Note
Equation (26), Q(cij ) truncated Gaussian truncation controlled
observed ordinal data zij .

Appendix C. Parameter Update Labels
variational distributions sw w Section 3.3 given
Q(sw ) =
Q( w ) =

k

i=1
k


si



w (1 )1sw ,

(26)


Beta(w
; ei1 , ei2 ),

(27)

i=1
)i hlog( )i + 1 hw 2 i( 1
= 1/ 1 + exp(hlog(1 w
w
2
2
1

1
))
22



, ei1 = + e1

ei2 = 1 + e2 .
required moments updating distributions calculated follows:

hlog(w
)i = (ei1 ) (ei1 + ei2 ),

hlog(1 w
)i = (ei2 ) (ei1 + ei2 ),
N (byi +1 |fi , 1) N (byi |fi , 1)
hfi = fi
.
(by +1 fi ) (by fi )




Note Q(fi ) also truncated Gaussian truncated region decided ordinal label yi . way, supervised information incorporated estimation
f estimation quantities recursive updates.

Appendix D. Parameter Update Latent Representation U
>
required moments hww> i, hG> Gi
Section 3.4 calculated
Ppand hH Hi introduced
>
>
>
hww = w + mm , hG Gi = i=1 + >


(
trace(i + >
) i=j
(hH> Hi)ij =
.
>
j
6= j

required moments already listed previous sections. moments regarding U required
variational distributionsPare hUi =
Pn the> updates
n
>


2
[1 , 2 , ...n ], hUU = i=1 +i , hui = [1 , 2 , ...in ]> hu>
ui =
j=1 (j ) +
(j )ii .

References
Alzheimers Association (2012). 2012 facts figures alzheimers disease facts figures.
Tech. rep..
Archambeau, C., & Bach, F. (2009). Sparse probabilistic projections. Advances Neural
Information Processing Systems 21, pp. 7380.
265

fiXu, Zhe, Qi, & Yu

Bach, F., & Jordan, M. (2005). probabilistic interpretation canonical correlation
analysis. Tech. rep., UC Berkeley.
Carvalho, C., Chang, J., Lucas, J., Nevins, J., Wang, Q., & West, M. (2008). Highdimensional sparse factor modeling: applications gene expression genomics. Journal
American Statistical Association, 103 (484), 14381456.
Chen, X., Liu, H., & Carbonell, J. (2012). Structured sparse canonical correlation analysis..
AISTATS12, Vol. 22, pp. 199207.
Chu, W., & Ghahramani, Z. (2005). Gaussian processes ordinal regression. Journal
Machine Learning Research, 6, 10191041.
Consoli, L., Lefevre, A., Zivy, M., de Vienne, D., & Damerval, C. (2002). QTL analysis
proteome transcriptome variations dissecting genetic architecture
complex traits maize. Plant Mol Biol., 48 (5), 575581.
Daniela, M., & Tibshirani, R. (2009). Extensions sparse canonical correlation analysis,
applications genomic data. Stat Appl Genet Mol Biol., 383 (1).
Falconer, D., & Mackay, T. (1996). Introduction Quantitative Genetics (4th ed.). Addison
Wesley Longman.
Gandhi, S., & Wood, N. (2010). Genome-wide association studies: key unlocking
neurodegeneration?. Nature Neuroscience, 13, 789794.
George, E., & McCulloch, R. (1997). Approaches bayesian variable selection.. Statistica
Sinica, 7 (2), 339373.
Goodfellow, I., Couville, A., & Bengio, Y. (2012). Large-scale feature learning spikeand-slab sparse coding. Proceedings International Conference Machine Learning.
Guan, Y., & Dy, J. (2009). Sparse probabilistic principal component analysis. Journal
Machine Learning Research - Proceedings Track, 5, 185192.
Hardoon, D., Leen, G., Kaski, S., & Shawe-Taylor, J. (Eds.). (2008). NIPS Workshop
Learning Multiple Sources.
Harold, H. (1936). Relations two sets variates. Biometrika, 28, 321377.
Hunter, D. (2012). Lessons genome-wide association studies epidemiology. Epidemiology, 23 (3), 363367.
Jones, B. F., et al. (2006). Differential regional atrophy cingulate gyrus Alzheimer
disease: volumetric MRI study. Cereb. Cortex, 16 (12), 17011708.
Khachaturian, S. (1985). Diagnosis Alzheimers disease. Archives Neurology, 42 (11),
10971105.
Kim, S., Sohn, K., & Xing, E. (2009). multivariate regression approach association
analysis quantitative trait network. Bioinformaics, 25 (12), 204212.
Lasserre, J., et al. (2006). Principled hybrids generative discriminative models.
IEEE Computer Society Conference Computer Vision Pattern Recognition,
Vol. 1, pp. 8794.
266

fiSimultaneous Association Discovery Diagnosis

Liu, J., Pearlson, G., Windemuth, A., Ruano, G., Perrone-Bizzozero, N., & Calhoun, V.
(2009). Combining fMRI SNP data investigate connections brain
function genetics using parallel ICA. Hum Brain Mapp, 30 (1), 241255.
MacKay, D. (1991). Bayesian interpolation. Neural Computation, 4, 415447.
Melacci, S., & Mikhail, B. (2011). Laplacian support vector machines trained primal.
Journal Machine Learning Research, 12, 11491184.
Mohamed, S., et al. (2012). Bayesian L1 approaches sparse unsupervised learning.
Proceedings International Conference Machine Learning.
Neal, R. M. (1996). Bayesian Learning Neural Networks. Springer-Verlag New York,
Inc.
Parkhomenko, E., Tritchler, D., & Beyene, J. (2007). Genome-wide sparse canonical correlation gene expression genotypes. BMC Proc.
Rish, I., Grabarnik, G., Cecchi, G., Pereira, F., & Gordon, G. (2008). Closed-form supervised dimensionality reduction generalized linear models. Proceedings
International Conference Machine Learning08, pp. 832839.
Rupnik, J., & Shawe-Taylor, J. (2010). Multi-view canonical correlation analysis. Proceedings SIG Conference Knowledge Discovery Mining10.
Slatkin, M. (2008). Linkage disequilibrium understanding evolutionary past
mapping medical future. Nature Reviews Genetics, Vol. 6, pp. 477485.
Sun, L., Ji, S., & Ye, J. (2011). Canonical correlation analysis multi-label classification:
least squares formulation, extensions analysis. IEEE Transactions Pattern
Analysis Machine Intelligence, 33 (1), 194200.
Tibshirani, R. (1994). Regression shrinkage selection via lasso. Journal Royal
Statistical Society, Series B, 58, 267288.
Tipping, M., & Bishop, C. (1999). Probabilistic principal component analysis. Journal
Royal Statistical Society Series B-statistical Methodology, 61, 611622.
Titsias, M., & Lazaro-Gredilla, M. (2011). Spike slab variational inference multitask multiple kernel learning. Advances Neural Information Processing
Systems11, pp. 23392347.
Virtanen, S., Klami, A., & Kaski, S. (2011). Bayesian CCA via group sparsity. Proceedings
International Conference Machine Learning11, pp. 457464.
Yu, S., Yu, K., Tresp, V., Kriegel, H., & Wu, M. (2006). Supervised probabilistic principal
component analysis. Proceedings SIG Conference Knowledge Discovery
Mining06, pp. 464473.
Yuan, M., & Lin, Y. (2007). Model selection estimation regression grouped
variables.. Journal Royal Statistical Society, Series B, 68 (1), 4967.
Zhe, S., Xu, Z., Qi, Y., & Yu, P. (2014). Supervised heterogeneous multiview learning
joint association study disease diagnosis. Pacific Symposium Biocomputing,
19.
267

fiXu, Zhe, Qi, & Yu

Zhe, S., Xu, Z., Qi, Y., & Yu, P. (2015). Sparse bayesian multiview learning simultaneous association discovery diagnosis alzheimers disease. Proceedings
Twenty-Ninth AAAI Conference Artificial Intelligence, January 25-30, 2015,
Austin, Texas, USA., pp. 19661972.
Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net.
Journal Royal Statistical Society, Series B, 67, 301320.

268

fiJournal Artificial Intelligence Research 56 (2016) 463-515

Submitted 12/15; published 07/16

Computing Repairs Inconsistent DL-Programs EL Ontologies
EITER @ KR . TUWIEN . AC .

Thomas Eiter
Michael Fink
Daria Stepanova

FINK @ KR . TUWIEN . AC .
DASHA @ KR . TUWIEN . AC .

Institut fr Informationssysteme, TU Wien,
Favoritenstrae 9-11, 1040 Vienna, Austria

Abstract
Description Logic (DL) ontologies non-monotonic rules two prominent Knowledge
Representation (KR) formalisms complementary features essential various applications. Nonmonotonic Description Logic (DL) programs combine formalisms thus providing support rule-based reasoning top DL ontologies using well-defined query interface
represented so-called DL-atoms. Unfortunately, interaction rules ontology may
incur inconsistencies DL-program lacks answer sets (i.e., models), thus yields
information. issue addressed recently defined repair answer sets, computing
effective practical algorithm proposed DL-Lite ontologies reduces repair computation constraint matching based so-called support sets. However, algorithm exploits
particular features DL-Lite readily applied repairing DL-programs
prominent DLs like EL. Compared DL-Lite , EL support sets may neither small
support sets might exist, completeness algorithm may need given
support information bounded. thus provide approach computing repairs
DL-programs EL ontologies based partial (incomplete) support families. latter
constructed using datalog query rewriting techniques well ontology approximation based
logical difference EL-terminologies. show maximal size number support sets given DL-atom estimated analyzing properties support hypergraph,
characterizes relevant set TBox axioms needed query derivation. present declarative implementation repair approach experimentally evaluate set benchmark
problems; promising results witness practical feasibility repair approach.

1. Introduction
Description Logics (DLs) powerful formalism Knowledge Representation (KR)
used formalize domains interest describing meaning terms relationships
them. well-suited terminological modelling contexts as, Semantic Web, data
integration ontology-based data access (Calvanese, De Giacomo, Lenzerini, Lembo, Poggi, &
Rosati, 2007b; Calvanese, De Giacomo, Lembo, Lenzerini, Poggi, & Rosati, 2007a), reasoning
actions (Baader, Lutz, Milicic, Sattler, & Wolter, 2005), spatial reasoning (zccep & Mller,
2012), runtime verification program analysis (Baader, Bauer, & Lippmann, 2009; Kotek,
Simkus, Veith, & Zuleger, 2014), mention few.
DLs fragments classical first-order logic, shortcomings modelling application settings, nonmonotonicity closed-world reasoning needs expressed.
Rules nonmonotonic logic programming offer features. addition, serve well
tool declaring knowledge reasoning individuals, modelling nondeterminism model generation possible Answer Set Programming. get best two
c
2016
AI Access Foundation. rights reserved.

fiE ITER , F INK & TEPANOVA



(1) Blacklisted Staff




(2) StaffRequest hasAction.Action hasSubject.Staff hasTarget.Project
= (3) BlacklistedStaffRequest StaffRequest hasSubject.Blacklisted



(4) StaffRequest(r1 ) (5) hasSubject(r1 , john) (6) Blacklisted (john)



(7) hasTarget(r1 , p1 ) (8) hasAction(r1 , read ) (9) Action(read )


(10) projfile(p1 ); (11) hasowner (p1 , john);






(12) chief (Y ) hasowner (Z , ), projfile(Z );









(13) grant(X) DL[Project projfile; StaffRequest](X), deny(X);
P=




(14) deny(X) DL[Staff chief ; BlacklistedStaffRequest](X);





(15)


hasowner
(Y,
Z),

grant(X),






DL[; hasTarget](X, ), DL[; hasSubject](X, Z).















Figure 1: DL-program policy ontology
worlds DLs nonmonotonic rules, natural idea combining led number
approaches combination, often called hybrid knowledge bases; see work
Motik Rosati (2010) references therein. Among them, Nonmonotonic Description Logic
(DL-)programs (Eiter, Ianni, Lukasiewicz, Schindlauer, & Tompits, 2008) prominent approach
so-called DL-atoms serve query interfaces ontology loose coupling enable bidirectional information flow rules ontology. possibility add
information rules part prior query evaluation allows adaptive combinations. However, loose interaction rules ontology easily lead inconsistency,
lack models answer sets.
Example 1 Consider DL-program = hO, Pi Figure 1 formalizing access policy
ontology = hT , Ai (Bonatti, Faella, & Sauro, 2010), whose taxonomy (TBox) given
(1)-(3), (4)-(9) sample data part (ABox) A. Besides facts (10), (11) simple
rule (12), rule part P contains defaults (13), (14) expressing staff members granted
access project files unless blacklisted, constraint (15), forbids owners project information lack access it. parts, P O, interact via DL-atoms
DL[Project projfile; StaffRequest](X). latter specifies temporary update via operator , prior querying it; i.e. additional assertions Project(c) considered individual c, projfile(c) true interpretation P, instances X StaffRequest
retrieved O. Inconsistency arises john, chief project p1 owner files,
access them.
Inconsistency well-known problem logic-based data intensive systems, problem treating logically contradicting information studied various fields, e.g. belief
revision (Alchourrn, Grdenfors, & Makinson, 1985; Grdenfors & Rott, 1995), knowledge base
updates (Eiter, Erdem, Fink, & Senko, 2005), diagnosis (Reiter, 1987), ontology based data access (Lembo, Lanzerini, Rosati, Ruzzi, & Savo, 2015), nonmonotonic reasoning (Brewka, 1989;
Sakama & Inoue, 2003), many others; (cf. Bertossi, Hunter, & Schaub, 2005; Nguyen, 2008;
Martinez, Molinaro, Subrahmanian, & Amgoud, 2013; Bertossi, 2011). hybrid formalisms
far inconsistency management concentrated mostly inconsistency tolerance. instance,
464

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

MKNF knowledge bases paraconsistent semantics developed Knorr, Alferes, Hitzler
(2008), Huang, Li, Hitzler (2013) Kaminski, Knorr, Leite (2015). DL-programs inconsistency tolerance issues targeted Fink (2012), paraconsistent semantics based
Logic introduced. Furthermore, Phrer, Heymans, Eiter (2010)
considered suppressing certain problematic DL-atoms. approaches aimed reasoning
inconsistent system rather making required changes system arrive consistent
state. contrast repair techniques recently developed Eiter, Fink,
Stepanova (2013, 2014d).
theoretical framework repairing inconsistent DL-programs proposed Eiter et al.
(2013), ontology ABox (a likely source errors) changed modified DL-program
answer sets, called repair answer sets. Different repair options including deletion ABox formulas various restricted forms addition considered together naive algorithm
computing repair answer sets lacked practicality.
effective repair algorithm DL-atoms decided without dynamic ontology
access presented Eiter, Fink, Stepanova (2015). based support sets (Eiter, Fink,
Redl, & Stepanova, 2014b) DL-atoms, portions input together ABox
determine truth value DL-atom. algorithm exploits complete support families, i.e.
stocks support sets value DL-atom every interpretation determined, (repeated) ontology access avoided. approach works well
DL-Lite , prominent tractable DL, since complete support families small easy
compute.
However, unfortunately, DLs approach readily usable, general
large infinite support families. applies even EL, another wellknown important DL offers tractable reasoning widely applied many domains, including biology, (cf. e.g., Schulz, Cornet, & Spackman, 2011; Aranguren, Bechhofer, Lord, Sattler, &
Stevens, 2007), medicine (Steve, Gangemi, & Mori, 1995), chemistry, policy management, etc. Due
features EL include range restrictions concept conjunctions left-hand side
inclusion axioms, DL-atom accessing EL ontology arbitrarily large infinitely
many support sets general. latter excluded acyclic TBoxes, often occurring
practice (Gardiner, Tsarkov, & Horrocks, 2006), complete support families still large,
constructing well managing might impractical. obstructs deployment
approach proposed Eiter et al. (2014d) EL ontologies. paper tackle issue
develop repair computation techniques DL-programs ontologies EL. focus EL,
since apart simple widely used, DL well-researched, available effective
algorithms query rewriting important reasoning readily used.
specifically, introduce general algorithm repair answer set computation operates partial (incomplete) support families along techniques families
effectively computed. problem computing repair answer sets DL-programs
EL ontologies P2 -complete (in formulation decision problem; refer work
Stepanova (2015) details complexity).
contributions advances previous works Eiter et al. (2014b, 2014d, 2015)
summarized follows:
effective computation repair answer sets exploit support sets Eiter et al.
(2014d). contrast approaches Eiter et al. (2014d, 2015), however, TBox
classification invoked, use datalog rewritings queries computing support sets
465

fiE ITER , F INK & TEPANOVA

(see also Hansen, Lutz, Seylan, & Wolter, 2014). introduce notion partial support
families, ontology reasoning access completely eliminated.
general constructing complete support families always feasible EL ontologies, provide novel methods computing partial support families exploiting ontology
approximation techniques based logical difference EL-terminologies considered Konev, Ludwig, Walther, Wolter (2012) Ludwig Walther (2014).
capture restricted classes TBoxes, complete support families still
effectively computed, consider support hypergraph DL-atoms, inspired
ontology hypergraphs (Nortje, Britz, & Meyer, 2013; Ecke, Ludwig, & Walther, 2013).
support hypergraph serves characterize TBox parts relevant deriving
query. analysis support hypergraphs allows us estimate maximal size
number support sets needed form complete support family.
generalize algorithm repair answer set computation proposed Eiter et al. (2014d)
EL ontologies handled. novel algorithm operates partial support
families, principle applied ontologies DLs beyond EL. uses
hitting sets disable known support sets negative DL-atoms performs evaluation
postchecks needed compensate incompleteness support families. Moreover, trades
answer completeness scalability using minimal hitting sets; however completeness may
ensured simple extension.
provide system prototype declarative realization novel algorithm repair
answer set computation. repair approach evaluated using novel benchmarks; results show promising potential proposed approach.
Organization. rest paper organized follows. Section 2, recall basic notions
preliminary results. Section 3 deals support sets computation, Section 4
discusses partial support family construction based TBox approximation techniques. Section 5
analyze properties support hypergraph estimating maximal size number support sets complete support family DL-atom. Section 6, algorithm repair answer
set computation declarative implementation presented. Experiments presented Section 7, followed discussion related work Section 8 concluding remarks Section 9.

2. Preliminaries
section, recall basic notions Description Logics, focus EL (Baader,
Brandt, & Lutz, 2005), DL-programs (Eiter et al., 2008). background Description
Logics, (see Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003).
2.1 Description Logic Knowledge Bases
consider Description Logic (DL) knowledge bases (KBs) signature = hI, C, Ri
set individuals (constants), set C concept names (unary predicates), set R role
names (binary predicates) usual. DL knowledge base (or ontology) pair = hT , Ai
TBox ABox A, finite sets formulas capturing taxonomic resp. factual
466

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

Tnorm


(1) StaffRequest hasAction.Action




(2)
StaffRequest hasSubject.Staff




(3)
StaffRequest
hasTarget.Project



(4) hasAction.Action ChasA.A
=
(5) hasSubject.Staff ChasS .St




(6) hasTarget.Project ChasT .P




(7) ChasA.A ChasS .St ChasA.AhasS .St



(8) ChasA.AhasS .St ChasT .P StaffRequest

Figure 2: Normalized TBox

























knowledge, whose form depends underlying DL. abuse notation, also write =
viewing set formulas.
Syntax. EL, concepts C, denoting sets objects, roles R, denoting binary relations
objects, obey following syntax, C atomic concept R R
atomic role:
C | | C C | R.C
EL, TBox axioms form C1 C2 (also called generalized concept inclusion axioms,
GCIs), C1 , C2 EL-concepts. ABox formulas form A(c) R(c, d),
C, R R, c, I. sequel, use P generic predicate C R (if
distinction immaterial).
example EL ontology given Figure 1.
Definition 2 (normalized TBox) TBox normalized, axioms one following forms:
A1 A2
A1 A2 A3
R.A1 A2
A1 R.A2 ,
A1 , A2 , A3 atomic concepts.
E.g., axiom (1) Example 1 normal form, axioms (2) (3) not.
EL TBox, equivalent TBox normal form constructible linear time (Stuckenschmidt,
Parent, & Spaccapietra, 2009) (over extended signature)1 (Baader et al., 2005).
special class TBoxes widely studied literature EL-terminologies, defined follows:
Definition 3 (EL-terminology) EL-terminology EL TBox , satisfying following conditions:
(1) consists axioms forms C C, atomic C
arbitrary EL concept;
(2) concept name occurs left hand side axioms .
example, TBox ontology Figure 1 EL-terminology.
Semantics. semantics DL ontologies based first-order interpretations (Baader et al.,
2005). interpretation pair = hI , non-empty domain interpretation
1. Linear complexity results obtained standard assumption DLs atomic concepts
constant size, i.e., length binary string representing atomic concept depend particular
knowledge base.

467

fiE ITER , F INK & TEPANOVA

function assigns individual c object cI , concept name C subset
C , role name R binary relation RI . interpretation extends
inductively non-atomic concepts C roles R according concept resp. role constructors;
EL, (R.C)I = {o1 | ho1 , o2 RI , o2 C } (C D)I = {o1 | o1 C , o1 DI }.
Satisfaction axiom resp. assertion w.r.t. interpretation I, i.e. |= , follows:
(i) |= C D, C DI ; (ii) |= C(a), aI C ; (iii) |= R(a, b), (aI , bI ) RI .
Furthermore, satisfies set formulas , denoted |= , |= .
TBox (respectively ABox A, ontology O) satisfiable (or consistent),
interpretation satisfies it. call ABox consistent TBox , consistent.
Since negation neither available expressible EL, EL ontologies consistent.
Example 4 ontology Figure 1 consistent; satisfying interpretation = hI ,
exists, = {john, read , p1 , r1 }, Action = {read }, Blacklisted = Staff = {john},
hasSubject = {r1 , john}, StaffRequest = BlacklistedStaffRequest = {r1 }, hasAction =
{r1 , read }, hasTarget = {r1 , p1 }.
Throughout paper, consider ontologies EL unique name assumption (UNA),
i.e., o1 6= o2 whenever o1 6= o2 holds interpretation. However, results carry
ontologies without UNA, hard see UNA EL effect query
answering, (cf. Lutz, Toman, & Wolter, 2009).
2.2 DL-Programs
DL-program = hO, Pi pair DL ontology set P DL-rules, extend
rules non-monotonic logic programs special DL-atoms. formed signature
= hC, P, I, C, Ri, P = hC, Pi signature rule part P set C constant
symbols (finite) set P predicate symbols (called lp predicates) non-negative arities,
= hI, C, Ri DL signature. set P disjoint C, R. simplicity, assume
C = I.
Syntax. (disjunctive) DL-program = hO, Pi consists DL ontology finite set P
DL-rules r form
a1 . . . b1 , . . . , bk , bk+1 , . . . , bm

(1)

negation failure (NAF)2 ai , 0 n, first-order atom p(~t)
predicate p P (called ordinary lp-atom) bi , 1 m, either lp-atom DLatom. rule constraint, n = 0, normal, n 1. call H(r) = {a1 , . . . , }
head r, B(r) = {b1 , . . . , bk , bk+1 , . . . , bm } body r. B + (r) = {b1 , . . . , bk }
B (r) = {bk+1 , . . . , bm } denote positive negative parts B(r) respectively.
DL-atom d(~t) form
DL[; Q](~t),
(2)

(a) = S1 op 1 p1 , . . . , Sm op pm , 0 input list i, 1 m, Si
C R, op {} update operator, pi P input predicate arity
Si ; intuitively, op = increases Si extension pi ;
2. Strong negation added resp. emulated usual (Eiter et al., 2008).

468

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

(b) Q(~t) DL-query, one forms (i) C(t), C concept
term; (ii) R(t1 , t2 ), R role t1 , t2 terms; (iii) C1 C2 ~t = .
Note inclusion DL-queries form C1 C2 easily reduced instance queries.3
Thus simplicity, consider work instance DL-queries.
Example 5 Consider DL-atom DL[Project projfile; StaffRequest](X ) rule (13)
Figure 1 X = r1 . DL-query StaffRequest(r1 ); list = Project projfile
contains input predicate projfile extends ontology predicate Project via update
operator .
Semantics. semantics DL-program
= hO, Pi given terms grounding
gr() = hO, gr(P)i C, i.e., gr(P) = rP gr(r) contains possible ground instances
rules r P C. remainder, default assume ground.
(Herbrand) interpretation set HB ground atoms, HB Herbrand base P =hC, Pi, i.e. set ground atoms P ; satisfies lp- DL-atom a,

(i) I, lp-atom,
(ii) (a) |= Q(~t) = hT , Ai, DL-atom form (2),
(d) =


[

Ai (I) Ai (I) = {Si (~t) | pi (~t) I}, 1 m.

(3)

i=1

Satisfaction DL-rule r (resp. set P rules) Herbrand interpretation = hP, Oi
usual, satisfies bj , satisfy bj ; satisfies , satisfies
r P. |=O denote satisfies (is model of) object , (DL)atom, rule set rules; superscript |= specifies ontology DL-atoms
evaluated. model minimal, model exists I.
Example 6 DL-atom = DL[Project projfile; StaffRequest](r1 ) satisfied interpretation = {projfile(p1 ), hasowner (p1 , john)}, since |= StaffRequest(r1 ). =

O\{StaffReqeust(r1 )} still holds |=O d, (d) |= StaffRequest(r1 ).
Repair Answer Sets. Various semantics DL-programs extend answer set semantics logic
programs (Gelfond & Lifschitz, 1991) DL-programs, (e.g., Eiter et al., 2008; Lukasiewicz, 2010;
Wang, You, Yuan, & Shen, 2010; Shen, 2011). concentrate weak answer sets (Eiter
et al., 2008), treat DL-atoms like atoms NAF, flp-answer sets (Eiter, Ianni, Schindlauer, & Tompits, 2005), obey stronger foundedness condition. like answer sets
ordinary logic program interpretations minimal models program reduct,
intuitively captures assumption-based application rules reconstruct interpretation.
I,O
P relative HB results gr(P) deleting
weak -reduct Pweak
(i) rules r either 6|=O DL-atom B + (r), |=O l l B (r);
(ii) DL-atoms B + (r) literals B (r).
3. Evaluating = DL[; C1 C2 ]() = reduces evaluating = DL[; AC2 ](a) =
{AC1 C1 , C2 AC2 } {AC1 (a)}, fresh constant AC1 , AC2 fresh concepts (similar
TBox normalization).

469

fiE ITER , F INK & TEPANOVA

I,O
flp-reduct Pflp
P results gr(P) deleting rules r, whose bodies

satisfied I, i.e. 6|= bi , bi , 1 k |=O bj , bj , k < j m.
illustrate notions example.

Example 7 Let Figure 1, let rule set P contain facts (10), (11) rules
(12), (13) X, Y, Z instantiated r1 , john, p1 respectively. Consider interpretation =
I,O
{projfile(p1 ), hasowner (p1 , john), chief (john), grant(r1 )}. flp-reduct Pflp
contains
I,O
rules P, weak -reduct Pweak
rule (13) replaced fact grant(r1 ).

Definition 8 (x-deletion repair answer set) interpretation x-deletion repair answer set

= hT A, Pi x {flp, weak }, minimal model PxI,T , A;
called x-deletion repair . = A, standard x-answer set.
Example 9 = {projfile(p1 ), chief (john), hasowner (p1 , john), grant(john)} weak
flp-repair answer set Example 1 repair = A\{Blacklisted (john)}.
Notation. denote normal logic program P (P) set answer sets P,
DL-program x () (resp. RAS x ()) set x-answer sets (resp. x-repair
answer sets) .
general flp-answer set weak -answer set, vice versa, i.e. flp-answer sets
restrictive notion; however, many cases weak flp answer sets coincide.
information reducts, see works Eiter et al. (2008) Wang et al. (2010).
Shifting Lemma. simplify matters avoid dealing logic program predicates separately, shall shift Eiter et al. (2014d) lp-input DL-atoms ontology. Given
DL-atom = DL[; Q](~t) P p , call Pp (c) input assertion d, Pp
fresh ontology predicate c C; Ad set assertions. TBox
DL-atom d, let Td = {Pp P | P p }, interpretation I, let
OdI = Td {Pp (~t) Ad | p(~t) I}. have:
Proposition 10 (Eiter et al., 2014d) every = A, DL-atom = DL[; Q](~t) interI
pretation I, holds |=O iff |=Od DL[; Q](~t) iff OdI |= Q(~t).
Unlike OI (d), OdI clear distinction native assertions input assertions
w.r.t. (via facts Pp axioms Pp P ), mirroring lp-input. Note normal
form, also Td normal form.

3. Support Sets DL-Atoms
section, recall support sets DL-atoms Eiter et al. (2014b), effective
optimization means (repair) answer set computation (Eiter et al., 2014d). Intuitively, support
set DL-atom = DL[; Q](~t) portion input that, together ABox assertions,
sufficient conclude query Q(~t) evaluates true; i.e., given subset
interpretation set ABox assertions ontology O, conclude
|=O Q(~t). Basically, method suggests precomputing support sets DL-atom
nonground level. DL-program evaluation, candidate interpretation ground instances
support sets computed, help prune search space (repair) answer sets.
470

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

Exploiting Proposition 10 following definition support sets using ontology
predicates.
Definition 11 (ground support sets) Given ground DL-atom = DL[; Q](~t), set AAd
support set w.r.t. ontology = hT , Ai, Td |= Q(~t). Supp (d) denote
set support sets w.r.t. O.
Support sets grouped families support sets simply support families. formally,
Definition 12 (support family) collection Supp (d) support sets DL-atom
w.r.t. ontology support family w.r.t. O.
general EL particular, even -minimal support sets arbitrarily large
infinitely many support sets may exist (not acyclic TBoxes , still exponentially many
support sets possible). However, nonetheless exploit repair answer set
computation algorithms Section 6.
Support sets linked interpretations following notion.
Definition 13 (coherence) support set DL-atom coherent interpretation I,
Pp (~c) holds p(c) I.
Example 14 DL-atom = DL[Project Projfile; StaffRequest](r1 ) Figure 1 two
support sets: S1 = {StaffRequest(r1 )} S2 = {hasSubject(r1 , john),Projectprojfile (p1 ),
Staff (john),hasAction(r1 , read ), Action(read )}. S1 coherent interpretation,
S2 coherent interpretations {projfile(p1 )}.
evaluation w.r.t. reduces search coherent support sets.
Proposition 15 Let = DL[; Q](~t) ground DL-atom, let = hT , Ai ontology,
let interpretation. Then, |=O iff Supp (d) exists s.t. coherent I.
Using sufficient portion support sets, completely eliminate ontology access
evaluation DL-atoms. naive approach, one precomputes support sets ground DLatoms respect relevant ABoxes, uses repair answer set computation.
scale practice, since support sets may computed incoherent
candidate repair answer sets.
alternative fully interleave support set computation search repair answer
sets. construct coherent ground support sets DL-atom interpretation
fly. input DL-atom may change different interpretations, support sets must
recomputed, however, since reuse may possible; effective optimizations immediate.
better solution precompute support sets nonground level, is, schematic support
sets, prior repair computation. Furthermore, may leave concrete ABox open;
support sets DL-atom instance easily obtained syntactic matching.
~ = DL[; Q](X)
~
Definition 16 (nonground support sets) Let TBox, let d(X)
~
nonground DL-atom. Suppose V X set distinct variables C set constants.
nonground support set w.r.t. set = {P1 (Y~1 ), . . . , Pk (Y~k )} atoms
471

fiE ITER , F INK & TEPANOVA

(i) Y~1 , . . . , Y~k V
~k )} support set
(ii) substitution : V C, instance = {P1 (Y~1 ), . . . , Pk (Y
~
d(X) w.r.t. OC = AC , AC set possible ABox assertions C.
ontology = AC , denote SuppO (d) set nonground support sets
w.r.t. .
AC takes care possible ABox, considering largest ABox (since
implies Supp (d) Supp (d)).
Example 17 = DL[Project projfile; StaffRequest](X ) set S1 = {StaffRequest(X )}
nonground support set, likewise set S2 = {Action(W ), Staff (Y ), hasSubject(X , ),
hasTarget(X , Z ), Projectprojfile (Z ), hasAction(X , W )}.
sufficiently large portion nonground support sets precomputed, ontology access
fully avoided. call portion complete support family.
Definition 18 (complete support family) family SuppO (d) nonground support sets
~ w.r.t. ontology complete, every support set
(non-ground) DL-atom d(X)
~
~
~ exist
Supp (d(X)), : X C, extension : V C V X


= .
Example 19 Consider DL-atom d(X) = DL[Project projfile; StaffRequest](X) Figure 1. family = {S1 , S2 , S3 , S4 , S5 , S6 } complete w.r.t. O, hT = hasTarget,
hS = hasSubject hA = hasAction:







S1
S2
S3
S4
S5
S6

= {StaffRequest(X )};
= {Project(Y ), hT (X , ), hS (X , Z ), Staff (Z ), hA(X , Z ), Action(Z )};
= {Projectprojfile (Y ), hT (X , ), hS (X , Z ), Staff (Z ), hA(X , Z ), Action(Z )};
= {Project(Y ), hT (X , ), hS (X , Z ), Blacklisted (Z ), hA(X , Z ), Action(Z )};
= {Projectprojfile (Y ), hT (X , ), hS (X , Z ), Blacklisted (Z ), hA(X , Z ), Action(Z )};
= {BlacklistedStaffRequest(X )}.


say two nonground support sets (resp. support families) ground-identical,
groundings coincide. E.g., support sets S1 = {P (X), r(X, )} S2 = {P (X), r(X, Z)}
ground-identical DL-atom d(X) = DL[; Q](X), respective support families
{S1 } {S2 }.
Definition 20 (subsumption) nonground support set subsumed , denoted S,
every ground instance ground instance exists S.
nonground support families, say S1 subsumed S2 , denoted S2 S1 ,
instance S1 instance S2 exists holds.
Example 21 = {BlacklistedStaffRequest(X ),hasSubject(X , ),Blacklisted (Y )} support
set DL-atom d(X) = DL[Staff chief ; BlacklistedStaffRequest](X) w.r.t. Figure 1, subsumed = {BlacklistedStaffRequest(X )}, i.e. S. Moreover,
S, = {S } ={S}, support families = {S, } = {S,
{BlacklistedStaffRequest(X ),hasSubject(X , Z ),Blacklisted (Z )}} mutually subsume other.
472

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

Axiom
Datalog rule
A1 A2
A2 (X) A1 (X)
A1 A2 A3 A3 (X) A1 (X), A2 (X)
R.A2 A1 A1 (X) R(X, ), A2 (Y )
A1 R.A2
R(X, oA2 ) A1 (X)
A2 (oA2 ) A1 (X)

Table 1: EL TBox Rewriting
Definition 22 (maximal support set size, maxsup) maximal support set size DL-atom
w.r.t. , denoted maxsup(d ), smallest integer n 0 every complete nonground support family w.r.t. support set |S| > n, support set
exists w.r.t. Suppd (O) |S | n.
instance, DL-atom TBox Example 19, maximal support set
size 6, i.e., maxsup(d) = 6.
3.1 Computing Support Sets
section, provide methods constructing nonground support sets. natural approach
computation nonground support sets exploit (conjunctive) query answering methods
EL (e.g., Rosati, 2007; Lutz et al., 2009; Kontchakov, Lutz, Toman, Wolter, & Zakharyaschev,
2010; Stefanoni, Motik, & Horrocks, 2012).
Suppose given DL-program = hO, Pi, = hT , Ai EL ontology,
~ = DL[; Q](X).
~ method construct nonground support sets d(X)
~
DL-atom d(X)
following three steps.
Step 1. DL-query Rewriting TBox. first step exploits rewriting DL~ TBox Td = {Pp P | P p } set datalog rules.
query Q d(X)
preprocessing stage, TBox Td normalized. technique restricts syntactic form
TBoxes decomposing complex simpler axioms. purpose, set fresh concept
symbols introduced. normalized form Td norm Td computed, rewrite part
TBox relevant query Q datalog program Prog Q,Tdnorm using translation
given Table 1, variant translation Prez-Urbina, Motik, Horrocks (2010)
Zhao, Pan, Ren (2009). rewriting axioms form A1 R.A2 (fourth axiom
Table 1), introduce fresh constants (oA2 ) represent unknown objects. similar rewriting
exploited R EQUIEM system (Prez-Urbina et al., 2010), function symbols used
instead fresh constants. result obtain:
Lemma 23 every data part, i.e., ABox A, every ground assertion Q(~c), deciding whether
Prog Q,Tdnorm |= Q(~c) equivalent checking Td norm |= Q(~c).
Step 2. Query Unfolding. second step proceeds standard unfolding rules
Prog Q,Td norm w.r.t. target DL-query Q. start rule Q head expand
body using rules program Prog Q,Tdnorm . applying procedure exhaustively,
get number rules correspond rewritings query Q Td norm . Note
always possible obtain rewritings effectively, since general might
473

fiE ITER , F INK & TEPANOVA

Prog Q,Td norm


(4 ) ChasA.A (X ) hasAction(X , ), Action(Y ).




(5 ) ChasS .St (X ) hasSubject(X , ), Staff (Y ).



(6 ) ChasT .P (X ) hasTarget(X , ), Project(Y ).
=

(7
) ChasA.AhasS .St (X ) ChasA.A (X ), ChasS .St (X ).





(8
)
StaffRequest(X ) ChasA.AhasS .St (X ), ChasT .P (X ).



(9) Project(X ) Projectprojfile (X ).

















Figure 3: DL-query Rewriting DL[Project projfile; StaffRequest](X) Td norm
infinitely many cyclic, still exponentially many acyclic ; discuss
techniques computing partial support families next section.
Step 3. Support Set Extraction. last step extracts nonground support sets rewritings
Step 2. select containing predicates Td obtain rules r form
~ P1 (Y~1 ), . . . , Pk (Y~k ), Pk+1
~k+1 ), . . . , Pnp (Y~n ),
Q(X)
(Y
(4)
pk+1

n

Pi native ontology predicate 1 k, predicate mirroring lp-input
otherwise. bodies rules correspond support sets given DL-atom, i.e.
~k+1 ), . . . , Pnp (Y
~n )}
= {P1 (Y~1 ), . . . , Pk (Y~k ), Pk+1
(Y
(5)
pk+1

n

following holds.
~ = DL[; Q](X)
~ DL-atom program = hO, Pi EL
Proposition 24 Let d(X)
~
ontology = hT , Ai. Every set constructed Steps 1-3 nonground support set d(X).
Shifting Lemma, working support sets focus ontology predicates
operate them. specifically, rules form (4) k n fully reflect nonground
support sets Definition 16, ground instantiations rule constants C
implicitly correspond ground support sets.
illustrate computation nonground support sets DL-atoms EL ontologies.
Example 25 Consider DL-atom DL[Project projfile; StaffRequest](X) accessing EL ontology = hT , Ai Figure 1. datalog rewriting computed Step 1 given
Figure 3. Step 2 obtain following query unfoldings StaffRequest:
(1) StaffRequest(X) StaffRequest(X);
(2) StaffRequest(X) hasAction(X, ), Action(Y ), hasSubject(X, ),
Staff (Y ), hasTarget(X, ), Projectprojfile (Y );
(3) StaffRequest(X) hasAction(X, ), Action(Y ), hasSubject(X, ),
Staff (Y ), hasTarget(X, ), Project(Y );
(4) StaffRequest(X) hasAction(X, ), Action(Y ), hasSubject(X, ),
Blacklisted (Y ), hasTarget(X, ), Project(Y );
(5) StaffRequest(X) hasAction(X, ), Action(Y ), hasSubject(X, ),
Blacklisted (Y ), hasTarget(X, ), Projectprojfile (Y ).
Step 3 thus get rule (2) S2 = {hasAction(X, ), Action(Y ), Staff (Y ),
hasSubject(X, ), hasTarget(X, ), Projectprojfile (Y )} rule (3) S3 ={Action(Y ),
hasAction(X, ),Staff (Y ),hasSubject(X, ), Project(Y ),hasTarget(X, )}. (1), (4)
(5) remaining support sets similarly obtained.

474

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

3.2 Partial Support Families
Finding support sets DL-atom tightly related computing solutions logic-based
abduction problem. Abduction important mode reasoning widely applied different areas
AI including planning, diagnosis, natural language understanding many others (Console,
Sapino, & Dupr, 1995). Various variants problem actively studied, e.g. Eiter,
Gottlob, Leone (1997) Bienvenu (2008). Unfortunately, practically important
problems context abduction intractable even restricted propositional theories (Eiter
& Makino, 2007). abduction problem EL TBoxes considered Bienvenu (2008),
represented tuple hT , H, Oi, TBox , set atomic concepts H atomic concept
O. explanation set {A1 , . . . , } H, |= A1 . . . O.
ABox Ad contains atomic concepts, computing nonground support sets =
DL[; Q](X) accessing = hT , Ai corresponds abduction problem hTd , sig(A Ad ), Qi.
roles occur Ad , one introduce new fresh concepts construct complex
concepts hypothesis, e.g., R.A inclusion CR.A R.A added Td , CR.A
H, CR.A fresh concept.
Unlike DL-Lite , support families DL-atoms EL ontologies particular
structure; large, maximal support set size exponential size .
Example 26 Consider following acyclic TBox , contains axioms:
(1) r.B0 s.B0 B1
(2) r.B1 s.B1 B2
...
(n) r.Bn1 s.Bn1 Bn
d1 = DL[; B1 ](X1 ), maximal support set size 4, witnessed
S1 = {r(X1 , X2 ), B0 (X2 ), s(X1 , X3 ), B0 (X3 )}.

DL-atom d2 = DL[; B2 ](X1 ), maxsup(d2 ) = 10, due S2 = {r(X1 , X2 ),
r(X2 , X3 ), B0 (X3 ), s(X2 , X4 ), B0 (X4 ), s(X1 , X5 ), r(X5 , X6 ), B0 (X6 ), s(X5 , X7 ), B0 (X7 )}.
Moreover, di = DL[; Bi ](X), maxsup(di ) = maxsup(di1 ) 2 + 2, 1 n.
Note maximal support set dn involves n + 3 predicates. Therefore, TBox
form, |sig(T )|= k, lower bound worst case support set size
2k1 + 2 = (2k ), single exponential size .

general many unfoldings produced Step 2, according recent results
Hansen et al. (2014), complete support families EL computed large classes ontologies. Therefore, still exploit support families, unlike Eiter et al. (2014d) require
complete, develop techniques computing partial (i.e. incomplete) support families DL-atoms. natural approach context aim finding support sets bounded
size. general, due cyclic dependencies r.C C, possible EL
DL-Lite , support sets arbitrary large. analysis vast number ontologies
revealed many realistic cases ontologies contain (nor imply) cyclic axioms (Gardiner
et al., 2006); thus assume practical considerations TBox ontology given
DL-program acyclic, i.e., entail inclusion axioms form r.C C. However, even
restriction support sets large Example 26 shows.
475

fiE ITER , F INK & TEPANOVA

computing complete support families computationally expensive, natural approach
produce support sets certain size k using e.g. limited program unfolding.
unfolding branch reaches depth k, stop expand different branch. Similarly, compute limited number k support sets stopping rule unfolding program Prog Q,Tdnorm
k-th support set produced. alternative approach, based TBox approximation
techniques, pursued next section.

4. Partial Support Family Construction via TBox Approximation
provide practical methods construct partial support families using TBox approximation.
4.1 TBox Approximation
approximation DL ontologies source language L different target language L
well-known important technique ontology management. Existing approaches
approximation roughly divided syntactic approaches semantic approaches. former,
e.g. Tserendorj, Rudolph, Krtzsch, Hitzler (2008) Wache, Groot, Stuckenschmidt (2005), focus syntactic form axioms original ontology appropriately
rewrite axioms comply syntax target language. rather effective general produce unsound answers (Pan & Thomas, 2007). Semantic approaches
focus model-based entailment original ontology, rather syntactic structure.
aim preserving entailments much possible transforming ontology
target language; general sound, might computationally expensive
(Console, Mora, Rosati, Santarelli, & Savo, 2014).
task computing partial support families, sound ontology approximation techniques
relevant. choose DL-Lite core target approximation language, lies intersection EL DL-Lite , complete support families effectively identified (Eiter
et al., 2014d). approach approximating TBox EL DL-Lite core exploits logical
difference EL TBoxes considered Konev et al. (2012). idea behind decide
whether two ontologies give answers queries given vocabulary (called signature)
, compute succinct representation difference empty. Typical queries include
subsumption concepts, instance queries conjunctive queries. setting subsumption queries particular interest, based nonground support families constructed.
~ ontology = hT , Ai,
approach follows. Given DL-atom = DL[; Q](X)
eliminate TBox Td axioms outside DL-Lite core language, obtain simplified
TBox Td . compute succinct representation logical difference Td Td
w.r.t. = {sig(Ad A) Q}; axioms logical difference fall DL-Lite core
added Td . restricting predicates potentially appear support sets avoid
redundant computations approximate relevant part TBox. approach
particularly attractive, logical difference EL intensively studied, e.g. Lutz,
Walther, Wolter (2007) Konev et al. (2012), polynomial algorithms available
EL-terminologies; thus confine latter.
present approximation approach formally, first recall notions introduced
Konev et al. (2012).
476

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

Definition 27 (-concept difference) -concept difference EL-terminologies T1
T2 set cDiff (T1 , T2 ) EL-inclusions T1 |= T2 6|= .
Example 28 terminologies T1 = {B E, E r., C B} T2 =
{C A, B, C} holds cDiff (T1 , T2 ) = = {A, B, C},

cDiff (T1 , T2 ) = {B r.} = {B, r}.
two EL-terminologies entail concept subsumptions signature , i.e. holds
cDiff (T1 , T2 ) = cDiff (T2 , T1 ) = , called -concept inseparable,
C
C
denoted T1 C
T2 . E.g. Example 28 T1 T2 T1 6 T2 .
logical difference terms instance queries defined follows.
Definition 29 (-instance difference) -instance difference terminologies T1 T2
set iDiff (T1 , T2 ) pairs form (A, ), -ABox -instance
assertion, T1 |= T2 6|= . say T1 T2 -instance inseparable,
symbols T1 T2 iDiff (T1 , T2 ) = iDiff (T2 , T1 ) = .
easily seen, T1 T2 implies T1 C
T2 . converse obvious also holds.
Theorem 30 (cf. Lutz & Wolter, 2010) EL-terminologies T1 T2 signature , T1 C

T2 iff T1 T2 .
4.2 Partial Support Family Construction
show DL-atom set support sets -concept inseparable terminologies. Prior that, establish following lemma.
Lemma 31 Let = DL[; Q](~t) DL-atom, let = hT1 , Ai EL ontology, let T2
C
TBox. T1 C
T2 , =sig(A) sig(Q) {P | P p }, T1 T2 ,

= sig(Ad ).
Armed this, obtain following result equivalence nonground support families.
~ DL-atom let T1 , T2 EL-terminologies
Proposition 32 Let = DL[; Q](X)
C
T1 T2 = sig(A Ad Q) {P | P p }. S1 S2 complete nonground
support families w.r.t. O1 = hT1 , Ai O2 = hT2 , Ai, respectively, S1 S2
ground-identical.
Given two EL-terminologies T1 T2 , inclusions C cDiff (T1 , T2 ) (resp.
C cDiff (T1 , T2 )) following Konev et al. (2012) called left (resp. right) witnesses denoted
lhs
cWTnrhs
(T1 , T2 ) (resp. cWTn (T1 , T2 )). shown every inclusion C concept difference T1 T2 contains either left right witness.
Theorem 33 (cf. Konev et al., 2012) Let T1 T2 EL-terminologies signature.
cDiff (T1 , T2 ), either C member cDiff (T1 , T2 ), sig()
concept name C EL-concepts occurring .
477

fiE ITER , F INK & TEPANOVA

Algorithm 1: PartSupFam: compute partial support family
~ ontology = hT , Ai
Input: DL-atom = DL[; Q](X),
Output: Partial nonground support family SuppO (d)
(a) {sig(A Ad ) Q}
(b) Td {Pp P | P p }
(c) Td Td \{C | C 6 {A, r.} 6 {A, r.}}
rhs
lhs
(d) lrw cWTn (Td , Td ) cWTn (Td , Td )


(e) Td Td {C lrw | C, {A, r.}}
(f) {ComplSupF am(d, Td )}
return
logical difference two EL-terminologies compact representation consists
inclusions atomic concept name either left right hand side. may
inclusions atomic concepts sides role restrictions form r., fall
target language DL-Lite core DL, therefore reintroduced.
ready describe algorithm P artSupF (see Algorithm 1) compute partial
~ ontology
families support sets. input given DL-atom = DL[; Q](X)
= hT , Ai, EL-terminology. first set signature (a) predicates
relevant support set computation d. construct TBox Td (b) simplified
version Td (c) removing Td axioms form C D, C complex
concept, i.e. axioms DL-Lite core fragment. (d) compute right-hand side
left-hand side witnesses Td Td store lrw . that, (e)
construct TBox Td extending Td axioms lrw , concepts form
r sides inclusions. Based support set construction method DL-Lite Eiter
et al. (2014d), obtain complete support family Td (f), partial support
family .
Proposition 34 family computed Algorithm 1 fulfills SuppO (d), i.e., partial
support family given DL-atom w.r.t. = A.
lwr = (d) cDiff (Td , Td ) = (e), guaranteed complete Proposition 32. general Algorithm 1 used computing support families DL-atoms
accessing arbitrary TBoxes4 , practically efficient procedures (d) available acyclic
EL-terminologies (Konev et al., 2012).

5. Bounded Support Sets
section, analyze size number support sets given DL-atom have.
bounds quantities hand, one limit search space support sets.
precisely, aim support set families sufficient evaluating DL-atom. support
sets (properly) subsumed another support set (i.e., ) dropped,
consider non-ground support families subsume (in particular, complete) support
family. formally,
4. computing logical difference arbitrary TBoxes recent results Feng, Ludwig, Walther (2015) might
exploited.

478

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

Definition 35 (-complete support family) say nonground support family DL-atom
-complete w.r.t. ontology O, SuppO (d).
Thus question bounds size support sets cardinality smallest S.
Throughout section, tacitly assume TBoxes acyclic, i.e. entail inclusions form R.C C.
5.1 Estimation Support Set Size Bounds
first consider estimate maximal size support sets smallest -complete support
family analyzing syntactic properties given TBox. start with, recall work
Konev et al. (2012) atomic concept primitive terminology , occurs
axiom left-hand side, pseudo-primitive, either primitive occurs
left-hand side axioms C, C arbitrary EL concept.
EL-terminology every pseudo-primitive |= A, =
A1 . . . r1 .C1 . . . rm .Cm , (atomic) conjunct Ai exists |= Ai
(Konev et al., 2012, Lemma 15). obtain:
Proposition 36 Let = DL[; Q](~t) DL-atom, let EL-terminology. Q
pseudo-primitive , maxsup(d) = 1.
Proposition 36 exploits specific case, support set size bound 1. providing
liberal syntactic conditions ensure bounded size support sets, use ontology hypergraphs (Nortje et al., 2013; Ecke et al., 2013). latter widely studied extracting
modules ontologies (Nortje et al., 2013), determining concept difference EL terminologies (Ecke et al., 2013), efficient reasoning OWL 2 QL (Lembo, Santarelli, & Savo, 2013),
important tasks.
First let us recall notion directed hypergraph, natural generalization
directed graph, proposed Ausiello, DAtri, Sacc (1983) context databases represent functional dependencies.
Definition 37 (directed hypergraph) directed hypergraph pair G = (V, E), V
set nodes graph E set directed hyperedges form e = (H, H ),
H, H V nonempty sets called hypernodes.
Given hyperedge e = (H, H ), call H tail e H head e, denoted
tail (e) head (e), respectively. hypernode singleton, |H| = 1, binary hypernode,
|H| = 2; abuse notation, singleton {v}, also simply write v. notion
ontology hypergraph DL EL introduced Ecke et al. (2013) follows.
Definition 38 (ontology hypergraph) Let EL TBox normal form, let C R.
ontology hypergraph GT directed hypergraph GT = (V, E),
V = {xA | C ( sig(T ))} {xr | r R ( sig(T ))} {x },
E = {({xA }, {xB }) | B }
{({xA }, {xr , xY }) | r.Y , C {}}
{({xr , xY }, {xA }) | r.Y , C {}}
{({xB1 , xB2 }, {xA }) | B1 B2 }.
479

fiE ITER , F INK & TEPANOVA

xr1

xr3

xA3

xA1

x C2

xr2

xA2

xA4

x C1

xD

xr4

Figure 4: Hypergraph GT Example 39
Example 39 Consider following TBox normal form:

(4) C1 C2

(1) r1 .A1 C1
(2) r2 .A2 C2
(5) A3 A2
=

(3) r .A
(6)
r4 .A4
3
3
1

ontology hypergraph GT =sig(T ) depicted Figure 4.







.



define notions directed path two nodes incoming path singleton
node ontology hypergraph; natural generalizations path standard graph.
Definition 40 (directed path, incoming path) Suppose EL TBox normal form,
GT = (V, E) ontology hypergraph, x, V singleton nodes occurring GT .
directed path x GT sequence = e1 , e2 , . . . , en (hyper) edges, that:
(i) tail (e1 ) x;
(ii) head (en ) y;
(iii) every ei , < n, successor s(ei ) = ej ei exists GT j > i, head (ei )
tail (ej ), s(ei ) = s(ei ) implies head (ei ) 6= head (ei ) 6= .
incoming path singleton node x V GT = (V, E) directed path = e1 , . . . , en
node V x, head (en ) = x. set incoming paths node x
hypergraph G denoted Paths(x , G).
Intuitively, hyperedges ontology hypergraph GT model inclusion relations (complex)
concepts . Consequently, incoming path singleton node xC GT models chain
inclusions logically follow , C rightmost element chain.
Example 41 Let us look ontology hypergraph GT Figure 4. sequence edges
1 = ({xr3 , xA3 }, xA1 ), ({xr1 , xA1 }, xC1 )
480

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

x 3p

xr1

3

xr3

xA3

xA1

x C2

xr2

xA2

xD

xC

xA

x C1

xB

xD

xQ


(a) Gsupp(d),T
Example 43


(b) Gsupp(d),T
Example 45

Figure 5: Examples support hypergraphs
incoming path xC1 GT reflects inclusions r1 .A1 C1 r1 .(r3 .A3 ) C1 ;
sequence
2 = ({xr3 , xA3 }, xA1 ), ({xr1 , xA1 }, xC1 ), ({xr2 , xA2 }, xC2 ), ({xC1 , xC2 }, xD )
incoming path singleton xD , following set inclusions extracted:
(1) C1 C2 D, (2) r2 .A2 C1 D, (3) r2 .A2 r1 .A1 D, (4) r2 .A2 r1 .(r3 .A3 ) D.

introduce notion support hypergraph DL-atom.
Definition 42 (support hypergraph) support hypergraph DL-atom d=DL[; Q](~t)

constructed follows:
normal ontology = hT , Ai hypergraph Gsupp(d),T
1. build ontology hypergraph GTd = (V, E), = sig(A Ad ) {Q};
2. leave nodes edges Paths(xQ , GTd ) remove nodes edges;
3. xC GTd C 6 , Paths(xC , GTd ) (hyper) node N exists {P | xP
N } leave xC , otherwise remove corresponding edges;
4. xr GTd , r 6 , leave e = ({xr , y}, xC ) (xC , {xr , y}) exists GTd ,
{xD , }, otherwise remove e.
Let us illustrate notion support hypergraph following example:
~
Example 43 Let Example 39 accessed DL-atom = DL[A3 p3 ; D](X),

Td = {A3p3 A3 }. support hypergraph Gsupp(d),T = sig(Td ) shown Figure 5a. node xD colored blue corresponds DL-query d. edge ({xD }, {xr4 , xA4 })

, lie incoming path xD .

Gsupp(d),T
481

fiE ITER , F INK & TEPANOVA

describing approach extracting support sets DL-atom hypergraph,
introduce notion tree-acyclicity. alternative definitions refer reader works,
e.g. Ausiello, DAtri, Sacc (1986), Gallo, Longo, Pallottino (1993) Thakur
Tripathi (2009).
Definition 44 (tree-acyclicity) hypergraph G = (V, E) called tree-acyclic, (i) one
directed path exists G singleton nodes x, V, (ii) G paths =
e1 , . . . , ek tail (e1 ) head (ek ) 6= .
refer hypergraphs tree-acyclic tree-cyclic.




= {B
Example 45 Gsupp(d),T
Figure 5a tree-acyclic, G = Gsupp(d),T


= {A
A3 , B A2 } = {B} not, neither G = Gsupp(d),T
,
1
C2 }.

hypergraph Gsupp(d),T
= DL[; Q](X), = {D C; C A; C B; B Q}
= sig(T ) given Figure 5b tree-cyclic, since contains two paths xD xQ ,
namely 1 = xD , xC , xA , {xA , xB }, xQ 2 = xD , xC , xB , {xA , xB }, xQ .


support hypergraph Gsupp(d),T
= (V, E) DL-atom = DL[; Q](X) contains
incoming paths xQ start nodes corresponding predicates Ad construction,
i.e. reflects inclusions Q right-hand side predicates Ad left
hand-side entailed Td . Hence, traversing edges incoming paths xQ ,
construct sufficiently many query rewritings Q TBox Td corresponding nonground
support sets allow subsume every nonground support family w.r.t. O.
support hypergraph given DL-atom tree-acyclic, support sets conveniently constructed annotating nodes variables Xi , N way described
hX
below. use subscripts annotations, e.g. xC means node xC annotated
hX ,X

variable Xi , xr j states xr annotated ordered pair variables Xi , Xj .
approach proceeds follows. start node xQ , annotate X0 ,
hX
i.e. xQ 0 ; traverse hypergraph backwards, going head edge tail.
every edge e encounter annotate tail (e) based form annotation
head (e), variable names occur annotation head (e) and/or fresh variable names Xi ,
N, following way:
(1) |tail (e)| = 1,
hX

(1.1) head (e) = {xC1 }, tail (e) annotated hXi i;
hXi1 ,Xi2

(1.2) head (e) = {xr1
hXi

hXi

, xC1 3 }, tail (e) = xC2 annotated hXi1 i, i.e.

obtain xC2 1 ;
hXi

(2) |tail (e)| = 2 head (e) = {xC

},
hX

hX

(2.1) tail (e)={xC1 ,xC2 }, xC1 xC2 annotated Xi , i.e. {xC1 ,xC2 };
hXi ,Xi1

(2.2) tail (e)={xr1 , xC1 }, get {xr1
482

hXi

, xC1 1 },

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

every annotated hypernode N , one create set nonground atoms predicate names
extracted labels hypernodes variable names annotations. nonground
support sets = DL[; Q](X0 ) constructed incoming paths xQ .
pick incoming path 1 xQ containing n edges, start traversing
edge en head (en ) = {xQ }. first immediate support set S1 = {Q(X0 )}; next
one, S2 , extracted annotated tail en taking nonground predicates labels
variables. pick edge ek head (ek ) tail (en ), obtain support
sets substituting nonground atoms correspond head (ek ) tail (en ) S2 atoms
extracted tail (ek ); repeated. One fact construct incoming path backwards
along support set extraction, maximal path obtained.

Example 46 Consider maximal incoming path xD Gsupp(d),T
Figure 5a:

= (xA3 p3 , xA3 ), ({xr3 , xA3 }, xA1 ), ({xr1 , xA1 }, xC1 ), ({xr2 , xA2 }, xC2 ), ({xC1 , xC2 }, xD ).
{z
} |
{z
} |
{z
} |
{z
}
|
{z
} |
e1

e2

e3

e4

e5

hX3
3i
Traversing path backwards, i.e. edges order e5 , e4 , e3 , e2 , e1 , obtain: (xhX
A3 p ,xA3 )e ,

{z
}1
hX
hX
hX
hX
hX hX
hX
hX
hX
0 ,X2
0 ,X1
2 ,X3
,xA1 2 },xC1 0 ) ,({xhX
,xA2 1 },xC2 0 ) ,({xC1 0 ,xC2 0 },xD 0 ).
({xhX
,xA3 3 },{xA1 2 }) ,({xhX
r1
r2
r3
{z
} |
{z
} |
{z
} |
{z
}
|
|

e2

e3

e4

3

e5

nonground support sets extracted resulting annotated path follows:

S0 = {D(X0 )} immediately obtained head (xD );
first incoming path consider 1 = e5 , get S1 = {C1 (X0 ), C2 (X0 )};
next path 2 = e4 , e5 head (e4 ) tail (e5 ), yielding support set S2 = {C1 (X0 ),
r2 (X0 , X1 ), A2 (X0 , X1 )};
then, 3 = e3 , e5 get S3 = {C1 (X0 ), r1 (X0 , X2 ), A1 (X2 )};
4 = e3 , e4 , e5 yields S4 = {r2 (X0 , X1 ), A2 (X1 ), r1 (X0 , X2 ), A1 (X2 )};
5 = e2 , e3 , e5 , extract S5 = {r1 (X0 , X2 ), r3 (X2 , X3 ), A3 (X3 ), C2 (X0 )};
6 = e2 , e3 , e4 , e5 yields S6 = {r1 (X0 , X2 ), r3 (X2 , X3 ), A3 (X3 ), r2 (X0 , X1 ), A2 (X1 )};
7 = e1 , e2 , e3 , e5 , extract S7 = {r1 (X0 , X2 ), r3 (X2 , X3 ), A3p3 (X3 ), C2 (X0 )};
finally, 8 = e1 , e2 , e3 , e4 , e5 get S8 = {r1 (X0 , X2 ), r3 (X2 , X3 ), A3p3 (X3 ),
r2 (X0 , X1 ), A2 (X1 )}.

following lemma formally asserts correctness procedure.

Lemma 47 Let SG support family constructed tree-acyclic hypergraph G=Gsupp(d),T
~ SG -complete w.r.t. O, i.e., SG every SuppO (d).
= DL[; Q](X).

particular, Lemma 47 holds complete w.r.t. ontology = hT , Ai. Thus
determine sufficiently many nonground support sets looking support hypergraph. Note restriction tree-acyclic TBoxes crucial correctness procedure
above, ensures every node hypergraph annotated once.
Lemma 47 allows us reason structure size support sets analyzing
parameters support hypergraph. One parameter, instance, maximal number
n(, G) hyperedges singleton head node excluding ({xr , }, xA ), occurring
incoming path xQ hypergraph G.
483

fiE ITER , F INK & TEPANOVA

xQ

xL

xE
xF

xD

xM

xB

xA

xK
xC


Figure 6: Support hypergraph Gsupp(d),T
Example 49

Proposition 48 Let = hT , Ai EL ontology normal form, let =
~ DL-atom tree-acyclic support hypergraph G
DL[; Q](X)
.
supp(d),T

maxsup(d) maxG

supp(d),T


(n(, Gsupp(d),T
)) + 1.

(6)

tree-cyclic hypergraphs, bound tight, illustrate next.
Example 49 Consider DL-atom d(X) = DL[; Q](X) accessing TBox Td :


(4) E F L


(1) F
(2) C K
(5) E K
.
Td =



(3) B E
(6) L Q

support hypergraph depicted Figure 6, = sig(Td ). six hyperedges singleton head nodes, maximal support set size d(X) 4, e.g. =
{A(X), B(X), D(X), K(X)}.

next define out- in-degrees nodes hypergraph.
Definition 50 (hyper-outdegree -indegree) Given directed hypergraph G = (V, E),
hyper-outdegree denoted hd+ (x) (resp., hyper-indegree hd (x)) singleton node x V
number hyperedges e E tail (e) x (resp., head (e) x) either |tail (e)| = 2
|head (e)| = 2. Similarly, outdegree d+ (x) (resp., indegree (x)) x number
edges e E tail (e) = {x} (resp., head (e) = {x}) |head (e)| = |tail (e)| = 1.


Example 51 nodes X V\{xA3p , xD } hypergraph Gsupp(d),T
Figure 5a hyper+
+
outdegree 1, xAp3 xD hd (xAp3 ) = hd (xD ) = 0, moreover, d+ (xAp3 ) = 1.
hyper-indegrees hd (xA3 ) = hd (xA1 ) = hd (xC1 ) = hd (xC2 ) = 1. graph

({xC2 , xA2 }, xD ), holds hd+ (xC2 ) = hd+ (xA2 ) = hd (xD ) = 2,
G = Gsupp(d),T
moreover, (xA3 ) = 1.


484

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

let us define
smax (x, G) = maxPaths(x,G) (n(, G) m(, G) + 1),
(7)
P
m(, G) = xA (hdc+ (xA ) 1), hdc+ (xA ) number hyperedges form
({xA , xB }, xC ) .


Example 52 Consider Gsupp(d),T
Figure 5a, Paths(xD , Gsupp(d),T
) contains single maximal path xD , viz. = (xA3p3 , xA3 ), ({xr3 , xA3 }, xA1 ), ({xr2 , xA2 }, xC2 ), ({xr1 , xA1 }, xC1 ),
({xC1 , xC2 }, xD ). n(, G) = 4, four hyperedges singleton head node,
m(, G) = 0, nodes hyper-outdegree 1; hence smax (xQ , G) = 4 0 + 1 =
5. hypergraph Figure 6 single maximal incoming path xQ , n(, G) = 6,
m(, G) = (hdc+ (xA ) 1) + (hdc+ (xE ) 1) = 3; thus smax (xQ , G) = 6 3 + 1 = 4.


generalize bound maximal support set size Proposition 48 using
parameter smax (xQ , G) node corresponding DL-query Q DL-atom d, obtain
following result hypergraphs possibly tree-cyclic:
Proposition 53 Let = hT , Ai EL ontology normal form, let =
~ DL-atom support hypergraph G
DL[; Q](X)
supp(d),T , role predi
cates. maxsup(d ) smax (xQ , Gsupp(d),T ).

) = 4,
Example 54 tree-cyclic hypergraph Figure 6 smax (xQ , Gsupp(d),T
4 indeed maximal support set size = DL[; Q](X). hypergraph Figure 5a

3 hyperedges, every node x V, hd+ (x) 1. Thus, smax (xQ , Gsupp(d),T
) = 4,
coincides maxsup(d ), = DL[A3 p3 ; Q](X).


Note Proposition 53, take computing m(, G) outgoing hyperedges
form ({xC , xD }, xE ) account, C, D, E concepts, moreover, roles occur .
Multiple outgoing hyperedges involving roles r r influence support set size.
Example 55 Let support hypergraph = DL[; Q](X) hyperedges ({xr , xC }, xD ),
({xC , xs }, xM ), ({xD , xM }, xQ ) r , reflecting axioms r.C D, s.C
DQ. largest minimal support set S={r(X, ), C(Y ), s(X, Z), C(Z)}; size
n + 1, n number hyperedges singleton head node, hd+ (xC ) = 2.
5.2 Number Support Sets
Orthogonal question considered previous section conditions given
number n support sets sufficient obtain -complete support family. problem tightly
related counting minimal solutions abduction problem, analyzed Hermann
Pichler (2010) propositional theories various restrictions. particular, counting minimal explanations shown # coNP-complete general propositional theories
#P -complete Horn propositional theories; EL subsumes propositional Horn logic, determining size smallest -complete support family least #P -hard thus intractable.
Like size support sets, support hypergraph fruitfully exploited estimating
maximal number support sets given DL-atom. provide estimate, traverse
support hypergraph forward starting leaves label every node xP number
rewritings P . conveniently compute labels, introduce support weight functions.
485

fiE ITER , F INK & TEPANOVA


Definition 56 (support weight function) Let Gsupp(d),T
= (V, E) support hypergraph
DL-atom d. support weight function ws : V N assigns every node xA V number
ws(xA ) rewritings w.r.t. .

every node tree-acyclic support hypergraph, value ws conveniently computed recursive manner.

Proposition 57 Let Gsupp(d),T
tree-acyclic support hypergraph DL-atom (normalized) ontology = hT , Ai. ws given follows, VC V set nodes
concepts:



1, P
Q
ws(x) = 1 + (x) x ws(x )

P
P

+ (x),T 6VC ({x },T )E ws(x ),

hd (x) = 0 (x) = 0 x
/ VC ,
otherwise.
(8)

(x) = {T | (T, {x}) E}.

demonstrate usage Proposition 57 following examples.

Example 58 compute ws(x) nodes Gsupp(d),T
Figure 5a, traverse graph
leaves root, x {xr1 , xA2 , xC2 , xr2 , xA3p3 , xr3 } obtain ws(x) = 1; furthermore,
ws(xA3 ) = ws(xC2 ) = 2, ws(xA1 ) = 3, ws(xC1 ) = 4. Finally, ws(xD ) = 1 + ws(xC1 )
ws(xC2 ) = 1 + 4 2 = 9, number rewritings D(X) (and hence support sets
d(X) = DL[A3 p3 ; D](X)) identified Example 46.

Example 59 Consider TBox = {A B Q; C A; A; E A; F B; G B; H
B; L} DL-atom = DL[; Q](X), whose support hypergraph = sig(T )
Figure 7. ws(xQ ) = 1 + ws(xB ) ws(xA ) = 1 + 4 4 = 17, indeed 17
rewritings Q(X), namely S1 = {A(X), B(X)}, S2 = {C(X), B(X)}, S3 = {D(X), B(X)},
S4 = {E(X), B(X)}, S5 = {A(X), F (X)}, S6 = {A(X), G(X)}, S7 = {A(X), H(X)}, S8 =
{C(X), F (X)}, S9 = {C(X), G(X)}, S10 = {C(X), H(X)}, S11 = {D(X), F (X)}, S12 =
{D(X), G(X)}, S13 = {D(X), H(X)}, S14 = {E(X), F (X)}, S15 = {E(X), G(X)}, S16 =
{E(X), H(X)}, S17 = {Q(X)}.
immediate corollary Proposition 57, obtain

= (V, E) tree-acyclic support hypergraph DL-atom =
Corollary 60 Let Gsupp(d),T
~ EL ontology = hT , Ai. edge e E satisfies |tail (e)|=|head (e)|=1,
DL[; Q](X)

X
ws(tail (e)) + 1.
(9)
ws(v) =
eE | head(e)=v

Thus query node xQ , get ws(xQ ) = |E| + 1. fact, Proposition 57 leads
simple bound size -minimal complete support families general cases.
486

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

xC

xD

xE

xF

xA

xG

xH

xB

xQ

Figure 7: Hypergraph Gsupp(d),T


= (V, E) tree-acyclic support hypergraph DL-atom
Proposition 61 Let Gsupp(d),T
~ EL ontology, every edge e = ({x, y}, z) E edges
= DL[; Q](X)
e1 , e2 E head (ei ) {x, y}, {1, 2}, holds head (e1 ) = head (e2 ).
|SG
| = |E| + 1.
supp(d),T


Example 62 hypergraph Gsupp(d),T
Figure 5a single maximal path length 5,
hyperedges satisfy condition Corollary 61. 6 support sets, |S| = |E| + 1 holds.

condition Proposition 61 e e1 , e2 violated, maximal size minimal complete support family assessed easily. instance, support hyper
Figure 7 contains 7 edges, 17 support sets. shown k
graph Gsupp(d),T

nodes Gsupp(d),T violate condition, SG
contains |E|k+1 + 1 support sets;
supp(d),T

considered example, yields bound 72 + 1 = 50, far tight.
note Proposition 57 applied tree-cyclic support hypergraphs.

= DL[; Q](X), =
Example 63 Consider tree-cyclic support hypergraph Gsupp(d),T
{D C; C A; C B; B Q} = sig(T ), shown Figure 5b. Using
Proposition 57 get ws(xD ) = 1, ws(xC ) = 2, ws(xA ) = 3, ws(xB ) = 3, ws(xQ ) =
33+1 = 10. However, Q(X) 4 rewritings: (1) S1 = {Q(X)}, (2) S2 = {A(X), B(X)},
(3) S3 = {C(X)}, (4) S4 = {D(X)}.
Intuitively, tree-cyclic hypergraphs support weight function ws may also account nonminimal rewritings {B(X), C(X)}, {A(X), C(X)}, {A(X), D(X)}, {B(X), D(X)},
rewritings counted multiple times. Thus general, ws(x) provides upper bound
number rewriting. Likewise, bound Proposition 61 tight even simple treecyclic support hypergraphs; e.g., one DL-atom = DL[; Q](X) w.r.t. TBox Bi ,
Bi Q, 1 n, contains 2 n edges, n + 2 support sets.


6. Repair Computation Based Partial Support Families
section, present algorithm SoundRAnsSet computing deletion repair answer
sets. shown Stepanova (2015), deciding whether given DL-program = hT A, Pi
EL ontology deletion repair answer set P2 -complete general case,
membership part established guessing candidate repair ABox along candidate
487

fiE ITER , F INK & TEPANOVA

answer set = hT , Pi, suitability guess checked using NP oracle.
Clearly efficient, |2n | candidate repair ABoxes n = |A|, even finding
answer set would cheap.
restrict search space repairs approach work Eiter et al. (2014d)
exploiting support families DL-atoms; however, contrast results Eiter et al. (2014d),
support families required complete. families complete (which may
known asserted construction), SoundRAnsSet guaranteed complete;
otherwise, may miss repair answer set, easy extension ensures completeness.
algorithm repair answer set computation, shown Algorithm 2, proceeds follows.
start (a) computing family nonground support sets DL-atom.
Next (b) so-called replacement program constructed.
replacement program obtained simple rewriting gr(), DL-atom
replaced ordinary atom ed (called replacement atom), disjunctive choice rule
ed ned added informally guesses truth value d, ed (respectively ned )
stands value true (respectively false). repair answer set augmented
proper choice ed resp. ned answer set (Eiter et al., 2013, Proposition 13); thus
search confined answer sets , found using standard ASP
solver.
(c) answer sets computed one one.
determine (d) sets Dp (resp. Dn ) DL-atoms guessed true (resp.
I,
A) instantiates DL-atoms
false) use function Gr(S, I,

Dp Dn relevant ground support sets, i.e., compatible I.
(e) loop minimal hitting sets H support sets DL-atoms Dn
consist ABox assertions, (f) construct H set Dp atoms
Dp least one support set disjoint H (thus removing H
affect values atoms Dp ).

(g) evaluate postcheck atoms Dn Dp \Dp A\H w.r.t. I.
Boolean flag rep stores evaluation result function eval n (resp. eval p ). specifically, given Dn (resp. Dp ), A\H, function eval n (resp. eval p ) returns true,
atoms Dn (resp. Dp ) evaluate false (resp. true).
\ H, P) succeeds, (h)
rep true foundedness check flpFND(I,


restriction I| original language output repair answer set.
remark many cases, foundedness check might trivial superfluous (Eiter,
Fink, Krennwallner, Redl, & Schller, 2014a), e.g., loops DL-atoms;
consider weak answer sets (Eiter et al., 2013), entirely skipped.
Example 64 Let DL-program Example 1 equivalence () axioms (2)
(3) weakened , assertions Project(p1 ) BlacklistedStaffRequest(r1 )
added ABox A. Moreover, assume d1 (r1 ) = DL[Project projfile; Staffrequest](r1 ),
d2 (r1 )=DL[Staff chief ; BlacklistedStaffRequest](r1 ), d3 (r1 ,p1 )=DL[; hasTarget](r1 ,p1 )
488

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

Algorithm 2: SoundRAnsSet: compute deletion repair answer sets
Input: =hT A, Pi
Output: set repair answer sets
(a) compute set nonground support families DL-atoms
(b) construct replacement program
(c) ()

Dn {d | ned I};
SIgr
A);
(d)
Dp {d | ed I};
Gr(S, I,


(e)
minimal hitting sets H Dn SIgr (d )

(f)
Dp {d Dp | SIgr (d) s.t. H = }
A\H) evalp (Dp \Dp , I,
A\H)
(g)
rep evaln (Dn , I,
hT A\H, Pi) output I|

(h)
rep flpFND(I,
end
end

d4 (r1 ,john) = DL[; hasSubject](r1 ,john). (b) following replacement program
constructed:



(1) ed1 (r1 ) ned1 (r1 ); (2) ed2 (r1 ) ned2 (r1 ); (3) ed3 (r1 , p1 ) ned3 (r1 , p1 );








(4)
e
(r1
,
john)

ne
(r1
,
john);
(5)
projfile(p1
);
(6)
hasowner
(p1
,
john);


d4
d4






(7) chief (john) hasowner (p1 , john), projfile(p1 );
=
.


(8) grant(r1 ) ed1 (r1 ), deny(r1 );










(9) deny(r1 ) ed2 (r1 );





(10) hasowner (p1 , john), grant(r1 ), ed3 (r1 , p1 ), ed4 (r1 , john).

Suppose = {ed1 , ned2 , ed3 , ed4 , hasowner (p1 , john), projfile(p1 ), chief (john)} returned
(c) following partial support families obtained (d):


SIgr (d1 ) = {S1 , S2 }, S1 = {hasAction(r1 , read ), hasSubject(r1 , john), Action(read ),
Staff (john), hasTarget(r1 , p1 ), Projectprojfile (p1 )} S2 = {StaffRequest(r1 )};


SIgr (d2 ) = {S1 ,S2 }, S1 = {StaffRequest(r1 ),hasSubject(r1 , john),Blacklisted (john)}
S2 = {BlacklistedStaffRequest(r1 )}.


SIgr (d3 ) = {S1 }, S1 = {hasTarget(r1 , p1 )};


SIgr (d4 ) = {S1 }, S1 = {hasSubject(r1 , john)}.
(e) get hitting set H = {StaffRequest(r1 ), BlacklistedStaffRequest(r1 )}, disjoint
S1 , S1 S1 . Thus (f) obtain Dp = {d1 , d3 , d4 } (g) check whether
d2 false A\H. true, rep = false pick different hitting set H , e.g.
{Blacklisted (john), BlacklistedStaffRequest(r1 )}. Proceeding H , get (g),
H) = true flp-check succeeds (f), interpretation I|
output.
eval n (d2 , I,

following results state algorithm works properly.
489

fiE ITER , F INK & TEPANOVA

Theorem 65 Algorithm SoundRAnsSet sound, i.e., given program = hT A, Pi, every
output deletion repair answer set .
know addition support families complete, postchecks (g)
redundant. Dp = Dp , set rep = true, otherwise rep = false.
Theorem 66 Suppose input program = hT A, Pi Algorithm SoundRAnsSet,
holds DL-atom support family computed Step (a) SoundRAnsSet
-complete. Algorithm SoundRAnsSet complete, i.e., outputs every deletion repair
answer set .
easily turn SoundRAnsSet complete algorithm, modifying (e) consider
hitting sets, minimal ones. worst case, means fallback almost naive
algorithm (note hitting sets enumerated efficiently relative number).
6.1 Optimizations Extensions
Research repairing databases (see work Bertossi, 2011, overview) suggests several
techniques, potential interest DL-programs, could exploited optimizing
extending repair approach. Localization repairs proposed Eiter, Fink, Greco,
Lembo (2008) one technique, cautiously part data affected inconsistency identified search repairs narrowed part. Using localization,
setting ontology ABox split safe set facts, touched
repair, set facts (probably) affected. affected part repaired, result
combined safe ABox part get final solution. find suitable ABox split, meta
knowledge ontology (e.g. modules, additional domain information) used.
Another common approach tackling inconsistency problem, proved effective
databases, decomposition (Eiter et al., 2008). Here, available knowledge decomposed
parts, reasons inconsistency identified part separately,
respective repairs conveniently merged. databases decomposition natural,
general unclear inconsistent DL-program effectively decomposed. One way
approach problem determining DL-atoms whose replacement atoms guessed true
(resp. false) answer sets . Given set DL-atoms, one aim first searching
repair every DL-atom desired value, extend solution
get final result. Modules DL-programs (as identified DLVHEX solver) also
exploited program decomposition.
repairs equally useful certain setting, various filterings repairs applied get plausible candidates. Here, qualitative domain-specific aspects repairs
crucial importance practicability. formulated terms additional local constraints express instance facts involving certain predicates constants must
preserved (resp. checked removal). Furthermore, number facts/predicates/constants allowed deletion bounded. filterings incorporated repair approach.
Yet several extensions possible like conditional predicate dependence. example, user might willing express condition StaffRequest(r ) eliminated
hasAction(r , read ) holds data part, Blacklisted staff members removed,
files, modifying separate StaffRequest issued non-blacklisted staff
member.
490

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

~ P (Y
~)
(r1 ) Supd (X)

A,P

~
~)
(Y
(r2 ) Supd (X)


~ P (Y
~)
(r5 ) ned (X),


~ ) . . . Pnd (Y
~ ) ned (X),
~ A,P (Y
~)
(r6 ) P1d (Y


~ ) rb(S p (Y
~ ))
(r3 ) SdP (Y

A,P ~
A,P ~

~ ))
(r4 ) Sd (Y ) rb(Sd (Y )), nd(SdA,P (Y

~ ed (X),
~ Cd , Supd (X)
~
(r7 ) eval (X)

~
~
(r8 ) eval (X) ned (X), Cd

~ Cd , Supd (X)
~
(r9 ) ed (X),

Figure 8: Rules Rd declarative implementation
6.2 Implementation
implemented repair approach C++ system prototype (dlliteplugin DLVHEX
system, 2015).
discussed, support sets EL ontologies rich structure, thus computation, TBox classification work Eiter et al. (2014d) insufficient. Indeed, need
identify inclusions atomic concepts, also inclusions form C B,
C arbitrarily complex concept B atomic. constructing support sets thus exploit R EQUIEM tool (Prez-Urbina et al., 2010), rewrites target query TBox
using datalog rewriting techniques. limiting number (resp. size) rewritings, partial
support families computed.
principle support sets may subsumed smaller support sets (e.g., {R(c, d),A(c)}
{A(c)}). support sets redundant thus eliminate implementation.
support families constructed, use declarative approach determining repair
answer sets, minimal hitting set computation accomplished rules. end,
~ three fresh predicates (i) Supd (X),
~ (ii) P (Y
~ ), (iii) A,P (Y
~ )
DL-atom d(X)


~ , intuitively say d(X)
~ = XX
~ (i) support set, (ii)
introduced,
support set involving rule predicates, (iii) support set involving ABox predicates (and
possibly rule predicates), called mixed support set. Furthermore, every DL-atom d(X), rules
Rd Figure 8 added replacement program .
~ known
rules, atom Cd informally says support family d(X)
complete. Information completeness support families certain DL-atoms added
declarative program form facts Cd . rules (r1 )-(r4 ) reflect information
~ potential repair; rb(S) stands rule body representing support
support sets d(X)
~ ), . . . , pPnd (Y
~ ),
set S, i.e. rb(S) = A1 , . . . Ak = {A1 , . . . , Ak }; nd(S) = pP1d (Y
~ ), . . . , pPnd (Y
~ )}, encodes ontology part pP (Y
~ ) states assertion
{pP1d (Y
id

~
~
Pi (Y ) marked deletion. constraint (r5 ) forbids d(X), guessed false matching
~ matching
support set consists input assertions; (r6 ) means instead d(X)
mixed support set, assertion ontology part must eliminated. rule (r7 )
~ guessed true, completeness support family unknown matching
says d(X)
~
support set available, evaluation postcheck necessary (eval (X));
rule (r8 ) similar

~
d(X) guessed false. rule (r9 ) states DL-atom guessed true must support
set, support family known complete.
set facts f acts(A) = {pP (~c) | P (~c) A}, encoding ABox assertions COMP
{Cd | Sd complete support family d} added program , answer sets
491

fiE ITER , F INK & TEPANOVA


(1) projfile(p1 ); (2) hasowner (p1 , john); (3) issued (john, r1 );





(4) chief (john) hasowner (p1 , john), projfile(p1 );





(5) deny(r1 ) ed (r1 );



(6) hasowner (p1 , john), issued (john, r1 ), deny(r1 );




(7) ed (r1 ) ned (r1 );





(8) supd (X ) pBlacklistedStaffRequest (X ), pBlacklistedStaffRequest (X);




(9) pBlacklistedStaffRequest (X ) ned (X ), pBlacklistedStaffRequest (X );
R = (10) supd (X ) pStaffRequest (X ), pStaffRequest (X ), phasSubject (X , ),



phasSubject (X, ), pBlacklisted (Y ), pBlacklisted (Y );





(11)
p
(X
) phasSubject (X , ) pBlacklisted (Y ) ned (X), pBlacklisted (Y ),
StaffRequest




pStaffRequest (X ),





phasSubject (X , );




(12)
eval
(X)

e
(X),

C
,

sup
(X);








(13) eval (X) ned (X), Cd ;



(14) ed (X), Cd , supd (X).



























































Figure 9: Program R Example 68
proceed evaluation postcheck atoms
computed. answer set I,
d(~c) fact eval (~c) answer set. evaluation postchecks succeed,
original program I.
way one identifies
extract repair answer set = I|
weak repair answer sets; flp-repair answer sets, additional minimality check needed.
many cases, however, flp weak answer sets coincide (cf. Eiter et al., 2014a); particular,
holds example benchmark programs consider.
formally show described approach indeed correctly computes weak repair answer sets.
Proposition 67 Let = hO, Pi ground DL-program, EL ontology, let
DL-atom Sd SuppO (d), let Rd set rules (r1 )-(r9 ) d. Define
1 = R f acts(A) COMP,

R = Rd , f acts(A) = {pP (~c) | P (~c) A} COMP {Cd | Sd -complete
w.r.t. O}. Suppose (1 ) evaluation postcheck succeeds every DL-atom
RAS weak (). Moreover, Cd COMP every DL-atom d,
Cd 6 COMP. I|

RAS weak () = {I| | (1 )}.
Let us demonstrate usage declarative implementation example.
Example 68 Consider Figure 9 replacement program rules R = hP, Oi,
Example 1, P follows:

(1) projfile(p1 ); (2) hasowner (p1 , john); (3) issued (john, r1 );




(4) chief (john) hasowner (p1 , john), projfile(p1 );
P=

(5) deny(r1 ) DL[Staff chief ; BlacklistedStaffRequest](r1 );



(6) hasowner (p1 , john), issued (john, r1 ), deny(r1 ).
492











.

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

Assume d(X) = DL[Staff chief ; BlacklistedStaffRequest](X) given
incomplete support family Sd = {S1 , S2 }, S1 = {BlackListedStaffRequest(X )} S2 =
{StaffRequest(X ), hasSubject(X , ), Blacklisted (Y )}. interpretation {ned (r1 ),
pStaffRequest (r1 ), pBlacklisted (john), evald } among answer sets R facts(A).
post-check needed d(r1 ); test succeeds, thus I|
repair answer set.
eval I,


7. Evaluation
repair answer set computation approach implemented within DLVHEX system; details
found work Stepanova (2015), software freely online available (dlliteplugin, 2015). approach evaluated multi-core Linux server running DLVHEX 2.4.0
HTCondor load distribution system (HTCondor, 2012), specialized workload management system compute-intensive tasks, using two cores (AMD 6176 SE CPUs) 8GB
RAM.
best knowledge, similar system repairing inconsistent DL-programs exists. list systems evaluating DL-programs includes DR E W system (DReW, 2012;
Xiao, 2014) dlplugin DLVHEX system (dlplugin, 2007). DR E W system exploits
datalog rewritings evaluating DL-programs EL ontologies; however, handle inconsistencies, focus work. Thus DR E W per se could used baseline
experiments. facilitate comparison, thus extended DR E W naive repair
technique, guess repair ABox followed check suitability. However,
immediate implementation turned infeasible even small instances, general
search space repairs ways large full exploitation; guided search needed ensure
scalability. dlplugin DLVHEX system invokes R ACER P RO reasoner (RacerPro, 2007)
back-end evaluating calls ontology. However, lightweight ontologies even
standard evaluation mode without repair extensions, scales worse dlliteplugin (Eiter
et al., 2014b); thus focus latter experiments.
7.1 Evaluation Workflow
general workflow experimental evaluation follows. first step, constructed benchmarks building rules constraints top existing ontologies
data parts constructed programs become inconsistent. instances generated using
shell scripts (DL-program benchmark generation scripts, 2015) size conflicting data
part parameter. benchmarks run using HTCondor system, times
extracted log files runs. run, measured time computing first
repair answer set, including support set computation, timeout 300 seconds.
benchmark, present experimental results tables. first column p specifies
size instance (varied according certain parameters specific benchmark),
parentheses number generated instances. E.g., value 10(20) first column states
set 20 instances size 10 tested. columns represent particular repair
configurations, grouped three sets.
first set refers settings -complete support families exploited,
second third refer settings size, respectively number computed sup493

fiE ITER , F INK & TEPANOVA

port sets restricted. -complete setting, addition limit number facts (lim_f ),
predicates (lim_p) constants (lim_c) involved facts removed; e.g., lim_p = 2
states set removed facts involve two predicates. parameter del _p stores
predicates deleted; e.g., del _p = StaffRequest means repairs obtained
removing facts StaffRequest.
restricted configurations, column size = n (resp. num = n) states
computed partial support families size (resp. number) support sets n; n = ,
fact support sets computed, system aware -completeness.
exploit partial -completeness number size restriction cases, i.e. support sets
atom computed number/size limits yet reached, support family
considered atom -complete.
entry t(m)[n], total average running time (including support set generation
timeouts), number timeouts n number found repair answer sets.
7.2 Benchmarks
evaluation developed algorithms, considered following benchmarks.
(1) policy benchmark variant Example 1, rule (14) P changed
deny(X ) DL[Staff chief ; UnauthorizedStaffRequest](X), two axioms,
namely UnauthorizedStaffRequest StaffRequest hasSubject.Unauthorized
Blacklisted Unauthorized added .
(2) OpenStreetMap benchmark contains set rules ontology enhanced personalized route planning semantic information (MyITS ontology, 2012) extended
ABox containing data OpenStreetMap project (OSM, 2012).
(3) LUBM benchmark comprises rules top well-known LUBM ontology (LUBM,
2005) EL.
describe benchmark results details. experimental data online available
(Experimental data, 2015).
7.2.1 ACCESS P OLICY C ONTROL
considered ABoxes n staff members, n {10, 250, 500}. data set 5
projects 3 possible actions; furthermore 20% staff members unauthorized 40%
blacklisted. generating instances, used probability p/100 (with p column 1)
fact hasowner (pi , si ) added rules part P si , pi , Staff (si ), Project(pi )
(i.e., instances vary facts hasowner (pi , si ) P.) parameter. Here, p ranges 20,
30, etc. 90 A10 5, 10 etc. 40 A250 A500 . total average running times
settings shown Tables 24, SR stands StaffRequest. experiments
performed ABoxes chosen size (i.e., A10 , A250 , A500 ) demonstrate
approach works small, medium large data.
regards A10 , limiting -complete setting number predicates removal slightly
increases running times. Restricting repairs removing facts StaffRequest
slow repair computation compared unrestricted case, many actual
repairs indeed satisfy condition. results bounded number size support sets
494

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

p
20 (20)
30 (20)
40 (20)
50 (20)
60 (20)
70 (20)
80 (20)
90 (19)

-complete support families
restr .
lim_p = 2
del_p = SR
1.92 (0)[20] 2.70 (0)[20]
1.91 (0)[20]
1.94 (0)[20] 2.72 (0)[20]
1.94 (0)[20]
1.93 (0)[20] 2.71 (0)[20]
1.93 (0)[20]
1.92 (0)[20] 2.70 (0)[20]
1.92 (0)[20]
1.94 (0)[20] 2.72 (0)[20]
1.95 (0)[20]
1.95 (0)[20] 2.73 (0)[20]
1.95 (0)[20]
1.94 (0)[20] 2.72 (0)[20]
1.95 (0)[20]
1.96 (0)[19] 2.74 (0)[19]
1.96 (0)[19]

Incomplete support families
size = 3
size = 5
num = 3
38.51 (0)[20]
33.86 (0)[20] 1.93 (0)[20]
86.35 (1)[19]
80.52 (1)[19] 1.95 (0)[20]
98.69 (1)[19]
96.45 (1)[19] 1.94 (0)[20]
100.46 (2)[18]
98.06 (2)[18] 1.93 (0)[20]
182.16 (3)[17] 186.20 (3)[17] 1.96 (0)[20]
153.66 (2)[18] 152.66 (2)[18] 1.96 (0)[20]
227.81 (6)[14] 223.24 (6)[14] 1.96 (0)[20]
267.52 (11)[8] 267.89 (12)[8] 1.96 (0)[19]

num =
1.92 (0)[20]
1.93 (0)[20]
1.93 (0)[20]
1.91 (0)[20]
1.94 (0)[20]
1.94 (0)[20]
1.95 (0)[20]
1.95 (0)[19]

Table 2: Policy benchmark, A10
p
5(20)
10(20)
15(20)
20(20)
25(20)
30(20)
35(20)
40(20)

-complete support families
restr .
lim_p = 2
del_p = SR
6.06(0)[20]
8.28 (0)[20]
6.05 (0)[20]
6.68(0)[20]
8.90 (0)[20]
6.68 (0)[20]
8.37(0)[20] 10.56 (0)[20]
8.35 (0)[20]
9.39(0)[20] 11.61 (0)[20]
9.40 (0)[20]
11.41(0)[20] 13.62 (0)[20]
11.41 (0)[20]
14.04(0)[20] 16.24 (0)[20]
14.09 (0)[20]
15.17(0)[20] 17.32 (0)[20]
15.19 (0)[20]
17.49(0)[20] 19.64 (0)[20]
17.47 (0)[20]

Incomplete support families
size = 6
num = 3
6.06 (0)[20]
6.07 (0)[20]
6.67 (0)[20]
6.69 (0)[20]
8.33 (0)[20]
8.34 (0)[20]
9.40 (0)[20]
9.43 (0)[20]
11.46 (0)[20] 11.40 (0)[20]
14.10 (0)[20] 14.05 (0)[20]
15.12 (0)[20] 15.16 (0)[20]
17.46 (0)[20] 17.45 (0)[20]

num =
6.05 (0)[20]
6.67 (0)[20]
8.34 (0)[20]
9.41 (0)[20]
11.40 (0)[20]
14.04 (0)[20]
15.17 (0)[20]
17.43 (0)[20]

Table 3: Policy benchmark, A250
p
5 (20)
10 (20)
15 (20)
20 (20)
25 (20)
30 (20)
35 (20)
40 (20)

-complete support families
restr .
lim_p = 2
del_p = SR
14.99 (0)[20]
18.71 (0)[20]
14.98 (0)[20]
23.57 (0)[20]
27.14 (0)[20]
23.52 (0)[20]
35.07 (0)[20]
38.85 (0)[20]
35.09 (0)[20]
73.43 (2)[18]
53.27 (0)[20]
73.29 (2)[18]
152.29 (8)[12]
64.91 (0)[20] 152.33 (8)[12]
288.06 (19)[1]
97.32 (1)[19] 288.08 (19)[1]
300.00 (20)[0]
153.03 (5)[15] 300.00 (20)[0]
300.00 (20)[0] 206.96 (10)[10] 300.00 (20)[0]

Incomplete support families
size = 6
num = 3
15.00 (0)[20]
14.97 (0)[20]
23.50 (0)[20]
23.51 (0)[20]
35.02 (0)[20]
35.12 (0)[20]
73.50 (2)[18]
73.32 (2)[18]
164.34 (9)[11] 152.25 (8)[12]
276.11 (18)[2] 288.05 (19)[1]
300.00 (20)[0] 300.00 (20)[0]
300.00 (20)[0] 300.00 (20)[0]

num =
14.97 (0)[20]
23.43 (0)[20]
35.13 (0)[20]
85.33 (3)[17]
164.32 (9)[11]
300.00 (20)[0]
300.00 (20)[0]
300.00 (20)[0]

Table 4: Policy benchmark, A500
almost constant, except size limited 5 smaller (just size 3 size 5 shown).
support sets exceed bound post-evaluation checks often fail, visibly impacts
running times. support sets large, them; seen
insignificant difference times num = 3 num = .
significantly larger ABox A250 , get value p considered settings
perform almost identical except lim_p = 2 bit slower. Moreover, running times increase
gracefully value p. bounding support set size 5 produces timeouts (thus
column omitted), computing support sets size 6 always sufficient identify repairs.
largest setting A500 , -complete case finding arbitrary repair faster
restriction lim_p = 2 , p = 15. p = 20 results lim_p = 2
495

fiE ITER , F INK & TEPANOVA

p
10 (20)
20 (20)
30 (20)
40 (20)
50 (20)
60 (20)
70 (20)
80 (20)
90 (20)

-complete support families
restr .
lim_f = 5
lim_c = 10
13.01 (0)[20]
13.04 (0)[20]
13.05 (0)[20]
13.10 (0)[20]
13.04 (0)[20]
13.08 (0)[20]
13.11 (0)[20]
13.07 (0)[20]
13.12 (0)[20]

16.50 (0)[20]
16.49 (0)[20]
16.54 (0)[20]
16.58 (0)[20]
16.60 (0)[20]
16.61 (0)[20]
16.68 (0)[20]
16.70 (0)[20]
16.81 (0)[20]

16.46 (0)[20]
16.48 (0)[20]
16.49 (0)[20]
16.47 (0)[20]
16.51 (0)[20]
16.55 (0)[20]
16.58 (0)[20]
16.53 (0)[20]
16.59 (0)[20]

size = 1
16.39 (0)[11]
20.98 (0)[5]
24.56 (0)[0]
59.26 (0)[1]
123.80 (0)[0]
106.63 (1)[0]
139.08 (2)[0]
211.33 (5)[0]
260.36 (11)[0]

Incomplete support families
size = 3
num = 1
13.03 (0)[20]
13.04 (0)[20]
13.06 (0)[20]
13.07 (0)[20]
13.10 (0)[20]
13.06 (0)[20]
13.07 (0)[20]
13.06 (0)[20]
13.10 (0)[20]

13.23 (0)[20]
13.35 (0)[20]
13.51 (0)[20]
13.55 (0)[20]
13.56 (0)[20]
13.60 (0)[20]
13.61 (0)[20]
13.61 (0)[20]
13.67 (0)[20]

num = 3
13.06 (0)[20]
13.01 (0)[20]
13.02 (0)[20]
13.09 (0)[20]
13.04 (0)[20]
13.08 (0)[20]
13.07 (0)[20]
13.06 (0)[20]
13.10 (0)[20]

num =
12.99 (0)[20]
13.02 (0)[20]
13.05 (0)[20]
13.05 (0)[20]
13.06 (0)[20]
13.08 (0)[20]
13.13 (0)[20]
13.08 (0)[20]
13.08 (0)[20]

Table 5: Open Street Map benchmark results

outperform unrestricted setting, posed limitation restricts search space repairs effectively. Removing facts StaffRequest longer always sufficient, witnessed
decreased number identified repairs del _p = StaffRequest compared lim_p = 2 .
time increases rather gracefully p long repair answer sets found.
7.2.2 PEN TREET AP
second benchmark, added rules top ontology developed MyITS project.
fixed ontology contains 4601 axioms, 406 axioms TBox 4195
ABox. fragment relevant scenario rules P shown Figure 10.
Intuitively, states building features located inside private areas publicly accessible
covered bus stop bus stop roof. rules P check public stations lack
public access, using CWA private areas.
used method introduced Eiter, Schneider, imkus, Xiao (2014) extract data
OpenStreetMap repository (OSM, 2012). constructed ABox extracting
sets bus stops (285) leisure areas (682) Irish city Cork, well isLocatedInside
relations (9) (i.e., bus stops located leisure areas). data gathered
many volunteers, chances inaccuracies may high (e.g. imprecise GPS data). Since
data roofed bus stops private areas yet unavailable, randomly made 80%
bus stops roofed 60% leisure areas private. Finally, added bsi
isLocatedInside(bsi , laj ) fact busstop(bsi ) P probability p/100. instances
inconsistent since data set roofed bus stops located inside private areas.
results shown Table 5. -complete setting arbitrary repairs computed
3.5 seconds faster repairs bounded changes. restricted configuration
times vary much except size = 1, significant time increase observed,
repairs found smaller instances. Like previous benchmark computing small
number support sets often sufficient, configuration num = 1 expected slightly
slower num = 3 (computing support sets cheap, postchecks take time).
496

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES



=



(1) BuildingFeature isLocatedInside.Private NoPublicAccess
(2) BusStop Roofed CoveredBusStop




(9) publicstation(X) DL[BusStop busstop; CoveredBusStop](X),



DL[; Private](X);
P=
(10)


DL[BuildingFeature
publicstation; NoPublicAccess](X),



publicstation(X ).









Figure 10: DL-program OpenStreetMap ontology
p
5 (20)
15 (20)
25 (20)
35 (20)
45 (20)
55 (20)
65 (20)
75 (20)
85 (20)
95 (20)

restr .
37.14 (0)[20]
35.74 (0)[20]
35.71 (0)[20]
36.07 (0)[20]
35.98 (0)[20]
35.92 (0)[20]
36.13 (0)[20]
36.07 (0)[20]
36.11 (0)[20]
36.38 (0)[20]

-complete support families
lim_f = 5
lim_p = 2
47.77 (0)[20] 43.74 (0)[20]
34.93 (0)[11] 42.74 (0)[20]
26.94 (0)[5] 42.80 (0)[20]
20.53 (0)[0] 43.04 (0)[20]
20.50 (0)[0] 43.11 (0)[20]
20.51 (0)[0] 43.11 (0)[20]
20.43 (0)[0] 43.44 (0)[20]
20.63 (0)[0] 43.45 (0)[20]
20.30 (0)[0] 43.35 (0)[20]
20.55 (0)[0] 43.24 (0)[20]

lim_c = 20
43.88 (0)[20]
41.51 (0)[19]
41.71 (0)[19]
26.91 (0)[7]
19.54 (0)[1]
18.47 (0)[0]
18.33 (0)[0]
18.28 (0)[0]
18.04 (0)[0]
18.20 (0)[0]

Incomplete support families
size = 1
size = 3
42.57 (0)[20] 36.52 (0)[20]
42.02 (0)[20] 35.96 (0)[20]
41.91 (0)[20] 35.80 (0)[20]
42.22 (0)[20] 36.00 (0)[20]
41.94 (0)[20] 36.40 (0)[20]
42.31 (0)[20] 35.98 (0)[20]
41.81 (0)[20] 36.02 (0)[20]
42.09 (0)[20] 36.21 (0)[20]
42.22 (0)[20] 36.15 (0)[20]
42.52 (0)[20] 36.17 (0)[20]

num =
36.26 (0)[20]
35.49 (0)[20]
35.49 (0)[20]
35.65 (0)[20]
35.66 (0)[20]
35.60 (0)[20]
35.92 (0)[20]
35.85 (0)[20]
35.83 (0)[20]
35.62 (0)[20]

Table 6: LUBM benchmark results
7.2.3 LUBM
also tested approach DL-programs = hP, Oi built EL version
LUBM ontology, whose TBox extended following axioms:
(1) GraduateStudent assists.Lecturer TA
(2) GraduateStudent teaches.UndergraduateStudent TA
rules follows:


(3) stud (X ) DL[; Employee](X ), DL[; TA](X );
;
P=
(4) DL[Student stud ; TAof ](X , ), takesexam(X , )
(3) states unless teaching assistant (TA) known employee, he/she student,
(4) forbids teaching assistants take exams courses teach.
ABox contains information one university 600 students, 29 teaching
assistants, constructed dedicated ABox generator (LUBM data generator, 2013). pairs
constants t, c, teachingAssistantOf (t, c) A, facts takesexam(t, c) randomly
added rules part probability p/100, thus contradicting part DL-program
growing respect p.
results benchmark provided Table 6. Bounding -complete setting
number removed facts 5 slows computation, repairs satisfying condition
exist. instances p 35 (i.e., inconsistency entrenched), 5 facts must
dropped obtain repair; moreover, often involve 20 constants according
497

fiE ITER , F INK & TEPANOVA

column 5. absence repairs lim_f = 5 lim_c = 20 found faster repair
unrestricted mode.
Limiting support set size 1 allows one find repairs instances delay less
10 seconds compared -complete setting. However, many support sets
benchmark, thus bounding number less effective.
7.3 General Results Discussion
One observe -complete settings settings post-evaluation checks fast,
running times vary slightly growing p. due declarative implementation,
computing repairs reduced finding answer sets program 1 = R facts(A)
COMP followed possible evaluation postchecks. benchmarks difference
instances size pi pi+1 data part logic program, small compared
part facts(A) 1 constant p. Thus long postchecks needed, times
required repairing differ much even though programs become inconsistent.
expected, using -complete support families works well practice. Naturally, takes
time compute restricted repairs rather arbitrary repairs; however, imposed restrictions strong repair satisfy them, solver may recognize faster.
reported Hansen et al. (2014), EL-TBoxes originate real-world applications
admit FO-rewritings (of reasonable size) almost cases. provides evidence realworld EL-TBoxes hardly contain involving constraints conceptual level, hence either
size number support sets DL-atoms often turn limited. novel algorithms
deletion repair answer set computation demonstrated applicability DL-programs
real world data (Open Street Map benchmark results Table 5).
benchmarks run synthetic, still vary w.r.t. TBox
ABox sizes. capability algorithms handling diverse DL-programs confirms
potential approach.

8. Related Work
Inconsistencies DL-programs studied several works (Phrer et al., 2010; Fink, 2012;
Eiter et al., 2013, 2014d). Phrer et al. proposed inconsistency tolerant semantics. Keeping
ontology untouched, DL-atoms introduce inconsistency well rules involving
deactivated. repair problem, outlined open issue Phrer et al., formalized Eiter
et al. (2013), notions repair repair answer sets together naive algorithm
computation proposed. latter optimized Eiter et al. (2014d, 2015)
DL-Lite effectively exploiting complete support families DL-atoms. approach
general, differs one Eiter et al. (2014d, 2015) uses partial (not
necessarily complete) support families applied ontologies DL, though
possible impact complexity.
hybrid formalisms, inconsistency management concentrated inconsistency tolerance rather repair. instance, Huang et al. (2013) presented four-valued paraconsistent
semantics based Belnaps logic (Belnap, 1977) hybrid MKNF knowledge bases (Motik &
Rosati, 2010), prominent tightly coupled combination rules ontologies.
Inspired paracoherent stable semantics Sakama Inoue (1995), work Huang
et al. (2013) extended Huang, Hao, Luo (2014) handle also incoherent MKNF KBs,
498

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

i.e. programs inconsistency arises result dependency atom default
negation analogy work Fink (2012). Another direction inconsistency handling
hybrid MKNF KBs using three-valued (well-founded) semantics Knorr, Alferes, Hitzler (2011), avoids incoherence disjunction-free stratified programs. recently,
extended Kaminski et al. (2015) additional truth values evaluate contradictory
pieces knowledge. works aim inconsistency tolerance rather repair, geared
spirit query answering inherent well-founded semantics; such, limited
normal logic programs, DL-programs allow disjunctive rule heads.
context Description Logics, repairing ontologies studied intensively, foremost
handle inconsistency. DL-program repair related ABox cleaning (Masotti, Rosati, &
Ruzzi, 2011; Rosati, Ruzzi, Graziosi, & Masotti, 2012). However, latter differs various
respects: aims restoring consistency inconsistent ontology deleting -minimal sets
assertions (i.e., computing -maximal deletion repairs); deal inconsistency incurred
top consistent ontology, arbitrary (non-monotonic) rules access query
interface. Furthermore, must consider multiple ABoxes (via updates), use EL instead
DL-Lite. Refining algorithm compute -maximal deletion repairs possible.
problem computing support families tightly related finding solutions abduction
problem, considered Bienvenu (2008) theories expressed EL-terminologies.
hypothesis H = {A1 , . . . , } set atomic concepts, observation another atomic
concept. solution abduction problem set H, |= Ai Ai O.
setting general involves also roles along atomic concepts. Abduction
studied various related areas e.g., DL-Lite ontologies Calvanese, Ortiz, Simkus,
Stefanoni (2013), propositional logic Eiter Makino (2007), datalog Eiter et al.
(1997) Gottlob, Pichler, Wei (2007), etc. Using incomplete support families DL-atoms
related spirit approximate inconsistency-tolerant reasoning DLs using restricted support
sets considered Bienvenu Rosati (2013); however, focus repair computation
model generation Bienvenu Rosati target inference repairs.
methods constructing partial support families exploit results logical difference
EL terminologies presented Konev et al. (2012) Ecke et al. (2013); recently
extended ELHR Ludwig Walther (2014) general TBoxes Feng et al.
(2015).
Repairing inconsistent non-monotonic logic programs investigated work
Sakama Inoue (2003), approach deleting rules based extended abduction
studied; however, restore consistency addition rules also possible. latter considered
Balduccini Gelfond (2003), Occams razor consistency-restoring rules may
added. Methods explaining inconsistency arises logic program studied, e.g.,
Syrjnen (2006), exploited model-based diagnosis Reiter (1987) debug logic program. Generalized debugging logic programs investigated e.g., Gebser, Phrer, Schaub,
Tompits (2008). recently, Schulz, Satoh, Toni (2015) considered characterization
reasons inconsistency extended logic programs (i.e., disjunction-free logic programs
strong (classical) negation weak negation) terms culprit sets literals, based
well-founded maximal partial stable model semantics, derivation-based method explain culprits described; however, remains open debugging logic programs
based culprit sets could done whether could fruitfully extended debugging DLprograms. latter addressed Oetsch, Phrer, Tompits (2012) related
F

499

fiE ITER , F INK & TEPANOVA

challenging but, best knowledge, unexplored problem repairing rule part
DL-program.

9. Conclusion
considered computing repair answer sets DL-programs EL ontologies,
generalized support set approach Eiter et al. (2014d, 2014b) DL-Lite work
incomplete families supports sets; advance needed since EL complete support families large even infinite. discussed generate support sets, exploiting query
rewriting ontologies datalog (Lutz et al., 2009; Rosati, 2007; Stefanoni et al., 2012),
contrast work Eiter et al. (2014d), TBox classification invoked. Moreover,
developed alternative techniques effective computation partial support families.
approach approximate relevant part TBox DL-Lite core exploiting notion logical
difference EL-terminologies, compute complete support families approximated TBox using methods Eiter et al. (2014d). obtained support family complete,
approximated TBox logically equivalent original one.
estimate maximal size support sets, analyzed properties novel support hypergraph, corresponds subgraph ontology hypergraph (Nortje et al., 2013;
Ecke et al., 2013), nodes encode ontology predicates (or pairs them), (hyper) edges
reflect TBox inclusions. shown traversing support hypergraph one conveniently
compute upper bound number support sets given DL-atom. If, addition,
support hypergraph satisfies certain conditions (e.g. tree-acyclicity), exact estimate
obtained.
developed sound algorithm computing deletion repair answer sets DL-programs
EL ontologies, complete case support families also known complete.
algorithm trades answer completeness scalability (a simple variant ensures completeness).
implemented novel algorithm using declarative means within system prototype,
invokes R EQUIEM reasoner partial support family computation. experimental assessment repair approach, set novel benchmarks constructed including real
world data. availability complete support families adds scalability repair
computation, partial support families work surprisingly well practice due structure
benchmark instances: support sets either small them, thus postevaluation checks cause much overhead. Overall, experimental evaluation revealed
promising potential novel repair methodology practical applications.
9.1 Outlook
directions future work considered area manifold. cover theoretical
practical aspects inconsistency handling approach. theoretical side, relevant open
issue sufficient conditions computing nonground support sets DL-atom
accessing EL ontology becomes tractable. Like work Gebser et al. (2008) bounded
tree-width might considered, also parameters like density support hypergraph
various acyclicity properties. Analyzing complexity counting support sets complete
support family might give hints possible restricted settings, support family computation
efficient, complexity analysis also interesting problem such. practical
500

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

side, optimization current implementation extending range applications real use
cases another issue.
Repair may intermingled stepping techniques used debugging DL-programs (Oetsch
et al., 2012). considered DL-programs monolithic structures applying repair
techniques, repair computation performed DL-program taken whole.
interesting relevant quest extend approach dealing modular DL-programs.
Splitting program separate components individually evaluated well-known
programming technique, studied context DL-programs (Eiter et al., 2008).
clear, however, extent program classes repair methods
adapted modular setting.
considered EL paper, basic algorithm approach applicable also
DLs. Extensions work EL+ EL++ easily possible. main difference
negation, expressible via concept; ontology get inconsistent
updates DL-atoms, leading increased number support sets need effectively
computed appropriately handled. extension expressive DLs SHIQ, SHOIN
even SROIQ challenging, efficient methods support set construction remain
developed; relatively high complexity DLs, comes computational cost.
hand, computation may done (even offline) reused; fortunately,
support families need complete, may expect return investment time support
set construction overall running time.
Orthogonal DLs, one study various additional repair possibilities, e.g. bounded
addition; overview repair possibilities see work Eiter et al. (2013).
concentrated repairing data part ontology, also natural allow changes
rules interfaces. repairing rules, works ASP debugging Frhstck, Phrer,
Friedrich (2013), Gebser et al. (2008), Syrjnen (2006) used starting point,
problem challenging search space possible changes large. Priorities rules
atoms involving might applied ensure high quality rule repairs. interfaces
similarly admit numerous modifications, makes type repair difficult; user interaction
probably required.
Last least one could develop methods repairing hybrid formalisms including
tightly-coupled hybrid KBs even general representations like HEX-programs (Eiter et al.,
2005), instead ontology arbitrary sources computation accessed logic
program. Heterogeneity external sources HEX-programs makes repair paraconsistent
reasoning challenging task.

Acknowledgments
thank anonymous reviewers detailed constructive suggestions helped
improve work. article significantly extends preliminary work Eiter, Fink, Stepanova
(2014c). research supported Austrian Science Fund (FWF) projects P24090
P27730.
501

fiE ITER , F INK & TEPANOVA

Appendix A. Proofs Section 3
A.1 Proof Proposition 15
() Proposition 10, |=O iff Td AI |= Q(~t), AI = {Pp (~t) Ad | p(~t) I}.
Thus, = AI support set w.r.t. O, coherent construction.
() Supp (d) coherent I, form = AI
AI AI , thus AI . Td |= Q(~t), monotonicity Td AI |= Q(~t),
hence Proposition 10 |=O d.
A.2 Proof Proposition 24
~
Consider instance = {P1 (Y1 ), . . . , Pk (Yk )} set form (5) d(X),
: V C. show support set w.r.t. OC = hT , AC (recall AC set
~
possible ABox assertions C), i.e., AC Ad (which clearly holds) Td |= Q(X).
~
latter equivalent Tdnorm |= Q(X),
turn Lemma 23 equivalent
0
~
Prog Q,Td norm |= Q(X). Let Prog = Prog Q,Td norm , let Prog i+1 , 0,
~
program results Prog unfolding rule w.r.t. target query Q(X).

i+1

~
~
Prog
|= Q(X) iff Prog |= Q(X) holds. construction S, rule
~ thus Prog |= Q(X).
~
r form (4) Prog . Clearly {r} |= Q(X)

0
~
~
~
follows Prog |= Q(X) hence Td norm |= Q(X) Td |= Q(X).

Appendix B. Proofs Section 4
B.1 Proof Lemma 31
Towards contradiction, assume T1d 6C
T2d . w.l.o.g. T1d |= P1 P2 T2d 6|= P1 P2 ,

P1 , P2 . Observe differ predicates Pp , P p occurs
, = T1d \T1 = T2d \T2 consists axioms Pp P Pp occur
T1 T2 . first show P2 must hold. Indeed, otherwise P2 \ thus P2 =
Pp sig(Ad ) P p . let = {P1 (c)} P1 , = {P1 (c), P1p (c)}
otherwise (i.e., P1 \ ), arbitrary c I. T1d model cI P1I
(resp. cI P1I cI P1 Ip ) Pp = (thus P1 6= P2 ), EL negation-free Pp occurs
axioms left. 6|= P1 Pp , follows T1d 6|= P1 P2 , contradiction.
proves P2 \ . two cases.
(i) P1 : T1 C
T2 implies T2 |= P1 P2 ; monotonicity T2d |= P1 P2 , contradiction.
(ii) P1 \ : P1 = Pp , P p occurs , P . claim T1 |= P P2 .
Indeed, otherwise T1 model P 6 P2 . easily seen interpretation


coincides Pp = P \ P2 Pp = Pp \ model
T1 ; however, 6|= Pp P2 , would contradiction. proves claim.
claim T1 C
T2 , follows T2 |= P P2 monotonicity T2d |= P P2 .
Pp P T2 , follows T2 |= P1 P2 ; contradiction.
B.2 Proof Proposition 32
Suppose S1 complete nonground support family w.r.t. O1 let instance

S1 ; = Ad AC Ad . Lemma 31, T1d C
T2d ; thus Theorem 30, T1d
502

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

T2 well. definition -instance inseparability, -ABoxes -assertions
T1d |= , holds T2d |= ; hence T2d Ad |= Q(~c). Consequently,
= Ad (ground) support set w.r.t. O2 . S2 complete nonground support family
w.r.t. O2 , follows instance S2 . converse membership symmetric.
Hence, S1 S2 ground-identical.
B.3 Proof Proposition 34
Towards contradiction, assume \ SuppO (d) exists. grounding exists
Td 6|= d(X). However, Td |= d(X), according (f), nonground
support set w.r.t. Td = Td lrw . Consequently, Td 6|= Td , contradiction,
Td Td construction (c) lrw = {C | Td |= C , Td 6|= C } Td
lhs
(d) definition cWTnrhs
cWTn .

Appendix C. Proofs Section 5
C.1 Proof Lemma 47

w.r.t. ontology =
construction support sets given hypergraph Gsupp(d),T
hT , Ai presented mimics DL-query unfolding TBox Td . formally
show (i) set extracted described way indeed nonground support sets d,
(ii) ground instance nonground support set d, (nonground) support
set constructed following procedure suitable ground
substitution . proves SG holds.
first prove (i) induction length n incoming paths, support sets
extracted.

Base: n=1. Consider path hypergraph Gsupp(d),T
. Assume single
(hyper-) edge e . construction, hyperedge must xQ head node, i.e. head (e) =
xQ . four possibilities: (1) tail (e) = {xC }, (2) tail (e) = {xr , xC }, (3) tail (e) =
{xC , xD } (4) tail (e) = {xr , }. annotate nodes path variables described
above, extract nonground atoms labels annotations nodes. result
case (1) obtain {C(X0 )}, (2): {r(X0 , X1 ), C(X1 )}, (3): {C(X0 ), D(X0 )},
(4): {r(X0 , X1 )}, X1 fresh variable. construction hypergraph edges
forms (1)-(4) correspond TBox axioms C Q, r.C Q, C Q r. Q
respectively. Therefore, sets constructed considered cases reflect
DL-query unfoldings d, hence represent nonground support sets Proposition 24.
Induction step: Suppose statement true n, i.e. path n edges sets extracted way described nonground support sets d. Consider path = e0 , . . . , en
n+1 edges, let e = e0 first edge . induction hypothesis, sets extracted
path \ e = e1 , . . . , en following approach support sets d. several
possibilities form e: (1) tail (e) = {xC } head (e) = {xD }, (2) tail (e) = {xr , xC }
head (e) = {xD }, (3) tail (e) = {xC , xD } head (e) = {xB }, (4) tail (e) = {xr , }
head (e) = {xC }, (5) tail (e) = {xC } head (e) = {xr , xD }.
(1), construction xC xD annotated Xi . Let family sets
extracted \e. pick set C(Xi ) occurs. substitute C(Xi ) D(Xi ),
obtain set . induction hypothesis must support set d. However,

503

fiE ITER , F INK & TEPANOVA

clearly also support set, mimics additional unfolding step accounts rule
C(X) D(X) datalog rewriting Td .
Let us look (2). Assume set D(Xi ) nonground atoms constructed using
procedure. Xi must annotation xD . According construction {xr , xD }
annotated {hXi , Xj i, hXj i}, Xj fresh variable. sets get result
substituting D(Xi ) {r(Xi , Xj ), C(Xj )}. latter mimics unfolding step
Q accounts rule D(Xi ) r(Xi , Xj ), C(Xj ) rewriting Td . support
set induction hypothesis, must support set well. cases (3)-(5)
analyzed analogously. Thus sets size n + 1 extracted support sets d.
remains prove (ii). Towards contradiction, assume ground instance
SuppO (d) exists, ground instance every SuppO (d)
constructed procedure 6 S. support set, definition Td norm
|= Q(~c), thus Lemma 23 Prog Q,Tdnorm |= Q(~c). turn means Q(~c)
~ 0 Sm = ,
backchaining proof S0 , S1 , . . . , Sm Prog Q,Td norm form S0 = Q(X)
~ 7 ~c, Si = (Si1 Hi + Bi )i , 1, Hi Bi
0 substitution X
rule resp. fact Prog Q,Td norm general unifier Hi atom Si1 .
Without loss generality, Hi = A2 (oA2 ) Hi1 = R2 (X, oA2 ) Bi
empty end, i.e. positions k, k + 1, . . . , m. Sj resp. Sj+1 , 0 j k


amounts instance support set Sj resp. Sj+1
generated Gsupp(d),T
. particular,


Sk1 instance Sk1 consequently {Hk , Hk+1 , . . . , Hm } ( S) instance Sk1





well. means instance = Sk1 , contradiction.
C.2 Proof Proposition 48
prove statement induction number n hyperedges singleton head node

G = Gsupp(d),T
DL-atom DL[; Q](X).
Base: n = 0. show maxsup(d ) = 1 hyperedges required form exist G.
Several cases possible: (i) G contains hyperedges form (xC , {xr , xD }); (ii) G
hyperedges form ({xr , }, xC ) (xC , {xr , }); (iii) G hyperedges.
(i) Consider hyperedge . ej must exist , head (ei ) tail (ej ).
latter implies ej form ({xr , xD }, xD ) n 6= 0, i.e. contradiction.
(ii) (iii), construction contains GCIs C C, either atomic
form r.. axioms fall DL-Lite core fragment, -minimal
support sets size 2; moreover, |S| = 2 reflects DL-Lite core inconsistency arising
updated ontology (Eiter et al., 2014d). negation available expressible EL,
exists thus maximal support set size 1.
Induction Step: Suppose statement true n; prove n+1. Let = e1 , . . . , ek

maximal number n+1 hyperedges singleton
incoming path xQ Gsupp(d),T
head node. Assume ei first hyperedge required form occurring . Let us
split two parts: e1 , . . . , ei ei+1 , . . . , ek . Consider hypergraph G = (V, E ),
E = E \ {e1 , . . . , ei }, TBox reconstructed it. induction hypothesis,
maxsup(d ) w.r.t. = hT , Ai bounded n + 1. let hypergraph G = (V, E )
E = E {ei } correspond TBox . assumption head (ei ) = xA , i.e. ei either
reflects B C r.B A. Two cases possible: either = Q 6= Q.
504

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

x C n1

x Cn

x C nk

xCn

...

xCn

1

..
.

k

..
.
...
xQ

Figure 11: Fragment hypergraph used illustration proof Proposition 53
former case, ei single hyperedge , i.e. n = 1. Support sets obtained rewriting Q
B C Q r.B Q size 2. support sets constructed combining
query rewritings predicates occurring left hand side GCIs reflected ei ;
rewritings size 1 shown base case. Thus overall support set size w.r.t.
bounded 2 n + 1.
Suppose 6= Q, i.e. ei reflects either B C r.B A. definition
incoming path (hyper) edge ej must exist, head (ei ) tail (ej ). Moreover, note ej
unique (hyper) edge connected ei , otherwise given hypergraph tree-cyclic, i.e.
contradiction. distinguish two cases: (1) head (ei ) = tail (ej ) ej corresponds . . . ;
(2) head (ei ) tail (ej ) ej reflects B . . . .
1. Consider maximal support set w.r.t. , suppose A(Y ) holds. induction

hypothesis |S| n. G = Gsupp(d),T
tree-acyclic, single atom might

occur S. Adding edge ei G obtain support set atom A(Y )
substituted atoms B(Y ) C(Y ), r(Z, ) B(Z) result additional
query unfolding step. Hence support set size bounded n + 2.
2. ej reflects B . . . , support set {A(Y ), B(Y )} must exist. unfolding
respective datalog rule, get bound n + 2 support set w.r.t. .

C.3 Proof Sketch Proposition 53
Observe tree-acyclic hypergraphs nodes hyper out-degree 1, hence
m(, G) = 0. Thus, G tree-acyclic, Proposition 48 support set size given
DL-atom bounded n(, G) 0 + 1, equals smax . show claimed bound
also correct tree-cyclic hypergraphs. Intuitively, m(, G) must subtracted n(, G)
avoid certain atoms support set counted multiple times. Regarding structure
support hypergraph distinguish two cases: (i) roles appear hypergraph; (ii) xr G,
holds r 6 .
505

fiE ITER , F INK & TEPANOVA

First consider (i). Since concepts appear support hypergraph assumption, support sets contain atoms single variable X0 occurs. Consider
node xCn hdc+ (xCn ) = k, k > 1, i.e., k outgoing hyperedges
xCn containing nodes corresponding concepts: ({xCn1 , xCn }, xCn ). . .({xCnk ,xCn }, xCn )
1
k
(see Figure 11). support sets {Cn 1 (X0 ), . . . , Cn k (X0 )} get support sets
{Cn1 (X0 ), . . . , Cnk (X0 ), Cn (X0 )}. Estimating maximal support set size number hyperedges hypergraph, Cn (X0 ) counted k times, appears (as variable
guaranteed X0 ). avoid multiple countings, m(, G) must subtracted n(, G).
Consider (ii). construction G, every hypernode {xr , xC } edges e1 =
(xA , {xr , xC }) e2 = ({xr , xC }, xB ) exist G. Thus xr occurs , consider
support set {B(X)}. Rewriting TBox axiom reflected e2 , get datalog rule
B(X) r(X, ), C(Y ). axiom r.C reflected e1 rewritten datalog rules
r(X, oC ) A(X); C(oC ) A(X). Unifying oC obtain unfolding A(X).
essentially shows role occurring support hypergraph , support sets
involve single variable; case, shown (i), provided bound correct.
C.4 Proof Proposition 57

proof induction number n (hyper) edges G = Gsupp(d),T
. Base: n=0. G
(hyper) edges, node one support set.
Induction step: Suppose statement holds n; show holds G n + 1 (hyper)
edges. Obviously, holds x VR . G tree-acyclic normal form, G
node x hd+ (x) = d+ (x) = 0, i.e., outgoing (hyper) edges, hd (x) 6= 0
(x) 6= 0, i.e., incoming (hyper) edge. G tree-acyclic, rewriting
set Qx = {A(X)}, x = xA consists Qx rewritings sets Qtail(e)
(hyper) nodes tail (e) head (e) = x. tail (e) {xB } (resp., {xB , xC }, {xr , xC })
rewritings {B(X)} (resp. {B(X), C(X)}, {R(X, ), C(Y )}). is, ws(xA )
sum number rewritings Qtail(e) denoted Qtail(e) , plus 1. Consider
arbitrary e head (e) = xA let G = G\e. G n edges tree-acyclic,
induction hypothesis node x V G , value ws(x), denoted wsG (x), (8).
Furthermore, ws(Qtail(e) ) ws(x ), x 6= xA G G. thus get x = xA :

wsG (x) = wsG (x) + ws(Qtail(e) )
X
wsG (x ) +
= 1+


= 1+



(x)

x

X



wsG (x ) +

X



wsG (x ) +

(x) x

X

ws(x ) + ws(Qtail(e) )

X

X

ws(x ) + ws(Qtail(e) )



(x),T 6VC ({x },T )E

(x),T 6VC ({x },T )E

(x) x

= 1+



X

X

X

ws(x )

(x),T 6VC ({x },T )E



(x) = {T | (T, {x}) E } E = E \ {e}, (x) above. obtain
ws(Qtail (e)), simply need count combinations rewritings node tail (e),
case tail (e) = {xr , xB } (where ws(xr ) = 1), need add number rewritings
tail hyperedge (T, {xr , xB }) (as normal form, must form {xC }).
506

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

C.5 Proof Corollary 60
Q
immediate Proposition 57: hypothesis, (8) form {y} VC ;
thus x ws(x ) = ws(y), i.e., ws(tail (e)) rightmost term 0.
C.6 Proof Sketch Proposition 61
condition e e1 , e2 , every set (x) Equation (8)
Q |T| = {x, y} > 1
contains (at least) one element, say x, ws(x) = 1, thus x wsG (x ) equals ws(y)

. inductive argument, obtain every node xA VC ,
G = Gsupp(d),T
ws(xA ) 1 number distinct edges G occur incoming paths xA
xB VC edge ({xB }, {xr , xA }) E, plus number edges.
turn implies query node xQ , ws(xq ) = |E| + 1 holds, construction edge e E
among respective edges xQ . result follows immediately.

Appendix D. Proofs Section 6
D.1 Proof Theorem 65
. get (h) answer set ,
Suppose SupRAnsSet outputs = I|
foundedness check w.r.t. ontology , = A\H succeeded. thus
remains show compatible set , i.e., DL-atom , Dp


iff |=O Dn iff 6|=O d. Towards contradiction, suppose case.
(d) partitioned DL-atoms two sets: Dp Dn , corresponding DL-atoms guessed

respectively, set SIgr
A). Since assume
true false I,
Gr(S, I,
compatible, one following must hold:

(1) DL-atom Dn , |=O d. two possibilities: (i) either

support set SIgr (d) (ii) support sets identified. case (i), guaranteed
support sets 6= , since otherwise hitting sets H found
(e). Hence must exist support set =
6 . According (e) H 6=
thus 6 Suppd (O ). rep = true (h), post-check must succeeded (g),

i.e. 6|=O must hold. contradiction. case (ii), likewise post-evaluation must
succeeded (h), raises contradiction.




(2) DL-atom Dp , 6|=O d. Hence SIgr (d) = , 6 Dp , post-evaluation
performed (g). latter, however, must succeeded, rep = true (h);
contradiction. Hence compatible set , thus deletion repair answer set .
D.2 Proof Theorem 66
following lemmas useful prove Theorem 66.
Lemma 69 Let ASx () x {flp, weak } = hT , A, Pi ground DL-program.
= {ed | DL , |=T d} {ned | DL , 6|=T d} answer set ,
DL set DL-atoms occurring .
lemma follows general result compatible sets basis evaluation
approach HEX-programs DLVHEX-solver (cf. (Eiter et al., 2014a)).
507

fiE ITER , F INK & TEPANOVA


Lemma 70 Let = hT , A, Pi ground DL-program let () = I|

ASx (), x {flp, weak }. Suppose DL-atom occurring P,

holds |=T iff |=T d. ASx ( ) = hT , , Pi.
, PxI,T PxI,T coincide; ASx (), minimal
Proof. note = I|

model PxI,T . Consequently, also model PxI,T . Moreover, minimal,

J satisfies PxI,T , J |= PxI,T ; hence answer set PxI,T , contradiction.

Suppose RAS x (). implies ASx ( ) = hT , Pi,
A. Lemma 69 answer set thus considered (c). (d), Dp

Dn set (correct) guess |=O DL-atom d, = .
)(d) 6=
Proposition 15 -completeness S, obtain Dp Gr(S, I,


)(d) = . Gr(S, I,
)(d) Gr(S, I,
A)(d) holds
Dn Gr(S, I,



DL-atom d, follows Dn Sgr (d) (A \ ) 6= ; means


H = \ hitting set Dn SIgr (d ), hence minimal hitting set H H





considered (e). (f), Dp set Dp Dp SIgr (d) exists
H = , hence H = . Thus (g) call eval p ( ) yields true,
\ H)(d) = ; thus rep true. Eventually, (h)
likewise call eval n ( ) Gr(S, I,

test flpFND(I, hT A\H, Pi) succeed, x-answer set = hT , Pi,
output.
Lemma 70 also = hT \ H, Pi, \ H. Thus step (h) = I|
D.3 Proof Proposition 67
RAS weak (). Towards contradiction,
first show every (1 ), holds I|


suppose (1 ) exists I| 6 RAS weak (). every
6 weak ( ) = hT , , Pi. particular, = A\{P (~c) | pP (~c) I}
I|
6 weak ( ) = hT , , Pi several possibilities: (i)
holds I|
c ; (ii)
guess replacement atoms ed , ned model
extension I|
,O
compatible set ; (iii) interpretation J I|
model P I|
.
extension I|
weak

c hence follows I|
c .
|=
case (i) impossible: =

. Towards contradiction, assume
Assume (ii) true. Consider interpretation I|

,
|=O ned I|
compatible . DL-atom either (1) I|





holds. case (1), I|
|=
d, ed I|
d, support set
(2) 6|=

exists. consider whether Sd 6 Sd . former case, must
coherent I|

contain ABox assertions SdA , otherwise constraint form (r5 ) violated. Due
rule (r6 ) least one assertion Pid SdA must marked deletion. Note Pid
present , relevant support set w.r.t. . Sd known complete,
immediately arrive contradiction. Otherwise, rule form (r8 ) applied,
evaluation postcheck succeeded assumption, get contradiction. 6 Sd ,
Sd known complete, rule form (r8 ) applied; due successful
6|=O d,
evaluation postcheck, contradiction obtained. suppose (2) true. I|
. Sd known complete,
support set exists w.r.t. coherent I|
508

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

constraint (r9 ) violated; contradicts |= 1 . Thus, body rule (r7 ) satisfied,
evaluation postcheck issued fails; hence get contradiction.
,O
model P I|
Finally, assume (iii) holds, i.e. interpretation J I|
weak .
\J contains atoms signature . Let us consider IM = \ .
set = I|



know (1 ); Hence rule rI must exist 1 IM |= B(rI ),
gl

gl

gl


IM 6|=
Recall 1 = ( R f acts(A) COMP). rgl
,O
,O
,O

I|
P I|
P I|

( f acts(A) COMP)Igl , rgl
iff rgl
weak
weak J 6|= Pweak construction
must RI . However,
GL weak reducts, contradiction. Therefore, rgl
gl


latter also raises contradiction: rule Rgl atoms signature head IM

coincide rule head; thus follows 6|= B(RIgl ), contradiction. Therefore,
( ) holds, global contradiction, i.e. I|
RAS weak () follows.
I|
).
H(rgl

consider case support family Sd known complete, prove
AS| (1 ) = RAS weak (). shown above, remains check
AS| (1 ) RAS weak (). Towards contradiction, assume RAS weak () exists

/ (1 ) every extension I. RAS weak (), ABox exists
( ) = hT , , Pi. construct extension follows:


= {ed | |=O d} {ned | 6|=O d}

{pP (~c) | P (~c) A\A } f acts(A) COMP
{Supd (~c) | d(~c) support set Sd coherent I}
{S P (~c) | |= rb(S A,P (~c))} {S A,P (~c) | |= rb(S A,P (~c)), nd(S A,P (~c))}.








Since assumption 6 (1 ), one following must hold:

(i) |6 = ( R f acts(A) COMP)Igl

(ii) J exists, J |= ( R f acts(A) COMP)Igl .

satisfies rules forms (r )-(r ).
First assume (i) true. construction I,
1
4
Moreover, constraints form (r5 ) violated, DL-atom d(~c) 6|=O d(~c)
support set consists input assertions. rules (r7 ) (r8 ) present

reduct 1 Igl , |= Cd DL-atom d(~c).

Thus rule r 1 6|= rI could form (r ) (r ). case
gl



6

9

form (r6 ), DL-atom d(~c) would exist 6|=O d(~c). Proposition 15 support set
Hence, r must
d(~c) would exist coherent I, construction SdA,P (~c)
/ I.

form (r9 ); however, |=O d(~c) completeness Sd Proposition 15, construction
implies r violated.
Supd (~c) I,

let (ii) hold, i.e. J exists s.t. J |= 1 Igl . J contains DL-atom
d(~c) exactly one ed (~c) ned (~c) 1 contains ed (~c) ned (~c), interpretations J
coincide replacement atoms ed (~c) ned (~c). Suppose \ J contains atoms
6|= P I,O ; hence rule rI,O P I,O , exists
language . J|
weak
weak
weak
6|= H(rI,O ). Consider respective rule rI . J 6|= H(rJ ),
|= B(rI,O ), J|
J|
gl
gl
gl
weak
weak
509

fiE ITER , F INK & TEPANOVA

). construction weak GL reduct, respectively, positive
must J 6|= B(rgl






J ) B(r I,O ) same. Hence, replacement atom e (~
normal atoms B(rgl
c) (resp.
weak






ned (~c)) must occur positively B(rgl ), ed (~c) \ J (resp. ned (~c) \ J).
already argued, latter possible, leading contradiction.

Consequently, \ J must contain atoms language R. every rule rgl


form (r ) (r ) J |= B(rI ) iff |= B(rI ), thus J agree atoms P (~c)
3
A,P
Sd (~c).

4

gl

gl



Similarly, via (r1 ) (r2 ) must J agree atoms Supd (~c). Finally,
conclusion,
holds pP (~c) pP (~c) rules (r6 ) construction I.


J = holds, violates (ii).
Thus, follows (1 ). Consequently, (1 ) RAS weak (1 ) holds; proves
result.

References
Alchourrn, C. E., Grdenfors, P., & Makinson, D. (1985). logic theory change: Partial
meet contraction revision functions. J. Symbolic Logic, 50(2), 510530.
Aranguren, M. E., Bechhofer, S., Lord, P. W., Sattler, U., & Stevens, R. D. (2007). Understanding
using meaning statements bio-ontology: recasting gene ontology OWL.
BMC Bioinformatics, 8(1), 113.
Ausiello, G., DAtri, A., & Sacc, D. (1983). Graph algorithms functional dependency manipulation. J. ACM, 30(4), 752766.
Ausiello, G., DAtri, A., & Sacc, D. (1986). Minimal representation directed hypergraphs. SIAM
J. Computing, 15(2), 418431.
Baader, F., Bauer, A., & Lippmann, M. (2009). Runtime verification using temporal description
logic. Proc. 7th Intl Symp. Frontiers Combining Systems, FroCoS 2009, pp. 149164.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. Proc. 19th Intl Joint Conf.
Artificial Intelligence, IJCAI 2005, pp. 364369.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.). (2003).
Description Logic Handbook: Theory, Implementation Applications. Cambridge University Press, 2003.
Baader, F., Lutz, C., Milicic, M., Sattler, U., & Wolter, F. (2005). Integrating description logics
action formalisms: First results. Proc. 20th National Conf. Artificial Intelligence 17th
Conf. Innovative Applications Artificial Intelligence, pp. 572577.
Balduccini, M., & Gelfond, M. (2003). Logic programs consistency-restoring rules. Intl
Symp. Logical Formalization Commonsense Reasoning, AAAI 2003 Spring Symposium Series, pp. 918.
Belnap, N. (1977). useful four-valued logic. Modern Uses Multiple-Valued Logic, pp. 737.
Reidel Publishing Company, Boston.
Bertossi, L. E. (2011). Database Repairing Consistent Query Answering. Morgan & Claypool
Publishers, Ottawa, Canada.
510

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

Bertossi, L. E., Hunter, A., & Schaub, T. (2005). Introduction inconsistency tolerance. Inconsistency Tolerance [result Dagstuhl seminar], pp. 114.
Bienvenu, M. (2008). Complexity abduction EL family lightweight description logics.
Proc. 11th Intl Conf. Principles Knowledge Representation Reasoning, KR 2008,
pp. 220230.
Bienvenu, M., & Rosati, R. (2013). New inconsistency-tolerant semantics robust ontology-based
data access. Proc. 26th Intl Workshop Description Logics, pp. 5364.
Bonatti, P. A., Faella, M., & Sauro, L. (2010). EL default attributes overriding. Proceedings 9th Intl Semantic Web Conf., ISWC 2010, pp. 6479.
Brewka, G. (1989). Preferred subtheories: extended logical framework default reasoning.
Proc. 11th Intl Joint Conf. Artificial Intelligence, IJCAI 1989, pp. 10431048.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., & Rosati, R. (2007a).
Ontology-based database access. Proc. 15th Italian Symposium Advanced Database
Systems, SEBD 2007, pp. 324331.
Calvanese, D., De Giacomo, G., Lenzerini, M., Lembo, D., Poggi, A., & Rosati, R. (2007b).
MASTRO-I: efficient integration relational data DL ontologies. Proc. 20th
Intl Workshop Description Logics.
Calvanese, D., Ortiz, M., Simkus, M., & Stefanoni, G. (2013). Reasoning explanations
negative query answers DL-Lite. J. Artificial Intelligence Research, 48, 635669.
Console, L., Sapino, M. L., & Dupr, D. T. (1995). role abduction database view updating.
J. Intelligent Information Systems, 4(3), 261280.
Console, M., Mora, J., Rosati, R., Santarelli, V., & Savo, D. F. (2014). Effective computation
maximal sound approximations description logic ontologies. Proc. 13th Intl Semantic
Web Conf., ISWC 2014, Part II, pp. 164179.
dlliteplugin DLVHEX system (2015). https://github.com/hexhex/dlliteplugin.
Scripts DL-program benchmark generation (2015).
dlliteplugin/benchmarks.

https://github.com/hexhex/

dlplugin DLVHEX system (2015). https://github.com/hexhex/dlplugin.
DR E W reasoner DL-Programs Datalog-rewritable Description Logics (2012). http://
www.kr.tuwien.ac.at/research/systems/drew/.
Ecke, A., Ludwig, M., & Walther, D. (2013). concept difference EL-terminologies using
hypergraphs. Proc. Intl Workshop Document Changes: Modeling, Detection, Storage
Visualization.
Eiter, T., Erdem, E., Fink, M., & Senko, J. (2005). Updating action domain descriptions. Proc.
19th Intl Joint Conf. Artificial Intelligence, IJCAI 2005, pp. 418423.
Eiter, T., Fink, M., Greco, G., & Lembo, D. (2008). Repair localization query answering
inconsistent databases. ACM Transactions Database Systems, 33(2).
Eiter, T., Fink, M., Krennwallner, T., Redl, C., & Schller, P. (2014a). Efficient HEX-program
evaluation based unfounded sets. J. Artificial Intelligence Research, 49, 269321.
511

fiE ITER , F INK & TEPANOVA

Eiter, T., Fink, M., Redl, C., & Stepanova, D. (2014b). Exploiting support sets answer set
programs external evaluations. Proc. 28th Conf. Artificial Intelligence, AAAI 2014,
pp. 10411048.
Eiter, T., Fink, M., & Stepanova, D. (2013). Data repair inconsistent DL-programs. Proc. 23rd
Intl Joint Conf. Artificial Intelligence, IJCAI 2013, pp. 869876.
Eiter, T., Fink, M., & Stepanova, D. (2014c). Computing repairs inconsistent DL-programs
EL ontologies. Proc. 14th Joint European Conf. Logics Artificial Intelligence, JELIA
2014, pp. 426441.
Eiter, T., Fink, M., & Stepanova, D. (2014d). Towards practical deletion repair inconsistent
DL-programs. Proc. 21st European Conf. Artificial Intelligence, ECAI 2014, pp. 285290.
Eiter, T., Fink, M., & Stepanova, D. (2014d). Data repair inconsistent DL-programs. Tech. rep.
INFSYS RR-1843-15-03, Institut f. Informationssysteme, TU Wien, A-1040 Vienna, Austria.
Eiter, T., Gottlob, G., & Leone, N. (1997). Abduction logic programs: Semantics complexity. Theoretical Computer Science, 189(1-2), 129177.
Eiter, T., Ianni, G., Lukasiewicz, T., Schindlauer, R., & Tompits, H. (2008). Combining answer
set programming description logics Semantic Web. J. Artificial Intelligence,
172(12-13), 14951539.
Eiter, T., Ianni, G., Schindlauer, R., & Tompits, H. (2005). uniform integration higher-order
reasoning external evaluations answer-set programming. Proc. 19th Intl Joint Conf.
Artificial Intelligence, IJCAI 2005, pp. 9096.
Eiter, T., & Makino, K. (2007). computing abductive explanations propositional Horn
theory. J. ACM, 54(5).
Eiter, T., Schneider, P., imkus, M., & Xiao, G. (2014). Using OpenStreetMap data create benchmarks description logic reasoners. Proc. 2nd Intl Workshop OWL Reasoner Evaluation, ORE 2014, Vol. 1207, pp. 5157.
Experimental data inconsistent DL-programs (2015). http://www.kr.tuwien.ac.at/
staff/dasha/jair_el/benchmark_instances.zip.
Feng, S., Ludwig, M., & Walther, D. (2015). logical difference EL: terminologies
towards tboxes. Proc. 1st Intl Workshop Sem. Technologies, IWOST 2015, pp. 3141.
Fink, M. (2012). Paraconsistent hybrid theories. Proc. 13th Intl Conf. Principles Knowledge Representation Reasoning, KR 2012, pp. 141151.
Frhstck, M., Phrer, J., & Friedrich, G. (2013). Debugging answer-set programs ouroboros extending sealion plugin. Proc. 12th Intl Conf. Logic Programming Nonmonotonic
Reasoning, LPNMR 2013, pp. 323328.
Gallo, G., Longo, G., & Pallottino, S. (1993). Directed hypergraphs applications. Discrete
Applied Mathematics, 42(2), 177201.
Grdenfors, P., & Rott, H. (1995). Belief revision. Handbook Logic Artificial Intelligence
Logic Programming, 4, 35132.
Gardiner, T., Tsarkov, D., & Horrocks, I. (2006). Framework automated comparison
description logic reasoners. Proc. 5th Intl Semantic Web Conf., ISWC 2006, pp. 654667.
512

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

Gebser, M., Phrer, J., Schaub, T., & Tompits, H. (2008). meta-programming technique
debugging answer-set programs. Proc. 23rd Conf. Artificial Intelligence, AAAI 2008, pp.
448453.
Gelfond, M., & Lifschitz, V. (1991). Classical negation logic programs disjunctive databases.
New Generation Computing, 9, 365385.
Gottlob, G., Pichler, R., & Wei, F. (2007). Efficient datalog abduction bounded treewidth.
Proc. 22nd Intl Conf. Artificial Intelligence, AAAI 2007, pp. 16261631.
Grau, B. C., Horrocks, I., Kazakov, Y., & Sattler, U. (2007). right amount: extracting
modules ontologies. Proc. 16th Intl Conf. World Wide Web, WWW 2007, pp. 717
726.
Hansen, P., Lutz, C., Seylan, I., & Wolter, F. (2014). Query rewriting EL TBoxes: Efficient
algorithms. Proc. 27th Intl Workshop Description Logics, pp. 197208.
Hermann, M., & Pichler, R. (2010). Counting complexity propositional abduction. J. Computer
System Sciences, 76(7), 634649.
HTCondor load distribution system, version 7.8.7 (2012). http://research.cs.wisc.edu/
htcondor/.
Huang, S., Hao, J., & Luo, D. (2014). Incoherency problems combination description logics
rules. J. Applied Mathematics, 604753:16.
Huang, S., Li, Q., & Hitzler, P. (2013). Reasoning inconsistencies hybrid MKNF knowledge
bases. Logic J. IGPL, 21(2), 263290.
Kaminski, T., Knorr, M., & Leite, J. (2015). Efficient paraconsistent reasoning ontologies
rules. Proc. 24th Intl Joint Conf. Artificial Intelligence, IJCAI 2015, pp. 30983105.
Knorr, M., Alferes, J. J., & Hitzler, P. (2008). coherent well-founded model hybrid MKNF
knowledge bases. Proc. 18th European Conf. Artificial Intelligence, ECAI 2008, pp.
99103.
Knorr, M., Alferes, J. J., & Hitzler, P. (2011). Local closed world reasoning description logics
well-founded semantics. Artificial Intelligence, 175(9-10), 15281554.
Konev, B., Ludwig, M., Walther, D., & Wolter, F. (2012). logical difference lightweight
description logic EL. J. Artificial Intelligence Research, 44, 633708.
Kontchakov, R., Lutz, C., Toman, D., Wolter, F., & Zakharyaschev, M. (2010). combined
approach query answering DL-Lite. Proc. 12th Intl Conf. Principles Knowledge
Representation, KR 2010, pp. 247257.
Kotek, T., Simkus, M., Veith, H., & Zuleger, F. (2014). Towards description logic program
analysis: Extending ALCQIO reachability. Proc. 27th Intl Workshop Description
Logics, pp. 591594.
Lembo, D., Lenzerini, M., Rosati, R., Ruzzi, M., Savo, D. F. (2015). Inconsistency-tolerant query
answering ontology-based data access. J. Web Sem., 33, 329.
Lembo, D., Santarelli, V., & Savo, D. F. (2013). graph-based approach classifying OWL 2 QL
ontologies. Proc. 26th Intl Workshop Description Logics, pp. 747759.
LUBM benchmark (2005). http://swat.cse.lehigh.edu/projects/lubm/.
513

fiE ITER , F INK & TEPANOVA

LUBM data generator (2013). http://code.google.com/p/combo-obda/.
Ludwig, M., & Walther, D. (2014). logical difference ELHr-terminologies using hypergraphs. Proc. 21st European Conf. Artifical Intelligence, ECAI 2014, pp. 555560.
Lukasiewicz, T. (2010). novel combination answer set programming description logics
semantic web. IEEE Trans. Knowledge Data Engineering, 22(11), 15771592.
Lutz, C., Toman, D., & Wolter, F. (2009). Conjunctive query answering description logic EL
using relational database system. Boutilier, C. (Ed.), Proc. 21st Joint Intl Conf. Artificial
Intelligence, IJCAI 2009, pp. 20702075.
Lutz, C., Walther, D., & Wolter, F. (2007). Conservative extensions expressive description logics.
Proc. 20th Intl Joint Conf. Artificial Intelligence, IJCAI 2007, pp. 453458.
Lutz, C., & Wolter, F. (2010). Deciding inseparability conservative extensions description
logic EL. J. Symbolic Computation, 45(2), 194228.
Martinez, M. V., Molinaro, C., Subrahmanian, V. S., & Amgoud, L. (2013). General Framework
Reasoning Inconsistency. Springer Briefs Computer Science. Springer, 2013.
Masotti, G., Rosati, R., & Ruzzi, M. (2011). Practical abox cleaning DL-Lite (progress report).
Proc. Description Logics Workshop.
Motik, B., & Rosati, R. (2010). Reconciling Description Logics Rules. J. ACM, 57(5),
162.
MyITS - Personalized Intelligent Mobility Service (2012). http://www.kr.tuwien.ac.at/
research/projects/myits/GeoConceptsMyITS-v0.9-Lite.owl/.
Nguyen, N. T. (2008). Advanced Methods Inconsistent Knowledge Management. Advanced
Information Knowledge Processing. Springer.
Nortje, R., Britz, A., & Meyer, T. (2013). Module-theoretic properties reachability modules
SRIQ. Proc. 26th Intl Workshop Description Logics, pp. 868884.
Oetsch, J., Phrer, J., & Tompits, H. (2012). Stepwise debugging description-logic programs.
J. Correct Reasoning, pp. 492508.
Open Street Map project (2012). http://www.openstreetmap.org/.
zccep, . L., & Mller, R. (2012). Combining DL Lite spatial calculi feasible geothematic query answering. Proc. 25th Intl Workshop Description Logics.
Pan, J. Z., & Thomas, E. (2007). Approximating OWL-DL ontologies. Proc. 22nd Intl Conf.
Artificial Intelligence, AAAI 2007, pp. 14341439.
Prez-Urbina, H., Motik, B., & Horrocks, I. (2010). Tractable query answering rewriting
description logic constraints. J. Applied Logic, 8(2), 186209.
Phrer, J., Heymans, S., & Eiter, T. (2010). Dealing inconsistency combining ontologies
rules using DL-programs . Proc. 7th Extended Semantic Web Conf., ESWC 2010, part
I, pp. 183197.
R ACER P RO reasoner OWL ontologies (2007). http://franz.com/agraph/racer/.
Reiter, R. (1987). theory diagnosis first principles. J. Artificial Intelligence, 32(1), 5795.
514

fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES

Rosati, R. (2007). conjunctive query answering EL. proceedings 20th Intl Workshop
Description Logics.
Rosati, R., Ruzzi, M., Graziosi, M., & Masotti, G. (2012). Evaluation techniques inconsistency handling OWL 2 QL ontologies. Proc. 11th Intl Semantic Web Conf., ISWC 2012,
pp. 337349.
Sakama, C., & Inoue, K. (1995). Paraconsistent stable semantics extended disjunctive programs.
J. Logic Computation, 5(3), 265285.
Sakama, C., & Inoue, K. (2003). abductive framework computing knowledge base updates.
Theory Practice Logic Programming, 3(6), 671713.
Schulz, C., Satoh, K., & Toni, F. (2015). Characterising explaining inconsistency logic
programs. Proc. 13th Intl Conf., LPNMR 2015, pp. 467479.
Schulz, S., Cornet, R., & Spackman, K. A. (2011). Consolidating SNOMED CTs ontological
commitment. Applied Ontology, 6(1), 111.
Shen, Y.-D. (2011). Well-supported semantics description logic programs. Proc. 22nd Intl
Joint Conf. Artificial Intelligence, IJCAI 2011, pp. 10811086.
Stefanoni, G., Motik, B., & Horrocks, I. (2012). Small datalog query rewritings EL. Proc.
25th Intl Workshop Description Logics.
Stepanova, D. (2015). Inconsistencies Hybrid Knowledge Bases. PhD thesis, Vienna University
Technology.
Steve, G., Gangemi, A., & Mori, A. R. (1995). Modelling sharable medical concept system:
Ontological foundation galen. AIME, pp. 411412.
Stuckenschmidt, H., Parent, C., & Spaccapietra, S. (Eds.). (2009). Modular Ontologies: Concepts,
Theories Techniques Knowledge Modularization, Vol. 5445 Lecture Notes Computer Science. Springer.
Syrjnen, T. (2006). Debugging Inconsistent Answer-Set Programs. Proc. 11th Intl Workshop
Nonmonotonic Reasoning, NMR 2006, pp. 7783.
Thakur, M., & Tripathi, R. (2009). Linear connectivity problems directed hypergraphs. Theoretical Computer Science, 410(27-29), 25922618.
Tserendorj, T., Rudolph, S., Krtzsch, M., & Hitzler, P. (2008). Approximate OWL-reasoning
Screech. Proc. 2nd Intl Conf. Web Reasoning Rule Systems, RR 2008, pp. 165180.
Wache, H., Groot, P., & Stuckenschmidt, H. (2005). Scalable instance retrieval semantic web
approximation. Proc. 1st Intl Workshops Web Information Systems Engineering,
WISE 2005, pp. 245254.
Wang, Y., You, J.-H., Yuan, L.-Y., & Shen, Y.-D. (2010). Loop formulas description logic
programs. Theory Practice Logic Programming, 10(4-6), 531545.
Xiao, G. (2014). Inline Evaluation Hybrid Knowledge Bases. Ph.D. thesis, Vienna University
Technology, Austria.
Zhao, Y., Pan, J. Z., & Ren, Y. (2009). Implementing evaluating rule-based approach
querying regular EL+ ontologies. Proc. 9th Intl Conf. Hybrid Intelligent Systems,
2009, pp. 493498.

515

fiJournal Artificial Intelligence Research 56 (2016) 1-59

Submitted 06/15; published 05/16

Query Predicate Emptiness Ontology-Based Data Access
Franz Baader

FRANZ . BAADER @ TU - DRESDEN . DE

TU Dresden, Germany

Meghyn Bienvenu

MEGHYN @ LIRMM . FR

CNRS, Universite de Montpellier
& INRIA, France

Carsten Lutz

CLU @ UNI - BREMEN . DE

University Bremen, Germany

Frank Wolter

WOLTER @ LIVERPOOL . AC . UK

Department Computer Science
University Liverpool, UK

Abstract
ontology-based data access (OBDA), database querying enriched ontology
provides domain knowledge additional vocabulary query formulation. identify query
emptiness predicate emptiness two central reasoning services context. Query emptiness asks whether given query empty answer databases formulated given
vocabulary. Predicate emptiness defined analogously, quantifies universally queries
contain given predicate. paper, determine computational complexity query
emptiness predicate emptiness EL, DL-Lite, ALC-families description logics,
investigate connection ontology modules, perform practical case study evaluate
new reasoning services.

1. Introduction
recent years, paradigm ontology-based data access (OBDA) gained increased popularity. general idea add ontology database querying provide domain knowledge
enrich vocabulary available formulation queries. particularly
useful data queried highly incomplete multiple data sources diverging vocabularies integrated (Poggi, Lembo, Calvanese, De Giacomo, Lenzerini, & Rosati,
2008). OBDA taken particular verve area description logic (DL)
studied intensively lightweight DLs members DL-Lite EL
families, tractable regarding data complexity querying, expressive
DLs ALC SHIQ families querying intractable data complexity. use OBDA former, see example work Calvanese, De Giacomo,
Lembo, Lenzerini, Poggi, Rodriguez-Muro, Rosati (2009), Lutz, Toman, Wolter (2009),
Perez-Urbina, Motik, Horrocks (2009), Chortaras, Trivela, Stamou (2011) Eiter, Ortiz,
Simkus, Tran, Xiao (2012), surveys Krotzsch (2012) Kontchakov, RodriguezMuro, Zakharyaschev (2013); latter, see work Glimm, Lutz, Horrocks, Sattler
(2008), Ortiz, Calvanese, Eiter (2008), Bienvenu, ten Cate, Lutz, Wolter (2014) well
references given work Ortiz Simkus (2012).
c
2016
AI Access Foundation. rights reserved.

fiBAADER , B IENVENU , L UTZ , & W OLTER

OBDA approach fueled availability ontologies aim providing standard
vocabulary targeted application domain. particular, many ontologies
bio-medical domain SNOMED CT (IHTSDO, 2016), NCI (Golbeck, Fragoso, Hartel, Hendler, Oberthaler, & Parsia, 2003), GO (Gene Ontology Consortium, 2016),
formulated DL allow comparably inexpensive adoption OBDA bio-medical applications querying electronic medical records (Patel, Cimino, Dolby, Fokoue, Kalyanpur,
Kershenbaum, Ma, Schonberg, & Srinivas, 2007). Ontologies kind typically
broad coverage vocabulary often contain tens even hundreds thousands predicates
embrace various subject areas anatomy, diseases, medication, even social context
geographic location. given application, small fragment ontologys vocabulary actually occur data. Still, remaining predicates potentially useful
formulating queries linked data vocabulary ontologythis precisely
OBDA enriches vocabulary available query formulation.
Due size complexity involved ontologies vocabularies, however,
difficult understand additional predicates useful query formulation
write meaningful queries extended vocabulary. Static analysis tools analyzing
queries would thus useful. paper, consider fundamental static analysis problem
query emptiness well natural variation called predicate emptiness. former,
problem decide whether given query q provides empty answer databases
formulated given data vocabulary . Query emptiness thus helps identify queries
useless due wrong use ontology vocabulary. standard static analysis problem many
subareas database theory XML, see e.g. work Benedikt, Fan, Geerts (2008)
references therein.
example, consider following simple ontology O:
DiabetesPatient Patient u disease.Diabetes
DiabetesPatientwithoutMedication Patient u disease.Diabetes u
medication for.Diabetes
Assume used support querying medical patient database unary table
concept names Patient Diabetes binary tables role names disease
medication for, distinguishing particular diabetes type 1 type 2. example,
database could given following set assertions:
Patient(a),
disease(a, type1),

Patient(b),

medication for(a, type1),

Diabetes(type1),

disease(b, type2),

Diabetes(type2).

Thus, patient diabetes type 1 medication b patient
diabetes type 2. OBDA, queries interpreted open world assumption thus one
interested certain answers query q w.r.t. A, is, answers q
extension satisfy ontology agreement. concrete query
q1 (x) = DiabetesPatient(x),
b certain answers q1 (x) w.r.t. Adespite fact predicate DiabetesPatient used q1 (x) occur A. Since formulated using data vocabulary
= {Patient, disease, medication for, Diabetes},
2

fiQ UERY P REDICATE E MPTINESS

Q UERY EVALUATION

E MPTINESS

DL

IQ

CQ

IQ-query / CQ-predicate

CQ-query

EL

PT IME-c.

NP-c.

PT IME-c.

PT IME-c

EL

PT IME-c.

NP-c.

E XP IME-c.

E XP IME-c.

ELI

E XP IME-c.

E XP IME-c.

E XP IME-c.

E XP IME-c.

Horn-ALCIF

E XP IME-c.

E XP IME-c.

E XP IME-c.

E XP IME-c.

NL OG PACE-c.

NP-c.

NL OG PACE-c.

coNP-c.

PT IME-c.

NP-c.

coNP-c.

coNP-c.

ALC

E XP IME-c.

E XP IME-c.

NE XP IME-c.

NE XP IME-c.

ALCI

E XP IME-c.

2E XP IME-c.

NE XP IME-c.

2E XP IME-c.

ALCF

E XP IME-c.

E XP IME-c.

undecidable

undecidable

DL-Litecore|F |R
DL-Litehorn

Figure 1: Known complexity results query evaluation new complexity results emptiness

say q1 (x) non-empty given O. regard evidence q1 (x) potentially
useful query databases formulated vocabulary . another example, consider query
q2 (x) = DiabetesPatientwithoutMedication(x)
certain answer q2 (x) w.r.t. since, open world assumption, mere
absence information b medication diabetes type 1 type 2 imply
negation true. One even show that, whatever database A0 formulated vocabulary
use, never certain answer q2 (x) w.r.t. A0 . case, say
q2 (x) empty given O. contrast q1 (x), query thus useless -databases.
also consider predicate emptiness, problem decide whether given predicate p
data vocabulary , case queries q involve p yield empty answer
-databases. example above, predicate DiabetesPatientwithoutMedication empty
w.r.t. important class conjunctive queries (queries constructed atomic formulas using conjunction existential quantification). Predicate emptiness used identify
predicates ontology useless query formulation, even starting construct
concrete query. graphical user interface, example, predicates would offered
users query formulation. notion predicate emptiness loosely related predicate
emptiness datalog queries studied e.g. Vardi (1989) Levy (1993).
aim paper perform detailed study query emptiness predicate emptiness
various DLs including members EL, DL-Lite, ALC families, concentrating
two common query languages DL-based OBDA: instance queries (IQs) conjunctive
queries (CQs). resulting combinations DLs query languages, determine
(un)decidability exact computational complexity query emptiness predicate emptiness.
results summarized right side Figure 1 range PT IME basic members
EL DL-Lite families via NE XP IME basic members ALC family undecidable ALC extended functional roles (ALCF). adopt standard notion combined
3

fiBAADER , B IENVENU , L UTZ , & W OLTER

complexity, measured terms size whole input (TBox, data vocabulary,
query predicate symbol).
restricted data vocabulary quantification -databases
definition, query emptiness predicate emptiness reduce standard reasoning problems
query evaluation query containment. Formally, demonstrated undecidability result ALCF, contrasted decidability query entailment
containment DL, shown Calvanese, De Giacomo, Lenzerini (1998). emptiness decidable, complexity still often differs query evaluation. simplify
comparison, display Figure 1 known complexity results query evaluation considered DLs; please consult work Baader, Brandt, Lutz (2005, 2008), Krotzsch, Rudolph,
Hitzler (2007), Eiter, Gottlob, Ortiz, Simkus (2008) results concerning EL
Horn extensions, work Calvanese, De Giacomo, Lembo, Lenzerini, Rosati (2007)
Artale, Calvanese, Kontchakov, Zakharyaschev (2009) results DL-Lite, work
Tobies (2001), Hustadt, Motik, Sattler (2004), Lutz (2008), Ortiz, Simkus, Eiter
(2008) results DLs ALC family. comparing two sides Figure 1,
observe clear relationship complexity emptiness checking
complexity query evaluation. Indeed, problems often similar complexity,
several cases emptiness checking difficult corresponding query evaluation problem. also way around, complexities also incomparable. Note
extension EL EL bottom concept (used express class disjointness),
observe particularly significant difference tractability evaluating instance queries
E XP IME-completeness checking IQ-query emptiness.
key ingredient developing algorithms establishing upper complexity bounds emptiness show that, searching databases witness non-emptiness (such database
non-emptiness q1 given example), one often focus single
database constructed specifically purpose class databases easier handle
class databases. single database / class databases consider depends
DL question. reason, secondary theme paper analyze shape witness
databases. turns ALC extension ALCI inverse roles, consider
single exponential-size database whose construction reminiscent type elimination filtration
constructions known modal logic literature. EL extension ELI, may also
concentrate single witness candidate, much simpler one: consists facts
constructed using data vocabulary single constant. extensions EL,
use class databases witness candidates, namely tree forest structure.
DL-Lite, may restrict attention class databases whose size bounded polynomially
w.r.t. input query ontology.
demonstrate predicate emptiness useful reasoning service static analysis,
perform experiments using well-known large-scale medical ontology SNOMED CT coupled
real-world data vocabulary (corresponding terms obtained analyzing clinical notes
hospital) randomly generated vocabularies. real world vocabulary,
contains 8,858 370,000 concept names 16 62 role names SNOMED CT,
16,212 predicates turned non-empty IQs 17,339 non-empty CQs. Thus,
SNOMED CT provides substantial number additional predicates query formulation
large number predicates cannot meaningfully used queries -databases;
thus, identifying relevant predicates via predicate emptiness potentially helpful.
4

fiQ UERY P REDICATE E MPTINESS

also consider use query predicate emptiness extraction modules
ontology. Thus, instead using emptiness directly support query formulation, show
used simplify ontology. -substitute ontology subset ontology
gives certain answers conjunctive queries -databases. Replacing
large ontology (potentially quite small) -substitute supports comprehension ontology
thereby formulation meaningful queries. show that, ELI, one use predicate emptiness extract particularly natural -substitute ontology, called CQ -core,
containing exactly axioms original ontology contain predicates
non-empty -databases. Thus, predicates CQ -core ontology meaningfully used queries posed -databases. example, CQ -core ontology
O0 = {DiabetesPatient Patient u disease.Diabetes}.
second axiom removed CQ contains DiabetesPatientwithoutMedication
empty given O. analyze practical interest CQ -cores, carry case study
compute CQ -cores ontology SNOMED CT coupled various signatures,
showing tend drastically smaller original ontology also smaller
-modules, popular way extracting modules ontologies (Grau, Horrocks, Kazakov, &
Sattler, 2008).
article structured follows. begin Section 2 recalling syntax semantics description logics considered work. Section 3, introduce four notions
emptiness (IQ-query, IQ-predicate, CQ-predicate, CQ-query) investigate formal relationships them. first observe IQ-query IQ-predicate emptiness coincide
(so three problems consider) CQ-predicate emptiness corresponds CQquery emptiness CQs restricted simple form. also exhibit two polynomial
reductions predicate query emptiness: DLs considered paper except
DL-Lite family, CQ-predicate emptiness polynomially reducible IQ-query emptiness, Horn-DLs considered paper, IQ-query emptiness polynomially reducible
CQ-predicate emptiness.
Section 4, investigate computational complexity decidability query predicate emptiness ALC family expressive DLs. ALC ALCI, provide tight
complexity bounds, showing NE XP IME-completeness three emptiness problems ALC
IQ-query emptiness CQ-predicate emptiness ALCI, 2E XP IME-completeness
CQ-query emptiness ALCI. situation dramatically (and surprisingly) different
ALCF, emptiness problems proven undecidable. previously mentioned,
complexity upper bounds ALC ALCI rely characterization non-emptiness terms
special witness database. complexity lower bounds undecidability results proven
means reductions tiling problems.
Section 5, continue investigation query predicate emptiness considering
DL EL Horn extensions. plain EL, provide simple characterization nonemptiness terms maximal singleton database, allows us show three emptiness
problems decided polynomial time. Using characterization fact
standard reasoning ELI E XP IME-complete, obtain E XP IME-completeness emptiness
checking ELI. extensions EL allow contradictions, singleton database may
consistent ontology, requiring another approach. handle extensions, show
5

fiBAADER , B IENVENU , L UTZ , & W OLTER

sufficient consider tree-shaped databases witnesses non-emptiness, devise
decision procedure emptiness checking based upon tree automata. obtain manner
E XP IME upper bound Horn-ALCIF, sharply contrasts undecidability result
(non-Horn) ALCF. Interestingly, show matching E XP IME lower bound
considerably simpler DL EL , standard reasoning tasks tractable.
Section 6, turn attention DL-Lite family lightweight DLs,
commonly considered DLs ontology-based data access. show CQ-query emptiness
coNP-complete considered DL-Lite dialects. IQ-query emptiness CQ-predicate
emptiness, show complexity depends whether considered dialect allows conjunctions left-hand side axioms. standard dialects like DL-Litecore , DL-LiteR ,
DL-LiteF , allow conjunction, show IQ-query emptiness CQ-predicate
emptiness NL OG PACE-complete. dialects like DL-Litehorn admits conjunctions,
IQ-query emptiness CQ-predicate emptiness coNP-complete. difference complexity due fact dialects allowing conjunction, need consider witnesses nonemptiness polynomial size, whereas absence conjunction, sufficient
consider databases consist single assertion.
Section 7, apply query predicate emptiness extract modules ontology.
introduce notion -substitute CQ -core ontology show ELI
CQ -core ontology -substitute. also relate -substitutes notions module
proposed literature. particular, observe semantic syntactic -modules (Grau
et al., 2008) examples -substitutes, thus, algorithms computing modules
also used compute (possibly non-minimal) -substitutes. demonstrate potential
utility -substitutes emptiness checking experiments based SNOMED CT.
Finally, Sections 8 9, conclude paper discussing related future work. Please
note improve readability text, technical proofs deferred appendix.

2. Preliminaries
DLs, concepts inductively defined help set constructors, starting countably infinite sets NC concept names NR role names. constructors important paper summarized Figure 2. inverse role form r r role name
role role name inverse role. uniformity, define double inverse identity,
is, (r ) := r role names r. Throughout paper, use A, B denote concept
names, C, denote (possibly compound) concepts, r denote roles.
shall concerned variety different DLs well-known literature.
least expressive ones EL DL-Lite, logical underpinnings OWL2
profiles OWL2 EL OWL2 QL, respectively (Motik, Grau, Horrocks, Wu, Fokoue, & Lutz,
2009). EL, concepts constructed according following grammar using constructors
top concept, conjunction, existential restriction:
C,

::=

>

|



| C uD

|

r.C

ranging concept names r role names. DL-Lite concepts TBoxes
introduced Section 6. basic expressive DL consider paper ALC
extension EL constructors bottom concept, negation, disjunction value restriction:
C,

::=

>

|



|

| C uD
6

|

C tD

|

r.C

|

r.C

fiQ UERY P REDICATE E MPTINESS

Name

Syntax

concept name
role name


r

Semantics
AI
rI

top concept
bottom concept
negation
conjunction
disjunction
existential restriction
value restriction
role inverse

>

C
C uD
C tD
r.C
r.C
r

>I =
=
\ C
C DI
C DI
{d | e C (d, e) rI }
{d | e : (d, e) rI e C }
{(d, e) | (e, d) rI }

concept inclusion
concept assertion
role assertion

CvD
A(a)
r(a, b)

C DI
aI AI
(aI , bI ) rI

Figure 2: Syntax semantics DL constructors, TBox axioms, ABox assertions.

availability additional constructors indicated concatenation letters subscripts:
letter stands addition inverse roles (inside existential value restrictions, present)
subscript stands adding . gives, example, extension ALCI ALC
inverse roles, whose constructors exactly ones shown Figure 2. also defines
extension ELI EL inverse roles existential restrictions bottom concept.
concept inclusion (CI) DL L takes form C v D, C L-concepts.
use C abbreviation CIs C v v C. description logic, ontologies
formalized TBoxes. Given DLs L introduced above, L-TBox finite set
CIs L. use letter F write LF denote description logic TBoxes consist
CIs L, also functionality statements funct(r), r role name
inverse role (if inverse roles admitted L). example, ALCF thus extension ALC
TBoxes contain CIs ALC functionality statements role names. use
term axioms refer concept inclusions functionality statements uniform way.
addition DLs introduced above, also consider DLs impose restrictions
constructors used side concept inclusions. Horn-ALCI concept
inclusion (CI) form L v R, L R concepts defined syntax rules
R, R0 ::= > | | | | R u R0 | L R | r.R | r.R
L, L0 ::= > | | | L u L0 | L L0 | r.L
ranging concept names r (potentially inverse) roles. Horn-ALCIF-TBox
finite set Horn-ALCI CIs functionality statements funct(r). Note different
definitions Horn-DLs found work Hustadt, Motik, Sattler (2007), Eiter et al.
(2008), Kazakov (2009). original definition Hustadt, Motik, Sattler based
polarity rather technical, prefer (equivalent) definition.
size |T | TBox obtained taking sum lengths axioms,
length axiom number symbols needed write word.
7

fiBAADER , B IENVENU , L UTZ , & W OLTER

Databases represented using ABox, finite set concept assertions A(a)
role assertions r(a, b), a, b drawn countably infinite set NI individual names,
concept name, r role name. Note role assertions cannot use inverse roles.
shortcut, though, sometimes write r (a, b) r(b, a) A. use Ind(A) denote
set individual names used ABox A. knowledge base pair K = (T , A)
TBox ABox.
semantics description logics defined terms interpretation = (I , ).
domain non-empty set interpretation function maps concept name NC
subset AI , role name r NR binary relation rI , individual
name element aI . extension compound concepts inductively defined
shown third column Figure 2. interpretation satisfies (i) CI C v C DI ,
statement funct(r) rI functional, (iii) assertion A(a) aI AI , (vi) assertion
r(a, b) (aI , bI ) rI . Then, model TBox satisfies axioms , model
ABox satisfies assertions A. TBox satisfiable model ABox
satisfiable w.r.t. TBox common model. write |= C v
models satisfy CI C v D.
consider two types queries. First, instance queries (IQs) take form A(v),
concept name v individual variable taken set NV . Note instance queries
used query concept names, role names. traditional definition, due
fact role assertions implied ABox explicitly contained it,
thus querying trivial.1 general conjunctive queries (CQs) take form ~u (~v , ~u)
conjunction atoms form A(v) r(v, v 0 ) v, v 0 individual variables
~v ~u NV . Variables existentially quantified called answer variables,
arity q defined number answer variables. Queries arity 0 called Boolean.
use var(q), avar(q), qvar(q) denote set variables, answer variables, quantified
variables respectively query q. on, use IQ refer set IQs CQ
refer set CQs.
Let interpretation q (instance conjunctive) query q arity k answer
variables v1 , . . . , vk . match q mapping : var(q) (v) AI
A(v) q, ((v), (v 0 )) rI r(v, v 0 ) q, every answer variable v var(q),
individual name (v) = aI . write |= q[a1 , . . . , ak ] match
q (vi ) = aIi every 1 k. knowledge base (T , A), write
, |= q[a1 , . . . , ak ] |= q[a1 , . . . , ak ] models A. case, (a1 , . . . , ak )
certain answer q w.r.t. A. use certT ,A (q) denote set certain answers
q w.r.t. A. Note q Boolean query, () certT ,A (q)
match q every model , A, otherwise certT ,A (q) = . query evaluation problem
CQs DL L problem decide L-TBox , ABox A, CQ q arity k, tuple
~a Ind(A)k , whether ~a certT ,A (q).
use term predicate refer concept name role name signature refer
set predicates (in introduction, informally called signature vocabulary). sig(q)
denotes set predicates used query q, similarly sig(T ) sig(A) refer
signature TBox ABox A. -ABox ABox uses predicates
signature , likewise -concept.
1. longer true presence role hierarchy statements which, however, consider paper.

8

fiQ UERY P REDICATE E MPTINESS

context query answering DLs, sometimes useful adopt unique name
assumption (UNA), requires aI 6= bI interpretations a, b NI
6= b. results obtained paper depend UNA. following well-known
lemma shows UNA make difference ALCI (and fragments EL
ALC) certain answers queries change.
Lemma 1 Let ALCI-TBox, ABox, q CQ. certT ,A (q) identical
without UNA.
analogous statement fails ALCF, e.g. ABox = {f (a, b), f (a, b0 )} satisfiable w.r.t. TBox = {funct(r)} without UNA (and thus certT ,A (A(v)) = ),
unsatisfiable UNA (and thus certT ,A (A(v)) = Ind(A)).

3. Query Predicate Emptiness
introduce central notions reasoning problems studied paper, show
interrelated, make basic observations used throughout paper. following
definition introduces different notions emptiness studied paper.
Definition 2 Let TBox, signature, Q {IQ, CQ} query language. call
Q-query q empty given -ABoxes satisfiable w.r.t. ,
certT ,A (q) = .
predicate Q-empty given every Q-query q sig(q) empty
given .
follows, signatures used ABoxes called ABox signatures. quantify
ABoxes formulated ABox signature address typical database applications
data changes frequently, thus deciding emptiness based concrete ABox
much interest. example, assume ABoxes formulated signature
= {Person, hasDisease, DiseaseA, DiseaseB}
following, upper-case words concept names lower-case ones
role names. signature typically fixed application design phase, similar schema
design databases. TBox, take
= {Person v hasFather.(Person u Male), DiseaseA v InfectiousDisease}.
IQ InfectiousDisease(v) CQ v hasFather(u, v) non-empty given
despite using predicates cannot occur data, witnessed -ABoxes {DiseaseA(a)}
{Person(a)}, respectively. illustrates TBox enriches vocabulary
available query formulation. contrast, CQ
vv 0 (hasFather(u, v) hasDisease(v, v 0 ) InfectiousDisease(v 0 )),
uses predicates plus additional one ABox signature, empty
given .
9

fiBAADER , B IENVENU , L UTZ , & W OLTER

Regarding predicate emptiness, interesting observe choice query language
important. example, predicate Male IQ-empty given , CQ-empty
witnessed -ABox {Person(a)} CQ v Male(v). thus makes sense use Male
instance queries -ABoxes given , whereas meaningfully used conjunctive
queries.
every IQ also CQ, predicate CQ-empty must also IQ-empty. illustrated
example, converse hold. Also note role names IQ-empty
given since role name cannot occur instance query. contrast, hasFather clearly
CQ-empty example.
follows Lemma 1 that, ALCI fragments, query emptiness predicate emptiness oblivious whether UNA made, IQ CQ. established
following lemma, also true ALCIF despite fact certain answers queries
differ without UNA.
Lemma 3 Let ALCIF-TBox. CQ q empty given UNA iff
empty given without UNA.
proof Lemma 3 given appendix. direction left right one assumes
q non-empty given without UNA takes witness -ABox A. Using model
satisfying without UNA identifying a, b Ind(A) aI = bI one
define -ABox A0 shows q non-empty given UNA.
Conversely, one assumes q non-empty given UNA takes witness
-ABox A. One use show q non-empty given without UNA.
exception DL-Lite dialect (containing role inclusions) DLs considered
paper fragments ALCIF. Thus, free adopt UNA not. remainder
paper, choose whatever convenient, careful always point explicitly
whether UNA made not. DL-Lite dialect covered formulation Lemma 3
observe discussion DL-Lite even Lemma 1 holds free adopt
UNA case well.
also relevant note decision disallow individual names query atoms without
loss generality. Indeed, easily verified predicate emptiness whether
admit individuals queries not. Moreover, immediate reduction query emptiness
generalized CQs (which may contain individual names) query emptiness CQs defined
paper: suffices replace every individual query fresh answer variable xa ,
test whether resulting query (without individuals) empty given .
Definition 2 gives rise four natural decision problems.
Definition 4 Let Q {IQ, CQ}.
Q-query emptiness problem deciding, given TBox , signature , Qquery q, whether q empty given ;
Q-predicate emptiness means decide, given TBox , signature , predicate S,
whether Q-empty given .
10

fiQ UERY P REDICATE E MPTINESS

IQ-query = IQ-predicate
emptiness
emptiness
Trivial

CQ-query
emptiness

Theorem 7
(materializable DLs)

Lemma 5

Theorem 6

CQ-predicate
emptiness

Figure 3: Polytime reductions emptiness notions.
Clearly, four problems intimately related. particular, IQ-query emptiness IQpredicate emptiness effectively problem since instance query consists
single predicate. reason, disregard IQ-predicate emptiness
speak IQ-query emptiness. CQ case, things different. Indeed, following lemma
shows CQ-predicate emptiness corresponds CQ-query emptiness CQs restricted
simple form. easy consequence fact that, since composite concepts queries
disallowed, CQs purely positive, existential, conjunctive.
Lemma 5 NC (resp. r NR ) CQ-predicate empty given iff conjunctive query
v A(v) (resp. vv 0 r(v, v 0 )) empty given .
Lemma 5 allows us consider queries form v A(v) vv 0 r(v, v 0 ) dealing
CQ-predicate emptiness. on, without notice.
Trivially, IQ-query emptiness special case CQ-query emptiness. following observation less obvious applies DLs considered paper except DL-Lite
family.
Theorem 6 DL contained ALCIF admits CIs r.B v B r.> v B B
concept name, CQ-predicate emptiness polynomially reduced IQ-query emptiness.
Proof. Let TBox, signature, B concept name occur ,
role name occur . prove
1. CQ-predicate empty given iff IQ B(v) empty {s} given TBox
0 = TB {A v B}, TB = {r.B v B | r = r occurs };
2. r CQ-predicate empty given iff IQ B(v) empty {s} given TBox
0 = TB {r.> v B}, TB above.
proofs Points 1 2 similar, concentrate Point 1. First suppose CQpredicate non-empty given . -ABox , |= v A(v). Choose
a0 Ind(A) set A0 := {s(a0 , b) | b Ind(A)}. Using fact , |= v A(v)
definition A0 0 , shown 0 , A0 |= B(a0 ). converse direction,
suppose B IQ-query non-empty {s} given 0 . {s}-ABox A0
0 , A0 |= B(a) Ind(A0 ). Let obtained A0 removing assertions
s(a, b). Using fact 0 , A0 |= B(a) definition A0 0 , shown
, |= v A(v).


11

fiBAADER , B IENVENU , L UTZ , & W OLTER

Figure 3 gives overview available polytime reductions four (rather: three)
problems. terms computational complexity, CQ-query emptiness thus (potentially) hardest problem, CQ-predicate emptiness simplest. precisely, CQ-query emptiness
DL L belongs complexity class C (larger equal PT IME), IQ-query emptiness
CQ-predicate emptiness L also C. Moreover, DLs L satisfying conditions
Theorem 6, C-hardness CQ-predicate emptiness L implies C-hardness CQ-query emptiness
IQ-query emptiness L.
certain conditions, also prove converse Theorem 6. Following work
Lutz Wolter (2012), call model TBox ABox materialization
every CQ q arity k tuple ~a Ind(A)k , |= q[~a] iff , |= q[~a]. DL L
called materializable every ABox satisfiable w.r.t. exists materialization
A. Typical DL-Lite dialects, DL EL, Horn-extensions EL ELIF
materializable (Lutz & Wolter, 2012).
Theorem 7 Let L materializable DL admits CIs form A1 u A2 v A3 ,
A1 , A2 , A3 NC . Then, L, IQ-query emptiness polynomially reduced CQ-predicate
emptiness.
Proof. claim IQ A(v) empty given iff B CQ-empty {X} given
TBox 0 = {A u X v B}, B X concept names occur .
direction, assume A(v) IQ non-empty given , let ABox , |= A(a) Ind(A). Set A0 := {X(a)}. easy see
0 , A0 |= v B(v) thus B CQ-predicate non-empty {X} given 0 .
direction, assume B CQ non-empty {X} given 0 , let A0
{X}-ABox satisfiable 0 0 , A0 |= v B(v). may assume
X(a) A0 Ind(A0 ) adding assertions neither result unsatisfiability
w.r.t. 0 invalidate 0 , A0 |= v B(v). assumption materializability, exists
materialization 0 0 A0 0 |= v B(v). definition 0 , may assume
0
0
0
0
X = Ind(A0 ) B = AI X (if case, take modified version,
00
00
0
0
00
0
00 , 0 defined setting X := Ind(A0 ), B := AI X , :=
0
remaining concept role names ; 00 still satisfies 0 A0 since X Ind(A0 )
00
0
u X v B inclusion containing X B still materialization since
concept role names ). 0 |= v B(v) implies Ind(A0 )
0 |= B(a). Since 0 materialization 0 A0 , implies 0 , A0 |= B(a). definition
0 , implies , |= A(a), obtained A0 dropping assertions form
X(b). Since -ABox satisfiable w.r.t. (since A0 satisfiable w.r.t. 0 ), witnesses
A(v) non-empty given .

final observation section, note deciding query predicate emptiness
essentially ABox satisfiability whenever contains symbols used TBox.
described reductions, suffices consider CQ-query emptiness. CQ q = ~u (~v , ~u)
associate every individual variable v q individual name av set
Aq = {A(av ) | A(v) conjunct } {r(av , av0 ) | r(v, v 0 ) conjunct }.
Theorem 8 Let ALCIF-TBox, signature sig(T ) , q CQ. q
empty given iff sig(q) 6 Aq unsatisfiable w.r.t. .
12

fiQ UERY P REDICATE E MPTINESS

Proof. (If) Assume q non-empty given . -ABox
satisfiable w.r.t. certT ,A (q) 6= . clearly implies sig(q) since otherwise
predicate sig(q) \ find model predicate
interpreted empty set, would mean certT ,A (q) = . thus remains show Aq
satisfiable w.r.t. . end, let model A. Since certT ,A (q) 6= , exists
match q I. Modify setting aIv = (v) variables v used q. readily checked
modified model Aq , thus Aq satisfiable w.r.t. required.
(Only if) Assume sig(q) Aq satisfiable w.r.t. . sig(Aq ) . Since
clearly certT ,Aq (q) 6= , means q non-empty given .


4. Expressive Description Logics
consider query predicate emptiness ALC family expressive DLs, establishing tight
complexity results ALC ALCI, undecidability ALCF. start upper bound
proofs, showing IQ-query emptiness CQ-predicate emptiness ALCI NE XP IME, CQ-query emptiness ALC. Moreover, establish CQ-query emptiness
2E XP IME. move corresponding lower bound proofs also establish
undecidability considered emptiness problems ALCF.
4.1 Upper Bounds
first main step proofs show that, deciding emptiness problems ALC
ALCI, suffices consider single special -ABox. Specifically, show construct
given satisfiable TBox ABox signature canonical -ABox ,
every CQ q, certT ,AT , (q) 6= exists -ABox satisfiable
w.r.t. certT ,A (q) 6= . prove NE XP IME upper bounds IQ-query emptiness ALCI computing , (in exponential time) guessing model , (of
exponential size |T | ) falsifies query; 2E XP IME upper bound CQ-query
emptiness ALCI obtained even simpler way computing , checking
whether certT ,AT , (q) = using known algorithms. Significantly work required obtain
NE XP IME upper bound CQ-query emptiness ALC. construct , , need
exercise lot care check whether certT ,AT , (q) = without leaving NE XP IME.
Let satisfiable ALCI-TBox ABox signature. define canonical -ABox
, introduce well-known notion types (or Hintikka sets) (Pratt, 1979; Kaminski,
Schneider, & Smolka, 2011). closure cl(T , ) smallest set contains
NC well concepts occur (potentially subconcept) closed
single negations. type set cl(T , ) model
, = tI (d), tI (d) = {C cl(T , ) | C } type
realized I. Let TT , denote set types . role name r
t, t0 TT , , say pair (t, t0 ) r-coherent write ;r t0
C t0 whenever r.C t,
C whenever r .C t0 .
seen implies also corresponding conditions existential restrictions,
C t0 r.C cl(C, ) implies r.C t.
13

fiBAADER , B IENVENU , L UTZ , & W OLTER

Definition 9 (Canonical -ABox) Let satisfiable ALCI-TBox ABox signature.
Fix (distinct) individual name TT , . canonical -ABox , defined
follows:
, = {A(at ) | TT , , NC }
{r(at , at0 ) | ;r t0 t, t0 TT , , r NR }.
cardinality TT , exponential size cardinality ,
set TT , computed exponential time making use well-known E XP IME procedures
concept satisfiability w.r.t. TBoxes ALCI (Gabbay, Kurucz, Wolter, & Zakharyaschev, 2003,
p. 72) Thus, , exponential size computed exponential time. interesting
note ABox , finitary version canonical model basic modal logic
essentially identical model constructed Pratts type elimination procedure (Pratt, 1979);
fact, exactly identical sig(T ). show , satisfiable w.r.t. .
Lemma 10 Let satisfiable ALCI-TBox ABox signature. , satisfiable
w.r.t. .
Proof. Let interpretation , defined setting
,

= TT ,

AIT ,

= {t TT , | t}

rIT ,

= {(t, t0 ) TT , TT , | ;r t0 }

NC r NR . One prove induction structure C C
cl(T , ), C iff C , . definition types, C v C imply t.
Thus, , model . immediate consequence definition , ,
also model , ; fact, , regarded reduct , signature .

crucial tool analyzing properties canonical ABoxes, introduce homomorphism
ABoxes. Let A0 ABoxes. ABox homomorphism A0 total
function h : Ind(A) Ind(A0 ) following conditions satisfied:
A(a) implies A(h(a)) A0 ;
r(a, b) implies r(h(a), h(b)) A0 .
next lemma identifies central property ABox homomorphisms regarding query answering.
Lemma 11 ALCI-TBox, q CQ , |= q[a1 , . . . , ], h ABox
homomorphism A0 , , A0 |= q[h(a1 ), . . . , h(an )].
Proof. prove contrapositive. Thus assume , A0 6|= q[h(a1 ), . . . , h(an )].
model 0 A0 0 6|= q[h(a1 ), . . . , h(an )]. Define model starting 0
0
reinterpreting individual names Ind(A) setting aI = h(a)I Ind(A). Since
individual names occur , model . also model A: A(a) A,
A(h(a)) A0 definition ABox homomorphisms. Since 0 model A0 definition
I, follows aI AI . case r(a, b) analogous. Finally, 0 6|= q[h(a1 ), . . . , h(an )]
definition yield 6|= q[a1 , . . . , ]. thus shown , 6|= q[a1 , . . . , ].

14

fiQ UERY P REDICATE E MPTINESS

following lemma characterizes satisfiability -ABoxes w.r.t. existence ABox
homomorphism , .
Lemma 12 Let satisfiable ALCI-TBox ABox signature. -ABox satisfiable
w.r.t. iff ABox homomorphism , .
Proof. Assume satisfiable w.r.t. . Let model A. Define homomorphism h
, setting h(a) = , type realized aI I. Using
definition , , one see h indeed ABox homomorphism. Conversely, let h
ABox homomorphism , . Lemma 10, , satisfiable w.r.t. . proof
Lemma 11 shows one construct model model , using
homomorphism h. Thus satisfiable w.r.t. .

ready prove main property , regarding emptiness, discussed
beginning section.
Theorem 13 Let satisfiable ALCI-TBox ABox signature. CQ q empty
given iff certT ,AT , (q) = .
Proof. direction follows directly fact , satisfiable w.r.t. (by
Lemma 10). direction, let certT ,AT , (q) = . show q empty given ,
take -ABox satisfiable w.r.t. . Lemmas 11 12, certT ,AT , (q) = implies
certT ,A (q) = , required.

employ Theorem 13 prove NE XP IME upper bounds IQ-query emptiness.
Theorem 14 ALCI, IQ-query emptiness NE XP IME.
Proof. Let satisfiable TBox, ABox signature, A(v) IQ emptiness
given decided. employ following:
Fact. ABox A, , 6|= A(a), exists model aI 6 AI
|I | |Ind(A)| + 2|T | .
Proof Fact. , 6|= A(a), exists model J aJ 6 AJ .
may assume {aJ | Ind(A)} disjoint domain TT ,0 interpretation ,0
defined proof Lemma 10 (where assume 0 := ). define union
restriction J {aJ | Ind(A)} interpretation ,0 expanded adding rI
pairs
(aJ , t) tJ (aJ ) ;r t, Ind(A), TT ,0 ;
(t, aJ ) ;r tJ (aJ ), Ind(A), TT ,0 .
model aI 6 AI required size. finishes proof fact.
NE XP IME algorithm computes canonical ABox , (in exponential time) guesses
every Ind(AT , ) model Ia |Ia | |Ind(AT , )| + 2|T | . algorithm returns yes
Ind(AT , ):
1. Ia model , ,
15

fiBAADER , B IENVENU , L UTZ , & W OLTER

2. aIa 6 AIa .
conditions checked exponential time. Thus, Theorem 13 fact above,
algorithm returns yes iff A(v) empty given .

Note Theorem 6 CQ-predicate emptiness ALCI NE XP IME well. CQ-query
emptiness ALCI, easily derive 2E XP IME upper bound using , results
work Calvanese et al. (1998) complexity query answering DLs.
Theorem 15 ALCI, CQ-query emptiness 2E XP IME.
Proof. 2E XP IME algorithm obtained first computing canonical ABox ,
certT ,AT , (q), checking whether latter empty. done 2E XP IME since
shown work Calvanese et al. (1998) , A, q ALCI-TBox,
p(n)
set certT ,A (q) computed time 2p(m)2
p polynomial, size A,
n size q.

provide improved NE XP IME upper bound CQ-query emptiness ALC,
allow us show ALC three emptiness problems complexity.
Theorem 16 ALC, CQ-query emptiness NE XP IME.
proof somewhat technical reuses machinery fork rewritings spoilers introduced
Lutz (2008), proves combined complexity CQ-answering DL SHQ
E XP IME. concretely, show one decide emptiness CQ q ABox
signature given ALC-TBox guessing extension AeT , canonical ABox ,
assertions prevent possible match q checking AeT , satisfiable w.r.t.
. example, q A(x), obviously suffices add A(a) every individual ,
(we allow also complex concepts used ABox). general case requires careful
analysis assertions considered additions, mentioned
fork rewritings spoilers enter picture. fact, used prove that, since
inverse roles TBox, suffices consider extensions , contain additional
individual names additional assertions taken candidate set whose size
polynomial size , q. remains show satisfiability (T , AeT , )
decided (non-deterministically) time single exponential size q. Full details
given appendix.
4.2 Lower Bounds Undecidability
prove matching lower bounds upper complexity bounds presented show undecidability IQ-query emptiness, CQ-predicate emptiness, CQ-query emptiness ALCF.
undecidability proof NE XP IME-lower bound proof reduction two different
tiling problems, first asks tiling finite rectangle (unbounded) size
second asks tiling 2n 2n -square. 2E XP IME lower bound CQ-query emptiness ALCI straightforward reduction query entailment ALCI. begin
NE XP IME lower bound.
Theorem 17 ALC, CQ-predicate emptiness NE XP IME-hard.
16

fiQ UERY P REDICATE E MPTINESS

Proof. proof reduction NE XP IME-hard 2n 2n -tiling problem. instance
tiling problem given natural number n > 0 (coded unary) triple (T, H, V )
non-empty, finite set tile types including initial tile Tinit placed lower left
corner, H horizontal matching relation, V vertical matching relation.
tiling (T, H, V ) map f : {0, . . . , 2n 1} {0, . . . , 2n 1} f (0, 0) = Tinit ,
(f (i, j), f (i + 1, j)) H < 2n 1, (f (i, j), f (i, j + 1)) V j < 2n 1.
NE XP IME-complete decide whether instance 2n 2n -tiling problem tiling.
reduction, let n > 0 (T, H, V ) instance 2n 2n -tiling problem
= {T1 , . . . , Tp }. construct signature TBox ALC (T, H, V )
solution selected concept name CQ-predicate empty given .
proof, convenient impose UNA.
formulating reduction TBox, use role names x represent 2n 2n grid two binary counters X counting 0 2n 1. counters use concept
names X0 , . . . , Xn1 Y0 , . . . , Yn1 bits, respectively. contains following wellknown inclusions stating value counter X0 , . . . , Xn1 incremented going
x-successors value counter Y0 , . . . , Yn1 incremented going y-successors:
k = 1, . . . , n 1,

u

0j<k





0j<k

Xj v (Xk x.Xk ) u (Xk x.Xk )

Xj v (Xk x.Xk ) u (Xk x.Xk )

similarly Y0 , . . . , Yn1 y. also states value counter X change
going y-successors value counter change going xsuccessors: = 0, . . . , n 1,
Xi v y.Xi ,

Xi v y.Xi

Yi v x.Yi ,

Yi v x.Yi .


addition, states counter X 2n 1, x-successor
counter 2n 1, y-successor:
X0 u u Xn1 v x.,

Y0 u u Yn1 v y..

states Tinit holds (0, 0) tiling complete:
X0 u u Xn1 u Y0 u u Yn1 v Tinit ,
states tiling condition violated, true:
0 < j p: Ti u Tj v A,
0 i, j p (Ti , Tj ) 6 H: Ti u x.Tj v A,
0 i, j p (Ti , Tj ) 6 V : Ti u y.Tj v A.
17

>v



1ip

Ti ,

fiBAADER , B IENVENU , L UTZ , & W OLTER

Finally, since cannot use negation ABoxes, states concept names X 0 , . . . , X n1
0 , . . . , n1 equivalent X0 , . . . , Xn1 Y0 , . . . , Yn1 , respectively: =
1, . . . , n 1:
Xi v X , Xi v X , Yi v , Yi v X .
set = {x, y, X0 , . . . , Xn1 , Y0 , . . . , Yn1 , X 0 , . . . , X n1 , 0 , . . . , n1 } show
Claim. (T, H, V ) 2n 2n -tiling iff exists -ABox satisfiable w.r.t.
, |= v A(v).
Proof claim. Assume first (T, H, V ) 2n 2n -tiling. construct A, regard
pairs (i, j) < 2n j < 2n individual names let x((i, j), (i + 1, j))
< 2n 1 y((i, j), (i, j + 1)) j < 2n 1. also set Xk (i, j) kth bit
1, X k (i, j) kth bit 0, Yk (i, j) kth bit j 1, k (i, j)
kth bit j 0. readily checked satisfiable w.r.t. , |= v A(v).
Conversely, assume (T, H, V ) 2n 2n -tiling given f : {0, . . . , 2n 1}
{0, . . . , 2n 1} T. Let -ABox satisfiable w.r.t. . show , 6|= v A(v).
Let model A. AI = , done. Otherwise re-define interpretation
T1 , . . . , Tp follows. Associate every uniquely determined pair (id , jd )
given values counters X I. set TkI iff f (id , jd ) = Tk let
AI = . readily checked resulting interpretation still model A.

follows preceding result IQ-query emptiness CQ-query emptiness
ALC ALCI NE XP IME-hard. CQ-query emptiness ALCI, easily derive
2E XP IME lower bound results complexity query entailment ALCI.
Theorem 18 ALCI, CQ-query emptiness 2E XP IME-hard.
Proof. shown Lutz (2008) CQ entailment ALCI 2E XP IME-hard already
ABoxes form {A(a)} Boolean CQs. clearly strengthened empty
ABoxes: replace A(a) empty ABox compensate adding TBox > v
r.A r fresh role name. thus remains observe Boolean CQ q entailed
empty ABox iff q non-empty = .

show simple addition functional roles ALC leads undecidability CQpredicate emptiness, thus also IQ-query emptiness CQ-query emptiness. proof
reduction tiling problem asks tiling rectangle finite size (which neither
fixed bounded). reduction involves couple technical tricks using concept
names universally quantified second-order variables. allows us enforce
grid structure using standard frame axioms modal logic (which second-order nature).
reduction requires role names functional inverse functional. Since inverse
functionality cannot expressed ALCF, also use modal logic frame axiom enforce
different, (forwards) functional role name interpreted inverse role name
interested in. course, undecidability carries variants ALCF use concept
constructor ( 1 r) instead functional roles additional sort, DLs qualified
unqualified number restrictions.
Theorem 19 ALCF, CQ-predicate emptiness undecidable.
18

fiQ UERY P REDICATE E MPTINESS

instance aforementioned tiling problem given triple (T, H, V ) non-empty,
finite set tile types including initial tile Tinit placed lower left corner final tile
Tfinal placed upper right corner, H horizontal matching relation, V
TT vertical matching relation. tiling (T, H, V ) map f : {0, . . . , n}{0, . . . , m}
n, 0, f (0, 0) = Tinit , f (n, m) = Tfinal , (f (i, j), f (i + 1, j)) H < n,
(f (i, j), f (i, j + 1)) v < m. undecidable whether instance tiling problem
tiling.
reduction, let (T, H, V ) instance tiling problem = {T1 , . . . , Tp }.
construct signature TBox (T, H, V ) solution selected
concept name CQ-predicate non-empty given .
ABox signature = {T1 , . . . , Tp , x, y, x , } T1 , . . . , Tp used concept
names, x, y, x , functional role names. use role names x represent horizontal vertical adjacency points rectangle, role names x
simulate inverses x y. , use additional auxiliary concept names. particular U
R mark upper right border rectangle, Zc,1 , Zc,2 , Zx,1 , Zx,2 , Zy,1 , Zy,2 serve
second-order variables, C serves flag indicates grid cells closed position
set, Ix Iy similar flags intended behavior role names x, x
y, . concept name propagated grid upper right corner lower
left one, ensuring flags set everywhere, every position grid labeled
least one tile type, horizontal vertical matching conditions satisfied.
lower left corner grid reached, set flag, query v A(v) asks
for.
TBox defined set following CIs, (Ti , Tj , T` ) range triples
(Ti , Tj ) H (Ti , T` ) V , e {c, x, y}, ranges
Boolean combinations concept names Ze,1 Ze,2 , i.e., concepts L1 u L2 Li
Ze,i Ze,i :
Tfinal v u U u R
x.(U u u Tj ) u Ix u Ti v U u
y.(R u u T` ) u Iy u Ti v R u
x.(Tj u u y.Y ) u y.(T` u u x.Y ) u Ix u Iy u C u Ti v
u Tinit v
Bx u x.x .Bx v Ix
u y.y .By v Iy
x.y.Bc u y.x.Bc v C
U v y. R v x. U v x.U

R v y.R



1s<tp

Ts u Tt v

CIs Ix Iy responsible enforcing x inverse x inverse
y, least ABox individuals interested in. fact, ABox contains
assertions x(a, b) x (b, c) thus violates intended interpretation x x ,
interpret Zx,1 Zx,2 left-hand sides possible instantiations CI
Ix violated, e.g. making Zx,1 Zx,2 true a, false c. ABox contains
x(a, b), x (b, a), possible. Since x functional, thus enforce x
19

fiBAADER , B IENVENU , L UTZ , & W OLTER

inverse functional. CIs C achieve similar way closing grid cells, i.e.,
x-y-successor y-x-successor every relevant ABox individual coincide. However,
seen proofs, works x inverse functional.
establish Theorem 19, suffices prove following lemma (see appendix details).
Lemma 20 (T, H, V ) admits tiling iff -ABox satisfiable
, |= v A(v).

5. EL Horn Extensions
study query predicate emptiness DL EL several Horn extensions. First,
show that, plain EL, three emptiness problems decided polynomial time. reason
case, exponential-size canonical ABox , Section 4 replaced
total -ABox contains single individual instance -predicates.
Note satisfiable w.r.t. EL-TBox EL cannot express unsatisfiability.
approach works ELI, case one obtains E XP IME upper bound
optimal since subsumption ELI already E XP IME-hard (Baader et al., 2005, 2008).
soon unsatisfiability expressed, situation changes drastically. fact, show
even EL subsumption standard reasoning tasks still tractable, (all versions
of) emptiness E XP IME-hard. Nevertheless, emptiness Horn extensions EL turns
easier emptiness expressive DLs. contrast undecidability result ALCF
NE XP IME-hardness result ALC, emptiness E XP IME even Horn-ALCIF.
reason unraveling tolerance Horn description logics observed work Lutz
Wolter (2012), implies looking ABoxes witness non-emptiness,
restrict tree-shaped ones. enables use automata-theoretic techniques
decide emptiness.
5.1 EL ELI
begin showing EL, CQ-query emptiness, CQ-predicate emptiness, IQ-query
emptiness PT IME. proofs transparent simple. signature , total
-ABox := {A(a ) | } {r(a , ) | r }.
Lemma 21 Let EL-TBox signature. CQ q empty given iff
certT ,A (q) = .
Proof. proof simplified version proof Theorem 13. (contrapositive the)
direction follows fact satisfiable w.r.t. . direction, let
certT ,A (q) = . show q empty given , take -ABox A. Define ABox
homomorphism setting h(a) := Ind(A ). Lemmas 11 12,
certT ,A (q) = implies certT ,A (q) = , required.

Lemma 21 provides polytime reduction CQ-query emptiness (and, therefore, IQ-query
emptiness CQ-predicate emptiness) query evaluation problem CQs .
appendix, show due simple shape , checking whether certT ,A (q) =
done polynomial time. fact, give polytime procedure either returns certT ,A (q) =
succeeds constructing Boolean forest-shaped query qb empty given iff q is.
20

fiQ UERY P REDICATE E MPTINESS

construction relies fact that, immediate consequence results proved Lutz
Wolter (2010), emptiness q given implies existence model
certT ,A (q) = shape tree extended reflexive loops root.
Checking , 6|= qb requires answer concept queries extension ELu EL
universal role, possible PT IME (Lutz & Wolter, 2010). obtain following
result.
Theorem 22 EL, IQ-query emptiness CQ-query emptiness decided PT IME.
matching PT IME lower bound Theorem 22 shown reduction subsumption
EL, PT IME-hard (Haase, 2007). Consider EL-TBox EL-concepts C D.
CI C v follows if, if, IQ B(v) non-empty signature {A}
given TBox {A v C, v B}, A, B concept names appear C,
. Thus, obtain
Theorem 23 EL, IQ-query emptiness CQ-query emptiness PT IME-hard.
Observe Lemma 7 materializability EL obtain CQ-predicate emptiness
PT IME-complete well EL.
Note need little proof Lemma 21 go through: suffices total
-ABox satisfiable every TBox. thus reduce emptiness query answering
total -ABox extension EL unable express contradictions. another
important example, consider ELI. Since CQ evaluation ELI E XP IME-complete,
obtain E XP IME upper bound case. matching lower bound obtained
E XP IME-hardness subsumption ELI simple reduction subsumption IQ-query
emptiness given above.
Theorem 24 ELI, IQ-query emptiness CQ-query emptiness E XP IME-complete.
follows Lemma 7 materializability ELI CQ-predicate emptiness E XP IMEcomplete ELI.
5.2 Horn Extensions Involving Negation Functionality
simplest extension EL express unsatisfiability EL . begin showing
IQ-emptiness EL E XP IME-hard, thus significantly harder subsumption instance checking (both decided polynomial time). end, first show
decide IQ-query emptiness EL sufficient consider emptiness w.r.t. directed treeshaped ABoxes, ABox called directed tree-shaped following conditions hold:
1. directed graph GdA = (Ind(A), {(a, b) | r(a, b) A}) tree;
2. a, b Ind(A), one role name r r(a, b) r(b, a)
(and one case).
Proposition 25 instance query B(v) non-empty signature given EL -TBox iff
exists directed tree-shaped -ABox satisfiable w.r.t. , |= B(a)
root A.
21

fiBAADER , B IENVENU , L UTZ , & W OLTER

Proof. provide sketch since result also follows general Proposition 30 proved below. Assume B(v) non-empty given . find -ABox
satisfiable w.r.t. , |= B(a). Let potentially infinite ABox obtained
unfolding follows: individuals words a0 r0 rn1 a0 =
ri (ai , ai+1 ) 0 < n; include A(a0 r0 rn1 ) iff A(an )
include r(a0 r0 , a0 r0 rn an+1 ) rn (an , an+1 ) A. One show satisfiable w.r.t. since is, , |= B(a) iff , |= B(a). compactness first-order
consequence, obtain finite ABox A0 , A0 |= B(a). A0 required.

Theorem 26 EL , IQ-query emptiness E XP IME-hard.
Proof. Let , , B(v) given. Proposition 25, B(v) non-empty given iff
exists directed tree-shaped -ABox witness non-emptiness B(v)
given . Directed tree-shaped -ABoxes viewed EL-concepts using symbols
only, vice versa. Thus, witness -ABox exists iff exists EL-concept C using
symbols C satisfiable w.r.t. |= C v B. following
established carefully analyzing reduction underlying Theorem 36 work Lutz
Wolter (2010): given EL -TBox , signature , concept name B, E XP IME-hard
decide exists EL-concept C using symbols C satisfiable
w.r.t. |= C v B. establishes E XP IME-hardness non-emptiness. Using fact
E XP IME = coE XP IME, hardness result transfers IQ-query emptiness.

Observe Lemma 7 materializability EL obtain CQ-predicate emptiness
E XP IME-hard well EL .
Instead proving matching E XP IME upper bound emptiness EL ,
expressive Horn DL Horn-ALCIF, EL fragment. fact, rest
section devoted proof following theorem. interesting contrast result
undecidability emptiness ALCF.
Theorem 27 Horn-ALCIF, CQ-query emptiness E XP IME.
strategy proof Theorem 27 follows. first exhibit polynomial-time reduction
CQ-query emptiness Horn-ALCIF CQ-query emptiness ELIF . Then, show
non-emptiness CQ q ELIF -TBox always witnessed ABoxes certain,
forest-like shape. consider canonical models forest-shaped ABoxes (and TBox
consideration), constructed chase-like procedure special kind
materialization (cf. Section 3), is, answers returned model precisely certain
answers. central observation matches q canonical models forest-shaped ABoxes
grouped equivalence classes induced certain splittings q. finally
show construct, equivalence class, tree automaton decides existence
forest-shaped witness ABox whose canonical model admits match q falls class.
Throughout proof, generally impose UNA.
begin reduction CQ-query emptiness ELIF . fact, reduction even
shows suffices consider ELIF -TBoxes normal form, mean
CIs take one forms
A1 u u v B,

v r.B,
22

r.A v B,

fiQ UERY P REDICATE E MPTINESS

A, A1 , . . . , , B NC {>, } r role name inverse role.
Proposition 28 every Horn-ALCIF TBox , ABox signature , CQ q, one construct
polynomial time ELIF -TBox 0 normal form q empty given iff q
empty given 0 .
proof Proposition 28 standard given appendix. follows, assume
ELIF TBoxes normal form.
next define canonical models. Let (T , A) ELIF KB satisfiable
w.r.t. . construct (typically infinite) canonical model ,A (T , A), start viewed
interpretation, is: ,A = Ind(A), AIT ,A = {a | A(a) A}, rIT ,A = {(a, b) |
r(a, b) A}. exhaustively apply following completion rules:


1. A1 u u v Ai ,A 1 n,
/ AIT ,A , add
AIT ,A .
2. r.A v B , (d, e) rIT ,A , e AIT ,A ,
/ B ,A , add B ,A ;
3. v r.B , AIT ,A , either
/ (r.B)IT ,A funct(r) 6
/






,A

,A

,A

,A

,A
(r.>)
, add (d, e) r
e

B
, e fresh
element.
/ B ,A , add e
4. v r.B , funct(r) , AIT ,A , (d, e) rIT ,A , e


,A
B
.
construction rendered deterministic using ordering inclusions domain
elements decide among different possible rule applications. reason, may speak
canonical model. call model U universal homomorphism U
model A, is, function h : U AU implies h(d) AI ,
(d, e) rU implies (h(d), h(e)) rI , h(aU ) = aI Ind(A). important
property ,A universal.2 fact, following standard prove omit
details, see example work Lutz Wolter (2012).
Lemma 29 Let ELIF -TBox ABox satisfiable w.r.t. . ,A
universal model (T , A).
Let ELIF TBox -ABox satisfiable w.r.t. . easy consequence
Lemma 29 -ABox witness CQ q non-empty given
satisfiable w.r.t. match q ,A .
next step proof Theorem 27 establish proposition constrains shape
ABoxes considered deciding emptiness ELIF . follows,
ABox called tree-shaped
1. undirected graph GA = (Ind(A), {{a, b} | r(a, b) A}) tree;
2. readers wondering relationship universal models materializations defined Section 3,
remark every universal model TBox ABox materialization A. Conversely,
materialization A, exists also universal model (Lutz & Wolter, 2012).

23

fiBAADER , B IENVENU , L UTZ , & W OLTER

2. a, b Ind(A), one role name r r(a, b) r(b, a) A,
one case.
working tree-shaped ABoxes, often designate one individuals root.
root tree-shaped ABox fixed, use A|a denote restriction
individuals b whose unique path root GA contains a, call b Ind(A)
r-successor (resp. r -successor) Ind(A) r(a, b) A|a (resp. r(b, a) A|a ).
also consider (rooted) tree-shaped interpretations tree-shaped queries, defined analogously
tree-shaped ABoxes.
-ABox forest-shaped ABoxes A0 , A1 , . . . , Ak following
conditions satisfied:
1. union A0 , A1 , . . . , Ak ;
2. k |Ind(A0 )|;
3. 1 < j k: Ind(Ai ) Ind(Aj ) = |Ind(Ai ) Ind(A0 )| = 1;
4. 1 k: Ai tree-shaped ABox rooted individual Ind(A0 ).
call A0 root component A1 , . . . , Ak tree components. width k.
degree smallest number n every tree component Ai every Ind(Ai ),
number assertions r(a, b) r(b, a) Ai bounded n. following proposition
clarifies role forest-shaped ABoxes witnesses non-emptiness.
Proposition 30 Let ELIF -TBox, ABox signature, q CQ. q non-empty
given , witnessed -ABox forest-shaped, width |q|,
degree |T |.
Proposition 30 proved appendix taking witness -ABox A, selecting part
size |q| identified match q serves root component forest-shaped
ABox, unraveling infinite ABox starting selected part, afterwards removing
unnecessary individual names obtain desired degree, finally applying compactness
make resulting witness finite.
Clearly, assume w.l.o.g. forest-shaped witness ABoxes according Proposition 30, individual names used root component taken fixed set Ind cardinality
|q|. make assumption without notice follows.
next analyze matches forest-shaped ABoxes, using splitting query components. similar splittings queries used Appendix B, simpler. forest
splitting CQ q tuple F = (q 0 , q0 , q1 , . . . , qn , ) q 0 obtained q identifying variables, q0 , q1 , . . . , qn partition atoms q 0 , : var(q0 ) Ind
following conditions satisfied
1. q1 , . . . , qn tree-shaped;
2. var(qi ) var(q0 ) 1 1 n;
3. var(qi ) var(qj ) = 1 < j n.
24

fiQ UERY P REDICATE E MPTINESS

Let ELIF -TBox, forest-shaped ABox root component A0 , match
q ,A . Note ,A consists extended (potentially infinite) trees attached
ABox individuals generated completion rules. type F =
(q 0 , q0 , q1 , . . . , qn , ) q 0 obtained q identifying variables sends
element, q0 consists atoms q 0 matches A0 -part ,A , q0 , . . . , qn
maximal connected components q 0 \ q0 , restriction range Ind. Note
that, matter match choose, maximal connected components q 0 \ q0 must
tree-shaped match tree-shaped part ,A , consists tree component
plus attached trees generated completion rules. Thus every match type
following immediate, WT ,q,F denotes set forest-shaped -ABoxes width
|q| degree |T | admit match q type F .
Lemma 31 Let ELIF -TBox, ABox signature, q CQ. q empty
given WT ,q,F empty every forest splitting F q.
on, let ELIF -TBox normal form, ABox signature, q CQ,
assume want decide whether q empty given . Lemma 31, suffices check
whether WT ,q,F empty every forest splitting F q.
Note defining set WT ,q,F possible definition forest splitting
refer particular ABox, turn due use fixed set individual
names Ind root components forest ABoxes. fact, first quantifying forest splittings
Lemma 31 quantifying forest-shaped ABoxes (when testing emptiness
WT ,q,F ) essential obtaining single exponential time upper bound. Since number
forest splittings single exponential |q|, obtain bound test emptiness
WT ,q,F time single exponential |T | + |q|. achieve constructing,
forest splitting F q, two-way alternating parity automaton infinite trees (TWAPA) AF
accepts non-empty language WT ,q,F 6= . Note infinite trees needed
automata take trees input represent (finite) forest-shaped -ABox A, also
(potentially infinite) model .
start introducing necessary background TWAPAs. Let
denote positive
integers. tree non-empty (and potentially infinite) set closed prefixes.
node root . use standard concatenation words (nodes trees) and,
convention, take x 0 = x (x i) 1 = x x . Note 1
undefined. 1, node x said child node x, x called parent
x i. slightly depart Vardis original definition TWAPAs (Vardi, 1998) working
trees full, is, define m-ary tree tree whose nodes
(rather exactly) children. W.l.o.g., assume nodes m-ary tree
{1, . . . , m} . infinite path P prefix-closed set P every n 0,
unique x P |x| = n.
set X, use B + (X) denote set positive Boolean formulas X, i.e.,
formulas built using conjunction disjunction elements X used propositional variables, special formulas true false allowed well. alphabet ,
-labeled tree pair (T, V ) tree V : node labeling function.

N

N

N
N
N

Definition 32 (TWAPA) two-way alternating parity automaton (TWAPA) m-ary trees
tuple = (S, , , s0 , F ) finite set states, finite alphabet, :
25

fiBAADER , B IENVENU , L UTZ , & W OLTER

B + (tran(A)) transition function tran(A) = {hii, [i] | {1, 0, . . . m}} set
transitions A, s0 initial state, F = (G1 , . . . , Gk ) sequence subsets
satisfying G1 G2 . . . Gh = S, called parity condition.
Intuitively, transition (hii, s) > 0 means copy automaton state sent
i-th successor current node, required exist; contrast, transition ([i], s)
sends copy i-th successor exists. transitions (hii, s) ([i], s) {1, 0}
interpreted similarly 1 indicates sending copy predecessor 0 indicates sending
copy current node. Note transition (h1i, s) cannot applied root.
Definition 33 (Run, Acceptance) run TWAPA = (S, , , s0 , F ) -labeled tree
(T, V ) S-labeled tree (T , ) () = (, s0 ) , (y) = (x, s)
(s, V (x)) = implies (possibly empty) set {(d1 , s1 ), . . . , (dn , sn )} tran(A)
satisfies 1 n:
1. di = hji, x j defined, x j , , (y i) = (x j, si ).
2. di = [j] x j defined belongs , (y i) = (x j, si ).
Given infinite path P , denote inf(P ) set states q infinitely
many P (y) form (d, q). say run (T , ) accepting
infinite paths P , exists even k inf(P ) Gk 6= inf(P ) Gk1 = .
-labeled tree (T, V ) accepted accepting run (T, V ). use
L(A) denote set -labeled trees accepted A.
note original definition TWAPAs (Vardi, 1998) uses transitions form
(hii, q) {1, . . . , m}, since (hii, q) ([i], q) coincide full m-ary trees. easy
see emptiness version TWAPAs reduced polynomial time emptiness
TWAPAs original definition since encode m-ary trees full m-ary trees. Vardi (1998)
shown emptiness problem TWAPAs E XP IME-complete. precisely,
algorithm that, given TWAPA = (S, , , s0 , F ), decides whether L(A) = runs
time exponential cardinality polynomial cardinality size .
also remind reader given two TWAPAs A1 A2 Ai = (Si , , , s0,i , Fi ),
easy construct (in polynomial time) TWAPA L(A) = L(A1 ) L(A2 )
state set S1 S2 .
make accessible TWAPAs, encode forest-shaped -ABoxes width |q|
degree |T | m-ary trees, = |q| |T |. already mentioned,
tree additionally encodes model encoded ABox. explain alphabet used
shape trees detail. root node labeled element alphabet
R consists sig(T )-ABoxes (i) Ind(A) Ind, (ii) r(a, b) implies r ,
(iii) satisfies functionality statements . Let sub(T ) denote set concepts
occur subconcepts. Non-root nodes labeled elements alphabet N
consists subsets
(NC sub(T )) ] {M } ] {r, r | r NR occurs } ] Ind ] {A | B v r.A }
26

fiQ UERY P REDICATE E MPTINESS

contains (i) exactly one role name inverse role, (ii) one element Ind,
(iii) either role name inverse role exactly one element form and,
latter case, also A.
tree hT, `i ` labeling described supposed represent forest-shaped ABox AhT,`i together model IhT,`i ABox . individuals AhT,`i
ABox labels root , plus non-root nodes whose label contains
marker . nodes denote domain elements IhT,`i identified
ABox individual. assertions AhT,`i concept role memberships IhT,`i
represented labels hT, `i. Note ABox sig(T )-ABox whereas AhT,`i uses
signature . fact, -assertions part AhT,`i assertions
part IhT,`i .
need impose additional conditions make sure R N -labeled tree hT, `i
indeed represents ABox model intended. call hT, `i proper satisfies following
conditions x :
1. root labeled element R nodes element N ;
2. `(x) N contains element Ind(A) x child root , element
Ind otherwise;
3. take path remove root node (because carries special label),
nodes whose label contains form finite (possibly empty) prefix resulting path;
4. child x `(y) N , B v r.A one
following true:
(a) x root, B `(x) r `(y);
(b) x root Ind, B(a) `(x) {a, r} `(y).
roles individual names element labels describe elements connected
elements via roles AhT,`i IhT,`i . particular, successor root contains
role r individual name Ind, node represents r-successor a. label
elements form serve special marking purpose: `(x), means
element x (which part IhT,`i AhT,`i since `(x) cannot contain )
satisfy concept inclusion B v r.A. later need special markers make
sure IhT,`i model AhT,`i , materialization AhT,`i .
explanations subsequent definitions, three conditions imposed elements
four conditions used define properness make sense reader.
Let hT, `i proper R N -labeled tree. define AhT,`i IhT,`i formally. Let
ABox labels root , let restriction signature .
-ABox AhT,`i described hT, `i
AhT,`i = {A(x) | `(x) NC `(x)}
{r(b, x) | {b, r, } `(x)} {r(x, b) | {b, r , } `(x)}
{r(x, y) | child x `(x) `(y) r `(y)}
{r(y, x) | child x `(x) `(y) r `(y)}
27

fiBAADER , B IENVENU , L UTZ , & W OLTER

interpretation IhT,`i follows:
IhT,`i

= (T \ {}) Ind(A)



IhT,`i

= {a | A(a) A} {x | `(x) NC }

r

IhT,`i

= {(a, b) | r(a, b) A} {(a, x) | {a, r} `(x)} {(x, a) | {a, r } `(x)}
{(x, y) | child x r `(y)} {(x, y) | x child r `(x)}

cIhT,`i

= c

c Ind(AhT,`i )

Apart represented ABox instead interpretation, AhT,`i identical
restriction IhT,`i individuals AhT,`i symbols , particular means
IhT,`i model AhT,`i . Note ABox AhT,`i forest-shaped -ABox. Conversely,
forest-shaped -ABox width |q| degree |T |, define proper
m-ary R N -labeled trees hT, `i AhT,`i = IhT,`i = ,A .
Let F = (q 0 , q0 , q1 , . . . , qn , ) splitting q. build TWAPA AF m-ary
R N -labeled trees accepts exactly trees hT, `i AhT,`i WT ,q,F .
number states AF polynomial |T | + |q| since checked time singleexponential number states whether L(AF ) = , obtain desired E XP IME upper
bound deciding whether WT ,q,F = . construct AF intersection following
TWAPAs:
1. Aprop , makes sure input tree proper;
2. , ensures input tree hT, `i IhT,`i model ;
3. Awf ensures hT, `i satisfies certain well-foundedness condition;
4. Amatch guarantees, exploiting conditions ensured previous automata,
input tree hT, `i AhT,`i WT ,q,F .
construction first automaton Aprop straightforward, details left reader.
Note that, enforce Condition 3 proper trees, automaton needs make use parity
acceptance condition (a co-Buchi condition would actually sufficient). second TWAPA
ensures following conditions satisfied non-root nodes x, x0 input tree:
r(a, b) `() funct(r) , {a, r} 6 `(x);
funct(r) {a, r} `(x) `(x0 ), x = x0 ;
funct(r) , x one child r `(y), additionally r `(x),
child y;
A1 u u v {A1 (a), . . . , (a)} `(), A(a) `();
A1 u u v {A1 , . . . , } `(x), `(x);
v r.B A(a) `(), (i) b {r(a, b), B(b)} `(),
(ii) child x root {a, r, B} `(x);
28

fiQ UERY P REDICATE E MPTINESS

v r.B `(x), (i) {a, r } `(x) B(a) `(), (ii) r `(x)
x non-root parent B `(y), (iii) x child {r, B} `(y);
r.A v B (i) {r(a, b), A(b)} `() (ii) child root
{a, r, A} `(y), B(a) `();
r.A v B (i) {a, r } `(x) A(a) label root, (ii) r `(x)
x parent `(y), (iii) x child {r, A} `(y), B `(x).
Working exact details Aprop left reader.
Ideally, would like third automaton Awf ensure IhT,`i canonical model
AhT,`i . However, seem easily possible model constructed
applying completion rules certain order difficult simulate automatonnote
applying rules different order might result construction interpretation
isomorphic one obtained following prescribed application order. thus define
Awf achieve crucial property canonical models positive information (concept
role memberships domain elements) reason, namely contained
AhT,`i logically implied AhT,`i together . formalize terms
derivations.

Let hT, `i proper R N -labeled tree, A0 (NC sub(T )) {>}, x0 A0 hT,`i .
derivation A0 x0 hT, `i finite L-labeled tree hT 0 , `0 i, L set pairs (A, x)
(NC sub(T )) {>} x AIhT,`i . require root 0 labeled
(A0 , x0 ) 0 minimal nodes z 0 `0 (z) = (A, x), one
following holds:
1. {>} x Ind(AhT,`i );
2. 6 `(x) CI A1 u u v children z1 , . . . , zn z 0
`0 (zi ) = (Ai , x) 1 n;
3. 6 `(x) CI r.A0 v child z 0 z 0 `0 (z 0 ) = (A0 , x0 )
(x, x0 ) rIhT,`i . Moreover, B `(x), child z 00 z 0
`0 (z 00 ) = (B, x).
4. 6 `(x) CI A0 v r.A funct(r) child z 0 z 0
`0 (z 0 ) = (A0 , x0 ) (x0 , x) rIhT,`i . Moreover, B `(x), child z 00
z 0 `0 (z 00 ) = (B, x).
5. = >, > 6 `(x), B `(x), child z 0 z 0 `0 (z 0 ) = (B, x).
6. `(x), CI A0 v r.A , child z 0 z 0 `0 (z 0 ) =
(A0 , x0 ) (x0 , x) rIhT,`i either (i) x child x0 , (ii) x child
root, x0 Ind, {r, x0 } `(x).
say hT, `i well-founded whenever x AIhT,`i , NC {>},
derivation x hT, `i. hard construct TWAPA Awf accepts precisely
well-founded proper R N -labeled trees; essentially, automaton verify existence
required derivations implementing Conditions 1 6 transitions, additionally
using co-Buchi condition ensure finiteness derivation.
Next let F = (q 0 , q0 , q1 , . . . , qn , ). automaton Amatch checks
29

fiBAADER , B IENVENU , L UTZ , & W OLTER

1. match q0 IhT,`i
2. match qi IhT,`i v var(q0 ) var(qi ), (v) = (v).
Amatch easy construct omit details. announced, define AF
accepts intersection languages accepted Aprop , , Awf , Amatch . remains
show WT ,q,F empty iff L(AF ) empty.
this, first clarify relation well-foundedness, canonical models, universal models. call proper R N -labeled tree hT, `i canonical (i) IhT,`i canonical
model AhT,`i , (ii) every x \{} 6 `(x), concept `(x)
element x created due application third completion rule inclusion
form B v r.A parent x .
Lemma 34
1. proper R N -labeled tree canonical, well-founded;
2. hT, `i proper R N -labeled tree well-founded IhT,`i model ,
IhT,`i universal model AhT,`i .
proof Lemma 34 found appendix. Point 1 established tracing applications
completion rules applied construct canonical model AhT,`i showing
addition make gives rise derivation. Point 2, first show one
make certain uniformity assumption derivations show define homomorphism
IhT,`i model AhT,`i starting part IhT,`i corresponds root
component AhT,`i moving downwards along tree-shaped parts IhT,`i .
Lemma 35 WT ,q,F = iff L(AF ) = .
Proof. First assume WT ,q,F 6= . forest-shaped -ABox width
|T | degree |q| satisfiable w.r.t. match q ,A type F .
Let hT, `i m-ary proper R N -labeled tree satisfies AhT,`i = canonical.
hT, `i L(Aprop ). Since satisfiable w.r.t. , IhT,`i = ,A model thus
hT, `i L(Aprop ). Point 1 Lemma 34, hT, `i L(Awf ). Finally, match witnesses
Conditions 1 2 definition Amatch satisfied thus hT, `i L(Amatch )
done.
Conversely, assume tree hT, `i L(AF ). Since hT, `i L(Aprop ), = AhT,`i
defined; definition, forest-shaped -ABox width |T | degree |q|.
remains show match q ,A type F . Since hT, `i L(AT )
L(Awf ), IhT,`i model hT, `i well-founded. Point 2 Lemma 34, IhT,`i thus
universal model A. hT, `i L(Amatch ), Conditions 1 2 definition
Amatch satisfied. verified that, consequently, match q IhT,`i
type F . Composing homomorphism IhT,`i ,A , exists since IhT,`i
universal, yields match q ,A type F .


30

fiQ UERY P REDICATE E MPTINESS

6. DL-Lite Family
study query predicate emptiness DL-Lite family description logics (Calvanese
et al., 2007; Artale et al., 2009). begin with, introduce dialects DL-Lite consider.
Basic concepts B defined
B

::=

>

|

|

r.>

ranges NC r (possibly inverse) roles. DL-Litecore TBox finite
set CIs form B1 v B2 B1 u B2 v , B1 B2 basic concepts. Thus,
DL-Litecore included ELI but, includes inverse roles, included EL. DL-LiteF
extension DL-Litecore functionality statements. DL-LiteR extension DLLitecore role inclusions r v s, r, roles. DL-LiteR logical underpinning
OWL profile OWL2 QL (Motik et al., 2009). Finally, DL-Litehorn extension DL-Litecore
conjunctions basic concepts left hand side CIs. Alternatively, defined
fragment ELI qualified existential restrictions r.C replaced unqualified existential
restrictions r.>. details, refer readers work Calvanese et al. (2007), Artale
et al. (2009), Calvanese, De Giacomo, Lembo, Lenzerini, Rosati (2013).
briefly discuss UNA DL-Lite dialects introduced above. First observe DLLitehorn DL-LiteF fragments ALCIF. Thus, Lemma 3, query emptiness predicate emptiness DL-Litehorn DL-LiteF oblivious whether UNA made not.
DL-LiteR fragment ALCIF. is, however, straightforward show DL-LiteR
certain answers CQs depend whether one adopts UNA not. Thus, also
DL-LiteR query emptiness predicate emptiness oblivious whether UNA made
not. following proofs make UNA.
main results follows: CQ-query emptiness coNP-complete DL-Lite dialects.
coNP-lower bound holds already fragment DL-Litecore without role names.
contrast, complexity deciding IQ-query emptiness CQ-predicate emptiness depends
whether conjunctions admitted left-hand side concept inclusions not.
conjunctions admitted (as DL-Litecore , DL-LiteR , DL-LiteF ), IQ-query emptiness
CQ-predicate emptiness NL OG PACE-complete. conjunctions admitted (as DLLitehorn ), IQ-query emptiness CQ-predicate emptiness coNP-complete. Again,
lower bound holds already fragments DLs without role names.
note follows use Theorem 6 gives polynomial reduction
CQ-predicate emptiness IQ-query emptiness certain DLs apply DL-Lite
dialects. Instead give direct proofs. results presented Figure 1 DL-Lite dialects
straightforward consequences results established section.
begin proving coNP lower bounds. Let Lcore DL admits CIs v B
u B v , let Lhorn DL admits CIs u A0 v B u B v ,
A, A0 , B concept names.
Theorem 36 Lhorn , IQ-query emptiness, CQ-query emptiness, CQ-predicate emptiness
coNP-hard. Lcore , CQ-query emptiness coNP-hard.
Proof. proofs reduction well-known coNP-complete problem testing whether
propositional formula conjunctive normal form (CNF) unsatisfiable. Let = 1 k
31

fiBAADER , B IENVENU , L UTZ , & W OLTER

CNF formula, v1 , . . . , vn variables used , A1 , . . . , Ak concept names representing clauses, Av1 , Av1 , . . . , Avn , Avn concept names representing literals. Let
additional concept name, set = {Av1 , Av1 , . . . , Avn , Avn }. Consider Lhorn -TBox
consisting following CIs:
Avj u Avj v 1 j n;
A`j v Ai 1 k `j = ()vj disjunct ;
A1 u u Ak v .
straightforward show A(u) empty given iff u A(u) empty given
iff unsatisfiable. Thus, deciding IQ-query emptiness, CQ-predicate emptiness, CQ-query
emptiness Lhorn coNP-hard. coNP-hardness result CQ-query emptiness Lcore ,
drop last CI use CQ A1 (u) Ak (u) instead.

prove matching upper complexity bounds, considering logics DL-Litecore , DLLiteR , DL-LiteF , DL-Litehorn . end, formulate general sufficient conditions
deciding emptiness PT IME coNP. say DL L polysize emptiness witness property whenever CQ q empty given , exists -ABox
polynomial size size q satisfiable w.r.t. certT ,A (q) 6= .
Lemma 37 Let L description logic polysize emptiness witness property
query evaluation problem CQs L NP. Moreover, assume satisfiability ABoxes
w.r.t. L-TBoxes decided polynomial time. CQ-query emptiness L coNP.
Proof. NP-algorithm deciding whether CQ q empty w.r.t. guesses (i) -ABox
polynomial size q, (ii) tuple ~a individual names Ind(A) appropriate
length, (iii) polysize certificate ~a certT ,A (q); verifies polynomial time
satisfiable w.r.t. guessed certificate valid.

Theorem 38 DL-Litecore , DL-LiteR , DL-LiteF , DL-Litehorn , deciding CQ-query emptiness
coNP.
Proof. conditions stated Lemma 37 shown Calvanese et al. (2007) Artale
et al. (2009). sketch proof polysize emptiness witness property. Assume ~a certT ,A (q)
CQ q = ~u (~v , ~u) TBox DLs listed theorem statement
assume satisfiable w.r.t. . use canonical model ,A Lemma 29
(for DL-Litecore , DL-LiteF , DL-Litehorn used without modification since
fragments ELIF ; DL-LiteR , one add following completion rule
construction ,A : (x, y) rIT ,A r v , add (x, y) sIT ,A ). Let
match q ,A . recall ,A consists restriction individuals aIT ,A
Ind(A) tree-shaped interpretations Ia attached aIT ,A . Let A00 set assertions
use individual names Ind(A) exists v var(q) (v) = aIT ,A
(v) Ia . Moreover, individual Ind(A00 ) selected exists
role r b r(a, b) A, select one r(a, b ) include A01 . Let A0 = A00 A01 .
Clearly ABox A0 required: polynomial size, satisfiable w.r.t. (being subset
A), construction, satisfies ~a certT ,A0 (q).


32

fiQ UERY P REDICATE E MPTINESS

say DL L singleton emptiness witness property whenever CQ q form
A(v) v A(v) empty given , exists -ABox containing one
assertion satisfiable w.r.t. certT ,A (q) 6= .
Lemma 39 Let L description logic singleton emptiness witness property
query evaluation problem CQs form A(v) v A(v) L NL OG PACE. Moreover, assume satisfiability singleton ABoxes w.r.t. L-TBoxes decided NL OG PACE.
IQ-query emptiness CQ-predicate emptiness L NL OG PACE.
Proof. non-deterministic logarithmic space algorithm deciding whether CQ form A(v)
v A(v) empty w.r.t. iterates -ABoxes containing one assertion
checks whether least one ABoxes satisfiable w.r.t. satisfies , |= v A(v)
or, respectively, , |= A(a), individual Ind(A).

Theorem 40 DL-Litecore , DL-LiteR , DL-LiteF , deciding IQ-query emptiness CQpredicate emptiness NL OG PACE-complete.
Proof. NL OG PACE-upper bound, conditions stated Lemma 39 shown
Calvanese et al. (2007) Artale et al. (2009). sketch proof singleton emptiness
witness property. Assume ~a certT ,A (q) CQ q form A(v) v A(v) TBox
DLs listed theorem statement. assume satisfiable w.r.t. .
consider case q = v A(v); case q = A(v) similar. proof Theorem 38,
use canonical model ,A . Let mapping ,A (v) AIT ,A
consider uniquely determined Ind(A) (v) = aIT ,A (v) Ia . Using
fact conjunctions occur left-hand side CIs , one show exists
single assertion form B(a) r(a, b) ABox A0 consisting assertion,
() certT ,A0 (q). follows A0 desired witness ABox.
matching lower bound follows directly fact deciding whether |= v B
NL OG PACE-hard TBoxes DL-Litecore (Artale et al., 2009).


7. Case Study Application Modularity
demonstrate usefulness emptiness two ways. First, carry case study
predicate emptiness medical domain, find use realistic ontology adds
significant number non-empty predicates ABox signature also large
number predicates empty. static analysis, thus potentially non-trivial user
manually distinguish non-empty empty predicates. Second, show (predicate)
emptiness used produce smaller version TBox tailor-made querying
given ABox signature (in sense: module TBox). Replacing potentially
much smaller module facilitates comprehension TBox, thus helping query formulation.
support claims experiments.
case study, use comprehensive medical ontology SNOMED CT, provides
systematic vocabulary used medical information interchange enable interoperable electronic health records. covers diverse medical areas clinical findings, symptoms, diagnoses, procedures, body structures, organisms, substances, pharmaceuticals, devices specimens.
33

fiBAADER , B IENVENU , L UTZ , & W OLTER

concepts roles

IQ
CQ axioms
axioms
non-empty non-empty -mod. CQ -core

500

16

3557

4631

8910

4597

500

31

3654

4734

8911

4696

1000

16

5827

7385

14110

7349

1000

31

6242

7762

14147

7731

5000

16

18330

21451

33469

21427

5000

31

18469

21557

33616

21532

10000

16

29519

33493

47044

33489

10000

31

30643

34645

47256

34637

Figure 4: Experimental Results
SNOMED CT formulated EL extended role inclusions (which removed experiments). contains 370,000 concept names 62 role names. use SNOMED CT
together ABox signature real-world application randomly generated ABox
signatures. real-world signature obtained analyzing clinical notes emergency
department intensive care unit two Australian hospitals, using natural language processing methods detect SNOMED CT concepts roles.3 contains 8,858 concepts 16 roles.
signature, 16,212 IQ-non-empty predicates 17,339 CQ-non-empty predicates
computed. Thus, SNOMED CT provides substantial number additional predicates query
formulation, roughly identical number predicates ABox signature. However,
numbers also show majority predicates SNOMED CT cannot meaningfully used
queries -ABoxes, thus identifying relevant ones via predicate emptiness potentially
helpful. Somewhat surprisingly, number CQ-non-empty predicates 10%
higher number IQ-non-empty symbols.
analyzed randomly generated signatures contain 500, 1,000, 5,000, 10,000
concept names 16 31 role names (1/2 1/4 role names ontology). Every
signature contains special role name role-group, used SNOMED CT implement
certain modeling pattern present also ABoxes allow pattern there.
number concept role names, generated 10 signatures. columns IQ nonempty CQ non-empty Figure 4 show results, numbers averages
10 experiments size. additional experiments confirm findings real-world
signature: case, substantial number additional predicates becomes available query
formulation, also large number predicates empty.
come application modularity. Recall main motivation studying
emptiness support query formulation: TBoxes large complex, difficult
understand whether TBox contains sufficient background knowledge given query q
non-empty answer -ABox. case, clearly make sense
3. See Current Collaborative Projects Health Information Technologies Research Laboratory University
Sydney (HITRL, 2016).

34

fiQ UERY P REDICATE E MPTINESS

pose q -ABox TBox used background ontology. Similarly,
hard find whether TBox sufficiently powerful entail given predicate occur
query non-empty answer -ABox. Again, case,
predicate used formulating queries. Here, go one step further: instead
using emptiness directly support query formulation, use simplify TBox.
precisely, consider problem extracting (hopefully small!) subset given TBox
gives exactly answers CQs (or IQs) -ABox. subset called
-substitute w.r.t. CQ (or IQ, respectively) original TBox replace original
TBox answering CQs (or IQs, respectively). Working small -substitute instead
original TBox supports comprehension TBox thereby formulation meaningful
queries.
beyond scope paper investigate -substitutes depth. Instead, show that,
description logic ELI, predicate emptiness gives rise particularly natural kind substitute call CQ -core. CQ -core obtained removing concept inclusions
contain predicate CQ-predicate empty w.r.t. TBox. Thus,
CQ -core give answers CQs original TBox -ABoxes, also
appealing property predicates occur used meaningfully CQ
querying -ABoxes.
also show widely known semantic -modules introduced Grau et al. (2008)
-substitutes CQ -cores cannot larger semantic -modules (unless original
ontology contains tautological concept inclusions). evaluate method practice compare
size CQ -cores -modules, also extend case study based SNOMED CT
extraction CQ -cores comparison -modules. start defining -substitutes
formal way.
Definition 41 Let 0 Q {IQ, CQ}. 0 -substitute w.r.t. Q
-ABoxes q Q, certT 0 ,A (q) = certT ,A (q).
aware -substitutes according Definition 41 studied before,
closely related types modules. example, -modules give answers
CQs formulated signature -ABoxes studied work Lutz Wolter (2010),
Kontchakov, Wolter, Zakharyaschev (2010), Konev, Ludwig, Walther, Wolter (2012), Botoeva, Kontchakov, Ryzhikov, Wolter, Zakharyaschev (2014, 2016), Romero, Kaminski,
Grau, Horrocks (2015). stronger version module provided -modules require original TBox model-conservative extension module regarding signature
, studied Konev, Lutz, Walther, Wolter (2013) Gatens, Konev, Wolter (2014).
However, important difference -modules -substitutes
latter restrict signature ABox, queries. contrast, mentioned
-modules guarantee answers CQs formulated signature (and -ABoxes).
particular, follows minimal modules, defined work Kontchakov et al. (2010)
Konev et al. (2013), general used -substitute.
show ELI (and, therefore, also fragment EL) one use CQ-predicate
emptiness straightforward way compute -substitute w.r.t. CQ. Let TBox
ABox signature. CQ -core , denoted TCQ , set concept inclusions
X sig() CQ-predicate empty given .

35

fiBAADER , B IENVENU , L UTZ , & W OLTER

Theorem 42 Let TBox ELI. CQ -core -substitute w.r.t. CQ
(and thus also w.r.t. IQ).
Proof. Let 0 CQ -core assume 0 , 6|= q[~a] -ABox A. Consider
canonical model 0 ,A , introduced Section 5.2. 0 ,A model 0 A,
0 ,A 6|= q[~a]. sufficient show 0 ,A model . Let C v \ 0
assume 0 ,A 6|= C v D. C 0 ,A 6= . Let qC (v) tree-shaped conjunctive
query corresponding C, constructed standard way (see Appendix B formal definition
similar construction). 0 ,A |= v qC (v) 0 , |= v qC (v). Hence , |=
v qC (v) X sig(C) CQ-empty given . Since C v , also
obtain , |= v qD (v), qD (v) tree-shaped conjunctive query corresponding D.
Thus, X sig(D) CQ-empty given . means C v 0 ,
contradiction.

Note Theorem 22, CQ -core computed polynomial time EL-TBox.
make simple observations regarding CQ -cores:
1. Theorem 42 fails DLs admit negation. example, = {A v B, B v E}
= {A}, -substitute w.r.t. CQ coincides , CQ -core
empty.
2. CQ -core always minimal -substitute w.r.t. CQ. Consider, example, =
{A v B1 , v B2 , B1 v B2 } let = {A}. 0 = {A v B1 , v B2 }
-substitute w.r.t. CQ CQ -core coincides .
3. let IQ -core TBox defined analogy CQ -core , based IQemptiness instead CQ-emptiness. IQ -core cannot serve -substitute
w.r.t. IQ even EL-TBox. example, let = {A v r.B, r.B v E}
= {A}. B IQ-empty given IQ -core empty. However,
empty TBox -substitute w.r.t. IQ since , |= E(a) = {A(a)}.
Interestingly, contrast -modules discussed above, -modules introduced Grau
et al. (2008) turn examples -substitutes. define -modules, let signature.
0
0
Two interpretations 0 coincide w.r.t. = X = X X .
subset 0 TBox called semantic -module w.r.t. every interpretation
0
interpretation 0 coincides w.r.t. sig(T 0 ) X = X 6
sig(T 0 ) model \ 0 . shown work Grau et al. (2008) extracting minimal
semantic -module complexity standard reasoning (that is, subsumption).
addition, shown syntactic approximation called syntactic -module computed
polynomial time (every syntactic -module semantic -module, necessarily
way around). following lemma establishes relationship -modules substitutes. concept inclusion C v tautological |= C v D.
Proposition 43 Let TBox formulated DLs introduced paper, let 0
semantic -module w.r.t. .
1. 0 -substitute w.r.t. CQ;
36

fiQ UERY P REDICATE E MPTINESS

2. sig(T 0 ) contains predicates CQ-empty given ;
3. ELI-TBox contain tautological CIs, CQ -core
contained 0 .
Proof. Point 1, suppose 0 , 6|= q[~a], 0 semantic -module w.r.t.
-ABox. Let model 0 6|= q[~a], consider interpretation 0
coincides 0 sig(T 0 ) X = remaining predicates X.
0 model since 0 semantic -module w.r.t. . 0 model since
-ABox. Since shrank extension predicates transitioning 0
6|= q[~a], 0 6|= q[~a]. Hence , 6|= q[~a], required.
Point 2, assume X CQ-empty given , X 6 sig(T 0 ). Suppose X =
concept name (the case X = r role name r similar left reader). Take
-ABox satisfiable w.r.t. , |= v A(v). Let model (T , A),
0
let 0 interpretation coincides sig(T 0 ) =
0
remaining (in particular AI = ). definition semantic -modules, 0 model
(T , A). derived contradiction 0 shows , 6|= v A(v).
Point 3, assume formulated ELI contains tautological inclusions. Let C v
\ 0 . Then, definition semantic -modules, sig(C v D) contains predicate
X 6 sig(T 0 ) (because otherwise C v tautology). Thus, Point 2, sig(C v D)
contains predicate CQ-empty given . C v CQ -core ,
required.

Point 1, use algorithms computing syntactic semantic -modules
ones provided work Grau et al. (2008) find -substitutes large variety DLs.
Point 2, modules also provide over-approximation set predicates
CQ-empty. Finally, Point 3 means that, ELI, -modules cannot smaller CQ -core
unless tautological concept inclusions. general, however, -modules larger
CQ -core TBox. following example shows case already acyclic
EL-TBoxes: let
= {A v s1 .r1 .> u s2 .r2 .>, B r1 .> u r2 .>}
= {A}. predicates CQ-empty given A, s1 , s2 , r1 , r2 . Hence
CQ -core contains first CI . However, non-trivial semantic -modules
w.r.t. (and thus syntactic ones either).
demonstrate potential usefulness -substitutes CQ -core extending
case study medical domain. use ontology SNOMED CT ABox
signatures described beginning section, analyzing size CQ -core comparing size original ontology syntactic -module. real-world
signature, CQ -core contains 17,322 370,000 concept inclusions SNOMED CT. Thus,
5% size original ontology. -module w.r.t. turns significantly
larger CQ -core, containing 27,383 axioms. random signatures, sizes CQ cores -modules shown two right-most columns Figure 4. confirm
findings real-world signature: CQ -core much smaller original ontology
-module.
37

fiBAADER , B IENVENU , L UTZ , & W OLTER

Q UERY C ONTAINMENT

Q UERY E MPTINESS

DL

IQ

CQ

IQ

CQ

EL

E XP IME-c.

E XP IME-c.

PT IME-c.

PT IME-c

EL

E XP IME-c.

E XP IME-c.

E XP IME-c.

E XP IME-c

ELI, Horn-ALCIF

E XP IME-c.

2E XP IME-c.

E XP IME-c.

E XP IME-c.

PT IME

coNP-c.

NL OG PACE-c.

coNP-c.

coNP-c.

p2 -c.

coNP-c.

coNP-c.

NE XP IME-c.

NE XP IME-h.,

NE XP IME-c.

NE XP IME-c.

NE XP IME-c.

2E XP IME-c.

DL-Litecore
DL-Litehorn
ALC

2NE XP IME
ALCI

NE XP IME-c.

2NE XP IME-c.

Figure 5: Query Containment vs Query Emptiness

8. Related Work
Query emptiness fundamental problem static analysis database queries. also
called query satisfiability problem. XML, example, takes following form: given
XPath query p DTD D, exist XML document conforms
answer p non-empty. complexity problem ranges tractable
undecidable depending XPath fragment, see e.g. work Benedikt et al. (2008)
references therein. DL context, query emptiness first considered work Lubyte
Tessaris (2008), use step guide enrichment ontologies.
query emptiness problem studied paper special case following query
containment problem, first considered work Bienvenu, Lutz, Wolter (2012).
regard pair (T , q) consists TBox query q compound query Q, called
ontology-mediated query (OMQ), answers Q certain answers q w.r.t.
(Bienvenu et al., 2014). take two OMQs Qi = (Ti , qi ), {1, 2}, q1 q2 IQs
CQs arity. Q1 -contained Q2 , ABox signature , ABoxes satisfiable w.r.t. T1 T2 , certT1 ,A (q1 ) certT2 ,A (q2 ). case,
write Q1 Q2 . notion containment generalizes traditional query containment
problem DLs (Calvanese et al., 2007) relativizing ABox signature admitting
distinct TBoxes T1 T2 . Query emptiness IQ q given clearly polynomially
reduced -containment setting T1 = , q1 = q, T2 = , q2 = A(x) fresh concept
name A, similarly CQs. Deciding -containment, however, often computationally harder
deciding query emptiness. Table 5 summarizes known results; results EL DLLite work Bienvenu et al. (2012), results ELI work Bienvenu,
Hansen, Lutz, Wolter (2016), results ALC ALCI work Bienvenu
et al. (2014) Bourhis Lutz (2016).
Query emptiness also closely related explaining negative answers queries. problem
studied, example, Calvanese, Ortiz, Simkus, Stefanoni (2013). Adopting abductive reasoning approach, described follows. Assume , 6|= q(~a) TBox , ABox
A, query q. explain ~a answer q, one wants find minimal ABoxes E
38

fiQ UERY P REDICATE E MPTINESS

certain signature interest E satisfiable w.r.t. , E |= q(~a).
-ABoxes E regarded explanation missing answer used debugging purposes. shown work Calvanese et al. (2013) query emptiness IQs
Boolean CQs reduces (under many-one logarithmic space reductions) problem deciding
existence explanation , 6|= q(~a) = . DL-LiteA , reduction even works
unions conjunctive queries arity. Calvanese et al. (2013) use observation obtain
lower complexity bounds explaining negative query answers, exploiting results published
conference predecessor paper (Baader, Bienvenu, Lutz, & Wolter, 2010). also
conjecture that, conversely, techniques proving upper complexity bounds query emptiness
(such ones paper) used obtain upper bounds explaining negative answers.

9. Conclusion
investigated computational complexity query predicate emptiness EL,
DL-Lite, ALC families DLs, concentrating instance queries conjunctive queries
showing complexities range NL OG PACE undecidable. also highlighted that,
different DLs query languages, different kinds witness ABoxes sufficient establish
non-emptiness. DLs queries considered paper, would interesting
investigate future work, include following:
DLs include transitive roles, role inclusions, symmetric roles, role inclusion axioms
(Horrocks, Kutz, & Sattler, 2006; Kazakov, 2010).
cases, straightforward reductions results presented paper possible.
example, IQ-query emptiness Horn-SHIF decidable E XP IME since every
Horn-SHIF TBox , IQ A(x), ABox signature , one construct polynomial
time Horn-ALCIF TBox 0 , |= A(a) iff 0 , A, |= A(a) -ABoxes
(Hustadt et al., 2007; Kazakov, 2009). cases, CQ-query emptiness
Horn-SHIF, seems reduction.
DLs include nominals.
important classes queries unions conjunctive queries (UCQs).
materializable DLs Horn-ALCIF,
W UCQ query emptiness reduced CQ
query emptiness since every UCQ q = iI qi (~x), , |= q(~a) iff
, |= qi (~a). simple reduction work non-Horn DLs
ALC.
would also interesting develop practical algorithms emptiness evaluate algorithms real-world ontologies queries. Note algorithms EL DL-Lite
easily implementable efficient presented paper. actually confirmed
case study Section 7. work required design efficient algorithms expressive DLs. Finally, would relevant investigate notion -substitute introduced
application modularity detail. example, open question compute minimal -substitutes expressive DLs ALC practice, involved
complexities.
39

fiBAADER , B IENVENU , L UTZ , & W OLTER

Acknowledgements
first author partially supported cfaed (Center Advancing Electronics Dresden)
second author partially supported ANR project PAGODA (ANR-12-JS02-007-01).
grateful Julian Mendez Dirk Walther supporting us case study. would
like thank anonymous reviewers provided excellent comments helped us improve
paper.

Appendix A. Proofs Section 3
formulate result proved again.
Lemma 3 Let ALCIF-TBox. CQ q empty given UNA iff
empty given without UNA.
Proof. Consider CQ q answer variables v1 , . . . , vn .
(Only if) Assume q non-empty given without UNA. ABox satisfiable w.r.t. without UNA certT ,A (q) 6= without UNA.
Take model suppose without loss generality infinite. Define
equivalence relation Ind(A) setting b whenever aI = bI . Choose single
representative equivalence class, denote representative equivalence
class containing a. Let A0 ABox obtained replacing individual .
show A0 satisfiable w.r.t. UNA certT ,A0 (q) 6= UNA.
Regarding satisfiability, easy see model (T and) A0 satisfies
UNA individuals appearing A0 . Moreover, since infinite, reinterpret
individual names NI \ Ind(A0 ) obtain interpretation model A0 satisfies
UNA.
showing certT ,A0 (q) 6= UNA. Take (a1 , . . . , ) certT ,A (q) without
UNA. aim show (a1 , . . . , ) certT ,A0 (q). Let J 0 model A0
0
satisfies UNA. show match q J 0 (vi ) = (ai )J every
0
1 n. Consider interpretation J obtained J 0 setting aJ = (a )J every
Ind(A). easy see J model without UNA, J |= q[a1 , . . . , ]
match q J (vi ) = aJ
every 1 n. also
desired match q J 0 , finishes proof.
(If) Assume q non-empty given UNA. -ABox
satisfiable w.r.t. UNA certT ,A (q) 6= UNA. Clearly,
also satisfiable w.r.t. without UNA remains show certT ,A (q) 6= without
UNA. Let (a1 , . . . , ) certT ,A (q) UNA, let model without
UNA. show |= q[a1 , . . . , ]. Ind(A), let Ia following
unfolding aI :
domain Ia Ia consists words d0 r0 d1 rk1 dk d0 , . . . , dk
r0 , . . . , rk1 (potentially inverse) roles d0 = aI , (di , di+1 ) riI < k,
ri 6= ri+1 functional ri+1 < k, r0 (a, b) 6 b Ind(A) r0
functional.
AIa = {d0 r0 d1 dk | dk AI } NC ;
40

fiQ UERY P REDICATE E MPTINESS

rIa = {(d0 r0 d1 dk , d0 r0 d1 dk rk+1 dk+1 ) | r = rk+1 }
{(d0 r0 d1 dk rk+1 dk+1 , d0 r0 d1 dk ) | r = rk+1 } r NR .
Assume Ia mutually disjoint let individual name root Ia .
obtain interpretation J taking disjoint union Ia , Ind(A), adding (a, b) rJ
whenever r(a, b) A, setting aJ = Ind(A). One show J model
UNA. Thus J |= q[a1 , . . . , ] match q J (vi ) = (ai )J
every 1 n. verified 0 defined setting 0 (vi ) = aI (vi ) = Ind(A)
0 (vi ) = dk (v) = d0 , . . . , dk k 1 match q I, thus |= q[a1 , . . . , ],
required.


Appendix B. Proofs Section 4
restate first result proved.
Theorem 16 ALC, CQ-query emptiness NE XP IME.
general idea proving Theorem 16 follows. Given ALC-TBox , signature ,
CQ q, Theorem 13 suffices test whether , , 6|= q. thus start computing , . check whether , , 6|= q, guess extension set 0
concept inclusions extension , set A0 ABox assertions 0 A0
satisfied models , q match; subsequently, remains
test satisfiability , A0 w.r.t. 0 . subtlety lies selecting class extensions
guessed careful enough way final satisfiability check carried
NE XP IME.
reuse technical definitions results work Lutz (2008) proves
combined complexity CQ-answering DL SHQ E XP IME. definitions
slightly modified since Lutz considers CQs without answer variables uses DL SHQ,
ALC proper fragment. However, straightforward verify proofs given
Lutz also work modified definitions.
CQ q viewed directed graph Gdq = (Vqd , Eqd ) Vqd = var(q) Eqd =
{(v, v 0 ) | r(v, v 0 ) q r NR }. call q directed tree-shaped Gdq directed tree
r(v, v 0 ), s(v, v 0 ) q implies r = s. q directed tree-shaped v0 root Gdq , call v0
root q. U var(q), write q|U denote restriction q atoms contain
variables U . set DTrees(q) directed tree-shaped subqueries q defined follows:
DTrees(q) = {q|U | U = Reachq (v), v var(q), q|U directed tree-shaped}
Reachq (v) set variables reachable v Gdq . say
q 0 obtained q performing fork elimination q 0 obtained q selecting two
atoms r(v 0 , v) s(v 00 , v) v 0 , v 00 , v qvar(q) v 0 6= v 00 , identifying v 0
v 00 ;
q 0 fork rewriting q q 0 obtained q repeatedly (but necessarily exhaustively) performing fork elimination;
41

fiBAADER , B IENVENU , L UTZ , & W OLTER

q 0 maximal fork rewriting q q 0 fork rewriting fork elimination
possible q 0 .
following shown work Lutz (2008), plays central role subsequent
definitions.
Lemma 44 variable renaming, every CQ unique maximal fork rewriting.
following definitions splittings spoilers also taken work Lutz. understand splitting CQ q intuitive level, useful consider matches q model
TBox ABox special shape: consists core part whose elements exactly
(the interpretations of) ABox individuals tree-shaped parts attached
element core part disjoint other. fact, proved Lutz , 6|= q,
model described shape 6|= q. match q model
described shape partitions variables q several sets: set R contains variables matched ABox individual; sets S1 , . . . , Sn represent disjoint tree-shaped
subqueries q matched tree part whose root connected variable
R via role atom q; set represents collection tree-shaped subqueries
q disconnected variables R Si . addition partitioning, splittings
record variable R set S1 , . . . , Sn connected ABox elements
variables R mapped to. define splittings formally.
Let K = (T , A) ALC-knowledge base q CQ. splitting q w.r.t. K tuple =
hR, T, S1 , . . . , Sn , , i, R, T, S1 , . . . , Sn partitioning var(q), : {1, . . . , n} R
assigns set Si variable (i) R, : R Ind(A) assigns variable R
individual A. splitting satisfy following conditions:
1. CQ q|T variable-disjoint union directed tree-shaped queries;
2. queries q|Si , 1 n, directed tree-shaped;
3. r(v, v 0 ) q, one following holds: (i) v, v 0 belong set R, T, S1 , . . . , Sn
(ii) v R, (i) = v, v 0 Si root q|Si ;
4. 1 n, unique r NR r((i), v0 ) q, v0 root q|Si ;
5. avar(q) R.
Let q directed tree-shaped CQ. define ALC-concept Cq,v v var(q):
v leaf Gdq , Cq,v =
otherwise, Cq,v =

u

A(v)q

Au

u C;
u r.C

A(v)q

r(v,v 0 )q

q,v 0 .

v root q, use Cq abbreviate Cq,v .
following, allow compound concepts negated roles used ABox assertions. semantics assertions corresponding KBs defined expected way:
interpretation satisfies C(a) aI C satisfies r(a, b) satisfy r(a, b).
Let = hR, T, S1 , . . . , Sn , , splitting q w.r.t. K q1 , . . . , qk (directed
tree-shaped) disconnected components q|T . ALC-knowledge base (T 0 , A0 ) spoiler q,
K, one following conditions hold:
42

fiQ UERY P REDICATE E MPTINESS

1. > v Cqi 0 , 1 k;
2. atom A(v) q v R A((v)) A0 ;
3. atom r(v, v 0 ) q v, v 0 R r((v), (v 0 )) A0 ;
4. D(((i))) A0 1 n, = r.Cq|S v0 root q|Si

r((i), v0 ) q.
call K0 spoiler q K (i) every fork rewriting q 0 q, every splitting q 0
w.r.t. K, K0 spoiler q 0 , K, ; (ii) K0 minimal Property (i). following
result proved work Lutz (2008).
Theorem 45 Let K = (T , A) ALC-knowledge base q CQ. K 6|= q iff
spoiler (T 0 , A0 ) q K A0 satisfiable w.r.t. 0 .
following lemma, observed Lutz, plays central role obtaining NE XP IME
decision procedure.
Lemma 46 Let K = (T , A) ALC-knowledge base, q CQ, q maximal fork rewriting,
K0 = (T 0 , A0 ) spoiler q K. K0 contains concept inclusions ABox
assertions following form:
1. > v Cq0 q 0 DTrees(q );
2. A(a) Ind(A) occurring q;
3. r(a, b) a, b Ind(A) r occurring q;
4. D(a) Ind(A) = r.Cq0 , r occurs q q 0 DTrees(q ).
Note definition spoiler q K refers fork rewritings q,
exponentially many, Lemma 46 refers unique maximal form rewriting q .
fact, since cardinality DTrees(q ) clearly bounded size q, number concept
inclusions assertions listed Lemma 46 polynomial size q.
set proof Theorem 16. Theorems 13 45, CQ q empty
signature given TBox iff spoiler (T 0 , A0 ) q (T , , ) , A0
satisfiable w.r.t. 0 . Given CQ q, signature TBox , thus decide emptiness
q given follows:
1. compute , ;
2. guess TBox 0 ABox A0 satisfy Conditions 1 4 Lemma 46 KB
K = (T , , ) role assertion r(a, b) , r(a, b) A0 ;
3. verify (T 0 , A0 ) spoiler q (T , , );
4. verify , A0 satisfiable w.r.t. 0 .
43

fiBAADER , B IENVENU , L UTZ , & W OLTER

remains argue yields NE XP IME algorithm. already noted, Step 1 carried
(deterministic) exponential time. Due Conditions 1 4 Lemma 46 since ,
size exponential , TBox 0 ABox A0 guessed Step 2
size exponential , size polynomial q. Step 3 implemented
straightforward iteration fork rewritings q 0 q splittings q 0 w.r.t. (T , , ),
requires exponential time.
thus remains deal Step 4. Let closure single negations union
following sets concepts:
NC ;
concepts occur (possibly subconcepts);
concept names occur q;
concepts Cq r.Cq subconcepts, q DTrees(q ), q maximal fork
rewriting q, r occurs q.
Based remark Lemma 46, easy verify (and crucial argument) ||
polynomial , , q. -type set model 0
, = {C | C }. Section 4, introduce notion
coherence types: say pair -types (t, t0 ) r-coherent, denoted ;r t0 ,
r.C , C t0 implies r.C t. set -types computed E XP IME.
verify satisfiability , A0 w.r.t. 0 , guess map : Ind(AT , ) , accept
following two conditions satisfied reject otherwise:
(i) C(c) , A0 implies C (c)
(ii) r(b, c) , A0 , C (c), r.C imply r.C (b).
Clearly, checking whether two conditions satisfied done single exponential time.
thus remains argue , A0 satisfiable w.r.t. 0 case exists
map verifying conditions. First note given model KB (T 0 , , A0 ),
define desired map setting (c) = {C | cI C }. Conversely, given map
satisfying conditions (i) (ii), define interpretation follows:
=

AI = {t | t}

rI = {(t, t0 ) | ;r t0 }

cI = (c)

readily verified C , C iff C . this,
show, using similar argument given Lemma 10, model (T 0 , , A0 ).
complete proof undecidability result (Theorem 19) proving Lemma 20.
Lemma 20 (T, H, V ) admits tiling iff -ABox satisfiable w.r.t.
, |= v A(v).
Proof. (Only if) Straightforward. Consider tiling f : {0, . . . , n} {0, . . . , m}
(T, H, V ). Create individuals ai,j 0 n 0 j m, consider ABox
composed following assertions:
44

fiQ UERY P REDICATE E MPTINESS

x(ai,j , ai+1,j ) 0 < n 0 j
x (ai+1,j , ai,j ) 0 < n 0 j
y(ai,j , ai,j+1 ) 0 j < 0 n
(ai,j+1 , ai,j ) 0 j < 0 n
Th (ai,j ) f (i, j) = Th .
easily verified satisfiable w.r.t. satisfies , |= v A(v).
(If) Let -ABox satisfiable w.r.t. , |= v A(v). first show
Ix Iy enforce x inverse x inverse y, respectively, C
forces relevant grid cells closed. r {x, y} call Ind(A) r-defect exists
b Ind(A) r(a, b) r (b, a) 6 A. call inv-defect x-defect
y-defect. call Ind(A) cl-defect exist x(a, b), y(a, c), y(b, d), x(c, e)
6= e inv-defect, b y-defect c x-defect.
Claim 1. exists model Ind(A):
(d1) aI 6 IrI , r-defects Ind(A) r {x, y};
(d2) aI 6 C , cl-defects Ind(A).
Moreover, satisfies following conditions Ind(A), role names r, h {1, . . . , p}:
1. = Ind(A);
2. aI = a;
3. (a, a0 ) rI implies r(a, a0 ) A;
4. ThI implies Th (a) A.
Proof Claim 1. Let r {x, y}. Call two-element set {a, b} r-defect witness exists
c Ind(A) r(a, c), r (c, b) A. Consider undirected graph G nodes Ind(A)
set r-defect witnesses edges. Note G degree two (since r r
functional). Hence G three-colorable. Choose three coloring G colors Br,1 = Zr,1 uZr,2 ,
Br,2 = Zr,1 u Zr,2 Br,3 = Zr,1 u Zr,2
concept names
Sand choose interpretation



Zr,1 , Zr,2 correspondingly. set Ir = i=1,2,3 (Br,i u r.r .Br,i ) .
Call two-element set {d, e} cl-defect witness exist x(a, b), y(a, c), y(b, d), x(c, e)
inv-defect, b y-defect c x-defect. Consider undirected
graph G nodes Ind(A) set cl-defect witnesses edges. Note G degree
two (again since x, x , y, functional). Hence G three-colorable colors
C1 = Zc,1 u Zc,2 , C2 = Zc,1 u Zc,2 C3 = Zc,1 u Zc,2
choose interpretation
concept names Zc,1 , Zc,2 correspondingly. set C = i=1,2,3 (x.y.Ci u y.x.Ci )I .
Since neither existential restrictions concept names Th occur right-hand side
CIs , hard verify interpret remaining concept names
way additional conditions satisfied. (End proof claim)
45

fiBAADER , B IENVENU , L UTZ , & W OLTER

Let model satisfying conditions Claim 1. additionally assume w.l.o.g.
A, -minimal: model J satisfying conditions Claim 1
AJ AI J least one inclusions proper.
Let aA AI . exhibit grid structure gives rise tiling (T, H, V ).
start identifying diagonal starts aA ends instance Tfinal .
Claim 2. set G = {r1 (ai0 ,j0 , ai1 ,j1 ), . . . , rk1 (aik1 ,jk1 aik ,jk ), Tfinal (aik ,jk )}

i0 = 0, j0 = 0, a0,0 = aA ;
1 ` < k, either (i) r` = x, i`+1 = i` + 1, j`+1 = j` (ii) r` = y,
j`+1 = j` + 1, i`+1 = i` .
Proof claim. sequence, convert new model J
interpreting false points reachable (equivalently: A) aA setting
AJ = AI \ {aA }, contradicts A, -minimality I. (End proof claim)
Let n number occurrences role x ABox G Claim 1 number
occurrences y. next show
Claim 3.
.
(a) a0,0 Tinit

(b) ai,j RI implies = n;
(c) ai,j U implies j = m;
(d) ai,j ai,j Ind(G);
(e) ai,j Ind(G), (unique) Th ai,j ThI , henceforth denoted Ti,j ;
(f) (Ti,j , Ti+1,j ) H ai,j , ai+1,j Ind(G) (Ti,j , Ti,j+1 ) V ai,j , ai,j+1
Ind(G).
Proof claim. Point (a) easy consequence fact a0,0 = aA , aA AI ,
A, -minimal. (b), first note unique ` k = n {`, . . . , k}
< n {0, . . . , ` 1}. Due CI R v x., ai`1 ,j`1
/ RI . show

ais ,js
/ R < ` 1, suffices use CIs R v x. R v y.R. proof
(c) similar. prove (d)-(f) together, showing induction ` (d)-(f) satisfied
initial parts
G` := {r1 (ai0 ,j0 , ai1 ,j1 ), . . . , r`1 (ai`1 ,j`1 ai` ,j` )}
G, ` k. base case, ai0 ,j0 = aA AI clearly implies ai0 ,j0 , thus (d)
satisfied. Point (e) follows (a) disjointness tiles expressed . Point (f)
vacuously true since single individual G0 . induction step, assume G`1
satisfies (d)-(f). distinguish four cases:
ai`1 ,j`1 (U u R)I .
46

fiQ UERY P REDICATE E MPTINESS

Since G`1 satisfies (d), ai`1 ,j`1 , definition A, minimality together fact ai`1 ,j`1 (U u R)I ensure
ai`1 ,j`1 (x.(Tg u u y.Y ) u y.(Th u u x.Y ) u Ix u Iy u C u Tf )I
(Tf , Tg ) H (Tf , Th ) V . Using functionality x y, easy
show G` satisfies (d)-(f).
ai`1 ,j`1 (U u R)I .
Since ai`1 ,j`1 RI , ensures x-successor ai`1 ,j`1 I. Moreover,
ai`1 ,j`1 . Together definition , get
ai`1 ,j`1 (y.(Tg u u R) u Iy u Tf )I
(Tf , Tg ) V . must i` = i`1 , j` = j`1 + 1, r`1 = y. Using
functionality y, easy show G` satisfies (d)-(f).
ai`1 ,j`1 (U u R)I .
Analogous previous case.
ai`1 ,j`1 (U u R)I .
neither x-successor y-successor ai`1 ,j`1 (U u R)I . follows
` 1 = k, contradiction ` k.
(End proof claim)
Next, extend G full grid Conditions (a)-(e) Claim 3 still satisfied.
achieved, trivial read solution tiling problem. construction grid
consists exhaustive application following two steps:
1. x(ai,j , ai+1,j ), y(ai+1,j , ai+1,j+1 ) G ai,j+1 Ind(G) y(ai,j , ai,j+1 )
G x(ai,j+1 , ai+1,j+1 ) G, identify ai,j+1 Ind(A) y(ai,j , ai,j+1 )
x(ai,j+1 , ai+1,j+1 ) add latter two assertions G.
2. y(ai,j , ai,j+1 ), x(ai,j+1 , ai+1,j+1 ) G ai+1,j Ind(G) x(ai,j , ai+1,j )
G y(ai+1,j , ai+1,j+1 ) G, identify ai+1,j Ind(A) x(ai,j , ai+1,j )
y(ai+1,j , ai+1,j+1 ) add latter two assertions G.
hard see exhaustive application rules yields full grid, i.e., final G
(i) Ind(G) = {ai,j | n, j m}, (ii) x(ai,j , ai0 ,j 0 ) G iff i0 = + 1 j = j 0 ,
(iii) y(ai,j , ai0 ,j 0 ) G iff = i0 j 0 = j + 1.
Since two steps construction completely analogous, deal Case 1
detail. Thus let x(ai,j , ai+1,j ), y(ai+1,j , ai+1,j+1 ) G ai,j+1
/ Ind(G). Clearly, < n
j < m. (b) (c), thus ai,j
/ (R U )I . Since ai,j (d)
A, -minimal, get
ai,j (x.(Tg u u y.Y ) u y.(Th u u x.Y ) u Ix u Iy u C u Tf )I
(Tf , Tg ) H (Tf , Th ) V . together minimality means
select ai,j+1 , b Ind(A) y(ai,j , ai,j+1 ), x(ai,j+1 , b) A, ai,j+1 , b , Ti,j+1 =
47

fiBAADER , B IENVENU , L UTZ , & W OLTER

Th . choice, (a), (d), (e), second half (f) clearly satisfied. get properties
required Step 1 above, show b = ai+1,j+1 . show this,
satisfaction (b) (c) apply construction step, CIs
R v x. R v y.R

U v y. U v x.U

ensure (b) (c) still satisfied construction step. Showing b = ai+1,j+1 also
give us first half (f). Finally, prove b = ai+1,j+1 sufficient show ai,j
cl-defect Ind(A). follows Claim 1 since ai,j C , ai,j IxI IyI , ai+1,j IyI ,
ai,j+1 IxI .
use completed grid build solution tiling problem: tile point
(i, j) unique tile satisfied ai,j Ind(A). Property (f) Claim 2
correctness grid construction ensure adjacent tiles satisfy vertical horizontal
constraints.


Appendix C. Proofs Section 5
Theorem 22. EL, CQ-query emptiness decided PT IME.
Proof. Lemma 21, suffices show n-ary CQ q alphabet , decided
PT IME whether , |= q[a , . . . , ] total -ABox. First note
, |= q[a , . . . , ] iff , Ab |= qb,
Ab obtained adding assertion X(a ), X concept name
occur , , q;
qb Boolean CQ obtained q adding conjunct X(v) answer variable v
quantifying away answer variables.
Recall discussion Lemma 44 every CQ q viewed directed graph
Gdq . say Boolean CQ q directed forest-shaped disjoint union directed treeshaped Boolean CQs. Every Boolean CQ q directed forest-shaped corresponds concept
Cq description logic ELu extends EL universal role u , |= q iff
, |= C(a) Ind(A) (Lutz & Wolter, 2010). Checking latter condition possible
PT IME (Lutz & Wolter, 2010). Thus, sufficient convert qb polynomial time directed
forest-shaped CQ qb0 , Ab |= qb iff , Ab |= qb0 .
construct qb0 qb, exhaustively apply following rewriting rules:
1. r(v, v 00 ) r(v 0 , v 00 ) query, identify v v 0 replacing occurrences
v 0 v;
2. r(v 0 , v) s(v 00 , v) query (with r 6= s), identify v, v 0 , v 00 replacing
occurrences v 0 v 00 v;
3. cycle r0 (v0 , v1 ), . . . , rn1 (vn1 , vn ), vn = v0 query {v0 , . . . , vn1 } contains
least two variables, identify variables v0 , . . . , vn1 replacing occurrences
v1 , . . . , vn1 v0 .
48

fiQ UERY P REDICATE E MPTINESS

resulting query contains reflexive loop r(v, v) r
/ , immediately return
no. Otherwise, replace final step reflexive loop r(v, v) r X(v).
query resulting last step qb0 . easy see query obtained point
directed forest-shaped since every variable one predecessor cycles
corresponding directed graph.
prove correctness algorithm, first establish following claim:
Claim. qb0 defined, , Ab |= qb iff , Ab |= qb0 .
suffices prove rule application preserves (non)entailment query Ab .
preliminary, recall that, shown Lutz Wolter (2010), exists materialization
JT ,Ab (T , Ab ) directed tree-shaped interpretation individual root
(potentially) additional reflexive loops added root (an interpretation directed treeshaped corresponding CQ domain elements interpretation regarded
variables directed tree-shaped). Assume rewriting rule 1 applied query p resulting
query p0 . clear , Ab |= pb0 implies , Ab |= pb. converse, assume , Ab |= pb
let JT ,Ab materialization Ab introduced above. match p
JT ,Ab . Since JT ,Ab contain domain elements d, d0 , d00 6= d0
J

J

role name r, (d, d00 ) r ,Ab (d0 , d00 ) r ,Ab , match p JT ,Ab must map
identified variables v v 0 domain element thus also match p0 .
two rules replacement r(v, v), r , X(v) dealt similar way.
claim, substitute qb qb0 intended. Moreover, easy see
, Ab 6|= qb algorithm returns due reflexive loop r(v, v) r
/ : simply use
interpretation JT ,Ab proof claim.

Proposition 28. every Horn-ALCIF TBox , ABox signature , CQ q, one construct
polynomial time ELIF -TBox 0 normal form q empty given iff q
empty given 0 .
Proof. proof similar reductions provided work Hustadt et al. (2007)
Kazakov (2009). Nevertheless, Kazakov considers reductions preserving subsumption only,
Hustadt, Motik, Sattler also Kazakov reduce ELIF TBoxes,
give detailed proof.
following rules used rewrite ELIF -TBox normal form (all freshly
introduced concept names sig(T ) sig(q). Assume L v R given.
L form L1 u L2 R concept name, take fresh concept name
replace L v R L v v R. R concept name, either L1 L2
concept names, take fresh concept names A1 , A2 replace L v R L1 v A1 ,
L2 v A2 A1 u A2 v R;
L form L1 L2 R concept name, replace L v R L1 v R
L2 v R. Otherwise take fresh concept name replace L v R L v v R;
L form r.L0 L0 concept name, take fresh concept name A0
replace L v R L0 v A0 r.A0 v R;
R form A, replace L v R L u v ;
49

fiBAADER , B IENVENU , L UTZ , & W OLTER

R form R1 u R2 L concept name, take fresh concept name
replace L v R L v v R. Otherwise take fresh concept names A1 , A2
replace L v R L v A1 , L v A2 , A1 v R1 , A2 v R2 ;
R form L0 R0 , replace L v R L u L0 v R0 ;
R form r.R0 R0 concept name, take fresh concept name A0
replace L v R L v r.A0 A0 v R0 ;
R form r.R0 , replace L v R r .L v R.
resulting TBox 0 required. particular, every -ABox model 0 ,
also model ; conversely, every model extended
model appropriately interpreting fresh concept names. Consequently,
certT (q, A) = certT 0 (q, A) thus q empty given iff q empty given 0 .

Proposition 30. Let ELIF -TBox, ABox signature, q CQ. q non-empty
given , witnessed -ABox forest-shaped, width |q|,
degree |T |.
Proof. Assume q answer variables v1 , . . . , vn non-empty given .
find -ABox satisfiable w.r.t. certT ,A (q) 6= . identify forestshaped witness non-emptiness q given , consider canonical model ,A
(T , A). construction, ,A consists ABox part I0 , restriction ,A
Ind(A), tree-shaped interpretations Ia , Ind(A), rooted containing ABox
individuals. Since ,A universal, match q ,A . Let consist individuals
Ind(A) v var(q) (v) Ia (possibly (v) = a). Let A0
ABox obtained restricting individuals ; going root component
forest-shaped witness seeking define (observe |Ind(A0 )| |q|). add
tree components, consider, , (typically infinite) tree-shaped ABox Aua
obtained unraveling starting a, work Lutz Wolter (2012):
Ind(Aua ) set sequences = c0 r0 c1 . . . rm1 cm c0 , . . . , cm Ind(A)
r0 , . . . , rm1 (possibly inverse) roles (i) c0 = a, (ii) c1 6 , (iii) rj (cj1 , cj )

0 j < m, (iv) (cj1 , rj1
) 6= (cj+1 , rj ) j > 0; say copy
cm ;
A(c) Ind(Aua ) copy c, A() Aua ;
Ind(Aua ) copy c, = rc0 Ind(Aua ), r(, ) Aua ;
Ind(Aua ) copy c, = r c0 Ind(Aua ), r(, ) Aua .
let Ab union A0 tree-shaped ABoxes {Aua | }. Observe Conditions
b Note
(ii) (iv) first item since satisfies functionality statements , A.
b
forest-shaped, need neither finite degree |T |; going fix
later.
next aim show Ab satisfiable w.r.t. certT ,Ab(q) 6= . end,
construct universal model J Ab . Start Ab viewed interpretation J0 ,
50

fiQ UERY P REDICATE E MPTINESS

construction canonical models. take, Ind(A) copies
b copy tree interpretation Ia (i) root , (ii) J0 = {},
Ind(A),
(iii) 6= implies disjointness , (iv) = identical original
tree interpretation Ia (and copy). d0 result renaming Ia , d0
called copy d. desired interpretation J obtained taking union J0 .
Note every element J copy element ,A , that, construction, J
b
model A.
straightforward show induction structure C every ELI-concept C
every element e J copy ,A , e C J iff C ,A . Since ,A
model , follows J model thus Ab satisfiable w.r.t. . sketch
proof J universal. Let model Ab . start define homomorphism h0
b remains extend h0 components
J setting h0 (a) = aI Ind(A).
J . copy tree interpretation Ia ,A copy a. shown
work Lutz Wolter (2012) that4
b copy Ind(A), AIT ,A implies , Ab |= A() concept
() Ind(A)
names A.
Recall ,A generated derivation rules building canonical models. Using
straightforward induction number rule applications exploiting () fact
normal form, one construct homomorphism ha Ia h(a) = .
renaming, obtain homomorphism h h () = . desired
homomorphism h union h0 h . thus established J universal. Going
construction J (and particular using Point (iv)), verified match
q ,A also match q J . Since J universal, yields certT ,Ab(q) 6= desired.
want remove individuals Ab resulting ABox degree |T |
still witnesses non-emptiness q given . Since J universal, homomorphism h J canonical model ,Ab. Composing match h, obtain match
q ,Ab sends every variable individual A0 element tree
individual. inductively mark individuals Ab relevant match , starting
individuals A0 proceeding follows: whenever Rule 2 4 adds marked individual
x AIT ,Ab construction ,Ab presence (x, y) rIT ,A (please see
formulation mentioned rules), mark y. verified every individual outside
A0 one marked neighbor existential restriction . (potentially infinite) forest-shaped ABox Abd obtained Ab dropping assertions involve least one
unmarked individual thus degree |T |. Moreover, marking construction ensures
canonical model ,Abd contains A0 interpretation Ia , Ind(A0 ), hence match
q ,Abd .
point, ABox Abd almost required forest witness, except may infinite.
remains invoke compactness obtain finite subset Abf Abd certT ,Abf (q) 6= .
Clearly, Abf contains forest witness non-emptiness q given .

4. Lutz Wolter (2012) actually show case root component A0 Ab start
unravel consists individual names A, contains concept role assertions; proof also goes
case.

51

fiBAADER , B IENVENU , L UTZ , & W OLTER

following lemmas establish two statements Lemma 34.
Lemma 47 Every canonical proper R N -labeled tree well-founded.
Proof. Let hT, `i canonical proper R N -labeled tree, let I0 , I1 , . . . interpretations encountered construction canonical model AhT,`i . Since hT, `i
canonical, IhT,`i canonical model AhT,`i .
slightly abuse terminology using term concept atom refer statements
form B(e) B concept name (or >) e domain element. role atom take
form r(e, e0 ) r role e, e0 domain elements. say concept atom B(e) (resp.
role atom r(e, e0 )) interpretation J e B J (resp. (e, e0 ) rJ ). atom
IhT,`i , rank smallest Ii . show induction rank
every concept atom IhT,`i derivation, thus hT, `i well-founded.
induction start straightforward concept atoms I0 involve concept {>}
element x either x Ind(A) = `() x \ {} `(x),
every atom derivation depth 0. induction step, let B(x) concept atom
Ii+1 \ Ii . consider rule application resulted addition B(x):
1. Assume B(x) Ii+1 application Rule 1, is, A1 u u v
B x AIj 1 j n.
every 1 j n, atom Aj (x) rank i, IH, derivation
hTj0 , `0j Aj x. obtain derivation hT 0 , `0 B x setting 0 = {} {jw | w
Tj0 }, `0 () = (B, x), `0 (jw) = `0j (w).
2. Assume B(x) Ii+1 application Rule 2, is, r.A v B
x (r.A)Ii .
x (r.A)Ii , must exist Ii (x, y) rIi AIi .
atom A(y) rank i, IH, derivation hT 00 , `00 y.
x I0 , x Ind(AhT,`i ), define derivation hT 0 , `0 B x setting
0 = {} {1w | w 00 }, `0 () = (B, x), `0 (1w) = `00 (w).
Next consider case x 6 I0 . x 6 Ind(AhT,`i ), properness ,
concept E NC {>} E `(x). Since x 6 I0 x 6 Ii ,
0 < j < x Ij \ Ij1 . Since hT, `i canonical, element x created
due application Rule 3 using concept inclusion form F v s.E, x E Ij .
Applying IH, obtain derivation hT 000 , `000 E x. thus define derivation
hT 0 , `0 B x setting 0 = {} {1w | w 00 } {2w | w 000 }, `0 () = (B, x),
`0 (1w) = `00 (w), `0 (2w) = `000 (w).
3. Assume B(x) Ii+1 application Rule 3 involving v r.B ,
is, Ii AIi (y, x) rIi+1 \ rIi .
atom A(y) rank i, IH, exists derivation hT 00 , `00 y.
Moreover, since x created applying inclusion v r.B y, second
condition canonicity ensures B `(x). thus define derivation B x
taking tree hT 0 , `0 0 = {} {1w | w 00 }, `0 () = (B, x), `0 (1w) = `00 (w).
52

fiQ UERY P REDICATE E MPTINESS

4. Assume B(x) = >(x) Ii+1 application Rule 3 involving v
r.E (E 6= >), is, Ii AIi (y, x) rIi+1 \ rIi .
atom A(y) rank i, IH, exists derivation hT 00 , `00 y.
Moreover, since x created applying inclusion v r.E y, second
condition canonicity ensures E `(x). thus define derivation > x
taking tree hT 0 , `0 0 = {, 1} {11w | w 00 }, `0 () = (>, x), `0 (1) = (E, x),
`0 (11w) = `00 (w).
5. Assume B(x) Ii+1 application Rule 4, is, v r.B ,
funct(r) , AIi , (y, x) rIi .
atom A(y) rank i, IH, exists derivation hT 00 , `00 y.
x I0 , obtain derivation B x taking tree hT 0 , `0 0 = {} {1w |
w 00 }, `0 () = (B, x), `0 (1w) = `00 (w). x 6 I0 , use
argument Point 2 find derivation hT 000 , `000 E x. obtain derivation
hT 0 , `0 B x setting 0 = {} {1w | w 00 } {2w | w 000 }, `0 () = (B, x),
`0 (1w) = `00 (w), `0 (2w) = `000 (w).

Lemma 48 Let hT, `i proper R N -labeled tree well-founded IhT,`i
model . IhT,`i universal model AhT,`i .
Proof. Assume hT, `i well-founded proper R N -labeled tree IhT,`i model
. obligation pair (A, x) x `(x). every obligation
(A, x), choose derivation hTA,x , `A,x x hT, `i minimal depth. obligations (A1 , x1 ),(A2 , x2 ), write (A1 , x1 ) (A2 , x2 ) (A1 , x1 ) occurs node label
hTA2 ,x2 , `A2 ,x2 i.
Claim. relation acyclic.
Proof claim. Assume contrary obligations (A0 , x0 ), . . . , (An , xn )
(Ai , xi ) (Ai+1 , xi+1 ) n (An+1 , xn+1 ) := (A0 , x0 ). may assume without loss generality 0 < j n, (Ai , xi ) 6= (Aj , xj ), i.e., obligations
(A0 , x0 ), . . . , (An , xn ) pairwise distinct. Let ki depth hTAi ,xi , `Ai ,xi `i
depth shallow derivation (Ai , xi ) contained hTAi+1 ,xi+1 , `Ai+1 ,xi+1 i.
hTAi ,xi , `Ai ,xi minimal depth, ki `i . Moreover, clearly also `i ki+1 .
thus shown k0 = `0 = = kn = `n . Consequently, derivation (A0 , x0 )
hTA1 ,x1 , `A1 ,x1 must start root hTAi ,xi , `Ai ,xi i, implies (A0 , x0 ) = (A1 , x1 )
contradiction fact obligations distinct. finishes proof claim.
claim, assume w.l.o.g. chosen derivation hTA,x , `A,x i, node
labeled (B, y), subtree hTA,x , `A,x rooted node chosen derivation
hTB,y , `B,y (uniformity assumption).
prove IhT,`i universal model AhT,`i , take model AhT,`i .
show homomorphism h IhT,`i I, constructing h step-by-step fashion.
start, set h(a) = aI individual names occur AhT,`i . extension
h, argue
53

fiBAADER , B IENVENU , L UTZ , & W OLTER

1. x AIhT,`i concept name, h(x) defined hTA,x , `A,x uses elements
domain h, h(x) AI ;
2. (x, y) rIhT,`i r role h(x), h(y) defined, (h(x), h(y)) rI ;
3. (x, y) rIhT,`i , child x , h(y) defined, h(x) also defined.
start observing initial mapping h, Point 2 trivial since model AhT,`i
role edges restriction IhT,`i domain h AhT,`i . Point 3, use
fact individual AhT,`i x parent , properness implies
x also individual AhT,`i (and hence x belongs domain h).
Point 1 proved induction depth hTA,x , `A,x i. induction start, consider
depth zero. {>}, x Ind(AhT,`i ), A(x) AhT,`i . Since model AhT,`i
definition h, h(x) AI .
induction step. Assume hTA,x , `A,x uses elements domain h.
definition derivations gives rise following cases:
6 `(x), CI A1 u u v 1 n, child
z 0 z TA,x `A,x (z 0 ) = (Ai , x).
1 n, let zi child z `A,x (zi ) = (Ai , x). subderivation
hTA,x , `A,x rooted zi chosen derivation hTAi ,x , `Ai ,x Ai x. follows
hTAi ,x , `Ai ,x uses elements domain h depth strictly smaller
hTA,x , `A,x i. therefore apply induction hypothesis get h(x) AIi . Since
model A1 u u v , obtain h(x) AI .
6 `(x), CI r.A0 v child z 0 z TA,x `A,x (z 0 ) = (A0 , x0 )
(x, x0 ) rIhT,`i .
subderivation hTA,x , `A,x rooted z 0 chosen derivation hTA0 ,x0 , `A0 ,x0 A0
x0 , thus contains elements domain h strictly smaller depth
hTA,x , `A,x i. thus use IH infer h(x0 ) B , use Point 2
get (h(x), h(x0 )) rI . Since model r.A0 v , h(x) AI .
6 `(x), CI A0 v r.A funct(r) child z 0 z TA,x
`A,x (z 0 ) = (A0 , x0 ) (x0 , x) rIhT,`i .
previous item, use IH Point 2 get h(x0 ) B (h(x), h(x0 ))
rI . Since model contains A0 v r.A funct(r), follows
h(x) AI .
= >, >
/ `(x), B `(x),and child z TA,x `A,x (z) = (B, x).
case applicable since B `(x), 6 `(x), hence x domain
h.
`(x), CI A0 v r.A , child z 0 z TA,x `A,x (z 0 ) =
(A0 , x0 ) (x0 , x) rIhT,`i either (i) x child x0 , (ii) x child
root, x0 Ind, {r, x0 } `(x).
case applicable since `(x), 6 `(x), hence x domain
h.
54

fiQ UERY P REDICATE E MPTINESS

extend h, first show h yet total, exists edge (b
x, yb) rIhT,`i

concept name h(b
x) defined, h(b
) undefined, `(b
) (and consequently
`(b
)), hTA,by , `A,by elements except root node domain
h.
Assume contrary h total edge, i.e., every edge (b
x, yb)
rIhT,`i h(b
x) defined, h(b
) undefined, `(b
), derivation hTA,by , `A,by
contains non-root node domain h. Pick one edge (b
x, yb) rIhT,`i
associated derivation hTA,by , `A,by minimal depth. Since h(b
x) defined h(b
)
undefined, follows Point 3 either x
b parent yb , yb child root node
{b
x, r} `(b
). Since derivation rule 6 applicable rule node label contains

formulation rule, must thus CI A0 v r.A unique
child z TA,by satisfies `A,by (z) = (A0 , x
b). Since x
b domain h, non-root node
domain h must somewhere z. Consequently, find nodes z1 , z2 TA,by
z2 successor z1 domain element x
b0 `A,by (z1 ) domain h,
0
domain element yb `A,by (z2 ) domain h. definition derivation rules,
must (b
x0 , yb0 ) sIhT,`i role s, Point 3, either yb0 child x
b0 , yb0
0
0
child root node label contains {b
x , s}. follows x
b related yb0 one
derivation rules 4 5. Consequently, B `(b
0 ) hTA,by , `A,by contains
IhT,`i
0
0
0
obligation (B, yb ). Thus, edge (b
x , yb )
satisfies conditions associated
derivation hTB,by0 , `B,by0 strictly smaller depth hTA,by , `A,by i, contradicting minimality
hTA,by , `A,by i.
extend h using edge (b
x, yb) rIhT,`i whose existence established.
definition derivations, CI A0 v r.A child z TA,by
`A,by (z) = (A0 , x
b). Since elements hTA,by , `A,by except root node domain h,
subderivation hTA,by , `A,by rooted z uses elements domain h.
uniformity assumption, derivation hTA0 ,bx , `A0 ,bx i, thus IH yields h(b
x) A0 IhT,`i .
IhT,`i
IhT,`i
0
Since v r.A , (b
x, d) r

. Set h(b
) = d.
remains show Points 1 2 satisfied extended h (Point 3 obviously is).
start Point 2. Assume (x, y) sIhT,`i . h(x) h(y) defined already
extension h, done. Otherwise, construction h must (x, y, s) = (b
x, yb, r)
(or (x, y, s) = (b
y, x
b, r ), equivalent). choice h(b
), (h(b
x), h(b
)) rI ,
hence (x, y) sI . Point 1 proved induction depth A(x) initial version
h. induction start exactly same, induction step, cases differ
following ones:

/ `(x), B `(x), = >, child z TA,x `A,x (z) = (B, x).
Immediate since = >.
`(x), CI A0 v r.A , child z 0 z TA,x `A,x (z 0 ) =
(A0 , x0 ) (x0 , x) rIhT,`i either (i) x child x0 , (ii) x child
root, x0 Ind, {r, x0 } `(x).
Since `(x), x 6 Ind(AhT,`i ), x must introduced domain
h examination edge (x0 , x) rIhT,`i . Since child z 0 labeled (A0 , x0 ),
use CI A0 v r.A choose h(x) h(x) AIhT,`i .

55

fiBAADER , B IENVENU , L UTZ , & W OLTER

References
Artale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). DL-Lite family
relations. Journal Artifical Intelligence Research (JAIR), 36, 169.
Baader, F., Bienvenu, M., Lutz, C., & Wolter, F. (2010). Query predicate emptiness description logics. Proceedings 12th International Conference Principles Knowledge
Representation Reasoning (KR).
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. Proceedings 19th
International Joint Conference Artificial Intelligence (IJCAI), pp. 364369.
Baader, F., Brandt, S., & Lutz, C. (2008). Pushing EL envelope further. Proceedings
Workshop OWL: Experiences Directions (OWLED).
Benedikt, M., Fan, W., & Geerts, F. (2008). XPath satisfiability presence DTDs. Journal
ACM, 55(2), 179.
Bienvenu, M., Hansen, P., Lutz, C., & Wolter, F. (2016). First-order rewritability conjunctive
queries Horn description logics. Proceedings 25th International Joint Conference
Artificial Intelligence (IJCAI).
Bienvenu, M., Lutz, C., & Wolter, F. (2012). Query containment description logics reconsidered.
Proceedings 13th International Conference Principles Knowledge Representation Reasoning (KR).
Bienvenu, M., ten Cate, B., Lutz, C., & Wolter, F. (2014). Ontology-based data access: study
disjunctive datalog, CSP, MMSNP. ACM Transactions Database System
(TODS), 39(4), 33.
Botoeva, E., Kontchakov, R., Ryzhikov, V., Wolter, F., & Zakharyaschev, M. (2014). Query inseparability description logic knowledge bases. Proceedings 14th International
Conference Principles Knowledge Representation Reasoning (KR).
Botoeva, E., Kontchakov, R., Ryzhikov, V., Wolter, F., & Zakharyaschev, M. (2016). Games
query inseparability description logic knowledge bases. Artificial Intelligence Journal
(AIJ), 234, 78119.
Bourhis, P., & Lutz, C. (2016). Containment monadic disjunctive datalog, mmsnp, expressive
description logics. Proceedings 15th International Conference Principles
Knowledge Representation Reasoning (KR).
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rodriguez-Muro, M., &
Rosati, R. (2009). Ontologies databases: DL-Lite approach. Tutorial Lectures
5th International Reasoning Web Summer School, Vol. 5689 Lecture Notes Computer
Science, pp. 255356. Springer.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable reasoning
efficient query answering description logics: DL-Lite family. Journal Automated
Reasoning (JAR), 39(3), 385429.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2013). Data complexity
query answering description logics. Artificial Intelligence Journal (AIJ), 195, 335360.
56

fiQ UERY P REDICATE E MPTINESS

Calvanese, D., De Giacomo, G., & Lenzerini, M. (1998). decidability query containment
constraints. Proceedings 17th ACM SIGACT-SIGMOD-SIGART Symposium
Principles Database Systems (PODS), pp. 149158.
Calvanese, D., Ortiz, M., Simkus, M., & Stefanoni, G. (2013). Reasoning explanations
negative query answers DL-Lite. Journal Artificial Intelligence Research (JAIR), 48,
635669.
Chortaras, A., Trivela, D., & Stamou, G. B. (2011). Optimized query rewriting OWL 2 QL.
Proceedings 23rd International Conference Automated Deduction (CADE), pp.
192206.
Eiter, T., Gottlob, G., Ortiz, M., & Simkus, M. (2008). Query answering description logic
Horn-SHIQ. Proceedings 11th European Conference Logics Artificial Intelligence (JELIA), pp. 166179.
Eiter, T., Ortiz, M., Simkus, M., Tran, T., & Xiao, G. (2012). Query rewriting Horn-SHIQ plus
rules. Proceedings 26th AAAI Conference Artificial Intelligence (AAAI).
Gabbay, D., Kurucz, A., Wolter, F., & Zakharyaschev, M. (2003). Many-Dimensional Modal Logics:
Theory Applications. Elsevier.
Gatens, W., Konev, B., & Wolter, F. (2014). Lower upper approximations depleting modules
description logic ontologies. Proceedings 21st European Conference Artificial
Intelligence (ECAI), pp. 345350.
Gene Ontology Consortium (2016). gene ontology. http://geneontology.org/. [Online; accessed
16-April-2016].
Glimm, B., Lutz, C., Horrocks, I., & Sattler, U. (2008). Answering conjunctive queries
SHIQ description logic. Journal Artificial Intelligence Research (JAIR), 31, 150197.
Golbeck, J., Fragoso, G., Hartel, F., Hendler, J., Oberthaler, J., & Parsia, B. (2003). national
cancer institutes thesaurus ontology. Journal Web Semantics: Science, Services
Agents World Wide Web, 1(1), 7580.
Grau, B. C., Horrocks, I., Kazakov, Y., & Sattler, U. (2008). Modular reuse ontologies: Theory
practice. Journal Artifical Intelligence Research (JAIR), 31, 273318.
Haase, C. (2007). Complexity subsumption extensions EL. Masters thesis, Dresden University Technology.
HITRL (2016). Health Information Technologies Research Laboratory. University Sydney.
http://sydney.edu.au/engineering/it/hitru. [Online; accessed 16-April-2016].
Horrocks, I., Kutz, O., & Sattler, U. (2006). even irresistible SROIQ. Proceedings
10th International Conference Principles Knowledge Representation Reasoning
(KR), pp. 5767.
Hustadt, U., Motik, B., & Sattler, U. (2004). decomposition rule decision procedures
resolution-based calculi. Proceedings 11th International Conference Logic
Programming Artificial Intelligence Reasoning (LPAR), pp. 2135.
Hustadt, U., Motik, B., & Sattler, U. (2007). Reasoning description logics reduction
disjunctive datalog. Journal Automated Reasoning (JAR), 39(3), 351384.
57

fiBAADER , B IENVENU , L UTZ , & W OLTER

IHTSDO (2016). SNOMED CT: global language healthcare. http://www.ihtsdo.org/snomedct. [Online; accessed 16-April-2016].
Kaminski, M., Schneider, T., & Smolka, G. (2011). Correctness worst-case optimality Prattstyle decision procedures modal hybrid logics. Proceedings 20th International Conference Automated Reasoning Analytic Tableaux Related Methods
(TABLEAUX), pp. 196210.
Kazakov, Y. (2009). Consequence-driven reasoning Horn-SHIQ ontologies. Proceedings
21st International Joint Conference Artificial Intelligence (IJCAI), pp. 20402045.
Kazakov, Y. (2010). extension complex role inclusion axioms description logic
SROIQ. Proceedings 5th International Joint Conference Automated Reasoning
(IJCAR), pp. 472486.
Konev, B., Ludwig, M., Walther, D., & Wolter, F. (2012). logical difference lightweight
description logic EL. Journal Artificial Intelligence Research (JAIR), 44, 633708.
Konev, B., Lutz, C., Walther, D., & Wolter, F. (2013). Model-theoretic inseparability modularity
description logic ontologies. Artificial Intelligence Journal (AIJ), 203, 66103.
Kontchakov, R., Rodriguez-Muro, M., & Zakharyaschev, M. (2013). Ontology-based data access
databases: short course. Proceedings International Reasoning Web Summer
School, pp. 194229.
Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2010). Logic-based ontology comparison
module extraction, application DL-Lite. Artificial Intelligence, 174(15), 1093
1141.
Krotzsch, M. (2012). OWL 2 profiles: introduction lightweight ontology languages. Tutorial Lectures 8th International Reasoning Web Summer School, Vol. 7487 Lecture
Notes Computer Science, pp. 112183. Springer.
Krotzsch, M., Rudolph, S., & Hitzler, P. (2007). Complexity boundaries Horn description logics.
Proceedings 22nd AAAI Conference Artificial Intelligence (AAAI), pp. 452457.
Levy, A. (1993). Irrelevance Reasoning Knowledge Based Systems. Ph.D. thesis, Stanford University.
Lubyte, L., & Tessaris, S. (2008). Supporting design ontologies data access. Proceedings 21st International Description Logic Workshop (DL).
Lutz, C. (2008). complexity CQ answering expressive description logics. Proceedings
4th International Joint Conference Automated Reasoning (IJCAR), pp. 179193.
Lutz, C., Toman, D., & Wolter, F. (2009). Conjunctive query answering description logic EL
using relational database system. Proceedings 21st International Joint Conference
Artificial Intelligence (IJCAI), pp. 20702075.
Lutz, C., & Wolter, F. (2010). Deciding inseparability conservative extensions description
logic EL. Journal Symbolic Computation, 45(2), 194228.
Lutz, C., & Wolter, F. (2012). Non-uniform data complexity query answering description
logics. Proceedings 13th International Conference Principles Knowledge
Representation Reasoning (KR).
58

fiQ UERY P REDICATE E MPTINESS

Motik, B., Grau, B. C., Horrocks, I., Wu, Z., Fokoue, A., & Lutz, C. (2009). OWL 2 Web Ontology Language: Profiles. W3C Recommendation. Available http://www.w3.org/TR/owl2profiles/.
Ortiz, M., Calvanese, D., & Eiter, T. (2008). Data complexity query answering expressive
description logics via tableaux. Journal Automated Reasoning (JAR), 41(1), 6198.
Ortiz, M., & Simkus, M. (2012). Reasoning query answering description logics. Proceedings 8th International Reasoning Web Summer School, Vol. 7487 Lecture Notes
Computer Science, pp. 153. Springer.
Ortiz, M., Simkus, M., & Eiter, T. (2008). Worst-case optimal conjunctive query answering
expressive description logic without inverses. Proceedings 23rd AAAI Conference
Artificial Intelligence (AAAI), pp. 504510.
Patel, C., Cimino, J. J., Dolby, J., Fokoue, A., Kalyanpur, A., Kershenbaum, A., Ma, L., Schonberg,
E., & Srinivas, K. (2007). Matching patient records clinical trials using ontologies.
Proceedings 6th International Semantic Web Conference (ISWC), pp. 816829.
Perez-Urbina, H., Motik, B., & Horrocks, I. (2009). comparison query rewriting techniques
dl-lite. Proceedings 22nd International Description Logic Workshop (DL).
Poggi, A., Lembo, D., Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2008). Linking
data ontologies. Journal Data Semantics, 10, 133173.
Pratt, V. R. (1979). Models program logics. Proceedings IEEE Annual Symposium
Foundations Computer Science (FOCS), pp. 115122.
Romero, A. A., Kaminski, M., Grau, B. C., & Horrocks, I. (2015). Ontology module extraction
via datalog reasoning. Proceedings 29th AAAI Conference Artificial Intelligence
(AAAI), pp. 14101416.
Tobies, S. (2001). Complexity Results Practical Algorithms Logics Knowledge Representation. Ph.D. thesis, RWTH Aachen.
Vardi, M. Y. (1989). Automata theory database theoreticans. Proceedings 7th ACM
SIGACT-SIGMOD-SIGART Symposium Principles Database Systems (PODS), pp. 83
92.
Vardi, M. Y. (1998). Reasoning past two-way automata. Proceedings 25th
International Colloquium Automata, Languages Programming (ICALP), pp. 628641.

59

fiJournal Artificial Intelligence Research 56 (2016) 693-745

Submitted 03/16; published 08/16

Qualitative Spatial Logics Buffered Geometries
Heshan Du

H.Du@leeds.ac.uk

University Leeds, UK

Natasha Alechina

Natasha.Alechina@nottingham.ac.uk

University Nottingham, UK

Abstract
paper describes series new qualitative spatial logics checking consistency
sameAs partOf matches spatial objects different geospatial datasets,
especially crowd-sourced datasets. Since geometries crowd-sourced data usually accurate precise, buffer geometries margin error level
tolerance R0 , define spatial relations buffered geometries. spatial logics
formalize notions buffered equal (intuitively corresponding possibly sameAs),
buffered part (possibly partOf), near (possibly connected) far (definitely disconnected). sound complete axiomatisation logic provided respect
models based metric spaces. logics, satisfiability problem shown
NP-complete. Finally, briefly describe logics used system generating debugging matches spatial objects, report positive experimental
evaluation results system.

1. Introduction
motivation work qualitative spatial logics comes needs integrating
disparate geospatial datasets, especially crowd-sourced geospatial datasets. Crowd-sourced
data involves non-specialists data collection, sharing maintenance. Compared
authoritative geospatial data, collected surveyors geodata professionals, crowd-sourced data less accurate less well structured, often provides richer
user-based information reflects real world changes quickly much lower cost
(Jackson, Rahemtulla, & Morley, 2010). interests national mapping agencies,
government organisations, users geospatial data able integrate
use different geospatial data synergistically.
Geospatial data matching refers problem establishing correspondences (matches)
spatial objects represented different geospatial datasets. essential step
data comparison, data integration enrichment, change detection data update.
Matching authoritative geospatial data crowd-sourced geospatial data non-trivial
task. Geometry representations location place different datasets usually exactly same. Objects also sometimes represented different levels
granularity. example, consider geometries objects Nottingham city centre given
Ordnance Survey Great Britain (OSGB) (2012) OpenStreetMap (OSM) (2012)
Figure 1. position shape Prezzo Ristorante represented differently
OSGB data (dotted) OSM data (solid). Victoria Shopping Centre represented
whole OSM, several shops OSGB.
c
2016
AI Access Foundation. rights reserved.

fiDu & Alechina

Figure 1: Prezzo Ristorante Victoria Shopping Centre represented OSGB (dotted)
OSM (solid)

order integrate datasets, need determine objects
(represent entity) sometimes objects one dataset parts objects
dataset (as example Victoria Shopping Centre). statements
representing two types relations referred sameAs matches partOf
matches respectively. One way produce matches use locations geometries
objects, although course also use lexical labels associated objects,
names restaurants etc. previous work (Du, Alechina, Jackson, & Hart, 2016),
present method generates matches using location lexical information
spatial objects. generated matches may contain errors, seen retractable
assumptions require validation checking. One way use logical reasoning
check consistency matches respect statements input datasets.
using description logic reasoning, correctness matches checked respect
classification information. example, wrong state spatial objects
b same, Bank b Clinic, concepts Bank Clinic
disjoint, containing common elements. However, sufficient validating
matches spatial objects1 . example, two spatial objects close
one dataset cannot matched two spatial objects far away apart
dataset, matter whether type not. Therefore, spatial
reasoning required validate matches regard location information, addition
description logic reasoning.
Spatial logic studies relations geometrical structures spatial languages describing (Aiello, Pratt-Hartmann, & van Benthem, 2007). variety
spatial relations, topological connectedness regions, relations based distances,
relations expressing orientations directions, etc. spatial logic, spatial relations
represented formal language, first order logic fragments, inter1. works (Lutz & Milicic, 2007) extending description logics concrete domains constraint systems, region connection calculus (RCC) (Randell, Cui, & Cohn, 1992) Allens
Interval Algebra (Allen, 1983). description logic reasoner Pellet (Sirin, Parsia, Grau, Kalyanpur,
& Katz, 2007) extended PelletSpatial (Stocker & Sirin, 2009), supports qualitative spatial
reasoning RCC. However, later show appropriate use RCC application.

694

fiQualitative Spatial Logics Buffered Geometries

preted structures based geometrical spaces, topological spaces, metric
spaces Euclidean spaces. field qualitative spatial reasoning, several spatial
formalisms developed representing reasoning topological relations,
Region Connection Calculus (RCC) (Randell et al., 1992), 9-intersection
model (Egenhofer & Franzosa, 1991) extensions (Clementini & Felice, 1997; Roy
& Stell, 2001; Schockaert, Cock, Cornelis, & Kerre, 2008b, 2008a; Schockaert, Cock, &
Kerre, 2009). addition, formalisms representing reasoning directional relations (Frank, 1991, 1996; Ligozat, 1998; Balbiani, Condotta, & del Cerro,
1999; Goyal & Egenhofer, 2001; Skiadopoulos & Koubarakis, 2004), well relative
absolute distances (Zimmermann, 1995; Clementini, Felice, & Hernandez, 1997; Wolter &
Zakharyaschev, 2003, 2005). Recent comprehensive surveys qualitative spatial representations reasoning provided Cohn Renz (2008) Chen, Cohn, Liu, Wang,
OuYang, Yu (2015).
Qualitative spatial reasoning shown applicable geospatial data (Bennett, 1996; Bennett, Cohn, & Isli, 1997; Guesgen & Albrecht, 2000; Mallenby, 2007; Mallenby & Bennett, 2007; Li, Liu, & Wang, 2013), location information spatial
objects comes single data source. application described paper different, location representations spatial object come different sources
usually exactly same. Rather treating differences geometric
representations logical contradictions, would tolerate slight geometric differences
treat qualitatively defined large differences logical contradictions used detecting
wrong matches. specifically, establishing matches two sets spatial
objects, set matches gives rise contradiction, match must wrong
retracted. addition, would provide explanations help users understand contradiction exists matches wrong. following,
assess appropriateness several existing spatial formalisms purposes.
Region Connection Calculus (RCC) (Randell et al., 1992) first order formalism
based regions connection relation C, axiomatised reflexive
symmetric. Two regions x, connected (i.e. C(x, y) holds), closures share
point. Based connection relation, several spatial relations defined regions.
Among them, eight jointly exhaustive pairwise disjoint (JEPD) relations identified:
DC (Disconnected), EC (Externally Connected), P (Partially Overlap), P P (Tangential
Proper Part), N P P (Non-Tangential Proper Part), P P (Inverse Tangential Proper
Part), N P P (Inverse Non-Tangential Proper Part) EQ (Equal). referred
RCC8, well-known field qualitative spatial reasoning.
9-intersection model developed Egenhofer Franzosa (1991) Egenhofer
Herring (1991) based point-set interpretation geometries. comparing
nine intersections interiors, boundaries exteriors point-sets, identifies 29
mutually exclusive topological relations. 9-intersection model provides comprehensive
formal categorization binary topological relations points, lines regions.
small number 29 relations realisable particular space (Egenhofer &
Herring, 1991). Restricting point-sets simple regions (regions homeomorphic disks),
512 relations collapse RCC8 relations.
described application, found difficult use spatial formalisms
Region Connection Calculus (Randell et al., 1992) 9-intersection model (Egenhofer
695

fiDu & Alechina

& Franzosa, 1991), since presuppose accurate geometries regions sharp boundaries define spatial relations based connection relation. strict
crowd-sourced geospatial data. shown Figure 2, a1 sameAs a2 , representing
Prezzo Ristorante; b1 sameAs b2 , referring Blue Bell Inn. Though sameAs
matches correct, topological inconsistency still exists, since a1 b1 disconnected
(DC), a2 b2 externally connected (EC), spatial relations DC EC
disjoint. Therefore, relations based connection strict crowd-sourced geospatial
data possibly inaccurate may contain errors.

Figure 2: OSGB data, Prezzo Ristorante (a1 ) Blue Bell Inn (b1 ) disconnected, whilst OSM data, (a2 b2 ) externally connected.
egg-yolk theory independently developed Lehmann Cohn (1994), Cohn
Gotts (1996b, 1996a), Roy Stell (2001) extending RCC theory
Clementini Felice (1996, 1997) extending 9-intersection model, order represent reason regions indeterminate boundaries. theory, region
indeterminate boundary (an indeterminate region) represented pair regions,
egg yolk, maximum extension minimum extension
indeterminate region respectively (similar upper approximation lower approximation rough set theory, Pawlak, Polkowski, & Skowron, 2007). yolk
empty always proper part egg. egg-yolk theory presupposes
existence core part region vague part. described application,
location represented using two disconnected polygons authoritative geospatial dataset crowd-sourced geospatial dataset respectively. case,
could define certain inner region disconnected polygons, otherwise,
inconsistent treat different representations location.
aware several approaches (Fine, 1975; Zadeh, 1975; Smith, 2008)
representing vague concepts relations, adopted extend classical
theories RCC 9-intersection model. main approach assign degree
truth degree membership concepts relations. example, fuzzy region
connection calculus (fuzzy RCC) (Schockaert et al., 2008b, 2008a, 2009), connection
relation C defined reflexive symmetric fuzzy relation. regions a, b, C(a, b)
denotes degree b connected. Using C primitive relation,
every RCC relation R redefined calculate degree R holds.
fuzzy RCC similar formalisms may applied case shown Figure 2.
example, appropriate membership function relation EC, EC(a1 , b1 ) = 0.8
696

fiQualitative Spatial Logics Buffered Geometries

Figure 3: Buffering geometry X distance ; three dashed circles buffered part
(BPT) solid circle; dashed circle solid circle buffered equal
(BEQ)

EC(a2 , b2 ) = 1, contradiction arise. adopt approach
matching problem, mainly good way define degree membership,
difficult generate user-friendly explanations matches wrong
underlying reasoning numerical relatively obscure.
logic S(M ) proposed developed Sturm, Suzuki, Wolter, Zakharyaschev (2000), Kutz, Sturm, Suzuki, Wolter, Zakharyaschev (2002), Kutz, Wolter,
Sturm, Suzuki, Zakharyaschev (2003), Wolter Zakharyaschev (2003, 2005),
Kutz (2007) reasoning distances. logic S(M ) makes possible define
concepts object within distance 100 meters School. S(M )
parameter set. typical example Q0 . satisfiability problem finite set
S(Q0 ) formulas metric space EXPTIME-complete (Wolter & Zakharyaschev,
2003). S(M ) developed problem geospatial data matching. However, designed logics introduced paper, discovered form
proper fragment S(Q0 ). detect problematic matches, also reason distances objects, reasoning restricted qualitative kind.
complexity satisfiability problem logics NP-complete, makes
somewhat suitable automatic debugging matches full S(Q0 ).
syntax semantics S(M ) proofs proper fragment relations
provided later paper (see Section 3).
paper, present series new qualitative spatial logics developed validating
matches spatial objects: logic NEAR FAR buffered points (LNF) (Du,
Alechina, Stock, & Jackson, 2013), logic NEAR FAR buffered geometries
(LNFS) logic part whole buffered geometries (LBPT) (Du & Alechina,
2014a, 2014b). notion buffer (ISO Technical Committee 211, 2003) used model
uncertainty geometry representations, tolerating slight differences margin
error level tolerance R0 . shown Figure 3, buffer geometry X
geometry contains exactly points within distance X. buffer
X denoted buffer (X , ). geometry X possibly represented inaccurately
within margin error one dataset, corresponding representation
dataset assumed somewhere within buffer (X , ).
spatial logics involve four spatial relations BufferedPartOf (BPT), BufferedEqual
(BEQ), NEAR FAR. formalize notions possibly partOf, possibly sameAs,
possibly connected (given possible displacement ) definitely disconnected
(even displaced ) respectively. geometry X BufferedPartOf geometry X 0 ,
X within buffer (X 0 , ); two geometries BufferedEqual, BufferedPartOf
697

fiDu & Alechina

Figure 4: NEAR FAR
(Figure 3). assume two geometries X X 0 two diferent
datasets may correspond object BufferedEqual. parameter
captures margin error representation geometries. Two geometries X,
NEAR, corresponding geometries X 0 , 0 dataset could connected,
i.e. distance(X, ) [0, 2] (Figure 4). Clearly, FAR(X , ) holds, NEAR(X , )
false X dataset. addition, want exclude
possibility NEAR(X 0 , 0 ) may hold X 0 , 0 (corresponding X, respectively)
dataset. Therefore define FAR(X , ) distance(X, ) (4, +) (Figure 4). possible two geometries X, NEAR FAR, is,
distance(X, ) (2, 4].
way defining BEQ, N EAR F AR similar defining distance
relations points Moratz Wallgrun (2012), point assigned one
reference distances. distance relations two points X, defined
comparing distance X, reference distances X .
different points different reference distances indicating nearness, distance
relations may symmetric. Differing work Moratz Wallgrun (2012),
relations defined points also general geometries, every
geometry reference distances (, 2 4), leads symmetric
definitions BEQ, N EAR F AR. provide sound complete sets axioms
support reasoning BEQ, BP , N EAR F AR relations (see Section 4). reasoning useful verifying matches spatial representations different sources.
explained previous work (Du et al., 2013), though relations named
N EAR F AR, attempt model human notions nearness proximity,
influenced several factors, absolute distance, relative distance, frame
reference, object size, travelling costs reachability, travelling distance attractiveness
objects (Guesgen & Albrecht, 2000). work, provide strict mathematical
definition calculation whether two objects considered N EAR
F AR, based margin error . makes approach less likely
suitable simulation human notions nearness, provides useful tool verifying
consistency matches. following arguments formalized checking consistency
sameAs partOf matches: spatial objects a1 , b1 sameAs partOf spatial objects
a2 , b2 respectively, a1 , b1 N EAR, a2 , b2 F AR, contradiction exists.
rest paper structured follows. Section 2, Section 3 Section 4 provide
introduction new spatial logics: syntax semantics, relationships
logic S(M ), axioms theorems. Section 5 Section 6 present
proofs soundness, completeness, decidability complexity theorems LBPT,
proofs LNF LNFS similar LBPT expressive LNF
LNFS. Section 7 describes LBPT used debugging matches objects
698

fiQualitative Spatial Logics Buffered Geometries

different geospatial datasets. Section 8 discusses generality limitations spatial
logics. Section 9 concludes paper.

2. Syntax Semantics
language L(LN F ) defined
, := BEQ(a, b) | N EAR(a, b) | F AR(a, b) | |
a, b individual names. def ( ). language L(LN F S)
exactly L(LN F ). language L(LBP ) almost L(LN F )
L(LN F S), except BP instead BEQ predicate. L(LBP ) defined
, := BP (a, b) | N EAR(a, b) | F AR(a, b) | | .
L(LN F ), L(LN F S) L(LBP ) interpreted models based metric
space. Every individual name involved LNF formula mapped point, whilst
involved LNFS/LBPT formula mapped arbitrary geometry
non-empty set points.
Definition 1 (Metric Space) metric space pair (, d), non-empty
set (of points) metric , i.e. function : R0 ,
x, y, z , following axioms satisfied:
1. identity indiscernibles: d(x, y) = 0 iff x = y;
2. symmetry: d(x, y) = d(y, x);
3. triangle inequality: d(x, z) d(x, y) + d(y, z).
Definition 2 (Metric Model LNF) metric model LNF tuple (, d, I, ),
(, d) metric space, interpretation function maps individual
name element , R0 margin error. notion |= (
true model ) defined follows:
|= BEQ(a, b) iff (I (a), (b)) [0 , ];
|= N EAR(a, b) iff (I (a), (b)) [0 , 2 ];
|= F AR(a, b) iff (I (a), (b)) (4 , );
|= iff 6|= ;
|= iff |= |= ,
a, b individual names, , formulas L(LN F ).
Definition 3 (Metric Model LNFS/LBPT) metric model LNFS/LBPT
tuple (, d, I, ), (, d) metric space, interpretation function
maps individual name non-empty set elements , R0 margin
error. notion |= ( true model ) defined follows:
699

fiDu & Alechina

|= BP (a, b) iff pa (a) pb (b) : (pa , pb ) [0 , ];
|= N EAR(a, b) iff pa (a) pb (b) : (pa , pb ) [0 , 2 ];
|= F AR(a, b) iff pa (a) pb (b) : (pa , pb ) (4 , );
|= iff 6|= ;
|= iff |= |= ,
a, b individual names, , formulas L(LN F S)/L(LBP ). BEQ(a, b)
defined BP (a, b) BP (b, a).
notions validity satisfiability metric models standard. formula
satisfiable true metric model. formula valid (|= ) true
metric models (hence negation satisfiable). logic LNF/LNFS/LBPT
set valid formulas language L(LN F )/L(LN F S)/L(LBP ) respectively.

3. Relationship logic S(M )
logic S(M ), well variations, developed Sturm et al. (2000), Kutz
et al. (2002, 2003), Wolter Zakharyaschev (2003, 2005), Kutz (2007) reasoning
distances.
S(M ) family logics defined relative parameter set Q0 .
subject following two conditions: a, b + b r, + b ,
r = sup bounded, otherwise r = ; a, b b > 0, b .
alphabet S(M ) consists
infinite list region variables X1 , X2 ,...;
infinite list location constants c1 , c2 ,...;
set constant {ci } every location constant ci ;
.
binary distance (), equality (=) membership () predicates;
boolean operators u, (and derivatives t, > );
two distance quantifiers <a , duals <a , , every ;
two universal quantifiers .
S(M ) terms defined as:
s, := Xi | {ci } | > | | | u | <a | | s.
addition standard description logic concept constructions, S(M ) define
concept objects distance less instances concept
s: <a s, similarly distance a. <a defined <a (s)
(s) respectively.
700

fiQualitative Spatial Logics Buffered Geometries

S(M ) formulas defined
.
, := c | = | (c1 , c2 ) < | (c1 , c2 ) | | .
.
.
.
Further, v abbreviation (s u t) = =
6 abbreviation (s = t).
(c1 , c2 ) > (c1 , c2 ) defined ((c1 , c2 ) a) ((c1 , c2 ) < a) respectively.
S(M )-model B structure form:
B
B = hW, d, X1B , X2B , ..., cB
1 , c2 , ...i

hW, di metric space (Definition 1), XiB subset W , cB

element W . value S(M )-term B computed inductively
follows:
>B = W , B = ;
{ci }B = {cB
};
(s)B = W sB ;
B
(s1 u s2 )B = sB
1 s2 ;

(<a s)B = {x W | sB : d(x, y) < a};
(a s)B = {x W | sB : d(x, y) a};
(s)B = {x W | sB }.
<a , dual <a , respectively. instance,
(<a s)B = {x W | W : (d(x, y) < sB )}.
truth condition B |= , S(M )-formula, defined follows:
B |= c iff cB sB ;
.
B
B |= s1 = s2 iff sB
1 = s2 ;
B |= (k, l) < iff d(k B , lB ) < a;
B |= (k, l) iff d(k B , lB ) a;
B |= iff B 6|= ;
B |= iff B |= B |= .
set S(M ) formulas satisfiable, exists S(M )-model B
B |= every . denoted B |= .
proved LNF/LNFS/LBPT proper fragments logic S(Q0 ).
Strictly speaking, holds Q0 , later show finite set
LNF/LNFS/LBPT formulas satisfiable R0 , satisfiable = 1.
words, acts scaling factor (see proof Lemma 43).
701

fiDu & Alechina

Lemma 1 individual names a, b, S(M ) formula {a} v {b} expressible
LNF.
Proof. Let M1 , M2 metric models2 . M1 = (1 , d, I1 , ), M2 = (2 , d, I2 , ).
M1 , 1 = {o1 , o2 }, d(o1 , o2 ) = . I1 (a) = o1 , I1 (b) = o2 . individual name x
differing a, b, I1 (x) = o1 .
M2 , 2 = {o}. I2 (a) = o, I2 (b) = o. individual name x differing a, b,
I2 (x) = o. individual name y, Ii ({y}) = {Ii (y)}, {1, 2}.
definitions M1 , M2 , individual names x, y, d(I1 (x), I1 (y)) [0, ],
d(I2 (x), I2 (y)) = 0. atomic LNF formula x, y, Definition 2, M1 |=
iff M2 |= . easy induction logical connectives, LNF formula , M1 |=
iff M2 |= .
Since I1 ({a}) = {o1 }, I1 ({b}) = {o2 } I2 ({a}) = I2 ({b}) = {o}, truth definition S(M ) formulas, M1 |= ({a} v {b}), M2 6|= ({a} v {b}). Hence, {a} v {b}
equivalent LNF formula.
Lemma 2 logic LNF proper fragment logic S(Q0 ).
Proof. Every atomic LNF formula expressible S(Q0 ):
BEQ(a, b) ((a, b) 0) ((a, b) );
N EAR(a, b) ((a, b) 0) ((a, b) 2);
F AR(a, b) ((a, b) > 4).
means LNF formulas expressed fragment S(Q0 ) (the image
LNF translation above) contains location constants, binary distance predicate boolean connectives , . Lemma 1, LNF proper fragment
S(M ).
Lemma 3 individual names a, b, S(M ) formula v b expressible
LNFS/LBPT.
Proof. Let M1 , M2 metric models3 . M1 = (1 , d, I1 , ), M2 = (2 , d, I2 , ).
M1 , 1 = {o1 , o2 }, d(o1 , o2 ) = . I1 (a) = {o1 }, I1 (b) = {o2 }. individual
name x differing a, b, I1 (x) = {o1 }.
M2 , 2 = {o}. I2 (a) = {o}, I2 (b) = {o}. individual name x differing
a, b, I2 (x) = {o}.
atomic LNFS/LBPT formula x, y, Definition 3, M1 |= iff
M2 |= . easy induction logical connectives, LNFS/LBPT formula ,
M1 |= iff M2 |= .
2. Note construct models one-dimensional two-dimensional Euclidean space similar
way prove lemma.
3. Note construct models one-dimensional two-dimensional Euclidean space similar
way prove lemma.

702

fiQualitative Spatial Logics Buffered Geometries

truth definition S(M ) formulas, M1 |= (a v b) M2 6|= (a v b).
Hence, v b equivalent LNFS/LBPT formula.
Lemma 4 logic LNFS/LBPT proper fragment S(Q0 ).
Proof. Every atomic LNFS/LBPT formula expressible S(Q0 ):
(For LNFS) BEQ(a, b) iff (a v ( b)) (b v ( a));
(For LBPT) BP (a, b) iff (a v ( b));
.
N EAR(a, b) iff (a u (2 b) =
6 );
.
F AR(a, b) iff (a u (4 b) = ).
Note formulas right belong fragment S(Q0 ) image
LNFS/LBPT translation above.
correctness translation BEQ(a, b) BP (a, b) S(Q0 ) follows directly truth definition BEQ BP (Definition 3). show translation N EAR F AR correct, consider truth definition N EAR(a, b)
equivalent 0 dmin (a, b) 2 F AR(a, b) dmin (a, b) > 4, dmin (a, b) =
inf{d(pa , pb ) | pa I(a), pb I(b)}. shown Wolter Zakharyaschev (2005)
.
dmin (a, b) iff u (m b) =
6 . makes translation formulas
truth conditions defined Definition 3. Lemma 3, LNFS/LBPT proper
fragment S(Q0 ).
Wolter Zakharyaschev (2003) proved satisfiability problem finite set
S(Q0 ) formulas metric space EXPTIME-complete, provides upper
bound complexity satisfiability problems LNF, LNFS LBPT metric
space.
Kutz et al. (2002) Kutz (2007) gave axioms inference rules connecting S(M )
terms (e.g. 0 s) S(M ) variants. However, axiomatisation
going present LNF, LNFS LBPT formulas (corresponding S(M ) formulas
rather S(M ) terms).

4. Axioms Theorems
section presents sound complete axiomatisation logic LNF/LNFS/LBPT
respectively. axiomatic systems used basis rule-based reasoner
described later Section 7 4 .
following calculus (which also refer LNF) sound complete
LNF:
Axiom 0 tautologies classical propositional logic
4. important complete axiomatisation. Otherwise, reasoner detect
LNF/LNFS/LBPT inconsistencies caused problematic matches.

703

fiDu & Alechina

Axiom 1 BEQ(a, a);
Axiom 2 BEQ(a, b) BEQ(b, a);
Axiom 3 N EAR(a, b) N EAR(b, a);
Axiom 4 F AR(a, b) F AR(b, a);
Axiom 5 BEQ(a, b) BEQ(b, c) N EAR(c, a);
Axiom 6 BEQ(a, b) N EAR(b, c) BEQ(c, d) F AR(d, a);
Axiom 7 N EAR(a, b) N EAR(b, c) F AR(c, a);
MP Modus ponens: , ` .
following calculus (which also refer LNFS) sound complete
LNFS:
Axiom 0 tautologies classical propositional logic
Axiom 1 BEQ(a, a);
Axiom 2 BEQ(a, b) BEQ(b, a);
Axiom 3 N EAR(a, b) N EAR(b, a);
Axiom 4 F AR(a, b) F AR(b, a);
Axiom 5 BEQ(a, b) BEQ(b, c) N EAR(c, a);
Axiom 6 BEQ(a, b) N EAR(b, c) BEQ(c, d) F AR(d, a);
Axiom 8 N EAR(a, b) BEQ(b, c) BEQ(c, d) F AR(d, a);
MP Modus ponens: , ` .
Axiom 7 calculus LNF holds points, arbitrary geometries,
two points within line polygon far other. Axiom 7
replaced Axiom 8 LNFS. axioms LNFS LNF.
following calculus (which also refer LBPT) sound complete
LBPT:
Axiom 0 tautologies classical propositional logic
Axiom 3 NEAR(a, b) NEAR(b, a);
Axiom 4 FAR(a, b) FAR(b, a);
Axiom 9 BPT (a, a);
Axiom 10 BPT (a, b) BPT (b, c) NEAR(c, a);
704

fiQualitative Spatial Logics Buffered Geometries

Axiom 11 BPT (b, a) BPT (b, c) NEAR(c, a);
Axiom 12 BPT (b, a) NEAR(b, c) BPT (c, ) FAR(d , a);
Axiom 13 NEAR(a, b) BPT (b, c) BPT (c, ) FAR(d , a);
MP Modus ponens: , ` .
calculus LBPT similar calculus LNFS. Changing predicates BEQ
BP , LNFS Axioms 1, 6, 8 replaced Axioms 9, 12, 13 respectively LBPT. Since
BP symmetric, LNFS Axiom 2 corresponding axiom LBPT,
LNFS Axiom 5 replaced two LBPT axioms, Axiom 10 Axiom 11.
notion derivability ` LNF/LNFS/LBPT calculus standard. formula
derivable ` . set LNF/LNFS/LBPT-inconsistent formula
derives .
proved following theorems LNF, LNFS LBPT.
Theorem 1 (Soundness Completeness) LNF/LNFS/LBPT calculus sound
complete metric models, namely
` |=
(every derivable formula valid every valid formula derivable).
Theorem 2 (Decidability Complexity) satisfiability problem finite set
LNF/LNFS/LBPT formulas metric space NP-complete.
following sections, give proofs results case LBPT.
proofs LNF LNFS similar. LBPT, following derivable formulas,
refer facts completeness proof:
Fact 14 BP (a, b) N EAR(a, b);
Fact 15 N EAR(a, b) F AR(a, b);
Fact 16 N EAR(a, b) BP (b, c) F AR(c, a);
Fact 17 BP (a, b) F AR(a, b);
Fact 18 BP (a, b) BP (b, c) F AR(c, a);
Fact 19 BP (b, a) BP (b, c) F AR(c, a);
Fact 20 BP (a, b) BP (b, c) BP (c, d) F AR(d, a);
Fact 21 BP (b, a) BP (b, c) BP (c, d) F AR(d, a);
Fact 22 BP (a, b) BP (b, c) BP (c, d) BP (d, e) F AR(e, a);
Fact 23 BP (b, a) BP (b, c) BP (c, d) BP (d, e) F AR(e, a);
Fact 24 BP (b, a) BP (c, b) BP (c, d) BP (d, e) F AR(e, a).
shown Facts 17-24, chain four BP implies negation F AR,
F AR defined > 4 distance away Definition 3.
705

fiDu & Alechina

5. Soundness Completeness LBPT
section shows LBPT calculus sound complete metric models.
Though several definitions lemmas presented previous work (Du et al.,
2013; Du & Alechina, 2014b), proofs presented complete, structured, accurate (small errors corrected) simplified.
proof soundness (every LBPT derivable formula valid: ` |= )
easy induction length derivation . Axioms 3, 4, 9-13 valid (by
truth definition BP , N EAR F AR) modus ponens preserves validity.
rest section, prove completeness (every LBPT valid formula derivable):
|= `
actually prove finite set LBPT formulas consistent,
metric model satisfying it. finite set formulas rewritten formula
conjunction formulas . consistent, iff consistent (6` ).
metric model satisfying , satisfies , thus 6|= . Therefore,
show consistent, exists metric model satisfying it,
show 6` , 6|= . shows 6` 6|= contraposition get
completeness.
completeness theorem proved constructing metric model maximal
consistent set (Definition 4) finite consistent set LBPT formulas (Lemma 5).
Definition 4 (MCS) set formulas language L(LBP ) maximal consistent, consistent, set LBPT formulas set individual names
properly containing inconsistent. maximal consistent set formulas,
call CS.
Proposition 1 (Properties MCSs) CS, then,
closed modus ponens: , , ;
derivable, ;
formulas : ;
formulas , : iff ;
formulas , : iff .
Lemma 5 (Lindenbaums Lemma) consistent set formulas language
L(LBP ), CS + set individual names
+ .
Let 0 , 1 , 2 , ... enumeration LBPT formulas set individual names
. + defined follows:
0 = ;
n+1 = n {n }, consistent, otherwise, n+1 = n {n };
706

fiQualitative Spatial Logics Buffered Geometries

+ =



n0 n .

finite consistent set formulas , construct metric model satisfying
maximal consistent set + , contains set individual names
, follows. Firstly, equivalently transform + B(+ ), set basic
quantified formulas. construct set distance constraints D(+ ) B(+ ).
key concept path-consistency set distance constraints.
Definition 5 (Non-negative Interval) interval h non-negative, h [0, +).
Definition 6 (Distance Constraint, Distance Range) distance constraint statement form d(p, q) g, p, q constants representing points, d(p, q) stands
distance p, q, g non-negative interval, stands distance
range p, q.
Definition 7 (Composition) d1 , d2 non-negative real numbers, composition {d1 } {d2 } defined as: {d1 } {d2 } = [|d1 d2 |, d1 + d2 ] 5 . g1 , g2 nonnegative intervals, composition interval union {d1 }{d2 },
d1 g1 , d2 g2 , is,

g1 g2 = d1 g1 ,d2 g2 {d1 } {d2 }.
assumed set distance constraints contains one distance range
pair constants p, q involved D, closed symmetry, i.e. d(p, q) g
D, d(q, p) g D.
Definition 8 (Path-Consistency) set distance constraints D, every pair
different constants p, q involved D, distance range strengthened successively
applying following operation fixed point reached:
: g(p, q) g(p, q) (g(p, s) g(s, q))
constant D, 6= p, 6= q, g(p, q) denotes distance range p, q.
process called enforcing path-consistency D. fixed point, every pair
constants p, q, g(p, q) 6= , called path-consistent.
paper, say interval referred process enforcing path-consistency
D, occurs involved enforcement operation g(p, q) g(p, q)
(g(p, s)g(s, q)). words, used g(p, q), g(p, s) g(s, q). distance constraint
appears process enforcing path-consistency D, distance range (an interval)
referred process enforcing path-consistency D.
way enforcing path-consistency set distance constraints defined
almost enforcing path-consistency binary constraint satisfaction
problem (CSP) (Renz & Nebel, 2007; van Beek, 1992), except operation :
g(p, q) g(p, q) (g(p, s) g(s, q)) ( composition operator non-negative intervals,
Definition 7) applied instead k : Rij Rij (Rik Rkj ) ( composition operator
5. Based d(x, z) d(x, y) + d(y, z) (Property 3 Definition 1).

707

fiDu & Alechina

relations). time complexity path-consistency algorithm CSP O(n3 ) (van
Beek, 1992; Mackworth & Freuder, 1985), n number variables involved
input set binary constraints. path-consistency algorithm CSP adapted
easily enforcing path-consistency set distance constraints. time complexity
resulting path-consistency algorithm also O(n3 ), n number constants
involved input set distance constraints. Later paper, show
process enforcing path-consistency D(+ ) terminates, fixed point
reached O(n3 ) (see Lemma 33).
constructing set distance constraints D(+ ) + , prove Metric
Model Lemma, Metric Space Lemma Path-Consistency Lemma stated below.
notion path-consistency acts bridge lemmas.
Lemma 6 (Metric Model Lemma) Let finite consistent set formulas, +
CS contains set individual names .
metric space satisfies D(+ ), extended metric model satisfying + .
Lemma 7 (Metric Space Lemma) Let finite consistent set formulas, +
CS contains set individual names . D(+ )
path-consistent, metric space (, d) distance constraints
D(+ ) satisfied.
Lemma 8 (Path-Consistency Lemma) Let finite consistent set formulas,
+ CS contains set individual names . Then,
D(+ ) path-consistent.
Using three lemmas, prove completeness LBPT: finite set formulas
LBPT-consistent, exists metric model satisfying it.
Proof. LBPT-consistent, Lindenbaums Lemma (Lemma 5), CS
+ set individual names + . Path-Consistency
Lemma (Lemma 8) Metric Space Lemma (Lemma 7), metric space (, d)
distance constraints D(+ ) satisfied. Metric Model Lemma
(Lemma 6), metric space extended metric model satisfying + . Since
+ , satisfies .
detailed proofs Metric Model Lemma, Metric Space Lemma PathConsistency Lemma provided Section 5.1, Section 5.2 Section 5.3 respectively.
Note that, paper, + denotes CS contains given finite consistent set
formulas set individual names .
5.1 Metric Model Lemma
section shows construct set distance constraints D(+ ) + ,
presents proof Metric Model Lemma.
definition properties MCSs (Definition 4 Proposition 1), following
lemma holds.
708

fiQualitative Spatial Logics Buffered Geometries

Lemma 9 + CS, pair individual names a, b occurring ,
exactly one following cases holds + :
1. case(a, b) = BP (a, b) BP (b, a);
2. case(a, b) = BP (a, b) BP (b, a);
3. case(a, b) = BP (a, b) BP (b, a);
4. case(a, b) = BP (a, b) BP (b, a) N EAR(a, b);
5. case(a, b) = N EAR(a, b) F AR(a, b);
6. case(a, b) = F AR(a, b),
case(a, b) denotes formula holds a, b case.
Lemma 9 proved using LBPT axioms facts (such Axiom 3, Facts 14, 15)
way proving lemma LNF (see Du et al., 2013). full proof Lemma 9
provided Appendix A.
construction set distance constraints D(+ ) + two main steps:
Step 1 every pair individual names a, b occurring , translate case(a, b)
set first order formulas equi-satisfiable case(a, b). union
sets first order formulas B(+ ) (hence, B(+ ) equi-satisfiable.).
step described Definition 9 Definition 10.
Step 2 construct set distance constraints D(+ ) B(+ ). step described Definitions 11-13.
LBPT formulas, first order formulas corresponding truth definition
Definition 3. use formulas form d(p, q) g abbreviations equivalent
first order formulas. example, d(p, q) [0, ] abbreviates d(p, q) 0 d(p, q) .
Observe that6
BP (a, b) pa pb b : (pa , pb ) [0 , ] equi-satisfiable ;
N EAR(a, b) pa pb b : (pa , pb ) [0 , 2 ] equi-satisfiable;
F AR(a, b) pa pb b : (pa , pb ) (4 , ) equi-satisfiable.
Definition 9 (Basic Quantified Formula) refer first order formulas
following forms basic quantified formulas:
pa pb b : (pa , pb ) g;
pa pb b : (pa , pb ) g;
6. Note pa pb b : (pa , pb ) [0 , ], actually quantifying metric space.
sense, precise say, example, BP (a, b) satisfiable metric model, iff
pa pb b : (pa , pb ) [0 , ] satisfiable metric space.

709

fiDu & Alechina

pa pb b : (pa , pb ) g;
pa pb b : (pa , pb ) g,
g non-negative interval. abbreviations four forms defined
(a, b, g), (a, b, g), (a, b, g) (a, b, g) respectively. words, example,
(a, b, g) (pa pb b : (pa , pb ) g).
translate formula case listed Lemma 9 basic quantified
formulas, used count number points needed interpreting individual
names occurring later.
Definition 10 (B(+ )) CS + set individual names ,
corresponding set basic quantified formulas B(+ ) constructed follows. every
pair individual names a, b, translate case(a, b) basic quantified formulas:
translate(BP (a, b) BP (b, a)) = {(a, b, [0, ]), (b, a, [0, ])};
translate(BP (a, b) BP (b, a)) = {(a, b, [0, ]), (b, a, (, ))};
translate(BP (a, b) BP (b, a)) = {(a, b, (, )), (b, a, [0, ])};
translate(BP (a, b) BP (b, a) N EAR(a, b)) = {(a, b, (, )),
(b, a, (, )), (a, b, [0, 2]), (b, a, [0, 2])};
translate(N EAR(a, b) F AR(a, b)) = {(a, b, (2, )), (b, a, (2, )),
(a, b, [0, 4]), (b, a, [0, 4])};
translate(F AR(a, b)) = {(a, b, (4, )), (b, a, (4, ))},
R0 fixed margin error. Let names() set individual names
occurring . Then,

B(+ ) = anames(),bnames() translate(case(a, b)).
following, set basic quantified formulas B(+ ), construct set
distance constraints D(+ ), show metric space satisfying D(+ ),
extended model + . words, constructing metric
set points used interpret individual names.
number points needed interpreting individual name depends
numbers different forms formulas B(+ ). individual name a, let us predict
many particular constants points(a) (points assigned individual name a)
specified finite set formulas B(+ ). points(a) contains least
one constant. formula B(+ ) says exists constant points(a),
constant particular constant within points(a). pair different individual names
a, b, (a, b, g) (b, a, g) B(+ ), count one them; (a, b, g)
B(+ ), map constants points(a) constant points(b).
Lemma 9 Definition 10, B(+ ), pair different individual names a, b
R {, , } never R(a, b, g1 ) R(a, b, g2 ), g1 6= g2 , time.
cardinality points(a) specified follows.
710

fiQualitative Spatial Logics Buffered Geometries

Definition 11 (num(a, B(+ )) points(a)) Let names() set individual names
occurring 7 . individual name names(),
num(a, B ( + )) = |{b names( ) | g : (a, b, g) B ( + )}|
num(a, B ( + )) = |{b names( ) | g : (a, b, g) B ( + )}|
num(a, B ( + )) = |{b names( ) | g : (b, a, g) B ( + )}|
num(a, B ( + )) = num(a, B ( + )) + num(a, B ( + )) + num(a, B ( + )).
points(a) set constants {p1a , . . . , pna }, n = num(a, B(+ )).
Definition 12 (Witness formula) witness formula (a, b, g) pair
constants pa points(a), pb points(b) d(pa , pb ) g. witness formula
(a, b, g) (b, a, g) constant pa points(a), constant pb points(b),
d(pa , pb ) g. constant clean formula, witness formula.
Definition 13 (D(+ )) Let B(+ ) corresponding set basic quantified formulas
CS + . every individual name , assign fixed set new constants points(a) it. construct set distance constraints D(+ ) follows,
iterating basic quantified formulas B(+ ) eliminating quantifiers new
constants. Initially, D(+ ) = {}. every individual name , every constant
pa points(a), add d(pa , pa ) {0} D(+ ). every pair different individual
names a, b,
(a, b, g) B ( + ), take clean constants pa points(a), pb points(b),
add (pa , pb ) = (pb , pa ) g D(+ ), pa , pb become witness (a, b, g);
(a, b, g) B ( + ), take clean constant pa points(a), every pb points(b),
add (pa , pb ) = (pb , pa ) g D(+ ), pa becomes witness (a, b, g);
(b, a, g) B ( + ), take clean constant pb points(b), every pa points(a),
add (pa , pb ) = (pb , pa ) g D(+ ), pb becomes witness (b, a, g);
(a, b, g) B ( + ), take clean constant pb points(b), every pa points(a),
add (pa , pb ) = (pb , pa ) g D(+ ), pb becomes witness (a, b, g);
(b, a, g) B ( + ), take clean constant pa points(a), every pb points(b),
add (pa , pb ) = (pb , pa ) g D(+ ), pa becomes witness (b, a, g);
(a, b, g) B ( + ), every pair constants pa points(a), pb points(b),
add (pa , pb ) = (pb , pa ) g D(+ ).
every pair different constants p, q D(+ ), add (p, q) = (q, p) [0 , )
D(+ ), repeatedly replace (p, q) = (q, p) g1 (p, q) = (q, p) g2
(p, q) = (q, p) (g1 g2 ), one distance range pair p, q
D(+ ).
7. definition + , + contains set individual names .

711

fiDu & Alechina

Definition 13, every pair different individual names a, b, check whether
(a, b, g) B ( + ) holds check whether (b, a, g) B ( + ) holds, possible one holds. reason, check (a, b, g) B ( + )
(b, a, g) B ( + ) separately. Definition 10, (a, b, g) B ( + ) iff (b, a, g) B ( + ).
Hence need check one them. check whether (a, b, g) B ( + )
holds, (a, b, g) B ( + ) iff (b, a, g) B ( + ).
Lemma 10 constructing D(+ ), individual name a, number clean
constants needed points(a) larger num(a, B(+ )).
Proof. Definition 10, individual name a, (a, a, [0, ]) B(+ ). Definition 11, num(a, B(+ )) 1.
involved formula form (a, b, g), (a, b, g) (b, a, g),
individual name b, Definition 11, num(a, B(+ )) = 1. Definition 13,
need clean constants points(a).
Otherwise, Lemma 9 Definition 10, B(+ ), pair different individual
names a, b R {, , }, never R(a, b, g1 ) R(a, b, g2 ), g1 6= g2 ,
time. Definition 13, (a, b, g) B(+ ), take one clean constant
points(a), num(a, B(+ )) clean constants needed total formulas
form. Similarly, num(a, B(+ )) (num(a, B(+ )) 1) clean constants needed
formulas forms (a, b, g) (b, a, g) respectively, a, b different individual
names. need clean constant points(a) formulas forms.
Definition 11, num(a, B(+ )) enough.
D(+ ) B(+ ) equi-satisfiable way assign witnesses
formulas. specifically, pair different individual names a, b, (a, b, g)
B(+ ), map constants points(a) constant points(b).
words, B 0 (+ ) set formulas resulting replacing every (a, b, g) B(+ )
(b, a, g), D(+ ) B 0 (+ ) equi-satisfiable. Since every individual name
interpreted non-empty set constants, model satisfies (b, a, g), satisfies
(a, b, g), vice versa. Hence, constructing D(+ ) B 0 (+ ) rather B(+ )
imposes stronger restrictions (i.e. (a, b, g) B(+ ) replaced (b, a, g) B 0 (+ ))
metric space compared required + . However, later show
+ consistent, D(+ ) satisfied metric space proving Metric
Space Lemma Path-Consistency Lemma following sections.
proving Metric Model Lemma, let us look important properties
D(+ ), shown Lemmas 11-13. proof Lemma 11 provided Appendix A.
Lemma 12 follows proof Lemma 11.
Lemma 11 distance range g occurring D(+ ),
g {{0}, [0, ], (, ), [0, 2], (2, ), (2, 4], (4, ), [0, )}.
Lemma 12 p points(a), q points(b), 6= b, d(p, q) {0} D(+ ).
Lemma 13 number constants D(+ ) finite.
712

fiQualitative Spatial Logics Buffered Geometries

Proof. assumed finite consistent set formulas n (a finite number) individual names. Lemma 9 Definition 10, B(+ ) contains f =
(n + 2n(n 1)) formulas n individual names. Definition 11, individual
name a, num(a, B(+ )) f . Definition 13, number constants D(+ )
nf .
Metric Model Lemma proved follows.
Lemma 14 metric model satisfies B(+ ), satisfies + .
Proof. lemma follows two observations. First, Lemma 9, + entailed
set C(+ ) = {case(a, b) : names(+ ), b names(+ )}. Second, Definition 10,
B(+ ) translation truth conditions C(+ ) first order logic. metric model
satisfies B(+ ), satisfies C(+ ), hence satisfies + .
Lemma 6 (Metric Model Lemma) Let finite consistent set formulas,
+ CS contains set individual names .
metric space satisfies D(+ ), extended metric model satisfying + .
Proof. Suppose metric space satisfies D(+ ). extend metric model
interpreting every individual name occurring + points(a), corresponding set
constants size num(a, B(+ )) (Definition 11 Definition 13). Definition 13,
formula form (a, a, [0, ]) satisfied . pair different individual names, every , formula witness, formulas also satisfied
. Therefore, metric model B(+ ). Lemma 14, metric model + .

5.2 Metric Space Lemma
process enforcing path-consistency (Definition 8) involves application
composition operator (Definition 7), present several lemmas Section 5.2.1 demonstrate main calculation rules properties intervals obtained composition. Section 5.2.2, characterize distance constraints D(+ ) appearing
process enforcing path-consistency D(+ ). Using definitions lemmas introduced Section 5.2.1 Section 5.2.2, Metric Space Lemma proved
Section 5.2.3.
5.2.1 Composition Operator
section, present several lemmas show main calculation rules composition operator properties intervals obtained composition. lemmas
important understanding several proofs later sections.
Lemmas 15-16 follow Definition 7.
Lemma 15 Let g1 , g2 non-negative intervals. d3 g1 g2 , exist d1 g1 ,
d2 g2 d3 [|d1 d2 |, d1 + d2 ].
713

fiDu & Alechina

Lemma 16 (Calculation Composition) (m, n), (s, t), (m, ), (s, ), {l}, {r}
non-negative non-empty intervals, H1 , H2 , H non-negative intervals, following
calculation rules hold:
1. {l} {r} = [l r, l + r], l r;
2. {l} (s, t) = (s l, + l), l;
3. {l} (s, t) = [0, + l), l (s, t);
4. {l} (s, t) = (l t, + l), l;
5. {l} (s, +) = (s l, +), l;
6. {l} (s, +) = [0, +), < l;
7. (m, n) (s, t) = (s n, + n), n;
8. (m, n) (s, t) = [0, + n), (m, n) (s, t) 6= ;
9. (m, n) (s, +) = (s n, +), n;
10. (m, n) (s, +) = [0, +), < n;
11. (m, +) (s, +) = [0, +);
12. H1 = ;
13. H1 H2 = H2 H1 ;
14. (H1 H2 ) H = (H1 H) (H2 H);


15. ( k Hk ) H = k (Hk H), k N>0 ;
16. (H1 H2 ) H = (H1 H) (H2 H), (H1 H2 ) 6= ;
17. (H1 H2 ) H = H1 (H2 H).
Lemma 16, Rule 14 special case Rule 15, k = 2. Rule 16 states
composition operation distributive non-empty intersections intervals.
necessary require H1 H2 6= , otherwise property may hold. example,
H1 = [0, 1], H2 = [2, 3], H = [1, 2], (H1 H2 ) H = whilst (H1 H) (H2 H) =
[0, 3] [0, 5] 6= . similar property defined Li, Long, Liu, Duckham,
(2015) RCC relations. proofs last three calculation rules provided
Appendix A, whilst others obvious.
interval h form (l, u), [l, u), (l, u] [l, u], call l greatest lower
bound h, represented glb(h), u least upper bound h, represented lub(h).
show interesting properties regarding composition intervals
greatest lower/least upper bounds.
Lemma 17 non-negative non-empty intervals g, h, following properties hold:
714

fiQualitative Spatial Logics Buffered Geometries

1. lub(g h) = lub(g) + lub(h);
2. glb(g h) max(glb(g), glb(h)).
Proof. Follows Lemma 16.
non-empty interval h right-closed, iff h = [x, y] h = (x, y]. h right-open, iff
h = [x, y) h = (x, y). h right-infinite, iff h = [x, ) h = (x, ). h left-closed, iff
h = [x, y] h = [x, y). h left-open, iff h = (x, y] h = (x, y).
Lemma 18 Let g1 , g2 , g3 non-negative non-empty right-closed intervals, g1 g2 g3 ,
lub(g1 ) lub(g2 ) + lub(g3 ).
Proof. Suppose g1 g2 g3 . Since lub(g1 ) g1 , lub(g1 ) g2 g3 . Lemma 15,
exist d2 g2 , d3 g3 , lub(g1 ) d2 + d3 . Since d2 lub(g2 ), d3 lub(g3 ),
lub(g1 ) lub(g2 ) + lub(g3 ).
Lemma 19 Let g1 , g2 , g3 non-negative non-empty intervals, g1 g2 g3 . g1 rightinfinite, g2 g3 right-infinite.
Proof. Suppose g1 right-infinite. Since g1 g2 g3 , g2 g3 right-infinite. Definition 7 Lemma 16, g2 g3 right-infinite.
5.2.2 Distance Constraints D(+ ) DS(+ )
section, characterize distance constraints appear process
enforcing path-consistency D(+ ) two main steps:
Step 1 characterize intervals involved D(+ ), well composition
intervals. step described Definition 14 Lemmas 20-24.
Step 2 introduce notion DS(+ ) set containing distance constraints
appearing process enforcing path-consistency D(+ ), characterize
distance constraints DS(+ ). step described Definitions 15-17
Lemmas 25-31.
Definition 14 (Primitive, Composite, Definable Intervals) Let h non-negative
interval. h primitive, h one [0, ], (, ), [0, 2], (2, ), (2, 4], (4, ),
[0, ). h composite, obtained composition least two primitive
intervals. h definable, primitive composite.
Lemma 20 non-negative interval h, h {0} = h.
Proof. Follows Definition 7.
Since Lemma 20 holds, call {0} identity interval.
715

fiDu & Alechina

Lemma 21 interval occurs D(+ ), identity interval primitive
interval.
Proof. Follows Definition 14 Lemma 11.
Lemma 22 h definable interval, h 6= .
Proof. Follows Definition 14 Definition 7.
Lemma 23 interval h definable, following properties hold:
1. glb(h) = n, n {0, 1, 2, 3, 4};
2. lub(h) = + lub(h) = m, N>0 .
Proof. Let us prove induction structure h.
Base case: h primitive. Definition 14, n {0, 1, 2, 4}, lub(h) = + {1, 2, 4}.
Inductive step: Suppose Properties 1, 2 hold interval ht obtained
composition primitive intervals, N>0 (induction hypothesis).
show Properties 1, 2 hold interval ht+1 obtained composition
(t + 1) primitive intervals.
ht+1 , exist ht primitive interval hp ht+1 = ht hp .
induction hypothesis, glb(ht ) = nt , nt {0, 1, 2, 3, 4}; lub(ht ) = + lub(ht ) = mt ,
mt N>0 . base case, glb(hp ) = np , np {0, 1, 2, 4}; lub(hp ) = + lub(hp ) =
mp , mp {1, 2, 4}. Lemma 17, lub(ht+1 ) = lub(ht ) + lub(hp ). Thus, Property 2 holds.
Lemma 16,
lub(ht ) < glb(hp ), glb(ht+1 ) = glb(hp ) lub(ht );
lub(hp ) < glb(ht ), glb(ht+1 ) = glb(ht ) lub(hp );
otherwise, glb(ht+1 ) = 0.
Since mt > 0 mp > 0, glb(ht+1 ) = nt+1 , nt+1 < 4. case, nt+1 {0, 1, 2, 3}
(Property 1 holds).
Lemma 24 h identity definable interval, then:
1. lub(h) = 0, iff h = {0};
2. lub(h) = , iff h = [0, ];
3. glb(h) = 4, iff h = (4, ).
Proof. Follows Lemma 17, Lemma 23 proof.
start characterize distance constraints appear process
enforcing path-consistency D(+ ).
716

fiQualitative Spatial Logics Buffered Geometries

Definition 15 (DS(+ )) DS(+ ) minimal set distance constraints
following holds:
distance constraint D(+ ) DS(+ );
distance constraints d(p, q) h d(q, s) g DS(+ ), d(p, s) h g
DS(+ );
distance constraints d(p, q) h d(p, q) g DS(+ ), d(p, q) h g
DS(+ ),
p, q, constants D(+ ).
definition above, DS(+ ) required minimal, interval
involved DS(+ ) either D(+ ) obtained applying composition intersection
operations intervals D(+ ). generality, restrict p, q, different
constants. example, possible p = q.
Lemma 25 distance constraint appears process enforcing path-consistency
D(+ ), DS(+ ).
Proof. Follows Definition 8 (path-consistency) Definition 15.
DS(+ ) covers distance constraints appearing process enforcing pathconsistency D(+ ). However, every distance constraint DS(+ ) necessarily
appears process enforcing path-consistency D(+ ). example, D(+ )
contains exactly one distance constraint d(p, p) [0, ], Definition 15, d(p, p)
[0, 2] DS(+ ) (so d(p, p) [0, n], n N>0 ), Definition 8, d(p, p)
[0, 2] appear process enforcing path-consistency. easy see
DS(+ ) infinite set.
concept DS(+ ) similar concept distributive subalgebra defined
Li et al. (2015), composition operation distributes non-empty intersections
intervals involved DS(+ ) (Rule 16 Lemma 16). However, work, composition
operation defined intervals rather relations.
Lemma 26 distance constraint d(p, q) h DS(+ ), h non-negative
interval.
Proof. distance constraint d(p, q) h D(+ ), Lemma 11, h non-negative
interval. Definitions 5, 7 definition intersection, applying composition
intersection non-negative intervals, obtain non-negative intervals. Definition 15,
h non-negative interval.
Differing previous version (Du & Alechina, 2014b), following definitions
lemmas restricted non-empty intervals.
Recall non-empty interval h right-closed, iff h = [x, y] h = (x, y]. h
right-open, iff h = [x, y) h = (x, y). h right-infinite, iff h = [x, ) h = (x, ). h
left-closed, iff h = [x, y] h = [x, y). h left-open, iff h = (x, y] h = (x, y).
717

fiDu & Alechina

Lemma 27 distance constraint d(p, q) h DS(+ ) h 6= , h either
right-infinite right-closed.
Proof. Let n denote total number times applying composition intersection
obtain h, n 0. prove induction n.
Base case: n = 0, d(p, q) h D(+ ). Lemma 11, h either right-infinite
right-closed. Inductive step: Suppose statement holds non-empty h
obtained applying composition intersection n times (induction
hypothesis). show also holds non-empty h obtained
applying composition intersection (n + 1) times.
last step obtain h intersection, Definition 15, exist non-empty
h1 , h2 h = h1 h2 . induction hypothesis, hi , {1, 2}, hi
either right-infinite right-closed. intersection rules, h either right-infinite
right-closed.
last step obtain h composition, Definition 15, exist nonempty h1 , h2 h = h1 h2 . induction hypothesis, hi , {1, 2}, hi
either right-infinite right-closed. composition rules (Lemma 16), h either
right-infinite right-closed.


Lemma 28 distance constraint d(p, q) h DS(+ ) h 6= , glb(h) 6= 0,
h left-open.
Proof. Let n denote total number times applying composition intersection
obtain h, n 0. prove induction n.
Base case: n = 0, d(p, q) h D(+ ). Lemma 11, glb(h) 6= 0,
h left-open. Inductive step: Suppose statement holds non-empty h
obtained applying composition intersection n times (induction
hypothesis). show also holds non-empty h obtained
applying composition intersection (n + 1) times.
last step obtain h intersection, Definition 15, exist nonempty h1 , h2 h = h1 h2 . induction hypothesis, hi , {1, 2},
glb(hi ) 6= 0, hi left-open. intersection rules, glb(h) 6= 0, h
left-open.
last step obtain h composition, Definition 15, exist non-empty
h1 , h2 h = h1 h2 . glb(h) 6= 0, composition rules (Lemma 16),
h1 h2 = . Suppose lub(h1 ) glb(h2 ), glb(h) = glb(h2 )lub(h1 ). Lemma 26
glb(h) 6= 0, glb(h) > 0, thus glb(h2 ) > lub(h1 ). Lemma 26, lub(h1 )
0, thus glb(h2 ) > 0. induction hypothesis, h2 left-open. composition rules
(Lemma 16), h left-open. Similarly, also holds lub(h2 ) glb(h1 ).
718

fiQualitative Spatial Logics Buffered Geometries


distance constraint d(p, q) h DS(+ ), Definition 15, h obtained
applying composition and/or intersection operations n 0 times intervals occurring
D(+ ). applying intersection operation generate new bound,
greatest lower/least upper bound (and openness) h must
interval D(+ ) composition intervals D(+ ). formalize rationale
concepts Left-Definable Right-Definable characterize distance constraints DS(+ ). Later, show every distance constraint d(p, q) h (h 6= )
DS(+ ) left-definable right-definable. Left-Definable Right-Definable key
concepts proving Path-Consistency Lemma, establish correspondences
distance constraint DS(+ ) sequence distance constraints D(+ ).
non-empty interval h left-open, greatest lower bound represented
glb (h). h left-closed, greatest lower bound represented glb+ (h). h
right-open, least upper bound represented lub (h). h right-closed,
least upper bound represented lub+ (h).
Definition 16 (Left-Definable) distance constraint d(p1 , pn ) hs (n > 1) leftdefinable, iff hs 6= exists sequence distance constraints d(pi , pi+1 ) hi
(0 < < n) D(+ ), = h1 ... hn1 , following holds:
1. hs left-open, left-open glb (m) = glb (hs );
2. hs left-closed, left-closed glb + (m) = glb + (hs );
3. hs m.
Definition 17 (Right-Definable) distance constraint d(p1 , pn ) hs (n > 1) rightdefinable, iff hs 6= exists sequence distance constraints d(pi , pi+1 ) hi
(0 < < n) D(+ ), = h1 ... hn1 , following holds:
1. hs right-open, right-open lub (m) = lub (hs );
2. hs right-closed, right-closed lub + (m) = lub + (hs );
3. hs m.
important distinguish definition left-definable/right-definable distance
constraints (Definitions 16 17) Definition 14 (Definable Intervals). example,
distance constraints d(p1 , p2 ) {0} d(p2 , p3 ) {0} D(+ ), d(p1 , p3 ) {0}
left-definable right-definable, {0} definable interval. distance constraints
d(p1 , p2 ) [0, ] d(p2 , p3 ) (4, ) D(+ ), d(p1 , p3 ) (3, 5] leftdefinable, (3, 5] definable interval.
Lemma 29 Let h, g non-negative intervals. distance constraints d(p, q) h
d(q, s) g left-definable right-definable, d(p, s) h g left-definable
right-definable.
719

fiDu & Alechina

Proof. Since d(p, q) h d(q, s) g right-definable, Definition 17, h 6= ,
g 6= . Definition 7, h g 6= . Definition 17, D(+ ), exist sequence
distance constraints d(p, x2 ) h1 , ..., d(xn1 , q) hn1 d(p, q) h sequence
distance constraints d(q, y2 ) g1 , ..., d(yt1 , s) gt1 d(q, s) g respectively satisfying
three properties. Let us take union two sequences new one, is,
d(p, x2 ) h1 , ..., d(xn1 , q) hn1 , d(q, y2 ) g1 , ..., d(yt1 , s) gt1 . composition
rules (Lemma 16), new sequence satisfies properties Definition 17 d(p, s) hg.
Hence, d(p, s) h g right-definable.
composition rules (Lemma 16), h g 6= , glb+ (h g) = 0. use
new sequence above. Let m1 = (h1 ... hn1 ), m2 = (g1 ... gt1 ). Definition 17,
h m1 , g m2 . m1 m2 6= , therefore, glb+ (m1 m2 ) = 0. Definition 7,
h g m1 m2 . Definition 16, d(p, s) h g left-definable.
h g = , let us suppose glb(h) lub(g). Since d(p, q) h left-definable
d(q, s) g right-definable, Definitions 16 17 respectively, D(+ ), exist
sequence distance constraints d(p, q) h sequence distance constraints
d(q, s) g, satisfying corresponding properties. composition rules (Lemma
16), union two sequences satisfies properties Definition 16 d(p, s) hg.
Hence, d(p, s) h g left-definable. Similarly, show d(p, s) h g left-definable,
glb(g) lub(h).
Lemma 30 Let h, g non-negative intervals. distance constraints d(p, q) h
d(p, q) g left-definable right-definable, h g 6= , d(p, q) h g leftdefinable right-definable.
Proof. applying intersections generate new bound h g 6= ,
left/right bound h g h g. left bound h g
h, Definition 16, sequence used showing d(p, q) h
left-definable used show d(p, q) h g left-definable. cases similar.
Lemma 31 distance constraint d(p, q) h DS(+ ) h 6= , leftdefinable right-definable.
Proof. Let n denote total number times applying composition intersection
obtain h, n 0. prove induction n.
Base case: n = 0, d(p, q) h D(+ ). Definitions 16 17, d(p, q) h
left-definable right-definable.
Inductive step: Suppose statement holds non-empty h obtained
applying composition intersection n times (induction hypothesis).
show also holds non-empty h obtained applying composition
intersection (n + 1) times. Definition 15, last operation obtain h either
composition intersection. former case, exist d(p, s) g1 d(s, q) g2
DS(+ ), g1 g2 = h. h 6= , Definition 7, gi 6= , {1, 2}. Since g1
g2 obtained applying composition intersection n times,
induction hypothesis, d(p, s) g1 d(s, q) g2 left-definable right-definable.
720

fiQualitative Spatial Logics Buffered Geometries

Lemma 29, d(p, q) h left-definable right-definable. latter case, exist
d(p, q) g1 d(p, q) g2 DS(+ ), g1 g2 = h. h 6= , intersection
rules, gi 6= , {1, 2}. induction hypothesis, d(p, q) g1 d(p, q) g2 leftdefinable right-definable. Lemma 30, d(p, q) h left-definable right-definable.

generality, exclude possibility d(p, q) DS(+ ). However,
follows proof Path-Consistency Lemma Section 5.3 d(p, q)
DS(+ ). Alternatively, direct proof, see Lemmas 45 46 Appendix C.
5.2.3 Proving Metric Space Lemma
following, show metric space satisfying D(+ ), D(+ ) pathconsistent (Metric Space Lemma). Firstly, show process enforcing patchconsistency D(+ ) terminates. Lemma 13, number constants D(+ )
finite. Let us suppose number constants D(+ ) N>0 .
Lemma 32 Let N>0 number constants D(+ ). non-empty
right-closed interval h referred process enforcing path-consistency D(+ ),
lub+ (h) 4t.
Proof. non-empty right-closed interval h occurs D(+ ), Lemma 11,
lub+ (h) 4 4t.
Otherwise, generated application composition and/or intersection operators Definition 8. Composition creates larger least upper bounds (Lemma 17), whilst
intersection not. Since h right-closed, lub+ (h) obtained composing right-closed
intervals (Lemma 16). constants, longest chain involves (t 1) intervals.
lub+ (h) maximal use (t 1) intervals least upper bound
interval 4. Thus, lub+ (h) 4t.
Lemma 33 Let N>0 number constants D(+ ). Enforcing path-consistency
D(+ ), fixed point reached O(t3 ).
Proof. Definition 8, Lemmas 23 fact intersection generate new
bounds, interval appearing process enforcing path-consistency D(+ ),
following properties hold:
1. glb(s) = n, n {0, 1, 2, 3, 4};
2. lub(s) = + lub(s) = m, N>0 .
interval h appearing D(+ ), enforcing path-consistency (Definition 8), h
become h0 h. Lemma 11, h 6= . Lemma 27, h either right-closed
right-infinite, h0 , right-closed right-infinite.
h right-closed, h0 = h0 right-closed. h0 right-closed,
Lemma 11, lub(h0 ) lub(h) 4. Properties 1, 2, finitely many
possibilities h0 .
721

fiDu & Alechina

h right-infinite, h0 , right-closed right-infinite.
h0 right-closed, Lemma 32, lub(h0 ) 4t. Properties 1, 2,
finitely many possibilities h0 .
h0 right-infinite, Property 1, finitely many possibilities
greatest lower bound, thus h0 .
Since case, finitely many possibilities h0 , fixed point always reached.
Suppose widest non-negative interval [0, ) appears process enforcing
path-consistency D(+ ). worst case, firstly, [0, ) strengthened [0, u],
u 4t (by Lemma 32), [0, u] strengthened time. Hence, [0, )
strengthened (4t + 1) times. constants, Definition 13, O(t2 )
distance constraints D(+ ). interval h appearing D(+ ), h [0, ), hence h
strengthened (4t + 1) times. Therefore, total time strengthening
distance constraints O(t3 ).
following lemma shows construct metric space D(+ ). used
prove Metric Space Lemma.
Lemma 34 Let N>0 number constants D(+ ), Df (+ ) fixed point
enforcing path consistency D(+ ). D(+ ) path-consistent, Ds (+ ) obtained
Df (+ ) replacing every right-infinite interval {5t}, every right-closed interval h
{lub(h)}, Ds (+ ) path-consistent.
Proof. Suppose D(+ ) path-consistent. Lemma 25, Df (+ ) DS(+ ). Definition 8, interval h appearing Df (+ ), h 6= . Lemma 27, h either right-infinite
right-closed. prove Ds (+ ) path-consistent, need show three
distance ranges, {npq }, {nqs }, {nps } Ds (+ ) three constants p, q, s,
1. npq nqs + nps ;
2. nqs npq + nps ;
3. nps npq + nqs .
Let hpq , hqs , hps denote corresponding distance ranges {npq }, {nqs }, {nps } respectively
Df (+ ), Definition 8,
hpq hqs hps ;
hqs hpq hps ;
hps hpq hqs .
prove Ds (+ ) path-consistent cases:
every hi (i {pq, qs, ps}) right-closed, ni = lub(hi ). Lemma 18, 1-3
hold.
722

fiQualitative Spatial Logics Buffered Geometries

Otherwise, right-closed. Lemma 19, least two
right-infinite.
right-infinite, ni = 5t. Since 5t 5t + 5t, 1-3 hold.
Otherwise, one right-closed. Let hpq right-closed. Then,
npq = lub(hpq ), nqs = 5t, nps = 5t. Lemma 32 R0 , lub(hpq )
4t < 5t. Lemma 26, lub(hpq ) 0. Since lub(hpq ) < 5t + 5t
5t 5t + lub(hpq ), 1-3 hold.

Lemma 7 (Metric Space Lemma) Let finite consistent set formulas, +
CS contains set individual names . D(+ )
path-consistent, metric space (, d) distance constraints
D(+ ) satisfied.
Proof. Suppose D(+ ) path-consistent. Let set constants D(+ ),
used interpret individual names occurring , shown Definition 13. = ,
trivial. Let us assume 6= . number constants denoted N>0 .
Lemma 33, fixed point Df (+ ) reached enforcing path-consistency D(+ ).
Let Ds (+ ) set distance constraints obtained Df (+ ) replacing every
right-infinite interval {5t}, every right-closed interval h {lub(h)}. Since every
distance constraint Ds (+ ) form d(p, q) {r}, r R0 , d(p, q) {r}
equivalent d(p, q) = r, metric (distance function) defined . Definition 13
Lemma 34, pair constants x, y, x = y, d(x, y) = 0 holds Ds (+ );
x 6= y, d(x, y) > 0 holds Ds (+ ). Thus, d(x, y) = 0 iff x =
Ds (+ ). Definitions 13 8, pair constants x, y, d(x, y) = d(y, x) holds
Ds (+ ). Lemma 34, Ds (+ ) path-consistent. Thus, constants x, y, z,
d(x, z) d(x, y) + d(y, z) holds Ds (+ ). Definition 1, (, d) Ds (+ )
metric space distance constraints D(+ ) satisfied.

5.3 Path-Consistency Lemma
section proves Path-Consistency Lemma contradiction, supposing D(+ )
path-consistent. examine every case first interval obtained
enforcing path-consistency. case, show derivable corresponding
LBPT formulas + using LBPT axioms. contradicts assumption +
consistent. Lemma 35 used generate possible cases make sure duplicated ones
generated. using Lemma 35, proof Path-Consistency Lemma largely
simplified, compared previous version (Du & Alechina, 2014b).
Lemma 35 Let g, h non-negative intervals. g h = iff (g h) {0} = .
Proof. g h 6= , Definition 7, 0 (g h).
0 (gh), Lemma 15, exist d1 g, d2 h 0 [|d1 d2 |, d1 +d2 ].
Thus, d1 = d2 . Therefore, g h 6= .
723

fiDu & Alechina

Since g h 6= iff 0 (g h), contraposition get g h = iff (g h) {0} = .
Knowing least upper bound greatest lower bound definable interval h, Lemmas 36-42 show possible ways h obtained composition primitive
intervals. Lemma 36 Lemma 39 proved below. Proofs lemmas similar omitted.
Lemma 36 interval h definable, lub(h) = 2, h primitive interval [0, 2]
h obtained composition two [0, ].
Proof. h primitive, Definition 14, h = [0, 2].
h composite, Definition 14, exist two definable intervals g1 , g2
g1 g2 = h. Lemma 17, lub(g1 ) + lub(g2 ) = 2. Lemma 23, lub(g1 ) , lub(g2 ) ,
thus lub(g1 ) = , lub(g2 ) = . Lemma 24, h = [0, ] [0, ].

Lemma 37 interval h definable, lub(h) = 3, h obtained composition
[0, ] [0, 2] composition three [0, ].
Lemma 38 interval h definable, lub(h) = 4, h primitive interval (2, 4],
h obtained composition two [0, 2], composition two [0, ] one
[0, 2] composition four [0, ].
Lemma 39 interval h definable, glb(h) = 3, h obtained composition
[0, ] (4, ).
Proof. Definition 14, h cannot primitive.
Since h composite, Definition 14, exist two definable intervals g1 , g2
g1 g2 = h. g1 g2 = , otherwise, Lemma 16, glb(h) = 0.
Without loss generality, let us suppose lub(g1 ) glb(g2 ). Lemma 16, glb(g2 )
lub(g1 ) = 3. Lemma 23, glb(g2 ) 4, lub(g1 ) , thus glb(g2 ) = 4, lub(g1 ) = .
Lemma 24, h obtained composition [0, ] (4, ).
Lemma 40 interval h definable, glb(h) = 2, h primitive interval (2, )
(2, 4], h obtained composition [0, 2] (4, ) composition
two [0, ] one (4, ).
Lemma 41 interval h definable, glb(h) = , h primitive interval (, ),
h obtained composition [0, ] (2, ), composition [0, ]
(2, 4], composition one [0, ], one [0, 2] one (4, ), composition
three [0, ] one (4, ).
Lemma 42 interval h definable left-open, glb(h) = 0, h obtained
exactly following ways:
composition [0, ] (, );
724

fiQualitative Spatial Logics Buffered Geometries

composition [0, 2] (2, );
composition two [0, ] one (2, );
composition [0, 2] (2, 4];
composition two [0, ] one (2, 4];
composition (2, 4] (4, );
composition two [0, 2] one (4, );
composition two [0, ], one [0, 2] one (4, );
composition four [0, ] one (4, ).
previous work (Du et al., 2013; Du & Alechina, 2014b), presented slightly
different way prove Path-Consistency Lemma LNF LBPT respectively:
first empty interval obtained using strengthening operator, is, g1 (g2 g3 ) =
gi 6= , {1, 2, 3}. gi may {0} primitive interval, written
xi (yi zi ), xi , yi , zi may identity primitive internal also.
latter case, since gi = xi (yi zi ) 6= , xi , yi , zi empty. Since composition
operation distributive non-empty intersections intervals (Rule 16 Lemma 16),
use Rule 16 repeatedly rewrite g1 (g2 g3 ) every interval identity
primitive interval. final form h1 ... hn = , n > 1, hx (0 < x n)
{0} definable interval. Thus exist two intervals hi , hj (0 < n, 0 < j n,
6= j) hi hj = . look different combinations
lub(hi ) glb(hj ). exactly 15 combinations. paper, proof
Path-Consistency Lemma largely simplified. shows sufficient examine 5
rather 15 combinations.
Lemma 8 (Path-Consistency Lemma) Let finite consistent set formulas,
+ CS contains set individual names . Then,
D(+ ) path-consistent.
Proof. Suppose D(+ ) path-consistent. Definitions 8, 15 Lemma 25,
d(p, q) DS(+ ), constants p, q. Lemma 11, distance range g
occurring D(+ ), g 6= . Definitions 15, 7, intersection rules, last operation
obtain first interval intersection. Definition 15, exist d(p, q) g1
d(p, q) g2 DS(+ ), g1 6= , g2 6= , g1 g2 = . Lemma 26, g1 , g2
non-negative intervals. Lemma 35, g1 g2 = iff (g1 g2 ) {0} = .
Definition 13 Definition 15, d(q, p) g2 DS(+ ). Since d(p, q) g1
DS(+ ), Definition 15, d(p, p) (g1 g2 ) DS(+ ). Definition 7, g1 g2 6= .
Lemma 31, d(p, p) (g1 g2 ) left-definable right-definable. Let h = g1 g2 .
Since d(p, p) h left-definable, Definition 16, exists sequence distance
constraints d(pi , pi+1 ) hi (0 < < n) D(+ ), p = p1 = pn h0 =
h1 ... hn1 , h h0 greatest lower bound (including value
725

fiDu & Alechina

openness) h h0 . Definition 14, Lemmas 21 20, h0 identity definable
interval. Lemma 23, glb(h0 ) {0, , 2, 3, 4}. Therefore, (g1 g2 ) {0} = iff one
following holds:
glb(h) {, 2, 3, 4};
h left-open glb (h) = 0.
check whether derived every case using axioms (or derivable facts).
Axiom 3 Axiom 4, N EAR F AR symmetric.
1. glb(h) = : look different ways h0 obtained sequence
distance constraints d(pi , pi+1 ) hi (0 < < n) D(+ ) p = p1 = pn
h0 = h1 ... hn1 (see Definition 16). every hi {0} primitive interval (by
Lemma 21), Lemma 41 specifies different ways obtain h0 :
(a) h0 primitive interval (, +): Definition 16, d(p1 , pn ) (, +)
D(+ ) n = 2. p = p1 = pn , d(p, p) (, +) D(+ ). Suppose
p points(a) individual name . proof Lemma 11, (, +)
come formulas form (x, y, (, )), x, individual names. Definition 10, (x, y, (, )) come BP (x, y).
Since d(p, p) (, +) D(+ ) p points(a), BP (a, a) + .
Axiom 9, BP (a, a) .
(b) h0 obtained composition [0, ] (2, ) composition
[0, ] (2, 4]:
proof Lemma 11 Definition 10, BP (a, b) + BP (b, a) + ,
N EAR(a, b) + N EAR(b, a) + .
Fact 14, BP (x1 , x2 ) N EAR(x1 , x2 ) , {x1 , x2 } = {a, b}.
(c) h0 obtained composition one [0, ], one [0, 2] one (4, ):
proof Lemma 11 Definition 10, BP (a, b) + BP (b, a) + ,
N EAR(b, c) + , N EAR(c, b) + , F AR(c, a) + , F AR(a, c) + .
Fact 16, BP (x2 , x1 )N EAR(x2 , x3 )F AR(x3 , x1 ) , {x1 , x2 , x3 } = {a, b, c}.
(d) h0 obtained composition three [0, ] one (4, +):
proof Lemma 11 Definition 10, three BP one
F AR four individual names a, b, c, d. BP refers either BP (x, y)
BP (y, x). cases (for example, + , BP (a, b), BP (c, b),
BP (d, c) F AR(a, d)) valid, different constants
taken points(b), individual name b (by Definition 13).
consequence, invalid case, sequence consisting distance constraints
d(pi , pi+1 ) hi (0 < < n, p = p1 = pn ) cannot exist D(+ ). need
consider valid cases, listed below.
i. BP (x1 , x2 ), BP (x2 , x3 ), BP (x3 , x4 ), F AR(x4 , x1 ),
{x1 , x2 , x3 , x4 } = {a, b, c, d}. Fact 20, BP (x1 , x2 ) BP (x2 , x3 )
BP (x3 , x4 ) F AR(x4 , x1 ) .
ii. BP (x2 , x1 ), BP (x2 , x3 ), BP (x3 , x4 ), F AR(x4 , x1 ),
{x1 , x2 , x3 , x4 } = {a, b, c, d}. Fact 21, BP (x2 , x1 ) BP (x2 , x3 )
BP (x3 , x4 ) F AR(x4 , x1 ) .
726

fiQualitative Spatial Logics Buffered Geometries

Cases 2-5 use similar arguments. following proof, BP refers either
BP (x, y) BP (y, x) (whichever makes corresponding case valid). N EAR
F AR symmetric, thus order x, matter.
2. glb(h) = 2: Definition 16 Lemma 21, Lemma 40 specifies different
ways obtain h0 sequence distance constraints d(pi , pi+1 ) hi (0 < < n)
D(+ ):
(a) h0 primitive interval (2, ) (2, 4]:
N EAR(a, a), using Axiom 9 Fact 14.
(b) h0 obtained composition [0, 2] (4, +) :
one N EAR one F AR, using Fact 15.
(c) h0 obtained composition two [0, ] one (4, +):
two BP one F AR, using Facts 18 19.
3. glb(h) = 3: Definition 16 Lemma 21, Lemma 39 specifies ways
obtain h0 sequence distance constraints d(pi , pi+1 ) hi (0 < < n)
D(+ ). Lemma 39, h0 obtained composition [0, ] (4, +).
one BP one F AR, using Fact 17.
4. glb(h) = 4: Definition 16 Lemma 21, Lemma 24 specifies ways
obtain h0 sequence distance constraints d(pi , pi+1 ) hi (0 < < n)
D(+ ). Lemma 24, h0 = (4, +). F AR(a, a), using Axiom 9 Fact 17.
5. glb (h) = 0: Definition 16 Lemma 21, Lemma 42 specifies ways
obtain h0 sequence distance constraints d(pi , pi+1 ) hi (0 < < n)
D(+ ):
(a) h0 obtained composition [0, ] (, ): Definition 13, ensuring
different constants taken points(x),
BP (x1 , x2 ) + BP (x1 , x2 ) + , {x1 , x2 } = {a, b}.
BP (x1 , x2 ) BP (x1 , x2 ) .
(b) h0 obtained composition [0, 2] (2, ) composition
[0, 2] (2, 4]:
one N EAR one N EAR, using Axiom 3.
(c) h0 obtained composition two [0, ] one (2, ) composition two [0, ] one (2, 4]:
two BP one N EAR, using Axioms 10 11.
(d) h0 obtained composition (2, 4] (4, ):
one F AR one F AR, using Axiom 4.
(e) h0 obtained composition two [0, 2] one (4, ):
two N EAR one F AR.
case invalid. Definition 16, D(+ ) contains d(pa , pb ) [0, 2], d(pb , pc )
[0, 2] d(pa , pc ) (4, ), pa points(a), pb points(b), pc
points(c), individual names a, b, c. Definitions 10 13, d(pa , pb ) [0, 2]
d(pb , pc ) [0, 2] cannot come N EAR(a, b) N EAR(b, c) + (by
727

fiDu & Alechina

proof Lemma 11, clear cannot come formulas
well), two different constants taken points(b) witnesses
(a, b, [0, 2]) (b, c, [0, 2]) respectively.
(f) h0 obtained composition two [0, ], one [0, 2] one (4, ):
two BP , one N EAR one F AR, using Axioms 12 13.
(g) h0 obtained composition four [0, ] one (4, ):
four BP one F AR, using Facts 22-24.
valid case, derivable using corresponding axioms facts, contradicts
assumption + consistent. Therefore, D(+ ) path-consistent.
alternative way prove Path-Consistency Lemma, believe
longer complicated one presented paper, since may provide
additional intuitions reader, sketch Appendix B.

6. Decidability Complexity LBPT
section, establish complexity LBPT satisfiability problem. complexity LNF/LNFS satisfiability problem established similar way.
complexity satisfiability problems important, related complexity
problem finding inconsistencies, basis approach debugging
matches geospatial datasets.
Definition 18 (Size Formula) size LBPT formula s() defined follows:
s(BP (a, b)) = 3, s(N EAR(a, b)) = 3, s(F AR(a, b)) = 3;
s() = 1 + s();
s( ) = 1 + s() + s(),
a, b individual names, , formulas L(LBP ).
set LBPT formulas conjunction formulas equi-satisfiable,
combined size LBPT formulas set defined size conjunction
formulas S.
Next prove Theorem 2 LBPT: satisfiability problem finite set LBPT
formulas metric space NP-complete.
Proof. NP-hardness LBPT satisfiability problem follows NP-hardness
satisfiability problem propositional logic, included LBPT.
prove LBPT satisfiability problem NP, show finite set
LBPT formulas satisfiable, guess metric model verify
model satisfies , time polynomial combined size formulas .
Suppose finite set LBPT formulas, number individual names
n. completeness proof shows that, satisfiable, satisfiable metric model
728

fiQualitative Spatial Logics Buffered Geometries

size polynomially bounded number individual names . recap
construction metric model , first construct B(+ ), corresponding set
basic quantified formulas MCS + containing , construct model
B(+ ). Definition 10, number formulas B(+ ) f = (n + 2n(n 1)).
Definitions 11 13, every individual name , assign fixed set new
constants, points(a) = {p1a , . . . , pxa }, x = num(a, B(+ )). Since x f , number
constants = nf . Lemma 34 proofs Metric Space Lemma,
model , every value assigned distance function form m, N0 ,
5t.
guess metric model like this. Let combined size formulas .
n < s. every individual name , assign {p1a , . . . , pxa }, x < 2s2 . results
set constants , size < 2s3 . every pair constants p, q ,
assign d(p, q), N0 , < 10s3 . verify (, d) metric space,
Definition 1, O(s9 ).
verify satisfies , need verify satisfies conjunction formulas . R(a, b), R {BP T, N EAR, F AR}, a, b individual names,
verify R(a, b) satisfied, takes time polynomial |points(a)| |points(b)|,
thus O(s4 ). Hence, verifying satisfies done O(s5 ).
Section 3, mentioned acts scaling factor metric model.
stated Lemma 43 follows proofs completeness theorem Theorem 2.
proof LBPT provided below. proof LNF/LNFS similar.
Lemma 43 finite set LNF/LNFS/LBPT formulas satisfiable metric model
R0 , iff satisfiable metric model = 1.
Proof.[for LBPT] Suppose finite set LBPT formulas, number individual
names n combined size formulas s. Definition 18, n < s.
completeness proof shows that, satisfiable, satisfiable metric model
= (, d, I, ) constructed shown Section 5.1 Section 5.2. Definition 13,
every individual name , assign {p1a , . . . , pxa }, x = num(a, B(+ )) < 2s2 .
results set constants , size < 2s3 . every constant p ,
assign d(p, p) = 0. Definition 13 Lemma 34, every pair different constants
p, q , assign d(p, q), N>0 , < 10s3 . metric model,
Definition 1 proof Metric Space Lemma, x, y, z ,
1. d(x, y) = 0, iff x = y;
2. d(x, y) = mxy , iff d(y, x) = mxy ;
3. d(x, z) = mxz , d(x, y) = mxy , d(y, z) = myz , mxz mxy + myz .
satisfies , Definition 3, following holds:
|= BP (a, b) iff pa (a) pb (b) : (pa , pb ) [0 , ];
|= N EAR(a, b) iff pa (a) pb (b) : (pa , pb ) [0 , 2 ];
729

fiDu & Alechina

|= F AR(a, b) iff pa (a) pb (b) : (pa , pb ) (4 , );
|= iff 6|= ;
|= iff |= |= ,
a, b individual names, , formulas L(LBP ).
setting = 1, (, d) still metric space, following holds x, y, z :
1. d(x, y) = 0 iff x = y;
2. d(x, y) = mxy , iff d(y, x) = mxy ;
3. d(x, z) = mxz , d(x, y) = mxy , d(y, z) = myz , mxz mxy + myz .
setting = 1, definitions BP , N EAR F AR change accordingly well.
One easily see still satisfies replacing every 1.
Similar, metric model = 1, obtain metric model
R0 multiplying every distance value . One easily see still satisfies multiplying distance values , multiplying greatest lower
bounds least upper bounds intervals truth definitions BP , N EAR
F AR .

7. Validating Matches using Spatial Logic
spatial logics LN F , LN F LBP used verify consistency sameAs
partOf matches spatial objects different geospatial datasets. every
spatial object point geometry, apply LN F , otherwise, apply LN F
LBP . LBP reasoning used together description logic reasoning
geospatial data matching system MatchMaps (Du, Nguyen, Alechina, Logan, Jackson, &
Goodwin, 2015; Du, 2015). LBP reasoning description logic reasoning complement
sense LBP reasoning verifies matches regarding spatial information
whilst description logic reasoning verifies matches regarding classification information,
unique name assumption stronger version it. following, describe
LBPT used debugging matches.
dedicated LBPT reasoner integrated assumption-based truth maintenance
system (ATMS) (de Kleer, 1986) developed part MatchMaps. implements
LBPT axioms definition BEQ(a, b) BP (a, b) BP (b, a) set inference
rules. efficiency reasons, one-to-one correspondence rules axioms. speed matching avoid cycles, facts N EAR(a, b) stored
one order b, symmetry axioms removed. remaining axioms
involving symmetric relation gives rise several rules, compensate removal
symmetry. example, axiom
BPT (b, a) NEAR(b, c) BPT (c, ) FAR(d , a)
also gives rise rule corresponding following implication:
BPT (b, a) NEAR(c, b) BPT (c, ) FAR(d , a)
730

fiQualitative Spatial Logics Buffered Geometries

(with N EAR(c, b) instead N EAR(b, c)). However set rules trivially equivalent
set axioms.
Possible matches form sameAs(a, b) partOf (a, b) (a partOf b) generated assumptions, withdrawn involved derivation contradiction
description logic LBPT. order apply LBPT reasoning, sameAs(a, b)
replaced BEQ(a, b), partOf (a, b) replaced BP (a, b). substitutions
affect correctness matching results MatchMaps, MatchMaps adopts definitions sameAs partOf sameAs(a, b) entails BEQ(a, b) partOf (a, b)
entails BP (a, b). N EAR(a, b) F AR(a, b) facts generated objects a, b
dataset involved matches across two datasets (there
object c dataset BEQ(a, c), BP (a, c) BP (c, a) holds,
similarly b).
LBPT reasoner derives new formulas applying inference rules previously
derived formulas, ATMS maintains dependencies derived consequences
set assumptions (corresponding possible matches). particular, maintains
minimal sets assumptions responsible derivation (false), referred nogoods
ATMS terminology. minimal sets assumptions responsible contradiction
used decide matches wrong withdrawn.
experiments, LBPT reasoner ATMS used validate matches
spatial objects OSM data (building layer) OSGB MasterMap data (Address Layer
Topology Layer) (Du et al., 2015). study areas city centres Nottingham UK
Southampton UK. Nottingham data obtained 2012, Southampton
data 2013. numbers spatial objects case studies shown Table 1.

Nottingham
Southampton

OSM spatial objects
281
2130

OSGB spatial objects
13204
7678

Table 1: Data used Evaluation

initial matches generated matching method implemented MatchMaps.
detailed matching method provided Du et al. (2016). method consists
two main steps: matching geometries matching spatial objects. spatial object
geospatial dataset ID, location information (coordinates geometry)
meaningful labels, names types, represents object real world.
geometry refers point, line polygon, used represent location
information geospatial datasets.
geometry matching requires level tolerance, difference geometry
representation spatial object expected different datasets. discussing
domain experts geospatial science, decided apply level tolerance
matching method spatial logic used MatchMaps. experiments
described Du et al. (2015), level tolerance geometry matching set
20 meters, based published estimate positional accuracy OSM data.
OSM positional accuracy estimated 20 meters UK (Haklay, 2010).
recent work (Du et al., 2016), analysed level tolerance affects
731

fiDu & Alechina

precision recall matching results geographic area Nottingham
(the data shown first row Table 1) using 12 different levels tolerance
within range 1 80 meters. shows that, Nottingham case, 20 meters
good estimate, though optimal value.
Following first step matching method, first aggregate adjacent single geometries, shops within shopping center, establish correspondences
aggregated geometries using geometry matching. second step, match
spatial objects located corresponding aggregated geometries comparing
similarity names types spatial objects several different cases (one-to-one, manyto-one many-to-many). difficult case match
two aggregated geometries contain objects {a1 , . . . , } one dataset objects
{b1 , . . . , bk } dataset (many-to-many matching case). cannot decide
exact matches automatically using names types objects, generate matches
possibly correct objects two sets: pair ai , bj similar labels, generate sameAs(ai , bj ), partOf (ai , bj ), partOf (bj , ai ). apply reasoning
LBPT description logic verify consistency matches. use description
logic reasoning described Du (2015).

Figure 5: Examples using LNFS LBPT validating matches
minimal set statements involved contradiction contains one
retractable assumption, domain expert needed decide correctness retractable assumptions remove wrong one(s) restore consistency. Location information visualized provided help domain experts make decisions.
shown Figure 5, a1 , b1 , c1 , d1 (dotted) OSGB data a2 , b2 , c2 , d2 (solid)
OSM data. left example, LNFS Axiom 6 (or LBPT Axiom 12
BEQ(a, b) BP (a, b) BP (b, a)), minimal set statements deriving inconsistency consists BEQ(a1 , a2 ), BEQ(b1 , b2 ), N EAR(a1 , b1 ), F AR(a2 , b2 ). clear
BEQ(b1 , b2 ) wrong, N EAR(a1 , b1 ) F AR(a2 , b2 ) facts. right example,
BP (d2 , d1 ) wrong, contradicts BP (c2 , c1 ), N EAR(c2 , d2 ), F AR(c1 , d1 )
LBPT Axiom 12. consequence, sameAs partOf matches corresponding
BEQ(b1 , b2 ) BP (d2 , d1 ) respectively also incorrect.
Table 2 shows numbers nogoods generated LBPT reasoner ATMS.
mentioned earlier, nogoods justifications false: minimal sets statements
contradiction derivable. number interactions number
times users asked take actions use strategies resolve problems (a strategy
heuristic allows users retract similar statements time, example,
732

fiQualitative Spatial Logics Buffered Geometries

Nottingham
Southampton

nogoods
172
268

retracted BEQ/BPT
31
114

retracted sameAs/partOf
1325
488

interactions
3
7

Table 2: LBPT Verification Matches

retracting partOf (o, x) x differing object o). result LBPT reasoning
removal BEQ BPT assumptions, withdraw 1325 sameAs/partOf assumptions
Nottingham case 488 sameAs/partOf assumptions Southampton case.
LBPT validation matches, MatchMaps achieved high precision ( 90%)
recall ( 84%) Nottingham Southampton cases.
described previous work (Du, Alechina, Hart, & Jackson, 2015), MatchMaps
used 12 experts University Nottingham Ordnance Survey Great
Britain match 100 buildings places Southampton. graphical user interface MatchMaps provided allowing users take different types actions remove
wrong matches. number actions decision time users recorded.
precision recall matching results compared obtained without using user-involved verification. Experimental results showed using reasoning
LBPT description logic, precision recall matches generated MatchMaps
improved average 9% 8% respectively. human effort also reduced,
sense decision time required much less fully manual matching
process.

8. Discussion
spatial logics LNF, LNFS LBPT generally applicable reason spatial objects whose locations represented different levels accuracy granularity different
datasets. Locations spatial objects represented using vector data (coordinates)
raster data (images). Sometimes, spatial objects different datasets, measuring
whether locations buffered equal directly difficult impossible, example,
locations represented images without knowing coordinates. cases,
spatial objects may matched comparing shapes images using lexical information.
matter matches spatial objects generated, LNF/LNFS/LBPT
reasoning could used verify consistency matches regard relative locations
(N EAR/F AR facts) spatial objects dataset, often reliable
easy capture.
Another potential application logics matching non-georeferenced volunteered spatial information sketch data (Egenhofer, 1997; Kopczynski, 2006; Wallgrun,
Wolter, & Richter, 2010). Instead created surveying mapping techniques, sketch data often created person memory schematizing authoritative geospatial data. sketch map cannot provide precise metric information
exact distance size spatial object, roughly shows several kinds qualitative
relations (e.g. nearness directions) spatial objects. work Wallgrun
et al. (2010), qualitative spatial reasoning (based dipole relation algebra presented
Moratz, Renz, & Wolter, 2000 checking connectivity cardinal direction calculus
733

fiDu & Alechina

presented Ligozat, 1998) used task matching sketch map road network larger geo-referenced data set, example, OpenStreetMap. Endpoints
junctions roads extracted relative directions represented checked
spatial reasoning. spatial logic LNF applied similarly check relative
distances endpoints junctions roads. N EAR/F AR relations
points indicate length roads. task matching sketch map polygonal
objects (e.g. buildings places), logic LNFS/LBPT applied. Suppose users
draw sketch map buildings estimate distances buildings
N EAR F AR regarding agreed level tolerance . N EAR F AR relations
buildings geo-referenced map calculated automatically. mapping
sketch map geo-referenced map checked reasoning logic
LNFS/LBPT. example, two buildings specified F AR sketch
map cannot matched two buildings N EAR geo-referenced map.
main limitation new spatial logics require level tolerance
using logics, value spatial objects different sizes
types (such buildings, roads, rivers lakes). example, margin error
used cities larger buildings. Ideally, value vary
size type spatial object checked. motivates development
new spatial logics reason sizes types spatial objects, addition
relative locations.
paper, theorems proved respect metric space. However,
models based metric space may realisable 2D Euclidean space,
realistic geospatial data. Suppose four points pi , {1, 2, 3, 4}.
point pi , d(pi , pi ) = 0. pair them, d(pi , pj ) = d(pj , pi ) = 1. clear
metric space satisfying distance constraints,
2D Euclidean space. Wolter Zakharyaschev (2003, 2005) proved satisfiability
problem finite set S(Q0 ) formulas 2D Euclidean space R2 undecidable,
whilst proper fragments may decidable. proved satisfiability problem
finite set LNF formulas 2D Euclidean space decidable PSPACE (Du,
2015), whether satisfiability problem finite set LNFS/LBPT formulas
2D Euclidean space decidable still unknown. also remains open whether
LNF/LNFS/LBPT calculus complete models based 2D Euclidean space. not,
theoretical challenge design logics complete 2D Euclidean spaces,
hence provide accurate debugging matches logics metric spaces.
Finally, use description logic new spatial logics may able detect
wrong matches. example, spatial objects X, one dataset X 0 , 0
dataset, sameAs(X, X 0 ) correct, N EAR south X, 0
N EAR north X 0 , sameAs(Y, 0 ) wrong cannot detected.
deal this, could extend logics existing spatial formalisms reasoning
directional relations (Frank, 1991, 1996; Ligozat, 1998; Balbiani et al., 1999; Goyal
& Egenhofer, 2001; Skiadopoulos & Koubarakis, 2004).
734

fiQualitative Spatial Logics Buffered Geometries

9. Conclusion Future Work
presented series new qualitative spatial logics LNF, LNFS LBPT validating
matches spatial objects, especially crowd-sourced geospatial data. models
based metric space, sound complete axiomatisation provided corresponding
theorems proved logic. LNF, LNFS LBPT satisfiability problems
metric space NP-complete. LBPT reasoner ATMS implemented
used part MatchMaps. Experimental results show LBPT reasoner
used verify consistency matches respect location information
detect obvious logical errors effectively. future work, investigate whether
LNF/LNFS/LBPT calculus complete models based 2D Euclidean space
develop new spatial logics (e.g. reasoning directional relations object sizes
addition distances) provide accurate debugging matches.

Acknowledgments
would like thank anonymous reviewers provided excellent comments
helped us improve paper.

Appendix A. Proofs
Lemma 9 + CS, pair individual names a, b occurring ,
exactly one following cases holds + :
1. case(a, b) = BP (a, b) BP (b, a);
2. case(a, b) = BP (a, b) BP (b, a);
3. case(a, b) = BP (a, b) BP (b, a);
4. case(a, b) = BP (a, b) BP (b, a) N EAR(a, b);
5. case(a, b) = N EAR(a, b) F AR(a, b);
6. case(a, b) = F AR(a, b),
case(a, b) denotes formula holds a, b case.
Proof. pair individual names a, b occurring + , have:
` (B B 1 N F ) (B B 1 N F ) (B B 1 N F ) (B B 1 N F )
(B B 1 N F ) (B B 1 N F ) (B B 1 N F ) (B B 1 N F )
(B B 1 N F ) (B B 1 N F ) (B B 1 N F ) (B B 1 N F )
(BB 1 N F )(BB 1 N F )(BB 1 N F )(BB 1 N F )
B, B 1 , N, F stand BP (a, b), BP (b, a), N EAR(a, b), F AR(a, b) respectively.
Table 3,
` (B B 1 ) (B B 1 ) (B B 1 ) (B B 1 N ) (N F ) F .
735

fiDu & Alechina

B
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0

B 1
1
1
1
1
0
0
0
0
1
1
1
1
0
0
0
0

N
1
1
0
0
1
1
0
0
1
1
0
0
1
1
0
0

F
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0

Prime Implicant

B B 1



B B 1



B B 1



B B 1 N
F
N F

Axiom/Fact used
Fact 15
Facts 14, 15, Axiom
Fact 14, Axiom 3
Fact 14, Axiom 3
Fact 15
Facts 14, 15, Axiom
Fact 14, Axiom 3
Fact 14, Axiom 3
Fact 15
Facts 14, 15, Axiom
Fact 14, Axiom 3
Fact 14, Axiom 3
Fact 15
Fact 15
Facts 14, 15, Axiom
Fact 14, Axiom 3

3

3

3

3

Table 3: truth table, 1 stands true, 0 stands false


Lemma 11 distance range g occurring D(+ ),
g {{0}, [0, ], (, ), [0, 2], (2, ), (2, 4], (4, ), [0, )}.
Proof. Suppose p, q constants d(p, q) g D(+ ). Let us look g
different cases:
p = q: Definition 13, g = {0}.
p 6= q:
p points(a), q points(a), individual name a:
Definition 13, g = [0, ).
p points(a), q points(b), different individual names a, b:
Lemma 9 Definition 10, exactly one following cases holds:
C1
C2
C3
C4
C5
C6

{(a, b, [0, ]), (b, a, [0, ])} B(+ )
{(a, b, [0, ]), (b, a, (, ))} B(+ )
{(a, b, (, )), (b, a, [0, ])} B(+ )
{(a, b, (, )), (b, a, (, )), (a, b, [0, 2]), (b, a, [0, 2])} B(+ )
{(a, b, (2, )), (b, a, (2, )), (a, b, [0, 4]), (b, a, [0, 4])} B(+ )
{(a, b, (4, )), (b, a, (4, ))} B(+ )

C1:
exactly one p, q witness (a, b, [0, ]) (b, a, [0, ]),
Definition 13, construction process, d(p, q) [0, ] added
736

fiQualitative Spatial Logics Buffered Geometries

D(+ ), d(p, q) [0, +) added D(+ ). Since [0, ]
[0, +) = [0, ], g = [0, ].
else p witness (b, a, [0, ]) q witness (a, b, [0, ]),
Definition 13, construction process, d(p, q) [0, ] added
D(+ ), d(p, q) [0, ] added D(+ ) again,
d(p, q) [0, +) added D(+ ). Since [0, ] [0, ] [0, +) =
[0, ], g = [0, ].
else, Definition 13, g = [0, +).
C2:
q witness (a, b, [0, ]), Definition 13, construction
process, d(p, q) [0, ] added D(+ ), d(p, q) [0, )
added D(+ ). Since [0, ] [0, ) = [0, ], g = [0, ].
else q witness (b, a, (, )), Definition 13, construction process, d(p, q) (, ) added D(+ ), d(p, q)
[0, ) added D(+ ). Since (, ) [0, ) = (, ), g = (, ).
else, Definition 13, g = [0, +).
C3:
p witness (a, b, (, )), Definition 13, construction
process, d(p, q) (, ) added D(+ ), d(p, q) [0, )
added D(+ ). Since (, ) [0, ) = (, ), g = (, ).
else p witness (b, a, [0, ]), Definition 13, construction
process, d(p, q) [0, ] added D(+ ), d(p, q) [0, )
added D(+ ). Since [0, ] [0, ) = [0, ], g = [0, ].
else, Definition 13, g = [0, +).
C4:
pair p, q witness (a, b, [0, 2]), Definition 13,
construction process, d(p, q) [0, 2] added D(+ ),
d(p, q) [0, ) added D(+ ). Since [0, 2] [0, ) = [0, 2],
g = [0, 2].
else exactly one p, q witness (a, b, (, )) (b, a, (, )),
Definition 13, construction process, d(p, q) (, )
added D(+ ), d(p, q) [0, ) added D(+ ). Since
(, ) [0, ) = (, ), g = (, ).
else p witness (a, b, (, )) q witness (b, a, (, )),
Definition 13, construction process, d(p, q) (, ) added
D(+ ), d(p, q) (, ) added D(+ ) again,
d(p, q) [0, ) added D(+ ). Since (, ) (, ) [0, ) =
(, ), g = (, ).
else, Definition 13, g = [0, +).
C5:
pair p, q witness (a, b, [0, 4]), Definition 13,
construction process, d(p, q) [0, 4] added D(+ ), then,
737

fiDu & Alechina

d(p, q) (2, ) added satisfy formulas, d(p, q)
[0, ) added D(+ ). Since [0, 4] (2, ) [0, ) = (2, 4],
g = (2, 4].
else, Definition 13, d(p, q) (2, ) added satisfy formulas, d(p, q) [0, ) added D(+ ). Since (2, ) [0, ) =
(2, ), g = (2, ).
C6, Definition 13, d(p, q) (4, ) added, d(p, q) [0, )
added D(+ ). Since (4, ) [0, ) = (4, ), g = (4, ).
Therefore, g {{0}, [0, ], (, ), [0, 2], (2, ), (2, 4], (4, ), [0, )}.
Lemma 16 (Calculation Composition) (m, n), (s, t), (m, ), (s, ), {l},
{r} non-negative non-empty intervals, H1 , H2 , H non-negative intervals,
following calculation rules hold:
1. {l} {r} = [l r, l + r], l r;
2. {l} (s, t) = (s l, + l), l;
3. {l} (s, t) = [0, + l), l (s, t);
4. {l} (s, t) = (l t, + l), l;
5. {l} (s, +) = (s l, +), l;
6. {l} (s, +) = [0, +), < l;
7. (m, n) (s, t) = (s n, + n), n;
8. (m, n) (s, t) = [0, + n), (m, n) (s, t) 6= ;
9. (m, n) (s, +) = (s n, +), n;
10. (m, n) (s, +) = [0, +), < n;
11. (m, +) (s, +) = [0, +);
12. H1 = ;
13. H1 H2 = H2 H1 ;
14. (H1 H2 ) H = (H1 H) (H2 H);


15. ( k Hk ) H = k (Hk H), k N>0 ;
16. (H1 H2 ) H = (H1 H) (H2 H), (H1 H2 ) 6= ;
17. (H1 H2 ) H = H1 (H2 H).
738

fiQualitative Spatial Logics Buffered Geometries



Proof.[for Rule 15] Suppose ( k Hk ) H.S Lemma 15, exist d1 k Hk
d2 H {d1 } {d2 }. Since d1 k Hk , exists k N>0
d1 Hk . SinceSd1 Hk k, d2 H, Definition 7, Hk H, k.
Therefore k (Hk H).

Suppose
k (Hk H). Then, thereSexists k N>0 Hk H.
Since Hk k Hk , Definition 7, ( k Hk ) H.
Proof.[for Rule 16] Suppose H1 H2 6= . Then, Hi 6= , {1, 2}. Since H1 , H2 , H
non-negative intervals, intersection rules Definition 7, (H1 H2 ) H
(H1 H)(H2 H) non-negative intervals. Let L = (H1 H2 )H, R = (H1 H)(H2 H).
H = , Rule 12, L = R = ; otherwise, show L = R cases:
H1 H2 H2 H1 : H1 H2 , Definition 7, H1 H H2 H.
L = H2 H = R. H2 H1 , similarly, L = H1 H = R.
H1 6 H2 H2 6 H1 : Without loss generality, let us suppose glb(H1 ) glb(H2 )
lub(H1 ) lub(H2 ). Then, glb(H1 H2 ) = glb(H2 ), lub(H1 H2 ) = lub(H1 ).
prove L = R, sufficient show following properties hold:
1. lub(L) = lub(R);
2. glb(L) = glb(R);
3. lub(L) L iff lub(R) R;
4. glb(L) L iff glb(R) R.
Rules 1-14 intersection rules, lub(L) = lub(H1 H2 ) + lub(H) = lub(H1 ) +
lub(H). lub(R) = min(lub(H1 ) + lub(H), lub(H2 ) + lub(H)) = lub(H1 ) + lub(H).
Thus, lub(L) = lub(R) (Property 1 holds).
lub(H1 ) H1 lub(H) H, Rules 1-14 intersection rules, lub(L) L
lub(R) R; otherwise, lub(L) 6 L lub(R) 6 R. Thus, Property 3 holds.
prove Property 2 Property 4 cases:
H H1 = H H2 = :
lub(H) glb(H1 ):
glb(L) = glb(H1 H2 ) lub(H) = glb(H2 ) lub(H).
glb(R) = max(glb(H1 ) lub(H), glb(H2 ) lub(H)) = glb(H2 ) lub(H).
Thus, glb(L) = glb(R) (Property 2 holds).
glb(H2 ) H2 lub(H) H, Rules 1-14 intersection rules,
glb(L) L glb(R) R; otherwise, glb(L) 6 L glb(R) 6 R. Thus,
Property 4 holds.
glb(H) lub(H2 ):
glb(L) = glb(H) lub(H1 H2 ) = glb(H) lub(H1 ).
glb(R) = max(glb(H) lub(H1 ), glb(H) lub(H2 )) = glb(H) lub(H1 ).
Similar case above, clear Property 2 Property 4 hold.
H H1 6= H H2 = : then, H (H1 H2 ) = .
glb(L) = glb(H1 H2 ) lub(H) = glb(H2 ) lub(H).
739

fiDu & Alechina

glb(R) = max(0, glb(H2 ) lub(H)) = glb(H2 ) lub(H).
Similar cases above, clear Property 2 Property 4 hold.
H H1 = H H2 6= : then, H (H1 H2 ) = .
glb(L) = glb(H) lub(H1 H2 ) = glb(H) lub(H1 ).
glb(R) = max(glb(H) lub(H1 ), 0) = glb(H) lub(H1 ).
Similar cases above, clear Property 2 Property 4 hold.
H H1 6= H H2 6= :
since H1 , H2 , H intervals H1 H2 6= , H (H1 H2 ) 6= .
glb(L) = 0.
glb(R) = max(0, 0) = 0.
Rules 1-14, glb(L) L glb(R) R.
clear Property 2 Property 4 hold.
every case, Properties 1-4 hold.
Therefore, L = R.
Proof.[for Rule 17] Let
L = (H1 H2 ) H, R = H1 (H2 H).
Definition 7, LS= ( d1 H1 ,d2 H2 {d1 } {d2 }) H.
Rule 15, L = d1 H1 ,d2 H2 (({d1 } {d2 }) H).
Rule 13, ({d1 } {d2 }) H = H
({d1 } {d2 }).
Rule 15, H ({d
1 } {d2 }) = dH ({d} ({d1 } {d2 })).
Rule 13, L =
d1 H1 ,d2 H2 ,dH (({d1 } {d2 }) {d}).
Similarly, R = d1 H1 ,d2 H2 ,dH ({d1 } ({d2 } {d})).
prove L = R, sufficient show
({d1 } {d2 }) {d} = {d1 } ({d2 } {d}).
Let l = ({d1 } {d2 }) {d}, l = [|d1 d2 |, d1 + d2 ] {d};
r = {d1 } ({d2 } {d}), r = {d1 } [|d2 d|, d2 + d].
prove l = r cases:
[|d1 d2 |, d1 + d2 ]: Definition 7, l = [0, d1 + d2 + d].
d1 + d2 d, d2 + d1 , d1 + d2 .
Thus, d1 [|d2 d|, d2 + d]. Definition 7, r = [0, d1 + d2 + d].
6 [|d1 d2 |, d1 + d2 ]:
> d1 + d2 : Definition 7, l = [d d1 d2 , d1 + d2 + d].
d1 < d2 = |d2 d|.
Definition 7, r = [d d2 d1 , d1 + d2 + d].
< |d1 d2 |: Definition 7, l = [|d1 d2 | d, d1 + d2 + d].
d1 d2 : < d1 d2 , is, d1 > d2 + d.
Definition 7, r = [d1 d2 d, d1 + d2 + d].
d1 < d2 : < d2 d1 , is, d1 < d2 d.
Definition 7, r = [d2 d1 , d1 + d2 + d].
case, l = r. Therefore, L = R.

740

fiQualitative Spatial Logics Buffered Geometries

Appendix B. Alternative Proof Path-Consistency Lemma
appendix, would like provide sketch alternative proof idea
Path-Consistency Lemma, since may appeal readers proof
presented Section 5.3. alternative proof uses Lemma 44.
Lemma 44 distance constraint d(p, q) h DS(+ ) h =
6 , exist
+
d(p, q) m1 d(p, q) m2 DS( ) h = m1 m2 , m1 m2
identity definable intervals.
Proof. Lemma 31, d(p, q) h left-definable right-definable. Definition 16,
exists sequence distance constraints d(pi , pi+1 ) hi (p1 = p, pn = q, 0 < < n)
D(+ ), m1 = h1 ... hn1 , h m1 , h m1 greatest
lower bound (both value openness). Definition 15, d(p, q) m1 DS(+ ).
Lemma 21 Definition 14, m1 identity definable interval. Similarly, Definition 17, exists m2 h m2 , h m2 least upper bound
(both value openness), d(p, q) m2 DS(+ ), m2 identity definable interval. intersection rules, h = m1 m2 .
Proof.[sketch alternative proof Path-Consistency Lemma] Suppose D(+ )
path-consistent. exist d(p, q) g1 d(p, q) g2 DS(+ ), g1 6= ,
g2 6= , g1 g2 = . Lemma 44, exist d(p, q) m1 d(p, q) s1 DS(+ )
g1 = m1 s1 , m1 s1 identity definable intervals. Similarly,
g2 = m2 s2 , m2 s2 identity definable intervals. g1 g2 =
holds iff one following holds: m1 m2 = , m1 s2 = , s1 m2 = , s1 s2 = .
Without loss generality, let us suppose m1 m2 = . Lemma 35, m1 m2 =
iff (m1 m2 ) {0} = . Let = m1 m2 . identity definable interval.
Since d(p, q) m1 d(p, q) m2 DS(+ ), m1 6= , m2 6= , Lemma 31,
d(p, q) m1 d(p, q) m2 left-definable right-definable. Since d(p, q) m2 ,
d(q, p) m2 . Lemma 29, d(p, p) left-definable right-definable. rest
proof almost proof Path-Consistency Lemma (starting
Lemma 23, glb(h0 ) {0, , 2, 3, 4}) presented Section 5.3. discuss different
ways obtain given greatest lower bound (the role similar h0 ) check
whether derived every valid case.

Appendix C. Consequences Path-Consistency Lemma
appendix, state explicitly implications Path-Consistency Lemma.
Lemma 45 Let finite consistent set formulas. distance constraint d(p, q) h
DS(+ ), h 6= .
Proof. Follows immediately proof Path-Consistency Lemma.
Lemma 46 Let finite consistent set formulas. distance constraint d(p, p) h
DS(+ ), 0 h.
741

fiDu & Alechina

Proof. Suppose distance constraint d(p, p) h DS(+ ) 0 6 h. Definition 13
Definition 15, d(p, p) {0} D(+ ). Definition 15, d(p, p) (h {0}) = .
contradicts fact d(p, p) DS(+ ) (by Lemma 45). Therefore, 0 h.

References
Aiello, M., Pratt-Hartmann, I., & van Benthem, J. (Eds.). (2007). Handbook Spatial
Logics. Springer.
Allen, J. F. (1983). Maintaining Knowledge Temporal Intervals. Communications
ACM, 26 (11), 832843.
Balbiani, P., Condotta, J., & del Cerro, L. F. (1999). New Tractable Subclass
Rectangle Algebra. Proceedings 16th International Joint Conference
Artifical Intelligence, pp. 442447.
Bennett, B. (1996). Application Qualitative Spatial Reasoning GIS. Proceedings
1st International Conference GeoComputation, Vol. I, pp. 4447.
Bennett, B., Cohn, A. G., & Isli, A. (1997). Logical Approach Incorporating Qualitative Spatial Reasoning GIS (Extended Abstract). Proceedings 3rd
International Conference Spatial Information Theory, Vol. 1329 Lecture Notes
Computer Science, pp. 503504. Springer.
Chen, J., Cohn, A. G., Liu, D., Wang, S., OuYang, J., & Yu, Q. (2015). survey
qualitative spatial representations. Knowledge Engineering Review, 30 (1), 106
136.
Clementini, E., & Felice, P. D. (1996). algebraic model spatial objects indeterminate boundaries. Proceedings GISDATA specialist meeting Geographic
Objects Undeterminate Boundaries, pp. 155169.
Clementini, E., & Felice, P. D. (1997). Approximate Topological Relations. International
Journal Approximate Reasoning, 16 (2), 173204.
Clementini, E., Felice, P. D., & Hernandez, D. (1997). Qualitative Representation Positional Information. Artificial Intelligence, 95 (2), 317356.
Cohn, A. G., & Gotts, N. M. (1996a). Representing Spatial Vagueness: Mereological Approach. Proceedings 5th International Conference Principles Knowledge
Representation Reasoning, pp. 230241.
Cohn, A. G., & Gotts, N. M. (1996b). Egg-Yolk Representation Regions
Indeterminate Boundaries. Proceedings GISDATA Specialist Meeting
Geographical Objects Undetermined Boundaries, pp. 171187.
Cohn, A. G., & Renz, J. (2008). Qualitative Spatial Representation Reasoning.
Handbook Knowledge Representation, pp. 551596. Elsevier.
de Kleer, J. (1986). assumption-based TMS. Artificial Intelligence, 28 (2), 127162.
Du, H. (2015). Matching Disparate Geospatial Datasets Validating Matches using Spatial
Logic. Ph.D. thesis, School Computer Science, University Nottingham, UK.
742

fiQualitative Spatial Logics Buffered Geometries

Du, H., & Alechina, N. (2014a). Logic Part Whole Buffered Geometries.
Proceedings 21st European Conference Artificial Intelligence, pp. 997998.
Du, H., & Alechina, N. (2014b). Logic Part Whole Buffered Geometries.
Proceedings 7th European Starting AI Researcher Symposium, pp. 91100.
Du, H., Alechina, N., Hart, G., & Jackson, M. (2015). Tool Matching Crowd-sourced
Authoritative Geospatial Data. Proceedings International Conference
Military Communications Information Systems, pp. 18. IEEE.
Du, H., Alechina, N., Jackson, M., & Hart, G. (2016).
Method Matching Crowd-sourced Authoritative Geospatial Data. Transactions GIS.
http://dx.doi.org/10.1111/tgis.12210.
Du, H., Alechina, N., Stock, K., & Jackson, M. (2013). Logic NEAR FAR.
Proceedings 11th International Conference Spatial Information Theory, Vol.
8116 Lecture Notes Computer Science, pp. 475494. Springer.
Du, H., Nguyen, H., Alechina, N., Logan, B., Jackson, M., & Goodwin, J. (2015). Using
Qualitative Spatial Logic Validating Crowd-Sourced Geospatial Data. Proceedings 29th AAAI Conference Artificial Intelligence (the 27th Conference
Innovative Applications Artificial Intelligence), pp. 39483953.
Egenhofer, M. J. (1997). Query processing spatial-query-by-sketch. Journal Visual
Languages Computing, 8 (4), 403424.
Egenhofer, M. J., & Franzosa, R. D. (1991). Point Set Topological Spatial Relations. International Journal Geographical Information Systems, 5 (2), 161174.
Egenhofer, M. J., & Herring, J. R. (1991). Categorizing Binary Topological Relations
Regions, Lines, Points Geographic Databases. Tech. rep., University
Maine.
Fine, K. (1975). Vagueness, truth logic. Synthese, 30, 263300.
Frank, A. U. (1991). Qualitative Spatial Reasoning Cardinal Directions. Proceedings
7th Austrian Conference Artificial Intelligence, pp. 157167.
Frank, A. U. (1996). Qualitative Spatial Reasoning: Cardinal Directions Example.
International Journal Geographical Information Science, 10 (3), 269290.
Goyal, R. K., & Egenhofer, M. J. (2001). Similarity Cardinal Directions. Jensen, C. S.,
Schneider, M., Seeger, B., & Tsotras, V. J. (Eds.), Advances Spatial Temporal
Databases, Vol. 2121 Lecture Notes Computer Science, pp. 3655. Springer.
Guesgen, H. W., & Albrecht, J. (2000). Imprecise reasoning geographic information
systems. Fuzzy Sets Systems, 113 (1), 121131.
Haklay, M. (2010). good volunteered geographical information? comparative
study OpenStreetMap Ordnance Survey datasets. Environment Planning
B: Planning Design, 37 (4), 682703.
ISO Technical Committee 211 (2003). ISO 19107:2003 Geographic information Spatial
schema. Tech. rep., International Organization Standardization (TC 211).
743

fiDu & Alechina

Jackson, M., Rahemtulla, H., & Morley, J. (2010). synergistic use authenticated
crowd-Sourced data emergency response. Proceedings 2nd International
Workshop validation GeoInformation products crisis management, pp. 9199.
Kopczynski, M. (2006). Efficient spatial queries sketches. Proceedings ISPRS
Technical Commission II Symposium, pp. 1924.
Kutz, O. (2007). Notes Logics Metric Spaces. Studia Logica, 85 (1), 75104.
Kutz, O., Sturm, H., Suzuki, N., Wolter, F., & Zakharyaschev, M. (2002). Axiomatizing
Distance Logics. Journal Applied Non-Classical Logics, 12 (3-4), 425440.
Kutz, O., Wolter, F., Sturm, H., Suzuki, N., & Zakharyaschev, M. (2003). Logics metric
spaces. ACM Transactions Computational Logic, 4 (2), 260294.
Lehmann, F., & Cohn, A. G. (1994). EGG/YOLK Reliability Hierarchy: Semantic
Data Integration Using Sorts Prototypes. Proceedings 3rd International
Conference Information Knowledge Management, pp. 272279.
Li, S., Liu, W., & Wang, S. (2013). Qualitative constraint satisfaction problems: extended framework landmarks. Artificial Intelligence, 201, 3258.
Li, S., Long, Z., Liu, W., Duckham, M., & Both, A. (2015). redundant topological
constraints. Artificial Intelligence, 225, 5176.
Ligozat, G. . (1998). Reasoning Cardinal Directions. Journal Visual Languages &
Computing, 9 (1), 2344.
Lutz, C., & Milicic, M. (2007). Tableau Algorithm Description Logics Concrete
Domains General TBoxes. Journal Automated Reasoning, 38 (1-3), 227259.
Mackworth, A. K., & Freuder, E. C. (1985). Complexity Polynomial Network
Consistency Algorithms Constraint Satisfaction Problems. Artificial Intelligence,
25 (1), 6574.
Mallenby, D. (2007). Grounding Geographic Ontology Geographic Data. AAAI
Spring Symposium - Logical Formalizations Commonsense Reasoning, pp. 101106.
Mallenby, D., & Bennett, B. (2007). Applying Spatial Reasoning Topographical Data
Grounded Ontology. Proceedings 2nd International Conference GeoSpatial
Semantics, No. 4853 Lecture Notes Computer Science, pp. 210227. Springer.
Moratz, R., Renz, J., & Wolter, D. (2000). Qualitative Spatial Reasoning Line
Segments. Proceedings 14th European Conference Artificial Intelligence,
pp. 234238.
Moratz, R., & Wallgrun, J. O. (2012). Spatial reasoning augmented points: Extending
cardinal directions local distances. Journal Spatial Information Science, 5 (1),
130.
OpenStreetMap (2012). Free Wiki World Map. http://www.openstreetmap.org.
Ordnance Survey (2012). Ordnance Survey. http://www.ordnancesurvey.co.uk.
Pawlak, Z., Polkowski, L., & Skowron, A. (2007). Rough Set Theory. Wiley Encyclopedia
Computer Science Engineering. John Wiley & Sons, Inc.
744

fiQualitative Spatial Logics Buffered Geometries

Randell, D. A., Cui, Z., & Cohn, A. G. (1992). Spatial Logic based Regions Connection. Proceedings 3rd International Conference Principles Knowledge
Representation Reasoning, pp. 165176.
Renz, J., & Nebel, B. (2007). Qualitative Spatial Reasoning Using Constraint Calculi.
Aiello, M., Pratt-Hartmann, I., & van Benthem, J. (Eds.), Handbook Spatial Logics,
pp. 161215. Springer.
Roy, A. J., & Stell, J. G. (2001). Spatial Relations Indeterminate Regions. International Journal Approximate Reasoning, 27 (3), 205234.
Schockaert, S., Cock, M. D., Cornelis, C., & Kerre, E. E. (2008a). Fuzzy region connection
calculus: interpretation based closeness. International Journal Approximate
Reasoning, 48 (1), 332347.
Schockaert, S., Cock, M. D., Cornelis, C., & Kerre, E. E. (2008b). Fuzzy region connection calculus: Representing vague topological information. International Journal
Approximate Reasoning, 48 (1), 314331.
Schockaert, S., Cock, M. D., & Kerre, E. E. (2009). Spatial reasoning fuzzy region
connection calculus. Artificial Intelligence, 173 (2), 258298.
Sirin, E., Parsia, B., Grau, B. C., Kalyanpur, A., & Katz, Y. (2007). Pellet: practical
OWL-DL reasoner. Journal Web Semantics, 5 (2), 5153.
Skiadopoulos, S., & Koubarakis, M. (2004). Composing cardinal direction relations. Artificial Intelligence, 152 (2), 143171.
Smith, N. J. (2008). Vagueness Degrees Truth. Oxford University Press.
Stocker, M., & Sirin, E. (2009). PelletSpatial: Hybrid RCC-8 RDF/OWL Reasoning Query Engine. Proceedings 5th International Workshop OWL:
Experiences Directions.
Sturm, H., Suzuki, N., Wolter, F., & Zakharyaschev, M. (2000). Semi-qualitative Reasoning
Distances: Preliminary Report. Proceedings Logics Artificial
Intelligence, European Workshop, JELIA, pp. 3756.
van Beek, P. (1992). Reasoning Qualitative Temporal Information. Artificial Intelligence, 58 (1-3), 297326.
Wallgrun, J. O., Wolter, D., & Richter, K. (2010). Qualitative matching spatial information. Proceedings 18th ACM SIGSPATIAL International Symposium
Advances Geographic Information Systems, pp. 300309.
Wolter, F., & Zakharyaschev, M. (2003). Reasoning Distances. Proceedings
18th International Joint Conference Artificial Intelligence, pp. 12751282.
Wolter, F., & Zakharyaschev, M. (2005). logic metric topology. Journal
Symbolic Logic, 70 (3), 795828.
Zadeh, L. A. (1975). Fuzzy logic approximate reasoning. Synthese, 30 (3-4), 407428.
Zimmermann, K. (1995). Measuring without Measures: Delta-Calculus. Proceedings
2nd International Conference Spatial Information Theory, pp. 5967.

745

fiJournal Artificial Intelligence Research 56 (2016) 573611

Submitted 11/15; published 8/16

Study Proxies Shapley Allocations Transport Costs
Haris Aziz

HARIS . AZIZ @ DATA 61. CSIRO . AU

Data61/CSIRO University New South Wales (UNSW),
Sydney, Australia

Casey Cahan

CCAH 002@ AUCKLANDUNI . AC . NZ

University Auckland,
Auckland, New Zealand

Charles Gretton

CHARLES @ HIVERY. COM

Hivery,
Sydney, Australia;
Australian National University (ANU),
Canberra, Australia;
Griffith University,
Gold Coast, Australia

Philip Kilby

PHILIP. KILBY @ DATA 61. CSIRO . AU

Data61/CSIRO Australian National University (ANU),
Canberra, Australia

Nicholas Mattei

NICHOLAS . MATTEI @ DATA 61. CSIRO . AU

Data61/CSIRO University New South Wales (UNSW),
Sydney, Australia

Toby Walsh

TOBY. WALSH @ DATA 61. CSIRO . AU

Data61/CSIRO University New South Wales (UNSW),
Sydney, Australia

Abstract
survey existing rules thumb, propose novel methods, comprehensively evaluate
number solutions problem calculating cost serve location single-vehicle
transport setting. Cost serve analysis applications strategically operationally
transportation settings. problem formally modeled traveling salesperson game (TSG),
cooperative transferable utility game agents correspond locations traveling salesperson problem (TSP). total cost serve locations TSP length optimal
tour. allocation divides total cost among individual locations, thus providing cost serve
them. one important normative division schemes cooperative games,
Shapley value gives principled fair allocation broad variety games including TSG.
consider number direct sampling-based procedures calculating Shapley value,
prove approximating Shapley value TSG within constant factor NP-hard.
Treating Shapley value ideal baseline allocation, survey six proxies
relatively easy compute. proxies rules thumb procedures
international delivery companies use(d) cost allocation methods. perform experimental
evaluation using synthetic Euclidean games well games derived real-world tours calculated scenarios involving fast-moving goods; deliveries made road network
every day. explore several computationally tractable allocation techniques good proxies
Shapley value problem instances size complexity commercially relevant.

c
2016
AI Access Foundation. rights reserved.

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

1. Introduction
study transport scenarios deliveries consumer goods made depot locations
road network. location customer, e.g., vending machine shop,
requested goods, e.g. soda, milk, crisps. vendor plans implements deliveries
faced two vexing problems. first difficult hurdle solving combinatorial optimization
problem routing scheduling vehicles deliver goods cost-effective manner. Many varieties first problem exist (Golden, Raghavan, & Wasil, 2008), proposes shall
refer vehicle routing problem (VRP). begin investigation supposing VRP
solved heuristically, therefore assignment locations routes (and delivery vehicles)
made.
second vexing problem determining evaluate cost serve location.
Specifically, vendor must decide apportion costs transportation location
equitable manner. results cost serve analysis variety important applications. Using allocation directly vendor course charge locations allocated portion
transportation costs. realistically, vendors use cost allocations (re)negotiating
contracts customers; extracting higher per-unit delivery prices expensive customers. Supply chain managers also reference cost allocations deciding whether
include/continue trade particular location. Techniques informed cost allocations planning profitable transport business recently reviewed Ozener, Ergun, Savelsbergh
(2013). Finally, provided market conditions favourable, sales managers instructed acquire new customers territories existing cost allocations relatively high order share
cost delivery among locations.
Addressing second vexing problem, paper stems work fast-moving consumer goods company operates nationally Australia New Zealand. company
serves nearly 20,000 locations weekly using fleet 600 vehicles. industry partner
increasing economic pressure realise productivity improvements optimisation logistical operations. key aspect endeavour understand contribution location
overall cost distribution. study, focus individual route level single
truck, apportion costs deliveries route constituent locations.
formalise setting traveling salesperson game (TSG) (Potters, Curiel, & Tijs, 1992),
cost serve locations given solution underlying traveling salesperson problem
(TSP). formalised game, use principled solution concepts cooperative game
theory, particular Shapley value (Shapley, 1953), order allocate costs locations
fair economically efficient manner. unique axiomatic properties Shapley value
enticing industry partner, allocation fair reasonable comprehensible sense.
Charging customers fair manner provides strong justification delivery prices encourages
trust operator customer.
Calculating Shapley value game notoriously hard problem (Chalkiadakis, Elkind,
& Wooldridge, 2011). direct calculation Shapley value TSG requires computation
optimal solutions exponentially many distinct instances TSP. Sampling procedures
used approximating value, however offer practical solution larger
games. Moreover, prove polynomial-time -approximation Shapley value
constant 1 unless P = NP. order practically applicable, must able
calculate cost allocation location route, 600 unique routes, may change

574

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

daily weekly customers change order volumes. Hence, need methods return
values within minutes, hours. Allocations also used heuristically evaluate assignments
locations trucks larger VRP. setting must able estimate cost allocations
within seconds fractions second, minutes.
circumvent computational difficulties calculating Shapley values, work explores
six proxies1 Shapley value. investigates three simple rules thumb, including simple
distance measure seen employed various industrial engagements. include
analyze good proxies relative Shapley value, stated ideal cost allocation rule.
proxies develop offer tractable alternatives Shapley value, cases appeal
allocation concepts cooperative game theory (Peleg & Sudholter, 2007; Curiel, 2008).
Two proxies appeal well-known Held-Karp (Held & Karp, 1962) Christofides
(Christofides, 1976) TSP heuristics, respectively.
report detailed experimental comparison proxies using large corpus synthetic Euclidean games, problems derived real-world tours calculated fast-moving consumer
goods businesses cities Auckland (New Zealand), Canberra, Sydney (Australia).
experimentation uncovers novel computationally cheap proxy gives good approximations
Shapley value. evaluation also considers ranking locationsleast costly
induced Shapley proxy values. Ranking locations common request industrial
partner relevant when, example, interested identifying costly locations
serve. find two proxies, one novel proxy, provide good ranking accuracy
respect rank induced Shapley value.

2. Preliminaries
use framework cooperative game theory gain deeper understanding delivery
cost allocation problems (Peleg & Sudholter, 2007; Chalkiadakis et al., 2011). cooperative
game theory, game pair (N, c) N set agents size |N| = n second
term c : 2N R characteristic function. Taking N, c(S) cost subset S. cost
allocation vector x = (x1 , . . . , xn ) denoting cost xi allocated agent n. restrict
attention economically efficient cost allocations, allocations satisfying xi = c(N)
i.e. sum allocated costs equal cost serving grand coalition.
cooperative game (N, c), solution concept assigns agent N cost
(N, c). may one allocation satisfying properties particular solution
concept, thus necessarily single-valued, might give set cost allocations (Peleg &
Sudholter, 2007). sometimes omit (N, c) notation solution concepts
context clear. minimal requirement solution concept anonymity, meaning
cost allocation must depend identities locations. Prominent solution concepts
include core, least core, Shapley value. 0, say cost allocation
(multiplicative) -core (N, c) (1 + )c(S) N (Faigle & Kern, 1993).
0-core referred simply core. core -core empty. -core
1. use word proxy instead approximation ease discussion and, technically, many measures
stand-ins Shapley value, approximations it; i.e., give guarantee quantitatively provable
approximation.

575

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

non-empty smallest possible called least core. particular referred
least core value.2
work focuses single-valued solution concept called Shapley value (Shapley,
1953). Writing SVi (N, c) Shapley value agent i, formally have:
SVi (N, c) =

|S|!(|N| |S| 1)!
(c(S {i}) c(S)).
|N|!
SN\{i}



(1)

words, Shapley value divides costs based marginal cost contributions agents.
traveling salesperson problem (TSP) salesperson must visit set locations N =
{1, . . . , n} {0} starting ending special depot location 0. i, j N {0} 6= j, di j
strictly positive distance traversed traveling location j. Here, di j = traveling
directly j impossible. Taking distinct i, j, k N {0}, problem symmetric
di j = ji i, j N {0}. satisfies triangle inequality di j + jk
dik (Garey & Johnson, 1979).
TSP Euclidean location given coordinates (two dimensional) Euclidean
space; therefore di j Euclidean distance j. Euclidean TSP symmetric
satisfies triangle inequality.
tour given finite sequence locations starts ends depot 0. length
tour sum distances consecutive locations. example, length [0, 1, 2, 0]
d01 + d12 + d20 . optimal solution TSP minimum length tour visits every location.
NP-hard find optimal tour, generally polynomial-time -approximation
unless P = NP (Sahni & Gonzalez, 1976). -approximation given optimisation
problem algorithm runs instance x returns feasible solution F(x)
cost c(F(x)) related optimal solution OPT (x) follows (Papadimitriou, 1994):
c(F(x))
.
c(OPT (x))
Informally, bound relative error approximation function. i, j di j finite,
triangle inequality, symmetry hold, polynomial-time approximations exist TSP
problem (Held & Karp, 1962; Christofides, 1976).
Given TSP, corresponding traveling salesperson game (TSG) pair (N, c). N set
agents corresponds set locations.3 second term c : 2N R characteristic
function. Taking N, c(S) length shortest tour locations S. cost
allocation vector x = (x1 , . . . , xn ) denoting cost xi allocated location N.
special depot location, shall always take x0 = 0 (Potters et al., 1992). Typically, depot
operated agent distributing costs want incur costs himself. Hence,
refer n number locations, corresponding TSP n + 1 points.
2. 0-core transport game focus work empty. However, game convex, Shapley
value lies core (Tamir, 1989).
3. focus restriction general games delivery games (TSGs) therefore use location
instead agent ease exposition.

576

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

3. Useful Properties Shapley Value
discussing cost allocations industrial partners, concept fairness often primary concern. fair principled cost allocation scheme would allow explain charges
customers objective way; making whole process transparent. Shapley value
general games unique assignment costs satisfies three natural axioms: (1) anonymity,
cost allocated particular location depends impact visiting locations
total cost; (2) efficiency, entire cost serving N locations allocated; (3) strong
monotonicity (Young, 1985), given two games (N, c) (N, c0 ), : ci (S) c0 (S) = (N, c)
(N, c0 ); marginal contribution ci (S) player total cost coalition is:
(
c(S) c(S \ {i})
ci (S) =
c(S {i}) c(S)
/ S.
Due derivative axiomatic properties, Shapley value termed
important normative payoff division scheme cooperative game theory (Winter, 2002).
axioms alone make Shapley value attractive cost allocation setting.
Shapley value additional attractive properties terms existence computability
used cost allocation scheme. example, whereas 0-core empty, therefore
yield allocation (Tamir, 1989), Shapley value always exists TSG setting.
logistics, often fixed cost associated serving particular location, e.g., special
parking permitting. treat variant TSG locations associated fixed
cost addition transportation costs e.g. parking loading fees Shapley
value allocate fixed costs associated locations. Formally, given fixed cost
f (i) serving location i, f (i) need removed computing Shapley value,
follows. Suppose c characteristic function TSG defined above, c0 satisfies
identity c0 (S) = c(S) + f (i). Then, additivity propertity Shapley value (Shapley,
1953) SVi (N, c0 ) = SVi (N, c) + f (i).
delivery settings, additional observation charging locations according Shapley value may incentivise recruit new customers vicinity. Locations recruit
nearby locations vendor reasonably expect lower transportation costs allocated. detail, consider vendor serving locations N = {1, . . . , n}. vendors perspective,
adding new location, n + 1, existing delivery route clearly good idea revenue
generated delivering location greater marginal cost c(N {n + 1}) c(N)
new delivery. existing locations vicinity n + 1 already paying deliveries, charging additional customer marginal quantity c(N {n + 1}) c(N) typically
unfair. case, existing customers would likely subsidizing new customers, therefore
disincentivised finding new business vendor. Shapley value mitigates this,
expected provide recruitment incentives. Making discussion concrete, suppose
game Euclidean N = {x} single agent distance 100 depot new agent
distance 5 x. transportation cost serving {x, y} high 210. Clearly, charging new agent c({x, y}) c({x}) = 10 x continues pay around 200 unfair.
hand, vendor allocates costs according Shapley value, existing customers
costs decrease new agent joins.
Another possible benefit delivery settings that, characteristic function concave
Shapley value lies non-empty 0-core. Formally, concavity satisfied N,
577

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

N \ {i} : c(S {i}) c(S) c(T {i}) c(T ). Charging customers according Shapley/core
values actually guarantees incentivised recruit. Specifically, N : SVi (N
{n+1}, c) < SVi (N, c). words, Shapley allocation costs existing locations decreases
new customer n + 1 added. Unfortunately general TSGs necessarily concave
characteristic functions. However, long cost function roughly concave
necessary existing locations realise savings. practice synergies, incentives
recruitment routes charge according Shapley value. empirical
data, even game concave frequently observe incentives given Shapley
allocation. compared charging customers according marginal contribution costs,
explicitly disincentivise recruitment. Summarizing, agent knows locations
charged according Shapley value, typically expect incentives recruit new locations
vicinity.

4. Computing Shapley Value
focus shifts computing Shapley value. Considering games general,
noted direct evaluation Equation 1 requires sum exponentially many quantities.
direct approach calculation Shapley value therefore practical game
reasonable size. Indeed, starting Mann Shapley (1962), authors motivate auxiliary restrictions constraints, example size importance coalitions, order describe
games Shapley value calculated. recent literature proposes variety approaches directly calculate Shapley value certain games (Conitzer & Sandholm, 2006;
Ieong & Shoham, 2005), however efficient calculation value TSGs remained elusive.
require accurate baseline order experimentally evaluate proxies later develop
Shapley value TSG. purpose investigate exact general sampling-based
approximations Shapley value. treat transport setting specifically, describing novel
procedure exact evaluation Shapley value TSG following Bellmans dynamic
programming solution underlying TSP. also discuss general Shapley value
evaluated approximately using sampling procedure. study sampling approach TSGs
using two distinct characterisations Shapley value amenable sampling-based
evaluation. perform detailed empirical study sampling-based evaluations using Synthetic
TSG instances underlying TSP model Euclidean. closing give hardness proof
relating computation Shapley value TSGs, showing approximation Shapley value TSGs intractable.
4.1 Dynamic Programming
found steps performed dynamic programming (DP) algorithm underlying
TSP expose marginsi.e. terms form c(S {i}) c(S)that summed direct evaluation Equation 1. Shapley value TSG therefore computed side
effect DP procedure computes optimal solution underlying TSP. procedure
formally captured Algorithm 1: DP-TSP-Shapley. algorithm written assumes distance costs symmetric location 0 special depot location, assumptions
relaxed general case simply computing Shapley values leveraging dynamic
programming.

578

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

Algorithm 1 DP-TSP-Shapley
Input: N = {1, . . . , n} {0} locations di j cost travel j.
Output: List SV SVi N.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

// c(S, j) length shortest path starting 0, locations S, ending j.
c []
// (S) length shortest tour locations starting ending location 0.
[]
SV []
{1, . . . , |N|}
c({0, i}, i) d0,i
({i}) 2 d0i
SVi (|N|1)!
|N|! ({i})
end
{2, . . . , |N|}
subset N size
SDEPOT {0}
j
c(SDEPOT , j) miniS,i6= j c(SDEPOT \ { j}, i) + di j
end
(S) min jS c(SDEPOT , j) + j0

SVi |S1|!(|N||S1|1)!
(T (S) (S \ {i}))
|N|!
end
end
end

ideas made concrete following procedure outlined Bellman (1962).
equations heart TSP solution procedure recursively define cost function, c(S, j),
shortest path locations starting depot 0 ending j.4
c({ j}, j) = d0 j .
c(S, j) =

min (c(S \ { j}, k) + dk j ).

kS,k6= j

Following recursive definition, DP process iteratively tabulates c(S, j) successively
larger coalitions S. iteration subset size |S| < |N| procedure tabulates quantities
c(S, j) taking |S| = n. computing values c(S, 0) |S| < |N|, access characteristic function evaluation c(S) subtours locations S, follows:

c(S) = c(S, 0) = min(c(S, j) + j0 ).
jS

Therefore, one incrementally evaluate sum Equation 1 TSG, calculating optimal subtours progressively larger coalitions (supersets) within classical DP procedure. Intuitively, compute tour using Bellmans algorithm, additionally evaluating c(S, 0)
4. notations depart slightly Bellmans seminal work. Whereas take c(S, j) cost optimal
tour-prefix path (i.e. starting depot 0 ending j), Bellman originally took c(S, j) cost optimal
tour-suffix paths starting j, traversing locations ending depot 0.

579

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

encountered subset obtain quantities required calculate marginal costs locations.
therefore highlighted concrete relationship classical procedure TSP
Shapley value corresponding TSG. dynamic programming algorithm fast
18 locations, size table number subsets become unmanageable.
4.2 Computational Complexity
consider, general setting TSG, complexity calculating Shapley
value. prove Shapley value location TSG cannot approximated
within constant factor polynomial-time unless P = NP.
Theorem 1 polynomial-time -approximation Shapley value location
TSG constant 1 unless P = NP.
Proof. Let G(N, E) graph nodes N edges E. -approximation exists use
solve NP-complete Hamiltonian cycle problem G. First, G, construct complete
weighted undirected graph G0 (N, E 0 ), (i, j) weight 1 (i, j) transitive closure
E, otherwise weight n!. Hamiltonian cycle G, Shapley value
N TSG posed G0 1. Suppose Hamiltonian cycle G.
show exists permutation N induces large Shapley value node j follows:
repeatedly add node N\ j remains Hamiltonian cycle amongst elements
; node add j. marginal cost adding j least n!.
Shapley value j average cost adding coalition N \ j, therefore Shapley
value least . Even though edge weights G0 large, represent G0 compactly
O(log(n) + n2 log()) space. -approximation G0 j therefore decides existence
Hamiltonian cycle G.
q

4.3 Sampling-Based Evaluation
Using either dynamic programming solution, indeed state-of-the-art TSP solver Concorde (Applegate, Bixby, Chvatal, & Cook, 2007) direct calculation Shapley value,
find impractical compute exact Shapley value instances TSG larger 10
locations (recall include depot, hence corresponding TSPs 11 points).
direct method requires exponential number characteristic function computations,
requires solving NP-hard problem. Figure 4.3 shows exponential increase runtime
computing Shapley value experimental setup (described detail Section 4.4) via
direct enumeration method. obtain accurate baseline games commercially interesting
size investigation turns sampling procedures. Indeed, Shapley value
population average reasonable estimate value using sampling procedure.
first use sampling approximate Shapley value games proposed studied
Mann Shapley (1960). Perhaps elegant general method proposed Mann
Shapley called Type-0 sampling. method repeatedly draws permutation locations
uniformly random. marginal cost agent calculated taking difference
cost serving agents including permutation cost serving agents
proceeding i. repeatedly sampling permutations computing marginal costs including
agent way, arrive unbiased estimate Shapley value.
580

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

Figure 1: Runtime computing Shapley value via brute force enumeration calls Concorde instances 4 10 locations. graphs show mean
standard deviation running time 1,070 games per number locations. Comparing Figure 3 observe time increasing exponentially, practical
limit hit around 10 locations.

Type-0 sampling appeared years various guises, reported different
names literature approximating power indicesof Shapley value one
coalitional games. recent rediscovery Type-0 sampling ApproShapley algorithm
Castro, Gomez, Tejada (2009); also provide asymptotic bounds sampling error
ApproShapley. ApproShapley shall focus sampling work, however prior giving
details, worth briefly reviewing classes game sampling-based evaluations
explored. Bachrach, Markakis, Resnick, Procaccia, Rosenschein, Saberi (2010)
previously examined Type-0 sampling simple gamesi.e. value coalition either
0 1deriving bounds probably approximately correct. words, actual Shapley
value lies within given error range high probability. Continuing line work, Maleki,
Tran-Thanh, Hines, Rahwan, Rogers (2013) show range variance marginal
contribution players known ahead time, focused (termed stratified) sampling
techniques may able decrease number samples required achieve given error bound.
methods approximating Shapley value, specifically weighted voting games,
appeared literature including based multi-linear extensions (Leech, 2003; Owen,
1972) focused random sampling (Fatima, Wooldridge, & Jennings, 2008, 2007). recently,
Type-0 sampling computing Shapley value applied planning setting set
delivery companies attempt pool resources order effectively service probabilistic
set orders appear within territory rolling horizon (Kimms & Kozeletskyi, 2015).

581

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

calculate Shapley value TSG via sampling employ Type-0 method suggested
Mann Shapley (1960) Castro et al. (2009) called ApproShapley; pseudocode given
Algorithm 2. Writing (N) set |N|! permutation orders locations N, taking (N)
write subset N precedes location . alternative formulation
Shapley value written terms (N), noting value equals marginal cost
location construct coalitions possible ways, follows.
SVi (N, c) =

1
(c(i {i}) c(i )).
|N|! (N)

(2)

sampled permutation, ApproShapley evaluates characteristic function
|N| computing length optimal tour set locations i-sized prefix.
construction, cost allocation produced ApproShapley economically efficient. small
important optimisation, work cache result evaluation characteristic
function avoid solving TSP twice. Note lines 15 17 Algorithm 2,
normalize values sum 1.0, strictly necessary since given algorithm efficient.
However, include code proxies algorithms surveyed return cost vector
sums 1.0.
Algorithm 2 ApproShapley

Algorithm 3 SubsetShapley

Input: N = {1, . . . , n} locations cost c(S) Input: N = {1, . . . , n} locations cost c(S)
serve subset N samples.
serve subset N samples.
Output: List SV SVi N.
Output: List SV SVi N.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

SV []
1 |N|
SVi 0
end
SampleNumber 1
// RAND(X) returns random element X.
Perm RAND((N))
0/
1 |N|
{Permi }
SVPermi SVPermi +(c(S)c(S\{Permi }))
end
end
TotalValue SVi
1 |N|
SVi SVi (c(N)/TotalValue)
end
return SV

582

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

SV []
1 |N|
SVi 0
end
SampleNumber 1
1 |N|
0/
j 1 n
// RAND(X) returns random element X.t
6= j RAND({0, 1}) = 1
{ j}
end
end
SVi SVi + |S|!(n |S| 1)!
(c(S {i}) c(S))
end
end
TotalValue SVi
1 |N|
SVi SVi (c(N)/TotalValue)
end
return SV

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

work, also considered alternative sampling method, samples permutations, rather subsets locations implied formulation Equation 1 Section 2.
fewer subsets permutations, seemingly advantage sampling-based
evaluation Shapley value. Using limited number subsets estimate Shapley value
explored, shown effective measure, Papapetrou, Gionis, Mannila (2011).
name method SubsetShapley provide pseudocode Algorithm 3. Like ApproShapley,
method produces economically efficient allocation. ApproShapley, estimate SVi
updated per drawn permutation SubsetShapley draw single random
subset, update estimate one location. Thus, SubsetShapley, every iteration
sampling loop Line 6 draw different set N \ {i} uniformly random location i, making two methods comparable based total number updates per location per
iteration. However, use one sample ApproShapley locations one sample per
location SubsetShapley. i, update SVi weighted marginal contribution,
formally SVi SVi + |S|!(n |S| 1)!(c(S i) c(S)). coefficient |S|!(n |S| 1)! ensures
subset sampled locations, account number permutations locations
ordered location i. Note without term, algorithm converge
Shapley value limit.
4.4 Experimental Setup Evaluation Sampling Methods
important later experimental evaluation confident
sampled sufficient number times sufficient number games establish confidence
sampling scheme ensure statistical significance results. must ensure that,
every game, taken enough samples high probability low error
individual Shapley value. overall evaluation must ensure sampled enough
games representative population possible games. section described
experimental setup derive precise statistical bounds results.
proxies estimators Shapley value consider yield economically efficient
allocations cost optimal tour. reason, discuss Shapley value
proxies terms induced fractional (also called normalized) allocation cost
optimal tour. Formally, iSV = SVi/ jn SV j . Fractional allocations allow us compare efficient
non-efficient cost allocations equal footing, way would used operational contexts
transport settings. formulation also enables us allocate cost optimal route
solve NP-hard TSP once.
generated collection Euclidian games call Synthetic dataset. Synthetic
dataset generate locations |N| {4, . . . , 20} 100 100 unit square. coordinates
locations generated independent identically distributed (i.i.d.) manner represented
32-bit floating point values.5 n players add depot location, also chosen uniformly
random square. Hence, reported results total n + 1 locations
underlying routing problem n locations must costs allocated them. timing experiments reported performed computer Intel Xeon E5405 CPU running 2.0 GHz
4 GB RAM running Debian 6.0 (build 2.6.32-5-amd64 Squeeze10). Additional computing
power non-timing experiments provided Data61/NICTAs heterogenous compute cluster.
5. Corpus available online https://github.com/nmattei/ShapleyTSG

583

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

use statistical measures report empirical results. provide brief overview
key concepts refer reader textbook Corder Foreman (2009) details.
Denote |x| absolute value quantity x. Writing x denotes average set {x1 , . . . , xn }

p let nxi denote estimate value xi set X. standard deviation ( ) x is: =
2
1/n
i=1 (xi x) . measure accuracy use root mean squared errors (RMSE), common
metric quantify error number predictions. set k paired observations X =
{x1 , . . . , xk } estimatesq
X = {x1 , . . . , xk }, RMSE X X (the RMSE X
respect X) is: RMSE = 1/k ki=1 (xi xi )2 .
perform similar analysis Castro et al. (2009) determine number samples
required high confidence values obtained via sampling methods setting.
establish error sampling procedure probability greater 1 ,
use central limit theorem assumption errors normally distributed, giving:

2
ci | ) 1 ,
= P(|SVi SV
2
Z N(0, 1) normal random variable. Given game, know variance
locations permutations, infeasible compute value. estimate
) minimum (xi ) change cost function individual
variance given maximum (xmax
min

location N. co-located depot minimum impact cost xmin
= 0.

100 100 unit square maximum possible distance two points 100 2 142.
greatest impact cost location added opposite depot

along diagonal, causing increase cost equal xmax
= 2 142 = 284.
maximum variance random variable reached variable takes two extreme
values probability 1/2. use following inequality estimate variance:
2
No. Samples Z/2



)2
xi + xmin
xi + xmin
(xi xmin
1
1
2 (xmax
max
)2 + (xmin
max
)2 max
.
2
2
2
2
4
Applying previous equation yields formula determining error setting:
)2
(xmax
ci SVi | ) 1 .
= P(|SV
4 2
tolerate error bound significant effect size games
actually use testing (as sampling time consuming). Selecting = 0.75 means
locations Shapley value 0.75 distance units (kilometers),
low fast moving consumer goods setting. derivation error gives us absolute
bound pairwise error locations actual Shapley value SVi estimated Shapley
ci , must derive maximum possible error points order bound error
value SV
SV
. total error game given by:
2
No. Samples Z/2


SVi +
n
j (SV j + )

=

SVi +
n
j (SV j ) + n

=

SVi + 0.75
.
n
j (SV j ) + 0.75n

Observe nj (SV j ) exact cost grand tour points overestimate
0.75n. Hence, overestimating grand tour 15 distance units (kilometers)
n = 20 instances. Thus error locations SV negligible. want small,
giving us high confidence converged; set = 0.005, giving us 99.5% probability
584

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

error less . Substituting error equation (Z0.05/2 = Z0.0025 = 2.81)
get:
(2 142)2
283, 052
4 0.752
order draw conclusions calculated RMSE values must statistical confidence mean (RMSE) set games. RMSE average normalized
values, take values [0, 1]. Assuming errors problem normally
distributed bound variance using techniques described above, arriving
variance 2 = 1/4. use standard techniques statistics engineering (Natrella, Croarkin, & Guthrie, 2012) determine number games need use order
95% confidence interval absolute error measurement RMSE,
aggregate measure error locations games, within 0.03 (or roughly 3%):
2


1/4

2
2
No. Games = 1, 070 Z=0.05
= (1.96)
1, 067.
2
(0.03)2
No. Samples = 300, 000 2.812

Intuitively, means 95% sure re-ran entire experiment new values
mean error particular proxy would fall within 3%. Hence say mean error value,
measured set 1,070 games, accurate.
compare performance ApproShapley SubsetShapley using Synthetic dataset.
use Concorde (Applegate et al., 2007) evaluate characteristic function TSG
solving underlying TSP.
optimal tour lengths calculated Concorde cached speed running time. Therefore,
never re-evaluate TSP set points. TSPs less four locations evaluated brute force. game Synthetic dataset calculated exact Shapley value
every location, could compare sampled allocation exact counterparts.
find ApproShapley method sampling permutations provides faster convergence,
seen Figure 2. 1000 iterations ApproShapley achieves RMSE 0.01, significantly smaller standard deviation SubsetShapley. Moving 100, 000 samples bottom
row Figure 2, see mean RMSE ApproShapley still significantly lower
SubsetShapley.
Figures 3 4 depict mean running time number calls Concorde two algorithms, respectively. see ApproShapley runs faster SubsetShapley instances
tested. difference runtime grows number locations increases. ApproShapleys
faster running time likely due need randomly generating one permutation instead
n sets. Figure 4 provides insight behavior two algorithms. SubsetShapley fills
cache much quicker ApproShapley, explains later flattening runtime curve
ApproShapley seen Figure 3. methods eventually evaluate 217 possible points, saturating cache. However, early filling cache SubsetShapley translate faster
overall runtime. practice, ApproShapley achieves lower error, earlier, continues converge
towards error 0 faster SubsetShapley.

585

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

Figure 2: Comparison accuracy ApproShapley (left) SubsetShapley(right) 10,000
iterations (top) 100,000 iterations (bottom) TSGs 10 locations. graphs
show RMSE standard deviation 1,070 instances sampled
actual Shapley values. ApproShapley converges fewer samples stable samples SubsetShapley.

586

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

Figure 3: Comparison runtime performance ApproShapley (left) SubsetShapley (right)
TSGs 4 17 locations (not including depot). graphs show mean
standard deviation 1,070 instances running time respective algorithm.
ApproShapley needs generate one permutation, compared SampleShapleys n sets, generally runs quickly.

Figure 4: Comparison number calls Concorde function sample number made
ApproShapley (left) SubsetShapley (right) TSGs 4 17 locations (not
including depot). graphs show mean standard deviation 1,070 instances number calls Concorde. SubsetShapley fills cache much quicker
ApproShapley, explains later flattening runtime curve ApproShapley seen Figure 3. However, earlier cache filling lead decrease
running time.

587

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

5. Proxies Shapley Value
use ApproShapley requires solve NP-hard problem time evaluate
characteristic function. feasible small TSG instances less dozen locations,
however create unacceptable computational burden larger, realistically sized games.
describe variety proxies Shapley value require much less computation
practice. seen proxies use real-world allocate costs, hence
inclusion analysis. define discuss proxies terms induced fractional
allocation, iSV = SVi/ jn SV j , discussed Section 4.4. overview worst case practical
running times algorithms presented Table 1.
Method
Proxy

Worst Case
Runtime

10 Loc.

ApproShapley (Concorde)

Exponential

30sec.

4,500 sec.

> 90,000 sec.

Shortcut Distance ( HORT )

Exponential

5 sec.

10 sec.

15 sec.

Exponential

20 sec.

25 sec.

30 sec.

Depot Distance ( EPOT )

O(n)

1 sec.

1 sec.

1 sec.

Moat Packing ( OAT )

Exponential

5 sec.

5 sec.

5 sec.

Christofides ( C HRIS )

O(n3 )

30 sec.

2,500 sec.

40,000 sec.

Exponential

5 sec.

5 sec.

5 sec.

Re-routed Margin

Blend

( R EROUTE )

( B LEND )

Practical Running Time
20 Loc.
30 Loc.

Table 1: Summary proxies Shapley value surveyed paper.

5.1 Depot Distance ( EPOT )
distance depot i.e. di0 location straightforward proxy.
allocate cost location proportional di0 . fraction allocation location
iD EPOT =

di0
.
n
i=1 di0

proxy, location twice distant depot another pay twice cost.
evaluate proxy time linear number locations. practice, computing
value instantaneous.
5.2 Shortcut Distance ( HORT )
Another proxy straightforward calculate used commercial routing
software shortcut distance. change cost realized skipping location
traversing given optimal tour. Without loss generality, suppose optimal tour visits locations according sequence [0, 1, 2, . . . ]. Formally, HORTi = di1,i + di,i+1 di1,i+1 ,
locations 0 n + 1 depot, di j cost travel location j. fractional

588

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

allocation given shortcut distance
HORT =

HORTi
.
jN HORT j

evaluate proxy solving one TSP instance one operation per location. practice
compute metric less 30 seconds.
5.3 Re-routed Margin ( R EROUTE )
location N, R EROUTEi defined c(N) c(N\i)). allocation player
computed two calls optimal TSP solver. fractional allocation
iR EROUTE =

(c(N) c(N\i))
.
j=N (c(N) c(N\ j))

evaluate proxy solving n + 1 TSPs: one grand tour one leaving
location. practice compute metric nearly instantaneously.
5.4 Christofides Approximation ( C HRIS )
sophisticated proxy obtained use heuristic performing characteristic function
evaluations ApproShapley, rather solving individual induced TSPs optimally.
proxy use sampling estimate Shapley value use approximation algorithm
estimate underlying TSP cost. approximate underlying TSP characteristic function,
Christofides (1976) heuristic, O(|N|3 ) time procedure used. obtain fractional quantity
iC HRIS , divide allocation location total allocated costs. Assuming symmetric
distance matrix satisfying triangle inequality, Christofides heuristic guaranteed yield
tour within 3/2 length optimal tour.
briefly describe heuristic works. TSP instance represented complete undirected graph G = (V, E), one vertex V location, edge E every
distinct pair vertices. i, j V edge (i, j) E weight di j . tour obtained
follows: (1) compute minimum spanning tree (MST) G, (2) find minimum weight perfect
matching complete graph vertices odd degree MST (typically performed
using Hungarian Algorithm), (3) calculate Eulerian tour graph obtained combining
MST Step 1 matched edges Step 2 (this guaranteed yield Eulerian
multigraph, i.e., graph every vertex even degree), (4) obtain final tour TSP
removing duplicate locations Eulerian tour.
best case, call Christofides heuristic return solution exactly
solution TSP. Hence, method requires many calls per number locations derived
Section 4.4. Figure 5 shows runtime ApproShapley replace calls Concorde
calls program solves TSP using Christofides heuristic. Comparing results
Figure 3, see small numbers locations ( 10) runtimes Concorde
Christofides heuristic almost same. However, number locations grows,
Christofides heuristic shows significant speed improvement. commercially interesting sizes,
20 locations 300,000 samples, computing C HRIS takes order 2500 seconds (about 30
minutes). practically computable ( 12 hours) problems 30+ locations, shown
Table 1.
589

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

Figure 5: Runtime performance ApproShapley calls Christofides heuristic TSGs
4 17 locations (not including depot). graphs show mean standard
deviation 1,070 instances running time respective algorithm. Comparing Figure 3 see Christofides heuristic decreases runtime
decrease grows larger number locations increases.

5.5 Nested Moat-Packing ( OAT )
Another way allocate costs TSGs based dividing locations regions using concept
called moat (Cook, Cunningham, Pulleylank, & Schrijver, 1998). Intuitively, given Euclidian
TSP set locations N depot location 0, moat closed strip constant width
separates set locations N compliment S. assume without loss generality
0 always S. order deliver location S, one would need traverse moat
order reach points set S, cross moat return point S. Hence,
reasonable cost allocation charge locations twice cost traversing moat
surrounding S. locations delivery truck, would reason cross
territory moat surrounding S. following use techniques described Faigle,
Fekete, Hochstattler, Kern (1998) additionally refined Ozener et al. (2013)
extensions setting.
Formally, given set locations N {0}, let N compliment S, let
set bipartitions locations {S, S} assumption 0 S. Let wS,S
width (distance) moat set S. refer vector moats ~w width
individual moat wS . Locations cannot occur moat, moat
strip unoccupied area map. Additionally, one needs consider circular moats
gives minimal straight-line distance location inside moat less equal
moat width. order well-formed cost allocation want moats large

590

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

possible, i.e., maximum width. Hence, formulate linear program find maximum
moat packing:
MPV = max



wS,S

S,SM

wS,S 0
s.t.



{S, S}

(3)

wS,S di j i, j N {0}.

iS, jS

Though Equation 3 exponentially many constraints, solved time polynomial
number locations using dual techniques, returning at-most polynomial number moats
width 0. shall use notation ~w refer small set moats non-zero width (Faigle
et al., 1998). vector moat widths ~w given solution Equation 3 may many
moats overlap, leading ambiguities widths allocate locations. Thus,
arbitrary solution Equation 3 yield cost allocation. reason must refine
maximum moat packing maximum nested moat packing. nested moat packing two
distinct intersecting subsets cannot encapsulated non-empty moat unless one
coalitions subset other. Formally, packing nested S0 , S00 s.t.
wS0 > 0 wS00 > 0, S0 S00 6 0/ either S0 S00 S00 S0 . optimal solution ~w
Equation 3 yielding objective value MPV set partitions corresponding nested
packing MPV (Cornuejols, Naddef, & Pulleyblank, 1985; Faigle et al., 1998).
nested moat packing clear set moats must crossed reach location
location, point derive cost allocation location. Figure 6
concrete example nested moat packing 6 locations. Figure 6 6 locations
moat (light colors). Additionally, moats locations 5 6 surrounded
outer darker moat set {5, 6}.
Given non-nested vector moats ~w follow post-processing procedure described
Ozener et al. (2013). nesting criteria defined violated must three distinct
non-empty sets locations S, S0 S00 , wSS0 > 0 wS0 S00 > 0. Given ~w update
values follows: let min{wSS0 , wS0 S00 }, make following assignment updates moat
widths: wS wS + , wS00 wS00 + , wSS0 wSS0 , wS0 S00 wS0 S00 . iterative
procedure terminates yielding nested packing, taking exponential time worst case. However,
experiments found nesting takes fraction second. leaves us
allocation:
1
wS
1
wS
iM OAT =

=
.

MPV wS >0,iS |S| wS >0 wS wS >0,iS |S|
two key observations allocation derived (nested) moat packing.
First, 2 MPV , i.e., sum crossing moats twice, exactly value Held-Karp
relaxation underlying TSP TSP symmetric satisfies triangle inequality (Held
& Karp, 1962). Thus, 2 MPV lower bound optimal tour underlying TSP
3 MPV upper bound.6 Secondly, observe allocation derived nested moat
packing xi cost location satisfies xi c(N) N : xi (1 + )c(S).
constraints exactly multiplicative core defined preliminaries
6. tightness bounds Held-Karp relaxation, i.e., integrality gap, longstanding open question
combinatorial optimisation (Cook et al., 1998).

591

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

Figure 6: optimal nested moat-packing (colored regions) optimal tour (line) TSP
6 locations. locations indicated labels {1,2, . . ., 6}, occur center
light moats. moat around locations 1 4 (light colored regions) associated
one location. moat around set locations {5, 6} (dark colored region)
nested. larger moat encloses two smaller, independent moats (light colored regions) around locations 5 6, respectively. 7 moats total, optimal
tour must cross moat twice.

-core. Although allocation achieved using nested moat packing distribute 3 MPV
economically efficient, core allocation approximate cost. Faigle et al. (1998) present
proof nested moat packing provides 21 -core allocation respect actual cost
function location distributing 3 MPV ; conjecture 13 .
5.6 Blended Proxy ( B LEND )
interesting question whether blending set proxies practically computable
could provide good estimate actual Shapley value. Framing prediction machine
learning problem, want learn model predict output SV given input set consisting
easily computable proxies, { EPOT , HORT , R EROUTE , OAT }. analysis section
carried using SciKitLearn (Pedregosa et al., 2011), machine learning library Python.
First, need decide sort model best setting. proxies
attempting estimate value, correlated. Consequently, one place start
use principal component analysis (PCA) (Bishop, 2006) understand much variance
captured low dimensional model given input set. use SciKitLearn run PCA
decomposition set Synthetic data. SciKitLearn uses linear algebra package
SciPy perform singular value decomposition (SVD) data matrix; keeping
significant singular vectors project data lower dimensional spaces. decomposition
shows 98% variance explained one component (vector), depicted Figure
7. Hence, simple linear blend subset proxies provide good predictive power.

592

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

Figure 7: explained variance ratio SV given PCA decomposition input set
{ EPOT , HORT , R EROUTE , OAT } (left). Using 1 dimensional model input set
explain 98% observed variance SV . Prediction error graph 10 fold
cross-validation Synthetic dataset linear model blending EPOT OAT
(right). point represents prediction error location Synthetic dataset
dotted line (y = x) would ideal predictor. correlation actual
predicted values 10 fold cross-validation R2 = 0.8825 = 0.0025.

selected simple linear model, must decide elements input
set proxies, { EPOT , HORT , R EROUTE , OAT }, use. want use minimal set
features, using many features cause overfitting (Bishop, 2006). selection, use
SciKit (Pedregosa et al., 2011) perform k-best feature selection takes input
set turn computes cross correlation element others, converted
using ANOVA score significance (p) value feature. compare scores
find individual elements input set significant. find
scores elements input set statistically significant, hence useable. Looking
normalized scores themselves, { EPOT = 1.0, HORT = 0.0260, R EROUTE = 0.4946, OAT =
0.6305}, see EPOT OAT two highest scoring indicators. choose limit
linear model two highest scoring elements EPOT OAT significantly higher
scoring others adding elements may cause overfitting.
model input variables train on, need learn model
perform cross-validation. tests take full Synthetic dataset perform 10-fold
cross-validation (Bishop, 2006). perform k-fold cross-validation, take dataset break
k equal sized folds F = { f1 , . . . , fk }. hold one pieces training
set turn (i.e., train F \ { fi }) use test set (i.e., predict fi ). select 10 folds
used cross-validation use stratified k-fold sampling, ensures every k fold
statistical distribution whole training set (Pedregosa et al., 2011). Since using
linear model, use coefficient determination, R2 , fitness measure. 10 fold
cross-validation, get mean R2 = 0.8825 standard deviation = 0.0025. graph

593

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

predicted SV function actual value shown Figure 7. low shows
model robust high value R2 indicates model good predictor. training
entire Synthetic dataset get final model:
B LEND = 0.579 EPOT + 0.318 OAT + 0.009.

6. Analysis Nave Proxies
refer three proxies EPOT , HORT R EROUTE nave. Contrastingly, call
C HRIS , OAT B LEND sophisticated proxies. formulation nave proxies EPOT
HORT make amenable direct analysis worst case performance. consider
settings nave proxies EPOT HORT perform quite badly.
order illustrate this, consider TSG depot one corner square dimension one location 3 corners. Locations nearest depot indexed 1
3, third location indexed 2.
Location 1



Location 2


Depot


Location 3



nave proxies yield following allocations:

SV
EPOT HORT
1, 3 0.299a 0.293a 0.333a
2 0.402a 0.415a 0.333a
Observe EPOT performs well case (maximum 11% error) HORT (minimum 16% error).
identify pathological cases HORT EPOT proxies perform
poorly. first result demonstrates EPOT HORT may under-estimate true Shapley
value badly.
Theorem 2 exists n location TSP problem which, location i, ratio iDEPOT/iSV
goes 0 n goes . instance, ratio iSHORT/iSV goes 0 n goes (n)
locations.
Proof. Suppose first n 1 locations distance depot, whilst nth location
located distance opposite direction depot.

Locations 1, . . . , n 1


Depot

594

Location n

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

Note normalization constant SV , jn SV j = 4a. nSV = 2a/4a = 1/2 since cost
adding nth location coalition 2a. Leaving, < n,
iSV =

2a/(n1)

4a

=

1
.
2(n 1)

hand, normalization constant EPOT , ni=1 di0 = na since locations
equidistant depot. Giving, n, iD EPOT = 1n .
Thus < n,
1/n
iD EPOT
2n 1
=
=
SV
1/2(n1)
n

goes 2 n . hand,
1/n
nD EPOT
1
=
=
1/2
nSV
2n

goes 0 n .
Note shortcut proxy, HORT performs poorly example. < n, HORT = 0
since locations co-located, leaving nS HORT = 1. < n iSV = 1/2(n 1). Thus,
< n,
HORT
0
=0
=
SV
1
/2(n1)



nS HORT
1
=
=2
SV
1
n
/2
q

second result demonstrates HORT also grossly over-estimate true Shapley
value.
Theorem 3 exists n location TSG ratio iSV/iDEPOT goes 0 n goes
(n) locations.
Proof. Suppose first n 1 locations distance depot, whilst nth location
located distance (n + 1)a depot opposite direction.

Locations 1, . . . , n 1

a(n + 1)
Depot

Location n

Note normalization constant SV , jn SV j = 2a + 2a(n + 1) = 2a(n + 2). Shapley
2a
value SVi < n n1
, thus
iSV =

2a/n1

2a(n + 2)

=

595

1
.
(n 1)(n + 2)

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

fractional Shapley allocation location n
nSV =

2a(n + 1) 1
= .
2a(n + 2) 2

normalization constant EPOT ni=1 di0 = a(n 1) + a(n + 1) = 2an. location n
assignment distance based proxy
nD EPOT =

a(n + 1) n + 1
=
.
2an
2n

< n,
iD EPOT =


1
= .
2an 2n

Thus, location n
nSV
nD EPOT
goes 1 n goes .
< n

iSV

iD EPOT

=

1/2

=

n+1/2n

1/(n1)(n+2)
1
2n

=

=

2n
2n + 1

2n
(n 1)(n + 2)

goes 0 n goes .
HORT < n, HORT = 0 leaving nS HORT = 1. Thus, nSV/nSHORT = 1/2
< n, iSV/iSHORT undefined.
q
third result demonstrates HORT may under-estimate Shapley value badly even
simple examples may embedded larger problems.
Theorem 4 exists 2 location TSG instance
locations.

HORT/ SV

= 0 one two

Proof. Suppose first location located distance depot second location
located distance farther road.

Depot


Location 1

Location 2

first location 1S HORT = 0, since removing effect distance second
location. leaves 2S HORT = 1. Shapley value first location
SV =

2a 0
+ = a.
2
2

gives SV = a/4 thus
HORT
0
=
= 0.
a/4
SV
q
fourth final result demonstrates HORT may over-estimate Shapley value badly.
596

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

Theorem 5 exists four location TSG SV/ SHORT = 0 two four cities.
Proof. Consider four location TSG locations 1 2 depot
cities 3 4 distance depot other.
Location 1

ka


Depot



Location 4




ka

Location 2

Location 3

note k a, hide terms O(). marginal cost saved
skipping location , means locations allocation according
HORT , namely {1, . . . , 4}, HORT = 1/4.
Note normalization constant SV , jn SV j = 2ka + O(). compute Shapley
values locations 1 2 observe that, given permutation, location adds multiple
, thus symmetry, {3, 4},
iSV =

O()
2ka + O()

compute Shapley value locations 3 4 observe that, matter permutation appear, first contributes 2ka contributes . Consequently,
symmetry, locations {3, 4},
iSV

=

2ka+O()
2

1
= .
2ka + O() 2

Thus, locations {1, 2},
SV
HORT

=

O()
2ka+O()
1/4

=

4O()
.
2ka + O()

term goes 0 k goes .
q
games illustrated illustrate poor performance proxies relatively
simple extremely degenerate. real-world settings would expect locations delivery setup along straight line symmetrical box. Hence motivated compare
proxies using data accurately reflects domain hope deploy proxies.

7. Empirical Study
implemented six proxies discussed, along version ApproShapley uses
Concorde (Applegate et al., 2007) evaluate characteristic function TSG. code
data used project available public Git repository at: https://github.com/nmattei/
ShapleyTSG. Rather calculating SV direct enumeration baseline compare proxies,
597

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

estimate value using ApproShapley Concorde. described Section 4.4, method
achieves extremely good approximation true Shapley value computable reasonable time testing games 20 locations.
use corpus 1070 Synthetic games, constructed described Section 4.4, games
n {4, . . . , 20} locations. also test corpus 119 Real-World games generated
large VRPs cities Auckland (New Zealand), Canberra, Sydney (Australia).
Heuristic solutions VRPs calculated using Indigo solver (Kilby & Verden, 2011).
Indigo flexible heuristic solver implementing Adaptive Large Neighbourhood Search,
basic structure described detail Ropke Pisinger (2006).7 give indication scale difficulty VRPs, Auckland model comprises 1, 166 locations
served using fleet 25 vehicles 7 day period. heuristic solutions
collect collect tours length 10 20 create TSGs testing. real-world distance
matrices asymmetric (in cases asymmetry negligible), induce symmetric problems
resolving greater di j ji , i.e., setting di j = ji = max{di j , ji }. total, obtain
71 Real-World games size 10 (14 Auckland, 5 Canberra, 52 Sydney) 48 games
size 20 (10 Auckland, 7 Canberra, 31 Sydney).8
evaluate well proxies perform approximating SV use several different test statistics, briefly review (Corder & Foreman, 2009). Already discussed Section 4.4
root-mean-squared-error (RMSE) game. Additionally, may want know maximum absolute point-wise error (MAPE), i.e., maximum absolute error values
point-wise estimate:
MAPE = arg maxx,x[X,X] |x x|.
use measure particular game compare average maximum absolute pointwise error set games (MAPE). value lets us know, average,
overcharging particular customer (unlike RMSE tells us aggregate error). Note
using arguments Section 4.4 guarantees acuracey MAPE
RMSE, i.e., 3%.
One question often repeated consultation logistics companies
expensive customer? order know focus efforts contract negotiations sales functions, companies desire understanding rank ordering cost servicing locations.
use Kendalls , written KT first introduced Kendall (1938), compare ranking, i.e.,
least expensive expensive, locations induced Shapley allocation proxies.
value measures amount disagreement two rankings. customary report
normalized value (correlation coefficient) 1 -1, = 1 means two lists
perfectly correlated (equal) = 1 means two lists perfectly anti-correlated (they
equal one list reversed). intuitive interpretation two lists %
orderings two lists same.
detail, let X two partial orders set items. b X say X
concordant (a, b). = b X say tie, otherwise (a, b)
7. Indigo strong vehicle routing solution platform, recently computing 5 new best solutions 1, 000 customer problems VRPTW benchmark library. solutions computed using Indigo certified
Dr. Geir Hasle, Chief Research Scientist SINTEF maintainer VRPTW benchmark library,
best currently known September 24th 2013. http://www.sintef.no/Projectweb/TOP/VRPTW/
Homberger-benchmark/1000-customers.
8. Due commercial agreements industrial partners cannot release Real-World games.

598

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

discordant. number concordant pairs, N discordant pairs, ties exclusively X,
U ties exclusively , normalised KT distance X is:
=p

MN
.
(M + N + ) (M + N +U)

analysis makes use significance, p-value, computed . p-value computed using two-tailed t-test null hypothesis correlation
orderings ( = 0). Taking significance threshold customary 0.05, reject null
hypothesis p 0.05. p 0.05 fail reject null hypothesis. Hence, p-value
0.05 statistically significant result. Intuitively means unlikely two random lists would show high degree correlation say two lists significantly
correlated.
7.1 Synthetic Data
Figure 8 gives overview data, showing RMSE proxy SV
game sizes Synthetic data. Tables 2 5 give in-depth look performance
proxies variety interesting measures including RMSE, MAPE, , number statistically
significant s, number games correctly identified top elements. general, HORT
R EROUTE proxies far worst, particularly terms approximating Shapley value,
also terms ranking induced corresponding allocations. computationally
expensive proxy R EROUTE always dominates HORT ; though proxies dominated
EPOT , OAT , B LEND , C HRIS tests save one.
10 Locations
RMSE


15 Locations
RMSE


20 Locations
RMSE


Data
RMSE


Shortcut Distance
Re-routed Margin
Depot Distance

0.3850
0.2565
0.0994

0.0968
0.0699
0.0275

0.3342
0.2168
0.0950

0.0764
0.0533
0.0235

0.2992
0.1915
0.0893

0.0606
0.0424
0.0195

0.3727
0.2493
0.0978

0.0564
0.0488
0.0059

Moat-Packing
Christofides
Blend

0.1617
0.0495
0.0710

0.0502
0.0216
0.0191

0.1437
0.0526
0.0742

0.037
0.0177
0.0168

0.1302
0.0523
0.0733

0.0293
0.0142
0.0154

0.1564
0.0520
0.0745

0.0197
0.0046
0.0075

Table 2: Average root mean squared error (RMSE) standard deviation ( ) Synthetic data
games 10, 15, 20 locations. Lower better.

599

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

Figure 8: Performance proxies according to: (left) RMSE 1070 games generated
number locations, (right) 1070 games generated number
locations. error bands correspond plus minus one standard deviation.
vertical axis plot inverted ease comparison, i.e., correlated
lists towards bottom graph (1.0).

10 Locations
MAPE


15 Locations
MAPE


20 Locations
MAPE


Data
MAPE


Shortcut Distance
Re-routed Margin
Depot Distance

0.2802
0.1866
0.0637

0.1088
0.0805
0.0238

0.2278
0.1460
0.0589

0.0843
0.0596
0.0226

0.1944
0.1203
0.0523

0.0700
0.0461
0.0193

0.2605
0.1741
0.0620

0.1155
0.089
0.0261

Moat-Packing
Christofides
Blend

0.1078
0.0311
0.0441

0.0452
0.0147
0.0154

0.0888
0.0318
0.0443

0.035
0.0137
0.0155

0.0722
0.0299
0.0417

0.0252
0.0108
0.0145

0.1003
0.0329
0.0472

0.0508
0.0158
0.0224

Table 3: Average maximum absolute error (MAPE) standard deviation ( ) Synthetic
data games 10, 15, 20 locations. Lower better.

Looking first error estimation SV , top Figure 8 depicts RMSE
1070 games proxy increase number locations per game. overall
trend positive proxy becoming accurate (lower RMSE) increase number
locations. figure C HRIS strictly dominates proxies RMSE performance.
However, also see B LEND EPOT competitive C HRIS terms RMSE
. B LEND winner category practical purposes offers performance extremely
close C HRIS , tighter distribution error EPOT , fraction computation

600

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

time. Table 2 shows detailed breakdown Figure 8 particular numbers locations.
table allows us see RMSE B LEND never goes 0.1 0.01 larger
instances commercially interesting.
Table 3 sheds light types error proxies likely make.
see B LEND achieves better MAPE proxies save C HRIS , average
overcharging worst case 4.7% = 2.2% true Shapley value, mere 1%
C HRIS . see EPOT fairly accurate proxy SV , overcharging
6% = 2.6% largest instanes tested. However, EPOT strictly dominated
B LEND error measures considered computed similar time. Given B LEND
computable fraction time C HRIS , competitive overall error, scales
beyond commercially interesting problem sizes. clear winner measure.
10 Locations



15 Locations



20 Locations



Data




Shortcut Distance
Re-routed Margin
Depot Distance

0.0098
0.4732
0.5815

0.2403
0.1947
0.1791

0.0031
0.4160
0.5400

0.1931
0.1505
0.1524

-0.0076
0.3908
0.5018

0.1604
0.1397
0.1454

-0.0027
0.4828
0.5659

0.0106
0.0892
0.0385

Moat-Packing
Christofides
Blend

0.4098
0.7186
0.6834

0.2235
0.1663
0.1567

0.3526
0.6791
0.6206

0.1787
0.1430
0.1385

0.3392
0.6463
0.5706

0.1610
0.1286
0.1369

0.4190
0.7048
0.6616

0.0829
0.0374
0.0593

Table 4: Average Kendalls tau rank correlation coefficient () Standard Deviation ( )
Synthetic data games 10, 15, 20 locations. Higher better; +1 means two
lists perfectly correlated 1 means two lists perfectly anti-correlated.

10 Locations
% Sig.
% Top

15 Locations
% Sig.
% Top

20 Locations
% Sig.
% Top

Data
% Sig.
% Top

Shortcut Distance
Re-routed Margin
Depot Distance

1.49%
18.13%
16.44%

19.81%
77.75%
68.59%

3.73%
49.90%
70.18%

9.62%
73.08%
51.40%

5.32%
60.74%
84.85%

6.91%
67.00%
46.26%

4.72%
53.15%
69.67%

10.45%
69.85%
52.04%

Moat-Packing
Christofides
Blend

12.42%
29.90%
29.90%

71.68%
82.52%
80.84%

39.53%
90.18%
89.15%

62.42%
78.31%
68.41%

45.60%
96.54%
94.11%

56.82%
74.39%
59.71%

41.76%
85.28%
83.12%

61.13%
76.08%
65.38%

Table 5: (Left columns) percentage games 1070 statistically significant
(p < 0.05) ranking induced SV ranking induced P ROXY . (Right
columns) percentage 1070 games expensive element according
raking induced SV matched expensive element ranking induced
P ROXY . Higher better statistics.

601

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

Turning proxies performance ranking, bottom Figure 8 depicts average
Kendalls tau rank correlation coefficient () standard deviation ( ) 1070 games
proxy increase number locations per game. overall trend graph, opposed
top one, slightly negative. increase number locations, ranking computed
proxy increasingly uncorrelated ranking induced SV . positive side,
C HRIS , B LEND , EPOT , return lists 0.6 correlation, i.e., 60% pairs
elements ordered correctly. Table 4 gives closer look results particular numbers
locations. see C HRIS B LEND maintain near 0.6 across range problems,
hence correctly order pairs elements. two proxies strictly dominate
proxies; even EPOT performs poorly measured .
Table 5 gives us nuanced look ranking results. see larger games
percentages statistically significant increase proxies locations, even
decrease. lists recovering significant portion
pairwise relations compared total number pairwise relations. see terms
statistically significant s, C HRIS B LEND strictly dominate proxies almost 15%
data considered. answer common customer question whose costing
most, results bit mixed. Comparing highest ranked elements see
performance B LEND drops performance R EROUTE , surpassingly strong
proxy measure. However, want top element according SV top 3
elements according P ROXY , B LEND achieves feat 90% time. Though
proxies see increase performance relaxed measure well, C HRIS B LEND
90% numbers locations studied. Hence, see B LEND provides strong
performance practically computable running time across range game sizes.
7.2 Real-World Data
Measuring performance proxies Real-World corpus Auckland, Canberra,
Sydney, find overall quality allocation slightly lower compared measurements
Synthetic corpus. identified significant performance differences cities,
therefore report data aggregate statistics Real-World corpus 71 games
10 locations 48 games 20 locations. Tables 6 9 provide in-depth perspective
performance proxies Real-World dataset measures Synthetic
dataset. see performance HORT R EROUTE strictly dominated according
statistical measures proxies; except R EROUTE ability select costly
location surprisingly high accuracy.

602

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

10 Locations
RMSE


20 Locations
RMSE


Games
RMSE


Shortcut Distance
Re-routed Margin
Depot Distance

0.4511
0.4380
0.1380

0.1477
0.1472
0.065

0.3245
0.3030
0.0838

0.0929
0.0934
0.0313

0.3878
0.3705
0.1109

0.0633
0.0675
0.0271

Moat-Packing
Christofides
Blend

0.2692
0.1519
0.1442

0.1486
0.0823
0.0687

0.2088
0.1104
0.0826

0.0937
0.0529
0.0292

0.2390
0.1311
0.1134

0.0302
0.0207
0.0308

Table 6: Average root mean squared error (RMSE) standard deviation ( ) Real-world
data 71 games 10 locations 48 games 20 locations. Lower better.

10 Locations
MAPE


20 Locations
MAPE


Games
MAPE


Shortcut Distance
Re-routed Margin
Depot Distance

0.3678
0.3568
0.0835

0.1716
0.1695
0.0388

0.2462
0.2286
0.0390

0.1050
0.1012
0.0118

0.3187
0.3051
0.0655

0.1599
0.1588
0.0378

Moat-Packing
Christofides
Blend

0.2196
0.1178
0.0961

0.1463
0.0780
0.0523

0.1498
0.0739
0.0412

0.0959
0.0536
0.0134

0.1914
0.1001
0.0740

0.1328
0.0725
0.0493

Table 7: Average maximum absolute error (MAPE) standard deviation ( ) Real-world
data 71 games 10 locations 48 games 20 locations. Lower better.

Turning first error estimation SV , see results reported Table 6
strictly higher every measure every proxy compared results Table 2, corresponding test Synthetic dataset. see error decreases increase number
locations proxies. difference Real-World Synthetic render
proxies unuseable. Observe RMSE B LEND increases 0.01 Synthetic
Real-world RMSE EPOT increases 0.003. solid indicator
usefulness B LEND , none Real-World instances included training set
model. interesting twist, computationally expensive C HRIS fares worse
Real-World data, doubling error (an increase 0.05) respect Synthetic dataset.
increase RMSE followed looking MAPE. Comparing Table 7 Synthetic dataset partner Table 3, see B LEND EPOT actually lower MAPE
lower Real-World datasets 20 locations. Observe comparing performance
according MAPE see B LEND EPOT separated 1% performance,
strictly outperform metrics, even C HRIS . see B LEND EPOT
reasonable proxies SV Real-world corpus, achieving overall RMSE less 0.09
absolute worst error per location less 0.05 (5% true cost).
603

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

possible explanation extremely good performance EPOT requires closer look
distribution costs Real-World dataset. locations along route heuristically
allocated larger VRP, allocations tend cluster around uniform allocation around
0.050.08 per location, many locations equidistant depot. Consequently, RealWorld data seems drawn different distribution Synthetic data (i.e., locations
selected uniformly random). Thus, performance B LEND ideal, uniformly
random case strongly degenerate real-world case strong argument portability
B LEND across domains.
10 Locations



20 Locations



Games



Shortcut Distance
Re-routed Margin
Depot Distance

0.0756
0.3651
0.1055

0.3015
0.2793
0.3416

0.0061
0.4734
0.3322

0.2148
0.1602
0.1932

0.0408
0.4193
0.2188

0.0348
0.0542
0.1134

Moat-Packing
Christofides
Blend

0.3480
0.2457
0.1498

0.2504
0.3408
0.3287

0.3814
0.5403
0.4093

0.1721
0.1589
0.1809

0.3647
0.3930
0.2796

0.0167
0.1473
0.1297

Table 8: Average Kendalls rank correlation coefficient () Standard Deviation ( )
Real-World data 71 games 10 locations 48 games 20 locations.
Higher better; +1 means two lists perfectly correlated 1 means two
lists perfectly anti-correlated.

10 Locations
% Sig.
% Top

20 Locations
% Sig.
% Top

Data
% Sig.
% Top

Shortcut Distance
Re-routed Margin
Depot Distance

4.22%
28.16%
12.67%

12.67%
57.74%
53.52%

8.33%
72.91%
43.75%

18.75%
70.83%
60.41%

5.88%
46.21%
25.21%

15.12%
63.02%
56.30%

Moat-Packing
Christofides
Blend

25.35%
22.53%
14.08%

60.56%
57.74%
56.33%

62.50%
89.58%
68.75%

56.25%
62.50%
64.58%

40.33%
49.57%
36.13%

58.82%
59.66%
59.66%

Table 9: (Left columns) percentage Real-World data 71 games 10 locations
48 games 20 locations statistically significant (p < 0.05)
ranking induced SV ranking induced P ROXY . (Right columns)
percentage Real-World data 71 games 10 locations 48 games
20 locations expensive element according raking induced SV
matched expensive element ranking induced P ROXY . Higher better
statistics.

604

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

Tables 8 9 give indication proxies perform terms ranking. closer look
Table 8 reveals difference B LEND EPOT . Again, comparing results
Synthetic dataset shows proxies perform strictly worse Real-world data, except
HORT manages go negative list correlation Synthetic dataset (barely)
positive correlation Real-world dataset. Judging performance see
proxies still recovering 50% pairwise comparisons Real-World data. Again,
also see good performance R EROUTE ranking metric. Additionally, games
20 locations, R EROUTE , C HRIS , OAT , B LEND , C HRIS best.
review Table 9 reveals measure lower overall, majority
ranking correlations still statistically significant 20 location games. first glance proxies appears hold looking top element. Every proxy sees decreased performance 60% accuracy selecting top element, R EROUTE best performance,
followed B LEND C HRIS . Relaxing notion top (most costly) element
Synthetic data, i.e., top element according SV top 3 elements according
P ROXY , B LEND outperforms proxies (including R EROUTE ) 20 location games
93% accuracy, comes within 3% outperforming R EROUTE entire corpus Real-world
data 79% accuracy.
summary see proxies perform worse terms RMSE RealWorld dataset Synthetic dataset. testing see B LEND , EPOT ,
C HRIS perform proxies majority measures. comparing proxies
variety decision criteria including practical running time, overall numerical error,
ranking performance, B LEND emerges clear winner overall consistent performer
Synthetic Real-world data.

8. Related Work
theory cooperative games rich history various solution concepts allocating
costs quantities proposed (Peleg & Sudholter, 2007; Young, 1994). addition
Shapley value, solution concepts include core, nucleolus bargaining set.
these, Shapley value considered important allocation scheme cooperative
game theory (Winter, 2002).
Application Shapley value spans well beyond transportation setting. example,
Shapley value applied allocating cost network infrastructure (Koster, 2009; Marinakis, Migdalas, & Pardalos, 2008), promoting collaboration agents (Zlotkin & Rosenschein, 1994) prescribing allocation incentivises agents collaborate completion
tasks, incentive compatible way share departmental costs corporations (Young,
1985). Considering applications networks broadly, use Shapley value follows general framework, agents correspond nodes (or edges) graph (Curiel, 2008; Koster,
2009; Marinakis et al., 2008; Tijs & Driessen, 1986; Aziz & de Keijzer, 2014). definition
characteristic function depends application domain, proposed evaluations based
on: (i) size maximum matching, (ii) network flow, (iii) weight minimum spanning
tree, (iv) weight Hamiltonian cycle (Curiel, 2008; Deng & Fang, 2008). Allocation
concepts solely devised employed allocating costs. example, Shapley value
used measure importance agents social networks (Moretti & Patrone, 2008),

605

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

measure centrality nodes networks (Michalak, Aadithya, Szczepanski, Ravindran,
& Jennings, 2013).
Another solution concept used gauge importance agents Banzhaf
value (Banzhaf III, 1964). Banzhaf value defined simple voting games i.e. cooperative
games value coalition either zero one Banzhaf value agent
suitably extended general cooperative games. However, even within context simple
voting games, Banzhaf value suitable measuring influence agent less
suitable allocate power agents (Felsenthal & Machover, 1998). Since focus
allocate costs, focus Shapley value.
solution concepts theory transferable utility (TU) cooperative games (Peleg
& Sudholter, 2007; Chalkiadakis et al., 2011) used allocations costs, Shapley
value rarely received serious attention transportation science literature. associated
computational cost prohibitively high general case, consequently strong notions
fairness often taken secondary consideration. Though ApproShapley FPRAS (fully
polynomial-time randomized approximation scheme) computing Shapley value game
convex (Liben-Nowell, Sharp, Wexler, & Woods, 2012), apply domain considered work. website Spliddit uses Shapley value split cab fares 6
people (Goldman & Procaccia, 2014).
prominent TU game solution concepts nucleolus core. TSGs introduced
Potters (1992), addition describing game, authors describe variety game
known routing game.9 latter auxiliary constraint forces locations visited,
coalition, order traversed specific tour. Assuming tour corresponds
optimal underlying TSP, game non-empty core. Derks Kuipers (1997)
presented quadratic-time procedure computing core allocation routing game.
also characterize suboptimal tours specify routing games non-empty cores.
noted known tractable procedures compute tour guarantees core
non-empty routing game. Conditions non-emptiness core TSGs
developed Tamir (1989). already noted Faigle et al. (1998) developed procedure
calculate multiplicative -core allocation Euclidean TSGs. Yengin (2012) develop notion
fixed route game appointments admits tractable procedure computing Shapley
values. model suitable typical scenarios involve delivery goods locations
depot. TU concepts TSGs routing games developed practical gas delivery
application Engevall et al. (1998).
Turning attention vehicle routing problems transportation settings generally,
Gothe-Lundgren, Jornsten, Varbrand (1996) develop column generation procedure calculate nucleolus homogeneous vehicle routing problem, i.e., vehicles equivalent.
develop procedure determine core vehicle routing game
empty. Engevall et al. (2004) extend work practical setting distributing gas using heterogeneous fleet vehicles. recently Ozener et al. (2013) examine number
solution conceptsincluding allocations derived according nested moat-packing Faigle
et al. (1998), highly specialized approximation Shapley allocationin deriving cost
allocations real-world inventory routing problems. show TU game allocations, espe9. Note journal publication Potters et al. (1992) extends technical report introducing game early
1987.

606

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

cially core/duality-based allocations, significant advantages existing cost allocations
industrial client using.

9. Conclusions Future Work
studied problem fairly apportioning costs transportation scenarios, specifically TSGs.
Shapley value appealing division concept task axiomatic fairness properties
ones appreciated commercial partners. Since Shapley value cannot evaluated
reasonable time, considered number proxies Shapley value. examined proxy
performance terms approximation quality respect Shapley value
induced ranking locations Shapley value, key question operational business concerns.
stand-out proxies respect measures tested Synthetic Real-world data
C HRIS B LEND , mixture EPOT OAT . However, taking computation time
account ability scale problems commercial interest: around 30 locations per route
600 total routes delivery day, B LEND remains feasible.
key extensions work general setting vehicle routing games (VRPs).
Shapley value would useful quantify importance location synergies unique
multi-vehicle model. transport companies interact desire understand impact
time windows (both duration position allowable service times), effect delivery frequency allocated costs. Thus, highly motivated rich variety problems available
future work. Additionally, future research consider weighted Shapley values situations coalitions (and therefore margins) likely occur others. Formal
approximation ratios, complement strong empirical evidence obtained, important
subject future research. also remains need formal studies employ proxy
allocations inform solutions hard optimisation problems transportation domains. Finally,
scaling larger transportation scenarios may require abstracting locations meaningful way.
approximation approach may fruitful proposed Soufiani, Charles, Chickering, Parkes (2014), agents partitioned groups assigned weights within
groups novel effective way.

Acknowledgments
Data61/CSIRO (formerly known NICTA) funded Australian Government
Department Communications Australian Research Council ICT Centre
Excellence Program. Casey Cahan supported Summer Research Scholarship Australian National University. Toby Walsh also receives support Asian Office Aerospace
Research Development (AOARD 124056) German Federal Ministry Education
Research Alexander von Humboldt Foundation.
would like thank Stefano Moretti Patrice Perny LIP6; Hossein Azari Soufiani
Harvard University; David Rey Vinayak Dixit rCiti Project University
New South Wales School Civil Environmental Engineering, Tommaso Urli Data61
ANU; reviewers attendees 5th Workshop Cooperative Games MultiAgent
Systems (CoopMAS-2014) helpful feedback comments early version work.

607

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

References
Applegate, D. L., Bixby, R. E., Chvatal, V., & Cook, W. J. (2007). traveling salesman problem:
computational study. Princeton University Press.
Aziz, H., & de Keijzer, B. (2014). Shapley meets Shapley. Proceeding 31st International
Symposium Theoretical Aspects Computer Science (STACS 2014), pp. 99111.
Bachrach, Y., Markakis, E., Resnick, E., Procaccia, A. D., Rosenschein, J. S., & Saberi, A. (2010).
Approximating power indices: theoretical empirical analysis. Autonomous Agents
Multi-Agent Systems, 20(2), 105122.
Banzhaf III, J. F. (1964). Weighted voting doesnt work: mathematical analysis. Rutgers Law
Review, 19, 317343.
Bellman, R. (1962). Dynamic programming treatment travelling salesman problem. Journal
ACM (JACM), 9(1), 6163.
Bishop, C. M. (2006). Pattern recognition machine learning. Springer.
Castro, J., Gomez, D., & Tejada, J. (2009). Polynomial calculation shapley value based
sampling. Comput. Oper. Res., 36(5), 17261730.
Chalkiadakis, G., Elkind, E., & Wooldridge, M. (2011). Computational aspects cooperative game
theory. Synthesis Lectures Artificial Intelligence Machine Learning, 5(6), 1168.
Christofides, N. (1976). Worst-case analysis new heuristic travelling salesman problem..
Tech. rep., DTIC Document.
Conitzer, V., & Sandholm, T. (2006). Complexity constructing solutions core based
synergies among coalitions. Artificial Intelligence, 170(6), 607619.
Cook, W. J., Cunningham, W. H., Pulleylank, W. R., & Schrijver, A. (1998). Combinatorial Optimization. John Wiley & Sons, Inc.
Corder, G. W., & Foreman, D. I. (2009). Nonparametric statistics non-statisticians: step-bystep approach. Wiley.
Cornuejols, G., Naddef, D., & Pulleyblank, W. (1985). traveling salesman problem graphs
3-edge cutsets. Journal ACM, 32(2), 383410.
Curiel, I. (2008). Cooperative combinatorial games. Chinchuluun, A., Pardalos, P., Migdalas, A.,
& Pitsoulis, L. (Eds.), Pareto Optimality, Game Theory Equilibria, Vol. 17 Springer
Optimization Applications, pp. 131157. Springer New York.
Deng, X., & Fang, Z. (2008). Algorithmic cooperative game theory. Chinchuluun, A., Pardalos,
P. M., Migdalas, A., & Pitsoulis, L. (Eds.), Pareto Optimality, Game Theory Equilibria,
Vol. 17 Springer Optimization Applications. Springer-Verlag.
Derks, J., & Kuipers, J. (1997). core routing games. International Journal Game
Theory, 26(2), 193205.
Engevall, S., Gothe-Lundgren, M., & Varbrand, P. (1998). traveling salesman game: application cost allocation gas oil company. Annals Operations Research, 82,
203218.

608

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

Engevall, S., Gothe-Lundgren, M., & Varbrand, P. (2004). heterogeneous vehicle-routing game.
Transportation Science, 38(1), 7185.
Faigle, U., & Kern, W. (1993). approximately balanced combinatorial cooperative games.
ZOR Methods Models Operations Research, 38(2), 141152.
Faigle, U., Fekete, S., Hochstattler, W., & Kern, W. (1998). approximately fair cost allocation
euclidean tsp games. Operations-Research-Spektrum, 20(1), 2937.
Fatima, S. S., Wooldridge, M., & Jennings, N. R. (2007). randomized method Shapley
value voting game. Proceedings 6th International Conference Autonomous
Agents Multiagent Systems (AAMAS 07), pp. 157165, New York, New York, USA.
Fatima, S. S., Wooldridge, M., & Jennings, N. R. (2008). linear approximation method
Shapley value. Artificial Intelligence, 172(14), 16731699.
Felsenthal, D. S., & Machover, M. (1998). Measurement Voting Power: Theory Practice,
Problems Paradoxes. Edward Elgar Cheltenham.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide Theory
NP-Completeness. New York: W.H. Freeman.
Golden, B. L., Raghavan, S., & Wasil, E. A. (2008). Vehicle Routing Problem: Latest Advances
New Challenges: latest advances new challenges, Vol. 43. Springer.
Goldman, J., & Procaccia, A. D. (2014). Spliddit: Unleashing fair division algorithms. Journal
ACM, 13(2), 4146.
Gothe-Lundgren, M., Jornsten, K., & Varbrand, P. (1996). nucleolus basic vehicle
routing game. Mathematical Programming, 72(1), 83100.
Held, M., & Karp, R. M. (1962). dynamic programming approach sequencing problems.
Journal Society Industrial & Applied Mathematics, 10(1), 196210.
Ieong, S., & Shoham, Y. (2005). Marginal contribution nets: compact representation scheme
coalitional games. Proceedings 6th ACM conference Electronic Commerce (EC
06), pp. 193202.
Kendall, M. G. (1938). new measure rank correlation. Biometrika, 30(1/2), 8193.
Kilby, P., & Verden, A. (2011). Flexible routing combing constraint programming, large neighbourhood search, feature-based insertion. 2nd Workshop Artificial Intelligence
Logistics. Barcelona, Spain.
Kimms, A., & Kozeletskyi, I. (2015). Shapley value-based cost allocation cooperative traveling salesman problem rolling horizon planning. EURO Journal Transportation
Logistics, 122.
Koster, M. (2009). Cost Sharing. Springer-Verlag New York.
Leech, D. (2003). Computing power indices large voting games. Management Science, 49(6),
831837.
Liben-Nowell, D., Sharp, A., Wexler, T., & Woods, K. (2012). Computing shapley value supermodular coalitional games. 18th International Conference Computing Combinatorics (COCOON 2012), pp. 568579.

609

fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH

Maleki, S., Tran-Thanh, L., Hines, G., Rahwan, T., & Rogers, A. (2013). Bounding estimation error sampling-based shapley value approximation with/without stratifying. CoRR,
abs/1306.4265.
Mann, I., & Shapley, L. S. (1960). Values large games IV: Evaluating electoral college
monte carlo. Technical report, RAND Corporation, Santa Monica, CA, USA.
Mann, I., & Shapley, L. S. (1962). Values large games IV: Evaluating electoral college
exactly. Technical report, RAND Corporation, Santa Monica, CA, USA.
Marinakis, Y., Migdalas, A., & Pardalos, P. M. (2008). Cost allocation combinatorial optimization
games. Chinchuluun, A., Pardalos, P., Migdalas, A., & Pitsoulis, L. (Eds.), Pareto Optimality, Game Theory Equilibria, Vol. 17 Springer Optimization Applications,
pp. 217244. Springer New York.
Michalak, T. P., Aadithya, K. V., Szczepanski, P. L., Ravindran, B., & Jennings, N. R. (2013).
Efficient computation Shapley value game-theoretic network centrality. Journal
Artificial Intelligence Research, 46, 607650.
Moretti, S., & Patrone, F. (2008). Transversality Shapley value. TOP, 16(1), 141.
Natrella, M., Croarkin, C., & Guthrie, W. (2012). NIST/SEMATECH e-Handbook Statistical
Methods. U.S. Department Commerce. URL: http://www.itl.nist.gov/div898/handbook/.
Owen, G. (1972). Multilinear extensions games. Management Science, 18(5-part-2), 6479.
Ozener, O. O., Ergun, O., & Savelsbergh, M. (2013). Allocating cost service customers
inventory routing. Oper. Res., 61(1), 112125.
Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley Publishing Company, Inc.
Papapetrou, P., Gionis, A., & Mannila, H. (2011). Shapley value approach influence attribution. Proceedings 2011 European Conference Machine Learning Principles
Knowledge Discovery Databases (ECML PKDD 2011), pp. 549564. Springer.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M.,
Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning Python. Journal
Machine Learning Research, 12, 28252830.
Peleg, B., & Sudholter, P. (2007). Introduction Theory Cooperative Games. Springer.
Potters, J. A., Curiel, I. J., & Tijs, S. H. (1992). Traveling salesman games. Mathematical Programming, 53(1-3), 199211.
Ropke, S., & Pisinger, D. (2006). adaptive large neighborhood search heuristic pickup
delivery problem time windows. Transportation Science, 40(4), 455472.
Sahni, S., & Gonzalez, T. (1976). P-complete approximation problems. Journal ACM, 23(3),
555565.
Shapley, L. S. (1953). value n-person games. Kuhn, H., & Tucker, W. W. (Eds.), Contributions Theory Games, Vol. 2 Annals Mathematical Studies. Princeton University
Press.

610

fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS

Soufiani, H. A., Charles, D. J., Chickering, D. M., & Parkes, D. C. (2014). Approximating shapley value via multi-issue decomposition. Proceedings 13th International Conference
Autonomous Agents Multiagent Systems (AAMAS 14), pp. 12091216.
Tamir, A. (1989). core traveling salesman cost allocation game. Operations Research
Letters, 8(1), 3134.
Tijs, S. H., & Driessen, T. S. H. (1986). Game theory cost allocation problems. Management
Science, 32(8), 10151028.
Winter, E. (2002). Shapley value. Handbook Game Theory Economic Applications,
chap. 53, pp. 20252054. Elsevier.
Yengin, D. (2012). Appointment games fixed-route traveling salesman problems Shapley
value. International Journal Game Theory, 41(2), 271299.
Young, H. P. (1985). Producer incentives cost allocation. Econometrica, 53(4), 757765.
Young, H. P. (1994). Cost allocation. Handbook Game Theory Economic Applications,
Vol. 2, pp. 11931235. Elsevier B.V.
Young, H. P. (1985). Monotonic solutions cooperative games. International Journal Game
Theory, 14(2), 6572.
Zlotkin, G., & Rosenschein, J. S. (1994). Coalition, cryptography, stability: Mechanisms
coalition formation task oriented domains. Proceedings 12th National Conference
Artificial Intelligence (AAAI 1994), pp. 432437.

611

fiJournal Artificial Intelligence Research 56 (2016) 657-691

Submitted 01/16; published 08/16

Engineering Note
IBaCoP Planning System: Instance-Based Configured Portfolios
Isabel Cenamor
Tomas de la Rosa
Fernando Fernandez

ICENAMOR @ INF. UC 3 . ES
TROSA @ INF. UC 3 . ES
FFERNAND @ INF. UC 3 . ES

Departamento de Informatica, Universidad Carlos III de Madrid
Avda. de la Universidad, 30. Leganes (Madrid). Spain

Abstract
Sequential planning portfolios powerful exploiting complementary strength
different automated planners. main challenge portfolio planner define
base planners run, assign running time planner decide order
carried optimize planning metric. Portfolio configurations usually derived
empirically training benchmarks remain fixed evaluation phase. work,
create per-instance configurable portfolio, able adapt every planning task.
proposed system pre-selects group candidate planners using Pareto-dominance filtering
approach decides planners include time assigned according predictive
models. models estimate whether base planner able solve given problem and,
so, long take. define different portfolio strategies combine knowledge
generated models. experimental evaluation shows resulting portfolios provide
improvement compared non-informed strategies. One proposed portfolios
winner Sequential Satisficing Track International Planning Competition held
2014.

1. Introduction
Planning process chooses organizes actions anticipating outcomes
aim achieving pre-stated objectives. Artificial Intelligence, Automated Planning (AP)
computational study deliberation process (Ghallab, Nau, & Traverso, 2004). Automated
planners systems that, regardless application domain, able receive declarative
representation environment, initial state set goals input. output synthesized plan achieve goals initial situation. context, International
Planning Competition (IPC) excellent initiative foster studying development automated planning systems. IPC created 1998 set common framework comparing
automated planners.
Different planning systems awards previous IPCs. However, one main invariants
competition single planner always best planner (or least equal)
every domain every problem. means that, although planner which, following
quality metrics competition, considered best, always find problems
different domains planners outperform overall winner. Therefore, assume
AP community generated set single planners better others specific
situations. reason, discarding priori solvers seems meaningless.
c
2016
AI Access Foundation. rights reserved.

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

fact, idea reusing set individual base systems generate accurate solutions obtained separately new Artificial Intelligence. instance, Machine
Learning, meta-classifiers use different base classifier increase coverage representation
bias resulting classifier (Dietterich, 2000). problem solving, portfolios search algorithms
also demonstrated outperform results single search strategy (Xu, Hutter,
Hoos, & Leyton-Brown, 2008; Xu, Hoos, & Leyton-Brown, 2010; Malitsky, Sabharwal, Samulowitz, & Sellmann, 2013). example, SAT competition 2013 included special track
portfolios. automated planning community, planner portfolios also subject great
deal interest. IPCs 2006 2014, portfolio approaches close winning
tracks took part.
However, although use portfolios become usual community, still
agreement planning portfolio (Vallati, Chrpa, & Kitchin, 2015). work,
assume portfolio planners set base planners selection strategy. selection
strategy generates specific portfolio configuration, whose goal maximize performance metrics. Therefore, configuration define three main elements: (1) sub-set
planners run, (2) long run planner? (3) order. many
techniques configure planning portfolio (Vallati, 2012), depending accurate
are, chances selecting best planner given situation increase. Note that,
definition, planner different configuration parameters modify behavior, parameterization considered different base planner, base planners considered black
boxes.
number planners state art huge, first filtering select minimum
number ensures best performance achieved, evaluated planning domain (or even
problem domain). Obviously, good results current domains ensure good
results new domains but, shown, good estimator. sense, Pareto efficiencybased approach (Censor, 1977) reduce number planners consider eligible
planning portfolio presented. However, show mechanism, first
aforementioned questions answered partially since number candidate planners
might still large.
best solution portfolio configuration problem oracle predicts,
given domain problem, planner obtain best performance long
take. Given oracle, work propose use predictive models, automatically generated Machine Learning Data Mining techniques. models summarize results candidate planners past: whether able solve planning
problems, well time required generate good solution (Cenamor, de la Rosa,
& Fernandez, 2012, 2013). Given knowledge past, inductive hypothesis gives also us
estimation behave future planning domains different problems,
order planners implemented given accuracy predictions.
Therefore, predictive models, able configure portfolio planning problem, like previous works use portfolios search (Gomes & Selman, 2001).
renewed idea automated planning since recent works focused static (Helmert, 2006)
domain-specific portfolios (Gerevini, Saetti, & Vallati, 2009, 2014), configuration
portfolio fixed domains chosen one respectively.
IBAC P (Instance-based Configured Portfolio) family planning portfolios built
competing IPC-2014. article first present IBAC P general framework
658

fiT IBAC P P LANNING YSTEM

ultimate goal building per-instance configurable portfolios. technique reproduced
whenever new automated planners new planning benchmarks arise. Then, describe
build different version IBAC P following defined processes. One versions
winner Sequential Satisficing Track IPC-2014. also include results empirical
study confirms good performance IBAC P planners compared different base
planners different portfolio configuration strategies. Then, summarize related work,
finally, last section sets conclusions future lines research.

2. System Architecture
section, present general idea building planning portfolio configured
particular planning task using predictive models. process seen general
technique given inputs (planners benchmarks) might change future due progress
planning community, new portfolio configurations generated use
new inputs.
2.1 Portfolio Construction
consider construction instance-based planning portfolio comprises three main
parts. (1) Planner filtering, making pre-selection good candidate planners set
known available planners. proposed pre-selection technique based multi-criteria approximation. previously unexplored technique selecting set planners provides
enough diversity planner portfolio. (2) Performance modeling, providing predictors
planners behavior function planning task features. research, include set
well-known features (Cenamor et al., 2012), built preprocessing step
FAST OWNWARD (Helmert, 2006). also take advantage output information
translation process (Fawcett, Vallati, Hutter, Hoffmann, Hoos, & Leyton-Brown, 2014)
heuristic values computed first step search process FAST OWNWARD. addition,
use several totally new features characteristics relaxed plan initial state
proposed. Finally, (3) strategy selection: establish procedure combines performance
predictions output portfolio configuration. propose novel strategy selection
exploit effectiveness predictive models. Next, explain details
construction steps.
2.1.1 P LANNER F ILTERING
planner filtering process consists pre-selection good candidate base planners
larger amount available planners. Even though sufficient evidence
overall best planner across variety benchmarks, verified empirically
dominance planners others. Therefore make sense include, base
planners, always worse terms performance metrics. want filtering process
select diverse, small, subset planners elements among divide
available execution time.
work, propose multi-criteria pre-selection mechanism focuses two IPC metrics (quality time) alternative extended ones planner filtering. example,
FDSS (Helmert, Roger, Seipp, Karpas, Hoffmann, Keyder, Nissim, Richter, & Westphal, 2011)
659

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

uses selection planners maximizes coverage; MIP LAN (Nunez, Borrajo, & Linares
Lopez, 2015) uses portfolio configuration obtains best achievable performance terms
score.
filtering propose run candidate planners representative set benchmarks
evaluate terms time quality. consider metrics propose
approach based Pareto-efficiency (Censor, 1977) allows us determine dominance
planners multi-criteria fashion. particular, select planner candidate
portfolio best planner least one domain terms IPC-2011 multi-criteria QT
score (Linares Lopez, Celorrio, & Olaya, 2015). Briefly, single problem, metric computes
tuple hQ, planner, Q quality planners best solution
time used find solution. Then, given planner, p, dominance relations p
rest planners computed.
tuple hQ, Pareto-dominates tuple hQ , Q Q < . Planner
p gets NN points, N number tuples p Pareto-dominates another planner,
N number different tuples planner p appears. Finally, QT-Pareto score
domain sum points achieved problems domain. idea selection
mechanism follows: planner shows good dominance property given domain,
included portfolio good candidate solving problems
domain even planning tasks similar characteristics. Therefore, simple strategy
filter first pool planners given procedure selects planners
maximum QT-Pareto score least one domain. refer procedure QT-Pareto Score
Filtering.
2.1.2 P ERFORMANCE ODELING
Given planning task, want predict selected base planners perform order
decide whether include make good assignment time ordering
configuring portfolio. Thus, modeling planner behavior function planning
task features becomes key process building instance-based portfolios. learn predictive
models follow Data Mining approach, shown Figure 1. case, start set
candidate planners set planning benchmarks. output process set models
predict performance candidate planners. defined data mining goal
creation two predictive models. First, whether planner able solve problem
(i.e. classification task) and, so, time required compute best plan (i.e.,
regression task).
first step mining process comprises generation training test datasets.
one hand, planners run set benchmarks obtain performance data.
data includes outcome execution (success failure) and, positive cases, time
elapsed finding best solution. hand, planning tasks processed extract set
features characterize them. features extended set previously proposed
set (Cenamor et al., 2013). According mechanism generating features, classify
following categories:
PDDL features: Basic features extracted PDDL representation domain
problem files, instance, number actions, objects goals.
660

fiT IBAC P P LANNING YSTEM

Figure 1: General Diagram Learning Planning Performance Predictive Models
FD Instantiation features: Fast-Downward pre-processor instantiates translates
planning tasks finite domain representation (Helmert, 2009). output take
general information number instantiated actions number relevant
facts, data specific FD-translator, number auxiliary atoms.
SAS+ features: finite domain representation SAS+ associated Causal Graph
(CG) set Domain Transition Graphs (DTGs). CG extract basic properties
(e.g., number variables edges), ratios properties. regards
DTGs, number graphs problem corresponds number edges CG,
makes difficult encode general attributes DTG. Therefore, summarize DTGs characteristics aggregating relevant properties graphs. Thus,
features DTGs statistics maximum, average standard
deviation graph properties.
Heuristic features: initial state, compute heuristic values using set widely-used
unit cost heuristic functions (e.g., hmax , hFF ,. . . ). compute heuristics
initial state, obtained reasonable cost. use unit cost heuristics
obtain domain-independent estimation helps characterization problem size
and/or difficulty.
Fact Balance Features: Using relaxed plan (RP ) initial state, extracted computing hFF heuristic, also compute set features represent fact balance
RP . define fact balance fact p, number times p appears added
effect action belonging RP , minus number times p deleted effect
action RP , considering original actions deletes ignored. intuition
behind fact balances high positive values would characterize easier (relaxed) problems
given domain, since achieved facts need deleted many times. Given
number relevant facts planning task variable, compute statistics (i.e., min, max,
average variance) fact balance relevant facts. Additionally, compute
statistics considering facts goals, following procedure.
661

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

complete set 89 features listed organized category Appendix A.
Data Integration process Figure 1 receives features performance datasets inputs
produce final dataset according modeling goal. dataset classification task,
training/test instance includes planning task features plus planner name Boolean
feature indicating whether planner solved planning task. dataset regression task
includes cases planning tasks solved. make exclusion
make sense model estimate planning time beyond given time limit
cases time unknown. training/test instance regression dataset includes
planning task features, planner name time planner used find best solution.
Feature Selection optional process reducing number features used
modeling. procedure applied might irrelevant redundant features
could degrade modeling capabilities learning techniques (Blum & Langley, 1997).
outcome process dependent original data. Thus, decision whether apply
taken based results model evaluation.
Modeling process, use off-the-shelf data-mining tool provides set learning
algorithms classification regression. generated models evaluated
Evaluation process determine best model classification regression tasks.
many different ways carrying model evaluation comparison (Han, Kamber, & Pei,
2011; Witten & Frank, 2005), reflect generalization ability different models
making predictions unseen data.
2.1.3 TRATEGY ELECTION
strategy selection final step construction IBAC P planner. Selecting strategy implies decide transform predictions best models actual
portfolio configuration. several alternatives range ignoring model predictions trusting completely. classification model, candidate planner get
yes/no prediction given new planning task. direct use Boolean variable makes difficult
decide planners include portfolio. Consider, instance. two extreme cases:
(1) planners get positive prediction, include them? (2) planners get
negative prediction, planner include portfolio? Instead using Boolean
prediction propose rank predictions confidence positive class,
make selection planners according ranking. Then, planner assigned
slide total time, assignment carried uniformly dependently, again,
predictive models learned. Therefore, depending use make predictive
models, propose three basic strategies:
1. Equal Time (ET): strategy use predictive models all. assign
equal time planner (uniform strategy). idea behind strategy
planners less time one. strategy obtained good results
portfolios (Seipp, Braun, Garimort, & Helmert, 2012).
2. Best N confidence (BN): strategy include subset N planners best
prediction confidence positive class portfolio. Then, get equal time
solving planning task. case, idea select subset promising planners
spend time solving planning task.
662

fiT IBAC P P LANNING YSTEM

3. Best N Estimated Time (BNE): subset planners selected mentioned before,
time assigned proportionally estimated time provided regression model.
2.2 Portfolio Configuration
instance-based configuration portfolio implies subset base planners time
assigned one varies function planning task features. set candidate planners,
predictive models configuration strategy previously fixed construction phase.
Algorithm 1 shows use components configure portfolio given planning
task.
Algorithm 1: Algorithm configuring portfolio particular planning task.
Data: Problem (), Domain (d), Set base planners (Pini ), Classification model (C),
Regression model (R), Available time (T ), Strategy (SN )
Result: Portfolio Configuration: sequence planners assigned runtime,
Portfolio = [hp1 , t1 i, . . . , hpc , tc i]
Portfolio=[];
SN == ET
/*(No classification regression models available)*/
n = size(Pini );
p Pini
append(hp, Tn i, Portfolio);
else
hF, tF = extractFeatures(d, );
pk Pini
predictionhpk , confk predict (C, hF, pk i);

sorted candidates sort(prediction, key = conf );
p sorted candidates[i . . . N ];
SN == BN
/*Classification model available, applying Best N confidence strategy*/
= 1 N
F
append(hpi ,
N i, Portfolio);
else
/*Regression model available, applying Best N Estimated Time*/
= 1 N
ti = predict time(R, hF, pi i);
= scaleTime(t, tF );
= 1 N
append(hpi , ti i, Portfolio) ;

method receives problem (), domain (d), set base planners (Pini ), classification model (C), regression model (R), time available (T ) portfolio configuration
strategy (SN {ET, BN, BN E}). procedure calls several functions described below:
663

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

extractFeatures: feature extraction procedure used portfolio construction phase. pair (domain, problem) function outputs set features F .
function also computes time (tF ) time spent extracting features.
predict: function query classification model C. receives new instance
represented tuple hF, pi, F previously computed features, p
planner name. result function ignore class, keep prediction
confidence positive class, forming tuple hp, conf i. output represents
confidence planner p solve problem.
predict time: function uses model R estimate execution time subset
planners PN Pini established best N candidates terms classification confidence. classification model, function receives input tuple
hF, pi.
scaleTime: function transforms vector estimated times another proportional
vector sum fits available time, original time bound minus
time used compute features tF . Thus, time assigned planner computed
F )t
formula = (TPt
N
i=1 ti

output algorithm sequence planners assigned time. execution
particular configuration portfolio comprises sequential execution base planners
ensuring CPU process exceed assigned time.

3. IBaCoP Planning System
section describe follow approach presented Section 2 build different
portfolios.
3.1 Candidate Planners
initial set planners includes 27 planners Sequential Satisficing Track IPC-2011
plus LPG- TD (Gerevini, Saetti, & Serina, 2006). Although LGP- Td compete IPC-2011
considered worthwhile include still considered state-of-the-art planner due
great performance previous competitions.
first step apply QT-Pareto Score Filtering described subsection 2.1.1 reduce
initial set candidate planners. benchmarks computing QT-Pareto Score set
domains problems Sequential Satisficing Track IPC-2011.
Table 1 shows best planner terms QT-Pareto score domain. Additionally,
include number problems solved best planner highlight correlation among
values. QT-Pareto score values closer 20 reflect planner able beat
planners problems. P ROBE best planner 4 domains. However
planners stood one domain. reinforces motivation find diverse subset
planners. Finally, 28 initial planners, QT-Pareto score filtering pre-selected candidate
planners subset 11 planners, made of: LAMA -2011, PROBE , ARVAND , FDSS 2, FD - AUTOTUNE -1, FD - AUTOTUNE -2, LAMAR , LAMA -2008, MADAGASCAR , YAHSP 2- MT
LPG- TD. brief description planners found Appendix D.
664

fiT IBAC P P LANNING YSTEM

Planner
PROBE
PROBE
PROBE
PROBE
ARVAND
MADAGASCAR
LAMA -2008
LAMA -2011
FD - AUTOTUNE -1
FD - AUTOTUNE -2
FDSS -2
LAMAR
YAHSP 2- MT
LPG- TD

Domain
scanalyzer
woodworking
tidybot
barman
pegsol
parcprinter
transport
openstacks
sokoban
nomystery
elevators
parking
visitall
floortile

total

QT
16.59
18.55
16.77
19.42
18.88
17.63
17.84
17.30
17.56
16.73
17.84
18.12
18.74
11.96
243.77

Coverage
20
20
18
20
20
20
19
20
19
19
20
20
20
12
267

Table 1: List best planners ordered QT-Pareto score domain IPC-2011.
Table 2 shows ranking planners IPC results (i.e., planner ordering established
quality score) (Linares Lopez et al., 2015) selected QT-Pareto Score
Filtering. worth noting attention 10 11 best planners IPC built top
FD, reduces diversity planners. However, QT-Pareto Score Filtering
includes 8 them. addition, pointed last three selections QT-Pareto
Score Filtering planners lower positions table which, demonstrated
later, increases diversity portfolio performance.
Ranking
1
2
3
4
5
6
7
8
9
10
11
17
22
24

planner
LAMA -2011
FDSS -1
FDSS -2
FD - AUTOTUNE -1
ROAMER
FORKUNIFORM
FD - AUTOTUNE -2
PROBE
ARVAND
LAMA -2008
LAMAR
YAHSP 2- MT
MADAGASCAR
LPG- TD

Eligible












FD











Table 2: List 11 best planners ordered score IPC-2011. third column shows whether
selected QT-Pareto Score Filtering. forth column shows planners
built top FD.

665

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

3.2 Performance Models
inputs performance modeling phase candidate planners (i.e., 11 candidates
selected previous section) benchmark planning tasks selected purpose. Next,
describe generated training data, inputs produce specific instances
IBAC P planners.
3.2.1 RAINING DATA
training data learning process requires set domains problems used gather
input features. need wide range domains problems generalize future unknown
planning tasks properly. included planning problems available IPC-2006 onwards. mention test set explicitly, always refer satisficing tracks
competitions. included domain problems are:
IPC-2006: openstacks, pathways, rovers, storage, tpp trucks.
IPC-2008: cybersec, elevators, openstacks, pegsol, pipesworld, scanalyzer, sokoban, transport woodworking.
IPC-2011: barman, elevators, floortile, nomystery, visitall, tidybot, openstacks, parcprinter,
parking, pegsol, sokoban, scanalyzer, transport woodworking.
Learning track IPC-2008: gold-miner, matching-bw, n-puzzle, parking, thoughful sokoban.
Learning track IPC-2011: barman, blockworld, depots, gripper, parking, rovers satellite,
spanner tpp.
list obtained 45 different domain descriptions. Although represent
alternative encodings domain, included. Candidate planners run
benchmarks obtain features related performance planners. Thus, used
total 1, 251 planning tasks. performance data comprises 13, 761 instances (i.e., 1, 251 problems 11 planners) 8, 697 successful 5, 394 failed. proportion instances
solved candidate planner different. Table 16 Appendix C shows per-planner summary
performance data.
89 features representing planning task automatically generated domain
problem definitions. PDDL features, FD instantiation SAS+ features computed
using FAST-D OWNWARD pre-processor. computation time needed extract features
negligible compared SAS+ translation, given compute sums statistics
data provided SAS+ representation. heuristic features computed using FASTD OWNWARD search engine, fact balance features generated using relaxed planning
graph structures (of initial state) provided FF planner (Hoffmann, 2003). FASTD OWNWARD pre-processor could fail instantiating planning task. case, regarding
features computed missing values assumed.
Table 3 shows success rate extracting features type training problems, average maximum time seconds extract them. PDDL, FD SAS+
features extracted FD pre-processor success rate.
time required compute heuristic features time calculating heuristic value
initial state, calculated FD pre-process finished successfully.
666

fiT IBAC P P LANNING YSTEM

Class
PDDL
FD
SAS+
Heuristic
Fact Balance
Total

Success
97%
97%
97%
87.54%
93%
-

Average (s.)
6.97
52.73
22.60
20.20
5.20
107.7

Max (s.)
46.00
141.40
60.60
30.50
21.20
299.7

# features
8
16
50
8
7
89

Table 3: Summary extracted features average maximum time seconds (s.)
extract them. processes top two first step planners based
FD.

3.2.2 F EATURE ELECTION
carried feature selection process two main reasons. one hand, features
might irrelevant whilst others might redundant modeling purpose. Therefore want
analyze whether possible obtain better models using subset available features.
hand, study allow us recognize relevant features characterizing
planning task.
feature selection carried using J48 algorithm, top-down induction algorithm
build decision trees (Quinlan, 1993), selecting features appear top nodes
tree (Grabczewski & Jankowski, 2005). Decision trees make implicit feature selection
model includes queries features considered relevant. applying feature selection
process feature dataset, total number features decreased 89 34. leads
dataset size reduction around 62%. Table 4 contains list features resulting
feature selection process. selection chooses features categories. modeling
evaluation process kept datasets separate, one available features (f-all)
one selected features (f-34).
3.2.3 C LASSIFICATION ODELS
trained classifiers using 31 classification algorithms provided Weka (Witten & Frank,
2005), includes different model types decision trees, rules, support vector machines
instance based learning. recall training instances include planning task features
described Section 2.1.2 plus planner name Boolean feature indicating whether
planner solved planning task not. performance predictive models evaluated
10-fold cross-validation uniform random permutation training data. best
model datasets f-all f-34 generated Rotation Forest (Rodriguez, Kuncheva,
& Alonso, 2006), achieving 93.39 92.35% accuracy respectively. results quite
better result default model (ZeroR), obtained 61.72% accuracy. See
results classification models Table 14 Appendix B.
Even though good accuracy classification model guarantee good performance
portfolio, result great starting point selecting promising planners. accuracy
results feature selection showed small differences compared results obtained
667

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

Type

PDDL
(4)

CG & DTG
(11)

Features
types
goal
objects
functions

numberVariablesCG
inputEdgeCGStd
outputEdgeCGAvg
outputWeightCGMax
outputWeightCGAvg
outputEdgeHVStd
outputWeightHVMax
numberVariablesDTG
totalEdgesDTG
inputWeightDTGMax
hvRatio

Type

FD
(6)

Heuristics
(7)

Balance
(6)

Features
auxiliary atoms
implied effects removed
translator facts
translator total mutex groups size
num relevant facts
num instance actions
Additive
Context-enhanced additive
FF
Goal count
Landmark count
Landmark-cut
Max
rp fact balance avg
rp fact balance var
rp goal balance min
rp goal balance avg
rp goal balance var
h ff ratio

Table 4: List features feature selection. complete set features listed Appendix A.

features. 3 algorithms statistically better accuracy f-34 dataset nine
similar accuracy, cases best achieved accuracy

3.2.4 R EGRESSION ODELS
trained regression models positive instances classification training
phase. classification phase, planners proportion instances,
case, planners number instances given solved different
number problems. Nevertheless consider relevant bias models
include planner name, somehow encodes single models planner, grouped
model. trained models 20 regression algorithms, also provided Weka.
best algorithm f-all Decision Table (Kohavi, 1995) Relative Absolute Error
(RAE) 49.87 best one f-34 Bagging (Breiman, 1996) RAE 50.62.
Nevertheless, simplicity selected Decision Table model regression task
datasets (f-all f-34). decision justified results show significant
difference t-test result. following sections, regression model always refer
trained Decision Table algorithm. See results regression models
Table 15 Appendix B.
668

fiT IBAC P P LANNING YSTEM

3.3 IBaCoP Strategies
considered various strategies configuration IBAC P portfolios. list
strategies ordered depending use make knowledge provided predictive
models. experiments, configuration run 1800 seconds. named
portfolios according names given IPC-2014.
IBAC P: portfolio uses equal time strategy (ET) set 11 candidate planners previously filtered QT-Pareto Score Filtering procedure. Therefore, single planners
run 163 seconds. strategy use predictive models. planner using
strategy awarded runner-up sequential satisficing track IPC-2014.
IBAC P2: portfolio uses Best N confidence strategy (BN), N = 5. means
5 planners best prediction confidence solving problem included
configuration. run time assigned uniformly planner (360 seconds).
strategy, using f-34 model winner sequential satisficing track IPC-2014.1
IBAC P2-B5E: portfolio uses Best estimated time strategy (BNE) N = 5. follows procedure IBAC P2 select 5 planners, time assigned
scaling time prediction provided regression model (Decision Table). strategy
participated learning track IPC-2014 name LIBAC P2. case
training data models generated domain separately, since learning track
provides training problem set domain priori.
addition, built portfolio configurations serve baseline comparison.
Overall Equal Time (OET): strategy non-informed strategy carry
planner filtering use predictive models. assigns equal time available planner.
Given 28 planners (all participants IPC-2011 plus LPG-td), planner
run 64 seconds. planner see need planner filtering since,
although already obtains results close current state art base planners, results
improved selecting reduced set planners.
Best 11 Planners (B11): strategy selects top 11 planners IPC-2011 ordered score
competition, shown Table 2. Although selecting best 11 planners good
choice intuitively, show table selection reduces planner diversity
portfolio, since top planners competition based FD, exception Probe. strategy comparable implemented BUS portfolio (Howe,
Dahlman, Hansen, Scheetz, & von Mayrhauser, 1999), control strategy ordering planners allocating time derived performance study data.
Random 5 Planners (Rand): strategy one baselines compare best 5 confidence strategy (IBAC P2). Given planning task, strategy takes random sample
5 planners population 11 candidate planners selected QT-Pareto filtering,
1. Predictive models submitted IBAC P2 IPC-2014 trained different benchmark set. case
best accuracy achieved Random Forest (Breiman, 2001).

669

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

assigns equal time them. expect wise selection 5 planners (IBAC P2)
average better random selection.
Default 5 Planners (Def): case, strategy always includes 5 best planners terms
quality score training data. 5 planners subset 11 candidate
planners selected QT-Pareto filtering (i.e., LAMA -2011, PROBE , FD -AUTOTUNE -1,
LAMA -2008 FD - AUTOTUNE -2). Then, time assigned equitably. want see
whether using best 5 planners better making per-instance selection 5 planners.
3.4 Implementation Details
section describe engineering details incorporated IBAC P
planners. instance, competition rules proposed include domains conditional effects.
this, included parser translates tasks conditional effects
equivalent planning task without property. translator based previous translator ADL2STRIPS (Hoffmann, Edelkamp, Thiebaux, Englert, dos Santos Liporace, & Trug, 2006).
Specifically, implemented compilation creates artificial actions effect evaluations (Nebel, 2000).
Furthermore, many 11 candidate planners built FAST-D OWNWARD framework, among things, separate planning process sub-process translation,
pre-processing search. Indeed, translation pre-process steps already executed
feature generation given task performed. take advantage fact avoid
first two steps repeatedly planners included configuration
portfolio regarding task. version compatibility reasons procedure divided
two groups. output FD pre-process, used feature extraction, also used
search input LAMA -2011, FDSS -2 FD - AUTOTUNE (1 & 2). previous FD pre-processor
2 used common LAMA -2008, ARVAND LAMAR . optimization used
strategies evaluated. remaining planners totally independent FD pre-processing.
Moreover, bugs arose execution IPC-2014, issues domain
models required updates (Vallati, Chrpa, & McMcluskey, 2014a), planners updated
Mercury (Vallati, Chrpa, & McMcluskey, 2014b). issues also fixed prior
running experimental evaluation presented article.

4. Experimental Evaluation
section, describe settings experimental evaluation present results
planners benchmarks used IPC-2014, specifically, Sequential Satisficing
track. addition, provide analysis diversity planner selection achieved
configurations.
4.1 Experimental Settings
evaluated different portfolio strategies described Section 3.3, permits different
portfolio configurations created. IBAC P2 IBAC P2-B5E run two predictive
model versions, one trained features (f-all) one trained selected fea2. version corresponds version used submit planners IPC-2011

670

fiT IBAC P P LANNING YSTEM

tures (f-34). Random strategy run 5 times average reported. addition,
included JASPER ERCURY planners comparison. planners also competed IPC-2014. ERCURY (Domshlak, Hoffmann, & Katz, 2015) second best planner
terms IPC score JASPER (Xie, Muller, & Holte, 2014) second best planner terms
problems solved (coverage). test set used benchmarks IPC-2014,
updates described Section 3.4. test set comprises 14 domains 20 problems
domain.
Experiments run cluster Intel XEON 2.93 Ghz nodes, 8 GB RAM,
using Linux Ubuntu 12.04 LTS. planners cutoff 1, 800 seconds 4 GB RAM.
IBAC P configurations requiring feature extraction, process limited 4 GB RAM
(following IPC competition rules) 300 seconds (which maximum time used training
set obtain features, described Table 3). time extract features included
execution portfolio where, worse case, feature extraction process took 300 seconds
and, therefore, candidate planners 1, 500 run. system extract
features time, input features treated missing values.
4.2 Results
Table 5 shows results evaluated planners using IPC quality score. recall

score gives ratio Q
Qi planner problem, Qi quality best solution
found planner i, Q best solution found planner. planner solve
problem score 0.

Hiking
Openstacks
Thoughtful
GED
Barman
Parking
Visitall
Maintenance
Tetris
Childsnack
Transport
Floortile
CityCar
CaveDiving
Total

IBaCoP2
f-all
f-34

IBaCoP2-B5S
f-all
f-34

Mercury

Jasper

OET

B11

Def

Rand

IBaCoP

18,9
19.6
12.7
19.4
14.6
18.0
20.0
5.1
16.3
0.0
19.9
2.0
4.1
7.0

18.1
18.8
16.4
17.9
19.0
17.0
15.4
10.0
16.2
0.0
12.0
2.0
11.5
8.0

18.2
15.4
14.5
18.3
16.7
17.6
13.3
15.0
5.0
12.0
8.9
4.8
6.0
0.0

19.2
17.2
19.4
17.1
16.7
13.8
8.1
15.9
11.5
3.4
3.8
3.4
8.8
0.0

18.7
19.2
19.2
16.3
17.2
18.0
13.7
11.6
9.3
2.6
6.9
4.1
5.0
7.0

18.4
16.3
17.4
13.0
13.8
11.6
15.0
14.5
11.9
8.9
8.2
12.3
9.4
7.0

19.0
17.8
19.2
17.5
16.9
16.3
15.2
15.6
13.3
19.2
10.3
16.2
12.5
6.3

18.9
18.6
19.2
17.6
17.1
18.1
18.0
15.5
12.5
18.4
11.5
15.3
9.0
7.0

18.6
18.5
17.4
17.5
17.1
18.1
18.0
15.4
11.9
18.9
11.6
17.2
6.2
7.0

18.8
18.2
17.6
17.6
17.2
18.5
18.0
15.5
15.7
15.0
11.1
17.5
9.9
7.0

18.6
18.3
19.2
17.5
17.2
18.1
18.0
15.4
13.6
18.9
12.1
12.0
7.78
7.0

177.6

182.1

165.7

158.3

168.8

177.6

215.3

216.5

213.4

217.4

213.7

Table 5: Results IBAC P configurations. table also includes results Jasper, Mercury
four baseline configurations, OET, Best 11, Default Random.

overall best planner IBAC P2-B5E (f-all), closely followed IBAC P2 (f-all).
difference two configurations negligible. configurations using predictive
models much better OET, Default, Best 11 Random. IBAC P good performance, comparable best performance. Moreover, big difference
671

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

configurations planners (Jasper Mercury). IBAC P based configurations 32
points higher cases.
Figure 2 details evolution number problems solved function run-time
elapsed. far right-hand point figure represents final coverage. best planner
terms coverage IBAC P, 249 problems, second IBAC P2 (f-all) 246.
Figure 2, planners show two different behaviors. one hand, asymptotic growing
number problems solved demonstrates giving time planners permit
number problems solved increased. JASPER extreme case, 300 seconds
almost unable improve. ERCURY problem, well portfolio configurations
take care diversity. However, IBAC P, IBAC P2 IBAC P2-B5E,
selected diverse set planners, show growing behavior throughout time.
250

200

Problems

150

100

IBaCoP
IBaCoP2
IBaCoP2-B5E
Random
Jasper
Default
Mercury
Best11
0ET

50

0
0

200

400

600

800

1000

1200

1400

1600

1800

Time

Figure 2: Comparison IBAC P configurations, baseline configurations, Mercury
Jasper planners.
results derive insights regarding different configurations. score
difference OET IBAC P reveals importance making pre-selection candidate
planner accurate filtering procedure. Pareto-dominance approach allows us
smaller set planners, means time per planner. trade-off
time per planner loosing diversity solvers, results suggest
important maintain diversity increasing running time per planner. instance,
11 best IPC-2011 planners (B11) obtain worse results using original 28 (OET), even
though B11 base planners longer running time. However, QT-Pareto filtering approach
able reduce number planners sacrificing diversity, produces good
results.
Reducing number planners portfolio configuration 11 5 puts risk
diversity solvers, shown results Def approach (the best 5 planners terms
672

fiT IBAC P P LANNING YSTEM

performance) Rand (the random selection 5 planners). Nevertheless, IBAC P2 (f-all)
(f-34) perform quite better Def Rand, demonstrates classification models
select average good subset planners solving particular task. results quite
promising exploiting empirical performance models planning portfolios. However,
current setting, results IBAC P2 quite similar IBAC P. Thus, classification models
manage reduce set planners without deteriorating performance fixed portfolio,
hardly contribute better overall performance.
Table 6 presents number problems solved 11 candidate planners. final
column maximum number problems solved complete set candidate
planners (i.e., problem solved least one candidate planners solved
problem). optimal selection 5 planners planning task would lead 253 problems
solved. IBAC P2 close optimum, confirming ability selecting good candidates
portfolio. default configuration solved 193 problems, average number problems
solved random configuration 207 problems. far best possible
value.
Hiking
Thoughtful
Openstacks
Tetris
GED
Transport
Parking
Barman
Maintenance
CityCar
Visitall
Childsnack
Floortile
CaveDiving
total

lama11
18
15
20
9
20
15
20
20
7
1
20
0
2
0

probe
20
12
4
14
20
12
9
19
8
0
10
0
2
0

FDA1
18
16
19
15
20
7
14
15
10
5
0
2
2
0

lama08
20
17
20
8
0
12
13
13
1
4
2
2
2
0

FDA2
20
12
20
1
0
6
2
2
8
5
0
0
5
0

lamar
20
14
20
13
0
7
18
15
1
8
0
2
2
0

arvand
20
20
20
18
0
5
0
0
17
19
2
8
1
0

fdss2
20
17
12
17
20
10
16
8
16
5
0
3
2
0

ya2-mt
4
13
0
0
0
20
0
0
3
2
20
0
0
0

LPG
20
8
1
0
14
0
0
0
8
0
1
7
19
0


3
5
0
0
0
0
0
0
6
14
0
20
0
7

Max
20
20
20
18
20
20
20
20
17
19
20
20
19
7

166

130

143

114

81

120

128

146

62

74

55

260

Table 6: Results candidate planners defined Table 1 maximum number problems
solved complete set planners.

set 5 planners selected per-instance configuration, regression
models contribute better performance. task estimating run time needed
solve problem difficult classification task (Schwefel, Wegener, & Weinert, 2013).
Additionally, given aggregated time predictions could exceed time limit, proposal rescales estimations alters real predictions. One alternative proposal keep
real prediction run planners order established confidence classification
prediction, one reaches time limit. However, preliminary experiments
development planner showed us approach compensate risk losing
diversity due fewer planner executions.
Another aspect analyzed performance planners new domains. IPC2014 incorporated seven new domains, means QT-Pareto Filtering predictive
models trained them. domains Cave Diving, Child-Snack, CityCar,
673

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

GED, Hiking, Maintenance Tetris. results conclude behavior
IBAC P configurations new domains average similar performance previously seen
domains.
4.3 Per-Instance Selection Planners
previous section showed benefit configuring portfolio per problem
set selected planners better adjusted problem, using fewer planners, providing
execution time planner. section want analyze diversity planner
selections made IBAC P2 see predictive models classifying planners good
solving specific domains identifying properties specific problems
different domains. Note test problems given domain usually range easy hard.
increase difficulty mainly due larger size problems. Nevertheless, increase
affects learning features different scale intensity.

lama-2011
probe
fd-autotune-1
lama-2008
fd-autotune-2
lamar
arvand
fdss-2
yahsp2-mt
LPG-td
madagascar

ita
Vis
rt
po
ns
l
Tra
tfu
gh
ou
Th
tris
Te
ng
rki
Pa tacks

en
e
Op nanc
e
int

ing

Hik


GE e
il
ort
Flo r

yC
k
Cit
ac
sn
ild
Ch
ing
Div

Ca n

rm
Ba

Figure 3: Proportion number times planner selected domain. red
dots, proportion IBAC P2 (f-all), blue dots, proportion IBAC P2
(f-34).

Figure 3 shows diversity planners according selection made IBAC P2 (blue
dots f-34 red dots f-all). x axis shows IPC-2014 domains axis lists
11 candidate planners portfolio use. size dots proportional
number times planner selected particular domain, i.e. number problems
planner selected. domain five dots one column (one domain), means
selected portfolio configuration problems domain. However, every
674

fiT IBAC P P LANNING YSTEM

column five dots reveals use different 5-planner sets different problems
domain. highlight analysis 11 planners selected
least one domain, 13 14 domains selections involve 5 planners.
Note, instance, LAMA -2011 best priori confidence solving problems,
sometimes used (i.e., selected 6 times Floortile 11 times Openstacks).
Furthermore, planners low priori probability selected, frequently
used domains (like LPG- TD Floortile).
Table 7 shows sum number times planner selected. maximum number times planner could selected 14 20 = 280. last column reports
average standard deviation number times planner selected
per domain approximations (all reduced set features).

LAMA -2011
PROBE
FD - AUTOTUNE -1
LAMA -2008
FD - AUTOTUNE -2
LAMAR
ARVAND
FDSS -2
YAHSP 2- MT
LPG- TD
MADAGASCAR

f-34
248
200
173
173
93
152
65
122
95
29
35

f-all
256
206
151
157
88
133
111
149
71
31
45

Average
18,00
14,50
11,57
14,50
6,46
10,18
6,29
9,68
5,93
2,14
2,86














STD
4,02
6,71
6,29
6,71
7,13
6,98
5,90
7,94
7,26
4,99
5,73

Table 7: Number times candidate planner selected two different classification
models (f-34 f-all).

addition previous analysis, wanted delve underlying mechanism
achieve per-instance selection planners. recall planners selected based
confidence success prediction. Therefore, order achieve different 5-planner sets
domain, ranking prediction confidence vary throughout problem.
visualize confirm fact, selected Tetris domain, one new
domains IPC-2014 shows good diversity selection shown Figure 3. domain
simplified version well-known Tetris game.
heatmap success prediction confidences appears Figure 4. glance realized general, planner higher success rate training time obtains higher confidence,
confidence ranking varies throughout different problems domain. Another way
read picture 5 darkest squares per column form set selected planners. instance, lama-2011 selected problems probe selected 18 times. hand
Madagascar selected, LPG-td selected 3 times.
675

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

lama-2011
probe
fd-autotune-1
lama-2008

Score

fd-autotune-2
lamar
arvand
fdss-2
yahsp2-mt
LPG-td
madagascar
0

1

2

3

4

5

6

7

8

9

10 11 12 13 14 15 16 17 18 19

Figure 4: Success prediction confidence provided classification model (f-all) planner
problem tetris Domain. Scale goes 0.0 (white) confidence
1.0 (dark blue) complete confidence.

5. Related Work
section, summarize relevant research portfolio configuration relates
work. addition, summarize different approaches characterization planning
tasks, cornerstone work predict behavior planners.
idea exploiting synergy different solvers improve performance individual ones applied propositional satisfiability problems (SAT), constraint satisfaction problems
(CSP), answer set programming (ASP) scope paper, Automated Planning.
SAT area carried extensive research importance selecting components
portfolio (Xu, Hutter, Hoos, & Leyton-Brown, 2012) select component (Lindauer, Hoos, Hutter, & Schaub, 2015b) automatically. study strategy selection area
includes per-instance selections (Lindauer, Hoos, & Hutter, 2015a). addition, intensive
study solver runtime prediction (Hutter, Xu, Hoos, & Leyton-Brown, 2015), including good
characterization satisfiability task. fields Artificial Intelligence, CSP portfolio configurations based machine learning techniques SUNNY (Amadini, Gabbrielli, &
Mauro, 2014b) empirical research (Amadini, Gabbrielli, & Mauro, 2014a). example
ASP, ASP-based Solver Scheduling (Hoos, Kaminski, Schaub, & Schneider, 2012) multicriteria optimization problem provides corresponding ASP encodings. paper
report main systems related Automated Planning detail.
5.1 Portfolios Automated Planning
Howe et al. (1999) describes one first planner portfolios. implement system called
BUS runs 6 planners whose goal find solution shortest period time.
achieve it, run planners portions time circular order one finds
solution. portfolio, planners sorted following estimation provided linear
676

fiT IBAC P P LANNING YSTEM

regression model success run-time so, case, use predictive models
behavior planners decide order execution. However, use 5 features
extracted PDDL description. domain, count number actions
number predicates. problem, count number objects, number predicates
initial conditions number goals. BUS minimizes expected cost implementing
sequence algorithms one works, contrast IBAC P IBAC P2, stop
assigned time over.
Fast Downward Stone Soup (FDSS, Helmert et al., 2011) based Fast Downward (FD)
planning system (Helmert, 2006), several versions different tracks. FDSS approach
select combine heuristics search algorithms. configuration combination search
algorithm group heuristics. training, evaluate possible configurations time
limit, select set configurations maximizes coverage. portfolio presented
IPC-2011 Sequential Satisficing Track, sort configurations decreasing order
coverage, hence beginning algorithms likely succeed quickly. time limit
component lowest value would still lead portfolio score training phase.
However, order important, since setting communicates quality best solution
found far following one, value used improve performance next
setting. Therefore, FDSS include configurations within FD framework. Conversely,
IBAC P IBAC P2 build portfolio using mixture generic planners different styles
techniques. Indeed FDSS one IBAC P candidate planners.
PbP (Gerevini et al., 2009) configures domain-specific portfolio. portfolio incorporates
macro-actions specific knowledge domains. incorporation knowledge establishes order subset planners contain macro-actions. running time assigned
round-robin strategy. portfolio incorporates seven planners (the latest version, PbP2,
adds lama-2008, see Gerevini et al., 2014). automatic portfolio configuration PbP IBA C P aims build different types planning systems: domain-optimized portfolio planner
given domain PbP IBAC P efficient domain-independent planner portfolio.
IBAC P PbP configuration processes significantly different. PbP uses several planners
focus macro-actions whilst IBAC P uses generic planners. execution scheduling
strategy PbP runs selected planners round-robin rather sequentially case
IBAC P.
Fast Downward Cedalion (Seipp, Sievers, Helmert, & Hutter, 2015) algorithm automatically configuring sequential planning portfolios parametric planner. Given parametric
planner set training instances, selects pair planner time iteratively. end
iteration instances current portfolio finds best solution removed
training set. algorithm stops total run time added configurations
reaches portfolio time limit training set becomes empty. Configurations generated
using SMAC (Hutter, Hoos, & Leyton-Brown, 2011) model-based algorithm configurator
remaining training instances. Cedalion configuration problems
different configuration per version IBAC P different configuration per problem. diversity candidate planner limited IBAC P may completely include independent base
planners. configuration processes resulting configured portfolios Cedalion
FDSS.
Fast Downward Uniform (Seipp et al., 2012) portfolio runs 21 automatically configured Fast
Downward instantiations sequentially amount time. Uniform portfolio approaches
677

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

configured using automatic parameter tuning framework ParamILS (Hutter, Hoos, LeytonBrown, & Stutzle, 2009) find fast configurations Fast Downward planning system 21
planning domains separately. runtime, configurations found run sequentially
amount time 85 seconds.
MiPlan (Nunez et al., 2015) sequential portfolio using Mixed-Integer Programming,
computes portfolio obtains best achievable performance respect selection
training planning tasks. case created sequential portfolio subset
sequential planners fixed times whilst IBAC P2 different configurations per problem.
approximation, planner consider portfolios, components.
contrast, IBAC P IBAC P2 includes planners appear competitions, i.e.
black boxes.
5.2 Features Planning Problems
construction models predict performance planners novel idea. Roberts et
al. (2008, 2009) showed models learned planners performance known benchmarks
2008 obtain high accuracy predicting whether planner succeed not.
use 19-32 features extracted domain problem definition. main difference
approach also include features based SAS+ , heuristics initial state
fact balance relaxed plan. features come ground instantiation
problem, key differentiate tasks share feature values PDDL
level.
Torchlight (Hoffmann, 2011) toolkit allows search space topology analyzed
without actually running search. analysis based relation topology
delete relaxation heuristics causal graph well DTGs. feature extraction
process built top FF planner (Hoffmann & Nebel, 2001).
Recently, Fawcett et al. (2014) generated models accurately predicting planner run
time. models exploit large set instance features, including many features depicted
Section 2.1.2. features derived PDDL SAS+ representations problem, SAT encoding planning problem short runs planners. features
extracted Torchlight (Hoffmann, 2011). experimental results work indicate
performance models generated able produce accurate run time predictions. study
empirical performance models applied portfolio configurations.

6. Conclusion Future Work
work introduced framework creation configurable planning portfolios,
IBAC P. first step portfolio creation find small number planners maintains
diversity initial planner set based QT-Pareto score filtering. train predictive
models select promising sub-set planners solving particular planning task.
experimental evaluation confirmed great performance IBAC P IBAC P2
IPC-2014. summarize lessons learned development current IBAC P
portfolios following:
really matters generation good portfolio selection diverse set
planners. shown QT-Pareto score filtering reduces set candidate plan678

fiT IBAC P P LANNING YSTEM

ners preserving diversity. filtering produces better results rankings
based coverage quality score.
selection smaller sets planners portfolio configuration (e.g., sub-set 5
planners experiments) dangerous given portfolio might lose planner diversity.
observed situation Def Random configurations, select 5 11
planners.
portfolio configurations using classification models able select good subset
5 planners, uniformly distributed time outperformed selection provided
random default selection number planners.
Estimating runtime solving problem still difficult reason regression models providing additional useful information portfolio construction.
current form, predictive models hardly contribute overall performance
portfolio. Per-instance configurations using classification models achieve similar performance fixed portfolio, running fewer planners.
Even though current architecture benefits using predictive models limited,
results promising good performance IBAC P2 compared baseline
configurations. think room research direction. argument
static portfolio configurations (including IBAC P) limited components fixed
time bound base planner. performance upper-limit, computed MiPlan,
smaller achievable performance dynamic configuration.
per-instance configuration portfolio strategy could assign different times base planners.
future work want study additional features better characterization planning
tasks. computation could carried pre-process step, even information
first evaluated search nodes, could help making predictive models accurate.
models could incorporate information, instance, landmark graph time elapsed
computing initial state heuristics. future work study importance created
features, including comparison different groups accordance semantics
features.

7. Acknowledgments
thank authors base planners work based largely previous effort.
work partially supported Spanish projects TIN2011-27652-C03-02, TIN201238079-C03-02 TIN2014-55637-C2-1-R.

679

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

A. Appendix: Complete Feature Description
Appendix present list features used characterize planning task.
feature include brief description computed. Features grouped
category separate tables.
A.1 PDDL Features
N.
1
2
3
4
5
6
7
8

Name
Objects
Goals
Init
Types
Actions
Predicates
Axioms
Functions

Description
number objects problem.
number goals problem.
number facts initial state.
number types domain.
number actions domain.
number predicates domain.
number axioms domain.
number functions domain.

Table 8: PDDL Features.

A.2 FD Instantiation Features
N.
9
10
11
12
13

Name
Relevant facts
Cost metric
Generated rules
Relevant atoms
Auxiliary atoms

14

Final queue length

15

18

Total queue pushes
Implied
effects
removed
Effect preconditions
added
Translator variables

19

Derived variables

20
21
22
23
24

Translator facts
Mutex groups
Total mutex size
Translator operators
Total task size

16
17

Description
number facts marked relevant FF instantiation.
Whether action costs used not.
number created rules translation process create SAS+ task.
number relevant atoms found translator process.
number auxiliary atoms found translator process.
length queue end translation. queue auxiliary
list used translation process compute model.
number times element pushed queue.
number implied effects removed. implied effects
translator knows already included.
number implied effects added.
number created variables SAS+ formulation.
number state variables correspond derived predicates
artificial variables directly affected operator applications.
number facts pre-process takes account.
number mutex groups.
sum mutex group sizes.
number instantiated operators SAS+ formulation.
allowed memory translation process.

Table 9: Features extracted console output FD system.

680

fiT IBAC P P LANNING YSTEM

A.3 SAS+ Feature Description
recall CG, high-level variables variables defined value
goal. Although common definition CG consider edges weighted,
FD system computes edge weights CG number instantiated actions induced
edge. also consider weights computing features.
N.

Name

25
26
27
28

Number Variables
High-Level Variables
TotalEdges
TotalWeight

29

VERatio

30

WERatio

31

WVRatio

32

HVRatio

33-35

InputEdge

36-38

InputWeight

39-41

OutputEdge

42-44

OutputWeight

45-47

InputEdgeHV

48-50

InputWeightHV

51-53

OutputEdgeHV

54-57

OutputWeightHV

Description
General Features
number variables CG.
number high-level variables.
number edges.
sum edge weights.
CG Ratios
ratio total number variables total number
edges. ratio shows level connection CG.
ratio sum weights number edges.
ratio shows average weight edges.
ratio sum weights number variables.
ratio number high-level variables total number variables. ratio shows percentage variables involved
problem goals.
Statistics CG
Maximum, average standard deviation number incoming
edges variable.
Maximum, average standard deviation sum weights
incoming edges variable.
Maximum, average standard deviation number outgoing
edges variable.
Maximum, average standard deviation sum weights
incoming edges variable.
Statistics high-level Variables
number incoming edges high level variables.
value produces three new features following computation
InputEdgeCG (features 33-35).
edge weight sum incoming edges high level
variables. value produces three new features following
computation InputWeightCG.
number outgoing edges high level variables.
sum weights incoming edges high level variables.

Table 10: Features Causal Graph.

681

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

N.

Name

58
59

Number Vertices
Total Edges

60

Total Weight

61

edVa Ratio

62

weEdRatio

63

weVaRatio

64-66

Input Edge

67-69

Input Weight

70-72

Output Edge

73-75

Output Weight

Description
General Aggregated Features DTG
sum number nodes DTGs.
sum number edges DTGs.
sum edge weights DTGs. edge weight DTG
corresponds cost applying action induced edge.
DTG Ratios
ratio total number edges total numbers
variables. ratio shows level connection DTG.
ratio sum weights number edges.
ratio shows number restrictions need make transition.
ratio sum weights number variables.
Statistics DTGs
Maximum, average standard deviation number incoming
edges vertex DTG.
Maximum, average standard deviation sum weights
incoming edges nodes.
Maximum, average standard deviation number outgoing
edges vertex DTG.
Maximum, average standard deviation sum weights
outgoing edges nodes.

Table 11: Features aggregate information DTGs.

682

fiT IBAC P P LANNING YSTEM

A.4 Heuristic Features
N.

Name

76

Max

77

Landmark cut

78

Landmark
count
Goal count

79

FF

80

Additive

81

Causal Graph

82

Contextenhanced
additive

Description
(Bonet, Loerincs, & Geffner, 1997; Bonet & Geffner, 2000) maximum
accumulated costs paths goal propositions relaxed
problem.
(Helmert & Domshlak, 2009) sum costs disjunctive action
landmark represents cut justification graph towards goal propositions.
(Richter, Helmert, & Westphal, 2008) sum costs minimum
cost achiever unsatisfied required landmark.
number unsatisfied goals.
(Hoffmann & Nebel, 2001) cost plan reaches goals
relaxed problem ignores negative interactions.
(Bonet et al., 1997; Bonet & Geffner, 2000) sum accumulated costs
paths goal propositions relaxed problem.
(Helmert, 2004) cost reaching goal given search state
solving number sub problems planning task derived
causal graph.
(Helmert & Geffner, 2008) causal graph heuristic modified use pivots
define contexts relevant heuristic computation.

Table 12: Unit cost heuristics included features.

A.5 Fact Balance
N.

Name

83-85

RP init

86-88

RP goal

89

Ratio ff

Description
Minimum, average variance number times fact
initial state deleted computation relaxed plan.
Minimum, average variance number times goal
deleted computation relaxed plan.
ratio value max FF heuristic. proportion
shows idea parallelization plan.

Table 13: Fact balance features.

683

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

B. Appendix: Learning Results
appendix shows detailed results machine learning algorithms used train predictive models.
B.1 Classification
Algorithm
rules.ZeroR
rules.Ridor
rules.PART
rules.JRip
rules.DecisionTable
rules.ConjunctiveRule
trees.REPTree
trees.RandomTree
trees.RandomForest
trees.LMT
trees.J48
trees.ADTree
trees.NBTree
trees.DecisionStump
lazy.LWL
lazy.IBk -K 1
lazy.IBk -K 3
lazy.IBk -K 5
meta.RotationForest
meta.AttributeSelectedClassifier
meta.ClassificationViaClustering
meta.ClassificationViaRegression
meta.Bagging
meta.MultiClassClassifier
functions.SimpleLogistic
functions.MultilayerPerceptron
functions.RBFNetwork
functions.SMO
bayes.NaiveBayes
bayes.NaiveBayesUpdateable
bayes.BayesNet

f-all dataset
61.72 0.03
82.52 2.48
90.81 0.89
87.21 1.38
85.78 0.98
69.33 1.20
89.08 0.85
86.39 1.81
90.96 0.78
91.11 0.72
90.84 1.01
75.46 1.24
90.38 0.88
67.96 0.96
67.96 0.96
85.93 0.84
86.04 0.90
85.36 0.91
93.39 0.70
89.69 0.89
52.32 1.98
90.82 0.84
90.99 0.74
77.15 1.09
76.37 1.12
87.27 1.65
67.71 1.03
75.39 1.16
69.00 0.98
69.00 0.98
75.43 1.29

f-34 dataset
61.72 0.03
81.76 2.11
89.62 0.89
86.26 1.20
84.94 1.37
69.64 1.61
88.06 0.89
87.91 0.95
90.27 0.85
90.03 0.94
89.24 0.87
74.39 1.30
89.47 0.92
64.10 1.30
63.48 1.86
82.97 1.03
84.13 1.03
84.17 1.01
92.35 0.73
88.64 1.00
57.99 2.66
89.80 0.75
89.83 0.85
75.02 1.14
74.48 1.23
88.65 1.01
68.10 1.17
73.94 1.10
68.87 0.97
68.87 0.97
75.05 1.21

























Table 14: Accuracy standard deviation training algorithm using 10-fold crossvalidation. Also, results t-test (OMahony, 1986) two training sets shown.
Symbols , means statistically significant improvement degradation respectively.
significance level t-test 0.05 baseline left column.

684

fiT IBAC P P LANNING YSTEM

B.2 Regression

trees.DecisionStump
trees.REPTree
trees.RandomTree
trees.RandomForest
functions.M5P
rules.ConjunctiveRule
rules.DecisionTable
rules.M5Rules
meta.Bagging
meta.AdditiveRegression
lazy.IBk 1
lazy.IBk 3
lazy.IBk 5
lazy.KStar
lazy.LWL
functions.LinearRegression
functions.MultilayerPerceptron
functions.LeastMedSq
functions.RBFNetwork
functions.SMOreg

f-all dataset
RAE

82.09 2.36 0.42 0.05
57.70 3.40 0.66 0.05
59.28 6.06 0.55 0.07
52.54 2.66 0.71 0.04
60.44 13.26 0.59 0.18
87.31 2.79 0.38 0.06
49.87 3.03 0.69 0.04
90.60 138.25 0.58 0.18
50.95 2.71 0.74 0.04
80.91 3.21 0.51 0.04
92.96 11.09 0.36 0.06
74.31 6.31 0.47 0.06
73.03 5.91 0.47 0.06
69.26 3.35 0.44 0.05
81.82 2.30 0.43 0.05
77.71 2.55 0.55 0.04
86.01 72.86 0.66 0.05
66.36 2.94 0.33 0.08
94.201.60 0.23 0.05
57.01 2.88 0.48 0.05

f-34 dataset
RAE

82.09 2.36 0.42 0.05
56.69 3.36 0.67 0.05
53.71 4.54 0.61 0.06
45.62 2.68 0.76 0.03
56.38 4.22 0.65 0.09
87.25 2.80 0.39 0.06
51.19 2.78 0.68 0.05
65.84 12.74 0.61 0.14
50.62 2.58 0.74 0.04
79.93 3.29 0.51 0.04
66.73 5.17 0.54 0.06
63.57 4.25 0.60 0.05
64.38 3.81 0.60 0.05
67.75 3.36
0.470.05
81.82 2.33 0.43 0.05
78.58 2.52 0.51 0.04
81.59 45.93 0.66 0.05
66.29 3.01 0.31 0.07
94.251.54
0.210.04
58.75 2.62 0.45 0.05








Table 15: Results 10-fold cross-validation regression models. RAE Relative
Absolute Error correlation coefficient. small RAE values better.
Symbols , means statistically significant improvement degradation respectively.
significance level t-test 0.05 baseline left column.

685

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

C. Appendix: Training Results

openstacks
pathways
rovers
storage
tpp
trucks
pipesworld
cybersec
Openstacks-adl
openstacks
pegsol
scanalyzer
sokoban
transport
woodworking
elevators
barman
elevators
floortile
nomystery
openstacks
parcprinter
parking
pegsol
scanalyzer
sokoban
tidybot
transport
visitall
woodworking
Gold-miner
Matching-bw
N-puzzle
parking
sokoban
thoughtful
barman
blocksworld
depots
gripper
parking
rovers
satellite
spanner
tpp
Total

L-11

Probe

FDA1

L-08

FDA2

Lamar

Arvand

FDSS2

ya2-mt

LPG

30
30
40
40
30
19
42
28
31
30
30
30
29
18
23
30
20
20
6
10
20
20
20
20
20
19
16
19
20
20
30
25
29
28
23
0
9
29
1
0
18
30
16
0
30

30
30
40
40
30
8
44
24
31
30
30
30
27
10
30
29
20
20
5
6
14
14
19
20
20
17
18
20
20
20
30
15
20
24
23
18
5
30
30
0
9
30
10
0
20

30
26
40
40
30
18
40
28
31
30
30
30
29
17
25
30
20
20
7
10
20
20
19
20
20
19
15
11
2
20
30
24
30
25
30
0
0
22
0
0
6
30
3
0
30

30
29
40
40
30
16
38
28
31
30
30
30
25
17
26
25
4
6
3
12
20
1
20
20
20
15
14
19
20
14
30
23
29
28
18
0
0
21
0
0
13
29
3
0
30

30
29
40
40
30
22
33
26
31
30
30
27
27
18
24
30
6
17
9
19
20
14
9
20
17
16
17
10
5
14
26
23
9
16
30
0
0
15
0
30
1
24
29
0
6

30
30
40
40
30
15
43
27
31
30
30
30
25
17
25
27
6
11
3
12
20
0
20
20
20
14
19
3
11
9
29
17
27
30
17
0
0
0
6
0
19
30
1
0
21

27
30
40
40
30
15
46
28
31
30
30
30
8
19
30
30
0
20
3
19
20
20
4
20
20
2
17
15
10
20
30
16
6
17
30
0
0
0
0
4
4
30
2
0
30

30
0
40
40
30
20
42
28
31
30
30
30
29
15
30
30
17
20
7
12
19
20
20
20
20
19
18
15
6
20
0
0
0
0
30
0
13
20
0
0
9
30
22
0
25

0
0
40
40
30
0
41
0
0
1
22
27
0
11
23
2
12
0
8
10
0
13
3
15
17
0
0
20
20
19
30
25
20
13
28
22
0
16
29
0
0
30
13
0
30

27
30
40
40
24
11
33
7
1
0
1
0
0
0
0
0
0
0
12
0
2
0
0
0
0
0
15
0
8
0
30
22
30
13
15
7
0
29
6
30
0
11
30
30
1


11
30
40
40
30
21
14
0
16
15
27
21
2
9
2
0
0
0
0
17
0
20
0
17
11
0
1
0
0
1
30
1
0
0
22
0
0
0
0
0
0
14
0
15
9

#
30
30
40
40
30
30
50
30
31
30
30
30
30
30
30
30
20
20
20
20
20
20
20
20
20
20
20
20
20
20
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30

998

960

927

877

869

835

823

837

630

505

436

1251

Table 16: Solved problems training phase. first part table results IPC2005, second part IPC-2008 IPC-2011 satisficing tracks. two last rows
(from Gold-miner tpp) IPC-2008-2011 learning track. last column
number problems included training.

686

fiT IBAC P P LANNING YSTEM

D. Appendix: Planners
following list set planners pre-selected candidates Pareto-dominance filtering
described Section 3.1
Arvand (Nakhost, Muller, Valenzano, & Xie, 2011): stochastic planner uses Monte
Carlo random walks balance exploration exploitation heuristic search. version
uses online learning algorithm find best configuration parameters given
problem.
Fast Downward Autotune-1 Fast Downward Autotune-2 (Fawcett, Helmert, Hoos, Karpas,
Roger, & Seipp, 2011): two instantiations FD planning system automatically configured performance wide range planning domains, using well-known ParamILS
configurator. planners use three main types search combination several heuristics.
Fast Downward Stone Soup-2 (Helmert et al., 2011) (FDSS-2): sequential portfolio
several search algorithms heuristics. Given results training benchmarks,
best combination algorithms heuristics found hill-climbing search. Here,
information communicated component solvers quality best
solution found far.
LAMA-2008 LAMA-2011 (Richter & Westphal, 2010; Richter, Westphal, & Helmert,
2011) propositional planner based combination landmark count heuristic
FF heuristic. search performs set weighted iteratively decreasing weights.
planner developed within FD Planning System (Helmert, 2006).
Lamar (Olsen & Bryce, 2011) modification LAMA planner includes randomized construction landmark count heuristic.
Madagascar (Rintanen, 2011): implements several innovations SAT planning, including
compact parallelized/interleaved search strategies SAT-based heuristics.
Probe (Lipovetzky & Geffner, 2011): exploits idea wisely constructed lookaheads
probes, action sequences computed without searching given state
quickly go deep state space, terminating either goal failure. technique
integrated within standard greedy best first search.
YAHSP2-MT (Vidal, 2011) extracts information relaxed plan order generate
lookahead states. strategy implemented complete best-first search algorithm,
modified take helpful actions account.
LPG-td (Gerevini et al., 2006) based stochastic local search space particular
action graphs derived planning problem specification.

References
Amadini, R., Gabbrielli, M., & Mauro, J. (2014a). Portfolio approaches constraint optimization
problems. TPLP, 8426, 2135.
687

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

Amadini, R., Gabbrielli, M., & Mauro, J. (2014b). SUNNY: lazy portfolio approach constraint
solving. TPLP, 14(4-5), 509524.
Blum, A. L., & Langley, P. (1997). Selection relevant features examples machine learning.
Artificial intelligence, 97(1), 245271.
Bonet, B., & Geffner, H. (2000). Planning heuristic search: New results. Recent Advances
AI Planning, pp. 360372. Springer.
Bonet, B., Loerincs, G., & Geffner, H. (1997). robust fast action selection mechanism
planning. AAAI/IAAI, pp. 714719.
Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123140.
Breiman, L. (2001). Random forests. Machine learning, 45(1), 532.
Cenamor, I., de la Rosa, T., & Fernandez, F. (2012). Mining IPC-2011 results. Proceedings
Third Workshop International Planning Competition - ICAPS.
Cenamor, I., de la Rosa, T., & Fernandez, F. (2013). Learning predictive models configure planning portfolios. Proceedings Workshop Planning Learning - ICAPS.
Censor, Y. (1977). Pareto optimality multiobjective problems. Applied Mathematics Optimization, 4(1), 4159.
Dietterich, T. G. (2000). Ensemble methods machine learning. Kittler, J., & Roli, F.
(Eds.), Multiple Classifier Systems, First International Workshop, MCS 2000, Cagliari, Italy,
June 21-23, 2000, Proceedings, Vol. 1857 Lecture Notes Computer Science, pp. 115.
Springer.
Domshlak, C., Hoffmann, J., & Katz, M. (2015). Red-black planning: new systematic approach
partial delete relaxation. Artificial Intelligence, 221, 73114.
Fawcett, C., Helmert, M., Hoos, H., Karpas, E., Roger, G., & Seipp, J. (2011). FD-Autotune:
Domain-specific configuration using fast-downward. Proceedings Workshop
Planning Learning - ICAPS, 2011(8).
Fawcett, C., Vallati, M., Hutter, F., Hoffmann, J., Hoos, H. H., & Leyton-Brown, K. (2014). Improved features runtime prediction domain-independent planners. Proceedings
24th International Conference Automated Planning Scheduling (ICAPS-14).
Gerevini, A., Saetti, A., & Vallati, M. (2009). automatically configurable portfolio-based planner
macro-actions: PbP. Proceedings 19th International Conference Automated
Planning Scheduling (ICAPS-09).
Gerevini, A., Saetti, A., & Serina, I. (2006). approach temporal planning scheduling
domains predictable exogenous events. Journal Artificial Intelligence Research, 25,
187231.
Gerevini, A., Saetti, A., & Vallati, M. (2014). Planning automatic portfolio configuration:
PbP approach. Journal Artificial Intelligence Research, 50, 639696.
Ghallab, M., Nau, D., & Traverso, P. (2004). Automated planning: theory & practice. Access Online
via Elsevier.
Gomes, C. P., & Selman, B. (2001). Algorithm portfolios. Artificial Intelligence, 126(1), 4362.
688

fiT IBAC P P LANNING YSTEM

Grabczewski, K., & Jankowski, N. (2005). Feature selection decision tree criterion. Proceedings Fifth International Conference Hybrid Intelligent Systems (HIS05), pp.
212217. IEEE.
Han, J., Kamber, M., & Pei, J. (2011). Data mining: concepts techniques. Elsevier.
Helmert, M. (2004). planning heuristic based causal graph analysis. Proceedings
14th International Conference Automated Planning Scheduling (ICAPS-04), Vol. 16,
pp. 161170.
Helmert, M. (2006). Fast Downward Planning System. Journal Artificial Intelligence Research, 26, 191246.
Helmert, M. (2009). Concise finite-domain representations PDDL planning tasks. Artificial
Intelligence, 173, 503535.
Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths abstractions: Whats difference anyway?. Proceedings 19th International Conference Automated
Planning Scheduling (ICAPS-09).
Helmert, M., & Geffner, H. (2008). Unifying causal graph additive heuristics. Proceedings 18th International Conference Automated Planning Scheduling (ICAPS08), pp. 140147.
Helmert, M., Roger, G., Seipp, J., Karpas, E., Hoffmann, J., Keyder, E., Nissim, R., Richter, S.,
& Westphal, M. (2011). Fast downward stone soup. Seventh International Planning
Competition, IPC-7 planner abstracts, 38.
Hoffmann, J. (2003). metric-FF planning system: Translating ignoring delete lists numeric
state variables. Journal Artificial Intelligence Research, 20, 291341.
Hoffmann, J. (2011). Analyzing search topology without running search: connection
causal graphs h+. Journal Artificial Intelligence Research, 41, 155229.
Hoffmann, J., Edelkamp, S., Thiebaux, S., Englert, R., dos Santos Liporace, F., & Trug, S. (2006).
Engineering benchmarks planning: domains used deterministic part IPC-4.
Journal Artificial Intelligence Research, 26, 453541.
Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation heuristic
search. Journal Artificial Intelligence Research, 14, 253302.
Hoos, H., Kaminski, R., Schaub, T., & Schneider, M. T. (2012). aspeed: ASP-based solver scheduling. ICLP (Technical Communications), 17, 176187.
Howe, A. E., Dahlman, E., Hansen, C., Scheetz, M., & von Mayrhauser, A. (1999). Exploiting
competitive planner performance. Biundo, S., & Fox, M. (Eds.), Recent Advances AI
Planning, 5th European Conference Planning, ECP99, Durham, UK, September 8-10,
1999, Proceedings, Vol. 1809 Lecture Notes Computer Science, pp. 6272. Springer.
Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2011). Sequential model-based optimization
general algorithm configuration. Learning Intelligent Optimization, pp. 507523.
Springer.
Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: automatic algorithm
configuration framework. Journal Artificial Intelligence Research, 36, 267306.
689

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

Hutter, F., Xu, L., Hoos, H., & Leyton-Brown, K. (2015). Algorithm runtime prediction: Methods
evaluation (extended abstract). Yang, Q., & Wooldridge, M. (Eds.), Proceedings
Twenty-Fourth International Joint Conference Artificial Intelligence, IJCAI 2015, Buenos
Aires, Argentina, July 25-31, 2015, pp. 41974201. AAAI Press.
Kohavi, R. (1995). power decision tables. Machine Learning: ECML-95, pp. 174189.
Springer.
Linares Lopez, C., Celorrio, S. J., & Olaya, A. G. (2015). deterministic part seventh
international planning competition. Artificial Intelligence, 223, 82119.
Lindauer, M. T., Hoos, H. H., & Hutter, F. (2015a). sequential algorithm selection parallel
portfolio selection. Dhaenens, C., Jourdan, L., & Marmion, M. (Eds.), Learning Intelligent Optimization - 9th International Conference, LION 9, Lille, France, January 12-15,
2015. Revised Selected Papers, Vol. 8994 Lecture Notes Computer Science, pp. 116.
Springer.
Lindauer, M. T., Hoos, H. H., Hutter, F., & Schaub, T. (2015b). Autofolio: automatically configured algorithm selector. Journal Artificial Intelligence Research, 53, 745778.
Lipovetzky, N., & Geffner, H. (2011). Searching plans carefully designed probes.
Proceedings 21st International Conference Automated Planning Scheduling
(ICAPS-11), pp. 154161.
Malitsky, Y., Sabharwal, A., Samulowitz, H., & Sellmann, M. (2013). Algorithm portfolios based
cost-sensitive hierarchical clustering. Proceedings Twenty-Third international
joint conference Artificial Intelligence, pp. 608614. AAAI Press.
Nakhost, H., Muller, M., Valenzano, R., & Xie, F. (2011). Arvand: art random walks.
Seventh International Planning Competition, IPC-7 planner abstracts, 1516.
Nebel, B. (2000). compilability expressive power propositional planning formalisms.
Journal Artificial Intelligence Research, 12, 271315.
Nunez, S., Borrajo, D., & Linares Lopez, C. (2015). Automatic construction optimal static
sequential portfolios AI planning beyond. Artificial Intelligence, 226, 75101.
Olsen, A., & Bryce, D. (2011). Randward Lamar: Randomizing FF heuristic. Seventh
International Planning Competition, IPC-7 planner abstracts, 55.
OMahony, M. (1986). Sensory evaluation food: statistical methods procedures, Vol. 16.
CRC Press.
Quinlan, J. R. (1993). C4. 5: programs machine learning, Vol. 1. Morgan kaufmann.
Richter, S., Helmert, M., & Westphal, M. (2008). Landmarks revisited. AAAI, Vol. 8, pp. 975
982.
Richter, S., & Westphal, M. (2010). LAMA planner: Guiding cost-based anytime planning
landmarks. Journal Artificial Intelligence Research, 39(1), 127177.
Richter, S., Westphal, M., & Helmert, M. (2011). Lama 2008 2011. Seventh International
Planning Competition, IPC-7 planner abstracts, 50.
Rintanen, J. (2011). Madagascar: Efficient planning SAT. Seventh International Planning
Competition, IPC-7 planner abstracts, 61.
690

fiT IBAC P P LANNING YSTEM

Roberts, M., & Howe, A. (2009). Learning planner performance. Artificial Intelligence, 173,
536561.
Roberts, M., Howe, A. E., Wilson, B., & desJardins, M. (2008). makes planners predictable?.
Proceedings 18th International Conference Automated Planning Scheduling (ICAPS-08), pp. 288295.
Rodriguez, J. J., Kuncheva, L. I., & Alonso, C. J. (2006). Rotation forest: new classifier ensemble
method. IEEE Transactions Pattern Analysis Machine Intelligence, 28(10), 1619
1630.
Schwefel, H.-P., Wegener, I., & Weinert, K. (2013). Advances computational intelligence: Theory
practice. Springer Science & Business Media.
Seipp, J., Braun, M., Garimort, J., & Helmert, M. (2012). Learning portfolios automatically tuned
planners. McCluskey, L., Williams, B., Silva, J. R., & Bonet, B. (Eds.), Proceedings
Twenty-Second International Conference Automated Planning Scheduling, ICAPS
2012, Atibaia, Sao Paulo, Brazil, June 25-19, 2012. AAAI.
Seipp, J., Sievers, S., Helmert, M., & Hutter, F. (2015). Automatic configuration sequential
planning portfolios. Bonet, B., & Koenig, S. (Eds.), Proceedings Twenty-Ninth
AAAI Conference Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., pp.
33643370. AAAI Press.
Vallati, M. (2012). guide portfolio-based planning. Multi-disciplinary Trends Artificial
Intelligence, pp. 5768. Springer.
Vallati, M., Chrpa, L., & Kitchin, D. E. (2015). Portfolio-based planning: State art, common
practice open challenges. AI Communications, 29, 117.
Vallati, M., Chrpa, L., & McMcluskey, L. (2014a).
https://helios.hud.ac.uk/scommv/IPC-14/benchmark.html.

Competition

Domains.

Vallati, M., Chrpa, L., & McMcluskey, L. (2014b). Source code Erratum Deterministic part.
https://helios.hud.ac.uk/scommv/IPC-14/errPlan.html.
Vidal, V. (2011). YAHSP2: Keep simple, stupid. Seventh International Planning Competition,
IPC-7 planner abstracts, 83.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical Machine Learning Tools Techniques.
2nd Edition, Morgan Kaufmann.
Xie, F., Muller, M., & Holte, R. (2014). Jasper: art exploration greedy best first search.
Planner abstracts. IPC-2014.
Xu, L., Hoos, H., & Leyton-Brown, K. (2010). Hydra: Automatically configuring algorithms
portfolio-based selection. Proceedings Twenty-Fourth AAAI Conference Artificial
Intelligence (AAAI 2010), pp. 210216.
Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2012). Evaluating component solver contributions portfolio-based algorithm selectors. Theory Applications Satisfiability
TestingSAT 2012, pp. 228241. Springer.
Xu, L., Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2008). SATzilla: Portfolio-based algorithm
selection SAT. Journal Artificial Intelligence Research, 32, 565606.

691

fiJournal Artificial Intelligence Research 56 (2016) 329-378

Submitted 12/15; published 06/16

DL-Lite Contraction Revision
Zhiqiang Zhuang

z.zhuang@griffith.edu.au

Institute Integrated Intelligent Systems
Griffith University, Australia

Zhe Wang

zhe.wang@griffith.edu.au

School Information Communication Technology
Griffith University, Australia

Kewen Wang

k.wang@griffith.edu.au

School Information Communication Technology
Griffith University, Australia

Guilin Qi

gqi@seu.edu.cn

School Computer Science Engineering
Southeast University, China
State Key Lab Novel Software Technology
Nanjing University, China

Abstract
Two essential tasks managing description logic knowledge bases eliminating problematic axioms incorporating newly formed ones. elimination incorporation
formalised operations contraction revision belief change. paper,
deal contraction revision DL-Lite family model-theoretic
approach. Standard description logic semantics yields infinite number models
DL-Lite knowledge bases, thus difficult develop algorithms contraction revision involve DL models. key approach introduction alternative
semantics called type semantics replace standard semantics characterising
standard inference tasks DL-Lite. Type semantics several advantages
standard one. succinct importantly, finite signature, semantics
always yields finite number models. define model-based contraction
revision functions DL-Lite knowledge bases type semantics provide representation theorems them. Finally, finiteness succinctness type semantics allow
us develop tractable algorithms instantiating functions.

1. Introduction
Description logic (DL) (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003)
knowledge bases (KBs) subject frequent change. instance, outdated incorrect
axioms eliminated KBs newly formed ones incorporated
them. Therefore mandatory task managing DL KBs deal changes.
field belief change, extensive work done formalising various kinds
changes KBs. particular, elimination old knowledge called contraction
incorporation new knowledge called revision. deal changes DL KBs,
makes sense take advantage existing techniques belief change. fact, many
investigated contraction revision DL KBs (DL contraction DL revision
c
2016
AI Access Foundation. rights reserved.

fiZhuang, Wang, Wang, & Qi

short) (Qi et al., 2006; Qi & Du, 2009; Qi et al., 2008; Ribeiro & Wassermann, 2009;
Wang et al., 2015).
dominant approach belief change called AGM framework (Alchourron,
Gardenfors, & Makinson, 1985; Gardenfors, 1988). framework, KB
changes made called belief set logically closed set formulas. AGM
.
contraction function takes input belief set K formula returns output
.
belief set K entail . Taking inputs, AGM revision function
returns belief set K entails . framework also provides rationality postulates
capture intuitions behind rational contraction revision. hallmark
framework called representation theorems proved ensure AGM
contraction revision functions sound also complete respect
rationality postulates.
Regardless wide acceptance, limitation AGM framework
minimal requirement underlying logic, logic subsumes classical propositional
logic. means underlying logic must fully support truth functional logical
connectives negation disjunction. Since DLs so, AGM framework incompatible DLs cannot applied directly deal changes
DLs KBs. incompatibility major difficulty defining DL contraction revision. Additionally, DL revision involved AGM revision. AGM revision aims
resolve inconsistency caused incorporating new formula. Since meaningful DL KB consistent coherent (i.e, absence unsatisfiable concepts),
DL revision resolve inconsistency also incoherence. Finally, despite
mathematical elegance, AGM framework grappled issue computational efficiency crucially important DL KBs. Therefore, DL contraction
revision lead tractable instantiations time respecting
rationality postulates AGM contraction revision.
Due many difficulties, existing works DL contraction revision fully
satisfactory. None provides representation theorem contraction revision
function except work Ribeiro Wassermann (2009) inherits representation results general work (Hansson & Wassermann, 2002).1 defining DL
revision, many appreciate incoherence resolving nature, revisions cannot
guarantee coherence outputs (Qi et al., 2006; Ribeiro & Wassermann, 2009; Wang
et al., 2015). Qi Du (2009) appreciate incoherence resolving nature, postulates provided capturing properties revision function formulated
appropriately capture rationales behind incoherence resolving.
paper, provide DL contraction revision overcome limitations.
Specifically, define contraction revision functions logically closed DL-Litecore
DL-LiteR KBs. DL-Litecore DL-LiteR main languages DL-Lite family
(Calvanese et al., 2007). defining functions take model-theoretic approach
1. Hansson Wassermann (2002) proved series representation theorems contraction revision
functions monotonic compact logics. Since Ribeiro Wassermann (2009) considered
contraction revision functions DLs monotonic compact, representation
results Hansson Wassermann (2002) also hold contraction revision functions. Since
approach defining contraction revision different Hansson Wassermann (2002),
inherit representation results prove scratch.

330

fiDL-Lite Contraction Revision

similar Katsuno Mendelzon (1992). Instead DL models functions
based models newly defined semantics DL-Lite called type semantics. Given
type semantics equivalent DL semantics respect major DL-Lite reasoning
tasks, models type semantics (i.e., type models) succinct DL models.
importantly, given finite signature, DL-Lite KB finite number type models,
whereas usually infinite number DL models.
fully appreciate incoherence resolving nature DL revision reflect
definition revision functions postulates capturing properties.
able prove AGM-style representation theorems contraction revision
functions. theorems crucial guarantee functions defined
method behave properly (in sense satisfying set commonly accepted postulates)
properly behaved functions defined method. addition
rigorous mathematical properties, provide tractable algorithms instantiating
contraction revision functions.
material paper presented previously Zhuang, Wang, Wang,
Qi (2014).

2. DL-Lite
DL-Litecore core language DL-Lite family. following syntax
B | R

R P | P

C B | B

E R | R

denotes atomic concept; P atomic role, P inverse atomic role P ; B
basic concept either atomic concept unqualified existential quantification;
C general concept either basic concept negation; R basic role
either atomic role inverse; E general role either basic role
negation. also include language nullary predicates > denote
universal false universal truth respectively. assume set basic concepts,
denoted B, set basic roles, denoted R, finite. inverse role
R = P , write R represent P .
DL-Litecore KB consists TBox ABox. sometime denote KB
(T , A) TBox KB ABox KB. TBox finite set
concept inclusions form B v C, B v , > v C. basic concept
> appear left-hand side concept inclusion. ABox finite set
concept assertions form A(a) role assertions form P (a, b),
atomic concept, P atomic role, a, b individuals. assume set individuals,
denoted D, finite. Throughout paper, individual names denoted lower case
Roma letters (a, b, . . .).
major extension DL-Litecore DL-LiteR . extends DL-Litecore role inclusions form R v E. basic roles appear left-hand side role
inclusion. concept role inclusion also called TBox axiom concept role
assertion also called ABox axiom. Throughout paper, TBox ABox axioms
denoted lower case Greek letters (, , . . .).
semantics DL-Lite language given terms interpretations. interpretation = (I , ) consists nonempty domain interpretation function
331

fiZhuang, Wang, Wang, & Qi

assigns atomic concept subset AI , atomic role P binary
relation P , individual element aI . interpretation
function extended general concept, general roles, special symbols follows:
=
>I =
(P )I = {(b, a) | (a, b) P }
(R)I = {a | b.(a, b) RI }
(B)I = \ B
(R)I = \ RI
set interpretations denoted . interpretation satisfies concept
inclusion B v C B C , role inclusion R v E RI E , concept assertion A(a)
aI AI , role assertion P (a, b) (aI , bI ) P .
satisfies KB K (a TBox ABox A) satisfies axioms K (resp. ,
A). model KB K (a TBox , ABox A, axiom ) written |= K, (resp.
|= , |= A, |= ) satisfies K (resp. , A, ). denote models
KB K (a TBox , ABox A, axiom ) |K| (resp. |T |, |A|, ||). Two axioms
logically equivalent, written , identical set models.
KB K (a TBox , ABox A, axiom ) entails axiom , written K |= (resp.
|= , |= , |= ), models K (resp. , A, ) also models . KB
(a TBox, ABox axiom) consistent least one model inconsistent
otherwise. use K , , denote respectively (unique) inconsistent KB, TBox,
ABox. use |= denote tautology (e.g., v A) 6|=
one.
closure TBox , denoted cl(T ), set TBox axioms
|= . say TBox closed = cl(T ). closure DL-Lite TBox
finite. Actually, size cl(T ) quadratic respect size (Pan & Thomas,
2007). closure ABox respect TBox , denoted clT (A), set
ABox axioms (T , A) |= . say ABox closed respect
= clT (A). closure ABox respect TBox DL-Lite finite
computed efficiently chase procedure (Calvanese et al., 2007). Section
4 Section 5, TBoxes ABoxes assumed closed.
basic concept B satisfiable respect TBox model
B non-empty unsatisfiable otherwise. easy see B unsatisfiable
respect B v cl(T ). TBox coherent basic concepts
satisfiable incoherent otherwise.2
defining contraction revision functions DL-Lite KBs, need refer
notion conjunction axioms. Given set axioms {1 , . . . , n }, conjunction
denoted 1 n . interpretation model 1 n satisfies
conjuncts |1 n | = |1 | |n |.
2. DL literatures, often coherence comes absence unsatisfiable atomic concepts. Since
DL-Lite unsatisfiable non-atomic concepts like R also unexpected use stricter condition
coherence.

332

fiDL-Lite Contraction Revision

3. Type Semantics
section, provide alternative semantics DL-Lite, namely type semantics.
short, type semantics takes semantics underlying propositional logic (i.e., propositional semantics) basis equips extra facilities take care nonpropositional behaviours DL-Lite.
first introduce simplified version type semantics called ct-type semantics (ct
stands Core TBox), sufficient characterising standard inference tasks
DL-Litecore TBoxes. important consideration defining semantics succinctness,
defined semantics avoid redundancy. consideration, cttype semantics facility required characterising inference
tasks DL-Litecore TBoxes. Accordingly, DL-Litecore TBoxes allow inferences
involve role inclusions ABox axioms, ct-type semantics incapable characterising
inferences. Next, extend ct-type semantics facilities role inclusions
results another simplified version type semantics called t-type semantics (t
stands TBox). semantics sufficient characterising standard inference
tasks DL-LiteR TBoxes. Again, sake succinctness, t-type semantics intended
capture inference tasks DL-LiteR TBoxes only, thus incapable characterising
involving ABox axioms. inferences, introduce a-type semantics (a
stands ABox). semantics sufficient DL-LiteR ABoxes (with background
TBox). also simplified version type semantics, built upon ct-type
t-type semantics. Finally, introduce full version type semantics, sufficient
DL-LiteR KBs. semantics includes facilities t-type a-type semantics.
Figure 1 shows hierarchy type semantics.

type
t-type
ct-type

a-type

Figure 1: rectangle represents semantics, largest representing type semantics. rectangle containing one smaller ones indicates represented
semantics larger rectangle subsumes smaller ones.

333

fiZhuang, Wang, Wang, & Qi

propositional origin assumption finite signature guarantee finiteness
type semantics. mentioned, important consideration defining type semantics
succinctness. succinct semantics efficient computations involving models. Type semantics replace ct-type semantics, t-type semantics, a-type
semantics characterising DL-Litecore TBoxes, DL-LiteR TBoxes, DL-LiteR ABoxes;
t-type semantics replace ct-type semantics characterising DL-Litecore TBoxes. However, waste computational power use type semantics characterise
instance DL-LiteR TBoxes facilities ABox axioms redundant DL-LiteR
TBoxes. holds using t-type semantics characterising DL-Litecore TBoxes
facilities t-type semantics role inclusions redundant DL-Litecore TBoxes.
finiteness succinctness significant advantages type semantics DL semantics DL-Lite KBs need represented model-theoretically related
computational tasks involve models.
Depending application scenarios, changes DL-Lite KBs may applied
(1) whole KB restricted either (2) TBox (3) ABox background
(unchanged) TBox. take model-theoretic approach addressing changes,
suitable semantics scenario (1) type semantics; scenario (2) ct-type
t-type semantics; scenario (3) a-type semantics.
3.1 Characterising DL-Litecore TBoxes
start ct-type semantics. Standard inference tasks DL-Lite TBoxes
checking satisfiability, subsumption, equivalence, disjointness, consistency reduced checking whether entailment relationship holds TBox
axioms. Given TBox , basic concept B satisfiable
entail B v ; subsumed B entails v B; B
equivalent entails v B B v A; disjoint B
entails v B; inconsistent |= > v . reason,
defining ct-type (t-type) semantics, suffices focus entailment relationships
DL-Litecore (resp. DL-LiteR ) TBox axioms.
propositional semantics standard DL semantics, notion interpretations. Analogously, type semantics, central notion types.3 definition
type given Section 3.4. ct-type semantics, need simplified version,
called ct-type. ct-type possibly empty set basic concepts (i.e., B).
example, B = {R, R , A}, {R, A} ct-type simplicity sometimes
write RA.4 denote set ct-types tc . consider basic concepts
propositional atoms, concept inclusion B v C propositional formula B C,
ct-type nothing interpretation (represented atoms interpreted true) propositional logic. Given DL-Litecore TBox , use kT ktc denote set propositional
models corresponding propositional formulas . Note kT ktc tc .
Many entailment relationships DL-Litecore TBox axioms propositional
sense entailments also hold treat axioms propositional formulas
3. notion types mentioned work Kontchakov et al. (2010), similar structures
ct-types paper cannot capture role inclusions ABox axioms.
4. work DL-Lite throughout paper. Since DL-Lite allow quantified existential quantifications R.C, ct-type RA cannot confused concept R.A

334

fiDL-Lite Contraction Revision

consider propositional semantics. example, entailment v B B v C
v C also holds corresponding propositional formulas B, B C,
C, propositional semantics. expected entailments
propositional. following example shows common pattern non-propositional
entailments. Note that, R v R v entail one another but, propositional
semantics, corresponding propositional formulas R R not. reason
simple. DLs, role R represents binary relation axiom R v
R v indicate relation empty. Propositional logic facility
binary relations entailments involving relations, thus characterise
entailments.
clear, ct-type semantics, need facilities propositional semantics
characterise propositional entailments extra one characterise nonpropositional ones. capture facilities conditions ct-type
satisfies DL-Litecore TBox. DL semantics, interpretations satisfying TBox called
models TBox. Analogously, ct-type satisfying DL-Litecore TBox called ctmodel TBox.
Definition 1 ct-model DL-Litecore TBox ct-type
1. kT ktc
2. |= R v R 6 .
ct-type satisfy TBox , firstly propositional model secondly
entails R v , contain basic concept R. first condition
guarantees proper handling propositional entailments second guarantees
non-propositional entailments.
Example 1 Consider fragment (slightly modified) NCI KB concerning heart diseases
associated anatomic locations, consists concepts Heart Disease (HD),
Cardiovascular System (CS), Respiratory System (RS), Organ System (OS), well
role relates diseases primary locations Disease Primary Anatomic Site
(Loc). Let DL-Litecore TBox consist following concept inclusions: HD v Loc,
Loc v CS, HD v OS, RS v OS, CS v OS, RS v CS.
ct-models 1 = {HD, Loc}, 2 = {Loc , CS, OS}, 3 = {RS, OS}.
concept inclusion Loc v RS , |= Loc v |= Loc v ,
neither 1 2 ct-model .
denote set ct-models TBox |T |tc . ct-models conjunction
axioms 1 2 n , denoted |1 2 n |tc , defined
|1 2 n |tc = |{1 , 2 , . . . , n }|tc .
ct-models negated (conjunctions of) axiom(s) , denoted ||tc , defined
tc \ ||tc .
335

fiZhuang, Wang, Wang, & Qi

notions entailment, logical equivalence, consistency ct-type semantics
defined manner DL semantics. ct-type semantics, TBox entailing
conjunction axioms written |=tc .
make non-propositional behaviour ct-type semantics explicit, propose
following notion role-complete set ct-types. set ct-types role-complete if,
R R, whenever ct-type R , ct-type 0
R 0 ( 0 may identical). Roughly speaking, role-completeness
indicates set ct-types complete information role R.
show set ct-models DL-Litecore TBox role-complete.
Proposition 1 Let DL-Litecore TBox. |T |tc role-complete.
show connections DL models ct-models DLLitecore TBox. Let DL interpretation. element domain I,
induces unique ct-type follows
(I, d) = {B B | B }.
call (I, d) ct-type induced I. model TBox
ct-type induced ct-model TBox.
Proposition 2 Let DL-Litecore TBox DL interpretation. |T | iff
(I, d) |T |tc .
Moreover, ct-model TBox, DL model TBox constructed
reversing inducing process.
Proposition 3 Let DL-Litecore TBox ct-model . |T |
(I, d) = .
connections, prove entailments DL-Litecore TBoxes
axioms induced ct-type semantics identical induced DL semantics.
Theorem 1 Let DL-Litecore TBox conjunction DL-Litecore TBox axioms.
|= iff |=tc .
Thus ct-type semantics effective DL semantics characterising standard inferences tasks DL-Litecore TBoxes. comparison DL semantics, ct-type semantics
clear advantage finite succinct. DL-Litecore TBox usually
infinite number DL models always finite number ct-models.
Proposition 4 Let DL-Litecore TBox. 2n ct-models, n
number basic concepts.
working coherent TBox , ct-type semantics shares one
property DL semantics, set ct-models identical intersection
set ct-models axiom . property turns essential
developing algorithms eliminating incorporating axioms DL-Lite KBs. allows
us deal axiom one one model-theoretic setting.
336

fiDL-Lite Contraction Revision

Theorem 2 Let DL-Litecore TBox = {1 , . . . , n }. coherent,
|T |tc = |1 |tc |n |tc .
shown, ct-type semantics shares many crucial properties DL semantics,
however differs DL one dealing unions axioms.
Theorem 3 Let DL-Litecore TBox , DL-Litecore TBox axioms. |T |
|| || |T |tc ||tc ||tc converse necessarily hold.
counter example, suppose B {A, B, C, D}, {A v D}, v B,
C v D. Lets work ct-models. ct-type satisfy v
contains D, get ct-models eliminating unsatisfying
ct-types set ct-types, |T |tc = c \ {AB, AC, ABC}. Similarly, ct-type
satisfy v B C v contains C B D,
||tc ||tc = c \ {AC}. Note |T |tc ||tc ||tc . let DL interpretation
= {a, b}, AI = {a}, B = {b}, C = {b}, DI = {a}. Since |= ,
6|= v B, 6|= C v D, |T | 6 ||||. Roughly speaking, reason
behaviour type semantics (and simplified versions) variant propositional
semantics lacks first-order structure DL semantics. Identification
behaviour turns crucial proving representation theorem contraction
functions.
DLs inexpressibility problem sets DL interpretations
syntactic representation. exception DL-Litecore ct-type semantics. Given
set ct-types , may DL-Litecore TBox whose set ct-models .
cases, define corresponding DL-Litecore TBox one
minimal set ct-models including .
Definition 2 Let set ct-types. DL-Litecore TBox corresponding DL0
Litecore TBox iff |T |tc DL-Litecore TBox
0
|T |tc |T |tc .
Given set ct-types, may several corresponding TBoxes. Let B consists
{R, R , A} set ct-types consists A, , R. Note
ct-type contains R none contains R . Proposition 1,
TBox whose set ct-models contains must ct-model contains R . Since,
current set basic concepts B, four ct-types containing R
R , R A, R R, R AR, four corresponding TBoxes
{A v R, v R , R v R }, {A v R, R v A, R v R }, {A v R, v
R , R v R}, {R v A, R v R}, one
ct-types ct-models.
shown example, R ct-types R not,
several ways generating corresponding TBox. Intuitively concepts,
dont many choices one generating corresponding TBox. Clearly,
role-complete show role-completeness sufficient guarantee
uniqueness corresponding TBox.
Theorem 4 Let set ct-types. role-complete, unique
corresponding DL-Litecore TBox .
337

fiZhuang, Wang, Wang, & Qi

3.2 Characterising DL-LiteR TBoxes
Ct-type semantics able characterise entailments DL-Litecore TBox axioms,
DL-LiteR ones, involve role inclusions. subsection,
present t-type semantics able so.
characterise entailments involving role inclusions, need introduce copy
B 0 set basic concepts B copy R0 set basic roles R. So,
B = {A, R, R } R = {R, R }, B 0 = {A0 , (R)0 , (R )0 } R0 = {R0 , (R )0 }.
also need notion extension DL-LiteR TBox. Let DL-LiteR TBox.
extension, denoted , TBox obtained adding new concept
inclusion concept inclusion B v C new role inclusion R0 v E 0
role inclusion R v E . Note C = B, C 0 denotes B 0 ; E = R,
E 0 denotes R0 .
t-type possibly empty set basic concepts, basic roles, copies (i.e.,
B R B 0 R0 ). Intuitively, t-type combines two ct-types (for pairs individuals)
set roles (between pairs individuals). pair individuals a, b
(a, b) relation captured role R, B part t-type aims characterise
constraints (in way ct-type characterises constraints
individual); B 0 part aims characterises constraints b (in way
ct-type characterises constraints individual); R R0 part aims
characterise constraints R (which ct-type consider). denote
set t-types tr .
consider basic concepts, basic roles, copies propositional atoms,
concept inclusion B v C role inclusion R v E propositional formulas B C
R E, t-type nothing interpretation (represented atoms interpreted
true) propositional logic. DL-LiteR TBox , use kT ktr denote set
propositional models corresponding propositional formulas . Note kT ktr
tr .
DL-Litecore permits non-propositional entailments, DL-LiteR .
one group non-propositional entailments DL-Litecore , four identified
DL-LiteR . (1) Apart implying R v , R v also implies R v R R v
R . (2) role inclusion R v implies concept inclusion R v S, R v ,
role inclusion R v . (3) role inclusion R v implies R v . (4)
concept inclusion R v implies role inclusion R v S.
following, give conditions t-type satisfies DL-LiteR TBox
. call t-types t-models TBox.
Definition 3 t-model DL-LiteR TBox t-type
1. kT ktr ;
2. |= R v R 6 (R)0 6 ;
3. |= R v R implies , (R)0 implies (S)0 ;
4. R R (R )0 ;
5. R iff (R )0 R R.
338

fiDL-Lite Contraction Revision

t-type satisfy , firstly propositional model , takes care
propositional entailments. conditions 25 take care four groups nonpropositional entailments summarised earlier. Conditions 4 5 required t-type
t-model (independent TBox), referred model conditions
t-type semantics. Note use copies basic concepts basic roles necessary.
Consider TBox two axioms R v v . entails R v
R v (by (3) (4)). would expect t-models also satisfy R v S. copies
discarded, t-type = {R, S, R, S} would t-model (omitting R0 (R)0
Definition 3), yet clearly satisfy R v S. cannot resolved adding
condition Definition 3 (for details refer proof Lemma 7 Appendix B).
Example 2 (contd Example 1) Consider another role associates diseases
anatomic sites, Disease Associated Anatomic Site (Das), DL-LiteR TBox obtained adding role inclusion Loc v Das .
t-models 10 = {HD, Loc, Das, Loc, Das, (Loc )0 , (Das )0 , (Loc )0 , (Das )0 ,
(CS)0 , (OS)0 }, 20 = {Loc , Das , CS, OS, Das , (Das)0 , (Das)0 }, 30 = {RS, OS}.
Given DL-LiteR TBox , denote set t-models |T |tr . t-models
conjunction DL-LiteR TBox axioms denoted defined manner
ct-type semantics. t-models negated (conjunction of) axiom(s) ,5 denoted
||tr , defined
{ tr \ ||tr | satisfies model conditions}.
notions entailment, logical equivalence, consistency t-type semantics
defined manner DL semantics. t-type semantics, TBox entailing
conjunction axioms written |=tr .
DL-Litecore , establish connection DL models t-models
DL-LiteR TBox. Let DL interpretation d, e pair (not necessarily
distinct) elements domain I. I, e induce t-type follows
(I, d, e) ={B B | B } {R R | (d, e) RI }
{B 0 B 0 | e B } {R0 R0 | (e, d) RI }.
call (I, d, e) t-type induced e I. show DL
interpretation I, DL model TBox t-type induced
t-model TBox.
Proposition 5 Let DL-LiteR TBox DL interpretation. |T |
iff (I, d, e) |T |tr pair d, e .
Moreover, given t-model TBox , DL model constructed
reversing inducing process.
5. simplicity, assume Definition 3 apply tautologies define, tautological
axiom , ||tr = tr ||tr = .

339

fiZhuang, Wang, Wang, & Qi

Proposition 6 Let DL-LiteR TBox t-model . |T |
d, e (I, d, e) = .
connections, prove t-type semantics induces set
entailments DL-LiteR TBox axioms induced DL semantics.
Theorem 5 Let DL-LiteR TBox conjunction DL-LiteR TBox axioms.
|= iff |=tr .
Extended roles copies basic concepts, number t-types
ct-types. However, compared DL semantics, t-type semantics still
advantage finite succinct.
Proposition 7 Let DL-LiteR TBox. 22n+2m t-models, n
number basic concepts basic roles.
sets ct-types, proposed condition called role-complete. condition characterises property ct-type semantics set ct-models DL-Litecore
TBox role-complete, role-complete set ct-types corresponds unique DLLitecore TBox. give corresponding role-complete condition sets t-types.
set t-types role-complete t-types satisfy model conditions
t-type semantics, R R, whenever t-type R
(R)0 , t-type 0 {R, R0 } 0 6= ( 0 may
identical).
set t-types , corresponding DL-LiteR TBoxes defined
way set ct-types. Also shown analogously |T |tr role-complete
DL-LiteR TBox role-complete guarantees existence unique
corresponding DL-LiteR TBox.
Theorem 6 Let set t-types. role-complete, unique
corresponding DL-LiteR TBox .
Moreover, properties ct-type semantics conjunctions unions axioms (i.e., Theorem 2 Theorem 3) also hold t-type semantics.
far shown t-type semantics possesses every property ct-type semantics,
except number possible models. connections? fact, t-type
semantics generalises ct-type semantics sense DL-Litecore TBox ,
ct-models exactly B-projections t-models .
Proposition 8 Let DL-Litecore TBox. |T |tc = { B | |T |tr }.
Hence, t-types contain information ct-types, enough
capture semantics DL-Litecore TBoxes.
Finally, extend notion coherence sets t-types. set t-types
coherent t-types satisfy model conditions t-type semantics,
satisfy B v R v R B B R R (i.e.,
6 |B v |tr 6 |R v R|tr ). coherent set t-types, every basic
340

fiDL-Lite Contraction Revision

concept role contained t-types . Therefore coherent set t-types
always role-complete. clear that, defining contraction revision functions
DL-LiteR TBoxes, sets t-types encounter always coherent thus
role-complete means always unique corresponding TBoxes. let Tr
function takes input set t-types coherent,
Tr (M ) closure corresponding DL-LiteR TBox, otherwise Tr (M ) = .
3.3 Characterising DL-LiteR ABoxes (with Background TBox)
T-type semantics extends ct-type semantics ability characterising entailments
involving role inclusions. them, however, incapable characterising entailments
ABox axioms. subsection, introduce a-type semantics able
so. TBoxes, also reduce standard inferences tasks ABoxes
checking entailment relationships ABox axioms. Thus defining a-type
semantics, suffices focus entailment relationships.
Although focus entailments ABox axioms, important note
entailments induced background TBox. example, A(a) entails B(a),
must background TBox entails v B. sake simplicity,
sometimes denote ABox indicate background TBox .
TBox captures subsumption relationships concepts (i.e., concept inclusions)
roles (i.e., role inclusions) whereas ABox captures assertions
individuals (i.e., concept assertions) pairs individuals (i.e., role assertions).
ABox, individual asserted element basic concept pair
individuals asserted element basic role. end, introduce
copy B B copy Rab R pair a, b D. So,
B = {A, R, R }, R = {R, R }, = {a, b}, B = {Aa , Ra , (R )a }, B b =
{Ab , Rb , (R )b }, Rab = {Rab , (R )ab }, Rba = {Rba , (R )ba }, Raa = {Raa , (R )aa },
Rbb = {Rbb , (R )bb }.
empty set copies basic concepts
a-type
possibly

ab
roles (i.e., aD B a,bD R ). denote set a-types ar .
DL-LiteR TBox , let TBox consists concept
inclusion B v C individual D, concept inclusion B v C ,
role inclusion R v E pair individuals a, b D, role inclusion Rab v E ab .
consider copies basic concepts roles propositional atoms, concept inclusion
B v C propositional formula B C, role inclusion R v E propositional formula
R E, concept assertion A(a) propositional formula Aa , (i.e., atomic formula)
role inclusion P (a, b) propositional formula P ab , (i.e., atomic formula) a-type
nothing interpretation (represented atoms interpreted true) propositional
logic. DL-LiteR ABox use kAT kar denote set propositional models
corresponding propositional formulas A. Note kAT kar ar .
Since entailments axioms ABox lot axioms
background TBox , embed facilities t-type semantics6 a-type
semantics. considerations, conditions a-type satisfies
DL-LiteR ABox defined follows call a-types a-models ABox.
6. Note conditions 15 Definition 4 adapted Definition 3.

341

fiZhuang, Wang, Wang, & Qi

Definition 4 a-model DL-LiteR ABox a-type
1. kAT kar ;
2. |= R v (R)a 6 D;
3. |= R v (R)a implies (S)a D;
4. Rab (R)a (R )b ;
5. Rab iff (R )ba R R pair a, b D.
Similarly, conditions 4 5 referred model conditions a-type semantics.
Example 3 (contd Example 2) Consider DL-LiteR KB K = (T , A) consists
concept assertion HD(d) role assertion Loc(d, s). a-model 00 =
{HDd , (Loc)d , (Das)d , Locds , Dasds , (Loc )sd , (Das )sd , (Loc )s , (Das )s , CSs , OSs }.
denote set a-models DL-LiteR ABox |AT |ar . a-models
conjunction 1 2 n DL-LiteR ABox axioms respect background TBox
, denoted |(1 2 n )T |ar , defined
|(1 2 n )T |ar = |{1 , 2 , . . . , n }T |ar .
a-models negated (conjunction of) axiom(s) respect background TBox
, denoted |T |ar , defined
|T |ar \ |T |ar
|T |ar set a-models empty ABox background TBox .
notions entailment, logical equivalence, consistency a-type semantics
defined manner DL semantics. a-type semantics, ABox entailing
conjunction ABox axioms written |=ar .
establish connection DL models a-models DL-LiteR
ABox, connection tighter one. Compared ct-type t-type semantics, atype semantics contains information individuals, thus closer DL interpretation.
reason, DL interpretation induces exactly one a-type. inducing process
follows.
(I) ={B c B c | c D, cI B } {Rcb Rcb | c, b D, (cI , bI ) RI }.
call (I) a-type induced I. show DL interpretation I,
model KB K = (T , A) model a-type induced
a-model ABox .
Proposition 9 Let K = (T , A) DL-LiteR KB DL interpretation.
|K| iff |T | (I) |AT |ar .
Moreover, given a-model ABox , DL model KB K = (T , A)
constructed reversing inducing process.
342

fiDL-Lite Contraction Revision

Proposition 10 Let K = (T , A) DL-LiteR KB a-type. |AT |ar ,
|K| (I) = .
connections, show a-type semantics induces set
entailments DL-LiteR ABox axioms (with background TBox) induced DL
semantics.
Theorem 7 Let K = (T , A) DL-LiteR KB, conjunction DL-LiteR ABox axioms.
K |= iff |=ar .
Although include multiple copies basic concepts roles capture DL-LiteR
ABoxes, semantics still superior DL semantics terms finiteness
succinctness.
2

Proposition 11 Let DL-LiteR ABox. 2nm+n l a-models,
n number individuals, basic concepts, l basic roles.
Moreover properties ct-type t-type semantics conjunctions unions TBox
axioms also hold a-type semantics ABox axioms.
important note that, TBoxes 0 equivalent, amodels may different 0 (i.e., |AT |ar 6= |AT 0 |ar ). Thus identify
corresponding ABoxes set a-types fix background TBox.
Definition 5 Let DL-LiteR TBox set a-types. corresponding
DL-LiteR ABox respect DL-LiteR ABox |AT |ar
DL-LiteR ABox A0 |A0T |ar |AT |ar .
Note ABox empty set, set a-models set
a-types, still background TBox restrict set satisfying a-types.
set a-types DL-LiteR TBox , say consistent a-types
a-models empty ABox background TBox (i.e., |T |ar ).
show set a-types , consistency background TBox ensures
existence uniqueness corresponding DL-LiteR ABox respect .
Theorem 8 Let DL-LiteR TBox set a-types. consistent
, unique corresponding DL-LiteR ABox respect .
let ATr function takes input set a-types
consistent , ATr (M ) closure corresponding DL-LiteR ABox
respect , otherwise ATr (M ) inconsistent ABox .
3.4 Characterising DL-LiteR KBs
shown t-type a-type semantics capable characterising respectively
entailments TBox KB ABox. Intuitively, combining
two characterise entailments KB. fact full version type
semantics defined.
343

fiZhuang, Wang, Wang, & Qi

0
Recall t-type subset
R R0 B 0 R0 copies B
B Ba
R a-type subset aD B a,bD Rab B Rab copies
B R individual pair individuals
a, b. AStype union
pair t-type a-type, B B 0 R R0 aD B a,bD Rab . denote
set types r .
0
Note type , t-type part obtained intersecting


B Bab
0

R R a-type part obtained intersecting aD B a,bD R .
type satisfies KB K = (T , A) t-model a-model
. call types type models K.

Definition 6 type model DL-LiteR KB K = (T , A) type |T |tr
|AT |ar .
denote set type models DL-LiteR KB K |K|r . type models
conjunction negation DL-LiteR axioms7 denoted defined
manner ct-type semantics. notions entailment, logical equivalence, consistency type semantics defined manner DL semantics. type
semantics, KB K entailing DL-LiteR axiom written K |=r .
establish connection DL models type models KB.
Let DL interpretation. pair (not necessarily distinct) elements d, e
domain I, e induce type follows.
(I, d, e) = (I, d, e) (I).
call (I, d, e) type induced e I. Note (I, d, e) induces t-type
(I) induces a-type forms receptively t-type a-type part
induced type. show DL interpretation I, model KB
type induced type model KB.
Proposition 12 Let K DL-LiteR KB DL interpretation. |K| iff
(I, d, e) |K|r pair d, e .
Also construct DL model KB type model KB.
Proposition 13 Let K DL-LiteR KB. |K|r , |K| d, e
(I, d, e) = .
connections, show type semantics induces set entailments DL-LiteR axioms induced DL semantics.
Theorem 9 Let K DL-LiteR KB conjunction DL-LiteR axioms.
K |= iff K |=r .
Since type semantics obtained combining t-type a-type semantics, inherits
finiteness succinctness properties two.
7. DL-LiteR axioms either DL-LiteR TBox axiom DL-LiteR ABox axiom.

344

fiDL-Lite Contraction Revision

2

Proposition 14 Let K DL-LiteR KB. K 2(n+2)m+(n +2)l type models,
n number individuals, basic concepts, l basic roles.
give corresponding role-complete condition sets types. set types
role-complete types satisfy model conditions t-type semantics
a-type semantics, R R, whenever type
R , (R)0 , (R)a D, type 0
{R, R0 , Rbc } 0 6= b, c ( 0 may identical, pair among a, b, c
may identical).
set types , corresponding KB defined way ct-type
semantics. Also shown analogously |K|r role-complete DL-LiteR
KB K role-complete guarantees existence unique corresponding DLLiteR KB.
Theorem 10 Let set types. role-complete, unique corresponding DL-LiteR KB .
introduced versions type semantics, ranging simplest
ct-type comprehensive one presented subsection. Assuming
signature, tc tr , ar r , ar tr r . characterising abilities
depicted Figure 1 match subset relationships.

4. Axiom Elimination
section, deal elimination axioms DL-Lite KBs. several
application scenarios elimination, (1) eliminate axioms TBox
ABox considered; (2) eliminate axioms ABox background
TBox assumed remains unchanged; (3) eliminate axioms KB
TBox ABox considered subject change. discussed
previous section, although type semantics used scenarios, waste
computational power use scenarios (1) (2) simpler ct-type, t-type
a-type semantics used. pursue scenario (1) two
handled manner. difference scenarios
switch underlying semantics a-type type semantics.
.
strategy axiom elimination define contraction function takes
input logically closed TBox conjunction TBox axioms returns output
.
TBox entailed. convenience, called original TBox,
.
contracting axiom, contracted TBox.
defining contraction functions, approach inspired Katsuno
Mendelzon (1992). However, take general approach explicit ordering
models assumed instead propositional models work t-type models.
Also assume original TBox coherent.
present contraction functions DL-LiteR TBoxes DL-Litecore
ones defined instantiated analogously. Thus KBs, TBoxes, ABoxes, axioms
assumed DL-LiteR ones throughout section.
345

fiZhuang, Wang, Wang, & Qi

4.1 Eliminating Axioms TBox
Intuitively, model set TBox contains counter-models axiom (i.e,
models ) TBox imply . Thus, eliminate axiom TBox
first add counter-models form intermediate model
set obtain corresponding TBox model set. Since intermediate model set
contains counter-models , sure obtained TBox entail .
Note apply approach, decision made counter-models
add. extralogical information required making decision could provided
domain expert KB. study theoretical properties assume
selection function plays role decision making. limiting case set
counter-models empty contracting axiom tautology. possible
stop TBox implying tautology, convenient reasonable way nothing
return original TBox. line intuition, selection function return
empty set cases. Formally, selection function set
t-types , (M ) non-empty subset unless empty.
Selection function restricted handle special case
entail . case, model set contains counter-models . Intuitively, asked
eliminate axiom entailed TBox nothing done
original TBox returned outcome. line intuition, selection
function required faithful intersection models
empty, selection function picks intersecting models others.
Formally, selection function faithful respect TBox (M ) = |T |tr
whenever |T |tr 6= , set t-types .
considerations, contraction function called T-contraction function
defined follows. Recall Tr function takes input set t-types
coherent, Tr (M ) closure corresponding DL-LiteR
TBox, otherwise Tr (M ) = .
.
Definition 7 function T-contraction function TBox iff conjunctions
TBox axioms
.
= Tr (|T |tr (||tr ))
faithful selection function .
Note r-model intermediate model set |T |tr (||tr ) satisfies model
conditions t-type semantics, since original TBox assumed coherent,
|T |tr (||tr ) includes models must coherent.
present properties T-contraction functions. commonly accepted
AGM postulates contraction best capture desirable properties contraction
functions. following, adapt AGM postulates alternatives
current contraction problem closed TBox , conjunctions
TBox axioms.
.
.
.
(T 1) = cl(T )
.
.
(T 2)
346

fiDL-Lite Contraction Revision

.
.
(T 3) 6|= , =
.
.
(T 4) 6|= , 6|=
.
.
(T 5) cl((T ) {})
.
.
.
(T de) |= |T |tr ||tr ||tr |=
.
.
.
(T 6) , =
.
According postulates, contraction function syntax-insensitive (T 6)
.
produces closed TBox (T 1) entail contracting axiom unless
.
.
tautology (T 4). produced TBox larger original one (T 2).
.
contracting axiom entailed, nothing done (T 3).
.
AGM origin (T 5) called Recovery main postulate formalising
minimal change principle contraction. requires information loss contraction minimal original TBox recovered expanding
contracted TBox contracting axiom. Recovery criticised many researchers among Hansson (1991) argued emerging property rather
fundamental postulate contraction. One evidence contraction
itself, satisfaction relies also properties (viz, AGM-compliance) underlying logic
(Ribeiro et al., 2013). particular DLs including DL-Lite incompatible
Recovery.
Due controversy Recovery, many proposed alternative postulates
capturing minimal change principle. quest proper postulate DL-Lite,
notice Recovery replaced following postulate Disjunctive Elimination
(Ferme et al., 2008):
.
.
K K K .
Disjunctive Elimination captures principle minimal change stating condition
retaining formula contraction. formula original belief set
disjunction contacting formula retained contraction
formula retained. Since disjunction axioms permitted DL-Lite, adapting
.
postulate describe disjunction terms models, thus postulate (T de).
.
Notice use t-models instead DL models (T de). Due property t-type
.
.
semantics unions axioms, |T | || || implies |T |tr ||tr ||tr
vice versa. Thus using DL models instead t-models enforces stricter condition
retaining means less number axioms retained contraction.
obvious principle minimal change favours use t-models.
.
.
.
.
show T-contraction function satisfies (T 1)(T 4), (T de), (T 6)
functions satisfying postulates T-contraction functions. words,
set postulates fully characterises properties T-contraction function.
.
.
.
Theorem 11 function T-contraction function TBox iff satisfies (T 1)
.
.
.
(T 4), (T de), (T 6).
347

fiZhuang, Wang, Wang, & Qi

presented definition T-contraction functions properties.
clear T-contraction function cannot seen update operator
KB update literatures. operators (e.g., Winsletts operator, see Winslett, 1990) usually
apply fixed rule update semantics (e.g., WIDTIO) determining update outcome.
T-contraction functions, rule deciding contraction outcome simulated
associated selection function. important note that, intentionally leave open
details selection function except require faithful. Thus
flexible enough simulate rules respect faithfulness condition. fact,
T-contraction function represents general framework dealing changes DL KBs
subsumes many update operators sense rules operators applied
simulated faithful selection functions. remaining section,
provide algorithm called TCONT implements one operator.
Algorithm 1: TCONT
Input: TBox conjunction TBox axiom
Output: TBox
1 tautology 6|=
2
return := ;

6

Let = PickCounterModel ();
foreach
6|=tr
:= \ {};

7

return := ;

3
4
5

TCONT takes input TBox conjunction TBox axioms , return
output TBox. TCONT first checks tautology implied (line 1)
case returned (line 2). Otherwise procedure PickCounterModel applied
picks counter-model (line 3). TCONT checks counter-model
axiom (line 4). axiom satisfied (line 5) removed
(line 6). Finally, whatever left returned (line 7).
procedure PickCounterModel takes conjunction TBox axioms return
counter-model . Essential, goal obtain t-model
achieved example following two steps: (1) Consider one conjunct 1 ,
1 = B v B, B, let contain B D; otherwise 1 = B v
(or 1 = R v R, R), let contain B (resp., R S);
otherwise 1 = R v S, let contain R (or contain R S). (2) Add
elements satisfies model conditions t-type semantics.
TCONT runs polynomial time respect size . particular,
checking whether tautology entails takes polynomial time (line 1), procedure
PickCounterModel shown runs linear time, satisfiability check (line 5)
runs linear time.
Proposition 15 Let TBox let conjunction TBox axioms.
TCONT (T , ) terminates returns TBox polynomial time respect size
348

fiDL-Lite Contraction Revision

.
.
.
function = TCONT (T , ), T-contraction
function.

Example 4 (contd Example 2) logical closure contains axiom, among others,
Loc v RS, derived Loc v CS RS v CS.
contract := Loc v RS , TCONT takes input.
line 3, suppose counter-model = {Loc , Das , CS, RS, OS} selected.
satisfy Loc v RS RS v CS, hence two axioms (and two)
eliminated closure.

5. Axiom Incoporation
section, deal incorporation axioms DL-Lite KBs. Similar
axioms elimination, three application scenarios often encountered,
(1) incorporate axioms TBox ABox considered; (2) incorporate
axioms ABox background TBox assumed remains unchanged;
(3) incorporate axioms KB TBox ABox considered
subject change. previous discussions, best use ct-type t-type semantics
scenario (1), a-type semantics scenario (2), scenario (3) use
full version type semantics. focus scenarios (1) (2). managing DL
KBs, scenario (3) less common handled (Wang et al., 2015)
similar approach.8
Similar axiom elimination, strategy define revision function takes
input logically closed TBox (or ABox ) conjunction TBox axioms (resp.
ABox axioms) returns output TBox (resp. ABox )
entailed. convenience, (or ) called original TBox (resp. ABox ),
revising axiom, (resp. ) revised TBox (resp. ABox ). defining
functions, assume original TBox coherent; original ABox consistent
background TBox9 background TBox coherent.
AGM framework, revision constructed indirectly contraction via
.
Levi identity (Levi, 1991). Formally, let contraction function belief set K,
.
revision function K defined K = Cn((K ) {}) formulas .
Since syntax DL-Lite permit axiom negation approach applicable
DL-Lite. define revision functions directly model-theoretic approach.
contraction approach inspired Katsuno Mendelzon (1992) based
type semantics.
present revision functions DL-LiteR DL-Litecore defined
instantiated analogously. Thus KBs, TBoxes, ABoxes, axioms assumed
DL-LiteR ones throughout section.
8. also proposed alternative semantic characterisation DL-Lite used structures could
exponentially larger type. Hence, polynomial time algorithm available.
9. consistent background TBox, ABox consistent.

349

fiZhuang, Wang, Wang, & Qi

5.1 Incorporating Axioms TBox
start revision function incorporating axioms TBox. presenting
function, need clarify fundamental difference AGM revision revision
DL TBoxes (TBox revision short). AGM revision aims incorporate new beliefs
resolving inconsistency. TBox revision goes beyond inconsistency resolving.
addition consistency, meaningful DL TBoxes coherent, thus TBox revision
resolve inconsistency incoherence caused incorporating new axioms.10
give intuitions behind revision function. model set TBox
subset axiom entails . Thus incorporate axiom
TBox , pick models form intermediate model set obtain
corresponding TBox. Since intermediate model set subset ,
sure obtained TBox entails .
Note apply approach, decision made models
pick. contraction, selection function assumed. Previously, contraction,
selection function returns empty set limiting case contracting axiom
tautology. limiting case revising axiom incoherent.
possible return coherent TBox entails incoherent axiom, convenient
reasonable way nothing return inconsistent TBox. Formally, define
that, function selection function set t-types , (M )
non-empty subset unless incoherent.
illustrate new definition selection function, suppose revising ,
incoherent axiom. discussed, case, revision fails. Since incoherent,
set t-models must incoherent. definition selection function guarantees
empty set t-types picked means revision outcome expected inconsistent
TBox indicating failure revision.
faithfulness condition also modified contraction case. selection
function faithful respect TBox satisfies
1. coherent, |T |tr (M ),
2. |T |tr coherent, (M ) = |T |tr .
revising , condition 1 deals case models overlaps
means {} consistent. line principle minimal change,
case, selection function pick overlapping models preserve many
possible original TBox axioms. Condition 2 deals case
overlapping exists also coherent. Since case {} consistent
also coherent, revision boils set union operation (i.e., cl(T {})).
selection function therefore picks overlapping models others.
illustrate new notion faithfulness, suppose revising , t-models
overlap (i.e., ||tr |T |tr ) set overlapping t-models coherent.
Since incoherence resolve, intuitive way deal revision
add without making change, union
10. fact concentrate incoherence resolving ABox considered. definition,
coherent TBox must consistent. Inconsistency resolving thus part incoherence resolving.

350

fiDL-Lite Contraction Revision

revision outcome. Since set t-models union ||tr |T |tr revision
outcome obtained taking corresponding TBox t-types picked selection
function, clear revision intuitive selection function picked
t-types ||tr |T |tr other. terms, call selection function faithful.
addition faithfulness, selection function guarantee t-types picked
coherent, thus introduce following condition. say selection function
coherent preserving coherent sets t-types , (M ) coherent.
considerations, revision function called T-revision function defined
follows.
Definition 8 function T-revision function TBox iff conjunctions
TBox axioms
= Tr ((||tr ))
faithful coherent preserving selection function.
present properties T-revision functions. Since AGM revision deals
inconsistency, AGM revision postulates formulated capture rationale behind
inconsistency resolving process. TBox revision also deals incoherence, thus
postulates TBox revision capture rationale behind inconsistency
also incoherence resolving. replacing conditions consistency coherence,
AGM revision postulates reformulated follows TBox revision, closed
TBox , conjunctions TBox axioms.
(T 1) = cl(T )
(T 2) |=
(T 3) coherent, cl(T {})
(T 4) {} coherent, cl(T {})
(T 5) coherent, coherent
(T 6) , =
(T f ) incoherent, =
According postulates, revised TBox closed (T 1), entails revising
axiom (T 2); revising axiom coherent, revised TBox entails axiom
entailed original TBox revising axiom (T 3); revising
axiom causes incoherence revised TBox closure original TBox
revising axiom (T 4). revised TBox coherent whenever revising axiom
(T 5); Also revision function syntax-insensitive (T 6). Since revising axiom
revised TBox, revising axiom incoherent revised TBox
must so. failure postulate (T f ) requires case simply return
inconsistent TBox. purpose TBox revision incorporate axiom resolve
incoherence caused. input axiom incoherent, revision doomed
351

fiZhuang, Wang, Wang, & Qi

failure. fails ground argue proper revision outcome
is, comes convention take. Following AGM revision, take
convention returning inconsistent TBox. AGM origin (T f ) states
revising formula inconsistent return inconsistent belief set, deducible
AGM postulates thus postulated explicitly AGM framework.
show T-revision function satisfies (T 1)(T 6), (T f )
functions satisfying postulates T-revision functions. words, set
postulates fully characterises properties T-revision function.
Theorem 12 function T-revision function TBox iff satisfies (T 1)
(T 6) (T f ).
T-contraction function, T-revision function update operator, rather
represents general framework incorporating axioms DL-Lite TBox.
T-contraction function, T-revision function subsumes many update operators.
following, provide algorithm called TREVI implements one operator.
Algorithm 2: TREVI
Input: TBox conjunction TBox axioms
Output: TBox
1 incoherent
2
return := ;

8

// N set atomic concepts atomic roles B R
foreach F N
{} |= F v F
Let = PickSatModel (, F );
foreach
6|=tr
:= \ {};

9

return := cl(T {});

3
4
5
6
7

TREVI takes input TBox conjunction TBox axioms , return
output TBox. TREVI starts checking whether incoherent (line 1),
returns inconsistent TBox (line 2). Otherwise, checks atomic concept
atomic role unsatisfiable union (line 34). union
incoherent one concept role unsatisfiable. unsatisfiable
concept role F , procedure PickSatModel applied picks t-model
includes F (line 5). TREVI checks axiom (line 6). axiom
satisfiable (line 7), removed (line 8). Finally, closure
union whatever left returned (line 9).
procedure PickSatModel takes conjunction TBox axiom atomic concept role F return t-model includes F . achieved example
following four steps: (1) Let contain F , extend satisfies {}11
11. Recall given TBox , represents extension .

352

fiDL-Lite Contraction Revision

propositionally. (2) |= R v R, R contains R (or (R)0 ), let
contain (resp., (S)0 ). (3) extend satisfies model conditions
t-type semantics. (4) Repeat steps (1)(3) till longer changes.
TREVI runs polynomial time respect size . particular,
checking coherence (line 1) takes polynomial time, concept role satisfiability check (line 4) takes polynomial time, procedure PickSatModel shown takes
polynomial time, satisfiability check (line 7) takes linear time.
Proposition 16 Let TBox let conjunction TBox axioms.
TREVI (T , ) terminates computes TBox polynomial time respect size
function = TREVI (T , ), T-revision
function.
Example 5 (contd Example 2) Adding = Das v RS introduces incoherence,
i.e., {} |= Loc v , due Loc v CS, RS v CS, Loc v Das .
revise , TREVI takes input. line 5, suppose t-type
= {Loc , Das , CS, RS, OS} picked. satisfy Loc v RS RS v CS,
hence two axioms (and two) eliminated closure
achieve coherent union .
5.2 Incorporating Axioms ABox
turn revision function incorporating axioms ABox. dealing
TBox revision, argued since meaningful TBox coherent, essential task
revision incoherence resolving. Coherence longer issue here, assume
background TBox coherent remains unchanged throughout revision process.
Therefore need concern inconsistency resolving. Also note
working a-type semantics now.
idea defining T-revision function also used here. First, pick
models revising ABox axiom form intermediate model set, return
corresponding ABox revised ABox. decision making models pick
modelled selection function. Formally, function selection function
set a-types , (M ) non-empty subset unless empty.
Recall mean ABox background TBox . revising
axiom , special case models overlap indicating
KB (T , {}) consistent. Since inconsistency resolve, simply
return union {} revised ABox. line intuition, selection
function pick overlapping models say selection
function faithful. Formally, selection function faithful respect ABox
(M ) = |AT |ar whenever |AT |ar 6= .
considerations, revision function called A-revision function defined
follows. Recall ATr function takes input set a-types
consistent , ATr (M ) closure corresponding
DL-LiteR ABox respect , otherwise ATr (M ) inconsistent ABox .

353

fiZhuang, Wang, Wang, & Qi

Definition 9 function A-revision function ABox iff conjunctions
ABox axioms
= ATr ((|{}T |ar ))
faithful selection function.
present properties A-contraction functions. AGM postulates revision
commonly accepted capture desirable properties revision. following,
adapt current revision problem closed ABox, ,
conjunctions ABox axioms.
(A 1) = clT (AT )
(A 2) |=ar
(A 3) clT (AT {})
(A 4) |(A {})T |ar 6= , clT (AT {})
(A 5) |{}T |ar 6= , |(AT )T |ar 6=
(A 6) , =
(A f ) |{}T |ar = , =
incoherence resolving picture, adapted postulates like AGM
origins concern inconsistency resolving. According postulates, revised ABox
closed (A 1); entails revising axiom (A 2); entails axiom
entailed original ABox revising axiom (A 3); closure union
original ABox revising axiom revising axiom causes inconsistency
(A 4), consistent whenever revising axiom (A 5); Also revision
function syntax-insensitive (A 6). limiting case revising axiom
inconsistent, since possible revised ABox entails
time consistent, take convention return inconsistent ABox revised
ABox (A f ).
show A-revision function satisfies (A 1)(A 6) (A f )
functions satisfying postulates A-revision functions. words, set
postulates fully characterises properties A-revision function.
Theorem 13 function A-revision function ABox iff satisfies (A1)
(A 6), (A f ).
T-contraction T-revision functions, A-revision function update
operator, rather represents general framework incorporating axioms DL-Lite
ABoxes. T-contraction T-revision functions, A-revision function subsumes
many update operators. following, provide algorithm called AREVI
implements one operator.
AREVI takes input TBox , ABox A, conjunction ABox axioms ,
return output ABox. AREVI first checks inconsistent (line 1)
354

fiDL-Lite Contraction Revision

Algorithm 3: AREVI
Input: TBox , ABox A, conjunction ABox axioms
Output: ABox
1 inconsistent
2
return := ;
3
4

consistent (T , A)
return := clT (A {});

8

Let = PickModel ();
foreach
6|=ar
:= \ {}

9

return := clT (A {});

5
6
7

case inconsistent ABox returned (line 2). Otherwise, revising axiom
consistent original ABox background TBox union axiom
original ABox returned (lines 34). Otherwise procedure PickModel applied
picks a-model (line 5). AREVI checks a-model axiom
(line 6). axiom satisfied (line 7) removed (line 8).
Finally, whatever left combined logical closure returned (line
9).
procedure PickModel takes ABox axiom returns a-model .
achieved example following five steps: (1) Let contain propositional
forms conjuncts (recall propositional form ABox axiom A(a) Aa ).
(2) Extend satisfies propositionally. (3) |= R v R, R
contains (R)a D, let contain (S)a . (4) extend
satisfies model conditions a-type semantics. (5) Repeat steps (2)(4) till
longer changes.
Proposition 17 Let ABox conjunction ABox axioms.
AREVI (T , A, ) terminates computes ABox polynomial time respect
size function = AREVI (T , A, ),
A-revision function.
AREVI runs polynomial time respect size , . particular,
checking consistency (line 1) (T , A) (line 3) take
polynomial time. procedure PickModel shown runs polynomial time,
satisfiability check (line 7) takes linear time.
Example 6 (contd Example 3) Adding = RS(s) introduces inconsistency, due
axioms Loc(d, s) A, Loc v CS RS v CS .
revise , AREVI takes , A, inputs. line suppose a-type =
{HDd , (Loc)d , (Das)d , Dasds , (Das )sd , (Das )s , RSs , OSs } picked. satisfy
Loc(d, s), hence assertion (and one) eliminated closure
achieve consistent union .
355

fiZhuang, Wang, Wang, & Qi

6. Related Work
dealing changes DL KBs, many like us considering belief change problem (Qi et al., 2006; Qi & Du, 2009; Qi et al., 2008; Ribeiro & Wassermann, 2009; Wang
et al., 2015). Qi et al. (2006) gave weakening based approach revising ALC KBs.
idea weaken TBox axioms inconsistencies resolved. Qi Du (2009)
Wang et al. (2015) adapted Dalals (1988) Satohs operators (1988) respectively
revising DL KBs. main issue works revision postulates
formulated appropriately capture rationales incoherence resolving. Moreover, adapted revision operators cannot guarantee coherence revision outcome.
contrast approach, Qi et al. (2008) Ribeiro Wassermann (2009) studied
contraction revision TBoxes KBs necessarily closed. particular, Qi et al. (2008) adapted kernel revision (Hansson, 1994). Ribeiro Wassermann
(2009) adapted partial meet contraction revision (Hansson, 1999) kernel contraction
revision (Hansson, 1994), semi-revision (Hansson, 1997).
Due popularity DL-Lite, many worked problem updating DLLite KBs (De Giacomo et al., 2009; Calvanese et al., 2010; Kharlamov & Zheleznyakov,
2011; Kharlamov et al., 2013; Lenzerini & Savo, 2011, 2012). update however
different meaning update operation belief revision literatures (Katsuno &
Mendelzon, 1991). works, interpreted contraction revision
mainly focused issues expressibility update outcome. also
tackled expressibility issues assuming type semantics. Due succinctness
finiteness type semantics, issue settled relatively easy. works
comparable ours, Lenzerini Savo (2011) dealt instance level
update, TBox remains unchanged ABox undergoes changes. Later
Lenzerini Savo (2012) extended approach updating inconsistent KBs. main
idea first obtain ABoxes (called repairs) consistent background
TBox; differ minimally original ABox; accomplish insertion deletion
certain axioms. intersection repairs taken update outcome.
problem setting similar A-revision functions. Although targeted
expressive DL-Lite (i.e., DL-LiteA,id ), considering DL-LiteR idea
simulated A-revision function. restricting associated selection function,
A-revision function always return outcome approach.
Grau et al. (2012) studied operations contract revise time.
constraint states set axioms incorporated eliminated
first specified. operation maps KB another satisfies constraint.
operation reduces revision contraction function making empty called
eliminating set incorporating set respectively. However, identify
postulates characterise contraction revision functions. working
DL-Lite, functions simulated T-contraction T-revision function.
general setting, Ribeiro et al. (2013) identified properties monotonic logic
contraction function defined satisfies Recovery postulate.
result, DL-Lite one logic, consistent (i.e., Theorem 11).
Axiom negation supported DLs required defining revision functions
contraction functions via Levi identity. Flouris, Huang, Pan, Plexousakis,
356

fiDL-Lite Contraction Revision

Wache (2006) proposed several notions negated axioms DLs. also explored
notions inconsistent incoherent TBoxes emphasised importance resolving
incoherence addition inconsistency.
Similar T-revision function, group works usually referred ontology debugging
also deals unsatisfiable concepts (e.g., Kalyanpur et al., 2006). method used
based notion Minimal Unsatisfiability Preserving Sub-TBoxes (MUPS ).
unsatisfiable concept B, MUPS based method first computes MUPSs
B, computes minimal hitting set MUPSs. incoherence resolved
removing axioms minimal hitting set. TREVI deals problem
efficient way. Roughly speaking minimal hitting set MUPSs corresponds
t-model formed line 5 TREVI , thus avoid computations MUPSs
minimal hitting sets significant saving computational power.

7. Conclusion
Due diversity DLs, difficult impossible come generalised
contraction revision functions work best DLs. DL unique
deserve treated individually make uniqueness. distinguishing feature DL-Lite allows restricted version existential
universal quantifiers. taking advantage feature, developed type semantics
DL-Lite resembles underlying semantics propositional logic. defined
instantiated contraction revision functions DL-Lite KBs whose outcomes
obtained manipulating type models KBs contracting revising axioms.
first contribution development type semantics DL-Lite. Given type
semantics equivalent DL semantics characterising standard inference tasks
DL-Lite, outperforms DL semantics terms finiteness succinctness. second
contribution axiomatic characterisation contraction revision functions.
key obtaining result T-revision functions reformulation AGM revision
postulates inconsistency centred incoherence centred. TBox revision deals
inconsistency also incoherence, postulates TBox revision must capture
rationales behind incoherence resolving. third contribution providing tractable
algorithms instantiate contraction revision functions.
future work, plan study contraction revision DLs
expressive DL-Lite. Since DLs may allow unrestricted existential universal
quantifiers, concepts formed unbound nesting quantifies. semantic
characterisation kinds concepts type semantics may possible.
need techniques tailored DLs.

Acknowledgement
thank anonymous reviews comments helped improve paper
substantially.
357

fiZhuang, Wang, Wang, & Qi

Appendix
presenting proofs technical results, introduce notions
simplify presentation proofs.
First, given ABox A, write P (a, b) mean P (b, a) A.
present notion chase. Given DL-LiteR (DL-Litecore ) ABox TBox
, chase w.r.t. , denoted chaseT (A) defined procedurally follows: initially
take chaseT (A) := A, exhaustively apply following rules:
A(s) chaseT (A), v A0 , A0 (s) 6 chaseT (A), chaseT (A) :=
chaseT (A) {A0 (s)};
A(s) chaseT (A), v R , R(s, t) chaseT (A),
chaseT (A) := chaseT (A) {R(s, v)} v fresh constant
appeared chaseT (A) before;
R(s, t) chaseT (A), R v , A(s) 6 chaseT (A), chaseT (A) :=
chaseT (A) {A(s)};
R(s, t) chaseT (A), R v , S(s, t) chaseT (A),
chaseT (A) := chaseT (A) {S(s, v)} v fresh constant
appeared chaseT (A) before;
R(s, t) chaseT (A), R v , S(s, t) 6 chaseT (A), chaseT (A) :=
chaseT (A) {S(s, t)}.
Note rules, R role inverse role. well
known ABox induces unique interpretation IA domain IA
consists constants A; concept name A, AIA = {d | A(d) A};
role name P , P IA = {(e, f ) | P (e, f ) A}. proofs, slightly misuse
notation let denote also interpretation induced A. way, chaseT (A)
also used denote interpretation.
Finally, present notions positive inclusions, negative inclusions, closures
negative inclusions. call TBox axioms forms B v R v positive inclusions
(PIs), call axioms forms B v R v negative inclusions (NIs),
B, B R, R. Given DL-LiteR (DL-Litecore ) TBox , closure NIs
, denoted cln(T ), defined inductively follows:
NIs cln(T );
B1 v B2 B2 v B3 B3 v B2 cln(T ), B1 v B3 cln(T );
R1 v R2 R2 v B B v R2 cln(T ), R1 v B cln(T );
R1 v R2 R2 v B B v R2 cln(T ), R1 v B cln(T );
R1 v R2 R2 v R3 R3 v R2 cln(T ), R1 v R3 cln(T );
R v R R v R R v R cln(T ), three cln(T ).
358

fiDL-Lite Contraction Revision

clear definition |= cln(T ). following result shown
(Calvanese et al., 2007) provides method build DL models using chase.
result used prove Propositions 3 6.
Lemma 1 Let (T , A) DL-LiteR (DL-Litecore ) KB. model cln(T ),
chaseT (A) model (T , A).
Proof Proposition 1
|T |tc R R R, condition 2 Definition 1,
6|= R v . Thus, model RI 6= . Suppose (d, e) RI , let
0 = (I, e). Then, R 0 . Also, Proposition 2, 0 |T |tc .

Proof Proposition 2
direction, suffices show axiom B v C ,
B implies C . Let = (I, d). Suppose B , definition
B . Since ct-model , satisfies B v C propositionally. C basic
concept, B implies C ; otherwise C = negated basic concept,
B implies 6 . cases, definition , C .
direction, let = (I, d) arbitrary . first show
kT ktc . concept inclusion B v C , assume B , B . C
basic concept, model implies C , turn implies C .
C = negated basic concept, similar argument, 6 DI 6 .
is, satisfies B v C propositionally. is, kT ktc . second half Definition 1,
|= R v RI = . Clearly, R 6 . shown |T |tc .

proving Proposition 3, first show following lemma preparation.
Lemma 2 DL-Litecore TBox , ct-model satisfies kcln(T )ktc .
Proof : Towards contradiction, suppose exists ct-model NI
cln(T ) satisfy propositionally. show must violate
NI (which contradicts fact ct-model ). prove
induction. convenience, assume inclusions cln(T ) added inductively
following definition.
initialization, violates NI . induction steps, show
added cln(T ) due another axiom already cln(T ), violates .
two cases: (1) Suppose B1 v B3 , added due PI B1 v B2 NI B2 v B3
(or B3 v B2 ) cln(T ). Then, fact satisfy propositionally,
{B1 , B3 } . Also, satisfies B1 v B2 , B2 . Hence, violates NI B2 v B3
(B3 v B2 ). (2) Suppose R v R, added due R v R cln(T ). Then,
fact satisfy propositionally, R . R v R |= R v ,
condition 2 Definition 1, violates NI R v R .

Proof Proposition 3
359

fiZhuang, Wang, Wang, & Qi

construct interpretation using chase: Let constant, NC
set concept names,
={A(d) | NC }
{R(d, ed,R ) | R , ed,R fresh constant}.
Take = chaseT (A ). want show model (I , d) = .
show former, Lemma 1, need show model cln(T ).
Towards contradiction, suppose case, axiom B v
cln(T ) violated . case |= B(s) |= D(s)
constant . essentially four cases (note B symmetric):
(i) Suppose B concept names, |= B(s) |= D(s)
= d, B(d) , D(d) . construction , {B, D} thus
propositionally satisfy B v D. is, 6 kcln(T )ktc , violates Lemma 2.
(ii) Suppose B = R R concept name, |= B(s)
= R(d, t) t, |= D(d) B(d) . Thus, {B, R}
thus propositionally satisfy B v R. Again, 6 kcln(T )ktc , violates
Lemma 2.
(iii) Suppose B = R R = R 6= S, |= B(s)
|= D(s) R(s, t) S(s, u) t, u. =
t, u fresh constants. case, {R, S} propositionally
satisfy R v S, violates Lemma 2.
(iv) Suppose B = R R = R, |= B(s) R(s, t)
t. = fresh constant; = fresh constant.
former case, R , latter case, R . Since R v cln(T ),
|= R v |= R v . cases violate fact |T |tc condition 2
Definition 1.
shown model cln(T ), thus model .
remains show (I , d) = . Since clear (A , d) = ,
definition chase, (A , d) (I , d). need show (I , d) (A , d).
equivalent show \ contain assertion form A(d)
R(d, s) assertion R(d, t) (as otherwise, R, respectively,
(I , d) \ (A , d) according definition (I , d)). Towards contradiction, suppose
assertion \ . chase rules, happen chase
rule applicable assertion g form B(d) S(d, t). Let g first
assertions triggers chase rule. chase rules, observe g must .
Suppose g = B(d), construction , B . g triggers chase
rule B v , condition 1 Definition 1, propositionally satisfies
B v A, hence A(d) , contradiction (the applicability
of) chase rule; otherwise, g triggers chase rule B v R ,
condition 1 Definition 1, kB v Rktc , thus R R(d, u)
u, contradicts chase rule.
Suppose g = S(d, t), construction , . g triggering
chase rule v A, condition 1 Definition 1, propositionally satisfies
360

fiDL-Lite Contraction Revision

v A. , A(d) , contradicts chase rule. g
triggering chase rule v R, propositionally satisfies v R,
thus R R(d, u) u, contradiction.
shown \ contain assertion form A(d) R(d, s).
Thus, (I , d) = (A , d) = .

Proof Theorem 1
|T | 6= , model |T |. Let , = (I, d).
Proposition 2, |T |tc . is, |T |tc 6= . Conversely, suppose |T |tc 6= , let |T |tc .
Proposition 3, model . is, |T | =
6 . Thus, |T | empty
|T |tc empty. |T | |T |tc empty, statement trivially holds.
follows, assume |T | |T |tc non-empty.
direction, want show 6|= |T |tc 6 ||tc . Then,
model satisfy . is, TBox axiom
conjunction satisfied I. Without loss generality, assume contains
(single) TBox axiom. Suppose B v C. Then, domain element
B 6 C . Let = (I, d). Since model , Proposition 2,
ct-model . C basic concept, B implies B 6 C ,
turn implies C 6 . C = negated basic concept, similar argument,
B implies . is, propositionally satisfy B v C 6 kktc .
shown |T |tc 6 ||tc .
direction, want show |T |tc 6 ||tc 6|= . Since

|T |c 6 ||tc , ct-type |T |tc 6 ||tc . Proposition 3, exists
model domain elements (I, d) = . need show
model . Suppose otherwise, model , Proposition 2,
must ct-model , contradicts fact 6 ||tc . Thus, model ,
shown 6|= .

Proof Theorem 2
|T |tc , condition 1 Definition 1, propositionally satisfies =
1, . . . , n. Moreover, coherent, monotonicity DL-Lite, exists R R
|= R v . Hence, |i |tc = 1, . . . , n. is, |T |tc |1 |tc |n |tc .
Conversely, |1 |tc |n |tc , condition 1 Definition 1, kT ktc . Further,
coherent, exists R R |= R v . Hence, |T |tc . is,
|1 |tc |n |tc |T |tc .

Proof Theorem 3
|T |tc , Proposition 3, model |T |
(I, d) = . Since |T | || ||, || ||. Suppose without loss generality
||, Proposition 2, (I, d) ||tc . is, ||tc . shown
|T |tc ||tc ||tc .

presenting proof Theorem 4, first show Lemmas 3 4 regarding
union TBoxes (or equivalently, conjunction TBox axioms). similar way
361

fiZhuang, Wang, Wang, & Qi

Lemma 2, show following lemma. difference cannot assume
ct-type |T1 |tc |T2 |tc satisfies |T1 T2 |tc , thus cannot apply condition 2
Definition 1 proof.
Lemma 3 two DL-Litecore TBox T1 T2 , role-complete set ct-types
|T1 |tc |T2 |tc , holds kcln(T1 T2 )ktc .
Proof : Towards contradiction, suppose exists ct-type NI
cln(T1 T2 ) satisfy propositionally. show ct-type 0
exists violates NI T1 T2 , contradicts fact |T1 |tc |T2 |tc
(as 0 |T1 |tc |T2 |tc implies 0 satisfies NIs T1 T2 propositionally). Similar
proof Lemma 2, prove induction.
initialization, T1 T2 let 0 = 0 violates NI T1 T2 .
induction steps, show added cln(T1 T2 ) due another axiom
already cln(T1 T2 ), show 00 exists violates (and eventually
take 0 = 00 T1 T2 ). two cases: (1) Suppose B1 v B3 , added
due PI B1 v B2 T1 T2 NI B2 v B3 (or B3 v B2 ) cln(T1 T2 ). Then,
fact satisfy propositionally, {B1 , B3 } . Also, satisfies B1 v B2
T1 T2 , B2 . Hence, let 00 = 00 violates NI B2 v B3 (B3 v B2 ). (2) Suppose
R v R, added due R v R cln(T1 T2 ). Then, fact
satisfy propositionally, R . role-complete, exists 00
R 00 . Hence, 00 violates NI R v R .

Lemma 4 Let set ct-types, 1 , 2 two conjunctions DL-Litecore TBox
axioms. Suppose role-complete, |1 |tc |2 |tc implies |1 2 |tc .
Proof : , want show |1 2 |tc . end, construct
model 1 2 . Let Ti set axioms (as conjuncts) = 1, 2,
proof Proposition 6, = chaseT1 T2 (A ). show model T1 T2
(I , d) = similar way proof Proposition 3 (by using Lemma 3 instead
Lemma 2). Except case (iv): Suppose violates R v R cln(T1 T2 ). Note
that, different proof Proposition 3, cannot assume either R v R cln(T1 )
R v R cln(T2 ) (That is, cannot apply condition 2 Definition 1). Yet
still derive contradiction. violates R v R R(d, t) fresh
constant R(s, d) fresh constant. former case, R ,
propositionally satisfy R v R. is, 6 kcln(T1 T2 )ktc , violates
Lemma 3. latter case, R . Since role-complete, R 0 0 .
Hence, 0 propositionally satisfy R v R. is, 0 6 kcln(T1 T2 )ktc ,
violates Lemma 3.
Now, shown model 1 2 (I , d) = . Proposition 2,
|1 2 |tc .

Proof Theorem 4
Suppose two TBoxes T1 T2 corresponding . is, |T1 |tc
|T2 |tc . Lemma 4, |T1 T2 |tc . minimality requirement
362

fiDL-Lite Contraction Revision

corresponding TBox, |T1 T2 |tc
6 |Ti |tc = 1, 2. is, |T1 T2 |tc = |Ti |tc = 1, 2.
Theorem 1, T1 equivalent T2 .

Proof Proposition 5
direction, suffices show concept inclusion B v C
, B implies C ; additionally, e (not necessarily 6= e)
role inclusion R v E , (d, e) RI implies (d, e) E . Let = (I, d, e).
Suppose B , definition B . Since t-model ,
propositionally satisfies B v C. C basic concept, B implies C ;
otherwise C = negated basic concept, B implies 6 . cases,
definition , C . role inclusion R v E, suppose (d, e) RI ,
definition R . t-model , propositionally satisfies R v E.
E role E ; otherwise E = negated role 6 . cases,
definition , (d, e) E .
direction, let = (I, d, e) arbitrary d, e . first show
satisfies condition 1 Definition 3. concept inclusion B v C , assume
B , definition , B . C basic concept, model
implies C , turn implies C . C = negated basic concept,
similar argument, 6 DI 6 . is, propositionally satisfies B v C.
similar way, concept inclusion B 0 v C 0 B 0 , C 0 B 0 , show
satisfies B 0 v C 0 . role inclusion R v E , assume R . Then,
definition , (d, e) RI . Since model , (d, e) E . E role E ;
otherwise E = negated role 6 . Thus, propositionally satisfies kR v E.
Similarly, satisfies R0 v E 0 role inclusion R0 v E 0 . shown
kT ktr .
next show satisfies conditions 25 Definition 3. condition 2, |=
R v RI = . Clearly, 6 (R)I e 6 (R )I . definition , R 6
(R )0 6 . condition 3, |= R v (R)I (S)I . Suppose R ,
definition implies (R)I . Then, (S)I , thus . Similarly,
suppose (R)0 , implies e (R)I . Hence, e (S)I , thus (S)0 .
condition 4, R definition , (d, e) RI , implies (R)I
e (R )I . definition , R (R )0 . Condition 5 clearly satisfied
definition (I, d, e).
shown satisfies conditions Definition 3, is, |T |tr .

proving Proposition 6, first show Lemma 5. Note cln(T ) obtained
cln(T ) adding copy axiom cln(T ), kcln(T )ktr set t-types
propositional models cln(T ) .
Lemma 5 DL-LiteR TBox , t-model satisfies kcln(T )ktr .
Proof : Towards contradiction, suppose exists t-model NI
cln(T ) propositionally satisfy . show must violate
NI , induction. consider NI cln(T ), case
copy NI cln(T ), i.e., cln(T ) \ cln(T ), shown similarly.
363

fiZhuang, Wang, Wang, & Qi

initialization, violates NI . induction steps,
show added cln(T ) due another axiom already cln(T ), violates .
(1) Suppose B1 v B3 , added due PI B1 v B2 NI B2 v B3 (or
B3 v B2 ) cln(T ). Then, fact propositionally satisfy ,
{B1 , B3 } . Also, satisfies B1 v B2 , B2 . Hence, violates NI
B2 v B3 (B3 v B2 ).
(2) Suppose R1 v B, added due PI R1 v R2 NI R2 v B (or
B v R2 ) cln(T ). Then, fact propositionally satisfy ,
{R1 , B} . Also, condition 3 Definition 3, R2 . Hence, violates NI
R2 v B (B v R2 ).
(3) Suppose R1 v B, added due PI R1 v R2 NI R2 v B (or
B v R2 ) cln(T ). Then, fact propositionally satisfy ,
{R1 , B} . Also, condition 3 Definition 3, R2 . Hence, violates NI
R2 v B (B v R2 ).
(4) Suppose R1 v R3 , added due PI R1 v R2 NI R2 v R3 (or
R3 v R2 ) cln(T ). Then, fact propositionally satisfy ,
{R1 , R3 } . Also, satisfies R1 v R2 , R2 . Hence, violates NI
R2 v R3 (R3 v R2 ).
(5) Suppose R v R, added due NI R v R (or R v R) cln(T ).
Then, fact propositionally satisfy , R . R v
R |= R v (R v R |= R v ), condition 2 Definition 3, violates NI
R v R (R v R).
(6) Suppose R v R, added due NI R v R (or R v R ) cln(T ). Then,
fact propositionally satisfy , R . Also, condition 4
Definition 3, R . R v R |= R v (R v R |= R v ),
condition 2 Definition 3, violates NI R v R (R v R ).

Proof Proposition 6
construct interpretation using chase: Let d, e two distinct constants,
NC NC0 set concept names B B 0 , respectively,
={A(d) | NC } {A(e) | A0 NC0 } {R(d, e) | R R }
{R(d, fd,R ) | R B , R 6 , fd,R fresh constant}
{R(e, fe,R ) | (R)0 B 0 , R 6 , fe,R fresh constant}.
Take = chaseT (A ). want show model (I , d, e) = .
show former, Lemma 1, need show model cln(T ).
Towards contradiction, suppose case, axiom B v
R v cln(T ) violated .
(1) Suppose violates B v D, case |= B(s) |= D(s)
constant . essentially four cases:
364

fiDL-Lite Contraction Revision

(i) Suppose B concept names, |= B(s) |= D(s)
= = e, B(s) , D(s) . = construction ,
{B, D} thus propositionally satisfy B v D; otherwise = e
{B 0 , D0 } propositionally satisfy B 0 v D0 . cases violate Lemma 5.
(ii) Suppose B = R R concept name, |= B(s) =
= e, R(s, t) t. |= D(s) D(s) . Suppose without loss
generality = (similar (i), case = e shown
way). = e construction , R , condition 4 Definition 3,
R ; otherwise, fresh constant, R . cases, {B, R} thus
propositionally satisfy B v R, violates Lemma 5.
(iii) Suppose B = R R = R 6= S,
|= B(s) |= D(s) R(s, t) S(s, u) t, u.
(a) = = e, t, u fresh constants; (b) = = u = e; (c)
= e = u = d. case (a), suppose w.o.l.g = d, {R, S}
propositionally satisfy R v S, violates Lemma 5. case (b), {R, S} ,
condition 4 Definition 3, {R, S} , violates Lemma 5. case (c),
{R , } . condition 4 Definition 3, {(R)0 , (S)0 } , hence
propositionally satisfy (R)0 v (S)0 . violates Lemma 5.
(iv) Suppose B = R R = R, |= B(s) R(s, t)
t. (a) = = e, fresh constant; (b) =
= e, fresh constant; (c) = = e; (d) = e = d.
case (a), suppose w.l.o.g. = d, R . case (c), R , condition 4
Definition 3, R . cases, propositionally satisfy R v R,
violates Lemma 5. case (b), suppose w.l.o.g. = d, R . case (d), R ,
condition 4 Definition 3, R . Since |= R v , violates condition 2
Definition 3.
(2) Suppose violates R v S, case |= R(s, t) |= S(s, t)
constants s, . case (a) = = e, (b) = e
= d, (c) R = = = e fresh constant, (d) R = = = e
fresh constant. case (a), {R, S} , propositionally satisfy
R v S. case (b), {R , } , condition 5 Definition 3, {R0 , 0 } .
Hence, propositionally satisfy R0 v 0 . neither case, kcln(T )ktr
violates Lemma 5. case (c), |= R v R, is, |= R v . = R ,
otherwise = e (R)0 , violates condition 2 Definition 3. Similarly,
case (d), |= R v . = R , otherwise = e (R )0 ,
violates condition 2 Definition 3.
shown model cln(T ), thus model .
remains show (I , d, e) = . Since clear (A , d, e) = ,
definition chase, (A , d, e) (I , d, e). need show (I , d, e)
(A , d, e). equivalent show \ contain assertion
form A(d), A(e), R(d, e), R(d, s) R(e, s) fresh constant
assertion R(d, t) respectively R(e, t) (as otherwise, A, A0 , R, R, (R)0 ,
respectively, (I , d, e) \ (A , d, e) according definition (I , d, e)).
Towards contradiction, suppose assertion \ . chase
rules, happen chase rule applicable assertion g form B(d),
365

fiZhuang, Wang, Wang, & Qi

B(e), S(d, e), S(d, t) S(e, t) fresh constant chase. Let g first
assertions triggers chase rule, chase rules, g must .
Suppose g = B(d), construction , B . g triggers chase
rule B v , condition 1 Definition 3, propositionally satisfies
B v A, hence A(d) , contradiction (the applicability
of) chase rule; otherwise, g triggers chase rule B v R ,
condition 1 Definition 3, propositionally satisfies B v R, thus R
R(d, u) u, contradicts chase rule. case
g = B(e) shown similarly, replacing propositionally satisfying B v
propositionally satisfying B 0 v A0 , propositionally satisfying B v R
propositionally satisfying B 0 v (R)0 .
Suppose g = S(d, e), construction , . conditions 4
5 Definition 3, , (S )0 , (S )0 .
g triggers chase rule v R , condition 1 Definition 3,
propositionally satisfies v R, R R(d, e) , contradicts
chase rule.
g triggers chase rule v R , condition 1 Definition 3,
propositionally satisfies (S )0 v R0 , R0 . condition 5 Definition 3,
R R (d, e) , contradicts chase rule.
g triggers chase rule v A, propositionally satisfies v A.
, A(d) , contradicts chase rule.
g triggers chase rule v A, propositionally satisfies (S )0 v
A0 . (S )0 , A0 A(e) , contradicts chase rule.
g triggers chase rule v R, propositionally satisfies v
R. , R . Hence, R(d, u) u,
contradiction.
Similarly show case g triggering chase rule v R.
Suppose g = S(d, t) fresh constant, construction ,
.
g triggers chase rule v R , R(d, t) added. condition 3
Definition 3, R , construction , R(d, u)
u. chase rules, R(d, t) behaves differently R(d, u)
chase. Thus, could equally consider g = R(d, u) discussion. is,
application chase rule v R effect proof.
g triggering chase rule v A, condition 1 Definition 3,
propositionally satisfies v A. , A(d) ,
contradicts chase rule.
g triggering chase rule v R, propositionally satisfies v
R, thus R R(d, u) u, contradiction.
case g = S(e, t) shown similarly way.
366

fiDL-Lite Contraction Revision

shown \ contain assertion form A(d), A(e), R(d, e),
R(e, d), R(d, s) R(e, s). Thus, (I , d, e) = (A , d, e) = .

Proof Theorem 5
|T | =
6 model |T |. Let d, e = (I, d, e).
Proposition 5, |T |tr . is, |T |tr 6= . Conversely, suppose |T |tr 6= , let |T |tr .
Proposition 6, model . is, |T | 6= . |T | |T |tr empty,
statement trivially holds. follows, assume |T | |T |tr non-empty.
direction, want show 6|= |T |tr 6 ||tr . Then,
model satisfy . Similar proof Theorem 1,
assume w.l.o.g. contains single TBox axiom. axiom form
B v C. Then, domain element B 6 C . Let
= (I, d, d). Since model , Proposition 5, |T |tr . C basic
concept, B implies B 6 C , turn implies C 6 . C =
negated basic concept, similar argument, B implies . is,
propositionally satisfy B v C. condition 1 Definition 3, 6 ||tr . Suppose
form R v E. Then, domain elements d, e (d, e) RI
(d, e) 6 E . Let = (I, d, e). Again, Proposition 5, |T |tr . E role,
R implies (d, e) RI (d, e) 6 E , turn implies E 6 . E =
negated role, similar argument, R implies . is,
propositionally satisfy R v E. condition 1 Definition 3, 6 ||tr . shown
cases |T |tr 6 ||tr .
direction, want show |T |tr 6 ||tr 6|= . Since

|T |r 6 ||tr , t-type |T |tr 6 ||tr . Proposition 6, exist
model domain elements d, e (I, d, e) = . need show
model . Suppose otherwise, model , Proposition 5,
must t-model , contradicts fact 6 ||tr . Hence, model
, shown 6|= .

presenting proof Theorem 6, first show Lemma 6 Lemma 7.
two lemmas extend Lemma 3 Lemma 4 respectively DL-LiteR .
Lemma 6 two DL-LiteR TBox T1 T2 , role-complete set t-types
|T1 |tr |T2 |tr , holds kcln(T1 T2 )ktr .
Proof : Towards contradiction, suppose exists t-type NI
cln(T1 T2 ) propositionally satisfy . show t-type
exists violates NI T1 T2 , contradicts fact |T1 |tr |T2 |tr .
Similar proof Lemma 5, prove induction. present
case NI cln(T1 T2 ), case copy NI cln(T1 T2 ),
i.e., cln(T1 T2 ) \ cln(T1 T2 ), shown similarly. Without loss generality,
assume axioms added cln(T1 T2 ) incrementally according definition
copies (e.g., B 0 v C 0 ) added immediately original axioms (B v B) added.
initialization, T1 T2 violates NI T1 T2 . induction
steps, show added cln(T1 T2 ) due another axiom already
367

fiZhuang, Wang, Wang, & Qi

cln(T1 T2 ) , show 0 exists violates . proof cases (1)(4)
proof Lemma 5, simply let 0 = .
cases (5), suppose R v R, added due NI R v R (or R v R)
cln(T1 T2 ) . Note (R )0 v (R )0 (resp., R0 v R0 ) also cln(T1 T2 ) . Then,
fact propositionally satisfy , R . role balance, exists
0 R 0 R0 0 . R 0 , conditions 4 5 Definition 3, (R )0 0 ,
0 violates NI (R )0 v (R )0 (resp., R v R). R0 0 , conditions 4 5
Definition 3, R 0 R 0 , 0 violates NI R v R (resp.,
R0 v R0 ).
case (6), suppose R v R, added due NI R v R (or R v R )
cln(T1 T2 ) . Note (R)0 v (R)0 (resp., (R )0 v (R )0 ) also cln(T1 T2 ) .
Then, fact propositionally satisfy , R . conditions 4
5 Definition 3, R (R )0 . Hence, violates NI R v R (resp.,
(R )0 v (R )0 ).

Lemma 7 Let set t-types 1 , 2 two conjunctions DL-LiteR TBox
axioms. Suppose role-complete, |1 |tr |2 |tr implies |1 2 |tr .
Proof : , want show |1 2 |tr . end, construct
model 1 2 way proof Proposition 6. Let Ti
set axioms = 1, 2, constructed way proof
Proposition 6, take = chaseT1 T2 (A ). show model T1 T2
(I , d, e) = similar way proof Proposition 6 (using Lemma 6 instead
Lemma 5), except cases (1) (iv) (2).
case (1) (iv), suppose violates R v R cln(T1 T2 ). Different
proof Proposition 6, cannot assume R v R either cln(T1 ) cln(T2 ), thus
cannot apply condition 2 Definition 3. Yet still derive contradiction. violates
R v R R R . former case, propositionally satisfy
R v R. is, 6 kcln(T1 T2 )ktr , violates Lemma 6. latter case, since
role-complete, R 0 t-type 0 . Hence, 0 propositionally
satisfy R v R, violates Lemma 6.
case (2), suppose violates R v cln(T1 T2 ). case {R, S} ,

{R , } , R = {R, R , (R)0 , (R )0 } 6= . first two cases
shown way proof Proposition 6. third case R = S,
different proof Proposition 6, cannot assume R v R either cln(T1 )
cln(T2 ), thus cannot apply condition 2 Definition 3. Note facts
{R, R , (R)0 , (R )0 } 6= role-complete, exists t-type
0 R 0 R0 0 . R 0 0 propositionally satisfy R v R;
otherwise R 0 , 0 propositionally satisfy R0 v R0 . cases, Lemma 6
violated.
Now, shown model T1 T2 (I , d, e) = . Proposition 5,
|1 2 |tr .

Proof Theorem 6
368

fiDL-Lite Contraction Revision

theorem proved similarly Theorem 4, proof based Lemmas 6
7.

Proof Proposition 8
ct-type |T |tc , Proposition 3, model
(I, d) = . Let 0 t-type 0 = (I, d, d). Then,
definitions (I, d) (I, d, d), = 0 B. Also, Proposition 5, 0 |T |tr . is,
{ B | |T |tr }, hence |T |tc { B | |T |tr }.
Conversely, ct-type { B | |T |tr }, t-type 0 |T |tr
= 0 B. Proposition 6, model d, e
(I, d, e) = 0 . easy see = (I, d) definitions (I, d) (I, d, e).
Proposition 2, |T |tc . is, { B | |T |tr } |T |tc .

following theorem generalises Theorem 2 DL-LiteR .
Theorem 14 Let DL-LiteR TBox = {1 , . . . , n }. coherent
|T |tr = |1 |tr |n |tr .
Proof : |T |tr , satisfies conditions 4 5 Definition 3. Also, = 1, . . . , n,
condition 1 Definition 3 w.r.t. , propositionally satisfies . is, satisfies
condition 1 w.r.t. . Further, coherent, monotonicity DL-Lite, exists
R R |= R v , trivially satisfies condition 2 w.r.t. . Moreover,
|= R v S, |= R v S, condition 3 Definition 3 w.r.t. , satisfies
condition 3 w.r.t. . Hence, |i |tr = 1, . . . , n. is, |T |tr |1 |tr |n |tr .
Conversely, |1 |tr |n |tr , construct model
way proofs Proposition 6 Lemma 7 induces .
Proposition 5, |T |tr . is, |1 |tr |n |tr |T |tr .

Proof Proposition 9
direction, since model , suffices show model A,
i.e., concept assertion A(a) A, aI AI , role assertion P (a, b) A,
(aI , bI ) P . Let = (I), kAT kar propositionally satisfies Aa . is,
Aa . definition , aI AI . Similarly, propositionally satisfies P ab
P ab , hence (aI , bI ) P . shown |K|.
direction, need show second half statement since
|K| implies |T |. Let = (I), want show |AT |ar . condition 1
Definition 4, show satisfies way proof Proposition 5.
A(a) P (a, b) A, since aI AI (aI , bI ) P , Aa
P ab . is, propositionally satisfies A. shown kAT kar . Further,
shown satisfies conditions 25 Definition 4 similar manner
proof Proposition 5 (roughly, replacing Definition 3 Definition 4,
D, e b D, B B , B 0 B b , R Rab , R (R)a , (R)0
(R)b , on).

presenting proof Proposition 10, first show Lemma 8. lemma
proved manner Lemma 5. Note cln(T )a TBox consists
369

fiZhuang, Wang, Wang, & Qi

copy concept inclusion cln(T ) individual D, copy
role inclusion cln(T ) pair individuals D.
Lemma 8 DL-LiteR KB K = (T , A), a-model propositional model
cln(T )a .
Proof Proposition 10
Similar before, construct interpretation using chase: Let NCa
set concept names B (with D),
Aa ={A(a) | D, Aa NCa } {R(a, b) | a, b D, Rab Rab }
{R(a, fa,R ) | D, (R)a B , Rab 6 b D, fa,R fresh constant}.
Take = chaseT (Aa ). want show model K (I ) = .
show former, first show Aa . concept assertion A(a) A,
a-model K, propositionally satisfies Aa . is, Aa hence A(a) Aa .
Similarly, role assertion P (a, b) A, P ab P (a, b) Aa . shown
Aa . show model K, want show model
(T , Aa ). Lemma 1, need show Aa model cln(T ).
shown similar way proof Proposition 6 (roughly, replacing Definition 3
Definition 4, Lemma 5 Lemma 8, D, e b D, B
B , B 0 B b , R Rab , R (R)a , (R)0 (R)b , on).
remains show (I ) = . Again, need show (I )

(Aa ). shown similar way proof Proposition 6.

Proof Theorem 7
|K| =
6 model |K|. Let = (I). Proposition 9, |AT |ar .
is, |AT |ar 6= . Conversely, suppose |AT |ar 6= , let |AT |ar . Proposition 10,
model K. is, |K| 6= . |K| |AT |ar empty, statement
trivially holds. follows, assume |K| |AT |ar non-empty.
direction, want show K 6|= |AT |ar 6 ||ar . Then,
model K satisfy . Let = (I). Proposition 9,
|AT |ar . Similar proof Theorem 1, assume w.l.o.g. contains
single ABox assertion. Suppose form A(a). Then, aI 6 AI . definition
(I), Aa 6 , hence 6 kkar . condition 1 Definition 4, 6 ||ar . Suppose
form P (a, b). Then, (aI , bI ) 6 P . Again, definition (I), P ab 6 ,
hence 6 ||ar . cases, |AT |ar 6 ||ar .
direction, want show |AT |ar 6 ||ar K 6|= . Since
|AT |ar 6 ||ar , a-type |AT |ar 6 ||ar . Proposition 10,
exist model K (I) = . need show model
. Suppose otherwise, model , Proposition 9, must a-model
, contradicts fact 6 ||ar . Hence, model , shown
K 6|= .

Proof Theorem 8
Let = {A(a) | Aa } {P (a, b) | P ab }.
want show unique corresponding ABox w.r.t. .
370

fiDL-Lite Contraction Revision

first show corresponding ABox. show |AT |ar , need
show a-type , satisfies conditions 15 Definition 4. consistent
, |T |ar . Hence, propositional model , satisfies conditions 25.
Also, construction A, satisfies Aa A(a) satisfies P ab
P (a, b) A. is, kAT kar . shown |AT |ar .
Further, ABox A0 |A0T |ar a-type , since
kA0T kar , satisfies Aa concept assertion A(a) A0 satisfies P ab
role assertion P (a, b) A0 . Note holds a-type . construction
A, A0 A. is, |AT |ar |A0T |ar . Thus, corresponding ABox. Also, based
observation, corresponding ABox must equivalent A.

following theorem proved similar Theorem 14. Since one TBox
concerned, proof require Lemmas 6 7.
Theorem 15 Let K = (T , A) DL-LiteR KB = {1 , . . . , n }.
|AT |ar = |{1 }T |ar |{n }T |ar .
Proof Proposition 12
Let K = (T , A). direction, suffices show (i)
concept inclusion B v C , B implies C ; (ii) e (not
necessarily 6= e) role inclusion R v E , (d, e) RI implies (d, e) E ;
(iii) concept assertion A(a) A, aI AI , role assertion P (a, b) A,
(aI , bI ) P . Let = (I, d, e) = (I, d, e) (I). Conditions (i) (ii) shown
proof Proposition 5. is, |T |. Then, condition (iii) shown proof
Proposition 9.
direction, let = (I, d, e) arbitrary d, e . is,
= (I, d, e) (I). want show (I, d, e) |T |tr (I) |AT |ar ,
shown proofs Propositions 5 9, respectively.

Proof Proposition 13
Let K = (T , A). construct interpretation using chase: Let

defined proofs Propositions 6 10, = chaseT (A Aa ).
want show model K (I , d, e) = . show former,
Aa proof Proposition 10, need show model
(T , Aa ). Lemma 1, suffices show Aa model cln(T ),
shown proofs Propositions 6 10. show later, (I , d, e) = ,
need show (I , d, e) (A , d, e). definition (I , d, e),
suffices show (I , d, e) (A , d, e) (I ) (Aa ),
shown proofs Propositions 6 10.

Proof Theorem 9
shown way proof Theorem 7 |K| empty
|K|r empty. follows, assume |K| |K|r non-empty.
direction, want show K 6|= |K|r 6 ||r . Then,
model K satisfy . Similar proof Theorem 1,
assume w.l.o.g. contains single TBox axiom single ABox assertion. Suppose
371

fiZhuang, Wang, Wang, & Qi

form B v C R v E, way proof Theorem 5,
construct = (I, d, e) d, e |K|r 6 ||r . Suppose
form A(a) P (a, b), way proof Theorem 7, show
|K|r 6 ||r .
direction shown way proof Theorem 7.

presenting proof Theorem 10, first show Lemma 9 Lemma 10.
two extend Lemmas 6 7 respectively DL-LiteR KBs.
Lemma 9 two DL-LiteR TBox T1 T2 , role-complete set types
|T1 |r |T2 |r , types must satisfy cln(T1 T2 ) cln(T1 T2 )a .
Proof : Towards contradiction, suppose exists type NI cln(T1
T2 ) cln(T1 T2 )a propositionally satisfy . show type
exists violates NI T1 T2 T1a T2a , contradicts fact
|T1 |r |T2 |r . shown similar way proof Lemma 6.
axiom cln(T1 T2 ) concerned, proof Lemma 6. axiom
cln(T1 T2 )a concerned, proof adapted replacing B B , B 0 B b , R
Rab , R (R)a , (R)0 (R)b , on.

Lemma 10 Let set types 1 , 2 two conjunctions DL-LiteR axioms.
Suppose role-complete, |1 |r |2 |r implies |1 2 |r .
Proof : type , want show |1 2 |r . end,
construct model 1 2 way proof Proposition 13. Let
sets respectively TBox axioms ABox axioms 1 2 ,
set ABox axioms Aa proof Proposition 13, take
= chaseT (A Aa ). show model (T , A) (I , d, e) =
similar way proofs Lemma 7 Proposition 13. Proposition 12,
|1 2 |r .

Proof Theorem 10
theorem proved similarly Theorem 4 proof based Lemma 9
Lemma 10.

following theorem proved similar Theorem 14.
Theorem 16 Let K DL-LiteR KB K = {1 , . . . , n }. K coherent
|K|r = |1 |r |n |r .
Proof Theorem 11
.
one direction, suppose T-contraction function TBox as.
.
.
.
sociated selection function . need show satisfies (T 1)(T 4), (T de),
.
.
.
.
.
(T 6). (T 1), (T 2), (T 4) (T 6) follow directly definition T-contraction
.
.
function. show proof (T 3) (T de).
372

fiDL-Lite Contraction Revision

.
(T 3): Suppose 6|= . |T |tr 6 ||tr implies |T |tr ||tr 6= . Thus
.
faithfulness , (||tr ) |T |tr . Thus = Tr (|T |tr (||tr )) = Tr (|T |tr ) = .
.
.
(T de): prove contrapositive. Suppose |= 6|= .
.
.
.
|T |tr ||tr |T |tr 6 ||tr . remains show |T |tr 6 ||tr ||tr . Assume |T |tr
.
||tr ||tr . Since definition T-contraction function, |T |tr (||tr ) |T |tr ,
|T |tr (||tr ) ||tr ||tr implies (||tr ), ||. Thus
|T |tr (||tr )) ||tr . follows definition corresponding TBoxes
.
|T |tr (||tr )) |T |tr ||tr , contradiction!
.
.
direction, suppose function TBox satisfies (T 1)
.
.
.
(T 4), (T de), (T 6). Let defined
.
(||tr ) = ||tr |T |tr
conjunctions TBox axioms . set t-types , conjunction
TBox axiom ||tr = , define (M ) = |T |tr whenever |T |tr 6=
otherwise. need show (1) faithful selection function (2)
.
= Tr (|T |tr (||tr )).
Part (1): faithful selection function, function first.
.
follows directly definition (T 6).
prove selection function, suppose = . need show (M ) = .
TBox axiom tautology, ||tr = = . Thus (M ) = (||tr ) =
.
||tr |T |tr = . suppose 6= . need show (M ) 6= . definition
, result trivially holds conjunction TBox axioms ||tr = .
.
||tr = , since ||tr 6= implies 6|= , follows (T 4)
.
.
|T |tr ||tr 6= . Thus (M ) = (||tr ) = ||tr |T |tr 6= .
prove faithful respect , suppose |T |tr 6= . need show
(M ) = |T |tr . Again, result trivially holds conjunction TBox axioms
||tr = . ||tr = , since ||tr |T |tr 6=
.
.
implies 6|= , follows (T 3) |T |tr = |T |tr . Thus (M ) = (||tr ) =
.
||tr |T |tr = ||tr |T |tr .
.
.
Part (2): Since (T 1) implies closed Tr function returns closed
.
TBoxes, suffices show |Tr (|T |tr (||tr ))|tr = |T |tr .
.
.
.
follows (T 2) implies |T |tr |T |tr . follows
.
.
definition (||tr ) |T |tr . |T |tr (||tr ) |T |tr implies
.
definition corresponding TBoxes |Tr (|T |tr (||tr ))|tr |T |tr .
.
remains show |T |tr |Tr (|T |tr (||tr ))|tr . Assume contrary
.
|T |r 6 |Tr (|T |tr (||tr ))|tr . Let conjunction TBox axioms
.
.
||tr = |Tr (|T |tr (||tr ))|tr . |= 6|= . follows (T de)
.
.
|T |r 6 ||tr ||tr = ||tr |Tr (|T |tr (||tr ))|tr . Let u |T |tr , u ||tr
u ||tr |Tr (|T |tr (||tr ))|tr u ||tr , definition ,
.
u (||tr ). Thus either case, u ||tr |Tr (|T |tr (||tr ))|tr implies |T |tr
||tr |Tr (|T |tr (||tr ))|tr = ||tr ||tr , contradiction!

Proof Proposition 15
.
complexity results explained earlier. Let function
.
.
= TCONT (T , ) TBox conjunction axioms . need show
373

fiZhuang, Wang, Wang, & Qi

.
.
.
T-contraction function. Theorem 11 suffices show satisfies (T 1)(T 4),
.
.
.
.
.
(T de), (T 6). (T 2), (T 3) (T 6) trivially satisfied.
Let counter-model picked line 3 TCONT . Since axioms violet
.
.
removed line 6, ||tr . Suppose = {1 , . . . , n }.
.
.
Theorem 14, |1 |tr |n |tr = |T |tr implies |T |tr .
.
.
.
.
obvious 6|= , (T 4) satisfied. (T 1), suppose |= .
.
.
.
.

need show . Since |= implies |T |r ||r |T |r ,
.
||tr |=tr . Thus removed line 6 means .
.
.
(T de), suppose |T |tr ||tr ||tr . follows 6 ||tr
.
|T |r ||tr . Thus removed line 6 TCONT means
.
.

Proof Theorem 12
one direction, suppose T-revision function TBox associated
selection function . need show satisfies (T 1)(T 6) (T f ).
(T 1), (T 2), (T 6), (T f ) follow immediately definition T-revision.
show proof (T 3)(T 5).
(T 3): Suppose coherent. follows definition T-revision function
(||tr ) |T |tr . Since faithful respect , |T |tr ||tr (||tr ). Thus
|T |tr ||tr |T |tr implies cl(T {}).
(T 4): Suppose {} coherent. |T |tr ||tr coherent. follows
faithfulness (||tr ) = |T |tr ||tr . definition T-revision function,
(||tr ) |T |tr . |T |tr ||tr |T |tr implies = cl(T {}).
(T 5): Suppose coherent. Since coherent preserving, (||tr ) coherent.
definition T-revision function, (||tr ) |T |tr . Thus |T |tr also coherent
implies coherent.
direction, suppose function TBox satisfies (T 1)(T 6)
(T f ). Let defined
(||tr ) = |T |tr
conjunctions TBox axioms . set t-types , conjunction
TBox axioms ||tr = , define (M ) = incoherent; (M ) =
|T |tr |T |tr coherent; (M ) = coherent |T |tr incoherent.
need show (1) faithful coherent preserving selection function
(2) = Tr ((||tr )).
Part (1): faithful coherent preserving selection function,
function first. follows directly definition (T 6). Let
set t-types. Suppose coherent. need show (M ) 6= . definition ,
result trivially holds conjunction TBox axioms ||tr = .
||tr = , (T 5) |T |tr coherent.
(||tr ) = |T |tr 6= .
Suppose incoherent. need show (M ) = . definition , result
trivially holds conjunction TBox axioms ||tr = .
||tr = , follows (T f ) |T |tr = . Thus (||tr ) = |T |tr = .
374

fiDL-Lite Contraction Revision

faithfulness, suppose coherent. need show |T |tr (M ).
definition , result trivially holds conjunction TBox axioms
||tr = . ||tr = , follows (T 3)
cl(T {}) implies |T |tr ||tr |T |tr . Since (||tr ) = |T |tr ,
|T |tr ||tr (||tr ). suppose |T |tr coherent. need
show (M ) = |T |tr . result trivially holds conjunction
TBox axioms ||tr = . ||tr = , follows
(T 3) (T 4) = cl(T {}) implies |T |tr ||tr = |T |tr . Since
(||tr ) = |T |tr , |T |tr ||tr = (||tr ).
coherent preserving, suppose coherent. need show (M ) coherent.
result trivially holds conjunction TBox axioms
||tr = . ||tr = , follows (T 5)
coherent implies |T |tr coherent. Since (||tr ) = |T |tr , (||tr ) coherent.
Part (2): definition , (||tr ) = |T |tr . Since follows (T 1)
closed, definition Tr = Tr (|T |tr ) = Tr ((||tr )).

Proof Proposition 16
complexity results explained earlier. Let function
= TREVI (T , ) TBox conjunction TBox axioms . need show
T-revision function. Theorem 12, suffices show satisfies (T 1)(T 6)
(T f ). (T 1), (T 2), (T 6), (T f ) trivially satisfied.
(T 3), {} inconsistent cl(T {}) includes axioms thus
postulates holds trivially. suppose {} consistent. |T {}|tr 6=
|T {}|tr |T |tr . new axiom added throughout TREVI , |T
{}|tr |REV I(T , )|tr implies TREVI (T , ) cl(T {}). (T 4), suppose
{} coherent. condition line 4 never fulfilled thus axioms get
removed means TREVI (T , ) = cl(T {}). focus (T 5). Given
t-type atomic concept role F , definition t-model F
6|=tr B v . hard see {1 , . . . , n } (|T |tr ||tr ) |TREVI (T , )|tr
t-models picked line 5 TREVI . Due line 48 TREVI , F
|T |tr ||tr |=tr F v {1 , . . . , n } F means
F {1 , . . . , n } (|T |tr ||tr ) 6|=tr F v implies |TREVI (T , )|tr 6|=tr F v .
Thus TREVI (T , ) coherent.

Proof Theorem 13
one direction, suppose A-revision function associated selection
function . need show satisfies (A 1)(A 6), (A f ).
(A 1), (A 2), (A 6), (A f ) follow immediately definition A-revision
function. show proof (A 3)(A 5).
(A 3), (A 4): |(A {})T |ar = , two postulates hold trivially. suppose
|(A {})T |ar 6= implies |AT |ar |{}T |ar 6= . Since faithful, |AT |ar
|{}T |ar = (|{}T |ar ). follows definition A-revision function
= ATr ((|{}T |ar )) = ATr (|AT |ar |{}T |ar ) = cl(AT {}).
(A 5): Suppose |{}T |ar 6= . definition , (|{}T |ar ) 6= . Since
follows definition A-revision function (|{}T |ar ) |AT |ar , |AT |ar 6= .
375

fiZhuang, Wang, Wang, & Qi

direction, suppose function ABox satisfies (A 1)
(A 6), (A f ). Let defined
(|{}T |ar ) = |(AT )T |ar
conjunctions ABox axioms . set a-types , conjunction
ABox axioms |{}T |ar = , define (M ) = |AT |ar whenever
|AT |ar 6= otherwise. need show (1) faithful selection function
(2) = ATr ((|{}T |ar )).
Part (1): faithful selection function, function first.
follows directly definition (A6). Let set a-types. Suppose 6=
. need show (M ) 6= . definition , result trivially holds
conjunction ABox axioms |{}T |ar = . |{}T |ar = ,
(A 5) |(AT )T |ar 6= . (|{}T |ar ) = |(AT )T |ar 6= .
Suppose = . need show (M ) = . axiom inconsistent
, |{}T |ar = . follows (A f ) case |(AT )T |ar = , thus
(|{}T |ar ) = |(AT )T |ar = . faithfulness, suppose |AT |ar 6= , need show
(M ) = |AT |ar . result holds trivially |{}T |ar = .
|{}T |ar = , |AT |ar |{}T |ar 6= implies
|A {}T |ar 6= . follows (A 3) (A 4) |clT (A {}T )|ar = |AT |ar .
Thus (|{}T |ar ) = |(AT )T |ar = |cl(A {}T )|ar = |AT |ar |{}T |ar .
Part (2): definition , (|{}T |ar ) = |(AT )T |ar . Since follows
(A1) closed, definition ATr = ATr (|(AT )T |ar ) =
ATr ((|{}T |ar )).

Proof Proposition 17
complexity results explained earlier. Let function
= AREVI (AT , ) ABox conjunction ABox axioms . need
show A-revision function. Theorem 13, suffices show satisfies (A1)(A6)
(A f ). (A 1), (A 2), (A 6), (A f ) trivially satisfied.
(A 3), suppose inconsistent consistent (T , A). line 2
line 3 AREVI (T , A, ) guarantee postulate holds. suppose consistent
inconsistent (T , A). Since new axiom added lines 68, line
9 returned ABox must subset clT (AT {}). (A 4), suppose consistent
(T , A). line 4 AREVI (T , A, ) guarantees clT (AT {}) = .
(A 5), suppose consistent . consistent (T , A), ABox
returned line 4 must consistent, inconsistent (T , A), lines 68 guarantee
axioms inconsistent removed, thus ABox returned
line 9 also consistent.


References
Alchourron, C. E., Gardenfors, P., & Makinson, D. (1985). logic theory change:
Partial meet contraction revision functions. Journal Symbolic Logic, 50 (2),
510530.
376

fiDL-Lite Contraction Revision

Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2003).
Description Logic Handbook. CUP, Cambridge, UK.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
reasoning efficient query answering description logics: DL-Lite family.
Journal Automatic Reasoning, 39 (3), 385429.
Calvanese, D., Kharlamov, E., Nutt, W., & Zheleznyakov, D. (2010). Evolution DL-Lite
knowledge bases. Proceedings 9th International Semantic Web Conference
(ISWC-2010), pp. 112128.
Dalal, M. (1988). Investigations theory knowledge base revision. Proceedings
7th National Conference Artificial Intelligence (AAAI-1988), pp. 475479.
De Giacomo, G., Lenzerini, M., Poggi, A., & Rosati, R. (2009). instance-level update
erasure description logic ontologies. Journal Logic Computation, 19 (5),
745770.
Ferme, E., Krevneris, M., & Reis, M. (2008).
axiomatic characterization
ensconcement-based contraction. Journal Logic Computation, 18 (5), 739753.
Flouris, G., Huang, Z., Pan, J. Z., Plexousakis, D., & Wache, H. (2006). Inconsistencies,
negations changes ontologies. Proceedings 21st National Concference
Artificial Intelligence (AAAI-2006).
Gardenfors, P. (1988). Knowledge Flux: Modelling Dynamics Epistemic States.
MIT Press.
Grau, B. C., Ruiz, E. J., Kharlamov, E., & Zhelenyakov, D. (2012). Ontology evolution
semantic constraints. Proceedings 13th International Conference
Principles Knowledge Representation Reasoning (KR-2012), pp. 137147.
Hansson, S. O. (1991). Belief Contraction Without Recovery. Studia Logica, 50 (2), 251260.
Hansson, S. O. (1994). Kernel contraction. Journal Symbolic Logic, 59 (3), 845859.
Hansson, S. O. (1997). Semi-revision. Journal Applied Non-Classical Logics, 7 (1-2),
151175.
Hansson, S. O. (1999). Textbook Belief Dynamics Theory Change Database Updating. Kluwer.
Hansson, S. O., & Wassermann, R. (2002). Local change. Studia Logica, 70 (1), 4976.
Kalyanpur, A., Parsia, B., Sirin, E., & Cuenca-Grau, B. (2006). Repairing unsatisfiable
concepts OWL ontologies. Proceedings 3rd European Semantic Web Conference (ESWC-2006), pp. 170184.
Katsuno, H., & Mendelzon, A. O. (1991). difference updating knowledge
base revising it. Proceedings 2nd International Conference Principles
Knowledge Representation Reasoning (KR-1991), pp. 387394.
Katsuno, H., & Mendelzon, A. O. (1992). Propositional knowledge base revision minimal change. Artificial Intelligence, 52 (3), 263294.
377

fiZhuang, Wang, Wang, & Qi

Kharlamov, E., & Zheleznyakov, D. (2011). Capturing instance level ontology evolution
DL-Lite. Proceedings 10th International Semantic Web Conference (ISWC2011), pp. 321337.
Kharlamov, E., Zheleznyakov, D., & Calvanese, D. (2013). Capturing model-based ontology
evolution instance level: case DL-Lite. Journal Computer System
Sciences, 79 (6), 835872.
Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2010). Logic-based ontology comparison
module extraction, application DL-Lite. Artificial Intelligence, 174 (15),
10931141.
Lenzerini, M., & Savo, D. F. (2011). evolution instance level DL-Lite
knowledge bases. Proceedings 24th International Workshop Description
Logics (DL-2011).
Lenzerini, M., & Savo, D. F. (2012). Updating inconsistent description logic knowledge
bases. Proceedings 20th European Conference Artificial Intelligence
(ECAI-2012), pp. 516521.
Levi, I. (1991). Fixation Beliefs Udoing. Cambridge University Press.
Pan, J. Z., & Thomas, E. (2007). Approximating OWL-DL ontologies. Proceedings
22nd National Conference Artificial Intelligence (AAAI-2007), pp. 14341439.
Qi, G., & Du, J. (2009). Model-based revision operators terminologies description
logics. Proceedings 21st International Joint Conferences Artificial Intelligence (IJCAI-2009), pp. 891897.
Qi, G., Haase, P., Huang, Z., Ji, Q., Pan, J. Z., & Volker, J. (2008). kernel revision operator
terminologies algorithms evaluation. Proceedings 7th International
Semantic Web Conference (ISWC-2008), pp. 419434.
Qi, G., Liu, W., & Bell, D. A. (2006). Knowledge base revision description logics.
Proceedings 10th European Conference Logics Artificial Intelligence
(JELIA-2006), pp. 386398.
Ribeiro, M. M., & Wassermann, R. (2009). Base revision ontology debugging. Journal
Logic Computation, 19 (5), 721743.
Ribeiro, M. M., Wassermann, R., Flouris, G., & Antoniou, G. (2013). Minimal change:
Relevance recovery revisited. Artificial Intelligence, 201, 5980.
Satoh, K. (1988). Nonmonotonic reasoning minimal belief revision. Proceedings
International Conference Fifth Generation Computer Systems, pp. 455462.
Wang, Z., Wang, K., & Topor, R. W. (2015). DL-Lite ontology revision based
alternative semantic characterization. ACM Transaction Computational Logic,
16 (4), 31:131:37.
Winslett, M. (1990). Updating Logical Databases. Cambridge University Press.
Zhuang, Z., Wang, Z., Wang, K., & Qi, G. (2014). Contraction revision DLLite TBoxes. Proceedings 28th AAAI Conference Atificial Intelligence
(AAAI-2014), pp. 11491156.

378

fiJournal Artificial Intelligence Research 56 (2016) 89

Submitted 10/15; published 05/16

Optimal Any-Angle Pathfinding Practice
Daniel Harabor

daniel.harabor@nicta.com.au

University Melbourne
National ICT Australia, Victoria Laboratory
115 Batman St, Melbourne, 3003, Australia

Alban Grastien

alban.grastien@nicta.com.au

National ICT Australia, Canberra Laboratory
7 London Circuit, Canberra, 2601, Australia

Dindar Oz

dindar.oz@yasar.edu.tr

Yasar University
Bornova, Izmir, 35100, Turkey

Vural Aksakalli

aksakalli@sehir.edu.tr

Istanbul Sehir University
Altunizade, Istanbul, 34662, Turkey

Abstract
Any-angle pathfinding fundamental problem robotics computer games.
goal find shortest path pair points grid map
path artificially constrained points grid. Prior research focused
approximate online solutions. number exact methods exist require
super-linear space pre-processing time. study, describe Anya: new
optimal any-angle pathfinding algorithm. works find approximate any-angle
paths searching individual points grid, Anya finds optimal paths
searching sets states represented intervals. interval identified on-thefly. interval Anya selects single representative point uses compute
admissible cost estimate entire set. Anya always returns optimal path
one exists. Moreover without offline pre-processing introduction
additional memory overheads. range empirical comparisons show Anya
competitive several recent (sub-optimal) online pre-processing based techniques
order magnitude faster common benchmark algorithm,
grid-based implementation A*.

1. Introduction
Any-angle pathfinding common navigation problem robotics computer games.
takes input pair points uniform two-dimensional grid asks shortest
path artificially constrained points grid. anyangle paths desirable compute typically shorter grid-constrained
counterparts following trajectory give appearance realism
intelligence; e.g. player computer game. Despite apparent simplicity anyc
2016
AI Access Foundation. rights reserved.

fiHarabor, Grastien, Oz & Aksakalli

angle pathfinding surprisingly challenging. far many successful popular methods
proposed, yet involve trade-offs kind. begin
examples highlight, broad strokes, main research trends limitations,
date.
communities Artificial Intelligence Game Development any-angle
pathfinding problem often solved efficiently using technique known string pulling.
idea compute grid-optimal path smooth result; either part
post-processing step (e.g. Pinter, 2001; Botea, Muller, & Schaeffer, 2004) interleaving
string pulling online search (e.g. Ferguson & Stentz, 2005; Nash, Daniel, Koenig, &
Felner, 2007). Regardless particular approach, string pulling techniques suffer
disadvantages: (i) require computation finding path
and; (ii) yield approximately shortest paths.
communities Robotics Computational Geometry related general problem well-studied: finding Euclidean shortest paths polygonal
obstacles plane. Visibility Graphs (Lozano-Perez & Wesley, 1979) Continuous Dijkstra paradigm (Mitchell, Mount, & Papadimitriou, 1987) among best known
influential techniques originate line research. Even though
methods optimal efficient practice nevertheless suffer
often undesirable properties: (i) search graph1 must pre-computed offline
pre-processing step; (ii) map changes point search graph invalidated
must recomputed, usually scratch.
date, clear exists any-angle pathfinding algorithm simultaneously online, optimal also practically efficient (i.e. least fast practice
grid-based pathfinding using A* search). manuscript, present new work
answers open question affirmative introducing new any-angle pathfinding
algorithm called Anya. approach bears similarity existing works
literature, notably algorithms based Continuous Dijkstra paradigm.
rough overview:
methods search individual nodes grid, Anya searches
contiguous sets states form intervals.
Anya interval single representative point used derive admissible cost estimate (i.e f -value) points set.
progress search process Anya projects interval, one row grid
onto another, target reached.
Anya always finds optimal any-angle path, one exists. addition Anya
rely pre-computation introduce memory overheads (in form
auxiliary data structures) beyond required maintain open closed list.
theoretical description algorithm previously appeared literature (Harabor
& Grastien, 2013). study extend work several ways: (i) give
1. distinguish search graph input grid map. Though contexts terms
coincide exactly true general. particular search graph may subset input
grid may related entirely separate data structure.

90

fiOptimal Any-Angle Pathfinding Practice

Visible

Visible

Non-visible

2

2

2

1

1

1

0

0
0

1

2

Non-visible

2
1

0
0

1

2

0

1

2

0
0

1

2

Figure 1: Examples visible non-visible pairs points.

detailed conceptual description Anya algorithm provide extended theoretical
argument optimality completeness; (ii) discuss practical considerations
arise implementing algorithm give technical description one possible
efficient implementation; (iii) make detailed empirical comparisons showing
Anya competitive range recent sub-optimal techniques literature,
including based offline pre-processing, one order magnitude better
benchmark grid-based implementation A*; (iv) discuss range possible
extensions improving current results.

2. Optimal Any-Angle Pathfinding Problem
grid planar subdivision consisting W H square cells. cell open set
interior points traversable non-traversable. vertices associated
cell called discrete points grid. Edges grid interpreted
open intervals intermediate points; one representing transition two
discrete points. type point p = (x, y) unique coordinate x [0, W ]
= [0, H], discrete points limited subset integer x values.
discrete intermediate point traversable adjacent least one traversable
cell. Otherwise non-traversable. discrete point common exactly four
adjacent cells called intersection. intersection three adjacent cells
traversable one called corner. Two points visible one another
connected straight-line path (i.e. sequence adjacent points, either
intermediate discrete) not: (i) pass non-traversable point
(ii) pass intersection formed two diagonally-adjacent non-traversable cells.
Figure 1 shows examples help better illustrate idea.
any-angle path sequence points hp1 , . . . , pk pi visible pi1
pi+1 . length cumulative distance every successive
pair points
p
0
0
0
0
d(p1 , p2 )+. . .+d(pk1 , pk ). function d(p = (x, y), p = (x , )) = (x x )2 + (y 0 )2
uniform Euclidean distance metric. say pi turning point segments
(pi1 , pi ) (pi , pi+1 ) form angle equal 180 2 . Finally, any-angle pathfinding
problem one requires input pair discrete points, t, asks anyangle path connecting them. point designates source (equivalently, start) location
2. well-known turning points optimal any-angle paths corner points; e.g. shown
Mitchell et al. (1987).

91

fiHarabor, Grastien, Oz & Aksakalli

point designates target (equivalently, goal) location. path optimal
exists alternative any-angle path strictly shorter.
Figure 2 provides example optimal any-angle pathfinding problem.
seen source, target obstacles discrete positions however path
need follow grid. Notice also trajectory path appears much
realistic alternative restricted turning modulo 45 deg 90 deg.
4
3
2
1




0
0
1
2
3
4
5
6
7
8
Figure 2: Example any-angle pathfinding problem together solution.

3. Overview Anya
Consider any-angle instance shown Figure 3. example optimal path
needs first head towards corner point n change direction
toward target t. One possible approach solving problem involves computing
visibility graph: i.e., identifying pairs corners visible one another,
also visible start target locations, searching path
graph. main drawback case visibility graph quite large (up
quadratic size grid) expensive compute.
alternative approach, avoids overheads, solve problem online.
Unfortunately online search methods generally consider discrete points grid
immediate neighbours. example, expanding point common
generate neighbours: (1, 0), (2, 1), (3, 0) example
Figure3. A*

f -value
three neighbours is, respectively, 1 + 34 ' 6.83, 1 + 20 ' 5.47,
1 + 26 ' 6.1 (using Euclidean-distance
heuristic). comparison optimal


any-angle path cost 10 + 5 ' 5.4. Immediately see heuristic
hand satisfy one essential properties A* search: f -value
node always underestimate actual distance goal. Without
property A* guaranteed optimal.
issue described comes fact optimal path go
points (1, 0), (2, 1), (3, 0). Instead optimal path crosses row 1 point y1 ,
part search space. ensure optimality consider points
y1 rather discrete points grid. however many
points including e.g., points y10 (leading (3, 6)), apriori seems reasonable
candidate expansion, appear optimal path.
92

fiOptimal Any-Angle Pathfinding Practice

6
5



4
n

3
2

y2

y10

1

y1


0
0

1

2

3

4

5

6

Figure 3: pathfinding n, online algorithms A* Theta*
expand discrete points grid never intermediate points yi .
general need consider potential yi points defined fraction wh
h {0, . . . , H} w {1, . . . , W }. set quadratic n = min(W, H). understand why, consider Farey Sequence order n, sequence (ordered increasing
number) rational numbers 0 1 written fraction whose
denominator integer lower n. instance, Farey Sequence order n = 6
is: 0, 16 , 15 , 14 , 31 , 25 , 21 , 53 , 23 , 43 , 54 , 65 , 1. Notice 13 = 26 , explains length
sequence n(n + 1) 2; still asymptotic cardinality sequence known
2
3n
(Graham, Knuth, & Patashnik, 1989, ch. 9).
2
Since quadratic behaviour Farey Sequence makes impractical enumerate
potential yi points propose consider, instead individual points, set points
appear together part contiguous interval grid. example Figure 3
would consider points lying (0, 1) (3, 1), time
part single A* search node. framework need to:
define formally Anya search node,
define set successors search node,
define compute f -value search node,
prove optimality returned path,
terminate search path available,
ensure Anya algorithm efficient practice.
93

fiHarabor, Grastien, Oz & Aksakalli

4. Algorithm Description
section presents detail Anya algorithm properties. Since Anya
variant A* first present search space: search nodes, successors node
evaluation function used rank nodes search. give pseudo-code
description algorithm discuss properties. Improvements make Anya
efficient practice presented next section.
4.1 Anya Search Nodes
define notion interval, core Anya.
Definition 1 grid interval set contiguous pairwise visible points drawn
discrete row grid. interval defined terms endpoints b.
possible exception b, interval contains intermediate discrete
non-corner points.
definition, points interval share position, positive
integer. Moreover, x position points interval (including endpoints
b) rational number3 . use normal parentheses ( ) indicate
interval endpoint open square brackets [ ] indicate interval endpoint
closed. example, interval = (a, b] open (i.e. include)
closed (i.e. include) b.
Identifying intervals simple: row grid naturally divided maximally contiguous sets traversable non-traversable points. traversable set forms
tentative interval split, repeatedly necessary, corner points
end points intervals. Intervals also identified operation called projection. discuss procedure next sub-section. note intervals
produced way projection also non-discrete non-corner endpoints.
significant advantage Anya construct intervals on-the-fly. allows
us start answering queries immediately discrete start-target pair. Similar
algorithms, e.g. Continuous Dijkstra (Mitchell et al., 1987), require pre-processing step
queries answered single fixed start point.
Definition 2 search node (I, r) tuple r 6 point called root
interval point p visible r. represent start node itself,
set = [s] assume r located plane visible s; cost r
case zero.
A* search node, together parents, traditionally represents single path
defined travelling straight line points search nodes root
current node. Anya search node similarly defines paths obtained visiting
roots nodes ending interval current node. node therefore
represents many paths root search node always last (common) turning
3. per problem definition, every point (x, y) appearing optimal any-angle path belongs
Farey Sequence points rational.

94

fiOptimal Any-Angle Pathfinding Practice

point paths: always either root parent node one end
points parent interval.
Besides start node, treat special case, two types
search nodes: cone nodes flat nodes. example cone node shown Figure 4.
nodes characterised fact root r row
associated interval I. Notice example although interval = [a, b] maximal,
endpoints obstacles, corners indeed even discrete points
grid (here left endpoint (2.5, 4) right endpoint b (5.5, 4)). Examples
flat nodes shown Figure 5. two nodes are: ((a1 , b1 ], r) ((a2 , b2 ], r). Flat
nodes characterised fact root r row interval I.
Notice examples given a1 = r (resp. a2 = b1 ) excluded first (resp.
second) interval. semantics every search node current position located
somewhere interval reach point any-angle path whose
recent turning point r.
5

5


4

b

4

3

3

2

2

1

r = a1

b1 = a2

b2

1

r

0

0
0

1

2

3

4

5

6

Figure 4: Example cone search node.

0

1

2

3

4

5

6

Figure 5: Example two flat search nodes.

4.2 Searching Anya: Successors
successors search node n identified computing intervals sets traversable
points; row grid current node n rows immediately
adjacent. want guarantee point set reached root
n via local path taut. Taut simply means pull endpoints
path cannot make shorter. provide formal definition successor
discuss definition applied practice.
Definition 3 successor search node (I, r) search node (I 0 , r0 )
1. points p0 0 , exists point p local path hr, p, p0 taut;
2. r0 last common point shared paths hr, p, p0 i;
3. 0 maximal according points definition search node.
95

fiHarabor, Grastien, Oz & Aksakalli

first requirement (tautness) implies successor p0 0 reached
root current node r path locally optimal. use property
next subsection show Anya always finds globally optimal path one exists
all. third property, requiring successor interval maximal,
exists purpose practical efficiency: simply put, want arbitrarily
small arbitrarily many successors. Instead, make successor interval
large possible. second property two interpretations. r0 = r say
successor node observable. Similarly r0 = p say successor
non-observable. explore ideas turn.
5
v1

4

v2

u2

u3

v3
r0



3

=b
u1

2
1

r

0
0

1

2

3

4

5

6

7

Figure 6: Successors cone search node, n = ([a, b], r). five successors: ([v1 , v2 ], r) ((v2 , v3 ], r) observable ((r0 , u1 ], r0 ), ((v3 , u2 ), r0 ),
([u2 , u3 ], r0 ) not.

5
4


3
2

b

c



e

1
0
0

1

2

3

4

5

6

Figure 7: Successors flat search node, n = ((a, b], a). two successors: ((b, c], a)
observable ([d, e], b) not.
96

fiOptimal Any-Angle Pathfinding Practice

Algorithm 1 Computing successor set
1: function successors(n = (I, r))
. Takes input current node
2:
n start node
3:
return generate-start-successors(I = [s])
4:
end
5:
successors
6:
n flat node
7:
p endpoint farthest r
. Successor interval starts p
8:
successors generate-flat-successors(p, r)
. Observable successors
9:
p turning point taut local path beginning r
10:
successors successors generate-cone-successors(p, p, r) . Non-observable successors
11:
end
12:
else
. node flat, must cone
13:
left endpoint
14:
b right endpoint
15:
successors generate-cone-successors(a, b, r)
. Observable successors
16:
turning point taut local path beginning r
17:
successors successors generate-flat-successors(a, r)
. Non-observable
18:
successors successors generate-cone-successors(a, a, r)
. Non-observable
19:
end
20:
b turning point taut local path beginning r
21:
successors successors generate-flat-successors(b, r)
. Non-observable
22:
successors successors generate-cone-successors(b, b, r)
. Non-observable
23:
end
24:
end
25: end function

observable successor characterised fact points p0 0 visible
current root point r. case last common point shared local paths
form hr, p, p0 r. Observable successors computed projecting current interval
next row. projection identifies maximal interval Imax split
internal corner point point. interval produced split operation leads
new observable successor, successors share root point original
(parent) node. process illustrated Figure 6 interval = [a, b] projected
onto next row. projection identifies maximal observable interval Imax = [v1 , v3 ]
subsequently split create two observable successors: ([v1 , v2 ], r) ((v2 , v3 ], r).
comparison, non-observable successor characterised fact points
0
p 0 visible current root r. case local paths form
hr, p, p0 must pass (visibility obstructing) corner point whose identity r0 := p.
Figure 6 illustrates process computing non-observable successors. First,
non-observable points right current interval = [a, b], construct single
flat successor 0 = (b, u1 ] root r0 := b. Non-observable points also exist
left current interval local path point (from r a)
taut. non-observable successors found rows grid adjacent
current interval I. projecting corner endpoint b onto next row grid
construct two non-observable successors: ((v3 , u2 ), b) ([u2 , u3 ], b).
Algorithm 1 give overview procedure generates successor set
search node. overview sub-functions appearing Algorithm 1 given
97

fiHarabor, Grastien, Oz & Aksakalli

appendix. implementation straightforward, requiring nothing complicated
grid scanning operations linear projections.
important note stage Anya perform visibility checks
generation successor nodes. Visibility checks heart many contemporary online algorithms, including Theta* (Nash & Koenig, 2013), must determine
whether successor visible node (e.g. grand-parent node).
one hand visibility checks help Theta* et al. find shorter paths expand fewer nodes
traditional A* search. hand, computational overhead introduced
checks means run-times often larger A*. comparison Anya
projects interval I, one row grid next, process involves local
reasoning. particular determine projection Imax valid, invalid
needs clipped simply testing traversability cells located above,
left right current interval proposed Imax . elimination
visibility checks important practical advantage Anya. see Section 9,
Anya finds shorter paths online methods Theta* et al. also
usually much efficient terms running time.
illustrate Algorithm 1 using previous examples. Consider flat node ((a, b], a)
Figure 7. point p Line 7 set b observable flat successor ((b, c], a)
generated Line 8. Furthermore since b turning point (Line 9), interval
Imax = [d, e] considered. Since Imax contains interior corner points split
single non-observable cone successor (I = Imax , b) generated (Line 10).
Next, consider cone node ([a, b], r) Figure 6. First generate observable
successors (Line 15): interval [a, b] projected maximal interval Imax = [v1 , v3 ]
identified. Imax split internal corner point v2 leading two observable cone
successor nodes, (I1 = (v1 , v2 ], r) (I2 = (v2 , v3 ], r). Notice line-of-sight visibility
check required here. Next since b turning point look non-observable successors
well (Lines 20-22). flat successor ((b, u1 ], b) generated per previous example.
Meanwhile maximal (non-observable) cone interval Imax = (v3 , u3 ] also identified.
interval split internal corner point u2 resulting two non-observable cone
successor nodes, (I3 = (v3 , u2 ], b) (I4 = (u2 , u3 ], b).
Algorithm 1 treats start node (Lines 2-4) special case root point
located grid. successors start node (i) non-observable intervals
root (ii) found left right start location, row
immediately start location row immediately below.
4.3 Evaluating Anya Search Node
search procedure Anya, similarly A*, always expands promising node found far. therefore necessary evaluate root interval pair.
evaluation corresponds estimate f minimal length path
source target current interval. optimality condition A*
estimate optimistic (i.e. never larger actual optimal path length).
classical A* search node n corresponds single point p grid value
f (n) computed sum g(p), length path source p, h(p),
(under)estimation length shortest path p target.
98

fiOptimal Any-Angle Pathfinding Practice

search node n = (I, r) represents set points f value minimum f value
points node:
f (n) = inf f (s, r, p, t)
pI

f (s, r, p, t) (under)estimate shortest path r p.
noted that, set points p continuous potentially open,
minimum replaced infimum. Since points interval visible r,
value broken follows:
f (s, r, p, t) = g(r) + d(r, p) + h(p)
d(r, p) distance points r p.
Finding point interval minimises f value may seem like hard problem
since interval contains large number points want avoid generating
them. However straight-line distance heuristic h (h(p) = d(p, t)) makes easy isolate
point p minimises f value, thanks two simple geometric observations.
precise heuristics available could make harder find point p.
Lemma 1 Let r two points s.t. interval row row
rows r t. point p infimal f -value point
closest intersection straight-line path ht, ri row I.
line r intersects interval point p intersection.
Otherwise point p one endpoints interval. event precondition Lemma 1 satisfied, possible replace mirrored version t0
thus satisfy precondition. case described Lemma 2.
Lemma 2 mirrored point t0 target interval d(p, t) = d(p, t0 )
p I.
Lemma 2 trivial geometrical result. lemmas illustrated Figure 8.
4.4 Search Procedure
search procedure employed Anya presented Algorithm 2. follows pattern
A* uses priority queue, open, stores yet-to-be-expanded search nodes
ordered f value. node stores pointer parent. step search
Anya extracts best node open checks corresponding interval contains
target. event target found (Line 6) returned path sequence
root points constructed following back-pointers, current node start
location. target found current node expanded successors
added priority queue (Line 8). successors may considered redundant
safely discarded without insertion priority queue (Line 9). discuss
aspect algorithm Section 6; suffices know successors
optimal path. expansion process continues target found
open list exhausted, case algorithm returns failure (Line 14).
next sections prove fundamental properties algorithm:
correctness, optimality completeness.
99

fiHarabor, Grastien, Oz & Aksakalli

4

t1
t04

t2

3

t3

2



b

1

t4

0
0

1

2

r

3

4

5

6

Figure 8: illustration Lemmas 1 2. evaluate node n = ([a, b], r).
points t1 t04 correspond case row target intersects interval
I; t2 t3 not; t4 mirrored target t04 must used.

5. Correctness Optimality
section prove Anya correct always finds optimal path. particular
show (i) optimal path appears search space, (ii) target
expanded found optimal path, (iii) node search space
reached finite number steps. topics termination completeness
discussed Section 6.
begin analysis recalling search node n = (I, r) represents set potential
paths (from r r point p I). Following semantics say
n search node path r intersects .
Lemma 3 n = (I, r) search node optimal path then: either n contains
target n least one successor n0 also search node .
Proof: Start node: n start node = [s] r located grid. Additionally, n search node (hypothesis). Algorithm 1 (Line 3) scans traversable
Algorithm 2 Anya
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

input: Grid, source location s, target location
open {(I = [s], r0 )}
open empty
(I, r) pop(open)

return path to(I)
end
(I 0 , r0 ) successors(I, r)
prune(I 0 , r0 )
open open {(I 0 , r0 )}
end
end
end
return null

100

. Start node root r0 located grid

. Successor pruning

fiOptimal Any-Angle Pathfinding Practice

points grid visible adjacent s. points located
left right located rows immediately immediately
row s. Algorithm 1 assigns points interval 0 0
associated successor node root r0 = s. Every optimal path must pass
= [s] traversable points reached without
passing interval associated successor s. sufficient satisfy
lemma.
nodes: n arbitrary node 6 (if successors
done). definition r p (apriori unknown) intersection
interval I. two possibilities consider, depending whether
p turning point not. show cases successor n whose
interval 0 intersects , sufficient satisfy lemma.
Case 1 p turning point. Algorithm 1 (Lines 8 15) scans points
adjacent (straight-line) visible r I. point assigned
successor observable interval 0 root point r. Thus least one
successors n intersects every straight line path r p means
least one successor n intersects .
Case 2 p turning point. case p must corner endpoint I, otherwise
taut thus cannot optimal. Algorithm 1 (Lines 10, 17, 18, 21) scans
points adjacent reachable r p taut local path.
points located row p row immediately adjacent.
point assigned successor non-observable interval 0 root
r = p. process exhaustive points reachable taut local path,
r though p, must assigned interval. Thus must intersect least one
successors n.

Corollary 4 path source target, open list always
contains search node optimal path (or node currently processed).
Proof: induction.
base case: initial search node node path s.
inductive case: assume open list contains search node optimal path.
node removed expanded. node contain
target know Lemma one successor generated search node
optimal path. Therefore new search node optimal path inserted
open list.


Lemma 5 first expanded node contains target corresponds one optimal
path t.
101

fiHarabor, Grastien, Oz & Aksakalli

Proof: Sketch. First notice f -value node indeed minimal value
nodes interval, means f estimate () actual cost
target. Second notice that, given search node (I, r) successor (I 0 , r0 ),
point p0 0 , f -value p0 greater equal f -value point
p ( p = r0 r0 6= r; p intersection (r, p0 ) otherwise); f function
therefore monotonically increasing. Finally, f function search node (I, r)
length path I. Hence f function nodes representing sub-optimal
path eventually exceed optimal path distance, f function nodes
representing optimal path always remain value.


Lemma 6 target reachable Anya eventually expand node whose interval
includes target.
Proof: contradiction, assume Anya expand node whose interval includes
target. Lemma 5 know failure expand node means Anya
expand infinitely many nodes. shall prove implies f value
nodes unbounded and, therefore, target reachable.
search nodes (I 0 , r0 ), interval 0 different row parent
(I, r). Therefore, nodes, value g(p0 ) larger value g(p) 1 more.
happen node flat, bounded number
successive flat nodes.4 Hence infinite sequence successive Anya nodes infinite length. Finally Anya node bounded number successors, meaning
infinite number expansions generate infinite number successive nodes.

6. Completeness Termination
specified policy Anya detect nodes
previously expanded. context optimal A* search policy essential
prevent cyclical re-expansion ensure algorithm eventually terminates,
even path start target locations. section describe
policy Anya. Conceptually similar A* closed list approach works
tracking best g-value associated every root location (cf. every search node)
encountered search.
motivating example consider Figure 9 root r reached via two paths
different length. example green path strictly longer red path
points reached via green path g-value strictly larger
point reached via red path. Figure 10 shows similar example green
red paths reach root point r cost, resulting two identical copies
successor node (I, r). Without strategy handle root-level redundancies
search process generate many unnecessary nodes slow progress goal.
Moreover, exists path start target location, search may
4. And, furthermore, value g(p) increase significantly unobservable flat cone.

102

fiOptimal Any-Angle Pathfinding Practice

6

6


5
r

4

3

2

2

1

1

0

r

4

3

0



5



0
1

2

3

4

5

6

0

Figure 9: Root r reached via two paths
different lengths.

1

2

3

4

5

6

Figure 10: Root r reached via two
paths equal length.

terminate (e.g. input graph contains cycles possible endlessly generate
copies states ever increasing g-values).
propose following strategy avoid root-level redundancies:
1. store hash table visited roots best g-values. call table
root history apply similar way (and indeed lieu of) traditional
A* closed list.
2. generating search node n check root already root history
g-value less equal current g-value.
3. current g-value root improves value stored root history
add node open list. also update g-cost root5 root
history list.
4. Alternatively, current g-value root improve value stored
root history simply discard node (i.e. added open).
root history implemented hash table. size O(n) n number
discrete points given input map. show keeping root history list
affect correctness optimality search Anya indeed complete
terminate.
Lemma 7 Anya search prunes sub-optimal paths.
5. Similar updates nodes closed list sometimes performed context incremental,
bounded cost bounded sub-optimal search. updates performed part operation called
node re-opening. updates node re-opening. particular root points never
directly expanded thus never appear open list (Anya search comprise root-interval pairs).

103

fiHarabor, Grastien, Oz & Aksakalli

Proof: Trivial. search node root sub-optimal g value, represents
sub-optimal path.


Lemma 8 Anya always terminates.
Proof: Anya terminate, must explore paths arbitrary length. paths
must eventually involve root twice root different in-between. Let
n n0 two search nodes. g value associated n0 must higher
g value associated n and, therefore, node n0 must pruned. Indeed sufficiently
long paths pruned open list eventually empty.


Lemma 9 Anya redundant node pruning keeps least one optimal path.
Proof: search node n = (I, r) removed exists another search node n0 (but
different search parents) smaller (or equal) g-value kept. Assume
n search node optimal path p1 , . . . , pk , let pi point path
intersects I. Since g-value n similar n0 , exists another path

p01 , . . . , p0i , pi+1 , . . . , pk similar length, path pruned.

7. Practical Pruning Strategies
A* orders nodes expansion evaluating ranking promising appear
(i.e. f -values). is, however, possible alter order expansion without
compromising guarantees provided A*: correctness, optimality completeness.
Indeed strategy even positive effect efficiency overall search.
section discuss two practical strategies modify expansion order speed
search. enhancements applied on-the-fly focus reducing size
priority queue. first strategy, Cul-de-sac Pruning, identifies nodes safely
discarded cannot possibly lead goal. second strategy, Intermediate
Pruning, similar works avoiding explicit generation nodes
single successor (these successors expanded immediately, without added
open list).
7.1 Cul-de-sac Pruning
One way reducing size priority queue involves early identification culde-sacs (cds). cds search node successor contain target.
definition cds need added open list since expansion cannot
lead target. simple test identify cds nodes given Algorithm 3 way
procedure Is-cul-de-sac.
Early pruning cds nodes speeds search (and reduces required memory) preventing unnecessary operations open also reducing size list,
104

fiOptimal Any-Angle Pathfinding Practice

Algorithm 3 Cul-de-sac intermediate node pruning.
1: function Is-cul-de-sac(n = (I, r))
. Assumes contain target point
2:
Imax projection n
. Flat projection cone projection depending n
3:
Imax valid
. Valid means every p Imax visible r
4:
return f alse
. n cannot cul-de-sac; least one successor interval 0 Imax
5:
end
6:
return true
. n cul-de-sac; cannot projected successors
7: end function
8: function Is-Intermediate(n = (I, r))
. Assumes contain target point
9:
n flat node
10:
p endpoint furthest r
11:
p turning point taut local path prefix hr, pi
12:
return f alse
. n least one non-observable successor; cannot intermediate
13:
end
14:
else
. n flat node must cone node
15:
closed endpoint also corner point
16:
return f alse
. n least one non-observable successor; cannot intermediate
17:
end
18:
0 interval projecting r
19:
0 contains corner points
20:
return f alse
. n one observable successors; cannot intermediate
21:
end
22:
end
23:
return true
24: end function

makes every operation faster. reference, open list implemented
binary heap add remove operation time complexity log n, n
size list. Examples cds pruning, cone nodes flat nodes, illustrated
Figure 11 Figure 12. cases current node root shown blue
intervals red pruned.
7.2 Intermediate Pruning
second pruning strategy described pushing expansion one direction
far possible long increase branching factor. Practically, search
node generated guaranteed one successor, immediately
generate successor instead originally intended node. said successor also
one successor process recursively applied. Examples showing application
intermediate pruning given Figure 13 cone nodes Figure 14 flat nodes.
simple test identify intermediate nodes given Algorithm 3 way procedure
Is-Intermediate.
first obvious benefit intermediate pruning reduction number operations open list. However second benefit pushing expansion node
lead cul-de-sac. happens node added open list,
helps keep size list small operations list fast.
potential issue Intermediate Pruning recursive application nonpromising successor nodes could costly (in terms time) simply adding
105

fiHarabor, Grastien, Oz & Aksakalli

4

4

3

3

2

2

c


1



e f
b

r



b

c

1
r

0

0
0

1

2

3

4

5

6

0

Figure 11: Cul-de-sacs cone nodes:
nodes ([c, d), r) ((e, f ], r) generated.

2
1

2

3

4

5

6

Figure 12: Cul-de-sac flat nodes: node
((b, c], r) generated.

4
3

1

4
r

3



b

c



2

r



b

c

1

0

0

0
1
2
3
4
5
6
Figure 13: Intermediate node ([a, b], r)
one successor, ([c, d], r), immediately generated.

0
1
2
3
4
5
6
Figure 14: Intermediate node ([a, b], r)
one successor, ((b, c], r) immediately generated.

nodes open. discuss issue detail Section 7.3. also note
run-time experiments application Intermediate Pruning net positive effect
performance search.
7.3 Discussion
introduced two different ways nodes frontier search
pruned: Cul-de-sac Pruning Intermediate Pruning. modify expansion order
search improve performance along single fixed path. pruning
away sterile branches skipping intermediate locations actual branching
occurs. Similar strategies previously discussed literature. example Culde-sac Pruning based set principles Dead-end Heuristic (Bjornsson &
Halldorsson, 2006); although method reasons locally applied purely online.
Intermediate Pruning shares similarities Fast Expansion (Sun, Yeoh, Chen, &
106

fiOptimal Any-Angle Pathfinding Practice

Koenig, 2009); main difference prune nodes without reference f -value.
Intermediate Pruning also similar Jump Point Search (Harabor & Grastien, 2014)
applied outside context symmetry breaking extended sets points taken
intervals rather applied individual cells grid.
Anyas root history list, discussed Section 6, also regarded type pruning
enhancement. case reason generally set possible paths
could used reach given point prune away successors cannot
possibly optimal path. approach taken similar principle
(but practice) pruning redundant states real-time search (Sturtevant &
Bulitko, 2011).
Pruning search nodes Anya difficult classical A* search
many modern progenitors. Anya node represents set positions
rather one. Consider example Figure 15; particularly interested
interval [a, b] generated root r1 r2 . shortest path
r1 ( 14.24 14.99 r2 ). However obstacle put
cell labeled O, optimal path switches r2 ( 15.62 15.94).
diagram suggests that, given target two search nodes sharing
interval, may possible prune either them.
situation described Figure 15 uncommon practice examples
may motivate us derive new sophisticated pruning rules enhance
performance Anya algorithm. must careful however weigh improved
pruning power new techniques overhead applying first
instance. example, alternative (arguably, better) approach avoiding redundant
node expansions keep interval history list addition (or instead of) root history.
method would certainly avoid problem outlined Figure 15 many
possible intervals roots, means size hash table potentially
much larger memory accesses potentially slower. Additionally, comparing intervals
equality membership requires extra time may worth investment6 .

6. attempted similar experiment results clearly positive.

107

fiHarabor, Grastien, Oz & Aksakalli



11
10


9
8
7
6


5

b

r1

4

r2

3
2



1
0
0

1

2

3

4

5

6

7

8

9

10

11

12

13

Figure 15: Illustrating search nodes cannot trivially pruned search nodes
n1 = ([a, b], r1 ) n2 = ([a, b], r2 ): obstacle optimal path
goes n1 (red); otherwise goes n2 (blue).

8. Experimental Setup
conduct experiments seven benchmark problem sets taken Nathan Sturtevants
well known repository (Sturtevant, 2012). Three benchmarks originate popular
computer games often appear literature. are: Baldurs Gate II , Dragon
Age Origins StarCraft. maps benchmarks vary size; several
thousand nodes several million. remaining four benchmarks comprise grids
size 512 512 randomly placed obstacles varying densities, 10% 40%.
Table 1 gives overview benchmark problems. give number maps
instances per problem set distribution number node expansions required
reference algorithm, A* using octile distance heuristic7 , solve problems
benchmark set. latter metric gives us baseline comparing difficulty
problems appearing benchmark set.
7. Octile distance analogous Manhattan distance generalised 8-connected grids.

108

fiOptimal Any-Angle Pathfinding Practice

Benchmark

#Maps

#Instances

Baldurs Gate II
Dragon Age
StarCraft
Random 10%
Random 20%
Random 30%
Random 40%

75
156
75
10
10
10
10

93160
159465
198230
16770
17740
19200
35360

Nodes Expanded A*
Min
Q1 Median Mean
Q3
Max StDev
2
166
2019 6302 9170
86720
9136
1
622
5880 14080 19150 126800 19744
3 4808
26840 50000 70110 578900 63507
2
239
548
1886 1485
59280
3921
3
749
3869 8606 14680
53760
9905
4 3520
14190 20290 33710
96090 19162
3 12520
42850 51920 83770 169900 43558

Table 1: overview seven benchmark problems used experiments. give
number maps problem instances benchmark distribution nodes expanded
reference algorithm (A*) solving problems benchmark set.

compare purely online optimal Anya algorithm number state-ofthe-art any-angle techniques. are: Theta* (Nash et al., 2007), Lazy Theta* (Nash,
Koenig, & Tovey, 2010), Field A* (Uras & Koenig, 2015a) any-angle variant
two-level Subgoal Graphs (SUB-TL) (Uras & Koenig, 2015b). approaches
near-optimal guaranteed return shortest path. methods Theta*, Lazy
Theta* Field A* purely online. SUB-TL relies offline pre-processing
step improve performance search. use C++ implementations
algorithms; source codes made publicly available Uras Koenig (2015a).
Anya implemented Java executed JVM 1.8. allow comparisons across
different implementation languages use A* algorithm (Hart, Nilsson, & Raphael,
1968), implemented C++ Java, reference point8 . compare performance Anya Java implementation A* algorithms
C++ implementation A*. experiments performed 3GHz Intel Core i7
machine 8GB RAM running OSX 10.8.4. Source code implementation
Anya available https://bitbucket.org/dharabor/pathfinding.

9. Results
evaluate performance using three different metrics: search time, nodes expanded
path length. results presented relative benchmark algorithm, A*,
combine standard octile distance heuristic. example, comparing search
time nodes expanded, give figures relative speedup algorithm vs
A*. paradigm search time speedup 2 means twice fast node
expansion speedup 2 means half many nodes expanded. comparing path
length give percent improvement path length vs A*. cases higher better.

8. C++ implementation due Uras Koenig (2015a); Java implementation own.

109

fiHarabor, Grastien, Oz & Aksakalli

Benchmark
Baldurs Gate II
Dragon Age
StarCraft 40%
Random 10%
Random 20%
Random 30%
Random 40%

Avg. Node Expansion Speedup
Anya Theta* L.Theta* F.A* SUB-TL
91.13
1.95
1.96 1.01 907.10
19.60
1.05
1.05 0.90
57.45
40.73
1.27
1.27 0.95 166.00
0.80
2.34
2.38 1.14
6.60
0.77
1.23
1.17 0.80
2.56
1.06
0.82
0.75 0.64
1.68
2.20
0.90
0.86 0.82
2.40

Avg. Path Length Improvement (%)
Anya Theta* L.Theta* F.A* SUB-TL
4.65% 4.62%
4.61% 4.38%
4.58%
4.34% 4.27%
4.22% 4.05%
4.28%
5.02% 4.95%
4.92% 4.70%
4.88%
4.77% 4.63%
4.58% 3.83%
4.59%
4.57% 4.34%
4.15% 3.26%
4.30%
4.44% 4.12%
3.77% 3.12%
4.03%
4.14% 3.95%
3.48% 3.22%
3.74%

Table 2: compare performance algorithm terms average node expansion speedup
average path length improvement. metrics taken respect reference algorithm
(A*). cases higher better.
begin Table 2 shows average performance figures nodes expanded
path length seven benchmark problem sets. make following observations:
Anya best four purely-online algorithms, expanding fewer nodes
five seven benchmarks. three benchmarks drawn real computer
games Anya expands one order fewer nodes, average, nearest purelyonline contemporary. pre-processing-based SUB-TL algorithm expands fewer
nodes, average.
Anya, methods comparison, struggles achieve speedup
four random benchmarks. two four cases performance
reference A* algorithm. Again, pre-processing-based SUB-TL algorithm
able achieve consistent, though much reduced, node expansion speedup.
Anya, optimal, shows best improvement path length; however algorithms comparison close optimal, average.
Next, evaluate performance terms search time. Rather taking simple
average per benchmark basis (or across benchmarks) instead sort instances
according difficulty, measured number node expansions required
reference A* algorithm solve problem. approach gives holistic overview
performance reduces effect bias associated selection instances
comprise benchmark set9 . Results analysis given Figure 16.
make following observations:
Anya often one order magnitude faster reference A* algorithm benchmarks drawn real computer games. Performance mixed
four random benchmarks, evaluated methods struggling achieve
speedup.
9. per Table 1, problem instances regarded easy often outnumber instances
regarded hard. difference effect skewing performance indicators
computed simple averages instances benchmark set.

110

fiOptimal Any-Angle Pathfinding Practice

Benchmarks

Baldur's Gate II
Anya
Theta*
Lazy Theta*
Field A*
SUBTL

Speedup vs A*

100

10

1000

Anya
Theta*
Lazy Theta*
Field A*
SUBTL

100

Speedup vs A*

1000

10

1

1

0.1

0.1
102

103

104
Nodes Expanded A*

105

106

102

103

Dragon Age Origins

104
Nodes Expanded A*

105

106

StarCraft
Anya
Theta*
Lazy Theta*
Field A*
SUBTL

Speedup vs A*

100

10

1000

Anya
Theta*
Lazy Theta*
Field A*
SUBTL

100

Speedup vs A*

1000

10

1

1

0.1

0.1
102

103

104
Nodes Expanded A*

105

106

102

Random; 512x512 10% obstacles

Speedup vs A*

105

106

Random; 512x512 20% obstacles
Anya
Theta*
Lazy Theta*
Field A*
SUBTL

100

104
Nodes Expanded A*

10

1000

Anya
Theta*
Lazy Theta*
Field A*
SUBTL

100

Speedup vs A*

1000

103

10

1

1

0.1

0.1
102

103

104
Nodes Expanded A*

105

106

102

Random; 512x512 30% obstacles

Speedup vs A*

105

106

Random; 512x512 40% obstacles
Anya
Theta*
Lazy Theta*
Field A*
SUBTL

100

104
Nodes Expanded A*

10

1000

Anya
Theta*
Lazy Theta*
Field A*
SUBTL

100

Speedup vs A*

1000

103

10

1

1

0.1

0.1
102

103

104
Nodes Expanded A*

105

106

102

103

104
Nodes Expanded A*

105

106

Figure 16: Search time speedup. compare performance seven benchmarks
terms search time. Figures given relative speedup vs. reference A* algorithm.
Problem instances sorted difficulty using A* node expansion rank. Note
plot log-log.

111

fiHarabor, Grastien, Oz & Aksakalli

Anya fastest four purely online methods evaluation. performance often comparable pre-processing based SUB-TL technique and,
particularly challenging instances StarCraft domain, Anya non-dominated10 .
Anyas performance terms search time less value suggested
(previously evaluated) node expansion metric. reflects fact node
expansion made Anya involves analysing grid; looking roots searching
intervals.
9.1 Discussion
seen Anya compares well current state-of-the-art any-angle pathfinding
algorithms. (almost) apples-to-apples comparison three contemporary purely
online search technique (Theta*, Lazy Theta* Field A*) seen Anya usually
expands fewer nodes per search terminates one order magnitude faster.
results underscored fact Anya online algorithm
guaranteed return Euclidean-optimal path. may surmise that, many cases
applications, Anya appears preferable alternative algorithms.
Next, make apples-to-oranges comparison purely online Anya algorithm near-optimal offline enhanced SUB-TL algorithm. seen
Anya usually fast SUB-TL performance sometimes comparable.
Moreover, Anya retains advantage solving especially challenging instances drawn
real computer games. SUB-TL appears preferable Anya cases additional space time available create store associated subgoal graph cases
overheads amortised many online instances. extra space
time available, cases map subject change (e.g. new obstacles
added existing obstacles removed), Anya appears preferable SUB-TL.
main strength Anya searches sets nodes grid rather
considering individual locations one time. Expansion thus considered
macro operator, meaning Anya bears similarity speedup techniques using
hierarchical abstraction; e.g. HPA* (Botea et al., 2004). important difference
Anya constructs abstract graph on-the-fly rather part pre-processing step.
One current drawback associated Anya nodes contain overlapping
intervals. occurs interval reachable two different root points, neither
pruned (e.g. root locations reached first time;
illustrated Figure 15). nodes are, either part whole, redundant
provided f -value smaller optimal distance goal
beget yet redundant successors. see behaviour especially results
benchmarks Random 10% Random 20% SUB-TL achieves speedup several
factors Anya struggles maintain parity reference A* algorithm. seems
reasonable improve current algorithm attempting identify overlaps order
prune consideration. efficient effective algorithm achieving
goal subject work.
10. Pareto sense; i.e. problem instances Anya better SUB-TL according
metric interest node expansions search time

112

fiOptimal Any-Angle Pathfinding Practice

10. Related Work
Among simplest popular approaches solving any-angle pathfinding
problem string-pulling. main idea find path input grid map, often
using variant A* (Hart et al., 1968), post-process path order
remove unnecessary turning points. Several methods appeared literature
Game Development; e.g. see work Pinter (2001) Botea et al. (2004).
number algorithms improve string-pulling interleaving node expansion
path post-processing online search. Particular examples include Field D* (Ferguson
& Stentz, 2005) Field A* (Uras & Koenig, 2015a), use linear interpolation
smooth grid paths one cell time, Theta* (Nash et al., 2007), introduces
shortcut time successful line-of-sight check made; parent current
node successors. Though still sub-optimal many cases approaches
nevertheless attractive able search purely online efficient
practice. addition two examples given numerous works, often
appearing literature Artificial Intelligence, apply improve basic
interleaving idea. refer interested reader Nash & Koenig, 2013 recent survey
overview.
Accelerated A* (Sislak, Volf, & Pechoucek, 2009) online any-angle algorithm
conjectured optimal strong theoretical argument made. Similar
Theta*, differs primarily line-of-sight checks performed set expanded
nodes rather single ancestor. size set loosely bounded and,
challenging problems, include large proportion nodes closed list.
One recent successful line research involves combination string-pulling
offline pre-processing step. works compelling significantly
improve performance purely online search; terms solution quality
also running time. Block A* (Yap, Burch, Holte, & Schaeffer, 2011) one example.
sub-optimal algorithm pre-computes database Euclidean-optimal distances
possible tile configurations certain size (e.g. possible 3x3 blocks). database
obviates need explicit visibility checks indeed type online string-pulling.
pre-processing step needs performed exactly once; database remains valid
tiles map change indeed map changes entirely. Another recent
work improves Theta* combining algorithm pre-processing based graph
abstraction technique (Uras & Koenig, 2015b). approach, referred Section 9
SUB-TL, shown improve running time solution quality Block A*.
main disadvantage (vs. Block A*) abstract graph needs re-computed
repaired time map changes.
Euclidean Shortest Path Problem well known well researched topic
areas Computational Geometry Computer Graphics. seen generalisation Any-angle Pathfinding Problem. asks shortest path plane
impose restrictions obstacle shape obstacle placement (cf. grid aligned
polygons made unit squares).
Visibility graphs (Lozano-Perez & Wesley, 1979) family well-known popular
techniques optimally solving Euclidean Shortest Path Problem. Searching
graphs requires O(n2 log2 n) time approach much faster practice.
113

fiHarabor, Grastien, Oz & Aksakalli

two main disadvantages: (i) computing graph requires offline pre-processing
step O(n2 ) space store; (ii) graph static must recomputed repaired
environment changes. sophisticated variants Tangent Graphs (Liu &
Arimoto, 1992) Silhouette Points (Young, 2001) particularly efficient variants
visibility graphs disadvantages apply.
Another family exact approaches solving Euclidean Shortest Path Problem
based Continuous Dijkstra paradigm (Mitchell et al., 1987). efficient algorithms (Hershberger & Suri, 1999) involves pre-computation requiring O(n log2 n) space O(n log2 n) time. result Shortest Path Map; planar
subdivision environment used find Euclidean shortest path
O(log2 n) time; queries originating fixed source. Like visibility graphs,
approach also introduces additional memory overheads (storing subdivision)
pre-processing step must re-executed time environment start location
changes.

11. Conclusion
study any-angle pathfinding: problem commonly found areas robotics
computer games. problem involves finding shortest path two points grid
asks path artificially constrained fixed points grid.
best known online algorithms any-angle problem, date, compute approximate
solutions rather optimal shortest paths. Additionally online methods
able achieve consistent speedup vs. A* algorithm common reference point
measuring performance literature. work present new online, optimal practically efficient any-angle technique: Anya. works obtain good
performance reasoning grid level method considers sets points
grid taken together contiguous intervals. approach requires revisiting
classical definition search nodes successors requires introduction new
technique computing f -value node. give thorough algorithmic description new search paradigm give theoretical arguments completeness
optimality preserving characteristics.
(almost) apples-to-apples comparison evaluate Anya three contemporary near-optimal online techniques: Theta*, Lazy Theta* Field A*. show
that, range popular benchmarks, Anya faster alternatives,
guaranteeing find optimal shortest path. apples-to-oranges comparison
evaluate Anya SUB-TL: fast pre-processing-based near-optimal any-angle
technique. show Anya non-dominated compared SUB-TL even
maintains advantage particularly challenging instances drawn real computer games. Another advantage that, unlike SUB-TL, Anya assume map
static; i.e. readily applied pathfinding problems involving dynamically changing
terrain.
Any-angle pathfinding received significant attention AI Game Development communities open question whether optimal
online algorithm exists. Anya answers question affirmative.
114

fiOptimal Any-Angle Pathfinding Practice

11.1 Future Work
several possible directions future work. Perhaps obvious development improvements extensions current Anya algorithm. example,
believe empirical performance Anya could enhanced generating successors
nodes contain redundant (or partially redundant) intervals. One possibility
keep closed list previously encountered intervals. stronger variant idea
involves bounding g-value grid intervals generating successor nodes
least one point inside candidate interval relaxed. related orthogonal improvement involves pre-processing grid identifying intervals apriori. enhancement
speed search avoiding entirely grid scanning interval projection operations
currently necessary order generate node.
seen reasoning sets points grid, rather individual
locations, computationally beneficial. believe type search paradigm
employed Anya generalised improve performance grid-optimal search
addition any-angle pathfinding.
final suggestion work, believe Anya might also generalised
two-dimensional maps arbitrarily shaped polygonal obstacles, rather grids.
benefit generalisation would avoid discretisation world
path searched for. would even improve quality path returned
optimal any-angle path often non optimal non-discretised version map.

Acknowledgements
thank Tansel Uras assistance source codes used experimental section
paper. also thank Adi Botea Patrik Haslum helpful suggestions
early development work.
work Daniel Harabor Alban Grastien supported NICTA. NICTA
funded Australian Government represented Department Broadband,
Communications Digital Economy Australian Research Council
ICT Centre Excellence program.
work Dindar Oz Vural Aksakalli supported Scientific Technological Research Council Turkey (TUBITAK), Grant No. 113M489.

115

fiHarabor, Grastien, Oz & Aksakalli

Appendix A.
provide additional details implementation Anyas successor set generation
algorithm. method depends basic operations technically simple: grid
scanning, traversability tests linear projection operations. attempt reproduce mechanical details operations. Instead focus presentation toward
intuitive understanding overall process.
Algorithm 4 Computing successor set, supplemental.
1: function generate-start-successors(a traversable discrete start location s)
1
2:
Construct maximal half-closed interval Imax
containing points observable left
2
3:
Construct maximal half-closed interval Imax
containing points observable right
3
4:
Construct maximal closed interval Imax
containing points observable row
4
5:
Construct maximal closed interval Imax
containing points observable row
k
6:
intervals Split Imax
corner point take union
7:
Construct intervals new (cone flat) successor node r =
8:
return start successors
9: end function
10: function generate-flat-successors(an interval endpoint p, root point r)
11:
p0 first corner point (else farthest obstacle vertex) row p hr, p, p0 taut
12:
Imax new maximal interval endpoints p (open) p0 (closed)
13:
points r p row
. Observable successors
14:
successors new flat node n = (Imax , r)
15:
else
16:
successors new flat node n = (Imax , p)
. Non-observable flat successors
17:
end
18:
return successors
19: end function
20: function generate-cone-successors(an interval endpoint a, interval endpoint b, root point r)
21:
b r row
. Non-observable successors flat node
22:
r0 b, whichever farthest r
. Previously established turning point
23:
p point adjacent row, reached via right-angle turn
. Obstacle following
24:
Imax maximum closed interval, beginning p entirely observable r0
25:
else == b
. Non-observable successors cone node
26:
r0
27:
p point adjacent row, computed via linear projection r
28:
Imax maximum closed interval, beginning p entirely observable r0
29:
else
. Observable successors cone node
30:
r0 r
31:
p point adjacent row, computed via linear projection r
32:
p0 point adjacent row, computed via linear projection r b
33:
Imax maximum closed interval, endpoints b, entirely observable r0
34:
end
35:
{ split Imax corner point }
36:
n0 new search node interval root point r0
37:
successors successors
38:
end
39:
return successors
40: end function

116

fiOptimal Any-Angle Pathfinding Practice

References
Bjornsson, Y., & Halldorsson, K. (2006). Improved Heuristics Optimal Path-finding
Game Maps. Proceedings Second Artificial Intelligence Interactive
Digital Entertainment Conference, June 20-23, 2006, Marina del Rey, California, pp.
914.
Botea, A., Muller, M., & Schaeffer, J. (2004). Near Optimal Hierarchical Path-Finding.
Journal Game Development, 1 (1), 728.
Ferguson, D., & Stentz, A. (2005). Field D*: Interpolation-based Path Planner
Replanner. Robotics Research: Results 12th International Symposium, ISRR
2005, October 12-15, 2005, San Francisco, CA, USA, pp. 239253.
Graham, R. L., Knuth, D. E., & Patashnik, O. (1989). Concrete Mathematics - Foundation Computer Science. Addison-Wesley.
Harabor, D. D., & Grastien, A. (2013). Optimal Any-Angle Pathfinding Algorithm.
Proceedings Twenty-Third International Conference Automated Planning
Scheduling, ICAPS 2013, Rome, Italy, June 10-14, 2013.
Harabor, D. D., & Grastien, A. (2014). Improving Jump Point Search. Proceedings
Twenty-Fourth International Conference Automated Planning Scheduling,
ICAPS 2014, Portsmouth, New Hampshire, USA, June 21-26, 2014.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). Formal Basis Heuristic Determination Minimum Cost Paths. IEEE Transactions Systems Science
Cybernetics, 4 (2), 100107.
Hershberger, J., & Suri, S. (1999). Optimal Algorithm Euclidean Shortest Paths
Plane. SIAM Journal Computing, 28 (6), 22152256.
Liu, Y.-H., & Arimoto, S. (1992). Path Planning Using Tangent Graph Mobile
Robots Among Polygonal Curved Obstacles. International Journal Robotics
Research, 11, 376382.
Lozano-Perez, T., & Wesley, M. A. (1979). Algorithm Planning Collision-Free Paths
Among Polyhedral Obstacles. Communications ACM, 22 (10), 560570.
Mitchell, J. S. B., Mount, D. M., & Papadimitriou, C. H. (1987). Discrete Geodesic
Problem. SIAM Journal Computing, 16 (4), 647668.
Nash, A., Daniel, K., Koenig, S., & Felner, A. (2007). Theta*: Any-Angle Path Planning Grids. Proceedings Twenty-Second AAAI Conference Artificial
Intelligence, July 22-26, 2007, Vancouver, British Columbia, Canada, pp. 11771183.
Nash, A., & Koenig, S. (2013). Any-Angle Path Planning. AI Magazine, 34 (4), 9.
Nash, A., Koenig, S., & Tovey, C. A. (2010). Lazy Theta*: Any-angle Path Planning
Path Length Analysis 3D. Proceedings Twenty-Fourth AAAI Conference
Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010.
Pinter, M. (2001). Toward Realistic Pathfinding. Game Developer Magazine, 8 (4).
117

fiHarabor, Grastien, Oz & Aksakalli

Sislak, D., Volf, P., & Pechoucek, M. (2009). Accelerated A* Trajectory Planning: Gridbased Path Planning Comparison. 4th ICAPS Workshop Planning Plan
Execution Real-World Systems.
Sturtevant, N. (2012). Benchmarks Grid-Based Pathfinding. Transactions Computational Intelligence AI Games, 4 (2), 144 148.
Sturtevant, N. R., & Bulitko, V. (2011). Learning Going Whence
Came: h- g-Cost Learning Real-Time Heuristic Search. 22nd International Joint Conference Artificial Intelligence, IJCAI 2011, pp. 365370.
Sun, X., Yeoh, W., Chen, P.-A., & Koenig, S. (2009). Simple Optimization Techniques
A*-based Search. 8th International Joint Conference Autonomous Agents
Multiagent Systems, AAMAS 2009, Budapest, Hungary, May 10-15, 2009, Volume 2,
pp. 931936.
Uras, T., & Koenig, S. (2015a). Empirical Comparison Any-Angle Path-Planning
Algorithms. Proceedings Eighth Annual Symposium Combinatorial Search,
SOCS 2015, 11-13 June 2015, Ein Gedi, Dead Sea, Israel, pp. 206211.
Uras, T., & Koenig, S. (2015b). Speeding-Up Any-Angle Path-Planning Grids.
Proceedings Twenty-Fifth International Conference Automated Planning
Scheduling, ICAPS 2015, Jerusalem, Israel, June 7-11, 2015, pp. 234238.
Yap, P., Burch, N., Holte, R. C., & Schaeffer, J. (2011). Block A*: Database-Driven Search
Applications Any-Angle Path-Planning. Proceedings Twenty-Fifth
AAAI Conference Artificial Intelligence, AAAI 2011, San Francisco, California,
USA, August 7-11, 2011.
Young, T. (2001). Optimizing Points-of-Visibility Pathfinding. Game Programming
Gems 2, pp. 324329. Charles River Media.

118

fiJournal Artificial Intelligence Research 56 (2016) 197-245

Submitted 07/15; published 06/16

Two Aspects Relevance Structured Argumentation:
Minimality Paraconsistency
Diana Grooters

dianagrooters@gmail.com

ORTEC Finance Rotterdam, Netherlands

Henry Prakken

H.Prakken@uu.nl

Department Information Computing Sciences, Utrecht University
Faculty Law, University Groningen
Netherlands

Abstract
paper studies two issues concerning relevance structured argumentation
context ASPIC + framework, arising combined use strict defeasible
inference rules. One issue arises strict inference rules correspond classical logic.
longstanding problem trivialising effect classical Ex Falso principle
avoided satisfying consistency closure postulates. paper, problem
solved disallowing chaining strict rules, resulting variant ASPIC + framework called ASPIC ? , disallowing application strict rules inconsistent sets
formulas. Thus effect Rescher & Manors paraconsistent notion weak consequence
embedded ASPIC ? .
Another issue minimality arguments. arguments apply defeasible inference
rules, cannot required subset-minimal premises, since defeasible rules
based information may well make argument stronger. paper instead
minimality required applications strict rules throughout argument. shown
plausible assumptions affect set conclusions. addition, circular arguments new ASPIC ? framework excluded way satisfies
closure consistency postulates generates finitary argumentation frameworks
knowledge base set defeasible rules finite. latter result exclusion
chaining strict rules essential.
Finally, combined results paper shown proper extension
classical-logic argumentation preferences defeasible rules.

1. Introduction
One tradition logical study argumentation allow arguments combine
strict defeasible inference rules. approach introduced AI Pollock (1987,
1990, 1992, 1994, 1995), studied past also e.g. Lin Shoham (1989), Simari
Loui (1992), Vreeswijk (1997), Prakken Sartor (1997) Garcia Simari (2004)
currently studied e.g. Dung Thang (2014), Dung (2014, 2016) work
ASPIC + framework (Prakken, 2010; Modgil & Prakken, 2013, 2014). Strict inference
rules intended capture deductively valid inferences, truth premises
guarantee truth conclusion. Defeasible inference rules meant capture presumptive inferences, premises create presumption favour conclusion,
refuted evidence contrary. Much research tradition shown
idea defeasible inference rules makes sense. example, Pollock applied
c
2016
AI Access Foundation. rights reserved.

fiGrooters & Prakken

formalize theory defeasible epistemic reasons, includes reasons concerning perception, memory, enumerative induction, statistical syllogism temporal persistence.
Moreover, several publications ASPIC + use defeasible inference rules formalise Walton
(1996)-style presumptive argumentation schemes (Prakken, 2010; Modgil & Prakken, 2014)
apply legal reasoning (Prakken, Wyner, Bench-Capon, & Atkinson, 2015)
policy debate (Bench-Capon, Prakken, & Visser, 2011). Finally, Garcia Simaris (2004)
Defeasible Logic Programming approach applied many domains.
tradition, two issues arise concerning relevance, namely, minimality arguments paraconsistency strict-rule application. study issues
context ASPIC + framework. choice ASPIC + purposes justified framework nature, allows study various classes instantiations.
Moreover, shown various approaches reconstructed instantiations ASPIC + framework. Prakken (2010) showed assumption-based
argumentation reconstructed Dung, Mancarella, Toni (2007) instance
abstract argumentation (Dung, 1995), result carries original formulation assumption-based argumentation (Bondarenko, Dung, Kowalski, & Toni, 1997)
known semantics except semi-stable eager semantics (cf. Caminada, Sa, Alcantara,
& Dvorak, 2015). Furthermore, Modgil Prakken (2013) reconstructed two forms
classical argumentation premise attack studied Gorogiannis Hunter (2011)
several uses Tarskian abstract logics studied Amgoud Besnard (2013)
instances ASPIC + . reasons, results terms ASPIC + representative
large classes argumentation systems.
ASPIC + sometimes criticised fact allows instantiations bad
properties criticism besides point, since ignores framework nature
ASPIC + (Prakken & Modgil, 2012). framework instead concrete system,
ASPIC + intended allow study properties various instantiations,
whether satisfy rationality postulates Caminada Amgoud (2007).
idea framework allow bad instantiations identified.
Therefore, framework cannot criticised existence bad instantiations.
Moreover, growing body results good instantiations ASPIC + (Caminada
& Amgoud, 2007; Prakken, 2010; Modgil & Prakken, 2013; Dung, 2014, 2016; Caminada,
Modgil, & Oren, 2014; Grooters & Prakken, 2014; Wu & Podlaszewski, 2015) paper
aims identifying another class good instantiations.
One relevance issue discussed paper minimality arguments. deductive
approaches argumentation (e.g., Besnard & Hunter, 2008; Gorogiannis & Hunter, 2011;
Amgoud & Besnard, 2013) arguments required subset-minimal set premises.
However, arguments apply defeasible inference rules, requirement undesirable, since defeasible rules based information may well make argument
stronger. example, Observations done ideal circumstances usually correct
stronger Observations usually correct. Note remark apply
strict inference rules, still makes sense improve efficiency requiring strict
inference rules applied subset-minimal set formulas. far, system
defeasible-rule tradition enforces requirement. One contribution paper
ASPIC + approach show plausible conditions
argument ordering, affect set conclusions.
198

fiTwo Aspects Relevance Structured Argumentation

Another aspect minimality circularity. far presentations ASPIC +
prevented arguments repeating conclusions subarguments. Yet argumentation theory circular arguments generally regarded fallacious, makes sense
exclude them. paper prove results rationality postulates affected it. Moreover, prove excluding non-circular arguments
computational benefits.
Another relevance issue arises strict inference rules chosen correspond
classical logic. longstanding unsolved problem originally identified Pollock (1994,
1995) trivialising effect classical Ex Falso principle avoided
two arguments use defeasible rules contradictory conclusions. problem
especially hard since solution arguably preserve satisfaction rationality
postulates consistency strict closure (Caminada & Amgoud, 2007).
nutshell, problem follows. Suppose two arguments contradictory
conclusions . strict inference rules include Ex Falso principle
inconsistent set implies formula, two arguments combined
argument formula . combined argument potentially defeat
argument applying Ex Falso inference rule joint conclusions.
arguments contradictory conclusions, argument potentially
threat, clearly undesirable, since conflict general unrelated .
Pollock (1994, 1995) thought avoided trivialising arguments allowing multiple labellings, Caminada (2005) showed Pollocks solution
fully avoid them. problem genuine one, since arguably real need
argumentation systems allow combinations strict defeasible inferences
that, moreover, allow full reasoning power deductive logic. Although many
cases less expressiveness may suffice, full theory logic argumentation cannot
exclude general case.
solve problem, two approaches possible. One change definitions
argumentation framework, derive strict inference rules
weaker logic classical logic. first approach taken Wu (2012) Wu
Podlaszewski (2015), ASPIC + framework require argument
set conclusions subarguments classically consistent. show
solution works restricted version ASPIC + without preferences, give
counterexamples consistency postulates case preferences.
second approach solve problem replace classical logic source
strict rules weaker, monotonic paraconsistent logic, order invalidate Ex
Falso principle valid strict inference rule. paper explores possibility. first
show two well-known paraconsistent logics, system C Da Costa (1974)
Logic Paradox Priest (1979, 1989), cannot used purposes, since
induce violation postulate indirect consistency. show using Rescher
Manors (1970) paraconsistent consequence notion satisfies closure consistency
postulates also avoids trivialisation. thus initially taking second approach,
combine first approach (changing definitions) since
turn chaining strict rules arguments disallowed. change turn
motivates new interpretation Caminada Amgouds (2007) strict-closure postulate
199

fiGrooters & Prakken

introduction new rationality postulate logical closure. contribution
paper based extends results Grooters Prakken (2014).
making contributions, argue combination shed light
relation adapted version ASPIC + classical argumentation studied
Besnard Hunter (2008) Gorogiannis Hunter (2011), arguments
essentially classical proofs consistent subset-minimal subsets classical
knowledge base. two versions classical argumentation premise attack
adapted version ASPIC + shown proper extension defeasible rules
preferences. observation justifies combined treatment issues (minimality
arguments paraconsistency) paper.
Caminada, Carnielli, Dunne (2012) formulated new set rationality postulates
addition Caminada Amgoud (2007), characterise cases
trivialisation problem avoided (called postulates non-interference crashresistance). Wu (2012) Wu Podlaszewski (2015) prove adaptation
ASPIC + consistent arguments new postulates satisfied complete
semantics. However, attempt prove Caminada et al.s (2012) postulates,
two reasons. First, want obtain results semantics well and, second,
argue Section 10 Caminada et al.s postulates fact capture stronger intuitive
notion one study paper.
remainder article organised follows. First Section 2 ASPIC +
framework summarised Section 3 rationality postulates Caminada Amgoud (2007) presented. Section 4 trivialisation problem illustrated
detail, Section 5 instantiations ASPIC + paraconsistent logics LP C studied attempt avoid trivialisation face inconsistency.
shown instantiations violate rationality postulate indirect consistency. Section 6 Rescher Manors (1970) paraconsistent consequence notion
introduced another attempt avoid trivialisation. turns embedding
ASPIC + requires adaptation ASPIC + framework framework called ASPIC ? , disallows chaining strict rules, turn motivates new notions strict
closure indirect consistency. Section 7 first main contribution paper proved: satisfaction closure consistency postulates instantiation
ASPIC ? framework Rescher Manors consequence notion. Section 8
second third main contribution presented: equivalence result versions
ASPIC ? without minimality constraints strict inferences, proofs show
version ASPIC ? excludes circular arguments well-behaved. Section 9
present fourth main result, namely, ASPIC ? minimal arguments properly
generalises two versions classical argumentation. Finally, Section 10 discuss
results put context related work.

2. ASPIC + Framework
section, ASPIC + framework reviewed. Since makes use Dungs (1995)
theory abstract argumentation, theory first briefly summarised. abstract
argumentation framework (AF ) pair (A, D), set arguments
binary relation defeat. argument defeats argument B (A, B) D.
200

fiTwo Aspects Relevance Structured Argumentation

set arguments defeats argument B argument defeats
B. set defeats set 0 argument 0 defeats A. set
arguments said conflict-free attack itself; otherwise conflicting.
set defends argument iff BinA defeats exists
C defeats B. set admissible conflict-free defends attacking
argument attacking S. argumentation framework zero extensions,
intuitively maximal sets arguments accepted together since
conflict-free defend members attacks. Formally, extensions
admissible sets additional properties. defined according Dungs
characteristic function.
Definition 2.1. [Dungs characteristic function F ] FAF : 2A 2A FAF (S) =
{A A|A defended S}.
Henceforth subscript AF omitted danger confusion.
Definition 2.2. [Extensions abstract argumentation frameworks] AF =
(A, D) E A:
E conflict-free iff A, B E (A, B) D.
E admissible iff E conflict-free E defends E.
E complete extension AF iff E conflict-free FAF (E) = E.
E preferred extension AF iff E set-inclusion-maximal complete extension
AF .
E stable extension AF iff E conflict-free 6 E exists
B E B defeats A.
E grounded extension AF iff E set-inclusion-minimal complete extension AF .
Finally, {complete, preferred, grounded, stable}, X sceptically credulously
-justified X belongs all, respectively least one, extension. notions
extensions proposed literature paper confine
four notions.
ASPIC + framework (Prakken, 2010; Modgil & Prakken, 2013) gives structure
Dungs arguments defeat relation. work Vreeswijk (1997) defines arguments directed acyclic inference graphs formed applying strict defeasible inference
rules premises formulated logical language. Intuitively, strict rules guarantee
truth consequent antecedents true, defeasible rules create
presumption favour truth consequent antecedents true. Arguments attacked (ordinary) premises applications defeasible
inference rules. attacks succeed defeats, partly determined preferences.
acceptability status arguments defined applying Dungs (1995)
semantics abstract argumentation frameworks resulting set arguments
defeat relation.
201

fiGrooters & Prakken

special case symmetric negation version ASPIC + defined
Modgil Prakken (2013) presented, minor improvements. Nontrivial
improvements indicated made.
ASPIC + system framework specifying systems. said above,
framework intended allow study properties instantiations, whether
satisfy rationality postulates Caminada Amgoud (2007). end
defines notion abstract argumentation system structure consisting logical
language L unary negation symbol , set R consisting two disjoint subsets Rs
Rd strict defeasible inference rules, naming convention n L defeasible
rules order talk L applicability defeasible rules. elements
left undefined general specified specific instantiation.
Definition 2.3. [Argumentation systems] argumentation system triple =
(L, R, n) where:
L nonempty logical language unary negation symbol .
R = Rs Rd set strict (Rs ) defeasible (Rd ) inference rules form
1 , . . . , n 1 , . . . , n respectively (where , meta-variables
ranging wff L), Rs Rd = .
n : Rd L naming convention defeasible rules.
Informally, n(r) wff L, says rule r R applicable. write =
case = = . Note part logical language L
metalinguistic function symbol obtain concise definitions. Furthermore,
danger confusion, sometimes write sequence antecedents strict
defeasible rule set.
Example 2.1. example argumentation system
L = {p, p, q, q, r, r, s, s, t, t, r1 , r2 , r1 , r2 },
Rs = {p, r s; r1 }, Rd = {q r; s},
n(q r) = r1 n(t s) = r2 .
ASPIC + framework abstracts origins strict defeasible rules.
Several ways identify rules possible. One way, quite usual AI, let rules
express domain-specific knowledge. example, strict rules could contain terminological knowledge bachelors married, defeasible rules could contain
defeasible generalisations Birds fly defeasible norms Thou shalt lie.
Another way base rules general accounts deductive defeasible reasoning. example, strict rules might chosen correspond monotonic logic
defeasible rules might instantiated argument schemes (Walton, 1996).
two ways identify inference rules pragmatically different; formally, ASPIC +
framework treats rules inference rule regardless origin. paper abstract
origin defeasible rules focus choice strict rules.
particular concerned instantiations ASPIC + strict rules chosen
correspond monotonic logic (although several results apply generally).
202

fiTwo Aspects Relevance Structured Argumentation

instantiations Rs defined follows, given monotonic consequence notion `L
logic L:
Rs = {S | `L finite}
Rs defined way logical language L, say Rs corresponds
logic L.
Definition 2.4. [Knowledge bases] knowledge base = (L, R, n) set K L
consisting two disjoint subsets Kn Kp (the necessary ordinary premises).
Intuitively, necessary premises certain knowledge thus cannot attacked,
whereas ordinary premises uncertain thus attacked.
Definition 2.5. [Consistency strict closure] X L, let closure X
strict rules, denoted ClRs (X), smallest set containing X consequent
strict rule Rs whose antecedents ClRs (X). set X L
directly consistent iff @ , X = ;
indirectly consistent iff ClRs (X) directly consistent.
Example 2.2. example argumentation system, example directly inconsistent
set {p, p} example directly consistent indirectly inconsistent set
{p, r, s}. Finally, example closure strict rules ClRs ({p, r}) = {p, r, s, r1 }.
Arguments constructed step-by-step knowledge bases chaining inference
rules directed acyclic graphs (or trees formula used once).
follows, given argument function Prem returns premises, Conc returns
conclusion Sub returns sub-arguments, TopRule returns last rule used
argument.
Definition 2.6. [Argument] argument basis knowledge base K
argumentation system (L, R, n) is:
1. K with:
Prem(A) = {};
Conc(A) = ;
Sub(A) = {};
TopRule(A) = undefined.
2. A1 , . . . / A1 , . . . , arguments Rs /Rd contains strict/defeasible
rule Conc(A1 ), . . . , Conc(An ) / , with:
Prem(A) = Prem(A1 ) . . . Prem(An ),
Conc(A) = ,
Sub(A) = Sub(A1 ) . . . Sub(An ) {A};
TopRule(A) = Conc(A1 ), . . . , Conc(An ) / .
203

fiGrooters & Prakken

functions Func also defined sets arguments = {A1 , . . . , }
follows: Func(S) = Func(A1 ) . . . Func(An ). argument uses strict rules,
argument said strict, otherwise defeasible. argument necessary
premises, argument firm, otherwise plausible. argument define
Premn (A) = Prem(A) Kn Premp (A) = Prem(A) Kp . set arguments
consist necessary premise denoted N P (S).
Example 2.3. example argumentation system combined knowledge base
Kn = {p} Kp = {q, t}, following arguments constructed:
A1
A2
A3
A4

=
=
=
=

p
q

A2 r

A5 =
A6 =
A7 =

A3
A1 , A4
A5 r1

Argument A1 strict firm, A2 A3 strict plausible, remaining
arguments defeasible plausible.
Figure 1 arguments visualised. type premise indicated
superscript defeasible inferences displayed dotted lines. dotted boxes
thick arrows explained Example 2.4.

Figure 1: Arguments attacks Example 2.1. premises bottom
conclusion top tree. Thin vertical links boxes
inferences thick diagonal links attacks. type premise
indicated superscript defeasible inferences, underminable premises
rebuttable conclusions displayed dotted lines.

Arguments attacked three ways: ordinary premises (undermining
attack), defeasible inference (undercutting attack) conclusion defeasible
inference.
Definition 2.7. [Attack] argument attacks argument B iff undercuts, rebuts
undermines B, where:
204

fiTwo Aspects Relevance Structured Argumentation

undercuts argument B (on B 0 ) iff Conc(A) = n(r) B 0 Sub(B)
B 0 top rule r defeasible.
rebuts argument B (on B 0 ) iff Conc(A) = B 0 Sub(B) form
B100 , . . . , Bn00 .
undermines argument B (on ) iff Conc(A) = Premp (B).
Example 2.4. running example A6 rebuts A5 therefore also A6 rebuts A7
A5 (since A5 subargument A7 ). Note A5 rebut A6 since A6 strict
top rule. Furthermore, A7 undercuts A4 A6 A4 . Figure 1 rebuttable conclusions
visualised dotted boxes direct defeat relations displayed thick arrows.
Note indirect attacks A6 A7 A7 A6 explicitly visualised.
argument basic fallible argument 1 iff Kp TopRule(A) Rd . basic
fallible argument thus argument defeasible top rule equates ordinary
premise attacked final conclusion inference. set basic
fallible arguments set arguments denoted F A(S).
Argumentation systems plus knowledge bases form argumentation theories.
turn combined preference ordering set arguments constructible
theory, induce structured argumentation frameworks. Like elements argumentation
systems, nature ASPIC + argument ordering undefined general
specified specific instantiation.
Definition 2.8. [Structured Argumentation Frameworks] Let argumentation theory (AS, K). structured argumentation framework ( SAF) defined ,
triple hA, C, set finite arguments constructed K AS,
binary relation A, (X, ) C iff X attacks .
Unlike Modgil Prakken (2013) consider versions ASPIC + require
arguments consistent premises (except briefly Section 9 purposes comparison). approach strict arguments inconsistent premises handled
choice let Rs correspond paraconsistent logic, prevent trivialisation, i.e.,
prevent systems generate arguments random conclusion contradictions. Furthermore, leave users whether want ensure
defeasible rules inconsistent antecedents. left user since allowing
defeasible rules inconsistent antecedents cause trivialisation. Furthermore,
results proved paper still hold whether defeasible rules inconsistent antecedents
excluded not.
notion defeat defined follows. Undercutting attacks succeed
defeats independently preferences arguments, since meant express
exceptions defeasible inference rules. Rebutting undermining attacks succeed
attacked argument stronger attacking argument (A B defined
usual B B 6 A).
Definition 2.9. [Defeat] defeats B iff:
1. renaming Dung Thangs (2014) notion basic defeasible argument.

205

fiGrooters & Prakken

1. undercuts B;
2. rebuts/undermines B B 0 B 0 .
Example 2.5. running example A6 defeats A5 A7 unless A6 A5 . Furthermore,
preference relation A4 A7 , even A7 A4 , A7 defeats
A4 (and thus A7 also defeats A6 ).
SAFs generate abstract argumentation frameworks sense Dung (1995),
used evaluate arguments conclusions:
Definition 2.10. [Argumentation frameworks]
abstract argumentation framework (AF ) corresponding SAF = hA, C,
pair (A, D) defeat relation determined SAF .
Let {complete, preferred, grounded, stable} let L defining
SAF . wff L sceptically -justified SAF conclusion sceptically
-justified argument, credulously -justified SAF sceptically -justified
conclusion credulously -justified argument.
Example 2.6. Suppose running example A6 defeats A5 A7
A7 defeats A4 A6 . resulting AF visualised twice Figure 2. grounded
extension {A1 , A2 , A3 } two preferred extensions E1 = {A1 , A2 , A3 , A4 , A6 }
E2 = {A1 , A2 , A3 , A5 , A7 }. preferred extensions also stable. two preferred
extensions visualised Figure 2: members extension coloured white.

Figure 2: Two preferred extensions Dung AF Example 2.1.
finally need notion strict continuation set arguments, define
slightly different way Modgil Prakken (2013). new definition arguably
simpler affect proofs Modgil Prakken. identifies arguments
formed extending set arguments strict inferences new argument,
new argument attacked arguments extends.
Definition 2.11. [Strict continuations] set strict continuations set arguments smallest set satisfying following conditions:
206

fiTwo Aspects Relevance Structured Argumentation

1. argument strict continuation {A}.
2. A1 , . . . , S1 , . . . , Sn {1, . . . , n}, Ai strict
continuation Si B1 , . . . , Bn strict-and-firm arguments,
Conc(A1 ), . . . , Conc(An ), Conc(B1 ), . . . , Conc(Bm ) strict rule Rs ,
A1 , . . . , , B1 , . . . , Bn strict continuation S1 . . . Sn .
argument strict continuation arguments A1 , . . . , , strict argument
{Conc(A1 ), . . . , Conc(An )}.
Example 2.7. running example arguments strict continuations
A6 strict continuation {A1 , A4 } A7 strict continuation A5 . Suppose
temporarily add strict rule p, r2 Rs . A8 = A1 , A6 r2 strict
continuation {A6 }.

3. Rationality Postulates
Extensions abstract argumentation frameworks intuitively maximal sets arguments
rationally accepted together given frameworks. Dungs (1995) various
semantics, yielding different types extensions, seen various alternative ways
formalize rationality constraints acceptable sets arguments. arguments
structure, additional rationality constraints defined extensions
abstract argumentation semantics. Caminada Amgoud (2007) proposed following
rationality postulates structured argumentation.
Subargument closure: every extension E, argument E
subarguments E.
Closure strict rules: every extension E, set Conc(E) closed
application strict rules.
Direct consistency: every extension E, set Conc(E) directly consistent.
Indirect consistency: every extension E, set Conc(E) indirectly
consistent.
Note closure strict rules direct consistency together imply indirect consistency.
Modgil Prakken (2013) identify set conditions ASPIC + satisfies
four postulates. first condition set strict rules either closed
transposition closed contraposition.
Definition 3.1. [Closure transposition, (Modgil & Prakken, 2013)] set
strict rules Rs said closed transposition rule 1 , . . . , n Rs
rules form 1 , . . . , i1 , , i+1 , . . . , n 2 also
belong Rs . argumentation theory (AS, K) closed transposition strict
rules Rs closed transposition.
2. Note wff one = . example, p = p p = p.

207

fiGrooters & Prakken

Definition 3.2. [Closure contraposition, (Modgil & Prakken, 2013)]
argumentation system said closed contraposition X L, X
holds ClRs (X) ClRs (X\{} {})
. argumentation theory (AS, K) closed contraposition argumentation
system closed contraposition.
second condition states argument ordering following properties:
Definition 3.3. [Reasonable argument ordering, (Modgil & Prakken, 2013)]
reasonable argument ordering if:
A, B, strict firm B plausible defeasible, B A;
A, B, B strict firm, B A;
A, A0 , B, C, C A, B A0 strict continuation {A},
C A0 , A0 B;
Let {C1 , . . . , Cn } finite subset = 1, . . . , n let C +/i strict
continuation {C1 , . . . , Ci1 , Ci+1 , . . . , Cn }. case i, C +/i
Ci .
Modgil Prakken (2013) identify several types argument orderings reasonable.
third condition axiom consistency.
Definition 3.4. [Axiom consistent, (Modgil & Prakken, 2013)] argumentation
theory axiom consistent ClRs (Kn ) consistent.
Modgil Prakken (2013) prove argumentation theory satisfies three
conditions induces extensions satisfy four rationality postulates.

4. Trivialisation Problem
section illustrate trivialisation problem detail. following abstract
example illustrates problems arise strict rules argumentation system
correspond classical logic, i.e. X Rs X ` X finite (where
` denotes classical consequence).
Example 4.1. Let Rd = {p q; r q; s}, Kp = Kn = {p, r, t}, Rs
corresponds classical logic. corresponding AF includes following arguments:
A1 : p
B1 : r
D1 :

A2 : A1 q
B2 : B1 q
2 : D1

C: A2 , B2

Figure 3 displays arguments attack relations. Dotted lines indicate defeasible inferences dotted boxes indicate rebuttable conclusions. Argument C attacks D2 .
208

fiTwo Aspects Relevance Structured Argumentation

Figure 3: Illustrating trivialisation
Whether C defeats D2 depends argument ordering plausible argument orderings
possible C 6 D2 C defeats D2 . problematic, since
formula, defeasible argument unrelated A2 B2 , D2 , can, depending
argument ordering, defeated C. Clearly, extremely harmful, since
existence single case mutual rebutting attack, common, could
trivialise system. noted simply disallowing application strict rules
inconsistent sets formulas help, since argument still
constructed follows:
A3 :
C 0:

A2 q
A3 , B2

Note argument C 0 apply strict inference rule inconsistent set
formulas.
example suggests following formalisation property trivialisation.
Definition 4.1 (Trivialising argumentation systems). argumentation system
trivialising iff , L knowledge bases K {, } K strict
argument basis K constructed conclusion .
interested defining classes non-trivialising argumentation systems.
argumentation system example clearly trivialising since Rs contains strict
rules , , L.
Example 4.1 cause problems preferred stable semantics, since A2
B2 attack least one attacks (with non-circular argument
orderings) succeed defeat. Therefore, preferred stable extensions contain either
A2 B2 both. Since A2 B2 attack C (by directly attacking one
subarguments), C preferred stable extension defeated least one argument
extension, C extensions, D2 extensions.
intuitively correct since connection D2 arguments A2
B2 .
fact, semantics defined Dung (1995) problems Example 4.1
grounded semantics. Since A2 B2 defeat other, neither
grounded extension. extension defend D2 C therefore
contain D2 .
209

fiGrooters & Prakken

Pollock (1994, 1995) thought just-given line reasoning preferred semantics
suffices show recursive-labelling approach (which later Jakobovits &
Vermeir, 1999 proved equivalent preferred semantics) adequately deals
problem. However, Caminada (2005) showed example extended ways
also cause problems preferred stable semantics. Essentially, replaced
facts p r defeasible arguments p r let arguments defeated
self-defeating argument. one hand, self-defeating arguments cannot
extension, since extensions conflict free. However, self-defeating argument
defeated arguments, prevents argument defeats
acceptable respect extension. example, A2 B2 defeated
self-defeating argument otherwise undefeated, neither A2 B2
extension, argument extension defends D2 C.
critic ASP IC + Pollocks approach might argue problem caused
combination strict (i.e., deductive) defeasible inference rules. Indeed, classical argumentation (Besnard & Hunter, 2008; Gorogiannis & Hunter, 2011) problem
easily avoided requiring premises argument consistent. However, reasons believe classical logic strong able model
forms defeasible reasoning; see, instance, discussions Brewka (1991), Ginsberg
(1994) Prakken (2012). Furthermore, assumption-based argumentation (ABA)
reconstructed Dung et al. (2007), strict inference rules
require classical, require premises arguments
consistent, problem may may arise depending instantiated.
reconstructed ASPIC + Prakken (2010), ABA arguments built ordinary
premises Kp strict inference rules Rs . following example (in notation
ASPIC + reconstruction ABA) shows trivialisation problem also arise
ABA.
Example 4.2. Take Kp = {p, p, s} let Rs correspond classical logic, i.e,
Rs iff finite set wff classically implies . following arguments
constructed.
A:p
B : p
C : A, B
D:s
trivialising argument C prevents argument extension.
problem instantiate and/or redefine ASPIC + way avoids
trivialising effects strict inferences inconsistent set, still satisfying
rationality postulates Caminada Amgoud (2007).

5. Instantiating ASPIC + Two Well-Known Paraconsistent Togics
said introduction, one way avoid trivialisation derive strict rules
ASPIC + paraconsistent logic. logical consequence relation `L said
paraconsistent explosive, i.e. hold B
210

fiTwo Aspects Relevance Structured Argumentation

{A, A} `L B. section investigate strategy two well-known paraconsistent
logics, Logic Paradox Priest (1979, 1989) system C Da Costa (1974).
Another well-known paraconsistent logic family relevant logics. However, logic
nonmonotonic (Read, 1988, p. 100). problem since idea ASPIC +
strict rules based logic, logic monotonic. reason, relevance
logics considered paper.
5.1 Logic Paradox
Logic Paradox (Priest, 1979, 1989) obtained relaxing assumption classical
propositional logic sentence cannot true false. Sentences Logic
Paradox (LP ) two truth values instead one. set possible truth values
{{1}, {0}, {0, 1}}, {0, 1} paradoxical true false.
semantics propositional version LP follows.
1. (a) 1 v(A) 0 v(A)
(b) 0 v(A) 1 v(A)
2. (a) 1 v(A B) 1 v(A) 1 v(B)
(b) 0 v(A B) 0 v(A) 0 v(B)
3. (a) 1 v(A B) 1 v(A) 1 v(B)
(b) 0 v(A B) 0 v(A) 0 v(B)
4. (a) 1 v(A B) 0 v(A) 1 v(B)
(b) 0 v(A B) 1 v(A) 0 v(B)
interpretation model formula f 1 v(f ) holds interpretation. model set formulas model every formula
set. semantical notion logical consequence defined follows:
LP evaluations v either 1 v(A) B , v(B) = {0}
shown LP coincides propositional logic tautologies
valid inferences. particular, although AA B tautology LP , corresponding
inferences {A A} |=LP B also {A, A} |=LP B invalid. counterexample,
consider evaluation B strictly false A, undetermined (both true
false), undetermined well. 0 v(A A), valuation
postulates 1 v(A B). Therefore, {A A} 2LP B also {A, A} 2LP B.
Therefore, Logic Paradox paraconsistent logic.
turns postulate indirect consistency satisfied case strict
rules ASPIC + instantiated valid inferences Logic Paradox, is,
Rs iff finite |=LP . following counterexample brought
attention Graham Priest (personal communication).
Example 5.1. Take SAF defined argumentation theory knowledge base
Kp Kn , Kn = Kp = {a, b, c, b c}. suppose Rs
211

fiGrooters & Prakken

corresponds Logic Paradox defeasible rules (Rd = ).
Finally, assume arguments least one ordinary premise equally preferred
according argument ordering SAF .
easily checked Kp implies least one a, b c must paradoxical.
Therefore, exists argument A1 : a, ab, ac, bc (aa)(bb)(cc).
Since tautologies preserved Logic Paradox, (bb), (aa) (cc)
also entailed K. implies exists argument A2 : a, ab, ac, bc
((a a) (b b) (c c)). arguments use strict rules
attacked premises. However, exist argument built K
conclusion Kp . show this, Kp model found
v(d) 6= {0} holds 1 v(d).
Model 1: show case follows Kp .
Take model v(a) = {1}, v(b) = {1} v(c) = {0, 1}. clear v(a) =
{1}, v(a b) = {1}, v(a c) = {0, 1} v(b c) = {0, 1}, 1 v(a).
Model 2: show case (a b) follows Kp .
Take model v(a) = {1}, v(b) = {1} v(c) = {0, 1}. clear v(a) =
{1}, v(a b) = {1}, v(a c) = {0, 1} v(b c) = {0, 1}, 1 v((a b)).
Model 3: show case (a c) follows Kp .
Take model v(a) = {1}, v(b) = {0, 1} v(c) = {1}. clear v(a) =
{1}, v(a b) = {0, 1}, v(a c) = {1} v(b c) = {0, 1}, 1 v((a c)).
Model 4: show case (b c) follows Kp .
Take model v(a) = {0, 1}, v(b) = {0} v(c) = {0}. clear v(a) =
{0, 1}, v(ab) = {0, 1}, v(ac) = {0, 1} v(bc) = {1}, 1 v((bc)).
means arguments defeat one arguments A1 A2 ,
A1 A2 elements complete extension E. means (aa)(bb)(cc)
((a a) (b b) (c c)) elements Conc(E). Therefore, argumentation
theory satisfy postulate direct consistency.
5.2 Da Costas Basic C-system: C
system C Da Costa (1974) adds axioms positive
logic, negation free first-order logic (these added axioms called Dialectic Double
Negation (DDN) Exclusive Middle (EM) respectively). C certain aspects
dual intuitionistic logic, since intuitionistic logic axiom EM invalid
axiom Non-Contradiction (NC, (A A)) valid. C , axiom EM valid NC
invalid. Intuitionistic logic tolerates incomplete situations avoid inconsistency,
C-systems admit inconsistent situations, incomplete situations removed.
example, C possible three sentences A, A, true. However unlike
Logic Paradox, sentences one truth value. Next semantics
proof theory given sound complete respect other.
propositional version C following bivalent valuation formulas built
logical language L.
1. v(A B) = 1 v(A) = 1 v(B) = 1
2. v(A B) = 1 v(A) = 1 v(B) = 1
212

fiTwo Aspects Relevance Structured Argumentation

3. v(A B) = 1 v(A) = 0 v(B) = 1
4. v(A) = 1 v(A) = 0
5. v(A) = 1 v(A) = 1
interpretation formula f valuation form model v(f ) = 1
interpretation. interpretation model set formulas
model every formula set. semantical logical consequence:
C evaluations v either v(A) = 1 B , v(B) = 0
easy show {A, A} 0C B, since following valuation function chosen:
v(A), v(A) = 1 v(B) = 0.
Replacing LP C source strict rules ASPIC + still yields counterexamples direct consistency. (In example, denotes material implication).
Example 5.2. Suppose knowledge base K = Kp Kn , Kn =
Kp = {a, b, b} following valuation: v(a) = 1, v(a b) = 1
v(a b) = 1. suppose Rs corresponds C defeasible
rules (Rd = ). Finally, assume basic fallible arguments equally strong.
following two arguments exist: A1 : a, b b A2 : a, b b.
shown Figure 4. two arguments use strict rule. means
arguments defeated premises. However, none a, (a b),
(a b) ClRs (K), arguments defeat A1 A2
premises. Therefore, A1 A2 elements complete extension E, means
b, b Conc(E).

Figure 4: Arguments Example 5.2

6. Another Attempt: Instantiating ASPIC + Weak Consequence
section investigate use another paraconsistent consequence notion, socalled weak consequence relation originally proposed Rescher Manor (1970). basic
idea sentence weakly follows set sentences classically follows
least one consistent subset S. notion clearly related classical argumentation,
formally show Section 9. also inspired early consistency-based approaches
argumentation, Krause, Ambler, Elvang-Gransson, Fox (1995).
knowledge, first use system defeasible rules. first discuss
weak-consequence notion define used overcome trivialisation
problem ASPIC + .
213

fiGrooters & Prakken

6.1 Weak Consequence
Weak consequence standard propositional language formally defined follows.
Definition 6.1 (Weak consequence relation, `W ). `W
maximal consistent subset ` classical logic.
Note word maximal fact required, since according Lindenbaums
Lemma every consistent set formulas extended maximally consistent one.
easy see {a, a} `W b hold, {a, a} maximal
consistent subset {a, a}. Therefore, consequence relation paraconsistent.
next discuss three common properties.
[Reflexivity] , `W .
property holds `W . belongs maximally consistent subset
. classical logic, holds , ` . Therefore, obviously holds
`W .
[Monotonicity] `W , , `W .
monotonicity property proven `W follows. must maximal
consistent subset ` . Since , must exist maximal
consistent extension , 0 , 0 ` . Therefore, , `W .
[Cut] , `W `W , `W .
rule hold. counterexample, consider set = {a, b}.
`W b , b `W b, case `W b.
Since Cut rule hold, naive instantiation ASPIC + strict rules
logic W would avoid explosion, shown following example:
Example 6.1. Consider following knowledge base Kp = {p, p, r}, Kn = , instantiate
strict rules valid inferences finite sets logic W let Rd = .
following arguments constructed:
A1 : p
B : p
D:r

A2 : A1 p r
C : A2 , B r

Argument C concludes r.
underlying reason problem Cut rule hold `W ,
example Kp 0W r. want ASPIC + strict part behave according `W ,
chaining strict rules excluded.3 Example 6.1, argument C allowed
3. similar idea suggested Martin Caminada personal communication. discuss idea
Section 10.

214

fiTwo Aspects Relevance Structured Argumentation

since A2 already strict top rule. prohibition chaining strict rules
prevent trivialisation. end, ASPIC + notion argument must redefined,
results ASPIC ? framework.
6.2 ASPIC ? Framework
change ASPIC + framework ASPIC ? framework disallowing
chaining strict rules arguments. first need change definition argument:
Definition 6.2. [Argument? ASPIC ? ] argument? basis knowledge
base K = (K, ) argumentation system (L, R, n, 0 ) is:
1. K
Prem() = {},
Conc() = ,
Sub(A) = {},
TopRule(A) = undefined.
2. A1 , . . . , A1 , . . . , arguments? defeasible top rule
K exists strict rule Conc(A1 ), . . . , Conc(An ) Rs .
Prem(A) = Prem(A1 ) . . . Prem(An ),
Conc(A) = ,
Sub(A) = Sub(A1 ) . . . Sub(An ) {A},
TopRule(A) = Conc(A1 ), . . . , Conc(An ) .
3. A1 , . . . , A1 , . . . , arguments? exists defeasible rule
Conc(A1 ), . . . , Conc(An ) Rd .
Prem(A) = Prem(A1 ) . . . Prem(An ),
Conc(A) = ,
Sub(A) = Sub(A1 ) . . . Sub(An ) {A},
TopRule(A) = Conc(A1 ), . . . , Conc(An ) .
Arguments? special case normal arguments. Therefore, definitions
(sets ) arguments case term argument replaced argument?
without problems. Attack defeat arguments? . Structured
abstract argumentation frameworks ASPIC ? framework except
contain arguments? . Accordingly, notions justified defensible
arguments? conclusions still defined Section 2.
new ASPIC ? framework motivates new interpretation strict-closure postulate case Rs corresponds logic L. fact weak-consequence notion `W
satisfy Cut rule shows closure Rs general coincide
closure consequence notion logic Rs corresponds. fact,
Example 6.1 easily extended counterexample `W , since r strict
closure {p, p} {p, p} 6`W r. turn gives reason doubt whether full
closure strict rules always desirable. Arguably desirability depends
properties logic Rs corresponds: logic satisfy Cut rule,
full strict closure desirable. Instead, desirable cases
extensions closed consequence adopted logic Rs .
215

fiGrooters & Prakken

prove ASPIC ? strict rules correspond `W . end restrict
strict-closure postulate allowed applications strict rules will, moreover,
introduce new rationality postulate logical closure.
think analysis compatible Caminada Amgouds (2007) reason
proposing strict-closure postulate, since arguably implicit assumption behind
postulate formulas strict closure always reachable arguments
constructed arguments extension:
idea closure answer argumentation-engine
closed strict rules. is, provide engine strict rule
b (. . . ), together various rules, inference engine outputs
justified conclusion, also output b justified conclusion.
Consequently, b also supported acceptable argument (emphasis
added current authors). (Caminada & Amgoud, 2007, p. 294).
quote compatible new account strict closure new logical-closure
postulate, since Rs based `W , implicit assumption strict
closure extension equates supportable acceptable argument satisfied.
formalise new interpretation strict-closure postulate, new notions strict
closure indirect consistency needed. first explain notation. Recall
Section 2 set arguments, F A(S) denotes set basic fallible arguments
N P (S) denotes set necessary premises argument S.
corresponding notions ASPIC ? denoted F A? (S) N P ? (S). Next,
set arguments? let # defined F A? (S) N P ? (S). set # contains
arguments? strict top rule, arguments?
conclusions strict rule applied form new arguments? . strict
closure indirect consistency redefined follows.
Definition 6.3. [Closure? ] X L, let closure ? X strict rules,
? (X), smallest set containing X consequent strict rule
denoted ClR

Rs whose antecedents X. set arguments? said closed? strict
? (Conc(S # )).
rules Conc(S) = ClR

new strict closure notion amounts one-steps application strict rules.
Definition 6.3 clarified following example.
Example 6.2. Suppose following sets: Kn = {p}, Kp = {q, t} Rd = {q
r, s} RS corresponds classical propositional consequence. following
set arguments? constructed.
A1 = p
A5 = A3
A2 = q
A6 = A1 , A4 p r
A3 =
A7 = A5 v
A4 = A2 r
example # = {A1 , . . . , A5 } allowed apply strict rules
arguments? . closed? strict rules, since example p q
/ Conc(S)
? (Conc(S # )).
argument? A8 = A1 , A2 p q constructed, p q ClR

216

fiTwo Aspects Relevance Structured Argumentation

Definition 6.4. [Indirect consistency? ] set X L indirectly consistent ?
? (X). Otherwise indirectly inconsistent? . set
L , ClR

arguments? said indirectly consistent ? Conc(S # ) indirectly consistent? .
Henceforth consistent ? mean indirectly consistent ? . rest ASPIC ?
framework ASPIC + framework.

7. Verifying Postulates ASPIC ? Weak Consequence
section investigate class instantiations just-defined ASPIC ? framework
language L (nonempty) propositional language set Rs strict rules
corresponds Rescher Manors (1970) notion weak consequence language.
precisely:
Rs = {S | `W finite}
speak instantiations ASPIC ? ASPIC ? SAFs `W .
theorem states SAFs avoid trivialisation sense Definition 4.1.
Theorem 7.1. ASPIC ? SAFs `W defined argumentation theory
trivialising argumentation system.
remains investigated whether class instantiations ASPIC ? satisfies
Caminada Amgouds (2007) rationality postulates newly proposed postulate
logical closure. end, first formally specify postulates ASPIC ? .4
Definition 7.1. [Rationality postulates ASPIC ? ] Let = (A, C, ) ASPIC ?
structured argumentation framework defined ASPIC ? = (L, R, n)
K = Kn Kp . Let AF abstract argumentation framework corresponding
let {complete, preferred, grounded, stable}. Then:
satisfies closure subarguments postulate iff -extensions E AF
holds argument? E subarguments? E;
satisfies consistency postulate iff -extensions E AF holds
Conc(E) consistent ? ;
satisfies strict closure postulate iff -extensions E AF holds
? (Conc(E # )).
Conc(E) = ClR

Rs corresponds logic L, satisfies L-closure postulate iff extensions E AF L holds Conc(E) `L Conc(E).
Since grounded (preferred, stable) extensions also complete extensions, suffices
prove postulates complete extensions. one way prove first three
postulates try adapt proofs Modgil Prakken (2013) ASPIC +
4. Caminada Amgoud (2007) also propose postulates intersection extensions conclusion sets, since satisfaction directly follows satisfaction postulates individual
extensions, postulates ignored.

217

fiGrooters & Prakken

ASPIC ? . However, problem general, Rs corresponds `W ,
closure transposition contraposition hold ASPIC ? . first give
counterexample closure transposition.
Example 7.1. Since {a, b, c} `W b monotonicity logic W ,
holds {a, b, c, a} `W b. means a, b, c, b Rs . However,
maximal consistent subset {a, b, (a b), a} proves c classical
logic. Therefore {a, b, (a b), a} 0W c a, b, (a b), c
/ Rs .
means strict rules Rs argumentation system argumentation
theory = (AS, K) instantiated valid inferences logic W ,
argumentation theory closed transposition.
similar counterexample given closure contraposition.
Example 7.2. Consider knowledge base Kn = Kp = {a, b, c, a}. Since
{a, b, c, a} `W ab {a, b, (ab), a} 0W c, follows ab ClRs ({a, b, c, a}),
c ClRs ({a, b, (a b), a}) (because chaining strict rules allowed).
Therefore, strict rules Rs argumentation system argumentation
theory instantiated valid inferences logic W , closed
contraposition.
Therefore, results Modgil Prakken (2013) cannot directly used
purposes. However, somewhat different formal setting, Dung Thang (2014) provide
weaker conditions satisfying rationality postulates, implied
imply closure transposition contraposition. therefore use results
guidance verifying postulates just-defined class instantiations ASPIC ? .
Dung Thang (2014) formulate results terms adaptation Amgoud
Besnards (2013) abstract-logic approach abstract argumentation abstract attack
support relations arguments. defining adaptation apply
call rule-based systems. turns purposes need Dung
Thangs general abstract framework instead adapt definitions
rule-based instantiation ASPIC ? . so, definitions
results indicate counterpart Dung Thangs rule-based systems. definitions
implicitly assume given ASPIC ? structured argumentation framework.
Definition 7.1. [Base argument? , (cf. Dung & Thang, 2014, Def. 6)] Let
argument? BA finite set subarguments? A. BA base
? (Conc(BA));
Conc(A) ClR


argument? C, C defeats C defeats BA.
following example shows intuitive idea base.
Example 7.3. Take Kn = , Kp = {a, b}, Rs = {c d} Rd = {a, b c}.
following arguments? constructed: A1 : a, A2 : b, A3 : A1 , A2 c A4 : A3 d.
See Figure 5.
A4 attacked subarguments? A1 , A2 , A3 strict top
rule. Every argument? attacks A1 A2 also attacks A3 , every argument?
218

fiTwo Aspects Relevance Structured Argumentation

Figure 5: Arguments Example 7.3
attacks A4 also attacks A3 . easy see every argument? attacks A3 also
attacks A4 . Conc(A4 ) ClRs (Conc(A3 )), {A3 } base A4 . kind
reasoning applies fact set {A1 , A2 , A3 } also base A4 .
However note set {A1 , A2 } base A4 , A3 rebutted without
A1 A2 attacked.
Definition 7.2. [Generation arguments? , (cf. Dung & Thang, 2014, Def. 7)]
argument? said generated set arguments? S, base B
B Sub(S). set arguments? generated denoted GN (S).
following lemma follows definition GN (S).
Lemma 7.2. [(cf. Dung & Thang, 2014, Lemma 1(2))] every set arguments?
S, Sub(S) GN (S).
Theorem 7.3. [(cf. Dung & Thang, 2014, Thm. 1)] Let E complete extension,
GN (E) = E.
Note Lemma 7.2 Theorem 7.3 immediately imply closure? subarguments?
postulate since every complete extension E: Sub(E) GN (E) = E.
Theorem 7.4. ASPIC ? SAF satisfies closure? subarguments? postulate.
Definition 7.3. [Compact, (cf. Dung & Thang, 2014, Def. 8)] ASPIC ? SAF
compact set arguments? S, GN (S) closed? strict rules. equal
? (Conc(GN (S)# )).
Conc(GN (S)) = ClR

following two theorems later combined proving closure? subarguments?
postulate.
Theorem 7.5. [(cf. Dung & Thang, 2014, Thm. 2)] compact ASPIC ? SAF
satisfies closure? strict rules postulate.
Theorem 7.6. ASPIC ? SAF `W compact.
Definition 7.4. [Cohesive, (cf. Dung & Thang, 2014, Def. 9)] ASPIC ? SAF
cohesive, inconsistent? set arguments? S, GN (S) conflicting (attacks itself).
219

fiGrooters & Prakken

Theorem 7.7. [(cf. Dung & Thang, 2014, Thm. 3)] cohesive ASPIC ? SAF
satisfies consistency? postulate.
next two definitions needed proving cohesiveness.
Definition 7.5. [Self-contradiction axiom, (cf. Dung & Thang, 2014, Def. 14)]
ASPIC ? SAF said satisfy self-contradiction axiom, minimal inconsistent
? (X).
set X L: X ClR

Definition 7.6. [Axiom consistent? ] ASPIC ? SAF axiom consistent?
? (K ) consistent? .
ClR
n

Theorem 7.8. [(cf. Dung & Thang, 2014, Thm. 5)] compact, axiom consistent?
ASPIC ? SAF reasonable argument ordering satisfies self-contradiction axiom,
SAF cohesive.
Theorem 7.9. ASPIC ? SAF `W satisfies self-contradiction axiom.
Combining Theorem 7.5, 7.6, 7.7, 7.8 7.9 results following important conclusion.
Theorem 7.10. ASPIC ? SAF `W axiom consistent? reasonable argument ordering satisfies strict-closure? consistency? postulates.
Finally, satisfaction proved newly proposed postulate logical closure.
Below, `CL denotes classical consequence.
Lemma 7.11. ASPIC ? SAF `W satisfies following property: set
arguments? holds Conc(S) `CL , Conc(S # ) `CL .
Lemma 7.12. ASPIC ? SAF `W satisfies following property: set E
arguments? holds Conc(E) strictly closed consistent , Conc(E)
classically consistent.
Theorem 7.13. ASPIC ? SAF `W axiom consistent? reasonable argument ordering satisfies logical closure postulate.
concluded identified class instantiations new ASPIC ? framework Rescher Manors (1970) weak consequence notion satisfies
consistency closure postulates preventing trivialisation case rebutting
arguments. order obtain results, ASPIC + framework adapted
prohibiting chaining strict rules, resulting new ASPIC ? framework new
notions strict closure indirect consistency, plus new postulate closure
logical consequence.

8. Minimality Arguments
section address two aspects minimality arguments. first explain issue
inference rules non-minimal sets antecedents detail, investigate
effects requiring strict rules minimal antecedent sets, finally study
issue non-circular arguments.
220

fiTwo Aspects Relevance Structured Argumentation

8.1 Issue Minimality Antecedent Sets
said introduction, deductive approaches argumentation (e.g., Besnard & Hunter,
2008; Gorogiannis & Hunter, 2011; Amgoud & Besnard, 2013) require arguments
subset-minimal set premises. example, Gorogiannis Hunter define argument
(S, p) set well-formed propositional formulas p well-formed
propositional formula, that:
1. consistent classical propositional logic
2. implies p classical propositional logic
3. proper subset implies p classical propositional logic
(Hunter 2007 explores relaxations properties notion approximate arguments.) However, arguments apply defeasible inference rules, third requirement undesirable, since defeasible rules based specific information may
well stronger. Consider following example:
Example 8.1. Consider argumentation system Rs = , Rd = {p q; p, r
q; r q} consider knowledge base Kn = {p, r} Kp = . following
arguments constructed.
A1 :
A2 :
B1 :
B2 :
C:

p
A1 q
r
A1 , B1 q
B1 q

real-world example two defeasible rules q consider example
introduction: Observations done ideal circumstances usually correct
reasonable account rule strength stronger Observations usually correct.
consider argument ordering arguments compared specificity.
C B2 A2 C incomparable. semantics unique extension
{A1 , A2 , B1 , B2 } results, intuitively correct outcome. However, arguments
required subset-minimal premises, B2 cannot constructed
outcome different: grounded extension {A1 , B1 } two preferred
stable extensions, namely, {A1 , B1 , A2 } {A1 , B1 , C}.
analysis holds defeasible versus strict rules. Consider defeasible rule
p q strict rule p, r q: clearly want rule argument
q premises p r, since could well stronger defeasible argument q
premise p. However, noted above, analysis apply strict inference
rules, since strict inference guarantees conclusion given premises. still makes
sense strict inference rules applied subset-minimal set formulas.
requirement, number arguments generated significantly
reduced case restriction introduced, result efficient system.
following example illustrates problems arise without requirement
strict rules subset-minimal sets antecedents.
221

fiGrooters & Prakken

Example 8.2. Suppose strict rules argumentation system instantiated
classical logic consider knowledge base Kn = Kp = {p, q, r, s, t, u}.
among things, argument A1 : p, q p q constructed. Since classical logic
monotonic, following arguments (and more) p q also constructed.
A2 : p, q, r p q
A4 : p, q, p q
A6 : p, q, r, p q

A3 : p, q, p q
A5 : p, q, u p q
A7 : p, q, t, u p q

arguments table Example 8.2 considered redundant given A1 .
Recall defeasible rules different, since specific defeasible rules
conclusion may well stronger, explained. problem adapt
minimality requirement setting defeasible rules investigate extent
affects conclusions drawn.
8.2 Minimal Arguments? ASPIC ? Framework
next investigate whether excluding strict inferences non-subset-minimal sets
formulas makes difference. line focus paper prove results
ASPIC ? , proofs ASPIC + counterparts would entirely similar.
particular, want know whether conclusions drawn argumentation framework affected case arguments? required minimal. prove
rather weak condition argument? ordering conclusions
cases.
First, described idea minimal arguments formally defined. main
difference Definition 6.2 clause (2) disallows application strict rules
non-minimal antecedent set.
Definition 8.1. [Minimal argument? ] minimal argument? basis knowledge base K = (K, ) argumentation system (L, R, n, 0 ):
1. K
Prem() = {},
Conc() = ,
Sub(A) = {},
TopRule(A) = undefined.
2. A1 , . . . , A1 , . . . , minimal arguments? defeasible top rule
K exists strict rule Conc(A1 ), . . . , Conc(An ) Rs
exist strict rule a1 , . . . , ai {a1 , . . . , ai } Conc({A1 , . . . , })
Rs .
Prem(A) = Prem(A1 ) . . . Prem(An ),
Conc(A) = ,
Sub(A) = Sub(A1 ) . . . Sub(An ) {A},
TopRule(A) = Conc(A1 ), . . . , Conc(An ) .
222

fiTwo Aspects Relevance Structured Argumentation

3. A1 , . . . , A1 , . . . , arguments? exists defeasible rule
Conc(A1 ), . . . , Conc(An ) Rd .
Prem(A) = Prem(A1 ) . . . Prem(An ),
Conc(A) = ,
Sub(A) = Sub(A1 ) . . . Sub(An ) {A},
TopRule(A) = Conc(A1 ), . . . , Conc(An ) .
Recall functions defined definition also defined sets
arguments? .
easy see minimality constraint exclude construction
argument B2 Example 8.1, desired.
order show non-minimal arguments? required obtain right
extensions, first define notions minimal extended version argument? .
Informally, minimal version argument? argument?
except non-minimal strict rule replaced minimal version
strict rule, i.e., strict rule consequent subset-minimal antecedent
set. may lead deletion subarguments? A, namely, subarguments?
deleted antecedents. Conversely, extended version minimal argument?
argument except zero strict rules
replaced non-minimal version strict rules. may lead addition
subarguments? , namely, subarguments? added antecedents.
Definition 8.2. [A ] argument? A, argument? minimal
argument? iff:
K = A;

form A1 , . . . , =
, . . . , Aj that:

Conc({Ai , . . . , Aj }) minimal subset Conc({A1 , . . . , })
Conc(Ai ), . . . , Conc(Aj ) Rs ;
?
every k {i, . . . , j} holds
k minimal argument Ak ;



form A1 , . . . , =
1 , . . . , every

1 n holds Ai minimal argument? Ai .

set arguments? S, define minimal arguments? S.
Below, write , mean argument? minimal argument? A.
Note guaranteed unique. example, argument? : pq, qp p
two minimal variants, namely A1 : p q p A2 : q p p.
Obviously, following structured argumentation frameworks ASPIC ? frameworks
contain arguments? .
Definition 8.3. [Minimal SAF , SAF ] SAF = (A, C, ), let SAF minimal
SAF SAF = (A , C , ). C defined C (A ) =
(A ).
223

fiGrooters & Prakken

Definition 8.4. [Extended argument? , A+ ] argument? A, argument? A+
extended argument? iff:
K A+ = A;
= A1 , . . . , , A+ = A01 , . . . , A0m n every A0i
(1 n) holds A0i extended argument? A+
Ai ;
= A1 , . . . , , A+ = A01 , . . . , A0n every A0i (1 n)
holds A0i extended argument? A+
Ai .
Below, whenever write A+ , mean argument? A+ extended argument?
A.
Note also A+ A+ . general, A+ unique.
following example clarifies definitions given above.
Example 8.3. Consider SAF following arguments:
A1 : p
A3 : r

A2 : p q
A4 : p, r q

A1 , A2 A3 minimal arguments? , SAF contains three arguments? .
A2 minimal argument? corresponding A4 , A2 =
4 . Now, also easy see
.
Furthermore


one
example


A+
indeed A4 A+
4
2.
2
results follow, needed argument? A, A+ cannot
strictly preferred argument ordering cannot strictly preferred
. implied current definition reasonable argument? ordering.
illustrated example.
Example 8.4. Consider Example 8.3, assume p r Kn assume
following argument? ordering (where x means usual x x):
A1 A2 ; A3 A4 , A2 A4 . argument? ordering satisfies properties reasonable
argument? ordering nevertheless counterintuitive, since difference
A2 A4 A4 contains non-minimal version strict rule applied A2 : since
strict rules guarantee consequent given antecedent, A4 intuitively
strictly preferred A2 .
Therefore, introduce definition tolerable argument? ordering.
Definition 8.5. [Tolerable argument? ordering] tolerable argument? ordering
if:
every A+ A, A+ A;
A, .
224

fiTwo Aspects Relevance Structured Argumentation

Intuitively, argument? ordering tolerable replacing minimal strict rule
one non-minimal versions cannot make argument? stronger replacing
non-minimal strict rule minimal version cannot make argument? weaker.
strict inference rules, meant capture deductively valid inferences,
natural property, since operations always amount adding, respectively, deleting
attackable subarguments? .
next lemmas needed proving equivalence conclusions
drawn minimal non-minimal structured argumentation frameworks.
Lemma 8.1. argument? extended argument? A+ following holds:
A0 Sub(A) argument? A00 Sub(A+ ) A00 = A0+ .
lemma clarified example below.
Example 8.5. Take following arguments? :
A1 : p p q
B1 : p, r p q

A2 : A1
B2 : B1

?
0
obvious B2 A+
2 . Lemma 8.1 states every subargument
+
0
0+
0
?
0
A2 subargument B A2 B = . example, take A1 .
B1 subargument? B2 B1 = A+
1.

preceding result needed proving argument? attacks/defeats B,
minimal argument? corresponding attacks/defeats every extended version B.
Lemma 8.2. tolerable argument? ordering argument? defeats/attacks B,
every defeats/attacks every B + .
following lemma follows Lemma 8.2.
Lemma 8.3. tolerable argument? ordering, complete extensions E:
1. E, E every ;
2. B
/ E, B +
/ E B + .
following lemma states Dungs characteristic function (see Definition 2.1)
monotonic bijection complete extensions SAF onto complete extensions
SAF . lemma based results Dung, Toni, Mancarella (2010)
ABA framework, discussed below. First, following definition needed.
Definition 8.6. [S ] set arguments S, let set arguments
argument minimal.
Lemma 8.4. Let SAF = (A , C , ) minimal structured argumentation framework corresponding SAF = (A, C, ) (for ASPIC ? framework), let AF
abstract argumentation framework corresponding SAF . Let tolerable argument?
ordering. Also, let C C sets complete extensions SAF SAF respectively let FAF Dungs characteristic function AF . Then:
225

fiGrooters & Prakken

1. E C : FAF (E) = E.
2. E C : FAF (E ) = E E C .
Clause (1) says set acceptable minimal arguments? w.r.t. extension
minimal SAF change also non-minimal arguments? considered. Clause (2)
says set acceptable arguments? w.r.t. extension non-minimal SAF
change minimal arguments? considered.
Below, main result stated conclusions drawn SAF
SAF s. theorem also based results Dung et al. (2010) ABA
framework.
Theorem 8.5. Let SAF = (A , C , ) minimal structured argumentation framework corresponding SAF = (A, C, ) (for ASPIC ? framework). Let tolerable
argument? ordering. Take {complete, grounded, preferred, stable} F defined
Definition 2.1, then:
1. Let E extension SAF , E extension SAF .
2. Let E extension SAF , F (E) extension SAF .
combining results concluded conclusions drawn
ASPIC ? structured argumentation framework affected case arguments?
required minimal.
results generalise Modgil Prakken (2013), prove result
special case arguments minimal sets premises (their Proposition 28).
related work Dung et al. (2010), study minimal arguments assumption-based
argumentation (ABA) reconstructed Dung et al. (2007) terms Dungs (1995)
abstract argumentation frameworks. mentioned above, version ABA
reconstructed special case ASPIC + framework without preferences defeasible
rules. fact, Dung et al. (2010) define notion non-redundant argument,
general notion minimal argument defined paper. non-redundant
argument turn defined terms less redundant relation. Dung et al. (2007)
prove similar results ABA Theorem 8.5 ASPIC ? . constructions
proofs clearly inspired work Dung et al. (2010). However, purely formal link
ASPIC ? cannot established, since unlike ABA, ASPIC ? allow chaining
strict rules. reason make detailed formal comparison here.
8.3 Disallowing Repetition Conclusions
address second aspect minimality, studying modification ASPIC ?
circular arguments? avoided, is, argument? cannot
conclusion one subarguments? . requirement part ASPIC +
defined Prakken (2010) Modgil Prakken (2013) part system
Vreeswijk (1997), ASPIC -style definition argument originates.
argumentation theory, circular arguments generally regarded fallacious, seems
good idea exclude them. addition, computational benefits, shown
section. First Definition 6.2 argument? modified follows.
226

fiTwo Aspects Relevance Structured Argumentation

Definition 8.7. [Strongly minimal arguments? ASPIC ? ] Strongly minimal arguments?
defined arguments? Definition 8.1 except following condition added
clauses (2) (3):
6 Conc{(A1 , . . . , )}.
call structured argumentation framework strongly minimal set
arguments? defined Definition 8.7.
Disallowing repetition conclusions general change extensions, following example shows.
Example 8.6. Consider argumentation theory Kn = Kp = Rd = { p,
p, p p}. least two arguments? p least one p:
A1 :
A2 :
B:

p
A1 p
p

ordering rules Rd p < p < p p (where x < means
strictly preferred x), last-link ordering defined Modgil
Prakken (2013), B defeats A1 A2 defeats B, yields grounded extension
contains neither three arguments? . However, A2 cannot constructed,
grounded extension contains B.
noted, excluding circular arguments? avoids fallacies also computational benefits. particular, shown K Rd finite,
argument? finite number attackers. words Dung (1995)
means induced abstract argumentation framework finitary. shown Dung,
finitary AFs number computational benefits. Among things, grounded
extension constructed iterative application F operator (see Definition 2.1
above) empty set.
prove result, first introduce notation relative given .
r defined Rs Rd let Cons(r) = .
set R let Cons(T ) = { | = Cons(r) r }.
Let X = {S Rs | K Cons(Rd )}. Informally, X set strict rules
potentially applicable AT, is, antecedents belong
K consequent defeasible rule. Note set
equal Rs , since might rules Rs apply set well-formed
formulas L yet
/ Kand defeasible rule consequent .
L define X = {S X | }. Informally, X set
potentially applicable rules formula .
next prove following lemma.
Lemma 8.6. finite L holds X finite.
227

fiGrooters & Prakken

following result proven.
Theorem 8.7. Let SAF = (A, C, ) strongly minimal structured argumentation
framework corresponding argumentation theory finite K Rd .
L set {A | Conc(A) = } finite.
Note result cannot proved without exclusion arguments chain
strict rules, since otherwise infinite sets arguments conclusion p
generated constructing arguments follows n, provides counterexample
Theorem 8.7 case arguments excluded:
A1 = p
A2 = A1 p
A3 = A2 p
...
= An1 p
remains verify rationality postulates strongly minimal SAFs. turns
result Section 7 needs reproved only-if half Theorem 7.6.
Theorem 8.8. strongly minimal ASPIC ? SAF `W compact.
Combined results Section 7 implies
Theorem 8.9. strongly minimal ASPIC ? SAF `W axiom consistent?
reasonable argument? ordering satisfies closure? consistency? postulates.

9. ASPIC ? Generalisation Classical Argumentation
section explain combining two main contributions far, class
instantiations ASPIC ? obtained proper generalisation three respects
two versions classical argumentation defined Besnard Hunter (2008) Gorogiannis Hunter (2011)5 . two versions holds particular interest
since Gorogiannis Hunter proven two seven versions
classical argumentation satisfy consistency postulates. observation
explained follows. Modgil Prakken (2013) prove classical argumentation
two forms premise attack called direct undercut direct defeat reconstructed
following class instantiations ASPIC + : defeasible rules, preference relations
arguments, ordinary premises, L negation symbol defined
Definition 2.3, arguments indirectly consistent premise sets, strict rules
instantiated classically valid inferences finite sets premises. Modgil
Prakken (2013) also prove requiring arguments subset-minimal premises
implying conclusion change result. shown
classical-logic instantiations prohibition ASPIC ? chain strict rules
make difference ASPIC + either. show this, shown
instantiations ASPIC + Rs corresponds Rescher Manors (1970) notion
5. thank Sanjay Modgil suggesting us personal communication

228

fiTwo Aspects Relevance Structured Argumentation

weak consequence arguments? minimal proper generalisations
classical argumentation. actually prove case nontrivial preferences
preferences fully determined premises; result case
empty preference relation follows special case.
minimal ASPIC + ASPIC ? argumentation theory mean argumentation theory arguments arguments? subset-minimal set premises.
Clearly, arguments? means also minimal sense Definition 8.1.
Moreover, c-structured argumentation framework notion Modgil Prakken
(2013): restricted present context amounts requirement SAFs
arguments classically consistent premises. notion minimal c-structured argumentation framework defined accordingly.
Theorem 9.1. Let argumentation system L classical-logic language
Rs corresponding classical logic, let K = Kp knowledge base L. Let
obtained removing Rs inference rules invalid according
`W . Let = (AS, K) minimal ASPIC + argumentation theory = (AS , K)
corresponding minimal ASPIC ? argumentation theory. let SAF = (A, C, )
SAF = (A , C , ) be, respectively, minimal c-structured argumentation framework
defined ASPIC + minimal structured argumentation framework defined
ASPIC ? that:
A, B, C A: Prem(A) = Prem(B) (C iff C B C iff
B C). Likewise .
, B : B iff B .6
{complete, grounded, preferred, stable} holds that:
1. Let E extension SAF , E extension SAF
Conc(E) = Conc(E ).
2. Let E extension SAF , F (E) extension SAF
Conc(E) = Conc(F (E)).
special case result case without defeasible rules, necessary
premises preferences ASPIC ? `W gives conclusion sets classical argumentation. first proper extension classical argumentation preferences.
second proper extension necessary premises Kn third proper extension case defeasible rules, observing ASPIC ? strict rule
applied conclusion least one defeasible subargument? applied subsetminimal classically consistent set formulas classically implies consequent
strict rule.

10. Summary, Discussion Conclusion
section summarise discuss results put context related
work.
6. well-defined since construction SAF SAF fact `W draws inferences
inconsistent sets holds A.

229

fiGrooters & Prakken

10.1 Summary Results
paper tackled several related issues concerning relevance structured argumentation. carried investigations context ASPIC + framework,
consolidates extends one main AI approaches argumentation: modelling combined reasoning strict defeasible inference rules. One main contribution
paper solve long-standing trivialisation problem first identified Pollock (1994,
1995). problem tame trivialising effect Ex Falso principle classical
logic way preserves consistency closure properties. solve problem,
instantiated strict rules ASPIC + Rescher Manors (1970) paraconsistent
logic W. make work, disallow chaining strict rules arguments since
logic W satisfy Cut rule; resulted adapted framework ASPIC ?
new view postulate strict closure. argued important
whether conclusion sets closed strict rules whether closed
consequence notion logic strict rules correspond. Accordingly,
modified notion strict closure introduced new rationality postulate logical
closure. proved new versions consistency closure postulates ASPIC ? .
also investigated whether two well-known paraconsistent logics, system C
Da Costa (1974) Logic Paradox Priest (1979, 1989), suitable sources
strict rules. showed cases would lead violation indirect consistency. future research want consider paraconsistent logics want
generally study properties paraconsistent logic satisfy useful
source strict rules ASPIC ? .
second issue studied paper minimality arguments. first showed
natural assumption argument ordering, restricting strict-rule application subset-minimal sets formulas affect conclusions drawn ASPIC ? .
many cases make reasoning efficient ignoring irrelevant information. also noted result easily adapted ASPIC + . disallowed
circular arguments ASPIC ? showed may change status arguments
affect satisfaction rationality postulates. addition, proved
finite set defeasible rules finite knowledge base ASPIC ? without circular
arguments computationally attractive property induced abstract argumentation frameworks finitary sense Dung (1995). latter result cannot
adapted ASPIC + since crucially relies prohibition chain strict rules.
results minimality hold independently choice strict rules.
Finally, proved combining contributions paper version ASPIC ?
obtained proper generalisation two versions classical argumentation
premise attack defined Besnard Hunter (2008) Gorogiannis Hunter (2011).
two versions particular interest since Gorogiannis Hunter
proven two seven versions classical argumentation satisfy
consistency postulates.
noted course investigations changed ASPIC +
framework originally defined Prakken (2010). fact, first so.
Modgil Prakken (2013) consider four variants ASPIC + framework. First,
consider versions without constraint arguments consistent premises
230

fiTwo Aspects Relevance Structured Argumentation

variants define variant conflict-freeness sets
arguments defined relative defeat relation attack relation. Furthermore, Caminada et al. (2014) study variant ASPIC + arguments also
rebutted conclusions derived strict rules, provided least one subargument
ordinary premise applies defeasible rule. Finally, Wu Podlaszewski (2015) study
variant ASPIC + set conclusions subarguments argument
consistent. Thus work ASPIC + resulted family frameworks
based core ideas making different design choices specific points, new
members family may result future. think healthy situation,
since amounts systematic investigation effects different design choices within
common approach, may applicable certain kinds problems.
10.2 Discussion Related Work
next discuss related work.
10.2.1 Additional Rationality Postulates Caminada et al. (2012)
mentioned introduction, Caminada et al. (2012) formulate new set rationality
postulates addition Caminada Amgoud (2007), characterise cases
trivialisation problem avoided (called postulates non-interference
crash-resistance). Wu (2012) Wu Podlaszewski (2015) prove adaptation
ASPIC + consistent arguments preferences new postulates
satisfied complete semantics. investigate semantics. However,
attempt prove Caminada et al.s postulates, two reasons. First, want
obtain results case preferences semantics well and, second,
seems us Caminada et al.s postulates fact capture stronger intuitive notion
one study paper, proving satisfaction new postulates
would required purposes paper.
intuition Caminada et al.s notion trivialisation follows. consider
knowledge bases, pairs sets formulas L sets defeasible rules.
Two knowledge bases defined disjoint composed disjoint sets
atomic formulas L. knowledge base KB1 = (K1 , D1 ) contaminating
every knowledge base KB2 = (K2 , D2 ) set extensions (under given semantics)
KB1 KB1 KB2 (where (K1 , D1 ) (K2 , D2 ) = (K1 K2 , D1 D2 ).
system said satisfy crash resistance exist contaminating
knowledge base system.
consider ASPIC + ASPIC ? instantiation Kp = , Kn = {p} single
defeasible rule d, n( d) = d. instantiation stable extensions, since
argument = defeats defeated argument.
knowledge base (, { d}) contaminating, since syntactically disjoint knowledge
base added extensions. generally, situation arise
knowledge base stable extensions.
However, paper interested excluding situations
taming contaminating effect Ex Falso principle. end introduced
simpler definition trivialisation Definition 4.1 managed avoid trivialisation
231

fiGrooters & Prakken

thus defined even stable semantics, since just-given example case
trivialisation sense Definition 4.1. conclude Caminada et al.s
postulates capture stronger notion notion paraconsistency (the focus
paper). future would interesting study whether class instantiations
ASPIC ? satisfies Caminada et al.s postulates, would ideally preceded
study exactly captured postulates.
10.2.2 Wu (2012), Wu Podlaszewski (2015)
alternative attempt solve trivialisation problem, Wu Podlaszewski (2015)
introduced inconsistency-cleaned ASP IC Lite system. system similar
argumentation formalism treated Caminada Amgoud (2007) seen
system specified ASPIC + arguments equally preferred. Wu Podlaszewski define argument consistent set conclusions subarguments
directly consistent. argumentation framework inconsistency-cleaned inconsistent arguments removed. Wu Podlaszewski prove inconsistency-cleaned
version ASP IC Lite satisfies original rationality postulates Caminada
Amgoud (2007) new postulates Caminada et al. (2012). solves
trivialisation problem retaining known results closure consistency. However,
Wu (2012) Wu Podlaszewski (2015) provide counterexample (originally due
Leon van der Torre) satisfaction consistency strict-closure postulates case
preferences added last-link argument ordering Prakken (2010) applied.
example originally presented ASP IC Lite system, translated
ASPIC + framework.
Example 10.1. [(Wu, 2012; Wu & Podlaszewski, 2015)] Given knowledge base K = ,
Rd = { p; p q; p q} Rs instantiated valid inferences classical
logic. Assume p priority 1 (lowest), p q priority 2 (middle)
p q priority 3 (highest). case, construct following arguments
associated (last-link principle) preferences. Table 1 depicts arguments
generated7 Figure 6 shows defeat relation arguments.
Argument
A1 : p
A2 : p q
A3 : A1 q
A4 : A1 , A2 q
A5 : A1 , A3 (p q)
A6 : A2 , A3 p

Preference
(1)
(2)
(3)
(1)
(1)
(2)

Table 1: Six arguments

Figure 6: Partial abstract AF

7. Note classical reasoning allows generation infinitely arguments irrelevant
six arguments.

232

fiTwo Aspects Relevance Structured Argumentation

Argument A6 inconsistent argument. according solution proposed
Wu Podlaszewski case without preferences, A6 needs deleted
argumentation framework. Figure 7 shows resulting argumentation framework.

Figure 7: Inconsistency-cleaned version
complete extension E = {A1 , A2 , A3 , A4 , A5 }. satisfy closure
strict rules A2 A3 E A6 E. Moreover, direct consistency
also satisfied since A3 A4 complete extension E, conclusions
q p q consistent.
Arguments A3 A4 opposite conclusions without preferences A4 would defeat
A3 . However, preference ordering chosen counterexample, A4 weaker
A3 A4 cannot defeat A3 . A3 A4 also attacked subarguments.
fact arguments complete extension causes problem.
Every argument concludes p uses A1 subargument, implies
inconsistent argument removed framework. Therefore,
A1 defeated argument, means A3 A4 complete
extension. concluded inconsistent argument like A6 really needed
defeat A3 A4 .
Furthermore, observed A6 deleted, problems all.
Figure 6 shows case one complete extension E = {A2 }.
satisfies consistency strict closure. Therefore, example, undesirable
A6 removed.
Consider next ASPIC ? framework defined paper. arguments
constructed original framework. way, one complete extension
{A2 } and, explained above, rationality postulates satisfied. approach
ASPIC ? therefore general solution Wu Podlaszewski (2015),
since applies frameworks include preferences defeasible rules.
noted idea forbid chaining strict rules earlier suggested
us Martin Caminada (personal communication). However, combined suggestion
idea disallow inconsistent arguments; case, counterexample
consistency strict closure still constructed. Apart this, one could say
logic W , satisfy Cut rule, provides theoretical foundation
idea disallow chaining strict rules.
first sight, would seem system allows inconsistent arguments flawed
even satisfies consistency closure postulates. However, note arguments
never extension. Although explained Caminada (2005),
inconsistent arguments sometimes prevent arguments extension,
problem arguments based Ex Falso principle, since
233

fiGrooters & Prakken

principle holds matter logic. Among things, means allowing arguments
based Ex Falso would dramatically increase number (counter)arguments,
would lead computational problems. contrast, inconsistent arguments argument
A6 Example 10.1 arise specific modeling choices Rd dictated logic
proliferate, logical computational point view
need exclude them.
10.3 Dungs (2014) Rule-Based Systems
Dung (2014) introduces formalism rule-based systems (further studied Dung, 2016),
essentially notational variant ASPIC + restricted literal languages empty
knowledge bases (necessary ordinary facts represented strict defeasible rules
empty antecedents). Dung introduces three new rationality postulates. postulate
attack monotonicity informally says strengthening argument cannot eliminate
attack argument another. postulate credulous cumulativity informally
means changing conclusion argument extension necessary fact
cannot eliminate extension. Finally, property irrelevance redundant defaults
says adding redundant defaults change set extensions.
Dung investigates argument orderings studied Modgil Prakken (2013)
whether satisfy consistency postulates, positive negative
results. results valuable, Dung (2014) unfortunately, somewhat confuses
matters referring orderings studied Modgil Prakken ASPIC +
semantics. Thus overlooks distinction ASPIC + general framework
instantiations framework. argument orderings studied Modgil Prakken
inherent ASPIC + framework example orderings. ASPIC +
framework variants leave every room ways define argument ordering.
noted Dung (2016) refer Modgil Prakkens orderings
ASPIC + semantics thus respects orderings inherent
ASPIC + .
present purposes Dungs findings strictly speaking irrelevant since
studied particular argument orderings. future research would interesting investigate whether use particular argument orderings ASPIC ? satisfies Dungs postulates
attack monotonicity irrelevance redundant defaults. However, disagree
Dung (2014) credulous cumulativity would desirable property. contrary,
Prakken Vreeswijk (2002, section 4.4) believe property instead
undesirable, since strengthening defeasible conclusion indisputable fact may make
arguments stronger before. give power defeat arguments
before. may well result loss extension
conclusion promoted indisputable fact.
10.4 Conclusion
paper successfully addressed open issues concerning relevance structured argumentation. solved trivialisation problems arise argumentation includes full classical logic created prospects reducing computational complexity enforcing minimality non-circularity arguments ensuring
234

fiTwo Aspects Relevance Structured Argumentation

closure consistency results. done context ASPIC approach,
resulting new variant ASPIC + framework called ASPIC ? well-behaved
class instantiations new framework. class instantiations shown
proper generalisation classical argumentation preferences defeasible rules.
class instantiations properties main contribution paper.
paper employed flexible attitude towards design choices within ASPIC
approach. thus shown approach fruitful one, provided distinction
frameworks instantiations kept mind.

Acknowledgement
thank three JAIR reviewers many useful comments earlier versions
paper.

Appendix A. Proofs
appendix contains proofs results reported paper.
A.1 Proofs Section 7
Theorem 7.1 ASPIC ? SAFs `W defined argumentation theory
trivialising argumentation system.
Proof. end, must show argumentation system propositional language Rs defined, knowledge base K defined
{, } K L hold argument?
conclusion constructed basis K AS. Consider AS.
choose K = Kn Kp Kp = Kn = {, } formula L (
guaranteed exist since L assumed nonempty). definition `W clearly holds
K 6`W ( ) since consistent subset K classically imply contradiction.
exists strict argument? ( ) basis K AS.
Theorem 7.3 Let E complete extension, GN (E) = E.
Proof. First note according Definition 7.2 set arguments? Sub(S)
GN (S), therefore E GN (E).
Suppose argument? C defeats argument? GN (E). Let BA base
BA Sub(E), C defeats BA. Hence C defeats Sub(E) defeats
E. Since E complete extension, every defeat E counter defeated E.
defended E, E. Therefore GN (E) E.
Theorem 7.5 compact ASPIC ? SAF satisfies closure? strict rules postulate.
Proof. Let E complete extension. compactness? implies GN (E) closed?
strict rules. Theorem 7.3 E closed? strict rules.
Theorem 7.6 ASPIC ? SAF `W compact.
235

fiGrooters & Prakken

Proof.
? (GN (S)# )]
[Conc(GN (S)) ClR

? (GN (S)# ). needs shown
Let set arguments? ClR

? (X).
Conc(GN (S)). Let X minimal subset Conc(GN (S)# ) ClR

?
Hence strict argument A0 X conclusion . let SX minimal set arguments? GN (S)# s.t. Conc(SX ) = X. Let argument? obtained
replacing leaf A0 (viewed directed acyclic graph) labelled literal
X argument? conclusion SX . Note possible since
arguments? SX basic fallible arguments? necessary premises. obvious conclusion . shown SX base A. Suppose B
argument? defeating A. Since A0 strict argument? X, B must defeat basic fallible
subargument? SX . Hence B defeats SX . Thus GN (S). Hence Conc(GN (S)).
? (GN (S)# )]
[Conc(GN (S)) ClR

? (GN (S)# ).
Suppose Conc(GN (S)), shown ClR

?
Conc(GN (S)) means argument GN (S) Conc(A) = .
Suppose form Kp , F A? (GN (S)) thus GN (S)# .
? () N P ? (GN (S)) respecSuppose form Kn , ClR

? (GN (S)# ).
tively, ClR

Suppose form A1 , . . . , , F A? (GN (S)) thus GN (S)# .
Finally, suppose form A1 , . . . , , since A1 , . . . , basic fallible
? (GN (S)# ).
arguments? A1 , . . . , GN (S)# . Therefore ClR

? (GN (S)# ).
concluded ClR

proven AF compact.
Theorem 7.7 cohesive ASPIC ? SAF satisfies consistency? postulate.
Proof. Let E complete extension. Suppose E inconsistent? . cohesion,
follows GN (E) conflicting. Theorem 7.3 states E conflicting.
contradiction since E complete extension, E consistent? .
Theorem 7.8 compact, axiom consistent? ASPIC ? SAF reasonable argument?
ordering satisfies self-contradiction axiom, SAF cohesive.
Proof. Let inconsistent? set arguments? take minimal inconsistent? subset
0 Sub(S). Definition 6.4 combined axiom consistency? minimality 0
causes 0 6= contains basic fallible arguments? necessary premises. Remark 0 cannot consist necessary premises, axiom consistency? .
note Conc(S 0 ) minimal inconsistent set. Since AF satisfies self? (Conc(S 0 )). Let B
contradiction axiom, Conc(S 0 ) holds ClR

?
0
weakest argument Conc(B) = . Note B cannot necessary premise reasonable argument? ordering fact 0 must
contain basic fallible arguments? . construction 0 holds 0 GN (S 0 )# .
? (Conc(GN (S 0 )# )). compactness AF follows
Therefore ClR

Conc(GN (S 0 )). Therefore, argument? GN (S 0 )
Conc(A) = . Hence attacks B. base 0 , concluded basic
fallible subarguments? 0 . B weakest argument? 0 ,
236

fiTwo Aspects Relevance Structured Argumentation

reasonable argument? ordering fact basic fallible subarguments?
0 implies B. means defeats B. Since B 0 GN (S 0 ) GN (S),
GN (S) conflicting. Therefore, AF cohesive.
Theorem 7.9 ASPIC ? SAF `W satisfies self-contradiction axiom.
Proof. proved every minimal inconsistent? set X L holds
? (X). Let X minimally inconsistent? set take = X\.
X, ClR

Note maximal consistent? subset X S, ` (where ` denotes
classical entailment). deduction theorem classical logic ` , implies
` . Since maximal consistent? subset X, X `W . holds every
? (X). concluded AF satisfies self-contradiction
X, ClR

axiom.
Lemma 7.11 ASPIC ? SAF `W satisfies following property: set
arguments? holds Conc(S) `CL , Conc(S # ) `CL .
Proof. Suppose Conc(S) `CL . Consider argument? Ti \ # .
definition `W choice Rs holds Conc(Ti ) `CL i. since `CL
satisfies Cut rule, Conc(S # ) `CL .
Lemma 7.12 ASPIC ? SAF `W satisfies following property: set E
arguments? holds Conc(E) strictly closed consistent , Conc(E)
classically consistent.
Proof. Assume Conc(E) strictly closed consistent suppose contradiction Conc(E) `CL , . Consider subset-minimal E Conc(S) `CL
, . Lemma 7.11 Conc(S # ) `CL , . Consider minimal # Conc(T ) `CL , . Note cannot empty.
Conc(T ) holds Conc(T )\{} `CL . choice holds Conc(T )\{}
classically consistent. since Conc(E) strictly closed argument? #
strict top rule, exists argument? 0 E 0
Conc(T 0 ) = Conc(T ) \ {}. Conc(E) consistent .
Theorem 7.13 ASPIC ? SAF `W axiom consistent? reasonable argument ordering satisfies logical closure postulate.
Proof. Suppose Conc(E) `W complete extension E consider subsetminimal E Conc(S) `W . definition `W choice Rs
holds Conc(S) classically consistent Conc(S) `CL . Lemma 7.11
Conc(S # ) `CL . Moreover, Conc(S # ) classically consistent
Lemma 7.12 fact subset classically consistent set also classically
consistent (note Theorem 7.10 Conc(E) strictly closed consistent ,
conditions Lemma 7.12 fulfilled). Conc(S # ) `W choice Rs
exists strict rule Conc(S # ) . Since argument? # strict top rule,
Conc(E) strict closure? E.
237

fiGrooters & Prakken

A.2 Proofs Section 8.2
Lemma 8.1 argument? extended argument? A+ following holds:
A0 Sub(A) argument? A00 Sub(A+ ) A00 = A0+ .
Proof. proof proof induction height argument? (viewed directed
acyclic graph). Suppose element K (so height 1), A0 A+
equal A. means A00 also equal easy see A00 = A0+ .
Suppose lemma holds arguments? height {1, 2, . . .}.
proven arguments? height + 1. Take arbitrary argument? height + 1
take subargument? A0 A. Note cannot element K since height
greater 1. Therefore, form A1 , . . . , / .
two possibilities: either (i) A0 subargument? one arguments? A1 , . . . , ,
(ii) A0 equal A.
(i). j {1, . . . , n} A0 subargument? Aj . Aj height i,
00
0+
00
must A00 Sub(A+
j ) = . According Definition 8.4,
?
+
subargument .
(ii). A0 equal A. Take A00 A+ , follows A00 = A0+ A00 Sub(A+ ).
proved every argument? lemma holds.
Lemma 8.2 tolerable argument? ordering argument? defeats/attacks B,
every defeats/attacks every B + .
Proof.
Attack
(i). Suppose undercuts B, Conc(A) = n(r) defeasible top rule r
B 0 Sub(B). definition , conclusion every A.
Lemma 8.1 holds every B + , subargument? B 00 Sub(B + )
B 00 = B 0+ . Note B 0 B 00 defeasible top rule, since definition
B + , strict rules mutate. follows every undercuts B +
B 00 .
(ii). Suppose undermines B, Conc(A) ordinary premise B 0 . Every
B + also ordinary premise. definition (Definition 8.2), conclusion
every A. Therefore undermines every B + B 0 .
(iii). Suppose rebuts B, Conc(A) conclusion basic fallible
argument? B 0 Sub(B). conclusion every A. Lemma 8.1,
holds every B + , subargument? B 00 Sub(B + ) B 00 = B 0+ .
Note conclusions B 0 B 00 B 00 also defeasible top
rule. Therefore rebuts B + B 00 .
Defeat
Suppose argument? defeats B. means attacks B subargument? B 0 .
(i) undercuts B, (ii) B 0 .
(i). Suppose undercuts B, follows every undercuts B + B 00 (see
reasoning attack (i)). implies defeats every B + .
(ii). Otherwise case B 0 . Note that, tolerable
argument? ordering, A. proof attack shown every attacks
238

fiTwo Aspects Relevance Structured Argumentation

every B + B 00 , B 00 = B 0+ . tolerable argument? ordering causes B 00 B 0 .
Therefore every B 00 holds B 00 , every defeats every B + .
Lemma 8.3 tolerable argument? ordering, complete extensions E:
1. E, E every ;
2. B
/ E, B +
/ E B + .
Proof. (1). Suppose E let B argument? defeating . Lemma
8.2, B defeats A. Therefore, must argument? C E C defeats B.
Hence, defended E thus E.
(2). Suppose B
/ E. exists argument? defeats B
exist C E defeats A. According Lemma 8.2, defeats every B + ,
B + defended E hence B +
/ E.
Lemma 8.4 Let SAF = (A , C , ) minimal structured argumentation framework corresponding SAF = (A, C, ) (for ASPIC ? framework), let AF
abstract argumentation framework corresponding SAF . Let tolerable argument?
ordering. Also, let C C sets complete extensions SAF SAF respectively let FAF Dungs characteristic function AF . Then:
1. E C : FAF (E) = E.
2. E C : FAF (E ) = E E C .
Proof. Take two sets arguments? X X . Suppose FAF (Y )
FAF (X), must argument? defended X . Since
X , defended . Therefore, monotonicity FAF (E) respect
set inclusion obvious.
(1). shown FAF (E) indeed function C C FAF (E) =
E showing FAF (E) complete extension SAF , E complete extension
SAF . Let E complete extension SAF . First shown E
admissible set SAF .
E conflict-free , conflict-free A. also defeats minimal
argument? defeating E contains every minimal argument? defended E.
E B defeats A, take B . B defeats
Lemma 8.2, C E defeats B . C also defeats B Lemma 8.2
combined fact B B + . means defended E.
concluded E defeats every argument? defeats E. already shown E
conflict-free, E admissible set.
argument? FAF (E), defended E , every
E. Therefore, (FAF (E) E) = , thus FAF (E) = E.
Now, shown FAF (E) complete extension. Let defended
FAF (E) let B defeat A. Hence, C FAF (E) defeating B. C FAF (E)
means C defended E, thus E defeats every argument? defeating C. Suppose
argument? defeats C , also defeats C Lemma 8.2. Therefore, E must defeat
D, C defended E. Since E complete extension SAF , C E. C
239

fiGrooters & Prakken

defeats B (Lemma 8.2). Therefore, E defeats B. Thus, defended E, therefore
FAF (E). consequence, FAF (E) complete.
(2). Let E complete extension SAF . shown E complete SAF .
First shown E admissible set . Since E conflict-free, E
conflict-free well.
Lemma 8.3 fact E complete SAF , minimal version
arguments? E belongs E. Let defeat E . Hence, B E defeating A.
According Lemma 8.3 B E, B E . Hence, B defeats (Lemma 8.2). Thus,
E admissible.
minimal argument? defended E defended E hence belongs E
E . E therefore complete.
Since E E E complete, clear FAF (E ) FAF (E) = E.
shown argument? defended E also defended E . Let argument?
defended E SAF let B argument? defeating A. Hence, argument?
C E defeating B C E defeats B. Hence E defeats B. Thus defended
E SAF . concluded FAF (E ) FAF (E) = E, i.e. FAF (E ) = E.
[Injective] Take X, C FAF (X) = FAF (Y ). obvious FAF (X) =
FAF (Y ) . according proof point (1) FAF (X) = X FAF (Y ) = .
Therefore, follows X = .
[Surjective] shown C X C FAF (X) = .
take X , proof point (2) provides X C
FAF (X) = .
[Bijective] Injectivity surjectivity provides FAF bijection C onto C.
Theorem 8.5 Let SAF = (A , C , ) minimal structured argumentation framework corresponding SAF = (A, C, ) (for ASPIC ? framework). Let tolerable
argument? ordering. Take {complete, grounded, preferred, stable}, then:
1. Let E extension SAF , E extension SAF .
2. Let E extension SAF , F (E) extension SAF .
Proof.
[T = complete]
Let C C sets complete extensions SAF SAF , respectively. Lemma
8.4 states FAF bijection C C. immediately provides F (E) C.
second point Lemma 8.4 states E C .
[T {grounded, preferred}]
Lemma 8.4 follows immediately E C , FAF (E) minimal
maximal respect set inclusion E E minimal maximal respectively
C . Hence E grounded preferred SAF FAF (E) grounded
preferred SAF , respectively.
[T = stable]
(1). Take E stable extension SAF . Suppose contradiction E
stable extension SAF . must argument? ,
/ E
defeated argument? E . However,
/ E implies
/ E since
?
minimal. E stable, thus must argument B E B defeats A.
240

fiTwo Aspects Relevance Structured Argumentation

B E (Lemma 8.3) defeats A. clear B E , contradiction
fact E defeat A. Therefore, E stable extension SAF .
(2). Take E stable extension SAF . Suppose contradiction FAF (E)
stable extension SAF . must argument?

/ FAF (E) FAF (E) defeat A.
/ FAF (E) means defended
E. Therefore, must argument? B defeats
C E defeats B. Since E stable SAF , B E E defeats A. implies
FAF (E) defeats A, contradiction fact FAF (E) defeat
A. Therefore, FAF (E) stable extension SAF .
A.3 Proofs Section 8.3
Lemma 8.6 finite L holds X finite.
Proof. Note single strict rule , since K
Rd finite, set sets equal set antecedents strict rule X
also finite.
Theorem 8.7 Let SAF = (A, C, ) strongly minimal structured argumentation framework corresponding argumentation theory finite K Rd . L
set {A | Conc(A) = } finite.
Proof. prove result iteratively constructing set finite arguments?
constructible K AS.
A0 = K Ad0 = As0 = ;
Ai+1 = Ai Adi+1 = Asi+1
Adi+1 = {A = Q | Q Ai Conc(Q) Rd 6 Conc(Q)}.
Asi+1 = {A = Q | Q Ai Conc(Q) Rs 6 Conc(Q)
B Q holds TopRule(B) Rs }.

easy see ni=0 Ai set constructible finite arguments? .
prove theorem, note first A0 finite since K finite Adi finite
since Rd finite. contrast, Asi infinite since given choice Rs ,
set wff infinite number strict rules applies. However, follows Lemma 8.6
L exists finite number arguments? Asi
conclusion . Together observations imply L exists
finite number arguments? conclusion Ai .
left prove construction finite. follows fact
K Rd finite fact Definition 8.7 rules cannot repeated
argument? . point, Adi = . also Asi+1 = since strict rules cannot
chained argument? . construction set finite arguments? constructible
K finite, L step construction finite
set arguments? conclusion created. proves theorem.

241

fiGrooters & Prakken

Theorem 8.8 strongly minimal ASPIC ? SAF `W compact.
Proof.
? (GN (S)# )]
[Conc(GN (S)) ClR

? (GN (S)# ). needs shown
Let set arguments? ClR

? (X).
Conc(GN (S)). Let X minimal subset Conc(GN (S)# ) ClR

Hence strict argument? A0 X conclusion . let SX
minimal set arguments? GN (S)# s.t. Conc(SX ) = X. problem
repetition conclusions here, since X A0 = . Let argument? obtained
replacing leaf A0 (viewed directed acyclic graph) labelled literal
X argument? conclusion SX . Note repeats conclusion
need since clearly Conc(GN (S)) done. Otherwise,
construction possible since, firstly, arguments? SX basic fallible arguments?
necessary premises and, second, definition GN (S)# holds
repeat conclusion X. obvious conclusion . shown
SX base A. Suppose B argument? defeating A. Since A0 strict argument?
X, B must defeat basic fallible subargument? SX . Hence B defeats SX . Thus
GN (S). Hence Conc(GN (S)).
? (GN (S)# )]
[Conc(GN (S)) ClR

Theorem 7.6.
proven strongly minimal SAF compact.
A.4 Proofs Section 9
Theorem 9.1 Let argumentation system L classical-logic language
Rs corresponding classical logic, let K = Kp knowledge base L. Let
obtained removing Rs inference rules invalid according
`W . Let = (AS, K) minimal ASPIC + argumentation theory = (AS , K)
corresponding minimal ASPIC ? argumentation theory. let SAF = (A, C, )
SAF = (A , C , ) be, respectively, minimal c-structured argumentation framework
defined ASPIC + minimal structured argumentation framework defined
ASPIC ? that:
A, B, C A: Prem(A) = Prem(B) C iff C B C iff
B C. Likewise .
, B : B iff B .
{complete, grounded, preferred, stable} holds that:
1. Let E extension SAF , E extension SAF
Conc(E) = Conc(E ).
2. Let E extension SAF , F (E) extension SAF
Conc(E) = Conc(F (E)).
Proof. First, easy see A. Next, show induction structure
arguments? argument? \ exists argument? B
Prem(A) = Prem(B) Conc(A) = Conc(B).
242

fiTwo Aspects Relevance Structured Argumentation

base case K. B = A. inductive case consider =
B1 , . . . , Bn . induction hypothesis arguments? B1 , . . . , Bn satisfy coni induction
ditions. Bi (1 n) form Bi = Bki , . . . , Bm




hypothesis yields Bk , . . . , Bm K. replacing Bi Bki , . . . , Bm
B1 , . . . , Bn yields argument? B satisfying conditions. Note corresponding strict inference rule Rs since classical logic satisfies cut rule
arguments? consistent premises.
easy prove along lines proof Lemma 8.2 argument?
argument? attacked/defeated also attacked/defeated B
vice versa, argument? attacking/defeating also attacks/defeats B vice
versa. proof Theorem 9.1 completed along lines proof
Theorem 8.5.

References
Amgoud, L., & Besnard, P. (2013). Logical limits abstract argumentation frameworks.
Journal Applied Non-classical Logics, 23, 229267.
Bench-Capon, T., Prakken, H., & Visser, W. (2011). Argument schemes two-phase
democratic deliberation. Proceedings Thirteenth International Conference
Artificial Intelligence Law, pp. 2130, New York. ACM Press.
Besnard, P., & Hunter, A. (2008). Elements Argumentation. MIT Press, Cambridge,
MA.
Bondarenko, A., Dung, P., Kowalski, R., & Toni, F. (1997). abstract, argumentationtheoretic approach default reasoning. Artificial Intelligence, 93, 63101.
Brewka, G. (1991). Nonmonotonic Reasoning: Logical Foundations Commonsense. Cambridge University Press, Cambridge.
Caminada, M. (2005). Contamination formal argumentation systems. Proceedings
Seventeenth Belgian-Dutch Conference Artificial Intelligence (BNAIC-05),
Brussels, Belgium.
Caminada, M., & Amgoud, L. (2007). evaluation argumentation formalisms.
Artificial Intelligence, 171, 286310.
Caminada, M., Carnielli, W., & Dunne, P. (2012). Semi-stable semantics. Journal Logic
Computation, 22, 12071254.
Caminada, M., Modgil, S., & Oren, N. (2014). Preferences unrestricted rebut. Parsons, S., Oren, N., Reed, C., & Cerutti, F. (Eds.), Computational Models Argument.
Proceedings COMMA 2014, pp. 209220. IOS Press, Amsterdam etc.
Caminada, M., Sa, S., Alcantara, J., & Dvorak, W. (2015). difference
assumption-based argumentation abstract argumentation. IFCoLog Journal
Logic Applications, 2, 1534.
Da Costa, N. (1974). theory inconsistent formal systems. Notre Dame Journal
Formal Logic, 15 (4), 497510.
243

fiGrooters & Prakken

Dung, P. (1995). acceptability arguments fundamental role nonmonotonic reasoning, logic programming, nperson games. Artificial Intelligence, 77,
321357.
Dung, P. (2014). axiomatic analysis structured argumentation prioritized default
reasoning. Proceedings 21st European Conference Artificial Intelligence
(ECAI 2014), pp. 267272.
Dung, P. (2016). axiomatic analysis structured argumentation priorities. Artificial Intelligence, 231, 107150.
Dung, P., Mancarella, P., & Toni, F. (2007). Computing ideal sceptical argumentation.
Artificial Intelligence, 171, 642674.
Dung, P., & Thang, P. (2014). Closure consistency logic-associated argumentation.
Journal Artificial Intelligence Research, 49, 79109.
Dung, P., Toni, F., & Mancarella, P. (2010). design guidelines practical argumentation systems. Baroni, P., Cerutti, F., Giacomin, M., & Simari, G. (Eds.),
Computational Models Argument. Proceedings COMMA 2010, pp. 183194. IOS
Press, Amsterdam etc.
Garcia, A., & Simari, G. (2004). Defeasible logic programming: argumentative approach.
Theory Practice Logic Programming, 4, 95138.
Ginsberg, M. (1994). AI nonmonotonic reasoning. Gabbay, D., Hogger, C., & Robinson, J. (Eds.), Handbook Logic Artificial Intelligence Logic Programming,
pp. 133. Clarendon Press, Oxford.
Gorogiannis, N., & Hunter, A. (2011). Instantiating abstract argumentation classicallogic arguments: postulates properties. Artificial Intelligence, 175, 14791497.
Grooters, D., & Prakken, H. (2014). Combining paraconsistent logic argumentation.
Parsons, S., Oren, N., Reed, C., & Cerutti, F. (Eds.), Computational Models
Argument. Proceedings COMMA 2014, pp. 301312. IOS Press, Amsterdam etc.
Hunter, A. (2007). Real arguments approximate arguments. Proceedings 22nd
National Conference Artificial Intelligence (AAAI-07), pp. 6671.
Jakobovits, H., & Vermeir, D. (1999). Robust semantics argumentation frameworks.
Journal Logic Computation, 9, 215261.
Krause, P., Ambler, S., Elvang-Gransson, M., & Fox, J. (1995). logic argumentation
reasoning uncertainty. Computational Intelligence, 11 (1), 113131.
Lin, F., & Shoham, Y. (1989). Argument systems. uniform basis nonmonotonic
reasoning. Principles Knowledge Representation Reasoning: Proceedings
First International Conference, pp. 245255, San Mateo, CA. Morgan Kaufmann
Publishers.
Modgil, S., & Prakken, H. (2013). general account argumentation preferences.
Artificial Intelligence, 195, 361397.
Modgil, S., & Prakken, H. (2014). ASPIC+ framework structured argumentation:
tutorial. Argument Computation, 5, 3162.
244

fiTwo Aspects Relevance Structured Argumentation

Pollock, J. (1987). Defeasible reasoning. Cognitive Science, 11, 481518.
Pollock, J. (1990). theory defeasible reasoning. International Journal Intelligent
Systems, 6, 3354.
Pollock, J. (1992). reason defeasibly. Artificial Intelligence, 57, 142.
Pollock, J. (1994). Justification defeat. Artificial Intelligence, 67, 377408.
Pollock, J. (1995). Cognitive Carpentry. Blueprint Build Person. MIT
Press, Cambridge, MA.
Prakken, H. (2010). abstract framework argumentation structured arguments.
Argument Computation, 1, 93124.
Prakken, H. (2012). reflections two current trends formal argumentation.
Logic Programs, Norms Action. Essays Honour Marek J. Sergot
Occasion 60th Birthday, pp. 249272. Springer, Berlin/Heidelberg.
Prakken, H., & Modgil, S. (2012). Clarifying misconceptions ASPIC+ framework. Verheij, B., Woltran, S., & Szeider, S. (Eds.), Computational Models
Argument. Proceedings COMMA 2012, pp. 442453. IOS Press, Amsterdam etc.
Prakken, H., & Sartor, G. (1997). Argument-based extended logic programming defeasible priorities. Journal Applied Non-classical Logics, 7, 2575.
Prakken, H., & Vreeswijk, G. (2002). Logics defeasible argumentation. Gabbay, D.,
& Gunthner, F. (Eds.), Handbook Philosophical Logic (Second edition)., Vol. 4, pp.
219318. Kluwer Academic Publishers, Dordrecht/Boston/London.
Prakken, H., Wyner, A., Bench-Capon, T., & Atkinson, K. (2015). formalisation
argumentation schemes legal case-based reasoning ASPIC+. Journal Logic
Computation, 25, 11411166.
Priest, G. (1979). logic paradox. Journal Philosophical Logic, 8 (1), 219241.
Priest, G. (1989). Reasoning truth. Artificial Intelligence, 39 (2), 231244.
Read, S. (1988). Relevant logic. Blackwell, Oxford.
Rescher, N., & Manor, R. (1970). inference inconsistent premises. Journal
Theory Decision, 1, 179219.
Simari, G., & Loui, R. (1992). mathematical treatment defeasible argumentation
implementation. Artificial Intelligence, 53, 125157.
Vreeswijk, G. (1997). Abstract argumentation systems. Artificial Intelligence, 90, 225279.
Walton, D. (1996). Argumentation Schemes Presumptive Reasoning. Lawrence Erlbaum
Associates, Mahwah, NJ.
Wu, Y. (2012). Argument Conclusion. Argument-based Approaches Discussion, Inference Uncertainty. Doctoral Dissertation Faculty Sciences, Technology
Communication, University Luxemburg.
Wu, Y., & Podlaszewski, M. (2015). Implementing crash-resistence non-interference
logic-based argumentation. Journal Logic Computation, 25, 303333.

245

fiJournal Artificial Intelligence Research 56 (2016) 429-461

Submitted 02/16; published 07/16

Efficient Mechanism Design Online Scheduling
Xujin Chen
Xiaodong Hu

xchen@amss.ac.cn
xdhu@amss.ac.cn

AMSS, Chinese Academy Science, Beijing, China

Tie-Yan Liu
Weidong
Tao Qin

tyliu@microsoft.com
weima@microsoft.com
taoqin@microsoft.com

Microsoft Research, Beijing, China

Pingzhong Tang

kenshin@mail.tsinghua.edu.cn

Tsinghua University, Beijing, China

Changjun Wang

wcj@amss.ac.cn

Beijing University Technology, Beijing, China

Bo Zheng

zhengb10@mails.tsinghua.edu.cn

Tsinghua University, Beijing, China

Abstract
paper concerns mechanism design online scheduling strategic setting.
setting, job owned self-interested agent may misreport release
time, deadline, length, value job, need determine
schedule jobs, also payment agent. focus design
incentive compatible (IC) mechanisms, study maximization social welfare (i.e.,
aggregated value completed jobs) competitive analysis. first derive two lower
bounds competitive ratio deterministic IC mechanism characterize
landscape research: one bound 5, holds equal-length jobs;
bound ln + 1 o(1), holds unequal-length jobs, maximum ratio
lengths two jobs. propose deterministic IC mechanism show
simple mechanism works well two models: (1) preemption-restart
model, mechanism achieve optimal competitive ratio 5 equal-length jobs
1

near optimal ratio ( (1)
2 + o(1)) ln unequal-length jobs, 0 < < 1
small constant; (2) preemption-resume model, mechanism achieve
optimal competitive ratio 5 equal-length jobs near optimal competitive ratio
(within factor 2) unequal-length jobs.

1. Introduction
Online scheduling widely studied literature (Baruah, Koren, Mao, Mishra,
Raghunathan, Rosier, Shasha, & Wang, 1992; Baruah, Haritsa, & Sharma, 1994; Porter,
2004; Zheng, Fung, Chan, Chin, Poon, & Wong, 2006; Ting, 2008), job characterized release time, deadline, length, value successful completion
deadline. Inspired emerging areas like computational economics cloud computing, consider strategic setting online scheduling problem, job
owned self-interested agent may incentive manipulate schedulc
2016
AI Access Foundation. rights reserved.

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

ing algorithm order better off. specific, agent may deliberately delay
release time job, inflate length, misreport value deadline.
Given situation, carefully designed online scheduling mechanism needed
regulate strategic behaviors agents (approximately) optimize system
objectives. work, focus maximization social welfare, i.e., total
value completed jobs.1 use competitive analysis (Lavi & Nisan, 2004) evaluate
performance mechanism, compares social welfare implemented
mechanism (without knowledge future jobs) optimal offline
allocation (with knowledge future jobs).
work, consider two scheduling models: preemption-restart model (Ting,
2008) preemption-resume model (Porter, 2004). preempted, jobs first
model restart beginning; jobs second model resume
break point. Since preemption always assumed work, two models also
referred restart model resume model, respectively, involved jobs
called non-resumable resumable, respectively.
1.1 Problem Formulation
consider online scheduling models infinite time period = R0 . Suppose
single machine processes one job given time. Jobs come
time, use J denote set jobs. job j J owned self-interested
agent (which also denoted j simplicity); characterized private type
j = (rj , dj , lj , vj ) R>0 R>0 , rj release time2 , dj deadline,
lj length (i.e., processing time), vj value job completed
deadline.
resumable job j completed processed lj time units total
release time rj deadline dj , non-resumable job j completed
processed lj consecutive time units release time rj deadline
dj .
Let = maxi,jJ llji maximum ratio lengths two jobs.
simplicity, assume job lengths normalized, i.e., lj [1, ] j J, assume
known advance following practice work Chan et al. (2004) Ting
(2008).
study direct revelation mechanisms, agent participates simply
declaring type job j = (rj , dj , lj , vj ) time rj . use denote profile
reported types agents. Given declared types agents, mechanism
used schedule/allocate jobs determine payment agent.
consider reasonable mechanisms (1) schedule job reported
deadline (2) schedule job processed reported length.
Given certain mechanism job sequence , use qj (, t) denote whether
job j completed time (if completed, qj (, t) = 1; otherwise qj (, t) = 0).
1. also referred weighted throughput scheduling literature.
2. Note release time also referred arrival time online auction literature (Parkes, 2007).
earliest time agent full knowledge job. Thus earliest time job
available scheduling process.

430

fiEfficient Mechanism Design Online Scheduling

value agent j extracts mechanism represented qj (, dj )vj ,
P
social welfare mechanism represented W (M, ) = j qj (, dj )vj .
Let pj () denote amount money mechanism charges agent j. assume agents quasi-linear preferences (Nisan, 2007), i.e., utility agent j
uj (, j ) = qj (, dj )vj pj ().
Since agents self-interested, may misreport types strategic way.
easy see misreport shorter length dominated strategy; otherwise,
job cannot completed even scheduled mechanism (since lj < lj ). Therefore,
agents underreport lengths jobs. Similar work Porter
(2004), assume system return completed job agent j dj .3
way, restrict agents report dj dj . addition, assume agent
knowledge job release time, also rj rj .
Considering potential misreport agents, concerned incentive
compatible individually rational mechanisms. mechanism incentive compatible
(IC) if, agent j, regardless behaviors agents, truthful reporting
type maximizes utility. mechanism individually rational (IR)
job j, truthful reporting leads non-negative utility. addition, would also like
mechanism (approximately) maximize social welfare. say mechanism (strictly)
c-competitive exist job sequence c W (M, ) < W (opt, ),
opt denotes optimal offline mechanism4 . Sometimes also say
competitive ratio c.
1.2 Related Work
online scheduling problem studied non-strategic setting (Lipton &
Tomkins, 1994; Borodin & El-Yaniv, 1998; Bar-Noy, Guha, Naor, & Schieber, 2001; Zheng
et al., 2006; Kolen, Lenstra, Papadimitriou, & Spieksma, 2007; Ting, 2008; Nguyen, 2011)
(whose focus algorithm design) strategic setting (Nisan & Ronen, 2001; Lavi &
Nisan, 2004; Friedman & Parkes, 2003; Porter, 2004; Hajiaghayi, Kleinberg, Mahdian, &
Parkes, 2005; Parkes, 2007) (whose focus mechanism design).
Non-strategic setting. case = 1, lower bound 4 competitive
ratio deterministic algorithm given Woeginger (1994). 4.56-competitive
deterministic algorithm constructed Zheng et al. (2006) restart model,
4.24-competitive deterministic algorithm designed Kim (2011) restart
resume models. 2-competitive randomized algorithm introduced restart model
work Fung et al. (2014), lower bound 1.693 provided work Epstein
Levin (2010). restricting release time deadlines integers, randomized
e
algorithm competitive ratio e1
1.582 proposed Chin et al. (2006),

deterministic algorithm competitive ratio 2 2 1 1.828 proposed Englert et
3. Actually, viewed decision mechanism designer rather assumption.
decision crucial ensure incentive compatibility see later.
4. Since care social welfare performance opt competitive analysis,
depends schedule, regardless payments, also call opt optimal offline allocation,
simply optimal allocation.

431

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

al. (2012). best lower bounds currently 1.25 randomized algorithms (Chin &
Fung, 2003) 1.618 deterministic algorithms (Hajek, 2001).

general values , lower bound competitive ratio deterministic
algorithm derived work Chan et al. (2004). lower bound improved

6
5/6 )
2 ln 1 Ting Fung (2008), algorithm competitive ratio log + O(
given restart model. scheduling problem discrete time considered
work Durr, Jez Nguyen (2012). particular, lower bound improved


model.
ln o(1), (3 + o(1)) ln -competitive algorithm designed resume q
randomized algorithm competitive ratio O(log()) lower bound ( logloglog )
provided Canetti Irani (1998).
Assuming maximum ratio value densities (value divided length)

two jobs bounded known number , (1 + )2 -competitive algorithm

given Koren Shasha (1995). bound (1 + )2 optimal matching lower
bound given Baruah et al. (1992).
also rich literature concerned non-preemptive scheduling (Lipton &
Tomkins, 1994; Goldman, Parwatikar, & Suri, 2000; Goldwasser, 2003; Ding & Zhang,
2006; Ding, Ebenlendr, Sgall, & Zhang, 2007; Ebenlendr & Sgall, 2009). However,
easily verified algorithm bounded competitive ratio cannot designed
setting unrestricted values arbitrary release time. Therefore, common
assumption added non-preemptive scheduling problem proportional values, i.e.,
value job proportional length. work Goldman et al. (2000),
tight upper lower bound 2 given deterministic competitiveness
jobs equal length (thus, equal value), 6(blog2 c + 1)-competitive randomized
algorithm provided general value , matching (log ) lower bound (Lipton &
Tomkins, 1994) within constant factor.
Strategic setting. work Lavi Nisan (2015), assuming integer time
points, scheduling problem = 1 case studied. authors show
incentive compatible mechanism obtain constant competitive ratio,
payment must made job completed. Hence, propose family
semi-myopic algorithms competitive ratio 3, assumption semi-myopic
strategies. work Hajiaghayi et al. (2005), specific scheduling problem
= 1 considered restart model. deterministic IC mechanism competitive
ratio 5 designed, lower bound 2 given deterministic IC mechanism.
However, knowledge, case > 1 either restart model resume model
studied perspective mechanism design (considering incentive
issues). work fills gap.
Assuming maximum ratio value densities (value divided length)
two jobs bounded known number , IC mechanism competitive


ratio (1 + )2 + 1 designed Porter (2004), proved (1 + )2 + 1
lower bound competitive ratio deterministic mechanism.
Recently, online scheduling mechanisms investigated cloud computing (Zaman & Grosu, 2012; Azar, Ben-Aroya, Devanur, & Jain, 2013; Zhang, Li, Jiang, Liu,
Vasilakos, & Liu, 2013; Lucier, Menache, Naor, & Yaniv, 2013; Mashayekhy, Nejad, Grosu,
& Vasilakos, 2014; Wu, Gu, Li, Tao, Chen, & Ma, 2014). works, mechanisms
432

fiEfficient Mechanism Design Online Scheduling

designed allocate computational resources users, users use virtual machines entire period requested. model, jobs non-preemptive,
differs setting.
1.3 Results
main results summarized follows.
First, order characterize boundary research, derive two lower bounds
competitive ratio online deterministic IC mechanism. One bound 5,
holds situation jobs equal length (i.e., = 1). bound
improves previous lower bound 2 (Hajiaghayi et al., 2005). bound

ln + 1 o(1), characterizes asymptotical property competitive ratio
variance job lengths, i.e., , sufficiently large.
Second, design simple mechanism 1 prove restart resume
models 1 IC, also achieves good social welfare.
restart model, 1 competitive ratio + 2 + (1 + 1 ) small (in
1

particular, ratio 5 = 1), ( (1)
2 + o(1)) ln large ( 16
enough), 0 < < 1 small constant.
resume model, 1 competitive ratio ( + 1)(1 + 1 ) + 1
2

small (in particular, ratio 5 = 1), ( (1)
2 + o(1)) ln large
( 16 enough), slightly worse restart model (within
factor 2).
also worth mentioning that:
Comparing lower bounds, see that, restart resume
models, 1 optimal equal-length jobs ( = 1), near optimal (within
constant factor) unequal-length jobs.
comparison best-known algorithms without considering incentive compat5
6
6
ibility, asymptotically speaking, 1 improves best-known ratio log
+O( ) (Ting,
1

2008) restart model ( (1)
2 + o(1)) ln ; improves best-known ratio
2

(3+o(1)) ln (Durr, Jez, & Nguyen, 2012) resume model ( (1)
2 +o(1)) ln .
Thus even one care strategic aspect, 1 would still
nice algorithm use.

Note designing mechanisms online scheduling problems generally difficult
since combines challenges mechanism design (i.e., ensuring incentive compatibility)
challenges online algorithms (i.e., dealing uncertainty future inputs).
would like highlight main techniques used work tackle challenges.
(1) allocation rule mechanism 1 uses carefully selected function trade-off
three key elements: value, length, degree completion. trade-off function
delicate sense ensures efficiency monotonicity
crucial incentive compatibility.
433

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

(2) order obtain good competitive ratios resume model, design two nontrivial virtual charging schemes bound performance proposed mechanism:
integral charging scheme segmental charging scheme.
focus single machine model paper, work extends multiple
identical machines. One way extension similar work Lucier et al. (2013),
assumed h machines allocated job given
time, parameter h stands common parallelism bound system.
details extension found Appendix E. Another way extend results
multiple identical machines assume job j needs fixed number machines
processed. Please refer working paper (Ma, Zheng, Qin, Tang, & Liu,
2014) details.5

2. Lower Bounds
section, present two lower bounds competitive ratio deterministic
IC mechanism, hold restart resume models.
competitive analysis interpreted game designer online
mechanism adversary. Given mechanism 1 , adversary selects sequence
jobs maximizes competitive ratio, ratio social welfare obtained
offline optimal algorithm social welfare obtained 1 . Therefore, key
proving lower bounds construct subtle adversary behaviors.
first introduce two notions, dominant job shadow job.
Definition 2.1 (Dominant Job). deterministic IC mechanism competitive
ratio c, job called dominant job release time ri , vi larger
c times total value jobs whose release time later ri .
easy see that, order obtain reasonable competitive ratio, dominant job
tight deadline, mechanism must schedule release time ri . Otherwise,
consider case jobs released ri . case, mechanism
cannot obtain competitive ratio c gives dominant job i.
Definition 2.2 (Shadow Job). Suppose job tight deadline, i.e., di = ri + li ,
job i0 called shadow job i, i0 parameters (ri , li , vi ) i, except
later deadline (d0i > di ).
Clearly, shadow job i0 flexible completed later. shadow
jobs, show following lemma holds IC mechanism non-trivial
competitive ratio.
Lemma 2.3 (Shadow Job Argument). deterministic IC mechanism
non-trivial competitive ratio c, completes job (with tight deadline di )
scenario I, scenario 0 , substitutes shadow job i0 job i, must
also complete job i0 time di .
5. working paper, consider restart model, ignore misreport release time
deadline.

434

fiEfficient Mechanism Design Online Scheduling

Proof. Suppose completed job i0 di scenario 0 , could consider subsidiary scenario 00 , includes jobs scenario 0 adds several dominant jobs.
Remember call job dominant value sufficiently large (see Definition 2.1).
dominant jobs released one one di , di + 1, . . . , di + bd0i di c respectively,
denoted 0, 1, . . . , bd0i di c accordingly, di deadline job d0i
deadline shadow job i0 . Whats more, dominant jobs unit length
tight deadline. claim that, achieve desired (non-trivial) competitive ratio,
must complete dominant jobs, thus time interval [di , d0i ) occupied. (The
reason follows: schedule dominant job j {0, 1, . . . , bd0i di c},
consider scenario 000 , includes jobs release time later di + j
00 . Since scenario 000 indistinguishable 00 time di + j, know
schedule dominant job j scenario 000 , hence cannot obtain competitive ratio c.)
subsidiary scenario 00 indistinguishable scenario 0 time di ,
job i0 completed di . Furthermore, existence dominant jobs,
job i0 completed finally. However, job i0 falsely declares type
job i, i.e., misreports deadline di , would completed time
di better off, contradicting incentive compatibility6 .
following, derive lower bounds leveraging Lemma 2.3. First, following
theorem specifies lower bound jobs equal length (i.e., = 1). Note
result concerns strategic setting, Woeginger (1994) shows competitive
ratio deterministic algorithm non-strategic setting least 4.
Theorem 2.4. = 1, deterministic IC mechanism obtain competitive ratio
less 5.
prove theorem, addition using adversary argument similar
work Woeginger (1994), need perturb job sequence leverage
shadow job argument.
Intuitively, construct special job set, tight-deadline jobs released one
one, two jobs collide (that is, deadline one job later
release time other, mechanism, impossible two jobs
completed). values jobs carefully selected later released
job valuable earlier one (predecessor), value difference
two neighboring jobs constrained small-enough additive constant. Furthermore,
job set, values first last jobs set obey specific amplification.
Along execution mechanism, adversary would release series job
sets. mechanism completes one job, adversary stops releasing job.
subtleness lies choosing time release job sets: mechanism almost
completes job job set, adversary may release new job set whose jobs
collide job collide predecessor job a. way,
mechanism would abandon current job complete it,
optimal allocation completes: (1) several jobs previous job sets, (2)
6. scenario contradicts monotonicity condition (see strict definition start Section 3.2);
Theorem 1.15 work Parkes (2007) shows monotonicity necessary incentive
compatibility.

435

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

valuable job (i.e., last job) newly released job set, (3) job a.7 However,
mechanism complete job a. discrepancy leads lower bound
competitive ratio. detailed proof found below.




SET
r1




vq = w




v2
v1 = v



Figure 1: Structure SET (v, w, t, )
Proof. Suppose contradiction exists deterministic IC mechanism
achieves competitive ratio 5 0 < < 1. adopt notation SET
introduced Woeginger (1994). Define SET (v, w, t, ) (for w v > 0, > 0 > 0)
set jobs {1, 2, . . . , q} satisfying following properties:
(1) v1 = v, vq = w, vj < vj+1 vj + 1 j q 1. Hence, q integer
less wv
e. call magnifying parameter SET .
(2) lj = dj rj = 1, j, i.e., jobs unit-length tight deadlines.
(3) 0 r1 < < rq < < d1 < < dq , thus, two jobs collide other.
call split point SET .
define release time SET release time first job. Figure 1 shows
visual structure SET (v, w, t, ). adversary behavior follows.
Adversary Behavior: adversary release SET one another depending
. First, SET0 = SET (1, , 1/2, ) released time 0, = 4 /2 < /4.
definition SET , know first job SET0 value 1, last job
SET0 value , value difference two neighboring jobs upper
bounded .
Next, specify: (1) adversary release new SETi (i 1), (2)
adversary sets parameters SETi (i 1). (1), specify Algorithm 1.
notations used Algorithm 1 detailed Table 1.
7. proof, construct new scenario, job perturbed later deadline, thus
completed later. make use shadow job argument analysis, makes lower
bound increased 1, compared previous lower bound non-strategic setting.

436

fiEfficient Mechanism Design Online Scheduling

SETi
job ij
rij , dij vij
wi
ti

job
job

Table 1: Summary notation proof Theorem 2.4
i-th released SET , full, SET (vi1 , wi , ti , )
j-th job SETi .
release time, deadline value job ij.
value last job SETi
split point SETi
magnifying parameter SETi
trigger job SETi1
preceding job SETi1

Algorithm 1: Adversary Behavior
1: Initial: Release SET0 time 0.
2: completed job,
3:
almost completes j-th job (j 2) SETi (Precisely, executing
job ij di(j1) rij period time since rij ).
4:
Release SETi+1 time di(j1) .
5:
else
6:
release job.
7:
end
8: end
worth mentioning that: (i) SETi+1 triggered non-first job SETi
almost completed, call job trigger job. (ii) SET released
job completed .
Suppose trigger jobs SET0 , . . . , SETi1 named 1 , . . . , successively. Accordingly, denote job release time earlier trigger job 1, . . . , i,
call preceding jobs. Line 4 Algorithm 1, know new SETi
released deadline i. Note trigger job preceding job
located SETi1 .
specify parameters SETi = SET (vi1 , wi , ti , ), 1. Remember
SET0 defined SET (1, , 1/2, ), = 4 /2 < /4.
adversary sets vi1 equal value trigger job SETi1 ,
vi = vi1 . Note vi1 value first job SETi .
Pi1
adversary sets = /2i , wi = max{( 1)vi1 j=1
vj1 , vi1 } 2,
w1 = ( 1)v11 .
adversary sets ti = (di + di )/2, di di deadlines trigger job
preceding job i. Note setting ti = (di + di )/2, jobs SETi
released di di . Hence, new jobs collide trigger job
none collides job i.
Figure 2 illustrates adversary releases new SET example.
example, almost completes j-th job (j 2) SETi . SETi+1 released
deadline job i(j 1), value first job SETi+1 equal vij .
437

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng





ri1




SETi




vi1

SETi+1


vij
vi(j1)



v(i+1)1 = vij

ti+1
Figure 2: example SETi+1 SETi

According Algorithm 1, always gives trigger jobs switches schedule
job newly released SET , adversary release new SET one another.
One may wonder whether adversary release new SET infinitely. words,
subscript SETi tend infinity?
answer no, seen definition wi . Since 2 < < 4,
Lemma 4.3 work Woeginger (1994), finite numberP(denote k) steps, vk1
must less corresponding sum term ( 1)vk1 k1
j=1 vj1 , wk = vk1
must hold. Remember vk1 wk denote value first job last job SETk
respectively, thus exists one job SETk . According Algorithm 1, matter
whether completes job not, adversary release job. Therefore,
SETk ultimate SET job k1 ultimate job.
far, clarified adversarys behaviors. Next, show derive
lower bound based adversary.
According Algorithm 1 structure SET , know adversary allows
complete one job. Actually, completed job be: (1) first job SET0
(i.e., job 01); (2) trigger job , 1 k; first job SETi , 1 < k (i.e., job
i1); (3) ultimate job k1. Let us analyze one one.
(1) completes job 01, consider scenario job 01 substituted
shadow job 010 , whose deadline late enough (i.e., even started executed
deadline last job SET0 , still completed time). According
Lemma 2.3, mechanism must complete job 010 time 1, thus abandon
last job (with value w0 = ) SET0 . Therefore, obtains social welfare
v01 = 1. However, optimal allocation (which first completes last job SET0
job 010 ) obtains social welfare + 1. contradicts fact
competitive ratio 5 , since + 1 = (4 /2) + 1 > 5 .
(2) completes trigger job job i1, 1 k, without loss generality,
denote job job ij, know vij = vi = vi1 . completes job ij,
1 k, similarly, consider scenario job ij substituted
shadow job (ij)0 , whose deadline late enough. Lemma 2.3, must complete
job (ij)0 time dij , obtaining social welfare vij = vi = vi1 . However, social
welfare optimal allocation
P(which completes jobs
P1, . . . , i, last job SETi ,
job (ij)0 ) least ij=1 vj + wi + vij > ij=1 (vj1 j1 ) + wi + vij >
438

fiEfficient Mechanism Design Online Scheduling

P
2 + ( 1)vi1 i1
j=1 vj1 + vij > ( + 1)vi1 /2 > (5 )vi1 .
contradicts fact competitive ratio 5 .

Pi

j=1 vj1

(3) completes ultimate job k1, consider scenario adversary
releases two copies job k1 SETk . Clearly, scenario, choose one
copy complete. denote completed copy job (k1)1 job
(k1)2 . consider scenario job (k1)1 substituted shadow job
(k1)0 , whose deadline unit-time later job k1. According Lemma 2.3,
must complete job (k1)0 dk1 obtains social welfare vk1 . However,
0
optimal allocation (which completes
Pk jobs 1, . . . , k, job (k1)
Pk 2 , job (k1) )
obtain social welfare least j=1 vj + vk1 + vk1 > j=1 (vj1 j1 ) + wk + vk1 >
Pk
Pk1
j=1 vj1 + vk1 > ( + 1)vk1 /2 > (5 )vk1 . Remember
j=1 vj 2 + ( 1)vk1
P
SETk , vk1 = wk = ( 1)vk1 k1
j=1 vj1 . contradicts fact
competitive ratio 5 .

Second, understand asymptotic property lower bound large,
construct scenarios inspired example Durr et al. (2012) obtain following
theorem.
Theorem 2.5. sufficiently large, deterministic IC mechanism obtain
competitive ratio less ln + 1 o(1). particular, deterministic IC mechanism
obtain competitive ratio less ln + 0.94 16.
Proof. convenience analysis, denote =
Let us consider following adversary behaviors.


ln ,

r = de 1, assume 16.

Adversary Behavior: time 0, long job B type B = (0, , , ) released,
well two short jobs a1 a1 type (0, 1, 1, 1). Moreover, integer
moment 0 1, mechanism schedules job B [0, t), two short jobs
at+1 at+1 unit length released t, tight deadline + 1, new job
released otherwise. values jobs satisfy:
(
1
< ,
v(at ) = v(at ) =
(1)

1
e
.
Note job job type, cases analyzed at0
naturally applied at0 .
According adversary behavior, know adversary allows complete
one job. Actually, completed job be: (1) job at0 t0 < ; (2) job at0
t0 ; (3) job B. analyze three cases follows.
(1) mechanism schedules job at0 t0 < , consider scenario
includes jobs B, a1 , a1 , . . . , at0 1 , at0 1 , at0 job a0t0 . Here, job a0t0 type (t0
1, + 1, 1, 1), shadow job at0 . According Lemma 2.3, mechanism must
complete job a0t0 t0 obtains social welfare 1. However, scenario,
optimal mechanism complete job B first, schedule a0t0 time
complete it, optimal social welfare + 1. ratio + 1.
439

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

(2) mechanism schedules job at0 t0 , consider scenario
includes jobs B, a1 , a1 , . . . , at0 1 , at0 1 , at0 , job a0t0 . Here, job a0t0 type (t0
1, t0 + 1, 1, et0 /1 ), shadow job at0 . According Lemma 2.3, mechanism
schedule job a0t0 time t0 complete a0t0 . Thus, mechanism
obtains social welfare v(a0t0 ). However, one optimal mechanisms
schedule complete jobs = 1, . . . , t0 , schedule a0t0 time t0
complete it, resulting following optimal social welfare
t0
X

de 1 +

e


1


+e

t0
1


=r+

t0
X

e


1


+e

t0
1


Z

t0

r



t0

e 1 + e 1

r

t=r+1

t=de

t0

r+

t0

=r e 1 + ( + 1)e 1 = f (, r) + ( + 1)e 1 = f (, r) + ( + 1)v(a0t0 ).
r

Here, introduced function f defined f (, r) r e 1 . Considering
= ln r = 1, r (0, 1]. ex 1 + x sides converge
1 x approaches 0,
r

f (, r) = r e 1 r

r
= 0,


(2)

f (, r) approaches 0 grows. ratio + 1 o(1).
(3) mechanism schedules completes job B, obtaining social welfare ,
consider scenario includes jobs a1 , a1 , . . . , , job B 0 . Here, job
B 0 type B 0 = (0, 2, , ), shadow job B. Similarly, claim
IC mechanism schedule job B 0 time 0 complete time . Thus,
mechanism obtains social welfare v(B 0 ). However, one optimal
mechanisms schedule complete small jobs = 0 1,
schedule complete job B 0 . leads social welfare least
Z


X
X



1
1
de 1 +
e + = r +
e 1 +
e + r +
r

t=r+1

t=de
r
1



1



1


+ e
+ = f (, r) + e
+ = f (, r) + eln 1 +

ln
ln
=f (, r) + + = f (, r) + 2
+ = f (, r) + (
+ 1)v(B 0 ).
e
e
e
=r e

(3)

16, e ln . equation larger f (, r) +
( + 1)v(B 0 ). Therefore ratio + 1 o(1).
Combining three cases together, prove nonexistence ( ln + 1 o(1))competitive mechanisms. Since f (, r) 0.06 16, competitive ratio
least ln + 0.94 16.

3. Mechanism Design
section, describe simple mechanism 1 (whose allocation payment rules
given Algorithm 2), works surprisingly well restart resume
440

fiEfficient Mechanism Design Online Scheduling

models, handles settings different values unified framework.
contrast, previous works (Durr et al., 2012) need design separate different
algorithms deal different values .
3.1 Mechanism 1
introducing mechanism, first introduce concept valid active time
uncompleted job j, time t, denoted

(
min{s|x(t0 ) = j, t0 [s, t)}, restart model
ej (t) := R
resume model
0 (x(s) = j)ds,

(4)

x(t) mechanisms allocation function, maps time point available job, 0 machine idle.8 () indicator function returns 1
argument true, zero otherwise. Note ej () also take vector
argument. example, ej (, t) shorthand ej (t) job sequence .
seen restart model, time t, job j received allocation
time t0 < preempted that, ej (t) = t0 . resume
model, ej (t) accumulated processing time job j time t.
say job j feasible time (1) reported release time t; (2)
completed yet; (3) enough time completed reported
deadline, i.e., dj lj ej (t). use JF (t) denote set feasible jobs time
t.
According Algorithm 2, time t, 1 assigns priority score, vj lj ej (,t) ,
feasible job j JF (t), always processes feasible job highest
priority (ties broken favor job smaller rj ). located (0, 1)
determined later competitive analysis. payment rule 1
essentially critical-value payment (Parkes, 2007), similar secondprice auction. Hence, payment equal minimum bid agents make
remain allocated.9 following pseudocode, j denotes reported types jobs
j.

8. Equation 4, since s=t valid candidate minimization, exist s, s.t.,
x(t0 ) = j, t0 [s, t) restart model, ej (t) = 0.
9. Note use critical-value payment, payment completed job j depends
jobs types rj dj . mechanism allows returning completed job reported
deadline, calculation critical-value payment face trouble: possible agent j misreports
much later deadline obtain cheaper payment, job completed returned
true deadline. reason restrict mechanism return completed job reported
deadline. worth mentioning that, payment must made job completed, (Lavi
& Nisan, 2015) shown incentive compatible mechanism obtain constant
competitive ratio.

441

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

Algorithm 2:
Allocation Rule
time
JF (t) 6=
x(t) arg maxjJF (t) (vj lj ej (,t) )
else x(t) 0
end
Payment Rule
job j
qj (, dj ) = 1
pj () = min(v 0 |qj (((rj , dj , lj , v 0 ), j ), dj ) = 1)
j

j

else pj () = 0
end
intuition mechanism two-fold. First, ensure efficiency, one must trade
value length: job larger value higher priority, job larger
remaining length lower priority. 1 uses simple priority function achieve
tradeoff: seen, priority score vj lj ej (,t) job positively correlated
value negatively correlated remaining length. Second, ensure IC,
1 uses critical-value payment rule monotone10 allocation rule.
Note allocation rule payment rule implemented efficiently.
allocation rule, enough consider time point new jobs arrive
existing jobs completed. And, give algorithms Appendix show
payment agent computed polynomial time.
Clearly, critical-value payment rule, 1 individually rational.
following subsection, prove incentive compatibility.
3.2 Incentive Compatibility
call allocation rule mechanism monotone, job truthfully reported type
j = (rj , dj , lj , vj ) cannot completed mechanism, dominated11 declaration
type j = (rj , dj , lj , vj ) cannot make completed either.
According Theorem 1.13 work Parkes (2007), order establish
truthfulness mechanism, enough prove monotonicity allocation rule.
Theorem 3.1. Mechanism 1 incentive compatible, restart model resume
model.
Proof. prove monotonicity allocation rule 1 . Assume job j
completed 1 j truthfully declared (we denote case rue).
show j cannot completed either declaring j = (rj , dj , lj , vj ), rj rj ,
lj lj , dj dj vj vj . denote case F alse.
Suppose job j ever executed k > 0 times rue case, define
following points execution job j: let tsi tpi ith time job j starts
10. strict definition monotonicity start Section 3.2.
11. say type j dominated type j (denoted j j ) rj rj , dj dj ,
lj lj vj vj .

442

fiEfficient Mechanism Design Online Scheduling

execution preempted respectively, = 1, 2, . . . , k, let ta = arg inf (ej (t) +
dj < lj ) time job j abandoned. job j never started, set
ts1 = tp1 = ta .
also refer P = [rj , ts1 ) [tp1 , ts2 ) . . . [tpk , ta ] = P0 P1 . . . Pk pending period
job j, = [ts1 , tp1 ) [ts2 , tp2 ) . . . [tsk , tpk ) = A1 A2 . . . Ak executing period
job j.
first consider monotonicity regard rj , regardless variables. Clearly,
definition ta , declaring rj > ta could cause job completed. Thus,
restrict attention rj [rj , ta ] = P A.
necessary condition job j completed (in F alse) job j
executed sometime period P . However, according Lemma 3.2 (see below), job j
cannot executed P . Therefore, declaring rj rj cannot cause job completed.
Intuitively, Lemma 3.2 says that, case rue F alse, set jobs
scheduled period P must same. Thus, job j cannot executed period P .
consider dj , lj vj . proof essentially proof rj :
declaring dj dj , lj lj vj vj improve job js priority, result,
cannot change execution jobs pending period P . declaring dj dj ,
lj lj vj vj cannot cause job completed. proves allocation
rule 1 monotone.
following, formally introduce Lemma 3.2, used theorem.
lemma introduce additional notation: case rue F alse, denote
J J respectively set jobs ever executed P , denote
respectively set jobs ever pending A.
Lemma 3.2. (1) J = , (2) J = , (3) J = J.
Proof. Consider job I, according defintion I, case rue, job
lower priority job j period P .
Relation (1) means that, case rue, job cannot executed period P .
obvious, since job j (with higher priority i) pending period P .
Relation (2) means that, case F alse, job cannot executed period P either.
prove contradiction. Suppose job executed time point P .
denote ti = min{t P |x(t) = i}, assume ti Pn 0 n k. observation
pending period Pn , 0 n k.
h(n)

Observation 3.3. pending period Pn , 1 schedules jobs sequence12 jn1 . . . jn
(h(n) 1, number active jobs Pn ) case rue, know (1)
h(n)
release time job jn2 . . . jn period Pn ; particular, release time job
h(n)
jn1 Pn (n 1) exactly time tpn (2) job jn1 . . . jn either completed abandoned
Pn ; idle time Pn .
Here, use fj (t) denote priority job j time t. Suppose that, case
h(n)
rue, job jni (one jn1 . . . jn ) executed ti , priority fjni (ti ).
12. job may appear sequence preempted resumed/restarted later.

443

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

case F alse, since executed ti , according Observation 3.3, deduce
priority job time ti , i.e., fi (ti ) must larger fjni (ti ).
Therefore, deduce must executed sometime period Ui =
(A1 . . .An ). Otherwise, also executed time ti case rue, contradicting
fact I. Similarly, denote si = min{t Ui |x(t) = i}, assume si
1 n.
claim, case F alse, priority job time si , i.e., fi (si ) satisfies
inequality below.
(
p
fjni (ti ) |An |+|An1 |++|Am+1 |+|tm si | ,
fi (si ) >
p
fjni (ti ) |tm si | ,

n 1;
= n.

Otherwise, priority job time ti fjni (ti ) (consider case
periods [si , tpm ), Am+1 , . . . , An1 , allocated i).
According definition si , know si first time executed period
A. Therefore, priority job si remains shifting case rue
case F alse. However, case rue, job j executed time si (hence,
priority larger fi (si )), periods [si , tpm ), Am+1 , . . . , An1 , allocated
j. Therefore, time ti , job j priority larger fjni (ti ), contradicting fact
jni executed time ti .
Relation (3) means that, matter case rue case F alse, jobs executed
period P same. Relation (3) derived naturally Relation (2).

4. Competitive Analysis
section, show mechanism 1 performs quite well terms social welfare
comparison optimal offline allocation, full knowledge future
jobs beginning execution.
perform competitive analysis, need design virtual charging schemes.
certain virtual charging scheme, every job j completed optimal allocation opt,
charge value (or partial value) job f completed 1 . virtual charging
scheme satisfies property every job f completed 1 receives total charge
cvf , succeed showing 1 competitive ratio c.
Designing ingenious virtual charging scheme crucial competitive analysis.
following, design different virtual charging schemes obtain competitive
ratio 1 restart model resume model respectively.
use parameter priority function mechanism 1 , first derive
competitive ratios functions . specify later (in Section 4.3) choose
suitable (with respect ) optimize performance 1 , derive competitive
ratios terms .
Here, introduce notation used Section 4.1 Section 4.2.
Denote (1, 2, . . . , F ) sequence jobs completed 1 time. job f
sequence, let tf time job f completed, convenience denote t0 = 0.
Divide time F + 1 intervals = [tf 1 , tf ), f = 1, 2, . . . , F , [tF , +).
444

fiEfficient Mechanism Design Online Scheduling

4.1 Analysis Restart Model
study restart model first. assume, without loss generality, optimal
allocation opt interrupt allocation, since interrupted jobs non-resumable.
following theorem.
Theorem 4.1. restart model, 1 competitive ratio

1
1

+

1


+ 1.

Proof. introduce virtual charging scheme follows. completed job j
opt, also completed mechanism 1 , value charged itself.
Otherwise (i.e., job j completed 1 ), consider time sj j begins
execution opt. Note opt interrupt allocation, j exactly allocated
time period [sj , sj + lj ). sj must time interval (recall = [tf 1 , tf )),
charge value j f . Define j := tf sj time amount sj
tf . job j feasible time sj , according Lemma 4.2, know priority
jobs j time sj vf tf sj = vf j ; meanwhile, priority j time
sj vj lj . vj lj vf j , i,e., vj vf j lj . defer formal statement
proof Lemma 4.2 end subsection.
calculate maximum total value charged completed job f 1 .
time interval , denote (1, 2, . . . , m), sequence jobs opt whose starting time sj
belongs ordered s1 > s2 > > sm . Remember define j := tf sj
time amount sj tf . clear 0 < 1 < 2 < <
j lj j1 2 j m, since j allocated completed time interval
[sj , sj1 ]. Furthermore, job lengths normalized, i.e., 1 lj , deduce
that:
(
0
j = 1
j
(5)
j 1 j 2.
Recall < 1 P
f may also completed opt. Therefore total charge
job f vf +
j=1 vj , upper bounded
vf + vf


X
j=1



j lj

vf (1 +

l1

+


X



j1

) vf (1 +

l1

+

j=1

j=2

1
+
shows mechanism 1 ( 1

m1
X

1


j

) vf (1 +



+


X
j=0

+ 1)-competitive.

1
Actually, competitive ratio obtained way tight, i.e., ratio 1
+ 1 + 1
best possible 1 . give example Appendix B show tightness.

Lemma 4.2. time point sj , job j (6= f ) feasible time sj ,
priority j sj vf tf sj . Moreover, value j, vj , vf tf sj lj .
Proof. Note that, sj time interval , according definition , know
f unique job completed 1 . prove lemma
enumerating possible cases.
(1) executing job sj job f , know priority job f time
sj exactly vf tf sj (because priority job f time tf vf ). Clearly, priority
j sj larger job f , thus larger vf tf sj .
445

j ).

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

(2) executing job sj job f , assume 1 executes job j1 , . . . , jk
f successively13 time period [sj , tf ), k 1. Since f unique job
completed , deduce that: j1 preempted j2 , j2 preempted j3 ,...,jk
preempted f , finally f completed time tf . Denote 1 , . . . , k time points
j1 , . . . , jk preempted respectively. also denote fj (t) priority job
j time t. use backward induction: First, know priority job jk
k larger job f , i.e., fjk (k ) vf tf k . Then, since jk1 preempted
jk k1 , know priority jk1 k1 larger jk .
Hence, fjk1 (k1 ) fjk (k1 ) = fjk (k ) k k1 vf tf k1 . eventually,
get fj1 (1 ) vf tf t1 . Since j1 executed time sj , deduce
fj1 (sj ) vf tf sj . Clearly, priority j time sj (i.e., vj lj ) larger
j1 , thus larger vf tf sj .
arranging vj lj ej (sj ) vf tf sj , get vj vf tf sj lj +ej (sj ) vf tf sj lj ,
ej (sj ) 0 valid active time job j time sj .
remarks Lemma 4.2: (1) f unique job completed 1
time interval , priorities executing jobs monotonically increase . (2)
Lemma 4.2 applies restart model resume model. (3) Lemma 4.2 provides
useful tool relate priority feasible job (j) time point (sj )
completed job f .
4.2 Analysis Resume Model
Compared restart model, competitive analysis resume model much
complicated, resume model, job executed several disjointed
time intervals. charging scheme used previous subsection longer works,
need design new virtual charging scheme.
introducing new virtual charging scheme, introduce notation
used subsection. Let (j) denote number disjoint time segments
(j)
allocated completed job j opt, s1j , s2j , . . . , sj denote corresponding starting
time segment.
say allocation contains violation exist two completed jobs j,
two segments starting time sai , sci sbj , sdj sai < sbj < sci < sdj .
allocation called standard contain violation. means allocation
standard, completed job, starting time execution two segments
another jobs allocation, completion time also time interval (i.e.,
two segments). provide obvious yet useful fact offline
optimal allocation below.
Claim 4.3. exists optimal allocation standard.
detailed proof, please refer Appendix C. Without loss generality, assume
optimal allocation opt standard.
Claim 4.4 presents important property standard allocation, used
following proofs.
13. Here, j1 job j, affect analysis.

446

fiEfficient Mechanism Design Online Scheduling

Claim 4.4. execution opt, job js execution-starting time two
segments another jobs allocation, job js completion time also time
interval (i.e., two segments).
analyze competitive ratio 1 resume model, propose two new virtual
charging schemes (referred integral charging scheme segmental charging scheme,
respectively). integral charging scheme, charge whole value job j
optimal allocation opt job completed mechanism 1 ; segmental
charging scheme, charge value j segment, different segments
job may charged different jobs completed mechanism 1 . using two

schemes, Theorem 4.5 upper bound competitive ratio mechanism 1 1
+1
1
2
+ ln +1 respectively. discussed Section 4.3, two ratios work situations
different values, i.e., first one works well small second one works
well large .
Theorem 4.5. resume model, competitive ratio 1


1



+ 1.

particular, satisfies , competitive ratio 1 min{ 1 + 1,
2
ln + 1}.

1


+

proof theorem given Section 4.2.1 Section 4.2.2.
4.2.1 Integral Charging Scheme
Remember denote (1, 2, . . . , F ) sequence jobs completed 1 time.
job f sequence, denote tf time job f completed.
integral charging scheme, restrict total number jobs (excluding f itself)
charged job f : allow number exceed btf tf 1 c+1. particular,
introduce notation saturation Definition 4.6.
Definition 4.6 (Saturated). job f , number jobs (excluding f )
charged f less btf tf 1 c + 1, say f unsaturated; otherwise f
saturated.
Let W denote set jobs completed opt, Wf W denote set jobs
j W s1j . Let denote set jobs W whose values already
charged jobs completed 1 .
integral charging scheme described Scheme 1. simplicity, refer Line
1 2 Step 1, Line 4 11 Step 2, Line 12 21 Step 3.
give intuitive explanations Step 2 Step 3.
Step 2, job f (f = 1, . . . , F ), pick btf tf 1 c + 1 jobs Wf
charge values f . rule picking jobs follows largest s1j first, k-th
picked job14 s1j later tf k + 1.
14. slight abuse notations, still denote job j, thus start time first segment s1j .

447

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

SCHEME 1: Integral Charging Scheme
1: Initial: .
2: job W , also completed mechanism 1 , charge value itself,
add A.
3: W \ 6= ,
4:
f = 1 F ,
5:
k = 0 btf tf 1 c,
6:
J k := {j 0 | (s1j 0 tf k) (j 0 Wf \ A)};
7:
J k 6= ,
8:
Set j = arg maxj 0 J k (s1j 0 ), add j A, charge value f .
9:
end
10:
end
11:
end
12:
f = F 1,
13:
Wf \ 6= ,
14:
Set j = arg maxj 0 Wf \A (s1j 0 ), add j A;
15:
16:
17:
18:
19:
20:
21:
22:

(j)

sj +hj 0 hj F f ,
Charge js value unsaturated job smallest completion time
set {f + 1, . . . , f + hj };
(j)
else sj [tF , +),
Charge js value unsaturated job smallest completion time
set {f + 1, . . . , F };
end
end
end
end

Step 3, consider jobs (in W ) whose values charged job first
(j)
two steps. Consider job j s1j located interval sj
located +hj (or
[tF , +)). charge value unsaturated job job set {f + 1, . . . , f + hj } (or
{f + 1, . . . , F }). rule selecting unsaturated job follows smallest completion time
first.
show three steps jobs W charged completed jobs
1 (see Claim 4.9). First, give two observations below.
Observation 4.7. integral charging scheme, job f {1, 2, . . . , F }
time , number jobs charged f start time opt [t, tf )
(charged step 2) btf tc + 1.
Observation 4.8. integral charging scheme, job f {1, 2, . . . , F } completed
mechanism 1 , total number jobs charged f (excluding f )
btf tf 1 c + 1.
Observation 4.7 derived Lines 5-6 Scheme 1, Observation 4.8 derived
restriction saturated job charged more.

448

fiEfficient Mechanism Design Online Scheduling

Claim 4.9. integral charging scheme, jobs W charged jobs
completed mechanism 1 .
Proof. Suppose contrary exists Wf charged job
Rt
{f, f + 1, . . . , f + hi }. Here, introduce notation ei (t) = 0 (opt(s) = i)ds denote
valid active time resumable job time opt. Since length every job
least 1,15 exists allocation segment [s0 , s00 ] job ei (s0 ) < 1, ei (s00 ) 1,
opt(t) = [s0 , s00 ]. Suppose s00 belongs +h . definition hi ,
h hi .
According assumption, know: (a) charged f . (b) jobs
{f + 1, f + 2, . . . , f + h} saturated charging process charge
job i.
point (a), deduce Step 2, least btf s1i c + 1 jobs (whose
values charged f ) s1j (s1i , tf ] (by Observation 4.7). Otherwise would
charged f Step 2. denote Ja set btf s1i c + 1 jobs.
point (b), recall job f 0 (f 0 {f + 1, . . . , f + h}) saturated
bti ti1 c+ 1 jobs whose values charged f 0 (see Definition 4.6). Hence, deduce
least (btf +1 tf c + 1) + + (btf +h tf +h1 c + 1) jobs (whose values
charged {f + 1, . . . , f + h}) starting time satisfying s1j (s1i , tf +h ].
particular, among jobs, (btf +h s00 c + 1) jobs s1j (s00 , tf +h ]
(whose value must charged f + h).16 Therefore, deduce least
(btf +1 tf c + 1) + + (btf +h tf +h1 c + 1) (btf +h s00 c + 1)
jobs (whose values charged {f + 1, . . . , f + h}) s1j (s1i , s00 ] (denote Jb
set jobs).
Note Ja Jb = , jobs Ja charged f , jobs Jb charged
{f + 1, . . . , f + h}. Therefore, deduce number jobs start time contained
(s1i , s00 ] least |Ja | + |Jb |, i.e.,
(btf s1i c + 1) + (btf +1 tf c + 1) + + (btf +h tf +h1 c + 1) (btf +h s00 c + 1)
>(tf +h s1i ) (btf +h s00 c + 1) bs00 s1i c 1.
(6)
So, bs00 s1i c 1 jobs different [s1i , s00 ]. Recall assume
opt standard, hence, jobs entirely scheduled (s1i , s00 ), i.e., time segments
job allocated (s1i , s00 ) (Claim 4.4). Since length every job least
1, reach contradiction.
According integral charging scheme, charges completed job f three
origins, corresponding three steps Scheme 1. Step 1, obviously, charge
job f vf . calculate maximum total charge Step 2.
15. stated Problem Formulation section, assume job lengths located [1, ] simplicity.
However, scaling, results proofs easily generalized case [lmin , lmin ],
lmin shortest length jobs.
16. Because: (i) Step 2, might (btf +h s00 c + 1) jobs s1j (s00 , tf +h ] could
charged f + h; (ii) Step 3, jobs s1j +h could charged f + h.

449

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

Suppose total number jobs charged f Step 2 m. rename
1, 2, . . . , according 1 2 , claim vj vf j lj (Lemma 4.2 used
here), j := tf s1j , 1 j m. According rule picking jobs Step 2,
j j 1. clear sum values jobs
vf


X
j=1

j lj vf


X

j1 = vf

j=1


X

j .

(7)

j=0

remains calculate maximum total charge Step 3. According Observation
4.8, know number jobs charged f Step 3 btf tf 1 c+1m.
need bound value job j. key build relationship
value value job f . However, according charging rule Step
3, start time s1j job j located time interval . case, cannot
use Lemma 4.2 directly derive inequality like vj vf j lj . remains
check whether j feasible tf 1 (note tf 1 left endpoint time interval ).
define critical time job tj := dj lj . prove tj tf 1 ,
job j must feasible time tf 1 1 . Thus, applying Lemma 4.2, easily get
vj vf tf tf 1 lj vf tf tf 1 .

(8)

Fortunately, following lemma shows tj tf 1 holds.
Lemma 4.10. According charging scheme, j Wf charged completed job
f + k (where 1 k hj ), critical time job j satisfies tj tf +k1 .
Proof. prove lemma contradiction suppose tj < tf +k1 . total length
(j)

jobs whose opt allocation s1j sj


(j)

(sj

dj s1j lj = (dj lj ) s1j = tj s1j < tf +k1 s1j .

(j)

+ lj

) s1j lj ,

(9)

Since j charged f +k, Step 3 know jobs {f +1, f +2, . . . , f +k1}
saturated. Thus, least
(btf s1j c + 1) + (btf +1 tf c + 1) + + (btf +k1 tf +k2 c + 1) btf +k1 s1j c + 1 (10)
jobs whose start time belongs interval (s1j , tf +k1 ).
Recall opt standard. Hence, jobs allocated time segments
first segment last segment job j (according Claim 4.4), Equation (9)
Equation (10) constitute contradiction since every jobs length least 1.
Combining analysis above, know that: (1) total charge f Step 1
vf ; (2) assuming
jobs charged f Step 2, total charge
Pm j
jobs vf j=0
according Equation (7); (3) number jobs charged
f Step 3 btf tf 1 c + 1 according Definition 4.6, value
450

fiEfficient Mechanism Design Online Scheduling

job vf tf tf 1 according Equation (8). Therefore, total charge
f
vf + vf

m1
X

btf tf 1 c+1

j + (btf tf 1 c + 1 m)vf tf tf 1 vf (1 +

j=0

X

j ),

j=0



upper bounded vf (1 + 1
), indicating competitive ratio mechanism

1 upper bounded


1

+ 1.

4.2.2 Segmental Charging Scheme
(j)

Recall use s1j , s2j , . . . , sj

denote starting time time segments allocated

(j)
1j , 2j , . . . , j

(j)

job j opt. Let
denote time segments, lj1 , lj2 , . . . , lj denote
length them.
segmental charging scheme, segment kj given value j ljk ,
v
j := ljj value density job j. describe segmental charging scheme
Scheme 2. simplicity, refer Line 2 3 Type-1 charge, Line 4 5 Type-2
charge, Line 6 7 Type-3 charge.
SCHEME 2: Segmental Charging Scheme
1:
2:
3:
4:
5:
6:
7:
8:
9:

segment kj opt
mechanism 1 also completes j deadline,
Charge value j ljk j.
else skj f {1, 2, , . . . , F }, j vf j 1 , j := tf skj ,

Charge value j ljk f .
else
Charge j ljk f , f first job completed 1 time tj on,
tj critical time job j.
end
end

clear Type-1 charge received job f vf . Next, bound
Type-2 Type-3 charges.
v

Lemma 4.11. total Type-2 charge job f receives lnf .
Proof. Let R2 denote set job segments whose charges f Type-2.
kj R2 , charge j ljk . Line 4 Scheme 2, know j vf j 1 ,
j = tf skj . Thus total Type-2 charge
X
kj R2

j ljk

vf

X
kj R2

j 1 ljk

vf

X Z
kj R2

j

j ljk



x1

Z
dx vf
0



x1 dx

vf
,
ln

second inequity holds < 1. Therefore, f receives total Type-2 charge
v
lnf .
451

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

following, study Type-3 charge denote R3 set job segments
constitute Type-3 charges f .
First, claim that, satisfies condition, get [skj , skj + ljk ] [tf , tf +
lj ] [tf , tf + ] kj R3 (Claim 4.12).
Claim 4.12. satisfies function: g(x) = x x 1 x ;
[skj , skj + ljk ] [tf , tf + lj ], kj R3 .
Proof. prove [skj , skj + ljk ] [tf , tf + lj ], need prove inequality below:
tf skj tf + lj ljk .

(11)

inequality skj tf + lj ljk holds (skj + ljk ) lj dj lj = tj tf .
Next prove tf skj . Suppose skj 0 f 0 (If 0 later , might
equal ). according Type-3 charging rule, j =

vj
lj

0

> vf 0 j 1 ,

j0 = tf skj .
use condition : g(x) = x x 1 x .
0
v
v
l
lj j , hence ljj vj lj 1 . Combining two inequalities ( ljj > vf 0 j 1
0

vj
lj

0

vj lj 1 ), vj lj > vf 0 j , thus vj > vf 0 j lj , contradicts fact
f 0 completed tf 0 priority vf 0 (Lemma 4.2 used here). Therefore,
tf skj .
Claim 4.12, know allocation segments P
Type-3 charges f
restricted interval [tf , tf + ]. Hence, derive k R3 ljk .
j

Lemma 4.13. satisfies function: g(x) = x x 1 x ; total
).
Type-3 charge job f receives vf ( 1
ln +
Proof. According Type-3 charging rule, j completed mechanism;
consider critical point j, i.e., tj (in time interval ), applying Lemma 4.2,

v
v
deduce vj lj vf tf tj vf . Therefore ljj flj . bound
lj

total Type-3 charge f receives
X
kj R3

j ljk =

X vj
X vf
X ljk
k
ljk
l
=
v
,
f
lj
g(lj )
lj lj j
k
k
k

j R3

j R3

Note function g(lj ) = lj lj increasing 1 lj
1
lj ln
.
g(lj )

(



1 lj


1
ln

(12)

j R3

1
ln

lj , >

1
ln .

1
ln

decreasing

(13)

Claim 4.12, know [skj , skj + ljk ] [tf , tf + lj ] [tf , tf + ] kj R3 .
1

Therefore, one hand, kj R3 ln
lj (denote set R3 ),
452

fiEfficient Mechanism Design Online Scheduling

k
kj R3a lj

; hand, kj R3 lj
P
1
set R3b ), k Rb ljk ln
.
3
j
Then, (12) becomes



P

X
kj R3

X

j ljk vf

kj R3

X ljk
X ljk
ljk
= vf (
+
)
g(lj )
g(lj )
g(lj )
k

k
b
j R3

P
vf (

1
ln

kj R3a



ljk

P
+

kj R3b



j R3

ljk

) vf (

means Type-3 charge bounded vf ( 1
ln +

(denote

(14)

1
ln


+
),



1
).

Based Lemmas 4.11 4.13, obtain ,17 total charge
job f completed mechanism 1 vf ( 1 + 2
ln + 1). implies
1
competitive ratio mechanism 1 upper bounded + 2
ln + 1.
4.3 Discussions
advantage mechanism handle settings different values
unified framework. need set parameter different values Theorem
4.1 Theorem 4.5 adapt different settings job lengths (as shown
following corollaries).
Corollary 4.14. setting = 1(1)2 ln , > 0 arbitrary small constant,
1

mechanism 1 achieves competitive ratio ( (1)
2 + o(1)) ln restart model
2
competitive ratio ( (1)
2 + o(1))


ln

resume model.

proof found Appendix D. Corollary 4.14, following
discussions:
1

(1) restart model, mechanism 1 achieves competitive ratio ( (1)
2 +o(1)) ln ,
5

6
6
improves upon best-known algorithmic result log
+ O( ) (Ting, 2008)
standard online scheduling without strategic behavior.

(2) resume model, large, mechanism 1 achieves competitive ratio
2

( (1)
2 + o(1)) ln , slightly worse result obtained restart
model (within factor 2). Asymptotically speaking, 1 near optimal, since
competitive ratio order (w.r.t. ) lower bound shown
Theorem 2.5. Furthermore, analysis generalizes results obtained Durr et
al. (2012) continuous value time strategic setting.
(3) relatively small, ratios given Corollary 4.14 become loose.
particular, approaches 1, ratios approach infinity since ln
approaches 0. case, need different setting (see Corollary 4.15).
1
1
17. Note that, function g(x) = x x increasing 1 x ln
decreasing x ln
. Therefore,


need require , naturally derive g(x) = x x 1 x .

453

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng


Corollary 4.15. choosing = +1
, competitive ratio mechanism 1 + 2 +
1
(1 + ) < + 2 + e restart model ( + 1)(1 + 1 ) + 1 resume model.

Similarly, following discussions:
(1) competitive ratio 1 linear , since (1 + 1 ) bounded e.
(2) particular, = 1, ratios corollary become 5
restart resume model, matches lower bound given Theorem 2.4.
regard, say 1 optimal. hand, also shows
lower bound 5 Theorem 2.4 tight.

5. Conclusion Future Work
paper, studied online scheduling problem strategic setting. summarized Table 2, proved restart model resume model,
competitive ratio IC mechanism cannot less 5 = 1 cannot less
ln + 1 o(1) large . designed simple IC mechanism 1 schedule jobs
single machine proved near optimal approximation guarantees (in terms
social efficiency) restart model resume model competitive
analysis: shown Table 2, mechanism optimal terms competitive ratio
restart model resume model = 1, near optimal
restart model large enough.
Table 2: Summary bounds competitive ratio
Restart Model
Resume Model
Model
=1
asymptotic
=1
asymptotic


LB IC mech.
5
+
1

o(1)
5
ln
ln + 1 o(1)
1
2


UB proposed mech.
5
( (1)
5
( (1)
2 + o(1)) ln
2 + o(1)) ln
proving lower bounds, introduce shadow job argument reflects
IC constraint. argument helpful extending bounds non-strategic setting
strategic setting. second contribution work design several virtual
charging schemes analyze competitive ratio mechanism. ideas
virtual charging schemes methodological significance may used address
problems.
multiple directions explore future.
interesting problem whether IC competitive mechanism designed
hybrid model, exist resumable non-resumable jobs. Many new
strategic issues may arise hybrid model. example, resumable job disguise
non-resumable job get better off?
Another open problem whether tighter competitive analysis 1 made
resume model. conjecture competitive ratio obtained 1
1
+ 1 + 1, restart model resume model.
uniform form: 1
Furthermore, given popularity cloud computing todays industry,
practical importance extend work setting job scheduling multiple heterogeneous machines.
454

fiEfficient Mechanism Design Online Scheduling

Appendix A. Algorithms Critical-Value Payment
Please note critical time point Algorithm 3 Algorithm 4 means time
point new jobs arrive existing jobs completed.
Algorithm 3: Compute critical-value payment restart model
job j completed
Run Algorithm 1 without job j. Let set critical time points
[rj , dj ).
every
exists job k x(t) = k, define ft = vk lk ek (t) ;
else ft = 0;
end-for
every time point t0 [rj , dj lj )
0
Define ft0 = max{ft / : (T [t0 , t0 + lj ))};
end-for
Let f = mint0 ft0 .
pj = f / lj .
end
Algorithm 4: Compute critical-value payment resume model
job j completed
Run Algorithm 1 without job j.
Let {t0 , t1 , . . . , tm } set (denoted ) critical time points [rj , dj ),
t0 = rj .
Denote period two critical time point zi = ti ti1 ,
= 1, 2, . . . , m.
every ti
exists job k x(ti ) = k, define fti = vk lk ek (ti ) ;
else fti = 0;
end-for
Initially, , h = 0.
h < lj
t0 = arg minti \T fti , ties broken favor smaller ti ;
Initially, e0 = 0;
0
every time point ti t0 satisfies fti ft0 e
e0 = e0 + zi ;
ti
/ ,
add ti , h = h + zi ;
end-for
end-while
Let t01 earliest critical time point . Let = arg maxti fti .
Denote critical time points t01 t02 , t03 , . . . , t0k .
Denote relevant periods critical time points z10 , z20 , . . . , zk0 , z .
0
0
pj = ft / lj (z1 ++zk ) .
end
455

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

Appendix B. Example Analysis Tightness
Example B.1. two types jobs: long short. length long jobs ,
length short jobs 1. Let p large integer, number long short
jobs p + 1 p 1 respectively. first long job J0l released time 0, type
0l = (0, , , 1). p 2 1, job Jil type il = (i( ), (i + 1) i, , ).
l
l
Long job Jp1
type p1
= ((p 1)( ), (p + 2), , (p1) + ). Job Jpl type
l
p = (p(), (p+1)p, , p+ ). Here, small constants satisfying p 1
. meanwhile, short jobs follows. j = 1, . . . , p1, denote
Jjs jth short job, whose type js = (j (p1), j +1(p1), 1, (j+1)+(p1) ).
j = 1, . . . , p 1.
l
verified one job Jp1
completed mechanism 1 ,
(p1)
social welfare
. optimal solution, short jobs
l
l
completed successively, social welfare
completed, that, Jp Jp1
2
3
(p1)
(
+
+ ... +
) + (p1) + p . Therefore, competitive ratio
p1
mechanism 1 least ( p2 + . . . + + 1) + 1 + = 1
+ 1 + , tends
1
1
1
+ 1 + 1, p .

Appendix C. Proof Claim 4.3
Proof. Suppose optimal allocation opt standard, i.e., exist completed job
two segments beginning time sai sci completed job j two segments
beginning time sbj sdj sai < sbj < sci < sdj . following process
obtain standard optimal allocation: length job js b-th segment (denote ljb )
larger c-th segment (denote lic ), exchange c-th segment js
b-th segment located [sbj , sbj + lic ]; otherwise, exchange js b-th segment c-th
segment located [sci + lic ljb , scj + lic ]. segments, order remains
unchanged. easy see new allocation still feasible obtains
social welfare. kind exchanges violation, obtain
standard optimal allocation.

Appendix D. Proof Corollary 4.14
Proof. every constant c < 1 large enough x, (1 xc )x e.
large enough, choosing = 1 (1 )2 ln , 1,
(1 )2 ln (1) ln (1) ln

)
e(1) ln (1) = o(
).

ln
using Taylors theorem, know
= (1

ln = (1 + o(1))(1 ) = (1 + o(1))
Thus competitive ratio

1
1

c2 ln
.


1

+ 1 + 1 = ( (1)
2 + o(1)) ln restart model,

2
ln

2

+ 1 + 1 = ( c2 ln (2 + o(1)) + o( ln )) + 1 = ( (1)
2 + o(1)) ln resume model,
respectively.

456

fiEfficient Mechanism Design Online Scheduling

Appendix E. Multiple Machines Extension
Suppose C identical machines, process one job
given time. Similar work Lucier et al. (2013), assume h
machines allocated single job given time. parameter stands
common parallelism bound system.
notion preemption specified follow: job may processed number
machines 1 h, number machines allocated job may fluctuate,
number decreases 0, treat job preempted. Thus, notation
preemption-restart preemption-resume defined accordingly.
job j J characterized private type j = (rj , dj , sj , vj ). Instead lj ,
use sj denote jobs size (e.g., number machine hours required complete
job). Without causing confusion, let maximum ratio sizes
two jobs: = maxi,jJ ssji . simplicity, assume job sizes fall [1, ]. = 1,
jobs identical size; otherwise different sizes.
E.1 Simple Case: h = 1
case, design new mechanism 2 based single-machine mechanism 1 .
payment rule 2 exactly 1 , allocation rule shown
Algorithm 2, also similar 1 . Since job processed
one machine, mechanism choose C jobs (if any) JF (t) highest
priorities vi si ei (,t) execute. Note valid active time job j time
computed
C Z
X
ej (t) =
(xi (s) = j)ds.
(15)
i=1

t0

P
() indicator function, t0 = arg maxst [ C
i=1 (xi (s) = j)] = 0.
say, treating resumable jobs non-resumable jobs simple. summarize
theoretical properties 2 Theorem E.1.
Algorithm 5: allocation rule Mechanism 2

|JF (t)| C
process C jobs highest priorities JF (t);
else process jobs JF (t).
end
Theorem E.1. Mechanism 2 IC following properties:

(+1) ,
1 (1 )2

restart model, setting =
1
)

get competitive ratio + 2 +

(1 +
2 ; setting =
ln arbitrary small > 0, get
1
another competitive ratio ( (1)2 + o(1)) ln .
resume model, setting = 1 (1 )2 ln arbitrary small > 0,
2

get competitive ration ( (1)
2 + o(1)) ln .
theorem, following discussions.
457

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

(1) Similar done single-machine setting, restart model,
give two competitive ratios 2 . small, first ratio better
(in particular, = 1, competitive ratio becomes 5 thus optimal
according Theorem 2.4). large, second ratio better instead
near optimal according Theorem 2.5.
(2) Different obtained single-machine setting, resume
model, cannot match lower bound 5 = 1 multi-machine setting.
Proof. proof Theorem E.1 essentially proof single machine
setting, virtual charging scheme, charge completed job optimal allocation
job completed 2 exactly machine. difference
integral charging scheme resume model apply multiple machines
setting more. use segment charging scheme resume model.
E.2 General Case: h 1
handle general case, design new mechanism 3 , divides C machines
bC/hc equally-sized virtual machines (each consisting h machines), treats every
virtual machine single machine performing scheduling. is, virtual
machine used process one job, remaining C bC/hc h machines
idle.
Algorithm 6: allocation rule Mechanism 3
(1) Divide C machines bC/hc equal-sized virtual machines.
(2) Run mechanism 2 following modification:
Capacity: bC/hc.
Demand size: sj /h job j.
compared case h = 1, setting h 1 imposes flexibilities
optimal offline allocation. example, job may processed number machines
1 h optimal allocation might always executed exactly
h machines. Fortunately, use similar segmental charging idea h = 1 case
resolve challenge get competitive ratio shown following theorem.
4

Theorem E.2. Mechanism 3 IC competitive ratio ( (1)
2 + o(1)) ln

3 setting = 1 (1 )2
resume model.

ln


arbitrary small > 0, matter restart model

following discussions theorem. setting h 1
complicated could always obtain results setting h = 1.
particular, h divides C, idle machine may obtain
competitive ratio setting h = 1. However, h divide C, idle
machines introduce additional factor 2 competitive ratio. Besides,
competitive ratio restart model better resume model,
competitive ratio cannot reach 5 = 1.
458

fiEfficient Mechanism Design Online Scheduling

Proof. need show exists optimal allocation (we view
jobs resumable jobs optimal allocation) time every job
processed either exactly h machines machine (assuming h divides C).
directly use results obtained special case h = 1. Suppose opt optimal
offline allocation J set jobs completed opt. j J , use
mj (t) denote number machines processing j time opt.
divide time intervals [tk , tk+1 ), k = 0, 1, 2, , time interval
[tk , tk+1 ), mj (t) change j J . show allocate jobs
time interval [tk , tk+1 ). bC/hc virtual machines, allocate jobs
one one, i.e., previous virtual machine full, start allocate jobs
another empty virtual machine tk (empty respect [tk ,Rtk+1 )). Besides,

every job allocated continuously one one size allocation tkk+1 mj (t)dt.
easily verified allocation every job j J allocated legitimately
(j allocated [rj , dj ] processed h machines time)
complete deadline since j legitimately completed opt.

References
Azar, Y., Ben-Aroya, N., Devanur, N. R., & Jain, N. (2013). Cloud scheduling setup
cost. Proceedings twenty-fifth annual ACM symposium Parallelism
algorithms architectures, pp. 298304. ACM.
Bar-Noy, A., Guha, S., Naor, J., & Schieber, B. (2001). Approximating throughput
multiple machines real-time scheduling. SIAM Journal Computing, 31 (2),
331352.
Baruah, S., Koren, G., Mao, D., Mishra, B., Raghunathan, A., Rosier, L., Shasha, D., &
Wang, F. (1992). competitiveness on-line real-time task scheduling. RealTime Systems, 4 (2), 125144.
Baruah, S. K., Haritsa, J., & Sharma, N. (1994). On-line scheduling maximize task
completions. Proceedings Real-Time Systems Symposium, pp. 228236. IEEE.
Borodin, A., & El-Yaniv, R. (1998). Online computation competitive analysis, Vol. 2.
Cambridge University Press Cambridge.
Chin, F. Y., & Fung, S. P. (2003). Online scheduling partial job values: timesharing randomization help?. Algorithmica, 37 (3), 149164.
Ding, J., Ebenlendr, T., Sgall, J., & Zhang, G. (2007). Online scheduling equal-length
jobs parallel machines. Proceedings 15th annual European conference
Algorithms, pp. 427438. Springer-Verlag.
Ding, J., & Zhang, G. (2006). Online scheduling hard deadlines parallel machines.
Algorithmic Aspects Information Management, pp. 3242. Springer.
Durr, C., Jez, L., & Nguyen, K. T. (2012). Online scheduling bounded length jobs
maximize throughput. Journal Scheduling, 15 (5), 653664.
459

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

Ebenlendr, T., & Sgall, J. (2009). lower bound scheduling unit jobs immediate
decision parallel machines. Approximation Online Algorithms, pp. 4352.
Springer-Verlag.
Friedman, E. J., & Parkes, D. C. (2003). Pricing wifi starbucks: issues online mechanism
design. Proceedings 4th ACM conference Electronic commerce, pp. 240
241. ACM.
Goldman, S. A., Parwatikar, J., & Suri, S. (2000). Online scheduling hard deadlines.
Journal Algorithms, 34 (2), 370389.
Goldwasser, M. H. (2003). Patience virtue: effect slack competitiveness
admission control. Journal Scheduling, 6 (2), 183211.
Hajek, B. (2001). competitiveness on-line scheduling unit-length packets
hard deadlines slotted time. Proceedings 35th annual Conference
Information Sciences Systems.
Hajiaghayi, M., Kleinberg, R., Mahdian, M., & Parkes, D. C. (2005). Online auctions
re-usable goods. Proceedings 6th ACM conference Electronic commerce,
pp. 165174. ACM.
Kolen, A. W., Lenstra, J. K., Papadimitriou, C. H., & Spieksma, F. C. (2007). Interval
scheduling: survey. Naval Research Logistics (NRL), 54 (5), 530543.
Lavi, R., & Nisan, N. (2004). Competitive analysis incentive compatible on-line auctions.
Theoretical Computer Science, 310, 159180.
Lavi, R., & Nisan, N. (2015). Online ascending auctions gradually expiring items.
Journal Economic Theory, 156, 4576.
Lipton, R. J., & Tomkins, A. (1994). Online interval scheduling. Proceedings
Fifth Annual ACM-SIAM Symposium Discrete Algorithms, Vol. 94, pp. 302311.
Lucier, B., Menache, I., Naor, J. S., & Yaniv, J. (2013). Efficient online scheduling
deadline-sensitive jobs. Proceedings 25th ACM symposium Parallelism
algorithms architectures, pp. 305314. ACM.
Ma, W., Zheng, B., Qin, T., Tang, P., & Liu, T. (2014). Online mechanism design cloud
computing. CoRR, abs/1403.1896.
Mashayekhy, L., Nejad, M. M., Grosu, D., & Vasilakos, A. V. (2014). Incentive-compatible
online mechanisms resource provisioning allocation clouds. Cloud Computing (CLOUD), 2014 IEEE 7th International Conference on, pp. 312319. IEEE.
Nguyen, K. T. (2011). Improved online scheduling maximizing throughput equal length
jobs. Computer ScienceTheory Applications, pp. 429442. Springer.
Nisan, N. (2007). Introduction mechanism design (for computer scientists). Algorithmic
game theory, 209, 242.
Nisan, N., & Ronen, A. (2001). Algorithmic mechanism design. Games Economic
Behavior, 35, 166196.
Parkes, D. C. (2007). Online mechanisms. Algorithmic Game Theory, ed. N. Nisan, T.
Roughgarden, E. Tardos, V. Vazirani, Cambridge University Press, 411439.
460

fiEfficient Mechanism Design Online Scheduling

Porter, R. (2004). Mechanism design online real-time scheduling. Proceedings
5th ACM conference Electronic commerce, pp. 6170. ACM.
Ting, H.-F. (2008). near optimal scheduler on-demand data broadcasts. Theoretical
Computer Science, 401 (1), 7784.
Wu, X., Gu, Y., Li, G., Tao, J., Chen, J., & Ma, X. (2014). Online mechanism design
VMS allocation private cloud. Network Parallel Computing, pp. 234246.
Springer.
Zaman, S., & Grosu, D. (2012). online mechanism dynamic vm provisioning
allocation clouds. 5th International Conference Cloud Computing (CLOUD),
pp. 253260. IEEE.
Zhang, H., Li, B., Jiang, H., Liu, F., Vasilakos, A. V., & Liu, J. (2013). framework
truthful online auctions cloud computing heterogeneous user demands.
Proceedings INFOCOM, pp. 15101518. IEEE.
Zheng, F., Fung, S. P., Chan, W.-T., Chin, F. Y., Poon, C. K., & Wong, P. W. (2006). Improved on-line broadcast scheduling deadlines. Computing Combinatorics,
pp. 320329. Springer.

461

fiJournal Artificial Intelligence Research 56 (2016) 269327

Submitted 12/15; published 05/16

Combining Delete Relaxation Critical-Path Heuristics:
Direct Characterization
Maximilian Fickert
Jorg Hoffmann
Marcel Steinmetz

9 MAFICK @ STUD . UNI - SAARLAND . DE
HOFFMANN @ CS . UNI - SAARLAND . DE
STEINMETZ @ CS . UNI - SAARLAND . DE

Saarland University, Saarbrucken, Germany

Abstract
Recent work shown improve delete relaxation heuristics computing
relaxed plans, i. e., hFF heuristic, compiled planning task C represents
given set C fact conjunctions explicitly. compilation view partial
delete relaxation simple elegant, meaning respect original planning
task opaque, size C grows exponentially |C |. herein provide direct characterization, without compilation, making explicit approach arises
combination delete-relaxation critical-path heuristics. Designing equations
characterizing novel view h+ one hand, generalized version hC hm
hand, show h+ (C ) characterized terms combined
hC+ equation. naturally generalizes standard delete-relaxation framework: understanding framework relaxation singleton facts atomic subgoals, one
refine relaxation using conjunctions C atomic subgoals instead. Thanks
explicit view, identify precise source complexity hFF (C ), namely maximization sets supported atomic subgoals relaxed plan extraction,
easy singleton-fact subgoals NP-complete general case. Approximating
problem greedily, obtain polynomial-time hCFF version hFF (C ), superseding
C compilation achieves
C compilation, superseding modified ce
complexity reduction information loss. Experiments IPC benchmarks
show theoretical advantages translate empirical ones.

1. Introduction
delete relaxation classical planning (McDermott, 1999; Bonet & Geffner, 2001) originates work using STRIPS representation, state variables Boolean, action
effects conjunctions literals, action preconditions well goal restricted
conjunctions positive literals (facts). relaxation assumes negative
(delete) effect literals, hence name. generally, i. e., non-Boolean state variables, amounts assuming state variables accumulate values, rather
switching them. optimal delete-relaxed planning still NP-hard, satisficing delete-relaxed planning polynomial-time (Bylander, 1994). Relaxed plan heuristics,
employing satisficing delete-relaxed planning generation inadmissible heuristic
functions, proved highly successful (e. g. Hoffmann & Nebel, 2001; Gerevini, Saetti,
& Serina, 2003; Richter & Westphal, 2010). form key ingredient successful satisficing planning (not giving optimality guarantee), particular almost winners
satisficing-planning tracks International Planning Competitions (IPC).
c
2016
AI Access Foundation. rights reserved.

fiF ICKERT & H OFFMANN & TEINMETZ

Despite success, pitfalls delete-relaxation heuristics (for example, ignoring
resource consumption) known since long time, intense
efforts outset take deletes account (e. g., Fox & Long, 2001; &
Kambhampati, 2001; Gerevini et al., 2003; Helmert, 2004; van den Briel, Benton, Kambhampati, & Vossen, 2007; Helmert & Geffner, 2008; Cai, Hoffmann, & Helmert, 2009; Baier
& Botea, 2009; Coles, Coles, Fox, & Long, 2013; Alcazar, Borrajo, Fernandez, & Fuentetaja,
2013). Two recent approaches, red-black planning (Domshlak, Hoffmann, & Katz, 2015)
refer explicit conjunctions, devised allow systematically:
partial delete relaxation, principle render heuristic estimate perfect. herein
focus explicit conjunctions.
summarize results follows, need basic notation concepts
well known planning community. denote perfect heuristic, returning precise remaining cost, h ; heuristic returning cost optimal relaxed plan h+ ;
relaxed plan heuristics hFF (from system FF first introduced,
see Hoffmann & Nebel, 2001). assume common method computing relaxed plan
heuristics relaxed plan extraction best-supporter function (Keyder & Geffner, 2008).
assume default best-supporter function derived max heuristic
hmax (Bonet & Geffner, 2001). Relaxed plan extraction hmax best-supporter function is,
unit-cost problems, equivalent original formulation terms relaxed planning
graphs Hoffmann Nebel (2001).
Explicit conjunctions first introduced Haslum (2009) compilation-based
characterization critical-path relaxation introduced earlier Haslum Geffner
(2000). relaxation assumes that, goal set facts (a fact conjunction
needs achieved point plan), suffices achieve costly
subgoal (subconjunction) size m. Here, parameter corresponding
heuristic denoted hm . special case = 1 equivalent max heuristic, i. e., h1 =
hmax . Haslums (2009) compiled planning task represents size- conjunction
c via newly introduced -fluent c , arranges preconditions effects
-fluents h1 (m ) = hm .
Subsequently, Haslum (2012) introduced modified compilation C , admits
arbitrary sets C conjunctions guarantees admissibility h+ compilation,
i. e., h+ (C ) h , true h+ (m ). furthermore showed
method converges h , i. e., h+ (C ) = h appropriately chosen C. downside
C size worst-case exponential |C |: Say action support
conjunction c c regressed a, i. e., makes part c true makes none c
false. order guarantee admissibility h+ (C ), C explicitly enumerates subsets
C 0 C conjunctions c occurrence action plan may support.
C compilation (Keyder, Hoffmann, & Haslum, 2012,
size explosion tackled ce
C still
2014), handles possibly-supported c separate conditional effect. ce
guarantees convergence, yet loses information ignores cross-context conditions, i. e.,
precondition -fluents arise combination several supported c C 0 .
One thing evident history explicit conjunctions resulting heuristic functions combine information inherent critical-path relaxation, information
inherent delete relaxation. way, exactly? compilation view
simple elegant, meaning respect original planning task opaque.
270

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

first simple observation step size- conjunctions arbitrary conjunctions C specific the, historically, simultaneous step critical-path partial
delete relaxation heuristics. hm heuristic straightforwardly generalizable consider,
size- subgoals, arbitrary set C subgoals. Intuitively, choose
set atomic subgoals, kept intact critical-path relaxation. denote
generalized heuristic hC .
second simple observation delete relaxation viewed allowing
achieve fact separation: achieving facts p goal set/conjunction one-by-one,
negative effects within step matter concern facts p.
Given this, h+ characterized terms equation related characterizing
h1 , requiring achieve size-1 subgoals instead achieving single
costly one.
Putting two observations together, obtain natural generalization standard delete-relaxation framework: standard delete relaxation, like h1 , works
singleton facts atomic subgoals, one use conjunctions C atomic subgoals instead.
spell form two heuristic functions denote hC+ hCFF :
(1) h1 -like equation characterizing h+ translates hC -like equation characterizing
hC+ , equivalent h+ (C ).
(2) Relaxed plan extraction obtain hFF h1 best-supporter function translates
relaxed plan extraction obtain hCFF (a relaxed plan C ) hC best-supporter
function.
Result (1) theoretical interest. formulates hC+ = h+ (C ) without compilation,
shedding different light Haslums (2012) equivalent proposal. Result (2)
immediate practical ramifications. provides alternative technique obtain relaxed
plans C , exponentially efficient worst case require
exhaustively enumerate subsets C 0 C. hC best-supporter function computed
time polynomial |C |, similar hm . Intuitively, critical path pertains single
atomic subgoals, need enumerate combinations atomic subgoals here.
relaxed plan extraction, avoid enumeration identifying tackling precise
source complexity.
Relaxed plan extraction hC complex relaxed plan extraction h1
two reasons, first corresponds Haslums (2012) observations, yet second
one becomes apparent new direct formulation:
(a) ensure convergence, allowing hFF (C ) find real plans limit, need
collect set action occurrences, i. e., pairs ( a, C 0 ) action set supported
conjunctions C 0 , instead set actions standard setting (where actions
merely support facts direct effect).
(b) every occurrence ( a, C 0 ) selected relaxed plan extraction, C 0
large possible, atomic subgoals (conjunctions) may overlap, incurring
risk dramatic overestimation (e. g., achieving every fact pair global goal separately). NP-complete find cardinality-maximal C 0 incur
infeasible cross-context conditions.
understand this, consider action support least one subgoal c C
relaxed plan extraction. need decide current subgoals c0 C
271

fiF ICKERT & H OFFMANN & TEINMETZ

support action occurrence. standard setting, c c0
singletons (e. g. c = { p} c0 = {q}), one simply support c0 positive effects
(e. g. add( a) = { p, q}). general case, arbitrary C, longer
part c0 contained positive effects gets propagated new subgoal
regressing (e. g. r1 c10 = {q, r1 } r2 c20 = {q, r2 }). Combinations several
c0 may incur cross-context conditions (e. g. {r1 , r2 }) harder achieve conjunctions
c0 isolation.
refer (b), maximization |C 0 | relaxed plan extraction, subgoalsupport selection problem. striking underlying phenomena supported
conjunction sets C 0 cross-context conditions previously identified addressed,
yet put specific context relevant relaxed plan extraction. perspective, Haslum (2012) solves NP-complete problem enumeratively, putting solution
candidates (choices C 0 ) memory form compiled task C ; Keyder et
C compilation over-simplifies problem, ignoring cross-context conal.s (2012, 2014) ce
ditions completely. Yet, cardinality-maximal C 0 real issue, dont simply
select subset-maximal C 0 instead? Using simple greedy approximation effect,
obtain hCFF , extracting relaxed plans C polynomial time without ignore
cross-context conditions. heuristic supersedes, theoretical perspective
C compilations.
far hFF concerned, C ce
point necessary mention observation (2) entirely new.
Alcazar et al. (2013) already devised heuristic call FFm , extracting relaxed plan
hm best-supporter function (they implement = 2). essentially
(2), without generality arbitrary conjunction set C (which easily fixed).
However, Alcazar et al.s work conducted part much broader scope addressing
heuristic search regression planning, investigate (2) detail. design
FFm recognize, therefore appropriately address, (a) (b). Regarding
(b), FFm always selects single conjunction C 0 = {c} support, trivial approximation
NP-complete |C 0 |-maximization problem, may lead dramatic overestimation.
overestimation counter-acted given FFm also disregards (a), collecting set
actions standard relaxed plan extraction methods. loses convergence
value FFm bounded number actions defeating purpose method.
empirical perspective, matters clear-cut. Obviously, one construct cases computational advantage C , information advanC FF2 , leads exponential savings. IPC benchmarks another matter.
tage ce
Evaluating heuristic functions, find larger conjunction sets C typically lead
smaller search spaces, hCFF indeed much faster hFF (C ) large C.
Unfortunately, even slowdown hCFF typically outweighs search space reduction,
best overall performance often obtained small C. positive side,
techniques yield advantages even small C, IPC benchmarks large
C beneficial.
C compilations SecWe next introduce basic notation well C ce
+
C
tion 2. spell direct characterization h ( ) Section 3, spell
generalized relaxed plan extraction methods Section 4. summarize implementation experiments Section 5, concluding Section 6. proofs
replaced main text brief proof sketches. Full proofs available Appendix A.
272

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

2. Notations Technical Background
use STRIPS framework. planning task tuple = (F , A, , G) F
set facts, set actions, F initial state, G F goal. action
triple (pre( a), add( a), del( a)) precondition, add list, delete list, subset
F . henceforth tacitly assume given input task = (F , A, , G).
state subset facts F . Action applicable pre( a) s; case,
applying leads state (s add( a)) \ del( a). plan sequence iteratively
applicable actions leading state contains goal G . plan task
plan initial state . plan optimal length minimal among plans.
assume throughout add( a) del( a) = . natural common assumption action adding p also delete without loss generality
facts intersection equivalently removed add( a). assumption
necessary achieving fact separation view delete relaxation,
outlined introduction.
Note that, simplicity, consider unit costs: action costs 1, plan quality
plan length. results straightforwardly extend arbitrary non-negative action
costs, plan quality measured terms summed-up cost.
Example 1 illustration, frequently consider following car-driving example.
car moves one-way line X Z locations, X Z. car move consumes
fuel unit, cars tank holds one unit must refuel Y.
encode STRIPS, design task = (F , A, , G) follows. F = {carX, carY,
carZ, f uel }, = {carX, f uel }, G = {carZ }. consists of: XY precondition
{carX, f uel }, add list {carY }, delete list {carX, f uel }; aYZ precondition {carY, f uel },
add list {carZ }, delete list {carY, f uel }; f uel precondition {carY }, add list
{ f uel }, empty delete list. plan task h XY , f uel , aYZ i.
Given planning task , denote set states S. heuristic (also heuristic
function) function h : 7 N0+ {} mapping states natural numbers including
0, indicate state dead-end. perfect heuristic h maps state
length optimal plan (or plan s). heuristic h
admissible h(s) h (s) S. Abusing notation, often identify heuristic
h value h(I) initial state. statements made generalize arbitrary states
setting := s. h(0 ), denote heuristic whose value given applying
h modified task 0 . make explicit h computed itself, write h().
characterize heuristic functions terms equations regressed subgoals.
regression fact set G action a, R( G, a), defined add( a) G 6= del( a)
G = . case, R( G, a) = ( G \ add( a)) pre( a); otherwise, write R( G, a) = .
critical-path relaxation (Haslum & Geffner, 2000) assumes that, goal set
facts, suffices achieve costly subgoal size m. Here, parameter
corresponding heuristic denoted hm . Precisely, hm defined hm := h(G)
h function fact sets G satisfies

GI
0
1 + minaA,R(G,a)6= h( R( G, a)) | G |
(1)
h( G ) =

0
maxG0 G,|G0 |m h( G )
else
273

fiF ICKERT & H OFFMANN & TEINMETZ

easy see exactly one h, assuming two functions h, h0
h( G ) 6= h0 ( G ) recursively leads contradiction initial state.1 argument applies h-defining equations considered herein, henceforth assume
uniqueness given.
= 1, definition hm becomes identical max heuristic hmax (Bonet
& Geffner, 2001), assumes that, achieve goal fact set, enough achieve
maximum costly single fact. = |F |, hm = h simply subgoals size > |F |
exist. Computing hm takes time exponential polynomial size .
delete relaxation assumes delete lists empty; plan relaxation
relaxed plan. ideal delete-relaxation heuristic h+ maps length optimal
relaxed plan s. optimal relaxed planning NP-complete (Bylander, 1994). relaxed plan heuristic maps length some, necessarily optimal, relaxed plan
s, computed easily (Hoffmann & Nebel, 2001). resulting heuristic functions admissible, often informative practice satisficing planning.
follow common approach considering idealized heuristic h+ theoretical examinations delete relaxation (compare, e. g., Hoffmann, 2005, 2011; Bonet &
Helmert, 2010), considering effective approximation relaxed plan heuristics practice.
Relaxed plan heuristics differ find relaxed plan. flexible way
specifying best-supporter functions introduced Keyder Geffner (2008).
best-supporter function maps fact p action relaxed plan use
support p. Given function, relaxed plan extraction starts goals, keeps selecting best supporters, opening preconditions new subgoal facts, initial state
facts reached. denote heuristic arising process hFF (disambiguating context needed). detailed formal characterization relaxed
plan extraction given Section 4, details technically relevant.
Practical best-supporter functions based hmax = h1 , selecting p action
A, R( G, a) 6= minimizing expression middle case Equation 1 = 1
(where subgoal G singleton set { p} identified element p).
Alternatively, one assign best supporters based additive heuristic hadd (Bonet &
Geffner, 2001) instead, differs h1 using sum, rather maximum,
estimated cost facts goal set (bottom case Equation 1). Note
(in hmax hadd ) may several actions eligible best supporter. Hence
construction best-supporter functions encompasses tie-breaking, sense
choosing action set actions supporting given fact p. tie-breaking
large effect empirical performance relaxed plan heuristic. get
back detail experiments.
Throughout paper, concerned conjunctions c. Following STRIPS
convention formulating conjunctive conditions (action preconditions goal)
fact sets, conjunction c fact set c F , e. g. c = { p, q}. However, improve
readability, often notate c conjunctive formula instead, e. g. c = p q.
1. Matters complicated case 0-cost actions, recursion may lead cycles
point-wise maximal h unique.

274

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

henceforth assume given set C conjunctions. practice, C computed input task , prior search.2 assume throughout C contains
singleton conjunctions, {{ p} | p F } C. convenience, notating facts
special case conjunctions. sometimes identify singleton conjunctions { p}
respective facts p, i. e., notate without set brackets avoid clutter; note
p also notation get writing { p} conjunctive formula.
convenient introduce shorthand operation collecting atomic
conjunctions contained fact set. Given set facts X F , assuming given
conjunction set C described, define X C := {c | c C, c X }. sometimes
extend notation sets X = { X1 , . . . , Xn } fact sets, X C defined pointwise,

i. e., X C := XiC .
C compilation relatives based representing conjunctions explicitly,
terms introducing new facts called -fluents. introduce one
fluent, c , c C. correspondence shorthand introduced, fact
set X F , X C := {c | c C, c X } denote set collecting -fluents
atomic conjunctions entailed X. extend notation sets fact sets
pointwise manner above.
Using notations, C defined follows:
Definition 1 Let = (F , A, , G) planning task, C set conjunctions containing singleton conjunctions. explicit-C compilation C planning task (F C , AC ,
C , G C ). Here, F C , C , G C defined per shorthand. set actions AC
contains action a[C 0 ], every pair 6= C 0 {c C | R(c, a) 6= }, a[C 0 ]
given
pre( a[C 0 ]) = [



cC 0 (pre( ) ( c

\ add( a)))]C ,

add( a[C 0 ]) = {c | c C 0 }.
C identical C except pre( [C 0 ]) =
cross-context explicit-C compilation nc
0
C
{pre( a) (c \ add( a)) | c C } .

refer pairs ( a, C 0 ) action set C 0 supported conjunctions, corresponding
C , action occurrences. refer c \ add( ),
compiled actions a[C 0 ] C nc
0
c C , context c a: support c, context must true preceding
C , extend original prestate. captured preconditions C nc
condition contexts supported conjunctions. C , context collected
C done supported conjunction inacross supported conjunctions, nc
dividually.
definition C diverges original definition Haslum (2012) several
minor ways. First, distinguish explicitly actions original effects
vs. supported conjunctions, instead expressing add list part set C 0
conjunctions may supported. possible C assumed contain
singleton conjunctions. consequence, demand C 0 6= (otherwise
action would effect thus useless). Second, automatically include
2. details exactly done relevant contribution. briefly describe
methods use (adopted Keyder et al., 2012, 2014) discussion experiments, Section 5.

275

fiF ICKERT & H OFFMANN & TEINMETZ

c facts relying context consisting non-deleted preconditions. Third,
demand C 0 downward closed, i. e., contain subsumed conjunctions c0 ,
exists c C 0 c0 c. Fourth, include delete effects.
None changes consequences results present. Changes two
four introduce superfluous actions, simplifying presentation affecting
results. fourth change suitable use C generating deleterelaxation heuristics. consistent use language, speak relaxed plans
C nevertheless.
also modify notation bit, relative Haslum (2012). notate actions
0
a[C 0 ] instead AC . convenient. furthermore somewhat modified definition a[C 0 ] preconditions, exploiting C 0 6= . Namely, pre( a[C 0 ]) =

[ cC0 (pre( a) (c \ add( a)))]C C equivalent perhaps intuitively straightS
forward definition, namely pre( a[C 0 ]) = [pre( a) cC0 (c \ add( a))]C used Haslum.
C , precondition pre( [C 0 ]) = {pre( ) ( c \ add( )) | c C 0 }C , given pointFor nc

wise interpretation C superscript, equals cC0 [pre( a) (c \ add( a))]C ,
perhaps intuitively straightforward definition pre( a[C 0 ]) =

pre( a)C cC0 [pre( a) (c \ add( a))]C . Observe modifications allow write
action preconditions terms regression, thanks R(c, a) = pre( a) (c \ add( a)).

C , reads pre( [C 0 ]) =
C , precondition reads pre( a[C 0 ]) = [ cC0 R(c, a)]C . nc
0
C
{ R(c, a) | c C } . simplified notations conveniently link-in concepts
introduce later on.
Note finally C compilation introduces atomic conjunctions action preconditions goal, even ones subsumed other, larger, atomic conjunctions contained precondition/goal. stick convention throughout, simplicity. practice, ignore subsumed conjunctions. remainder paper,
corresponds modified C superscript, including conjunctions c C, c X,
exist c0 C, c0 X, c ( c0 ; correspondingly C
superscript. leaves results intact exactly stated.
Example 2 Reconsider car-driving example task Example 1. delete relaxation
ignores negative effect XY , shortest relaxed plan h XY , aYZ h+ = 2.
However, say set C contain (all singleton conjunctions well as) c = carY fuel.
Then, C , c precondition actions aYZ [C 0 ], i. e., actions adding goal carZ.
actions adding c form arefuel [C 0 ] c C 0 . Hence h XY , aYZ
relaxed plan C . Instead, need perform refueling action, example relaxed plan
h XY [{carY }], arefuel [{carY fuel}], aYZ [{carZ }]i. get h+ (C ) = 3 = h ().
C exponential |C | action occurrences enumerThe growth C nc
ate subsets C 0 C supported conjunctions. complexity necessary because,
C ) would admissible. simple example, say
otherwise, h+ (C ) h+ (nc
goal n { g1 , . . . , gn }, C contains singleton conjunctions well fact
pairs, single action achieving { g1 , . . . , gn }. h (n ) = 1,
h+ (Cn ) = 1 thanks optimal plan h a[C 0 ]i C 0 set conjunctions,
C 0 = C = {{ gi } | 1 n} {{ gi , g j } | 1 6= j n}. However, achieve
every conjunction separately Cn , is, included AC actions
n(n1)
form a[{c}] c C, would get h+ (Cn ) = n +
would
2

276

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

achieve every conjunction c C separate compiled action. (This observation
become relevant later on, compare Example 7 Section 4.3.1.)
C latter, former, ignores
difference C nc
termed cross-context conditions: conjunction preconditions a[C 0 ] C
arise combination several c C 0 . Precisely, cross-context condition

C 0 conjunction c C c cC0 [pre( a) (c \ add( a))],
C compilation, preexist single c C 0 c pre( a) (c \ add( a)). nc
0
condition a[C ] contain cross-context conditions, superscript
C pre( a[C 0 ]) = {pre( a) (c \ add( a)) | c C 0 }C , i. e., collection conjunctions, done context c C 0 separately. contrast C where,

pre( a[C 0 ]) = [ cC0 (pre( a) (c \ add( a)))]C , conjunctions collected union
contexts across c C 0 .
Example 3 illustrate cross-context conditions, consider following abstract example (given Keyder et al., 2014, part proof Theorem 3). = (F , A, , G)
F = { g1 , g2 , p, q1 , q2 }, = {q1 }, G = { g1 , g2 } consists of: g1 precondition { p, q1 },
add list g1 , empty delete list; g2 precondition { p, q2 }, add list g2 , empty delete list;
p empty precondition, add list { p}, empty delete list; aq2 precondition {q1 }, add
list {q2 }, delete list {q1 , p}. construction, q1 q2 mutex; achieving g1 requires
q1 thus done first, via p g1 . achieve g2 , require q2 . Getting q2 aq2
deletes p, must apply p second time applying g2 . delete relaxation
never need apply action twice, h+ = 4 < 5 = h .
Say set C contain cq1p = q1 p, cq2p = q2 p, cq1q2 = q1 q2 .
relaxed plan C must contain two occurrences p : cq1p cq2p required achieve
goal; p action support conjunctions (note aq2 deletes p);
p [{cq1p , cq2p }] supporting conjunctions single action occurrence unreachable
cross-context condition cq1q2 . Consequently, h+ (C ) = 5 = h ().
C , [{ c
contrast, nc
p
q1p , cq2p }] cross-context condition, h aq2 [{ q2 }],
C ) = 4 = h + ( ) < h ( ).
p [{ p, cq1p , cq2p }], g1 [{ g1 }], g2 [{ g2 }]i relaxed plan h+ (nc
C compilation, achieves
Keyder et al. (2012, 2014) introduced ce
C
effect nc size polynomial |C |. done augmenting every original
action one conditional effect c C regressed a,
adding c requiring context c \ add( a) effect condition.
C compilation equivalent C sense h+ ( C ) = h+ ( C ). InThe ce
nc
ce
nc
C , conditional effects set
tuitively, occurrence action ce
C action [C 0 ] C conjunctions
conjunctions C 0 fire, equivalent nc
ce
c C 0 handled separately, ignoring cross-context conditions.3 Given equivalence,
theoretical discussion heuristic functions properties size
C C matter refer throughout C rather
difference nc
ce
nc
C . simplifies matters switch formalisms
ce
(STRIPS vs. without conditional effects).
C ) h+ ( C ) proved Keyder et al. (2014) proof Lemma 2,
3. Technically, h+ (ce
nc
C ) h+ ( C ) symmetric.
opposite direction h+ (ce
nc

277

fiF ICKERT & H OFFMANN & TEINMETZ

C correWe see Section 4.3 complexity reduction C ce
spondence complexity reduction subgoal-support selection problem (maximization |C 0 | relaxed plan extraction): problem NP-complete C ,
C.
polynomial-time nc

3. Combining Delete Relaxation Critical Paths: hC+
spell observation (1) introduction, characterizing combination
delete relaxation critical paths directly, without compilation, terms
heuristic function call hC+ . Section 3.1 starts simple novel views
two components, Section 3.2 combines equation characterizing hC+ . Section 3.3 sketches proof correctness i. e., hC+ = h+ (C ). Section 3.4 summarizes
properties hC+ , pointing combination delete relaxation critical paths naturally generalizes components.
3.1 Novel Views hm h+
First, consider following straightforward characterization h , relaxed
different manners below: h := h(G) h function fact sets G satisfies

0
GI
h( G ) =
(2)
1 + minaA,R(G,a)6= h( R( G, a)) else
equation obviously characterizes optimal planning, therewith h : minimize
plan length actions support subgoal G.
Slightly rephrasing critical-path relaxation, assumes that, achieve subgoal G,
suffices achieve costly atomic subgoal, notion atomic subgoal
parameter. traditional formulation, parameter instantiated fact
sets size m. need restrictive. atomic subgoals
arbitrary set fact-sets, words: arbitrary set C conjunctions. merely
need replace subgoal-selection mechanisms hm (Equation 1) accordingly generalized ones. denote resulting heuristic hC , defined hC := h(G) h
function fact sets G satisfies

GI
0
1 + minaA,R(G,a)6= h( R( G, a)) G C
(3)
h( G ) =

0
maxG0 G,G0 C h( G )
else
Trivially, hC = hm C consists conjunctions size m. shall see below,
hC = h1 (C ) one would expect. latter property useful theoretical
perspective though, connecting hC known results h1 . practice, hC computed like hm , fixed point process value assignments atomic subgoals C,
taking time polynomial |C |, contrast size C . particular implementation
described Section 5.1.
worth pointing simple generalization hm hC already
quite useful:

278

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

Example 4 Consider car-driving example Example 1, without refuel action.
modified task unsolvable, h2 = recognizes that. Yet, need reason fact
pairs arrive conclusion: Considering single fact pair C = {c} c = carY fuel,
like Example 2, suffices get hC = , c becomes precondition achieving goal,
action c regressed. particular example 4
facts thus 6 fact pairs, could scale arbitrarily adding solvable parts, blowing
computational overhead h2 still recognizing unsolvability using single fact pair c.
Getting back discussion alternate ways relax h , i. e., Equation 2, observe
Equation 3 uses correct regression semantics, relaxes subgoals considered.
delete relaxation viewed approaching vice-versa, keeping correct
subgoaling relaxing regression semantics. immediately visible following straightforward characterization h+ , h+ := h(G) h function
fact sets G satisfies

0
GI
h( G ) =
(4)
1 + minaA,6=Gadd(a) h(( G \ add( a)) pre( a)) else
identical Equation 2 except pretending R( G, a) 6= even del( a) G 6=
, i. e., replacing R( G, a) relaxed concept asks non-empty add-list
intersection.
basic observation towards combining hC h+ underlying relaxation
principles, though seem unrelated given Equations 3 4, viewed
relaxations pertaining subgoaling structure. becomes visible following
alternative characterization h+ :
Lemma 1 Let = (F , A, , G) planning task, let h function fact sets G
satisfies

0
GI

h( G ) =
(5)
0
1 + minaA,6=G0 ={ p| pG,R({ p},a)6=} h(( G \ G ) pG0 R({ p}, a)) else
h(G) = h+ .
Proof: Observe that, singleton fact sets G = { p}, (a) regressability G trivializes
add-list intersection, i. e., R({ p}, a) 6= iff p add( a), add( a) del( a) =
get p 6 del( a); (b) G regressed regression simply generates action precondition new subgoal, i. e., R({ p}, a) = pre( a). Equation 5
simplifies

0
GI
h( G ) =
1 + minaA,6=G0 =Gadd(a) h(( G \ G 0 ) pre( a)) else
G 0 = G add( a) G \ G 0 = G \ add( a), equivalent Equation 4.
per Equation 5, delete relaxation understood splitting subgoals
singleton facts, considering regression separately respect these. singleton
regression trivializes, effect need worry part subgoal
279

fiF ICKERT & H OFFMANN & TEINMETZ

support, parts action may contradict.4 reformulation awkward useful standard setting, exhibits possible refinement
setting: instead singleton facts, consider atomic subgoals form arbitrary
set C conjunctions.
3.2 Combined Heuristic
Consider Equation 5, compare following equation characterizing h1 :

GI
0
1 + minaA,R({ p},a)6= h( R({ p}, a)) G = { p}
(6)
h( G ) =

max pG h({ p})
else
Equation 5 understood less relaxed version Equation 6. decompose
subgoal G atomic subgoals, instantiated singleton facts, minimize
actions regressing atomic subgoals. difference that, Equation 6 picks
single costly atomic subgoal, Equation 5 requires achieve every atomic subgoal (in
particular, including ones supported a, i. e., G \ G 0 , recursive invocation
h). set G consists exactly atomic subgoals, Equation 5 need third
case identifying atomic subgoals.
Now, hC generalizes h1 considering general atomic subgoals C. Applying
similar generalization Equation 5, obtain desired combination delete
relaxation critical paths:
Definition 2 Let = (F , A, , G) planning task, C set conjunctions containing singleton conjunctions. critical-path delete relaxation heuristic, short C-relaxation
heuristic, defined hC+ := h(G C ), h function conjunction sets G satisfies
(
0
c G : c
h( G ) =
(7)
C
0
0
1 + minaA,6=G0 {c|cG,R(c,a)6=} h(( G \ G ) Gr ) else
Gr0 defined Gr0 := cG0 R(c, a).
cross-context critical-path delete relaxation heuristic, short nc-C-relaxation heurisC + , defined identically hC + except define G 0 : = { R ( c, ) | c G 0 }.
tic, denoted hnc
r


Recall that, fact set X, X C := {c | c C, c X } denotes set atomic
C+ )
conjunctions contained X, set fact sets (as case Gr0 hnc
apply notation pointwise. convention, refer expression ( G \
C
G 0 ) Gr0 Equation 7, related equations, recursive subgoal, Gr0
regressed subgoal.
Intuitively, hC+ supports atomic subgoals C individually regression hC ,
instead achieving costly one, achieves them. parallels
previous comparison h+ h1 . subgoals G recursed sets
conjunctions, difference h+ (Equation 5) atomic subgoals conjunctions
4. independence assumptions identified Keyder Geffner (2009) somewhat related this.
formulation pertains delete relaxation heuristic h+ itself, ignoring negative side effects; whereas
Keyder Geffners observations pertain simplifying assumptions approximations h+ , ignoring
positive side effects.

280

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

instead single facts, difference hC (Equation 3) estimate cost sets
atomic subgoals instead single atomic subgoals. initializing call G C inserts
atomic conjunctions global goal, recursive subgoals insert atomic
conjunctions regressed subgoal Gr0 . Hence, like Equation 5 set G consists
exactly atomic subgoals, need third case identifying atomic
subgoals.
top case Equation 7 self-explanatory. bottom case generalizes
Equation 5. atomic subgoals non-unit conjunctions, difference
arguments Lemma 1, regression longer trivializes: allowed contradict
conjunction c G 0 , R(c, a) may proper superset pre( a), longer trivializing
action precondition. Hence, difference Equation 5, complex notation
necessary. also major new source complexity, relative Equation 5,
namely need allow G 0 subset supportable atomic subgoals, rather
setting G 0 entire set. corresponds aforementioned subgoal-support
selection problem. illustrate problem Example 5 below; Section 4.3 conducts
in-depth analysis context relaxed plan extraction.
C + is, notation suggests, designed match
difference hC+ hnc
C . expressions
0
difference C nc
c G 0 R ( c, ) vs. { R ( c, ) | c G }
obvious correspondence action preconditions Definition 1 (thanks
modifications respect original definition). pair ( a, G 0 ) action subset supported atomic subgoals hC+ equation corresponds C action a[C 0 ]
C + C . instructive alternative way read rewhere C 0 = G 0 , similarly hnc
nc

V
gressed subgoals Gr0 terms conjunctions. gives cG0 R(c, a) = cG0 R(c, a) =
V
V
0
0
c G 0 ,p R(c,a) p, vs. { R ( c, ) | c G } = { p R(c,a) p | c G }: one large conjunction vs.
several small ones. makes difference larger conjunctions may contain larger
C
atomic subgoals, captured Definition 2 respective use Gr0 .
Example 5 Consider, Example 2 (page 276), car-driving example C containing
singleton conjunctions well c = carY fuel. get hC+ = h({carZ }), i. e., h defined
per Equation 7, applied conjunction set containing single goal atomic conjunction
carZ. ( a, G 0 ) pair supporting carZ ( aYZ , {carZ }). Selecting ( a, G 0 ), get
recursive subgoal G = {carY, fuel, carY fuel}. carY fuel cannot supported XY
deletes fuel, supporting action subgoal arefuel . Say select action,
G 0 := {carY fuel}. recursive subgoal {carY, fuel} conjunctions
carY fuel G included G 0 . detail, recursive subgoal results exS
pression ( G \ G 0 ) [ cG0 R(c, a)]C = ({carY, fuel, carY fuel} \ {carY fuel}) R(carY
fuel, arefuel )C = {carY, fuel} {carY }C = {carY, fuel} {carY }. subgoal resolved
using ( XY , {carY }), yielding hC+ = h+ (C ) = 3 due relaxed plan Example 2.
consider, Example 3 (page 277), abstract example C containing singleton conjunctions well cq1p = q1 p, cq2p = q2 p, cq1q2 = q1 q2 .
hC+ = h({ g1 , g2 }), requiring support two goal facts (written singleton conjunctive formulas here). done ( g1 , { g1 }) ( g2 , { g2 }) respectively; using
these, get recursive subgoal G = {q1 , q2 , p, q1 p, q2 p}. non-trivial
subgoal-support selection problem. Ignoring subsumed subgoals q1 , q2 , p tackled
side effect tackling non-subsumed ones q1 p q2 p, choose (a)
281

fiF ICKERT & H OFFMANN & TEINMETZ

( p , {q1 p, q2 p}), (b) ( p , {q1 p}), (c) ( p , {q2 p}). choose (a), subgoal

fully supported i. e., G \ G 0 = , yet [ cG0 R(c, p )]C = {q1 , q2 }C = {q1 , q2 , q1 q2 }.
cross-context conjunction q1 q2 supported action, cannot get initial state
way. Instead, need take either (b) (c), yielding recursive subgoals (b) {q2 p, q1 }
respectively (c) {q1 p, q2 }, necessitates support aq2 well another occurrence
p , leading hC+ = h+ (C ) = h = 5.
C + instead, option (a) produces different subgoal { R ( c, ) | c G 0 }C =
Using hnc
{{q1 }, {q2 }}C = {q1 , q2 }, containing cross-context conjunction q1 q2 . subgoal
C + = h+ ( C ) = h+ = 4.
feasible, requires support aq2 , leading hnc
nc
3.3 Proof Correctness
prove Equation 7 indeed capture h+ (C ), i. e., h+ (C ) = hC+ ().
illustration, first consider simple case C contains singleton conjunctions:
Proposition 1 Let = (F , A, , G) planning task, C = {{ p} | p F }.
h+ = hC + .
Proof: C = {{ p} | p F }, recursive subgoals G Equation 7 sets singleton
fact-sets, instead perceive G set facts. re-write Equation 7 to:

0
GI

h( G ) =
0
1 + minaA,6=G0 { pG| R({ p},a)6=} h(( G \ G ) pG0 R({ p}, a)) else
identical Equation 5 except G 0 allowed subset { p G |
R({ p}, a) 6= }. However, R({ p}, a) = pre( a), minimum bottom case
always achieved using G 0 = { p G | R({ p}, a) 6= }, yield smaller
recursive subgoals G 0 { p G | R({ p}, a) 6= }. concludes proof
Lemma 1.
Observe Proposition 1 proves h+ (C ) = hC+ () C = {{ p} | p F }:
singleton conjunctions only, h+ = h+ (C ), Proposition 1 h+ (C ) = h+ =
hC+ () desired. extend general case, arbitrary conjunction sets:
Theorem 1 Let = (F , A, , G) planning task, C set conjunctions containing
singleton conjunctions. h+ (C ) = hC+ ().
Proof Sketch: apply Equation 5 C , characterizing h+ (C ). Making explicit
individual facts C -fluents, obtain: h+ (C ) = h({c | c G C }),
h function fact sets G satisfies h( G ) =

0
c G : c C
0
0
1 + mina[C0 ]AC ,6=G0 ={c |c G,R({c },a[C0 ])6=} h(( G \ G ) Gr ) else
Gr0 defined Gr0 := c G0 R({c }, a[C 0 ]).
condition R({c }, a[C 0 ]) 6= simplifies c C 0 , exactly
-fluents added a[C 0 ]. G 0 = {c | c G, c C 0 } minimization
a[C 0 ] supporting non-empty subset subgoals c . c principle


282

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

include C 0 are, definition C , exactly R(c, a) 6= .
point including c c 6 G, support subgoals yet
result larger precondition. Hence, renaming C 0 G 0 order unify notation,
obtain h( G ) =

0
c G : c C
1 + mina[G0 ]AC ,6=G0 {c|c G,R(c,a)6=} h(( G \ G 0 ) Gr0 ) else
Gr0 defined Gr0 := cG0 R({c }, a[ G 0 ]).
Comparing equation Equation 7, easy see equations
exact correspondence via () G = {c | c G [7]}, G [7] denotes subgoal sets
Equation 7. (*) true definition initializing calls, hC+ () = h(G C ) respectively h+ (C ) = h({c | c G C }). (*) invariant bottom cases


equations, Gr0 = cG0 R({c }, a[ G 0 ]) = pre( a[ G 0 ]) = [ cG0 (pre( a) (c \ add( a)))]C =


[ cG0 R(c, a)]C , matches regressed subgoal Gr0 [7] = [ cG0 R(c, a)]C Equation 7 desired.


C hC + :
similar proof shows correspondence nc
nc

Theorem 2 Let = (F , A, , G) planning task, C set conjunctions containing
C ) = h C + ( ).
singleton conjunctions. h+ (nc
nc
3.4 Properties Combination
combination delete relaxation critical paths, per Definition 2, naturally
generalizes properties components. follows known results along
following simple observation:5
Theorem 3 Let = (F , A, , G) planning task, C set conjunctions containing
C ) = h C ( ).
singleton conjunctions. h1 (C ) = h1 (nc
Proof Sketch: Consider first C . Applying Equation 6 (page 6) characterizing h1 C ,
get h1 (C ) = h(G C ) h function fact sets G satisfies

G C
0
0
1 + mina[C0 ]AC ,R(G,a[C0 ])6= h( R( G, a[C ])) G = {c }, c C
h( G ) =

maxc G h({c })
else
Observe that, middle case, must c C 0 otherwise c 6 add( a[C 0 ]);
point including conjunctions C 0 , i. e., C 0 ) {c}, yield larger recursive subgoal R( G, a[C 0 ]). Hence re-write
previous equation to:

G C
0
1 + mina[{c}]AC ,R(G,a[{c}])6= h( R( G, a[{c}])) G = {c }, c C
h( G ) =

maxc G h({c })
else
C ) part observation, using different proof
5. Keyder et al. (2014) already proved h1 (C ) h1 (nc
argument.

283

fiF ICKERT & H OFFMANN & TEINMETZ

Comparing equation Equation 3 (page 278) characterizing hC , easy see
equations exact correspondence via () G = {c | c C, c G [3]}, G [3]
denotes subgoal (fact) sets Equation 3.
C identical because, single-conjunction sets C 0 = { c }, two
argument nc
compilations coincide.
Note that, step first second equation stated proof, exponential size C reduced polynomial-size compilation like C
includes actions a[{c}] pairs c C c regressed
a. Intuitively, h1 considers singleton subgoals, need enumerate supported conjunction sets size greater 1. reduced compilation essentially
version Haslums (2009) compilation arbitrary conjunction sets C. (This simple
generalization mentioned Haslum works C .)
Together results Haslum (2012) Keyder et al. (2014), well basic known
results h1 h+ , Theorems 1 3 immediately imply properties one would
C + have:
naturally expect hC+ hnc
Corollary 1 Let planning task. Then, set C conjunctions containing
singleton conjunctions, have:
C + hC + h ;
(i) hC , h+ hnc
C + = iff hC = .
(ii) hC+ = iff hnc
C + converge h , i. e., exist sets C conjunctions
Furthermore, hC+ hnc
C
+

C+ = h .
(iii) h = h respectively (iv) hnc
C + = h+ ( C ) Theorem 3 hC = h1 ( C ),
Proof: Regarding (i): Theorem 2, hnc
nc
nc
C
C
+
1
+
C ) = h+ ( C ) hence hC + = h+ ( C ),
h hnc follows h h . h+ (nc
ce
nc
ce
C + holds corresponding result Keyder et al. (2014) (h+ h+ ( C ),
h+ hnc
ce
C drops preconditions C , get hC + = h+ ( C ) h+ ( C ),
Corollary 1). nc
nc
nc
+
C
C
h ( ) = h + Theorem 1. Finally, hC+ h holds corresponding result
Haslum (2012) (h+ (C ) h , Theorem 4).
C + = h+ ( C ), hC = h1 ( C ) = h1 ( C ),
Regarding (ii): hC+ = h+ (C ), hnc
nc
nc
follows h+ = iff h1 = .
Finally, (iii) holds convergence h+ (C ) (Haslum, 2012, Theorem 5) hC+ =
+
C ) (Keyder et al., 2014,
h (C ) per Theorem 1, (iv) holds convergence h+ (ce
C ) = h+ ( C ) hC + = h+ ( C ) per Theorem 2.
Theorem 5) h+ (nc
ce
nc
nc

4. Extracting Relaxed Plans: hCFF
observed that, like standard setting, hC+ = iff hC = , i. e., relaxed plan
exists iff critical-path component heuristic solvable. hC behaves like h1
role deciding relaxed plan existence. then, hC also fulfill role h1
relaxed plan extraction, i. e., finding necessarily optimal relaxed plan?
Implicitly, already done C compilation, via relaxed plan extraction h1 (C ) = hC , construction wasteful computing hC actually
require exponential blow-up inherent C . make without blow-up?
284

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

indicated introduction (in observation (2)), answer yes, form
heuristic function denote hCFF : Relaxed plan extraction obtain hFF h1 bestsupporter function translates relaxed plan extraction obtain hCFF hC best-supporter
function. Thanks direct formulation, using compilation, hCFF computes relaxed
plans C time polynomial |C |.
spell detail, start Section 4.1 by, similarly before, reformulating
standard relaxed plan extraction way preparing generalization arbitrary conjunction sets C. Section 4.2 specifies generalization proves correct. Section 4.3
analyzes subgoal-support selection problem, benign standard setting
NP-complete general case; define heuristic function hCFF using greedy
solution problem.
Throughout, use equation-based formulations generalize directly arbitrary action costs. improve readability, also include pseudo-code formulations
apply simpler unit-cost case. Like before, distinguish variC ).
ants taking cross-context conditions account (C ) vs. (nc
4.1 Relaxed Plan Extraction h1
Relaxed plan extraction first formulated terms best-supporter functions Keyder Geffner (2008). advantage traditional relaxed planning graph formulations (Hoffmann & Nebel, 2001) best-supporter functions flexible,
allowing use hadd instead h1 , generalizing arbitrary action costs. shall
see, best-supporter formulation also generalizes easily use hC instead h1 .
Best-supporter functions map facts actions. Based h1 , fact p mapped
action achieving h1 ({ p}), i. e., achieving minimum h1 equation (Equation 6,
page 279). unit-cost actions, latter equivalent h1 (pre( a)) = h1 ({ p}) 1.
hence write Keyder Geffners formulation relaxed plans FF input task

= (F , A, , G) FF := pG ( g), function facts p satisfies:
( p) =






pI
(q) { a} A,
p add( a), h1 (pre( a)) = h1 ({ p}) 1 else

qpre( a)

(8)

easy see action set FF sequentialized form relaxed plan .
generalization arbitrary cost done working h1 (pre( a)) = h1 ({ p})
c( a) instead, using modified action-costs function c0 := c + e, e > 0, case
0-cost actions (e principle chosen preserve optimality;
minor concern relaxed-plan heuristic functions inadmissible anyway).
Note special case h1 (pre( a)) = p add( a), hence
h1 ({ p}) = . situation, p best supporter Keyder Geffners
formulation. formulation, ( p) undefined (i. e., notation equal
1). abstract issue throughout present subsection, assuming
h1 ({ p}) < facts. deal issue extension conjunction
sets C, partial function.
Note furthermore intentionally specify function satisfies Equation 8. relaxed plan FF unique tie-breaking. Keyder Geffners
285

fiF ICKERT & H OFFMANN & TEINMETZ

formulation moves tie-breaking definition best supporters. find
convenient, purposes here, make tie-breaking explicit part equations (i. e., Equation 8 relaxed-plan equations below).
Towards generalization arbitrary C, first change Keyder Geffners equation account positive side effects, extent supporting, action,
open subgoals action best supporter. Reformulating Equation 8
end, obtain FF := (G), function fact sets G satisfies:

GI

0
(( G \ G ) pre( a)) { a} A,
(9)
(G) =

6= G 0 = { p G | p add( a), h1 (pre( a)) = h1 ({ p}) 1} else
Compared Equation 8, need recurse single facts sets facts,
recursive call knows open facts select entire best-supported
subset thereof.6
Equation 9 correspondence typical relaxed planning graph based implementations, depicted Algorithm 1. definition G 0 equation corresponds
maintenance TRUE flags facts relaxed planning graph layers, upon
selecting action layer add effects marked TRUE (to see this, observe that, h1 (pre( a)) = 1, h1 (pre( a)) = h1 ({ p}) 1 iff h1 ({ p}) = i).
extend Algorithm 1 relaxed plan extraction hC below. Note Algorithm 1
deviates bit common descriptions, explicitly including computation h1
instead assuming input relaxed planning graph caching outcome computation. simplify notation tie easily extension below.
step Equation 8 Equation 9 benign standard setting, sense
practical impact expected small: single action typically add
many open facts, i. e., support many open atomic subgoals. Yet step
paramount importance generalization atomic subgoals arbitrary conjunctions
C. general setting, atomic subgoals typically overlap, supporting subgoal
means add part it, may well case many subgoals.
finally need formulate delete relaxation, terms relaxing regression semantics, terms splitting subgoals singleton facts, considering
correct regression semantics separately respect singleton-fact
subgoals. words, need use formulation underlying h+ Equation 5.
reminder convenience, equation is: h( G ) =

0
GI

1 + minaA,6=G0 ={ p| pG,R({ p},a)6=} h(( G \ G 0 ) pG0 R({ p}, a)) else
similar transformation step Equation 9, obtain FF := (G),
function fact sets G satisfies ( G ) =

GI

0
0
(( G \ G ) Gr ) { a} A,
(10)

6= G 0 = { p | p G, R({ p}, a) 6= , h1 ( R({ p}, a)) = h1 ({ p}) 1} else
6. Note selection dynamic function open facts, opposed up-front design
best-supporter function sharing supporting actions much possible. important
standard setting here. Yet, discuss detail below, become important using arbitrary
conjunctions C atomic subgoals.

286

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

Algorithm 1: Relaxed plan extraction h1 .
1
2
3
4
5
6
7
8
9
10

11
12

compute h1 ({ p}) p F
:= max pG h1 ({ p})
=
return
:= m, . . . , 1
Gi := { p | p G , h1 ({ p}) = }
:=
:= m, . . . , 1
ex. p Gi s.t. p TRUE
select 6= { p Gi | p TRUE } add( a),
h1 (pre( a)) = 1
foreach p Gi add( a)
mark p TRUE

14

foreach q pre( a)
Gh1 ({q}) := Gh1 ({q}) {q}

15

:= { a}

13

16

return

Gr0 defined Gr0 := pG0 R({ p}, a).
Relative Equation 9, simple reformulation, using regression notation

trivializes singleton conjunctions. particular, subgoal Gr0 = pG0 R({ p}, a) generated action simplifies pre( a) here. Relative Equation 5, instead heuristic
value, compute relaxed plan. Instead minimizing action choices
corresponds h+ , impose use best supporters corresponds relaxed
plan extraction h1 .


4.2 Relaxed Plan Extraction hC
Equation 10, obtain CFF similar generalizations made get h+
hC+ . Extending hC sets G conjunctions hC ( G ) := maxcG hC (c), definition
reads:
Definition 3 Let = (F , A, , G) planning task, C set conjunctions containing singleton conjunctions. hC -based critical-path delete-relaxed plan, short C-relaxed
plan, set CFF action occurrences ( a, G 0 ) CFF = (G C ), partial
function conjunction sets G defined G C satisfies ( G ) =


c G : c



C
0
0
0
(( G \ G ) Gr ) {( a, G )} A,
(11)

6= G 0 {c | c G, R(c, a) 6= , hC ( R(c, a)) = hC (c) 1},


hC ( Gr0 ) = hC ( G 0 ) 1
else
287

fiF ICKERT & H OFFMANN & TEINMETZ

Gr0 defined Gr0 := cG0 R(c, a).
hC -based cross-context critical-path delete-relaxed plan, short nc-C-relaxed plan,
CFF action occurrences property, except define G 0 : = { R ( c, ) |
set nc
r
0
c G }.


definition parallels definition hC+ (Definition 2). subgoaling structure
same, sets conjunctions C must achieved
regression. Instead heuristic value, compute relaxed plan (consisting action
occurrences, action plus supported subgoals G 0 hC+ C , opposed actions
standard case). Instead minimizing action occurrence choices, impose
use best supporters according hC . major new source complexity, relative
Equation 10, allow G 0 subset best-supported atomic subgoals,
similarly Definition 2. relaxed plan always exist, allow
partial define CFF defined goal. show below, possible
iff hC < , i. e., C-relaxed plan exists iff relaxed plan C exists.
Observe that, relative Equation 10, added new additional condition

hC ( Gr0 ) = hC ( G 0 ) 1. understand condition, consider (a) Gr0 = cG0 R(c, a)
union regressions individual supported subgoal c G 0 , (b)
every subgoal c G 0 hC ( R(c, a)) = hC (c) 1, i. e., action selected
best supporter c. (b), one would surmise hC ( Gr0 ) = hC ( G 0 ) 1,
regressions R(c, a) Gr0 one step easier solve original counterparts
c G. so, however, cross-context conditions: otherwise,
union (a) may difficult achieve components. get back
detail Section 4.3. now, keep mind additional condition
hC ( Gr0 ) = hC ( G 0 ) 1 required due possible cross-context conditions. (We remark
condition equivalent hC ( Gr0 ) < hC ( G 0 ), hC value cannot decrease
1 single regression step; written hC ( Gr0 ) = hC ( G 0 ) 1 merely use
specific write-up.)
instructive consider CFF procedural perspective. Algorithm 2 provides
corresponding extension Algorithm 1. previously computed h1 facts,
compute hC conjunctions C. previously subgoal sets Gi
sets facts, sets conjunctions C. previously new subgoals
generated selected actions precondition facts, atomic conjunctions contained regressed subgoal Gr0 . Note pointwise interpretation ncC-relaxed plans, Gr0 = { R(c, a) | c G 0 } set fact sets. line 10, select
action occurrence ( a, G 0 ) instead action a, resulting additional choice
supported atomic subgoals G 0 , accordingly complicated structure regressed subgoal Gr0 . Note that, every c Gi , hC (c) = and, action
a0 , hC ( R(c, a0 )) hC (c) 1 = 1. Furthermore, hC ( Gr0 ) = 1 hC ( R(c, a)) 1
every c G 0 . Putting observations together, get hC ( R(c, a)) = hC (c) 1
every c G 0 , choice G 0 Algorithm 2 equivalent Equation 11.
Example 6 Consider, Example 5 (page 281), car-driving example C containing
singleton conjunctions well c = carY fuel. hC ({carX }) = 0, hC ({fuel}) = 0,
hC ({carY }) = 1, hC ({carY, fuel}) = 2, hC ({carZ }) = 3.
288

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

Algorithm 2: Relaxed plan extraction hC . C-relaxed plan, use Gr0 =

0
0
c G 0 R ( c, ); nc-C-relaxed plan, use Gr = { R ( c, ) | c G }.
1
2
3
4
5
6
7
8
9
10

11
12

compute hC (c) c C
:= maxcC,cG hC (c)
=
return
:= m, . . . , 1
Gi := {c | c C, c G , hC (c) = }
:=
:= m, . . . , 1
ex. c Gi s.t. c TRUE
select ( a, G 0 ) A, 6= G 0 {c Gi | R(c, a) 6= , c TRUE },
hC ( Gr0 ) = 1
foreach c0 G 0
mark c0 TRUE

14

foreach c0 C s.t. ex. c Gr0 c0 c
G h C ( c 0 ) : = Gh C ( c 0 ) { c 0 }

15

:= {( a, G 0 )}

13

16

return

Tracing Equation 11 initializing call CFF = ({carZ }), get exact
recursive development Example 5. First, carZ supported ( aYZ , {carZ }),
hC ({carY, fuel}) = 2 = 3 1 = hC ({carZ }) 1 required supported conjunction
c = carZ. Similarly, hC ( Gr0 ) = hC ( G 0 ) 1 Gr0 = {carY, fuel} G 0 = {carZ } single
supported conjunctions difference. recursive subgoal {carY, fuel, carY fuel}.
supporting action carY fuel arefuel . action best supporter carY fuel
hC ({carY }) = 1 = hC ({carY, fuel}) 1. action best supporter fuel though,
fuel true initially hC ({fuel}) = 0. possible choice supporting carY fuel
arefuel G 0 := {carY fuel}. get Gr0 = {carY } hC ( Gr0 ) = 1 = hC ( G 0 ) 1
desired. recursive subgoal {fuel, carY }, supported ( XY , {carY }) yielding Gr0 = {carX }
hC ( Gr0 ) = 0 = hC ( G 0 ) 1.
Taking procedural perspective Algorithm 2, start inserting carZ G3 . layer
= 3 support ( aYZ , {carZ }) Gr0 = {carY, fuel} hC ( Gr0 ) = 2 = 1.
Similarly, layers 2 1 mirror exactly respective recursive invocations Equation 11.
next prove C-relaxed plans nc-C-relaxed plans indeed correspond
C . start C-relaxed plans:
relaxed plans C respectively nc
Theorem 4 Let = (F , A, , G) planning task, C set conjunctions containing
singleton conjunctions. C-relaxed plan CFF sequentialized form relaxed
plan C .
289

fiF ICKERT & H OFFMANN & TEINMETZ

Proof Sketch: Sequencing CFF h( a0 , G00 ), . . . , ( an1 , Gn0 1 )i inverse order action
occurrence selection Equation 11, i. e., placing outcome recursive invocations
front, h a0 [ G00 ], . . . , an1 [ Gn0 1 ]i relaxed plan C . easy show induction
length sequence. G0 , . . . , Gn recursive subgoals generated
Equation 11, si state applying ai [ Gi0 ] C , holds {c | c Gi }
si . obvious = 0. holds i, also holds + 1 (a) Gi+1 \ Gi0
part Gi+1 also part Gi hence true induction hypothesis; (b) Gi0 part
Gi+1 made true ai [ Gi0 ], applicable si induction hypothesis
precondition conjunctions contained Gi .
almost identical proof shows corresponding property nc-C-relaxed plans:
Theorem 5 Let = (F , A, , G) planning task, C set conjunctions containing
CFF sequentialized form relaxed
singleton conjunctions. nc-C-relaxed plan nc
C.
plan nc
C exists C-relaxed plan
Finally, relaxed plan C respectively nc
respectively nc-C-relaxed plan exists. simply properties
fully determined critical-path component. proof shows via deriving
intermediate equation, Equation 12 below, characterizes behavior CFF
CFF restricting choice supported subgoal sets G 0 singletons. Equation 12
nc
play important role comparison related work, experiments.

Theorem 6 Let = (F , A, , G) planning task, C set conjunctions containing
singleton conjunctions. C-relaxed plan exists nc-C-relaxed plan exists
hC < .
Proof Sketch: show claim two parts, (a) C-relaxed plan exists
hC < , (b) nc-C-relaxed plan exists hC < .
directions follow corollaries (a) Theorems 3 4 respectively (b) Theorems 3 5:
hC = , neither C-relaxed plan nc-C-relaxed plan exist, otherwise
C would exist Theorem 4 respectively Theorem 5,
relaxed plan C respectively nc
C ) per Theorem 3.
contradiction hC = = h1 (C ) = h1 (nc
CFF restricting choice
directions, consider versions CFF nc
supported subgoal sets G 0 singletons, i. e., single conjunctions G 0 = {c}. CFF
CFF simplifies
nc
cG C ( c ), (.) partial function conjunctions
c satisfies


cI

0 ) {( a, { c })} A,

(
c
(c) =
(12)
0
C
c R(c,a)
C
C
R(c, a) 6= , h ( R(c, a)) = h (c) 1 else
Note similarity Equation 8 (page 285): back common notation
relaxed plan extraction (over C instead singleton facts), extracting best supporters
one-by-one.
changing subgoaling structure, one transform Equation 12 form
(G) partial function fact sets G satisfies
290

fiC OMBINING h+ hm : IRECT C HARACTERIZATION



GI



( R( G, a)) {( a, G )} A,
(G) =
R( G, a) 6= , hC ( R( G, a)) = hC ( G ) 1 G C



0
else
G 0 G,G 0 C ( G )

(13)

Comparing hC equation (Equation 3, page 278), clear subgoaling
structure two equations coincides subgoals c hC (c) < , particular,
hC < Equation 13 solution defined G . Therefore, Equation 12
solution defined c G C . Equation 12 captures restricted version CFF
CFF , C-relaxed nc-C-relaxed plans exist desired.
nc

4.3 Subgoal-Support Selection Problem
far shown C-relaxed plans nc-C-relaxed plans extracted.
CFF defined. Given
yet explained actual heuristic functions hCFF hnc
CFF
CFF
C
Theorem 6, h
hnc return case h = . case hC < ,
description relaxed plan extraction far specify choose supported
CFF .
subgoal sets G 0 . Taking choice particular ways yields functions hCFF hnc
choice non-trivial number possible action occurrences worst-case
exponential |C |. refer choice subgoal-support selection problem.
start discussing optimization objective problem. fix soluCFF CFF order, defining desired heuristics hCFF hCFF
tions, nc
nc
corresponding specializations Equation 11. close section brief discussion
prior work light findings.
4.3.1 PTIMIZATION BJECTIVE
Assume hC < . argued proof Theorem 6, know Equation 12
solution, principle could restrict | G 0 | = 1, resulting |A| |C |
different action occurrence choices. However, result dramatic overestimation:
Example 7 Consider task = (F , A, , G) F = { g1 , . . . , gn }, = , G =
{ g1 , . . . , gn } contains single action whose precondition delete list empty
whose add list { g1 , . . . , gn }. Obviously, h = h+ = hFF = 1. However, even C containing singleton conjunctions { gi }, Equation 12 results dramatic overestimation:
C-relaxed plan collect separate occurrence ( a, { gi }) every gi , resulting relaxed plan
length n. C also contains fact-pair conjunctions { gi , g j } get C-relaxed plan size
n(n1)

n+
. general, get C-relaxed plan size |C |.
2
extreme example, similar situations arise whenever conjunctions overlap, action adding single fact p possible supporter conjunctions contain
p. With, e. g., conjunctions containing fact pairs, means number top-level
goal conjunctions supported least |G|. domains many top-level goal facts including current IPC benchmarks and, generally, e. g. typical transportation, construction,
puzzle problems clearly detrimental. (Replacing G single fact new goal-achiever
action moves problem precondition action.)
291

fiF ICKERT & H OFFMANN & TEINMETZ

essentially observation made Haslum (2009, 2012), non-admissibility
h+ (m ) every conjunction must achieved separately, prompted design
C every action may achieve arbitrary subsets conjunctions. new
particular context consider issue, namely choice G 0 CFF
CFF per Equation 11: moved issue generic planning-task level
nc
specific subgoal-support selection level. specific perspective identifies
precise source complexity, far relaxed plan extraction concerned: choose
sets G 0 Equation 11 equivalently, implement line 10 Algorithm 2 manner
avoiding overestimation extent possible?
intuitive answer question, given Example 7, certainly choose G 0 large
possible. intuition entirely correct. detail Example 9 (Appendix A),
cases supporting conjunction c, even though feasible, better done
later recursion, action whose precondition easier combine c.
Nevertheless, employ | G 0 | maximization here, deeming safe presume overlapping conjunctions per Example 7 much practically relevant contrived
situations per Example 9.
convenient introduce terminology feasible choices G 0 . per
Equation 11, possible choices G 0 best supporter every
c G 0 , i. e., hC ( R(c, a)) = hC (c) 1, overall regressed subgoal feasible,

hC ( Gr0 ) = hC ( G 0 ) 1. case, say CFF context, i. e., Gr0 = cG0 R(c, a),
CFF context, i. e., G 0 = { R ( c, ) | c G 0 }, G 0
G 0 C-feasible. say nc
r
nc-C-feasible.
maximization problems are:
Definition 4 C-SubgoalSupport denote following problem:
Given planning task , set conjunctions C containing singleton conjunctions,
G C, action , K N. exist G 0 {c G | R(c, a) 6= , h1 ( R(c, a)) =
h1 (c) 1} G 0 C-feasible | G 0 | K?
define nc-C-SubgoalSupport accordingly nc-C-feasible G 0 .
CFF H EURISTIC
4.3.2 hnc
CFF , i. e., nc-C-SubgoalSupport, easy
subgoal-support selection problem nc
0
solve. Indeed, choice G nc-C-feasible:

Proposition 2 Let planning task, C set conjunctions containing singleton
conjunctions, G C, action . G 0 {c G | R(c, a) 6= , h1 ( R(c, a)) =
h1 (c) 1} nc-C-feasible.
Proof: definition, G 0 nc-C-feasible hC ( Gr0 ) = hC ({ R(c, a) | c G 0 }) = hC ( G 0 )
1. Now, hC ({ R(c, a) | c G 0 }) = maxcG0 hC ( R(c, a)) construction equals
maxcG0 (hC (c) 1). latter equals (maxcG0 hC (c)) 1 = hC ( G 0 ) 1 desired.
CFF , additional condition hC ( G 0 ) = hC ( G 0 ) 1 Definition 3
words, nc
r
redundant. maximize | G 0 |, simply include G 0 c hC ( R(c, a)) =
CFF as:
hC ( G 0 ) 1. Accordingly, define heuristic function hnc

292

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

Definition 5 Let = (F , A, , G) planning task, C set conjunctions conCFF =
taining singleton conjunctions. nc-C-relaxed plan heuristic defined hnc
CFF = | CFF | CFF = (G C ) satisfies ( G ) =
hC = , otherwise hnc
nc
nc

c G : c

C
0
0
0
(14)
(( G \ G ) Gr ) {( a, G )} A,

0
C
C
6= G = {c | c G, R(c, a) 6= , h ( R(c, a)) = h (c) 1} else
Gr0 defined Gr0 := { R(c, a) | c G 0 }.
words, restrict Equation 11 maximal choice G 0 middle case, using
G 0 = {c G | R(c, a) 6= , hC ( R(c, a)) = hC (c) 1} instead G 0 {c G | R(c, a) 6=
C variant regressed subgoal G 0 ,
, hC ( R(c, a)) = hC (c) 1}. use nc
r
C
0
C
0
drop condition h ( Gr ) = h ( G ) 1 redundant variant.
4.3.3 hCFF H EURISTIC
Matters simple CFF , i. e., C-SubgoalSupport, requires C-feasible sets
CFF setting trivial
G 0 opposed nc-C-feasible ones. feasible choice G 0 nc
CFF ignores cross-context conditions. ignoring conditions,
(Proposition 2) nc
CFF
, longer true:
Example 8 Consider, Example 5 (page 281), abstract example conjunctions cq1p =
q1 p, cq2p = q2 p, cq1q2 = q1 q2 . supporting goal facts, get
subgoal G = {q1 , q2 , p, q1 p, q2 p}. Ignore, like Example 5, subsumed subgoals q1 , q2 , p
tackled side effect tackling non-subsumed ones q1 p q2 p.
possible supporting action latter subgoals p adds p (q1 true initially,
action adding q2 deletes p cannot support q2 p). three possible choices G 0 :
0 : = { q p, q p }, G 0 : = { q p }, G 0 : = { q p }.
G12
2
2
1
1
2
1

G10 , Gr0 = cG10 R(c, a) = {q1 } hC ({q1 }) = 0 = hC ({q1 , p}) 1. G10 CS
feasible. G20 , Gr0 = cG20 R(c, a) = {q2 } hC ({q2 }) = 1 = hC ({q2 , p}) 1. G20
0 C-feasible, G 0 =
0 R ( c, ) = { q1 , q2 },
C-feasible well. However, G12
c G12
r
corresponding atomic conjunction q1 q2 . Selecting atomic subgoals q1 p q2 p,
even though feasible individually, incurs cross-context condition q1 q2 , atomic
conjunction present regression either q1 p q2 p individually. example
0 )1 =
constructed hC ({q1 , q2 }) = , hence particular hC ({q1 , q2 }) = 6= hC ( G12
C
h ({{q1 , p}, {q2 , p}}) 1 = 1.
0 CFF , get G 0 = { R ( c, ) | c G 0 } = {{ q }, { q }} instead.
Note that, G12
2
1
nc
r
12
words, get set containing two small conjunctions q1 q2 , instead set containing
one big conjunction q1 q2 . hC ({{q1 }, {q2 }}) = 1 = hC ({{q1 , p}, {q2 , p}}) 1,
0 (not C-feasible but) nc-C-feasible.
G12
example shows, cross-context conditions may render particular combinations supported conjunctions G 0 infeasible. used formulation, come
surprise maximizing | G 0 | avoiding combinations computationally hard:
Theorem 7 C-SubgoalSupport NP-complete.
293

fiF ICKERT & H OFFMANN & TEINMETZ

Proof Sketch: Membership guess check. Hardness via reduction Hitting Set:
Given set elements E collection subsets b E elements, construction
that, particular point C-relaxed plan extraction, choosing G 0 amounts
choosing E0 E, E0 C-feasible (results hC value 6= ) iff exists b
b E0 . Given this, E0 \ E hitting set, maximizing | E0 | equivalent finding
minimum-size set.
hard find cardinality-maximal feasible set supported conjunctions
CFF . Presuming want invest effort solve problem exactly
(many times extraction C-relaxed plan every search state), need
approximate solution. canonical choice approximating cardinality-maximality
subset-maximality. say G 0 {c G | R(c, a) 6= , h1 ( R(c, a)) = h1 (c) 1}
subset-maximally C-feasible G 0 C-feasible and, every G 00 G 0 ( G 00 {c
G | R(c, a) 6= , h1 ( R(c, a)) = h1 (c) 1}, G 00 C-feasible. heuristic function hCFF
defined using corresponding restriction Equation 11:
Definition 6 Let = (F , A, , G) planning task, C set conjunctions containing singleton conjunctions. C-relaxed plan heuristic defined hCFF = hC = ,
otherwise hCFF = | CFF | CFF = (G C ) satisfies ( G ) =


c G : c



C
0
0
0
(( G \ G ) Gr ) {( a, G )} A,
(15)

6= G 0 {c | c G, R(c, a) 6= , hC ( R(c, a)) = hC (c) 1},


G subset-maximally C-feasible
else
Gr0 defined Gr0 :=



c G0

R(c, a).

subset-maximally C-feasible set G 0 found simple greedy algorithms,
adding conjunctions one-by-one shown Algorithm 3. candidate conjunctions
c G R(c, a) 6= hC ( R(c, a)) = hC (c) 1. Starting empty G 0 ,
try candidate c exactly once. suffices get subset-maximal G 0 because, G 0
grow, adding c feasible first time around adding c cannot
feasible later either.
Algorithm 3: Greedy selection subset-maximally C-feasible set supported subgoals G 0 C-relaxed plan extraction. Implements line 10 Algorithm 2 obtain
heuristic function hCFF .
1 select c Gi , c TRUE
2 select R ( c, ) 6 = hC ( R ( c, )) = hC ( c ) 1
3 G0 := {c}
4 foreach c0 Gi s.t. c0 TRUE i, R ( c0 , ) 6 = , hC ( R ( c0 , )) = hC ( c0 ) 1
5
G 0 {c0 } C-feasible
6
G 0 := G 0 {c0 }

remark that, Example 9 (Appendix A) shows, cases selecting
non-subset-maximally C-feasible G 0 leads strictly smaller C-relaxed plan.
words, like cardinality maximization, subset-maximization fail-safe.
294

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

4.3.4 P REVIOUS W ORKS R ELATED UBGOAL -S UPPORT ELECTION P ROBLEM
Interestingly, subgoal-support selection problem never previously identified, already solved. plainly put, previous works area
viewed solving problem abstract level, identifying precisely
problem is, thus ending solutions solve problem, using unnecessarily drastic measures. Mostly due compilation view, relaxed
plan extraction becomes standard technique, yet subgoal-support selection problem
solved STRIPS level, form compiled task. single noncompilation-view prior work, Alcazar et al. (2013), conducted part much
broader scope, address subgoal-support selection problem detail.
Let us closer look Alcazar et al.s heuristic, FFm (which implement
= 2). extracts relaxed plan hm , restricting C contain exactly conjunctions size m. restriction easily removed. notation, FFm corresponds

C-relaxed plan extracted using equation: FFm = cG C (c)


cI

0
(c) =
(16)
0
C ( c ) { } A,
c R(c,a)
R(c, a) 6= , hC ( R(c, a)) = hC (c) 1 else
CFF simplify reThis almost exactly definitions CFF nc
0
stricting choice G support single conjunction G 0 = {c}, i. e., almost
identical Equation 12 derived proof Theorem 6. Repeating Equation 12
CFF =
convenience: CFF = nc
cG C ( c )


cI

0 ) {( a, { c })} A,

(
c
(c) =
0
C
c R(c,a)
R(c, a) 6= , hC ( R(c, a)) = hC (c) 1 else

difference two equations FFm collects set actions
CFF collect set (single-supported-subgoal) action
standard setting, CFF nc
occurrences.
sense, Alcazar et al.s approach over-simplifies choice G 0 , singleton sets.
C rather C because, | G 0 | = 1, cross-context conditions
effectively tackles nc
never occur. would furthermore run risk excessive overestimation pointed
Example 7 actually collect action occurrences, rather actions. latter
might viewed trick avoid overestimation, yet theoretical perspective
rather defeats purpose using explicit conjunctions first place. Whereas relaxed
planning C converges h , FFm bounded number actions, |A|.
Altogether, findings allow understand prior work subject follows:
0

C Compilation (Haslum, 2012): Includes one compiled action aG every possible
pair action possible set supported subgoals G 0 . sense, solves
NP-complete problem C-SubgoalSupport enumeratively, in-memory.7
Lesson learned hCFF : need pre-generate possible conjunction subsets
action could support. focus subgoals actually arise relaxed plan
extraction.
0

7. Plus, without actually giving optimality guarantee: optimal aG set choices
relaxed plan extraction/the best-supporter function, guarantee selected.

295

fiF ICKERT & H OFFMANN & TEINMETZ

C Compilation (Keyder et al., 2012, 2014): Includes one conditional effect every
ce
pair action possibly supported conjunction c. ignores cross-context
conditions hence trivializes C-SubgoalSupport nc-C-SubgoalSupport.
Lesson learned hCFF : need ignore cross-context conditions completely.
greedily select supported conjunctions whose cross-context conditions feasible.

FFm (Alcazar et al., 2013): Restricts conjunction set C size- conjunctions
hm . Restricts supported subgoals G 0 single conjunctions, thus trivializing
C compilation.
C-SubgoalSupport ignoring cross-context conditions like ce
Collects actions instead action occurrences, losing convergence h .
Lesson learned hCFF : weaknesses avoided.

5. Experiments
CFF heuristic functions relative closely
evaluate benefits hCFF hnc
related previous heuristics. state key issues consider terms four
hypotheses, formulating major expectations regarding algorithm behavior IPC benchmarks, standard means evaluation planning community:8

(H1) hCFF relative hFF (C ), hypothesis (H1) avoiding exponential blowup |C | typically yields faster heuristic thus improved performance.
C ), hypothesis (H2) accounting cross-context
(H2) hCFF relative hFF (ce
conditions yield informed heuristic thus improved performance.
difference typically (H1) vs. (H2) intended. Crosscontext conditions presumably important particular cases, whereas
advantage hCFF smaller representation presumably helps cases.
CFF relative hFF ( C ), hypothesis (H3) implemen(H3) hCFF hnc
ce
CFF typically effective thus yields improved performance.
tation hCFF hnc
CFF direct, using compilation, thus
expect hCFF hnc
FF
C ).
specialized h (ce
CFF hFF ( C ) equivalent except implementation,
Note hnc
ce
use information scaling behavior |C |. (In
contrast comparison hCFF vs. hFF (C ), dominated (H1)
drastically different scaling behavior |C |.)

(H4) furthermore compare hCFF variant denote hCFF
, per Equation 12
| G 0 |=1
0
restrict | G | = 1, hypothesis (H4) non-trivial subgoal
support selection hCFF typically yields informed heuristic thus improved performance.
finally include variant denote hCFF
, per Equation 16 | G 0 | = 1
| G 0 |=1A
set actions (as opposed action occurrences) selected. serves comparison
Alcazar et al.s (2013) work.
8. hypotheses intended formal statements statistically accept reject;
intended exhaustive representation issues discuss. merely serve red
thread discussion large-scale experiments.

296

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

Section 5.1 describes key points implementation, Section 5.2 explains
experimental setup. Section 5.3 provides comprehensive results, across planner variants,
small conjunction sets C turn best terms overall performance.
Section 5.4 analyzes behavior function growing C.
5.1 Implementation
CFF FD (Helmert, 2006). furthermore implemented
implemented hC , hCFF , hnc
Cadd
C-additive heuristic h
, defined exactly like hC (Equation 3) except maximization atomic subgoals replaced summation subgoals:

GI
0
1 + minaA,R(G,a)6= h( R( G, a)) G C
(17)
h( G ) =

0
else
G0 G,G0 C h( G )

words, step hC hCadd parallels hmax hadd (Bonet & Geffner,
2001). dont use hCadd heuristic function per se: contrast standard setting,
atomic subgoals overlap general case, summation doesnt make sense.
use hCadd alternative best-supporter function relaxed plan extraction.
purpose, turns fairly useful empirically.
computing critical-path heuristics becomes expensive many conjunctions,
key practicality efficient implementation hC . end, extend counterbased algorithm originally implemented FF (Hoffmann & Nebel, 2001) computing h1
(aka relaxed planning graph). extended algorithm easily described modification original algorithm. Assume input state s. FFs original algorithm associates
precondition action counter, denoted count( a), initialized
|pre( a)|. Facts p maintained priority queue ordered associated v( p) value,
equals h1 ( p) p dequeued. queue initialized facts p
true s, associated value v( p) = 0. main loop dequeues facts, activates
new actions, maintains v values. fact p dequeued, loop actions
p pre( a) decrements count( a). results count( a) = 0 action activated, enqueuing every q add( a) value v(q) = 1 + max p0 pre(a) v( p0 ), reducing
v(q) value case q already queue higher value.9 algorithm
stops either goal facts dequeued h1 (s) = max pG v( p), queue
become empty h1 (s) = .
extension hC works much way. need maintain values
v(.) conjunctions c C instead single facts, need maintain counters pairs
action supported conjunction instead actions. Precisely, create counter
count(c, a) every c C R(c, a) 6= R(c, a) contain
mutex, i. e., fact pair known unreachable. latter corresponds mutex pruning
C . reduces computational effort well
discussed Keyder et al. (2014) C ce
strengthens heuristic.
extended algorithm, counter count(c, a) initialized |{c0 | c0 C, c0
R(c, a)}|, i. e., number sub-conjunctions need make true order
able achieve c using (remember C contains singleton conjunctions).
9. general action costs c( a), one simply use v(q) = c( a) + max p0 pre( a) v( p0 ) here.

297

fiF ICKERT & H OFFMANN & TEINMETZ

queue contains conjunctions c0 C instead facts. initialized conjunctions c0 s, v(c0 ) = 0. Dequeueing conjunction c0 , loop counters
count(c, a) c0 R(c, a), decrementing count(c, a). results count(c, a) = 0,
enqueue/upgrade c value 1 + maxc0 C,c0 R(c,a) v(c0 ).
compute hCadd , use exact algorithm except maximization replaced summation, i. e., instead 1 + maxc0 C,c0 R(c,a) v(c0 ) use 1 + c0 C,c0 R(c,a) v(c0 ).
Based conjunction values v(c) computed hC respectively hCadd , impleCFF follows Algorithm 2 (page 288). particular, select
mentation hCFF hnc
CFF fix G 0 = { c | c
action/supported subgoals ( a, G 0 ) previously discussed. hnc
CFF
0
G, R(c, a) 6= , v( R(c, a)) = v(c) 1}. h , select G per Algorithm 3 (page 294).
Here, v(c) values single conjunctions c readily available. Values v( X ) fact
set X required X := R(c, a), well check C-feasibility hCFF

(Algorithm 3 line 5), X := c00 G0 {c0 } c00 G 0 current set supported
subgoals c0 candidate inclusion set. compute v( X ) loop
facts p X, using lists C [ p] containing c C p c, maximizing
respectively summing v(c) c C [ p] c X.
Helpful actions (Hoffmann & Nebel, 2001), i. e., FD preferred operators, defined
similarly hFF . action applicable state preferred C-relaxed plan
contains a, i. e., action/supported subgoals pair form ( a, G 0 ). corresponds
C ), based compiled actions
selection preferred operators hFF (C ) hFF (ce
a[C 0 ] occurring relaxed plans respective compilations (Keyder et al., 2014).
performance satisficing search planning known brittle respect
minor differences heuristic functions (e. g. Valenzano, Sturtevant, Schaeffer, & Xie,
2014). important also setting. unavoidable implementation differences
new heuristics predecessors turn major complication
fair comparison. heuristics extract relaxed plans hC hCadd best-supporter
C ) via compilation, hCFF hCFF not.
function, yet hFF (C ) hFF (ce
nc
relaxed plan extraction algorithms work different representations. particular,
choice action hFF (C ), i. e., compiled action a[C 0 ], corresponds
choice action/supported subgoals pair ( a, G 0 ) hCFF . design, hFF (C ) cannot
distinguish choosing vs. choosing G 0 . contrast, design hCFF chooses first
action assembles G 0 greedy C-feasible maximization.
offset unavoidable differences relaxed plan extraction, experiment across
C , tiea variety tie-breaking strategies choice best supporters. C ce
CFF applies actions supportbreaking applies compiled actions a[C 0 ], hCFF hnc
C
C
ing conjunction c h ( R(c, a)) = h (c) 1 respectively hadd ( R(c, a)) =
hadd (c) 1. strategies are:
Arbitrary: Choose arbitrary best supporter, i. e., first one find. used (with
C ).
hadd best supporters) FDs implementation hFF , well hFF (C ) hFF (ce
Random: Choose random best supporter. use 3 different random seeds experiments gauge performance variance incurred criterion. turns
that, almost cases, variance small performance change relative
arbitrary tie-breaking consistent across random seeds, i. e., consistently positive
298

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

consistently negative. Thus random tie-breaking exhibits reliable behavior
algorithm option.
Difficulty (hC only): tie breaking mechanism used FF (Hoffmann & Nebel,
2001). selects best supporter minimizes summed-up hmax values
CFF , translates summing hC
supporters preconditions. hCFF hnc
values conjunctions contained regressed subgoals R(c, a). Remaining
ties broken arbitrarily.
use 3 tie-breaking strategies hC , first 2 strategies hCadd ,
CFF , hFF ( C ), hFF ( C ), hCFF , hCFF
.
6 heuristics hCFF , hnc
ce
| G 0 |=1
| G 0 |=1A
5.2 Experiments Setup Design
able compare different heuristic functions directly, comparisons
use conjunction set C every heuristic. find sets C using exact
methods, implementation, used Keyder et al. (2014). motivation
contribution pertain methods finding C, re-using established methods provides better comparability. understand experiments,
necessary understand generation C detail, give brief summary only.
Keyder et al.s (2014) method variant method proposed earlier Haslum
(2012). pre-process actual search, C learned iteratively refining deleterelaxed plan initial state. Starting empty C, relaxed plan + C generated. + real plan (a plan original input task), process stops. Else,
analysis step finds set C 0 new conjunctions exclude + , i. e., +
longer relaxed plan C setting C := C C 0 . process iterates. Running
ad infinitum, one eventually find plan input task. typically
feasible. find instead set C heuristic search, algorithm applies both, time
limit T, size limit x C relative action set size increase, i. e., |AC |/|A|. either
two criteria applies, process stops hands current set C search.
(Each limit may set , meaning termination criterion disabled.)
heuristics experiments use explicit conjunctions, use
set C, separate generation C actual experiments. apply separate
runtime limits C-generation search respectively, report
performance search C-learning. Given this, merely serves means
keep experiments feasible even large size limits x. fix 30 minutes.
Throughout, use FDs lazy-greedy best-first search dual open queue preferred operators (Helmert, 2006), profits search space pruning afforded
preferred operators, yet preserves completeness keeping pruned nodes
second open queue. canonical search algorithm satisficing planning
delete-relaxation heuristics, widely used baseline yields competitive performance
reasonably simple. (Textbook single-queue greedy best-first search lags far behind state art, either use preferred operators, loses solutions
cases preferred operators restrictive.) experiments run
cluster machines Intel Xeon E5-2660 processors running 2.2 GHz. memory limit set 4 GB. used benchmarks satisficing tracks two
recent International Planning Competitions, IPC11 IPC14. include
299

fiF ICKERT & H OFFMANN & TEINMETZ

400000

104

hFF (C )
hCFF

370
360

103

300000

350
340
330

102

320

200000

310

101

300
290

100000

280

100

270
260

01
2

22

23

24

25

26

27

28

29

210

101 1
10

100

(a)

101

(b)
hCFF

hFF (C ).

102

103

104

21

hFF (hadd arb)
hFF (hadd rnd)
22

23

24

25

26

27

28

29

210

(c)
hCFF

Figure 1: Data preview
vs.
(a) Number counters
vs. number
C
C
actions |A | , function size limit x. (b) States per second
x = , x-axis hCFF , y-axis hFF (C ). (c) Total coverage function x. (b)
(c), hCFF hFF (C ) run hCadd using random tie-breaking.
(c), also include hFF baseline, two tie-breaking variants: hadd
arbitrary tie-breaking corresponds FD default, hadd random tiebreaking better performance benchmarks. Recall comparison
effort C-learning included hCFF hFF (C ) (see text).
IPC14 CityCar domain, FD translator generates actions conditional
effects, supported implementation. domain, test suite 20 instances (some domains used IPC11 IPC14 two test suites).
6 heuristic functions, 5 best-supporter definitions (hC vs. hCadd , tie breaking),
numeric size-limit parameter x, experiments space large. motivate
organize exploration space follows, Figure 1 gives data preview.
major benefit hCFF hFF (C ) better scaling |C |. One would expect manifest itself, large C, (a) smaller representation thus (b) faster
heuristic function. Figure 1 (a) (b) confirm indeed so.10 x = ,
C contains conjunctions learned within 30 minutes, get speed-ups 14 orders
magnitude. Now, good news, turns cases large C detrimental. search space size generally decrease increasing size limit
x, heuristic functions also become slower. slowdown dramatic hFF (C ).
much less dramatic hCFF , still typically enough outweigh search space
reduction. Figure 1 (c) shows effect: overall coverage becomes worse growing
x, dramatically hFF (C ), benign manner still almost monotonically
hCFF . best overall coverage often (across heuristics, configurations, domains)
obtained x = 2, also best setting x Keyder et al.s (2014) experiments.
hFF baselines Figure 1 (c) based hCFF using singleton conjunctions (x = 1), better comparability methods, 5 tiebreaking strategies disposal. comparison baselines, hCFF consistently out10. Figure 1 (a), x = 2 number counters hCFF exceeds |AC |. cannot happen theory,
per Definition 1, C includes action a[{c}] every counter count(c, a). happen
practice due handling facts, i. e., actions original pre/add/del lists: Definition 1
handles singleton conjunctions, implementation C uses effective special-case
handling per Haslums (2012) definition.

300

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

hCFF
hC
domain
Barman11
Barman14
CaveDiving14
ChildSnack14
Elevators11
Floortile11
Floortile14
GED14
Hiking14
Maintenance14
Nomystery11
Openstacks11
Openstacks14
Parcprinter11
Parking11
Parking14
Pegsol11
Scanalyzer11
Sokoban11
Tetris14
Thoughtful14
Tidybot11
Transport11
Transport14
Visitall11
Visitall14
Woodworking11


hCadd

arb rnd dif arb rnd
3
1
7
2
11
20
20
20
15
10
7
19
17
16
5
1
20
17
18
7
13
11
0
0
2
0
20

2
0
7
0
19
20
20
20
17
13
6
19
17
10
20
17
20
18
16
7
13
15
2
0
18
4
20

8
3
7
14
18
20
20
20
14
10
10
19
15
13
8
1
20
20
17
8
9
13
6
2
3
0
20

8
3
7
2
20
20
20
20
14
9
7
19
15
12
8
4
20
20
17
8
10
14
12
9
16
4
20

12
7
7
1
20
20
20
20
16
14
7
20
16
12
14
9
20
20
17
8
11
16
13
7
17
4
20

282 340 318 338 368

hFF (C )
hadd (C )
arb rnd dif arb rnd
h1 ( C )

0
1
7
2
8
20
20
20
16
11
7
20
15
12
2
0
20
18
18
4
13
10
2
0
15
4
20

0
0
7
0
12
20
20
20
10
10
6
16
8
12
19
9
20
18
16
7
10
15
1
0
13
4
20

1
0
7
0
15
20
20
20
16
11
10
20
15
13
4
0
20
20
18
3
15
15
5
2
16
4
20

14
7
7
0
19
20
20
20
15
11
12
20
14
12
11
5
20
19
17
5
12
14
10
7
16
4
20

1
0
7
0
18
20
20
20
13
11
6
20
16
7
20
20
20
20
17
8
10
19
11
7
14
4
20

285 293 310 351

349

C)
hFF (ce
hadd (Cce )
arb rnd dif arb
rnd

h1 (Cce )

1
1
7
2
9
20
20
20
16
11
7
20
14
8
2
0
20
18
18
4
13
13
2
0
3
0
20

0
0
7
0
12
20
20
20
11
9
6
16
9
11
16
8
20
19
16
9
9
17
3
0
13
4
20

2
1
7
0
15
20
20
20
15
11
11
20
13
10
4
0
20
20
16
4
15
15
4
1
3
0
20

15
8
7
0
20
20
20
20
15
11
12
19
12
9
12
5
20
19
17
4
11
14
10
10
16
4
20

2
1
7
0
19
20
20
20
12
12
6
20
16
10
20
19
20
20
17
9
10
18
11
9
14
4
20

269 295 287 350

356

hC

CFF
hnc

hCadd
arb rnd dif arb rnd
2
1
7
2
11
20
20
20
15
10
7
20
17
16
5
1
20
18
18
7
12
11
0
0
2
0
20

2
0
7
0
20
20
20
20
16
12
6
19
16
10
20
20
20
20
17
9
12
15
2
0
18
4
20

8
3
7
14
18
20
20
20
12
11
10
19
15
14
7
1
20
20
17
8
9
13
5
2
3
0
20

11
4
7
2
20
20
20
20
15
10
7
20
13
12
8
5
20
20
17
8
10
14
14
6
18
4
20

13
7
7
1
20
20
20
20
15
14
7
20
17
13
14
9
20
20
17
7
11
16
13
9
18
4
20

282 345 316 345 372

C ), hCFF , differTable 1: Coverage results x = 2, hCFF , hFF (C ), hFF (ce
nc
C
Cadd
ent best-supporter functions (h vs. h
) tie-breaking strategies. Best results highlighted boldface. Abbreviations: arb arbitrary tie-breaking; rnd
random tie-breaking (per-instance median seed, see also Table 2); dif difficulty
tie-breaking.

performs even competitive, non-standard hFF variant random tie-breaking.
hFF (C ), true small x values. Recall, however, report
performance search, C-learning: data evaluates exclusively
merits respective heuristic functions, overhead required obtain
first place. stick rationale throughout, differences explicitconjunction heuristics contribution here. completeness, Appendix B shows
coverage plots counting C-learning part solving effort, i. e., 30-minute limit
time taken C-learning search together.
Given typically detrimental effect large x, follows first (Section 5.3)
explore case x = 2, examining detail space heuristic functions best supporters. Subsequently (Section 5.4), examine detail happens scale x.
make latter experiments feasible, fix heuristic function performant best-supporter method; turn out, scaling x, method
every heuristic, namely hCadd random tie-breaking.
301

fiF ICKERT & H OFFMANN & TEINMETZ

5.3 Small C: Heuristics, Best Supporters, Tie Breaking x = 2
examine first performance main heuristic functions, i. e., hCFF vs. comC ), well hCFF essentially
peting previous variants hFF (C ) hFF (ce
nc
C ). discuss
perceived alternative, no-compilation, implementation hFF (ce
CFF
CFF
CFF
behavior h|G0 |=1 h|G0 |=1A , relative h , below. Consider Table 1.
striking observation data differences heuristic functions
dominated tie-breaking strategies. function tie-breaking, range
C ), 282
overall coverage 282368 hCFF , 285351 hFF (C ), 269356 hFF (ce
CFF . relatively small role heuristic function differences, x = 2, makes
372 hnc
CFF different scaling C, cross-context conditions,
sense advantages hCFF hnc
non-compilation implementation naturally impact larger C is.
cases though even small C makes difference.
Comparing tie-breaking strategies, hCadd best supporters superior hC best supporters, typically per domain almost consistently total. makes sense
heuristics run risk over-estimation, hCadd better hC finding cheap relaxed plans. several cases combination heuristic
CFF hC difficulty tietie-breaking method works exceptionally well, e. g. hCFF /hnc
CFF hC arbitrary tie-breaking Parcprinter11,
breaking ChildSnack14, hCFF /hnc
FF
C
FF
C
add
h ( )/h (ce ) h
arbitrary tie-breaking Barman11. performance peaks consistent across tie-breaking methods respective heuristics,
consider outliers caused brittleness search.
Comparing heuristic functions h vs. h0 , way identifying strong advantages
consider domains Table 1 h consistent advantage h0 , i. e., h
least good h0 tie-breaking methods, strictly better least one method.
Call advantage strict h strictly better tie-breaking methods. comparison hCFF vs. hFF (C ), hCFF consistent advantage 5 domains (ChildSnack14,
Elevators11, Openstacks14, Tetris14, Transport14), hFF (C ) consistent
advantage 2 domains (Sokoban11 Visitall14). advantage strict hCFF
Elevators: cases, tie-breaking methods work equally well
heuristics. Overall, despite noise data (somewhat) favor hCFF .
C ) yields similar picture, 4 consistent (non-strict)
comparison hCFF vs. hFF (ce
CFF
advantages h
(ChildSnack14, Elevators11, Openstacks14, Sokoban11) vs. 1 conC ) (Tidybot11). illuminating offset
sistent (non-strict) advantage hFF (ce
CFF
observations data hnc : every domain hCFF consistent adC ), hCFF also consistent advantage hFF ( C ),
vantage hFF (ce
nc
ce
CFF consistent disadvantage vs. hFF ( C ) hCFF ,
domain hnc
ce
C )
Tidybot11. Hence reason differences hCFF hFF (ce
cross-context conditions. Presumably, cross-context conditions occur
specific situations, small C play role.
CFF .
evidence towards conclusion comes comparison hCFF vs. hnc
Actually, regarding cross-context conditions, comparison informative
C ): all, hCFF hCFF differ accounting respecthat hCFF vs. hFF (ce
nc
tively accounting cross-context conditions. terms consistent advantages,
CFF , 8 consistent (non-strict) advantages hCFF
comparison clearly favor hnc
nc
302

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

hCFF
worst med best

domain

s1

s2

s3

Barman11
Barman14
CaveDiving14
ChildSnack14
Elevators11
Floortile11
Floortile14
GED14
Hiking14
Maintenance14
Nomystery11
Openstacks11
Openstacks14
Parcprinter11
Parking11
Parking14
Pegsol11
Scanalyzer11
Sokoban11
Tetris14
Thoughtful14
Tidybot11
Transport11
Transport14
Visitall11
Visitall14
Woodworking11

13
8
7
2
20
20
20
20
18
13
6
20
18
13
14
9
20
20
16
8
11
15
14
9
17
4
20

12
5
7
1
20
20
20
20
14
13
7
20
16
10
14
11
20
20
17
9
10
16
12
7
18
4
20

12
7
7
2
19
20
20
20
15
13
7
20
16
12
16
8
20
20
18
7
12
16
11
7
17
4
20

10
5
7
0
19
20
20
20
13
10
6
20
16
8
13
7
20
20
16
6
9
14
9
7
17
4
20

12
7
7
1
20
20
20
20
16
14
7
20
16
12
14
9
20
20
17
8
11
16
13
7
17
4
20

15
8
7
4
20
20
20
20
18
15
7
20
18
15
17
12
20
20
18
10
13
17
15
9
18
4
20

375 363 366

336

368

400



hFF (C )
worst med best

var



s1

s2

s3

1
3
0
1
1
0
0
0
4
0
1
0
2
3
2
3
0
0
2
2
2
1
3
2
1
0
0

+5
+5
0
1
1
0
0
0
+4
+4
1
+1
+3
2
+8
+7
0
0
1
1
+2
+2
2
2
+2
0
0

3
0
7
1
18
20
20
20
13
11
6
20
16
9
20
20
20
20
17
11
11
17
12
8
14
4
20

1
0
7
1
17
20
20
20
13
8
6
20
16
9
20
17
20
20
17
9
10
18
11
6
14
4
20

1
1
7
2
19
20
20
20
13
12
6
19
14
9
20
19
20
20
17
7
11
17
11
7
13
4
20

1
0
7
0
17
20
20
20
11
5
6
19
14
6
20
16
20
20
17
7
8
14
9
5
13
4
20

1
0
7
0
18
20
20
20
13
11
6
20
16
7
20
20
20
20
17
8
10
19
11
7
14
4
20

3
1
7
4
19
20
20
20
15
15
6
20
16
14
20
20
20
20
17
12
14
19
14
9
14
4
20

358 344 349

319

349

383

var



2 13
1 7
0
0
1 +2
2 2
0
0
0
0
0
0
0 2
4 3
0 6
1 1
2 +2
0 3
0 +9
3 +15
0
0
0 +1
0
0
4 +6
1 2
1 +4
1 +2
2 1
1 3
0
0
0
0

Table 2: Coverage results x = 2, showing effect different random seeds
random tie-breaking hCadd best-supporters. s1, s2, s3 denote
3 random seeds (fixed throughout experiment). best, med,
worst columns assess per-instance aggregation methods, selecting
best/median/worst seed per instance respectively. var assesses per-domain
performance variance, terms difference best worst coverage. assesses per-domain consistency performance change relative
baseline, i. e., relative hCadd best-supporters arbitrary tie-breaking.
shows maximum absolute difference, + coverage better
seeds, coverage worse seeds, otherwise, i. e., coverage
actually gets better worse depending seed.
vs. 2 consistent (non-strict) advantages hCFF . However, examining closely,
advantages small scale. Whereas, comparisons above, average
coverage difference consistent-advantage domains typically 2 3,
CFF usually 0.2 maximum 0.8.
comparison hCFF hnc
conclude regarding experimental hypotheses, (H1) advantage hCFF
C ) thanks
thanks better scaling |C |, (H2) advantage hCFF hFF (ce
CFF hFF ( C ) thanks
cross-context conditions, (H3) advantage hCFF hnc
ce
implementation? Table 1 provides evidence favor (H1) (H3), though
domains, subject substantial noise tie-breaking. support (H2).
evidence suggests that, x = 2, taking cross-context conditions account
neither substantial positive effects substantial negative effects.
hFF (C )

303

fiF ICKERT & H OFFMANN & TEINMETZ

words order regarding use random tie-breaking. crucial observations that, per domain, (a) variance random seeds typically small,
(b) performance change relative baseline typically consistent. makes random tie-breaking comparable non-randomized algorithm options. said,
domains either (a) (b) false, total differences would sum up.
counteract per-instance aggregation method obtain per-instance data
reduces variance relative individual seeds, interpolates seeds
terms overall performance. per-instance median seed, 3 random seeds ran
experiments, turns suitable (for coverage, counts instance solved
least 2 3 randomized runs solved it). used per-instance median seed
Figure 1 Table 1, use cases random tie-breaking
employed. Table 2 shows data supporting observations design decisions.
quick look var columns Table 2 confirms observation (a). difference
best worst per-seed coverage 2 except 5 domains hCFF
3 domains hFF (C ). hand, looking bottom row comparing
seeds, differences add up. would especially select
best worst seed per instance (best worst columns), resulting coverage
differences around 70 total. However, using median (med column) seed
results per-instance aggregation desired properties.
Regarding observation (b), consider columns Table 2. domains
performance relative baseline consistent, i. e., gets better worse depending
random seed, marked symbol. symbols sparse
table. 4 27 domains hCFF , 2 hFF (C ),
randomization changes performance consistently. (It rarely deteriorates performance
hCFF , hFF (C ) picture mixed depending domain.) shows
clearly random tie-breaking reliable baseline.
Indeed, findings contradict Keyder et al.s (2014) use random tie-breaking
measure noise. Keyder et al.s idea account brittleness search randomizing baseline heuristic h (in case, hFF hadd best supporters arbitrary
tie-breaking), measuring Table 2 yet ignoring distinctions +, , , i. e.,
considering absolute maximum difference. deem heuristic h0 significantly better h improvement h larger intuitively, larger
random noise. However, approach assumes random tie-breaking yields distribution around baseline average, much data. Consider
example hFF (C ) Barman11. According Keyder et al.s method, random noise
13, heuristic significantly better hFF (C ) must hence
increase coverage least 14. noise effect random tie-breaking
consistently detrimental. Similar examples abound.
conclude that, specific context experiments, Keyder et al.s measure
appropriate random tie-breaking typically source noise. contrary, performance 3 separate runs random tie-breaking reliably reported
like single planner run, per-instance median seed aggregation.
Let us finally consider behavior hCFF relative hCFF
trivializes sub| G 0 |=1
goal support selection, relative hCFF
also trivializes C-relaxed plan
| G 0 |=1A
(into set actions instead action occurrences). Table 3 shows data.
304

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

hCFF
hC
domain
Barman11
Barman14
CaveDiving14
ChildSnack14
Elevators11
Floortile11
Floortile14
GED14
Hiking14
Maintenance14
Nomystery11
Openstacks11
Openstacks14
Parcprinter11
Parking11
Parking14
Pegsol11
Scanalyzer11
Sokoban11
Tetris14
Thoughtful14
Tidybot11
Transport11
Transport14
Visitall11
Visitall14
Woodworking11


hCadd

arb rnd dif arb rnd
3
1
7
2
11
20
20
20
15
10
7
19
17
16
5
1
20
17
18
7
13
11
0
0
2
0
20

8
3
7
14
18
20
20
20
14
10
10
19
15
13
8
1
20
20
17
8
9
13
6
2
3
0
20

2
0
7
0
19
20
20
20
17
13
6
19
17
10
20
17
20
18
16
7
13
15
2
0
18
4
20

8
3
7
2
20
20
20
20
14
9
7
19
15
12
8
4
20
20
17
8
10
14
12
9
16
4
20

12
7
7
1
20
20
20
20
16
14
7
20
16
12
14
9
20
20
17
8
11
16
13
7
17
4
20

282 318 340 338 368

hC

hCFF
| G 0 |=1

hCadd

arb rnd dif arb rnd
2
2
7
2
10
20
20
20
10
4
7
19
17
13
2
1
20
19
16
7
9
10
0
0
12
4
20

3
1
7
10
17
20
20
19
10
5
6
19
17
11
2
0
20
19
17
4
14
11
4
1
11
3
20

0
0
7
1
18
20
20
20
14
4
5
20
20
3
14
3
20
20
17
14
10
12
2
0
20
11
20

13
6
7
1
20
20
20
20
14
6
6
20
16
8
2
0
20
20
17
4
13
14
13
8
20
7
20

13
4
7
2
20
20
20
20
13
5
6
19
15
7
2
0
20
20
16
11
14
16
13
7
20
5
20

273 291 315 335 335

hCFF
| G 0 |=1A

hC

hCadd
arb rnd dif arb rnd
0
1
7
2
8
20
20
20
12
7
6
20
16
16
4
2
20
17
16
8
9
10
1
0
1
0
20

3
1
7
10
12
20
20
20
11
10
9
19
17
11
3
0
20
20
17
10
13
12
3
2
1
0
20

0
0
7
0
19
20
20
20
15
10
5
20
16
5
20
14
20
19
17
11
9
16
1
0
20
9
20

6
1
7
2
20
20
20
20
15
10
8
19
16
7
15
6
20
20
17
8
14
13
11
9
20
8
20

1
1
7
0
20
20
20
20
14
10
7
20
16
8
18
13
20
20
16
10
12
15
9
5
19
5
20

263 291 333 352 346

Table 3: Coverage results x = 2, hCFF vs. hCFF
hCFF
, different best
| G 0 |=1A
| G 0 |=1
supporter functions tie-breaking strategies. Best results highlighted boldface. Abbreviations Table 1.
Like Table 1, lot noise due tie-breaking strategies. Note though
heuristics shown work representation based
implementation. differences subgoal support selection/relaxed plan
definition. Comparisons tie-breaking strategies across heuristics hence direct.
data shows clear advantage hCFF hCFF
. every tie-breaking strategy,
| G 0 |=1
hCFF strictly better total (the margin varying 3 hCadd arbitrary tiebreaking 33 hCadd random tie-breaking). Using per-domain comparison
across tie-breaking strategies, hCFF consistent advantage 10 domains (3
strict), consistent disadvantage 2 domains (both strict, namely two
Visitall variants). Comparing search space size states per second commonly solved
instances, advantages hCFF partly due quality (e. g. Parking11 994.4 vs. 8, 091.1
geometric mean state evaluations), supporting hypothesis (H4) non-trivial
subgoal support selection hCFF yields informed heuristic hCFF
.
| G 0 |=1
also several cases advantage speed (e. g. Elevators11 4713.0 vs. 4591.1
states per second). possible cause lies different states evaluated:
hCFF easier evaluate, typically case states closer goal.
(We remark that, like standard relaxed plan heuristic, heuristic function
305

fiF ICKERT & H OFFMANN & TEINMETZ

runtime, typically 90% more, spent computation best-supporter function,
hC respectively hCadd case.)
Relative hCFF
, hCFF still advantage picture mixed.
| G 0 |=1A
terms total coverage, hCFF dominant except hCadd arbitrary tie-breaking. Per
domain, hCFF consistent advantage 6 cases (1 strict), vs. consistent disadvantage
4 cases (none strict). appears that, least setting x IPC benchmarks, relative hCFF
prone over-estimation, trivialized C-relaxed plan
| G 0 |=1
hCFF
result better heuristic. Comparing hCFF
vs. hCFF
directly, hCFF
| G 0 |=1A
| G 0 |=1A
| G 0 |=1
| G 0 |=1A
dominates total except hC arbitrary tie-breaking, consistent advantage 5 domains (3 strict), consistent disadvantage 3 domains (none strict).
terms search space size states per second commonly solved instances, advantages hCFF
mostly due quality (most notably Parking11, 1102.7 vs. 8091.1
| G 0 |=1A
state evaluations), except Hiking14 hCFF
faster (526.7 vs. 639.8 states per
| G 0 |=1A
second).
5.4 Scaling C: Performance Function x
examine search behavior C becomes larger. naturally presented terms
plots performance measures function size limit x. Keep mind, especially
comparison hFF baselines, C-learning conducted separately
search, separate 30-minute time limit. Appendix B shows coverage
plots included below, counting C-learning part solving effort.
following discussions, include brief summaries data.
Figure 2 shows total coverage function x. Consider first Figure 2 (b) settles question competitive tie-breaking strategies. seen
Tables 1 2, total coverage x = 2 maximal using hadd random tie-breaking,
almost heuristic functions. two exceptions hFF (C ) hCFF
.
| G 0 |=1A
these, hadd arbitrary tie-breaking works better hadd random tie-breaking
x = 2 (2 instances solved hFF (C ), 6 instances solved hCFF
). How| G 0 |=1A
ever, Figure 2 (b) shows, advantage arbitrary tie-breaking x = 2 turns
substantial disadvantage larger values x. Hence, remainder experiments, fix hadd random tie-breaking best-supporter method throughout.
Consider Figure 2 (a). previously hinted (Figure 1 (c)), total coverage tends
decrease function x. heuristics consistently outperform hFF baselines
though, except hFF (C ) whose per-state runtime overhead drags coverage
hFF x 32 (and except temporary dip hCFF
hFF random
| G 0 |=1
tie-breaking x = 16).
CFF , hFF ( C ), hFF ( C ) decrease, relatively speaking,
Note hCFF , hnc
ce
5
steeply x = 2 , less steeply afterwards. because, around point,
many domains C-learning reaches time limit size limit. hFF (C )
remaining domains still becomes substantially worse, heuristics
less pronounced. Observe coverage difference x = 2 x = 210 ,
except hFF (C ), small, around 20 instances. Indeed, decrease caused
domains only, namely Barman, Parking, Sokoban, Visitall.
306

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

370

370

360

360

350

350

340

340

330

330

320

320

310

310

300
290
280
270
260
250
240
21

300

CFF
hnc
hCFF

290

C)
hFF (ce
FF
h (C )
hCFF
| G 0 |=1A
hCFF
| G 0 |=1
hFF (arb)
hFF (rnd)

22

280
270
260
250
240

23
24
add

(a) h

25

26

27

28

29

210

21

hFF (C ) (rnd)
hFF (C ) (arb)
hCFF
(rnd)
| G 0 |=1A
hCFF
(arb)
0
| G |=1A
22

23

24

25

26

27

28

29

210

(b) hadd , random vs. arbitrary tie-breaking

, random tie-breaking

Figure 2: Total coverage function size bound x. (a) heuristics using hadd
random tie-breaking (median per-instance seed; heuristic functions listed topdown order coverage x = 2). (b) hadd random vs. arbitrary
tie-breaking hFF (C ) hCFF
, two heuristics arbitrary
| G 0 |=1A
tie-breaking yields higher total coverage x = 2. (a), comparison
include hFF baseline, using hadd arbitrary tie-breaking (FD default),
using hadd random tie-breaking. Recall comparison effort
C-learning included explicit-conjunction heuristics.
CFF , hFF ( C ) fairly close other, follow simThe curves hCFF , hnc
ce
CFF
C ), hCFF consistently better
ilar pattern. h
consistently better hFF (ce
nc
FF
C
9
6
CFF ,
h (ce ) except x = 2 . x 2 , consistent advantage hCFF hnc
indicating beneficial impact cross-context conditions.
Regarding hCFF
hCFF
, latter consistently much better former.
| G 0 |=1
| G 0 |=1A

hCFF
achieves competitive performance x 25 . heuristics exhibit
| G 0 |=1A
clear trend x. latter due difference heuristic function speed (as
shall see below, speed hCFF
hCFF
similar hCFF , across x values).
| G 0 |=1
| G 0 |=1A
Rather, caused particular behaviors domains causing coverage decline tendency Figure 2 (a). Compared heuristics, hCFF
hCFF
scale
| G 0 |=1
| G 0 |=1A
better x Barman (only hCFF
) Visitall (both hCFF
hCFF
). also
| G 0 |=1A
| G 0 |=1
| G 0 |=1A
cases, e. g. Barman hCFF
Parking hCFF
hCFF
,
| G 0 |=1
| G 0 |=1
| G 0 |=1A
heuristics bad begin with, solving first place instances lost
heuristics larger x, hence suffering less large x.
domains Barman, Parking, Sokoban, Visitall, heuristic
suffering large x hFF (C ), heuristic suffers all. heuristics,
growing x marginally negative effect, inconclusive effect, effect all.
also 4 domains heuristics tend improve coverage x grows.
Figure 3 shows data these. coverage growth x consistent across
307

fiF ICKERT & H OFFMANN & TEINMETZ

20

20

15

15

10

10

5

5

01
2

22

23

24

25

26

27

28

29

01
2

210

22

23

(a) Maintenance
20

15

15

10

10

5

5

22

23

24

25

26

27

25

26

27

28

29

210

28

29

210

(b) Parcprinter

20

01
2

24

28

29

01
2

210

(c) Tetris

22

23

24

25

26

27

(d) Thoughtful

Figure 3: Coverage individual domains. explicit-conjunction heuristics use hadd
random tie-breaking. Recall comparison hFF baselines
effort C-learning included explicit-conjunction heuristics.
heuristics Parcprinter. domains, picture mixed, lot
CFF
variance Maintenance Thoughtful, mainly hCFF
, hCFF
, hCFF , hnc
| G 0 |=1 | G 0 |=1A
profiting large x Tetris. (The curves remain flat Tetris x 27
C-learning time-limit applies new conjunctions added.)
picture change imposing 30-minute limit time taken Clearning search together? Naturally, tendency coverage decline growing
x becomes stronger, yet relative performance explicit-conjunction heuristics remains
similar.
total coverage, shown Figure 7 (page 323), performance substantially worse
search-only already x = 2 (by 20-30 instances), declines steeply
x heuristics. relative performance heuristics, however, conclusions
CFF beat
remain exactly above. respect baselines, hCFF hnc
FF
non-default h (random tie-breaking), x = 2. inferior default hFF
308

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

baseline beat explicit-conjunction heuristics medium-large x values (x = 16
x = 32), except hFF (C ) worse x 8, hCFF
marginally
| G 0 |=1
beats default hFF x = 2.
individual domains Figure 3, questions whether (a) x > 2 still improves coverage including effort C-learning, whether (b) explicitconjunction heuristics still beat hFF baselines. Figures 8 9 (pages 324
325) show, answer (a) (b) yes. Maintenance Thoughtful, much changes respect Figure 3. Parcprinter Tetris, large values
x detrimental, moderate ones arent. Coverage increases certain point,
namely x = 23 Parcprinter x = 24 Tetris, decreases point.
Let us get back hypotheses (H1)(H4). Figure 2 (a) confirms (H1) hCFF typically advantage hFF (C ); somewhat supports (H2) cross-context conC ) (and, directly, hCFF ); confirms
ditions hCFF yield advantage hFF (ce
nc
CFF
CFF
C ); (H4) confirms hCFF
hnc advantage hFF (ce
(H3) h
advantage hCFF
. examine reason advantages, thus evaluate
| G 0 |=1
specific claim hypothesis, consider fine-granular performance
measures, namely search space size (number state evaluations), states per second,
search runtime, commonly solved instances.
specific claim hypothesis (H1) that, thanks avoiding exponential blowup |C |, hCFF typically faster hFF (C ) thus improves performance. Figure 4
confirms this. top row plots shows main data (the data overall performance).
see top left plot, terms quality two heuristics similar. terms
speed, hFF (C ) suffers growing x, overall consistently individual
domains, effect runtime, like coverage discussed above, suffers well.
much less hCFF , leading dramatic performance advantage large x.11
hand, hCFF also suffers large x, though less hFF (C ),
advantage hFF (C ) overall mute. relevant domains where,
thanks speed advantage, hCFF benefits growing x, hence improves
best performance obtainable hFF (C ) value x. coverage, happens
Maintenance Tetris (both, without including C-learning, cf. Figures 3, 8,
9). search runtime commonly solved instances, happens Hiking, Pegsol,
Tetris, Thoughtful. Figure 4 showcases Hiking Tetris, also use
showcases nicely illustrate main points.
CFF ,
next compare three heuristic functions other, namely hCFF , hnc
FF
C
h (ce ). serves examine hypotheses (H2) (H3). specific claim
former asserts that, thanks accounting cross-context conditions, hCFF
C ). latter asserts implementation hCFF hCFF typiinformed hFF (ce
nc
C ). advantage compare three heuristics together
cally faster hFF (ce
as, evaluate importance cross-context conditions, comparison hCFF
CFF direct. Figure 5 shows data.
hnc
CFF faster
Hypothesis (H3) confirmed consistently, small scale. hCFF hnc
FF
C
h (ce ) across x values overall, small advantage grows x.
11. remark that, x = 1 hCFF hFF (C ) variants hFF , hardly speed
differences, neither hCFF hFF (C ) compared FDs standard implementation hFF .

309

fiF ICKERT & H OFFMANN & TEINMETZ

105

104

hCFF
hCFF (*)
hFF (C )
hFF (C ) (*)

103

102
21

22

23

24

25

26

27

28

29

210

103

103

102

102

101

101

100 1
2

22

23

24

25

26

27

28

29

210

100 1
2

22

23

24

25

26

27

28

29

210

28

29

Overall: search space size, states per second, search runtime
105

103

103

104

102

102

103

101

101

102
21

22

23

24

25

26

27

28

29

100 1
2

22

23

24

25

26

27

28

29

100 1
2

22

23

24

25

26

27

Hiking: search space size, states per second, search runtime
105

103

103

104

102

102

103

101

101

102
21

22

23

24

25

26

27

100 1
2

22

23

24

25

26

27

100 1
2

22

23

24

25

26

27

Tetris: search space size, states per second, search runtime
Figure 4: Data hCFF vs. hFF (C ). Geometric means. curves use instances
solved values x, curves (*) solved heuristics (174
instances overall), without (*) solved respective heuristic.
Essentially behavior occurs every individual domain, single exception,
C ) consistently faster. Hiking Tetris Figure 5 two
namely Tidybot hFF (ce
CFF consistently (across almost
typical examples. terms coverage, hCFF hnc
FF
C
values x) dominate h (ce ) Barman, Elevators, Hiking, Maintenance, Sokoban,
Visitall; opposite happens Parking Tidybot.
Regarding (H2), top left plot Figure 5 shows, three heuristics yield similar
search space sizes overall. domains hCFF consistently, across values
CFF . However, domains hCFF
x, yields smaller search spaces hnc
notable advantage large values x. mainly Hiking Tetris, shown
Figure 5. Tetris, advantage basically consistent beyond x = 24 . Hiking behaves
similarly except degradation largest two x values. domains, hCFF also
CFF . Overall, support hypothesis
corresponding coverage advantages hnc
310

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

104

103

103

102

105

104
hCFF
hCFF
nc

103 1
2

hFF (C
ce )
22

23

24

25

26

27

28

29

210

102 1
2

22

23

24

25

26

27

28

29

210

101 1
2

22

23

24

25

26

27

28

29

210

26

27

28

29

210

26

27

28

29

210

Overall: search space size, states per second, search runtime
10

103

103

102

102

5

104

103 1
2

22

23

24

25

26

27

28

29

210

101 1
2

22

23

24

25

26

27

28

29

210

101 1
2

22

23

24

25

Hiking: search space size, states per second, search runtime
103

103

102

102

105

104

103 1
2

22

23

24

25

26

27

28

29

210

101 1
2

22

23

24

25

26

27

28

29

210

101 1
2

22

23

24

25

Tetris: search space size, states per second, search runtime
CFF vs. hFF ( C ). Geometric means. curves use
Figure 5: Data hCFF vs. hnc
ce
instances solved values x heuristics (225 instances overall). Note
that, better readability, y-scales show 2 orders magnitude (in difference Figure 4).

(H2) weak, data give evidence cross-context conditions can,
cases, advantage.
context, worth coming back briefly discussion Table 1, x =
CFF somewhat favor hCFF (many
2, comparison hCFF hnc
nc
per-domain advantages, small scale). growing x, picture becomes
CFF consistently
favorable hCFF , though still small scale. domain hnc
CFF
faster, higher quality, yields better coverage, h . hand,
hCFF consistently faster Barman, GED, Openstacks, plus favorable behavior
Hiking Tetris shown Figure 5.
311

fiF ICKERT & H OFFMANN & TEINMETZ

103

105

103

hCFF
hCFF
0

|G =1|

hCFF
|G0 =1|A
104

102

102

103
21

101
22

23

24

25

26

27

28

29

210

10

1

21

22

23

24

25

26

27

28

29

210

21

22

23

24

25

26

27

28

29

210

26

27

28

29

210

26

27

28

29

210

Overall: search space size, states per second, search runtime
105

103

103

104

102

102

103
21

101
22

23

24

25

26

27

28

29

210

10

1

21

22

23

24

25

26

27

28

29

210

21

22

23

24

25

Hiking: search space size, states per second, search runtime
10

103

5

103

104

102

102

103
21

101
22

23

24

25

26

27

28

29

210

10

1

21

22

23

24

25

26

27

28

29

210

21

22

23

24

25

Tetris: search space size, states per second, search runtime
Figure 6: Data hCFF vs. hCFF
vs. hCFF
. Geometric means. curves use
| G 0 |=1
| G 0 |=1A
instances solved values x heuristics (208 instances overall). Note
that, better readability, y-scales show 2 orders magnitude (in difference Figure 4).
Let us finally consider hypothesis (H4), asserts that, thanks non-trivial
subgoal support selection, hCFF typically yields informed heuristic hCFF
.
| G 0 |=1
comparison completeness. Figure 6 shows data.
include hCFF
| G 0 |=1A
three heuristics perform similarly overall. partly due particularities common instance basis. Several domains all, hardly, contained
instance basis Figure 6. pertains particular Barman, Maintenance, Parcprinter, Parking, hCFF large coverage advantages hCFF
.
| G 0 |=1
Nevertheless, Figure 6 allows confirm (H4), albeit small scale. overall,
consistently across values x, hCFF slightly smaller search space hCFF
. (It
| G 0 |=1
also slightly faster hCFF
, consequently results slightly better runtime.) Per
| G 0 |=1
312

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

domain, hCFF search advantages 9 cases, disadvantages 3 cases. Figure 6 showcases Hiking Tetris, respectively represent domain classes,
heuristics benefit growing x.
domains, like overall hCFF slight speed advantage hCFF
.
| G 0 |=1
already observed discussion Table 3, must caused different
states evaluated, hCFF evaluating states close goal thus faster.
terms coverage, hCFF clearly dominates hCFF
overall (Figure 2),
| G 0 |=1
strong advantages 8 domains (e. g. Maintenance Parcprinter, cf. Figure 3),
hCFF
advantages 3 domains (Tetris cf. Figure 3, Thoughtful, Visitall).
| G 0 |=1
Let us finally compare hCFF
hCFF
. overall coverage difference clearly
| G 0 |=1
| G 0 |=1A
favor hCFF
. Per domain, hCFF
coverage advantage 5 domains
| G 0 |=1A
| G 0 |=1A
disadvantage 9, yet disadvantages typically marginal whereas advantages
substantial. Regarding speed search space size commonly solved instances,
speed similar almost universally. Search space size also often similar
(in overall, hCFF
hCFF
almost indistinguishable). exceptions
| G 0 |=1
| G 0 |=1A
individual domains, specifically Elevators, Pegsol, Visitall hCFF
better,
| G 0 |=1
GED, Hiking, Parking hCFF
better.
| G 0 |=1A
Summing observations, data confirms (H1) impressively, caveat
IPC domains large C beneficial. (H3) (H4) confirmed consistently, overall across domains. evidence (H2)
weaker, good cases Hiking Tetris. entirely unexpected given
cross-context conditions occur specific situations, important theory but, evidently, rare practice far reflected IPC benchmarks.

6. Contribution Summary Future Work
work contributes new understanding recent compilation-based partial delete relaxation heuristics, terms combination delete relaxation critical-path
heuristics. key insight view priori unrelated relaxations
defined underlying set atomic subgoals, relaxation consists decomposing non-atomic conjunctive goals atomic subgoals. Critical-path heuristics require achieve costly atomic subgoal, delete relaxation requires
achieve atomic subgoals. standard delete-relaxation framework becomes
special case atomic subgoals singleton facts, entire standard machinery h+ , relaxed plan existence testing based h1 , additive heuristic hadd , relaxed
plan extraction best-supporter function extends naturally along dimension
allowing arbitrary atomic subgoals C instead.
direct characterization identifies precise new source complexity relaxed plan extraction process, namely selecting subset atomic subgoals support
given action. Thanks this, design new C-relaxed plan heuristics, hCFF
CFF , avoiding shortcomings previous compilation-based heuristics. theoretical
hnc
advantages hCFF reflected empirically IPC benchmarks. improvement
state art, overall, marginal though, relates new heuristics
implementation advantages theoretical ones.
313

fiF ICKERT & H OFFMANN & TEINMETZ

view, main value work lies understanding compilation
heuristics actually do, spelling framework C-delete relaxation, replacing
C ) direct natural hCFF respectively hCFF .
hFF (C ) hFF (ce
nc
new heuristics may yield dramatic benefits cases, certainly reliable somewhat efficient predecessors, reason
use them. nice side benefit simple yet useful generalization hm hC .
believe still many exciting avenues future research area.
expect results help many them, efficient direct
implementation, alternate less opaque formulation.
obvious topic use backward search instead forward search, paralleling
design HSP-r (Bonet & Geffner, 1999) need compute hC
initial state. Alcazar et al. (2013) already took direction, didnt explore detail.
still glaring hole understanding C-relaxation heuristics, namely
role conjunction set C. good sets C? find them? literature
far offers preliminary answers second question, offers answer
first one. particular, proof convergence via hm : select large enough
hm = h , simulate via C hC = hm , hC hC+ get
hC+ = h QED. completely ignores (a) free choose set C,
size-m conjunctions, (b) hC lower bound hC+ , trivial one
hC+ typically much higher (similarly well-known relation h1 h+ ).
many/which conjunctions actually needed render hC+ perfect?
Preliminary results obtained bottom-up approach trying identify
planning fragments small sets C suffice obtain hC+ = h (Hoffmann, Steinmetz, &
Haslum, 2014). approach proved exceedingly difficult though, complex
case considerations already trivial fragments. instead explore top-down,
identifying conjunctions needed render hC+ perfect, thus guide C-learning
mechanisms? IPC benchmarks suffice use fact pairs,
h+ topology (Hoffmann, 2005) change ones? learn something
particular planning sub-structures handled?
practical approach design alternate C-learning methods. particular,
learn C search? Learning mistakes proved extremely successful constraint-satisfaction problems like SAT (e. g. Marques-Silva & Sakallah, 1999;
Moskewicz, Madigan, Zhao, Zhang, & Malik, 2001)? Recent work (Steinmetz & Hoffmann,
2016) devised approach dead-end detection, refining hC dead-ends
become known search (i. e., search explored descendants). depth-first search, algorithm approaches elegance clause learning SAT, learning generalizing knowledge refuted search subtrees.
search guidance non-dead-end states? usefully refine hCFF
search? difficulty that, whenever C increased, re-adjust relative
ordering states principle would need re-evaluate hCFF entire open list.
interesting option local search: use hill-climbing local minimum reached,
refine C eliminate local minimum hCFF search surface. words:
caught local minimum, rather giving heuristic relying search
instead commonly done across AI sub-areas refine heuristic exhibit exit.
314

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

summary, satisfied understanding C-relaxation heuristics,
believe key fully exploiting power lies better understanding
design methods finding atomic subgoals C.

Acknowledgments
work partially supported German Research Foundation (DFG),
grant HO 2169/5-1. thank anonymous reviewers, whose comments helped improve paper.

Appendix A. Proofs
Theorem 1 Let = (F , A, , G) planning task, C set conjunctions containing
singleton conjunctions. h+ (C ) = hC+ ().
Proof: Denote = (F , A, , G). proof via comparing two equations. Equation
simply Equation 7 (page 280, hC+ equation), characterizing hC+ (). derive Equation II applying Equation 5 (page 279, non-standard characterization h+ ) C ,
characterizing h+ (C ).
Repeating Equation 7, convenience: hC+ () = h(G C ), h function
conjunction sets G satisfies h( G ) =
(
0
c G : c
C
0
0
1 + minaA,6=G0 {c|cG,R(c,a)6=} h(( G \ G ) Gr ) else
Gr0 defined Gr0 := cG0 R(c, a).
Reconsider Equation 5, written as: h+ () = h(G), h
function fact sets G satisfies h( G ) =

0
GI
1 + minaA,6=G0 ={ p| pG,R({ p},a)6=} h(( G \ G 0 ) Gr0 ) else


Gr0 defined Gr0 := pG0 R({ p}, a).
apply previous equation C . Making explicit individual facts
C -fluents, obtain: h+ (C ) = h({c | c G C }), h function
fact sets G satisfies h( G ) =




0
c G : c C
0
0
1 + mina[C0 ]AC ,6=G0 ={c |c G,R({c },a[C0 ])6=} h(( G \ G ) Gr ) else

Gr0 defined Gr0 := c G0 R({c }, a[C 0 ]).
One already see correspondence Equation I, conjunctions c corresponding -fluents c . major difference set action/supportedsubgoal-set pairs minimized bottom cases.
Consider set G 0 = {c | c G, R({c }, a[C 0 ]) 6= } supported atomic subgoals
per last equation. condition R({c }, a[C 0 ]) 6= simplifies c C 0 ,


315

fiF ICKERT & H OFFMANN & TEINMETZ

exactly -fluents added a[C 0 ]. Thus, removing G 0 variable
fixed anyway, equation simplifies to: h( G ) =


0
c G : c C
0
0
1 + mina[C0 ]AC ,6={c |c G,cC0 } h(( G \ G ) Gr ) else

minimize actions a[C 0 ] C , C 0 needs support
non-empty subset subgoal -fluents/conjunctions c . possible choices
C 0 ? c principle include C 0 , i. e. subgoals principle
support using action are, definition C , exactly R(c, a) 6= .
Observe point including c c 6 G: support
subgoals yet result larger precondition. Hence choice C 0 exactly
6= C 0 {c | c G, R(c, a) 6= }. Renaming C 0 G 0 order unify notation
Equation I, yields final Equation II: h+ (C ) = h({c | c G C }), h
function fact sets G satisfies h( G ) =


0
c G : c C
1 + mina[G0 ]AC ,6=G0 {c|c G,R(c,a)6=} h(( G \ G 0 ) Gr0 ) else

Gr0 defined Gr0 := cG0 R({c }, a[ G 0 ]).
spell correspondence Equations I, view
tree whose root node initializing call respective input tasks goal.
two trees isomorphic sense one-to-one mapping tree
nodes, and, using suffixes [ ] [ ] identify respective tree, pair G [ ]
G [ ] corresponding tree nodes have:


() G [ ] = {c | c G [ ]}
true definition root nodes ( ) hC+ () = h(G C ) respectively ( ) h+ (C ) =
h({c | c G C }).
Consider corresponding bottom-case nodes (*). Observe choice atomic
subgoals c G 0 sides: Equation allows c G [ ]
R(c, a) 6= , Equation allows c G [ ] R(c, a) 6= .
map children nodes using action supported subgoal set
0
G [ ] = G 0 [ ] =: G 0 sides. use action Equation action a[ G 0 ]

Equation I. Consider recursive subgoals, ( G [ ] \ G 0 ) [ cG0 R(c, a)]C Equation I,

( G [ ] \ {c | c G 0 }) cG0 R({c }, a[ G 0 ]) Equation I.
G \ G 0 parts expressions exact match (*) construction,


remains consider [ cG0 R(c, a)]C vs. cG0 R({c }, a[ G 0 ]). regression singleton

fact sets yields action precondition, cG0 R({c }, a[ G 0 ]) simplifies pre( a[ G 0 ]).

definition C , equals [ cG0 (pre( a) (c \ add( a)))]C . R(c, a) = pre( a)


(c \ add( a)), equals [ cG0 R(c, a)]C . desired match [ cG0 R(c, a)]C
obvious, showing (*) preserved, concludes argument.
Theorem 2 Let = (F , A, , G) planning task, C set conjunctions containing
C ) = h C + ( ).
singleton conjunctions. h+ (nc
nc
316

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

Proof: proof similar Theorem 1. Equation same, except
C + . Equation II exactly same,
Gr0 := { R(c, a) | c G 0 } per definition hnc

difference Gr0 := cG0 R({c }, a[ G 0 ]) interpreted per
C , opposed C . arguments exactly same, exdefinition nc
cept last part proof showing correspondence { R(c, a) | c G 0 }C vs.

({c }, a[ G 0 ]). regression singleton fact sets yields action preconc G 0 RS
C , equals
dition, cG0 R({c }, a[ G 0 ]) simplifies pre( a[ G 0 ]). definition nc
{pre( a) (c \ add( a)) | c G 0 }C . R(c, a) = pre( a) (c \ add( a)), equals { R(c, a) |
c G 0 }C , matching { R(c, a) | c G 0 }C desired.
Theorem 3 Let = (F , A, , G) planning task, C set conjunctions containing
C ) = h C ( ).
singleton conjunctions. h1 (C ) = h1 (nc
Proof: Denote = (F , A, , G). Consider first C . compare respective characterizing
equations. First, Equation 6 (page 279) characterizes h1 ; applying C , get
h1 (C ) = h(G C ) h function fact sets G satisfies

G C
0
0
1 + mina[C0 ]AC ,R(G,a[C0 ])6= h( R( G, a[C ])) G = {c }, c C
h( G ) =

maxc G h({c })
else
Observe that, middle case, must c C 0 otherwise c 6 add( a[C 0 ]);
point including conjunctions C 0 , i. e., C 0 ) {c}, yield larger recursive subgoal R( G, a[C 0 ]). Hence re-write
previous equation to:

G C
0
1 + mina[{c}]AC ,R(G,a[{c}])6= h( R( G, a[{c}])) G = {c }, c C
h( G ) =

maxc G h({c })
else
Refer Equation I.
Recall hC () = h(G) h function fact sets G satisfies

GI
0
1
+
min
h
(
R
(
G,

))
G
C
h( G ) =
aA,R( G,a)6=

0
maxG0 G,G0 C h( G )
else
Refer Equation II.
Similarly proof Theorem 1, view equations tree whose
root node initializing call (I) h1 (C ) = h(G C ) respectively (II) hC () = h(G).
two trees isomorphic sense one-to-one mapping
tree nodes, and, using suffixes [I] [II] identify respective tree, pair G [ ]
G [ ] corresponding tree nodes have:

() G [ ] = {c | c C, c G [ ]}
obviously true root nodes, obviously invariant bottom case
map children node pairs corresponding c G [ ] respectively c G [ ].
317

fiF ICKERT & H OFFMANN & TEINMETZ

Consider corresponding middle-case nodes G [ ] = {c } G [ ] = c.
First, C actions a[{c}] definition satisfy R( G [ ], a[{c}]) = R({c }, a[{c}]) 6= .
choice a[{c}] thus corresponds choice actions original task
action a[{c}] included C . exactly actions
c regressed, R(c, a) 6= , hence R( G [ ], a) = R(c, a) 6= .
choice actions minimized sides, map children
node pairs corresponding a.
pair, recursive subgoal Gr [ ] generated (II) R(c, a) = (c \ add( a))
pre( a). recursive subgoal Gr [ ] generated (I) R({c }, a[{c}]) = pre( a[{c}]),
definition C equals [(c \ add( a)) pre( a)]C . latter defined {c0 | c0
C, c0 (c \ add( a)) pre( a)}. equals {c0 | c0 C, c0 Gr [ ], showing (*)
concluding argument.
C identical because, single-conjunction sets C 0 = { c }, two
argument nc
C ).
compilations coincide (specifically, precondition a[{c}] C nc
Theorem 4 Let = (F , A, , G) planning task, C set conjunctions containing
singleton conjunctions. C-relaxed plan CFF sequentialized form relaxed
plan C .
Proof: Denote = (F , A, , G). Recall definition CFF , CFF = (G C ),
partial function conjunction sets G defined G C satisfies ( G ) =


c G : c



C
0
0
0
(( G \ G ) Gr ) {( a, G )} A,

6= G 0 {c | c G, R(c, a) 6= , hC ( R(c, a)) = hC (c) 1},


hC ( Gr0 ) = hC ( G 0 ) 1
else
Gr0 defined Gr0 := cG0 R(c, a).
Starting (G C ), say keep tracing recursive invocations equation, using suitable ( a, G 0 ) choice CFF whenever bottom case equation applies.
construction (because CFF = (G C )), make choices way
eventually reach top case, terminate. Denote h( a0 , G00 ), . . . , ( an1 , Gn0 1 )i
inverted sequence action occurrences selected along trace, i. e., deeper recursion steps correspond smaller indices, ( a0 , G00 ) action occurrence whose selection
lead terminating top case, ( an1 , Gn0 1 ) action occurrence selected
initializing call. show h a0 [ G00 ], . . . , an1 [ Gn0 1 ]i relaxed plan C .
Denote Gi , 1 n, subgoal tackled selection ( ai1 , Gi01 )
middle case, denote G0 final subgoal tackled top case. Denote si
state resulting applying h a0 [ G00 ], . . . , ai1 [ Gi01 ]i C . show induction
(*) {c | c Gi } si . = n, Gn = G C , shows sn G C desired.
Induction base case, = 0: Here, (*) follows directly definition because, top
case fired G0 , c G0 c , hence c C = s0 .
induction step, assume (*) true i. show holds + 1.

construction, Gi recursive subgoal ( Gi+1 \ Gi0 ) [ cGi0 R(c, ai )]C . Denote left

half expression LH := Gi+1 \ Gi0 , right half RH := [ cGi0 R(c, ai )]C .


318

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

induction hypothesis, () {c | c LH RH } si . Consider Gi+1
si+1 . First, atomic subgoals achieved ( ai , Gi0 ), namely Gi+1 \ Gi0 , tackled
LH: () {c | c LH } = {c | c Gi+1 \ Gi0 } si . planning delete-free
immediately yields {c | c Gi+1 \ Gi0 } si+1 . Second, atomic subgoals
achieved ( ai , Gi0 ), namely Gi0 , clearly true si+1 well: simply
add( a[ Gi0 ]) = {c | c Gi0 }.
remains show ai [ Gi0 ] applicable si . definition C , preconS
dition [ cGi0 (pre( a) (c \ add( a)))]C . R(c, a) = pre( a) (c \ add( a)), equals

[ cGi0 R(c, a)]C . latter exactly pre( ai [ Gi0 ]) = {c | c RH }, done
() {c | c RH } si concludes proof.
Theorem 5 Let = (F , A, , G) planning task, C set conjunctions containing
CFF sequentialized form relaxed
singleton conjunctions. nc-C-relaxed plan nc
C
plan nc .
CFF , CFF = (G C ),
Proof: Denote = (F , A, , G). Recall definition nc
partial function conjunction sets G defined G C satisfies ( G ) =


c G : c



C
0
0
0
(( G \ G ) Gr ) {( a, G )} A,

6= G 0 {c | c G, R(c, a) 6= , hC ( R(c, a)) = hC (c) 1},


hC ( Gr0 ) = hC ( G 0 ) 1
else

Gr0 defined Gr0 := { R(c, a) | c G 0 }.
proof Theorem 4 remains valid exactly written, except RH =
C,
{ R(c, a) | c Gi0 }C . need show ai [ Gi0 ] applicable si . definition nc
precondition {pre( a) (c \ add( a)) | c Gi0 }C . R(c, a) = pre( a) (c \ add( a)),
equals { R(c, a) | c Gi0 }C . latter exactly pre( ai [ Gi0 ]) = {c | c RH },
done () {c | c RH } si .
Theorem 6 Let = (F , A, , G) planning task, C set conjunctions containing
singleton conjunctions. C-relaxed plan exists nc-C-relaxed plan exists
hC < .
Proof: Denote = (F , A, , G). show claim two parts, (a) C-relaxed plan
exists hC < , (b) nc-C-relaxed plan exists hC < .
consider first part (a).
direction corollary Theorems 3 4: hC = , Theorem 3
h1 (C ) = relaxed plan C exist, Theorem 4 implies
C-relaxed plan cannot exist either.
direction, say hC < . need show exists C-relaxed
plan. end, consider simpler version equation defining CFF (Equation 11), restricting choice G 0 singletons G 0 = {c}. easy simplifications,
get: ( G ) =

c G : c

(( G \ {c}) R(c, a)C ) {( a, {c})} A,

c G, R(c, a) 6= , hC ( R(c, a)) = hC (c) 1 else
319

fiF ICKERT & H OFFMANN & TEINMETZ

Recall, equations below, function partial, defines Crelaxed plan defined (the atomic conjunctions of) global goal G C .
Observe that, previous equation, always support single atomic
subgoal anyhow, need recurse sets atomic subgoals. instead
recurse single atomic subgoals, replace initializing recursive calls,
sets atomic subgoals, union call elements. results

characterization given Equation 12: CFF = cG C (c), partial
function conjunctions c satisfies (c) =


cI

0 ) {( a, { c })} A,

(
c
0
C
c R(c,a)
R(c, a) 6= , hC ( R(c, a)) = hC (c) 1 else
Note similarity Equation 8 (page 285): back common notation
relaxed plan extraction (over C instead singleton facts), extracting best supporters
one-by-one.
Towards proving claim, transform equation way making link
hC obvious. Instead union operations initial recursive calls,
enumerate atomic subgoals contained given set facts (G respectively R(c, a)),
recurse directly fact sets, G, introduce third case performing union
atomic conjunctions contained G. hence get characterization given
Equation 13: CFF = (G), partial function fact sets G satisfies
(G) =


GI



( R( G, a)) {( a, G )} A,
R( G, a) 6= , hC ( R( G, a)) = hC ( G ) 1 G C



0
else
G 0 G,G 0 C ( G )
Compare Equation 3 defining hC : h( G ) =

GI
0
1 + minaA,R(G,a)6= h( R( G, a)) G C

maxG0 G,G0 C h( G 0 )
else
bottom cases equations obvious correspondence. G h( G ) < ,
middle cases correspondence, too, sense choice action occurrences Equation 13 exactly choice minimizing actions Equation 3: hC ( G ) <
, actions minimizing 1 + hC ( R( G, a)) hC ( R( G, a)) = hC ( G ) 1.
So, finite-value subgoals, subgoaling structure two equations coincides,
particular, hC < , exists solution Equation 13 defined
G . Therefore, Equation 12 solution defined c G C . Equation 12 captures
restricted version CFF , applying conditions smaller choice action occurrences, implies exists C-relaxed plan desired, concluding part (a)
proof.
part (b), direction follows manner corollary Theorems 3 5. direction also follows manner, difference lies definition Gr0 , singleton G 0 difference disappears: G 0 = {c}
320

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

CFF , vs. G 0 = R ( c, ) CFF . atomic conjunctions conwe Gr0 = { R(c, a)} nc
r
tained expressions same. Hence, restricting choice G 0 singletons,
CFF simplifies Equation 12 exactly above,
equation defining nc
proof identical.

Example 9 construct example advantage select smaller set supported
subgoals G 0 , even though larger set strict superset would feasible. construction
extends abstract example (Example 3).
Consider planning task = (F , A, , G) defined follows. F = { g1 , g2 , p, q1 , q2 , r1 ,
N
1
N
1
r2 , r10 , r20 , q10 , . . . , q10 , q20 , . . . , q20 }, = {q1 , q2 }, G = { g1 , g2 }. consists of: (abbreviating
action form : precondition facts positive negative effect literals)
Achieving goals. a[ g1 ]: p, q1 g1 . a[ g2 ]: p, q2 g2 .
Achieving p. a1 [ p]: r1 p. a2 [ p]: r2 p.
Achieving preconditions p. a[r1 ]: r10 r1 , q2 . a[r2 ]: r20 r2 , q1 .
Achieving preconditions ri . a[r10 ]: r10 . a[r20 ]: r20 .
1

N

r2 , q1 . 1 N: a[q10 ]: q10 .

1

N

r1 , q2 . 1 N: a[q20 ]: q20 .

Reachieving q1 . a[r2 , q1 ]: q10 , . . . , q10
Reachieving q2 . a[r1 , q2 ]: q20 , . . . , q20

construction, achieving r1 q1 takes 2 steps, achieving r1 q2 takes N + 1 steps;
symmetrically, achieving r2 q2 takes 2 steps, achieving r2 q1 takes N + 1 steps.
CFF must
use properties construct case smallest-possible nc-C-relaxed plan nc
use a1 [ p] achieve p g1 , use a2 [ p] achieve p g2 , thus relying non-maximal sets
best-supported subgoals relaxed plan extraction. arguments apply C-relaxed
plans CFF which, example, behave identically.
Say C contains singleton conjunctions well c pq1 = p q1 , c pq2 = p q2 ,
cr1q2 = r1 q2 , cr2q1 = r2 q1 .
CFF =
Constructing nc-C-relaxed plan according Definition 3 (page 287), start nc
({ g1 , g2 }) requiring support two (atomic-singleton-conjunction) goal facts.
done ( a[ g1 ], { g1 }) ( a[ g2 ], { g2 }) respectively. using bottom
case Equation 11, get recursive subgoal G = {c pq1 , c pq2 } (as well subsumed
conjunctions p, q1 , q2 irrelevant following discussion). a1 [ p] a2 [ p]
support conjunctions. Indeed, best supporter
conjunctions.
see this, observe first hC (c pq1 ) = 3 e. g. via a[r10 ], a[r1 ], a1 [ p]; hC (c pq2 ) = 3
e. g. via a[r20 ], a[r2 ], a2 [ p]; clear already, ai [ p] best supporter c pqi . Regarding cross-over combinations, R(c pq2 , a1 [ p]) = cr1q2 ; a[r1 ] deletes q2 ,
1
N
regressed via a[r1 , q2 ], leading subgoal {q20 , . . . , q20 } whose hC value clearly 1,
hC ( R(c pq2 , a1 [ p])) = 2 = hC (c pq2 ) 1 desired. Similarly, R(c pq1 , a2 [ p]) = cr2q1 whose hC
value 2 desired.
So, subgoal G = {c pq1 , c pq2 }, choose among six action occurrences, using either
0 := {c
0
0
a1 [ p] a2 [ p] support either G12
pq1 , c pq2 }, G1 : = { c pq1 }, G2 : = { c pq2 }.
321

fiF ICKERT & H OFFMANN & TEINMETZ

Now, cross-over combinations suitable far hC concerned,
suitable obtain shortest relaxed plans. Say include c pq2 supported subgoal set

a1 [ p]. regressed subgoal cr1q2 , requiring us use a[r1 , q2 ] well N actions a[q20 ],
N + 1 actions total. hand, using a1 [ p] support c pq1 , regressed subgoal
{r1 , q1 } two singleton conjunctions supported using action occurrences
( a[r10 ], {r10 }), ( a[r1 ], {r1 }) (recall q1 true initially). Similarly, using a2 [ p] support
c pq1 incurs cost N + 1 using a2 [ p] support c pq2 incurs cost 2.
Getting back choice subgoal G = {c pq1 , c pq2 }, use ( a1 [ p], G10 ),
thereafter use ( a2 [ p], G20 ) get nc-C-relaxed plan cost 8: 2 previously achieving
facts gi , 2 two occurrences achieving c pqi , 4 afterwards achieving facts ri . Similarly
use ( a2 [ p], G20 ) first. If, however, start action occurrence, incur
cost N + 1 least one c pqi , exceeding optimal cost 8 sufficiently large N.
0 ) feasible, supports strict superset
particular, action occurrence ( a1 [ p], G12
atomic subgoals supported ( a1 [ p], G10 ), leads strictly larger relaxed plan.
Theorem 7 C-SubgoalSupport NP-complete.
Proof: Membership obvious guess check. hardness, show polynomial
reduction Hitting Set problem set B subsets b E finite set
elements E, question whether exists hitting set size L.
Denote E = {e1 , . . . , en }. construct planning task = (F , A, , G) follows.
F := E { p0 , p1 , p2 } { g1 , . . . , gn }, := { p0 }, G := { g1 , . . . , gn }. action set contains a[ p1 ] precondition p0 , add p1 , empty delete, well a[ p2 ] precondition p1 , add p2 , empty delete. action set furthermore contains action a[ei ]
every ei E, pre( a[ei ]) = { p0 }, add( a[ei ]) = {ei }, del( a[ei ]) = { p2 } {e j |
ex. b B : {ei , e j } b}. Finally, action set contains actions a[ g1 ], . . . , a[ gn ]
pre( a[ gi ]) = {ei , p2 }, add( a[ gi ]) = { gi }, delete empty. set C := {{ p} | p
F } B {{ei , p2 } | ei E}.
think hC terms (C-)relaxed planning graph (RPG), layer
corresponds conjunctions g hC ( g) t. None conjunctions b B
achieved, exists action b regressed. However,
facts ei E achieved isolation. Consider layer 1 RPG. key property
exploit (*) subset E0 = {e1 , . . . , ek } E feasible layer 1, i. e. hC ( E0 ) 1,
iff exist b B s.t. b E0 . right left, b E0 E0 infeasible
simply hC (b) = . Vice versa, say exist b B s.t. b E0 .
c0 E0 , c0 C set singleton conjunctions {ei }, get hC ( E0 ) = 1
{ei } achieved single action.
RPG layer 1, apply a[ p2 ]. ei already present, get
conjunctions {e1 , p2 }, . . . , {en , p2 } layer 2. this, a[ gi ] actions become feasible,
goal reached layer 3.
Consider relaxed plan extraction. get goal, must select a[ gi ] actions.
Say selected sequence. get subgoal {{e1 , p2 }, . . . , {en , p2 }}
layer 2 (plus subsumed singleton conjunctions, omit readability).
action regressed a[ p2 ]: recall a[ei ] actions delete
p2 . maximal subset G 0 := {{ei1 , p2 }, . . . , {eik , p2 }} {{e1 , p2 }, . . . , {en , p2 }}
choose support?
322

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

subset yields new generated subgoal {ei1 , . . . , eik , p1 } RPG layer 1.
p1 achieved a[ p1 ] interact anything critical: hC ({ei1 , . . . , eik ,
p1 }) = hC ({ei1 , . . . , eik }). Denote E0 := {ei1 , . . . , eik }. action occurrence ( a[ p2 ], G 0 )
C-feasible RPG layer 1 iff E0 feasible RPG layer 1. (*), latter case iff
exist b B s.t. b E0 . then, consider E \ E0 . construction,
hitting set iff E0 feasible: E \ E0 hitting set b fully contained E0 ,
b fully contained E0 E \ E0 must hit every b. Setting K := n L, thus get
exists C-feasible G 0 | G 0 | K iff exists feasible E0 | E0 | n L
iff exists hitting set size n (n L) = L. concludes proof.

370

370

350

350

330

330

310

310

290

290

270

270
CFF
hnc
hCFF

250
230
210
190
170
150
21

250
230

C)
hFF (ce
FF
h (C )
hCFF
| G 0 |=1A
hCFF
| G 0 |=1
hFF arb
hFF rnd

22

23

210
190
170
24

25

26

27

28

29

210

150
21

22

(a) Search-only coverage

23

24

25

26

27

28

29

210

(b) Inclusive coverage

Figure 7: Total coverage.

Appendix B. Coverage Including C-Learning Time Limit
give coverage plots Section 5.4, imposing 30-minute limit Clearning search together (inclusive Figures 7, 8, 9). convenience, also
include search-only plots Section 5.4.

References
Alcazar, V., Borrajo, D., Fernandez, S., & Fuentetaja, R. (2013). Revisiting regression
planning. Rossi, F. (Ed.), Proceedings 23rd International Joint Conference
Artificial Intelligence (IJCAI13), pp. 22542260. AAAI Press/IJCAI.
Baier, J. A., & Botea, A. (2009). Improving planning performance using low-conflict relaxed
plans. Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings
19th International Conference Automated Planning Scheduling (ICAPS09), pp. 10
17. AAAI Press.
323

fiF ICKERT & H OFFMANN & TEINMETZ

20

20

15

15

10

10

5

5

01
2

22

23

24

25

26

27

28

29

01
2

210

22

Maintenance search-only
20

15

15

10

10

5

5

22

23

24

25

26

27

28

24

25

26

27

28

29

210

29

210

Maintenance inclusive

20

01
2

23

29

01
2

210

Parcprinter search-only

22

23

24

25

26

27

28

Parcprinter inclusive

Figure 8: Coverage individual domains.
Bonet, B., & Geffner, H. (1999). Planning heuristic search: New results. Biundo, S.,
& Fox, M. (Eds.), Proceedings 5th European Conference Planning (ECP99), pp.
6072. Springer-Verlag.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129(12),
533.
Bonet, B., & Helmert, M. (2010). Strengthening landmark heuristics via hitting sets.
Coelho, H., Studer, R., & Wooldridge, M. (Eds.), Proceedings 19th European Conference Artificial Intelligence (ECAI10), pp. 329334, Lisbon, Portugal. IOS Press.
Bylander, T. (1994). computational complexity propositional STRIPS planning. Artificial Intelligence, 69(12), 165204.
Cai, D., Hoffmann, J., & Helmert, M. (2009). Enhancing context-enhanced additive
heuristic precedence constraints. Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings 19th International Conference Automated Planning
Scheduling (ICAPS09), pp. 5057. AAAI Press.
Coles, A. J., Coles, A., Fox, M., & Long, D. (2013). hybrid LP-RPG heuristic modelling
numeric resource flows planning. Journal Artificial Intelligence Research, 46, 343
412.
324

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

20

20

15

15

10

10

5

5

01
2

22

23

24

25

26

27

28

29

01
2

210

22

23

Tetris search-only
20

15

15

10

10

5

5

22

23

24

25

26

27

25

26

27

28

29

210

28

29

210

Tetris inclusive

20

01
2

24

28

29

01
2

210

Thoughtful search-only

22

23

24

25

26

27

Thoughtful inclusive

Figure 9: Coverage individual domains.
Do, M. B., & Kambhampati, S. (2001). Sapa: domain-independent heuristic metric temporal planner. Cesta, A., & Borrajo, D. (Eds.), Proceedings 6th European Conference Planning (ECP01), pp. 109120. Springer-Verlag.
Domshlak, C., Hoffmann, J., & Katz, M. (2015). Red-black planning: new systematic
approach partial delete relaxation. Artificial Intelligence, 221, 73114.
Fox, M., & Long, D. (2001). Hybrid STAN: Identifying managing combinatorial optimisation sub-problems planning. Nebel, B. (Ed.), Proceedings 17th International Joint Conference Artificial Intelligence (IJCAI-01), pp. 445450, Seattle, Washington, USA. Morgan Kaufmann.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning stochastic local search
temporal action graphs. Journal Artificial Intelligence Research, 20, 239290.
Haslum, P. (2009). hm ( P) = h1 ( Pm ): Alternative characterisations generalisation hmax hm . Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.),
Proceedings 19th International Conference Automated Planning Scheduling
(ICAPS09), pp. 354357. AAAI Press.
325

fiF ICKERT & H OFFMANN & TEINMETZ

Haslum, P. (2012). Incremental lower bounds additive cost planning problems.
Bonet, B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings 22nd
International Conference Automated Planning Scheduling (ICAPS12), pp. 7482.
AAAI Press.
Haslum, P., & Geffner, H. (2000). Admissible heuristics optimal planning. Chien, S.,
Kambhampati, R., & Knoblock, C. (Eds.), Proceedings 5th International Conference
Artificial Intelligence Planning Systems (AIPS-00), pp. 140149, Breckenridge, CO.
AAAI Press, Menlo Park.
Helmert, M. (2004). planning heuristic based causal graph analysis. Koenig, S.,
Zilberstein, S., & Koehler, J. (Eds.), Proceedings 14th International Conference
Automated Planning Scheduling (ICAPS04), pp. 161170, Whistler, Canada. Morgan Kaufmann.
Helmert, M. (2006). Fast Downward planning system. Journal Artificial Intelligence
Research, 26, 191246.
Helmert, M., & Geffner, H. (2008). Unifying causal graph additive heuristics.
Rintanen, J., Nebel, B., Beck, J. C., & Hansen, E. (Eds.), Proceedings 18th International Conference Automated Planning Scheduling (ICAPS08), pp. 140147. AAAI
Press.
Hoffmann, J. (2005). ignoring delete lists works: Local search topology planning
benchmarks. Journal Artificial Intelligence Research, 24, 685758.
Hoffmann, J. (2011). Analyzing search topology without running search: connection causal graphs h+ . Journal Artificial Intelligence Research, 41,
155229.
Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation
heuristic search. Journal Artificial Intelligence Research, 14, 253302.
Hoffmann, J., Steinmetz, M., & Haslum, P. (2014). take render h+ ( c )
perfect?. ICAPS 2014 Workshop Heuristics Search Domain-Independent
Planning (HSDIP14).
Keyder, E., & Geffner, H. (2008). Heuristics planning action costs revisited.
Ghallab, M. (Ed.), Proceedings 18th European Conference Artificial Intelligence
(ECAI-08), pp. 588592, Patras, Greece. Wiley.
Keyder, E., & Geffner, H. (2009). Trees shortest paths vs. Steiner trees: Understanding
improving delete relaxation heuristics. Boutilier, C. (Ed.), Proceedings
21st International Joint Conference Artificial Intelligence (IJCAI 2009), pp. 17341739,
Pasadena, California, USA. Morgan Kaufmann.
Keyder, E., Hoffmann, J., & Haslum, P. (2012). Semi-relaxed plan heuristics. Bonet,
B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings 22nd International Conference Automated Planning Scheduling (ICAPS12), pp. 128136. AAAI
Press.
Keyder, E., Hoffmann, J., & Haslum, P. (2014). Improving delete relaxation heuristics
explicitly represented conjunctions. Journal Artificial Intelligence Research,
50, 487533.
326

fiC OMBINING h+ hm : IRECT C HARACTERIZATION

Marques-Silva, J., & Sakallah, K. (1999). GRASP: search algorithm propositional
satisfiability. IEEE Transactions Computers, 48(5), 506521.
McDermott, D. V. (1999). Using regression-match graphs control search planning.
Artificial Intelligence, 109(1-2), 111159.
Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering
efficient SAT solver. Proceedings 38th Conference Design Automation
(DAC-01), Las Vegas, Nevada, USA. IEEE Computer Society.
Richter, S., & Westphal, M. (2010). LAMA planner: Guiding cost-based anytime planning landmarks. Journal Artificial Intelligence Research, 39, 127177.
Steinmetz, M., & Hoffmann, J. (2016). Towards clause-learning state space search: Learning
recognize dead-ends. Schuurmans, D., & Wellman, M. (Eds.), Proceedings
30th AAAI Conference Artificial Intelligence (AAAI16). AAAI Press.
Valenzano, R. A., Sturtevant, N. R., Schaeffer, J., & Xie, F. (2014). comparison
knowledge-based GBFS enhancements knowledge-free exploration. Chien,
S., Do, M., Fern, A., & Ruml, W. (Eds.), Proceedings 24th International Conference
Automated Planning Scheduling (ICAPS14). AAAI Press.
van den Briel, M., Benton, J., Kambhampati, S., & Vossen, T. (2007). LP-based heuristic
optimal planning. Bessiere, C. (Ed.), Proceedings Thirteenth International
Conference Principles Practice Constraint Programming (CP07), Vol. 4741
Lecture Notes Computer Science, pp. 651665. Springer-Verlag.

327

fiJournal Artificial Intelligence Research 56 (2016) 119-152

Submitted 07/15; published 05/16

Budgeted Optimization Constrained Experiments
Javad Azimi

jaazimi@microsoft.com

Microsoft, Sunnyvale, CA, USA

Xiaoli Z. Fern

xfern@eecs.oregonstate.edu

School EECS, Oregon State University

Alan Fern

afern@eecs.oregonstate.edu

School EECS, Oregon State University

Abstract
Motivated real-world problem, study novel budgeted optimization problem
goal optimize unknown function f () given budget requesting
sequence samples function. setting, however, evaluating function
precisely specified points practically possible due prohibitive costs. Instead,
request constrained experiments. constrained experiment, denoted Q,
specifies subset input space experimenter sample function from.
outcome Q includes sampled experiment x, function output f (x). Importantly,
constraints Q become looser, cost fulfilling request decreases,
uncertainty location x increases. goal manage trade-off selecting
set constrained experiments best optimize f () within budget. study
problem two different settings, non-sequential (or batch) setting set
constrained experiments selected once, sequential setting experiments
selected one time. evaluate proposed methods settings using
synthetic real functions. experimental results demonstrate efficacy
proposed methods.

1. Introduction
work motivated experimental design problem optimizing power output
nano-enhanced microbial fuel cells. Microbial fuel cells (MFCs) (Bond & Lovley, 2003;
Fan, Hu, & Liu, 2007; Park & ZeikusG, 2003; Reguera, McCarthy, Mehta, Nicoll, Tuominen,
& Lovley, 2005) use micro-organisms break organic matter generate electricity.
particular MFC design, critical optimize biological energetics
microbial/electrode interface system, research shown depend strongly
surface properties anodes (Park & ZeikusG, 2003; Reguera et al., 2005).
motivates design nano-enhanced anodes, nano-structures (e.g., carbon nanowire) grown anode surface improve MFCs power output. Unfortunately,
little understanding interaction various possible nano-enhancements
MFC capabilities different micro-organisms. Thus, optimizing anode design
particular application largely guesswork. goal develop algorithms aid
process.
Traditional experimental design, Bayesian optimization response surface methods
(Myers, Montgomery, & Anderson-Cook, 1995; Jones, 2001; Brochu, Cora, & De Freitas,
2010) commonly assume experimental inputs specified precisely attempt
c
2016
AI Access Foundation. rights reserved.

fiAzimi, Fern, & Fern

optimize design requesting specific experiments. example, requesting anode
tested nano-wire specific length density. However, parameters
unlike usual experimental control parameters (such temperature) easily set
precise values. Manufacturing nano-structures rather art difficult
achieve precise parameter setting. Instead, practical request constrained
experiments, place constraints parameters. example, may specify
intervals length density nano-wire. Given request, nano-materials
satisfy given set constraints produced cost, typically
increase tighter constraints.
Note possible alternative requesting constrained experiments would treat
nano-structure manufacturing parameters experimental inputs. inputs
precisely specified, hence standard methods used. However, approach
yield satisfactory solution problem. particular, mapping
manufacturing parameters produced nano-structures extremely noisy. makes
difficult find manufacturing parameters optimize expected MFC power
output. Further, scientists primarily interested learning nano-structure
properties optimize MFC power output, rather knowing specific manufacturing parameters, vary significantly lab lab. Thus focus directly
optimizing space nano-structure properties via constrained experiments.
Based motivation, paper, study associated budgeted optimization problem where, given budget, goal optimize unknown function f ()
requesting set constrained experiments. Solving problem requires careful consideration trade-off cost uncertainty constrained experiment:
weakening constraints lower cost experiment, increase uncertainty
location next observation. Prior work experimental design, stochastic
optimization, active learning directly apply constrained experiments
typically assume precise experiments.
problem formulated theoretical framework partially observable
Markov decision processes (POMDPs), optimal solution corresponds finding
optimal POMDP policy. However, solving optimal even near-optimal policies
computationally intractable, even case traditional optimization problems.
led researchers develop variety myopic policies context traditional
optimization, observed achieve good performance, even comparison
sophisticated, less myopic strategies (Moore & Schneider, 1995; Jones, 2001; Madani,
Lizotte, & Greiner, 2004; Brochu et al., 2010).
problem considered two different settings, non-sequential sequential.
non-sequential setting, also referred batch setting, constrained
experiments must selected once. setting appropriate applications
multiple experimental facilities experiments time consuming
run sequentially. contrast, sequential setting allows us request one constrained
experiment time, wait outputs previous experiments making
next request. sequential setting advantage allows us use maximum
available information selecting experiment, generally expected outperform
non-sequential setting total running time concern. paper,
study settings.
120

fiBudgeted Optimization Constrained Experiments

non-sequential setting, introduce non-decreasing submodular objective function select batch constrained experiments within given budget. given set
constrained experiments, objective measures expected maximum output. present
computationally efficient greedy algorithm approximately optimizes proposed objective.
sequential setting, build set classic myopic policies
shown achieve good performance traditional optimization (Moore & Schneider, 1995;
Jones, 2001; Madani et al., 2004; Brochu et al., 2010) introduce non-trivial extensions
make applicable constrained experiments.
present experimental evaluations settings using synthetic functions
functions generated real-world experimental data. results indicate that,
settings, proposed methods outperform competing baselines.
remainder paper organized follows. introduce background
related work Section 2. Section 3 describes problem setup. non-sequential
setting studied Section 4. Section 5 introduces proposed methods selecting constrained experiments sequential setting. Section 6 presents empirical evaluation
proposed methods. conclude paper discuss future work Section 7.

2. Background Related Work
Given unknown black box function costly evaluate, interested finding
extreme point (minimizer maximizer) function via small number function
evaluations. solve problem, Bayesian Optimization (BO) approaches
heavily studied (Jones, 2001; Brochu et al., 2010) demonstrated significant promises.
two key components basic framework Bayesian Optimization. first
component probabilistic model underlying function built based prior
information (i.e., existing observed experiments). Gaussian process (GP) regression
widely used literature BO purpose (Brochu et al., 2010).
unobserved point, GP models function output normal random variable,
mean predicting expected function output point variance capturing
uncertainty associated prediction.
second key component BO selection criterion used determine
experiment select based learned model. existing literature, various
selection criteria proposed combination exploring
unexplored input space function (i.e., areas high variance) exploiting
promising area (i.e., area large mean). selection criterion either sequential
(Jones, 2001; Locatelli, 1997; Moore, Schneider, Boyan, & Lee, 1998; Srinivas, Krause,
Kakade, & Seeger, 2010) one experiment asked iteration nonsequential (Schonlau, 1997; Azimi, Fern, & Fern, 2010; Ginsbourger, Riche, & Carrarog,
2010) batch experiments requested iteration.
review successful sequential selection criteria literature
BO. One first sequential policies based selecting sample maximum
probability improving (MPI) best current observation, ymax (assuming want
maximize f ), given margin (Elder, 1992; Stuckman, 1988). Let best current
observation ymax . goal MPI select next experiment highest
121

fiAzimi, Fern, & Fern

probability producing output smaller (1 + )ymax . One issue approach
performance often sensitive value margin parameter (Jones,
2001). small values , MPI focus promising area first
move onto unexplored areas. contrast, large values , MPI primarily explore
converge slowly. Selecting proper value challenging practice.
maximum expected improvement (MEI) (Locatelli, 1997) criterion avoids issue
selects experiment directly maximizes expected improvement current
best observation. Heuristics based upper-confidence bounds also explored
(Srinivas et al., 2010), shown competitive MEI given appropriate
parameter selection. However, selecting best parameters particular application
empirically challenge. alternative approach received attention
Thompson Sampling (Chapelle & Li, 2011), randomized strategy managing
exploration-exploitation trade-off. approach first samples underlying uncertainty,
case unknown function f , returns experiment maximizes
sample. work, focused extending deterministic methods
BO constrained experiments. Extending alternatives Thompson sampling
potentially interesting future direction.
Recently, researchers begun consider non-sequential batch Bayesian optimization (Azimi et al., 2010; Ginsbourger et al., 2010; Desautels, Krause, & Burdick, 2014),
selects multiple experiments once. Non-sequential BO considered appropriate applications need capability run multiple experiments
simultaneously. One non-sequential approach (Azimi et al., 2010) selects k > 1 experiments
matching behavior executing given sequential policy (e.g., MEI) k
steps. another non-sequential approach (Ginsbourger et al., 2010), authors tried
select batch experiments lead highest expected improvement. However, shown expected improvement set jointly normal random
variables closed form solution k > 2, solved efficiently using numerical methods. Instead, simple heuristics proposed approximate
expected improvement select batch accordingly. recently, algorithm
based upper-confidence bounds also introduced (Desautels et al., 2014),
computationally cheaper prior work requires careful parameter selection.
Note aforementioned approaches assume unknown function
aim optimize sampled precisely specified points, making unsuitable
tasks motivating nano application, sampling function exact locations impractical. proposed sequential approaches paper, previously
presented less detail (Azimi et al., 2010). paper, provide complete
formal description sequential approaches additional empirical results.
addition, introduce evaluate batch selection algorithm chooses batch
constrained experiments iteration.

3. Problem Setup
Let X Rd d-dimensional input space, dimension bounded [Ai ,
Bi ]. often refer elements X experiments. assume unknown
real-valued function f (x) : X <, represents expected value dependent
122

fiBudgeted Optimization Constrained Experiments

variable running experiment x. motivating application, f (x) expected
power output produced particular nano-structure x. Conducting experiment x
produces noisy outcome = f (x) + , noise term.
traditional optimization settings (Jones, 2001; Brochu et al., 2010), goal find
x X approximately optimizes f () requesting set experiments observing
outcomes. Since sampling function exactly specified points prohibitively
expensive application, request constrained experiments, define subset
experiments X . Specifically, define constrained experiment hyper-rectangle
X , denoted Q = (q1 , q2 , , qd ), qi = (ai , bi ) Ai ai < bi Bi defines
range values considered admissible input dimension i. Note
computational reasons, work consider discretized input space,
input dimension divided equal-sized intervals. such, constrained experiment Q
indicate dimension first (specified ai ) last (specified bi )
intervals included hyper-rectangle. remainder paper,
interchangeably use terms constrained experiment, hyper-rectangle query.
Given constrained experiment Q, experimenter first construct experiment
x (we assume x precisely measured produced) satisfies
given constraints Q, run experiment, return noisy observation f (x). Note
x random variable given Q, assume conditional distribution, px (x|Q),
known priori part problem inputs. precisely, query Q,
experimenter return 2-tuple (x, y), where:
x = (x1 , x2 , , xd ) experiment satisfies constraints Q,
noisy observation function f () x, = f (x) + .
practice, cost c fulfilling constrained experiment variable depending size hyper-rectangle. particular, higher cost associated
tighter constraints smaller hyper-rectangles. assume cost modeled
deterministic function fc (), provided us part inputs. example,
motivating application, fc () dominated time required produce nano
material satisfies given constraints, inversely correlated size
constraints. addition, must operate within total budget B. Thus, objective
find set queries within budget B leads best estimate maximizer
function input space X .
summarize, inputs problem include set prior experiments (which
could potentially empty), budget B, deterministic cost function fc () fulfilling
constrained experiment Q, conditional probability density function px (x|Q)
specific experiment x generated given constrained experiment Q.
Given inputs, task select set constrained experiments Q = {Q1 , Q2 ,
, Qk } whose total cost within budget B. Running selected constrained experiments
result set k tuples (xi , yi )ki=1 , must determine final output
x {x1 , . . . , xk }, prediction maximizer f () among observed
experiments. Note restrict returning experiment actually
observed, even cases might predict non-observed experiment
better. formulation matches objective motivating application produce
123

fiAzimi, Fern, & Fern

good nano-structure x using given budget, rather make prediction
nano-structure might good.
study problem two different settings, non-sequential (or batch) sequential.
non-sequential setting, must decide entire set queries time.
contrast, sequential setting requests constrained experiments sequentially one
time: receiving result previous request, another query selected
presented experimenter. procedure repeated reach budget limit.
following two sections, introduce proposed solutions settings.

4. Non-sequential Approach
section, consider non-sequential setting, must select entire
batch queries Q within given budget B once. Note general batches
multi-sets repeated queries, may desirable noisy settings.
also called non-adaptive (Goel et al., 2006; Krause et al., 2008) Batch (Azimi et al.,
2010) setting. setting commonly used applications must start multiple
experiments cannot wait outputs previous queries decide
next queries (Tatbul et al., 2003).
4.1 Objective Function
Let QB set feasible solutions Q QB total cost Q
greater budget B. goal find optimal multi-set queries
Q = {Q1 , Q2 , , Qk } QB . define mean optimal, let us consider
outcome queries, set tuples: (xi , yi ) , = 1, 2, . . . , k. xi
experiments produced experimenter given queries yi represent
experimental output (i.e., noisy observation f (xi )). select final output
x {x1 , . . . , xk } believed achieve maximal f () value. such,
Q QB , measure good Q based maximal value resulting
set queries. Specifically, captured by:
h
h
ii

fifi
J(Q) = E(x1 ,,x|Q| ) E(y1 ,,y|Q| ) max y1 , . . . , y|Q| fiD, (x1 , , x|Q| ) ,
(1)
first expectation taken possible values xi s, represent
individual experiments created query Q, second expectation taken
possible yi s, represents experimental outcomes xi s. mentioned
previously, xi distributed according px (xi |Qi ), part inputs.
distribution yi given xi depends posterior distribution f () given D.
work, use Gaussian processes model distribution f (). Consequently,
set yi jointly normal conditioned xi D.
Since input space discretized, enumerate possible constrained experiments denote QM = {Q1 , Q2 , ..., QM }, total number possible
constrained experiments, let c1 , . . . , cM corresponding cost (i.e., ci = fc (Qi )).
Let = {1, ..., } subset indices QS denote collection queries
indexed S, i.e., QS = {Qi : S}. goal stated selecting
corresponding QS maximizes objective (Equation 1) subject constraint
124

fiBudgeted Optimization Constrained Experiments

P

ci B. Unfortunately, optimizing objective intractable due combinatorial nature problem exponentially many possible solutions consider.
reformulate objective demonstrate non-decreasing submodular set
function introduce algorithm approximation guarantee.
Specifically, consider slightly different equivalent view querying
process. far view chosen, query Q QS result
experiment x, viewed random sample drawn distribution
px (x|Q) (note work px uniform within hyper-rectangle defined
query). process point view, clearly matter whether random
draw happens Q chosen, beginning whole process Q
chosen. Following reasoning, could assume every possible query QM ,
random experiment drawn beginning process results stored
used later selected. Let XM = {x1 , . . . , xM } denote random variables
representing outcome random draw Q1 , ..., QM respectively. objective
reformulated as:
h
h
ii

fifi
J(S) = EXS E(y1 ,...,y|S| ) max y1 , . . . , y|S| fiD, XS ,
(2)

XS = {xi : S} subset XM defined S, yi noisy
outcomes xi XS .
4.2 Approximation Algorithm
Since standard batch Bayesian Optimization special case optimizing J(S), hardness optimizing J(S) follows NP-hardness Bayesian Optimization. Thus,
show J(S) non-decreasing submodular set function present algorithm bounded approximation factor.
Definition 1. Suppose finite set, g : 2S R+ submodular set function
S1 S2 x \ S2 , holds g(S1 {x}) g(S1 ) g(S2 {x}) g(S2 ).
Thus, set function submodular adding element smaller set provides
less improvement adding element larger superset. Also, set function
non-decreasing set element x g(S) g(S {x}).
show J(S) submodular, rewrite objective function defining
JXM (S) inner expectation Equation 2 fixed realization random variable
XM :
h


fifi
JXM (S) = E(y1 ,...,y|S| ) max y1 , . . . , y|S| fiD, XS .
Lemma 1. given XM , JXM (S), returns expected maximum set
jointly distributed random variables, monotonically non-decreasing submodular set
function.
proof Appendix.
proposed objective function, J(S) = EXM [JXM (S)] takes expectation JXM (S)
possible values XM . JXM (S) non-decreasing, easy verify
J(S) also non-decreasing. note set submodular functions
125

fiAzimi, Fern, & Fern

closed expectation, thus conclude proposed objective, J(S),
non-decreasing submodular function.
present proposed algorithm optimizing J(S). inputs algorithm include set possible constrained experiments, QM = {Q1 , ..., QM }, associated costs
P c1 , ..., cM , total budget B, output subset = {1, ..., }
ci B. first introduce simple greedy algorithm, begins
initial empty set = greedily adds one constrained experiment (its index)
time total cost reaches B. step, let current set C
total cost S, greedy algorithm selects index that:
= argmax
iS;c
/
BC

J(S i) J(S)
.
ci

(3)

words, step, algorithm greedily selects new constrained experiment
within budget leads largest cost-normalized improvement
objective.
known simple greedy algorithm bounded approximation
factor (Khuller, Moss, & Naor, 1999). Previous work (Khuller et al., 1999; Krause &
Guestrin, 2005) introduced small change greedy algorithm provides us
bounded approximation factor. particular, one needs consider, alternative
output greedy algorithm, single query within budget achieves
best objective value (denoted Sa ). comparing alternative output
greedy algorithm, guaranteed achieve bounded approximation factor.
complete algorithm summarized Algorithm 1. approximation bound
algorithm follows following theorem.
Theorem 1. (Khuller et al., 1999) Let J() monotonically non-decreasing submodular
set function J() = 0, output Algorithm 1. Suppose OPT
optimal solution, following bound guaranteed
"

|S |+1 #
1
1
J(S )
1 1
J(OPT)
2
|S | + 1


1 e1

J(OPT).
2
e


(4)

dominating factor run time linear dependence , number
possible queries. Note discretized setting consider, exponential
number dimensions. scientific application domains motivate work,
number dimensions typically small (e.g., 2 3). However, working
fine resolution discretization, computation time still significant. address
issue, next section describe simple strategy soundly pruning candidate
queries consideration, yields significant speedups. Problems significantly
higher dimensions, however, require continuous rather discretized optimization.
126

fiBudgeted Optimization Constrained Experiments

Algorithm 1 Greedy Non-Sequential Algorithm
Input: D, B > 0, {Q1 , ..., QM }, {c1 , ..., cM }
P
Output: set indices = {1, ..., } ci B
- = argmaxiS,ci B J({i})
Sa {i }
- , C 0
(C < B)
J(S {i}) J(S)
Select that:
= argmax
ci
iS,c
/ BC
- C C + ci
- {i }
end
J(S) J(Sa )
- return
else
- return Sa
end

Algorithm 2 Accelerated Greedy Algorithm
Input: D, B > 0, {Q1 , ..., QM }, {c1 , ..., cM }
P
Output: set indices = {1, ..., }, s.t., sS ci B
- = argmaxiS,ci B J({i})
- Sa {i }
-S , C = 0, (i) = J({i})/ci , = 1, . . . ,
(C < B)
true
Set z = argmaxi:iS\S (i), ci B C, re-calculate (z)
J(S {z}) J(S)
(z) =
cz
(z) maxiS\{Sz} (i)
Break
end
end
- C C + cz , {z}
end
J(S) J(Sa )
- return
else
- return Sa
end

127

fiAzimi, Fern, & Fern

4.3 Accelerated Greedy Algorithm
section, following prior applications submodular optimization (e.g. Krause &
Guestrin, 2005), describe accelerated greedy algorithm, yields significant gains
computational efficiency. greedy step, let represent set queries
selected far. make another greedy selection, need compute cost
normalized incremental difference (i) = J(Si)J(S)
candidate query i,
ci

/ ci B C. computation expensive number candidate
queries often large. Fortunately, carefully maintaining normalized incremental
differences calculated first greedy step, avoid recomputing large majority
later iterations.
Specifically, first iteration compute (i) values S. sort
decreasing order based values, select first query remove
list. next iteration, move next query sorted list
recompute value. value remains largest, immediately select
query remove list, proceed next iteration without recomputing
values. Otherwise, proceed evaluate next query sorted list
finding one whose recomputed value greater stored values
select query. submodular property objective guarantees approach
makes choices full greedy algorithm, effectively avoids large number
computations values practice. proposed accelerated algorithm summarized
Algorithm 2.
4.4 Computation Expected Maximum
given set S, compute J(S), need compute expected maximum value
set jointly distributed random variables (y1 , ..., y|S| ). Unfortunately, expected
maximum set dependent random variables generally closed-form
solution (Ross, 2008). Instead, use Monte-Carlo simulation approach computing
expected maximum value. Specifically, given S, compute J(S), first sample one
experiment Q QS , resulting {x1 , ..., x|S| }. sample yi
posterior distribution py (y1 , ..., y|S| |x1 , ..., x|S| , D) take maximum sampled
yi s. repeated l independent times expected maximum obtained
averaging across l results. Note computation expected maximum
standard
value simulation exact. Denoting simulated results J,

Chernoff bounds used bound error J(S) high probability. Assuming

bounded error, |J(S) J(S)|
0, following theorem holds
non-decreasing submodular objective functions:
Theorem 2. (Krause & Guestrin, 2005) Let J() non-decreasing submodular function

= maxS:c(S)B J(S ) cost constrained optimizer J. J()

|J(S) J(S)|
S, Algorithm 1 run using J place J, returned
set satisfies following approximation bound, cmin = mini ci :




1 e1
1 c(S )


J(S)
J(S )
+ |S | .
(5)
2
e
2 cmin

128

fiBudgeted Optimization Constrained Experiments

5. Sequential Approach
section, consider sequential setting (Deshpande et al., 2004; Silberstein et al.,
2006) query selected one time result previous query
becomes available. commonly studied setting Bayesian Optimization
appropriate many applications single experimental facility
process queries.
inputs problem remain same, include B, total budget, fc ()
cost function, px (x|Q), distribution constructed experiment x given query Q,
D, set observed experiments. sequential setting, given inputs must
request sequence (one time) constrained experiments whose total cost within
budget.
Leveraging extensive body research traditional Bayesian Optimization, design sequential selection policies extending number well-established myopic sequential selection policies literature. existing policies traditional Bayesian
Optimization viewed defining greedy heuristic assigns score candidate experiment x based current experimental state, denote (Dc , Bc ),
Dc represent current set prior experiments, Bc represents current
remaining budget. reviewed Section 2, many existing heuristics observed perform well traditional Bayesian Optimization problems. Unfortunately
cannot directly used problem since select individual specific experiments
rather constrained experiments, require.
5.1 Model-Free Policies
Model-free policies consider statistical models data making selection.
work consider two model-free policies, Round Robin (RR) Biased Round Robin
(BRR), motivated previous work budgeted multi-armed bandit problems
(Lizotte et al., 2003; Madani et al., 2004).
5.1.1 Round Robin (RR)
multi-armed bandit setting, RR policy seeks keep number pulls
arm uniform possible. context constrained experiments, apply
principle keep experiments uniformly distributed possible experimental
space X . Given current experimental state (Dc , Bc ), define RR policy return
largest hyper-rectangle least costly query Q contain previous
experiment Dc . cost Q exceeds current budget Bc , return constrained
experiment cost Bc contains fewest experiments Dc . Ties broken
randomly. Note RR outputs previous queries effect
selecting next queries. However, exact location previous experiments
significant effect next query selection. Therefore, consider RR
non-sequential approach.
129

fiAzimi, Fern, & Fern

5.1.2 Biased Round Robin (BRR)
BRR policy behaves identically RR, except repeats previously selected constrained experiment long outcome constrained experiment improved
performance exceed budget. particular, given current experimental state (Dc , Bc ), query Q repeated long results outcome improves
current best outcome set Dc , fc (Q) Bc . Otherwise, RR policy
followed. policy analogous BRR multi-armed bandit problems (Madani et al.,
2004) arm pulled long positive outcome.
5.2 Model-Based Policies
model-based policies, assumed conditional posterior distribution p(y|Dc , x)
outcome individual experiment x Q learned set currently
observed experiments Dc . Existing model-based myopic policies traditional experimental
design typically select experiment x maximizes certain heuristics computed
statistics posterior (Jones, 2001). heuristics provide different mechanisms
trading exploration (probing unexplored regions experimental space)
exploitation (probing areas appear promising) given Dc . Note experiment x
fixed known point experimental design literature running real
experiment lab since assumed ask particular fixed point.
However, constrained experiment application, ask hyper-rectangle query
Q rather fixed experiment point x. Therefore conditional posterior distribution
constrained experiment Q defined p(y|Q, Dc ) , Ex|Q [p(y|x, Dc )]. definition corresponds process drawing experiment x Q drawing
outcome x p(y|x, Dc ). p() effectively allows us treat constrained experiments
individual experiments traditional optimization problem. Thus, define heuristics constrained experiments computing statistics posterior
p(), used traditional optimization. work consider four heuristics.
5.2.1 Maximum Mean (MM)
context traditional optimization individual experiments, MM heuristic,
also known PMAX (Moore & Schneider, 1995; Moore et al., 1998; Schneider & Moore,
2002), simply selects experiment largest expected outcome according
current posterior. constrained experiments, MM heuristic computes expected
outcome given query according current posterior p(). MM arbitrary
query Q computed follows:

MM(Q|Dc ) = E [y|Q, Dc ] , p(y|Q, Dc ).

(6)

MM purely exploitative heuristic weakness often
greedy get stuck poor local maximum point exploring rest experimental space.
130

fiBudgeted Optimization Constrained Experiments

5.2.2 Maximum Upper Interval (MUI)
MUI heuristic, also known IEMAX previous work (Moore & Schneider, 1995;
Moore et al., 1998; Schneider & Moore, 2002), attempts overcome greedy nature
MM exploring areas non-trivial probability achieving good result measured
upper bound 95% confidence interval output prediction. Thus, MUI
heuristic arbitrary constrained experiments Q calculated follow (assuming
Gaussian process used estimating posterior distribution f ()):
p
(7)
MUI(Q|Dc ) = E [y|Q, Dc ] + 1.96 Var [y|Q, Dc ], p(y|Q, Dc ).
Intuitively, MUI aggressively explore untouched regions experimental space
since outcomes regions high posterior variance. However, experimentation continues long time uncertainty decreases, MUI focus
promising areas behaves like MM. Note MUI specific case general
heuristic GP-UCB (Cox & John, 1992, 1997), constant 1.96 replaced
varying parameter results certain theoretical guarantees. Empirically GP-UCB
observed perform comparatively MEI heuristic introduce later
section.
5.2.3 Maximum Probability Improvement (MPI)
context individual experiments, MPI heuristic corresponds selecting
experiment highest probability generating outcome outperforms
best current outcome Dc . issue basic MPI strategy
tendency behave similar MM focuses areas currently look promising,
rather exploring unknown areas. reason behavior basic MPI
take consideration amount improvement current best outcome.
particular, typical posterior assign small amounts variances
outcomes well explored regions. means might good probability
observing small amounts improvement, probability substantial improvement
small. Hence, common consider use margin using MPI,
refer MPI(). Let yc represent current best outcome observed
Dc , MPI (Q|Dc ) equal probability outcome constrained
experiment Q greater ((1 + )yc ) (assuming non-negative yc values). MPI
heuristic given by:
MPI (Q|Dc ) = p (y (1 + )yc |Q, Dc ) ,

p(y|Q, Dc ).

(8)

MPI() heuristic sensitive margin parameter. Adjusting margin
small large causes heuristic change behavior exploitive
explorative.
5.2.4 Maximum Expected Improvement (MEI)
maximum expected improvement (Locatelli, 1997) heuristic seeks improve
basic MPI heuristic without requiring margin parameter . Rather focus
probability improvement, considers expected amount improvement according
131

fiAzimi, Fern, & Fern

current posterior. particular let I(y, yc ) = max(y yc , 0). Then, MEI heuristic
defined as:
MEI(Q|Dc ) = Ey [I(y, yc )|Dc , x] , p(y|Q, Dc ).

(9)

5.3 Cost-Sensitive Policies
introduced sequential heuristics take variable cost constrained
experiment account. heuristic value used selection criterion,
costly constrained experiment might selected. fact, nature heuristics
typically assign highest score constrained experiments maximally
constrained centered around individual experiment maximizes heuristic.
Unfortunately, constrained experiments also maximally costly. generally,
assume cost constrained experiment Q monotonically decreases size
region support, natural case consider. easy show
heuristics, value constrained experiment Q monotonically
non-decreasing respect cost Q. true reducing size
constrained experiment would remove points heuristic values less
constrained experiment value.
Thus, maximizing defined heuristics leads selection costly
experiments, might consume budget warranted. suggests
fundamental trade-off heuristic values cost constrained
experiments must address. Below, introduce two approaches attempt
address trade defining cost-sensitive policies cost insensitive heuristics.
5.3.1 Cost Normalized (CN) Policies
Cost normalized policies widely used budgetd optimization settings
costs non-uniform across experiments, (e.g., see Krause et al., 2008; Snoek, Larochelle,
& Adams, 2012). simply selects constrained experiment achieves highest
expected improvement per unit cost, rate improvement.
Suppose H heuristic. define corresponding CN policy heuristic
constrained experiment Q given current experimental state {Dc , Bc } follows:


H(Q|Dc )
CNH (Dc , Bc ) = argmax
,
(10)
fc (Q)
Q:fc (Q)<Bc
H(Q|Dc ) assigns score constrained experiment Q given set observed experiments Dc .
cost normalization approach natural baseline suggested
context optimization problems (e.g., Krause et al., 2008). However,
prior work, actual empirical evaluations involved uniform cost models thus
little empirical data regarding performance normalization. setting constrained experiments, uniform cost model option, since selecting among constrained experiments varying variable cost fundamental aspect problem. Thus,
empirical evaluation, Section 6, necessarily provides substantial evaluation
normalization principle.
132

fiBudgeted Optimization Constrained Experiments

Unfortunately, experimental results show proposed cost normalized approach
outperformed random policy cases. prompts us introduce
constrained minimum cost(CMC) approach select constrained experiment
expected perform better random policy spending amount
budget.
5.3.2 Constrained Minimum Cost (CMC) Policies
heuristic constrained experiments H(Q|Dc ), assigns score constrained
experiment Q given set observed experiments Dc , define associated CMC
policy. principle behind CMC select least cost constrained experiment
satisfies following two conditions:
Condition 1: approximately optimizes heuristic value,
Condition 2: expected improvement (EI) worse random
policy spending amount budget.
first condition encourages selection constrained experiments look promising according H, might result selection overly costly experiment.
second condition helps place limit much willing pay achieve
good heuristic value. Specifically, willing pay cost c single
constrained experiment Q expected improvement worse achieved
set random experiments whose total cost c.
formalize policy, first make notion approximately optimize precise
introducing parameterized version CMC policy show parameter
automatically selected via condition 2. given heuristic H, let h value
highest scoring constrained experiment fits within current budget. Note
necessarily one constrained (i.e. expensive) experiments
within budget. given parameter [0, 1], CMCH, policy selects leastcost constrained experiment achieves heuristic value least h . Formally,
defined
(11)
CMCH, (Dc , Bc ) = argmin {fc (Q) | H(Q|Dc ) h }
Q:fc (Q)Bc

value controls trade cost Q heuristic value.
Smaller/larger values result less/more costly experiments, smaller/larger
heuristic values. preliminary work, experimented CMCH, policy
found difficult select value worked well across wide range
optimization problems, cost structures, budgets. motivated introduction
condition 2 help us adaptively select appropriate value decision
point.
formalize CMC class policies. objective select largest
value experiment suggested CMCH, satisfies condition 2.
guarantee selected constrained experiment will: 1) achieve heuristic value close
possible h , 2) outperforms random policy given cost allocation.
following, define EIR(Dc , C) expected improvement set random
133

fiAzimi, Fern, & Fern

experiments total cost C Q constrained experiment returned
CMCH, . Also, let EI(Q ) expected improvement constraint experiment Q
c cost. parameter-free CMC policy defined as:
CMCH (Dc , Bc ) = CMCH, (Dc , Bc )


= arg max{ [0, 1] | EI(Q |Dc ) EIR(Dc , dc e)}.

(12)

practice compute EIR(Dc , C) EI(Q ) via Monte Carlo simulation.
straightforward process cases. EIR compute one EIR sample, randomly
select set experiments within budget C sample outcomes experiments via Gaussian Process conditioned Dc . improvement best outcome
taken result EIR sample. estimate EIR average L
EIR samples. similar process used EI, except rather drawing random
experiments sample use experiments Q .
following steps summarize overall computational process CMC-MEI.
1. Compute h maximizing H constrained experiments fall within current budget. Note requires optimizing set minimum-sized
constrained experiments cost less budget.
2. Perform discretized line search find according Equation 12.
3. Return CMCH, (Dc , Bc ) according Equation 11.

6. Experimental Results
goal evaluate performance proposed policies scenarios resemble typical real-world scientific applications. particular, experimental domains
motivate work paper focus low-dimensional optimization problems.
choice based two reasons. First, typical budgets number total experiments
often limited, makes optimizing many dimensions impractical. practice,
scientists often carefully select key dimensions consider. Second,
real-world applications, motivating problem, prohibitively difficult satisfy constraints couple experimental variables. Thus, relevant
scenarios us many problems moderate numbers experiments small
dimensionality.
6.1 Experimental Setup
describe set experiments.
6.1.1 Test Functions
evaluate policies using five 2-dimensional functions [0, 1]2 . first three functions: Cosines, Rosenbrock, Discontinuous benchmarks widely used
previous studies stochastic optimization (Anderson, Moore, & Cohn, 2000; Brunato,
Battiti, & Pasupuleti, 2006; Azimi et al., 2010). mathematical expressions
functions listed Table 1 contour plots given Figure 1.
134

fiBudgeted Optimization Constrained Experiments

Cosines

Rosenbrock

Hydrogen

Fuel Cell

Discontinuous
Figure 1: Contour plots test functions.
135

fiAzimi, Fern, & Fern

Table 1: Benchmark Functions.
Function
Cosines
Rosenbrock
Discontinuous

Mathematical representation
1 (u2 + v 2 0.3cos(3u) 0.3 cos(3v)),

u = 1.6x 0.5, v = 1.6y 0.5

10 100(y x2 )2 (1 x)2
1 2((x 0.5)2 + (y 0.5)2 ) x < 0.5,

0 otherwise

two remaining functions derived real-world experimental data sets involving hydrogen production motivating fuel cell application. former
utilize data collected part study biosolar hydrogen production (Burrows, Wong,
Fern, Chaplen, & Ely, 2009), goal maximize hydrogen production
cyanobacteria Synechocystis sp. PCC 6803 optimizing pH Nitrogen levels
growth media. data set contains 66 samples uniformly distributed 2-d input
space. data used simulate true function fitting Gaussian Process (GP)
model RBF kernel, picking kernel parameters via standard validation techniques
cross validation. model simulated experimental design process
sampling posterior GP obtain noisy outcomes requested experiments.
purpose, used zero-mean Gaussian noise model variance equal 0.01.
See Figure 1 contour plot.
motivating application (described introduction), utilize data
set initial microbial fuel cell experiments using anodes different nano-enhancements.
particular, anode coated gold nano-particles different fabrication
conditions leading varying particle densities, shapes, sizes. construction
anode required approximately two days.1 anode installed microbial fuel
cell run using pure Shewanella oneidensis bacterial cultures grown fed-batch mode
one week recording current density regular intervals. temporally averaged
current density taken dependent variable optimized modifying
nano-structure. characterize nano-structure anode, captured images using
scanning electron microscopy used standard image processing software compute two
features: average area individual particles, average circularity individual particles.
features selected independent variables design since
roughly controlled fabrication process appear influence current
density. Unfortunately, due high cost running experiments, precisely
motivation paper, data set currently consists 16 data points,
relatively uniformly distributed experimental space. Due sparse data,
utilize polynomial Bayesian regression degree 4, rather Gaussian processes
RBF kernels, simulate true function. See Figure 1 contour plot.
1. first round experiments constraints provided scientist constructing anodes.
Rather goal generate diverse set anodes provide good set data seeding
experimental design process. construction time would likely two days presence
constraints since number growth conditions trials would necessary.

136

fiBudgeted Optimization Constrained Experiments

6.1.2 Model Definitions
work, assume density px (x|Q) experiments given query Q
uniform ranges specified Q.
compute conditional posterior p(y|x, D) required non-sequential approach
model-based sequential heuristics, use Gaussian process (Rasmussen & Williams,
2006) zero mean prior covariance specified RBF kernel function:
1
|xi xj |2 ),
(13)
2
length scale parameter considered distance
move input space function value changes significantly, f signal
variance specifies maximum possible variance point. paper
2
set = 0.02 signal variance f = ymax
, ymax upper bound output
values (this typically easy elicit scientists serves set prior uncertainty
non-trivial probability assigned expected range output).
optimize parameters, empirically verify GP behaved reasonably
test functions.
cov (xi , xj ) = k(xi , xj ) = f exp(

6.1.3 Cost Function
motivating application, cost setting running fuel cell experiment
given constrained experiment request roughly considered two components.
first component corresponds cost setting experiment (producing
nano-structure) satisfies given constraints, variable depending
size constraints. tighter constraints, costly be.
second component corresponds cost running constrained experiment,
generally constant. two-component cost structure common real-world
applications portion experimental process controlled precisely
uniform costs across different queries, portions less controllable
cost inversely proportional size constraints place them.
capture structure, define following cost function fc () : Q <+ :


slope
fc (Q) = 1 +
.
(bi ai )

(14)

i=1

formulation, constant one captures stationary part cost,
second term captures variable portion inversely proportional size
constrains query Q. value slope parameter dictates quickly
cost increases size constrained experiment decreases. evaluate proposed
approaches considering three different slope values; slope = 0.1, 0.15, 0.30. Note
proposed approaches readily applied cost functions.
6.1.4 Discretizing Input Space
mentioned previously, policies assume input space discretized. particular, divide input dimension 100 equal-length subintervals. Note
137

fiAzimi, Fern, & Fern

implementation appropriate low dimensional optimization problems,
described previously situation often encounter real-world applications.
6.1.5 Evaluation Settings
evaluation, test proposed policies comparison random policy
(i.e., policy always selects entire input space constrained experiment).
Given budget B function f () optimized, run policy results
set observed experiments Dc . Let x experiment Dc predicted
maximum expected outcome . regret policy particular run
defined ymax , ymax maximum value f (). test function
choice budget cost structure (i.e., choice slope), evaluate policy
averaging regret 200 runs. run starts n = 5 randomly selected initial
points = {(x0 , y0 ), , , (x5 , y5 )}, policies used select constrained
experiments budget runs out, point regret measured. order
ease comparison regret values across different functions, report normalized
regret values, computed dividing regret policy mean regret
achieved random policy. normalized regret less one indicates approach
outperforms random, value greater one indicates approach worse
random. first round experiments, fixed total budget B = 15
examine effect cost-model slope parameter values 0.1, 0.15 0.3. later
experiments, consider larger budgets.
Note non-sequential policy used consume experimental budget
once. However, practice typically limit number constrained
experiments processed simultaneously due limited resources. such,
non-sequential setting policy used select five simultaneous queries subject
budget constraint. repeat process budget consumed.
run time selecting single experiment sequential setting order
minutes (generally five) experiments un-optimized matlab implementation. run time selecting batch five fewer queries never
30 minutes.
6.2 Results Discussions
results individual functions shown Table 2, corresponding standard
deviations shown inside parentheses. first row table presents results
non-sequential greedy algorithm (NS-Greedy). Rows 2 6 show performance
model-based sequential policies CMC CN cost policies. Note that,
CN policy, report results CN-MEI, performed best among CN policies.
addition, nice interpretation maximizing rate expected improvement per
unit cost. Finally, last row shows performance model-free sequential policies.
order provide assessment overall performance different methods, Table 3
presents normalized regrets policy averaged across five functions.
different columns table correspond different slope values cost function.
discuss results different methods detail.
138

fiBudgeted Optimization Constrained Experiments

Table 2: Normalized regrets individual functions varying cost models (i.e., slopes)
slope = 0.1
slope = 0.15
slope = 0.30
Cosines, Normalized Regret (95% Confidence Interval)
NS-Greedy
0.767 (0.04)
0.838 (0.05)
0.841 (0.05)
CN-MEI
0.569 (0.05)
0.714 (0.06)
0.826 (0.06)
CMC-MEI
0.417 (0.04)
0.514 (0.06)
0.794 (0.06)
CMC-MPI(0.2)
0.535 (0.05)
0.584 (0.06)
0.616 (0.06)
CMC-MUI
0.797 (0.06)
0.804 (0.06)
0.817 (0.06)
CMC-MM
0.708 (0.07)
0.767 (0.07)
0.736 (0.06)
RR/BRR
0.84(0.06)/0.83(0.06)0.86(0.06)/0.86(0.06)0.89(0.06)/0.88(0.06)
Discontinuous, Normalized Regret (95% Confidence Interval)
NS-Greedy
0.528 (0.06)
0.690 (0.06)
0.748 (0.05)
CN-MEI
0.527 (0.06)
0.497 (0.06)
0.626 (0.08)
CMC-MEI
0.564 (0.06)
0.677 (0.08)
0.779 (0.09)
0.954 (0.11)
0.940 (0.10)
0.951 (0.11)
CMC-MPI(0.2)
CMC-MUI
0.710 (0.10)
0.709 (0.11)
0.693 (0.09)
CMC-MM
1.289 (0.15)
1.225 (0.16)
1.116 (0.16)
RR/BRR
0.60(0.07)/0.58(0.07)0.61(0.07)/0.60(0.07)0.63(0.08)/0.63(0.08)
Rosenbrock, Normalized Regret (95% Confidence Interval)
NS-Greedy
0.650 (0.05)
0.877 (0.06)
0.930 (0.06)
CN-MEI
0.602 (0.06)
0.665 (0.07)
0.736 (0.08)
CMC-MEI
0.547 (0.35)
0.556 (0.39)
0.630 (0.47)
CMC-MPI(0.2)
0.503 (0.05)
0.594 (0.06)
0.608 (0.07)
CMC-MUI
0.805 (0.11)
0.974 (0.16)
0.913 (0.14)
CMC-MM
0.721 (0.09)
0.740 (0.10)
0.662 (0.08)
RR/BRR
0.89(0.12)/0.88(0.12)0.93(0.12)/0.92(0.12)0.96(0.14)/0.95(0.14)
Hydrogen, Normalized Regret (95% Confidence Interval)
NS-Greedy
0.879 (0.06)
0.969 (0.08)
0.993 (0.09)
CN-MEI
0.176 (0.04)
0.354 (0.06)
0.852 (0.09)
CMC-MEI
0.129 (0.04)
0.233 (0.06)
0.420 (0.07)
CMC-MPI(0.2)
0.408 (0.09)
0.449 (0.10)
0.613 (0.10)
CMC-MUI
0.716 (0.08)
0.695 (0.08)
0.868 (0.09)
0.728 (0.11)
0.605 (0.10)
0.691 (0.11)
CMC-MM
RR/BRR
1.10(0.09)/1.06(0.09) 1.16(0.10)/1.23(0.10) 1.17(0.09)/1.14(0.09)
Fuel Cell, Normalized Regret (95% Confidence Interval)
NS-Greedy
0.980 (0.02)
0.985 (0.02)
0.995 (0.03)
CN-MEI
0.929 (0.02)
0.950 (0.02)
0.986 (0.03)
CMC-MEI
0.931 (0.02)
0.908 (0.02)
0.940 (0.02)
CMC-MPI(0.2)
0.932 (0.02)
0.930 (0.03)
0.943 (0.03)
CMC-MUI
0.971 (0.03)
0.973 (0.03)
0.995 (0.03)
CMC-MM
0.945 (0.03)
0.963 (0.04)
0.963 (0.05)
RR/BRR
1.03(0.03)/1.02(0.03)1.04(0.03)/1.04(0.03)1.04(0.03)/1.04(0.03)

139

fiAzimi, Fern, & Fern

Table 3: Normalized Overall Regrets.
slope = 0.1 slope = 0.15 slope = 0.30
NS-Greedy
0.760
0.871
0.901
CN-MEI
0.560
0.636
0.805
CMC-MEI
0.517
0.578
0.712
CMC-MPI(0.2)
0.666
0.698
0.746
CMC-MUI
0.800
0.831
0.857
CMC-MM
0.874
0.889
0.834
RR
0.897
0.925
0.944
BR
0.879
0.911
0.934

6.2.1 Non-Sequential
first examine performance non-sequential greedy policy (NS-Greedy).
Recall present normalized regret results, thus smaller value indicates
better performance. Further, policy outperforms random whenever normalized regret
less 1.
Table 2, observe proposed greedy algorithm (NS-Greedy) performs
consistently better random policy functions. Among functions,
seen performance advantage NS-Greedy significant slope
parameter cost function smaller. consistent expectation:
smaller slope, cost query grows slower tighten constraints.
allow algorithm aggressively select tighter constraints based posterior
model function. fact, slope large enough, one would expect optimal
policy completely random.
Comparing sequential approaches, first observe NS-Greedy compared favorably two model-free methods. surprising RR/BRR
consider posterior model function selecting queries. hand, also
observe NS-Greedy algorithm performs significantly worse best modelbased sequential policies, CMC-MEI. result expected sequential
policies allow us update improve model function query. Therefore, generally expect sequential policies perform better non-sequential methods
common phenomenon active learning literature (Azimi, Fern, Fern, Borradaile, & Heeringa, 2012).
6.2.2 Sequential
section examine performance sequential policies, including modelfree model-based methods.
Model-Free Policies. Table 3 see RR BRR achieve improvement
random approximately 10% across different slopes. shows heuristic
trying evenly cover space pays compared random. BRR also observed
perform slightly better RR, indicates additional exploitive behavior
BRR pays overall. Looking individual results Table 2, see
140

fiBudgeted Optimization Constrained Experiments

Hydrogen Fuel Cell functions, BRR RR perform worse random.
investigation reveals reason poor performance RR/BRR bias
toward experiments near center input space. bias result fact
constrained experiments (hyper-rectangles) required fall completely within
experimental space fewer hyper-rectangles contain points near
edges corners. Hydrogen Fuel Cell functions optimal points near
corners space, explaining poor performance.
Model-Based Policies. focus performance proposed model-based
sequential policies. averaged overall results (Table 3), first observation
model-based policies general perform better random policy. Specifically,
looking results individual functions, see model-based policies outperform
random, exception CMC-MM Discontinuous function. shows
two proposed approaches considering cost able avoid catastrophic choices
expend budget quickly warranted.
analysis poor performance CMC-MM Discontinuous function revealed CMC-MM would often get stuck poor local optima cease explore
space adequately. Although step CMC-MM policy determined selection
better random near term, translate long term improvement
random due lack exploration. Discontinuous function particularly
prone elicit behavior due fact large sub-optimal nearly
uniform region, difficult CMC-MM escape from. overly greedy performance consistent prior observations MM heuristic largely addressed
heuristics provide measure exploratory value. fact, CMC-MM
highly dependent initial given random points. example, initial given points
chosen non-optimal region, 50% input space
Discontinuous function, CMC-MM approach cannot give satisfactory performance. seen standard deviation CMC-MM, higher
model-based model-free methods. shows performance CMC-MM
changes significantly iteration initial points.
addition, Table 3, seen model-based approaches outperform model free approaches. indicates heuristics considering
GP probabilistic model providing useful information effectively guiding
constrained experiment selection.
Comparing different model-based heuristics, see MEI-based methods (CNMEI CMC-MEI) top contenders among methods. Examining results
individual functions, see holds functions except Rosenbrock,
CMC-MPI slightly better MEI-based methods. Upon closer examination
behavior MPI MEI heuristics, found MPI often selects slightly
fewer experiments MEI, believe due fact MEI heuristic tends
smoother MPI experimental space. smoothness MEI allows
CMC policy select less constrained queries still achieve approximately optimal
heuristic value, leading constrained experiments. general recommend CMCMEI preferable heuristic use based consistently superior performance
fact parameter free.
141

fiAzimi, Fern, & Fern

0.5

CMCMEI
CNMEI
CMCMPI
NSGreedy
Random

0.4

Regret

0.4

Regret

0.5

CMCMEI
CNMEI
CMCMPI
NSGreedy
Random

0.3

0.3

0.2
0.2
0.1

0.1
10

20

30

Budget

40

50

0
10

60

20

Cosines

Budget

40

50

60

Rosenbrock

0.35

0.6

CMCMEI
CNMEI
CMCMPI
NSGreedy
Random

0.3
0.25

CMCMEI
CNMEI
CMCMPI
NSGreedy
Random

0.55

Regret

0.2
0.15
0.1

0.5

0.45

0.05
0
10

20

30

Budget

40

50

0.4
10

60

20

30

Hydrogen

Budget

40

50

Fuel Cell
CMCMEI
CNMEI
CMCMPI
NSGreedy
Random

0.1

0.08

Regret

Regret

30

0.06

0.04

0.02
10

20

30

Budget

40

50

60

Discontinuous
Figure 2: Un-normalized regret function budget (slope=0.1).
142

60

fiBudgeted Optimization Constrained Experiments

also interested comparing performance two proposed schemes
handling cost, namely CN CMC. Focusing CMC-MEI CN-MEI, see
CMC-MEI generally outperforms CN-MEI. differences behavior
two policies appear subtle, experimental investigation show CN-MEI tends
overly conservative toward selecting costly experiments comparison CMC-MEI,
especially later stages experimental process.
6.3 Varying Budget
round experiments, fixed cost model slope 0.1 varied budget
10 60 units increments 10. interested examining relative performance
different model-based policies (including sequential non-sequential) compared
random policy increase budget.
Figure 2 plots absolute regret (rather normalized regret) versus budget
best sequential policies including CMC-MEI, CN-MEI CMC-MPI,
proposed non-sequential policy (NS-Greedy). also plotted performance
random policy reference baseline. use experimental setting used
previously. Specifically, sequential methods, iteration select one query
budget completely consumed. proposed non-sequential approach, select
five queries iteration budget consumed.
First, observe performance NS-Greedy continues dominate Random
increase budget. suggests performance advantage NS-Greedy
Random robust amount experimental budget. also observe NS-Greedy
generally outperformed lead sequential policies, CMC-MEI, CMCMPI. consistent previous observations fixed budget varying slope.
Finally, see polices based MEI MPI heuristics generally achieve best
performance across wide range budgets. particular, consistently maintain
significant advantage Random. MEI-based CMC-MPI policies roughly
comparable functions except Fuel Cell function. case CMC-MPI
slightly outperforms CMC-MEI large budgets.
Overall, given results previous experiments, CMC-MEI still considered recommended method, due combination good performance, smoothness
robustness. CMC-MEI also preferable require selection
margin parameter.
6.4 Comparison Precise Experiments
section, compare performance using constrained experiments performance achieved using precisely specified experiments. particular, compare CMC-MEI
precise counterpart MEI. this, use CMC-MEI select fifteen
constrained experiments (with infinite budget) function, step evaluate
regret. repeated 100 times generate average performance curve
CMC-MEI function number constrained experiments. done two
different cost models slope set 0.1 0.3 respectively, resulting two curves
CMC-MEI. Similarly, use MEI select sequence fifteen precisely specified experiments generate average performance curve (over 100 random runs). Finally,
143

fiAzimi, Fern, & Fern

0.8
0.7

MEI
CMCMEI(0.1)
CMCMEI(0.3)
Random

1

Regret

0.6

Regret

1.5

MEI
CMCMEI(0.1)
CMCMEI(0.3)
Random

0.5
0.4

0.5

0.3
0.2
0.1
1

5

10

0
1

15

# Experiments

5

Cosines
0.5

MEI
CMCMEI(0.1)
CMCMEI(0.3)
Random

0.7

Regret

0.2

0.6
0.5
0.4

0.1

5

10

1

15

# Experiments

5

10

# Experiments

Hydrogen

Fuel Cell

0.2

MEI
CMCMEI(0.1)
CMCMEI(0.3)
Random

0.15

Regret

Regret

0.8

0.3

0
0

15

Rosenbrock

MEI
CMCMEI(0.1)
CMCMEI(0.3)
Random

0.4

10

# Experiments

0.1

0.05

0
1

5

10

# Experiments

15

Discontinuous
Figure 3: Un-normalized regret function number experiments.
144

15

fiBudgeted Optimization Constrained Experiments

reference point, also plot performance experiments selected randomly.
Figure 3 shows performance curves MEI, CMC-MEI (with slope = 0.1 0.3 respectively) random.
figure see cases CMC-MEI performed comparably MEI.
cases, observe detrimental effective use constrained experiments.
compare efficiency CMC-MEI slopes 0.3 0.1 (larger slopes yield higher
experimental costs), see cases comparable. However, Fuel
Cell Hydrogen functions, smaller slope consistently better (by small margin).
Further, also two functions precise experiments show significant
advantage CMC-MEI (in particular slope =0.3). likely explanation
optimal regions two functions fairly small, highly peaked near
boundaries. make difficult effectively explore region using constrained
experiments, especially larger slopes.
6.5 Comparison Constant Window Experiments
final experiments, compare performance CMC-MEI approach
Constant Window (CW) approach, constant constraint sizes used throughout
optimization process. goal understand importance dynamically selecting
window size done CMC-MEI. consider three different window sizes, denoted
CW5, CW20, CW50, correspond constraint sizes 5%, 20% 50%
dimension respectively. Thus, cost CW5 significantly CW50
precision control final selected samples. compare CW
approaches CMC-MEI. cost model parameter set slope = 0.1
budget varied 10 60 denomination 10. results provided Figure 4.
First, observe best performing CW approach varies significantly across
benchmarks budgets. indicates choosing window size particular
application non-trivial. Second, see CMC-MEI, adaptively selects
window size, performs best competitive best CW approach.
another indication CMC-MEI effective strategy choosing window sizes.
analysis experiments indicates CMC-MEI tends select experiments close
CW50 beginning decreases window size several experiments.

7. Summary Future Directions
Motivated real-world application, paper introduced novel framework budgeted Bayesian optimization constrained experiments. framework, instead
asking samples unknown function precisely specified inputs, ask constrained experiment cost constrained experiments variable depending
tightness constraints. studied problem two different settings.
non-sequential setting, multiple constrained experiments selected once.
setting, introduced non-decreasing submodular objective function presented
greedy algorithm approximately optimizing proposed objective. Empirical evaluation
indicates proposed non-sequential algorithm consistently outperforms baseline
random policy across different budget cost configurations.
145

fiAzimi, Fern, & Fern

0.6

0.4

CMCMEI
CW 5
CW 20
CW 50

0.3

0.4

Regret

Regret

0.5

0.3

0.2

0.1

0.2

0.1
10

CMCMEI
CW 5
CW 20
CW 50

20

30

40

50

0
10

60

20

30

Budget

40

50

60

Budget

Cosines

Rosenbrock

0.25
CMCMEI
CW 5
CW 20
CW 50

Regret

0.15

0.1

0.6

0.5
0.05

0
10

20

30

40

50

0.4
10

60

20

30

40

50

Budget

Budget

Hydrogen

Fuel Cell

0.12
CMCMEI
CW 5
CW 20
CW 50

0.1
0.08

Regret

Regret

0.2

CMCMEI
CW 5
CW 20
CW 50

0.7

0.06
0.04
0.02
0
10

20

30

40

50

60

Budget

Discontinuous
Figure 4: CMC-MEI performance versus constant window size experiments.
146

60

fiBudgeted Optimization Constrained Experiments

sequential setting problem, one constrained experiment selected
iteration. extended number classic Bayesian optimization experiment design
heuristics constrained experiments. Direct use heuristics select constrained
experiments select overly tight constraints consume budget once. Thus,
introduced two general cost policies, namely CN CMC, achieve balance
moderating cost experiments optimizing heuristics. experiments show
sequential policies generally outperform non-sequential policy, proposed CN
CMC cost policies effective dispensing budget rationally. Overall found
CMC used MEI heuristic (CMC-MEI) demonstrated robust performance
parameter-free, making recommended method.
work described focused methods optimizing low-dimensional functions,
typical types scientific engineering applications motivated
work. Extending methods higher dimensions requires optimizing selection criteria
continuous rather discrete input spaces. number straightforward
approaches future work could include evaluating approaches
designing sophisticated ones. interesting direction future work
continue enriching cost action models supported Bayesian Optimization methods
closely match needs real-world applications. Solutions extended
models require tighter integration planning scheduling techniques
ideas developed far traditional Bayesian Optimization.

Acknowledgments
research supported NSF grant IIS 1320943.
Appendix A. Proof Lemma 1
Lemma 1. Let XM = {x1 , . . . , xM } denote random variables representing outcome
random draw QM = {Q1 , ..., QM } respectively QM set possible
queries. given XM , JXM (S), returns expected maximum set
jointly distributed random variables, monotonically non-decreasing submodular set
function.

Proof. Suppose finite set. g : 2S R+ submodular set function
S1 S2 x \ S2 , holds g(S1 {x}) g(S1 ) g(S2 {x}) g(S2 ).
addition set function g() called monotonically non-decreasing g(S1 ) g(S2 ).
first prove E[max()] monotonic function show
submodular objective function.
Assume S1 = {x1 , x2 , , xp } p k. need prove
fi
fi
h
h
fi
fi
E max (y1 , y2 , , yp , , yk ) fiD E max (y1 , y2 , , yp ) fiD .
147

(15)

fiAzimi, Fern, & Fern

use definition expectation prove result.

fi
fi
E max (y1 , y2 , , yp , , yk ) fiD
Z
Z
= max (y1 , y2 , , yp , , yk ) py1 ,y2 ,,yp ,,yk |D dy1 dy2 dyp dyk
Z
Z
max (y1 , y2 , , yp ) py1 ,y2 ,,yp ,,yk |D dy1 dy2 dyp dyk
Z

Z
Z
Z
= max (y1 , y2 , , yp )
py1 ,y2 ,,yp ,,yk |D dyp+1 dyk dy1 dy2 dyp
Z
Z
= max (y1 , y2 , , yp ) py1 ,y2 ,,yp |D dy1 dy2 dyp
fi
h
fi
= E max (y1 , y2 , , yp ) fiD .
(16)
h

shows E[max()] nondecreasing monotonic function.
prove submodularity property, need show

fi
fi
h
h
fi
fi
E max (y1 , y2 , , yp , ) fiD E max (y1 , y2 , , yp ) fiD
fi
fi
h
h
fi
fi
E max (y1 , y2 , , yp , , yk , ) fiD E max (y1 , y2 , , yp , , yk ) fiD .

(17)

prove this, start right hand side inequality basic definition
expectation.

fi
fi
h
h
fi
fi
E max (y1 , y2 , , yp , , yk , ) fiD E max (y1 , y2 , , yp , , yk ) fiD
Z
Z
= max (y1 , y2 , , yp , , yk , ) py1 ,y2 ,,yp ,,yk ,y |D dy1 dy2 dyp dyk dy
Z
Z
max (y1 , y2 , , yp , , yk ) py1 ,y2 ,,yp ,,yk |D dy1 dy2 dyp dyk
Z
Z
= max (y1 , y2 , , yp , , yk , ) py1 ,y2 ,,yp ,,yk ,y |D dy1 dy2 dyp dyk dy
Z
Z
max (y1 , y2 , , yp , , yk ) py1 ,y2 ,,yp ,,yk ,y |D dy1 dy2 dyp dyk dy
Z
Z
= [max (y1 , y2 , , yp , , yk , ) max (y1 , y2 , , yp , , yk )]
(18)
148

fiBudgeted Optimization Constrained Experiments

py1 ,y2 ,,yp ,,yk ,y |D dy1 dy2 dyp dyk dy
Z


Z


[max (y1 , y2 , , yp , ) max (y1 , y2 , , yp )]
py1 ,y2 ,,yp ,,yk ,y |D dy1 dy2 dyp dyk dy

Z

Z


=
Z

[max (y1 , y2 , , yp , ) max (y1 , y2 , , yp )] py1 ,y2 ,,yp ,y |D dy1 dy2 dyp dy

Z

max (y1 , y2 , , yp , ) py1 ,y2 ,,yp ,y |D dy1 dy2 dyp dy
Z
Z
max (y1 , y2 , , yp ) py1 ,y2 ,,yp |D dy1 dy2 dyp
fi
fi
h
h
fi
fi
= E max (y1 , y2 , , yp , ) fiD E max (y1 , y2 , , yp ) fiD

=



(19)
Notice inequality holds prove:
max (y1 , y2 , , yp , , yk , ) max (y1 , y2 , , yp , , yk )
max (y1 , y2 , , yp , ) max (y1 , y2 , , yp )

(20)

two possible cases follows:
(


max (y1 , y2 , , yp , , yk , ) =


max (y1 , y2 , , yp , , yk ) .

(21)

1. first case, max (y1 , y2 , , yp , , yk , ) = ,
also max (y1 , y2 , , yp , ) = . Hence,
max (y1 , y2 , , yp , , yk , ) max (y1 , y2 , , yp , , yk )
= max (y1 , y2 , , yp , , yk )
max (y1 , y2 , , yp )

(22)

= max (y1 , y2 , , yp , ) max (y1 , y2 , , yp )
2. second case, max (y1 , y2 , , yp , , yk , ) = max (y1 , y2 , , yp , , yk ),then

max (y1 , y2 , , yp , , yk , ) max (y1 , y2 , , yp , , yk )
=0
max (y1 , y2 , , yp , ) max (y1 , y2 , , yp )
= max (y1 , y2 , , yp , ) max (y1 , y2 , , yp )
Notice max (y1 , y2 , , yp , ) max (y1 , y2 , , yp ) always non-negative.

149

(23)

fiAzimi, Fern, & Fern

References
Anderson, B., Moore, A., & Cohn, D. (2000). nonparametric approach noisy costly
optimization. ICML.
Azimi, J., Fern, A., & Fern, X. (2010). Batch bayesian optimization via simulation matching.
NIPS, pp. 109117.
Azimi, J., Fern, A., Fern, X. Z., Borradaile, G., & Heeringa, B. (2012). Batch active learning
via coordinated matching. ICML.
Azimi, J., Fern, X., Fern, A., Burrows, E., Chaplen, F., Fan, Y., Liu, H., Jaio, J., & Schaller,
R. (2010). Myopic policies budgeted optimization constrained experiments.
AAAI.
Bond, D. R., & Lovley, D. R. (2003). Electricity production geobacter sulfurreducens
attached electrodes. Applications Environmental Microbiology, 69, 15481555.
Brochu, E., Cora, V. M., & De Freitas, N. (2010). tutorial bayesian optimization
expensive cost functions, application active user modeling hierarchical
reinforcement learning. arXiv preprint arXiv:1012.2599.
Brunato, M., Battiti, R., & Pasupuleti, S. (2006). memory-based rash optimizer. AAAI06 Workshop Heuristic Search, Memory Based Heuristics applications.
Burrows, E. H., Wong, W.-K., Fern, X., Chaplen, F. W., & Ely, R. L. (2009). Optimization
ph nitrogen enhanced hydrogen production synechocystis sp. pcc 6803
via statistical machine learning methods. Biotechnology Progress, 25, 10091017.
Chapelle, O., & Li, L. (2011). empirical evaluation thompson sampling. Advances
neural information processing systems, pp. 22492257.
Cox, D. D., & John, S. (1992). statistical method global optimization. IEEE
Conference Systems, Man Cybernetics, pp. 12411246.
Cox, D. D., & John, S. (1997). Sdo: statistical method global optimization.
Multidisciplinary Design Optimization: State-of-the-Art, pp. 315329.
Desautels, T., Krause, A., & Burdick, J. W. (2014). Parallelizing exploration-exploitation
tradeoffs gaussian process bandit optimization. Journal Machine Learning
Research, 15 (1), 38733923.
Deshpande, A., Guestrin, C., Madden, S. R., Hellerstein, J. M., & Hong, W. (2004). Modeldriven data acquisition sensor networks. VLDB 04: Proceedings Thirtieth
international conference large data bases, pp. 588599. VLDB Endowment.
Elder, J.F., I. (1992). Global rd optimization probes expensive: grope algorithm. IEEE International Conference Systems, Man Cybernetics, pp.
577582.
Fan, Y., Hu, H., & Liu, H. (2007). Enhanced coulombic efficiency power density
air-cathode microbial fuel cells improved cell configuration. Journal Power
Sources, press.
Ginsbourger, D., Riche, R. L., & Carrarog, L. (2010). Kriging well-suited parallelize
optimization..
150

fiBudgeted Optimization Constrained Experiments

Goel, A., Guha, S., & Munagala, K. (2006). Asking right questions: model-driven optimization using probes. PODS 06: Proceedings twenty-fifth ACM SIGMODSIGACT-SIGART symposium Principles database systems, pp. 203212.
Jones, D. R. (2001). taxonomy global optimization methods based response surfaces.
Journal Global Optimization, 21, 345383.
Khuller, S., Moss, A., & Naor, J. (1999). budgeted maximum coverage problem. Inf.
Process. Lett., 70 (1), 3945.
Krause, A., & Guestrin, C. (2005). note budgeted maximization submodular
functions. Technical Report, CMU-CALD-05-103.
Krause, A., Singh, A., & Guestrin, C. (2008). Near-optimal sensor placements Gaussian processes: Theory, Efficient Algorithms Empirical Studies. Journal
Machine Learning Research, 9, 235284.
Lizotte, D., Madani, O., & Greiner, R. (2003). Budgeted learning naive-bayes classifiers.
UAI.
Locatelli, M. (1997). Bayesian algorithms one-dimensional global optimization. Journal
Global Optimization, 10 (1), 5776.
Madani, O., Lizotte, D., & Greiner, R. (2004). Active model selection. UAI.
Moore, A., & Schneider, J. (1995). Memory-based stochastic optimization. NIPS.
Moore, A., Schneider, J., Boyan, J., & Lee, M. S. (1998). Q2: Memory-based active learning
optimizing noisy continuous functions. ICML, pp. 386394.
Myers, R. H., Montgomery, D. C., & Anderson-Cook, C. M. (1995). Response surface
methodology: process product optimization using designed experiments. Wiley.
Park, D. H., & ZeikusG, J. G. (2003). Improved fuel cell electrode designs producing
electricity microbial degradation. Biotechnol Bioeng, 81 (3), 348355.
Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian Processes Machine Learning.
MIT.
Reguera, G., McCarthy, K. D., Mehta, T., Nicoll, J. S., Tuominen, M. T., & Lovley, D. R.
(2005). Extracellular electron transfer via microbial nanowires. Nature, 10981101.
Ross, A. M. (2008). Computing Bounds Expected Maximum Correlated Normal
Variables . Methodology Computing Applied Probability.
Schneider, J., & Moore, A. (2002). Active learning discrete input spaces. Interface
Symposium.
Schonlau, M. (1997). Computer Experiments Global Optimization. Ph.D. thesis, University Waterloo.
Silberstein, A., Braynardand, R., Ellis, C., Munagala, K., & Yang, J. (2006). samplingbased approach optimizing top-k queries sensor networks. ICDE 06: Proceedings 22nd International Conference Data Engineering, p. 68, Washington,
DC, USA. IEEE Computer Society.
151

fiAzimi, Fern, & Fern

Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical bayesian optimization machine
learning algorithms. Advances neural information processing systems, pp. 2951
2959.
Srinivas, N., Krause, A., Kakade, S., & Seeger, M. (2010). Gaussian process optimization
bandit setting: regret experimental design. Proc. International
Conference Machine Learning (ICML).
Stuckman, B. E. (1988). global search method optimizing nonlinear systems. IEEE
transactions systems, man, cybernetic, Vol. 18, pp. 965977.
Tatbul, N., Cetintemel, U., Zdonik, S. B., Cherniack, M., & Stonebraker, M. (2003). Load
shedding data stream manager. VLDB 2003: Proceedings 29th international conference large data bases, pp. 309320. VLDB Endowment.

152

fiJournal Artificial Intelligence Research 56 (2016) 61-87

Submitted 09/15; published 05/16

Automatic Wordnet Development Low-Resource
Languages using Cross-Lingual WSD
Nasrin Taghizadeh

nsr.taghizadeh@ut.ac.ir

School Electrical Computer Engineering
College Engineering, University Tehran, Tehran, Iran

Hesham Faili

hfaili@ut.ac.ir

School Electrical Computer Engineering
College Engineering, University Tehran, Tehran, Iran

Abstract
Wordnets eective resource natural language processing information
retrieval, especially semantic processing meaning related tasks. far, wordnets
constructed many languages. However, automatic development wordnets low-resource languages well studied. paper, ExpectationMaximization algorithm used create high quality large scale wordnets poorresource languages. proposed method benefits possessing cross-lingual word sense
disambiguation develops wordnet using bi-lingual dictionary monolingual corpus. proposed method executed Persian language
resulting wordnet evaluated several experiments. results show
induced wordnet precision score 90% recall score 35%.

1. Introduction
One important projects natural language processing years
construction English wordnet (WordNet) Princeton University direction George A. Miller (1995). WordNet consists lexical database, English
words grouped sets cognitive synonyms called synsets. eectiveness WordNet wide range language technology applications inspired many researchers create
wordnets languages. first attempts led construction EuroWordNet (Vossen, 1998) BalkaNet (Tufis, Cristea, & Stamou, 2004). EuroWordNet
deals European languages English, Dutch, German, French, Spanish, Italian,
Czech Estonian; BalkaNet covers languages Balkan zone. interconnect wordnets dierent languages, EuroWordNet links synsets language
interlingual index (ILI). ILI allows find equivalent synsets across languages
connected ILI.
Although first wordnet created manually, several automatic semi-automatic
techniques used developing wordnets. methods usually
divided merge expansion approaches (Fellbaum & Vossen, 2012; Oliver & Climent, 2012; Erjavec & Fiser, 2006). However, methods combine merge
expansion models benefit advantages approaches (Prabhu, Desai,
Redkar, Prabhugaonkar, Nagvenkar, & Karmali, 2012; Apidianaki & Sagot, 2014).
merge approach, small wordnet created manually, contains high-level
c
2016
AI Access Foundation. rights reserved.

fiTaghizadeh & Faili

basic concepts. Next, small wordnet developed using automatic semi-automatic
techniques. process, mono-lingual resources language-specific properties
employed. Wordnets created manner later mapped onto either WordNet
ILI. using expansion approach, multilingual wordnet constructed translating words inside synsets WordNet (or existing wordnets) target
language using multi-lingual resources. Therefore structure original wordnet
preserved words translated.
Among dierent methods proposed wordnet construction, applicable low-resource languages. Methods follow merge approach labourintensive time-consuming. Moreover, need vast knowledge language also require many resources, main obstacle low-resource languages
- makes approach inapplicable practice. hand, methods
follow expansion approach usually adopt WordNet structure find correct
translation associated words WordNet synsets target language.
process, multilingual resources comparable corpora (Kaji & Watanabe, 2006),
parallel corpora (Oliver & Climent, 2012; Kazakov & Shahid, 2009; Fiser, 2009; Diab, 2004),
thesaurus (Gunawan & Saputra, 2010), machine translators (Saveski & Trajkovski, 2010)
multiple bi-lingual machine readable dictionaries (Atserias, Climent, Farreres, Rigau,
& Rodrguez, 2000; Patanakul & Charnyote, 2005; Bond, Isahara, Kanzaki, & Uchimoto,
2008; Lam, Al Tarouti, & Kalita, 2014) used, causes bottleneck low-resource
languages.
Taking deeper look expansion-based methods, synset WordNet
kept words associated translated target language. bi-lingual
dictionary usually employed English words inside WordNet synsets translated.
Since dictionaries translate word sense word sense, rather word word,
translations ambiguous disambiguated. Looking carefully,
translating English words inside WordNet synset, set candidate words target
language obtained; equivalent senses English words
thus omitted. Methods following expansion approach rank
candidate words omit low-rated ones candidate sets. task scoring
candidate words WordNet synsets considered optimization problem, (sub)optimal values found using algorithms Expectation-Maximization (Montazery
& Faili, 2011). proposed method extension work low-resource languages.
paper, problem automatically constructing large scale high quality
wordnets low-resource languages studied. two major approaches, merge
expansion, first one suitable; requires vast knowledge
target language also many language resources. preferred approach utilize wordnets languages adopting structure translating content.
Finding correct senses target language words AI-complete problem (Mallery,
1988), is, analogy NP-completeness complexity theory, problem
whose diculty equivalent solving central problems AI (Navigli, 2009).
paper, iterative optimization method based cross-lingual WSD proposed find
local optimum problem reasonable time. main idea iteratively
improve estimation probability selecting WordNet synsets words
target language. Additionally, proposed method needs resources suitable
62

fiAutomatic Wordnet Development Low-Resource Languages

poor-resource languages. investigate performance proposed method, Persian selected poor-resource language resulting wordnet examined
conducting several experiments.
roadmap paper follows: Section 2 presents related works; Section 3
explains wordnet construction problem proposed formulation; Section 4 presents
case study Persian language error analysis; conclusions given future
works suggested last section, Section 5.

2. Related Work
section, automatic methods constructing wordnets reviewed
based expansion approach. main stage expansion-based methods finding
set words lexicalizes concept captured given synset existing wordnet
another language. candidate words usually extracted dictionary scoring
system utilized find correct words.
work Kaji Watanabe (2006), gloss information WordNet
used automatic construction Japanese wordnet. Given English synset,
calculates score Japanese translation candidates according gloss
appended synset. score defined sum correlations translation candidates associated words appear gloss. pair words
deemed associated amount mutual information predefined threshold. Since availability bi-lingual corpora limited, iterative approach
proposed calculating pair-wise correlations.
Another study creating wordnet automatically expanding WordNet describes
Romanian wordnet. work Barbu Barbu Mititelu (2005), order identify
Romanian words corresponding WordNet synset, several heuristics proposed. According first heuristic, words related synset share common meaning.
Therefore, intersection translations words associated WordNet synsets
considered. second heuristic states synset hypernym share
meaning. Therefore, intersection word translations given WordNet synset
hypernym selected Romanian synset. According third heuristic,
translations domain label selected given WordNet synset.
fourth heuristic, Romanian word selected English translations words based
definition maximum similarity words gloss given synset.
research conducted Patanakul Charnyote (2005), semi-automatic expanding approach presented construct Thai wordnet. Candidates links
Thai words WordNet synsets derived WordNet translations. rank links, 13 criteria used categorized three groups:
monosemic, polysemic, structural criteria. Monosemic criteria focus English words
one meaning assume English words one synset
WordNet. Polysemic criteria focus English words multiple meanings,
believe English words multiple synsets WordNet. Structural criteria
focus structural relations among synsets respect wordnet 1.7.
Another idea creating wordnet use word-aligned parallel corpus n languages, annotate word lexical sense tag consists n-tuple aligned
63

fiTaghizadeh & Faili

words. result, occurrences given word text language L considered
sense, provided tagged multi-lingual synset. However, kind corpus easily available languages. research,
conducted Oliver Climent (2012), two strategies automatic construction
corpora proposed: (i) machine translation sense-tagged corpora, (ii)
automatic sense tagging bi-lingual word-aligned corpora. results Spanish
language showed first strategy works better second. suggests
lexical selection errors made machine translation systems less important
sense tagging errors.
BabelNet project, undertaken Navigli Ponzetto (2010, 2012a),
large multi-lingual semantic network constructed. project, original wordnet
used lexicographic resource well Wikipedia pages dierent languages
encyclopedic knowledge. First mapping English Wikipedia pages
synsets original wordnet established. Given Wikipedia page w
mapping, Babel synset created using wordnet synset s, page w, inter-language
links, translation w languages. project, coverage
resulting network analyzed comparing gold-standard wordnets
terms synset coverage, word coverage, synset extra coverage. results show
synset coverage varies dierent languages 52% Italian 86% French.
work Bond Foster (2013), open multi-lingual wordnet
eighty languages developed. project, common interface accessing multiple
wordnets created gathering existing freely available wordnets dierent languages automatically linking WordNet. Next, wordnets extended
using Unicode Common Locale Data Repository (UCLDR) Wiktionary. rank
candidate links WordNet synsets Wiktionary, several similarity measures
employed. results show precision score 85%-99% measured sense.
Arabic wordnet created follows EuroWordNet methodology manually
encoding set base concepts maximizing compatibility across Arabic English
wordnets (Black, Elkateb, & Vossen, 2006; Elkateb, Black, Rodrguez, Alkhalifa, Vossen,
Pease, & Fellbaum, 2006). Next, project, performed Rodrquez et al.
(2008), machine learning algorithm employed extending Arabic wordnet
augmenting formal specification senses synsets. order associate Arabic
words WordNet synsets, Bayesian network four layers proposed. Four
layers respectively represent: Arabic words; corresponding English translation
Arabic words first layer; synsets English words second layer;
WordNet synsets linked synsets layer three. set candidates word-synset
built pairs <x, y>, x Arabic word WordNet synset third
layer Bayesian network non-null probability path
x y. score link calculated posterior probability y, given
evidence provided network. tuples score threshold selected
inclusion final set candidates word-synset. best results method
proposed study noted score 71% precision.
work Boudabous et al. (2013), Arabic wordnet enriched via adding
semantic relations synsets. method consisted two main phases; first
phase consisted defining morpho-lexical patterns using study corpora extracted
64

fiAutomatic Wordnet Development Low-Resource Languages

Arabic Wikipedia. second phase consisted using morpho-lexical patterns, defined
previous phase, order extract new semantic relations Arabic Wikipedia.
Extracted relations validated, added Arabic wordnet data base.
Piasecki et al. (2011) proposed algorithm automatically expanding Polish
wordnet. method uses heterogeneous knowledge sources, extracted
large corpus, combines based weighted voting scheme. method extracts
potential instances lexicon-semantic relations corpus measures semantic
similarity lexical units. analyzes eect using dierent knowledge resources
performance algorithm. Due high accuracy results, approach
said good basis semi-automatic methods constructing wordnets using
human knowledge correct output automatic approaches.
Lam et al. (2014) proposed automatic method constructing wordnet synsets
uses publicly available wordnets, machine translator bi-lingual dictionaries.
purpose, synset existing wordnet translated target language,
ranking method applied resulting translation candidates find best
translations. generate candidate synsets, three approaches proposed; first one
directly translates synsets WordNet target language. second one uses intermediate wordnets handle ambiguities synset translations. case dictionaries
available, addition wordnets intermediate languages, third approach
used. experimental results showed resulting wordnets coverage
19%, 65%, 37%, 21% 83% Karbi, Arabic, Assamese, Dimasa Vietnamese
languages, respectively.
project, conducted Hasanuzzaman et al. (2014), method
constructing Tempo-wordnet suggested. According method, WordNet
augmented temporal information following two-step process: first
step, synsets WordNet classified atemporal temporal. Next, synsets
associated past, present future probabilities. obtained Tempo-wordnet
used time-related applications.
work Shamsfard (2008), semi-automated method proposed developing Persian lexical ontology called FarsNet. 1,500 verbs 1,500 nouns
gathered manually make wordnets core. that, two heuristics Word Sense
Disambiguation (WSD) method used find likely related Persian synsets.
practical evaluation proposed automatic method used studt shows score
70% correctness covers 6,500 entries WordNet. extension work
(Shamsfard, Hesabi, Fadaei, Mansoory, Famian, Bagherbeigi, Fekri, Monshizadeh, & Assi,
2010a), known first published Persian wordnet, FarsNet, contains
18,000 Persian words covers 6,500 WordNet synsets.
research, performed Montazery Faili (2010), automatic
approach Persian wordnet construction based WordNet introduced.
proposed approach uses two mono-lingual corpora English Persian, bilingual dictionary order construct mapping WordNet synsets Persian
words using two dierent methods; links selected directly using heuristics
recognize links unambiguous. types links ambiguous,
scoring method used select appropriate synset. practical evaluation links
500 randomly selected Persian words shows 76.4% quality terms accuracy.
65

fiTaghizadeh & Faili

augmenting Persian wordnet unambiguous words, total accuracy
automatically extracted Persian wordnet becomes 82.6%.

3. Iterative Method Wordnet Construction
construct multi-lingual wordnet, several methods presented; however,
paid attention low-resource languages. Creating wordnet scratch
languages time-consuming expensive process. Instead, new wordnets could
developed adopting structure existing wordnets languages (usually WordNet) translating words associated synsets target language. One
important advantage approach resulting wordnet aligned WordNet ILI, thus interesting contrastive semantic analysis particularly
useful multi-lingual tasks multi-lingual information retrieval (Dini, Peters, Liebwald, Schweighofer, Mommers, & Voermans, 2005; Otegi, Arregi, Ansa, & Agirre, 2015)
multi-lingual semantic web (Buitelaar & Cimiano, 2014). main assumption
one develop wordnet using expansion approach concepts
semantic relations common among dierent languages. Therefore, language-specific
concepts relations may covered resulting wordnet.
general, regardless approach taken, main step toward constructing
complete wordnet generate synonym sets. section, automatic method
extracting synsets languages limited resources proposed. proposed method
follows expansion approach; start, wordnet initialized WordNet synsets.
every WordNet synset s, translations English words inside extracted bilingual dictionary links translation words WordNet synsets established.
Since dictionaries translate word word, word sense word sense, translations
ambiguous. Therefore, task score links find incorrect ones. consider
scores probability selecting candidate synset word target
language.
paper, task finding correct translation words associated
WordNet synsets regarded optimization problem. sensed-tagged corpus similar
English SemCor (Landes, Leacock, & Tengi, 1998) exists target language,
problem creating wordnet converted maximum likelihood estimation (MLE).
English SemCor corpus sense-tagged corpus created Princeton University
wordnet project research team. corpus consists subset Brown Corpus
contains 700,000 words. SemCor words POS tagged
200,000 content words sense-tagged reference WordNet lexical database.
Since resources may exist, use word sense disambiguation method find
correct sense word raw corpus. shown research, conducted
Mallery (1988), WSD AI-complete problem whose diculty equivalent solving
central problems AI. class problems analogous NP-complete problems
complexity theory, classified dicult problems. proposed
idea use iterative algorithm finds local optima problem
iterations reasonable time. work regarded extension work
performed Montazery Faili (2011). proposed method adopts
66

fiAutomatic Wordnet Development Low-Resource Languages

Mono-lingual
corpus

extract
unique words

Bi-lingual
Dictionary

w

extract English
translations

WordNet

(w, e)

extract
WordNet
synsets

(w, s)

EM algorithm
(w, s, p)

Synsets
target
language

deleting
low-rated links

Figure 1: overview proposed approach constructing wordnet

work low-resource languages; method additionally attempts solve major
drawbacks.
idea proposed work Montazery Faili (2011) wordnet construction,
use bi-lingual dictionary well raw-corpus. First, Farsi word
corpus, translations extracted bi-lingual dictionary. Next, synsets
English translations considered candidate synsets Farsi word.
score calculated pair Farsi words WordNet synsets using expectationmaximization (EM) algorithm. expectation step, use relative-based WSD
method (PMI), co-occurrence frequency pairs words Farsi language
used disambiguate words corpus. Experimental results showed
precision method varies dierent POS tags. highest precision shown
adjectives 89.7%; next adverbs, 65.6%; lowest precision
nouns 61.6%.
major drawbacks method calculating co-occurrence pair words target language usually requires large corpus, may
easily found low-resource languages; important quality
resulting wordnet highly depends co-occurrence values. result, propose
change expectation step PMI-based algorithm WSD procedure
performed without needing additional corpus language resources. Figure 1
represents overview proposed method. Next, experimental analysis,
re-implement work baseline compare proposed method it.
EM iterative algorithm finding maximum likelihood parameters statistical model cases equations cannot directly solved. models typically
consist latent variables addition unknown parameters known data observations.
is, either missing values among data, model formulated
simply assuming existence additional unobserved data points. basic
idea EM follows:
1. actual sucient statistics data, compute parameter
values maximize likelihood data. problem learning
probabilistic model complete data.
67

fiTaghizadeh & Faili

Maximization Step

initial values

Parameters w,s

sense-tagged corpus

Expectation Step

Figure 2: Expectation-Maximization algorithm wordnet construction
2. actually succeed learning model parameters, could compute
probability distribution values missing attributes.
case problem, EM algorithm find probability mapping
word target language candidate synsets. candidate synset represents
correct sense word target language, expected sense occurs
corpus containing word. observed data words corpus target
language; unseen part data WordNet sense tag words.
Th EM algorithm switches two stages: 1) finding approximate distribution
missing data given parameters; 2) finding better parameters given approximation. first step known expectation E-step, second step
called maximization M-step. Figure 2 represents overview EM algorithm
used learning words connected WordNet synsets. Next, details step
proposed algorithm presented.
3.1 E-Step
Similar work Montazery Faili (2011), word target language,
w, WordNet synset, s, w,s defined probability choosing WordNet
synset word w, P (s|w). words, number times word w appears
large corpus sense divided total number appearance w. is:
w, :
w :

w,s [0, 1].


w,s = 1.

(1)

(2)



step, current values parameters w,s used label corpus sense
tags. word w appearing corpus, appropriate sense among candidate
WordNet synsets chosen. task, unsupervised cross-lingual word
sense disambiguation (WSD) could employed. WSD algorithms aim resolve word
ambiguity without use annotated corpora. Unsupervised WSD well-studied task
literature. Among these, two categories knowledge-based algorithms gained
popularity: overlap- graph-based methods. former owns success simple
intuition underlies family algorithms, diusion latter started
growing development semantic networks (Basile, Caputo, & Semeraro, 2014).
68

fiAutomatic Wordnet Development Low-Resource Languages

Within graph-based framework WSD, graph built lexical knowledge
base (usually WordNet) representing possible senses word sequence
disambiguated. Graph nodes correspond word senses, whereas edges represent dependencies senses. dependencies include hypernymy, synonymy, antonymy, etc.
Next, graph structure analyzed determine importance node. Finding
right sense word sequence amounts identifying important
node among set graph nodes representing candidate senses. main challenge
graph-based WSD methods create graph, especially dependencies
chosen graphs edges, connectivity measure used
score nodes graph.
research, conducted Navigli Lapata (2010), comprehensive
study unsupervised graph-based WSD conducted. evaluated wide range
local global measures graph connectivity aim isolating
particularly suited task. Local measures include degree, page-rank, HITS, KPP
betweenness, whereas global measures consist compactness, graph entropy, edge
density. results indicate local measures yield better performance global
ones. best local measures Degree PageRank.
task wordnet development, adapt graph-based WSD method presented
work Navigli Lapata (2010), problem sense labelling corpus
using current parameters w,s . assumed true sense word
corpus determined senses words sentence. every sentence
corpus, following procedure executed:
word w sentence, candidate WordNet synsets picked, one
terminal node synset graph created. set terminal nodes
called Vw .
terminal node v, depth-first search (DFS) WordNet graph performed. Every time node v Vw (w = w ) along path length L encountered,
intermediate nodes edges path v v added graph. L
parameter algorithm usually takes small values 3, 4 5.
Terminal nodes graph scored according degree follows: node
v Vw ,
deg(v)
C(v) =
,
(3)
maxuVw (deg(u))
deg(v) number edges terminating v graph G = (V, E):
deg(v) = |{(u, v) E : u, v V }|,

(4)

Relations chosen graphs edges consist lexical semantic relations
defined WordNet addition gloss relation. pair synsets connected
via gloss relation unambiguous word w occurs gloss s. word
w must unambiguous; otherwise, connected appropriate
sense w (Navigli & Lapata, 2010). use gloss relation WSD procedure, sense
disambiguated glosses WordNet utilized (Semantically Tagged glosses, 2016),
69

fiTaghizadeh & Faili

word forms glosses WordNets synsets manually linked contextappropriate sense WordNet. Therefore, gloss relation established ,
appears correct sense word gloss .
time complexity calculating degree measure less PageRank,
performance shown better; last step WSD procedure,
degree measure preferred scoring nodes graph. illustrate steps
WSD procedure, provide example next section.
3.1.1 WSD Persian Sentence
order better understand WSD procedure, example presented. Consider following Persian sentence means Workers thirty years service become retired.

.

|{z}
punc

JPA


J PAK
AK. @Y
g K . @X
. IY
YK
| {z } | {z } | {z } | {z } |{z} |{z} | {z } |{z} | {z }
verb

adj

noun

noun noun num

noun

prep

noun

Preposition, number punctuation tags involved wordnet
/retired sentence. According Aryanignored. Consider word J PAK
.
pour dictionary, word three translations: emeritus; pensionary; retired. According
wordnet 3.0: first translation one noun synset one adjective synset;
second one two noun synsets; third one eleven verb synsets one
adjective synset. Since word noun adjective Persian corpus, verb
synsets ignored. definitions synsets follows:
{10051861} (noun.person) emeritus#1 (a professor minister retired
assigned duties)
{01645490} (adj.all) emeritus#1 (honorably retired assigned duties retaining title along additional title emeritus professor emeritus)
{10414612} (noun.person) pensioner#1, pensionary#1 (the beneficiary pension
fund)
{10176913} (noun.person) hireling#1, pensionary#2 (a person works
money)
{00035368} (adj.all) retired#1 (no longer active work profession)



. /retired consists five
Therefore, candidate set Persian word J PAK
synsets. general, synsets could correct sense sentence.
However, POS tag word given sentence come aid
WSD procedure order filter synsets. Indeed WSD procedure,
70

fiAutomatic Wordnet Development Low-Resource Languages

Table 1: Persian words candidate synsets.
Persian word


YJPA

@X

K .

g
IY

J PAK
.

POS
noun
noun
noun
noun

noun
adjective
verb

Translations
employee,
worker,
member
relieve, own,
year
background,
antecedent,
history,
record, service
work, job, activity,
profession
retired, emeritus,
wind, grow, lapse,
branch, become,

candidate synsets

selected synset

correct

10

workern1

3

1
4
40

have1n
yearn1
record1n

3
3
7

30

job1n

3

2
42

retired1a
growv3

3
7

synsets POS given POS sentence involved.
/retired adjective POS sentence, adjective
Since word J PAK
.
synsets involved graphs construction. Following steps
words sentence leads finding candidate synsets word
accounted WSD graph. Table 1 represents Persian words, translations,
number candidate synsets regarding POS tag Persian words.
candidate synsets represent terminal nodes WSD graph. Figure 3 shows,
candidate synsets Persian word given sentence grouped dotted
box.
next step, DFS algorithm run terminal node WordNet graph
length three. Upon finding path one terminal node another,
intermediate nodes edges added WSD graph. Part WSD graph
shown Figure 3. word graph associated POS, denoted
subscript: n stands noun, v verb, adjective, r adverb. superscript
denotes sense number associated word WordNet 3.0. graph three


/become A/year
separate components; one component word K
component remaining words. means word given sentence indicates
sense words.
construction WSD graph, correct sense Persian word
determined. this, synset degree among candidate set
/retired;
word chosen correct synset word. Consider word J PAK
.
1
WSD graph Figure 3, node retireda degree one; whereas node
emeritus1a degree zero. selected sense word retired1a . Using
degree measure, selected sense word given sentence determined,
represented bold box. Table 1 summarizes steps taken WSD procedure
given sentence. last column shows, selected sense words

/become.
correct except K. A/background
71

fiTaghizadeh & Faili

.
J PAK
retired1a

workn3

workn1

record1n

move2n

wind3n

historyn2
photographyn1

unf ortunate1n

grown3

ancendent1n
relative1n

job10
n
job6n

wind1n

be1n

...

processorn1

have1n

wind2n

person1n

job7n

employee1n
...

...

decade1n

yearn1

period1n

yearn3

season1n

yearn2



workern1


YJPA

job1n

...


@X

g
IY

activityn1

traveln1



YK

occupation1n employment2n

service5n

K .

prof ession1n

emeritus1a

...


JPA
.

J PAK
g K . @X
AK. @Y
. IY
Figure 3: Part WSD graph sentence YK
3.2 M-Step
maximization step, new estimation models parameters calculated
based sense-tagged corpus resulted expectation step. Similar
work Montazery Faili (2011), iteration j, new value parameter w,s ,
denotes probability assigning sense tag word w, equal averaging
conditional probability P (s|j1 ) dierent occurrences w corpus, j1
set parameters w,s iteration j 1. formal notation,
n
P (si |w1n , j1 )
i=1
w
=w,s
=s


j
w,s
=
,
(5)
N (w)
j
w,s
denotes value w,s iteration j, w1n presents sequence corpus words
N (w) number occurrence w w1n .
iteration EM algorithm, likelihood data given new parameter
values least great likelihood given old ones. EM behaves similar
gradient descent; step, adjusts parameter values improve
likelihood data. follows EM converges set parameter values
locally maximizes likelihood.
proposed EM method repeated changes probability selecting
candidate synset word target language becomes negligible. So, end
iteration, maximum change probabilities computed. value less
t, algorithm stops. execution EM algorithm, links score
threshold tremove (w,s tremove ) deleted wordnet. Also

72

fiAutomatic Wordnet Development Low-Resource Languages



. /retired per iteration.
Table 2: Assigned probabilities word J PAK
Synset ID
Noun:10051861
Adjective:01645490
Noun:10414612
Noun:10176913
Adjective:00035368
Entropy

Correct
7
7
7
7
3

Itr #0
0.2
0.2
0.2
0.2
0.2
2.1502

Itr #1
0.11111
0.29885
0.11111
0.11111
0.36781
1.8340

Itr # 2
0.11111
0.08315
0.11111
0.11111
0.58350
1.7880

Itr #3
0.11111
0
0.11111
0.11111
0.66666
1.7797

Itr #4
0.11111
0
0.11111
0.11111
0.66666
1.7781

Itr #5
0.11111
0
0.11111
0.11111
0.66666
1.7768

iteration, links current score ignored corresponding senses
presented graphs construction WSD procedure. end,
words target language mapped onto synset WordNet make
synsets resulting wordnet.
better follow process updating probabilities word per iteration,
example presented here. demonstrating probability adjustment iteration,
/retired. expectation step, words corpus
consider word J PAK
.
disambiguated. Next maximization step, new value probabilities

computed. Table 2 represents probabilities synsets assigned word J PAK
.
/retired iteration. first second columns show synset ID
correction synsets specified word, respectively. following columns represent
probability values first five iterations. Values less 0.005 considered
0. table shows probabilities start uniformly; iteration,
probability correct synsets increases probability incorrect synsets
frequent enough corpus decreases change. Indeed,
/retired corpus, tagged
number occurrences word J PAK
.
specific WordNet sense iteration iteration 1, probability
/retired change iteration i.
sense given word J PAK
.
value becomes greater, probability increases value becomes smaller,
probability decreases. particular example, five iterations, synset achieving
highest probability correct synset. iteration three, probability word
. /retired assigned second synset goes 3.9E-7,
J PAK
threshold. next iterations, synset considered WSD procedure
probability zero. last row table presents entropy value
respect iteration. steady decrease entropy indicates iteration,
distinction candidates synsets word becomes clear, leads
identification correct synsets. subject analysis entropy word
per iteration discussed later Section 4.2.1.

4. Case Study: Persian Language
section, proposed method automatic wordnet construction applied
Persian low-resource language. following subsections, experimental setup
evaluation methods described; that, results presented.
73

fiTaghizadeh & Faili

4.1 Experimental Setup Data
section, required resources setup experiments explained1 .
construct wordnet Persian language, Bijankhan Persian corpus2
used. collection gathered daily news common texts,
documents categorized dierent subjects political, cultural on.
Bijankhan contains ten million manually-tagged words tag set containing 550
fine-grained Persian POS tags (Oroumchian, Tasharofi, Amiri, Hojjat, & Raja, 2006).
Although POS tags explicitly used proposed method, get better WSD
results, one use POS tags prune synsets along tags candidate
set word explained Section 3.1.1. result, WSD procedure,
synsets POS tag word corpus taken part. WordNet,
four categories tags included: noun, verb, adverb adjective. Thus words
corpus tags pronoun preposition ignored.
Bijankhan large corpus. low-resource languages may large
corpus. order evaluate behaviour proposed method corpus size
limited, part Bijankhan picked training Persian wordnet.
PMI-based graph-based method conducted using part. part
includes nearly 13% total size corpus. remaining 87% used
testing phase coverage wordnet corpus evaluated.
details coverage analysis presented Section 4.2.4. Also, complete analysis
eect corpus size quality final wordnet presented Section
4.4.
words corpus appear inflected forms may found
dictionary. Therefore beginning proposed algorithm, lemmatizer
used dierent inflected forms words converted base form.
example, plural nouns converted singular form. this, STeP-1 tool
(Shamsfard, Jafari, & Ilbeygi, 2010b) utilized. STeP-1 package set
fundamental tools Persian text processing provides support tokenization, spell
checking, morphological analysis, POS tagging.
Another required resource proposed method bi-lingual machine readable
dictionary. electronic version Aryanpour dictionary3 used extract
English equivalent Bijankhan words. Also, WordNet version 3.0 used
extract synsets English equivalents.
WSD procedure, context word sentence containing word.
depth-first search WSD performed maximum depth 3 similar
work Navigli Ponzetto (2012b). mentioned Section 3.2, probability
WordNet sense given word w less equal t, sense ignored
WSD process EM algorithm. experiments, set = 0.005.

1. source code freely available download http://ece.ut.ac.ir/en/node/940
2. See http://ece.ut.ac.ir/dbrg/bijankhan/
3. See http://www.aryanpour.com

74

fiAutomatic Wordnet Development Low-Resource Languages

Iteration
Entropy

Table 3: Entropy values respect iteration
0
1
2
3
4
5
2.15025 1.83406 1.78804 1.77978 1.77813 1.77680

6
1.77677

4.2 Evaluation Results
section, results evaluation proposed method various experiments
presented.
4.2.1 Convergence Proposed Method
EM algorithm iterates expectation maximization steps,
criteria satisfied. experiment, iteration, entropy synset probabilities per word calculated average entropy words considered.
changing value two consecutive iterations becomes near zero, EM algorithm stops. Formally, entropy probability distribution defined equation
6:
H(w) =



w,s log(w,s ).

(6)



Entropy best understood measure uncertainty, entropy larger
random values. Indeed first, links Persian word equal probability,
maximum entropy granted. iteration, links sink threshold
probability thus probability links increases. expected final
step incorrect links obtain low probability correct links obtain
high probability. Therefore, entropy analysis demonstrate behaviour EM
method changing probabilities. Table 3, entropy values per iteration shown.
iteration 6, changing entropy values reaches predetermined threshold 0.001
EM algorithm stops.
4.2.2 Precision Recall Wordnet
primary goal work construct high quality wordnet low-resource
languages. execution EM algorithm, probability assigning candidate
synset word target language finalized. probabilities sorted
links probability threshold tremove removed final
wordnet. value tremove determines size wordnet aects quality
wordnet. So, experiments conducted used dierent values tremove
including 0.1, 0.05, 0.02, 0.01, 0.005 0.0.
evaluate resulting wordnet, re-implemented PMI-based method (Montazery & Faili, 2011) compared wordnet baseline. experiments,
wordnet referred graph-based wordnet, contrast PMI-based
wordnet. evaluation process, two data sets used: 1) FarsNet 2) Manual judges.
FarsNet semi-manually created wordnet Persian, available two versions;
second release FarsNet contains 36,000 Persian words phrases
organized 20,000 synsets nouns, adjectives, adverbs verbs. FarsNet 2
75

fiTaghizadeh & Faili

also inter-lingual relations connect Persian synsets English ones
Princeton wordnet 3.0.
second data set consists subset 1,750 links resulting wordnet,
selected randomly judged manually. link (w, s) given two annotators
decide Persian word w semantically equal WordNet s. ensure
accuracy judges, annotators selected among people native speakers
Persian time learn English professionally. case disagreement two judges, third annotator asked decide link. inter-annotator
agreement 80%, means 80% judgements, two annotators agreed.
Additionally, computed Cohens Kappa coecient (Cohen, 1960), two annotators,
takes account amount agreement could expected occur
chance. Kappa computed follows:
=

po pe
,
1 pe

(7)

po relative observed agreement among annotators, pe hypothetical
probability chance agreement. two annotators, Kappa value 0.55.
general, annotators complete agreement, = 1. agreement
annotators would expected chance (as given pe ), 0.
carrying manual judgements, precision recall resulting wordnet
measured set.
precision resulting wordnet defined number correct links
wordnet also exist test data correct links, divided total number links
wordnet exist test data. Also, recall wordnet defined
number correct links wordnet also exist test set correct links, divided
total number correct links test set. accuracy wordnet another
measure, defined number correct links wordnet also exist
test set plus number incorrect links test set exist wordnet,
divided total number links test set. definitions precision, recall,
accuracy wordnet also used BabelNet project (Navigli & Ponzetto,
2010).
Figure 4a Figure 4b represent precision recall PMI-based method
proposed method according FarsNet. shown, precision recall
wordnet better PMI-based method. figures, precision 18%,
seems low wordnet considered reliable resource language.
Additionally, recall 49%. due lack correct links FarsNet.
evaluation resulting wordnet according FarsNet link (w, s) placed
one categories:
Persian word w exist FarsNet. link ignored counted.
Persian word w exists FarsNet; however WordNet synset given it.
link ignored counted.
Persian word w exists FarsNet least one WordNet synset given it.
one WordNet synsets, link counted correct else counted
incorrect.
76

fiAutomatic Wordnet Development Low-Resource Languages

WordNet sense distinctions fine-grained, meaning several WordNet
synsets may mapped onto one synset FarsNet; given
FarsNet. Therefore, correct links wordnet counted incorrect. Figure 4c
shows accuracy wordnets according FarsNet, shows graph-based
wordnet surpasses PMI-based wordnet.
reasons low precision according FarsNet follows:
Translations Persian words inaccurate incomplete, meaning
correct WordNet synset according FarsNet exist candidate set.
J/motaalleqAt/possession, three equivalent
example, Persian word HA
English words written Aryanpour dictionary: Appurtenance, Paraphernalia,
J/possession determined
Belongings. wordnet, correct synsets HA
follows: {13244109} (noun.possession), property#1, belongings#1, holding#2
(something owned; tangible intangible possession owned someone;
hat property; man property). However according FarsNet,
correct synset {00032613} (noun.Tops) possession#2 (anything owned
J/possession synset Noun-02671421
possessed). evaluation, link HA
considered incorrect penalized.
Persian word lemmatized correctly; English translations consequently candidate set contain correct synset. example, Persian
word @P K. /bArAk/Barak proper noun, stemmer recognizes PA K.
/bAr/load stem, means load.
resolve problems, set manually judged links used second
experiment. Figure 5 represents precision recall resulting wordnet dierent
values tremove according manual judges. Parameter tremove demonstrates threshold,
links score lower deleted final wordnet. High
values tremove result wordnet high precision low recall. hand,
low values tremove cause low precision high recall wordnet. Thus trade-o
precision recall. tremove = 0.1, precision PMI-based wordnet
86%, precision wordnet created proposed method 90% according
manual judges. tremove = 0, means links contained final
wordnet, precision 74%. Therefore, initial wordnet seen without executing
EM algorithm 74% precision. Figures 4d 5c show another quality measure
wordnets, F -measure. Definition F -measure complete analysis
presented Section 4.3.
4.2.3 Size Polysemy Rate Wordnet
One important aspects wordnets size. Large wordnets may tens
thousands sysnsets (Miller, 1995; Patanakul & Charnyote, 2005; Black et al., 2006;
Piasecki et al., 2011). hand, wordnets polysemic words
useful NLP IR tasks. Polysemic words words one
sense wordnet. Finding correct sense polysemic words great significance
automatic wordnet construction.
77

fiTaghizadeh & Faili

Graph-based
PMI-based

0.15

Recall

Precision

0.8

0.6

Graph-based
PMI-based

0.1
0

2 102 4 102 6 102 8 102

0.4

0.1

0

2 102 4 102 6 102 8 102

tremove

0.1

tremove

(a) Precision

(b) Recall

0.8

F1

Accuracy

0.25
0.7
0.6
Graph-based
PMI-based

0.5
0

2 102 4 102 6 102 8 102

0.2
Graph-based
PMI-based

0.15

0.1

0

2 102 4 102 6 102 8 102

tremove

0.1

tremove

(c) Accuracy

(d) F-measure

Figure 4: Comparison wordnets according FarsNet.
0.8

Recall

0.85
Graph-based
PMI-based

0.8
0

2 102 4 102 6 102 8 102

Graph-based
PMI-based

0.6

0.4

0.1

0

2 102 4 102 6 102 8 102

tremove

tremove

(a) Precision

(b) Recall
0.8

Graph-based
PMI-based

0.7
F1

Precision

0.9

0.6
0.5
0

2 102 4 102 6 102 8 102

0.1

tremove

(c) F-measure

Figure 5: Comparison wordnets according manual judges.
78

0.1

fiAutomatic Wordnet Development Low-Resource Languages

Table 4: Comparison size wordnets

Threshold
0.1
0.05
0.02
0.01
0.005
0

PMI-based wordnet
unique words word-synset polysemy
11,880
27,358
0.63
11,969
36,922
0.71
11,974
49,070
0.76
11,974
58,874
0.78
11,974
71,761
0.80
11,974
141,103
0.85

Graph-based wordnet
unique words word-synset polysemy
11,899
29,944
0.73
11,972
43,690
0.79
11,972
61,823
0.80
11,972
74,619
0.80
11,972
86,879
0.83
11,972
141,103
0.85

section, size resulting wordnet polysemy rate two wordnets,
PMI-based graph-based wordnets, reported. Table 4 presents number unique
words, number Persian word-WordNet synset links proportion polysemic words based dierent values tremove . tremove decreases 0.1 0.01,
unique words contained wordnets, number word-synset links increases,
also proportion polysemic words unique words wordnet increases.
seen, wordnet created result graph-based method surpasses PMI-based
wordnet.
links included wordnet, polysemic words 85% unique
words. However, wordnet, removing links probability less
0.1, 73% words polysemic, 10% better PMI-base wordnet.
tremove = 0.1, wordnets 12,000 unique words. Since methods
executed corpus, significant dierence sizes.
4.2.4 Coverage Wordnet
evaluate coverage resulting wordnet, interested observing coverage
WordNet synsets also coverage language words. section, three
experiments performed: 1) core concepts coverage, 2) WordNet synset coverage,
3) corpus coverage.
first experiment, coverage wordnet core synsets evaluated. BoydGraber et al. (2006) published list 5,000 word-senses WordNet 3.0,
contains 5,000 frequently used word-senses (Core WordNet, 2015). Coverage
wordnet list regarded covering common concepts
language. core wordnet used measure percentage synsets list
covered PMI-based graph-based wordnets. Figure 6a represents core coverage
dierent values tremove . Selecting links, (tremove = 0), causes coverage 88%
core wordnet, choosing links probable 0.1, leads coverage
53% 34% core wordnet graph-based PMI-based wordnets, respectively.
second experiment, coverage wordnets WordNet synsets studied.
Since resulting wordnet multi-lingual wordnet, coverage WordNet
synsets measure quality. Figure 6b represents coverage PMI-based
graph-based wordnets WordNet 3.0 synsets dierent values tremove . figure
shows graph-based wordnet covers WordNet synsets PMI-based wordnet
values tremove . example, selecting links probability higher 0.1,
79

fiGraph-based
PMI-based

0.8
Core Coverage

WordNet synsets Coverage

Taghizadeh & Faili

0.6

0.4
0

2 102 4 102 6 102 8 102

0.1

0.25

Graph-based
PMI-based

0.2
0.15
0.1
2 102 4 102 6 102 8 102

0

tremove

0.1

tremove

(a) Core Coverage

(b) WordNet Coverage

Figure 6: Coverage wordnets core synsets synsets WordNet.
Table 5: Comparison coverage wordnets.

FarsNet
PMI-based wordnet
graph-based wordnet

Coverage Bijankhan (unique words)
3,050
11,523
11,543

graph-based wordnet covers 14% WordNet synsets; PMI-based wordnet
covers 10% WordNet synsets.
third experiment, coverage wordnets Bijankhan corpus evaluated.
Bijankhan large corpus proposed method trained 13% it. rest
corpus used measuring word coverage wordnets. Table 5 demonstrates
number unique words corpus, covered PMI-based graph-based wordnets,
tremove = 0.1. evaluation also performed FarsNet baseline
also presented Table 5. Although training testing corpus separate,
significance dierence FarsNet EM-based wordnets coverage.
4.3 Parameter Selection
proposed method wordnet construction convergence EM algorithm,
set links words target language synsets source language
obtained. links scored lower threshold tremove removed final
wordnet. previous experiments showed, value tremove aects quality
resulting wordnet. experiments section 4.2.2 illustrated changing tremove
0.005 0.1 positive eect precision negative eect recall
resulting wordnet. Indeed, trade-o precision recall.
question may arise; best value tremove .
section, F -measure used investigate quality wordnet, considering
precision recall. formula F1 follows:
F1 = 2.

precision.recall
,
precision + recall
80

(8)

fiAutomatic Wordnet Development Low-Resource Languages

F1 harmonic mean precision recall. order gain insight
optimum value tremove , F1 resulting wordnet calculated
dierent values tremove . precision recall, F1 calculated
manual judgement FarsNet. Figure 5c shows F1 decreases 77%
50% tremove increases 0.005 0.1 graph-based wordnet according
manual judgement. means precision value important
recall rate precision decreasing higher rate recall
increasing. Therefore, gain precise wordnet, increase tremove ; however,
must accept loosing recall.
hand, Figure 4d shows highest value F1 graph-based
wordnet obtained tremove = 0.1 according FarsNet. fact means
recall value eect F1 precision value. reason dierence
due low precision values obtained evaluation according
FarsNet, reported Section 4.2.2. FarsNet lacks correct mappings
Persian words WordNet synsets. Indeed wordnet construction, precision
final wordnet important recall.
Finally, choosing threshold tremove important eect quality resulting
wordnet. However, matter depends application. applications,
precise wordnet preferential large accurate enough one.
cases, greater values tremove preferential. Although, applications high
recall needed, one choose low values tremove .
4.4 Eect Corpus Size Dictionary
section, eect required resources final wordnet looked at.
proposed method needs bi-lingual dictionary mono-lingual corpus. previous
experiments, Aryanpour dictionary Bijankhan corpus used. Since
Bijankhan large corpus 13% used previous experiments.
investigate eect corpus size quality resulting wordnet, proposed
method executed using four sizes Bijankhan: 5%, 10%, 20% 50%.
Additionally, examine eect dictionary quality final wordnet,
Google translator4 used another experiment instead Aryanpour; resulting
wordnet compared wordnet created using Aryanpour size
Bijankhan. link removal threshold tremove experiments section
0.1. resulting wordnets evaluated precision, recall, accuracy, coverage
WordNet core synsets, coverage synsets WordNet, number
Persian words.
shown Figure 7, size corpus increases 5% 50%
Bijankhan corpus using dictionary, measures increase except precision,
either change changes slightly. result beyond expectation. Indeed, precision resulting wordnet depends precision WSD
procedure depend size corpus. However, new possible senses
words discovered increasing size corpus therefore recall,
accuracy, coverage size wordnet increase growth corpus size.
4. http://translate.google.com/

81

fiTaghizadeh & Faili

Aryanpour dictionary
Google translator
0.5

0.38

0.9

0.89

accuracy

recall

precision

0.36
0.34
0.32

0.48

0.46

0.3
0.28

0.88
5 10

20

5 10

50

20

5 10

50

20

(a) precision

50
size

size

size

(b) recall

(c) accuracy
104

0.56

0.16

1.4

0.52
0.5
0.48
0.46

number words

synset coverage

core coverage

0.54
0.14

0.12

1.2

1

0.1

0.8

0.44
5 10

20

50

5 10

20

size

(d) core coverage

50
size

(e) WordNet synset coverage

5 10

20

50
size

(f) number words

Figure 7: Evaluation resulting wordnet trained dierent sizes Bijankhan.

Figure 7f demonstrates, wordnet least 10,000 words, corpus size
least 10% Bijankhan corpus. Figure 7 also illustrates wordnet trained
Aryanpour dictionary excels wordnet derived Google translator.
experiment demonstrates dictionary heavily aects final wordnet even
corpus size. result, small corpus large dictionary results
precise wordnet large corpus small dictionary.
last experiment, proposed method executed using full Bijankhan
corpus Aryanpour dictionary. precision, recall accuracy resulting
wordnet 90%, 41% 52%, respectively. Comparing wordnet, created
using 13% Bijankhan dictionary, recall accuracy increased 6%
3%, accordingly; precision change. wordnet 15,406 Persian
word covers 61% core synsets WordNet. Considering synsets
WordNet, covers 20% them.

5. Conclusion
paper, EM algorithm employed order develop wordnet lowresourced languages. successfully applied unsupervised cross lingual WSD expectation step algorithm. proposed method use features specific
82

fiAutomatic Wordnet Development Low-Resource Languages

target language, used languages generate wordnets.
Resources needed proposed algorithm include bi-lingual dictionary monolingual corpus. proposed method belongs expansion approach creates
multi-lingual wordnet word target language, equivalent synset
WordNet known.
proposed method applied Persian language quality resulting wordnet examined several experiments. precision 18% according
FarsNet 90% according manual judgement. reason dierence
WordNet synsets fine-grained comparison FarsNet synsets,
synsets FarsNet mapped onto one synset WordNet;
however FarsNet provides one two WordNet synsets FrasNet synsets.
problem means correct links resulting wordnet considered
incorrect thus reported precision becomes low. Also, resulting wordnet
contains 12,000 words Persian language using 13% Bijankhan
corpus, several wordnets languages. Additionally, 53% core
synsets 14% synsets WordNet covered. Analysis eects corpus
size dictionary size resulting wordnet showed dictionary size aect
precision wordnet corpus size therefore important use
large-enough dictionaries.

Acknowledgements
research part supported Institute Research Fundamental Sciences
(No. CS1395-4-19).

References
Apidianaki, M., & Sagot, B. (2014). Data-driven synset induction disambiguation
wordnet development. Language Resources Evaluation, 48 (4), 655677.
Atserias, J., Climent, S., Farreres, X., Rigau, G., & Rodrguez, H. (2000). Combining
multiple methods automatic construction multilingual wordnets. Amsterdam
studies theory history linguistic science series 4, 327340.
Barbu, E., & Barbu Mititelu, V. (2005). case study automatic building wordnets.
Proceedings OntoLex 2005- Ontologies Rexical Resources, pp. 8590, Jeju
Island, Korea. Asian Federation Natural Language Processing.
Basile, P., Caputo, A., & Semeraro, G. (2014). enhanced lesk word sense disambiguation
algorithm distributional semantic model. Proceedings COLING 2014,
25th International Conference Computational Linguistics: Technical Papers,
pp. 15911600, Dublin, Ireland. International Committee Computational Linguistics.
Black, W., Elkateb, S., & Vossen, P. (2006). Introducing Arabic wordnet project.
Proceedings Third International WordNet Conference (GWC-06), pp. 295299,
South Jeju Island, Korea. Global WordNet Association.
83

fiTaghizadeh & Faili

Bond, F., & Foster, R. (2013). Linking extending open multilingual wordnet. Proceedings 51st Annual Meeting Association Computational Linguistics,
pp. 13521362, Sofia, Bulgaria. Association Computational Linguistics.
Bond, F., Isahara, H., Kanzaki, K., & Uchimoto, K. (2008). Boot-strapping wordnet using
multiple existing wordnets. Proceedings Sixth International Conference
Language Resources Evaluation (LREC08), pp. 16191624, Marrakech, Morocco.
European Language Resources Association (ELRA).
Boudabous, M. M., Chaaben Kammoun, N., Khedher, N., Belguith, L. H., & Sadat, F.
(2013). Arabic wordnet semantic relations enrichment morpho-lexical patterns. Proceeding 1st International Conference Communications, Signal Processing, Applications (ICCSPA), pp. 16, American University Sharjah,
United Arab Emirates. IEEE.
Boyd-Graber, J., Fellbaum, C., Osherson, D., & Schapire, R. (2006). Adding dense, weighted
connections WordNet. Proceedings third International WordNet Conference (GWC-06), pp. 2935, South Jeju Island, Korea. Global WordNet Association.
Buitelaar, P., & Cimiano, P. (2014). Towards Multilingual Semantic Web. Springer
Berlin Heidelberg.
Cohen, J. (1960). coecient agreement nominal scales. Educational Psychological Measurement, 20 (1), 3746.
Core

WordNet (2015)
core-wordnet.txt.

http://wordnetcode.princeton.edu/standoff-files/

Diab, M. (2004). feasibility bootstrapping Arabic wordnet leveraging parallel
corpora English WordNet. Proceedings Arabic Language Technologies
Resources, Cairo, NEMLAR.
Dini, L., Peters, W., Liebwald, D., Schweighofer, E., Mommers, L., & Voermans, W. (2005).
Cross-lingual legal information retrieval using WordNet architecture. Proceedings
10th international conference Artificial intelligence law (ACAIL), pp.
163167, Bologna, Italy. ACM.
Elkateb, S., Black, W., Rodrguez, H., Alkhalifa, M., Vossen, P., Pease, A., & Fellbaum, C.
(2006). Building wordnet Arabic. Proceedings 5th international conference Language Resources Evaluation (LREC 2006), Genoa, Italy. European
Language Resources Association (ELRA).
Erjavec, T., & Fiser, D. (2006). Building Slovene wordnet. Proceedings 5th International Conference Language Resources Evaluation (LREC 2006), Genoa,
Italy. European Language Resources Association (ELRA).
Fellbaum, C., & Vossen, P. (2012). Challenges multilingual wordnet. Language Resources Evaluation, 46 (2), 313326.
Fiser, D. (2009). Human language technology. Leveraging Parallel Corpora Existing
Wordnets Automatic Construction Slovene Wordnet, pp. 359368. Springer
Berlin Heidelberg.
84

fiAutomatic Wordnet Development Low-Resource Languages

Gunawan, G., & Saputra, A. (2010). Building synsets Indonesian wordnet monolingual lexical resources. Proceedings International Conference Asian Language
Processing (IALP), pp. 297300, Harbin, China. IEEE.
Hasanuzzaman, M., Caen, F., Dias, G., Ferrari, S., & Mathet, Y. (2014). Propagation strategies building temporal ontologies. Proceedings 14rd conference European
Chapter Association Computational Linguistics, pp. 611, Guthenburg, Sweden. Association Computational Linguistics.
Kaji, H., & Watanabe, M. (2006). Automatic construction Japanese wordnet. Proceedings 5th International Conference Language Resources Evaluation
(LREC 2006), Genoa, Italy. European Language Resources Association (ELRA).
Kazakov, D., & Shahid, A. R. (2009). Unsupervised construction multilingual wordnet
parallel corpora. Proceedings Workshop Natural Language Processing
Methods Corpora Translation, Lexicography, Language Learning, pp. 912,
Borovets, Bulgaria. Association Computational Linguistics.
Lam, K. N., Al Tarouti, F., & Kalita, J. (2014). Automatically constructing wordnet synsets.
52nd Annual Meeting Association Computational Linguistics (ACL 2014),
pp. 106111, Baltimore, USA. Association Computational Linguistics.
Landes, S., Leacock, C., & Tengi, R. I. (1998). Building semantic concordances. WordNet:
electronic lexical database, 199 (216), 199216.
Mallery, J. C. (1988). Thinking foreign policy: Finding appropriate role artificially intelligent computers. Ph.D. thesis, MIT Political Science Department.
Miller, G. A. (1995). WordNet: lexical database english. Communications ACM,
38 (11), 3941.
Montazery, M., & Faili, H. (2010). Automatic Persian wordnet construction. Proceedings
23rd International Conference Computational Linguistics: Posters, pp. 846
850, Beijing, China. Association Computational Linguistics.
Montazery, M., & Faili, H. (2011). Unsupervised learning Persian wordnet construction.
Proceedings Recent Advances Natural Language Processing (RANLP), pp.
302308, Hissar, Bulgaria. Association Computational Linguistics.
Navigli, R. (2009). Word sense disambiguation: survey.
(CSUR), 41 (2), 10.

ACM Computing Surveys

Navigli, R., & Lapata, M. (2010). experimental study graph connectivity unsupervised word sense disambiguation. Pattern Analysis Machine Intelligence, IEEE
Transactions on, 32 (4), 678692.
Navigli, R., & Ponzetto, S. P. (2010). BabelNet: Building large multilingual semantic network. Proceedings 48th annual meeting association computational linguistics, pp. 216225, Uppsala, Sweden. Association Computational
Linguistics.
Navigli, R., & Ponzetto, S. P. (2012a). BabelNet: automatic construction, evaluation
application wide-coverage multilingual semantic network. Artificial Intelligence, 193, 217250.
85

fiTaghizadeh & Faili

Navigli, R., & Ponzetto, S. P. (2012b). Multilingual WSD lines code:
BabelNet API. Proceedings 50th Annual Meeting Association Computational Linguistics (ACL 2012), pp. 6772, Jeju, Republic Korea. Association
Computational Linguistics.
Oliver, A., & Climent, S. (2012). Parallel corpora wordnet construction: machine translation vs. automatic sense tagging. Proceedings 13th International Conference
Intelligent Text Processing Computational Linguistics, pp. 110121, New Delhi,
India. Springer.
Oroumchian, F., Tasharofi, S., Amiri, H., Hojjat, H., & Raja, F. (2006). Creating feasible
corpus Persian POS tagging. Tech. rep. TR3/06, University Wollongong, Dubai.
Otegi, A., Arregi, X., Ansa, O., & Agirre, E. (2015). Using knowledge-based relatedness
information retrieval. Knowledge Information Systems, 44 (3), 689718.
Patanakul, S., & Charnyote, P. (2005). Construction Thai wordnet lexical database
machine readable dictionary. Conference Proceedings: tenth Machine
Translation Summit, pp. 8792, Phuket, Thailand. Language Technology World.
Piasecki, M., Kurc, R., & Broda, B. (2011). Heterogeneous knowledge sources graph-based
expansion Polish wordnet. Intelligent Information Database Systems,
Vol. 6591, pp. 307316. Springer.
Prabhu, V., Desai, S., Redkar, H., Prabhugaonkar, N., Nagvenkar, A., & Karmali, R. (2012).
ecient database design IndoWordNet development using hybrid approach.
Proceedings 3rd Workshop South Southeast Asian Natural Language
Processing (SANLP), pp. 229236, Mumbai, India. International Committee Computational Linguistics.
Rodrquez, H., Farwell, D., Ferreres, J., Bertran, M., Alkhalifa, M., & Mart, M. A.
(2008). Arabic wordnet: Semi-automatic extensions using Bayesian inference.
Proceedings Sixth International Conference Language Resources Evaluation (LREC08), Marrakech, Morocco. European Language Resources Association
(ELRA).
Saveski, M., & Trajkovski, I. (2010). Automatic construction wordnets using machine translation language modeling. Proceedings 13th International
Multiconference, pp. 7883, Ljubljana, Slovenia. Information Society.
Semantically Tagged glosses (2016) http://wordnet.princeton.edu/glosstag.shtml.
Shamsfard, M. (2008). Towards semi automatic construction lexical ontology Persian. Proceedings 6th International Conference Language Resources
Evaluation (LREC 2008), Marrakech, Morocco. European Language Resources Association (ELRA).
Shamsfard, M., Hesabi, A., Fadaei, H., Mansoory, N., Famian, A., Bagherbeigi, S., Fekri, E.,
Monshizadeh, M., & Assi, S. M. (2010a). Semi automatic development FarsNet;
Persian wordnet. Proceedings 5th Global WordNet Conference, Mumbai, India.
Global WordNet Association.
86

fiAutomatic Wordnet Development Low-Resource Languages

Shamsfard, M., Jafari, H. S., & Ilbeygi, M. (2010b). STeP-1: set fundamental tools
Persian text processing. Proceedings 7th International Conference Language Resources Evaluation (LREC 2010), Valletta, Malta. European Language
Resources Association (ELRA).
Tufis, D., Cristea, D., & Stamou, S. (2004). BalkaNet: Aims, methods, results perspectives. general overview. Romanian Journal Information Science Technology,
7 (1-2), 943.
Vossen, P. (1998). Introduction EuroWordNet. EuroWordNet: multilingual database
lexical semantic networks, pp. 117. Springer.

87

fi
