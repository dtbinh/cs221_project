Journal Artificial Intelligence Research 10 (1999) 399-434

Submitted 10/98; published 6/99

Extensible Knowledge Representation: Case
Description Reasoners
Alex Borgida

borgida@cs.rutgers.edu

Dept. Computer Science
Rutgers University
New Brunswick, NJ 08904 USA

Abstract
paper offers approach extensible knowledge representation reasoning
Description Logic family formalisms. approach based notion adding
new concept constructors, includes heuristic methodology specifying desired
extensions, well modularized software architecture supports implementing
extensions. architecture detailed falls normalize-compared paradigm,
supports intentional reasoning (subsumption) involving concepts, extensional
reasoning involving individuals incremental updates knowledge base.
resulting approach used extend reasoner specialized notions
motivated specific problems application areas, reasoning
dates, plans, etc. addition, provides opportunity implement constructors
currently yet sufficiently well understood theoretically, needed practice.
Also, constructors provably hard reason (e.g., ones whose presence
would lead undecidability), allows implementation incomplete reasoners
incompleteness tailored acceptable application hand.

1. Introduction Motivation
Description Logics (DLs) family object-centered formalisms representing knowledge reasoning individuals grouped classes (here called concepts)
related binary relations (here called roles). Descriptions usually term-like notation uses concept constructors identifiers build definitions complex
concepts simpler ones. example, description Figure 1 supposed capture
noun phrase collection objects books written two
authors, Venusians.
and(
BOOK
at-least(2,authoredBy)
all(authoredBy, VENUSIAN) )
Figure 1: example description.
accomplished using concept constructor conjoin terms represent
component notions:
c
1999
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiBorgida

BOOK: objects books concept declared elsewhere, probably primitive;
at-least(2,authoredBy): objects related least 2 objects authoredBy role; concept constructor at-least here;
all(authoredBy, VENUSIAN): objects related authoredBy role objects concept VENUSIAN; concept constructor all.
concept VENUSIAN might defined whose address includes planet
value Venus: and(BEING, all(address, fills(planet, Venus))). (Note descriptions
nested.) precise introduction DLs presented Section 2.
Description logics reason intensional notions concepts,
extensional aspects individuals ascribed descriptions
participate specific relationships. DLs found variety applications areas
data management (Borgida, 1995), software engineering (Devanbu & Jones, 1997),
configuration management (Wright et al., 1993), well general AI.
particular description language characterized, among others, choice term
constructors it, significant features DLs clear, precise semantics terminating reasoning algorithms tasks determining whether concept coherent,
whether general another one.
common difficulty faced designers users knowledge-base management
systems (KBMSs) based DLs (or logic, matter) many applications need keep information specialized kinds data, including strings, dates,
pictures, sequences values various kinds, etc., practically impossible anticipate part language design.
related problem reasoning complete efficient, even decidable,
expressed language must limited. example, Figure 1
also wanted say authors speak language book written,
concept constructor already discussed literature, called subset-of,
one could express constraint subset-of( [writtenIn] [authoredBy,speaks] ).
Unfortunately, reasoning language supports constructors and, subset-of
known undecidable (Schmidt-Schauss, 1989). means selection concept
constructors language matter careful consideration system designer,
faced several choices: (a) Select particular subset constructors
sound complete reasoning algorithm known; language sufficiently limited,
procedure guaranteed fast (e.g., polynomial time); otherwise, worst case
complexity non-polynomial, though practice algorithm may behave well. Living
limited languages however always easy (Doyle & Patil, 1991). (b) Choose
larger set constructors (possibly one even undecidable), implement
incomplete reasoner; however requires explain user inferences
made. alternatives different shortcomings, but, viewpoint,
significant common problem cases designer DL system
makes decisions ahead time, leaving user sort consequences later on.
propose attack problems starting relatively small, kernel
language system, providing facilities extend adding new concept constructors. extensions undertaken lightly, since provide opportunities
400

fiExtensible DL Representation Reasoning

errors. extensions standard DL constructors complete reasoning implemented. extensions could readily available library1 .
extensions involve standard DL constructors known hard reason
with, incomplete reasoning algorithm provided, because, example, problem undecidable algorithms currently known involve
combinatorial search. second case, consultation user may help determine
inferences implementation make. example, classic, sake
clearer semantics system designers chose draw inferences individual
based presence concept (Borgida & Patel-Schneider, 1994), although
inferences hard implement. Finally, new concept constructors added
notions either domain specific (e,g,. plans, time), great practical utility whose interaction full spectrum DL constructors yet fully
understood theoretically; notions include keys/unique identifiers (Borgida & Weddell,
1997) part-whole relationships (Padgham & Lambrix, 1994).
support this, develop well-modularized architecture DL-KBMSs
implemented using normalize-compare approach (see Section 2.4); architecture
expects set procedures filled new concept constructor extending
original language. addition, propose methodological heuristics specifying
procedures need do. Note goal obtain close efficiency
would offered custom-built DL reasoner.
course, approach presented panacea knowledge representation
reasoning. First all, extent normalize-compare algorithms unable
reason complete manner DL constructors involving incomplete knowledge
disjunction, present system also likely suffer deficiencies. Second,
present work yet addressed DL notions role constructors, recursive concepts,
general constraints (to described later). Finally, many notions
knowledge representation, full spectrum epistemic non-monotonic
reasoning, abduction, case-based reasoning, etc., likely require thorough
overhaul entire reasoning architecture, hence likely accommodated
properly present approach.
outline rest paper follows: Section 2 provides introduction
DLs, syntax semantic description, services provided KBMSs concerning
reasoning concepts, especially subsumption relationship. Section 3 introduces
architecture proposed protodl approach DL reasoning, provides overview
methodology extending it, illustrates constructors involving dates;
terminates discussing successes limitations proposed protodl approach
extension, relationship one particular approach directly relevant.
Sections 4 5 repeat process, considering time reasoning
individuals, focusing support efficiently incremental updates
knowledge base.
conclude discussing relevant related work summarizing contributions
limitations protodl approach.
1. important clarify beginning addition constructors often simple
incremental process: constructors may behave well independently, cause problems brought
language.

401

fiBorgida

2. Description Logics: Introduction
DLs used describe situations using various kinds individuals, related roles,
grouped concepts. Roles restricted (partial) functions distinguished,
called attributes.
section present syntax semantics DLs, well outlining
interaction typical DL-based KBMS, implementation strategies.
2.1 Syntax Semantics
illustrated Figure 1, DLs provide compositional structured language
talking kinds things. Composite concepts obtained according
syntax presented Table 1, includes concept constructors mentioned
paper. meta-symbols following referents: CN concept name, p
atomic role (including attribute), f attribute, C general concepts, b
individual, n integer; subscripts may occasionally added above.
RoleChain ::= [p1, . . . , pn]
AttributeChain ::= [f1 , . . . , fn ]
C ::= thing | nothing | CN
| and( C1 , . . . , Cn ) | at-least(n,p) | at-most(n,p)
| all(p,C) | some(p,C) | fills(p,b) | one-of(b1 , . . . bn )
| same-as(AttributeChain1 , AttributeChain2 )
| subset-of(RoleChain1 , RoleChain2 )
Table 1: Syntax Concept Constructors Used.
give meaning syntactic terms, one give descriptions denotational
semantics using interpretation I=(I ,I ). starts assigning concept name
subset domain , role subset , attribute element
restricted functional, individual element .
interpretation extended composite terms follows. First, role chains (resp.
attribute chains) interpreted mappings resulting relation composition:


[p1, , pn] = {x 7 Sx | Sx = {y | z1 , ..., zn+1. z1 = x zn+1 =

n
^

(zi , zi+1 ) pIi }}

i=1

Table 2 presents interpretation complex terms using interpretation
components.
Alternatively, noting concepts like unary predicates, roles like binary
predicates, offer translation schemes descriptions Predicate Calculus.
example, and(AMERICAN,OLD) corresponds formula AMERICAN()OLD(),
free variable, all(authoredBy,VENUSIAN) corresponds y.authoredBy(, y)
VENUSIAN(y).
connection two kinds specifications interpretations
applied predicate calculus formulas well concepts (Baader, 1996; Borgida,
402

fiExtensible DL Representation Reasoning

TERM
thing
nothing
and( C1 , . . ., Cn )
at-least(n, p)
at-most(n, p)
all(p, C)
some(p, C)
fills(p, b)
one-of(b1 ,...,bm)
same-as(F C1 , F C2)
subset-of(RC1 , RC2)

INTERPRETATION


C1I . . . CnI
{ | |pI (d)| n }
{ | |pI (d)| n }
{ | pI (d) C }
{ | pI (d) C 6= }
{ | bI pI (d) }
{ bI1 , . . . , bIm }
{ | F C1I (d) = F C2I (d) F C1I (d) 6= }
{ | RC1I (d) RC2I (d) }

Table 2: Interpretation DL Constructors.
1996): given description C, corresponding logical formula C (), interpretation I, denotation C identical set values C ()I true,
extended map d.
Finally, remark many DLs also allow role constructors; example, isAuthorOf
defined converse authoredBy relation, writing inverse(authoredBy).
paper consider role constructors (hence equating roles role names).
2.2 Subsumption Reasoning Concepts
real significance DLs one reason descriptions. Traditionally,
standard question one asks whether one description general (subsumes)
another. example, would expect description Figure 1 subsumes
following description, requires addition books published
four countries enumerated, least three (rather two) authors,
married earthlings:
and(
BOOK
all(publishedIn, one-of(Usa,France,Germany,Italy))
at-least(3,authoredBy)
all(authoredBy, and( VENUSIAN,
all(marriedTo, TERRESTRIAL)))
)
Formally, subsumption concepts C D, written C = D, holds iff C DI
interpretations I. addition, say description C incoherent iff
denotation C empty set interpretations I. Note case iff
C = nothing.
semantics concept constructors, especially deductions performed
particular system also specified using proof theory. proof-theoretic specification
403

fiBorgida

natural semantics style (Borgida, 1992a), would indicate, among others, greater
bounds number role fillers role p lead specialized concepts:
mn
at-least(m, p) = at-least(n, p)

rule paraphrased one prove n one write next
line proof at-least(m, p) = at-least(n, p). rule allows us prove
at-least(2,authoredBy) = at-least(1,authoredBy).
usual, possible (and desirable) demonstrate rules inference
sound complete respect denotational semantics. Rules inference useful
characterizing reasoners incomplete respect standard denotational
semantics, describing either inferences performed, ones missing. Also,
shall argue below, rules inference form good starting point developing
implementations.
2.3 DL Concept KBMS
concept knowledge base CKB (also known T-box) records constraints concept
names, including definitions (such concept VENUSIAN mentioned examples)
necessary conditions primitive concepts (e.g., BOOK would required least
one author). DLs possible state general subsumption constraints
arbitrary descriptions, permitted paper.
Formally, CKB tuple (R, F , C, O, N , D) R, F , C, respectively sets
role, attribute, concept individual object identifiers declared. Concept names
either primitive/atomic concept names, P N , associated necessary condition
C set N necessary conditions; defined concept names, DN ,
PN
.
associated definition DN = C set definitions. interpretation model
.
C iff P N C , model DN =
P N
C iff DN = C . interpretation
model CKB model conditions N D.
C said subsume presence knowledge base CKB (written CKB
|= C = D), iff C DI models CKB.
paper restricting non-recursive knowledge bases, definitions necessary conditions given time name declared,
involve previously declared identifiers.
DL-based knowledge base management system supports certain update operations,
affect CKB (R, F , C, O, N , D) follows:
Operation
declare-primitive-role(p)
declare-primitive-attribute(f )
declare-individual(b)
declare-primitive-concept(P N ,D)
declare-defined-concept(CN ,D)

Effect
p added R
f added F
b added
N
P N added C, P N
.
CN added C, CN =

return, expect KBMS respond inquiries, include retrieving declarations entered least following operations:
404

fiExtensible DL Representation Reasoning

Question
ask-subsumes?(C, D)
ask-ancestors(C)
ask-is-incoherent?(C)

Answer type
Boolean
SET(ConceptName)
Boolean

Response
true iff CKB |= C =
{CN C | CKB |= C = CN }
true iff CKB |= C = nothing

number operations concepts found useful, including
(i) computing least common subsumer two concepts (Cohen, Borgida, & Hirsh,
1992) (useful machine learning), (ii) matching concepts patterns (Borgida &
McGuinness, 1996) (useful large KBs viewing relevant aspects concepts), (iii)
finding additional information deduced concept especially
individual given information KB (see Section 4.2).
facilitate answering questions, others like them, DL-KBMS almost
always performs concept classification: named concepts (classes) organized
so-called IsA hierarchy, finding class specific classes subsume
it. classification algorithm relies = relationships, treating subroutine,
largely DL-independent. Interesting previous work area
reported Baader et al (Baader et al., 1994).
KBMS also charged number clerical tasks, including keeping symbol
table declarations, maintaining accessing efficiently precomputed IsA hierarchy, signaling definitions/declarations redundant (i.e., concept equivalent
previously defined one) incoherent.
DL-KBMS perform inferences individuals, well concepts. operations involved described Section 5.
2.4 Reasoning Strategies
two general approaches answering fundamental subsumption question
underlying DL-KBMS operations.
One approach based theorem proving techniques specially adapted descriptions,
particularly variants tableau techniques determine subsumption = B
checking unsatisfiability AB; systems kris (Baader et al., 1994),crack
(Bresciani, Franconi, & Tessaris, 1995), FaCT (Horrocks, 1998) DLP (Patel-Schneider,
1998) follow approach. systems advantage provably complete.
Although worst-case complexity bounds subsumption problems sometimes
quite high (EXPTIME complete), recent empirical evidence shows performance
computing subsumption large realistic KBs quite effective (Horrocks & PatelSchneider, 1998).
implemented DL KBMS systems wide distribution use,
back (von Luck et al., 1987), classic (Borgida et al., 1989), loom (MacGregor, 1986), follow so-called normalize-compare paradigm, reasoning
work performed initial normalization phase, whose goal find normal
form concepts explicates implicit facts, eliminates redundancies detects incoherencies. done, when, example, comes comparing two concepts
subsumption, often (but always) matter comparing structurally similar elements ones built constructor. example, since descriptions
405

fiBorgida

all(pet,nothing) at-most(0,pet) equivalent (subsume other), normal form would appear, subsumption comparison all(pet,DOG) would
require looking all(pet,nothing), comparison at-most(3,pet) would
locate at-most(0,pet).
present work carried period several years within normalizecompare paradigm. original reasons choice included following features,
available paradigm: sizable user base; supporting additional KBMS operations, least common subsumer, pattern matching, explanation;
reasoning large knowledge bases individuals, especially incrementally
environment one needs logical completion individual.
remark tableau technology recently become competitive sizable
knowledge bases concepts. promise provably complete effective reasoners
expressive languages enticing, therefore study extensibility tableaux
approaches (Baader & Hanschke, 1991; Bresciani et al., 1995) great interest.

3. Concept Reasoning protodl
approach extensible KBMSs based idea case current
KBMS meet users representation reasoning needs, one extend
adding one new description constructors would considered logical
connectives.
Although approach clearly restrictive, advantage specific
directed; allows us develop implementation architecture accompanying
methodology support process extension. approach also supported empirical evidence use classic system many applications: test-defined
concepts classic crucial escaping expressive limitations basic
language, test function acts essentially new concept constructor2.
3.1 Modularized Implementation Architecture
previously described, concept reasoning services DL-KBMS delivered
operations rely = relationship, computed function Subsumes?.
implementation paradigm, function takes arguments two normalized concepts.
Hence need way take input convert normalized concepts. means
parsing concept (function Parse) normalizing (function Normalize).
uniform prefix nature DL syntax, parsing easily performed recursive descent single token look-ahead; associate every concept constructor
Q, function Q::Parse.
examining earlier implementations applications, key decision underlying
protodl implementation normalized concept viewed conjunction
collection component concepts, either concept identifiers normalized
terms built concept constructors Q and. Therefore, possible view
normalized concept data abstraction encapsulates way various
2. Note however reasoning test-concepts classic minimal: subsumption reasoning
identity checking performed, individual reasoning limited recognition.

406

fiExtensible DL Representation Reasoning

components stored accessed thru set functions put Q(value) get Q()
one concept constructor Q. example, put one-of operation stores set
individuals S, get one-of returns set3. hand, constructor
fills, (normalized) value stored set terms type <Role SET(Individual)>,
since role, need store set current fillers. constructors
fills, associated single role, overload put get functions
accept separate role argument, allowing us access value independently (e.g.,
get fills(p:<Role>) returns <SET(Individual)>). fact, constructors
efficient group role identifier, simplify exposition shall
pursue distinction rest section.
mentioned earlier, ideal normalized concept would property subsumption could determined comparing, using Q::Subsumes?, components built
constructor. words, Subsumes? would pseudo-code
Subsumes?(hiNC, lowNC: <NormalizedConcept>) returns <Bool>
;; structural subsumption
{For every Q(T) getComponents(hiNC)
{ lowComponent := get Q(lowNC);
Q::Subsumes?(T,lowComponent)
return false; }
return true; }
shall see Section 3.3, restrictive however, pass parameter
entire normalized lower concept, lowNC, rather lowComponent. Therefore,
every constructor Q expect function
Q::Subsumes?(<NormalizedQ-term>,<NormalizedConcept>) .

order normalize concept, normalize component separately,
combine them. Since normalized concept represents conjunction, combinator function named Conjoin. remaining functions provided ahead
time core implementation:
Normalize(pC : <ParsedConcept> )

returns <NormalizedConcept>
throws IncoherentExn;
/* Start ND data structure constraints i.e.like thing */
ND := copy NormalFormthing;
{ /* Normalize component conjoin onto ND */
every Q(T) pC {
norm := Q::Normalize(T);
Q::Conjoin(T norm,ND); }
/* Replace concept ids normalized form (saved declare ) */
every concept identifier N pC {
N norm := look normalized form N;
Conjoin(N norm,ND); }

3. type parameter indicated enclosing angle-brackets. Thus, would referred
put one-of(s: <SET(Individual)>).

407

fiBorgida

/* Normalize Conjoin detects incoherence, propagate up,
since whole conjunct incoherent. */
}
catch IncoherentExn { throw IncoherentExn }
return ND;

Conjoin(fromNC, ontoNC: <NormalizedConcept>) throws IncoherentExn
/* Take component fromNC conjoin ontoNC;
modifies ontoNC. */
{For every Q(T) getComponents(fromNC)
{Q::Conjoin( T, ontoNC)
} catch IncoherentExn {throw IncoherentExn }
}
Therefore, every constructor Q, also expect corresponding functions Q::Normalize
Q::Conjoin . Note software architecture treats incoherent concepts (ones
equivalent nothing) special manner: whenever encountered, special exception IncoherentExn raised. Conjunction propagates exceptions, constructors may trap handle way. example, all::Normalize
accepts restriction incoherent, must ensure at-most(0) also added
normalized concept role.
retrospect, three functions (Subsumes?, Normalize Conjoin) really
represent semantics constructor (and hence appropriately called
and::Subsumes?, etc.) well expansion necessary conditions/definitions
named concepts (inheritance), parts meaning built-in concepts nothing thing. make complete, add beginning Subsumes?
function statement: equal(hiNC,NormalFormthing) equal(lowNC,NormalForm nothing) return true;
sense, protodl starts minimal, core language specified syntax
C ::= thing | nothing | CN | (and C1 , . . . , Cn )
say everyone must start minimalist language, hardly
useful. advantage constructors (even useful ones,
like all, at-least, etc.) implemented extensions, according uniform
paradigm esoteric extensions. essential able
easily modify implementation standard constructors (like all) called upon
implement non-standard ones (like dates), may interact them.
3.2 Overview Process Extension.
Suppose want extend system particular stage new concept constructor. following suggested methodology accomplishing this, illustrated
familiar concept constructor, all.
408

fiExtensible DL Representation Reasoning

(1) Determine syntax extension. concepts LISP-like syntax (as
classic loom), constructor might given syntax (:all Role Concept).
terms following constructor called arguments, version
eventually stored internal representation normalized concepts. One
implement function all::Parse, case would invoke Role::parse
Concept::parse.
(2) Determine semantics new concept constructor.
First, requires settling domain values denotation come.
might set ordinary objects unique intrinsic identity instances special
class any-object, built-in direct subclass thing. denotation might also
new kind value (e.g., triples dates, lists objects), case
necessary introduce (possibly new concept constructor, takes argument)
top class kinds values (e.g., any-date dates).
time clarify intended meaning new constructors. One alternative,
explored here, express semantics using First Order Predicate Calculus. Another
assign denotation values concepts built new constructor. all,
usual interpretation
all(p, C)I = { | pI (d) C }
Rather move implementation right away, experience indicates
easier first describe deductions performed concise manner:
rules inference. Based empirical evidence, see rules inference
naturally grouped several categories4
Rules dealing one constructor, Q.
Normalizing arguments Q, composite objects.
C
all(p, C) all(p, D)

Reasoning concepts form Q(args).
concept incoherent itself?
all(p, C) never incoherent.
concept equivalent entire domain values?
all(p, thing) any-object
Q(arg1) subsume Q(arg2) terms arg1 arg2? (The socalled structural subsumption relationship.)
C =
all(p, C) = all(p, D)
4. case, give general description desired new constructor Q, illustrate
specific constructor all.

409

fiBorgida

Q(arg) entail description built constructor?
all(p, nothing) at-most(0, p)
Reasoning conjunctions descriptions built Q only. usually
results normal-form either combines arguments single one,
keeps arguments list, set multi-set.
and(all(p, C), all(p, D)) all(p, and(C, D))
Rules inference dealing combinations several concept constructors.
rules grouped similar families above, except involve two
kinds constructors.
Since rule involving all, offer alternative example:
and(at-most(n, p), at-least(m, p)) nothing n<m
augmented set inference rules proven sound complete respect
semantics specified earlier. logic, soundness relatively easy show,
completeness much harder. Royer Quantz propose one approach (Royer & Quantz,
1992) generating inference rules complete, based translation First Order
Logic, technique easy apply.
(3) Extend implementation. requires first determining appropriate normal
form argument constructor Q, declaring data structure hold information,
completing put Q get Q procedures access normalized concept.
simplicity, address detailed data structure issues here; instead, use
simple notation based terms style Prolog represent data structures.
case all, representation pair all(<Role>,<NormalizedConcept>).
note wanted keep track original form concept (so printed
users entered), could added extra field data structure.
emphasize normalized form description (e.g., finite automaton) might
even resemble original syntax (e.g., regular expression), though examples
case.
Next, need implement three procedures: Q::Normalize,Q::Conjoin,
Q::Subsumes?, whose invocation orchestrated corresponding functions
constructor.
analyzing normalization algorithms numerous constructors, arrived
refined understanding prototypical Q::Conjoin procedure. presented
Appendix Figure 3, uses following other, smaller functions
Q::Universal? , Q::Incoherent? , Q::SubsumesSame? , Q::ConjoinToSame ,
Q::IncoherentWDifferent? , Q::SubsumesDifferent? , Q::ConjoinToDifferent ,
Q::FindOtherImplications .
functions correspond categories inference rules mentioned above.
decomposition applied, provides two advantages: improved chances
correct implementation making smaller, specialized modules; independent reuse,

410

fiExtensible DL Representation Reasoning

Q::Subsumes?(T:<NormalizedQ-Term>, lower:<NormalizedConcept>)

returns <Bool>
{if Q::SubsumesSame?(T,get Q(lower)) return false;
Q::SubsumesDifferent?(T,lower) return false;
return true; }
course, specific constructor need implement subprocedures, corresponding lines generic implementations omitted, order
avoid cost useless function invocations.
simple example, Q=all, all::Normalize(all(role,vr)) returns
all(role, and::Normalize(vr)).
constructor at-most, following functions used all:Conjoin
all::Subsumes? needed:
Q::Universal?(T): equivalent top hierarchy set values?
all::Universal?(all(role,vr)) returns true vr NormalFormthing.
Q::SubsumesSame?(T,OldTs): implied Q-constructed terms already seen
oldTs? basically structural subsumption algorithm.
all::SubsumesSame?(all(role,vr1),all(role,vr2)) would return true iff
and::Subsumes?(vr1,vr2) returns true.
Q::ConjoinToSame(T,OldTs): Merge preceding terms built Q.
all::ConjoinToSame(all(role,vr1),all(role,vr2)) returns
all(role,and::Conjoin(vr1,vr2)).
Q::FindOtherImplications(T,This,ImplicationsToDoList): additional
constructors added Q, put onto ImplicationsToDoList.
all::FindOtherImplications adds ImplicationsToDoList description
at-most(0,role) first argument all(role,nothing).
Conjoin, Normalize Subsumes? functions implemented new
constructor Q, corresponding functions need lines added invoke
according pattern presented preceding section (unless programming
language highly polymorphic).
Next, use reasoning dates, extension desired classic applications,
give complex example methodology building extensions protodl.
3.3 Dates: Example Concept-Level Extension
imagine application individual dates values attributes
even roles, concepts describe collections dates, specified various ways
(e.g., ranges periods).
begin with, important clarify individuals look like
information kept. case dates, know wish treat
temporal points, associated information year, month day
date occurs. Given date d, components referred year(d),
411

fiBorgida

month(d), day(d). two different ways thinking date: abstract
mathematical value (e.g., triple integers) individual object attributes
year, month, day. last approach disadvantage need develop
separate theory identity dates (two dates supposed identical
components), although advantage allowing incomplete information
exact occurrence time point. Since feature desired, adopt
first approach, convenience writing date individuals 1996/7/25. addition,
usual total (reflexive) order dates, well two special date constants,
BeginTime EndTime, BeginTime EndTime dates d.
ready introduce concept-forming operators dates useful
application. First, introducing new kind value, useful define top
concept, contain values. case, let us call any-date.
One obvious grouping dates ranges: June 1st August 31, 1996.
might thus propose concept constructor dateRange, takes argument pair
dates, denoting ends range, e.g., dateRange(1996/6/1,1996/8/31). since
base language support disjunction, want allow descriptions
summers 1995 1996, fact dateRange take argument set
date pairs, dateRange({(1995/6/1,1995/8/31) , (1996/6/1,1996/8/31)}).
established syntax concept constructors, implemented dateRange::Parse, present denotational semantics:
dateRange(SD)I = { | b, e.(b, e) SD b e }
Next, look inference rules describing desired reasoning dateRange. Following heuristics Section 3.2, come Table 3.
implement inferences dateRange, must find normal form write functions dateRange::Normalize, dateRange::Conjoin dateRange::Subsumes?,
components.
useful determine first structural subsumption function, dateRange::SubsumesSame?, since drives requirements others. case, obvious
representation works fine: date data structure, list three values
record three fields; single date-pair list two values record two
fields; set date-pairs making disjunction list date-pairs.
best encapsulate implementation choices using abstract data types Date,
DatePair DateRange, appropriate accessor functions constructors. must
also extend representation normalized concepts implement put dateRange
get dateRange.
Returning dateRange::SubsumesSame?, function simply implements
subsumption inference rules
dateRange::SubsumesSame?(high,low : <SET(DateDate)> ): returns <Bool>
{for every (b,e) low
find (b,e) high b b e e;
}

412

fiExtensible DL Representation Reasoning

Universal

dateRange({(BeginTime,EndTime)}) any-date

Incoherent

dateRange({}) nothing

Subsumption

b1 b2 e2 e1
dateRange({(b2,e2)}) = dateRange({(b1,e1)})
dateRange(1 ) = dateRange(2)
dateRange(1) = dateRange(2 )
dateRange(1 1) = dateRange(2 2)

Conjunction

and(dateRange({(b2,e2)}), dateRange({(b1,e1)}))
dateRange({ (max(b1,b2),min(e1,e2))})
and(dateRange(),dateRange(1)) dateRange(1)
and(dateRange(),dateRange(2)) dateRange(2)
and( dateRange(), dateRange(1 2)) dateRange(1 2)

Normalize

e (b + 1 day)
dateRange({(b,e),}) dateRange({})
b1 b2 (e1 + 1 day) e2
dateRange({(b1,e1),(b2,e2)}) dateRange({(b1,e2)})

Table 3: Inference rules dateRange.

dateRange::Normalize verifies (if already done so) dates b e
pair (b,e) valid according usual calendar, b e. pairs
satisfying conditions eliminated. also needs merge overlapping adjacent
intervals maximally long ones, since otherwise subsumption algorithm
recognize dateRange({(1996/1/2,1996/1/4), (1996/1/5,1996/1/6)}) subsumes
dateRange({1996/1/2, 1996/1/6}).
function dateRange::Conjoin standard implementation (see Appendix
A), functions dateRange::Universal?, dateRange::Incoherent?, dateRange::ConjoinToSame dateRange::SubsumesSame? need implemented, performing
exactly actions specified rules inference.
Consider adding another concept constructor dates, help represent periodic
time, every summer every Christmas. single period range constraint possible values components date, year. So, summer
days would represented [(6 . 8) (1 . 31)], Christmas would [(12 . 12)
(25 . 25)]. sake brevity, new period constructor take argument
single period (rather set them, interpreted disjunctively)
sketch implementation extensions. Also presented here, useful extension
413

fiBorgida

Universal

period([(1 . 12) (1 . 31)]) any-date

Incoherent

not( bm em bd ed )
period([(bm.em) (bd.ed)]) nothing

Subsumption

bm1 bm2 em2 em1 bd1 bd2 ed2 ed1
period([(bm2.em2)(bd2.ed2)]) = period([(bm1.em1)(bd1.ed1)])

Conjunction

and(period([(bm1.em1)(bd1.ed1)]), period([(bm2.em2)(bd2.ed2)]))

period([(max(bm1,bm2) . min(em1,em2))
(max(bd1,bd2) . min(ed1,ed2))])

Table 4: Inference rules period.
periods allowing constraints days week, every Saturday Sunday,
even every 3rd Sunday.
denotation period terms also quite simple:
period([(m1.m2)(d1.d2)])I = {e | m1 month(e) m2, d1 day(e) d2}
rules inference reasoning period concepts alone, presented Table 4
also straightforward. implementation functions follows immediately
rules inference.
interesting reasoning involves conjunction ranges periods. rules,
appearing Figure 5, expressed succinctly showing certain periods
either eliminated retained unchanged, relying dateRange normalization rule, applied reverse, cut range appropriate strips. inference
rules imply need also implement period::ConjoinToDifferent (p:<Period>,
other:<NormalizedConcept>), basically uses period cookie cutter
create sub-interval ranges satisfy periods restriction. course,
done procedurally manner efficient suggested rules inference.
Moreover, must revisit implementation dateRange, put similar dateRange::ConjoinToDifferent function, since concept range may conjoined
onto one already period it.
Finally, presence dateRange period constructors, structural subsumption enough, since want period([(4 . 4) (1 . 31)]) subsume dateRange((1988/4/1, 1988/4/21)), dateRange((1990/4/1, 1992/4/1)),
finite normal form would list ranges satisfying period. need
also write function period::Subsumes Different?(p:<Period>, lower:<NormalizedConcept>), checks every interval (b,e) get dateRange(lower) make
sure year(b)=year(e) month day meet conditions period.
interesting complication arises dates discrete, hence one count
number dates dateRange. dateRange appear value restriction
414

fiExtensible DL Representation Reasoning

and(dateRange( ), period())

and(and(dateRange(), period()), and(dateRange(), period()))
bm month(dt1) = month(dt2) em
bd day(dt1) day(dt2) ed
and( dateRange({ (dt1,dt2) }) , period([(bm.em),(bd.ed)]) )

dateRange({ (dt1,dt2) })
month(dt1)=month(dt2)
(bm month(dt1) em bd day(dt1) day(dt2) ed )
and( dateRange({(dt1,dt2)}) , period([(bm.em),(bd.ed)]) ) nothing

Table 5: Inference rules conjoining period dateRange.
general roles, all(freeForMeeting, dateRange({(1995/6/1,1996/6/5)})) attributes, problem. Otherwise, however infer cardinality constraints
number fillers: dateRange({(1995/6/1,1996/6/5)}) allows 5 different values. language supports constructor at-most, would therefore conclude
at-most(5,freeForMeeting). implementation achieves introducing helper
function dateRange::countDays, applied normalized dateRange object; then, function all::FindOtherImplications needs modified, invokes
dateRange::countDays value restriction date type, normalized
at-most restriction posted role ImplicationsToDoList. Later processing list remove at-most constraint conjoin onto concept.
3.4 Experience Extensions
part development architecture, considered extending
original core constructors classic (all, at-least, at-most, fills, one-of, integer
ranges), well primitive concept negation, negation fills. cases
reproduced inferences classic almost exactly internal actions
classic implementation; i.e., perform checks despite fact
implementation made standard modules constructor.
Two kinds concept constructors seem difficult add normalize-compare algorithm way preserves completeness reasoning architecture system.
Disjunction would naturally handled disjunctive normal form,
disjunct purely conjunctive. difficult achieve nested disjunctions
(inside restrictions say). Note problem one-level disjunction
dateRange.
second kind construct difficult add efficiently completely same-as.
reason same-as interacts way generates potentially infinite number restrictions; therefore, implementation same-as best combined
415

fiBorgida

all, resulting non-tree data structure (Borgida & Patel-Schneider, 1994).
speculative way preserve structural subsumption might allow apply
chains attributes represented regular expressions. (In general, complications
implementing same-as reason appear C-classic neo-classic.)
Finally, mentioned earlier, currently cover role constructors recursive
concept constraints. Furthermore, constructors require entirely new deductive mechanism (e.g., epistemic reasoning, defaults, forward-chaining rules) also difficulty
integrated properly framework.
positive side, considered extensions supporting strings (Borgida, Isbell,
& McGuinness, 1996), elaborately, reconstruction clasp reasoner
actions plans (Borgida, 1992b). summarize success briefly, clasp (Devanbu &
Litman, 1996) system built top classic reasoning actions (which
represented propositional STRIPS-style, concepts pre- post-conditions,
well add delete lists); plans represented regular expressions actions,
illustrated below. goal apply protodl approach clasp, hoping
able reproduce original, custom-made implementation discussed Devanbu
Litman (Devanbu & Litman, 1996). First, introduced concept constructor actions,
act(P reC, P ostC, AddC, DeleteC), expected logical properties expressed rules
inference various categories (e.g., P reC incoherent, action also
incoherent). implementation followed immediately.
interesting problem dealing plans. constructors single, seq,
loop, altern used build complex plans actions, seq( single(DIAL),
loop(single(RING))). plans denote sequences action instances (e.g.,
example, dialing action followed number rings). Although provided rules
inference plans too, turns normal form regular expressions! Instead, implementation, Normalize seq, altern loop built nondeterministic finite automaton, made deterministic,
certain chains arcs removed (if post-condition action incoming edge inconsistent pre-condition action outgoing edge).
Moreover, containment algorithm finite automata take account
fact actions transitions (e.g., MOVE) may represent generalizations others (e.g., MOVE-FAST). implementation achieved protodl introducing
hidden concept constructor, top-plan-exp, enclosed top plan.
top-plan-exp::Normalize made automaton deterministic removed
transitions, top-plan-exp::Subsumes? implemented special subsumption algorithm. Moreover, requirement implement top-plan-exp::Conjoin,
present original clasp system, made us realize without this, clasp plans could
appear concepts, expressions like and(all(p,PLAN1), all(p,PLAN2))
could normalized. result believe reconstructed improved original
clasp proposal, characterizing inferences performed rules inference,
allowing plans first-class values, could appear ordinary roles.
416

fiExtensible DL Representation Reasoning

3.5 Relationship Concrete Domain Extensions
Although shall address general work extensible KR&R conclusion,
one specific approach involving description logics deserves closer scrutiny stage,
details present work still fresh.
Baader Hanschkes proposal (Baader & Hanschke, 1991) extending DLs
concrete domains allows concepts consist arbitrary predicates involving values
domain ordinary objects (i.e., elements ), long
values fillers attributes ordinary objects. example, suppose concrete
domain dates; above, predicates like BEF ORE, corresponding
; then, two date-valued attributes arrival departure, could define
concept BEFORE(arrival,departure), denoting ordinary objects (not sets dates!)
whose attributes appropriately related date values.
order keep reasoning decidable, mechanism requires concrete domain
admissible: (i) must predicate denoting universe values
concrete domain; (ii) set predicates must closed negation; (iii) must
possible decide satisfiability finite conjunction predicates.
interesting note requirements match part heuristics new concept
constructors: also argued need add top concept hierarchy new values
(and negation, bottom hierarchy), need able compute
conjunction descriptions.
admissibility concrete domain ensures protodl approach implement extension follows: Syntactically, domain corresponds concept
constructor. Therefore, concept like BEFORE(arrival,departure) domain DATE,
would represented protodl DATE(BEFORE,arrival,departure). Then,
necessary protodl functions programmed follows: DATE::Normalize tests
predication satisfiable (otherwise signaling IncoherentExn), creates singleton
list containing predication; DATE::Conjoin concatenates list predications
arguments, checking consistency conjunction; DATE::Subsumes?(C,D)
creates conjunction predicates C, negation D, returns true
result unsatisfiable.
Conversely, Baader Hanschkes approach could well applied date ranges, since
essentially closed negation. since offered context
tableau theorem-proving technique mentioned Section 2.4, continues
advantages elegance complete reasoning, long concrete domain reasoner
proven complete. case, entire system would proven complete.
believe though protodl somewhat general, since allows concrete
domains value roles, attributes (see earlier discussion dates values
roles). Also, advantage able deal non-admissible domains
cases negation absolutely needed, addition would cause increase
computational complexity; example, adding negation/complement regular expressions
makes containment problem non-elementary (Stockmeyer, 1974).
417

fiBorgida

4. Processing Individuals DLs: Introduction
Concept descriptions, introduced above, intensional objects, suited capturing
generic information domain, ontology terms. DL-KBMSs must also
manage extensional/factual information individual objects so-called A-box5 .
4.1 Inferences Involving Individuals
Normally, one assert information fact object Anni known
instance concept CHILD (written Anni:CHILD),
object, Lego filler hasToys role (written (Anni,Lego):hasToys). Based
information, KBMS deduce information individuals membership descriptions (written using ); example, case know Anni
at-least(1,hasToys). DL-KBMS usually make closed-world assumption, also necessary record set fillers complete individuals
role. done using (auto-epistemic) assertion like
Anni:allFillersKnown(hasToys,{Lego,Barbie}). result assertion,
deduce Anni at-most(2,hasToys). (Note distinguished
three kinds assertions A, namely b : C, (b, e) : p, b : allFillersKnown(p, S),
corresponding judgements deduced logic: b C, b fills(p,e),
b closedFillers(p,S).)
Information fillers roles closed (i.e., fillers known) also
deduced, case KB contains A={Lori:all(hasToys,one-of(Lego45)),
Lori:at-least(1, hasToys)}, conclude Lego45 toy owned
Lori (written Lori fills(hasToys,Lego45)) complete set fillers
hasToys { Lego45 } (written Lori closedFillers(hasToys,{ Lego45 })).
note elegant formalization notions obtained adding epistemic
modal constructor K DLs (Donini et al., 1998). constructor number
uses, shorten presentation introduced explicitly.
Formally, define knowledge base KB concept knowledge base CKB, extended
set assertions form b : C, (b, e) : p b : allFillersKnown(p, S),
set individuals. interpretation said model b : C
bI C , model (b, e) : p eI pI (bI ), model b : allFillersKnown(p, S)
(eI pI (bI ) e S)6.
judgment KB |= b C holds iff every model KB, bI C ; judgement KB |= b fills(p, e) holds iff every model KB, (bI , eI ) pI ; finally,
KB |= b closedFillers(p, S) holds iff KB |= b fills(p, bi) every bi S,
every individual e S, interpretation KB
6 KB |= b fills(p, e) . concept language may negation,
open world assumption, also need able talk non-membership
concept: b 6 C; naturally, KB |= b 6 C iff model KB, bI 6 C .
usual, KB called inconsistent iff models.
5. thorough theoretical investigation individual reasoning presented Andrea
Schaerfs PhD thesis derived publications (Schaerf, 1994). note however
expressive DLs proposed recently, individual reasoning yet addressed.
6. usual, make unique-name assumption, fact include named individuals domain

418

fiExtensible DL Representation Reasoning

specification reasoning individuals represented inference
rules (Borgida, 1992a). example, constructor all, proffer following three
rules inference, describe structural membership/non-membership rules,
well kind inference called propagation, information individual
b results new information deduced another individual e:
KB ` b closedFillers(p,S)
KB ` bi C
KB ` b all(p,C)

= {b1, , bn}, n 0

KB ` b fills(p,e)
KB ` e 6 C
KB ` b 6 all(p,C)
KB ` b all(p,C)
KB ` b fills(p,e)
KB ` e C

Note inconsistent KB, KB ` b nothing, deduce
information fillers (e.g., KB ` b at-least(3,pets)) contradicts information
asserted deduced descriptors (e.g., KB ` b at-most(2,pets)).
4.2 DL-KBMS Operations Individuals
point living open-world assumption allow information accumulated incrementally, case designing artifact (one successful
applications classic).
functional point view, DL-KBMS therefore supports following update operations incrementally adding information individuals:
Operation
assert-member(b, C)
assert-fills(b, p, b1)
assert-closed(b, p)

Effect
b : C added
(b, b1) : p added
b:allFillersKnown(p,S) added A,
set individuals returned current KB
operation ask-for-fillers(b, p), defined below.

result update, KB inconsistent, update rejected
state KB supposed remain unchanged.
point, KBMS able respond inquiry operations relationships
involving individuals:
Question
ask-member?(b, C)
ask-non-member?(b, C)
ask-for-fillers(b, p)

Answer type
Boolean
Boolean
SET(Individual)

ask-closed?(b, p)

Boolean

Response
true iff KB |= b C
true iff KB |= b 6 C
{ e | KB |= b fills(p, e) }
true iff set
KB |= b closedFillers(p, S)

concepts, many DL-KBMS pre-compute b C judgment, individual
concept names, finding specific named descriptions individual
b provably belongs. Similarly, DL-KBMS pre-computes caches fillers closed
419

fiBorgida

information individuals roles. done order detect inconsistencies
time update, decrease amortized cost case queries much frequent
updates, support queries form additional information
deduce v?. utility queries shown, among others, applications
involving information discovery software development (Devanbu, 1994).

5. Individuals protodl
Reasoning individuals goal implementation support efficiently
update operations, incremental, inferences precomputed:
individual, know least classes instance of, fillers
role, whether role closed individual. architecture
describes extension rationalization individual processing usually
carried normalize-compare DL-KBMS classic. novelty
systematic separation interaction various kinds updates various kinds
inferences, concomitant use truth-maintenance links.
5.1 Basic Architecture Individual Reasoning
systems, one try reduce individual reasoning concept reasoning associating single, maximally complete description individual. However,
always possible, shown Schaerf (Schaerf, 1994).
Instead, data structure every individual includes concept description (to
called Descriptor), may existing class unnamed complex description, individual role-filler information recording specific values assigned far,
whether role closed not. (This information accessed built-in functions
add filler, put closed, get fillers closed?.)
processing update individual b, must therefore resort two
kinds reasoning: (i) subsumption/incoherence reasoning involving description
Descriptor(b) concepts; (ii) reasoning specific individual especially
role fillers.
former kind reasoning motivated general inference rules connecting membership subsumption (e.g., b C C = b D). Since subsumption
extensibility described preceding section, henceforth concentrate
second aspect individual reasoning.
understand tasks involved, let us illustrate kinds reasoning need
performed whenever fact asserted (or inferred) individual b
KB may become inconsistent, conflict individual descriptor filler information either b individual. example,
fillers might added role permitted at-most restriction.
requires entire update rejected. Note update may result several
individuals inconsistent information them. system required
detect one them, rejecting update.
420

fiExtensible DL Representation Reasoning

individual b may end re-classified. monotonic system,
present one, means individual may belong specialized
class(es) hierarchy.
New information roles individual b inferred. example, learning
role closed allows us count fillers hence obtain exact upper
bound recordable at-most.
New assertions inferred individuals, usually ones related b via
roles. example, all(friends,HAPPY) description applies individual
x, asserted friends-filler x, infer
instance HAPPY.
support first task above, protodl might use function ConsistentW?7,
returns true indicate information added individual consistent.
second task, use function Recognizes?. third fourth tasks,
involve individuals one update occured, use function
InferFrom.
functions used implement various assert KBMS operations, well
ClassifyIndividual function, used protodl find lowest classes
hierarchy individual belongs.
case concept reasoning, use constructor drive processing,
modularize implementation kind constructor Q set functions: Q::Recognizes? , Q::ConsistentW? , Q::InferFrom . becomes
important distinguish constructors, like at-most, associated single
role, contrast generic constructors one-of (which roles associated)
same-as (which many roles, none special). former, called branch
constructors, functions arguments include individual
normalized representation Q-terms, also fillers closed information
role. (This done prevent repeated retrieval values
constructor.) Therefore, general form and::Recognizes?, provided protodl,
and::Recognizes?(b:<Individual>,NC:<NormalizedConcept>)returns <Bool>
{ every Q(T) getNonbranchComponents(NC)
Q::Recognizes?(b,Q(T)) return false;
every role p getRestrictedRoles(NC)
{F:= get fillers(b,p);
clsd? := closed?(b,p);
every Q(T) getBranchComponents(NC,p)
{if Q::Recognizes?(b,Q(T),F,clsd?)
return false};};
return true; }
7. Later, introduce incremental reasoning, identify number variants function.

421

fiBorgida

functions and::ConsistentW? and::InferFrom similar structure,
presented due space limitations. important note ConsistentW?
functions return false indicate unable prove conclusively
individual consistent inconsistent; inconsistency found, InconsistentExcn
raised. Also, update may several individuals incompatible
information asserted. system guarantees find one case,
rejects update.
following example specialized function case Q=all, implements first inference rule presented earlier
all::Recognizes?(b:<Individual>, all(r:<Role>,vr:<NormalizedConcept>),
fillers:<SET(Individual)>, clsd?:<Bool>) returns <Bool>
{if clsd? return false
/*More fillers might come later*/
else every f fillers
{if and::Recognizes?(f,vr)
return false;}
return true }

5.2 Reasoning Incremental Updates
Since updates individuals incremental nature, order improve efficiency
want take account fact KBMS already performed inferences
to, including, current update. example, individual Tintin
asserted instance all(pet,DOG), already pet-fillers d1 d2,
current update assert-fills(Tintin,pet,Fido), need propagate
information pet-fillers DOGs Fido, since others would processed
earlier. And, current update assert-closed(Tintin,pet), new role-filler
information inferred restriction, even
bother calling all::InferFrom. therefore distinguish following three variants
Q::InferFrom:
Q::InferFromFilling(<Individual>,<NormalizedQ-term>,<Role>,<Individual>)
Q::InferFromClosing(<Individual>,<NormalizedQ-term>,<Role>) ,
Q::InferFromAsserting(<Individual>,<NormalizedQ-Term>)

plus similar variants Q::ConsistentW?. example
all:ConsistentWFilling?(ind, all(r,vr), r, newfiller) invokes and::ConsistentWAsserting?(newfiller,vr) check new filler contradict
role restriction vr;
all::ConsistentWClosing?(ind,all(r,vr),r) returns true, hence hopefully eliminated compiler;
422

fiExtensible DL Representation Reasoning

all::ConsistentWAsserting?(ind,all(r,vr)) verifies r-fillers ind
consistent vr invoking and::ConsistentWAsserting?(y,vr).
One might also consider variants Q::Recognizes?, but, shall see, case
limited use since propose keep information parts
recognition test succeeded already, therefore need start beginning.
5.3 Dependency Links
Clearly, results function calls, Recognizes?, individual Bob might
change update Bob itself. However, membership Bob class Bobs
consistency may depend facts asserted individuals database.
example, Bobs friends known (i.e., role friends closed),
one MARRIED, holdout, Larry, also asserted inferred
MARRIED, Bob reclassified instance all(friends,MARRIED).
means without special data structure, every update individual unsophisticated implementation reconsider every individual
KB.
One alternative, used loom (MacGregor, 1986), keep track questions asked
individual part previous processing (hits misses), answers
change result update re-processing needed. Another
alternative use elaborate truth-maintenance system (as available kl-two (Vilain,
1985)), kind judgment. opinion, approaches might however
become expensive terms space often computation time, maintain
many details.
follow intermediate stance, first suggested Peter Patel-Schneider classic implementation, intended reduce needless re-testing incurring relatively less dependency maintenance overhead. Essentially, coarse-grained
dependency structure, one individual e may point another, b, result
decision latter may change result (unspecified) additional information added former. example, presence definition
.
CanineLover=all(pet,DOG), Tintin Fido pet filler, neither Recognizes?(Fido,DOG) ConsistentW?(Fido,DOG) return true, add link
Tintin. Thereafter, change status Fido
form: Fido RecognizeDependsOnMe
cause Tintins classification (with respect CanineLover, well pending classes)


re-done. Similarly,
ConsistentDependsOnMe InferDependsOnMe links
objects.
Note following dependency links back, know properties
fillers role might changed, could variants
and::RecheckConsistentW? and::RedoInferFrom, perform less work omitting checks involving constructors known affected aspects
(e.g., at-most cares count fillers, properties).
order set dependency links, every function Q::Recognizes? needs
keep track list individuals whose modification might change result
function. values must eventually appropriately linked dependencies either
functions calling environment.
423

fiBorgida

5.4 Coordinating Components
clear examples, task top-level KBMS update operations (asserts) include invoking appropriate functions individual
updated, also setting appropriate dependency links, gathering queues
objects whose dependency links tickled latest update, repeating operations individuals brought focus inferences dependency
links.
appropriate software architecture representing processing seems
blackboard model, 5 lists functions post tasks carried
out:
Two lists, ToRecheckConsistencyList, ToRedoInferFromList, objects
reached individual x via corresponding kind dependency link result
change x.
list, ToPerformUpdateList, collects additional inferences made
found InferFrom RedoInferFrom function calls. simplicity,
recorded additional calls versions three kinds assert functions.
list, ToReclassifyList, receives objects need checked case
classified hierarchy; objects come
dependency links also additional asserts triggered.
Finally, list ToAddDependencyList, keeps track dependency links need
added knowledge base; list augmented various functions,
mentioned previous section.
Because, example, re-classification may cause new inferences, may cause reclassification due constructor, linear order lists
processed. may therefore loop demon removes arbitrary
object list, invokes required function, repeat process.
requirement dependencies ToAddDependencyList also considered
installed, whenever chasing objects may affected update. Note
that, surprisingly, hard make general statements termination
algorithm skeleton, since various functions, especially InferFrom. free
whatever want, including increasingly longer descriptions..
Several policies help performance system:
cases, helpful eliminate duplicates lists.
Locality context may improve performance; therefore grouping together operations
performed one object advisable (e.g., gather together dependencies
object ToAddDependencyList).
expect inconsistencies arise infrequently, ToRecheckConsistencyList processed end.
424

fiExtensible DL Representation Reasoning

assert-fills(ind, role, filler)
{if redundant information signal RedundantExn;
add filler(ind,role,filler); /* may rolled back error */
initialize blackboard lists empty;
ind-descr := Descriptor(ind);
/* deal consistency issues involving ind */
and::ConsistentWFilling?(ind, ind-descr, role, filler);
/* preceding function post dependencies
answer definite TRUE */
ToRecheckConsistencyList += getConsistentDependsOnMe(ind);
/* puts reconsideration objects
dependent ind */
/* check inferences */
and::InferFromFilling(ind,ind-descr,role,filler);
/* may post new updates and/or dependencies */
ToRedoInferFromList += getInferDependsOnMe(ind)
/* reclassify least individual */
Reclassify(ind);
ToReclassifyList += getRecognizeDependsOnMe(ind);
/* Process tasks left blackboard */
ProcessBlackBoard;
}
catch InconsistentExcn
{roll back update; print error msg; }

Figure 2: protodl pseudo-code assert-fills.
Figure 2 contains pseudo-code assert-fills, offered protodl. Notice
advantage program using exception handling propagation
dynamic function call hierarchy way dealing inconsistency: wherever
detected ConsistentW?, exception passes levels function calls,
interrupted, handler encountered usually external internal
assert operation; issues appropriate error messages, resets local changes made,
re-raises/propagates exception up, top level reached.
5.5 Extending Individual Reasoning Example
Let us consider extensions needed rather complex new concept constructor:
same-as. semantics same-as([f1 , , fn ],[g1, , gm]), introduced earlier,
denotes individuals two attribute chains [f1 , , fn ] [g1, , gm]
known filler. (The attributes must exactly one filler.)
constructor useful representing relationship actions subactions. example, suppose try define action BUYing terms two GIVEs:
425

fiBorgida

would assert BUY subsumed and(all(giving1,GIVE),all(giving2,GIVE)),
add constraints attributes BUY (such buyer seller)
GIVE (such giver receiver), including same-as([buyer],[giving1
giver]) same-as( [seller],[giving1 receiver]).
procedures need written extend implementation are: same-as::Recognize, same-as::InferFromFilling, same-as::InferFromClosing, same-as::InferFromAsserting, same-as::ConsistentWFilling?, same-as::ConsistentW Closing?, same-as::ConsistentWAsserting?. addition, previously written procedures may need augmented.
Rules inference 6 presented way describing tasks
performed functions. However, case, functions also need worry
dependency pointers, mapping less direct.
following inference rule describes standard recognition procedure individuals
(there additional rules subsumption course):
KB ` b fills([f1, , fn ], e)
KB ` b fills([g1, , gm], e)
KB ` b same-as([f1, , fn ], [g1, , gm])
recognition function follow chain individual e, missing
filler next attribute chain, dependency e must recorded.
same-as::Recognizes?(b, same-as(chain1,chain2))
{ (e1,p) := follow chain1 b, returning furthest
individual e1 reached, remainder chain p
possible follow;
not(p=nil)
b; return false;}
{ post dependency e1 RecognizeDependsOnMe
(e2,q) := follow chain2 b, above;
not(q=nil)
b; return false;}
{ post dependency e2 RecognizeDependsOnMe
(e1=e2) & p=q return true else return false.
}
function called inside and::Recognizes?(b,C) follows:
...
every same-as(chn1,chn2) get sameas(C)
same-as::Subsumes?(same-as(chn1,chn2), Descriptor(b))
same-as::Recognizes?(b,same-as(chn1,chn2))
return false.
...
Reasoning same-as concerning individuals actually complex,
additional rule inference,
KB ` b fills(chain1, e)
KB ` b same-as(chain1, chain2)
KB ` e same-as(chain3, chain4)
KB ` b same-as(chain1 chain3, chain2 chain4)
426

fiExtensible DL Representation Reasoning

Applying rule straightforwardly verify whether b same-as([f1, , fn ],
[g1, , gm]) requires one try possible ways dividing chain [f1, , fn ]
subchains, each, one needs try derive subchain equalities alternately
individual fillers along chain, explicit assertions same-as descriptions
individuals. cases, implementor, consultation knowledge engineer
use extended system, would make judgment necessity implementing
expensive inference. rules inference used describe deductions
implemented not.
order check consistency, need consider three kinds updates: assertions,
fillers, closings. equality first asserted individual, would invoke:
same-as:: ConsistentWAsserting?(ind, same-as(chain1,chain2))
{ (e1,p) := follow chain1 ind;
not(p=nil)
ind; return true;}
{ post dependency e1 ConsistentDependsOnMe
(e2,q) := follow chain2 ind;
not(q=nil)
ind; return true;}
{ post dependency e2 ConsistentDependsOnMe
e1=e2 return true else signal InconsistentExcn;
}
Since attributes one value, attributes get closed automatically
filler provided, usually explicit closing role affect
same-as, need same-as::ConsistentWClosing?.
Finally, C includes conjunct same-as(chain1,chain2), and::ConsistentWFilling?(ind,C,p,newfiller) invoke
same-as::ConsistentWFilling?(ind,same-as(chain1,chain2),p,newfiller)
{if (member(p,chain1) member(p,chain2) )
same-as:ConsistentWAsserting?(ind,same-as(chain1,chain2))}
Equalities lead inferences one chains completely known,
values last attribute second chain known, illustrated
following example:
{ : same-as([q] , [p, r]), (a, e) : q, (a, b) : p } |= b fills(r, e)
links watch cases
inference requires us set InferDependsOnMe
sufficient information make inference.
point same-as presents one example situation performance
penalty paid separating consistency checking, recognition inference:
functions attempts traverse individuals along two chains equality
effort reach ends. price negligible relatively individuals
same-as conditions attached, chains usually one two attributes long,
case application same-as illustrated earlier. Otherwise, introduce
427

fiBorgida

caching speed code: associate every equality individual b two pairs (e1,r1)
(e2 ,r2) representing ends ei reached chain, remainder chains
ri left traversed. (So chain completely known, r = nil.) final alternative
add code same-as::InferFrom same-as::ConsistentW? function,
latter performs deductions involving same-as. disadvantage approach,
many optimizations, loss modularity.
Finally, mention extensions reasoning individual level
considered include database aggregate functions like sum (e.g., sum([departments
budget], totalBudget) used model totalBudget sum values
budget fillers department fillers); epistemic constructors allow one
query state knowledge (Donini et al., 1998) (e.g., known-all(friends, knownat-most(1,pets)) recognizes individuals whose known friend fillers
one pet recorded KB, without roles closed).

6. Conclusions
began hypothesis perfect DL ever built,
need application-specific reasoning, potential incompleteness reasoning due
expressiveness-(tractability/decidability) trade-off. argued issues
best attacked per-application basis. resolve problem, proposed use
extensible DL-KBMS, one tries go far possible initial set wellunderstood concept constructors, then, encountering unsolvable expressiveness
problems (Doyle & Patil, 1991), add new concept constructors overcome them.
also pointed limitations approach, include
conducive including new forms reasoning abduction, contexts, etc.,
difficulties complete inferences useful concept constructors require reasoning
contradiction, best handled alternative DL reasoning paradigm tableaux.
6.1 Implementation Status
Prototype implementations aspects concept individual reasoning protodl
carried Rutgers. However, experience fielded systems indicates
order magnitude work done making system usable
others developers. reason, goal add results protodl
research existing DL-reasoner. fact, several ideas transfered,
collaboration Charles Isbell, newest version classic system released
AT&T Research. particular, classic supports test-defined concepts ones allow
recognition individuals use arbitrary LISP function. (This function
seen combination Recognizes? ConsistentW? functions discussed
present paper.) newest version classic, one simulate addition
new concept constructors using keyword test-c. example, concept
all(vacation,dateRange(1996/6/1,1996/8/31)) would represented
(test-concept dateRange vacation ( (1996 6 1) (1996 8 31)) )
428

fiExtensible DL Representation Reasoning

Although aspects protodl system implemented, current
extensions (Borgida et al., 1996) allow significant subsumption reasoning done test
extensions, thus provide classic bases extensible reasoning.
6.2 Related Work
order incorporate new concept constructors reasoner, need extend
implementation. One approach would offer form declarative description inferences performed, meta-interpreter executes
them. fact, approaches tried past kinds representation formalisms (Greiner & Lenat, 1980; Genesereth, 1983). Except cycl (Lenat &
Guha, 1990), allows addition new forms inference rule schemas First
Order Predicate Calculus, see little evidence meta-interpreter chance
nearly efficient custom-built implementations, opted different
approach.
Joshua (Rowley, Shrobe, & Cassels, 1986) also effort providing extensible reasoning, allows user (knowledge system engineer) ability change compile-time
implementation elements protocol inference, describes
reasoning system. Joshua close spirit work sense tries
maintain uniform knowledge level view system, identifies,
protocol inference, specific aspects system customized
(re)programming Lisp functions. efforts differ Joshua interested
DLs (Joshuas protocol concerned mostly rule triggering truth maintenance), wish support incrementing syntax knowledge-level interface,
also care semantics extension.
Gaines also advocated utility declarative specification clean, extensible modularization DL-reasoner (Gaines, 1993). concept level, one difference
approaches seems protodl assumes concepts
form upper-semilattice (the constructor built-in), wider variety inferences implemented new constructors. individual-reasoning level, protodl
starts much restricted basis, uses extensibility deal aspects
propagations (e.g., example dealing same-as reasoning appears built-in
KRSn). hand, KRSn built-in support rules exceptions them,
important component knowledge-based environment.
Finally, suggested tableau-based approach, crack,
essentially extensible addition new completion rules (Bresciani et al.,
1995), traditionally used build model certain knowledge base, prove
inconsistency. tableau completion rules also implement incomplete reasoners
using subset inferences. concrete domain extensions (Section 3.5),
advantages extensible tableau techniques lie clean formalism lead complete
reasoning, disadvantages involve language extensions negation
(e.g., clasp see Section 3.4), and, moment, lack experience large
A-boxes incremental updates. One exciting (though likely difficult)
future prospects combining two implementation paradigms, extensibility
features.
429

fiBorgida

6.3 Summary
advocated approach extensible DL reasoning implementation,
normalize-compare paradigm, two components: declarative specification
modularized implementation framework.
specification offered using rules
inference natural semantics style, heuristic methodology suggests various
categories rules looked for. rules often correlate well implementations various functions, protodl offers implementor opportunity
use different implementation. later needed case constructors whose
argument example regular expression case plans strings,
implementation needs use kind finite automaton representation, since regular
expressions normal form.
modularized software architecture protodl reasoner, every
new concept constructor added language, well-defined set functions
need implemented. These, turn, sometime detailed skeletons composed
other, smaller functions, abstracted analyzing variety implementations. invocation functions organized built-in functions protodl,
usually involving constructor and. paid particular attention supporting
efficient implementations, offering implementor quite lot freedom within
confines major functions. individual reasoning, first paper consider
need efficient face incremental updates DL-KBMSs, live
open-world assumption, use concept descriptions infer new properties,
rather verify correctness individual facts. solutions involved proposing
function variants based form update, use various kinds associated
dependency links.
major open areas involve adding framework role constructors, epistemic
rules (like classic, characterized Donini et al (Donini et al., 1998)), ability
express least simple recursive declarations primitive concepts (e.g., parents
person persons), connections tableau-based implementations DL
reasoning.

Acknowledgements
grateful Ron Brachman, who, among others, joined initial explorations protodl idea; Daniel Kudenko, implemented significant part
individual reasoning; Charles Isbell, implemented extensibility features
classic 2.3; Peter Patel-Schneider, years discussion subtleties
classic language implementation. Extremely useful comments presentation
organization provided Peter Clark anonymous reviewers.
work supported part NSF grants IRI-91-19310 IRI-9619979.

Appendix A. Generic Conjunction Function.
following pseudo-code Q::Conjoin shows one construct function
several, smaller functions.
430

fiExtensible DL Representation Reasoning

/* Conjoin new term Q(T) onto already-normalized description This*/
Q::Conjoin(T:<NormalizedQ-term>,This:<NormalizedConcept>){

old := get Q(This) /* find part dealing constructor Q*/
not(old=nil)
Q::SubsumesSame?(T,old)
signal RedundantExn;
:= Q::ConjoinToSame(T,old); /* combine old*/
Q::SubsumesDifferent?(T,This) /* implied constructors */
signal RedundantExn;
T:= Q::ConjoinToDifferent(T,This) /* obtain possibly stronger T*/
(type(T) 6= Q) /* stronger description might use another constructor*/
{post ImplicationsToDoList; /* later processing*/
return; }
Q::ConsistentWithDifferent?(T,This) /* Check coherent rest
This; not, exception raised.*/
put Q(T,This) /* add description normalized descriptor This*/
Q::FindOtherImplications(T,This,ImplicationsToDoList)
/* add additional constructors implied */
Figure 3: Pseudo-code generic conjunction function.
consider individual functions introduced above, remark constructor Q implies terms built constructors (i.e., adds entries ImplicationsToDoList), and::Conjoin function needs augmented loop
end, conjoins ontoNC normalized terms ImplicationsToDoList:
notEmpty(ImplicationsToDoList)
{ another := removeAnElement(ImplicationsToDoList);
suppose type(another)=Q;
Q::conjoin(another,ontoNC)
Note processing may add new entries ImplicationsToDoList.
case many additions done ImplicationsToDoList, useful
optimization might test remove redundancies ImplicationsToDoList.
constructors making additions, different optimization might applied:
add code end corresponding constructors Conjoin program.
following intended purpose component functions used
Q::Conjoin function Figure 3. Note particular constructor (e.g., all),

every function needs implemented. using good optimizing compiler,
default programs could provided, return constant values. Otherwise,
Conjoin edited.
431

fiBorgida

Q::Universal?(T): equivalent top hierarchy set
values?
example, at-least::Universal?
returns true processing
at-least(0,players). used eliminate unnecessary constructs
normal form.
Q::Incoherent?(T): incoherent ?
example, some(players ,nothing) incoherent since nothing instances.
Q::IncoherentWithDifferent(T,This): inconsistency consider
conjoining constructors? example, Q=at-least, conjoining
at-least(3,players) concept already containing at-most(1,players) would
lead inconsistency.
Q::SubsumesSame?(T,oldTs): implied Q-constructed terms already seen
oldTs? basically structural subsumption part. example,
Q=at-least, oldTs at-least(4,players), at-least(3,players) redundant
Q::SubsumesDifferent?(T,This): implied constructors? example,
at-least(1,players) implied some(players, OLD).
Q::ConjoinToSame(T,OldTs): Merge preceding terms built K.
example, combine all(players,GENTLEMAN) all(players,SCHOLAR),
obtain all(players,and(GENTLEMAN, SCHOLAR)).
Q::ConjoinToDifferent(T,This): Produce stronger using information
constructors. example, processing some(player, TALL),then
concept already all(player,OLD), some::ConjoinToDifferent
returns description some(players,and(TALL,OLD)).
Q::FindOtherImplications(T,This,ImplicationsToDoList): Add constructors
implied Q(T), possibly conjunction rest
concept, This.
example, adding all(players,TALL)
concept at-least(3,players), all::FindOtherImplications would add
some(players,TALL) list constructors conjoined This.

References
Baader, F. (1996). formal definition expressive power terminological knowledge
representation languages. J. Logic Computation, 6, 3354.
Baader, F., & Hanschke, P. (1991). scheme integrating concrete domains concept
languages. Proceedings IJCAI91.
Baader, F., Hollunder, B., Nebel, B., Profitlich, H.-J., & Franconi, E. (1994). empirical
analysis optimization techniques terminological representation systems - making kris get move on. Applied Intelligence, 4, 109132.
432

fiExtensible DL Representation Reasoning

Borgida, A. (1992a). type systems knowledge representation: Natural semantics
specifications description logics. Int. J. Intelligent Cooperative Information
Systems, 1, 259269.
Borgida, A. (1992b). Towards systematic development terminological reasoners:
clasp reconstructed. Proceedings KR92.
Borgida, A. (1995). Description logics data management. IEEE Trans. Knowledge
Data Engineering, 7, 671682.
Borgida, A. (1996). relative expressiveness description logics predicate logics.
Artificial Intelligence, 82, 353367.
Borgida, A., Brachman, R. J., McGuinness, D. L., & Resnick, L. A. (1989). classic:
structural data model objects. Proceedings SIGMOD89.
Borgida, A., Isbell, C., & McGuinness, D. (1996). Reasoning black boxes: Handling
test concepts Classic. Proceedings Intern. Workshop Description Logics
(DL96).
Borgida, A., & McGuinness, D. L. (1996). Asking queries frames. Proceedings
KR96.
Borgida, A., & Patel-Schneider, P. F. (1994). semantics complete algorithm subsumption classic description logic. Journal Artificial Intelligence Research,
1, 277308.
Borgida, A., & Weddell, G. (1997). Uniqueness constraints description logics. Proceedings Conf. Deductive Object-Oriented Databases (DOOD97).
Bresciani, P., Franconi, E., & Tessaris, S. (1995). Implementing testing expressive
description logics: preliminary report. Proceedings KRUSE95 Symposium.
Cohen, W., Borgida, A., & Hirsh, H. (1992). Computing least common subsumers
description logics. Proceedings AAAI92.
Devanbu, P. (1994). Software Information Systems. Ph.D. thesis, Rutgers University, New
Brunswick, NJ, USA.
Devanbu, P., & Jones, M. (1997). use description logics kbse systems. ACM
Transactions Software Engineering Methodology, 6.
Devanbu, P., & Litman, D. (1996). Taxonomic plan reasoning. Artificial Intelligence, 84,
135.
Donini, F., Lenzerini, M., Nardi, D., Nutt, W., & Schaerf, A. (1998). epistemic operator
description logics. Artificial Intelligence, 100, 225274.
Doyle, J., & Patil, R. (1991). Two theses knowledge representation: language restrictions, taxonomic classification, utility representation services. Artificial
Intelligence, 48, 261298.
433

fiBorgida

Gaines, B. (1993). class library implementation principled open architecture knowledge representation server plug-in data types. Proceedings IJCAI93.
Genesereth, M. (1983). overview meta-level architecture. Proceedings AAAI83.
Greiner, R., & Lenat, D. (1980). rll: representation language language. Proceedings
KR94.
Horrocks, I. (1998). Using expressive description logic: Fact fiction?. Proceedings
KR98.
Horrocks, I., & Patel-Schneider, P. (1998). DL systems comparison. Proceedings
International Workshop Description Logics - DL98.
Lenat, D., & Guha, R. (1990). Building Large Knowledge-Based Systems. Addison Wesley.
MacGregor, R. (1986). deductive pattern matcher. Proceedings AAAI86.
Padgham, L., & Lambrix, P. (1994). framework part-of hierarchies terminological
logics. Proceedings KR94.
Patel-Schneider, P. (1998). dlp system description. Proceedings International
Workshop Description Logics (DL98).
Rowley, S., Shrobe, H., & Cassels, R. (1986). Joshua: Uniform access heterogeneous
knowledge structures. Proc AAAI86.
Royer, V., & Quantz, J. (1992). Deriving inference rules terminological logics.
Proceedings JELIA92.
Schaerf, A. (1994). Reasoning individuals concept languages. Data Knowledge
Engineering, 12, 141176.
Schmidt-Schauss, M. (1989). Subsumption KL-ONE undecidable. Proceedings
KR89.
Stockmeyer, L. (1974). Complexity Decision Problems Automata Theory
Logic. Ph.D. thesis, MIT, Cambridge, MA. Project MAC TR 138.
Vilain, M. (1985). restricted language architecture hybrid representation system.
Proceedings IJCAI85.
von Luck, K., Nebel, B., Peltason, C., & Schmiedel, A. (1987). anatomy back
system. KIT 41, Technical University Berlin, Berlin, Germany.
Wright, J., Weixelbaum, E., Vesonder, G., Brown, K., Palmer, S., Berman, J., & Moore, H.
(1993). knowledge-based configurator supports sales, engineering, manufacturing AT&T network systems. AI Magazine, 14, 6980.

434

fiJournal Artificial Intelligence Research 10 (1999) 117-167

Submitted 2/98; published 3/99

Modeling Belief Dynamic Systems
Part II: Revision Update
Nir Friedman

nir@cs.huji.ac.il

Institute Computer Science
Hebrew University, Jerusalem, 91904, ISRAEL



http://www.cs.huji.ac.il/ nir

Joseph Y. Halpern

halpern@cs.cornell.edu

Computer Science Department
Cornell University, Ithaca, NY 14853
http://www.cs.cornell.edu/home/halpern

Abstract

study belief change active area philosophy AI. recent years
two special cases belief change, belief revision belief update, studied
detail. companion paper (Friedman & Halpern, 1997), introduce new framework
model belief change. framework combines temporal epistemic modalities
notion plausibility, allowing us examine change beliefs time. paper,
show belief revision belief update captured framework.
allows us compare assumptions made method, better understand
principles underlying them. particular, shows Katsuno Mendelzon's notion
belief update (Katsuno & Mendelzon, 1991a) depends several strong assumptions
may limit applicability artificial intelligence. Finally, analysis allow us
identify notion minimal change underlies broad range belief change operations
including revision update.

1. Introduction
study belief change active area philosophy artificial intelligence.
focus research understand agent change beliefs result
getting new information. Two instances general phenomenon studied
detail. Belief revision (Alchourron, Gardenfors, & Makinson, 1985; Gardenfors, 1988)
focuses agent change (set of) beliefs adopts particular
new belief. Belief update (Katsuno & Mendelzon, 1991a), hand, focuses
agent change beliefs realizes world changed.
approaches attempt capture intuition agent make minimal changes
beliefs order accommodate new belief. difference belief revision
attempts decide beliefs discarded accommodate new belief,
belief update attempts decide changes world led new observation.1
1. Throughout paper use \revision" refer AGM's proposal revision (Alchourron et al., 1985)
generic term general approach initiated AGM; similarly, use \update" refer
KM's proposal update (Katsuno & Mendelzon, 1991a).

c 1999 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiFriedman & Halpern

Belief revision belief update two many possible ways modeling belief change.
(Friedman & Halpern, 1997), introduce general framework modeling belief
change. start framework analyzing knowledge multi-agent systems,
introduced (Halpern & Fagin, 1989), add measure plausibility
situation. define belief truth plausible situations. resulting
framework expressive; captures time knowledge well beliefs.
time allows us reason framework changes beliefs agent. also
allows us relate beliefs agent future actual beliefs
future. Knowledge captures precise sense non-defeasible information agent
world, belief captures defeasible assumptions implied plausibility
assessment. framework allows us represent broad spectrum notions belief
change. paper, focus how, particular, belief revision update
represented.
certainly first provide semantic models belief revision update.
example, (Alchourron et al., 1985; Grove, 1988; Gardenfors & Makinson, 1988; Rott,
1991; Boutilier, 1992; de Rijke, 1992) deal revision, (Katsuno & Mendelzon, 1991a;
del Val & Shoham, 1992) deal update. fact, several works literature
capture using machinery (Katsuno & Satoh, 1991; Goldszmidt & Pearl,
1996; Boutilier, 1998), others simulate belief revision using belief update (Grahne,
Mendelzon, & Rieter, 1992; del Val & Shoham, 1994). approach different
construct specific framework capture one belief
change paradigms. Instead, start natural framework model agent's
knowledge changes time add machinery captures defeasible notion
belief.
believe representation offers number advantages, gives deeper
understanding revision update. one thing, show revision
update viewed proceeding conditioning initial prior plausibilities. Thus,
representation emphasizes role conditioning way understanding minimal
change. Moreover, shows major differences revision update
understood corresponding differences initial beliefs. example, revision
places full belief assumption propositions used describe world
static, change truth value time. way contrast, update allows
possibility propositions change truth value time. However, family
prior plausibilities use capture update framework property
prefer sequences events abnormal events occur late possible.
property, conditioning update always \explains" observations recent changes.
fact time appears explicitly framework allows us make issues precise.
literature, revision viewed dealing static worlds (although
agent's beliefs may change, underlying world agent reasoning
not) update viewed dealing dynamic worlds (see, example, (Katsuno & Mendelzon, 1991a)). believe distinction static dynamic
worlds somewhat misleading. fact, important revision world
static, propositions used describe world static. example, \At
time 0 block table" static proposition, \The block table"
not, since implicitly references current state affairs. (Note assumption
118

fiModeling Belief Dynamic Systems. Part II.

propositions static unique belief revision. Bayesian updating, example,
makes similar assumptions.) model time explicitly framework,
examine issue detail. fact, Section 7, show relate two
viewpoints. precisely, given system, replace proposition p used system
family propositions \p true time m", one time m. resulting system
describes exactly process original system, different linguistic perspective. show, original system corresponds KM update, resulting
system close satisfying requirements AGM revision. requirement
met prior totally ordered, ranked. requirement, however,
relaxed several variants revision (Katsuno & Mendelzon, 1991b; Rott, 1992).
Thus, large part difference revision update understood
difference language used describe happening.
generality framework forces us clear assumptions make
process capturing revision update. consequence, deal
issues largely ignored previous semantic accounts. One issues
status observations. show below, capture either revision update,
assume observations minimally informative|the information carried
observation ' ' believed. strong assumption, since
observations carry additional information. example, trekking Nepal,
one expect observe weather Boston. agent observes
fact raining Boston, \observation" might well provide extra information
world (for example, cable television available Nepal). remark
(Boutilier, Friedman, & Halpern, 1998) treatment revision framework
observations allowed convey additional information.
Finally, representation makes clear intuitions revision update
applied settings postulates used describe sound.
example, consider situations may irreversible changes (such death,
breaking glass vase), agent may perform actions beyond making
observations. Revision update, stand, cannot handle situations.
show, framework allows us extend natural way do.
rest paper organized follows. Section 2, give overview
framework introduced (Friedman & Halpern, 1997). Section 3, give brief review
belief revision belief update. Section 4, define specific class structures
embody assumptions common update revision. Section 5,
describe additional assumptions required capture revision. Section 6,
describe assumptions required capture update. Section 7, reexamine
differences similarities belief revision update. Section 8, consider
possible extensions setup revision update, discuss extensions
handled framework. Finally, Section 9, conclude discussion
related future work.
119

fiFriedman & Halpern

2. Framework
review framework Halpern Fagin (1989) modeling knowledge,
extension dealing belief change. reader encouraged consult (Fagin,
Halpern, Moses, & Vardi, 1995) details motivation.

2.1 Modeling Knowledge

framework Halpern Fagin developed model knowledge distributed
(i.e., multi-agent) systems (Halpern & Fagin, 1989; Fagin et al., 1995). paper,
restrict attention single agent case. key assumption framework
characterize system describing terms state changes time.
Formally, assume point time, agent one possibly infinite
set (local) states. point, put structure states
(although, shall see examples, model situations natural way,
states typically great deal meaningful structure). Intuitively, local state
encodes information agent observed thus far. also environment,
whose state encodes relevant aspects system part agent's local
state.
global state pair (se ; sa ) consisting environment state se local state
sa agent. run system function time (which, ease exposition,
assume ranges natural numbers) global states. Thus, r run,
r(0); r(1); : : : sequence global states that, roughly speaking, complete description
happens time one possible execution system. Given run r,
define two functions ra map time states environment
agent, respectively, taking (m) state environment global state
r(m) ra(m) agent's local state r(m). thus identify run r
pair functions hre; rai. take system consist set runs. Intuitively,
runs describe possible behaviors system, is, possible sequences
events could occur system time.
Given system R, refer pair (r; m) consisting run r 2 R time
point. say two points (r; m) (r0; m0) indistinguishable agent,
write (r; m) (r0; m0), ra(m) = ra0 (m0), i.e., agent local state
points. Finally, interpreted system tuple (R; ) consisting system R together
mapping associates point truth assignment set primitive
propositions. interpreted system talk agent's knowledge: agent
knows ' point (r; m) ' holds points (r0; m0) (r; m) (r0; m0).
Intuitively, agent knows ' (r; m) ' implied information local state
ra(m). give formal semantics language knowledge (and time plausibility)
Section 2.3.

Example 2.1: circuit diagnosis problem well studied literature (see
(Davis & Hamscher, 1988) overview). Consider circuit contains n logical
components c1; : : :; cn k lines l1; : : :; lk . agent set values input lines
circuit observe values output lines. agent compares actual
output values expected output values attempts locate faulty components.
120

fiModeling Belief Dynamic Systems. Part II.

Since single test usually insucient locate problem, agent might perform
sequence tests.
want model diagnosis using interpreted system. so, need describe
agent's local state, state environment, appropriate propositions
reasoning diagnosis. Intuitively, agent's state sequence input-output
relations observed, environment's state describes current state circuit.
consists failure set , is, set faulty components circuit
values lines circuit. run describes results specific series
tests agent performs results observes. make two additional assumptions:
(1) agent forget tests performed results, (2) faults
persistent change time.
make precise, define environment state point (r; m) consist
failure set (r; m), denote fault(r; m), well values lines
circuit. require environment state consistent description
circuit. Thus, example, c1 gate input lines l1 l2
output line l3, (m) says c1 faulty, require
1 l3 1 l1 l2.2 capture assumption
faults persistent requiring fault(r; m) = fault(r; 0). later results,
useful describe agent's observations using logical language. Consider set
diag = ff1 ; : : :; fn ; h1; : : :; hk g primitive propositions, fi denotes component
faulty hi denotes 1 line (that is, line \high" state).
observation conjunction literals form hi :hi . agent's state time
sequence observations. Formally, define agent's state ra(m)
ho1 ; : : :; omi, where, intuitively, ok formula describing input-output relation
observed time k. use notation io(r; k) denote formula describing
observation made agent point (r; k). Given language, define
interpretation diag obvious way. say observation consistent
environment state (m) states input/output lines (m) agree
o. system Rdiag consists runs r satisfying requirements io(r; m)
consistent (m) times m.
Given system (Rdiag ; diag ), examine agent's knowledge making
sequence observations o1; : : :; om . easy see agent knows fault set
must one observations consistent. However, agent cannot rule
fault sets. Thus, even observations consistent circuit
fault-free, agent know circuit fault-free, since might
fault manifests configurations yet tested. course,
agent might strongly believe circuit fault-free, cannot (yet) express
fact formalism. next section rectifies problem. ut
2. Note means recover behavior circuit (although necessarily exact
description) simply looking environment state point failures. course,
could yet richer environment state encodes actual description circuit,
unnecessary analysis here.

121

fiFriedman & Halpern

2.2 Plausibility Measures

non-probabilistic approaches belief change require (explicitly implicitly)
agent ordering possible alternatives. example, agent might
preference ordering possible worlds (Boutilier, 1994b; Grove, 1988; Katsuno &
Mendelzon, 1991b) entrenchment ordering formulas (Gardenfors & Makinson,
1988). ordering dictates agent's beliefs change. example, (Grove, 1988),
new beliefs characterized preferred worlds consistent
new observation, (Gardenfors & Makinson, 1988), beliefs discarded according
degree entrenchment consistent add new observation
resulting set beliefs. represent ordering using plausibility measures ,
introduced (Friedman & Halpern, 1995, 1998b). brie review relevant definitions
results here.
Recall probability space tuple (W; F ; Pr), W set worlds, F
algebra measurable subsets W (that is, set subsets closed union
complementation assign probability), Pr probability measure, is,
function mapping set F number [0; 1] satisfying well-known probability
axioms (Pr(;) = 0, Pr(W ) = 1, Pr(A [ B ) = Pr(A) + Pr(B ), B disjoint).
Plausibility spaces direct generalization probability spaces. simply replace
probability measure Pr plausibility measure Pl, which, rather mapping sets
F numbers [0; 1], maps elements arbitrary partially ordered set.
read Pl(A) \the plausibility set A". Pl(A) Pl(B ), B least plausible
A. Formally, plausibility space tuple = (W; F ; Pl), W set worlds, F
algebra subsets W , Pl maps sets F domain plausibility values
partially ordered relation (so exive, transitive, anti-symmetric).
assume pointed : is, contains two special elements >D , ?D
?D >D 2 D; assume Pl(W ) = >D Pl(;) =?D .
usual, define ordering <D taking d1 <D d2 d1 d2 d1 6= d2 . omit
subscript , <D , >D , ?D whenever clear context.
Since want set least plausible subsets, require
A1 B, Pl(A) Pl(B).
brief remarks definition: deliberately suppressed domain
tuple , since purposes paper, ordering induced
subsets F relevant. algebra F also play significant role
paper. Unless say otherwise, assume F contains subsets interest suppress
mention F , denoting plausibility space pair (W; Pl).
Clearly plausibility spaces generalize probability spaces. (Friedman & Halpern, 1998b,
1995) show also generalize belief function (Shafer, 1976), fuzzy measures (Wang
& Klir, 1992), possibility measures (Dubois & Prade, 1990), ordinal ranking (or -ranking )
(Goldszmidt & Pearl, 1996; Spohn, 1988), preference orderings (Kraus, Lehmann, & Magidor, 1990; Shoham, 1987), parameterized probability distributions (Goldszmidt, Morris,
& Pearl, 1993) used basis Pearl's -semantics defaults (Pearl, 1989).
goal describe agent's beliefs terms plausibility. this, describe
evaluate statements form B' given plausibility space. fact, use
richer logical language also allows us describe agent compares different
122

fiModeling Belief Dynamic Systems. Part II.

alternatives. logic conditionals. Conditionals statements form
' , read \given ', plausible" \given ', default ". syntax
logic conditionals simple: start primitive propositions close
conjunction, negation modal operator . resulting language denoted LC .
plausibility structure tuple PL = (W;Pl; ), W set possible worlds,
Pl plausibility measure W , (w) truth assignment primitive propositions.
Given plausibility structure PL = (W;Pl; ), define [ '] PL = fw 2 W : (w) j= 'g
set worlds satisfy '. omit subscript PL, clear
context. Conditionals evaluated according rule essentially
one used Dubois Prade (1991) evaluate conditionals using possibility measures:
PL j= ' either Pl([['] ) =? Pl([[' ^ ] ) > Pl([[' ^ : ] ).
Intuitively, '
holds vacuously ' impossible; otherwise, holds ' ^
plausible ' ^ : . show (Friedman & Halpern, 1998b), semantics
conditionals also generalizes semantics conditionals -ranking (Goldszmidt & Pearl,
1996), PPD structures (Goldszmidt et al., 1993). also show (Friedman &
Halpern, 1998b), semantics conditionals generalizes semantics preferential
structures. relationship plays role discussion below, review necessary
definitions here. preferential structure tuple (W; ; ), partial order
W . Roughly speaking, w w0 holds w preferred w0 .3 intuition (Shoham,
1987) preferential structure satisfies conditional '
preferred
worlds (i.e., minimal worlds according ) [ '] satisfy . However, may
minimal worlds [ '] . happen [ '] contains infinite descending sequence
: : : w2 w1. structures? number options: first
assume that, formula ', minimal worlds [ '] ; assumption
actually made (Kraus et al., 1990), called smoothness assumption. yet
general definition|one works even smooth|is given (Lewis, 1973;
Boutilier, 1994a). Roughly speaking, '
true if, certain point on, whenever '
true, . formally,
(W; ; ) satisfies ' , every world w1 2 [ '] , world w2
(a) w2 w1 (so w2 least normal w1), (b) w2 2 [ ' ^ ] ,
(c) worlds w3 w2 , w3 2 [ ' ) ] (so world normal
w2 satisfies ' also satisfies ).
easy verify definition equivalent earlier one smooth.

!

!

!
!

!

!

!

Proposition 2.2: (Friedman & Halpern, 1998b) preference ordering W ,
plausibility measure Pl W (W; ; ) j= '!
(W; Pl ; ) j= '! .
brie describe construction Pl here, since use sequel. Given
preference order W , let D0 domain plausibility values consisting one
3. follow standard notation preference (Kraus et al., 1990), uses (perhaps confusing) convention placing likely (or less abnormal) world left operator.
Unfortunately, translated plausibility, mean w w0 holds iff Pl(fwg > Pl(fw0 g).

123

fiFriedman & Halpern

element dw every element w 2 W . define partial order D0 using : dv < dw
w v . (Recall w w0 denotes w preferred w0.) take
smallest set containing D0 closed least upper bounds (so every set
elements least upper bound D). subset W , define
Pl (A) least upper bound fdw : w 2 Ag. Since closed least upper
bounds, Pl(A) well defined. show (Friedman & Halpern, 1998b), choice
Pl satisfies Proposition 2.2.
results (Friedman & Halpern, 1998b) show semantics conditionals
generalizes previous semantics conditionals. semantics capture intuitions
conditionals? AI literature, little consensus \right"
properties defaults (which essentially conditionals). However,
consensus reasonable \core" inference rules default reasoning. core usually
known KLM properties (Kraus et al., 1990), includes properties
' 1 ' 2 infer ' 1 ^ 2
'1 '2 infer '1 _ '2
constraints plausibility spaces gives us KLM properties? Consider following
two conditions:
A2 A, B, C pairwise disjoint sets, Pl(A[B) > Pl(C ), Pl(A[C ) >
Pl(B ), Pl(A) > Pl(B [ C ).
A3 Pl(A) = Pl(B) =?, Pl(A [ B) =?.
plausibility space (W; Pl) qualitative satisfies A2 A3. plausibility structure (W; Pl; ) qualitative (W; Pl) qualitative plausibility space. (Friedman &
Halpern, 1998b), show that, general sense, qualitative plausibility structures
capture default reasoning. precisely, show KLM properties sound
respect class plausibility structures class consists qualitative plausibility structures. (We also provide weak condition show necessary
sucient KLM properties complete.) results show plausibility
structures provide unifying framework characterization default entailment
different logics.

!
!

!
!

!

2.3 Plausibility Knowledge

!

(Friedman & Halpern, 1997) show plausibility measures incorporated
multi-agent system framework (Halpern & Fagin, 1989). allows us describe
agent's assessment possible states system point time.
time also introduce conditionals logical language order reason
plausibility assessments. review relevant details.
(interpreted) plausibility system tuple (R; ; P ) where, before, R set
runs maps point truth assignment, P plausibility assignment function mapping point (r; m) qualitative plausibility space P (r; m) =
(W(r;m); Pl(r;m)). Intuitively, plausibility space P (r; m) describes relative plausibility events point view agent (r; m). paper, restrict
attention plausibility spaces satisfy two additional assumptions:
124

fiModeling Belief Dynamic Systems. Part II.

W(r;m) = f(r0; m0)j(r; m) (r0; m0)g. Thus, agent considers plausible situa-

tions possible according knowledge.
(r; m) (r0; m0) P (r; m) = P (r0; m0). means plausibility space
function agent's local state.4
define logical language reason interpreted systems. syntax logic
simple; start primitive propositions close conjunction, negation,
K modal operator (K' says agent knows '), modal operator ( ' says
' true next time step), modal operator. resulting language
denoted LKPT .5 recursively assign truth values formulas LKPT point (r; m)
plausibility system . truth primitive propositions determined ,
(I ; r; m) j= p (r; m)(p) = true.
Conjunction negation treated standard way, knowledge: agent
knows ' (r; m) ' holds points cannot distinguish (r; m). Thus,
(I ; r; m) j= K' (I ; r0; m0) j= ' (r0; m0) (r; m).
' true (r; m) ' true (r; + 1). Thus,
(I ; r; m) j= ' (I ; r; + 1) j= '.
Finally, define conditional operator describe agent's plausibility assessment
current time. Let [ '] (r;m) = f(r0; m0) 2 W(r;m) : (I ; r; m) j= 'g.

!

!

!

(I ; r; m) j= '

either Pl(r;m) ([['] (r;m)) = ? Pl(r;m)([[' ^ ] (r;m)) > Pl(r;m) ([[' ^ : ] (r;m)).

define notion belief. Intuitively, agent believes ' ' plausible
not. Formally, define B' , (true ').
(Friedman & Halpern, 1997) prove that, framework, knowledge S5
operator, conditional operator satisfies usual axioms conditional logic (Burgess,
1981), satisfies usual properties temporal logic (Manna & Pnueli, 1992).
addition, properties imply belief K45 operator, interactions
knowledge belief captured axioms K' ) B' B' ) KB'.

!

!

Example 2.3: (Friedman & Halpern, 1997) add plausibility measure system
defined Example 2.1. define Idiag = (Rdiag ; diag ; Pdiag ), Pdiag plausi-

bility assignment describe. assume failures individual components
independent one another. also assume likelihood component failing
same, also likelihood small (i.e., failures exceptional),
construct plausibility measure follows. (r0; m) (r00; m) two points
W(r;m), say (r0; m) plausible (r00; m) jfault(r0; m)j < jfault(r00; m)j,
4. framework presented (Friedman & Halpern, 1997) general this, dealing multiple
agents allowing agent consider several plausibility spaces local state. simplified
version present suces capture belief revision update.
5. easy add temporal modalities until, eventually, since , etc. play role
paper.

125

fiFriedman & Halpern

is, failure set (r0; m) consists fewer faulty components (r00; m).
extend comparisons sets: Pl(r;m) (A) Pl(r;m)(B ) min(r0 ;m)2A (jfault(r0; m)j)
min(r0 ;m)2B (jfault(r0; m)j); is, less plausible points failure sets
larger cardinality minimal one B . plausibility measure,
agent's observations time consistent failures,
agent believes components functioning correctly. hand,
observations match expected output circuit, agent considers
minimal failure sets consistent observations. Thus, observations
consistent failure c1, failure c3 , combined failure c2 c7,
agent believes either c1 c3 faulty, both.
make precise. failure set (i.e., diagnosis) characterized
complete formula f1 ; : : :; fn |that is, one determines truth values
propositions. example, n = 3, f1 ^:f2 ^:f3 characterizes failure set fc1g.
define D(r;m) set failure sets (i.e., diagnoses) agent considers possible
(r; m); D(r;m) = ff 2 F : (Idiag ; r; m) j= :B :f g F set possible
failure sets.
Belief change Idiag characterized following proposition.

Proposition 2.4: f 2 D(r;m) consistent new observation

io(r; + 1), D(r;m+1) consists failure sets D(r;m) consistent
io(r; +1). f 2 D(r;m) inconsistent io(r; +1), D(r;m+1) consists
failure sets cardinality j consistent io(r; 1); : : :; io(r; + 1), j
least cardinality least one failure set consistent observations.

Thus, Idiag , new observation consistent current set likely explanations
reduces set (to consistent new observation). hand,
surprising observation (one inconsistent current set likely explanations)
rather drastic effect. easily follows Proposition 2.4 io(r; + 1)
surprising, D(r;m) \ D(r;m+1) = ;, agent discards current explanations
case. Moreover, easy induction shows D(r;m) \ D(r;m+1) = ;,
cardinality failure sets D(r;m+1) greater cardinality failure sets
D(r;m). Thus, case, explanations D(r;m+1) complicated
D(r;m). ut

2.4 Conditioning

interpreted system, agent's beliefs change point point plausibility
space changes. general framework put constraints plausibility
space changes. thinking probabilistically, could imagine agent starting
prior runs system. Since run describes complete history
time, means agent puts prior probability possible sequences events
could happen. would expect agent modify prior conditioning
whatever information learned. show below, notion conditioning
closely related belief revision update. remark first applying
conditioning context belief change (cf. (Goldszmidt & Pearl, 1996; Spohn, 1988));
details little complex framework, model time explicitly.
126

fiModeling Belief Dynamic Systems. Part II.

start making simplifying assumption dealing synchronous
systems agents perfect recall (Halpern & Vardi, 1989). Intuitively, means
agent knows time forget observations made.
Formally, system synchronous (r; m) (r0; m0) = m0 . synchronous
systems, agent perfect recall (r0; + 1) (r; + 1) implies (r0; m) (r; m).
Thus, agent considers run r possible point (r; + 1) also considers
possible (r; m). means runs considered impossible (r; m) also
considered impossible (r; + 1): agent forget knew.
probability, assume agent prior plausibility measure
runs describes prior assessment possible executions system.
agent gains knowledge, updates prior conditioning. precisely, point
(r; m), agent conditions previous assessment set runs considered possible
(r; m). results updated assessment (posterior) plausibility runs.
posterior induces, via projection runs points, plausibility measure points.
think agent's posterior time simply prior conditioned
knowledge time m.
Formally, prior plausibility agent plausibility measure Pa = (R; Pla )
runs system. set points, define R(A) = fr : 9m((r; m) 2 A)g
set runs points lie. agent updates plausibilities conditioning
following condition met:
PRIOR prior Pa = (R; Pla) runs r 2 R, times m,
sets A; B W(r;m), Pl(r;m) (A) Pl(r;m) (B ) Pla (R(A))
Pla (R(B )).
definition implies agent's plausibility assessment point determined,
straightforward fashion, prior.
shown (Friedman & Halpern, 1997), synchronous systems satisfy PRIOR
agent perfect recall, say even more: agent's plausibility measure
time + 1 determined plausibility measure time m. make precise,
set points, let prev(A) = f(r; m) : (r; + 1) 2 Ag.

Theorem 2.5: (Friedman & Halpern, 1997). Let synchronous system satisfying
PRIOR agents perfect recall. Pl(r;m+1)(A) Pl(r;m+1) (B )
Pl(r;m)(prev(A)) Pl(r;m)(prev(B )), runs r, times m, sets A; B W(r;m+1).
Thus, synchronous systems agents perfect recall PRIOR implies \local"
rule update incrementally changes agent's plausibility step. local
rule consists two steps. First, agent's plausibility time projected time
+ 1 points. Second, time + 1 points inconsistent agent knowledge
(r; + 1) discarded. procedure implies relative plausibility two sets
runs change unless one incompatible new knowledge.

Example 2.6: easy verify system Idiag consider Example 2.3 satisfies
PRIOR. prior Pa determined failure set run manner similar
construction Pl(r;m) . is, R1 plausible R2 run R1
smaller failure set runs R2 . ut
127

fiFriedman & Halpern

3. Review Revision Update
present brief review belief revision update.
Belief revision attempts describe rational agent incorporates new beliefs.
said earlier, main intuition changes possible made. Thus,
something learned consistent earlier beliefs, added set
beliefs. interesting situation agent learns something inconsistent
current beliefs. must discard old beliefs order incorporate
new belief remain consistent. question ones?
widely accepted notion belief revision defined AGM theory (Alchourron et al., 1985; Gardenfors, 1988). theory originally developed philosophy
science, one attempts understand scientist changes beliefs (e.g., theory physical laws) rational manner. context, seems reasonable assume
world static ; is, laws physics change scientist
performing experiments.
Formally, theory assumes logical language Le set e primitive propositions consequence relation `L contains propositional calculus satisfies
deduction theorem. AGM approach assumes agent's epistemic state
represented belief set, is, set K formulas language Le .6 also
assumed revision operator takes belief set formula ' returns
new belief set ', intuitively, result revising '. following AGM postulates
attempt characterize intuition \minimal change":
e

(R1)
(R2)
(R3)
(R4)
(R5)
(R6)
(R7)
(R8)

' belief set
'2A'
' Cl(A [ f'g)7
:' 62 Cl(A [ f'g) '
' = Cl(false) `L :'
`L ' , ' =
(' ^ ) Cl(A ' [ f g)
: 62 ' Cl(A ' [ f g) (' ^ ).
e

e

essence postulates following. revision ' belief set
include ' (postulates R1 R2). new belief consistent belief set,
revision remove old beliefs add new beliefs
except implied combination old beliefs new belief (postulates
R3 R4). condition called persistence . next two conditions discuss
coherence beliefs. Postulate R5 states agent capable incorporating
consistent belief postulate R6 states syntactic form new belief
affect revision process. last two postulates enforce certain coherency
6. example, Gardenfors (1988, p. 21) says \A simple way modeling epistemic state individual
represent set sentences."
7. Cl(A) = f'jA `Le 'g deductive closure set formulas A.

128

fiModeling Belief Dynamic Systems. Part II.

outcome revisions related beliefs. Basically, state consistent
' (' ^ ) ' .
notion belief update originated database community (Keller & Winslett,
1985; Winslett, 1988). problem knowledge base change something
learned world. example, suppose transaction adds knowledge
base fact \Table 7 Oce 2", contradicts previous belief \Table 7
Oce 1". else change? intuition update attempts capture
transaction describes change occurred world. Thus,
example, applying update might conclude reason table Oce
2 moved, earlier beliefs false. example shows that,
unlike revision, update assume world static.
Katsuno Mendelzon (1991a) suggest set postulates update operator
satisfy. update postulates expressed terms formulas, belief sets.
is, update operator maps pair formulas, one describing agent's current
beliefs describing new observation, new formula describes
agent's updated beliefs. unreasonable, since identify formula '
belief set Cl('). Indeed, finite (which Katsuno Mendelzon assume)
every belief set associated formula 'A Cl('A) = A,
every formula ' corresponds belief set Cl('). Thus, update operator induces
operator maps belief state observation new belief state. slightly
abuse notation use symbol denote types mappings. say
belief set complete if, every ' 2 Le , either ' 2 :' 2 A. formula
complete Cl() complete.
KM postulates are:
(U1) `L ' ) '
(U2) `L ) ', `L ' ,
(U3) `L : ' `L : `L :'
(U4) `L 1 , 2 `L '1 , '2 `L 1 '1 , 2 '2
(U5) `L ( ') ^ ) (' ^ )
(U6) `L '1 ) '2 `L '2 ) '1, `L '1 , '2
(U7) complete `L ( '1 ) ^ ( '2) ) ('1 _ '2)
(U8) `L (1 _ 2 ) ' , (1 ') _ (2 ').
essence postulates following. learning ', agent believes '
(postulate U1, analogous R2). ' already believed, updating '
change agent's beliefs (postulate U2, weaker version R3 R4).
next two postulates (U3 U4) deal coherence belief change process.
analogous R5 R6, respectively, minor differences. Postulates U5 U6
deal observations related other. U5 states beliefs learning
' consistent also believed learning ' ^ . U6 states '2
believed learning '1 '1 believed learning '2, learning either '1
'2 leads belief set. Finally, U7 U8 deal decomposition properties
update operation. U7 states essentially truth assignment L,
e

e

e

e

e

e

e

e

e

e

e

e

e

e

e

129

fiFriedman & Halpern

believed learning '1 also believed learning '2 believed
learning '1 _ '2 . U8 states update knowledge base computed
independent updates sub-part knowledge. is, = 1 _ 2 ,
apply update 1 2 , combine results.

4. Belief Change Systems

want model belief change|particularly belief revision belief update|in
framework systems. so, consider particular class systems call
belief change systems. belief change systems, agent makes observations
external environment. (implicitly) assumed revision update,
assume observations described formulas logical language.
make assumptions regarding plausibility measure used agent.
formalize assumptions conditions BCS1{BCS5, described below, say
system = (R; ; P ) belief change system satisfies conditions. denote
C BCS set belief change systems.
Assumption BCS1 formalizes intuition language includes propositions
reasoning environment, whose truth depends environment state.

BCS1 language L includes propositional sublanguage Le set e
primitive propositions. Le contains usual propositional connectives
comes equipped consequence relation `L . interpretation (r; m)
e

assigns truth propositions e way

(a) (r; m) consistent `L , is, fp : p 2 e ; (r; m)(p) = trueg [
f:p : p 2 e; (r; m)(p) = falseg `L consistent,
(b) (r; m)(p) depends (m) propositions e ; is, (r; m)(p) =
(r0; m0)(p) whenever (m) = re0 (m0 ).
e

e

Part (b) BCS1 implies evaluate formulas Le respect environment
states; is, ' 2 Le (m) = re0 (m0), (I ; r; m) j= ' (I ; r0; m0) j= '.
Since environment relevant formulas Le , ' 2 Le , write se j= '
(I ; r; m) j= ' point (r; m) (m) = se .
BCS2 concerned form agent's local state. Recall that, framework,
local state captures relevant aspects agent's epistemic state. functional
form revision update operators suggests matters regarding
agent changes beliefs agent's current epistemic state (which taken
AGM KM belief set) learned. terms framework,
suggests agent's local state time + 1 function local state
time observation made time m. fact make stronger assumption
agent's state consists sequence observations made agent.
means agent remembers past observations. Note surely implies
agent's local state time + 1 determined state time
observation made time m. make assumption observations made
agent described formulas Le . Although quite strong assumption
expressive power Le , standard literature: revision update
130

fiModeling Belief Dynamic Systems. Part II.

assume observations expressed formulas language (see Section 3).
assumptions formalized BCS2:
BCS2 r 2 R m, ra(m) = ho(r;1); : : :; o(r;m)i
o(r;k) 2 Le 1 k m.
Intuitively, o(r;k) observation agent makes immediately transition
time k , 1 time k run r. Thus, represents agent observes new
state system time k. Note BCS2 implies agent's state time 0
empty sequence runs. Moreover, implies ra(m + 1) = ra(m) o(r;m+1),
append operation sequences. is, agent's state (r; + 1) result
appending previous state latest observation made system.
hard show belief change systems synchronous agents
perfect recall. (We remark agents' local states modeled similar way
model knowledge bases presented (Fagin et al., 1995).)
Clearly want reason language observations agent makes.
Thus, assume language includes propositions describe observations
made agent.
BCS3 language L includes set obs primitive propositions disjoint
e obs = flearn(') : ' 2 Le g. Moreover, (r; m)(learn(')) =
true o(r;m) = ' runs r times m.
system satisfying BCS1{BCS3, talk belief change. agent's state
encodes observations, propositions allow us talk observed.
next assumption somewhat geared situations observations always
\accepted", agent observes ', believes '. necessary
assumption, made belief revision belief update. capture assumption
perhaps simplest possible way: assuming observations
reliable, agent observes ' current state environment satisfies
'. certainly way enforcing assumption observations
accepted, perhaps simplest, focus here. shall see,
assumption consistent revision update, sense capture
systems satisfying it.
BCS4 (I ; r; m) j= o(r;m) runs r times m.
Note BCS4 implies agent never observes false. Moreover, implies
observing ', agent knows ' true. (Boutilier et al., 1998), consider
instance framework observations unreliable (so BCS4 hold
general), examine status R2, acceptance postulate, case.
Finally, assume belief change proceeds conditioning. certainly
assumptions made, tried argue, conditioning principled
approach captures intuitions minimal change, given observations. And,
shall see, conditioning (as captured PRIOR) consistent revision
update.
BCS5 satisfies PRIOR.
131

fiFriedman & Halpern

Many interesting systems viewed BCS's.

Example 4.1: Consider systems Idiag;1 Idiag;2 Example 2.1. systems

BCSs? quite, since diag defined primitive propositions form learn('),
easily embed systems BCS. Let Ldiag propositional language defined
diag , let +diag consist diag together primitive propositions
+ obvious extension
form learn(') ' 2 Ldiag . Let diag
diag +
diag , defined
+
BCS3 holds. easy see (Rdiag ; diag ; Pdiag;i ) BCS: take
e BCS1 diag , define `Ldiag enforces relationships determined
circuit layout. Thus, example, c1 gate input lines l1 l2
output line l3, would `Ldiag :f1 ) (h3 , h1 ^ h2 ). easy see
BCS2{BCS5 hold construction. ut
definitions set background presentation belief revision belief
update.

5. Capturing Revision

Revision captured restricting BCSs satisfy several additional assumptions. describing assumptions, brie review well-known representation
revision help motivate them.
several representation theorems belief revision, clearest perhaps
following (Grove, 1988; Katsuno & Mendelzon, 1991b). associate belief
set set WA possible worlds consists worlds true. Thus,
agent whose belief set believes one worlds WA real world. agent
performs belief revision behaves though belief state ranking ,
i.e., total preorder, possible worlds minimal (i.e., plausible)
worlds ranking exactly WA . revising ', agent chooses
minimal worlds satisfying ' ranking constructs belief set them. easy
see procedure belief revision satisfies AGM postulates. Moreover,
(Grove, 1988; Katsuno & Mendelzon, 1991b), shown belief revision operator
described terms ranking.
representation suggests capture belief revision framework.
define C R C BCS set belief change systems = (R; ; P ) satisfy
conditions REV1{REV4 define below.
Revision assumes world change revision process. Formally implies propositions e change truth value along run,
i.e., (I ; r; m) j= p (I ; r; + 1) j= p p 2 e . says state
world respect properties agent reasons (i.e.,
propositions e ).

REV1 (r; m)(p) = (r; 0)(p) p 2 e points (r; m).
Note REV1 necessarily imply (m) = (m + 1). is, REV1 allows
changing environment. restriction truth value propositions
describe environment change. return issue Section 7.
132

fiModeling Belief Dynamic Systems. Part II.

representation (Grove, 1988; Katsuno & Mendelzon, 1991a) requires agent
totally order possible worlds. put similar requirement agent's plausibility
assessment. Recall BCS5 says agent's plausibility induced prior Pla;
REV2 strengthens assumption.
REV2 prior Pla BCS5 ranked; is, A; B R, either
Pla (A) Pla(B ) Pla (B ) Pla (A), Pl(A [ B ) = max(Pl(A); Pl(B )).
representation (Grove, 1988; Katsuno & Mendelzon, 1991a) also requires
agent considers truth assignments possible. need similar condition, except
want truth assignments considered possible,
nontrivial plausibility (i.e., plausible ?) well.
make precise, helpful introduce notation useful
later definitions well. Given system two sequences '1 ; : : :; 'k o1; : : :; ok0
formulas Le , let R['1; : : :; 'k ; o1; : : :; ok0 ] consist runs r
1 k, formula 'i true (r; i) agent observes o1 ; : : :; ok0 . is,
R['0; : : :; 'k ; o1; : : :; ok0 ] = fr 2 : (I ; r; i) j= 'i; = 0; : : :; k; ra(k0) = ho1; : : :; ok0 ig.
allow either sequence formulas empty, so, example, R['; ] consists
runs ' true initial state. (Note REV1 holds, means '
true subsequent states well.) use notation R['1; : : :; 'm] abbreviation
R['1; : : :; 'm; ].
REV3 ' 2 Le consistent, Pla(R[']) > ?.
might seem REV1{REV3 capture assumptions made representation (Grove, 1988; Katsuno & Mendelzon, 1991a). However, another assumption
implicit way revision performed representations must make explicit
representation, way distinguished observing ' (captured
formula learn(')) ' itself. Intuitively, agent observes ', updates
plausibility assessment conditioning '. essentially think
earlier representations doing. However, representation, agent condition ', fact observed '. Although require ' must
true agent observes (BCS4), agent may general gain extra information
observing '.
understand issue, consider following example. Suppose R
agent observes p1 time (r; m) p2 q also true (r; m), observes
p1 ^ p2 (r; m) q false. easy construct BCS satisfying REV1{REV3
also satisfies requirements. system, observing p1 , agent believes
p2 q. According AGM's postulate R7 (and also KM's postulate U5) agent must
believe q observing p1 ^ p2 . see this, note assumptions R
phrased AGM language p2 ^ q 2 K p1 :q 2 K (p1 ^ p2 ). Postulate
R7 states K (p1 ^ p2 ) Cl(K p1 [ fp2g). Since p2 2 K p1,
Cl(K p1 [fp2g) = K p1. Thus, R7 implies case q 2 K (p1 ^ p2). However,
R, agent believes (indeed knows) :q observing p1 ^ p2.8 Thus, revision update
8. stress mean p1 ^ p2 implies :q R. may well points R
p1 ^ p2 ^ q true. However, points, agent would observe p1 ^ p2 , since agent observes
p1 ^ p2 q false.

133

fiFriedman & Halpern

implicitly assuming observation ' provide additional
knowledge. following assumption ensures case revision (a
general version required update; see Section 6).
REV4 Pla(R['; o1; : : :; om]) Pla(R[ ; o1; : : :; om]) Pla(R[' ^
o1 ^ : : : ^ om ]) Pla (R[ ^ o1 ^ : : : ^ om ]).
assumption captures intuition observing o1; : : :; ok provides information fact o1 ^ : : : ^ om true. is, agent compares
plausibility ' way conditioning observations o1 ; : : :; om
conditioning fact o1 ^ : : : ^ om true. easily follows REV4
PRIOR agent believes observing o1 ^ : : : ^ om exactly o1 ^ : : : ^ om ^
initially considered plausible o1 ^ : : : ^ om ^ : . Thus, agent believes
observing o1 ^ : : : ^ om exactly initially, believed conditional o1 ^ : : : ^ om :
observations provide extra information beyond fact oi 's true.
REV4 quite strong assumption. say observations
give agent additional information (beyond fact true), also says
consistent observations made (since ' ^ consistent, must
Pla(R['; o]) = Pla (R[' ^ o]) > ?, REV3 REV4). might instead consider using
weaker version REV4 says that, provided observation made, gives
additional information. Formally, would captured
REV40 Pla(R['; o1; : : :; om]) > 0, Pla(R['; o1; : : :; om]) Pla(R[ ; o1; : : :; om])
Pla(R[' ^ o1 ^ : : : ^ om ]) Pla (R[ ^ o1 ^ : : : ^ om ]).
following examples suggests REV40 may reasonable practice
REV4. used REV4 comes closer spirit requirement
revision observations possible.

Example 5.1: Consider system Idiag;1 described Example 2.1. discussed Ex-

ample 4.1, system viewed BCS. revision system? easy see
Idiag;1 satisfies REV2 REV3. clearly satisfy REV1, since propositions
describe input/output lines change values one point next. However,
show, slight variant Idiag;1 satisfy REV1. fundamental
problem Idiag;1 satisfy REV4. inherent assumption
agent never directly observes faults, that, example, Pldiag;1(R[; f1]) = ?,
Pldiag;1 (R[f1]) > ?. does, however, satisfy REV40 .
see modify Idiag;1 satisfy REV1, recall diagnosis task,
agent mainly interested beliefs faults. Since faults static Idiag;1 ,
satisfy REV1 ignore propositions except f1 ; : : :; fn . Let 0diag = ff1 ; : : :; fn g let
L0diag propositional language 0diag . every observation made agent
regarding value lines, corresponds formula L0diag characterizes
fault sets consistent o. Thus, every run r Idiag;1 , construct
0
run r0 agent's local state sequence formulas L0diag . Let Idiag
0
system consisting runs r . clearly put plausibility assignment
0 isomorphic obvious sense. particular, agent
runs Idiag;1 Idiag
beliefs formulas L0diag corresponding points two systems.
134

fiModeling Belief Dynamic Systems. Part II.

0 ; r; m) j= ' (Idiag;1; r; m) j= '
precisely, ' 2 L0diag , (Idiag
0 satisfies REV1{REV3 REV40,
points (r; m) Idiag;1. easy verify Idiag
although still satisfy REV4.
0 instead Idiag |Idiag seems us perfectly
advocating using Idiag
reasonable way modeling situation. Rather, point want BCS
satisfy properties validate AGM postulates, must make strong,
always natural, assumptions. ut

want show revision operator corresponds system C R vice
versa. so, need examine beliefs agent point (r; m). First
note (r; m) (r0; m0) (I ; r; m) j= B' (I ; r0; m0) j= B';
consequence requirement that, defined interpreted systems,
agent's plausibility assessment function local state. Thus, think agent's
beliefs function local state. use notation (I ; sa) j= B' shorthand
(I ; r; m) j= B' (r; m) ra(m) = sa . Let sa local state
agent. define agent's belief state sa
Bel(I ; sa) = f' 2 Le : (I ; sa) j= B'g:
Since agent's state sequence observations, agent's state observing '
simply sa ', append operation. Thus, Bel(I ; sa ') belief state
observing '. adopt convention agent never attain local state sa
, Bel(I ; sa) = Le . definitions, compare agent's belief state
observing ', Bel(I ; sa) Bel(I ; sa ').
start showing every AGM revision operator represented C R.

Theorem 5.2: Let AGM revision operator let K Le consistent belief
state. system I;K 2 C R Bel(I;K ; hi) = K
Bel(I;K ; hi) ' = Bel(I;K ; h'i)
' 2 Le .
Proof: See Appendix A.1. ut
Thus, Theorem 5.2 says represent revision operator sense
family systems I;K 2 C R , one consistent belief state K , K
agent's initial belief state I;K , formula ' Le , agent's belief state
learning ' K '. Notice restrict attention consistent belief states K .
AGM postulates allow agent \escape" inconsistent state, K '

may consistent even K inconsistent. might thus hope extend theorem
also applies inconsistent belief state, impossible framework.
false 2 Bel(I;K ; sa) state sa , ra(m) = sa , Pl(r;m) (W(r;m)) = ?. Since
update conditioning, must Pl(r;m+1)(W(r;m+1) ) = ?, agent's belief
state remain inconsistent matter learns. Although could modify
framework allow agent escape inconsistent states, actually consider
defect AGM postulates, framework. see why, suppose
135

fiFriedman & Halpern

agent's belief set inconsistent sa , ra(m) = sa . Thus, agent considers states
W(r;m) completely implausible (since Pl(r;m) (W(r;m)) = ?). hand,
escape inconsistency, must plausibility ordering worlds W(r;m).
two requirements seem somewhat inconsistent.9
surprisingly, inconsistency creates problems semantic representations
literature. example, Boutilier's representation theorem (1992) states
every revision operator belief state K , ranking R 2 K '
believed minimal '-worlds according R. examine theorem,
note state minimal (i.e., preferred) worlds R correspond
belief state K (in sense minimal worlds precisely
formulas K hold); would analogue requiring Bel(I;K ; hi) = K .
fact, K `L -consistent, minimal worlds correspond K . However, K
inconsistent, cannot, since nonempty ranking induces consistent set beliefs.
could state weaker version Theorem 5.2 would correspond exactly Boutilier's
theorem. presented stronger result (that apply inconsistent belief states)
bring believe problem AGM postulates. See (Friedman &
Halpern, 1998a) discussion issue.
Theorem 5.2 shows that, precise sense, map AGM revision operations
C R. direction? next theorem shows first belief change
step systems C R satisfies AGM postulates.
Theorem 5.3: Let system C R. AGM revision operator

Bel(I ; hi) ' = Bel(I ; h'i)
' 2 Le .
Proof: See Appendix A.1. ut
remark used REV40 instead REV4, would able prove
result formulas ' observable (i.e., Pl(R[']) > ?).
Theorems 5.2 5.3 apply one-step revision, starting initial (empty)
state. happens allow iterated revision? framework, observations
taken known, agent makes inconsistent sequence observations,
belief state inconsistent, (as observed above) remain inconsistent
on, matter observes. creates problem try get analogues
Theorems 5.2 5.3 iterated revision. following theorem demonstrates,
already see problem consider one-step revisions state initial
state.
Theorem 5.4: Let system C R let sa = h'1; : : :; 'ki local state .
AGM revision operator ;s
e



Bel(I ; sa) ;s ' = Bel(I ; sa ')


9. One strength AGM framework deal inconsistent sequence observations,
is, cope observation sequence form hp; :p; p; :p; : : :i. stress able
cope inconsistent sequence observations require allowing agent escape
inconsistent belief sets. two orthogonal issues.

136

fiModeling Belief Dynamic Systems. Part II.

formulas ' 2 Le '1 ^ : : : ^ 'k ^ ' consistent.

Proof: See Appendix A.1. ut

cannot better this. '1 ^ : : : ^ 'k ^ ' inconsistent then,
requirements observations must true current state environment
(BCS4) propositions static (REV1), cannot global state
agent's local state sa '. Thus, Bel(I ; sa ') inconsistent, contradicting R5.
another problem trying get analogue Theorem 5.3 iterated
revision, problem seems inherent AGM framework. framework makes
clear distinction agent's epistemic state point (r; m) ,
identify local state sa = ra(m), agent's belief set (r; m), Bel(I ; sa),
set formulas believes. system C R, agent's belief set
general determine agent's beliefs revised; epistemic state does.
hand, AGM postulates assume revision function agent's
belief set observations. suppose system two points (r; m)
(r; m0) run r 2 (1) agent's belief set (r; m)
(r; m0), Bel(I ; ra(m)) = Bel(I ; ra(m0)), (2) agent observes ' (r; m)
(r; m0), (3) Bel(I ; ra(m + 1)) 6= Bel(I ; ra(m0 + 1). hard construct system
. However, cannot analogue Theorem 5.3 , even restrict
consistent sequences observations. suppose revision operator
Bel(I ; hi)) '1 'k = Bel(I ; h'1; : : :; 'k i) '1 ; : : :; 'k '1 ^ : : : ^ 'k
consistent. would Bel(I ; ra(m +1)) = Bel(I ; ra(m)) ' = Bel(I ; ra(m0)) ' =
Bel(I ; ra(m0 + 1)), contradicting assumption.
culprit assumption revision depends agent's belief set.
see unreasonable assumption, consider situation time 0
agent believes p q , belief q stronger belief p (i.e.,
plausibility q greater p). well imagine observing :p _:q
time 1, would believe :p q . However, first observed p time 1
:p _:q time 2, would believe p :q, because, result observing p, would
assign p greater plausibility q . Note, however, AGM postulates dictate
observation already believed, agent change beliefs. Thus,
AGM setup would force agent beliefs learning :p _ :q
situations.
great deal work problem iterated belief revision (Boutilier,
1996a; Darwiche & Pearl, 1997; Freund & Lehmann, 1994; Lehmann, 1995; Levi, 1988;
Nayak, 1994; Williams, 1994)). Much recent work moves away assumption
belief revision depends solely agent's belief set. example approaches
Boutilier (1996a) Darwiche Pearl (1997) define revision operators map
(rankings formulas) rankings. framework makes clear distinction
epistemic states belief states, gives us natural way maintaining
spirit AGM postulates assuming revision function epistemic states.
Rather taking function (belief states formulas) belief states,
take function (epistemic states formulas) epistemic states.
leaves open question represent epistemic states. Boutilier Darwiche Pearl use rankings represent epistemic states. framework, represent
137

fiFriedman & Halpern

epistemic states local states interpreted systems. is, pair (I ; sa) denotes
agent's state interpreted system, pair determines agent's relevant epistemic attitudes, beliefs, beliefs changed given particular observations,
plausibility assessment runs, on. system understood, simply
use sa shorthand representation epistemic state.
easily modify AGM postulates deal revision operators
epistemic states. start assuming set epistemic states function
Bel() maps epistemic states belief states. analogues
AGM postulates, obtained replacing belief set beliefs corresponding
epistemic state. example,

(R10) E ' epistemic state
(R20) ' 2 Bel(E ')
(R30) Bel(E ') Cl(Bel(E ) [ f'g)
on, obvious transformation.10
get strong representation theorems work level epistemic states.
Given language Le (with associated consequence relation `L ), let EL consist
finite sequences formulas Le . Note allow EL include sequences formulas
whose conjunction inconsistent. define revision EL obvious way: E 2 EL ,
E ' = E '.
e

e

e

e

e

Theorem 5.5: Let system C R whose local states EL . function

BelI maps epistemic states belief states

e

sa local state agent , Bel(I ; sa) = BelI (sa),
(; BelI ) satisfies R10{R80 .
Proof: Roughly speaking, define BelI (sa) = Bel(I ; sa) sa local state .
sa , set BelI (sa) = Bel(I ; s0), s0 longest consistent sux
sa. See Appendix A.1 details. ut
Notice that, definition, BelI (I ; hiI '1 : : : 'k ) = BelI (I ; h'1; : : :; 'k i),

so, level epistemic states, get analogue Theorem 5.3. remark
ensure R50 holds (; BelI ), need define BelI (E ) appropriately sequences
E 2 EI whose conjunction inconsistent.
Theorem 5.5 shows system C R corresponds revision operator
epistemic states satisfies generalized AGM postulates. would hope
converse also holds. Unfortunately, quite case. revision operators
epistemic states satisfy generalized AGM postulates correspond
system C R. systems C R satisfy additional postulate:

(R90) 6`L :(' ^ ) Bel(E ' ) = Bel(E ' ^ ).
e

10. problematic postulate R6. question whether R60 \If `Le ' ,
Bel(E ') = Bel(E )" \If `Le ' , E ' = E ". Dealing either version
straightforward. definiteness, adopt first alternative here.

138

fiModeling Belief Dynamic Systems. Part II.

show R90 sound C R proving following strengthening Theorem 5.5.
Proposition 5.6: Let system C R whose local states EL . function
BelI maps epistemic states belief states
sa local state agent , Bel(I ; sa) = BelI (sa),
(; BelI ) satisfies R10{R90 .
Proof: show function BelI defined proof Theorem 5.5 satisfies R90.
See Appendix A.1 details. ut
prove converse Proposition 5.6: revision system epistemic states
satisfies generalized AGM postulates R90 correspond system C R .
Theorem 5.7: Given function BelL mapping epistemic states EL belief sets
Le BelL (hi) consistent (BelL ; ) satisfies R10{R90, system 2 C R
whose local states EL BelL (sa) = Bel(sa ) local state sa .
Proof: According Theorem 5.2, system Bel(I ; hi) = BelL (hi)
Bel(I ; h'i) = BelL (h'i) ' 2 Le . show Bel(I ; sa) = BelL (sa ) local
states sa . See Appendix A.1. ut
Notice that, definition, system Theorem 5.7, Bel(hi'1: : :'k ) =
Bel(h'1; : : :; 'k i) long '1 ^ : : : ^ 'k consistent.
e

e

e

e

e

e

e

e

e

e

6. Capturing Update

Update tries capture intuition preference runs
observations made true, changes one point next along run
minimized.
start reviewing Katsuno Mendelzon's semantic representation update.
characterize agent beliefs, Katsuno Mendelzon consider set \worlds" agent
considers possible. representation, associate world truth assignment
primitive propositions. (In terminology, think world environment
state.) capture notion \minimal change world world", Katsuno
Mendelzon use distance function worlds. Given two worlds w w0 , d(w; w0)
measures distance them. Intuitively, larger distance, larger
change required get world w w0 . (Note distances necessarily
symmetric, is, might require smaller change get w w0, w0
w.) Distances might incomparable, require map pairs worlds
partially ordered domain unique minimal element 0 d(w; w0) = 0
w = w0.
Katsuno Mendelzon show close relationship update operators
distance functions. make relationship precise, need introduce
definitions. update structure tuple U = (W; d; ), W finite set worlds,
distance function W , mapping worlds truth assignments Le

(w) `L consistent,
e

139

fiFriedman & Halpern

6`L :', w 2 W (w)(') = true,
w 6= w0 (w) 6= (w0) w; w0 2 W .
Given update structure U = (W; d; ), define [ '] U = fw : (w)(') = trueg. Kate

suno Mendelzon use update structures semantic representations update operators.
Given update structure U = (W; d; ) sets A; B W , Katsuno Mendelzon define minU (A; B ) set worlds B closest worlds A, according
d. Formally, minU (A; B ) = fw 2 B : 9w0 2 A8w0 2 B d(w0; w0) 6< d(w0; w)g.

Theorem 6.1: (Katsuno & Mendelzon, 1991b) belief change operator satisfies U1{U8
update structure U = (W; ; d)
[ ' ] U = minU ([['] U ; [ ] U ):

Thus worlds agent believes possible updating worlds
closest world considered possible learning .
Katsuno Mendelzon's account update \static" sense describes
single belief change. Nevertheless, clear intuition world w0 2 [ ' ] U
result considering minimal change world w 2 [ '] U . However,
Katsuno Mendelzon's representation, keep track worlds \lead to"
worlds current belief set.
try capture behavior similar Katsuno Mendelzon's semantics
framework. define systems run describes sequence changes,
plausible runs, given set observations, correspond worlds define belief
set Katsuno Mendelzon's semantics. precisely, given sequence observations
1; : : :; n, world [ ' 1 : : : n ] U \traced" back series minimal
changes world [ '] U . model, trace corresponds one
plausible runs, environment state time mth world trace.
capture intuition using family priors particular form.
start preliminary definitions. Let BCS, let s0 ; : : :; sn
set environment states . define [s0; : : :; sn ] set runs (i) = si
0 n. Thus, [s0 ; : : :; sn ] describes set runs share common prefix
environment states. prior plausibility space Pa = (R; Pla ) consistent distance
measure following holds:
Pla ([s0; : : :; sn ]) < Pla ([s00; : : :; s0n ]) j < n
sk = s0k 0 k j , sj+1 6= s0j +1, d(sj ; sj +1) < d(sj ; s0j+1).
Intuitively, compare events form [s0 ; : : :; sn ] using lexicographic ordering based
d. Notice ordering focuses first point difference. Runs smaller
change point preferred, even later abnormal changes. point
emphasized borrowed car example below.
Pla prefix-defined plausibility event uniquely defined plausibility
run-prefixes contained it,
Pla (R['0; : : :; 'm]) Pla (R[ 0; : : :; m]) [s0 ; : : :; sm ]
R[ 0; : : :; m] , R['0; : : :; 'm] [s00; : : :; s0m] R['0; : : :; 'm]
Pla([s00 ; : : :; s0m]) > Pla ([s0; : : :; sm ]).
140

fiModeling Belief Dynamic Systems. Part II.

Roughly speaking, requirement states compare events properties dominance. property similar one satisfied plausibility measures get
preference ordering using construction Proposition 2.2.
define set C U consist BCSs = (R; ; P ) satisfy following four
requirements UPD1{UPD4. UPD1 says finitely many possible truth
assignments, one-to-one map environment states truth
assignments.
UPD1 set e propositions (of BCS1) finite
environment states s, s0 , 6= s0 , formula ' 2 Le
j= ' s0 j= :'.
UPD2{UPD4 analogues REV2{REV4. Like REV2, UPD2 puts constraints
form prior, consider lexicographic priors form described above.
UPD2 prior BCS5 prefix defined consistent distance
measure.
Recall REV3 requires truth assignments initially nontrivial plausibility. case revision, truth assignment change time, since
dealing static propositions. case update, truth assignment may
change time, UPD3 requires consistent sequences truth assignments
nontrivial plausibility.
UPD3 'i 2 Le , = 0; : : :; k, consistent formulas, Pl(R['0; : : :; 'k]) >
?.
Finally, like REV4, UPD4 requires agent gain information observations beyond fact true.
UPD4 Pla(R['0; : : :; 'k+1; o1; : : :; ok]) Pla(R[ 0; : : :; m+1; o1; : : :; om])
Pla (R['0; '1 ^ o1 ; : : :; 'm ^ om ; 'm+1 ]) Pla (R[ 0; 1 ^ o1 ; : : :; ^
om; m+1])
remark presence REV1, UPD4 equivalent REV4. might consider
generalized versions UPD4, two sequences formulas arbitrary
relative lengths; version suces purposes. also define analogue
UPD40 spirit REV40 , applies Pl(R['0; : : :; 'm+1; o1; : : :; om ]) > ?.
show C U corresponds (KM) update. Recall Katsuno Mendelzon
define update operator mapping pair formulas (; '), describes agent's
beliefs ' describes observation, new formula ' describes agent's
new beliefs. However, discussed Section 3, e finite, also treat
mapping belief state formula new belief state. Also recall Bel(I ; sa)
agent's belief set local state sa .
Theorem 6.2: belief change operator satisfies U1{U8 system
2 C U
Bel(I ; sa) = Bel(I ; sa )
epistemic states sa formulas 2 Le .
141

fiFriedman & Halpern

Proof: Roughly speaking, show system C U corresponds Katsuno
Mendelzon update structure. Suppose =2 C U set environment
states Se prior BCS5 consistent distance function d. define
update structure UI . show belief change corresponds belief change

UI sense Theorem 6.1. Since Theorem 6.1 states belief change operation
defined update structure satisfies U1{U8, suce prove \if" direction
theorem. prove \only if" direction theorem, show
update structure U , system 2 C U UI = U .
See Appendix A.2 details. ut
result immediately generalizes sequences updates.

Corollary 6.3: belief change operator satisfies U1{U8 system
2 C U 1; : : :; k 2 Le ,
Bel(I; sa ) 1 : : : k = Bel(I; sa 1 : : : k ):
results show update, unlike revision, systems consider
belief state determine result update, i.e., Bel(I ; sa ) = Bel(I ; s0a),
' get Bel(I ; sa ') = Bel(I ; s0a '). Roughly speaking, reason
distance measure determines prior change time. allows
us get elegant representation theorem, also causes problems applicability
update, shall see below.
Note that, since world allowed change, problem update
sequence 1; : : :; k consistent formulas 1 ^ : : : ^ k inconsistent.
requirement formulas 1; : : :; k true simultaneously. matters
true time i. Also note update inconsistent formula pose
problem framework. follows postulates U1 U2 agent
learns inconsistent formula (i.e., false), believes false on.
reasonable notion update? discussion UPD2 suggests,
preference deferring abnormal events. makes quite similar Shoham's
chronological ignorance (1988), suffers problems. Consider
following story, call borrowed-car example.11 time 1, agent parks
car front house full fuel tank. time 2, house. time 3,
returns outside find car still parked left it. Since agent
observe car inside house, reason revise beliefs
regarding car's location. Since finds parked time 3, still reason
change beliefs. Now, agent believe when, time 4, notices
fuel tank longer full? agent may want consider number possible
explanations time-4 observation, depending considers
likely sequence(s) events time 1 time 4. example, previous
gas leaks, may consider leakage plausible explanation.
hand, spouse also car keys, may consider possible used car
absence. Update, however, prefers defer abnormalities, conclude
11. example based Kautz's stolen car story (1986), due Boutilier, independently
observed problem [private communication, 1993].

142

fiModeling Belief Dynamic Systems. Part II.

fuel must disappeared, inexplicable reasons, times 3 4. see this,
note runs car taken ride abnormality time 2,
runs car move time 2 fuel suddenly disappeared,
first abnormality time 4, thus preferred!
Suppose formalize example using propositions car-parked-outside, fueltank-full, etc. Let agent's belief set time , = 1; : : :; 4. Notice 1 includes
belief car parked front house full fuel tank. (That is,
`L 1 ) fuel-tank-full ^ car-parked-outside.) time 2 agent makes observations
since house, 2 = 1 true = 1 U2. time 3 agent observes
car outside house, 3 = 2 car-parked-outside = 1 , U2. Finally,
4 = 3 :fuel-tank-full. observation :fuel-tank-full time 4 must explained
means. semantics, answer clear. plausible runs
car parked time 3, somewhere time 3 4 change
occurred.
counterintuitive conclusion artifact representation? extent
is. issue cannot formally addressed within Katsuno Mendelzon's semantic
framework, since framework provide account sequences changes.
Moreover, one might argue within framework might families priors
satisfy U1{U8, offer alternative explanations surprising observation
time 4. Nevertheless, claim semantics captures, believe
straightforward way, intuition embedded Katsuno Mendelzon's
representation. particular, condition UPD2, enforces delay abnormal events,
needed order capture \pointwise" nature update. would interesting
know whether natural way capturing update framework
suffer problems.
way capturing update semantically ever lead reasonable results?
course, depends interpret \reasonable". brie consider one approach
here.
world w, agent beliefs described by, say, formula '.
beliefs may may correct (where say belief ' correct world w ' true
w). Suppose something happens world changes w0 . result agent's
observations, new beliefs, described '0 . Again, reason believe
'0 correct. Indeed, may quite unreasonable expect '0 correct, even
' correct. Consider borrowed-car example. Suppose agent sitting
inside house, car was, fact, taken ride. Nevertheless, reasonable
belief agent hold observes car still parked
leaves house along.
problem information agent obtains times 2 3 insucient
determine happened. cannot expect agent's beliefs correct
point. hand, obtain sucient information change
beliefs initially correct, seems reasonable expect new beliefs
correct. counts sucient information?
say ' provides sucient information change w w0
world w00 satisfying ' d(w; w00) < d(w; w0). words, ' sucient
information if, observing ' world w, agent consider real world (w0) one
e

143

fiFriedman & Halpern

likely worlds. Note definition monotonic, ' sucient
information change, formula implies ' (as long holds
w0). Moreover, definition depends agent's distance function d. constitutes
sucient information one agent might another. would hope function
realistic sense worlds judged closest according really
likely occur.
show update property agent correct beliefs
receives sucient information change, continue correct beliefs.

Theorem 6.4: Let 2 C U . agent's beliefs (r; m) correct o(r;m) provides

sucient information change (m) (m + 1), agent's beliefs
(r; + 1) correct.

Proof: Straightforward; left reader. ut
observed earlier, cannot expect agent always correct beliefs. Nevertheless, might hope agent (eventually) receive suciently detailed
information, realize beliefs incorrect. precisely
happen borrowed-car example. Intuitively, agent observes
fuel tank full, sucient information eliminate possibility car remained parking lot. However, not. Roughly speaking,
update focuses current state world, thus cannot go back
revise beliefs past.
problem due fact belief update determined
agent's belief state epistemic state. Thus, update take account
agent's current beliefs information, sequence observations
led beliefs. example, limit attention beliefs car's
whereabouts fuel tank, since agent belief state time 1
3, must change beliefs manner times. implies
observation fuel tank full time 4 cannot sucient information
past, since fuel leak might plausible explanation missing fuel time 2.12
discussion update shows update guaranteed safe situations
always enough information characterize change occurred.
may plausible assumption database applications, seems somewhat less
reasonable AI examples, particularly cases involving reasoning action.13

7. Synthesis

previous sections analyzed belief revision belief update separately. provided
representation theorems notions discussed issues specific notion.
section, try identify common themes points difference.
12. example usual intuition that, given observation tank full, agent
revise belief manner instead performing update. immediately raises
question agent knows right belief change operation here. return
issue below.
13. Similar observations independently made Boutilier (1996b), although representation quite
different ours.

144

fiModeling Belief Dynamic Systems. Part II.

Restriction
Environment changes
Initial plausibility
Belief change

Revision
change
(Static propositions)
Total preorder
Conditioning

Update
possible sequences
Lexicographic
Conditioning

Table 1: summary restrictions impose capture revision update.
Katsuno Mendelzon (1991a) focused following three differences AGM
revision KM update:
1. Revision deals static propositions, update allows propositions
static.
2. Revision update treat inconsistent belief states differently. Revision allows
agent \recover" inconsistent state observing consistent formula.
Update dictates agent inconsistent beliefs, continue
inconsistent beliefs. noted above, seems revision's ability recover
inconsistent belief set leads several technical anomalies iterated revision.
3. Revision considers total preorders, update allows partial preorders.
framework suggests different approach categorizing differences
revision update (and approaches belief change): focusing restrictions
added basic BCSs obtain systems C R C U , respectively.
particular, focus three aspects system:

environment state change?
agent form initial beliefs? regularities appear agent's

beliefs initial state?
agent change beliefs?

Table 1 summarizes answers questions revision update; highlights
different restrictions imposed each. Revision puts severe restriction changes
environment (more precisely, describe environment language)
rather mild restriction agent's prior beliefs (they must form total preorder).
hand, update allows sequences environment states, requires
agent's prior beliefs specific form. formal properties match intuitive
description revision update given (Alchourron et al., 1985; Katsuno & Mendelzon,
1991b). However, explicit representation time framework allows us make
intuitions precise. Moreover, framework makes explicit assumptions made
revision update. example, lexicographic nature update immediately
evident presentation (Katsuno & Mendelzon, 1991b).
145

fiFriedman & Halpern

key point notice table belief change revision update
done conditioning. observation, naturalness conditioning notion
change, support claim conditioning adopted semantic foundations
minimal change.
significant differences revision update? claim
differences result different ways modeling underlying process.
Recall introduction noted restriction static propositions
serious limitation belief revision, since always convert dynamic proposition
static one adding timestamps. precisely, replace proposition p
family propositions pm stand \p true time m". makes possible
use revision reason changing world. show revision update
related viewpoint.
make discussion precise, need introduce formal definitions. Let
= (R; ; P ) BCS. \statify" system = (R; ; P ) replacing
underlying language static propositions.
Let e = fpm : p 2 e ; 2 N g set timestamped propositions let Le
logical language based propositions. easily \timestamp" every formula L.
define timestamp('; m) recursively follows. base case timestamp(p; m) = pm
p 2 e . standard logical connectives, simply apply transformation recursively,
example timestamp(' ^ ) = timestamp('; m) ^ timestamp( ; m).
Next, define set runs \statified" system. run r 2 R,
define r R follows. environment states r defined whole
sequence environment states r, is, re(m) = . ra(m) = ho(r;1); : : :; o(r;m)i,
define ra(m) = htimestamp(o(r;1); 1); : : :; timestamp(o(r;m); m)i. define interpretation obvious way: (r; m)(pm0 ) = true (r; m0)(p) = true
(r; m)(learn(')) = true o(r ;m) = '.
Finally, need define prior plausibility Pla . define prior isomorphic
Pla transformation r 7! r. is, set runs R R, define
Pla(R ) = Pla (fr 2 R : r 2 R g).
clear two systems describe underlying process. Perhaps
significant difference environment state run encodes
future run. necessary environment state could determine
truth propositions form pm , satisfy BCS1. Without requirement,
could simply changed left R P unchanged.
different base languages used , agent different beliefs
two systems. easy show that, ' 2 Le , (I ; r; m) j= B' iff
(I ; r; m) j= B (timestamp('; m)). However, (r; m) agent also beliefs
propositions describe past future times. Thus, set beliefs agent
viewed superset beliefs corresponding points.
following result makes precise relationship terms
properties considering.
Proposition 7.1: Let BCS let transformed system defined above.
BCS, is, satisfies BCS1{BCS5.
satisfies REV1.
146

fiModeling Belief Dynamic Systems. Part II.

satisfies UPD3, satisfies REV3.
satisfies UPD4, satisfies REV40.

Proof: Straightforward; left reader. ut
Thus, BCS, . Moreover, 2 C U , satisfies two
requirements C R. First, necessarily satisfy REV2, since prior systems
C U is, general, ranked. Second, satisfies REV40, weaker version REV4.
reason runs allow sequences possible observations.
Remember language Le , agent observe proposition p2 (i.e.,
p true time 2) time 1. However, original system, agent observes
properties current time. Thus, o(r ;m) involves propositions deal time
m.

Neither shortcomings serious. First, variants AGM revision involve
partial orders discussed literature (Katsuno & Mendelzon, 1991b; Rott, 1992).
fairly straightforward show captured systems using BCSs
satisfy REV1, REV3, REV4. Second, easy add runs get system
satisfies REV4. Moreover, way change agent's
beliefs sequences observations observed . Thus, \statified" version
system C U displays behavior much spirit belief revision.
result may seem somewhat surprising light significant differences
AGM postulates KM postulates. part, shows much bound
choice language. (Recall similar issues arose Example 5.1.) highlights
sensitivity postulate approach modeling assumptions make. Unfortunately,
modeling assumptions rarely discussed belief change literature. (See (Friedman & Halpern, 1998a) detailed discussion point.)
Table 1 emphasizes that, despite well-known differences revision update,
viewed sharing one important feature: use conditioning
belief change. Thus, common mechanism understanding extending
them. certain extent, results show revision general update,
sense view statified version system C U performing revision
(possibly unranked prior) runs.

8. Extensions
preceding sections, introduced several assumptions needed capture
revision update. course, ways capturing notions require
somewhat different assumptions. Nevertheless, assumptions give insight underlying choices made, either explicitly implicitly, definition revision update.
addition, thinking terms restrictions makes straightforward extend
intuitions revision update beyond context originally applied.
section, consider number extensions, illustrate point.
147

fiFriedman & Halpern

8.1 Knowledge
many domains interest, agent knows sequences observations
impossible. already saw circuit-diagnosis problem observing failures
impossible. context update, know cannot observe person die
alive, despite fact dead alive consistent states.
easily maintain regard defining properties revision update,
discussed previous section: change environment state ranked prior
case revision, lexicographic prior case update, belief change
proceeding conditioning cases. simply drop REV3 replace REV4
REV40 (resp., drop UPD3 replace UPD4 UPD40 ). remark change
affects postulates. example, consider update. Suppose agent considers
possibility Mr. Bond dead. observes Mr. Bond alive well then,
according update, must account new observation change
worlds previously considered possible. However, transition worlds
Mr. Bond dead account new observation. Thus,
agent knows certain transitions impossible, observations (e.g., observing
Mr. Bond alive) require remove consideration worlds
previously considered possible. consequence, postulate U8 hold, since
agent's new beliefs determined pointwise update worlds
previously considered possible. (Boutilier (1998) uses related semantic framework
draw similar conclusions analysis update.)

8.2 Language Beliefs
analysis revision update, focused agent's beliefs current
state environment. Often also interested agent changes beliefs
types statements, beliefs future states environment,
beliefs agents' beliefs, introspective beliefs beliefs. Again,
straightforward framework deal enriched language lets us express statements. example, (Friedman & Halpern, 1994) examine Ramsey
conditionals. formulas form ' > , read saying \after
learning ', agent believes ". formula expressed learn(') ) B
language LKPT . well known, belief sets include Ramsey conditionals (and
propositional formulas), AGM postulates become inconsistent (at least, provided
least three mutually exclusive consistent formulas language) (Gardenfors,
1986). Similar inconsistency results arise one tries add forms introspective
beliefs (Fuhrmann, 1989). setting, easy see problem arises. Even
allow belief sets include nonpropositional formulas, still seems quite clear
want distinguish propositional formulas formulas talk explicitly
agent's beliefs. example, clear allow observation
formula ' > . would mean observe formula? clearly seems
quite different observing propositional formula. make sense extend
assumption REV1 arbitrary formulas. may reasonable restrict
static propositions viewing making statements relatively stable
148

fiModeling Belief Dynamic Systems. Part II.

environment, seems far less reasonable assume formulas talk agent's
beliefs static, especially trying model belief change!
course, allow propositional formulas learned (or observed),
restrict REV1 propositional formulas, easy see results still
hold, even full language quite rich; avoid triviality result completely.

8.3 Observations
One strongest assumptions made revision update involves treatment
observations. assumption seems unreasonable domains. REV4 UPD4
essentially assume observation agent makes chosen randomly among
formulas consistent current state world. Suppose ' says
agent outdoors, says agent basement, o1 says basement
light on. may well Pla (R[' ^ o1 ]) > Pla (R[ ^ o1 ]). example, agent
may hardly ever go basement frequently go outdoors, children may often
leave basement light on. Nevertheless, may also Pla (R['; o1]) < Pla (R[ ; o1]),
contradicting REV4. Indeed, may well impossible agent observe
basement light outdoors, Pla (R['; o1]) = ?,
permitted according REV4 UPD4.
many domains useful reason hidden quantities simply cannot
observed. example, event component ci faulty Example 5.1 basic
event description problem, yet cannot observed. Similarly, event
patient disease X opponent planning capture queen useful
reasoning medical diagnosis game strategy, yet directly observable
practice. Thus, requirement formulas language observed seems
quite unnatural. note explicitly modeling sensory input standard practice
control theory stochastic processes (e.g., hidden Markov chains). fields,
one models probability observation various situations. Making observation
increases probability situations observation likely observation
decreases probability situations unlikely. Again, straightforward
consider detailed model observation process framework; see (Friedman,
1997, Chapter 6) (Boutilier et al., 1998).

8.4 Actions
definition belief change systems essentially assumes agent passive.
situation complex agent uence environment. agent's choice
action interacts beliefs. clear performing action, agent
change beliefs.14 Moreover, information content observations depends
action agent performed. example, agent might consider hearing
loud noise surprising. However, would expected agent pulls trigger
gun.
14. Indeed, alternative interpretation update postulates describe agent
update beliefs action \achieve '" (Goldszmidt & Pearl, 1996; del Val & Shoham, 1992,
1993). However, works show, update postulates problematic interpretation.

149

fiFriedman & Halpern

8.5 Summary

list possible extensions clearly exhaustive; many others
may want consider. Nevertheless, extensions seem interest.
main points want make (1) easy accommodate extensions
framework still maintaining main characteristics revision update,
(2) dicult deal extensions focus postulates.

9. Conclusion

shown framework introduced (Friedman & Halpern, 1997) used
capture belief revision update. Modeling revision update framework also
gives us great deal insight properties, emphasizes role
conditioning way capturing minimal change.
course, revision update two points wide spectrum possible types
belief change. ultimate goal use framework understand whole spectrum
better help us design belief change operations overcome diculties
observed revision update. particular, want belief change operations
handle dynamic propositions, still able revise information
past.
framework suggests construct belief change operations. framework, belief change operations determined choosing plausibility measure
captures agent's preferences among sequences worlds. agent's prior plausibility, captures initial beliefs relative likelihood runs. agent
receives information, changes beliefs using conditioning. paper show
revision update correspond two specific families priors. Clearly, however,
prior plausibilities that, conditioned surprising observation, allow agent
revise earlier beliefs assume change occurred. One obvious
problem that, even two possible states, uncountably many
possible runs. agent describe prior plausibility complex space?
One approach based intuition probabilistic settings.
settings, standard solution problem assume state transitions
independent occur, is, probability system going state
state s0 independent sequence transitions brought system state
s. Markov assumption significantly reduces complexity problem.
necessary describe probability state transitions. (Friedman & Halpern,
1996; Friedman, 1997) define notion plausibilistic independence, show
describe priors satisfy Markov assumption consequences belief change.
See also (Boutilier, 1998; Boutilier et al., 1998) recent proposals along lines.
Whether particular approach turns useful one, clear
types questions asking. works show, framework
provides useful basis answering them.
Finally, note approach quite different traditional approach
belief change (Alchourron et al., 1985; Gardenfors, 1988; Katsuno & Mendelzon, 1991a).
Traditionally, belief change viewed abstract process. framework,
hand, models agent environment situated in, change time.
150

fiModeling Belief Dynamic Systems. Part II.

allows us model concrete agents concrete settings (for example, diagnostic systems
analyzed (Friedman & Halpern, 1997) throughout paper), reason
beliefs knowledge agents. investigate plausibility
ordering induces beliefs match intuitions. gaining better understanding
concrete situations, better investigate abstract notions belief change.
generally, believe that, studying belief change, important specify
underlying ontology: is, exactly scenario underlies belief-change process.
specified one scenario here. others certainly possible, view
defect literature belief change underlying scenario rarely discussed.
framework introduced provides way making formal scenario
is. (See (Friedman & Halpern, 1998a) discussion issue.)

Acknowledgments
authors grateful Craig Boutilier, Ronen Brafman, Adnan Darwiche, Moises Goldszmidt, Adam Grove, Alberto Mendelzon, Alvaro del Val, particularly Daphne Koller
Moshe Vardi, comments drafts paper useful discussions relating
work. work done authors IBM Almaden Research Center. first author also Stanford much work done. IBM
Stanford's support gratefully acknowledged. work also supported part
Air Force Oce Scientific Research (AFSC), Contract F49620-91-C-0080
grant F94620-96-1-0323 NSF grants IRI-95-03109 IRI-96-25901.
first author also supported part IBM Graduate Fellowship Rockwell
Science Center. preliminary version paper appears J. Doyle, E. Sandewall,
P. Torasso (Eds.), Principles Knowledge Representation Reasoning: Proc. Fourth International Conference , 1994, pp. 190{201, title \A knowledge-based framework
belief change, Part II: revision update."

Appendix A. Proofs

A.1 Proofs Section 5

start proof Theorems 5.2 5.3. this, need preliminary
definitions lemmas. Figure 1 shows general outline intermediate representations use proofs. Roughly speaking, show map revision
operator consistent belief set K ranking, similarly map
ranking AGM revision operator. rankings correspond, direct way, priors
systems C R , thus close connection beliefs agent various states.
mapping AGM revision operators rankings related representation theorems Boutilier (1994b), Grove (1988), Katsuno Mendelzon (1991a).
However, exact details representations different Boutilier, Grove,
Katsuno Mendelzon. Thus, completeness provide full proofs here.
start mapping revision operator applied specific belief set
ranking. intermediate step construct set defaults follows. use
results (Friedman & Halpern, 1998b) construct ranked plausibility structure
satisfies defaults.
151

fiFriedman & Halpern

Lemma A.1

AGM
Revision
K;



Set
Defaults

X
X





X
X

Lemma A.2

X
X



Ranked
Structure

Lemma A.3
C
C

C
C

Bel(I a)
;s



X
X

Lemma A.4

X
X



Characteristic
Structure
PLI

Figure 1: Schematic description entities lemmas involved proof Theorems 5.2 5.3.

Lemma A.1: Let AGM revision operator, let K Le consistent belief set,

let

!

(;K ) = f'
following true:
(a)
(b)
(c)

: '; 2 Le ; 2 K 'g:

(;K ) closed rules system P,
' false 62 (;K) consistent ' 2 Le ,
(;K ) satisfies rational monotonicity; is, '
' ^
2 (;K).

!

!

!

2 (;K) '!: 62 (;K),

Proof: start part (a):
LLE Assume `L ' '0 '! 2 (;K ). Thus, 2 K '. R5,
follows 2 K '0, thus '0 ! 2 (;K ).
RW Assume `L ) 0 '! 2 (;K ). Thus, 2 K '. Since K '
belief set, closed logical consequence. particular, 0 2 K ', hence
'! 0 2 (;K).
REF R2, ' 2 K ', thus, '!' 2 (;K ).
Assume '! 1; '! 2 2 (;K ). Thus, 1; 2 2 K '. Since K ' belief
set, 1 ^ 2 2 K '. Thus, '! 1 ^ 2 2 (;K ).
Assume '1 ! ; '2! 2 (;K ). two cases. K ('1 _ '2 )
inconsistent, 2 K ('1 _ '2) thus '1 _ '2 ! 2 (;K ). K ('1 _ '2)
e

e

152

fiModeling Belief Dynamic Systems. Part II.

consistent, then, R2, '1 _ '2 2 K ('1 _ '2 ). Thus, cannot :'1
:'2 K ('1 _ '2). Without loss generality, assume :'1 62 K ('1 _ '2).
Using R7 R8, get K (('1 _ '2 ) ^ '1) = Cl(K ('1 _ '2 ) [ f'1g).
Using R6, get K (('1 _ '2) ^ '1) = K '1. Thus, conclude
K '1 = Cl(K ('1 _ '2 ) [ f'1g). Since '1 ! 2 (;K ), 2 K '1.
Thus, get '1 ) 2 K ('1 _ '2 ). :'2 62 K ('1 _ '2), similar arguments
get '2 ) 2 K ('1 _ '2 ). implies ('1 _ '2) ) 2 K ('1 _ '2 ),
thus 2 K ('1 _ '2). hand, :'2 2 K ('1 _ '2 ), then, since
'1 _ '2 2 K ('1 _ '2), get '1 2 K ('1 _ '2), thus 2 K ('1 _ '2).
CM Assume '! 1 ; '! 2 2 (;K ). K ' inconsistent, using R5 get
' inconsistent. Thus, ' ^ 1 inconsistent, 2 2 K (' ^ 1 ). assume
K ' consistent. Since '! 1, 1 2 K '. Since K '
consistent, get : 1 62 K '. Applying R8, get K ' K (' ^ 1 ).
Since '! 2 2 (;K ), 2 2 K '. Thus, 2 2 K (' ^ 1 ).
implies (' ^ 1 )! 2 2 (;K ).
prove part (b). Let ' 2 Le consistent formula. Then, using R5, get
K ' consistent. Thus, '!false 62 (;K ).
Finally prove part (c). Assume '! 2 (;K ), ' ^ ! 62 (;K ). Since
'! 2 (;K), 2 K '. : 62 K ', then, using R8,
Cl(K ' [fg) K (' ^ ). implies 2 K (' ^ ). However, since assumed
' ^ ! 62 (;K ), 62 K (' ^ ); thus, get contradiction.
conclude : 2 K '. Thus, '!: 2 (;K ). ut

use result show exists plausibility structure corresponds
applied K .
Lemma A.2: Let AGM revision operator, let K Le consistent belief set.
plausibility structure PL = (W; Pl; ) Pl ranked, PL j= '

2 K ', Pl([['] ) > ? `L -consistent formulas ' 2 Le .
Proof: use basic techniques described proof (Friedman & Halpern, 1998b,
Theorem 8.2). Let (;K ) set defaults defined Lemma A.1. construct
plausibility space PL0 = (W; Pl0 ; ) PL0 j= '
'
2 (;K).
define PL0 follows:
W = fwV : V Le maximal `L -consistent setg,
(wV )(p) = true p 2 V ,
Pl0([['] ) Pl0([[ ] ) (' _ ) ' 2 (;K).
Using (Friedman & Halpern, 1998b, Lemma 4.1), get PL0 j= '

' 2 (;K). Lemma A.1 (c) results (Friedman & Halpern, 1998b),
follows ranked plausibility measure Pl default-isomorphic Pl0 ,
(W; Pl; ) satisfies precisely defaults (W; Pl0; ). Let PL = (W; Pl; ).
Since PL default-isomorphic PL0, PL j= '
'
2
(;K ). Moreover, using Lemma A.1, '
2 (;K) 2 K '.
Thus, PL j= '
2 K '. Finally, let ' `L -consistent formula.

!

e

!

!

e

!

!

!

!

!

!

e

153

!

fiFriedman & Halpern

!

Lemma A.1 (b), get ' false 62 (;K ). Since (;K ) closed rules
system P, conclude (' _ false) false 62 (;K ). Thus, Pl0 ([['] ) 6 ? = Pl0([[false] ),
thus Pl0 ([['] ) > ?. Since Pl default-isomorphic Pl0 , conclude Pl([['] ) > ?.

!

ut

prove converse Lemma A.2.

Lemma A.3: Let PL = (W; Pl; ) ranked plausibility structure (w) `L consistent worlds w, PL 6j= '!false `L -consistent formulas ' 2 Le ;
let K = f' 2 Le : PL j= true!'g. AGM revision operator
2 K ' PL j= '! .
Proof: Let belief change operation K ' = f : PL j= '! g. Since
requirement constrains result applying K , assume without loss
generality satisfies AGM postulates applied belief sets K .
Thus, need prove satisfies AGM postulates revision applied K .
e

e

(Note proofs R3 R4 follow proofs R7 R8, respectively.)

R1 Since PL qualitative, f : PL j= '! g belief set, is, closed
R2
R5

logical consequences.
Axiom C1 implies PL j= ' '. Thus, ' 2 K '.
assumptions, ' `L -consistent, Pl([['] ) > ?, thus PL 6j= ' false.
hand, ' `L -consistent, [ '] = ;, thus Pl([['] ) = ?.
conclude Pl([['] ) = ? `L :'. implies PL j= ' false
`L :'. Thus, K ' = Cl(false) `L :'.
Assume `L ' , '0 . Then, assumption, (w)(') = (w)('0). Thus,
[ ' ^ ] = [ '0 ^ ] formulas 2 Le . conclude PL j= '

PL j= '0 . implies K ' = K '0.
two cases: either Pl([[' ^ ] ) = ? Pl([[' ^ ] ) > ?. Pl([[' ^ ] ) =
?, ' ^ inconsistent. According R2, ' 2 K '. Thus,
' ^ 2 Cl(K ' [ f g). implies Cl(K ' [ f g) contains false, thus
K (' ^ ) Cl(K ' [ f g). Pl([[' ^ ] ) > ?, let 2 K (' ^ ). show
2 Cl(K ' [ f g). show K (' ^ ) Cl(K ' [ f g). Since
2 K (' ^ ), get PL j= (' ^ ) . Since Pl([[' ^ ] ) > ?, get
Pl([['^ ^ ] ) > Pl([['^ ^: ] ). Pl([['^( ) )]]) > Pl([['^:( )
))]]), since (' ^ ^ ) ) (' ^ ( ) )) (' ^ :( ) )) ) (' ^ ^ : ).
also implies Pl([['] ) > ?. Thus, PL j= ' ( ) ). So, ( ) ) 2 K ',
thus 2 Cl(K ' [ f g).
Assume : 62 K '. Let 2 Cl(K ' [ f g). show 2 K (' ^ ).
show Cl(K ' [f g) K (' ^ ). Let = [ ' ^: ] , B = [ ' ^ ^ ] ,
C = [ ' ^ ^ : ] . easy verify sets pairwise disjoint. Since
' ^ ( ) ) (' ^ : ) _ (' ^ ^ ) (' ^ :( ) )) (' ^ ^ : ), conclude
[ ' ^ ( ) )]] = [ B , [ ' ^ :( ) )]] = C . Since 2 Cl(K ' [ f g),
( ) ) 2 K '. means PL j= ' ( ) ). Thus, either

!

!
!

e

e

e

R6
R7

e

!

e

!

e

!

!

R8

!

154

fiModeling Belief Dynamic Systems. Part II.

Pl([['] ) = ? Pl(A [ B ) > Pl(C ). Pl([['] ) = ?, according A1, get
Pl([[' ^ ] ) = ?. Thus, PL j= (' ^ ) vacuously, 2 K (' ^ ) desired.
assume Pl(A [ B ) > Pl(C ). Since Pl ranked, satisfies A40 A50.
According A50, get either Pl(A) > Pl(C ) Pl(B ) > Pl(C ). Assume
Pl(A) > Pl(C ) Pl(B ) 6> Pl(C ). Then, using A40, get Pl(A) > Pl(B ).
Applying A2, get Pl(A) > Pl(B [ C ). However since = [ ' ^ : ]
B [ C = [ ' ^ ] , implies : 2 K ', contradicts assumption.
Thus, conclude Pl(B ) > Pl(C ). Since B = [ ' ^ ^ ] C = [ ' ^ ^ : ] ,
get PL j= (' ^ ) , thus 2 K (' ^ ).
R3 R4 definition implies K true = K . According R6,
K (true ^ ') = K '. Combining two facts, get R3 R4 special
cases R7 R8, respectively.

!

!

ut
results show map ranked plausibility structures AGM revision
operators. relate systems C R ranked plausibility structures. Let =
(R; ; P ) 2 C R. Recall REV2 requires prior ranking. Thus,
construct ranked plausibility structure worlds runs R. define
characteristic structure PLI = (R; Pla ; PlI ), Pla agent's prior
runs PlI (r)(p) = (r; 0)(p) p 2 e . Note [ '] PLI = R['].
use PLI describe beliefs agent local state.

LemmaVA.4: Let 2 C R let sa = ho1; : : :; omi.
; sa)
Vm 'o2) toBelbe(Itrue
PLI j= (

)
!
'
.
(By
convention,


=
0
,

take
(
.)
i=1
i=1
Proof: Let 2 C R let sa = ho1; : : :; omi. two cases: either sa local state
, not.
sa local state , suppose ra(m) = sa . Note ' 2 Bel(I ; sa)
Pl(r;m)([['] (r;m)) > Pl(r;m)([[:'] (r;m)). Recall that, according definition
conditioning, Pl(r;m) () isomorphic Pla (jR[; o1; : : :; om]). Thus, Pl(r;m)([['] (r;m)) >
Pl(r;m)([[:'] (r;m)) Pla (R['] j R[; o1; : : :; om ]) > Pla (R[:'] j R[; o1; : : :; om]).
Using C1, true Pla (R['V; o1; : : :; om ]) > Pla (R[:V'; o1; : : :; om ]). Using
REV4, true Pla (RV
[' ^ mi=1 oi ]) > Pla (R[:V' ^ mi=1 oi ]). get
' 2 Bel(I ; sa) Pla(R['V^ mi=1 oi ]) > Pla(R[:' ^ mi=1 oi ]). implies
' 2 Bel(I ; sa) PLI j= ( mi=1 oi )!'.
sa local state , R[; o1; : : :; om] = ;, definition Pla (R[; o1; : : :; om ]) =
?. Using C1 REV4, get PLa(R[Vmi=1 oi]) = ?, thus PLI j= (Vmi=1 oi)!'
' 2 Le . Since sa local state , definition
Bel(I ; sa) = Le . Hence,
V

conclude ' 2 Bel(I ; sa) PLI j= ( i=1 oi )!'. ut
show given ranked plausibility structure PL construct system
whose characteristic structure default-isomorphic PL.

Lemma A.5: Let PLK = (WK ; PlK ; K ) plausibility space satisfies conditions
Lemma A.3. system 2 C R PLI = PLK .
155

fiFriedman & Halpern

Proof: Let PLK = (WK ; PlK ; K ) plausibility space satisfies conditions
Lemma A.3. world w 2 WK sequence observations o1 ; o2; : : :, let rw;o ;o ;:::
run defined rew;o ;o ;:::(m) = w raw;o ;o ;:::(m) = ho1 ; : : :; om m. Let
R = frw;o ;o ;::: : k (w)(oi) = true ig. Define (r; m)(p) = K (re(m))(p)
p 2 e , (r; m)(learn(')) = true o(r;m) = ' ' 2 Le . Finally, define
prior plausibility Pla Pla (R) = PlK (fw : 9r 2 R(w = (0))g. easy check
definition implies Pla (R[']) = PlK ([['] PL ). Thus, PLI = PLK . Since PlK
ranking, Pla also ranking thus qualitative.
verify resulting interpreted system indeed C R. easy
check belief change system; is, satisfies BCS1{BCS5. construction
(m) = (0) runs r times m. Thus, satisfies REV1. Since
prior Pla ranking, system also satisfies REV2. Lemma A.2 implies '
consistent formula, PlK ([['] PL ) > ?. implies Pla (R[']) > ?,
thus system satisfies REV3. Finally, easy show Pla (R['; o1; : : :; om ]) =
Pla(R[' ^ o1 ^ : : : ^ om ]) = PlK ([[' ^ o1 ^ : : : ^ om ] PL ). Thus, system satisfies REV4.
1

1

1

2

1

2

2

2

K

K

ut

K

finally ready prove Theorem 5.2.
Theorem 5.2: Let AGM revision operator let K Le consistent belief
set. system I(;K ) 2 C R Bel(I(;K ); hi) = K
Bel(I(;K ); hi) ' = Bel(I(;K ); h'i)
' 2 Le .
Proof: Let AGM revision operator let K Le consistent belief set.
Lemmas A.2 A.5, system I(;K ) = (R(;K ); (;K ); P(;K )) 2 C R
PLI j= '
2 K '. construction 2 K '
PLI j= ' . Using Lemma A.4, get PLI j= '

2 Bel(I(;K); h'i). Thus, K ' = Bel(I(;K); h'i).
Finally, show Bel(I(;K ); hi) = K . start showing K true = K . Using R3,
get K true Cl(K [ftrueg) = K . Since K consistent, R4, Cl(K [ftrueg)
K true. Thus, K true = K . Lemma A.4, Bel(I ; hi) = Bel(I ; htruei).
Since Bel(I ; htruei) = K true, conclude Bel(I(;K ); hi) = K . ut
next prove Theorem 5.3.
Theorem 5.3: Let system C R. AGM revision operator

Bel(I ; hi) ' = Bel(I ; h'i)
' 2 Le .
Proof: Let = (R; ; P ) system C R. easy verify PLI satisfies
conditions Lemma A.3 K = Bel(I ; hi). lemma implies revision
operator 2 K ' PLI j= ' . Using Lemma A.4,
2 Bel(I ; h'i) PlI j= ' . Thus, K ' = Bel(I ; h'i)
formulas '. ut
(

!

;K )

(

;K )

!

(

!

156

!

;K )

!

fiModeling Belief Dynamic Systems. Part II.

Theorem 5.4: Let system C R sa = ho1; : : :; omi local state .
AGM revision operator ;s
Bel(I ; sa) ;s ' = Bel(I ; sa ')
formulas ' 2 Le o1 ^ : : :om ^ ' consistent.
Proof: structure proof similar Theorem 5.3. proof,




construct ranked plausibility structure use Lemma A.3 find AGM revision
operator. main difference observing '1 ; : : :; 'k , events considered
impossible. Lemma A.3, however, requires possible formulas assigned positive
plausibility. overcome problem assigning \fictional" positive plausibility
non-empty events ruled previous observations.
proceed follows. Let d0 new plausibility value less plausible
positive plausibilities Pla; is, Pla (A) > ?, Pla (A) > d0 . Let =
(R; ; P ) 2 C R ; let sa = hoV1 ; : : :; om i. define PL = (R; Pl; PLI ), Pl
]); ) consistent formulas '. definition implies
Pl([['] ) = max(Pla (R[' ^ V
0
i=1
V
' consistent mi=1 oi , Pl([['] PLV) = Pla (R['1 ^ mi=1 oi ]).
Vprove ' consistent mi=1 oi , PL j= '

PLI j= (' ^

)
.

i=1
V
Vm
\if" part, assume PLI j=V('^ mi=1 oi ) . Since ' consistent

i=1
])) > ?. Thus, Pl (R[(' ^ (Vm )) ^ ]) >
follows,
REV3,

Pl
(
R
[
'
^
(





=1

=1
V
V . implies
Pla(R[(' ^ ( mi=1 oi )) ^ : ]) ?V. Thus, ' ^ consistent mi=1
V
Pl([[' ^ ] ) = Pla (R[(' ^ ( mi=1 oi )) ^ ] > max(d0; Pla (R[(' ^ ( mi=1 oi )) ^ : ]) =
Pl([[' ^ : ] ). conclude PL j= ' .
Vm )) . implies
\only
if"
part,
assume

PL
j
6
=
(
'
^
(

i=1
V
V
Pla(R[(' ^ ( mi=1 oi )) ^ V]) 6> Pla (R[(' ^ ( mi=1 oi ))V ^ : ]). Since Pla ranking,
follows
Pla(R[(' ^ ( mi=1 oiV)) ]) Pla (R[(' ^ ( mi=1 oi ))V ^ : ]). Since ? < Pla (R[' ^
V

( i=1 oi )]) =Vmax(Pla (R[(' ^ ( mi=1 oi )) ^ ]); Pla (R[(' ^ ( mi=1 oi )) ^ : ])),
Pla(R[(' ^ ( mi=1 oi )) ^ : ]) > ?. conclude Pl([[' ^ : ] ) Pl([[' ^ ] ). Thus,
PL 6j= ' .
easy verify PL ranked, satisfies requirements Lemma A.3. Thus,
exists revision operator ;s 2 K ;s ' Vif PL j= ' ,
K = f' : PL j= true 'g. Moreover,Vsince ' consistent mi=1 oi
thatPL j= '
PLI j= (' ^ ( mi=1 oi )) V , then, Lemma A.4, follows
K = Bel(I ; sa) ' consistent mi=1 oi , PL j= '

2 Bel(I ; sa '). ut

!

!

!

!

!

!

!

!



!

!



!

Theorem 5.5: Let system C R whose local states EL . function
e

BelI maps epistemic states belief states
sa local state agent , Bel(I ; sa) = BelI (sa),
(; BelI ) satisfies R10{R80 .

Proof: said earlier, roughly speaking, define BelI (sa) = Bel(I ; sa) sa
local state . sa , set BelI (sa) = Bel(I ; s0), s0 longest
157

fiFriedman & Halpern

consistent sux sa . make definition precise, show resulting
BelI satisfies R10 {R80 .
proceed follows. define function f () maps sequences observations
suxes follows: 8
><> hi
= 0,

> 0 om inconsistent,
f (ho1; : : :; omi) = > hhfalse

;
:
:
:;


otherwise, k minimal index
k

>:
s. t. 6`L :(ok ^ : : : ^ om ).
Aside special case om inconsistent, simply choose longest sux
sa still consistent. define BelI (sa) = Bel(I ; f (sa)). Clearly, sa local state
, f (sa ) = sa , BelI (sa ) = Bel(I ; sa).
show (; BelI ) satisfies R10 {R80. proof outline follows.
Given particular state sa , construct ranked plausibility structure corresponds,
sense Lemma A.2, belief change sa . use Lemma A.3 show
belief changes sa satisfies AGM postulates, i.e., R1{R8. Since true
sa , get BelI satisfies R10 {R80 .
Let sa = ho1; : : :; om i. define ranked plausibility space following
structure. plausible events ones consistent o1 ^ : : : ^ om .
ordered according prior ranking conditioned o1 ^ : : : ^ om . next tier events
inconsistent o1 ^ : : : ^ om consistent o2 ^ : : : ^ om . Again,
ordered according prior ranking conditioned o2 ^ : : : ^ om . continue
way; last tier consists events inconsistent om .
V Formally, let PL = (RV; Pl; PLI ), Pl Pl([['] ) Pl([[ ] ) Pla(R[' ^
( mi=k oi ])) Pla (R[ ^ ( mi=k oi ])) k V + 1 greatest integer
j < k, ' inconsistent mi=j oi. easy see PL ranked,
' consistent, Pl([['] ) > ?.
Let ' 2 Le . show PL j= '
2 BelI (sa '). '
inconsistent, PL j= ' . Moreover, since ' inconsistent, f (sa ') = hfalsei,
thus BelI (sa ') = Le . conclude '
2 BelI (sa '). '
consistent,

let
k


+1


greatest
integer

j < k, ' inconsistent
V

i=j oi . easy verify f (sa ') = hok ; : : :; om ; 'i. Lemma
V A.4, follows
2 BelVI (sa ') = Bel(I ; hok ; : : :; om ; 'i) Pla (R[(' ^ ( mi=k oi )) ^ ]) >
Pla(R[(' ^ ( mi=k oi )) ^ : ]).VWe show Vcase PL j= ' .
Suppose thatVPLa (R[(' ^ ^( mi=k oi )) ^ ]) > PLa (R[(' ^ ( mi=k oi )) ^ : ]). Then, clearly,
Pla(R[(' ^ ( mi=k oi )) ^ ]) > ?, thus ' ^ consistent ok ; : : :; om . Since
' ^ ' ^ : inconsistent oj ; : : V:; om j < k,
V Pl([[' ^ ] ) >
Pl([[' ^ : ] ). hand, Pla (R[(' ^V( mi=k oi )) ^ ]) 6> Pla (R[(' ^ (V mi=k oi )) ^ : ]),
since Pla ranking PLa(R[(' ^ ( mi=k oi )) ^ ]) PLa (R[(' ^ ( miV=k oi )) ^ : ]).
Moreover, since ' consistent Vwith ok ^ : : : ^ om , Pla (R[' ^ ( mi=k oi )]) > ?.
implies Pla (R[(' ^ ( mi=k oi )) ^ : ]) > ? thus Pl([[' ^ ] ) Pl([[' ^ : ] ).
conclude PL j= '
2 BelI (sa ').
Lemma A.3, revision operator satisfies R1{R8 2 K '
PL j= ' . hard check implies change
BelI (sa ) BelI (sa ') satisfies R10 {R80 . ut
e

!

!
!

!

!

!



158

fiModeling Belief Dynamic Systems. Part II.

Proposition 5.6: Let system C R whose local states EL . function
e

BelI maps epistemic states belief states
sa local state agent , Bel(I ; sa) = BelI (sa),
(; BelI ) satisfies R10{R90 .
Proof: said main text, show function BelI defined proof
Theorem 5.5 satisfies R90 . Let sa = ho1; : : :; om i, let '; 2 Le formulas
6`L :(' ^ ). Since ' consistent , get f (sa ' ) = hok ; : : :; om; '; i,
k least integer ' ^ consistent ok ; : : :; om .
reason, get f (sa ' ^ ) = hok ; : : :; om; ' ^ i. Using Lemma A.4 immediately
get Bel(I ; hok ; : : :; om ; '; i) = Bel(I ; hok ; : : :; om ; ' ^ i). Thus, conclude
BelI (sa ' ) = BelI (sa ' ^ ). ut
e

Theorem 5.7: Given function BelL mapping epistemic states EL belief sets
Le BelL (hi) consistent (BelL ; ) satisfies R10{R90, system 2 C R
whose local states EL BelL (sa) = Bel(I ; sa) local state sa .
Proof: show Bel(I ; sa) = BelL (sa) local states sa , system
guaranteed exist Theorem 5.2 Bel(I ; hi) = BelL (hi) Bel(I ; h'i) =
BelL (h'i) ' 2 Le . prove induction length sa .
1, true choice . induction case, let sa = ho1; : : :; omi
local state . Thus,, o1 ^ : : : ^ om consistent. R90 , follows
BelL (ho1; : : :; om i) = BelL (ho1; : : :; om,2; om,1 ^ om i). Using induction hypothesis,
BelL (ho1; : : :; om,2; om,1 ^ om i) = Bel(I ; ho1; : : :; om,2 ; om,1 ^ om i). Using
Lemma A.4, get Bel(I ; ho1; : : :; om,2 ; om,1 ^ om i) = Bel(I ; ho1; : : :; omi). Thus,
conclude BelL (ho1 ; : : :; om i) = Bel(I ; ho1; : : :; om i). ut
e

e

e

e

e

e

e

e

e

e

e

e

e

A.2 Proofs Section 6

section prove Theorem 6.2. show system C U corresponds
update structure. Suppose = (R; ; P ) 2 C U set environment
states Se prior BCS5 consistent distance function d. Define update
structure UI = (Se ; e; d), p 2 e , e (se )(p) = ((se; sa ))(p) choice
sa. BCS1, choice sa matter. easy see UPD1 ensures
Se e satisfy requirements definition update structures. want show
belief change corresponds belief change UI sense Theorem 6.1.
Since Theorem 6.1 states belief change operation defined update structure
satisfies U1{U8, suce prove \if" direction Theorem 6.2. prove
\only if" direction Theorem 6.2, show update structure U ,
system 2 C U UI = U .
start preliminary definitions lemmas \if" direction Theorem 6.2.
Let sa = ho1; : : :; omi. define States (I ; sa) = fs 2 Se : j= 2 Bel(I ; sa)g.
Clearly, ' Bel(I ; sa) = Cl('), States (I ; sa) = [ '] UI . show
belief change corresponds belief change UI show
States (I ; sa ) = minUI (States (I ; sa); [ ] UI ):
159

fiFriedman & Halpern

proved Lemma A.8. prove lemma, need preliminary lemmas.

Lemma A.6: Let 2 C U , let sa = ho1; : : :; omi. ' 2 Bel(I ; sa)
(I ; r; 0) j= ( o1 ^ : : : ^ om )! ' run r R.
Proof: proof lemma analogous proof Lemma A.4, using UPD3
UPD4 instead REV3 REV4. repeat argument here. ut
provide alternative characterization States (I ; sa) terms agent's
prior run-prefixes.

Lemma A.7: Let 2 C U let sa = ho1; : : :; omi. sm 2 States (I ; sa)
sequence states [s0 ; : : :; sm] R[true; o1; : : :; om] Pla ([s0; : : :; sm ]) <
6
Pla(R[true; o1; : : :; om] , [s0 ; : : :; sm]).
Proof: \if" direction, assume sequence s0; : : :; sm
[s0 ; : : :; sm ] R[true; o1; : : :; om], Pla ([s0; : : :; sm ]) <
6 Pla(R[true; o1; : : :; om] ,
[s0 ; : : :; sm ]). way contradiction, assume sm 62 States (I ; m). Thus, formula 2 Bel(I ; m) sm j= : . Lemma A.6 follows since 2 Bel(I ; sa),
(I ; r; 0) j= ( o1 ^ : : : ^ om )! run r R. definition conditioning follows Pla (R[true; o1; : : :; om,1 ; om ^ ]) > Pla (R[true; o1; : : :; om,1; om ^ : ]).
Since sm j= : , get [s0; : : :; sm ] R[true; o1; : : :; om,1 ; om ^ : ]
R[true; o1; : : :; om,1; om ^ ] R[true; o1; : : :; om] , [s0; : : :; sm]. A1, follows
Pla([s0 ; : : :; sm]) < Pla (R[true; o1; : : :; om ] , [s0; : : :; sm ]), contradicts starting
assumption. conclude sm 2 States (I ; sa).
\only if" direction, assume sm 2 States (I ; a). Since Se finite e
assigns different truth assignment state Se , formula 2 Le
characterizes sm ; is, j= = sm . Since sm 2 States (I ; sa),
: 62 Bel(I ; sa). Using Lemma A.6, get (I ; r; 0) 6j= ( o1 ^ : : : ^ om)! :
runs r 2 R. BCS5, true Pla (R[true; o1; : : :; om ]) > ?
Pla(R[true; o1; : : :; om,1; om ^ ]) <
6 Pla(R[true; o1; : : :; om,1; om ^ :]). UPD2,
sequence [s0 ; : : :; sm] R[true; o1; : : :; om,1 ; om ^ ] Pla ([s0; : : :; sm ]) <
6
Pla([s00 ; : : :; s0m]) [s00 ; : : :; s0m] R[true; o1; : : :; om,1 ; om ^: ]. Moreover, without loss
generality, assume Pla ([s0; : : :; sm]) <
6 Pla([s00; : : :; s0m]) [s00; : : :; s0m]
R[true; o1; : : :; om,1; om ^ ], since finitely many sequences. Thus,
UPD2, Pla ([s0; : : :; sm ]) <
6 Pla(R[true; o1; : : :; om] , [s0; : : :; sm]). ut
prove belief change corresponds belief change UI .
Lemma A.8: Let = (R; ; P ) 2 C U
States (I ; sa ) = minUI (States (I ; sa); [ ] UI )
local states sa formulas 2 Le .
Proof: Let Pla prior ; assume Pla consistent distance function d.
Let sa = ho1 ; : : :; om i.
160

fiModeling Belief Dynamic Systems. Part II.

show minUI (States (I ; sa); [ ] UI ) States (I ; sa ), suppose 2
minUI (States (I ; sa); [ ] UI ). Thus, state sm 2 States (I ; sa) d(sm ; s0) 6<
d(sm ; s) states s0 satisfy . want show 2 States (I ; sa
). Lemma A.7, follows that, since sm 2 States (I ; sa), sequence
s0 ; : : :; sm,1 [s0; : : :; sm ] 2 R[true; o1; : : :; om,1 ; om] Pla ([s0; : : :; sm]) 6<
Pla(R[true; o1; : : :; om,1; om ] , [s0 ; : : :; sm ]). show Pla ([s0; : : :; sm; s]) 6<
Pla(R[true; o1; : : :; om; ] , [s0 ; : : :; sm ; s]). Lemma A.7, suces show
2 States (I ; sa ). Suppose [s00 ; : : :; s0m+1] R[true; o1; : : :; om ; ] , [s0; : : :; sm; s].
[s0; : : :; sm ] = [s00 ; : : :; s0m ], d(s0m ; s0m+1 ) 6< d(sm ; s). Since Pla consistent d, follows Pla ([s0; : : :; sm ; s]) 6< Pla ([s00; : : :; s0m ; s0m+1]). [s0 ; : : :; sm] 6=
[s00 ; : : :; s0m ], then, since Pla ([s0 ; : : :; sm ]) 6< Pla ([s00; : : :; s0m]) Pla consistent d,
Pla ([s0; : : :; sm ; s]) 6< Pla ([s00 ; : : :; s0m ; s0m+1 ]).
Since
Pla ([s0; : : :; sm ; s])
6<
Pla ([s00; : : :; s0m ; s0m+1 ])
0
0
[s0; : : :; sm+1 ] R[true; o1; : : :; om ; ] , [s0 ; : : :; sm ; s] Pla prefix-defined,
Pla([s0 ; : : :; sm; s]) 6< Pla (R[true; o1; : : :; om ; ] , [s0; : : :; sm ; s]. Lemma A.7,
2 States (I ; sa ), desired.
show States (I ; sa ) minUI (States (I ; sa); [ ] UI ), suppose 2
States (I ; sa ). Lemma A.7, sequence s0 ; : : :; sm [s0 ; : : :; sm ; s])
R[true; o1; : : :; om; ] Pla([s0; : : :; sm; s]) 6< Pla(R[true; o1; : : :; om; ] , [s0; : : :; sm; s]).
want show sm 2 States (I ; sa) d(sm ; s0) 6< d(sm ; s) s0 satisfy
. suces prove 2 minUI (States (I ; sa); [ ] UI ).
show sm 2 States (I ; sa), Lemma A.7, suces show Pla ([s0; : : :; sm ]) 6<
Pla(R[true; o1; : : :; om] , [s0; : : :; sm ]). Let s00 ; : : :; s0m sequence [s00 ; : : :; s0m]
R[true; o1; : : :; om]. definition, [s00; : : :; s0m; s] R[true; o1; : : :; om; ]. Thus,
choice s0 ; : : :; sm , follows Pla ([s0; : : :; sm ; s]) 6< Pla ([s00; : : :; s0m; s]). Since Pla
consistent d, follows Pla ([s0; : : :; sm]) 6< Pla ([s00; : : :; s0m ]). Thus, Lemma A.7,
sm 2 States (I ; sa). see d(sm; s0 ) 6< d(sm ; s) s0 satisfy , let s0 6=
s0 j= . Thus, [s0 ; : : :; sm ; s0] [true; o1; : : :; om; ]. choice s0 ; : : :; sm,
follows Pla ([s0 ; : : :; sm ; s]) 6< Pla([s0 ; : : :; sm; s0 ]). Since Pla consistent d,
follows d(sm ; s0) 6< d(sm ; s). conclude 2 minUI (States (I ; sa); [ ] UI ). ut
tools prove \if" direction Theorem 6.2.

Lemma A.9: = (R; ; P ) 2 C U , belief change operator satisfies
U1{U8

Bel(I ; sa) = Bel(I ; sa )
local states sa formulas 2 Le .

Proof: Let 2 C U . Using arguments presented above, easy check UI
update structure. Theorem 6.1, belief change operator satisfies
U1{U8 [ ' ] UI = minUI ([['] UI ; [ ] UI ) '; 2 Le . Lemma A.8,
follows Bel(I ; sa) = Bel(I ; sa ): ut
prove \only if" direction Theorem 6.2. Suppose belief change
operator satisfies U1{U8. According Theorem 6.1, update structure U
corresponds . Thus, suces show system UI = U.
161

fiFriedman & Halpern

Lemma A.10: Let U = (W; d; U ) update structure. system 2 C U
UI = U .

Proof: Given sequences w0; w1; : : : 2 W o1; o2; : : : 2 Le, let rw ;w ;:::;o ;o ;:::
run defined rew ;w ;:::;o ;o ;:::(m) = wm raw ;w ;:::;o ;o ;:::(m) = ho1; : : :; om i. Let
R = frw ;w ;:::;o ;o ;::: : U (wm)(om) = true mg. Define (r; m)(p) =
U (re(m))(p) p 2 e (r; m)(learn(')) = true o(r;m) = ' ' 2 Le.
clear (R; ) satisfies BCS1{BCS4 UPD1. Thus, remains show
0

0

0

1

1

1

1

2

0

1

1

1

1

2

2

2

prior plausibility measure Pla satisfies UPD2{UPD4. ensure
(R; ; P ) 2 C U .
proceed follows. define preferential space (R; ) r r0
(k) = re0 (k) 0 k m, (m + 1) 6= re0 (m + 1),
d(re(m); re(m +1)) < d(re0 (m); re0 (m +1)). Recall r r0 denotes r preferred
r0. Thus, ordering consistent comparison events form [s0 ; : : :; sn ]
according UPD2.
Using construction Proposition 2.2, plausibility space (R; Pla )
Pla(A) Pla (B ) r 2 B , A, run r0 2 r0 r
r00 2 B , r00 r0. (Friedman & Halpern, 1998b, Theorem 5.5),
Pla qualitative plausibility measure. show satisfies UPD2{UPD4.
start UPD2. show PlA consistent d, need show
Pla([s0 ; : : :; sn ]) < Pla ([s00; : : :; s0n ]) < n sk = s0k
0 k m, d(sm ; sm+1) > d(s0m ; s0m+1 ). Suppose Pla ([s0; : : :; sn ]) <
Pla([s00 ; : : :; s0n ]). Let r run [s0 ; : : :;0n ]. Without loss generality assume
(m) = (n) > n. Since Pla ([s0; : : :; sn ]) < Pla ([s00 ; : : :; s0n ]), run
r0 2 [s00 ; : : :; s0n] r0 r. definition, implies
re(k) = re0 (k) 0 k m, d(re0 (m); re0 (m + 1)) < d(re(m); re(m + 1)). claim
< n. n, (m +1) = (m) construction, d(re(m); re(m +1)) =
d(re(m); re(m)) d(re0 (m); re0 (m + 1)) r0 6 r, contradiction. Thus, sk = s0k
0 k m, d(s0m ; s0m+1 ) < d(sm ; sm+1 ).
converse, suppose < n sk = s0k 0 k m,
d(s0m ; s0m+1) < d(sm ; sm+1 ). Let r0 run re0 (k) = s0k k n, re0 (k) = s0n
k n, o(r0 ;k) = true k. follows r0 r runs r0 2 [s0 ; : : :; sn ]. Thus,
Pla([s0 ; : : :; sn ]) < Pla ([s00; : : :; s0n ]).
show Pla prefix-defined, must show Pla (R['0; : : :; 'n ]) Pla (R[ 0; : : :; n])
[s0; : : :; sn ] R[ 0; : : :; n] ,R['0; : : :; 'n], [s00 ; : : :; s0n ]
R['0; : : :; 'n] Pla([s00; : : :; s0n]) > Pla([s0; : : :; sn]). Suppose Pla(R['0; : : :; 'n])
Pla(R[ 0; : : :; n ]). Let [s0; : : :; sn ] R[ 0; : : :; n ] , R['0; : : :; 'n ]. Let r 2 [s0 ; : : :; sn ]
run (m) = (n) n. Since Pla (R['0; : : :; 'n]) Pla(R[ 0; : : :; n ])
run r0 2 R['0; : : :; 'n] r0 r. implies
(k) = re0 (k) 0 k m, d(re0 (m); re0 (m + 1)) < d(re(m); re(m + 1)).
before, < n, thus Pla([re0 (0); : : :; re0 (n)]) > Pla ([s0 ; : : :; sn ]). Since
r0 2 R['0; : : :; 'n], also [re0 (0); : : :; re0 (n)] R['0; : : :; 'n ], desired.
converse, assume [s0 ; : : :; sn ] R[ 0; : : :; n ] , R['0; : : :; 'n]
[s00; : : :; s0n ] R['0; : : :; 'n] Pla([s00 ; : : :; s0n ]) > Pla ([s0; : : :; sn ]). implies Pla (R['0; : : :; 'n]) > Pla ([s0; : : :; sn ]) [s0 ; : : :; sn ] R[ 0; : : :; n ] ,
162

fiModeling Belief Dynamic Systems. Part II.

R['0; : : :; 'n]. Since finitely many sequences states length m, apply A2, conclude Pla(R['0; : : :; 'n ]) > Pla(R[ 0; : : :; n ] , R['0; : : :; 'n ]). Thus,
Pla(R['0; : : :; 'n ]) Pla ((R[ 0; : : :; n])).
UPD3, recall construction Proposition 2.2 Pla(R) > ?
non-empty R R. Since, construction, set R['0; : : :; 'n] non-empty
sequences '0 ; : : :; 'n consistent formulas, UPD3 must hold.
Finally, consider UPD4. show Pla(R['0; : : :; 'n+1 ; o1; : : :; on])
Pla(R[ 0; : : :; n+1 ; o1; : : :; ]) Pla (R['0; '1^o1 ; : : :; 'n^on ; 'n+1 ]) Pla (R[ 0; 1^
o1 ; : : :; n ^on; n+1 ]). construction, R['0; : : :; 'n+1; o1; : : :; ] R['0; '1^o1 ; : : :; 'n ^
; 'n+1]. hand, run r 2 R['0; '1 ^ o1; : : :; 'n ^ ; 'n+1 ]
run r0 2 R['0; : : :; 'n+1 ; o1; : : :; ] re0 (m) = (m) m, o(r;m) = om
1 n. Since preference ordering runs function environment states, clear r r0 compared manner;
r00, r00 r r00 r0, r r00 r0 r00. Thus, conclude
purposes preference ordering, R['0; '1 ^ o1 ; : : :; 'n ^ ; 'n+1 ]
R['0; : : :; 'n+1; o1; : : :; on] compared manner sets. easy see
suces show Pla satisfies UPD4. ut
Finally, prove Theorem 6.2.
Theorem 6.2: belief change operator satisfies U1{U8 system
2 C U
Bel(I ; sa) = Bel(I ; sa )
epistemic states sa formulas 2 Le .
Proof: \if" direction follows Lemma A.9. \only if" direction, assume
satisfies U1{U8. Theorem 6.1, update structure U [ ' ] UI =
minUI ([['] UI ; [ ] UI ) '; 2 Le . Lemma A.10, system 2 C U
UI = U. Lemma A.8, follows Bel(I ; sa) = Bel(I ; sa ) local states
sa formulas 2 Le . ut

References

Alchourron, C. E., Gardenfors, P., & Makinson, D. (1985). logic theory change:
partial meet functions contraction revision. Journal Symbolic Logic, 50,
510{530.
Boutilier, C. (1992). Normative, subjective autoepistemic defaults: adopting Ramsey test. Principles Knowledge Representation Reasoning: Proc. Third
International Conference (KR '92), pp. 685{696. Morgan Kaufmann, San Francisco,
Calif.
Boutilier, C. (1994a). Conditional logics normality: modal approach. Artificial Intelligence, 68, 87{154.
Boutilier, C. (1994b). Unifying default reasoning belief revision modal framework.
Artificial Intelligence, 68, 33{85.
163

fiFriedman & Halpern

Boutilier, C. (1996a). Iterated revision minimal change conditional beliefs. Journal
Philosophical Logic, 25, 262{305.
Boutilier, C. (1996b). Abduction plausible causes: event-based model belief update.
Artificial Intelligence, 83, 143{166.
Boutilier, C. (1998). unified model qualitative belief change: dynamical systems
perspective. Artificial Intelligence, 98, 281{316.
Boutilier, C., Friedman, N., & Halpern, J. Y. (1998). Belief revision unreliable observations. Proceedings, Fifteenth National Conference Artificial Intelligence
(AAAI '96), pp. 127{134.
Burgess, J. (1981). Quick completeness proofs logics conditionals. Notre Dame
Journal Formal Logic, 22, 76{84.
Darwiche, A., & Pearl, J. (1997). logic iterated belief revision. Artificial Intelligence, 89, 1{29.
Davis, R., & Hamscher, W. (1988). Model-based reasoning: troubleshooting. Shrobe,
H., & Artificial Intelligence, T. A. A. (Eds.), Exploring AI, pp. 297{346. Morgan
Kaufmann, SF.
de Rijke, M. (1992). Meeting neighbors. Research report LP-92-10, University
Amsterdam.
del Val, A., & Shoham, Y. (1992). Deriving properties belief update theories
action. Proceedings, Tenth National Conference Artificial Intelligence (AAAI
'92), pp. 584{589. AAAI Press, Menlo Park, CA.
del Val, A., & Shoham, Y. (1993). Deriving properties belief update theories action
(II). Proc. Thirteenth International Joint Conference Artificial Intelligence
(IJCAI '93), pp. 732{737 San Francisco. Morgan Kaufmann.
del Val, A., & Shoham, Y. (1994). unified view belief revision update. Journal
Logic Computation, 4.
Dubois, D., & Prade, H. (1990). introduction possibilistic fuzzy logics.
Shafer, G., & Pearl, J. (Eds.), Readings Uncertain Reasoning, pp. 742{761. Morgan
Kaufmann, San Francisco, Calif.
Dubois, D., & Prade, H. (1991). Possibilistic logic, preferential models, non-monotonicity
related issues. Proc. Twelfth International Joint Conference Artificial Intelligence (IJCAI '91), pp. 419{424. Morgan Kaufmann, San Francisco.
Fagin, R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1995). Reasoning Knowledge.
MIT Press, Cambridge, Mass.
Freund, M., & Lehmann, D. (1994). Belief revision rational inference. Tech. rep. TR
94-16, Hebrew University.
164

fiModeling Belief Dynamic Systems. Part II.

Friedman, N. (1997). Modeling Beliefs Dynamic Systems. Ph.D. thesis, Stanford.
Friedman, N., & Halpern, J. Y. (1994). Conditional logics belief change. Proc. National
Conference Artificial Intelligence (AAAI '94), pp. 915{921. AAAI Press, Menlo
Park, CA.
Friedman, N., & Halpern, J. Y. (1995). Plausibility measures: user's manual. Besnard,
P., & Hanks, S. (Eds.), Proc. Eleventh Conference Uncertainty Artificial Intelligence (UAI '95), pp. 175{184. Morgan Kaufmann, San Francisco.
Friedman, N., & Halpern, J. Y. (1996). qualitative Markov assumption implications belief change. Proc. Twelfth Conference Uncertainty Artificial
Intelligence (UAI '96), pp. 263{273.
Friedman, N., & Halpern, J. Y. (1997). Modeling belief dynamic systems. part I: foundations. Artificial Intelligence, 95 (2), 257{316.
Friedman, N., & Halpern, J. Y. (1998a). Belief revision: critique. Journal Logic,
Language Information, appear. Also available http://www.cs.huji.ac.
il/~nir. preliminary version appeared L. C. Aiello, J. Doyle, S. C. Shapiro
(eds.) Principles Knowledge Representation Reasoning: Proc. 5'th International Conference, pp. 421{431, 1996.
Friedman, N., & Halpern, J. Y. (1998b). Plausibility measures default reasoning.
Journal ACM, appear. Also available http://www.huji.ac.il/~nir.
preliminary version appeared Proc., 13'th National Conference Artificial Intelligence, pp. 1297{1304, 1996.
Fuhrmann, A. (1989). ective modalities theory change. Synthese, 81, 115{134.
Gardenfors, P. (1986). Belief revision Ramsey test conditionals. Philosophical
Review, 91, 81{93.
Gardenfors, P. (1988). Knowledge Flux. MIT Press, Cambridge, Mass.
Gardenfors, P., & Makinson, D. (1988). Revisions knowledge systems using epistemic
entrenchment. Proc. Second Conference Theoretical Aspects Reasoning
Knowledge, pp. 83{95. Morgan Kaufmann, San Francisco, Calif.
Goldszmidt, M., Morris, P., & Pearl, J. (1993). maximum entropy approach nonmonotonic reasoning. IEEE Transactions Pattern Analysis Machine Intelligence,
15 (3), 220{232.
Goldszmidt, M., & Pearl, J. (1996). Qualitative probabilities default reasoning, belief
revision, causal modeling. Artificial Intelligence, 84, 57{112.
Grahne, G., Mendelzon, A., & Rieter, R. (1992). semantics belief revision systems.
Moses, Y. (Ed.), know92, pp. 132{142. Morgan Kaufmann, San Francisco, Calif.
Grove, A. (1988). Two modelings theory change. Journal Philosophical Logic, 17,
157{170.
165

fiFriedman & Halpern

Halpern, J. Y., & Fagin, R. (1989). Modelling knowledge action distributed systems.
Distributed Computing, 3 (4), 159{179. preliminary version appeared Proc. 4th
ACM Symposium Principles Distributed Computing, 1985, title \A
formal model knowledge, action, communication distributed systems: preliminary report".
Halpern, J. Y., & Vardi, M. Y. (1989). complexity reasoning knowledge
time, I: lower bounds. Journal Computer System Sciences, 38 (1), 195{237.
Katsuno, H., & Mendelzon, A. (1991a). difference updating knowledge base revising it. Principles Knowledge Representation Reasoning:
Proc. Second International Conference (KR '91), pp. 387{394. Morgan Kaufmann,
San Francisco, Calif.
Katsuno, H., & Mendelzon, A. (1991b). Propositional knowledge base revision minimal
change. Artificial Intelligence, 52 (3), 263{294.
Katsuno, H., & Satoh, K. (1991). unified view consequence relation, belief revision
conditional logic. Proc. Twelfth International Joint Conference Artificial
Intelligence (IJCAI '91), pp. 406{412.
Kautz, H. A. (1986). Logic persistence. Proceedings, Fifth National Conference
Artificial Intelligence (AAAI '86), pp. 401{405. AAAI Press, Menlo Park, CA.
Keller, A. M., & Winslett, M. (1985). use extended relational model handle
changing incomplete information. IEEE Transactions Software Engineering, SE11 (7), 620{633.
Kraus, S., Lehmann, D., & Magidor, M. (1990). Nonmonotonic reasoning, preferential
models cumulative logics. Artificial Intelligence, 44, 167{207.
Lehmann, D. (1995). Belief revision, revised. Proc. Fourteenth International Joint
Conference Artificial Intelligence (IJCAI '95), pp. 1534{1540. Morgan Kaufmann,
San Francisco.
Levi, I. (1988). Iteration conditionals Ramsey test. Synthese, 76, 49{81.
Lewis, D. K. (1973). Counterfactuals. Harvard University Press, Cambridge, Mass.
Manna, Z., & Pnueli, A. (1992). Temporal Logic Reactive Concurrent Systems,
Vol. 1. Springer-Verlag, Berlin/New York.
Nayak, A. C. (1994). Iterated belief change based epistemic entrenchment. Erkenntnis,
41, 353{390.
Pearl, J. (1989). Probabilistic semantics nonmonotonic reasoning: survey. Brachman, R. J., Levesque, H. J., & Reiter, R. (Eds.), Proc. First International Conference
Principles Knowledge Representation Reasoning (KR '89), pp. 505{516.
Reprinted Readings Uncertain Reasoning, G. Shafer J. Pearl (eds.), Morgan
Kaufmann, San Francisco, Calif., 1990, pp. 699{710.
166

fiModeling Belief Dynamic Systems. Part II.

Rott, H. (1991). Two methods constructing contractions revisions knowledge
systems. Journal Philosophical Logic, 20, 149{173.
Rott, H. (1992). Two methods constructing contraction revisions knowledge
systems. Journal Logic, Language Information, 1, 45{78.
Shafer, G. (1976). Mathematical Theory Evidence. Princeton University Press, Princeton, N.J.
Shoham, Y. (1987). semantical approach nonmonotonic logics. Proc. 2nd IEEE
Symp. Logic Computer Science, pp. 275{279. Reprinted M. L. Ginsberg (Ed.),
Readings Nonmonotonic Reasoning, Morgan Kaufman, San Francisco, Calif., 1987,
pp. 227{250.
Shoham, Y. (1988). Chronological ignorance: experiments nonmonotonic temporal reasoning. Artificial Intelligence, 36, 271{331.
Spohn, W. (1988). Ordinal conditional functions: dynamic theory epistemic states.
Harper, W., & Skyrms, B. (Eds.), Causation Decision, Belief Change,
Statistics, Vol. 2, pp. 105{134. Reidel, Dordrecht, Netherlands.
Wang, Z., & Klir, G. J. (1992). Fuzzy Measure Theory. Plenum Press, New York.
Williams, M. (1994). Transmutations knowledge systems. Principles Knowledge
Representation Reasoning: Proc. Fourth International Conference (KR '94), pp.
619{629. Morgan Kaufmann, San Francisco, Calif.
Winslett, M. (1988). Reasoning action using possible models approach. Proceedings, Seventh National Conference Artificial Intelligence (AAAI '88), pp. 89{93.
AAAI Press, Menlo Park, CA.

167

fi
ff
fi
! #"$ %
'&)( *,+( ---/.1020435062

789:; <)( */=
->!?A@9 %&<B6/=
--

CEDGFIH1JKMLINOJQP8FIRSCEDGFITUP
JVP/DWFIXZY\[]Y/XZFIH_^a`bXdcEeIfgDGKVfihkjl[mKVDinWfgK

oMprqsqstvuwt'xysz1x#{|x

}~1$G~$}g~O$1~$)

l
)
8A $ g/8/

O'

sA

GgMMv

g

vsV
)! !A

ssslA$A 8
)s4 Ws4;44gl/4s
Zl/sa/4!ZA;ff;!sa;48l!4

ss!BAO4
Alas
I$4s48s4 WsW4U !8WABs
4GAs

4ZsA;lIs)A
GAZs
/W!AV/44a
A8I
4IV# 4G4s8Vi!4Ms$Z4! 8V4isrsaGl8 5
!A

$vs
rsv4v|4O5m4i4B a8;/s4 s5
AaOs4|
WrA!AOgA4
GsslAAB5mvff;sV4MVaA4A8Wff
4Gl GA#44 lAss44g/s48A1sB1
Al 4/sl4AA
45GAIl slAssa58sI88AIAB4/sa44A
a4As8Q48sO
Ags4AsB)s8Offs8s)5Gss
4/4485s4A!4BAg/ssrAsZs8Offs8s5GAsQAl
48;/4a8
A484s4A!;UGs4W'4s!
V ;,i
ZW1/
fffi ff

fffififffiff fffiff !"#
$"#%#&'(!*)+,)
)
fi /-. /0 1%Q 802$34 5fiff6Q05 fffi5+ff/ ff/ 78 '0ff+9: ff
fffi))020ff/$!Z/!ff !; '<-=5$3O) fiff+% />? @ff/ Afiff+ B
55 0C 5>(D E1
69 lF $ ;= Q05 fffiD6/ff ffG /|H ff U
!ff ! !&I
ff/+/ 0!1
- 50*3C4 6$/ff 6(D J // LK'0 #ff6!&!/ff )M5G
) 0/
= AG0 55
ffMN
= 0ff !
& /ff % + A7L*)+O"P /ff )|H ff !?$/ff G ' !)M *&Pfiff 5ff /ff

Qfiff ff 3
R Sff@ T8 BU/ff V)DW0 WQ
% /ff U1ff@ TX ff B Z
ff
|
H ff !1 ,7P*)+@. 2Z "/ff fifffi8/ff /I

;fffiQ
% , 05 [3\]C/ff Iff fi7
$fffiT *2 M%L^ ff
_Q"*/ff ff
`, # baG
P dcP"fffi/ ;fffiD / 0 $"/ff N 0D ff !O/ff ff
02
ff/I
Q?./ff ,ff =P0 ff !@C_Q$
3 Z0/,;A 5 F
Q05 ff !/ff ff
02 6/ff Q%e/ff 80 / 036)
f B 5ff 9/ff B
2 ;fffiIQ
% !ff #ff #ff
: ff gN/ff h)M5$3I_ = 1/ff ff, 02 g/ff $8 65 &Lg/ff ffg-5fiffTifffii # ja"O6%L
^ ff,_
gffg/ff 0/hifffi. P kcPM
" 5\
7 l/ff h%L^ ff0"e-=m ifffi5 / l0 $"O l l/ff h%L^ ff,m!ff03
R .X!*)/h/ff B
$" P nch;2L !ff i!&'!C%L^ ff+_offi5# pa3
q fiff./ff $m rff.)M 7sW!ff ! G

% t0 DU ff.s/ff $l5 fiffvu
Aff
rff;= ff <uw5$0v=
5 ! ."P/ff 0D!&P 8C)
) 8 B
Q0ff
%QfiffOQ
%
|H 0 ff QI/ff ; !5!/ff yxezh{N|~} q 0_T fiff 06+ r%=!fffiff0"LaLa*Dg
3 Z
!5/ff fiff ff g)+!/ff l
= 0ff = m/ff ffI fiff TM 5ff !/ff h 6 m/ff
ff efiff ff 3;4N ; X i%P&?%= 7Pfffi5 7L? 5Ov
3 Z$, /i6 AL i/ff $ 58



( ---O %%=!<4Z
<B/
MV
!fi:g
N9! %&% s&/%1%/ <

fi+!=

fi 0 ;05fiPLfig ;!0fiM$9Lfi !C5 fie8fiIX<=
9PBrfi L0.0Ee05 !m,fi ? ,lL ?0D !lgfimfi
:<=9!06 P1efir NL= e0M0e05 !#A5.fifi5r Q
!;D05
B !;V!D! A@ghN:C0@L! $gS/C*0`O`$
:G5+JM0=MLfii5 <w508!5! M=iL! !
0m+?0fi05A p8! !! Gfi fi '50 fi#fih0Dfi58
*/fi005=!fi05 + GNA 55?h fiT!5! A6 L0G! /L
!= !5 r =ie05fi;+ ?A g ig ,#fi L'fi
!0fi$V@#fiP85 Q!I! !!1,$T
0 8 8!fi05 !Q 0e5l0 5r/C 01P.0o/5l+ 1
<e0 #Ge fieTfi, +!fi0D ! A0G;6!0fi++
00 i`@ e+! .$fi rfiL/eiX<=!O
=0?Mm50i! !X lfifi5?er',L!0,
fifi@9LfiALfi A!D! A; + L,L! eQP!0dL
! e/BfiAfim fiA fi=!. !005M i1=
PhT 0@vDL8 M=Q:fi 5$L*DP8Q5r6M h=
Pi5 v=!i!5 A:,fihPA$
PCD fi@G AA0 .05
X50M !' i5fivw50 0fi.!''VPA?!NM@5#fi@V?
'0!# !L ?XA 0 fi# l! !=1 V fifi5fi
fi*@efir ;('0!#A!Ar !O+i'0! P! #fi0 0'+!
,5r+50 ;6 fi=!D! A;.0 0$=?:,fi8bPAOO
Mh fiN 6fi5 g ofi 9e! P+fi,fi0 G=X58
' e ! !N! =@ !G=,Pm v==!]m!D!
Mie, fi!IifiA L0$N/*M00= ; DfiI ,0fi
. QL!0+1! G.hm v==!]B5!8(5!!
?+ / fifi5 fi8N ge!LA! / !b$fifi 8
fifiIC! !fi;0 !'P;e!8fi I!!]'0 ;TC
N.P fi=V fi5 mB0fi'~:mfiP?e05Z
!+mie!LA<w ,0D<=0 !l g e05 gm hl50 h
+mfi 'X h! `fi fiQM !$=*M00O#!gfifi5
B~vX 1 X1==g 5 T.'Q !+# 0!iXL0fiB X
VL0 !l Q5!0, [g!fi0D !l## <0DI#$B ! !
8 gTC
, efiT1 DifiA! !== i.fi5 !?fiAh
$ =! /05! !@6 v=!'C e ! !eX58M
=!We G0 gPfiD Lwe#r <=IP!(586g# <IMP!
X5Q55fi05!0? '0,; 'e!PA6!0D55#:M$ Gh Gg5
M!+ e ! !(DQ/55fi05!0g g!!A0 TCP
!O 8fiD lMP# <lM#V(DQ8/! rI.0fi
!(8 ?!!0 h6 m/QM'+6 ,60fi05A.fifi?fi ,X
!9?I 5$AM0fihl0Ifi 5L!]rw 0 0 5
!. 0 h+?e!LA$ fi5 !1( !O=fi? e ! !
fi=!/+/(5!!,fi?fi!8!+P@1!D! += (LA 5!#
9 e ! X58


fi
fiff
ff

ff




"!#%$
&'$(#)+*,+-.)0/&.1*2#435&.,+67-88-9:,;=<>1@?A#4BDC0*-15EF9G#=3*,0BH
,0,IC0!#J)fi#4,0H8C0,K-1LC0!#MBD-N%$
HO
C0&'C0*-1&.8PBD-NQ$
8#DR*CTS=-.6UBD-1
3*C0*-1&.8($8&.11*1/VA9W!*Bfi!X&')0#+C0!#YNQ-.C0*Z'&'C0*-1F67-.):C0!#[&'$$)0-&.B\!F9#
!&]Z.#YBfi!-,^#41_;:?A#4BDC0*-1X`J$)0#4,0#41aC0,W3*b(#)0#41aC:C^)fi&.1,08&'C0*-1,c-.6dBD-13*C0*-1&.8_$
8&.1
1*1/[C^-MeAH&.1C0*f
#43
g --8#4&.16h-.)fiN[H8&'#.;i"!#cNQ-.)fi#G/.#41#)\&.86h)fi&.N%#9G-.)0jY-.6BD-13
*C0*-1&.8$
8&.11
*1/:&.88-]9:,iNQ-.)0#c3#/.)fi##4,
-.6U67)0##43-Nk*1lBfi!-A-,0*1/=9:!&'CWjA*m13F-.6U$8&.1,:&=$
8m&.11#):$
)0-A3
HBD#4,;"n#QBD-1,0*m3#)o&M/.#41#)fi&.8p67-.)^O
N%&.8*24&'C0*-1Q*1[9:!*B\!*1aC^#)\1&.8,^C0&'C^#"C^)fi&.1,fi*C0*-1,p-.6(&I$
8&.1&')0#"3#4,0BD)\*q(#43[&.,pfr1*C^#c&.HC^-N%&'C0&V&.13
8#4,0,"/.#41#)fi&.8_67-.)fiN%&.8*24&'C0*-1,s9:*C0!tN%-.)0#u)0#4,^C^)\*BDC^#43tC^)fi&.1,fi*C0*-1v6wH1BDC0*-1,4;ix-.)W&.88(67-.)fiN%&.8*24&'C0*-1,
9#+$)0#4,0#41aCWC^)fi&.1,08&'C0*-1,"-.6d$)fi-.q
8#4Ny*1
,^C0&.1BD#4,WC^-MeAH&.1aC0*f
#43 g -A-8#4&.1X67-.)fiN[H8&'#.;"z{eAH&.1C0*f
#43
g --8#4&.1L67-.)fiN[H8&tC0!&'C[*88H
,^C^)fi&'C^#4,IC0!#MC^)fi&.1,08&'C0*-1,u*,+/*Z.#41L*1|?A#4BDC0*-15}A;tn#v!&]Z.#M,^-8Z.#43|&
1AHN+q(#)c-.6d,0*N%$
8#o$)0-.q
8#4NJ,c*1tBD-13*C0*-1&.8($
8&.11
*1/[qSvH,0*1/&%C0!#-.)0#4NO~$)fi-]Z.#):67-.)I g x|9#
!&]Z.#3#Z.#48-.$(#43_;W"!#+C0!#-.)0#4NQO~$)0-]Z.#)u*m,:q)\*#D
St3*,0BH
,0,^#43X*m1?A#4BDC0*-1M&.13lC0!#Y#DR$(#)fi*NQ#41C0,
*1X?A#4BDC0*-1l;xi*1&.88S.V*m1X?A#4BDC0*-1l[9#+3*m,0BH,0,s#4&')fi8m*#)c9G-.)fij=C0!
&'C"*,c)0#48&'C^#43FC^-%-H)fi,;
iQM
TU(a

H&.1C0*f
#43 g -A-8#4&.1t67-.)fiNYH
8&'#u&')0#K-.6pC0!#K6h-.)\N'^04ifi4a
l9:!#)0#u*,s&.1FH1eAH&.1C0*f
#43

$)0-.$(-,0*C0*-1&.86h-.)\NYH8m&&.1
3C0!#t$)0#Df
R|BD-1,0*m,^C0,Q-.6IH1*Z.#)fi,fi&.8G&.13#DR*,^C^#41C0*&.8:@eAH&.1aC0*f
#)fi,
] &.1
3XC0!#[$)0-.$(-,0*C0*-1&.8Z'&')fi*&'q8#4," C0!&'Co-BBH)K*1lP;IW#Dfr1#i c&.,oC0!#

67-.)fiNYH
8&IC0!&'C*,U-.qC0&.*1#43Q67)0-N=qAS[)0#$
8&.B*1/o-BBH)0)fi#41BD#4, -.6rC0!#"$)fi-.$r-,fi*C0*-1&.8AZ'&')fi*&'q
8#cMqS
C0!#[6h-.)\NYH8m&JW;u"!#+C^)\HC0!-.6eH
&.1aC0*f#43 g -A-8#4&.16h-.)\NYH8m&'#[*,o3#Df1#43X)0#4BH)fi,0*Z.#48SF&.,I67-88-9:,;
"!#C^)fiHC0![-.6r&I6h-.)fiN[H8&WC0!&'Cd3-A#4,d1-.CdBD-1aC0&.*m1Z&')fi*m&'q
8#4,V'C0!&'CU*,4VC0!
&'CiBD-1
,0*,^C0,i-.6C0!#cBD-1,^C0&.1C0,
C^)fiH#J&.1
367&.8,0#%&.1
3BD-11#4BDC0*Z.#4,Vp*,K3#Df1#43*m1lC0!#Q-.qZ*-H,I9s&4SqASXC^)fiHC0!O~C0&'q
8#4,I67-.)uC0!#
BD-11#4BDC0*Z.#4,;cz67-.)fiN[H8&JA(*,cC^)fiH#u*6p&.1
3F-18Sv*6diup-.)oiup*,"C^)fiH#.;Gz6h-.)\NYH8m&Yr
*,C^)fiH#v*6W&.13@-18S5*6Wiu
W&.13iu
:&')0#tC^)fiH#.;dR&.NQ$
8#4,-.6:C^)fiH#FeAH&.1C0*f
#43 g -A-8#4&.1
67-.)fiNYH
8&'#[&')fi#+r(A_7W&.13A(A_7Jt\;u"!#[67-.)fiNYH
8&'#Q_7W&.13Xr(7%t
&')0#W67&.8,0#.;Gs!&.1/*1/KC0!#:-.)fi3#)G-.6_CT9G-BD-1,^#4BHC0*Z.#WZ'&')fi*&'q
8#4,eAH&.1aC0*f
#43%qASC0!#o,fi&.NQ#WeAH&.1C0*f
#)
3-A#4,K1-.Cu&b(#4BDCuC0!#QC^)fiHC0!O~Z&.8H#[-.6C0!#Q67-.)fiN[H8&;+<>Cu*,I-.67C^#41H,^#6wH8pC^-F*/1-.)fi#QC0!#-.)fi3#)fi*1/=-.6
BD-1,^#4BHC0*Z.#IZ&')fi*m&'q
8#4,&.1
3JZ*#9#4&.Bfi!teAH&.1aC0*f
#)G&.,seAH&.1aC0*6hS*1/[&Y,^#C-.667-.)fiN[H8&'#.VA6h-.)#DR&.N%$
8#
;="!#J,0*2#%-.6c&leH&.1C0*f
#43 g -A-8#4&.156h-.)\NYH8m&tB&.1Lq(#J3#Dfr1#43&.,uC0!#M1AHN+q(#)u-.6
-BBH)0)0#41BD#4,"-.6p$
)0-.$(-,0*C0*-1&.8rZ'&')fi*&'q
8#4,c*1t*C;
"!#=*1C^#)0#4,^C*15eH&.1C0*f
#43 g --8#4&.1567-.)fiN[H8&'#v*1C0!#MC0!#-.)fiS-.6:BD-NQ$
HC0&'C0*-1&.8sBD-NQ$
8#DRAO
*CTS,^C^#4N%,u67)0-NC0!#%6w&.BDCYC0!
&'CY8*j.#J$)0-.$(-,0*C0*-1
&.8d,0&'C0*,f&'q*8*CTSlBfi!
&')fi&.BDC^#)fi*2#4,C0!#J$)0-.q
8#4NJ,u*1
VeAH&.1C0*f
#43 g -A-8#4&.1t67-.)fiNYH
8&'#I9:*C0!t3*b(#)0#41Cs$)0#DfR#4,cBfi!&')\&.BDC^#)fi*2#+3*br#)0#41C"B8&.,0,0#4,c*1vC0!#
$(-8S1-N%*&.8i!*#)\&')fiBfi!S g &.8mB_'& 24&')Y#C[&.8;VG4..}\;F"!#%BD-NQ$
8#DR*CSB8&.,fi, BD-1
,0*,^C0,K-.6c3#4B*m,0*-1
$)0-.q8#4N%,IC0!&'C+&')fi#J,^-8Z'&'q
8#%*1$r-8SA1-NJ*&.8pC0*NQ#QqAS&X3#C^#)fiN%*1
*,^C0*BpH)fi*1/tN%&.B\!*1#.; W *,
C0!#%B8&.,fi,I-.63#4B*m,0*-1$)0-.q
8#4N%,oC0!&'C+&')0#%,^-8Z&'q
8#[*1$(-8S1-N%*m&.8C0*N%#[qAS&t1-13#C^#)fiNJ*1*,^C0*mB
pH)fi*1/N%&.B\!*1#.;U"!#oB8&.,fi,cBD-'O W BD-1,0*,^C0,s-.6PC0!-,^#K$)fi-.q
8#4N%,C0!#KBD-NQ$
8#4NQ#41aC0,s-.6p9:!*mBfi!v&')0#
*1 W ;(<1l/.#41#)fi&.8V(C0!#B8&.,0,IBD-'O>BD-1,0*m,^C0,:-.6G$)0-.q
8#4N%,"9:!-,^#QBD-NQ$
8#4NQ#41C0,I&')0#Q*1lC0!#[B8m&.,0,
o;
:!#u$(-8S1-N%*&.8_!*#)fi&')fiBfi!S *,:&.1l*1f1*C^#K!*#)\&')fiBfi!Sv-.6UBD-NQ$
8#DR*CStB8&.,fi,^#4,oi .VrG .Vr&.13
K67-.):&.88(GC0!&'C:*,:3#Df1#43vqAS=H
,0*1/-.)fi&.B8#YPH)\*1/QN%&.Bfi!
*1#4,"*1vC0!#K6h-88-]9:*1/[9s&4S.;



oG


G


BD-'Od




i\cP~'fiw~^o\~'>~]sfi4\~"~p~\cd\fi~\\
]




fi:










ff




fi














fi

fi





















fi





"


!

fi


$

%
#






&


!


'
(





)



&





fi
fi
$


*
$





"











+









/
,

.
+


!


0
8
7

>
1
+
!
fi/?" @ + 2
.fiff-3,/
.4+
!+
093
5+
2!&fi
<-! ;6
>!
+
+85
.3fiA9!
fiff ff
C4
BD,: .-+!&!'
0)-;,/
.++!!0Afi:<-
!+ !&=.fi
$!fiT*$S/UWVY (X Zff[] \J E<) FG>^_FHIF`IJ
L+Ka-b/ ffcdfiff$ fiEefaJ
b6(L
99
gFh1
fi"! M!HBC
fi<)fiff!fiA H!&`$= fiffL
IN= )FY$==fiff ?!OP
/!
+% +1RgFQ
!j

%k
fiQ |j;} fi)!M 4L
!filfi+fil>^m~. $9 !I==`Fn N=j !&
!L
`;IoLX $p=qrLUffs p.tL ffZff`t+[ !uhvwyIx$=zt-Fn ffG=! {%!
k ! L!fic%=5`
l

fffi+!
;fi+
( !! =fi!J=.D AQ
|Y ff Q)gW?2\JOi4}
Q_%!E
!fiAS/=Uv1s Fo-%!8ztft"}!%Q! /| I?M=H0=fifi2 ffFA (
fi>fi8J^.
$ !=!fi^8 Nef8 !&fi+ ?
J
8a,( ff1b8. ff+j.
3cYJ


+>9Y.<).
! N ; !#<)!$ A#8*4Y%=$ #
*95g
+ 4 . !fiA<)8
!%$ ef$=D L!fi8$= ff$G#L*!"Y ffJ>
8 $ ;ef!fida )mb8$m="
ff?j+ ff; 0
!=
fiA+

c
Y.
g !#+fic
ef$=L ffAJ
(+$#*Yfi)N <)! TdA
ff1 !=fi?
2G>:8y ydL>:y=ykI)>fI"W


Fn!&fifi>!fiEfiff!&fi3$ ff!l=ff.fiff<8 !`JF>


TFY
!r` ff$=L $ *n! WF+=

+T$= ff>$!= !=!
!)=

%%!&
0?m!,(09<8h
.
I= fiff? ]

#%

T=Oi

fi).+N ff
]FMn ff
+h

fi=0


+ ff+L! ffA+
$= ! !! ff=fifffiff

fi)Y $&
9filfi ff M! !50-fiff
#%<"+ 4=..!!4 fFGL!
3hj ff J
L=I=fi<
! IA!==fi
!.
ff!I=E5


+/4;.
Y0=!
;?Ecd

T`\fi

Y< c
J
+ 9 E. !&AfiE
4=L
1fiT.n>!fiE!
g E EJ0

fi!1

?/! fFHMJ

(! !=+fiffcY=g. !&E0Lfi$.= fiE!! !=!
ffl
(!&Y0.?
N !#M8Y=

ff9

b"NfidFL

a-Ib !=ef$Fn0=LN=Ffi!
ff g-!&fifi#%!

c
!I&! fDFL=!!fiL0=fikI
ff=fiffFn >8==;$=! L
Y% !~$
*n$
! " f?2F ;
3afi+fi:bj
aeffi$b/fi+=!c

& ff
)+!
5
0cg3<)
J4
I+<-fiff ff ff+!+
+!$ fffi+ > ff ff

0=A! fiff?
1

. ff$>
!TNY )
!`fi+!=\G

=niE$fi.< fiE! fi3 3 ff+A
fifi& !fi #%! 0%!!
WFLfifi
!:
J
+4&
. !A0!H ff4 J
+4
.I =lfi!! G!=


fifi
!I0==fiL! !=N fiE

2
0=!l!fiEfiff
ffI+=
fi!0=! g!= ffJ

<35
++4?".Y=A.!& M!=N;.%fi
!=fi3 ?

!M
fi(fi !fiffJFn!04 ff+. ney
.
$=
I%Hfiff=fi ff>0H<! ff MfiE # + 9fiff ($<)=!
y? iL$*>,(! fF`!&fiEfiff ffL+
!&fi$fiA
!a=fi(b ff]\~
)fiIfi+= .Fn!9=0H !&
j fi ! I=LFn&I
==fiAFY$!&==
ey !!
!L /!=! ff
++j
+fi1&&
g Fh !=!Y0Hfi3fiAJY=
fi
$fiff ff=+
!0=! g! ff=J

<3n +
ff+!
0fi ff !=
fifiEfi+! +
$*n
=!g0 !

y? ff,(A
I!N= fi.! ! !=!

nfi
fiff =!&fiT#%J
)!&$! f=F
?/ !
!=
%(fi
!fi !
0
!0
=.1! 5fiFn!!30Ha!b`I!fi2= Fn j=!

: !;Lfi/L!& 4a b!

$ .E
fiff=F`
!fiMNF9fiff=0=&.. !fi=fi!?L0l

#%!! 0> ffETfiff=$=. !=! L!=





= 2!(90

L0=.+
Yfifi II
J
!+L ff39!A&
I=Fn. 2= fffiff!
!= 0!L
c=
fi/ E3N
.9<8I
k'n
fi .!+2
g& /!!++..fifi

$$fifi
! (!fiEF>!&IHl0
$*YI=+
kN
! E
I5=
Mfi! fffi!lI
:J
+$=
! R!=!
fiff

$
?!,(04NfiffFH$=fi fi !!fiff#%1 !!=! fF;fiE
fi.0
0

0
1! fi EM ?
(
,




























<

fi




<
L
fi































l





ff



+



&
!



!


h
0
)
<










L









0
=




(











87
cN1
;;J =J

E+F!
! F!
n!'
fiff F ffl!fij=
(
LI 9)IL=
fiffI 8 $=L$=L$*$*
!% fFL
fi fi+!fi"a b/)?N,($=L)$
*+! WF %!&
! WfiF fi

/ 0
=
J=
!! !
fiff fffi-!fi3Y.!
g 3 ffL $*!fiff ff$
/$=! !=

fi"=F;J


u







fi8%+NNn%%=
n
GL$Lgffffh4$$$nLn
ff3T+$]$
=l

ff%==+
" /ff
hk
$=`&
$

=+)
]
%
&;
j5
2>


/(ffL
ff;
ff`Eff1ff$ ffff;nY
ff;

k+>$N=
=~l
+
8=N8ff

+8
f3+

$n ff
4$>

; ffj ff>ff $I;
(g :

+($
=+%ff;
AJn


ff; ff LN;4A

$=$+ 4+
TgM "GH
4$(nff 8$

T$=)=
dN&)
)+J
l$ffn

$- ff->G`$=-=
l
I=
T%ff
8
Y)$
$ )
(=A
2A$=N(
EN
+
$

ATff==















fiff



ff
ff

ff

ff





"!$#


%&('*)+& , %-.%/)+& 0+132 1 0+&4&4%&56%7809-.:;%<2 1>=?.@4A;BCA;DfiEGFHI=J:K=L@M%7
0M7N=-O)QP61R%-S=:Q0 1<7UT'HV0+:W0X'N-S=:;%>YJ%&5$-HI=Z%&4%-.%/0+1[7-\0X-S=7S]^8_G=J_G`;=J:;7Z)WPBa0+:K=b2I0 %:;7dcfe
ghTS-HI=
);2i=:Q0X-\)+:*7]bFHi=J:K=jc(0 & ,Cg0+:K=fi7=-7)WPk1l%.-S=J:W0+1l7*^m0+& ,CDn%o7081R%-S=:Q0+1pTS-HI=q5)N0+1 ]+r
# 2 :W)s`1 =_t%&i7-\0 & ';=
u% PmP)+:b=Jv =J:wx%&4%-.%/0+1y7J-\0 -S=6Tz0C2{:Q);2i)|7;%.-.%.) & 0+1m_O)N,s=1 ]8}
7;~4'H-HV0 -y}
h@-HI=J:K=d%7L0b7N=KN~4=& ';=sNAAK|)QPC);2i=J:W0 -\)+:b0;2X2 1R%.';0 -.%/)+&i7O-HV0 --.:W0+&i7PJ) :;_-Hi=
%&4%-.%/0+1y7J-\0 -S=L}
-\)90x7J-\0 -S=L}"7;~4'JH-HV0 -}"p DOr3HI=0;2X2 1R%/'*0 -.%/)+&u)QPmceg%&6}k_G=K0+&i7
-HV0 -} cU0 & ,PJ)+:-HI=jPJ) 11 )+Fy%&587-\0 -S=} > [ g0+& ,[P)+:0 11I2{:Q);2i)|7;%.-.%.) & 0+1yv|0 :;%/0s`J1>=7Z-HI0 ,X)8& ) -m)N';'~I:k%&xg+^3} $% Pfi0+&{,)+&41RwG% P} > r

(



+




P





(

J








N








N








8


=










J




+

4






)




H








G






+





E
















$$>L
l$=G===
jA

ATN1
g % ]4J
+>45
+>4ff ]4Affff++ ;JE


:=ff+ y8
>J

ff
+
2 ] + N
NA
AJff
+4LI`=+
ffff];
ff+=ff+ff
+ yr
&Tl
ff];`$ffnffffg
r +
&

4 ffI

ffJ+=L )
~
(ff
;
9
+
+
j +&
/A ff$;
ffI>=
ff]ff ~
I$9T N>$

3j
ff ff1> =
(
+=8M
lN$nY&ffff
gN&
% 8+N =%
8ff+5
+ 4y
8 (
l N)
g% ;KJ
$=+4+ff9ff+


`
ff= = 8$==ffffffff+1$$>
LIff=
Y$= K

h&L+H =
-ff+
L
(h=4 ) 4R

ff)ff+ ;4$n&ffff$4
ff==)$==

-" f + %
)

4 =T


TI+ ff
&+
ff ff
I(r$ n+& ffG ff
-
+9
%NM r
&ff+

])r )+&ff +
k
4ff
~)
ff
(

-
&1; ff Y+;Y$n$-l

=I
d+d ff

I+
ffE gl ff2+

( Nrl
+&I + ff
3GffL+
g3
~
Iff
J
+9 - -I 3EffJ
+E
nI

=Lg8
:Yff+y

8ff






l

J




+

4










ff +

/

NGM&ff
ff ff % ) & -4 4ffIff$+
ff+3
Tyr

+=)LJ
N3
:ff1
L V|V

2 1 0+&4&4%&5%7

3Hi=82 :W)s`J1>=J_)QP=KX%o7J-S=J& '=Z)QP97J) 1l~i-.%/)+&i7CPJ) :G2{:Q)s`1 =_%&i7-\0 & ';=J79%&';)+& ,+%.-.%.) & 0+1
+ HV0+:W,Xr

:W)N)QP

ff



+






x

C











ff

dff



+
U



+


q


ff






+

Z C


J 8












9

N
4

x 3KiVI{; [fff


ff



ff

J Lf?.@4A;BdA;DE

Dh"70 -.



|j7J0 -\A; ANA; A;y AA;WA* XA



[eIAA*[e A.NAAK e70 -.

@

B

eW A;V \36|o
+





[e





JAA*;





[ANqUj X
+

8

+







j b

fi. ANAK e7J0

+

clcqG c
9\ ce A\ec Al ce A. AK e70 -

CAAW
CVI{* q
}
Q m@ ff}

ANA*
b3AAW

qIJAAW
me|A;y



fi 4X

biW*O>4N((C38K 8xWNViN4J9JX4K>WKoiZ qI J>UiXW{;+W *
!>#"pN8VG4XWq{;+W *$ &%(' ) K4*b4++$
fi 4
ff
fi K48+fi
fi,- $ )
fi 4. fi0/132546 487 ! % N %9:
41 <;[>=V X K K3? N
ifio4 K>{WK+WA@
WGOKK+WA@B*4*+<@CED B41 GF+4K *IHJ4 OK >iK X
K KK4OIHL4 OW >iK>XGF3? *K X4CNM*Nb W;iPOQ=+>iNmWSR3NN(RN{
T@
U4*{X* K X44OVi6{K4G+V@
0W 48 XZY \[^] (_a`b@
R fi dce fi
Wi@BK4x4+j@CaD k41
-Z fg""i*>GWNViN4JLsK+ y*+W *K3?Voih@
l L!mY n[^] (o`8 KK X - W
fi Sp
fi Vi;k>6NNK *Ciq7*Wb$ fiV
% ' ) 4 /12546 i*">W Vr>mN WWbi#""4+4fi KK XCNsWhR (R N


K+K>(7NMt[uF+4K *kf>mK+Kov7 +
w

x X
4 K X4> 4>ib>q>Wi84+*i 4K NGX6iOKNJX4y = 6 piq{ >^O
iXOoz4 *+*s l JX44>K X4{> IiXO>{K}|k>>N WWGig"4+7{44>id
> o>ikJX~ I>S> Kj: F+ib4KV omJX4WW;4JK>iOO4X4iW*Oo4>WK>jFi*>i
4>i4+*44> IiXO>{K>Ok 44WN x * k q4K N>8fj F+iIFi*>i
4>i87 *WCXiNKKN8y IiXOo Ni UWW;>i4+8K4KNWNKG 44>+W> F+i
* iNO*4N?I"Mii oi W4+444WXC ;4OWK 4JNi X>j 4i
KN iN:F+ij * >3*4KNWNWNLkqiX44W*O>4oWK>:Fi*>iO *4oi4+XiNKKN W;iPO
=+>iN !JXK>i Nj. JKb.>G4X4iW*Oo4>WK>T IiXOoIK>Cg 4G4NOINiWNi
> .>4W*O>4oWK>A{ >ViXG>3KoCg[ 4 J4K XKK+WS"p fii KN *4N~F+i
JX~4K+K Xb ; [oj N+ O>hfj K pXiTF i;>ifiO 4>i*44j>biX4iW*G>4>WKo
IiXO>{K>Cb" fjU * ib4K N W }= N>>a |
F+4bJX~ I>SZ KK>> 4>i>A?Ii#"M>iK !/ypP> 4i8 ]6 Lig"L4+
> K*>z> 4>iC>7 WkiXO >>+TP K xT OSJX~ W X*K !o S8 W; JK+ > 44>4
444KVK JK>mKNKW*>JK X4XOi> d*+W *G4 yNC>= NWK X+WNOVqjP> 44 4d
b
?IWWgX 4f6 ]6 ;+F * JK+!o k 4 = NXZ " 8= KxK= KWIK JK>
KNWW;>JK X4
u~~GP:u8EqiGGQ QG{MizGGunzuz mQZzk:E^GQ{


46= mi=I>WNLW= *VW* >+K X43 JX44>K X4> 44oimW[V4 Kd7NUp N L *LZ>+

F+i*6+*94KW+*+WZ>*KiN>i6W* 4o+K X JX44>K X4j> 4>iZWUV4 Kd7N
p >N $ *LZ>+ JF<i7 *W 4+xo8i(4}c{KNbKXW* >+K>iM> K*><> 44>4ZW
4KXK>K X4V X> >d4bNJV4oi6 INiK X4C M> 4NF+iJ KKN{XiN4J"pN
> INiK X4C 4o 4d>4 C>XW C>M> KK>Vo 44>i dXi> O4#=
W= *[4dcKNOINiK X40F+iWNJX4>KKixoCi*4KNWNK+K XU b> 4nG> 48+K
PWNJK4+kO3ZKK+WOWid*+W *WdVN4WN bKV4>ibK4JNKW kKK+W
F+im *d>KK4 (Nd7 pWU l >imK4KNKNsK+K XG {V4 Kd7 +K XO#= !i>4 KoVKK+WN
4 444JKK osK NN:rzeINJK X4 5V ] 4 5V5~"pI4*{XK"pS"pV iX>iC4o
Z>}? xoU> KKo> 4>i."<iK8o 4OK>S (N MZWNViN4Jx q*+W *G4+
+KkINiWN9JX4WNiK!= 6 x 4iWK+W ifiN^=I KX4ONs+W[N *{;+K Xx>4 qO
X4X4 ~?Ii#"b>KN i8+m4j> 44>4fiK>C JX4 K X4> 4T4#= WCq+ WC64#=
4dcKN 4ik4dcKNk *4GWK 4JN l iGN^=I KX4ONskO6{C4}c{KN
X4dcKNjINiK X4 3i > b 4G4KON8{qiX44W*O>4oWK>= NKc{NJKoiq>
INiK X:F+i4dcKNsy*N(X4WNyKNV4 *NKX JX44 K XP> 4y G{+ 4Z NdVCi7 PO
>i8JX44 K Xz> 4 QNJKj" osW*WK+Wd4+*NJK[ii*KNs[ 4xN+>

g6

fiV*}yPZd!q g

(3(6 A6P} Z~6 !ei}6MP(6^b(Z3b*6j{*3(be{U>6Z(6z
6SP!!Zjb*Z6(63} SG Z}} Z>!<6X} Zm(I{+S+ *hg
I!(*Zz3( }6MZ*!ZAXiP6} EZgb{*3(*+(S{b>6(6e3
(*h!6>iA!^(Zz(3(
ZqX6hT ZZ!} Z!Z P(6b<}y!(*Zm(3(h3*!}*6(Ig
j} Sz!*!Z!ZuZ Z!q{+P(6Z6z Z< ZZ}! ZP!ZGZ!Z*(6S!iP6}
5>!IP!X}}~Z*6(6^Z(3(Iaq!8+( S3( 3<S36+(Z}} (6
q(*3} : Z*Z!ZjZ6^P} ZS6^ (3(++( S3( h3:6*SX~T X^
(*h!6V<Z!3(*+3bP6(6zV!~}STm Z}} Z!V3*bZ6(6(6
!>6! Z 5>5S 5>5>
Z.*hUZ!bA}}}.!Z}XM(3(6Z Z(S!Z!*8<3y{
!~{uZiZ3*!!A3*!<!SS~3Z!X3} Za<g#>*!~!X}A6(63}
VG{ (({ uVZ!} Z6(6!M Z(*h!Z!0(I>6} Zb*( Z*!
Z}iZ!6,!Z(Z6+<}8(*!Z}X3(6
}6,!(Z6Q~iqmT Z*!(




3<~Z}(b(ja3(*+mI*
><h(<


fiff ff







b<* eZ

A3 Z!(I(:!}(*X

VZ3<(S!Ig!P6} ZP6Z

><S*UZ!h*Z3*( 6!~A!Z}!{(3(66Z
<S*UZ!~*Z3(* 6!hb (3(66

$#%& ')(* )+-,.
#/0ff1)(*$2)+3,4 5!
6ff789 ':( ;+
=<?>A@ #%& '0(CB+@ED
@ #/0ff1(CBF+@@ BHG JI
K
LM
NLO
LP -#QR R0(ST+ #/0ff1)(S:+ USVGWLM
X! ZYJ[,\@ ]@
^
/_
ZK
H`
Na/bdcfe;a/gJh
]b
ie
ig
-h

_


"!

ZSZ3e>ZA{h3( SXX~Z!(
Z
j<M
Q aEITz3(*G<

*Z~Z3U6*{*3(~Z ~!*} 6eZ >S!(6
I+mX^(*M} Z6(S{*3(*<X6(68>
VZ3a<
VZZ8Z3+
(*3!
{
MZ
+
Z6~U>>} jS6Z!
(6!6}i8{*3(M<~+<}8!+!Z{ j

ZZ}} u!(*h!6A~!T!Z!!u(3(6S A!Z3! ZU+S !
6Z}6je>6! Z3M6*Z6jh a(3( ZX+!6SX+A!<:6(63}
ZZ}! Z:!Z!8 >Zd6 V> }6*UZ!3
j<
!b(
Z{ !} Z{33*!3}6<Z3jZ6(6<!ZZ33*!3}6<! J*Z6(6<U!Z}Xz(3(6bZ
!6Z!6EZ.3!3}6 !
6(6MP6} Z !Z6 U*UZ!
!
ZZ} :AZX3IZ3<*SX A} !m Z6} Z<{V6e{ }} ZT*Z
(6!~X>6! ZZXZZ!}!Z8 3(6
ZZ* *}} Z3!3}6mZ(6q!U6ZPZ!j ZZ}! Z>!ZZX<36}{6A! 3}

3
UZ6(63} T!X!ZZ!Zi!e~*~V SZ >6XS v >

Z(6:Z<33*!3}6V
Z}
Z{ }} P3*X3}
6(6uM(*
:!}(* :3M!SI{ !^ - jh{ }}I!!(*

uM T!U!Z
Z8<SZ 3}!}(

v
}\!n!Z! >*3!v3X!}

>6!hzA I!ZZZX} Z{
ZZee**h!S Z! ~!m(8iZ6 QZgI<}e q*A!P Z!M!j{6Z(
~Z*}6 6ZPZ! b{6Z. yZ~!y ZS!bZ8P} Zb { ZZ.
} hZi*6 QZgS!I*(!!6 <! e*UZ!Z3U6Z>Z6AZh!}6(
!(6(!ZeXz:Z Z* X*Z!}\!6(6Z^Xe!!A Zz !

lFlFm+


ts

j &
Vno p ]bqnro p ^
Z(Cs1+Ep
vu w
xsq,y z(Cs1+Ep{,"btno p |S
fsf,~}Q t(Cs1+Epz,"}{bqno p
(Cj &W
lFl +
_

_

)

k( lFl
%


fi )Q
qr'


fi


V
r

V R R
V 66CFFV'&F&'3Zv/R&66CV FR|-66RCR :FQ|R RQ:QFQ E* F*
$&R&Z'RFR Ri | R Q& ;RR&F&R&F
V |'R&FFz&R&;5 q ZR -6Cfi
V |'R&FFz&R&;5 q ZR -{
V 6QF-V-RR&|VR6
V 6F'&F3v -VR661/R -AVF-&R&;U1ARvA
FV -A% Vv/&q RvR RvR6&'F
6F'&F3VdVfi Vv f RvR RR
&'FQF-&|FX RV%RR R '|F&F
Q FFzRFRR F%0 vN |/

] 'F&' ?Ai F FV R|/R ZR/rFH AZ|FV&'F3&R&J
9%f]3F/&&F'R&U& &' / Fv ]AFR H
FHF';-
A&R fFy -F?RR&
&kA -&' FFdRFR RF3'Q J
R'&vQFN
R
-R&'&Z &'FRR %
&' F JR&
RR F5 /d
FU/N -RQ ;Q A'AU F'RF - V RR] $ /R/x= Fd V
/R/J %? RQ ]F? &F?F F JQ A'Q RRV&
; ;R'
Z&| |FRJ1 qA|qRF&qF1'Zq 'R|QFWAd 6 /R/V-
)FFR% N
6 )VF'&F'
AdV9 'R41 ZvAd9RR
6A69 'R1
'v/Z9 '&
1 |9 'RAdvRF&vF1'
1 9RR4A|vRF&ZvQ1'
x
z0
/%90r% x x0r%t]
ff fit
F y&AFCRRNFF'&F-&'A?F F F4 RFV&R&FU
/R F FF F|R/-R
FvRR&F;&R&z ]F&6F
]$FF&R&F R' &ZF/R ; WrF'J] ; ''
RF&R&5FZ'A3FzF &R&vURR&FX&R& XFR'&q& vQR
F F'&F'Z&'
Q Ff HFRRFX F F/ R FZ
&dF5F'&F'&N%ZA&dF/
VF5RFQ RFd R Ft F
&dFXF'&F'U -i&' FF5CFRFd
&'FtRR&;F5 ZFx
'RU F'J ;&' F'&F 'Z&$A&?&|&FF ;
R
vrFF R F{ R F%FFV ZrF'VN- Z&&
F/R 5'R'&&] FVFR'FFUF3|'R:Q ZRF ZF&
A]FfV F V JJF&-yFRRFQF$F

/R ] %F RN &AJF;C&RF|F'&F'3F/R R rFF ?F
F';$ &R& $R R Fk&R RF6F'&F'iCR ] 9]F


fi
"!$#&%
'&!$()
*+',.-
(!$(/$0
1324150



687"9:<;=9?>$@)A4:CBEDGF.9HDIBG9JLKF$;MN689OPDGB7G;QDG9@/MRDGF$;@/7EDS7"T$DGF,UWVH9X/T$;B YB"O"F$;:C9[ZH\^]4_`\
YaZHb/Z_cd8e f5ghYSYiWjk_lfmonQnQn?m+Yiqp,_lfrmsYi8tj _lfquvjwmonQnQn?m+Yi8p5t x _lfquvjP_
68A79X/X~}+,QbQbQb?S}S$~Z?
YaZHb^]4_=YiW_ fy YiW_ fquvjwy c p&zae fy nQnQn c pH{|e f
A7A~;Q7"9HDSA7"BE@qM[BGO`F$;:C9oZH\/ZHrS"5Y8a_EiWjQbQbQbGiqp.<9M.R$?kWHY8_Ei8tj QbQbQb5Gi8p?t x 4\F$;
687"9:<;<9?>,@/A4:BG9J,BDGF.9HD@)6X/@)DS;Q7"9Xi@qBI69X/BS;<9HD}9M.oDS7"T$;<9HD}ZHDGF$;MsA4M$;3A6DGF.;<Ar;Q7`9HDSA7"B
j QbQbQb DGF
9HD:C9H;iDS7"T.;=@/BE;P>,;OQT$DS;[9HD}k\
6K;N9X/X)A5KhDGF$;L;P>;OQT$DG@/A4MA6IBS;QV;Q7`9XEA~;Q7"9HDSA7"B<BG@/:T.X)DG9M.;QA4T.BGX)JK;NM$;Q;68A7":T
X/9H;LDGF.9HD
BSDG9HDS;DGF.9HDDKAA~;Q7"9HDSA7"B9H7G;M$AD;P>,;OQT$DS;9HDDGF.;IBG9:<;IDG@/:<;@)6~DGF$;QJC9H7G;I.;Qr;M
$;M DQDGF.9HD@/BQ&@/6
9.7GA~A4BG@)DG@/A4M.9X$VH9H7"@/9H
X);@/MDGF$;rA4BGDGOPA4M..@)DG@)A4MA6A4M$;A,OQOQT$7"B@/MDGF$;.7G;OPA4M..@/DG@)A4MCA6vDGF$;IADGF.;Q7\
6vM$A3
9H7"9XqX);X/@/BG:@/B9X/X)A5K;$K;=F.9V;6A7":T.X/9H; EYWc d8e f mLc e f _|68A7E}+,QbQbQb5S}G$
Z?9M.68A7
9X/XG$BGT.O"FNDGF.9HD
.\
F.;oBG@/Q;A6=DGF.;sBG;QDA6 6A7":T.X/9H;oA.DG9@/M.;687GA4:BGO"F.;:C9HDG9ZH\/Zs9M.ZH\^]@qBA6 DGF$;A7`$;Q7
YG Qk)5GaQYWc _S_l}G,G\

8[ MDGF$;
X)A,OG,BKA7"Xq;P>$9:<
X/;DGF.;68A7":T.X/9H;$;BGOP7`@)
@/M$ DGF$;.7G;OPA4M.
@)DG@)A4M.B9M.
DGF$;=~A4BSDGOPA4M
.@)DG@)A4M.BA6wDGF$;A~;Q7"9HDSA7"BI9H7G;=DGF$;68A4X/X)A5K@/M$368A7}+,Z?4\
cI e f ghYSH,I f m+)"HS f m+HW P/" fquvj mNEI fquvj ms)"HS fquvj _
c jSe f5ghYSH,fm+)"HSfmH
W Q)`fquvjmNEHEfquvjm+Q/GHGfquvjk_
=
c e f ghYSHW P/" f m+)"HS f m+I fquvj mNQ/GHG fquvj mNEHW P/" qf uvj _
cI e f ghYSHW P/" f m+)"HG f m+ fquvj mNQ/GHG fquvj m[EH
W Q)` qf uvj _
F$;687"9:<;=9?>$@)A4:CB9H7G; 9B68A4X/X)A5KB68A7},Z?4\
EHf Hfquvj c e f
H fy EH fquvjy c e f
H
W Q)` fy EH
W Q)` fquvjy
c ef
EH
W&P/" fy H
W Q)` fquvjy c e f
Q/GHG fy EQ/GS fquvjy c jSe f
Q/GHG fy E)"HS fquvjy c e f

EH,f Hfquvj c=jSe f
H fy EH, fquvjy cI e f
H
W&P/" fy E
W P/" fquvjy c jSe f
EHW P/" fy HW P/" fquvjy cI e f
E)"HS fy )"HS fquvjy cI e f
E)"HS fy )"HG fquvjy c e f

F$;BG@/:T.X)DG9M$;QA4T
BI9H.X/@/OQ9HDG@)A4M[A6|DKALAr;Q7`9HDSA7"B@/BIM.AD9X/X/AK;~KF.@/O`F[@/BI7G;Q.7G;BG;M DS;oJNDGF$;
68A4X/X)AKI@/M$36A7`:T.Xq9H;6A79XqXG,3 ,ZH"]"&CBGT.O`FRDGF.9HD
9M
R6A7I9X/X~}s,Z?4\
EYWc d8e f mNc e f _

vA7";Q.7G;BS;M&DOQXq9BGBG@/OQ9X
X/9M
M.@/M$9Bw9BG9HDG@/BS9H
@/X/@/DJ.7GA
X/;:R9Bv
7GA~A4BS; J 9T$DS9M
<,;X/:C9M
YaZ]$Z4_` @/M=9
.@)DG@)A4MDSADGF$;9H~A5V;68A7":T.X/9H;@/DBGT,OP;BDSAI4@/V;9BG;QDA6.X/@)DS;Q7`9X/BDGF.9HDw$;BGOP7"@/r;
9Mo@/M.@)DG@/9XBGDG9HDS;9M.9C68A7":T.X/9<DGF.9HD$;BGOP7`@)~;BEDGF$; A49XqBQ\F$;M9BG9HDG@/BS9H
@/X/@/DJL9X)A7"@)DGF.:OQ9M
~;=T.BS;R68A7rM..@/M$<9<DS7`T$DGF,UWVH9X/T$;=9BGBG@/4M.:<;M&DEDGF.9HDIBG9HDG@qBa
;BEDGF$; .7GA~A4BG@)DG@)A4M
9X~6A7":T.X/9H;\F$;
DS7"T$DGF$UWV?9X/T.;Bw6A7.7"ArA4B"@)DG@)A4M.9X&V?9H7`@/9H
X);B|cd8e fSSNS}s,S} $ NZ?@/M
.@/OQ9HDS;KF.@/O`F3Ar;Q7`9HDSA7"B
BGF$A4T
X/r;=9H

X/@);DSAC7G;9O`F[DGF.;A49X/BQ\
?".a^"W8Qk^5WSk`4?55WkWk5 ff
fi5W ^$fi5~H5" v)"S
r?"`fifi5~5W5
"!$#%
W fi?w? Ea"$ fi?`Wa5` 5 ?

&'&)(

fi*,+-/.102-43)-

5 687:9<;ff=>;ff?@;AB'C/BDFEAGE2HJIKEAMLNDFB1DOE2A%CQPSRTPOCQA%?
USV)W4XQYZ[YV)W4\"]^Q]_\"WQ`\ba[cJV"defc@ghZ[`,Z[i4\bZjk\b^lZ[i/cTg m/aa[c@WnZ\"WQXo^Q\"`fZV"dQ`[c a[p\bZ[Y_V)W4`ZfVqZ[i/cTV"^c a\bZfV"a`
ZfVrdsctchuvc@g m4Zfc@XwGxSyzZ[i4c{U|ivm/agi/}~%m/aY_W/{Z[i/c@`[Y_` jV)`fZq"c@W/c a\"]ghV)j^Qm/Z[\bd2]clW/V"Z[YV)W4`V"`[m4gi
jq\b^4^2Y_W/)`t\ba[cc@ffmQYp\"]_c@WnZtZfV~m/aY_W/jq\"gi4Y_W/c@` w:,V1c p"c a'YZoY_`oW/V"ZoW/c@ghc@`[`[\bayZfVghV)W4`Y_X/c
jq\b^4^2Y_W/)`FaV)j\ba[dQYZfa\ba[yTV"dQ`[c a[p\bZ[Y_V)W4`ZfVT`fc@vm/c@W4ghc@`V"V"^sc a\bZ[Y_V)W4` wc,ghV)W4`[Y_X4c aV)W4]y`fyv`[Zfc@jq`
Z[i4\bZJ\ba[ca[c ^4ac@`fc@WnZfc@Xdvyo2WQYZfc`[c Z[`V"$\"ghZ[` \"W4Xi/c@W4ghcZ[i/c^Q]_\"WQ`X/VtW/V"ZJi4\'p"cZfV<dsc\bdQ]cZfV
a[c@`f^sV)W4XtZfV\ba[dQYZfa\baY_]yqghV)j^Q]_chu<dsc@i4\'pvYV"a,V"%Z[i/cTc@WnpYa[V)W4jqc@WnZ w
cghV)W4`Y_X/c aNghV)W4X4YZ[YV)WQ\"])^Q]_\"WQ`MZ[i4\bZ\ba[c2W4Y_Zfc`fZ[\bZfc"ffZ[i4\bZY_` "Y_W\"X4X4YZ[YV)WTZfVZ[i/cY_W/OV"ajq\bZ[YV)W
V"d4Z[\"Y_W4c@X\"`V"dQ`fc a[pb\bZ[YV)W4`@ffV)W4]_yq\J2W4Y_Zfc\"jV)m4WffZ$V"%Y_W/OV"ajq\bZ[YV)WY_WnZfc aW4\"]4ZfVZ[i/c,^2]_\"WkY_`Sm4`fc@XY_W
X/c Zfc ajkY_W4Y_W/,i4Y_giV"^sc a\bZ[Y_V)W4`ZfVt^c aFV"aj\"W4Xi/V1Z[i/cY_WffZfc aW4\"]`fZ[\bZfcqc p"V)]p"c@` w~i/cghV)WnZfaV)]
QV1Y_WZ[i4Y_`Y_W4XQ`V"S^Q]_\"WQ`Y8`J`[Y_jqY_]8\baZfVlZ[iQ\bZV"$W4YZfc\"m/ZfV)jq\bZ[\V"ac@vm4Ypb\"]c@WnZ[]_ylZfVlZ[i4\bZV"
^4a[V""a\"jq`|Y_W\`[Y8j^Q]c^4a[V""a\"jqjkY_W/K]8\"W/)m4\b"c,YZ[i<YZfc a\bZ[YV)W<V"a\"V"ZfVb}`fZ[\bZfc@jc@WffZ\"WQX<`Y_j^Q]c
YF}Z[i/c@W}c@]_`[clghV)W4X4YZ[YV)WQ\"]_` w~i/ctW4YZfcl\"jV)m4WffZqV"JY_W/OV"ajq\bZ[Y_V)W$Z[i4\bZY_`Z[i/cY8WnZfc aWQ\"]`fZ[\bZfcV"
Z[i/c^Q]8\"W{XQm/aY_W/chuvc@g m4Z[YV)WMg \"WdscgiQ\ba\"ghZfc aY c@Xdffy{\o`fZ[\bZfcqpb\baY_\bdQ]cZ[i4\bZghV"a[ac@`f^sV)W4X4`ZfV\
^4a[V""a\"jghV)m4WffZfc a@w
USV)WQX4YZ[YV)W4\"],^Q]_\"W4`<,Y_Z[im4W4a[c@`fZfaY_ghZfc@XZfa\"WQ`[YZ[YV)WOmQW4ghZ[YV)W4`<\ba[cp"c a[yGchu^4a[c@``[Yp"cdQm/Z<Z[i/c
Wvm4jTdsc aV"s^Q]_\"WQ`N,YZ[ic p"c@Wk\T`[jq\"]_]/Wvm4jKdc a$V"`fZ[\bZfc@`S\"W4XV"d2`fc a[pb\bdQ]c\"ghZ[`$Y_`p"c ayiQY)i",i4Y8gi
jq\b"c@`^Q]_\"W`fc@\bagi{XQYg m4]Z w`Z[i/c a[cY_` 2Y8Wl"c@W/c a\"]\Zfa\"X/ch}Vbdsc Zc c@W{Z[i/cchuv^Qa[c@`[`[YpYZy<V"
Z[i/cac ^4a[c@`fc@WffZ[\bZ[YV)Wk\"WQXqZ[i/c,XQYg m4]ZyTV"2W4XQY_W/^Q]_\"W4`@)c\"]_`fVTghV)W4`Y_X/c a$jV"aca[c@`fZfaY8ghZfc@XqOV"ajq`
V"ghV)W4X4YZ[Y_V)W4\"]^Q]_\"W4`Y_Wc@ghZ[YV)W4`/wvwk\"W4Xo/wvwvw
82ff %kOGTM2@O4J F GS 2 %
~i/cQa`[ZMOV"ajq\"]_Y@\bZ[Y_V)WTV"QghV)WQX4YZ[YV)W4\"]n^Q]_\"W4`m4`fc@`M2W4Y_Zfc\"m/ZfV)jq\bZ[\OV"aNa[c ^4a[c@`[c@WnZ[Y_W4,Z[i/c|Y_WffZfc aW4\"]
`fZ[\bZfcJZfa\"WQ`[YZ[YV)W4`$Z[i/c^Q]_\"W<jq\b"c@` w$~,i/c`[mQg ghc@`[`fV"a`fZ[\bZfcJV"M\`fZ[\bZfcJY_`|X/c Zfc ajqY_W/c@XdffykZ[i/cZfam/Z[i}
pb\"]_m/cSV"Q\"WV"dQ`fc a[pb\bdQ]cO\"ghZ\"`[`fVg Y_\bZfc@X,YZ[iKZ[i/c|`fZ[\bZfc"),iQY_giKcg \"]_]ObbObV"QZ[i/cS`[Z[\bZfc"w
~i/cZfa\"W4`[YZ[YV)Wm4W4ghZ[YV)WQ`NV"2Z[i/c\"m4ZfV)jq\bZ[\Tjq\'ydcghyg ]_Y_gY_WZ[i4c`fc@W4`fcZ[i4\bZ\"Wq\"m/ZfV)jq\bZfV)Wjq\@y
a[c Z[m/aWtZfVk\q`fZ[\bZfcYZi4\"`V)W4ghcK]c FZ w
\"gi`fZ[\bZfcV"S\<ghV)WQX4YZ[YV)W4\"]%^Q]_\"Wi4\"`\"W\"`[`[Vvg Y_\bZfc@X`[c ZV"V"^sc a\bZfV"a` wc`[\'yZ[iQ\bZOV"a\
)Yp"c@Wo`fZ[\bZfc"/Z[i4c@`fcV"^c a\bZfV"a`\ba[ckh2ffh_[Y_W<YZ wM\"W<V"^c a\bZfV"aY_`Sc@W4\bdQ]c@X<Y_WZ[i/cJg m/a[a[c@WffZS`[Z[\bZfc"
YZY_`|chuc@g m/Zfc@X{Y%Y_Z[`^4a[c@ghV)W4XQYZ[YV)W4`|\ba[cZfam/c"w
WX/V)jk\"Y_W4`Y_Wr,i4Y_giV)W4]y{^Q]_\"Wchuc@g m/Z[YV)Wzjq\@yrg \"mQ`fcgi4\"W/"c@`Y_WZ[i/cqc@WffpYa[V)W4jc@WffZ Z[i4Y_`
OV"ajV"T^Q]8\"W4`tY_`t`[mg Yc@WffZ ,i/c@W/c p"c al\^4aV"dQ]c@jY_W4`fZ[\"WQghcY8WghV)WQX4YZ[YV)W4\"]^Q]8\"W4W4Y_W/zi4\"`l\
`fV)]_m4Z[YV)W4Z[i4\bZY_` 4Z[i4c a[cKY_`,^Q]_\"Wl\"g ghV"aX4Y_W/qZfVt`fV)jcKa[c@\"`fV)W4\bd2]cTW/V"Z[YV)WV"ghV)W4X4YZ[Y_V)W4\"]^Q]_\"WQ` 4YZ
i4\"`S\K`[V)]_m/Z[YV)W\"`Z[i4cY_W4XkV"^Q]_\"WkXQY_`[g m4`[`[c@XkY_WkZ[i4Y_`S`fc@ghZ[YV)Ww/V"a|Z[i/c`[Y_jq^Q]c a$W/V"Z[Y_V)W4`$V"^2]_\"W4`
Y_Wvc@ghZ[Y_V)W4`/wvwq\"WQXo/wvwZ[i4Y8`Y_`W/V"ZZ[i4cg \"`fc<`fc c u/\"j^Q]c/wqY8Wvc@ghZ[YV)Wo/wvwvw
~i4cSWvm4jKdc aV"Q\"m/ZfV)jk\bZ[\,YZ[iKc p"c@W\`jq\"]_]vWffm4jKdsc a%V"2`fZ[\bZfc@`Y_`%O\"Y_a]yi4Y)i\"W4XKZ[i/c a[cY_`W/V
\^4aY_V"aYQm/^Q^c aSdsV)m4W4X<V)WtZ[i/cJWvm4jTdsc a|V"%`[Z[\bZfc@`,W/c c@X/c@X `[V^2\ba\"jc Zfc aY@Y8W/Z[i/cc@W4ghVvXQY_W/,YZ[i
a[c@`f^sc@ghZZfVZ[i/cWvm4jKdsc a|V"N`fZ[\bZfc@`Y_`W/c@ghc@`[`[\bay"w|vV)]_m4Z[YV)W4`|\ba[cQa`[Z|`[V)m/)inZOV"advykamQW4W4Y_W/\
Z[i/c V"a[c@j}^4a[V'p"c a,YZ[i<c@W4ghVX4Y_W/)`S,Y_Z[i<\`jq\"]_]sWffmQjTdsc a|V"`[Z[\bZfc@`,\"W4Xt^sV)Y_WffZ[`SV"%Z[Y_jqc"\"WQXtZ[i/c@W
"a\"X4mQ\"]_]yY8W4gha[c@\"`[Y_W4Z[i4cJp\"]_m4c@`V"NZ[i/c@`fcJ^Q\ba\"jqc Zfc a` w
~i4cFV"ajm4]_\bc,FV"aOV"ajq\"]8Y@Y_W/JghV)W4X4Y_Z[YV)W4\"]^Q]_\"WQ`V"sZ[i4Y_`NOV"aj\ba[c)Yp"c@WqY_WqNY)m4a[cbwch2W/c
@ b '[ wgi4c@jq\bZ[\vw_\"W4XGvwz`fZ[\bZfcZ[iQ\bZkOV"akc p"c a[y`fZ[\bZfcZ[i/c a[c{Y_`chu/\"ghZ[]yV)W/c
ghV)W4X4Y_Z[YV)WwKgi/c@jq\bZ[\lvw_}vw`fZ[\bZfckZ[i4\bZFV"aJc p"c a[y{`fZ[\bZfcqZ[i/c acY8`chu/\"ghZ[]yV)W/c`[m4g ghc@`[`fV"aK`fZ[\bZfc
OV"aTdsV"Z[iZ[i/cZfam/ck\"W4XrZ[i/ck\"]_`fcpb\"]_m/cqV"|Z[i/c<ghV)W4X4YZ[Y_V)Wwk~i4Y8`Y_`W/c c@X4c@XrZfVc@WQ`[m/a[cZ[i4\bZKZ[i/c
'1

fiS2Q/ffQff/Q24Q/_2/Q)1QQ
K OM4
ffMfiM 2
!#"$ %
&' (*),+- (/.0+2131314+5 (/6


78:9;:<<ff=?>5@BA!CEDGF'CIHJLKM@BNP33ORQTSGUWVRUT;/VF5X H!Z
;:[T\-;:[W]B[QT^_]39G;/VR`a8'[Wb0c3C Cdbfeg8:7h@BN
K OM4
@, ff i!jk mlT !KFn !op jflM, 2 q , !
rs!sTEtu#"?s!s %

r&'s!sTBvw#"?s!s % v
ORQiSIUWVRUT;/V{F-X H

7
:
8
x
9
:
;

<

<
B

R
=
z
C
:
F

C

H

J
n
K
@
r&r's!s tq+Ws!|sff}Gtq+2131314+-sffsf~dt
r*s!s c vM+-s!|sff}EvM+2131314+5s!s~ v
78:9;:<<ff=?>5@BA
c
*lj 8, mlT !

c
K OM4
2jfjf4, B *lT !
s! #"$sI

33

7 8:9;:<<ff{>pDC CdRJ'CEDB=RCzFJ_Kn@BAWOIQTSIUWVRUi;/V=X F
jfl fPln@, ff j mlT !
s! - % u
% 5s!sTtu#sI
&'s - % "0 % -s vw#s R yc
LTT lT MPp ijflT ffj

c

33

78:9x;:<<>
DC Cddff J'Z
DB=ICzFJ>5@ CIHu>5@ N

|* -sTR ip !w13131/py(:#x
33>5
@BAffCd=?>w@B ?Z
|*&' dx -s c z+2131314+ ~ Wsf~ 78:9x;:<<TF
{>pDC Cd J 33 y(
c
c
;:[T\WU]39R]RR =
cBC C J
/?

`a'Q9R]
[TSE8\T`[8:7hSE8'[T\T`aVR`8'[T;:<!i<;:[TOx`VRUWQT[9R]BOdVd9G`SEVd]B\uVd9I;:[TOR`VR`a8'[u7Qi[TSEVR`a8'[TO

Vd9I;:[TOI`aVR`a8'[_7QT[TSEVR`a8'[iO$@BAD4LCG J
@BA8:7ffi<y;:[TO$;/9R]]B<<az\]E[]B\f 8:9I^LQT<; OdVR;/Vd]BO{VRUT;/VVRU]
i<;:[]E]BS3QVR`a8'[uOdVR;/9RVRO`[OdVR;/Vd] ;/VVR`^_ ] U]SGU8'`SE]78:9ORVR;/Vd] `O;/9R`aVd9I;/9R:Z;:[T\8:*`a8'QTOR<
\8]BO[T8:V{OR;:SE9G`SE]x:]B[T]39I ;: <`aV:SI U ]B^; OdVR;/Vd]BO{VRUT;/V{VRU]i<;:[S3;:[i[8:Vff]`[V8_OdVR;/Vd]BO;/V{VRU]
OR;:^_]VR`y^_]:SIUT]B^;/VR; ;:[T\ SIUT8*8'Od]VRUT]OIQTS3SE]BOROd8:9xORVR;/Vd ] 8'[uVRU] ;:OR`O{8:7VRU]Vd9IQVRU|/;:<Q]
8:7VRU]2SE8'[T\T`aVR`a8'[n;:[T\nVRUT]5Vd9I;:[TOI`aVR`a8'[P9R]B<y;/VR`a8'[fgSIUT]B^;/VR; ;:[T\ ;/T<aq]E;:SEVR<aPVRUT8'Od]
8:ff]39I;/Vd8:9IOxVRUT;/V;/9R]]B[T;/<a]B\-`[uVRU]S3QT9R9R]B[*VOdVR;/Vd];:[T\-78:9UT`SIUu VRUT]T9R]B SE8'[T\i`aVR`a8'R[T O;/ } 9R]Vd 9G-Q]:!
}
}
}
}

9Ri]3
VRU]WORSG U]B^;/WVR;
`y[ `a'QT9R] ;/9I|]


UTW
]OI`a 3]BO 8:7VRU]7:8:W
9I^L QT<;/]
T9R ]BOR]B[mVd]B\u


R
R G
R G

]B[TSE]VRU]OI`a3]8:70VRU]xU8'<a]OR]3V8:7,78:9I^LQT<;/]`O8:7,8:9I\T]39
} -

W }

|




R 3G





fixaT'

i5y:RiaRx{:I5E:_a:i{TITiERBiETy:iy:TxaR5z{
dR/dB3
TIIB/R
ff
fiaBwR_'yaxy:ILT/:
'
p:RRi_RT/
R:ER
"!#%$&')
(*$+&!,-' :T.
/'10n/I:id33 2//ia:

465-7ff8:9<;>=@?+A BDC)EGFH465-7ffIA BJ=:KDC
465-7ff8:9<;>=@?+A BDC)EGFH465-7ff8:9 CZY
465-7ffIA B]=:KDC)EGFR46537ff8:9<;>=@?+A B]C
465-7ffIA B]=:KDC)EGFR46537ff8:9 CZY
465-7ff8:9 CY[EGFR46537ffI+A B]=:KDC
4 5-7ff8:9 CY EGFR4 537ff8:9 ;>=@?A B]C
465-7ffIA B]=:KDCbUW46537ff8:9 ;>=@?A B]CcUa465-7ff8:9 CY

LNML)MPOQEGFRL)MLTSO
LNMLTSXOQEGFRL)ML)M-O
L[S^L)MPOQEGFRLTS\LTSO
L[S^LTSXOQEGFRLTS\L)M-O
LNML)M3_EGFRL)MXLTS%_
L L _EGFRL L _
L[S^L)M3_EGFRLTS*LTS%_
L[S^LTS%_EGFRLTS*L)M3_

LNMLNM<OVUWL)MXLTSO
L[S\LNM<OVUWLTS*LTSO

LNMLNM-_`UaL)MLTS%_
L L _`UaL L _

,IRd"2/:_:RR'T_B*RRT/R/Rd:fi2RBd_:Ii/R3TRBdB*dI:TIaRa'2iTERa'T:E'd
TaR'T:fi:i3eTRB:i_:Ii/BREGaff R:-3d3IBxiI5:!3G/d:I
/REB3dBf:b
fgh^id*Z_RIB/Rj

:T
k lfiaBR'ax:Ii/:

L)M-7
L)M-7 noEpFRLTS7 nqL[S7 n*EpFRL)M-7 n
r GB/Rjs:Tas>LRT/BREGaffdR/ddG:TRaRa'iHfi aB:co^tuvw:Txfey^id*Z

R'axL:Ii/:

Lz5-7 n {a46537ffI+A B]=:KDC1{ (\$&3!,3' n|{ L[5L 5~} OQEGL 5"}u7 nu)M
Lz5-7 n {a46537ffI+A B]=:KDC1{ F (\$&3!,3' n|{aL[5XL 5~} _EpL 5"}u7 nu)M
Lz5-7 n {a46537ff8:9 ;>=@?A B]Cc{ "!#\$+&X' n|{ L[5L 5 } OQEGL 5 } 7 nu)M
Lz5-7 n {a46537ff8:9 ;>=@?A B]Cc{aF "!#\$+&X' n|{aL[5XL 5~} _EpL 5"}u7 nu)M
Lz5-7 n {a46537ff8:9~CY{ /'10 n|{WL[5L 5"} OaEGL 5~}7 nu)M
Lz5-7 n {a46537ff8:9~CY{WF d'b0 n|{aL[5L 5 } _EGL 5 } 7 nu)M
r GB/Rx*:T*>fi aBu:fg^id*Z:iWlR'ax:Ii/:
m7 5N{WL[537 n { /'10 n{ (\$&3!,3' n"Epm7 n
bm7 noE - m7ffMN{WL)M-7 n~zU m7 SH{aLTS7 n -
M-7 5N{WL[537 n { /06' n{ (\$&3!,30 n EGlM-7 n
lM-7 noE - M-7ffMN{WL)M-7 n~zU M-7 SH{aLTS7 n -
S7 5N{WL[537 n { "!#\$+&X' n|{ (\$&3!,30 n~EGbS7 nbS7 noE - S7ffMN{WL)M-7 n~zU S7 SH{aLTS7 n -
7 5 {WL 537 n { "!#\$+&X0 n { (\$&3!,3' n EG 7 n 7 n E -b 7ffM {WL M-7 n zU b 7 {aL S7 n -


RT:G:+B/Ra' E'TiaRa'T:i:i/RT3d3IB /finRh2/:T/Ra' :24/Iy/iaB
L"LO TL @L*_ 4e:7 :i 37 ,k/:2Ei3aRw_B:i ::TRT3Rw/R
/uaB:d
z{uff'RRaiyaRaB3
+:2 L:I:%iaRa'W:?E'iTaRa'T:i:Tx:E:_iaL:iR::I:
-Ry_iaTR::G:-y:'T/:xRpE'TTaRa'i::T d3I/Ra'f,:'+:2 5_BIT:iR :
EB3RRT$i:Tfy_i3R+
fi R3TRBRBmdBfi RR/2 :T/R'Tf:R?/:R%z _B*Ra'B42 /Iy/iaB3
pG'dRy/dd3x:ad3IT/R:2 ffB3:Tdax_:RddI:'md:R/If
r :RRi_{T*:2 /2 :T/Ra' E ZX RT/:RIa'TdIRd"/2 :BdRd3 :
TR:ff'RRa'T:|24/Iy/iaBRT/xy_i3R+fiR3TRBdB*_E'TTRa'T:!i:f:T-Ri/I:_3d3f- =
RT/xyRaB:R-:RTi:uEB3Ra'fHxTREBTRyW'RL
EB3dBxRT_iy3aR+fi
^*

fig| + d| D|
Jd
J
xb-)e-H
6H-
%/*\ 3 3 \+ [T\ Xe3 : Rw
Q l3*% + gT\
J H3 " 3 R
P J e "[cw
3 J H3X T"[XT:Hw
- b 1H J P eNd J 3
J1
H1
3l e 3/% 3c%/*\
3\*- [ e 3u 3 \3+d3\:\ *3* x h-
T\ g 3*% : XRw 1 T\:3 j -
T\ 3%d*\ [ / *33X* 3 /ff
l6 3 fi
z *+ \ /
"!#$%!&'!()*!,+-(%)/.0+"!&1&+"!&24365&!&1&+(%1%
1 g c87/+ g3* + 397\3 \ fi x :T\6 87 * ;7*+
T<7\3 + fi X >= *? \j % %
@x\ \A % + 3B *j 73 + bj3*
% 3 + +x+*3%dT 3;7:j6
u3\3- -* 7 \ 3 = l 6 387d+

-* + fi X 3 u 3-d\ - eT\ X + h j 3 fi c
3 3 + 333* X [l 3 / 3 + -j
T\ 1 1 3 u + fi c+ \3 + T\
3*% + bRu 3 3 % 3 - 9C \3
3 j3W\ fi % 3 Q uc3\%*-b 3
e 3B +* 3 %* 7 3 + X 3\ 3- ED * +

7 - + Q l: u3\3- Q -* + z
FHGI,JLKNMPORQ%SUTWV 3 \ l:+o T\
HXZY8[]\_^>`0[baZcdcdc*9egfihRHXZY8[]\_^>`0[baZcdcdc*9ekjXHXZYl[nmpo,[rqtsdXBuvX
HXZY8[]\_^>`0[baZcdcdc*9egfihRHXZY8[]\_^>`0[baZcdcdc*9ekjXHXZYl[nmpo,[rwx^>y0mp`
{z X{X{|>[nmPo&[rqtsdXBuvXZjXHXZYl[nmPo&[rqtsdXBuvXdjXHXZYl[ \_^}`[~^9efhRHXZY8[]\_^>`0[b~^9e/j0hRHXZYl[ \Zody0s
{z X{X{|>[nmPo&[rwx^>ym`ljXHXZYl[nmpo,[rwx^>y0mp`0jXHXZY8[]\^}`0[b~^9efhRHXZYl[ \_^}`[~^9e<j0hRHXZYl[ \ZoBys
% + Q z XX{|>[nmpo&[vqtsdXBuvX /z X{X|>[ mPo&[vw^>ymp` u X fi 6
HXZYl[ \_^}`[acdcdc*9ekjXXYl[ \_^}`0[b~^9e HXZYl[ \ZoBys 3 fi ` 3* 3
- R ) hRHXZY8[]\_o_Bys
C h b : \3- kD * + k
R +87 :+o
x + x \ ) 3 *l < c fi \T j X


fix;&d
6&%p"P%/&,ZP%Z<%&&&%
r_UxP
br /88}kU ?bP /88><P} bxPff?b /88}kP 0bb
PBxBU< 88 bp } bR{&x8H"b 88 n}v P 88 P
N8p"{LZ&"
r_U"P :b %88}9l "Ub PBxBUR{< 88 b p }
r_d"P <b N88}9 0B"P
"Z&"
UHb
tP%&%%&L{&"
B>UH"P PBxBU< 88 b p l{] {?&?,>x
& &P&"P%%<&,&%%
{_Ut P R b 88> bp /88}kp} bB P
{_dtb /88>< } B P
{_ dtbv P ? n 88}<v p N bB P
PBxBU< 88 b p } bR
fiff:R{&
?8H v 88 ,

}v P 88 P
&l Bl
&B ,U &Bxb & &l{B b8>b R "80>bB

8{!#"$&%('dBv
fil{)*+"
-,/.>0p b&1324*>b & 8>bB6
>E&5,;
b>b 8>bB $ 176859 d&:B_ dB B ;9B ?BU=<{8 &BN&>,U?1
BRpBA@ BB, U& C 8,b b{l 8 C*bdU& b D&5,;@E1
24 BUF: t" 80>bB9
>
, 8,5 ;1/6B& : tB{b6 8>bBH8G5,5,;
5 {8B&b

8Eb
& >A H B 4
BI ;
& & =: ;: >J ;
8
&l/
=< {8 K, >BU;{ L B{UNMO =PQ& ?1SHR $@ 8>bB & ,5 ; $- 8>bB L B
B88B T:@U@ 8b d$ 88PB 8>bB
, VP b;1xW 8 {!+"
&%(d' Bv
X
BUb
Ul {)*+"
-,/>. 0p bB& dB8
B/5 B0 ;B9 ?1
BYt@ &><&>& l J, U& > B9 1ZR [< >:\>9 > ,5 ;{ P & =PQ
0@ 1 =<& {6&>-&E& {l
& tBH & 5 ; " 80>bB >?b?>-

b@ B > dbl
& > BUb1] 2 p PBb-pB6BU 8>bB:&k BU
8,5 ;
8>bBH&B9 K5 8R
$, $R Eb& b &^ =< >b 8@ ^ =< 0@ 1


B8 ?=_< {8 4 l _ { @ Rb>b @ 1 1 BA6@ &U 1 >b{
&>&U, U
=< {8 L b> k b>b > -@ N
* @ B 1 b>b{t&>], U
8
B_5 9 6@ &; 8d,: ]U N` 8 b>b{;1R8
8>bB
, x& , ,5 ;
a)a)b

fic7dQe$fAg[hji$kjg[le$mnkdQe
o$lNg[lFdQe[p$qKrqsp$e$f
tFuwvx[y{z;|
}}yujv3~TvvTy8tv~C
}yz=u

tFvtu
~]}AyGvT}A|
yGu
H~TDy{Itv~C~Tvz=u

tvtFu
~]}y{F~Ty
~zYx[yUF!vx
y~TyUz=u

tvtFu
~B}yUu[v>|
NQFFy?Qvx
yDy;}YvT}ItF~Bu[vV
$ty ~zAx[y]v
Vu$fiK8u]|[
y;}|
u
K+}Lvx[y~At;y/vx[y/+}A(|$Fy/tFuUt|[}Ay>tF~0s4 - !K[#)
^0KT[#)nQ G/ KA*#{0s4 - !K[#){0s4 - *[#sE^=^TT[#?x$tFzAxUt~}Y[y;}
#Q(0KT#$HYF)T - !^T[#4
8$S+ [}(vx[yD$z~>7}AFJvx[y]yu
z=$tFu[EtF~V~>+Fs~>+}VVH)fizAx[y]F
tyF
~vx[y>+FstFu[K+}A^|
Fy>vx
vtu

tFz;vTyBx[yufiy;}YvT}A~/}yI$$FtFz;$Fy
0 8 Q; Q $- QE 8 Qn;
0 ? 0 ? TT
; fi Q- fi n
? ? TT
$- J fi 8 fi_ fi $-
ST ? ? TT
$- n; fi fi_ fi $-
ST ? ? TT
!u[y;}AvT}VtF~>
QFtFz;$yvT}Au
~AtvtuGvTEvx[yU~|
z;z=y~~}V~TvvTy]tF~BU[y ~zYx[yUF)YSu

vx[y;}tF~Ty>vx[yV~TvvTy^~Tv ~/vx
yI~Dy ~AzAx[yUC
E 0 fi E fi8 j
Q 80 ST
0
zYx[yUv;F(u
nF tyFvx
y>#FF)tFu
^+}A^|
Fy

L LT
zYx[yU{F tyF
~
#}BFSKAj?vx
yI+F)tFu[U+}A^|
FyIvx$v[y~z=}YtyVx[yuy;}YvT}A~
}yI$$Fty?
0 E 8 n; fi Q- E n 0
E $ Qn; Qfi $-
Qn;
E $-j J; fi fi/; fi $-j
E $-j n fi fi/; $-
8u
3Qu
F ~zYx[yUv]u
nU~A x[yuEy;}AvT}Y~/}yIu[v
$Fty
$- fi n 0 0 L 0 L 0
$- ; L L
Qfi/; Qfi $-j LST L
fi/; $- L L

x
y3F|
vtFuvx[y
}~tvtFu
L}AtF$Fy~ D[y;vTy;}AUtu[y~(nz=u$
tvtu
$Fu x[y

}z=y
|[}AyBtuEStF|[}y>Uy=yz;|[vTy~vx[yVtFD$tFz;tv }y;$}y~TyujvTyE$Fu?
)

fi[sQ

>


ff
fi

"! ($ #&%'!)( $ *,+ "-.!
/(10 * (&(2-,34"5 * -,5Y76$!8/9 9 *;:=<?>A@CB DFELHGI*,+)J
9385$ - +)J # #&- + -,K-,34"5 * -,56 * 5$ 5L! *,+)J
-.%M-,K 9)3=- - +)J # #1- + * 5$NK * (F
9)"5$P#F + -Q-,3="5 * -,5A6$ !8/9 9 *;:=<?>A@CB DFELHG
*,+)J 385$ - +)J # #&- + -,K6 * 5$ 5/!) *,+)J -.%R-,K 934- - +8J # #1- +
ff
> F
U

* 5/NK * (

V #&W.!5$YX ffZ $5 - J !5$YK-,5; "! # + W * 38( *,+

>
ff
fi

"! ($ #&%'!)( $ *,+ "-.!
/(10 * (&(2-,34"5 * -,5Y76$!8/9 9 *;:=<?> @CB [ EL\G
*,+)J
+)J +
* *
> 9)N 3)5$ - # #1- ]-,K^-,34"5 -,5A6 5/ 5/!
U

V #&W.!5$N_ ffZ $5 - J !5$YK-,5; "! # + W * 38( *,+

`2acbda1egf7hjilkmnimQo=pdqlrsp)ktdpdmnulvwodp8x2mRusvzyn{)p)|2i)x2us|sm
} 0, $#c%R38(1"5lKC-,5/%~-,K)38( *,+ # J + #18 97# j+ "5 +)* ( $* -,K * 38( *,+ # 9 9"!5$5$ +jS #&%R]3=-.# +"
-,5 !)#1 * (1 + (&0 .* * /9 #&%M]34-.# j+ 9738( *,+ % * *,+ ! + - +)J # #&- +)* ( 5 *,+ $# #1- +^ - * $!)"A-,5
$* * ,*] *
!* $ 9# j+ + " 5 +)j+ * L( $+)* * $-.* # + "# J *,# + 9 *,#&+)%RJ 9"5$+ #F J + + - + " J * KC-, 5($"3 * * 5 * $*
5/# (1 9 5$"3)5$ $ # "5 7( n-,KA38(
9 - # W#P3 5 #&"!)( 5/(&0J/#&%R38(1
+

+
+
*
+
- $# # Wn-,K^- V $L9 % - (10,
4< F E;;@CB [<?>;@B [8<?FE[d""<?cE[d7<<?C E[d""<?C E[EE
KC-,5 * (&(,6 *,+)J """ $ 9"5/^d$$ < 6 LE """$&8]*,+)J F < 6 E """ $
9D $#&"'-,K 9D -,KKC-,5/%'!)( * nKC5$-.% $/9 % n
* F# U-,K-,5 J "5nL&$ <E 9'3)5$- J !5$
+# V #1W.!5/P_n "! 9N3d( *,+ 5$"3)5$ $ + J 0 9 5/! 9 * (&!) -,K 9) * 5/# * (1 >;@B [
Ul8Csc V -,5 9 (1- -,5/( J * %R38(1 9)NK-,5L%!)( * * 5$ * 7KC-.(&(1- 7KC-,5 z
FB [<?>;FB [= , [d 1/$ [dA< 8"1L [d [d "&$ [EE
NB [<?>B [= , [d 1/ [d7< 8j& [d [d 1/ [EE
FB [<?>;FB [= ,8&/ [d 1/ [d7< [d "&$, [d ,8&/ [EE
FB [<?>;FB [= ,8& [d "&$$ [d7< [d "&$, [d d& [EE



fi]d8/j8j18d)8&d8n.88
l&Q8=Cdfij=j==?sdC
~.))1$1.8,dd&," Mc
,&2ff fid&)$1.
,)
A$,4ff$1$1.),/
fi81 ] ) $" / Y&)&$&, $ 7,8, ) .j$& , 8"1 M,! $1. ) $ ,"
,.,#
$
$%&)' ($" /
)) *+ )A
,*+),j$ -8 .0/21435.06879 /A & -: *+),0
$ -8 ]; , ;8&, $" $ j <
fi>= ? L& fi81 & @ "A /CB .8E , ,c=.j$& , 8"1
3FB,)R )G /n;
, ! $&. 6 $:H=n ; 0! -=)1$1.n
4*+),$G -8 H]j.& , C
J)&
B
C

," d&,K
/ ,& 0, P
$G . R j L /& fi8& ;&M
3NJ) fi=4ff $G fi8&
$O )'

/& fi81 &P
3QJ2 fi4P1
.&",& =&)"4 ) j $R P
M,> ="
B4; 7ff , EB2 P ) 0,' ;

.j$& , j2 ?, $n"4 ) j;. , , E$2S
Y!
R81n& 5 fid1 T+)7]
/&4 B4G (Ug .
fi4P.M
U $D%L T,"" )5 /& fi8& ;&W
3 1
.&",&G =c)"4 ) j B7] & U",8,
V B V ",),2
*+),$ X=E
, AN.j$& , jA ?, A)G $ $ =M
A7ff@ 8 ,NM"&
$G Y=Q
$G . R L 3 R )ff
)L $" $ jQ,&&E 7] ff fid&)$1.
0, R,)fiw ff $ ))z, B7,) .) =
$*+)G $ )P! )
! $&. & ,)& MI ,.,Z $J
U C
R E$[n) / Afi4,

^ N,1 L)$G , &fi4 1; 7@$
]?_<fi4 `

"5)cW

&81$&,A $P
7;1 ,c7 $,4ff $1$&. Cfi/! /1)
7;& cb0B
\ "^

g
X
h

,)e
d,fXghXiI$ Q&& /G=w K
C
J)&W
C
Q`
,.,) fi$! /&)
7;1 kj $ \ "<
lmfi4n
C
J)&
$" / $c R8&, 7,8! $&. ;
A8 $ $ &K
$1. &p$qN,)<
p$sr+$
t4uGvwuyx{zJ|,}~#'+'~ZcD|,'X},+c}4'}

-"A,1 /)$ ,@ 7ff5 N
0&&& =IL&fi81A;F
9.&&E79$:%9D
"5)c ] _
/,
NY,N*+)G,1
"5)c)]?_ cN)Y))$,
M,
$R$ff'sA$,4ff$1$1.
& ] _ fi , " ]&RN)y)8(B0^)
@fi4A$"8c, Ifi+=)Y))
)ff"
,)<
5 $ ) J $81 B , =)' Y8))
"/ )]
s!,$G=)?/
R?/cfi81$
{fi4 ') Y))
] _ $%9P+ fi4 &w

0c&& =L /c fi81 5 Q,&1
\ "?
&j , &
$
;*>),4M&
wWBn )A C
7, =) Y))ff $N R) 4 $ j 0,
$ . R j
0&&& =P /& fid1 $MR9 n,n4E 7] R
7] Bff! -=^


DC
R,&
$ \ "?
fi4 C
58&
mqEr
R





$$
$


^ P^ 0

^ M^ 0

( M^ 0


( `


Z( `<F>4ZF




/" $ j$$1.
R/
fi81 &$,8N&)A"
$P

%&

.0/21,3 .0 k^d fXg0hXi l

/ .$
ALL&fi815 ,) /&fi8&@C
'.j$&, )"&571ff
" $ );&K]?_B),) ' "A
$,4ff$&$1.),#L&fi817,&</e3C$
79 ^3


Cs /" $ j ) ),j$ d"$1. , ) /"&81$&,&$ ^7]P " 7]
&&& L& 81
, 8
4 "
N 8 ,& $G=aq 1
>
r+$%&
P*+
E

2 Z
%

`
C

0
= fi @
n fi

Lr
r B
*> G
EE

fi9G;ff


G
!@K>DYffGE)FX
"5'YX
&
Yff+!Gffs+s+
9 ?D D) D9 ?99@9999

w9C
,+2 0 C
! G
)

)
) fiff 5


















^

C


2








W



! 9W !
+)&+A



C 2
G
!A G )n G W+2!A+9

^C G )
G
n
G 9& G A+2W+9

C #"@
&+2 $
% 2ffIff'&0
)(+
>+*,(>
+*2
wffGGff
-"G!@
.

A/-

0(>
>+*(+
>+*Z1-
AG
>.+
JAIff'&>
2(+
23
+ *,
(+
>+ *?
4
-G>' &+Gff
2
X

-

Z 5 -

6
(>
+ *7
-'G
8
9;:5<):>=#?A@BDCDEGF>H0IDF>CJLKfiMDCDEGF>CNJDOPCDERQB;SGE;TVUAFXW;OSGEYZI
% I!ff

G"5-
[(+
>+*D\-
J
'-
'D

#ffGGff
] >@


^3_-


!9 ?'G
`,fi-

<G
.DJ
wI
n'&0Gff

M50!D<
ff

% C>Gffn

G

!@^A
5A
9XffE9
2a? ] 2b _c edDfXg2hXi !$
! ff?
F
#ffGGff
-G?
@' alk ]F % FY
J^0D
AF'
L)^+!C
ff
,@)^D'&0Gff&^
^
@G'

&0Gff!
wffna2
3_-
P
GffIn
] <a
L><G


c
9Z
-

KD
ff

j

b

p4qrsutvxw!yDzx{|% DG0}0
'<'&0
IG@^'A
A>
&YffG;9

09 ?@ D9 ?9 D)9D&99)
9 G )I G F
+)C09

ff +@E+2&5+? G ? G 95
! @
! W
!
+)
+

9
G
&
G &

!
W
+)A
+&
~

W

! 9


!
&+2
W
+&
+)
~ G 9
$

8
yz5ywtwZw2G1r^XGnNNw1w0sRNx1s
% 5Gff^<5-0GffA!ff&!LffG^ff}0^
:!
>.@wff3
GGL
-

(
, j ;-
Z.ff?X

}`
?L

GD22LG>


A>J
,}0
!
>G.5}
?ff"IA
w

^ff3
IML
-0GffC % I"

55L!ff!
C
&GffI5

Z\

fi ^^52P+^/^\

^x /Po^Z' 5)^PZ'PP/5P4P)'PP5/P) ooP/ PZ
/!0Z'/^/4/)^X AP/o ZV/PP)Z'Z;
/^!) 5PZ'P55P!1>|l|5 P5'GZ'



x \x
P1ux 1x
\ ^/ L/ ^Z|^^ V/////^^2


^ 6/P//^'2Z^5/4G
o/A)/fi>fiPZZZ0/5
P
/fiPVP6PP5/5!0P+Z / /Z^) >fi'0Z^ZuoP
P5 /4/ ^Z^')Z'
/oZA^
/4 ^ ^Z^) fi^'GZ'
5o'GZ'4/,025P
Z^ >5Z5VP
/P[5
PAx ^5VoPPX AP/^Z ' 5/ )
'PP/5P4)
PZ)
5Z 'Z'o^^P^V/P/ PP ZoVPA')Z''P55PN^
/5 D!Z 0 1 Z P 2'_ 1 \


/P
fiff 7
'


'^fi

x
x

V ^5V/ Z^ 0 ^^X _ _ XP> PZZ
^ )55PZXP P ,Gx ! Z #
" 60Z'5^/ ^ P5
G 55
PZ ^ P///PP) N 6 P ^%L
$ PZ^/ ^P^ 6/P/>
'^/1
'^

) P6G^P 4>V^Z25PVZ
^4Zx P Z)Z'fi
'>/Z; P>^^[X V[^5
xA/5
P2^ /Z \ ^[,^

&



P









/

,



P



/

P

^

'





N



^






G



Z 0P
*)


/1 ^Z / ^ ^/'PP'
&P^ 6 /P/>AP )V2^Z5Z0 P5Z+-,
. ZVP
/5V^^
('

P/ P'

/7PoX V25

/

V///) o'^'PPoP^[5 P5')Z'ZA)Z'Z
^5 Z2

+
0+

^ZV^PZ'P55 / ^PXP>^Z' // ^')Z'G/2/P) P/Z
005 P
^! >/5Z P
^!fi^A^'0)//
^Z!6 >5

/

P ZZ> PfiPP^ V>P/ /^ P^fi ^1^Z //^o'P P
5[x ^5Vx>5 //[ P7Z/0+ZV///6^44N^P^ 6/P/>
)



'


'

/5G /
^oX2/'/P/ PP'P/ P' P'5
P PRx>5 / Z
[^Z)Z'57X^5VPP/ P'P/A^
' PP'57



1+

'

2+



('

3%46587:9<;1=0>@?ACB
ED/ZXP>
x

^fi'/P+Z

Fff

^ \_52'254PZ5)Z>[P4Z'Z2

/ Z'/P/ / Z^'2P/ x AP/'P>,XV/5Z5N>7'0Z^/P,x
0Z'5^/ X AP
/x V>5Z/ ^[/Pfix
2Z'5^ 0 Px AP/Vx0P+P
5X
0Z'5^ 0/7^P Z'PP554^V)'P55Pfi^V)

fi

^Z'5)Z[07^ox/51/

fiff fi

G

XP>oX 2Z

IH !J*KIL MK !J ONP QK
!J*LK ML !J L


!J ONP QK MLVRJSKIL

IW
UKVRJSLTK,
!J ONP QL





Y[ZO\






,RJSKIL ML

,!JSLTK, UK
UL ,!J ONP QK
U,K X,!J L




fi]^`_bac_edf_
gihbjlkmQnRopjFn
qbr`sfo6tikusRmivw8xy*z[{
|pn!mUj}nRt~ksf`st
~!SV8!SI#
~!*i!STFM
!S ~!S
!ST ~!*
!VCO ` X~!VCO ` X # !VCPQ ~RVCOPQ
~!VCPQ 8!VCO ` X ~!COPQ !COPQ
U!U URM
`Q!M `Q!M
U!U `Q!MX#
`Q!M `Q!U
gihbjRsfnR%rtiMshen[RjUhbjV`s*UsfMsRsRk1V`s*Un!kuMj mUhbjjPqSj[ bUrsfsRk1UhejVnRigihertIrt
mUj emQj[tMj[OMj[XUhbjn!Msfo6rlksRmQo<en !*I ghbjFrer`UrnR2tUUn!Mj[tIrUhbjFemUsR`j[oPsfetUrtMUt~sRk1nR
Uhbjl0sftUtUrV`jIn!mQmQnRbRj[opj[Ut~UhbjlVsSQSt~:nRe@ nR0jr1rtTsfertTsfbsRmi0sRUhn!mUj
sfUhbjUn!V`jRghertemUsRVj[ohenRt~nptUsfbUr`sfErnR2UhbmUj jksRmQo6tTsRknRetTrSj[PUr`sfbfiSVnReEj
etMjlUhejFsfbjlkmUsfoSj[PUr`sfbfiSfiS8jFRj IUhbjFksf`srb<kusRmoen!jlkusRmIvwxy*z[{
|f
# (# 2 RSI `Q!U !VCO ` ~!S U!U
(lM 2 RST `Q!U !VCPQ ~!* `Q!M
( 2 RVCOPQ `Q!U !S URM ~RVCOPQ
( RVCOPQ U!U !STi URMi ~!COPQ
gsXmUj emQj[tMj[OUhbjSenRUr n!Ur`sfsRj mUhbjrer`UrnR1tMUn!Mj[t<jbj j[snRbq*rrn!mUX!n!mQrn!`j[tF
nRe ghbjFrer`UrnR2tUUn!Mj[tn!mUjFmQj emUj[tMj[Mj[SEUhejlkusfsre<ksRmQo<en!jR
R U!U URM RVCOPQ !COPQ ~RSI i!ST
R URMI `Q!U ~!VCO `i !VCO ` !S ~!ST
R `Q!M `Q!M !VCO ` i!VCO ` 1 ~!S !ST
R
gihbjsfbMj mQo6sftMTjPq*rtUMj[OUrnR0SenRUrVj m~VnROUrj[tUhejVmUsR0sftUr`Ur`sfenRV
n!mrn!V`j[tbj[tUPmQrVrbFnRet
Uhbjeer`Rj mtUnRVSenRUrVj mTenRUrVj[tsRj m~UhejrVr`UrnRetMUn!Mj[t[enReUhejrVbj mQopsftMjPq*rtUMj[OUrnR!n!mQr
n!V`jFSenRUrVj[t~UhejFmUj[tMisRk%Uhbj}
n!mQrn!V`j[tTUhen!imQj emUj[tMj[ijPq*j[ bUr`sfetsRkUhejFVnRksRmVn!mQUr en!m
!nRbj[t~sRkUhbj}eVr`Rj mQtUnRVnROUrj[
n!mQrn!V`j[t
I# lM l l#lMi

RSI6!ST U!U U!U6!VCO `IRVCOPQ}I# }FM } } l
gihejSenRUrVj[<sSsf`j[nR<ksRmQoVnirUh}UhbjTPsf
UeePUr`sf<sRkeUhejksRmQo<en!jTbj[tUPmQr`0j[}j[n!mQr`j m%nRt
Uhbj0sSe6nReUhejn!0sRjFenRUrVj mtnRtTUhbjemQjPeq2*rtMmQbjr`knReEsfe6r`kUhbjemUsRVj[orVtMUnRePj
rSbj[tMUr`sfhenRtlntMsfeUr`sfXIr`UhnRjPqSj[ bUrsfsRkUhbmUj j0sfrUtsRkUropjRgihbjptMsfbUrsf nR80j
ksfeepS6nlUhbj sRmUj[opCemUsRj mTksRmTT8UhVn!mUj UbmetnlMmbUh*C!nRbjnRtUtQr`feopj[Ms<UhbjisfbMj mQo6sftM
jPqbrtMMj[UrnR`pSenROUr`Vj[6!n!mQrn!Vj[t # z FM z z kusRmvw8xy*z[{
|lUhen!mQj emUj[tMj[VnRet bj
tMsfeUr`sfXnRtUtQr`fet (V Ms nRV 0nRe Rfi[ MsnRsRUhbj m!n!mQrn!V`j[t IIj[ePjRn!Uropj
yUhbjlVsSQrt~opsRj[kumQsfoMsRsRksfMspUhbjlUn!V`jFr`kr`irtsfMsRsRkn!iy*VnRVn!~Uropj6{
UhbjF`sSQ:rtiopsRj[kmUsfoUn!V`j}sfUhbjFMsRsRk%V`s*Ur`k%:rt~sfUhbj}Un!V`j}n!iUropj{!Tgihert
sRSSr`sfVtU`mQj[nRQhbj[tUhbjFRsfnRMrtTsfXMsRsRk1T6tMUn!mUUrepkumUsfonR0UhbmUj j}rer`UrnR2tMUn!Mj[t
[


fiVQbVb`V8*eVbbVfVV
F ff
fi ffffff !#"
$T&
%')(

!+*-,/.101
32 4!#" 5
')6798:!<;>I
= ?( i#;@A(CBDFEfiG)F5
')6H!I,J7;>I
= ?( i#;@A(C2)KL.F5
K MONQPFRDFEfiE>PDKL
fiDSTE3 GU0NF2A
fi0!VBWKNMX 5
')6 ,C7Y;>I
= ?( i#;@ Z(C ff
[ 0NF2ZF ffffffQ !R" 5
*
]
,
^





#

_

ffKANFB`a 5
\
ab*-,/ cd egf5
')6:;>
= (
')6Y1 ff
fi Faffffff 1 !ahid egfj"
;>I
= ?( i#;@ A(C2)KL.F5
km*l
')6Y01NF2Z ff
fi ffffffQ !ah:d enfj"
;>I
= ?( i#;@ A(CBDFEfiG)F5
i#;@ A(C ff
fi Qffffffo !phdjq`enfj"
?(@
rm
3s.KRtu*?vw1 ff
fiG
3N0yx1KLNuz 1.1KbBWNFKZ{u.1DF0|2
~} NuNE3 DF0BNFKLM.1EfiD
m^H1nTzTg1
Y1DPFffPF E3NFx_ JDH2ffNFK MO4x1KNPFffKiBWNFK{u.1DF0|2
~} N|NEfi DF0JBWNFK M#.1E[D @
fi0|2DF0 0>9 FF"
DFGyDF0a \ 2) 01G
fiN0NFBR2ibD Po
fiG?.201DFMx1KNoz 1.KLBNFK2GD2
fiG)}TDS
fiEfi
fi2NFB&BWNFK M#.1E[Di
fi02
x1KNFx_NG
fi2
3N01DFE`E3NFs
fi bD Po
fiGffgNFsF M^DF010>`nNPF E[DF01> FtF" ^2&
fiGbG)2)KLDF
fis2)BNFKDKL2)N \ 2) 01
2DPu
[G.20DFMx1KNoz 1.KL&2)N1DF01E3&.101
fiPFffKLGDFEn{u.1DF0|2
~}ffKLGff5N0RG.1 \ 2) 01G
fiN0DF01iG)NMO

fiMOxKNPF MO 0|2GDK GzK
3S_ aSupDFNE[
@ff2DFE FF" +rm
3KLG22PDKL
fiDSEfi G{u.1DF0|2
~} pS|
2N.2)ffKLMONG)29{u.1DF02
3}ffK#DKyzN01G
[ffK >2 0Y21PDKL
fiDSE3 GRS|21G) zN01H{u.1DF0|2
~}ffK mDF01
G)NN0n \
fiG)2) 0|2
fiDFEPDKL
fiDSEfi GzNFKK G)x_N012)NNFK)0No G&
fi02G) DKLL2)KffF`.101
3PFffKLGLDFEmPjDKL
[DSE3 G
zNFKK GxN02)NDF0o0No GffUA&STDFG
fiRDFEfisFNFKL
321M
[GAs
3PF 0:
fi0rm
3s.1K#tuAAR}KLG)2ZxDKLDFMOff2)ffKb
fiG
1
3Bm2RN.2)ffKLMONG)2b{u.1DF02
3}ffKZZ
fi2DFz2
3PFPjDKL
[DSE3 GZ
[GU \
[G)2) 02
[DFEDF01zF NF21ffKZ
fiG)FT2
G) zN01:xDK DFMOff2)ffKKffx1K G 02G2O{u.1DF02
3}ffKLGff_DF01:2921
3KLi
fiG@2OM^D2)KL
\ NFB29BWNFKLM.1EfiD
fi0
ffEfiDF.1GLDFEBNFKLMmrNFK2BNFKLM#.EfiDue e LTo) ueTF ) e Fo " & e | e ")"n21DFE3sFNFK
321Mw
fiGmffDFEfiE3
Z
32DKs.MO 02G^ eg)eTjfzd Qfzd e fj zd en ff)e e fj" #A1G.1S1x1KNoz 1.K
o1 xffKLBWNFKLMG`.01
32mK G)NEfi.12
3N09DF01O.101
32G.1SG.1MOx2
3N0>gA1STDFG
fiDFE3sFNFKL
32M]Z
3292AG)2DF01DKL
zff
3 0|2#
[MOxE3 MO 0|2D2
3N02) 101
fi{u. GBNFKR21bD Po
fiG?.201DFMxKNuz .K^
fiG&DSTE3^2)NiGNE3PFN01Efi
G
fiM^xE3xEfiDF010
fi0sYxKNFSE3 M^G DF01C?YD PF1ffPF E3NFx_ C0ff2) 101
fi{u. GBNFKG)x_ff 1
fi01sH.1xC2
DFE3sFNFKL
fi21M9.Kb2ffNFK M94x1KLNPFffKR@
32:2^0ff]2) L10
fi{|.1 G1
fiGDSEfi :ffDF00NF2&G)NE3PFODF0|iNFB2
S_ 01LM^DKoGU
fi0DSTE3RO
fi0E3 GLGU21DF01N.KLG& zFFG) zN011Gff-"
A12) L01
fi{u. GRBNFK9
fiMOx1KNQPu
[0s2NFS|Po
3N.1G#DFE3sFNFKL
321M<DKSDFG) N0YBDF
fiE3 Efi
32)ffKLDFE?ff2)
2
3N0V rKff MDF0> FFu5#g
v0|S.1EfiDsDF0># FF" bDF0BNFKyBWNFK M#.1E[Du N0px_ffKBNFKLM^
fi0s
zNMOx.12D2
3N0@
322.101
3PFffKLGLDFE_PjDKL
[DSE3 G S_ffBNFKRDFEfiEgPjDKL
[DSE3 GA
fi0D PFSff 0iDFGG
3s01
2)KL.24PjDFEfi.1F#rm
3KLG)2ff_S_ffBNFK9xffKLBWNFKLM
fi0s \ 1DF.1G2
3PF^G) DKL N0DFEfiE`x_NGG
3SEfiR2)KL.24PjDFEfi.1DFGGL
3s0o
MO 0|2GU2)N^PDKL
fiDSEfi GU
fi0YDFGGL
3s01
fi0s9DF0|2)K .2o4PDFEfi. G2)NG)NMORNFB2PDKL
fiDSTE3 G
fi0 DF0y2 0
x_ffKBNFKLM^
fi0s.101
fi2#K GNEfi.2
3N0HNFB2) 0o
3 Efi1G2)KL.24PjDFEfi.1 G92)NG)NMONFB2PDKL
fiDSE3 G9
fi0HA
Q

fiZ3QT1
)L4jFfi1 RF1F[ L #1O1)R_F3 ):)yj fi3 ffu z1>OF
oFZ) L )ffFfiZF3fi:)L)yCJF1HF)Lo4Ffi ))^F@
Lfi3 U[
+F1y
ffLWFLfi9113 )fi13yufi fi1Oz) F1fiz3>1 91FU)^_
Ffi Ffi)F
A1fiL>ff) z[Ffi3 :fi3)ff FfiAu11fiA )fi13iffFFfi_
_ffFLO
Lfi3 1@ffF1FU_L) iFU& oU1LFL1fi1#LfifiFuZfiLfi3

u1F|fi ff
u)^&FffZF&ffL A1)ffLO)
u1F|fi ffff
fi _FF|@L ) LL) fi F)1[ @3

fib VWF&y) #FL1fi
Z3

11fiFffLFfi3
u1F|fi ff HLfifi ffFfi1FfiFb
)Lo4FfiFfi1O |^ffF
_^ Ffi1 >
1 |313:) L11 |
^
T3fi
L331fi1yff[F1)^)ff"
!:R F
oFg) L )ff#
!)91U9$ )ffU FLfi3 fiz^On% FA)O"
TfiF11fi1
1FT3 ^ffT&
afi' fifiA))ffFffLFfffiF1#)ff Zfii3( Vu1#ffF13FffLFgjL[3
offffLfi1y[ FL>mF1fi"
u1#ffRFFL31O |&1#1 F)_^z1fiffL fi
1Lfi U1F


)+*,.-0/2143658791;:=<?>
@ FZ L3)) A1LFFLF 1)LF1fi) z1133F
fiF11[F3 ^9)B
?F1
_ffFLO z fi1FCff fiO |fiDZ1fi z1133F+TfiF11F^_ff W1|:
ffF OE1 Fffb1fiLff1) yfiu zfiFu
fi|?) F1fi3gUF)^[ffFfi3F ffL) 9 )ffmF1_ ffL)FLmF1#?FLo
fi1m zL3_?[13fiFFRFFo)) ffGZ
1fiH FFLFwfi1fffi11 nF)^fiF ff 3
F?[LfiF|bWF1. F3 fi1)FzFIA
O1Ffi|jL[FJEK ffffofi1GL |1ffLffGMONNPRQ
[F nCMONNP?`T F#L fi) ) L11 | `FU1 L11[A) L O' Fz mfiF11fi)fi11fi1[
&F z |#fffiFLfiffF0 [F1ffLff
fiLfiF|" FYWFL1fiOR^)L1^fiFfi
F 13)) UFI F3 +fi1)FzF0V |j fiFff)ff ^fi |b ff )FLUF1
fi1fifiF_)) ffF1ff WX fiF LLiFfi)^fiz1331FY [F11fi
@ F 1$ )ffLfi 9F@_ 1 1^[oZ ff\A ff )9 L1^]
Z O))L) ^T fiF11fi1
111ff%
fi__ 1 |)Lz ?Fg11zffLFfiO1zF '_ 11)`1 LF3 /fi)F1z aZ 3o
^1zffFfi|FbA
1[_ )yF@1[#_ 1 1^[
Z fi9)Y ^1))L)13OffF_OF
1 F`F:1 F3ffb))3F9`Z
fiz1331F+ fiF1fi^1 LF3 1zFgfi1) FiF
)3ofi&zFL '_ 11fifffiF[ffF; F3 fi1F1z U)_ L) fiF%A
) z1y_ 1 1^[Z fi
Ofiu]uZ "
FLfiD@ 3)ffFffLF?fi13fiFm)) @ 3ff1&ffF 9E FffTff 1z1331F
fiF1ZL FL b3F FF`))O)[ffFffL_ 3fi&))FcA fiU_ 1L^[
Z fi
LF^ff)ffLWffd u^u1#_ffF3o[oZ ffgFD? ^ffF3FO1 FT3 eZ 3g
f 3o[oZ fi
F)39fiOF. FIuh +y
F F1i fiu]uZ b`1 LF3 1Fb '_ z3F fihkjlM =f?j?hC^M Fm?ikFmm
fi1fifiF|) ffuF9)LF1[)F1F1W
n ff133 `fi9)LFfifi
:
_1 L ) fiOFT1 )
)) @)fffiF1LF_WFL
A1o ffL@z 1 3z1Lfi)@F)O u 1z#Fu^cZ3F3@FuFL@z zfizo
) fffiF#u^ff#
p0Fz39uF@F F
F3bfiA _ >_F1i_ffWF# u ff1fiI
TfiF3fi
@
Fo
Zo Z
ZfiL>
A^FF?fi&)FiWL
ffTL)u)ifiF)ff F1fi_ 1
^[ ZZ3i9 1zo1fiWL z3D
fq uq muI
V:1fibffF)OffL9[@3`
LF^ff)ff1
fifiz F) 11Lfi
fiF) L>`^3 FF?&
fiF>
r@ 1z^^1ffF 9E 1QFff#fi"
ffL)
ffFfi3
Z3WFL1fiyR 1zo
TfiF1t sQ ff31&FA3 Fb
M`
@3FL1fiyF
3 Fu
u`FY)i>X
<^
ff )WF #1[1#[W1)i)L
fiF1# FL1
&FF`ffF_ o)LFz) >

V
T3
&3F)fi)fiffA&ffFfi1fiFmWF #1["
Z3

vOw(w

fixzy;{}|]~l}~lW{}ACy;{4}fi~ly;{l}`?}{}|
[R?^




(


k
C





'[k'O _4'O kt ] 4[ lClO
??
6
C q

CO
6 _?k
Cq


6
Cq

?k
6 6
C


OC(
C

OC_6k
Ok
Cq
(
C O6 k?
Cq

6 O??
Cq

_k6?k 6?k kl
k
k
k??RO ?k
k
C
4O6kk (k klO
kq

k? k
kq

k}W R% 46[
[l R?^2}[} WO

[lO4C4lDbRO[W? lqRqR\ z[ltI4 kk]'tlX4\[[O'Y?4'A[l[4['O'
}AOl[4"o 4 ]'? l[W?4l'[ & 0_'O_[l^]4[Oo_k4k[4X[l^'
]I}kc[}k
][O'Y?4'[l['_2} WO4[4
k["[^W_ 4c;]'a? 4^[l[k}W
?WOz[l}Y_%=[?&_[4#'O?}&[lR4Y_zH4W[ 4'[k'O_l[l#[4Wt^[l#4IY_a
_4[Oz [4R4[fi}Oz?O].4CC[l?l[[[l"R4Y_z=4]?]W[W?4}k] k}WO
D[4k].4C=[l`}[A[l]4[H}A[l&[WR[D[l^R4IY_"
4?C$WOka4RlOD[l
'Ok]t']_ 0[4^]44oa_[?l4#W'] [ z[R[[k[W?cW[ c }[RO]'
l"t46[
W[[lO4R}l`]?RO[W? lqRq k[?;k]k}W
k}W ?[4'[k[ '[_X?[l[?l[W?`'?A}WC[Cz]4[}O^ l}W
'?4[W?&;4%k]#?O 4YO44fi IC lc?Y[lO'4]}WO^az'I[k"[l}R]Ra'
"_[[&?C}?l]k[?o'[[?[44 C[l2}WC[Ct+[?l4[?l}?l]k[W?}
O]G[l[fi}[}WOe4'[}O_Y W[[l[_`?l}R]R4g?l.%[l`[l]_.}[}WO
O4C4l?c4[O[O6'OgOk]_O;z`4O`Ol_]k'O].4kI"'__t+;k]_'_"klOc[4k
]}k]'_] D[lD[ Oo[lD'?4[W?4 lg[[k['[_?WO k} k[D[lg']l
]I}k[4ka[_4[O'O%I'?l[W?'%^O]'_G].4kt}^[l' ].4k [4k
[[O[?}'^[l4?lO'
}k]_'_c lO2 4t[l_[k]l^'?4[W?}4
#[lI]4420[4[l_[OE4[(_ Ot].4`z?W.[lo?W l^ lt^k[W?
l.}t'"?4^D?WOo[l^'O[W? 4t[4O4R}lX"lO[tWYOY[l^'O?4? 4^
[l#4IY_GY}WC[C_6[4 [4Wt`[lcR4I_%Y 4W[l'[k'OOR[4?4[[^[lcWOl[^H?lO'
CO_l[W?Al_OlOH[4I;[g[l`}Y_a'[k'O" [4I} B%'_}k]k'`[?e[l}Y_
U[oY?[tt4[4[fiC[[lI4IY_0_4'OcX[l].4C}[l'_O[g[lR4Y_
4[Y?[[W?4ktk}WO_=[l&OW?[A[l4IY_o2'O?44IW"[kO.?l[l_]O`E4[_
'_k4k'"[l]I}CC[l"4[&[loR4IY_aUl?C$WOk+lR4Oz [4[Ok]]X']_44[l
'O[^[l'tl[CEkl2Y[l].4 W[^[lO4C4l"[?CO[W? lqR [l[c]
l;4Oo.OW]k'k["}[_[k;W [l_
[ ?4Y4'OX
zOl4[l"]4?
U[c^OXYO Y[[lol''O}o+;424X[4o[ o}4
4A]4[OX k}W [?4?4W9?l}][l lOB[4k[lA;[_4[O[O6[k[W?}
RO[W?} lqR lqRq 4 lqRq k[lO[O[ l?WA?}l[k[?4WDlO^4}l l]W?[W
Wa_]4[g4l[ Ok}WbWa_}Y_&IlR4O&[lg[Ok]]9'[_g[l }WC[


fi Wl;4?
O4C44 ;WR]R '[k'O [ '[k'O _4'O kt ]4[ l ClO l
RlqR





C

'
RlqR




l C '
RlqR




R

't l
RlqRq





C

'
RlqRq





ff
Cfi

'
RlqRq




? k
R
'tl
RlqRq




6 ?
C

'
RlqRq





k

'tl
RlqR

k

k k Ck

'
RlqR

k

CO


RlqR

k




RlqRq

k

C 4k

'
RlqRq

k

k C Rq

'
RlqRq

k

? Ckq 'tl
RlqRq

k


C k kq

'
RlqRq

k


? R ? 'tl
k}W 4
[O
WC[CA]
l!}W#" W $!}O4R}l [? O[W? lqRq %" W ]!}W_"O}R4 l [?
O[W? lqRq ^'& _[ ["Yo[l(!4t[l "4*)_[O}+"a?4 '_O '"44"'^
kG W' "R4IY_ '_]4C'[k'O-, [_/.0. !}} [? O[W? lqRq+ ]O0 0!}
'`'[ && &'?c c[[k'O z'__]1!Y?6[% [ 32 4!}} ]? O[? lqRq _[#
l.[4 [O''] [W?&4 #'_[% !Y_]k't%O4k;WO&kzO !Y?G [ &&^Y 4*)_[O
I?4''][?'?l[?5
!}4 k[o _[ [.l c[W c I[ !}W_2[!}[O'O[k[W?76
4^[o'Ok] 4_OlO 0 8;44 l.9 !}
W O' }R]R
"a] 9 !4[;WO 4[[4OG #'?l[? ' !;k]k': !}[}WO^%G_( &
O;& c YO'c_ [[_7!;4l_] k#^kO# O'<!}[}WO^ 4>=_4 kc <!;4
[ !4]O'O6`? !Y?[[W;W^ CO_l[W?4 6G} ?}''][.? $ !;4.k[Xl.`[ `
IO} ^k]R# k}W I'!}ktk'`_ [[_@!}44 lA!4[}WO& B /"a__'67"
?4[ l_]l` kc R4I_2 OWO^O6[# o]O[4W[4 !}42[Ok[WO* & W ,C8 '_O
[ ;W_+ !4[}O^D .c4D ^l[?D !} 4"I} ]^? !}_k'O
_[]_E !}4}lF 6R o]4[O2k]l4] !G!Y?6[ l
?^% a}'_[kk[W?4kY?l@ !;o'Ok] o?lU !G!4]? k[z 6'_[O[[l H ?l
_[Ik[o [_ JR4[> 8}_] 6l?l _[O !4[(_lROl !Y_ ] 'Okt ?k] k}WO0 JR4
[> 8}O^ & c W]`?4 ka[ !4[O'O !; CO_l[W?4 G%O_}' : !}7 6?] !4[O'O'O
& `?l'_]^?'k] k}WO 6'_ _< "cW `}4W_][ * &5J4[> 8}Ogkk]k}O ?
[lO}_WO 6+}4K JRlO* &l_'_t^l & RO_l[? k ?}4L
" W ?4o'Ok] H "o4OK &





?M
!;k9 " W ^ k?44[W?44 !}}4lD`?b 'O?49W_O2 N
!Y?* &Cl?^
W_tk] &6}l2? " W]X
!4[ 8O5PffQ7P oO4C4l?2&W ]lO'
R U ' "cI k;W 6 z]4[O
!}IOl_]k[?._.Y%.4 WO]= .4Ok='
R4Y_U 4W[R'[k'O U ?l2 2Ok]K&I?44[W?4!}4}lc]W ^0 0k}W
'o W}W
[&k (R VY k# 6Y &N!4[C44W !}4c X!Y?lO6[ +WO4 } _] [?4]4
X!?4O6[4[_O?&[ !}W0!4]}WO^0 O' (]k}Wc]4[OGk[#4l2'I?l
YZ/[

fi\?]_^O`baFc dOe aFf*^OgheX]_^GiOf>aFfK]_^FjOkAlk/jO^O`

m(nFopq(orsutGq(p'voqwxrtOy*oroz m({ m(w*pz7|3}Fpq4o ~F{rtOy*opqUm(nFotGq(p_y*or9?wKzS{ Oy*o+FzG{ *voo ~Xm;ozG(wKpzG
p?m(nFo9-{vXwKsGm(zG{rtGq(pX oGFqbom;pL-4} pzO(wKFoq{yKyp?m(nFo
m;qbFm(nXsuv {yKFo{(bw*zGroz m(-m;p
m(nFoGzGw*voqbb{yKy*#G{z m(w>Oov
{ qbwx{ Oy*o(G;mm;p#voqbwKCm(nG{ m-{9tOyx{z5m(nG{ m-nG{EoozCpGzON{ m(G{yKy*
q(o{DnFo0m(nFop{y7wKz${yKyE{;o|}GGq(m(nFoq0Fovoy*ptOr9ozffm(0wKz#m(nFopq(orsutGq(p'vXwKzFm;obnOzGwKFo?Cpq:-4}
{zG5tOq(ptEp(w*m(w*pzG{yE({ m(wx_{ OwKyxw*m{ qboyKw*oy*#m;pwKrtGqbp'vom(nFo(o+qbGz m(wKro4Fq(m(nFoq|
LO_O@
?pm(nAIopm3{zGXr9wKm(nS{zGq(pq{zG?pyKyKwKzOItGq(o;oz mI{yKpqbw*m(nGr9@Cpq3 pzGOw*m(w*pzG{y
tOyK{zOzGwKzFm(nG{ m{ q(oO{;opzm(nGoy*o{;ms pr9rw*m(roz mpqtO{ qbm(wK{y>supqbFoq3t_yK{zGzGwKzG:tO{ qD{Gw*r5|?pm(n
{y*pqbwKm(nGr94?pq($yxw*o+ pq(q(o;tEpzGGwxzFyK{((wK{yEt_yK{zGzGwKzGW{y*pqbwKm(nGr90Gz m(wKyE{9(FOp{y7wK0Gy>_yKyKo
Wm(nFo0{ tGt_yKwK{ m(w*pzpE{zptEoqb{ m;pqm(nG{ mFpo3zFpmnG{'vo{+GzGwx Fo?pFm( pro m(nG{ mwKm(nFo0ptEoqb{ m;pq
wKzFpzOFom;oqbr9wKzOwK;m(wK |4mm(nO{ mt1pwxzffmAm(nFoNFovoyKptOroz mp:m(nFo5 pzGOw*m(w*pzG{y?tOyK{zwx(tOyKw*mm;p{
zGr<Eoqp0;ot_{ qb{ m;o(FOtGq(pOy*or:m(nO{ mW{ q(o(py*vo;ot_{ qb{ m;oy*So{bn pq(q(o;tEpzGGwxzFm;ppzFo9p
m(nFo:pFm( pro?p7m(nFo:zGpzGFom;oqbr9wxzGwK;m(wK4ptEoqb{ m;pq'|nFotGqbpOy*orwKm(n9m(nGwKU{ tGtGq(p{Dn#wxm(nG{ m?m(nFo
(w*oUp pzGGwKm(w*pzG{yFtOyx{zG{ q(o:o ~tEpzFoz m(wK{yOpzm(nFo-zGr<EoqpIGzG oq(m({wKz m(w*o{zG{?ozFoqD{ m(wKzF
{N;pyxFm(w*pzLm({ o{ m<Eo;myKwKzGo{ q+m(wKro9pzhm(nGo(w*o9p?m(nFo(pyKFm(w*pzO(G{yKy*o ~tEpzFoz m(wK{yCDIm(nGwK
XwKzGp7{y*pqbw*m(nGr3wKzGnFoq(oz m(y* pzG(Gro{+yKpmp1 pr9tOFm({ m(w*pzG{yGq(o;pFqb o|}OFq(m(nFoqbr9pq(offpm;oz
m(nFowKrtGqbp'voroz mWp/voqm(nFo$m;qbw*vXwK{y pzGGwKm(w*pzG{yt_yK{zGzGwKzGN{y*pqbw*m(nGrm(nG{ mA(wKrt_y*qboGG om(nFo
tGq(p_y*orm;p{zGrW1oq7pFyK{b(wK{ytOyx{zGzGwKzFUtGq(pOyKor9Em(nG{ m@{ q(oU;py*vo<(otO{ qb{ m;oy* wK(r{yKy|nFo
{ q(o<Eom;m;oq-m(nG{zm(nFo<m;qbwKvwK{y@{y*pqbw*m(nGrnFozFovoq+;pr9o<pm(nFo pzffm(wxzFozGw*o{ q(oAw*q(q(oy*ov {z mwKz
q(o{DnGwKzFm(nFo<p{yGpqw*Sm(nFo;ot_{ qb{ m;o<tOyx{zG0nG{vo<tO{ q(m(wKz5 prrpz7|
4wxr9{ m;m(wom{y|%WtOq(ptEp;o{z{y*pqbw*m(nGrpq pzGOw*m(w*pzG{ytOyK{zGzGwxzFm(nG{ mWozGroqD{ m;o
m(nFo$;m({ m;o#;t_{ o|m({ q(m(wKzF5q(prm(nFop{y?;m({ m;om(nFo$;om(<p0;m({ m;oq(pr:nGwKbn{Np{y?;m({ m;o
wKAq(o{DnG{ Oy*ow*m(n;m;otOpq#y*o(9{ q(o prtOFm;o7|nFozpq;pr9o5m(nFo;om9wKzOyKGFo
{yKySwKzGw*m(wx{y@;m({ m;o{$tOyx{znG{EoozCpOzG7|-GqbwKzF#m(nFoAoz Gr9oqb{ m(w*pz7Eo{bnh(m({ m;o9wK:{b;pwx{ m;o
w*m(n{zMptEoqb{ m(w*pzm(nG{ m#wK{y*pzF%{(nFpq(m;o;m#tO{ m(nMm;p{p{y;m({ m;o|:p'm(nFoNp{yK${zEo
q(o{DnFoCq(pr{z Np3m(nFoAwKzGw*m(wK{y7(m({ m;o- 5q(otEo{ m;oGyKN{ tGtOy*XwKzFm(nFoptEoqb{ m;pq-{(;pXwK{ m;ow*m(n
m(nFoFq(q(oz m#;m({ m;o|:#m(nFozGrWEoq9p;m({ m;o s{ m(w*pztO{wKqb9wKzm(nFo;ot_yK{zG9wK#{#nOw*n{#m(nFo
zGr<Eoq-p4;m({ m;o@tOq(pOy*orwKzG(m({zG o+w*m(nOwK(m({ m;o;tO{ pzO(Grorpq(o9r9orpq(m(nO{zwK
yKw*oyKm;pEo9{'v
{wKyx{ Oy*o|9Ip{yKy*ovXwK{ m;om(nGwx+tGq(pOyKor4wKr9{ m;m(wom{y|9tGq(ptEp;om(nGoG;op4OwKzO{ q(
FowKbw*pzGwx{ qb{r9+?q({zffmUpq?ozG pOwKzF<m(nGo;m({ m;o s{ m(wKpz5m({ Oy*o|4--0{ q(o-wKzozGoqb{y
zFpm{ t_{ Oy*o+pSq(otGq(o;oz m(wKzFo ~XtEpzFoz m(wK{y7(w*o<O{ m({9;m;qbG m(Gq(o0wKz5tEpy*XzFpr9wK{y1;tO{ o|
Xrw*m(nW{zOWhoyxo ~m;ozG$A7GOffL4yKGr}GFqb;mGIm;pnO{zGGy*o?GzG oq(m({wKz
{zG;ovoqb{ywKzGwKm(wK{y;m({ m;o|+nGo<tOyK{zOtGq(pXGG m(nFow*qt_yK{zGzFoq{ q(o(o FozO o:ppt1oqD{ m;pqb
yKw*owxzNyK{((wx{y7tOyK{zGzOwKzFFOFm{:m(nFoo Eo m(pptEoqb{ m;pqb:r9{Eo< pzGGwKm(w*pzG{yEpz(pro{ m(
m(nFotOyx{zG+r9{h{DnGw*ovo9m(nFop{yxovoz:nFozh;m({ q(m(wKzG5m(nFotOyK{zLo ~XoFm(w*pzwxzhGw>Eoq(ozffm(m({ m;o
pq<:nFozm(nFoqbo#wKzGpzGFom;oqbr9wxzGwK(r5|Xr9w*m(n{zOhoyK%{yKym(nGwx pzFpqbr9{z m<tOyK{zOzGwKzFF|$nFow*q
tOyK{zOzGwKzF{y*pqbwKm(nGro ~XtOyKwKw*m(yK9q(otGq(o(ozffm(wKzFpqbr9{ m(w*pzpz{yKy7o ~XoFm(w*pzGp{tOyK{z|UnGwx0r9{
EotEp((wKOy*onGoz9m(nFo-z Or<Eoq?p@wKzGw*m(wx{yG;m({ m;o0wK?(r9{yKyu Ft$m;p{A pFt_y*op@Gpoz#pq0{n OzGFq(o
pz(r9{yKyUtGq(p_y*orwxzG;m({zG o3_Fm<pq9rpq(o prtOy*o ~%tGq(pOy*orWw*mwKAzFpmCo{(wKOy*oEo{G(o$p
nGw*n5r9orpq( pzG(GrtOm(w*pz7|3otGqbo;ozffm(wxzF9 pzFpqbr9{z mtOyK{zGzOwKzFAwKzpFqqb{ro?pq(5wK4o{;|
Gq$?pq({zG({ m(wK_{ _wKyKw*mt_yK{zGzGwKzG {Fm;h{zGoyxr9{zW{ q(ohy*p;oy*
q(oyK{ m;o|r9{/;pqGw>Eoq(ozG o+wK?m(nG{ m0?o{zNGwKq(o m(y*{GGq(o(4{rWObn5wxFoqUqD{zFo+pIt_yK{zGzGwKzG
tGq(p_y*or9Aw*m(nzGpzGFom;oqbr9wxzGwK;m(wK$bnO{zFo{zG;ovoqb{ywxzGw*m(wK{y4;m({ m;o|?o{G;op-m(nFo{GFo


fi*F/_G




fiff



fiff

"! # $ %! &

# fiff
'
+

*

%! # ' ()!

' ,
$

fiff

-
#
. /


0
1
)!
2
3 %4 5
' )!
76 98 ;:



=< >
,?@7A/B DCFEHGJIKEL

, #
N


# # O<P
( Q
2







(






R
$ )!
/
U
V
fiff $ "

# W

'
X Q
OY ' ;
fiff 2 #
Z ! X $
$ +
[


\

/
/
] (
U


N

fiff
^



1
[
(_ >
'M


Fbx*:(FLG(O*
? ; GFK Fb-E*F%;(F 9O* F* x
K ; F W G ((xff% (KKKG ; 7 FA;b bK (*
ff(xG*h F ;; (K
Gff( ff 9(F ; G ( U* b GK : (; G X (K :F9(F ( b GK
*(G F3(F G ( ( ( ( _ O*
F;b (K (K K(G ; ; ;(Kx ff(x9(F(
OK (G( (ff( (*
G*(*GOK F<(F ;$(F$( G*(xF ;
b GK
( bK9KK 0 _K NE G $ ( ( _ _KK**b*(O
G ( G E( 0E ?(F ( ( _ _KK*<*b*(O
F x9
F(F( uO( 9 W(G (F b9

@S ($O ; %K ;
F (N*b*(G Wb1 ; O*%;b; F ; F( F5 b*G A(G ( ( %(F ;9
K (
?F<1xff (x (F(:< G b ;:(O UK_ (*G9ff UG E ( F( 7
(F( ((F (9*b*(G bAG _ O*A G;b9KGxF(F G ( ( _ _KK*$ ;
K (
FD_E (W* ; bL*b*(O
F$F XG ;( *$(F( F(F
; (O _(G F;D9KF :*( ((K (G : *(N ((KG(E(
F F F ;
* ; D#Kb*(G _ 9 G*(*GX_KA F* F;D9KF
*
b( (F
*(F F ( ;; ( K* ( FbKFK <OxG (* 0
(KF K OGK
G ;( FO* 4(G (F <_K#x (( E G
Fb (- 0* ;4(F ;-O b U
*bK(G
G*(KGGOxGGKF+(O b (G 4WOK (( 0G -;WE ( ;; (

`acb;de_f3g5h[ikjPdUe[iTlme[npo+h+qh_rsutvdHrw
\ Gb1(-Gx
GG(D 5; G K(*GEOKOGKFW(G 4O; N(G(; (KFO(O*
K(( G
ff("O! 0?& * bG K ? 0bKFF ;9 ; (F(Su< G(> U1! G KF
OK$ N G- GG(D -E(L(Fb( K*LL
Gb (( K*.? K3(
; 2 GD (

(" (*T?
X;-(F+( /x 4 ( (9!_ OKKK9*bK(Gzy 6 F 980{Z: K7}|~~4 K
Kb FOKOGKF N FGbO* F
;bKGKF+;F (O<P F 3 G ff("O! ?& * bG K


3-FDK"8 (* (F0GbO* b ()!_ OxK*< Gb1b*(*G DG
x 2 +(Fb(
Y;(%!- (KU " ; O*fiFff *(O*(G 0F $;;b ;9(G K+KLFbF b*O*
;(X( ()!_ _KK**b*(OK]
*(*G1OKOGKF
N O/?Q G%4E# b bx*V?N
G*(KGOxGGKF9x.;xbS((1 ' GK"
(F4OKOGKF*D*(G+ U 4
#(F [{X: 9*(7O|~~M (+{? KKxO|~~D
?/ F+F( 1 GK(*G OKGGxF;-(F/(xO*; _KGGKG/*(FGG$ ((K (*
2 +(F^ WF 3(F((( GG*O((OS( (*=? G E0KK%< 9("
; *3;K" K
GFE FF4K<E*XF9x(K (,(F0!_ GxFOKS;(G(@( $ (*
9k
* ( ;(K 4 x9 ;( y)|~~W " 5%Kb*(G(G JG 9b ;A(FT;( ;
;O ? G*(*OOKGOKF0G(_* N FOK$U ;; ; +(FKI*bK(GfiffXOKK(*
;O K ;1D (*
K(x (u;( ;%
(G<xGF( (*h* W;_*NOK$ Q WF
* /:F(F(F&,R;DGG F0(FGb1(-k<(G (*_* V 9O*fiffG(O*
\ ;G x *Eh
G(;EK_* ( (*L (G(=u< G(^ R&
L
fiffM<

N
EbK ; c
*(GbM $ KF $G *(*G1Ox*(* G+(G*4 (+O(KK9xG (ff-F 4"
Y;(%!7 (*;9F
GO(b }[(KOKED G9 #
G(OK?(G /? G
E ; %;xG *
;9 (F< bx* $G *(*G@OKGG# ( G X *Q;
K(KW(G -'#FOKKG

W

\
(FJO
<E< KG*(K_;( ;K%(F#O(O*K;( EK*x #(G WfiXff G(#(KF
G %<
(*GOxGGKF3-(F(=u< O(>X
KF-(9A9k *(K3;F ( +GbF;b OG F S(G

EFfiO
! (GV ;;#$ (* *(*GF_Kff$ ;W;WG ff( ;b GG F (G [ OFUE
J G* (*<E F 5
xO ((xG x 0(F(=u< Gb>X KF bx?
^

fi,-$#3J$J3"$O-$%3-3$=^$$
33U
MOMJ 3O
MOMJOO
MOMJO3
MOMJJ

3JkM
3Jk3M
3Jk3O
3Jk$

3
3
3
3


















33+
MOMJOO 33M3 J 3O
MOMJ 3O 3JkM 3
MOMJ 3 3Jk 3 k33
33_
MOMJ3J 3Jk
MOMJ 3O 3JkM
MOMJJ 3Jk$
MOMJ 3Jk$
33[
MOMJ 3O 3JkM
MOMJ 3Jk$
MOMJ 3
MOMJ 3

3
3
3
3







3
3
3Jk
3Jk














3 kJ
3
k33

"3J_[k$"Tmx#k9//#W9k9
c^HM;J$OuM7>
('#9k##,0$3
JW33#3#J$39S$#k=3ZJ =3
9=V(3,9S3;3z(T"#-k"]$T3z(;'x0T3x"fi
XUx99x#HU$""-$xk3mxV"x+"x}M$"V37R'"RM(P#33
_1-__ ff
5 fi
}
OS-"/$$_JS3,3xSPx/k#'cm"^X;3'$#9+$$" V";3x
$"OO /Wz;9F v 3"(/9k9]k'3
7#"$"
fiO$#k"1X3xX-"M#M 30$]fiO13X03fi#7](WMfi"3
D"$3=9k9x
u#Fk9T
M0mx/mx#k9#=kck$" =3fi"m
"3=J X Mx".30$Q9k#V k9 !-$Q3X1##3
kV" 3
xc7([USR9#"9S=xfi#9/9k9X,3Rc3R$[U93O k3
/ fi$
#&%!')(*F$k=k#c3fi73M+
#,%!'-(.*+/ 01%!'-(.*324#5687!91(;: 3;9""$fi"
0 k-"< 3=
)_ Mx303;$ 9#"X3ST9 >M3fi
!@?) )
Ac9k9k03
[9fi#$"(9fi 3$"OOx B ,'$0VX3,fiO3
Mfi"C
3 3=+9Q ;pD HD } 3S"(9k9k
EFHG

fiIJKMLHNOKQPRK

S1T&UWVQX8YS8YZRV

[\Q]V

^`_ba

c,d!e)fhg

^`_ba

ijc,d!e)fhg

kl_bk

c,d!e)mng

kl_po

ijc,d!e)mng

al_bk

c,d!e)mng

al_bk

ijc,d!e)mng

oh_bk

c,d!e)mng

oh_q^

ijc,d!e)mng

r U!sut]

oMvr T,UWVQX8YS8YZRVwyxQVQzS8YZRV

{}|M~R|M
))-WM- )~R

M-!QW |M~W )~R

))-WM )~R

M-!Q)W |M~W )~R

))-WM )~R

M-!QRW |M~W )~R

))-W )~R

M-!M)W |M~W )~R

))-WM )~R

M-!MW |M~W )~R

))-W- )~R

M-!MW |M~W )~R

))-WQ- )~R

M-!uW |M~W )~R

))-WQ )~R

M-!uRW |M~W )~R

))-WQR )~R

M-!uWW |M~W )~R

))-W)M )~R

M-!W |M~W )~R

))-W) )~R

M-!)W |M~W )~R

))-W)Q )~R

M-!MW |M~W )~R

{}|M~R|M
))-W )~R R )~R

MWM)MR -~ WM )~R

))-W |M~W )~R )~R

M-! |~W )~R !M

))-W |M~W )~R )~R

M-! |~W )~R !uR

{}|M~R|M
))-W |M~W )~R )~R

YRxMT8]l v

M-! |~W )~R !Q-

VQU!sut]3ZW]T&U!S1ZWT,XwZWT]UWz&\.X1S8U!S1]

UWttS8\M]ZRX8X8YsOt]lzZRVuRxQT&U!S8YZRVQXDZWwS8\Q]hwZRxMTsutZz8X

r

\Q]utUWV.\QUWXS8\QT8]]nX1S8U!S1]X

r

\M]]VuU!sut]

ZW]T&U!S1ZWT&XffZWw]UWz&\3X1S8U!S1];U!T8]U!QutY]nT8]]U!S1]QtnxQV-S8YtQVMZhnZWT8]DZW]T&U!S1ZWT&XU!T8]DU!uutYzU!sut]WUWVQ
S8\M]V3UhS1T&UWVQX8YS8YZRVS1ZS8\M];X8xuzz]X8X1ZWTjX1S8U!S1]l
U!T8]RYW]V.YV

YRxMT8])j\M]V]YS8YVMX1S8U!S1]

^

YX<UWM]W r

\Q]`]VQU!sOt]ZW]T&U!S1ZWT,XwZWT]UWz&\X1S8U!S1]

^ OUWttsutZz8XU!T8]ZRV$S8\M]S8U!sut]WVX1S8U!S1]

k uOT&X1S

YX`nZW]ZRV.S1ZW.ZWwUWVQYX`nZHW]ZRV$S1ZWCZWwhOUWVQ.S8\M]VYX<ZW]wT&ZRZRV.S1ZW.ZWw
YVQUWtt

YV=X8S8U!S1]

uYXjnZHW]ZRV=S1ZWZWwD

H

fiOu&M-u-MuOQuMOMuRHuu

CQQRuffu
;W
811 R
=
Du



WWR,RnO&8Q8ffW`

!#"$%&')(( *-,,+-W/.0+WW)

21 3

54 26 879: 1 3W8;




W !
&


Q= l H



8R -



)

QM ff
fi


u

=<!u!&M1 >4MWWR,@?ABC&DAEff
fi #GFH



CWJIK&M

n)Q&

L M6 ONuM&1P M6WQW,NQW1OCQCffSRM&RRuCuTW&!RluW)&UVAB WX&D
!#"$%&')Y[Z BI\-R,,-]'.0^__

a`"5bjW-R, )lR )R<QuQ!8dcSR WGeM8De uQ!&feQ8geQLI
,W<TUh8ijff
fi $"@?ABkHLl 8m/no^R,,-W^/.0^D])
W

CeMqp;WL*-,rpsRM$RnOM8!8QWR<uMWDQ8WR88uWjps`s;Et uCQM
UuAB WX&%o !#"$%&' )vY BI\R- ,DW+ /.0-_M*

e

XP

< /w Ge 'RQ!8 TP DW]R,x +W#WW&SRu 1 %w!WQ!1zy)Q-8|{uDe

};~AE
D&BGG0"l
E 9a B' z
ff
DDAS&'@
dUVAB WX&D
!#"$%&' UVU U ' Y@z z ff
DDAS&B
C
k kUq o&Go
l
EUVA
WX&D !#"$%&'@ U U ' YHMu-+-/.0-+Q-

MR ! Q! u
)R W&hQ!W

ls`/wW&sP p &/wW&1MtDW]R,`Q1R<!8 7u7 IuW1DeWDM,!8 W
Q#W
w &8WuQ MIeM1&<u18eMRWQr;ExAE
/&'GG $"l
E z G

ff
D%%AJ%&'
UuAB WX&%o |"0&B UVU U ' Y@ % ff
DDAS&'


kLkUT [ !o&G
Cl}
EVUVA WX&%o %#"$&' UVU ' Y} QQC]!Q /.0]] !

!18

7DwP 6WC <6/wWCe 7+-R,><W'RQMu8W&W
ff
fi}fi9Co&Go
lh
E U8iC8!^WL*L.0^WQ-

SR

W QW8

/w

u8

NM8 4MaWWR, fi AJ
k%fi% l
dAE
B
Ll'o
V?lWT[ !H?,SAE&%UV"[
AB $fil
tX
R h
7 QSMR 8C u
#Ww &8 WfftDCQ1#w!Q


<D8%wQal MGR)M8a6W]R,O;EM8&C18!1$Q81&W-8nWeQR<WIeMDCeMD-
OCQ
CM ;EqAJ
D&''S $"l
J@ @a G%
ff
D%%AJ%&'
OUVAB WX&D %
"0&B UVU U ' f%
D%%AJ%&'
C
kL khUq o&Go
l@
JUVAB WX&D
!#"$%&'@ U U ' YH MQuR_ /). D)-
<D8DD+WR,zuu!8 WSRQW89IQ8Mw)$1u8Wu 8R#w)M;Ed WW57hb

`W81

6 X
P Dbau
e , qAJ
D&''S $"l
E (Ll ABo
u
@ff
/%%AJ%&'


UuAB WX&%o !#"$%&' MQ- /.0-W^ WSQ
R W 1
7"DCW"WQ<
"WMJ3WQ< W-R,tffCQlWj8!8J{O!uW2;E`Q< bae ,CAE
/&'GG
$"l
E ([Z qAJ
' Sff
D%%AJ%&'
OUVAB WX&D %#"$&'Qff^WW/.0^+^D
DQ11,W
4 RC
)u

"WMJ3W, )< W+R,utQGRQnSRMhD$wWWOCQCMMQ8WR88uWR!Ce
81)'Q
R W18h1!&'R q;~fqAJ
D&''S $"lh
E >[ AB'
ff
DDAS&'
UVAB WX&D
!#"$%&' }q#"$ C
kk9UT !o&Go
l
JUuAB WX&%o %|"0&Bzff
D%%A
%&' QQ WL*LO
. D-M_ h.
P DQ

!8 WWGu
0VV;
&8 }psMR P;Ep
8&


fi#MC

> VTg#sV$CED' DGJS%SDxCCCSS# SSE#\
S#DDEqJD''S $dEf d[ O'%'ffDDSBxVB XD
' !#$%'%CC8C/009 M,[2
% DJJ%Dus# sSD[#J%V}ED/' JJDSCCD@CErVD2
% 2h aC%!'$E/'GG $ Es ff
fi>ffDDS'VVB XD
La#
' !#$%'%CC8C L0@ DqG z SDS%


%%g# #2}s>ED'TC#S#C,CDqC5~ DCC%D> X2!'
E/'GG $Eh a'Bq'Bo ffDDSB@fV X%o>' |0B|C0
}'C82
/D J%,q#'CzS C ! %G%


GD# TC%#"9ED' C h[SCDC%#D$
) (BJVV XD' !#$%'+*B%GJ[-,D00

D%S&%JDgCCS'2

[SD80,ED')xCC#'#C CaJD#SDS#CJDG!82~TC20."}#
! 0,%S%$a/
h# C#S V XC%!'' D10BE32)4#GD$5*s60J%f
*SL%C $8
7qJDBGS$hJ : 9 'Bosff/%%J%'<
; 2+*>=
?@MC8/D0




$

J





~





#




























C














%

'

%




[SD8C0ED'EhCSCBDhD[SJVTBLS#D0C$S|DFEX0#D'C[E9E)G
BGG0Eu / H 'Bo8 ff/%%J%'uB X%o'%|0BzG
C %'%>J@C,DD
sSDCS!CD2 CED/' J[DG$ #%q,%GSDSIBd~OVDDD X2!'
E/'GG $E KJD d'Boq ffDDSBguB X%off' !#$%'%ffC8
'0>s J%qG/ LxM
C2


2}8s#$NLD2$} ED')q'[O"uG2CEqJD''S $E P G
fioVffDDSBuB X%o)'%#$'A; V V'GQ
?@ KR% OffD%%J%'
') SLT SU
080!oGC}EVV X%o'%#$'V
; JV ' GW
?@LCC8X
/Y 0

ZC[\

fififififififififififififiFactorized First Order

Factorized Second Order
300

120
250
200

Frequency

Frequency

100
80

150

60
40

100

20

50

0
0

0.02

0.04

0.06
0.08
absolute error

0.1

0.12

0
0

0.14

0.02

Decimatable First Order

0.04

0.06
0.08
absolute error

0.1

0.12

0.14

0.12

0.14

Decimatable Second Order

150

500
400
Frequency

Frequency

100

300
200

50

100
0
0

0.02

0.04

0.06
0.08
absolute error

0.1

0.12

0.14

0
0

0.02

0.04

0.06
0.08
absolute error

0.1

fiFactorised Paired First Second Order

Decimatable Paired First Second Order

60

60
Frequency

80

Frequency

80

40

40

20

0
0

20

0.02

0.04
0.06
absolute error

0.08

0.1

0
0

0.02

0.04
0.06
absolute error

0.08

0.1

fiFactorized First Order estimates first moments

Factorized Second Order estimates first moments

150

350
300
250
Frequency

Frequency

100

200
150

50

100
50

0
0

0.02

0.04
0.06
absolute error

0.08

0
0

0.1

Factorized First Order estimates correlations

0.02

0.04
0.06
absolute error

0.08

0.1

Factorized Second Order estimates correlations
350

150
300
Frequency

Frequency

250
100

200
150

50

100
50

0
0

0.02

0.04

0.06
0.08
absolute error

0.1

0.12

0
0

0.02

0.04

0.06
0.08
absolute error

0.1

0.12

fifiLearning using second order estimates

Learning using standard MF estimates
2

total pattern likelihood

total pattern likelihood

2

1.5

1

0.5

0
0

10

20
time

30

40

1.5

1

0.5

0
0

50

100
time

150

200

fififififiJournal Artificial Intelligence Research 10 (1999) 291-322

Submitted 10/98; published 5/99

Variational Probabilistic Inference
QMR-DT Network

Tommi S. Jaakkola

tommi@ai.mit.edu

Artificial Intelligence Laboratory,
Massachusetts Institute Technology,
Cambridge, 02139 USA

Michael I. Jordan

Computer Science Division Department Statistics,
University California,
Berkeley, CA 94720-1776 USA

jordan@cs.berkeley.edu

Abstract

describe variational approximation method ecient inference large-scale
probabilistic models. Variational methods deterministic procedures provide approximations marginal conditional probabilities interest. provide alternatives approximate inference methods based stochastic sampling search. describe
variational approach problem diagnostic inference \Quick Medical Reference" (QMR) network. QMR network large-scale probabilistic graphical model
built statistical expert knowledge. Exact probabilistic inference infeasible
model small set cases. evaluate variational inference algorithm
large set diagnostic test cases, comparing algorithm state-of-the-art stochastic
sampling method.

1. Introduction
Probabilistic models become increasingly prevalent AI recent years. Beyond
significant representational advantages probability theory, including guarantees
consistency naturalness combining diverse sources knowledge (Pearl, 1988),
discovery general exact inference algorithms principally responsible
rapid growth probabilistic AI (see, e.g., Lauritzen & Spiegelhalter, 1988; Pearl, 1988;
Shenoy, 1992). exact inference methods greatly expand range models
treated within probabilistic framework provide unifying perspective
general problem probabilistic computation graphical models.
Probability theory viewed combinatorial calculus instructs us
merge probabilities sets events probabilities composites. key operation marginalization, involves summing (or integrating) values
variables. Exact inference algorithms essentially find ways perform sums
possible marginalization operations. terms graphical representation
probability distributions|in random variables correspond nodes conditional
independencies expressed missing edges nodes|exact inference algorithms
define notion \locality" (for example cliques appropriately defined graph),
attempt restrict summation operators locally defined sets nodes.
c 1999 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiJaakkola & Jordan

approach manages stave exponential explosion exact probabilistic
computation, exponential explosion inevitable calculus explicitly
performs summations sets nodes. is, models interest
\local" overly large (see Jordan, et al., press). point view, perhaps
surprising exact inference NP-hard (Cooper, 1990).
paper discuss inference problem particular large-scale graphical
model, Quick Medical Reference (QMR) model.1 QMR model consists combination statistical expert knowledge approximately 600 significant diseases
approximately 4000 findings. probabilistic formulation model (the QMR-DT),
diseases findings arranged bi-partite graph, diagnosis problem
infer probability distribution diseases given subset findings. Given
finding generally relevant wide variety diseases, graph underlying
QMR-DT dense, ecting high-order stochastic dependencies. computational complexity treating dependencies exactly characterized terms size
maximal clique \moralized" graph (see, e.g., Dechter, 1998; Lauritzen & Spiegelhalter, 1988). particular, running time exponential measure size.
QMR-DT, considering standardized \clinocopathologic conference" (CPC) cases
discuss below, find median size maximal clique moralized graph
151.5 nodes. rules use general exact algorithms QMR-DT.
general algorithms take advantage particular parametric form
probability distributions nodes graph, conceivable additional
factorizations might found take advantage particular choice made
QMR-DT. factorization fact found Heckerman (1989); \Quickscore
algorithm" provides exact inference algorithm tailored QMR-DT. Unfortunately, however, run time algorithm still exponential number positive
findings. CPC cases, estimate algorithm would require average
50 years solve inference problem current computers.
Faced apparent infeasibility exact inference large-scale models
QMR-DT, many researchers investigated approximation methods. One general
approach developing approximate algorithms perform exact inference,
partially. One consider partial sets node instantiations, partial sets hypotheses,
partial sets nodes. point view led development algorithms
approximate inference based heuristic search. Another approach developing approximation algorithms exploit averaging phenomena dense graphs. particular, laws
large numbers tell us sums random variables behave simply, converging
predictable numerical results. Thus, may need perform sums explicitly, either
exactly partially. point view leads variational approach approximate
inference. Finally, yet another approach approximate inference based stochastic
sampling. One sample simplified distributions obtain information
complex distribution interest. discuss methods turn.
Horvitz, Suermondt Cooper (1991) developed partial evaluation algorithm
known \bounded conditioning" works considering partial sets node instan1. acronym \QMR-DT" use paper refers \decision-theoretic" reformulation
QMR Shwe, et al. (1991). Shwe, et al. replaced heuristic representation employed
original QMR model (Miller, Fasarie, & Myers, 1986) probabilistic representation.

292

fiVariational Probabilistic Inference QMR-DT

tiations. algorithm based notion \cutset"; subset nodes whose
removal renders remaining graph singly-connected. Ecient exact algorithms exist
singly-connected graphs (Pearl, 1988). Summing instantiations cutset, one
calculate posterior probabilities general graphs using ecient algorithm
subroutine. Unfortunately, however, exponentially many cutset instantiations. bounded conditioning algorithm aims forestalling exponential growth
considering partial sets instantiations. Although algorithm promise graphs
\nearly singly-connected," seems unlikely provide solution dense graphs
QMR-DT. particular, median cutset size QMR-DT across
CPC cases 106.5, yielding unmanageably large number 2106:5 cutset instantiations.
Another approach approximate inference provided \search-based" methods,
consider node instantiations across entire graph (Cooper, 1985; Henrion, 1991;
Peng & Reggia, 1987). general hope methods relatively small fraction
(exponentially many) node instantiations contains majority probability mass,
exploring high probability instantiations (and bounding unexplored
probability mass) one obtain reasonable bounds posterior probabilities. QMRDT search space huge, containing approximately 2600 disease hypotheses. If, however,
one considers cases small number diseases, hypotheses involving
small number diseases contain high probability posteriors, may
possible search significant fraction relevant portions hypothesis space.
Henrion (1991) fact able run search-based algorithm QMR-DT inference
problem, set cases characterized small number diseases. cases,
however, exact Quickscore algorithm ecient. general corpus
CPC cases discuss current paper characterized small number
diseases per case. general, even impose assumption patients limited
number N diseases, cannot assume priori model show sharp cutoff
posterior probability disease N . Finally, high-dimensional search problems
often necessary allow paths limited target hypothesis subspace;
particular, one would like able arrive hypothesis containing diseases
pruning hypotheses containing additional diseases (Peng & Reggia, 1987). Imposing
limitation lead failure search.
recent partial evaluation methods include \localized partial evaluation" method
Draper Hanks (1994), \incremental SPI" algorithm D'Ambrosio (1993),
\probabilistic partial evaluation" method Poole (1997), \mini-buckets" algorithm
Dechter (1997). former algorithm considers partial sets nodes, latter three
consider partial evaluations sums emerge exact inference run.
promising methods, like partial evaluation methods yet clear
restrict exponential growth complexity ways yield realistic accuracy/time
tradeoffs large-scale models QMR-DT.2
Variational methods provide alternative approach approximate inference.
similar spirit partial evaluation methods (in particular incremental SPI
mini-buckets algorithms), aim avoid performing sums exponentially
2. D'Ambrosio (1994) reports \mixed" results using incremental SPI QMR-DT, somewhat
dicult set cases Heckerman (1989) Henrion (1991), still restricted number
positive findings.

293

fiJaakkola & Jordan

many summands, come problem different point view.
variational point view, sum avoided contains sucient number terms
law large numbers invoked. variational approach inference
replaces quantities expected beneficiary averaging process
surrogates known \variational parameters." inference algorithm manipulates
parameters directly order find good approximation marginal probability
interest. QMR-DT model turns particularly appealing architecture
development variational methods. show, variational methods simple
graphical interpretation case QMR-DT.
final class methods performing approximate inference stochastic sampling methods. Stochastic sampling large family, including techniques rejection
sampling, importance sampling, Markov chain Monte Carlo methods (MacKay, 1998).
Many methods applied problem approximate probabilistic inference graphical models analytic results available (Dagum & Horvitz, 1993).
particular, Shwe Cooper (1991) proposed stochastic sampling method known
\likelihood-weighted sampling" QMR-DT model. results promising results date inference QMR-DT|they able produce reasonably
accurate approximations reasonable time two dicult CPC cases. consider
Shwe Cooper algorithm later paper; particular compare algorithm
empirically variational algorithm across entire corpus CPC cases.
Although important compare approximation methods, emphasized
outset think goal identify single champion
approximate inference technique. Rather, different methods exploit different structural
features large-scale probability models, expect optimal solutions involve
combination methods. return point discussion section,
consider various promising hybrids approximate exact inference algorithms.
general problem approximate inference NP-hard (Dagum & Luby, 1993)
provides additional reason doubt existence single champion approximate
inference technique. think important stress, however, hardness result,
together Cooper's (1990) hardness result exact inference cited above,
taken suggest exact inference approximate inference \equally hard."
take example related field, exist large domains solid uid mechanics
exact solutions infeasible approximate techniques (finite element
methods) work well. Similarly, statistical physics, models exactly solvable,
exist approximate methods (mean field methods, renormalization group methods)
work well many cases. feel goal research probabilistic inference
similarly identifying effective approximate techniques work well
large classes problems.

2. QMR-DT Network
QMR-DT network (Shwe et al., 1991) two-level bi-partite graphical model (see
Figure 1). top level graph contains nodes diseases , bottom level
contains nodes findings .
294

fiVariational Probabilistic Inference QMR-DT

number conditional independence assumptions ected bi-partite
graphical structure. particular, diseases assumed marginally independent.
(I.e., independent absence findings. Note diseases assumed
mutually exclusive; patient multiple diseases). Also, given states
disease nodes, findings assumed conditionally independent. (For
discussion regarding medical validity diagnostic consequences
assumptions embedded QMR-DT belief network, see Shwe et al., 1991).
diseases

d1

f1

dn

fm
findings

Figure 1: QMR belief network two-level graph dependencies
diseases associated findings modeled via noisy-OR gates.
state precisely probability model implied QMR-DT model, write
joint probability diseases findings as:

P (f; d) = P (f jd)P (d) =

"





3
#2

P (fi jd) 4 P (dj )5

j

(1)

f binary (1/0) vectors referring presence/absence states diseases
positive/negative states outcomes findings, respectively. conditional
probabilities P (fi jd) represented \noisy-OR model" (Pearl, 1988):

P (fi = 0jd) = P (fi = 0jL) P (fi = 0jdj )
(2)
= (1 , qi0 )





j 2i

(1 , qij )dj

j 2
P
,
i0 , j2i ij dj
e
;

(3)

(4)
set diseases parents finding fi QMR graph, qij =
P (fi = 1jdj = 1) probability disease j , present, could alone cause
finding positive outcome, qi0 = P (fi = 1jL) \leak" probability, i.e.,
probability finding caused means diseases included
QMR model. final line, reparameterize noisy-OR probability model
using exponentiated notation. notation, model parameters given
ij = , log(1 , qij ).
295

fiJaakkola & Jordan

3. Inference
Carrying diagnostic inference QMR model involves computing posterior
marginal probabilities diseases given set observed positive (fi = 1) negative
(fi0 = 0) findings. Note set observed findings considerably smaller set
possible findings; note moreover (from bi-partite structure QMR-DT graph)
unobserved findings effect posterior probabilities diseases.
brevity adopt notation fi+ corresponds event fi = 1, fi, refers
fi = 0 (positive negative findings respectively). Thus posterior probabilities
interest P (dj jf + ; f , ), f + f , vectors positive negative findings.
negative findings f , benign respect inference problem|they
incorporated posterior probability linear time number associated diseases
number negative findings. discuss below, seen
fact probability negative finding Eq. (4) exponential expression
linear dj . positive findings, hand, problematic.
worst case exact calculation posterior probabilities exponentially costly
number positive findings (Heckerman, 1989; D'Ambrosio, 1994). Moreover, practical
diagnostic situations number positive findings often exceeds feasible limit
exact calculations.
Let us consider inference calculations detail. find posterior probability
P (djf + ; f ,), first absorb evidence negative findings, i.e., compute P (djf ,).
P (f , jd)P (d) normalization. Since P (f , jd) P (d) factorize
diseases (see Eq. (1) Eq. (2) above), posterior P (djf , ) must factorize well.
normalization P (f , jd)P (d) therefore reduces independent normalizations
disease carried time linear number diseases (or negative
findings). remainder paper, concentrate solely positive findings
pose real computational challenge. Unless otherwise stated, assume
prior distribution diseases already contains evidence negative findings.
words, presume updates P (dj ) P (dj jf , ) already made.
turn question computing P (dj jf + ), posterior marginal probability
based positive findings. Formally, obtaining posterior involves marginalizing
P (f + jd)P (d) across remaining diseases:

P (dj jf + ) /

X

dndj

P (f + jd)P (d)

(5)

summation possible configurations disease variables
dj (we use shorthand summation index n dj this). QMR model
P (f + jd)P (d) form:

P (f + jd)P (d) =
=

"
"





3
#2

P (fi+ jd) 4 P (dj )5

j

3
2
#
1 , e ,i0 , j ij dj 4 P (dj )5





(6)

P

j

296

(7)

fiVariational Probabilistic Inference QMR-DT

follows Eq. (4) fact P (fi+ jd) = 1 , P (f , jd). perform
summation Eq. (5) diseases, would multiply terms 1 , efg
corresponding conditional probabilities positive finding. number
terms exponential number positive findings. algorithms exist
attempt find exploit factorizations expression, based particular pattern
observed evidence (cf. Heckerman, 1989; D'Ambrosio, 1994), algorithms limited
roughly 20 positive findings current computers. seems unlikely sucient
latent factorization QMR-DT model able handle full CPC corpus,
median number 36 positive findings per case maximum number 61 positive
findings.

4. Variational Methods
Exact inference algorithms perform many millions arithmetic operations applied
complex graphical models QMR-DT. proliferation terms expresses
symbolic structure model, necessarily express numeric structure
model. particular, many sums QMR-DT inference problem sums
large numbers random variables. Laws large numbers suggest sums
may yield predictable numerical results ensemble summands, fact
might enable us avoid performing sums explicitly.
exploit possibility numerical regularity dense graphical models develop
variational approach approximate probabilistic inference. Variational methods
general class approximation techniques wide application throughout applied mathematics. Variational methods particularly useful applied highly-coupled systems. introducing additional parameters, known \variational parameters"|which
essentially serve low-dimensional surrogates high-dimensional couplings
system|these methods achieve decoupling system. mathematical machinery
variational approach provides algorithms finding values variational parameters decoupled system good approximation original coupled
system.
case probabilistic graphical models variational methods allow us simplify
complicated joint distribution one Eq. (7). achieved via parameterized transformations individual node probabilities. see later, node
transformations interpreted graphically delinking nodes graph.
find appropriate transformations? variational methods consider
come convex analysis (see Appendix 6). Let us begin considering methods
obtaining upper bounds probabilities. well-known fact convex analysis
concave function represented solution minimization problem:

f (x) = min
f x , f ( ) g


(8)

f ( ) conjugate function f (x). function f ( ) obtained
solution minimization problem:

f ( ) = min
x f x , f (x) g:

297

(9)

fiJaakkola & Jordan

formal identity pair minimization problems expresses \duality" f
conjugate f .
representation f Eq. (8) known variational transformation. parameter known variational parameter. relax minimization fix
variational parameter arbitrary value, obtain upper bound:

f (x) x , f ( ):

(10)

bound better values variational parameter others,
particular value bound exact.
also want obtain lower bounds conditional probabilities. straightforward
way obtain lower bounds appeal conjugate duality express functions terms maximization principle. representation, however, applies convex
functions|in current paper require lower bounds concave functions. concave functions, however, special form allows us exploit conjugatePduality
different way. particular, require bounds functions form f (a + j zj ),
f concave function, zj 2 f1; 2; : : :; ng non-negative variables,
constant. variables zj expression effectively coupled|the impact
changing one variable contingent settings remaining variables. use
Jensen's inequality, however, obtain lower bound variables decoupled.3
particular:

f( +

X

j

qj zqj )
j
j
X

qj f ( + zqj )
j
j

zj ) = f ( +

X

(11)
(12)

qj viewed defining probability distribution variables zj .
variational parameter case theP probability distribution q . optimal setting
parameter given qj = zj = k zk . easily verified substitution
Eq. (12), demonstrates lower bound tight.

4.1 Variational Upper Lower Bounds Noisy-OR

Let us return problem computing posterior probabilities QMR
model. Recall conditional probabilities corresponding positive findings
need simplified. end, write
P

P (fi+ jd) = 1 , e ,i0 ,

j ij dj

= e log(1,e,x )

(13)

P

x = i0 + j ij dj . Consider exponent f (x) = log(1 , e,x ). noisy-OR,
well many conditional models involving compact representations (e.g., logistic
regression), exponent f (x) concave function x. Based discussion
P

P

3. Jensen's inequality, states f (a + j qj xj ) j qj f (a + xj ), concave
f ,
P
0 qj 1, simple consequence Eq. (8), x taken + j qj xj .

298

P

qj

= 1,

fiVariational Probabilistic Inference QMR-DT

previous section, know must exist variational upper bound function
linear x:

f (x) x , f ()

(14)

Using Eq. (9) evaluate conjugate function f ( ) noisy-OR, obtain:

f () = , log + ( + 1) log( + 1)

(15)

desired
bound obtained substituting Eq. (13) (and recalling definition
x = i0 + Pj ij dj ):

P (fi+ jd)

=




P

e f (i0 + Pj ij dj )

e (i0+ j ij dj ),f (i)
P (fi+ jd; i):

(16)
(17)
(18)

Note \variational evidence" P (fi+ jd; i) exponential term linear
disease vector d. negative findings, implies variational
evidence incorporated posterior time linear number diseases
associated finding.
also graphical way understand effect transformation. rewrite
variational evidence follows:
P



P (fi+jd; i) = e i(i0 + j ij dj ),f (i)
id
Yh
= e i0 ,f (i) e iij j :
j

(19)
(20)

Note first term constant, note moreover product factorized
across diseases. latter factors multiplied pre-existing
prior corresponding disease (possibly modulated factors negative
evidence). constant term viewed associated delinked finding node fi .
Indeed, effect variational transformation delink finding node fi
graph, altering priors disease nodes connected finding node.
graphical perspective important presentation variational algorithm|
able view variational transformations simplifying graph point
exact methods run.
turn
lower bounds conditional probabilities P (fi+ jd). expoP
nent f (i0 + j ij dj ) exponential representation form applied
Jensen's inequality previous section. Indeed, since f concave need identify
non-negative variables zj , case ij dj , constant a,
i0. Applying bound Eq. (12) have:

P (fi+ jd)

=

P

e f ( i0 + j ij dj )

e

ij dj
j qjji f io + qjji

P



299

(21)
(22)

fiJaakkola & Jordan

= e
= e

h

P





ij
j qjji dj f io + qjji +(1,dj ) f ( io )




ij
j qjji dj f io + qjji ,f ( io ) +f ( io )

P

h

(23)

(24)

(25)
allowed different variational distribution qji finding. Note
bound linear exponent. case upper bound,
implies variational evidence incorporated posterior distribution
time linear number diseases. Moreover, view variational
transformation terms delinking finding node fi graph.

P (fi+ jd; qji)

4.2 Approximate Inference QMR

previous section described variational transformations derived individual findings QMR model; discuss utilize transformations
context overall inference algorithm.
Conceptually overall approach straightforward. transformation involves
replacing exact conditional probability finding lower bound upper
bound:
P (fi+ jd; qji) P (fi+ jd) P (fi+ jd; i):
(26)
Given transformations viewed delinking ith finding node
graph, see transformations yield bounds, also yield simplified graphical structure. imagine introducing transformations sequentially
graph sparse enough exact methods become feasible. point stop
introducing transformations run exact algorithm.
problem approach, however. need decide step
node transform, requires assessment effect overall accuracy
transforming node. might imagine calculating change probability interest
given transformation, choosing transform node
yields least change target probability. Unfortunately unable calculate
probabilities original untransformed graph, thus unable assess effect
transforming one node. unable get algorithm started.
Suppose instead work backwards. is, introduce transformations
findings, reducing graph entirely decoupled set nodes. optimize
variational parameters fully transformed graph (more optimization
variational parameters below). graph inference trivial. Moreover, also easy
calculate effect reinstating single exact conditional one node: choose
reinstate node yields change.
Consider particular case upper bounds (lower bounds analogous).
transformation introduces upper bound conditional probability P (fi+ jd). Thus
likelihood observing (positive) findings P (f + ) also upper bounded variational
counterpart P (f + j ):
X
X
P (f + ) = P (f + jd)P (d) P (f + jd; )P (d) P (f + j )
(27)




300

fiVariational Probabilistic Inference QMR-DT

assess accuracy variational transformation introducing optimizing variational transformations positive findings. Separately
positive finding replace variationally transformed conditional probability P (fi+ jd; i)
corresponding exact conditional P (fi+ jd) compute difference
resulting bounds likelihood observations:

= P (f + j ) , P (f + j n )

(28)

P (f + j n ) computed without transforming ith positive finding. larger
difference is, worse ith variational transformation is. therefore
introduce transformations ascending order s. Put another way,
treat exactly (not transform) conditional probabilities whose measure large.
practice, intelligent method ordering transformations critical. Figure 2
compares calculation likelihoods based measure opposed method
chooses ordering transformations random. plot corresponds representative diagnostic case, shows upper bounds log-likelihoods observed
findings function number conditional probabilities left intact (i.e.
transformed). Note upper bound must improve (decrease) fewer transformations. results striking|the choice ordering large effect accuracy
(note plot log-scale).
30

loglikelihood

35
40
45
50
55
60
0

2

4
6
8
10
# exactly treated findings

12

Figure 2: upper bound log-likelihood delta method removing transformations (solid line) method bases choice random ordering
(dashed line).
Note also curve proposed ranking convex; thus bound improves
less fewer transformations left. first remove worst
transformations, replacing exact conditionals. remaining transformations better indicated delta measure thus bound improves less
replacements.
make claims optimality delta method; simply useful heuristic
allows us choose ordering variational transformations computationally
ecient way. Note also implementation method optimizes variational
parameters outset chooses ordering transformations
based fixed parameters. parameters suboptimal graphs
301

fiJaakkola & Jordan

substantial numbers nodes reinstated, found practice
simplified algorithm still produces reasonable orderings.
decided nodes reinstate, approximate inference algorithm
run. introduce transformations nodes left transformed
ordering algorithm. product exact conditional probabilities graph
transformed conditional probabilities yields upper lower bound overall
joint probability associated graph (the product bounds bound). Sums
bounds still bounds, thus likelihood (the marginal probability findings)
bounded summing across bounds joint probability. particular, upper
bound likelihood obtained via:
X
X
P (f + ) = P (f + jd)P (d) P (f + jd; )P (d) P (f + j )
(29)








dndj

dndj

corresponding lower bound likelihood obtained similarly:
X
X
P (f + ) = P (f +jd)P (d) P (f + jd; q )P (d) P (f + jq )

(30)

cases assume graph suciently simplified variational
transformations sums performed eciently.
expressions Eq. (29) Eq. (30) yield upper lower bounds arbitrary
values variational parameters q . wish obtain tightest possible bounds,
thus optimize expressions respect q . minimize respect
maximize respect q. Appendix 6 discusses optimization problems
detail. turns upper bound convex thus adjustment
variational parameters upper bound reduces convex optimization problem
carried eciently reliably (there local minima). lower bound
turns maximization carried via EM algorithm.
Finally, although bounds likelihood useful, ultimate goal approximate
marginal posterior probabilities P (dj jf + ). two basic approaches utilizing
variational bounds Eq. (29) Eq. (30) purpose. first method,
emphasis current paper, involves using transformed probability model (the
model based either upper lower bounds) computationally ecient surrogate
original probability model. is, tune variational parameters transformed
model requiring model give tightest possible bound likelihood.
use tuned transformed model inference engine provide approximations
probabilities interest, particular marginal posterior probabilities P (dj jf + ).
approximations found manner bounds, computationally ecient
approximations. provide empirical data following section show
approach indeed yields good approximations marginal posteriors QMR-DT
network.
ambitious goal obtain interval bounds marginal posterior probabilities themselves. end, let P (f + ; dj j ) denote combined event QMR-DT
model generates observed findings f + j th disease takes value dj .
bounds follow directly from:
X
X
P (f + ; dj ) = P (f + jd)P (d) P (f + jd; )P (d) P (f + ; dj j)
(31)
302

fiVariational Probabilistic Inference QMR-DT

P (f + jd; ) product upper-bound transformed conditional probabilities
exact (untransformed) conditionals. Analogously compute lower bound P (f + ; dj jq )
applying lower bound transformations:

P (f + ; dj ) =

X

dndj

P (f +jd)P (d)

X

dndj

P (f + jd; q )P (d) P (f + ; dj jq )

(32)

Combining bounds obtain interval bounds posterior marginal probabilities diseases (cf. Draper & Hanks 1994):

P (f +; dj j)
P (f + ; dj jq)
+)

P
(

j
f
j
P (f +; dj j) + P (f + ; dj jq )
P (f + ; dj j ) + P (f + ; dj jq) ;
dj binary complement dj .

(33)

5. Experimental Evaluation

diagnostic cases used evaluating performance variational techniques cases abstracted clinocopathologic conference (\CPC") cases. cases
generally involve multiple diseases considered clinically dicult cases.
cases Middleton et al. (1990) find importance sampling method
work satisfactorily.
evaluation variational methodology consists three parts. first part
exploit fact subset CPC cases (4 48 cases)
suciently small number positive findings calculate exact values
posterior marginals using Quickscore algorithm. is, four cases
able obtain \gold standard" comparison. provide assessment accuracy
eciency variational methods four CPC cases. present variational
upper lower bounds likelihood well scatterplots compare variational
approximations posterior marginals exact values. also present comparisons
likelihood-weighted sampler Shwe Cooper (1991).
second section present results remaining, intractable CPC cases.
use lengthy runs Shwe Cooper sampling algorithm provide surrogate
gold standard cases.
Finally, third section consider problem obtaining interval bounds
posterior marginals.

5.1 Comparison Exact Marginals

Four CPC cases 20 fewer positive findings (see Table 1), cases
possible calculate exact values likelihood posterior marginals
reasonable amount time. used Heckerman's \Quickscore" algorithm (Heckerman 1989)|an algorithm tailored QMR-DT architecture|to perform exact
calculations.
Figure 3 shows log-likelihood four tractable CPC cases. figure also shows
variational lower upper bounds. calculated variational bounds twice,
differing numbers positive findings treated exactly two cases (\treated exactly"
303

fiJaakkola & Jordan

case # pos. findings # neg. findings
1
20
14
2
10
21
3
19
19
4
19
33

20

20

30

30

40

loglikelihood

loglikelihood

Table 1: Description cases evaluated exact posterior marginals.

50

40

50

60
60
70
70

(a)

0

1

2
3
sorted cases

4

(b)

5

0

1

2
3
sorted cases

4

5

Figure 3: Exact values variational upper lower bounds log-likelihood
log P (f + j ) four tractable CPC cases. (a) 8 positive findings
treated exactly, (b) 12 positive findings treated exactly.
simply means finding transformed variationally). panel (a) 8
positive findings treated exactly, (b) 12 positive findings treated exactly.
expected, bounds tighter positive findings treated exactly.4
average running time across four tractable CPC cases 26.9 seconds
exact method, 0.11 seconds variational method 8 positive findings treated
exactly, 0.85 seconds variational method 12 positive findings treated exactly.
(These results obtained 433 MHz DEC Alpha computer).
Although likelihood important quantity approximate (particularly applications parameters need estimated), interest QMR-DT setting
posterior marginal probabilities individual diseases. discussed
previous section, simplest approach obtaining variational estimates quantities define approximate variational distribution based either distribution
P (f + j ), upper-bounds likelihood, distribution P (f + jq ), lowerbounds likelihood. fixed values variational parameters (chosen provide
tight bound likelihood), distributions provide partially factorized approximations joint probability distribution. factorized forms exploited
4. Given significant fraction positive findings treated exactly simulations, one
may wonder additional accuracy due variational transformations. address
concern later section demonstrate variational transformations fact responsible
significant portion accuracy cases.

304

fiVariational Probabilistic Inference QMR-DT

1

1

0.8

0.8
variational estimates

variational estimates

ecient approximate inference engines general posterior probabilities, particular
use provide approximations posterior marginals individual diseases.
practice found distribution P (f + j ) yielded accurate posterior
marginals distribution P (f + jq ), restrict presentation P (f + j ). Figure 4 displays scatterplot approximate posterior marginals, panel (a) corre-

0.6

0.4

0.2

(a)

0
0

0.6

0.4

0.2

0.2

0.4
0.6
exact marginals

0.8

(b)

1

0
0

0.2

0.4
0.6
exact marginals

0.8

1

Figure 4: Scatterplot variational posterior estimates exact marginals.
(a) 8 positive findings treated exactly (b) 12 positive findings
treated exactly.
sponding case 8 positive findings treated exactly panel (b) case
12 positive findings treated exactly. plots obtained first extracting
50 highest posterior marginals case using exact methods computing
approximate posterior marginals corresponding diseases. approximate
marginals fact correct points figures align along diagonals
shown dotted lines. see reasonably good correspondence|the variational
algorithm appears provide good approximation largest posterior marginals. (We
quantify correspondence ranking measure later section).
current state-of-the-art algorithm QMR-DT enhanced version likelihoodweighted sampling proposed Shwe Cooper (1991). Likelihood-weighted sampling
stochastic sampling method proposed Fung Chang (1990) Shachter Peot
(1990). Likelihood-weighted sampling basically simple forward sampling method
weights samples likelihoods. enhanced improved utilizing \selfimportance sampling" (see Shachter & Peot, 1990), version importance sampling
importance sampling distribution continually updated ect current
estimated posterior distribution. Middleton et al. (1990) utilized likelihood-weighted sampling self-importance sampling (as well heuristic initialization scheme known
\iterative tabular Bayes") QMR-DT model found work satisfactorily. Subsequent work Shwe Cooper (1991), however, used additional
enhancement algorithm known `Markov blanket scoring" (see Shachter & Peot,
1990), distributes fractions samples positive negative values node
proportion probability values conditioned Markov blanket
node. combination Markov blanket scoring self-importance sampling yielded
305

fiJaakkola & Jordan

effective algorithm.5 particular, modifications place, Shwe Cooper
reported reasonable accuracy two dicult CPC cases.
re-implemented likelihood-weighted sampling algorithm Shwe Cooper,
incorporating Markov blanket scoring heuristic self-importance sampling. (We
utilize \iterative tabular Bayes" instead utilized related initialization scheme{
\heuristic tabular Bayes"{also discussed Shwe Cooper). section discuss
results running algorithm four tractable CPC cases, comparing
results variational inference.6 following section present fuller comparative
analysis two algorithms CPC cases.
Likelihood-weighting sampling, indeed sampling algorithm, realizes timeaccuracy tradeoff|taking additional samples requires time improves accuracy.
comparing sampling algorithm variational algorithm, ran sampling
algorithm several different total time periods, accuracy achieved
sampling algorithm roughly covered range achieved variational algorithm.
results shown Figure 5, right-hand curve corresponding sampling runs.
figure displays mean correlations approximate exact posterior
marginals across ten independent runs algorithm (for four tractable CPC cases).
1

mean correlation

0.98
0.96
0.94
0.92
0.9
0.88
0.86 1
10

0

1

10
10
execution time seconds

2

10

Figure 5: mean correlation approximate exact posterior marginals
function execution time (in seconds). Solid line: variational estimates;
dashed line: likelihood-weighting sampling. lines sampling result represent standard errors mean based ten independent
runs sampler.
Variational algorithms also characterized time-accuracy tradeoff. particular,
accuracy method generally improves findings treated exactly,
cost additional computation. Figure 5 also shows results variational
algorithm (the left-hand curve). three points curve correspond 8, 12
5. initialization method proved little effect inference results.
6. also investigated Gibbs sampling (Pearl, 1988). results Gibbs sampling good
results likelihood-weighted sampling, report latter results remainder
paper.

306

fiVariational Probabilistic Inference QMR-DT

16 positive findings treated exactly. Note variational estimates deterministic
thus single run made.
figure shows achieve roughly equivalent levels accuracy, sampling
algorithm requires significantly computation time variational method.
Although scatterplots correlation measures provide rough indication accuracy approximation algorithm, deficient several respects. particular,
diagnostic practice interest ability algorithm rank diseases correctly,
avoid false positives (diseases fact significant included
set highly ranked diseases) false negatives (significant diseases omitted set highly ranked diseases). defined ranking measure follows (see
also Middleton et al., 1990). Consider set N highest ranking disease hypotheses,
ranking based correct posterior marginals. Corresponding set
diseases find smallest set N 0 approximately ranked diseases includes
N significant ones. words, N \true positives" approximate method
produces N 0 , N \false positives." Plotting false positives function true positives
provides meaningful useful measure accuracy approximation scheme.
extent method provides nearly correct ranking true positives plot
increases slowly area curve small. significant disease appears
late approximate ordering plot increases rapidly near true rank missed
disease area curve large.
also plot number \false negatives" set top N highly ranked diseases.
False negatives refer number diseases, N highest ranking diseases,
appear set N approximately ranked diseases. Note unlike
previous measure, measure reveal severity misplacements,
frequency.
improved diagnostic measure hand, let us return evaluation
inference algorithms, beginning variational algorithm. Figure 6 provides plots
60

7

50

6

false negatives

false positives

5
40
30
20

4
3
2

10

(a)

0
0

1

10

20
30
true positives

40

(b)

50

0
0

10

20
30
approximate ranking

40

50

Figure 6: (a) Average number false positives function true positives variational method (solid lines) partially-exact method (dashed line). (b) False
negatives set top N approximately ranked diseases. figures 8
positive findings treated exactly.
false positives (panel a) false negatives (panel b) true positives
307

fiJaakkola & Jordan

40

4

35

3.5

(a)

3
false negatives

false positives

30
25
20
15

2.5
2
1.5

10

1

5

0.5

0
0

10

20
30
true positives

40

(b)

50

0
0

10

20
30
approximate ranking

40

50

Figure 7: (a) Average number false positives function true positives variational method (solid line) partially-exact method (dashed line). (b) False
negatives set top N approximately ranked diseases. figures 12
positive findings treated exactly.
tractable CPC cases. Eight positive findings treated exactly simulation shown
figure. Figure 7 displays results 12 positive finding treated exactly.
noted earlier, 8 12 positive findings comprise significant fraction
total positive findings tractable CPC cases, thus important verify
variational transformations fact contributing accuracy posterior
approximations beyond exact calculations. comparing
variational method method call \partially-exact" method
posterior probabilities obtained using findings treated exactly
variational calculations (i.e., using findings transformed).
variational transformations contribute accuracy approximation,
performance partially-exact method comparable
variational method.7 Figure 6 Figure 7 clearly indicate case.
difference accuracy methods substantial computational load
comparable (about 0.1 seconds 433MHz Dec Alpha).
believe accuracy portrayed false positive plots provides good indication potential variational algorithm providing practical solution
approximate inference problem QMR-DT. figures show, number
false positives grows slowly number true positives. example, shown
Figure 6 eight positive findings treated exactly, find 20 likely diseases
would need entertain top 23 diseases list approximately ranked
diseases (compared 50 partially-exact method).
ranking plot likelihood-weighted sampler shown Figure 8,
curve variational method Figure 7 included comparison. make
plots, ran likelihood-weighted sampler amount time (6.15 seconds)
7. noted conservative comparison, partially-exact method fact
benefits variational transformation|the set exactly treated positive findings selected
basis accuracy variational transformations, accuracies correlate
diagnostic relevance findings.

308

fiVariational Probabilistic Inference QMR-DT

40
35

false positives

30
25
20
15
10
5
0
0

10

20
30
true positives

40

50

Figure 8: Average number false positives function true positives likelihoodweighted sampler (dashed line) variational method (solid line) 12
positive findings treated exactly.
comparable time allocated slowest variational method (3.17 seconds;
case 16 positive findings treated exactly. Recall time required
variational algorithm 12 positive findings treated exactly 0.85 seconds.)
plots show, tractable CPC cases, variational method significantly
accurate sampling algorithm comparable computational loads.

5.2 Full CPC Corpus

consider full CPC corpus. majority cases (44 48 cases),
20 positive findings thus appear beyond reach exact methods.
important attraction sampling methods mathematical guarantee accurate
estimates limit suciently large sample size (Gelfand & Smith, 1990). Thus
sampling methods promise providing general methodology approximate
inference, two caveats: (1) number samples needed dicult
diagnosis, (2) many samples may required obtain accurate estimates.
real-time applications, latter issue rule sampling solutions. However, long-term
runs sampler still provide useful baseline evaluation accuracy faster
approximation algorithms. begin considering latter possibility context
likelihood-weighted sampling QMR-DT. turn comparative evaluation
likelihood-weighted sampling variational methods time-limited setting.
explore viability likelihood-weighted sampler providing surrogate
gold standard, carried two independent runs consisting 400,000 samples.
Figure 9(a) shows estimates log-likelihood first sampling run
CPC cases. also show variational upper lower bounds cases
(the cases sorted according lower bound). Note bounds
rigorous bounds true log-likelihood, thus provide direct indication
accuracy sampling estimates. Although see many estimates lie
bounds, also see many cases sampling estimates deviate substantially
bounds. suggests posterior marginal estimates obtained
samples likely unreliable well. Indeed, Figure 9(b) presents scatterplot
309

fiJaakkola & Jordan

20
1

40
0.8
sampling estimates 2

loglikelihood

60
80
100
120
140
160

0.6

0.4

0.2

180

(a)

200
0

10

20
30
sorted cases

40

50

(b)

0
0

0.2

0.4
0.6
sampling estimates 1

0.8

1

Figure 9: (a) Upper lower bounds (solid lines) corresponding sampling estimates (dashed line) log-likelihood observed findings CPC cases.
(b) correlation plot posterior marginal estimates two independent sampling runs.
estimated posterior marginals two independent runs sampler. Although
see many cases results lie diagonal, indicating agreement
two runs, also see many pairs posterior estimates far diagonal.
results cast doubt viability likelihood-weighted sampler
general approximator full set CPC cases. Even problematically appear
without reliable surrogate gold standard cases, making dicult
evaluate accuracy real-time approximations variational method. Note,
however, estimates Figure 9(a) seem fall two classes|estimates
lie within variational bounds estimates rather far bounds.
suggests possibility distribution sampled multi-modal,
estimates falling within correct mode providing good approximations
others falling spurious modes providing seriously inaccurate approximations.
situation holds, accurate surrogate gold standard might obtained using
variational bounds filter sampling results retaining estimates
lie bounds given variational approach.
Figure 10 provides evidence viability approach. 24 48
CPC cases independent runs sampler resulted estimates loglikelihood lying approximately within variational bounds. recomputed posterior
marginal estimates selected cases plotted figure.
scatterplot shows high degree correspondence posterior estimates
cases. thus tentatively assume estimates accurate enough serve
surrogate gold standard proceed evaluate real-time approximations.
Figure 11 plots false positives true positives 24 selected CPC
cases variational method. Twelve positive findings treated exactly
simulation. Obtaining variational estimates took 0.29 seconds computer time per
case. Although curve increases rapidly tractable CPC cases,
variational algorithm still appears provide reasonably accurate ranking posterior
marginals, within reasonable time frame.
310

fiVariational Probabilistic Inference QMR-DT

1

sampling estimates 2

0.8

0.6

0.4

0.2

0
0

0.2

0.4
0.6
sampling estimates 1

0.8

1

Figure 10: correlation plot selected posterior marginal estimates two
independent sampling runs, selection based variational
upper lower bounds.
70
60

false positives

50
40
30
20
10
0
0

10

20
30
true positives

40

50

Figure 11: Average number false positives function true positives variational method (solid line) likelihood-weighted sampler (dashed line).
variational method 12 positive findings treated exactly,
sampler results averages across ten runs.
compare variational algorithm time-limited version likelihood-weighted
sampler ran latter algorithm period time (8.83 seconds per case) roughly comparable running time variational algorithm (0.29 seconds per case). Figure 11
shows corresponding plot false positives true positives, averaged ten independent runs. see curve increases significantly steeply
variational curve. find 20 likely diseases variational method
would need entertain top 30 diseases list approximately ranked
diseases. sampling method would need entertain top 70 approximately
ranked diseases.

5.3 Interval Bounds Marginal Probabilities

Thus far utilized variational approach produce approximations posterior marginals. approximations discussed originate upper lower
311

fiJaakkola & Jordan

bounds likelihood, bounds. is, guaranteed lie true posteriors, see Figure 4. discussed
Section 4.1, however, also possible induce upper lower bounds posterior
marginals upper lower bounds likelihood (cf. Eq. 33). section
evaluate interval bounds QMR-DT posterior marginals.
Figure 12 displays histogram interval bounds four tractable CPC cases,
24 selected CPC cases previous section, CPC cases. histograms
include diseases QMR-DT network. case tractable cases
0.8

0.8

0.8

0.6

0.6

0.4

0
0

0.6

0.4

0.2

(a)

Frequency

1

Frequency

1

Frequency

1

0.4

0.2

0.2

0.4

0.6

Interval size

0.8

1

(b)

0
0

0.2

0.2

0.4

0.6

Interval size

0.8

1

(c)

0
0

0.2

0.4

0.6

0.8

Interval size

Figure 12: Histograms size interval bounds diseases QMRDT network (a) four tractable CPC cases, (b) 24 selected CPC cases
previous section, (c) CPC cases.
variational method run 12 positive findings treated exactly. remaining
CPC cases variational method run 16 positive findings treated exactly.
running time algorithm less 10 seconds computer time per CPC case.
tractable CPC cases, interval bounds tight nearly diseases
network. However, (1) positive findings treated variationally
cases, (2) need practice compute variational bounds cases.
get somewhat better picture viability variational interval bounds
Figure 12(b) Figure 12(c), picture decidedly mixed. 24 selected
cases, tight bounds provided approximately half diseases. bounds
vacuous approximately quarter diseases, range diseases
between. consider CPC cases, approximately third bounds
tight nearly half vacuous.
Although results may indicate limitations variational approximation,
another immediate problem appears responsible looseness
bounds many cases. particular, recall use Quickscore algorithm
(Heckerman, 1989) handle exact calculations within framework variational
algorithm. Unfortunately Quickscore suffers vanishing numerical precision large
numbers positive findings, general begin run numerical problems,
resulting vacuous bounds, 16 positive findings incorporated exactly
variational approximation. Thus, although clearly interest run variational
algorithm longer durations, thereby improve bounds, unable
within current implementation exact subroutine.
312

1

fiVariational Probabilistic Inference QMR-DT

clearly worth studying methods Quickscore treating exact findings within variational algorithm, also interest consider combining
variational methods methods, search-based partial evaluation
methods, based intervals. methods may help simplifying posterior
obviating need improving exact calculations.
also worth emphasizing positive aspect results potential
practical utility. previous section showed variational method provide accurate approximations posterior marginals. Combined interval bounds
section|which calculated eciently|the user obtain guarantees approximately third approximations. Given relatively benign rate increase false
positives function true positives (Figure 11), guarantees may suce. Finally,
diseases bounds loose also perturbation methods available
(Jaakkola, 1997) help validate approximations diseases.

6. Discussion
Let us summarize variational inference method evaluate results
obtained.
variational method begins parameterized upper lower bounds individual conditional probabilities nodes model. QMR-DT, bounds
exponentials linear functions, introducing model corresponds
delinking nodes graph. Sums products bounds yield bounds, thus
readily obtain parameterized bounds marginal probabilities, particular upper
lower bounds likelihood.
exploited likelihood bounds evaluating output likelihood-weighted
sampling algorithm. Although sampling algorithm yield reliable results across
corpus CPC cases, utilized variational upper lower bounds select
among samples able obtain sampling results consistent
runs. suggests general procedure variational bounds used assess
convergence sampling algorithm. (One also imagine intimate relationship
algorithms variational bounds used adjust on-line
course sampler).
fact bounds likelihood (or marginal probabilities)
critical|the bounding property allows us find optimizing values variational parameters minimizing upper-bounding variational distribution maximizing
lower-bounding variational distribution. case QMR-DT network (a bipartite noisy-OR graph), minimization problem convex optimization problem
maximization problem solved via EM algorithm.
variational parameters optimized, resulting variational distribution
exploited inference engine calculating approximations posterior probabilities.
technique focus paper. Graphically, variationally transformed
model viewed sub-graph original model finding
nodes delinked. sucient number findings delinked variationally
possible run exact algorithm resulting graph. approach yields
approximations posterior marginals disease nodes.
313

fiJaakkola & Jordan

found empirically approximations appeared provide good approximations true posterior marginals. case tractable set CPC cases
(cf. Figure 7) and|subject assumption obtained good surrogate
gold standard via selected output sampler|also case full CPC
corpus (cf. Figure 11).
also compared variational algorithm state-of-the-art algorithm QMRDT, likelihood-weighted sampler Shwe Cooper (1991). found variational algorithm outperformed likelihood-weighted sampler tractable cases
full corpus. particular, fixed accuracy requirement variational algorithm significantly faster (cf. Figure 5), fixed time allotment variational
algorithm significantly accurate (cf. Figure 8 Figure 11).
results less satisfactory interval bounds posterior marginals.
Across full CPC corpus found approximately one third disease
bounds tight half diseases bounds vacuous. major impediment
obtaining tighter bounds appears lie variational approximation per se
rather exact subroutine, investigating exact methods improved
numerical properties.
Although focused detail QMR-DT model paper, worth
noting variational probabilistic inference methodology considerably general.
Specifically, methods described limited bi-partite
graphical structure QMR-DT model, necessary employ noisy-OR nodes
(Jaakkola & Jordan, 1996). also case type transformations
exploited QMR-DT setting extend larger class dependence relations
based generalized linear models (Jaakkola, 1997). Finally, review applications
variational methods variety graphical model architectures, see Jordan, et al.
(1998).
promising direction future research appears integration various
kinds approximate exact methods (see, e.g., Dagum & Horvitz, 1992; Jensen, Kong,
& Kjrulff, 1995). particular, search-based methods (Cooper, 1985; Peng & Reggia,
1987, Henrion, 1991) variational methods yield bounds probabilities, and,
indicated introduction, seem exploit different aspects structure complex probability distributions. may possible combine bounds
algorithm|the variational bounds might used guide search, searchbased bounds might used aid variational approximation. Similar comments
made respect localized partial evaluation methods bounded conditioning
methods (Draper & Hanks, 1994; Horvitz, et al., 1989). Also, seen variational
bounds used assessing whether estimates Monte Carlo sampling algorithms
converged. interesting hybrid would scheme variational approximations refined treating initial conditions sampler.
Even without extensions results paper appear quite promising.
presented algorithm runs real time large-scale graphical model
exact algorithms general infeasible. results obtained appear
reasonably accurate across corpus dicult diagnostic cases. work
needed, believe results indicate promising role variational inference
developing, critiquing exploiting large-scale probabilistic models QMR-DT.
314

fiVariational Probabilistic Inference QMR-DT

Acknowledgements
would like thank University Pittsburgh Randy Miller use
QMR-DT database. also want thank David Heckerman suggesting attack
QMR-DT variational methods, providing helpful counsel along way.

Appendix A. Duality
upper lower bounds individual conditional probability distributions form
basis variational method based \dual" \conjugate" representations
convex functions. present brief description convex duality appendix,
refer reader Rockafellar (1970) extensive treatment.
Let f (x) real-valued, convex function defined convex set X (for example,
X = Rn ). simplicity exposition, assume f well-behaved (differentiable)
function. Consider graph f , i.e., points (x; f (x)) n + 1 dimensional space.
fact function f convex translates convexity set f(x; ) : f (x)g
called epigraph f denoted epi(f ) (Figure 13). elementary property
f(x)
epi(f)

x - - f*() 0

x - - f*() 0

x-y-0

Figure 13: Half-spaces containing convex set epi(f ). conjugate function f ( )
defines critical half-spaces whose intersection epi(f ), or, equivalently,
defines tangent planes f (x).
convex sets represented intersection half-spaces
contain (see Figure 13). parameterizing half-spaces obtain dual
representations convex functions. end, define half-space condition:
(x; ) xT , , 0

(34)

parameterize (non-vertical) half-spaces. interested characterizing half-spaces contain epigraph f . require therefore points
epigraph must satisfy half-space condition: (x; ) 2 epi(f ), must
xT , , 0. holds whenever xT , f (x) , 0 points epigraph
property f (x). Since condition must satisfied x 2 X , follows
315

fiJaakkola & Jordan


max
f xT , f (x) , g 0;
x2X

(35)

well. Equivalently,

max
f xT , f (x) g
x2X

(36)

right-hand side equation defines function , known
\dual" \conjugate" function f ( ). function, also convex function, defines
critical half-spaces needed representation epi(f ) intersection
half-spaces (Figure 13).
clarify duality f (x) f (x), let us drop maximum rewrite
inequality as:

xT f (x) + f ( )

(37)

equation, roles two functions interchangeable may suspect
also f (x) obtained dual function f (x) optimization procedure.
fact case have:

f (x) = max
f xT , f () g
2

(38)

equality states dual dual gives back original function. provides
computational tool calculating dual functions.
concave (convex down) functions results analogous; replace max
min, lower bounds upper bounds.

Appendix B. Optimization Variational Parameters

variational method described involves replacing selected local conditional
probabilities either upper-bounding lower-bounding variational transformations.
product bounds bound, variationally transformed joint probability
distribution bound (upper lower) true joint probability distribution. Moreover, sums bounds bound sum, obtain bounds marginal
probabilities marginalizing variationally transformed joint probability distribution.
particular, provides method obtaining bounds likelihood (the marginal
probability evidence).
Note variationally transformed distributions bounds arbitrary values
variational parameters (because individually transformed node conditional probability bound arbitrary values variational parameter). obtain optimizing
values variational parameters, take advantage fact transformed
distribution bound, either minimize (in case upper bounds) maximize
(in case lower bounds) transformed distribution respect variational
parameters. optimization process provides tight bound marginal
probability interest (e.g., likelihood) thereby picks particular variational
distribution subsequently used approximate inference.
316

fiVariational Probabilistic Inference QMR-DT

appendix discuss optimization problems must solve case
noisy-OR networks. consider upper lower bounds separately, beginning
upper bound.

Upper Bound Transformations

goal isPto compute tight upper bound likelihood observed findings:
P (f + ) = P (f + jd)P (d). discussed Section 4.2, obtain upper bound
P (f + jd) introducing upper bounds individual node conditional probabilities.
represent upper bound P (f + jd; ), product across individual variational transformations may contain contributions due findings treated
exactly (i.e., transformed). Marginalizing across obtain bound:

P (f + )

X



P (f + jd; )P (d) P (f + j):

(39)

latter quantity wish minimize respect variational parameters
.
simplify notation assume first positive findings transformed (and therefore need optimized) remaining conditional probabilities
treated exactly. notation P (f + j ) given

P (f + j )

=

/

2
X
4

3"

#


+
+
P (fi jd; i)5
P (fi jd) P (dj )
i>m
j
im
8
9
<Y
=
E : P (fi+ jd; i); ;
im

(40)
(41)

expectation taken respect posterior distribution diseases
given positive findings plan treat exactly. Note proportionality
constant depend variational parameters (it likelihood exactly
treated positive findings). insert explicit forms transformed conditional
probabilities (see Eq. (17)) Eq. (41) find:

P (f + j) /
=

8
9
< ( +P ),f ( ) =

E : e i0 j ij j
;
im
P

P

e im (i i0 ,f (i)) E e j; im ij dj

(42)
(43)

simply converted products sums exponent pulled
terms constants respect expectation. log-scale,
proportionality becomes equivalence constant:
P

X




ij
j
+

j;i


log P (f j ) = C + (i i0 , f (i)) + log E e
im

317

(44)

fiJaakkola & Jordan

Several observations order. Recall f (i ) conjugate concave function
f (the exponent), therefore also concave; reason ,f (i ) convex.
Appendix C prove remaining term:
P

log E e

j;im iij dj



(45)

also convex function variational parameters. Now, since sum convex
functions convex, conclude log P (f + j ) convex function variational
parameters. means local minima optimization problem.
may safely employ standard Newton-Raphson procedure solve r log P (f + j ) = 0.
Alternatively utilize fixed-point iterations. particular, calculate derivatives
variational form iteratively solve individual variational parameters k
derivatives zero. derivatives given follows:
8

9

@ log P (f + j) = + log k + E <X =
k0
kj j ;
:
@k
1 + k
j

(46)

@ 2 log P (f + j) = 1 , 1 + Var <X = ;
: j kj j ;
@ 2 k
k 1 + k

(47)

8

9

expectation variance respect posterior approximation
P (djf + ; ), derivatives computed time linear number associated diseases finding. benign scaling variance calculations comes
exploiting special properties noisy-OR dependence marginal independence
diseases.
Calculating expectations Eq. (7) exponentially costly number exactly
treated positive findings. large number positive findings,
recourse simplified procedure optimize variational parameters
transformed positive findings. resulting variational parameters
suboptimal, found practice incurred loss accuracy typically quite
small. simulations reported paper, optimized variational parameters
approximately half exactly treated findings introduced. (To precise,
case 8, 12 16 total findings treated exactly, optimized parameters
4, 8, 8 findings, respectively, introduced).

Lower Bound Transformations

Mimicking case upper bounds, replace individual conditional probabilities
findings lower-bounding transformations, resulting lower-bounding expression
P (f + jd; q). Taking product P (d) marginalizing yields lower bound
likelihood:
X
P (f + ) P (f + jd; q )P (d) P (f + jq):
(48)


wish maximize P (f + jq ) respect variational parameters q obtain
tightest possible bound.
318

fiVariational Probabilistic Inference QMR-DT

problem mapped onto standard optimization problem statistics.
particular, treating latent variable, f observed variable, q parameter
vector, optimization P (f + jq ) (or logarithm) viewed standard maximum
likelihood estimation problem latent variable model. solved using EM
algorithm (Dempster, Laird, & Rubin, 1977). algorithm yields sequence variational
parameters monotonically increase objective function log P (f + jq ). Within EM
framework, obtain update variational parameters maximizing expected
complete log-likelihood:




E log P (f + jd; q )P (d) =

X



n



E log P (fi+ jd; qji) + constant;

(49)

q old denotes vector variational parameters update, constant term independent variational parameters q expectation
respect posterior distribution P (djf + ; q old ) / P (f + jd; q old)P (d). Since variational
parameters associated conditional probabilities P (fi+ jd; qji) independent one
another, maximize term sum separately. Recalling form
variational transformation (see Eq. (24)), have:
!

"

#

E
=
qjji E fdjg f io + qij , f ( io )
j ji
j
+f ( io )
(50)
maximize respect qj ji keeping expectations E fdj g fixed.
n

log P (fi+ jd; qji)



X

optimization problem solved iteratively monotonically performing
following synchronous updates normalization:

qj ji

!

"

!

E fdj g qjji f io + qij , ij f 0 io + qij , qjji f ( io )
j ji
j ji

#

(51)

f 0 denotes derivative f . (The update guaranteed non-negative).
algorithm easily extended handle case positive
findings transformed. new feature conditional
probabilities products P (f + jd; q old) P (f + jd; q ) left intact, i.e.,
transformed; optimization respect variational parameters corresponding
transformed conditionals proceeds before.

Appendix C. Convexity

purpose appendix demonstrate function:
P

log E e

j;im iij dj



(52)

convex function variational parameters . note first
ane transformaP
tions change convexity properties. Thus convexity X = j;im ij dj implies
319

fiJaakkola & Jordan

convexity variational parameters . remains show
n



log E e X = log

X



pi e Xi = f (X~ )

(53)

convex function vector X~ = fX1 : : :Xn gT ; indicated discrete
values range random variable X Xi denoted probability measure
values pi . Taking gradient f respect Xk gives:

@ f (X~ ) = Ppk e Xk q
k
@Xk
pi e Xi

(54)

2
Hkl = @X@ @X f (X~ ) = kl qk , qk ql

(55)

X
X
X
Z~ HZ~ = qk Zk2 , ( qk Zk )( ql Zl) = VarfZ g 0

(56)

qk defines probability distribution. convexity revealed positive semidefinite Hessian H, whose components case
k

l

see H positive semi-definite, consider
k

k

l

VarfZ g variance discrete random variable Z takes values Zi
probability qi .

References

D'Ambrosio, B. (1993). Incremental probabilistic inference. Proceedings Ninth
Conference Uncertainty Artificial Intelligence. San Mateo, CA: Morgan Kaufmann.
D'Ambrosio, B. (1994). Symbolic probabilistic inference large BN20 networks. Proceedings Tenth Conference Uncertainty Artificial Intelligence. San Mateo,
CA: Morgan Kaufmann.
Cooper, G. (1985). NESTOR: computer-based medical diagnostic aid integrates
causal probabilistic knowledge. Ph.D. Dissertation, Medical Informatics Sciences,
Stanford University, Stanford, CA. (Available UMI
http://wwwlib.umi.com/dissertations/main).
Cooper, G. (1990). computational complexity probabilistic inference using Bayesian
belief networks. Artificial Intelligence, 42, 393{405.
Dagum, P., & Horvitz, E. (1992). Reformulating inference problems selective
conditioning. Proceedings Eighth Annual Conference Uncertainty
Artificial Intelligence.
Dagum, P., & Horvitz, E. (1993). Bayesian analysis simulation algorithms inference
Belief networks. Networks, 23, 499{516.
320

fiVariational Probabilistic Inference QMR-DT

Dagum, P., & Luby, M. (1993). Approximate probabilistic reasoning Bayesian belief
networks NP-hard. Artificial Intelligence, 60, 141{153.
Dechter, R. (1997). Mini-buckets: general scheme generating approximations automated reasoning. Proceedings Fifteenth International Joint Conference
Artificial Intelligence.
Dechter, R. (1998). Bucket elimination: unifying framework probabilistic inference.
M. I. Jordan (Ed.), Learning Graphical Models. Cambridge, MA: MIT Press.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete data
via EM algorithm. Journal Royal Statistical Society B, 39, 1{38.
Draper, D., & Hanks, S. (1994). Localized partial evaluation belief networks. Proceedings Tenth Annual Conference Uncertainty Artificial Intelligence.
Fung, R., & Chang, K. C. (1990). Weighting integrating evidence stochastic simulation Bayesian networks. Proceedings Fifth Conference Uncertainty
Artificial Intelligence. Amsterdam: Elsevier Science.
Gelfand, A., & Smith, A. (1990). Sampling-based approaches calculating marginal Densities. Journal American Statistical Association, 85, 398{409.
Heckerman, D. (1989). tractable inference algorithm diagnosing multiple diseases.
Proceedings Fifth Conference Uncertainty Artificial Intelligence.
Henrion, M. (1991). Search-based methods bound diagnostic probabilities large
belief nets. Proceedings Seventh Conference Uncertainty Artificial Intelligence.
Horvitz, E. Suermondt, H., & Cooper, G. (1989). Bounded conditioning: Flexible inference
decisions scarce resources. Proceedings Fifth Conference Uncertainty Artificial Intelligence.
Jaakkola, T. (1997). Variational methods inference learning graphical models.
PhD thesis, Department Brain Cognitive Sciences, Massachusetts Institute
Technology.
Jaakkola, T., & Jordan, M. (1996). Recursive algorithms approximating probabilities
graphical models. Advances Neural Information Processing Systems 9. Cambridge, MA: MIT Press.
Jensen, C. S., Kong, A., & Kjrulff, U. (1995). Blocking-Gibbs sampling large
probabilistic expert systems. International Journal Human-Computer Studies, 42,
647{666.
Jensen, F. (1996). Introduction Bayesian networks. New York: Springer.
321

fiJaakkola & Jordan

Jordan, M., Ghaharamani, Z. Jaakkola, T., & Saul, L. (in press). introduction
variational methods graphical models. Machine Learning.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computations probabilities graphical structures application expert systems (with discussion). Journal
Royal Statistical Society B, 50, 157{224.
MacKay, D. J. C. (1998). Introduction Monte Carlo methods. M. I. Jordan (Ed.),
Learning Graphical Models. Cambridge, MA: MIT Press.
Middleton, B., Shwe, M., Heckerman, D., Henrion, M., Horvitz, E., Lehmann, H., & Cooper,
G. (1990). Probabilistic diagnosis using reformulation INTERNIST-1/QMR
knowledge base II. Evaluation diagnostic performance. Section Medical Informatics Technical report SMI-90-0329, Stanford University.
Miller, R. A., Fasarie, F. E., & Myers, J. D. (1986). Quick medical reference (QMR)
diagnostic assistance. Medical Computing, 3, 34{48.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. San Mateo, CA: Morgan
Kaufmann.
Peng, Y., & Reggia, J. (1987). probabilistic causal model diagnostic problem solving {
Part 2: Diagnostic strategy. IEEE Trans. Systems, Man, Cybernetics: Special
Issue Diagnosis, 17, 395{406.
Poole, D. (1997). Probabilistic partial evaluation: Exploiting rule structure probabilistic
inference. Proceedings Fifteenth International Joint Conference Artificial
Intelligence.
Rockafellar, R. (1972). Convex Analysis. Princeton University Press.
Shachter, R. D., & Peot, M. (1990). Simulation approaches general probabilistic inference
belief networks. Proceedings Fifth Conference Uncertainty Artificial
Intelligence. Elsevier Science: Amsterdam.
Shenoy, P. P. (1992). Valuation-based systems Bayesian decision analysis. Operations
Research, 40, 463{484.
Shwe, M., & Cooper, G. (1991). empirical analysis likelihood { weighting simulation
large, multiply connected medical belief network. Computers Biomedical
Research, 24, 453-475.
Shwe, M., Middleton, B., Heckerman, D., Henrion, M., Horvitz, E., Lehmann, H., & G.
Cooper (1991). Probabilistic diagnosis using reformulation INTERNIST1/QMR knowledge base I. probabilistic model inference algorithms. Methods
Information Medicine, 30, 241{255.

322

fiJournal Artificial Intelligence Research 10 (1999) 199-241

Submitted 9/98; published 4/99

Probabilistic Deduction
Conditional Constraints Basic Events
Thomas Lukasiewicz

Institut fur Informatik, Universitat Gieen
Arndtstrae 2, D-35392 Gieen, Germany

lukasiewicz@informatik.uni-giessen.de

Abstract

study problem probabilistic deduction conditional constraints basic
events. show globally complete probabilistic deduction conditional constraints
basic events NP-hard. concentrate special case probabilistic
deduction conditional constraint trees. elaborate ecient techniques globally
complete probabilistic deduction. detail, conditional constraint trees point
probabilities, present local approach globally complete probabilistic deduction,
runs linear time size conditional constraint trees. conditional
constraint trees interval probabilities, show globally complete probabilistic
deduction done global approach solving nonlinear programs. show
nonlinear programs transformed equivalent linear programs,
solvable polynomial time size conditional constraint trees.

1. Introduction
Dealing uncertain knowledge plays important role knowledge representation
reasoning. many different formalisms methodologies handling uncertainty.
directly indirectly based probability theory.
paper, focus probabilistic deduction conditional constraints basic
events (that is, interval restrictions conditional probabilities elementary events).
considered probabilistic deduction problems consist probabilistic knowledge base
probabilistic query. give classical example. probabilistic knowledge base,
may take probabilistic knowledge ostriches birds, probability
Tweety bird greater 0.90, probability Tweety ostrich
provided bird greater 0.80. probabilistic query, may wonder
entailed greatest lower least upper bound probability Tweety
ostrich. solution probabilistic deduction problem 0.72 entailed
greatest lower bound 1.00 entailed least upper bound.
generally, probabilistic deduction conditional constraints propositional
events done global approach linear programming local approach
iterative application inference rules. Note immediately NP-hard, since
generalizes satisfiability problem classical propositional logic (see Section 2.2).
Research global approach spread particular important work probabilistic logic Nilsson (1986) (see also work Paa, 1988). main focus
analyzing computational complexity satisfiability entailment probabilistic logic developing ecient linear programming algorithms problems.
c 1999 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiLukasiewicz

Georgakopoulos et al. (1988) show satisfiability problem probabilistic logic
NP-complete propose apply column generation techniques processing.
approach developed Kavvadias Papadimitriou (1990), Jaumard et al.
(1991), Andersen Hooker (1994), Hansen et al. (1995). particular, Jaumard et
al. (1991) report promising experimental results eciency special cases probabilistic satisfiability entailment. Moreover, Kavvadias Papadimitriou (1990)
Jaumard et al. (1991) identify special cases probabilistic satisfiability solved
polynomial time. work global approach concentrates reducing number linear constraints (Luo et al. 1996) number variables (Lukasiewicz, 1997).
Finally, Fagin et al. (1992) present sound complete axiom system reasoning
probabilities expressed linear inequalities propositional events. show
satisfiability problem quite expressive framework still NP-complete.
early work, Dubois Prade (1988) use inference rules model default reasoning imprecise numerical fuzzy quantifiers. reason, subsequent research
inference rules especially aims analyzing patterns human commonsense reasoning
(Dubois et al. 1990, 1993; Amarger et al. 1991; Thone, 1994; Thone et al. 1995). Frisch
Haddawy (1994) discuss use inference rules deduction probabilistic logic.
Recent work inference rules concentrates integrating probabilistic knowledge description logics (Heinsohn, 1994) analyzing interplay taxonomic
probabilistic deduction (Lukasiewicz 1998a, 1999a).
summarize main characteristics global local approach.
global approach performed within quite rich probabilistic languages (Fagin
et al., 1992). Crucially, probabilistic deduction linear programming globally complete
(that is, really provides requested tightest bounds entailed whole probabilistic
knowledge base). However, main drawback global approach generally
provide useful explanatory information deduction process. Finally, results
special-case tractability global approaches driven technical possibilities
linear programming techniques needs artificial intelligence applications.
Hence, seem useful artificial intelligence context.
main advantage local approach deduced results explained
natural way sequence applied inference rules (Amarger et al. 1991; Frisch
& Haddawy, 1994). However, iterative application inference rules generally restricted quite narrow probabilistic languages. Moreover, rarely within
restricted languages globally complete (Frisch Haddawy, 1994, give example
globally complete local probabilistic deduction restricted framework). Finally,
far computational complexity concerned, experimental
theoretical results special-case tractability local approaches.
main motivating idea paper elaborate ecient local techniques
globally complete probabilistic deduction. Inspired previous work inference rules,
focus research language conditional constraints basic events:
Dubois Prade (1988) study chaining two bidirectional conditional constraints
basic events (\quantified syllogism rule") special cases. Dubois et
al. (1990) additionally discuss probabilistic deductions conjunctions basic events.
Furthermore, describe open problem probabilistic deduction along chain
two bidirectional conditional constraints basic events. later work, Dubois
200

fiProbabilistic Deduction Conditional Constraints Basic Events

et al. (1993) use qualitative version \quantified syllogism rule" approach
reasoning linguistic quantifiers. Amarger et al. (1991) propose apply \quantified syllogism rule" \generalized Bayes' rule" sets bidirectional conditional
constraints basic events. report promising experimental results global completeness computational complexity presented deduction technique. However,
deduction technique generally globally complete. Thone (1994) examines trees
bidirectional conditional constraints basic events. presents linear-time deduction
technique based system inference rules computes certain logically
entailed greatest lower bounds (in technical notions paper, defined
below, tight lower answers conclusion-restricted queries computed).
first contribution paper, show globally complete probabilistic deduction conditional constraints basic events NP-hard. surprising
quite restricted class probabilistic deduction problems still computationally dicult. Hence, unlikely algorithm eciently solves probabilistic
deduction problems conditional constraints basic events. However, still
hope ecient special-case, average-case, approximation algorithms.
paper, elaborate ecient special-case algorithms. detail, concentrate probabilistic deduction conditional constraint trees. interesting subclass probabilistic deduction problems conditional constraints basic events.
Conditional constraint trees undirected trees basic events nodes bidirectional conditional constraints basic events edges nodes (that is,
deduction conditional constraint trees generalization deduction along chain
bidirectional conditional constraints basic events). Like Bayesian networks, conditional
constraint trees represent well-structured probabilistic knowledge base. Differently
Bayesian networks, encode probabilistic independencies.
main contribution paper, following results. conditional constraint trees point probabilities, present functions deducing greatest lower
least upper bounds linear time size conditional constraint trees. Moreover,
conditional constraint trees interval probabilities, show greatest lower bounds
deduced way, linear time size conditional constraint trees.
However, computing least upper bounds turns computationally dicult.
done solving special nonlinear programs. show nonlinear programs
transformed equivalent linear programs. resulting linear programs
number variables inequalities linear polynomial, respectively, size
conditional constraint trees. Thus, way deducing least upper bounds still runs
polynomial time size conditional constraint trees, since linear programming
runs polynomial time size linear programs.
Another important contribution paper related question whether
perform probabilistic deduction conditional constraints iterative application
inference rules linear programming. one hand, idea inference rules carries
us ecient techniques globally complete probabilistic deduction conditional
constraint trees. particular, considered deduction problems generalize patterns
commonsense reasoning. However, hand, corresponding proofs soundness
global completeness technically quite complex. Hence, seems unlikely
results work extended significantly general probabilistic deduction
201

fiLukasiewicz

problems. Note companion paper (1998a, 1999a) reports similar limits local
approach probabilistic deduction taxonomic knowledge.
rest paper organized follows. Section 2, formulate probabilistic deduction problems considered work. Section 3 focuses probabilistic
satisfiability conditional constraint trees. Section 4 deals globally complete probabilistic deduction exact general conditional constraint trees. Section 5, give
comparison Bayesian networks. Section 6 summarizes main results work.

2. Formulating Probabilistic Deduction Problem

section, introduce syntactic semantic notions related probabilistic
knowledge general conditional constraint trees particular.

2.1 Probabilistic Knowledge

focusing details conditional constraint trees, give general introduction
kind probabilistic knowledge considered work. deal conditional
constraints propositional events. represent interval restrictions conditional
probabilities propositional events. Note formal background introduced
section commonly accepted literature (see especially work Frisch Haddawy, 1994, work spirit).
assume nonempty finite set basic events B = fB1 ; B2 ; : : : ; Bn g. set
conjunctive events CB closure B Boolean operation ^. abbreviate
conjunctive event C ^ CD . set propositional events GB closure B
Boolean operations ^ :. abbreviate propositional events G ^ H :G
GH G, respectively. false event B1 ^ :B1 true event :(B1 ^ :B1 )
abbreviated ? >, respectively. Conditional constraints expressions form
(H jG)[u1 ; u2 ] real numbers u1 ; u2 2 [0; 1] propositional events G H .
conditional constraint (H jG)[u1 ; u2 ], call G premise H conclusion.
define probabilistic interpretations propositional events conditional constraints, introduce atomic events binary relation ) atomic propositional events. set atomic events AB defined AB = fE1 E2 En j Ei = Bi
Ei = B 2 [1: n]g. Note atomic event interpreted possible
world (which corresponds mapping B ftrue; falseg). atomic events
propositional events G, let ) G iff AG propositional contradiction.
probabilistic interpretation Pr mapping AB [0; 1] Pr (A)
2 AB sum 1. Pr extended well-defined way propositional events G by:
Pr (G) sum Pr (A) 2 AB ) G. Pr extended conditional
constraints by: Pr j= (H jG)[u1 ; u2 ] iff u1 Pr (G) Pr (GH ) u2 Pr (G).
Note conditional constraints characterize conditional probabilities events, rather
probabilities conditional events (Coletti, 1994; Gilio & Scozzafava, 1994). Note also
Pr (G) = 0 always entails Pr j= (H jG)[u1 ; u2 ]. semantics conditional probability
statements also assumed Halpern (1990) Frisch Haddawy (1994).
notions models, satisfiability, logical consequence conditional constraints
defined classical way. probabilistic interpretation Pr model conditional
constraint (H jG)[u1 ; u2 ] iff Pr j= (H jG)[u1 ; u2 ]. Pr model set conditional
202

fiProbabilistic Deduction Conditional Constraints Basic Events

constraints KB , denoted Pr j= KB , iff Pr model (H jG)[u1 ; u2 ] 2 KB . KB
satisfiable iff model KB exists. (H jG)[u1 ; u2 ] logical consequence KB; denoted
KB j= (H jG)[u1 ; u2 ], iff model KB also model (H jG)[u1 ; u2 ].
conditional constraint (H jG)[u1 ; u2 ] set conditional constraints KB , let
u denote set real numbers u 2 [0; 1] exists model Pr KB
u Pr (G) = Pr (GH ) Pr (G) > 0. Now, easily verify (H jG)[u1 ; u2 ] logical
consequence KB iff u1 inf u u2 sup u.
observation yields canonical notion tightness logical consequences conditional constraints. conditional constraint (H jG)[u1 ; u2 ] tight logical consequence
KB; denoted KB j=tight (H jG)[u1 ; u2 ], iff u1 = inf u u2 = sup u.
set u closed interval real line (Frisch & Haddawy, 1994). Note
u = ;, canonically define inf u = max [0; 1] = 1 sup u = min [0; 1] = 0. Thus, u = ;
iff KB j= (Gj>)[0; 0] iff KB j=tight (H jG)[1; 0] iff KB j= (H jG)[u1 ; u2 ] u1 ; u2 2 [0; 1].
Based introduced notion tight logical consequence, probabilistic deduction
problems solutions formally specified follows.
probabilistic knowledge base (B; KB ) consists set basic events B set
conditional constraints KB GB u1 u2 (H jG)[u1 ; u2 ] 2 KB . probabilistic
query probabilistic knowledge base (B; KB ) expression form 9(F jE )[x1 ; x2 ]
E; F 2 GB two different variables x1 x2 . tight answer substitution
= fx1 =u1 ; x2 =u2 g u1 ; u2 2 [0; 1] KB j=tight (F jE )[u1 ; u2 ] (we call 1 =
fx1 =u1 g tight lower answer 2 = fx2 =u2 g tight upper answer). correct answer
substitution = fx1 =u1 ; x2 =u2 g u1 ; u2 2 [0; 1] KB j= (F jE )[u1 ; u2 ].
Finally, define notions soundness completeness related inference
rules techniques probabilistic deduction. inference rule KB ` (H jG)[u1 ; u2 ]
sound iff KB j= (H jG)[u1 ; u2 ], (H jG)[u1 ; u2 ] conditional constraint KB
set conditional constraints. sound locally complete iff KB j=tight (H jG)[u1 ; u2 ].
technique probabilistic deduction sound set probabilistic queries Q iff computes correct answer given query Q. sound globally complete Q
iff computes tight answer given query Q.

2.2 Computational Complexity

framework conditional constraints propositional events, optimization problem computing tight answer probabilistic query immediately NP-hard, since
generalizes satisfiability problem classical propositional logic (the NP-complete problem deciding whether propositional formula conjunctive normal form satisfiable;
see especially survey Garey Johnson, 1979).
Surprisingly, optimization problem computing tight answer probabilistic
query remains NP-hard even consider conditional constraints basic events:

Theorem 2.1 optimization problem computing tight answer probabilistic

query basic events directed probabilistic knowledge base basic events
NP-hard.

Proof. NP-complete decision problem graph 3-colorability (Garey & Johnson, 1979)
polynomially-reduced optimization problem computing tight answer
203

fiLukasiewicz

probabilistic query basic events directed probabilistic knowledge base
basic events. proof follows similar lines proof NP-hardness 2PSAT
given Georgakopoulos et al. (1988).
Let (V; E ) finite undirected graph. construct probabilistic knowledge base
(B; KB ) follows. initialize (B; KB ) (fB g; ;). node v 2 V , increase
B new basic events Bv1, Bv2, Bv3. node v 2 V 2 f1; 2; 3g,
increase KB (B jBvi )[1; 1] (Bvi jB )[1=3; 1=3]. node v 2 V
i; j 2 f1; 2; 3g < j , increase KB (Bvj jBvi )[0; 0]. edge fu; vg 2 E
2 f1; 2; 3g, increase KB (Bvi jBui )[0; 0]. easy see probabilistic
knowledge base (B; KB ) constructed polynomial time size (V; E ).
Now, show (V; E ) 3-colorable iff fx1 =1; x2 =1g tight answer
probabilistic query 9(B jB )[x1 ; x2 ] (B; KB ), equivalently, iff KB satisfiable:
(V; E ) 3-colorable, exists mapping c1 V f1; 2; 3g c1 (u) 6=
c1 (v) edges fu; vg 2 E . Thus, cyclic permutation members f1; 2; 3g
c2 ; c3 : V ! f1; 2; 3g defined c2 (v) = (c1 (v)) c3 (v) = (c2 (v))
nodes v 2 V , also c2 (u) 6= c2 (v) c3 (u) 6= c3 (v) edges fu; vg 2 E .
j 2 f1; 2; 3g, let Aj 2 AB Aj ) B Aj ) Bvi iff cj (v) = nodes v 2 V
2 f1; 2; 3g. Pr : AB ! [0; 1] defined Pr (A) = 1=3 2 fA1 ; A2 ; A3 g
Pr (A) = 0 2 AB n fA1 ; A2 ; A3 g, Pr model KB .
Conversely, model Pr KB , atomic event 2 AB
Pr (A) > 0. Thus, c : V ! f1; 2; 3g defined c(v) = iff ) Bvi nodes v 2 V ,
c(u) 6= c(v) edges fu; vg 2 E . Hence, (V; E ) 3-colorable. 2
Hence, unlikely ecient algorithm computing tight answer
probabilistic queries basic events directed given probabilistic
knowledge base basic events. However, may still ecient algorithms
solving specialized probabilistic deduction problems.
rest work deals probabilistic deduction conditional constraint trees.
next section provides motivating example, gives evidence practical
importance kind probabilistic deduction problems.

2.3 Motivating Example
senior student mathematics describes experience successful university follows. success student (su) uenced well-informed (wi)
well-prepared (wp) student is. Well-informedness reached interviewing
professors (pr) asking senior students (st). well-prepared uenced
much time invested books (bo), exercises (ex), hobbies (ho).
estimated probability student successful given wellinformed lies 0.60 0.70, probability student well-informed
given successful greater 0.85, probability student successful
given well-prepared greater 0.95, probability student
well-prepared given successful greater 0.95.
probabilistic knowledge completed probabilistic estimations given
probabilistic knowledge base (B; KB ) Fig. 1, B set nodes fsu; wi; wp; pr;
204

fiProbabilistic Deduction Conditional Constraints Basic Events

st; bo; ex; hog KB least set conditional constraints contains (Y jX )[u1 ; u2 ]
arrow X labeled u1 ; u2 .
st

ho
.6,.7

.95,1

.35,.4

.6,.7

wi

su
.85,1

.95,1

.35,.4

.95,1

.85,.9

wp
.95,1

.05,.1

.95,1

pr

ex
.85,.9
.95,1

bo

Figure 1: Conditional Constraint Tree
may wonder whether useful successful university interview
professors, study books, spend time one's hobbies, studying
books spending time one's hobbies. expressed probabilistic queries 9(sujpr)[x1 ; x2 ], 9(sujbo)[x1 ; x2 ], 9(sujho)[x1 ; x2 ], 9(sujbo ho)[x1 ; x2 ],
yield tight answers fx1 =0:00, x2 =1:00g, fx1 =0:90; x2 =1:00g, fx1 =0:30; x2 =0:46g,
fx1 =0:71; x2 =1:00g, respectively.
may wonder whether successful students university interviewed professors, whether studied books, whether spent time hobbies,
whether studied books spent time hobbies.
expressed probabilistic queries 9(prjsu)[x1 ; x2 ], 9(bojsu)[x1 ; x2 ], 9(hojsu)[x1 ; x2 ],
9(bo hojsu)[x1 ; x2 ], yield tight answers fx1=0:00, x2=0:17g, fx1=0:90; x2 =1:00g,
fx1 =0:30; x2 =0:45g, fx1=0:25; x2 =0:45g, respectively.

2.4 Conditional Constraint Trees

formally define conditional constraint trees queries conditional constraint trees.
provide additional examples, subsequently used running examples.
(general) conditional constraint tree probabilistic knowledge base (B; KB )
undirected tree (a singly connected undirected graph) (B; $) exists
KB contains exactly one pair conditional constraints (B jA)[u1 ; u2 ] (AjB )[v1 ; v2 ]
u1 ; v1 > 0 pair adjacent nodes B (note B = fB g implies KB = ;).
basic event B 2 B called leaf (B; KB ) iff exactly one neighbor (B; $).
conditional constraint tree exact iff u1 = u2 (B jA)[u1 ; u2 ] 2 KB .
query conditional constraint tree probabilistic query 9(F jE )[x1 ; x2 ] two
conjunctive events E F disjoint basic events paths
basic event E basic event F least one basic event common.
query 9(F jE )[x1 ; x2 ] conditional constraint tree premise-restricted iff E basic
event. conclusion-restricted iff F basic event. strongly conclusion-restricted
iff F basic event contained paths basic event E F .
complete iff EF contains exactly leaves (B; $).
205

fiLukasiewicz

Fig. 2 shows two conditional constraint trees one left side exact.

9(STUjMNQR)[x1 ; x2] query, 9(MSjQU)[x1 ; x2 ] query conditional
constraint trees Fig. 2. Furthermore, 9(STUjM)[x1 ; x2 ] premise-restricted query,
9(OjQRSTU)[x1 ; x2] strongly conclusion-restricted query, 9(QRSTUjM)[x1 ; x2 ] prem-

ise-restricted complete query conditional constraint trees Fig. 2.
1)

2)


.85

.85

P
1

.85



N
.85

.3,.4



Q

.9,1

.5,.6

N
.8,.9

.95
.95

.15

1

.8,.9



U

.8,.9

.95

1

.8,.9

P

.95

.55

.9,1

.9,1



U

.85
.35

.8,.9

.95

.95





.9,1


1
.1,.2

R

Q
.9,1
.9,1

R

Figure 2: Two Conditional Constraint Trees
conditional constraint trees (B; KB ), conjunctive events C , basic events B ,
write C ) B iff exists path G1 ; G2 ; : : : ; Gk basic event G1 C basic
event Gk = B (Gi+1 jGi )[1; 1] 2 KB 2 [1 : k , 1]. write B ) C iff
paths G1; G2 ; : : : ; Gk basic event G1 = B basic event Gk C , holds
(Gi+1 jGi )[1; 1] 2 KB 2 [1 : k , 1]. is, conditions C ) B B ) C
immediately entail KB j= (B jC )[1; 1] KB j= (C jB )[1; 1], respectively.
Note restriction u1 ; v1 > 0 (B jA)[u1 ; u2 ], (AjB )[v1 ; v2 ] 2 KB made
technical convenience. deduction technique Section 4 easily generalized
conditional constraint trees (B; KB ) satisfy restriction u1 > 0 iff v1 > 0
(B jA)[u1 ; u2 ]; (AjB )[v1 ; v2 ] 2 KB (Lukasiewicz, 1996).
restriction query 9(F jE )[x1 ; x2 ], paths basic event E
basic event F least one basic event common crucial deduction
technique Section 4. assures problem computing tight answer
complete query reduced problems computing tight answer premiserestricted complete query tight answer strongly conclusion-restricted complete
query. Note restriction trivially satisfied premise- conclusion-restricted
queries (for example, queries Section 2.3).
Especially tight answers conclusion-restricted queries seem quite important
practice. may used characterize probability uncertain basic events given
collection basic events known certainty.
206

fiProbabilistic Deduction Conditional Constraints Basic Events

3. Probabilistic Satisfiability

section, show conditional constraint trees nice property
always satisfiable. is, within conditional constraint trees, user prevented
specifying inconsistent probabilistic knowledge.
First, note conditional constraint trees always trivial model
probability conjunction negated basic events one probability
atomic events zero.
next lemma shows that, given model Pr conditional constraint tree
real number [0; 1], construct new model Pr setting Pr (A) = Pr (A)
atomic events different conjunction negated basic events.
Note Pr 0 coincides trivial model Pr 1 identical Pr . lemma
crucial inductively constructing models conditional constraint trees.

Lemma 3.1 Let (B; KB ) conditional constraint tree B = fB1 ; B2 ; : : : ; Bng. Let
Pr model KB let real number [0; 1].
mapping Prs : AB ! [0; 1] Prs (A) = Pr (A) 2 AB n fB 1 B 2 B n g
Prs (B 1 B 2 B n ) = Pr (B 1 B 2 B n ) , + 1 model KB.
Proof. easily verify Prs probabilistic interpretation. remains show
Prs also model KB . Let (H jG)[u1 ; u2 ] 2 KB . Since Pr model KB ,
Pr j= (H jG)[u1 ; u2 ], hence u1 Pr (G) Pr (GH ) u2 Pr (G), thus also u1 Pr (G)
Pr (GH ) u2 Pr (G). Since neither B 1 B2 B n ) G B 1 B 2 B n ) GH , get
u1 Prs (G) Prs (GH ) u2 Prs (G) thus Prs j= (H jG)[u1 ; u2 ]. 2

Finally, following theorem shows conditional constraint trees always
nontrivial model basic events probability greater zero.

Theorem 3.2 Let (B; KB ) conditional constraint tree B = fB1 ; B2 ; : : : ; Bn g.
model Pr KB Pr (B1 B2 Bn ) > 0.
Proof. sucient show claim exact conditional constraint trees. claim

proved induction number basic events.
Basis: (B; KB ) = (fB g; ;), model Pr KB Pr (B ) > 0 given B; B 7! 0; 1
(note B; B 7! 0; 1 abbreviation Pr (B ) = 0 Pr (B ) = 1).
Induction: let (B; KB ) = (B1 [ B2 ; KB 1 [ KB 2 ) two exact conditional constraint trees
(B1 ; KB 1 ) = (fB; C g; f(C jB )[u; u]; (B jC )[v; v]g) (B2 ; KB 2 ) = (fC; D1 ; : : : ; Dk g; KB 2 )
B1 \ B2 = fC g. model Pr 1 KB 1 Pr 1 (BC ) > 0 given by:

B C; BC; BC; BC 7!

uv v,uv u,uv uv
u+v ; u+v ; u+v ; u+v

:

induction hypothesis, model Pr 2 KB 2 (that defined atomic
events B2 ) Pr 2 (CD1 Dk ) > 0. Lemma 3.1, assume Pr 2 (C ) = Pr 1 (C ).
probabilistic interpretation Pr atomic events B defined by:
Ac )Pr 2 (Ac A2 )
Pr (Ab Ac A2 ) = Pr 1 (AbPr
2 (Ac )

207

fiLukasiewicz

atomic events Ab , Ac , A2 fB g, fC g, B2 n fC g, respectively. easily
verify Pr (Ab Ac ) = Pr 1 (Ab Ac ) Pr (Ac A2 ) = Pr 2 (Ac A2 ) atomic events
Ab , Ac , A2 fB g, fC g, B2 n fC g, respectively. Hence, Pr model KB .
Moreover, Pr 1 (BC ) > 0 Pr 2 (CD1 Dk ) > 0 entails Pr (BCD1 Dk ) > 0. 2

4. Probabilistic Deduction

section, present techniques computing tight answers queries directed
exact general conditional constraint trees, analyze computational complexity. precisely, problem computing tight answer query reduced
problem computing tight answer complete query. latter problem
reduced problems computing tight answer premise-restricted complete
query tight answer strongly conclusion-restricted complete query.

4.1 Premise-Restricted Complete Queries
4.1.1 Exact Conditional Constraint Trees

focus problem computing tight answers premise-restricted complete
queries directed exact conditional constraint trees.
Let (B; KB ) exact conditional constraint tree let 9(F jE )[x1 ; x2 ] premiserestricted complete query. compute tight answer 9(F jE )[x1 ; x2 ], start
defining directed tree (that is, directed acyclic graph node exactly
one parent, except root, any):

! B iff $ B closer E B .
directed tree (B; !) uniquely determined conditional constraint tree
premise-restricted complete query. Fig. 3 shows (B; !) premise-restricted complete
query 9(QRSTUjM)[x1 ; x2 ] exact conditional constraint tree Fig. 2, left side.

Now, set nodes B partitioned several strata. lowest stratum contains
nodes children (B; !), highest stratum contains nodes
parents (B; !) (that is, exactly node premise E query). Fig. 3 also
shows different strata example.
node (B; !), compute certain tightest bounds logically entailed
KB . precisely, tightest bounds node B computed locally, exploiting
tightest bounds previously computed children B . Hence,
iteratively compute tightest bounds nodes stratum, starting
nodes lowest stratum terminating nodes highest stratum.
distinguish three different ways computing tightest bounds node:
initialization leaf (Leaf),
chaining arrow subtree via common node (Chaining),
fusion subtrees via common node (Fusion).
Let us consider premise-restricted complete query 9(QRSTUjM)[x1 ; x2 ]
exact conditional constraint tree Fig. 2, left side. Fig. 4 illustrates three different ways
208

fiProbabilistic Deduction Conditional Constraints Basic Events


P




N

U

Q

R
3

4

1

2

0

strata

Figure 3: Directed Tree (B; !)
computing tightest bounds node (the common nodes Chaining Fusion
filled black). Table 1 shows greatest lower least upper bounds computed
node B stratum. precisely, bounds ff1 = inf Pr (BD )=Pr (B ),
ff2 = sup Pr (BD )=Pr (B ), fi2 = sup Pr (BD )=Pr (B ), 2 = sup Pr (D )=Pr (B ) subject
Pr j= KB Pr (B ) > 0. Table 1 also shows requested tight answer fx1 =0:02; x2 =0:17g,
given tightest bounds ff1 ff2 computed premise M.
strata B

0
U
P
1 P
P
P
1 Q
R

2

2
3 N
4





U


U
STU
Q
R
STU
Q
R
QRSTU
QRSTU
QRSTU

ff1

1:0000
1:0000
1:0000
0:8500
0:8500
0:8500
0:5500
1:0000
1:0000
0:4474
0:9500
0:9500
0:3474
0:1911
0:0169

ff2

1:0000
1:0000
1:0000
0:8500
0:8500
0:8500
0:8500
1:0000
1:0000
0:7605
0:9500
0:9500
0:7605
0:4183
0:1722

fi2

0:0000
0:0000
0:0000
0:0447
0:0447
0:0000
0:0000
0:0000
0:0000
0:0447
0:0500
5:3833
0:0447
0:0246
0:0719

2

1:0000
1:0000
1:0000
0:8947
0:8947
0:8500
0:8500
1:0000
1:0000
0:7605
1:0000
6:3333
0:7605
0:4183
0:1722

(Leaf)
(Leaf)
(Leaf)
(Chaining)
(Chaining)
(Chaining)
(Fusion)
(Leaf)
(Leaf)
(Chaining)
(Chaining)
(Chaining)
(Fusion)
(Chaining)
(Chaining)

Table 1: Locally Computed Tightest Bounds
209

fiLukasiewicz



Initialization leaf:
P





Q

N

U

R



Chaining arrow subtree:
P





Q

N

U

R



Fusion subtrees:
P





N

Q

R

Figure 4: Local Computations (B; !)
210

U

fiProbabilistic Deduction Conditional Constraints Basic Events

focus technical details. present functions H1ff , H2ff , H2fi , H2 ,
compute described greatest lower least upper bounds. purpose,
need following definitions. Let Pr (C jB ) denote u (C jB )[u; u] 2 KB .
node B leaf children. leaves B , let B " = B .
nodes B , let B " conjunction children B . leaves C , let
L(C ) = C . conjunctive events C , let L(C ) conjunction
leaves C descendants node C .
sequel, let B node let C = B " . case C = B refers initialization
leaf B , case C = B1 node B1 6= B chaining arrow B ! B1
subtree via common node B1 , case C = B1 B2 : : : Bk k > 1 nodes
B1; B2 ; : : : ; Bk fusion k subtrees via common node B .
define function H1ff computing greatest lower bounds: let H1ff (B; C ) = ff1
(note ff1 coincide greatest lower bound Pr (BL(C )) = Pr (B ) subject
Pr j= KB Pr (B ) > 0), ff1 Leaf (C = B ), Chaining (C = B1 ), Fusion
(C = B1 B2 : : : Bk k > 1) given follows:
Leaf

:

ff1 = 1
:
ff
"
ff1 = max(0; Pr (C jB ) (1 + H1Pr(C;(BCjC)),1 ))

Chaining

Fusion

:

ff1 = max(0; 1 , k + P H1ff (B; Bi))
k

i=1

express H1ff computes greatest lower bounds, need following definitions.
Let B(B; C ) comprise B , nodes C descendants node C . Let KB (B; C )
set conditional constraints KB B(B; C ). Let Mo (B; C ) set
models KB (B; C ) defined atomic events B(B; C ).
Now, function H1ff sound globally complete respect B C iff
ff
H1 (B; C ) = ff1 greatest lower bound Pr (BL(C )) = Pr (B ) subject Pr 2 Mo (B; C )
Pr (B ) > 0. Thus, next theorem shows soundness global completeness H1ff .

Theorem 4.1

a) probabilistic interpretations Pr 2 Mo (B; C ), holds ff1 Pr (B ) Pr (BL(C )).
b) exists probabilistic interpretation Pr 2 Mo (B; C ) Pr (B ) > 0, ff1 Pr (B ) =
Pr (BL(C )), Pr (BL(C )) = 0 iff L(C ) ) B .

Proof. proof given full detail Appendix B. 2

Next, present functions H2ff , H2fi , H2 computing least upper bounds. Note
H2ff , H2fi , H2 show crucial result exact conditional constraint trees,
local probabilistic deduction techniques sound globally complete.
211

fiLukasiewicz

detail, let H2ff (B; C ) = ff2 , H2fi (B; C ) = fi2 , H2 (B; C ) = 2 (note ff2 , fi2 ,
2 coincide least upper bound Pr (BL(C )) = Pr (B ), Pr (BL(C )) = Pr (B ),
Pr (L(C )) = Pr (B ), respectively, subject Pr j= KB Pr (B ) > 0), ff2 , fi2 ,
2 Leaf (C = B ), Chaining (C = B1 ), Fusion (C = B1 B2 : : : Bk k > 1)
given follows:
Leaf

:

ff2 = 1
fi2 = 0
2 = 1
:

"
ff
"
ff2 = min(1; Pr (C jB ) HPr2 ((C;BjCC )) ; 1 , Pr (C jB ) (1 , HPr2 ((C;BjCC ) ) );

Chaining

fi

Pr (C jB ) (1 + HPr2 ((C;BjCC ) ) ))
fi

"



H2 (C; C )
fi2 = min(Pr (C jB ) ( H2Pr(C;(BCjC)+1
) , 1); Pr (C jB ) Pr (B jC ) )


"

"

2 = Pr (C jB ) HPr2 ((C;BjCC ))
Fusion

"

:

ff2 = i2min
H ff (B; Bi )
[1:k] 2
fi2 = i2min
H fi (B; Bi )
[1:k] 2

2 = min(i2min
H2 (B; Bi ); min (H2ff (B; Bi ) + H2fi (B; Bj )))
[1:k]
i;j 2[1:k];i6=j
functions H2ff , H2fi , H2 sound globally complete respect B
C iff H2ff (B; C ) = ff2 , H2fi (B; C ) = fi2 , H2 (B; C ) = 2 least upper bounds
Pr (BL(C )) = Pr (B ), Pr (BL(C )) = Pr (B ), Pr (L(C )) = Pr (B ), respectively, subject
Pr 2 Mo (B; C ) Pr (B ) > 0. Hence, following theorem shows soundness
global completeness H2ff , H2fi , H2 (actually, shows even enable proof
induction recursive definition H2ff , H2fi , H2 ).

Theorem 4.2

a) probabilistic interpretations Pr 2 Mo (B; C ), holds Pr (BL(C )) ff2 Pr (B ),
Pr (BL(C )) fi2 Pr (B ), Pr (L(C )) 2 Pr (B ).
b) exists probabilistic interpretation Pr 2 Mo (B; C ) Pr (B ) > 0, Pr (BL(C )) =
ff2 Pr (B ), Pr (L(C )) = 2 Pr (B ).
c) exists probabilistic interpretation Pr 2 Mo (B; C ) Pr (B ) > 0, Pr (BL(C )) =
fi2 Pr (B ), Pr (L(C )) = 2 Pr (B ).
212

fiProbabilistic Deduction Conditional Constraints Basic Events

Proof. proof given full detail Appendix B. 2
Note Theorem 4.2 also shows H2 (B; Bi ) H2ff (B; Bi ) + H2fi (B; Bi )
2 [1: k]. Thus, expression mini;j 2[1:k];i6=j (H2ff (B; Bi ) + H2fi (B; Bj )) definition

2 Fusion replaced ff2 + fi2 increased eciency computing 2
exploiting already computed values ff2 fi2 .
Brie y, Theorems 4.1 4.2, tight answer premise-restricted complete
query 9(F jE )[x1 ; x2 ] given fx1 =H1ff (E; E " ); x2 =H2ff (E; E " )g.
4.1.2 Conditional Constraint Trees

focus computing tight answer premise-restricted complete queries
general conditional constraint trees. sequel, let (B; KB ) conditional constraint
tree let 9(F jE )[x1 ; x2 ] premise-restricted complete query.
may think local deduction technique exact conditional constraint trees
Section 4.1.1 easily generalized conditional constraint trees. fact, true
far computation greatest lower bounds concerned. However, computation
least upper bounds cannot generalized easily exact conditional constraint
trees conditional constraint trees. precisely, generalizing computation least
upper bounds results solving nonlinear programs. nonlinear programs way
solve illustrated following chaining example.
Let conditional constraint tree (B; KB ) given B = fM; N; O; Pg KB =
f(NjM)[u1 ; u2 ]; (MjN)[v1 ; v2 ], (OjN)[x1 ; x2 ], (NjO)[y1 ; y2 ], (PjO)[r1 ; r2], (OjP)[s1 ; s2]g. Let us
consider premise-restricted complete query 9(PjM)[z1 ; z2 ].
Theorem 4.2 straightforward arithmetic transformations, requested
least upper bound maximum z subject u 2 [u1 ; u2 ], v 2 [v1 ; v2 ], x 2 [x1 ; x2 ],
2 [y1; y2 ], r 2 [r1 ; r2 ], 2 [s1 ; s2 ], nonlinear inequalities (1) (5):
(1)
z 1
(2)
z 1 , u + uv , uxv + uxr
vy
uxr
ux
(3)
z 1 , u + v , vy + uxr
vys
uxr + uxr
z u , uxv + ux
(4)
,
vy
vy
vys
uxr
(5)
z vys
system nonlinear inequalities, upper bounds z monotonically decreasing
v, y, s. Hence, equivalently maximize z subject u 2 [u1 ; u2 ], x 2 [x1 ; x2 ],
r 2 [r1 ; r2 ], nonlinear inequalities (6) (10):
(6)
z 1
uxr
z 1 , u + vu1 , ux
(7)
v1 + v1 y1
uxr
uxr
(8)
z 1 , u + ux
v1 , v1 y1 + v1 y1 s1
ux
uxr
uxr
(9)
z u , ux
v1 + v1 y1 , v1 y1 + v1 y1 s1
z v1uxr
(10)
y1 s1
example, requested least upper bound u1 = u2 = u x1 = x2 = x shown
Fig. 5 u; x 2 [0; 1], r1 = r2 = 0:15, v1 = 0:8, y1 = 0:8, s1 2 f0:05; 0:1g. requested
least upper bound u1 < u2 x1 < x2 maximum value [u1 ; u2 ] [x1 ; x2 ].
213

fiLukasiewicz

s1=0.05

z2
0.99
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

1
0.9
0.8
0.7
0.6
0.1

0.2

0.5
0.3

0.4
0.4

0.5

x

0.3
0.6

u

0.7

0.2
0.8

0.9

0.1
1

s1=0.1

z2
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0.9
0.8
0.7
0.6
0.1

0.2

0.5
0.3

0.4
0.4
u

0.5

0.3
0.6

0.7

x

0.2
0.8

0.9

0.1
1

Figure 5: Least Upper Bound z2 Chaining Example
214

fiProbabilistic Deduction Conditional Constraints Basic Events

transform nonlinear program equivalent linear program (by replacing 1, u, ux, uxr new variables xM, xN , xO , xP , respectively).
precisely, maximum z subject u 2 [u1 ; u2 ], x 2 [x1 ; x2 ], r 2 [r1 ; r2 ], nonlinear inequalities (6) (10) coincides maximum z subject following
system linear inequalities z xB 0 (B 2 B):

z
z
z
z
z







xM
xM + 1,v1v1 xN , vy1 1y1 xO + v1 ys11 s1 xP
xM , vv11 xN + vy1 1y1 xO + v11,y1ss11 xP
v1 x + 1,y1 x + 1,s1 x
v1 N
v1 y1
v1 y1 s1 P
1
v1 y1 s1 xP

1
u1 xM
x1 xN
r1 xO






xM
xN
xO
xP






1

u2 xM
x2 xN
r2 xO

generally, tight upper answers premise-restricted complete queries conditional
constraint trees computed solving similar nonlinear programs, similarly
transformed linear programs.
example, let us consider premise-restricted complete query 9(QRSTUjM)[x1 ; x2 ]
conditional constraint tree Fig. 2, right side. requested least upper bound
maximum x subject system linear inequalities Fig. 6 (we actually
generated 72 linear inequalities 31 trivially subsumed others). Note
nine variables xM xU correspond nine nodes U.

x
x
x
x
x
x
x
x
x
x
x
x














xM
25 x
18 Q
25 xR
2
25 x
18
25 x
18
25 x
18 U
xN + 19 xP
xN + 19 xQ
xN + 19 xR
5x + 5 x
4 36 P
5x + 5 x
4 36 Q
5 x + 45 x
4 4 R

x
x
x
x
x
x
x
x
x
x
x













5
5
4 xP + 36 xQ
5
5
36 xP + 4 xQ
5
5
36 xP + 4 xR
5
5
36 xQ + 4 xR
xM , xN + 45 xO + 365 xR
xM + 14 xN , 45 xO + 54 xP
xM + 14 xN , 45 xO + 54 xQ
xM + 14 xN , 45 xO + 54 xR
xM + 14 xN , 45 xP + 25
18 xS
1
5
xM + 4 xN , 4 xP + 25
18 xT
xM + 14 xN , 45 xP + 25
18 xU

1
3
10 xM
1x
2 N
9
10 xO
9
10 xO
4
5 xO
4
5 xP
4
5 xP
4
5 xP











xM
xN
xO
xR
xQ
xP
xS
xT
xU











1

2x
5
3x
5 N

xO
xO
9
10 xO
9
10 xP
9
10 xP
9
10 xP

Figure 6: Generated Linear Inequalities Chaining Example
Thus, example, tight upper answer computed solving linear program
10 variables 72 linear inequalities. Note computing tight upper answer
classical linear programming approach would result solving linear program
29 = 512 variables 4 9 , 2 = 34 linear inequalities (see Section 4.6).
215

fiLukasiewicz

Let us focus technical details. subsequently generalize function H1ff
Section 4.1.1 straightforward way compute greatest lower bounds conditional
constraint trees. Moreover, present linear program computing requested least
upper bound conditional constraint trees.
Let Pr 1 (C jB ) denote u1 (C jB )[u1 ; u2 ] 2 KB . sequel, let B node
let C = B " . Again, cases C = B , C = B1 node B1 6= B , C = B1 B2 : : : Bk
k > 1 nodes B1 ; B2 ; : : : ; Bk refer Leaf, Chaining, Fusion, respectively.
define generalized function H1ff computing greatest lower bounds conditional constraint trees: let H1ff (B; C ) = ff1 (note ff1 coincide greatest
lower bound Pr (BL(C )) = Pr (B ) subject Pr j= KB Pr (B ) > 0), ff1 Leaf
(C = B ), Chaining (C = B1 ), Fusion (C = B1 B2 : : : Bk k > 1) given by:
Leaf

:

ff1 = 1
:
ff
"
ff1 = max(0; Pr 1 (C jB ) (1 + H1Pr(C;1 (BCjC),) 1 ))

Chaining

Fusion

:

ff1 = max(0; 1 , k + P H1ff (B; Bi))
k

i=1

H1ff sound globally complete respect B C iff H1ff (B; C ) = ff1
greatest lower bound Pr (BL(C )) = Pr (B ) subject Pr 2 Mo (B; C ) Pr (B ) > 0.
Thus, next theorem shows soundness global completeness H1ff .

Theorem 4.3

a) probabilistic interpretations Pr 2 Mo (B; C ), holds ff1 Pr (B ) Pr (BL(C )).
b) exists probabilistic interpretation Pr 2 Mo (B; C ) Pr (B ) > 0, ff1 Pr (B ) =
Pr (BL(C )), Pr (BL(C )) = 0 iff L(C ) ) B .

Proof. claims follow Theorem 4.1. 2

Next, focus requested least upper bound, computed solving
linear program described two examples.
start defining functions ff , fi , variables xB (B 2 B). Let
ff (B; C ) = ff2 , fi (B; C ) = fi2 , (B; C ) = 2 , ff2 , fi2 , 2 Leaf (C = B ),
Chaining (C = B1 ), Fusion (C = B1 B2 : : : Bk k > 1) given follows:
Leaf

:

ff2 = xB
fi2 = 0
2 = xB
216

fiProbabilistic Deduction Conditional Constraints Basic Events

:
(C; C " ) ; x + fi (C; C " ) ; x , x + ff (C; C " ) )
ff2 = min(xB ; Pr
C Pr 1 (BjC ) B
C Pr 1 (BjC )
1 (B jC )

Chaining

(C; C ) ;
fi2 = min( 1,PrPr1 (1B(BjCjC) ) xC + Pr
1 (B jC )
fi

2 =
Fusion

(C; C " )
Pr 1 (BjC )

"

(C; C " )
Pr 1 (BjC ) )

:

ff2 = i2min
ff(B; Bi )
[1:k]
fi2 = i2min
fi (B; Bi )
[1:k]

2 = min(i2min
(B; Bi ); i;j 2min
(I ff (B; Bi ) + fi (B; Bj )))
[1:k]
[1:k];i6=j
system linear inequalities J (B; C ) defined least set linear inequalities
xG 0 (G 2 B(B; C )) contains 1 xB 1 u1 xG xH u2 xG
(H jG)[u1 ; u2 ] 2 KB (B; C ) G ! H (that is, G parent H ).

intuition behind definitions described follows.
xG (G 2 B(B; C )) satisfies J (B; C ) corresponds exact conditional constraint tree (B(B; C ); KB 0 (B; C )), KB 0 (B; C ) contains pair (H jG)[xH =xG ; xH =xG ]
(GjH )[v1 ; v1 ] pair (H jG)[u1 ; u2 ]; (GjH )[v1 ; v2 ] 2 KB (B; C ) G ! H .
show least upper bound Pr (BL(C ))=Pr (B ), Pr (BL(C ))=Pr (B ),
Pr (L(C ))=Pr (B ) subject Pr j= KB 0 (B; C ) Pr (B ) > 0 given ff (B; C ),
fi (B; C ), (B; C ), respectively. follow least upper bound
Pr (BL(C ))=Pr (B ), Pr (BL(C ))=Pr (B ), Pr (L(C ))=Pr (B ) subject Pr j= KB (B; C )
Pr (B ) > 0 given maximum ff (B; C ), fi (B; C ), (B; C ), respectively,
subject xG (G 2 B(B; C )) satisfying J (B; C ).
is, implicitly performed variable transformation described two examples. transformation indeed correct conditional constraint trees:

Lemma 4.4
a) xG (G 2 B(B; C )) satisfies J (B; C ), conditional constraints (H jG)[u1 ; u2 ] 2
KB (B; C ) G ! H , exists uH 2 [u1 ; u2 ] xH = uH xG .
b) Let uH 2 [u1 ; u2 ] (H jG)[u1 ; u2 ] 2 KB (B; C ) G ! H . exists xG
(G 2 B(B; C )) J (B; C ) xH = uH xG nodes H parent G.
Proof. a) nodes H parent G, let uH defined uH = xH = xG .
b) Let xB = 1, nodes H parent G, let xH defined xH = uH xG . 2

ready formulate optimization problem computing requested
least upper bound.
Theorem 4.5 Let X2 maximum x subject x ff(E; E " ) J (E; E " ).
a) Pr (EL(E " )) X2 Pr (E ) Pr 2 Mo (E; E " ).
b) exists Pr 2 Mo (E; E " ) Pr (E ) > 0 Pr (EL(E " )) = X2 Pr (E ).
217

fiLukasiewicz

Proof. Let Pr (B jC ) = v1 (B jC )[v1 ; v2 ] 2 KB B ! C . Theorem 4.2,
requested least upper bound maximum x subject x H2ff(E; E " )
Pr (C jB ) = uC 2 [u1 ; u2 ] (C jB )[u1 ; u2 ] 2 KB B ! C . Lemma 4.4,
equivalently maximize x subject x ff (E; E " ) J (E; E " ). 2
wonder solve generated optimization problem, since ff (E; E " ) may
still contain min-operations cannot tackled linear programming. Moreover, given
method solving optimization problem, also interested rough idea
overall time complexity computing requested least upper bound way. Finally,
interested possible improvements increase eciency. topics discussed
rest section.
ff (E; E " ) contain min-operations all, generated optimization
problem already linear program. Otherwise, easily transformed linear
program. first transformation step, inner min-operations eliminated.
easily done due well-structuredness ff (E; E " ). second step,
remaining outer min-operation eliminated introducing exactly one linear inequality
contained operand. linear inequalities, operands outer minoperation upper bounds x.
get rough idea time complexity computing requested least upper
bound way, must analyze size generated linear programs. given
number variables, number linear inequalities J (E; E " ), number linear inequalities extracted x ff (E; E " ). latter quite worrying,
since (B; C ) Fusion seems produce many min-operands. Moreover, (B; C )
ff
ff

"
Fusion contains (B; Bi ), (B; C ) Chaining contains (C; C ). So, due
crossed dependency, overall number generated linear inequalities likely
`explode' trees branch often.
avoid problems, introduce auxiliary functions J ff , J fi , J
variables xB (B 2 B). Let J ff (B; C ) = ff02 , J fi (B; C ) = fi20 , J (B; C ) = 20 , ff02 , fi20 ,
20 Leaf (C = B ), Chaining (C = B1 ), Fusion (C = B1 B2 : : : Bk k > 1)
given follows:
Leaf

:

ff02 = xB
fi20 = 0
20 = xB
Chaining

:
fi

ff

ff02 = min(xB ; xC + JPr(1C;(BCjC )) ; xB , xC + JPr (1C;(BCjC )) )
"

fi
"
fi20 = 1,PrPr1 (1B(BjCjC) ) xC + JPr(1C;(BCjC ))

20 =

J (C; C " )
Pr 1 (BjC )

218

"

fiProbabilistic Deduction Conditional Constraints Basic Events

Fusion

:

ff02 = i2min
J ff(B; Bi )
[1:k]
fi20 = i2min
J fi (B; Bi )
[1:k]

20 = min(i2min
J (B; Bi ); i;j 2min
(J ff (B; Bi ) + J fi (B; Bj )))
[1:k]
[1:k];i6=j
Note ff02 Chaining separated cases C " = C C " 6= C . Since
simply ff02 = xC C " = C , reduce number generated linear inequalities way.
next lemma shows functions ff , fi , expressed terms
auxiliary functions J ff , J fi , J .

Lemma 4.6 xB (B 2 B) satisfy J (E; E " ):
ff2 = min(ff02 ; 20 ), fi2 = min(fi20 ; 20 ), 2 = 20 :

Proof sketch. claim proved induction recursive definition

functions ff , fi , . 2
Brie y, Theorem 4.3, Theorem 4.5, Lemma 4.6, tight answer premiserestricted complete query 9(F jE )[x1 ; x2 ] given fx1 =H1ff (E; E " ); x2 =X2 g, X2
maximum x subject x J ff (E; E " ), x J (E; E " ), J (E; E " ).
example, get fx1 =0:00; x2 =0:27g tight answer premise-restricted
complete query 9(QRSTUjM)[x1 ; x2 ] conditional constraint tree Fig. 2, right side.
time complexity computing requested greatest lower bound especially
requested least upper bound way analyzed Section 4.5.

4.2 Strongly Conclusion-Restricted Complete Queries

focus computing tight answer strongly conclusion-restricted complete
queries general conditional constraint trees. sequel, let (B; KB ) conditional
constraint tree let 9(F jE )[x1 ; x2 ] strongly conclusion-restricted complete query.
tight upper answer 9(F jE )[x1 ; x2 ] always given fx2 =1g. compute
tight lower answer 9(F jE )[x1 ; x2 ], first compute tight lower answer fy1 =u1 g
premise-restricted complete query 9(E jF )[y1 ; y2 ]. distinguish following cases:
u1 > 0, tight lower answer 9(F jE )[x1 ; x2 ] computed locally function
H1 (like tight lower answer premise-restricted complete queries Section 4.1.2).
u1 = 0 E ) F , tight lower answer 9(F jE )[x1 ; x2 ] given fx1 =1g.
Otherwise, tight lower answer 9(F jE )[x1 ; x2 ] given fx1 =0g.
focus technical details. Let (B; !) directed graph belongs
premise-restricted complete query 9(E jF )[y1 ; y2 ] (see Section 4.1.1). Let Pr 1 (B jC )
denote v1 (B jC )[v1 ; v2 ] 2 KB . sequel, let B node let C = B " . Again,
cases C = B , C = B1 node B1 6= B , C = B1 B2 : : : Bk k > 1 nodes
B1; B2 ; : : : ; Bk refer Leaf, Chaining, Fusion, respectively.
define function H1 computing greatest lower bounds case H1ff (B; C ) > 0
follows. Let H1 (B; C ) = 1 (note 1 coincide greatest lower bound
Pr (BL(C )) = Pr (L(C )) subject Pr j= KB Pr (L(C )) > 0), 1 Leaf
219

fiLukasiewicz

(C = B ), Chaining (C = B1 ), Fusion (C = B1 B2 : : : Bk k > 1) given
follows (note H1ff (C; C " ) H1ff (B; Bi ) defined like Section 4.1.2):
Leaf:
1 = 1
:
jC ),1 )
1 = H1 (C; C " ) (1 + PrH11ff((BC;C
")

Chaining

:

1,1
0
ff

H (B;Bi )(1=H1 (B;Bi ),1) C
B1 + i2min
[1:k] 1
CA
1 = B
@
Pk

Fusion

1,k+

i=1

H1ff (B;Bi )

induction definition H1 , easy see H1ff (B; C ) > 0 entails 1
defined 1 > 0 (note H1ff (B; C ) = ff1 Leaf, Chaining, Fusion
defined like Section 4.1.2). case, H1 sound globally complete respect
B C iff H1 (B; C ) = 1 greatest lower bound Pr (BL(C )) = Pr (L(C )) subject
Pr 2 Mo (B; C ) Pr (L(C )) > 0. Thus, next theorem shows soundness global
completeness H1 . also shows that, C = B1 B2 : : : Bk k > 1, least upper
bound Pr (BL(C )) = Pr (L(C )) subject Pr 2 Mo (B; C ) Pr (L(C )) > 0 given 1.

Theorem 4.7

a) ff1 > 0, Pr 2 Mo (B; C ), holds 1 Pr (L(C )) Pr (BL(C )).
b) ff1 > 0, probabilistic interpretation Pr 2 Mo (B; C ) Pr (B ) > 0,
Pr (L(C )) > 0, 1 Pr (L(C )) = Pr (BL(C )), ff1 Pr (B ) = Pr (BL(C )).
c) ff1 > 0 C = B1 B2 : : : Bk k > 1, Pr 2 Mo (B; C )
Pr (B ) > 0, Pr (L(C )) > 0, 1 Pr (L(C )) = Pr (BL(C )), ff1 Pr (B ) = Pr (BL(C )).
d) ff1 = 0 C = B1 B2 : : : Bk k > 1, " > 0 Pr 2 Mo (B; C )
Pr (B ) > 0, Pr (L(C )) > 0, 1 Pr (L(C )) = Pr (BL(C )), " Pr (B ) Pr (BL(C )).

Proof. proof given full detail Appendix C. 2

ready give following characterization tight answers strongly
conclusion-restricted complete queries conditional constraint trees.
Theorem 4.8 Let (B; KB ) conditional constraint tree let 9(F jE )[x1 ; x2 ]
strongly conclusion-restricted complete query. Let tight lower answer premiserestricted complete query 9(E jF )[y1 ; y2 ] given fy1 =u1 g.
(1) u1 > 0, tight answer 9(F jE )[x1 ; x2 ] given fx1 =H1 (F; F " ); x2 =1g.
(2) u1 = 0 E ) F , tight answer 9(F jE )[x1 ; x2 ] given fx1 =1; x2 =1g.
(3) Otherwise, tight answer 9(F jE )[x1 ; x2 ] given fx1 =0; x2 =1g.
Proof. proof given full detail Appendix C. 2
220

fiProbabilistic Deduction Conditional Constraints Basic Events

4.3 Complete Queries

show problem computing tight answers complete queries
reduced problems computing tight answers premise-restricted complete queries
computing tight answers strongly conclusion-restricted complete queries.
detail, complete query premise-restricted, strongly conclusion-restricted,
reduced premise-restricted complete queries strongly conclusionrestricted complete queries. example, given complete query 9(STUjMQR)[x1 ; x2 ]
conditional constraint tree Fig. 2, right side, first compute tight answer
fy1=u1 ; y2=u2 g premise-restricted complete query 9(MQRjO)[y1 ; y2] (directed
corresponding subtree) tight answer fz1 =v1 ; z2 =v2 g strongly conclusion-restricted complete query 9(OjMQR)[z1 ; z2 ] (directed corresponding subtree).
generate new conditional constraint tree replacing subtree nodes M, N,
O, Q, R pair conditional constraints (BjO)[u1 ; u2 ] (OjB)[v1 ; v2 ]
nodes B (note B represents MQR). Finally, compute tight answer
premise-restricted complete query 9(STUjB)[x1 ; x2 ] new conditional constraint tree.
Note reduction always done, since query 9(F jE )[x1 ; x2 ],
paths basic event E basic event F least one basic event common.

Theorem 4.9 Let (B; KB ) conditional constraint tree let 9(F jE )[x1 ; x2 ] com-

plete query premise-restricted strongly conclusion-restricted.
a) exists basic event G 2 B two conditional constraint trees (B1 ; KB 1 )
(B2 ; KB 2 ) B1 \ B2 = fGg, B1 [ B2 = B, 9(GjE )[z1 ; z2 ] strongly conclusion-restricted complete query (B1 ; KB 1 ).
b) Let tight answer premise-restricted complete query 9(E jG)[y1 ; y2 ] (B1 ; KB 1 )
given fy1 =u1 ; y2 =u2 g let tight answer strongly conclusion-restricted
complete query 9(GjE )[z1 ; z2 ] (B1 ; KB 1 ) given fz1 =v1 ; z2 =v2 g.

(1) u1 > 0, also v1 > 0 tight answer complete query 9(F jE )[x1 ; x2 ]
(B; KB ) coincides tight answer premise-restricted complete query
9(F jB )[x1 ; x2 ] (B2 [ fB g; KB 2 [ f(B jG)[u1 ; u2 ]; (GjB )[v1 ; v2 ]g), B new
basic event B 62 B2 . particular, exact conditional constraint trees (B; KB ),
tight answer complete query 9(F jE )[x1 ; x2 ] given by:

fx1 = max(0; v1 , uv11 + vu1 s11 ); x2 = min(1; 1 , v1 + vu1 s12 ; t2 ,st22+u1 )g ;
s1 = H1ff (G; G" ), s2 = H2ff (G; G" ), t2 = H2 (G; G" ) (note H1ff , H2ff ,
H2 defined like Section 4.1.1).

(2) u1 =0, v1 =1, G ) F , tight answer complete query 9(F jE )[x1 ; x2 ]
(B; KB ) given fx1 =1; x2 =1g.
(3) Otherwise, tight answer complete query 9(F jE )[x1 ; x2 ] (B; KB ) given
fx1 =0; x2 =1g.

Proof. proof given full detail Appendix D. 2
221

fiLukasiewicz

4.4 Queries

problem computing tight answers queries reduced specialized
problem calculating tight answers complete queries.
precisely, given query 9(F jE )[x1 ; x2 ] conditional constraint tree (B; KB ),
complete query 9(F 0 jE 0 )[x1 ; x2 ] conditional constraint tree (B0 ; KB 0 ) generated by:
1. (B; KB ) contains leaf B contained EF : remove B B
remove corresponding pair (C jB )[u1 ; u2 ]; (B jC )[v1 ; v2 ] 2 KB KB .
2. EF contains basic event B leaf (B; KB ): increase B new
basic event B 0, increase KB pair (B 0 jB )[1; 1] (B jB 0 )[1; 1], replace
occurrence B 9(F jE )[x1 ; x2 ] new basic event B 0 .
remains show generated probabilistic deduction problem
solution original probabilistic deduction problem:
Theorem 4.10 tight answer query 9(F jE )[x1 ; x2 ] (B; KB ) coincides
tight answer complete query 9(F 0 jE 0 )[x1 ; x2 ] (B0 ; KB 0 ).
Proof. Let (B00; KB 00 ) conditional constraint tree generated step 1 let
(F jE )[u1 ; u2 ] tight logical consequence KB 00 . show (F jE )[u1 ; u2 ] also
tight logical consequence KB . First, (F jE )[u1 ; u2 ] logical consequence KB , since
KB 00 subset KB . Moreover, model Pr 00 KB 00 (that defined atomic
events B00 ) extended model Pr KB (that defined atomic events
B) Pr (A) = Pr 00 (A) atomic events B00 different
conjunction negated basic events B00 , real number (0; 1].
model constructed inductively like proof Theorem 3.2. Thus, u 2 [u1 ; u2 ],
Pr 00 (E ) > 0 u Pr 00 (E ) = Pr 00 (EF ) entails Pr (E ) > 0 u Pr (E ) = Pr (EF ).
Finally, (F jE )[u1 ; u2 ] tight logical consequence KB 00 iff (F 0 jE 0 )[u1 ; u2 ] tight
logical consequence KB 0 , since introduce synonyms basic events step 2. 2

4.5 Computational Complexity
4.5.1 Exact Conditional Constraint Trees

show exact conditional constraint trees, technique compute tight
answer queries runs linear time number nodes tree. sequel, let
(B; KB ) exact conditional constraint tree let n denote number nodes.

Lemma 4.11 tight answer premise-restricted strongly conclusion-restricted
complete query computed linear time n.

Proof. exact conditional constraint trees, approach
compute tight upper
fi


answer premise-restricted complete queries H2ff , H2 , H2 runs time O(n):
directed tree computed time O(n). initialization leaf
constant number assignments performed exactly leaf directed tree,
chaining constant number arithmetic operations performed exactly
arrow directed tree. Hence, initializing leaves performing chainings runs
222

fiProbabilistic Deduction Conditional Constraints Basic Events

time O(n). fusion done branching directed tree, using linear time
number branches. Thus, fusions together run time O(n).
Even general conditional constraint trees, tight lower answer premise-restricted
complete queries, hence also tight answer strongly conclusion-restricted complete
queries, analogously computed time O(n). 2

Theorem 4.12 tight answer query computed linear time n.
Proof. assume set basic events B totally ordered basic events
conjunctive events E F query 9(F jE )[x1 ; x2 ] written order.

First, query reduced complete query according Section 4.4. reduction
done time O(n). Now, generated complete query premise-restricted
strongly conclusion-restricted, claim follows immediately Lemma 4.11.
Otherwise, generated complete query reduced premise-restricted strongly
conclusion-restricted complete queries according Section 4.3. Also reduction
done time O(n), since basic event G Theorem 4.9 a) computable time O(n).
Hence, claim follows Theorem 4.9 Lemma 4.11. Note t2 = H2 (G; G" )
Theorem 4.9 b) (1) also computed time O(n). 2
4.5.2 Conditional Constraint Trees

general conditional constraint trees, technique compute tight lower answer
queries runs still linear time, technique compute tight upper answer
queries runs polynomial time number nodes tree. sequel, let
(B; KB ) general conditional constraint tree let n denote number nodes.

Lemma 4.13

a) tight lower answer premise-restricted complete query tight answer
strongly conclusion-restricted complete query computed linear time n.
b) tight upper answer premise-restricted complete query computed polynomial time n.

Proof. a) claim already shown proof Lemma 4.11.

b) linear programming technique compute tight upper answer premiserestricted complete queries runs polynomial time n:
Linear programming runs polynomial time size linear programs (Papadimitriou & Steiglitz, 1982; Schrijver, 1986), size linear program given
number variables number linear inequalities.
show size linear programs Section 4.1.2 polynomial n.
number variables n + 1. number linear inequalities J (E; E " ) 2n.
induction recursive definition J ff , J fi , J , shown
number min-operands J ff (B; C ), J fi (B; C ), J (B; C ) limited jB(B; C )j2 ,
jB(B; C )j, jB(B; C )j4 , respectively. Hence, number linear inequalities extracted
x J ff (E; E " ) x J (E; E " ) limited jB(E; E " )j2 + jB(E; E " )j4 = n2 + n4 .
Thus, overall number generated linear inequalities l limited lu = 2n + n2 + n4 .
223

fiLukasiewicz

Finally, note lu rough upper bound l, many conditional constraint
trees (especially branch rarely), l much lower lu . example,
taking complete binary tree n = 127 nodes, get l = 19 964 compared
lu = 260 161 024. example Section 4.1.2 n = 9 nodes, get l = 72
compared lu = 6 660. Another example tree degenerated chain basic
events. case, even get l = 5n + 1, is, overall number generated linear
inequalities linear n. 2

Theorem 4.14

a) tight lower answer query computed linear time n.
b) tight upper answer query computed polynomial time n.

Proof. assume set basic events B totally ordered basic events
conjunctive events E F query 9(F jE )[x1 ; x2 ] written order.
Like proof Theorem 4.12, query reduced complete query according
Section 4.4. reduction done time O(n). Now, generated complete query premise-restricted strongly conclusion-restricted, claims follow
immediately Lemma 4.13.
Otherwise, generated complete query reduced premise-restricted strongly
conclusion-restricted complete queries according Section 4.3. Again, reduction
done time O(n), since basic event G Theorem 4.9 a) computable time O(n).
Thus, claims follow Theorem 4.9 Lemma 4.13. Note Theorem 4.9 b) (1),
tight lower answer 9(F jB )[x1 ; x2 ] computed without u2 v2 . 2
4.6 Comparison Classical Linear Programming Approach

comparison, brie describe probabilistic deduction conditional constraint trees done classical linear programming approach (Paa, 1988; van
der Gaag, 1991; Amarger et al. 1991; Hansen et al. 1995). sequel, let 9(F jE )[x1 ; x2 ]
query exact general conditional constraint tree (B; KB ) n nodes.
tight answer 9(F jE )[x1 ; x2 ] computed solving two linear programs.
detail, requested greatest lower least upper bound given optimal values
following two linear programs xA 0 (A 2 AB ) opt 2 fmin; maxg:
opt

P

A2AB ; A)EF xA

subject

P
A2AB ; A)E xA = 1
P
P
A2AB ; A)GH xA u1 A2AB ; A)G xA (H jG)[u1 ; u2 ] 2 KB
P
P
A2AB ; A)GH xA u2 A2AB ; A)G xA (H jG)[u1 ; u2 ] 2 KB
is, tight answer computed solving two linear programs 2n variables
4n , 2 linear inequalities. example, tight answer premise-restricted
complete query 9(QRSTUjM)[x1 ; x2 ] conditional constraint trees Fig. 2 yields two
linear programs 29 = 512 variables 4 9 , 2 = 34 linear inequalities.
224

fiProbabilistic Deduction Conditional Constraints Basic Events

Hence, solve two linear programs standard simplex method
standard interior-point technique, need immediately exponential time n.
still open question whether column generation techniques help solve two
linear programs less exponential time n worst case.

5. Comparison Bayesian Networks
section, brie discuss relationship conditional constraint trees
Bayesian networks (Pearl, 1988).
Bayesian network defined directed acyclic graph G discrete random variables X1 ; X2 ; : : : ; Xn nodes conditional probability distribution Pr (Xi jpa(Xi ))
random variable Xi instantiation pa(Xi ) parents pa(Xi ). specifies
unique joint probability distribution Pr X1 ; X2 ; : : : ; Xn by:
Pr (X1 ; X2 ; : : : ; Xn ) =

n

i=1

Pr (Xi j pa(Xi )) :

is, joint probability distribution Pr uniquely determined conditional
distributions Pr (Xi jpa(Xi )) certain conditional independencies encoded G.
Hence, Bayesian trees (that is, Bayesian networks directed tree associated
directed acyclic graph) binary random variables seem close exact
conditional constraint trees. However, exact general conditional constraint trees
associated undirected tree encode independencies! reason, exact general conditional constraint trees describe convex sets joint probability
distributions rather single joint probability distributions.
But, would possible additionally assume certain independencies? course,
exact general conditional constraint tree (B; KB ), associate probabilistic
interpretations Pr models KB additionally undirected tree
(B; $) I-map (Pearl, 1988). is, would independencies without causal
directionality like Markov trees (Pearl, 1988). However, idea carry us
single probabilistic interpretation (neither exact conditional constraint trees,
general conditional constraint trees), interesting topic future research
investigate computation tight answers exact general conditional constraint
trees changes kind independencies (which yield tighter bounds, since
reduce number models exact general conditional constraint trees).
Finally, additionally fix probability exactly one node, exact conditional constraint tree described independencies specifies exactly one probabilistic
interpretation (note that, keep satisfiability, probability node must respect certain
upper bounds, entailed exact conditional constraint tree). But, exact
conditional constraint trees fact Bayesian trees binary random variables.

6. Summary Conclusions
showed globally complete probabilistic deduction conditional constraints
basic events NP-hard. concentrated special case probabilistic deduction
225

fiLukasiewicz

exact general conditional constraint trees. presented ecient techniques
globally complete probabilistic deduction. precisely, exact conditional constraint
trees, presented local approach runs linear time size conditional
constraint trees. general conditional constraint trees, introduced global approach
runs polynomial time size conditional constraint trees.
Probabilistic deduction conditional constraint trees motivated previous work
literature inference rules. generalizes patterns commonsense reasoning
thoroughly studied work. Hence, presented new class tractable
probabilistic deduction problems, driven artificial intelligence applications.
also important note deduction process exact general conditional
constraint trees easily elucidated graphical way. example, computation
tight answer premise-restricted complete query 9(QRSTUjM)[x1 ; x2 ]
exact conditional constraint tree Fig. 2, left side, illustrated labeling node
directed tree Fig. 3 corresponding tightest bounds Table 1.
Like Bayesian networks, conditional constraint trees well-structured probabilistic
knowledge bases intuitive graphical representation. Differently Bayesian
networks, conditional constraint trees encode probabilistic independencies. Thus,
also understood complement Bayesian networks, useful restricted
applications well-structured independencies hold dicult access.
Conditional constraint trees quite restricted expressive power. However,
general probabilistic knowledge bases, probabilistic deduction conditional contraint
trees may always act local inference rules. example, case desire explanatory
information specific local deductions subset whole knowledge base
(which could especially useful design phase probabilistic knowledge base).
important conclusion paper concerns question whether perform probabilistic deduction iterative application inference rules linear programming.
techniques paper elaborated following idea inference rules
probabilistic deduction. Hence, one hand, paper shows idea inference
rules indeed bring us ecient techniques globally complete probabilistic deduction
restricted settings. However, hand, given technical complexity
corresponding proofs, seems unlikely results extended probabilistic
knowledge bases significantly general conditional constraint trees.
is, far significantly general probabilistic deduction problems conditional constraints concerned, iterative application inference rules seem
promising globally complete probabilistic deduction. Note similar
conclusion drawn companion paper (1998a, 1999a), shows limits locally
complete inference rules probabilistic deduction taxonomic knowledge.
example, probabilistic deduction probabilistic logic programs assume probabilistic independencies (Ng & Subrahmanian 1993, 1994; Lukasiewicz, 1998d)
better done iterative application inference rules. Note much
promising techniques are, example, global techniques linear programming
(Lukasiewicz, 1998d) particular approximation techniques based truth-functional
many-valued logics (Lukasiewicz 1998b, 1999b).
226

fiProbabilistic Deduction Conditional Constraints Basic Events

Acknowledgements
grateful Michael Wellman referees useful comments. also
want thank Thomas Eiter valuable comments earlier version paper.
paper extended revised version paper appeared Principles Knowledge
Representation Reasoning: Proceedings 6th International Conference, pp. 380{391.

Appendix A. Preliminaries Proofs Sections 4.1 4.3

section, make technical preparations proofs Theorems 4.1, 4.2,
4.7, 4.8, 4.9. sequel, use notation

x1;1 x1;2
x2;1 x2;2
c1 c2

r1
r2

abbreviation following system equations:
(11)

x1;1 + x1;2 = r1
x2;1 + x2;2 = r2

x1;1 + x2;1 = c1
x1;2 + x2;2 = c2 :

next lemma provides optimal values two linear programs solved
proofs Theorems 4.1, 4.2, 4.7, 4.8, 4.9.
Lemma A.1 Let r1 ; r2 ; c1 ; c2 0 r1 + r2 = c1 + c2 . i; j 2 f1; 2g:
a) min(ri ; cj ) = max xi;j subject (11) xn;m 0 n; 2 f1; 2g.
b) max(0; ri , c3,j ) = min xi;j subject (11) xn;m 0 n; 2 f1; 2g.
Proof. claims easily verified (Lukasiewicz, 1996). 2
Let us assume conditional constraint tree union two subtrees
one node common. model subtree third model related
common node combined model whole conditional constraint tree.
important result follows next lemma.
Lemma A.2 Let B1 B2 sets basic events B1 \B2 = ;. Let B0 new basic
event contained B1 [ B2 . Let Pr 1 Pr 2 probabilistic interpretations
atomic events B1 [fB0 g B2 [fB0 g, respectively. Let B1 B2 conjunctive
events B1 B2 , respectively. Let Pr 0 probabilistic interpretation atomic
events fB0 ; B1 ; B2 g Pr 0 (H0 H1 ) = Pr 1 (H0 H1 ) Pr 0 (H0 H2 ) = Pr 2 (H0 H2 )
atomic events H0 , H1 , H2 fB0 g, fB1 g, fB2 g, respectively.
probabilistic interpretation Pr atomic events B1 [B2 [fB0 g with:
(12)

Pr (H0 H1 H2 ) = Pr 0 (H0 H1 H2 );
Pr (H0 A1 ) = Pr 1 (H0 A1 ); Pr (H0 A2 ) = Pr 2 (H0 A2 )
227

fiLukasiewicz

atomic events H0 , H1 , H2 , A1 , A2 sets basic events fB0 g, fB1 g,
fB2 g, B1, B2, respectively.

Proof. Let probabilistic interpretation Pr atomic events B1 [ B2 [ fB0 g

defined follows:

8
1 (H0 A1 ) Pr 2 (H0 A2 )
<Pr 0(H0 H1 H2 ) Pr
Pr 1 (H0 H1 ) Pr 2 (H0 H2 ) Pr 1 (H0 H1 ) Pr 2 (H0 H2 ) > 0
Pr (H0 A1 A2 ) = :
0
Pr 1 (H0 H1 ) Pr 2 (H0 H2 ) = 0
atomic events H0 , A1 , A2 fB0 g, B1 , B2 , respectively, atomic events
H1 fB1 g H2 fB2 g A1 ) H1 A2 ) H2 .

Now, must show Pr satisfies (12). Let H0 , H1 , H2 atomic events
fB0 g, fB1 g, fB2 g, respectively. Pr 1(H0 H1 ) > 0 Pr 2 (H0 H2 ) > 0, get:
Pr (H0 H1 H2 ) =

P

A1 2AB1 ; A1 )H1
A2 2AB2 ; A2 )H2

1 (H0 A1 ) Pr 2 (H0 A2 )
Pr 0 (H0 H1 H2 ) Pr
Pr 1 (H0 H1 ) Pr 2 (H0 H2 ) = Pr 0 (H0 H1 H2 ) :

Pr 1 (H0 H1 ) = 0 Pr 2 (H0 H2 ) = 0, get Pr (H0 H1 H2 ) = 0 = Pr 0 (H0 H1 H2 ).
Let H0 , H1 , A1 atomic events fB0 g, fB1 g, B1 , respectively,
A1 ) H1. Pr 1(H0 H1 ) > 0, Pr 2 (H0 B2 ) > 0, Pr 2 (H0 B 2 ) > 0, holds:

P

Pr (H0 A1 ) =

A2 2AB2 ; A2 )B2

+

P

A2 2AB2 ; A2 )B 2

=

Pr 1 (H0 A1 ) Pr 2 (H0 A2 )
Pr 0 (H0 H1 B2 ) Pr
1 (H0 H1 ) Pr 2 (H0 B2 )
1 (H0 A1 ) Pr 2 (H0 A2 )
Pr 0 (H0 H1 B 2 ) Pr
Pr 1 (H0 H1 ) Pr 2 (H0 B 2 )
1 (H0 A1 )
Pr 0 (H0 H1 ) Pr
Pr 1 (H0 H1 )

= Pr 1 (H0 A1 ) :

Pr 1 (H0 H1 ) > 0, Pr 2 (H0 B2 ) > 0, Pr 2 (H0 B 2 ) = 0, get:
Pr (H0 A1 ) =

=

P

A2 2AB2 ; A2 )B2

Pr 1 (H0 A1 ) Pr 2 (H0 A2 )
Pr 0 (H0 H1 B2 ) Pr
1 (H0 H1 ) Pr 2 (H0 B2 )
1 (H0 A1 )
Pr 0 (H0 H1 ) Pr
Pr 1 (H0 H1 )

= Pr 1 (H0 A1 ) :

proof similar Pr 1 (H0 H1 ) > 0, Pr 2 (H0 B2 ) = 0, Pr 2 (H0 B 2 ) > 0.
Pr 1 (H0 H1 ) = 0, get Pr (H0 A1 ) = 0 = Pr 1 (H0 A1 ).
Finally, proof Pr (H0 A2 ) = Pr 2 (H0 A2 ) atomic events H0 fB0 g
A2 B2 done analogously. 2

Appendix B. Proofs Section 4.1

section, give proofs Theorems 4.1 4.2. is, show global
soundness global completeness functions H1ff, H2ff , H2fi , H2 . proofs
done induction recursive definition H1ff , H2ff , H2fi , H2 .
228

fiProbabilistic Deduction Conditional Constraints Basic Events

prove global soundness, show local soundness computations
Leaf, Chaining, Fusion. prove global completeness, construct two models
conditional constraint tree, one related greatest lower bound another one
related least upper bound computed Leaf, Chaining, Fusion.
Leaf, model trivially given. Chaining, combine model
arrow, model subtree, model connected common node model
extended conditional constraint tree. Fusion, combine models subtrees
model connected common node model extended conditional constraint
tree. precisely, Chaining Fusion, models subtrees related
previously computed tightest bounds, model connected common node
related tightest bounds computed running Chaining Fusion.
need following technical preparations. next lemma helps us show
global completeness functions H2ff , H2fi , H2 Chaining Fusion.
Lemma B.3 a) real numbers u; v 2 (0; 1], x2 2 [0; 1], x2 ; z2 2 [0; 1)
x2 ; x2 z2 z2 x2 + x2, x 2 [z2 , x2 ; x2 ] with:
(13)
min(1; uzv 2 ; 1 , u + uxv ; u , uxv + uzv 2 ) = min(1; uzv 2 ; 1 , u + uxv 2 ; u + uxv 2 ) :
b) v2 ; x2 2 [0; 1] v2 ; x2 ; w2 ; z2 2 [0; 1) v2 w2 , v2 w2 , x2 z2 , x2 z2 ,
w2 v2 + v2 , z2 x2 + x2 , v 2 [w2 , v2 ; v2 ] x 2 [z2 , x2 ; x2 ] with:
min(w2 ; z2 ; v + z2 , x; x + w2 , v) = min(w2 ; z2 ; v2 + x2 ; x2 + v2 )
(14)
min(v; x) = min(v2 ; x2 ) :
Proof. claims easily verified (Lukasiewicz, 1996). 2
following lemma helps us prove local soundness local completeness
functions H1ff , H2ff, H2fi , H2 Chaining Fusion.
Lemma B.4 a) Let u; v 2 (0; 1], x 2 [0; 1], x 2 [0; 1). probabilistic interpretations Pr Pr (B ) > 0, conditions u Pr (B ) = Pr (BC ), v Pr (C ) = Pr (BC ),
x Pr (C ) = Pr (CL(C " )), x Pr (C ) = Pr (C L(C " )) equivalent to:
Pr (B C L(C " ))
Pr (B)
Pr (BC L(C " ))
Pr (B)

Pr (B CL(C " ))
Pr (B)
Pr (BCL(C " ))
Pr (B)

Pr (C L(C " ))
Pr (B)

ux
v

Pr (B C )
Pr (B)

1,u

Pr (BCL(C " ))
Pr (B)
Pr (BC L(C " ))
Pr (B)

Pr (B CL(C " ))
Pr (B)
Pr (BCL(C " ))
Pr (B)

u , ux
v
v

ux
v

u ,u
v

u

b) Let v; x 2 [0; 1] v; x 2 [0; 1). probabilistic interpretations Pr Pr (B ) > 0,
conditions v Pr (B ) = Pr (BL(G )), v Pr (B ) = Pr (BL(G )), x Pr (B ) = Pr (BL(H )),
x Pr (B ) = Pr (BL(H )) equivalent to:
Pr (B L(G) L(H ))
Pr (B)
Pr (BL(G)L(H ))
Pr (B)

Pr (B L(G)L(H ))
Pr (B)
Pr (BL(G )L(H ))
Pr (B)

Pr (B L(H ))
Pr (B)

x

Pr (B L(G))
Pr (B)

v

229

Pr (BL(G) L(H ))
Pr (B)
Pr (BL(G )L(H ))
Pr (B)

Pr (BL(G)L(H ))
Pr (B)
Pr (BL(G )L(H ))
Pr (B)

1,x

x

1,v

v

fiLukasiewicz

Proof. claims verified straightforward arithmetic transformations based

properties probabilistic interpretations. 2
preparations, ready prove global soundness global
completeness functions H1ff , H2ff , H2fi , H2 .
Proof Theorem 4.1. claims proved induction recursive definition
H1ff . case C = B1 : : : Bk tackled iteratively splitting C two conjunctive
events. Thus, reduced C = GH conjunctive events G H disjoint
basic events. C = B1 , define u = Pr (C jB ), v = Pr (B jC ), x1 = H1ff (C; C " ).
C = B1 : : : Bk , hence C = GH , let v1 = H1ff (B; G) x1 = H1ff (B; H ).
a) models Pr 2 Mo (B; C ) Pr (B ) = 0 satisfy indicated condition. sequel,
let Pr 2 Mo (B; C ) Pr (B ) > 0.
Basis: Let C = B . Since C = L(C ), get:

ff1 Pr (B ) = 1 Pr (B ) = Pr (BC ) = Pr (BL(C )) :
Induction: Let C = B1 . models Pr 2 2 Mo (C; C " ), get induction hypothesis
x1 Pr 2 (C ) Pr 2 (CL(C " )). Thus, Pr satisfies conditions. Since L(C " ) = L(C )
Lemmata A.1 B.4 a), get:

ff1 Pr (B ) = max(0; u , uv + uxv 1 ) Pr (B ) Pr (BL(C " )) = Pr (BL(C )) :

Let C = GH . Pr 1 2 Mo (B; G) Pr 2 2 Mo (B; H ), get induction
hypothesis v1 Pr 1 (B ) Pr 1 (BL(G)) x1 Pr 2 (B ) Pr 2 (BL(H )). Thus, Pr satisfies
conditions. Since L(G)L(H ) = L(GH ) = L(C ) Lemmata A.1 B.4 b):
max(0; v1 + x1 , 1) Pr (B ) Pr (BL(G)L(H )) = Pr (BL(C )) :
b)
Basis: Let C = B . model Pr 2 Mo (B; C ) Pr (B ) > 0, 1 Pr (B ) = ff1 Pr (B ) =
Pr (BL(C )), Pr (BL(C )) = 0 given B; B 7! 0; 1.
Induction: Let C = B1 . Let model Pr 1 f(C jB )[u; u]; (B jC )[v; v]g Pr 1 (B ) > 0
Pr 1 (C ) > 0 defined like proof Theorem 3.2.
choose appropriate model Pr 2 2 Mo (C; C " ). Let us first consider
case x1 > 0, v = 1, L(C " ) ) C . induction hypothesis, exists model
Pr 2 2 Mo (C; C " ) Pr 2 (C ) > 0, x1 Pr 2 (C ) = Pr 2 (CL(C " )), Pr 2 (CL(C " )) = 0
iff L(C " ) ) C . Let us next assume x1 = 0, v < 1, L(C " ) ) C . Theorem 3.2,
exists model Pr 002 2 Mo (C; C " ) Pr 002 (CL(C " )) > 0. induction hypothesis,
exists model Pr 02 2 Mo (C; C " ) Pr 02 (C ) > 0 0 Pr 02 (C ) = Pr 02 (CL(C " )).
Hence, exists model Pr 2 2 Mo (C; C " ) Pr 2 (C ) > 0
min(1 , v; Pr 002 (CL(C " )) = Pr 002 (C )) Pr 2 (C ) = Pr 2 (CL(C " )) :
Lemma 3.1, choose Pr 1 Pr 2 Pr 1 (C ) = Pr 2 (C ) Pr 1 (B C )
Pr 2 (CL(C " )). Lemmata A.1 B.4 a), choose probabilistic interpretation
230

fiProbabilistic Deduction Conditional Constraints Basic Events

Pr 0 fB; C; L(C " )g Pr 0 (A1 ) = Pr 1 (A1 ) Pr 0 (A2 ) = Pr 2 (A2 ) atomic
events A1 A2 fB; C g fC; L(C " )g, respectively, that:
Pr 0 (BCL(C " )) = max(0; Pr 2 (CL(C " )) , Pr 1 (B C )) = 0
Pr 0 (BCL(C " )) = max(0; Pr 2 (CL(C " )) , Pr 1 (BC )) :

Lemma A.2 B1 = fB g, B2 = B(C; C " ) nfC g, B0 = C , B1 = B , B2 = L(C " ),
exists probabilistic interpretation Pr B(B; C ) (12). Hence, holds Pr 2
Mo (B; C ) Pr (B ) > 0. Lemma B.4 a), get:

ff1 Pr (B ) = max(0; u , uv + uxv 1 ) Pr (B ) = Pr (BL(C " )) = Pr (BL(C )) :
Moreover, easy see Pr (BL(C )) = 0 iff L(C ) ) B .
Let C = GH . induction hypothesis, models Pr 1 2 Mo (B; G)
Pr 2 2 Mo (B; H ) Pr 1 (B ) > 0, Pr 2 (B ) > 0, v1 Pr 1 (B ) = Pr 1 (BL(G)), x1 Pr 2 (B ) =
Pr 2 (BL(H )), Pr 1 (BL(G)) = 0 iff L(G) ) B , Pr 2 (BL(H )) = 0 iff L(H ) ) B .
Lemma 3.1, choose Pr 1 Pr 2 Pr 1 (B ) = Pr 2 (B ) Pr 1 (B L(G))
Pr 2 (BL(H ). Lemmata A.1 B.4 b), choose probabilistic interpretation
Pr 0 fB; L(G); L(H )g Pr 0 (A1 ) = Pr 1 (A1 ) Pr 0 (A2 ) = Pr 2 (A2 ) atomic
events A1 A2 fB; L(G)g fB; L(H )g, respectively, that:
Pr 0 (BL(G)L(H )) = min(Pr 2 (BL(H )); Pr 1 (BL(G))
Pr 0 (BL(G)L(H )) = max(0; Pr 2 (BL(H )) , Pr 1 (BL(G))) :

Lemma A.2 B1 = B(B; G) nfB g, B2 = B(B; H ) nfB g, B0 = B , B1 = L(G),
B2 = L(H ), exists probabilistic interpretation Pr B(B; C ) (12). Hence,
holds Pr 2 Mo (B; C ) Pr (B ) > 0. Lemma B.4 b), get:
max(0; v1 + x1 , 1) Pr (B ) = Pr (BL(G)L(H )) = Pr (BL(C )) :
Moreover, easy see Pr (BL(C )) = 0 iff L(C ) ) B . 2

Proof fiof Theorem
4.2. claims proved induction recursive definition


H2ff , H2 , H2 . Again, case C = B1 : : : Bk tackled iteratively splitting C
two conjunctive events. Thus, reduced C = GH conjunctive events G H
disjoint basic events. C = B1 let u = Pr (C jB ), v = Pr (B jC ),

x2 = H2ff (C; C " ); x2 = H2fi (C; C " ); z2 = H2 (C; C " ) :
C = B1 : : : Bk , hence C = GH , define:
v2 = H2ff(B; G); v2 = H2fi (B; G); w2 = H2 (B; G)
x2 = H2ff (B; H ); x2 = H2fi (B; H ); z2 = H2 (B; H ) :
a) Pr 2 Mo (B; C ) Pr (B ) = 0, get Pr (N ) = 0 N 2 B(B; C ). Thus, Pr
satisfies indicated conditions. Next, let Pr 2 Mo (B; C ) Pr (B ) > 0.
231

fiLukasiewicz

Basis: Let C = B . Since L(C ) = C , get:
Pr (BL(C )) = Pr (BC ) = 1 Pr (B ) = ff2 Pr (B )
Pr (BL(C )) = Pr (BC ) = 0 Pr (B ) = fi2 Pr (B )
Pr (L(C )) = Pr (C ) = 1 Pr (B ) = 2 Pr (B ) :
Induction: Let C = B1 . models Pr 2 2 Mo (C; C " ), get induction hypothesis
Pr 2 (CL(C " )) x2 Pr 2 (C ), Pr 2 (CL(C " )) x2 Pr 2 (C ), Pr 2 (L(C " )) z2 Pr 2 (C ).
Hence, Pr satisfies conditions. Since L(C ) = L(C " ) Lemmata A.1
B.4 a), get:
Pr (BL(C )) = Pr (BL(C " )) min(1; uzv 2 ; 1 , u + uxv 2 ; u + uxv 2 ) Pr (B ) = ff2 Pr (B )
Pr (BL(C )) = Pr (BL(C " ))
min( uxv 2 + uv , u; uzv 2 ) Pr (B ) = fi2 Pr (B )
uz2 Pr (B ) = 2 Pr (B ) :
Pr (L(C )) = Pr (L(C " ))
v
Let C = GH . models Pr 1 2 Mo (B; G) models Pr 2 2 Mo (B; H ), get
induction hypothesis:
Pr 1 (BL(G)) v2 Pr 1 (B ); Pr 2 (BL(H )) x2 Pr 2 (B )
Pr 1 (BL(G)) v2 Pr 1 (B ); Pr 2 (BL(H )) x2 Pr 2 (B )
Pr 1 (L(G)) w2 Pr 1 (B ); Pr 2 (L(H )) z2 Pr 2 (B ) :
Thus, Pr satisfies conditions. Since L(C ) = L(GH ) = L(G)L(H ) Lemmata A.1 B.4 b), get:
Pr (BL(C )) = Pr (BL(G)L(H )) min(v2 ; x2 ) Pr (B )
Pr (BL(C )) = Pr (BL(G)L(H )) min(v 2 ; x2 ) Pr (B )
Pr (L(C )) = Pr (L(G)L(H )) min(w2 ; z2 ; v2 + x2 ; x2 + v 2 ) Pr (B ) :
b) c)
Basis: Let C = B . model Pr 2 Mo (B; C ) Pr (B ) > 0 satisfying Pr (BL(C )) =
1 Pr (B ) = ff2 Pr (B ), Pr (BL(C )) = 0 Pr (B ) = fi2 Pr (B ), Pr (L(C )) = 1 Pr (B ) =
2 Pr (B ) given B; B 7! 0; 1.
Induction: Let C = B1 . Let model Pr 1 f(C jB )[u; u]; (B jC )[v; v]g Pr 1 (B ) > 0
Pr 1 (C ) > 0 defined like proof Theorem 3.2.
proof c), induction hypothesis, Pr 2 2 Mo (C; C " )
Pr 2 (C ) > 0, Pr 2 (CL(C " )) = x2 Pr 2 (C ), Pr 2 (L(C " )) = z2 Pr 2 (C ).
Lemma 3.1, choose Pr 1 Pr 2 Pr 1 (C ) = Pr 2 (C ) Pr 1 (B C )
Pr 2 (CL(C " )). Lemma A.1, choose probabilistic interpretation Pr 0
fB; C; L(C " )g Pr 0 (A1 ) = Pr 1 (A1 ) Pr 0(A2 ) = Pr 2(A2 ) atomic events A1
A2 fB; C g fC; L(C " )g, respectively, that:
Pr 0 (B CL(C " )) = min(Pr 1 (B C ); Pr 2 (CL(C " ))) = Pr 2 (CL(C " ))
Pr 0 (BCL(C " )) = min(Pr 1 (BC ); Pr 2 (CL(C " ))) :
232

fiProbabilistic Deduction Conditional Constraints Basic Events

Lemma A.2 B1 = fB g, B2 = B(C; C " ) n fC g, B0 = C , B1 = B , B2 =
L(C "), probabilistic interpretation Pr B(B; C ) (12). Hence, holds
Pr 2 Mo (B; C ) Pr (B ) > 0. Lemma B.4 a), get:
Pr (BL(C )) = Pr (BL(C " )) = min( uxv 2 + uv , u; uzv 2 ) Pr (B ) = fi2 Pr (B )
uz2 Pr (B ) = 2 Pr (B ) :
Pr (L(C )) = Pr (L(C " )) =
v

proof b), induction hypothesis, models Pr 1;2 ; Pr 2;2 2
Mo (C; C " ) Pr 1;2 (C ) > 0, Pr 2;2 (C ) > 0,
(15)

Pr 1;2 (CL(C " )) = x2 Pr 1;2 (C ); Pr 1;2 (L(C " )) = z2 Pr 1;2 (C )
Pr 2;2 (CL(C " )) = x2 Pr 2;2 (C ); Pr 2;2 (L(C " )) = z2 Pr 2;2 (C ) :

conditions already entail x2 z2 x2 z2 . results a), additionally get z2 x2 + x2 . Lemma B.3 a), x 2 [z2 , x2 ; x2 ] (13). (15),
Pr 2 2 Mo (C; C " ) Pr 2 (C ) > 0
Pr 2 (CL(C " )) = x Pr 2 (C ); Pr 2 (L(C " )) = z2 Pr 2 (C ) :
Lemma 3.1, choose Pr 1 Pr 2 Pr 1 (C ) = Pr 2 (C ). Lemma A.1,
choose probabilistic interpretation Pr 0 fB; C; L(C " )g Pr 0 (A1 ) = Pr 1 (A1 )
Pr 0 (A2 ) = Pr 2 (A2 ) atomic events A1 A2 fB; C g fC; L(C " )g,
respectively, that:
Pr 0 (BCL(C " )) = min(Pr 1 (BC ); Pr 2 (CL(C " )))
Pr 0 (BCL(C " )) = min(Pr 1 (BC ); Pr 2 (CL(C " ))) :

Lemma A.2 B1 = fB g, B2 = B(C; C " ) n fC g, B0 = C , B1 = B , B2 =
L(C "), probabilistic interpretation Pr B(B; C ) (12). Hence, holds
Pr 2 Mo (B; C ) Pr (B ) > 0. Lemma B.4 a), get:
Pr (BL(C )) = Pr (BL(C " )) = min(1; uzv 2 ; 1 , u + uxv 2 ; u + uxv 2 ) Pr (B ) = ff2 Pr (B )
uz2 Pr (B ) = 2 Pr (B ) :
Pr (L(C )) = Pr (L(C " )) =
v

Let C = GH . show b), claim c) proved analogously. induction hypothesis, models Pr 1;1 ; Pr 2;1 2 Mo (B; G) Pr 1;2 ; Pr 2;2 2 Mo (B; H )
Pr 1;1 (B ) > 0, Pr 2;1 (B ) > 0, Pr 1;2 (B ) > 0, Pr 2;2 (B ) > 0,
(16)

Pr 1;1 (BL(G)) = v2 Pr 1;1 (B );
Pr 2;1 (BL(G)) = v 2 Pr 2;1 (B );
Pr 1;2 (BL(H )) = x2 Pr 1;2 (B );
Pr 2;2 (BL(H )) = x2 Pr 2;2 (B );

Pr 1;1 (L(G)) = w2 Pr 1;1 (B )
Pr 2;1 (L(G)) = w2 Pr 2;1 (B )
Pr 1;2 (L(H )) = z2 Pr 1;2 (B )
Pr 2;2 (L(H )) = z2 Pr 2;2 (B ) :

conditions already entail v2 w2 , v 2 w2 , x2 z2 , x2 z2 . results
a), additionally get w2 v2 + v2 z2 x2 + x2 . Lemma B.3 b),
233

fiLukasiewicz

v 2 [w2 , v2 ; v2 ] x 2 [z2 , x2; x2 ] (14). (16), Pr 1 2 Mo (B; G)
Pr 2 2 Mo (B; H ) Pr 1 (B ) > 0, Pr 2 (B ) > 0,
Pr 1 (BL(G)) = v Pr 1 (B ); Pr 1 (L(G)) = w2 Pr 1 (B )
Pr 2 (BL(H )) = x Pr 2 (B ); Pr 2 (L(H )) = z2 Pr 2 (B ) :

Lemma 3.1, choose Pr 1 Pr 2 Pr 1 (B ) = Pr 2 (B ). Lemma A.1,
choose probabilistic interpretation Pr 0 fB; L(G); L(H )g Pr 0 (A1 ) =
Pr 1 (A1 ) Pr 0 (A2 ) = Pr 2 (A2 ) atomic events A1 A2 fB; L(G)g
fB; L(H )g, respectively, that:
Pr 0 (BL(G )L(H )) = min(Pr 1 (BL(G)); Pr 2 (BL(H )))
Pr 0 (BL(G )L(H )) = min(Pr 1 (BL(G)); Pr 2 (BL(H ))) :

Lemma A.2 B1 = B(B; G) n fB g, B2 = B(B; H ) n fB g, B0 = B , B1 = L(G),
B2 = L(H ), probabilistic interpretation Pr B(B; C ) (12). Hence,
holds Pr 2 Mo (B; C ) Pr (B ) > 0. Lemma B.4 b), get:
Pr (BL(C )) = Pr (BL(G)L(H )) = min(v2 ; x2 ) Pr (B )
Pr (L(C )) = Pr (L(G)L(H )) = min(w2 ; z2 ; v2 + x2 ; x2 + v2 ) Pr (B ) : 2

Finally, note computing least upper bounds dicult computing greatest lower bounds, since edge B ! C , Lemmata 3.1 B.4 a), greatest lower
bound Pr (BCL(C " ))=Pr (B ) subject Pr 2 Mo (B; C ) Pr (B ) > 0 always 0,
least upper bound Pr (BCL(C " ))=Pr (B ) subject Pr 2 Mo (B; C ) Pr (B ) > 0
generally 1.

Appendix C. Proofs Section 4.2

section, give proofs Theorems 4.7 4.8.
need technical preparations follows. next lemma helps us show
local soundness function H1 Fusion.

Lemma C.5 real numbers u1 ; u; v1 ; v; x1 ; x; y1; 2 (0; 1] u1 u, v1 v,
x1 x, y1 y, u1 + x1 > 1, holds:
min(u=v , u; x=y , x) = (u + x , 1) min(u1 =v1 , u1 ; x1 =y1 , x1 ) = (u1 + x1 , 1) :

Proof. claim easily verified (Lukasiewicz, 1996). 2

following lemma helps us show local soundness local completeness
function H1 Chaining Fusion.

Lemma C.6 a) Let u, v, x, real numbers (0; 1]. probabilistic interpretations Pr Pr (L(C " )) > 0, conditions u Pr (B ) = Pr (BC ), v Pr (C ) = Pr (BC ),
x Pr (C ) = Pr (CL(C " )), Pr (L(C " )) = Pr (CL(C " )) equivalent to:
234

fiProbabilistic Deduction Conditional Constraints Basic Events

Pr (B C L(C " ))
Pr (L(C " ))
Pr (BC L(C " ))
Pr (L(C " ))

Pr (B CL(C " ))
Pr (L(C " ))
Pr (BCL(C " ))
Pr (L(C " ))

Pr (C L(C " ))
Pr (L(C " ))

1,y

Pr (B C )
Pr (L(C " ))
yv yv
xu , x

Pr (BCL(C " ))
Pr (L(C " ))
Pr (BC L(C " ))
Pr (L(C " ))

Pr (B CL(C " ))
Pr (L(C " ))
Pr (BCL(C " ))
Pr (L(C " ))


x ,y



yv
x, x
yv
x

b) Let u, v, x, real numbers (0; 1]. probabilistic interpretations Pr
Pr (B ) > 0, conditions u Pr (B ) = Pr (BL(G )), v Pr (L(G)) = Pr (BL(G )),
x Pr (B ) = Pr (BL(H )), Pr (L(H )) = Pr (BL(H )) equivalent to:
Pr (B L(G) L(H ))
Pr (B)
Pr (BL(G)L(H ))
Pr (B)

Pr (B L(G)L(H ))
Pr (B)
Pr (BL(G )L(H ))
Pr (B)

Pr (B L(H ))
Pr (B)

x ,x


Pr (B L(G))
Pr (B)
u ,u
v

Pr (BL(G) L(H ))
Pr (B)
Pr (BL(G )L(H ))
Pr (B)

Pr (BL(G)L(H ))
Pr (B)
Pr (BL(G )L(H ))
Pr (B)

1,x

x

1,u

u

Proof. claims verified straightforward arithmetic transformations based

properties probabilistic interpretations. 2
ready prove Theorems 4.7 4.8.
Proof Theorem 4.7. claims proved induction recursive definition
H1 . proof C = B1B2 : : : Bk k > 1 done k = 2. easily generalized
k 2. C = B1 , define u1 = Pr 1 (C jB ), v1 = Pr 1 (B jC ), x1 = H1ff (C; C " ),
y1 = H1 (C; C " ). Note ff1 > 0 entails x1; y1 > 0 v1 + x1 > 1. C = B1 B2 ,
define G = B1 , H = B2 , u1 = H1ff (B; G), v1 = H1 (B; G), x1 = H1ff (B; H ),
y1 = H1 (B; H ). Note ff1 > 0 entails u1 ; v1 ; x1 ; y1 > 0 u1 + x1 > 1.
a) models Pr 2 Mo (B; C ) Pr (L(C )) = 0 satisfy indicated condition.
sequel, let Pr 2 Mo (B; C ) Pr (L(C )) > 0 thus also Pr (B ) > 0.
Basis: Let C = B . Since C = L(C ), get:

1 Pr (L(C )) = 1 Pr (L(C )) = Pr (CL(C )) = Pr (BL(C )) :
Induction: Let C = B1 . models Pr 2 2 Mo (C; C " ), get x1 Pr 2 (C ) Pr 2 (CL(C " ))
Theorem 4.3 a), y1 Pr 2 (L(C " )) Pr 2 (CL(C " )) induction hypothesis. Thus,
Pr satisfies conditions. Since L(C " ) = L(C ) Lemmata A.1 C.6 a):

1 = y1 , xy11 + yx1 v11 Pr (CL(C " )) = Pr (L(C " )) = Pr (CL(C )) = Pr (L(C )) :
Let C = GH . models Pr 1 2 Mo (B; G) Pr 2 2 Mo (B; H ), get
Theorem 4.3 a) induction hypothesis, respectively:

u1 Pr 1 (B ) Pr 1(BL(G)); x1 Pr 2 (B ) Pr 2(BL(H ))
v1 Pr 1(L(G)) Pr 1(BL(G)); y1 Pr 2(L(H )) Pr 2(BL(H )) :
235

fiLukasiewicz

Hence, Pr satisfies conditions. Since L(G)L(H ) = L(GH ) = L(C ) Lemmata A.1, C.5, C.6 b), get:
(BL(G)L(H ))=Pr (B )
1 = 1 = (1 + min(u1 =vu11,+ux11;,x11 =y1,x1 ) ) 1 = (1 + Pr
Pr (BL(G)L(H ))=Pr (B) ) =

Pr (BL(C ))
Pr (L(C ))

:

b)
Basis: Let C = B . model Pr 2 Mo (B; C ) Pr (B ) > 0, Pr (L(C )) > 0,
1 Pr (L(C )) = Pr (BL(C )), 1 Pr (B ) = Pr (BL(C )) given B; B 7! 0; 1.
Induction: Let C = B1 . Let model Pr 1 f(C jB )[u1 ; u1 ]; (B jC )[v1 ; v1 ]g Pr 1 (B ) > 0
Pr 1 (C ) > 0 defined like proof Theorem 3.2.
induction hypothesis, Pr 2 2 Mo (C; C " ) Pr 2 (C ) > 0, Pr 2 (L(C " )) > 0,
y1 Pr 2 (L(C ")) = Pr 2 (CL(C ")), x1 Pr 2 (C ) = Pr 2 (CL(C ")). Lemma 3.1,
choose Pr 1 Pr 2 Pr 1 (C ) = Pr 2 (C ) Pr 1 (B C ) Pr 2 (CL(C " )). Lemmata A.1 C.6 a), choose probabilistic interpretation Pr 0 fB; C; L(C " )g
Pr 0 (A1 ) = Pr 1 (A1 ) Pr 0 (A2 ) = Pr 2 (A2 ) atomic events A1 A2
fB; C g fC; L(C " )g, respectively, that:
Pr 0 (BCL(C " )) = max(0; Pr 2 (CL(C " )) , Pr 1 (B C )) = 0
Pr 0 (BCL(C " )) = max(0; Pr 2 (CL(C " )) , Pr 1 (BC )) :

Lemma A.2 B1 = fB g, B2 = B(C; C " ) nfC g, B0 = C , B1 = B , B2 = L(C " ),
exists probabilistic interpretation Pr B(B; C ) (12). Hence, holds Pr 2
Mo (B; C ), Pr (B ) > 0, Pr (L(C )) > 0. Moreover, Lemma C.6 a), get:

1 = y1 , xy11 + yx1v11 = Pr (CL(C ")) = Pr (L(C " )) = Pr (CL(C )) = Pr (L(C ))
= Pr (CL(C )) = Pr (C ) :
ff1 = u1 , uv11 + u1v1x1 = Pr (CL(C ")) = Pr (C )
Let C = GH . induction hypothesis, models Pr 1 2 Mo (B; G)
Pr 2 2 Mo (B; H ) Pr 1 (B ) > 0, Pr 2 (B ) > 0, Pr 1 (L(G)) > 0, Pr 2 (L(H )) > 0,

u1 Pr 1 (B ) = Pr 1 (BL(G)); x1 Pr 2(B ) = Pr 2 (BL(H ))
v1 Pr 1(L(G)) = Pr 1 (BL(G)); y1 Pr 2 (L(H )) = Pr 2 (BL(H )) :
Lemma 3.1, choose Pr 1 Pr 2 Pr 1 (B ) = Pr 2 (B ) Pr 1 (B L(G))
Pr 2 (BL(H ). Lemmata A.1 C.6 b), choose probabilistic interpretation
Pr 0 fB; L(G); L(H )g Pr 0 (A1 ) = Pr 1 (A1 ) Pr 0 (A2 ) = Pr 2 (A2 ) atomic
events A1 A2 fB; L(G)g fB; L(H )g, respectively, that:
Pr 0 (BL(G)L(H )) = min(Pr 2 (BL(H )); Pr 1 (BL(G)))
Pr 0 (BL(G)L(H )) = max(0; Pr 2 (BL(H )) , Pr 1 (BL(G))) :

Lemma A.2 B1 = B(B; G) nfB g, B2 = B(B; H ) nfB g, B0 = B , B1 = L(G),
B2 = L(H ), exists probabilistic interpretation Pr B(B; C ) (12). Hence,
236

fiProbabilistic Deduction Conditional Constraints Basic Events

holds Pr 2 Mo (B; C ) Pr (B ) > 0. Lemma C.6 b), get Pr (L(C )) > 0
Pr (BL(G)L(H ))=Pr (B) ) = Pr (BL(C ))
1 = 1 = (1 + min(u1 =vu11,+ux11;,x11=y1 ,x1) ) = 1 = (1 + Pr
(BL(G)L(H ))=Pr (B)
Pr (L(C ))

Pr (BL(G)L(H ))
(BL(C )) :
ff1 =
u1 + x1 , 1
=
= PrPr
Pr (B)
(B )
c) Let C = GH . Theorem 4.3 b), exist Pr 1 2 Mo (B; G) Pr 2 2 Mo (B; H )
Pr 1 (B ) > 0, Pr 2 (B ) > 0, u1 Pr 1 (B ) = Pr 1 (BL(G)), x1 Pr 2 (B ) = Pr 2 (BL(H )).
Lemma 3.1, choose Pr 1 Pr 2 Pr 1 (B ) = Pr 2 (B ) Pr 1 (B L(G))
Pr 2 (BL(H ). Lemmata A.1 C.6 b), choose probabilistic interpretation
Pr 0 fB; L(G); L(H )g Pr 0 (A1 ) = Pr 1 (A1 ) Pr 0 (A2 ) = Pr 2 (A2 ) atomic
events A1 A2 fB; L(G)g fB; L(H )g, respectively, that:
Pr 0 (BL(G)L(H )) = max(0; Pr 2 (BL(H ) , Pr 1 (B L(G))) = 0
Pr 0 (BL(G)L(H )) = max(0; Pr 2 (BL(H )) , Pr 1 (BL(G))) :
Lemma A.2 B1 = B(B; G) nfB g, B2 = B(B; H ) nfB g, B0 = B , B1 = L(G),
B2 = L(H ), exists probabilistic interpretation Pr B(B; C ) (12). Hence,
holds Pr 2 Mo (B; C ) Pr (B ) > 0. Lemma C.6 b), get Pr (L(C )) > 0
1 = Pr (BL(G)L(H )) = Pr (L(G)L(H )) = Pr (BL(C )) = Pr (L(C ))
ff1 = Pr (BL(G)L(H )) = Pr (B ))
= Pr (BL(C )) = Pr (B ) :
d) Let C = GH . Theorem 3.2, model Pr 000 2 Mo (B; C ) Pr 000 (BL(C )) > 0.
Theorem 4.3 b), model Pr 00 2 Mo (B; C ) Pr 00 (B ) > 0 0 Pr 00 (B ) =
ff1 Pr 00(B ) = Pr 00 (BL(C )). Hence, model Pr 0 2 Mo (B; C ) Pr 0 (B ) > 0
min("; Pr 000 (BL(C )) = Pr 000 (B )) Pr 0 (B ) = Pr 0 (BL(C )) :
Let models Pr 1 2 Mo (B; G) Pr 2 2 Mo (B; H ) defined Pr 1 (A1 ) = Pr 0 (A1 )
Pr 2 (A2 ) = Pr 0 (A2 ) atomic events A1 A2 B(B; G) B(B; H ), respectively.
Lemma 3.1, choose Pr 1 Pr 2 Pr 1 (B ) = Pr 2 (B ) Pr 1 (B L(G))
Pr 2 (BL(H ). Lemmata A.1 C.6 b), choose probabilistic interpretation
Pr 0 fB; L(G); L(H )g Pr 0 (A1 ) = Pr 1 (A1 ) Pr 0 (A2 ) = Pr 2 (A2 ) atomic
events A1 A2 fB; L(G)g fB; L(H )g, respectively, that:
= 0
Pr 0 (BL(G)L(H )) = max(0; Pr 2 (BL(H ) , Pr 1 (B L(G)))
Pr 0 (BL(G)L(H )) = min("; Pr 000 (BL(C )) = Pr 000 (B )) Pr 0 (B ) :
Lemma A.2 B1 = B(B; G) n fB g, B2 = B(B; H ) n fB g, B0 = B , B1 = L(G),
B2 = L(H ), probabilistic interpretation Pr B(B; C ) (12). Hence, holds
Pr 2 Mo (B; C ), Pr (B ) > 0, Pr (L(C )) > 0, Pr (BL(C )) = 0, " Pr (B ) Pr (BL(C )). 2
Proof Theorem 4.8. u1 > 0, claim immediate Theorem 4.7 a) c).
Let u1 = 0 E ) F . holds 1 Pr (E ) = Pr (EF ) models Pr KB . Moreover,
Theorem 3.2, exists model Pr KB Pr (E ) > 0.
Let u1 = 0 E ) F . Theorem 4.3 b), exists model Pr KB
Pr (E ) > 0 Pr (EF ) = 0. Theorem 4.7 d), exists model Pr KB
Pr (E ) > 0 1 Pr (E ) = Pr (EF ). 2

237

fiLukasiewicz

Appendix D. Proofs Section 4.3
section, give proof Theorem 4.9.
next lemma help us show global tightness computed lower bound
case (3) Theorem 4.9 b).

Lemma D.7 Let x 2 [0; 1] v; x 2 [0; 1). probabilistic interpretations Pr
Pr (G ) > 0, conditions Pr (EG ) = 0, v Pr (G) = Pr (EG ), x Pr (G) = Pr (GF ),
x Pr (G) = Pr (GF ) equivalent to:
Pr (E G F )
Pr (G)
Pr (EG F )
Pr (G)

Pr (E GF )
Pr (G)
Pr (EGF )
Pr (G)

Pr (G F )
Pr (G)

x

Pr (E G)
Pr (G)

v

Pr (EGF )
Pr (G)
Pr (EG F )
Pr (G)

Pr (E GF )
Pr (G)
Pr (EGF )
Pr (G)

1,x

x

1
0

Proof. claim verified straightforward arithmetic transformations based

properties probabilistic interpretations. 2
ready prove Theorem 4.9.
Proof Theorem 4.9. a) definition queries conditional constraint trees,
paths basic event E basic event F least one basic event common.
Hence, choose basic event G basic events common
9(GjE )[z1 ; z2 ] strongly conclusion-restricted complete query subtree.
b) u1 > 0, claim follows Theorem 4.7 a) c). special case exact
conditional constraint trees (B; KB ), claim follows Theorems 4.3 4:5.
Let u1 = 0, v1 = 1, G ) F . holds 1 Pr (E ) = Pr (EF ) models Pr KB .
Moreover, Theorem 3.2, exists model Pr KB Pr (E ) > 0.
Let u1 = 0, v1 = 0, G ) F . easy see (1) Theorem 4.7 d),
tight upper answer given fx2 =1g. show tight lower answer given
fx1 =0g. Theorem 4.3 b), exists model Pr 1 KB 1 Pr 1 (E ) > 0, Pr 1 (G) > 0,
Pr 1 (EG ) = 0. Theorem 3.2, exists model Pr 2 KB 2 Pr 2 (G) > 0.
Lemma 3.1, choose Pr 1 Pr 2 Pr 1 (G) = Pr 2 (G) Pr 1 (E G) Pr 2 (GF ).
Lemmata A.1 D.7, choose probabilistic interpretation Pr 0 fE; G; F g
Pr 0 (A1 ) = Pr 1 (A1 ) Pr 0 (A2 ) = Pr 2 (A2 ) atomic events A1 A2
fE; Gg fG; F g, respectively, that:
Pr 0 (EGF ) = max(0; Pr 2 (GF ) , Pr 1 (E G)) = 0
Pr 0 (EGF ) = 0 :

Lemma A.2, exists probabilistic interpretation Pr B (12) atomic
events H0 , H1 , H2 , A1 , A2 sets basic events fGg, fE g, fF g, B1 n fGg,
B2 n fGg, respectively. Hence, Pr model KB Pr (E ) > 0 Pr (EF ) = 0.
u1 = 0 G ) F , claim follows (1) Theorem 4.7 d). 2
238

fiProbabilistic Deduction Conditional Constraints Basic Events

References

Adams, E. W. (1975). Logic Conditionals, Vol. 86 Synthese Library. D. Reidel,
Dordrecht, Netherlands.
Amarger, S., Dubois, D., & Prade, H. (1991). Constraint propagation imprecise
conditional probabilities. Proceedings 7th Conference Uncertainty
Artificial Intelligence, pp. 26{34. Morgan Kaufmann.
Andersen, K. A., & Hooker, J. N. (1994). Bayesian logic. Decision Support Systems, 11,
191{210.
Bacchus, F. (1990). Representing Reasoning Probabilistic Knowledge: Logical
Approach Probabilities. MIT Press, Cambridge, USA.
Bacchus, F., Grove, A., Halpern, J. Y., & Koller, D. (1996). statistical knowledge
bases degrees beliefs. Artificial Intelligence, 87, 75{143.
Carnap, R. (1950). Logical Foundations Probability. University Chicago Press, Chicago.
Coletti, G. (1994). Coherent numerical ordinal probabilistic assessments. IEEE Transactions Systems, Man, Cybernetics, 24 (12), 1747{1754.
de Finetti, B. (1974). Theory Probability. Wiley, New York.
Dubois, D., & Prade, H. (1988). fuzzy syllogisms. Computational Intelligence, 4 (2),
171{179.
Dubois, D., Prade, H., Godo, L., & de Mantaras, R. L. (1993). Qualitative reasoning
imprecise probabilities. Journal Intelligent Information Systems, 2, 319{363.
Dubois, D., Prade, H., & Touscas, J.-M. (1990). Inference imprecise numerical quantifiers. Ras, Z. W., & Zemankova, M. (Eds.), Intelligent Systems, chap. 3, pp. 53{72.
Ellis Horwood.
Fagin, R., Halpern, J. Y., & Megiddo, N. (1990). logic reasoning probabilities.
Information Computation, 87, 78{128.
Frisch, A. M., & Haddawy, P. (1994). Anytime deduction probabilistic logic. Artificial
Intelligence, 69, 93{122.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide
Theory NP-Completeness. Freeman, New York.
Georgakopoulos, G., Kavvadias, D., & Papadimitriou, C. H. (1988). Probabilistic satisfiability. Journal Complexity, 4 (1), 1{11.
Gilio, A., & Scozzafava, R. (1994). Conditional events probability assessment revision. IEEE Transactions Systems, Man, Cybernetics, 24 (12), 1741{1746.
Halpern, J. Y. (1990). analysis first-order logics probability. Artifical Intelligence,
46, 311{350.
239

fiLukasiewicz

Hansen, P., Jaumard, B., Nguetse, G.-B. D., & de Arag~ao, M. P. (1995). Models algorithms probabilistic Bayesian logic. Proceedings 14th International
Joint Conference Artificial Intelligence, pp. 1862{1868.
Heinsohn, J. (1994). Probabilistic description logics. Proceedings 10th Conference
Uncertainty Artificial Intelligence. Morgan Kaufmann.
Jaumard, B., Hansen, P., & de Arag~ao, M. P. (1991). Column generation methods
probabilistic logic. ORSA Journal Computing, 3, 135{147.
Kavvadias, D., & Papadimitriou, C. H. (1990). linear programming approach reasoning
probabilities. Annals Mathematics Artificial Intelligence, 1, 189{205.
Lukasiewicz, T. (1996). Precision Probabilistic Deduction Taxonomic Knowledge.
Doctoral Dissertation, Universitat Augsburg.
Lukasiewicz, T. (1997). Ecient global probabilistic deduction taxonomic probabilistic knowledge-bases conjunctive events. Proceedings 6th International Conference Information Knowledge Management, pp. 75{82. ACM
Press.
Lukasiewicz, T. (1998a). Magic inference rules probabilistic deduction taxonomic
knowledge. Proceedings 14th Conference Uncertainty Artificial Intelligence, pp. 354{361. Morgan Kaufmann.
Lukasiewicz, T. (1998b). Many-valued first-order logics probabilistic semantics. Proceedings Annual Conference European Association Computer Science
Logic. appear.
Lukasiewicz, T. (1998c). Probabilistic deduction conditional constraints basic
events. Principles Knowledge Representation Reasoning: Proceedings
6th International Conference, pp. 380{391. Morgan Kaufmann.
Lukasiewicz, T. (1998d). Probabilistic logic programming. Proceedings 13th European Conference Artificial Intelligence, pp. 388{392. J. Wiley & Sons.
Lukasiewicz, T. (1999a). Local probabilistic deduction taxonomic probabilistic
knowledge-bases conjunctive events. International Journal Approximate Reasoning. appear.
Lukasiewicz, T. (1999b). Probabilistic truth-functional many-valued logic programming. Proceedings 29th IEEE International Symposium Multiple-Valued
Logic. appear.
Luo, C., Yu, C., Lobo, J., Wang, G., & Pham, T. (1996). Computation best bounds
probabilities uncertain data. Computational Intelligence, 12 (4), 541{566.
Ng, R. T., & Subrahmanian, V. S. (1993). semantical framework supporting subjective
conditional probabilities deductive databases. Journal Automated Reasoning,
10 (2), 191{235.
240

fiProbabilistic Deduction Conditional Constraints Basic Events

Ng, R. T., & Subrahmanian, V. S. (1994). Stable semantics probabilistic deductive
databases. Information Computation, 110, 42{83.
Nilsson, N. J. (1986). Probabilistic logic. Artificial Intelligence, 28, 71{88.
Nilsson, N. J. (1993). Probabilistic logic revisited. Artificial Intelligence, 59, 39{42.
Paa, G. (1988). Probabilistic Logic. Dubois, D., Smets, P., Mamdani, A., & Prade,
H. (Eds.), Non-Standard Logics Automated Reasoning, chap. 8, pp. 213{251. Academic Press.
Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial Optimization, Algorithms
Complexity. Prentice-Hall, Englewood Cliffs, NJ.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible
Inference. Morgan Kaufmann, San Mateo, CA.
Pittarelli, M. (1994). Anytime decision making imprecise probabilities. Proceedings
10th Conference Uncertainty Artificial Intelligence, pp. 470{477. Morgan
Kaufmann.
Schrijver, A. (1986). Theory Linear Integer Programming. Wiley, New York.
Thone, H. (1994). Precise Conclusion Uncertainty Incompleteness Deductive
Database Systems. Doctoral Dissertation, Universitat Tubingen.
Thone, H., Kieling, W., & Guntzer, U. (1995). cautious probabilistic inference
default detachment. Annals Operations Research, 55, 195{224.
van der Gaag, L. (1991). Computing probability intervals independency constraints.
Uncertainty Artificial Intelligence 6, pp. 457{466. North-Holland, Amsterdam.
Walley, P. (1991). Statistical Reasoning Imprecise Probabilities. Chapman Hall,
New York.

241

fiJournal Artificial Intelligence Research 10 (1999) 375-397

Submitted 2/99; published 6/99

Ecient Heuristic Hypothesis Ranking
steve.chien@jpl.nasa.gov
andre.stechert@jpl.nasa.gov
darren.mutz@jpl.nasa.gov

Steve Chien
Andre Stechert
Darren Mutz
Jet Propulsion Laboratory
California Institute Technology
4800 Oak Grove Drive, M/S 126-347
Pasadena, CA 91109-8099

Abstract

paper considers problem learning ranking set stochastic alternatives based upon incomplete information (i.e., limited number samples). describe
system that, decision cycle, outputs either complete ordering hypotheses
decides gather additional information (i.e., observations) cost. ranking
problem generalization previously studied hypothesis selection problem|in selection, algorithm must select single best hypothesis, ranking, algorithm
must order hypotheses.
central problem address achieving desired ranking quality minimizing cost acquiring additional samples. describe two algorithms hypothesis
ranking application probably approximately correct (PAC) expected
loss (EL) learning criteria. Empirical results provided demonstrate effectiveness
ranking procedures synthetic real-world datasets.
1. Introduction

many applications, cost information quite high, imposing requirement
learning algorithms glean much usable information possible minimum
data. example:



Data may scarce, making learning possible limited training data
key.



speedup learning, minimizing processing time critical. Here, reducing number
necessary training examples key since expense processing example
significant (Tadepalli, 1992).



decision tree learning, cost using available training examples evaluating potential attributes partitioning computationally expensive (Musick,
Catlett, & Russell, 1993).



evaluating medical treatment policies, acquiring additional training examples might
imply human subjects exposed experimental treatment longer
period necessary.

one wishes sort guarantee quality solution, statistical decision
theoretic framework useful. framework answers questions: much information

c 1999 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiChien, Stechert, & Mutz
enough? point adequate information rank alternatives
requested confidence?
paper focuses parametric ranking problems, general class statistical machine learning problems goal rank set alternative hypotheses
goodness hypothesis function set parameters whose values unknown
(e.g., Chien, Stechert, & Mutz, 1998; Gratch, 1992; Greiner & Jurisica, 1992; Kaelbling,
1993; Moore & Lee, 1994; Musick et al., 1993). learning system determines refines estimates parameters using training examples, secondary goal
minimizing learning cost.
principal contributions paper are:



define two families hypothesis ranking algorithms, based recursive selection
adjacency, respectively. provide specific details apply using
probably approximately correct (PAC) expected loss (EL) decision criteria.



provide empirical results demonstrating effectiveness algorithms
achieving requested decision criteria synthetic data.



provide empirical results showing algorithms significantly outperform
existing statistical methods real-world data spacecraft design optimization
image compression applications.

remainder paper structured follows. First, describe hypothesis ranking problem formally, including definitions probably approximately
correct (PAC) expected loss (EL) decision criteria. define two algorithms
establishing criteria hypothesis ranking problem|a recursive hypothesis selection algorithm adjacent comparison algorithm. Next, describe empirical tests
demonstrating effectiveness algorithms well documenting improved
performance standard algorithm statistical ranking literature. Finally,
describe related work future extensions algorithms.
2. Hypothesis Ranking Problems

Hypothesis ranking problems abstract class learning problems algorithm
given set hypotheses rank. ranking desired orders hypotheses
expected utility, determined hypothesis' underlying probability
distribution. expected utilities unknown algorithm must estimated
training data.
Hypothesis ranking problems extension hypothesis selection problems (Chien,
Gratch, & Burl, 1995), learning system attempts select best alternative
set hypotheses. distinction hypothesis ranking hypothesis selection selection learning algorithm interested single best hypothesis,
ranking learning algorithm must determine relative order hypotheses1 .
Hypothesis selection ranking important aspect many machine learning
problems. example, utility problem speedup learning viewed selection
1. algorithms results described paper extend straightforward fashion hybrid rankingselection problems system must select rank top N hypotheses.

376

fiEfficient Heuristic Hypothesis Ranking
problem single problem-solving heuristic strategy chosen larger set
candidates. case, expected utility typically defined average time solve
problem (Gratch, 1992; Greiner & Jurisica, 1992; Minton, 1988). attribute selection
problem machine learning also viewed hypothesis selection problem
one must select best attribute split set possible attribute splits utility
often measured information gain (Musick et al., 1993). reinforcement learning,
system must learn appropriate action context, utility interpreted
expected reward (Kaelbling, 1993).2
key observation regarding problems (and learning problems, general) could viewed optimization problem, utility
function optimized. Then, application traditional (or non-traditional)
optimization methods yield good results within guarantees provided algorithm depending features landscape optimized. However,
addition model sampling cost, new degree freedom added problem.
cost samples high, traditional optimization algorithms fare poorly.
Additionally, many mentioned applications system chooses single
alternative never revisits decision, also many cases system
want investigate several prioritized options (either serially parallel), hence
ranking useful. Motivation provided following scenarios:



Upper lower bounds, span: Minimax search algorithms use metaknowledge

(such upper lower bounds node) pruning parts tree. Also,
times knowing span expected utilities candidate set
useful (e.g., checking convergence conditions adaptive algorithm
GA).



Augmenting external knowledge: Another area hypothesis ranking may



entire ranking: cases, entire ranking significant. instance,

important applications hypothesis selection human supervision.
stochastic objective function (i.e., hypothesis) represents part problem, ranking used augment external knowledge problem.
example, engineering simulations usually capture physical properties candidate designs, usually choose forego details manufacturing, logistics,
economics.
evolutionary algorithms, individuals propagated future generations
often selected likelihood proportionate rank current
generation (Goldberg, 1989). Another example arises case search algorithms
take advantage node ordering heuristics, beam search iterative
broadening (Ginsberg & Harvey, 1992).

hypothesis evaluation problem, always achieving correct ranking impossible
practice, exact underlying probability distributions unknown. Thus,
always (perhaps vanishingly) small chance algorithms unlucky
2. Note analogous reinforcement learning problem one learning appropriate action immediate feedback rather delayed feedback.

377

fiChien, Stechert, & Mutz
finite number samples taken. Consequently, rather always
requiring algorithm output correct ranking, impose probabilistic criteria
rankings produced. several families requirements exist, paper
examine two criteria: probably approximately correct (PAC) model selecting
hypothesis function approximates well target function (Valiant, 1984)
expected loss (EL) requirement frequently used decision theory gaming problems
(Russell & Wefald, 1992). Informally, satisfy PAC requirement, algorithm must
produce result high probability close correct (e.g., incorrect orderings
likely occur hypotheses similar expected utilities). satisfy
EL requirement, hand, bound must established expected loss
result, loss difference utilities two incorrectly ordered hypothese
incorrect ranking.
expected utility hypothesis estimated observing values
finite set training examples. However, satisfy decision criteria, algorithm must
also able reason potential difference estimated true utilities
hypotheses. Let Ui denote true expected utility hypothesis let U^i
estimated expected utility hypothesis i. Without loss generality, let us presume
proposed ranking hypotheses U1 > U2 >; :::; > Uk 1 > Uk .
PAC requirement states that, user-specified , probability 1 :
k^1

[(Ui + ) > MAX (Ui+1 ; :::; Uk )]

(1)

i=1

context PAC criterion, number called indifference interval

overall ranking error total error rate. 3

issue allocate overall ranking error among many possible pairwise
comparisons hypotheses discussed next section.
Correspondingly, selecting hypothesis H1 best set k hypotheses H1 ; :::; Hk , let selection loss L follows.
L(H1 ; fH1 ; :::; Hk g) = MAX (0; MAX (U2 ; :::; Uk )

U1 )

(2)

Then, ranking loss RL ranking H1 ; :::; Hk would be:
RL(H1 ; :::; Hk ) =

k 1
X

L(Hi ; fHi+1 ; :::; Hk g)

(3)

i=1

3. distinction betwen true means estimated means (for use sample means)
confusing one. assessing validity ranking produced algorithm, one would use
true means distributions (if available, test distributions) accurate estimation
possible (such edxtremely large sampling distribution). However, ranking algorithm
uses estimated parameters (including sample mean) estimate error. estimation single
mean estimate mean normally distributed around true mean usage
justified. However, proven (and indeed unsure) whether using estimate
complex ranking selection contexts guaranteed correct (see later section heuristic nture
algorithms).

378

fiEfficient Heuristic Hypothesis Ranking
hypothesis ranking algorithm obeys expected loss requirement must produce
rankings average less ranking loss requested expected loss bound.
policy loss allocation also discussed next section.
example, consider ranking hypotheses expected utilities: U1 = 1:0; U2 =
0:95; U3 = 0:86. ranking U2 > U1 > U3 valid PAC ranking indifference
interval = 0:06 = 0:01 observed ranking loss 0:05 + 0 = 0:05.
However, confidence pairwise comparison two hypotheses well
understood complement probability comparison's result
error, less clear define ensure desired confidence met set
comparisons required selection even complex set comparisons required
ranking. Equation 4 defines confidence Ui + > Uj , utilities
normally distributed unknown unequal variances.

pn



= (U^i

j + )
^

Si

(4)

j

represents cumulative standard normal distribution function, n, U^i j ,
S^i j size, sample mean, sample standard deviation blocked differential
distribution4, respectively.
Likewise, computation expected loss asserting ordering pair
hypotheses well understood, estimation expected loss entire ranking
less clear. Equation 5 defines expected loss drawing conclusion Ui > Uj ,
assumption normality (see Chien et al., 1995, details).
EL[Ui > Uj ] =

S^i

je

U^i j 2
)
j

0:5n( ^
Si

p

2n

+

U^i

p

j

2

Z

1

U^i j pn
S^i j

e

0:5z 2

dz

(5)

next two subsections, describe two interpretations estimating likelihood
overall ranking satisfies PAC EL requirements estimating combining
pairwise PAC errors EL estimates. interpretations lends directly
algorithmic implementation described below.
2.1 Ranking Recursive Selection

One obvious way determine ranking H1 ; :::; Hk view ranking recursive selection
set remaining candidate hypotheses. view, overall ranking error,
specified desired confidence PAC algorithms loss threshold EL
algorithms, first distributed among k 1 selection errors subdivided
pairwise comparison errors (Figure 1). Data sampled estimates
pairwise comparison error (as dictated equation 4 5) satisfy bounds set
algorithm.
4. Note approach block, match, examples reduce sampling complexity. Blocking
makes estimates using difference utility competing hypotheses observed example. Blocking significantly reduce variance data hypotheses independent.
differential distribution formed taking differences blocked individual samples form
new distribution. trivial modify formulas address cases possible
block data (see Moore & Lee, 1994; Chien et al., 1995, details).

379

fiChien, Stechert, & Mutz

H1

H2

H3

H4

H5

H2

H3

H4

H5

H3

H4

H5

H4

H5


*

Figure 1: Computing overall error recursive ranking. per-comparison errors
summed level recursion, overall sum (across levels)
compared specified total error, .

Thus, another degree freedom design recursive ranking algorithms
method overall ranking error ultimately distributed among individual pairwise comparisons hypotheses. Two factors uence way compute
error distribution. First, model error combination determines error allocated
individual comparisons selections combines overall ranking error therefore
many candidates available distribution error.
Using Bonferroni's inequality, asserts probability union events
greater sum probabilities individual events5 , one would inclined
combine errors additively. However, following conservative approach, one
assert predicted \best" hypothesis may change sampling
worst case, conclusion might dependon possible pairwise comparisons
error distributed among n2 pairs hypotheses.6
Second, policy respect allocation error among candidate comparisons
selections determines samples distributed. example, contexts,
consequences early selections far outweigh later selections. scenarios,
implemented ranking algorithms divide overall ranking error unequally
5. Note simplest Bonferonni inequalities, fall clean correspondence
terms expansion probability union events according principle
inclusion exclusion natural way.
6. discussion issue, see pp. 18-20 (Gratch, 1993).

380

fiEfficient Heuristic Hypothesis Ranking
favor earlier selections.7 Also, possible divide selection error pairwise error
unequally based estimates hypothesis parameters order reduce sampling cost
(for example, Gratch, Chien, & DeJong, 1994, allocates error rationally).
Within scope paper, consider algorithms that: (i) combine pairwise
error selection error additively, (ii) combine selection error overall ranking error
additively, (iii) allocate error equally level.
One disadvantage recursive selection hypothesis selected,
removed pool candidate hypotheses. issue rare cases when,
sampling increase confidence later selection, estimate hypothesis'
mean changes enough previously selected hypothesis longer dominates it.
However, remains original hypotheses shown dominate others
specified level certainty, .
assumptions result following formulations (where (U1 fU2 ; :::; Uk g)
used denote error due action selecting hypothesis 1 Equation 1
set fH1 ; :::; Hk g (U1 fU2 ; :::; Uk g) denotes error due selection loss
situations Equation 2 applies):
rec (U1 > U2 > ::: > Uk ) =

rec (U2 > U3 > ::: > Uk )
+ (U1 fU2 ; :::; Uk g)

(6)

rec (Uk ) = 0 (the base case recursion) selection error defined
(Chien et al., 1995):
(U1 fU2 ; :::; Uk g) =

k
X

1;i

(7)

i=2

using Equation 4 compute pairwise confidence.

Algorithmically, implement following pseudo-code:

ensure n0 samples per hypothesis
distribute error individual selections
(stopping criteria met)
take samples
(means ordered differently ranking)
restart algorithm
analogous recursive selection algorithm based expected loss defined follows
ELrec (U1 > U2 > ::: > Uk ) =

ELrec (U2 > U3 > ::: > Uk )
+EL(U1 fU2 ; :::; Uk g)

(8)

ELrec(Uk ) = 0 selection EL defined (Chien et al., 1995):
EL(U1 fU2 ; :::; Uk g) =

k
X
i=2

7. Space constraints preclude description here.

381

EL(U1 ; Ui )

(9)

fiChien, Stechert, & Mutz

*



1,2

H1

2,3

H2

k-1,k

H3

Hk-1



Hk

Figure 2: Computing overall error adjacent ranking. Per-comparison errors neighboring hypotheses proposed ranking summed compared
required total error, .

2.2 Ranking Adjacency Comparison

Another interpretation ranking confidence (or loss) adjacent elements
ranking need compared. case, overall ranking error divided directly
k 1 pairwise comparison errors (Figure 2). leads following confidence equation
PAC criteria:
adj (U1 > U2 > ::: > Uk ) =

k 1
X

i;i+1

(10)

i=1

following equation EL criteria.
ELadj (U1 > U2 > ::: > Uk ) =

k 1
X

EL(Ui ; Ui+1 )

(11)

i=1

ranking comparison adjacent hypotheses establish dominance
loss bounds non-adjacent hypotheses (where hypotheses ordered
observed mean utility), advantage requiring fewer comparisons recursive
selection (and thus may require fewer samples recursive selection). However,
reason, adjacency algorithms may less likely recursive selection algorithms
bound probability correct ranking (or average loss) correctly. case
PAC algorithms, -dominance necessarily transitive. case
EL algorithms, expected loss necessarily additive considering two
hypothesis comparisons sharing common hypothesis.8
8. example ranking loss non-adjacent hypotheses exceeds desired loss bound
ranking, even though sum adjacent losses not, occurs blocked differential
distribution induced two non-adjacent hypotheses high variance relative hypothesis adjacent

382

fiEfficient Heuristic Hypothesis Ranking
2.3 Heuristic Nature Algorithms

recusrsive selection adjacency algorithms heuristic sense
proven statistically meet specified decision criteria (i.e., PAC criteria
select ranking satisfies equation (1) probability 1 similarly EL
criteria average ranking loss specified equation (3) less requested bound.
Indeed, several aspects algorithms make extremely dicult prove
would (probabilistically) achieve corresponding decision criteria. aspects include:



Sharing samples: order n1 samples differential distribution (i.e.



Heuristic error combination: recursive selection adjacency error com-



Ignorance lead switches multiple comparison paths: sampling pro-



blocking) H1 H2 , takes n1 samples H1 n1 samples
problems H2 . algorithms reduce sampling cost reusing
samples differential distributions comparing H1 hypotheses H2
hypotheses. makes errors derived samples independent.
Hence traded accuracy ease analysis algorithms heuristic
eciency. Particularly recursive selection approach, samples lowest
ranking hypothesis would used k 1 differential comparisons.
bination models heuristic means combining pairwise errors.
pairwise errors independent (see above). Empirically observed
pairwise errors tend overestimated error combination function
tends under-combine. Overall empirically combined error estimates tend
reasonably accurate, remaining sections show.
cess, ordering hypotheses may change (e.g., ordering sample means
may change). means implicitly, decision depended additional
pairwise comparison may ected final set comparisons contributing pairwise error. complexity could avoided fixing order
hypotheses n0 samples. However, would require samples would
involve showing -dominance hypothesis higher sample mean hypothesis
(indeed, may never converge). choose ignore complexity base
combined error used stopping condition final ordering.

Use non-normal distributions: many applications described re-

mainder article, real-world data distributed manner simlar
normal distributions (we investigate issue later article).
algorithms describe heuristic presume data normally
distributed even though case.

(i.e., currently ranked them). variance differential distribution makes
maximum contribution sample set small, so, e.g., 1 2 = 2, 1 2 = 2, n1 2 = 2,
2 3 = 2, 2 3 = 2, n2 3 = 2, exists configuration 1 3 = 4, 1 3 = 8.
expected losses EL(H1 ; H2 ) = 2:05, EL(H2 ; H3 ) = 2:05, EL(H1 ; H3 ) = 4:80 > 4:10.

383

fiChien, Stechert, & Mutz
2.4 Relevant Approaches

standard statistical ranking/selection approaches make strong assumptions
form problem (e.g., variances associated underlying utility distribution
hypotheses might assumed known equal). Among these, method Turnbull
Weiss (Turnbull & Weiss, 1984) comparable PAC-based approach.9
Turnbull Weiss' algorithm sequential interval-based procedure selecting
member population largest mean. treat hypotheses normally
distributed random variables unknown mean unknown possibly unequal
variance. algorithm also carries additional stipulation hypotheses
independent. procedure consists taking initial sample n0 observations
hypotheses taking samples sequentially according stopping criteria.
stopping criteria satisfied, hypothesis highest sample mean
2
chosen. stopping criteria inequality Snii n1 satisfied, Si
ni sample mean number samples ith hypothesis n chosen
2
according indifference
interval confidence level . particular, n = d2
R1
chosen satisfy 1 (F (y + d))k 1f (y)dy = F (y) f (y) cumulative
distribution function probability density function standard normal distribution.
still reasonable use approach candidate hypotheses
independent, excessive statistical error unnecessarily large training set sizes may result.
case hypotheses truly independent, Turnbull Weiss' technique
able exploit knowledge outperform methods adopt
assumption.
3. Empirical Performance Evaluation

turn empirical evaluation hypothesis ranking techniques synthetic
real-world datasets. evaluation serves three purposes. First, demonstrates
techniques perform predicted (in terms bounding probability incorrect selection expected loss). Second, validates performance techniques compared
standard algorithms statistical literature. Third, evaluation demonstrates
robustness new approaches real-world hypothesis ranking problems.
experimental trial consists solving hypothesis ranking problem given
technique given set problem control parameters. measure performance
(1) well algorithms satisfy respective criteria; (2) number
samples taken or, alternatively, cost (in seconds) executing algorithm. Since
performance statistical algorithms single trial provides little information
overall behavior, trial repeated multiple times results averaged
across trials. Synthetic experimental trials repeated 500 times, trials
real-world data repeated 100 times. PAC expected loss criteria
directly comparable, approaches analyzed separately.
9. PAC-based approaches investigated extensively statistical ranking selection literature topic confidence interval based algorithms (see Haseeb, 1985, review recent
literature).

384

fiEfficient Heuristic Hypothesis Ranking
Hk

H4

H3

H2

H1

-(k-1)

-3

-2

-



utility

Figure 3: stepped means hypothesis configuration.
3.1 Evaluation Synthetic Datasets

Evaluation synthetic data used show that: (1) techniques correctly bound probability incorrect ranking expected loss predicted underlying assumptions
valid even underlying utility distributions inherently hard rank 10 ,
(2) PAC techniques compare favorably algorithm Turnbull Weiss
wide variety circumstances.
synthetic datasets, utility distributions hypotheses modeled
random variables defined underlying parameterized distribution. Thus, characterizing ranking problem consists choosing number hypotheses rank
assigning values parameters representing utility distributions hypotheses. case, model utilities independent normal random variables
mean standard deviation. Thus, let k number hypotheses, hypothesis ranking problem described 2k parameters specifying expected utility
utility standard deviation hypothesis. general, several parameters may required characterize ranking problem fully11, number hypotheses
choices parameters utility distributions underlying hypotheses
characterize overall diculty ranking problem.
statistical ranking selection community uses standard family selection
problems known diculty analyze performance hypothesis selection strategies.
method, called least favorable configuration (LFC) population means
assignment parameters distributions likely cause technique
choose wrong hypothesis thus provides severe test technique's abilities.
configuration, utilities independent normally distributed variables equal
variance. k 1 hypotheses utilities equal expectation, , remaining
hypothesis expected utility + .
interested hypothesis ranking problems rather selection problems,
use generalization LFC call stepped means. configuration, one
hypotheses assigned expected utility successive hypotheses assigned
expected utility 1; :::; k 1 (Figure 3).
general, problems based least favorable configuration become dicult
(i.e., require samples) number hypotheses k increases, common utility
variance 2 increases, difference means utility distributions decreases.
standard methodology, technique evaluated ability achieve confidence
10. Configurations contain hypotheses high variance relative separation means
dicult rank.
11. instance, samples allocated rationally (Chien et al., 1995), becomes necessary assign
parameters cost distribution well, candidate hypotheses ranked,
number hypotheses rank would another problem parameter.

385

fiChien, Stechert, & Mutz
correct selection using several settings k . last ratio combines
single quantity which, increases, makes problem dicult. methodology
extends stepped means directly.
hypothesis ranking strategies algorithm control parameters
govern attack problem. PAC techniques three control parameters:
initial sample size n0 , desired confidence correct ranking indifference setting
12 . expected loss techniques two control parameters: initial sample size n0
loss threshold H .
observed number samples required achieved accuracy PAC techniques
stepped means configuration shown Table 3.1. results indicate
systems roughly comparable number examples required choose hypotheses.
expected, number examples increases k, , . P ACadj algorithm
required least number samples inconsistent meeting desired accuracy
bound (as indicated failure meet prescribed error bound several cases).
interesting Turnbull Weiss method significantly outperform PAC
techniques despite fact algorithm assumes hypotheses independent
(as case stepped means configuration), PAC approaches make
assumption. comparison, principal performance metric number
samples required achieve requested ranking, methods effective achieving
requested accuracy.
expected loss experiments, ran expected loss hypothesis ranking algorithms
stepped means configurations described range expected loss
bounds. Table 3.1 shows results experiment, displaying number samples
required produce ranking average observed loss configuration.
results show ELrec algorithm correctly bounded loss ELadj algorithm required less samples ELrec algorithm, correctly bound
expected loss (since observed loss greater loss bound H .13
3.2 Evaluation Real Datasets

test real-world applicability based data drawn several datasets relating
spacecraft design processing science data gathered context planetary
exploration. first two datasets investigate relate spacecraft design optimization
problems hypotheses wish rank candidate solutions design
problem. third last dataset examine involves ranking various lossless image
compression approaches based performance large set terrestrial images collected spacecraft Galileo. Cost evaluation given seconds empirical data
12. Note formulation stepped means test PAC approaches, difference
expected mean successive hypotheses indifference interval algorithm. Thus,
plays roles problem parameter control parameter here.
13. One confusing point identical hypothesis ranking algorithm settings, one observe
lower loss ranking larger number hypotheses. algorithm first divides
loss number pirwise comparisons. Thus, overall error (or expected loss bound),
hypotheses, pairwise expected error (or loss) smaller hypotheses.
ranking loss defined previously. Thus, possible observed loss increase decrease
compared settings fewer hypotheses.

386

fiEfficient Heuristic Hypothesis Ranking

k
3
3
3
3
3
3
5
5
5
5
5
5
10
10
10
10
10
10



0.75
0.75
0.90
0.90
0.95
0.95
0.75
0.75
0.90
0.90
0.95
0.95
0.75
0.75
0.90
0.90
0.95
0.95




2
3
2
3
2
3
2
3
2
3
2
3
2
3
2
3
2
3

TURNBULL
62 (0.88)
117 (0.89)
97 (0.96)
183 (0.99)
130 (0.97)
231 (0.96)
177 (0.83)
321 (0.95)
245 (0.98)
445 (0.98)
299 (0.98)
541 (0.98)
558 (0.92)
1,015 (0.94)
700 (0.97)
1,254 (0.97)
821 (1.00)
1,462 (0.99)

P ACrec

55 (0.95)
101 (0.86)
86 (0.94)
152 (0.96)
122 (0.97)
204 (0.95)
165 (0.95)
314 (0.93)
245 (0.97)
409 (0.91)
294 (0.98)
538 (0.98)
624 (0.91)
1,042 (0.95)
742 (0.96)
1,359 (0.97)
877 (0.97)
1,569 (0.98)

P ACadj

38 (0.78)
49 (0.80)
58 (0.92)
96 (0.89)
89 (0.97)
146 (0.94)
105 (0.87)
161 (0.75)
163 (0.91)
290 (0.92)
216 (1.00)
377 (0.92)
345 (0.85)
635 (0.83)
523 (0.91)
883 (0.90)
661 (0.94)
1,164 (0.93)

Table 1: Estimated expected total number observations PAC algorithms
stepped means configuration. Achieved probability correct ranking shown
parenthesis.

Parameters
k H
3 2 1.0
3 2 0.75
3 2 0.5
3 2 0.25
5 2 1.0
5 2 0.75
5 2 0.5
5 2 0.25
10 2 1.0
10 2 0.75
10 2 0.5
10 2 0.25

ELrec

Samples
96
102
139
235
320
343
464
575
1,136
1,325
1,533
1,856

Loss
0.6
0.5
0.2
0.1
0.7
0.4
0.4
0.2
0.5
0.5
0.3
0.1

ELadj

Samples
43
56
73
139
140
169
247
350
572
668
872
1,153

Loss
1.2
1.0
0.6
0.4
1.3
1.2
0.7
0.5
1.4
1.1
0.7
0.4

Table 2: Estimated expected total number observations EL algorithms stepped
means configuration. Observed average loss produced rankings.

387

fiChien, Stechert, & Mutz
because, unlike synthetic problems, cost sampling hypothesis constant
domains. Table 3 gives summary three ranking problems considered.
Dataset
DS-2 Penetrator

fixed parameters
penetrator diameter
penetrator length

DS-2 Aeroshell

fore body overlap
nose cone angle
bluntness ratio
fillet radius
outer diameter
tail geometry
compression method

Lossless Image Comp.

random variables
impact orientation
impact velocity
soil density
stagnation pressure coef.

optimization criteria
maximize penetration probability
maximize penetration depth

randomly selected test image

maximize compression ratio

minimize weight
achieve target entry velocity

Table 3: Description datasets used algorithm evaluation.

3.2.1 DS-2 Penetrator

goal New Millennium Deep Space Two (DS-2) mission deliver pair
microprobes planet Mars scientific study Martian soil. probes
released orbit, travel Martian atmosphere, embed
soil near southern polar ice cap. primary science objectives mission
(Balacuit., 1997):





determine ice present surface Mars,
measure local atmospheric pressure,
characterize thermal properties Martian subsurface soil.

goal spacecraft design problem determine good set physical dimensions penetrator|a small, robust probe designed impact surface extremely
high velocity operate extreme cold. Specifically, use design simulation
data DS-2 mission penetrator design.
casting design problem, hold shape penetrator constant
generate design candidates based different values variables penetrator diameter
length. specific design sample taken acquiring impact orientation, impact
velocity, soil density parameterized multivariate distribution calling
complex physical simulation determine depth penetrator bored
Martian surface. goal penetrator design problem determine physical
dimensions penetrator maximize probability penetration, cases
penetration, maximize penetration depth.
Tables 4 5 show results applying PAC-based, Turnbull, expected loss
algorithms ranking problem system requested rank 10 penetrator
designs.14 problem utility function depth penetration penetrator,
14. \True" expected utility values computed performing 20,000 samples using sample mean
large sample ground truth. expected utilities used compute PAC -validity
rankings observed loss using provided definitions.

388

fiEfficient Heuristic Hypothesis Ranking
cases penetrator penetrate assigned zero utility.
shown Table 4, PAC algorithms significantly outperformed Turnbull algorithm,
expected hypotheses somewhat correlated (via impact orientations soil densities). Table 5 shows ELrec expected loss algorithm effectively
bounded actual loss ELadj algorithm inconsistent.
k
10
10
10



0.75
0.90
0.95




2
2
2

TURNBULL
534 (0.96)
667 (0.98)
793 (0.99)

P ACrec

144 (1.00)
160 (1.00)
177 (1.00)

P ACadj

92 (0.98)
98 (1.00)
103 (0.99)

Table 4: Estimated expected total number observations rank DS-2 spacecraft designs.
Achieved probability correct ranking shown parenthesis.

Parameters
k
H
10
0.10
10
0.05
10
0.02

ELrec

Samples
152
200
378

Loss
0.05
0.03
0.03

ELadj

Samples
77
90
139

Loss
0.14
0.06
0.03

Table 5: Estimated expected total number observations expected loss incorrect
ranking DS-2 penetrator designs.

3.2.2 DS-2 Aeroshell Design Ranking

objective problem design aeroshell soil penetrator described
previous section gives appropriate entry velocity minimum weight. Design
candidates defined six continuous variables represent various geometric quantities: extent fore body overlaps aftbody, nose cone angle, bluntness
ratio, fillet radius, outer diameter, tail geometry. Candidate designs (hypotheses)
evaluated running simple physical simulation aeroshell's behavior.
sample taken running simulation fixed design variables hypothesis
value stagnation pressure coecient taken normal distribution.
simulation computes values achieved entry velocity mass aeroshell;
weighted sum reciprocals values maximized.
give results ranking three, five, ten hypotheses using Turnbull, PAC,
expected loss algorithms Tables 6 7.15
previous experiment, PAC-based algorithms outperformed Turnbull
algorithm cases. P ACadj algorithm represents significant increase
15. Again, deep sampling (500 samples) performed obtain \correct" ranking,
algorithms compared.

389

fiChien, Stechert, & Mutz
performance here, note achieve desired level confidence cases;
Turnbull P ACrec algorithms achieve required confidence.

k
3
3
3
3
3
3
5
5
5
5
5
5
10
10
10
10
10
10



0.75
0.75
0.90
0.90
0.95
0.95
0.75
0.75
0.90
0.90
0.95
0.95
0.75
0.75
0.90
0.90
0.95
0.95




2
3
2
3
2
3
2
3
2
3
2
3
2
3
2
3
2
3

TURNBULL
8.9 (1.00)
22.9 (1.00)
17.1 (1.00)
38.2 (1.00)
22.6 (1.00)
52.0 (1.00)
29.1 (0.92)
69.0 (1.00)
42.4 (0.99)
94.7 (0.99)
51.9 (0.98)
117.9 (0.99)
84.0 (0.99)
196.6 (1.00)
112.0 (0.98)
252.9 (0.99)
129.1 (1.00)
315.7 (1.00)

P ACrec

8.4 (1.00)
11.3 (1.00)
14.0 (1.00)
18.6 (1.00)
21.6 (1.00)
32.1 (1.00)
20.1 (0.94)
33.9 (0.96)
30.0 (0.93)
54.8 (0.96)
43.6 (1.00)
81.5 (0.99)
42.0 (0.94)
57.9 (0.96)
53.8 (0.98)
85.5 (1.00)
61.3 (0.97)
125.7 (1.00)

P ACadj

3.5 (1.00)
3.8 (1.00)
7.1 (1.00)
7.2 (1.00)
7.1 (1.00)
7.3 (1.00)
11.8 (0.91)
11.7 (0.91)
11.7 (0.91)
11.8 (0.84)
11.7 (0.91)
11.5 (0.92)
22.1 (0.92)
22.1 (0.90)
22.6 (0.89)
21.6 (0.91)
20.6 (0.90)
20.4 (0.92)

Table 6: Estimated expected cost (in seconds) rank aeroshell designs. Achieved probability correct ranking shown parenthesis.

Parameters
k
H
3
20
3
30
3
40
5
20
5
30
5
40
10
20
10
30
10
40

ELrec

Execution Cost
9.5
7.6
7.3
21.7
18.1
15.0
55.3
42.6
38.2

Loss
4.3
3.4
4.1
7.0
12.0
9.3
9.7
8.9
10.4

ELadj

Execution Cost
7.9
7.3
6.9
7.2
6.4
10.5
18.3
14.2
13.1

Loss
3.4
3.7
2.7
8.6
12.4
8.5
7.9
9.8
9.6

Table 7: Estimated expected cost (in seconds) expected loss incorrect ranking
DS-2 aeroshell designs.

390

fiEfficient Heuristic Hypothesis Ranking
3.2.3 Lossless Image Compression Galileo Image Data

problem utilizes large set raw image data acquired Galileo spacecraft.
images 256 256 size made greyscale pixels ranging 0 255
intensity. goal select lossless compression method16 performs best
class images. performance image compression algorithm particular image
could measured number ways. example, execution time, compression ratio,
image quality (in case lossy compression methods considered) could
define algorithm performance. tests chose consider compression ratio
achieved given compression method utility function. sample method
(hypothesis), image randomly selected, method applied image,
achieved compression ratio recorded.
Given (Tables 8 9) results ranking three, five, seven hypotheses
using Turnbull, PAC, expected loss algorithms. Ranking correctness determined
comparison \correct" ranking established sampling compression method
set 1500 distinct images.
note substantial performance improvement PAC-based algorithms
Turnbull algorithm. Although Turnbull algorithm PAC
algorithms (Table 8) achieved desired confidence level, adjacent version EL
algorithm (Table 9) failed bound loss specified level half cases.
interesting consider results presented section light fact
statistical techniques used makes form normality assumption.
fact, three problem domains investigate number hypotheses whose
utility functions normally distributed. past experience known utility
functions DS-2 Penetrator domain (Section 3.2.1) highly non-normal; Figure 4
illustrates difference data normally distributed data not.
0.08

0.12

0.07
0.1
0.06
0.08
0.05

0.04

0.06

0.03
0.04
0.02
0.02
0.01

0
0.2

0
0.3

0.4

0.5

0.6

0.7

0.8

0

50

100

150

200

250

300

350

400

450

500

Figure 4: comparison (a) data normally distributed high likelihood (b)
data likely normally distributed. case, histogram
experimental data shown solid boxes; data drawn normal distribution
mean standard deviation shown dashed lines.
determine extent utilities hypotheses remaining two domains normally distributed applied Kolmogorov-Smirnov test (see Appendix
16. seven compression methods considered were: CALIC, lossless JPEG, GIF, TIFF, pack, gzip,
compress.

391

fiChien, Stechert, & Mutz

k
3
3
3
3
3
3
5
5
5
5
5
5
7
7
7
7
7
7



0.90
0.90
0.95
0.95
0.99
0.99
0.90
0.90
0.95
0.95
0.99
0.99
0.90
0.90
0.95
0.95
0.99
0.99




2
3
2
3
2
3
2
3
2
3
2
3
2
3
2
3
2
3

TURNBULL
62.8 (1.00)
150.8 (1.00)
84.6 (1.00)
206.8 (1.00)
142.0 (1.00)
359.4 (1.00)
134.7 (1.00)
329.9 (1.00)
176.1 (1.00)
399.8 (1.00)
249.3 (1.00)
598.1 (1.00)
210.8 (1.00)
499.3 (1.00)
250.3 (1.00)
608.7 (1.00)
339.6 (1.00)
813.7 (1.00)

P ACrec

30.1 (1.00)
30.5 (1.00)
28.6 (1.00)
29.0 (1.00)
30.1 (1.00)
30.6 (1.00)
39.5 (1.00)
39.9 (1.00)
39.3 (1.00)
39.3 (1.00)
39.2 (1.00)
39.2 (1.00)
35.6 (1.00)
35.7 (1.00)
37.4 (1.00)
36.0 (1.00)
36.5 (1.00)
37.2 (1.00)

P ACadj

14.8 (1.00)
14.8 (1.00)
15.0 (1.00)
20.5 (1.00)
23.3 (1.00)
23.2 (1.00)
29.9 (1.00)
30.0 (1.00)
29.8 (1.00)
29.6 (1.00)
29.9 (1.00)
30.7 (1.00)
37.2 (1.00)
34.5 (1.00)
35.6 (1.00)
35.0 (1.00)
34.5 (1.00)
35.3 (1.00)

Table 8: Estimated expected cost (in seconds) rank lossless image compression approaches Galileo image data. Achieved probability correct ranking shown
parenthesis.

Parameters
k
H
3
10
3
5
3
1
5
10
5
5
5
1
7
10
7
5
7
1

ELrec

Execution Cost
31.7
32.5
33.7
80.6
83.5
101.0
99.5
105.7
119.8

Loss
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

ELadj

Execution Cost
24.9
24.9
24.9
32.7
33.7
32.3
42.3
33.3
30.4

Loss
0.0
0.0
0.0
17.4
69.4
49.4
17.4
34.7
86.8

Table 9: Estimated expected cost (in seconds) expected loss incorrect ranking
DS-2 penetrator designs.

392

fiEfficient Heuristic Hypothesis Ranking
details). test determined none ten hypotheses DS-2 Aeroshell
domain (Section 3.2.2) normally distributed utility. Additionally, two seven
hypotheses image compression domain (Section 3.2.3) shown greater
90% likelihood normally distributed utility functions17. reasons,
evaluating ranking strategies datasets provides particularly strong test
applicability techniques.
draw reader's attention particularly large disparity performance
Turnbull algorithm PAC-based algorithms image compression domain,
especially apparent number hypotheses, confidence level, high.
Additionally, problem domain two hypotheses normally distributed utility
five non-normal. observations suggest PAC-based algorithms
perform better (in relative terms) faced domain violates assumption
normality.
4. Discussion Conclusions

number areas related work. First, considerable analysis
hypothesis selection problems. Selection problems formalized using Bayesian
framework (Moore & Lee, 1994; Rivest & Sloan, 1988) require initial
sample, uses rigorous encoding prior knowledge. Howard (Howard, 1970) also
details Bayesian framework analyzing learning cost selection problems. one
uses hypothesis selection framework ranking, allocation pairwise errors
performed rationally (Gratch et al., 1994). Reinforcement learning work (Kaelbling, 1993)
immediate feedback also viewed hypothesis selection problem.
framework presented invites future work number directions. Currently,
stopping criteria used relaxations ranking requirement. Another approach
could used bound resources available ranking. Limiting number
samples sample cost high limiting time computation (so
anytime algorithm) two straightforward application areas.
Another area future work discovery composite strategies hypotheses. Thus
far examined ranking (and articles, selection) hypothesis highest expected value entire distribution. example, learning scheduling control
strategy well distribution problems. However, likely
distributions problems, exists composite strategy would outperform
single strategy. example, single strategy might apply method solve
problem. composite strategy would be, test problem feature X, X true apply method A, else apply method B. composite strategies correspond algorithm
portfolios named Operations Research. Indeed results applying methods could
also viewed strategies. One might composite strategy trying method
10 CPU seconds, fails trying method B. course, composition portfolio approaches, diculty iseciently proposing evaluating plausible
compositions. even small set base strategies number copositions enormous.
17. reference, data Figure 4 (a) normally distributed 97.5% likelihood, according
Kolmogorov-Smirnov test.

393

fiChien, Stechert, & Mutz
summary, paper described hypothesis ranking problem, extension
hypothesis selection problem. defined application two decision criteria, probably approximately correct expected loss, problem. defined two families
algorithms, recursive selection adjacency, solution hypothesis ranking problems.
Finally, demonstrated effectiveness algorithms synthetic realworld datasets, documenting improved performance existing statistical approaches.
Acknowledgments

work performed Jet Propulsion Laboratory, California Institute Technology, contract National Aeronautics Space Administration.
Appendix A. Applying K-S Test Real Datasets

Kolmogorov-Smirnov Test statistical means accepting, certain level
confidence, hypothesis sampleset fits parametric distribution given
set parameters. method compares CDF generated empirical distribution
corresponding parametric distribution (i.e., estimated parameters).
K-S test gives confidence based maximum, D, discrepancies
two CDFs:
= maxjF1 (x)

F2 (x)j

purposes wish determine, hypothesis given domain, whether
values utility function normally distributed not. case, half
utility samples taken used compute mean standard deviation normal;
remaining half used compute CDF.
A.1 DS-2 Penetrator

20000 samples taken.
design number
1
2
3
4
5
6
7
8
9
10

maxjF1 (x)

F2 (x)j
0.1415
0.1202
0.1020
0.1261
0.1207
0.1261
0.1020
0.1493
0.1461
0.1261

394

normally distributed?
90% likely
90% likely
90% likely
90% likely
90% likely
90% likely
90% likely
90% likely
90% likely
90% likely

fiEfficient Heuristic Hypothesis Ranking
A.2 DS-2 Aeroshell Design Ranking

500 samples taken.
design number
1
2
3
4
5
6
7
8
9
10

maxjF1 (x)

F2 (x)j

0.08
0.08
0.08
0.08
0.08
0.08
0.08
0.08
0.08
0.08

normally distributed?
< 90% likely
< 90% likely
< 90% likely
< 90% likely
< 90% likely
< 90% likely
< 90% likely
< 90% likely
< 90% likely
< 90% likely

A.3 Lossless Image Compression Galileo Image Data

200 samples taken.
compression method
gif
compress
calic
gzip
jpegls
pack
tiff

maxjF1 (x)

F2 (x)j

0.10
0.14
0.19
0.09
0.18
0.12
0.11

normally distributed?
90% likely
< 90% likely
90% likely
97.5% likely
90% likely
< 90% likely
< 90% likely

References

Balacuit., C. P. (1997). Deep Space 2 { Mars Microprobe Home Page (mission objectives
statement). Tech. rep. http://nmp.jpl.nasa.gov/ds2, NASA/JPL.
Chien, S. A., Gratch, J. M., & Burl, M. C. (1995). Ecient Allocation Resources
Hypothesis Evaluation: Statistical Approach. IEEE Trans. Pattern Analysis
Machine Intelligence, 17 (7), 652{665.
Chien, S. A., Stechert, A. D., & Mutz, D. H. (1998). Ecient Heuristic Ranking Hypotheses. Advances Neural Information Processing Systems 10 (Jordan, Kearns,
Solla eds.), pp. 444{450 Denver, Colorado. NIPS.
Ginsberg, M., & Harvey, W. (1992). Iterative Broadening. Artificial Intelligence Journal,
55, 367{383.
395

fiChien, Stechert, & Mutz
Goldberg, D. (1989). Genetic Algorithms Search, Optimization, Machine Learning.
Addison-Wesley.
Gratch, J. (1992). COMPOSER: Probabilistic Solution Utility Problem Speed-up
Learning. Proceedings Tenth National Conference Artificial Intelligence,
pp. 235{240 San Jose, CA. AAAI.
Gratch, J. (1993). COMPOSER: Decision-theoretic Approach Adaptive Problem Solving. Tech. rep. UIUCDCS-R-93-1806, Department Computer Science, University
Illinois.
Gratch, J., Chien, S., & DeJong, G. (1994). Improving Learning Performance
Rational Resource Allocation. Proceedings Twelfth National Conference
Artificial Intelligence, pp. 576{582 Seattle, WA. AAAI.
Greiner, R., & Jurisica, I. (1992). Statistical Approach Solving EBL Utility
Problem. Proceedings Tenth National Conference Artificial Intelligence,
pp. 241{248 San Jose, CA. AAAI.
Haseeb, R. M. (1985). Modern Statistical Selection. American Sciences Press, Columbus,
OH.
Howard, R. A. (1970). Decision Analysis: Perspectives Inference, Decision, Experimentation. Proceedings IEEE, 58 (5), 823{834.
Kaelbling, L. P. (1993). Learning Embedded Systems. MIT Press, Cambridge, MA.
Minton, S. (1988). Learning Search Control Knowledge: Explanation-Based Approach.
Kluwer Academic Publishers, Norwell, MA.
Moore, A. W., & Lee, M. S. (1994). Ecient Algorithms Minimizing Cross-Validation
Error. Proceedings International Conference Machine Learning New
Brunswick, MA.
Musick, R., Catlett, J., & Russell, S. (1993). Decision Theoretic Subsampling Induction Large Databases. Proceedings International Conference Machine
Learning, pp. 212{219 Amherst, MA.
Rivest, R. L., & Sloan, R. (1988). New Model Inductive Inference. Proceedings
Second Conference Theoretical Aspects Reasoning Knowledge.
Russell, S., & Wefald, E. (1992). Right Thing: Studies Limited Rationality. MIT
Press, Cambridge, MA.
Tadepalli, P. (1992). theory unsupervised speedup learning. Proc. Tenth
National Conference Artificial Intelligence, pp. 229{234 San Jose, CA. AAAI.
Turnbull, B. W., & Weiss, L. I. (1984). Class Sequential Procedures k-sample
Problems Concerning Normal Means Unknown Equal Variances. Santner,
T. J., & Tamhane, A. C. (Eds.), Design Experiments: Ranking Selection, pp.
225{240. Marcel Dekker.
396

fiEfficient Heuristic Hypothesis Ranking
Valiant, L. G. (1984). Theory Learnable. Communications ACM, 27,
1134{1142.

397

fiJournal Artificial Intelligence Research 10 (1999) 1-38

Submitted 4/98; published 1/99

Order Magnitude Comparisons Distance
Ernest Davis

davise@cs.nyu.edu

Courant Institute
New York, NY 10012 USA

Abstract
Order magnitude reasoning | reasoning rough comparisons sizes quantities | often called \back envelope calculation", implication
calculations quick though approximate. paper exhibits interesting class constraint sets order magnitude reasoning demonstrably fast. Specifically,
present polynomial-time algorithm solve set constraints form \Points
b much closer together points c d." prove algorithm
applied \much closer together" interpreted either referring infinite difference
scale referring finite difference scale, long difference scale greater
number variables constraint set. also prove first-order theory
constraints decidable.
1. Introduction
Order magnitude reasoning | reasoning rough comparisons sizes quantities |
often called \back envelope calculation", implication calculations
quick though approximate. Previous AI work order magnitude reasoning, however,
focussed expressive power inferential structure, computational
leverage (Raiman, 1990; Mavrovouniotis Stephanopoulos, 1990; Davis, 1990; Weld,
1990).
paper exhibit interesting case solving set order magnitude
comparisons demonstrably much faster solving analogous set simple order
comparisons. Specifically, given set constraints form \Points b much
closer together points c d," consistency set determined
low-order polynomial time. contrast, easily shown solving set constraints
form \The distance b less equal distance c d"
one dimension NP-complete, higher dimensions hard solving arbitrary
set algebraic constraints reals.
particular, paper presents following results:
1. algorithm \solve constraints(S )" solves system constraints form \Points
b infinitely closer points c d" polynomial time (Section 5).
2. improved version algorithm runs time O(max(n2 ff(n)); ne; s) n
number variables, ff(n) inverse Ackermann's function, e number
edges mentioned constraint set, size constraint set. (Section
6.1).
3. extended version algorithm allows inclusion non-strict constraints
form \Points b infinitely apart points c d."
c 1999 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiDavis

running time modified algorithm slower solve constraints,
still polynomial time. (Section 6.2)
4. different extension algorithm allows combination order magnitude
constraints distances order comparisons points form \Point
precedes point b." (Section 6.3)
5. algorithm applied constraints form \The distance
b less 1=B times distance c d," B given finite value,
long B greater number variables constraint set. (Section 7)
6. first-order theory constraints decidable. (Section 8)
preliminary steps, begin small example informal discussion (Section
2). give formal account order-of-magnitude spaces (Section 3) present
data structure called cluster tree, expresses order-of-magnitude distance comparisons (Section 4). conclude paper discussion significance
results (Section 9).

2. Examples
Consider following inferences:

Example 1: wish buy house rent oce space suburb Metropolis.
obvious reasons, want house close school, house close
oce, oce close commuter train station. told Elmville
train station quite far school, Newton close together.
Infer able satisfy constraints Elmville, may able
Newton.
Example 2: Empire State Building much closer Washington Monument
Versailles. Statue Liberty much closer Empire State Building
Carnegie Hall Washington Monument.
Infer Carnegie Hall much closer Empire State Building Versailles.
Example 3: carry collection computational tasks covering wide
range diculty. instance
a.
b.
c.
d.
e.
f.

Add column 100 numbers.
Sort list 10,000 elements.
Invert 100 100 matrix.
Invert 1000 1000 matrix.
Given O.D.E. x = cos(et x), x(0) = 0, find x(20) 32-bit accuracy.
Given online collection 1,000 photographs GIF format, use state-ofthe-art image recognition software select show man
horseback.

2

fiOrder Magnitude Comparisons Distance

g. Web search collected 100 pictures men horseback,
using state-of-the-art image recognition software.
h. Using state-of-the-art theorem proving software, find proof medians triangle concurrent.
i. Using state-of-the-art theorem proving software, find proof Fermat's
little theorem.
plausible suppose that, many cases, say reliably one
task take much longer another, either human judgment using expert
system. instance, task (a) much shorter others. Task (b) much
shorter others except (a) possibly (h). Task (c) certainly much
shorter (d), (f), (g), (i). However, certain pairs (c) (h) (c)
(e) would dicult guess whether one much shorter another, whether
comparable diculty.
number independent identical computers, unknown vintage characteristics, schedule tasks kinds. Note that, circumstances, way predict absolute time required tasks within
couple orders magnitude. Nonetheless, comparative lengths presumably still stand.
Given: particular schedule tasks machines, infer relative
order completion times. example, given following schedule
Machine M1: tasks a,b,h,d.
Machine M2: tasks c,i.
possible predict (a) (b) complete (c); (c)
complete (d); (d) complete (i); possible
predict order (c) (h) complete.
three examples, given information form \The distance points
W X much less distance Z ". examples 1 2,
points geometric. example 3, points start completion times
various tasks, constraints relative lengths put form \The distance
start(a) end(a) much less distance start(c) end(c)", on.
example 3, also ordering information: start task precedes end;
end (a) equal start (b); on. problem make inferences based
weak kind constraint.
noted examples meant illustrative, rather serious applications. Example 1 extend obvious way class natural,
large problems. Example 2 implausible state knowledge; reasoner
find knowing order-of-magnitude relations among distances
geometric information? Example 3 contrived. Nonetheless, illustrate kinds
situations order-of-magnitude relations distance arise; express
substantial part knowledge reasoner; inferences based purely
order-of-magnitude comparisons yield useful conclusions.
methods presented paper involve construing relation \Distance much
shorter distance E" \Distance infinitesimal compared distance
E." shall see, interpretation, systems constraints distances

3

fiDavis

solved eciently. logical foundations dealing infinitesimal quantities lie
non-standard model real line infinitesimals, developed Abraham Robinson
(1965). (A readable account given Keisler, 1976.) Reasoning quantities
infinitely different scale known \order magnitude" reasoning.
reader may ask, \Since infinitesimals physical reality, value
developing techniques reasoning them?" none examples, all,
smaller quantity truly infinitesimal larger one truly infinite. example 1
2, ratio successive sizes somewhere 10 100; example 3,
100 rather large number dicult estimate; one always give
kind upper bound. essentially certain, instance, ratio
times required tasks (a) (i) less 10100;000 . use best real-valued
estimate instead?
first answer idealization. Practically physical reasoning
calculation rest one idealization another: idealization situation calculus
time discrete; idealization solid objects rigid, employed mechanics programs; idealization physical properties density, temperature,
pressure continuous rather local averages atoms, underlies uses
partial differential equations; idealization involved use Dirac delta function;
on. idealization short distance infinitesimally smaller
long one simplifies reasoning yields useful results long care taken stay within
appropriate range application.
second answer technique mathematical approximation,
using turn intractable problem tractable one. would analogous
linearizing non-linear equation small neighborhood; approximating sum
integral.
circumstances sure approximation gives answer
guaranteed exactly correct; namely actual ratio implicit comparison
\D much smaller E " larger number points involved system
constraints. proven Section 7. also broader, less well-defined, class
problems approximation, though guaranteed correct, reliable
links reasoning. instance, suppose one consider
instance example 3 involving couple hundred tasks, apply order-of-magnitude
reasoning, come answer determined wrong. possible
error would due order-of-magnitude reasoning. However, seems safe
say that, cases, error likely due mistake estimating
comparative sizes.

3. Order-of-magnitude spaces
order-of-magnitude space, om-space, space geometric points. two points
separated distance. Two distances e compared relation e,
meaning \Distance infinitesimal compared e" or, loosely, \Distance much
smaller e."
example, let < non-standard real line infinitesimals. Let <m
corresponding m-dimensional space. let point om-space point

4

fiOrder Magnitude Comparisons Distance

Rm . distance two points a; b Euclidean distance, non-negative
value < . relation e holds two distances d; e, d=e infinitesimal.
distance operator comparator related number axioms, specified
below. interesting called om-triangle inequality: ab bc
much smaller xy, ac much smaller xy. combines ordinary
triangle inequality \The distance ac less equal distance ab plus distance bc"
together rule order-of-magnitude algebra, \If p r q r p+q r."
simplify exposition if, rather talking distances, talk
orders magnitude. defined follows. say two distances e
order magnitude neither e e d. < condition d=e
finite: neither infinitesimal infinite. (Raiman, 1990 uses notation \d Co e"
relation.) rules order-of-magnitude calculus, equivalence relation.
Hence define order magnitude equivalence class distances
relation \same order magnitude". two points a; b, define function od(a; b)
order magnitude distance b. two orders magnitude p; q,
define p q if, representatives 2 p e 2 q, e. rules orderof-magnitude calculus, holds representatives, holds representatives.
advantage using orders-of-magnitude function \od", rather distances
distance function, allows us deal logical equality rather
equivalence relation \same order magnitude".
example, non-standard real line, let positive infinitesimal value.
values f1; 100; 2 50 + 1002 : : :g, order magnitude, o1.
values f; 1:001; 3 + e 1= : : :g different order magnitude o2 o1. values
f1=; 10= + 5 : : :g third order magnitude o3 o1.
Definition 1: order-of-magnitude space (om-space)
consists of:







set points P ;
set orders magnitude D;
distinguished value 0 2 D;
function \od(a; b)" mapping two points a; b 2 P order magnitude;
relation \d e" two orders magnitude d; e 2

satisfying following axioms:
A.1 orders magnitude d; e
e d, = e.

2 D, exactly one following holds: e,

A.2 d; e; f 2 D, e e f f .
(Transitivity. Together A.1, means total ordering orders
magnitude.)
A.3 2 D, 0.
(0 minimal order magnitude.)

5

fiDavis

A.4 points a; b 2 P , od(a; b) = 0 = b.
(The function od positive definite.)
A.5 points a; b 2 P , od(a; b) = od(b; a).
(The function od symmetric.)
A.6 points a; b; c 2 P , order magnitude 2 D,
od(a; b) od(b; c) od(a; c) d.
(The om-triangle inequality.)
A.7 infinitely many different orders magnitude.
A.8 point a1 2 P order magnitude
a2 ; a3 : : : od(ai ; aj ) = 6= j .

2 D, exists infinite set

example given om-space, non-standard Euclidean space, wild
woolly hard conceptualize. two simpler examples om-spaces:
I. Let infinitesimal value. define point polynomial integer
coecients, 3 + 5 85 . define order-of-magnitude power .
define n > n; example, 6 4 . define od(a; b) smallest power
b. example, od(1 + 2 33 ; 1 52 + 44 ) = 2 .
II. Let N infinite value. define point polynomial N integer
coecients. define order magnitude power N . define N p N q
p < q; example, N 4 N 6 . define od(a; b) largest power N b.
example, od(1 + N 2 3N 3 ; 1 5N 2 + 4N 4 ) = N 4 .
shown om-space either contains subset isomorphic (I) subset
isomorphic (II). (This special case general rule infinite total
ordering contains either infinite descending chain infinite ascending chain.)
use notation \de" abbreviation \d e = e".

4. Cluster Trees
Let P finite set points om-space. distances different pairs
points P different orders magnitude, om-space imposes unique treelike hierarchical structure P . points naturally fall clusters, cluster C
collection points much closer one another point
P outside C . collection clusters P forms strict tree subset
relation. Moreover, structure tree comparative sizes different clusters
tree captures order-of-magnitude relations pair points P .
tree clusters thus powerful data structure reasoning points
om-space, is, indeed, central data structure algorithms develop
paper. section, give formal definition cluster trees prove basic
results foundations algorithms.

Definition 2: Let P finite set points om-space. non-empty subset C P
called cluster P every x; 2 C , z 2 P C , od(x; y) od(x; z ). C cluster,
diameter C , denoted \odiam(C )", maximum value od(x; y) x; 2 C .

6

fiOrder Magnitude Comparisons Distance

n1
5
n2

n3

4

3

n4

n5





0

3

0

0

e

g

f

b

c

0

0

0

0

0

Figure 1: Cluster tree
Note set single element P trivially cluster P . entire set P
likewise cluster P . empty set definition cluster P .

Lemma 1: C clusters P , either C
disjoint.

D, C , C

Proof: Suppose not. let x 2 C \ D, 2 C D, z 2 C . Since C cluster,
od(x; y) od(x; z ). Since cluster, od(x; z ) od(x; y). Thus contradiction.

2

virtue lemma 1, clusters set P form tree. develop representation order magnitude relations P constructing tree whose nodes correspond
clusters P , labelled indication relative size cluster.

Definition 3: cluster tree tree

Every leaf distinct symbol.
Every internal node least two children.
internal node labelled non-negative value. Two nodes
may given value. (For purposes Sections 5-7, labels may taken
non-negative integers; Section 8, useful allow rational labels.)

Every leaf tree labelled 0.
label every internal node tree less label parent.
node N , field \N .symbols" gives set symbols leaves
subtree rooted N , field \N .label" gives integer label node N .

7

fiDavis

Thus, example, Figure 1, n3.label=3 n3.symbols = fa; dg; n1.label = 5
n1.symbols = fa; b; c; d; e; f; gg.
shall see, nodes tree represent clusters set points,
labels represent relative sizes diameters clusters.

Definition 4: valuation set symbols function mapping symbol
point om-space. cluster tree, valuation valuation .symbols.
N node valuation , write (N ) abbreviation
(N .symbols).
define cluster tree expresses order magnitude relations
set points P .
Definition 5: Let cluster tree let valuation . Let P = (T ),
set points image . say j=T (read satisfies instantiates
) following conditions hold:
i. internal node N , (N ) cluster P .
ii. cluster C P , node N C = (N ).
iii. nodes N , .label < N .label odiam( (M )) odiam( (N )).
iv. label(M ) = 0, odiam(M ) = 0. (That is, children assigned
value .)
following algorithm generates instantiation
procedure

variable

given cluster tree :

instantiate(in : cluster tree;
: om-space)
return : array points indexed symbols

G[N ] : array points indexed nodes ;

Let k number internal nodes ;
Choose 0 = 0 1 2 : : : k k + 1 different orders magnitude;
/* values chosen virtue axiom A.7 */
pick point x 2
;
G[root ] := x;
instantiate1(T;
; 1 : : : k ; G);
return restriction G symbols .
end instantiate.
instantiate1(in N : node cluster tree;
: om-space; 1 : : : k : orders magnitude;
G : array points indexed nodes )
N leaf
let C1 : : : Cp children N ;
x1 := G[N ];
q := N .label;
pick points x2 : : : xp
i; j 2 1 : : : p, 6= j od(xi ; xj ) = q ;
/* points chosen virtue axiom A.8 */
8

fiOrder Magnitude Comparisons Distance



= 1 : : : p
G[Ci ] := xi ;

instantiate1(Ci ;
; 1 : : : k ; G);

endfor
endif end

instantiate1.

Thus, begin picking orders magnitude corresponding values labels.
pick arbitrary point root tree, recurse nodes
tree. node N , place children points lie separated desired
diameter N . final placement leaves desired instantiation.
Lemma 2: cluster tree
om-space, instantiate(T;
) returns
instantiation .
proof given appendix.
Moreover, clear instantiation generated possible output
instantiate(T;
). (Given instantiation , pick G[N ] stage
symbol N .)
Note that, given valuation finite set symbols , exists cluster
tree .symbols = satisfies . essentially unique
isomorphism set labels preserves label 0 order labels.

5. Constraints
section, develop first algorithms. Algorithm solve constraints tests
collection constraints form \a much closer b c d," consistency. set consistent, algorithm returns cluster tree satisfies
constraints. algorithm builds cluster tree top bottom dealing first
large distances, proceeding smaller smaller distances.
Let system constraints form od(a; b) od(c; d); let cluster
tree. say `S (read \T satisfies ") every instantiation satisfies .
section, develop algorithm finding cluster tree satisfies given set
constraints.
algorithm works along following lines: Suppose solution satisfying .
Let diameter solution. contains constraint od(a; b) od(c; d) then,
since od(c; d) certainly D, follows od(a; b) much smaller D.
label ab \short" edge.
two points u v connected path short edges, triangle
inequality edge uv also short (i.e. much shorter D). Thus, compute
connected components H edges labelled short, edges
H likewise labelled short. example, table 3, edges vz , wx, xy
labelled \short".
hand, shall prove below, edge set H ,
reason believe much shorter D. can, fact, safely posit
o.m. D. label edges \long".
assume connected component points connected short edges
cluster, child root cluster tree. root cluster tree
given largest label. children given smaller labels. \long" edge

9

fiDavis

connects symbols two different children root. Hence, instantiation tree
make long edge longer short edge.
edges labelled \long" | is, H contains complete graph
symbols | inconsistency; edges much shorter longest edge.
instance, table 4, since vw, wx, xy much smaller zy, follows
triangle inequality vy much smaller zy. since also
constraints zy much smaller vz vz much smaller vy,
inconsistency.
algorithm iterates, next smaller scale. Since taken care
constraints od(a; b) od(c; d), cd labelled \long", drop
. Let greatest length edges remain . constraint
od(a; b) od(c; d) new , know od(a; b) much shorter D,
label \short". continue above. algorithm halts constraints
satisfied, therefore empty; encounter contradiction,
above.
give formal statement algorithm. algorithm uses undirected
graph variable symbols . Given graph G, constraint C
form od(a; b) od(c; d), refer edge ab \short" C , edge
cd \long" C . shorts system set shorts constraints
longs set longs constraints. edge may
short long appears one side one constraint another
constraint.
procedure

type:

solve constraints(in : system constraints form od(a; b) od(c; d))
return either cluster tree satisfying consistent;
false inconsistent.

node N cluster tree contains
pointers parent children N ;
field N.label, holding integer label;
field N.symbols, holding list symbols leaves N .

variables:

begin

integer;
C constraint ;
H; undirected graphs;
N; nodes ;

contains constraint form, \od(a; b) od(c; c)" return false;

:= number variables ;
initialize consist single node N ;
N .symbols:= variables ;
repeat

H := connected components shorts ;
H contains edges return(false) endif;
leaf N
vertices N connected H
N .label := m;
connected component N .symbols H

10



fiOrder Magnitude Comparisons Distance

construct node new child N ;
.symbols:= vertices ;

endfor endif endfor

:= subset constraints whose long H ;
:= 1;



empty;

leaf N
N .label := 0;
N .symbols one symbol
create leaf N symbol N .symbols;
label leaf 0;
endif endfor end solve constraints.

Tables 3 4 give two examples working procedure solve constraints. Table
3 shows procedure used establish following constraints
consistent:
Empire State Building (x) much closer Washington Monument (w)
Notre Dame Cathedral (v).
Bunker Hill (y) much closer Empire State Building Eiffel
Tower (z ).
distance Eiffel Tower Notre Dame much less distance
Washington Monument Bunker Hill.
Table 4 shows following inference justified:

Given: distances Statue Liberty (v) World Trade Center
(w), World Trade Center Empire State Building (x),
Empire State Building Chrysler Building (y) much less
distance Chrysler Building Washington Monument (z ).
Infer: Washington Monument much nearer Chrysler Building
Statue Liberty.
inference carried asserting negation consequent, \The Washington Monument much nearer Chrysler Building Statue Liberty,"
showing collection constraints inconsistent. Note change \much
less" \much nearer" example \less" \nearer", inference longer
valid.
Theorem 1 states correctness algorithm solve constraints. proof given
appendix.
Theorem 1: algorithm solve constraints(S ) returns cluster tree satisfying
consistent, returns false inconsistent.
may many cluster trees satisfy given set constraints. Among these,
cluster tree returned algorithm solve constraints important property:
fewest possible labels consistent constraints. words, uses
minimum number different orders magnitude solution. Therefore, algorithm
used check satisfiability set constraints om-space violates

11

fiDavis

contains constraints
1. od(w; x) od(x; v).
2. od(x; y) od(y; z ).
3. od(v; z ) od(w; y).
algorithm proceeds follows:
Initialization:
tree initializes single node n1.
n1.symbols := f v; w; x; y; z g.
First iteration:
shorts f wx; xy; vz g.
Computing connected components, H set f wx; xy; wy; vz g.
n1.label := 5;
Two children n1 created:
n11.symbols := w; x; ;
n12.symbols := v; z ;
xv H , delete constraint #1 .
yz H , delete constraint #2 .
contains constraint #3.
Second iteration:
shorts f vz g.
connected components H fvz g.
n11.label := 4;
Three children n11 created:
n111.symbols := w;
n112.symbols := x;
n113.symbols := z ;
wy H , delete constraint #3 .
empty.
Cleanup:
n12.label := 0;
Two children n12 created:
n121.symbols := v;
n122.symbols := z ;
(See Figure 2.)
Table 1: Example computing cluster tree

12

fiOrder Magnitude Comparisons Distance

n1
0th iteration
v,w,x,y,z

n1

1st iteration

5
v,w,x,y,z
n11

n12

w,x,y

v,z

n1
2nd iteration
5
v,w,x,y,z
n11
4

n12
w,x,y
v,z

w

x



n1
Cleanup
5
v,w,x,y,z
n11
4

w

x

n12
w,x,y

0



v

Figure 2: Building cluster tree

13

v,z

z

fiDavis

contains constraints
od(v; w) od(z; y).
od(w; x) od(z; y).
od(x; y) od(z; y).
od(z; y) od(v; z ).
algorithm proceeds follows:
Initialization:
tree initializes single node n1.
n1.symbols := f v; w; x; y; z g.
First iteration:
shorts f vw; wx; xy; zy; vz g.
H set connected components, complete graph v; w; x; y; z .
algorithm exits returning false
Table 2: Example determining inconsistency
axiom A.7 finitely many different orders magnitude. algorithm returns
different labels number different orders magnitude
space, constraints satisfiable. uses labels space
orders magnitude, constraints unsatisfiable.
proof easier present rewrite algorithm solve constraints following
form, returns number different non-zero labels used, actually
construct cluster tree.1

num labels(S );

function

return
else return

empty

(0)
(1 + num labels(reduce constraints(S )))
function reduce constraints(S )
H := connected components shorts ;
H contains edges return(false) top-level
else return(the set constraints whose long H )

easily verified sequence values successive recursive calls
num labels sequence values main loop solve constraints.
Therefore num labels returns number different non-zero labels tree constructed
solve constraints.
1. reader may wonder simpler algorithm presented complicated
algorithm solve constraints. reason proof found system constraints consistent num labels return
constructive solve constraints.

14

false

relies relation num labels

fiOrder Magnitude Comparisons Distance

Theorem 2: solutions set constraints , instantiations
solve constraints(S ) fewest number different values od(a; b), a; b range
symbols . number given num labels(S ).
proof given appendix.

6. Extensions Consequences
next present number modifications algorithm solve constraints. first
ecient implementation. second extends algorithm handle non-strict
comparisons. third extend algorithm handle combination order-of-magnitude
comparisons distance order comparisons, one-dimensional space.

6.1 Ecient Implementation Solve constraints
possible implement algorithm solve constraints somewhat eciently
naive encoding description. key observe graph H connected
components computed explicitly; suces compute implicitly using
merge-find sets (union-find sets). Combining suitable back pointers edges
constraints, formulate ecient version algorithm.
use following data structures subroutines:

node N cluster tree contains pointers parents children; field

N .label, holding integer label; field N .symbols, holding list symbols
leaves N ; field N .mfsets, holding list connected components
symbols N . described below, connected component implemented
merge-find set (MFSET).

edge E graph symbols contains two endpoints,

symbol; field E .shorts, list constraints E appears short;
field E .longs, list constraints E appears long.

constraint C two fields, C .short C .long, edges. also

pointers lists C .short.shorts C .long.longs, enabling C removed
constant time constraint lists associated individual edges.

use disjoint-set forest implementation MFSETs (Cormen, Leiserson,

Rivest, 1990, p. 448) merging smaller sets larger path-compression.
Thus, MFSET upward-pointing tree symbols, node tree
symbol. tree whole represented symbol root. symbol
following fields:

{
{
{
{

A.parent pointer parent MFSET tree.
A.cluster leaf pointer leaf cluster tree containing A.
root MFSET A.size holds size MFSET.
root MFSET, A.symbols holds elements
MFSET.

15

fiDavis

{ root MFSET A.leaf ptr holds pointer pointer
N .mfsets N = A.cluster leaf.
describe algorithm.
procedure

variables:

0.
1.
2.
3.
4.
5.
6.
7.

begin

8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.

solve constraints1(in : system constraints form od(a; b) od(c; d)).
return either cluster tree satisfying consistent;
false inconsistent.
integer;
a; b symbols;
C constraint ;
H undirected graph;
E; F edges;
P MFSET;
N; nodes ;

contains constraint form, \od(a; b) od(c; c)" return false;

H := ;;

constraint C short E long F
add E F H ;
add C E .shorts F .longs endfor;
:= number variables ;
initialize contain root N ;
N .symbols := variables ;




leaf N , INITIALIZE MFSETS(N );
edge E = ab H
E .shorts non-empty FIND(a) 6= FIND(b)
MERGE(FIND(a), FIND(b)) endif endfor
every edge E = ab H satisfies FIND(a) = FIND(b)

repeat


return(false) endif



current leaf N
N .mfsets one element
mfset P N .mfsets
construct node new child N ;
.symbols:= P .symbols;
endfor endif endfor

edge E = ab H
FIND(a) 6= FIND(b)
constraint C E .longs
delete C ;
delete C E .longs;
delete C C .short.shorts endfor
delete E H endif endfor
:= 1;
empty;




leaf N
N .label := 0;
N .symbols one symbol
16

fiOrder Magnitude Comparisons Distance

32.
33.

create leaf N label 0 symbol N .symbols;
solve constraints1.


endif endfor end

INITIALIZE MFSETS(N : node)
: symbol;
N .mfsets := ;;
N .symbols
A.parent := null;
A.cluster leaf := N ;
A.symbols := fAg;
A.size := 1;
N .mfsets := cons(A,N .mfsets);
A.leaf ptr := N .mfsets;
endfor end INITIALIZE MFSETS.
procedure
var

MERGE(in A; B : symbol)
.size
.size swap(A; B );
A.parent := B ;
B .size := B .size + A.size;
B .symbols := B .symbols [ A.symbols;
Using A.leaf ptr, delete A.cluster leaf.mfsets;
end MERGE.
procedure

>B

FIND(in : symbol) return symbol;
: symbol;
.parent = null return
:= FIND(A.parent);
A.parent := R; /* Path compression */
return(R)
end FIND.
procedure
var R

else R

Let n number symbols ; let e number edges; let
number constraints. Note n=2 e n(n 1)=2 e=2 e(e 1)=2.
running time solve constraints1 computed follows. iteration
main loop 8-28 splits least one connected components H ,
n 1 iterations. MERGE-FIND operations loop 9-11 take together time
O(max(nff(n); e)) ff(n) inverse Ackermann's function. iteration
inner loop lines 16-18 creates one node tree. Therefore,
O(n) iterations loop entire algorithm. Lines 14, 15 outer
loop require n iterations iteration main loop. loop 22-26
executed exactly course entire execution algorithm
constraint C , hence takes time O(s) entire algorithm. Steps 20-21
require time O(e) iteration main loop. easily verified remaining
operations algorithm take time these. Hence overall running time
O(max(n2 ff(n); ne; s)).

17

fiDavis

6.2 Adding Non-strict Comparisons
algorithm solve constraints modified deal non-strict comparisons
form od(a; b) od(c; d) by, intuitively, marking edge ab \short" iteration
edge cd found short.
Specifically, algorithm solve constraints, make following two changes. First,
revised algorithm takes two parameters: , set strict constraints, W , set
non-strict constraints. Second, replace line
H := connected components shorts

following code:

1.
2.
3.
4.
5.

H := shorts ;
repeat H := connected components H ;
weak constraint od(a; b) od(c; d)
cd H add ab H endif endfor
change made H last iteration.

proof revised algorithm correct slight extension proof
theorem 1 given appendix.
Optimizing algorithm eciency little involved, new
operations must included, also four parameters | n,
number symbols; e, number edges mentioned; s, number strict comparison;
w, number non-strict comparisons | optimal implementation varies
depending relative sizes. particular, either w, though both, may
much smaller n, cases requires special treatment optimal eciency.
best implementation found case w
(n)
running time O(max(n3 ; nw; s)). details implementation straightforward
sucient interest worth elaborating here.
immediate consequence result couple problems inference
easily computed:

determine whether constraint C consequence set constraints ,
form set [ :C check consistency. [ :C inconsistent Sj=C .
Note negation constraint od(a; b) od(c; d) constraint
od(c; d) od(a; b).
determine whether two sets constraints logically equivalent, check
constraint first consequence second, vice versa.

6.3 Adding Order Constraints
Example 3 Section 2 involves combination order-of-magnitude constraints distances together simple ordering points, points lie one-dimensional
line. next show extend algorithm solve constraints deal complex situation.

18

fiOrder Magnitude Comparisons Distance

terms axiomatics, adding ordering points involves positing
relation p < q total ordering ordering points related order
magnitude comparisons distances following axiom.
A.9 points a; b; c 2 P , < b < c od(a; b) od(a; c).
following rule easily deduced: C disjoint clusters, either every
point C less points D, vice versa.
extending algorithm, begin defining ordered cluster tree cluster
tree where, every internal node N , partial order children N .
B children N ordered B , instantiation tree,
every leaf must precede every leaf B . Procedure instantiate1 modified
deal ordered cluster trees follows:

instantiate1(in N : node cluster tree;
: om-space; 1 : : : k : orders magnitude;
G : array points indexed nodes )
N leaf
let C1 : : : Cp children N topologically sorted order;
x0 := G[N ];
q := N .label;
pick points x1 : : : xp increasing order
i; j 2 0 : : : p, 6= j od(xi ; xj ) = q ;
/* points chosen virtue axiom A.8 */
= 1 : : : p
G[Ci ] := xi ;
instantiate1(Ci ;
; 1 : : : k ; G)
endfor
endif end

instantiate1.

Algorithm solve constraints modified follows:
procedure

fNEWg

variables:

begin

solve constraints2(in : system constraints form od(a; b) od(c; d) ;
: system constraints form < b)
return either ordered cluster tree satisfying
consistent;
false inconsistent.
integer;
C constraint ;
H; undirected graphs;
M; N; P nodes ;
a; b; c; symbols;

contains constraint form, \od(a; b) od(c; c)"

return false

;

fNEWg internally inconsistent (contains cycle) return false;
:= number variables ;
initialize consist single node N ;
N .symbols:= variables ;
repeat

H := connected component shorts ;

19

fiDavis

fNEWg

H := incorporate order(H; O);
H contains edges


return false

leaf N
vertices N connected H
N .label := m;
connected component N .symbols H
construct node new child N ;
.symbols:= vertices ;
endfor endif

fNEWg
fNEWg
fNEWg
fNEWg
fNEWg



constraint < b 2
.symbols b P .symbols
P different children N
add ordering arc P ;
endif endfor

endfor

:= subset constraints whose long H ;
:= 1;



empty;

leaf N
N .label := 0;
N .symbols one symbol
create leaf N symbol N .symbols;
label leaf 0;
endif endfor

end

solve constraints2.

fNEWg

function

incorporate order(in H : undirected graph;
: system constraints form < b)
return undirected graph;

variables:

G : directed graph;
a; b : vertices H ;
A; B : connected components H ;
V [A] : array vertices G indexed connected components H ;
: subset vertices G;

connected component H create vertex V [A] G;
constraint < b 2
let B connected components H containing b respectively;
6= B add arc G V [A] V [B ] endif endfor;
strongly connected component G
pair distinct vertices V [A]; V [B ] 2
2 b 2 B add edge ab H endfor endfor




endfor

20

fiOrder Magnitude Comparisons Distance

end

incorporate order.

Function incorporate order serves following purpose. Suppose
midst main loop solve constraints2, partially constructed cluster tree,
currently working finding sub-clusters node N . original
form solve constraints, find connected components shorts order-ofmagnitude constraints. Let C1 : : : Cq ; know diameter Ci
much smaller diameter N . Now, suppose, example,
constraints a1 < a5 ; b5 < b2 ; c2 < c1 , a1 ; c1 2 C1 ; b2 ; c2 2 C2 ; a5 ; b5 2 C5 .
follows axiom A.9 C1 , C2 , C5 must merged single cluster,
whose diameter less diameter N . Procedure incorporate order finds
loops constructing graph G whose vertices connected components H
whose arcs ordering relations computing strongly connected
components G. (Recall two vertices u; v directed graph strongly
connected component cycle u v u.) merges together
connected components H lie single strongly connected component G.
proof correctness algorithm solve constraints2 analogous structure proof theorem 1, given appendix.
implementing manner Section 6.1, algorithm made run
time O(max(n2 ff(n); ne; no; s)), number constraints O.

7. Finite order magnitude comparison
section, demonstrated algorithm solve constraints applied systems
constraints form \dist(a; b) < dist(c; d) / B " finite B ordinary Euclidean
space long number symbols constraint network smaller B .
could sure immediately result must apply finite B .
fundamental property non-standard real line sentence first-order
theory reals holds infinite values holds suciently large finite
value, sentence holds infinite value holds arbitrarily large
finite values. Hence, since answer given algorithm solve constraints works
set constraints constraint \od(a; b) od(c; d)" interpreted \od(a; b)
< od(c; d)/B infinite B ", answer must valid suciently large finite B .
interesting find simple characterization B terms ; namely,
B larger number symbols .
begin modifying form constraints, interpretation cluster
tree. First, avoid confusion, use four-place predicate \much closer(a; b; c; d)"
rather form \od(a; b) od(c; d)" going give interpretation
\od" function. fix finite value B > 1, interpret \much closer(a; b; c; d)"
mean \dist(a; b) < dist(c; d) / B ."
next redefine means valuation instantiate cluster tree:

Definition 6: Let cluster tree let valuation symbols . say
`T following holds: symbols a; b; c; , let least common
ancestor a; b let N least common ancestor c; d. .label < N .label
much closer(a; b; c; d).

21

fiDavis

Procedure \instantiate", generates instantiation cluster tree, modified
follows:
procedure

instantiate(in : cluster tree;
: Euclidean space; B : real);
return : array points indexed symbols ;

Let n number nodes ;
ff := 2 + 2n + Bn;
Choose 1 ; 2 : : : n < i+1 =ff;
pick point x 2
;
G[T ] := x;
instantiate1(T;
; 1 : : : n ; G);
return restriction G symbols .
end instantiate.
instantiate1(in N : node cluster tree;
: Euclidean space;
1 : : : n : orders magnitude;
G : array points indexed nodes )
N leaf
let C1 : : : Cp children N ;
x1 := G[N ];
q := N .label;
pick points x2 : : : xp
i; j 2 1 : : : p, 6= j q dist(xi ; xj ) < nq
/* possible since p n. */
= 1 : : : p
G[Ci ] := xi ;
instantiate1(Ci ;
; 1 : : : n ; G)
endfor
endif end

instantiate1.

analogue lemma 2 holds revised algorithm:
Lemma 22: cluster tree instantiation Euclidean space <m dimensionality m.
state theorem 3, asserts correctness algorithm \solve constraints"
new setting:

Theorem 3: Let set constraints n variables form \dist(a; b) <
dist(c; d) / B ", B > n. algorithm solve constraints(S ) returns cluster tree
satisfying consistent Euclidean space, returns false inconsistent.
proofs lemma 22 theorem 3 given appendix.
examination proof lemma 22 shows result depend
relation n B . Therefore, solve constraints(S ) returns tree ,
consistent satisfies regardless relation n B . However,
possible consistent solve constraints(S ) return false n B .
hand, one see proof theorem 3 (particularly lemma 23) B > n
solve constraints(S ) returns false inconsistent metric space. However,
metric spaces <m cluster tree returned solve constraints
may instantiation.

22

fiOrder Magnitude Comparisons Distance

8. first-order theory
final result asserts om-space rich enough full first-order language
order-of-magnitude distance comparisons decidable. Specifically, collection
orders magnitude dense unbounded above, decision algorithm
first-order sentences formula, \od(W; X ) od(Y; Z )" runs time O(4n (n!)2 s)
n number variables sentence length sentence.
basic reason following: observed corollary 4, cluster
tree determines truth value constraints form \od(a; b) od(c; d)"
a; b; c; symbols tree. is, two instantiations two omspaces agree constraint. require om-spaces dense
unbounded, much stronger statement holds: two instantiations
om-spaces agree first-order formula free symbols relation
\od(W; X ) od(Y; Z )". Hence, suces check truth sentence possible
cluster trees variables sentence. Since finitely many cluster
trees fixed set variables (taking account relative order labels
numeric values), decidable procedure.
Let L first-order language equality constant function symbols,
single predicate symbol \much closer(a; b; c; d)". easily shown L
expressive language function symbol \od" relation symbol .

Definition 7: om-space
orders magnitude
following axiom:

dense satisfies

A.9 orders magnitude 1 3 D, exists order magnitude 2
1 2 3 .

unbounded satisfies following:
A.10 every order magnitude 1 exists 2 1 2 .
collection orders magnitude hyperreal
line,
p
satisfied. axiom [A.9], 0 1 3 , choose 2 = 1 3 , geometric mean.
0 = 1 3 , choose 2 = 3 1. axiom [A.10] choose 2 = 1 =
0 < 1.

Definition 8: Let cluster tree. Let l0 = 0; l1 ; l2 : : : lk distinct labels
ascending order. extending label either (a) li i; (b) lk + 1 (note
lk label root); (c) (li 1 + li )=2 1 k.
Note k distinct non-zero labels, 2k + 2 different extending
labels .

Definition 9: Let cluster tree. Let x symbol . cluster tree
0
extends x 0 formed applying one following operations (a
single application single operation).
1. null tree 0 tree containing single node x.

23

fiDavis

2. consists single node symbol y. Make new node , make x
children , set label either 0 1.
3. internal node N (including root), make x child N .
4. Let symbol , let N father. N .label 6= 0, create new node
extending label .label < N .label. Make child N ,
make x children .
5. Let C internal node root, let N father. Create
new node extending label C .label < .label < N .label.
Make child N make x C children .
6. Let R root . Create new node .label = R.label + 1. Make
R x children . Thus root new tree 0 .
(See Figure 3.)
Note tree n symbols n 1 internal nodes

n 1 ways carry step 3.
n possible ways choose symbol step 4, 2n 2
label each.

n 2 different choices C step 5, 2n 3 choices
label each.

one way carry step 6.
Hence, less 4n2 different extensions x. (This almost certainly
overestimate least factor 2, final algorithm entirely impractical
worthwhile precise.)

Definition 10: Let cluster tree, let formula L open variables
. satisfies every instantiation satisfies .
Theorem 4: Let cluster tree. Let open formula L, whose free variables
symbols . Let
om-space dense unbounded above. Algorithm
decide(T; ) returns true satisfies false otherwise.

decide(T : cluster tree; : formula) return boolean
convert equivalent form logical symbols
: (not), ^ (and), 9 (exists), = (equals) variable names,
non-logical symbol predicate \much closer".
function

case

form X = : return (distance(X; Y; ) = 0);
form \much closer(W; X; Y; Z )": return distance(W; X; ) < distance(Y; Z; ));
form : : return not(decide(T; ))
form ^ : return(decide(T; ) decide(T; ))
form 9X ff;

24

fiOrder Magnitude Comparisons Distance

P

P

P

2

2

2
w

Q

Q

1

w

Q

w

x

1

1

u

u

v

u

v

v

x

Operation 3:

Operation 3:
Original

N = P

N = Q

P
2
w

Q
1

P

P

2

2
w

Q
1

Operation 4:

Operation 4:
y=v

y=u



Q

0/0.5/1/1.5

1
u

v




0/0.5

0/0.5

u

u

v

x

w

Operation 4:

x

y=w


P

3

2
x


w

P
2

1.5
Operation 5:
x

w

Q
1

C=Q, N=P

Q
1
u

u

v

v

v
Operation 6:
R=P

Figure 3: Extensions cluster tree

25

x

fiDavis

extension

, decide(T ; ff) = true


T0 X
return true
else return false endif endcase

end

0

decide

distance(X; : symbol; : cluster tree) return
N := common ancestor X ;
return(N .label)
function

end

integer

distance

proof theorem 4 given appendix.
Running time: remarked above, tree size k 4k2
extensions considered. total number cluster trees considered therefore
bounded nk=1 4k2 = 4n (n!)2 . easily verified logical operators
quantifiers add factor length sentence. Hence running
time bounded O(4n (n!)2 s).
key lemma, interest itself, states following:
Lemma 28: Let cluster tree. Let open formula L, whose free variables
symbols . Let
om-space dense unbounded above. one
instantiation
satisfies every instantiation
satisfies .
is, either true instantiations none. proof given
appendix.
observed conditions
lemma 28 necessary,
statement false otherwise. example, let
om-space described
example I, Section 3, polynomials infinitesimal .
unbounded
above; maximum order-of-magnitude O(1). Let starting tree Figure
3 (upper-left corner). Let formula \9X od(V; W ) od(W; X )", free V W .
valuation fU ! ; V ! 0; W ! 1g satisfies , whereas valuation
fU ! 2 ; V ! 22 ; W ! g satisfies .

9. Conclusions
applications specific algorithms undoubtedly limited; aware
practical problems solving systems order-of-magnitude relations distances
central problem. However, potential applications order-of-magnitude reasoning
generally widespread. Ordinary commonsense reasoning involves distances spanning ratio 108 , fraction inch thousands miles, durations
spanning ratio 1010 , fraction second human lifetime. Scientific
reasoning spans much greater ranges. Explaining dynamics star combines reasoning
nuclear reactions reasoning star whole; differ ratio
1057 . techniques needed compute quantities vastly differing
sizes quite different techniques needed compute quantities similar
sizes. paper small step development analysis computational
techniques.
results also significant encouragement give hope
order-of-magnitude reasoning specifically, qualitative reasoning generally, may lead

26

fiOrder Magnitude Comparisons Distance

useful quick reasoning strategies broader range problems. often found
AI moving greater lesser precision mode inference type
knowledge lead quick dirty heuristic techniques, rather slow
dirty techniques. Nonmonotonic reasoning notorious example this,
arises well many types automated reasoning, including qualitative spatial
physical reasoning. algorithms developed paper welcome exception
rule. currently studying algorithmic techniques order-of-magnitude
problems, optimistic finding similar favorable results.

Acknowledgements
research supported NSF grant #IRI-9625859. Thanks Ji-Ae Shin,
Andrew Gelsey, reviewers helpful comments.

Appendix A. Proofs
appendix, give proofs various results asserted body paper.

Proof Lemma 2
Lemma 2: cluster tree
om-space, instantiate(T;
) returns
instantiation .
Proof: Let 0 = 0. node N , i=N .label, define (N ) = . proof
proceeds following steps:
i. nodes ,N , descendant N od(G[M ]; G[N ]) (N ).
Proof: child N , immediate construction x2 : : : xp
instantiate1. Else, let N = N1 ; N2 : : : Nq = path N
. definition cluster tree, follows Ni .label < N .label, > 1
therefore (Ni ) (N ). Thus od(G[M ]; G[N ]) (by o.m.-triangle inequality)
maxi=1:::q 1 (od(G[Ni+1 ]; G[Ni ])) maxi=1:::q 1 ((Ni )) (since Ni+1 child
Ni ) (N ).
ii. Let N node ; let C1 C2 two distinct children N ; let M1
M2 descendants C1 C2 respectively. od(G[M1 ]; G[M2 ]) = (N ).
Proof: construction x2 : : : xp instantiate1(N ), od(G[C1 ]; G[C2 ]) = (N ).
part (i.), od(G[M1 ]; G[C1 ]) (C1 ) (N ) likewise od(G[M2 ]; G[C2 ])
(N ). Hence, axiom A.6, od(G[M1 ]; G[M2 ]) = (N ).
iii. Let b two leaves , let N least common ancestor
b. od(G[a]; G[b]) = (N ). Proof: Immediate (ii).
iv. node N , odiam( (N )) = (N ). Proof: (iii), two leaves descending
different children N distance order (N ), two leaves N
distance order greater (N ).

27

fiDavis

v. node N , (N ) cluster (T ). Proof: Let b leaves N ,
let c leaf N . Let common ancestor b
let J common ancestor c. either N descendant N
J proper ancestor N . Therefore part (i), (I ) (J ). (iii),
od( (a); (b)) = (I ) (J ) = od( (a); (c)).
vi. internal nodes N; .label < N .label odiam( (M )) odiam( (N )).
Proof: Immediate (iv) construction .
vii. C cluster (T ) node N C = (N ). Proof: Let
set symbols corresponding C let N least common ancestor
. Let b two symbols different subtrees N .
(iii), od(G[a]; G[b]) = (N ). Let x symbol N .symbols. (iii)
od(G[a]; G[x]) (N ). Hence G[x] 2 C .

2
Proof Theorem 1
prove correctness algorithm solve constraints. assume throughout
two variables long constraint distinct.
Lemma 3: Let cluster tree let instantiation . Let b
symbols . Let N least common ancestor b . od( (a); (b)) =
odiam( (N )).
Proof: Since (a) (b) elements (N ), follows definition odiam
od( (a); (b)) odiam( (N )). Suppose inequality strict; is, od( (a); (b))
odiam( (N )). let C set symbols c od( (a); (c))
od( (a); (b)). odiam( (C )) = od( (a); (b)) odiam( (N )). easily shown
(C ) cluster (T ). Therefore, property (ii) definition 5, must
node .symbols = C . Now, certainly ancestor N , since
odiam( (M )) odiam( (N )) .symbols contains b. contradicts
assumption N least common ancestor b. 2
Corollary 4: Let cluster tree let instantiation . Let a; b; c;
symbols . Let N least common ancestor c , let
least common ancestor b . od( (a); (b)) od( (c); (d))
.label < N .label.
Proof: Immediate lemma 3 property (iii) definition 5 instantiation. 2

Lemma 5: Let set constraints form od(a; b) od(c; d). Let H
connected components shorts . consistent, every edge
H.
Proof: Let valuation satisfying . Find edge pq od( (p); (q))
maximal. Now, ab short | is, constraint od(a; b) od(c; d)
| od( (a); (b)) od( (c); (d)) od( (p); (q)).

28

fiOrder Magnitude Comparisons Distance

Now, let ab edge H , connected components shorts .
path a1 = a; a2 : : : ak = b edge ai ai+1 short = 1 : : : k 1.
Thus, om-triangle inequality, od( (a); (b)) maxi=1::k 1(od( (ai ); (ai+1 )))
od( (p); (q)). Hence pq 6= ab, pq H . 2

Lemma 6: values H iteration supersets values later
iteration.

Proof: reset subset end iteration. H defined terms
monotonic manner. 2

cannot two successive iterations main loop.
Proof: contradiction. Suppose two successive iterations. H
same, since defined terms . H constructed contain shorts
, Since resetting end first iteration change , H must
contain longs well. Thus, H contains edges . case,
algorithm terminated failure beginning first iteration. 2
Lemma 7:

Lemma 8: Algorithm solve constraints always terminates.
Proof: lemma 7, algorithm exit failure, iteration
constraints removed . Hence, number iterations main loop
original size . Everything else algorithm clearly bounded. (Note
bound number iterations improved Section 6.1 n 1, n
number symbols.) 2

Lemma 9: algorithm solve constraints returns false, inconsistent.

Proof: algorithm returns false, transitive closure shorts contains
edges . lemma 5, inconsistent.

Lemma 10: constraint C form od(a; b) od(c; d) initial value ,
edge cd H particular iteration, constraint C start
iteration.
Proof: Suppose C deleted particular iteration. edge cd,
long C , cannot H iteration. is, possible edge cd persist
H iteration C deleted . Note that, lemma 6, cd
eliminated H , remains H . 2
Lemma 11: following loop invariant holds: end loop iteration,
values L.symbols, L leaf current state tree, exactly
connected components H .

Proof: first iteration, initially root R, containing symbols,
child R created connected component H .
Let Ti Hi values H end ith iteration. Suppose
invariant holds end kth iteration. lemma 6, Hk+1 subset Hk .
Hence, connected component Hk+1 subset connected component Hk .

29

fiDavis

Moreover, connected component J Hk either connected component Hk+1
partitioned several connected components Hk+1 . former case, leaf
Tk corresponding J unchanged remains leaf Tk+1 . latter case, leaf
corresponding J gets assigned one child connected component Hk+1
subset J . Thus, connected components Hk+1 correspond leaves Tk+1 . 2

Lemma 12: procedure solve constraints return false, returns wellformed cluster tree .
Proof: Using lemma 11, cleanup section solve constraints creates final
leaves symbols, follows every symbol ends single leaf .
decremented iteration, iteration adds new node children
node, follows label internal node less label father.
Hence constraints cluster trees (definition 3) satisfied. 2
Lemma 13: Let a; b two distinct symbols let cluster tree returned
solve constraints . Let N least common ancestor a; b . either N
assigned label first iteration edge ab H , edge ab
final value H loop exited N assigned label final cleanup
section.
Proof: above, let Hi value H ith iteration.
N root, assigned label first iteration. Clearly, b,
different subtrees N , must different connected components H1 .
Suppose N assigned label kth iteration loop k > 1. lemma 11,
end previous iteration, N .symbols connected component Hk 1 ,
therefore contained edge ab. Since N least common ancestor a; b, follows
b placed two different children N ; hence, two different connected
components Hk . Thus edge ab cannot Hk .
Suppose N assigned label cleanup section algorithm. lemma
11, N .symbols connected component final value H . Hence edge ab
final value H . 2
Lemma 14: Let initially contain constraint C form od(a; b) od(c; d). Suppose
solve constraints(S ) returns cluster tree . Let least common ancestor a; b
let N least common ancestor c; d. .label < N .label.
Proof: Suppose N given label given iteration. lemma 13, cd eliminated
H iteration. lemma 10, constraint C must start
iteration. Hence ab short iteration, therefore H . Hence
given label later iteration, therefore given lower label.
easily seen cd cannot H final iteration loop, hence N
assigned label cleanup section. 2
Lemma 15: Suppose solve constraints(S ) returns cluster tree . instantiation satisfies constraints .
Proof: Immediate lemma 14 corollary 4.

30

fiOrder Magnitude Comparisons Distance

Theorem 1: algorithm solve constraints(S ) returns cluster tree satisfying
consistent, returns false inconsistent.
Proof: solve constraints(S ) returns false, inconsistent (lemma 9).
return false, returns cluster tree (lemma 12). Since instantiation
(lemma 2) since every instantiation solution (lemma 15), follows
consistent satisfies . 2
Proof Theorem 2
Lemma 16: S1 S2 consistent sets constraints, S1 S2
reduce constraints(S1 ) reduce constraints(S2 ).
Proof: Immediate construction. value H case S1 superset value
case S2 , hence reduce constraints(S1 ) superset reduce constraints(S2 ).
Lemma 17: S1 S2 consistent sets constraints, S1 S2 num labels(S1)
num labels(S2).
Proof induction num labels(S2). num labels(S2) = 0, statement trivial.
Suppose statement holds 0 , num labels(S 0) = k.
Let num labels(S2) = k + 1.
k + 1 = num labels(S2 ) = 1 + num labels(reduce constraints(S2 )),
k =num labels(reduce constraints(S2 )). Now, suppose S1 S2 . lemma 16
reduce constraints(S1 ) reduce constraints(S2 ). inductive hypothesis
num labels(reduce constraints(S1 )) num labels(reduce constraints(S2 )),
num labels(S1) num labels(S2). 2
Lemma 18: Let set constraints, let solution . graph G
symbols , let nd(G; ) number different non-zero values od(a; b)
edge ab G. Let edges(S ) set edges . nd(edges(S ), )
num labels(S ).
Proof: induction num labels(S ). num labels(S ) = 0, statement trivial.
Suppose k, statement holds 0 num labels(S 0) = k, suppose
num labels(S ) = k + 1. Let pq edge maximal length. set edges E ,
let small-edges(E; ) set edges ab E
od( (a); (b)) od( (p); (q)). Since small-edges(E ) contains edges every order
magnitude E except order magnitude pq, follows
nd(small-edges(E; ), ) = nd(E; ) 1. Let G complete graph symbols
. argument lemma 5, small-edges(G; ) H , H connected
components shorts , computed reduce constraints(S ). Let 0 set
constraints whose longs small-edges(G; ). follows 0 reduce constraints(S ).
small-edges(G; ) edges(S 0 ) edges(reduce constraints(S )).
Hence nd(edges(S ), ) = nd(G; ) = nd(small-edges(G; ), ) + 1
nd(edges(reduce constraints(S ))) + 1 (by inductive hypothesis)
num labels(reduce constraints(S )) + 1 = num labels(S ). 2

31

fiDavis

Theorem 2: solutions set constraints , instantiations
solve constraints(S ) fewest number different values od(a; b), a; b range
symbols . number given num labels(S ).
Proof: Immediate lemma 18.
Corollary 19: Let
properties om-space except k
different orders magnitude. system constraints solution

tree returned solve constraints(S ) uses k different labels.
Proof: Immediate theorems 1 2. 2
Proof Algorithm Non-strict Comparisons
prove revised algorithm presented Section 6.2 non-strict comparisons
correct. proof slight extension proof theorem 1, given above.
Recall revised algorithm Section 6.2 replaces line solve constraints
H := connected components shorts
following code:

1.
2.
3.
4.
5.

H := shorts ;
repeat H := connected components H ;
weak constraint od(a; b) od(c; d)
cd H add ab H endif endfor
change made H last iteration.

need following new lemmas proofs:

Lemma 20: Let set strict comparisons, let W set non-strict comparisons. Let H set edges output code. [ W consistent,
edge H .
Proof: proof lemma 5, let valuation satisfying [ W let pq
edge od( (p); (q)) maximal. wish show that, every edge
ab 2 H , od( (a); (b)) od( (p); (q)), hence ab 6= pq. Proof induction: suppose
holds edges H point code, ab
added H . three cases consider.

ab added step [1]. Then, lemma 5, constraint od(a; b) od(c; d)
. Hence od( (a); (b)) od( (c); (d)) od( (p); (q)).
ab added step [2]. path a1 = a; a2 : : : ak = b edge
ai ai+1 H = 1 : : : k 1. inductive hypothesis, od( (ai ); (ai+1 ))
od( (p); (q)). om-triangle inequality,
od( (a); (b)) maxi=1::k 1(od( (ai ); (ai+1 ))) od( (p); (q)).

ab added step [4]. constraint od(a; b) od(c; d) W
cd H . inductive hypothesis, od( (c); (d)) od( (p); (q)).
32

fiOrder Magnitude Comparisons Distance

2

Lemma 21: Let W contain constraint od(a; b) od(c; d). Suppose algorithm
returns cluster tree . Let least common ancestor b , let N
least common ancestor c d. .label N .label.
Proof: lemma 13, N assigned label first iteration H include
edge cd. previous iterations, since cd H , ab likewise put H .
Hence get assigned label N , .label N .label.
remainder proof correctness revised algorithm exactly
proof theorem 1.
Validation Algorithm Solve constraints2
proof correctness algorithm solve constraints2 analogous structure
proof theorem 1. sketch below: details dicult fill in.
1. (Analogue lemma 2:) ordered cluster tree, revised version
instantiate(T ) returns instantiation . proof exactly lemma
2, additional verification instantiate2 preserves orderings .
2. (Analogue lemma 5:) Let set order-of-magnitude constraints distances,
let set ordering constraints points. Let H graph given
two statements

H := connected components shorts ;
H := incorporate order(H; O);
consistent, H contain edges .
Proof: proof lemma 5, choose valuation satisfying ; let pq
edge od( (p); (q)) maximal. Following informal argument
presented Section 6.3, easily shown pq longer edges
added two statements, hence H .
3. (Analogue lemma 9:) solve constraints2 returns false, ; inconsistent.
Proof: Immediate (2).
4. (Analogue lemma 12:) solve constraints2(S ; O) return false,
returns well-formed ordered cluster tree.
Proof: merging strongly connected components G, incorporate order always
ensures ordering arcs connected components H form DAG.
arcs precisely ones later added among children node N
ordering arcs. Thus, ordering arcs children node cluster tree
form DAG. Otherwise, construction tree lemma 12.
remainder proof proof theorem 1.

33

fiDavis

Proof Theorem 3
begin proving lemma 22, revised version \instantiate", given Section
6.3, gives instantiation cluster tree Euclidean space.
Lemma 22: cluster tree instantiation Euclidean space <m dimensionality m.
proof essentially proof Lemma 2, except
keep track real quantities. node N , i=N .label, define (N ) = .
proof proceeds following steps:
i. < j , < j =ffj . Immediate construction.
ii. nodes ,C , descendant C
dist(G[M ]; G[C ]) < ffn(C )=(ff 1).
Proof: Let C = C0 ; C1 : : : Cr = path P
C .
dist(
triangle inequality) ri=01 dist(G[Ci+1]; G[Ci ])
Pr 1G(n[M(];CG)[=ffC ])i) < (by
(ff=(ff 1))(n(C )).
i=0
iii. Let N node ; let C1 C2 two children N ; let M1 M2
descendants C1 C2 respectively.
(N )(1 2n=(ff 1)) < dist(G[M1]; G[M2 ]) < n(N )(1 + 2=(ff 1))
Proof: triangle inequality,
dist(G[C1 ]; G[C2 ]) dist(G[C1 ]; G[M1 ]) + dist(G[M1 ]; G[M2 ]) + dist(G[M2 ]; G[C2 ]).
Thus, dist(G[C1]; G[C2 ]) dist(G[C1]; G[M1 ]) dist(G[M2 ]; G[C2 ]) dist(G[M1 ]; G[M2 ]).
Also, triangle inequality,
dist(G[M1 ]; G[M2 ]) dist(G[C1]; G[C2 ]) + dist(G[C1]; G[M1 ]) + dist(G[M2 ]; G[C2 ]).
construction, (N ) dist(G[C1]; G[C2 ]) < n(N ),
part (ii), = 1; 2, dist(G[Mi]; G[Ci ]) < ffn(C )=(ff 1) < n(N )=(ff 1)
(C ) < (N )=ff.
iv. symbols a; b; c; , let P least common ancestor a; b let N
least common ancestor c; d. P .label < N .label
much closer(G[a]; G[b]; G[c]; G[d]).
Proof: part (iii), dist(G[a]; G[b]) < n(P )(1 + 2=(ff 1))
dist(G[c]; G[d]) > (N )(1 2n=(ff 1)). Since (P ) < (N )=ff since
ff = 2 + 2n + Bn, follows straightforward algebra
dist(G[a]; G[b]) < dist(G[c]; G[d]) / B .

2

next prove analogue lemma 5.

Lemma 23: Let set constraints n variables form
\dist(a; b) < dist(c; d) / B ", B > n. consistent, edge
connected components shorts .
Proof: Let valuation satisfying . Let pq edge dist( (p); (q))
maximal. Now, ab short | is, constraint much closer(a; b; c; d)
| dist( (a); (b)) < dist( (c); (d))/B dist( (p); (q))/B .

34

fiOrder Magnitude Comparisons Distance

Now, let ab edge H , connected components shorts .
simple path a1 = a; a2 : : : ak = b edge ai ai+1 short
= 1 : : : k 1. Note k n. Then, triangle inequality,
dist( (a); (b))
dist( (a1 ); (a2 )) + dist( (a2 ); (a3 )) + . . . + dist( (ak 1 ); (ak ))
(k 1)dist( (p); (q)) / B < dist( (p); (q))

Hence pq 6= ab, pq H . 2

Theorem 3: Let set constraints n variables form \dist(a; b) < dist(c; d)
/ B ", B > n. algorithm solve constraints(S ) returns cluster tree satisfying
consistent Euclidean space, returns false inconsistent.
Proof: Note semantics constraints \much closer(a; b; c; d)" enters
proof Theorem 1 lemmas 2 5. remainder proof Theorem 1
purely relation structure structure tree. Hence,
since shown analogues lemmas 2 5 hold set constraints
kind, proof completed exactly way. 2
Proof Theorem 4
Lemma 24: Let cluster tree let valuation om-space
satisfying .
Let x symbol , let point
, let 0 valuation [ fx ! ag.
exists extension 0 x 0 satisfies 0 .
Proof: empty tree, statement trivial. contains single symbol y,
= (y) operation (2) applies .label=0; 6= (y) operation (2)
applies .label=1.
Otherwise, let symbol od( (y); a) minimal. (We deal
case ties step (D) below.) Let F father .
Let D=od( (y); a). Let V set orders magnitude od( (p); (q)),
p q range symbols . define L suitable label follows:
2 V , L label corresponding D. larger value V
L label root plus 1. 62 V , value V larger D,
let D1 largest value V less D; let D2 smallest value V greater
D; let L1 , L2 labels corresponding D1 , D2 ; let L = (L1 + L2 )=2.
One following must hold:
A. (y) = a, F .label=0. apply operation (3) N = F .
B. (y) = F .label 6= 0. apply operation (4) .label = 0.
C. (y) 6= a, od( (y); a) less od( (z ); a) symbol z =
6 .
Apply operation (4) .label set suitable value .
D. one value y1 : : : yk od( (yi ); a) = D. easily shown
case internal node Q y1 : : : yk set symbols
subtree Q. three cases consider:

35

fiDavis

D.i D=odiam( (Q.symbols)). apply operation (3) N = Q.
D.ii > odiam( (Q.symbols)), Q root. apply operation (5)
C = Q. Set .label suitable value D. (It easily shown
< odiam( (N .symbols)), N father Q.)
D.iii > odiam( (Q.symbols)), Q root. Apply operation (6).

2

Lemma 25: Let = fa1 : : : ak g finite set points whose diameter order-ofmagnitude D. exists point u that, = 1 : : : k, od(u; ai ) = D.
Proof: Let b1 = a1 . axiom A.8 exists infinite collection points b2 ; b3 : : :
od(bi ; bj ) = 6= j . Now, value ai one value bj
od(ai ; bj ) D; two values bj 1 bj 2, om-triangle
inequality, od(bj 1; bj 2 ) D. Hence, k different values bj least
ai . Let u values bj . since od(u; a1 ) = od(a1 ; ai )
i, follows od(u; ai ) ai . Thus, since od(u; ai ) od(u; ai )
D, follows od(u; ai) = D. 2
Lemma 26: Let cluster tree; let valuation om-space
satisfying ;
let 0 extension x.
dense unbounded above,
value valuation [ fx ! ag satisfies 0 .
Proof: operations (1) (2) statement trivial.
Otherwise, let L extending label . L = 0, set = 0. L ,
let order magnitude corresponding L . L1 < L < L2
L1 L2 labels consecutive values , let D1 D2 orders
magnitude corresponding L1 , L2 . Let chosen D1 D2 .
L greater label tree, choose greater diameter
tree .
0 formed operation (3), using lemma 25 let point
od(a; (y)) = odiam(N ) N .symbols.
0 formed operation (4), let point od(a; (y)) =
D.
0 formed operation (5), let point od(a; (y)) =
C .symbols. (Note that, since .label < N .label, < odiam(N .symbols).)
0 formed operation (6), let point od(a; (y)) =
R.symbols.
cases, straightforward verify [ fx ! ag satisfies 0 . 2
observed Section 8 regarding lemma 28, conditions
lemma 26
necessary, statement false otherwise. example, let
om-space
described example I, Section 3, polynomials infinitesimal .

unbounded above; maximum order-of-magnitude O(1). Let starting tree
Figure 3 (upper-left corner), let 0 result applying operation 6 (middle
bottom). Let valuation fu ! ; v ! 2; w ! 1g. satisfies , cannot
extended valuation satisfies 0 , would require x given value
od(v; w) od(x; w), value exists within
. point lemma

36

fiOrder Magnitude Comparisons Distance

that,
required dense unbounded above, cannot get \stuck"
way.

Lemma 27: Let cluster tree. Let X variable among symbols .
Let ff open formula L, whose free variables symbols variable
X . Let formula 9X ff. Let
om-space dense unbounded above.
exists instantiation
satisfies exists
extension 0 instantiation 0 0 extends satisfies ff.
Proof: Suppose exists instantiation satisfies 9X ff. Then,
definition, point
satisfies ff(X=a). is, instantiation
[ fX ! ag satisfies ff. Let 0 = [ fX ! ag. lemma 24, cluster tree 0
corresponding 0 extension .
Conversely, suppose exists extension 0 instantiation 0 0
satisfying ff. Let restriction 0 symbols . clearly satisfies
formula 9X ff. 2
Lemma 28: Let cluster tree. Let open formula L, whose free variables
symbols . Let
om-space dense unbounded above. one
instantiation
satisfies every instantiation
satisfies .
Proof: assume without loss generality logical symbols :
(not), ^ (and), 9 (exists), = (equals) variables names, non-logical
symbol predicate \much closer". proceed using structural induction
form . Note equivalent statement inductive hypothesis is, \For formula
, either true every instantiation , false every instantiation
."
Base case: atomic formula \X = " \much closer(W; X; Y; Z )"
follows immediately corollary 4.
Let form : . true , false . inductive hypothesis, false every instantiation . Hence true every
instantiation .
Let form ^ . true true .
inductive hypothesis, true every instantiation . Hence
true every instantiation .
Let form 9X ff. true lemma 27, exists
extension 0 instantiation 0 0 ff true 0 . inductive
hypothesis, ff true every instantiation 0 . Now, 0 instantiation 0
satisfies ff, restriction 0 variables , clearly satisfies
9X ff. lemma 26, every instantiation extended instantiation 0
0 . Therefore, every instantiation satisfies . 2
Theorem 4: Let cluster tree. Let open formula L, whose free variables
symbols . Let
om-space dense unbounded above. Algorithm
decide(T; ) returns true satisfies false otherwise.
Proof: Immediate proof lemma 28. 2

37

fiDavis

References
Cormen, T.H., Leiserson, C.E., Rivest. R.L. (1990). Introduction Algorithms. Cambridge, MA: MIT Press
Davis, E. (1990). Order Magnitude Reasoning Qualitative Differential Equations.
D. Weld J. de Kleer (Eds.) Readings Qualitative Reasoning Physical Systems.
San Mateo, CA: Morgan Kaufmann. 422-434.
Keisler, J. (1976). Foundations Infinitesimal Calculus. Boston, MA: Prindle, Webber,
Schmidt.
Mavrovouniotis, M. Stephanopoulos, G. (1990). \Formal Order-of-Magnitude Reasoning Process Engineering." D. Weld J. de Kleer (Eds.) Readings Qualitative
Reasoning Physical Systems. San Mateo, CA: Morgan Kaufmann. 323-336.
Raiman, O. (1990). \Order Magnitude Reasoning." D. Weld J. de Kleer (Eds.)
Readings Qualitative Reasoning Physical Systems. San Mateo, CA: Morgan Kaufmann. 318-322.
Robinson, A. (1965). Non-Standard Analysis. Amsterdam: North-Holland Publishing Co.
Weld, D. (1990). \Exaggeration." D. Weld J. de Kleer (Eds.) Readings Qualitative
Reasoning Physical Systems. San Mateo, CA: Morgan Kaufmann. 417-421.

38

fi
ff
fi
! #"$ %
'&)( *,+( ---/.$0/1/243022

56789 :)((4;
-<!=?>7 %&:A@/;
--

BDC9EFCHGJIHK9LNMPOJQRC
SUTWVYX[Z\Z^]`_ba/]dcfe9gihjRkmloniprqWets
~

Znqprh6qW

lvuwZ^x9Vy]znWuwV|{|V,a}Wj

Z^]VFaj

$$ffr#

$$$

'9,'H

ff6$$$

')FA/
A)!4/U/

ff)4

w
,,49z64?ffJ4Ayy46F64/\ffm6,
J?F4m 4H46i6f,/46,
)
/A4! 6#9!/J?4Jyit
#64464
ff H4m?o46#,64#?i?4W
r4?J6wfi ff/Hff)
JA4?
v 4

$/
?m mWff)^
!9?
?6?
64m!4 6?i/ 6?A4 m?J4mFJ
?
/JA ? 44y
?H4J9? w 94A
!#"$
964&
%('*)
+,

/44?)?:
,6 44!zi 646

- .0/2143 ,5+764829

4ffff6W9;4/\?< 4

=<>!?A@ 7B:C DF7EFB @
G HJILKNMPORQTSfiULQU7VfiHJWfiHU7MTOLXLYZULS4[\XO&]^HMP_a`&bcULS:d`Rbfe
U7MK;[gONhjifOLMkWlW5URmnMUoipirqO
WHVWfiqKaMk\[jsKtS

uvwx ULYHJWfiV;[!UnyKNXVtezWfiqTO
W{HVteOLVfiV4HJ|7MT[!KNMkWfiVaULYW5S4\TWfiq}I
OLX\KNVaW5UgHWfiVaIAO
S4H~O
sXJKNV WfiqTO
W{VfiO
WfiHV5YZh]PT{qTHV
QTSfiULsXJKN[ULY4LzkgNqTOLVMz\T[!KtS4U7\TVO
QTQXHtO
WfiHU7MTV v ULWfiq<ePNLL x OLMkh^H[gQULS4WfiOLMW
R QTSfiULsXJKN[gV}SfiKNk\HJSfiKU7\TMWfiH~M|[!UzyTKNXVULY(QTSfiULQU7VfiHWfiHJU7MTOLX{YZULS4[(\TXOLVULScO
SfiKSfiKNy\TtHJsXJKW5UWfiqTHV
QTSfiULsXJKN[ [!U7M|WfiqKNVfiKQTSfiULsXJKN[VO
SfiKL v x _aU7[!Q\WfiHMT|yKt|LS4KtKULYrsKNXHJKtYrH~MOQSfiULQU7VfiHJWfiHJU7MTOLX
V5WfiO
W5KN[!KNMkWf{irHJWfiqPSfiKNVfiQKNW:W5UOQTSfiULQU7VfiHJWfiHJU7MOLXmnMUoiRXJKNy|LK;sOLV5K{OLVOU7MTyHJWfiHJU7MTOLXzQTSfiULsO
sHXHJWh
ULY|7HJILKNM} MWfiqKO
sV5KNMTKULYOPVfi\}tHJKNMkWfV5WfiO
WfiH~V5WfiHtVaWfiqTHVfQTS4ULsO
sHXHWh!tOLMsKRKNV5WfiH~[gO
W5KNyskh
uv 7 x uv x v x MYKtS4KNMTK;HMPfOohLKNVfiHOLM!sKNXHKtYMKtWifULSfimnVt V ULWfiq v NLL x VfiqTUoifKNy<e
WfiqK;QTS4ULsXJKN[ULY<U7\TMkWfiHM|[!UnyKNXVlULYOQTSfiULQU7VfiHWfiHJU7MTOLXkYULS4[(\TXOtOLMgsK;S4KNyT\TKNy!W5UWfiqTK;QTSfiULsXJKN[ULY
U7[!Q\TWfiHM|(WfiqKQTSfiULsO
sH~XHJWh!WfiqO
WaOPMUnyKHMO!aONhLKNVfiHOLMcsKNX~HJKtYMKtWifULSfimHVfW5S4\KL v x _fU7[gQ\WfiHM|
WfiqKMz\T[jsKtSULYVfiU7X\WfiHJU7MTV}ULYOU7MV5W5S4OLHMkWVfiO
WfiHV5YOLWfiHJU7MQTSfiULsXJKN[ _a`&b*YULS4[(\TXO w U7M^


S4HO
sXJKNV (ttt [ONh^sKSfiKt|7O
SyKNyOLVOU7MTV5W5S4OLH~MWVfiO
WfiHVfiYZOLWfiHJU7MQSfiULsXJKN[HM^irqTH~4q^WfiqK
yU7[gOLH~MULYKNOL4qI

S4HO
sXJK&HVRo A7eOLMTy}KNOL4qctX~OL\TV5KHV O(SfiKNX~O
WfiHJU7M}o ULYOLX~XMnFWfi\TQXJKNV
YZULSirqH4qO
WPXJKNOLV5WU7MKXHJW5KtS4OLX:ULYROLVfiV4\T[!KNVWfiqTK}I
OLX\K
{qKNMWfiqTKQTSfiULsXJKN[ULY{U7[!Q\WfiHM|
WfiqKMk\[jsKtSjULY{VfiU7X\WfiHJU7MTVjULYrVfi\TqQSfiULsXJKN[gVHVWfiqTK}U7MKULY{U7[!Q\TWfiHM| uvw(x v xj V5WfiH~[gO
WfiHM|
WfiqK\WfiHX~HJWhgULYSfiKNOLV5U7MTH~M|PirHJWfiqO
QTQTSfiUonH[O
W5KWfiqKtULSfih}rHMTVfiW5KNOLy}ULYWfiqKULS4HJ|7H~MTOLXWfiqTKtULSfih{qTHV
\WfiHX~HJWhyKtQKNMTyTVfU7McWfiqKV4HJtKULY uv { x
^ uv x v ULWfiq<eNLL x
MULWfiqKtSlH[!QULSfiWfiOLMkWO
QTQX~HtO
WfiHJU7MjH~VW5UrS4KNOLV5U7MTHM|RirHJWfiqHMTU7[!QXKtW5KHMYZULS4[gO
WfiHJU7M v G&SfiUoILKaKtWlOLXJe
NLAULNHMTV5mnHHe<NLLzeNLL x MTyKtKNy<eTHYl]yTKNVfiS4HJsKNV;O!SfiKNOLXi ULS4X~yYOLHJWfiqY\TXXJhLezWfiqKNM|7HILKNMO
YZULS4[\XOVfi\Tq!WfiqTO
W MTKNHJWfiqKtS MULSa:HVOXJUL|7H~tOLXU7MTV5KNz\KNMTKrULY]ezOSfiKNOLV5U7MTO
sXJK{OLVfiVfi\T[gQTWfiHJU7M
HVaWfiqTO
WrWfiqKj[!ULSfiK[gUzyKNX~V;ULY:]OLVfiV5KtSfiW&eWfiqTKj[!ULSfiKXHmLKNXJh}HV{WfiqTO
WRHV;W5S4\TKHM


( --- %%!:t
:aA/
fw
!fi8y
7! %&% &/%%~/ :

fi

RRT

{qTKcQTSfiULsXKN[ULYU7\TMkWfiHM|[gUzyKNX~VHV}!:U7[!QXKtW5KHM|LKNMKtS4OLX vZ OLXHOLMkWteN7
x eROLMTyV5Ue
OLtULS4yTH~M|rW5UWfiqK QTSfiKNV5KNMkWVfiWfiO
W5K ULYTWfiqTKfO
SfiWlHMjWfiqKKNX~y<eAU7[gQ\WfiO
WfiHJU7MTOLX~XJhHMW5SOLWfiO
sXJKaHMWfiqKfifULS4V5W
tOLV5KL{d\sU7HV v NLn x HMkW5SfiUnyT\TKNycOLMOLXJ|LULS4HJWfiq[WfiqTO
WRYULS&OLMhYZULS4[(\TXO}]ULYlIAO
SHO
sXJKNV{OLMyc

tXOL\TVfiKNVtekKNOLqctXOL\TVfiK&U7MkWfiOLHMTHMT|jX~HJ W5KtS4OLXVt ezU7\TMkWfiVf[gUzyKNX~VULY]H~MWfiH[gK v c x en sKNHM|WfiqK
QU7VfiHJWfiHJILK}SfiUzULW(ULYrWfiqKQU7XhzMU7[HOLX: ( ttT v


0
ttt


OLMTy}XH[
ff x KNKNMWfiXJhL e fiqOLM| v NLL x QTSfiKNV5KNMkW5KNyOLMOLXJ|LULS4HWfiqT[sOLVfiKNygU7MVfiH[gHX~O
SHyKNOLV
OLMTypirHWfiqV4H[gHXO
S(WfiH[!KU7[!QXJKnHWhLp{qKWfiH[!KcU7[!QXJKnHWhOLqTHJKtILKNyszhWfiqKNV5KcOLXJ|LULS4HWfiqT[gVPHV
OLMH[!QULSfiWfiOLMkWRH[gQTSfiUoILKN[gKNMW&UoILKtSWfiqTO
W&ULYOMTOLHJILKPOLXJ|LULS4HJWfiq[ irqH4q4qKN4mzVOLX~XOLVfiVfiH|7MT[!KNMkWfiVRULY
W5S4\WfiqcIAOLX\TKNVaW5UgWfiqKI

S4HO
sXKNVaULYlOPYZULS4[(\TXO]HMWfiH[!K v
x
UV4O
WfiHV5YZhQTS4OLWfiHtOLXfO
QTQXH~tO
WfiHJU7MTVteU7MKqTOLVjKNHWfiqKtSW5USfiKNV5ULS4WW5UO|LUzUn
2 LZ
7
ULY
&
e
L
U
c


\
5
V

K
L



L

J
X
L
|
L
U
4

J
H
fi
W

q
[
Z

L
U
c



7
U
!
[

Q

\
fi
W

H


|






N







r

J
H
fi
W

q


fi

N
K
L

5
V
7
U







J
X
K








okWfiH[!K
uvwx
uvwx
U7[!QXKnHJWhL
Kt|7O
S4yTHM|WfiqKS4VfiWOLXJW5KtS4MO
WfiHJILKLe\TskhOLMy KNX~"H ! mLUoInH # v NLn x QTSfiKNVfiKNMW5KNypOyKtW5KtS4[gH~MTHV5WfiH
OLXJ|LULS4HWfiqT[YZULSO
QQTSfiUNH[gO
WfiH~M|{WfiqK QTSfiULQULSfiWfiHJU7MULYW5S\WfiqjOLVfiV4HJ|7MT[!KNMkWfiVWfiqO
WVfiO
WfiHVfiYhORd&`&bcYZULS4[(\TXO

]Kt%
W $' & )
w ( yKNMULW5KWfiqTH~VaQTSfiULQULSfiWfiHJU7M v $* & +
w ( ff uvw(x iRqKtSfiKPH~VaWfiqKMk\[jsKtS;ULYIAO
S4H~O
sXJKNV
ULY] x GHJILKNM}V4[gOLXXTMz\T[sKtSV , / .10 nenWfiqK&OLXJ|LULS4HJWfiq[ U7[!Q\TW5KNV OLM}KNV5WfiH[gO
W53
K 2UL4
$' & )
w ( irqTH~4q
VfiO
WfiHV5KNV
v . x $* & +
w ( ,6587958$* & +
w (;: ,
KtW OLMTysKgWfiqTK}Mz\T[jsKtSULY{tX~OL\TV5KNV(OLMTyI

S4HO
sXKNVULYR]eSfiKNVfiQKNWfiHILKNXJhL{qKS\TMTMTHMT|
WfiH[!KULYlWfiqKOLX|LULS4HJWfiqT[H~VaKNV5WfiH[gO
W5KNyskhO!QU7XJhnMU7[gHOLXHMOLMTy [(\TXJWfiHQXHJKNy}skh
=<

/?> + A@BDC @EF> .+ ;BGC -F> C H?> ..
+

IzUeTWfiqTKS4\TMTMTH~M|WfiH~[!KULYWfiqTKjOLXJ|LULS4HJWfiq[|LSfiUoirVrYZOLVfiW{irHJWfiqWfiqKQSfiKNtHVfiHJU7MS4KNk\THSfiKNy<
HMTHOLXTOLMy}`&HVfiOLM v NL
x V5Wfi\TyTHKNygWfiqK&QTSfiULsXKN[ULYU7[!Q\WfiHM|WfiqTK&VfiHtKRULYO(\TMTHJU7MULYO|7HJILKNM
YOL[gHXJh}ULYV5KtWfiV+J ( KJ tt KJ B szh[gKNOLMTV{ULYWfiqK MTtX\TV4HJU7Mn tX\TV4HJU7MgYZULS4[(\TXO
B
L J ff P
J P
J M'W J U :
J W J UHW J (tt
P
P
MO#N (
(RQ*MSQ*B
(RQ*MSTVUQ*B
(RQ*MSTVUXT YZQ*B
: v x BG#C ( J ( W tt W J B
g
vx

qTKthVfiqTUoifKNyWfiqTO
W{WfiqKV4HJtKULYlWfiqK\TMTHJU7MtOLMsKO
QTQSfiUNH[gO
W5KNycOLtt\TS4O
W5KNXJhirqKNMWfiqKV4HJtKNV
{
ULYaU7MTXJhQO
SfiWULYaWfiqKgV5KtWjH~MW5KtS4VfiKNWfiHJU7MTVO
SfiKmzMUirM<[In\TQTQU7V5KPWfiqKgHMkW5KtS4V5KNWfiHJU7MVfiHJtKNVO
SfiK!mnMUirM
YZULS(OLXX Vfi\sTYOL[gHX~HJKNVU7MWfiOLH~MTHM|O
WP[!U7V5]
W \V5KtWfiVt _
^a`9b dv c x elWfiqKNMWfiqTK}\TMTHU7MVfiHJtKtOLMsK

QTQTS4UNH[gO
W5KNyirHWfiqO!SfiKNXO
WfiHJILKKtS4SfiULS{ULY fv e YhgRi B x
V!HMHOLXOLMTyp`&HVfiOLM v NL
x QU7HMkW5KNyU7\TWteWfiqTHV(SfiKNVfi\XJWtOLMpsKO
QTQXHJKNyW5UO
QTQTSfiUonH~[gO
WfiHM|
WfiqK!Mz\T[jsKtS&ULY [!UnyKNXVRULY Od`&bYZULS4[(\TXOk
] j KNOL4qtXOL\TV5K(ULYa] qTOLVHJWfiVV5KtWULYf[!UzyTKNXVte<OLMTy
W \W5KtS4[gVRULYWfiqK
uvwx HV{KNz\TOLXW5U}WfiqTK(VfiHtKjULYWfiqK(\TMTHJU7MULYOLXXWfiqTU7V5K(VfiKtWfiVtR{qz\TVteU7MTXJhWfiqKjS4V5l
YZULS4[\XO v x
SfiKRMTKtKNyKNyYZULSaO|LUzUzyO
QTQTSfiUonH~[gO
WfiHJU7M<ekirqKtSfiK c
5n^o5q
pRUoifKtILKtSNeYZULSfOqTHJ|7q
QTSfiKNtH~VfiHJU7M<re \O
QQTSfiU7OL4qTKNVPbULS&HMTV5WfiOLMTKLeYZULS{ ff tL}OLMTycOLMKtS4SfiULS{ULYat e ^]sA
ifOL[O v NLL x H~MW5SfiUnyT\TKNy!OLMOLXJ|LULS4HJWfiqT[YULSW5KNVfiWfiHM|jVfiO
WfiH~V2O
sHX~HJWhULY<Oj_a`&bYZULS4[\XOj]UILKtS
^IAO
S4H~O
sXJKNV(skhU7\TMWfiH~M|WfiqKW5S4\WfiqpOLVfiVfiHJ|7M[!KNMWfiV(YOLXVfiHJYZhnHM|] OLMyqKNfimnHM|HY;WfiqKNHJS!Mk\[jsKtS
HVXJKNVfiVWfiqOLt
{qKU7\TMkWfiHM|HVOLtU7[!QXHVfiqKNy^WfiqS4U7\|7qOLM MtX\TVfiHJU7M tX\TVfiHJU7MQTSfiUnKNVfiVte
irqKtS4KWfiqKjVfiHtKjULYlWfiqKVfiKtWrULYlW5S4\WfiqOLVfiVfiHJ|7MT[gKNMWfiVrYZOLXV4HJYhnHM|]HVrU7[!Q\W5KNycirHWfiqWfiqKOLH~yULY:WfiqK
ULsV5KtS4IAO
WfiHJU7MWfiqO
W;WfiqTHV;V5KtW{HVaWfiqTK\MTHJU7MULYWfiqTKV5KtWfiV;ULYW5S\WfiqOLVfiVfiHJ|7M[!KNMWfiVaYOLXVfiHYhnHM|KNOL4qtX~OL\TV5K

uwvXx

fiy w zR{d|,'}RR8|,~$R]wl$'}R 3 ~ff
ULYR]( iaOL[gOOLMOLXJhktKNyWfiqKOoILKtS4O
|LKS4\TMTMTH~M|WfiH[!KgULY{qTH~VOLXJ|LULS4HJWfiq[ U7MWfiqKVfiOL[!K}YS4OL[gKti ULSfim
OLVifKgyTHyYULSjWfiqK!OLXJ|LULS4HJWfiqT[ ifK!QTSfiKNVfiKNMWHMWfiqKgV5KNz\KNXprKgV4qUoifKNyWfiqTO
WYZULSO_a`&bYZULS4[(\TXO
:e<irqKtSfiK(HV&WfiqK
VfiO
WfiHVfiYhnHM|WfiqK!U7MTyTHJWfiHU7MWfiqO
W&YZULSOU7MTV5WfiOLMkl
W AeVfi\TqWfiqTO
WX6
` X~Ma
Mz\T[jsKtSULYI

S4HO
sXKNVte H~V:WfiqKrMz\T[sKtSULYtXOL\TV5KNVtezOLMTy HV:WfiqKRQTSfiULsO
sHXHJWhWfiqTO
W Oj|7HJILKNMX~HJW5KtS4OLX

QTQKNO
S4V{H~MOgtXOL\TV5K v WfiqKjVfiOL[!KjYULS&OLXX<XHJW5KtS4OLX~V x eWfiqKjOoILKtS4O
|LKS\TMTMTHMT|WfiH[gKULYWfiqTKjOLXJ|LULS4HJWfiq[HV
vK
C#( x
ULNHMTV5mnHH v NL x KN[!QXJUhLKNyOLMOLX|LULS4HJWfiqT[ irqTHqH~V(V4H[gHXO
SW5UWfiqTO
WPQTSfiKNV5KNMkW5KNyskh iaOL[gO
v NLL x eYZULSPU7\TMWfiH~M|[!UnyKNXVNK
prKVfiqUi KNyWfiqTO
WP\TMTyKtSSfiKNOLV5U7MTO
sXK}OLVfiVfi\[!QTWfiHJU7MTVNeWfiqKONILKtS4O
|LK
S4\TMMTHM|gWfiH~[!KULYfU7[!Q\WfiH~M|}WfiqTKKOLWMk\[jsKtSULY[!UnyKNXV&ULYOYZULS4[\XOiRHJWfiqtX~OL\TV5KNV&U7Mp


S4HO
sXJKNV;H~Vr v x eTirqTKtSfi

K ff v XJUL| x
MWfiqK&VfiKNk\KNXifKQTSfiKNV5KNMkWfOLMOLX|LULS4HJWfiqT[*irqTHqifKtOLXX_ad v _fU7\TMkWfiHM|(skh}d&OoIzH~V2\WfiMOL[ x
YZULSPU7[gQ\WfiHM| uvwx QTSfiKNtHVfiKNXJhL XJ|LULS4HJWfiq[ _adHVsOLV5KNyU7MWfiqKdONInHV2\TWfiMTOL[ QTSfiUnKNyT\SfiK
V \WfiMOL[eNL
x YULSV5U7XJInHM|WfiqKV4O
WfiHV2O
sH~XHJWhQTSfiULsXJKN[ v x KV5Wfi\TyTh
v xv dONInHK
YZKNO
Wfi\SfiKNVgULYdefOLMTyVfiqUiqUiHJWfiVgS4\TXJKNVgtOLMsKc[!UzyH KNy<e SfiKNVfi\TXJWfiH~M|HMOLMOLXJ|LULSHJWfiqT[ YZULS
U7\TMkWfiHM|c[!UnyKNXVN MOLMTOLXJhnVfiHVVfiqTUoirVWfiqO
WYULSOcYULS4[(\TXOc] iRHJWfiqp tXOL\TVfiKNVU7MIAO
S4H~O
sXJKNVte
HVWfiqK{QTSfiULsO
sHXHJWh
WfiqKrOoILKtS4O
|LKS4\TMTMHM|&WfiH[!K{ULY_adHV v K
x ekirqKtSfi
K ff +( ( . ezOLMT



@
WfiqTO
WrOLMkhXHJW5KtS4OLXUntt\S4VrHMOP|7HJILKNMtX~OL\TV5KL
IzKNWfiHU7MpyKNVfiSHJsKNVMz\T[!KtSfiU7\VKnQKtSH[!KNMkWfiVjirHWfiqp_adVfiqTUoirHMT|WfiqTO
W!HJWfiV(OLWfi\TOLXaONILKtS4O
|LK
WfiH[!KcU7[gQXJKHJWhpHVgVfiH|7MTH tOLMkWfiXJhXJUi KtS}WfiqTOLMWfiqK\QTQKtSPsU7\MTyTV(QTSfiUIzHyTKNypskh[gO
WfiqKN[O
WfiHtOLX
OLMTOLXJhnVfiHVH~
IzKNWfiHJU7
cOLMTyHMQTS4KtIzHJU7\Vi ULSfimnV v d\sU7HVte NLn
iaOL[gOneaNLLzULNHMTVfimzHHFeNL z
fiqTOLMT|eNLL x - KjOLX~V5U!QU7HMW;U7\TW{OLyIAOLMkWfiO
|LKNVRULYWfiqTH~V{MKtiOLXJ|LULS4HWfiqT[ UoILKtSRWfiqKU7MKNV{QTSfiKNV5KNMkW5KNy
HMWfiqKNV5KQTSfiKtInHJU7\TVi ULSfimnVtblHMTOLX~XJhLeifKRKNVfiWfiH[gO
W5KNyc_adpQKtSfiYULS[gOLMTKrU7M}YZULS4[\XOLViRHJWfio
q X~HJW5KtS4OLX
tXOL\TVfiKNVteOLMTyU7[!QO
SfiKNyWfiqKSfiKNVfi\XJWfiVW5UPWfiqU7VfiK&ULY4qKN4mzH~M|PWfiqKVfiO
WfiHV2O
sHXHJWh(ULYVfi\T4qYZULS4[(\TXOLV skh
dQTSfiKNV5KNMkW5KNycszhHJWfiqKNXX<KtWrOLXF v NL x
BE-
2B}BD
l>

@ 7E @



B:Cq 4


WfiqKjV5KNz\KNX<ifKjU7MTV4HyKtS;QTSfiULQU7VfiHWfiHJU7MTOLXYULS[\TX~OLV;UoILKtSOgV5KtW ff N (NtttN jULYIAO
SHO
sXJKNVte<
yKNMULW5KNVROgXHJW5KtSOLX v"E N (NtttN (ttt x eOLMTy V5WfiOLMyTV;YZULS{WfiqKMKt|7O
WfiHJU7MULY

f HHh/i4#^V#

{qKdONInHV2 \WfiMTOL[QTS4UzKNyT\TSfiK v xv dONInHV \TWfiMTOL[ePNL
x HVHMkW5KNMTyKNy^YULScyTKNtHyTHM|
VfiO
WfiHV5O
sHXHWhULY OQSfiULQU7VfiHJWfiHJU7MTOLXl_a`&bYZULS4[(\TXO] v SfiKt|7O
S4yTKNyOLVOV5KtWULY tXOL\V5KNV x eOLMTytOLMsK
QTSfiKNVfiKNMW5KNycOLVRO!SfiKNt\SVfiHJILKsUkU7XJKNOLMcYZ\TMWfiHJU7M v OLVfiVfi\T[HM|PWfiqTO
WrMUgtX~OL\TV5KULY]HV;OgWfiOL\W5U7XJUL|Lh x
YZ\TMWfiHJU7Md v ]PQTSfiULQU7VfiHWfiHJU7MTOLX<_a`RbpYZULS4[(\TXO x

rHY w ~H VaKN[!QTWhWfiqKNM
SfiKtWfi\S4M"#
zrHY:]U7MkWfiOLHMTVROLMKN[!QTWhtXOL\TV5KWfiqKNM
SfiKtWfi\S4

tt
z vd {qKQ\S4KXHJW5KtS4OLXS4\TXK Ax
HYWfiqKtS4KKnHVfiWfiV{O!Q\S4KX~HJW5KtS4OLXllHM] vd V4\T4qWfiqO
W yUkKNVrMULWrO
QTQKNO
SrHM] x WfiqKNM
w ( ff w (7 dv yKNXJKtW5KYZSfiU7[]OLX~XtXOL\TV5KNVrU7MWfiOLHMHM| Ax
SfiKtWfi\S4Mcd vw ( x dv ]H~V;VfiO
WfiHV2O
sXJKH w ( HV{VfiU Ax

uwvX

fi

RRT

dv {qK\TMTHWatX~OL\TV5KS4\TXK Ax
HY:]U7MkWfiOLHMTVROg\TMTHWatX~OL\TV5KP vd OgtX~OL\TV5KU7MTVfiH~V5WfiHM|PULY:O!VfiHM|7XKX~HJW5KtS4OLX Ax WfiqKNM
w ( ff w G (7 vd yKNXJKtW5KYZSfiU7[ ] OLMycOLXX<tXOL\TVfiKNV{U7MWfiOLH~MTHM| x
SfiKtWfi\S4Mcd vw ( x dv ]H~V;VfiO
WfiHV2O
sXJKH w ( HV{VfiU Ax
z vd {qKV5QX~HJW5WfiHM|(S4\TXJK Ax
qUzU7V5KOPI

S4HO
sXl
K cULY](









w ( ff
w (7





N



w (7
w ff
S4KtWfi\S4M v vw ( _
x vw x5x dv ]HV;VfiO
WfiH~V2O
sXJKH vw ( w x HV;V5U Ax
WrtOLMcsKV5KtKNMWfiqTO
W{WfiqKjQ\SfiKXHJW5KtSOLXS4\TXJKOLMTycWfiqK\TMTHJW{tX~OL\TV5KS4\TXKO
S4KjOLWfi\TOLXXJhQO
S4WfiHt\TXO

tOLV5KNVaULYWfiqTKVfiQXHJW5WfiHMT|S4\TXJKL YWfiqKtSfiKKHV5WfiVfOQ\SfiKRX~HJW5KtS4OLXHM]enOLMTyWfiqKV5QXHW5WfiHM|S4\XJKRqUkU7VfiKNV
eWfiqKNM w ( w
eOLMTyqKNMK vw ( w x HVVfiO
WfiH~V2O
sXJKPH w ( HVV5U tULSyTHM|W5UcWfiqK!V5QXHJW5WfiHM|
S4\TXKLez]H~VVfiO
WfiHV5O
sXJKrH vw ( jw x HV V5UenOLMTyqKNMKLen]HVVfiO
WfiH~V2O
sXJKRH w ( H~VV5U{qHV:HVKOLWfiXJh
WfiqKQ\SfiKXHJW5KtS4OLXS\TXJKL
Y&] U7MkWfiOLHMTV!O\TMTHJW(tXOL\TV5K 7eOLMTyWfiqKV5QX~HJW5WfiHM|cS\TXJK}qUzU7V5KNVeWfiqKNM w U7MWfiOLH~MTV!OLM
KN[!QTWhtXOL\V5K v WfiqK\TMHJWtXOL\V5K}iRHJWfiqU7\W(HJWfiV(U7MTXJhXHJW5KtSOLX x e:OLMTypV5UeHV(\TMTVfiO
WfiHV5O
sXJKL
IzUe ] HV
VfiO
WfiHV5O
sXJK!H w ( H~VV5U{qTH~VHVWfiqKg\TMTHWtX~OL\TV5K!S4\TXKL v MTtH~yKNMWfiOLX~XJhLe<WfiqKV5QXHJW5WfiHM|S4\TXK(W5S4KNO
WfiV
ifKNXXWfiOL\TW5U7XJUL|7HtOLX:tXOL\TVfiKNVrW5UzU YOtXOL\V5
K ULYf]*U7MWfiOLHMV&OXHW5KtS4OLXOLMyHJWfiVU7[!QXJKN[!KNMkW e
OLMTyWfiqTKV5QXHJW5WfiH~M|S\TXJKHV{O
QTQXHJKNyW5UceTWfiqKN8
irHX~XsKKtS4OLV5KNyYS4U7[ sULWfiq w ( OLMTy w
x
{qz\TVteAifKf[gOohU7MTVfiHyTKtSdOLVsKNHM|{sOLVfiKNyU7MWfiqKfVfiQXHJW5WfiHMT|fS\TXJKU7MTXhLeAiRqTHXJK:WfiqK WifUULWfiqKtS
S4\TXKNVV5KtSfiILKOLVf|7\THyTKNVYZULS;4qUzU7VfiHM|(OX~HJW5KtS4OLXK}tHJKNMkWfiXJhL MyKtKNy<enHJY]U7MWfiOLHMVfO(\TMTHWtXOL\TV5K
ULSOQ\SfiK{XHJW5KtSOLXe7WfiqTKNMgO
QTQXJhnHM|WfiqK{VfiQXHJW5WfiHMT|&S\TXJKaW5UghzHJKNX~yTVU7MTXJh
RYZULS4[\XOneLs\WqUkU7V4HM|
OgMU7MFQ\SfiKOLMTycMTU7Mn\TMTHJW2tX~OL\TV5KXHJW5KtS4OLX<XJKNOoILKNVrWi U}YULS[\TX~OLVaYULS{Y\SfiWfiqTKtS;QTSfiUnKNVfiVfiHM|
Y]U7MkWfiOLHMTV:MTKNHJWfiqKtSO\TMTHJWltXOL\TVfiK{MULS:OQ\SfiK;XHJW5KtSOLXe
WfiqKNMPWfiqK{XHJW5KtSOLXYZULSWfiqK;V5QX~HJW5WfiHM|RS4\TXJK
VfiqU7\XysKqU7V5KNMcHMV4\T4qOPiaONhWfiqTO
W;QTS4UzyT\KNV;V5KtWfiV w ( OLMTy w &WfiqTO
WRO
SfiKOLV{Vfi[gOLX~XOLV{QU7V4VfiHJsXJKL
KOLyySfiKNVfiVaWfiqTH~V;HVfiVfi\KH~MWfiqKV5KNz\KNX




r b

#

{qTH~MmzH~M|ULYRU7\TMkWfiHM|[!UnyKNXVULYROQTSfiULQU7VfiHJWfiHU7MTOLXYZULS4[(\TXO]elifKMTULWfiHKWfiqTO
WPskhqUzU7VfiHM|O


S4HO
sXJK)LenWfiqKV5QXHJW5WfiHM|S4\TXJK&OLWfi\TOLXXJhV5QXHWfiVWfiqKV5KtWaULY[!UzyTKNXVfULYl]HMkW5UPWifUgyTHVS5U7HMkW Vfi\sV5KtWfiVt
U7MK!HMirqTH
q HV&W5S4\K v MTOL[!KNXhLe<WfiqK([gUzyKNX~VRULY w ( x eOLMyWfiqTKULWfiqKtS v [gUzyKNX~VRULY w x HMirqTH~4q
PHVRYZOLX~V5KL{qKP4qU7V5KNMI

S4HO
sXK
QTQKNO
S4VMKNHWfiqKtS&H~M w ( MULSHM w
eOLMyWfiqKtS4KP[ONhcsK(ULWfiqKtS


S4HO
sXJKNVWfiqTO
WO
QTQKNO
S:HM!]s\WMTULWlH~M w ( v ULSMULW:HM w x l{qKNV5K;O
SfiKaWfiqKaI

S4HO
sXKNVirqH4q(sKNXJU7M|
U7MTXJhW5UWfiqK}tXOL\V5KNVWfiqTO
W(U7MWfiOLH~MWfiqKXHJW5KtS4OLX v ULS elSfiKNVfiQKNWfiHILKNXJh x
pRKNMTKLelKtILKtSfih[!UnyKNXULY



L





j
K


7
U
g
[

Q
J
X

K
5
W
N
K
c

5
W

U


g
[
z
U


N
K

X
L
U



]
z

c
h
L

fi
V
fi
V
J
H
7
|


~
H


c
|

"
















eSfiKNV5QKNWfiHJILKNXh x W5
U LeOLMTy
w ( vw x
v
" #TRULS hL N&O
SfisHJW5S4O
SHXJhW5UKtILKtSfih!IAO
SHO
sXJKaULY]WfiqO
WyUzKNVMULWO
QQKNO
HM w ( vw ekSfiKNV5QKNWfiHJILKNXJh x
KtW uvw(x VfiWfiOLMTyYULS;WfiqTKMk\T[sKtSaULY[!UnyKNXVfULYl]eTOLMTy ( v x yKNMULW5KWfiqKMz\T[sKtSaULYIAO
S4H~O
sXJKNV


v ULWfiqKtSRWfiqTOLM x irqTHqUztt\SRHM]s\TW{MULW{HM w ( vw x eWfiqKNM uvwx ff > uvw (
x : @ uvw x
VHWqTOLVsKtKNMVfiqTUoirM(H
InKNWfiHJU7
z
e7HYT4qKN4mzHMT|&VfiO
WfiH~V2O
sHX~HJWhHVWfiqTK |LU7OLXe7OLMyjORQ\SfiKX~HJW5KtS4OLX
iv rqTH~4q!HVMTULWO\TMTHJWltXOL\TVfiK x HV:4qTU7V5KNMgYZULSV5QX~HJW5WfiHM|e
WfiqKNM!U7MXJh w ( VfiqU7\TX~yPsKaQTS4UzKNVfiVfiKNy!YZ\TSfiWfiqKtSNe
irqTH~XJK w [ONhsKyTHV5S4Kt|7O
S4yKNy<G
prUi KtILKtSNeYZULSRU7\TMkWfiHM|[gUzyKNX~V;sULWfiq w ( OLMTy w [gO
W5W5KtSN;`RULW5K
WfiqTO
WHMcWfiqHVRtOLVfiK w U7MkWfiOLHMTVRWfiqKPVfiOL[!KPMk\[jsKtSrULY tXOL\TV5KNV&OLV]4
IzUeHMU7MkW5S4OLV5WW5U4qKN4mzH~M|
VfiO
WfiHV5O
sHXHWhLenWfiqKQ\S4KXHJW5KtS4OLXS4\XJKWfi\S4MTVaU7\W;W5UsKH~MK}tHJKNMkW;YULSRU7\TMWfiH~M|g[!UnyKNXVt

uZ

fiy w zR{d|,'}RR8|,~$R]wl$'}R 3 ~ff
aOLV5KNyU7MWfiqKNV5KULsV5KtSfiI

WfiHJU7MTVteTWfiqTKYU7XXUoirHMT|(Y\TMTWfiHU7M_adU7\TMkWfiV{[!UnyKNXV{ULYO!YZULS4[(\TXO]
UoILKtS(IAO
S4H~O
sXJKNVt
YZ\TMWfiHJU7M_ad v ](QTSfiULQU7VfiHJWfiHJU7MOLX<_;`RbpYZULS4[(\TXOn HMW5Kt|LKtS x

rHY w H~VaKN[!QTWhWfiqKNM
SfiKtWfi\S4

zrHY:]U7MkWfiOLHMTVROLMKN[!QTWhtXOL\TV5KWfiqKNM
SfiKtWfi\S4Mcn
zrHY:]U7MkWfiOLHMTVROg\TMTHWatX~OL\TV5KP jWfiqKNM
w ( ff w G (7
SfiKtWfi\S4M_;d& vw (N x
rqUzU7V5KOPI

S4HO
sXl
K cULY]
w ( ff w (7
w ff N w (7
S4KtWfi\S4M_ad vw ( _
x : _ad vw x
MhHW5KtS4O
WfiHJU7MULY_adyKNXJKtW5KNVWfiqKqU7V5KNMIAO
S4H~O
sXJKLeOLMyjV5UeASfiKNyT\TKNVWfiqK Mz\T[sKtSULYnIAO
S4H~O
sXJKNV
ULY] skhO
W{XJKNOLV5W;U7MKLrqTHVf|7\TO
S4OLMkW5KtKNV{WfiqTO
WR_;d&W5KtS4[gHMTO
W5KNV v KNHJWfiqKtS;U7MV5W5KtQULS;U7MVfiW5Kt
Q x
=4D
l>

@@ E @

sE


WfiqKOLMTOLXhzVfiH~VULYWfiqTK&OoILKtS4O
|LKS4\TMMTHM|WfiH[gKRULYWfiqTK&Y\TMTWfiHU7M_adpifK&YZU7XXJUiGU7XysKtSfi| v N7
x
OLMTyGU7XysKtSfi|PKtWrOLX v NL x eirqU!V5Wfi\yTHJKNyWfiqKjOoILKtS4O
|LK(S4\TMTMTH~M|jWfiH[gKULYld

f H;#= + #'

KtWR OLMTyccyKNMTULW5KrWfiqK&Mk\T[sKtSULYtXOL\V5KNV OLMygI

S4HO
sXJKNVULY]ezSfiKNV5QKNWfiHJILKNXhL VfiVfi\[!KrWfiqTO
WfOLXX

WfiqKPXHJW5KtS4OLX~VRqTOoILKPWfiqK(V4OL[!KPQTSfiULsO
sH~XHJWh]W5UO
QTQKNO
SHMKNOLqtXOL\TV5KULY;] v UV4H[!QXHYhWfiqK
OLMTOLXJhnVfiHVri KPOLXXJUi] W5UU7MWfiOLH~My\QXHtO
W5KtXOL\TV5KNV&OLMTyWfiOL\W5U7XJUL|7HJKNVt x VfiVfi\T[!K(OLXV5UWfiqTO
W&WfiqKtSfiK
HV&MU}yTKtQKNMyKNMThOL[!U7M|WfiqK(Uztt\S4SfiKNMTKNVRULYyTH KtSfiKNMkWrI

S4HO
sXJKNVROLMTyOL[!U7M|WfiqKUntt\SfiS4KNMTKNV
ULYWfiqTK(VfiOL[gK(I

S4HO
sXJKHMyTH KtSfiKNMkWtXOL\TV5KNVtbULSO|7HJILKNMyHV5W5S4HJs\WfiHJU7MULE
eXJKtW& v x yTKNMULW5K
WfiqKjOoILKtS4O
|LK(S4\TMTMTH~M|jWfiH[gKULY_;d& vw x irHWfiq]U7MkWfiOLHMTHM| tXOL\TV5KNVt
Y] U7MkWfiOLHMTVtX~OL\TV5KNVPU7MI

S4HO
sXJKNVteWfiqKNMPsU7\MTyTVWfiqKcVfiHJtKULY] OL4qV5W5KtQULY
Y\TMTWfiHJU7M_adtOLMcsKOLtU7[!QXH~VfiqKNyyT\SHM|PO
Wr[!U7V5W{Wi UQOLVfiV5KNVrUoILKtS]
N LkN#c

wf #SntSnhfic~K4L#
#ThSL
!]4
L
tPfiLzAjSnPZR(zfitZk
hRL
#nN
4+%



W;[gHJ|7qkW sK&WfiqU7\|7qkWteWfiqTO
W;HMULS4yKtSaW5UPULsTWfiOLHMO(|LUkUnysU7\TMTy}U7M}WfiqTKOoILKtS4O
|LKS4\MTMTHM|jWfiH[!K
ULYaYZ\MTWfiHJU7M_adeU7MKg[(\TV5WjV4qUoiWfiqO
WWfiqK\TMHJW&tX~OL\TV5KgS4\XJK!HVO
QTQXHKNyYZSfiKNz\KNMkWfiXJhLeVfiH~MTK!WfiqK
V5QX~HJW5WfiHM|(S4\TXJKHVaWfiqKjU7MKWfiqTO
WrHMkW5SfiUnyT\TKNVfWfiqKQU7VfiVfiHsHXHJWh!ULYKnQU7MKNMkWfiHOLXXJhXU7M|gU7[!Q\WfiO
WfiHU7MTVt
KVfiqUoi^WfiqTO
W KtILKNMHYU7MXJhPWfiqKRV5QXHJW5WfiHM|S4\XJKrHV
QTQXHJKNye7XJUoi^sU7\TMTyTVO
S4KrULsTWfiOLHMTKNy<:{qKtSfiKtYZULSfiKLe
ifKOLVfiVfi\T[gKgWfiqTO
WjMU\TV5KgULYaWfiqK\MTHJWtXOL\TV5KS4\TXJK!HV[OLyKLeOLMTyOLMTOLXJhztKOI

S4HOLMkWULYr_adeHM
irqTH~4qcV5W5Kt3
Q !HVrU7[gHJW5W5KNy<6
&skInHJU7\TV4XJhLeWfiqTHV{qTOLM|LKPyUzKNV{MULWRwO KNWRKN[g[gO
K(OLXV5UOLVfiV4\T[!K
WfiqTO
W{WfiqTKjXHJW5KtS4OLXYZULSrWfiqKVfiQXHJW5WfiHMT|S4\XJKHV{4qTU7V5KNMcS4OLMyU7[gXJhL
KtW{sK;WfiqKRXHJW5KtS4OLXnWfiqTO
WfqTOLV:sKtKNMgqU7V5KNMgYZULSWfiqTK{V5QXHJW5WfiH~M|S4\TXJKLq
InHMTKaWfiqKrQTSfiULsO
sH~XHJWhjWfiqTO
W
xB
Untt\S4V:H~M(OtX~OL\TV5K{H=
V ekWfiqKaQTSfiULsO
sH~XHJWhWfiqO
6
W \jU7\WlULY tXOL\TV5KNV:U7MkWfiOLHMHV v x v +

uZ

fi

RRT

{qKVfiOL[gKqU7XyTVfYZULS {qKMk\[jsKtS;ULYltXOL\TV5KNVaULYYULS[\TX~OLV w ( e w QTSfiUnyT\TKNyskhV5QXHW5WfiHM|g]HV
KNz\TOLXW5U(WfiqKMk\[jsKtSULYOLXXtXOL\V5KNVfULY][HMk\VWfiqKMk\[jsKtSULYtXOL\TV5KNV ULY]irqTHqU7MkWfiOLHMWfiqK
XHJW5KtSOLXle eSfiKNVfiQKNWfiHILKNXJhL

_t#(nA w w ]5N
T4Zk] 4L
Z( ^t
zt]:S
7
5VZ" v x v x B (
InH~MTK;KNOL4q}ULY w ( e w {U7MkWfiOLHMTV
W[!U7V5W {IAO
S4H~O
sXJKNVteLWfiqKRYU7X~XJUoirH~M|SfiKNt\SfiSfiKNMKrHMKNz\TOLXHJWh
YZULS; v x qU7X~yTVt
v x

# :
BYN#( v x v x B

5 BYNr* v x v x B


v
v

^
^

Rx :
x




`


ff ] P ff

v x


qKgS4V5WW5KtS[ v # x HVjWfiqKgWfiH[gKMKtKNyKNyYULSU7MK}HW5KtS4O
WfiHJU7MULYr_adrqKV5KNU7MTyOLMTyWfiqTHJS4y
{
W5KtS4[gV(O
SfiK}WfiqKKnQKNW5KNyWfiH[gKNVjMKtKNyKNyYZULSU7[!Q\TWfiHM|cWfiqKMk\[jsKtSjULY{[gUzyKNX~VULY w ( OLMTy w
e
SfiKNV5QKNWfiHJILKNXhL6
InH~MTKWfiqKX~HJW5KtS4OLXlTO
QTQKNO
S4V{H~M]eHJWrH~V{H[!QU7VfiVfiHsXJK&WfiqTO
WrWfiqKMk\T[sKtS{ULY:tXOL\TV5KNVRHM
irqTH~4qcUntt\S4VfHVneOLMTy}V5UezWfiqKRVfi\[g[gO
WfiHJU7MULYWfiqKRVfiKNU7MTyW5KtS[*V5WfiO
S4WfiVfO
W&
{qK&XHJW5KtS4OLX [gONh
MULW;O
QTQKNO
S{O
W;OLXXHMc] v HJYHVfQ\SfiK x _fU7MV5KNk\TKNMWfiXJhLenWfiqKVfi\[g[gO
WfiHJU7MULYWfiqTK&WfiqHJS4y}W5KtS4[sKt|7HMTV

Wrn
{qTK&SHJ|7qW2qOLMTyVfiHyK&ULY v x UILKtSfiKNV5WfiH[gO
W5KNV{ v x VfiHMKLekS4V5WtenWfiqKMk\[jsKtSaULYI

S4HO
sXKNV ULY
e
V L;gAl
eOLMTy<e<V5KNU7My<e<YZ\MTWfiHJU7M_ad[gOohV5W5ULQirqTKNM] U7MWfiOLHMVOLMKN[!QTWh
w ( w PH
tXOL\TVfiKLeKtILKNMsKtYZULSfiKP ULSsKNU7[!KNV{n
UgKNV5WfiH[O
W5K v x eTifKjVfiqTUoieS4V5WteTWfiqTO
WrYULS ff ze v x ff v x eOLMTy<eVfiKNU7MTy<e
WfiqTO
W{YZULSrOLMk[
h e v x ff v x irqKtS4

K ff +( ( .
@
4 ff
H[*3#'=ki'


bU7XXUoirHMT|GU7XysKtSfi| v N7
x eifKV5WfiO
SfiWirHJWfiq^WfiqKOLMOLXJhzV4HVULYWfiqKOoILKtS4O
|LKS\TMTMTHMT|WfiH[!KYZULS
ff z MWfiqTHVatOLV5KLeKNOL4qI

S4HO
sXJK&Untt\S4V;HM}KNOL4qctX~OL\TV5KQU7VfiHJWfiHJILKNXJhLenMKt|7O
WfiHJILKNXhULS{MULW;O
W;OLXX
irHJWfiqWfiqTKjVfiOL[!KQTS4ULsO
sHXHWh v x %IzUeTSfiKNt\TSfiSfiKNMTK v x WfiO
mLKNVRWfiqKYZULS4[
v x

:
v x B v xR:

5 BYN#( v xv ( x v x B



H ] ff ( <

v x
S4UkULYlHVa|7HJILKNMHM QQKNMyTH

ff

v x

v

^

`

x


ff A( ff

v x


H[*3#'=ki' 4r
{qKOLVfiV4\T[!QTWfiHJU7MULY ff
HVPU7[g[!U7MTXhOLyULQTW5KNyH~MQSfiULsO
sHX~HV5WfiHgOLMOLXJhzV4HV(ULYROLXJ|LULS4HWfiqT[gV
qTOLMTyXHM|c_a`&bULSjd`RbYZULS4[(\TXOLV v bSOLMTU OL\XXeNLz fG&U7X~ysKtSfi|eN7
z G&U7XyTsKtS4|KtWjOLXJe
uZZ

fiy w zR{d|,'}RR8|,~$R]wl$'}R 3 ~ff
NL x eLirqTH~4q[!KNOLMTVWfiqTO
WYZULSKNOL4qI

S4HO
sXJKLeAHJWfiV<Untt\SfiSfiKNMKHMO{tXOL\V5K:irHJWfiqULSirHJWfiqU7\WMTKt|7O
WfiHJU7M
ULS{MU7MnFUntt\SfiS4KNMTKLeTOLXXqTONILKWfiqTKVfiOL[!KQTSfiULsO
sHXHJWhLpRUoifKtILKtSNeHMSfiKNOLXtOLV5KNV{WfiqTH~Vf[ONhMULW;qU7Xy<e
V5U(i KOLMTOLXJhztKrWfiqTK&OoILKtS4O
|LKS4\TMTMHM|WfiH[!KRULY_adpYULSaOLMkhgQTSfiULsO
sHXHJW

h !ULYO(XHJW5KtS4OLXUztt\S4SfiKNMTK
HMOgtX~OL\TV5KULY]
fKtYZULSfiKOLMTOLXhkNHMT|WfiqKcSfiKNt\S4SfiKNMTKcKNz\TO
WfiHJU7MXJKtW\TVg[O
mLKWfiqKcYZU7XXJUirHM|KNV5WfiH~[gO
WfiHJU7M<^{qK
Y\TMTWfiHJU7M_ad v iRHJWfiqU7\W;WfiqK\MTHJWatXOL\TV5KS4\XJK x [gONhsKSfiKt|7O
S4yKNyOLVROLMOLXJ|LULS4HJWfiqT[iRqTH4qV4tOLMTV
OcsHMTO
SfihW5SfiKtKLc{qKSfiUkULWjULY;WfiqKW5SfiKtK}SfiKtQTSfiKNVfiKNMWfiVWfiqKHMTQ\WYZULS4[\XO]{qTK4qTH~XySfiKNMULY;KNOLq
HMkW5KtS4MTOLXlMUnyKPSfiKtQTSfiKNVfiKNMWWfiqKPYZULS4[(\TXOLVULsTWfiOLHMTKNyOLVOSfiKNVfi\TXWRULYaO
QTQXJhnHM|}WfiqKgV5QX~HJW5WfiHM|}S4\TXJK(W5U
WfiqKYZULS4[(\TXO!O
W{WfiqTO
WRHMW5KtSMTOLX<MUnyKLeTOLMTyWfiqKjXKNONILKNV{ULYlWfiqKW5S4KtKSfiKtQTSfiKNVfiKNMW{KN[gQTWhYULS[\TX~OLV{OLMTy
YZULS4[\XOLVairqTHqcU7MkWfiOLHMcOLMcKN[gQTWhtXOL\TV5KL
YOLMHMkW5KtS4MTOLXMUnyKSfiKtQTS4KNV5KNMWfiV;OPYZULS4[(\TXO(irHJWfi
q \}tX~OL\TV5KNVtenWfiqKKnQKNW5KNycMk\[jsKtSaULYtXOL\V5KNV
x :{qK;KnQKNW5KNygqKNHJ|7qkWlULYWfiqK;W5S4Kt)
HM(KNOL4qULYHWfiV:4qTHX~ySfiKNMPHq
V ^ v +
K PH~VlWfiqKtSfiKtYZULSfiK{O
sU7\W:XJULV| z
W HV{O
sU7\
W e
irqKtS41
K ff ( ( f{qKjMz\T[sKtS;ULYlMTUzyKNVRHMOU7[gQXJKtW5KsH~MTO
SfihW5SfiKtKULY:qKNHJ|7qk)



OLMTyWfiqHV{HVaWfiqKjKzQKNW5KNyMz\T[sKt6
ULY:HJW5KtSO
WfiHJU7MTV;ULYlWfiqKY\TMTWfiHU7M_ad IzU*e ff ff AK
Le
(
irqKtS41
K ff +( . &{qTKjS4\TMMTHM|!WfiH[gKULYKNOLqHJW5KtS4O
WfiHJU7MHV *v x irqKtSfiK HVRWfiqKPMk\[jsKtS
ULYtXOL\V5KNVaOLMTy @
HV WfiqKMz\T[jsKtS ULYI

S4HO
sXJKNV ULYWfiqKYZULS4[(\TXOWfiqO
WaH~VW5SfiKNO
W5KNyO
WfWfiqO
WfHJW5KtSO
WfiHJU7M<

v x ff v x qEhfi+ ff +( ( .
@
S4UkULYlHVa|7HJILKNMHM QQKNMyTH
YZKti SfiKN[gO
S4mzVO
SfiKHMULSyKtSN}blHJSV5WteifKOLVfiVfi\[!KNyWfiqTO
WjWfiqKgQTSfiULsO
sHXHJWhULYaUntt\SfiS4HMT|cHMO
tXOL\TVfiKjHV;WfiqK(VfiOL[!KYULS&OLXXXHJW5KtSOLXVt YlWfiqTHV{H~V{MULWrWfiqTKjtOLV5KLeWfiqKNM{qTKtULSfiKN
[ qU7XyTV;YZUL6
sKNHM|
WfiqKjXUoifKNV5W{Untt\SfiSfiKNMTKQSfiULsO
sHX~HJWhOL[gU7M|OLXX<XHW5KtS4OLXVaULY] IzKNU7MTy<eH
H~V{OLVfiVfi\T[gKNyU7MTV5WfiOLMkWte
WfiqKNMWfiqKPVfiHJtKPULYtXOL\V5KNVteirqTHqH~VrULY ULS4yKt

:e<HVRQTSfiULQULSfiWfiHJU7MTOLXW5UWfiqK!Mk\T[sKtSRULYIAO
S4H~O
sXJKNVte
OLMTycqTKNMTKLeHV;Vfi\TQTQU7V5KNyW5U4qTOLMT|LKjirHJWfiq

H

> 3'

@ E@

vHo

{qTH~V;V5KNWfiHJU7McyKNV4S4HJsKNV;V5U7[!KS4KMKN[!KNMkWfiV;W5U_adH[!QTSfiUIzH~M|!HJWfiVaQKtSfiYULS[gOLMTKL
^$$4[Z#'X'#
f #w

{qKYZ\MTWfiHJU7M^_;d&eaXHJmLKcWfiqTKdONInHV2 \WfiMTOL[ QTSfiUnKNyT\SfiKLea|7HJILKNVMUSfiKNU7[g[!KNMyTO
WfiHJU7M^qUoi W5U
4qTUkU7V5KOpIAO
S4H~O
sXJKYULScOVfiQXHJWt9
prUi KtILKtSNeOp|LUkUny4qTU7HKULY(WfiqTHVIAO
SHO
sXJK[gOohSfiKNy\TKWfiqK
S4\TMMTHM|}WfiH[!KPULY WfiqTK!YZ\TMWfiHJU7M<P
h |LUzUzyqU7HK ifKg[!KNOLMU7MTK!WfiqTO
WtOL\TV5KNVWfiqTK!YULS4[(\TXOLV w (
OLMTy w W5UgsKOLV{Vfi[OLXX<OLV{QU7VfiVfiHsXJKL
{qTK{VfiHJtKaULYWfiqTKNV5K{YZULS4[\XOLV:HV:yKtW5KtS4[HMKNy(skh(sULWfiq*OLMTy V[gH~MTH[gHJNH~M|{WfiqK{Mz\T[sKtS:ULY


S4HO
sXJKNVHM w ( e w tOL\TV5KNVlOLM(\TwM fi\TV5WfiH KNyU7[!Q\WfiO
WfiHU7MjUILKtS4qKNOLy<e7ifKaU7MTKNMW5SO
W5KNyPU7MjSfiKNy\TtHM|
WfiqKaMk\[jsKtSULYtXOL\TV5KNV ( e
cHM w ( e w
e
KNHJWfiqKtSskhj[HMTH[gHNHM|a ( : c v szhj[!KNOLMTVULYqUkU7V4HM|
OI

S4HO
sXJKrO
QTQKNO
S4HMT|jHMO[gOAH[gOLXMz\T[sKtSULYtXOL\TVfiKNVULY] x ezULS szhg[gHMTH~[gHJNHM|&K
k v (N c x
{qKXO
W5W5KtSRtOLMsKOL4qTHJKtILKNyszh[gOAnH~[gHJNHM|PUILKt+
WfiqKk\TOLMkWfiHJWh
F v L v x ehv x5x eiRqKtSfiK
L v x&v ehv x5x yKNMULW5KNVaWfiqTKMz\T[sKtSfULYtXOL\TV5KNVfULY:]HM}irqTHq cUztt\TS4Va\TMTMKt|7O
W5KNy v MKt|7O
W5KNye
SfiKNV5QKNWfiHJILKNXh x rqKNV5KWi UO
QTQTS4U7OL4qKNVP[gOohsKU7[jsHMKNy< MTyKtKNy<elKnQKtS4H[!KNMkWfiVSfiKtQULSfiW5KNyHM
IzKNWfiHJU7M}VfiqUoifKNysKNV5WrQKtSfiYZULS4[gOLMTK(HJYlWfiqKPV5QXHJWrIAO
S4H~O
sXJKjqTOLVRsKtKNMqU7V5KNMyT\KW5U}
v ( :
c x es\WRHJYWfiqKtSfiKPqTOoILKsKtKNM[gULSfiKjWfiqTOLMU7MK(Vfi\T4qI

S4HO
sXJKLeWfiqKNMO}Y\SfiWfiqKtSrqU7HKPOL[!U7M|WfiqK
U7[!QKtWfiHM|!I

S4HO
sXJKNV;qTOLV;sKtKNMc[gOLyTKyT\KW5Ug
F<K
k v (t c x

uZ

fi

RRT

V;qTOLVasKtKNMVfi\|L|LKNV5W5KNycskhOLMOLMU7Mhn[!U7\TVaS4KtIzHJKtifKtSNeszh}\TV4HM|(QTSfiULQKtS;yTO
WfiO!V5W5S4\TWfi\TSfiKNVtenWfiqK
U7[!Q\TWfiO
WfiHJU7MULY L v x OLMTy ehv x YZULS{OLXXIAO
S4H~O
sXJKNVDctOLMsKyU7MKK}tHJKNMkWfiXJhLetOL\TVfiHMT|U7MTXh}O
Vfi[gOLX~X<U7[!Q\WfiO
WfiHJU7McUoILKtS4qKNOLy


=''=w

'#ff

#$

{qKPY\TMTWfiHJU7M_adVfiW5ULQVirqKNM]U7MkWfiOLHMTVjOLMKN[!QTWhtXOL\TVfiK(ULSiRqKNM]HV&KN[!QTWhL^qTO

]U7MVfiHV5WfiV;ULYU7MTXJhU7MKtXOL\V5KjU7MkWfiOLHMTHM
| \XHJW5KtS4OLX~V {qKVfiQXHJW5WfiHMT|S4\XJKirHXXsKO
QTQXHKNy}W5U](e
SfiKNO
WfiHMT| w ( OLMTy w Vfi\T4qcWfiqTO
W;U7MKULYWfiqTKN[[gONhsKKN[!QTWhLes\WaWfiqKULWfiqKtSrV5WfiH~XXU7MVfiHV5WfiVaULYU7MK
tXOL\TVfiKHMILU7XIzHMT_
| fi\TV5W;U7MTKIAO
S4H~O
sXJKXJKNVfiV;WfiqOLM]r IzUeTV5QX~HJW5WfiHM|(ULYWfiqTKjVfiHM|7XKtX~OL\TV5KULY]irHX~XsK
SfiKtQKNO
W5KN
^WfiH~[!KNVt
V4H[gHXO
S;QSfiUzKNV4VrULYH~MK}tHJKNMkWRV5QXHJW5WfiHM|!WfiO
mLKNVQXOLKOLXVfiUU7MYULS[\TX~OLVrU7MTVfiH~V5WfiHM|gULYSfiKNXOA
WfiHJILKNXJhYZKti*tXOL\V5KNVt WjWfi\TS4MTVU7\WWfiqTO
WjWfiqK}OLXJ|LULS4HJWfiqT[QTSfiKNV5KNMkW5KNyszhULNHMTVfimzHH v NL x HVj[!ULSfiK
K}tHJKNMkWYZULSqTOLMTyXHM|aVfi[gOLXX
V5KtWfiVULYtXOL\TV5KNVt nQKtS4H[!KNMkWfiVU7MS4OLMTyTU7[gXJhrqU7V5KNMYZULS4[(\TXOLVV4qUoifKNy
WfiqTO
WaYZULS YZULS4[(\TXOLVfirHJWfiq}XJKNVfiVfWfiqTOLMtX~OL\TV5KNVfWfiqKOLXJ|LULS4HJWfiqT[*ULYULNHMTV5mnHH v NL x QKtSfiYZULS4[gV sKtW5W5KtS
WfiqTOLMYZ\TMWfiHJU7M}_adHMgHJWfiV:ULSHJ|7HMTOLXYZULS4[l{qKtS4KtYULSfiKLekirqKNMWfiqKrMz\T[jsKtSULY<tXOL\V5KNVULY<OYZULS4[(\TXO
sKNHM|PQTSfiUnKNVfiV5KNyHVaSfiKNyT\TKNy\TMTyKtSrze_;d&S4\TMTVaWfiqTKOLXJ|LULS4HJWfiqT[ULYULNHMTV5mnHH v NL x

dn

l>
fiff $E

@



JX |LULS4HJWfiqT[_;d&QTSfiKNV5KNMkW5KNyHMWfiqTKQTSfiKtInHJU7\TV<V5KNWfiHU7MTVqTOLV<sKtKNMQTS4UL|LS4OL[g[!KNyH~M !e
OLMTyS4\M}U7Mf_&enirHJWfiqWfiqKQ\SfiQU7V5K&ULYyKtW5KtS[gHMTHMT|OLMK}tHKNMW;iaONhULYqUzU7VfiHM|gOPI

S4HO
sXKRYZULS
WfiqKPV5QXHW5WfiHM|gS4\XJKLeTMyTHM|OLMO
QTQTS4ULQTS4HO
W5K(Mk\T[sKtSRULY tXOL\TV5KNVHMOYZULS4[(\TXO}YULSV5irHJWfiqTHM|}WfiqK
Y\TMTWfiHJU7M_adW5UWfiqK(OLX|LULS4HJWfiqT[ ULYULNHMTV5mnHH v NL x eOLMTyOLVfiV5KNVfiVfiH~M|}WfiqTK(OLWfi\TOLXlS\TMTMTHMT|!WfiH[!K
ULY_ad
MjWfiqTKfKnQKtS4H[!KNMkWfiVte_adqTOLVsKtKNM(O
QQXHJKNyjW5URS4OLMyU7[gXJhQTS4UzyT\KNyjV5KtWfiVlULYtX~OL\TV5KNVirHJWfiqWfiqK
YZU7XXJUoiRHM|!QO
S4OL[gKtW5KtS4Vtt]
5^ 5s
Lne:t]
5^
5
OLMy ( k7eirqKtSfiK ( e
yKNMULW5KWfiqKQTS4ULsO
sHXHWh!ULYO(I

S4HO
sXKRW5UPUntt\S{HMOPtXOL\TV5K\TMTMKt|7O
W5KNyULS{MTKt|7O
W5KNy<eTSfiKNV5QKNWfiHJILKNXh
v ( OLMTy
S4KMULWgMKNKNVfiV4O
S4HXJhWfiqKcVfiOL[!K x bTULSgKNOL4q^U7[jsHMTO
WfiHJU7MpULY&WfiqKcQO
SOL[!KtW5KtS4VtetL
S4OLMTyTU7[ HMTV5WfiOLMTKNVULY;]qONILKgsKtKNMQTSfiUnKNVfiV5KNyPKtSfiYZULS4[gOLMKgULY;_;d& qTOLVsKtKNM[!KNOLVfi\TSfiKNyskh
HJWfiV;S\TMTMTHMT|jWfiH[!KOLMyskhWfiqKMz\T[sKtS;ULYS4KNt\S4VfiHJILKtOLXX~V;W5UgWfiqKj[OLHMYZ\MTWfiHJU7M<
K(qTONILKULsV5KtSfiILKNycWfiqO
W{YULSrKNOL4qcQOLHJS;ULYlQTSfiULsO
sH~XHJWfiHJKNV (t AeTWfiqKtSfiKH~V{O!WfiqSfiKNV4qU7XyMk\[jsKtS
ULY{tXOL\TVfiKNV
elVfi\T4qWfiqTO
WjYZULSj `
WfiqKqTO
S4ytOLV5KNVPXHKgO
SfiU7\TMTyOULSfiSfiKNV5QU7MTyHM|cMz\T[jsKtSULY


S4HO
sXJKNVNbULS(HMTV5WfiOLMTKLeWfiqK!qTO
S4ytOLV5KNVjYULS ( ff ff gOLMTy ff
cO
SfiKgXUztO
W5KNyO
S4U7\TMTy
ff
nblHJ|7\S4KNV{rOLMT[
QTSfiKNV5KNMkWWfiqKRONILKtS4O
|LKMz\T[jsKtSULYSfiKNt\TS4VfiHJILK{tOLXX~V:QKtSfiYULS[!KNy!szh!_adHM
WfiqKqTO
S4y}tOLV5KNVtULSfiKyTO
WfiOO
S4KR|7HJILKNMHMO
sXJKNVR#jULY QTQKNMTyTH P&nbTU7XXJUoiRHM|jO(Vfi\|L|LKNV5WfiHU7M}ULY
U7MKaULYWfiqTK SfiKtInHJKtifKtS4VteLifK;4qKN4mLKNygWfiqKaV5WfiOLMTyTO
Sy(yTKtIzHO
WfiHU7M(H~MOLXXztOLV5KNV v VfiKtKaO
sXJK{RHM QQKNMyTH
x XXWfiqKjKnQKtS4H[!KNMkWfiV{S4OLMU7MOQKNMWfiH\[sOLV5KNyf_ v
pR x R{qK(OLWfi\TOLXS4\TMTMTH~M|PWfiH[!K
ULY_adS4OLMT|LKNyYS4U7[XJKNVfiV;WfiqOLMU7MKV5KNU7MTyW5U}!qU7\TS4Vt
b U7X~XJUoirH~M|WfiqKgyTHV4UoILKtSfihULYfqO
S4ytOLV5KNVYZULSI v HJWfiqKNXX:KtWjOLXFJeNL x eifKgS4OLM_;d&U7M
OVfiKtS4HJKNVRULY:S4OLMTyU7[V5KtWfiV&ULYHXHJW5KtS4OLXtXOL\TV5KNVRirHJWfiqWfiqTKS4O
WfiHUULY: YZSfiU7[n gW5Uz nRbULSKNOLq
U7[sHMTO
WfiHJU7MULY OLMTy:*e
LPHMTV5WfiOLMTKNVfifKtSfiKU7MTVfiHyTKtSfiKNy<:blHJ|7\SfiK)|7HJILKNVfWfiqK[!KNyTHOLMMk\[jsKtS
ULYS4KNt\S4VfiHJILKPtOLXXVRQKtSfiYZULS4[!KNyskh_adYULSOVfiOL[!QXJKULY ff
7
nPULSfiKgyO
WfiOH~MO
sXJKg

uZXu

fiy w zR{d|,'}RR8|,~$R]wl$'}R 3 ~ff
#%
#$
##
#"
'&
'%
1. 8 9 . / '$
5 ) * + ,. / 0 2 113 '#
'"
&

: ; < = > ?= @ AB C E F G E H G ;
J ; < = > ? = @ B C E F G E H G ; IH
H ; < = > ?= @ B C E F G E H G ; IJ

%
$
#
"
"

#"

$" %"
&" '"" '# " '$ " '%"
( ) * + , - . / 0 12 ) 3 , 3 45 67 , /. - * ) 12

'&"

#""

blHJ|7\SfiKg
ILKtS4O
|LKPMk\[jsKtS;ULYSfiKNt\S4VfiHILKtOLXXV;YZULSryTHKtSfiKNMkW;IAOLX~\KNVaULY



LK
PO
PN
PM
PL
ZW b W X
^ R U V PK
W X [ ZZ\

N

c e f g h f j k l n p q n r p qr
e f g hf jk l n p q n r p qs
r e f g h f j k l n p qr n r p qs


L
K
K

LK

K N K OK PK K PL K PM K PN K
Q R U V W X Z[ R \U \ ]^ _` U XW V R Z[

POK

LKK

blHJ|7\TSfiKlz ILKtS4O
|LKMz\T[sKtS;ULYlSfiKNt\TS4VfiHJILKtOLXX~VaYULSryHKtS4KNMW;I
OLX\KNV;ULY



v U7MWfiHMz\KNy x

ULY QTQKNMTyH }& v KNyHOLMcMz\T[jsKtS{HV;V4qUoirMW5UKNMTO
sXJKU7[!QO
S4HV5U7MirHWfiqWfiqKSfiKNVfi\TXWfiVaQTSfiKNV5KNMkW5KNy
zs hHJWfi4qTKNXX<KtWrOLXJeNL x

uZZv

fi

RRT

~

u v } ~~
uu
ut





yx
yw
yv
yu
yt
x
w
v
u





u

z
v
{
w
} ~ ~

|

x

blHJ|7\SfiK
z KNyTHOLMMz\T[sKtS;ULYS4KNt\S4VfiHJILKtOLX~XVaYULSXHJW5KtS4OLX<YZULS4[(\TXOLVt
MOLX~XKnQKtSH[!KNMkWfiV{irHJWfiqU7\TMkWfiHM|}[gUzyKNX~V{ifK(qTOoILKPULsV5KtSfiILKNyOQqKNMU7[!KNMU7MO
mzH~McW5UWfiqTO
W
4 qO
S4OLW5KtS4HV5WfiH~ULY:4qKN4mzH~M|VfiO
WfiHV5O
sHXHWhLeTMTOL[!KNXhLeTYULS&O!|7HJILKNMXJKNM|LWfiqULY:tXOL\TVfiKNV{neTWfiqKS\TMTMTHMT|
WfiH[!K(ULYOLXJ|LULSHJWfiqT[ _adSfiKNOL4qKNVHWfiVR[gOAH[(\T[
W&OKtSfiWfiOLHMI
OLX\K(ULYWfiqKPtXOL\TV5KNV2FW5U
FI

S4HO
sXJKNV
S4O
WfiHJU:bULS: ff v blHJ|7\S46
K x WfiqH
V 5qTO
S4y RS4O
WfiHJUHVa
zeiRqTHXJKfHJWqOLVlsKtKNM(SfiKtQULSfiW5KNyPHMP[gOLMkhjifULSfimnV
Kv L |JeHWfi4qKNXXTKtW OLXJeNL x WfiqTO
WfqTO
S4ygtOLVfiKNVUL4
XHJK{O
SfiU7\TMy! ff z{qHV:VfiqTHJYZW:ULYWfiqK
[gOAH[(\T[ S\TMTMTHMT|&WfiH~[!KRW5U(WfiqKXJKtYZW v YZSfiU7[ ff W5U ff x tOLMsKrKnQXOLHMTKNygszhgWfiqK
YOLWWfiqTO
WirqKNMOcQTSfiUL|LSOL[ YZUL
yTHV4UoILKtS4V(OV4O
WfiHV5YZhzHMT|cOLVfiVfiHJ|7MT[gKNMWte:HJWjVfiW5ULQVjQSfiUzKNV4VfiHM|e
irqTH~XJK!U7\TMWfiH~M|[!UnyKNXVqTOLVjW5U|LUcU7MUoILKtSPOLXXVfiO
WfiH~V5YhnHM|cOLV4VfiHJ|7MT[!KNMkWfiVt
IzUeWfiqK}qTHJ|7qKtSjH~VWfiqK
QTSfiULsO
sHXHJWhWfiqTO
WOgYZULS4[(\TXOgH~VrVfiO
WfiHV5O
sXJKLeOLMycWfiqK([!ULSfiK[gUzyKNX~V{HJWRH~VrXHJmLKNXh}W5UqTONILK v OLMTyWfiqTHV
HV;iRqTO
W{qTO
QTQKNMTV;HJYWfiqKS4O
WfiHUg yKNSfiKNOLV5KNV x eWfiqKjXU7M|LKtSr[!UnyKNX<U7\TMkWfiHM|gWfiO
mLKNV&U7[!QO
S4KNyW5U
4qTKNfimnHM|VfiO
WfiH~V2O
sHX~HJWhL
l>

}B @ 'D CE @ 83 or9

KPqTONILKQTSfiKNV5KNMkW5KNyOLMOLXJ|LULS4HJWfiq[

v _ad x YULS&U7\TMWfiH~M|}[!UnyKNXV{ULYlOQTSfiULQU7VfiHJWfiHU7MTOLXYULS[\TX~O](
{qKQSfiKtIzHU7\TV!V5KNWfiHJU7MTVU7MTVfiHyKtS4KNy_a`RbYZULS4[(\TXOLVtefqUoifKtILKtSNe{HJWgHVPKNOLVfihW5UU7MTV5W5S4\TW}OyT\TOLX
OLXJ|LULS4HWfiqT[YULS&d`RbpYZULS4[(\TXOLVt
_fU7[gQO
S4HM|ROLXJ|LULS4HJWfiqT[_adirHJWfiqWfiqO
WULY ifOL[gO v NLL x OLMTy(ULNHMTVfimzHH v NL x e7i KaMULW5K WfiqTO
W
sULWfiqqTONILKjOPU7[g[gU7McKNMW5SOLXYKNO
Wfi\S4KLWfiqK[!ULSfiKI

S4HO
sXKNV qTOoILKsULWfiqMKt|7O
W5KNycOLMTy\TMMKt|7O
W5KNy
Untt\SfiSfiKNMTKNVHMWfiqTKtXOL\TV5KNVULYTO{|7HJILKNMjYULS[\TX~OneWfiqK sKtW5W5KtSHVWfiqKQKtSfiYZULS4[gOLMTK:ULYWfiqTK OLXJ|LULS4HWfiqT[gVt
{qTH~VrYZKNO
Wfi\SfiKgHVOLXVfiUcU7[g[!U7MW5UWfiqKgOLXJ|LULSHJWfiqT[gV&QTSfiKNVfiKNMW5KNyszhd\sU7HV v NLn x OLMysz
h fiqTOLM|

uZ

fiy w zR{d|,'}RR8|,~$R]wl$'}R 3 ~ff
v NLL x MU7MkW5S4OLV5WPW5U4qKN4mzHMT|VfiO
WfiH~V2O
sHX~HJWhLeQ\SfiKXHW5KtS4OLXVVfiXUoiyUoirMU7\TMkWfiHM|[gUzyKNX~Vskh
WfiqKNV5KOLXJ|LULS4HJWfiqT[Vt
{qTKgKzQKtS4H~[!KNMWfiVj[gOLyKgirHWfiqOLXJ|LULS4HJWfiqT[ _ad qTOoILK}VfiqTUoirMWfiqTO
WjHJWQKtSfiYZULS4[gVsKtW5W5KtSWfiqTOLM
WfiqTO
WULY<ULNHMTV5mnHH v NL x H
pRUoifKtILKtSfWfiqTKr[gOLHM!OLyI
OLMkWfiO
|LKrULY_adHVHMPWfiqKrOL[!U7\MWULYV5W5ULS4O
|LK&HJW
SfiKNz\THJSfiKNVN M!WfiqK{KnQKtS4H[!KNMkWfiVlWfiqTO
WfqTONILKRsKtKNMgQKtS4YULS4[gKNyPskhPULNHMV5mzH~H v NL x YZULS 5tLOLMTy
5n
neWfiqKtSfiKifKtSfiKjtOLVfiKNVrirHWfiqWfiqU7\TVfiOLMyTVaULY:tXOL\TV5KNVRV5W5ULSfiKNyy\S4HM|(WfiqKS\TMULYlWfiqKjOLXJ|LULSHJWfiqT[
InHMKgOLXJ|LULS4HJWfiqT[ _adQKtSfiYZULS4[gVOcyTKtQTWfiqnS4VfiWV5KNO
S44qULYaWfiqKV5QX~HJWW5SfiKtKLeOLMyskhKNOLqV5QXHJWO
W
XJKNOLV5WrU7MKIAO
SHO
sXJKHV{yTKNXJKtW5KNy<eWfiqKj[OAnH[OLX<Mk\T[sKtS;ULY:V5W5ULSfiKNyctXOL\V5KNV{HV;sU7\TMyKNyskh:
d\S4H~M|jWfiqKKnQKtS4H[!KNMkWfiVfi K4qTKNfimLKNyOLX~V5UWfiqTK[OAnH[OLXMz\T[sKt
ULYV5W5ULSfiKNytXOL\TVfiKNVt:{qK
SfiKNVfi\XJWfiVVfiqUi KNyjWfiqTO

W yUkKNVOLX[!U7V5WMTULWyKtQKNMTyU7MgeAVfi\4qWfiqTO

W 5^N
2eAirqKtSfi
K G5 {H~VMULW
yKtQKNMTyTH~M|rU7M ( OLMy e7irqTHXKrryKtQKNMTyVU7M ( OLMy lO
sXJK;yHV5QXOohzV {OLVOY\TMTWfiHJU7MPULY (
OLMTy AblHJ|7\S4K{HM QTQKNMTyTHJ&VfiqTUoirVONILKtS4O
|LKaI
OLX\KNV<ULY YZULS ff
( ff ff z
{qK[OAnH[(\T[UL
HMOLXXU7\S{KnQKtS4H[!KNMkWfiV;iaOLVn v YZULSr ff
ff
L x



(

n
n
n
n
n
n


n
n
n
n
n
n




n
n 7
n
n

n


sXJKg
;jOLV{OgYZ\TMWfiHJU7MULY


(

OLMTy




qTK}OLMTOLXJhnVfiHVjULY{WfiqKS4\TMTMHM|WfiH[!K}ULY&_adYU7X~XJUoirVjWfiqK[!KtWfiqUnyyKtILKNXULQKNyszhG&U7XysKtSfi|
{
v N7
x OLMTyszhGU7XysKtSfi|KtWOLX v NL x grqTHV&[!KtWfiqUnyiaOLV\TVfiKNyOLX~V5Uskh iaOL[gO v NLL x !{qK
ifKNXXmzMTUoirMcSHJWfiHtHVfi[V szhbS4OLMTU}OLMTyclOL\TXX v NL x eOLMTyszhHJWfiqKNXXKtWrOLX v NL x QU7HMkWfiV;U7\W
WfiqTO
W&WfiqKPHMTV5WfiOLMTKNV&|LKNMKtSO
W5KNyskhWfiqTH~VR[!KtWfiqTUzyO
SfiKP[!U7V5WfiXJhcV4O
WfiHV2O
sXKLeOLMTyWfiqTO
WHV&HMTyKtKNyWfiqK
SfiKNOLV5U7MYULSWfiqTK!|LUkUnyQKtSfiYULS[gOLMTKPULYfV4O
WfiHV2O
sH~XHJWh4qKN4mzHMT|cOLXJ|LULS4HJWfiqT[VtWfiqKgOLXJ|LULS4HJWfiq[gVRMTy
S4O
QH~yTXJhcU7MKg[gUzyKNXlULYfWfiqK!YZULS4[(\TXOn!{qTHVQU7HMkWyUzKNVMULWO
QTQXhirqKNMifKgV5KtKtmMULWU7MTX
h fi\TV5W
U7MKj[gUzyKNXs\W{OLXXWfiqK[!UnyKNXVN
blHMTOLX~XJhLeHJWPVfiqU7\TXysKMULW5KNypWfiqTO
WPWfiqKcOLXJ|LULS4HWfiqT[gVYULSgU7\TMkWfiHM|[!UnyKNXVPyKtILKNXULQKNypV5UYO


SfiKyKNVfiHJ|7MTKNyYZUL
5 n

r
PYULS4[(\TXOLVN VHJWqTOLVsKtKNM^[!KNMkWfiHJU7MKNy^HMWfiqKHMkW5SfiUnyT\TWfiHJU7M<e
U7MKO
QTQXHtO
WfiHU7MULYU7\TMWfiH~M|[!UnyKNXVHVgHMS4KNOLV5U7MTHM|irHJWfiqH~MTU7[!QXJKtW5KcH~MYULS[gO
WfiHJU7MHMXJUL|7H
V5hnV5W5KN[gV{\Vfi\TOLXXJhKzQTS4KNVfiV5KNycH[
r54
Lg
JtnzL rqKtSfiKtYZULSfiKLeTyKtILKNXJULQH~M|gU7\TMkWfiHM|gOLXJ|LULS4HWfiqT[gV
YZULS{QTSfiKNyTH~tO
W5KtOLXt\TX\TVaYZULS4[(\TXOLV{H~V;OgMKnW{4qTOLXXKNM|LKULYlO!|LSfiKNO
WRH~[!QULSfiWfiOLMTKL




@ BCqn @



KO
SfiKr|LS4O
W5KtY\TXW5UWfiqTKROLMU7Mkhz[gU7\TVS4KtYKtSfiKtKNV YZULS WfiqKNHS [!U7V5WfO
QTW OLMyHMTVfiQHJS4HMT|&U7[g[gKNMWfiVt {qTHV
SfiKNV5KNO
S4qqTOLV{sKtKNMcQO
SfiWfiXJhVfi\QTQULSfiW5KNyszhWfiqK V5S4O
KNXEnI tHJKNMTKjbTU7\TMTyTO
WfiHJU7MOLyT[gHMHV5W5S4O
W5KNyszhWfiqK
V5S4O
KNX tOLyKN[hULYHnI tHKNMTKNV{OLMTyK&
p \T[gOLMHJWfiHJKNVt
uZ

fi

4
H ] ff ( < v x ff v x
^w bULS ff ( eWfiqKSfiKNt\TSfiSfiKNMTKHMTKNk\TOLX~HJWh}WfiO
mLKNV{WfiqKYZULS4[
:
v x B v xR:
B ( B v ^
v x 5 q BY#
N
( v xv x v x







@ CE ff





RRT

>

7BlB



`

x

G&U7XyTsKtS4| v N7
x VfiqUi KNyeTWfiqTO
W{YZULS{WfiqKYZU7XXJUirHM|PSfiKNt\TSfiSfiKNMTKLeJ v
:

J v x ff q BYN#( v xv ( x v x B J v ^ x





ff A( ff

x ff v

vx

x

`

ff A( ff

v x


{qKyHKtS4KNMTKsKtWifKtKNM v x OLMTy v x ejWfiqTKSfiKNt\SfiS4KNMTKYULSJ v x OLMTyWfiqKSfiKNt\TSfiSfiKNMTKYZULS
v x eTHV WfiqKKnW5S4OPW5KtS4[ v x B v x H~MWfiqKXO
W5W5KtSoU!U7[!QO
SfiK& v x OLMTyJ v x
ifKS4V5W;MKtKNyWfiqKYZU7XXJUoiRHM|PSfiKNVfi\TXWt


`
L

ZT`;EJ v x 5 v x B ( #H
]
^w G&U7XysKtSfi| v N7
x V4qUoifKNyWfiqO
)W J v x 5s# :+InHMTK v x B ( YULSR `zeWfiqTHV
QTSfiUILKNV&WfiqKXJKN[[gOgYZULSROLXX `}OLMTyOLX~Xk
`^n&d&HSfiKNWRU7[!Q\WfiO
WfiHJU7MULY J v x ey\KjW5U v x e
YZULS{ ff U7[!QXJKtW5KNVrWfiqKQTS4UkULYYULSrOLX~X<m
`!OLMTycOLX~X
`n
`RUoi v x tOLMsKKNV5WfiH[O
W5KNyOLV;YZU7XXJUirVt

L

Z' < v x 5 J v x
]
^w hHMyT\TWfiHJU7MU7M
{qKOLV4V5KtSfiWfiHJU7McH~VaW5S4\KYZULS{3
5

In\QQU7VfiKHWaH~V;W5S4\KYZULS{KtILKtSfih :eWfiqKNM
B B B

v x 5 # :v x B v 4

x : P v xv x v x v ^ x


tULS4yTHM|!W5U v x e

YhN#(

5 # : v x B J v x4:
B
P v xv x v x B J v ^
YN#(

x

B B B



#






:
ff
P v xv x v x J v ^ x
YN#(

B

V5Uen v x 5nJ v x: v x J v
x #:bTULSrOLXX 0 neJ v x 5J v

OLMTyszhKN[g[gO
eTYULSROLXXlOLMTyOLX~X:Te v x B J v x 5n#:{qHVaU7[!QXJKtW5KNVrWfiqKQTSfiUzULY0
InH~MTK-J v x ff v x OLMTy v x 5 J v x e
ifK|LKtWYZULS ff ( eo v x ff v
J

v x

uZx

xe
x

fiy w zR{d|,'}RR8|,~$R]wl$'}R 3 ~ff
H v x ff v x qE hfi + ff +( ( .
@
K{OLMTOLXhktKfWfiqKaSfiKNt\SfiS4KNMTKfH~MKNk\OLXHJWhYZULS:OLMh
e7\TVfiH~M|RU7MTV4HyKtS4O
WfiHJU7MVVfiH~[gHXO
SW5U&WfiqTU7V5K
^w
ULYG&U7XysKtSfi|PKtWrOLXF v NL x
{qKS4KNt\SfiSfiKNMTKH~MKNk\OLXHJWh}WfiO
mLKNVrWfiqTKYULS4[c
v x

# :
v x B v xR:

5 q BYN#( v x v K x B


v

^

`

x



v x

ff ] P ff



W{HV;KNOLV5hW5UVfiqTUoieTszhHMTyT\TWfiHU7M}U7MTeTWfiqTO
W; v x |LSfiUoirVrirHJWfiqT-prKNMTKLe
v x

58# :v x B

B
P v x v x B
YN#(

v x4:

^

v



x

v x

V5U7X\WfiHJU7MULY v x HVaXHMKNO
S;HMTeTV4HMTKWfiqTHVaSfiKNt\TSfiSfiKNMTKHVaXHMTKNO
SaH~M}WfiqTK `RUoiYZULSrOLMkhI
OLX\K
* XJKtW{\TVr4qUzU7V5KjOgU7MV5WfiOLMW}V4\T4qWfiqO
W{YULSROLXX<m5 *


irqKtS4K


ff @ +( ( .

v x 58k
{qKNMYZULSROLXXm5 * e v x e v x Hg
[ QXJh

v v x B x v x

B
5 # : P v x v x B v ^ x
YB N#(
5 # : P v x v x B v ^ x
Y#N ( B
^
ff # : k P v x v x B v x
YN#(

VRG&U7XysKtSfi|!KtWROLX v N L x V4qUoifKNy<e

OLMTyWfiqTKtSfiKtYULS4KLe

# ff

P v x v x B


v v x B x v x
v x eTOLMycV5Ue
v x

^
ff

v x

5# : k

5 k v K B x : k
v
x

v x

v

x

uZ

:

v K x

v ( x

: k

Bff+(

v

KO
S4K|LU7HM|MUiW5U!MyU7\WrU7MTyTHWfiHJU7MTV;\TMTyTKtSfiRqTH4q
v x k B ff+(
v

B :


v x

(4.

x

v

(4.

5n


x

v x


(

x

fi

WfiqTO
WrH~V

RRT

x
v
v x
xB (
v K
blHJS4V5Wte<XJKtWyKNMULW5KPWfiqK!Vfi\sKnQTSfiKNVfiV4HJU7McULY v x H~MV4k\TO
S4KsTSOLfimLKtWfiVt(bULSOLXX;
eHVXJKNVfiV&WfiqTOLMp
e
qKNMTKLe[\V5W;sK|LSfiKNO
W5KtSRWfiqOLMW5U}VfiO
WfiH~V5Yh v Bff+( (4. x 5K

G
IzUe
v

Bff+(

(4.

&

5
x

0





v x

v x B yTH[gH~MTHVfiqKNV:[gU7MULW5U7MTHtOLXXhLezOLMTy}W5KNMTyTVfW5UPPOLV ^eqKNMTK
W5U[gO
mLK(QU7VfiHJWfiHJILKLenifKjMKtKNy
0 XJUL| K
v x
v x
{qz\TVteqUzU7VfiHM|g v OLMTy}qKNMTKLez * x Vfi\n}tHJKNMkWfiXJhPXO
Sfi|LK v yKtQKNMTyTHM|U7MWfiqKrI
OLX\K;ULY<WfiqK v ( x
KnQTSfiKNVfiVfiHU7M}HM v x5x eTOLMTyyKtW5KtS4[gH~MTHM|WfiqKI
OLX\KNV ULY !OLMTyVfiO
WfiHV5YZhnHM|U7MyTHJWfiHJU7MTV v x e v x e v x e
KNV5WfiO
sX~HVfiqKNVf v x ff v K
x eirqTKtSfi

K ff +( ( ./


IzKNU7MTyekYZULS;



e

@

u

fi

w zR{d|,'}RR8|,~$R]wl$'}R 3 ~ff






@ CE ff>

B

@

fiff $E



`&\T[sKtS;ULY:tXOL\V5KNV





7









tL
X

7
N

N


L

7!I

S4HO
sXJKNV
N

N
L
LA
NLLL
LA
tN

NLnNL





t7 L
NAL7

7LLA
LLTo


7L




gI

S4HO
sXJKNV
N
Nn
X




N LA

t7 7
AL
LnNL



AL
L
A7
LN L
ALL77L
LnLN




sXJKg
ILKtS4O
|LKPMk\[jsKtS;ULYSfiKNt\S4VfiHILKtOLXXV;YZULS
`&\T[sKtS;ULY:tXOL\V5KNV





7









tL
X

7
N

N


L


!I

S4HO
sXJKNV
gI

S4HO
sXJKNV




Lt
L

NL




7
L
X

XA 7
NL


7L

n XL
L Ak
L


7
Ln

7n
XL Ln
LnX
NLLL


sXJKlz ILKtS4O
|LKPMk\[jsKtS;ULYSfiKNt\S4VfiHILKtOLXXV;YZULS

u


!I

S4HO
sXJKNV
X
N

LLL
LX 7A
LA7

Ak
N LA
nLNLL
L7zX
NL7zk
LL7zX
t7nX L n


L
7 LnX


(ff ff



7!I

S4HO
sXJKNV

L


N



L
nN
X L
NAk
7
n
L
AL

L
X nN

(ff ff



fi

RRT

`&\T[jsKtS;ULY:tXOL\TVfiKNV





7









tL
X

7
N

N


L


!I

S4HO
sXKNV
gIAO
S4H~O
sXJKNV



N




L


L
L
L



nN

LA
L








XA


nN

L
L






sXJKlz ILKtS4O
|LKPMk\[jsKtS;ULYSfiKNt\S4VfiHILKtOLXXV;YZULS

`&\T[sKtS;ULY:tXOL\V5KNV





7









tL
X

7
N

N


L


!I

S4HO
sXJKNV
L
L


tL



TNL

7
Lo7
NA
LLLL
LkL
L

t7nXA
LLL
NL nN

uX

(ff ff

7gI

S4HO
sXJKNV



L



oA7
L
L

t7nN

nN
7
Ak
7

t7AL
NLLLL


L
L



sXJK ILKtS4O
|LK(Mk\T[sKtS;ULYS4KNt\S4VfiHJILKtOLX~XV;YZULS



(ff




!I

S4HO
sXJKNV



nL
7
X L
Ak
nL


t7LL
N7
7A7n
L LA
Lozo


N


Ln

e

ff



fiy w zR{d|,'}RR8|,~$R]wl$'}R 3 ~ff

`&\T[sKtS;ULY:tXOL\V5KNV





7









tL
X

7
N

N


L


!I

S4HO
sXJKNV

n



L


NL
L

L

nX
LLn
7

nX7
L7


77L


7gI

S4HO
sXJKNV


L
L
L7


LLo
oA7

L

77
n
N
77
L
nNL


sXJKjz ILKtS4O
|LK(Mk\T[sKtS;ULYS4KNt\S4VfiHJILKtOLX~XV;YZULS

`&\T[jsKtS;ULY:tXOL\TVfiKNV





7









tL
X

7
N

N


L

(ff


!I

S4HO
sXJKNV

L
7

7
Ln



X
NnN
LLL
L7
LA
n
N 7z





e

ff



ff




!I

S4HO
sXKNV
gIAO
S4H~O
sXJKNV



n
L

NA

7
L

7


LN
N
7
nNA
LL7
L


sXJKjz ILKtS4O
|LK(Mk\T[sKtS;ULYS4KNt\S4VfiHJILKtOLX~XV;YZULS

u




XA
NL

L
Lk
L7
Ln
XL
N n
nNL
7

(ff

ze

fi

SfiULsO
sHXHJWfiHJKNV
( ff ff


S4HO
sXKNV
7



(ff ff





7

(ff ff
(ff









e



ff

7



(ff


e



ff



7

(ff

ze

ff

RRT








_aXOL\TV5KNV
X


L
X


L
X


L
X


L
X


L
X


L
X


L
X


L
X


L
X


L
X


L
X


L

LI KtS4O
|LK
t7L
7L


AL
LnLN


LAk
NLLL
7
n
X nN
L



7

L

7A7n


Ln
LLn
77L


77
nNL
LN
L
Ln
7

IzWfiOLMyTO
S4yyKtInHO
WfiHJU7M
TXA
L n tL

L TN
7
L7A7L

7
L7
k

77

t7
L

nNA
X Lnoz
Lt7n

7L7
AL
AL
X
LA
N
n
X L
L


sXKkGInOL[!QXKNVaULYV5WfiOLMyTO
S4ycyKtInHO
WfiHJU7MULY:Mz\T[sKtS;ULYSfiKNt\SVfiHJILKtOLXXV

uhu

fiy w zR{d|,'}RR8|,~$R]wl$'}R 3 ~ff

_aXOL\TV5KNV5FW5U
FIAO
S4H~O
sXJKNV;S4O
WfiHU
n
n
n
n

$















!I

S4HO
sXJKNV
L
7
L
L
X

Nn
L
XL
XL
t7


L
LA

L7

X L
L
L
L


z
z

z




z










7IAO
S4H~O
sXJKNV
t7L
oLzN
LX nNL
L

L

LLL
LL7



TX


L
NAk

t7
7L

7A
A777
NL7

L7
nN
L
L
N

X L


!I

S4HO
sXJKNV

7
L
L7L7
LN A7A
nX 7
LLL
AA
L
LAL

L
7AL7
LATNnX
LAA777
Lo
Ln
7L
L
7 7


7nk
L
A7
nX L
L
L

k



sXJKjzKNyTHOLMcMz\T[sKtS;ULYSfiKNt\TS4VfiHJILKtOLXX~V;YULSXHJW5KtS4OLX<tXOL\V5KNVteT

uXv

ff


7


fi

RRT










































blHJ|7\SfiKg
ILKtS4O
|LK(ULY[gOAH[gOLXMz\T[sKtSrULY:tXOL\TV5KNVRV5W5ULSfiKNyyT\S4H~M|PWfiqKjS4\MTMTHM|PULY_ad^YULS

IAO
S4H~O
sXJKNVt

u

fiy w zR{d|,'}RR8|,~$R]wl$'}R 3 ~ff
3* $ @
dONInHVtezJe\WfiMOL[eVp v NL
x
Rj
Sn ete
;nNz

U7[!Q\WfiH~M|QTSfiUnKNyT\SfiKaYZULS z\TOLMkWfiH
WfiHJU7M!WfiqTKtULSfihLn
#


d\sU7HVteT v NLn x n_fU7\MWfiHMT|;WfiqKMk\[jsKtSULYnV5U7X\WfiHU7MTVYULSHMTV5WfiOLMTKNVULYnV4O
WfiHV2O
sH~XHJWhL=nfi5N

:L_ nNtte v x ezA
bS4OLMUeJe=lOL\TX~Xe v NL x }SfiULsO
sHXHV5WfiH~OLMTOLXJhnVfiHV&ULYfWfiqKgdONInHV2\TWfiMTOL[ QTSfiUnKNyT\S4KYZULS
VfiU7XJIzH~M|PWfiqKjVfiO
WfiHV5O
sHXHWhgQTS4ULsXJKN[Hj~tfitD4
LSntLte
eLz7k

G&U7XyTsKtS4|e v N7
x ILKtS4O
|LK(tOLVfiKjU7[!QXJKHJWhULYWfiqKjV4O
WfiHV2O
sH~XHJWhQTSfiULsXJKN[c M62o44
Zk

R jS nTS/\/zLnFL7fiPfi

\V5WfiHM<eKnOLVt

G&U7XyTsKtS4|e Je \S4yU7[eJefSfiUirM<e_ v NL x ILKtS4O
|LKjWfiH~[!KOLMTOLXJhnVfiHVfULY:VfiH[!QXH KNy}d&OoIzH~V2
\WfiMTOL[ QSfiUzKNy\SfiKNVt Xt#7
3D5N4Zkt4#e v x e<n
z

G&SfiUILKLe JepROLXQKtSM<eTJeU7XXJKtSNe
d( v NLA x OLMTyU7[ifULS4XyTVOLMTy([gOAH[\[KNMkW5SfiULQzhLn
#

R N"L Z z#r#t/5he7ezLz
ifOL[gOneg v NLL x f
_;`RbVfiO
WfiHV2O
sHXHJWhPW5KNV5W;skh}U7\TMWfiH~M|!OLMTyQU7XhzMU7[HOLXOoILKtS4O
|LKWfiH[!KL


#
L :L_nZk7e v x eLL;Ln




H~MTHOLXel`jJeq`RH~VfiOLM<e` v NL
x QTQSfiUNH[gO
W5KHMTtX\VfiHJU7MnFKtX\TVfiHU7M<:
]ZLF#
e v x e
;LLz
ULNH~MTV5mnHHe v NLL x TM V5ifKtS4HM|gO
W5U7[gH~jz\KtS4HJKNV{H~McHMTyKMTHJW5KyKNyT\WfiHJILKjyTO
WfiO
sOLVfiKNVt h#

L
L ZJkt A(e v x e7Lz
ULNH~MTV5mnHHe v NL x _fU7\TMkWfiHM|QTSfiULQU7VfiHWfiHJU7MTOLXz[!UzyTKNXVt X tL
o62o#ZkNhe
7 ;z
ULNH~MTV5mnHHe v NLL x V WfiqKtSfiKOLMOLXJW5KtS4MTO
WfiHJILK&W5U(QO
S4VfiH[gU7MTHJU7\TVVfiKN[gOLMWfiH~tV7
L<R
ZL
=n4fit/
ftf
Z z#eterLn;7
z



v xe



\TskhLenJe' KNXH"! mLUoInH
# eT& v NLn x qMyKtW5KtS4[gHMHV5WfiH;O
QTQSfiUNH[gO
WfiHJU7MULYd`Rbf M62o44
Zk

Rff

fi

HJWfi4qKNX~Xerd(Je+IzKNX~[gOLM<e{&Je+ KtILKNV4k\KLe+p v NL x &
p
S4yOLMTy^KNOLV5h^yTHV5W5S4Hs\WfiHJU7MTVPULYI
QSfiULsXJKN[gVN
62N#44
z
( 7
ULWfiqe d( v NLL x nMpWfiqKcqO
S4yTMKNVfiV(ULY&O
QTQTSfiUonH[O
W5KSfiKNOLV5U7MTHMT|
7 ;
z

tf
Z z#e 7e


OLXHOLMkWte v N7
x {qTKU7[!QXJKnHWhULY&U7[!Q\WfiH~M|WfiqTKQKtS4[OLMKNMWt
tF#eLeNL;



=nfifit/L1:
h


fiqTOLMT|e_ v NLL x `&\T[sKtS}ULYP[!UnyKNXVOLMTyVfiO
WfiHV2O
sHXHJWhULYV5KtWfiVULY(tXOL\TV5KNVN = nfi5N

L_ nNtte v x e7L;LLz
:
u

fiJournal Artificial Intelligence Research 10 (1999) 271-289

Submitted 11/98; published 5/99

Issues Stacked Generalization
Kai Ming Ting

kmting@deakin.edu.au

Ian H. Witten

ihw@cs.waikato.ac.nz

School Computing Mathematics
Deakin University, Australia.
Department Computer Science
University Waikato, New Zealand.

Abstract

Stacked generalization general method using high-level model combine lowerlevel models achieve greater predictive accuracy. paper address two crucial
issues considered `black art' classification tasks ever since
introduction stacked generalization 1992 Wolpert: type generalizer
suitable derive higher-level model, kind attributes used
input. find best results obtained higher-level model combines
confidence (and predictions) lower-level ones.
demonstrate effectiveness stacked generalization combining three different
types learning algorithms classification tasks. also compare performance
stacked generalization majority vote published results arcing bagging.

1. Introduction
Stacked generalization way combining multiple models learned
classification task (Wolpert, 1992), also used regression (Breiman, 1996a)
even unsupervised learning (Smyth & Wolpert, 1997). Typically, different learning
algorithms learn different models task hand, common form
stacking first step collect output model new set data.
instance original training set, data set represents every model's prediction
instance's class, along true classification. step, care taken ensure
models formed batch training data include instance
question, way ordinary cross-validation. new data treated
data another learning problem, second step learning algorithm
employed solve problem. Wolpert's terminology, original data models
constructed first step referred level-0 data level-0 models,
respectively, set cross-validated data second-stage learning algorithm
referred level-1 data level-1 generalizer.
paper, show make stacked generalization work classification tasks
addressing two crucial issues Wolpert (1992) originally described `black art'
resolved since. two issues (i) type attributes
used form level-1 data, (ii) type level-1 generalizer order get improved
accuracy using stacked generalization method.
Breiman (1996a) demonstrated success stacked generalization setting
ordinary regression. level-0 models regression trees different sizes linear
c 1999 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiTing & Witten

regressions using different number variables. instead selecting single model
works best judged (for example) cross-validation, Breiman used different level0 regressors' output values member training set form level-1 data.
used least-squares linear regression, constraint regression coecients
non-negative, level-1 generalizer. non-negativity constraint turned
crucial guarantee predictive accuracy would better achieved
selecting single best predictor.
show stacked generalization made work reliably classification
tasks. using output class probabilities generated level-0 models
form level-1 data. level-1 generalizer use version least squares linear
regression adapted classification tasks. find use class probabilities crucial
successful application stacked generalization classification tasks. However,
non-negativity constraints found necessary Breiman regression found
irrelevant improved predictive accuracy classification situation.
Section 2, formally introduce technique stacked generalization describe
pertinent details learning algorithm used experiments. Section 3 describes
results stacking three different types learning algorithms. Section 4 compares
stacked generalization arcing bagging, two recent methods employ sampling
techniques modify data distribution order produce multiple models single
learning algorithm. following section describes related work, paper ends
summary conclusions.

2. Stacked Generalization

Given data set L = f(y ; x ); n = 1; : : : ; N g, class value x vector
representing attribute values nth instance, randomly split data J almost
equal parts L1 ; : : : ; L . Define L L(, ) = L , L test training sets
j th fold J -fold cross-validation. Given K learning algorithms, call level-0
generalizers, invoke kth algorithm data training set L(, ) induce
model M(, ) , k = 1; : : : ; K . called level-0 models.
instance x L , test set j th cross-validation fold, let z denote
prediction model M(, ) x . end entire cross-validation process,
data set assembled outputs K models
n

n

n

J

j

j

n

j

j

j

k

n

j

kn

j

n

k

L = f(y ; z1 ; : : : ; z ); n = 1; : : : ; N g:
CV

n

n

Kn

level-1 data. Use learning algorithm call level-1 generalizer
~ function (z1 ; : : : ; z ). level-1
derive data model
model. Figure 1 illustrates cross-validation process. complete training process,
final level-0 models , k = 1; : : : ; K , derived using data L.
let us consider classification process, uses models , k = 1; : : : ; K ,
~ . Given new instance, models produce vector (z1 ; : : : ; z ).
conjunction
~ , whose output final classification result
vector input level-1 model
instance. completes stacked generalization method proposed Wolpert
(1992), also used Breiman (1996a) LeBlanc & Tibshirani (1993).
K

k

k

k

272

K

fiIssues Stacked Generalization

~


L CV
Level-1
Level-0

(-j)

(-j)

M1

(-j)

Mk

MK

(-j)

L

Figure 1: figure illustrates j -fold cross-validation process level-0; level-1
~.
data set L end process used produce level-1 model
CV

~ ,
well situation described above, results level-1 model
present paper also considers situation output level-0 models
set class probabilities rather single class prediction. model M(, ) used
classify instance x L , let P (x) denote probability ith output class,
vector
P = (P 1 (x ); : : : ; P (x ); : : : ; P (x ))
gives model's class probabilities nth instance, assuming classes.
level-1 data, assemble together class probability vector K models, along
actual class:
L0 = f(y ; P1 ; : : : ; P ; : : : ; P ); n = 1; : : : ; N g:
~ 0 contrast
~.
Denote level-1 model derived
following two subsections describe algorithms used level-0 level-1 generalizers experiments reported Section 3.
j

k

j

ki

kn

CV

n

k

n

n

ki

kn

n

kI

n

Kn

2.1 Level-0 Generalizers

Three learning algorithms used level-0 generalizers: C4.5, decision tree learning
algorithm (Quinlan, 1993); NB, re-implementation Naive Bayesian classifier (Cestnik,
1990); IB1, variant lazy learning algorithm (Aha, Kibler & Albert, 1991)
employs p-nearest-neighbor method using modified value-difference metric nominal
binary attributes (Cost & Salzberg, 1993). learning algorithms
show formula use
P estimated output class probabilities P (x)
instance x (where, cases, P (x) = 1).
C4.5: Consider leaf decision tree instance x falls. Let
number (training) instances class leaf, suppose majority class








273

fiTing & Witten

P
leaf I^. Let E = 6= ^ . Then, using Laplace estimator,






P ^(x) = 1 , PEm+ +1 2 ;






P (x) = (1 , P ^(x)) ; 6= I^:






Note pruned trees default settings C4.5 used experiments.
NB: Let P (ijx) posterior probability class i, given instance x.
P (x) = PP P(ij(xij)x) :




Note NB uses Laplacian estimate estimating conditional probabilities
nominal attribute compute P (ijx). continuous-valued attribute,
normal distribution assumed case conditional probabilities
conveniently represented entirely terms mean variance observed
values class.
IB1: Suppose p nearest neighbors used; denote f(y ; x ); = 1; : : : ; pg
instance x. (We use p = 3 experiments.)




P f (y )=d(x; x )
P (x) = P=1 1=d(x; x ) ;
p









p

=1





f (y ) = 1 = 0 otherwise, Euclidean distance function.




three learning algorithms, predicted class level-0 model, given instance
x, I^
P ^(x) > P (x) 6= I^:




2.2 Level-1 Generalizers

compare effect four different learning algorithms level-1 generalizer: C4.5,
IB1(using p = 21 nearest neighbors),1 NB, multi-response linear regression algorithm,
MLR. last needs explanation.
MLR adaptation least-squares linear regression algorithm Breiman (1996a)
used regression settings. classification problem real-valued attributes
transformed multi-response regression problem. original classification problem
classes, converted separate regression problems, problem
class ` instances responses equal one class ` zero otherwise.
input MLR level-1 data, need consider situation model
0
~
, attributes probabilities, separately model M~ ,
1. large value used following Wolpert's (1992) advice \ reasonable `relatively global,
smooth ' level-1 generalizers perform well."
p

:::

:::

274

fiIssues Stacked Generalization

classes. former case, attributes already real-valued, linear
regression class ` simply

X
LR (x) = ff
K

`

k`

P (x):
k`

k

latter case, classes unordered nominal attributes. map binary
values obvious way, setting P (x) 1 class instance x ` zero otherwise;
use linear regression.
Choose linear regression coecients fff g minimize
k`

X X
j

(

yn ;xn

)2Lj

k`

(y ,
n

Xff

P (, )(x ))2 :
j

k`

k`

n

k

coecients fff g constrained non-negative, following Breiman's (1996a) discovery necessary successful application stacked generalization regression problems. non-negative-coecient least-squares algorithm described Lawson
& Hanson (1995) employed derive linear regression class. show
later that, fact, non-negative constraint unnecessary classification tasks.
place, describe working MLR. classify new instance
x, compute LR (x) classes assign instance class `
greatest value:2
LR (x) > LR (x) `0 6= `:
next section investigate stacking C4.5, NB IB1.
k`

`

`

`0

3. Stacking C4.5, NB IB1

3.1 Stacked Generalization Work?

experiments section show
successful stacked generalization necessary use output class prob~ 0 rather
~;
abilities rather class predictions|that is,
MLR algorithm suitable level-1 generalizer, among four
algorithms used.
use two artificial datasets eight real-world datasets UCI Repository
machine learning databases (Blake, Keogh & Merz, 1998). Details given
Table 1.
artificial datasets|Led24 Waveform|each training dataset L size 200
300, respectively, generated using different seed. algorithms used
experiments tested separate dataset 5000 instances. Results expressed
average error rate ten repetitions entire procedure.
real-world datasets, W -fold cross-validation performed. fold
cross-validation, training dataset used L, models derived evaluated
2. pattern recognition community calls type classifier linear machine (Duda & Hart, 1973).

275

fiTing & Witten

Datasets # Samples # Classes # Attr & Type
Led24
200/5000
10
10N
Waveform 300/5000
3
40C
Horse
368
2
3B+12N+7C
Credit
690
2
4B+5N+6C
Vowel
990
11
10C
Euthyroid
3163
2
18B+7C
Splice
3177
3
60N
Abalone
4177
3
1N+7C
Nettalk(s)
5438
5
7N
Coding
20000
2
15N

N-nominal; B-binary; C-Continuous.

Table 1: Details datasets used experiment.
test dataset. result expressed average error rate W -fold crossvalidation. Note cross-validation used evaluation entire procedure,
whereas J -fold cross-validation mentioned Section 2 internal operation
stacked generalization. However, W J set 10 experiments.
~
section, present results model combination using level-1 models
0
~
, well model selection method, employing J -fold cross-validation procedure. Note difference model combination model selection
whether level-1 learning employed not.
Table 2 shows average error rates, obtained using W -fold cross-validation, C4.5,
NB IB1, BestCV, best three, selected using J -fold crossvalidation. expected, BestCV almost always classifier lowest error rate.3
~ ,
Table 3 shows result stacked generalization using level-1 model
~ 0 ,
level-1 data comprise classifications generated level-0 models,
level-1 data comprise probabilities generated level-0 models. Results
shown four level-1 generalizers case, along BestCV. lowest error
rate dataset given bold.
Table 4 summarizes results Table 3 terms comparison level-1
~ 0 derived
model BestCV totaled datasets. Clearly, best level-1 model
using MLR. performs better BestCV nine datasets equally well tenth.
~ derived NB, performs better BestCV seven
best performing
datasets significantly worse two (Waveform Vowel). regard difference
two standard errors significant (95% confidence). standard error figures
omitted table increase readability.
datasets shown order increasing size. MLR performs significantly
better BestCV four largest datasets. indicates stacked generalization
likely give significant improvements predictive accuracy volume data
large|a direct consequence accurate estimation using cross-validation.
3. Note BestCV always select classifier folds. error rate
always equal lowest error rate among three classifiers.
W

276

fiIssues Stacked Generalization

Datasets

Level-0 Generalizers
C4.5 NB
IB1
Led24
35.4 35.4
32.2
Waveform 31.8 17.1
26.2
Horse
15.8 17.9
15.8
Credit
17.4 17.3
28.1
Vowel
22.7 51.0
2.6
Euthyroid 1.9 9.8
8.6
Splice
5.5 4.5
4.7
Abalone
41.4 42.1
40.5
Nettalk(s) 17.0 15.9
12.7
Coding
27.6 28.8
25.0

BestCV
32.8 0.6
17.1 0.3
17.1 1.6
17.4 1.2
2.6 0.2
1.9 0.3
4.5 0.4
40.1 0.6
12.7 0.4
25.0 0.3

Table 2: Average error rates C4.5, NB IB1, BestCV|the best among
selected using J -fold cross-validation. standard errors shown last
column.
Datasets

~
Level-1 model,
C4.5 NB IB1 MLR
34.0 32.4 35.0 33.3
17.7 19.2 18.7 17.2
16.9 14.9 17.6 16.3
18.4 16.1 16.9 17.4
2.6 3.8 3.6
2.6

BestCV
Led24
32.8
Waveform
17.1
Horse
17.1
Credit
17.4
Vowel
2.6
Euthyroid
1.9 1.9 1.9 1.9
Splice
4.5 3.9 3.9 3.8
Abalone
40.1 38.5 38.5 38.2
Nettalk(s)
12.7 12.4 11.9 12.4
Coding
25.0 23.2 23.1 23.2

1.9
3.8

38.1
12.6
23.2

~0
Level-1 model,
C4.5 NB IB1 MLR
41.7 35.7 32.1 31.3
20.6 17.6 17.8 16.8
18.0 18.5 17.7 15.2
15.4 15.9 14.3 16.2
2.7 7.2 3.3 2.5
2.2 4.3 2.0 1.9
4.0 3.9 3.8 3.8
43.3 37.1 39.2 38.3
14.0 14.6 12.0 11.5
22.3 21.2 21.2 20.7

Table 3: Average error rates stacking C4.5, NB IB1.
~
~0
Level-1 model,
Level-1 model,
C4.5 NB IB1 MLR C4.5 NB IB1 MLR
#Win vs. #Loss 3-5 2-7 4-5 2-5 7-3 6-4 4-6 0-9
~
~ 0.
Table 4: Summary Table 3|Comparison BestCV

277

fiTing & Witten

one level-0 models performs significantly much better rest, like
Euthyroid Vowel datasets, MLR performs either good BestCV selecting
best performing level-0 model, better BestCV.
MLR advantage three level-1 generalizers model
easily interpreted. Examples combination weights derives (for probability~ 0 ) appear Table 5 Horse, Credit, Splice, Abalone, Waveform, Led24
based model
Vowel datasets. weights indicate relative importance level-0 generalizers
prediction class. example, Splice dataset (in Table 5(b)), NB
dominant generalizer predicting class 2, NB IB1 good predicting class
3, three generalizers make worthwhile contribution prediction class 1.
contrast, Abalone dataset three generalizers contribute substantially
prediction three classes. Note weights class sum one
constraint imposed MLR.

3.2 Non-negativity Constraints Necessary?
Breiman (1996a) LeBlanc & Tibshirani (1993) use stacked generalization
method regression setting report necessary constrain regression
coecients non-negative order guarantee stacked regression improves predictive accuracy. investigate finding domain classification tasks.
assess effect non-negativity constraint performance, three versions
~ 0:
MLR employed derive level-1 model
i. linear regression MLR calculated intercept constant (that is,
+ 1 weights classes) without constraints;
ii. linear regression derived neither intercept constant (I weights
classes) constraints;
iii. linear regression derived without intercept constant, nonnegativity constraints (I non-negative weights classes).
third version one used results presented earlier. Table 6 shows
results three versions. almost indistinguishable error rates. conclude
classification tasks, non-negativity constraints necessary guarantee
stacked generalization improves predictive accuracy.
However, another reason good idea employ non-negativity constraints. Table 7 shows example weights derived three versions MLR
Led24 dataset. third version, shown column (iii), supports perspicuous
interpretation level-0 generalizer's contribution class predictions
two. dataset, IB1 dominant generalizer predicting classes 4, 5 8,
NB IB1 make worthwhile contribution predicting class 2, evidenced
high weights. However, negative weights used predicting classes render
interpretation two versions much less clear.
278

fiIssues Stacked Generalization

Horse
Credit
Class C4.5 NB IB1 C4.5 NB IB1
1
0.36 0.20 0.42 0.63 0.30 0.04
2
0.39 0.19 0.41 0.65 0.28 0.07
C4.5 ff1 ; NB ff2 ; IB1 ff3 .
~ 0 ) Horse Credit datasets.
Table 5: (a) Weights generated MLR (model
Class
1
2
3

C4.5
0.23
0.15
0.08

Splice
NB
0.43
0.72
0.52

IB1
0.36
0.12
0.40

Abalone
C4.5 NB IB1
0.25 0.25 0.39
0.27 0.20 0.25
0.30 0.18 0.39

Waveform
C4.5 NB IB1
0.16 0.59 0.34
0.14 0.72 0.07
0.04 0.65 0.23

~ 0 ) Splice, Abalone Waveform
Table 5: (b) Weights generated MLR (model
datasets.
Vowel
C4.5 NB IB1
0.04 0.00 0.96
0.03 0.00 0.97
0.01 0.00 1.00
0.05 0.25 0.86
0.01 0.08 0.97
0.15 0.00 0.92
0.03 0.01 1.02
0.04 0.01 0.96
0.03 0.00 1.02
0.08 0.01 0.93
0.00 0.04 0.96
~ 0 ) Led24 Vowel datasets.
Table 5: (c) Weights generated MLR (model
Class
1
2
3
4
5
6
7
8
9
10
11

C4.5
0.46
0.00
0.47
0.00
0.00
0.35
0.15
0.00
0.00
0.00
{

Led24
NB
0.65
0.37
0.00
0.13
0.19
0.14
0.43
0.00
0.38
0.50
{

IB1
0.00
0.43
0.54
0.65
0.64
0.35
0.36
0.68
0.29
0.24
{

279

fiTing & Witten

Datasets

MLR
Constraints Intercept Non-Negativity
Led24
31.4
31.4
31.3
Waveform
16.8
16.8
16.8
Horse
15.8
15.8
15.2
Credit
16.2
16.2
16.2
Vowel
2.4
2.4
2.5
Euthyroid
1.9
1.9
1.9
Splice
3.7
3.8
3.8
Abalone
38.3
38.3
38.3
Nettalk(s)
11.6
11.5
11.5
Coding
20.7
20.7
20.7
Table 6: Average error rates three versions MLR.
Class
1
2
3
4
5
6
7
8
9
10

ff0

0.00
0.02
0.00
0.04
0.03
0.01
0.01
0.02
0.04
0.04

ff1

(i)

ff2

ff3

ff1

(ii)

ff2

ff3

ff1

0.45 0.65 0.00 0.46 0.65 0.00 0.46
{0.42 0.47 0.56 {0.40 0.49 0.56 0.00
0.46 {0.01 0.54 0.47 {0.01 0.54 0.47
{0.33 0.15 0.84 {0.29 0.21 0.81 0.00
{0.37 0.26 0.84 {0.32 0.26 0.84 0.00
0.35 0.12 0.35 0.36 0.14 0.35 0.35
0.15 0.43 0.36 0.15 0.43 0.36 0.15
{0.05 {0.25 0.72 {0.03 {0.19 0.72 0.00
{0.08 0.32 0.32 {0.05 0.40 0.30 0.00
{0.06 0.43 0.25 {0.01 0.50 0.24 0.00

(iii)

ff2

0.65
0.37
0.00
0.13
0.19
0.14
0.43
0.00
0.38
0.50

ff3

0.00
0.43
0.54
0.65
0.64
0.35
0.36
0.68
0.29
0.24

Table 7: Weights generated three versions MLR: (i) constraints, (ii) intercept,
(iii) non-negativity constraints, LED24 dataset.

280

fiIssues Stacked Generalization

Dataset
#SE BestCV Majority MLR
Horse
0.5
17.1
15.0 15.2
Splice
2.5
4.5
4.0 3.8
Abalone
3.3
40.1
39.0 38.3
Led24
8.7
32.8
31.8 31.3
Credit
8.9
17.4
16.1 16.2
Nettalk(s) 10.8
12.7
12.2 11.5
Coding
12.7
25.0
23.1 20.7
Waveform 18.7
17.1
19.5 16.8
Euthyroid 26.3
1.9
8.1 1.9
Vowel
242.0
2.6
13.0 2.5
~ 0 ), along
Table 8: Average error rates BestCV, Majority Vote MLR (model
number standard error (#SE) BestCV worst performing
level-0 generalizers.

3.3 Stacked Generalization Compare Majority Vote?
~ 0 , derived MLR, majority vote,
Let us compare error rate

simple decision combination method requires neither cross-validation level1 learning. Table 8 shows average error rates BestCV, majority vote MLR.
order see whether relative performances level-0 generalizers effect
methods, number standard errors (#SE) error rates
worst performing level-0 generalizer BestCV given, datasets re-ordered
according measure. Since BestCV almost always selects best performing level-0
generalizer, small values #SE indicate level-0 generalizers perform comparably
one another, vice versa.
MLR compares favorably majority vote, eight wins versus two losses.
eight wins, six significant differences (the two exceptions Splice
Led24 datasets); whereas losses (for Horse Credit datasets) insignificant
differences. Thus extra computation cross-validation level-1 learning seems
paid off.
interesting note performance majority vote related size
#SE. Majority vote compares favorably BestCV first seven datasets,
values #SE small. last three, #SE large, majority vote performs
worse. indicates level-0 generalizers perform comparably, worth
using cross-validation determine best one, result majority vote|which
far cheaper|is significantly different. Although small values #SE necessary
condition majority vote rival BestCV, sucient condition|see Matan
(1996) example. applies majority vote compared MLR. MLR
performs significantly better five datasets large #SE values,
one cases.
281

fiTing & Witten

M~ versus M~ 0

C4.5 NB IB1 MLR
#Win vs. #Loss 8-2 5-4 3-6 1-7
~ versus
~ 0 generalizer|summarized results Table 3.
Table 9:
worth mentioning method averages P (x) level-0 models,
yielding P (x), predicts class I^ P^(x) > P (x) 6= I^: According
Breiman (1996b), method produces error rate almost identical majority
vote.








3.4 Stacked Generalization Work Best M~ 0 Generated
MLR?
shown stacked generalization works best output class probabilities
(rather class predictions) used MLR algorithm (rather C4.5, IB1,
NB). retrospect, surprising, explained intuitively follows.
level-1 model provide simple way combining evidence available.
evidence includes predictions, confidence level-0 model
predictions. linear combination simplest way pooling level-0 models'
confidence, MLR provides that.
alternative methods NB, C4.5, IB1 shortcomings. Bayesian approach could form basis suitable alternative way pooling level-0 models' confidence, independence assumption central Naive Bayes hampers performance
datasets evidence provided individual level-0 models certainly
independent. C4.5 builds trees model interaction amongst attributes|particularly
tree large|but desirable combining confidences. Nearest neighbor methods really give way combining confidences; also, similarity metric
employed could misleadingly assume two different sets confidence levels similar.
~
~ 0 level-1
Table 9 summarizes results Table 3 comparing
generalizer, across datasets. C4.5 clearly better label-based representation,
discretizing continuous-valued attributes creates intra-attribute interaction addition interactions different attributes. evidence Table 9 NB
indifferent use labels confidences: normal distribution assumption
embodies latter case could another reason unsuitable combining
confidence measures. MLR IB1 handle continuous-valued attributes better
label-based ones, since domain designed work.
Summary

summarize findings section follows.

None four learning algorithms used obtain model M~ perform satisfactorily.
282

fiIssues Stacked Generalization

MLR best four learning algorithms use level-1 generalizer
~ 0.
obtaining model
obtained using MLR, M~ 0 lower predictive error rate best model
selected J -fold cross-validation, almost datasets used experiments.

Another advantage MLR three level-1 generalizers interpretability.
weights ff indicate different contributions level-0 model k makes
prediction classes `.
k`

Model M~ 0 derived MLR without non-negativity constraints.
constraints make little difference model's predictive accuracy.

use non-negativity constraints MLR advantage interpretability. Non-

negative weights ff support easier interpretation extent model
contributes prediction class.
k`

derived using MLR, model M~ 0 compares favorably majority vote.
MLR provides method combining confidence generated level-0 models
final decision. various reasons, NB, C4.5, IB1 suitable task.

4. Comparison Arcing Bagging
section compares results stacking C4.5, NB IB1 results arcing
(called boosting originator, Schapire, 1990) bagging reported Breiman
(1996b; 1996c). arcing bagging employ sampling techniques modify data
distribution order produce multiple models single learning algorithm.
combine decisions individual models, arcing uses weighted majority vote
bagging uses unweighted majority vote. Breiman reports arcing bagging
substantially improve predictive accuracy single model derived using base
learning algorithm.

4.1 Experimental Results

First describe differences experimental procedures. results
stacking averaged ten-fold cross-validation datasets except Waveform,
averaged ten repeated trials. Standard errors also shown. Results arcing
bagging obtained Breiman (1996b; 1996c), averaged 100 trials.
Breiman's experiments, trial uses random 9:1 split form training test
sets datasets except Waveform. Also note Waveform dataset used 19
irrelevant attributes, Breiman used version without irrelevant attributes (which would
expected degrade performance level-0 generalizers experiments).
cases 300 training instances used dataset, used 5000 test instances
whereas Breiman used 1800. Arcing bagging done 50 decision tree models
derived CART (Breiman et al., 1984) trial.
283

fiTing & Witten

Dataset
#Samples stacking arcing bagging
Waveform
300
16.8 0.2 17.8
19.3
Glass
214
28.4 2.9 22.0
23.2
Ionosphere
351
9.7 1.5
6.4
7.9
Soybean
683
4.3 1.1
5.8
6.8
Breast Cancer
699
2.7 0.8
3.2
3.7
Diabetes
768
24.2 1.2 26.6
23.9
Table 10: Comparing stacking arcing bagging classifiers.
results six datasets given Table 10, indicate three methods
competitive.4 Stacking performs better arcing bagging three
datasets (Waveform, Soybean Breast Cancer), better arcing worse
bagging Diabetes dataset. Note stacking performs poorly Glass
Ionosphere, two small real-world datasets. surprising, cross-validation
inevitably produces poor estimates small datasets.

4.2 Discussion

Like bagging, stacking ideal parallel computation. construction level-0
model proceeds independently, communication modeling processes
necessary.
Arcing bagging require considerable number member models
rely varying data distribution get diverse set models single learning
algorithm. Using level-1 generalizer, stacking work two three level-0
models.
Suppose computation time required learning algorithm C , arcing
bagging needs h models. learning time required = hC . Suppose stacking requires
g models model employs J -fold cross-validation. Assuming time C needed
derive g level-0 models level-1 model, learning time stacking
= (g(J + 1) + 1)C . results given Table 10, h = 50, J = 10, g = 3; thus
= 50C = 34C . However, practice learning time required level-0
level-1 generalizers may different.
Users stacking free choice level-0 models. may either derived
single learning algorithm, variety different algorithms. example Section
3 uses different types learning algorithms, bag-stacking|stacking bagged models
(Ting & Witten, 1997)|uses data variation obtain diverse set models single
learning algorithm. former case, performance may vary substantially
level-0 models|for example NB performs poorly Vowel Euthyroid datasets
compared two models (see Table 2). Stacking copes well situation.
performance variation among member models bagging rather small
derived learning algorithm using bootstrap samples. Section 3.3








4. heart dataset used Breiman (1996b; 1996c) omitted much modified
original one.

284

fiIssues Stacked Generalization

shows small performance variation among member models necessary condition
majority vote (as employed bagging) work well.
worth noting arcing bagging incorporated framework
stacked generalization using arced bagged models level-0 models. Ting & Witten
(1997) show one possible way incorporating bagged models level-1 learning, employing MLR instead voting. implementation, L used test set
bagged models derive level-1 data rather cross-validated data.
viable bootstrap sample leaves 37% examples. Ting & Witten
(1997) show bag-stacking almost always higher predictive accuracy bagging
models derived either C4.5 NB. Note difference whether
adaptive level-1 model simple majority vote employed
According Breiman (1996b; 1996c), arcing bagging improve predictive accuracy learning algorithms `unstable.'5 unstable learning algorithm
one small perturbations training set produce large changes
derived model. Decision trees neural networks unstable; NB IB1 stable.
Stacking works both.
MLR successful candidate level-1 learning found,
algorithms might work equally well. One candidate neural networks. However,
experimented back-propagation neural networks purpose found
much slower learning rate MLR. example, MLR took 2.9
seconds compare 4790 seconds neural network nettalk dataset;
error rate. possible candidates multinomial logit model
(Jordan & Jacobs, 1994), special case generalized linear models (McCullagh
& Nelder, 1983), supra Bayesian procedure (Jacobs, 1995) treats level-0
models' confidence data may combined prior distribution level-0 models
via Bayes' rule.

5. Related Work

analysis stacked generalization motivated Breiman (1996a), discussed
earlier, LeBlanc & Tibshirani (1993). LeBlanc & Tibshirani (1993) examine stacking
linear discriminant nearest neighbor classifier show that, one artificial
dataset, method similar MLR performs better non-negativity constraints
without. results Section 3.2 show constraints irrelevant MLR's
predictive accuracy classification situation.
LeBlanc & Tibshirani (1993) Ting & Witten (1997) use version MLR
employs class probabilities level-0 model induce linear regression.
case, linear regression class `

LR (x) =
`

XXff
K



k



ki`

P (x):
ki

implementation requires fitting KI parameters, compared K parameters
version used paper (see corresponding formula Section 2.2).
5. Schapire, R.E., Y. Freund, P. Bartlett, & W.S. Lee (1997) provide alternative explanation
effectiveness arcing bagging.

285

fiTing & Witten

versions give comparable results terms predictive accuracy, version used
paper runs considerably faster needs fit fewer parameters.
limitations MLR well-known (Duda & Hart, 1973). -class problem,
divides description space convex decision regions. Every region must singly
connected, decision boundaries linear hyperplanes. means MLR
suitable problems unimodal probability densities. Despite limitations,
MLR still performs better level-1 generalizer IB1, nearest competitor deriving
M~ 0. limitations may hold key fuller understanding behavior stacked
generalization. Jacobs (1995) reviews linear combination methods like used MLR.
Previous work stacked generalization, especially applied classification tasks,
limited several ways. applies particular dataset (e.g., Zhang,
Mesirov & Waltz, 1992). Others report results less convincing (Merz, 1995).
Still others different focus evaluate results datasets (LeBlanc
& Tibshirani, 1993; Chan & Stolfo, 1995; Kim & Bartlett, 1995; Fan et al., 1996).
One might consider degenerate form stacked generalization use crossvalidation produce data level-1 learning. Then, level-1 learning done `on
y' training process (Jacobs et al., 1991). another approach, level-1 learning
takes place batch mode, level-0 models derived (Ho et al., 1994).
Several researchers worked still degenerate form stacked generalization
without cross-validation learning level 1. Examples neural network ensembles
(Hansen & Salamon, 1990; Perrone & Cooper, 1993; Krogh & Vedelsby, 1995), multiple
decision tree combination (Kwok & Carter, 1990; Buntine, 1991; Oliver & Hand, 1995),
multiple rule combination (Kononenko & Kovacic, 1992). methods used level 1
majority voting, weighted averaging Bayesian combination. possible methods
distribution summation likelihood combination. various forms re-ordering
class rank, Ali & Pazzani (1996) study methods rule learner. Ting
(1996) uses confidence prediction combine nearest neighbor classifier
Naive Bayesian classifier.

6. Conclusions
addressed two crucial issues successful implementation stacked generalization classification tasks. First, class probabilities used instead single
predicted class input attributes higher-level learning. class probabilities serve
confidence measure prediction made. Second, multi-response least squares
linear regression technique employed high-level generalizer. technique
provides method combining level-0 models' confidence. three learning algorithms either algorithmic limitations suitable aggregating confidences.
combining three different types learning algorithms, implementation
stacked generalization found achieve better predictive accuracy model
selection based cross-validation majority vote; also found competitive arcing bagging. Unlike stacked regression, non-negativity constraints
least-squares regression necessary guarantee improved predictive accuracy
classification tasks. However, constraints still preferred increase
interpretability level-1 model.
286

fiIssues Stacked Generalization

implication successful implementation stacked generalization earlier
model combination methods employing (weighted) majority vote, averaging, computations make use level-1 learning, apply learning improve
predictive accuracy.

Acknowledgment

authors grateful New Zealand Marsden Fund financial support
research. work conducted first author Department Computer
Science, University Waikato. authors grateful J. Ross Quinlan providing
C4.5 David W. Aha providing IB1. anonymous reviewers editor
provided many helpful comments.

References

Aha, D.W., D. Kibler & M.K. Albert (1991). Instance-Based Learning Algorithms. Machine Learning, 6, pp. 37-66.
Ali, K.M. & M.J. Pazzani (1996). Error Reduction Learning Multiple Descriptions. Machine Learning, Vol. 24, No. 3, pp. 173-206.
Blake, C., E. Keogh & C.J. Merz (1998). UCI Repository machine learning databases
[http:// www.ics.uci.edu/ mlearn/MLRepository.html]. Irvine, CA: University California, Department Information Computer Science.
Breiman, L. (1996a). Stacked Regressions. Machine Learning, Vol. 24, pp. 49-64.
Breiman, L. (1996b). Bagging Predictors. Machine Learning, Vol. 24, No. 2, pp. 123-140.
Breiman, L. (1996c). Bias, Variance, Arcing Classifiers. Technical Report 460. Department Statistics, University California, Berkeley, CA.
Breiman, L., J.H. Friedman, R.A. Olshen & C.J. Stone (1984). Classification Regression Trees. Belmont, CA: Wadsworth.
Cestnik, B. (1990). Estimating Probabilities: Crucial Task Machine Learning.
Proceedings European Conference Artificial Intelligence, pp. 147-149.
Chan, P.K. & S.J. Stolfo (1995). Comparative Evaluation Voting Meta-learning
Partitioned Data. Proceedings Twelfth International Conference Machine Learning, pp. 90-98, Morgan Kaufmann.
Cost, & S. Salzberg (1993). Weighted Nearest Neighbor Algorithm Learning
Symbolic Features. Machine Learning, 10, pp. 57-78.
Fan, D.W., P.K. Chan, S.J. Stolfo (1996). Comparative Evaluation Combiner
Stacked Generalization. Proceedings AAAI-96 workshop Integrating Multiple
Learned Models, pp. 40-46.
Hansen, L.K. & P. Salamon (1990). Neural Network Ensembles. IEEE Transactions
Pattern Analysis Machine Intelligence, 12, pp. 993-1001.
287

fiTing & Witten

Ho, T.K., J.J. Hull & S.N. Srihari (1994). Decision Combination Multiple Classifier
Systems. IEEE Transactions Pattern Analysis Machine Intelligence, Vol. 16,
No. 1, pp. 66-75.
Jacobs, R.A. (1995). Methods Combining Experts' Probability Assessments. Neural
Computation 7, pp. 867-888, MIT Press.
Jacobs, R.A., M.I. Jordan, S.J. Nowlan & G.E. Hinton (1991). Adaptive Mixtures Local
Experts. Neural Computation 3, pp. 79-87.
Jacobs, R.A. & M.I. Jordan (1994). Hierachical Mixtures Experts EM Algorithms. Neural Computation 6, pp. 181-214.
Kim, K. & E.B. Bartlett (1995). Error Estimation Series Association Neural Network
Systems. Neural Computation 7, pp. 799-808, MIT Press.
Kononenko, I. & M. Kovacic (1992). Learning Optimization: Stochastic Generation
Multiple Knowledge. Proceedings Ninth International Conference
Machine Learning, pp. 257-262, Morgan Kaufmann.
Krogh, A. & J. Vedelsby (1995). Neural Network Ensembles, Cross Validation, Active
Learning. Advances Neural Information Processing Systems 7, G. Tesauro, D.S.
Touretsky & T.K. Leen (Editors), pp. 231-238, MIT Press.
Kwok, S. & C. Carter (1990). Multiple Decision Trees. Uncertainty Artificial Intelligence 4, R. Shachter, T. Levitt, L. Kanal J. Lemmer (Editors), pp. 327-335,
North-Holland.
Lawson C.L. & R.J. Hanson (1995). Solving Least Squares Problems. SIAM Publications.
LeBlanc, M. & R. Tibshirani (1993). Combining Estimates Regression Classification. Technical Report 9318. Department Statistics, University Toronto.
Matan, O. (1996). Voting Ensembles Classifiers (extended abstract). Proceedings
AAAI-96 workshop Integrating Multiple Learned Models, pp. 84-88.
McCullagh, P. & J.A. Nelder (1983). Generalized Linear Models. London: Chapman
Hall.
Merz, C.J. (1995). Dynamic Learning Bias Selection. Proceedings Fifth International Workshop Artificial Intelligence Statistics, Ft. Lauderdale, FL:
Unpublished, pp. 386-395.
Oliver, J.J. & D.J. Hand (1995). Pruning Averaging Decision Trees. Proceedings
Twelfth International Conference Machine Learning, pp. 430-437, Morgan
Kaufmann.
Perrone, M.P. & L.N. Cooper (1993). Networks Disagree: Ensemble Methods
Hybrid Neural Networks. Artificial Neural Networks Speech Vision, R.J.
Mammone (Editor). Chapman-Hall.
Quinlan, J.R. (1993). C4.5: Program machine learning. Morgan Kaufmann.
288

fiIssues Stacked Generalization

Schapire, R.E. (1990). Strength Weak Learnability. Machine Learning, 5, pp.
197-227, Kluwer Academic Publishers.
Schapire, R.E., Y. Freund, P. Bartlett, & W.S. Lee (1997). Boosting margin: new
explanation effectiveness voting methods. Proceedings Fourteenth
International Conference Machine Learning, pages 322-330, Morgan Kaufmann.
Smyth, P. & D. Wolpert (1997). Stacked Density Estimation. Advances Neural Information Processing Systems.
Ting, K.M. (1996). Characterisation Predictive Accuracy Decision Combination. Proceedings Thirteenth International Conference Machine Learning,
pp. 498-506, Morgan Kaufmann.
Ting, K.M. & I.H. Witten (1997). Stacking Bagged Dagged Models. Proceedings
Fourteenth International Conference Machine Learning, pp. 367-375, Morgan
Kaufmann.
Weiss S. M. & C. A. Kulikowski (1991). Computer Systems Learns. Morgan Kaufmann.
Wolpert, D.H. (1992). Stacked Generalization. Neural Networks, Vol. 5, pp. 241-259,
Pergamon Press.
Zhang, X., J.P. Mesirov & D.L. Waltz (1992). Hybrid System Protein Secondary
Structure Prediction. Journal Molecular Biology, 225, pp. 1049-1063.

289

fi
fffi


! #"$ % '&)( *,+'( ---.$/10 24315



64789 :;/1<-3!=>7
%&:@?<--

BDCFEHGJIKMLNKPORQTSVUHW4KXICZY\[HK]CFLNKMSV^_Ca`bBDC;OcQdGHegfih!GjK

kmlonqpsrutwvyx,z|{~}'r,pff

oTssooo

Ns,~PN]'Ms1sMd1a
ffjq
mmsqFff 1

y;m
# a#4, ! |Hjq@|,F |

Taq;9d41a)q! ,q# ]aP dffJ!
]@ !u4 Tm9@T@141a;94 F
a49~@1Tq M1 $
Nj 9]P
F]
$
ff
fi1 H
! 1"#$
" 9 % 1]1& '
(
)*q+,+-M .0/132(45,6"7%8 19MF
*:
;$ "+<;
=
("1>?
1=
&1
(m,$&@=

(4
A$
(&B/DCFEHGI6 JK ", L
M$fi N
( AOPH 4R$
9CRSU
Q 7V ==&" 1W]< /GAXYC6Z&K
P&

< [/DCFEHG6"\]K ; R |
;
;< [/G^XYC_6B
(a
$I1& fi `
&M >fi %Ga* qbC7 (
- .

;$ " Hc;
< [/G+XYC6/;d
& Gd$fi qff1] B q
; &&@
(KKHP

;< [/ G+XYC6%J
Ge fOg6"o
\
9$\ & :=j

;$ " ihjK 4


Q T\
kflox <;[/ G^XYC_6 h%/[;< /G^XYC_6!6F FCRSU

fi
ff;< [/GdmnG:opXYC6qff
|;$ "i9<;[/G_opXrGdmsC6q
fib<]/G^XYC6"\9
\

;$
" uteH 4|


& NI


$ 1

Q T7
kuvx <;[/GwmuG XYC6 t/[<;[/G XrGwmfC_6"EK<;[/G+XYC_6!6xGymfCzS{
| ff" )$
@ L]< $F
M&$
(j;$ "J\qffM9
jK
(Mh%/;}]6 1]~N}^
fitN/;}]E"$6
}''7@M
- .+N
( %4IM

( &9
("K$ " W
("K ,
%t;
`= Bfi$&1q"?
((\ =^

" ,B1 fif$fi &K*(
"* (\c


fi
qh
`= fffi & q"?
((79a
$fi )
& 1
("KN " \
4
=

ff]< F H
&"o R
10
i1& $
( `nfi "&"'s" n N1 q1 j
1& Nff

" , $ 1 @1T;$ " :A { H 4
ZJ>]< x
B1& $
( `_fi "&K$s"

iO\$

fi
'/[;< /G^XYC_6!6Fu'/[;
< /DC_6!6 '/[;
< /GymfC6!6F CS{
Q T\
/1(6
d& :]< /DC_6F=
f
(&1*?
" |&=;
< [/DCXrOg6"7
- .JW& K F
(W
"&H
( 1fin
N1&
M$fi
( 11& \L$
(& " ?
(&"i
| ffMK &"&K" \J@
N
. ?ff q"& u Nff$ u
fiL\$H
& W& q"\ :B Aff$ 7o
& .

!qc;[!) [K[Z[H:JD%r,K(qH@K([=FppYZY_;)KH(HH%[K[
[(KMD(? ;;DcH9KY:;D!J p(`@Z%YA[p@HpJ`Y !c:9HYcp
pHpYL[FD3K(BDZ[p3$$(;DDpxY(D[D=Hp)pK(H[=Y=[pH
[[K[p




%



F





( --- %% !: : M1 !fi8, 7!
%&%

&1%o% 1 :

fi



- 1 N
A/132,6
(
(fiff#""&1
(&"Hq&x 1P 1K
fi$
(&"fi^/[<%
"?
6

$&1$
(`$&!>7F8 ??
(&1q"?Hq"F
(&.$&1"1fi,I
B/132(>\c7>4>6">fi$fiL\
- .J=)
@
& )
F _ & &" 1$
x
'1& qB0/13225,6"7
&= $
(&" Z$1& $

&!* "(\ K &KA
J\'
fiiZ
ff"A/1325,6% 1fii B
(=
I$
(K
$
fi ffT
& & $
( ":
($1&
(4 )
1N1&
(
(s=$ 1& K
( `7
K &HN
f/132,6x fi F
(F
:$
("P &@$1& *fi :

.
"
" &@ J Jfi$
1(7
)IA
( f "&"$" PM$
( &Ma
1u a/p,H

F

y. $1& .
N 6


BM
- .=1& K )$fi )
fffiu ^~# 1M$fi N
( \ * qf$ $fi &% "1& ff
(KK$ "
hj
fi
t/p1 "1& ,& $
j1 BN
($fi ),NM
- .A
H
fi 1 =A
($fi ) N
('$
( &"x1& 3*> _*(
(&K?
"] L@
- .J
& K"6"7^8 ff# 1$fi
( :
(1& N
(&"$
(
1
$1 H)H
: 11& nBq
($
" \
K 1 "
x
(&" q"# & " =1& $
( `:$
(1
fi
- .1& H Dw

fi ff& " "#$
"
"?N?
(&: n"$&" Dwff MRK
( qnB
u&K
( %K
( \
fi &1& :
(& 1& *
fiL7
- .J
("H " @
(1& 9 HN qP1_$1& *
1& * &\ $9 $11& .
N =H "]
)@
& K* q u # 1M$fi N
( 7
1

$

ff1 H
N
(KK$ " :1&
(&"fi$ ft
fihff ^N
($fi H1i1& 3* +@
- .J
& K7qA
fi@&H
($fi /1322,6* 0
w.
N
1

" ;
< p\%$fi ~# w
fi
~# 1
fi
( Js\
9)
$
ff)1 &"o
1
ff1& '
( +fi "&"'s" J7$ @
& 94 ;
< p\


K
(
ut/;}ZE"6 J/;}]E"$6_
fijh%/;}c6 1_~j}Z78 ^N j
ff= ufi$ 1& q"?
((\@
- .J

("K$ " )$,H $Mq!@&K
($fi F.
$ (7
T$&:
s&"
$
* NA
($fi fi$ 1& q_
("K$ " 7W$ B/13255>\@s8 " 0/p)
& 1(6!6
fi
$
ff_A
( ff
n
("KN "
(sMtI\Z'sJI$fi :N
( RM
|ff&
("KN " N\
(!

! >K
F&K
($fi H.
(7A)
ff#$&"1 :J

J$N;
< [/GAXYC)
6 K
( R
* &!
*
(?M +1 H
q&K
Nr(E":p\'= _7x MM @&H
($fi a.
N (\ M$fi
( A9~# 1(\
q & K
(
$ffJfic7qd
1
fiT

: FGa
fiG:oZ
(1& fffip m\ q1& ff:

" ,);$ " ig ? \' "&" "u 1&
("
(4 0
(&" q\H 4 $


k+m x ;< [/GwuG_opXYC6 /[]< /GAXYC6"EK]< `/G_opXYC6!6"7
Vw H B
("K$ " \ =* )
_1& Jff ! =K&" P
;]M
- H
. 1 3 $
;
<
; "1 q"?
(A
:& $
( Afi "&"$" J7M %
fiNF&K
($fi 9 ]s]
\ j;
& .
N (\
1
& W@
$ " +K
" ffWA/ * q+ LM
qfi1& B& > & H
qP
== " ,

fi1 "&" "u 1&
(" N
(4 0
(&" q6"7 ?

6
(&" &B$1& * fii
1& K =K??
(M
& 1NW$ ;\L$ $fi &)1

9 "& &
4 q,$
(4 s/132(42,@

("K$ " 7 u$
(1& " ?
(&~\ :
(KKHfi+q>\$= fg 7
T$&%*(
(&"?

"

- .J)1& K
* _
(1 Iq^ "$fi & fiu q 1&K
K &1(7@o
& .
N (\


K





K
&








/
3
1

2


,

9
6





fi


!
&

*


"

(

\


K





K
&





J


\






fi
Z










ff

"

^


/
3
1

2


,
5
9
6
(


"

K











B



= " >



W

fii1 "&" "i 1&
(K
(4
(&" qW
fihB " >
fii1 "&" "i$fi 1&
(K 7
8 9N , ff "&" "I " , j
(4 N
(&"Hq\ ,;
(9 & "&" " R17+B ?$$
(
/132,6@* %
$ff&% " ]
("KN " @
fiA?
(?NP
]NK N 1
(&K
1


B;< cd
"1 q"?
(i
I1& '
( ufi "&K$s" J7
$_H`!$DD J"!][p%[q9M px(`pYNFNHF[I;D=sH[);YH pH()HL[
DH(3pYK?$!JD3[H '[HxDK)YxcH]DqpKpW[!HDZ`( pH,
3'[(K"['p![;`pYF[K
NJ[;`pxY(Dp!HYY@![%Kp)D , cppY[[pH +JD
[D9cDHW;!c9[(K
ff fi fiH ]c" >3;@[(" JKc[;`p9Dp!HFHK>
HpH)'


fi

o$s$!P"$$#@$~%PF!&


o$('*)$

$I#$&" 11&"* N ff1q"?
(1& $ M= bM
)
- .J&1K _x
(&"^/1322(4>6"7fWA$s"
\_H-@.J&1=Hff\F&4
(\@
(&K&1
(I1 ^fi$
"II&1&ff
fidq


1 $ WMN
(ofi F1^#$Z 0
( $fi K
(q1 H

uff
"&K
("*q"9&"
(B17
x
(&"_$1& *$fi _
u&K 1& :& ]]$I1& K \
("KN
JI&K
H;
< K
( $fi
>E1%
fi "
("KN " M"?N?
(F
& 1 1 &!* "(\ K &HN
J\x
fin
ff"(7A
$
(& " ?
(&N\
("HH)

WtzW " >
fi0 "&" "0 1&
(" s/[>E1 ?
|
fi
_h
fi$1&
(K 7
* &#\ ffN
( : ,$ff
(fi$fi "
(x
("KN "
\]
(J
R ?1 xH
\
)
ff=* &!
(
(
Q T\
k,+Nx $&W
(].-0/@E21E435-{1
fi687>\$$&1_
(&1:"MC (:9 C ?;9 C 9 C<:K !
MC S{

|

fi
(4 %X ;< [/DC < XYC 6Z=
~ /X\X ;
< [/DC XYC ? 6]>
~ 1FX\'
fisX <;[/DC ? XYC ( ]6 ~53xX9")
67
| ff"$
) W
("KN" &")$_&K
<;N1N:fioq 1_b >E1p7=) WH

\
u$
(&1" ?
(&\$_fi$N
(O +d4<;cBfi$#~fiu
$$ff=q#~ 1(7
)
q
(KK$ " 1&
($"K
(&!?x
(&"qK "T
MW$ ,fiW1H Nff7
/p)
"KNfiK "1 fin i; &&:ofi K
(F _76jd
$11& .
_$
( &*
; &
& *$fi q (7% B $ =)

qM
- .W1& K)
(W u# 1:$fi
( o\ * qf ;

("K

B&K

c;< $ f >E1p\$h%/;}]6 1~N}/p1
\> N$
(& " ?
(& \ hnP `= Bfi$ 1& q"?
(q
fi
$ff1
(n$fi 1&
(" >6"\FI/;}]E"$6 }+y$\x
fit ~# 1fi 1& q"?
($^
fis "&" "
H
1
&
(K R
f/[>E1 ? 7xV
; &$&
("HH)
)td% AK
"* (\t/[>E!}c6 t/;}]EK,6 >\



fi
Mt/;}ZE1(6 t/1E!}]6 }Z7:)
.
N
("

aff
(
($
@
- .J:1& K MJ
;
(J
&
(&"&

&
0%
(_1& * "f * fiL7 q& N
(
q>$ "

(F
1uT

& 1&
n
(1& $&"?
1I "1& q ff $q ;
("K$ "

$fi M* ^
- .B1& K 9 +# 1:1 " 7@)

1& Md
&&)fi" "" 9"HM fs8 " (@,7

( #\ R.
N ff BF

* qH
1& (7: ff &" ; M1& `\ZM
- .?
(?a
1u 3


9tgff %q
+
(" ,?
"* $ " Js\
9s\
9t/;}ZE"t/p$2E A6!6 t/ptN/;}]E"$6"2E A>6"7o @
&
< # $11& .
N (\ 1& _
fN
(K1 ,?
"* R;$ " ftK
" ^q,7) =d
;

1&"*(

"
B)
_ 1A 3= j

d1&
$1& $
( `fi "&"$" 1 &"o
1N;
< p7
V
]c )

J& ?^B K
(\M
- .Z1& " c $ =m $
]t/;}]E"tN/p$2E A>6!6 t/ptN/;}]E"$6"2E A>6
_
& $1 "&" /;}ZE"$2E A>60K 4_$
H
\ &1 "C ( \^C ? \+C \
fiRC < \ff;
b
*
} ;< [/DC < XYC mC ? mC ( 6"\x ;< [/DC XYC ? mC ( 6"\@
B
fi ]< /DC ? XYC ( 6"70 M$A1 R)K !
"&" q/;}ZE"$2
E A>6@%$fi q1 = >E1 s\ qAM
B ?fi$B,N " ,
)td)
(" ,?
"* (7)

1qj)W4f1& 1
$N1 R)K ! "&"$ $fi q 1 >E1?

7 a% &"1 (\x BO
B~# 1(\$M
_
offa$
* $fi q " `79Bq+ o11& .
$ _ 3=\';
$fi ffW q&H
(
*

("1 >?
"* +# 1M$fi N
( 7 & * &\ 9?
(H
(" ,?
"*
i1& K 9 J;
(? &1

- .J
$1& u7
UK??
(&1& 1 ] 1. ) AB' ;9& x/[
()
(1&
(fi 1&!* fi+,
(&KW/1322(4>6!6"7
VD MB' ;W1& ]$fi
ff9 * * : 3= R

9tgB
("1 >?
"* (\' 9$fi ) * * 3=



(" ,?
"* (7uW
( J\
ff$
(&"y
fi 1i $
I_
(K1 ,?
"* &_
(& &"?
1
"&" \$"
(J

(1 &_t7+<%s 1 NF

W$ )
(1 fi$

(KK$ "


(&K
1#
F
(1& &K?
11 #"&"$ ]]ofi q 1 (\
fi: ] ffZ
(#&
#
("KN "
C

fi



fi$ff ;
(9
(&K
1a 7 < W) T++8"i,\q&1
(
(&"1) !q,$
(4J
1& `7
$N$1&1.
1-M .& u\=KH>fi#$
"\
b
(10A 1fi 1
)
3
9
ff&; 3d
+1& K % W 1&K
K &1q@
ff) 1^ &"1& 7 %1 ff

(
1& $
($ 0
fi>
( K
"* N1& $
( `b/132(,6"\ ff "$fi &K:
$> ,$H&"
ff" jEDGFHJIKLMKNPORQTSVU4D2FWXYORNPOFWKZ [JI!L\F^]_K%]`ORZaORNPb\>d
!
( =% u1:H
0Cw* q^GU@
F
(

(1& $
(^
(CWo* qjGMo\x$fi qff1fiCIXrd
G caCBo[XrG:o7fM
- fi$ "
ecz
(1& N* qi

(1&
?
(?H
R
fi 1F&" ;@. 1q @Z/[
H

ff9& 6]T

$ " I]< H 4
CXrf
G c{C XrG
j]< /DCIXrGI;
6 gd]< /DCBoXrGMo6=
fin

(" ,?
"* $ " taK
"1 > uq,7+/p)
MM)
$1&
@
H
- $
( 1&9") /p $(\]132(,6"76
* &\ _;
< $fi ~# fii i^ o11& .
$ J1^@
- .J
1
&
u: 1fi 1N*
$11& .
a1H =1& H =
(9M
;7
1& "

\ + ff 0#$&" "?H

sK??
(& &"1& &
(+q ff1fiw $1
;$
"
(N
" 7%'
(?N
( A/132>1(6%* q
ffT
& .
N I/p
N
(1 _ * *
As"
>$fi Z! :q
*> &6@
fi q"
H

)q $ =ff1
9
( @M
"?N?
(M
& .
N )
:$!>!

( 1&K
K &17
$W& N
( $fi
)
& 9$
( &%@
&"
fi0
(@
3=7@ Js. 91 " & M9
IH
1&
fi K
( fifi$" "" m$W$1& R uM
$
- .J)1& 7d
B o11& .
$ d1M
- .M
$1& z
* q s8 " >7_d
B u1 " $ =T

: MM
(1
^ $11& .
N 1+ $(
1
& 7x8 " h@= ? fioZB _1 @fiK "" J\$
(1& " ?
(&"

("H " Z$ ofi &cd
!
- .)

1& N )$fiL7
i#kj,lnmBo

qp

m!r





lt

u8v


u
w

fi$&"K
fib&1$ Bw-@.Ju&1\9^
(K
(y"fi$& !q'
(4J$&1`\
! 0WK??
(&WfK&":-M.JM&1)/pMq
(K$
(*q1&a1+W$ ;:&16"\L$sW1
ff
(fi$fi "
(@
("H " \Ld
1
! nN
( :
(" J
& 1 . ?
( n ofi K
(;7NB' ;\FM
- .\
fi
! q,$
(4 f
(cN
( q&" "
(] T#o "
(m
" 9 &)$1& `\
fi uN
(
K
I/p1 u$" "~# fiJ6
(f
= &"& " fi "9 &9$1& 7
R)$ffK
" F$
( &\ ! q'
(4 0/132(42>\ L7xY5 @`x>56x
("HH9/1(#6
P=&K


Z;
< [M/ yzX y69
ffK 1M) >E1p\]/6F;
< [/G+XYC_6 1M |
C { GN\L/[,,6
9 G
fiG
(1& qfip \
qA]
< `/GnffGMo[XYC_6 ;< [/G+XYC_6f;< [/G_o[XYC_6%'/ , s\ B
("K ,
W
fi\>=
6"\Z
fi/p4>
6
_M fiMB
"
_Mfi$ 1& q"?
((7i/ 1&
(&">d
$
J
& KMfi$] * q^= $s
("H " f/p4>6"\
( W1& L)H
1
1& B
1fiLW$ L
;
(
=$fi )
ffBA
( :

("K$ " ^ N/p4>6"76
3= $?
( ^G MI,fG ( G ? \$d
& G (
fi
4 q,$
(4 LM1& $1& , fi$B
(T
G ?
(1& :fi; \;

< [/Gym/G ( G ? 6XYC6 tN/[]< /G ( G ? XrGwmfC_6"EK;
;
< /G^XYC_6!_6 }
/6
W" j
F;
(
qgWff\ ; :?AH
fi?
1^
< [/Gwm0/G ( iG ? 6XYC6 ;
;
< [/GwmuG ( XYC6Jj;
< [/GymG ? XYC_6
/[,6
~$L;:[p!pc[(KZFBDH[pDK)YxcK9DH[pDK)Y[=D` c[HpqY(`F@`3Yp
p;)!J[("Z[x[HHH$JD,LY [JZ!JD3!cHYH]H[pKp3Y9JLY[M3]pH
H`;KY;!][Kx3xKp)xH@K3YM[:
(3[%H9KY(D`D :H`@LKp)H[
pHY9LL[_3ZpK* ;HK[FDH)3KpY>M _?xxK[p"!qp!;3]Y_=Y%H;Y;L[
[@"MKJ)p4 Yp)D ]K TGff2ff\?`D xHp D`4 ,% M`c3MZYqH`9[p
p4 Yp)D [(HW ~3(K(=[ LK;)p4 Yp!J[9"YB[%Y [


fi

o$s$!P"$$#@$~%PF!&


o$('*)$


fi

t/[<;[/G ( G ? XrGwmfC6"EK<;/G^XYC_6!6
t/[<;`/G ( XrGymfC_6Lj<;[/G ? XrGymfC6"EK<]`/GAXYC6!6
&1 *&\J,^M,\M_
(1
* (~\ &: 1EH,\

/p4>6

<;/GymG"XYC_6 t/[<;/GymG!XrGwmfC6"EK<]`/GAXYC6!6_}
F " j1&/6"\Z/[,6"\Z/p4>6"\J
fi/@6"\$M:$


/@6

t/[<;[/GwmG ( XrGym0C_6"EK<;[/G+XYC_6!6qjt/[<;/GymG ? XrGymfC6"EK<]`/G+XYC_6!6
/[5,6
t/[;< `/GymuG ( XrGwmiC_6J<]/GymiG ? XrGwmiC_6"EK<;/G+XYC6!6_}

(,} ;< [/GmG ( XrGbm^C_6"\ <]/GbmAG ? XrGmAC6"\
fiA <;[/G+XYC_6i/[5,6"\ Mq
J;$ "$
(m
>
"
t/;}]2E A>6ct/p'2E A>6 tN/;}I'2E A>_6 }
/6
8 1 )$
9;
("K /[
( 4 $q$
(! i?N "u$fi ,6
)
$ "
(m >$
"
fiP
&@
(Z/;}]E"'2E A>6 |
;
/
]
}
"
E
'

E
2
A>
6
>E1 }9u"

-{
1
7)



)

&
1



1
P







)1
& $ B


("7&K
$\ K
(> } ff n/6"\$ 3=)
$

t/[>2E A>6Ljt/p$2E A6 t/p'2E A>6"E
& zd !+M:

1

/[>E2A>6 }
N
| .\J# .A+
fifBY(/;}c6 t/;}ZE2A>6"7:8 t\J,f
("KN"J\$fi$&1q"?
((\N&1 /6M

*J


/;}c6 ? /pt/;}'2E A>6Z~nt/;}Z2E A>\6 $6 ? t/p'2E A>\6 ! }


*



*

)> Bd$
=o /;}c6)W
K
\L fi$q fi$q)x}Z7W8 _K
:A
fi$q fi
BA9
\ 1& ^ H
;$ "bK ! $
_ /;}c6 c/A>6"7BK | ;
(
fft/[>E2A>6 >\
H
qK
(&!u
( ? )1B

Y/;}]6 t/;}]E2A6 c/A>6[}}
W" j:
(KK$"
)&=
(CFEHGN\
*<;[/G^XYC_6 1C{ G\ ;:d

<]/G^XYC6 <;[/GwmuG+XYC_6 tN/[]< `/G+XrG mfC6"EK;
< /G^XYC_6!6 t/1EK<;/G^XYC_6!6_}
)> \ MJ
*


t/1E2A6 c /A>6

A}

V _? fi$a
=tN/;}]E2A>6 }A7
| ff1(\P M*&\9
? "sfi$q fiMb
&K?
()

("H"

$
"
(P
" y/F
6 $fi
&
(B/;}]E"'2E A>.
6 I7 5 ;
( \x
(]
;A
s? fi$
1
& /[5,6FM

) )fiM
&9
(x/;}]E"'2E A>6@K !
M1& F. WC\'G\G ( \
fiiG ? \ B +G (
fi
G ? fi; \$H 4 $
9} ;< [/GwmG ( XrGwmfC_6"\ ;
< /GymG ? XrG mfC6"\'

fi ;
< [/G+XYC_6"7
'[(KY[c`pY33@H
F; 9`![K$[J(pH("(4 ("pY)K 'x;D'H[;YY! c[
c;Y
F


fi



@ K

#

@d

"&" ;
K
""o# 9 x fi$"ff;\DGFWNPL\KOW!S2X_/p")@ZK
"
&K
(s "&K
("_?N1fit
fis $" &1I& 4$q$
(!J\u1
fi " ff" b1&
"?N?
(&Hfio#~fij |$.N"J76RWIIHq"fi

(&" &\LW$
(
^
("HH)

W;< [/G+XYC_M
6 K
(
f
(Z*
(?B r(E"_p\Jd
1& ff ;
< [/ XYC6

fi ;< [/DCXYC_6"7q/; ! q'
(4 Ja

&Kff ?
" J\
fi 176ffd
1& _
(1& JM
)

1ff 1&K1
& )
("KN " J7x)
BM

(N 1&"1& K
" u;

@M
&
(4 +(
} >E1p$\ 1& T.
CFEHG H 4
;< [/G^XYC_6 }]7u)
"& 1&"1& K
" b


&
(! Ca
fi}]\ 1&
.
"WGK !
B;< [/G^XYC_6 }]7 9)
ff9
(&Bd
! + 1&"$1& K
" i= 1q ofi fi+W$ ;7
& $,*> KKN )
1N1& 3* J
T* &! "&" "&K
( fiL\L
(
|
fi 91 ?
"a$
= B
$
1& M1 fi0
("KN " J7
u
^
(1 (~\ $&)B' # $& 4 q,$
(! 1 :R


fi 1I! K
:F>
" /]6 $fi
1
& sI7W/ | &W$fi WM
- |
.
& =

( a;$ "
( >
" L\ &B$fi j_
&")

& & q
fi 4 fi1& 1 qK
" J
H
)M
- .1& K \]H 4
(_,
/13225,6M
fiZ&K$
/13252,6"76 3; * &\' ;K&
s;1IW 1F "K
(&! 1I$fi 7 & * &\ %%
(@
& $
%
~# 1(\ $1& W
(& T~# 1H
K % "_
9
(1& "&K
( fiL\$
fiA )@
ffMW
(1



( w7BM
F $
(> F Ms. 1 " L\ u 1&!*
" j
(1 &" >q

()
;
(&=
(=
( 1 M& =
(1& : & fiL7
ukj,lnm0t






u9

r

p

ff

!u8v




j,lnm

mr

)M
(#=1"i1& *
Pmx;l$

tNpl~p


KYNPOffbTOW^.



JS`Z

*








L4SOhKW!D`NPOFW5JS Z
* KWORNS"XYFH.KOW
KWXk"L4SIS_DNPORQTS ZabkD
N KN

/G^XYC_6qb >E1 FL:CRS{
Q

_h

KWXW!D`NPOFW


/;}]E"$6 }^wU F5N KN=OkNPLGOD`NPZab$ORWDLMS2KOW^ORW0S2KYD




KYLYHkS`WNKWX5O,OW WORNS`Zzb

KLPHkS WN
OW

KLPHkS WN*OW+ >E1 ?
KWXNPLODNPZzbORW

tOD2FYHVHhNKNPOQS t/;}]EK,6 t/[>E!}c6

WFWX%S2D`L4S_KORW.OWS2KYD

I/[>E1 ?%8
KWXfft/;}ZE1(6 t/1E!}]6 }
DLMS2KOW^ORWS2KYD

FS QTS L







tO8OW WORNS`ZzbVXO ;S L4S WNPOK%] ZS



+

KWX



h)/;}c6 1B~}UFN KNhO;NPLODNPZzbX%S_DLMS2KOW^KYWXORW`WONS Zab"XO L4S WNPOK%]`ZS4[
XYO S`LMS`WNPOK%] ZSM[



FL4S_FYQS L


NAJ >E1J >E1KYNPOffbTOW^U4[

N L4SVOWFFW!S \NFMFW!ShFWNFWDNPOFW





>ff,I<; * \>h\>I\
fiItU
(&19

(F
(x1"&1 :
(99N
(fi$I
(
Jff$&9*
(&"?
"d-M.J=&1H \d a$_
("KN" @^
(&1MM
(&$
1N
(fi$
$*
(&"?
" 7$
& .
$ (#\ &
$1& 1& H
qJ
_ " > &_ &1
(K
&a

qW;< * :
u& $
( fi "&K$s" /[
( n
(&K_
fiW$ ff1& 3* R
\
$
fi )
& &M
("H " \$i
K
( q1+K
"1 0
( 1 ff1& 1& q"6"7:d
B &!*
1
N
( a$: $11& .
N : 1: "& 7
|

ff1@ 9@

`

fi

(


o$s$!P"$$#@$~%PF!&


o$('*)$

$&1j)$&1 >71j "&H "*(7
)
- "fi$&0
fioN
( =1"
E }}}E (? 7V
("1>?
1:=
(4 0O
I;
J/ 6"\J
( =7
/
/
/

<
6
/ < 6
@ff13 <
/
? 6
5 6 5I13
<
/
/ 6 I13
6 5
3
( 3
/
/
0 6 ff13
( * 6 ff13
3
( 3
/
/
3 6 ff13
(( 6 13
/
6- ff13 3 / (? 6 1 413 ( 3
$&
0K $1NC qO\x;+fi$#~$"/DC_6 SY / 6"7)> \;+
fi$#^
f&1'
(
fi"&K$s"uF&@fO K
(> @&3/DC_6 /DC\6 T/Og6"7
#o=ffi$q"
(T1
\ . $
o/ ( * 6 /[^~6: 13 ( 3
fio/ (( 6 /ff
6_
13 ( 3 \=d&1(+$fi ~# fi 3:7W
( J\B;
. 1q
fi 1bK $1" ofi ~#
o;/DC_6
Q \$ofi ~#
o/ 6"7 WOo |
( * E (( E (? 7 @CRS{
o/GwmfC\
6 T/DC_6 xOUo {wC
<; * /GAXYC6
/GymiC_\6 T/DC_6 ff&!B1 (7


(

<; *
(&"ff*&!ff1@1MF&7LCSw
Q \$qff,
(!1:1M
BX <; * /G^XYC_6~I@&3/GAXYC6X
X /GymiC6Z~>/GymfC_6XT/DC6q-07@V :!1h7ff1H

F&/G^XYC6q7j@&3/G:o[XYCBo6"\$qi<] * /GAXYC6q7<; * /G:o[XYCWo6"7

/[,6

8 a$:&K
J@&=)#~ 1(\'
(]KNq"uKN
(
K
"1/[,6"7
$.
( +! yq; "
( * 0 ffA$
(1& " ?
(&"?N &1K
7_J|
+
)
? & K
@ @
J = "
( "
F& / ( X ( E ? (6 F&/ ( * X ( * E (( (6 @
F& / ( E ? ,X ( E ? E (6 F& / <,X <E 5 (6 @,11
F& / < E 5 ,X < E 5 E / (6 F& / 0 E 3 ,X 0 E 3 E - (6 11 ,132
/[2,6
F& / <X <E 5 E / (6 F&/ ( * E (( ,X ( * E (( E (? (6 @,132
F& / ( X ( E ? E (6 F&/ 0 X 0 E 3 (6 ,11 }
9)

(! 1! K
d.
( " _K
H

( "
$fi ;
:& ?
( :@&=u;
< * 7
V
B;< * H
"!o#
$:1& 1& H
q"
)
& >71,
I1 q J
(7
9#$&"
q, $9 j
)
1_ $ =

F;
< *
offF)1 &"o
1
:1& '
( H
;$
" J7B 1)
;
( ff/p1& * fi f
>7Y,@
6
W @;
< *
1& H
&"o J
1+
1& '
(
;$
" J#\ qy1& ffM
fi $
* H1i|

$ " taK
"1
_
("1 >?
"* (7^W
L\
(x
3d
ff ff
=>7,\ @;$ " fft{K
"1 MqW
ff;K
( qR1M% ~# 1
fi 1& q"?
(=
fi 1&
(" j
(! N
(&"$Hq\ @
( " F +/[2,6KN M1:$
(&K
1

=

$$ff=aK
( q 1:
("1 >?
"* (\
=\M
:$fi H$ff= u q&K
(#
*
t/;}ZE"t/p'E2A>6!6 t/pt/;}]E"$6"E2A>6_}
fi$fiL\&1J+
("1>?
"*j$ " ftzK
" +q,\N*q;fi&1&1 &1 Hq"

=tMfi$&1q"?
(&9 1
&
(" 7


fi

]p=

Nx?v
{

Jlml2


fi $


FLJS Z

KX%SPW!S2XK%]_FQTS


^tKYNPOffbTOW^k

N L4SVOffWFKGF`DOKYNPORQTSWDNPOFYW



8 1T&1qM&1:H 4
H$"utI7@& [/ 2,6"\$M:ff )
*F

t/ @,11E11 ,132,6
< * / < E 5 ,X < E 5 E / (6!6
t/[;< * / < X < E 5 (6"EK;
<
;



/
X
E
E
(

6
<
<
5 /


@,132
*


3=)$


fi $


*





t/[Y@,EG@,11(6
t/[;< * / ( X ( E ? (6"EK<; * / ( E ? , X
< * / ( X ( E ? E (6 Y,11}
;


(

E ? E ( 6!6

t/[Y@,E"t/@,11E11>132,6K6 t/[Y@,EG@,132,6

/ptN/[Y@,EG@,11(6"E11>132>6 tN/[Y,11E11,132,6_}

)> \tgM&1_
("1>?
"*(\;MM fi$
*
F

t/[Y@,EG@,132,6 t/[Y,11E11,132,6_}
$Fff&d
fiL\&1
/[2,6%
(
( J\M
:1 J

t/[Y@,EG@,132,6
< * / ( * E (( ,X
t/[<; * / ( * X ( * E (( (6"EK;
<
;



/
X
E
E
(

~ \6 ,132>E

* ( * ( * (( (? 6 /[:



( *

E

((

E

(?

(6!6


t/[Y,11E11,132,6
t/[;< * / 0 X 0 E 3 (6"EK<; * / 0 E 3 ,X 0 E 3 E - (6!6
;< * / 0 X 0 E 3 E - (6 Y,132}
3=)$
=te
$ffB:
(K1 ,?
"* (7J
f$fi$&"K
fi $ z NA
+>7u1& ?
11&_fi" ""b8s"b|;N&
= !q'
(4Jx& `\M
K
N/;}ZE"$2E A6LV

D2FWNPL\KYORW!S2XVNPLGOI!ZS0 1& ]. 1 "FC ( 9 C ? 9
C 9 C < =bC
Q H 4y$
M} ;
< * /DC < XYC 6"\] ;
< * /DC XYC ? 6"\

fi ;
< * /DC ? XYC ( 6"7
H
(! 11

&" 1u
(" ,?
"* "&K
( $b
fi "&" \F"
<; * /DC XYC ( 6%
fi ;< * /DC <,XYC ? 6"\,uq,\M

* tN/;}]E"t/p'2E A>6!6 t/;}]E 6 ;
< * /DC <,XYC ( 6

fi0t/pt/;}ZE"6"2E A>6 t/ 2E A>6 ;< * /DC < EC ( 6"7BW4+K



d$1 ax "&H
(
fi "&" q
fi q >E1?7
$
V K??
(&"$fi ~# /;}ZE"
6 1u

DGFWNPLMKOW!S_X$IKOL0 d1& . N "+C ( 9 C ? 9 C
= C ? Q K 4i
ff} ;< * /DC XYC ? 6:
fis ;
< * /DC ? XYC ( 6"7V +K


u/DC ( EC ? EC 6
D2FYLGL4SIFWX 1 ff
"&H
( fin$
(&I/;}ZE"6"7+/ | ff1H
F1&
fffH
1&
y$R"&"
1
"= &K1& " fi R
1A
"&K
( $fii$
(&76 /DC ( EC ? EC 6F &K1& " fiM
1 : "&H
( fi
$
(&I/;}ZE"6B
fit K
"!o# :M,N\ q0M
ff F
* Nt/;}]E"$6 ;
< * /DC XYC ( 6"7 | ff1j
:ff
/[Y @,GE @,11(6
fi / @,11E11 ,132,6
(1& 1 "&K
( fib$
(&"\
( $H"&" i/[Y @,GE @,11E11 ,132>6
)
ffB "&K
( fic7 9)

;
(
9M
_ u
ff>7,7
$J. B NN
3=
)
$
B;< *
$ff=M1 &"o
1A
ff$1& $
( ` ;$ " L7


fi

o$s$!P"$$#@$~%PF!&


o$('*)$

{N
x . FLVJS`Z * KT.X%SPW!S2XK%]2FQS N L4S"Off.WF(FW!S`NFMFW!S"FWNFW!D`NPOFWu >E1
>E1KNPORbTORWU4[
1 M&1=;&BK4^
J;$ "$7@&"Mff1)$
@'/[<; * /DC6!6BS Q CRS{
Q 7o&
Jlml2 8
]'/[;< * /DC_6!6 >$\ qu 3=d
1& /1(;
6
)&=
(Z
G {wC\

*
'/[;< * /G6!6 '/[;< * /G+XYC_6!6F+/[]< * /DC_6!6 '/[;
< * /G+XYC_6!6Fi }
]p=

)> \3'/[<; * /G6!6 '/[<; * /DC6!6&Z
(K1"G 'C7x8 9$fi$#~ "J<; *
(&K
1


B;< * /G6=S Q <; * /DC6FGB
ff1"&"=K $1C\$ 9"&K
(fi")_
("H" $
%

1 $(7)
> \/[<] * /DC_6!6WS Q @CzS{
Q 7) _3=)&1 /1(6;
=@CRS{
Q \oq
'/[<; * /G^XYC_6!6 '/[<; * /GwmfC_6!6\3'/[<; * /DC_6!6_}
/13,6
| {fi$#~$=t/;}ZE"6 ( /'/;}c6c'/p$6!6"7V B$
Ft fi$#fiN F%
AK
"!#~FM

fiB
("1>?
"*(7%d %=c* 9
"&K
(fi"1N
>7,7
ff1T
)tgK
"!o# )M,o\ ff"
\ ,^
(
a1&!*
"u
( *M&1
1fi\
GymfCRS{
Q \$;:


t/[<; * /G XrGymiC6"EK<; * /G^XYC_6!6
( /!/'/[<; * /G:o[XrGwmfC_6!6^'/[<; * /G+XYC_6!6
( /!/'/[;< * /G mGwmiC_6!6\3/[<] * /GwmfC_6!6!6n/'/[<; * /GymfC_6!6\3/[<] * /DC_6!6!6!6
( /'/[;< * /G:omGymfC_6!\6 3/[]< * /DC_6!6!6
( /'/[;< * /G:omG^XYC_6!6!6
< * /G miGAXYC_6 }
;






)> \teH
"!#o=M,7
1 F$
=teB
(K1,?
"*(\mff1F$


t/pt/;}ZE"6"E2A6 ( /'/ ( /'/;}c6%+'/p$6!6!6+'/A6!6
( /'/;}]6+'/p$6F+/A>6!6
( /'/;}]6+'/ ( /'/p$6+'/A6!6!6!6
t/;}]E"tN/p$E2A>6!6_}
):*I Fofi "1& fi "&K
(fi " i1f NA
+>7,7^ =
<; *
$ff
1 &"o T1N
I$&1$
( ` ;$ " L8
7
a"1],;
(
;< * u ff]1 &"o , 1B
9$1& $
( `J
$ " J\ ;$ " h\(tI\
fi
g
i:fi$~# fi $
BK
"1 u_1\q,\J
fiq>\'1& K "* \L
fii
( Jff&B1& 1& H
q"
K
1fi f)
1& >717M)

(&" qT&:hy
fia

(!]
( ;
&"i q 1^$1& *


B

(1& &K?
1:t . "7
Nx+V S`LMSS_%OffNKWfORW qWORNS`ZzbXYO S`LMS`WNPOK%] ZS NPLODNPZzbX%S2D`L4S_KORWWD`NPOFWh
>E1VD N KNJS`Z * / G+XYC_6 h)/\S`Z * /G+XYC_6!6; FLKZZ S`N^CEHG{ EORN C Q
ShD2KYW(NKTSMh%/;}]6
WVK%D`N
1=~}

F?AH
fi?
1@1&
$1&!*
"
;
< * / G+XYC_6 1Z~+]< * /G^XYC_u6 &9CFEHf
G {
Jlml2 )

7

]p=

>E1_

{

`

fi
Nxff.

]p=
OWS2KYD

{

KYLYHkS`WN



ffg' >E1 ? >E1 ORW!D`L4S_KORW^
N KN*O )CFEHGEHG_o{
Gm_G:o SUT KWXCzS{
Q N W,JS Z * /G_GMopXYC6

/G EC6!6 WVK%D`N ShD2KYW(NKTSMI/;}ZE"6 }ff

S`LMSS2YONJKWOW WONS ZabXO S`LMS`WNPOK^] ZSWDNPOFYW


I/\JS`Z * /G+XYC_6"EJS Z *

99?AHfi?
1F&1 $Mfi$#~ " x<; * 78
)
)> \>
($
&1 N
( xP1_$ $
F
N
($&1&"?
19t .1"7)9I1A@& *fi$fi
, = j
>\9T
4 "1q"?
({3=
&1
s;Mfio#~fiwt

&1
(K 7
Jlml2

Q KWXIG ? miG ( S{
Q N W
Nx J MC ? mfC ( S{
U4K[BO JS`Z /G
*
XrG ? mNG ( 6-JS`Z * /DC XYC ? mAC ( 6JKYWX.JS`Z * /G ? XrG ( 6-JS`Z * /DC
G ? XrG ( 6 -JS Z * /DC miC ? XYC ( 6
U_]M[=O 8JS Z /G
*
XrG ? m:G ( 6@JS Z * /DC XYC ? mC ( 6 JS Z * /G ? XrG ( 6q-JS Z * /DC ? XYC (
KWX"JS Z * /DC ? XYC ( 67 N WJS Z * /G mG ? XrG ( 6@JS Z * /DC mfC ? XYC
U4D4[=O 8JS Z /G
*
XrG ? m:G ( 6 -JS Z * /DC XYC ? mC ( 6 JS Z * /G ? XrG ( 6FJS Z * /DC ? XYC (
KWX"JS Z * /DC ? XYC ( 6 7 N WJS Z * /G mG ? XrG ( 6@JS Z * /DC mfC ? XYC

]p=

{



?

6
(

6
(

XYC ( 6
6
6

N WS`Z

*

/G

JS Z

*

/DC YX C ? mC ( 67

JS Z

*

/DC YX C ? mC ( 67

&"1 1&"*
0<; * /G XrG ? G ( 6- <; * /DC XYC ? mwC ( 6+
fid<; * /G ? XrG ( 6< * D/ C ? XYC ( 6"\q &1a/[,6"\>P BP
F@& /G XrG ? mG ( 6q-@&/DC XYC ? mC ( 6
fiF&/G ? XrG ( 6;
@&/DC ? YX C ( 6"7;M
*M&F&/G XrG ? m9G ( 6@@& /DC XYC ? mBC ( 6m&@&/G ? XrG ( 6@F& /DC ? XYC ( 6"\q
MR$
* j&MF& /G mfG ? XrG ( 6W@&/DC mC ? XYC ( 6@&qF& /DC XYC ? mC ( 6 &MF&/DC ? XYC ( 6 >7
3=
$&A<; * /G mbG ? XrG ( 6u <; * /DC mjC ? XYC ( 6+/' i/[,6ff
(
(J6R&

< * /G muG ? XrG ( 6 ;< * /DC miC ? XYC ( 6 >7@ $&=
(1 (~\ NA
$fi7
;
> \M i& N
( 1 $fi
(M= $n
(1 $
i@& /G XrG ? mG ( 6 @&/DC XYC ? C ( 6^
fi
)
@&/G ? XrG ( 6 @&/DC ? XYC ( 6"\@
fi q ^@&/G mnG ? XrG ( 6 F& /DC msC ? XYC ( 6"70d
Aofi K
(



("B
(& : )1j:
( q fi.J

7
Jlml2

]p=

Nx
=
{



S2FLMS`H UMEORN

0t x >E1 ?

L4S,S2YONKWDNPOFW
L4SIS_D`N8NFJS Z

*

>E1JKNPORbTORW^(KZZnN

S,KGHJI!NPOFW,FM

[

a#~I
^$
(&"?
(u;$ "0t >E1 ? d$1fffi$N
(
"1"FF
(x "&H
(fi
$
(&" 7zo&f
"&H
( fiU$
(&\qM
fi$#~tMoq >$f)
{&1 &1fi 1K
"1yM,7

I!LOFLGO\Bt:o=N

$ff^uM
Bofi ~# fiLq A+ ""|
$&1. "&"0/DC ( EC ? EC 6

fi{/G ( EHG ? EHG J
6
Iff &"1& " fi 1j/;}]E"$6N/p;7 (7\} ;
< * /DC XYC ? 6 ;
< * /G XrG ? 6
fi
;< * /DC ? XYC ( 6 ;< * /G ? XrG ( 6!6@K !
=;
< * /DC XYC ( 6BS Q ;
< * /G XrG ( 6"7x %M
1& F$M
(1 (\
qIt /;}ZE"6]M
fi $ff%M
$fi # fiL7 3;
* &\
W>7Y5MK
>P
,
offP
( qJ7
i\$
fiu "&" "+ 1&
("
(=
1& * &\
>7Y5
("K &9
9tMoJ) 1&
(" j

(J
F "
(&"Hq"M
ff>7I ofi fiL\] ]1& I_
"&K u/DC ( EC ? EC 6= &"1& K fi 1
/;}]E"$6FK4
( * E (( E (? N{w
Q C ( o\ q+M
_ff
* _t /;}]E"$6 }''7
$u$fi N
(
)
BtMo)I
# 1(7j Ao u AK
"* K &1 f\F1 y$

"1 "j


fi
()'
(&"+/p$E!}]6_K 4 $
+/;}ZE"6_I f>
7 xs. 1q fit 1
f AffsK
"*
;$
" ^tMo ~o Ao',N$fi ~# t:o o;/p'E!}c6 tMo/;}ZE"6 F/;}]E"$q6 f7tMo o'%M
ofi ~# fi+
1(\

(_

("nff* &"o# fiL\ q/;}]E"$6q
fi/p'E!}c6q
(1& Nff f#\ $H} &0ff :+1\x
fi
Jlml2














ff

fi









fi

o$s$!P"$$#@$~%PF!&


o$('*)$

tMo;/;}]E1(6 tMop/1E!}c6 }Z7-%
(&"tMo oFIAffsK
"*(7sI
(1 &1
(K 7bo&IK 1
/;}]E"$6"E /;}$opE",o6 Ao\L}B-}$o\]
fi$-,o7:@ffb/;}]E"$6B
fi/;}$o;E",o6B
(& f\J;ff 1a
*
/;}]E"$J
6 -Ut /;} E" 6"\cK fft &1
(K 78?N?
(&"\L@ffb/p$E!}]69
fib/p E!} 6=
(&1ff f\
M_ff )
* _tMo op/;}]E"$6 tMo/p'E!}c
6 -tMop/p,oE!}'o6 tMo op/;}$oE",o6"7
(\ %/;}ZE"6)
fin/p,oE!}'o6)
(&1
f\]
+
"&K
( 1&!)
(&"fin! K
* &
(x ""R H
q"M
=a
4
$
F

( q
,"&K ff/DC ( EC ? EC 69

fis/G ( EHG ? EHG 6% &K1& " fi 1/;}]E"$69
fis/p,oE!}'o6)
(1& K !


( * E (( E (?
$ff
uK $1M &IC ( &G ( 7A =

tMo/;}ZE"6 }'
fi
/p E!} 6 } \1
(
( iM
_ )$
Bt = 1&
(" 7) "??
(&W
(&" q=4 =)

=t
9 "&K " 1&
("
(9 N
(d
J "B
(&" q"=)
ffB>7

"&K
( 1&!%
(&Ki
fi 1ys. 1q fibtMo ,o 10
f NffsK
"* (\ ~# 1nfi 1& q"?
((\F
fib >
1&
(" )
;$ " :t $fi ~#
fi _
( c >E1 ? \(d
4 "&" ": &
("
N/[>E1 ? \
fi:K
"!~#
t/;}ZE1(6 t/1E!}]6 }
fi0tN/;}]EK,6 t/[>E!}c6 >7MV I1& > fi
(F
B7:V #$&" as. 1q fi
?
1
$fi ~# $
fi &
( $
(&")/;}ZE"
6 >E1 K !
x
} g_1
P$
(u )1& 1& fi
1& & " 7 }jd$\cM
Rq$fi # ff1
_t/;}ZE"6 tN/p$E!}]6"78 ItMo oM AffsK
"* (\
A$
fi ~# "
(& N= tMo o;/;}ZE"
6 &I}e '7y-%
(&"jt NffsK
"*
fi ~# 1
fi 1& q"?
((7W ^1
WtRq 1&
(" \JH 1 F
W>
} -y}
fif
-{ 7W, W
(q

(1 TctMo o\> %%?NH
fi$?
1@
%t% 1&
(K LffN
} g
fiN}'o g,$o &%ffI}fN
fi
} od
7 a&!=1 (\K 1 q
} gyu
fii g } 7))
q;

*
- $
} -y} -y 798 _tR
1
&
(K
/;}ZE"6(
} g! \(; @
* %t/;}]E"$q6 -t/;}'o;E"q6 -jtN/;}$o;E!}$o6 -t/p,o;E!}$o6 t/;}'o;E"o6"7
K??
(&x
(&" q 3=u
xt{ "&" "ff 1&
(" M$ "u % "@
(&" q"x>7x $
(\

(&"+K
"!o# )M,\ " ff/pA "&K " J6@t $fi \
fi+q_$s"% 1 "&K
( "@

fi
( t:o8
$
7
$1& a>7
)
1 $ =T
1& z
(9>7Y>\>74\>z7 @,\
fii>7,7










,kj,lnm0t






u9

r

p

ffJ v

j"lm

N

mr

1&111fisnd
N
(,DGFHJIKLMKNPORQTSDGFWXONPOFWKZI!LMF%]_K%]`ORZaONPb7d, \]&K
&


("1 >?
" 0
u1&
(P >&q=
(4 " fi " $
(P "0G^XYCu\ $s":
&"$fi &" 5
c
K ! " 7qWM K
(;\]G^XYC G XYC
K
( q1+

(&*>?
" y&:GAXYC cG XYC
fi
ff /G_o[XYCB
c G^XYC_6"7
ffM 11& 1fin 0d
$qiK4
y&"$fi &K fi$ fi0
+1&
([*(
(?$fi ]o "
= &
(1
(A1& & " 7 K
>


1&
([*(
(?$i
fi ;$ " 5 nK ! ",
KL4S2S
EON
c /G^XYC_
6 g/G XYC 6B G^XYC cG XYC 7^ qn K$fi &"

,F
& )
. N


;c{ FK
"1 7@$
& &$ &K \ qH
1 F1& *
)
(1& Td% $B$fi q$ff1 _-9-W1\
:-9-=,\ :-9:
- @,\$
fi :-9-9,7
:-=-W1%!9K

$
cU=
$
()
& &"$fi &












fi

l~x G+XYC|cyGMoXYCBo&WG:o[XYCBoc G^XYC7

:-=-9K
$
cU@"&K
""*(


G YX C \qiG ( XYC ( c G XYC 7
:-=-:@B
R1!$
( fi"i** $ff" &"fi$&)1(7)$M&1*(
qfi$#~
" _
(&H1fi $&1u/p1/p(\F132(,T
6 &:$fi K
(6"\" :-=-:@,\Z
(:H$1&!*\#$fi
*
( "u+#~1Mfi$N
( M/'$J
11& d& 6"7
fi

vmx G ( XYC

(

yG ? XYC
c

?


fifG ? XYC

?

c







fi

mx

fi



)$M1 G+XYC$
(=
I$K
(:$
(K=

&Kfi$&@1( fi$ fiu, M7

F


(\ :-9-9"q"?
(K
>)
cUB &1
("\$




M1q1J NA
ff>7Y5>7



x

fi

[/
,6NxG XrG ? muG ( c{C XYC ? miC (
fifG ? XrG ( cwC ? XYC ( qfG muG ? XrG ( c{C miC ? XYC ( 7
/pJ6xG XrG ? muG ( c{C ? XYC (
fiiG ? XrG ( cwC XYC ? mfC ( qfG muG ? XrG ( c{C miC ? XYC ( 7
/p6N G XrG ? mG ( {C XYC ? mfC ( \G ? XrG ( cUC ? XYC ( \J
fifG ? XrG ( XrO\$q0G mfG ? XrG (
C mfC ? XYC ( 7
F$qu?
(?@$J3= &1 u
-
(1&Z"\)& d,6 cKNPORS
n

p n tplp /p (\ 132(>\@


S`WN L4SVS_YONJFHkSKL4S2S`ORW^:W!D`NPOFWk L4SVS_YONKW!D`NPOFWt
FM;NPnFkQKLGOK%]`ZSJD





N



#"%$$

!

"%$$



"%$$'&




N KN

N/GymiG XYC_6 t/P/G XrGwmfC6"E_/G+XYC_6!6 /
tN/;}]E"$6 t/p'E!}c6
tN/;}]E"$
6 OORW!D`L4S_KORW^ORW^}FYL9"70/ XrOe6








(
&
)

|c

tN/;}]E"t/p'E2A>6!6
/pt/;}ZE"6"E2A>6
tNP/ /OzXYC_6"E"$6
tNP/ / XYC_6"E"$6 / XYC_6
KZF"KYNPOffS*"%$$,+

$T^1& *
=?
1
)

& &)' &" 1 %
(1& _-%?
1I/1(6"\$d
4 ! 1%M,\'
fii-)?
1
p/ 4>6"\ !+K

)tg9
("1>?
"*(7%B%
>7 B\s&1q@I
("1>?
"*Jo "
K
"1 MR&B<] * 7FB)@33:\$ =

=()&1 fio)$ff=>1Jfi
&7
< 1& $fi u \Z $&"
;
1 4
uK " IKKff1&
(&"fi I$fi
( yJcM7I

o11& .
$ HP$1& * :1 " L\;
< * /G^XYC_6B$fi ~# fin
(
(C Sz
Q 7N
fi H
$
ffN
("K$H
j5c1& ?
" H
"K
(&"j$fi # fi
(@
"AGAXYCK !

CFEHG {dO
fibC Sg
Q 7
("K

a1& ff:

( &K
@K1"a
'/
:\]

)K $1"9 1 fi0$ $fi &%~# 1M 1&" " W
fiu qK
" 6%
fii
IK1 Io
1
1 fiu$ ofi &~# 1W 1&"1 " 9
fi ff% K
(
N K4
cy%$fi ~# fi
fi " $
( "9G^XYCK 4
9

G z
fii|
C Io7F8 Io$F fi+o $fi &@ 1&"1 "

finofi

ff K
( $H `1 \
$$ff_ K
( bfip _1 "7^ )O ~# 1(#\ q
)
+

" Io'
u ()1& 1 "&" " u% m1& q%1 T$ `N1 BC *
K !
B
( H
q"= IoL K
( nC * 7@d
91& "&K " 9
(&" 1 "& j
1j. 1q


'
(&K
"* + fi "
()& $
( bff 1q ofi
fi 1 q$&K
( u1& $
($ 7n )@&ff

1& '
( H
;$ " J\ qA F & K
(
( 1 q 1 )1 N$
(1& 9F& /G^XYC_6
fiNF& /GMo[XYCBou6 * q
3 Yp;)Z[("
: r
> ) fi p@Z"FpH[p_[HpH)ZpF
DH([(BL[:!Y L[pDq
.-

/

0/

/

fi/

2/

fi/

*/

!3

4

65

75 4




/

1/

fio$s$!P"$$#@$~%PF!&





o$('*)$

)Cz
fibCBo
(&1Ifi;M1" 7i &K*(
1IAo
"J\x1322Y@@K1fi
M_
)1P
& 1: "&K
( :-9-9W1

=$fi ff@ fi " H* q"=C
%
(&1) *
(q]1
$1& ffCg)
*(
( qT1 cUCg
fiC c 6"7=8 F * qa>*(
( q)1
/;d

$1& .
N u$_1& * W1 " 0 "1 `\ W

$
) $11& .
N

uq 1fi+= %!
(7)
)%d

)9ofi q M1& c :7@F 3U 3D
1
>fi o11& .
$ :1 j
9 BK
"!o# B $()
H
&"
(Z1& "&" " 7
fi

;l

tNpl~p




+Nx L4SS_%OffN,KWFLMX%S LOW^5cKNPORbTORW8"%$$n

N KYN FLS QTS Lb;WDNPOFYWKL4S2S`ORWEON

QKYLGOK%] ZSD

c





"%$$



"%$$'&



KWX8"%$$,+

ftdF4kNPnF



N L4S,O.W!FKGF`DOKYNPORQTSJWDNPOFW

/GwmG:o6XYC6 t/PN/G:oXrGymfC_6"E_N/GuXYC_6!6

N KN*


fi;< * ^
(ff ^$1&1.
N+iA$&1*>"J7a#~(c

Jlml2
1
^<; *
(&N= cM7{)> \9GAXYC c G:o[XYCBo9<; * /G+XYC_6g <] * /GMoXYCBo6"7-%
(&"
cK
"!#~ :-9-q1u
fi
:-9-9,7jWI%
(AHq"fiw
(&"&\%" fO I#~1(\Jc*
( "
K
"!~# :-=:
- @,7x NA
M>7Y5_4 =P
;cK
"!#~F$
(&1"B/[
,6
fii/p69 :-9-=,7 4

c

( _K
"!~# F$
(1& =/pJ6 :-=-9,\M
Bff $1& * )
F L;
< * /G XrG ? mG ( 6g<; * /DC ? XYC ( 6x
fi
< * /G ? XrG (
;
6 gg;< * /DC XYC ? mC ( 6"\ qn;
< * /G m0G ? XrG (
6 ge;
< * /DC mnC ? XYC ( 6"7Ad
I1& ]

(?H
ofi q"
(o 1J
P
q>7Y5>>;
9K?
.!
$)1& P J@&/G ? XrG ( 6
fi
@&/G XrG ? mqG ( 6c
$1& `7Z
* )%$fi K
(9 1a%1&
(ofi &7x NA
=>7q4 =u $
$1& )
_
(K1
,?
"* a;$ " NtK
"1 _Ma
&F;
< * 7xBo
@)
(F1fiN R$=1& '%
(] ;
(


:;< * K
"!o#
fi $ $>
( "
=/[2,6"7<%sa
( " _ff 1afi &M

"

(1& := cM7)
,\ .
( "R
)H
H
%1& 3=u
{@

;$ "
(1& :=
cM
\ qa& # B
("1 >?
"* ;;$ " :tK
"1
/GffmFG XYC_6 tP/ /G XrGImC_6"_E N/G^XYC6!6"7
*

9

,

2

:



% ? fi$J 91 " i+&" ^" "4 3\M$1&1.
_
u:H>fi#ofi+1

IK
"!#~I$(H&"
()&11"&""J7 fi$#~O
(fifi yuH& q * 7
fio#~;f
fio$
:/ * 6 o;/ * 6 13^ 5 +
(fifi"L\&fi$#~;i
fioo \ / \

fi (? \ N
(@
1Nofi 1&
(1 F&9;
B,f13 5 \MMT * 7)> \
- \$
.-



5 ~j13^ 5 \
/ 6 o/ 6 :
13 < ~j31
/ / 6 / / 6


5

\

~ 13^ 5 '\
fi
/ - 6 o/ - 6 13 3 j
/

(?

6


o/

(?

6 1 413

( 3

~j13


5

7


(\'&fi$#~_Oo1 * E ( * E (( E (? 7@)Mfi$# " <] * 1&K@E\o\'
fi
Oo1& N
( K
H
(7ZVy 1F&1fi$#~" \F$&1s$@&1* 1")&1
"1
q"?
($ !
fiL7 $
(1& " ?
(&\
( " @ ^/[2,#6 3 fiff
%
(fi$fi * 1F* &!1 7
Io " ]c
('K ", cO K
( * 7 | ff" )
Io 1 fi+$ $fi &x 1&"1 "

finofi

ff_ K
( ` 7A)
N?
(K )
("1 >?
"*> `s # NA
u>7+
i$
fi$ "&K
1fi^A fi "
^1 "% ffo7W%
1 q (\
W )
$11& .
d1
(
$1& * qd
q+1& "&K " H
1 fi "
(N "
$
=K
"1 |
91& "&K " J7
;/

;/

/

C

fi

<#>=



ffPffff

HffKAN
(&"IK
K @*
(&" _&1K ":
$
( &

IP$I$1&.
NH]



- .J&
(&"
(nK
1fibfi$ff$fi# 1fi$A
( 7 &1 *& \,*q
@
f#~1_fi$A
( \$$1&1.

fi$_fi" ""8s"IH

1& 9
("K$ "
(& )1& >1& H
fi & "@ &"& "7x I$
(& " ?
(&\ )?
(? H @1&
$

=t=
(K1 ,?
"* $fi )
ff) _7
W 0$1&1.
Ni*q &i ff+
$1&1.
N|1bW$ ;|
& uo\ =
(KK$" )fi$ ffB1 a"& jq |1
(&K
1J
dF;$ "fe


(K1 ,?
"* (\c
()
$M?
(?=B7
d9*
(&"?
"]]-M .;& K
1fi+, H&KN
0/132,6"\ &!*>"(\ K&KA
J\
fi

ff"N/1325,6"\L
fiW ?$
(/132,6)
(]K 1j_$1&.
N(7
d?
(?
H$"jt ff
(",?
"*( &1 N &K&17
H$
(:


( _&K I/p(\@132(>\x-@
(1&q"\J)&1 4>6)&_$ fi"
(
N$
(&K
"* ff1& '
( * *> +
;$ " fR
(W iW$ ;F

1& 7=)

"
1?
(? fi 1B
("
,?
"* (\'
fiA
(
( J\ $fi ;
$ffF1 1 3e/[
( +
o11& .
$ :$fi )

ffB
($ 1j$
1& u6"7
aB
&"1 (]\ + 1& " n " w3 ffd

IM
fi K
( 1&13*&^-M .H
&1 7x
(&"q
("K$"uW4NHN\J
(=fi$"&1 &q
("K$" 9$
1s/p1
$ffff1I4>6"7BWB; R
* R&!* fiL\JB4 &K
$_$fi
( ;
< # 1^: #~ 1(\L
(Wfiod

("K$ "
j+&K
B;< 9
(@
>E1p7bV u

(%
> . 1q fi
ofi N
( i1n

~# 1Hy $fi fiL\$ $K
(Hy$fi
(
("K$
MM

* N
~# 1I "
$fi q ofi qa
(&M \Z

fi
MM

K
(0
(sFs" F
@ y1K1 _
(qM

(F

&"
(
* q"W $fi N
( J7M/p)

q1 s. 1q fi `>u
("H " i)
;
(&"i K
fi$
(&"fiL
@
& .
N (\ 99N
($fi q,+8
*(
( /13Y2 @4>6 +> 1:
fi 1& q9 1s. 76 +K !u
. 1q$fi fi
fi$A
( J\ 1 1&
(1
($ 1
(1 f
(KKHH
ff;
< F*(
(&K $ &KN0`;
qb/p 1& K
(
;
(1
q,fiJ6)
fi1^/p & K
( "&Ks6"7% ZM

(
("K$H:W4i/ &W1 H
_ 6"\M
_
q
&1 3* &AM
- .Jj
& u7 | ff" (;\ 3;
* &;
\
R * = IfiK
( 3=Ny

ff" B


K
(
+~# 1+A
+&K
(fi$
" 7
ff&9 "K +@

1H&!*
9M
:
(1& Jff9 11& 1fif !@M$fi N
( + + ?

" J7

&\T


B
(1& B 11& 1 1fiA NF

$ff" J L]< ~
F
($ &H
1
(
fi
( 7_)
$
> N\ * qn =/DCFEHG6=
fij/DC EHG 6=
(1& I$
(&"F
FK 1"F
Ffi 1& q/p & $
(a
* q
fi; 6)ofi N
( \ @;< [/G+XYC_69
fi]< /G:o[XYCBo69
(1& ff1 ,m\ qfM
_M

fi .
1
fi qff1K
H
$
_1& ?
"* "1& q ff `7= | W1 " \c


( RxW4A qH
1&
&
(1 $
((7N)
1

: \]M


("HH
J&_
(F(
-/@2E 14E 3e-R1N
=
fi 6V7e>#\ 1& I1 H

fi
( 0O
fiK 1"C ( \C ? \C \]
fisC < FO H 4
aff ? K y@W4 $fi7q
$
Md; &&
("K
;To " tI\ I\
fi+hb
(1& q
(1 ff$ &K
(& "%$fi N
( B'/
)\


:1\]M,\
fiq fi
& ffK
H
ff4 $ HFt\ff\]
fihy y* &"0$fi
( 6"m\ q0M

0


(
( i1& * &MM
- .)
1& u7

'JLHYLLHcYp9;p!B KDH;q ;HK[`)"pH,M_?


@?



fi

o$s$!P"$$#@$~%PF!&


o$('*)$

$M$fi H
)

u
* ^H

ff" |x$ & K
( `
B
($ B$ &KNN
(cofi N
( 9
?$ff1H%fi" ""ff
F,
&1q@ff&1'
(&!+/13225,6"7,

, F
(?H
1 ]. ? "* N~# 19ofi N
( 7 3 W;
=K
_! $&" (s\ * &!1& $ ff
K
(& = sK !n~# 1N1 ff1& $
($ " ] s. 1q "
10 ~# 1A1 "ff &KN 1fi
q
W)
$_1& Kd
IM
$fi ~# $fi0
fii;
q
* fi? " A1& > "d
1&
I~# 11 7
N
( 1 q 1 9 W? " N1& > "\J q1


Mff =
("KN j

TK
H

ff" $
& K
( u
( ) +
(L$fi
( 7 & * &o\ $W
uA
(
(&"Hq")
(
(
1
" >

q; W "$fi &%K4A? " 1& > "1 \M
B
^
(%
>%# fiNK1"=C ( \
C ? \C \L
fiC < H
KN q"f&"! /p$sW~# 1
6 s. 1q " y,$&K
(x$fi N
( fK !


=W4 fi$7
V R M1 NM &
(d

H
1 W1&
(
(
(fifi " $
(
("KN " q1& 1&
fi 1
ff@

- .J:1& K \ _$fi :1& 1& J1 "$fi &N
0$fi
( :
(7 1& 3* &\x _$fi
ffI
(

ff" ) @
j
(

n~# 1sA
&K
(fi'
" \@
( u

$ff"
$

B
( =W1 F* q"d
1q "$fi 1& fif '
(&K
(M u >fiL7
8 1 ff; A1&
(
(1& 11& 1 1fis N$
(1& " ?
(&:~# 1N$fi
( J\
fin;
A$fi ff%

1
. 1q fi R&ff "$fi &ff
(M
ff&ff K"Nofi N
( 7iV

("K$ " $fi f;
q fi
1
MM
- .J)
$1& "
?i)
: o11& .
$ :* q& : fiiM&" $_* q1fii,+1& >&"


t=
("1 >?
"*
(o K =/p&H
,&
M!Pj$9 "&K
( $fi "&"$ 6"7 3;
* & \
M:1&
(
(1& 11& 1fii
ff" :$fi
( Js\ _ ff"*(
" &=A
(, I& > & H
q"@

q
* P
& Jt *(
(?]
@ofi ffF
(&" =] $ff@1 :
(&7 1& 3* &\ F]
(,& 1&
(&


%
("KN F

t%
("1 >?
"* MKN ] 1_$1& * d$1& u7x$
& .
(\
- .J%1&
N
( ) )L*
(&" ;o "
( >
" ) * * fftg
fi+h%\

( M1T
" /6



(
(&"_ s8 " j,7fd
1 ;$ " $
(P
"
(1&
("1 q 1$i
fi & 1& K
(
K 7
* &\
(; ^K
z js8 " j,9\ ^1& )1&
(& > &


$i
fi ,
& KYZRZ
K 7@
("KN F

%t
(" ,?
"* M$fi
ff%
($
(P& 1ffKN )1
(&H
1T
;
;$
"
(
" % * * Nh fi &%
( K $ 7$ &$&%
(KK$ " F
(
(@
& "H
(&!7

(u ( K &fi
3= j fi$ " J\
| &u&" fi$A
g &"*
1 Nff$
" J
! uK
>)

)"1 q"?
(i
(] =
(1& :fi$ " \$K N
GN\ CWo G_o\$
fin/DCEHGI6WS Q /DCBoEHG_o6"$\ $q;
< [/DCIXrG6BS Q ;
< [/DCWopXrG_o6"7
C

* qs fi " bKA u\ $ff1
$1& ? fi$u\
& .
(\x
0$ &K 1& '
(
fi "&K$s" J\$
|
fi , B
(
( 1 N=$ fi$N1& "&" "* (7
ff&) ""$A 11& " F1& 1
(&K4 uM


Z4
(&H
( 1&" H
$T;$ "


K
"1 bM
- .I
("H " 7B
.
N ^* q $1& N $ =9\ $N?
("j
)K4i$ "
? fio;
;$ " ] $
)
(& a$ff% H
&"o
1I
^1& $
($ ;$ " L7x@ K &1T
)
;
(
B = ?fi$
;$ " @
$
B
(1& _ 1 H
M1 q1 u" 1 1Nj

;$ " u H
&"o
1A

1& '
( ^fi "&"$" J\
( $ ^ %@
ff%
(@
& .
( " 3z" 1 fiA=ofi ~# fi0/ &
3 1
& " H
9?
("W1&
(u9 u&K
( " 6"7
8


^$fi ^
()
AK
1&
(&"fi$ 0 1M1& $
( ` ? | ff+ff !J7{B $


* |"&" w
fi 1s
(&"&
^M
- .:!"#$
" bW1& '
( j $ff> 1f
(N "1&
(
3'[(K 3!pYpHM[=pHpY(D>K;DD3!>HK[!:!H;Y[p@p;p"Y
3pDxDJ=[J
3$[ppH 3!$;4
G3Z( Yx3%Y[(">"Yc3K(YHKY,$DY),K(c(K;p"Y
[3p!:"ppD99!_9Wp;H3="Y[Dp(KpY%[B[[3pYKpH3Y_[D; ZY (;Y!39HffK(_
;4 G 4 G34 G](!@p!Dp:3DYM;([%[;:K(M;"c:[("Zp"3!9(;`(Z
[Y Y%HH]3DK]p!;HY9K(WDYDpY
C

BA



>A



D?

E?

GF



63

fi



1& * "s * fic\F
fi +
("KN" ff$ fi$&" N*
(&"?
"H9Hfi?
(&"#$
"L\

ffJ"&"> |1K
_&$
($ fin
($
fi$$fiL7N)&
(&1NN
yff&

" "#$
" d&="B 1(7


IH !Jp

K
6

rem~



fi|1
j
$NW$ ;\Wu1&+-@$1 N
J\% &"&!$(\ '
(J\ | &A&"fi$A
J\

*fi K&KA
J\@&K &!*"(\-@ &K1o& $\'0x
(&" \$
fi
)&1&1
&+1=
N q" f'
(& 7fiy
(11
, fi$
nu
(&"a&+"
1^
fi
qW$ , &W " sM$
(?A
( (M$
( &7W)
B;&"
! q,$
(4 J:; &"|
)
(F?
(&"
(&K&" fi sFd
))
(
])!< B?A
($fi q
(&"4 +M
- q1&7x< @K &
&K
1; _
(Ks 3= fi fiL7x)

&"W)
(Z
(1 BK & 1fiM M'
(& ],T$ | 8 F\$ $fi &L&K
"c `
2 @3`>132u
fii `25D @(2>1\]


fi :W&Bo &" RqN J@8 q"#$ 1
(&"! /pW8-B6"\o $fi &
&K
x2(45(`25"1`>7{ & ?
(&!* &"" wd $
( &I
(
(&"I :L\F`D KNPOFWKZ
EFW `S L4S W!DGShFW;LNPO qD`OKZ WNS`ZRZaO^S WDSUPJ
[
IYI T^T T^
7

ML

$



P Q )

8m~

Pu_

ho

wJ
w#S

mrr



IR



Q

uT


(

(
&1 N
( ]J@&1# NN
)>7Y5=1Bfi$
(BFF
(1,
@& /G XrG ?
G ( 6 @& /DC XYC ? m)C ( 6L
fi:@&3/G ? XrG ( 6 @&/DC ? XYC ( 6"\
fi$q @F&/G mFG ? XrG ( 6 F& /DC m%C ? XYC ( 6"7
< 1& F$1& , fi$ )= J@& `\ ff1B x1 H
;
q$&K
( ;
( "x
(]@&71
C xK
(R
fi 1MJNKYWXYKLMX cC
qK ( E ? E \ < E 5 E / \ 0 E 3 E - \ &
E
E 71&
(m ,& N)K
(fi 1ffVL4S ZS QKYWN% m1& a. ")1 H
W K
fi'
(&"fifCd
fi
( * (( (?
1 _
(&""&K
(&"GK4 $
F& /G^XYC_6"7 | ff"
T* qi FCaS
Q
K
fi$
(&"fic\ qJ\
K
(> CWm
1AF$_ K
fi'
(&"fiiK1dCgd
4
(
1&
1 qM
\ qXD@& /G^XYC_6~
@&/G+XYCWo6Xx }Y,7s/pd

A1&
(1

"I
(1& ^" fis,
( 1&KH 4b
(
13 < \J13 3 \
fif13 ( 3 76_)
> s\ &)
AK $1"9Gg
fifC O\>;
F
* d$
%F& /G^XYC_6) 1 T1

I1& *
d,$&M/;d
$1& +" 1
I!= 7Y>6"7
-)
(Wy

"&" /DCFEHGEHGo
6 qK1"H
_O YFF`X M;
< * /G_omG+XYC6 ;
< * /G_opXrGemjC_6q
< * /G+XYC_6"7-)
(&" )ff/DC ( EC ? EC 6q
fiy/G ( EHG ? EHG 6:
(1& ^ >fiLu\ q N AN|
;

$fi7
ffF ,fic\ qC 9 ( * E (( E (? W
,
fi /Gnm ( * E (( E (? (6BS Q
| ff" d$
F x/DCFEHGEHG_o6M
/Gjm
E E (6"\
! uH

]
=Gjm ( * E (( E (? Mff 1F K
( ( *
fi
( * (( ((

\
$

)
$

ff

=


ff


J

$
\





fi


>


=
ff


9aJ ( * \ (( \ ( * E (?
\ & (( E (? 7
((
> \,; WN
N
(@M
)

(KKHM$
%

( ;$)x/DC ( EC ? EC u6 &B/G ( EHG ? EHG 6M
ff >fiL7

=
(1 (\$?
(?
)F 3= Nff )$fiL
VU

WU

G ? rX G ( 6 `/G XrG ? mG ( 6 <] * /DC XYC ? mfC ( 6 <; * D/ C mfC ? XYC ( 6
<; * /G
C ( C ? m0C (
fifG mG ? miG ( G ? mG (
C mfC ? f
/DC ( 6 /G ( 6%
fi/DC ( mfC ? 6 /G ( miG ? 6
YX

[Z

|#$&"1B
(1(\LM$
*I
(&1
(fi 01q
d$
$fi7W1 fi
(1(\J;j
*
< * /G mG ? XrG ( 6 ]< * /G ? XrG ( 6"\x;< * /DC mC ? XYC ( 6 ;
;
< * /DC ? XYC ( 6"\x
fis<; * /G XrG ? mG ( 6
< * /DC XYC ? mC ( 6 1\1 RM
@
;

("u1 q 1 fiL7
(\' &"fiu
(1(\ff"


I" uF&/DC ? mbC XYC ( 6 F& /G ? msG XrG ( 6"\@M
uff
(y$
*
/DC ( msC ? mbC 6


fi

o$s$!P"$$#@$~%PF!&

o$('*)$


/G ( m_G ? m_G 6"7 &1 *& \FP
(!R1_@
F
(o1
("%Pfi+&1$?
(fi
,o7@W
(J\$M
?N fi?
1 B7
+1& * ff?
(?u\ &Wofi #~ 1q" \L
(KKHJ
I/DC ( EC ? EC 6%Fffq,fi/[
0fi$q"
(

(&" qM
&",) F/G ( EHG ? EHG 6
ff) ,fiL6"71& q!
(&K
( 1&"
" 0
( * Fm"&"$



(1& jffq >fiL\L F 3=F

/DC ( mC ? 6 +13 ( 3 :
5
fi /DC ( 6 132As13 ( 3 \T
1&
"
,EK>E135>E1^ A/p$fi q fi$ J
C ? ( * E (( E (? (6"\
fiAff E qy(B+13 3 7F-%
(&"s\

&1 *
T,& 1 1IF&/DC ? XYC ( 6@ ,132>7%8 M@&/G ? XrG ( 6 @& /DC ? XYC ( 6@+
("KN " J\
@&/G ? XrG ( 6Wff
(1 f 1 ,132>70)
> \;
^
*
./G ( mG ? 6 i13 q
fi
/G
6( 132%I13 Ho\T
1& , >E"4EK>E13 7 H
( \> P
(!
1:1 @$
ff, &@J
&)13>\
" 1&
(1& R^1& *
J>&K)
9&K ,1320/ & ,EK>E135>E1^ (F
6

(1& ff 1
1F&/G^XYC6B =
C { ( E ? E E < E 5 E / 7I n
(fifi " L\] 13>\ $q KoE HoFe(^b13 3 \
>o\ q E (13 < 7F<F^1 K
fi$
(&"fii
(&K H
"_A
$ ?
" L\>M

* J

13 ( 3 / ~j132 6Jy13 /132 %~ 36Jw/ ~ 36 }
>o\ qu =d

(! 11 F
=;
_ff 1@$
*
/11(6
~j132 >\c132 ~

fi ~ >\
13>~\ q+M
:ff )
*
132/ %~ "o6J '/ Ho ~ 36
fi Ho ~ Ko >7
/16
q

(1
$
(>" 7B&" qK 1 a$
>7B)
$quM
ff
* Ko Ho >\
| g H
" Ho=S Q >\ qy1& /11(6=M
H
* j
Kffo Ko ,132>\x
fi :J

(!
11 R
1&
fi ff . +1 " (
fi ? K4
/ ( 6 "oJ
$
\ / ? 6 Ho\B
fi Ko Ho ,132>\B=
, \' 3=a

q@&/DC ? XYC ( 6 @&/G ? XrG ( 6 ,132>7 1& * & \c;
ffff
E -(N13 < 7Wd


* qG ( |
$7 ; 3=]

0 E 3 E - W
fi+G ? mG ( & 0 d& 3 E - \,$fi q fi$ F
@&/G XrG ? mG ( 6Fff 1%F >E1 ,E1 7B8 :@&/DC XYC ? mfC ( 6 @&/G XrG ? miG ( 6"\M
_ff


*
@F&/DC XYC ? mC ( q6 >E1 ,E1 798 WC ? mC ( K
( ;
.
( "H
) ( *
fi (( \

(!J
1W1 P$
Z@&3/DC XYC ? m=C ( 6L
$ff)1 ,7 F& /DC XYC ? m=C ( 6 @&/G XrG ? m)G ( 6 >ff\ q
C mC ? mIC ( G mffG ? mffG ( S{T \
fiA;
W]
* W;
< * /DC mC ? XYC ( 6 ;
< * /G mffG ? XrG ( 6 >\
+?
(?V
1
= 7 Fi ffH
&
fiL\ 9@&3/DC XYC ? mC ( 6 @&/G XrG ? mG ( 6 1P\ q
C m0C ? mfC ( C ? mfC (
fifG muG ? mG ( G ? miG ( \'
|
fi M?
(?
(
( 3=7
_ff )$
*
"o7 W?NH
fi$?
1
| K 1 13>7% Ho~\ qu,/16"\M
=d

J/DC ( 6 /G ( 6%

fi /DC ( mfC ? 6 /G ( mG ? 6"\'1 j:?
(? fi 7@)
> \
:

K $ Q Ho7i8 $ $
Q /[
n$fi q"
()
(&"Hq:M
&">_ Q ,6"7u)
q 1&
.
"F H
%}S Q 1BK !j
} Ho;7@8 Ho ~ Ko >\, ; 3=]
} "o7F8 $" Ks"
} & =
fi} & q /16"\$M

:/19~}]6 >/19~}]6 ,132>\ 1& RT
4 + ) B


"o Ho ,132>7 1& 3* &\J; ff
(1
& H
& ,132>7) BT

(! 1
$K
4

ff q & &135>7 ,132>\ q0;
Iff
*
fi 7
WFM
F
* qJs\ )KN ; 1ff1& 3* aW?
(?u7)
> \>;
q
u
("HH)
>7F<%s
H

F
$
IC ( ( * E (( E (? \
fi
C ( mnC ?
$& ( * & (( E (? 7A
=d

)$J u "" "
&B@&/DC XYC ? m0C ( 6%
(1& _>\Z1 (>\L (>\ &:17% =)

(! 1N1


)@&/G XrG ? mG ( 6x
$ff):1 (
&= (>\
)W
(1 %d
$1& = )M
&)
&B1M
(1&
("
K
(
q
(& `\$
(B
( * (7
9 1
)
:1& _?
(?
|
fi #:
>
7
U

\

O]

U

*\ ^]

GU

U

_

]

U




bU

c\

Y\

U

Ga

6a

`_

Va

\

^]

^]

dUD]

_

I\

I\

Ue]

f\g]

ff\

]

7a

Ue]

I\

I\

Ue]

\g]

h\

]

6a

f\

ff\

8U

d]

]

\g]

h\ i]

%a

,]

j\

*l

\

9l

k]

>\

]

U

ml

\

ml

]

n\

^]

k]

U

U

oU

#a

]

p]

]

W\

%\

q]

k]

W\

j]

0]

r\

]

]

q]

@\g]

fi\ ]

G]

,\

s\

U

#U

\

t]

%\

,\ k]

U

]

\

U

W\ k]

U

\

\


\



]

]

]

fi

w
u









W$ ;\ 7/13255,6"7 S2D`NPL4SFW WD`NPOFWKZ `KNPOFW:KWX OLnI%I!ZzOD2KYNPOFWH7$W
(fi$ @&"\
| P&K$7
W$ ;\J 7\
(1& >\ 7L/132@6"7 W S_KLMSF4 W`FLH.KNPOFWKWX OL KLMKYDNS LO KNPOFYWH7
W
($fi N:@1& K\ | P&"'7
W ?$
(\ 7B/132,6"7g KAN
(&! :
&HN
"*&!b:&$
("0(7
:L\FDGS2S2XOW^FMN FYLN
FL2 FIFYW
WDGS`LGNKYORWNPb5OWBLGNPO qD`OKZ WNS ZZaO^S WDS
>

$
(
\ f| \L7Z x$1 47W1 f 78
(4 1&\xM7]# *> \x@7 _

(;\x
fis7] AH
&\
fi$ 1&"\ WDGS`LGNKYORWNPbORW$;LNPO qDOKYZ WNS ZZzO%S`WDGS
$
( I1322 x(5>7 | 1& > ?
fiL\ |

P&K$\c1322>7
- N
J\x7)/132,6"7sa
@
&!s 1 's1&o $fi &" K
fi 7 EFYHINKYNPOFW!KZ WNS`ZRZaOR
^S WDS\
/1(6"\ @( x>55>7
- .\ 7x/132(45,6"7ffF1& $
( `\ 1& >$q \]
fi1&
(1 $
(R. K
" Jk

7 ;HkS LODGKW ^FLWKZnFM
\ /1(6"\Z1 x$13>7
bOD
\7\ F&K
($fi (\ 7/1322,6"7d

(%* 9 fi "
fi "ff
($
" 1
"K


fi *$fi q &" 7 WNS LWKNPOFWKYZ FLWKZFMh8IYI!LMF`YOH.KNS:S_KTFWOW^\
/1(6"\c( x,45>7
$
(?A
( (\%7@-W7)/132>1(6"b
7 Jj
f& &"1& qK W
?
(""
(
;$ "
(@

" j& K 7
FLWKZF4
KN H.KNPOD2KZ bTD FZF2b
\ %Z/6"\132 x$132>7
(\$M7@7]/132(,6"7 S_FLOPSVFM :L\F^]_K%]`ORZaORNPb7%B
(ofi M@& "\ | P&"'7
&" fi'N
J\ | 7\

( & J\ 7 ff7ff/132Y2 @6"7 @?
" e
(K &1d
{ 1&N
>
(;7
:L\FD :ZS`QS`WN
EFYW S`LMS`WDGSFW
WDS LNKOWNPb=OW;LGNPO qDOKZ WNS ZZzO%S`WDGS>U
[\9L7
1 @`x$13(47
&" fi'N
J\ | 7\
( 4& J\@ 7 7)/13225,6"7n@?
" H

(K&1
fis$fi ;
ff1&
(1 70
:L\FDGS2S2XOW^
KNPOFYWKZ EFYW S`LMS`WDGS$FYWLGNPO qD`OKZ WNS`ZRZaO%S W!DGS>UP
[\
ORLNS2S`WN

$L7L1(2` x$13(47
&" fi'N
J\ | 7\
( & J\J 7 7/13226"7 >$fi ^ x 0fi $
:! 1 N7'$
(1& =~ $>
fi'
" 7 ;LNPO qD`OKZ WNS`ZRZaO%S W!DGS\ ]/6"\c @` x>>135>7
n
. N
" &K
;
&K
&: fi'
17ff # NH
& \L7c7\
K &KA
J\Z7@/132,6"7^T


(;\7 | 7P/ @fi$76"\ W!DGS LNKOWNPbOWV;LNPO D`OKZ WNS ZZzO%S`WDGS:\L7,11 x,7 | 1& > ?
fiL\
q1 1&"fi$
u7
&H
H

&"j
&% N$
(&"
( 1&
&!*> "(\ =7>7\ K &HN
J\ 7\
ff"(\'-W7Z7J/1325,6"7xD
$

"* &KA
(K
&W?
"1&
(1 7= :L\F`DS2S2XORW N KNPOFWKYZ EFYW S`LMS`WDGS
FYWLGNPO qD`OKZ WNS ZZzO%S`WDGSkUP
[3\'$L7$,13
x,1 47

$Y\ 97M7/132(,6"7D
V 1& %$fi B; K
R
fi ffN
. ?ff\q"1& , ?7 * (\ 77\ yZ&K$ \
7)P/ @fi 76"\ KYOHVH 8WNPL\FIb FLH.KZzOH\%L7F1 @`x$113>7 R@1& " \F-)
&"fi (\

("7
qv

xw


nz
|{

q}

~

$



z

L





2

#

(

z

$

(



L

(

7{

,

(



L

@L

#{

L

w

nz

Wz

$



'{

k

9P Q &

2z

N

L

:{

$

MP Q )

z

Q &



1{

6

|{

L

N

,$

P )

I{

w



fi

o$s$!P"$$#@$~%PF!&


o$('*)$


s$\97'M7c/13225,6"7 8LMF%]_K%]`ORZaONPb S2FLb F2ODkFM !D`OPS WDS7@a $fiL
*
(?
(ff

"L $

7rB 1";7fi$J7
x
(&"\ 7<=7/1322(4>6"7 WDGS`LGNKYORW>:S2KFYW!S L EFHJIKWOFW 7-)
&"fia *&"Kn@&"\
-9
$&"fi (\7 I7
! q,$
(4 J\ 7/132(42,6"7 S_FLbVF4 8LMF%]_K^] OZzONPb7a *&""`jJ-)
(& ?
qF&1"\<;&"7
:
"&K
"?
"
fin1& *" ; a&KN
fi " J\Z' fin
( K L2D OW

ZaOD YS ONZS L4S\ 132Y
@,7
8
*(
( (\7'7c/13Y2 @4>6"7 FWXYKNPOFW.FM NKNPONPOD7F o+Vy s8 \ | P&"'7
&"$ \ 7J/13252,6"
7 8KNPOFWKZ hSD`LOI!NPOFYW hS_DOffOFYW KW!X hSOYWH7u &"
N AF1& "\ |
P&K$7

L

`

v

#

`

ff

P $



0L

*







c{



j

z



nz

fi
ff fi


"!$#%'&#(((*)+#,-(."#(/

0*12345(67(/891
!4;:6((

<>=?=A@CB3D5E$FHG-=JILKMBNFPOQBRB$ILST=A@?U VW=ROXILEAI?Y[Z\=]FF^=J_`UbaW@
Sdc?B'=]D5B$_

efD5='gAB3D5h

ikjlnmpoNq5r s5t

uvxwyz{}|~+vn+'x+~ w++~ u

A$^ * 7x
P-}Ax*x33*
lxrko3q5rst

vnwyz{-|~+vn++Rx+~ +w+'w+ yz+ u

Pn R-}'R>x *
n}nx *x$*A

' ^
Hx" 3MkPJ" W x x AkP*P-P} x}
k } x}*J" C ] }PM k-x; Ax+
}
;$ x"x3 -}Hx" 3-55xx- P*
;$} x3 ^ n}P 5- ^" ^'x} n -
;'A}-'-nP-?";$x3? $PJ" -J"; -x
3*x3 3P3 x ?5 JJ" 3$x" 3J *^x
3] -5 n}x }5 x--?x3 R 3
x $ };3J $ R J -?}?]b 3J-
k?} - J *J'x 5?P}} P-CkJ
"A5 3 x RJC x'3;'J -'}P]}
} } - ? RJ x? x-} -}53
}}}x^ R x;' J x R -"P}P- P
^xx;3J R R +z+P } {*{ P-"?-JJ '-Q''
} Px}+^ 3xP n}

ff
'fi


!"#$!&%('$)!&%+*&,-%.*&/*0'$1%23(45367*&2894)%:5#%+4;<=%23(4
%95%>@? #A94B5%+C#DE!0F23(2#<)2*&2*"!4/B7G2#$/<*"5H!I#J3(23KL5M,N!&94
53(7*&2O%QP 3(R94S<3(2OPTCK943(2U536V/!W#;>TXY%%2#<!W*&*"DZ[P\3Y]^3(%6_ 3(23-943(2
53(/!I#;O(,-`a!I#)5<3!&;O%ffP\3ff2*&2*"!b3(c!I#)%dSegf:hi(jfk l+m:no&m2po&qb*"!WrKsOf2jto^to&qusOqvlnw\qvfl
xvy XNZ{zg/*W#Z |2}~GZ^|}-25N.3623(%!&/*&D73(2<rKV,c#+#{93#%(P\3RO;<*!W#<
%97;*&%T94 2<#]^#*&*&D7G53(/2#!WO!W*"DK,N!&94O:4!"O%T3b,Q!":4.%%95!&#%-
3(!W#;)94S53(P>NNfw\w\f<sSi(phLm9n<o"mpo&qff*&!Ira.pht2hfaqvw\quf<l x >;[>WZa4a!I3cR#!W#;23Z|}}
;M94=:423,ND17DE53(!I#;*&;!&=#%2#%{P36:4!W#5{%+#!"*S<#17/!&%
!W##%!&%2#D{!&%23(!&/>
42#C536!I#;3(%:*"%aP/V36!"%536V/<23(% x >;>WZ*"!0FSH9#23Zg|2}}<b!&!&%N7/!&%
94 53(/23(%b7%O#!0F23(2#<5<3!&;O% P\2#724/T!&-!0F2362#*&D>TQ423(3653(7*&2O%
,423(7<_5C94362536V/<23(%N523P\3#%!&237*&D),-*&*Z[75<_ ,#.536V/<23(%N53(*&DZ
#=/!&/23(%:>N4.a!I#3(2%#P\3c94!&%S!&%94S7<_553(/23(%cP2#%9<F23QP3694
*WrMPS;*0_ 36!"2#<9!"#MPS94!W3%23(4gZN753(a]P 3(:4!I3C%936#;B3(##D#<93(*
O4#!&%9O%2>$?#L#<93a%2Zff5<_ ,#L53(/23(%K53(]OP 3(:4!I3K;<*0_ 3(!&2#<9!&#L7K%9<F23
P 3(!W#%9<.!&2#K3(##DL#<93(*\>=N4!&%O2#<9!&*&%.*&#;=53(P*&2#;94%P3O<#D=53(7*&2O%
#((-(fi+fi4 5 4ff$- ^ *ff23 1
! fi

x!*Pv4

fi



wyz{

wyz{

x >;[>WZzgO*\>WZ |2}}a>NN423(:P\3(ZgC5!&O944%OO!I#<K94P2%P3(%23(4)!&%94
!W#;3!&#MaP7G94L553(4%>5G!0]2*&*WDZc5G23!&#L7G(,2#943(253(/236% x >;>Z
-#36DN*\>WZg|2}}4##gZ|2}}<2#!W#;23Z|2}<}ff#a!I#CQ%!W#;Z|}}T#!W#Z
|2}}<~ *0P^Mg4%2Z|2}}<2g4%Z|2}<}7gZ|2}}a27%#c5<_ ,#<#S7_553(!W#!W5*&%
55G23(%N7GS53(O!&%!W#;)72%S7<DK4#;!W#;!W#<P\3!"#K24)<553(42<#)53(]-P 3(
94{9423>E?\K!&%*&%L5<%%!W7*&O!0PDL2a*"2*&!3C53(/23(%,4!&4,3r=36!I#;L=#
53a!";%S%ff!W#:3(%95G% P94N<9423 53!&;!W#!&2> N4!&%2Z4V,-/23Z3(!W3(%-
*&TaP!W5*&2O2#<9!"#*:F3( `O!0PDO94536V/<23(%2Z,c423(2%T3 553(4O%#-3(!I36
4#;%Pc94536V/<23(%7#*&DB4#;%OP94!W3O!W#52> K2#L42#+2C5*"DL37!&93<3(D
%9a:_ Pv_ 94:_3(N53(/23(%>
?#<P3!&#94c!&%,*&*0_ %9!&P\3Q!W53(/!W#;)945G23P3#OPT5<_ V,c#+53(/23(%c3(
*&2%OE7DB7G_5B536V/<23(%2>N4%K*"2C%3()L{94)!W#5P{5<_
,#L53(/23O#L2#L4*W5%943(2#M:4+53(aP*&2#;94L7<DM!ICO!W*&DL%*&/!I#;M%97;*&%2>
3*&*&DZc9425*&D53(P`53(3(%+2<#1%!";#!]2<#*&D536]P 3(:4536PS*&2#;:4
3(!&#{7:!W#>NN4`%PT*&2%Z4,/<23Z*&%K!W536%a!"!"#*36##D+!W#<
942*&2*I%>Q4!"%S2#%N94#{#7G#=%OPY7G_5);2#23+*&2C%,Q!":4
%!W#;K4#!&%cP3-4%!W#;K#*&D=(t2o&t2nlwff*&2% x !>>*&2a%N,4!&4{*&2{.3(!"#
Pg94N%2364O:F362#23NS536P!&%#<T%2#%!W7*&>-? #O94!&%-3(!&*&Z!I#C#<93%<9423
553(4%ff,4!&4K;<2#23N*&2C%-D#!"2a*"*&DK3(!W#;O94536P3# x c%93a4#OH!&ra*Z
|2}}<%:34#.zg/*W#Z|2}<}Z<,,N#<-25*&D)7_5)53(/23YP\3-;2#23!W#;
*&2%)!W#$L536253(%%!W#;A54a%>QP\23K94;2#23!"#1aPL5*P)o&t2sOsOnLm9nl[jqvjnw\t
3(*&/V<#N*&2C%3(c%*&P3(:4!"%c5*#)94QP3`*WO7GS3(:P )!&%;O2#<)7D
94%S7G_5K;2#23)P\3O*W%2>
Q4%#C!W#+%95GN94Q,#%!&23N!&%c5<_ ,#7G`_5K!I#<;3!&#)7<D.93#%(_
P\233(!W#;B!W#<P\3a!&#P365<_ ,#L53(/23==7_5L53(/23 x >;>WZY^4%2Z|}}>
3S<553(4!&%S{93#%6P23c5<_ V,c#+;<2#23Lp <f2no mo"n<pt'E,4!&4{%%2#!W*&*&DL3(253(:_
%2#<93#%(P\3C!&#KP#3(!&;!W#*b;*g*W%O!W#<.%:7;*&%('E.K7_5+536V/<23N#
;O2#<!&%S!W#5.7<D94%*W%%>+N4!&%S!W#<93(%;*0_ 36!"2#<M5G#2#!W#
7<_553(/23,4!&4=2<#2#7*&)!&`)%*&/)53(aPQ5367*&2O%O#%!&237*&DP%23>.Q,-_
/23Z[%!&%c94S2a%S,N!&94*&2%2Zg#)#7#93#%(P\23-P %97;**W%%!&%#N%2#%!W7*&>
N4%2Z[,;<2#23O;!W#)%:7;**W%%!W#53(253(%%!W#;+54%`#{!I#<;3S#*"D)%O
PT94%S*W%%!W#<)94S!W#5%cPK7_5{53(/23>NN4!&%S#%%!&9a%4#!"%P\3
t2o&t9mw\qulS(to"t2nl[wp <f2no[m2o&np<tZ!\>>4#!&%-P\3 %*&!W#;.%-P%97;a**W%%-,4!&4
2#K23(2a%S94%23(4K:F3(-PYC7_5)53(/23ff!W#)3623-O]^#53(P6>
?#)3623NKO!W#O94!&%r<!W#=P 5<_ ,#7G`_5)!W#;3!&#,-`3(%:3(!&36%*&/%
M947G_5L%95G235<%!&!&#E2*&2*W%#E94{5<_ ,###!&#E9<7*"2<12a*"2*W%2>
N4%O2*&2*&!N3(/23(D+!W5G3(9<#%!W#:4D3(O947%!"%cP\3<#D+4!&;4<_5G23P3#K94:_
3(253(/23(%2>C3!W#%9#Z :4.7_5=536V/<23(%. x !&2#74`*\>WZ-|2}}<~#
<#*0P x <O2Z|}}ff94Q,236`O%N%9%%(P *b!I#3(2#<53(/!W#;K5!"!"#%25*&VD
%95G235%!"!"#B#3(23(=53<O*W!"#gZ36%95!&/*&D>KN4##!&#97*&22*&2*W%
x 3ff!&%36%93(!&!&#{O*g*&!WO!W#!&#c!&%*&%C94S7%!&%QP3ff/236D%9%%(P *b5<_ ,#K53(/23(%2Z
>;>Z xy <%23O.*\>WZ|}}O3 XTNXffc x c%934<#Lzg/*W#Z|2}<}|2>E? #3
5!W#!&#{94#25%c!W#:3({P\3ff%95235%!&!&##{94##!&#{9<7*"2<)2*&2*W%2#
3:423T2a%!&*"D793#%(P\233()O9423Q7_5)#)5<_ V,c#K2*&2*"!\>NQ2#Z944!&SP
94%(,-O2*&2*&! %93(*&D)!&% 6%!0]>
92

fi

J J++uM'++'

+x+~+



'z+++ ;+{

Q4S3(!&*&S!&%N36;#!&%ffP\*&*&V,Q%2> %:3(-,N!&94)73(!&:PT/23(/!&,EP%:5235G%!&!&##
**&!I!I#a!&# x !&#<> 3(/23Z,-!&%2%%-%:3(2#;94%T#C,2<r#%%% P^94Q2*&2*"!

!W#O3(9!&*<#+!W#:3(C3N553(4P3NO7!W#!W#;)94O%93(2#;<94%NP7G94)2a*"2*&!\>S?#
!&#Kc,-3(%%ff:F% P94N!W#<;3!"#SaP X%:7;**W%%-!W#<S94Q%23(4%9-P
C%95235G%!&!&#<_7%53(/23>g3(94233(Z[,O%236!I7G(,-./3(!W#<%aPK3(*&/V#D<_7%
]*&23(!W#;P%97;a**W%%2> !&#N2*&% ,N!&9494%TP7_5;2#23*&2C%2>
!&%2%%c!W#+:!&*94S7!&*&!&D{P 94S53(*W%%!W#)3(23-.4*W5{23(2%S53(aP*&2#;94%
P\336:P!W#;+K;!&/2#{%aPT*I<%%`%,-*&* %)3(3(23c94O%2<3(4%95O!W#=#553(53(!W
##23>S-%#):4!"%!&%2%%!&#gZb,!W#936%/23* 3(*&/V#D=O2a%93(%2>S?#!&#=Z
#K523(!WO2#<9*%9D#+,N!&94K944!&;4<_523P3#S:43(2R53(/23(%M#
.3(/<2*&%S94.5G2#<!W*TaP34#!&%2> .4/O4<%2#=:4%%D%2O%!W#=3623
%94,:4S3#252#2%!&*&D7G.!W#<;3B!W#<+!&%!W#;=%D%2O%O#M!"%O/2#M7*&)
!W53(/#)94O523P\3C#SPT/<23(D5G,23P*536V/<23(%2>- !W#*&*&DZg!I#!&#~K#)/236/!&,$P
3(*W553(a4%-P\3ff5<_ ,#<7_5K!W#;3!&#K#*I%:4S3(!&*&>

3 g$.95.
Tff fi Y=G



)nA=Jb

? #94CP<*"*&,N!W#;Z ,-!W#<93()(D5!&2*-3(253(%2#<9!&/%OPN5<_ V,c#=#L7_52*&2*"!
#!&%2%%N94!W3 %:3(2#;94%-#.,-2r#%%%ff!W#9a!"*\> QP\23 94a2Z,-N%9r4O947%!&%ffP3
O94*&;DK!W#)3623-O7!W#S:4%2*&2*&!>
v q ff
sfi Hl?ljCjxsff;q+lHtjj>>"!#j j Hj
N4;2#23*5367*&2!I#]^3(%(_ 3(23-:43(253(/!W#;!&%%:4V,J94S!W##%!&%2#DaPYC%"$
P*W%%2>Y*W%!&%%TaP*&!&23a*"%> %-*W3(2D!&%2%%Z943(253(/23(%T%9*&*&D)!&*"!&
!&9423Q5<_ ,#K3-7<_5K2a*"2*&!bP\3-5*&!&%94!W#;+:4!"%Q9%9r[>
ffD5!&2*&*"DZff7G_52*&2*W%#<9!W#%O%/23a* !I#<P\23(2#3*&%`,c4!"42#B7G<55*"!&
)%P*W%%O94aS#%!":94K%23(4%:>)c2#23*&*&DZ 94!W#<P\23(2#+3*&%O2#=7G
!&/!&!I#<(,-)*Ia%%%2d&%('hnlqvflM!I#<P\23(2#3*&%5G23O!&94K;2#23a!&#=aPQ#,R*W%%
#m9f<lw\(nm2w\qvfl)!W#<P\23(2#`3*"%Q*"*I<%%N3-3(25*W942R7<D94236%2> N4SO%-55*W3
7<_5K2a*"2*W%!&%N94(tfo&pw\quf<l2*&2*W% x 7!W#%#gZ|2}~<>N4236Z945#%!&#+3*"%
3(3(%*W!&#E#Pa3(!W#;>$N43(%*W!&#2*&2*W%+2<#172#E,N!&94L#93!&#
3*&%2Z>;>[94*&!&#PT9<*"<;!&%2>`?PN*&!&(D!"%!W#</*&/=!W#=)5367*&2 !&S!&%%2#%!W7*&)
25*&D94N%:5235G%!&!&#K2*&2*W% x -4!W3T<#!I#;<23Z|2}<}Z,4!&42#%N3(%<*I!"#
,N!&94O%:5!0]S3*&%T%9!&9<7*"QP\3T4#*&!W#;Oa!&#%2>TN4Q5#%!"#3*&%-P^94Q%95235G%!&!&#
2*&2*W%<3(.%:5235G%!&!&#gZ-*&!"(DL3(%*W!&#gZ-#Ma*"!&(DP3(!W#;>c;!W#gZa!"!"#*
#<93!"#3*"%O%94=%9<*"<;D+*&!&#gZ %:7%95!&#gZb#2#%!W#;Z #M3(,3(!&!W#;2#
725*&D>R?\)!&%)L7=2C54%!&$94CP\33K%9D1,-{25*&VDE94{/23(%!&#PO94
%95G235%!"!"#$2*&2*W%!W#9367<D-4!W3)#H#!W#;23 x |}}>H5!0]2*&*WDH94!&%
2#<9!&*&%94aTP 3(!W#;!&%#*&D+55*&!&5%!"!"/<S*&!&23*&%2>
E7_59436253(/23 %9*&*&D.!W#<9!W#%T%*),++Pg%_ 2*&*&Ohfw\t2l[w\quno[3hn9qv2t
m2o&np<tP3(U,4!&4{!"%*"%`#=3(2O/%c#*I<%,-@C!WO>SN4!&%*W%!&%S5!W#<
94%#)/.BPNnmw\qunw\t9j)mo&npt>-!&/*W%%3(Zg#*"!WrK52#<!W**W%%2Z*&*&V,-
53(#,*I<%%/!WK9455*&!&2!&#PT%O!W#<P23(2#K3*&%2>N4O!I#<P\233(=#,*W%%
3(5-!W#0),+-> ? #!&!W*&*&DZ),.132S#4),+516$T> N4!W#23O!W#!&%!&O%*&!&#K3cnmw\qunw\quf<l
9

fi



wyz{

wyz{

w\t(h!&%3(2*&!&7<D4236!"%!&2#%2>T94!&%N2#Zg423(!&%!&07@%%!W%O#93*#`7G23
8:9<; ? ,N!&94M24=- ; ),+TZ-#L944- ; ),+$,N!&94M94)%:*&*&%.,-!&;4< 8:9 !&%%*&>
#+!W5G3(9<#53(5G23((D)P423(!&%!"%!&%94!W3?>nqv:lt:>N4236!"%!&!&%2*&*"P !W3N!0P!&%*&%
5<2#!Ia**W%%!W#+%:4)##23c:4#K*W%.362!W#%S5%%!&/O!W#<]^#!&*&D=*&#;>A@Q%9*&*&D
94P!W3#%%PT94`%4236!"%!&!W5*&!"%O94aN9453(/23-!&%5*&Zg!\>>g!&c2#{23(!&/O94
25(D*W%,c42#K79!W#!W#;.#K!W##%!&%2#N!W#5c% x 53(/!"):4#23(*&D!W#;.2*&2*W%c!"%
5*&2>
Q4a!I#C%9362#;94CP7<_52*&2*"! #)53(/23(%ff!"%Q94!W3T%93(#;O3(##D)#<93(*\>
#94C#.4#Z #<D+!W#<P\23(2#%O,4!&4M3(O:]^#!&*&DL##%%93(D!W#M+53(PZg>;>g!W#<P23_
2#%-!W#/<*"/!W#;O9*&;<!"%Z3(-!">T#O94Q9423T4<#Z#<93!"#O!W#<P23(2#3*&%-*&!Ira
%97%9C5!&#=/!&=943(25G!&!&#P5#%!&#!W#<P\23(2#%O!W#/<*"/!W#;94O%9O x 33(O!W#<_
%9<#!W*W%%>E7!&;!"%:/V#<9;<aP^7_52*&2*&!!&% 94!W3b*WrP;a*_ 3(!&2#9a!&#g>
ff2%23(9!W#)!W#<P\23(2#%S3(cP /3(K/23ff9423(%QS:4c]{%23(4K%93a;D.#{94
423(!&%!&,-!&;4<aP94c*I<%%N53(-P!&2Z!"cO!&;4-794c2%c94YP3/23(D*&#;O!WOc#*&D
*W%%c,4!&4{3(S#c53(-PT#D)53(PT3(2#O23>
**&!WO!W#!&#!&%S(D5!&2* 5<_ ,#2*&2*I%,4!&4,%94*&* !W#:3(O!W#94P\3
P94Sm:fl[lt:m2w\qvfl.w\no&t9n<pm9n<o"mpo&p< x -ABC-O x zgQ*\>WZ[|2}}>? #C3(23YS!W#<93(#-DBC-$,-
,N#<T%93( ,Q!":4947%!& x P 3(/3(!W7*&2ff97*&2K2*&2*W% x >;>WZ !&!W#;Zg|2}}<~P3T*W%%2>
97*&2EBJP3?$B!&%S:3(S,4<%`##<_36c#%S36S*W7*&=,Q!":4+*&!&23a*"%S<#+94QP *]*&*&%
94)#!&!&#gd?vP94{!ICO!W+%:%%3#%GF #IHIJJIJKH FL=P=#FP#B3()*W7*&
,N!&94C*&!"23*&%#M #IHIJIJJNH MLZ942#K:4*W%DOPM #QHJIJJKH ML R x w\no&t:np.mo&npt9Y!&%<#!W#%9<#PS*W%
!W#5$T>?#949<7*"2<2*&2*W%(,)!W#<P\23(2#)3*&%<3(%Z #O*&D94+t'hnlGqvfl+<#=94
(t9j<pmw\quflK3*& x >;>WZ !&!W#;Zb|2}}~>#+55*&!&2!&#{P 94S5#%!&#{3*&`O2<#%N%*&!W#;+
*W%OP 3(S$1#Ba94!W#;)94*&!&23*&%OPK/3(!W#<SP-!&S+{pfnoT,4!&4!&%`*"!&23*
-94*&2P PT#+fht2lK73#4 x O73#4K94-%#-#<9!W#K(,OC5*"22#9<3(D*&!&23*&%9>
b7*&2)3(!&#*"<%%O73#4)7DK#!P\D!I#;)%97;a*T,N!&94K94c5*&2O2#<NPT*"!&23*
U x 2#.7<D,V U b#O94-%9<O73#4gZ<#.55*&D!W#;O94-%97%!":!&#94N,4*&N:7*&2g>
##!&#{97*&2{2*&2*&!-,3rK#)##=97*&2g>97*&2!&%2*&*&m:fl[lt:m2w\t:j
3=m:fll[t9mw\quflMw\no"t:npK!0P24=!W##23#4F x ##<_ *&2P#2S,c4!"4!"%O*W7G*",N!&94*"!&23*
MN4%O)*&2P#4FXW O#;{!&%S!WO!W)%9%%3#%O94a!"%O*W7G*",N!&94=*"!&23*CMYW
5*&2O2#<936DZM(>bN4T!W#<P\23(2#3*&%T36Nw\n9w\Zt['Vw\tlqufl[Z2#)(t9j<pmw\qufl[>N4ff%9363*&-!"%
*&,ND%ff94N]^36%-!I#<P\23(2#%25P S236!"/!&#g>T?N523O!&%cS:7*&25#%!&#K:4T2#C#*&D
755*&!&+O936!"/!W*g97*&2gZ!\>>#c#%!&%!W#;OP#*&D.##> N94aT94c%936T3*&
2#73(%93(!&K%_ 2*&*&Lw\n9w-(t2o&t2nl[w-*I<%%,N!&942%!I#;)!W#C5*"2#%%2>O93(
3(*&/V<#DBP+*W%.!&%:]^#1%P\*&*&V,Q%2>?PC$!&%#M#%9!&%(]^<7*"{%SaP*W%%2Z ,-K2*&*
\ ; $+%93(-3(*&/V#<-!P 9423(c!&%N%9!"%6]^7*&%97%?$ Wfi] $+%:4K94a*$ W_^ OP\"R!&%N#%9!&%(]^<7*">
!W#94c%TP #;!&/c*W%%-#<9!W#%NT*&2%-#c%9<3(T3(*&/V#<-*W%Z,-*&%O#%!"23
S36%93(!&K2*&2*W%N,4!&4C#*&D25*&D%#;!&/c*I<%%ffP3Y94N%:3(T5#%!&# x -DBC-a`Ibdc2>
N43(!&#E3*&!"%):4+%9%K!W#L94#</2#<!&#*N9<7*"2<12a*"2*W%2>XY2#%!"#1!&%)
O7!W#!&#PN5<#%!&#L#L3(!&#g>?\O!&%5G23P\3OL7D%*&!W#;B{%97;<*(TK!W#M94
97*&2EBSZg55*&D!I#;#)5#%!&#{%25+eT<Z^#!WO!W*"D=5G23P3O!W#;+.36!&#{%25
,N!&94=T)#M#.aP!&%#,N*&D=2362M%9%%36%2> 94aS!W#M94.362+aPQQ3#*I<%%!&
!&%%:<.!&2#cO25*&D)%93(N#)2#%!&#gZ[!\>>94S36!&#)!W#<P\23(2#O!"%##%%9<3(D x >;>Z
##!"Rz<#;25Zff|2}}>NN4%2Zg,-a%%9O94c,%O/236%!&#%aPf-ABg-3C-DBC-a`Ibdc
94-)#-25*&D+3(!&#)%25%N!W#)94O3(2PTN3#K*W%%2>
9Ih

fi

J J++uM'++'

+x+~+



'z+++ ;+{

-DBC-13-DBC-a`Ibdc-#<T42/<T%95G!]Q!I#<P\23(2#3*"%bP\3b4#*&!W#;`*&!&(D>?#%2Z,c42#
2*&!W#;=,N!&94*&!&(DZ 94.!&!"2a!&#=O%`7G.2#L7D94K3(ji!&/!&DZY%DCO93(DZ
93<#%!&!&/!&(DZ#%97%!&9!"#!"O% P94-*&!"(D%DO7G*\>?#Z94N%-P^#O!"!&
P\3aPa*"!&(DK!"%-!W#)#O%2#%c5!W*\>T--!&N!&%-/23(D!0.2*&NO/*&5)94%ffP\3T%!I#;
7!&*&(_ !W#a*"!&(D)!W#)9<7*"2<K2*&2*"! :4-D!&*&#/!W#!W#;+3(%9*&%N!W#+53!&>
?P%97;*fiTc7% x P23 %N!W#<P23(2#%:42a.P*"<% x %97<_(:7*&2O,-N2*&*g94
79a!I#{%97%!&9!"#+Kfo&pw\qvflCP:T>
Q4#!&#Pc97*&223(!&/V!&#K#.%23(493(N!&%ff!IC53(:#d N%:2DDBlk4B W !0P x #
#*&D!0PY97*&20BCW2<#7c23(!&/CP3(mB7<D55*&D!W#;%93(T3*&c!0PnB1!&%T:4936!"/!W*:7*&2gZ
3ff7D#C2#%!&#3(!&#K3*&S%97;*[!W#,BS> N4c##!&#C97*&2C2*&2*W%N!&%N#
x 53(Pb#oi^2#2> ?#3623 %94,94#%9!"%6]^7!&*"!&(DKPc*I<%%*$TZ%23(493(Z;<!"/<2#
%-P\*&*&,N%2Z4%c.7GSO!W#+!W#p>nqvO,D x 24)#OP 9493(O`%7/!&%!&=P\23-
]^#!&O#<-PT!WO2N#<!&* O*&%97*&223(%>TRt9n<m[qw\(t:t#r:]^#7<D+O2*&2*W%
#.% Pg*I<%%?$!&%-;!&/2#7<Dc93(N,4<%3( !&%T*W7G*"),Q!":4O94N936!"/!W*:7*&2g> X /23(D
!W##23#.!W#sr *I<7*&1,N!&9497*&2tBU4%`a%`!W!Ia.%:%%3(%O94)#%SP 3( 94
!W*%aOPF #QHJIJJKH FLRZ,4236uFv!&%N*W7G*&=,N!&94wBv<#xBlkwBvZ|aylz*yl{b>
!W#S#ff#*&D94#O723ffP53(P7(%7N*&%O94!W3ff%!&c!I#2362%%-3(!W#;94536P
53(%%2Z-5*&!"!&)9<7*"2<L2#O23!&#15363(%K94C#%93K*&*9<7*"2<L!I#6r!I#
73(2:4<_\]^3(% ##23b3(#<3(2a%#7*&> N2#Z!W5*&!&!&T2#O23!"#`5363(% :455*&D
m:flt9mpw\qv2tod|fpljt:jSqvw\t2(nw\qv2tcjt:t(htlqvlct:n(mqSk qvwYqCnm[}Vw\(nm}qul x~ 3PZ|}b3(-#3*&*&D
%>T?#:4!"%N55364O!&23!&/*&D*W36;23b]^#!&c!W#!"!Ia*53(%ffP^:4%2<3(493(#r36N5*&36
,N!&942594<_\]^3(%%23(4g>)]^#!&K%;O2#<S!&%#3a*"*&D=:]^#17<D=)%a_ 2*&*"$m:fsQh^o&t2w\tlt9
fpl[j x ,c4!"4)5<%%-%9393*^3(%93(!&!&#%-#C94:7*&2,4!&4)3(*&*&,-.!W#K94c233(2#<
%;2#2Z[%7*&,c#=O]=#93a*#O723Zg%_ 2*&*&1(tfp(m:t>c?\23!&/O252#!W#;
!&%.523P3O17D%93(!I#;,N!&94L7%!"+36%3(K/V*W5{ ; ? #L!&23!&/*&DB!W#23(2a%!W#;{
#<!&*+536P-!&%P\#B,Q!":4!I#94O]^#!&C!I#!&!W*-%;O2#Pr :]^#L7D#.7G#L#{b>
36O!W#2#O5*&%P3S5*&2#%%.7G#%3(K94j<t(h^wYqfpljZ-qvl>t2(t2l[m9t)fpl[jZ #
k t2q&qw\t9ji(jt(hgwYqfplj<>
Q4N2594)7#.*&!WO!&%N94!W*[2594aP!W##23#% x ##<_ *"2VP#%:T!W#K97*&2
,423(:4233(2#<3(%36A{!&%N94S!W*25:4)523O!& x 94S36N#`4%-2594{>
? #53!&Zb9425947#M!&%S!".%9%%(P* x zgCS*\>WZT|2}<}c336!"%#gZT|2}<}~c7S!&
%9<F23(%P36$:4*W36;T!W#23(2%NP94ff%;O2#< x :]^#7DO36%3(C{g2<%7DO#!W#23(2%
Pn{b> N4N!W#<P2362#S7#)*&*&V,Q%c*&/*7DO*&/*[5*&3a!&#OP94N%23(4O93( x >;[>WZ!&ra*Z
|2}<>?#)C53(!&%#K,N!&94K:42594+7G#Z94!W#<P\23(2#7#r%NO%9:4!W#23(2%
PT94O%2<3(4{%95a5G%%!W7*&Z 7!&!"%!W#<P23(!&3c)94O25947#=!W#=53!&>S? #3(23c
O7!W#94/#:;%cPT94O2594=#94O!W#<P\23(2#.7#Z94O,-!&;4_ 2594=7#
,N% !W#:3()7<D %23bT*\> x |2}<}>N4!&%N7#%23(!W7%cc*Ia%%TP5G%%!W7*&7G#%-94
3(%:3(!&N9497*&22594+<#)94S#O7G23NP !W#<P2362#%*&*&,-+!W#<P\23%95G!0]`:7*&2g>
c*0_ 3(!&2#<9!&#caP-ABC-KZV%3(D5!&2*5<_ ,#2*&2*W%2Z!&%b;!&/2#O7D94ff###%%
#!&!&#g>-N4!&%-#!&!&#K2#:!&*"%Q94T/<23(D*&!&23*g!W#)97*&2K723(%N3(*W!&#K94%93(
*W%>SQ4`%P5%%!W7*"C%93(c*W%%2##3*&*&D7K3(%93(!&K!"K%:*&*%P
*W%%O,4!&4B36%9<.!&2#<!W#=3623{;3#<O5*&2#%% x >;>Z <%23O*\>WZ-|2}}Z
>;>:4% P#;!&/-*W%%-%*W3(2aD`O2#<!&#>TN4%2Z<#*&D`2369!W#O%2##<%-42/!W#;
##!&#S%93( x ;*Wff*W%O3(c2#O23> C!W#)53(7*&2RPT53(P%-,N!&94w-DBC-

9I

fi



wyz{

wyz{

TD

CTD

TD

BU

CBU

BU

C

!&;3(|dff-523!"#)7(,-2#)O5<_ ,#)#+C7_5)53(/23
!&%-94T!W#K;2#23*94D363a9423 *&#;> ?#CPZX-ABg-!&%NO#;94N,-2r%T2*&2*&!,42#C94
*&2#;94%NaP!&%!W#;.53(P\%3(c#%!&23(>-N423(:P\3(ZzN*\> x |2}}aT53(5%)2#%!&#%
P(-DBC-,4!&43(`7a%+#{#936*&*"!W#<;3!"#)PT94Km2pw-9po&t>cN4%O2#%!&#%2#
*&%+7G%2#%S3(%:3(!&*"2C)O4#!&%9O%2>P 3(942353(7*"2 !&%94cP2#+36!I#;{94
%2364M9<7*"2<B,N!&9494)%9.3%97%9O%97;*&%23K3(252*"D>N423(<3(.%O
2#%!&#%cP:42*&2*I%O53(5G%)/236O%94)53(7*&2O%2>-XN>;>WZ7D)zg*\> x |2}}
K3(%93(!&{%97%95!"#+#25S#7<D)%934#+<#!&r* x |2}}24!W#;)4#!&%
4/S72#!W#936>
WR

rs5j jfisDfiljs?l ;l5rttjfi

ff3q

q 5j

3K553(4P!W#<;3a!W#;=5<_ ,#L#17G_5L53(/23(%.7<DM5G23!&#M!&%K4<3:_

23(!&17<D=hgt6h^(f2m:t:qvl#Eqvlh^pwSnpsOtlw\nw\qvfl>=Q2#:P\3(94gZff*&A$$7)94)!W#!&!W*N*W%
%O,4%!W##%!&%2#DL%94*"L7)%:4V,c#g>+? #94)53(253(%%!W#;B54%K94)7_5%9<_
5235%!&!&#E53(/23;2#23%.%`aP*W%%/$:J%94M940$ 1$*T>M#*&;%*&DZ-,-
93cP 3(K23(9!W#=#O723P53(PN25%P-94 X$53(/23K*W%%p$ %94
94$ 1$ >+N42#gZb94K!W#5%a$1aP:4.%95G235<%!&!&#L53(/23!&%;2#M7D5$ Z
#)94!W#5cP 94 X153(/23ff,N!&94/$:ff> QP23T:4N794{53(/23(%-2#)53(+`,-3rO!W#
53a*"*&*\>ff !";3(|c!"%:5*ID%N94!&%r<!W#+P5G23!&#K94a!&%N%%2#!Ia*"*&D)7%)#)%2#<!W*
#22#!&#)aPY7G94+536V/<23(%2>TN4O553(a42#+7GS%2#{%C%95!0]O!W#%9#<!W!&#{P 94
;2#23a*523!"#)553(4)NXffN x 2#!I#;<23^4%2Z|2}}<>
!W#O94%95235G%!&!&#=536V/<23N,-3r<%N#{%23(4)%9a,4!&4{#<9!W#%%PT*W%%2Z
!&S!&%O/23(D2%D);<2#23.%SaP/*&!&B*W%%O!W#M+53(253(%%!W#;=54%>KU/<23(D+%!W5*&
O94S!&%b523P\3$]`#`7G23nz[P!&/!&#%25% #SN;2#23 94T*W%-%%),. v
#5) + v aP!&/#5a%%!&/O*W%%2>SQ42#gZg,O%*&S*&*>nm2wvZ[!>>5%!&!&/#!&S*W%%2Z
P 3() . v >H!W#=94!W#<P23(2#%{PS=%95G235<%!&!&#<_7%53(/23K3(+%#ZN!&)53(%
#*&DL*&;!&+#%2#%)aPC$T>L%C,)%:4*&*5*W!W#1!W#!&#L=!W#O3()9a!"*\Z-!&K!"%K#
%2#%!W7*&K=*&*;2#235<%!&!&/#!"%94O!W#5P 94S5<_ V,c#+53(/23ZG!>>C%
$ : 1OI-d-!&%P H - ; ) . v R>+Q,-/23Z!&O!&%O,N!&%.+%*&#*&D=%O.#!&%O,N!&94M
P #!&#x* Z!\>>$ *x13: x OI-d-!&%P H - ; )/. v RV>
ff2%##!&#:7*&2<_7%.536V/<23(%4/NQ%23(4%9T,4!&4O#<9!W#%T!&#%
x 9<7*"2<!W#%2MP*W%%2Z !&O!"%K]^3(%`%!";4#7/!&%4,+93O/Va*"!&L*W%%
P 3( %94=%23(4L%9a+,4!&4,N!&*"*S7G,*&*0_ %9!&EP\3K=%:5235G%!&!&#<_7%H53(/23>$
9

fi

J J++uM'++'

+x+~+



'z+++ ;+{

#O94!W#3(23c)93c/V*&!&M*I<%%S!&%)25*&D*&2)O4#!&%9O%P X
53(/23(%>Tc%%9O94O*&!&23**TO!&%C*W7*bPT94O3(#`aPYC*&%%9797*&2&B#>Nzg
#QHJIJIJH ML7G94c*"!&23*&%94a3(%)!W#+36!&#K%25%ffP\3ff*"<%!W#;,B #):436%!&
P*B#>-N42#gZ[94*W%wOPVT H VM #QHJIJJKH VMLR`C2D)723(!&/a%.#,J*"2C x %!W#`!&N!&%
*&;!&2*b#%2#PT9497*&2*I<%%!W#xB >N4*&2C*&=7O93#%(P\233(+.
7<_5{53(/23>-c%S552a*"!W#;%:4!"%c!&2K%#%2Zg!&4%%S%/236`3(%:3(!&!&#%2dN4
*&2%%9*&*&D<3(Z>;>O!W#%9<#!W!"#%,4!&4{,236`53(/!&%*&D#K*&%S<9423
73#4%2Zg#%Q;2#23*a%94DK*&=7>NQ2#ZaP2#94DK2##7`%+!W#{!I#<P\23(2#%2Z
#O%95G!Ia*"*&D#b!W#O#<93!W#;N!W#<P\23(2#%T,c4!"4O3(ff/23(D!W53(9#<P\3b7_553(/23(%2>
!W#94%C*I<%%S3(O;2#23a3(!W#;)9453(aP3#{!W#=K39423#%D%2C!&O,D)94D
`#<362*&*"DC!I#<93(SO4;*0_ 3(!&2#<9!&#K#)42##-rc%aP:4/#<9;%
P 94%2364K%42OD5!&2*gP\3 XN>
Q4S#25PT%:7;*b*W%%S523O!&%94O;2#23!"#)PT*W%%23(!&/=7D)!W#<P\23(2#%
!W#/<*&/!W#;)53(P;<*\d
iwof5jj

v tbqrX^qt[fi'tbq$rX^q5t [t

|<>NzgC$L7%PT*W%%2Zg*&aB 7GC97*&2P\3$Y>QRpf2n<omo"n<pt,\ 3(;3(!W#;wB
!"%Q94*W%u\x1OM #HIJJIJKH ML RZ,423(:4*"!&23*&%ZMv3(c94%97;<*&%NP:49<7*"2<wBS>
G>Nzg#R7O7#Z{ ; ? 7O3(%3(Z#w$B7OO%NaP*W%%2>
?vP-ABC-E!&%N%Z:4pf2n<om2o&np<tQtw L ,S>3>>PKZX{bZ#$YZ<!&%T:]^#)7DD L u1
OP\LdBH!"%O:7*&2{!I#94O!I#!&!W* %;O2#<PT94O%23(4):3(P3$#5-DBC-R94c!"%
:]^#7<DeR#5{R"?$T>
?vPfi-DBC-a`Ibdc-!&% !W#%Z94cpfnomo&nptQtw `I bdc L !"%ff94-% `I bc L 1OP\d \ ; L Z
94%9365#%!&#P*B$!&%523P\3+,N!&94)#;!&/S*W%R>
Q94-%97;a**W%%<3(N/Va*"!&{*I<%%2Z!\>>*&;!&2*g#%2#%NPb94!W#!&!W**W%
%2>ffN4cP<*"*&,N!W#;KC5*"!&*"*W%:3%c3N#!"#)P%:7;**W%%2>
! 5 v zg"$x1OXOfiR H OP #KHIJJIJKH L H fiR H OP* #KHIJJIJKH *j H RXR>bN42#gZOP #IHIJJIJNH LR
g
!&%T94Q%97;**W%a\ 7*&#;<!I#;O:4N97*&2O79!W#,42#O2#!W#;O94-;*,N!&94
94*I<%4OP #HIJJIJKH L H fiR>-?vP-,-S25*&Dxl1H!W#<P\23(2#7# x l>c#=3(%36u41Z
942#&f A1fff`Ibd c [ 1OXOP #IHJIJJH LR H OP* #HIJIJJKH *jRXR>

%97;*[*W%A\3(253(%2#<%N93#%(P\3C!&#P#C3(!&;!W#*g;**W% x ,4!&4C!&%N94
J
%9<3(O*I<%)P94)97*&2=B!W#<B#,U%97;<*&%.3(2*&!&$7DM94)!&#,4!&4*&1
94O97*&2EBS>N4O%pf ` [ !&%94O%cP*&*T5G%%!W7*&;a*93#%(P\3!"#%N!W#)%:7;*
*W%%ff,N!&94!W#GO!W#<P\23(2#%-!0P,-N#%!&23Nnovoqvlh^pwbm2o&np<tS7G;<**I<%%2Z94-%( `I `bc
!&%:4%aPY*&*5G%%!W7*&O;*g93#%(P\3!"#%!W#<K%97;*g*W%%c,N!&94!W#5K!W#<P23(2#%!0PT,-
#*&D#%!&23wYqt.l[t<nw\qutm2o&np<tO7G;a**W%%2> 3(a*&DZf ` +!&%94O*&%936
PN*&* x ;*WQ*W%%S,>3>2> x O]B#O723a{PQ2#%!&#=#=3(!&#%25%2Z `I `bdc [ !"%
94*&%936SPT*&*#;!"/< x ;*W-*W%%,S>03>2> K2#%!"#+#3(!&#)%25%2>
?#K3(23-5*&`# XL#%95235G%!&!&#53(/23Z,-;2#23a!W#)94S53(2536%%!I#;
54%,Q!":4)94!W#<P\23(2#`7G##+`]3(%3(Aw$|!":423N94%Cf ` .3T:4%
9I

fi



wyz{

wyz{

`I ` db c [ Z25G2#!W#;=#,49423a-DBC-3p-ABC-`IbcK!&%%>)?\O!&%#%2#%!W7*&+)%a$1
f ` [ +3p$m1<"`I `bdc [ #=;2#94C!I#5OPN94K7_5=53(/237D94!&%%
x %c!&#.<> N4%Z,-%;!W#]*&23YP#!&#wMP\3b%*&!W#;%ON%97;<**W%%2>
N4Q!"%Z $ 13 x f ` "s" ` .3*$s1 x "`I `bdc [ "sf`I `bdc [ >
!W#*&*&DZ ,-O,N#)5*W!W#L4,3S94'$53(253(%%!W#;=#B;O2#:!&#{PN94
!W#5NP9453(/23(%('E!&%T!W#+,-*&*0_ %9!&KP3T/236O!W#;S94c!&%9/#:;%-P9453(/23(%2>
%93(-,N!&94K:45<_ ,#)53(/23>*@Q%9*&*&D)94S*W%%C$:;2#23+7<D+%95235%!0_
!&#{53(/23-3(!"S;2#23a*72%`%:5!W*&!&=*W%%3(*&!WO!W#B7<D.%:7%95!&#3
3(,36!"!I#;[>T?\N!&%42#S!&`53(77*&S94-94DK2#KP\2#)7G`%KP\3-*&%!W#;K52#{73#4%
PN97*&22#O23L7DB5<_ ,#=536V/<23k qvwYqfpwSqvlw\nlw\qvnw\qvl{wYqt)w\no&t:npo'V>?vP%94
Kr!W#=P-*&2)4!W#; x >;[>WZ?\,N#Zb|2}}<N!&%S5G%%!W7*&.,-O3(7*&O.*&%.73#4%
,N!&94!W#<93(!W#;#,%:7;*&%33(!W#;{94.5<%%!W7!&*"!&(D=94a94K3(2!W#!W#;+%97;<*&%
3(K%*&/7*&>B?vPc%97;*&%O,4!&4MP2#=23!I#M:7*&2=2#L7G.%<*"/<Z 94K*"2C%.36.
;O2#%QP3N3(##D#<93(*\> 3(/23Z%!I#O94O%2<3(4K%42OOPTC%95235G%!&!&#
53(/23Q!0F236%P 3( 94cP# XE53(/23Q!"!&%+7O5M94c!&2#{;2#23O*W%%
,N!&94CP\,!W#<P\23(2#%c94N2#K*&%`73<#4K,4!&4)*&{#*&D+7S*&%7<D<#D!W#<P\23(2#%
,42#M%!W#;B#*&2%2>=Q42#B53(P*&2#;:4%<3(.3%!&2*&*&D36>B)!&#M{P3
O369!&*&+%23(!W5!&#{P 94S%P *"2C%2>
Q4!W#5 PN%:5235G%!&!&#K53(/23!&%T;O2#`7<DS%97;**W%%T,4!&4`<3(-94N3(%9*&
PT:3#%(P\3a!&#aP#K3(!&;!W#*b;**I<%O!I#<%:7;*&%2>NQ2#Zg94S;<*0_ 3(!&2#<9!&#KP
%95235%!&!&#{53(/23 !&%N!W#23(2a%+!0P!&%%-%94K93<#%(P3O;**W%%Q!I#K!&%Q!I#<P\23(2#%2>
!W#S94 X53(/23ff25*&VD%O!0F23(2#<N%2364K%42O!&N2#K/<23(D.!&r<*&D)#c23(9!W#
%25%Q94-94S%:5235G%!&!&#+536V/<23-2##36#%93N7G2%SaP!&%4236!"%!&`%23(4g>TN4
%2364CP\3NO53(P 2#)942#)7G`3(>
w
' 5g.g
'95 =G " )^xE= 2

fi



? #M:4!"%C%!&#M,-)<O!W#)94)!W#;3!&#P%97;<*-*W%%!W#<=94K!W#5.%OP%9<_
5235%!&!&#53(/23>NQN]^3(%,-`a%%9OO94*&*b%97;*b*W%%;2#23,N!&94!W#K23(9!W#
#`7G23cP!W#<P\23(2#%`<3(.:4`!W#5%S#,-S;!&/%O`36%9*&%S3(;36!I#;+536P
*&2#;94L#M%2364M3(!&#g>QP2394Z,-K5*W!W#M94)#%%!&(DMPN%*&!W#;B#*&D=%O
%97;*g*I<%%<#+!W#:3((,O%*&!&#{O94%7a%)#K94943(!&!&%2%%!"#g>
v Nq5rjlp?la*fixs 3filxrsxs5lIqfisffqp0^q5t+t

45G23!&#=94!I#<93(M!I#94.53(!W#;%!&#;<!"/<%`3(!&%)94%!&#P
N
,49423O{hgffN>)o"tl<wYq(t:jpm2w\qvfl1!&%.5G%%!W7*&Zff!>> ,494239423(+<3(.%:43(23%95235G%!&!&#
53(P\%`aP94)!W##%!"%2#DLPp$ ^ ` 3a$ ^ `I `bc 94<#MP94)!W##%!&%2#DLP#$T>
x 94N,-O2%936S94*"2#;<94PK53(PfR7D)#<!W#;)94O#O723caPT!I#<P\23(2#%25%
,b!W#!">U!W#%O)!W#<P\23(2#%25%.36+#%%93(DP\3O2#O23!W#;M%97;a**W%%.,-
%94*&{93(DK]^#+N,c49423-94%!W#<P23(2#%2<#)7S%:2/<),42#)%!W#;.:4*I<%%2>
Q4!"%K%!&#L!&%+a!I#*&DP:43(!&2*c!W#236%.72<%!I#;2#23*7<_553(/23(%
1#K2#23=O!W#!W*O53(aP%2> 3(/23ZN7_51536V/<23(%%:*&*"DH523P\3M*&
P##%%93(DL!W#<P\23(2#%2>1?!&%.O3()!W5G3(9<#O=#*&D+,4:4237_5L53(/23
9I

fi

J J++uM'++'

+x+~+



'z+++ ;+{

2#L7G2#:]P 3(94+%)aP%97;a**W%%!W#L94KP\3Ph^(f2fN>Kt:n(mqMt:jpm2w\qvflZY!>>WZ
=3(!"#MP:4+#O723P!W#<P2362#%)94+53(/23O#%!W#L3623O{]^#53(P>?O!"%
53(!"2*W36*"D)!W#<23(%!I#;K!&2#<!0PD):4S2%%QP!W*536P %2364)3(!"#g>
fi n425b[vsn-DBC !W3(%*&DZff,-.O!W#+:4)2%K,423(),-)25*&DB2*&2*W%w-ABC-KZ !\>> ;2#23aG ` >
%%:OP3(:423c94#)*&!&(D!"%!W#</*&/=!W#:45367*&2)Z!\>>g%95G235<%!&!&#=33(%95G#%
x 3(23(-3(%<*I!"#g>

Hl

v

X w$ tn)t2w-fN>S<f<pl[j)m2o&np<t`lf<wNm9f<lw\nqvlqvlKtj2pno&quw|No&t2w"; $*Nnl[j)o"tw(5|t
n.l[nw\p(nolpst2 tw # nlj0tOsOqul[qusOnogo&t2lwYq.(tfo&pw\quf<l(t>pw\nw\qvflCh^(f2fN>:n>f?$
nlj$ ^ ` T(tht:m2w\qv2tod| eqt2lTquwqfo&jIa #
<f +t:nmq |LwYqt2(tBqnMt2wOfN>llflGi<f<pl[jjBm2o&np<t,$ l[fwOm9f<lw\nqvlqvlMtj2pno&quw|
3; $ P pmq)wYqnw-lfKsOqul[qusOnoo&tlwYqKtf<o"pw\qvflt>pw\nw\qvflCh^(f2fN>?>f<?$ ^ ` [ [ qW
_qf<9w\twYqnl=nKsOqul[qusOnoo&tlwYq)(tfo&pw\qvfl{t>pw\nw\qvflKh^(ffN>*>f<"$
(ffN>[

|<> 94N#OP 3(!&2!"#)%25%c3(S#+!W#{942%P;3(#)*W%% x 362*&*94
*I<%%<3(+%%PS*&!&23*&%9>$Q42#gZ-94+*W!W!&%.:3(!&/!W*c%!W#943(%:*"KP94)]^36%
3(%*W!&#%25KP 24)O!W#!IC*536Pb!"%#K*&2O2#<aP: ` [ >
G>Nzge|<>$zg,$ 7:]^#$7D=$ 1OXOP x # HIJJIJNH x QR H O_ x # HIJIJJKH x jRXR>
zg4C127K94K3(23(!W#;=%MP\33(23(L3(%*W!&#g>N42#gZTO!W#!W*N3(%<*I!"#
3(:P:!&#O53(PgP3$ 3(!W3(%"C)|c7!I#<3(DP3(!&2!&#%25% x 3(%9*&!W#;O!W#O94N*W%
O_ x # jRVc#36%*W!&#{%25%2>^3(9423O3(Z[!&2#{2%!&*&D7G.3(;#!"94c9423(
3(T!W#Df ` Iff#*&DS*W%% ,c4!"4O#<9!W#*&2%#-5%!"!"/<#S##;!&/T*&!&23a*>
N4%Z##NaP^94%c*W%%ff2#O*&2KSS3(:P 9!"#53(PP\3$ ^ ` [ Q!W#*&%%ff94#
X0L|!W#<P2362#%2>

Q2#ZNM3(!&#P9453(PS*&2#;94!&%+*&2%)5%%!W7*"!W#94+;3(#12a%>$N4
x 423(!&%!"253(P%2<3(4{P)36%*W!&#<_7%L53(/23cD+#S53(]cP 3( 94.536PT*&2#;:4
3(!&#C79a!I#>Y3YC5*"!&ff!"%N5G%%!W7*&94-*&*^*W%%aPO!W#!W*3(:P 9!&#536P
P$4/-%9*&*&23 423(!&%!&c,!&;4<% 94<#S:4*W%%bP3( ` [ #O,N!&*&*^42#7GN!&/V
7:P\3(:42)>T-#%!"23P\*&*",N!W#;K<5*&d
!g5
v zg"C12N7G-94T3623(!W#;%P33623(`3(%*W!&#g> zg:4T*W%ff%$.7G
;!&/2#7Dp$x1OXOP* H * Hj R H H [R H OPR H OjfiR H O[ RoR>N4-423(!&%!&C7336%95#%bN:4TT?\
423(!&%!&>ff^3(:423O3(ZP\394-]^3(%({)!&/V!"#O%25% x { ; ? H {51}Z3(%*&/2#%TPg94-,-
O%S3(2#*"D=!"/=*W%%`3(53(:P\233(M7D57+>N42#gZP\*&*&V,Q!I#;{*W%%`<3(!&/V
7<D.9453(/23 x !W#K94!&%N3623d?OP* H * H_ R H OP H R H OP* H HK R H OIR H OP HK R H O[fiR H R H R H c>
^369423O36Z!0P:4`%:7;*b*W%%P"f ` [ +3(O!W#%23(=724!W#M94O3(!&;!W#* !"O%94
53(/23-,Q!"*&*b]^#94%9OO3(%*W!&#3(:P 9!&#{53(PP\3$ ^ ` [ x E<#{94`536P
%2364K%#<7G2#:]cP36R5%%!W7*"53(P *&2#;94+36!&#g>

92

fi



wyz{

wyz{

!W#94S7/c5*& x %95G!Ia*"*&D{944<%2#)423(!&%!&2Q!"%Q%O,4a#936!"/<Z!&N2#
7 K5MP\3SC#DM53(7*&2O%O94*I<%%P3(f ` [ ,N!&*&*N7+!"/>? #94!&%O2%
9423(Q!"%N*&%S#;3<#Q94 94536Pg%23(4O!&%-!W53(/.7G2%c94N%97;**W%%-2#
!W53(!&!&#* 3(##D>
-DBC-R!0F236%('A5<3(NP 3( 94O*Wr)PffP3(!&2!&#<'$!W#*&D+!W#944#*&!W#;PTa*"!&(D
P 3(94)%95G235%!"!"#L2*&2*W%2>1?#L94)2%.94a`*&!&(DL!"%C!I#</*&/!W#M94+53(7*&2)Z-
53(P *"2#;<94+3(!"#)/2#CP\3-;36#)*W%%c!"%#N;3#<>

Hl W ft:nmqB(tfp(m9twl |wYqt2(t+q)ntwfN>(fplj=plqvwt_pn<w\quflGA$;

$+kqt2(tKwYqt)sOqul[qusOnobpht2hfqvw\quflM(t>pw\n<w\quflh^(f2f>9C>fg$ ^ ` [ n(t)l[fw-jqf:w\t2wYqn<l
sOqul[qusOnoh^(f2fN>:*>f<?$
(ffN>[ffzgaC12`7G94c3(23(!W#;.%KP\3ff%95G235%!"!"#g>N-#%!&23-94c%-P#!&ca!&#%
$1OoOP1R H OP # x E1
[ # x jRXR> )%%:O.:44C12!&%%1%#M3(23(!W#;P\3
%95G235%!"!"#g>N42#gZO!W#!W*%95G235%!"!"#.3(:P 9!"#`536P[P3$3(!W3(%T(,-c!I#<P\23(2#%2Z
%95235%!&!&#{%25!W#<G # x p1
[ # x T36%9*&!W#;.!W#K94!W#!&#x [ # x p13
[ # x Z

`

#942##*&!&(D=3(%*W!&#{%25g>?#+:4`%af [ +3(O!&9423##<_#!&S*W%%,c4%
3(:P 9!&#C3(!W3(%cT*&2%N!I#<P\23(2#%N3 94#!&%*1OXO v x p1
v x jR H x *1 x jRd
5ymzamw| H Ety,$|R>!W#.94K3(:P:!&#P?$ ^ @*&%3(!I36%`)!W#<P\23(2#%
3(!&#P 94S53(P*&2#;:4)!&%!W5G%%!W7*&>

?#=%:3(D={3(!&#MPN94)423(!&%!&)%2364P3S+536PN2##<7G.;3#<B7G:_
2%K94.536Pc*"2#;<94BC2D=#O7)3(!W7*&ZT94K%97;*T*W%%D=7K!&;#3(L7<D=94
%95G235%!"!"#=53(/23Z3N94%97;a**W%%SC2DK!W53(.O4{!&!&#*T3(##D>
N4,-O*W23c5<!I#<%*&%)4*&+!W#{94S;36#+2%,N!&94c*&!&(D+,4236`N*&2%536P
*&2#;94{3(!&#)!&%c;3<#>
Q4%#5G!W#ff!&%#S3(2*5367*&2%!I#:'$%T3b5G23(!WO2#%%:4V,-'$%9*&*&D`%:7;*
*W%%ff,N!&*"*g7a!&/V#C,N!&*"*g7-!W#/<*&/!W#O94N%23(4`536%% aP53(/23>bN4N3(!&%9r94
%97;**W%%<3(S!&;#36=2#=7G.O!W#!WO!&7<D%*&!W#;+%95G!Ia*"*&DM%94%97;a**W%%
,4!&4,N!&*&*g9rN536T!W#:4N%23(4O,N!&94KS4!&;4K53(7<7!"*&!&(DZ>;>*W%%,Q!":4.c%9C*&*^423(!&%!&
,-!&;4S36;3(!W#;K94423(!&%!&PN%95G235<%!&!&#B536V/<23>? #{3Q%*&!&#B536%%,-O#
#%S%94+P\2%3(!W#;.C945G23(!WO2#<9* 3(%9*&%2>NN4c]^36%<#)94!W3(5G!W#%94,
%`:43(!&2* ,-2r#%%%O7!I###!&#,N!&94=553(53(!W3(*&/V#D<_7%L%*"!"#
4#!&%,-S*"7%23(/S!W#53!"943(%:393(!W#;KPT94S%23(4)2%7D+%!I#;
%97;*g*I<%%a*"*&,N%53(P%Q7P\#KP %23>
fi n425b[vsn-DBC-a`Ibdc

#*&DZ,-NO!W#94N2a%,c423(,-N2C5*"D:42*&2*W%C-DBC-a`IbdcZ!\>>;2#23C `I `bdc [ >
N42#gZff/2#P3;36#B*W%%#O#<9!W#!W#;=*&!&DM!&!&%.5<%%!W7*&+94OO!W#!W*53(P\%
2##c7S%:43(2#),42#25*&D!W#;K%97;**I<%%2>

Hl eqt2(t=qntwOfN>+(fplj1mo&npt0$@kqt2(t=lfsOqul[qusOnoco&t2l<wYq1(tfo&pw\qvfl
(t>pw\n<w\quflCh^(ffN>*>f<?$ ^ `I `bc qjqf:w\t2wYqnln.sOqvlqvs`n<o^o&tlwYq)(tfo&pw\qvfl{t>pw\nw\qvflCh^(f2fN>
>f"$

9I

fi

J J++uM'++'

+x+~+



'z+++ ;+{

(ffN>[ff-#%!"23Q94cP\*&*&V,Q!I#;K%"$=Pb*I<%% x ;!W#K,-25*&DC12d
OM H ,IH M[RZ
OPM :PH *M , RZ
OP*M H RZ


OPM H ,IH *M[RZ
OPM :H *MRZ
O*M H *MIRZ OPM H *M H *M , RZ
OPM # H H *M : RZ OP*M # H H *M : R

XTa4)O!W#!W* 3(:P 9!&#)536PbP\3ff94!&%%c3(!W3(%}3(%*W!&#)%25%2Z>;>Wd

$&1

| H fi OPM H ,PH MjR
ff
H fi OPM H , H *M R
ff
H fi OPM :H *MIR
ff
H fi OPM :H *M , R
ff
H fi OP*MY H *MR
ff ~
H fi OPM H *M H *M , R
ff
H fi OP*MY H MjR
ff
H fi OPM #H H *M : R
ff }
H fi OP*M # H H *M : R
,SZ!&4*&%2d
ff

|2
ff ||
ff |2
ff |2
ff |
ff |2
ff |2~
ff |2
ff |2
ff

H U
H U
H U
H U
H U
H U
H U
H U
H U

x| H
x~ H
x H
x H
x H
x H
x H
x |
x |2

fi
| fi
| fi
||2 fi
| fi
| fi
} fi
H |2~< fi
H |2< fi

OM H , R
OM H *M R
OM :H , R
O*M H MIR
O*M [R
OM : R
OM H *M : R
O*M : R


OP*MY H ,IH *
MRZ OPM #QH *M :H *MRZ
OP*MY H *M H *
M, R

42#2#O23!W#;a*"* 53(P\%NP\3?$ ^ `I `bc #O2#+3(<;#!&`:4N94OO!W#!IC*536P
*&2#;942##7G`3(>

?#E942%,4236=a*"!&(DE!&%+!W#/<*&/$!W#E3)53(P53(7*&2O%2Z,-79a!I#EM943(2
#*&;<%NO94O53(/!&%c#d
`I ` bdc
1

OP*MY H ,H RZ
OP*M #IH *M :H *
MIRZ


Hl fTt:nm[q(tfp(m9t?4$|SwYqt2(tqNnt2wf>plqvwtj2pnw\qvfl$BkqttcwYqtcsOqul[qusOno
pht2hfquw\qvfl(t>pw\n<w\quflCh^(ffN>9*>f$ ^ `I `bc n<tClfwY_qf<9w\twYqnl=sOqvlqvs`n<oh^(f2fN>:*>f?$

(ffN>[ff? #{#*&;DKN4362U>>
S2#{3(;#!&O94QP3C-DBC-a`IbdcS:4`3(%9*&%S<3(S%%2#!W*&*&D+:4`%:O`a%-P3C-DBC-.>N?#
;2#23a*T94)3(!&#PN94)423(!&%!&)%23(4P353(PN2##O7K;3<#L#B536P
*&2#;94%D#<`*&,ND%S7G3(>{? #M53!&Z 4,/<23Z,-*&L;!W#7%23(/:4S
3(%:393(!W#;OP94c%23(4O2%)7<D%97;a**I<%%NP\2#*&*&,N%N53(P%ff`7GNP\#CPa%23>
?#K94cP\*&*",N!W#;,-%!W5*&D+%%9O94-!W#<P\23(2#%,Q!":4K%97;*g*W%%3(S#<O!&
7<D1=%95G235<%!&!&#$53(/23Zff!\>>ff9494DE3()!W#</*&/E!W#1:453(PS%2364g> ,N#<
2*N!W#LO3()9!&*N,N!&94M:4+53(7*&2aP!&2#!0P\D!W#;L%97;<*-*W%%K,4!&4M2#M*&2=
*W3(;3(!"#+aPT94O%23(4{:F3(#=4,,-O2#:.!&2#*&D%*&S%:4{%97;**W%%2>
N4!&%53(7*&24%7G)9r*&M,N!&94M423(!&%!&%%!W#)9423(C!"%6'A%O,-.4/O!W#'A#
9436!&2*;3##*&%.#.94)!&`,c49423-%97;**W%%3(S%:P *\>
W< +rt[ w+ljsA;qgu^q5t+t
X /<2#=,42#M%!W#;=%9C*&*3(%3(%,=94C%%0 ` [ #= `I `bdc 2#=7GO)!&.*W36;>
N4%2Zff!"C!"%K#O%2#%!W7*&!W#<;3a+*&*-%97;*-*W%%CP 3(f ` [ 3A"`I `bc !W#=94

9

fi



wyz{

wyz{

%2364O%9NaP%95235%!&!&#<_7a%53(/23>?#;3!W#;O#<DO*I<%%%9*&*&D%N#
2#<9!&*P/37*&36233#;<2O2#ffP 94%23(4)72%94423(!&%!&;<%-*"<% !W#K94S4;
#`7G23NaP*W%%c,4!&4)2#)7GS23(!&/{P 3(R#<D.%:7;**W%%2>?N!&%42#3(2a%#7*&
M/*&54#!"%)P\3]*&23(!W#;L%97;a**W%%):4<5523K2#9a!"*S*W3(;{;!W#!W#
:.!&2#DP3)%95G235<%!&!&#153(/23!0P94D=2#7+53(/2#g>+Q4S!&%2Zb,-<3(!W#23(%M!W#
]*&23(!W#;L(t2o&t2nlw-%97;<* *W%%2>Oc%`a*I362D+%236!I7GL!W#=!&#=Z,-`;<2#23{t2wNfN>
pfnom2o&np<tcm:nljqvjnw\t#942#,-N%*&ff%O-%97;**W%% P 3(J94!&%T%2> N4N4%2#
%97;*T*W%%36.M+94C%23(4=%9OPN94)7_5=536V/<23>)? #94P\*&*&,N!W#;Z
,-O,N!&*"*-c]^3(%c!W#:3(K%`236!"23(!WCP32%93(!W#;+:436*"/#D=PK*W%>N42#gZ,-
%94*&* !W#<93(`(,-K4#!&%cP3N;2#23!W#;)%97;a**W%O2#!&%`#2*b,N!&94{94
%*&!&#{P36*"/#<%97;<**W%%>
fifi! < n#"%$pWW'&1bgn($p2

-,-!W#4323(!&%!&% P%97;**W%% 2##:3(!W7-c%95G_5OP94-53(aP^%2364g>
!W3(%*&DZ%!W#3(!W#;K.!&#>W|%97;**I<%%!W#<93(!&!&#* 3(##D
!&-!"%Q!IC53(:#-94aT%Pb94*W%%2#K7S536V/<2#!&2%!&*&DZ94ff!&%O3(c2%!&*&DK94#
94K3(!&;!W#*-;a* x %9>)? #3(23{%!WC94!&%2Z !&O!"%#%%:3(D=C6;<.,4:423S:4D=2#
h^(fnod|)tCfo&2t:j),N!&94{944*W5PT*W%%PT94O!W#5S%2> 2%936!I#;)%!I!"*W36!"(D=7(,-2#
{;*N#L<9423*I<%%.,Q!":4M94)4#!"%K/*&5E7DM2#!W#;23#Lg4% x |2}}Z
2#!W#;23Q*\> x |2}<}Z<3-^4% x |}}-D.7GS,-*&*_ %:!"+P\3-:4!"%c%!Wa!&#g>
#*"DZN{%<*I!"#LP#,N*&DL!W#<93(1%97;*-*I<%=jqfpo&j=lfwno&k nX|tlw\nqvo-n
fo&pw\qvfl=fN>Onl=f:q&qvlnogfnok qvwYqqul>tk1w\t(h[`fN>OwYqtpht2hfaqvw\quf<li6nt9jh^(ft2:>N?vP:4!"%c,-23(
94c2%N:42#94Q!I#<;3!&#CP#,1%97;**W%%-,-*&)#53(O!&%O4O;!W#g>TQ3(!&23(!W
!W#3(23)%!WaO94!&%`3(d !I36%*&DZ94C93#%(P\3C!&#)aP#3(!&;!W#*T;**W%.!W#<+
%97;*g*I<%S7DK# XL53(/23ff%94*"+4/72#)5G23P\3O+7<D.%!W#;.<#D!W#<P\23(2#%2Z[!\>>
%94*"L7C!&)4!&;4g>)N42#gZ%*W!&#P)#,%97;*ff*W%.%9a*"*&D=%#2#<9!&*
K%*W!&#P#{36!";<!I#a*T;a*,N!&94!W#{P\,%25%O72%9453(77!&*&!&(D=!&%`3:4234!";494
O7_5K53(/23ff2##('E`!&%c423(!&%!&S%2<3(4<'E!&r*&D)3(#%:3-94!W#<P\23(2#%
#1!W#<P\23:4.36!";<!I#a*N;*\>L#*&DZ-!0P:423()!&%.{%97;*-*W%5\$#L%.P
949<7*"2<K*I<%%aP94:7*&2wB4/SO4!&;4+423(!&%!"O,-!&;4<N,S>3>2>94S4236!"%!&SP 94
%95G235%!"!"#<_7%+53(/23Z4!&;4;!W#OP:.!&2#D2#7G5.!0P9453(/23b2<#.53(/
\ >N4!&%S!&%O):4SP 94a!I#<P\23(2#%#M)!W#<P23943(!&;!W#*ff;* %!W#;\13(
!0.2*&cP3T:4S%95G235%!"!"#<_7%=53(/23>
fifi*)

vg',`[vnMn-E[uL.&Mbgn/$#a2

&+&

? #3(23c);2#23K%P-!I#<23(%!W#;+%97;* *W%%S!&!"%!W5369#<94c,C25*&VD
*W3(;c3(%3(P3b;<2#23!W#;S%:7;**W%%> %ff,-4/N*W3(2D!&%2%%Z%:7;**W%%
94O3(;<2#23=,Q!":4B%9*&*N#O7G23PN!W#<P2362#%=#`53(O!&%O4;!W#M72%
K7_553(/23cD)2%!&*&D3(#%9394!I#<P\23(2#%#=K!W#<P23c:42)>N,-/23Z
!&S!&%`#<S5%%!I7*&K);2#23a*&*T%97;**W%%u" ` )3pf`I `bdc [ P3K%9<.!&2#*"D*I<3(;
3(%3(A.a%%:7;*g*W%2#!&%72%S94!W3N4;#O723QO2#%-94-94%%TP
;2#23a!&#)#)!&!&#*%*&!&#{3(c4!&;4g>ffN2#Z,-O%N3(%93(!&N3(%*&/%-%
P %97;a**W%2#!&%94aN!&%SKp:t2w-aP( ` [ 3" `I `bdc [ Zfi%9<.!&2#*"D{*W3(; x %
!&#{>
_2

fi

J J++uM'++'

+x+~+



'z+++ ;+{

!&;3(Od ? #<P\23(2#:_7%;2#23!&#KaPYO%aP%97;<**W%2#!&%
3]^3(%-/V36!I<#2Z[#qul[>t(t2lm:tinat:jO94Z%9<3(%N7DK;2#23a!W#;%97;a*^*W%%NP 3(
` [ 3 `I `bc P\3Q39423g*W36;3(%3(f#S%5%,42#10 2 %:7;**W%T2#!&%
3(;2#23>SN4/V<#9a;SPT:4!"%S94!&%94c!&!&%S/23(D)2a%D+#2#7:K!"2#<*&D
!W5*&2O2#<>7*&2E3({2#O231,Q!":41]E%93;DP3O%*&!W#;L%97;a*"%KP\3
!W#<P2362#% x %9a*"*&D*&:P6_O%9a2594<_\]^3(%9c#{P\3N2a497*&2!&%c%97;a**W%C!"%%3(>
N4)!W#!"%:/V#<9;<.PN94!&%O94M!"%O94aSK+94C]B%:3;D=#M94*&!WO!&P
94=#`7G23KPS%97;*N*W%%2Zc,-+#*"DE7:!W#L%97;**W%%),4!&4$<3(+!W#<P\233(EP 3(
;a**W%%7D)5#!W#;+53(!&2*W3Q%97;a*"%c,N!&94{*W3(;<`#`7G23NaP!W#<P\23(2#%S#+<9423
%97;*&%S,N!&94#*&DB)%:*&*#O723PN!W#<P23(2#%> x K*&% !";3(.dOc/V*&%.<3(97*&2
!W#E{]^#!&%;O2#<P94+%23(4L93(xrKZff94+*&!W#%+3(253(%2#K94kJ3(*W!&#g>H36DLV/*&%
3(253(%2#2#O23:7*&2^Zg!\>>g:4!I3%97;a**W%%S<3(O%3(Z[,4!&/*&%S3(253(%2#<
97*&2),c4!"4+<3(S#N2#O23>ON4%Z94`94)!&%c%O,c4N#!I#<*&*&!&;2#O72%
#=!W#<P\3!"#L7C94.*&!&(DLP94)93#%(P\3!"#=P#M3(!&;!W#*N;<*-*W%)!W#B
%97;**W%O!&%%>N-23(9!W#93#%(P\3C!&#%36cP/36+;!W#%N<9423(%c#*&D)`94
#!W#<P3O+%:7;*%*"!"#)%93;D>
3c%#/V3(!W#<2Zb#=nj<n:h^w\qv2t+O:4Zg936!"%./23(OO94%O!&%9/V<#9a;%!W#94
P\*&*",N!W#;),ND^dff? #%2a+P5G23O!&!W#;+O36!I#<P\23(2#%,42#);2#23!W#;.%:7;*b*W%%c
#=#!W#<P3OM%97;*ff%*&!&#%93;DZ,-,N#<a*"*&,3(!W#<P\23(2#%.23(9!W#
qvlw\t2(tw\qvlhfquw\qvflGaP94S%23(4K93(ArHP\3N;!&/2#K%NP *W%%g$T>
?#{9!&*\Z355364+!&%S%cP\*&*&,N%2dNQc]^3(%2Z[,-`;<2#23*&*b%97;* *W%%a ` 4 3K
30 `I `bc 4 3K ,N!&94B36%3(x # ,4!&4L!&%)%9a*"*&23C5361M94K]^3(%K/V<3(!W#>$N42#gZN
]#`7G25
3 076 b OaP%97;<*g*W%%c!"%c4%2#K,4!&4+53(!"%S94S4!&;4%Q;!W#)Pb:.!&2#D
3(;<3(!W#;S9453(/!"%*&D.O2#<!&#)23(!&23(!W> 3(N*&DZ,-N4%N%97;a**W%%-,4!&4
3(SC!Wa*^,>3>2>O%*"!"#KP #!&
# 8c> #`5G%%!W7*&3(2*&!&2!&#{9
P 8!&%2d

8

8

x \g*1%:

#<;=

x \^?>@:n ; x O7 x -`-d-!&%97*&2*W%S!W#&BaRV
x
x \ H -`ffd- ; $ H -e1|RV
>':
:5; C OPTzBA

Q44!&;423O:4+#O723P!W#<P2362#% = x \gS,c4!"4L3(+#L!W#<P\23,\TZY944!&;423
x \g%94*&L7>KN2#ZC: # %94*&L7.5G%!&!&/>)!W#;(:nw)!&%O*&%+%2#%!I7*&>+?PN9423(
_

fi



wyz{

wyz{

!&;36`dbc5!&/S;<2#23!&#KPTO%QP%:7;*g*W%2#!&%
3(C97*&2*I<%%!W#BR,4!&4B4/K4!&;4M423(!&%!&),-!&;4<`3(;<3(!W#;:4423(!&%!"47P
94%95235G%!&!&#<_7%536V/<23-,-S2#<'$%N*W3(2D)!&%2%%'E;!W#O*&cP:K!"2#D>cN4
P #!&#TIDz AO2%93(%,c49423c*&!"23*&%P 3(\L2#53(7<7*"D=7G`%*&/=,N!&94#!&S*W%%
P 3($T>E?\)5%.M5!W3aP*W%%)B=3(2*#`7G23>EN4+*W3(;<23GTBz x \ H -`94+*W36;23
94%!WO!&*W3(!&(D7G,-2#ff\#E94=#!&)*I<%5-.> =!&*&!"L/V3(!W#<KPS94{P#!"#
F ,c4!"4K!&%-:]^#+7DO2#!W#;23N#K^4% x |2}}agP\3ff5*&!&%94!W#;.:49a%9r> c%
E { G
:
:na@
: : 1OO94S!W#23(2a%!W#;./;2#%%cP 9423(!&23(!W>
# @
H%JCKMLONxm" ` 4 3N 1
3 H%JCKMLON4mf`I `bdc 4 3N 7K94%PN4%2#=%97;<*T*W%%2>
,SZ*&I
N42#gZ,-N;2#23aN%97;**W%%N,N!&94CS3(%3(Z 7-25*&VDC94N*W%%ffP 3(
H J KMLONN%
%9<3(*W%%P394ff%97;a**W%T2#O23a!&#g> T2*&*94T%P%97;**W%% ;2#23
,N!&94M94!&%.94s ` Q Pj RTS KOLMN > x #%!&23*&%= !&;36d)N4)<L*&!W#+%94,N%O,4!&4
%97;**W%%-3( ;2#23aO,N!&94S3(%3(C # > N42#%OTaP942<3(T%*& x 7*Wrc/*&%9
#.%.% %9<3(!W#;S5!W#<% P\394N;2#23!&#OP#,L%97;a**I<%%T,N!&94C3(%3(p>ON4
3(%3(# %94*&);!W##-7NO4!";4!W#C3(23 S*&*",EcP % 2#O23!&#CP94N%:7;*
*W%%2>cN4O%cP%:7;*b*W%O2#!&%!&%c942#{;!&/2#+7<D4 ` 4 3K ^ ` Q P R KMLON x !P
,-25*&D-DBC-`c3`7<D5f`I `bdc 4 3N ^ " ` P_ RTS KOLMN x !0P,-25*&D-ABg-a`Ibc>N4%Z%:7;*
*W%2#!&%3(c#C94#4#+*&*g%97;*[*W%%c23(!&/),N!&94KS2369!W#)#O7G23C #
P-!I#<P\23(2#%O%9494!&2#7.%%9O=94a94K53(aP*&2#;94!&%O3(>.#:4`<9423
4#Zg,-`4/%O%97;a**W%2#!&%,4!&43(23(!&/=,N!&94{.4!&;423#O7G23NP
!W#<P2362#%2ZNO%f # >t > N4%c%97;*g*I<%%53(!"%O4!&;4K;!W#CP:.!&2#D+72%
94D3(O23(!&/P 3( %97;a**W%%%*&=,N!&94P#!&U
# 8>N4a!"%Zg94D36`236!"/<
P 3( *W%%,4!&4L<3(.#%!&23(L=7K/23(DB36*"/#<OP3S{%95235%!&!&#<_7a%1943(2
53(/23>
[3 %*"!I#;K%97;*g*I<%%NP 3(94%-P%:7;*g*W%2#!&%N,-c25*&VDCP #:_
!&#0',c4!"4!&%T!W#*&D7a%O#94ffP #!&V
# 8Y'A<#%*&T*W%% ,N!&9494N4!&;4% ,-!&;4<
3(;<3(!W#;G->(M!&%N:]^#=7D
x \*1W8 x \^*(X x \ J

_Ih

fi

J J++uM'++'

+x+~+



'z+++ ;+{

X%!W5*&DB#%{,-!&;4B%9Pc94#O723P/3(!W7*&%!W#s\$#M(,{!WO%O94
#`7G23-PbP#!"#K3-53(!&2S%DO7<*"%Q!I#E\ >TQ2#Z!&Y;2#23a*ZC%97;*g*W%%c3(
53(:P\233(>TQ4!"%c!&%%2#%!W7*"7G2%S:4D.2#{%9*&*&D+7%*&/3(c2%!&*&D>

]\ L[_^c$Q^fi *` T[5aO fi$fi
[

fi



? #94!&%S%!&#,OO!W#943(!&2*T#=53!"2a*%95G%#23#!W#;94O!W#;3!&#{P
%95G235%!"!"#<_ ;2#23*"2C%c!I#<:4S!W#5%NPCO*b*&!WO!W#!&#=53(/23>TN]^3(%2Z
,-53(%2#%O36%9*&%36;3(!W#;53(aP*"2#;<94S#%23(4S36!&#%2>b% 7:P\3(Z,-O2a%93(
9453(P*"2#;<94)7DO94#`7G23ffP!W#<P2362#%N!W#K!"> 3(P%23(4!&%NO2%:3(7<D94#`7G23
P-!I#<P\23(2#%:4536V/<23`%`523P3!I#3(23cK]^#)53(P>.N42#gZ,-!W#936%O
O94%-P\3N36*"/#D<_7%{]*&23(!W#;.aP*&2a%2>
v lI#*fixs Nqrj


4 2#.a!I#;K5%!&!&/#!&N*&2%?$:{PO7<_5C53(/23 94!&C!&2!&#$+P
5<_ ,#K53(/23ZO53(P *"2#;<94)3(!&#!"%c5%%!I7*&!PT:4NP\*&*&V,Q!I#;K%!&9!"#K23(%>T?vP*B
!&%Tc97*&294T36253(%2#<%536P^<#GTjS!&%Tc*&!&23*,4!&44%TQ2594O%9C*&*"23b:4#0{CK|TP
S73#4C!W#wB1,N!&94C25944{,Q2#.369453(P*&2#;94C!09
P b*Tj!&%N#!0]^7*&S,N!&94K*"2C
; $ * >Q2#Z[,S<3(!W#236%+!W#94S%!&#P,c49423-,-2#K]^#{*&2%%:P *!W#
94%23(!W7L%2#%K!0P,-4%0$ *l1S: x OI-d-!&%P H - ; )/. v RV%`5365%=!W#
!&#>p@#<P\3(9#a*&DZ/2#{!0P*%*&%S*&*bPa%N!W#tOI-Ud -!&%`P H - ; ) . v R#5z
!&%<37!&933(DC*I<3(;9423(!&%#;3#94a,-2#C]^#%:P **&2O!W#94!&%N%2>TQ4!"%Q!"%
/2#93!0P:4`7<_5)536V/<23T2C5*"D%P !W34236!"%!&>

Hl v fOt9n<mqez ; c wYqt2(t)q`nm2o&np<tt2w$vSnl[j+nD>nquDqtp:qw\qvm07,vpm[qwYqn<w
lfhfquw\qv2t{pl[quwo&ts`sOn/>(fs),. v vtltn<w\t9j=|nM(tfo&pw\qvfl{h^(ft2Kw\n9w\qvlk quwYqE$v`nl[j
t2sQh^o&fX|qvl/qtp:qWw\qvm#7/vY.m9nl{(t9jpm:t`wYqtch^(f2fN>So&tlwYq)fN>Snh^(ffN>*>f<"$vNk quwYq,-DBC- -ABC-`IbcQ
(ffN>[bzgz7-N#93*#`7G23<#,C12N7GT94 3(23(!W#;%P\3g3(236S3(%*W!&#g>b3
*&!&23*M(ZMK2#<%T94c#O723YP^%DO7<*"%ff!W#,M(>$ v1OXOj x jR H x H x x R H OP* x jR H
OP* x H x QR H OP x :jRR>^zf7/v x OM #HIJJIJNH MLRV*%
1 e L # V
7 gv x h Z<,N!&94 h 1M Z!0P:M !"%N5G%!&!&/Z#
f
hM

1 b Z9423(,Q!"%> 3(/23Z
MK
H M#i x F H FT!&%23
7 vg x M\
1
>ljz >MK H #M i3 x F H FT!&%O23)>
5

Q42#)!&4<*"%d-3N]53<O23pzZ7,v !&%SP !W3c423(!&%!&> 3(/23Z9423(O3(#*&D
=
X 53(P\% P^94Q!I##%!&%2#DP$ v[,4!&4C#<9!W#*&!"23*&%-,N!&94O5<_ %DO7<* >),. v #<9!W#%
#*&D=*&!&23*&%,Q!":4=5<_ %DO7G*nZb94;4g>+Q2#Z !&`!&%O!W5<%%!W7*&+94O)*&2{PC) . v !"%
55*&!&27*&>

g3(943(!&2*g5!W#<-P/!&,E,-4/;a!I#94#;!"/<3(%:*"ff94ff!I#C;2#23a*^%:P *
*&2%3(S#<N*&2O2#%P) . v >TQV,-/23Z2C5!I36!"2a*%:!"% x %S!&#+<N3(/2*g94aN!W#
94O%2%%%:P * *&2%S<3(S;2#23=7D+C%95G235%!"!"#B53(/23>Q2#Z,-`%%9O
94K%:P*c*&2%)3()!W#$ *#142#:P\3(94!I#,4!&4L*&2a%('A7G!I#;15<3(PS
53(Pv'2#C*&2+K4!&;4+53(P %23(4)3(!&#PT# XL53(/23>
_I

fi



wyz{

wyz{

W lIafilxrs Nqrj


N4O:F%3(;3(!W#;)94S%9393(PT94%23(4)%95a`aP,-DBC- x -DBC-a`Ibdc2c53(/23-2%
7<D`:4%cP*&2%-3(N*&%*"D)3(*WK94Spw\quo&qvw|Nhgfo&t2s x >;>Z !W#<#gZ[|2}}P 3(94
3(2caP^5*I<#!&#<_7%K*&23#!W#; x XT-z <#.23(5233*&23#!I#; x %*&% 3r/!&4
H2Z[|2}}>Nff94-]^3(%ff%!&;4<2Z*"2CS%c*&+7c!W#2353()%T!W#<93(!W#;.#,E;%
!W#943(!&;!W#*%23(4K93(arR72%S%97<_ !&# x 53(PPT*&2-2<#)7`36
#Q!I#<P\23(2#S7<D55*&D!W#;c*&2>TQ4!"%ff33(%:5#%-Sa23(5233b*&23#!W#;O3 Xffz
,423(c!W#<P2362#4!W#%N3(c;2#23a*"!&+#K!&%\#!&/*&D)%3(K%#,E5G2336% 3 #25
%23(!W5!&#% x >;>Z !I#<#gZb|2}}<Z3(%:5!&/*&D> O%94*"=#!"Z4,-/23Z[94Q94%SP
*&2%-%N#-#*&D!W#%23(N#,E;%N7a*"%#,l[f2jt!W#<O94c%23(4O93(>ffN4!&%O%
P 3(R94Pa94aN94S%9393(PT9<7*"2<xB # ,4236`C*"2C!&%55*&!&=!0F236%cP3(R94
%9393(cP#!W#K:4235<3(%T*g97*&2/BN,c423(:4*&2O53(P!&I
% 5#j >TQ4!"%
4%#!W#oi^2#+#94K!I#<P\23(2#%.5G%%!W7*&+,Q!":4B # #=B x 94K;%O;<!W#;+P 3( 94
#%AF # #F:4<3(O*I<7*&=,N!&945B # #xBZg3(%95!"/<*"DO7!&S4%S<#+:Fc#{94
/Va*IOO5*&2#%%7G#4$%%!&;#%N`:497*&2g>T-#%!&23(!W#;K94S7#%!W#<93(
!W#B!"#BZB # 2#=7G.2#O23=,N!&94M+3(%3(/Va*I),c4!"4!"%%9*&*&2394#3c*
{94`#L+2#O23wB2>? #M#*&;<D=C23(+5233*&23#!W#;=#M#%!&23(!W#;
94%)3(2<3r%Z,-)#,%9C3(!&)94)/V#<9;<%`#M!&%9/V<#9a;%OP%!W#;*&2%+qvl
m:fll[t9mw\qufl{k quwYq)qvw\t2(nw\qv2tOjt:t(htlqvlh^(f2m:t9jp(t>
$O!W#3ff/V<#9a;-P!I#<93(!W#;*&2!"%-94c/V#<9;<aPYj<t9mt:nqulffhn<wYq.m9fawvZ!\>>
94%%NPT3(253(!W#;)94!W#<P23(2#%S#{P\3-!&%536P>TN4O(3ff/V#<9;<PY%!I#;
*&2%Q!"%c94Q94DCrSK(tw\9pm2w\p9qvlfN>SwYqtwYqtt9n<m[q.hn<m9thf:q\o&t>
#94-#c4#Z#N2#%9/ff945G%%!W7*&D4!";4O%23(4:F3( #OP3 53(/!W#;S%:P *
*&2 x a%%9O!W#;+:4`*&2C)53(aP2#7C5#,N!&94!W#=94]^#!&%;O2#<PfrU7G
#%!&23(>T#:4:423T4#Z!&-!"%N5G%%!W7*&94-*&%K97*&2C2#K73(24.,N!&94!W#K
%9a*"*&23N36%3(c/V*W x :3(%3(3(!&k# >TN42#gZ94S3(3(23(!W#;O:F%%:*&*"D)*&*&,%
%*&/`53(7*"2%94a,-23(O53(/!"%*&D+NaPY3(2a4)7G2%O94%2364{53(36S;%Q*"<%
!W#:4 x %9*&*&DK5G#2#!Ia*"*&D-*W3(;23Y%;O2#< P94c%23(4O936N:]^#7<D*W3(;<23T3(%3(>
?\-!&%-*&23Z4,-/23Z<94T:4!"%N/#<9;c#*&D.4*&%N!0P94c%;2#-P94c%23(4C93(c:]^#
7<DS94-*&,23 36%3(-/Va*Ic!&%T# !W#23(2%CSO4O7D94N%NP94Q*"2C%2> -#%!"236!I#;
3b%2<3(4O7G#%-,-2#%-94T#3C*&*"D3(%3(3(!"#%T2##T7GN;3#`,c42#
%!W#;)%95G235%!"!"#=;2#23*&2%c!W#=# X536V/<23> 42#=%!W#;)94O!W#<P\23(2#.7#
M3(%3(=3(!"#1!&%);<3#<1!0P7<D1%!W#;L*&2%+M53(PS*&2#;94E3(!&#A2#17G
79a!I#>? #{94S2a%OPT94O2594#,-!";4_ 25:47G#=!W#{;2#23* #c/2#.536P
*&2#;94{3(!&#)*&2%c3(%3(`36!&#g>
ff%!&%:45G%!&!&/+:F%P%!W#;=*&2C%2Z %O#;!&/K:F%a*"%23>MN4%
%2 P 3(#{!W#23(2%M3(##D>N4!W#{!&%9/#:;.3(;3(!W#;)94%OP*&2C%
!&%94)!W#23(2%+P:4+73#4!I#;L3.P94K%23(4M93(>L?!&%.5%%!W7*"94aMsOqo&t:njqvl
fo&pw\qvfl{PY%97;*D)77:!W#+94Q*&#N7P#7:P\3(,Q!":4!I#;!&/2#]^#!"
%;2#2> XY/2#!P3(%3(3(!"#P36m{Ka{ W 6{K23(% !&ff!"%-5%%!W7*"c94 %*W!&#%TP
%97;*&%:4T*&+#-7cP\#K,N!&94.36%3(a{ x ,Q!":4-*"2C%9ff2#K#,7QP\#),N!&94
3(%3(#{ W #C*"2C%2> N4!&%-2#K3(3(23Y94N%2364O%:5!W#KS436*"DC#:3(*&*W7*&<##23>
-#%!"236!I#;94)!W#<P23(2#7#L!W#M%O.2a%%O97*&2M,4!&4M*&1#7G)2#O23
,N!&94K3(%36A{)2#K#,A7G2#O23a),N!&94C*"2C%2>T?-!"%N5G%%!W7*&S94a:4%cP*&2C%
_

fi

J J++uM'++'

+x+~+



'z+++ ;+{

%95<3(% =O3(K94#={6{ W !W#<P\23(2#%2>L?vP,-%):4)2594L7#L!&.!";4.7G)94O!W#L
97*&2%OS73#4%Q,4!&4{2#)7GS*&%{!I#O25:4);3(2a23ff94#4{,N!&94N*&2%c2#
#,7O*&%!W#O2594{%9a*"*&23c94<#K3N*bG{ W >NN42#+C*&cP%:523i^%!W#<P\23(2#%
2#=7G!W#:3()94.#,O!W#!W*-53(PN%;O2#<2>#*&;<%:F%9r5*W,c42#
%!W#;94c,-!&;4_ 2594)7# x %95G!W*&*"D*&%S,c42#)%!W#;`:4#<];3!&#KP947#
%N%23(!W7=7D %23-c*\> x |2}}<>
c!&!&#*&*&DZjph^o&qum:nw\qvflfN>+t(sOt2l[wv+fN>wYqtt9n(m[qBhnm:tK2#23>%%9O!W#;L94
5#*"2C.536PT*&!&%,N!&94!W#94!W#!"!Ia* %;O2#<aPrR.7O#%!&23(Zg:4`%P#
!W33(*&/V<#*&2C2#2%`K3(25G2+5*&3!"#)P536%NP 94S%23(4)%95,4!&4{%
#ff#:!W#S53(Pd !I#S%9523i^%-%*W!&#CP%97;*!&%TP\#K(,N!& x /!I94c*"2C
#E7D15G23P3O!W#;B:4+!W#<P2362#%+#B53(/+:4)*"2CC943(%:*"!I#;L%:523i^%
!W#<P2362#%4/K=7523P\3OM(,N!&ZT>=Q4!"%K!&%9/#:;ZT4,-/23Z 2<#B%9a*"*&D17G
/23(7<D%!I#;K*&2a*P !&*W3(S24!W#; x zg*\>WZ|2}}>
ff%!&%N:4%:F%Z,c4!"4K2%O3(%9393(!W#;P94c%23(4C%95Z!&-!&%N/2#K5%%!I7*&
94Q94`%P *"2C%Oqul[m2(t9natwYqtClpst2f>Sfo&pw\qvfl-P 23(9!W#%97;*&%c94N!&%%c!W#
94O,4*&O%236493(>N4!&%!&%S72%O94%OPT*&2C%2#)%:4,,-*&*0_r#,#=53#!I#;
4#!&%O*&!Wr36;*W3(!&(D+%!W#)#3(;*W36!"(D4r<%S3(.5G%%!W7*&K!I#9453(PNPK*&2C>
^369423O36Z-94=#,N*&D!W#<93($*&2%)2<%9453(7*"2 94K!W#12a4!W#<P2362#B
5<%%!W7*&D)*I<3(;S#`7G23-P*&2C%4%-7%K!W#)3623T`23O!W#S,4:423T!W#<P\23(2#%
3(S5G%%!W7*& x n:hhgo"qvm:nqvo&quw|Ow\twW>TQ4!"%c#%%!&9%#,#!]2a!&#+25%2>
?#L%9C!&#M3*"2CB4#!&%9 !&%.!W#;2#23*#<.7*&+M53({*&2%K94
*&2+.53(P *"2#;<94+3(!"#+#{94%Q.3(%3(S3(!&#g> /23(:4*"%%523(!&2#
%94,N%94!W#BC#D2%%36!&#%OPN94.53(PN*&2#;94M#=94)#36%3(K2#=7G
79a!I#> 42#C%9a*"*g#O7G23 P*&2%ff!&%%:<.!&2#ffP\3 S3(%3(3(!"#94#`7G23
P!W#<P23(2#% ,c4!"42#S7GT%95<3(`7<D%!I#;*&2C%2<#cO94-#O723P#,B%:523i^%
!W#<P2362#%7DK;#!&9%2>ffN4%Z4#!&%9%<3(#)!W#K3623TS%*&S(t2o&t2nl[w*&2C%
P 3(OI-Ud -!"%Pa H - ; ) . v R`94a%94*"=7G`!W#%236!I#</$ * >c?vP,-2#]^#=K39423
%9a*"**"2C%,4!&4+5G23O!&%3(%36`3(!&#{942#K!W#*WO%*&*2%%c,-2#K]^#
53(PO4CP%23ff94#K,-*&+7S5G%%!W7*&O,N!&94N%!W#;.*&2C% x %S!"#)> <94
94c2%cP*W3(;3(!&#KaP:453(P*&2#;:4K,N!&94NO3(!&#KP94S3(%3(#
#3C*&*"D+5G23P\3O%N%!";#!]2<#*&D{,3(%94#K:4S2%aPYC%9*&*536P *&2#;94{3(!&#{,N!&94
3(!"#)P 94S3(%3(S#>

< +rt[ =+rj


#*&;%cK94P36;!W#;.%!&#{,O#,,N#<!W#936%O`<7%93c53(!W#!I5*&%OP\3
]*&23(!W#;*&2a%7%K#C94!&%2%%!&#+3(;3(!W#;O94%9393(cP:4%2<3(4C%95>ffN42#gZ
,-2*,N!&94K#23(`4236!"%!&%55*&!&P\3ff%*&!W#;)*&2%2>
fi! < n#"%$pWW'&nmbjopoQ
l

!W#)%95235%!&!&#L536V/<23(%25*&DB)!0F23(2#<`%23(4%42O.:4# X536V/<23(%S#M%!W#
94D=4/O:F!&/.O4#!&%9O%P\34#*&!W#;*&!&(DZ,-2<#=%%994SCP\,%97;<*&%
,4!&4+3(S4<3()O%*&/ x :4S53(aPY#%%!":%O*W36;S3(%36,S>3>>;!&/2#5*&2#%%
7#S,Q!":4=# X$53(/23c2<#=7K%*&/=,Q!":4*&2%2>)Q,/<23Zg,42#=%!W#;*&2%!W#
_I

fi



wyz{

wyz{

3(23*&%)%O)%97;*&%`aP#M5G2#M97*&2M94)3(2!W#!W#;M52#M%97;<*&%%94*&17G
2%DKO%*&/,>3>2><94c;!&/2#+7G#>Tc9423(,Q!"%Z,-c%!&*&*2##N%*&/94S53(7*&2R,Q!":4!I#{
%9a*"*&233(%3(> O94,-%9*&*&D=2##594*&*T73#4%SPN)53(P2#7G
%943(2#j
+7<D)%95235G%!&!&#<_ ;2#23M*"2C%2>S!W#3c*"2CK;2#23!"#536V/!&%S#
;3#:4%:P*[*"2C%T3(-;2#23 x %!"#O>W|2b%9*&*&D#*&Dc%9a*"*#O723bP
*&2%ff2#7G25*&D!W#K53(P6>bc*&*!W#.a*"*[,-79!W#O94q
!I#<23(%!W#r; `53(P\% x !\>>:4%
,-S,N#<NO]^#QP\3N<#+55*&!&2!&#aPT*"2C%3(`536P\%N94a#<9!W#{#<D.%97;<*&%94
3(2a%D)%<*"/<:'A#{2#+42#O7O%*&/@
#</2#!"#*&*"kD K,N!&94!W#C%9a*"*b3(%3(:'$#
#*&D.P,E43(%97;<*&%T94aT`%7c%*&/C,N!&94*&2C%2> N42#gZ,-N2#O5-94T%!I#;
*&2%-*&2%N`3(%3(36!&#%2>-3 ]*&234#!&%N%94*&+42#936D]^#)*&2C%
94NO!&;4<7S53(QP%:4)53(P\%2>
g3(94233(Z,-S%:4*&=%!W`4, C#D+#,H%95G23i^%S!W#<P\23(2#%`<3(S!W#<93(
7<D`c*&2> N4-!W#<;3!"#SaP^#,1*&2%TO% # !W#23(2%N94N73#4!W#;`3TO4g>
c9423(,Q!"%Z94;!W#{PK5%%!W7*"K3(%3(3(!"#!&%S#;=7<D)94O*I<3(;O/2342 x %
!&#>>
Q4%=23(!&23(!W*"2a%+L:43(=!0F23(2#K]*&23P#!"#%:4)#2#<93#A23(9!W#
%95G%-P3(*&/V#D>NQ4N]*&23YP #!&#%3(Q,*&*0_ %9!&P3ff*&*gP9453(/!"%*&D.!W#<93(
5*&2#%%7G#%2>ffSO94S/;2#%%NaP94c]*&23Q23(!&23(!WO,-S%2a4]*"23ffP#!"#
!W#3(23O4%{%O{*"2C% x %!"#1> ):4!&.!&%+7G23C=%*&+{P\,
##%%9<3(D)*&2%N:4#K!"Q94%*&!&#{P !IC53(:#-#%>
fi*)ts nIvCuj$g[[vn?vxwNj-L[vn
l

4O]^36%c]*&23cP #!&#!&%S2a*"*&sC:
N
>ON4!&%P #!&#=!&%O39423c%!W5*&)#=!WO%`ac]^#!I#;

*&2%ff94ff`#ff*"2a`4!&;4!W#23(2a%aP:473#4!W#;3>n *
5*&!&%94%-94!&%N7D
%!W#;.r#,N*&;79!W#)!W#K94c*&2;2#23!"# x 53(253(%%!W#;N54a%aP947_5

53(/23>O? #9!&*\Zn *
%*&%P%,N!&94{94.4!&;4%/V*W.3(;3(!W#;;O2#<cP#!"#
>
8 *
iwof5jj

vQz q,-qn5 rj 8 :y
3TS5G%!&!&/S#!&CM(Z;2#23!W#K9453(253(%%!W#;54%Z*&|{ x M\ff#T} x M\T7c94#O7G23(%
P 5<#%!&#+#)#93!&#K!W#<P2362#%2Z^36%95!&/*&DZg94a#M,N%Q!I#</*&/{!W#g>TN42#


8

:y x M\n1%} x M\/{ x M\ J

#<%:4)!I#<P\23(2#%)94a`2a4*"2C2#!&{,N%!W#/<*&/1!W#1<#13%<_
*
5#!W#;+!W#<P\23(2#%`#;!&/Zg#<93!I#;)!W#<P\23(2#%`5G%!&!&/>O?vPNCPaM ,N%aP2#!W#/<*"/<
!W#B<#5<#!I#;!I#<P\23(2#K*&!Ira+3(%*W!&#3c%95G235%!"!"#gZ 942#=M-3#<D%9723%SPN!&
3(#!0]^7*&S,N!&94 x %9723O%-P x a!W*WY*"!&23*&%P !&%3 23(!&/{*I<%%2>TQ2#Z!0PM
!&%a+:4`!&!&2!"#)P 945<_ ,#)536V/<23-!&2#{75G94#M3Q23(9!W#
%2##<%aP!&2<#=/236DaP2#9ra536S!W#M2#%!&#M%25%2>!W#+94!&%O*&2%4!";4
!W#23(2%cP9473#4!W#;3N,-3aN94!&%N#;!&/> QP3(%Z,-3(N!W#<23(%.!W#C94NP
94NO*&2C2#)7G`55*&!&=7DK94 XL53(/23> Q,/<23Z%!W#S*&2%cP%95235G%!&!&#
53(/23N<3(`%9a*"*&D!"`;<2#23*O<%aP942D+7%P3N*&%!W#;)233(!W#;)%97;*&%2>
8

_I

fi

J J++uM'++'

+x+~+



'z+++ ;+{

g(b) = f(h(b))
transitivity "="
g(b) = f(f(b))

f(f(b)) = f(h(b))

symmetry "="

congruence "="
f(f(b)) = g(b)

f(b) = h(b)

f(f(b)) = g(b)

h(b) = f(b)

axiom

symmetry "="
axiom
h(b) = f(b)

!&;36dT!WO*Ia!W#;O%:5235G%!&!&#{%25K,N!&94 X
3Q23(!&23(!&#!WO%!W#*&DaN*W!W#;+*&2C%:4<3(S55*&!&27*&.!W#{.#<D)2%%#
42#O!W#936.C#D)%*W!&#%PT%97;*&%94N+#c*&2.K3(:P 9!&#{PT94!W#<_
5*W%%2>?##<93%N)5#!W#;+!W#<P2362#%`,-3S#<93!"#+!W#<P\23(2#%5%!"!"/<*"D>
? #Z XM53(/23(%ff`#-4/N#<93!I#;!W#<P23(2#%>-N%T%:4V,c#7DzgN*\> x |2}}Z
%97%9C5!&#{2#53(*&D+7O%!WO*W=7<D=ppsQh^w\quf<l=m9f<lw\n<qul[wv>-N2#Z*W%%94c3(
7*&S#<93N#<D:423-*W%%c2#)%955G3(N%23(4)53#!W#;.4#!&%>
fi*~ vn?vxwNj-E[u
l

4%#)]*&23P #!&#x * :3(!&%C%*&NP %N:43(S<7*"OC*"<%S%97;<*&%N94c3(
N
/23(D=436=+%*&/,N!&94MK##!&#<_ 9<7*"2<<_7%536V/<23>.?#=3623)%!IC94!&%2Zb,-
#%!&2394S23(!&/!&#=4!&%3(D+PCP 2> 25*&D:4!"%c]*&23QP #!&##*"D{!0P*&!"(D{!"%
!W#/<*&/+!W#+C53(7*&2)>
!g5 bv zgtOP x x E1 x QR#mO x x1 x QRB7B!&O%2>RN42#gZ:4*W%
x *1 x x QR2<#+7O23(!&/=7<D)#S%:5235G%!&!&#%25g>NN,-/23Z!0Pf x (1 x x ff!"%
O%97;<*gPT# X153(PZ!&%53(P !"%cO3(C5*"!&2 x % !&;3(>

?#=;<2#23*\Z !0P:4.%95G235<%!&!&#L%25=!&%.5G23P3OL`5<%!&!&#5L# n2#%O94
2594aP945<%!&!&# x 7G/x n1|2Zg:42#aSO%, n>EK!I#<P\23(2#%O3(#B!W#3(23
.536V/<94O3(%9*&NP %94+O%:5235G%!&!&#{%25g>TQ4`53(PT#%%!&9%cO%2594)P
n4 >>
Q4!"%<5*&)%94,N%9494%!WO*W!&#MPN94%:5!0])a!&#*T523!"#%`aPQ%9<_
5235%!&!&#{53(/23-#%%!&9%O4!&;4)25:4)%N,-*&*%-!W#<P23(2#O3(%36!I#{# XL53(/23>
N4ff!&%2Z*&2a%236!"/<7<D#<D`%:5235G%!&!&#C%25%<3(7*&cS*&%Q%97;*&%-94 2<##
7C%*&/L7D# X$53(/23c,N!&94!W#%9*&*-3(%3(%>.Q2#Z!0PN%94*&2%O3(.<55*"!&27*&
*W3(;S36%3(O3(!&#%5%%!I7*&D{23P3T25:4)3-!W#<P\23(2#O3(!&2#<7G#%2>-N4N;_
O2#<ffP#!&
# 8 * 25*&D%N94!&%N23(!&23(!&#g>Y;a!I#gZG94N]*&23P#!&#E : %*&%-P %-,N!&94
94S4!&;4%c/Va*I3(;3(!W#;94!&% ;O2#<-P #!&#g>
_2

fi



wyz{

wyz{

WsQz q,-qn5 rj 8 :
3NO5%!&!&/#!&aM;2#23)!W#)94O53(253(%%!W#;)54%Z*&q8 : x M\7GS:]^#=7<D
iwof5jj









8

8


* x M\*1




>8
xM#?
:
L 8
e
vf #

H M!&%#)!&
H M!&%N23(!&/7D+O%:5235G%!&!&#)%25),N!&94)5362O!&%%
# #xM
MH !&%N23(!&/7D+##<_ %95235G%!&!&#{!W#<P2362#
!W#</*&/!W#;)94*&!&23a*"%aM #IHIJIJJKH ML x { ; ? J


x MY9?>J|
*

: x Mvv

$ oc
"v N
w b2j-E[vn
fil
l

9 !WO%%*&!W#;=*&2%O94a`3(K7*&.{%*&/K%OK43(
394!W3(]*"23P #!&#= :


%97;*&%NP X%97;a*^*W%%%94K94Q94S3(%9*&!W#;K52#%97;*&%N2<#K2%!&*&D+7S%*&/>
9 #%!"236%:4N%%ffP%97;**W%%C L
Q2#Z:4 6;O2#ffP #!&]
# 8 :9 %7<D, :
L
3 `Ibd c P\3 23(:!W#3(%36#{b> 3b2a4O%:7;**W%aTjZ!0P*&2CAMg,N!&97
4 8 :9 2 x M\?1
!"%%2Z94*&2,Mb,N!&9494S4!&;4%ff6;O2#q
8 *9 2 x M\ff!&%c%*&#<!&*C!IC*#`7G23
P *&2%N!&%c%*& x %S!&#> N4!&%ff6;2#N!&%N5a%-P<*"*&,N%2>

sQz q,-qn5 rj 8 :9 2
3)15<%!&!&/#!&M;2#23$!W#$94M53(253(%%!W#;$54%B#M%97;*S*W%=Tj1
OPM # HJIJJKH RZ*"q
8 *9 2 x M\-7GS:]^#+!W#94P\*&*&V,Q!I#;)##23d
?vP#%97;a*2#O7G%*&/,N!&940M(Z!\>>C
z H |ay6z*n
H W
1 A/ x bnMv H M\Z:42_
# 8 *9 2 x M\*1$>
c9423(,Q!"%Z*&CTj51O #IHIJJIJKH Ru6Tj.7%-P*&!&23*&%<# 7GS%97%!":!&#%`:4
!&%%N;2#23*,N!&94g
H |a'
,yff)d x bj2<
x M\> 36V/<23Z#23*&*b%97%%QPfTj+#
%97%!":!&#%T,N!&9494!&%-53(5236DZ*& 94Q%:TQ<#`:4%:7%!&9!&# 7Gc!IOHPg94
P #!&/
# 2 Z[:]^#=7
2 x #HIJJIJKH R 4H *1
&Z ) > # Q42#gZg94`3(2C!W#!I#;
B
# e
3+

*&!&23*&%cP:Tj)3(aTjr 1O U #HIJJIJNH U RZ1TQg"TQoff>Tzg
+7S
OC
5*"!&(DP #!&#gZ!\>j> +C5%
*&!&23*&%S ff 2| fi #=4!&;4/Va*I%
P !I#!&2.94a94K3(%95!"/<*&!"23* x %97;<*W552<3(%
.7G%*&/V<7*">NQ42#gZ
iwof5jj



8

9 2 x M\*1
*






x x U






2G

2

x x [4 J

8 *9 2 x M\3(2*&*&DB3%AMff,N!&94+4!&;4/*W.!0P<#D43(=%97;<*&%
`2<#=3(;#!&K94
x ,S>03>2>bPnTjK2#.7G%*&/.,Q!":44M(> 3(/23Z Mg!&%N3K,N!&94KS4!&;4K/Va*I!0P:423(<3(N#*&D
P,H%97;a*"%c!W#)94O%97;<**W%,TQ94aN2##7O%*&/7D)*&2C%<#:455G23
S7G%<*"/7*&S39423b2a%!&*"D x ,S>3>2> >b? #C3T362*&!"2a!&#gkZ .#%!"236%%:7;*&%T`7Q%*&/V7*&
94T<3(T%9C*&*Z4/Nc39423i^ 23H%93:3(Z##<DS/3(!W7*&%-!I#53(!&%#S,Q!":4O94
23 %!&>?#P936S,-S,N!&*&*bP 3(9423N3(:]^#V
8 *9 2 7D)5*&!&!"*"D#%!&23(!W#;+945*&2#%%
7#+,4!&4)!&%%)P\3ff94S5<_ V,c#.53(P %23(4g>
4D OqB4D5OxBqQt C4xOp5QxVrq O4QTO|45G1xVGxO B
dBGGQ5dD

OODdOrQx|BBOCrdddZq4?BQt dd44

Q qrCxOQOQDB4QqGtC

_I

fi




J J++uM'++'

+x+~+



'z+++ ;+{

TNfi $ g
TC

a1

? #{3623-K#<#)523(!WO2#<9* /*W!&#{P 3N!W#<;3a!&#)PT5<_ ,#7<_5
53(/23(%Z,-S5*&=(,-.3(2#,#53(/23(%2dff94 X536V/<23L#94%95235G%!&!&#
53(/23O> .4/.%94./<23(%!&#PE%S%23(!W717<D %23c`*\> x |2}}>
S4%7G2#)25*&D{!I#/23(%!&#)G>>
v


lxrsj+rxql

'Ds^ j^l,#xs!Clxjgfi;t

35G23(!WO2#<9*T2#</!I36#O2#<`2<#=7K%23(!W7E%P\*&*&,N%2dOXT4=53(/233#%O#!"%S,#
53(%%3K#79!W#%)94{!I#!&!W**W%%,$ a%.!W#52> {25*&VD<AM3:423O:.!&2#
O9436;#!&O9453(253(%%!W#;>SXY%%2#<!W*&*"DZg:4`5<_ ,#+53(/23Q;2#23a%%:7;*
*W%%ff,N!&94#TPg94-(,Q/V3(!W#<%2> ?#S32#</!W3(#O2# :4!"%ff%T#<3(!W3(N4#;%b!W#O94
5<_ ,#K53(/23ff72<#.7G5G23P3O),N!&94K7!&*"6_ !I#%cP:4 zN_ %D*&!W#5N*W#;;
P`[T>M!I#MJ25*&VD%,-DBC-a`Ibdc),-.5236!I2##*&DB,Q!":4M%97;*-*W%%
79a!I#C,N!&94#;!"/<N%936 *I<%%2>TN42#gZ<94%N%:7;**W%%-3(-]*&236Z93<#%(P233(
94-7_5S536V/<23ZV<#S!W#;3!W#c!&%b%23(4%:>N4N53(253(%%!W#;P:47<_
5S536V/<23!&% 523P\3O!W#`5<3*&*&*,N!&9494-53(253(%%!W#;SaP94ff5<_ ,#53(/23>N4N53(/23
!&/% *W%%ff,N!&94O!&%T7%!&4236!"%!&#<!&*94-5<_ V,c#`536V/<23g]^#!&%94%-!&%T53(253(%%!W#;>
N4%2Z[,O4!"/<`%D#43(#!&2!&#P:4536V/<23(%2>-QP23Q94Z,-S93a94O5%!"!"/<#!&%
P 3(H94N%TP!&/-P %ffP<#O]*&23 %O-P %T% %236!I7G>N!W#,-N2#25*&VD
9 :4;2#23)%:7;*
94;2#23)%97;*g*I<%%aPP\3ff94c]*&23-P #!&#4 *
*W%%2#{7%%a!"!"#*!W#5P[2b2O%N,-*&*a%NP\3-94%*&!&#P *&2%cP\3
ff> !W#*&*&DZ94S536V/<23(%53(+`:r<*&9453(7*&2U!W#)53*&*"*,N!&94K94!W3-%9<#3(
%!I#;<%2>ffD1%!W#;L94!&%K2#/!W3(#O2#<K,)2<#14!&/5G23!&#7DM4#;<!I#;L*&2C%
#%97;**W%%,N!&94#O#25!&%937!I#;{94O9423>?#+#93%2Zg7:4+#25%
%955G3(T2a4C9423ff72%3(%9*&%-7:!W#P 3(H#53(253(%%!W#;O2#K7c25*&D.!W#C94
9423>
T523(!WO2#<O!W#S:4 *"!&;4< P53(7*&2O% %2O!W#;NP 3(A94T,-*&*0_r#,#O53(7*&2J*&!I733(D
/g>W|>0>W| x *&!0Fca*>Z|2}}a*&!0FSJ9#23Zg|2}}<>? #C3(23YS79!W#S36*"!W7*&
*&*&!&#KPg9Z,-N25*&D.a*"*[!W#%-#<9!W#!W#C94 % 3Y% %>ff2%
94%Q!W#% /23 c,N!&3<#;-P^/236DS!0F23(2#-53(7*&2O% ,-%%9O-94b94!&%T!&%36*"!W7*&
%Q%2>
!W#O94O #<9!W#%c.#<D+53(7*&2O%cK*"!&%#!&%2%%94O3#<!WO%aPT%!W#;*&
53(7*&2O%Z,-{,N!&*"*O53(%2#<<#1/236/!&, PS94=#O723KPS%<*"/<53(7*&2O%)!W#E94
*&!W733(D>^3(:423O3(Z[,-S%:D+!W#,4!&4+!W#%S523!&#{!&%%95!W*&*&D=!W53(9#<S#
2*b,N!&94{94!W#P2a93(%3(%95#%!W7*"P3Q94!&%2>? #!&!&#{,%9D{943(%9*&%!W#{943(
a!I#%!W#=3(O9!&* K;!&/.#{!W53(%%!&#{P\3N:4`2362%!W#=3#{!WO>ON4!&%#23#%
94K!W#%.T x 2;36D+9436DZ zgc x zc_*&;273%9ZT#LNz x O7!W#3(D*"<;!&2>
N4S53(7*"2%!W#):4Sa!I#%c-T$#zgH#<9!W#K*&!"(D+%Q,*&*a%##<_Q3#)*W%%2>
Nz!&%Q3#<_ *&!"(D)C!W#g>
?#O9!&*\Z:45323(% Pg3b5G23(!WO2#<9*%D%23(dQ4N%97;a**I<%N2#!&%
,-23(;2#23=!W#M%94=),ND94cP\3/3(!W#<.|C,C25*&VD<B94)3(%3(G1 |),4!&4
523P3O7G%-!W#94c523(!WO2#<%2>TQ4%QP4!&;423-3(%36%T!&+#TD!&*&+723-3(%:*"%2>
0 2 1$G>3 /V<3(!W#N,-N25*&D4 # 1 "1$}
c*&!I!")94%aP%:7;*[*I<%%7<
_

fi



wyz{

wyz{

%.36%3(%2>c%%936*W%%CP3C#L5!&/3(:]^#2O2#<.,-)%*&%076 b 1%:7;*
*W%%2>HN4%=53<O23(%.a*"*&,-194{:.!&2#);<2#23!&#P`*&*%:7;*N*W%%),N!&94!W#
94{!I#!&!W*%;O2#<%KPS94{%23(4L936>@Q%9*&*&D,Q!":41:4!"%)O94$)O%.<=%:7;*
*W%%,-23(;2#23Z[!\>>g7c94O%9S#O723a%,42#25*&D!W#;+/3(!W#<`|<>-3Q94
%*&!&#KP%97;<**W%%N94-3(NO793#%9O!&C.N,-%K!W#<_ 252#2#<
53<O23(%2>H[3)-TZ-Nz Zc#$zg,-%|=*W%%2>?#E949423K!W#%)!W#
53(*&!WO!W#3(D{5G23(!WO2#%94S%SPT*W%%4!&/{94`7G%3(%:*"%2>

9 > T%*"
ZP * ZV#, *
Q47<_5*&2%b,-23(T%*&O/!Wc94 P #!&#%* :
,N!&9424KaP94cP #!&#%!WORPN|2*I<%%2>
Q4`%!W#;+P[1,N%!&2*&*&D4%2#=%%23(!W7G7<D %23cSa*> x |2}}>
N4+[2b2O%:#3(B4236!"%!&.%%2#!W*&*&D=%*&%O*W%%OP-94%9C*&*"%S%!&> 23(!&!&2*&*&DZ
*W%%3(%*&{,N!&94+736294<_\]^3(%Q%23(4g>
W !gn+lnj5 tbqt


? #{94P\*&*&V,Q!I#;{,-S5<3(O94`3(%:*"%P3Q523a!&/53(/23N,Q!":4+:4`%!I#;<*"K53(/23(%2>
N4!&%S53(!&%#!"%O523P\3B3(;36!I#;{94O,4*&. *&!W733(D>KQP23c942Zg,-#*&D
3#<!WO%c!I#P,J%*&+C!W#%P >
$ oc2fiv2M.&/-&+&ggsnWn/ MU
fifi


b7*&|O53(%2#<%3(%9*&%cPT3Q5G23(!WO2#<%2>?%:4V,Q%94O#O7G23NPT%*&/43653(7*&2O%
!W#2369!W#K!W#%-P >*&/)O2#%-94-S53(P*&+7QP\#.,Q!":4!I#)<%#%2>
#%!&23K53(7*&2 .7G`43({!0P#!":423SS#3M3(O7*&S%*&/S!&c,N!&94!W#
|2%#%>TN497*&#*&D)53(%2#<%N94S36%9*&%NP%:4KC!W#%N,423(S43653(7*"2%!&%
#M,423(+O*&2%O#)43(15367*&2*&17)%<*"/<7<D<#D=aP:4)#%!&23(L/V<3(!W#%2>
c94-9497*&2##<;<!"/<S4!I#<%N#K945G,23ffPb94%!W#;*&`53(/236%2> N4!&%N!&%72%S!&
%# ;!&/Q94N5*&c#O723YP5367*&2O%-,4!&4O2<#7N%*&/.7<D`2a4%!W#;*&53(/23b!W#
94,4<*"!W#g>T!W#`<#D.##<_43(+53(7*&2O%c3(c!I#94S 94!&%#`7G23-!&%%9*&*&D
O4)4!&;42394#K94S#O723-Pb%*&/+43(+5367*&2O%2> /23694*&%%2Z94:7*&!"%Q%9<.!&2#
P\3N#*&D!W#;+94O523P\3<#SPT35G23!&/O%D%2 %!W##*&D)9443(53(7*&2O%3(
!W#23(%!W#;CP3ff%:D!W#;K94`5G2#<!W*gP5G23!&#g>
*W#=|PT94O97*&!"%:5*ID%S94O#OPT94O!W#g>c-*W#%SK#=.53(%2#c94
#`7G23ffP%*&/.53(7*"2%aPY[2b2c#[ x #S c-%:!&#<_<|2,42#C,3r_
!W#;`*&#>ff-*W#O%94,N% :4#O723YP^%<*"/<.53(7*&2O%ffP-,42#O!&ff79!W#%T%:7;*
*W%%YP3(R{,4!&4<3(N;2#23a3(;3(!W#;/V3(!W#<T> N4!&%T/3(!W#<5G23P3O%T723
94#K/3(!W#<| x %S*&%C94NP\*&*&,N!W#;.%97%!&#>T-<*IC#+!&%95*W2D%c94S#O7G23-P %*&/
53(7*&2O%CPS$,c42#=!&7:!W#%7G_5=;<2#23M*&2%P 3(2<>?#M94
2%,-S*&,D%N25*&D+/3(!W#<P3;2#23!W#;%:7;**W%% x 3(2*&*94aN94S%*"!"#
P*&2C%O252#%#94K,N2D=4,R%97;a*T*W%%.<3(.;<2#23>{-*W#L~{;!&/%O94
#`7G23ffP%*&/+5367*&2O%-P5G!&!&//23(%!&#CP[2b2#!W#K3(23ffO%94,
94b3523!"/<53(/23!&% !I#K`4S3(5G,23P*94#N%!W5*&N5!&!&/N53*&*"*
53(/23>b !W#*&*&DZg!W#)*W#{O,-2#C]^#+94S#O723QP%<*"/<53(7*&2O%cP 3-523!&/
%D%2)>
2

fi

J J++uM'++'

+x+~+

!W#

-
-T
N?
Nz
Xff

X
zNz
zgc
@
zg


XT

k

[2b2




~

|


|

|

|





|
|




~

|
|



||2
<

5!&/ &* 2




||

|


|

||
~

}

~
}
}
|
|




~

|


|

|
|2



'z+++ ;+{

5!&!&/ 5G23!&/



~

|
|
|
|
|2~
|
|2
<

|
|2
<
|
~
}
|
|


}
}


<}


|
|2
||

b7*&.|dN?#;3!&#P5<_ ,#<7_51<553(4%+7DE523!&/B53(/236%2dM%*&/
436+53(7*&2O%
Q4S3(%9*&%3(/2*:4`4!&;4+5G2#<!W*P 3c553(a4KC%!&;#!0]2#*"D{!W53(/#%!W#;*&
5 3(/23(%>.O!&%S#*&D=7*&K%*&/K>}r P-94.53(7*&2O%,4!&42#7%<*"/<B7D5<_
23!"#gZ 2#{#*&D%*&/.~>0r)>S-5!&!&#P53(/236%!&%S/<23(D+%9%%(P*-72%
P-94O/23(D+!0F23(2#<`7G242/!&3aP9453(/23(%>S-S/2#K5G!&!&/)53(/23N#%!&%!W#;+P
`<#BM2#{#*&D)%<*"/<G|>r RaP9453(7*"2%%<*"/7*&.7<D)5G23!&#g>cN2#Z
523a!&#!&%-3(2*&*&D!W5369#<T!W#3(23 !W#23(2%c94N%9%%3> 42#O!W#;3!W#;S%:7<_
;a**W%%N!W#<)2c!&%-%<*"/7!&*&!"(D+3aN!&%N!W#23(2%+7<D.G>r} )>b? #K94O%-2%%%:7;*
*W%%O9ra.53(!I#M:4%23(4M53(%%`#M2#M4*I5M3(362394K%23(4!W#M)P /3<7*"
##23>+N4)%.aP*&2C%O!I#2362%%+|
%O523P\3<#7<D=>r >Q4.!W#23(2%.P
94%<*"/7!&*&!"(D=3P[=!&%3(2*&*&D)S233(!W#;)3(%36`3(!&#%>?#+*WO%*&*
2%% ,c423(Nc%97%9#<!W*%95_5C!&%T7:!W#`,--*&O]^#.536P,Q!":4N%:*&*&23T3(%3(>
N42#gZ94c*&2%<3(%+%ff5GZ!\>>94D)3(<7*"S*&%%97;<*&%-94T23NP\23
P\,$!W#<P23(2#%S<#),4% X1536P ,-*&+3(!I36`#<DK!I#<P\23(2#%2>
42#9r<!W#;M*&%23K*"r.:436%9*&%),-2<#36;#!&=94{P\*&*",N!W#;> 53(/23
,4!&41*W3(2D%94,N%=39423%9a!&%(Pa3(D=724/!&3!W#L{%95G!]C!W#M2#MP\2#B536]
P 3( 9423(%2>-5G23!&#2#{2#9a!"* :4<94234<3(=53(7*&2O%2#!&!&#*&*&DB7G`%*&/>
QV,-/23Z!P S53(/23Y!"%N#ff%9!&97*&cP\3T23(9!W#C!W#C942#O523a!&#,Q!"*&*#3C*&*"DK#
3(%9*&O!W#L{%!&;#!0]2#O!W#23(2%)PN!"%.523P\3C#>ff2%.PN94CP O94.[$#
N%94,$/<23(D!0F23(2#N7G242/!&3ff!W#K94%T2a%%NT*&2%-#53(/23Y2#.7G!W53(/
!W#+O2369!W#)C!W#g>


fi



wyz{

wyz{

?S!&%S!W#<23(%!I#;K]^#S,4:42323(9a!I#4323(!&%!&%OPN53(7*&2O%O*&2=)4!";4
3O*&,@5G23P3#{P:4+523a!&/+%D%2> +O!W#{,49423C94+4323(!&%!&%
9!W#M#<9!W#%Oa*"!&(k
<#
9{C!W#=#<9!W#%##<_Q3#M53(7*&2O% !I#oi^2#{94
523P3#>ff !I36%2Z,-%94*&=#:4N94%4<323(!&%!"%Q#Q5*&*&D+23O!W#
94)523P\3C#.aP94K5G23!&/K%D%2)>{N423(+3(C;!W#%OP:.!&2#DP3O*&*-r!W#%P
53(7*&2)Zb3(;36*"%%P94O(D5OP*W%%2336!I#;{!W#9453(7*&2O%2>O-,-`2<#a*"2a%
7%236/%O2#2#!"%>
!W3(%*&DZ,-c2#C7%23(/c94-94c523a!&#)553(a4O!&%N%:5!W*&*&D+,-*&*_ %:!")P\3-53(7<_
*&2O%#<9!W#!W#;=*&!&D>=Q47G%.3(%9*&%36.7:!W#L!I#M:4.C!W#%TZff Zb#
XTJ,4!&4+#9a!I#{#<D+53(7*&2O%,N!&94a*"!&(D> 42#<#*&D!W#;+53(P3#%c,-S2#]^#
(,-3(2a%#% P\3b94!&%2> ?#O%94O!W#%c2Q!&%7*&N%:553(c72%N!&4%-`4
%936#;23Q!I#<P\23(2#%P3c4#*&!W#;+*&!&D94<#B[T>OS2<#+aP2#{23(!&//
!0.2*&
*&2%O,N!&94P\,!W#<P\23(2#%2Zff!\>>*&2a%`,c4%K23(!&/V!&#M,-*&L3(!I36+#<D!W#<P\23(2#%
7<D=T>c2#)%:553(OS72<%S!&c!"%7*&O.<r93#%6P3!&#%-P 94
53(P ;*:4S2##523P3U72%`aP!&%Q]{3(23(!W#;)%{P3-%95235%!&!&#g>
N4!&%c2#K!W#23(2%94ai!W7!"*&!&(D+P 94S53(P %23(4)523P3O7<D[2b2>
#*"DZ,-T#%!&23 ,49423Y94ffPa94aTc!W##<9!W#%TO%*"DON3#3 ##<_Q3#
53(7*&2O%ff!W#oi^2#%N94N523P\3C#NPg94N523a!&#553(4g>b-#%!&23(!W#;O94N!W#%
,423(94523a!&#+55364K*&+%9%%6P*&*&D7G`55*&!&,2<#+#!&94-%9%%%
*&L7K79!W#P3N3# x >;[>WZNzO%,*&*-%`##<_Q3#!W#% x >;>ZXTN>b? #94
a!I#%b,423(N#43(O53(7*&2O%b*&7G-%*&/ x #!":423b%2#<!W*&*&D#3,Q!":4523a!&#
P\2#94)52362#9a;.aPQ##<_N3#=*W%%O!&%394234!&;4 x #94aS94%Ka!I#%O#
55G23O!W#94)97*&2>1Q4!W#L3(2a%#P\3O94!&%2Z-4,/<23Z<5523(%CB7{94O94{%!W#;*&
53(/23(%C%94,@,-2rM523P\3<#+!W#L94%+a!I#%>1 %:3(#;B3(*W!"#%94!W57G(,2#94
523P3#cP94N523a!&/536V/<23T#K94-P T,c49423-S53(7*&2!&%NN3#3ff##<_Q3#
*&#<3(2*&*&D+7P#{!W#)94523(!WO2#<%2>
fifi gn4"uO&xgoc,YL2[j-'-oNnY


@c5K#V,E,-#*&DK#%!&23(+:4S#O723P%*&/+5367*&2O%2>T?#+!&!&#gZ!&N!&%N!W#<23(%!I#;
)#*&DS,49423c:4`%OP %97;a**W%%3-*&2%2#%95_594`53(PT%2<3(4!W#
;2#23a*ZY!>>a*"%+P\3O53(7*&2O%O94a`2<#B7G)%*&/L7D=%!I#;<*"+536V/<23(%2>43(3#!WO%3(
%95G!Ia*"*&D!IC53(:#O!0P943(253(/23(%O3()%L,N!&94!W#L!W#<23!"/<+53(/232#</!W3(#O2#%2>
O3(%93(!&N3(%*&/%c94943(a!I#%-TZgNz Z[#zgc#+36S;!W#;.#*&D
94S3#<!WO%c!W#+O369!&*\>
b7*&Q53(%2#<%94-3#!I%,42#9r*&!W#;43(S53(7*&2O%bP94T:43( %!W#%2>
O!&`*&*53(7*&2O% :4*"#!&9423ff7-%*&/7<DSN%!W#;*&536V/<23,42#,-3r<!W#;a*"#Z#3
7<D`#<DSPg94-523!"#O/V36!I<#%>-*W#|-P^94-9<7*"Q!"%:5*ID%T94N#NP945367*&2)>
-*W#%c`<#+53(%2#<-94S3#<!WO%NP[2b2#= x #K c-%9a!&#<_G|2
,42#O,-3r!W#;S*&#Z<*W#%TS#Kc943#<!WO% P 2-,c42#O!&T79a!I#%ff%97;a**W%%
P 3(M,4!&436S;2#23a=3(;3(!W#;K/V36!I<#%O|O#=Z^36%95!&/*&D> 94c94
3#<!WO%-!W#*I94c;2#23!&#K#%*"!"#)!WOcP%97;a**W%%2Z#K94c93#%:O!&%%!&#
L>B*W#L~!&%95*WD%:4+3#!I)P[E!0PS!"C79!W#%7_5M;2#23
*&2%CP 3(2<>1? #942a%),-+*&,D%25*&D1/3(!W#<.{P\3O;2#23!W#;B%:7;*
*W%%2>cc*&%)94%O3#!I%!I#*WK94`536253(%%!W#;)P2O#9493#%:O!&%%!&##

Ih

fi

B OQ

J J++uM'++'

+x+~+

4+Q

4GGGGG

4

4GGGGG

4

4GGGG
4GGGGG

r

dDBDQD4Q



'z+++ ;+{


OQtt

Q|rQOO

4



4

4

4


4

4

4





4

4



4




4

















x



4




4
















x


44



4













4GGGGG






4G G




4G GG




4GG

4

4GG G



QG rDBO4

GGG













GGG G

4







4





GGG 4

4

4











GGGGGG








4


4

GGGGG








44



44

GGG4G







4

4



GGGGG



x





44





4




4

GGGGG




4

GGGGGG













GGGGG4

4




4


4

4

GG G













GGG G













GGQG

4







4

4



GG GG






4



4

GGGGGG











4

GGGGGG







4

4

4

GGGGG4













GGGGGG








4


44

GGGGGG













GGGGGG


4










GGG4







4

4

4

GGGGG


4






4



GGGGG







4 4



4 4

44



4

GGGGG


4




4

GG4r







44



44

GG4GG








44



44

GG4GG













GG4G4








4


4

GGGGG







4



4

GGGGG




4

4




4

GGG


4










4


4


4




b7*&`d ? #<;3!&#KP 5<_ ,#7G_5.<553(4%7<D.523!&/`536V/<23(%2d 3#<!WO%



fi



wyz{

wyz{

!W#;3!&#{PT:4`*&2C%2>S-<*IC#K;<!"/<%943#<!WOOPNC5!&!&//<23(%!&#{P
#M x !I#!WOP 943#<!WO%aPT*W#%S.<#>- !W#*&*&DZ!W#{*W#C,2#
]^#)943#<!WOP3ff523!&/%D%2 x O!W#!WORP94S3#<!WO%-P<*IC#%O#+~>
N42#<93(
Q+.2#%N94-94S5367*&2U*&=#N7%*&/{,N!&94!W#=|2<S%#%2>
!W#S*&*ga!I#%-#<9!W#Ca*"!&(DK9436%9*&%N3(723ff94<#943(%9*&%-/23Y94,c4*&
>3Q523a!&/S53(/23ff2#K%*&/Sa*"**"!&%53(7*&2O%Z,42362%2!"%Q#*&D+7*&S
%*&/K>r Z#*&D>r >C5!&!&/)53(/23Q#%!&%!W#;P#1
2#=23(*&D%*&/~>r aP9453(7*&2O%> #*"D94C%9%%`37`*&%):4.3#<!WO%
3(Q*"2<3(*&D.!W53(/C,42#.%!I#;K523a!&/536V/<23>N43#<!WO%N3(NaP2#23(2%)7D
%97%:#!Ia*bP3(% x !W#%95!&OP:4OPa94aS3##!W#;+94O(,-53(/236%!I#53*&*&*T#%:O%
(,N!&S%O4K9* @$!I2>
42#O%9D!W#;943#<!WO%N#.53(P\% 79!W#)7DK{,-N2#O7%23(/N94P*&*&,-_
!W#;>S?PT%95_5%OPL<3(3(2*&*&D+OK2336!I#;3(%36`3(!&#%Z!W#*WO%*&*
2%%N%97%9#<!W*[%95_5K!&%N7:!W#>TO!WO%6'1P\3b!W#%9#!W#C94Qz+!W#<',-
4/N94Q%!&9!&#C,423(#O3(%363(!&#K9<r%T5*W7-3(3(236!I#;O:F%-*&*",]^#_
!W#;536P\%QP%23>-? #):4!"%%!":!&#gZ[94S%95G_5%S3(*",S>-zgS%c9rS*"<%23c*&r)aN94
3#<!WO%P,42#%!W#;)%97;*b*W%%2> 42#{#%!&23(!W#;+:4`3(%9*&%aPT/V3(!W#<|Z
9436%9*&%T%94,L94aTS#!&/<#.#!W#<P3O.;<2#23!&#P%97;**W%%%:*&*"DC%-#
2#<9!&*`4O;a!I#g>ffZ#*&Dr P945367*&2O%N2#7G%*&/)%!W#;94!&%-/V36!I<#2
> T36!I<#NZ
4,-/23Z%94,N%-!&OO%9!"%6P3(D.7G242/!&3>TQ2#Zg#K!W#*&*&!&;2#<;2#23!&#KaPYO%:7;*
*W%O5*3(2*&*&D)%%:3(#;*&DK!I#oi^2#O94:K!"2#D>
=fix
'xfi


? #<;3!&#P5<_ ,#O#.7G_5O53(/23(%T7<DS25*&D!W#;O523a!&#O!&%T/236D`53(!"%!I#;
!W#=:4]*&MPa=!&#g>)SK{23(9!W#%:3(2#;94%O#B,-2r#%%%P53(/23(%
P\*&*",N!W#;!0F23(2#<53a!";O%2Z4#!&%c94-936DOO7!W#S94%:3(2#;94%N7<D.523!&#
2#{*&*",#!W53(/2O2#<aPT94S!&/C%D%2)>3c553(4PTO7!W#!W#;)5<_ ,#
#=7<_553(/236%7<D53(%%!W#;{5<_ ,#);<2#23%97;**W%%S!W#=K7_5
53(/234!&/%S:4!"%`7!W#!"#B7<D+!W#<93(!I#;;*0_ 3(!&2#:!&#{!W#+{7_553(/23
94%O7!W#!I#;{%93(#;3(#<#D#936*O4#!&%9O%O#;*0_ !W3(=%23(4g>SN4%
P7<_5;2#23K*&2%-!W#K5<_ ,#C53(/23 2##:3(!W7S%!&;#!0]2#<*&D36
53(P *"2#;<94%%:4K94a53(P%-2#{7P#),Q!":4)%9a*"*&23c3(%36%2>
c*W)553(a4%ffP\3 %955G3(!W#;5<_ ,#.7<D.7G`_5O!W#<P23(2#O*&%!W#*&Da!I
c25*&D!W#;7G_5)23(2a*"2C%c!I#5<_ V,c#+53(/23>!WO!&*W3.3cO94=7D
4<## x |2}}a<#^4% x |}}Z<|2}}}*&2%-3(N2362O!W#.53(2536%%!I#;54%#
94-!W#5T*W%%-3(N;2#7<DS94%-P3O*Ia%2>N4Na!I#!F23(2#QP^94%<553(4%
#3c553(a4{!&%94r!W#=aPT94%=*&2%>Q423(Zg94 XE!W#<P23(2#KO4<#!"%:!"%
%{!I#3(23-;2#23*"2C%2>-N4!&%4a%N94`a/V#<9;:4-!I#%O2%%('E!W#K#:3%
3ff4#!&:'A53(P*&2#;:43ff3(%363(!"#%3(N;3#<> N,-/23Z<94c*"2C
O4#!&%9O%T%7<DSg4% x |}}7gZ|}}Z<|2}}};2#23ff!&7
2%kD *&2C%2> N2#Z:4!I3
5<2#!Ia*,S>3>2>94%!&SPb94`3(%3(S3(!&#!&%*&!WO!&>
c:423S553(4%O936D+D#O!&2*&*&DB23(2a#!&O*&2%O3(!W#;=:4.53(P3#PN94
XU53(/23 x c%934# !"r*\ZO|2}}G?,N#CZS|2}<}c%934#Ezg/*W#ZO|2}}>


fi

J J++uM'++'

+x+~+



'z+++ ;+{

QP23Q24{%9%%(P * %*W!&#{PN%97;<* *&2)!";4S7G`;<2#23=#=K94
!W#5*W%%2>=Q4+!WaP94!&%r!W#LP*&2C+;<2#23!&#M!&%O=53()*"2C%O94O3(
7*&K3(K94K%23(4=#O7D*&!WO!W#!W#;L3(252B%97<_ !"#%2>#K23(!&!&!&%9
3(;<3(!W#;94!&%r<!W#+Pb*"2C;<2#23!&#K!&%N94Pa:4-!"c!&%#*&23Q,49423Q3-#%:P *
*&2%K2#7+;<2#23>EN423({!"%)#M;3#<K94*&2C%.2<#1753(36!I#;
9453(P3#,4!&42#{#:3(!W7))53(P6Zg!\>>g,c4!"42#7(
936:_%j >^3(9423O3(Z
%*W3(2DO2#!"#Z94O;2#23a*"2C%S3(O%9*&*&D=#S%c;2#23* %S5<%%!W7*&
!W#%9#<!W!&#%CO!W#;=P 3(94)%*W!&#%P%97;a*"%K53(/!&%*"DL%*&/>1N4!&%2#L36
94S55*&!&27!&*&!&D{PTS*&2C*&94;4K:4
;2#23*&!&j )53(aP*&7GS3(:_%KP\3N3(:P !W#;
94{!I#5)*W%%2>N4%2ZN*&94;4L%O43653(7*&2O%)*"E#*&D17G%*&/1,N!&94%94
*&2K4#!&% x %`c%934<#+Rzg/*W#ZT|}}Zg#)%9<7*"O%9%%`4%S7G2#=3(253(
/23O{*I<3(;)%OP5367*&2O%2>LN4+!W#M!&%9/#<9;%aP*&*553(4%O,4!&4L#*&DL!W
N%:553(!I#;K5<_ ,#)536V/<23(%-3(!&;!W#P3(R94cP N94Q!I#%O!W#%2Z%95G!W*&*"D!0P
*&!"(D{!"%!W#</*&/Zg%95G235<%!&!&#<_7%B536V/<23(%N*&23(*&D95G23P3 XE53(/23(%2>NN4%2Z[!W#
%94{!W#%c!"SC2D)7O3(S%2#%!I7*&./*&5{4#!&%!W#+3(23N.%955G3(c94`3(
5,-23P *7_5)53(/23-:4#K94,-2r23-5<_ ,#.536V/<23>
?#O3(23b!IC53(/7<_5C53(aP^%2364O7<D%!I#;5<_ ,#5G23P\3O!W#<P\23(2#%-94
P\*&*",N!W#;553(4%T4/c72#C25*&VD<> !W3(%*&DZa;!W#O#536V/<23 x 94N7G`_553(/23
!&%%%!&%L7<D=*W%%23(!&/MP 3(#942353(/23 x 94.5<_ ,#=53(/23>+N4)553(4
P 3(U*&!0F x |}}-%%N*&2C%;<2#23+7<D;!&*&!W#23c!&#%D%2 x <#+#
%97;**W%%9N!W#3(23N.%955G3(3(%*W!&#<_7a%B53(/23(%>-S`.94*WrPT;a*3(!0_
2#<9!&# x %O%23(!W7J!I#E!&#>94!&%O:4*&#KD!&*&E#</!W#!W#;13(%9*&%K!W#
53!">ff#*&DZ94236S3(553(4%N<r7G_5.536V/<23(%NO3(c;*0_ !W3(7D
P\3(!W#;.:42R,-3r#*&D),N!&94)%OS3(*&/V#<c*I<%%,c4!"43(7<D.5<_ V,c#K2*0_
2*W!&#%2>ffN494%-%236!I7G7<D<#!"*W4#N*\> x |2}<~Z!&r* x |2}}aZ#)a%;,N
*\> x |2}}Y93#%6P3%NP*W%%!W#<.#:423-*W%S%N,4!&4!&%942#K/*W{!I#{
7<_5<##23>N4%95G!0])93#%(P\3C!&#+536V/!&%`KO7!W#!&#PN5<_ ,##
7<_5C53(%%!W#;.#)53#%-947G_5O/Va*Ia!&#K`36*"/#<*W%% x ,4!&4.7G23
C##!&#)K53(aP;a*I>N4%2Zg7</!&%*"D{947_5{53(aP%2364{7O%S3(
;a* 3(!&2#>)*&%{94)O94=%23(!W7E7D=zg/*W#M`a*> x |2}}<53(/!&%+36*"/#D
%!I#;P3C7_5M2*&2*W!&#%2>$-%1#M5<_ V,c#L53(aP25%O943(*&/V#DLP
)*W%.!&%SD#O!&2*&*&DM23O!W#M3(!W#;:47G`_5+2a*"2*W!"#g>+? ##<93%)3
O94)!W#)94%`553(4%Q94S7_5)53(/23-4%Q9r<*":4,4*&`53(P:%9r[>T3
553(4P\3%!I#;L5<_ ,#M;2#231%97;*N*W%%K!W#7_5L53(/23O%)#
53(/!")+36*"/#D=%!W#;aP7<_5!W#<P2362#7O%955G3(%94.7<_5!I#<P\23(2#
53(%%7<DO%!W5*&!0PD!W#;.94Q3(!&;!W#*;<*\> N4%Z94536P*&2#;94KD`7G%:43(2#>b^369423_
O36Z53(%TP94N%23(4%95aNP947_553(/23Z,4!&4#<9!W#36*"/#<N*W%%N7
D+7O!0.2*&S.2#O23Z2#7G`:3/23(%+7<DC%!W#;*&!W#<P2362#,4!&453(/!&%*I<3(;
%2364)3(!"#%2>
9<


##!"gZ`>ZV$z#;25GZXQ> x |2}}a>55*&D!W#;.zc_3(%*W!&#OSc*Ia%%TP##<_43#*"<;!&
53(;3<O%2> Npouo&tw\qulfN>SwYqt Z x <Z|4a>


fi



wyz{

wyz{

c%934#gZ-O>WZTzg/*W#Z-> x |2}<}|2> XTNXffcgdffN!&;4523P\3<#+9436253(/23(%
%!W#;O**&!WO!W#!&#g>?#pw\fsOnw\t:j-t9nfl[qul%ff9nX|Tqvlfl[fTfN>)f2fjX|cNo"t:jf2t>
~ *I<,-23-c22O!& 7*&!&%9423(%2>
c%934#gZO>WZzg/*W#Z> x |2}}>N4S%P *"2C%N!W#):4`O**&!WO!W#!&#53(:_
3(
> fp:ln<ofN> pw\fsOnw\t:jt:nflqvl<Z
x |2Z^|<|2^|[|>
c%934#gZO>WZJ!&r*\Z > x |2}}>Q4!W#;S#C*"2C!&!W#;`!W#*[*"!WO!W#!"#943(2
53(/23(%2> ? # (f2m:t9t:jqvlcfN>fiff. p%Ti Z55g>aG>53(!W#;23Z[z ?N~<>
-4!W3Z-z >WZT #!W#;23ZN> x |}}>,c3(!&:_7%!"#*N9436253(/!W#;M,N!&94
%*&!&#+#{%!W5*&!0]2!&#g>fp:ln<ofN> f:qvmSnl[jffTf<sNhgpw\nw\qvfl[Z x Z|G>
-#!&*W4#gZ N>WZ !&23Zg>WZa;!&/^Z`>WZb @Q*"*W<##gZ[> x |2}~<> ;!&%%S#=9423c%93#;
,D%+!W5*&2O2#<*&;!&+53(<;3O%>.? # (f2m:t9t:jqvlOfN>wYq/ff ci ffe
fi|sQhfqupsUfl 9qvlmqh^o&tfN
> nw\nn
fi|w\t2sZ55g>|4^|2>
ff#!W#Z > x |2}}~<>.#94)3(#%93!&#P536P\%S!W#!&%93(!W7L943(253(/!W#;dO
O!0]{*I<%:_ !0F%!"#94>fp9l[nogfN>fi|sf<o"qvmfffffsQh^pw\n<w\qufl[Z x Z<G>
ff#!W#Z >WZUQ%!W#;Z > x |2}}<>N4*W%:_ !0F%!&#LO:4*&;<D+P\3!"%93(!W7M:_
!&#g> pl[jnsOt2l[w\n l> f9sOnw\qvm:nt2Z Z|2>
-#36DZg>XN>Z 2? #<%94gZG>![>WZ D23Z`>O> x |2}<}>bcXTgd!&%93(!W7=
3(2%#!W#;%D%2)> ? # (f2m:t9t:jqvlcfN>pp #" Z55g>>
2#!W#;23Z$[> x |2}}> ~ #V,Q*";<:_7%+!&%:3(!W7)%23(4)%!W#;.-2<S,-3r>g? # fm9t:t:jqul
fN> ff[i Z55g>G|4><c?v_ 36%%2>
2#!W#;23Z%>WZA^4%2Z> x |}}>XT#4#!W#;O#</2#<!&#*%2364O%D%2O% ,N!&94O*&!0_;2#<
4#!&%2dT2%%9D>?# (f2m:t9t:jqvlaf> ffi #& Z55g>[|2}<>? XffXTXL-C523
!&(D>
2#!W#;23Z#>Z2Lg4%2Z > x |2}}>a*3(!&2#<S!&#*<943(2H53(/!W#;>?# fm9t:t:jqul
fN
> ' Z55g> [>53(!W#;23Zgz ?-~G|>
2#!W#;23Z(>WZ^4%2Z >WZVg4%2Z > x |2}}<>Q!";4`5G23P\3#ffT %D%2%7DO7!W#!I#;
%/23*g?-O94%2>T? # (f2m:t9t:jqvlcfN> )ff. fi* $+ Z55g>|^|2<>
!&!W#;Z > x |2}<}~> qvwvi,N(jt f:qvmSnl[j_pw\fsOnw\t:j=eqt:f(t2s (f2qvl<> 53(!W#;23>
^4%2Zb> x |2}<}>5G23!&#PN5<_ ,#=#M7_5943(253(/236%`7<D%:7;*
*I<%S93#%(P\23> ? # (f2m:t9t:jqvlNfN>q ff #& Z55g>|2^|2~}>53(!W#;23Zgz ?c|<~>
^4%2Zc> x |2}}7>-5*&!W#;E%993a!&#<_7%536V/<23(%+7DE4<#;!W#;5<%!&!&/2#;!"/<
!I#<P\3C!&#g>T?# (f2m:t9t:jqvlafN>eji -& Z55g>|2|>53(!W#;23Zgz Q|2}>
^4%2ZQ> x |}}2>Ec!I362O2#6_7%$523!&/943(253(/!I#;[> ?# (f2m:t9t:jqvlafN>
% -& Z55g>|2}g|2>53(!W#;23Zgz ?c|}G>


fi

J J++uM'++'

+x+~+



'z+++ ;+{

^4%2Z > x |2}}<> *"!W7*&+53(aPu_3(25*WD,N!&94M423(!&%!&%2>)? # (f2m:t:t9jqvlaSfN>,%
| ^|2>53(!W#;23Zgz ?c|2<>
4

* $+ Z 55g>

^4%2Z > x |}}>c*&/V#D<_7%H*&2L%*"!"#JP3)**"!WO!W#!"#%!W#;1*WD
97*&2K2#O23!&#g>-? # (f2m:t9t:jqvlNfN>D%.ff. #& Z55g>^a~<>#4# !&*&D#%2Z
zg>
^4%2Z > x |2}}<7>gD%27%:32db!I!"*W36!"(D<_7%{*"2C;2#23!&#P\3 O*^*&!WO!W#_
!&#g>T? # (f2m:t:t9jqvlaNfN>fiff. p%Ti Z55g>>53(!W#;23Zgz ?c||<>
^4%2Z > x |2}}}<>)zg2{;2#23a!&#P\3O*ff*&!I!I#a!&#17<D=O7!W#!W#;=5<_ ,#M#
7_5K!W#<P2362#>?# (fm9t:t9j<qulfN> /ff.
> 3(;# ~ <P ##gZ.552<3>
c33(!&%#gZ#[> x |}}~>5!I!"!W#;53(P%2364!W#OO**&!WO!W#!&#g>? # fm9t:t:jqul fN>ff. #%ffi
# 0 Z55g>G|2<>53(!W#;23Z^z ?|<|2>
c%;,NZff`>WZ-? #Z ~ >ZN49GZt`>WZN ~ %:4!IO3GZ > x |2}<}> #<_43#;!&%%K
!I#353K5<_ ,#!W#<P\23(2#)!W#=7G`_5=943(253(/!W#;>?# (f2m:t:t9jqvla`fN>
ff. p%Ti Z55g>|2~ ^|}>536!I#;<23Zz ?|2}>
?\,N#Z ~ > x |2}<}>Kzg24!W#;{P3S N _7%M5<_ ,#943(253(/23>)?#
(f2m:t9t:jqvlaf>fiff. #%ffi GZ55g>|~^|2~>53(!W#;23Zgz ?c|2}>
~ 3PZ`>XN> x |2}<>[2594<_\]^3(%-!&23a!&/:_ 252#!W#;d-#K5!W*!"%%!W7*"93(c%23(4g>. Z
+ Z}|}> X *&%/!&23 7*&!&%9423(%Sc>dO> x 3694<_N<*"*W#>

zgZG`>WZ 2D3Z ~ >WZ*&*&23Z> x |2}}a>-#<93(<*"*&)!W#<;3!"#Pb942N3*&S!W#`##:_
!&#K97*&22*&2*"!\>fp9l[nofN>ppw\fsOnw\t:jt:nflqvl<Z -0 Z}>
zgZffO>WZ 4##gZ[>WZT-D23(*\Zg>Z ff!W7*\Z > x |2}}<>XTNcXTOdb 4!";4<_523P\3<#
943(2U536V/<23
> fp:lnofp
> pw\f<s`n<w\t9j-t9nfl[qulZ & x Z|2|2>
zg/*W#Zff> x |2}~<> 4<#!"2a*N943(2`_53(/!W#;B7<DBO*-*"!WO!W#!"#g>1f<p:lno-fN>+wYqt
ffZ x >
zg/*W#Z> x |2}<>.pw\fsOnw\t:j=eqt:f(t2s (f2qvl Nn f9qvm:no[naq> 3(94<_Q*&*W#>
zg/*W#Z>WZ<Z<>WZ !&*&%#gZ> x |2}}>TNN cXQdTNN J,N!&94CXY*&/V#Dg>
fp:lnofp> pw\f<s`n<w\t9j-t9nfl[qulZ Z<G|>
3r/!&4gZg>WZ2Z > x |2}}<>? #<P\3C!&#]*"23(!W#;d*&!&#{O4#!&%9O%N!W#)*&23#!I#;
%D%2O%2
> n<mqqvlt t9n<9l[qulZ -" x <Zg||2^||>
!W##gZg> x |2}}<>!2#<!&9!"/<36%9*&%T#23#!I#;94N!&*&!"(DOPg5*W#!"#<_7%*&23#!W#;>
9w\q 3mqun<o lw\t2ovo&q"<t2l[m9tZ
Z~<}|<>X *&%/!&23!&2# 7*&!"%:423(%2>
%23Z >WZ?72#%2ZO>ZzZ`>WZ!W#7a4gZ$[>WZ[*&*&23Z>WZ4##gZ$[>WZ D3Z ~ > x |2}}>
N4O**&!WO!W#!&#$53(/236%XffNXff#X_XTNcXTO>4fp:ln<ofN>pw\fsOnw\t9j
t:nf<lqvlZ # & x >
2

fi



wyz{

wyz{

c7!W#%#gZ%> x |2}~<>C4!W#36!"2#<K*&;!&S7%K#C9436%*W!&#K53(!W#!I5*&>5f<p:lnogfN>
wYqtqffZ x |2>
4<##gZ> x |2}}>L*&9_=7<_553(253(%%3P\3O5<_ ,#943(2 53(/23(%2>
%D%2R7%93a2> ? # (f2m:t9t:jqvlcfN>ff. #%ffi Z[55g>>53(!W#;23Zgz ?-|>
!&r*\Z > x |2}<>R53(*&;)4#*&;D94362 53(/23d?5*&2O2#<9!&#=7<D=#{2#
53(*&;C5!"*&23>5fp:ln<ofN> pw\fsOnw\t:j6t:nf<lqvlZ$Z[<>
!&r*\Z > x |2}}>l@c5%!&:_ V,c#1O9V_ !I#<235369!&#P:4O*N*&!WO!W#!&#E943(2_
53(/!W#;S53(3(QP3b!"#)#.7G!&#g> f<p:lno[fN>pw\fsOnw\t:jt:nflqvl<Z -0 Z
|2} G|2>
*&!0FZO> x |2}}<>E423(;2#%53a*"*&*g!&#C%D%2)>?# (f2m:t9t:jqvl-fN>
)fK}_qf:h7 0 >


ff *

*&!0FZ`>ZV9#23Z> x |2}<}>N4N 53(7*&2H*&!W733(Dgd 3(*&2%-/g|>>W|<>%fp:lno
fNp
> pw\fsOnw\t9
j -t9nafl[qulZ Z|2<G>
*&!0FZO>WZ9#23Z^> x |2}}<>N4O3(%9*&%cP 94S-cSXb_9|2OT %D%2R5!&!&#g>
fp:lnofp> pw\f<s`n<w\t9
j -t9nfl[qulZ #& x Z|~>
*&!0FZO>WZ9#23Z>WZ 2O2#!&%2Z> x |2}}a>N4N 53(7*&2*&!I733(D>g? # fm9t:t:jqul
fN
> ff. #%ffi Z[55g>~~>z ?-|>
bO2Z> x |2}}<> #*0P6>8fp:lnof>
pw\fsOnw\t:j-t9nfl[qulZ #& x Zg|2}}>
!&2#74gZ->WZffZ >ZTUcr[Zff`> x 2| }}~>5a%%U *&23 236%!&#BG>>+?# (f2m
ff. p%Ti # 0 Z55g>|4| ^|>536!I#;<23Zz ?||2>
*0PZSO>WZ^4%2Z > x |2}}>-523a!&/5<3*&*&*.C$943(2 53(/!W#;>?#
4#2r2#73(;23Z>Z *"*W#23ZcO> x XY%2>Z a|Vl[nsOqum f2n<j9 qw\9q\pw\quf<lx>f n(nouo&to
-hhgo"qvm:nw\quf<lZ55g>b|2
} ^|>27#23>



fiJournal Artificial Intelligence Research 10 (1999) 353-373

Submitted 8/98; published 5/99

\Squeaky Wheel" Optimization
David E. Joslin

david joslin@i2.com

i2 Technologies
909 E. Las Colinas Blvd.
Irving, TX 75039

David P. Clements

Computational Intelligence Research Laboratory
1269 University Oregon
Eugene, 97403-1269

clements@cirl.uoregon.edu

Abstract

describe general approach optimization term \Squeaky Wheel" Optimization (SWO). SWO, greedy algorithm used construct solution
analyzed find trouble spots, i.e., elements, that, improved, likely
improve objective function score. results analysis used generate
new priorities determine order greedy algorithm constructs next
solution. Construct/Analyze/Prioritize cycle continues limit reached,
acceptable solution found.
SWO viewed operating two search spaces: solutions prioritizations.
Successive solutions indirectly related, via re-prioritization results
analyzing prior solution. Similarly, successive prioritizations generated constructing analyzing solutions. \coupled search" interesting properties,
discuss.
report encouraging experimental results two domains, scheduling problems
arise fiber-optic cable manufacturing, graph coloring problems. fact
domains different supports claim SWO general technique optimization.

1. Overview
describe general approach optimization term \Squeaky Wheel" Optimization (SWO) (Joslin & Clements, 1998). core SWO Construct/Analyze/Prioritize
cycle, illustrated Figure 1. solution constructed greedy algorithm, making decisions order determined priorities assigned elements problem.
solution analyzed find elements problem \trouble makers."
priorities trouble makers increased, causing greedy constructor deal
sooner next iteration. cycle repeats termination condition
occurs.
iteration, analyzer determines elements problem causing
trouble current solution, prioritizer ensures constructor
gives attention elements next iteration. (\The squeaky wheel gets
grease.") construction, analysis prioritization terms elements

c 1999 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiJoslin & Clements

Analyzer
Blame

Solution

Constructor

Priorities

Prioritizer

Figure 1: Construct/Analyze/Prioritize cycle
define problem domain. scheduling domain, example, elements might
tasks. graph coloring, elements might nodes colored.
three main components SWO are:

Constructor. Given sequence problem elements, constructor generates solution

using greedy algorithm, backtracking. sequence determines order
decisions made, thought \strategy" \recipe"
constructing new solution. (This \solution" may violate hard constraints.)

Analyzer. analyzer assigns numeric \blame" factor problem elements

contribute aws current solution. example, minimizing lateness
scheduling problem one objectives, blame would assigned late
tasks.
key principle behind SWO solutions reveal problem structure. analyzing solution, often identify elements solution work well,
elements work poorly. resource used full capacity, example, may
represent bottleneck. information problem structure local,
may apply part search space currently examination, may
useful determining search go next.
Prioritizer. prioritizer uses blame factors assigned analyzer modify
previous sequence problem elements. Elements received blame moved
toward front sequence. higher blame, element
moved.
priority sequence plays key role SWO. dicult problem element moves
forward sequence handled sooner constructor. also tends handled
better, thus decreasing blame factor. Dicult elements rise rapidly place
sequence handled well. there, blame assigned drops,
causing slowly sink sequence parts problem
handled well given increased priority. Eventually, dicult elements sink back
point longer handled well, causing receive higher blame
move forward sequence again. Elements always easy handle sink
end sequence stay there.
354

fi\Squeaky Wheel" Optimization

Iteration: 1 Priorities: C,A,B
Late: (20), B (30)

0

10

20

C

Iteration: 2 Priorities: B,A,C
Late: (20), C (10)

0

10



20

B
Iteration: 3 Priorities: A,C,B
Late: B (30)

0

10



30



C

50

B

30

20

40

40

Task

B
C

Duration Deadline
10
10
20
20
40
20

50

C
30

40

50

B

Figure 2: Simple example
illustrate SWO cycle, consider simplified scheduling example. Suppose
single production line, three tasks schedule, A, B C . one task
performed time. Execution starts = 0. duration deadline task
shown Figure 2. objective minimize number late tasks. optimal
solution one late task.
Suppose initial priority sequence hC; A; B i, constructor schedules tasks
order, earliest possible time. resulting schedule two late tasks (B
A). Suppose analyzer assigns one point \blame" late task,
unit time late. case, A, B , C receive 20, 30 0 units blame,
respectively. Figure 2 shows prioritization, schedule constructor builds
prioritization, late tasks blame assigned each.
next cycle, prioritizer must take previous priority sequence,
blame assigned analyzer, generate new priority sequence. simple prioritizer
might sort tasks numeric blame descending order, resulting new
priority sequence hB; A; C i.
second cycle, tasks C late, scoring 20 10 points blame,
respectively. new priority sequence hA; C; B i.
third solution, constructed priority sequence, one late task, B ,
receives 30 points blame. point optimal solution. continue
running SWO, however, might expect since typically know
reached optimality, SWO attempt fix wrong current solution.
Here, since task B late, priority would increased, resulting solution would
fix problem expense others. (We would also enter short cycle, alternating
last two schedules. address introducing randomization
prioritizer.)
Although example highly simplified, would clearly better
sophisticated ways implement three modules, Figure 3 shows behavior
illustrated simple example ected real domain. figure shows changing
position priority sequence three tasks scheduling domain described
detail following section. One task (\Job 24") starts high priority,
remains relatively high priority level. see task scheduled
effectively, therefore receives little blame, priority tends drop,
355

fiPriority

(high)

Joslin & Clements

(low)

Job 24
Job 26
Job 39
1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20
Iteration

Figure 3: Examples priority changes time
drop far ceases scheduled well, acquires significant level
blame, moves quickly back higher priority.
two tasks shown figure behave quite differently. One task (\Job 39")
starts relatively high priority, task \easy" schedule, little
blame, even scheduled late sequence. successive iterations,
priority task tend decrease steadily. task illustrated (\Job
26") opposite, starting low priority moving fairly steadily toward
high priority.
following section discusses characteristics SWO make effective technique optimization. discuss implementations SWO scheduling
graph coloring problems. final sections discuss related work, describe directions
future research, summarize findings.

2. Key ideas

experimental results show, SWO general approach optimization.
section, explore insights makes SWO effective.
useful think SWO searching two coupled spaces, illustrated Figure 4.
One search space familiar solution space, priority space. Moves
solution space made indirectly, via re-prioritization results analyzing
prior solution. Similarly, successive prioritizations generated constructing
analyzing solution, using blame results analysis modify
previous prioritization.
point solution space represents potential solution problem,
corresponding point priority space, derived analyzing solution, attempt
capture information structure search space vicinity solution.
SWO constructs new solution scratch, priorities thought providing
356

fi\Squeaky Wheel" Optimization

Construct

p
Analyze/
Prioritize

p



Construct


Priority space
Solution space

Figure 4: Coupled search spaces
information pitfalls common current region solution space. elements solution tended sources diculty number iterations,
increasing priority makes likely constructor handle elements
good way.
One consequence coupled search spaces small change sequence
elements generated prioritizer may correspond large change corresponding
solution generated constructor, compared solution previous iteration. Moving element forward sequence significantly change state
resulting solution. addition, elements occur sequence must
accommodate element's state. example, scheduling domain, moving task
earlier priority sequence may allow placed different manufacturing line,
thus possibly changing mix jobs run line, line
scheduled previous iteration. One small change consequences
element follows it, lower-priority tasks \fill gaps" left
higher-priority tasks scheduled.
result large move \coherent" sense similar
might expect moving higher priority task, propagating effects
change moving lower priority tasks needed. single move may correspond
large number moves search algorithm looks local changes solution,
may thus dicult algorithm find.
fact SWO makes large moves search spaces one obvious difference
SWO traditional local search techniques, WSAT (Selman, Kautz, & Cohen,
1993). Another difference SWO, moves never selected based effect
objective function. Instead, unlike hillclimbing techniques, move made
response \trouble spots" found current solution. resulting move may
uphill, move always motivated trouble spots.
357

fiJoslin & Clements

priority space \local optima" elements solution
assigned equal blame. SWO tends avoid getting trapped local optima, analysis
prioritization always (in practice) suggest changes sequence, thus changing
solution generated next iteration. guarantee SWO
become trapped small cycle, however. implementations introduced
small amounts randomness basic cycle. also restart SWO periodically
new initial sequence.
Another aspect local search typically point solution space associated single value, objective function score solution. talk
hillclimbing, generally refer \terrain" described objective function score,
space solutions. process analysis SWO thought synthesizing
complex description terrain, breaking solution component
elements assigning score each. Prioritization translates analysis
\strategy" constructor use generate next solution.
Assigning scores individual elements solution allows SWO take advantage
fact real problems often combine elements dicult get right,
plus others easy. scheduling problems presented below, tasks
assigned production lines, others allow much exibility.
due dates close release time, others lot leeway. sometimes
possible identify \dicult" elements problem static analysis, interactions
complex, elements causing diculty one part search space may
trouble another. Rather trying identify elements globally
dicult analyzing entire problem, SWO analyzes individual solutions order find
elements locally dicult. Globally dicult elements tend identified time,
dicult across large parts search space.
assigning blame adjusting priorities based identified problems actual solutions, SWO avoids dependence complex, domain dependent heuristics. belief
independence particularly important complex domains even best
heuristics miss key interactions therefore inhibit search exploring
good areas heuristic incorrectly labels unpromising. SWO uses actual solutions
discover areas search space promising not.

3. SWO scheduling
section describes application SWO fiber-optic production line scheduling
problem, derived data provided Lucent Technologies. particular plant,
cable may assembled one 13 parallel production lines. cable type,
subset production lines compatible, time required produce
cable depend compatible lines selected. cable also setup
time, depends cable type predecessor. Setups
certain pairs cable types infeasible. Task preemption allowed, i.e. cable
started processing line, finishes without interruption.
cable assigned release time due date. Production cannot begin
release time. objective function includes penalty missing due dates, penalty
setup times.
358

fi\Squeaky Wheel" Optimization

3.1 Implementation

describe implementation terms three main components SWO:

Constructor. constructor builds schedule adding tasks one time,

order occur priority sequence. task added selecting line
position relative tasks already line. task may inserted
two tasks already line beginning end line's schedule.
Changes relative positions tasks already line considered.
task line assigned earliest possible start time, subject
ordering, i.e., task starts either release time, immediately previous
task line, whichever greater.
possible insertion points schedule, relative tasks already
line, constructor calculates effect objective function, task
placed best-scoring location. Ties broken randomly. tasks
placed, constructor applies SWO individual line schedules, attempting
improve score line reordering cables assigned it.

Analyzer. assign blame task current schedule, analyzer first calculates

lower bound minimum possible cost task could contribute
schedule. example, task release time later due date,
late every schedule, minimum possible cost already includes
penalty. Minimum possible setup costs also included. given schedule,
blame assigned task \excess cost," difference actual
cost minimum possible cost. Excess lateness costs assigned tasks
late, excess setup costs split adjacent tasks.

Prioritizer. blame assigned, prioritizer modifies previous sequence tasks moving tasks non-zero blame factors forward sequence.
Tasks moved forward distance increases magnitude blame.
move back sequence front, task must high blame
factor several iterations. call \sticky sort."

current implementation considerable room improvement. analysis
feedback currently used simple, construction schedules could take
various heuristics account, preferring place task line
\slack," things equal.

3.2 Experimental results

six sets test data, ranging size 40 297 tasks, 13 parallel
production lines. largest problem largest manufacturer required
practice. compare following solution methods:
SWO

Applies SWO architecture problem, running fixed number iterations
returning best schedule finds.
359

fiJoslin & Clements

Data
Set
40
50
60
70
148
297

Best
Obj
1890
3101
2580
2713
8869
17503

SWO

Avg
Obj
1890
3156
2584
2727
8927
17696

Avg
Time
48
57
87
124
431
1300

TABU

Obj
1911
3292
2837
2878
10421
|

Time
425
732
1325
2046
17260
|

IP

Obj
1934
3221
2729
2897
|
|

Time
20
175
6144
4950
|
|

Table 1: Experimental results: scheduling
TABU

IP

Uses TABU search (Glover & Laguna, 1997), local search algorithm moves
increase cost permitted avoid getting trapped local optima. avoid
cycling, \uphill" move made, allowed immediately undone.

Applies Integer Programming (IP) solver, using encoding described (?).

297 task problem, SWO far effective either TABU IP. TABU,
example, failed find feasible schedule running 24 hours.
smallest problems, TABU IP able find solutions, SWO outperformed
substantial margin.
Table 1 presents results problem SWO, TABU IP. SWO, ten trials
run results averaged. TABU IP implementations deterministic,
results single run shown. second column table shows best
objective function value ever observed problem. remaining columns
show objective function value running times SWO, TABU IP. IP
experiments run Sun Sparcstation 10 Model 50. IP experiments run
IBM RS6000 Model 590 (a faster machine).
best values observed result combining SWO IP, reported
(?). work, SWO generated solutions, running produced number
\good" schedules. IP solver invoked re-combine elements solutions
better solution. Although improvements achieved IP solver relatively
small, order 1.5%, achieved improvement quickly, SWO unable
achieve degree optimization even given substantially time.
noting hybrid approach effective SWO alone, much
effective IP alone, focus performance individual techniques.
also note first, fairly naive implementation SWO scheduling
problems already outperformed TABU IP. Moreover, improved implementation,
reported above, still fairly simple, successful without relying domain-dependent
heuristics. take evidence effectiveness approach due
cleverness construction, analysis prioritization techniques, due
effectiveness SWO cycle identifying responding whatever elements
problem happen causing diculty local region search.
360

fi\Squeaky Wheel" Optimization

10

# lines job position run

8
6
4
2
0

0

2

4

6

0

2

4

6

8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38
Position priority sequence
Order based # lines job run

10
8
6
4
2
0

8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38
Position priority sequence
Order 14th iteration, producing solution 0.05% best known

Figure 5: Comparison heuristic priorities priorities derived SWO
also instructive compare results good heuristic ordering, sequence derived SWO. good heuristic scheduling domain (and one used
initially populate priority sequence) sort tasks number production
lines task could feasibly assigned empty schedule. task
scheduled many lines likely easier schedule one compatible
small number lines, therefore expected need lower priority.
top graph Figure 5 shows sequence tasks, determined heuristic.
lower graph illustrates changes priority tasks, SWO run fourteen
iterations (enough improve solution derived sequence within 0.05 percent
best known solution).
figure illustrates, heuristic generally accurate, SWO move
tasks compatible production lines positions relatively
high priority, ecting fact contrary heuristic, tasks turned
relatively dicult schedule well. tasks compatible production lines actually easy schedule well, moved relatively low priorities.

3.3 Restarts
SWO solver used produce results reported Table 1 restarted priority
queue every n=2 iterations, n number jobs problem. noisy
heuristic used initially populate priority queue also used restart
it. restart cutoff picked rather ad hoc manner. careful analysis
361

fiJoslin & Clements

Iterations
Feasible
< 18000
< 17700
per Success Mean Success Mean Success
Mean Sample
Restart
Rate Cost
Rate Cost
Rate
Cost
Size
10 0.8542
5.9 0.0504 195.3 0.0002 49994.5 10000
20 0.9722
6.0 0.2052 90.9 0.0006 33328.3
5000
30 0.9955
5.8 0.3812 67.5 0.0030 9895.5
3300
40 0.9996
5.8 0.5488 56.7 0.0060 6658.2
2500
50 0.9995
6.0 0.6330 57.0 0.0160 3112.7
2000
60 1.0000
5.7 0.7242 52.9 0.0188 3170.4
1650
70 1.0000
5.7 0.8079 50.2 0.0350 1973.5
1400
80 1.0000
6.2 0.8552 49.5 0.0296 2670.0
1250
90 1.0000
5.8 0.8827 48.9 0.0300 2965.3
1100
100 1.0000
5.9 0.8840 52.4 0.0400 2452.3
1000
200 1.0000
6.0 0.9680 53.0 0.0600 3204.3
500
300 1.0000
5.3 0.9967 50.1 0.0567 5090.8
300
400 1.0000
5.8 1.0000 52.9 0.0720 5320.2
250
500 1.0000
5.8 1.0000 52.8 0.1000 4692.6
200
600 1.0000
5.8 1.0000 57.2 0.0867 6590.8
150
700 1.0000
6.1 1.0000 42.4 0.1200 5472.4
100
800 1.0000
5.6 1.0000 53.0 0.1200 6210.3
100
900 1.0000
5.3 1.0000 45.8 0.1700 4691.6
100
1000 1.0000
6.0 1.0000 45.4 0.1800 4838.1
100

Table 2: Experimental results: restarts scheduling domain
different restart cutoff values might lead producing better solutions faster,
additional insight workings SWO.
Restarts often used non-systematic search avoid getting trapped local optima
cycles. (See Parkes Walser, 1996, empirical study WSAT
references.) Restarts also used systematic search escape exponentially large
regions search space contain solution (Gomes, Selman, & Kautz, 1998).
Local optima pose little threat SWO, since directly driven uphill/downhill
considerations. SWO, use large coherent moves, also tends escape unpromising parts search space quickly. However, SWO open getting trapped cycle,
restarts used means escape them.
scheduling problems, SWO unlikely get tight cycle priority
queues solutions repeat exactly. due presence random tie breaking
several places, presence noise prioritizer. However, belief
SWO get trapped cycle similar priority queues solutions repeat.
ran series experiments 297 task problem determine impact
various restart cutoffs. results summarized Table 2. Restart cutoffs ranged
every 10 iterations every 1000 iterations. success rate mean cost
shown value three different solution qualities. success rate indicates
probability solution least given quality found given pass.
mean cost average number total iterations get solution quality.
feasible 18000 solution thresholds, SWO reaches 100 percent success rate
well reaching maximum restart cutoff 1000 used experiments.
sense, easy SWO produce solutions least qualities. results
362

fi\Squeaky Wheel" Optimization

2 thresholds indicate easy SWO solve problem, cutoff
greater average number uninterrupted iterations takes produce solution
used solve problem minimum cost. \easy" problems, appears
small restart cutoff hurt, big cutoff not.
numbers 17700 solution quality threshold, tell different story. success
rate still climbing experiment ends, mean cost actually risen
minimum. solution quality, restart cutoff minimizes mean cost falls
around range 70 100. Mean costs rise steeply restart cutoffs range,
slowly cutoffs larger that. example hard problem SWO,
shows care needs taken choosing restart strategy problems.
Additional research needed determine set restart cutoff automatically
arbitrary problems.
data indicates SWO benefit restarts, point. 17700
threshold, restart cutoffs 100, increase cutoff general led
superlinear increase success rate. (This also another indicator SWO learning
iteration iteration.) 100 iterations per restart, success rate initially
climbs sublinearly appears level out. open question tells us
search space.

4. SWO graph coloring
also applied SWO different domain, graph coloring. objective
color nodes graph two adjoining nodes color,
minimizing number colors.

4.1 Implementation

priority sequence graph coloring consists ordered list nodes. solver
always trying produce coloring uses colors target set, one less
color used color best solution far. Again, describe implementation
terms three main components SWO:

Constructor. constructor assigns colors nodes priority sequence order.

node's color previous solution still available (i.e. adjacent node using
yet), target set, color assigned. fails, tries
assign color current target set, picking color least constraining
adjacent uncolored nodes, i.e. color reduces adjacent nodes' remaining
color choices least. none target colors available, constructor tries
\grab" color target set neighbors. color grabbed
neighbor nodes color least one choice within target
set. multiple colors grabbed, least constraining one picked.
color target set grabbed color outside target set assigned.
Nodes early priority sequence likely wide range
colors pick from. Nodes come later may grab colors earlier nodes,
earlier nodes color options within target set.
363

fiJoslin & Clements

SWO
IG
Dist. impasse
Par. impasse
TABU
Problem
colors
time colors
time colors
time colors
time colors
time
DSJC125.5
18.3
1.6 18.9
2.5 17.0
6.3 17.0 4043.6 20.0 153.3
DSJC250.5
31.9
8.3 32.8
6.9 28.0
268.5 29.2 4358.1 35.0 3442.2
DSJC500.5
56.3
40.9 58.6
18.2 49.0 8109.1 53.0 4783.9 65.0 3442.2
DSJC1000.5 101.5 208.6 104.2
67.6 89.0 41488.7 100.0 5333.8 117.0 3442.2
C2000.5
185.7 1046.2 190.0 272.4 165.0 14097.9
|
|
|
|
C4000.5
341.6 4950.8
346.9 1054.1
|
|
|
|
|
|
R125.1
5.0
0.2
5.0
2.0
5.0
0.2
5.0
64.6
5.0
0.4
R125.1c
46.0
5.1 46.0
1.1 46.0
0.2 46.0
85.0 46.0
0.9
R125.5
36.0
2.8 36.9
1.9 36.0
0.2 37.0
33.0 36.0
0.7
R250.1
8.0
0.5
8.0
7.0
8.0
0.2
8.0
22.0
8.0
0.2
R250.1c
64.0
30.6 64.0
4.6 64.0
0.5 64.0 278.2 65.0
46.4
R250.5
65.0
14.7 68.4
8.3 65.0
82.2 66.0
39.9 66.0
59.0
DSJR500.1
12.0
2.0 12.0
21.1 12.0
0.2 12.0
26.6 12.0
0.5
DSJR500.1c
85.2
96.9 85.0
14.6 85.0
59.1 85.2 5767.7 87.0 3442.2
DSJR500.5
124.1
68.7 129.6
26.1 123.0
175.3 128.0
90.5 126.0 395.1
R1000.1
20.0
8.0 20.6
87.2 20.0
8.2 20.0
49.9 20.0
1.7
R1000.1c
101.7 433.2 98.8
49.1 98.0
563.3 102.6 3940.0 105.0 3442.2
R1000.5
238.9
574.5 253.2 102.9 241.0
944.0 245.6 215.9 248.0 3442.2
at300 20 0
25.3
16.4 20.2
3.8 20.0
0.2 20.0 274.3 39.0 3442.2
at300 26 0
35.8
12.0 37.1
7.7 26.0
10.0 32.4 6637.1 41.0 3442.2
at300 28 0
35.7
11.9 37.0
9.6 31.0 1914.2 33.0 1913.5 41.0 3442.2
at1000 50 0 100.0 203.9 65.6 146.3 50.0
0.2 97.0 7792.7
|
|
at1000 60 0 100.7 198.0 102.5
87.3 60.0
0.2 97.8 6288.4
|
|
at1000 76 0 100.6 208.4 103.6
79.6 89.0 11034.0 99.0 6497.9
|
|
latin sqr 10
111.5 369.2 106.7
59.7 98.0 5098.0 109.2 6520.1 130.0 3442.2
le450 15a
15.0
5.5 17.9
17.0 15.0
0.2 15.0 162.6 16.0
17.8
le450 15b
15.0
6.1 17.9
16.2 15.0
0.2 15.0 178.4 15.0
28.4
le450 15c
21.1
8.0 25.6
14.5 15.0
57.2 16.6 2229.6 23.0 3442.2
le450 15d
21.2
7.8 25.8
13.5 15.0
36.3 16.8 2859.6 23.0 3442.2
mulsol.i.1
49.0
5.9 49.0
4.2 49.0
0.2 49.0
27.2 49.0
0.3
school1
14.0
8.4 14.0
10.5 14.0
0.2 14.0
46.3 29.0
90.7
school1 nsh
14.0
7.2 14.1
8.9 14.0
0.2 14.0
66.4 26.0
31.2

Table 3: Experimental results: graph coloring problems

Analyzer. Blame assigned node whose assigned color outside target set,
amount blame increasing additional color must added
target set. ran experiments several different variations color-based
analysis. performed reasonably.

Prioritizer. prioritizer modifies previous sequence nodes moving nodes
blame forward sequence according much blame received.
done way done scheduling problems. initial sequence list
nodes sorted decreasing degree order, noise added slightly shue
sort.
364

fi\Squeaky Wheel" Optimization

4.2 Experimental results
applied SWO standard set graph coloring problems, including random graphs
application graphs model register allocation class scheduling problems.
collected Second DIMACS Implementation Challenge (Johnson & Trick, 1996),
includes results several algorithms problems (Culberson & Luo, 1993; Glover,
Parker, & Ryan, 1993; Lewandowski & Condon, 1993; Morgenstern, 1993). Problems range
125 nodes 209 edges 4000 nodes 4,000,268 edges.
Glover et al. (1993) paper based general search technique, TABU
branch bound, rather graph coloring specific algorithm. approach
worst reported average results group. Morgenstern (1993) used distributed
IMPASSE algorithm best overall colorings, also required target
number colors, well several problem specific parameters passed
solver. Lewandowski & Condon (1993) also found good solutions problem set.
approach used hybrid parallel IMPASSE systematic search 32 processor
CM-5. Culberson & Luo (1993) used Iterated Greedy (IG) algorithm bears
similarity SWO. IG simplest algorithm group. solution quality falls
IMPASSE algorithms TABU solves entire set 1 2 percent time
taken methods. IG IMPASSE discussed related
work.
Table 3 compares SWO results IG (Culberson & Luo, 1993), distributed
IMPASSE (Morgenstern, 1993), parallel IMPASSE (Lewandowski & Condon, 1993), TABU
(Glover et al., 1993). each, one column shows number colors required
problem, run time (in CPU seconds). Bold face indicates number colors
within 0.5 best result table.
used Pentium Pro 333MHz workstation running Linux SWO graph coloring
experiments. times shown four algorithms based reported
(Johnson & Trick, 1996). results IG, IMPASSE TABU normalized times
using DIMACS benchmarking program dfmax, provided purpose. Therefore,
timing comparisons approximate. machine ran dfmax r500.5 benchmark
86.0 seconds; times reported machines used algorithms
86.9 seconds TABU experiments, 192.6 seconds IG, 189.3 seconds IMPASSE,
2993.6 seconds parallel IMPASSE. dfmax benchmark runs single
processor, unsuitable normalizing times parallel IMPASSE. report
unnormalized times.
variety termination conditions used. SWO terminated 1000 iterations.
IG terminated 1000 iterations without improvement. Distributed IMPASSE used
wide variety different termination conditions solve different problems.
common element across problems distributed IMPASSE stopped target
number colors, provided input parameter, reached. times reported
parallel IMPASSE times took find best solution found, time
took algorithm terminate, always 3 hours. TABU ran algorithm
determined could make progress, hour passed, whichever came
first.
365

fiJoslin & Clements

25
TABU

Avg. percent best group

20

15
Iterated Greedy

10
Squeaky Wheel

5

Par IMPASSE

Dist IMPASSE
0
0

10000

20000

30000
Time (CPU seconds)

40000

50000

60000

Figure 6: Experimental results: quality solution vs. time
TABU numbers single run problem. numbers
algorithms averages 4 runs (parallel IMPASSE), 5 runs (distributed IMPASSE, parallel
IMPASSE) 10 runs (SWO, IG, distributed IMPASSE) problem.
Figure 6 summarizes performance technique set 27 problems
algorithms solved. solver graph indicates average solution quality
average amount time needed solve set. ideal location graph
origin, producing high quality solutions little time. points shown
techniques points reported papers. curve shown SWO
shows performs given varying amounts time solve set. graph
shows, SWO clearly outperforms TABU, general purpose technique,
terms quality speed. SWO also outperforms IG, graph coloring specific algorithm,
terms quality speed. IMPASSE solvers clearly produce best solutions
group. However, IMPASSE domain specific method, solvers represent
much programming effort. SWO solver uses general purpose search technique
implemented less month single programmer.

4.3 Alternate configurations SWO
note that, scheduling work, first, naive implementation SWO graph
coloring produced respectable results. Even without color reuse, color grabbing, least
constraining heuristic (the first free color found picked), SWO matched IG 6 problems
beat 10. However, half remaining problems IG better 10
colors.
explore sensitivity SWO implementation details tried following
approaches constructor prioritizer, ran SWO using combinations:
366

fi\Squeaky Wheel" Optimization

Construction: without color grabbing
Analysis: Either blame nodes receive color outside target set,

first node (in priority sequence) causes new color outside target set
introduced. color grabbing used, determination blame based
final color assigned node.

difference solution quality worst combination best combination
less 15 percent. Even alternative using standard sort instead
\sticky" sort (a fairly fundamental change) added mix, spread
worst best still 20 percent.

5. Related work

importance prioritization greedy algorithms new idea. \First Fit"
algorithm bin packing, example, relies placing items bins decreasing order
size (Garey & Johnson, 1979). Another example GRASP (Greedy Randomized Adaptive
Search Procedure) (Feo & Resende, 1995). GRASP differs approach several ways.
First, prioritization construction aspects closely coupled GRASP.
element added solution constructed, remaining elements reevaluated heuristic. Thus order elements added solution
may depend previous decisions. Second, order elements selected
trial determined heuristic (and randomization), trials independent.
learning iteration iteration GRASP.
Doubleback Optimization (DBO) (Crawford, 1996) extent inspiration
SWO another similar algorithm, Abstract Local Search (ALS) (Crawford, Dalal, &
Walser, 1998). designing SWO, began looking DBO, extremely
successful solving standard type scheduling problem. However, DBO useful
objective minimize makespan, also limited types constraints
handle. limitations, began thinking principles behind
DBO, looking effective generalization approach. DBO can, fact, viewed
instance SWO. DBO begins performing \right shift" schedule, shifting
tasks far right go, boundary. resulting right-shifted
schedule, left-most tasks are, extent, tasks critical.
corresponds analysis SWO. Tasks removed right-shifted schedule,
taking left-most tasks first. ordering corresponds prioritization SWO.
task removed, placed new schedule earliest possible start time, i.e., greedy
construction.
Like SWO, ALS result attempt generalize DBO. ALS views priority space
(to use terminology SWO) space \abstract schedules," performs local
search space. Unlike SWO, prioritization modified, corresponding
move solution space downhill (away optimal), modified prioritization
discarded, old prioritization restored. usual local search, ALS also
sometimes makes random moves, order escape local minima.
ALS, also List Scheduling (Pinson, Prins, & Rullier, 1994), scheduling algorithms
deal domains include precedence constraints tasks. accommodate
367

fiJoslin & Clements

precedence constraints constructing schedules left-to-right temporally. task cannot
placed schedule predecessors placed. order
analysis, prioritization construction appropriately coupled, sucient
simply increase priority task late, constructor may able
place task lot decisions made. Consequently,
amount blame must propagated task's predecessors.
commercial scheduler OPTIFLEX (Syswerda, 1994) uses genetic algorithm approach
modify sequence tasks, constraint-based schedule constructor generates
schedules sequences. OPTIFLEX also viewed instance SWO,
genetic algorithm replacing analysis. effect, \analysis" instead emerges
relative fitness members population.
Two graph coloring algorithms also bear similarity SWO. Impasse Class Coloration Neighborhood Search (IMPASSE) (Morgenstern, 1993; Lewandowski & Condon,
1993), like SWO, maintains target set colors produces feasible colorings. Given
coloring, IMPASSE places nodes colored outside target set impasse
set. iteration node selected impasse set, using noisy degree-based
heuristic, assigned random color target set. neighbor nodes
con ict moved impasse set.
Iterated Greedy (IG) (Culberson & Luo, 1993), like SWO, uses sequence nodes
create new coloring iteration, uses coloring produce new
sequence next iteration. method used generate new sequence differs
SWO. key observation behind IG nodes color
current solution grouped together next sequence (i.e. adjacent
sequence), next solution worse current solution. IG achieves
improvement manipulating order groups occur new sequence,
using several heuristics including random based color, descending based color,
ascending based cardinality group. IG learns groupings nodes runs,
learn diculty nodes. node's place sequence
indicates nothing expected detected diculty.

6. Analysis future work

section summarizes several areas future research suggested results reported
previous sections.

6.1 Scaling

SWO uses fast, greedy algorithms constructing solutions, demonstrated
effectiveness problems realistic size, greatest threat scalability SWO
constructs new solution scratch iteration. partial solution
problem seen use \history" mechanism graph coloring problems. Using
color node previous solution means many cases
need check possible colors. significantly speeds construction.
fundamental solution problem would develop incremental version
SWO. selective reuse colors graph coloring solver small step
direction. allows constructor avoid spending time evaluating alternatives
368

fi\Squeaky Wheel" Optimization

previous choice still works. generally, may possible look
changes made prioritization, modify corresponding solution way
generates solution would constructed scratch based new
prioritization. seems feasible could done domains, least small
changes prioritization, may large portions solution
unaffected.
interesting possibility based view SWO performing local search
plus certain kind propagation. small change priorities may correspond large
change solution. example, increasing priority one task scheduling
problem may change position schedule, and, consequence, lower priority
tasks may shued around accommodate change. similar
might expect moving higher priority task, propagating effects
change moving lower priority tasks well. single move may correspond large
number moves search algorithm looks local changes schedule,
may thus dicult algorithm find.
Based view, investigating algorithm call \Priority-Limited Propagation" (PLP). PLP, local changes made solution, propagation
allowed occur, subject current prioritization. Propagation allowed occur
direction lower-priority elements. effect, small change made,
consequences change allowed \ripple" plan. propagation
occur directions decreasing priority, ripples propagation decrease
magnitude propagation possible. new prioritization generated
analyzing resulting solution. (It possible analysis incrementally,
well.) resulting approach identical SWO, many interesting
characteristics.

6.2 Coordination modules
SWO effective, obvious analysis, prioritization construction must
work together improve quality solutions. already discussed complications arise constraints placed order constructor
make decisions, case List Scheduling ALS, construction done
strictly left-to-right. Without complex analysis, search spaces effectively become uncoupled, changes priority don't cause constructor fix problems
discovered analysis.
Another way search become uncoupled related notion \excess cost,"
discussed scheduling implementation. calculation excess cost analyzer
turned key idea improving performance SWO. However, problems sometimes tasks must handled badly order achieve good overall solution. One
scheduling problems described previously two \sacrificial" tasks. Whenever
good solution found, analyzer assigns high blame sacrificial tasks,
constructor handles well next iteration. means resulting solution
poor overall quality, aws cause tasks move ahead
sacrificial tasks priority sequence SWO again, brie y, explore space
369

fiJoslin & Clements

good solutions. cases, extent analysis actually hurting ability
SWO converge good solutions.
Ideally, would like generalize notion excess cost recognize sacrificial
tasks, allow tasks handled badly without receiving proportionate blame.
problems task must sacrificed solutions, may possible use
learning mechanism accomplish this.
However, notion sacrificial task subtle this. Suppose
example scheduling construction two airplanes, P1 P2,
key task, T1 T2, respectively, requiring shared resource, R.
resource con ict, must either give R T1 early schedule, starting
construction plane P1 P2, must give R T2 early schedule,
opposite result. Whichever two tasks started early finish time,
late.
Suppose construct schedule T1 goes first, T2 late, thus receiving
heavy blame factor. SWO increases priority T2, consequence, T2 goes first
subsequent schedule. T1 late, next iteration go
first. could alternate manner forever, result would SWO would
fail explore either option effectively, would jumping back forth
option building plane P1 first, option building plane P2 first,
without remaining one region search space long enough refine solution.
diculty neither T1 T2 identified sacrificial task. Assuming
two planes identical, cannot simply argue symmetry
pick one two tasks sacrificed. If, however, could identify sacrificial
task role plays solution, could achieve need. Here, task
sacrificed must one belongs whichever plane started later. analyzer
could reduce blame assigned task schedule, whichever task happens
be, would allow SWO explore region search much effectively.
problem interchangeable roles would arise even clearly introduction conditional elements solution. Suppose, example, scheduling
problem constructor may choose include include task instances
type, adding however many instances needed satisfy resource requirement.
tasks instances task type, interchangeable,
penalizing one may simply cause shuing instances really address
problem. Moreover, conditional tasks, clear analyzer
assign blame set task instances current schedule may different
set task instances successor schedules.
address concerns, notion prioritization could generalized apply
additional aspects problem. scheduling might mean prioritizing tasks,
also resources various time intervals. also propose prioritizations
limited \fixed" elements problem. scheduling problems, example,
may non-conditional tasks, resources, etc. (In example domains,
elements fixed sense, issue.)
One intuition behind proposal elements tend define
roles. earlier example tasks T1 T2, corresponding two planes
built, critical element either task per se, actually resource R, early
370

fi\Squeaky Wheel" Optimization

schedule. phase resource R receives high priority, later phase resource
R receives lower priority, whichever two tasks occurs later recognized
less critical. exactly capture notion \role" would like,
comes lot closer current approach. addition, assigning priorities fixed
elements problem advantage applicable problems conditional
tasks. Research currently way explore approach.

6.3

SWO

local search

Although ability make large, coherent moves strength approach, also
weakness. SWO poor making small \tuning" moves solution space,
coupled-search view SWO suggests obvious remedy. SWO could combined local
search solution space, look improvements vicinity good solutions.
Similarly, making small changes prioritization would generally result smaller moves
solution space result going full analysis re-prioritization
cycle.
Yet another alternative genetic algorithm techniques \crossover" types
mutation pool nodes, done OPTIFLEX. Many hybrid approaches possible, believe coupled-search view SWO helps identify interesting
strategies combining moves various sizes kinds, search spaces, adapting
dynamically relative solution qualities.

7. Conclusions
experience fairly straightforward implement SWO new domain,
usually fairly obvious ways construct greedy solutions, analyze
solution assign \blame" elements. Naive implementations SWO tend
perform reasonably well.
found view SWO performing \coupled search" two different
search spaces informative. helpful characterize kinds moves
SWO makes search spaces, effect avoiding local optima,
etc. hope continuing gain deeper understanding makes SWO work
able say effective design SWO algorithms.
number directions future research suggests, begun scratch
surface \Squeaky Wheel" Optimization.

Acknowledgments
authors wish thank Robert Stubbs Lucent Technologies providing data
used scheduling experiments. authors also wish thank George L. Nemhauser,
Markus E. Puttlitz Martin W. P. Savelsbergh collaborated using SWO
hybrid AI/OR approach. Many useful discussions came collaboration,
without would access Lucent problems. Markus also wrote
framework scheduling experiments TABU IP implementations.
371

fiJoslin & Clements

authors also thank members CIRL, James Crawford i2 Technologies,
helpful comments suggestions. would like thank Andrew Parkes
particular suggestions insights graph coloring domain.
effort sponsored Air Force Oce Scientific Research, Air Force Materiel Command, USAF, grant number F49620-96-1-0335; Defense Advanced
Research Projects Agency (DARPA) Rome Laboratory, Air Force Materiel Command,
USAF, agreements F30602-95-1-0023 F30602-97-1-0294; National
Science Foundation grant number CDA-9625755.
U.S. Government authorized reproduce distribute reprints Governmental purposes notwithstanding copyright annotation thereon. views conclusions
contained herein authors interpreted necessarily representing ocial policies endorsements, either expressed implied, Defense
Advanced Research Projects Agency, Rome Laboratory, Air Force Oce Scientific
Research, National Science Foundation, U.S. Government.
work reported paper done authors CIRL.

References

Crawford, J., Dalal, M., & Walser, J. (1998). Abstract local search. Proceedings
AIPS-98 Workshop Planning Combinatorial Search. conjunction
Fourth International Conference Artificial Intelligence Planning Systems (AIPS98).
Crawford, J. M. (1996). approach resource constrained project scheduling. Proceedings 1996 Artificial Intelligence Manufacturing Research Planning Workshop, pp. 35{39.
Culberson, J. C., & Luo, F. (1993). Exploring k{colorable landscape iterated
greedy. (Johnson & Trick, 1996), pp. 245{284.
Feo, T. A., & Resende, M. G. (1995). Greedy randomized adaptive search procedures.
Journal Global Optimization, 6, 109{133.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide
Theory NP-Completeness. W. H. Freeman.
Glover, F., & Laguna, M. (1997). Tabu Search. Kluwer.
Glover, F., Parker, M., & Ryan, J. (1993). Coloring tabu branch bound. (Johnson
& Trick, 1996), pp. 285{307.
Gomes, C., Selman, B., & Kautz, H. (1998). Boosting combinatorial search randomization. Proceedings AAAI-98, pp. 431{437.
Johnson, D. S., & Trick, M. A. (Eds.). (1996). Cliques, Coloring, Satisfiability: Second
DIMACS Implementation Challenge, 1996, Vol. 26 DIMACS Series Discrete
Mathematics Theoretical Computer Science. American Mathematical Society.
372

fi\Squeaky Wheel" Optimization

Joslin, D., & Clements, D. (1998). \Squeaky wheel" optimization. Proceedings AAAI98, pp. 340{346.
Lewandowski, G., & Condon, A. (1993). Experiments parallel graph coloring heuristics
applications graph coloring. (Johnson & Trick, 1996), pp. 309{334.
Morgenstern, C. (1993). Distributed coloration neighborhood search. (Johnson & Trick,
1996), pp. 335{357.
Parkes, A., & Walser, J. (1996). Tuning local search satisfiability testing. Proceedings
AAAI-96, pp. 356{362.
Pinson, E., Prins, C., & Rullier, F. (1994). Using tabu search solving resourceconstrained project scheduling problem. EURO-WG PMS 4 (EURO Working
Group Project Management Scheduling), pp. 102{106 Louvain, Belgium.
Selman, B., Kautz, H. A., & Cohen, B. (1993). Local search strategies satisfiability
testing. (Johnson & Trick, 1996), pp. 521{531.
Syswerda, G. P. (1994). Generation schedules using genetic procedure.. U.S. Patent
number 5,319,781.

373

fiJournal Artificial Intelligence Research 10 (1999) 243-270

Submitted 10/98; published 5/99

Learning Order Things
William W. Cohen
Robert E. Schapire
Yoram Singer

AT&T Labs, Shannon Laboratory, 180 Park Avenue
Florham Park, NJ 07932, USA

wcohen@research.att.com
schapire@research.att.com
singer@research.att.com

Abstract

many applications desirable order rather classify instances. consider problem learning order instances given feedback
form preference judgments, i.e., statements effect one instance
ranked ahead another. outline two-stage approach one first learns
conventional means binary preference function indicating whether advisable rank
one instance another. consider on-line algorithm learning preference
functions based Freund Schapire's \Hedge" algorithm. second stage,
new instances ordered maximize agreement learned preference function. show problem finding ordering agrees best learned
preference function NP-complete. Nevertheless, describe simple greedy algorithms
guaranteed find good approximation. Finally, show metasearch
formulated ordering problem, present experimental results learning combination \search experts," domain-specific query expansion strategy
web search engine.
1. Introduction

Work inductive learning mostly concentrated learning classify. However,
many applications desirable order rather classify instances.
example might personalized email filter prioritizes unread mail.
consider problem learning construct orderings given feedback
form preference judgments, i.e., statements one instance ranked ahead
another.
orderings could constructed based learned probabilistic classifier regression model fact often are. instance, common practice information retrieval
rank documents according probability relevance query, estimated
learned classifier concept \relevant document." advantage learning orderings
directly preference judgments much easier obtain labels required
classification learning.
instance, email application mentioned above, one approach might rank
messages according estimated probability membership class \urgent"
messages, numerical estimate urgency obtained regression. Suppose,
however, user presented ordered list email messages, elects read
third message first. Given election, necessarily case message three
urgent, sucient information estimate numerical urgency measures.

c 1999 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiCohen, Schapire, & Singer
However, seems quite reasonable infer message three ranked
ahead others. Thus, setting, obtaining preference information may easier
natural obtaining labels needed classification regression approach.
Another application domain requires ordering instances collaborative filtering; see,
instance, papers contained Resnick Varian (1997). typical collaborative
filtering task, user seeks recommendations, say, movies likely enjoy.
recommendations usually expressed ordered lists recommended movies, produced
combining movie ratings supplied users. Notice user's movie ratings
viewed set preference judgements. fact, interpreting ratings preferences
advantageous several ways: instance, necessary assume rating
\7" means thing every user.
remainder paper, investigate following two-stage approach
learning order. stage one, learn preference function, two-argument function
PREF(u; v) returns numerical measure certain u ranked
v. stage two, use learned preference function order set new instances
X ; accomplish this, evaluate learned function PREF(u; v) pairs instances
u; v 2 X , choose ordering X agrees, much possible, pairwise
preference judgments.
stage one, describe specific algorithm learning preference function
set \ranking-experts". algorithm on-line weight allocation algorithm, much like
weighted majority algorithm (Littlestone & Warmuth, 1994) Winnow (Littlestone,
1988), and, directly, Freund Schapire's (1997) \Hedge" algorithm. stage two,
show finding total order agrees best preference function NPcomplete. Nevertheless, show ecient greedy algorithms always find
good approximation best ordering.
present experimental results algorithm used combine
results several \search experts," domain-specific query expansion
strategy web search engine. Since work touches several different fields defer
discussion related work Sec. 6.
2. Preliminaries

Let X set instances. simplicity, paper, always assume X
finite. preference function PREF binary function PREF : X X ! [0; 1]. value
PREF(u; v) close 1 (respectively 0) interpreted strong recommendation
u ranked (respectively, below) v. value close 1=2 interpreted
abstention making recommendation. noted earlier, hypothesis
learning system preference function, new instances ranked
agree much possible preferences predicted hypothesis.
standard classification learning, hypothesis constructed combining primitive
features. Similarly, paper, preference function combination primitive
preference functions. particular, typically assume availability set N
primitive preference functions R1 ; : : : ; RN . combined usual ways,
instance boolean linear combination values. especially
interested latter combination method.
244

fiLearning Order Things
1/4





c

c

3/4



1/8

c

1

b



b

f(a)=1 f(b)=2
f(c)=0 f(d)=



g(a)=0 g(b)=2
g(c)=1 g(d)=2

b

7/8 1/8
1

7/8



1/4f() + 3/4g()

Figure 1: Left middle: Two ordering functions graph representation. Right:
graph representation preference function created weighted ( 41
3 ) combination two functions. Edges weight 1 0 omitted.
4
2
convenient assume Ri 's well-formed certain ways. end,
introduce special kind preference function called rank ordering defined
ordering function. Let totally ordered set. assume without loss generality
R . ordering function function f : X ! , interpret
inequality f (u) > f (v) mean u ranked v f . sometimes convenient
allow ordering function \abstain" give preference pair u, v.
therefore allow include special symbol ? R , interpret f (u) = ?
mean u \unranked." define symbol ? incomparable elements
(that is, ? 6< 6< ? 2 ).
ordering function f induces preference function Rf , defined

8
>
<1
Rf (u; v) = 0
>
: 12

f (u) > f (v)
f (u) < f (v)
otherwise.

call Rf rank ordering X . Rf (u; v) = 1, say u preferred
v, u ranked higher v. Note Rf (u; v) = 12 either u v (or both)
unranked.
sometimes describe manipulate preference functions directed weighted
graphs. nodes graph correspond instances X . pair (u; v) connected directed edge weight PREF(u; v). Since ordering function f induces
preference function Rf , also describe ordering functions graphs. Fig. 1
give example two ordering functions corresponding graphs. brevity,
draw edges (u; v) PREF(u; v) = 12 PREF(u; v) = 0.
give concrete example rank orderings, imagine learning order documents
based words contain. model this, let X set documents
repository, N words w1 ; : : : ; wN , let fi(u) number occurrences word
wi document u. Rf prefer u v whenever wi occurs often u v.
second example, consider metasearch application goal combine


245

fiCohen, Schapire, & Singer
rankings several web search engines fixed query. N search engines e1 ; : : : ; eN ,
one might define fi Rf prefers web page u web page v whenever u ranked
ahead v list Li produced corresponding search engine. this, one could
let fi (u) = k web page u appearing k-th position list Li , let
fi (u) = (where > jLi j) web page u appearing Li .
Feedback user represented similar general way.
assume feedback set element pairs (u; v), representing assertion form
\u preferred v." definition feedback less restricted ordering
functions. particular, assume feedback consistent|cycles,
> b > a, allowed.


3. Learning Combination Ordering Functions

section, consider problem learning good linear combination set
ordering functions. Specifically, assume access set ranking experts,
generates ordering function provided set instances. instance,
metasearch problem, ranking expert might function submits user's query
different search engine; domain instances might set web pages
returned ranking experts; ordering function associated
ranking expert might represented example (i.e., letting fi (u) = k
k-the web page u returned i-th search engine, letting fi (u) = web
page u retrieved i-th search engine). user's feedback set pairwise
preferences web pages. feedback may obtained directly, example,
asking user explicitly rank URL's returned search engine; feedback
may obtained indirectly, example, measuring time spent viewing
returned pages.
note metasearch problem, approach works directly
numerical scores associated different search engines might feasible;
numerical scores might comparable across different search engines, might
provided search engines. Another problem web pages
indexed search engines. easily modeled setting: rather
letting fi (u) = web page u ranked search engine i, one could let
fi (u) = ?. corresponds assumption search engine's preference u
relative ranked web pages unknown.
describe weight allocation algorithm
functions Ri
P usesw Rthe(u;preference
learn preference function form PREF(u; v) = N
v
).

adopt
on-line
i=1
learning framework first studied Littlestone (1988) weight wi assigned
ranking expert updated incrementally.
Formally, learning assumed take place sequence rounds. round t,
assume learning algorithm provided set X instances ranked,
ranking expert 2 f1; : : : ; N g provides ordering function fit . (In metasearch,
instance, fit ordering function associated list Lti web pages returned
i-th ranking expert t-th query, X set web pages appear
lists Lt1 ; : : : ; LtN .) ordering function fit induces preference function Rf ,
denote brevity Rit . learner may compute Rit (u; v) preference



246

fiLearning Order Things
functions Rit pairs u; v 2 X producing combined preference function PREFt ,
used produce ordering ^t X . (Methods producing ordering
preference function discussed below.)
producing ordering ^t , learner receives feedback environment.
assume feedback arbitrary set assertions form \u
preferred v." is, feedback t-th round set F pairs (u; v).
algorithm propose problem based \weighted majority algorithm" Littlestone Warmuth (1994) and, directly, Freund Schapire's
(1997) \Hedge" algorithm. define loss preference function R respect
user's feedback F
Loss(R; F ) =

P

2

(u;v) F (1

jF j

R(u; v))

1 X
jF j (u;v)2F R(u; v) :

=1

(1)

loss natural probabilistic interpretation. R viewed randomized prediction
algorithm predicts u precede v probability R(u; v), Loss(R; F )
probability R disagreeing feedback pair (u; v) chosen uniformly random
F .
worth noting assumption form feedback relaxed
allowing user indicate degree prefers u v. case,
loss normalized weighted sum feedback pairs. Since generalization
rather straightforward, assume brevity feedback unweighted set
assertions element pairs.
use Hedge algorithm almost verbatim, shown Figure 2.
algorithm maintains positive weight vector whose value time denoted wt =
). prior knowledge ranking experts, set initial
(w1t ; : : : ; wN
weights equal wi1 = 1=N .
round t, weight vector wt used combine preference
functions
P
N

different experts obtain preference function PREF (u; v) = i=1 wit Rit (u; v).
preference function next converted ordering ^t current set elements
X . purposes section, method producing ordering immaterial;
particular, methods described Sec. 4 could used here. Based ordering,
user provides feedback F , loss preference function Loss(Rit ; F )
evaluated Eq. (1). Finally, weight vector wt updated using multiplicative
rule
wt fi Loss(R ;F )
wit+1 =
Zt
fi 2 [0; 1] parameter, Zt normalization constant, chosen
weights sum one update. Thus, round, weights ranking
experts adjusted experts producing preference functions relatively large
agreement feedback increased.
give theoretical rationale behind algorithm. Freund Schapire (1997)
prove general results Hedge applied directly loss function.
results imply almost immediately bound cumulative loss preference function
PREFt terms loss best ranking expert, specifically:



247



fiCohen, Schapire, & Singer
Allocate Weights Ranking Experts
P
Parameters: fi 2 [0; 1], initial weight vector w1 2 [0; 1]N Ni=1 wi1 = 1
N ranking experts, number rounds
= 1; 2; : : : ;

1. Receive set elements X ordering functions f1t ; : : : ; fNt . Let Rit denote
preference function induced fit .
2. Compute total order ^t approximates
PREFt (u; v) =

N
X
i=1

wit Rit (u; v)

(Sec. 4 describes several ways approximating preference function total
order.)
3. Order X using ^t .
4. Receive feedback F user.
5. Evaluate losses Loss(Rit ; F ) defined Eq. (1).
6. Set new weight vector
wit+1 =

wit fi Loss(R ;F
Zt





Zt normalization constant, chosen

)

PN

t+1
i=1 wi

= 1.

Figure 2: on-line weight allocation algorithm.

Theorem 1 algorithm Fig. 2,

X
t=1

Loss(PREFt ; F ) afi min

afi = ln(1=fi )=(1

P
Note



fi ) cfi = 1=(1


X
t=1

Loss(Rit ; F ) + cfi ln N

fi ).


Loss(PREF ; F )
Loss(Rit ; F )

cumulative loss combined preference funcP
tions
cumulative loss ith ranking expert. Thus,
Theorem 1 states cumulative loss combined preference functions
much worse best ranking expert.
Proof:
1 X X
w R (u; v)
Loss(PREFt ; F ) = 1
F (u;v)2F
PREFt ,

=

0
X t@
wi 1


1



1 X
Rt (u; v)A
F (u;v)2F


248

fiLearning Order Things

X

=



wit Loss(Rit (u; v); F ):

Therefore, Freund Schapire's (1997) Theorem 2,

X
t=1

Loss(PREFt ; F ) =



X
X
t=1

afi min

2



wit Loss(Rit (u; v); F )

X
t=1

Loss(Rit ; F ) + cfi ln N:

course, interested loss PREFt (since ordering),
rather performance actual ordering ^t computed learning algorithm.
Fortunately, losses related using kind triangle inequality. Let
DISAGREE(; PREF) =

X

u;v:(u)>(v)

(1

PREF(u; v)) :

(2)

Theorem 2 PREF, F total order defined ordering function ,
Loss(R ; F )

DISAGREE(; PREF)
+ Loss(PREF; F ):
jF j

(3)

x; 2 [0; 1], let us define d(x; y) = x(1 y) + y(1 x). show
satisfies triangle inequality. Let x, z [0; 1], let X , Z
independent Bernoulli (f0; 1g-valued) random variables probability outcome 1 equal
x, z , respectively.

Proof:

d(x; z ) = Pr[X 6= Z ]
= Pr[(X 6= ^ = Z ) _ (X =
Pr[X 6= _ 6= Z ]
Pr[X 6= ] + Pr[Y 6= Z ]
= d(x; y) + d(y; z ):

^ 6= Z )]

[0; 1]-valued functions f; g defined X X , next define
D(f; g) =

X
d(f (u; v); g(u; v)):
u;v:u=
6v

Clearly, also satisfies triangle inequality.
Let F characteristic function F F : X X ! f0; 1g F (u; v) = 1
(u; v) 2 F . definition Loss DISAGREE,

jF j Loss(R ; F )

2

= ( R ; F )
D(R; PREF) + D(PREF; F )
= DISAGREE(; PREF) + jF j Loss(PREF; F ):

Notice learning algorithm Hedge minimizes second term right hand
side Eq. (3). Below, consider problem finding ordering minimizes
first term, namely, DISAGREE.
249

fiCohen, Schapire, & Singer
4. Ordering Instances Preference Function

4.1 Measuring Quality Ordering
consider complexity finding total order agrees best learned
preference function. analyze this, must first quantify notion agreement
preference function PREF ordering. One natural notion following: Let X
set, PREF preference function, let total ordering X , expressed
ordering function (i.e., (u) > (v) u v order).
analysis section, convenient use measure AGREE(; PREF),
defined sum PREF(u; v) pairs u; v u ranked v :
AGREE(; PREF) =

X

u;v:(u)>(v)

PREF(u; v):

(4)

Clearly, AGREE linear transformation measure DISAGREE introduced Eq. (2),
hence maximizing AGREE equivalent minimizing DISAGREE. definition
also closely related similarity metrics used decision theory information processing (Kemeny & Snell, 1962; Fishburn, 1970; Roberts, 1979; French, 1989; Yao, 1995) (see
discussion Sec. 6).

4.2 Finding Optimal Ordering Hard
Ideally one would like find maximizes AGREE(; PREF). general optimization problem little interest setting, since many constraints
preference function imposed learning algorithm. Using learning algorithm Sec. 3, instance, PREF always linear combination simpler functions.
However, theorem shows optimization problem NP-complete even
PREF restricted linear combination well-behaved preference functions. particular, problem NP-complete even primitive preference functions used
linear combination rank orderings map set three elements,
one may may ?. (Clearly, consists three elements
problem still hard.)

Theorem 3 following decision problem NP-complete set jS j 3:
Input: rational number ; set X ; collection N ordering functions fi : X ! ;
preference function PREF defined
PREF(u; v) =

N
X
i=1

wi Rf (u; v)

(5)



P

w = (w1 ; : : : ; wN ) rational weight vector [0; 1]N N
i=1 wi = 1.
Question: exist total order AGREE(; PREF) ?

Proof: problem clearly NP since nondeterministic algorithm guess total

order check weighted number agreements polynomial time.
prove problem NP-hard reduce CYCLIC-ORDERING (Galil &
Megido, 1977; Gary & Johnson, 1979), defined follows: \Given set collection
250

fiLearning Order Things
C ordered triples (a; b; c) distinct elements A, one-to-one function
f : ! f1; 2; : : : ; jAjg (a; b; c) 2 C either f (a) > f (b) > f (c)
f (b) > f (c) > f (a) f (c) > f (a) > f (b)?"
Without loss generality, either f0; 1; ?g f0; 1; 2g. first show
problem finding optimal total order hard = f0; 1; ?g. Given instance
CYCLIC-ORDERING, let X = A. triplet = (a; b; c) introduce three
ordering functions ft;1 , ft;2 , ft;3 , define ft;1 (a) > ft;1 (b), ft;2 (b) >
ft;2 (c), ft;3 (c) > ft;3 (a). this, let ft;1 (a) = ft;2 (b) = ft;3 (c) = 1, ft;1 (b) =
ft;2 (c) = ft;3 (a) = 0, ft;i () = ? cases. let weight vector uniform,
wt;i = 3j1C j . Let
=

P

5 jAj(jAj 1)=2
+
3
2

3

:

Define Rt (u; v) = 3i=1 wt;i Rf (u; v), contribution three functions
PREF(u; v). Notice triplet = (a; b; c) 2 C , Rt (a; b) = 3j2C j whereas
Rt (b; a) = 3j1C j , similarly b; c c; a. addition, pair u; v 2
least one appear t, get Rt (u; v) = 2j1C j . Since total order
satisfy two three conditions (a) > (b), (b) > (c), (c) > (a),
largest possible weighted number agreements associated triple exactly
=jC j.
number weighted agreements least , must exactly , argument
above; exactly weighted agreements, total order must satisfy
exactly 2 possible 3 relations three elements form triplet
C . Thus, constructed rank ordering instance positive original
CYCLIC-ORDERING instance positive.
case = f0; 1; 2g uses similar construction; however, triplet =
(a; b; c), define six ordering functions, ft;j 1 , ft;j 2 , ft;j 3 , j 2 f0; 1g. basic
0 f 1 , agree single
idea replace ft;i two functions, ft;i
t;i
ordering constraint associated ft;i , disagree orderings. instance,
define functions ft;j 1 (a) > ft;j 1 (b) j = 0 j = 1,
pairs u; v, ft;11 (u) > ft;11 (v) iff ft;01 (v) > ft;01 (u). Averaging two orderings ft;01 ft;11
thus yield preference expressed original function ft;1 (i.e., preference
> b only).
detail, let ft;j 1 (a) = ft;j 2 (b) = ft;j 3 (c) = 2 j , ft;j 1 (b) = ft;j 2 (c) = ft;j 3 (a) = 1 j ,
j
ft;i
() = 2j cases. let weight vector uniform,
P
j
wt;i = 6j1C j . Similar first case, define Rt (u; v) = i;j wt;i Rf (u; v).
verified Rt identical Rt constructed first case. Therefore,
argument, constructed rank ordering instance positive original
CYCLIC-ORDERING instance positive. 2
Although problem hard jS j 3, next theorem shows becomes
tractable linear combinations rank orderings set size two. course,
jS j = 2, rank orderings really binary classifiers. fact special
case tractable underscores fact manipulating orderings (even relatively simple
t;i

j
t;i

251

fiCohen, Schapire, & Singer
ones) computationally dicult performing corresponding operations
binary classifiers.

Theorem 4 following optimization problem solvable linear time:
Input: set X ; set jS j = 2; collection N ordering functions fi : X ! ;
preference function PREF defined Eq. (5).
Output: total order defined ordering function maximizes
AGREE(; PREF).
Assume
P without loss generality two-element set f0; 1g,
define (u) = wi fi(u). show total order1 consistent maximizes
AGREE(; PREF). Fix pair u; v 2 X let

Proof:

qb1 b2 =

X



s.t.

fi (u)=b1 ;fi (v)=b2

wi :

rewrite PREF
(u) = q10 + q11
(v) = q01 + q11

PREF(u; v) = q10 + 21 q11 + 21 q00
PREF(v; u) = q01 + 12 q11 + 12 q00 :

Note (u) (v) PREF(u; v) PREF(v; u) equal q10 q01 . Hence,
(u) > (v) PREF(u; v) > PREF(v; u). Therefore, pair u; v 2 X , order
defined agrees pairs pairwise preference defined PREF.
words, shown
AGREE(; PREF) =

X
maxfPREF(u; v); PREF(v; u)g
fu;vg

(6)

sum unordered pairs. Clearly, right hand side Eq. (6) maximizes
right hand side Eq. (4) since one (u; v) (v; u) included
latter sum. 2

4.3 Finding Approximately Optimal Ordering
Theorem 3 implies unlikely find ecient algorithm finds optimal
total order weighted combination rank orderings. Fortunately, exist ecient algorithms finding approximately optimal total order. fact, finding good
total order closely related problem finding minimum feedback arc set,
exist good approximation algorithms; see, instance, (Shmoys, 1997)
references therein. However, algorithms achieve good approximation results minimum feedback arc set problem based (or approximate)
linear-programming relaxation (Seymour, 1995; Even, Naor, Rao, & Schieber, 1996; Berger
& Shor, 1997; Even, Naor, Schieber, & Sudan, 1998) rather complex implement
quite slow practice.
1. Notice case tie, (u) = (v ) distinct u; v , defines partial order.
theorem holds total order consistent partial order, i.e.,
(u) > (v ) ) (u) > (v ).
0

0

0

252

fiLearning Order Things

Algorithm Greedy-Order
Inputs: instance set X ; preference function PREF
Output: approximately optimal ordering function ^
let V = X
P
P PREF(u; v)
v 2 V (v) = u2V PREF(v; u)
u2V
V non-empty
let = arg maxu2V (u)
let ^(t) = jV j
V = V ftg
v 2 V (v) = (v) + PREF(t; v)

endwhile

PREF(v; t)

Figure 3: greedy ordering algorithm.
describe instead simple greedy algorithm simple implement. Figure 3 summarizes greedy algorithm. shortly demonstrate, algorithm
produces good approximation best total order.
algorithm easiest describe thinking PREF directed weighted graph,
initially, set vertices V equal set instances X , edge u ! v
weight PREF(u; v). assign vertex v 2 V potential value (v),
weighted sum outgoing edges minus weighted sum ingoing edges. is,
(v ) =

X
X
PREF(v; u)
PREF(u; v) :
u2V
u2V

greedy algorithm picks node maximum potential2 , assigns
rank setting ^(t) = jV j, effectively ordering ahead remaining nodes.
node, together incident edges, deleted graph, potential
values remaining vertices updated appropriately. process repeated
graph empty. Notice nodes removed subsequent iterations
progressively smaller smaller ranks.
example, consider preference function defined leftmost graph Fig. 4.
(This graph identical weighted combination two ordering functions
Fig. 1.) initial potentials algorithm assigns are: (b) = 2, (d) = 3=2, (c) = 5=4,
(a) = 9=4. Hence, b maximal potential. given rank 4, node b
incident edges removed graph.
result middle graph Fig. 4. deleting b, potentials remaining
nodes updated: (d) = 3=2, (c) = 1=4, (a) = 5=4. Thus, assigned
rank jV j = 3 removed graph, resulting rightmost graph Fig. 4.
updating potentials again, (c) = 1=2 (a) = 1=2. c assigned
rank jV j = 2 removed, resulting graph containing single node a,
2. Ties broken arbitrarily case two nodes potential.

253

fiCohen, Schapire, & Singer
1/4

1/4
3/4



1/8

1

b

3/4



c

1/4

1/8

7/8 1/8
1

7/8



c

3/4

c

7/8 1/8
7/8





Figure 4: Behavior greedy ordering algorithm. leftmost graph original
input. graph, node b assigned maximal rank deleted,
leading middle graph; graph, node deleted, leading
rightmost graph. rightmost graph, node c ranked ahead node a,
leading total ordering b > > c > a.
finally assigned rank jV j = 1. ordering produced greedy algorithm
thus b > > c > a.
next theorem shows greedy algorithm comes within factor two
optimal.

Theorem 5 Let OPT(PREF) weighted agreement achieved optimal total order
preference function PREF, let APPROX(PREF) weighted agreement
achieved greedy algorithm.
1
APPROX(PREF) OPT(PREF) :
2

Proof:

Consider edges incident node vj selected
j -th repetition loop Figure 3. ordering produced algorithm
agree outgoing edges vj disagree ingoing edges. Let
aj sum weights outgoing edges vj , dj sum weights
P
ingoing edges. Clearly APPROX(PREF) jjV=1j aj . However, every repetition,
total weight
Pall incoming edges must equal total weight outgoing edges.
means v2V (v) = 0, hence node v? maximal potential,
(v? ) 0. Thus every repetition j , must aj dj ,
OPT(PREF)



jV j
X
j =1

(aj + dj )



jV j
X

(aj + aj )

j =1

2 APPROX(PREF):

first inequality holds OPT(PREF) best include every edge graph,
since every edge removed exactly once, edge must contribute aj
dj . 2
254

fiLearning Order Things
1

2
1
k+2
2
k

k+3
k+1
k+1

k+2

2k+3
k

2k+3

Figure 5: example graph (left) node-based greedy algorithm achieves
approximation factor 21 constructing partial order right.

passing, note natural greedy algorithms achieve
good approximations. Consider, example, algorithm starts graph consisting nodes edges, iteratively adds highest weighted edge
graph, avoiding cycles. shown algorithm produce
poor partial order, given adversarially chosen graph; cases optimal
total order achieves multiplicative factor O(jV j) weighted agreements
\edge-based" greedy algorithm.

4.4 Improvements Greedy Algorithm
approximation factor two given Theorem 5 tight. is, exist problems
greedy algorithm approximation worse optimal solution
factor arbitrarily close two. Consider graph shown left-hand side Fig. 5.
optimal total order ranks instances according position figure, left right,
breaking ties randomly, achieves OPT(PREF) = 2k +2 weighted agreements. However,
greedy algorithm picks node labeled k + 1 first orders remaining nodes
randomly, achieving APPROX(PREF) = k + 2 agreements. large k, ratio
APPROX(PREF)=OPT(PREF) approaches 12 .
graph Figure 5, another simple algorithm produces optimal
ordering: since graph already partial order, picking total order consistent
partial order gives optimal result. cope problems one Figure 5,
devised improvement greedy algorithm combines greedy method
topological sorting. aim improvement find better approximations graphs
composed many strongly connected components.
before, modified algorithm easiest describe thinking PREF
weighted directed graph. Recall pair nodes u v, exist two edges:
one u v weight PREF(u; v) one v u weight PREF(v; u).
modified greedy algorithm pre-process graph. pair nodes,
255

fiCohen, Schapire, & Singer
Algorithm SCC-Greedy-Order
Inputs: instance set X ; preference function PREF
Output: approximately optimal ordering function ^
Define PREF0 (u; v) = maxfPREF(u; v) PREF(v; u); 0g :
Find strongly connected components U1 ; : : : ; Uk graph G = (V; E )
V = X E = f(u; v) j PREF0 (u; v) > 0g :

Order strongly connected components way consistent partial order
<scc :

U <scc U 0 iff 9u 2 U; u0 2 U 0 : (u; u0 ) 2 E

Use algorithm Greedy-Order full enumeration order instances within component Ui according PREF0 .

Figure 6: improved greedy ordering algorithm.
remove edge smaller weight set weight edge

j PREF(v; u)

PREF(u; v) j :

special case PREF(v; u) = PREF(u; v) = 12 , remove edges.
reduced graph, one directed edge pair nodes. Note
greedy algorithm would behave identically transformed graph since based
weighted differences incoming outgoing edges.
next find strongly connected components3 reduced graph, ignoring (for
now) weights. One split edges reduced graph two classes: intercomponent edges connect nodes u v, u v different strongly connected
components; intra-component edges connect nodes u v strongly
connected component. straightforward verify optimal order agrees
inter-component edges. Put another way, edge node u node v
two different connected components reduced graph, (u) > (v) optimal
total order .
first step improved algorithm thus totally order strongly connected
components way consistent partial order defined inter-component
edges. precisely, pick total ordering components consistent
partial order <scc , defined follows: components U U 0 , U <scc U 0 iff
edge node u 2 U node u0 2 U 0 reduced graph.
next order nodes within strongly connected component, thus providing
total order nodes. greedy algorithm used. alternative,
cases component contains elements (say five), one find
optimal order elements component brute-force approach, i.e.,
full enumeration permutations.
3. Two nodes u v strongly connected component iff directed paths u
v v u.

256

fiLearning Order Things
0.4

b



0.2



b



0.6

0.2

b

0.45

0.5
0.95

0.9

0.45

0.55
0.05
0.5

0.55
0.65

c


0.35

0.9

0.1

)

0.1

0.3

c



0.2
b

c

0.3



0.9

)

0.1

c

0.1

0.3





0.1

Figure 7: illustration approximation algorithm finding total order
weighted combination ordering functions. original graph (top left)
reduced removing least one edge edge-pair (u; v) (v; u) (middle).
strongly connected components found (right). Finally, ordering
found within strongly connected component yield order b > c >
> (bottom).
improved algorithm summarized Figure 6 illustrated Figure 7.
four elements Figure 7 constitute two strongly connected components
reduced graph (fbg fa; c; dg). Therefore, b assigned top rank ranked
a, c d. brute-force algorithm used order components, would
check 3! permutations a, c output total order b > c > > a,
optimal order toy example.
worst case, reduced graph contains single strongly connected component. case, improved algorithm generates ordering greedy
algorithm. However, experiments metasearch problems described Sec. 5, many
strongly connected components small; average size strongly connected
component less five. cases these, improved algorithm often
improve simple greedy algorithm.

4.5 Experiments Ordering Algorithms
Ideally, algorithm would evaluated determining closely approximates
optimal ordering large, realistic problems. Unfortunately, finding optimal ordering
large graphs impractical. thus performed two sets experiments ordering
algorithms described above. first set experiments, evaluated algorithms
small graphs|specifically, graphs optimal ordering could feasibly found
brute-force enumeration. experiments, measure \goodness"
resulting orderings relative optimal ordering. second set experiments,
evaluated algorithms large graphs optimal orderings unknown.
experiments, compute \goodness" measure depends total weight
edges, rather optimal ordering.
257

fiCohen, Schapire, & Singer
addition simple greedy algorithm improvement, also considered
following simple randomized algorithm: pick permutation random, output
better permutation reverse. easily shown algorithm
achieves approximation bound expected performance greedy algorithm.
(Brie y, one two permutations must agree least half weighted edges
graph.) random algorithm improved repeating process, i.e.,
examining many random permutations reverses, choosing permutation
achieves largest number weighted agreements.
first set experiments, compared performance greedy approximation
algorithm, improved algorithm first finds strongly connected components,
randomized algorithm graphs nine fewer elements. number elements,
generated 10;000 random graphs choosing PREF(u; v) uniformly random, setting
PREF(v; u) 1 PREF(u; v). randomized algorithm, evaluated 10n random
permutations (and reverses) n number instances (nodes).
fair comparison different algorithms smaller graphs, always used
greedy algorithm (rather brute-force algorithm) order elements
strongly connected component graph.
evaluate algorithms, examined reduced graph calculated average
ratio weights edges chosen approximation algorithm weights
edges chosen optimal order. precisely, let optimal order
^ order chosen approximation algorithm. random graph,
calculated
X
maxfPREF(u; v) PREF(v; u); 0g
u; v : ^(u) > ^(v)
X
:
maxfPREF(u; v) PREF(v; u); 0g
u; v : (u) > (v)
measure 0.9, instance, total weight edges total order
picked approximation algorithm 90% corresponding figure optimal
algorithm.
averaged ratios random graphs size. results
shown left hand side Figure 8. right hand side figure,
show average running time algorithms function number
elements. number ranked elements five, greedy algorithms
outperform randomized algorithm, running time much smaller. Thus,
full enumeration used find optimal order small strongly connected
components, approximation would consistently better randomized
algorithm.
note greedy algorithm also generally performs better average
lower bound given Theorem 5. fact, combining greedy algorithm prepartitioning graph strongly connected components often yields optimal order.
second set experiments, measured performance running time larger
random graphs. Since large graphs cannot find optimal solution brute-force
enumeration, use \goodness" measure ratio weights edges
left reduced graph applying approximation algorithm total weight
258

fiLearning Order Things

0.08
1

Greedy
SCC + Greedy
Randomized

Greedy
SCC + Greedy
Randomized

0.07

0.06

Running time (seconds)

Fraction optimal solution

0.98

0.96

0.94

0.92

0.05

0.04

0.03

0.02
0.9
0.01

0.88

3

4

5

6

7

8

0

9

3

4

5

Number elements

6

7

8

9

Number elements

Figure 8: Comparison goodness (left) running time (right) approximations
achieved greedy algorithms randomized algorithm function
number ranked elements random preference functions 3 9
elements.

0.45

Greedy
SCC + Greedy
Randomized

0.95

0.9

0.35

Running time (seconds)

Fraction total weight

Greedy
SCC + Greedy
Randomized

0.4

0.85

0.8

0.75

0.7

0.3

0.25

0.2

0.15

0.1
0.65
0.05
0.6
0
5

10

15

20

25

30

5

Number elements

10

15

20

25

30

Number elements

Figure 9: Comparison goodness (left) running time (right) approximations
achieved greedy algorithms randomized algorithm function
number ranked elements random preference functions 3 30
elements. Note graphs Greedy SCC+Greedy coincide
points.

259

fiCohen, Schapire, & Singer
edges graph. is, random graph calculated

X

maxfPREF(u; v)

u; v : ^(u) > ^(v)
X
maxfPREF(u; v)
u; v

PREF(v; u); 0g

PREF(v; u); 0g

:

ran three algorithms parameters (i.e., 10;000 random
graphs). results given Figure 9. advantage greedy algorithms
randomized algorithm even apparent larger problems. Note also
large graphs performance two greedy algorithms indistinguishable.
mainly due fact large random graphs strongly connected high probability.
summarize experiments, six elements greedy algorithm
clearly outperforms randomized algorithm even many randomly chosen permutations
examined. Furthermore, improved algorithm first finds strongly connected
components outperforms randomized algorithm graph sizes. practice
improved greedy algorithm achieves good approximations|within 5 percent
optimal, cases optimal graphs feasibly found.
5. Experimental Results Metasearch

far, described method learning preference function, means
converting preference function ordering new instances. present
experimental results learning order. particular, describe results
learning combine orderings several web \search experts" using algorithm
Figure 2 learn preference function, simple greedy algorithm order instances
using learned preference function. goals experiments illustrate
type problems solved method; empirically evaluate learning
method; evaluate ordering algorithm large, non-random graphs, might arise
realistic application; confirm theoretical results preceding sections.
thus restrict comparing learned orderings individual search experts,
suggested Theorem 1, rather attempt compare application learningto-order previous experimental techniques metasearch, e.g., (Lochbaum & Streeter,
1989; Kantor, 1994; Boyan, Freitag, & Joachims, 1994; Bartell, Cottrell, & Belew, 1994).
note metasearch problem exhibits several properties suggest general
approach ours. instance, approaches learn combine similarity scores
applicable, since similarity scores web search engines often unavailable.
experiments presented here, learning algorithm provided ordered lists
search engine without associated scores. demonstrate merits
approach, also describe experiments partial feedback|that is, preference
judgments less informative relevance judgments typically used
improving search engines.
260

fiLearning Order Things

ML Search Experts
NAME
\NAME"
title:\NAME"
NAME +LASTNAME title:\home page"
NAME +LASTNAME title:homepage
NAME +LASTNAME machine learning
NAME +LASTNAME \machine learning"
NAME +LASTNAME case based reasoning
NAME +LASTNAME \case based reasoning"
NAME +LASTNAME PLACE
NAME +LASTNAME \PLACE"
NAME +LASTNAME url:index.html
NAME +LASTNAME url:home.html
NAME +LASTNAME url:~*LASTNAME*
NAME +LASTNAME url:~LASTNAME
NAME +LASTNAME url:LASTNAME

UNIV Search Experts
NAME
\NAME"
\NAME" PLACE
title:NAME
title:\NAME"
title:\NAME" PLACE
NAME title:\home page"
NAME title:\homepage"
NAME welcome
NAME url:index.html
NAME url:home.html
\NAME" title:\home page"
\NAME" title:\homepage"
\NAME" welcome
\NAME" url:index.html
\NAME" url:home.html
\NAME" PLACE title:\home page"
\NAME" PLACE title:\homepage"
\NAME" PLACE welcome
\NAME" PLACE url:index.html
\NAME" PLACE url:home.html

Table 1: Search (and ranking) experts used metasearch experiments. associated queries, NAME replaced person's (or university's) full name,
LASTNAME person's last name, PLACE replaced person's aliation (or university's location). Sequences words enclosed quotes
must appear phrase, terms prefixed title: url: must appear
part web page. Words prefixed \+" must appear web
page; words may may appear.

5.1 Test Problems Encoding
chose simulate problem learning domain-specific search engine|i.e., engine
searches pages particular, narrow type. Ahoy! (Shakes, Langheinrich, & Etzioni,
1997) one instance domain-specific search engine. test cases, picked two
problems: retrieving home pages machine learning researchers (ML), retrieving
home pages universities (UNIV). obtain sample queries, obtained listing
machine learning researchers, identified name aliated institution, together
home pages,4 similar list universities, identified name (sometimes)
geographical location.5 entry list viewed query, associated
URL sole relevant web page.
4. http://www.aic.nrl.navy.mil/aha/research/machine-learning.html, list maintained David
Aha.
5. Yahoo!

261

fiCohen, Schapire, & Singer
constructed series special-purpose \search experts" domain.
implemented query expansion methods converted name/aliation pair (or
name/location pair) likely-seeming Altavista query. example, one expert
UNIV domain searched university name appearing phrase, together
phrase \home page" title; another expert ML domain searched
words person's name plus words \machine" \learning," enforces
strict requirement person's last name appear. Overall, defined 16 search
experts ML domain 22 UNIV domain; summarized Table 1.
search expert returned top 30 ranked web pages. ML domain,
210 searches least one search expert returned named home page;
UNIV domain, 290 searches. task learning system find
appropriate way combining output search experts.
give precise description search experts, query t, first
constructed set X consisting web pages returned expanded queries
defined search experts. Next, search expert represented preference
function Rit . chose preference functions rank orderings defined respect
ordering function fit natural way: assigned rank fit = 30 first
listed page, fit = 29 second-listed page, on, finally assigning rank fit = 0
every page retrieved top 30 expanded query associated expert i.
encode feedback, considered two schemes. first, simulated complete
relevance feedback|that is, query, constructed feedback sole
relevant page preferred pages. second, simulated sort
feedback could collected \click data"|i.e., observing user's interactions
metasearch system. query, presenting ranked list pages, noted
rank one relevant web page. constructed feedback ranking
relevant page preferred preceding pages. would correspond observing
link user actually followed, making assumption link preferred
previous links.
emphasized forms feedback simulated, contain
less noise would expected real user data. reality fraction
relevance feedback would missing erroneous, fraction click data would
satisfy assumption stated above.

5.2 Evaluation Results
evaluate expected performance fully-trained system novel queries
domain, employed leave-one-out testing. query t, trained learning system
queries, recorded rank learned system query t.
complete relevance feedback, rank invariant ordering training examples,
\click data" feedback, not; feedback collected stage depends
behavior partially learned system, turn depends previous training
examples. Thus click data training, trained 100 randomly chosen permutations
training data recorded median rank t.
262

fiLearning Order Things
5.2.1 Performance Relative Individual Experts

theoretical results provide guarantee performance relative performance
best individual search (ranking) expert. therefore natural consider comparing
performance learned system best individual experts. However,
search expert, top 30 ranked web pages query known; single
relevant page query among top 30, impossible compute
natural measures performance query. complicates comparison
learned system individual search experts.
However, spite incomplete information performance search
experts, usually possible tell learned system ranks web page higher
particular expert.6 Motivated this, performed sign test: compared rank
learning systems rank given search expert, checking see whether
rank lower, discarding queries comparison impossible.
used normal approximation binomial distribution test following two null
hypotheses (where probability taken distribution queries
drawn):

H1. probability least 0.5, search expert performs better learning
system (i.e., gives lower rank relevant page learning system does.)
H2. probability least 0.5, search expert performs worse learning
system (i.e., gives equal lower rank relevant page.)
training, explored learning rates range [0:001; 0:999]. complete feedback
ML domain, hypothesis H1 rejected high confidence (p > 0:999) every
search expert every learning rate 0:01 fi 0:99. holds UNIV domain
learning rates 0:02 fi 0:99. results click data training nearly strong,
except 2 22 search experts UNIV domain show greater sensitivity
learning rate: engines, H1 rejected high confidence
0:3 fi 0:6. summarize, high confidence, domains, learned ranking
system worse individual search expert moderate values fi .
Hypothesis H2 stringent since rejected sure
learned system strictly better expert. complete feedback ML domain
0:3 fi 0:8, hypothesis H2 rejected confidence p > 0:999 14 16
search experts. remaining two experts learned system perform better
often, difference significant. UNIV domain, results similar.
0:2 fi 0:99, hypothesis H2 rejected confidence p > 0:999 21 22
search experts, learned engine tends perform better single remaining
expert.
Again, results click data training slightly weaker. ML domain,
hypothesis H2 rejected three experts extreme learning
rates; UNIV domain, hypothesis H2 rejected two experts 0:4
fi 0:6. remaining experts learning rates differences statistically
6. time cannot determined neither learned system expert ranks
relevant web pages top 30, case little practical interest.

263

fiCohen, Schapire, & Singer
significant; however, always case learned engine tends perform
better.
summarize experiments, moderate values fi learned system is,
high confidence, strictly better search experts domains, never
significantly worse expert. trained full relevance judgments, learned
system performs better average individual expert.
5.2.2 Performance Measures

measured number queries correct web page top k ranked
pages, various values k. results shown Figure 10. lines show
performance learned systems (with fi = 0:5, generally favorable learning rate)
points correspond individual experts. cases, learned system closely
tracks performance best expert every value k. especially interesting
since single expert best values k.
final graph figure investigates sensitivity measure learning
rate fi . representative illustration, varied fi ML domain plotted
top-k performance system learned complete feedback three values k. Note
performance roughly comparable wide range values fi .
Another plausible measure performance average rank (single) relevant
web page. computed approximation average rank artificially assigning rank
31 every page either unranked, ranked rank 30. (The latter case
fair learned system, one rank greater 30
possible.) summary results fi = 0:5 given Table 2, together
additional data top-k performance. table, give top-k performance three
values k, average rank several ranking systems: two learned systems; naive
query, i.e., person university's name; single search expert performed
best respect performance measure. Note experts
distinct since several experts scored best one measure.
table illustrates robustness learned systems, nearly always
competitive best expert every performance measure listed. exception
system trained click data trails best expert top-k performance
small values k. also worth noting domains, naive query (simply
person university's name) effective: even weaker click data feedback,
learned system achieves 36% decrease average rank naive query ML
domain, 46% decrease UNIV domain.
summarize experiments, domains learned system performs
much better naive search strategies, also consistently performs least well as,
perhaps slightly better than, single domain-specific search expert. observation
holds regardless performance metric considered; nearly every metric computed,
learned system always equals, usually exceeds, performance search expert
best metric. Finally, performance learned system almost
good weaker \click data" training complete relevance feedback.
264

fiLearning Order Things
ML: queries answered top k
250

Learned System - Full feedback
Learned System - Click data
Individual Rankers

# queries top k

200
150
100
50
0
0

5

10

15

20

25

30

35

k
UNIV: queries answered top k
Learned System - Full feedback
Learned System - Click data
Individual Rankers

# queries top k

300
250
200
150
100
50
0
0

5

10

15

20

25

30

35

k

0.85
0.80

k=8

% Relevant

0.75
0.70
0.65

k=4

0.60
0.55
0.50

k=1

0.45
0.40
0

0.2

0.4
0.6
Learning Rate

0.8

1

Figure 10: Top middle: Performance learned system versus individual experts
two different domains. Bottom: percentage time relevant web page
top-k list k = 1,4, 8.
265

fiCohen, Schapire, & Singer

Learned (Full Feed.)
Learned (Click Data)
Naive
Best (Top 1)
Best (Top 10)
Best (Top 30)
Best (Avg Rank)

Top 1
114
93
89
119

114
97
114

ML Domain
Top 10 Top 30

Avg Rank

185

198

4.9

185

198

4.9

165
170
182
181
182

176
184
190
194
190

7.7
6.7
5.3
5.6
5.3

Top 1
111
87
79
112

111
111
111

University Domain
Top 10 Top 30 Avg Rank
225
253
7.8
229

157
221
223
223
223

259

191
247
249
249
249

7.8

14.4
8.2
8.0
8.0
8.0

Table 2: Comparison learned systems individual search queries.

6. Related Work

Problems involve ordering ranking investigated various fields
decision theory, social sciences, information retrieval mathematical economics (Black,
1958; Kemeny & Snell, 1962; Cooper, 1968; Fishburn, 1970; Roberts, 1979; Salton & McGill,
1983; French, 1989; Yao, 1995). Among wealth literature subject, closest
appears work Kemeny Snell (1962) extended Yao (1995)
used Balabanovc Shoham (1997) FAB collaborative filtering system.
works use similar notion ordering functions feedback; however, assume
ordering functions feedback complete transitive. Hence,
possible leave elements unranked, inconsistent feedback violates
transitivity requirements. therefore dicult combine fuse inconsistent
incomplete orderings Kemeny Snell model.
also several related intractability results. concerned
diculty reaching consensus voting systems based preference ordering. Specifically,
Bartholdi, Tovey Trick (1989) study problem finding winner election
preferences voters irre exive, antisymmetric, transitive, complete.
Thus, setting restrictive ours. study two similar schemes decide
winner election. first invented Dodgson (1876) (better known
pen name, Lewis Carroll) second due Kemeny (1959). models,
show problem finding winner election NP-hard. Among
two models, one suggested Kemeny closest ours. However, mentioned
above, model restrictive allow voters abstain (preferences
required complete) inconsistent (all preferences transitive).
illustrated experiments, problem learning rank closely related
problem combining results different search engines. Many methods
proposed information retrieval community, many adaptive, using relevance judgments make appropriate choice parameters. However,
generally, rankings combined combining scores used rank documents (Lochbaum & Streeter, 1989; Kantor, 1994). also frequently assumed
properties objects (documents) ranked available, word frequencies.
contrast, experiments, instances atomic entities associated properties
except position various rank-orderings. Similarly, make minimal assump266

fiLearning Order Things
tions rank-orderings|in particular, assume scores available.
methods thus applicable broader class ranking problems.
General optimization methods also adopted adjust parameters IR
system improve agreement set user-given preference judgments. instance, Boyan, Freitag, Joachims (1994) use simulated annealing improve agreement
\click data," Bartell, Cottrell Belew (1994) use conjugate gradient descent
choose parameters linear combination scoring functions, associated
different search expert. Typically, approaches offer guarantees eciency,
optimality, generalization performance.
Another related task collection fusion. Here, several searches executed disjoint
subsets large collection, results combined. Several approaches problem rely combining ranking scores described (Towell, Voorhees,
Gupta, & Johnson-Laird, 1995; Voorhees, Gupta, & Johnson-Laird, 1994). However, although problem superficially similar one presented here, assumption
different search engines index disjoint sets documents actually makes problem
quite different. particular, since impossible two engines give different relative
orderings pair documents, combining rankings done relatively
easily.
Etzioni et al. (1996) formally considered another aspect metasearch|the task
optimally combining information sources associated costs time delays. formal
results disjoint theirs, assume every query single recognizable
correct answer, rendering ordering issues unimportant.
many applications machine learning, reinforcement learning, neural
networks, collaborative filtering employ ranking preferences, e.g., (Utgoff &
Saxena, 1987; Utgoff & Clouse, 1991; Caruana, Baluja, & Mitchell, 1996; Resnick & Varian,
1997), work directly relevant, might possible use framework
suggested paper similar settings. one future research goals.
Finally, would like note framework algorithms presented paper
extended several ways. current research focuses ecient batch algorithms
combining preference functions, using restricted ranking experts
problem finding optimal total ordering solved polyomial time (Freund, Iyer,
Schapire, & Singer, 1998).
7. Conclusions

many applications, desirable order rather classify instances. investigated
two-stage approach learning order one first learns preference function
conventional means, orders new set instances finding total ordering
best approximates preference function. preference function learned
binary function PREF(u; v), returns measure confidence ecting likely
u preferred v. learned set \experts" suggest specific
orderings, user feedback form assertions form \u preferred
v".
presented two sets results problem. First, presented online
learning algorithm learning weighted combination ranking experts based
267

fiCohen, Schapire, & Singer
adaptation Freund Schapire's Hedge algorithm. Second, explored
complexity problem finding total ordering agrees best preference
function. showed problem NP-complete even highly restrictive case,
namely, preference predicates linear combinations certain class well-behaved
\experts" called rank orderings. However, also showed preference predicate,
greedy algorithm always obtains total ordering within factor
two optimal. also presented algorithm first divides set instances
strongly connected components uses greedy algorithm (or full enumeration,
small components) find approximately good order within large strongly connected
components. found approximation algorithm works well practice
often finds best order.
also presented experimental results algorithms used combine
results number \search experts," corresponds domain-specific
strategy searching web. showed two domains, learned system closely
tracks often exceeds performance best search experts. results
hold either traditional relevance feedback models learning, weaker feedback
form simulated \click data." performance learned systems also clearly
exceeds performance naive approaches searching.
Acknowledgments

would like thank Noga Alon, Edith Cohen, Dana Ron, Rick Vohra numerous
helpful discussions. extended abstract paper appeared Advances Neural
Information Processing Systems 10, MIT Press, 1998.
References

Balabanovc, M., & Shoham, Y. (1997). FAB: Content-based, collaborative recommendation. Communications ACM, 40 (3), 66{72.
Bartell, B., Cottrell, G., & Belew, R. (1994). Automatic combination multiple ranked
retrieval systems. Seventeenth Annual International ACM SIGIR Conference
Research Development Information Retrieval.
Bartholdi, J., Tovey, C., & Trick, M. (1989). Voting schemes dicult
tell elections. Social Choice Welfare, 6, 157{165.
Berger, B., & Shor, P. (1997). Tight bounds acyclic subgraph problem. Journal
Algorithms, 25, 1{18.
Black, D. (1958). Theory Committees Elections. Cambridge University Press.
Boyan, J., Freitag, D., & Joachims, T. (1994). machine learning architecture optimizing web search engines. Tech. rep. WS-96-05, American Association Artificial
Intelligence.
268

fiLearning Order Things
Caruana, R., Baluja, S., & Mitchell, T. (1996). Using future `Sort Out' present:
Rankprop multitask learning medical risk evaluation. Advances Neural
Information Processing Systems (NIPS) 8.
Cooper, W. (1968). Expected search length: single measure retrieval effectiveness
based weak ordering action retrieval systems. American Documentation, 19,
30{41.
Dodgson, C. (1876). method taking votes two issues. Clarendon Press,
Oxford. Reprinted discussion (Black, 1958).
Etzioni, O., Hanks, S., Jiang, T., Karp, R. M., Madani, O., & Waarts, O. (1996). Ecient
information gathering internet. Proceedings 37th Annual Symposium Foundations Computer Science (FOCS-96) Burlington, Vermont. IEEE
Computer Society Press.
Even, G., Naor, J., Rao, S., & Schieber, B. (1996). Divide-and-conquer approximation algorithms via spreading metrics. 36th Annual Symposium Foundations Computer
Science (FOCS-96), pp. 62{71 Burlington, Vermont. IEEE Computer Society Press.
Even, G., Naor, J., Schieber, B., & Sudan, M. (1998). Approximating minimum feedback
sets multicuts directed graphs. Algorithmica, 20 (2), 151{174.
Fishburn, F. (1970). Utility Theory Decision Making. Wiley, New York.
French, S. (1989). Decision Theory: Introduction Mathematics Rationality.
Ellis Horwood Series Mathematics Applications.
Freund, Y., Iyer, R., Schapire, R., & Singer, Y. (1998). ecient boosting algorithm
combining preferences. Machine Learning: Proceedings Fifteenth International Conference.
Freund, Y., & Schapire, R. (1997). decision-theoretic generalization on-line learning
application boosting. Journal Computer System Sciences, 55 (1),
119{139.
Galil, Z., & Megido, N. (1977). Cyclic ordering NP-complete. Theoretical Computer
Science, 5, 179{182.
Gary, M., & Johnson, D. (1979). Computers Intractability: Guide Theory
NP-completeness. W. H. Freeman Company, New York.
Kantor, P. (1994). Decision level data fusion routing documents TREC3
context: best case analysis worst case results. Proceedings third text
retrieval conference (TREC-3).
Kemeny, J. (1959). Mathematics without numbers. Daedalus, 88, 571{591.
Kemeny, J., & Snell, J. (1962). Mathematical Models Social Sciences. Blaisdell, New
York.
269

fiCohen, Schapire, & Singer
Littlestone, N. (1988). Learning quickly irrelevant attributes abound: new linearthreshold algorithm. Machine Learning, 2 (4).
Littlestone, N., & Warmuth, M. (1994). weighted majority algorithm. Information
Computation, 108 (2), 212{261.
Lochbaum, K., & Streeter, L. (1989). Comparing combining effectiveness latent semantic indexing ordinary vector space model information retrieval.
Information processing management, 25 (6), 665{676.
Resnick, P., & Varian, H. (1997). Introduction special section Recommender Systems.
Communication ACM, 40 (3).
Roberts, F. (1979). Measurement theory applications decision making, utility,
social sciences. Addison Wesley, Reading, MA.
Salton, G., & McGill, M. (1983). Introduction Modern Information Retrieval. McGrawHill.
Seymour, P. (1995). Packing directed circuits fractionally. Combinatorica, 15, 281{288.
Shakes, J., Langheinrich, M., & Etzioni, O. (1997). Dynamic reference sifting: case study
homepage domain. Proceedings WWW6.
Shmoys, D. (1997). Cut problems application divide-and-conquer.
Hochbaum, D. (Ed.), Approximation algorithms NP-Hard Problems. PWS Publishing Company, New York.
Towell, G., Voorhees, E., Gupta, N., & Johnson-Laird, B. (1995). Learning collection fusion
strategies information retrieval. Machine Learning: Proceedings Twelfth
International Conference Lake Taho, California. Morgan Kaufmann.
Utgoff, P., & Clouse, J. (1991). Two kinds training information evaluation function
learning. Proceedings Ninth National Conference Artificial Intelligence
(AAAI-91), pp. 596{600 Cambridge, MA. AAAI Press/MIT PRess.
Utgoff, P., & Saxena, S. (1987). Learning preference predicate. Proceedings
Fourth International Workshop Machine Learning, pp. 115{121 San Francisco,
CA. Morgan Kaufmann.
Voorhees, E., Gupta, N., & Johnson-Laird, B. (1994). collection fusion problem. Sev-

enteenth Annual International ACM SIGIR Conference Research Development
Information Retrieval.

Yao, Y. (1995). Measuring retrieval effectiveness based user preference documents.
Journal American Society Information Science, 46 (2), 133{145.

270

fiJournal Artificial Intelligence Research 10 (1999) 87-115

Submitted 9/98; published 2/99

Ecient Implementation Plan Graph STAN
Derek Long
Maria Fox

d.p.long@dur.ac.uk
maria.fox@dur.ac.uk

Department Computer Science
University Durham, UK

Abstract

Stan Graphplan-based planner, so-called uses variety STate ANalysis techniques enhance performance. Stan competed AIPS-98 planning
competition compared well competitors terms speed, finding
solutions fastest many problems posed. Although domain analysis techniques
Stan exploits important factor overall performance, believe speed
Stan solved competition problems largely due implementation
plan graph. implementation based two insights: many graph
construction operations implemented bit-level logical operations bit vectors,
graph explicitly constructed beyond fix point. paper
describes implementation Stan's plan graph provides experimental results
demonstrate circumstances advantages obtained using
implementation.

1. Introduction
Stan domain-independent planner STRIPS domains, based graph construc-

tion search method Graphplan (Blum & Furst, 1997). name derived
fact performs number preprocessing analyses, STate ANalyses, domain
planning, using Type Inference Module Tim described Fox Long (1998).
Stan competed AIPS-98 planning competition achieved excellent overall
performance rounds. results competition, found
URL given Appendix A, show Stan able solve problems notably
quickly could find optimal parallel solutions problems could
solved optimally planner competition, example Gripper
domain. problems posed competition give Stan much opportunity
exploit domain analysis techniques, performance due mainly underlying
implementation plan graph Stan constructs searches. detailed
discussion competition, competitors' point view, preparation.
design Stan's plan graph based two insights. First, observe action
pre- post-conditions represented using bit vectors. Checking mutual exclusion
pairs actions directly interact implemented using logical operations
bit vectors. Mutual exclusion (mutex relations) facts implemented
similar way. order best exploit bit vector representation information
construct two-layer graph called spike avoids unnecessary copying data
allows layer-dependent information node clearly separated layerindependent information node. spike allows us record mutex relations
c 1999 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiLong & Fox

using bit vectors, making mutex testing indirect interaction much ecient (we
distinguish direct indirect interaction Section 2.1). Second, observe
advantage explicit construction graph beyond stage
fix point reached. plan graph maintains wave front keeps track
goal sets remaining considered search. Since new facts, actions mutex
relations added beyond fix point goal sets considered without explicit
copying fact action layers. wave front mechanism allows Stan solve
large problem instances using fraction time space consumed Graphplan
Ipp (Koehler, Nebel, & Dimopoulos, 1997). example, using heuristic discussed
Section 5.1, Stan solve 10-disc Towers Hanoi problem (a 1023 step plan) less
9 minutes.
paper describe spike wave front mechanisms provide experimental
results indicating performance advantages obtained.

2. Spike Graph Structure
Graphplan (Blum & Furst, 1997) uses constraint satisfaction techniques search layered graph represents compressed reachability analysis domain. layers
correspond snapshots possible states instants time line initial
goal state. layer graph comprises set facts represents union
states reachable preceding layer. compression guarantees plan graph
constructed time polynomial number action instances domain.
expansion graph, solutions extracted, partially encoded
binary mutex relations computed construction layer. STAN implements
ecient representation graph wave front, discussed Section 4,
supports compression. Graphplan-style planners search plan, layer k,
involves selection exploration collection action choices see whether plan
constructed, using actions kth time step. plan found planner
backtracks action choices. Two important landmarks arise construction
plan graph. first point graph opens sense
problem becomes, principle, solvable. layer top level goals
first become pairwise non-mutex referred opening layer. second
fix point, referred level Blum Furst (1997), layer
changes made either action, fact mutex information recorded
graph layer.
original implementation Graphplan graph implemented alternating sequence layers fact nodes action nodes, arcs connecting actions
preconditions previous layer postconditions subsequent layer.
layers constructed explicitly involving repeated copying large portions
graph stage maintaining graph structure. copying due two
features graph. First, since actions satisfied preconditions one layer continue
satisfied preconditions subsequent layers, actions added
layer appear every successive action layer name
pre- post-conditions. Second, since facts achieved effects
action always achieved action, continue appear every
88

fiEfficient Implementation Plan Graph STAN

successive fact layer layer first appeared. Although layers
get deeper every successive stage duplicate information present previous
layer, small amount new information added every stage. proportion new material, relative copied material, decreases progressively graph
develops.
original Graphplan, mutex relations checked maintaining lists facts,
corresponding pre- post-conditions actions, checking membership
facts within lists. need copy information new layer, preand post-conditions actions duplicated even though information vary
layer layer (it determined point instantiation
schema). possible identify layer-independent information, node
graph, stored using different representation graph structure.
spike representation reimplements graph single fact array, called fact
spike, single action array, called action spike, divided ranks corresponding
layers original Graphplan graph structure. observations leading
compressed implementation plan graph made independently Smith Weld
(1998). Stan, fact rank consecutive sequence fact headers storing layerindependent information associated associated facts corresponding fact
layer. Similarly, action rank consecutive sequence action headers storing layerindependent information associated actions corresponding action layer.
header tuple containing, amongst things, name fact action
associated structure stores layer-dependent information relevant
fact action. case fact headers structure called fact level package
case action headers action level package. Figure 1 shows simple
graph structure viewed spike.
spike positions fact action headers fixed referred
indexing appropriate array. point, sizes arrays referred
using constant MaxSize, large number setting upper bound size
spike. vectors allocated also initialised size, although used
word-sized increments. saves effort re-allocating copying vectors
spike increases size towards MaxSize. define data types far introduced.

Definition 1 spike vector bit vector size MaxSize.
Definition 2 fact header tuple six components: name predicate

arguments comprise fact itself; index, i, giving position fact
fact array; bit mask spike vector ith bit set bits
unset; reference identifying achieving no-op; spike vector consumers bits set
actions use fact precondition fact level package storing
layer-dependent information fact.

Definition 3 action header tuple eight components: name action;

index, i, giving position action action array; bit mask
spike vector ith bit set bits unset; ag indicating whether
action no-op; three spike vectors, called precs, adds dels action level
package storing layer-dependent information action. bit precs, adds
89

fiLong & Fox

Fact Layer 0

P

Action Layer 1

Noop

P

Noop

Q

Q
R

Fact Layer 1

Noop
op1
op2

P
Noop
Q
Noop



Noop

U

Fact Layer 2

Noop

R



Action Layer 2

R


Noop

Noop
U

op1
op2
op3

V
W

Fact Spike
Fact Level Packages

rank 0

rank 1

Action Spike
Action Level Packages

P

Noop
P

Q

Noop
Q

R

Noop
R



op1



op2

U

Noop


V

Noop


W

Noop
U

rank 2

rank 1

rank 2

op3

Figure 1: Representation plan graph spike. fact spike, ranks 0, 1 2
correspond fact layers 0, 1 2 respectively. action spike, ranks 1
2 correspond action layers 1 2 respectively.

90

fiEfficient Implementation Plan Graph STAN

dels corresponds index fact array set precs fact
index precondition (and unset otherwise), adds fact index add list
element (and unset otherwise) dels fact index delete list element
action (and unset otherwise).

Definition 4 fact mutex vector (FMV) fact, f , spike vector bits
correspond indices fact array bit set corresponding fact
mutex f .
Definition 5 action mutex vector (AMV) action, a, spike vector

bits correspond indices action array bit set corresponding
action mutex a.

Definition 6 fact level package fact, f , array pairs, one rank

spike, containing fact mutex vector f vector achievers, called
achievement vector (AV), previous action rank.

Definition 7 action level package action, a, array triples, one
rank spike, containing action mutex vector list actions mutex
(MAs).
Using definitions provide detailed description spike construction process.

2.1 Spike Construction Process

make use header access functions following discussion:
mvec : fact ! fact mutex vector
precs : action ! precs
adds : action ! adds
dels : action ! dels
spike construction process takes place within loop stops goals
pairwise achievable, thereafter alternates search fix point reached
wave front mechanism takes over. use wave front discussed Section 4.
key component process rank construction algorithm builds fact
rank action rank extending previous fact action ranks spike.
action rank started adding no-ops fact headers previous fact
rank. soon added, fact headers updated refer, index
action rank, achieving no-ops. information allows Stan give preference,
searching, plans use no-op achieve goal rather
achiever. Graphplan preference ensured keeping no-ops top
graph layers considering achievers order search.
possible action instances considered. applicable action instances
enacted removed never reconsidered enactment.
identify mutex relations actions new action rank, facts
new fact rank.
91

fiLong & Fox

Graphplan, action instance applicable rank preconditions
present non-mutex previous rank. way preconditions tested
mutual exclusion Stan based bit vector representation fact mutex relations.
take logical fact mutex vectors preconditions, logically
result precondition vector action. result non-zero
mutex preconditions action applicable. test corresponds
checking whether action considered mutex - condition define
self-mutex.

Definition 8 action a, preconditions 1::a , self-mutex if:
(mvec(a 1) _ mvec(a 2) _ ::: _ mvec(a )) ^ precs (a)
p

p

pn

p

pn

non-zero.

applicable action enacted adding action header new rank setting
name name action bit mask record position spike.
Figures 2 3 no-ops given names facts achieve identified
no-ops ag components headers. allocate space action level
package create set pres, adds dels vectors. add new facts
add list action corresponding new fact rank. addition new facts
requires new fact headers initialised.
identify mutex actions mutex facts new ranks. Mutex actions
identified two phases. Actions non-mutex previous rank remain
non-mutex considered stage. First, existing action mutex relations
checked see whether hold new rank. Second, new action mutex relations
must deduced addition new actions construction rank. first
consider existing action mutex relations.
Two actions mutex, Graphplan, con icting add delete lists,
con icting precondition delete lists mutex preconditions. first two cases
actions directly, permanently, mutex never need re-tested although
mutex relationship must recorded rank. third case actions
indirectly, temporarily, mutex must retried subsequent ranks. keep track
actions retry order avoid unnecessary retesting. confirm two actions,
b, temporarily mutex previous rank still temporarily mutex
using following logical operations fact mutex vectors action preconditions.
first logically together mutex vectors a's preconditions result
precondition vector b. result non-zero b mutex.
procedure, expressed concisely Definition 9, identical checking
whether action self-mutex except that, case, result oring fact mutex
vectors preconditions one action anded precondition vector
action. Since mutex relations symmetric irrelevant action plays role
test.

Definition 9 Two actions (with n preconditions 1::a ) b temporarily mutex



p

pn

(mvec(a 1) _ mvec(a 2) _ ::: _ mvec(a )) ^ precs (b)
p

p

pn

92

fiEfficient Implementation Plan Graph STAN

non-zero.

consider new mutex relations inferred introduction
new actions. necessary check new actions actions spike.
check done one direction - low-indexed actions high-indexed actions
- test done pair. check permanent
temporary mutex relations. permanent mutex test done first, two actions
permanently mutex interest find also temporarily mutex.
Definition 10 provides logical operation used confirm two actions permanently
mutex. Temporary mutex relations checked using logical operation defined
Definition 9.
Definition 10 Two actions b permanently mutex result
((precs (a) _ adds (a)) ^ dels (b))_
((precs (b) _ adds (b)) ^ dels (a))
non-zero.

add mutex relations setting appropriate bits mutex vectors
new actions. done oring mutex vector first action
bit mask second action, vice versa. list mutex actions also maintained
use search spike.
refinement action mutex checking done Stan use record actions
whose preconditions lost mutex relations since last layer graph. record
enables Stan avoid retesting temporary mutex relations actions mutex
relations preconditions cannot changed. use bit vector called
changedActs record information. fact loses mutex relations
layers adds consumers changedActs. impact refinement eciency
discussed Section 3.
concludes construction new action rank. new fact rank already
partially constructed addition spike fact headers add list
elements, new actions, already present. necessary determine
mutex relations pairs facts spike. must first complete
achievement vectors fact headers new rank. non-mutex pairs
remain non-mutex, actions, effort focussed deciding whether previously
mutex facts still mutex following addition new actions, whether new
facts induce new mutex relations. Two facts mutex way achieving
involves use mutex actions. therefore consider every new fact every
fact, one direction. pair f , g mutex new rank every possible
achiever f mutex every possible achiever g . test exclusion done
using g 's achievement vector result logically anding action mutex vectors
possible achievers f . following definition gives details:

Definition 11 Two facts, f g, mutex if:
vec ^ mutex = vec
g

f

93

g

fiLong & Fox

vec g 's achievement vector mutex consequence anding
action mutex vectors f 's possible achievers.
f

g

matter order f g treated. computation
condition corresponds testing truth

8a 8b (achiever(a; f ) ^ achiever(b; g) ! mutex(a; b))
Since mutex relations symmetric quantifiers freely reordered expression equally corresponds
vec ^ mutex = vec
f g found mutex set fact mutex vector f oring
g 's bit mask fact mutex vector g set conversely. concludes rank
construction process one iteration spike construction process.
f

g

2.2 Subset Memoization Stan

f

search machinery used Stan essentially identical Graphplan.
is, goal set considered identifying appropriate achieving actions previous
layer propagating preconditions back graph. use spike
bit vector representations impact search algorithm. experimented
using bit vector representations bad goal sets memoization process, order
exploit logical bit operations test subset relations sets goals,
proved expensive rely upon trie data structure. benefits marginally
spike goal sets need sorted subset testing. order
goals generated spike taken canonical ordering since
goal sets formed simple sweep spike successive layer. Stan
implements improvement goal set memoization Graphplan. original
Graphplan, goal set could achieved particular layer entire set
memoized bad set layer. Stan version 2, subset goals
satisfied point failure, within layer, actually memoized. goal
sets likely contain smaller memoized subset would likely contain
complete original failing goal set. therefore allows us prune search branches earlier.
method weak version Kambhampati's (1998, 1999) EBL (Explanation-Based
Learning) modifications. EBL allows identification subset goal set
really responsible failure yield plan. Memoization smaller sets increases
eciency planner reducing overhead necessary identifying failing goal sets.
DDB (Dependency-Directed Backtracking) improves search performance ensuring
backtracking returns point last choice responsible failure
made. modifications result smaller sets memoized ecient search
behaviour which, combination trie, ensure higher proportion failing
search paths terminated early.
experimented implementation full EBL/DDB modifications
proposed Kambhampati, interaction EBL/DDB machinery
wave front Stan currently attempting resolve. experiments
far indicate wave front EBL/DDB significant beneficial impact
94

fiEfficient Implementation Plan Graph STAN

search, consistently across problems. believe enhance
advantages wave front full integration EBL/DDB, remains
demonstrated.

2.3 Worked Example

demonstrate spike construction process action simple blocks world example two blocks two table positions. initial state, blocks
table, one two positions. Consequently clear table
positions. initial spike consists fact rank containing fact headers four facts
describe initial state. single operator schema, puton(Block; To; From),
follows:

puton(X,Y,Z)

Pre:
on(X,Z), clear(X), clear(Y)
Add:
on(X,Y), clear(Z)
Del:
on(X,Z), clear(Y)
action rank initially empty. first iteration loop first action rank
constructed creating no-ops every fact zeroth fact rank. Two actions
applicable enacted, facts addlists used create new fact
rank. results partially developed spike shown Figure 2.
observed Figure 2 that, following enactment, fact headers associated
newly added facts incomplete, although new fact level action level
packages allocated yet contain values. new fact headers
missing references no-ops used achieve next action rank.
new fact level packages blank corresponding fact headers
level information rank 0.
identification mutex actions mutex facts, picture shown Figure 3.
action level packages, lists mutex actions given lists indices
sake clarity. fact lists pointers actions, order avoid indirection
involved use indices. None action pairs temporarily mutex rank 1
fact mutex vectors rank 0 zero-valued.

3. Empirical Results

section present results demonstrating eciency spike vector representation plan graph used Stan. consider graph construction
section { eciency search Stan demonstrated Section 4. show
eciency graph construction Stan showing relative performance figures Stan
competition version Ipp several competition domains two
standard bench mark domains. Graphplan version Travelling Salesman
domain (Blum & Furst, 1997), uses complete graph referred
Complete-Graph Travelling Salesman domain, Ferry domain available PDDL
release.
compare Stan Ipp because, best knowledge, Ipp
fast Graphplan-based planner currently publicly available. use competition
95

fiLong & Fox

Fact Spike
name: on(a,t1)
index: 0
msk 10000000
noop: 0

Action Spike

(rank 0)
FMV: 0...0
AV:

name: on(b,t2)
index: 1
msk 01000000
noop: 1

name: clear(b)
index: 3
msk 00010000
noop: 3
name: on(a,b)
index: 4
msk 00001000
noop:

0...0

0...0

FMV: 0...0
AV:

name: clear(a)
index: 2
msk: 00100000
noop?: True
precs: 00100000
adds: 00100000
dels: 00000000

0...0

FMV: 0...0
AV:

Action Level Packages
(rank 1 - yet
uninstantiated)

name: on(b,t2)
index: 1
msk: 01000000
noop?: True
precs: 01000000
adds: 01000000
dels: 00000000

FMV: 0...0
AV:

name: clear(a)
index: 2
msk 00100000
noop: 2

name: on(a,t1)
index: 0
msk: 10000000
noop?: True
precs: 10000000
adds: 10000000
dels: 00000000

Fact Level Packages

0...0

name: clear(b)
index: 3
msk: 00010000
noop?: True
precs: 00010000
adds: 00010000
dels: 00000000

name: clear(t1)
index: 5
msk 00000100
noop:

name: puton(a,b,t1)
index: 4

name: clear(t2)
index: 6
msk 00000010
noop:

msk: 00001000
noop?: False
precs: 10110000
adds: 11000000
dels: 10010000

name: on(b,a)
index: 7
msk 00000001
noop:

name: puton(b,a,t2)
index: 5
msk: 00000100
noop?: False
precs: 01110000
adds: 00000011
dels: 01100000

Figure 2: spike enactment rank 1 actions.

96

fiEfficient Implementation Plan Graph STAN

Action Spike

Fact Spike

name: on(a,t1)
index: 0
msk 10000000
noop: 0
name: on(b,t2)
index: 1
msk 01000000
noop: 1
name: clear(a)
index: 2
msk 00100000
noop: 2
name: clear(b)
index: 3
msk 00010000
noop: 3
name: on(a,b)
index: 4
msk 00001000
noop:

name: on(a,t1)

Fact Level Packages
(ranks 0 1)

FMV: 0..0

00001000

AV: 0..0

10000000

index: 0
msk: 10000000
noop?: True
precs: 10000000
adds: 10000000
dels: 00000000

AMV: 00001000
MAs: 4

name: on(b,t2)
FMV: 0..0

00000100

AV: 0..0

01000000

FMV: 0..0

00000011

AV: 0..0

00100000

FMV: 0..0

00001000

AV: 0..0

00010000

index: 1
msk: 01000000
noop?: True
precs: 01000000
adds: 01000000
dels: 00000000

AMV: 00000100
MAs: 5

name: clear(a)
index: 2
msk: 00100000
noop?: True
precs: 00100000
adds: 00100000
dels: 00000000

AMV: 00000100
MAs: 5

name: clear(b)
index: 3
msk: 00010000
noop?: True
precs: 00010000
adds: 00010000
dels: 00000000

10000011

name: clear(t1)
index: 5
msk 00000100
noop:

00001000

name: clear(t2)
index: 6
msk 00000010
noop:

00001000

AMV: 00001000
MAs: 4

name: puton(a,b,t1)
index: 4

01000011

msk: 00001000
noop?: False
precs: 10110000
adds: 11000000
dels: 10010000

00100100
00000100

name: on(b,a)
index: 7
msk 00000001
noop:

Action Level Packages
(rank 1)

name: puton(b,a,t2)
index: 5
msk: 00000100
noop?: False
precs: 01110000
adds: 00000011
dels: 01100000

00101100
00000100

AMV: 10010100
MAs: 0,3,5

AMV: 01101000
MAs: 1,2,4

Figure 3: spike end rank 1 construction phase.

97

fiLong & Fox

100000

33 3
3

10000

3
3

1000

100
100
IPP

3

1000

10000

STAN

100000

Figure 4: Graph construction logistics domain: Stan shows constant factor improvement performance Ipp.
1000

100

3

IPP
10

3

3

3
3

33
3
3

10

3

100
STAN

Figure 5: Graph construction Gripper domain.
98

1000

fiEfficient Implementation Plan Graph STAN

version Ipp date version available Freiburg
webpage time writing. order focus graph construction phase,
eliminate search phase planners, constructed versions Stan
Ipp terminate graph opened. removed Stan
unnecessary pre-processing, domain analysis additional features contribute later
search eciency. However, since Ipp designed build one layer opening
strictly necessary, include dummy goal corresponding achievement
conjunction top level goal set, make Stan build one extra layer
two systems comparable. removed meta-strategy control Ipp,
forcing Ipp directly graph construction. possible streamlined graph
constructor could built Ipp elimination processing, observed,
experimentation Ipp, pre-processing accounts insignificant proportions
timings reported below. therefore confident streamlining
would minimal effects results. order compare Stan Ipp accurately
necessary modify timing mechanisms ensure precisely elements
timed. Unix/Linux diff file available Stan website, Online Appendix
1, anyone interested reconstructing Ipp graph construction system used.
domains problems used, graph construction version Stan, also
found locations.
experiments reported paper carried P300 Linux PC,
128Mb RAM 128Mb swap space. timings data sets reported
milliseconds.
graphs log-log scaled. necessary combat long scales caused
large timings associated instances domain. graphs show Ipp's
construction performance compared Stan's construction performance measured
problems six domains. straight line shows equal performance
would be. Points line indicate superior performance Stan points
line indicate superior performance Ipp. first five data sets, Stan clearly
out-performs Ipp. last data set (Figure 9), Ipp convincingly out-performs Stan
consider detailed analysis characteristics domains instances
explain data sets.
first four data sets reveal similar performance. points broadly parallel equal performance line, indicating Stan performs constant multiple
performance Ipp. Despite trend data sets reveal, occasional data
points deviate significantly behaviour. ects fact different structures particular problems exercise different components graph construction system.
Components include instantiation operators, application individual operator instances
corresponding extension fact layers checking re-checking mutex relations
facts actions. observed problem instances, 50 per
cent construction time spent action mutex checking, whilst others
instantiation dominated. density permanent mutex relations actions,
degree persistence temporary mutex relations actions, significant determining eciency performance. example problem 8 Mystery
domain, 21 layers constructed graph opens, 9 per cent
action pairs discarded permanently mutex and, temporary mutex pairs,
99

fiLong & Fox

100000

3
3

10000

3 3
3

1000
IPP

3

3

100
10

3

10

100

1000
STAN

10000

100000

Figure 6: Graph construction Mystery domain. Stan's performance domain
consistently better Ipp, shows marked variation revealing
benefits spike problem-dependent.
100000

3

10000

3

3

3

33

3

IPP
1000
1000

10000
STAN

Figure 7: Graph construction Mprime domain.
100

100000

fiEfficient Implementation Plan Graph STAN

1000

100

33
3
3
33
3

3
3
3

10
IPP
1

1

10

STAN

100

1000

Figure 8: Graph construction Ferry domain. Stan shows polynomially better graph
construction performance Ipp.
average number re-tests across entire graph construction 7. use
changedActs mechanism described Section 2.1, avoid retesting actions
precondition mutex relations changed previous layer, gave us 50 per
cent improvement performance accounts 40 second advantage
Ipp construction phase problem.
problems much higher percentage action pairs permanently mutex,
allowing early elimination many action pairs retesting. mutex relations highly persistent similar elimination rate possible. allows much
faster construction Stan. Ipp benefit way,
distinguish temporary permanent mutex try identify
pairs actions retested.
Ferry domain, Figure 8, 7 layers constructed open graph regardless
instance size. Analysis reveals approximately 25 per cent action pairs permanently mutex average persistence temporary mutex relations slightly
2 layers. Since Ipp intelligently eliminate actions retesting, implication Ipp unnecessarily re-checks mutex relations polynomially increasing
number pairs actions. explains polynomial advantage obtained Stan
domain.
last data set shows rather different pattern performance
others. Complete-Graph Travelling Salesman domain used produce data set
Figure 9 simplified version, graph fully connected, well known
101

fiLong & Fox

1000

100

3

IPP
10

10

3

3

100
STAN

3

3

3

1000

Figure 9: Graph construction Complete-Graph Travelling Salesman domain. Stan
displays polynomially deteriorating graph construction performance.
discussed text.

NP-hard TSP. is, principle, eciently solvable. Figure 9 Ipp's performance appears
polynomially better Stan. Analysis graph structure built
different instances reveals that, instance sizes, graph opens layer 3.
graphs interesting pattern observed mutex relations actions:
vast majority action pairs mutex first application layer 2 (because
salesman ever one place). mutex relations considered,
Stan Ipp, temporary although fact persist. consequence
Stan Ipp retest pairs next layer. Stan obtains advantage use
changedActs distinction temporary permanent mutex relations
domain. number mutex pairs checked increases quadratically increase
instance size, line Stan's performance. Ipp clearly pays much less
retesting, despite fact amount work. fact, together
profiling systems, leads us believe disadvantage suffered Stan
due overhead supporting object member applications C++ implementation.
worth pointing Complete-Graph Travelling Salesman domain, well
Gripper Ferry, construction time planners 1 second
instances tested discrepancies performance three domains insignificant
compared discrepancies measured seconds (for large instances)
domains.
102

fiEfficient Implementation Plan Graph STAN

Fix Point

G1

Buffer

G

G2
G3
G4

G1

G5

G2

Figure 10: wave front Stan.

4. Wave Front
layer reached top level goals pairwise non-mutex Graphplanbased planners begin searching plan. plan found, new layers constructed alternately search fix point graph reached. Graphplan
Ipp graph continues explicitly constructed beyond fix point, even though
layers built beyond point sterile (contain new facts, actions
mutex relations). construction necessary allow conditions achievement
goal sets established, fix point current layer. However, constitutes significant computational effort copying existing structures unnecessary
searching duplicate structures. Instead building sterile layers explicitly,
Stan maintains single layer, called buffer, beyond fix point together queue
goal sets remaining considered. time goal set removed queue,
considered buffer, goal sets generates fix point layer,
103

fiLong & Fox

previously marked unsolvable, added queue. goal sets
queue considered order, always achievement buffer layer. Thus, rather
constructing new layer time top level goal set proves unsolvable,
reconsidering achievers new layer, goal sets queue simply
considered buffer layer. call mechanism wave front pushes goal
sets forward fix point layer buffer, recedes consider another
goal set fix point layer. goal sets generated fix point, join
queue propagation, referred candidate goal sets. wave front depicted
Figure 10. underlying implementation plan graph remains based spike,
figure depicts graph traditional way simplicity.
picture, G represents top level goal set used initiate plan
search buffer layer generates sequence goal sets G1, G2 G3
fix point layer. Assuming fail, first set queue, G1, propagated
forward buffer leading generation goal sets G4 G5 fix point layer.
added end queue G2 next goal set selected
queue propagate forward.
order demonstrate wave front machinery maintains appropriate behaviour three questions considered.
1. every goal set would considered buffer layer, graph
constructed explicitly, still considered using wave front? question concerns completeness search process.
2. every plan generated achieve goal set considered buffer layer
correspond plan would generated graph explicitly
constructed? question concerns soundness.
3. final question concerns whether termination properties Graphplan
maintained.

Definition 12 k-level goal tree goal set G layer n plan graph, GT ,
general tree depth k nodes goal sets parent-child relationship
defined follows. goal set x tree level goal set child
x minimal goal set containing mutex goal pairs achievement
layer n , , 1 plan graph enables achievement x layer n , graph.
take root level 0 tree leaves level k , 1.
k;G;n

Lemma 1 n , k FP GT
point layer plan graph.

k;G;n

= GT

+1 ,

k;G;n

FP number fix

Proof definition fix point, layers plan graph beyond fix point contain

exact replica information contained fix point layer. Since, definition
goal tree, parent-child relationship depends exclusively upon relationship
two consective layers plan graph, layers cannot change fix
point, follows x parent layer beyond fix point
parent-child relationship x must hold pair consecutive layers beyond
fix point. Further, new parent-child relationships arise beyond fix point.
104

fiEfficient Implementation Plan Graph STAN

restriction n , k FP ensures layers goal trees lie region beyond
fix point.

2
completeness Stan follows completeness Graphplan provided
goal sets would appear layer fix point explicit graph
arise candidates considered buffer layer using wave front. prove
condition satisfied first proving leaves goal trees generated
successive layers plan graph used generate candidates Stan. Since goal
sets considered Graphplan always subsets leaves goal trees shown
completeness Stan follows.

Theorem 1 Given goal set, G, plan graph n layers, containing plan G
length n , 1, fix point layer FP (n > FP ), leaves GT ,

n

generated candidates Stan.

F P;G;n

Proof proof induction n, base case n = FP + 1. base case

result follows trivially leaf GT1
+1 top level goal set G
generated initial candidate Stan.
Suppose n > FP + 1. inductive hypothesis states leaves tree
GT ,1,
,1 generated candidates Stan. Since plan graph constructed
Stan identical Graphplan layer FP + 1, candidates used
initiate search layer FP + 1, leaves GT ,
,1 also generated goal
sets layer FP Stan. goal sets used Stan construct candidates.
Stan generate multiple copies candidates, new goal set generate
new candidate.
Lemma 1, GT ,
= GT ,
gener,1 , leaves GT ,
ated candidates Stan.
;G;F P

n

F P;G;n

n

n

F P;G;n

n

F P;G;n

F P;G;n

n

F P;G;n

2
definition goal trees captures precisely relationship goal sets
search paths considered Graphplan. However, Graphplan memoizes failed
goal sets prune parts goal tree regresses explicit plan graph
search. Whenever goal set contains memoized goal set search terminates along
branch none children generated. seen Graphplan
generate layer FP + 1 subset leaves GT ,
, searching
layer n goal set G, whereas Theorem 1 demonstrates Stan construct
leaves candidates.
argument might suggest Stan engages unnecessary search generating
candidates Graphplan prune, using memos, layers constructed
explicitly Stan. fact, Stan generates candidates Graphplan generates
goal sets layer FP +1. Indeed, Stan achieves dramatic reduction search exploiting
correspondence goal trees generated layers n n , 1, demonstrated
Lemma 1. correspondence need construct layers
FP + 1 n explicitly, undertake concommitant search layers.
n

105

F P;G;n

fiLong & Fox

Layer 0

FP

FP+1

n-1

n

G1

L1
L2

G2

G

G3

Ln

L1

G1

L2

G2

G

G3

Ln

Figure 11: sliding window layers FP + 1 n.
Graphplan rebuilds sliding window, shown Figure 11, layers FP n , 1
layers FP + 1 n. Stan simply promotes leaves tree, generated
layer FP GT ,
,1 , layer FP + 1.
straightforward show wave front maintains soundness. search
Graphplan performs generates goal tree goal sets, defined Definition 12.
example Figure 10, tree rooted G, G1, G2 G3 children G4
G5 children G1. seen picture tree structure generated
Graphplan, successive layer would embedded separate layer
explicitly constructed graph, appears spiral related goal sets fix point
buffer layers. candidate goal sets lie search tree therefore
additional goal sets generated. Graphplan constructs final plan reading
sequence action choices layer final graph. Stan, plan obtained
reading initial fragment plan way, layers preceding
fix point. rest plan extracted spiral. extraction process
yields path action choices top level goal set candidate goal set
would recorded explicitly Graphplan plan graph.
question remaining considered whether wave front
termination properties Graphplan. seen since, new unsolvable
goal sets generated fix point, queue become empty planner
terminates. corresponds exactly termination conditions Graphplan.
subtlety concerns interaction wave front subset memoization
discussed Section 2.2. principle, subset memoization could cause loss three
desired properties graph. way Stan generates candidate goal sets
n

F P;G;n

106

fiEfficient Implementation Plan Graph STAN

simultaneously generating candidate set whenever goal set memoized fix point.
candidate set memoized set one same, memoization
subset goal set lead propagation subset actual candidate
goals buffer soundness might undermined. use subset memoization
wave front question arises whether sets contain memoized subset
propagated forward candidates. not, completeness potentially
lost, since might action sequences could constructed following
propagation found. are, termination potentially lost,
since set led construction memoized subset might generated
candidate. could happen, example Figure 10, G1 unsolvable fix
point generated consideration later candidate buffer.
avoid problems restored full subset memoization wave front.
alternative solution, currently exploring, separate subsets
goals memoized identification candidate sets. solutions avoid loss
soundness candidates constructed entire goal sets rather
subsets. first solution, termination preserved memoizing full goal sets
ensures repeated candidates correctly identified recur. second
solution, would separately memoize candidates generated avoid repeated
generation, thereby maintaining termination. cases, completeness preserved
propagating goal-sets forward new candidates provided contain
previously encountered candidates subsets. potential candidate superset
entire memoized candidate correct propagate potential candidate
buffer memoized candidate cannot solved buffer superset
solved either.

5. Experimental Results
results presented use Stan version 2 (available website). performed
experiments comparing Stan without wavefront order demonstrate
advantages obtained use wave front. performed experiments
compare Stan competition version Ipp. minor discrepancies
timing mechanisms two systems. Stan measures elapsed time entire
execution, whereas Ipp measures user+system time graph construction search
parsing problem domain instance. single user machine used
experiments discrepancy negligible.
problem domains used section selected emphasise benefits
offered wave front. important characteristic early
fix point relative length plan instances grow. comparisons Ipp
wave front accounts trends performance, although Stan employs range
mechanisms give minor advantages. Amongst Tim machinery,
decoupled problem domains used standard typed ones
significant advantage obtained inferring type structures automatically.
resource invariants inferred Tim exploited Stan version 2,
indicated gives us advantage Ipp. ablation data sets confirm
107

fiLong & Fox

100000

3

10000

3
3

1000
IPP

3

100
10

10

3

100

STAN

1000

10000

Figure 12: Stan compared Ipp: solving Towers Hanoi problems 3-7 discs.
wave front significant component performance Stan
experiments.
Stan capable eciently solving larger Towers Hanoi instances presented
graph Figure 12, accounts additional point Figure 13. Stan
wave front found 511-step plan 9-disc problem less 7 minutes using
15Mb memory. experiments reported here, Ipp terminated
15 minutes reached layer 179 255 layers 8-disc problem. observe
machine 1Gb RAM, Ipp solved problem 8 minutes.
results Gripper domain demonstrate small advantage Stan.
reason search space grows exponentially size graph Gripper
domain, cost searching dominates everything else. Although search spaces
Towers Hanoi instances also grow exponentially, grow 2 whereas Gripper
instances grow x (where x number discs balls respectively). Although
wave front helps conditions, size search space dwarfs benefits
offers. Ferry domain less rapidly growing version gripper domain since
one vehicle carried journey, reducing number choices layer.
difference benefits obtained Towers Hanoi domain relative Gripper
Ferry domains explained consideration table Figure 16. benefits
wave front proportional number layers exist implicitly
buffer layer plan ultimately found. Towers Hanoi
number implicit layers exponential number discs whereas number
layers initial layer buffer linear number discs. Therefore
benefits offered use wave front magnified exponentially problem
x

x

108

fiEfficient Implementation Plan Graph STAN

1e+06

3

100000

3
3

10000

3

1000
100

3

10
STAN wf 10

3
100

1000
STAN wf

10000

100000

Figure 13: Stan without wave front: solving Towers Hanoi problems 3-8
discs.
1e+06

3

100000

3

10000
1000

3

IPP 100
10

3
10

100

1000

STAN

10000

100000

1e+06

Figure 14: Stan compared Ipp: solving Gripper problems 4-10 balls.
109

fiLong & Fox

1e+06

3

100000

3

10000
1000
100
10
STAN wf 10

3
3
100

1000
10000
STAN wf

100000

1e+06

Figure 15: Stan without wave front: solving Gripper problems 4-10 balls.
Domain
Towers Hanoi
Gripper
Ferry
Complete-Graph TSP

Parameter n Plan Length Buffer
no. discs
2 ,1
n+3
no. balls
2n , 1
5
no. vehicles
4n , 1
7
no. cities
n
4
n

Figure 16: Relative values plan length number layers buffer four domains.
instance grows. hand, Gripper Ferry linear growth
difference plan length fix point layer, benefits magnified
linearly. analysis confirmed observation Figures 12, 14 17.
benefit wave front measured terms cost construction
avoided explicitly building layers beyond buffer, also terms
search avoided layers. Crudely, benefits measured
number layers constructed multiplied search effort avoided
layers. Thus, number layers constructed magnifies benefits obtained
searching amongst them. simplification, since search effort avoided
successive layers increases get away fix point, gives guide
kind benefits expected wave front.
Stan obtains significant advantages Ipp Complete-Graph Travelling Salesman domain, Figure 19 demonstrates. advantages obtained ex110

fiEfficient Implementation Plan Graph STAN

1e+06

3

100000

3

10000

3

1000

3
3

IPP 100
10

3
10

100

1000
STAN

10000

100000

Figure 17: Stan compared Ipp: solving Ferry problems 2-12 cars.
1e+06

3

100000

3

10000

3

1000

3

100
10
STAN wf 10

3

3
100

1000
STAN wf

10000

100000

Figure 18: Stan without wave front: solving Ferry problems 2-12 cars.
111

fiLong & Fox

1e+10

3

1e+09

3

1e+08

3

1e+07

3

1e+06

3

100000
IPP

10000

3

1000
100
100

1000
STAN

10000

Figure 19: Stan compared Ipp: solving Complete-Graph Travelling Salesman problems 10-20 cities.
ploiting resource analysis techniques Tim (Fox & Long, 1998), whilst significant
proportion advantage obtained use wave front, Figure 20 shows.
Resource analysis allows lower bound determined number layers must
built plan graph worth searching plan. Complete-Graph
Travelling Salesman domain powerful, calculated bound n, number
cities instance, precisely correct plan length. domain,
search done n layers constructed, search needs done since
doesn't matter order cities visited. would allow problem
solved polynomial time (of course, makes sense Complete-Graph
TSP used simpler NP-hard TSP). However, wave front used,
buffer layer 4 way finding plan generate candidate
goal sets layer 4, exponential number. use wave front
domain therefore forces Stan take exponential time size instances.
Despite wave front offers great advantages. benefits increase exponentially
instance sizes grow although magnification benefits layer linear,
see Figure 16, although benefits offset exponential growth number
candidates. must observed Figure 19, figures extrapolated Ipp
instances n greater 14. extrapolation based Ipp's performance
instance sizes 2 14, demonstrates clear exponential growth.
appears could allow resource analysis over-ride wave front
domain encountered guaranteed explicit construction graph
112

fiEfficient Implementation Plan Graph STAN

10000

3
3
3
3

1000

3
3
100
STAN wf 100

1000
STAN wf

10000

Figure 20: Stan without wave front: solving Complete-Graph TSP problems.
ecient. practice Complete-Graph Travelling Salesman domain seems
exceptional, since search eliminated graph constructed layer n,
case explicit construction subsequent search would costly
use wave front.

5.1 Wave Front Heuristic
queue candidate goal sets considered buffer implemented unordered structure goal sets selected consideration according sophisticated criteria order stored. principle, could save
much searching effort since could avoid costly consideration goal sets turn
unsolvable meeting solvable goal set. experimented number
goal set selection heuristics favour goal sets search progresses deepest
graph structure. sets considered closer solvable sets
fail layer close buffer. Candidates evaluated considering
length plan fragment associated candidate extent failed
search penetrated graph initiated fix point layer candidate
first generated. search penetration maximized plan fragment
length minimized. Considering goal sets order
generated affect formal properties planner
optimality plans generated. Non-optimal plans favoured
113

fiLong & Fox

1e+06

3

100000

3

10000

3
Stanh

1000
100
100

3 33

3
1000

10000

Stan

100000

1e+06

Figure 21: Towers Hanoi (Stanh) without heuristic: 3-9 discs.
balance fragment length penetration cause candidates shorter
fragments overlooked.
Using heuristic Stan able solve Towers Hanoi problems eciently,
Figure 21 shows. previously, graph log-log scaled. line indicates least
polynomial improvement size instances. heuristic originally developed
consideration blocks world problems, also performs well. However,
provide reliable advantage used Stan version 2. used problems
competition often represented heavy overhead Stan. continuing
experiment alternative domain-independent evaluation criteria.

6. Conclusion
paper presents two improvements representation plan graph exploited
Graphplan-based planners. are: representation graph single pair
layers, called spike, built around bit vectors logical operations, use wave
front avoids explicit construction graph beyond fix point. describe
highly ecient procedure checking mutex relations actions explain
characteristics problems allow full exploitation. spike wave front
implemented Stan, Graphplan based planner version 11 competed
successfully AIPS-98 planning competition. presented empirical evidence
support improvements. first set data demonstrates increase graph
1. Version 1 contained implementations spike wave front. Version 2 enhances
mechanisms improved implementation addition changedActs mechanism
discussion Section 2.1.

114

fiEfficient Implementation Plan Graph STAN

construction eciency obtained use spike. second set data shows
advantages obtained search plan graph using wave front.
Stan also employs state invariant inference machinery Tim (Fox & Long, 1998),
version 2 integration invariants graph construction process
still partial. observe mutex relations generated Complete-Graph
TSP, particular, almost entirely domain invariants kind inferred Tim.
Integration inferred invariants graph would allow mutex relations
identified immediately permanent eliminate retesting, dramatically
enhancing Stan's graph construction performance domain. similar advantage
would obtained across domains since many mutex relations inferred
graph construction correspond invariants various forms inferred eciently Tim
preprocessing stage.

Appendix A. Website Addresses

Online Appendix 1 contains complete collection domains problems used
paper, executables (Linux Sparc-Solaris binaries) Stan reduced version
Stan graph construction, diff file showing graph constructing version
IPP generated.
results AIPS-98 planning competition found at:
http://ftp.cs.yale.edu/pub/mcdermott/aipscomp-results.html.
Stan website found at:
http://www.dur.ac.uk/dcs0www/research/stanstuff/planpage.html.

References

Blum, A., & Furst, M. (1997). Fast Planning Planning Graph Analysis. Artificial
Intelligence, 90, 281{300.
Fox, M., & Long, D. (1998). Automatic Inference State Invariants TIM. JAIR,
9, 317{371.
Kambhampati, S. (1998). EBL DDB Graphplan. Tech. rep. ASU CSE TR 98-008,
Arizona State University.
Kambhampati, S. (1999). Relations Intelligent Backtracking Explanation Based Learning Planning CSP. Artificial Intelligence, 105 (1-2).
Koehler, J., Nebel, B., & Dimopoulos, Y. (1997). Extending Planning Graphs ADL
Subset. Proceedings 4th European Conference Planning.
Smith, D., & Weld, D. (1998). Incremental Graphplan. Tech. rep. TR 98-09-06, University
Washington.

115

fi
