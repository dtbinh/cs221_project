Journal Artificial Intelligence Research 33 (2008) 521-549

Submitted 6/08; published 12/08

Multiagent Reinforcement Learning Algorithm
Non-linear Dynamics
Sherief Abdallah

SHERIEF. ABDALLAH @ BUID . AC . AE

Faculty Informatics
British University Dubai
United Arab Emirates
(Fellow) School Informatics
University Edinburgh
United Kingdom

Victor Lesser

LESSER @ CS . UMASS . EDU

Department Computer Science
University Massachusetts Amherst
United States

Abstract
Several multiagent reinforcement learning (MARL) algorithms proposed optimize
agents decisions. Due complexity problem, majority previously developed
MARL algorithms assumed agents either knowledge underlying game (such
Nash equilibria) and/or observed agents actions rewards received.
introduce new MARL algorithm called Weighted Policy Learner (WPL), allows
agents reach Nash Equilibrium (NE) benchmark 2-player-2-action games minimum
knowledge. Using WPL, feedback agent needs local reward (the agent
observe agents actions rewards). Furthermore, WPL assume agents know
underlying game corresponding Nash Equilibrium priori. experimentally show
algorithm converges benchmark two-player-two-action games. also show
algorithm converges challenging Shapleys game previous MARL algorithms failed
converge without knowing underlying game NE. Furthermore, show WPL
outperforms state-of-the-art algorithms realistic setting 100 agents interacting
learning concurrently.
important aspect understanding behavior MARL algorithm analyzing dynamics algorithm: policies multiple learning agents evolve time agents
interact one another. analysis verifies whether agents using given MARL
algorithm eventually converge, also reveals behavior MARL algorithm prior
convergence. analyze algorithm two-player-two-action games show symbolically proving WPLs convergence difficult, non-linear nature WPLs dynamics,
unlike previous MARL algorithms either linear piece-wise-linear dynamics. Instead,
numerically solve WPLs dynamics differential equations compare solution dynamics
previous MARL algorithms.

1. Introduction
decision problem agent viewed selecting particular action given state.
well-known simple example single-agent decision problem multi-armed bandit (MAB)
problem: agent needs choose lever among set available levers. reward executing action drawn randomly according fixed distribution. agents goal choose
c
2008
AI Access Foundation. rights reserved.

fiA BDALLAH L ESSER

lever (the action) highest expected reward. order so, agent samples
underlying reward distribution action (by trying different actions observing resulting rewards). goal reinforcement learning algorithms general eventually stabilize
(converge) strategy maximizes agents payoff. Traditional reinforcement learning algorithms (such Q-learning) guarantee convergence optimal policy stationary environment
(Sutton & Barto, 1999), simply means reward distribution associated action
fixed change time.
multiagent system, reward agent receives executing particular action depends
agents actions well. example, consider extending MAB problem multiagent case. reward agent gets choosing lever 1 depends lever agent B
chosen. agents B learning adapting strategies, stationary assumption
single-agent case violated (the reward distribution changing) therefore single-agent
reinforcement learning techniques guaranteed converge. Furthermore, multi-agent
context optimality criterion clear single agent case. Ideally, want
agents reach equilibrium maximizes individual payoffs. However, agents
communicate and/or agents cooperative, reaching globally optimal equilibrium
always attainable (Claus & Boutilier, 1998). alternative goal pursue converging Nash Equilibrium (NE) (Bowling, 2005; Conitzer & Sandholm, 2007; Banerjee & Peng,
2007), definition local maximum across agents (no agent better deviating
unilaterally NE).
important aspect understanding behavior MARL algorithm analyzing dynamics algorithm: policies multiple learning agents evolve time interacting one another. analysis reveals whether agents using particular MARL
algorithm eventually converge, also points features MARL algorithm
exhibited convergence period. Analyzing dynamics MARL algorithm even
simplest domains (particularly, two-player-two-action games) challenging task therefore performed few, relatively simple, MARL algorithms (Singh, Kearns, & Mansour,
2000; Bowling & Veloso, 2002) restrictive subset games. analyzed dynamics
either linear piece-wise linear.
Recently, several multi-agent reinforcement learning (MARL) algorithms proposed
studied (Claus & Boutilier, 1998; Singh et al., 2000; Peshkin, Kim, Meuleau, & Kaelbling,
2000; Littman, 2001; Bowling & Veloso, 2002; Hu & Wellman, 2003; Bowling, 2005; Abdallah &
Lesser, 2006; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007). MARL algorithms
assumed agent knew underlying game structure, Nash Equilibrium (NE) (Bowling &
Veloso, 2002; Banerjee & Peng, 2007). even required knowing actions agents
executed rewards received (Hu & Wellman, 2003; Conitzer & Sandholm, 2007).
assumptions restrictive open domains limited communication (such ebay
Peer-to-Peer file-sharing) agent rarely knows agents existence let alone observing
actions knowing actions affect agents local reward.
hand, agents unaware underlying game (particularly NE)
observing other, even simple game two players two actions
challenging. example, suppose Player 1 observes getting reward 10 executing
first action reward 8 executing action 2. time passes, Player 2 changes policy,
Player 1 observes switch reward associated action: action 1 reward
7 action 2 reward 9. Note cases Player 1 unaware NE strategy
522

fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS

oblivious current policy Player 2. feedback Player 1 getting change
reward function, turn depends Player 2s policy. situation applies
reverse Player 2.
paper propose new MARL algorithm enables agents converge Nash
Equilibrium, benchmark games, assuming agent oblivious agents receives
one type feedback: reward associated choosing given action. new algorithm
called Weighted Policy Learner WPL reasons become clear shortly. experimentally show WPL converges well-known benchmark two-player-two-action games.
Furthermore, show WPL converges challenging Shapleys game, state-of-theart MARL algorithms failed converge (Bowling & Veloso, 2002; Bowling, 2005),1 unlike WPL.
also show WPL outperforms state-of-the-art MARL algorithms (shorter time converge,
better performance convergence, better average reward) realistic domain
100 agents interacting learning one another. also analyze WPLs dynamics show
non-linear. non-linearity made attempt solve, symbolically, differential
equations representing WPLs dynamics unsuccessful. Instead, solve equations numerically
verify theoretical analysis experimental results consistent. compare
dynamics WPL earlier MARL algorithms discuss interesting differences similarities.
paper organized follows. Section 2 lays necessary background, including game
theory closely related MARL algorithms. Section 3 describes proposed algorithm. Section 4
analyzes WPLs dynamics restricted class games compares previous algorithms
dynamics. Section 5 discusses experimental results. conclude Section 6.

2. Background
section introduce necessary background contribution. First, brief review
relevant game theory definitions given. review relevant MARL algorithms provided,
particular focus gradient-ascent MARL (GA-MARL) algorithms closely related
algorithm.
2.1 Game Theory
Game theory provides framework modeling agents interaction used previous
researchers order analyze convergence properties MARL algorithms (Claus & Boutilier,
1998; Singh et al., 2000; Bowling & Veloso, 2002; Wang & Sandholm, 2003; Bowling, 2005;
Conitzer & Sandholm, 2007; Abdallah & Lesser, 2006). game specifies, compact simple
manner, payoff agent depends agents actions. (normal form) game
defined tuple hn, A1 , ..., , R1 , ..., Rn i, n number players2 game, Ai
set actions available agent i, Ri : A1 ... < reward (payoff) agent
receive function joint action executed agents. game two
players, convenient define reward functions payoff matrix shown Table
1. cell (i, j) matrix represents payoff received row player (Player 1)
column player (Player 2), respectively, row player plays action column player
1. Except MARL algorithms assumed agents know information underlying game (Hu & Wellman,
2003; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007).
2. use terms agent player interchangeably.

523

fiA BDALLAH L ESSER

plays action j. Table 1 Table 2 provide example benchmark games used evaluating
previous MARL algorithms (Bowling & Veloso, 2002; Bowling, 2005; Conitzer & Sandholm, 2007;
Banerjee & Peng, 2007).
Table 1: Benchmark 2-action games. coordination game two pure NE(s): [(0, 1)r , (0, 1)c ]
[(1, 0)r , (1, 0)c ]. matching-pennies tricky games one mixed NE
actions played equal probability, NE=[( 21 , 12 )r , ( 21 , 12 )c ]
(a) coordination

a1
a2

a1
2,1
0,0

(b) matching pennies

a2
0,0
1,2

H


H
1,-1
-1,1


-1,1
1,-1

(c) tricky

a1
a2

a1
0,3
1,0

a2
3,2
2,1

(d) general
game

a1
a2

2-player-2-action

a1
r11 , c11
r21 , c21

a2
r12 , c12
r22 , c22

Table 2: Benchmark 3-action games. games one mixed NE actions played
equal probability, NE= [( 13 , 13 , 13 )r , ( 31 , 13 , 13 )c ].
(a) rock paper scissors

r1
r2
r3

c1
0,0
1,-1
-1,1

c2
-1,1
0,0
1,-1

(b) Shapleys

c3
1, -1
-1,1
0,0

r1
r2
r3

c1
0,0
0,1
1,0

c2
1,0
0,0
0,1

c3
0,1
1,0
0,0

policy (or strategy) agent denoted P D(Ai ), P D(Ai ) set
probability distributions actions Ai . probability choosing action ak according
policy (ak ). policy deterministic pure probability playing one action 1
probability playing actions 0, (i.e. k : (ak ) = 1 l 6= k : (al ) = 0),
otherwise policy stochastic mixed.
joint policy collection individual agents policies, i.e. = h1 , 2 , ..., n i,
n number agents. convenience, joint policy usually expressed = hi , i,
collection policies agents agent i.
V
Let variable Ai = {ha1 , ..., : aj Aj iP
6= j}.
P expected reward agent would get,
agents follow joint policy , Vi (hi , i) = ai Ai ai Ai (ai )i (ai ).Ri (ai , ai ),
i.e. reward averaged joint policy. joint policy Nash Equilibrium, NE, iff

agent get higher expected reward changing policy unilaterally. formally, hi ,
i) V (h , i). NE pure constituting policies
NE iff : Vi (hi ,


pure. Otherwise NE called mixed stochastic. game least one Nash equilibrium,
may pure (deterministic) equilibrium.
Consider benchmark games Table 1. coordination game (Table 1(a))
example games least one pure NE. matching pennies game (Table 1(b))
example games pure NE mixed NE (where player
plays a1 a2 equal probability). Convergence GA-MARL algorithms games pure
524

fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS

NE easier games NE mixed (Singh et al., 2000; Bowling & Veloso, 2002;
Zinkevich, 2003). tricky game similar matching pennies game one
mixed NE (no pure NE), yet GA-MARL algorithms succeeded converging matching
pennies games, failing tricky game (Bowling & Veloso, 2002).
Table 2 shows well-known 2-player-3-action benchmark games. Shapleys game, particular,
received considerable attention MARL community (Bowling, 2005; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007) remains challenging state-of-the-art MARL algorithms despite apparent simplicity (and similarity rock-paper-scissors game
challenging). proposed algorithm first MARL algorithm converge Shapleys game
without observing agents knowing underlying NE strategy.
introducing MARL algorithms following section, two issues worth noting.
first issue general assumption MARL context: agents play game repeatedly
large number times. necessary assumption order agents gain experience
learn. second issue games rewards, described section, deterministic
given joint action. However, agents perspective, rewards stochastic
randomness caused agents actions system.
2.2 Multiagent Reinforcement Learning, MARL
Early MARL algorithms based Q-learning algorithm (Sutton & Barto, 1999), therefore could learn deterministic policy (Claus & Boutilier, 1998), significantly limiting
applicability competitive partially observable domains. Joint Action Learner (Claus &
Boutilier, 1998) example family algorithms also required knowledge
agents chosen actions.
Another class MARL algorithms class Equilibrium learners Nash-Q (Hu &
Wellman, 2003), OAL (Wang & Sandholm, 2003), AWESOME (Conitzer & Sandholm, 2007).
algorithms assumed agent observed agents actions addition
knowing underlying game.3 agent computed Nash Equilibria. purpose
learning allow agents converge particular Nash Equilibrium (in case agents execute
MARL algorithm).
Observing agents knowing underlying game structure applicable open
large domains, motivated development gradient ascent learners. Gradient ascent
MARL algorithms (GA-MARL) learn stochastic policy directly following expected reward
gradient. ability learn stochastic policy particularly important world fully
observable competitive nature. Consider example blind robot maze (i.e. robot
cannot distinguish maze locations). deterministic policy (i.e. one always chooses
particular action everywhere) may never escape maze, stochastic policy chooses
action non-zero probability eventually escape maze.4 Similarly, competitive
domain stochastic policy may stable policy (e.g. Nash Equilibrium competitive
game). remainder section focuses family algorithms closely related
proposed algorithm.
3. Nash-Q require knowing underlying game required observing agents rewards addition
chosen actions.
4. Note uniformly random policy may optimal maze biased specific direction.

525

fiA BDALLAH L ESSER

Infinitesimal Gradient Ascent algorithm (IGA) (Singh et al., 2000) generalization
(Generalized IGA GIGA) (Zinkevich, 2003) proved converge games pure NE.
However, algorithms failed converge games mixed NE, therefore may
suitable applications require mixed policy.
Several modifications IGA GIGA proposed avoid divergence games
mixed NE, including: IGA/PHC-WoLF (Bowling & Veloso, 2002), PHC-PDWoLF (Banerjee &
Peng, 2003), GIGA-WoLF (Bowling, 2005). used form Win Learn Fast
heuristics (Bowling & Veloso, 2002), whose purpose speedup learning agent
worse NE policy (losing) slow learning agent better NE
policy. main problem heuristic agent cannot know whether better
worse NE policy without knowing underlying game prior. Therefore, practical
implementation WoLF heuristic needed use approximation methods predicting
performance agents NE policy. algorithm, called WPL, uses different heuristic
require knowing NE policy consequently, show, converges tricky
game Shapleys game, algorithms relying WoLF heuristic failed. Furthermore,
show large-scale partially-observable domain (the distributed task allocation problem,
DTAP) WPL outperforms state-of-the-art GA-MARL algorithm GIGA-WoLF.
following section reviews detail well known GA-MARL algorithms IGA,
IGA-WoLF, GIGA-WoLF use compare algorithm against.
2.3 Gradient-Ascent MARL Algorithms
first GA-MARL algorithm whose dynamics analyzed Infinitesimal Gradient Ascent
(IGA) (Singh et al., 2000). IGA simple gradient ascent algorithm agent updates
policy follow gradient expected payoffs (or value function) Vi , illustrated
following equations.
Vi ( )


projection(i + it+1 )
it+1

it+1

Parameter called policy-learning-rate approaches zero limit ( 0), hence
word Infinitesimal IGA. Function projection projects updated policy space
valid policies. original IGA paper (Singh et al., 2000) defined projection function (and
therefore IGA) case agent two actions choose from. general definition
projection function later developed resulting algorithm called Generalized IGA
GIGA (Zinkevich, 2003). generalized function projection(x) = argminx0 :valid(x0 ) |x x0 |,
|x x0 | Euclidean distance x x0 . APvalid P
policy (over set actions
A) must satisfy two constraints: : 1 (a) 0 = aA (a) = 1.
words, space valid policies simplex, line segment (0,1),(1,0)
case two actions, triangular surface (1,0,0), (0,1,0), (0,0,1) case three actions,
on. joint policy, , point simplex.
quite possible (especially trying
P
follow approximate policy gradient) deviates 1 goes beyond simplex.
generalized projection function projects invalid policy closest valid policy within
simplex (Zinkevich, 2003).
526

fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS

IGA algorithm converge two-player-two-action games (Singh et al., 2000).
Algorithm IGA-WoLF (WoLF stands Win Learn Fast) proposed (Bowling & Veloso,
2002) order improve convergence properties IGA using two different learning rates
follows. player getting average reward lower reward would get executing
NE strategy, learning rate large. Otherwise (the player current policy better
NE policy), learning rate small. formally,
Vi ()
(a)
(a)




lose Vi (i , ) < Vi (i , )
win otherwise

projection(i + )
NE policy agent lose > win learning rates. dynamics
IGA-WoLF analyzed proved converge 2-player-2-action games (Bowling &
Veloso, 2002), briefly review Section 2.4. IGA-WoLF limited practical use, required agent know equilibrium policy (which means knowing underlying game).
approximation IGA-WoLF proposed, PHC-WoLF, agents NE strategy approximated averaging agents policy time (Bowling & Veloso, 2002). approximate
algorithm, however, failed converge tricky game shown Table 1(c).
GIGA-WoLF algorithm extended GIGA algorithm WoLF heuristic. GIGAWoLF kept track two policies z. Policy used select actions execution. Policy z
used approximate NE policy. update equations GIGA-WoLF (Bowling, 2005,
2004):
t+1 = projection( + rt )
z

t+1

t+1

= projection( + r /3)


||z t+1 z ||
= min 1, t+1
z





t+1 = t+1 + t+1 (z t+1 t+1 )

(1)
(2)
(3)
(4)

main idea variant WoLF heuristics (Bowling & Veloso, 2002). agent changes
policy faster agent performing worse policy z, i.e. V < V z . z moves
slower , GIGA-WoLF uses z realize needs change current gradient direction.
approximation allows GIGA-WoLF converge tricky game, Shapleys game
(Bowling, 2005).
following section briefly reviews analysis IGAs IGA-WoLFs dynamics, showing
joint policy two agents evolves time. build analysis Section 4
analyze WPLs dynamics.
2.4 Dynamics GA-MARL Algorithms
Differential equations used model dynamics IGA (Singh et al., 2000). simplify
analysis, authors considered two-player-two-action games, also here.
refer joint policy two players time probabilities choosing first action
(pt , q ), 1 = (pt , 1 pt ) policy player 1 2 = (q , 1 q ) policy
527

fiA BDALLAH L ESSER

player 2. notation omitted affect clarity (for example,
considering one point time).
IGAs update equations simplified (note rij cij payoffs row
column players respectively):
pt+1 = pt +

Vr (pt , q )
= pt + (Vr (1, q ) Vr (0, q ))
p




Vr (1, q ) Vr (0, q ) = r11 q + r12 (1 q ) r21 q + r22 (1 q )
= q (r11 r12 r21 + r22 ) + (r12 r22 )
= u1 q + u2

(5)

similarly,
q t+1 = q + (u3 pt + u4 )
u1 , u2 , u3 , u4 game-dependent constants following values.
u1 = r11 r12 r21 + r22
u2 = r12 r22
u3 = c11 c12 c21 + c22
u4 = c21 c22
analyzing IGA, original paper distinguished three types games depending
u1 u3 parameters: u1 u3 > 0, u1 u3 = 0, u1 u3 < 0. Figure 1(a), Figure 1(b), Figure
1(c) taken (Bowling & Veloso, 2002) illustrate dynamics IGA
three cases. figure displays space joint policies, horizontal axis represent
policy row player, p, vertical axis represents policy column player q.5
joint policy two agents evolves time following directed lines
figures. example, Figure 1(b) illustrates starting joint policy, two players
eventually evolve one two equilibriums, either bottom-left corner, upper-right corner.
noted that, simplicity, dynamics shown Figure 1 unconstrained: effect
projection function (the simplex described Section 2.3) shown. IGAs analysis
(Singh et al., 2000), effect projection function taken account considering
possible locations simplex.
first second cases, IGA converged. third case IGA oscillated around
equilibrium strategy, without ever converging it. happens point time, unless
agents NE, one agent better changing policy closer NE strategy,
agent better changing policy away NE strategy.
roles switch one agents crosses NE strategy shown Figure 1(c).
described Section 2.3, IGA-WoLF proposed (Bowling & Veloso, 2002) extension IGA ensures convergence third case. idea IGA-WoLF, described earlier,
5. Note Figure 1(b) Figure 1(c) divided four quadrants A, B, C, clarification. gradient
direction quadrant illustrated arrows Figure 1(c).

528

fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS

Figure 1: illustration IGAs dynamics (Bowling & Veloso, 2002).
two learning rates agent moving toward away NE. following equations capture dynamics IGA-WoLF, hp , q NE. equations use
factored form lose win .
p =p


q =q


t1

+ (u1 q

t1

t1

+ (u3 p

t1

+ u2 )



llose Vr (pt1 , q t1 ) < Vr (p , q t1 )
lwin otherwise

+ u4 )



llose Vc (q t1 , pt1 ) < Vc (q , pt1 )
lwin otherwise

dynamics IGA-WoLF best illustrated visually Figure 2, taken (Bowling & Veloso, 2002). switching learning rates causes switching ellipses
smaller diameters, eventually leading convergence.
main limitation IGA-WoLF assumed agent knows NE policy (needed
switch two modes learning rate). following section presents WPL,
make assumption, expense complex dynamics show
Section 4.

3. Weighted Policy Learner (WPL)
propose paper Weighted Policy Learner (WPL) algorithm, following
update equations:
529

fiA BDALLAH L ESSER

Figure 2: illustration IGA-WoLFs dynamics case mixed NE (Bowling & Veloso, 2002).

Vi ()
(a)

(a)

(

()
(a)
V
(a) < 0
1 (a) otherwise

projection(i + )
projection function adopted GIGA (Zinkevich, 2003) minor modification:a :
1 (a) (the modification ensures minimum amount exploration ). algorithm works
follows. gradient particular action negative gradient weighted (a),
otherwise (gradient positive) gradient weighted (1i (a)). effectively, probability
choosing good action increases rate decreases probability approaches 1 (or
boundary simplex). Similarly, probability choosing bad action decreases rate
also decreases probability approaches zero.
unless gradient direction changes, WPL agent decreases learning rate agent
gets closer simplex boundary. means, 2-player-2-action game, WPL agent move
towards NE strategy (away simplex boundary) faster moving away NE
strategy (towards simplex boundary), NE strategy always inside simplex.
WPL biased pure (deterministic) policies reach pure policy
limit (because rate updating policy approaches zero). theoretical limitation
little practical concern two reasons. first reason exploration: 100% pure strategies
bad prevent agents exploring actions (in open dynamic environment
reward action may change time).
second reason action dominates another action (the case game
Vi ()
()
pure NE), V
(a) large enough (a) > 1. case WPL jump
pure policy one step.6 Note Section 4, order simplify theoretical analysis, assume
0. practical perspective, however, set value close 0, never 0. holds
6. fact, WPL even go beyond simplex valid policies, projection function comes play

530

fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS

gradient-ascent MARL algorithms (Bowling & Veloso, 2002; Bowling, 2005; Banerjee &
Peng, 2007).
key differences IGA-WoLF WPLs update rules despite apparent
similarity. algorithms two modes learning rates (corresponding two conditions
policy update rule). However, IGA-WoLF needs know equilibrium strategy order
distinguish two modes, unlike WPL needs know value action.
Another difference IGA-WoLF used two fixed learning rates (l > w ) two modes,
WPL uses continuous spectrum learning rates, depending current policy.
understood definition WPLs update equations, includes additional
(continuous) scaling factor: . particular feature causes WPLs dynamics non-linear,
discuss following section.

4. Analyzing WPLs Dynamics
Claim 1 WPL non-linear dynamics.
Proof.
policies two agents following WPL expressed follows


t1

+ (u3 p



t1

+ (u1 q

q q

t1

+ u4 )



1 q t1 u3 pt1 + u4 > 0
q t1
otherwise

+ u2 )



1 pt1 u1 q t1 + u2 > 0
pt1
otherwise


p p

t1

Note u1 q t1 + u2 = Vr (1, q t1 ) Vr (0, q t1 ) Equation 5. follows
pt pt1
=
(t (t 1))
(u1 q

t1

+ u2 )



1 pt1 Vr (1, q t1 ) > Vr (0, q t1 )
pt1
otherwise



1 q t1 Vc (1, pt1 ) > Vc (0, pt1 )
q t1
otherwise

analogously column player
q q t1
=
(t (t 1))
(u3 p

t1

+ u4 )

0, equations become differential:
q 0 (t) =
(u3 p

t1

+ u4 )



1 q t1 Vc (1, pt1 ) > Vc (0, pt1 )
q t1
otherwise
531

(6)

fiA BDALLAH L ESSER

p0 (t) =
(u1 q

t1

+ u2 )



1 pt1 Vr (1, q t1 ) > Vr (0, q t1 )
pt1
otherwise

(7)

Notice NE strategy appear WPLs equations, unlike IGA-WoLF. Furthermore, IGA IGA-WoLF needed take projection function account,
safely ignore projection function analyzing dynamics WPL two-player-twoaction games. due way WPL scales learning rate using current policy.
definition p0 (t), positive p0 (t) approaches zero p approaches one negative p0 (t)
approaches zero p approaches zero. words, p (or q) approaches 0 1, learning
rate approaches zero, therefore p (or q) never go beyond simplex valid policies.7
observation become apparent Section 4.2 compare dynamics WPL
dynamics IGA IGA-WoLF.
Following IGA-WoLFs analysis (Bowling & Veloso, 2002), illustrated Figure 3,
focus challenging case deterministic NE (the NE inside joint policy
space) analyze p q evolve time. case original IGA oscillated
shown Figure 1. important note, however, gradient ascent MARL algorithms
(including WPL) converge 2x2 games cases (at least) one pure NE,
dynamics loops eventually lead one pure equilibriums (Singh et al.,
2000; Bowling & Veloso, 2002).
solve WPLs differential equations period 0 4 Figure 3, assuming
Player 2 starts NE policy q time 0 returns q time 4. prove that,
period 0 4, Player 2s policy p(t) gets closer NE policy p , i.e. pmin2 pmin1 > 0
Figure 3, induction next time period p get closer equilibrium on.
readability, p q used instead p(t) q(t) remainder section.
Without loss generality, assume u1 q + u2 > 0 iff q > q u3 p + u4 > 0 iff p < p .
overall period 0 4 divided four intervals defined times 0, 1, 2, 3, 4.
period corresponds one combination p q follows. first period 0 1,
p < p q > q , therefore agent p better moving toward NE, agent q better
moving away NE. differential equations solved dividing p0 q 0
dp
(1 p)(u1 q + u2 )
=
dq
(1 q)(u3 p + u4 )
separation
Z

p

pmin1

u3 p + u4
dp =
1p

Z

qmax

q

u1 q + u2
dq
1q

1 pmin1
=
1 p
1 q
u1 (qmax q ) + (u1 + u2 )ln
1 qmax

u3 (p pmin1 ) + (u3 + u4 )ln

7. practice WPL still needs projection function must set small value strictly larger 0.

532

fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS

Policy
q
qmax

(p*,qmax)

q*
qmin

(pmin2,q*)
(pmin1,q*)

pmax

(pmax,q*)

p

p*
pmin2

pmin1

T1

T2

T3

time

T4

Figure 3: illustration WPLs dynamics. figure left shows policies evolution time,
figure right shows joint policy space.

Similarly, period 1 2, p > p q > q
1 p
=
1 pmax
q
u1 (q qmax ) + u2 ln
qmax

u3 (pmax p ) + (u3 + u4 )ln

period 2 3, p > p q < q
u3 (p pmax ) + u4 ln

p
pmax

= u1 (qmin q ) + u2 ln

qmin
q

finally period 3 4, p < p q < q
pmin2
=
p
1 qmin
u1 (q qmin ) + (u1 + u2 )ln
1 q
u3 (pmin2 p ) + u4 ln

4 non-linear equations (note existence x ln(x) equations)
5 unknowns (pmin , pmin2 , pmax , qmin1 , qmax ), along inequalities governing constants
u1 , u2 , u3 , u4 .
WPLs dynamics non-linear, could obtain closed-form solution therefore could formally prove WPLs convergence.8 Instead, solve equations numerically
following section. Although numerical solution still formal proof, provides useful
insights understanding WPLs dynamics.
8. equations linear, could substituted unknowns terms pmin1 p2min easily
determined whether pmin2 pmin1 > 0, similar IGA-WoLF.

533

fiA BDALLAH L ESSER

4.1 Numerical Solution
used Matlab solve equations numerically. Figure 4 shows theoretical behavior
predicted model matching-pennies game. clear resemblance actual
(experimental) behavior game (Figure 5). Note time-scale horizontal
axes figures effectively same, displayed horizontal axis
Figure 5 decision steps simulation. multiplied actual policy-learning-rate
(the time step) used experiments, 0.001, axes become identical.
1
0.9
0.8
0.7

policy

0.6
0.5
0.4
0.3
0.2
0.1
0

0

2

4

6

8

10

time

Figure 4: Convergence WPL predicted theoretical model matching pennies game.

1

player1
player2

0.8

policy

0.6
0.4
0.2
0
0

2000

4000

6000

8000

10000

time

Figure 5: Convergence WPL experiments, using = 0.001 .
Figure 6 plots p(t) versus q(t), game NE= (0.9, 0.9) (u1 = 0.5, u2 = 0.45, u3 =
0.5, , u4 = 0.45) starting 157 different initial joint policies (40 initial policies
side joint policy space). Figure 7 plots p(t) q(t) time, showing convergence
157 initial joint policies.
repeated numerical solution 100 different NE(s) make 10x10 grid
p-q space (and starting 157 boundary initial joint policies individual NE).
WPL algorithm converges NE spiral fashion similar specific case Figure 6
100 NE(s). Instead drawing 100 figures (one NE), Figure 8 plots merge
100 figures compact way: plotting agents joint policy time 700 time 800 (which
enough convergence Figure 7 shows). two agents converge 100 NE cases,
indicated centric points. Note algorithm converging, joint policies
trajectories time 700 occupy space, 157 initial joint policies
policy space boundary.
534

fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS

q

p
Figure 6: illustration WPL convergence (0.9,0.9) NE p-q space: p horizontal axis
q vertical axis.

policy

time

Figure 7: illustration WPL convergence (0.9,0.9) NE: p(t) (gray) q(t) (black) plotted
vertical axis time (horizontal axis).

Figure 8: illustration WPLs convergence 10x10 NE(s).
535

fiA BDALLAH L ESSER

4.2 Comparing Dynamics IGA, IGA-WoLF, WPL
differential equations modeling three algorithms, compare dynamics
point main distinguishing characteristics WPL. Matlab used solve
differential equations (of three algorithms) numerically. Figure 9 shows dynamics
game u1u3 < 0 NE=(0.5,0.5). joint strategy moves clockwise direction.
dynamics WPL close IGA-WoLF, IGA-WoLF converging bit faster (after one
complete round around NE, IGA-WoLF closer NE WPL). still impressive
WPL comparable performance IGA-WoLF, since WPL require agents know
NE strategy underlying game priori, unlike IGA-WoLF.
1
IGA
IGAWoLF
WPL

0.9
0.8
0.7

player 2

0.6
0.5
0.4
0.3
0.2
0.1
0

0

0.2

0.4

0.6

0.8

1

player 1

Figure 9: Dynamics IGA, IGA-WoLF, WPL game NE=(0.5,0.5).
Figure 10 shows dynamics game u1u3 < 0 NE=(0.5,0.1). Three interesting
regions figure designated A,B, C. Region shows IGA IGAWoLF dynamics discontinuous due effect projection function. WPL uses
smooth policy weighting scheme, dynamics remain continuous. also true region B.
region C, WPL initially deviates NE IGA, eventually converges well.
reason NE, case, closer boundary, policy weighting makes
vertical player move much slower pace moving downward (the right half)
horizontal player.
Figure 11 shows dynamics coordination game (Table 1(a)), starting initial joint
policy (0.1,0.6). coordination game two NEs: (0,0) (1,1). algorithms converge
closer NE, (0,0), see IGA IGA-WoLF discontinuity
dynamics, unlike WPL smoothly converge NE. Notice WPL converges pure
NE limit, graph shows joint policy space, depiction time.
continuity WPLs dynamics comes play target policy mixed. IGA,
GIGA, GIGA-WoLF algorithms go extreme deterministic policies transient
period prior convergence, cause ripple effect realistic settings large number
agents interacting asynchronously (e.g. network). WPL never reaches extremes
experimental results Section 5.4 show GIGA-WoLF takes significantly time
converge compared WPL domain 100 agents.
536

fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS

1
IGA
IGAWoLF
WPL

0.8

player 2


0.6

0.4

0.2

0

C

B
0

0.1

0.2

0.3

0.4

0.5
player 1

0.6

0.7

0.8

0.9

1

Figure 10: Dynamics IGA, IGA-WoLF, WPL game NE=(0.5,0.1). Three regions
particular interest highlighted figure: A, B, C.

1
IGA
IGAWoLF
WPL

0.9
0.8
0.7

player 2

0.6
0.5
0.4
0.3
0.2
0.1
0

0

0.1

0.2

0.3

0.4

0.5
0.6
player 1

0.7

0.8

0.9

Figure 11: Dynamics IGA, IGA-WoLF, WPL coordination game two NEs=(0,0)
(1,1). Note IGAs dynamics exactly IGA-WoLFs dynamics case.

following section presents experimental results benchmark 2-player-2-action
games larger games involving actions agents.

5. Experimental Results
section divided three parts. Section 5.1 discusses main learning parameters need
set practice. Section 5.2 presents experimental results 2x2 benchmark games.
Section 5.3 presents experimental results domains larger number actions and/or
agents.
537

fiA BDALLAH L ESSER

5.1 Learning Parameters
Conducting experiments WPL involves setting two main parameters: policy-learning-rate,
, value-learning-rate, . theory (as discussed Section 4) policy-learning-rate,
, approach zero. practice, however, possible tried setting
different small values 0.01 0.00001. smaller is, longer take WPL
converge, smaller oscillations around NE become (and vice versa), similar
previous GA-MARL algorithms. reasonable value used experiments
= 0.002.
value-learning-rate used compute expected reward action time t,
rt (a), known priori. common approach, used previous GA-MARL
algorithms also use here, using equation rt+1 (a) Rt + (1 )rt (a), Rt
sample reward received time 0 1 (Sutton & Barto, 1999). tried three
values : 0.01,0.1, 1.
5.2 Benchmark 2x2 Games
implemented set previous MARL algorithms comparison: PHC-WoLF (the realistic
implementation IGA-WoLF), GIGA, GIGA-WoLF. experiments used
decaying rates. reason open system dynamics continuously change, one
would want learning continuous well. fixed exploration rate 0.1 (which
comes play modified projection function Section 3), policy-learning-rate
0.002, value-learning-rate 0.1 (unless otherwise specified).
first set experiments show results applying algorithm three benchmark
games described Table 1 10 simulation runs. Figure 12 plots (r1) (c1) (i.e. probability choosing first action row player column player respectively) time.
matching pennies tricky games, initial joint policy ([0.1, 0.9]r , [0.9, 0.1]c ) (we
also tested 0.5,0.5 initial joint policy similar results) plot average across
10 runs, standard deviation shown vertical bar. coordination game plotted single
run two probable pure NEs averaging runs capture that. WPL
converges one NE strategies runs.
Figure 13 shows convergence GIGA, PHC-WoLF, GIGA-WoLF algorithms
coordination game. expected, GIGA, PHC-WoLF, GIGA-WoLF converges one NE
strategies faster, WPL slightly biased pure strategies.
Figure 14 shows convergence previous GA-MARL algorithms matching pennies
game. GIGA hopelessly oscillates around NE, expected. PHC-WoLF better, relatively high standard deviation across runs (when compared WPL). GIGA-WoLF comparable
performance WPL, GIGA-WoLF takes longer converge (the width oscillations around
NE higher WPL).
tricky game deserves attention one challenging two-player-two-action
games (Bowling & Veloso, 2002). Figure 15 shows convergence PHC-WoLF, GIGA,
GIGA-WoLF. GIGA-WoLF converges slower rate approach (Figure 12(c)).
performance GIGA, PHC-WoLF, GIGA-WoLF conforms results reported previously authors (Bowling & Veloso, 2002; Bowling, 2005). remainder experiments (Section 5.3) focuses GIGA-WoLF performed best among previous
GA-MARL algorithms.
538

fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS

1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(a) WPL: coordination game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(b) WPL: matching pennies game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(c) WPL: tricky game

Figure 12: Convergence WPL benchmark two-player-two-action games. figures plot probability playing first action player (vertical axis) time (horizontal axis).

5.3 Games Larger 2x2
Figures 16 17 plot policy row player time games Table 2
WPL GIGA-WoLF initial joint strategy ([0.1, 0.8, 0.1]r , [0.8, 0.1, 0.1]c ) (we
also tried ([ 13 , 13 , 31 ]r , [ 31 , 13 , 13 ]c ), produced similar results), = 0.001, two values
: 0.1 1. rock-paper-scissors game (Figure 16) GIGA-WoLF WPL converge
= 0.1, WPL converges = 1. Shapleys game (Figure 17) GIGAWoLF keeps oscillating = 1 = 0.1 (GIGA-WoLFs performance gets worse
539

fiA BDALLAH L ESSER

1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(a) PHC-WoLF: coordination game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(b) GIGA: coordination game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(c) GIGA-WoLF: coordination game

Figure 13: Convergence previous GA-MARL algorithms (GIGA, PHC-WoLF, GIGA-WoLF)
coordination game. figures plot probability playing first action player
(vertical axis) time (horizontal axis).

increases). WPL hand performs better increases converges Shapleys game
= 1.
believe reason small leads out-of-date reward estimate turn
leads agents continuously chase NE without successfully converging. Smaller means
samples contribute computed expected reward. Using samples estimate reward
makes estimate accurate. However, time required get samples may fact
degrade accuracy reward estimate, expected reward changes time.
540

fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS

1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(a) PHC-WoLF: matching game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(b) GIGA: matching game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(c) GIGA-WoLF: matching game

Figure 14: Convergence GA-MARL algorithms (GIGA, PHC-WoLF, GIGA-WoLF) matching pennies game. figures plot probability playing first action player
(vertical axis) time (horizontal axis).

Setting value always 1 result sub-optimal performance, illustrated
following experiment. Consider biased game shown Table 3. NE policy biased
game mixed probabilities uniform across actions, unlike previous benchmark games
mixed NE uniform probability across actions. set 0.01, WPL converges
policy close NE policy shown Figure 18. = 1, WPL converged policy
far NE policy shown Figure 19.
541

fiA BDALLAH L ESSER

1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(a) PHC-WoLF: tricky game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(b) GIGA: tricky game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(c) GIGA-WoLF: tricky game

Figure 15: Convergence GA-MARL algorithms (GIGA, PHC-WoLF, GIGA-WoLF) tricky
game. figures plot probability playing first action player (vertical axis)
time (horizontal axis).

Table 3: Biased game: NE=(0.15,0.85) & (0.85,0.15)
a1
a2
a1 1.0,1.85 1.85,1.0
a2 1.15,1.0 1.00,1.15

542

fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS

1

1

a0

a0

a1
0.8

a1
0.8

a2

0.6

0.6

policy

policy

0.4

0.4

0.2

0.2

0

a2

0
0

50000

100000

150000

200000

0

50000

time

100000

150000

200000

time
(b) WPL, = 0.1

(a) GIGA-WoLF, = 0.1
1

1

a0

a0

a1
0.8

a1
0.8

a2

0.6

a2

0.6

policy

policy

0.4

0.4

0.2

0.2

0

0
0

100000

200000

300000

400000

500000

600000

0

time

50000

100000

150000

200000

time

(c) GIGA-WolF, = 1

(d) WPL, = 1

Figure 16: Convergence GIGA-WoLF WPL rock-paper-scissors game. figures plot
probability playing action first player (vertical axis) time (horizontal axis).

understand effect = 1 case WPL, consider following scenario
biased game. Suppose policy column player fixed 0.7,0.3. value small,
action value function approximates well expected value action. value
first row action (from row player perspective) = 0.71 + 0.31.85= 1.255. Similarly,
value second row action = 0.7 1.15 + 0.31 = 1.105. Unless column player changes
policy, row player gradient clearly points toward increasing probability choosing
first action (up 1). consider case = 1. case action value reflects
latest (sample) reward, average. case, probability choosing first action, p,
increases average
=

0.7 [0.7 (1 1.15) p + 0.35 (1 1)] + 0.3
[0.7 (1.85 1.15) (1 p) + 0.3 (1.85 1) (1 p)]

=

0.297p + 0.2235

means row players policy effectively stabilize = 0 p = 0.75.
words, possible players stabilize equilibrium NE. Note
problem occurs mainly biased games. common benchmark games NE uniform
across actions pure, WPL still converge NE even = 1, shown Figure 16
Figure 17.
Tuning easy task expected reward dynamically changing (because
agents changing policies concurrently). currently investigating extension
543

fiA BDALLAH L ESSER

1

1

a0

a0

a1
0.8

a1
0.8

a2

0.6

a2

0.6

policy

policy

0.4

0.4

0.2

0.2

0

0
0

50000

100000

150000

200000

0

50000

time
(a) GIGA-WolF, = 0.1

100000

150000

200000

time
(b) WPL, = 0.1

1

1

a0

a0

a1
0.8

a1
0.8

a2

0.6

0.6

policy

policy

0.4

0.4

0.2

0.2

0

a2

0
0

50000

100000

150000

200000

0

50000

100000

time

150000

200000

time

(c) GIGA-WolF, = 1

(d) WPL, = 1

Figure 17: Convergence GIGA-WoLF WPL Shapleys game. figures plot probability
playing action first player (vertical axis) time (horizontal axis).

1

player1
player2

0.8
0.6

policy
0.4
0.2
0
0

200000

400000

600000

800000

1e+006

time

Figure 18: Convergence WPL fixed value-learning-rate 0.01 biased game. Horizontal axis
time. Vertical axis probability first action player.

WPL automatically recognizes oscillations adjusts value accordingly. Preliminary
results promising.
following section illustrates applicability algorithm applying WPL
realistic setting involving 100 agents.
544

fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS

1

player1
player2

0.8

policy

0.6
0.4
0.2
0
0

500000

1e+006

1.5e+006

2e+006

2.5e+006

3e+006

time

Figure 19: Convergence WPL fixed value-learning-rate 1.0 biased game. Horizontal axis
time. Vertical axis probability first action player.

5.4 Distributed Task Allocation Problem (DTAP)
use simplified version distributed task allocation domain (DTAP) (Abdallah & Lesser,
2007), goal multiagent system assign tasks agents service
time task minimized. illustration, consider example scenario depicted Figure
20. Agent A0 receives task T1, executed agents A0, A1, A2, A3,
A4. agents agent A4 overloaded, therefore best option agent A0
forward task T1 agent A2 turn forwards task left neighbor (A5) task
T1 reaches agent A4. Although agent A0 know A4 under-loaded (because agent
A0 interacts immediate neighbors), agent A0 eventually learn (through experience
interaction neighbors) sending task T1 agent A2 best action without even
knowing agent A4 exists.

A1

A0
A5

A2
A3

T1

A4

Figure 20: Task allocation using network agents.
Q-learning appropriate setting due communication delay (which results
partial observability). example, even two neighbors practically load, Q-learning
assign incoming requests one neighbors feedback received later indicating
change load. noted Q-learning successfully used packet routing
domain (Boyan & Littman, 1994; Dutta, Jennings, & Moreau, 2005), load balancing
545

fiA BDALLAH L ESSER

main concern (the main objective routing packet particular source particular
destination).
let us define different aspects DTAP domain formally. time unit,
agents make decisions regarding task requests received time unit. task,
agent either execute task locally send task neighboring agent. agent decides
execute task locally, agent adds task local queue, tasks executed
first come first serve basis, unlimited queue length.
agent physical location. Communication delay two agents proportional
Euclidean distance them, one time unit per distance unit. Agents interact via two
types messages. REQUEST message hi, j, indicates request sent agent agent
j requesting task . UPDATE message hi, j, T, Ri indicates feedback (reward signal)
agent agent j task took R time steps complete (the time steps computed since agent
received request).
P main goal DTAP reduce total service time, averaged tasks, ST =


ST (T )

, set task requests received time period TST
total time task spends system. TST consists time routing task request
network, time task request spends local queue, time actually executing
task.
noted although underlying simulator different underlying states,
deliberately made agent oblivious states. feedback agent gets (consistent
initial claim) reward. agents learn joint policy makes good compromise
different unobserved states (because agents distinguish states).
evaluated WPLs performance using following setting.9 100 agents organized
10x10 grid. Communication delay two adjacent agents two time units. Tasks arrive
4x4 sub-grid center rate 0.5 tasks/time unit. agents execute task rate
0.1 task/time unit (both task arrival service durations follow exponential distribution).
Figure 21 shows results applying GIGA, GIGA-WoLF WPL using value-learning-rate
1 policy-learning-rate 0.0001. GIGA fail converge, WPL GIGA-WoLF
converge. WPL converges faster better ATST: GIGA-WoLFs ATST converges around
100 time units, WPLs ATST converges around 70 time units.
believe GIGA-WoLFs slow convergence due way GIGA-WoLF works. GIGAWoLF relies learning slowly moving policy addition actual policy order approximate NE policy. requires time WPL algorithm. Furthermore, GIGA-WoLFs
dynamics discontinuous prior convergence reach extreme deterministic policies even
NE policy mixed. large system, ripple effect slow system-wide
convergence. WPL, hand, continuous dynamics, allowing faster collective convergence.
|T |

6. Conclusion Future Work
work presents WPL, gradient ascent multiagent reinforcement learning algorithm (GAMARL) assumes agent neither knows underlying game observes agents.
experimentally show WPL converges benchmark 2-player-2-action games. also show
9. simulator available online http://www.cs.umass.edu/shario/dtap.html.

546

fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS

3000

WPL
GIGA

2500

GIGAWoLF

2000

ATST

1500
1000
500
0
0

20000

40000

60000

80000

100000

time

Figure 21: ATST 10x10 grid different MARL algorithms. Horizontal axis time vertical axis
ATST.

WPL converges Shapleys game none previous GA-MARL algorithms successfully converged. verify practicality algorithm distributed task allocation domain
network 100 agents. WPL outperforms state-of-the-art GA-MARL algorithms
speed convergence expected reward. analyze dynamics WPL show
continuous non-linear dynamics, previous GA-MARL algorithms discontinuous dynamics. show predicted theoretical behavior consistent experimental
results.
work briefly illustrated importance value-learning-rate affects convergence, particularly proposed algorithm WPL. Finding right balance theoretically
analyzing affects convergence interesting research questions. currently investigating extension WPL automatically finds good value-learning-rate. Preliminary experimental results promising.
Another future direction considering extending theoretical analysis games
actions players order verify experimental findings Shapleys game
distributed task allocation domain. currently investigating alternative methodologies
analyzing dynamics, including evolutionary dynamics (Tuyls, Hoen, & Vanschoenwinkel,
2006) Lyapunov stability theory (Khalil, 2002).

7. Acknowledgments
work based previous conference publications (Abdallah & Lesser, 2006, 2008).

References
Abdallah, S., & Lesser, V. (2006). Learning task allocation game. Proceedings International Joint Conference Autonomous Agents Multiagent Systems, pp. 850857.
Abdallah, S., & Lesser, V. (2007). Multiagent reinforcement learning self-organization
network agents. Proceedings International Joint Conference Autonomous
Agents Multiagent Systems.
547

fiA BDALLAH L ESSER

Abdallah, S., & Lesser, V. (2008). Non-linear dynamics multiagent reinforcement learning algorithms. Proceedings International Joint Conference Autonomous Agents
Multiagent Systems, pp. 13211324.
Banerjee, B., & Peng, J. (2003). Adaptive policy gradient multiagent learning. Proceedings
International Joint Conference Autonomous Agents Multi Agent Systems, pp.
686692.
Banerjee, B., & Peng, J. (2007). Generalized multiagent learning performance bound. Autonomous Agents Multiagent Systems, 15(3), 281312.
Bowling, M. (2004). Convergence no-regret multiagent learning. Tech. rep., University
Alberta.
Bowling, M. (2005). Convergence no-regret multiagent learning. Proceedings
Annual Conference Advances Neural Information Processing Systems, pp. 209216.
Bowling, M., & Veloso, M. (2002). Multiagent learning using variable learning rate. Artificial
Intelligence, 136(2), 215250.
Boyan, J. A., & Littman, M. L. (1994). Packet routing dynamically changing networks:
reinforcement learning approach. Proceedings Annual Conference Advances
Neural Information Processing Systems, pp. 671678.
Claus, C., & Boutilier, C. (1998). dynamics reinforcement learning cooperative multiagent
systems.. Proceedings National Conference Artificial intelligence/Innovative
Applications Artificial Intelligence, pp. 746752.
Conitzer, V., & Sandholm, T. (2007). AWESOME: general multiagent learning algorithm
converges self-play learns best response stationary opponents. Machine
Learning, 67(1-2), 2343.
Dutta, P. S., Jennings, N. R., & Moreau, L. (2005). Cooperative information sharing improve
distributed learning multi-agent systems. Journal Artificial Intelligence Research, 24,
407463.
Hu, J., & Wellman, M. P. (2003). Nash Q-learning general-sum stochastic games. Journal
Machine Learning Research, 4, 10391069.
Khalil, H. K. (2002). Nonlinear Systems. Prentice-Hall, Upper Saddle River, NJ, USA.
Littman, M. (2001). Value-function reinforcement learning Markov games. Cognitive Systems
Research, 2(12), 5566.
Peshkin, L., Kim, K.-E., Meuleau, N., & Kaelbling, L. P. (2000). Learning cooperate via policy
search. Proceedings Conference Uncertainty Artificial Intelligence, pp. 307
314.
Singh, S., Kearns, M., & Mansour, Y. (2000). Nash convergence gradient dynamics generalsum games. Proceedings Conference Uncertainty Artificial Intelligence, pp.
541548.
548

fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS

Sutton, R., & Barto, A. (1999). Reinforcement Learning: Introduction. MIT Press.
Tuyls, K., Hoen, P. J., & Vanschoenwinkel, B. (2006). evolutionary dynamical analysis
multi-agent learning iterated games. Autonomous Agents Multi-Agent Systems, 12(1),
115153.
Wang, X., & Sandholm, T. (2003). Reinforcement learning play optimal Nash equilibrium
team Markov games. Proceedings Annual Conference Advances Neural
Information Processing Systems, pp. 15711578.
Zinkevich, M. (2003). Online convex programming generalized infinitesimal gradient ascent..
Proceedings International Conference Machine Learning, pp. 928936.

549

fiJournal Artificial Intelligence Research 33 (2008) 615-655

Submitted 09/08; published 12/08

Latent Relation Mapping Engine:
Algorithm Experiments
Peter D. Turney

peter.turney@nrc-cnrc.gc.ca

Institute Information Technology
National Research Council Canada
Ottawa, Ontario, Canada, K1A 0R6

Abstract
Many AI researchers cognitive scientists argued analogy core
cognition. influential work computational modeling analogy-making
Structure Mapping Theory (SMT) implementation Structure Mapping Engine
(SME). limitation SME requirement complex hand-coded representations.
introduce Latent Relation Mapping Engine (LRME), combines ideas
SME Latent Relational Analysis (LRA) order remove requirement handcoded representations. LRME builds analogical mappings lists words, using
large corpus raw text automatically discover semantic relations among words.
evaluate LRME set twenty analogical mapping problems, ten based scientific
analogies ten based common metaphors. LRME achieves human-level performance
twenty problems. compare LRME variety alternative approaches
find able reach level performance.

1. Introduction
faced problem, try recall similar problems faced
past, transfer knowledge past experience current
problem. make analogy past situation current situation,
use analogy transfer knowledge (Gentner, 1983; Minsky, 1986; Holyoak & Thagard,
1995; Hofstadter, 2001; Hawkins & Blakeslee, 2004).
survey computational modeling analogy-making, French (2002) cites
Structure Mapping Theory (SMT) (Gentner, 1983) implementation Structure
Mapping Engine (SME) (Falkenhainer, Forbus, & Gentner, 1989) influential
work modeling analogy-making. SME, analogical mapping : B
source target B. source familiar, known, concrete,
whereas target relatively unfamiliar, unknown, abstract. analogical mapping
used transfer knowledge source target.
Gentner (1983) argues two kinds similarity, attributional similarity
relational similarity. distinction attributes relations may understood terms predicate logic. attribute predicate one argument,
large(X), meaning X large. relation predicate two arguments,
collides with(X, ), meaning X collides .
Structure Mapping Engine prefers mappings based relational similarity
mappings based attributional similarity (Falkenhainer et al., 1989). example, SME
able build mapping representation solar system (the source)
c
2008
National Research Council Canada. Reprinted permission.

fiTurney

representation Rutherford-Bohr model atom (the target). sun mapped
nucleus, planets mapped electrons, mass mapped charge. Note
mapping emphasizes relational similarity. sun nucleus different
terms attributes: sun large nucleus small. Likewise,
planets electrons little attributional similarity. hand, planets revolve
around sun like electrons revolve around nucleus. mass sun attracts
mass planets like charge nucleus attracts charge electrons.
Gentner (1991) provides evidence children rely primarily attributional similarity
mapping, gradually switching relational similarity mature. uses
terms mere appearance refer mapping based mostly attributional similarity, analogy
refer mapping based mostly relational similarity, literal similarity refer
mixture attributional relational similarity. Since use analogical mappings solve
problems make predictions, focus structure, especially causal relations,
look beyond surface attributes things (Gentner, 1983). analogy
solar system Rutherford-Bohr model atom illustrates importance
going beyond mere appearance, underlying structures.
Figures 1 2 show LISP representations used SME input analogy
solar system atom (Falkenhainer et al., 1989). Chalmers, French,
Hofstadter (1992) criticize SMEs requirement complex hand-coded representations.
argue hard work done human creates high-level
hand-coded representations, rather SME.
(defEntity sun :type inanimate)
(defEntity planet :type inanimate)
(defDescription solar-system
entities (sun planet)
expressions (((mass sun) :name mass-sun)
((mass planet) :name mass-planet)
((greater mass-sun mass-planet) :name >mass)
((attracts sun planet) :name attracts-form)
((revolve-around planet sun) :name revolve)
((and >mass attracts-form) :name and1)
((cause and1 revolve) :name cause-revolve)
((temperature sun) :name temp-sun)
((temperature planet) :name temp-planet)
((greater temp-sun temp-planet) :name >temp)
((gravity mass-sun mass-planet) :name force-gravity)
((cause force-gravity attracts-form) :name why-attracts)))

Figure 1: representation solar system SME (Falkenhainer et al., 1989).
Gentner, Forbus, colleagues attempted avoid hand-coding
recent work SME.1 CogSketch system generate LISP representations
simple sketches (Forbus, Usher, Lovett, Lockwood, & Wetzel, 2008). Gizmo system
generate LISP representations qualitative physics models (Yan & Forbus, 2005).
Learning Reader system generate LISP representations natural language text
(Forbus et al., 2007). systems require LISP input.
1. Dedre Gentner, personal communication, October 29, 2008.

616

fiThe Latent Relation Mapping Engine

(defEntity nucleus :type inanimate)
(defEntity electron :type inanimate)
(defDescription rutherford-atom
entities (nucleus electron)
expressions (((mass nucleus) :name mass-n)
((mass electron) :name mass-e)
((greater mass-n mass-e) :name >mass)
((attracts nucleus electron) :name attracts-form)
((revolve-around electron nucleus) :name revolve)
((charge electron) :name q-electron)
((charge nucleus) :name q-nucleus)
((opposite-sign q-nucleus q-electron) :name >charge)
((cause >charge attracts-form) :name why-attracts)))

Figure 2: Rutherford-Bohr model atom SME (Falkenhainer et al., 1989).

However, CogSketch user interface requires person draws sketch identify basic components sketch hand-label terms knowledge
base derived OpenCyc. Forbus et al. (2008) note OpenCyc contains
58,000 hand-coded concepts, added hand-coded concepts OpenCyc,
order support CogSketch. Gizmo system requires user hand-code physical
model, using methods qualitative physics (Yan & Forbus, 2005). Learning Reader
uses 28,000 phrasal patterns, derived ResearchCyc (Forbus
et al., 2007). evident SME still requires substantial hand-coded knowledge.
work present paper effort avoid complex hand-coded representations. approach combine ideas SME (Falkenhainer et al., 1989) Latent
Relational Analysis (LRA) (Turney, 2006). call resulting algorithm Latent Relation Mapping Engine (LRME). represent semantic relation two terms
using vector, elements derived pattern frequencies large corpus
raw text. semantic relations automatically derived corpus, LRME
require hand-coded representations relations. needs list terms
source list terms target. Given two lists, LRME uses corpus
build representations relations among terms, constructs mapping
two lists.
Tables 1 2 show input output LRME analogy solar
system Rutherford-Bohr model atom. Although human effort involved
constructing input lists, considerably less effort SME requires input
(contrast Figures 1 2 Table 1).
Scientific analogies, analogy solar system RutherfordBohr model atom, may seem esoteric, believe analogy-making ubiquitous
daily lives. potential practical application work task identifying
semantic roles (Gildea & Jurafsky, 2002). Since roles relations, attributes,
appropriate treat semantic role labeling analogical mapping problem.
example, Judgement semantic frame contains semantic roles judge,
evaluee, reason, Statement frame contains roles speaker, addressee, message, topic, medium (Gildea & Jurafsky, 2002). task identifying
617

fiTurney

Source
planet
attracts
revolves
sun
gravity
solar system
mass

Target B
revolves
atom
attracts
electromagnetism
nucleus
charge
electron

Table 1: representation input LRME.
Source
solar system
sun
planet
mass
attracts
revolves
gravity

Mapping








Target B
atom
nucleus
electron
charge
attracts
revolves
electromagnetism

Table 2: representation output LRME.
semantic roles automatically label sentences roles, following examples (Gildea & Jurafsky, 2002):
[Judge She] blames [Evaluee Government] [Reason failing enough
help].
[Speaker We] talked [Topic proposal] [Medium phone].
training set labeled sentences testing set unlabeled sentences,
may view task labeling testing sentences problem creating analogical
mappings training sentences (sources) testing sentences (targets). Table 3 shows blames Government failing enough help. might
mapped blame company polluting environment. mapping
found, transfer knowledge, form semantic role labels, source
target.
Source

blames
government
failing
help

Mapping






Target B

blame
company
polluting
environment

Table 3: Semantic role labeling analogical mapping.
Section 2, briefly discuss hypotheses behind design LRME.
precisely define task performed LRME, specific form analogical mapping,
618

fiThe Latent Relation Mapping Engine

Section 3. LRME builds Latent Relational Analysis (LRA), hence summarize LRA
Section 4. discuss potential applications LRME Section 5.
evaluate LRME, created twenty analogical mapping problems, ten science analogy problems (Holyoak & Thagard, 1995) ten common metaphor problems (Lakoff &
Johnson, 1980). Table 1 one science analogy problems. intended solution
given Table 2. validate intended solutions, gave colleagues lists
terms (as Table 1) asked generate mappings lists. Section 6
presents results experiment. Across twenty problems, average agreement
intended solutions (as Table 2) 87.6%.
LRME algorithm outlined Section 7, along evaluation twenty
mapping problems. LRME achieves accuracy 91.5%. difference
performance human average 87.6% statistically significant.
Section 8 examines variety alternative approaches analogy mapping task.
best approach achieves accuracy 76.8%, approach requires hand-coded partof-speech tags. performance significantly LRME human performance.
Section 9, discuss questions raised results preceding
sections. Related work described Section 10, future work limitations considered
Section 11, conclude Section 12.

2. Guiding Hypotheses
section, list assumptions guided design LRME.
results present paper necessarily require assumptions, might
helpful reader, understand reasoning behind approach.
1. Analogies semantic relations: Analogies based semantic relations
(Gentner, 1983). example, analogy solar system Rutherford-Bohr model atom based similarity semantic relations
among concepts involved understanding solar system semantic
relations among concepts involved Rutherford-Bohr model atom.
2. Co-occurrences semantic relations: Two terms interesting, significant semantic relation tend co-occur within relatively
small window (e.g., five words) relatively large corpus (e.g., 1010 words).
interesting semantic relation causes co-occurrence co-occurrence reliable
indicator interesting semantic relation (Firth, 1957).
3. Meanings semantic relations: Meaning relations among
words individual words. Individual words tend ambiguous polysemous.
putting two words pair, constrain possible meanings. putting
words sentence, multiple relations among words sentence,
constrain possible meanings further. focus word pairs (or tuples), instead
individual words, word sense disambiguation less problematic. Perhaps word
sense apart relations words (Kilgarriff, 1997).
4. Pattern distributions semantic relations: many-to-many mapping semantic relations patterns two terms co-occur.
example, relation CauseEffect(X, ) may expressed X causes ,
619

fiTurney

X, due X, X, on. Likewise, pattern
X may expression CauseEffect(X, ) (sick bacteria)
OriginEntity(X, ) (oranges Spain). However, given X , statistical distribution patterns X co-occur reliable signature
semantic relations X (Turney, 2006).
extent LRME works, believe success lends support hypotheses.

3. Task
paper, examine algorithms generate analogical mappings. simplicity,
restrict task generating bijective mappings; is, mappings injective
(one-to-one; instance two terms source map term
target) surjective (onto; source terms cover target terms;
target term left mapping). assume entities
mapped given input. Formally, input algorithms two sets terms,
B.
= {hA, Bi}

(1)

Since mappings bijective, B must contain number terms, m.
= {a1 , a2 , . . . , }

(2)

B = {b1 , b2 , . . . , bm }

(3)

term, ai bj , may consist single word (planet) compound two words
(solar system). words may part speech (nouns, verbs, adjectives, adverbs).
output bijective mapping B.
= {M : B}

(4)

(ai ) B

(5)

(A) = {M (a1 ), (a2 ), . . . , (am )} = B

(6)

algorithms consider accept batch multiple independent mapping
problems input generate mapping one output.
= {hA1 , B1 , hA2 , B2 , . . . , hAn , Bn i}

(7)

= {M1 : A1 B1 , M2 : A2 B2 , . . . , Mn : Bn }

(8)

Suppose terms arbitrary order a.
= ha1 , a2 , . . . ,
mapping function : B, given a, determines unique ordering b B.
620

(9)

fiThe Latent Relation Mapping Engine

b = hM (a1 ), (a2 ), . . . , (am )i

(10)

Likewise, ordering b B, given a, defines unique mapping function . Since
m! possible orderings B, also m! possible mappings B. task
search m! mappings find best one. (Section 6 shows
relatively high degree consensus mappings best.)
Let P (A, B) set m! bijective mappings B. (P stands permutation, since mapping corresponds permutation.)
P (A, B) = {M1 , M2 , . . . , Mm! }

(11)

= |A| = |B|

(12)

m! = |P (A, B)|

(13)

following experiments, 7 average 9 most, m! usually around
7! = 5, 040 9! = 362, 880. feasible us exhaustively search P (A, B).
explore two basic kinds algorithms generating analogical mappings, algorithms
based attributional similarity algorithms based relational similarity (Turney,
2006). attributional similarity two words, sima (a, b) <, depends
degree correspondence properties b. correspondence
is, greater attributional similarity. relational similarity two
pairs words, simr (a : b, c : d) <, depends degree correspondence
relations : b c : d. correspondence is, greater relational
similarity. example, dog wolf relatively high degree attributional similarity,
whereas dog : bark cat : meow relatively high degree relational similarity.
Attributional mapping algorithms seek mapping (or mappings) maximizes
sum attributional similarities terms corresponding
terms B. (When multiple mappings maximize sum, break tie
randomly choosing one them.)
= arg max


X

sima (ai , (ai ))

(14)

P (A,B) i=1

Relational mapping algorithms seek mapping (or mappings) Mr maximizes
sum relational similarities.
Mr = arg max

X

X

simr (ai : aj , (ai ) : (aj ))

(15)

P (A,B) i=1 j=i+1

(15), assume simr symmetrical. example, degree relational similarity
dog : bark cat : meow degree relational similarity
bark : dog meow : cat.
simr (a : b, c : d) = simr (b : a, : c)

(16)

also assume simr (a : a, b : b) interesting; example, may constant
value b. Therefore (15) designed always less j.
621

fiTurney

Let scorer (M ) scorea (M ) defined follows.

scorer (M ) =
scorea (M ) =

X

X

simr (ai : aj , (ai ) : (aj ))

i=1 j=i+1

X

sima (ai , (ai ))

(17)

(18)

i=1

Mr may defined terms scorer (M ) scorea (M ).
Mr = arg max scorer (M )

(19)

P (A,B)

= arg max scorea (M )

(20)

P (A,B)

Mr best mapping according simr best mapping according sima .
Recall Gentners (1991) terms, discussed Section 1, mere appearance (mostly attributional similarity), analogy (mostly relational similarity), literal similarity (a mixture
attributional relational similarity). take Mr abstract model mapping based analogy model mere appearance. literal similarity,
combine Mr , take care normalize scorer (M ) scorea (M )
combine them. (We experiment combining Section 9.2.)

4. Latent Relational Analysis
LRME uses simplified form Latent Relational Analysis (LRA) (Turney, 2005, 2006)
calculate relational similarity pairs words. briefly describe past
work LRA present LRME.
LRA takes input set word pairs generates output relational
similarity simr (ai : bi , aj : bj ) two pairs input.
= {a1 : b1 , a2 : b2 , . . . , : bn }

(21)

= {simr : <}

(22)

LRA designed evaluate proportional analogies. Proportional analogies form
: b :: c : d, means b c d. example, mason : stone :: carpenter : wood
means mason stone carpenter wood. mason artisan works
stone carpenter artisan works wood.
consider proportional analogies special case bijective analogical mapping,
defined Section 3, |A| = |B| = = 2. example, a1 : a2 :: b1 : b2 equivalent
M0 (23).
= {a1 , a2 } , B = {b1 , b2 } , M0 (a1 ) = b1 , M0 (a2 ) = b2 .
definition scorer (M ) (17), following result M0 .
622

(23)

fiThe Latent Relation Mapping Engine

scorer (M0 ) = simr (a1 : a2 , M0 (a1 ) : M0 (a2 )) = simr (a1 : a2 , b1 : b2 )

(24)

is, quality proportional analogy mason : stone :: carpenter : wood given
simr (mason : stone, carpenter : wood).
Proportional analogies may also evaluated using attributional similarity.
definition scorea (M ) (18), following result M0 .
scorea (M0 ) = sima (a1 , M0 (a1 )) + sima (a2 , M0 (a2 )) = sima (a1 , b1 ) + sima (a2 , b2 )

(25)

attributional similarity, quality proportional analogy mason : stone :: carpenter :
wood given sima (mason, carpenter) + sima (stone, wood).
LRA handles proportional analogies. main contribution LRME extend
LRA beyond proportional analogies bijective analogies > 2.
Turney (2006) describes ten potential applications LRA: recognizing proportional
analogies, structure mapping theory, modeling metaphor, classifying semantic relations,
word sense disambiguation, information extraction, question answering, automatic thesaurus generation, information retrieval, identifying semantic roles. Two
applications (evaluating proportional analogies classifying semantic relations) experimentally evaluated, state-of-the-art results.
Turney (2006) compares performance relational similarity (24) attributional
similarity (25) task solving 374 multiple-choice proportional analogy questions
SAT college entrance test. LRA used measure relational similarity variety
lexicon-based corpus-based algorithms used measure attributional similarity.
LRA achieves accuracy 56% 374 SAT questions, significantly
different average human score 57%. hand, best performance
attributional similarity 35%. results show attributional similarity better
random guessing, good relational similarity. result consistent
Gentners (1991) theory maturation human similarity judgments.
Turney (2006) also applies LRA task classifying semantic relations nounmodifier expressions. noun-modifier expression phrase, laser printer,
head noun (printer) preceded modifier (laser). task identify semantic
relation noun modifier. case, relation instrument;
laser instrument used printer. set 600 hand-labeled noun-modifier pairs
five different classes semantic relations, LRA attains 58% accuracy.
Turney (2008) employs variation LRA solving four different language tests,
achieving 52% accuracy SAT analogy questions, 76% accuracy TOEFL synonym
questions, 75% accuracy task distinguishing synonyms antonyms, 77%
accuracy task distinguishing words similar, words associated,
words similar associated. core algorithm used
four tests, tuning parameters particular test.

5. Applications LRME
Since LRME extension LRA, every potential application LRA also potential
application LRME. advantage LRME LRA ability handle bijective
623

fiTurney

analogies > 2 (where = |A| = |B|). section, consider kinds
applications might benefit ability.
Section 7.2, evaluate LRME science analogies common metaphors,
supports claim two applications benefit ability handle larger sets
terms. Section 1, saw identifying semantic roles (Gildea & Jurafsky, 2002)
also involves two terms, believe LRME superior LRA
semantic role labeling.
Semantic relation classification usually assumes relations binary; is,
semantic relation connection two terms (Rosario & Hearst, 2001; Nastase
& Szpakowicz, 2003; Turney, 2006; Girju et al., 2007). Yuret observed binary relations may linked underlying n-ary relations.2 example, Nastase Szpakowicz
(2003) defined taxonomy 30 binary semantic relations. Table 4 shows six binary relations Nastase Szpakowicz (2003) covered one 5-ary relation,
Agent:Tool:Action:Affected:Theme. Agent uses Tool perform Action. Somebody
something Affected Action. whole event summarized Theme.
Nastase Szpakowicz (2003)
Relation
Example
agent
student protest
purpose
concert hall
beneficiary
student discount
instrument
laser printer
object
metal separator
object property sunken ship

Agent:Tool:Action:Affected:Theme
Agent:Action
Theme:Tool
Affected:Action
Tool:Agent
Affected:Tool
Action:Affected

Table 4: six binary semantic relations Nastase Szpakowicz (2003)
viewed different fragments one 5-ary semantic relation.

SemEval Task 4, found easier manually tag datasets expanded
binary relations underlying n-ary relations (Girju et al., 2007). believe
expansion would also facilitate automatic classification semantic relations. results
Section 9.3 suggest applications LRA discussed Section 4
might benefit able handle bijective analogies > 2.

6. Mapping Problems
evaluate algorithms analogical mapping, created twenty mapping problems,
given Appendix A. twenty problems consist ten science analogy problems, based
examples analogy science Chapter 8 Holyoak Thagard (1995), ten
common metaphor problems, derived Lakoff Johnson (1980).
tables Appendix show intended mappings twenty problems. validate mappings, invited colleagues Institute Information
Technology participate experiment. experiment hosted web server
2. Deniz Yuret, personal communication, February 13, 2007. observation context
work building datasets SemEval 2007 Task 4 (Girju et al., 2007).

624

fiThe Latent Relation Mapping Engine

(only accessible inside institute) people participated anonymously, using web
browsers offices. 39 volunteers began experiment 22
went way end. analysis, use data 22 participants
completed mapping problems.
instructions participants Appendix A. sequence problems
order terms within problem randomized separately participant,
remove effects due order. Table 5 shows agreement intended
mapping mappings generated participants. Across twenty problems,
average agreement 87.6%, higher agreement figures many
linguistic annotation tasks. agreement impressive, given participants
minimal instructions training.
Type

science
analogies

common
metaphors

Mapping
A1
A2
A3
A4
A5
A6
A7
A8
A9
A10
M1
M2
M3
M4
M5
M6
M7
M8
M9
M10

Source Target
solar system atom
water flow heat transfer
waves sounds
combustion respiration
sound light
projectile planet
artificial selection natural selection
billiard balls gas molecules
computer mind
slot machine bacterial mutation
war argument
buying item accepting belief
grounds building reasons theory
impediments travel difficulties
money time
seeds ideas
machine mind
object idea
following understanding
seeing understanding

Average

Agreement
90.9
86.9
81.8
79.0
79.2
97.4
74.7
88.1
84.3
83.6
93.5
96.1
87.9
100.0
77.3
89.0
98.7
89.1
96.6
78.8
87.6


7
8
8
8
7
7
7
8
9
5
7
7
6
7
6
7
7
5
8
6
7.0

Table 5: average agreement intended mappings mappings
22 participants. See Appendix details.

column labeled gives number terms set source terms
mapping problem (which equal number terms set target terms).
average problem, = 7. third column Table 5 gives mnemonic summarizes
mapping (e.g., solar system atom). Note mnemonic used input
algorithms, mnemonic shown participants experiment.
agreement figures Table 5 individual mapping problem averages
mappings problem. Appendix gives detailed view, showing
agreement individual mapping mappings. twenty problems contain
total 140 individual mappings (20 7). Appendix shows every one 140
625

fiTurney

mappings agreement 50% higher. is, every case, majority
participants agreed intended mapping. (There two cases agreement
exactly 50%. See problems A5 Table 14 M5 Table 16 Appendix A.)
select mapping chosen majority 22 participants,
get perfect score twenty problems. precisely, try m! mappings
problem, select mapping maximizes sum number participants
agree individual mapping mappings, score
100% twenty problems. strong support intended mappings
given Appendix A.
Section 3, applied Genters (1991) categories mere appearance (mostly attributional similarity), analogy (mostly relational similarity), literal similarity (a mixture
attributional relational similarity) mappings Mr , Mr
best mapping according simr best mapping according sima . twenty
mapping problems chosen analogy problems; is, intended mappings
Appendix meant relational mappings, Mr ; mappings maximize relational
similarity, simr . tried avoid mere appearance literal similarity.
Section 7 use twenty mapping problems evaluate relational mapping
algorithm (LRME), Section 8 use evaluate several different attributional
mapping algorithms. hypothesis LRME perform significantly better
attributional mapping algorithms twenty mapping problems,
analogy problems (not mere appearance problems literal similarity problems).
expect relational attributional mapping algorithms would perform approximately
equally well literal similarity problems, expect mere appearance problems
would favour attributional algorithms relational algorithms, test
latter two hypotheses, primary interest paper analogy-making.
goal test hypothesis real, practical, effective, measurable
difference output LRME output various attributional mapping algorithms. skeptic might claim relational similarity simr (a : b, c : d)
reduced attributional similarity sima (a, c) + sima (b, d); therefore relational mapping
algorithm complicated solution illusory problem. slightly less skeptical claim
relational similarity versus attributional similarity valid distinction cognitive
psychology, relational mapping algorithm capture distinction. test
hypothesis refute skeptical claims, created twenty analogical mapping
problems, show LRME handles problems significantly better
various attributional mapping algorithms.

7. Latent Relation Mapping Engine
Latent Relation Mapping Engine (LRME) seeks mapping Mr maximizes
sum relational similarities.
Mr = arg max

X

X

simr (ai : aj , (ai ) : (aj ))

(26)

P (A,B) i=1 j=i+1

search Mr exhaustively evaluating possibilities. Ties broken randomly. use simplified form LRA (Turney, 2006) calculate simr .
626

fiThe Latent Relation Mapping Engine

7.1 Algorithm
Briefly, idea LRME build pair-pattern matrix X, rows correspond
pairs terms columns correspond patterns. example, row xi: might
correspond pair terms sun : solar system column x:j might correspond
pattern X centered . patterns, wild card, match
single word. value element xij X based frequency pattern
x:j , X instantiated terms pair xi: . example,
take pattern X centered instantiate X : pair sun : solar system,
pattern sun centered solar system , thus value element
xij based frequency sun centered solar system corpus. matrix
X smoothed truncated singular value decomposition (SVD) (Golub & Van Loan,
1996) relational similarity simr two pairs terms given cosine
angle two corresponding row vectors X.
detail, LRME takes input set mapping problems generates
output corresponding set mappings.

= {hA1 , B1 , hA2 , B2 , . . . , hAn , Bn i}

(27)

= {M1 : A1 B1 , M2 : A2 B2 , . . . , Mn : Bn }

(28)

following experiments, twenty mapping problems (Appendix A) processed
one batch (n = 20).
first step make list R contains pairs terms input I.
mapping problem hA, Bi I, add R pairs ai : aj , ai aj
members A, 6= j, pairs bi : bj , bi bj members B, 6= j.
|A| = |B| = m, m(m 1) pairs m(m 1) pairs B.3
typical pair R would sun : solar system. allow duplicates R; R list
pair types, pair tokens. twenty mapping problems, R list 1,694 pairs.
pair r R, make list S(r) phrases corpus contain
pair r. Let ai : aj terms pair r. search corpus phrases
following form:
[0 1 words] ai [0 3 words] aj [0 1 words]

(29)

ai : aj R, aj : ai also R, find phrases members pairs
orders, S(ai : aj ) S(aj : ai ). search template (29) used
Turney (2008).
following experiments, search corpus 5 1010 English words (about 280
GB plain text), consisting web pages gathered web crawler.4 retrieve phrases
3. m(m 1) here, m(m 1)/2, need pairs orders. want
calculate simr one order pairs, always less j (26); however, ensure
simr symmetrical, (16), need make matrix X symmetrical, rows
matrix orders every pair.
4. corpus collected Charles Clarke University Waterloo. provide copies
corpus request.

627

fiTurney

corpus, use Wumpus (Buttcher & Clarke, 2005), efficient search engine
passage retrieval large corpora.5
1,694 pairs R, find total 1,996,464 phrases corpus, average
1,180 phrases per pair. pair r = sun : solar system, typical phrase
S(r) would sun centered solar system illustrates.
Next make list C patterns, based phrases found. pair
r R, r = ai : aj , found phrase S(r), replace ai X
replace aj . remaining words may either left replaced
wild card symbol . replace ai aj X, replace
remaining words wild cards leave are. n remaining
words s, ai aj replaced, generate 2n+1 patterns s, add
patterns C. add new patterns C; is, C list pattern types,
pattern tokens; duplicates C.
example, pair sun : solar system, found phrase sun centered solar
system illustrates. replace ai : aj X : , X centered
illustrates. three remaining words, generate eight patterns,
X illustrates, X centered , X illustrates, on.
patterns added C. replace ai : aj : X, yielding centered X
illustrates. gives us another eight patterns, centered X . Thus
phrase sun centered solar system illustrates generates total sixteen patterns,
add C.
revise R, make list pairs correspond rows frequency
matrix F. remove pairs R phrases found corpus,
terms either order. Let ai : aj terms pair r. remove
r R S(ai : aj ) S(aj : ai ) empty. remove rows
would correspond zero vectors matrix F. reduces R 1,694 pairs 1,662
pairs. Let nr number pairs R.
Next revise C, make list patterns correspond columns
frequency matrix F. following experiments, stage, C contains millions
patterns, many efficient processing standard desktop computer. need
reduce C manageable size. select patterns shared
pairs. Let c pattern C. Let r pair R. phrase S(r),
pattern generated identical c, say r one
pairs generated c. sort patterns C descending order number
pairs R generated pattern, select top tnr patterns
sorted list. Following Turney (2008), set parameter 20; hence C reduced
top 33,240 patterns (tnr = 20 1,662 = 33,240). Let nc number patterns
C (nc = tnr ).
rows R columns C defined, build frequency matrix
F. Let ri i-th pair terms R (e.g., let ri sun : solar system) let cj
j-th pattern C (e.g., let cj X centered ). instantiate X
pattern cj terms ri ( sun centered solar system ). element fij F
frequency instantiated pattern corpus.
5. Wumpus developed Stefan Buttcher available http://www.wumpus-search.org/.

628

fiThe Latent Relation Mapping Engine

Note need search corpus instantiated pattern
fij , order find frequency. process creating pattern, keep track
many phrases generated pattern, pair. get frequency fij
checking record patterns generated ri .
next step transform matrix F raw frequencies form X
enhances similarity measurement. Turney (2006) used log entropy transformation,
suggested Landauer Dumais (1997). kind tf-idf (term frequency
times inverse document frequency) transformation, gives weight elements
matrix statistically surprising. However, Bullinaria Levy (2007) recently
achieved good results new transformation, called PPMIC (Positive Pointwise Mutual
Information Cosine); therefore LRME uses PPMIC. raw frequencies F used
calculate probabilities, calculate pointwise mutual information
(PMI) element matrix. element negative PMI set zero.

fij
pij = Pnr Pnc

j=1 fij

i=1

(30)

Pnc

j=1 fij
pi = Pnr Pnc

(31)

Pnr
f
Pncij
= Pnr i=1

(32)

i=1

pj

i=1



j=1 fij
j=1 fij

pij
pi pj



pmiij = log

pmiij pmiij > 0
xij =
0 otherwise

(33)
(34)

Let ri i-th pair terms R (e.g., let ri sun : solar system) let cj
j-th pattern C (e.g., let cj X centered ). (33), pij estimated probability
pattern cj instantiated pair ri ( sun centered solar system ), pi
estimated probability ri , pj estimated probability cj . ri cj
statistically independent, pi pj = pij (by definition independence), thus
pmiij zero (since log(1) = 0). interesting semantic relation
terms ri , pattern cj captures aspect semantic relation,
expect pij larger would ri cj indepedent; hence find
pij > pi pj , thus pmiij positive. (See Hypothesis 2 Section 2.)
hand, terms completely different domains may avoid other, case
find pmiij negative. PPMIC designed give high value xij
pattern cj captures aspect semantic relation terms ri ; otherwise,
xij value zero, indicating pattern cj tells us nothing
semantic relation terms ri .
experiments, F density 4.6% (the percentage nonzero elements)
X density 3.8%. lower density X due elements negative PMI,
transformed zero PPMIC.
629

fiTurney

smooth X applying truncated singular value decomposition (SVD) (Golub
& Van Loan, 1996). use SVDLIBC calculate SVD X.6 SVDLIBC designed
sparse (low density) matrices. SVD decomposes X product three matrices
UVT , U V column orthonormal form (i.e., columns orthogonal
unit length, UT U = VT V = I) diagonal matrix singular values
(Golub & Van Loan, 1996). X rank r, also rank r. Let k ,
k < r, diagonal matrix formed top k singular values, let Uk Vk
matrices produced selecting corresponding columns U V. matrix
Uk k VkT matrix rank k best approximates original matrix X, sense
minimizes approximation errors. is, X = Uk k VkT minimizes kX XkF
matrices X rank k, k . . . kF denotes Frobenius norm (Golub & Van
Loan, 1996). may think matrix Uk k VkT smoothed compressed version
original matrix X. Following Turney (2006), set parameter k 300.
relational similarity simr two pairs R inner product two
corresponding rows Uk k VkT , rows normalized unit length.
simplify calculations dropping Vk (Deerwester, Dumais, Landauer, Furnas, & Harshman,
1990). take matrix Uk k normalize row unit length. Let W
resulting matrix. let Z WWT , square matrix size nr nr . matrix contains
cosines combinations two pairs R.
mapping problem hA, Bi I, let : a0 pair terms let b : b0
pair terms B. Suppose ri = : a0 rj = b : b0 , ri rj
i-th j-th pairs R. simr (a : a0 , b : b0 ) = zij , zij element i-th
row j-th column Z. either : a0 b : b0 R, S(a : a0 ), S(a0 : a),
S(b : b0 ), S(b0 : b) empty, set similarity zero. Finally, mapping
problem I, output map Mr maximizes sum relational similarities.

Mr = arg max

X

X

simr (ai : aj , (ai ) : (aj ))

(35)

P (A,B) i=1 j=i+1

simplified form LRA used calculate simr differs LRA used Turney
(2006) several ways. LRME, use synonyms generate alternate forms
pairs terms. LRME, morphological processing terms. LRME uses
PPMIC (Bullinaria & Levy, 2007) process raw frequencies, instead log entropy.
Following Turney (2008), LRME uses slightly different search template (29) LRME
sets number columns nc tnr , instead using constant. Section 7.2,
evaluate impact two changes (PPMIC nc ), tested
changes, mainly motivated desire increased efficiency
simplicity.
7.2 Experiments
implemented LRME Perl, making external calls Wumpus searching corpus
SVDLIBC calculating SVD. used Perl Net::Telnet package interprocess
6. SVDLIBC work Doug Rohde available http://tedlab.mit.edu/dr/svdlibc/.

630

fiThe Latent Relation Mapping Engine

communication Wumpus, PDL (Perl Data Language) package matrix manipulations (e.g., calculating cosines), List::Permutor package generate permutations
(i.e., loop P (A, B)).
ran following experiments dual core AMD Opteron 64 computer, running
64 bit Linux. running time spent searching corpus phrases. took
16 hours 27 minutes Wumpus fetch 1,996,464 phrases. remaining steps
took 52 minutes, SVD took 10 minutes. running time could cut half
using RAID 0 speed disk access.
Table 6 shows performance LRME baseline configuration. comparison,
agreement 22 volunteers intended mapping copied Table 5.
difference performance LRME (91.5%) human participants
(87.6%) statistically significant (paired t-test, 95% confidence level).

Mapping
A1
A2
A3
A4
A5
A6
A7
A8
A9
A10
M1
M2
M3
M4
M5
M6
M7
M8
M9
M10
Average

Source Target
solar system atom
water flow heat transfer
waves sounds
combustion respiration
sound light
projectile planet
artificial selection natural selection
billiard balls gas molecules
computer mind
slot machine bacterial mutation
war argument
buying item accepting belief
grounds building reasons theory
impediments travel difficulties
money time
seeds ideas
machine mind
object idea
following understanding
seeing understanding

Accuracy
LRME Humans
100.0
90.9
100.0
86.9
100.0
81.8
100.0
79.0
71.4
79.2
100.0
97.4
71.4
74.7
100.0
88.1
55.6
84.3
100.0
83.6
71.4
93.5
100.0
96.1
100.0
87.9
100.0
100.0
100.0
77.3
100.0
89.0
100.0
98.7
60.0
89.1
100.0
96.6
100.0
78.8
91.5
87.6

Table 6: LRME baseline configuration, compared human performance.
Table 6, column labeled Humans average 22 people, whereas LRME
column one algorithm (it average). Comparing average several scores
individual score (whether individual human algorithm) may give
misleading impression. results individual person, typically several
100% scores scores 55-75% range. average mapping problem seven
terms. possible exactly one term mapped incorrectly;
incorrect mappings, must two incorrect mappings. follows
nature bijections. Therefore score 5/7 = 71.4% uncommon.
631

fiTurney

Table 7 looks results another perspective. column labeled LRME wrong
gives number incorrect mappings made LRME twenty problems.
five columns labeled Number people N wrong show, various values N ,
may 22 people made N incorrect mappings. average mapping problem,
15 22 participants perfect score (N = 0); remaining 7 participants, 5
made two mistakes (N = 2). Table 7 shows clearly Table 6 LRMEs
performance significantly different (individual) human performance. (For yet
another perspective, see Section 9.1).

Mapping
A1
A2
A3
A4
A5
A6
A7
A8
A9
A10
M1
M2
M3
M4
M5
M6
M7
M8
M9
M10
Average

LRME
wrong
0
0
0
0
2
0
2
0
4
0
2
0
0
0
0
0
0
2
0
0
1

Number people N wrong
N =0 N =1 N =2 N =3 N 4
16
0
4
2
0
14
0
5
0
3
9
0
9
2
2
9
0
9
0
4
10
0
7
2
3
20
0
2
0
0
8
0
6
6
2
13
0
8
0
1
11
0
7
2
2
13
0
9
0
0
17
0
5
0
0
19
0
3
0
0
14
0
8
0
0
22
0
0
0
0
9
0
11
0
2
15
0
4
3
0
21
0
1
0
0
18
0
2
1
1
19
0
3
0
0
13
0
3
3
3
15
0
5
1
1


7
8
8
8
7
7
7
8
9
5
7
7
6
7
6
7
7
5
8
6
7

Table 7: Another way viewing LRME versus human performance.
Table 8, examine sensitivity LRME parameter settings. first row
shows accuracy baseline configuration, Table 6. next eight rows show
impact varying k, dimensionality truncated singular value decomposition,
50 400. eight rows show effect varying t, column factor,
5 40. number columns matrix (nc ) given number rows (nr
= 1,662) multiplied t. second last row shows effect eliminating singular
value decomposition LRME. equivalent setting k 1,662, number
rows matrix. final row gives result PPMIC (Bullinaria & Levy,
2007) replaced log entropy (Turney, 2006). LRME sensitive
manipulations: None variations Table 8 perform significantly differently
baseline configuration (paired t-test, 95% confidence level). (This necessarily mean
manipulations effect; rather, suggests larger sample problems
would needed show significant effect.)
632

fiThe Latent Relation Mapping Engine

Experiment
baseline configuration

varying k

varying

dropping SVD
log entropy

k
300
50
100
150
200
250
300
350
400
300
300
300
300
300
300
300
300
1662
300


20
20
20
20
20
20
20
20
20
5
10
15
20
25
30
35
40
20
20

nc
33,240
33,240
33,240
33,240
33,240
33,240
33,240
33,240
33,240
8,310
16,620
24,930
33,240
41,550
49,860
58,170
66,480
33,240
33,240

Accuracy
91.5
89.3
92.8
91.3
92.6
90.6
91.5
90.6
90.6
86.9
94.0
94.0
91.5
90.1
90.6
89.5
91.7
89.7
83.9

Table 8: Exploring sensitivity LRME various parameter settings modifications.

8. Attribute Mapping Approaches
section, explore variety attribute mapping approaches twenty mapping
problems. approaches seek mapping maximizes sum
attributional similarities.
= arg max


X

sima (ai , (ai ))

(36)

P (A,B) i=1

search exhaustively evaluating possibilities. Ties broken randomly. use variety different algorithms calculate sima .
8.1 Algorithms
following experiments, test five lexicon-based attributional similarity measures
use WordNet:7 HSO (Hirst & St-Onge, 1998), JC (Jiang & Conrath, 1997), LC (Leacock & Chodrow, 1998), LIN (Lin, 1998), RES (Resnik, 1995). five implemented
Perl package WordNet::Similarity,8 builds WordNet::QueryData9 package. core idea behind treat WordNet graph measure semantic
distance two terms length shortest path graph.
Similarity increases distance decreases.
7. WordNet developed team Princeton available http://wordnet.princeton.edu/.
8. Ted Pedersens WordNet::Similarity package http://www.d.umn.edu/tpederse/similarity.html.
9. Jason Rennies WordNet::QueryData package http://people.csail.mit.edu/jrennie/WordNet/.

633

fiTurney

HSO works nouns, verbs, adjectives, adverbs, JC, LC, LIN, RES
work nouns verbs. used WordNet::Similarity try possible parts speech
possible senses input word. Many adjectives, true valuable,
also noun verb senses WordNet, JC, LC, LIN, RES still able
calculate similarity them. raw form word found WordNet,
WordNet::Similarity searches morphological variations word.
multiple similarity scores, multiple parts speech multiple senses, select
highest similarity score. similarity score, word WordNet,
JC, LC, LIN, RES could find alternative noun verb form
adjective adverb, set score zero.
also evaluate two corpus-based attributional similarity measures: PMI-IR (Turney,
2001) LSA (Landauer & Dumais, 1997). core idea behind word
characterized company keeps (Firth, 1957). similarity two terms
measured similarity statistical distributions corpus. used corpus
Section 7 along Wumpus implement PMI-IR (Pointwise Mutual Information
Information Retrieval). LSA (Latent Semantic Analysis), used online
demonstration.10 selected Matrix Comparison option General Reading
1st year college (300 factors) topic space term-to-term comparison type. PMI-IR
LSA work parts speech.
eighth similarity measure based observation intended mappings
map terms part speech (see Appendix A). Let POS(a) partof-speech tag assigned term a. use part-of-speech tags define measure
attributional similarity, simPOS (a, b), follows.

100 = b
10 POS(a) = POS(b)
(37)
simPOS (a, b) =

0 otherwise
hand-labeled terms mapping problems part-of-speech tags (Santorini,
1990). Automatic taggers assume words tagged embedded
sentence, terms mapping problems sentences, tags
ambiguous. used knowledge intended mappings manually disambiguate
part-of-speech tags terms, thus guaranteeing corresponding terms
intended mapping always tags.
first seven attributional similarity measures above, created seven
similarity measures combining simPOS (a, b). example, let simHSO (a, b)
Hirst St-Onge (1998) similarity measure. combine simPOS (a, b) simHSO (a, b)
simply adding them.
simHSO+POS (a, b) = simHSO (a, b) + simPOS (a, b)

(38)

values returned simPOS (a, b) range 0 100, whereas values returned
simHSO (a, b) much smaller. chose large values (37) getting POS tags
match weight similarity measures. manual POS tags
10. online demonstration LSA work team University Colorado Boulder.
available http://lsa.colorado.edu/.

634

fiThe Latent Relation Mapping Engine

high weight simPOS (a, b) give unfair advantage attributional mapping
approach, relational mapping approach afford generous.
8.2 Experiments
Table 9 presents accuracy various measures attributional similarity.
best result without POS labels 55.9% (HSO). best result POS labels 76.8%
(LIN+POS). 91.5% accuracy LRME (see Table 6) significantly higher
76.8% accuracy LIN+POS (and thus, course, significantly higher everything else
Table 9; paired t-test, 95% confidence level). average human performance 87.6%
(see Table 5) also significantly higher 76.8% accuracy LIN+POS (paired t-test,
95% confidence level). summary, humans LRME perform significantly better
variations attributional mapping approaches tested.
Algorithm
HSO
JC
LC
LIN
RES
PMI-IR
LSA
POS (hand-labeled)
HSO+POS
JC+POS
LC+POS
LIN+POS
RES+POS
PMI-IR+POS
LSA+POS

Reference
Hirst St-Onge (1998)
Jiang Conrath (1997)
Leacock Chodrow (1998)
Lin (1998)
Resnik (1995)
Turney (2001)
Landauer Dumais (1997)
Santorini (1990)
Hirst St-Onge (1998)
Jiang Conrath (1997)
Leacock Chodrow (1998)
Lin (1998)
Resnik (1995)
Turney (2001)
Landauer Dumais (1997)

Accuracy
55.9
54.7
48.5
48.2
43.8
54.4
39.6
44.8
71.1
73.6
69.5
76.8
71.6
72.8
65.8

Table 9: accuracy attribute mapping approaches wide variety measures
attributional similarity.

9. Discussion
section, examine three questions suggested preceding results.
difference science analogy problems common metaphor
problems? advantage combining relational attributional mapping approaches? advantage relational mapping approach attributional
mapping approach?
9.1 Science Analogies versus Common Metaphors
Table 5 suggests science analogies may difficult common metaphors.
supported Table 10, shows agreement 22 participants
intended mapping (see Section 6) varies science problems metaphor
635

fiTurney

problems. science problems lower average performance greater variation
performance. difference science problems metaphor problems
statistically significant (paired t-test, 95% confidence level).
Participant
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
Average
Standard deviation

20
72.6
88.2
90.0
71.8
95.7
83.4
79.6
91.9
89.7
80.7
94.5
90.6
93.2
97.1
86.6
80.5
93.3
86.5
92.9
90.4
82.7
96.2
87.6
7.2

Average Accuracy
10 Science 10 Metaphor
59.9
85.4
85.9
90.5
86.3
93.8
56.4
87.1
94.2
97.1
83.9
82.9
73.6
85.7
95.0
88.8
90.0
89.3
81.4
80.0
95.7
93.3
87.4
93.8
89.6
96.7
94.3
100.0
88.5
84.8
80.2
80.7
89.9
96.7
78.9
94.2
96.0
89.8
84.1
96.7
74.9
90.5
94.9
97.5
84.6
90.7
10.8
5.8

Table 10: comparison difficulty science problems versus metaphor problems 22 participants. numbers bold font scores
scores LRME.
average science problem terms (7.4) average metaphor problem
(6.6), might contribute difficulty science problems. However, Table 11
shows clear relation number terms problem (m
Table 5) level agreement. believe people find metaphor problems
easier science problems common metaphors entrenched
language, whereas science analogies peripheral.
Table 12 shows 16 algorithms studied perform slightly worse science
problems metaphor problems, difference statistically significant
(paired t-test, 95% confidence level). hypothesize attributional mapping approaches performing well enough sensitive subtle differences science
analogies common metaphors.
Incidentally, tables give us another view performance LRME comparison human performance. first row Table 12 shows performance LRME
636

fiThe Latent Relation Mapping Engine

Num terms
5
6
7
8
9

Agreement
86.4
81.3
91.1
86.5
84.3

Table 11: average agreement among 22 participants function number
terms problems.

Algorithm
LRME
HSO
JC
LC
LIN
RES
PMI-IR
LSA
POS
HSO+POS
JC+POS
LC+POS
LIN+POS
RES+POS
PMI-IR+POS
LSA+POS
Average
Standard deviation

20
91.5
55.9
54.7
48.5
48.2
43.8
54.4
39.6
44.8
71.1
73.6
69.5
76.8
71.6
72.8
65.8
61.4
14.7

Average Accuracy
10 Science 10 Metaphor
89.8
93.1
57.4
54.3
57.4
52.1
49.6
47.5
46.7
49.7
39.0
48.6
49.5
59.2
37.3
41.9
42.1
47.4
66.9
75.2
78.1
69.2
70.8
68.2
68.8
84.8
70.3
72.9
65.7
79.9
69.1
62.4
59.9
62.9
15.0
15.3

Table 12: comparison difficulty science problems versus metaphor problems 16 algorithms.

science metaphor problems. Table 10, marked bold font cases
human scores greater LRMEs scores. 20 problems, 8
cases; 10 science problems, 8 cases; 10 metaphor problems, 10 cases. evidence LRMEs performance
significantly different human performance. LRME near middle range
performance 22 human participants.
9.2 Hybrid Relational-Attributional Approaches
Recall definitions scorer (M ) scorea (M ) given Section 3.
637

fiTurney

scorer (M ) =
scorea (M ) =

X

X

simr (ai : aj , (ai ) : (aj ))

i=1 j=i+1

X

sima (ai , (ai ))

(39)

(40)

i=1

combine scores simply adding multiplying them, scorer (M )
scorea (M ) may quite different scales distributions values; therefore
first normalize probabilities.
scorer (M )
Mi P (A,B) scorer (Mi )

(41)

scorea (M )
Mi P (A,B) scorea (Mi )

(42)

probr (M ) = P
proba (M ) = P

probability estimates, assume scorer (M ) 0 scorea (M ) 0.
necessary, constant value may added scores, ensure negative.
combine scores adding multiplying probabilities.


Mr+a = arg max probr (M ) + proba (M )

(43)

P (A,B)



Mra = arg max probr (M ) proba (M )

(44)

P (A,B)

Table 13 shows accuracy LRME combined LIN+POS (the best attributional mapping algorithm Table 9, accuracy 76.8%) HSO (the best
attributional mapping algorithm use manual POS tags, accuracy
55.9%). try adding multiplying probabilities. own, LRME
accuracy 91.5%. Combining LRME LIN+POS increases accuracy 94.0%,
improvement statistically significant (paired t-test, 95% confidence level). Combining LRME HSO results decrease accuracy. decrease significant
probabilities multiplied (85.4%), significant probabilities
added (78.5%).
summary, experiments show significant advantage combining LRME
attributional mapping. However, possible larger sample problems would
show significant advantage. Also, combination methods explored (addition
multiplication probabilities) elementary. sophisticated approach,
weighted combination, may perform better.
9.3 Coherent Relations
hypothesize LRME benefits kind coherence among relations.
hand, attributional mapping approaches involve kind coherence.
638

fiThe Latent Relation Mapping Engine

Components
Relational Attributional
LRME
LIN+POS
LRME
LIN+POS
LRME
HSO
LRME
HSO

Combination
add probabilities
multiply probabilities
add probabilities
multiply probabilities

Accuracy
94.0
94.0
78.5
85.4

Table 13: performance four different hybrids relational attributional mapping
approaches.

Suppose swap two terms mapping. Let original mapping
let 0 new mapping, 0 (a1 ) = (a2 ), 0 (a2 ) = (a1 ), 0 (ai ) = (ai )
> 2. attributional similarity, impact swap score mapping
limited. Part score affected.

scorea (M ) = sima (a1 , (a1 )) + sima (a2 , (a2 )) +


X

sima (ai , (ai ))

(45)

sima (ai , (ai ))

(46)

i=3

scorea (M 0 ) = sima (a1 , (a2 )) + sima (a2 , (a1 )) +


X
i=3

hand, relational similarity, impact swap limited
way. change part mapping affects whole score. kind global
coherence relational similarity lacking attributional similarity.
Testing hypothesis LRME benefits coherence somewhat complicated,
need design experiment coherence effect isolated
effects. this, move terms outside accuracy calculation.
Let : B one twenty mapping problems, intended
mapping = |A| = |B|. Let A0 randomly selected subset size m0 . Let B 0
(A0 ), subset B maps A0 .

A0

(47)

0

B B
0

(48)
0

B = (A )

m0 = A0 = B 0
0

<m

(49)
(50)
(51)

two ways might use LRME generate mapping 0 : A0 B 0
new reduced mapping problem, internal coherence total coherence.
1. Internal coherence: select 0 based hA0 , B 0 alone.
639

fiTurney

A0 = {a1 , ..., am0 }

(52)

0

B = {b1 , ..., bm0 }

(53)
m0

0 = arg max

m0

X X

P (A0 ,B 0 ) i=1 j=i+1

simr (ai : aj , (ai ) : (aj ))

(54)

case, 0 chosen based relations internal hA0 , B 0 i.
2. Total coherence: select 0 based hA, Bi knowledge 0
must satisfy constraint 0 (A0 ) = B 0 . (This knowledge also embedded
internal coherence.)

= {a1 , ..., }

(55)

B = {b1 , ..., bm }


P (A, B) = | P (A, B) (A0 ) = B 0
X

X
0
= arg max
simr (ai : aj , (ai ) : (aj ))
0

(56)
(57)
(58)

P 0 (A,B) i=1 j=i+1

case, 0 chosen using relations internal hA0 , B 0
relations hA, Bi external hA0 , B 0 i.

Suppose calculate accuracy two methods based subproblem hA0 , B 0 i. first might seem advantage total coherence,
must explore larger space possible mappings internal coherence (since
|P 0 (A, B)| larger |P (A0 , B 0 )|), additional terms explores
involved calculating accuracy. However, hypothesize total coherence
higher accuracy internal coherence, additional external relations
help select correct mapping.
test hypothesis, set m0 3 randomly generated ten new reduced
mapping problems twenty problems (i.e., total 200 new problems size
3). average accuracy internal coherence 93.3%, whereas average accuracy
total coherence 97.3%. difference statistically significant (paired t-test, 95%
confidence level).
hand, attributional mapping approaches cannot benefit total
coherence, connection attributes hA0 , B 0
attributes outside. decompose scorea (M ) two independent parts.
640

fiThe Latent Relation Mapping Engine

A00 = \ A0
0

(59)
00

A=A


P (A, B) = | P (A, B) (A0 ) = B 0
X
0 = arg max
sima (ai , (ai ))

(60)

0

(61)
(62)

P 0 (A,B)



= arg max
P 0 (A,B)


X

ai

sima (ai , (ai )) +

A0

X
ai

sima (ai , (ai ))

(63)

A00

two parts optimized independently. Thus terms external
hA0 , B 0 influence part 0 covers hA0 , B 0 i.
Relational mapping cannot decomposed independent parts way,
relations connect parts. gives relational mapping approaches inherent
advantage attributional mapping approaches.
confirm analysis, compared internal total coherence using LIN+POS
200 new problems size 3. average accuracy internal coherence
88.0%, whereas average accuracy total coherence 87.0%. difference
statistically significant (paired t-test, 95% confidence level). (The reason
difference that, two mappings score, break ties randomly.
causes random variation accuracy.)
benefit coherence suggests make analogy mapping problems easier
LRME adding terms. difficulty new terms cannot randomly
chosen; must fit logic analogy overlap existing terms.
course, important difference relational attributional mapping approaches. believe important difference relations
reliable general attributes, using past experiences make
predictions future (Hofstadter, 2001; Gentner, 2003). Unfortunately, hypothesis difficult evaluate experimentally hypothesis coherence.

10. Related Work
French (2002) gives good survey computational approaches analogy-making,
perspective cognitive science (where emphasis well computational systems
model human performance, rather well systems perform). sample
systems survey add mentioned.
French (2002) categorizes analogy-making systems symbolic, connectionist, symbolicconnectionist hybrids. Gardenfors (2004) proposes another category representational
systems AI cognitive science, calls conceptual spaces. spatial geometric systems common information retrieval machine learning (Widdows, 2004;
van Rijsbergen, 2004). influential example Latent Semantic Analysis (Landauer &
Dumais, 1997). first spatial approaches analogy-making began appear around
time Frenchs (2002) survey. LRME takes spatial approach analogy-making.
641

fiTurney

10.1 Symbolic Approaches
Computational approaches analogy-making date back Analogy (Evans, 1964)
Argus (Reitman, 1965). systems designed solve proportional analogies
(analogies |A| = |B| = 2; see Section 4). Analogy could solve proportional
analogies simple geometric figures Argus could solve simple word analogies.
systems used hand-coded rules able solve limited range problems
designers anticipated coded rules.
French (2002) cites Structure Mapping Theory (SMT) (Gentner, 1983) Structure
Mapping Engine (SME) (Falkenhainer et al., 1989) prime examples symbolic
approaches:
SMT unquestionably influential work date modeling
analogy-making applied wide range contexts ranging
child development folk physics. SMT explicitly shifts emphasis analogymaking structural similarity source target domains. Two
major principles underlie SMT:
relation-matching principle: good analogies determined mappings relations attributes (originally identical predicates
mapped)
systematicity principle: mappings coherent systems relations
preferred mappings individual relations.
structural approach intended produce domain-independent mapping process.
LRME follows principles. LRME uses relational similarity; attributional similarity involved (see Section 7.1). Coherent systems relations preferred
mappings individual relations (see Section 9.3). However, spatial (statistical,
corpus-based) approach LRME quite different symbolic (logical, hand-coded)
approach SME.
Martin (1992) uses symbolic approach handle conventional metaphors. Gentner,
Bowdle, Wolff, Boronat (2001) argue novel metaphors processed analogies,
conventional metaphors recalled memory without special processing. However,
line conventional novel metaphor unclear.
Dolan (1995) describes algorithm extract conventional metaphors
dictionary. semantic parser used extract semantic relations Longman
Dictionary Contemporary English (LDOCE). symbolic algorithm finds metaphorical
relations words, using extracted relations.
Veale (2003, 2004) developed symbolic approach analogy-making, using WordNet lexical resource. Using spreading activation algorithm, achieved score
43.0% set 374 multiple-choice lexical proportional analogy questions SAT
college entrance test (Veale, 2004).
Lepage (1998) demonstrated symbolic approach proportional analogies
used morphology processing. Lepage Denoual (2005) apply similar approach
machine translation.
642

fiThe Latent Relation Mapping Engine

10.2 Connectionist Approaches
Connectionist approaches analogy-making include ACME (Holyoak & Thagard, 1989)
LISA (Hummel & Holyoak, 1997). Like symbolic approaches, systems use handcoded knowledge representations, search mappings takes connectionist approach, nodes weights incrementally updated time,
system reaches stable state.
10.3 Symbolic-Connectionist Hybrid Approaches
third family examined French (2002) hybrid approaches, containing elements
symbolic connectionist approaches. Examples include Copycat (Mitchell,
1993) Tabletop (French, 1995). Much work Fluid Analogies Research
Group (FARG) concerns symbolic-connectionist hybrids (Hofstadter & FARG, 1995).
10.4 Spatial Approaches
Marx, Dagan, Buhmann, Shamir (2002) present coupled clustering algorithm,
uses feature vector representation find analogies collections text. example,
given documents Buddhism Christianity, finds related terms, {school,
Mahayana, Zen} Buddhism {tradition, Catholic, Protestant} Christianity.
Mason (2004) describes CorMet system extracting conventional metaphors
text. CorMet based clustering feature vectors represent selectional preferences
verbs. Given keywords source domain laboratory target domain finance,
able discover mappings liquid income container institution.
Turney, Littman, Bigham, Shnayder (2003) present system solving lexical
proportional analogy questions SAT college entrance test, combines thirteen
different modules. Twelve modules use either attributional similarity symbolic
approach relational similarity, one module uses spatial (feature vector) approach
measuring relational similarity. module worked much better
modules; therefore, studied detail Turney Littman (2005).
relation pair words represented vector, elements pattern
frequencies. similar LRME, one important difference Turney
Littman (2005) used fixed, hand-coded set 128 patterns, whereas LRME automatically
generates variable number patterns given corpus (33,240 patterns
experiments here).
Turney (2005) introduced Latent Relational Analysis (LRA), examined
thoroughly Turney (2006). LRA achieves human-level performance set 374
multiple-choice proportional analogy questions SAT college entrance exam. LRME
uses simplified form LRA. similar simplification LRA used Turney (2008),
system processing analogies, synonyms, antonyms, associations. contribution
LRME go beyond proportional analogies, larger systems analogical mappings.
10.5 General Theories Analogy Metaphor
Many theories analogy-making metaphor either involve computation
suggest general principles concepts specific particular computational
643

fiTurney

approach. design LRME influenced several theories type (Gentner,
1983; Hofstadter & FARG, 1995; Holyoak & Thagard, 1995; Hofstadter, 2001; Gentner,
2003).
Lakoff Johnson (1980) provide extensive evidence metaphor ubiquitous
language thought. believe system analogy-making able
handle metaphorical language, ten analogy problems derived
Lakoff Johnson (1980). agree claim metaphor merely
involve superficial relation couple words; rather, involves systematic set
mappings two domains. Thus analogy problems involve larger sets words,
beyond proportional analogies.
Holyoak Thagard (1995) argue analogy-making central daily thought,
especially finding creative solutions new problems. ten scientific analogies
derived examples analogy-making scientific creativity.

11. Limitations Future Work
Section 4, mentioned ten applications LRA, Section 5 claimed
results experiments Section 9.3 suggest LRME may perform better LRA
ten applications, due ability handle bijective analogies > 2.
focus future work testing hypothesis. particular, task semantic
role labeling, discussed Section 1, seems good candidate application LRME.
input LRME simpler input SME (compare Figures 1 2
Section 1 Table 1), still human effort involved creating input.
LRME immune criticism Chalmers, French, Hofstadter (1992),
human generates input work computer makes
mappings, although trivial matter find right mapping 5,040 (7!)
choices.
future work, would like relax requirement hA, Bi must bijection
(see Section 3), adding irrelevant words (distractors) synonyms. mapping
algorithm forced decide terms include mapping terms
leave out.
would also like develop algorithm take proportional analogy (m = 2)
input (e.g., sun:planet::nucleus:electron) automatically expand larger analogy
(m > 2, e.g., Table 2). is, would automatically search corpus new terms
add analogy.
next step would give computer topic source domain (e.g.,
solar system) topic target domain (e.g., atomic structure), let work
rest own. might possible combining ideas LRME ideas
coupled clustering (Marx et al., 2002) CorMet (Mason, 2004).
seems analogy-making triggered people encounter problem
(Holyoak & Thagard, 1995). problem defines target us, immediately
start searching source. Analogical mapping enables us transfer knowledge
source target, hopefully leading solution problem. suggests
input ideal analogical mapping algorithm would simply statement
644

fiThe Latent Relation Mapping Engine

problem (e.g., structure atom?). Ultimately, computer might
find problems well. input would large corpus.
algorithms considered perform exhaustive search set
possible mappings P (A, B). acceptable sets small, here,
problematic larger problems. future work, necessary use
heuristic search algorithms instead exhaustive search.
takes almost 18 hours LRME process twenty mapping problems (Section 7).
better hardware changes software, time could significantly
reduced. even greater speed, algorithm could run continuously, building large
database vector representations term pairs, ready create mappings
soon user requests them. similar vision Banko Etzioni (2007).
LRME, like LRA LSA (Landauer & Dumais, 1997), uses truncated singular value
decomposition (SVD) smooth matrix. Many algorithms proposed
smoothing matrices. past work LRA (Turney, 2006), experimented
Nonnegative Matrix Factorization (NMF) (Lee & Seung, 1999), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999), Iterative Scaling (IS) (Ando, 2000), Kernel
Principal Components Analysis (KPCA) (Scholkopf, Smola, & Muller, 1997).
interesting results small matrices (around 1000 2000), none algorithms
seemed substantially better truncated SVD, none scaled matrix
sizes (1,662 33,240). However, believe SVD unique,
future work likely discover smoothing algorithm efficient effective
SVD. results Section 7.2 show significant benefit SVD. Table 8
hints PPMIC (Bullinaria & Levy, 2007) important SVD.
LRME extracts knowledge many fragments text. Section 7.1, noted
found average 1,180 phrases per pair. information 1,180
phrases combined vector, represent semantic relation pair.
quite different relation extraction (for example) Automatic Content Extraction
(ACE) Evaluation.11 task ACE identify label semantic relation single
sentence. Semantic role labeling also involves labeling single sentence (Gildea & Jurafsky,
2002).
contrast LRME ACE analogous distinction cognitive
psychology semantic episodic memory. Episodic memory memory
specific event ones personal past, whereas semantic memory memory basic facts
concepts, unrelated specific event past. LRME extracts relational information
independent specific sentence, like semantic memory. ACE concerned
extracting relation specific sentence, like episodic memory. cognition, episodic
memory semantic memory work together synergistically. experience event,
use semantic memory interpret event form new episodic memory,
semantic memory constructed past experiences, accumulated
episodic memories. suggests synergy combining LRME-like
semantic information extraction algorithms ACE-like episodic information extraction
algorithms.
11. ACE annual event began 1999. Relation Detection Characterization (RDC)
introduced ACE 2001. information, see http://www.nist.gov/speech/tests/ace/.

645

fiTurney

12. Conclusion
Analogy core cognition. understand present analogy past.
predict future analogy past present. solve problems searching
analogous situations (Holyoak & Thagard, 1995). daily language saturated
metaphor (Lakoff & Johnson, 1980), metaphor based analogy (Gentner et al.,
2001). understand human language, solve human problems, work humans,
computers must able make analogical mappings.
best theory analogy-making Structure Mapping Theory (Gentner, 1983),
Structure Mapping Engine (Falkenhainer et al., 1989) puts much burden
analogy-making human users (Chalmers et al., 1992). LRME attempt
shift burden onto computer, remaining consistent general
principles SMT.
shown LRME able solve bijective analogical mapping problems
human-level performance. Attributional mapping algorithms (at least, tried
far) able reach level. supports SMT, claims relations
important attributes making analogical mappings.
still much research done. LRME takes load human
user, formulating input LRME easy. paper incremental step
towards future computers make surprising useful analogies minimal
human assistance.

Acknowledgments
Thanks colleagues Institute Information Technology participating
experiment Section 6. Thanks Charles Clarke Egidio Terra corpus.
Thanks Stefan Buttcher making Wumpus available giving advice use.
Thanks Doug Rohde making SVDLIBC available. Thanks WordNet team
Princeton University WordNet, Ted Pedersen WordNet::Similarity Perl package,
Jason Rennie WordNet::QueryData Perl package. Thanks LSA team
University Colorado Boulder use online demonstration LSA.
Thanks Deniz Yuret, Andre Vellino, Dedre Gentner, Vivi Nastase, Yves Lepage, Diarmuid
Seaghdha, Roxana Girju, Chris Drummond, Howard Johnson, Stan Szpakowicz,
anonymous reviewers JAIR helpful comments suggestions.

Appendix A. Details Mapping Problems
appendix, provide detailed information twenty mapping problems.
Figure 3 shows instructions given participants experiment
Section 6. instructions displayed web browsers. Tables 14, 15, 16,
17 show twenty mapping problems. first column gives problem number
(e.g., A1) mnemonic summarizes mapping (e.g., solar system atom).
second column gives source terms third column gives target terms.
mappings shown tables intended mappings. fourth column
shows percentage participants agreed intended mappings. example,
646

fiThe Latent Relation Mapping Engine

Systematic Analogies Metaphors
Instructions
presented twenty analogical mapping problems, ten based scientific
analogies ten based common metaphors. typical problem look like this:
horse
legs
hay
brain
dung













?
?
?
?
?

may click drop-down menus above, see options available.
task construct analogical mapping; is, one-to-one mapping
items left items right. example:
horse
legs
hay

car
wheels
gasoline





brain
dung

driver
exhaust




mapping expresses analogy horse car. horses legs like
cars wheels. horse eats hay car consumes gasoline. horses brain controls
movement horse like cars driver controls movement car. horse
generates dung waste product like car generates exhaust waste product.
duplicate items answers right-hand side.
duplicates missing items (question marks), get error message
submit answer.
welcome use dictionary work problems, would find
helpful.
find instructions unclear, please continue exercise.
answers twenty problems used standard evaluating output
computer algorithm; therefore, proceed confident
understand task.
Figure 3: instructions participants experiment Section 6.

647

fiTurney

Mapping
A1
solar system
atom

A2
water flow
heat transfer

A3
waves
sounds

A4
combustion
respiration

A5
sound
light

Source
solar system
sun
planet
mass
attracts
revolves
gravity
Average agreement:
water
flows
pressure
water tower
bucket
filling
emptying
hydrodynamics
Average agreement:
waves
shore
reflects
water
breakwater
rough
calm
crashing
Average agreement:
combustion
fire
fuel
burning
hot
intense
oxygen
carbon dioxide
Average agreement:
sound
low
high
echoes
loud
quiet
horn
Average agreement:










Target
atom
nucleus
electron
charge
attracts
revolves
electromagnetism










heat
transfers
temperature
burner
kettle
heating
cooling
thermodynamics










sounds
wall
echoes
air
insulation
loud
quiet
vibrating










respiration
animal
food
breathing
living
vigorous
oxygen
carbon dioxide









light
red
violet
reflects
bright
dim
lens

Agreement
86.4
100.0
95.5
86.4
90.9
95.5
81.8
90.9
86.4
95.5
86.4
72.7
72.7
95.5
95.5
90.9
86.9
86.4
77.3
95.5
95.5
81.8
63.6
100.0
54.5
81.8
72.7
95.5
90.9
72.7
59.1
77.3
77.3
86.4
79.0
86.4
50.0
54.5
100.0
90.9
77.3
95.5
79.2

POS
NN
NN
NN
NN
VBZ
VBZ
NN
NN
VBZ
NN
NN
NN
VBG
VBG
NN
NNS
NN
VBZ
NN
NN
JJ
JJ
VBG
NN
NN
NN
VBG
JJ
JJ
NN
NN
NN
JJ
JJ
VBZ
JJ
JJ
NN

Table 14: Science analogy problems A1 A5, derived Chapter 8 Holyoak
Thagard (1995).

648

fiThe Latent Relation Mapping Engine

Mapping
A6
projectile
planet

A7
artificial selection
natural selection

A8
billiard balls
gas molecules

A9
computer
mind

A10
slot machine
bacterial mutation

Source
projectile
trajectory
earth
parabolic
air
gravity
attracts
Average agreement:
breeds
selection
conformance
artificial
popularity
breeding
domesticated
Average agreement:
balls
billiards
speed
table
bouncing
moving
slow
fast
Average agreement:
computer
processing
erasing
write
read
memory
outputs
inputs
bug
Average agreement:
slot machines
reels
spinning
winning
losing
Average agreement:










Target
planet
orbit
sun
elliptical
space
gravity
attracts









species
competition
adaptation
natural
fitness
mating
wild










molecules
gas
temperature
container
pressing
moving
cold
hot











mind
thinking
forgetting
memorize
remember
memory
muscles
senses
mistake







bacteria
genes
mutating
reproducing
dying

Agreement
100.0
100.0
100.0
100.0
100.0
90.9
90.9
97.4
100.0
59.1
59.1
77.3
54.5
95.5
77.3
74.7
90.9
72.7
81.8
95.5
77.3
86.4
100.0
100.0
88.1
90.9
95.5
100.0
72.7
54.5
81.8
72.7
90.9
100.0
84.3
68.2
72.7
86.4
90.9
100.0
83.6

POS
NN
NN
NN
JJ
NN
NN
VBZ
NNS
NN
NN
JJ
NN
VBG
JJ
NNS
NN
NN
NN
VBG
VBG
JJ
JJ
NN
VBG
VBG
VB
VB
NN
NNS
NNS
NN
NNS
NNS
VBG
VBG
VBG

Table 15: Science analogy problems A6 A10, derived Chapter 8 Holyoak
Thagard (1995).

649

fiTurney

Mapping
M1
war
argument

M2
buying item
accepting belief

M3
grounds building
reasons theory

M4
impediments travel
difficulties

M5
money
time

Source
war
soldier
destroy
fighting
defeat
attacks
weapon
Average agreement:
buyer
merchandise
buying
selling
returning
valuable
worthless
Average agreement:
foundations
buildings
supporting
solid
weak
crack
Average agreement:
obstructions
destination
route
traveller
travelling
companion
arriving
Average agreement:
money
allocate
budget
effective
cheap
expensive
Average agreement:










Target
argument
debater
refute
arguing
acceptance
criticizes
logic









believer
belief
accepting
advocating
rejecting
true
false








reasons
theories
confirming
rational
dubious
flaw









difficulties
goal
plan
person
problem solving
partner
succeeding








time
invest
schedule
efficient
quick
slow

Agreement
90.9
100.0
90.9
95.5
90.9
95.5
90.9
93.5
100.0
90.9
95.5
100.0
95.5
95.5
95.5
96.1
72.7
77.3
95.5
90.9
95.5
95.5
87.9
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
95.5
86.4
86.4
86.4
50.0
59.1
77.3

POS
NN
NN
VB
VBG
NN
VBZ
NN
NN
NN
VBG
VBG
VBG
JJ
JJ
NNS
NNS
VBG
JJ
JJ
NN
NNS
NN
NN
NN
VBG
NN
VBG
NN
VB
NN
JJ
JJ
JJ

Table 16: Common metaphor problems M1 M5, derived Lakoff Johnson (1980).

650

fiThe Latent Relation Mapping Engine

Mapping
M6
seeds
ideas

M7
machine
mind

M8
object
idea

M9
following
understanding

M10
seeing
understanding

Source
seeds
planted
fruitful
fruit
grow
wither
blossom
Average agreement:
machine
working
turned
turned
broken
power
repair
Average agreement:
object
hold
weigh
heavy
light
Average agreement:
follow
leader
path
follower
lost
wanders
twisted
straight
Average agreement:
seeing
light
illuminating
darkness
view
hidden
Average agreement:










Target
ideas
inspired
productive
product
develop
fail
succeed









mind
thinking
awake
asleep
confused
intelligence
therapy







idea
understand
analyze
important
trivial










understand
speaker
argument
listener
misunderstood
digresses
complicated
simple








understanding
knowledge
explaining
confusion
interpretation
secret

Agreement
90.9
95.5
81.8
95.5
81.8
100.0
77.3
89.0
95.5
100.0
100.0
100.0
100.0
95.5
100.0
98.7
90.9
81.8
81.8
95.5
95.5
89.1
100.0
100.0
100.0
100.0
86.4
90.9
95.5
100.0
96.6
68.2
77.3
86.4
86.4
68.2
86.4
78.8

POS
NNS
VBD
JJ
NN
VB
VB
VB
NN
VBG
JJ
JJ
JJ
NN
NN
NN
VB
VB
JJ
JJ
VB
NN
NN
NN
JJ
VBZ
JJ
JJ
VBG
NN
VBG
NN
NN
JJ

Table 17: Common metaphor problems M6 M10, derived Lakoff Johnson
(1980).

651

fiTurney

problem A1, 81.8% participants (18 22) mapped gravity electromagnetism.
final column gives part-of-speech (POS) tags source target terms.
used Penn Treebank tags (Santorini, 1990). assigned tags manually.
intended mappings tags chosen mapped terms tags.
example, A1, sun maps nucleus, sun nucleus tagged NN.
POS tags used experiments Section 8. POS tags used LRME
shown participants experiment Section 6.

References
Ando, R. K. (2000). Latent semantic space: Iterative scaling improves precision interdocument similarity measurement. Proceedings 23rd Annual ACM SIGIR
Conference Research Development Information Retrieval (SIGIR-2000), pp.
216223.
Banko, M., & Etzioni, O. (2007). Strategies lifelong knowledge extraction web.
Proceedings 4th International Conference Knowledge Capture (K-CAP
2007), pp. 95102.
Bullinaria, J., & Levy, J. (2007). Extracting semantic representations word cooccurrence statistics: computational study. Behavior Research Methods, 39 (3),
510526.
Buttcher, S., & Clarke, C. (2005). Efficiency vs. effectiveness terabyte-scale information retrieval. Proceedings 14th Text REtrieval Conference (TREC 2005),
Gaithersburg, MD.
Chalmers, D. J., French, R. M., & Hofstadter, D. R. (1992). High-level perception, representation, analogy: critique artificial intelligence methodology. Journal
Experimental & Theoretical Artificial Intelligence, 4 (3), 185211.
Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A.
(1990). Indexing latent semantic analysis. Journal American Society
Information Science (JASIS), 41 (6), 391407.
Dolan, W. B. (1995). Metaphor emergent property machine-readable dictionaries. Proceedings AAAI 1995 Spring Symposium Series: Representation
Acquisition Lexical Knowledge: Polysemy, Ambiguity Generativity, pp. 2732.
Evans, T. (1964). heuristic program solve geometric-analogy problems. Proceedings
Spring Joint Computer Conference, pp. 327338.
Falkenhainer, B., Forbus, K. D., & Gentner, D. (1989). structure-mapping engine:
Algorithm examples. Artificial Intelligence, 41 (1), 163.
Firth, J. R. (1957). synopsis linguistic theory 19301955. Studies Linguistic
Analysis, pp. 132. Blackwell, Oxford.
Forbus, K., Usher, J., Lovett, A., Lockwood, K., & Wetzel, J. (2008). Cogsketch: Opendomain sketch understanding cognitive science research education.
Proceedings Fifth Eurographics Workshop Sketch-Based Interfaces Modeling, Annecy, France.
652

fiThe Latent Relation Mapping Engine

Forbus, K. D., Riesbeck, C., Birnbaum, L., Livingston, K., Sharma, A., & Ureel, L. (2007).
prototype system learns reading simplified texts. AAAI Spring Symposium
Machine Reading, Stanford University, California.
French, R. (1995). Subtlety Sameness: Theory Computer Model AnalogyMaking. MIT Press, Cambridge, MA.
French, R. M. (2002). computational modeling analogy-making. Trends Cognitive
Sciences, 6 (5), 200205.
Gardenfors, P. (2004). Conceptual Spaces: Geometry Thought. MIT Press.
Gentner, D. (1983). Structure-mapping: theoretical framework analogy. Cognitive
Science, 7 (2), 155170.
Gentner, D. (1991). Language career similarity. Gelman, S., & Byrnes, J.
(Eds.), Perspectives Thought Language: Interrelations Development, pp.
225277. Cambridge University Press.
Gentner, D. (2003). smart. Gentner, D., & Goldin-Meadow, S. (Eds.),
Language Mind: Advances Study Language Thought, pp. 195235.
MIT Press.
Gentner, D., Bowdle, B. F., Wolff, P., & Boronat, C. (2001). Metaphor like analogy.
Gentner, D., Holyoak, K. J., & Kokinov, B. N. (Eds.), analogical mind: Perspectives Cognitive Science, pp. 199253. MIT Press, Cambridge, MA.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling semantic roles. Computational
Linguistics, 28 (3), 245288.
Girju, R., Nakov, P., Nastase, V., Szpakowicz, S., Turney, P., & Yuret, D. (2007). Semeval2007 task 04: Classification semantic relations nominals. Proceedings
Fourth International Workshop Semantic Evaluations (SemEval 2007), pp.
1318, Prague, Czech Republic.
Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations (Third edition). Johns
Hopkins University Press, Baltimore, MD.
Hawkins, J., & Blakeslee, S. (2004). Intelligence. Henry Holt.
Hirst, G., & St-Onge, D. (1998). Lexical chains representations context detection
correction malapropisms. Fellbaum, C. (Ed.), WordNet: Electronic
Lexical Database, pp. 305332. MIT Press.
Hofmann, T. (1999). Probabilistic Latent Semantic Indexing. Proceedings 22nd
Annual ACM Conference Research Development Information Retrieval (SIGIR 99), pp. 5057, Berkeley, California.
Hofstadter, D. (2001). Epilogue: Analogy core cognition. Gentner, D., Holyoak,
K. J., & Kokinov, B. N. (Eds.), Analogical Mind: Perspectives Cognitive
Science, pp. 499538. MIT Press.
Hofstadter, D., & FARG (1995). Fluid Concepts Creative Analogies: Computer Models
Fundamental Mechanisms Thought. Basic Books, New York, NY.
653

fiTurney

Holyoak, K., & Thagard, P. (1989). Analogical mapping constraint satisfaction. Cognitive
Science, 13, 295355.
Holyoak, K., & Thagard, P. (1995). Mental Leaps. MIT Press.
Hummel, J., & Holyoak, K. (1997). Distributed representations structure: theory
analogical access mapping. Psychological Review, 104, 427466.
Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity based corpus statistics
lexical taxonomy. Proceedings International Conference Research
Computational Linguistics (ROCLING X), pp. 1933, Tapei, Taiwan.
Kilgarriff, A. (1997). dont believe word senses. Computers Humanities, 31,
91113.
Lakoff, G., & Johnson, M. (1980). Metaphors Live By. University Chicago Press.
Landauer, T. K., & Dumais, S. T. (1997). solution Platos problem: latent semantic analysis theory acquisition, induction, representation knowledge.
Psychological Review, 104 (2), 211240.
Leacock, C., & Chodrow, M. (1998). Combining local context WordNet similarity
word sense identification. Fellbaum, C. (Ed.), WordNet: Electronic Lexical
Database. MIT Press.
Lee, D. D., & Seung, H. S. (1999). Learning parts objects nonnegative matrix
factorization. Nature, 401, 788791.
Lepage, Y. (1998). Solving analogies words: algorithm. Proceedings 36th
Annual Conference Association Computational Linguistics, pp. 728735.
Lepage, Y., & Denoual, E. (2005). Purest ever example-based machine translation: Detailed
presentation assessment. Machine Translation, 19 (3), 251282.
Lin, D. (1998). information-theoretic definition similarity. Proceedings 15th
International Conference Machine Learning (ICML-98).
Martin, J. H. (1992). Computer understanding conventional metaphoric language. Cognitive Science, 16 (2), 233270.
Marx, Z., Dagan, I., Buhmann, J., & Shamir, E. (2002). Coupled clustering: method
detecting structural correspondence. Journal Machine Learning Research, 3,
747780.
Mason, Z. (2004). CorMet: computational, corpus-based conventional metaphor extraction system. Computational Linguistics, 30 (1), 2344.
Minsky, M. (1986). Society Mind. Simon & Schuster, New York, NY.
Mitchell, M. (1993). Analogy-Making Perception: Computer Model. MIT Press, Cambridge, MA.
Nastase, V., & Szpakowicz, S. (2003). Exploring noun-modifier semantic relations.
Fifth International Workshop Computational Semantics (IWCS-5), pp. 285301,
Tilburg, Netherlands.
Reitman, W. R. (1965). Cognition Thought: Information Processing Approach. John
Wiley Sons, New York, NY.
654

fiThe Latent Relation Mapping Engine

Resnik, P. (1995). Using information content evaluate semantic similarity taxonomy.
Proceedings 14th International Joint Conference Artificial Intelligence
(IJCAI-95), pp. 448453, San Mateo, CA. Morgan Kaufmann.
Rosario, B., & Hearst, M. (2001). Classifying semantic relations noun-compounds
via domain-specific lexical hierarchy. Proceedings 2001 Conference
Empirical Methods Natural Language Processing (EMNLP-01), pp. 8290.
Santorini, B. (1990). Part-of-speech tagging guidelines Penn Treebank Project. Tech.
rep., Department Computer Information Science, University Pennsylvania.
(3rd revision, 2nd printing).
Scholkopf, B., Smola, A. J., & Muller, K.-R. (1997). Kernel principal component analysis.
Proceedings International Conference Artificial Neural Networks (ICANN1997), pp. 583588, Berlin.
Turney, P. D. (2001). Mining Web synonyms: PMI-IR versus LSA TOEFL.
Proceedings Twelfth European Conference Machine Learning (ECML-01),
pp. 491502, Freiburg, Germany.
Turney, P. D. (2005). Measuring semantic similarity latent relational analysis. Proceedings Nineteenth International Joint Conference Artificial Intelligence
(IJCAI-05), pp. 11361141, Edinburgh, Scotland.
Turney, P. D. (2006). Similarity semantic relations. Computational Linguistics, 32 (3),
379416.
Turney, P. D. (2008). uniform approach analogies, synonyms, antonyms, associations. Proceedings 22nd International Conference Computational
Linguistics (Coling 2008), pp. 905912, Manchester, UK.
Turney, P. D., & Littman, M. L. (2005). Corpus-based learning analogies semantic
relations. Machine Learning, 60 (13), 251278.
Turney, P. D., Littman, M. L., Bigham, J., & Shnayder, V. (2003). Combining independent
modules solve multiple-choice synonym analogy problems. Proceedings
International Conference Recent Advances Natural Language Processing
(RANLP-03), pp. 482489, Borovets, Bulgaria.
van Rijsbergen, C. J. (2004). Geometry Information Retrieval. Cambridge University
Press, Cambridge, UK.
Veale, T. (2003). analogical thesaurus. Proceedings 15th Innovative Applications Artificial Intelligence Conference (IAAI 2003), pp. 137142, Acapulco,
Mexico.
Veale, T. (2004). WordNet sits SAT: knowledge-based approach lexical analogy.
Proceedings 16th European Conference Artificial Intelligence (ECAI 2004),
pp. 606612, Valencia, Spain.
Widdows, D. (2004). Geometry Meaning. Center Study Language
Information, Stanford, CA.
Yan, J., & Forbus, K. D. (2005). Similarity-based qualitative simulation. Proceedings
27th Annual Meeting Cognitive Science Society, Stresa, Italy.

655

fiJournal Artificial Intelligence Research 33 (2008) 149178

Submitted 03/08; published 09/08

Complexity Strategic Behavior Multi-Winner Elections
Reshef Meir
Ariel D. Procaccia
Jeffrey S. Rosenschein
Aviv Zohar

reshef24@cs.huji.ac.il
arielpro@cs.huji.ac.il
jeff@cs.huji.ac.il
avivz@cs.huji.ac.il

School Engineering Computer Science
Hebrew University Jerusalem

Abstract
Although recent years seen surge interest computational aspects
social choice, specific attention previously devoted elections multiple
winners, e.g., elections assembly committee. paper, characterize
worst-case complexity manipulation control context four prominent multiwinner voting systems, different formulations strategic agents goal.

1. Introduction
Computational aspects voting focus much interest, variety fields.
multiagent systems, attention motivated applications well-studied
voting systems1 method preference aggregation. instance, Ghosh, Mundhe,
Hernandez, Sen (1999) designed automated movie recommendation system,
conflicting preferences user may movies represented agents,
movies suggested selected according voting scheme (in example
multiple winners, several movies recommended user). general, candidates
virtual election entities beliefs, joint plans (Ephrati & Rosenschein, 1997),
schedules (Haynes, Sen, Arora, & Nadella, 1997).
Different aspects voting rules explored computer scientists. issue
particularly well-studied manipulation. many settings, voter may
better revealing preferences untruthfully. instance, real-life elections
voter awards single point favorite candidate, may judged pointless
vote candidate appears, polls, sure loser, even candidate
voters truthful first choice.
celebrated Gibbard-Satterthwaite Theorem (Gibbard, 1973; Satterthwaite, 1975)
implies non-dictatorial voting scheme (i.e., single voter
always dictates outcome election), always exist elections voter
improve utility lying true preferences.2 Nevertheless,
suggested bounded-rational agents may find hard determine exactly lie
use, thus may give manipulations altogether. words, computational
complexity may obstacle prevents strategic behavior. first address
1. use terms voting schemes, voting rules, voting systems, voting protocols interchangeably.
2. theorem also generalized multiple winner setting (Duggan & Schwartz, 2000).
c
2008
AI Access Foundation. rights reserved.

fiMeir, Procaccia, Rosenschein, & Zohar

point Bartholdi, Tovey Trick (1989); Bartholdi Orlin (1991) later showed
manipulating important Single Transferable Vote (STV) voting rule N P-complete
problem.
recently, shown voting protocols tweaked adding
elimination preround, way makes manipulation hard (Conitzer & Sandholm, 2003).
Conitzer, Sandholm, Lang (2007) studied setting entire coalition
manipulators. setting, problem manipulation coalition N P-complete
variety protocols, even number candidates constant.
Another related issue received significant attention computational difficulty controlling election. Here, authority conducts elections attempts
achieve strategic results adding removing registered voters candidates.
rationale work well: computationally hard determine improve
outcome election control, chairman might give cheating altogether.
Bartholdi, Tovey Trick (1992) first analyzed computational complexity different
methods controlling election Plurality Condorcet protocols.
paper, augment classical problems manipulation control introducing multiple winners. Specifically, assume manipulator utility function
candidates, manipulators goal achieve set winners
total utility threshold. study abovementioned problems respect
four simple important multi-winner voting schemes: SNTV, Bloc voting, Approval,
Cumulative voting.
paper proceeds follows. Section 2, describe voting rules question.
Section 3, deal manipulation problems. Section 4, deal control problems.
discuss related work Section 5, conclude Section 6.

2. Multi-Winner Voting Schemes
section present several multi-winner voting systems significance. Although
discussion self-contained, interested readers find details article Brams
Fishburn (2002).
Let set voters V = {v1 , v2 , . . . vn }; let set candidates C = {c1 , . . . cm }.
Furthermore, assume k N candidates elected.
Multi-winner voting rules differ single-winner ones properties
expected satisfy. major concern multi-winner elections proportional representation: faction consists fraction X population represented
approximately fraction X seats assembly. property satisfied
(generalizations of) many rules usually considered respect single-winner
elections.
Thus, examine four prevalent multi-winner voting rules. four,
candidates given points voters, k candidates points win
election. schemes differ way points awarded candidates.
Single Non-Transferable Vote (SNTV): voter gives one point favorite candidate.3
3. SNTV single winner elections also known Plurality.

150

fiComplexity Strategic Behavior Multi-Winner Elections

Bloc voting: voter gives one point k candidates.4
Approval voting: voter approves disapproves candidate; approved
candidate awarded one point, limit number candidates
voter approve.
Cumulative voting: allows voters express intensities preference, asking
distribute fixed number points among candidates. Cumulative voting
especially interesting, since encourages minority representation maximizes
social welfare (Brams & Fishburn, 2002).
Scoring rules prominent family voting rules. voting rule family
defined vector integers
~ = h1 , . . . , i, l l+1 l = 1, . . . , 1.
voter reports ranking candidates, thus awarding 1 points top-ranked
candidate, 2 points second candidate, general l points candidate
ranked place l. Notice SNTV family scoring rules defined |C|
vector h1, 0, . . . , 0i, Bloc family scoring rules defined |C|, k
vector h1, . . . , 1, 0, . . . , 0i, number 1s k.

3. Manipulation
voter considered manipulator, said vote strategically, voter
reveals false preferences attempt improve outcome election. Settings
manipulation possible avoided, since may lead socially undesirable
outcome emerging winner election. Therefore, computational resistance
manipulation considered advantage.
classical formalization manipulation problem (Bartholdi et al., 1989),
given set C candidates, set V voters, distinguished candidate p C.
also full knowledge voters votes. asked whether possible cast
additional vote, manipulators ballot, way makes p win election.
generalizing problem k-winner case, several formulations possible.
general formulation given following definition.
Definition 3.1. Manipulation problem, given set C candidates,
set V voters already cast vote, number winners k N, utility
function u : C Z, integer N.
P asked whether manipulator cast
vote resulting election, cW u(c) t, W set winners,
|W | = k.
Notice number winners k parameter problem.
Remark 3.2. manipulators utility function implicitly assumed additive. One
consider elaborate utility functions, ones investigated context
combinatorial auctions, beyond scope paper.
4. Bloc voting also known k-Approval.

151

fiMeir, Procaccia, Rosenschein, & Zohar

Remark 3.3. make standard assumption tie-breaking adversarial
manipulator (Conitzer et al., 2007), i.e., several candidates perform equally
well election, ones lower utility manipulator elected.
number winners k = 1, assumption equivalent formulating
manipulation problems unique winner version, ballot must cast
way designated candidate strictly better rest (e.g., higher score).
One might argue general formulation problem given makes manipulation harder. Indeed, manipulator might following, specific, goals
mind.
1. manipulator specific candidate interested seeing among
winners (constructive manipulation).
2. manipulator specific candidate interested excluding
set winners (destructive manipulation) (Conitzer et al., 2007).
3. manipulator (additive) boolean-valued utility function candidates u : C {0, 1}.
first second settings naturally special cases third, third
special case Definition 3. intend explore foregoing formulations
Manipulation problem. Notice one also consider goals, example
manipulator favorite set candidates interested seeing them,
many possible them, among winners. However, investigate
goals insofar special cases boolean-valued utility function.
Remark 3.4. Unless explicitly mentioned otherwise, usually assume general (additive)
utility function, Definition 3.1.
find convenient represent SNTV Bloc voting using common framework. consider l-Bloc voting rulesvoting rules every voter gives one point
exactly l candidates, l k. Notice SNTV l = 1, Bloc voting
l = k. remind reader number winners k constant, rather
parameter Manipulation problem.
Proposition 3.5. Let l {1, . . . , k}. Manipulation l-Bloc voting P.
Proof. manipulator faced score awarded candidates voters
V ; let s[c] total score candidate c. Order candidates score, let
s0 score kth highest candidate. example, k = 3, |C| = = 4,
initial scores 8, 5, 5, 3, s0 = 5. addition, let
= {c C : s[c] > s0 }.
Notice |A| k 1. Let
B = {c C : s[c] = s0 }.
B may large, particular candidates score, B = C.
152

fiComplexity Strategic Behavior Multi-Winner Elections

Now, candidates least s0 + 2 initial points elected regardless
actions manipulator, manipulator award one point
candidate. Candidates exactly s0 +1 points elected, unless candidates
lower utility ultimately receive s0 + 1 points due manipulators voteas ties
broken adversarially manipulator. Let us examine candidates less
s0 points. Notice candidates s[c] s0 2 lose case. Moreover,
since ties broken adversarially, voting candidates s0 1 points cannot benefit
manipulator. conclude point, manipulator better make
sure candidates B (with exactly s0 points) eventually elected high
utility possible.
Therefore, put simply, manipulators optimal strategy vote top
(in terms utility) k |A| (top l l < k |A|) candidates B, thus guaranteeing
candidates among winners, cast remaining votes favor
candidates (which win anyway). discussion leads conclusion
Algorithm 1 decides Manipulation problem. Clearly computational complexity
Algorithm 1 Decides Manipulation problem l-Bloc voting
1: procedure Manipulate-Bloc(V, C, k, u, t, l)
2:
s[c] |{v V : v votes candidate c}|
3:
s0 score kth highest candidate
4:
{c C : s[c] > s0 }
|A| k 1
5:
B {c C : s[c] = s0 }, ordered u(c) decreasing order
6:
l k |A|
7:
manipulator votes top l candidates B
8:
else
9:
manipulator votes top k |A| candidates B l + |A| k candidates

10:
end
11:
utility winners
12:
return true
13:
else
14:
return false
15:
end
16: end procedure
algorithm polynomial input size.
following immediate corollary:
Corollary 3.6. Manipulation SNTV Bloc voting P.
situation Approval voting dissimilar. Indeed, question is: voter
gain approving k candidates? priori, answer yes. However, given
votes voters, clearly manipulator cannot gain approving candidates
k eventual winners election. hand, manipulator also
benefit approving less k voters. Say manipulator approved l < k
153

fiMeir, Procaccia, Rosenschein, & Zohar

voters, candidates W , |W | = k, eventually election. Let C
candidates manipulator approved, W = W \ C winners
manipulator approve; W k l. manipulator approves l candidates
C well k l candidates W , set winners clearly still going
W . Therefore, manipulator better approve exactly k candidates;
already demonstrated Proposition 3.5 accomplished optimally
efficiently. Therefore:
Corollary 3.7. Manipulation Approval P.
Remark 3.8. Formally, manipulation Approval subtle issue, since issue may
ill-defined voters assumed linear preferences candidates.
case, multiple sincere ballots (where approved candidates preferred
disapproved candidates). specific settings, voter cannot gain casting
insincere ballot (Endriss, 2007), always true. case, manipulation
problem according definition well-defined nontrivial Approval.
contrast abovementioned three voting rules, Cumulative voting turns
computationally hard manipulate general utility function.
Proposition 3.9. Manipulation Cumulative voting N P-complete.
implicit assumption made proposition number points
distributed constant, rather parameter Manipulation problem
Cumulative voting.
proof Proposition 3.9 relies reduction one well-known
N P-complete problems, Knapsack problem.
Definition 3.10. Knapsack problem, given set items = {a1 , . . . , },
weight w(a) N value (a),
P N.
P capacity b N,

asked whether subset aA (a) aA w(a) b.

Proof Proposition 3.9. problem clearly N P.
see Manipulation Cumulative voting N P-hard, prove Knapsack
reduces problem. given input hA, w, , b, ti Knapsack, construct
instance Manipulation Cumulative voting follows.
Let n = |A|. 2n voters, V = {v1 , . . . , v2n }, 3n candidates, C = {c1 , . . . , c3n },
n winners. addition, voter may distribute b points among candidates.
want voters V cast votes way following three conditions
satisfied:
1. j = 1, . . . , n, cj b w(aj ) + 1 points.
2. j = n + 1, . . . , 2n, cj b points.
3. j = 2n + 1, . . . , 3n, cj exactly b points.
easily done. Indeed, = 1, . . . , n, voter vi awards b w(ai ) + 1 points
candidate ci , awards remaining w(ai ) 1 points candidate cn+i . Now,
= 1, . . . , n, voter n + awards b points candidate c2n+i .
154

fiComplexity Strategic Behavior Multi-Winner Elections

define utility u candidates follows:
(
(aj ) j = 1, . . . , n
u(cj ) =
0
j = n + 1, . . . , 3n
transformation clearly polynomial time computable, remains verify
reduction. Assume subset total weight b
total value least t. Let C = {cj : aj }. manipulator awards w(aj ) points
candidate c C , raising total score candidates b + 1. Since initially
candidates b points, candidatesPc C among
P n winners
u(c)
=
election. total utility candidates is:

aA (a) (since
cC
j = 1, . . . , n, u(cj ) = (aj )).
direction, assume manipulator able distribute b points way
winners election total utility least t. Recall initially
least n candidates b points utility 0, ties broken adversarially
manipulator. Therefore, must subset C C candidates ultimately
score least b+1, total utility least t. Let corresponding
items Knapsack instance, i.e., aj cj C . total weight
items b, b points distributed among candidates C
manipulator, cj C initially b w(aj ) + 1 points. also holds
total utility items exactly total utility candidates C , namely
least t.
next proposition gives negative answer question whether Manipulation
Cumulative voting still hard restricted formulations manipulators
goal, discussed beginning section. Indeed, put forward algorithm
decides problem boolean-valued utility function.
Proposition 3.11. Manipulation Cumulative voting boolean-valued utility
function u : C {0, 1} P.
Remark 3.12. result holds even number points distributed exponential
number voters candidates.
Proof Proposition 3.11. Let s[c] score candidate c C manipulator
cast vote, [c] cs score manipulators vote taken account.
Assume without loss generality s[c1 ] s[c2 ] . . . s[cm ]. Let = {d1 , d2 , . . .}
set desirable candidates C u(d) = 1, assume sorted
nonincreasing scores.
Informally, going find threshold thresh pushing candidates
threshold guarantees victory. check whether possible distribute
L points least candidates pass threshold, L number points
available voter.
Formally, consider Algorithm 2 (w.l.o.g. k |D| t, otherwise manipulation
impossible). algorithm clearly halts polynomial time. remains prove
correctness algorithm.
155

fiMeir, Procaccia, Rosenschein, & Zohar

Algorithm 2 Decides Manipulation Cumulative voting boolean-valued utility
1: j max{j : |{c1 , c2 , . . . , cj1 } D| + k + 1 j cj
/ D} j exists, since
condition holds first candidate
2: thresh s[cj ]
Pt
3:
j=1 max{0, thresh + 1 s[dj ]}
4: L
5:
return true
6: else
7:
return false
8: end
Lemma 3.13. Algorithm 2 correctly decides Manipulation Cumulative voting
boolean-valued utility function.
Proof. Denote W = {c1 , . . . , ck } k candidates highest score (sorted)
manipulators vote, W final set k winners. threshold candidate cj
partitions W two disjoint subsets: Wu = {c1 , . . . , cj 1 }, Wd = {cj , . . . , ck }.
maximality j , holds that:
|Wu D| + |Wd | = |Wu D| + (k + 1 j ) = t.

(1)

Note exact number votes required push desirable candidates
threshold. Now, must show manipulator cast vote way
winner set W satisfies |W D| if, if, L S.
Suppose first L. clearly possible push desirable candidates
thresh. Wu threshold already; follows Wd replaced
entirely desirable candidates.
Let W = {w1 , . . . , wk } set new winners. particular, write W =
Wu {wj , . . . , wk }. Wu contains |Wu D| desirable candidates, {wj , . . . , wk } consists
purely desirable candidates. Equation (1):
|W D| = |Wu D| + |{wj , . . . , wk }|
= |Wu D| + |Wd |
=t
Conversely, suppose > L. must show manipulator cannot distribute L
points way candidates among winners.
Clearly possibility push desirable candidates thresh. Consider
ballot cast manipulator, assume w.l.o.g. manipulator distributed
points among candidates D. Denote new set winners W = Wu Wd ,

Wu = {c C : [c] > thresh}
Wd = {c C : [c] thresh}.
claim
|Wu D| = k t,
156

(2)

fiComplexity Strategic Behavior Multi-Winner Elections

= C \ D. Indeed, Equation (1)
|Wu D| = k 1 + j ,
therefore
|Wu D| = |Wu D| = |Wu | |Wu D|
= (j 1) (t k 1 + j )
=kt
first equality follows fact points distributed candidates
D.
Denote F set candidates pushed threshold. Formally:
F = {c : [c] > thresh s[c] thresh}
Thus:
Wu = Wu F.
Let w new position candidate cj candidates sorted nonincreasing [c]. holds
w = j + |F |.
claim
|Wd D| 1.

(3)

Indeed,
|Wu D| <



|Wu D| + |F | = |Wu D| < = |Wu D| + k + 1
|F | < k + 1
w

=

j

+ |F | <

j

j

+k+1

w

k

=k+1

|Wd D| 1
combining Equations (2) (3), finally obtain:
|W D| = k |W D|
= k (|Wu D| + |Wd D|)
=t1
<t

proof Proposition 3.11 completed.
157




j

cj W

k (k + 1)

j





fiMeir, Procaccia, Rosenschein, & Zohar

Remark 3.14. proof shows manipulation Cumulative voting coalition
(even weighted) voters, work Conitzer et al. (2007), tractable
boolean-valued utility function. follows simply joining (weighted) score pools
voters coalition.
Remark 3.15. possible show number points distributed
polynomially bounded, manipulation Cumulative voting P even general utility
functions.

4. Control
control setting, assume authority controlling election (hereinafter,
chairman) power tweak elections electorate slate candidates
way might change outcome. also form undesirable strategic behavior,
part behind-the-scenes player supposed take active part
election.
one setting, chairman might add remove voters support candidate,
number voters add/remove without alerting attention actions
limited. problems formally defined follows:
Definition 4.1. problem Control Adding Voters, given set
C candidates, set V registered voters, set V unregistered voters, number
winners k N, utility function u : C Z, integers r, N. asked
whether possible register r voters V resulting election,
P
cW u(c) t, W set winners, |W | = k.

Definition 4.2. problem Control Removing Voters, given set C
candidates, set V registered voters, number winners k N, utility function
u : C Z, integers r, N. asked whether
possible remove
P
r voters V resulting election, cW u(c) t, W set
winners, |W | = k.
Another possible misuse chairmans authority tampering slate
candidates. Removing candidates obviously helpful, even adding candidates
sometimes tip scales direction chairmans favorites.
Definition 4.3. problem Control Adding Candidates, given
set C registered candidates, set C unregistered candidates, set V voters,
number winners k, utility function u : C C Z, integers r, N. voters
preferences candidates C C . asked whether possibleP
add
r candidates C C , resulting elections C C , cW u(c) t,
W set winners, |W | = k.
Definition 4.4. problem Control Removing Candidates, given
set C candidates, set V voters, number winners k, utility function u : C Z,
integers r, N. asked whetherPit possible remove r candidates
C , resulting elections cW u(c) t, W set winners,
|W | = k.
158

fiComplexity Strategic Behavior Multi-Winner Elections

clarification order. context scoring rules, assumption
two problems voters rankings candidates C C . Therefore,
candidates added removed, voters preferences new set candidates
still well-defined. goes Approval: voter approves disapproves
every candidates C C . However, context Cumulative voting, problems
control adding/removing candidates well-defined. Indeed, one would require
specification voters distribute points among every possible subset
candidates, would require representation exponential size.5 Consequently,
consider control adding removing candidates Cumulative voting.
Remark 4.5. authors (e.g., Hemaspaandra et al., 2007b) considered types
control, control partitioning set voters. paper, restrict
attention four types control mentioned above.
Remark 4.6. Unless stated otherwise, assume ties broken adversarially
chairman.
before, unless explicitly mentioned otherwise, going assume general
additive utility function, definitions.
Remark 4.7. clear computational problems N P voting
rules question. Therefore, N P-completeness proofs show hardness.
4.1 Controlling Set Voters
Control Adding Voters tractable SNTV, though procedure trivial.
Bartholdi et al. (1992) showed control adding voters easy SNTV
single winner, former result seen extension latter (to case
multiple winners general [additive] utility function).
Proposition 4.8. Control Adding Voters SNTV P.
Proof. describe algorithm, Control-SNTV, efficiently decides Control
SNTV. Informally, algorithm works follows. first calculates number points
awarded candidates voters V . Then, stage, algorithm analyzes
election l top winners original election remain winners, attempts
select k l winners way maximizes utility. done setting
threshold one point score (l + 1)-highest candidate; algorithm
pushes scores potential winners threshold (see Figure 1 illustration).
formal description Control-SNTV given Algorithm 3. procedure Push
works follows: first parameter threshold thr, second parameter
number candidates pushed, pushN um. procedure also implicit access
input Control-SNTV, namely parameters given Control instance. Push
returns subset V V registered. say procedure pushes candidate
c threshold exactly thr s[c] voters v V vote c registered.
words, procedure registers enough voters V order ensure cs score
5. possible imagine compact representations, beyond scope paper.

159

fiMeir, Procaccia, Rosenschein, & Zohar

6

03

22

4
3

22

0

1

1

2

0

5

03

22

22

2

0

1

5

Figure 1: left panel illustrates input Control problem SNTV.
candidate represented circled numberthe utility candidate.
location circle determines score candidate, based voters
V . Let k = 5; winners blackened. Now, assume 6 voters
V , 3 voting two bottom candidates, r = 3. chairman
award 3 points candidate utility 5 score 0, would
change result election. Alternatively, chairman award 3
points candidate utility 2 score 1, thus improving utility 1,
seen right panel. election considered algorithm
l = 4, s[ci5 ] = 3, threshold 4.

reaches theP
threshold. Push finds subset C candidates size pushN um
maximizes cC u(c), restriction candidates C simultaneously
pushed threshold registering subset V V s.t. |V | r. procedure returns
subset V .
Now, assume procedure Push always correct (in maximizing utility
k l candidates able push threshold s[cl+1 ] + 1, registering
r voters) runs polynomial time. Clearly, Control-SNTV also runs
polynomial time. Furthermore:
Lemma 4.9. Control-SNTV correctly decides Control problem SNTV.
Proof. Let W = {cj1 , . . . , cjk } k winners election take
account votes voters V (the original election), sorted descending score,
candidates identical score, ascending utility. Let W = {cj1 , . . . , cjk }
candidates controlled election maximum utility, sorted descending
score, ascending utility; let [c] final score candidate c optimal
election. Let min smallest index cjmin
/ W (w.l.o.g. min exists, otherwise

W = W done). holds candidates c W , [c] s[cjmin ]. Now,
160

fiComplexity Strategic Behavior Multi-Winner Elections

Algorithm 3 Decides Control problem SNTV.
1: procedure Control-SNTV(C, V, V , k, u, r, t)
2:
s[c] |{v V : v votes candidate c}|
3:
Sort candidates descending score
Break ties ascending utility
4:
Let sorted candidates {ci1 , . . . , cim }
5:
l = 0, . . . , k
Fix l top winners
6:
V Push(s[cl+1 ] + 1, k l)
Select winners; see details
7:
ul utility election V registered
8:
end
9:
maxl ul return true
10:
else
11:
return false
12:
end
13: end procedure

assume w.l.o.g. c W [c] = s[cjmin ] c W (and consequently,
c = cjq q < min). Indeed, must hold u[c] u[cjmin ] (as tie-breaking
adversarial chairman), indeed c
/ W even though c W , chairman
must registered voters vote c, although lower total utility
keep unchanged.
sufficient show one elections considered algorithm
set winners utility least W . Indeed, let W = {cj1 , . . . , cjmin1 } W ;
k min + 1 candidates c W \ W s[c] s[cjmin ] + 1. algorithm considers
election first min 1 winners, namely W , remain fixed, threshold
s[cjmin ] + 1. Surely, possible push candidates W \ W threshold,
election, winners would W . Since Push maximizes utility
k min + 1 candidates pushes threshold, utility returned Push
l = min 1 least large total utility winners W .

remains explain procedure Push implemented run polynomial
time. Recall Knapsack problem; general formulation problem
two resource types. item two weight measures, w1 (ai ) w2 (ai ),
specify much resource consumes type, knapsack two
capacities: b1 b2 . requirement total amount resource first
type consumed exceed b1 , total use resource second
type exceed b2 . problem, often two dimensions,
called Multidimensional Knapsack. Push essentially solves special case twodimensional knapsack problem, capacities b1 = r (the number voters
chairman allowed register), b2 = pushN um (the number candidates
pushed). threshold thr, candidate cj supported least
thr s[cj ] voters V , set w1 (aj ) = thr s[cj ], w2 (aj ) = 1, (aj ) = u(cj ).
Multidimensional Knapsack problem solved time polynomial
number items capacities knapsack (Kellerer, Pferschy, & Pisinger, 2004)
161

fiMeir, Procaccia, Rosenschein, & Zohar

(via dynamic programming, example). Since case capacities bounded
|V | m, Push designed run polynomial time.
following lemma allows us extend results Control Adding Voters
Control Removing Voters, vice versa. shall momentarily apply
SNTV, also prove useful later on.
Lemma 4.10. Let R = hV, C, r, t, k, ui instance Control Removing Voters
voting rule, let = hV , V , C , r , , k , u instance Control
Adding Voters voting rule, that:
C = C
r = r
=

X

u(c)

cC

k = |C| k
u (c) = u(c)
V = V
Let U subset voters selected chairman V = V . Denote sU [c], sU [c]
final score candidate c election obtained removing adding voters
U , respectively. holds
c, c C, U V,

sU [c] sU [c ] sU [c] sU [c ]

(4)

R Control Removing Voters Control Adding
Voters.
Proof. condition (4) sU , sU holds k = |C| k winners
constructed instance exactly |C| k losers original instance.6 is, W
winners given instance W winners constructed instance,
W = C \ W .
follows that,
X
X
X
u(c) (t +
u(c))
u(c) =
cW

cC

cC\W

=

X

u (c) +

cC\W

=

X

u (c)

X

u (c)

cC

cW

P
Thus, choice subset U removed added, cW u(c) if,
P
if, cW u (c) . conclude given instance yes instance
constructed instance yes instance.
6. statement also takes account tie-breaking scheme, candidates lower utility
original instance candidates higher utility.

162

fiComplexity Strategic Behavior Multi-Winner Elections

Applying lemma, easily obtain:
Proposition 4.11. Control Removing Voters SNTV P.
Proof. give polynomial time reduction Control Removing Voters
SNTV Control Adding Voters SNTV, shown (Proposition 4.8) P. Given instance hV, C, r, t, k, ui Control Removing Voters,
define equivalent instance hV , V , C , r , , k , u latter problem. want
use Lemma 4.10, need define V correctly.
voter v V = V , let f (v) C candidate voter v ranks first; f (v)
gives relevant information voter vs ballot. ballots voters V
defined following rule. candidate c C,
|{v V : f (v ) = c}| = |V | |{v V : f (v) = c}|
holds that:
sU [c] = |{v V : f (v) = c}| |{v U : f (v) = c}|
definition V ,
sU [c] = |{v V : f (v ) = c}| + |{v U : f (v ) = c}|
= |V | |{v V : f (v) = c}| + |{v U : f (v) = c}|
= |V | (|{v V : f (v) = c}| |{v U : f (v) = c}|)
= |V | sU [c]
Hence c, c C, U V :
sU [c] sU [c ] sU [c] sU [c ].
implies conditions Lemma 4.10 hold, thus polynomial reduction
Control Removing Voters SNTV Control Adding Voters
SNTV. Since latter problem P, former.
rest section prove control adding/removing voters hard
three voting rules consideration, even chairman simply wants include
exclude specific candidate. fact, statement includes 12 different results (3 voting
rules adding/removing include/exclude). Instead proving result separately,
use generic reductions. proof scheme follows: shall first establish
Control Removing Voters hard Approval, Bloc, Cumulative voting,
even chairman wants include candidate. use Lemma 4.10 show
adding voters hard foregoing rules, even chairman wants exclude
candidate. Finally, Lemma 4.17 give us six remaining results: removing
excluding, adding including. overall scheme illustrated Figure 2.
following result known work Hemaspaandra et al. (2007b).
Proposition 4.12. (Hemaspaandra et al., 2007b) Control Removing voters
Approval N P-complete, even chairman trying make distinguished candidate
win single winner elections.
163

fiMeir, Procaccia, Rosenschein, & Zohar

Figure 2: Scheme hardness proofs control adding/removing voters
chairman wants include/exclude distinguished candidate, voting rules:
Bloc, Approval, Cumulative voting.

result, results appear later, stated single winner
elections, i.e., number winners k satisfies k = 1. Clearly hardness
problem k = 1 implies hardness general formulation problem
k also parameter.
Proposition 4.13. Control Removing Voters Cumulative voting N P-complete,
even chairman trying make distinguished candidate win single winner elections.
Remark 4.14. instances control problems chairman simply wants include/exclude distinguished candidate, replace utility function u distinguished candidate p, i.e., u(p) = 1 (respectively, u(p) = 1) include (resp., exclude)
u(c) = 0 candidates c. threshold = 1 (resp., = 0).
Proof Proposition 4.13. use reduction Control Removing Voters
Approval. Let hV, C, p, ri instance problem Approval, p C
distinguished candidate (and k = 1). Define instance problem Cumulative
voting follows: pool points satisfies L = |C|; r = r; p = p; C = C C ,
C contains |V | L candidates.
construct set voters. voter v V , add V voter
awards 1 point every candidate v approves, gives points (no
164

fiComplexity Strategic Behavior Multi-Winner Elections

L ) distinct candidates C . V contains two voters;
voters gives one point candidate C. Finally, let V = V V .
Removing voters V cannot promote p , chairman may limit without
loss generality removing voters V . Now, every candidate C one
point, none beats candidate C (each least two points).
Furthermore, every selection voters V corresponding voters V
remove, candidate C gets score new instance got original
instance, plus 2. directly implies control possible constructed instance
(of Cumulative voting) possible given instance (of Approval).
order complete hardness proofs removing voters including distinguished candidate, give similar result Bloc. Notice, however, reduction
constructs instances multiple winners.
Proposition 4.15. Control Removing Voters Bloc N P-complete, even
chairman simply wants include distinguished candidate among winners.
Proof. prove proposition polynomial time reduction Control Removing Voters Approval. Let hV, C, p, k, ri instance latter problem (with
k winners; possible let k = 1). Denote, usual, n = |V | = |C|. Define:
C = C C C , C contains new candidates, C contains 10 k n new
candidates; p = p; k = k + m; r = r. need design new instance
candidates C among winners, candidates C among
losers. define voter set accordingly.
1. V contains one voter voter V (n total). v V , v gives point
candidates C v approved. points go candidates C ,
C .
2. V contains r + 2 voters. voter gives point candidate C, gives
k points candidates C (arbitrarily).
3. V contains 4n voters. voter awards points C , k candidates
C (in way specified later).
Finally let V = V V V .
claim hV , C , p , k , r Control Removing Voters Bloc
hV, C, p, k, ri Control Removing Voters Approval.
candidate c, denote s[c], [c] total number votes c obtains original
constructed instances, removing voters. Note whole set C gets
(4n)k + nk votes, less size (10kn), possible scatter votes
V candidate C 1 point. addition, candidate
C least 4n points. also holds
c C, [c] = s[c] + r + 2,
(from votes V , V ), thus:
c C, 4n r > r + n + 2 [c] > r + 1.
165

fiMeir, Procaccia, Rosenschein, & Zohar

conclude that:
c C, c C , [c] < [c ] r,

c C, c C , [c] > [c ] + r.
Without loss generality chairman removes voters V (= V ), since
votes may influence result elections. Denote sU [c], sU [c] score
candidate c instances, removing subset U V = V . U
|U | r holds that:
c1 , c2 C, sU [c1 ] sU [c2 ] sU [c1 ] sU [c2 ].
Moreover,
c C, c C , sU [c] < sU [c ],

c C, c C , sU [c] > sU [c ].
words, candidates C among winners, whereas candidates
C among losers, ranking candidates C change.
conclude k winners constructed instance exactly candidates
C together k winners given Approval instance. Thus control possible
original instance Approval (i.e., possible make p win)
possible constructed instance Bloc.
promised, use Lemma 4.10 transform results three voting rules
Control (Include winner) Removing Voters Control (Exclude winner) Adding
Voters.
Proposition 4.16. Control Adding Voters Approval, Bloc, Cumulative voting N P-hard, even chairman simply wants exclude distinguished candidate
set winners.
Proof. prove proposition using Lemma 4.10 similar notations. Consider
instance Control Removing Voters chairman wants include
candidate. utility function u 1 p 0 otherwise, = 1. utility
function u constructed Lemma 4.10 -1 p 0 otherwise, threshold
1 = 0; is, chairman wants exclude p. obtain desired
result directly showing lemmas condition holds. words, must show
three voting rules question, given instance Control Removing
Voters, possible construct instance Control Adding Voters
satisfies condition (4).
Approval Bloc: proof similar Proposition 4.11.
voter Control Removing Voters instance, add voter V gives
points complement subset candidates. Formally, denote Cv C candidates
v awards points. v V , add v V , Cv = C \ Cv . Note
166

fiComplexity Strategic Behavior Multi-Winner Elections

construction creates legal Bloc instance since |Cv | = |C \ Cv | = |C| k = k ,
thus

(v V, |Cv | = k) v V , |Cv | = k .
Let U V ; holds voting rules that:

sU [c] = |{v V : c Cv }| |{v U : c Cv }|
and,
sU [c] = |{v V : c Cv }| + |{v U : c Cv }|
= |{v V : c
/ Cv }| + |{v U : c Cv }|
= |V | |{v V : c Cv }| + |{v U : c Cv }|
= |V | (|{v V : c Cv }| |{v U : c Cv }|)
= |V | sU [c],
thus:
c, c C, U V,

sU [c] sU [c ] sU [c] sU [c ].

Cumulative voting: original voter, add new voter V
distributes pool way complements original
distribution. Formally, let svc
P
v
score voter v gives candidate c; v V,
cC sc = L. point distribution

new voter v V svc = L svc , c C. Note legal Cumulative
voting instance, L = L(m 1):
X
X X
L svc = mL
svc = mL L = L(m 1) = L .
v V ,
svc =
cC

cC

cC

look final score adding/removing voters U V :
X
X
X
sU [c] =
svc =
svc
svc .
vV

vV \U

vU

Further,
sU [c] =

X

svc

vV U

=

X



svc +

v V

=

X

X

vU

(L

svc )

+

vV

= L|V |

svc
X

vU

X

svc

+

vV

= L|V |

svc

X

vV

= L|V | sU [c]
167

X

svc

vU

svc



X

vU

svc

!

fiMeir, Procaccia, Rosenschein, & Zohar

Therefore, have:
c, c C, U V,

sU [c] sU [c ] sU [c] sU [c ].

final generic transformation lemma shows constructive destructive
settings, i.e., including/excluding distinguished candidate, basically equivalent
three voting rules question. subtle point, shall deal formulate
prove lemma, lemma reverses tie-breaking scheme: adversarial tiebreaking (as assumed far) becomes friendly tie-breaking, i.e., ties broken
favor candidates higher utility; vice versa.

Lemma 4.17. every instance Control Adding (resp., Removing) Voters
adversarial tie-breaking W C made win adding (resp., removing)
r voters, instance Control Adding (resp., Removing) Voters
friendly tie-breaking candidate set C C\W made win adding
(resp., removing) r voters. Similarly, friendly tie-breaking transformed adversarial tiebreaking. statement holds Approval, Bloc, Cumulative Voting.

Proof. clarity, prove lemma adding voters; proof removing voters
practically identical. also prove result Cumulative voting (with pool
points size L), straightforward extend Approval Bloc, generally
replacing L 1.
Given instance hV, V , C, p, k, ri, let U V subset r voters adding
voters induces set winners W . Construct instance Control Adding
Voters follows: set number winners k = |C| k, set pool points
L = L(|C| 1). voter v V, V voter v gives L svc
candidate c, svc score given c voter v. Define set V (resp., V )
set containing voters corresponding voters V (resp., V ). Note
v V V ,

X

cC



svc =

X

L svc = L|C|

cC

X

svc = L|C| L = L(|C| 1) = L .

cC

Denote V final set voters results adding voters U , V
corresponding set voters constructed instance (i.e., v V v V ). Also
denote s[c], s[c] score candidate c C original instance new
instance, adding voters.
168

fiComplexity Strategic Behavior Multi-Winner Elections

previous proofs, get candidates order reversed. Formally,
comparing score two candidates:
X
X
s[c1 ] s[c2 ] =
svc1
svc2
v V

=

X

v V

(L svc1 )

vV

X

vV

= |V |L

X

(svc1 ) |V |L +

vV

=

X

vV

(L svc2 )

svc2



X

X

(svc2 )

vV

svc1

vV

= s[c2 ] s[c1 ]
Therefore, since assumed tie-breaking scheme reversed, k candidates
highest score original instance k candidates lowest score
constructed instance, and, moreover, C \ W form k = k winners new
instance.7
Lemma 4.17 allows us derive straightforward reductions versions Control
chairman wants include/exclude distinguished candidate: possible
include candidate given instance possible exclude candidate
constructed instance, vice versa. point address change
tie-breaking scheme. Fortunately, easy obtain versions Propositions 4.12,
4.13, 4.15 friendly tie-breaking, slight modifications proofs. Hence,
lemma, applied three propositions, yields:
Proposition 4.18. Control Removing Voters Bloc, Approval, Cumulative
voting N P-complete, even chairman simply wants exclude distinguished
candidate.
Recall Lemma 4.10 preserves tie-breaking scheme. Thus, obtain
friendly version Proposition 4.16. Then, Lemma 4.17 applied friendly Proposition 4.16 gives us final result section. Note constructive version (i.e.,
including distinguished candidate) Control Adding Voters Approval already known N P-hard even single winner elections (Hemaspaandra et al., 2007b).
Proposition 4.19. Control Adding Voters Approval (Hemaspaandra et al.,
2007b), Bloc, Cumulative voting N P-complete, even chairman simply wants
include distinguished candidate.
emerges issue worth clarifying. noted above, constructive version (i.e.,
including distinguished candidate) Control Adding/Removing Voters Approval already known N P-hard even single winner elections (Hemaspaandra
et al., 2007b). hand, Hemaspaandra et al. (2007b) show destructive
7. precise, candidates utilities identical C \ W winners; names
winners may change, irrelevant purposes.

169

fiMeir, Procaccia, Rosenschein, & Zohar

version problems (i.e., adding/removing voters excluding distinguished
candidate) P k = 1. surprising results imply longer case
number winners k also parameter.
first may appear follows Lemma 4.17 constructive destructive versions problem equivalent, would seem contradictory. closer
examination reveals conflict: apply lemma constructive
result k = 1, obtain reduction destructive version, k = |C| 1.
4.2 Controlling Set Candidates
turn problem controlling set candidates. noted beginning
section, problem ill-defined comes Cumulative voting,
following restrict SNTV, Bloc voting, Approval voting.
Another subtle point problem destructive control removing candidates
single winner elections, assumed chairman cannot remove distinguished
candidate p. seem elegant way generalize assumption
multi-winner elections. Hence, discuss control removing candidates
goal exclude distinguished candidate.
Now, recall Bartholdi et al. (1992) show control adding/removing candidates SNTV N P-complete, even single winner elections (in particular, even
chairman wants include single candidate among winners). Hemaspaandra et
al. (2007b) extended result destructive control. Crucially, since results hold
single winner elections, also apply Bloc voting, Bloc voting SNTV
identical k = 1. So, fact, Bartholdi et al. Hemaspaandra et al. together
imply control adding candidates SNTV Bloc N P-complete, even
chairman wants include exclude distinguished candidate, control removing candidates SNTV Bloc N P-complete, even chairman wants
include distinguished candidate (again, discuss exclusion candidate).
Although Approval voting seems complicated SNTV (or even Bloc), surprisingly much easier control tampering set candidates.
Proposition 4.20. Control Adding Candidates Approval P,
utility function.
Interestingly, single winner setting, Approval immune control adding
candidates (Hemaspaandra et al., 2007b), i.e., possible get candidate elected
adding candidates. However, multiple winner model clearly possible
gain utility adding candidates.
Proof Proposition 4.20. actually solve following problem: chairman
add exactly r candidates, way added candidates become winners,
utility least t? Solving problem entails chairman also solve original
problem, simply run algorithm every r r.
First note candidate c C, C fixed number points s[c], regardless
participation candidate. chairman selects subset C , |D| = r,
winners k candidates C highest score. Therefore,
effective add candidates actually winners.
170

fiComplexity Strategic Behavior Multi-Winner Elections

Say indeed W , W set winners. Regardless identity
candidates D, winners C exactly |C| r candidates highest
score C. Since r fixed (without loss generality) r k, utility winners
C also fixed, need optimize D, i.e., select best r candidates
C made winners.
Lemma 4.21. Let C = {c1 , . . . , cm } sorted nonincreasing score, nondecreasing
utility, i.e., s[cj ] s[cj+1 ] j = 1, . . . , 1, s[cj ] = s[cj+1 ] u(cj )
u(cj+1 ). Let W set winners candidates C , |D| = r,
added. W D, one following holds:
1. s[d] > s[ckr+1 ].
2. s[d] = s[ckr+1 ] u(d) < u(ckr+1 ).
Proof. Assume first W . Among k candidates highest score, least r
C. Thus, least r candidates among highest-scoring k candidates C
excluded W ; candidates {ckr+1 , . . . , ck } certainly excluded set
winners. Since candidates lower score excluded first, equality broken
favor candidates lower utility, either c W , s[ckr+1 ] < s[c],
equality holds utility c lower; particular, true W .
Conversely, suppose D, either condition 1 condition 2 holds.
exactly k candidates preferred (by voting rule tie-breaking mechanism)
ckr+1 : {c1 , . . . , ckr }, well candidates D. Thus k winners.
Denote thresh(r) = s[ckr+1 ]. Lemma 4.21, sufficient consider candidates
{c C : s[c] > thresh(r) s[c] = thresh(r) u(c) < u(ckr+1 )}.
Clearly, possible achieve utility accomplished adding
r candidates highest utility set.
Proposition 4.22. Control Removing Candidates Approval P,
utility function.
Proof. actually easier problem problem control adding candidates,
give simpler algorithm: First, sort candidates decreasing score, sorting
candidates tied scores increasing utilities. Next, remove r candidates
lowest utility first k + r candidates. Finally, check total utility
winner set reaches t.
candidates ranked position k + r cannot elected therefore irrelevant
chairmans perspective. additivity utility function, clear
leaving k candidates highest utility maximizes total utility chairman.
Thus, simple algorithm optimal, fails deduce control
impossible.
171

fiMeir, Procaccia, Rosenschein, & Zohar

5. Related Work
Academic interest complexity manipulation voting initiated Bartholdi,
Tovey Tricks (1989) seminal paper. authors suggested computational complexity may prevent manipulation practice, presented voting rule, namely SecondOrder Copeland, hard manipulate. paper examined single-winner elections,
goal manipulator cast vote way makes given candidate
win election.
Bartholdi Orlin (1991) later proved important Single Transferable Vote
(STV) rule N P-hard manipulate. STV one prominent voting rules
literature voting. proceeds rounds; first round, voter votes
candidate ranks first. every subsequent round, candidate least number
votes eliminated, votes voters voted candidate transfered
next surviving candidate ranking.
Conitzer Sandholm (2003) examined voting rules usually easy manipulate
single-winner elections, induced hardness introducing notion preround.
preround, candidates paired; candidates pair compete
other. introduction preround make election N P-hard, #P-hard,
PSPACE-hard, depending whether preround precedes, comes after, interleaved
voting rule, respectively. Elkind Lipmaa (2005a) generalized approach
using Hybrid voting rules, composed several base voting rules.
authors also considered setting entire coalition manipulators.
setting, standard formulation manipulation problem follows:
given set votes cast, set manipulators. addition, votes
weighted, e.g., voter weight k counts k voters voting identically. asked
whether manipulators cast vote way makes specific candidate win
election.
Conitzer, Sandholm, Lang (2007) shown problem N P-hard
variety voting rules. Indeed, setting manipulators must coordinate
strategies, top taking weights account, manipulation made much
complicated. fact, problem complicated hardness results hold even
number candidates constant. Hemaspaandra Hemaspaandra (2007)
generalized last results exactly characterizing scoring rules
manipulation N P-hard. Elkind Lipmaa (2005b) shown use one-way
functions make coalitional manipulation hard.
However, recent papers argued worst-case computational hardness may
sufficient prevent manipulation. Indeed, although hardness implies
problem infinite number hard instances, may still case reasonable real-world distributions instances, problem easy solve.
theme discussed Procaccia Rosenschein (2007b, 2007a, 2008), Conitzer
Sandholm (2006), Erdelyi et al. (2007).
complexity control chairman received somewhat less attention,
nevertheless much already known. Bartholdi, Tovey Trick (1992) studied
complexity seven different types control two voting rules (and single-winner setting): adding voters/candidates, deleting voters/candidates, partitioning voters,
172

fiComplexity Strategic Behavior Multi-Winner Elections

Manipulation
Rule
SNTV
Bloc
Approval
Cumulative

In/Ex candidate

P
P
P
P

Boolean utility

General utility

P
P
P
P

P
P
P
N P-c










Table 1: complexity manipulation multi-winner elections
two types candidate partitioning. authors reached conclusion different
voting rules differ significantly terms resistance control. Hemaspaandra
Hemaspaandra (2007b) extended results destructive control, chairman
wants candidate elected.
Hemaspaandra, Hemaspaandra Rothe (2007a) contributed investigation examining twenty different types control. showed unique winner
setting, voting rule resistant twenty types. However, unclear
whether one natural single-winner voting rules property total resistance manipulation (though voting rules come close (Faliszewski, Hemaspaandra,
Hemaspaandra, & Rothe, 2007)).
Finally, structured setting, voters essentially preferences subsets
candidates, related recent work combinatorial voting (see, e.g., Lang (2007)
references listed there).
Note paper subsumes parts Procaccia, Rosenschein Zohar (2007)
Meir, Procaccia Rosenschein (2008).

6. Discussion
analysis paper focused complexity manipulating controlling four simple voting schemes often considered context multi-winner
elections: SNTV, Bloc, Approval, Cumulative voting. formulation computational problems, assumed manipulator (or chairman) additive
utility function candidates. distinguished three major cases: manipulator wants one candidate included set winners, excluded set
winners; manipulator binary utility function; manipulator general
utility function.
point wish direct readers attention Table 1, summarizes
results regarding manipulation. left (respectively right) implication arrow table
means result cell implied result left (respectively right)
cell. general utility function, manipulation hard Cumulative voting, easy
three. results Cumulative voting, however, carry
manipulator binary utility function. Another interesting result,
appear above, restrict boolean utility functions, scoring
rule easily manipulated. result formally stated proven Appendix A.
173

fiMeir, Procaccia, Rosenschein, & Zohar

Control Adding/Removing Voters
Rule
In/Ex candidate Boolean utility General utility
SNTV
P [2]
P

P
Bloc
N P-c
N P-c
N P-c
Approval
N P-c*
N P-c
N P-c
Cumulative
N P-c
N P-c
N P-c
Control Adding/Removing Candidates
Rule
In/Ex candidate** Boolean utility General utility
SNTV
N P-c [1,2]
N P-c
N P-c
Bloc
N P-c [1,2]
N P-c
N P-c
Approval
P

P

P
Cumulative
Irrelevant
Irrelevant
Irrelevant
Table 2: complexity control adding/removing voters/candidates multi-winner
elections. * result known including candidate (Hemaspaandra et al.,
2007b), even single-winner elections. However, k = 1, destructive control
adding/removing voters Approval tractable, case
k parameter. ** context control removing candidates,
discuss case excluding candidate. [1] Bartholdi et al. (1992).
[2] Hemaspaandra et al. (2007b).

remains open question scoring rules (if any) N P-hard manipulate
general utility functions.
Table 2 summarizes results regarding control. Notice respect control,
results true three types utility functions appear table. results
imply control adding removing voters easy SNTV hard three
rules. Surprisingly, situation turned head comes control adding
removing candidates: Approval voting easy control,
rules hard. short, clear hierarchy resistance control. conclude
one adopt voting rule ad hoc, depending whether control tampering
set voters, set candidates, major concern.
Note types control, partitioning set voters set
candidates (Bartholdi et al., 1992), investigated paper.
Finally, wish connect work ongoing discussion worst-case versus
average-case complexity manipulation control elections. mentioned Section 5, strand recent research argues worst-case hardness strong enough
guarantee resistance strategic behavior, showing manipulation problems
known N P-hard tractable according different average-case notions (Conitzer
& Sandholm, 2006; Procaccia & Rosenschein, 2007a; Zuckerman et al., 2008; Erdelyi et al.,
2007). However, research still highly inconclusive. Therefore, worst-case complexity manipulation control remains important benchmark comparing different
174

fiComplexity Strategic Behavior Multi-Winner Elections

voting rules, still inspires considerable steadily growing body work (Conitzer
et al., 2007; Hemaspaandra et al., 2007a, 2007b; Faliszewski et al., 2007).

Acknowledgments
authors wish thank anonymous JAIR referees helpful comments.
work partially supported grant #898/05 Israel Science Foundation. Ariel
Procaccia supported Adams Fellowship Program Israel Academy Sciences
Humanities.

Appendix A. Manipulating Scoring Rules
Section 3, shown SNTV Bloc voting, scoring rules,
easy manipulate general utility function. next proposition establishes
true scoring rule, boolean-valued utility function.
Proposition A.1. Let P scoring rule defined parameters
~ = h1 , . . . , i.
Manipulation P boolean-valued utility function u : C {0, 1} P.
Proof. Let
~ = h1 , . . . , parameters scoring rule question. Denote
score candidate c C, manipulator cast vote, s[c]. Let J
manipulators preference profile, given by:
J = c j 1 c j 2 . . . cj
Suppose candidate c C ranked place l manipulator, c = cjl . Denote
final score candidate c, according manipulators profile J , by:
sJ [c] = s[c] + l
Finally, denote winner set results manipulators ballot J WJ .
Lemma A.2. Given C C, |C| = k, possible determine polynomial time
exists J s.t. C = WJ .
Proof. Denote C = {c1 . . . ck },
C = C \ C = {c1 , . . . cmk },
C , C sorted nondecreasing score s[c]. Let
J = c1 c2 . . . ck c1 . . . cmk
preference profile ranks players C first, giving points candidates
lower initial score. Candidates C ranked next, rule applies.
intuition would like candidates C high-as-possible,
less balanced, score. Likewise, would like candidates C low-as-possible
balanced score. strategy generalizes algorithm Bartholdi et al. (1989).
175

fiMeir, Procaccia, Rosenschein, & Zohar

claim exists J s.t. C = WJ C = WJ . C = WJ
obviously exists J s.t. C = WJ . Conversely, suppose exists J #
C = WJ # . Without loss generality, holds (by adversarial tie breaking
assumption)8
c C , c C , sJ # [c ] < sJ # [c ].

(5)

argue possible obtain J J # iteratively transposing pairs
candidates, without changing winner set. Indeed, distinguish three cases:
1. j1 , j2 {1, 2, . . . , k} s[cj1 ] > s[cj2 ] , J # holds cj1 cj2 .
Now, transpose rankings cj1 cj2 J # , i.e., consider preference profile
identical J # except places cj1 cj2 switched. Denote
W new set winners.
score cj2 increased, certainly still W . Moreover, new final
(possibly lower) score cj1 is:
s[cj1 ] + j2 s[cj2 ] + j2 = sJ # [cj2 ]
(5) that:
c C , sJ # [c ] < sJ # [cj2 ]
Therefore, cj1 W even transposition. conclude still holds
C = W .
2. j1 , j2 {1, 2, . . . , k} s[cj1 ] > s[cj2 ], J # holds cj1 cj2 .
similar argument holds case.
3. c C , c C J # holds c c . Clearly desirable candidate c rank higher transpose two candidates.
Using three types transpositions, replace couple candidates
step obtain J J # . step remains true C = W , thus
C = WJ .
Lemma A.3. Given C C, C k, possible determine polynomial time
exists J s.t. C WJ .
Proof. Let C C, |C | = k < k. add C k k candidates C
highest score (according s[c]), denote new set size k C . According
Lemma A.2, determine efficiently exists J C = WJ .
argue enough check C . Indeed, assume J

C WJ . Let c C \ WJ exists c WJ s[c ] < s[c]. Now,
transpose, ranking J , candidates c c , clearly c becomes winner c
becomes loser. Therefore, possible make C set winners.
8. Tie breaking works candidates utility 1 (which ones ultimately care about),
favor candidates C utility 0. However, ease exposition, deal
borderline cases here.

176

fiComplexity Strategic Behavior Multi-Winner Elections

complete proof proposition, denote set candidates whose
utility 1. total utility least subset size
included W . Let C candidates highest score s[c] D. similar
arguments before, subset cannot win subset size can.
Lemma A.3 efficiently find whether possible include C W .

References
Bartholdi, J., & Orlin, J. (1991). Single transferable vote resists strategic voting. Social
Choice Welfare, 8, 341354.
Bartholdi, J., Tovey, C. A., & Trick, M. A. (1989). computational difficulty manipulating election. Social Choice Welfare, 6, 227241.
Bartholdi, J., Tovey, C. A., & Trick, M. A. (1992). hard control election.
Mathematical Computer Modelling, 16, 2740.
Brams, S. J., & Fishburn, P. C. (2002). Voting procedures. Arrow, K. J., Sen, A. K., &
Suzumura, K. (Eds.), Handbook Social Choice Welfare, chap. 4. North-Holland.
Conitzer, V., & Sandholm, T. (2003). Universal voting protocol tweaks make manipulation hard. Proceedings International Joint Conference Artificial Intelligence (IJCAI), pp. 781788.
Conitzer, V., & Sandholm, T. (2006). Nonexistence voting rules usually hard
manipulate. Proceedings 21st National Conference Artificial Intelligence
(AAAI), pp. 627634.
Conitzer, V., Sandholm, T., & Lang, J. (2007). elections candidates
hard manipulate?. Journal ACM, 54, 133.
Duggan, J., & Schwartz, T. (2000). Strategic manipulability without resoluteness shared
beliefs: Gibbard-Satterthwaite generalized. Social Choice Welfare, 17 (1), 8593.
Elkind, E., & Lipmaa, H. (2005a). Hybrid voting protocols hardness manipulation.
16th Annual International Symposium Algorithms Computation, Lecture
Notes Computer Science, pp. 206215. Springer-Verlag.
Elkind, E., & Lipmaa, H. (2005b). Small coalitions cannot manipulate voting. International Conference Financial Cryptography, Lecture Notes Computer Science.
Springer-Verlag.
Endriss, U. (2007). Vote manipulation presence multiple sincere ballots. Proceedings 11th Conference Theoretical Aspects Rationality Knowledge
(TARK), pp. 125134.
Ephrati, E., & Rosenschein, J. S. (1997). heuristic technique multiagent planning.
Annals Mathematics Artificial Intelligence, 20, 1367.
Erdelyi, G., Hemaspaandra, L. A., Rothe, J., & Spakowski, H. (2007). approximating optimal weighted lobbying, frequency correctness versus average-case polynomial
time. Tech. rep., arXiv:cs/0703097v1.
177

fiMeir, Procaccia, Rosenschein, & Zohar

Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007). Llull
Copeland voting broadly resist bribery control. Proceedings 22nd National Conference Artificial Intelligence (AAAI), pp. 724730.
Ghosh, S., Mundhe, M., Hernandez, K., & Sen, S. (1999). Voting movies: anatomy
recommender system. Proceedings 3rd Annual Conference Autonomous
Agents (AGENTS), pp. 434435.
Gibbard, A. (1973). Manipulation voting schemes. Econometrica, 41, 587602.
Haynes, T., Sen, S., Arora, N., & Nadella, R. (1997). automated meeting scheduling system utilizes user preferences. Proceedings 1st International Conference
Autonomous Agents (AGENTS), pp. 308315.
Hemaspaandra, E., & Hemaspaandra, L. (2007). Dichotomy voting systems. Journal
Computer System Sciences, 73 (1), 7383.
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007a). Hybrid elections broaden
complexity-theoretic resistance control. Proceedings 20th International
Joint Conference Artificial Intelligence (IJCAI), pp. 13081314.
Hemaspaandra, E., Hemaspaandra, L. A., & Rothe, J. (2007b). Anyone him:
complexity precluding alternative. Artificial Intelligence, 171 (56), 255285.
Kellerer, H., Pferschy, U., & Pisinger, D. (2004). Knapsack Problems. Springer.
Lang, J. (2007). Vote aggregation combinatorial domains structured preferences.
Proceedings 20th International Joint Conference Artificial Intelligence
(AAAI), pp. 13661371.
Meir, R., Procaccia, A. D., & Rosenschein, J. S. (2008). broader picture complexity
strategic behavior multi-winner elections. Proceedings 7th International
Joint Conference Autonomous Agents Multiagent Systems (AAMAS), pp. 991
998.
Procaccia, A. D., Rosenschein, J. S., & Zohar, A. (2007). Multi-winner elections: Complexity manipulation, control winner-determination. Proceedings 20th
International Joint Conference Artificial Intelligence (IJCAI), pp. 14761481.
Procaccia, A. D., & Rosenschein, J. S. (2007a). Average-case tractability manipulation
elections via fraction manipulators. Proceedings 6th International Joint
Conference Autonomous Agents Multiagent Systems (AAMAS), pp. 718720,
Honolulu, Hawaii.
Procaccia, A. D., & Rosenschein, J. S. (2007b). Junta distributions average-case
complexity manipulating elections. Journal Artificial Intelligence Research, 28,
157181.
Satterthwaite, M. (1975). Strategy-proofness Arrows conditions: Existence correspondence theorems voting procedures social welfare functions. Journal
Economic Theory, 10, 187217.
Zuckerman, M., Procaccia, A. D., & Rosenschein, J. S. (2008). Algorithms coalitional
manipulation problem. Proceedings 19th ACM-SIAM Symposium Discrete
Algorithms (SODA), pp. 277286, San Francisco, California.

178

fiJournal Artificial Intelligence Research 33 (2008) 403-432

Submitted 07/08; published 11/08

Computational Complexity Dominance
Consistency CP-Nets
Judy Goldsmith

GOLDSMIT @ CS . UKY. EDU

Department Computer Science
University Kentucky
Lexington, KY 40506-0046, USA

Jrme Lang

LANG @ IRIT. FR

IRIT
Universit de Toulouse, UPS
31062 Toulouse Cedex, France

Miroslaw Truszczynski

MIREK @ CS . UKY. EDU

Department Computer Science
University Kentucky
Lexington, KY 40506-0046, USA

Nic Wilson

N . WILSON @4 C . UCC . IE

Cork Constraint Computation Centre
University College
Cork, Ireland

Abstract
investigate computational complexity testing dominance consistency CP-nets.
Previously, complexity dominance determined restricted classes
dependency graph CP-net acyclic. However, preferences interest define
cyclic dependency graphs; modeled general CP-nets. main results, show
dominance consistency general CP-nets PSPACE-complete.
consider concept strong dominance, dominance equivalence dominance incomparability,
several notions optimality, identify complexity corresponding decision problems. reductions used proofs STRIPS planning, thus reinforce earlier
established connections areas.

1. Introduction
problems eliciting, representing computing preferences multi-attribute domain arise many fields planning, design, group decision making. However,
multi-attribute preference domain, computations may nontrivial, show
CP-net representation. Natural questions arise preference domain are, item preferred one?, set preferences consistent? formally, set preferences
consistent item preferred itself. assume preferences transitive,
i.e., preferred , preferred , preferred .
explicit representation preference ordering elements, also called outcomes,
multi-variable domains exponentially large number attributes. Therefore, AI researchers
developed languages representing preference orderings succinct way. formalism
CP-nets (Boutilier, Brafman, Hoos, & Poole, 1999) among popular ones. CP-net
c
2008
AI Access Foundation. rights reserved.

fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON

provides succinct representation preference ordering outcomes terms local preference
statements form p : xi > x j , xi , x j values variable X p logical condition.
Informally, preference statement p : xi > x j means given p, xi strictly preferred x j ceteris
paribus, is, things equal. meaning CP-net given certain ordering relation, called dominance, set outcomes, derived reading preference
statements. one outcome dominates another, say dominant one preferred.
Reasoning preference ordering (dominance relation) expressed CP-net far
easy. key problems include dominance testing consistency testing. first problem,
given CP-net two outcomes , want decide whether dominates . second
problem asks whether dominance cycle dominance ordering defined input
CP-net, is, whether outcome dominates (is preferred to) itself.
study computational complexity two problems. results obtained prior
work concerned restricted classes CP-nets, requiring graph variable dependencies implied preference statements CP-net acyclic. certain assumptions,
dominance-testing problem NP and, additional assumptions, even P (Domshlak
& Brafman, 2002; Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004a). show complexity general case PSPACE-complete, holds even propositional case,
exhibiting Section 4 PSPACE-hardness proof dominance testing.
turn consistency testing. acyclic CP-nets guaranteed consistent,
case general CP-nets (Domshlak & Brafman, 2002; Brafman & Dimopoulos, 2004).
Section 5, show consistency testing hard dominance testing.
following two sections study decision problems related dominance optimality
CP-nets. First, consider complexity deciding strict dominance, dominance equivalence
dominance incomparability outcomes CP-net. Then, study complexity deciding
optimality outcomes, existence optimal outcomes, several notions optimality.
prove hardness part results, first establish PSPACE-hardness problems related propositional STRIPS planning. show problems reduced
CP-net dominance consistency testing exploiting connections actions STRIPS
planning preference statements CP-nets.
complexity results paper address CP-nets whose dominance relation may contain
cycles. earlier work concentrated acyclic model. However, argued earlier,
instance Domshlak Brafman (2002), acyclic CP-nets sufficiently expressive capture human preferences even simple domains.1 Consider, instance, diner
choose either red white wine, either fish meat. Given red wine, prefer meat,
conversely, given meat prefer red wine. hand, given white wine, prefer fish,
conversely, given fish prefer white wine. gives consistent cyclic CP-net,
acyclic CP-net giving rise preferences outcomes. So, cyclicity preference
variables necessarily lead cyclic order outcomes.

1. mean say cyclic CP-nets sufficient capture possible human preferences simple domains
obviously true. However, note every preference relation extends preference relation induced
CP-net possibly cyclic dependencies. property longer true cyclic dependencies
precluded but, case binary variables, number linear orders extends acyclic CP-net
exponentially smaller number linear orders (Xia, Conitzer, & Lang, 2008).

404

fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS

assume familiarity complexity class PSPACE. refer Papadimitriou
(1994) details. particular, later use identities NPSPACE = PSPACE = coPSPACE.
several places, consider versions decision problem, input instances
assumed additional property. problems usually formulated following
way: Q, given R2 . first note Q, given R problem Q R. Let
us recall definition decision problem presented Ausiello et al. (1999). decision
problem pair P = hIP ,YP IP set strings (formally, subset ,
finite alphabet), decision problem P = hIP ,YP reads follows: given string x IP , decide
whether x YP . problem hIP ,YP complexity class C language YP C (this
depend IP ). problem hIQ ,YQ reducible hIP ,YP polynomial-time
function F (1) every x IQ , F(x) IP , (2) every x IQ , x YQ
F(x) YP . Thus, P decision problem Q, given R, IP set strings
satisfying R, YP set strings satisfying R Q. problems, granted
input belongs R; solve check input string indeed
element R. problems Q, given R widespread literature. However,
cases, R simple property, checked polynomial (and often linear) time,
decide whether graph possesses Hamiltonian cycle, given every vertex degree
3. Here, however, consider several problems Q, given R R
class P (unless polynomial hierarchy collapses). However, said above, complexity
recognizing whether given string R matter. words, complexity Q,
given R same, whether R recognized unit time PSPACE-complete.
come back first problem appears paper (cf. proof Proposition 5).
case consider complexity R greater complexity Q.
part paper (up Section 5) extended version earlier conference publication
(Goldsmith, Lang, Truszczynski, & Wilson, 2005). Sections 6 7 entirely new.

2. Generalized Propositional CP-Nets
Let V = {x1 , . . . , xn } finite set variables. variable x V , assume finite domain
Dx values. outcome n-tuple (d1 , . . . , dn ) Dx1 Dxn .
paper, focus propositional variables: variables binary domains. Let V
finite set propositional variables. every x V , set Dx = {x, x} (thus, overload
notation write x variable one values). refer x x literals.
Given literal l write l denote dual literal l. focus binary variables makes
presentation clearer impact complexity results.
also note case binary domains, often identify outcome set
values (literals). fact, also often identify sets conjunctions elements.
Sets (conjunctions) literals corresponding outcomes consistent complete.
conditional preference rule (sometimes, preference rule rule) V expression p : l > l, l literal atom x V p propositional formula V
involve variable x.
2. literature one often finds following formulation: Q, even R, exactly
meaning Q, given R. Specifically, saying Q NP-complete, even R, one means Q NP-complete,
Q, given R NP-complete well.

405

fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON

rest paper, need refer two different languages: conditional preference
language every (binary) variable x, conditional preference table x needs specify
preferred value x every possible assignment parent variables, general
language tables may incomplete (for values parents, preferred value
x may specified) and/or locally inconsistent (for values parents, table may
contain information x preferred information x preferred). call
languages respectively CP-nets GCP-nets (for generalized CP-nets). Note GCP-nets
new, similar structures discussed (Domshlak, Rossi, Venable, & Walsh,
2003). reason use terminology (CP-nets GCP-nets) twofold. First, even
assumptions completeness local consistency CP-nets sometimes relaxed,
papers CP-nets make them. Second, could used CP-nets locally consistent,
complete CP-nets instead GCP-nets CP-nets, felt notation simpler
transparent.
Definition 1 (Generalized CP-net) generalized CP-net C (for short, GCP-net) V
set conditional preference rules. x V define pC+ (x) pC (x), usually written just:
p+ (x) p (x), follows: pC+ (x) equal disjunction p exists rule
p : x > x C; pC (x) disjunction p exists rule p : x > x C.
define associated directed graph GC (the dependency graph) V consist pairs (y, x)
variables appears either p+ (x) p (x).
complexity results also need following representation GCP-nets: GCPnet C said conjunctive form C contains rules p : l > l p (possibly
empty) conjunction literals. case formulas p (x), p+ (x) disjunctive normal form,
is, disjunction conjunctions literals (including empty conjunction literals).
GCP-nets determine transitive relation outcomes, interpreted terms preference.
preference rule p : l > l represents statement given p holds, l preferred l ceteris
paribus. intended meaning follows. outcome satisfies p l, preferred
outcome differs assigns l variable x. situation say
improving flip sanctioned rule p : l > l.
Definition 2 0 , . . . , sequence outcomes 1 next outcome
sequence obtained previous one improving flip, say 0 , . . . ,
improving sequence 0 GCP-net, dominates 0 , written 0 .
Finally, GCP-net consistent outcome strictly preferred itself,
is, .
main objective paper establish complexity following two problems
concerning notion dominance associated GCP-nets (sometimes restrictions
class input GCP-nets).
Definition 3
GCP - DOMINANCE : given GCP-net C two outcomes , decide whether C,
is, whether dominates C.
GCP - CONSISTENCY : given GCP-net C, decide whether C consistent.
406

fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS

GCP-nets extend notion CP-nets (Boutilier et al., 1999). two properties
GCP-nets essential linking two notions.
Definition 4
GCP-net C V locally consistent every x V , formula pC (x) pC+ (x) unsatisfiable. locally complete every x V , formula pC (x) pC+ (x) tautology.
Informally, local consistency means outcome x preferred
x x preferred x. Local completeness means that, every variable x, every outcome
either x preferred x x preferred x.
Definition 5 (Propositional CP-net) CP-net set V (propositional) variables locally consistent locally complete GCP-net V .
easy decide whether GCP-net actually CP-net. fact, task coNPcomplete.
Proposition 1 problem deciding, given GCP-net C, whether C CP-net coNPcomplete.
Proof: Deciding whether GCP-net C CP-net consists checking local consistency local
completeness. tasks amounts n validity tests (one variable). follows
deciding whether GCP-net CP-net intersection 2n problems coNP. Hence,
coNP, itself. Hardness comes following reduction UNSAT. propositional
formula assign CP-net C(), defined set variables Var(){z}, z 6 Var(),
following tables:

+
variable x 6= z: pC()
(x) = ; pC()
(x) = ;

+
(z) = .
(z) = ; pC()
pC()


+
+
(z) = . There(z) pC()
(x) = ; moreover, pC()
(x) pC()
variable x 6= z, pC()

+
fore, C() locally consistent. Now, variable x 6= z, pC() (x) pC()
(x) = .

+
Moreover, pC() (z) pC() (z) = . Thus, C() locally complete unsatisfiable.
follows C() CP-net unsatisfiable.


Many works CP-nets make use explicit conditional preference tables list every combination values parent variables (variables x depends) exactly once, combination designating either x x preferred.3 Clearly, CP-nets restricted sense
regarded CP-nets sense that, every variable x, satisfy following condition:
y1 , . . . , yk atoms appearing p+ (x) p (x) every complete
consistent conjunction literals {y1 , . . . , yn } appears disjunct exactly one
p+ (x) p (x).
3. exceptions. discussed instance Boutilier et al. (2004a) Section 6 paper.

407

fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON

embedding, concepts dominance consistency introduced GCP-nets
generalize ones considered CP-nets defined Boutilier et al. (2004a).
Problems CP - DOMINANCE CP - CONSISTENCY defined analogously Definition 3.
paper interested complexity dominance consistency problems GCPnets CP-nets. Therefore, matter way nets (especially CP-nets,
GCP-nets alternative proposals) represented important. representation
CP-nets often compact one proposed Boutilier et al. (2004a), formulas
p+ (x) p (x) implied conditional preference tables often given equivalent,
exponentially smaller, disjunctive normal form representations. Thus, defining decision
problem, critical specify way represent input instances, representation may
affect complexity problem. Unless stated otherwise, assume GCP-nets (and thus,
CP-nets) represented set preference rules, described Definition 1. Therefore,
size GCP-net given total size formulas p (x), p+ (x), x V .
note key property consistent GCP-nets, use several times later
paper.
Proposition 2 GCP-net C consistent locally consistent.
Proof: C locally consistent exists variable x outcome satisfying
pC (x) pC+ (x). shown flipping x current value dual value
flipping back: since satisfies pC (x) pC+ (x), since pC (x) pC+ (x) involve
occurrences x, flips allowed.

Finally, conclude section example illustrating notions discussed above.
Example 1 Consider GCP-net C variables V = {x, y} four rules, defined follows:
x : > y; x : > y; : x > x; : x > x. p+ (y) = x, p (y) = x, p+ (x) =
p (x) = y. Therefore C locally consistent locally complete, CP-net.
cycle dominance outcomes: x x x x x y,
C inconsistent. shows consistency strictly stronger property local
consistency.

3. Propositional STRIPS Planning
section derive technical results propositional STRIPS planning form
basis complexity results Sections 4 5. establish complexity plan existence
problems propositional STRIPS planning restrictions input instances make
problem use studies dominance consistency GCP-nets.
Let V finite set variables. state V complete consistent set literals
V , often view conjunction members. state therefore equivalent
outcome, defined CP-nets context.
Definition 6 (Propositional STRIPS planning) propositional STRIPS instance mean
tuple hV, 0 , , ACTi,
1. V finite set propositional variables;
408

fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS

2. 0 state V , called initial state;
3. state called goal;4
4. ACT finite set actions, action ACT described consistent conjunction literals pre(a) (a precondition) consistent conjunction literals post(a) (a
postcondition, effect).5
action executable state |= pre(a). effect state , denoted eff (a, ),
state containing literals variables mentioned post(a),
literals post(a). assume action applied state,
change state preconditions hold: 6|= pre(a) (given states complete,
equivalent |= pre(a)) eff (a, ) = . assumption influence far
complexity results concerned.
PROPOSITIONAL STRIPS PLAN EXISTENCE problem, STRIPS PLAN short, decide whether given propositional STRIPS instance hV, 0 , , ACTi finite sequence
actions leading initial state 0 final state . sequence plan
hV, 0 , , ACTi. plan irreducible every one actions changes state.
assume, without loss generality, action a, literal post(a) appears also
pre(a); otherwise omit literal post(a) without changing effect action;
post(a) becomes empty conjunction, action omitted ACT
effect.
following result due Bylander (1994).
Proposition 3 (Bylander, 1994)

STRIPS PLAN

PSPACE-complete.

Typically, propositional STRIPS instances require goals states. Instead, goals
defined consistent conjunctions literals need complete. setting,
plan sequence actions leads start state state goal holds.
restrict consideration complete goals. restriction effect complexity plan
existence problem: remains PSPACE-complete goal-completeness restriction (Lang,
2004).
3.1 Acyclic STRIPS
Definition 7 (Acyclic sets actions) set actions ACT (we use notation Definition 6) acyclic state hV, , , ACTi non-empty irreducible plan,
say, non-trivial directed cycles graph states induced ACT.
establish complexity following problem:
ACTION - SET ACYCLICITY :

given set ACT actions, decide whether ACT acyclic.

Proposition 4
ACTION - SET ACYCLICITY PSPACE-complete.
4. Note standard STRIPS goal partial state. point discussed Proposition 3.
5. emphasize allow negative literals preconditions goals. definitions STRIPS allow
this. particular variant STRIPS sometimes called PSN (propositional STRIPS negation) literature.

409

fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON

Proof: argument membership PSPACE standard; nevertheless give details.
omit details proofs membership PSPACE. following nondeterministic algorithm decides ACT cycle:
guess 0 ;
:= 0 ;
repeat
guess action ACT ;
:= eff (a, );
:=
= 0 .
algorithm works nondeterministic polynomial space (because need store 0 ,
), shows ACTION - SET ACYCLICITY NPSPACE, therefore PSPACE,
since NPSPACE = PSPACE. Thus, ACTION - SET ACYCLICITY coPSPACE, hence PSPACE,
since coPSPACE = PSPACE.
show complement ACTION - SET ACYCLICITY problem PSPACEhard reducing ACYCLIC STRIPS PLAN problem it.
Let PE = hV, 0 , , ACTi instance ACYCLIC STRIPS PLAN problem. particular,
ACT acyclic. Let new action defined pre(a) = post(a) = 0 . easy
see ACT {a} acyclic exists plan PE. Thus, PSPACEhardness complement ACTION - SET ACYCLICITY problem follows Proposition
5. Consequently, ACTION - SET ACYCLICITY problem coPSPACE-hard. Since PSPACE =
coPSPACE, ACTION - SET ACYCLICITY problem PSPACE-hard, well.

Next, consider STRIPS planning problem restricted instances acyclic sets
actions. Formally, consider following problem:
ACYCLIC STRIPS PLAN : Given propositional STRIPS instance hV, 0 , , ACTi
ACT acyclic 0 6= , decide whether plan hV, 0 , , ACTi

first problems form Q, given R encounter illustrates
well concerns discussed end introduction. Here, R set propositional
STRIPS instances hV, 0 , , ACTi ACT acyclic, Q set instances
plan hV, 0 , , ACTi. Checking whether given propositional STRIPS instance
actually acyclic PSPACE-complete (this Proposition 4 states),
matter comes solving ACYCLIC STRIPS PLAN: considering instance ACYCLIC
STRIPS PLAN , already know acyclic (and reflected reduction below).
Proposition 5
ACYCLIC STRIPS PLAN

PSPACE-complete.

Proof: argument membership PSPACE standard (cf. proof Proposition 4).
prove PSPACE-hardness, first exhibit polynomial-time reduction F STRIPS PLAN. Let
PE = hV, 0 , , ACTi instance STRIPS PLAN. idea behind reduction introduce
counter, time action executed, counter incremented. counter may
count 2n , n = |V |, making use n additional variables. counter initialized
410

fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS

0. reaches 2n 1 longer incremented action executed. Hence,
set actions resulting instance STRIPS PLAN acyclic: guaranteed produce
instance R. describe reduction, write V {x1 , . . . , xn }. define F(PE) = PE =
hV , 0 , , ACT follows:
V = {x1 , . . . , xn , z1 , . . . , zn }, zi new variables use implement counter;
0 = 0 z1 zn ;
= z1 zn ;
action ACT, include ACT n actions ai , 1 n, that:

pre(ai ) = pre(a) zi zi+1 zn
n 1 :
post(ai ) = post(a) zi zi+1 zn ,

pre(an ) = pre(a) zn
= n :
post(an ) = post(a) zn .
Furthermore, include ACT n actions bi , 1 n, that:

pre(bi ) = zi zi+1 zn
n 1 :
post(bi ) = zi zi+1 zn ,

pre(bn ) = zn
= n :
post(bn ) = zn .
denote states V pairs (, k), state V k integer, 0
k 2n 1. view k compact representation state variables z1 , . . . , zn : assuming
binary representation k d1 . . . dn (with dn least significant digit), k represents
state contains zi di = 1 zi , otherwise. instance, let V = {x1 , x2 , x3 }.
V = {x1 , x2 , x3 , z1 , z2 , z3 }, state x1 x2 x3 z1 z2 z3 denoted (x1 x2 x3 , 5).
note effect ai bi state (, k) either void, increments counter:
eff (ai , (, k)) =



(eff (a, ), k + 1) ai executable (, k)
(, k)
otherwise

eff (bi , (, k)) =



(, k + 1) bi executable (, k)
(, k)
otherwise

Next, remark one ai one bi executable given state (, k).
precisely,
k < 2n 1, exactly one bi executable (, k); denote i(k) index bi(k)
executable (, k) (this index depends k). also ai(k) executable
(, k), provided executable .
k = 2n 1, ai bi executable (, k).
411

fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON

show PE acyclic. Assume irreducible plan hV , , , ACT i. Let
= (, k). k < 2n 1, empty, since action ACT state either nonexecutable increments counter, irreducible plan contains actions whose effect
non-void. k = 2n 1, action ACT executable empty. Thus,
exists non-empty irreducible plan hV , , , ACT i, holds . Therefore PE
acyclic.
claim plan PE plan PE . First, assume
plan PE. Let shortest plan PE let length (the number actions
used). 2n 1, since state along repeats (otherwise, shorter plans PE
would exist). Let 0 , 1 , . . . , = sequence states obtained executing . Let
action used transition k k+1 . Since k < 2n 1 (because 2n 1 k 1),
exactly one i, 1 n, action ai applies state (, k) V . Replacing
ai yields plan started (0 , 0) leads (m , m) = (, m). Appending
plan appropriate actions bi increment counter 2n 1 yields plan PE . Conversely,
plan PE , plan obtained removing actions form b j replacing
action ai plan PE, since ai effect V does. Thus, claim
follows.



emphasize reduction F STRIPS PLAN ACYCLIC STRIPS PLAN (or, equivalently, STRIPS PLAN given ACTION - SET ACYCLICITY) works satisfies following
two conditions:
1. every instance PE STRIPS PLAN, F(PE) instance ACYCLIC STRIPS PLAN (this
holds every PE, F(PE) acyclic);
2. every PE STRIPS PLAN, F(PE) positive instance ACYCLIC
PE positive instance STRIPS PLAN.

STRIPS PLAN



3.2 Mapping STRIPS Plans Single-Effect STRIPS Plans
Versions STRIPS PLAN ACYCLIC STRIPS PLAN problems important us allow actions exactly one literal postconditions input propositional STRIPS
instances. call actions single-effect actions.6 refer restricted problems SE
STRIPS PLAN ACYCLIC SE STRIPS PLAN , respectively.
prove PSPACE-hardness problems, describe mapping STRIPS instances
single-effect STRIPS instances.7
Consider instance PE = hV, 0 , , ACTi STRIPS PLAN problem, ACT necessarily acyclic. action ACT introduce new variable xa , whose intuitive meaning
action currently executed.
V
set X = aACT xa . is, X conjunction negative literals additional
V
variables. addition, ACT set Xa = xa bACT{a} xb . define
instance PE = hV , 0 , , S(ACT)i SE STRIPS PLAN problem follows:
6. actions also called unary actions planning literature. stick terminology single-effect
although less commonly used, simply explicit.
7. PSPACE-completeness propositional STRIPS planning single-effect actions proved already Bylander
(1994). However, deal acyclicity need give different reduction one used paper.

412

fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS

Set variables: V = V {xa : ACT};
initial state: 0 = 0 X;
goal state: = X;
set actions: S(ACT) = {ai : ACT, = 1, . . . , 2|post(a)| + 1}.
Let action ACT post(a) = l1 lq , l1 , . . . , lq literals.
= 1, . . . , q, define action ai setting:
pre(ai ) = pre(a) X li ; post(ai ) = xa .
role ai enforce Xa holds ai successfully applied,
way enable starting execution a, provided action currently
executed, ith effect already true, precondition true.
= q + 1, . . . , 2q, define action ai setting:
pre(ai ) = Xa ; post(ai ) = li .
role ai make ith effect true.
Finally, define a2q+1 setting:
pre(a2q+1 ) = Xa l1 lq ; post(a2q+1 ) = xa .
Thus, a2q+1 designed X holds a2q+1 successfully applied; is, a2q+1
closes execution a, thus allowing next action executed.
Let sequence actions ACT. define S() sequence actions S(ACT)
obtained replacing action a1 , . . . , a2q+1 , q = |post(a)|. consider
sequence actions S(ACT). Remove every action ai 6= 2|post(a)| + 1,
replace actions form a2|post(a)|+1 a. denote resulting sequence actions
ACT (). note (S()) = . following properties hold.
Lemma 1 definitions,
(i) plan PE S() plan PE ;
(ii) irreducible plan PE () irreducible plan PE;
(iii) ACT acyclic S(ACT) acyclic.
Proof: (i) Let ACT action, let state let state obtained
applying a. Let V -state obtained applying sequence actions ha1 , . . . , a2q+1
(where q = |post(a)|) state X PE . show = X.
note = 1, . . . , q, state X satisfy pre(ai ) sequence
actions ha1 , . . . , a2q+1 effect, state still X. happen, either doesnt
satisfy pre(a), l1 , . . . , lq already hold post(a) holds . either case, = ,
= X.
413

fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON

Suppose {1, . . . , q}, satisfy pre(ai ). first action
causes xa hence Xa hold. applying actions aq+1 , . . . , a2q , l1 lq holds,
post(a) holds. applying a2q+1 post(a) X hold. variable V changed,
= X, required.
Applying result iteratively implies plan PE S() plan PE .

ai

(ii) Let irreducible plan PE , every action changes state, implies
every action performed state precondition true. show ()
/ = 0,
/ () = 0,
/ too, assertion follows.
plan PE. assume 6= 0.
j

Write first action , ACT, let maximal initial subsequence
consisting actions form ai . must j |post(a)|, since X holds 0 (by
assumption above, action j apply) X inconsistent precondition ai
> |post(a)|. Also, pre(a j ) l j hold 0 so, 0 well. Thus, 0 satisfies pre(a),
applying changes state, since l j holds 0 post(a) |= l j . Let us denote state
resulting applying 0 . noted, 6= 0 ,
Let state resulting applying 0 . goal state X holds .
goal state 6= . Let bi action directly following last action .
definition , 6= b. applying j , Xa holds, either Xa holds X holds. Thus,
Xb hold, 6= b. Since bi changes state, must {1, . . . , |post(b)|}, X holds
case, too.
Hence last action a2q+1 , q = |post(a)|. Since variables V
affected actions ai appear literals post(a) since action a2q+1
executed (otherwise would belong ), follows = X.
Applying reasoning repeatedly, show applying () 0 yields ,
action () changes state, () irreducible plan PE, non-empty
non-empty.
(iii) Suppose ACT acyclic, exists state non-empty irreducible plan
PE = hV, , , ACTi. Then, (i), S() plan PE = hV , X, X, S(ACT )i.
non-empty irreducible, changes state, S() also changes state, hence
reduced non-empty irreducible plan PE . Therefore S(ACT) acyclic.
Conversely, suppose S(ACT) acyclic. exists state non-empty
irreducible plan hV , , , S(ACT)i. first prove X holds state obtained
execution plan.
/
Suppose X holds state, let j first action . note 6= 0.
assumption, X hold either applying j . Therefore q + 1 j 2q,
q = |post(a)|. Since irreducible, j changes state. Thus, l j holds l j holds
state resulting applying j .
assumption, Xa holds applying j . Thus, next action, one,
must also form ai q + 1 2q. Repeating argument implies actions
form ai q + 1 2q. Since set literals post(a) consistent, l j
never reset back l j . Thus, state resulting applying different ,
contradiction.
Thus, X holds state reached execution . Let us consider one state.
written X, state V . cyclically permute generate
non-empty irreducible plan hV , X, X, S(ACT)i. part (ii), ( ) non-empty
414

fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS

irreducible plan hV, , , ACTi. Therefore ACT acyclic.



Proposition 6
SE STRIPS PLAN

ACYCLIC SE STRIPS PLAN PSPACE-complete.

Proof: Again, argument membership PSPACE standard. PSPACE-hardness
ACYCLIC SE STRIPS PLAN shown reduction ACYCLIC STRIPS PLAN . construction shows STRIPS PLAN reducible SE STRIPS PLAN, thus SE STRIPS PLAN
PSPACE-complete.
Let us consider instance PE = hV, 0 , , ACTi ACYCLIC STRIPS PLAN. define PE =

hV , 0 , , S(ACT)i, Lemma 1(iii) instance ACYCLIC SE STRIPS PLAN problem. Lemma 1(i) (ii) exists plan PE exists plan PE .
implies ACYCLIC SE STRIPS PLAN PSPACE-hard.


4. Dominance
goal section prove GCP - DOMINANCE problem PSPACE-complete,
complexity go even restrict class inputs CP-nets.
use results propositional STRIPS planning Section 3 prove general GCP DOMINANCE problem PSPACE-complete. show complexity change
require input GCP-net locally consistent locally complete.
similarities dominance testing CP-nets propositional STRIPS planning
first noted Boutilier et al. (1999). presented reduction, discussed later detail
Boutilier et al. (2004a), dominance problem plan existence problem class
propositional STRIPS planning specifications consisting unary actions (actions single
effects). prove results GCP - DOMINANCE GCP - CONSISTENCY problems constructing reduction direction.
reduction much complex one used Boutilier et al. (1999), due
fact CP-nets impose restrictions STRIPS planning. Firstly, STRIPS planning allows
multiple effects, GCP-nets allow flips x > x x > x change value one
variable; constructed reduction STRIPS planning single-effect STRIPS
planning last section. Secondly, CP-nets impose two restrictions, local consistency
local completeness, natural counterparts context STRIPS planning.
dominance consistency problems consider, membership PSPACE
demonstrated similarly membership proof Proposition 4, namely considering nondeterministic polynomial space algorithms consisting repeatedly guessing appropriate improving flips
making use fact PSPACE = NPSPACE = coPSPACE. Therefore,
provide arguments PSPACE-hardness problems consider.
4.1 Dominance Generalized CP-Nets
prove GCP - DOMINANCE problem PSPACE-complete reduction
problem SE STRIPS PLAN, know PSPACE-complete.
415

fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON

4.1.1 APPING INGLE -E FFECT STRIPS P ROBLEMS
P ROBLEMS



GCP-N ETS OMINANCE

Let hV, 0 , , ACTi instance SE STRIPS PLAN problem. every action ACT
denote la unique literal postcondition a, is, post(a) = la . denote
pre (a) conjunction literals pre(a) different la (we recall convention
adopted earlier, pre (a) contain la ). define ca conditional preference rule
pre (a) : la > la define M(ACT) GCP-net C = {ca : ACT}, conjunctive
form.
sequence states plan corresponds improving sequence 0 , leads
following result.
Lemma 2 notation,
(i) non-empty irreducible plan hV, 0 , , ACTi dominates 0
M(ACT);
(ii) ACT acyclic M(ACT) consistent.
Proof: first note following equivalence. Let action ACT, let
different outcomes (or, STRIPS setting, states). action applied yields
rule ca sanctions improving flip . applied yields
satisfies pre(a) differ literal la , satisfying la satisfying
la . satisfies pre (a) differ literal la , satisfying
la , satisfying la . This, turn, equivalent say rule ca sanctions improving flip
.
Proof (i): Suppose first exists non-empty irreducible plan a1 , . . . , hV, 0 , , ACTi.
Let 0 , 1 , . . . , = corresponding sequence outcomes, and, = 1, . . . , m, action ai , applied state i1 , yields different state . equivalence,
= 1, . . . , m, cai sanctions improving flip i1 , implies 0 , 1 , . . . ,
improving flipping sequence M(ACT), therefore dominates 0 M(ACT).
Conversely, suppose dominates 0 M(ACT), exists improving flipping
sequence 0 , 1 , . . . , = , 1. = 1, . . . , m, let cai element
M(ACT) sanctions improving flip i1 . Then, equivalence,
action ai , applied state i1 yields (which different i1 ), a1 , . . . ,
non-empty irreducible plan hV, 0 , , ACTi.
Proof (ii): ACT acyclic exists state non-empty irreducible
plan hV, , , ACTi. (i) exists outcome dominates
M(ACT), M(ACT) consistent.


Theorem 1 GCP - DOMINANCE problem PSPACE-complete. Moreover, remains
restrictions GCP-net consistent conjunctive form.
Proof: PSPACE-hardness shown reduction ACYCLIC SE STRIPS PLAN (Proposition 6).
Let hV, 0 , , ACTi instance ACYCLIC SE STRIPS PLAN problem. Lemma 2(ii),
M(ACT) consistent GCP-net conjunctive form. Since 0 6= (imposed definition
416

fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS

problem ACYCLIC SE STRIPS PLAN), plan hV, 0 , , ACTi
non-empty irreducible plan hV, 0 , , ACTi, which, Lemma 2(i), dominates
0 C.

Theorem 1 implies PSPACE-completeness dominance general conditional
preference language introduced Wilson (2004b), conditional preference rules written conjunctive form.
4.2 Dominance CP-Nets
section show GCP - DOMINANCE remains PSPACE-complete restriction
locally consistent locally complete GCP-nets, is, CP-nets. refer restriction
GCP - DOMINANCE CP - DOMINANCE .
Consistency GCP-net implies local consistency (Proposition 2). Therefore, reduction proof Theorem 1 (from ACYCLIC SE STRIPS PLAN GCP - DOMINANCE restricted
consistent GCP-nets) also reduction GCP - DOMINANCE restricted locally consistent
GCP-nets. PSPACE-hardness ACYCLIC SE STRIPS PLAN (Proposition 6) implies GCP DOMINANCE restricted locally consistent GCP-nets PSPACE-hard, and, fact, PSPACEcomplete since membership PSPACE easily obtained usual line argumentation.
show PSPACE-hardness CP - DOMINANCE reduction GCP - DOMINANCE
consistent GCP-nets.
4.2.1 APPING L OCALLY C ONSISTENT GCP-N ETS



CP-N ETS

Let C locally consistent GCP-net. Let V = {x1 , . . . , xn } set variables C. define
/ define GCP-net C V ,
V = V {y1 , . . . , yn }, {y1 , . . . , yn } V = 0.
show CP-net. end, every z V define conditional preference rules
q+ (z) : z > z q (z) : z > z included C specifying formulas q+ (z) q (z).
First, variable xi V , set
q+ (xi ) = yi q (xi ) = yi .
Thus, xi depends yi . also note formulas q+ (xi ) q (xi ) satisfy local consistency local completeness requirements.
Next, variable yi , 1 n, define
ei = (x1 y1 ) (xi1 yi1 ) (xi+1 yi+1 ) (xn yn ),
fi+ = ei p+ (xi ) fi = ei p (xi ).
Finally, define
q+ (yi ) = fi+ ( fi xi )

q (yi ) = fi ( fi+ xi ).
Thus, yi depends every variable V itself.
note local consistency C, formulas fi+ fi , 1 n, unsatisfiable.
Consequently, formulas q+ (yi ) q (yi ), 1 n, unsatisfiable. Thus, C locally consistent.
417

fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON

Finally, q+ (yi ) q (yi ) equivalent fi+ xi fi xi , tautology. Thus, C locally
complete hence CP-net V .
Let outcomes {x1 , . . . , xn } {y1 , . . . , yn }, respectively. denote
outcome V obtained concatenating n-tuples . Conversely, every outcome C
written way.
Let outcome V . define outcome {y1 , . . . , yn } obtained
replacing every component form xi yi every component xi yi .
every i, 1 n, |= ei .
Let sequence 0 , . . . , outcomes V . Define L(s) sequence V outcomes: 0 0 , 0 1 , 1 1 , 1 2 , . . . , . Further, let sequence 0 , 1 , . . . , V outcomes 0 = = . Define L (t) sequence obtained projecting
element V iteratively removing elements sequence
predecessor (until two consecutive outcomes different).
Lemma 3 definitions,
(i) improving sequence C L(s) improving sequence C
;
(ii) improving sequence L (t) improving sequence ;
(iii) C consistent C consistent.
Proof: Let e = ni=1 (xi yi ). definitions arranged GCP-net C
CP-net C following properties:
(a) e hold outcome V , every improving flip applicable changes
value variable xi yi xi yi holds flip.
Indeed, let us assume improving flip outcome V .
flip concerns variable xi , xi yi holds . Consequently, xi yi holds .
Thus, let us assume flip concerns variable yi . ei holds then, since e not,
xi yi holds . Thus, xi yi holds . ei hold neither fi+ fi does.
Thus, xi (xi , respectively) holds , yi (yi , respectively) holds . Since flip concerns yi ,
follows xi yi holds .
(b) improving flip changes variable xi .
Indeed, variable xi , since e holds , xi yi holds , too. Thus, improving
flip changes xi .
(c) improving flip C changes variable yi outcome
improving flip GCP-net C outcome changes variable xi . applying
improving flip (changing variable yi ) , exactly one improving flip possible. changes
xi results outcome , outcome V resulting applying
improving flip changing variable xi .
prove (c), let us first assume yi holds observe case xi holds
, too. follows q+ (yi ) holds p+ (xi ) holds . Consequently, changing
yi improving flip C changing xi improving flip C.
argument case yi holds analogous (but involves q (yi ) p (xi )). Thus,
first part (c) follows.
V

418

fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS

Let outcome obtained applying improving flip xi . follows
improving flip changing value yi results outcome . outcome, (a),
improving flip must concern x j j x j j holds flip. Since every j 6= i,
x j j holds , improving flips concern either xi yi . local consistency
C , yi cannot flipped right back. Clearly, changing xi improving flip applied
. discussion, improving flip applicable results outcome
. proves second part (c).
Proof (i): assertion follows iterative application (c).
Proof (ii): Suppose improving sequence 0 , 1 , . . . , V -outcomes 0 =
= . Since e holds 0 , (b) implies first flip changes variable yi , (c)
implies second flip changes variable xi make xi yi hold again. Hence 2 written
. (c) improving flip C outcome changing variable xi , is, leading
. Iterating process shows L (t) improving sequence .
Proof (iii): Suppose C inconsistent. exists outcome improving
sequence C . (i), L(s) improving sequence , proving C
inconsistent.
Conversely, suppose C inconsistent, exists improving sequence C
outcome itself. (a), improving flip applied outcome e hold
increases (by one) number xi yi holds. implies e must hold
outcome t, acyclic. Write outcome . cyclically permute
form improving sequence t2 itself. Part (ii) implies L (t2 ) improving
flipping sequence C itself, showing C inconsistent.


Theorem 2 CP - DOMINANCE PSPACE-complete. holds even restrict CP-nets
consistent.
Proof: use reduction PSPACE-hardness GCP - DOMINANCE problem
GCP-nets restricted consistent (Theorem 1). Let C consistent, hence locally
consistent, GCP-net V , let outcomes V . Consider CP-net C
variables V constructed above. Lemma 3(i) (ii) imply dominates C
dominates C . Moreover, C consistent Lemma 3(iii). Consequently, hardness part
assertion follows.

Note PSPACE-hardness obviously remains require input outcomes different,
reduction Theorem 1 uses pair different outcomes.
Notice huge complexity gap problem deciding whether exists nondominated outcome, NP-complete (Domshlak et al., 2003, 2006).

5. Consistency GCP-Nets
section show
Sections 3 4.

GCP - CONSISTENCY

419

problem PSPACE-complete, using results

fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON

Theorem 3
GCP - CONSISTENCY PSPACE-complete. holds even restriction GCP-nets
conjunctive form.
Proof: PSPACE-hardness shown reduction ACTION - SET ACYCLICITY. apply function Section 3.2 followed Section 4.1. maps instances ACTION - SET
ACYCLICITY instances GCP - CONSISTENCY conjunctive form. Lemma 1(iii) Lemma
2 (ii), instance ACTION - SET ACYCLICITY acyclic corresponding instance
GCP - CONSISTENCY consistent, proving result.

show consistency testing remains PSPACE-complete CP-nets (GCP-nets
locally consistent locally complete).
Theorem 4

CP - CONSISTENCY

PSPACE-complete.

Proof: use reduction GCP - CONSISTENCY restriction GCP-net
conjunctive form. Let C GCP-net conjunctive form. define CP-net C follows. C conjunctive form, local consistency decided polynomial time, amounts
checking consistency conjunction conjunctions literals. C locally consistent
set C predetermined inconsistent locally consistent CP-net, example
Section 2. Otherwise, C locally consistent C take CP-net constructed
Section 4.2. mapping locally consistent GCP-nets CP-nets, described Section 4.2,
preserves consistency (Lemma 3 (iii)). Since local inconsistency implies inconsistency (Proposition 2), GCP-net C consistent CP-net C consistent. Thus,
PSPACE-hardness CP - CONSISTENCY problem follows Theorem 3.


6. Additional Problems Related Dominance GCP-Nets
proved main results consistency dominance GCP-nets, move
additional questions concerning dominance relation. state them, introduce
terminology.
Let outcomes GCP-net C. say dominance-equivalent C,
written C , = , C C . Next, dominance-incomparable C
6= , C C . Finally, strictly dominates C 6C .
Definition 8
define following decision problems:
SELF - DOMINANCE : given GCP-net C outcome , decide whether C , is, whether
dominates C.
STRICT DOMINANCE : given GCP-net C outcomes , decide whether strictly dominates C.
DOMINANCE EQUIVALENCE : given GCP-net C outcomes , decide whether
dominance-equivalent C.
DOMINANCE INCOMPARABILITY : given GCP-net C outcomes , decide whether
dominance-incomparable C.
420

fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS

establishing complexity problems, use polynomial-time reductions
problem GCP - DOMINANCE. Let H GCP-net set variables V = {x1 , . . . , xn },
let outcome. define GCP-net G = 1 (H, ) set variables W = V {y}
setting conditions flips variables xi , = 1, . . . , n, follows:
1. xi :
+
p+
G (xi ) = pH (xi )

p
G (xi ) = pH (xi )
2. xi :
+
p+
G (xi ) = pH (xi )

p
G (xi ) = pH (xi )
3. p+
G (y) =
4. p
G (y) = .
mapping 1 computed polynomial time. Moreover, one check H
locally consistent GCP-net, 1 (H, ) also locally consistent. Finally, H CP-net, 1 (H, )
CP-net, well.
every V -outcome , let + = = y. note every W -outcome
form + . explain structure GCP-net G, point improving
flip G + + improving flip H (thus, G restricted
outcomes form + forms copy GCP-net H). Moreover, improving flip
G agrees exactly one variable xi does.
Finally, improving flip moves outcomes different type transforms
+ , + 6= .
formalize useful properties GCP-net G = 1 (H, ). use notation
introduced above.
Lemma 4 every V -outcome , G + and, 6= , + G + (in words, + dominates
every W -outcome).
Proof: Consider V -outcome 6= . C since, given y, changing literal
form improving flip. definition, also C
G (as 6= ). follows G + + G G + . Thus, assertion
follows.


Lemma 5 arbitrary V -outcome different , following statements equivalent:
1. H ;
2. + G + ;
3. + G + .
421

fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON

Proof: Lemma 4, + G + . Thus, conditions (2) (3) equivalent.
[(1)(2)] Clearly (recall discussion structure G), improving flip
H, improving flip + + G. Thus, improving
sequence H , improving sequence G + + .
[(2)(1)] Let us assume + G + , let us consider improving sequence minimum length
+ + . minimality, internal element sequence + . Thus, internal
element equals either (as improving flip leads + ). Since improving flip
+ requires = , outcomes sequence form + . dropping
outcome sequence, get improving flipping sequence H.
Thus, H .

Lemma 6 Let H consistent let different V -outcomes. Then, + G +
H .
Proof: Suppose exists improving sequence + itself. must outcome
sequence form (otherwise, dropping every outcome yields improving
sequence H, contradicting consistency H). perform improving flip
need hold, implies + appears sequence. Thus, + G + .
Lemma 5, H .
Conversely, let us assume H . Lemma 5, + G + . Lemma 4, + G + .
Thus, + G + .

next construction similar. Let H GCP-net variables V = {x1 , . . . , xn }, let
outcome. define GCP-net F = 2 (H, ) follows. before, set W = V {y}
set variables F. define conditions flips variables xi , = 1, . . . , n,
follows:
+
1. p+
G (xi ) = pH (xi )

2. p
G (xi ) = pH (xi )

3. p+
G (y) =
4. p
G (y) = .
Informally, outcomes form + form F copy H. improving flips
outcomes form . improving flip + and, every 6= ,
+ . particular, F consistent 2 (H, ) consistent, mapping 2 computed
polynomial time also following property.
Lemma 7 Let V -outcome different . following conditions equivalent:
1. H
2. strictly dominates F
3. dominance-incomparable F.
422

fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS

Proof: exists improving sequence first improving flip sequence changes + . Moreover, improving flip + = .
Thus, F H . Since F three conditions equivalent.


Proposition 7 following problems PSPACE-complete: SELF - DOMINANCE, STRICT
INANCE , DOMINANCE EQUIVALENCE , DOMINANCE INCOMPARABILITY .

DOM -

Proof: four problems, membership proven easily problems earlier sections.
PSPACE-hardness proofs, use problem CP - DOMINANCE version
required input CP-net consistent two input outcomes different. problem
PSPACE-hard Theorem 2.
Let H consistent CP-net set V variables, let two different V -outcomes.
Lemma 5, H decided deciding problem DOMINANCE EQUIVALENCE +
+ GCP-net 1 (H, ). Thus, PSPACE-hardness DOMINANCE EQUIVALENCE
follows.
Next, equivalence Lemma 6, + G + H , holds due consistency H,
shows problem SELF - DOMINANCE PSPACE-hard.
Finally, Lemma 7, H decided either deciding problem STRICT DOMI NANCE outcomes 2 (H, ), deciding complement problem DOM INANCE INCOMPARABILITY GCP-net 2 (H, ). follows STRICT DOM INANCE DOMINANCE INCOMPARABILITY (the latter fact coPSPACE=PSPACE)
PSPACE-complete.8


Corollary 1 problems SELF - DOMINANCE DOMINANCE EQUIVALENCE PSPACE-complete restriction CP-nets. problems STRICT DOMINANCE DOMINANCE COMPARABILITY remain PSPACE-complete restriction consistent CP-nets.
Proof: Since proof Proposition 7 H CP-net, claim first two
problems follows remarks mapping 1 preserves property CP-net.
last two problems, observe since H proof Proposition 7 assumed
consistent, F = 2 (H, ) consistent, too. Thus, also locally consistent mapping
F F used proof Theorem 2 applies. particular, F consistent CP-net
following properties (implied Lemma 3):
1. strictly dominates F strictly dominates F
2. dominance-incomparable F dominance-incomparable F .
Since F consistent CP-net, claim last two problems follows, too.



8. STRICT DOMINANCE, result could also obtained simple corollary Theorem 2, since
consistent GCP-nets dominance equivalent strict dominance.

423

fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON

7. Problems Concerning Optimality GCP-Nets
dominance relation C GCP-net C determines certain order relation, gives rise
several notions optimality. introduce study complexity corresponding
decision problems.
first observe dominance equivalence relation indeed equivalence relation (reflexive, symmetric transitive). Thus, partitions set outcomes non-empty equivalence classes, call dominance classes. denote dominance class outcome
GCP-net C []C .
relation C induces set dominance classes strict order relation (a relation
irreflexive transitive). Namely, define []C Cdc []C []C 6= []C (equivalently, 6C )
C . One check definition relation Cdc dominance classes independent
choice representatives classes.
Definition 9 (Non-dominated class, optimality GCP-nets) Let C GCP-net. dominance
class []C non-dominated maximal strict order Cdc (there dominance class
[]C []C Cdc []C ). dominance class dominating every dominance class []C ,
[]C = []C []C Cdc []C .
outcome weakly non-dominated belongs non-dominated class. weakly
non-dominated element dominance class, non-dominated.
outcome dominating belongs dominating class. outcome strongly
dominating dominating non-dominated.
Outcomes weakly non-dominated, non-dominated, dominating strongly dominating
capture notions optimality. context CP-nets, weakly non-dominated nondominated outcomes proposed studied (Brafman & Dimopoulos, 2004).
referred weakly strongly optimal there. Similar notions optimality also studied
earlier problem defining winners partial tournaments (Brandt, Fischer, & Harrenstein,
2007). study complexity problems decide whether given outcome optimal
whether optimal outcomes exist.
First, note following general properties (simple consequences properties finite strict
orders).
Lemma 8 Let C GCP-net.
1. exist non-dominated classes so, weakly non-dominated outcomes.
2. Dominating outcomes nondominated outcomes weakly non-dominated.
3. strongly dominating outcome dominating non-dominated.
4. following conditions equivalent:
(a) C unique non-dominated class;
(b) C dominating outcome;
(c) weakly non-dominated dominating outcomes C coincide.
consistent GCP-nets two different notions optimality remain.
424

fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS

Lemma 9 Let C consistent GCP-net. Then:
1. dominance class singleton, C strict order, C Cdc coincide (modulo
one-to-one onto correspondence 7 []C )
2. weakly non-dominated outcome, non-dominated (weakly non-dominated
non-dominated outcomes coincide)
3. dominating outcome, strongly dominating (strongly dominating dominating
outcomes coincide).
4. Finally, unique (weakly) non-dominated outcome strongly dominating.
Next, observe concepts optimality introduced different. end,
show GCP-nets single non-dominated class singleton, multiple non-dominated
classes, singleton, single non-dominated class singleton,
multiple non-dominated classes, containing one element. also show GCPnet two non-dominated classes, one singleton one consisting several
outcomes.
Example 2 Consider following GCP-net C two binary variables b
: >
: b > b
GCP-net determines strict preorder dominance classes, {ab}
maximal class (in fact, dominance classes singletons). Thus, ab non-dominated
dominating so, strongly dominating.
Example 3 Consider following GCP-net C two binary variables b
b : >
b : >
: b > b
: b > b
GCP-net determines strict preorder, {ab} {ab} two different non-dominated
classes. Thus, ab ab non-dominated dominating outcome.
Example 4 Consider GCP-net variables a, b c, defined follows:
: b > b
: b > b
b : >
b : >
ab : c > c

425

fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON

two dominance classes: Sc = {abc, abc, abc, abc} Sc = {abc, abc, abc, abc}. Every
outcome Sc strictly dominates every outcome Sc , therefore, Sc unique non-dominated
class every outcome Sc dominating. Sc singleton, nondominated outcomes (and so, strongly dominating outcome, either).
Example 5 Let us remove GCP-net Example 4 preference statement ab : c > c.
Sc Sc still two dominance classes, every outcome Sc incomparable
outcome Sc . Thus, Sc Sc non-dominated. Since two non-dominated
classes, dominating outcome. Since class one element,
non-dominated outcomes. outcomes weakly non-dominated, though.
Example 6 Let us modify GCP-net Example 4 changing preference statement b : >
bc : > a. dominance relation GCP-net satisfies following properties: (i)
four outcomes Sc dominate other; (ii) abc abc abc abc; (iii) outcome Sc
dominates abc (and, fortiori, abc). One check five dominance classes: Sc , {abc},
{abc}, {abc} {abc}. Two non-dominated: Sc {abc}. Since two nondominated classes, dominating outcome. hand, {abc} non-dominated
outcome (a unique one).
consider following decision problems corresponding notions optimality
introduced.
Definition 10
given GCP-net C:
WEAKLY NON - DOMINATED OUTCOME : given outcome , decide whether weakly nondominated C
NON - DOMINATED OUTCOME : given outcome , decide whether non-dominated C
DOMINATING OUTCOME : given outcome , decide whether dominating C
STRONGLY DOMINATING OUTCOME : given outcome , decide whether strongly dominating C
EXISTENCE NON - DOMINATED OUTCOME : decide whether C non-dominated outcome
EXISTENCE DOMINATING OUTCOME : decide whether C dominating outcome
EXISTENCE STRONGLY DOMINATING OUTCOME : decide whether C strongly dominating outcome.
hardness proofs, use reductions 1 2 , described
previous section. note following additional useful properties GCP-net G = 1 (H, ).
Lemma 10 arbitrary V -outcome different , following statements equivalent:
1. + G +
2. + weakly non-dominated G
3. + dominating outcome G.
426

fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS

Proof: Since + dominating G (Lemma 4), weakly non-dominated outcomes dominating
outcomes coincide (Lemma 8). follows conditions (1)-(3) equivalent other.

Proposition 8 following problems PSPACE-complete: WEAKLY NON - DOMINATED OUTCOME DOMINATING OUTCOME . result holds also problems restricted CP-nets.
Proof: membership easy prove techniques similar used earlier.
PSPACE-hardness proofs, use reductions CP - DOMINANCE consistent CPnets (in version two input outcomes different). Let H CP-net,
two different V -outcomes. Lemmas 5 10, H decided deciding either
problems WEAKLY NON - DOMINATED OUTCOME DOMINATING OUTCOME GCPnet G = 1 (H, ) outcome + . observed earlier, H CP-net,
G = 1 (H, ). Thus, second part assertion follows.

Next, consider problem STRONGLY DOMINATING OUTCOME. exploit
reduction F = 2 (H, ), discussed previous section. observe following
property F.
Lemma 11 Let H GCP-net F = 2 (H, ). strongly dominating F
dominating H.
Proof: Let us assume dominating H. definition F, follows every
V -outcome 6= , + F + F + . Since + F , dominating F. Since
improving flip leading , strongly dominating.
Conversely, let us assume strongly dominating F let V -outcome different . Let us consider improving sequence + . outcomes sequence
last one, , form + . Moreover, outcome directly preceding
+ . Dropping every outcome segment sequence + + yields
improving sequence H.

following consequence result.
Proposition 9 problem STRONGLY
stricted CP-nets.

DOMINATING OUTCOME

PSPACE-complete, even re-

Proof: Let H CP-net (over set V variables) outcome. Lemma 11, problem DOMINATING OUTCOME decided deciding problem STRONGLY DOMINATING
OUTCOME F = 2 (H, ) . Thus, PSPACE-hardness STRONGLY DOMINATING
OUTCOME follows Proposition 8. membership PSPACE is, cases, standard
omitted.
Since H CP-net, locally consistent so, F locally consistent, too. proof
Corollary 1 use mapping GCP-net F CP-net F defined Section 4.2. Lemma
3, strongly dominating outcome F dominates every outcome form
, strongly dominating outcome F , since F -outcome
dominated outcome form (using rules q+ (xi ) = yi q (xi ) = yi ). Therefore
427

fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON

F decided deciding
F . Thus, second part claim follows.

STRONGLY DOMINATING OUTCOME
NATING OUTCOME

STRONGLY DOMI

problem NON - DOMINATED OUTCOME easier. known P CP-nets (Brafman
& Dimopoulos, 2004). result extends GCP-nets. Indeed, H GCP-net outcome,
non-dominated improving flip applies . latter holds
every variable x H, x (respectively, x) holds , p (x) (respectively, p+ (x))
hold . Since conditions checked polynomial claim holds
following result.
Proposition 10 problem NON - DOMINATED

OUTCOME

GCP-nets P.

Next, consider problems concerning existence optimal outcomes. Let H
GCP-net set variables V = {x1 , . . . , xn }, let two different V -outcomes.
every = 1, 2, . . . , n, define formulas follows. xi , conjunction
literals , except instead xi take xi . Similarly, xi , conjunction
literals , except instead xi take xi . Thus, outcome results
literal corresponding xi flipped dual.
define GCP-net E = 3 (H, , ) taking W = V {y} set variables E
defining flipping conditions follows:
+
1. p+
E (xi ) = (pH (xi ) y) (y )


pE (xi ) = pH (xi )

2. p+
E (y) =
3. p
E (y) = .
GCP-net 3 (H, , ) following properties. outcomes form + (= y)
form copy H. improving flip outcome (= y). Next,
improving flip outcome form . see this, let us assume flip
exists concerns variable, say, xi . follows = . definition flipping conditions,
improving flip involves xi impossible, contradiction. Thus, improving
flip leads originates + .
also every outcome , E . follows fact
every outcome , improving flip. Indeed,
variable xi (i) xi false , (ii) flipping literal xi dual lead
(that is, ). (For even = i, then, , 6= , exists 6=
differ xi , xi satisfies (i) (ii).) Thus, flip variable improving.
improving flips outcomes containing result one variable xi assigned
true, thus status , E follows.
Finally, E + and, every outcome , + E . leads
following property E = 3 (H, , ).
Lemma 12 Let H GCP-net let two different outcomes. H
3 (H, , ) (strongly) dominating outcome.
428

fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS

Proof: (Only if) Based earlier remarks, + E . Moreover, since H ,
+ E + . addition, every different , + E E E + . Thus,
dominating strongly dominating (the latter follows fact improving flips
lead ).
(If) Let us assume dominating (and so, argument applies also strongly
dominating). improving sequence + . Let us consider shortest
sequence. Clearly, + outcome sequence (as pointed out, improving flip outcome form possible). Moreover, definition
3 (H, , ) fact considering shortest sequence + , every outcome
sequence + + form + . dropping outcomes,
get improving sequence .


Proposition 11 problem EXISTENCE DOMINATING OUTCOME problem EXISTENCE
STRONGLY DOMINATING OUTCOME PSPACE-complete, even restricted CP-nets.
Proof: show hardness part only, membership part straightforward. prove hardness notice Lemma 12, given consistent CP-net H two outcomes , H
decided deciding either problems EXISTENCE DOMINATING OUTCOME
EXISTENCE STRONGLY DOMINATING OUTCOME 3 (H, , ). prove second part
assertion, note H consistent, E = 3 (H, , ) consistent, so, mapping
locally consistent GCP nets CP-nets applies. Let us denote result applying mapping E E . Then, using argument proof Proposition 9, E (strongly)
dominating outcome E strongly dominating outcome. Thus, one decide
whether H consistent CP-net H deciding either problems EXISTENCE DOM INATING OUTCOME EXISTENCE STRONGLY DOMINATING OUTCOME E .

also note problem EXISTENCE
standard complexity theory assumptions).

NON - DOMINATED OUTCOME

Proposition 12 problem EXISTENCE NON - DOMINATED

OUTCOME

easier (under

NP-complete.

Proof: note case GCP-nets conjunctive form problem known NP-hard
(Domshlak et al., 2003, 2006). Thus, problem NP-hard GCP-nets. membership
class NP follows Proposition 10.

restrict consistent GCP-nets, situation simplifies. First, recall (Lemma 9)
GCP-net consistent weakly non-dominated non-dominated outcomes coincide,
true dominating strongly dominating outcomes. Moreover, consistent GCP-nets,
non-dominated outcomes exist (and so, corresponding decision problem trivially P). Thus,
consistent GCP-nets consider problems DOMINATING OUTCOME EXISTENCE
DOMINATING OUTCOME .
Proposition 13 problems DOMINATING OUTCOME
COME restricted consistent GCP-nets coNP.
429

EXISTENCE DOMINATING OUT-

fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON

Proof: Using Lemmas 8 9, dominating outcome exists outcome
6= non-dominated. Similarly, dominating outcome consistent GCP-net
least two non-dominated outcomes. Thus, guessing non-deterministically
outcome 6= , verifying non-dominated, non-deterministic polynomial-time
algorithm deciding complement problem DOMINATING OUTCOME. argument
problem similar.

know bounds Proposition 13 tight, is, whether two problems
coNP-complete. conjecture are.

8. Concluding Remarks
shown dominance consistency testing CP-nets PSPACE-complete. Also
several related problems related dominance optimality CP-nets PSPACE-complete, too.
repeated use reductions planning problems confirms importance structural similarity STRIPS planning reasoning CP-nets. suggests welldeveloped field planning algorithms STRIPS representations, especially unary operators
(Brafman & Domshlak, 2003), could useful implementing algorithms dominance
consistency CP-nets.
theorems extend CP-nets non-binary domains, extensions variations
CP-nets, TCP-nets (Brafman & Domshlak, 2002; Brafman, Domshlak, & Shimony, 2006)
allow explicit priority variables others, general language
conditional preferences (Wilson, 2004a, 2004b), conditional preference rules written
conjunctive form.
complexity result dominance also relevant following constrained optimisation
problem: given CP-net constraint satisfaction problem (CSP), find optimal solution (a
solution CSP dominated solution CSP). computationally complex, intuitively complete algorithm involves many dominance checks
definition dominance constraints allows dominance paths go outcomes
violating constraints (Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004b).9 problem
checking whether given solution CSP non-dominated seen PSPACE-complete
reduction CP-dominance uses CSP exactly two solutions.
results reinforce need work finding special classes problems dominance
consistency tested efficiently (Domshlak & Brafman, 2002; Boutilier et al., 2004a),
incomplete methods checking consistency constrained optimisation (Wilson, 2004a,
2006).
Several open problems remain. know complexity deciding whether preference relation induced CP-net complete. know whether dominance consistency
testing remain PSPACE-complete number parents dependency graph bounded
constant. also know whether two problems remain PSPACE-complete
CP-nets conjunctive form (the reduction used prove Theorems 2 4 yields CP-nets
conjunctive form). Two additional open problems listed end Section 7.
9. another possible definition, going outcomes violating constraints allowed (Prestwich,
Rossi, Venable, & Walsh, 2005), dominance testing needed check whether given solution non-dominated.

430

fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS

Acknowledgments
Jrme Langs new address is: LAMSADE, Universit Paris-Dauphine, 75775 Paris Cedex 16,
France. authors grateful reviewers excellent comments, Pierre Marquis
helpful discussions. work supported part NSF Grants ITR-0325063,
IIS-0097278 KSEF-1036-RDE-008, ANR Project ANR05BLAN0384 Preference
Handling Aggregation Combinatorial Domains, Science Foundation Ireland Grants
No. 00/PI.1/C075 05/IN/I886, Enterprise Ireland Ulysses travel grant FR/2006/36.

References
Ausiello, G., Crescenzi, P., Gambosi, G., Kann, V., Marchetti-Spaccamela, A., & Protasi, M. (1999).
Complexity Approximation. Combinatorial optimization problems approximability properties. Springer Verlag.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004a). CP-nets: tool
representing reasoning conditional ceteris paribus statements. Journal Artificial
Intelligence Research, 21, 135191.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004b). Preference-based constrained optimization CP-nets. Computational Intelligence, 20(2), 137157.
Boutilier, C., Brafman, R., Hoos, H., & Poole, D. (1999). Reasoning conditional ceteris paribus
preference statements. Proceedings UAI-99, pp. 7180.
Brafman, R., Domshlak, C., & Shimony, E. (2006). graphical modeling preference
importance. Journal Artificial Intelligence Research, 25, 389424.
Brafman, R., & Dimopoulos, Y. (2004). Extended semantics optimization algorithms CPnetworks. Computational Intelligence, 20(2), 218245.
Brafman, R., & Domshlak, C. (2002). Introducing variable importance trade-offs CP-nets.
Proceedings UAI-02, pp. 6976.
Brafman, R., & Domshlak, C. (2003). Structure complexity planning unary operators.
Journal Artificial Intelligence Research, 18, 315439.
Brandt, F., Fischer, F., & Harrenstein, P. (2007). computational complexity choice sets.
Proceedings TARK-07, pp. 8291.
Bylander, T. (1994). computational complexity propositional STRIPS planning. Artificial
Intelligence, 69(12), 165204.
Domshlak, C., & Brafman, R. (2002). CP-netsreasoning consistency testing. Proceedings
KR-02, pp. 121132.
Domshlak, C., Prestwich, S., Rossi, F., Venable, K., & Walsh, T. (2006). Hard soft constraints
reasoning qualitative conditional preferences. Journal Heuristics, 12(4/5), 263
285.
Domshlak, C., Rossi, F., Venable, K., & Walsh, T. (2003). Reasoning soft constraints
conditional preferences: complexity results approximation techniques. Proceedings
IJCAI-03, pp. 215220.
431

fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON

Goldsmith, J., Lang, J., Truszczynski, M., & Wilson, N. (2005). computational complexity
dominance consistency CP-nets. Proceedings IJCAI-05, pp. 144149.
Lang, J. (2004). Logical preference representation combinatorial vote. Annals Mathematics
Artificial Intelligence, 42(1), 3771.
Papadimitriou, C. (1994). Computational complexity. Addison-Wesley.
Prestwich, S., Rossi, F., Venable, B., & Walsh, T. (2005). Constraint-based preferential optimization.
Proceedings AAAI-05, pp. 461466.
Wilson, N. (2004a). Consistency constrained optimisation conditional preferences.
Proceedings ECAI-04, pp. 888892.
Wilson, N. (2004b). Extending CP-nets stronger conditional preference statements. Proceedings AAAI-04, pp. 735741.
Wilson, N. (2006). efficient upper approximation conditional preference. Proceedings
ECAI-06, pp. 472476.
Xia, L., Conitzer, V., & Lang, J. (2008). Voting multiattribute domains cyclic preferential
dependencies. Proceedings AAAI-08, pp. 202207.

432

fiJournal Artificial Intelligence Research 33 (2008) 551574

Submitted 09/08; published 12/08

Learning Reach Agreement Continuous Ultimatum Game
Steven de Jong
Simon Uyttendaele

STEVEN . DEJONG @ MICC . UNIMAAS . NL

MICC, Maastricht University
P.O. Box 616, 6200 MD Maastricht, Netherlands

Karl Tuyls

K . P. TUYLS @ TUE . NL

Eindhoven University Technology
P.O. Box 513, 5600 MB Eindhoven, Netherlands

Abstract
well-known acting individually rational manner, according principles classical game theory, may lead sub-optimal solutions class problems named social dilemmas.
contrast, humans generally much difficulty social dilemmas, able
balance personal benefit group benefit. agents multi-agent systems regularly confronted social dilemmas, instance tasks resource allocation, agents may
benefit inclusion mechanisms thought facilitate human fairness. Although many
mechanisms already implemented multi-agent systems context, application usually limited rather abstract social dilemmas discrete set available strategies
(usually two). Given many real-world examples social dilemmas actually continuous
nature, extend previous work general dilemmas, agents operate continuous strategy space. social dilemma study well-known Ultimatum Game,
optimal solution achieved agents agree common strategy. investigate whether
scale-free interaction network facilitates agents reach agreement, especially presence
fixed-strategy agents represent desired (e.g. human) outcome. Moreover, study influence rewiring interaction network. agents equipped continuous-action
learning automata play large number random pairwise games order establish common strategy. experiments, may conclude results obtained discrete-strategy
games generalized continuous-strategy games certain extent: scale-free interaction network structure allows agents achieve agreement common strategy, rewiring
interaction network greatly enhances agents ability reach agreement. However, also
becomes clear alternative mechanisms, reputation volunteering, many
subtleties involved convincing beneficial effects continuous case.

1. Introduction
Sharing limited resources others common challenge individuals human societies
well agents multi-agent systems (Chevaleyre et al., 2006). Often, conflict
interest personal benefit group (or social) benefit. conflict prominently
present class problems named social dilemmas, individuals need consider
personal benefit, also effects choices others, failure may
lead sub-optimal solutions. dilemmas, classical game theory, assumes players
agents completely individually rational strategic circumstances, seems limited value
(Gintis, 2001; Maynard-Smith, 1982), individually rational players socially conditioned.
Humans hand generally show remarkable ability address social dilemmas, due
c
2008
AI Access Foundation. rights reserved.

fiD E J ONG , U YTTENDAELE & UYLS

tendency consider concepts fairness addition personal benefit (see, e.g.
Dannenberg et al., 2007; Fehr & Schmidt, 1999; Gintis, 2001; Oosterbeek et al., 2004).
prime example social dilemma modeled well-known Ultimatum Game (Gueth
et al., 1982).1 game, two agents bargain division amount R, obtained
outsider. first agent proposes offer r2 second agent (e.g. receive $4
$10). second agent accepts, agent gets share (i.e. first agent receives R r2 ,
second receives r2 ); however, second agent rejects, agents left nothing.
individually rational first agent would offer smallest amount possible, knowing second
agent choose obtaining amount accepting, nothing rejecting. Thus,
accepting smallest amount possible individually rational response. contrast, human
players game hardly (if ever) offer less 20%, offer occurs,
likely accepted (Bearden, 2001; Oosterbeek et al., 2004). Thus, individually rational player
plays proposer human player probably gain money.
Researchers proposed various mechanisms may responsible emergence
fair strategies human populations playing social dilemmas, well resistance
strategies invasion individually rational strategies (see, e.g. Fehr & Schmidt, 1999; Gintis, 2001; Nowak et al., 2000; Santos et al., 2006a). Often, mechanisms implemented multi-agent systems validation purposes, i.e. agents shown prefer
fair strategies individually rational ones, makes plausible humans actually
affected underlying mechanisms. However, argue multi-agent systems driven fairness mechanisms may used validate mechanisms, also allow agents act
fair way real-world applications. Given agents often face tasks resource sharing
allocation (if explicitly, implicitly, e.g. sharing limited computational resources),
tasks regularly contain elements social dilemmas, important enable agents
act based individual rationality, also based fairness (Chevaleyre et al., 2006).
Unfortunately, existing work usually introduces number abstractions allow resulting multi-agent systems applied realistic problems resource allocation.
prominently, work focused social dilemmas discrete strategy sets (usually limited
two).2 abstraction simplifies dilemmas hand reflect potential realworld nature, since many dilemmas, especially related real-world resource allocation,
continuum strategies (i.e. continuous strategy space) rather discrete set
pure strategies. Moreover, social dilemmas continuous strategy spaces, qualifications
cooperation defection, often used discrete social dilemmas, actually relative: certain strategy may seen cooperative (i.e. desirable) certain context, whereas
may either defective simply naive another one. clear dilemma may far
complicated continuous strategy spaces, agents may need use different way
determining whether behavior desirable.
1. analogy Ultimatum Game social dilemmas, Public Goods Game, shown
full mathematical rigor (Sigmund et al., 2001). De Jong & Tuyls (2008) report preliminary results applying
methodology described paper Public Goods Game.
2. Theoretical work field evolutionary game theory occasionally limited discrete strategy sets. Worth
mentioning work Peters (2000), introduces theoretical extension evolutionary stable
strategy concept (ESS) continuous strategy spaces, i.e. extended stability calculus. extension provides
theoretical solution concept clarify egalitarian outcomes. However, concept shed light
learning agents possibly achieve fair outcomes social dilemmas. work therefore complementary
work, aims mechanisms enabling agents find fair outcomes.

552

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

paper, generalize existing work achieving agreement, cooperation fairness
social dilemmas continuous strategy spaces, aim presenting methodology
allows agents reach satisfactory outcomes dilemmas, well real-world problems
containing elements dilemmas. apply proposed methodology Ultimatum
Game, example social dilemma. game, population agents needs reach agreement (i.e. converge common strategy) order obtain satisfactory payoff. agreement
specifies populations cooperative, desirable strategy. precise nature agreement
may vary.3 population agents, strategy agreed upon sufficient number
agents successful, dictate culture, desirable strategy, population.
wish agents learn human strategy, may introduce number simulated human
agents, i.e. agents play according desirable strategy. learning agents
able imitate strategy, even already reached agreement different, possibly
relatively defective, strategy.
remainder paper structured follows. First, 2, give brief overview
related work paper aims continue. Next, 3, discuss methodology, aimed
establishing agreement large populations learning agents. 4, present experiments
results. 5, outline number alternative approaches proposed dilemmas
discrete strategy sets, fail impress dilemma continuous strategy space. discuss
case. Finally, conclude paper 6.

2. Related Work
work basically builds upon two tracks existing work. give overview
tracks indicate related current work. extensive discussion,
refer previous work (De Jong et al., 2008b).
2.1 Learning Fairness Bargaining
De Jong et al. (2008a) investigated behavior agents playing Ultimatum Game
Nash Bargaining Game continuous action learning automata. games, agents
interacting time. Ultimatum Game, required extension two
players agents, one other, demanded portion reward hand. last
player received left. Homo Egualis utility function, developed Fehr & Schmidt
(1999) Gintis (2001), used represent desired outcome, i.e. required minimal amount
every agent wished obtain. 100 agents able successfully find maintain agreements games. addition, observed solutions agreed upon corresponded
solutions agreed upon humans, reported literature. work, similarly use continuous action learning automata learn agreement Ultimatum Game. However, multi-agent
system, organized network structure, efficiently populated much larger number
agents (e.g. thousands). contrast previous work, agents play pairwise games. Moreover,
3. Note analogy humans, cultural background one primary influences constitutes fair,
cooperative, desirable strategy. Although general tendency deviate pure individual rationality
favor socially-aware strategies, exact implications vary greatly (Henrich et al., 2004; Oosterbeek et al.,
2004; Roth et al., 1991). Ultimatum Game instance, actual amounts offered minimally accepted
vary 10% much 70%, depending various factors, amount bargain (Cameron,
1999; De Jong et al., 2008c; Slonim & Roth, 1998; Sonnegard, 1996), culture (Henrich et al., 2004). Cultural
differences may persist groups agents, also within groups (Axelrod, 1997).

553

fiD E J ONG , U YTTENDAELE & UYLS

use Homo Egualis utility function. Instead, desired, human-inspired outcome
offered Homo Egualis utility function replaced (potentially) including agents
always play according certain fixed strategy (i.e. simulated human players).
2.2 Network Topology
Santos et al. (2006b) investigated impact scale-free networks resulting strategies social
dilemmas. scale-free network used order randomly determine two agents (neighbors) would play together various social dilemmas, Prisoners Dilemma
Snowdrift Game. agents limited two strategies, i.e., cooperate defect,
initially equally probable. observed that, due scale-free network, defectors could
spread entire network games, network structures. authors
identified topology network contributed observed maintained cooperation.
subsequent research, Santos et al. (2006a) introduced rewiring network played many different social dilemmas, two strategies per agent. concluded ease
(measure individuals inertia readjust ties) rewiring increasing rate
cooperators efficiently wipe defectors. contrast work Santos et al. (2006a,b),
used discrete strategy set, work uses continuous strategy space. requires another view
fairness, cooperation agreement, departing traditional view fairness achieved
driving agents (manually labeled) cooperative strategies. social dilemmas
Ultimatum Game, strategy agents may agree leads satisfactory outcomes.

3. Methodology
discussing methodology detail, first outline basic setting. continue
explaining continuous-action learning automata, central methodology. Next,
discuss structure topology networks interaction use. discuss
agent types initial strategies agents. elaborate provide additional
possibility rewiring connections agents. Finally, explain experimental setup.
3.1 Basic Setting
study large group adaptive agents, driven continuous action learning automata, playing
Ultimatum Game pairwise interactions. Pairs chosen according (scale-free) network
interaction. Every agent randomly assigned role proposer responder Ultimatum
Game. Agents start different strategies. good performance, need converge
agreement playing many pairwise games, i.e. need learn common strategy.
agents may fixed strategies; agents represent external strategy adaptive
agents need converge to, instance preference dictated humans. addition
basic setting, study influence adding option agents rewire network
interaction response agent behaved defecting manner.
3.2 Continuous Action Learning Automata
Continuous Action Learning Automata (CALA; Thathachar & Sastry, 2004) learning automata
developed problems continuous action spaces. CALA essentially function optimizers:
every action continuous, one-dimensional action space A, receive feedback
554

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

(x) goal optimize feedback. CALA proven convergence (local) optima,
given feedback function (x) sufficiently smooth. advantage CALA many
reinforcement learning techniques (see, e.g. Sutton & Barto, 1998), necessary
discretize continuous action spaces, actions simply real numbers.
3.2.1 H OW CALA W ORK
Essentially, CALA maintain Gaussian distribution actions pulled. contrast
standard learning automata, CALA require feedback two actions, action corresponding
mean Gaussian distribution, action corresponding sample x, taken
distribution. actions lead feedback () (x), respectively, turn,
feedback used update probability distributions . precisely, update formula
CALA written as:
x
= + (x)()
()
()


2
(x)()
x
= + ()
1 K ( L )
()

(1)

equation, represents learning rate; K represents large constant driving .
variance kept threshold L keep calculations tractable even case convergence.4
implemented using function:
() = max (, L )

(2)

intuition behind update formula quite straightforward (De Jong et al., 2008a). Using
update formula, CALA rather quickly converge (local) optimum. multiple (e.g. n) learning
automata, every automaton receives feedback respect joint actions, respectively ()
(x), = 1 , . . . , n x = x1 , . . . , xn . case, still convergence
(local) optimum (Thathachar & Sastry, 2004).
3.2.2 ODIFICATIONS CALA P URPOSES
outlined above, use CALA enable agents learn sensible proposer responder strategy Ultimatum Game. playing Ultimatum Game, two agents may agree
one two joint actions (i.e. obtain one high one low feedback),
may even disagree (i.e. obtain two low feedbacks). situations need
additional attention, occurrence prevents CALA converging correct solutions.
address situations, propose two domain-specific modifications update formula
CALA (De Jong et al., 2008a).5
First, case joint actions yield feedback 0, CALA unable draw effective
conclusions, even though may tried ineffective strategy thus actually
4. used following settings initial experiments: = 0.02, K = 0.001 L = 107 .
precise settings L decisive influence outcomes, although values may lead slower
convergence. K chosen large, (rather vaguely) implied Thathachar & Sastry (2004), decreases
fast, i.e. usually CALA stop exploring sufficient solution found.
5. Note modifications uncommon literature; see, e.g. work Selten & Stoecker (1986)
learning direction theory. Grosskopf (2003) successfully applied directional learning setting Ultimatum
Game, focusing responder competition (which addressed paper).

555

fiD E J ONG , U YTTENDAELE & UYLS

learn. order counter problem, introduce driving force, allows agents update
strategy even feedback received 0. driving force defined as:

proposers: () = x
iff () = (x) = 0
(3)
responders: () = x
effect modification, call zero-feedback avoidance (ZFA), agent
playing proposer learn offer more, agent playing responder accept
lower expectation. roles, lead probable agreement.
Second, one joint action yields agreement, feedback 0, CALA may adapt
strategies drastically favor first joint action fact, shifts values greater
109 observed (De Jong & Tuyls, 2008; De Jong et al., 2008a). tackle problem,
restrict difference possible two feedbacks CALA receive every
iteration. precisely, empirically set:
fi
fi
fi () (x) fi
fi
fi1
(4)
fi
fi
()
Thus, large difference feedback -action x-action, preserve
direction indicated feedback, prevent automaton jump far direction.
call modification strategy update limitation (SUL).
3.3 Network Interaction
scale-free network (Barabasi & Albert, 1999) network degree distribution follows
power law. precisely, fraction P (k) nodes network k connections
nodes goes large values k P (k) k . value constant typically
range 2 < < 3. Scale-free networks noteworthy many empirically observed networks
appear scale-free, including world wide web, protein networks, citation networks, also
social networks. mechanism preferential attachment proposed explain power law
degree distributions networks. Preferential attachment implies nodes prefer attaching
nodes already large number neighbors, nodes small
number neighbors.
Previous research indicated scale-free networks contribute emergence cooperation (Santos et al., 2006b). wish determine whether phenomenon still occurs
continuous strategy spaces therefore use scale-free topology interaction network, using Barabasi-Albert model. precisely, probability pi newly introduced node
connected existing node degree ki equal to:
ki
pi = P
(5)
j kj
construct network, two first nodes linked other,
nodes introduced sequentially connected one existing nodes, using pi .
way, newly introduced node probably connect heavily linked hub one
connections. simulations, connect every new node one, two three
existing ones (uniform probabilities). yields networks interaction realistic
acyclic ones obtained always connecting new nodes one existing node. example,
network modeling friendship network, avoiding cycles means assuming friends
certain person never friends other.
556

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

3.4 Agent Types Strategies
order study agreement concerning common strategy emerges, need make
agents learn reach common strategy, starting situation absent (i.e.
agents different strategies). Moreover, need study whether common strategy
established example agents, whether robust agents use different,
potentially relatively defective strategy.
3.4.1 WO YPES AGENTS
introduce two types agents, i.e. dynamic strategy (DS) agents fixed strategy (FS) agents.
DS agents learning agents. start certain predefined strategy allowed
adapt strategy constantly, according learning mechanism learning automaton.
Basically, agents similar used earlier work (De Jong et al., 2008a). FS agents
(optional) good examples: model example strategy needs learned
(other) agents system, therefore refuse adapt strategy.
3.4.2 NE WO CALA PER AGENT ?
outlined above, agent needs able perform two different roles Ultimatum Game, i.e. playing proposer well playing responder. words,
agent one two distinct states, state requires learn different strategy. CALA
stateless learners, agent therefore would require two CALA. Nonetheless, remainder
paper, equip every DS agent one CALA, representing agents proposer
strategy well responder strategy.
choice one CALA motivated two observations, i.e. (1) human behavior, (2)
initial experiments. First, human strategies often consistent, implying generally
accept offers, reject offers lower (Oosterbeek et al., 2004), even high
amounts stake (De Jong et al., 2008c; Sonnegard, 1996). Second, set initial experiments,
observed agents using two CALA generally converge one single strategy anyway.
illustration, three learning curves obtained fully connected network three agents playing
Ultimatum Game displayed Figure 1. clearly visible agents proposer strategies
(bold lines) strongly attracted agents responder strategies (thin lines), especially
lowest responder strategies. presence FS agent offers 4.5 accepts
least 1, first strategy immediately ignored favor (lower) second one. DS
agents, strategies attracted lowest responder strategy present.6
future work, study observation perspective evolutionary game theory
replicator equations (Gintis, 2001; Maynard-Smith & Price, 1973). current paper,
use observation justify abstraction, i.e. limit complexity agents equipping
one CALA. CALA represents agents proposer strategy well
responder strategy. updated agent plays proposer well plays
responder, according CALA update formula presented 3.2.1 modifications
presented 3.2.2. Thus, agents single CALA receive twice much feedback two separate
CALA would. abstraction therefore increases efficiency learning process.
6. agents quickly adapt strategies downward upward (Figure 1). Therefore, multiple (e.g. 10)
DS agents learning (i.e. without FS agents), strategy usually converges 0. due artifact
learning process; two CALA trying learn others current strategy tend driven downward.

557

fi5

5

4.5

4.5

4

4

3.5

3.5

3

Strategy

Strategy

E J ONG , U YTTENDAELE & UYLS

2.5
2
1.5

2
1.5

1

1

0.5

0.5

0

0
500 1000 1500 2000 2500 3000 3500 4000 4500
Iteration

500 1000 1500 2000 2500 3000 3500 4000 4500
Iteration

10

(top-left) Two DS agents, one starting offering accepting 4.5, one starting offering accepting 0.01, learn
play optimally FS agent offering
4.5 accepting 1. DS agents rather quickly learn
offer accept 1.

9
8
7
Strategy

3
2.5

6

(top-right) Two DS agents, starting offering 4.5
accepting 1, learn play optimally
FS agent also offering 4.5 accepting 1. DS
agents learn offer accept 1.

5
4
3

(bottom-left) Three DS agents, starting different initial
strategies (i.e. offering 9, 4.5, 1, accepting 3, 2
1, respectively), quickly learn single, similar strategy.

2
1
0
500 1000 1500 2000 2500 3000 3500 4000 4500
Iteration

Figure 1: Evolving strategies fully connected network three agents. Proposal strategies
indicated bold line, response strategies indicated thin line. Agents
converge situation two initial strategies become similar.

3.4.3 AGENTS TRATEGIES
simulations, use two types DS agents one type FS agents. precisely,
DSr agents learning agents start rational solution offering X N (0.01, 1) (and
also accepting amount more). DSh agents start human, fair solution, i.e.
offering X N (4.5, 1) (and also accepting amount more). Since FS agents
examples desired solution, equip fair, human-inspired solution see whether
agents able adapt solution. FS agents always offer 4.5, accept
offer 4.5 more. agents limited strategies taken continuous interval
c = [0, 10], 10 chosen upper bound (instead common 1)
common amount money needs shared Ultimatum Game. agents strategy
falls outside interval c, round strategy nearest value within interval.
3.5 Rewiring
Agents play together based connections interaction network. Thus, order avoid
playing certain undesirable neighbor j, agent may decide break connection
558

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

j create new link random neighbor j (Santos et al., 2006a).7 rewiring,
use heuristic proposed Santos et al.: agents want disconnect (relative)
defectors, prefer play relative cooperators. Thus, probability agent unwires
agent j, calculated as:
si sj
pr =
(6)
C
Here, si sj agents current strategies (more precisely, agent responder strategy
agent js proposer strategy), C amount stake Ultimatum Game, i.e. 10. Even
agents determine want unwire probability, may still allowed
to, breaks last link one them. unwiring takes place, agent creates new wire
random neighbor agent j.
3.6 Experimental Setup
Using aforementioned types agents, need determine whether proposed methodology
possesses traits would like see. population said established
successful agreement manages reach common strategy incorporates preferences
good examples, time discouraging agents try exploit dominant
strategy. Thus, population consisting DS agents, strategy shared (or
all) agents leads good performance, since agents agree games, yielding average payoff
5 per game per agent architecture able find common strategy.
using DS well FS agents, FS agents dictate strategy DS agents converge
to, regardless whether start DSh DSr agents.
order measure whether agents achieved satisfactory outcome, study four quantities
related learning process final outcome, viz. (1) point convergence, (2)
learned strategy, (3) performance (4) resulting network structure. briefly explain
four quantities below. general, remark every simulation lasts 3, 000 iterations
per agent, i.e. 3, 000n iterations n agents. repeat every simulation 50 times obtain reliable
estimates quantities interest.
3.6.1 P OINT C ONVERGENCE
important quantity concerning agents learning process point convergence,
which, present, tells us many games agents needed play order establish agreement. determine point convergence, calculate save average population strategy
avg(t) pairwise game (i.e. iteration learning process). iterations,
obtain ordered set averages, i.e. {avg(1), . . . , avg(T )}. Initially, average population
strategy changes time, agents learning. certain point time t, agents stop
learning, result, average population strategy avg(t) change much anymore.
estimate point t, i.e. point convergence, find lowest standard
deviation subset {avg(t), . . . , avg(T )} 103 . Subsequently, report number
games per agent played iteration t, i.e. nt . experiments, every simulation repeated
7. Note may also choose allow agent create new connection specific agents instead
random neighbors neighbor j. However, especially combination reputation (see 5.1), allows
(relative) defectors quickly identify (relative) cooperators, may connect
attempt exploit. Preliminary experiments shown behavior leads interaction network losing
scale-freeness, may seriously impair emergence agreement.

559

fiD E J ONG , U YTTENDAELE & UYLS

Avg
Std
Conv

0

0

00
45

0
00

00
40

35

00

0

0
30

0

00
25

0
00

00
20

0
15

00

00
50

Iteration

10

Strategy

10
9
8
7
6
5
4
3
2
1
0
0

0
00

0

Avg
Std
Conv

Population strategy

45

0
00

00
40

35

00

0

0
30

0

00
25

0
00

00
20

00

00

0
15

10

50

0

Strategy

Population strategy
10
9
8
7
6
5
4
3
2
1
0

Iteration

Figure 2: Two examples convergence point single run. graphs, display
average strategy population (bold line) well standard deviation
average (thin line). dotted vertical line denotes convergence point, found
analysis detailed text.

50 times, resulting 50 convergence points. use box plot visualize distribution
50 convergence points.8
example, Figure 2 (left), see 17 FS agents, 17 DSh agents 16 DSr agents
converge agreement, using rewiring. first 50, 000 games shown. addition
bold line denoting average population strategy, also plot thinner line, denoting standard
deviation average. Using method outlined above, point convergence determined
around 27, 500 games, i.e. approximately 550 games per agent necessary. Figure
2 (right), show similar results 10 FS agents 40 DSr agents, using rewiring.
Here, point convergence around 34, 000 games, i.e. approximately 680 games per agent
necessary, means learning reach agreement difficult.
3.6.2 L EARNED TRATEGY
established iteration agents converged, state average
learned strategy precisely avg(t). repeat every simulation 50 times obtain reliable estimate average. again, results, use box plot visualize distribution
average learned strategy.
3.6.3 P ERFORMANCE
measure performance, first allow agents learn playing 3, 000 Ultimatum Games
each. Then, fix strategies DS agents. let every agent play proposer
neighbors (one one), count number games successful.9 divide
8. box plots, report average instead median, average informative quantity, e.g.
comparing results existing work. may result box plots mid point located outside box.
9. Note CALA update formula prevents agents converging exact strategy, standard deviation
CALAs Gaussian kept artificially strictly positive. Therefore, noise strategies agents
converged to. counter noise measuring performance, set responders strategies 99%
actual strategies. Thus, agent strategy 4 propose 4 accept offer 3.96 more.

560

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

number total number games played (i.e. twice number edges interaction
network). resulting number denotes performance, lies 0 (for utterly catastrophic) 1 (for complete agreement). Human players Ultimatum Game typically achieve
performance 0.80.9 (Fehr & Schmidt, 1999; Oosterbeek et al., 2004). again, 50
repetitions lead 50 measures performance, displayed box plot results.
3.6.4 R ESULTING N ETWORK TRUCTURE
Since network interaction may rewired agents satisfied neighbors, interested network structure resulting agents learning processes.
examine network structure looking degree distribution nodes network (i.e.
number neighbors agents). 50 repeated simulations, may draw single box
plot expressing average degree distribution.

4. Experiments Results
present experiments results two subsections. First, study setup without rewiring
setup rewiring, varying number agents, keeping proportion DSh, DSr
FS agents constant equal (i.e. 33% type agent). Second, study
two setups various population sizes, time varying proportion FS agents,
remainder population half DSh half DSr. general, remark every experiment
reports results averaged 50 simulations. every simulation, allow agents
play 3, 000n random games, n denotes number agents (i.e. population size).
4.1 Varying Population Size
many multi-agent systems, increasing number agents (i.e. population size) causes
difficulties. Many mechanisms work relatively low number agents stop working
well high number agents, instance due computational complexity undesired
emergent properties. According previous research, issue scalability also applies
task learning social dilemmas. Indeed, previous research using evolutionary algorithms games
discrete strategy sets mentions number games needed converge agreement
(i.e. cooperation) may prohibitively large (Santos et al., 2006a).10
Since agents learning continuous strategy spaces, may expect scalability issue
well. determine whether proposed methodology issue, vary population
size 10 10, 000 (with steps between), keeping proportion FS,
DSh DSr agents constant one-third each. study setup without rewiring well
setup rewiring, determine (1) point convergence, i.e. number games per agent
needed reach convergence; (2) average learned strategy agents converged to; (3) final
performance system; finally (4) resulting network structure. Especially first
third quantities give indication scalability methodology.
10. order limit time taken learning, Santos et al. (2006a) terminate learning process 108 iterations,
using 103 agents, leading average (more than) 105 games per agent available. Still,
high number games per agent, report agents occasionally converge.

561

fiD E J ONG , U YTTENDAELE & UYLS

W ITH REWIRING
3000

2500

2500
Games per agent

Games per agent

N REWIRING
3000

2000
1500
1000

2000
1500
1000
500

500

0

0
10

50

100 200 500
Number agents

10

1000 10000

50

100 200 500 1000 10000
Number agents

Figure 3: Games per agent convergence; without rewiring (left) rewiring (right).

4.1.1 P OINT C ONVERGENCE
setup without rewiring (Figure 3, left) tends require games per agent total number
agents increases. certain point, i.e. around population size 200 agents, tendency
stops, mainly average number games per agent approaches maximum, i.e. 3, 000
games per agent. setup rewiring (same figure, right) convincingly outperforms one without
rewiring, increasing population size hardly affects number games per agent required
reach convergence. Independent population size, setup requires approximately 500
games per agent converge. Note difference previous research (i.e. Santos et al., 2006a),
reports requiring 105 games per agent (or more).
4.1.2 L EARNED TRATEGY
setup without rewiring (Figure 4, left) average converges strategy offering well
accepting around 3, 4.5 would required, 33% FS agents present population
play strategy (i.e. 66% DS agents average strategy 2). increasing
population size, average strategy affected; however, becomes
certainty established. again, setup rewiring (same figure, right) shows convincingly
better results. Independent population size, learning agents converge desired
strategy, i.e. 4.5.
4.1.3 P ERFORMANCE
setup without rewiring (Figure 5, left), already saw average learned strategy
DS agents good. Performance seriously affected; around 60%, indicates
DS agents ever agree FS agents. However, average performance influenced
population size. learned strategy, performance around 60% becomes
certainly established. expected, setup rewiring (same figure, right) shows much
satisfying results, i.e. generally 80% agreement. results actually positively affected
population size, average performance increases increasing population.
562

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

N REWIRING

W ITH REWIRING
5
Average converged strategy

Average converged strategy

5
4
3
2
1

4
3
2
1
0

0
10

50

100
200
500
Number agents

1000

10

10000

50

100 200 500
Number agents

1000 10000

Figure 4: Average learned strategy; without rewiring (left) rewiring (right).

W ITH REWIRING

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Performance convergence

Performance convergence

N REWIRING

10

50

100
200
500
Number agents

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10

1000 10000

50

100 200 500
Number agents

1000 10000

Figure 5: Final performance; without rewiring (left) rewiring (right).

4.1.4 R ESULTING N ETWORK TRUCTURE
look network structure resulting learning reach agreement, determine whether
structure influenced population size. Obviously, setup without rewiring (Figure 6,
left) display influence here, network static. setup rewiring (same
figure, right) shows interesting tendency. average degree resulting network stays low,
maximum degree increases increasing population size. Clearly, population
size increases, hubs scale-free network receive preferential attachment,
correspondingly, less densely connected nodes become even less densely connected.
examine number times agents actually rewire, find number generally lies
1, 000, i.e. low percentage total number games played actually made agents
rewire random neighbor undesired proposer.
563

fiD E J ONG , U YTTENDAELE & UYLS

W ITH REWIRING
150

120

120

Degree distribution

Degree distribution

N REWIRING
150

90
60
30

90
60
30
0

0
10

50

100 200 500
Number agents

10

1000 10000

50

100 200 500
Number agents

1000 10000

Figure 6: Resulting network structure; without rewiring (left) rewiring (right).
4.1.5 N C ONCLUSION
conclusion subsection, may state proposed methodology suffering
severe scalability issues. setup include rewiring clearly outperformed one
include rewiring, neither setup without rewiring, setup rewiring, suffer
severely increasing number agents.
4.2 Varying Proportion Good Examples (FS Agents)
section, investigate behavior proposed methodology proportion
good examples population (i.e. FS agents strategy 4.5) varied. remainder
population consists DSh DSr agents equal proportions. experimented
number population sizes, ranging 50 500.
Since results population size rather similar, restrict graphically
reporting analyzing results experiments 100 agents remainder
section. selection remaining results given Table 1. Specifically, setup without rewiring setup rewiring, report population size (Pop), percentage FS
agents used (%FS), average number games per agent needed converge (Games), average learned strategy (Strat), average performance (Perf), finally, maximum number
connections single agent agents network (Netw). discuss
below, results reported Table 1 population sizes 100 highly similar
population size 100 agents.
4.2.1 P OINT C ONVERGENCE
setup without rewiring (Figure 7, left) requires games per agent converge,
proportion FS agents reaches around 30%. Then, required number games decreases
again, although great deal uncertainty. Introducing rewiring (same figure, right) yields
much better results. number games required per agent hardly exceeds 700, number
decreases steadily increasing proportion population FS agent.
564

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

N REWIRING
Pop

% FS

50

W ITH REWIRING

Games

Strat

Perf

Netw

Pop

% FS

0
30
50
80

663.80
2,588.50
1,800.02
259.86

0.01
2.87
4.12
4.47

0.63
0.59
0.70
0.87

15
15
16
15

50

0
30
50
80

200

0
30
50
80

671.30
2,796.85
1,354.80
288.35

0.01
2.64
4.17
4.47

0.63
0.57
0.70
0.88

18
17
18
18

200

500

0
30
50
80

662.50
2,793.55
1,237.75
264.60

0.01
2.85
4.18
4.47

0.64
0.59
0.69
0.89

20
20
21
21

500

Games

Strat

Perf

Netw

639.38
528.52
485.60
356.34

0.01
4.45
4.47
4.49

0.63
0.81
0.89
0.96

22
38
29
23

0
30
50
80

743.00
540.40
493.40
382.20

0.01
4.45
4.47
4.49

0.62
0.87
0.91
0.97

20
52
28
24

0
30
50
80

650.20
549.95
498.00
380.91

0.01
4.45
4.47
4.49

0.65
0.87
0.92
0.97

60
100
55
35

Table 1: Summary results experiments proportion FS agents varied.
details additional results, see main text.
W ITH REWIRING

3000

3000

2500

2500
Games per agent

Games per agent

N REWIRING

2000
1500
1000

2000
1500
1000

500

500

0

0
0

0 10 20 30 40 50 60 70 80 90 100
Percentage FS agents

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

Figure 7: Games per agent convergence; without rewiring (left) rewiring (right).

4.2.2 L EARNED TRATEGY
Interestingly, population consisting DS agents tends converge offering accepting
lowest amount possible, setup use rewiring (Figure 8, left), well
setup (same figure, right). explained 3, DS agents tend adapt
strategies downward easily upward. Thus, two DS agents approximately
strategy, may slowly pull others strategy downward. many DS agents,
probability happens increases. Adding FS agents population results different
behavior two setups. setup without rewiring difficulties moving away lowest
amount possible; sufficient number FS agents (i.e. 30% population)
average learned strategy reflect DS agents move towards strategy dictated FS
agents. rewiring, results convincingly better; even 10% FS agents, DS agents
average converge towards offering accepting amount dictated agents, i.e. 4.5.
565

fiD E J ONG , U YTTENDAELE & UYLS

N REWIRING

W ITH REWIRING
5
Average converged strategy

Average converged strategy

5
4
3
2
1

4
3
2
1
0

0
0

10

20

30 40 50 60 70
Percentage FS agents

80

0

90 100

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

Figure 8: Average learned strategy; without rewiring (left) rewiring (right).
W ITH REWIRING

Performance convergence

Performance convergence

N REWIRING
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

Figure 9: Final performance; without rewiring (left) rewiring (right).
4.2.3 P ERFORMANCE
observations concerning learned strategy, reported above, reflected performance
collective agents. setup without rewiring (Figure 9, left), performance decreases
initially increasing proportion FS agents, DS agents refuse adapt dictated
strategy. proportion FS agents becomes large enough, DS agents start picking
strategy, resulting increasing performance. setup rewiring (same figure, right)
better, performance increases increasing number FS agents. Even though average
learned strategy close 4.5 every proportion FS agents, low proportions FS agents
still display less performance higher proportions. may require additional explanation.
Note box plot Figure 8 shows distribution average strategy 50 repeated
simulations; i.e. show strategy distribution within single simulation.
Thus, even though average strategy single simulation always close 4.5,
still variance. low number FS agents, variance prominently caused inertia,
566

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

W ITH REWIRING

Degree distribution

Degree distribution

N REWIRING
100
90
80
70
60
50
40
30
20
10
0
0

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

100
90
80
70
60
50
40
30
20
10
0
0

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

Figure 10: Resulting network structure; without rewiring (left) rewiring (right).
i.e. DS agents directly connected FS agent, implies need learn
desired strategy neighboring agents also learning. Especially rewiring,
may result two agents playing together compatible neighbors,
(yet) other.
4.2.4 R ESULTING N ETWORK TRUCTURE
Clearly, network structure setup without rewiring (Figure 10, left) influenced
varying proportion FS agents. rewiring used (same figure, right), observe
interesting phenomenon, closely related observations 4.1. again, number
times agents actually rewire generally lies 1, 000. Even though low number,
affect network structure useful way. low proportion FS agents, large
tendency increased preferential attachment. 10% FS agents instance, single
agent connects 70 100 agents. increasing proportion FS agents,
maximum degree network decreases, finally, closely resembles original network.
Clearly, presence examples desired strategy, DS agents attempt connect
agents provide examples. interesting useful emergent behavior.
4.2.5 N C ONCLUSION
compare results obtained population 100 agents results
population sizes, reported Table 1, see highly similar. conclusion
subsection, may state setup using rewiring severe difficulties converging
desired example proportion FS agents providing example low. for, e.g. half
population consisting examples, half learn desired behavior. setup
using rewiring absolutely problems converging desired strategy, even
low proportion FS agents. cases, completely omitting examples leads agents
converging individually rational solution. caused artifact learning method
used, i.e. mentioned before, two CALA trying learn others strategy tend driven
downward lowest value allowed.
567

fiD E J ONG , U YTTENDAELE & UYLS

5. Discussion
results presented previous section suggest mechanisms lead cooperative solutions social dilemmas discrete set strategies (e.g. scale-free networks rewiring),
also lead agreement social dilemmas continuous strategy space. section, however, show trivial issue. precisely, discuss number mechanisms
enhance agents abilities reach cooperation social dilemmas discrete strategy sets,
directly enhance agents abilities reach agreement continuous strategy spaces.
empirically analyze case.
5.1 Reputation
Reputation one main concepts used behavioral economics explain fairness
emerges (e.g. Bowles et al., 1997; Fehr, 2004). Basically, assumed interactions
people lead expectations concerning future interactions. expectations may positive
negative may kept oneself, actually shared peers.
work closely related work, Nowak et al. (2000) show reputation deters agents
accepting low offers Ultimatum Game, information spread, leading
agents also receiving low offers return. Then, agents refuse accept low offers,
provide high offers. Thus, Nowak et al. argue population goes toward providing
accepting high offers. However, note shared strategy (i.e. agreement)
Ultimatum Game yields expected payoff 50% amount stake agents. Thus,
reputation may indeed help agents decide strategy play others, preference
playing cooperatively (i.e. providing high offers) directly result reputation.
5.1.1 PREADING R EPUTATION
study effects reputation optionally adding second network system.
interaction network, consider reputation network scale-free. contrast
interaction network however, reputation network assumed static, agents truthful
concerning reputation, making unnecessary agents consider rewiring. Note two agents
sharing reputation information may may connected well interaction network,
consequence, two agents playing Ultimatum Game may may share reputation
information other. effect, every Ultimatum Game, responding agent may
broadcast reputation information neighbors reputation network. information sent
responder concerns offer done proposer; information
guaranteed correct. Agents receive information probability:
pij = 1


H

(7)

Here, distance sender (potential) receiver j reputation network.
Thus, reputation information may travel H hops, decreasing probability per hop.
simulations, set H = 5. note relatively small networks, implies
reputation information essentially public.
Note reputation information may helpful allow agents something
information. work Nowak et al. (2000), instance, reputation others used
agents determine offer others. Given (1) observation reputation, used
568

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

way, necessarily promote cooperative strategies (see above), (2) fact
already use CALA determine agents offer other, want reputation affect
something else agents strategies. discuss number ways agents may use
reputation, taken literature, i.e. interacting preferred neighbor (below) using
reputation facilitate voluntary participation (5.3).
5.1.2 U SING R EPUTATION
Without reputation, agents play random neighbor interaction network. Reputation
may used make agents prefer interacting specific neighbors Chiang (2008) discusses
strategies fairness could evolve dominant agents allowed choose preferred partners
play against. Chiang allows agents select partners helped agent previously.
determine preferred partner, use heuristic proposed Santos et al. (2006a),
i.e. agent prefers playing (relative) cooperators, help obtaining high payoff
responder. Thus, probability agent plays agent j Ni , Ni set
agent neighbors, is:
sj si
pij = P
(8)
kNi sk
Here, si , sj sk agents current strategies (for agents i, estimates
based reputation previous experience).
two problems approach. First, number times agent receives
information agent j Ni may rather low, especially many agents. Even
50 agents, observe around 25% reputation information received agents actually concerned one neighbors. problem may addressed making reputation
network identical interaction network (as neighbor relations networks identical). However, may seen considerable abstraction. Second, probability agent
information concerning neighbors low, need specify default values s0j .
Clearly, default value often wrong right, unless use centralized mechanism
estimate by, instance, using current average population strategy,
simulations.
mechanism place, perform experiments 4, i.e. vary
population size 10 10, 000 agents, proportion FS agents steps 10%.
statistical analysis reveals significant difference setup uses reputation setup
not. analyze results, see that, expected, agents almost always
need resort default values neighbors strategies. Thus, average, reputation
system often change probabilities certain neighbors selected.
5.2 Reputation Rewiring
seen 4, rewiring works well without reputation (i.e. purely based agents
experience). Adding reputation may beneficial agents, longer need interact
allowed unwire. Thus, agents may increase preference
certain others. Reputation information (i.e. amount offered certain agent) propagates
(static) reputation network, allowing agents receiving information potentially
unwire one neighbors consider neighbors behavior undesirable.
rewiring mechanism used detailed 3 (i.e. Equation 6). allow responder
569

fiD E J ONG , U YTTENDAELE & UYLS

Ultimatum Game broadcast reputation information reputation network,
maximum H = 5 hops.
again, perform experiments 4, again, significant
difference main results. analyze number times agents actually rewired,
find number average increases factor 2 respect setup reputation
used (i.e. reported 4.3). However, increase increase performance. average, agents neighbors; thus, generally receive reputation information concerning
neighbor that, absence reputation, would play soon anyway.
5.3 Volunteering
According existing research human fairness (e.g. Boyd & Mathew, 2007; Hauert et al., 2007;
Sigmund et al., 2001) mechanism volunteering may contribute reaching cooperation
games two strategies. mechanism volunteering consists allowing players
participate certain games, enabling fall back safe side income depend
players strategies. risk-averse optional participation prevent exploiters
gaining upper hand, left empty-handed cooperative players preferring
participate. Clearly, side income must carefully selected, agents encouraged
participate population sufficiently cooperative. Experiments show volunteering indeed
allows collective players spend time happy state (Boyd & Mathew, 2007)
players cooperative.
biggest problem applying volunteering basically introduce yet another
social dilemma. agent may refrain participating make statement
agent, may convince agent become social future, make
statement, agent must refuse expected positive payoff: Ultimatum Game randomly
assigned roles, expected payoff always positive. Nonetheless, study whether volunteering
promotes agreement games continuous strategy spaces. use heuristic
proposed Santos et al. (2006a), already applied various mechanisms
paper: agent thinks agent j (relative) cooperator, agrees playing.
agents agree, game played. prevent agents playing game (after all,
agents see relative cooperator already playing strategy),
introduce 10% probability games played anyway, even one agents want
to. Note reputation may used here, may allow agents estimate whether one
neighbors relative cooperator not, without play neighbor.
Unfortunately, experimental results point agents using volunteering (with without
reputation) severe difficulties establishing common strategy (Uyttendaele, 2008). result, measuring performance, see around 50% games played.
games played, performance similar setup rewiring (e.g. 80%), may
expected, two agents usually agree play strategies similar. reason
agents converge properly quite simple: avoid playing agents
different them. Therefore, learn behave way similar others.
5.4 General Discussion
general, may state mechanism rewiring, clearly find good balance
allowing agents play preferred neighbors one hand, forcing agents
570

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

learn different hand. additions discussed allow
agents selective, i.e. much influence play against.
may interest individual agents, generally leads agents playing others
different them, instead learning others, required obtain convergence
agreement.

6. Conclusion
paper, argue mechanisms thought allow humans find fair, satisfactory solutions
social dilemmas, may useful multi-agent systems, many multi-agent systems need
address tasks contain elements social dilemmas, e.g. resource allocation (Chevaleyre et al.,
2006). Existing work concerning (human-inspired) fairness multi-agent systems generally restricted discrete strategy sets, usually two strategies, one deemed
cooperative (i.e. desirable). However, many real-world applications multi-agent systems pose
social dilemmas require strategy taken continuous strategy space, rather discrete strategy set. observed traditional concept cooperation trivially applicable
continuous strategy spaces, especially since longer feasible manually label certain
strategy cooperative absolute manner. certain strategy may cooperative certain
culture, whereas may defective naive another. Thus, cooperation relative rather
absolute concept continuous strategy spaces.
propose concept agreement (as introduced statistical physics; DallAsta et al.,
2006) may used alternative cooperation. discuss emergence agreement
continuous strategy spaces, using learning agents play pairwise Ultimatum Games, based
scale-free interaction network possibility rewire network. Ultimatum
Game, two agents agree offering agent offers least minimal amount satisfies
responding agent (in case, two agents cooperate). Thus, population agents
agree many random pairwise games, agents converge strategy. Without
external influences, shared strategy sufficient. external influences, e.g. preference
dictated humans, agents adapt dictated strategy, even already agreeing
completely different strategy. propose methodology, based continuous-action learning
automata, interactions scale-free networks, rewiring networks, aimed allowing
agents reach agreement. set experiments investigates usability methodology.
conclusion, give four statements. (1) proposed methodology able establish agreement common strategy, especially agents given option rewire network
interaction. Humans playing Ultimatum Game reach agreement approximately 80-90%
(Oosterbeek et al., 2004). Without rewiring, agents worse (generally, 65% games
successful); rewiring, well humans. Thus, games discrete strategy set,
rewiring greatly enhances agents abilities reach agreement, without compromising scalefree network structure. indicates interactions scale-free networks, well rewiring
networks, plausible mechanisms making agents reach agreement. (2) comparison
methodologies reported related work (e.g. Santos et al., 2006b), methodology facilitates
convergence low number games per agent needed (e.g. 500 instead 10,000).
indicates continuous-action learning automata satisfactory approach aiming
allowing agents learn relatively low number examples. (3) performance
collective seriously influenced size. clearly influenced characteristics
571

fiD E J ONG , U YTTENDAELE & UYLS

scale-free, self-similar network. (4) Concepts reputation volunteering,
reported facilitate cooperative outcomes discrete-strategy games, seem
(additional) benefits continuous strategy spaces.
Although Ultimatum Game one example social dilemma, core difficulty
present social dilemmas: selecting individually rational action, optimize ones
payoff, actually may hurt payoff. Ultimatum Game, problem caused fact
may play (as proposer) someone would rather go home empty-handed
accept deal perceived unfair. Similar fairness-related problems may arise various
interactions, e.g. bargaining division reward, resource allocation (Chevaleyre
et al., 2006; Endriss, 2008). Many multi-agent systems need allocate resources, explicitly
assigned task, implicitly, instance multiple agents share certain, limited
amount computation time. Thus, fair division important area research, recently
receiving increasing attention multi-agent systems community (Endriss, 2008). humans
often display adequate immediate ability come fair division accepted
them, definitely pay allow agents learn imitate human strategies.
paper, examined task may executed.

Acknowledgments
authors wish thank anonymous referees valuable contributions. Steven de Jong
funded Breedtestrategie programme Maastricht University.

References
Axelrod, R. (1997). Dissemination Culture: Model Local Convergence Global
Polarization. Journal Conflict Resolution, 41:203226.
Barabasi, A.-L. Albert, R. (1999). Emergence scaling random networks. Science, 286:509
512.
Bearden, J. N. (2001). Ultimatum Bargaining Experiments: State Art. SSRN eLibrary.
Bowles, S., Boyd, R., Fehr, E., Gintis, H. (1997). Homo reciprocans: Research Initiative
Origins, Dimensions, Policy Implications Reciprocal Fairness. Advances Complex
Systems, 4:130.
Boyd, R. Mathew, S. (2007). Narrow Road Cooperation. Science, 316:18581859.
Cameron, L. (1999). Raising stakes ultimatum game: Evidence Indonesia. Journal
Economic Inquiry, 37:4759.
Chevaleyre, Y., Dunne, P., Endriss, U., Lang, J., Lematre, M., Maudet, N., Padget, J., Phelps, S.,
Rodriguez-Aguilar, J., Sousa, P. (2006). Issues Multiagent Resource Allocation. Informatica, 30:331.
Chiang, Y.-S. (2008). Path Toward Fairness: Preferential Association Evolution Strategies Ultimatum Game. Rationality Society, 20(2):173201.
572

fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME

DallAsta, L., Baronchelli, A., Barrat, A., Loreto, V. (2006). Agreement dynamics smallworld networks. Europhysics Letters, 73(6):pp. 969975.
Dannenberg, A., Riechmann, T., Sturm, B., Vogt, C. (2007). Inequity Aversion Individual
Behavior Public Good Games: Experimental Investigation. SSRN eLibrary.
de Jong, S. Tuyls, K. (2008). Learning cooperate public-goods interactions. Presented
EUMAS08 Workshop, Bath, UK, December 18-19.
de Jong, S., Tuyls, K., Verbeeck, K. (2008a). Artificial Agents Learning Human Fairness.
Proceedings international joint conference Autonomous Agents Multi-Agent
Systems (AAMAS08), pages 863870.
de Jong, S., Tuyls, K., Verbeeck, K. (2008b). Fairness multi-agent systems. Knowledge
Engineering Review, 23(2):153180.
de Jong, S., Tuyls, K., Verbeeck, K., Roos, N. (2008c). Priority Awareness: Towards Computational Model Human Fairness Multi-agent Systems. Adaptive Agents Multi-Agent
Systems III. Adaptation Multi-Agent Learning, 4865:117128.
Endriss, U. (2008). Fair Division. Tutorial International Conference Autonomous Agents
Multi-Agent Systems (AAMAS).
Fehr, E. (2004). Dont lose reputation. Nature, 432:499500.
Fehr, E. Schmidt, K. (1999). Theory Fairness, Competition Cooperation. Quarterly
Journal Economics, 114:817868.
Gintis, H. (2001). Game Theory Evolving: Problem-Centered Introduction Modeling Strategic
Interaction. Princeton University Press, Princeton, USA.
Grosskopf, B. (2003). Reinforcement Directional Learning Ultimatum Game Responder Competition. Experimental Economics, 6(2):141158.
Gueth, W., Schmittberger, R., Schwarze, B. (1982). Experimental Analysis Ultimatum
Bargaining. Journal Economic Behavior Organization, 3 (4):367388.
Hauert, C., Traulsen, A., Brandt, H., Nowak, M. A., Sigmund, K. (2007). Via freedom
coercion: emergence costly punishment. Science, 316:19051907.
Henrich, J., Boyd, R., Bowles, S., Camerer, C., Fehr, E., Gintis, H. (2004). Foundations
Human Sociality: Economic Experiments Ethnographic Evidence Fifteen Small-Scale
Societies. Oxford University Press, Oxford, UK.
Maynard-Smith, J. (1982). Evolution Theory Games. Cambridge University Press.
Maynard-Smith, J. Price, G. R. (1973). logic animal conflict. Nature, 246:1518.
Nowak, M. A., Page, K. M., Sigmund, K. (2000). Fairness versus reason Ultimatum
Game. Science, 289:17731775.
573

fiD E J ONG , U YTTENDAELE & UYLS

Oosterbeek, H., Sloof, R., van de Kuilen, G. (2004). Cultural Differences Ultimatum Game
Experiments: Evidence Meta-Analysis. Experimental Economics, 7:171188.
Peters, R. (2000). Evolutionary Stability Ultimatum Game. Group Decision Negotiation,
9:315324.
Roth, A. E., Prasnikar, V., Okuno-Fujiwara, M., Zamir, S. (1991). Bargaining Market
Behavior Jerusalem, Ljubljana, Pittsburgh, Tokyo: Experimental Study. American
Economic Review, 81(5):106895.
Santos, F. C., Pacheco, J. M., Lenaerts, T. (2006a). Cooperation Prevails Individuals
Adjust Social Ties. PLoS Comput. Biol., 2(10):12841291.
Santos, F. C., Pacheco, J. M., Lenaerts, T. (2006b). Evolutionary Dynamics Social Dilemmas
Structured Heterogeneous Populations. Proc. Natl. Acad. Sci. USA, 103:34903494.
Selten, R. Stoecker, R. (1986). End behavior sequences finite Prisoners Dilemma supergames : learning theory approach. Journal Economic Behavior & Organization, 7(1):47
70.
Sigmund, K., Hauert, C., Nowak, M. A. (2001). Reward punishment. Proceedings
National Academy Sciences, 98(19):1075710762.
Slonim, R. Roth, A. (1998). Learning high stakes ulitmatum games: experiment
Slovak republic. Econometrica, 66:569596.
Sonnegard, J. (1996). Determination first movers sequential bargaining games: experimental study. Journal Economic Psychology, 17:359386.
Sutton, R. S. Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press,
Cambridge, MA. Bradford Book.
Thathachar, M. A. L. Sastry, P. S. (2004). Networks Learning Automata: Techniques
Online Stochastic Optimization. Kluwer Academic Publishers, Dordrecht, Netherlands.
Uyttendaele, S. (2008). Fairness agreement complex networks. Masters thesis, MICC,
Maastricht University.

574

fiJournal Artificial Intelligence Research 33 (2008) 349-402

Submitted 05/08; published 11/08

Learning Partially Observable Deterministic Action Models
EYAL @ ILLINOIS . EDU

Eyal Amir
Computer Science Department
University Illinois, Urbana-Champaign
Urbana, IL 61801, USA

ALLENC 256@ YAHOO . COM

Allen Chang
2020 Latham st., apartment 25
Mountainview, CA 94040, USA

Abstract
present exact algorithms identifying deterministic-actions effects preconditions
dynamic partially observable domains. apply one know action model (the
way actions affect world) domain must learn partial observations time.
scenarios common real world applications. challenging AI tasks
traditional domain structures underly tractability (e.g., conditional independence) fail
(e.g., world features become correlated). work departs traditional assumptions
partial observations action models. particular, focuses problems actions
deterministic simple logical structure observation models features observed
frequency. yield tractable algorithms modified problem domains.
algorithms take sequences partial observations time input, output deterministic action models could lead observations. algorithms output one
models (depending choice), exact model misclassified given
observations. algorithms take polynomial time number time steps state features
traditional action classes examined AI-planning literature, e.g., STRIPS actions.
contrast, traditional approaches HMMs Reinforcement Learning inexact exponentially intractable domains. experiments verify theoretical tractability guarantees,
show identify action models exactly. Several applications planning, autonomous
exploration, adventure-game playing already use results. also promising
probabilistic settings, partially observable reinforcement learning, diagnosis.

1. Introduction
Partially observable domains common real world. involve situations one
cannot observe entire state world. Many examples situations available
walks life, e.g., physical worlds (we observe position items rooms),
Internet (we observe web pages time), inter-personal communications (we observe state mind partners).
Autonomous agents actions involve special kind partial observability domains.
agents explore new domain (e.g., one goes building meets new person),
limited knowledge action models (actions preconditions effects). action models change time, may depend state features. agents act
intelligently, learn actions affect world use knowledge respond
goals.
c
2008
AI Access Foundation. rights reserved.

fiA MIR & C HANG

Learning action models important goals change. agent acted while,
use accumulated knowledge actions domain make better decisions. Thus, learning
action models differs Reinforcement Learning. enables reasoning actions instead
expensive trials world.
Learning actions effects preconditions difficult partially observable domains.
difficulty stems absence useful conditional independence structures domains.
fully observable domains include structures, e.g., Markov property (independence
state time + 1 state time 1, given (observed) state time t).
fundamental tractable solutions learning decision making.
partially observable domains structures fail (e.g., state world time + 1
depends state time 1 observe state time t), complex
approximate approaches feasible path. reasons, much work far
limited fully observable domains (e.g., Wang, 1995; Pasula, Zettlemoyer, & Kaelbling, 2004),
hill-climbing (EM) approaches unbounded error deterministic domains (e.g., Ghahramani, 2001; Boyen, Friedman, & Koller, 1999), approximate action models (Dawsey, Minsker,
& Amir, 2007; Hill, Minsker, & Amir, 2007; Kuffner. & LaValle, 2000; Thrun, 2003).
paper examines application old-new structure learning partially observable
domains, namely, determinism logical formulation. focuses deterministic domains tractable learning feasible, shows traditional assumption form
determinism (the STRIPS assumption, generalized ADL, Pednault, 1989) leads tractable
learning state estimation. Learning domains immediate applications (e.g., exploration planning, Shahaf, Chang, & Amir, 2006; Chang & Amir, 2006) also serve
basis learning stochastic domains. Thus, fundamental advance application
structure important opening field new approaches broader applicability.
following details technical aspects advance.
main contribution paper approach called SLAF (Simultaneous Learning
Filtering) exact learning actions models partially observable deterministic domains.
approach determines set possible transition relations, given execution sequence actions
partial observations. example, input could come watching another agent act
watching results actions execution. approach online, updates
propositional logical formula called Transition Belief Formula. formula represents possible
transition relations world states every time step. way, similar spirit Bayesian
learning HMMs (e.g., Ghahramani, 2001) Logical Filtering (Amir & Russell, 2003).
algorithms present differ range applicability computational
complexity. First, present deduction-based algorithm applicable nondeterministic
learning problem, takes time worst-case exponential number domain fluents.
Then, present algorithms update logical encoding consistent transition relations
polynomial time per step, limited applicability special classes deterministic
actions.
One set polynomial-time algorithms present applies action-learning scenarios
actions ADL (Pednault, 1989) (with conditional effects) one following
holds: (a) action model already preconditions known, observe action failures (e.g.,
perform actions domain), (b) actions execution always succeeds (e.g.,
expert tutor performs actions).
350

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

algorithms output transition belief formula represents possible transition relations
states partial observations state actions. updating component
formula separately linear time. Thus, updating transition belief formula every
action execution observation takes linear time size input formula.
Processing sequence action executions observations takes time O(T 2 n) case
(b). main reason linear growth representation size transition belief
formula: time t, iterative process updates formula would process formula
size linear t.
case (a) processing sequence length takes polynomial time O(T n k )),
observe every feature domain every k steps expectation, fixed k. reason
transition belief formula kept k-CNF (k Conjunctive Normal
V Form),
W thus
size O(nk ). (Recall propositional formula k-CNF, form im jk li,j ,
every li,j propositional variable negation.) Case (b) takes time O(T n)
assumption.
Another set polynomial-time algorithms present takes linear time representation
size. case actions known injective, i.e., map states 1:1. There, bound
computation time steps O(T nk ), approximate transition-belief formula
representation k-CNF formula.
contrast, work learning Dynamic Bayesian Networks (e.g., Boyen et al., 1999), reinforcement learning POMDPs (e.g., Littman, 1996), Inductive Logic Programming (ILP)
(e.g., Wang, 1995) either approximate solution unbounded error deterministic domains,
n
take time (22 ), inapplicable domains larger 10 features. algorithms
better respect, scale polynomially practically domains 100s features
more. Section 8 provides comparison works.
conduct set experiments verify theoretical results. experiments show
algorithms faster better qualitatively related approaches. example,
learn ADL actions effects domains > 100 features exactly efficiently.
important distinction must made learning action models traditional creation
AI-Planning operators. perspective AI Planning, action models result
explicit modeling, taking account modeling decisions. contrast, learning action models
deducing possible transition relations compatible set partially observed
execution trajectories.
particular, action preconditions typically used knowledge engineer control
granularity action model leave aside specification unwanted cases. example,
driving truck insufficient fuel one site another might generate unexpected situations
modeller want consider, simple precondition used avoid considering case. intention paper mimic modeling perspective, instead
find action models generate sound states starting sound state. Sound state
state system practice, namely, ones observations real executions
reflect.
technical advance deterministic domains important many applications
automatic software interfaces, internet agents, virtual worlds, games. applications,
robotics, human-computer interfaces, program machine diagnosis use deterministic
action models approximations. Finally, understanding deterministic case better help us
351

fiA MIR & C HANG

develop better results stochastic domains, e.g., using approaches Boutilier,
Reiter, Price (2001), Hajishirzi Amir (2007).
following, Section 2 defines SLAF precisely, Section 3 provides deduction-based exact
SLAF algorithm, Section 4 presents tractable action-model-update algorithms, Section 5 gives sufficient conditions algorithms keeping action-model representation compact (thus, overall
polynomial time), Section 7 presents experimental results.

2. Simultaneous Learning Filtering (SLAF)
Simultaneous Learning Filtering (SLAF) problem tracking dynamic system
sequence time steps partial observations, systems complete
dynamics initially. solution SLAF representation combinations action models
could possibly given rise observations input, representation
corresponding states system may (after sequence time steps
given input occurs).
Computing (the solution for) SLAF done recursive fashion dynamic programming
determine SLAF time step t+1 solution SLAF time t. section
define SLAF formally recursive fashion.
Ignoring stochastic information assumptions, SLAF involves determining set possible
ways actions change world (the possible transition models, defined formally below)
set states system might in. transition model determines set possible states,
solution SLAF transition model associated possible states.
define SLAF following formal tools, borrowing intuitions work Bayesian
learning Hidden Markov Models (HMMs) (Ghahramani, 2001) Logical Filtering (Amir &
Russell, 2003).
Definition 2.1 transition system tuple hP, S, A, Ri,
P finite set propositional fluents;
P ow(P) set world states.
finite set actions;
R transition relation (transition model).
Thus, world state, S, subset P contains propositions true state (omitted
propositions false state), R(s, a, s0 ) means state s0 possible result action
state s. goal paper find R, given known P, S, A, sequence actions
partial observations (logical sentences subset P).
Another, equivalent, representation also use paper following.
literal proposition, p P, negation, p. complete term P conjunction
literals P every fluent appears exactly once. Every state corresponds complete
term P vice versa. reason, sometime identify state term. E.g.,
states s1 , s2 , s1 s2 disjunction complete terms corresponding 1 , s2 , respectively.
transition belief state set tuples hs, Ri state R transition relation.
Let R = P ow(S S) set possible transition relations S, A. Let = R.
hold transition belief state consider every tuple hs, Ri possible.

352

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

Figure 1: Locked door unknown key domain.
Example 2.2 Consider domain agent room locked door (see Figure 1).
possession three different keys, suppose agent cannot tell observation
key opens door. goal agent unlock door.
domain represented follows: let set variables defining state space
P = {locked} locked true door locked. Let set states
= {s1 , s2 } s1 = {locked} (the state door locked) s2 = {} (here
door unlocked). Let = {unlock1 , unlock2 , unlock3 } three actions wherein agent
tries unlocking door using three keys.
Let R1 = {hs1 , unlock1 , s2 i, hs1 , unlock2 , s1 i, hs1 , unlock3 , s1 i} represent transition relation key 1 unlocks door keys not. Define R 2 R3 similar
fashion (e.g., R2 key 2 unlocks door keys 1 3 not). transition belief state
represents set possibilities consider consistent observations far. Consider
transition belief state given = {hs1 , R1 i, hs1 , R2 i, hs1 , R3 i}, i.e., state world
fully known action model partially known.
would like agent able open door despite knowing key opens it.
this, agent learn actual action model (i.e., key opens door). general,
learning action model useful achieving immediate goal, knowledge
useful agent attempts perform tasks domain.2
Definition 2.3 (SLAF Semantics) Let transition belief state. SLAF
actions observations haj , oj i1jt defined
1. SLAF [a]() =
{hs0 , Ri | hs, a, s0 R, hs, Ri };
2. SLAF [o]() = {hs, Ri | true s};
3. SLAF [haj , oj iijt ]() =
SLAF [haj , oj ii+1jt ](SLAF [oi ](SLAF [ai ]())).
Step 1 progression a, Step 2 filtering o.
Example 2.4 Consider domain Example 2.2. progression action unlock 1
given SLAF [unlock1 ]() = {hs2 , R1 i, hs1 , R2 i, hs1 , R3 i}. Likewise, filtering
observation locked (the door became unlocked) given SLAF [locked]() = {hs 2 , R1 i}.2
353

fiA MIR & C HANG

Example 2.5 slightly involved example following situation presented Figure 2.
There, two rooms, light bulb, switch, action flipping switch, observation, E (we east room). real states world action, s2, s2 0 ,
respectively (shown top part), known us.
West

East


PSfrag replacements

West


s2 = sw lit E



=
sw-on

<s1,R1>

1

East

s20 = sw lit E
<s1,R1>

<s3,R3>

<s1,R1>
<s3,R3>

<s2,R2>



<s2,R2>

2

Figure 2: Top: Two rooms flipping light switch. Bottom: SLAF semantics; progressing
action (the arrows map state-transition pairs) filtering observation
(crossing pairs).
bottom Figure 2 demonstrates knowledge evolves performing action sw-on.
There, 1 = {hs1 , R1 i, hs2 , R2 i, hs3 , R3 i} s1 , R1 , s3 , R3 , s2 = {E}, R2 includes hs2 , sw-on, s02 (the identity full details R1 , R2 , R3 irrelevant here, omit
them). 2 resulting transition belief state action sw-on observation E: 2 =
SLAF [sw-on, E](1 ). 2
assume observations (and observation model relating observations state fluents)
given us logical sentences fluents performing action. denoted
o.
approach transition belief states generalizes Version Spaces action models (e.g.,
Wang, 1995) follows: current state, s, known, version spaces lattice contains
set transition relations = {R | hs, Ri }. Thus, perspective version spaces,
SLAF semantics equivalent set version spaces, one state might be.
semantics also generalizes belief states: transition relation, R, known,
belief state (set possible states) R = {s | hs, Ri } (read restricted R), Logical
Filtering (Amir & Russell, 2003) belief state action equal (thus, define as)
F ilter[a]() = (SLAF [a]({hs, Ri | }))R .
Thus, SLAF semantics equivalent holding set belief states, conditioned transition
relation, similar saying transition relation R, belief state (set states) R .

3. Learning Logical Inference
Learning transition models using Definition 2.3 directly intractable requires space (2 2 )
many cases. reason explicit representation large set possible
transition-state pairs. Instead, section rest paper represent transition belief states compactly using propositional logic. many scenarios amount
structure exploited make propositional representation compact.
|P|

354

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

combinatorial argument implies encoding compact sets. Nonetheless,
motivated success propositional (logical) approaches logical filtering (Amir & Russell,
2003; Shahaf & Amir, 2007) logical-database regression (Reiter, 1991, 2001), observe
propositional logic represents compactly natural sets exponential size.
section re-define SLAF operation propositional logical formulas
propositional formula output. SLAFs input propositional formula represents transition
belief state, SLAF computes new transition belief formula input sequence
actions observations.
want find algorithms SLAF manipulate input formula produce correct
output. use general-purpose logical inference task section. later sections
sidestep expensive general-purpose inference, make assumptions lead tractable algorithms. rest paper focus deterministic transition relations, namely, transition
relations partial functions (every action one outcome state every state).
3.1 Representing Transition Relations Logic
initial algorithm solving SLAF (to presented momentarily) compact
representation transition belief states. present logical encoding transition belief states
first, define deduction-based algorithm next section.
use following general terminology propositional logical languages (all terminological conventions apply without subscripts superscripts). L denotes vocabulary, i.e.,
set propositional variables use present context. L denotes language, i.e., set
propositional sentences. , , script Greek letters stand propositional formulas
language present context. F, G also stand formulas, restricted context
(see below). L() denotes vocabulary . L(L) denotes language built propositions
L using standard propositional connectives (, , ,...). L() shorthand L(L()).
represent deterministic transition relations propositional vocabulary, L , whose
propositions form aFG , A, F literal P, G logical formula. F
effect aFG , G precondition aFG . proposition aFG takes truth value TRUE,
intended meaning G holds present state, F holds state
results executing a.
let F P {p | p P} set effects, F , consider. let G
set preconditions, G, consider. rest section Section 4 assume
G represents single state S. Recall identify state complete term
conjunction literals hold state. use representation states write Fs
instead aFG . Later build definition consider Gs general formulas.
assumption (G now, stated above) conclude L O(2|P| 2|P|
|A|) propositional variables. prove fundamental results language set axioms,
disregarding size language moment. Section 5 focuses decreasing language
size computational efficiency.
semantics vocabulary LA lets every interpretation (truth assignment), , LA correspond transition relation, RM . Every transition relation least one (possibly more)
interpretation corresponds it, correspondence surjective (onto) injective
(1-to-1). Every propositional sentence L(LA ) specifies set transition models follows:
355

fiA MIR & C HANG

set models1 (satisfying interpretations) , I[] = {M interpretation LA | |= },
specifies corresponding set transition relations, {RM | I[]}.
Informally, assume propositions aFs 1 , ...aFs k LA take value TRUE ,
propositions precondition
V take value FALSE. Then, R (with action0 a) takes
0
state state satisfies ik Fi , identical otherwise. exists (e.g.,
Fi = Fj , i, j k), RM takes s0 (thus, executable according
RM ).
following paragraphs show interpretations L correspond transition relations.
culminate precise definition correspondence formulas L(L P)
transition belief states S.
E NTERPRETATION LA C ORRESPONDS U NIQUE RANSITION R ELATION
Every interpretations LA defines unique transition relation RM follows. Let interpretation LA . every state action either define unique state 0
hs, a, s0 RM decide s0 hs, a, s0 RM .
gives interpretation every proposition aFs , F fluent negation.
fluent p P, [aps ] = [ap
] = TRUE (M [] truth value according interpretation
), decide s0 hs, a, s0 RM . Otherwise, define
s0 = {p P | |= aps } {p | |= ap
}
left-hand side consider cases p P [a ps ] 6= [ap
],
right-hand side treat cases p P [aps ] = [ap
] = F ALSE (this
called inertia p keeps previous value lack specifications). Put another way,
0
s0 [p] = [aps ] (s[p] [ap
]), view interpretation P. RM well defined, i.e.,
one RM every .
E RANSITION R ELATION
LA



L EAST NE C ORRESPONDING NTERPRETATION



possible RM = RM 0 6= 0 . occurs two circumstances: (a) cases
hs, a, s0 RM s, (b) [aps ] = [ap
] = F ALSE (inertia)
p
p
0
0
[as ] = s[p], [as ] = s[p] (not inertia).
example first circumstance, let p fluent, let interpretation
0
[aps ] = [ap
] G. Define interpretation identical propositions
p p
p
0
besides , follows. Define [as ] opposite truth assignment [aps ] (FALSE
p
instead TRUE, TRUE instead FALSE). Define 0 [ap
] = [as ].
Then, RM = RM 0 map pairs s, way. particular, state
corresponds G, hs, a, s0 RM similarly hs, a, s0 RM 0 .
Finally, every transition relation R least one interpretation R = R . see
this, define MR every hs, a, s0 R interpretation aps (p fluent) MR [aps ] = RU E iff
/ s0 . Finally, s,
p s0 . Also, hs, a, s0 define MR [ap
] = F ALSE iff p
p
p
s0 , define MR [as ] = MR [as ] = RU E. Then, R = RMR .
1. overload word model multiple related meanings. model refers satisfying interpretation logical
formula. Transition model defined Definition 2.1 transition relation transition system. Action model
define Introduction section well-defined specification actions preconditions effects.

356

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

E RANSITION R ELATION EFINES F ORMULA LA
Every deterministic transition relation R defines logical formula whose set models map
R. many possible formulas, define general one (up logical
equivalence) make use inertia.
Define h(R) follows.
F
0
0
h0 (R) = {aFs , aF
| LA , hs, a, R, |= F }
p
p
h1 (R) = {as | p P, S}

W
0
0
h2 (R) = { pP (aps ap
) | , hs, a, R}
h(R) = h0 (R) h1 (R) h2

h0 addresses fluent changes, h1 addresses fluent innertia (effectively disallowing innertia
definition), h2 addresses conditions actions executable. Thus, h(R)
includes model every interpretation satisfies RM = R requires inertia
definition RM . represents R models satisfies RM = R.
illuminating see modeling decisions (above throughout section) lead
last definition. one hand, choose every interpretation L correspond
transition relation (we simplify later arguments logical entailment). Consequently, associate interpretations [aFs ] = [aF
] = F ALSE transition relations R(s, a, s0 ) keep value F fixed s, s0 (this inertia F a, s).
hand, define h(R) above, choose axioms exclude models (thus,
avoid models include inertia) simplifies later discussion learning algorithms.
summary, consider every interpretation LA representing exactly one transition relation, consider set axioms defining R define directly, i.e., without inertia
(without [aFs ] = [aF
] = F ALSE).
RANSITION B ELIEF TATES C ORRESPOND F ORMULAS LA P
Thus, every
W transition belief state define formula L(L P) corresponds
it: h() = hs,Ri (s h(R)). formulas exist would characterize similar way,
equivalent. stronger formulas L(L )
|= h(R) h(R) 6|= every model, , satisfies RM = R.
Similarly, every formula L(LA P) define transition belief state () = {hM P
, RM | |= , }, i.e., state-transition pairs satisfy (M P restricted
P, viewed complete term P). say formula transition belief formula,
h(()) (note: (T h()) = always holds).
3.2 Transition-Formula Filtering
section, show computing transition belief formula SLAF [a]() successful
action transition belief formula equivalent logical consequence finding operation.
characterization SLAF consequence-finding permits using consequence finding
algorithm SLAF, important later paper proving correctness
tractable, specialized algorithms.
Let CnL () denote set logical consequences restricted vocabulary L. is,
L
Cn () contains set prime implicates contain propositions set L.
357

fiA MIR & C HANG

Recall implicate formula clause entailed ( |= ). Recall prime implicate formula implicate subsumed (entailed) implicates
.
Consequence finding process computes CnL () input, . example, propositional resolution (Davis & Putnam, 1960; Chang & Lee, 1973) efficient consequence finder
used properly (Lee, 1967; del Val, 1999) (Marquis, 2000, surveys results prime implicates consequence finding algorithms). Thus, Cn L () { L(L)| |= }.
set propositions P, let P 0 represent set propositions every proposition primed (i.e., proposition f annotated become f 0 ). Typically, use primed
fluent denote value unprimed fluent one step future taking action. Let
[P 0 /P] denote formula , primed fluents replaced unprimed counterparts. example, formula (a b0 )[P 0 /P] equal b b P. (See Section 8
discussion comparison relevant formal verification techniques.)
following lemma shows logical equivalence existential quantification quantified
boolean formulas consequence finding restricted vocabulary. Recall quantified boolean
formulas (QBF) propositional formulas addition existential universal quantifiers
propositions. Informally, QBF x. true given interpretation
exists true/false valuation x makes true assignment. lemma prove
useful showing equivalence SLAF consequence-finding.
Lemma 3.1 x. CnL()\{x} (), propositional logic formula propositional variable x.
P ROOF
See Section B.1. 2
lemma extends easily case multiple variables:
Corollary 3.2 formula set propositional variables X, X. Cn L()\X ().
present algorithm updating transition belief formulas whose output equivalent
SLAF (when SLAF applied equivalent transition belief state). algorithm applies
consequence finding input transition belief formula together set axioms define
transitions time steps. present set axioms first.
deterministic (possibly conditional) action, a, action model (for time t) axiomatized
V
Teff (a) = lF ,GG ((alG G) l0 )
W
V
(1)
l
0
lF (l ( GG (aG G)))

first part (1) says assuming executes time t, causes l G holds, G
holds time t, l holds time + 1. second part says l holds execution,
must alG holds also G holds current state. two parts similar
(in fact, somewhat generalize) effect axioms explanation closure axioms used Situation
Calculus (see McCarthy & Hayes, 1969; Reiter, 2001).
Now, ready describe zeroth-level algorithm (SLAF 0 ) SLAF transition
belief formula. Let L0 = P 0 LA vocabulary includes fluents time t+1 effect
propositions LA . Recall (Definition 2.3) SLAF two operations: progression (with
action) filtering (with observation). time apply progression given action
current transition belief formula, , apply filtering current observations:
358

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

0

t+1 = SLAF0 [at , ot ](t ) = (CnL (t Teff (at )))[P 0 /P] ot

(2)

identical Definition 2.3 (SLAF semantics), replacing 1 2.
0
stated above, SLAF0 implement CnL () using consequence finding algorithms
resolution variants (e.g., Simon & del Val, 2001; McIlraith & Amir, 2001;
Lee, 1967; Iwanuma & Inoue, 2002). following theorem shows formula-SLAF algorithm correct exact.
Theorem 3.3 (Representation) transition belief formula, action,
SLAF [a]({hs, Ri | hs, Ri satisfies }) =
{hs, Ri | hs, Ri satisfies SLAF0 [a]()}
P ROOF
See Section B.2. 2
theorem allows us identify SLAF0 SLAF , throughout rest
paper. particular, show polynomial-time algorithms SLAF special cases correct
showing output logically equivalent SLAF 0 .
U SING



UTPUT



SLAF0

output algorithm SLAF transition belief formula logical formula. way
use formula answering questions SLAF depends query form
output formula. wish find transition model state possible, wish see
|= , interpretation L = P LA output SLAF0 .
answer found simple model-checking algorithm 2 . example, check
interpretation satisfies logical formula assign truth values variables interpretation formula; computing truth value formula done linear time.
Thus, type query SLAF takes linear time size output formula
SLAF final query propositional interpretation propositional formula.
wish find transition model possible state possible,
propositional satisfiability (SAT) solver algorithms (e.g., Moskewicz, Madigan, Zhao, Zhang, &
Malik, 2001). Similarly, wish answer whether possible models satisfy property
use SAT solver.
Example 3.4 Recall Example 2.4 discuss locked door three combinations. Let
0 = locked, let 1 = SLAF0 [unlock2 , locked](0 ). wish find 1 implies trying
unlock door key 2 fails open it. equivalent asking models consistent
1 give value TRUE unlock2locked
locked .
answer query taking SLAF0 output formula, 1 , checking 1
unlock2locked
locked SAT (has model). (Follows Deduction Theorem propositional logic:
|= iff SAT.)
One example application approach goal achievement algorithm Chang Amir
(2006). relies SAT algorithms find potential plans given partial knowledge encoded
transition belief formula.
2. model checking sense used Formal Verification literature. There, model transition
model, checking done updating formula OBDD transformations

359

fiA MIR & C HANG

zeroth-level algorithm may enable compact representation, guarantee
it, guarantee tractable computation. fact, algorithm maintain compact representation tractable computation general. Deciding clause true result SLAF
coNP-hard similar decision problem Logical Filtering coNP-hard (Eiter & Gottlob, 1992; Amir & Russell, 2003) even deterministic actions. (The input representation
problems includes initial belief state formula CNF. input representation Filtering
includes propositional encoding CNF (known) transition relation.)
Also, representation transition belief states uses poly(|P|) propositions grows exponentially (in number time steps |P|) starting transition belief states action
sequences, actions allowed nondeterministic 3 . question whether exponential growth must happen deterministic actions flat formula representations (e.g., CNF,
DNF, etc.; see Darwiche & Marquis, 2002) open (logical circuits known give solution
deterministic actions, representation given terms fluents time 0, Shahaf et al.,
2006).

4. Factored Formula Update
Update representation hard must consider set interactions parts
representation. Operations like used SLAF 0 consider interactions, manipulate
them, add many interactions result. processing broken independent
pieces, computation scales linearly number pieces (i.e., computation time
total times takes piece separately). So, important find decompositions
enable independent pieces computation. Hereforth examine one type decomposition,
namely, one follows logical connectives.
Learning world models easier SLAF distributes logical connectives. function,
f , distributes logical connective, {, , ...}, f ( ) f () f (). Computation
SLAF becomes tractable, distributes , . bottleneck computation case
becomes computing SLAF part separately.
section examine conditions guarantee distribution, present linear-time
algorithm gives exact solution cases. also show algorithm
gives weaker transition belief formula distribution possible.
Distribution properties always hold SLAF follow set theoretical considerations
Theorem 3.3:
Theorem 4.1 , transition belief formulas, action,
SLAF [a]( ) SLAF [a]() SLAF [a]()
|= SLAF [a]( ) SLAF [a]() SLAF [a]()
P ROOF
See Appendix B.3. 2
Stronger distribution properties hold SLAF whenever hold Logical Filtering.
Theorem 4.2 Let 1 , 2 transition belief states.
SLAF [a](1 2 ) = SLAF [a](1 ) SLAF [a](2 )
3. follows theorem filtering Amir Russell (2003), even provide proper axiomatization
(note axiomatization deterministic actions only).

360

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

iff every R
R
R
R
F ilter[a](R
1 2 ) = F ilter[a](1 ) F ilter[a](2 ).

conclude following corollary Theorems 3.3, 4.2 theorems Amir Russell
(2003).
Corollary 4.3 , transition belief formulas, action, SLAF [a]( ) SLAF [a]()
SLAF [a]() every relation R , one following holds:
1. R maps states 1:1
2. R conditional effects, includes prime implicates, observe
fails
3. state known R: one s, hs, Ri .
Condition 2 combines semantics syntax. particularly useful correct computation
SLAF later sections. states particular syntactic form (namely, together
include joint prime implicates), simple enough (but necessarily 1:1),
computation SLAF broken separate SLAF .
Figure 3 presents Procedure Factored-SLAF, computes SLAF exactly conditions Corollary 4.3 hold. Consequently, Factored-SLAF returns exact solution whenever
actions known 1:1. actions conditional effects success/failure
observed, modified Factored-SLAF solve problem exactly (see Section 5).
PROCEDURE Factored-SLAF(hai , oi i0<it ,)
i, ai action, oi observation, transition belief formula.
1. 1 do,
(a) Set Step-SLAF(oi ,ai ,).
(b) Eliminate subsumed clauses .
2. Return .
PROCEDURE Step-SLAF(o,a,)
observation formula L(P), action, transition belief formula.
1. literal, return oLiteral-SLAF(a,).
2. = 1 2 , return Step-SLAF(o,a,1 )Step-SLAF(o,a,2 ).
3. = 1 2 , return Step-SLAF(o,a,1 )Step-SLAF(o,a,2 ).
PROCEDURE Literal-SLAF(a,)
action, proposition Lt negation.
0
1. Return CnL ( Teff (a))[P 0 /P ] .

Figure 3: SLAF using distribution ,
pre-compute (and cache) 2n possible responses Literal-SLAF, every time step
procedure requires linear time representation size , transition belief formula
time step. significant improvement (super exponential) time taken
straightforward algorithm, (potentially exponential) time taken general-purpose
consequence finding used zeroth-level SLAF procedure above.
Theorem 4.4 Step-SLAF(a, o, ) returns formula 0 SLAF [a, o]() |= 0 . every run
Literal-SLAF takes time c, Step-SLAF takes time O(||c). (recall || syntactic,
representation size .) Finally, assume one assumptions Corollary 4.3,
0 SLAF [a, o]().
361

fiA MIR & C HANG

belief-state formula transition belief formula effect propositions, i.e., includes fluent variables propositions form FG . identical traditional
use term belief-state formula, e.g., (Amir & Russell, 2003). give closed-form solution SLAF belief-state formula (procedure Literal-SLAF Figure 3). makes
procedure Literal-SLAF tractable, avoiding general-purpose inference filtering single literal,
also allows us examine structure belief state formulas detail.
V
Theorem 4.5 belief-state formula L(P), action a, Ca = GG,lF (alG al
G ),
G1 , ..., Gm G terms G Gi |= ,
SLAF [a]()

^


_

li
(li aG
) Ca


l1 ,...,lm F i=1

V

Here, l1 ,...,lm F means conjunction possible (combinations of) selections literals
F.
P ROOF
See Appendix B.4. 2
theorem significant says write result SLAF
prescribed form. form still potentially exponential size, boils simple
computations. proof follows straightforward (though little long) derivation possible
prime implicates SLAF [a](t ).
consequence Theorem 4.5 implement Procedure Literal-SLAF using
equivalence


Theorem 4.5
L(l) P
SLAF [a](l)
l SLAF [a](TRUE) otherwise

Notice computation Theorem 4.5 = l literal simple G 1 , ..., Gm
complete terms L(P) include l. computation require general-purpose
consequence finder, instead need answer 2n+1 queries initialization phase, namely,
storing table values SLAF [a](l) l = p l = p p P also
l = TRUE.
general, could high 2|P| , number complete terms G, finding G1 , ..., Gm
may take time exponential |P|. Still, simplicity computation formula provide
basic ingredients needed efficient computations following sections restricted cases.
also give guidelines future developments SLAF algorithms.

5. Compact Model Representation
Previous sections presented algorithms potentially intractable long sequences actions
observations. example, Theorem 4.5, could high 2 |P| , number complete
terms G. Consequently, clauses may exponential length (in n = |P|) may
super-exponential number clauses result.
section focus learning action models efficiently presence action preconditions failures. important agents partial domain knowledge
therefore likely attempt inexecutable actions.
restrict attention deterministic actions conditional effects, provide
overall polynomial bound growth representation, size many steps,
362

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

time taken compute resulting model. class actions generalizes STRIPS (Fikes, Hart,
& Nilsson, 1972), results apply large part AI-planning literature.
give efficient algorithm learning non-conditional deterministic action effects
preconditions, well efficient algorithm learning actions effects presence
action failures.
5.1 Actions Limited Effect
many domains assume every action affects k fluents, small k > 0.
also common assume actions STRIPS, may fail without us knowing,
leaving world unchanged. assumptions together allow us progress SLAF limited
(polynomial factor) growth formula size.
use language similar one Section 3, uses action propositions
alG G
fluent term size k (instead fluent term size n G). Semantically,
V
l
al1 ...lk lk+1 ,...,ln all1 ...ln .
Theorem 5.1 Let L(P) belief-state formula, STRIPS action k fluents
affected precondition term. Let G k set terms k fluents L(P)
consistent . Then,
SLAF [a]()

^

k
_

li
(li aG
) Ca


i=1
G1 , ..., Gk G k
G1 ... Gk |=
l1 , ..., lk F

V
Here, ... refers conjunction possible (combinations of) selections literals
F G1 , ..., Gk G k G1 ... Gk |= .
P ROOF
See Section B.5. 2
main practical difference theorem Theorem 4.5 smaller number
terms need checked practical computation. limited language enables entails
limited number terms play here. Specifically, k literals
preconditions, need check combinations k terms G 1 , ..., Gk G k , computation
bounded O(exp(k)) iterations.
proof uses two insights. First, one case change occurs, every
clause Theorem 4.5 subsumed clause entailed SLAF [a]( ), one
alGi per literal li (i.e., li 6= lj 6= j) Gi fluent term (has disjunctions). Second, every
alG G term equivalent formula alGi Gi terms length k, affects k
fluents.
Thus, encode clauses conjunction using subset (extended) action
effect propositions, alG , G term size k. O(nk ) terms, O(nk+1 )
propositions. Every clause length 2k, identity clause determined
2
first half (the set action effect propositions). Consequently, SLAF [a]( ) takes O(nk +k k 2 )
2
space represent using O(nk +k ) clauses length 2k.
363

fiA MIR & C HANG

5.2 Actions Conditional Effects: Revised Language
thisSsection reformulate representation presented Section 3.1. Let
L0f = aA {af , af , af , a[f ] , a[f ] } every f P. Let vocabulary formulas representing transition belief states defined L = P L0f . intuition behind propositions
vocabulary follows:
V
al causes l literal l. Formally, al sS als .
V
af keeps f . Formally, af sS ((s f ) afs ) ((s f ) af
).
V
).
(Thus, l
a[l] causes FALSE l. Formally, a[l] sS (s l) (als al

precondition executing a, must hold executes.)
model transition belief formula L, valuation fluents P defines
state. valuation propositions L0f defines unconditional deterministic transition
relation follows: action proposition af (af ) true action transition
relation causes f (f ) hold executed. Action proposition f true
action affect fluent f . Action proposition a[f ] (a[f ] ) true f (f )
precondition a. assume existence logical axioms disallow inconsistent
impossible models. axioms are:
1. af af af
2. (af af ) (af af ) (af af )


3. a[f ] a[f ]

possible f P. first two axioms state every action model, exactly one
af , af , af must hold (thus, causes f , negation, keeps f unchanged). last axiom
disallows interpretations a[f ] a[f ] hold. state axioms
need represent constraints explicitly transition belief formula itself.
use set theoretic propositional logic notations transition belief states interchangeably. Note vocabulary defined sufficient describing unconditional
STRIPS action model, deterministic action model general.
Example 5.2 Consider domain Example 2.2. transition belief state represented transition belief formula:
locked
((unlock1locked unlock2locked unlock3locked )
(unlock1locked unlock2locked unlock3locked )
(unlock1locked unlock2locked unlock3locked )).
2
provide axiomatization equivalent SLAF special case eff (1)
notation above. P P 0 . Recall intend primed fluents represent
value fluent immediately action taken.
364

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

eff (a)

^

Prea,f Effa,f

f P

Prea,f



^

(a[l] l)

l{f,f }

Effa,f



^

((al (af l)) l0 ) (l0 (al (af l))).

l{f,f }

Prea,f describes precondition action a. states literal l occurs precondition
literal l must held state taking a. formula Eff a,f describes effects
action a. states fluents taking action must consistent according
action model defined propositions af , af , af .
show revised axiomatization action models, eff , leads equivalent
definition SLAF within restricted action models.
0

Theorem 5.3 successful action a, SLAF [a]() Cn LP ( eff (a))[P 0 /P] .
P ROOF

See Appendix B.6. 2

5.3 Always-Successful Non-Conditional Actions
ready present algorithm learns effects actions conditional
effects. algorithm allows actions preconditions fully known. Still,
assumes filtered actions executed successfully (without failures), cannot effectively
learn preconditions (e.g., would know knew originally
preconditions seeing sequence events). sequence actions might, example,
generated expert agent whose execution traces observed.
algorithm maintains transition belief formulas special fluent-factored form, defined below. maintaining formulas special form, show certain logical consequence
finding operations performed efficiently. formula fluent-factored, conjunction formulas f f concerns one fluent, f , action propositions.
Also, every fluent, f , f conjunction positive element, negative element,
neutral one f (f explf ) (f explf ) Af , explf , explf , Af formulae action
propositions af , af , a[f ] , a[f ] , af (possibly multiple different actions). intuition
explf explf possible explanations f true false, respectively.
Also, Af holds knowledge actions effects preconditions f , knowledge
depend f current value. Note formula L(L f ) represented fluentfactored formula. Nonetheless, translation sometimes leads space blowup, maintain
representation form construction.
new learning algorithm, AS-STRIPS-SLAF4 , shown Figure 4. simplify exposition,
described case single action-observation pair, though obvious
apply algorithm sequences actions observations. Whenever action taken, first
subformulas Af , explf , explf f updated according steps 1.(a)-(c). Then,
4. AS-STRIPS-SLAF extends AE-STRIPS-SLAF (Amir, 2005) allowing preconditions actions

365

fiA MIR & C HANG

Algorithm AS-STRIPS-SLAF[ha, oi]()
Inputs:
V Successful action a, observation term o, fluent-factored transition belief formula =
f P f .

Returns: Fluent-factored transition belief formula SLAF [ha, oi]()
1. every f P
(a) Set Af (a[f ] explf ) (a[f ] explf ) Af
(b) Set explf (af (af a[f ] explf ))
(c) Set explf (af (af a[f ] explf ))

(d) |= f (f observed) seta , f (f >) (f ) Af explf
(e) |= f set f (f ) (f >) Af explf [Note: 6|= f
6|= f , nothing beyond earlier steps.]
2. Simplify (e.g., eliminate subsumed clauses ).
3. Return
a. term (f >) new explf , (f ) new explf . appear without simplification
conform Step 1a algorithm. also emphasizes syntactic nature procedure,
implicit logical simplification assumed.

Figure 4: SLAF algorithm always successful STRIPS actions.
observation received, f updated according observation according steps
1.(d)-(e). Step 2 merely indicates implementations, likely simplification
procedure used formula subsumption elimination. However, use
simplification procedure strictly necessary order theoretical guarantees
algorithm hold.
example, know nothing actions affect f (e.g., start exploration),
f = (f RU E) (f RU E) RU E. representation, SLAF [a](f )
conjunction (f explf )(f explf )Af computed step 1 Procedure AS-STRIPS-SLAF.
similar formula holds observations.
following theorem shows correctness algorithm. shows steps taken
algorithm produce result equivalent logical consequence-finding characterization
SLAF Theorem 5.3.
Theorem 5.4 SLAF[ha, oi]() AS-STRIPS-SLAF[ha, oi]() fluent-factored formula ,
successfully executed action a, observation term o.
P ROOF
See Appendix B.7 2
time space complexity procedure AS-STRIPS-SLAF given following
theorem. time guarantee, shown procedure takes linear time size input
formula. condition algorithm receives observations often enoughspecifically
366

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

every fluent observed least every k calls procedureit possible show
transition belief formula remains k-CNF indefinitely (recall k-CNF
fixed k, conjunction clauses size k). Thus, regardless length actionobservation input sequence, output AS-STRIPS-SLAF value throughout
computation k-CNF. amounts space guarantee size formula.
Theorem 5.5 following true AS-STRIPS-SLAF:
1. procedure takes linear time size input formula single action, observation
pair input.
2. every fluent every k steps observation fluent one steps,
input formula k-CNF, resulting formula (after arbitrary number
steps) k-CNF.
3. input AS-STRIPS-SLAF fluent-factored, output.
P ROOF
See Appendix B.8 2
following corollary follows immediately above.
Corollary 5.6 order process steps actions observations, AS-STRIPS-SLAF requires
(T |P|) time. Additionally,
every fluentis observed every k steps, resulting
formula always size |P| |A|k .
Corollary holds Theorem 5.5(2) guarantees bound size belief-state
formula point algorithm.

5.4 Learning Actions May Fail
many partially observable domains, decision-making agent cannot know beforehand whether
action decides take fail succeed. section consider possible action failure,
assume agent knows whether action attempts fails succeeds trying
action.
precisely, assume additional fluent OK observed agent
OK true action succeeded. failed action, case, may viewed
extra observation agent preconditions action met. is,
action failure equivalent observation
^

(a[f ] f ) (a[f ] f ).
(3)
f P

Action failures make performing SLAF operation considerably difficult. particular,
observations form (3) cause interactions fluents value particular fluent
might longer depend action propositions fluent, action propositions
fluents well. Transition belief states longer represented convenient fluentfactored formulas cases, becomes difficult devise algorithms give
useful time space performance guarantees.
367

fiA MIR & C HANG

Algorithm PRE-STRIPS-SLAF[a, o]()
Inputs: Action observation
term o. transition belief formula following facV W
tored form: = j i,j , i,j fluent-factored formula.
Returns: Filtered transition belief formula
1. |= OK:
W
(a) Set F (li ) li literals appearing precondition,
F (l) V
fluent-factored formula equivalent l (i.e., F (l) = ((l >) (l
) >) f P ((f >) (f >) >))

(b) Set i,j AS-STRIPS-SLAF[o](i,j ) i,j
2. Else (o |= OK):
(a) i,j

i. Set i,j AS-STRIPS-SLAF[P ](i,j ), P precondition
ii. Set i,j AS-STRIPS-SLAF[ha, oi](i,j )
3. i,j factored Ai,j Bi,j Bi,j contains (and only) clauses containing
fluent P.
W exists B j, B i,j B, replace
W


B

j Ai,j
j i,j
4. Simplify i,j (e.g. remove subsumed clauses)
5. Return
Figure 5: Algorithm handling action failures preconditions known.
shall demonstrate, action failures dealt tractably assume action
preconditions known agent. is, agent must learn effects actions
take, need learn preconditions actions. particular, means
action a, algorithm given access formula (more precisely, logical term) P describing
precondition action a. Clearly, algorithm need learn preconditions
actions, restrict action proposition vocabulary used describe belief states
ones forms af , af , af , longer need action propositions forms a[f ]
a[f ] .
present procedure PRE-STRIPS-SLAF5 (Figure 5) performs SLAF transition belief
formulas presence action failures actions non-conditional effects. maintains transition belief
conjunctions disjunctions fluent-factored formulas (formulas
V formulas
W
form = j i,j i,j fluent factored). Naturally, formulas superset
fluent-factored formulas.
5. PRE-STRIPS-SLAF essentially identical CNF-SLAF (Shahaf et al., 2006)

368

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

algorithm operates follows: action executes successfully (and ensuing observation received), component fluent-factored formulas i,j filtered separately
according AS-STRIPS-SLAF procedure action-observation pair (Step 2).
hand, action fails, disjunction fluent-factored formulas appended transition
belief formula (Step 1). component disjunction corresponds one possible reasons action failed (i.e., one literals occurring actions precondition). Finally,
observations accumulated learning algorithm, collapses disjunctions fluent-factored
formulas occurring belief formula together (Step 3) simplifies generally (Step 4),
decreasing total size formula. case AS-STRIPS-SLAF, simplification
steps necessary order time space guarantees hold.
proof correctness Algorithm PRE-STRIPS-SLAF relies distribution results
Section 4, Theorem 4.1 Corollary 4.3.
proceed show correctness PRE-STRIPS-SLAF. following theorem shows
procedure always returns filtered transition belief formula logically weaker
exact result, always produces safe approximation. Additionally, theorem shows
conditions Corollary 4.3, filtered transition belief formula exact result.
Theorem 5.7 following true:
1. SLAF[a, o]() |= PRE-STRIPS-SLAF[a, o]()
2. PRE-STRIPS-SLAF[a, o]() SLAF[a, o]() Corollary 4.3 holds.
P ROOF
See Appendix B.9. 2
consider time space complexity algorithm. following theorem shows
(1) procedure time efficient, (2) given frequent enough observations (as Theorem
5.5), algorithm space efficient transition belief formula stays indefinitely compact.
Theorem 5.8 following true PRE-STRIPS-SLAF:
1. procedure takes time linear size formula single action, observation pair
input.
2. every fluent observed every k steps input formula k-CNF,
filtered formula k-CNF, maximum number literals action
precondition.
P ROOF
See Appendix B.10 2
Therefore, get following corollary:
Corollary 5.9 order process steps actions observations, PRE-STRIPS-SLAF requires
(T |P|) time. every fluent
observed least
frequently every k steps, resulting
mk
formula always size |P| |A|
.

6. Building Results
section describe briefly one might extend approach include elaborate
observation model, bias, parametrized actions.
369

fiA MIR & C HANG

6.1 Expressive Observation Model
observation model use throughout paper simple: every state, fluent
observed value v, value current state. consider observation
model general.
observation model, O, set logical sentences relates propositions set Obs
fluents P. Obs includes propositions appear P, independent
previous following state (times 1 + 1) given fluents time t.
SLAF result conjoining CnLt (oO), i.e., finding prime implicates
conjoining . embed extension SLAF algorithms above,
maintain structures algorithms use. k-CNF every step
observe (at most) 1 variable, finding prime implicates easy. Embedding
transition-belief formula done conjunction prime implicates formula,
removal subsumed clauses. resulting formula still fluent factored, input
fluent factored. Then, algorithms remain applicable time complexity,
replacing ot prime implicates ot Ot .
Using Model algorithms described provide exact solution SLAF,
tuples hs, Ri solution consistent observations. compute
solution SLAF represented logical formula. use SAT solver (e.g., Moskewicz
et al., 2001) answer queries formula, checking entails f , action
fluent f . would show consistent models action makes f value TRUE.
number variables result formula always independent , linear |P|
algorithms. Therefore, use current SAT solvers treat domains 1000
features more.
Preference Probabilistic Bias Many times information leads us prefer
possible action models others. example, sometimes assume actions change
fluents, suspect action (e.g., open-door) affect features (e.g.,
position) normally. represent bias using preference model (e.g., McCarthy, 1986;
Ginsberg, 1987) probabilistic prior transition relations (e.g., Robert, Celeux, & Diebolt,
1993).
add bias end SLAF computation, get exact solution
compute effect bias together logical formula efficiently. Preferential biases
studied fit easily result algorithms (e.g., use implementations
Doherty, Lukaszewicz, & Szalas, 1997, inference bias).
Also, algorithms inference probabilistic bias logical sentences emerging
used (Hajishirzi & Amir, 2007). There, challenge enumerate
tentative models explicitly, challenge overcome success work Hajishirzi
Amir (2007) similar task filtering. use algorithms apply probabilistic
bias resulting logical formula.
example, given probabilistic graphical model (e.g., Bayesian Networks) set propositional logical sentences, consider logical sentences observations. approach,


logical sentence gives rise characteristic function (
x ) 1
x satisfies
0 otherwise. conjunction clauses get set functions (one per clause). Thus,
inference combined probabilistic-logical system probabilistic inference. example,
370

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

one consider variable elimination (e.g., Dechter, 1999) additional potential
functions.
Parametrized Actions many systems situations natural use parametrized actions.
action schemas whose effect depend parameters, definition applies
identically instantiations.
example, move(b, x, y) action moves b position x position y,
b, x, parameters action. common planning systems (e.g., STRIPS,
PDDL, Situation Calculus). complete treatment parameterized actions outside scope
paper, give guidelines generalization current approach actions.
Consider domain set fluent predicates universe named objects. propositional fluents defined domain ground instantiations predicate fluents. SLAF
work set propositional fluents instantiated actions manner


rest paper. action propositions a( X )lG instantiated every vector object names


X.




)lG .
different treatment comes additional axioms say
x,
.a(
x )lG a(
Inference transition belief state axioms able join information collected
different instantiations actions. expect thorough treatment
able provide efficient algorithms whose time complexity depend number action
schemas instead number instantiated actions.
Several approaches already start address problem, including work Nance, Vogel,
Amir (2006) filtering work Shahaf Amir (2006) SLAF.

7. Experimental Evaluation
Previous sections discussed problem settings consider algorithms solutions. showed modifying traditional settings learning dynamic partially observable
domains important. Determinism alone lead tractability, additional assumptions simple, logical action structure bounded 0 frequency observations fluents
do. Specifically, far showed time space computing SLAF length-T time
sequence n fluents polynomial n.
section considers practical considerations involved using SLAF procedures.
particular, examines following questions:
much time space SLAF computations take practice?
much time required extract model logical formula result
SLAF procedures?
quality learned model (taking arbitrary consistent model)? far
true (generating) model?
conditions algorithms correctness hold practice?
learned model used successful planning execution? learning
procedures fit planning execution?
implemented algorithms ran experiments AS-STRIPS-SLAF following domains taken 3rd International Planning Competition (IPC): Drivelog, Zenotravel,
Blocksworld, Depots (details domains learning results appear Appendix C).
371

fiA MIR & C HANG

experiment involves running chosen algorithm sequence randomly generated
action-observation sequences 5000 steps. Information recorded every 200 steps.
random-sequence generator receives correct description domain, specified
PDDL (Ghallab, Howe, Knoblock, McDermott, Ram, Veloso, Weld, & Wilkins, 1998; Fox & Long,
2002) (a plannig-domain description language), size domain, starting state. (The
size domain number propositional fluents it. set specification
number objects domain number arity predicates domain.) generates
valid sequence actions observations domain starting state, i.e., sequence
consistent input PDDL generator actions may fail (action failure
consistent PDDL action attempted state canot execute).
experiments, chose observations follows: every time step select 10
fluents uniformly random observe. applied additional restrictions (such making sure
fluent observed every fixed k steps).
SLAF algorithm receives sequences actions observations, domain
information otherwise (e.g., receive size domain, fluents, starting state,
PDDL). starting knowledge algorithm empty knowledge, TRUE.
domain ran algorithm different numbers propositional fluents (19 250
fluents). collected time space taken SLAF computation plotted
function input-sequence length (dividing total computation time steps).
time space results shown Figures 6, 7, 8, 9. graphs broken different
domains compare time space taken different domain sizes. time SLAF-time
without CNF simplification (e.g. remove subsumed clauses)
much time space SLAF computations take practice? answer first
question now. observe figures time per step remains relatively constant
throughout execution. Consequently, time taken perform SLAF different domains grows
linearly number time steps. Also, see time SLAF grows domain
size, scales easily moderate domain sizes (1ms per step SLAF domains 200 fluents).
much time required extract model logical formula result
SLAF procedures? SLAF procedures return logical formula sequence actions
observations. need apply work extract candidate (consistent) model
formula. computation done SAT solver CNF formulas.
quality learned model (taking arbitrary consistent model)? far
true (generating) model? sometimes many possible models, little
bias must consider possible likely. decided introduce one
bias, namely, actions instances actions schemas. Thus, actions assumed
effect parameters (or objects), given properties parameters. Thus,
actions effects assumed independent identity parameter.
So, vanilla implementation, propositions look like
((STACK
((STACK
((STACK
((STACK
etc.

E
E



G)
G)
B)
A)

CAUSES
CAUSES
CAUSES
CAUSES

(ON
(ON
(ON
(ON

E
G



G))
E))
B))
A))

372

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

SLAF Time/Step: Blocksworld Domain
1.6

1.4

1.2

Time (ms)

1

19 fluents
41 fluents
71 fluents
131 fluents
209 fluents

0.8

0.6

0.4

0.2

0
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

SLAF Time/Step: Depots Domain
1.6

1.4

1.2

Time (ms)

1

58 fluents
94 fluents
138 fluents
190 fluents
250 fluents

0.8

0.6

0.4

0.2

0
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

Figure 6: SLAF-time without CNF simplification domains Blocksworld Depots

Instead, replace ground propositions like schematized propositions:
((STACK ?X ?Y) CAUSES (ON ?X ?Y))
((STACK ?X ?Y) CAUSES (ON ?Y ?X))
((STACK ?X ?Y) CAUSES (ON ?X ?Y))
etc.
Thus, belief-state formula looks something like:
373

fiA MIR & C HANG

SLAF Time/Step: Driverlog Domain
1.8
1.6
1.4

Time (ms)

1.2
31 fluents
76 fluents
122 fluents
186 fluents
231 fluents

1
0.8
0.6
0.4
0.2
0
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

SLAF Time/Step: Zeno-Travel Domain
0.8

0.7

0.6

Time (ms)

0.5
58 fluents
91 fluents
134 fluents

0.4

0.3

0.2

0.1

0
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

Figure 7: SLAF-time without CNF simplification domains Driverlog Zeno-Travel

(AND
(AND
(OR (ON E G)
(OR ((STACK ?X ?Y) CAUSES (NOT (ON ?X ?Y)))
(AND ((STACK ?X ?Y) KEEPS (ON ?X ?Y))
(NOT ((STACK ?X ?Y) NEEDS (ON ?X ?Y))))))
...
374

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

SLAF Space: Blocksworld Domain
300K

Space (#lisp symbols)

250K

200K
19 fluents
41 fluents
71 fluents
131 fluents
209 fluents

150K

100K

50K

K
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

SLAF Space: Depots Domain
160K

140K

Space (#lisp symbols)

120K

100K

58 fluents
94 fluents
138 fluents
190 fluents
250 fluents

80K

60K

40K

20K

K
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

Figure 8: SLAF space domains Blocksworld Depots

example fragment model (the complete output given Appendix C) consistent
training data
Blocksworld domain:
* 209 fluents
* 1000 randomly selected actions
* 10 fluents observed per step
* "schematized" learning
375

converting CNF
clause count: 235492
variable count: 187
adding clauses
calling zchaff

fiA MIR & C HANG

SLAF Space: Driverlog Domain
250K

Space (#lisp symbols)

200K

150K

31 fluents
76 fluents
122 fluents
186 fluents
231 fluents

100K

50K

K
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

SLAF Space: Zeno-Travel Domain
60K

Space (#lisp symbols)

50K

40K
58 fluents
91 fluents
134 fluents

30K

20K

10K

K
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

Figure 9: SLAF space domains Driverlog Zeno-Travel

* 1:1 precondition heuristics

parsing result
SLAF time: 2.203
Inference time: 42.312
Learned model:

(UNSTACK NEEDS (NOT (CLEAR ?UNDEROB)))
(UNSTACK NEEDS (CLEAR ?OB))
(UNSTACK NEEDS (ARM-EMPTY))
376

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
...

NEEDS (NOT (HOLDING ?OB)))
NEEDS (ON ?OB ?UNDEROB))
CAUSES (CLEAR ?UNDEROB))
CAUSES (NOT (CLEAR ?OB)))
CAUSES (NOT (ARM-EMPTY)))
CAUSES (HOLDING ?OB))
CAUSES (NOT (ON ?OB ?UNDEROB)))
KEEPS (ON-TABLE ?UNDEROB))
KEEPS (ON-TABLE ?OB))
KEEPS (HOLDING ?UNDEROB))
KEEPS (ON ?UNDEROB ?UNDEROB))
KEEPS (ON ?OB ?OB))
KEEPS (ON ?UNDEROB ?OB))

Sometimes, multiple possible schematized propositions correspond ground
action proposition, case disjoin propositions together replacement
(i.e., single ground propositional symbol gets replaced disjunction schema propositions).
replacement simple procedure, one effective deriving information fewer steps also speeding model finding SLAF formula. implemented
run SLAF runs. One could SAT-solving portion algorithm,
equivalent result.
Regarding latter, ran scaling issues SAT solver (ZChaff , Moskewicz et al.,
2001; Tang, Yinlei Yu, & Malik, 2004) common lisp compiler large experiments. got
around issues applying replacement scheme above, thus reducing greatly number
variables SAT solver handles.
Another issue ran SAT solver tended choose models blank preconditions (the sequences used experiments include action failure, preconditions never eliminated algorithm). add bias extracted action model,
added axioms following form:
(or (not (,a causes ,f)) (,a needs (not ,f)))
(or (not (,a causes (not ,f))) (,a needs ,f))
axioms state action causes fluent f hold, requires f hold
precondition (similarly, analagous axiom f ). Intuitively, axioms cause
sat solver favor 1:1 action models. got idea heuristic work Wu,
Yang, Jiang (2007), uses somewhat similar set axioms bias results terms
learning preconditions. Clearly, axioms dont always hold, results, one see
learned preconditions often inaccurate.
inaccuracies learned action models reasonable. example, fluent
never changes course action sequence, algorithm may infer arbitrary action
causes fluent hold.
conditions algorithms correctness hold practice? scenarios
report here, conditions guarantee correctness algorithms hold. experiments assumed main conditions algorithms hold, namely, actions
377

fiA MIR & C HANG

deterministic preconditions. enforce observations every fluent
every (fixed) k steps. latter condition necessery correctness algorithm,
necessary guarantee polynomial-time computation. experiments verify necessary practice, indeed case algorithms polynomial-time guarantee
modified observation
earlier work Hlubocky Amir (2004) included modified version AS-STRIPSSLAF architecture tested suite adventure-game-like virtual environments
generated random. include arbitrary numbers places, objects various kinds,
configurations settings those. There, agents task exit house, starting
knowledge state space, available actions effects, characteristics objects.
experiments show agent learns effects actions efficiently. agent
makes decisions using learned knowledge, inference resulting representation fast
(a fraction second per SAT problem domains including 30 object, modes,
locations).
learned model used successful planning execution? learning
procedures fit planning execution? learned model used planning
translating PDDL. However, model always correct one domain,
plan may feasible may lead required goal. cases, interleave
planning, execution, learning, described work Chang Amir (2006). There,
one finds short plan consistent action model, executes plan, collects observations,
applies SLAF those. plan failure detected (e.g., goal achieved),
results Chang Amir (2006) guarantee joint planning-execution-learning procedure
would reach goal bounded amount time. bounded time fact linear length
longest plan needed reaching goal, exponential complexity action
model need learn.

8. Comparison Related Work
HMMs (Boyen & Koller, 1999; Boyen et al., 1999; Murphy, 2002; Ghahramani, 2001) used
estimate stochastic transition model observations. Initially, expected compare
work HMM implementation Murphy (2002), uses EM (a hill-climbing approach).
Unfortunately, HMMs require explicit representation state space, smallest domain (31 features) requires transition matrix (231 )2 entries. prevents initializing HMMs
procedures current computer.
Structure learning approaches Dynamic Bayes Nets (DBNs) (e.g., Ghahramani & Jordan,
1997; Friedman, Murphy, & Russell, 1998; Boyen et al., 1999) use EM additional approximations (e.g., using factoring, variation, sampling), tractable. However, still
limited small domains (e.g., 10 features , Ghahramani & Jordan, 1997; Boyen et al., 1999),
also unbounded errors discrete deterministic domains, usable settings.
simple approach learning transition models devised work Holmes
Charles Lee Isbell (2006) deterministic POMDPs. There, transition observation models
deterministic. approach close represents hidden state possible models using finite structure, Looping Prediction Suffix Tree. structure seen
related representation grow models relate action histories possible
transition models. work interactions realized recursive structure
378

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

transition-belief formula built AS-STRIPS-SLAF, e.g.,
(af (af a[f ] explf ))
explf refers similar formula created previous time step.
main difference draw work Holmes Charles
Lee Isbell (2006) latter refers states explicitly, whereas work refers features.
Consequently, representation (provably) compact procedures scale larger
domains theoretically practice. Furthermore, procedure provably maintains reference possible models data insufficient determine single model, whereas
work Holmes Charles Lee Isbell (2006) focuses limit case enough information determining single consistent model. down-side, procedure consider
stochasticity belief state, remains area development.
similar relationship holds work Littman, Sutton, Singh (2002).
work, model representation given size linear number states. model,
predictive state representation (PSR), based action/observation histories predicts behavior based histories. work prefers low-dimensional vector basis instead featurebased representation states (one traditional hallmarks Knowledge Representation
approach). necessary correspondence basis vectors intuitive features real world necessarily. enables representation world closely
based behavior.
Learning PSRs (James & Singh, 2004) nontrivial one needs find good lowdimensional vector basis (the stage called discovery tests). stage learning PSRs requires
matrices size (n2 ), states spaces size n.
work advances line work providing correct results time polylogarithmic number states. Specifically, work learns (deterministic) transition models
polynomial time state features, thus taking time O(poly(log n)).
Reinforcement Learning (RL) approaches (Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1996)
compute mapping world states preferred actions. highly intractable
partially observable domains (Kaelbling, Littman, & Cassandra, 1998), approximation (e.g.,
Kearns, Mansour, & Ng, 2000; Meuleau, Peshkin, Kim, & Kaelbling, 1999; Even-Dar, Kakade, &
Mansour, 2005; McCallum, 1995) practical small domains (e.g., 10-20 features)
small horizon time .
contrast HMMs, DBNs, RL, algorithms exact tractable large domains (>
300 features). take advantages properties common discrete domains, determinism,
limited effects actions, observed failure.
Previous work learning deterministic action models AI-Planning literature assumes
fully observable deterministic domains. learn parametrized STRIPS actions using, e.g., version spaces (Gil, 1994; Wang, 1995), general classifiers (Oates & Cohen, 1996), hill-climbing
ILP (Benson, 1995). Recently, work Pasula et al. (2004) gave algorithm learns stochastic actions conditional effects. work Schmill, Oates, Cohen (2000) approximates
partial observability assuming world fully observable. apply partially
observable learning problems (sometimes) using space belief states instead world states,
increases problem size exponentially, practical problem.
Finally, recent research learning action models partially observable domains includes
works Yang, Wu, Jiang (2005) Shahaf Amir (2006). works Yang et al.
379

fiA MIR & C HANG

(2005), example plan traces encoded weighted maximum SAT problem,
candidate STRIPS action model extracted. general, may many possible action models
given set example traces, therefore approach nature approximate (in contrast
ours, always identifies exact set possible action models). work also introduces
additional approximations form heuristic rules meant rule unlikely action models.
work Shahaf Amir (2006) presents approach solving SLAF using logicalcircuit encodings transition belief states. approach performs tractable SLAF general deterministic models present Section 5, also requires SAT solvers
logical circuits. SAT solvers optimized nowadays comparison CNF SAT
solvers, overall performance answering questions SLAF lower ours.
importantly, representation given Shahaf Amir (2006) grows (linearly) number
time steps, still hinder long sequences actions observations. comparison,
transition belief formula bounded size independent number time steps
track.
encoding language, LA typical methods software hardware verification
testing. Relevant books methods (e.g., Clarke, Grumberg, & Peled, 1999) closely related
representation, results achieve applicable vice versa.
main distinction draw work done Formal Methods (e.g., Model
Checking Bounded Model Checking) able conclude size bounds logical formulas involved computation. OBDDs used success Model Checking,
CNF representations used success Bounded Model Checking, little
bounds sizes formulas theory practice. conditions available AI applications
used current manuscript deliver bounds yield tractability scalability results
theoretical practical significance.
Interestingly, methods use Linear Temporal Logics (LTL) cannot distinguish
happen actually happens (Calvanese, Giacomo, & Vardi, 2002). Thus, cannot
consider causes occurence. method similar consider alternate
futures state explicitly. However, use extended language, namely L , makes
alternatives explicit. allows us forego limitations LTL produce needed result.

9. Conclusions
presented framework learning effects preconditions deterministic actions
partially observable domains. approach differs earlier methods focuses determining exact set consistent action models (earlier methods not). showed several
common situations done exactly time polynomial (sometime linear) number time steps features. add bias compute exact solution large domains
(hundreds features), many cases. Furthermore, show number action-observation
traces must seen convergence polynomial number features domain.
positive results contrast difficulty encountered many approaches learning
dynamic models reinforcement learning partially observable domains.
results presented promising many applications, including reinforcement
learning, agents virtual domains, HMMs. Already, work applied autonomous
agents adventure games, exploration guided transition belief state compute
380

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

information gain criteria. future plan extend results stochastic domains,
domains continuous features.

Acknowledgments
wish thank Megan Nance providing code samples sequence generator,
also wish thank Dafna Shahaf encouraging collaboration enhanced development understanding results. first author also wishes acknowledge stimulating
discussion Brian Hlubocky related topics. wish acknowledge support DAF
Air Force Research Laboratory Award FA8750-04-2-0222 (DARPA REAL program). second
author wishes acknowledge support University Illinois Urbana-Champaign, College
Engineering fellowship. Finally, first author acknowledges support joint Fellowship
(2007-2008) Center Advanced Studies Beckman Institute University
Illinois Urbana-Champaign.
Earlier versions results manuscript appeared conference proceedings (Amir,
2005; Shahaf et al., 2006).

Appendix A. Representation Domain Descriptions
transition relation RM interpretation LA defined Section 3.1 way
similar interpretation Domain Descriptions. Domain Descriptions common method
specifying structured deterministic domains (Fagin, Ullman, & Vardi, 1983; Lifschitz, 1990;
Pednault, 1989; Gelfond & Lifschitz, 1998). methods equivalent contexts
include Successor-State Axioms (Reiter, 2001) Fluent Calculus (Thielscher, 1998).
relevance influence work merit separate exposition relationship work.
domain description finite set effect rules form causes F G describe effects actions, F G state formulas (propositional combinations fluent
names). say F head effect G precondition rule. write
causes F , causes F TRUE. denote ED (a) set effect rules action
A. effect rule e, let Ge precondition Fe effect. Rule e active state s,
|= Ge (s taken interpretation P).
Every domain description defines unique transition relation R (s, a, s0 ) follows.
V
Let F (a, s) conjunction effects rules active a, i.e., {Fe | e
ED (a), |= Ge }. set F (a, s) = RU E rule active s.
Let I(a, s) set fluents affected s, i.e., I(a, s) = {f P | e
ED (a) (s |= Ge ) (f
/ L(Fe ))}.
Define (recalling world states sets fluents)
0



0 (s I(a, s)) = (s I(a, s))
RD = hs, a,
(4)
s0 |= F (a, s)

Thus, action effect FALSE s, cannot execute s.
definition applies inertia (a fluent keeps value) fluents appear active rule.
contexts useful specify inertia explicitly extra effect rules form
keeps f G, fluent f P. shorthand writing two rules causes f f G
causes f f G. includes inertia (keeps) statements, say
complete domain description.
381

fiA MIR & C HANG

Example A.1 Consider scenario Figure 2 assume actions observations occur
Figure 10. Actions assumed deterministic, conditional effects, preconditions
must holds execute successfully. Then, every action affects every fluent either negatively,
positively, all. Thus, every transition relation complete domain description
includes rules form causes l keeps l, l fluent literal (a fluent
negation).
Time step
Action
Location
Bulb
Switch

1

go-W

E
?
sw

2
E
lit
?

go-E

3
E
?
sw

sw-on

4
E
?
sw

go-W

5
E
lit
?

go-E

6
E
?
sw

Figure 10: action-observation sequence (table entries observations). Legend: E: east; E:
west; lit: light on; lit: light off; sw: switch on; sw: switch off.

Consequently, every transition relation R completely defined domain description
(viewing tuple set elements)




causes E, causes sw, causes lit,

causes E causes sw causes lit
R



9 keeps E
8
keeps lit
keeps sw
> go-W >


>
<

>
=

go-E >
>
>
;
: sw-on >

Say initially know effects go-E, go-W, know sw-on does. Then,
transition filtering starts product set R (of 27 possible relations) possible 2 3
states. Also, time step 4 know world state exactly {E, lit, sw}. try sw-on
get F ilter[sw-on](4 ) includes set transition relations
transition relations projecting state {E, lit, sw} appropriate choice S.
receive observations o5 = E sw time step 5, 5 = F ilter[o5 ](F ilter[sw-on](4 ))
removes transition belief state relations gave rise E sw. left
transition relations satisfying one tuples




sw-on causes lit


sw-on causes E,
sw-on causes lit
sw-on causes sw
sw-on keeps E


sw-on keeps lit

Finally, perform action go-W, update set states associated every
transition relation set pairs 5 . receive observations time step 6,
conclude 6 = F ilter[o6 ](F ilter[go-W](5 )) =





sw-on keeps E,
sw-on causes E,

+
+ *E
*E










sw-on causes sw,
sw-on causes sw,
lit
lit
(5)
,
,
,
sw-on causes lit,
sw-on causes lit,










sw
sw





go-E...
go-E...
2
382

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

Appendix B. Proofs
B.1 Proof Lemma 3.1: Consequence Finding Existential Quantification
P ROOF
Consider CNF form . Suppose clauses containing literal x x
1 , . . . , x 1 , . . . , clauses. Suppose clauses containing literal x
x 1 , . . . , x b . V
Suppose clauses
containing x x 1 , . . . , c . note
V
L()\{x}
Cn
() ( 1ic ) ( 1ia,1jb j ) (the formula produced adding
resolvents variable x removing clauses belonging L(L()\{x})), since
resolution complete consequence finding.
Necessity. (x. |= CnL()\{x} ()) Consider model x.. definition,
extended L() (i.e., assigning value m(x)) m() = 1. Extend
case. suppose contradiction model Cn L()\{x} (). cannot
case m(k ) = 0 k, m() = 0, contradiction. Then, must case
m(i j ) = 0 1 1 j b. Therefore, m(i ) = 0 m(j ) = 0.
model , m(x ) = 1 m(x j ) = 1. Thus either m(i ) = 1
m(j ) = 1, contradiction.
Sufficiency. (CnL()\{x} () |= x.) Consider model CnL()\{x} (). Suppose
contradiction m(x.) = 0. is, extended L(), m() = 0. Now, extend
L() m(x) = 0. cannot case m(k ) = 0 k since models
CnL()\{x} (). m(x) = 1, cannot case m(x j ) = 0 j.
Therefore m(x ) = 0 1 a. Therefore, m(i ) = 0. m(i j ) = 1
1 j n, must m(j ) = 1 j. alter m(x) = 1,
satisfies , contradiction. 2
B.2 Proof Theorem 3.3
P ROOF
sides equality relation sets state-transition-relation pairs. show
two sets elements. show first left-hand side equality
contained right-hand side.
Take hs0 , Ri SLAF [a]({hs, Ri | hs, Ri satisfies }). Definition 2.3
{s | hs, Ri satisfies } hs, a, s0 R. words,
hs, Ri satisfies hs, a, s0 R.
prove hs0 , Ri satisfies SLAF0 [a]() need show Teff (at ) model
RM = R s0 interprets P 0 . Let interprets P, s0 interprets P 0 , MR
interpreting LA previous
section. interpretation

Wsatisfy formula
V
V
one conjuncts lF ,GG ((alG G) l0 ) lF (l0 ( GG (alG G))) falsified.
cannot case choice s.
Assume contradiction (alG G) l0 ) fails l. Then, (alG G) hold l0
FALSE. portion interprets LA built according MR , R. Since hs, R, s0
know s0 satisfies l, construction MR . contradicts l 0 FALSE
(M interprets P 0 according s0 ), therefore conclude (alG G) l0 ) every l0 .
W
Similarly, assume contradiction (l 0 ( GG (alG G))) fails l. Then, l 0 holds
s0 , als fails. Again, way constructed MR must als takes value
corresponds l 0 truth value s0 . Thus, must als takes value TRUE ,
done first direction.
383

fiA MIR & C HANG

opposite direction (showing right-hand side contained left-hand side), take
hs0 , Ri satisfies SLAF [a](). show
hs0 , Ri SLAF [a]({hs, Ri | hs, Ri satisfies }).
hs0 , Ri |= SLAF [a]() implies (Corollary 3.2) hs 0 , R, si |=
Teff (a) (s0 , R, interpreting P 0 , LA , P, respectively). similar argument one give
first part shows hs, a, s0 R, hs0 , Ri SLAF [a]({hs, Ri | hs, Ri satisfies }).
2
B.3 Proof Theorem 4.1: Distribution SLAF Connectives
first part, show sets models SLAF [a]( ) SLAF [a]()
SLAF [a]() identical.
Take model SLAF [a]( ). Let 0 model SLAF [a](M 0 ) =
. Then, 0 model model . Without loss generalization, assume
model . Thus, |= SLAF [a](), follows model SLAF [a]()
SLAF [a]().
direction, take model SLAF [a]() SLAF [a](). Then, model
SLAF [a]() model SLAF [a](). Without loss generalization assume
model SLAF [a](). Take 0 model = SLAF [a](M 0 ). So, 0 |= .
follows |= SLAF [a]( ).
similar argument follows case. Take model SLAF [a]( ). Let 0
model SLAF [a](M 0 ) = . Then, 0 model . Thus, |=
SLAF [a]() |= SLAF [a](). follows model SLAF [a]()SLAF [a]().
2
B.4 Proof Theorem 4.5: Closed form SLAF belief state formula
P ROOF SKETCH
follow characterization offered Theorem 3.3 Formula (1).
take Teff (a, t) resolve literals time t. resolution guaranteed generate
set consequences equivalent CnLt+1 (t Teff (a, t)).
V
Assuming , Teff (a, t) logically equivalent Teff (a, t)| = lF ,GG,G|= ((alG Gt )
V
lt+1 ) lF ,GG,G|= (lt+1 (Gt alG )). follows two observations. First, notice
implies G G G 6|= get Gt alG (alG Gt ) lt+1 (the
antecedent notWhold, formula true). Second, notice that,
V second part
original Teff (a, t), ( GG,G|= (alG Gt )) equivalent (assuming ) ( GG,G|= (Gt alG )).
Now, resolving literals time Teff (a, t)| consider
resolutions clauses (Gt term) form alG Gt lt+1 clauses form
lt+1 (Gt alG ) other. yields equivalent
^


_

[ lit+1
i=1
G1 , ...,
WGm G
|= im Gi
l1 , ..., lm F


_
li
alGi )]
(aG

i=1

eliminate W
literals time resolve together sets clauses matching Gi |= Gi . formula encodes resulting clauses chosen
384

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

li

set G1 , ..., Gm chosen set literals l1 , ..., lm . reason including (al
Gi aGi )
always choose clause Gi , li specific type (either one includes alG
one produces al
G.
Finally, get formula theorem aFG aF
G (G characterizes exactly one
state S), fact one set G1 , ..., Gm stronger rest (it entails
rest) G1 , ..., Gm complete terms. set complete fluent assignments G
satisfy . 2

B.5 Proof Theorem 5.1: Closed form k affected fluents
P ROOF SKETCH
literal l clause C conjunction Theorem 4.5, aggregate
action propositions single action proposition alGl , G disjunction complete
preconditions l C (notice cannot ls negation C tautology).
Lemma B.2 shows Gl equivalent fluent term. First prove restricted Lemma
B.1.
W
li
t+1
) clause formula Theorem 4.5, let Gl =
Lemma B.1 Let C =
aG
i=1 (li

W
{Gi | li = l, Gi |= li }, literal l6 . Assume effect deterministic
one case term precondition (if case hold, nothing changes). Then, G l
equivalent term.
P ROOF
Gl disjunction complete state terms Gi , represents set states
corresponds Gi s. intuition apply
W
lit+1 alGi
C
W
Vm i=1
t+1
( i=1 alGi ) (
i=1 li )
0
l
Gl l C
C 0 part C affect l. reason complete terms G know
li
l

al
Gi aGi . Thus, choice G includes conditions l
changes, assume precondition alGl .
compute action model l updating copy G l . Let li fluent literal,
set Glt = Gl .
1. Gl2 Glt Gl2 |= li , terms Glt include li . Thus,
Glt Glt li , alGl alGl li . Thus, add li conjunct Glt .
2. Otherwise, Gl2 Glt Gl2 |= li , terms Glt include li .
Thus, Glt Glt li , alGl alGl li . Thus, add li conjunct Glt .
3. Otherwise, know li li value fluent l changes l =
TRUE. Since assume value l changes single case preconditions
(i.e., conditional effects - either succeeds change, fails change),
li cannot part preconditions, i.e., every term G Glt replace li
li vice versa, precondition would still entail l action. Thus,
alGl alGl \{l } , Glt \ {li } result replacing li li TRUE.






6. related literal l proof above.

385

fiA MIR & C HANG

result replacements term Glt consistent (it consistent
original Gl ) satisfies 1 case |= alGl alGl . 2


W
li
t+1
cl =
Lemma B.2 Let C =
aG
clause formula Theorem 4.5, let G
i=1 li

W
{Gi | li = l}, literal l. Assume effect deterministic one case
cl equivalent
term precondition (if case hold, nothing changes). Then, either G
cl
term, C subsumed another clause entailed formula G
equivalent term.
Gl .

cl
P ROOF
Consider Gl Lemma B.1, let Gl1 complete fluent term G
l
l
l
Thus, G1 |= l. Let Gt term equivalent G according Lemma B.1.
Clause C equivalent alGl al
.... However, alGl already asserts change l l
Gl




1

result action a, al
asserts change different condition l l. Thus,
Gl
1

. get subsuming clause C 0 = C \ al
. way
1 case |= alGl al
Gl
Gl


1

1

remove C literals alGi Gi Gl .

cl . However, clause C
process left clause C Gl G
form Theorem 5.1 Gi missing. represent
theorem? must allow Gi missing.
2
Proof theorem continues Thus, representation C 0 (the new clause) takes space O(n2 )
(each propositional symbol encoded O(n) space, assuming constant encoding every
fluent p P).
However, number propositional symbols still large (there O(2 n ) fluent terms,
encoding still requires O(2n n) (all preconditions effects) new propositional symbols.
notice limit attention clauses C k literals l whose action
proposition alGl satisfies |= Gl l. k propositions C,
V
W
l
say {alGi l }ik0 , C ( ki=1 alGi l ) aGk+1
... im lit+1 , always subsumed
l


k+1
V
l
.

latter


sentence
always true, assume change
( ki=1 alGi l ) aGk+1
l


k+1

k fluents (aGk+1
asserts lk+1 remains same, alGi l asserts li changes
lk+1

one conditions satisfied Gli ).
V
l
(which state k effects)
Using clauses form ( ki=1 alGi l ) aGk+1
l
l



k+1

l

resolve away aGj l j > k every clause C original conjunction. Thus,
j
W
V
left clauses form C ( ki=1 alGi l ) im lit+1 . Now, since choice literals

l1 , ..., lk independent rest clause, every polarity
fluents, get clauses resolve (on combinations literals) (and
subsumed by)
k
_
^
lit+1
(6)
C ( alGi l )
i=1



386

ik

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

Thus, get conjunction clauses form (6), G li (i k) fluent term. So,
conjunction clauses Theorem 4.5 equivalent conjunction clauses
clause 2k literals. Thus, space required represent clause 2kn.
Finally, use fact every action dependent k fluents. Every proposition asserts no-change li equivalent conjunction literals stating none
possible k preconditions consistent affect li . example alli1 ...lk lu implies

alli1 ...lk alli1 ...lk1 lu .... Similarly, one elements conjunction implies

alli1 ...lk lu . 2

B.6 Proof Theorem 5.3: Equivalent Restricted Definition Action Axioms
P ROOF
Let 0 = P.(eff (a)). claim successful actionS
a, SLAF [a]()
0
[P 0 /P] . see this, consider model 0 . valuation fluents f P L0f define
transition relation R valuation fluents P 0 define state s0 hs0 , Ri 0 .
definition 0 , hs0 , Ri 0 exists hs, Ri eff (a) satisfied. Finally note eff (a) satisfied preconditions action met
s0 consistent effects action applied s. is, eff (a) satisfied
hs, a, s0 R. Together, observations Corollary 3.2 yield theorem. 2
B.7 Proof Theorem 5.4: AS-STRIPS-SLAF Correct
P ROOF
Let shorthand notation C() denote C() CnL ()[P 0 /P] .
definition, SLAF [ha, oi]() SLAF [o](SLAF [a]()). Theorem 5.3,
SLAF [a]() C( eff (a)). formula equivalent C( eff (a)) may generated
resolving
V fluents P (by following procedure proof Lemma 3.1). Suppose
= f P f fluent-factored form. may rewrite C( eff (a)) as:
0



SLAF [a]()










^

f P

^

f P

^

f P

^

f P



C(Prea,f ) C(Effa,f ) C(f )

(7)



C(Prea,f Effa,f )


C(f Prea,f )


C(f Effa,f )

equivalence holds resolvents generated resolving literals P C(
eff (a)) still generated formula. is, pair clauses possibly resolved together (where fluent P resolved out) eff (a) generate new
consequence C ( eff (a)) appear together one C () components (7).
387

fiA MIR & C HANG

every clause eff (a) contains one literal P, see possible consequences
generated.
note Effa,f may rewritten follows:
Effa,f



^

((al (af l)) l0 ) (l0 (al (af l)))

(8)

l{f,f }



^

(l (l0 al )) (l0 (al af ))

l{f,f }

straightforward verify equivalence rewritten formula original formula. Note
performing rewriting, may discard clauses form f af af , must
true every consistent model (given axioms described Section 5.2).
consider consequences generated component (7). may compute
consequences performing resolution. C(Pre a,f ) a[f ] a[f ] ,
may discard clauses inconsistent action models violate clause.
definition fluent-factored formulas, C(f ) Af . Next, remaining components
computed straightforwardly:
^
C(Effa,f )
(l0 al af )
l{f,f }

C(f Prea,f ) C(f ) C(Prea,f )

^

(a[l] expll )

l{f,f }

C(Prea,f Effa,f ) C(Prea,f ) C(Effa,f )

^

(l0 al a[l] )

l{f,f }

C(f Effa,f ) C(f ) C(Effa,f )

^

(l0 al expll )

l{f,f }

Finally, difficult see steps (a)-(c) procedure sets following formula (in
fluent-factored form):
^
^
SLAF [a]()
Af
(a[l] expll ) (l0 al (af a[l] expll )
f P

l{f,f }

Now, note SLAF [o](SLAF [a]()) SLAF [a]() o. Note term,
SLAF [a]() made fluent-factored performing unit resolution. exactly
steps 1.(d)-(e) do. 2
B.8 Proof Theorem 5.5: AS-STRIPS-SLAF Complexity
P ROOF
Consider size formula returned AS-STRIPS-SLAF. Overview: note
formula i-CNF, integer > 0, filtered formula one step
(i + 1)-CNF. Then, note every observation fluent f resets f -part belief state
formula 1-CNF (thus, = 1).
Details: first part, call procedure appends
one literal existing clauses formula, new clauses length k + 1
388

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

generated. Additionally, every fluent observed every k steps, transition belief
formula stays k-CNF (i.e., indefinitely compact). existing clauses may grow
length (1 literal per timestep) augmented steps 1.(a)-(c), appropriate fluent
observed steps 1.(d)-(e), clauses stop growing. Finally, easy see
steps 1.(a)-(e) performed polynomial time. 2
B.9 Proof Theorem 5.7: PRE-STRIPS-SLAF Correct
P ROOF
Consider semantics SLAF filtering STRIPS action known precondition. case action failure, world filtered transition belief state
world meet action precondition (and satisfies observation). Clearly, step 1
algorithm performs filtering conjoining belief formula negation action
precondition (converted logically equivalent disjunction fluent-factored formulas).
case action success, filtering performed first removing worlds
satisfy action precondition (so remaining worlds, action executable)
filtering remaining worlds using algorithm AS-STRIPS-SLAF. Moreover, Theorem 4.1
Corollary 4.3 follows filtering formula performed filtering
subformulas i,j separately. Furthermore, SLAF[ha, oi]() |= PRE-STRIPS-SLAF[ha, oi](),
PRE-STRIPS-SLAF[ha, oi]() SLAF[ha, oi]() conditions Corollary 4.3.
filtering subformula performed steps 2 3 algorithm.
Finally, note steps 3 4 serve simplify belief formula produce logically
equivalent formula. 2
B.10 Proof Theorem 5.8: PRE-STRIPS-SLAF Complexity
P ROOF
Note call AS-STRIPS-SLAF subformula takes time linear size
subformula, steps involving AS-STRIPS-SLAF performed linear time.
Thus total time complexity linear. Additionally, note every fluent observed every
k steps, every fluent-factored subformula i,j belief formula k-CNF,
theorem Amir
W (2005). action preconditions contain literals, disjunction
form j i,j contains disjuncts. Therefore, entire belief formula stays
k-CNF, indefinitely. 2

Appendix C. Experiments Outputs
experiments (Section 7) examine properties algorithms learning action models.
show learning tractable exact. Appendix section bring generating models
learned models detailed comparison reader. Recall algorithms
output representation set models possible given input. bring
one satisfying model learned formula.
experiments include following domains International Planning Competition
(IPC): Drivelog, Zenotravel, Blocksworld, Depots.
C.1 Driverlog Domain
Driverlog domain following generating PDDL:
(define (domain driverlog)

389

fiA MIR & C HANG

(:requirements :typing)
(:types
location locatable - object
driver truck obj - locatable )
(:predicates
(at ?obj - locatable ?loc - location)
(in ?obj1 - obj ?obj - truck)
(driving ?d - driver ?v - truck)
(path ?x ?y - location)
(empty ?v - truck) )
(:action LOAD-TRUCK
:parameters
(?obj - obj
?truck - truck
?loc - location)
:precondition
(and (at ?truck ?loc) (at ?obj ?loc))
:effect
(and (not (at ?obj ?loc)) (in ?obj ?truck)))
(:action UNLOAD-TRUCK
:parameters
(?obj - obj
?truck - truck
?loc - location)
:precondition
(and (at ?truck ?loc) (in ?obj ?truck))
:effect
(and (not (in ?obj ?truck)) (at ?obj ?loc)))
(:action BOARD-TRUCK
:parameters
(?driver - driver
?truck - truck
?loc - location)
:precondition
(and (at ?truck ?loc) (at ?driver ?loc) (empty ?truck))
:effect
(and (not (at ?driver ?loc)) (driving ?driver ?truck)
(not (empty ?truck))))
(:action DISEMBARK-TRUCK
:parameters
(?driver - driver
?truck - truck
?loc - location)
:precondition
(and (at ?truck ?loc) (driving ?driver ?truck))
:effect
(and (not (driving ?driver ?truck)) (at ?driver ?loc)
(empty ?truck)))
(:action DRIVE-TRUCK
:parameters
(?truck - truck
?loc-from - location
?loc-to location
?driver - driver)
:precondition
(and (at ?truck ?loc-from)
(driving ?driver ?truck)
(path ?loc-from ?loc-to))
:effect
(and (not (at ?truck ?loc-from)) (at ?truck ?loc-to)))
(:action WALK
:parameters
(?driver - driver
?loc-from - location
?loc-to - location)
:precondition
(and (at ?driver ?loc-from) (path ?loc-from ?loc-to))
:effect
(and (not (at ?driver ?loc-from)) (at ?driver ?loc-to))) )

One learned model (one possible satisfying model formula) random-sequence
input Driverlog domain following (brought together experimental parameters).
Driverlog domain:
* IPC3 problem 99
* 231 fluents
* 1000 randomly selected actions
* 10 fluents observed per step

390

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

* "schematized" learning
* 1:1 precondition heuristics
* Action distribution:
((BOARD-TRUCK . 52) (DRIVE-TRUCK . 86) (DISEMBARK-TRUCK . 52)
(WALK . 529) (UNLOAD-TRUCK . 139) (LOAD-TRUCK . 142))
converting CNF
clause count: 82338
variable count: 210
adding clauses
calling zchaff
parsing result
SLAF time: 2.469
Inference time: 8.406
Learned model:
(WALK NEEDS (AT ?DRIVER ?LOC-FROM))
(WALK NEEDS (NOT (AT ?DRIVER ?LOC-TO)))
(WALK CAUSES (AT ?DRIVER ?LOC-TO))
(WALK CAUSES (NOT (AT ?DRIVER ?LOC-FROM)))
(WALK KEEPS (PATH ?LOC-FROM ?LOC-FROM))
(WALK KEEPS (PATH ?LOC-TO ?LOC-TO))
(WALK KEEPS (PATH ?LOC-TO ?LOC-FROM))
(WALK KEEPS (PATH ?LOC-FROM ?LOC-TO))
(DRIVE-TRUCK NEEDS (NOT (AT ?TRUCK ?LOC-TO)))
(DRIVE-TRUCK NEEDS (AT ?TRUCK ?LOC-FROM))
(DRIVE-TRUCK CAUSES (AT ?TRUCK ?LOC-TO))
(DRIVE-TRUCK CAUSES (NOT (AT ?TRUCK ?LOC-FROM)))
(DRIVE-TRUCK KEEPS (AT ?DRIVER ?LOC-TO))
(DRIVE-TRUCK KEEPS (AT ?DRIVER ?LOC-FROM))
(DRIVE-TRUCK KEEPS (DRIVING ?DRIVER ?TRUCK))
(DRIVE-TRUCK KEEPS (PATH ?LOC-TO ?LOC-TO))
(DRIVE-TRUCK KEEPS (PATH ?LOC-FROM ?LOC-FROM))
(DRIVE-TRUCK KEEPS (PATH ?LOC-FROM ?LOC-TO))
(DRIVE-TRUCK KEEPS (PATH ?LOC-TO ?LOC-FROM))
(DRIVE-TRUCK KEEPS (EMPTY ?TRUCK))
(DISEMBARK-TRUCK NEEDS (NOT (AT ?DRIVER ?LOC)))
(DISEMBARK-TRUCK NEEDS (DRIVING ?DRIVER ?TRUCK))
(DISEMBARK-TRUCK NEEDS (NOT (EMPTY ?TRUCK)))
(DISEMBARK-TRUCK CAUSES (AT ?DRIVER ?LOC))
(DISEMBARK-TRUCK CAUSES (NOT (DRIVING ?DRIVER ?TRUCK)))
(DISEMBARK-TRUCK CAUSES (EMPTY ?TRUCK))
(DISEMBARK-TRUCK KEEPS (AT ?TRUCK ?LOC))
(DISEMBARK-TRUCK KEEPS (PATH ?LOC ?LOC))
(BOARD-TRUCK NEEDS (AT ?DRIVER ?LOC))
(BOARD-TRUCK NEEDS (NOT (DRIVING ?DRIVER ?TRUCK)))
(BOARD-TRUCK NEEDS (EMPTY ?TRUCK))
(BOARD-TRUCK CAUSES (NOT (AT ?DRIVER ?LOC)))
(BOARD-TRUCK CAUSES (DRIVING ?DRIVER ?TRUCK))
(BOARD-TRUCK CAUSES (NOT (EMPTY ?TRUCK)))
(BOARD-TRUCK KEEPS (AT ?TRUCK ?LOC))
(BOARD-TRUCK KEEPS (PATH ?LOC ?LOC))
(UNLOAD-TRUCK NEEDS (NOT (AT ?OBJ ?LOC)))
(UNLOAD-TRUCK NEEDS (IN ?OBJ ?TRUCK))
(UNLOAD-TRUCK CAUSES (AT ?OBJ ?LOC))

391

fiA MIR & C HANG

(UNLOAD-TRUCK CAUSES (NOT (IN ?OBJ ?TRUCK)))
(UNLOAD-TRUCK KEEPS (AT ?TRUCK ?LOC))
(UNLOAD-TRUCK KEEPS (PATH ?LOC ?LOC))
(UNLOAD-TRUCK KEEPS (EMPTY ?TRUCK))
(LOAD-TRUCK NEEDS (AT ?OBJ ?LOC))
(LOAD-TRUCK NEEDS (NOT (IN ?OBJ ?TRUCK)))
(LOAD-TRUCK CAUSES (NOT (AT ?OBJ ?LOC)))
(LOAD-TRUCK CAUSES (IN ?OBJ ?TRUCK))
(LOAD-TRUCK KEEPS (AT ?TRUCK ?LOC))
(LOAD-TRUCK KEEPS (PATH ?LOC ?LOC))
(LOAD-TRUCK KEEPS (EMPTY ?TRUCK))

C.2 Zeno-Travel Domain
Zeno-Travel domain following generating PDDL:
(define (domain zeno-travel)
(:requirements :typing)
(:types aircraft person city flevel - object)
(:predicates (at ?x - (either person aircraft) ?c - city)
(in ?p - person ?a - aircraft)
(fuel-level ?a - aircraft ?l - flevel)
(next ?l1 ?l2 - flevel))
(:action board
:parameters (?p - person ?a - aircraft ?c - city)
:precondition (and (at ?p ?c) (at ?a ?c))
:effect (and (not (at ?p ?c)) (in ?p ?a)))
(:action debark
:parameters (?p - person ?a - aircraft ?c - city)
:precondition (and (in ?p ?a) (at ?a ?c))
:effect (and (not (in ?p ?a)) (at ?p ?c)))
(:action fly
:parameters (?a - aircraft ?c1 ?c2 - city ?l1 ?l2 - flevel)
:precondition (and (at ?a ?c1) (fuel-level ?a ?l1) (next ?l2 ?l1))
:effect (and (not (at ?a ?c1)) (at ?a ?c2) (not (fuel-level ?a ?l1))
(fuel-level ?a ?l2)))
(:action zoom
:parameters (?a - aircraft ?c1 ?c2 - city ?l1 ?l2 ?l3 - flevel)
:precondition (and (at ?a ?c1) (fuel-level ?a ?l1) (next ?l2 ?l1)
(next ?l3 ?l2) )
:effect (and (not (at ?a ?c1)) (at ?a ?c2) (not (fuel-level ?a ?l1))
(fuel-level ?a ?l3) ))
(:action refuel
:parameters (?a - aircraft ?c - city ?l - flevel ?l1 - flevel)
:precondition (and (fuel-level ?a ?l) (next ?l ?l1) (at ?a ?c))
:effect (and (fuel-level ?a ?l1) (not (fuel-level ?a ?l)))))

One learned model (one possible satisfying model formula) random-sequence
input Zeno-Travel domain following (brought together experimental parameters).
Zenotravel domain:

392

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

*
*
*
*
*
*
*

IPC3 problem 9
91 fluents, 21000 possible unique actions
1000 actions learned action sequence
5 observed fluents per step
"schematized" learning
1:1 precondition heuristics
Action distribution: ((ZOOM . 27) (FLY . 216) (REFUEL . 264)
(BOARD . 249) (DEBARK . 244))

converting CNF
clause count: 71119
variable count: 138
adding clauses
calling zchaff
parsing result
SLAF time: 1.109
Inference time: 11.015
Learned model:
(REFUEL NEEDS (FUEL-LEVEL ?A ?L))
(REFUEL NEEDS (NOT (FUEL-LEVEL ?A ?L1)))
(REFUEL CAUSES (NOT (FUEL-LEVEL ?A ?L)))
(REFUEL CAUSES (FUEL-LEVEL ?A ?L1))
(REFUEL KEEPS (NEXT ?L ?L))
(REFUEL KEEPS (NEXT ?L ?L1))
(REFUEL KEEPS (NEXT ?L1 ?L))
(REFUEL KEEPS (NEXT ?L1 ?L1))
(ZOOM NEEDS (NOT (FUEL-LEVEL ?A ?L3)))
(ZOOM NEEDS (FUEL-LEVEL ?A ?L1))
(ZOOM CAUSES (FUEL-LEVEL ?A ?L3))
(ZOOM CAUSES (NOT (FUEL-LEVEL ?A ?L1)))
(ZOOM KEEPS (FUEL-LEVEL ?A ?L2))
(ZOOM KEEPS (NEXT ?L3 ?L3))
(ZOOM KEEPS (NEXT ?L3 ?L2))
(ZOOM KEEPS (NEXT ?L3 ?L1))
(ZOOM KEEPS (NEXT ?L2 ?L3))
(ZOOM KEEPS (NEXT ?L2 ?L2))
(ZOOM KEEPS (NEXT ?L2 ?L1))
(ZOOM KEEPS (NEXT ?L1 ?L3))
(ZOOM KEEPS (NEXT ?L1 ?L2))
(ZOOM KEEPS (NEXT ?L1 ?L1))
(FLY NEEDS (NOT (FUEL-LEVEL ?A ?L2)))
(FLY NEEDS (FUEL-LEVEL ?A ?L1))
(FLY CAUSES (FUEL-LEVEL ?A ?L2))
(FLY CAUSES (NOT (FUEL-LEVEL ?A ?L1)))
(FLY KEEPS (NEXT ?L2 ?L2))
(FLY KEEPS (NEXT ?L2 ?L1))
(FLY KEEPS (NEXT ?L1 ?L2))
(FLY KEEPS (NEXT ?L1 ?L1))
(DEBARK NEEDS (IN ?P ?A))
(DEBARK CAUSES (NOT (IN ?P ?A)))
(BOARD NEEDS (NOT (IN ?P ?A)))
(BOARD CAUSES (IN ?P ?A))

393

fiA MIR & C HANG

C.3 Blocks-World Domain
Blocksworld domain following generating PDDL:
(define (domain blocksworld)
(:requirements :strips)
(:predicates (clear ?x - object)
(on-table ?x - object)
(arm-empty)
(holding ?x - object)
(on ?x ?y - object))
(:action pickup
:parameters (?ob - object)
:precondition (and (clear ?ob) (on-table ?ob) (arm-empty))
:effect (and (holding ?ob) (not (clear ?ob)) (not (on-table ?ob))
(not (arm-empty))))
(:action putdown
:parameters (?ob - object)
:precondition (holding ?ob)
:effect (and (clear ?ob) (arm-empty) (on-table ?ob)
(not (holding ?ob))))
(:action stack
:parameters (?ob - object
?underob - object)
:precondition (and (clear ?underob) (holding ?ob))
:effect (and (arm-empty) (clear ?ob) (on ?ob ?underob)
(not (clear ?underob)) (not (holding ?ob))))
(:action unstack
:parameters (?ob - object
?underob - object)
:precondition (and (on ?ob ?underob) (clear ?ob) (arm-empty))
:effect (and (holding ?ob) (clear ?underob) (not (on ?ob ?underob))
(not (clear ?ob)) (not (arm-empty)))))

One learned model (one possible satisfying model formula) random-sequence
input Blocksworld domain following (brought together experimental parameters).
Blocksworld domain:
* 209 fluents
* 1000 randomly selected actions
* 10 fluents observed per step
* "schematized" learning
* 1:1 precondition heuristics
converting CNF
clause count: 235492
variable count: 187
adding clauses
calling zchaff
parsing result
SLAF time: 2.203
Inference time: 42.312

394

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

Learned model:
(UNSTACK NEEDS (NOT (CLEAR ?UNDEROB)))
(UNSTACK NEEDS (CLEAR ?OB))
(UNSTACK NEEDS (ARM-EMPTY))
(UNSTACK NEEDS (NOT (HOLDING ?OB)))
(UNSTACK NEEDS (ON ?OB ?UNDEROB))
(UNSTACK CAUSES (CLEAR ?UNDEROB))
(UNSTACK CAUSES (NOT (CLEAR ?OB)))
(UNSTACK CAUSES (NOT (ARM-EMPTY)))
(UNSTACK CAUSES (HOLDING ?OB))
(UNSTACK CAUSES (NOT (ON ?OB ?UNDEROB)))
(UNSTACK KEEPS (ON-TABLE ?UNDEROB))
(UNSTACK KEEPS (ON-TABLE ?OB))
(UNSTACK KEEPS (HOLDING ?UNDEROB))
(UNSTACK KEEPS (ON ?UNDEROB ?UNDEROB))
(UNSTACK KEEPS (ON ?OB ?OB))
(UNSTACK KEEPS (ON ?UNDEROB ?OB))
(STACK NEEDS (CLEAR ?UNDEROB))
(STACK NEEDS (NOT (CLEAR ?OB)))
(STACK NEEDS (NOT (ARM-EMPTY)))
(STACK NEEDS (HOLDING ?OB))
(STACK NEEDS (NOT (ON ?OB ?UNDEROB)))
(STACK CAUSES (NOT (CLEAR ?UNDEROB)))
(STACK CAUSES (CLEAR ?OB))
(STACK CAUSES (ARM-EMPTY))
(STACK CAUSES (NOT (HOLDING ?OB)))
(STACK CAUSES (ON ?OB ?UNDEROB))
(STACK KEEPS (ON-TABLE ?UNDEROB))
(STACK KEEPS (ON-TABLE ?OB))
(STACK KEEPS (HOLDING ?UNDEROB))
(STACK KEEPS (ON ?UNDEROB ?UNDEROB))
(STACK KEEPS (ON ?OB ?OB))
(STACK KEEPS (ON ?UNDEROB ?OB))
(PUTDOWN NEEDS (NOT (CLEAR ?OB)))
(PUTDOWN NEEDS (NOT (ON-TABLE ?OB)))
(PUTDOWN NEEDS (NOT (ARM-EMPTY)))
(PUTDOWN NEEDS (HOLDING ?OB))
(PUTDOWN CAUSES (CLEAR ?OB))
(PUTDOWN CAUSES (ON-TABLE ?OB))
(PUTDOWN CAUSES (ARM-EMPTY))
(PUTDOWN CAUSES (NOT (HOLDING ?OB)))
(PUTDOWN KEEPS (ON ?OB ?OB))
(PICKUP NEEDS (CLEAR ?OB))
(PICKUP NEEDS (ON-TABLE ?OB))
(PICKUP NEEDS (ARM-EMPTY))
(PICKUP NEEDS (NOT (HOLDING ?OB)))
(PICKUP CAUSES (NOT (CLEAR ?OB)))
(PICKUP CAUSES (NOT (ON-TABLE ?OB)))
(PICKUP CAUSES (NOT (ARM-EMPTY)))
(PICKUP CAUSES (HOLDING ?OB))
(PICKUP KEEPS (ON ?OB ?OB))

395

fiA MIR & C HANG

C.4 Depot Domain
Depot domain following generating PDDL:
(define (domain Depot)
(:requirements :typing)
(:types place locatable - object
depot distributor - place
truck hoist surface - locatable
pallet crate - surface)
(:predicates (at ?x - locatable ?y - place)
(on ?x - crate ?y - surface)
(in ?x - crate ?y - truck)
(lifting ?x - hoist ?y - crate)
(available ?x - hoist)
(clear ?x - surface))
(:action Drive
:parameters (?x - truck ?y - place ?z - place)
:precondition (and (at ?x ?y))
:effect (and (not (at ?x ?y)) (at ?x ?z)))
(:action Lift
:parameters (?x - hoist ?y - crate ?z - surface ?p - place)
:precondition (and (at ?x ?p) (available ?x) (at ?y ?p) (on ?y ?z)
(clear ?y))
:effect (and (not (at ?y ?p)) (lifting ?x ?y) (not (clear ?y))
(not (available ?x)) (clear ?z) (not (on ?y ?z))))
(:action Drop
:parameters (?x - hoist ?y - crate ?z - surface ?p - place)
:precondition (and (at ?x ?p) (at ?z ?p) (clear ?z) (lifting ?x ?y))
:effect (and (available ?x) (not (lifting ?x ?y)) (at ?y ?p)
(not (clear ?z)) (clear ?y) (on ?y ?z)))
(:action Load
:parameters (?x - hoist ?y - crate ?z - truck ?p - place)
:precondition (and (at ?x ?p) (at ?z ?p) (lifting ?x ?y))
:effect (and (not (lifting ?x ?y)) (in ?y ?z) (available ?x)))
(:action Unload
:parameters (?x - hoist ?y - crate ?z - truck ?p - place)
:precondition (and (at ?x ?p) (at ?z ?p) (available ?x) (in ?y ?z))
:effect (and (not (in ?y ?z)) (not (available ?x)) (lifting ?x ?y))) )

One learned model (one possible satisfying model formula) random-sequence
input Depot domain following (brought together experimental parameters).
Depots domain:
* IPC3 problem 5
* 250 fluents
* 1000 randomly selected actions
* 10 fluents observed per step
* "schematized" learning
* 1:1 precondition heuristics
converting CNF

396

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

clause count: 85359
variable count: 236
adding clauses
calling zchaff
parsing result
SLAF time: 2.797
Inference time: 8.062
Learned model:
(UNLOAD NEEDS (IN ?Y ?Z))
(UNLOAD NEEDS (NOT (LIFTING ?X ?Y)))
(UNLOAD NEEDS (AVAILABLE ?X))
(UNLOAD CAUSES (NOT (IN ?Y ?Z)))
(UNLOAD CAUSES (LIFTING ?X ?Y))
(UNLOAD CAUSES (NOT (AVAILABLE ?X)))
(UNLOAD KEEPS (AT ?Z ?P))
(UNLOAD KEEPS (AT ?Y ?P))
(UNLOAD KEEPS (AT ?X ?P))
(UNLOAD KEEPS (ON ?Y ?Y))
(UNLOAD KEEPS (CLEAR ?Y))
(LOAD NEEDS (NOT (IN ?Y ?Z)))
(LOAD NEEDS (LIFTING ?X ?Y))
(LOAD NEEDS (NOT (AVAILABLE ?X)))
(LOAD CAUSES (IN ?Y ?Z))
(LOAD CAUSES (NOT (LIFTING ?X ?Y)))
(LOAD CAUSES (AVAILABLE ?X))
(LOAD KEEPS (AT ?Z ?P))
(LOAD KEEPS (AT ?Y ?P))
(LOAD KEEPS (AT ?X ?P))
(LOAD KEEPS (ON ?Y ?Y))
(LOAD KEEPS (CLEAR ?Y))
(DROP NEEDS (NOT (AT ?Y ?P)))
(DROP NEEDS (NOT (ON ?Y ?Z)))
(DROP NEEDS (LIFTING ?X ?Y))
(DROP NEEDS (NOT (AVAILABLE ?X)))
(DROP NEEDS (CLEAR ?Z))
(DROP NEEDS (NOT (CLEAR ?Y)))
(DROP CAUSES (AT ?Y ?P))
(DROP CAUSES (ON ?Z ?Z))
(DROP CAUSES (NOT (ON ?Z ?Z)))
(DROP CAUSES (ON ?Z ?Y))
(DROP CAUSES (NOT (ON ?Z ?Y)))
(DROP CAUSES (ON ?Y ?Z))
(DROP CAUSES (NOT (LIFTING ?X ?Y)))
(DROP CAUSES (LIFTING ?X ?Z))
(DROP CAUSES (NOT (LIFTING ?X ?Z)))
(DROP CAUSES (AVAILABLE ?X))
(DROP CAUSES (NOT (CLEAR ?Z)))
(DROP CAUSES (CLEAR ?Y))
(DROP KEEPS (AT ?Z ?P))
(DROP KEEPS (AT ?X ?P))
(DROP KEEPS (ON ?Z ?Z))
(DROP KEEPS (ON ?Z ?Y))
(DROP KEEPS (ON ?Y ?Y))
(DROP KEEPS (LIFTING ?X ?Z))

397

fiA MIR & C HANG

(LIFT NEEDS (AT ?Y ?P))
(LIFT NEEDS (ON ?Y ?Z))
(LIFT NEEDS (NOT (LIFTING ?X ?Y)))
(LIFT NEEDS (AVAILABLE ?X))
(LIFT NEEDS (NOT (CLEAR ?Z)))
(LIFT NEEDS (CLEAR ?Y))
(LIFT CAUSES (NOT (AT ?Y ?P)))
(LIFT CAUSES (NOT (ON ?Y ?Z)))
(LIFT CAUSES (ON ?Z ?Z))
(LIFT CAUSES (NOT (ON ?Z ?Z)))
(LIFT CAUSES (ON ?Z ?Y))
(LIFT CAUSES (NOT (ON ?Z ?Y)))
(LIFT CAUSES (LIFTING ?X ?Y))
(LIFT CAUSES (LIFTING ?X ?Z))
(LIFT CAUSES (NOT (LIFTING ?X ?Z)))
(LIFT CAUSES (NOT (AVAILABLE ?X)))
(LIFT CAUSES (CLEAR ?Z))
(LIFT CAUSES (NOT (CLEAR ?Y)))
(LIFT KEEPS (AT ?Z ?P))
(LIFT KEEPS (AT ?X ?P))
(LIFT KEEPS (ON ?Y ?Y))
(LIFT KEEPS (ON ?Z ?Z))
(LIFT KEEPS (ON ?Z ?Y))
(LIFT KEEPS (LIFTING ?X ?Z))
(DRIVE NEEDS (AT ?X ?Y))
(DRIVE NEEDS (NOT (AT ?X ?Z)))
(DRIVE CAUSES (NOT (AT ?X ?Y)))
(DRIVE CAUSES (AT ?X ?Z))

References
Amir, E. (2005). Learning partially observable deterministic action models. Proc. Nineteenth
International Joint Conference Artificial Intelligence (IJCAI 05), pp. 14331439. International Joint Conferences Artificial Intelligence.
Amir, E., & Russell, S. (2003). Logical filtering. Proc. Eighteenth International Joint Conference
Artificial Intelligence (IJCAI 03), pp. 7582. Morgan Kaufmann.
Benson, S. (1995). Inductive learning reactive action models. Proceedings 12th International Conference Machine Learning (ICML-95).
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming first-order MDPs.
Proc. Seventeenth International Joint Conference Artificial Intelligence (IJCAI 01), pp.
690697. Morgan Kaufmann.
Boyen, X., Friedman, N., & Koller, D. (1999). Discovering hidden structure complex dynamic
systems. Proceedings 15th Conference Uncertainty Artificial IntelligenceUAI
1999, pp. 91100. Morgan Kaufmann. Available http://www.cs.stanford.edu/ xb/uai99/.
Boyen, X., & Koller, D. (1999). Approximate learning dynamic models. Kearns, M. S., Solla,
S. A., & Kohn, D. A. (Eds.), Advances Neural Information Processing Systems 11: Proceedings 1998 ConferenceNIPS 1998, pp. 396402. Cambridge: MIT Press. Available http://www.cs.stanford.edu/ xb/nips98/.
398

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

Calvanese, D., Giacomo, G. D., & Vardi, M. Y. (2002). Reasoning actions planning
LTL action theories. Principles Knowledge Representation Reasoning: Proc. Eighth
Intl Conference (KR 2002), pp. 593602. Morgan Kaufmann.
Chang, A., & Amir, E. (2006). Goal achievement partially known, partially observable domains.
Proceedings 16th Intl Conf. Automated Planning Scheduling (ICAPS06).
AAAI Press.
Chang, C.-L., & Lee, R. C.-T. (1973). Symbolic Logic Mechanical Theorem Proving. Academic
Press.
Clarke, E. M., Grumberg, O., & Peled, D. A. (1999). Model Checking. MIT Press.
Darwiche, A., & Marquis, P. (2002). knowledge compilation map. Journal Artificial Intelligence Research, 17, 229264.
Davis, M., & Putnam, H. (1960). computing procedure quantification theory. Journal
ACM, 7, 201215.
Dawsey, W., Minsker, B., & Amir, E. (2007). Real-time assessment drinking water systems using
Bayesian networks. World Environmental Water Resources Congress.
Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial Intelligence,
113(12), 4185.
del Val, A. (1999). new method consequence finding compilation restricted language. Proc. National Conference Artificial Intelligence (AAAI 99), pp. 259264.
AAAI Press/MIT Press.
Doherty, P., Lukaszewicz, W., & Szalas, A. (1997). Computing circumscription revisited: reduction algorithm. Journal Automated Reasoning, 18(3), 297336.
Eiter, T., & Gottlob, G. (1992). complexity propositional knowledge base revision, updates, counterfactuals. Artificial Intelligence, 57(2-3), 227270.
Even-Dar, E., Kakade, S. M., & Mansour, Y. (2005). Reinforcement learning POMDPs.
Proc. Nineteenth International Joint Conference Artificial Intelligence (IJCAI 05), pp.
660665. International Joint Conferences Artificial Intelligence.
Fagin, R., Ullman, J. D., & Vardi, M. Y. (1983). semantics updates databases.
Proceedings Second ACM SIGACT-SIGMOD Symposium Principles Database
Systems, pp. 352365, Atlanta, Georgia.
Fikes, R., Hart, P., & Nilsson, N. (1972). Learning executing generalized robot plans. Artificial
Intelligence, 3, 251288.
Fox, M., & Long, D. (2002). PDDL2.1: extension PDDL expressing temporal planning
domains. http://www.dur.ac.uk/d.p.long/IPC/pddl.html. Used AIPS02 competition.
Friedman, N., Murphy, K., & Russell, S. (1998). Learning structure dynamic probabilistic
networks. Proc. Fourteenth Conference Uncertainty Artificial Intelligence (UAI 98).
Morgan Kaufmann.
Gelfond, M., & Lifschitz, V. (1998). Action languages. Electronic Transactions Artificial Intelligence (http://www.etaij.org), 3, nr 16.
399

fiA MIR & C HANG

Ghahramani, Z. (2001). introduction Hidden Markov Models Bayesian networks. International Journal Pattern Recognition Artificial Intelligence, 15(1), 942.
Ghahramani, Z., & Jordan, M. I. (1997). Factorial hidden markov models. Machine Learning, 29,
245275.
Ghallab, M., Howe, A., Knoblock, C., McDermott, D., Ram, A., Veloso, M., Weld, D., & Wilkins,
D. (1998). PDDL Planning Domain Definition Language, version 1.2. Tech. rep. CVC
TR-98-003/DCS TR-1165, Yale center computational vision control.
Gil, Y. (1994). Learning experimentation: Incremental refinement incomplete planning domains. Proceedings 11th International Conference Machine Learning (ICML-94),
pp. 1013.
Ginsberg, M. L. (1987). Readings Nonmonotonic Reasoning, chap. 1, pp. 123. Morgan Kaufmann, Los Altos, CA.
Hajishirzi, H., & Amir, E. (2007). Stochastic filtering probabilistic action models. Proc. National Conference Artificial Intelligence (AAAI 07).
Hill, D. J., Minsker, B., & Amir, E. (2007). Real-time Bayesian anomaly detection environmental
sensor data. 32nd Congress International Association Hydraulic Engineering
Research (IAHR 2007).
Hlubocky, B., & Amir, E. (2004). Knowledge-gathering agents adventure games. AAAI-04
Workshop Challenges Game AI. AAAI Press.
Holmes, M. P., & Charles Lee Isbell, J. (2006). Looping suffix tree-based inference partially
observable hidden state. Proceedings 23rd International Conference Machine
Learning (ICML-06), pp. 409416. ACM Press.
Iwanuma, K., & Inoue, K. (2002). Minimal answer computation sol. Logics Artificial
Intelligence: Proceedings Eighth European Conference, Vol. 2424 LNAI, pp. 245
257. Springer-Verlag.
James, M., & Singh, S. (2004). Learning discovery predictive state representations dynamical systems reset. Proceedings 21st International Conference Machine
Learning (ICML-04), pp. 417424. ACM Press.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting partially
observable stochastic domains. Artificial Intelligence, 101, 99134.
Kearns, M., Mansour, Y., & Ng, A. Y. (2000). Approximate planning large POMDPs via reusable
trajectories. Proceedings 12th Conference Neural Information Processing Systems
(NIPS98), published 1999, pp. 10011007. MIT Press.
Kuffner., J. J., & LaValle, S. M. (2000). Rrt-connect: efficient approach single-query path
planning.. IEEE International Conference Robotics Automation (ICRA), pp. 995
1001.
Lee, R. C.-T. (1967). Completeness Theorem Computer Program Finding Theorems
Derivable Given Axioms. Ph.D. thesis, University California, Berkeley.
Lifschitz, V. (1990). semantics STRIPS. Allen, J. F., Hendler, J., & Tate, A. (Eds.),
Readings Planning, pp. 523530. Morgan Kaufmann, San Mateo, California.
400

fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS

Littman, M. L. (1996). Algorithms sequential decision making. Ph.D. thesis, Department
Computer Science, Brown University. Technical report CS-96-09.
Littman, M. L., Sutton, R., & Singh, S. (2002). Predictive representations state. Proceedings
15th Conference Neural Information Processing Systems (NIPS01), published 2002.
MIT Press.
Marquis, P. (2000). Consequence finding algorithms. Gabbay, D., & Smets, P. (Eds.), Handbook Defeasible Reasoning Uncertainty Management Systems, Vol. 5: Algorithms
defeasible uncertain reasoning. Kluwer.
McCallum, R. A. (1995). Instance-based utile distinctions reinforcement learning hidden
state. Proceedings 12th International Conference Machine Learning (ICML-95).
Morgan Kaufmann.
McCarthy, J. (1986). Applications Circumscription Formalizing Common Sense Knowledge.
Artificial Intelligence, 28, 89116.
McCarthy, J., & Hayes, P. J. (1969). philosophical problems standpoint artificial intelligence. Meltzer, B., & Michie, D. (Eds.), Machine Intelligence 4, pp. 463502.
Edinburgh University Press.
McIlraith, S., & Amir, E. (2001). Theorem proving structured theories. Proc. Seventeenth
International Joint Conference Artificial Intelligence (IJCAI 01), pp. 624631. Morgan
Kaufmann.
Meuleau, N., Peshkin, L., Kim, K.-E., & Kaelbling, L. P. (1999). Learning finite-state controllers
partially observable environments. Proc. Fifteenth Conference Uncertainty Artificial
Intelligence (UAI 99). Morgan Kaufmann.
Moskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering
Efficient SAT Solver. Proceedings 38th Design Automation Conference (DAC01).
Murphy, K. (2002). Dynamic Bayesian Networks: Representation, Inference Learning. Ph.D.
thesis, University California Berkeley.
Nance, M., Vogel, A., & Amir, E. (2006). Reasoning partially observed actions. Proc. National Conference Artificial Intelligence (AAAI 06). AAAI Press.
Oates, T., & Cohen, P. R. (1996). Searching planning operators context-dependent
probabilistic effects. Proc. National Conference Artificial Intelligence (AAAI 96), pp.
863868. AAAI Press.
Pasula, H. M., Zettlemoyer, L. S., & Kaelbling, L. P. (2004). Learning probabilistic relational
planning rules. Proceedings 14th Intl Conf. Automated Planning Scheduling
(ICAPS04). AAAI Press.
Pednault, E. P. D. (1989). ADL: exploring middle ground STRIPS situation
calculus. Proc. First International Conference Principles Knowledge Representation
Reasoning (KR 89), pp. 324332.
Reiter, R. (2001). Knowledge Action: Logical Foundations Describing Implementing
Dynamical Systems. MIT Press.
401

fiA MIR & C HANG

Reiter, R. (1991). frame problem situation calculus: simple solution (sometimes)
completeness result goal regression. Lifschitz, V. (Ed.), Artificial Intelligence
Mathematical Theory Computation (Papers Honor John McCarthy), pp. 359380.
Academic Press.
Robert, C. P., Celeux, G., & Diebolt, J. (1993). Bayesian estimation hidden Markov chains:
stochastic implementation. Statist. Prob. Letters, 16, 7783.
Schmill, M. D., Oates, T., & Cohen, P. R. (2000). Learning planning operators real-world, partially observable environments. Proceedings 5th Intl Conf. AI Planning
Scheduling (AIPS00), pp. 246253. AAAI Press.
Shahaf, D., & Amir, E. (2006). Learning partially observable action schemas. Proc. National
Conference Artificial Intelligence (AAAI 06). AAAI Press.
Shahaf, D., & Amir, E. (2007). Logical circuit filtering. Proc. Twentieth International Joint Conference Artificial Intelligence (IJCAI 07), pp. 26112618. International Joint Conferences
Artificial Intelligence.
Shahaf, D., Chang, A., & Amir, E. (2006). Learning partially observable action models: Efficient
algorithms. Proc. National Conference Artificial Intelligence (AAAI 06). AAAI Press.
Simon, L., & del Val, A. (2001). Efficient consequence-finding. Proc. Seventeenth International
Joint Conference Artificial Intelligence (IJCAI 01), pp. 359365. Morgan Kaufmann.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: introduction. MIT Press.
Tang, D., Yinlei Yu, D. R., & Malik, S. (2004). Analysis search based algorithms satisfiability quantified boolean formulas arising circuit state space diameter problems.
Proceedings Seventh International Conference Theory Applications Satisfiability Testing (SAT2004).
Thielscher, M. (1998). Introduction fluent calculus. Electronic Transactions Artificial
Intelligence (http://www.etaij.org), 3, nr 14.
Thrun, S. (2003). Robotic mapping: survey. Exploring artificial intelligence new millennium, pp. 135. Morgan Kaufmann.
Wang, X. (1995). Learning observation practice: incremental approach planning operator acquisition. Proceedings 12th International Conference Machine Learning
(ICML-95), pp. 549557. Morgan Kaufmann.
Wu, K., Yang, Q., & Jiang, Y. (2007). Arms: automatic knowledge engineering tool learning
action models ai planning. Knowledge Engineering Review, 22(2), 135152.
Yang, Q., Wu, K., & Jiang, Y. (2005). Learning actions models plan examples incomplete
knowledge.. Biundo, S., Myers, K. L., & Rajan, K. (Eds.), ICAPS, pp. 241250. AAAI.

402

fiJournal Artificial Intelligence Research 33 (2008) 223-258

Submitted 5/08; published 10/08

Completeness Performance APO Algorithm
Tal Grinshpoun
Amnon Meisels

grinshpo@cs.bgu.ac.il
am@cs.bgu.ac.il

Ben-Gurion University Negev
Department Computer Science
Beer-Sheva, Israel

Abstract
Asynchronous Partial Overlay (APO) search algorithm uses cooperative mediation solve Distributed Constraint Satisfaction Problems (DisCSPs). algorithm
partitions search different subproblems DisCSP. original proof completeness APO algorithm based growth size subproblems.
present paper demonstrates expected growth subproblems occur
situations, leading termination problem algorithm. problematic parts
APO algorithm interfere completeness identified necessary
modifications algorithm fix problematic parts given. resulting
version algorithm, Complete Asynchronous Partial Overlay (CompAPO), ensures
completeness. Formal proofs soundness completeness CompAPO
given. detailed performance evaluation CompAPO comparing DisCSP
algorithms presented, along extensive experimental evaluation algorithms
unique behavior. Additionally, optimization version algorithm, CompOptAPO,
presented, discussed, evaluated.

1. Introduction
Algorithms solve Distributed Constraint Satisfaction Problems (DisCSPs) attempt
achieve concurrency problem solving order utilize distributed nature
problems. Distributed backtracking, forms majority DisCSP algorithms,
take many forms. Asynchronous Backtracking (ABT) (Yokoo, Durfee, Ishida, & Kuwabara,
1998; Yokoo & Hirayama, 2000), Asynchronous Forward-Checking (AFC) (Meisels & Zivan, 2007), Concurrent Dynamic Backtracking (ConcDB) (Zivan & Meisels, 2006a)
representative examples family distributed backtracking algorithms.
algorithms maintain one partial solutions DisCSP attempt extend
partial solution complete one. ABT algorithm attempts achieve concurrency
asynchronously assigning values variables. AFC algorithm performs value
assignments synchronously, achieves concurrency performing asynchronous computation form forward checking. ConcDB algorithm concurrently attempts
extend multiple partial solutions, scanning different parts search space.
completely different approach achieve concurrency merging partial
solutions complete one. inherent concurrency merging partial solutions makes
fascinating paradigm solving DisCSPs. However, approach prone many
errors deadlocks could prevent termination, failures could occur attempt
merge partial solutions. Consequently, hard develop algorithm
c
2008
AI Access Foundation. rights reserved.

fiGrinshpoun & Meisels

correct well performing. recently published algorithm, Asynchronous
Partial Overlay (APO) (Mailler, 2004; Mailler & Lesser, 2006), attempts solve DisCSPs
merging partial solutions. uses concept mediation centralize search
procedure different parts DisCSP. Due unique approach, several researchers
already proposed changes modifications APO algorithm (Benisch & Sadeh,
2006; Semnani & Zamanifar, 2007). Unfortunately, none studies examined
completeness APO. Additionally, distinctive behavior APO algorithm calls
thorough experimental evaluation. present paper presents in-depth investigation
completeness termination APO algorithm, constructs correct version
algorithm CompAPO goes present extensive experimental evaluation
complete APO algorithm.
APO algorithm partitions agents groups attempt find consistent
partial solutions. partition mechanism dynamic search enables dynamic
change groups. key factor termination (and consequently completeness)
APO algorithm presented original correctness proof (Mailler & Lesser, 2006)
monotonic growth initially partitioned groups search. growth arises
subproblems overlap, allowing agents increase size subproblems
solve. discovered expected growth groups occur
situations, leading termination problem APO algorithm. Nevertheless,
unique way APO attempts solve DisCSPs encouraged us try fix it.
termination problem APO algorithm shown section 4 constructing scenario leads infinite loop algorithms run (Grinshpoun, Zazon,
Binshtok, & Meisels, 2007). running example essential understanding
APOs completeness problem, since algorithm complex. help understand
problem, full pseudo-code APO follows closely original presentation
algorithm (Mailler & Lesser, 2006) given. erroneous part proof APOs
completeness presented Mailler Lesser (2006) shown problematic parts
algorithm interfere completeness identified. Necessary modifications
algorithm proposed, order fix problematic parts. resulting version
algorithm ensures completeness, termed Complete Asynchronous Partial
Overlay (CompAPO) (Grinshpoun & Meisels, 2007). Formal proofs soundness
completeness CompAPO presented.
modifications CompAPO may potentially affect performance algorithm.
Also, evaluation original APO algorithm (Mailler & Lesser, 2006), compared AWC algorithm (Yokoo, 1995), efficient DisCSP solver (Zivan,
Zazone, & Meisels, 2007). Moreover, tests work Mailler Lesser (2006)
performed relatively sparse problems, comparison AWC
made use problematic measures. extensive experimental evaluation
CompAPO compares performance DisCSP search algorithms randomly
generated DisCSPs. experiments show CompAPO performs significantly different DisCSP algorithms, surprising considering singular way
problem solving.
Asynchronous Partial Overlay actually family algorithms. completeness
termination problems presented corrected present study apply
members family. OptAPO algorithm (Mailler & Lesser, 2004; Mailler, 2004)
224

fiCompleteness Performance APO Algorithm

optimization version APO solves Distributed Constraint Optimization Problems
(DisCOPs). present paper proposes similar modifications APO order
achieve completeness OptAPO. resulting CompOptAPO algorithm evaluated
extensively randomly generated DisCOPs.
plan paper follows. DisCSPs presented briefly section 2. Section 3, gives short description APO algorithm along pseudo-code.
infinite loop scenario APO described detail section 4 problems
lead infinite looping analyzed section 5. Section 6 presents detailed solution
problem forms CompAPO version algorithm, followed proofs
soundness completeness CompAPO (section 7). optimization version
algorithm, CompOptAPO, presented discussed section 8. extensive performance evaluation CompAPO CompOptAPO section 9. conclusions
summarized section 10.

2. Distributed Constraint Satisfaction
distributed constraints satisfaction problem DisCSP, composed set k agents
A1 , A2 , ..., Ak . agent Ai contains set constrained variables xi1 , xi2 , ..., xini . Constraints relations R subsets Cartesian product domains constrained variables. set constrained variables xik , xjl , ..., xmn , domains values
variable Dik , Djl , ..., Dmn , constraint defined R Dik Djl ... Dmn .
binary constraint Rij two variables xj xi subset Cartesian
product domains Rij Dj Di . distributed constraint satisfaction problem
(DisCSP), agents connected constraints variables belong different agents (Yokoo et al., 1998). addition, agent set constrained variables,
i.e. local constraint network.
assignment (or label) pair < var, val >, var variable agent
val value var domain assigned it. compound label set
assignments values set variables. solution DisCSP compound label
includes variables agents, satisfies constraints. Agents check
assignments values non-local constraints communicating agents
sending receiving messages.
Current studies DisCSPs follow assumption agents hold exactly one variable (Yokoo & Hirayama, 2000; Bessiere, Maestre, Brito, & Meseguer, 2005). Accordingly,
present study often uses variables name xi represent agent belongs (Ai ).
addition, following common assumptions used present study:
amount time passes sending receiving message
finite.
Messages sent agent Ai agent Aj received Aj order sent.

3. Asynchronous Partial Overlay
Asynchronous Partial Overlay (APO) algorithm solving DisCSPs applies
cooperative mediation. pseudo-code Algorithms 1, 2, 3 follows closely
presentation APO work Mailler Lesser (2006).
225

fiGrinshpoun & Meisels

Algorithm 1 APO procedures initialization local resolution.
procedure initialize
1: di random Di ;
2: pi sizeof (neighbors) + 1;
3: mi true;
4: mediate false;
5: add xi good list;
6: send (init, (xi , pi , di , mi , Di , Ci )) neighbors;
7: init list neighbors;
received (init, (xj , pj , dj , mj , Dj , Cj ))
1: add (xj , pj , dj , mj , Dj , Cj ) agent view ;
2: xj neighbor xk good list
3:
add xj good list;
4:
add xl agent view xl
/ good list connected good list;
5:
pi sizeof (good list);
6: xj
/ init list
7:
send (init, (xi , pi , di , mi , Di , Ci )) xj ;
8: else
9:
remove xj init list;
10: check agent view;
received (ok?, (xj , pj , dj , mj ))
1: update agent view (xj , pj , dj , mj );
2: check agent view;
procedure check agent view
1: init list 6= mediate 6= false
2:
return;
3: mi hasConf lict(xi );
4: mi j(pj > pi mj == true)
5:
(di Di )(di agent view conflict) di conflicts exclusively lower
priority neighbors
6:
di di ;
7:
send (ok?, (xi , pi , di , mi )) xj agent view;
8:
else
9:
mediate;
10: else mi 6= mi
11:
mi mi ;
12:
send (ok?, (xi , pi , di , mi )) xj agent view;

226

fiCompleteness Performance APO Algorithm

Algorithm 2 Procedures mediating APO session choosing solution
APO mediation.
procedure mediate
1: pref erences ;
2: counter 0;
3: xj good list
4:
send (evaluate?, (xi , pi )) xj ;
5:
counter counter + 1;
6: mediate true;
received (wait!, (xj , pj ))
1: update agent view (xj , pj );
2: counter counter 1;
3: counter == 0 choose solution;
received (evaluate!, (xj , pj , labeled Dj ))
1: record (xj , labeled Dj ) preferences;
2: update agent view (xj , pj );
3: counter counter 1;
4: counter == 0 choose solution;
procedure choose solution
1: select solution using Branch Bound search that:
2:
1. satisfies constraints agents good list
3:
2. minimizes violations agents outside session
4: satisfies constraints
5:
broadcast solution;
6: xj agent view
7:
xj pref erences
/ agent view
8:
dj violates xk xk
9:
send (init, (xi , pi , di , mi , Di , Ci )) xk ;
10:
add xk init list;
11:
send (accept!, (dj , xi , pi , di , mi )) xj ;
12:
update agent view xj ;
13:
else
14:
send (ok?, (xi , pi , di , mi )) xj ;
15: mediate false;
16: check agent view;

227

fiGrinshpoun & Meisels

Algorithm 3 Procedures receiving APO session.
received (evaluate?, (xj , pj ))
1: mj true;
2: mediate == true k(pk > pj mk == true)
3:
send (wait!, (xi , pi )) xj ;
4: else
5:
mediate true;
6:
label Di names agents would violated setting
di d;
7:
send (evaluate!, (xi , pi , labeled Di )) xj ;
received (accept!, (d, xj , pj , dj , mj ))
1: di d;
2: mediate false;
3: send (ok?, (xi , pi , di , mi )) xj agent view;
4: update agent view (xj , pj , dj , mj );
5: check agent view;

beginning problem solving, APO algorithm performs initialization
phase, neighboring agents exchange data init messages (procedure initialize Algorithm 1). Following that, agents check agent view identify conflicts
neighbors (procedure check agent view Algorithm 1).
check, agent finds conflict one neighbors, expresses desire
act mediator. case agent neighbors wish mediate
wider view constraint graph itself, agent successfully assumes
role mediator.
Using mediation (Algorithms 2, 3), agents solve subproblems DisCSP
conducting internal Branch Bound search (procedure choose solution Algorithm 2). complete solution DisCSP, solutions subproblems must
compatible. solutions overlapping subproblems conflicts, solving agents
increase size subproblems work on. original paper (Mailler &
Lesser, 2006) uses term preferences describe potential conflicts solutions
overlapping subproblems. present paper use term external constraints
describe conflicts. detailed description APO algorithm found
work Mailler Lesser (2006).

4. Infinite Loop Scenario
Consider 3-coloring problem presented Figure 1 solid lines. agent
assign one three available colors Red, Green, Blue. standard inequality
constraints solid lines represent, four weaker constraints (diagonal dashed lines)
added. dashed lines represent constraints allow combinations
(Green,Green) (Blue,Blue) assigned agents. Ties priorities agents
broken using anti-lexicographic ordering names.
228

fiCompleteness Performance APO Algorithm

A1
Red

A2
Red

A8
Green

A3
Blue

A7
Blue

A4
Green

A6
Red

A5
Red

Figure 1: constraints graph initial assignments.
Agent
A1
A2
A3
A4
A5
A6
A7
A8

Color
R
R
B
G
R
R
B
G

mi
m1
m2
m3
m4
m5
m6
m7
m8

=t
=t
=f
=f
=t
=t
=f
=f

dj
d2
d1
d1
d3
d3
d5
d1
d1

values
=R, d3
=R, d3
=R, d2
=B, d5
=B, d4
=R, d7
=R, d5
=R, d7

=B, d7 =B, d8 =G
=B
=R, d4 =G, d5 =R
=R
=G, d6 =R, d7 =B
=B
=R, d6 =R, d8 =G
=B

mj
m2
m1
m1
m3
m3
m5
m1
m1

values
= t, m3 = f , m7 = f , m8 = f
= t, m3 = f
= t, m2 = t, m4 = f , m5 =
= f , m5 =
= f , m4 = f , m6 = t, m7 = f
= t, m7 = f
= t, m5 = t, m6 = t, m8 = f
= t, m7 = f

Table 1: Configuration 1.
initial selection values agents depicted Figure 1. initial state,
two constraints violated (A1 , A2 ) (A5 , A6 ). Assume agents A3 , A4 , A7 ,
A8 first complete initialization phase exchanging init messages
neighbors (procedure initialize Algorithm 1). agents conflicts,
therefore set mi false send ok? messages neighbors
runs check agent view procedure (Algorithm 1). arrival ok?
messages agents A3 , A4 , A7 , A8 , agents A1 , A2 , A5 , A6 accept last init
messages neighbors complete initialization phase. Agents A2
A6 conflicts, complete check agent view procedure without mediating
changing state. true, agent views A2 A6 , m1 = true
m5 = true, respectively. neighbors higher priority agents A2 A6
respectively. denote configuration 1 states agents point
processing present configuration Table 1.
agents complete initializations, agents A1 A5 detect
conflicts, neighbor higher priority wants mediate.
Consequently, agents A1 A5 start mediation sessions, since cannot change
color consistent state neighbors.
229

fiGrinshpoun & Meisels

Agent
A1
A2
A3
A4
A5
A6
A7
A8

Color
G
B
R
G
R
R
B
R

mi
m1
m2
m3
m4
m5
m6
m7
m8

=f
=f
=f
=f
=t
=t
=f
=f

dj values
d2 =B, d3 =R, d7 =B, d8 =R
d1 =G, d3 =R
d1 =G, d2 =B, d4 =G, d5 =R
3 =B, d5 =R
3 =B, d4 =G, d6 =R, d7 =B
d5 =R, d7 =B
d1 =G, d5 =R, d6 =R, 8 =G
d1 =G, d7 =B

mj
m2
m1
m1
m3
m3
m5
m1
m1

values
= f , m3 = f , m7 = f , m8 = f
= f , m3 = f
= f , m2 = f , m4 = f , m5 =
= f , m5 =
= f , m4 = f , m6 = t, m7 = f
= t, m7 = f
= f , m5 = t, m6 = t, m8 = f
= f , m7 = f

Table 2: Configuration 2 obsolete data agent views bold face.
Let us first observe A1 mediation session. A1 sends evaluate? messages neighbors A2 , A3 , A7 , A8 (procedure mediate Algorithm 2). agents reply
evaluate! messages (Algorithm 3). A1 conducts Branch Bound search find
solution satisfies constraints A1 , A2 , A3 , A7 , A8 , also minimizes external constraints (procedure choose solution Algorithm 2). example,
A1 finds solution (A1 Green, A2 Blue, A3 Red, A7 Blue, A8 Red),
satisfies internal constraints, minimizes zero external constraints. A1 sends
accept! messages neighbors, informing solution. A2 , A3 , A7 , A8
receive accept! messages send ok? messages new states neighbors (Algorithm 3). However, ok? messages A8 A7 A3 A4
A5 delayed. Observe algorithm asynchronous naturally deals
scenarios.
Concurrently mediation session A1 , agent A5 starts mediation
session. A5 sends evaluate? messages neighbors A3 , A4 , A6 , A7 . Let us assume
message A7 delayed. A4 A6 receive evaluate? messages reply
evaluate!, since know agents higher priority A5 want
mediate. A3 , A1 mediation session, replies wait!. denote
configuration 2 states agents point processing (see Table 2).
A1 mediation session over, A7 receives delayed evaluate? message
A5 . Since A7 longer mediation session, expect mediation session
node higher priority A5 (see A7 view Table 2), agent A7 replies
evaluate!. Notice A7 view d8 obsolete (the ok? message A8 A7 still
delayed). agent A5 receives evaluate! message A7 , continue
mediation session involving agents A4 , A5 , A6 , A7 . Since ok? messages A3
A4 A5 also delayed, agent A5 starts mediation session knowledge
agents A3 A8 updated (see bold-faced data Table 2).
Agent A5 conducts Branch Bound search find solution satisfies
constraints A4 , A5 , A6 , A7 , also minimizes external constraints.
example, A5 finds solution (A4 Red, A5 Green, A6 Blue, A7 Red),
satisfies internal constraints, minimizes zero external constraints (remember
A5 wrong data assignments A3 A8 ). A5 sends accept! messages
A4 , A6 , A7 , informing solution. agents receive messages
send ok? messages new states neighbors. now, delayed
230

fiCompleteness Performance APO Algorithm

A1
Green

A2
Blue

A8
Red

A3
Red

A7
Red

A4
Red

A6
Blue

A5
Green

Figure 2: graph configuration 3.
Agent
A1
A2
A3
A4
A5
A6
A7
A8

Color
G
B
R
R
G
B
R
R

mi
m1
m2
m3
m4
m5
m6
m7
m8

=f
=f
=t
=t
=f
=f
=t
=t

dj
d2
d1
d1
d3
d3
d5
d1
d1

values
=B, d3 =R, d7 =R, d8 =R
=G, d3 =R
=G, d2 =B, d4 =R, d5 =G
=R, d5 =G
=R, d4 =R, d6 =B, d7 =R
=G, d7 =R
=G, d5 =G, d6 =B, d8 =R
=G, d7 =R

mj
m2
m1
m1
m3
m3
m5
m1
m1

values
= f , m3 = t, m7 = t, m8 =
= f , m3 =
= f , m2 = f , m4 = t, m5 = f
= t, m5 = f
= t, m4 = t, m6 = f , m7 =
= f , m7 =
= f , m5 = f , m6 = f , m8 =
= f , m7 =

Table 3: Configuration 3.
messages get destinations, two constraints violated (A3 ,A4 ) (A7 ,A8 ).
Consequently, agents A3 , A4 , A7 , A8 want mediate, whereas agents A1 , A2 , A5 ,
A6 wish mediate, since conflicts. denote configuration
3 states agents A5 solution assigned delayed messages
arrived destinations (see Figure 2 Table 3).
now, shown series steps led configuration 1 configuration
3. careful look Figures 1 2 reveals configurations actually isomorphic.
Consequently, next show similar series steps lead us right back
configuration 1.
Agents A3 A7 detect conflicts neighbor
higher priority wants mediate. Consequently, agents A3 A7 start mediation
sessions, since cannot change color consistent state neighbors.
first observe A3 mediation session. A3 sends evaluate? messages
neighbors A1 , A2 , A4 , A5 . agents reply evaluate! messages. A3
conducts Branch Bound search find solution satisfies constraints
A1 , A2 , A3 , A4 , A5 , also minimizes external constraints. Agent A3 finds
solution (A1 Green, A2 Red, A3 Blue, A4 Green, A5 Red), satisfies
231

fiGrinshpoun & Meisels

Agent
A1
A2
A3
A4
A5
A6
A7
A8

Color
G
R
B
G
R
B
B
R

mi
m1
m2
m3
m4
m5
m6
m7
m8

=f
=f
=f
=f
=f
=f
=t
=t

dj values
2 =B, d3 =B, d7 =R, d8 =R
d1 =G, d3 =B
d1 =G, d2 =R, d4 =G, d5 =R
d3 =B, d5 =R
d3 =B, d4 =G, d6 =B, d7 =R
5 =G, d7 =R
d1 =G, 5 =G, d6 =B, d8 =R
d1 =G, d7 =R

mj
m2
m1
m1
m3
m3
m5
m1
m1

values
= f , m3
= f , m3
= f , m2
= f , m5
= f , m4
= f , m7
= f , m5
= f , m7

= f,
=f
= f,
=f
= f,
=t
= f,
=t

m7 = t, m8 =
m4 = f , m5 = f
m6 = f , m7 =
m6 = f , m8 =

Table 4: Configuration 4 obsolete data agent views bold face.

internal constraints, minimizes zero external constraints. A3 sends accept!
messages neighbors, informing solution. A1 , A2 , A4 , A5 receive
accept! messages send ok? messages new states neighbors.
However, ok? messages A2 A1 A5 A6 A7 delayed.
Concurrently mediation session A3 , agent A7 starts mediation
session. A7 sends evaluate? messages neighbors A1 , A5 , A6 , A8 . Let us assume
message A1 delayed. A6 A8 receive evaluate? messages reply
evaluate!, since know agents higher priority A7 want
mediate. A5 , A3 mediation session, replies wait!. denote
configuration 4 states agents point processing (see Table 4).
A3 mediation session over, A1 receives delayed evaluate? message
A7 . Since A1 longer mediation session, expect mediation session
node higher priority A7 (see A1 view Table 4), agent A1 replies
evaluate!. Notice A1 view d2 obsolete (the ok? message A2 A1 still
delayed). agent A7 receives evaluate! message A1 , continue
mediation session involving agents A1 , A6 , A7 , A8 . Since ok? messages A5
A6 A7 also delayed, agent A7 starts mediation session knowledge
agents A2 A5 updated (see bold-faced data Table 4).
Agent A7 conducts Branch Bound search find solution satisfies
constraints A1 , A6 , A7 , A8 , also minimizes external constraints.
example, A7 finds solution (A1 Red, A6 Red, A7 Blue, A8 Green),
satisfies internal constraints, minimizes zero external constraints (remember
A7 wrong data A2 A5 ). A7 sends accept! messages A1 , A6 , A8 ,
informing solution. agents receive messages send ok? messages
new states neighbors. now, delayed messages get
destination, two constraints violated (A1 ,A2 ) (A5 ,A6 ). Consequently, agents
A1 , A2 , A5 , A6 want mediate, whereas agents A3 , A4 , A7 , A8 wish
mediate, since conflicts. Notice agents returned
exact states configuration 1 (see Figure 1 Table 1).
cycle shown configuration 1 configuration 3
continue indefinitely. example contradicts termination completeness
APO algorithm.
232

fiCompleteness Performance APO Algorithm

noted mention messages passed running
example. mentioned messages important understanding
example, since example complicated enough. instance, agent A1 completes
mediation session (before configuration 2 ), straightforward exchange
messages agents, mj values agents become correct (as presented
Table 2).

5. Analyzing Problems
previous section termination problem APO algorithm described
constructing scenario leads infinite loop algorithms run. better
understand completeness problem APO, one must refer completeness proof
APO algorithm given Mailler Lesser (2006). proof based
incorrect assertion mediation session terminates three possible outcomes:
1. solution external conflicts.
2. solution exists.
3. solution least one external violated constraint.
first case, mediator presumably finds solution subproblem.
second case, mediator discovers overall problem unsolvable. third
case, mediator adds agent (or agents) external conflicts found
good list, used define future mediations. way, either solution
solution found (first two cases), good list grows, consequently bringing
problem solving closer centralized solution (third case).
However, infinite loop scenario section 4 shows assertion claiming
three cases cover possible outcomes mediation session incorrect.
two possible reasons incorrectness. first reason possibility
mediator initiates partial mediation session without obtaining lock agents
good list. second reason incorrect information external constraints
neighboring mediation sessions performed concurrently. reasons relate
concurrency mediation sessions.
5.1 Partial Mediation Sessions
first reason incorrectness always growth assertion possibility
mediator initiates partial mediation session without obtaining lock
agents good list. possibility occur earlier engagements
good lists members mediation sessions. APOs code, agents send
wait! message.
Let us consider partial mediation session. Assume mediator finds
solution subproblem, external conflicts agents outside
mediation session. Assume also conflicts agents already
mediators good list. Notice possible, since agents engaged
mediation sessions earlier sent wait! messages mediator. present
mediation session falls case 3 original proof. However, apparent new
agents added good list contradicting assertion.
233

fiGrinshpoun & Meisels

Another possible outcome partial mediation sessions situation agent
several agents entire graph good list try mediate, fail
get lock agents good list. Consequently, situation single
agent holds entire constraint graph, necessarily lead solution, due
oscillation.
5.2 Neighboring Mediation Sessions
second reason incorrectness assertion original proof (Mailler &
Lesser, 2006) potential existence obsolete information external constraints.
reason involves scenario two neighboring mediation sessions performed concurrently. mediation sessions scenario end finding solution
presumably external conflicts, combination solutions causes new
conflicts. case mediation sessions agents A1 A5 example
section 4. scenario seemingly fits first case assertion,
external conflicts found mediation sessions. Consequently, externalconflict-free partial solution found contradicting assertion. Furthermore, none
mediators increase good list. enables occurrence infinite loop,
displayed section 4.

6. Complete Asynchronous Partial Overlay
two-part solution solves completeness problem APO presented. first
part solution insures first reason incorrectness assertion (see
section 5.1) could occur. achieved preventing partial mediation sessions
go without participation entire mediators good list. second part
solution addresses scenario two neighboring mediation sessions performed
concurrently. scenario, results mediation sessions create new
conflicts. order ensure good lists grow rule infinite loop, second
part solution makes sure least one good lists grows. Combined
first part insures mediation sessions involve entire good lists
mediators, completeness APO secured.
6.1 Preventing Partial Mediation Sessions
proposed algorithm disables initiation partial mediation sessions making
mediator wait obtains lock agents good list. Algorithm 4 presents
changes additions APO needed preventing partial mediation sessions.
mediator receives wait! message least one agents
good list, simply cancels mediation session (wait!, line 2) sets counter
special value -1 (wait!, line 3). notify participants canceled mediation
session, mediator sends cancel! message participants (wait!, line 4).
Upon receiving cancel! message, receiving agent updates agent view (cancel!,
line 1) frees mediators lock (cancel!, line 2). However, agent still
234

fiCompleteness Performance APO Algorithm

Algorithm 4 Preventing partial mediation sessions.
received (wait!, (xj , pj ))
1: update agent view (xj , pj );
2: mediate false;
3: counter 1;
4: send (cancel!, (xi , pi )) xj good list;
5: check agent view;
received (evaluate!, (xj , pj , labeledDj ))
1: update agent view (xj , pj );
2: counter 6= 1
3:
record (xj , labeledDj ) preferences;
4:
counter counter 1;
5:
counter = 0 choose solution;
received (cancel!, (xj , pj ))
1: update agent view (xj , pj );
2: mediate false;
3: check agent view;

aware mediators willingness mediate. Consequently, join mediation
session lower priority agent. special value counter used mediator
disregard evaluate! messages arrive wait! message (that causes cancellation)
due asynchronous message passing (evaluate!, line 2).
cancellation mediation session upon receiving single wait! message introduces
need unique identification mediation sessions. Consider wait! message
mediator receives. Upon receiving message, mediator cancels mediation session
calls check agent view. may decide initiate new mediation session. However,
might receive wait! message another agent corresponding previous, already
cancelled, mediation session. Consequently, new mediation session would mistakenly
cancelled too. prevent occurrence problem, unique id added
mediation session. way, mediator could disregard obsolete wait! evaluate!
messages. unique identification mediation sessions removed pseudo-code
order keep simple possible.
approach may imply kind live-lock, repeatedly agent succeeds
initiating mediation sessions. However, live-lock cannot occur due priorities
agents. Consider agent xp highest priority among agents
wish mediate. case agent xp obtains lock agents good list,
initiate mediation session live-lock. interesting situation agent
xp fails get lock agents good list (receives least one wait! message).
Even case agent xp eventually succeed initiating mediation session, since
agents good list aware willingness mediate. agents
moment locked mediators (both initiated mediation sessions mediation
235

fiGrinshpoun & Meisels

sessions canceled) eventually freed either cancel! accept!
messages. Since agents aware agent xp willingness mediate,
join mediation session agent xp (unless xp informs
longer wishes mediate). Consequently, agent xp eventually obtain lock
agents good list contradicting implied live-lock.
6.2 Neighboring Mediation Sessions
Sequential concurrent neighboring mediation sessions may result new conflicts
created without good lists growing. mediation sessions may lead
infinite loop depicted section 4. A7 configuration 2 example agent
participates sequential neighboring mediation sessions (of mediators A1 A5 ).
hand, A3 configuration 2 example agent whose neighbors
incorrect view of, due concurrent mediation sessions.
solution problem subsequent neighboring mediation sessions could obtained agent (for example, A7 configuration 2 ) would agree participate new
mediation session agent view updated changes previous
mediation session. achieved mediator sending entire solution
accept! messages, instead specific dj s. Therefore, sending accept! messages
(choose solution, line 11) Algorithm 2 changed following:
send (accept!, (s, xi , pi , di , mi )) xj ;
Upon receiving revised accept! message (Algorithm 5), agent updates
dk received solution accept dk agent view (accept!, lines
1-3). Notice agent still send ok? messages neighbors (accept!, line 7),
since neighbors necessarily involved mediation session.
solution problem concurrent neighboring mediation sessions could obtained mediator informed post factum new conflict created due
concurrent mediation sessions. manner, mediator add new conflicting
agent good list. Algorithm 5 presents changes additions APO
needed handling concurrent neighboring mediation sessions.
agent xi participating mediation session receives accept! message
mediator, keeps list neighbors (in constraint graph)
included accept! message (not part mediation session), associated
mediator (accept!, lines 4-5). list named conc list, since contains agents
potentially involved concurrent mediation sessions.
Upon receiving ok? message agent xj belonging conc list (ok?, line
2), agent xi checks data received ok? message generates new conflict
xj (ok?, line 3). new conflict generated, agent xj removed
conc list (ok?, line 6). However, case conflict generated (ok?, lines 3-5), agent xi
perceives agent xj involved concurrent mediation sessions
created new conflicts. case, agent xi mediator add agent xj agent view
good list. Hence, agent xi sends new add! message mediator (associated
agent xj conc list). mediator receives add! message adds agent xj
agent view good list (add!, lines 1-2).
236

fiCompleteness Performance APO Algorithm

Algorithm 5 Handling neighboring mediation sessions.
received (accept!, (s, xj , pj , dj , mj ))
1: xk agent view (starting xi )
2:
xk
3:
update agent view (xk , dk );
4:
else di generate conflict existing dk
5:
add (xk , xj ) conc list;
6: mediate false;
7: send (ok?, (xi , pi , di , mi )) xj agent view;
8: update agent view (xj , pj , dj , mj );
9: check agent view;
received (ok?, (xj , pj , dj , mj ))
1: update agent view (xj , pj , dj , mj );
2: xj conc list
3:
dj generates conflict di
4:
tuple (xj , xk ) conc list
5:
send (add!, (xj )) xk ;
6:
remove tuples (xj , xk ) conc list;
7: check agent view;
received (add!, (xj ))
1: send (init, (xi , pi , di , mi , Di , Ci )) xj ;
2: add xj init list;

slight problem solution, since may push problem solving
process become centralized. may happen ok? message agent xj
generates new conflict may actually result later mediation session
agent xj involved in. case, xj mediator already added agent xi
good list. Adding agent xj good list xi mediator necessary
completeness algorithm. lead faster convergence problem
centralized one. Nevertheless, experiments show effect growth good lists
negligible (see section 9.3).
6.3 Preventing Busy-Waiting
insure partial mediation sessions occur, wait! message received mediator (Algorithm 4) causes cancel mediation session (section 6.1). cancellation
session immediately followed call check agent view (wait!, line 5).
call likely result additional attempt agent start mediation
session, due high probability agents view change since previous
mediation attempt. reasons failed previous mediation attempt may well
cause new mediation session succeed also. subsequent mediation attempts
may occur several times mediation session succeeds mediator decides
237

fiGrinshpoun & Meisels

stop attempts. matter fact, mediator remains busy-waiting mode,
either view changes, reasons mediation sessions failure longer valid.
latter case enables mediation session take place.
state busy-waiting adds unnecessary overhead computation load
problem solving. particular, increases number sent messages. prevent
overhead, mediating agent xm work interrupt-based manner rather
busy-waiting manner. interrupt-based approach mediator notified (interrupted)
reason previous mediation sessions failure longer valid. done
ok? message sent mediator agent xw sent preceding
wait! message, caused mediation session fail. agent xw send
ok? message reason caused send wait! message becomes
obsolete. Namely, one following occurs:
mediation session xw involved over.
agent higher priority xm longer wants mediate.
init list xw emptied out.
order remember agents notified (interrupted) one
instances occurs, agent maintains list pending mediators called wait list.
time agent sends wait! message mediator, adds mediator wait list.
Whenever agent sends ok? messages, clears wait list.
changes pseudo-code must applied order use interrupt-based
method. maintain wait list, following line added Algorithm 3
line 3 evaluate? (inside statement):
add xj wait list;
Also, sending ok? messages entire agent view, done example procedure
check agent view line 7 (Algorithm 1), following line added:
empty wait list;
Finally, need interrupt pending mediators whenever reason mediation
sessions failure may longer valid. example, agent removed
init list (init, line 9) Algorithm 1, following lines need added (inside else
statement):
init list ==
send (ok?, (xi , pi , di , mi )) xw wait list;
empty wait list;
lines handle case init list emptied out. Similar additions must
applied deal mentioned cases. Applying interrupt-based method
rules need busy-waiting. Thus, call check agent view (wait!, line 5)
discarded.
238

fiCompleteness Performance APO Algorithm

7. Soundness Completeness
section show CompAPO sound complete. proofs
follow basic structure assumptions original APO proofs (Mailler & Lesser,
2006). original completeness proof incorrect incompleteness
original algorithm. Consequently, use assertion discussed
detail section 3 played key role original (and incorrect) proof
completeness (Mailler & Lesser, 2006). following lemmas needed proofs
soundness completeness.
Lemma 1 Links bidirectional. i.e. xi xj agent view eventually xj
xi agent view.
Proof (as appears work Mailler Lesser, 2006):
Assume xi xj agent view xi agent view xj .
order xi xj agent view, xi must received init message
point xj . two cases.
Case 1: xj init list xi . case, xi must sent xj init message
first, meaning xj received init message therefore xi agent view
contradiction.
Case 2: xj init list xi . case, xi receives init message
xj , responds init message. means reliable communication
assumption holds, eventually xj receive xi init message add xi agent view
also contradiction.
Definition 1 agent considered stable state waiting messages,
message ever reach it.
Definition 2 deadlock state agent conflicts desires
mediate enters stable state.
Lemma 2 deadlock cannot occur CompAPO algorithm.
Proof:
Assume agent xi enters deadlock. means agent xi desires mediate,
stable state. consequence agent xi would able get
lock agents good list.
One possibility xi already invited members good list join mediation session sending evaluate? messages. finite time receive either
evaluate! wait! messages agents good list. Depending replies,
xi either initiates mediation session cancels it. Either way, xi stable state
contradicting assumption.
possibility xi reach stage invites agents
join mediation session. happen, exists least one agent xj
xi point view desires mediate (mj = true) higher priority xi
(pj > pi ). two cases xj would mediate session included xi ,
xi expecting to:
239

fiGrinshpoun & Meisels

Case 1: xi mj = true agent view actual value false.
Assume xi mj = true agent view actually mj = false. would
mean point xj changed value mj false without informing xi
it. one place xj changes value mj check agent view
procedure. Note procedure, whenever flag changes true false,
agent sends ok? messages agents agent view. Since Lemma 1
know xi agent view xj , agent xi must eventually received message
informing mj = false, contradicting assumption.
Case 2: xj believes xi mediating xi believe be.
xj point view, mi = true pi > pj . previous case, know xj
believes mi true (mi = true) must case. need show
condition pi > pj impossible. Assume xj believes pi > pj fact
pi < pj . means point xi sent message xj informing current
priority pi . Since know priorities increase time (all good lists
get larger), know pi pi (xj always correct value underestimate
pi ). Since pi < pj pi pi pi < pj contradiction assumption.
Definition 3 algorithm considered stable state agents
stable state.
Theorem 1 CompAPO algorithm sound. i.e., reaches stable state
either found answer solution exists.
Proof:
assume agents reach stable state, consider cases
happen.
Case 1: agent conflicts. case, agents stable state
conflicts. means current value agent variable
satisfies constraints. Consequently, current values valid solution
overall problem, CompAPO algorithm found answer.
Case 2: solution message broadcast. case, least one agent
found subproblem solution, informed agents
broadcasting solution message. Consequently, agent receives message
(all agents) stops run reports solution exists.
Case 3: agents conflicts. Let us consider agent xi conflict.
Since conflict, xi desires mediate. able perform mediation session
stable state contradiction assumption. Therefore, condition
xi remain stable state expecting mediation request
higher priority agent xj send words, deadlocked.
Lemma 2 cannot happen.
Since cases 1 2 occur, CompAPO algorithm reaches stable state
either found answer solution exists. Consequently, CompAPO
algorithm sound.

240

fiCompleteness Performance APO Algorithm

Lemma 3 exist agents hold entire graph good list desire
mediate, one agents perform mediation session.
Proof:
shall consider two cases one agent holds entire
graph good list desires mediate, several agents.
Case 1: Consider agent xi agent holds entire graph
good list desires mediate. Since xi entire graph good list
highest possible priority. Moreover, agents aware xi priority (pi ) desire
mediate (mi ) due ok? messages received xi containing information
(xi sent ok? messages agents agent view, holds entire graph).
Consequently, agent engage point on, mediation session
xi s. Since mediation sessions finite new mediation sessions occur, agent
xi eventually get lock agents perform mediation session.
Case 2: several agents exist, tie priorities broken
agents index. Consider xi one highest index agents,
apply proof case 1.
Lemma 4 agent holding entire graph good list performs mediation session,
algorithm reaches stable state.
Proof:
Consider mediator agent xi . Following first part CompAPOs solution
(section 6.1), agent perform mediation session received evaluate! messages agents good list. Since xi holds entire graph good list,
means agents graph sent evaluate! messages xi set
mediate flags true. means xi completes search returns accept! messages solution, agent change assignment. Assuming
centralized internal solver xi uses sound complete, find solution
entire problem solution exists, alternatively conclude solution exists.
solution exists, xi informs agents problem solving
terminates. Otherwise, agent receives accept! message xi contains
solution entire problem. Consequently, agent conflicts algorithm
reaches stable state.
Lemma 5 Infinite value changes without mediation sessions cannot occur.
Proof:
proof focus line 6 check agent view procedure, place
code value changed without mediation session. reminder, notice
agents graph ordered priority (ties broken IDs
agents).
Consider agent lowest priority (xp1 ). Agent xp1 cannot change
value, since line 5 check agent view procedure states order reach
value change line 6, current value must conflict exclusively lower priority agents.
Clearly impossible agent xp1 , lowest priority graph.
241

fiGrinshpoun & Meisels

Now, consider next agent ordering, xp2 . Agent xp2 change current
value conflict exclusively lower priority agents. lower priority
agent case xp1 . xp1 xp2 neighbors, agent xp2 cannot change
value reason agent xp1 . Otherwise, agent xp2 know upto-date value agent xp1 finite time (any previously sent updates regarding xp1 value
eventually reach agent xp2 ), since proved value xp1 cannot changed
without mediation session. xp2 up-to-date value lower priority
neighbors (only xp1 ), change value without mediation session.
Eventually, neighbors xp2 updated final change value.
general, agent xpi (including highest priority agent) finite time
up-to-date values lower priority agents. happens, change
value without mediation session. Eventually, neighbors xpi
updated final change value. Thus, infinite value changes without
mediation sessions cannot occur.
proof implicitly relies fact ordering agents change.
However, priority agent may change time. Nevertheless, priorities
bounded size graph, number priority changes finite. proves
value changes cannot indefinitely occur without mediation sessions.
Lemma 6 point agent mediate desire mediate, algorithm
reach stable state.
Proof:
shall consider two cases messages yet arrived
destinations, messages.
Case 1: Consider case messages yet arrived
destination. agent desires mediate, mi false, meaning
agent graph conflicts. Consequently, current state graph solution
satisfies constraints, algorithm reaches stable state.
Case 2: Consider case messages yet arrived
destinations. Eventually messages arrive. According assumption
lemma, arrival messages make agents desire mediate.
Next, consider arrival type message show cannot lead infinite
exchange messages:
evaluate?, evaluate!, wait!, cancel!: messages must belong obsolete
mediation session, otherwise contradict assumption lemma. Accordingly,
may result limited exchange messages (e.g., sending wait! line 3
evaluate?). messages may lead call check agent view
procedure.
accept!: message cannot received without contradicting assumption,
since receiving agent active mediation session receiving
accept! message.
init: message part handshake two agents. Consequently,
single additional init message sent. leads call
check agent view procedure involved agents.
242

fiCompleteness Performance APO Algorithm

add!: message results sending single init message.
ok?: message may result sending finite number add! messages.
also leads call check agent view procedure.
examining types messages, conclude message lead
finite exchange messages, finite number calls check agent view
procedure. need show call check agent view cannot lead infinite
exchange messages.
check agent view procedure 4 possible outcomes. may simply return (line
2), change value variable (lines 6-7), mediate (line 9), update desire mediate (lines 11-12). According assumption lemma, cannot mediate. update
desire mediate, means value mi true, updated true.
Either way, contradiction assumption lemma. Consequently,
possibilities simple return, change value variable. According Lemma 5, value changes cannot indefinitely occur without mediation
sessions. Consequently, final messages eventually arrive destinations,
first case proof hold.
Definition 4 One says algorithm advances least one good lists grows.
Lemma 7 every n mediation sessions, algorithm either advances reaches
stable state.
Proof:
Consider mediation session agent xi . mediation session three possible
outcomes solution satisfying constraints within good list, solution satisfying
constraints within good list violations external constraints, solution
satisfying constraints within good list external constraints.
Case 1: solution satisfies constraints within xi good list exists, therefore
entire problem unsatisfiable. case, xi informs agents
problem solving terminates.
Case 2: xi finds solution satisfies constraints within good list violates
external constraints. case, xi adds agents external conflicts
good list. agents already xi good list, since mediation session
included entire good list xi (according section 6.1). Consequently, xi good list
grows algorithm advances.
Case 3: xi finds solution satisfies constraints within good list
external constraints. Following second part CompAPOs solution (section 6.2), agents
xi good list maintain conc list, would notify xi add agents good list
case experience new conflicts due concurrent mediation sessions. case,
xi would notified, good list would grow algorithm would advance.
situation algorithm advance reach stable state,
mediation sessions experience case 3, concurrent mediation sessions
create new conflicts. case, n mediation sessions (equal overall
number agents), agents would desire mediate. According Lemma 6,
algorithm reaches stable state.
243

fiGrinshpoun & Meisels

Lemma 8 exists group agents desire mediate, mediation session
eventually occur.
Proof:
agent manage get lock agents good list (essential
mediation session occur) agents group sent evaluate? messages
got least one wait! message each. case, consider xi highest priority
agent among group.
wait! message agent xi received either agent member
group agent outside group, currently involved another mediation
session. case agent (xj ) belongs group, xj also got wait! message
(clearly, wait! message arrived xj sent wait! xi ). xj therefore cancel
mediation session, wait xi next evaluate? message (since xj aware
xi desire mediate pi highest priority among agents currently desire
mediate). case xj belong group, mediation session xj involved
eventually terminate, xi get lock, unless xj higher priority
xi (pj > pi ) also xj desires mediate session terminates. case,
xj eventually get lock reasons.
Theorem 2 CompAPO algorithm complete. i.e., solution exists, algorithm
find it, solution exist, algorithm report fact.
Proof:
Since shown Theorem 1 whenever algorithm reaches stable state,
problem solved finds subset variables unsatisfiable
terminates, need show always reaches one two states finite
time.
According Lemma 6, point time agent mediate desire
mediate, algorithm reach stable state. According Lemma 8 exist
agents desire mediate, eventually mediation session occur. Lemmas 6
8 conclude possibility algorithm reach stable state
continuous occurrences mediation sessions. According Lemma 7, every n
mediation sessions, algorithm either advances reaches stable state. Consequently,
algorithm either reaches stable state continuously advances.
case algorithm continuously advances, good lists continuously grow.
point, agents (eventually agents) hold entire graph good list.
One agents eventually desire mediate (if not, according Lemma 6,
algorithm reaches stable state). According Lemma 3, one agents perform
mediation session. According Lemma 4, algorithm reaches stable state.

8. OptAPO Optimizing APO
Distributed Constraint Optimization Problems (DisCOPs) version distributed constraint problems, goal find optimal solution problem, rather
satisfying one. optimization problem, agent associates cost violated
constraints maintains bounds costs order reach optimal solution
minimizes number violated constraints.
244

fiCompleteness Performance APO Algorithm

number algorithms proposed last years solving DisCOPs.
simplest algorithm Synchronous Branch Bound (SyncBB) (Hirayama &
Yokoo, 1997), distributed version well-known centralized Branch
Bound algorithm. Another algorithm uses Branch Bound scheme Asynchronous Forward Bounding (AFB) (Gershman, Meisels, & Zivan, 2006), agents
perform sequential assignments propagated bounds checking early detection need backtrack. number algorithms use pseudo-tree derived
structure DisCOP order improve process acquiring solution
optimization problem. ADOPT (Modi, Shen, Tambe, & Yokoo, 2005)
asynchronous algorithm assignments passed pseudo-tree. Agents
compute upper lower bounds possible assignments send costs parents pseudo-tree. costs eventually accumulated root agent. Another
algorithm exploits pseudo tree DPOP (Petcu & Faltings, 2005). DPOP,
agent receives agents sons pseudo-tree, combinations
partial solutions sub-tree corresponding costs. agent calculates
generates possible partial solutions include partial solutions received
sons assignments sends resulting combinations pseudotree. root agent receives information sons, produces optimal
solution propagates pseudo-tree rest agents.
Another different approach implemented Optimal Asynchronous Partial Mediation (OptAPO) (Mailler & Lesser, 2004; Mailler, 2004) algorithm,
optimization version APO algorithm. Differently APO, OptAPO algorithm
introduces second type mediation sessions called passive mediation sessions. goal
passive sessions update bounds costs without changing values
variables. sessions add parallelism algorithm accelerate distribution
information. might solve many problems result incorrect information,
discussed section 5.2. However, active mediation sessions also occur OptAPO.
active sessions may consist parts good list (partial mediation sessions),
result lead problems described section 5.1. Moreover, satisfiable problem
also solved OptAPO, returning zero optimal cost. Therefore, infinite loop scenario described section 4 also occur OptAPO, behaves like APO
problem satisfiable.
OptAPO algorithm must corrected order aforementioned problems
solved. section 6 several modifications APO algorithm proposed.
changes turn APO complete search algorithm CompAPO. Equivalent modifications
must also applied OptAPO algorithm order ensure correctness. Interestingly, modifications APO procedures similar APO OptAPO.
main differences APO OptAPO addition passive mediation sessions (procedure check agent view) OptAPO, internal search
mediators perform (procedure choose solution). However, neither procedures
effected modifications CompAPO. Thus, pseudo-code changes must
applied OptAPO similar modifications CompAPO, therefore
omitted paper. performance resulting algorithm CompOptAPO
evaluated section 9.5. full pseudo-code original OptAPO algorithm
found work Mailler Lesser (2004).
245

fiGrinshpoun & Meisels

9. Experimental Evaluation
original (and incomplete) version APO algorithm evaluated Mailler
Lesser (2006). compared AWC algorithm (Yokoo, 1995),
efficient DisCSP solver (Zivan et al., 2007). experiments performed 3-coloring
problems, subclass uniform random constraints problems. problems
characterized small domain size, low constraints density, fixed constraints tightness
(for characterization random CSPs see works Prosser, 1996 Smith, 1996).
comparison APO AWC (Mailler & Lesser, 2006) made respect
three measures number sent messages, number cycles, serial runtime.
number sent messages important widely accepted measure,
measures problematic. cycle, incoming messages delivered,
agent allowed process information, messages created
processing added outgoing queue delivered beginning
next cycle. meaning cycle APO mediation session possibly
involves entire graph takes single cycle. measure clearly problematic,
since every centralized algorithm solves problem one cycle. Measuring serial
runtime also adequate distributed CSPs, since take account
concurrent computations problem solving. order measure concurrent
runtime DisCSP algorithms implementation independent way, one needs count
non-concurrent constraint checks (NCCCs) (Meisels, Razgon, Kaplansky, & Zivan, 2002).
measure gained global agreement DisCSP DisCOP community (Bessiere
et al., 2005; Zivan & Meisels, 2006b) used present evaluation.
modifications CompAPO, especially prevention partial mediation
sessions (section 6.1) add synchronization algorithm, may tax heavily
performance algorithm. Thus, important evaluate effect changes
comparing CompAPO (incomplete) versions APO algorithm. Additionally,
evaluate effectiveness interrupt-based method compared busy-waiting.
9.1 Experimental Setup
experiments use simulator agents simulated threads,
hold shared memory communicate message passing. network
constraints, experiments, generated randomly selecting probability
p1 constraint among pair variables probability p2 , occurrence
violation among two assignments values constrained pair variables.
uniform random constraints networks n variables, k values domain, constraints
density p1 tightness p2 commonly used experimental evaluations CSP
algorithms (Prosser, 1996; Smith, 1996).
Experiments conducted several density values. setup included problems
generated 15 agents (n = 15) 10 values (k = 10). drew 100 different instances
combination p1 p2 . experiments agent holds single
variable.
246

fiCompleteness Performance APO Algorithm

Figure 3: Mean NCCCs sparse problems (p1 = 0.1).

Figure 4: Mean NCCCs medium density problems (p1 = 0.4).
9.2 Comparison Algorithms
performance CompAPO compared three asynchronous search algorithms
well known Asynchronous Backtracking (ABT) (Yokoo et al., 1998; Yokoo & Hirayama,
2000), extremely efficient Asynchronous Forward-Checking Backjumping (AFCCBJ) (Meisels & Zivan, 2007), Asynchronous Weak Commitment (AWC) (Yokoo,
1995), used original APO evaluation (Mailler & Lesser, 2006).
Results presented three sets tests different values problem density
sparse (p1 = 0.1), medium (p1 = 0.4), dense (p1 = 0.7). sets value p2
varies 0.1 0.9, cover ranges problem difficulty.
247

fiGrinshpoun & Meisels

Figure 5: Mean NCCCs dense problems (p1 = 0.7).

Figure 6: Mean number messages medium density problems (p1 = 0.4).

order evaluate performance algorithms, two independent measures
performance used search effort form NCCCs communication load
form total number messages sent. Figures 3, 4, 5 present number
NCCCs performed CompAPO solving problems different densities. Figure 6
shows total number messages sent problem solving process. figures
exhibit phase-transition phenomenon increasing values tightness, p2 , problem
difficulty increases, reaches maximum, drops back low value.
termed easy-hard-easy transition hard problems (Prosser, 1996), observed
DisCSPs (Meisels & Zivan, 2007; Bessiere et al., 2005).
248

fiCompleteness Performance APO Algorithm

performance CompAPO NCCCs turns poor phase
transition region compared asynchronous search algorithms. worst results
problems relatively sparse (Figures 3 4). However, even dense problems
ABT AFC-CBJ clearly outperform CompAPO (Figure 5). comparing
CompAPO AWC, results significantly different. AWC known perform best
sparse problems. Thus, like ABT AFC-CBJ clearly outperforms CompAPO
problems (Figure 3). medium density problems, AWC still performs better
CompAPO difference performances algorithms much smaller
(Figure 4). dense problems, AWC performs extremely bad ten times
NCCCs CompAPO. results AWC omitted Figure 5, since
finish running set tests reasonable time stop run 40
hours.
Notice scale Figure 4 different Figures 3 5. due
especially poor performance APO around phase transition medium density
problems. behavior untypical, since DisCSP algorithm suffer worst
performance around phase transition high density problems (Figure 5). fact
performance CompAPO better high density problems medium
density ones explained faster convergence centralized solution dense
problems. problems around phase transition, CompAPO algorithm frequently
reaches full centralization anyway. Thus, faster convergence centralized solution
actually improves performance algorithm.
search effort performed agents running CompAPO extremely high,
communication load system remains particularly low. seen
Figure 6, medium density problems. Similar results achieved sparse dense
problems. surprising, since major part search effort carried
agents performing mediation sessions without need extensive exchange
messages.
9.3 Comparison Versions APO
Several versions APO algorithm proposed Benisch Sadeh (2006). One
versions (APO-BT) uses simple backtracking mediation procedure, instead
Branch Bound originally proposed APO (APO-BB). performance
CompAPO compared two incomplete versions algorithm.
modifications CompAPO, especially prevention partial mediation
sessions (section 6.1) add synchronization algorithm. potential partial mediation
session must wait sessions end mediator able get lock entire
good list. synchronization may tax performance algorithm. Nevertheless,
experiments show CompAPO actually performs slightly better APO-BB
measured NCCCs (Figures 7 8). improved performance explained
better distribution data entire solution sent accept! message
(section 6.2). Figure 9 shows effect CompAPOs modifications even greater
communication load. substantial advantage CompAPO may explained
use interrupt-based approach (section 6.3) helps performance eliminating
unnecessary overhead busy-waiting.
249

fiGrinshpoun & Meisels

Figure 7: Mean NCCCs medium density problems (p1 = 0.4).

Figure 8: Mean NCCCs dense problems (p1 = 0.7).
Figure 10 presents mean size largest mediation session occurring search,
medium density problems (p1 = 0.4) 15 variables. average size largest
mediation session around 12 (out maximum 15). occurs problems
phase transition region p2 0.5 0.6. Although number far
maximum 15, suggest considerable portion hard problems
solved without reaching full centralization.
part code CompAPO solves neighboring mediation sessions problem (section 6.2) implies potential additional growth good lists (ok?, line 5),
may result faster centralization problem solving. Nevertheless, Figure 10 clearly
shows CompAPO centralize faster original version APO (APOBB), except tight, unsolvable problems.
250

fiCompleteness Performance APO Algorithm

Figure 9: Mean number messages medium density problems (p1 = 0.4).

Figure 10: Mean size largest mediation session (p1 = 0.4 n = 15).
experiments show medium density problems, APO-BT version performs
poorly respect NCCCs number sent messages comparison
APO-BB CompAPO (Figures 7 9). reason ABO-BTs poor performance
easily explained frequent convergence full centralization shown Figure 10. Nevertheless, APO-BT lower communication load APO-BB phase
transition. reason actually reason leads APO-BTs extensive search effort. prompt convergence full centralization yields high search effort
(NCCCs), time may reduce communication load.
Figure 8 shows dense problems APO-BT performs better APO-BB
almost CompAPO. supports results reported Benisch Sadeh
(2006) dense random DisCSPs. paper also presents results structured
251

fiGrinshpoun & Meisels

Figure 11: Interrupt-based vs. busy-waiting (mean NCCCs p1 = 0.4).
3-coloring problems, APO-BT outperformed APO-BB. Similar behavior
observed experiments conducted sparser problems (Figure 7),
suggests variance APO-BTs performance density
problem structure.
Benisch Sadeh propose additional version APO algorithm,
mediation session selection rule inverse original selection rule (Benisch & Sadeh,
2006). version called IAPO instructs agents choose smallest mediation session
rather largest one. clear IAPO turned correct algorithm,
since correctness proofs presented section 7 rely fact largest mediation
sessions chosen. Consequently, evaluation IAPO omitted paper.
9.4 Interrupt-Based Versus Busy-Waiting
Figures 11 12 present two measures performance comparing different methods
synchronization needed order avoid conflicts concurrent mediation
sessions interrupt-based busy-waiting (section 6.3). interrupt-based method
clearly outperforms busy-waiting harder problem instances. Predictably, difference
performance pronounced measuring number messages (Figure 12).
9.5 Evaluation CompOptAPO
original (and incomplete) version OptAPO algorithm evaluated Mailler
Lesser (2004). compared ADOPT algorithm (Modi et al., 2005),
best DisCOP solver. Similarly original results APO (Mailler & Lesser,
2006), comparison OptAPO ADOPT (Mailler & Lesser, 2004) made
respect three measures number sent messages, number cycles,
serial runtime. reasons DisCSP algorithms, cycles serial runtime
also problematic measuring performance DisCOP algorithms. case
252

fiCompleteness Performance APO Algorithm

Figure 12: Interrupt-based vs. busy-waiting (mean number messages p1 = 0.4).
CompAPO, CompOptAPO algorithm also evaluated counting NCCCs
number sent messages.
Distributed Optimization problems used following experiments random
Max-DisCSPs. Max-DisCSP subclass DisCOP constraint costs (weights)
equal one (Modi et al., 2005). feature simplifies task generating random problems, since using Max-DisCSPs one decide costs
constraints. Max-CSPs commonly used experimental evaluations constraint
optimization problems (COPs) (Larrosa & Schiex, 2004). experimental evaluations
DisCOPs include graph coloring problems (Modi et al., 2005; Zhang, Xing, Wang, &
Wittenburg, 2005), subclass Max-DisCSP. advantage using random
Max-DisCSP problems fact create evaluation framework known
exhibit phase-transition phenomenon centralized COPs. important
evaluating algorithms solving DisCOPs, enabling known analogy behavior centralized algorithms problem difficulty changes. problems solved section
randomly generated Max-DisCSP 10 agents (n = 10) 10 values (k = 10), constraint density either p1 = 0.4 p1 = 0.7, varying constraint tightness 0.4 p2 < 1.
performance CompOptAPO compared three search algorithms Synchronous Branch Bound (SyncBB) (Hirayama & Yokoo, 1997), AFB (Gershman et al.,
2006), ADOPT (Modi et al., 2005). ADOPT used original OptAPO evaluation (Mailler & Lesser, 2004).
must noted experiments original OptAPO algorithm,
experienced several runs algorithm failed advance reach
solution. shows termination problem OptAPO occurs practice
theory, scenario involves particular message delays one presented
section 4 APO algorithm. Additionally, discovered OptAPO may
always able report optimal cost (i.e., number broken constraints MaxDisCSP experiments). understand happen consider almost disjoint
253

fiGrinshpoun & Meisels

Figure 13: example 3-coloring problem.

Figure 14: Mean NCCCs sparse optimization problems (p1 = 0.4).

graph, one depicted Figure 13. example assume agents A1
A2 conflicts. Consequently, knowledge regarding cost F
exchanged two groups, agent holds correct overall cost
problem (F = 3). Nevertheless, OptAPO algorithm terminates,
optimal solution. Thus, optimal value derived upon termination summing
number broken constraints agents. result must divided two
account broken constraints counted involved agents.
performance CompOptAPO NCCCs comparable DisCOP algorithms problems relatively loose (low p2 value), ADOPT
algorithm performing slightly better. case sparse dense problems
(Figures 14 15, respectively). problems become tighter, CompOptAPO clearly
outperforms ADOPT SyncBB. fact, ADOPT algorithm failed terminate
reasonable time tight problems (p2 > 0.8 Figure 14 p2 > 0.6 Figure 15). However, tight problems AFB algorithm much faster CompOptAPO. Actually,
AFB algorithm experiments managed terminate reasonable
time problems dense (p1 = 0.7) tight (p2 = 0.9).
254

fiCompleteness Performance APO Algorithm

Figure 15: Mean NCCCs dense optimization problems (p1 = 0.7).

Figure 16: Mean number messages dense optimization problems (p1 = 0.7).

Similarly CompAPO, communication load system remains particularly
low running CompOptAPO algorithm. seen Figure 16 dense
problems. Similar results observed sparse problems. surprising, since
major part search effort carried agents performing mediation sessions
without need extensive exchange messages.

255

fiGrinshpoun & Meisels

10. Conclusions
APO search algorithm asynchronous distributed algorithm DisCSPs.
algorithm partitions search different subproblems. subproblem solved
selected agent mediator. conflicts arise solution subproblem
neighboring agents, conflicting agents added subproblem. Ideally,
algorithm either leads compatible solutions constraining subproblems,
growth subproblems whose solution incompatible neighboring agents. twooption situation used original APO paper (Mailler & Lesser, 2006) prove
termination completeness algorithm.
proof completeness APO algorithm presented Mailler Lesser
(2006) based growth size subproblems. turns expected
growth groups occur situations, leading termination problem
algorithm. present paper demonstrates problem following example
terminate. Furthermore, paper identifies problematic parts
original algorithm interfere completeness applies modifications solve
problematic parts. resulting CompAPO algorithm ensures completeness
search. Formal proofs soundness completeness CompAPO presented.
CompAPO algorithm forms class DisCSP search algorithms. contrast backtracking concurrent search processes, achieves concurrency solving
subproblems concurrently. therefore interesting important evaluate
performance CompAPO compare DisCSP search algorithms.
Asynchronous Partial Overlay actually family algorithms. completeness
termination problems presented corrected present study apply
members family. OptAPO algorithm (Mailler & Lesser, 2004; Mailler, 2004)
optimization version APO solves Distributed Constraint Optimization Problems
(DisCOPs). present paper shows similar modification ones made
APO algorithm must also applied OptAPO order ensure correctness.
changes call performance evaluation resulting CompOptAPO algorithm.
experimental evaluation presented section 9 demonstrates
performance CompAPO poor compared asynchronous search algorithms.
randomly generated DisCSPs runtime APO, measured NCCCs, longer
two orders magnitude ABT (Yokoo et al., 1998; Yokoo & Hirayama,
2000) AFC-CBJ (Meisels & Zivan, 2007).
total number messages sent CompAPO considerably smaller corresponding number ABT AFC-CBJ. clear result fact hard problem
instances tend solved small number mediators semi-centralized manner.
runtime performance CompOptAPO better ADOPT (Modi et al.,
2005) SyncBB (Hirayama & Yokoo, 1997) hard instances randomly generated
DisCOPs. Similarly DisCSP case, total number messages sent CompOptAPO considerably smaller corresponding number DisCOP algorithms.
However, phase-transition region randomly generated DisCOPs, runtime
CompOptAPO longer order magnitude AFB (Gershman
et al., 2006).
256

fiCompleteness Performance APO Algorithm

References
Benisch, M., & Sadeh, N. (2006). Examining DCSP coordination tradeoffs. Proceedings
Fifth International Joint Conference Autonomous Agents Multiagent
Systems (AAMAS06), pp. 14051412. ACM.
Bessiere, C., Maestre, A., Brito, I., & Meseguer, P. (2005). Asynchronous backtracking
without adding links: new member ABT family. Artificial Intelligence, 161:12, 724.
Gershman, A., Meisels, A., & Zivan, R. (2006). Asynchronous forward-bounding distributed constraints optimization. Proc. ECAI-06, pp. 103107.
Grinshpoun, T., & Meisels, A. (2007). CompAPO: complete version APO algorithm. Proceedings 2007 IEEE/WIC/ACM International Conference
Intelligent Agent Technology (IAT 2007), pp. 370376.
Grinshpoun, T., Zazon, M., Binshtok, M., & Meisels, A. (2007). Termination problem
APO algorithm. Proceedings Eighth International Workshop Distributed
Constraint Reasoning (DCR07), pp. 113124.
Hirayama, K., & Yokoo, M. (1997). Distributed partial constraint satisfaction problem.
Proceedings Third International Conference Principles Practice
Constraint Programming (CP-97), pp. 222236.
Larrosa, J., & Schiex, T. (2004). Solving weighted csp maintaining arc consistency.
Artificial Intelligence, 159, 126.
Mailler, R. (2004). mediation-based approach cooperative, distributed problem solving.
Ph.D. thesis, University Massachusetts.
Mailler, R., & Lesser, V. (2004). Solving distributed constraint optimization problems using
cooperative mediation. Proceedings Third International Joint Conference
Autonomous Agents MultiAgent Systems (AAMAS04), pp. 438445. ACM.
Mailler, R., & Lesser, V. (2006). Asynchronous partial overlay: new algorithm solving
distributed constraint satisfaction problems. Journal Artificial Intelligence Research
(JAIR), 25, 529576.
Meisels, A., Razgon, I., Kaplansky, E., & Zivan, R. (2002). Comparing performance
distributed constraints processing algorithms. Proc. AAMAS-2002 Workshop
Distributed Constraint Reasoning DCR, pp. 8693.
Meisels, A., & Zivan, R. (2007). Asynchronous forward-checking DisCSPs. Constraints,
12 (1), 131150.
Modi, P. J., Shen, W., Tambe, M., & Yokoo, M. (2005). Adopt: asynchronous distributed
constraints optimization quality guarantees. Artificial Intelligence, 161:1-2, 149
180.
Petcu, A., & Faltings, B. (2005). scalable method multiagent constraint optimization.
Proceedings International Joint Conference Artificial Intelligence, pp.
266271.
257

fiGrinshpoun & Meisels

Prosser, P. (1996). empirical study phase transitions binary constraint satisfaction
problems. Artificial Intelligence, 81, 81109.
Semnani, S. H., & Zamanifar, K. (2007). MaxCAPO: new expansion APO solve
distributed constraint satisfaction problems. Proceedings International Conference Artificial Intelligence Soft Computing (ASC 2007).
Smith, B. M. (1996). Locating phase transition binary constraint satisfaction problems. Artificial Intelligence, 81, 155 181.
Yokoo, M. (1995). Asynchronous weak-commitment search solving distributed constraint satisfaction problems. Proceedings First International Conference
Principles Practice Constraint Programming (CP-95), pp. 88 102.
Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1998). Distributed constraint satisfaction problem: Formalization algorithms.. IEEE Trans. Data Kn. Eng.,
10, 673685.
Yokoo, M., & Hirayama, K. (2000). Algorithms distributed constraint satisfaction:
review. Autonomous Agents Multi-Agent Systems, 3 (2), 185207.
Zhang, W., Xing, Z., Wang, G., & Wittenburg, L. (2005). Distributed stochastic search
distributed breakout: properties, comparison applications constraints optimization problems sensor networks. Artificial Intelligence, 161:1-2, 5588.
Zivan, R., & Meisels, A. (2006a). Concurrent search distributed CSPs. Artificial Intelligence, 170 (4), 440461.
Zivan, R., & Meisels, A. (2006b). Message delay DisCSP search algorithms. Annals
Mathematics Artificial Intelligence(AMAI), 46(4), 415439.
Zivan, R., Zazone, M., & Meisels, A. (2007). Min-domain ordering asynchronous backtracking. Proceedings 13th International Conference Principles Practice Constraint Programming (CP-2007), pp. 758772.

258

fiJournal Artificial Intelligence Research 33 (2008) 465-519

Submitted 05/08; published 12/08

AND/OR Multi-Valued Decision Diagrams (AOMDDs)
Graphical Models
Robert Mateescu

MATEESCU @ PARADISE . CALTECH . EDU

Electrical Engineering Department
California Institute Technology
Pasadena, CA 91125, USA

Rina Dechter

DECHTER @ ICS . UCI . EDU

Donald Bren School Information Computer Science
University California Irvine
Irvine, CA 92697, USA

Radu Marinescu

R . MARINESCU @4 C . UCC . IE

Cork Constraint Computation Centre
University College Cork, Ireland

Abstract
Inspired recently introduced framework AND/OR search spaces graphical models, propose augment Multi-Valued Decision Diagrams (MDD) nodes, order
capture function decomposition structure extend compiled data structures general weighted graphical models (e.g., probabilistic models). present AND/OR Multi-Valued
Decision Diagram (AOMDD) compiles graphical model canonical form supports polynomial (e.g., solution counting, belief updating) constant time (e.g. equivalence
graphical models) queries. provide two algorithms compiling AOMDD graphical
model. first search-based, works applying reduction rules trace memory
intensive AND/OR search algorithm. second inference-based uses Bucket Elimination
schedule combine AOMDDs input functions via APPLY operator.
algorithms, compilation time size AOMDD are, worst case, exponential
treewidth graphical model, rather pathwidth known ordered binary decision
diagrams (OBDDs). introduce concept semantic treewidth, helps explain
size decision diagram often much smaller worst case bound. provide
experimental evaluation demonstrates potential AOMDDs.

1. Introduction
paper extends decision diagrams AND/OR multi-valued decision diagrams (AOMDDs)
shows graphical models compiled data-structures. work presented
paper based two existing frameworks: (1) AND/OR search spaces graphical models
(2) decision diagrams.
1.1 AND/OR Search Spaces
AND/OR search spaces (Dechter & Mateescu, 2004a, 2004b, 2007) proven unifying
framework various classes search algorithms graphical models. main characteristic
exploitation independencies variables search, provide exponential
speedups traditional search methods viewed traversing structure.
c
2008
AI Access Foundation. rights reserved.

fiM ATEESCU , ECHTER & ARINESCU

nodes capture problem decomposition independent subproblems, nodes represent branching according variable values. AND/OR spaces accommodate dynamic variable
ordering, however current work focuses static decomposition. Examples AND/OR
search trees graphs appear later, example Figures 6 7.
AND/OR search space idea originally developed heuristic search (Nilsson, 1980).
context graphical models, AND/OR search (Dechter & Mateescu, 2007) also inspired
search advances introduced sporadically past three decades constraint satisfaction
recently probabilistic inference optimization tasks. Specifically, resembles
pseudo tree rearrangement (Freuder & Quinn, 1985, 1987), adapted subsequently distributed constraint satisfaction Collin, Dechter, Katz (1991, 1999) recently
Modi, Shen, Tambe, Yokoo (2005), also shown related graph-based backjumping (Dechter, 1992). work extended Bayardo Miranker (1996) Bayardo
Schrag (1997) recently applied optimization tasks Larrosa, Meseguer, Sanchez
(2002). Another version viewed exploring AND/OR graphs presented recently constraint satisfaction (Terrioux & Jegou, 2003b) optimization (Terrioux & Jegou,
2003a). Similar principles introduced recently probabilistic inference, algorithm Recursive Conditioning (Darwiche, 2001) well Value Elimination (Bacchus, Dalmao, & Pitassi,
2003b, 2003a), currently core advanced SAT solvers (Sang, Bacchus,
Beame, Kautz, & Pitassi, 2004).
1.2 Decision Diagrams
Decision diagrams widely used many areas research, especially software hardware
verification (Clarke, Grumberg, & Peled, 1999; McMillan, 1993). BDD represents Boolean
function directed acyclic graph two terminal nodes (labeled 0 1), every internal
node labeled variable exactly two children: low 0 high 1. isomorphic
nodes merged, would full search tree, also called Shannon tree,
usual full tree explored backtracking algorithm. tree ordered variables encountered
order along every branch. compressed merging isomorphic nodes
(i.e., label identical children), eliminating redundant nodes (i.e., whose
low high children identical). result celebrated reduced ordered binary decision
diagram, OBDD short, introduced Bryant (1986). However, underlying structure
OR, initial Shannon tree tree. AND/OR search trees reduced node
merging redundant nodes elimination get compact search graph viewed
BDD representation augmented nodes.
1.3 Knowledge Compilation Graphical Models
paper combine two ideas, creating decision diagram AND/OR structure, thus exploiting problem decomposition. detail, number values also increased
two constant. context constraint networks, decision diagrams used
represent whole set solutions, facilitating solutions count, solution enumeration queries
equivalence constraint networks. benefit moving structure AND/OR
lower complexity algorithms size compiled structure. typically moves
bounded exponentially pathwidth pw , characteristic chain decompositions
linear structures, exponentially bounded treewidth w , characteristic tree
466

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

structures (Bodlaender & Gilbert, 1991) (it always holds w pw pw w log n,
n number variables model). cases, compactness result achieved
practice often far smaller bounds suggest.
decision diagram offers compilation propositional knowledge-base. extension
OBDDs provided Algebraic Decision Diagrams (ADD) (Bahar, Frohm, Gaona, Hachtel,
Macii, Pardo, & Somenzi, 1993), terminal nodes 0 1, take values
arbitrary finite domain. knowledge compilation approach become important research
direction automated reasoning past decade (Selman & Kautz, 1996; Darwiche & Marquis,
2002; Cadoli & Donini, 1997). Typically, knowledge representation language compiled
compact data structure allows fast responses various queries. Accordingly, computational
effort divided offline online phase work pushed
offline. Compilation also used generate compact building blocks used online
algorithms multiple times. Macro-operators compiled prior search viewed
light (Korf & Felner, 2002), graphical models building blocks functions
whose compact compiled representations used effectively across many tasks.
one example, consider product configuration tasks imagine user chooses sequential options configure product. naive system, user would allowed choose valid
option current level based initial constraints, either product configured,
else, dead-end encountered, system would backtrack previous state
continue there. would fact search space possible partial configurations. Needless say, would unpractical, would offer user guarantee
finishing limited time. system based compilation would actually build backtrack-free
search space offline phase, represent compact manner. online phase,
valid partial configurations (i.e., extended full valid configuration) allowed,
depending query type, response time guarantees offered terms size
compiled structure.
Numerous examples, diagnosis planning problems, formulated
graphical models could benefit compilation (Palacios, Bonet, Darwiche, & Geffner, 2005;
Huang & Darwiche, 2005a). diagnosis, compilation facilitate fast detection possible faults
explanations unusual behavior. Planning problems also formulated graphical
models, compilation would allow swift adjustments according changes environment.
Probabilistic models one used types graphical models, basic query
compute conditional probabilities variables given evidence. compact compilation
probabilistic model would allow fast response queries incorporate evidence acquired time.
example, two important tasks Bayesian networks computing probability
evidence, computing maximum probable explanation (MPE). model
variables become assigned (evidence), tasks performed time linear compilation size, practice many cases smaller upper-bound based treewidth
pathwidth graph. Formal verification another example compilation heavily used
compare equivalence circuit design, check behavior circuit. Binary Decision
Diagram (BDD) (Bryant, 1986) arguably widely known used compiled structure.
contributions made paper knowledge compilation general decision diagrams particular following:
1. formally describe AND/OR Multi-Valued Decision Diagram (AOMDD) prove
canonical representation constraint networks, given pseudo tree.
467

fiM ATEESCU , ECHTER & ARINESCU

2. extend AOMDD general weighted graphical models.
3. give compilation algorithm based AND/OR search, saves trace memory
intensive search reduces one bottom pass.
4. present APPLY operator combines two AOMDDs show complexity
quadratic input, never worse exponential treewidth.
5. give scheduling order building AOMDD graphical model starting
AOMDDs functions based Variable Elimination algorithm.
guarantees complexity exponential induced width (treewidth) along
ordering.
6. show AOMDDs relate various earlier recent compilation frameworks, providing unifying perspective methods.
7. introduce semantic treewidth, helps explain compiled decision diagrams
often much smaller worst case bound.
8. provide experimental evaluation new data structure.
structure paper follows. Section 2 provides preliminary definitions, description
binary decision diagrams Bucket Elimination algorithm. Section 3 gives overview
AND/OR search spaces. Section 4 introduces AOMDD discusses properties. Section
5 describes search-based algorithm compiling AOMDD. Section 6 presents compilation
algorithm based Bucket Elimination schedule APPLY operation. Section 7 proves
AOMDD canonical representation constraint networks given pseudo tree, Section
8 extends AOMDD weighted graphical models proves canonicity. Section 9 ties
canonicity new concept semantic treewidth. Section 10 provides experimental
evaluation. Section 11 presents related work Section 12 concludes paper. proofs
appear appendix.

2. Preliminaries
Notations reasoning problem defined terms set variables taking values finite
domains set functions defined variables. denote variables subsets
variables uppercase letters (e.g., X, Y, . . .) values variables lower case letters (e.g.,
x, y, . . .). Sets usually denoted bold letters, example X = {X1 , . . . , Xn } set
variables. assignment (X1 = x1 , . . . , Xn = xn ) abbreviated x = (hX1 , x1 i, . . . ,
hXn , xn i) x = (x1 , . . . , xn ). subset variables Y, DY denotes Cartesian product
domains variables Y. projection assignment x = (x1 , . . . , xn ) subset
denoted xY x[Y]. also denote = (or short) assignment values
variables respective domains. denote functions letters f , g, h etc.,
scope (set arguments) function f scope(f ).
2.1 Graphical Models
EFINITION 1 (graphical model) graphical model 4-tuple, = hX, D, F, i, where:
468

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

1. X = {X1 , . . . , Xn } finite set variables;
2. = {D1 , . . . , Dn } set respective finite domains values;
3. F = {f1 , . . . , fr } set positive real-valued discrete functions (i.e., domains
listed), defined subset variables Si X, called scope, denoted
scope(fi ).
Q P
4. combination operator1 (e.g., { , , 1} product, sum, join), take
input two (or more) real-valued discrete functions, produce another real-valued discrete
function.
graphical model represents combination functions: ri=1 fi .
Several examples graphical models appear later, example: Figure 1 shows constraint
network Figure 2 shows belief network.
order define equivalence graphical models, useful introduce notion
universal graphical model defined single function.
EFINITION 2 (universal equivalent graphical model) Given graphical model
=
hX, D, F1 , universal equivalent model u(M) = hX, D, F2 = {fi F1 fi }, i.
Two graphical models equivalent represent function. Namely,
universal model.
EFINITION 3 (weight full partial assignment) Given graphical model =
hX, D, Fi, weight full assignment x = (x1 , . . . , xn ) defined w(x) =
f F f (x[scope(f )]). Given subset variables X, weight partial assignment
combination functions whose scopes included (denoted FY ) evaluated
assigned values. Namely, w(y) = f FY f (y[scope(f )]).
Consistency graphical models, range functions special zero value 0
absorbing relative combination operator (e.g., multiplication). Combining anything
0 yields 0. 0 value expresses notion inconsistent assignments. primary
concept constraint networks also defined relative graphical models
0 element.
EFINITION 4 (consistent partial assignment, solution) Given graphical model 0
element, partial assignment consistent cost non-zero. solution consistent assignment variables.
EFINITION 5 (primal graph) primal graph graphical model undirected graph
variables vertices edge connects two variables appear scope
function.
primal graph captures structure knowledge expressed graphical model.
particular, graph separation indicates independency sets variables given assignments
variables. advanced algorithms graphical models exploit graphical structure,
using heuristically good elimination order, tree decomposition similar method.
use concept pseudo tree, resembles tree rearrangements introduced Freuder
Quinn (1985):
1. combination operator also defined axiomatically (Shenoy, 1992).

469

fiM ATEESCU , ECHTER & ARINESCU

E







E

B


F

B

G

F

C

G
C

(a) Graph coloring problem

(b) Constraint graph

Figure 1: Constraint network
EFINITION 6 (pseudo tree) pseudo tree graph G = (X, E) rooted tree
set nodes X, every arc E backarc (A path rooted tree starts
root ends one leaf. Two nodes connected backarc exists path
contains both).
use common concepts parameters graph theory, characterize connectivity graph, close tree chain. induced width graphical model
governs complexity solving Bucket Elimination (Dechter, 1999), also shown
bound AND/OR search graph memory used cache solved subproblems (Dechter &
Mateescu, 2007).
EFINITION 7 (induced graph, induced width, treewidth, pathwidth) ordered graph
pair (G, d), G = ({X1 , . . . , Xn }, E) undirected graph, = (X1 , . . . , Xn )
ordering nodes. width node ordered graph number neighbors
precede ordering. width ordering d, denoted w(d), maximum width
nodes. induced width ordered graph, w (d), width induced ordered graph
obtained follows: node, last first d, preceding neighbors connected
clique. induced width graph, w , minimal induced width orderings.
induced width also equal treewidth graph. pathwidth pw graph
treewidth restricted class orderings correspond chain decompositions.
Various reasoning tasks, queries defined graphical models. defined formally using marginalization operators projection, summation minimization.
However, since goal present compilation graphical model independent
queries posed it, discuss tasks informal manner only.
information see work Kask, Dechter, Larrosa, Dechter (2005).
Throughout paper, use two examples graphical models: constraint networks
belief networks. case constraint networks, functions understood relations. words, functions (also called constraints) take two values, {0, 1},
{f alse, true}. 0 value indicates corresponding assignment variables inconsistent (not allowed), 1 value indicates consistency. Belief networks example
general case graphical models (also called weighted graphical models). functions case
conditional probability tables, values function real numbers interval [0, 1].

470

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

Example 1 Figure 1(a) shows graph coloring problem modeled constraint network. Given map regions, problem color region one given colors {red,
green, blue}, neighboring regions different colors. variables problems
regions, one domain {red, green, blue}. constraints relation
different neighboring regions. Figure 1(b) shows constraint graph, solution
(A=red, B=blue, C=green, D=green, E=blue, F=blue, G=red) given Figure 1(a).
detailed example given later Example 8.
Propositional Satisfiability special case CSP propositional satisfiability (SAT). formula conjunctive normal form (CNF) conjunction clauses 1 , . . . , , clause
disjunction literals (propositions negations). example, = (P Q R)
clause, P , Q R propositions, P , Q R literals. SAT problem
decide whether given CNF theory model, i.e., truth-assignment propositions
violate clause. Propositional satisfiability (SAT) defined CSP,
propositions correspond variables, domains {0, 1}, constraints represented clauses,
example clause (A B) relation propositional variables allows tuples
(A, B) except (A = 1, B = 0).
Cost Networks immediate extension constraint networks cost networks set
functions real-valued cost functions, primary task optimization. Also, GAI-nets
(generalized additive independence, Fishburn, 1970) used represent utility functions.
example cost functions appear Figure 19.
EFINITION
P8 (cost network, combinatorial optimization) cost network 4-tuple,
hX, D, C, i, X set variables X = {X1 , . . . , Xn }, associated set
discrete-valued domains, = {D1 , . . . , Dn }, set cost functions C = {C1 , . . . , Cr }.
Ci real-valued function defined subset variables Si X. combination operator,
P
. reasoning problem find minimum cost solution.

Belief Networks (Pearl, 1988) provide formalism reasoning partial beliefs conditions uncertainty. defined directed acyclic graph vertices representing random
variables interest (e.g., temperature device, gender patient, feature object, occurrence event). arcs signify existence direct causal influences
linked variables quantified conditional probabilities attached cluster parentschild vertices network.
Q
EFINITION 9 (belief networks) belief network (BN) graphical model P = hX, D, PG , i,
X = {X1 , . . . , Xn } set variables domains = {D1 , . . . , Dn }. Given directed acyclic graph G X nodes, PG = {P1 , . . . , Pn }, Pi = {P (Xi | pa (Xi ) ) }
conditional probability tables (CPTs short) associated Xi , pa(Xi )
parents Xi
Qacyclic graph G. belief network represents probability distribution X,
P (x1 , . . . , xn ) = ni=1 P (xi |xpa(Xi ) ). evidence set e instantiated subset variables.
formulated graphical model, functions F denote conditional probability tables
scopes functions determined directed acyclic graph G: function
Q
fi ranges variable Xi parents G. combination operator product, = .
primal graph belief network (viewed undirected model) called moral graph.
connects two variables appearing CPT.
471

fiM ATEESCU , ECHTER & ARINESCU

Season

Sprinkler B

Watering



C Rain

B

F Wetness



G Slippery

C

F

G

(a) Directed acyclic graph

(b) Moral graph

Figure 2: Belief network
Example 2 Figure 2(a) gives example belief network 6 variables, Figure 2(b)
shows moral graph . example expresses causal relationship variables Season
(A), configuration automatic sprinkler system (B), amount rain expected
(C), amount manual watering necessary (D), wetness pavement (F )
Whether pavement slippery (G). belief network expresses probability distribution P (A, B, C, D, F, G) = P (A) P (B|A) P (C|A) P (D|B, A) P (F |C, B) P (G|F ).
Another example belief network CPTs appears Figure 9.
two popular tasks belief networks defined below:
EFINITION 10 (belief updating, probable explanation (MPE)) Given belief network
evidence e, belief updating task compute posterior marginal probability variable
Xi , conditioned evidence. Namely,
X

Bel(Xi = xi ) = P (Xi = xi | e) =

n


P (xk , e|xpak ),

{(x1 ,...,xi1 ,xi+1 ,...,xn )|E=e,Xi =xi } k=1

normalization constant. probable explanation (MPE) task find
complete assignment agrees evidence, highest probability among
assignments. Namely, find assignment (xo1 , . . . , xon )
P (xo1 , . . . , xon ) = maxx1 ,...,xn

n


P (xk , e|xpak ).

k=1

2.2 Binary Decision Diagrams Review
Decision diagrams widely used many areas research represent decision processes.
particular, used represent functions. Due fundamental importance Boolean
functions, lot effort dedicated study Binary Decision Diagrams (BDDs),
extensively used software hardware verification (Clarke et al., 1999; McMillan,
1993). earliest work BDDs due Lee (1959), introduced binary-decision program, understood linear representation BDD (e.g., depth first search ordering
nodes), node branching instruction indicating address next instruction 0 1 value test variable. Akers (1978) presented actual graphical
472

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS


0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

C
0
1
0
1
0
1
0
1

f(ABC)
0
0
0
1
0
1
0
1





B

C

C

0

C

0

(a) Table

0

B

B

1

0

B

0

1

B

C

1

0

(b) Unordered tree

C

0

0

C

1

0

C

1

0

1

(c) Ordered tree

Figure 3: Boolean function representations
representation developed BDD idea. However, Bryant (1986) introduced
called Ordered Binary Decision Diagram (OBDD). restricted order variables along path diagram, presented algorithms (most importantly apply procedure, combines two OBDDs operation) time complexity quadratic
sizes input diagrams. OBDDs fundamental applications large binary functions,
especially many practical cases provide compact representations.
BDD representation Boolean function. Given B = {0, 1}, Boolean function
f : Bn B, n arguments, X1 , , Xn , Boolean variables, takes Boolean
values.
Example 3 Figure 3(a) shows table representation Boolean function three variables.
explicit representation straightforward, also costly due exponential
requirements. function also represented binary tree, shown Figure 3(b),
exponential size number variables. internal round nodes represent
variables, solid edges 1 (or high) value, dotted edges 0 (or low) value.
leaf square nodes show value function assignment along path. tree
shown 3(b) unordered, variables appear order along path.
building OBDD, first condition variables appear order (A,B,C)
along every path root leaves. Figure 3(c) shows ordered binary tree function.
order imposed, two reduction rules transform decision diagram
equivalent one:
(1) isomorphism: merge nodes label children.
(2) redundancy: eliminate nodes whose low high edges point node, connect
parent removed node directly child removed node.
Applying two reduction rules exhaustively yields reduced OBDD, sometimes denoted
rOBDD. use OBDD assume completely reduced.
Example 4 Figure 4(a) shows binary tree Figure 3(c) isomorphic terminal nodes
(leaves) merged. highlighted nodes, labeled C, also isomorphic, Figure
4(b) shows result merged. Now, highlighted nodes labeled C B
redundant, removing gives OBDD Figure 4(c).
2.3 Bucket Elimination Review
Bucket Elimination (BE) (Dechter, 1999) well known variable elimination algorithm inference graphical models. describe using terminology constraint networks,
473

fiM ATEESCU , ECHTER & ARINESCU





B

B

C

C

C

0

1



B

C

B

C

C

0

(a) Isomorphic nodes

B

C

1

0

(b) Redundant nodes

1

(c) OBDD

Figure 4: Reduction rules
A:


C1(AC)
C2(AB)
C3(ABE)

B

C

C4(BCD)

h4(A)

B:

C2(AB)

E:

C3(ABE)



h3(AB)

h2(AB)

AB bucket-B
AB

ABE

C:
E



(a) Constraint network

D:

C1(AC)

h1(BC)

bucket-A



bucket-E

AB

ABC bucket-C
BC

BCD bucket-D

C4 (BCD)

(b) execution

(c) Bucket tree

Figure 5: Bucket Elimination
also applied graphical model. Consider constraint network R = hX, D, Ci
ordering = (X1 , X2 , . . . , Xn ). ordering dictates elimination order BE, last
first. variable associated bucket. constraint C placed bucket
latest variable d. Buckets processed Xn X1 eliminating bucket variable (the
constraints residing bucket joined together, bucket variable projected out)
placing resulting constraint (also called message) bucket latest variable d.
execution, renders network backtrack free, solution produced assigning
variables along d. also produce solutions count marginalization done summation
(rather projection) functional representation constraints, join substituted
multiplication.
also constructs bucket tree, linking bucket Xi destination bucket
message (called parent bucket). node bucket tree typically bucket variable,
collection constraints, scope (the union scopes constraints). nodes
bucket tree replaced respective bucket variables, easy see obtain pseudo
tree.
Example 5 Figure 5(a) shows network four constraints. Figure5(b) shows execution
Bucket Elimination along = (A, B, E, C, D). buckets processed A.2 Figure
5(c) shows bucket tree. pseudo tree corresponding order given Fig. 6(a).
2. representation Figure 5 reverses top bucket processing described earlier papers (Dechter, 1999).

474

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

Procedure GeneratePseudoTree(G, d)
1
2
3
4
5

input : graph G = (X, E); order = (X1 , . . . , Xn )
output : Pseudo tree
Make X1 root
Condition X1 (eliminate X1 incident edges G). Let G1 , . . . , Gp resulting connected
components G
= 1 p
Ti = GeneratePseudoTree (Gi , d|Gi )
Make root Ti child X1

6 return

2.4 Orderings Pseudo Trees
Given ordering d, structural information captured primal graph scopes
functions F = {f1 , . . . , fr } used create unique pseudo tree corresponds
(Mateescu & Dechter, 2005). precisely bucket tree (or elimination tree),
created (when variables processed reverse d). pseudo tree created
conditioning primal graph, processing variables order d, described Procedure
GeneratePseudoTree. following, d|Gi restriction order nodes
graph Gi .

3. Overview AND/OR Search Space Graphical Models
AND/OR search space recently introduced (Dechter & Mateescu, 2004a, 2004b, 2007)
unifying framework advanced algorithmic schemes graphical models. main virtue consists exploiting independencies variables search, provide exponential
speedups traditional search methods oblivious problem structure. Since AND/OR MDDs
based AND/OR search spaces need provide comprehensive overview sake
completeness.
3.1 AND/OR Search Trees
AND/OR search tree guided pseudo tree primal graph. idea exploit
problem decomposition independent subproblems search. Assigning value
variable (also known conditioning), equivalent graph terms removing variable (and
incident edges) primal graph. partial assignment therefore lead decomposition
residual primal graph independent components, searched (or solved)
separately. pseudo tree captures precisely decompositions given order variable
instantiation.
EFINITION 11 (AND/OR search tree graphical model) Given graphical model =
hX, D, Fi, primal graph G pseudo tree G, associated AND/OR search tree
alternating levels nodes. nodes labeled Xi correspond
variables. nodes labeled hXi , xi (or simply xi ) correspond value assignments.
structure AND/OR search tree based . root node labeled
root . children node Xi nodes labeled assignments hXi , xi

475

fiM ATEESCU , ECHTER & ARINESCU





B

1

B

B

0
E

E

0

C



1
C

0 1

E

0

1


0 1

(a) Pseudo tree

0 1

0
C

E

0

1





0 1

0 1

0 1

1
C

E

0

1





0 1

0 1

0 1

C
0

1







0 1

0 1

0 1

(b) Search tree

Figure 6: AND/OR search tree
consistent assignments along path root. children node
hXi , xi nodes labeled children variable Xi pseudo tree .
Example 6 Figure 6 shows example AND/OR search tree graphical model given
Figure 5(a), assuming tuples consistent, variables binary valued. tuples
inconsistent, paths tree exist. Figure 6(a) gives pseudo tree
guides search, top bottom, indicated arrows. dotted arcs backarcs
primal graph. Figure 6(b) shows AND/OR search tree, alternating levels
(circle) (square) nodes, structure indicated pseudo tree.
AND/OR search tree traversed depth first search algorithm, thus using linear
space. already shown (Freuder & Quinn, 1985; Bayardo & Miranker, 1996; Darwiche, 2001;
Dechter & Mateescu, 2004a, 2007) that:
HEOREM 1 Given graphical model n variables, pseudo tree depth m,
size AND/OR search tree based O(n k ), k bounds domains variables.
graphical model treewidth w pseudo tree depth w log n, therefore

AND/OR search tree size O(n k w log n ).
AND/OR search tree expresses set possible assignments problem variables
(all solutions). difference traditional search space solution longer
path root leaf, rather tree, defined follows:
EFINITION 12 (solution tree) solution tree AND/OR search tree contains root node.
every node, contains one child nodes nodes contains
child nodes, leaf nodes consistent.
3.2 AND/OR Search Graph
AND/OR search tree may contain nodes root identical subproblems. nodes said
unifiable. unifiable nodes merged, search space becomes graph. size
becomes smaller expense using additional memory search algorithm. depth first
search algorithm therefore modified cache previously computed results, retrieve
nodes encountered again. notion unifiable nodes defined formally next.

476

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

EFINITION 13 (minimal AND/OR graph, isomorphism) Two AND/OR search graphs G G0
isomorphic exists one one mapping vertices G vertices G0
vertex v, (v) = v 0 , v v 0 root identical subgraphs relative .
AND/OR graph called minimal isomorphic subgraphs merged. Isomorphic nodes
(that root isomorphic subgraphs) also said unifiable.
shown Dechter Mateescu (2007) that:
HEOREM 2 graphical model unique minimal AND/OR search graph relative
pseudo-tree .
minimal AND/OR graph graphical model G relative pseudo tree denoted
MT (G). Note definition minimality used work Dechter Mateescu (2007)
based isomorphism reduction. extend also including elimination
redundant nodes. previous theorem shows given AND/OR graph, merge
operator fixed point, minimal AND/OR graph. show paper
AOMDD canonical representation, namely two equivalent graphical models
represented unique AOMDD given accept pseudo tree,
AOMDD minimal terms number nodes.
unifiable nodes identified based contexts. define graph based
contexts nodes nodes, expressing set ancestor variables
completely determine conditioned subproblem. However, shown using caching
based contexts makes caching based contexts redundant vice versa,
use caching. value assignment context X separates subproblem X
rest network.
EFINITION 14 (OR context) Given pseudo tree AND/OR search space,
context(X) = [X1 . . . Xp ] set ancestors X , ordered descendingly, connected primal graph X descendants X.
EFINITION 15 (context unifiable nodes) Given AND/OR search graph, two nodes n1
n2 context unifiable variable label X assignments
contexts identical. Namely, 1 partial assignment variables along path n1 ,
2 partial assignment variables along path n2 , restriction context
X same: 1 |context(X) = 2 |context(X) .
depth first search algorithm traverses AND/OR search tree, modified
traverse graph, enough memory available. could allocate cache table variable X,
scope table context(X). size cache table X therefore product
domains variables context. variable X, possible assignment
context, corresponding conditioned subproblem solved computed
value saved cache table, whenever context assignment encountered again,
value subproblem retrieved cache table. algorithm traverses
called context minimal AND/OR graph.
EFINITION 16 (context minimal AND/OR graph) context minimal AND/OR graph obtained AND/OR search tree merging context unifiable nodes.
477

fiM ATEESCU , ECHTER & ARINESCU

R

F

G

B

[]

C


J

K

[C]

H

[C]

L

[CK]



[CH]

N

[CKL]

B

[CHA]



[CKLN]

P

[CKO]

H
E
C



L

E

[CHAB]

R

[HAB]

J

[CHAE]

F

[AR]



[CEJ]

G

[AF]



[CD]

K

N
P


(a) Primal graph

(b) Pseudo tree
C

0

0

K

H

K

0

1

0

1

L

L

L

L

H

0

1



0



1





0

1

0

1

0

1

0

1

0

1

0

1

0

1

0

1

N

N

N

N

N

N

N

N

B

B

B

B

B

B

B

B

0

1

0

1

0

1

0

1

0

1

0

1

0

1

0

1

































0

E

0

1

E

E

E

0

1

E

E

E

0

1

E

E

E

E

0

1

E

E

E

E

1

E

0

0

1

R

R

R

0

1

R

R

R

R

1

R

0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
0 1 0 1 0 1 0 1 0 1
P

P

P

P

P

P

P

0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1

0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1

P
J

J

J

J

J

J

J

J

J

J

J

J

J

J

J

J

0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1









0 1 0 1 0 1 0 1







F

F

F

F

0 1 0 1 0 1 0 1



0 1 0 1 0 1 0 1

G

G

G

G

0 1 0 1 0 1 0 1








0 1 0 1 0 1 0 1

(c) Context minimal graph

Figure 7: AND/OR search graph
already shown (Bayardo & Miranker, 1996; Dechter & Mateescu, 2004a, 2007) that:
HEOREM 3 Given graphical model M, primal graph G pseudo tree , size
context minimal AND/OR search graph based , therefore size minimal AND/OR

search graph, O(n k wT (G) ), wT (G) induced width G depth first traversal
, k bounds domain size.

Example 7 Lets look impact caching size search space examining larger
example. Figure 7(a) shows graphical model binary variables Figure 7(b) pseudo tree
drives AND/OR search. context node given square brackets. context
minimal graph given Figure 7(c). Note far smaller AND/OR search tree,
28 = 256 nodes level alone (because depth 8 pseudo tree).
shaded rectangles show size cache table, equal number nodes
appear one. cache entry useful whenever one incoming edges
node. Incidentally, caches useful (namely nodes one incoming
arc), called dead caches (Darwiche, 2001), determined based pseudo
478

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

tree inspection, therefore cache table need allocated them. context minimal graph
also explain execution along pseudo tree (or, equivalently, along depth
first traversal order). buckets shaded rectangles, processing done bottom up.
number possible assignments bucket equals number nodes appear
it. message scope identical context bucket variable, message
identical corresponding cache table. details relationship AND/OR
search see work Mateescu Dechter (2005).
3.3 Weighted AND/OR Graphs
previous subsections described structure AND/OR trees graphs. order
use solve reasoning task, need define way using input function values
traversal AND/OR graph. realized placing weights (or costs)
OR-to-AND arcs, dictated function values. functions relevant contribute
OR-to-AND arc weight, captured buckets relative pseudo tree:
EFINITION 17 (buckets relative pseudo tree) Given graphical model = hX, D, F,
pseudo tree , bucket Xi relative , denoted BT (Xi ), set functions whose
scopes contain Xi included pathT (Xi ), set variables root Xi
. Namely,
BT (Xi ) = {f F|Xi scope(f ), scope(f ) pathT (Xi )}.

function belongs bucket variable Xi iff scope fully instantiated
Xi assigned. Combining values functions bucket, current assignment, gives weight OR-to-AND arc:
EFINITION 18 (OR-to-AND weights) Given AND/OR graph graphical model M,
weight w(n,m) (Xi , xi ) arc (n, m) Xi labels n xi labels m, combination
functions BT (Xi ) assigned values along current path node m, .
Formally, w(n,m) (Xi , xi ) = f BT (Xi ) f (asgn(m )[scope(f )]).
EFINITION 19 (weight solution tree) Given weighted AND/OR graph graphical model
M, given solution tree OR-to-AND set arcs arcs(t), weight defined
w(t) = earcs(t) w(e).
Example 8 start straightforward case constraint networks. Since functions
take values 0 1, combination product (join relations), follows ORto-AND arc weight 0 1. example given Figure 8. Figure 8(a) shows
constraint graph, 8(b) pseudo tree it, 8(c) four relations define constraint
problem. Figure 8(d) shows AND/OR tree traversed depth first search algorithm
checks consistency input functions (i.e., constraint propagation used).
Similar OBDD representation, OR-to-AND arcs weight 0 denoted dotted
lines, tree unfolded them, since contain solution. arcs
weight 1 drawn solid lines.
479

fiM ATEESCU , ECHTER & ARINESCU


B
C





F

B

E

(a) Constraint graph

0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

B
0
0
0
0
1
1
1
1

C RABC
0
1
1
1
0
0
1
1
0
1
1
1
0
1
1
0

C
0
0
1
1
0
0
1
1

C

E



F

(b) Pseudo tree

RBCD
0
1
1
1
0
1
1
0
0
1
1
0
0
1
1
1


0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

E RABE
0
1
1
0
0
1
1
1
0
0
1
1
0
1
1
0


0
0
0
0
1
1
1
1

E
0
0
1
1
0
0
1
1

F RAEF
0
0
1
1
0
1
1
1
0
1
1
1
0
1
1
0

(c) Relations


1

1

0

1

B

B

1

1

1
1

0
C

C

E

1
1

0
C

E

C

E

E

1

1

1

0

0

1

1

1

1

1

0

1

1

0

1

0

0

1

0

1

0

1

0

1

0

1

0

1

0

1

0

1







F

1

1 1

0 0

1

0

1

1

1

0

0

1
0

F



F

1 0

1 1

1

1

1

1

1

0

0

0





F

F

1 1

0

1

0

1

0

1

1

0

1

0

1

0

1
0

1
1

(d) AND/OR tree

Figure 8: AND/OR search tree constraint networks
Example 9 Figure 9 shows weighted AND/OR tree belief network. Figure 9(a) shows
directed acyclic graph, dotted arc BC added moralization. Figure 9(b) shows pseudo
tree, 9(c) shows conditional probability tables. Figure 9(d) shows weighted AND/OR
tree.
constraint networks, move weighted AND/OR search trees
weighted AND/OR search graphs merging unifiable nodes. case arc labels
also considered determining unifiable subgraphs. yield context-minimal weighted
AND/OR search graphs minimal weighted AND/OR search graphs.

4. AND/OR Multi-Valued Decision Diagrams (AOMDDs)
section begin describing contributions paper. context minimal AND/OR
graph (Definition 16) offers effective way identifying unifiable nodes execution search algorithm. Namely, context unifiable nodes discovered based
paths root, without actually solving corresponding subproblems. However, merging based context complete, means may still exist unifiable nodes
search graph identical contexts. Moreover, nodes context
480

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

P(A)




0
1



P(B | A)

P(A)
.6
.4


0
1

P(C | A)

0
1

B=1
.6
.9

B=0
.4
.1

P(D | B,C)

B
B

B
0
0
1
1

C

E
E

C




(a) Belief network

C
0
1
0
1

P(E | A,B)
D=1
.8
.9
.7
.5

D=0
.2
.1
.3
.5

C=1
.8
.3

C=0
.2
.7

(b) Pseudo tree


0
0
1
1

B
0
1
0
1

E=0
.4
.5
.7
.2

E=1
.6
.5
.3
.8

(c) CPTs



.6

.4

0

1

B

B

.4
0

0

C

.6
1

E

.2

.8

.5

0

1

0



.2
0

1



.8
1

.1
0

.2

.8

.7

0

1

0



.9
1

.3
0

C

0

E

.3

.7

.3

1

0

1





.7 .5
1

1

E

C

.5

.9

0

1

E

.4

.1

.6

.2

.5

0

1

.2
0

C

.8

.7

.3

1

0

1



.8
1



.1
0

.9
1

.3
0



.7 .5
1

0

.5
1

(d) Weighted AND/OR tree

Figure 9: Weighted AND/OR search tree belief networks
minimal AND/OR graph may redundant, example set solutions rooted variable Xi dependant specific value assigned Xi (this situation detectable based
context). sometimes termed interchangeable values symmetrical values.
overviewed earlier, Dechter Mateescu (2007, 2004a) defined complete minimal AND/OR
graph AND/OR graph whose unifiable nodes merged, Dechter Mateescu
(2007) also proved canonicity non-weighted graphical models.
paper propose augment minimal AND/OR search graph removing redundant variables common OBDD representation well adopt notational conventions
common community. yields data structure call AND/OR BDD, exploits
decomposition using nodes. present extension multi-valued variables yielding
AND/OR MDD AOMDD define general weighted graphical models. Subsequently
present two algorithms compiling canonical AOMDD graphical model: first
search-based, uses memory intensive AND/OR graph search generate context minimal
AND/OR graph, reduces bottom applying reduction rules; second inferencebased, uses Bucket Elimination schedule combine AOMDDs initial functions
APPLY operations (similar apply OBDDs). show, approaches
worst case complexity AND/OR graph search context based caching, also
complexity Bucket Elimination, namely time space exponential treewidth

problem, O(n k w ). benefit generation schemes discussed.

481

fiM ATEESCU , ECHTER & ARINESCU





1

(a) OBDD

2



k

(b) MDD

Figure 10: Decision diagram nodes (OR)




1







(a) AOBDD

2

k







(b) AOMDD

Figure 11: Decision diagram nodes (AND/OR)
4.1 AND/OR Search Graphs Decision Diagrams
AND/OR search graph G graphical model = hX, D, F, represents set
possible assignments problem variables (all solutions costs). sense, G
viewed representing function f = fi F fi defines universal equivalent graphical
model u(M) (Definition 2). full assignment x = (x1 , . . . , xn ), x solution expressed
tree tx , f (x) = w(tx ) = earcs(tx ) w(e) (Definition 19); otherwise f (x) = 0 (the
assignment inconsistent). solution tree tx consistent assignment x read G
linear time following assignments root. x inconsistent, dead-end
encountered G attempting read solution tree tx , f (x) = 0. Therefore, G
viewed decision diagram determines values f every complete assignment x.
see process AND/OR search graph reduction rules similar
case OBDDs, order obtain representation minimal size. case OBDDs,
node labeled variable name, example A, low (dotted line) high (solid
line) outgoing arcs capture restriction function assignments = 0 = 1.
determine value function, one needs follow either one (but both)
outgoing arcs (see Figure 10(a)). straightforward extension OBDDs multi-valued
variables (multi-valued decision diagrams, MDDs) presented Srinivasan, Kam, Malik,
Brayton (1990), node structure use given Figure 10(b). outgoing arc
associated one k values variable A.
paper generalize OBDD MDD representations demonstrated Figures 10(a)
10(b) allowing outgoing arc arc. arc connects node set
nodes, captures decomposition problem independent components. number
arcs emanating node two case AOBDDs (Figure 11(a)), domain size
variable general case (Figure 11(b)). given node A, k arcs
connect possibly different number nodes, depending problem decomposes based
particular assignment A. arcs depicted shaded sector connects
outgoing lines corresponding independent components.

482

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS


1



2



k





(a) Nonterminal meta-node

0

1

(b) Terminal meta-node 0

(c) Terminal meta-node 1

Figure 12: Meta-nodes
define AND/OR Decision Diagram representation based AND/OR search graphs.
find useful maintain semantics Figure 11 especially need express
redundancy nodes, therefore introduce meta-node data structure, defines small
portions AND/OR graph, based node children:
EFINITION 20 (meta-node) meta-node u AND/OR search graph either: (1)
terminal node labeled 0 1, (2) nonterminal node, consists node labeled
X (therefore var(u) = X) k children labeled x1 , . . . , xk correspond value
assignments X. node labeled xi stores list pointers child meta-nodes, denoted
u.childreni . case weighted graphical models, node xi also stores OR-toAND arc weight w(X, xi ).
rectangle Figure 12(a) meta-node variable A, domain size k. Note
similar Figure 11, small difference information value
corresponds outgoing arc stored nodes meta-node.
showing weights figure. larger example AND/OR graph meta-nodes
appears later Figure 16.
terminal meta-nodes play role terminal nodes OBDDs. terminal metanode 0, shown Figure 12(b), indicates inconsistent assignments, terminal meta-node 1,
shown figure 12(c) indicates consistent ones.
AND/OR search graph viewed diagram meta-nodes, simply grouping
nodes children, adding terminal meta-nodes appropriately.
defined meta-nodes, easier see variable redundant respect outcome function based current partial assignment. variable redundant
assignments leads set solutions.
EFINITION 21 (redundant meta-node) Given weighted AND/OR search graph G represented
meta-nodes, meta-node u var(u) = X |D(X)| = k redundant iff:
(a) u.children1 = . . . = u.childrenk
(b) w(X, x1 ) = . . . = w(X, xk ).
AND/OR graph G, contains redundant meta-node u, transformed equivalent graph G 0 replacing incoming arc u common list children u.children1 ,
absorbing common weight w(X, x1 ) combination weight parent meta-node
corresponding incoming arc, removing u outgoing arcs G.
value X = x1 picked arbitrarily, isomorphic. u root
483

fiM ATEESCU , ECHTER & ARINESCU

Procedure RedundancyReduction
: AND/OR graph G; redundant meta-node u, var(u) = X; List meta-node parents u,
denoted P arents(u).
output : Reduced AND/OR graph G elimination u.
1 P arents(u) empty
2
return independent AND/OR graphs rooted meta-nodes u.children1 , constant w(X, x1 )
input

3 forall v P arents(u) (assume var(v) == )
4
forall {1, . . . , |D(Y )|}
5
u v.childreni
6
v.childreni v.childreni \ {u}
7
v.childreni v.childreni u.children1
8
w(Y, yi ) w(Y, yi ) w(X, x1 )
9 remove u
10 return reduced AND/OR graph G

Procedure IsomorphismReduction
: AND/OR graph G; isomorphic meta-nodes u v; List meta-node parents u, denoted
P arents(u).
output : Reduced AND/OR graph G merging u v.
forall p P arents(u)
u p.childreni
p.childreni p.childreni \ {u}
p.childreni p.childreni {v}

input

1
2
3
4

5 remove u
6 return reduced AND/OR graph G

graph, common weight w(X, x1 ) stored separately constant. Procedure
RedundancyReduction formalizes redundancy elimination.
EFINITION 22 (isomorphic meta-nodes) Given weighted AND/OR search graph G represented
meta-nodes, two meta-nodes u v var(u) = var(v) = X |D(X)| = k
isomorphic iff:
(a) u.childreni = v.childreni {1, . . . , k}
(b) wu (X, xi ) = wv (X, xi ) {1, . . . , k}, (where wu , wv weights u v).
Procedure IsomorphismReduction formalizes process merging isomorphic metanodes. Naturally, AND/OR graph obtained merging isomorphic meta-nodes equivalent
original one. define AND/OR Multi-Valued Decision Diagram:
EFINITION 23 (AOMDD) AND/OR Multi-Valued Decision Diagram (AOMDD) weighted
AND/OR search graph completely reduced isomorphic merging redundancy removal,
namely:
(1) contains isomorphic meta-nodes;
(2) contains redundant meta-nodes.

484

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS





2k

1

B

c

2



2

k

z


1

1

z

k





c







(b) eliminating B
meta-node

(a) Fragment AOMDD

Figure 13: Redundancy reduction

1



2k

1

2

k

B
1

C

2



2

1

C


1

B

k

1

e

2



k

C


k

2


k

1





(a) Fragment AOMDD

2

k

e





(b) merging isomorphic C meta-nodes

Figure 14: Isomorphism reduction
Example 10 Figure 13 shows example applying redundancy reduction rule portion
AOMDD. left side, Figure 13(a), meta-node variable B redundant (we
dont show weights OR-to-AND arcs, avoid cluttering figure). values
{1, . . . , k} B lead set meta-nodes {c, d, . . . , y}, coupled
arc. Therefore, meta-node B eliminated. result shown Figure 13(b),
meta-nodes {c, d, . . . , y} z coupled arc outgoing = 1.
Figure 14 show example applying isomorphism reduction rule. case,
meta-nodes labeled C Figure 14(a) isomorphic (again, omit weights). result
merging shown Figure 14(b).
Examples AOMDDs appear Figures 16, 17 18. Note weight OR-toAND arc zero, descendant terminal meta-node 0. Namely, current path
dead-end, cannot extended solution, therefore linked directly 0.

5. Using AND/OR Search Generate AOMDDs
Section 4.1 described transform AND/OR graph AOMDD applying
reduction rules. Section 5.1 describe explicit algorithm takes input graphi485

fiM ATEESCU , ECHTER & ARINESCU

cal model, performs AND/OR search context-based caching obtain context minimal
AND/OR graph, Section 5.2 give procedure applies reduction rules bottom
obtain AOMDD.
5.1 Algorithm AND/OR-S EARCH -AOMDD
Algorithm 1, called AND/OR-S EARCH -AOMDD, compiles graphical model AOMDD.
memory intensive (with context-based caching) AND/OR search used create context minimal AND/OR graph (see Definition 16). input AND/OR-S EARCH -AOMDD graphical
model pseudo tree , also defines OR-context variable.
variable Xi associated cache table, whose scope context Xi .
ensures trace search context minimal AND/OR graph. list denoted LXi
(see line 35), used variable Xi save pointers meta-nodes labeled Xi .
lists used procedure performs bottom reduction, per layers AND/OR
graph (one layer contains nodes labeled one given variable). fringe search
maintained stack called OPEN. current node (either node) denoted
n, parent p, current path n . children current node denoted
successors(n). node n, Boolean attribute consistent(n) indicates current path
extended solution. information useful pruning search space.
algorithm based two mutually recursive steps: Forward (beginning line 5)
Backtrack (beginning line 29), call (or themselves) search terminates.
forward phase, AND/OR graph expanded top down. two types nodes,
OR, treated differently according semantics.
node expanded, cache table variable checked (line 8). entry
null, link created already existing node roots graph equivalent
current subproblem. Otherwise, node expanded generating descendants.
OR-to-AND weight (see Definition 18) computed line 13. value xi Xi checked
consistency (line 14). least expensive check verify OR-to-AND weight non-zero.
However, deterministic (inconsistent) assignments extracted form constraint
network. level constraint propagation performed step (e.g., look ahead, arc
consistency, path consistency, i-consistency etc.). computational overhead increase,
hope pruning search space aggressively. note constraint propagation
crucial algorithm, complexity guarantees maintained even simple
weight check performed. consistent nodes added list successors n (line
16), inconsistent ones linked terminal 0 meta-node (line 19).
node n labeled hXi , xi expanded (line 20) based structure pseudo
tree. Xi leaf , n linked terminal 1 meta-node (line 22). Otherwise,
node created child Xi (line 24).
forward step continues long current node dead-end still unevaluated
successors. backtrack phase triggered node empty set successors (line 29).
Note that, successor processed, removed set successors line 42.
backtrack reaches root (line 32), search complete, context minimal AND/OR graph
generated, Procedure B OTTOM U P R EDUCTION called.
backtrack step processes node (line 31), saves pointer cache,
also adds pointer corresponding meta-node list LXi . consistent attribute

486

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

Algorithm 1: AND/OR EARCH - AOMDD
input : = hX, D, Fi; pseudo tree rooted X1 ; parents pai (OR-context) every variable Xi .
output : AOMDD M.
1 forall Xi X
Initialize context-based cache table CacheXi (pai ) null entries
2
3 Create new node t, labeled Xi ; consistent(t) true; push top OPEN
4 OPEN 6=
5
n top(OPEN); remove n OPEN
// Forward
6
successors(n)
7
n node labeled Xi
// OR-expand
CacheXi (asgn(n )[pai ]) 6= null
8
Connect parent n CacheXi (asgn(n )[pai ])
9
// Use cached pointer
10
11
12
13
14
15
16
17
18
19

else
forall xi Di
Create new node t, labeled hXi , xi
w(X, xi )

f (asgn(n )[pai ])
f BT (Xi )

hXi , xi consistent n
consistent(t) true
add successors(n)
else
consistent(t) f alse
make terminal 0 child

20
21
22
23
24
25
26
27

n node labeled hXi , xi
childrenT (Xi ) ==
make terminal 1 child n
else
forall childrenT (Xi )
Create new node t, labeled
consistent(t) f alse
add successors(n)

28
29
30
31
32
33

Add successors(n) top OPEN
successors(n) ==
let p parent n
n node labeled Xi
Xi == X1
Call BottomUpReduction procedure

34
35
36
37
38
39

// Constraint Propagation

// AND-expand

// Backtrack

// Search complete
// begin reduction AOMDD

Cache(asgn(n )[pai ]) n
Add meta-node n list LXi
consistent(p) consistent(p) consistent(n)
consistent(p) == f alse
remove successors(p) OPEN
successors(p)

40
41

n node labeled hXi , xi
consistent(p) consistent(p) consistent(n);

42
43

remove n successors(p)
np

487

// Save cache

// Check p dead-end

fiM ATEESCU , ECHTER & ARINESCU

Procedure BottomUpReduction
: graphical model = hX, D, Fi; pseudo tree primal graph, rooted X1 ; Context
minimal AND/OR graph, lists LXi meta-nodes level Xi .
output : AOMDD M.
Let = {X1 , . . . , Xn } depth first traversal ordering
n 1
Let H hash table, initially empty
forall meta-nodes n LXi
H(Xi , n.children1 , . . . , n.childrenki , wn (Xi , x1 ), . . . , wn (Xki , xki )) returns meta-node
p
merge n p AND/OR graph
input

1
2
3
4
5
6
7
8
9
10
11
12

else n redundant
eliminate n AND/OR graph
combine weight parent
else
hash n table H:
H(Xi , n.children1 , . . . , n.childrenki , wn (Xi , x1 ), . . . , wn (Xki , xki )) n

13 return reduced AND/OR graph

parent p updated conjunction consistent(n). parent p becomes
inconsistent, necessary check remaining successors (line 38). backtrack
step processes node (line 40), consistent attribute parent p updated
disjunction consistent(n).
AND/OR search algorithm usually maintains value node, corresponding task
solved. include values description AOMDD equivalent
representation original graphical model M. task solved traversal
AOMDD. however user include information meta-nodes (e.g.,
number solutions subproblem).
5.2 Reducing Context Minimal AND/OR Graph AOMDD
Procedure BottomUpReduction processes variables bottom relative pseudo tree .
use depth first traversal ordering (line 1), bottom ordering good.
outer loop (starting line 2) goes level context minimal AND/OR graph
(where level contains nodes labeled variable, words
contains meta-nodes variable). efficiency, ensure complexity guarantees
prove, hash table, initially empty, used level. inner loop (starting
line 4) goes metanodes level, also saved (or pointers saved)
list LXi . new meta-node n list LXi , line 5 hash table H checked
verify node isomorphic n already exists. hash table H already contains node p corresponding hash key (Xi , n.children1 , . . . , n.childrenki , wn (Xi , x1 ), . . . , wn (Xki , xki )),
p n isomorphic merged. Otherwise, new meta-node n redundant,
eliminated AND/OR graph. none previous two conditions met,
new meta-node n hashed table H.

488

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS




G

C

B

F

E



H

B
C


(a)

F
E

G

H

(b)

Figure 15: (a) Constraint graph C = {C1 , . . . , C9 }, C1 = F H, C2 = H,
C3 = B G, C4 = F G, C5 = B F , C6 = E, C7 = C E, C8 = C D,
C9 = B C; (b) Pseudo tree (bucket tree) ordering = (A, B, C, D, E, F, G, H)

Proposition 1 output Procedure BottomUpReduction AOMDD along
pseudo tree , namely resulting AND/OR graph completely reduced.
Note explicated Procedure BottomUpReduction separately clarity. practice, actually included Algorithm AND/OR-S EARCH -AOMDD, reduction rules
applied whenever search backtracks. maintain hash table variable, AND/OR search, store pointers meta-nodes. search backtracks
node, already check redundancy meta-node, also look hash table
check isomorphism. Therefore, reduction AND/OR graph done
AND/OR search, output AOMDD M.
Theorem 3 Proposition 1 conclude:
HEOREM 4 Given graphical model pseudo tree primal graph G, AOMDD

corresponding size bounded O(n k wT (G) ) computed Algorithm

AND/OR-S EARCH -AOMDD time O(n k wT (G) ), wT (G) induced width G
depth first traversal , k bounds domain size.

6. Using Bucket Elimination Generate AOMDDs
section propose use Bucket Elimination (BE) type algorithm guide compilation
graphical model AOMDD. idea express graphical model functions
AOMDDs, combine APPLY operations based schedule. APPLY
similar OBDDs (Bryant, 1986), adapted AND/OR search graphs.
takes input two functions represented AOMDDs based pseudo tree, outputs
combination initial functions, also represented AOMDD based pseudo tree.
describe detail Section 6.2.
start example based constraint networks. easier understand
weights arcs 1 0, therefore depicted figures solid dashed
lines, respectively.
Example 11 Consider network defined X = {A, B, . . . , H}, DA = . . . = DH = {0, 1}
constraints (where denotes XOR): C1 = F H, C2 = AH, C3 = ABG, C4 = F G,
489

fiM ATEESCU , ECHTER & ARINESCU

m7



m7



0

0

1

B
0

1

0

C
0

C
1

0

1

0

0

1

C
1

0



1

B

B

0

0

F

C
1

B
1

0

1

F
1

0

1

F
1

B

F

0

1

0

1

B

C

0



G

E
1

0

0

0

1

0

1

m6

G
1

0

H
1

0

0

1



E

1

m3

F

H
1

G

m3

m6


0

0

B

C

0

C
0



1

0

1


1

0

0

C

C
1

0

0

F

B

0

1

0



1

0

0

G

1

0

0

1

0

0

H
0

1

B

F
1

G
1

1

F

F

1

F
0

1



B
1

B
1

E


1

1

B



0

H

1

H
1

0

F
1

C

0 1

0

1

0

m5

C9

C

0

E

0

C


1

0

0

0

E



1

C
1

0

1

0

0

1

G

1

0

1

0

C



0 1

C6

C7

1

0

1

0

1

0

1

0

B

H

0 1

1


1

0

H

1

G

E

C3

0

1



1

G
0

G

F

F
0

B

G

H

m1

m1

1

B

0 1

C8

m2





1

E

E

1

0

C5

m2


1







m4

m5
0

0 1

1
m4

C4

0

H
1

0

F

1

F

0 1

0 1

G

C1

C2

H

Figure 16: Execution AOMDDs

0



1

B
0

B
1

0

B

1

B

C
C
0

C
1

0

C
1

0

C
1

0

F
1

0

F
1

0

F
1

0

C

1

0



1



E


0


1

0

E
1

0

G
1

0

G
1

0

H
1

0

C

C

F

0





E

F

H
1



E

F

F

F

1

G

G

G

G

H

0



1

H

1

(a)

G

0

(b)

Figure 17: (a) final AOMDD; (b) OBDD corresponding
C5 = B F , C6 = E, C7 = C E, C8 = C D, C9 = B C. constraint graph
shown Figure 15(a). Consider ordering = (A, B, C, D, E, F, G, H). pseudo tree (or
bucket tree) induced given Fig. 15(b). Figure 16 shows execution AOMDDs
along ordering d. Initially, constraints C1 C9 represented AOMDDs placed
bucket latest variable d. scope original constraint always appears

490

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

Algorithm 2: BE-AOMDD
: Graphical model = hX, D, Fi, X = {X1 , . . . , Xn }, F = {f1 , . . . , fr } ; order
= (X1 , . . . , Xn )
output : AOMDD representing fi
1 = GeneratePseudoTree(G, d);
2 1 r
// place functions buckets
3
place Gfaomdd


bucket


latest
variable



input

4 n 1
message(Xi ) G1aomdd
5
6
bucket(Xi ) 6=
7
pick Gfaomdd bucket(Xi );
8
9
10

// process buckets
// initialize AOMDD 1 ;
// combine AOMDDs bucket Xi

bucket(Xi ) bucket(Xi ) \ {Gfaomdd };
message(Xi ) APPLY(message(Xi ), Gfaomdd )
add message(Xi ) bucket parent Xi

11 return message(X1 )

path root leaf pseudo tree. Therefore, original constraint represented
AOMDD based chain (i.e., branching independent components point).
chain scope constraint, ordered according d. bi-valued variables,
original constraints represented OBDDs, multiple-valued variables MDDs. Note
depict meta-nodes: one node two children, appear inside gray
node. dotted edge corresponds 0 value (the low edge OBDDs), solid edge
1 value (the high edge). redundancy notation, keeping value nodes
arc-types (dotted arcs 0 solid arcs 1).
scheduling used process buckets reverse order d. bucket processed
joining AOMDDs inside it, using APPLY operator. However, step elimination
bucket variable omitted want generate full AOMDD. example,
messages m1 = C1 ./ C2 m2 = C3 ./ C4 still based chains, therefore
OBDDs. Note contain variables H G, eliminated. However,
message m3 = C5 ./ m1 ./ m2 OBDD anymore. see follows
structure pseudo tree, F two children, G H. nodes corresponding
F two outgoing edges value 1.
processing continues manner. final output algorithm, coincides
m7 , shown Figure 17(a). OBDD based ordering shown Fig.
17(b). Notice AOMDD 18 nonterminal nodes 47 edges, OBDD 27
nonterminal nodes 54 edges.
6.1 Algorithm BE-AOMDD
Algorithm 2, called BE-AOMDD, creates AOMDD graphical model using schedule APPLY operations. Given order variables, first pseudo tree created based
,
primal graph. initial function fi represented AOMDD, denoted Gfaomdd

placed bucket. obtain AOMDD function, scope function ordered
according d, search tree (based chain) represents fi generated, reduced
Procedure BottomUpReduction. algorithm proceeds exactly like BE, difference combination functions realized APPLY algorithm, variables
491

fiM ATEESCU , ECHTER & ARINESCU

eliminated carried destination bucket. messages buckets initialized
dummy AOMDD 1, denoted G1aomdd , neutral combination.
order create compilation graphical model based AND/OR graphs, necessary
traverse AND/OR graph top bottom up. similar inward outward
message passing tree decomposition. Note BE-AOMDD describes bottom traversal
explicitly, top phase actually performed APPLY operation. two
AOMDDs combined, top chain portion pseudo tree processed, remaining
independent branches attached participate newly restricted set solutions.
amounts exchange information independent branches, equivalent
top phase.
6.2 AOMDD APPLY Operation
describe combine two AOMDDs. APPLY operator takes input two
AOMDDs representing functions f1 f2 returns AOMDD representing f1 f2 .
OBDDs apply operator combines two input diagrams based variable ordering.
Likewise, order combine two AOMDDs assume pseudo trees identical.
condition satisfied two AOMDDs bucket BE-AOMDD. However,
present version APPLY general, relaxing previous condition
identical compatible pseudo trees. Namely, pseudo tree
embedded. general, pseudo tree induces strict partial order variables
parent node always precedes child nodes.
EFINITION 24 (compatible pseudo trees) strict partial order d1 = (X, <1 ) set X
consistent strict partial order d2 = (Y, <2 ) set Y, x1 , x2 X Y,
x1 <2 x2 x1 <1 x2 . Two partial orders d1 d2 compatible iff exists partial
order consistent both. Two pseudo trees compatible iff partial orders induced
via parent-child relationship, compatible.
simplicity, focus restricted notion compatibility, sufficient
using like schedule APPLY operator combine input AOMDDs (as described
Section 6). APPLY algorithm present extended general notion
compatibility.
EFINITION 25 (strictly compatible pseudo trees) pseudo tree T1 set nodes X1
embedded pseudo tree set nodes X X1 X T1 obtained
deleting node X \ X1 connecting parent descendents. Two
pseudo trees T1 T2 strictly compatible exists T1 T2
embedded .
Algorithm APPLY (algorithm 3) takes input one node Gfaomdd list nodes
Ggaomdd . Initially, node Gfaomdd root node, list nodes Ggaomdd fact
also made one node, root. sometimes identify AOMDD root
node. pseudo trees Tf Tg strictly compatible, target pseudo tree .
list nodes Ggaomdd always special property: node
ancestor another (we refer variable meta-node). Therefore, list z1 , . . . , zm
492

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

APPLY (v1 ; z1 , . . . , zm )
input : AOMDDs Gfaomdd nodes vi Ggaomdd nodes zj , based strictly compatible pseudo
trees Tf , Tg embedded .
var(v1 ) ancestor var(z1 ), . . . , var(zm ) .
var(zi ) var(zj ) ancestor-descendant relation , 6= j.
output : v1 (z1 . . . zm ), based .
H1 (v1 , z1 , . . . , zm ) 6= null return H1 (v1 , z1 , . . . , zm );
// cache
(any v1 , z1 , . . . , zm 0) return 0
(v1 = 1) return 1
(m = 0) return v1
// nothing combine
create new nonterminal meta-node u
var(u) var(v1 ) (call Xi , domain Di = {x1 , . . . , xki } )
j 1 ki
u.childrenj
// children j-th node u
// assign weight v1
wu (Xi , xj ) wv1 (Xi , xj )
( (m = 1) (var(v1 ) = var(z1 ) = Xi ) )
temp Children z1 .childrenj
// combine input weights
wu (Xi , xj ) wv1 (Xi , xj ) wz1 (Xi , xj )

Algorithm 3:

1
2
3
4
5
6
7
8
9
10
11
12
13
14

else

15
16
17
18
19

group nodes v1 .childrenj temp Children several {v 1 ; z 1 , . . . , z r }
{v 1 ; z 1 , . . . , z r }
APPLY(v 1 ; z 1 , . . . , z r )
(y = 0)
u.childrenj 0; break

20
21

temp Children {z1 , . . . , zm }

else
u.childrenj u.childrenj {y}

22
23
24

(u.children1 = . . . = u.childrenki ) (wu (Xi , x1 ) = . . . = wu (Xi , xki ))
promote wu (Xi , x1 ) parent
return u.children1
// redundancy

25
26

(H2 (Xi , u.children1 , . . . , u.childrenki , wu (Xi , x1 ), . . . , wu (Xki , xki )) 6= null)
return H2 (Xi , u.children1 , . . . , u.childrenki , wu (Xi , x1 ), . . . , wu (Xki , xki ))
// isomorphism

27 Let H1 (v1 , z1 , . . . , zm ) = u
28 Let H2 (Xi , u.children1 , . . . , u.childrenki , w u (Xi , x1 ), . . . , w u (Xki , xki )) = u
29 return u

// add u H1
// add u H2

g expresses decomposition respect , nodes appear different branches.
employ usual techniques OBDDs make operation efficient. First, one
arguments 0, safely return 0. Second, hash table H1 used store nodes
already processed, based nodes (v1 , z1 , . . . , zr ). Therefore, never need
make multiple recursive calls arguments. Third, hash table H2 used detect
isomorphic nodes. typically split separate tables variable. end
recursion, returning value, discover meta-node variable,
children weights already created, dont need store simply
return existing node. fourth, end recursion discover created
redundant node (all children weights same), dont store
it, return instead one identical lists children, promote common weight.
493

fiM ATEESCU , ECHTER & ARINESCU


0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

C
0
1
0
1
0
1
0
1

f(ABC)
0
0
0
1
0
1
0
1


0
0
0
0
1
1
1
1


B
C



A1
0

0

C

A3

*

0

A4

B2

A5

B3

0

B

B





1


1



C


0

0

0



A1B1
B

1

B4



1

B
0

1

1

g(ABC)
0
0
0
1
0
1
1
0


0

1

0


0
1
0
1
0
1
0
1

B1
1

B

A2

B
0
0
1
1
0
0
1
1

0

B6

=

B

A2B2

B7

0

1

C
0

B5

0

A4B6

1

A4

1

1



B4
1

0

B
0

1



B7
1

0

1

1

Figure 18: Example APPLY operation
Note v1 always ancestor z1 , . . . , zm . consider variable
ancestor itself. self explaining checks performed lines 1-4. Line 2 specific
multiplication, needs changed combination operations. algorithm creates
new meta-node u, whose variable var(v1 ) = Xi recall var(v1 ) highest (closest root)
among v1 , z1 , . . . , zm . Then, possible value Xi , line 7, starts building list
children.
One important steps happens line 15. two lists meta-nodes, one
original AOMDD f g, refer variables, appear .
lists important property mentioned above, nodes ancestors
other. union two lists grouped maximal sets nodes, highest node
set ancestor others. follows root node set belongs one
original AOMDD, say v 1 f , others, say z 1 , . . . , z r g. example,
suppose pseudo tree Fig. 15(b), two lists {C, G, H} f {E, F }
g. grouping line 15 create {C; E} {F ; G, H}. Sometimes, may
case newly created group contains one node. means nothing join
recursive calls, algorithm return, via line 4, single node. on, one
input AOMDDs traversed, important complexity APPLY, discussed
below.
Example 12 Figure 18 shows result combining two Boolean functions operation
(or product). input functions f g represented AOMDDs based chain pseudo
trees, results based pseudo tree expresses decomposition variables
B instantiated. APPLY operator performs depth first traversal two input
AOMDDs, generates resulting AOMDD based output pseudo tree. Similar
case OBDDs, function AOMDD identified root meta-node. example
input meta-nodes labels (A1 , A2 , B1 , B2 , etc.). output meta-node labeled A2 B2
494

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

root diagram represents function obtained combining functions rooted A2
B2 .
6.3 Complexity APPLY BE-AOMDD
provide characterization complexity APPLY, based different criteria.
following propositions inspired results govern OBDD apply complexity,
adapted pseudo tree orderings.
AOMDD along pseudo tree regarded union regular MDDs, restricted
full path root leaf pseudo tree. Let path . Based
definition strictly compatible pseudo trees, corresponding paths Tf Tf Tg Tg .
MDDs f g corresponding Tf Tg combined using regular MDD
apply. process repeated every path . resulting MDDs, one path
need synchronized common parts (on intersection paths). algorithm
proposed processing once, depth first search traversal inputs. Based
construction, give first characterization complexity AOMDD APPLY
governed complexity MDD apply.
Proposition 2 Let 1 , . . . , l set paths enumerated left right let Gfi
Ggi MDDs restricted path , size output AOMDD apply

P
P bounded
| |G | n max |G | |G |. time complexity also bounded
| |G |
|G
|G

g
g
g
f
f
f
n maxi |Gfi | |Ggi |.
second characterization complexity given, similar MDD case, terms
total number nodes inputs:
Proposition 3 Given two AOMDDs Gfaomdd Ggaomdd based strictly compatible pseudo trees,
size output APPLY O(| Gfaomdd | | Ggaomdd |).
detail previous proposition follows. Given AOMDDs Gfaomdd Ggaomdd ,
based compatible pseudo trees Tf Tg common pseudo tree , define intersection pseudo tree Tf g obtained following two steps: (1) mark
subtrees whose nodes belong either Tf Tg (the leaves subtree
leaves ); (2) remove subtrees marked step (1) . Steps (1) (2) applied
(that is, recursively). part AOMDD Gfaomdd corresponding variables Tf g
denoted Gff g , similarly Ggaomdd denoted Ggf g .
Proposition 4 time complexity
|Gfaomdd | + |Ggaomdd |).

APPLY

size output O(|Gff g | |Ggf g | +

turn complexity BE-AOMDD algorithm. bucket associated
bucket pseudo tree. top chain bucket pseudo tree variable Xi contains
variables context(Xi ). variables appear bucket pseudo tree,
associated buckets already processed. original functions belong bucket
Xi scope included context(Xi ), therefore associated AOMDDs based
495

fiM ATEESCU , ECHTER & ARINESCU

chains. functions appear bucket Xi messages received independent branches below. Therefore, two functions bucket Xi share variables
context(Xi ), forms top chain bucket pseudo tree. therefore characterize
complexity APPLY terms treewidth, context size bucket variable.
Proposition 5 Given two AOMDDs bucket BE-AOMDD, time space complexity APPLY exponential context size bucket variable
(namely number variables top chain bucket pseudo tree).
bound complexity BE-AOMDD output size:
HEOREM 5 space complexity BE-AOMDD size output AOMDD

O(n k w ), n number variables, k maximum domain size w treewidth

bucket tree. time complexity bounded O(r k w ), r number initial
functions.

7. AOMDDs Canonical Representations
well known OBDDs canonical representations Boolean functions given ordering
variables (Bryant, 1986), namely strict ordering CNF specification
Boolean function yield identical OBDD, property extends MDDs (Srinivasan
et al., 1990). linear ordering variables defines chain pseudo tree captures
structure OBDD MDD. case AOBDDs AOMDDs, canonicity
respect pseudo tree, transitioning total orders (that correspond linear ordering)
partial orders (that correspond pseudo tree ordering). one hand gain ability
compact compiled structure, hand canonicity longer respect
equivalent graphical models, relative graphical models consistent
pseudo tree used. Specifically, start strict ordering generate chain
AOMDD canonical relative equivalent graphical models. however want
exploit additional decomposition use partial ordering captured pseudo-tree create
compact AOMDD. AOMDD however canonical relative equivalent graphical
models accept pseudo tree guided AOMDD. general, AOMDD
viewed flexible framework compilation allows partial total orderings.
Canonicity restricted subset graphical models whose primal graph agrees partial
order relevant larger set orderings consistent pseudo-tree.
following subsection discuss canonicity AOMDD constraint networks.
case general weighted graphical models discussed Section 8.
7.1 AOMDDs Constraint Networks Canonical Representations
case constraint networks straightforward, weights OR-to-AND
arcs 0 1. show equivalent constraint networks, admit
pseudo tree , AOMDD based . start proposition help prove
main theorem.
Proposition 6 Let f function, always zero, defined constraint network X. Given
partition {X1 , . . . , Xm } set variables X (namely, Xi Xj = , 6= j, X =
496

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS




i=1 X ), f = f1 . . . fm f = g1 . . . gm , scope(fi ) = scope(gi ) = X
{1, . . . , m}, fi = gi {1, . . . , m}. Namely, f decomposed
given partition, decomposition unique.

ready show AOMDDs constraint networks canonical representations
given pseudo tree.
HEOREM 6 (AOMDDs canonical given pseudo tree) Given constraint network,
pseudo tree constraint graph, unique (up isomorphism) AOMDD represents it, minimal number meta-nodes.
constraint network defined relations (or functions). exist equivalent constraint
networks defined different sets functions, even different scope signatures.
However, equivalent constraint networks define function, ask AOMDD
different equivalent constraint networks same. following corollary derived
immediately Theorem 6.
Corollary 1 Two equivalent constraint networks admit pseudo tree
AOMDD based .

8. Canonical AOMDDs Weighted Graphical Models
Theorem 6 ensures AOMDD canonical constraint networks, namely functions
take values 0 1. proof relied fact OR-to-AND weights
0 1, Proposition 6 ensured unique decomposition function defined
constraint network.
section turn general weighted graphical models. first observe Proposition 6 longer valid general functions. valid solutions (having strictly
positive weight) weight decomposed one way product positive
weights.
Therefore raise issue recognizing nodes root AND/OR graphs represent
universal function, even though graphical representation different. see
AOMDD weighted graphical model unique current definitions,
slightly modify obtain canonicity again. note canonicity AOMDDs
weighted graphical models (e.g., belief networks) far less crucial case OBDDs
used formal verification. Even that, sometimes may useful eliminate
redundant nodes, order maintain simpler semantics AND/OR graph represents
model.
loss canonicity AOMDD weighted graphical models happen
weights OR-to-AND arcs, suggest possible way re-enforcing compact
canonical representation needed.
Example 13 Figure 19 shows weighted graphical model, defined two (cost) functions,
f (M, A, B) g(M, B, C). Assuming order (M,A,B,C), Figure 20 shows AND/OR search
tree left. arcs labeled function values, leaves show value
corresponding full assignment (which product numbers arcs path).
497

fiM ATEESCU , ECHTER & ARINESCU




0
0
0
0
1
1
1
1






B
B
C

C


0
0
1
1
0
0
1
1

B f(M,A,B)
0
12
1
5
0
18
1
2
0
4
1
10
0
6
1
4


0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

C g(M,B,C)
0
3
1
5
0
14
1
12
0
9
1
15
0
7
1
6

Figure 19: Weighted graphical model

0

1

0

1









0

1

B

0

B

5

18

2

0

1

0

1

C

C

5

0

1

36 60

14
0

C

3

5

1

0

1

70 60

54 90

14
0

0

B

4

10

0

C

12

1

B

12

3



C

B

6

1

0

C

C

9

15

7

6

9

15

7

1

0

1

0

1

0

1

0

36 60

70 60

54 90

0

B

12

5

18

2

1

0

1

0

1

C

6
1

28 24

5

0

1

14
0

B

4
0

C

3

1

B

4

C

12

28 24

1

10

6

4

1

0

1

C

C

12

9

15

7

6

1

0

1

0

1

36 60

Figure 20: AND/OR search tree context minimal graph

see either value (0 1) gives rise function (because leaves two
subtrees values). However, two subtrees identified representing
function usual reduction rules. right part figure shows context minimal
graph, compact representation subtree, share parts.
would like case method recognizing left right subtrees
corresponding = 0 = 1 represent function. normalizing
values level, processing bottom up. Figure 21 left, values OR-to-AND
arcs normalized, variable, normalization constant promoted
value. Figure 21 right, normalization constants promoted upwards
multiplication. process change value full assignment, therefore
produces equivalent graphs.
see already nodes labeled C merged, producing graph
Figure 22 left. Continuing process obtain AOMDD weighted graph,
shown Figure 22 right.
define AOMDD weighted graphical model follows:
EFINITION 26 (AOMDD weighted graphical model) AOMDD weighted graphical
model AND/OR graph, meta-nodes, that: (1) meta-node, weights sum
1; (2) root meta-node constant associated it; (3) completely reduced, namely
isomorphic meta-nodes, redundant meta-nodes.

498

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS




0

1





0

1

B

0

B

B

12

5

18

2

0

1

0

1

8
3/8
0

5/8

14/26

1

0

12/26

3/8

6

4

1

0

1

1

0

14/26

1

0

1

26*5

0

1

3/8

26*2

0

1

1

0

7/13

1

0

B

24*4

13*10

0

24*6

1

C

5/8

1

B

8*18

C

12/26

0

B

8*12

13 C
5/8



B

10

24 C

1


0

B

4
0

26 C

C

1

0

13*4

0

1

C

6/13

C

3/8

1

5/8

0

7/13

1

0

6/13
1

Figure 21: Normalizing values bottom
844



1/2
0



1/2

1
0



1




0

1

B

96
0

0

B

130

144

1

0

1

B

52
1

96
0

B

130

144

1

0

C

3/8
0

1

7/13
0

196/422

0

1

B

52

B

96/226

1

130/226

0

1

C

5/8

226/422

144/196

52/196

0

1

C

6/13

3/8

1

0

C

5/8
1

7/13
0

6/13
1

Figure 22: AOMDD weighted graph
procedure transforming weighted AND/OR graph AOMDD similar
Procedure B OTTOM U P R EDUCTION Section 5. difference new layer
processed, first meta-node weights normalized promoted parent,
procedure continues usual reduction rules.
HEOREM 7 Given two equivalent weighted graphical models accept common pseudo tree
, normalizing arc values together exhaustive application reduction rules yields
AND/OR graph, AOMDD based .
Finite Precision Arithmetic implementation algorithm described section may
prove challenging machines used finite precision arithmetic. Since weights
real-valued, repeated normalization may lead precision errors. One possible approach,
also used experiments, define -tolerance, user defined sufficiently
small , consider weights equal within other.

9. Semantic Treewidth
graphical model represents universal function F = fi . function F may represented
different graphical models. Given particular pseudo tree , captures structural
information F , interested graphical models accept pseudo tree, namely
primal graphs contain edges backarcs . Since size AOMDD F
based bounded worst case induced width graphical model along ,
define semantic treewidth be:
499

fiM ATEESCU , ECHTER & ARINESCU

1


2

3

B
C

C


B



1

2



3

4




C








B

4



B
1 3
1 4
2 4
3 1
4 1
4 2




(a) two solutions

C
1 2
1 4
2 1
2 3
3 2
3 4
4 1
4 3


1 2
1 3
2 1
2 3
2 4
3 1
3 2
3 4
4 2
4 3


B C
1 3
1 4
2 4
3 1
4 1
4 2

B
1 2
1 4
2 1
2 3
3 2
3 4
4 1
4 3

C
1 3
1 4
2 4
3 1
4 1
4 2

(b) First model



B
2 4
3 1

B

C

B C
1 4
4 1



C
1 3
4 2

(c) Second model

Figure 23: 4-queen problem
EFINITION 27 (semantic treewidth) semantic treewidth graphical model relative
pseudo tree denoted swT (M), smallest treewidth taken models R
equivalent M, accept pseudo tree . Formally, defined swT (M) =
minR,u(R)=u(M) wT (R), u(M) universal function M, wT (R) induced
width R along . semantic treewidth graphical model, M, minimal semantic
treewidth pseudo trees express universal function.
Computing semantic treewidth shown NP-hard.3
HEOREM 8 Computing semantic treewidth graphical model NP-hard.
Theorem 8 shows computing semantic treewidth hard, likely actual
complexity even higher. However, semantic treewidth explain sometimes minimal
AND/OR graph OBDD much smaller exponential treewidth pathwidth upper
bounds. many cases, could huge disparity treewidth semantic
treewidth along .
Example 14 Figure 23(a) shows two solutions 4-queen problem. problem expressed complete graph treewidth 3, given Figure 23(b). Figure 23(c) shows equivalent
problem (i.e., set solutions), treewidth 1. semantic treewidth
4-queen problem 1.
Based fact AOMDD canonical representation universal function
graphical model, conclude size AOMDD bounded exponentially
semantic treewidth along pseudo tree, rather treewidth given graphical model
representation.
Proposition 7 size AOMDD graphical model bounded O(n k swT (M )),
n number variables, k maximum domain size swT (M) semantic
treewidth along pseudo tree .
3. thank David Eppstein proof.

500

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS




B

B

C

C

B







C

N



P

0

1

(a) OBDD representation

(b) Primal graph hidden
variables M, N P .

Figure 24: parity function
Example 15 Consider constraint network n variables every two variables constrained equality (X = ). One graph representation complete graph, another chain
another tree. problem specified complete graph, use linear order,
OBDD linear size exists representation pathwidth 1
(rather n).
semantic treewidth yield much better upper bound AOMDD, also
bad bound. well known parity function n variables compact,
chain-like OBDD representation. Yet, constraint network representation parity function
function (namely complete graph variables), whose treewidth semantic
treewidth number variables, n. OBDD representation parity function suggests
addition hidden variables simplify presentation. show example Figure
24. left side, Figure 24(a) OBDD representation parity function
four binary variables. graphical model would represent function complete graph
four variables. However, could add extra variables M, N P Figure 24(b), sometimes
called hidden variables, help decompose model. case form constraint
together B represents parity B, namely = 1 B = 1,
parity (XOR) operator. Similarly, N would capture parity C, P
would capture parity N D, would also give parity initial four variables.
two structures surprisingly similar. would interesting study connection
hidden variables compact AOBDDs, leave future work.

10. Experimental Evaluation
experimental evaluation preliminary stages, results already encouraging. ran search-based compile algorithm, recording trace AND/OR search,
reducing resulting AND/OR graph bottom up. results applied reduction isomorphism still kept redundant meta-nodes. implemented algorithms
C++ ran experiments 2.2GHz Intel Core 2 Duo 2GB RAM, running Windows.

501

fiM ATEESCU , ECHTER & ARINESCU

10.1 Benchmarks
tested performance search-based compilation algorithm random Bayesian networks, instances Bayesian Network Repository subset networks UAI06
Inference Evaluation Dataset.
Random Bayesian Networks random Bayesian networks generated using parameters
(n, k, c, p), n number variables, k domain size, c number conditional
probability tables (CPTs) p number parents CPT. structure network
created randomly picking c variables n and, each, randomly picking p parents
preceding variables, relative ordering. remaining n c variables called root
nodes. entries probability table generated randomly using uniform distribution,
table normalized. also possible control amount determinism
network forcing percentage det CPTs 0 1 entries.
Bayesian Network Repository Bayesian Network Repository4 contains collection belief
networks extracted various real-life domains often used benchmarking probabilistic inference algorithms.
UAI06 Inference Evaluation Dataset UAI 2006 Inference Evaluation Dataset5 contains
collection random well real-world belief networks used first UAI 2006
Inference Evaluation contest. purpose selected subset networks derived
ISCAS89 digital circuits benchmark.6 ISCAS89 circuits common benchmark used
formal verification diagnosis. circuits converted Bayesian network
removing flip-flops buffers standard way, creating deterministic conditional probability
table gate, putting uniform distributions input signals.
10.2 Algorithms
consider two search-based compilation algorithms, denoted AOMDD-BCP AOMDDSAT, respectively, reduce context minimal AND/OR graph explored via isomorphism,
exploiting determinism (if any) present network. approach take handling
determinism based unit resolution CNF encoding (i.e., propositional clauses) zero
probability tuples CPTs. idea using unit resolution search Bayesian networks first explored Allen Darwiche (2003). AOMDD-BCP conservative applies
unit resolution node search graph, whereas AOMDD-SAT aggressive
detects inconsistency running full SAT solver. used zChaff SAT solver (Moskewicz,
Madigan, Zhao, Zhang, & Malik, 2001) unit resolution well full satisfiability.
comparison, also ran version AOMDD-BCP, called MDD-BCP.
reference also report results obtained ACE7 compiler. ACE compiles Bayesian
network Arithmetic Circuit (AC) uses AC answer multiple queries respect network. arithmetic circuit representation equivalent AND/OR graphs
(Mateescu & Dechter, 2007). time ACE compiler invoked, uses one two algorithms
basis compilation. First, elimination order generated network
4.
5.
6.
7.

http://www.cs.huji.ac.il/compbio/Repository/
http://ssli.ee.washington.edu/bilmes/uai06InferenceEvaluation
Available at: http://www.fm.vslib.cz/kes/asic/iscas/
Available at: http://reasoning.cs.ucla.edu/ace

502

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

Network

(w*, h)

(n, k)

ACE
#nodes time

MDD w/ BCP
AOMDD w/ BCP
AOMDD w/ SAT
#meta #cm(OR) time
#meta #cm(OR)
time #meta #cm(OR)
time
Bayesian Network Repository
alarm
(4, 13) (37, 4)
1,511 0.01 208,837 682,195 73.35
320
459
0.05
320
459
0.22
cpcs54
(14, 23) (54, 2)
196,933 0.06
- 65,158 66,405
6.97 65,158 66,405
6.97
cpcs179
(8, 14) (179, 4)
67,919 0.05
9,990 32,185 46.56 9,990 32,185 46.56
cpcs360b (20, 27) (360, 2) 5,258,826 1.72
diabetes
(4, 77) (413, 21) 7,615,989 1.81
hailfinder (4, 16) (56, 11)
8,815 0.01
2,068
2,202
0.34 1,893
2,202
1.48
mildew
(4, 13) (35, 100) 823,913 0.39
- 73,666 110,284 1367.81 62,903 65,599 3776.82
mm
(20, 57) (1220, 2)
47,171 1.49
- 38,414 58,144
4.54 30,274 52,523 99.55
munin2
(9, 32) (1003, 21) 2,128,147 1.91
munin3
(9, 32) (1041, 21) 1,226,635 1.27
munin4
(9, 32) (1044, 21) 2,423,009 4.44
pathfinder (6, 11) (109, 63)
18,250 0.05 610,854 1,303,682 352.18
6,984 16,267 30.71 2,265 15,963 50.36
pigs
(11, 26) (441, 3) 636,684 0.19
- 261,920 294,101 174.29 198,284 294,101 1277.72
water
(10, 15) (32, 4)
59,642 0.52 707,283 1,138,096 95.14 18,744 20,926
2.02 18,503 19,225
7.45
UAI06 Evaluation Dataset
BN 42
(21, 62) (851, 2)
4,860 1.35
- 107,025 341,428 53.50 42,445 43,280 57.36
BN 43
(26, 65) (851, 2)
10,373 1.62
- 1,343,923 1,679,013 1807.63 313,388 314,669 434.38
BN 44
(25, 56) (851, 2)
4,235 1.31
- 155,588 187,589 20.90 47,222 48,540 66.09
BN 45
(22, 54) (851, 2)
12,319 1.50
- 390,795 487,593 68.81 126,182 126,929 177.50
BN 46
(20, 46) (851, 2)
5,912 2.90 1,125,658 1,228,332 94.93 16,711 17,532
1.31 7,337
7,513
5.54
BN 47
(39, 57) (632, 2)
1,448 1.17 42,419 47,128 2.87
1,873
2,663
0.24 1,303
2,614
2.36
BN 49
(40, 60) (632, 2)
1,408 1.16 18,344 19,251 1.32
1,205
1,539
0.19
952
1,515
1.34
BN 51
(41, 68) (632, 2)
1,467 1.15 63,851 68,005 4.22
4,442
5,267
0.50 3,653
5,195
4.58
BN 53
(47, 87) (532, 2)
1,357 0.91 14,210 19,162 1.49
4,819
9,561
0.74 1,365
1,719
1.36
BN 55
(49, 92) (532, 2)
1,288 0.93
5,168
6,088 0.57
1,972
2,816
0.26
790
904
0.75
BN 57
(49, 85) (532, 2)
1,276 0.90 48,436 51,611 3.52
4,036
5,089
0.37
962
1,277
1.01
BN 59
(52, 87) (511, 2)
1,749 0.93 332,030 353,720 25.61 22,963 29,146
2.14 10,655 18,752 14.17
BN 61
(41, 64) (638, 2)
1,411 1.10 20,459 20,806 1.45
1,244
1,589
0.17 1,016
1,528
1.37
BN 63
(53, 95) (511, 2)
1,324 0.90 11,461 17,087 1.28
7,182 14,048
1.07 1,419
2,177
1.69
BN 65
(56, 86) (411, 2)
1,184 0.75
- 20,764 23,102
1.52 12,569 19,778 12.90
BN 67
(54, 88) (411, 2)
1,031 0.74
- 179,067 511,031 154.91
716
1,169
0.78
Positive Random Bayesian Networks (n=75, k=2, p=2, c=65)
r75-1
(12, 22) (75, 2)
67,737 0.31
- 21,619 21,619
2.59 21,619 21,619
2.59
r75-2
(12, 23) (75, 2)
46,703 0.29
- 18,083 18,083
1.88 18,083 18,083
1.88
r75-3
(11, 26) (75, 2)
53,245 0.30
- 18,419 18,419
1.86 18,419 18,419
1.86
r75-4
(11, 19) (75, 2)
28,507 0.29
8,363
8,363
1.16 8,363
8,363
1.16
r75-5
(13, 24) (75, 2)
149,707 0.36
- 42,459 42,459
4.61 42,459 42,459
4.61
r75-6
(14, 24) (75, 2)
132,107 1.19
- 62,621 62,621
6.95 62,621 62,621
6.95
r75-7
(12, 24) (75, 2)
89,913 0.36
- 21,583 21,583
2.42 21,583 21,583
2.42
r75-8
(14, 24) (75, 2)
86,183 0.36
- 49,001 49,001
6.23 49,001 49,001
6.23
r75-9
(11, 19) (75, 2)
29,025 0.30
7,681
7,681
0.81 7,681
7,681
0.81
r75-10
(10, 24) (75, 2)
20,291 0.28
5,905
5,905
0.63 5,905
5,905
0.63
Deterministic Random Bayesian Networks (n=100, k=2, p=2, c=90) det = 25% CPTs containing 0 1 entries
r100d25-1 (13, 31) (100, 2)
68,398 0.38
- 34,035 34,075
2.94 34,035 34,075 12.77
r100d25-2 (16, 28) (100, 2) 150,134 0.46
- 70,241 70,931
7.72 70,241 70,931 27.17
r100d25-3 (16, 29) (100, 2) 705,200 0.96
- 134,079 135,203 13.80 134,079 135,203 50.51
r100d25-4 (16, 31) (100, 2) 161,902 0.54
- 79,366 79,488
7.26 79,366 79,488 28.06
r100d25-5 (16, 29) (100, 2) 185,348 0.53
- 140,627 140,636 14.57 140,627 140,636 49.42
r100d25-6 (18, 28) (100, 2) 148,835 0.66
- 204,232 210,066 17.56 197,134 210,066 92.24
r100d25-7 (16, 29) (100, 2) 264,629 0.60
- 134,344 135,008 14.26 133,850 135,008 55.60
r100d25-8 (17, 27) (100, 2)
65,186 0.46
- 36,857 36,887
2.95 36,857 36,887 11.97
r100d25-9 (14, 27) (100, 2) 140,014 0.40
- 58,421 59,791
6.88 58,172 59,791 23.21
r100d25-10 (16, 27) (100, 2) 173,808 0.58
- 69,110 69,136
7.50 69,110 69,136 26.50

Table 1: Results experiments 50 Bayesian networks 3 problem classes; w =
treewidth, h = depth pseudo tree, n = number variables, k = domain size, time
given seconds; bold types highlight best results across rows.

503

fiM ATEESCU , ECHTER & ARINESCU

sufficiently small induced width, tabular variable elimination used basis.
algorithm similar one discussed Chavira Darwiche (2007), uses tables represent factors rather ADDs. induced width large, logical model counting
used basis. Tabular variable elimination typically efficient width small cannot
handle networks width larger. Logical model counting, hand, incurs
overhead tabular variable elimination, handle many networks larger treewidth.
tabular variable elimination logical model counting produce ACs exploit local structure, leading efficient online inference. logical model counting invoked, proceeds
encoding Bayesian network CNF (Chavira & Darwiche, 2005; Chavira, Darwiche, &
Jaeger, 2006), simplifying CNF, compiling CNF d-DNNF, extracting AC
compiled d-DNNF. dtree CNF clauses drives compilation step.
experiments report compilation time seconds (time), number
nodes context minimal graph explored (#cm), number meta-nodes resulting
AOMDD (#meta), well size AC compiled ACE (#nodes). network
specify number variables (n), domain size (k), induced width (w ) pseudo tree depth (h).
- stands exceeding 2GB memory limit respective algorithm. best performance
points highlighted.
10.3 Evaluation Bayesian Networks
Table 1 reports results obtained experiments 50 Bayesian networks. AOMDD
compilers well ACE used min-fill heuristic (Kjaerulff, 1990) construct guiding
pseudo tree dtree, respectively.
10.3.1 BAYESIAN N ETWORKS R EPOSITORY
see ACE overall fastest compiler domain, outperforming AOMDD-BCP
AOMDD-SAT several orders magnitude (e.g., mildew, pigs). However,
diagrams compiled ACE AOMDD-BCP (resp. AOMDD-SAT) comparable size.
cases, AOMDD-BCP AOMDD-SAT able compile much smaller diagrams
ACE. example, diagram produced AOMDD-BCP mildew network 13 times
smaller one compiled ACE. principle output produced ACE AOMDD
similar guided pseudo tree/dtree. scheme viewed
compilation alternative (1) extends decision diagrams (2) mimics traces search
properties may make representation accessible. compiler MDD-BCP able
compile 3 14 test instances, sizes far larger produced
AOMDD-BCP. instance, pathfinder network, AOMDD-BCP outputs decision
diagram almost 2 orders magnitude smaller MDD-BCP.
10.3.2 UAI06 DATASET
UAI06 Dataset instances picked randomly 30 variables instantiated
evidence. see ACE best performing compiler dataset. AOMDD-BCP
competitive ACE terms compile time 9 16 test instances. AOMDD-SAT
able compile smallest diagrams 6 networks (e.g., BN 47, BN 49, BN 55, BN 57,
BN 61, BN 67). before, difference size compiled data-structures produces
MDD-BCP AOMDD-BCP 2 orders magnitude favor latter.
504

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

10.3.3 R ANDOM N ETWORKS
problem instances denoted r75-1 r75-10 generated class random
belief networks parameters (n = 75, k = 2, p = 2, c = 65). Similarly, instances denoted
r100d25-1 r100d25-10 belong class parameters (n = 100, k = 2, p = 2, c =
90). latter case, det = 25% CPTs deterministic, namely contain 0
1 probability tuples. test instances compiled without evidence. see
domain AOMDD-BCP/AOMDD-SAT able compile smallest diagrams,
average 2 times smaller produced ACE. However, ACE fastest
compiler. Notice compiler MDD-BCP ran memory test cases.
10.4 Impact Variable Ordering
theory dictates, AOMDD size influenced quality guiding pseudo tree.
addition min-fill heuristic also considered hypergraph heuristic constructs
pseudo tree recursively decomposing dual hypergraph associated graphical model.
idea also explored Darwiche (2001) constructing dtrees guide ACE.
Since min-fill hypergraph partitioning heuristics randomized (namely ties
broken randomly), size AOMDD guided resulting pseudo tree may vary significantly one run next. Figure 25 displays AOMDD size using hypergraph min-fill
based pseudo trees 6 networks selected Table 1, 20 independent runs. also record
average induced width depth obtained pseudo trees (see header plot
Figure 25). see two heuristics dominate other, namely variance output
size quite significant cases.
10.5 Memory Usage
Table 2 shows memory usage (in MBytes) ACE, AOMDD-BCP AOMDD-SAT, respectively, Bayesian networks Table 1. see cases AOMDD based compilers require far less memory ACE. example, mildew network, AOMDDBCP AOMDD-SAT use 22 MB memory compile AND/OR decision diagram,
ACE requires much 218 MB memory. Moreover, compiled AOMDD
case one order magnitude fewer nodes constructed ACE. comparing
two AND/OR search-based compilers, observe networks significant amount
determinism, UAI06 Evaluation dataset, AOMDD-SAT uses average two
times less memory AOMDD-BCP. dramatic savings memory usage due aggressive constraint propagation employed AOMDD-SAT compared AOMDD-BCP
seen BN 67 network. case, difference memory usage AOMDD-SAT
AOMDD-BCP 2 orders magnitude favor former.

11. Related Work
related work viewed along two directions: (1) work related AND/OR search
idea graphical models (2) work related compilation graphical models exploits
problem structure.
extensive discussion (1) provided previous work Dechter Mateescu
(2007). Since focus paper, mention AND/OR idea origi505

fiM ATEESCU , ECHTER & ARINESCU

Figure 25: Effect variable ordering.

506

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

Network

ACE
AOMDD w/ BCP
AOMDD w/ SAT
#nodes memory (MB) #nodes memory (MB) #nodes memory (MB)
Bayesian Network Repository
alarm
1,511
0.00
320
0.0206
320
0.0206
cpcs54
196,933
4.00 65,158
3.4415 65,158
3.4415
cpcs179
67,919
5.00
9,990
1.9263 9,990
1.9263
cpcs360b 5,258,826
204.00
diabetes
7,615,989
449.00
hailfinder
8,815
0.00
2,068
0.1576 1,893
0.1740
mildew
823,913
218.00 73,666
22.5781 62,903
22.1467
mm
47,171
369.00 38,414
1.5719 30,274
1.3711
munin2
2,128,147
202.00
munin3
1,226,635
150.00
munin4
2,423,009
n/a
pathfinder
18,250
10.00
6,984
0.6009 2,265
0.3515
pigs
636,684
31.00 261,920
23.3761 198,284
17.7096
water
59,642
161.00 18,744
1.09578 18,503
1.3258
UAI06 Evaluation Dataset
BN 42
4,860
n/a 107,025
4.5622 42,445
1.9323
BN 43
10,373
n/a 1,343,923
57.8422 313,388
14.2828
BN 44
4,235
n/a 155,588
6.5613 47,222
2.1628
BN 45
12,319
n/a 390,795
17.9325 126,182
5.7958
BN 46
5,912
n/a
16,711
0.6929 7,337
0.3401
BN 47
1,448
n/a
1,873
0.0720 1,303
0.0583
BN 49
1,408
n/a
1,205
0.0449
952
0.0409
BN 51
1,467
n/a
4,442
0.1689 3,653
0.1633
BN 53
1,357
n/a
4,819
0.1814 1,365
0.0587
BN 55
1,288
n/a
1,972
0.0723
790
0.0336
BN 57
1,276
n/a
4,036
0.1495
962
0.0411
BN 59
1,749
n/a
22,963
0.8501 10,655
0.4587
BN 61
1,411
n/a
1,244
0.0463 1,016
0.0445
BN 63
1,324
n/a
7,182
0.2728 1,419
0.0607
BN 65
1,184
n/a
20,764
0.7539 12,569
0.5384
BN 67
1,031
n/a 179,067
6.9603
716
0.0304
Positive Random Bayesian Networks parameters (n=75, k=2, p=2, c=65)
r75-1
67,737
1.00 21,619
1.2503 21,619
1.2503
r75-2
46,703
1.00 18,083
0.9957 18,083
0.9957
r75-3
53,245
1.00 18,419
0.9955 18,419
0.9955
r75-4
28,507
1.00
8,363
0.5171 8,363
0.5171
r75-5
149,707
3.00 42,459
2.3299 42,459
2.3299
r75-6
132,107
3.00 62,621
3.4330 62,621
3.4330
r75-7
89,913
2.00 21,583
1.1942 21,583
1.1942
r75-8
86,183
2.00 49,001
2.8130 49,001
2.8130
r75-9
29,025
1.00
7,681
0.4124 7,681
0.4124
r75-10
20,291
1.00
5,905
0.3261 5,905
0.3261
Deterministic Random Bayesian Networks parameters (n=100, k=2, p=2, c=90)
r100d25-1
68,398
5.00 34,035
1.6290 34,035
1.7149
r100d25-2 150,134
10.00 70,241
3.6129 70,241
3.7810
r100d25-3 705,200
40.00 134,079
6.6372 134,079
6.9873
r100d25-4 161,902
22.00 79,366
3.8113 79,366
4.0079
r100d25-5 185,348
15.00 140,627
7.0839 140,627
7.4660
r100d25-6 148,835
37.00 204,232
9.1757 197,134
9.6542
r100d25-7 264,629
19.00 134,344
6.9619 133,850
6.9961
r100d25-8
65,186
21.00 36,857
1.6872 36,857
1.8278
r100d25-9 140,014
6.00 58,421
3.1058 58,172
3.2055
r100d25-10 173,808
27.00 69,110
3.5578 69,110
3.6636

Table 2: Memory usage MBytes ACE, AOMDD-BCP AOMDD-SAT 50 Bayesian
networks Table 1. Bold types highlight best performance across rows. n/a
indicates respective memory usage statistic available ACEs output.

507

fiM ATEESCU , ECHTER & ARINESCU

nally developed heuristic search (Nilsson, 1980). mentioned introduction, AND/OR
search graphical models based pseudo tree spans graph model, similar
tree rearrangement Freuder Quinn (1985, 1987). idea adapted distributed
constraint satisfaction Collin et al. (1991, 1999) recently Modi et al. (2005),
also shown related graph-based backjumping (Dechter, 1992). work extended
Bayardo Miranker (1996), Bayardo Schrag (1997) recently applied optimization tasks Larrosa et al. (2002). Another version viewed exploring AND/OR
graphs presented recently constraint satisfaction (Terrioux & Jegou, 2003b) optimization (Terrioux & Jegou, 2003a). Similar principles introduced recently probabilistic
inference, algorithm Recursive Conditioning (Darwiche, 2001) well Value Elimination
(Bacchus et al., 2003b, 2003a), currently core advanced SAT solvers (Sang
et al., 2004).
direction (2), various lines related research. formal verification literature,
beginning work Bryant (1986) contains large number papers dedicated
study BDDs. However, BDDs fact structures (the underlying pseudo tree chain)
take advantage problem decomposition explicit way. complexity bounds
OBDDs based pathwidth rather treewidth.
noted earlier, work Bertacco Damiani (1997) Disjoint Support Decomposition
(DSD) related AND/OR BDDs various ways. main common aspect approaches show structure decomposition exploited BDD-like representation. DSD
focused Boolean functions exploit refined structural information inherent Boolean functions. contrast, AND/OR BDDs assume structure conveyed
constraint graph, therefore broadly applicable constraint expression also
graphical models general. allow simpler higher level exposition yields graphbased bounds overall size generated AOMDD. full relationship two
formalisms studied further.
McMillan (1994) introduced BDD trees, along operations combining them.
2w
circuits bounded tree width, BDD trees linear space upper bound O(|g|2w2 ),
|g| size circuit g (typically linear number variables) w treewidth.
bound hides large constants claim linear dependence |g| w bounded.
However, McMillan maintains input function CNF expression BDD-trees
bounds AND/OR BDDs, namely exponential treewidth only.
sketch short comparison McMillans BDD trees AOMMDs, consider
example simple pseudo tree root , left child right child .
nodes may stand set variables. BDD trees, assignments grouped
equivalence classes according cofactors generated remaining .
example assignments 1 2 equivalent generate function .
node represented BDD whose leaves cofactors. done .
node represented matrix BDDs, column corresponds cofactor
line cofactor . contrast, AOMDD represents node BDD whose
leaves cofactors (the number distinct functions ) cofactor
root decomposition (an node) . Moreover, representations (as
descendants different cofactor ) shared much possible goes .
high level description, becomes slightly complicated redundant nodes
eliminated, idea remains same.
508

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

AND/OR structure restricted propositional theories similar deterministic decomposable negation normal form (d-DNNF) (Darwiche & Marquis, 2002; Darwiche, 2002).
recently, Huang Darwiche (2005b) used trace DPLL algorithm generate OBDD,
compared typical formal verification approach combining OBDDs input
function according schedule. structures investigated case still OR.
idea extended present work AND/OR search compilation algorithm.
McAllester, Collins, Pereira (2004) introduced case factor diagrams (CFD), subsume Markov random fields bounded tree width probabilistic context free grammars (PCFG).
CFDs much related AND/OR graphs. CFDs target minimal representation,
exploiting decomposition (similar nodes) also exploiting context sensitive information allowing dynamic ordering variables based context. CFDs eliminate
redundant nodes, part cause use zero suppression. claim
CFDs canonical forms, also description combine two CFDs.
numerous variants decision diagrams designed represent integer-valued
real-valued functions. comprehensive view refer reader survey Drechsler
Sieling (2001). Algebraic decision diagrams (ADDs) (Bahar et al., 1993) provide compilation general real-valued rather Boolean functions. main drawback
size increases fast number terminals becomes large. several approaches
try alleviate problem. However structure capture still OR,
exploit decomposition. alternatives introduce edge values (or weights) enable
subgraph sharing. Edge-valued binary decision diagrams (EVBDDs) (Lai & Sastry, 1992)
use additive weights, multiplicative weights also allowed called factored
EVBDDs (FEVBDDs) (Tafertshofer & Pedram, 1997). Another type BDDs called K*BMDs
(Drechsler, Becker, & Ruppertz, 1996) also use integer weights, additive multiplicative
parallel. ADDs also extended affine ADDs (Sanner & McAllester, 2005),
affine transformations achieve compression. result shown beneficial
probabilistic inference algorithms, tree clustering, still exploit
structure.
recently, independently parallel work AND/OR graphs (Dechter & Mateescu, 2004a, 2004b), Fargier Vilarem (2004) Fargier Marquis (2006, 2007) proposed compilation CSPs tree-driven automata, many similarities work.
main focus transition linear automata tree automata (similar
AND/OR), possible savings tree-structured networks hyper-trees constraints
due decomposition. compilation approach guided tree-decomposition
guided variable-elimination based algorithms. well known Bucket Elimination
cluster-tree decomposition principle (Dechter & Pearl, 1989).
Wilson (2005) extended OBDDs semi-ring BDDs. semi-ring treatment restricted
search spaces, allows dynamic variable ordering. otherwise similar aim
scope AOMDD. restricting AOMDD graphs only, two closely related,
except express BDDs using Shenoy-Shafer axiomatization centered two
operation combination marginalization rather semi-ring formulation. Minimality
formulation Wilson (2005) general allowing merging nodes different values
therefore capture symmetries (called interchangeability).
Another framework similar AOMDDs, became aware recently, Probabilistic Decision Graphs (PDG) Jaeger (2004). work preceded relevant work
509

fiM ATEESCU , ECHTER & ARINESCU

discussed (Fargier & Vilarem, 2004; Wilson, 2005) went somewhat unnoticed, perhaps due notational cultural differences. however similar motivation, framework
proposed algorithms. believe AND/OR framework accessible. define framework multi-valued domains, provide greater details algorithms complexity analysis,
make explicit connection search frameworks, fully address issues canonicity well
provide empirical demonstration. particular, claim canonicity PDGs similar
one make AOMDDs weighted models, relative trees (or forests)
represent given probability distribution.
another line research Drechsler group (e.g. Zuzek, Drechsler, & Thornton,
2000), use AND/OR graphs Boolean function representation, may seem similar
approach. However, semantics purpose AND/OR graphs different.
constructed based technique recursive learning used perform Boolean reasoning,
i.e. explore logic consequences given assumption based structure circuit,
especially derive sets implicants. meaning case related
meaning gates/functions, case meaning related semantic
functions. AND/OR enumeration tree results circuit according Zuzek et al.
(2000) related AND/OR decomposition discuss.

12. Conclusion
propose AND/OR multi-valued decision diagram (AOMDD), emerges study
AND/OR search spaces graphical models (Dechter & Mateescu, 2004a, 2004b; Mateescu &
Dechter, 2005; Dechter & Mateescu, 2007) ordered binary decision diagrams (OBDDs) (Bryant,
1986). data-structure used compile graphical model.
Graphical models algorithms search-based compiled data-structures BDDs
differ primarily choices time vs. memory. move regular search
space AND/OR search space spectrum algorithms available improved time
vs. memory decisions. believe AND/OR search space clarifies available choices
helps guide user making informed selection algorithm would fit best
particular query asked, specific input function available computational resources.
contribution work is: (1) formally describe AOMDD prove
canonical representation constraint network. (2) extend AOMDD general weighted
graphical models. (3) give compilation algorithm based AND/OR search, saves
trace memory intensive search (the context minimal AND/OR graph), reduces
one bottom pass. (4) describe APPLY operator combines two AOMDDs
operation show complexity quadratic input, never worse exponential
treewidth. (5) give scheduling order building AOMDD graphical model
starting AOMDDs functions based Variable Elimination algorithm.
guarantees complexity exponential induced width (treewidth) along
ordering. (6) show AOMDDs relate various earlier recent compilation frameworks,
providing unifying perspective methods. (7) introduce semantic treewidth,
helps explain compiled decision diagrams often much smaller worst case
bound. Finally, (8) provide preliminary empirical demonstration power current
scheme.

510

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

Acknowledgments
work done Robert Mateescu Radu Marinescu University California, Irvine. authors would like thank anonymous reviewers constructive
suggestions improve paper, David Eppstein useful discussion complexity issues,
Lars Otten Natasha Flerova comments final version manuscript. work
supported NSF grants IIS-0412854 IIS-0713118, initial part Radcliffe fellowship 2005-2006 (through partner program), Harvard undergraduate student John Cobb.

Appendix
Proof Proposition 1
Consider level variable Xi , meta-nodes list LXi . one pass
meta-nodes LXi (the inner loop), two meta-nodes level Xi
AND/OR graph isomorphic, would merged line 6. Also,
pass meta-nodes LXi redundant meta-nodes LXi eliminated
line 8. Processing meta-nodes level Xi create new redundant isomorphic
meta-nodes levels processed before. follows resulting AND/OR
graph completely reduced. 2
Proof Theorem 4
bound size follows directly Theorem 3. AOMDD size smaller

size context minimal AND/OR graph, bounded O(n k wT (G) ). prove
time bound, rely use hash table, assumption efficient implementation allows access time constant. time bound AND/OR-S EARCH -AOMDD

O(n k wT (G) ), Theorem 3, takes time linear output (we assume
constraint propagation performed search). Procedure B OTTOM U P R EDUCTION (procedure 1) takes time linear size context minimal AND/OR graph. Therefore, AOMDD

computed time O(n k wT (G) ), result algorithm performs
reduction search. 2
Proof Proposition 2
complexity OBDD (and MDD) apply known quadratic input. Namely,
number nodes output product number nodes input. Therefore,
number nodes appear along one path output AOMDD product
number nodes input, along path, |Gfi | |Ggi |. Summing paths
gives result. 2
Proof Proposition 3
argument identical case MDDs. recursive calls APPLY lead combinations
one node Gfaomdd one node Ggaomdd (rather list nodes). number
total possible combinations O(| Gfaomdd | | Ggaomdd |). 2
Proof Proposition 4
recursive calls APPLY generate one meta-node output combination
511

fiM ATEESCU , ECHTER & ARINESCU

nodes Gff g Ggf g . Lets look combinations nodes Gff g Ggaomdd \ Ggf g .
meta-nodes Ggaomdd \ Ggf g participate combinations (lets call set A)
levels (of variables) right Tf g . mechanics
recursive calls APPLY. Whenever node f belongs Gff g combined node
g belongs A, line 15 APPLY expands node f , node (or nodes)
remain same. happen nodes f combined
node (or nodes) A, point APPLY simply copy remaining portion
output Ggaomdd . size therefore proportional | Ggf g | (because layer
metanodes immediately Ggf g ). similar argument valid symmetrical case.
combinations nodes Ggaomdd \ Ggf g Ggaomdd \ Ggf g . bound follows
arguments. 2
Proof Proposition 5
APPLY operation works constructing output AOMDD root leaves. first creates
meta-node root variable, recursively creates children metanodes using APPLY
corresponding children input. worst case happen output
reduced all, recursive call made possible descendant. corresponds
unfolding full AND/OR search tree based context variables, exponential
context size. APPLY finishes context variables, arrives first branching
bucket pseudo tree, remaining branches independent. Similar case OBDDs,
one function occupies single place memory, APPLY simply create link
corresponding branches inputs (this happens line 4 APPLY algorithm).
Therefore, time space complexity exponential context size. 2
Proof Theorem 5
space complexity governed BE. Since AOMDD never requires space

full exponential table (or tree), follows BE-AOMDD needs space O(n k w ).
size output AOMDD also bounded, per layers, number assignments
context layer (namely, size context minimal AND/OR graph). Therefore,

context size bounded treewidth, follows output size O(n k w ).
time complexity follows Proposition 5, fact number functions
bucket cannot exceed r, original number functions.
2
Proof Proposition 6
suffices prove proposition = 2. general result obtained induction.
essential function defined constraint network (i.e., values 0 1),
function takes value 1 least one assignment. value 1 denotes consistent assignments (solutions), 0 denotes inconsistent assignments. Suppose f = f1 f2 . Lets denote
x full assignment X, x1 x2 projection x X1 X2 , respectively.
write x = x1 x2 (concatenation partial assignments). follows f (x) = f1 (x1 ) f2 (x2 ).
Therefore, f (x) = 1, must f1 (x1 ) = 1 f2 (x2 ) = 1. claim x1 ,
f1 (x1 ) = 1 exists x2 f (x1 x2 ) = 1. Suppose contradiction
exist x1 f1 (x1 ) = 1 f (x1 x2 ) = 0 x2 . Since f always zero,

512

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

follows f2 always zero, therefore must x2 f2 (x2 ) = 1.
leads contradiction, therefore functions f1 f2 uniquely defined f . 2
Proof Theorem 6
proof structural induction depth pseudo tree . follows canonicity
proofs OBDDs (Bryant, 1986) MDDs (Srinivasan et al., 1990), extends linear
orderings tree orderings capture function decomposition according pseudo tree .
depth , along paths root leaf, actually size dependency set,
set variables value function depends. Remember AOMDD
AND/OR graph completely reduced. use word function, denoted f , refer
universal relation, characteristic function, defined constraint network.
Assume depth 0. means function depend variable,
must one constants 0 1. Suppose function constant 0. Then, must
AOMDD contain terminal meta-node 1, since nodes must reachable
along path, would mean function also evaluate 1. Suppose AOMDD
contains nonterminal meta-node, say labeled X, X take k different values. must
k children meta-nodes X terminal meta-node 0. one
terminal 0, AOMDD completely reduced. one 0, follows
meta-node labeled X redundant. Therefore, above, follows AOMDD
representing constant 0 made terminal 0. unique, contains smallest
number nodes. similar argument applies constant 1.
Now, suppose statement theorem holds constraint network admits
pseudo tree depth strictly smaller p, constraint network pseudo
tree depth equal p, p > 0. Let X root , domain {x1 , . . . , xk }.
denote fi , {1, . . . , k}, functions defined restricted constraint network
X = xi , namely fi = f |X=xi . Let Y1 , . . . , Ym children X . Suppose two
AOMDDs f , denoted G G 0 . show two AND/OR graphs isomorphic.
functions fi decomposed according pseudo tree root X removed.
fact forest independent pseudo trees (they share variables), rooted
Y1 , . . . , Ym . Based Proposition 6, unique decomposition fi = fiY1 . . . fiYm ,

{1, . . . , k}. Based induction hypothesis, function fi j unique AOMDD.
AND/OR graphs G G 0 , look subgraphs descending X = xi ,
completely reduced define function, fi , therefore exists isomorphic mapping
them. Let v root metanode G v 0 root G 0 . claim G G 0
isomorphic according following mapping:
0
v,
u = v;
(u) =
(u), u subgraph rooted hX, xi i.
prove this, show well defined, isomorphic mapping.
meta-node u G contained subgraphs rooted hX, xi hX, xj i,
AND/OR graphs rooted (u) j (u) isomorphic one rooted u, therefore
other. Since G 0 completely reduced, contain isomorphic subgraphs, therefore
(u) = j (u). Therefore well defined.
show bijection. show one-to-one, assume two distinct metanodes u1 u2 G, (u1 ) = (u2 ). Then, subgraphs rooted u1 u2 isomorphic
513

fiM ATEESCU , ECHTER & ARINESCU

subgraph rooted (u1 ), therefore other. Since G completely reduced, must
u1 = u2 . fact onto isomorphic mapping follows definition
fact onto new node root meta-node. Since AOMDDs
contain one root meta-node (more one root would lead conclusion root
meta-nodes isomorphic merged), conclude G G 0 isomorphic.
Finally, show among AND/OR graphs representing f , AOMDD
minimal number meta-nodes. Suppose G AND/OR graph represents f , minimal
number meta-nodes, without AOMDD. Namely, completely reduced.
reduction rule would transform G AND/OR graph smaller number meta-nodes,
leading contradiction. Therefore, G must unique AOMDD represents f . 2
Proof Corollary 1
proof Theorem 6 rely scopes define constraint network. long
network admits decomposition induced pseudo tree , universal function defined
constraint network always AOMDD, therefore constraint network
equivalent admits also AOMDD. 2
Proof Theorem 7
constant associated root actually sum weights solutions.
derived definition weighted AOMDD. weights meta-node
normalized (they sum 1), therefore values computed node AND/OR search
always 1 (when task computing
P sum solution weights). Therefore, constant
weighted AOMDD always x w(x) regardless graphical model. prove
weighted AOMDDs canonical functions normalized.
Assume two different weighted AOMDDs, denoted G 1 G 2 , normalized function f . Let root variable A, domain {a1 , . . . , ak }. Let x denote full
assignment variables. Similar argument root constant,
P
meta-nodes normalized weights, follows w1 (A, a1 ) = w2 (A, a1 ) = x|A=a1 f (x).
superscript w1 w2 indicates AOMDD, summation possible assignments restricted = a1 . follows root meta-nodes identical. value
root variable, restricted functions represented G 1 G 2 identical, recursively
apply argument above.
However, proof complete, discuss case restricted function
decomposed independent functions, according pseudo tree. Suppose two
independent components, rooted B C. one 0 function, follows
entire function 0. prove meta-nodes B G 1 G 2 identical. B
one value b1 extendable solution, weight must 1 meta-nodes, meta-nodes
identical. B one value, suppose without loss generality weights
different first value b1 ,
w1 (B, b1 ) > w2 (B, b1 ).

(1)

Since f 6= 0, must value C = c1 B = b1 , C = c1 extended full
solution. sum weights possible extensions
X
f (x) = w1 (B, b1 ) w1 (C, c1 ) = w2 (B, b1 ) w2 (C, c1 ).
(2)
x|B=b1 ,C=c1

514

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

Equations 1 2 fact weight non-zero, follows
w1 (C, c1 ) < w2 (C, c1 ).

(3)

Equation 1, fact B one value fact weights B
normalized, follows value b2
w1 (B, b2 ) < w2 (B, b2 ).

(4)

Equations 3 4, follows
w1 (B, b2 ) w1 (C, c1 ) < w2 (B, b2 ) w2 (C, c1 ).

(5)

However, sides
P Equation 5 represent sum weights solutions B =
b2 , C = c1 , namely x|B=b2 ,C=c1 f (x), leading contradiction. Therefore, must
Equation 1 false. Continuing argument values B, follows metanodes B identical, similarly meta-nodes C identical.
decomposition two components, argument applies, B
first component C meta-variable combines components.
2
Proof Theorem 8
Consider well known NP-complete problem 3- COLORING: Given graph G,
3-coloring G? Namely, color vertices using three colors, two
adjacent vertices different colors? reduce 3- COLORING problem computing
semantic treewidth graphical model. Let H graph 3-colorable, nontrivial semantic treewidth. easy build examples H. G 3-colorable, G H
also 3-colorable non-trivial semantic treewidth, adding G simplify
task describing colorings H. However, G 3-colorable, G H also
3-colorable, semantic treewidth zero. 2
Proof Proposition 7
Since AOMDDs canonical representations graphical models, follows graphical
model actual semantic treewidth realized AOMDD M,
therefore AOMDD bounded exponentially semantic treewidth. 2

References
Akers, S. (1978). Binary decision diagrams. IEEE Transactions Computers, C-27(6), 509516.
Allen, D., & Darwiche, A. (2003). New advances inference recursive conditioning. Proceedings Nineteenth Conference Uncertainty Artificial Intelligence (UAI03), pp.
210.
Bacchus, F., Dalmao, S., & Pitassi, T. (2003a). Algorithms complexity results #SAT
Bayesian inference. Proceedings 44th Annual IEEE Symposium Foundations
Computer Science (FOCS03), pp. 340351.
515

fiM ATEESCU , ECHTER & ARINESCU

Bacchus, F., Dalmao, S., & Pitassi, T. (2003b). Value elimination: Bayesian inference via backtracking search. Proceedings Nineteenth Conference Uncertainty Artificial
Intelligence (UAI03), pp. 2028.
Bahar, R., Frohm, E., Gaona, C., Hachtel, G., Macii, E., Pardo, A., & Somenzi, F. (1993). Algebraic decision diagrams applications. IEEE/ACM International Conference
Computer-Aided Design (ICCAD93), pp. 188191.
Bayardo, R., & Miranker, D. (1996). complexity analysis space-bound learning algorithms
constraint satisfaction problem. Proceedings Thirteenth National Conference
Artificial Intelligence (AAAI96), pp. 298304.
Bayardo, R. J., & Schrag, R. C. (1997). Using CSP look-back techniques solve real world SAT
instances. Proceedings Fourteenth National Conference Artificial Intelligence
(AAAI97), pp. 203208.
Bertacco, V., & Damiani, M. (1997). disjunctive decomposition logic functions.
IEEE/ACM International Conference Computer-Aided Design (ICCAD97), pp. 7882.
Bodlaender, H. L., & Gilbert, J. R. (1991). Approximating treewidth, pathwidth minimum
elimination tree height. Tech. rep., Utrecht University.
Bryant, R. E. (1986). Graph-based algorithms Boolean function manipulation. IEEE Transactions Computers, 35, 677691.
Cadoli, M., & Donini, F. M. (1997). survey knowledge compilation. AI Communications,
10(3-4), 137150.
Chavira, M., & Darwiche, A. (2005). Compiling Bayesian networks local structure.
Proceedings Nineteenth International Joint Conference Artificial Intelligence (IJCAI05), pp. 13061312.
Chavira, M., & Darwiche, A. (2007). Compiling Bayesian networks using variable elimination.
Proceedings Twentieth International Joint Conference Artificial Intelligence (IJCAI07), pp. 24432449.
Chavira, M., Darwiche, A., & Jaeger, M. (2006). Compiling relational Bayesian networks exact
inference. International Journal Approximate Reasoning, 42(1-2), 420.
Clarke, E., Grumberg, O., & Peled, D. (1999). Model Checking. MIT Press.
Collin, Z., Dechter, R., & Katz, S. (1991). feasibility distributed constraint satisfaction.
Proceedings Twelfth International Conference Artificial Intelligence (IJCAI91),
pp. 318324.
Collin, Z., Dechter, R., & Katz, S. (1999). Self-stabilizing distributed constraint satisfaction.
Chicago Journal Theoretical Computer Science, 115, special issue self-stabilization.
Darwiche, A. (2001). Recursive conditioning. Artificial Intelligence, 125(1-2), 541.

516

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

Darwiche, A. (2002). logical approach factoring belief networks. Proceedings
Eighth International Conference Principles Knowledge Representation Reasoning
(KR02), pp. 409420.
Darwiche, A., & Marquis, P. (2002). knowledge compilation map. Journal Artificial Intelligence Research (JAIR), 17, 229264.
Dechter, R. (1992). Constraint networks. Encyclopedia Artificial Intelligence, 276285.
Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial Intelligence,
113, 4185.
Dechter, R., & Mateescu, R. (2007). AND/OR search spaces graphical models. Artificial Intelligence, 171(2-3), 73106.
Dechter, R., & Mateescu, R. (2004a). Mixtures deterministic-probabilistic networks
AND/OR search space. Proceedings Twentieth Conference Uncertainty Artificial Intelligence (UAI04), pp. 120129.
Dechter, R., & Mateescu, R. (2004b). impact AND/OR search spaces constraint satisfaction counting. Proceedings Tenth International Conference Principles
Practice Constraint Programming (CP04), pp. 731736.
Dechter, R., & Pearl, J. (1989). Tree clustering constraint networks. Artificial Intelligence, 38,
353366.
Drechsler, R., Becker, B., & Ruppertz, S. (1996). K*BMDs: new data structure verification.
Proceedings 1996 European conference Design Test (ED&TC96), pp. 28.
Drechsler, R., & Sieling, D. (2001). Binary decision diagrams theory practice. International
Journal Software Tools Technology Transfer (STTT), 3(2), 112136.
Fargier, H., & Marquis, P. (2006). use partially ordered decision graphs knowledge
compilation quantified boolean formulae. Proceedings Twenty-First National
Conference Artificial Intelligence (AAAI06), pp. 4247.
Fargier, H., & Marquis, P. (2007). valued negation normal form formulas. Proceedings
Twentieth International Joint Conference Artificial Intelligence (IJCAI07), pp. 360365.
Fargier, H., & Vilarem, M. (2004). Compiling CSPs tree-driven automata interactive solving. Constraints, 9(4), 263287.
Fishburn, P. C. (1970). Utility Theory Decision Making. Wiley, NewYork.
Freuder, E. C., & Quinn, M. J. (1985). Taking advantage stable sets variables constraint
satisfaction problems. Proceedings Ninth International Joint Conference Artificial
Intelligence (IJCAI85), pp. 10761078.
Freuder, E. C., & Quinn, M. J. (1987). use lineal spanning trees represent constraint
satisfaction problems. Tech. rep. 87-41, University New Hampshire, Durham.
517

fiM ATEESCU , ECHTER & ARINESCU

Huang, J., & Darwiche, A. (2005a). compiling system models faster scalable diagnosis. Proceedings 20th National Conference Artificial Intelligence (AAAI05),
pp. 300306.
Huang, J., & Darwiche, A. (2005b). DPLL trace: SAT knowledge compilation.
Proceedings Nineteenth International Joint Conference Artificial Intelligence
(IJCAI05), pp. 156162.
Jaeger, M. (2004). Probabilistic decision graphs - combining verification AI techniques
probabilistic inference. International Journal Uncertainty, Fuzziness KnowledgeBased Systems, 12, 1942.
Kask, K., Dechter, R., Larrosa, J., & Dechter, A. (2005). Unifying cluster-tree decompositions
reasoning graphical models. Artificial Intelligence, 166 (1-2), 165193.
Kjaerulff, U. (1990). Triangulation graph-based algorithms giving small total state space. Tech.
rep., University Aalborg, Denmark.
Korf, R., & Felner, A. (2002). Disjoint pattern database heuristics. Artificial Intelligence, 134(1-2),
922.
Lai, Y.-T., & Sastry, S. (1992). Edge-valued binary decision multi-level hierarchical verification.
Proceedings Twenty Nineth Design Automation Conference, pp. 608613.
Larrosa, J., Meseguer, P., & Sanchez, M. (2002). Pseudo-tree search soft constraints.
Proceedings European Conference Artificial Intelligence (ECAI02), pp. 131135.
Lee, C. (1959). Representation switching circuits binary-decision programs. Bell System
Technical Journal, 38, 985999.
Mateescu, R., & Dechter, R. (2005). relationship AND/OR search variable elimination. Proceedings Twenty First Conference Uncertainty Artificial Intelligence
(UAI05), pp. 380387.
Mateescu, R., & Dechter, R. (2007). AND/OR multi-valued decision diagrams (AOMDDs)
weighted graphical models. Proceedings Twenty Third Conference Uncertainty
Artificial Intelligence (UAI07), pp. 276284.
McAllester, D., Collins, M., & Pereira, F. (2004). Case-factor diagrams structured probabilistic
modeling. Proceedings Twentieth Conference Uncertainty Artificial Intelligence (UAI04), pp. 382391.
McMillan, K. L. (1993). Symbolic Model Checking. Kluwer Academic.
McMillan, K. L. (1994). Hierarchical representation discrete functions application model
checking. Computer Aided Verification, pp. 4154.
Modi, P. J., Shen, W., Tambe, M., & Yokoo, M. (2005). ADOPT: asynchronous distributed constraint optimization quality guarantees. Artificial Intelligence, 161, 149180.

518

fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS

Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering
efficient SAT solver. Proceedings Thirty Eighth Design Automation Conference, pp.
530535.
Nilsson, N. J. (1980). Principles Artificial Intelligence. Tioga, Palo Alto, CA.
Palacios, H., Bonet, B., Darwiche, A., & Geffner, H. (2005). Pruning conformant plans counting models compiled d-DNNF representations. Proceedings 15th International
Conference Planning Scheduling (ICAPS05), pp. 141150.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.
Sang, T., Bacchus, F., Beame, P., Kautz, H., & Pitassi, T. (2004). Combining component caching
clause learning effective model counting. Proceedings Seventh International
Conference Theory Applications Satisfiability Testing (SAT04).
Sanner, S., & McAllester, D. (2005). Affine algebraic decision diagrams (AADDs) application structured probabilistic inference. Proceedings Nineteenth International
Joint Conference Artificial Intelligence (IJCAI05), pp. 13841390.
Selman, B., & Kautz, H. (1996). Knowledge compilation theory approximation. Journal
ACM, 43(2), 193224.
Shenoy, P. (1992). Valuation-based systems Bayesian decision analysis. Operations Research,
40, 463484.
Srinivasan, A., Kam, T., Malik, S., & Brayton, R. K. (1990). Algorithms discrete function
manipulation. International conference CAD, pp. 9295.
Tafertshofer, P., & Pedram, M. (1997). Factored edge-valued binary decision diagrams. Formal
Methods System Design, 10(2-3), 243270.
Terrioux, C., & Jegou, P. (2003a). Bounded backtracking valued constraint satisfaction
problems. Proceedings Ninth International Conference Principles Practice
Constraint Programming (CP03), pp. 709723.
Terrioux, C., & Jegou, P. (2003b). Hybrid backtracking bounded tree-decomposition constraint networks. Artificial Intelligence, 146, 4375.
Wilson, N. (2005). Decision diagrams computation semiring valuations. Proceedings
Nineteenth International Joint Conference Artificial Intelligence (IJCAI05), pp.
331336.
Zuzek, A., Drechsler, R., & Thornton, M. (2000). Boolean function representation spectral
characterization using AND/OR graphs. Integration, VLSI journal, 29, 101116.

519

fiJournal Artificial Intelligence Research 33 (2008) 1-31

Submitted 05/08; published 09/08

Anytime Induction Low-cost, Low-error Classifiers:
Sampling-based Approach
Saher Esmeir
Shaul Markovitch

esaher@cs.technion.ac.il
shaulm@cs.technion.ac.il

Computer Science Department
TechnionIsrael Institute Technology
Haifa 32000, Israel

Abstract
Machine learning techniques gaining prevalence production wide range
classifiers complex real-world applications nonuniform testing misclassification
costs. increasing complexity applications poses real challenge resource
management learning classification. work introduce ACT (anytime
cost-sensitive tree learner), novel framework operating complex environments.
ACT anytime algorithm allows learning time increased return lower
classification costs. builds tree top-down exploits additional time resources
obtain better estimations utility different candidate splits. Using sampling
techniques, ACT approximates cost subtree candidate split favors
one minimal cost. stochastic algorithm, ACT expected able
escape local minima, greedy methods may trapped. Experiments
variety datasets conducted compare ACT state-of-the-art cost-sensitive
tree learners. results show majority domains ACT produces significantly
less costly trees. ACT also exhibits good anytime behavior diminishing returns.

1. Introduction
Traditionally, machine learning algorithms focused induction models
low expected error. many real-word applications, however, several additional constraints
considered. Assume, example, medical center decided use machine learning techniques build diagnostic tool heart disease. comprehensibility
decision tree models (Hastie, Tibshirani, & Friedman, 2001, chap. 9) makes preferred choice base tool. Figure 1 shows three possible trees. first tree
(upper-left) makes decisions using results cardiac catheterization (heart cath).
tree expected highly accurate. Nevertheless, high costs risks associated heart cath procedure make decision tree impractical. second tree
(lower-left) dispenses need cardiac catheterization reaches decision based
single, simple, inexpensive test: whether patient complains chest pain.
tree would highly accurate: people experience chest pain
indeed healthy. tree, however, distinguish costs different types
errors. false positive prediction might result extra treatments, false negative
prediction might put persons life risk. Therefore, third tree (right) preferred, one
attempts minimize test costs misclassification costs simultaneously.
c
2008
AI Access Foundation. rights reserved.

fiEsmeir & Markovitch

heart cath


normal

chest pain


yes

yes

blood
pressure

alerting



blood
pressure


yes

cardiac
stress

normal

normal



normal

normal

heart cath

yes





yes

yes

alerting

yes

normal

alerting

normal

heart cath

chest pain


yes

alerting

Figure 1: Three possible decision trees diagnosis heart diseases. upper-left tree
bases decision solely heart cath therefore accurate prohibitively
expensive. lower-left tree dispenses need heart cath reaches
decision using single, simple, inexpensive test: whether patient
complains chest pain. tree would highly accurate
distinguish costs different error types. third (right-hand)
tree preferable: attempts minimize test costs misclassification costs
simultaneously.

cost(a1-10) = $$

a9
a10
0

1

a7

a10
1

a6

a9

0

cost(a1-8) = $$
cost(a9,10) = $$$$$$

a1

0

a9
1

1

a4
0

0

a4
1

1

0

Figure 2: Left: example difficulty greedy learners might face. Right: example
importance context-based feature evaluation.

Finding tree lowest expected total cost least NP-complete.1
cost insensitive case, greedy heuristic used bias search towards low-cost trees.
Decision Trees Minimal Cost (DTMC), greedy method attempts minimize
1. Finding smallest consistent tree, easier problem, NP-complete (Hyafil & Rivest, 1976).

2

fiAnytime Induction Low-cost, Low-error Classifiers

types costs simultaneously, recently introduced (Ling, Yang, Wang, &
Zhang, 2004; Sheng, Ling, Ni, & Zhang, 2006). tree built top-down, greedy split
criterion takes account testing misclassification costs used. basic
idea estimate immediate reduction total cost split, prefer
split maximal reduction. split reduces cost training data,
induction process stopped.
Although efficient, DTMC approach trapped local minimum
produce trees globally optimal. example, consider concept costs
described Figure 2 (left). 10 attributes, a9 a10 relevant.
cost a9 a10 , however, significantly higher others. high costs may
hide usefulness a9 a10 , mislead learner repeatedly splitting a18 ,
would result large, expensive tree. problem would intensified a9
a10 interdependent, low immediate information gain (e.g., a9 a10 ).
case, even costs uniform, local measure might fail recognize relevance
a9 a10 .
DTMC appealing learning resources limited. However, requires
fixed runtime cannot exploit additional resources escape local minima. many
real-life applications, willing wait longer better tree induced (Esmeir &
Markovitch, 2006). example, importance model saving patients lives may
convince medical center allocate 1 month learn it. Algorithms exploit
additional time produce better solutions called anytime algorithms (Boddy & Dean,
1994).
ICET algorithm (Turney, 1995) pioneer non-greedy search tree
minimizes test misclassification costs. ICET uses genetic search produce new
set costs reflects original costs contribution attribute
reducing misclassification costs. builds tree using EG2 algorithm (Nunez,
1991) evolved costs instead original ones. EG2 greedy cost-sensitive
algorithm builds tree top-down evaluates candidate splits considering
information gain yield measurement costs. not, however, take
account misclassification cost problem.
ICET shown significantly outperform greedy tree learners, producing trees
lower total cost. ICET use additional time resources produce generations
hence widen search space costs. genetic operations randomized,
ICET likely escape local minima EG2 original costs might
trapped. Nevertheless, two shortcomings limit ICETs ability benefit extra time.
First, search phase, uses greedy EG2 algorithm build final tree.
EG2 prefers attributes high information gain (and low test cost),
usefulness highly relevant attributes may underestimated greedy measure
case hard-to-learn concepts attribute interdependency hidden. result
expensive trees. Second, even ICET overcomes problem randomly
reweighting attributes, searches space parameters globally, regardless
context tree. imposes problem attribute important one subtree
useless another. better understand shortcomings, consider concept described
tree Figure 2 (right). 10 attributes similar costs. value a1
determines whether target concept a7 a9 a4 a6 . interdependencies result
3

fiEsmeir & Markovitch

low gain attributes. ICET assigns costs globally, attributes
similar costs well. Therefore, ICET able recognize one relevant
context. irrelevant attributes cheaper, problem intensified
model might end relying irrelevant attributes.
Recently, introduced cost-insensitive LSID3 algorithm, induce
accurate trees allocated time (Esmeir & Markovitch, 2007a). algorithm evaluates candidate split estimating size smallest consistent tree
it. estimation based sampling space consistent trees, size
sample determined advance according allocated time. LSID3 designed,
however, minimize test misclassification costs. work build LSID3
propose ACT, anytime cost-sensitive tree learner exploit additional time
produce lower-cost trees. Applying sampling mechanism cost-sensitive setup,
however, trivial imposes three major challenges: (1) produce sample,
(2) evaluate sampled trees, (3) prune induced trees. Section
3 show obstacles may overcome.
Section 4 report extensive set experiments compares ACT several
decision tree learners using variety datasets costs assigned human experts
automatically. results show ACT significantly better majority
problems. addition, ACT shown exhibit good anytime behavior diminishing
returns.

2. Cost-Sensitive Classification
Offline concept learning consists two stages: learning stage, set labeled
examples used induce classifier; classification stage, induced
classifier used classify unlabeled instances. two stages involve different types
costs (Turney, 2000). primary goal work trade learning speed
reduction test misclassification costs. make problem well defined, need
specify: (1) misclassification costs represented, (2) test costs calculated,
(3) combine types cost.
answer questions, adopt model described Turney (1995). problem
|C| different classes, misclassification cost matrix |C| |C| matrix whose Mi,j
entry defines penalty assigning class ci instance actually belongs
class cj . Typically, entries main diagonal classification cost matrix (no error)
zero.
classifying example e using tree , propagate e tree along
single path root one leaves. Let (T, e) set tests along
path. denote cost() cost administering test . testing cost e
P
therefore tcost(T, e) = cost(). Note use sets notation tests
appear several times charged once. addition, model described Turney
(1995) handles two special test types, namely grouped delayed tests.
Grouped Tests. tests share common cost, would like charge
once. Typically, test also extra (possibly different) cost. example, consider
tree path tests like cholesterol level glucose level. values measured,
blood test needed. Taking blood samples measure cholesterol level clearly lowers
4

fiAnytime Induction Low-cost, Low-error Classifiers

cost measuring glucose level. Formally, test possibly belongs group.2
first test group administered, charge full cost. another
test group already administered earlier decision path,
charge marginal cost.
Delayed Tests. Sometimes outcome test cannot obtained immediately, e.g.,
lab test results. tests, called delayed tests, force us wait outcome
available. Alternatively, Turney (1995) suggests taking account possible outcomes:
delayed test encountered, tests subtree administered
charged for. result delayed test available, prediction hand.
One problem setup follows paths subtree, regardless
outcome non-delayed costs. Moreover, possible distinguish delays
different tests impose: example, one result might ready several minutes
another days. work handle delayed tests,
explain ACT modified take account.
test misclassification costs measured, important question
remains: combine them? Following Turney (1995), assume
cost types given scale. general model would require utility function
combines types. Qin, Zhang, Zhang (2004) presented method handle
two kinds cost scales setting maximal budget one kind cost minimizing
one. Alternatively, patient preferences elicited summarized utility
function (Lenert & Soetikno, 1997).
Note algorithm introduce paper adapted cost model.
important property cost-sensitive setup maximizing generalization accuracy,
goal existing learners, viewed special case: accuracy
objective, test costs ignored misclassification cost uniform.

3. ACT Algorithm
ACT, proposed anytime framework induction cost-sensitive decision trees, builds
recently introduced LSID3 algorithm. LSID3 adopts general top-down scheme
induction decision trees (TDIDT): starts entire set training examples,
partitions subsets testing value attribute, recursively builds
subtrees. Unlike greedy inducers, LSID3 invests time resources making better split
decisions. every candidate split, LSID3 attempts estimate size resulting
subtree split take place. Following Occams razor (Blumer, Ehrenfeucht, Haussler, & Warmuth, 1987; Esmeir & Markovitch, 2007b), favors one smallest
expected size.
estimation based biased sample space trees rooted evaluated
attribute. sample obtained using stochastic version ID3 (Quinlan, 1986),
call SID3. SID3, rather choosing attribute maximizes information
gain (as ID3), choose splitting attribute semi-randomly. likelihood
attribute chosen proportional information gain. Due randomization,
2. model test may belong single group. However, easy extend work allow
tests belong several groups.

5

fiEsmeir & Markovitch

Procedure LSID3-Choose-Attribute(E, A, r)
r = 0
Return ID3-Choose-Attribute(E, A)
Foreach
Foreach vi domain(a)
Ei {e E | a(e) = vi }
mini
Repeat r times
SID3(Ei , {a})
mini min (mini , Size(T ))
P|domain(a)|
totala i=1
mini
Return totala minimal
Figure 3: Attribute selection LSID3
repeated invocations SID3 result different trees. candidate attribute a, LSID3
invokes SID3 r times form sample r trees rooted a, uses size smallest
tree sample evaluate a. Obviously, r larger, resulting size estimations
expected accurate, improving final tree. Consider, example, 3-XOR
concept several additional irrelevant attributes. LSID3 prefer one relevant
attributes root, one trees samples relevant attributes must
smallest. probability event increases increase sample size.
LSID3 contract anytime algorithm parameterized r, sample size. Additional
time resources utilized forming larger samples. Figure 3 lists procedure
attribute selection applied LSID3. Let = |E| number examples
n = |A| number attributes. runtime complexity LSID3 O(rmn3 ). LSID3
shown exhibit good anytime behavior diminishing returns. applied
hard concepts, produced significantly better trees ID3 C4.5.
ACT takes sampling approach LSID3. three major components
LSID3 need replaced order adapt cost-sensitive problems are: (1)
sampling space trees, (2) evaluating tree, (3) pruning tree.
3.1 Obtaining Sample
LISD3 uses SID3 bias samples towards small trees. ACT, however, would like
bias sample towards low-cost trees. purpose, designed stochastic version
EG2 algorithm, attempts build low cost trees greedily. EG2, tree
built top-down, test maximizes ICF chosen splitting node, where,
ICF () =

2I() 1
.
(cost () + 1)w

information gain (as ID3). parameter w [0, 1] controls bias
towards lower cost attributes. w = 0, test costs ignored ICF relies solely
6

fiAnytime Induction Low-cost, Low-error Classifiers

Procedure SEG2-Choose-Attribute(E, A)
Foreach
(a) Information-Gain(E, a)
c (a) Cost(a)
2I(a) 1
p (a) (c(a)+1)
w
Choose attribute random A;
attribute a, probability
selecting proportional p (a)
Return
Figure 4: Attribute selection SEG2
information gain. Larger values w strengthen effect test costs ICF.
discuss setting value w Section 3.5.
stochastic EG2 (SEG2), choose splitting attributes semi-randomly, proportionally
ICF. SEG2 stochastic, expect able escape local minima
least trees sample. Figure 4 formalizes attribute selection component
SEG2. obtain sample size r, ACT uses EG2 SEG2 r 1 times. EG2
SEG2 given direct access context-based costs, i.e., attribute already
tested, cost zero another attribute belongs group
tested, group discount applied.
3.2 Evaluating Subtree
LSID3 cost-insensitive learning algorithm. such, main goal maximize
expected accuracy learned tree. Occams razor states given two consistent
hypotheses, smaller one likely accurate. Following Occams razor, LSID3
uses tree size preference bias favors splits expected reduce final
size.
cost-sensitive setup, however, goal minimize expected total cost
classification. Therefore, rather choosing attribute minimizes size,
would like choose one minimizes total cost. Given decision tree, need
come procedure estimates expected cost using tree classify
future case. cost two components: test cost misclassification cost.
3.2.1 Estimating Test Costs
Assuming distribution future cases would similar learning
examples, estimate test costs using training data. Given tree, calculate
average test cost training examples use estimate test cost new
cases. (sub)tree built E, set training examples, denote average
cost traversing example E
tcost(T, E) =

1 X
tcost(T, e).
eE
7

fiEsmeir & Markovitch


estimated test cost unseen example e therefore tcost(T,
e ) = tcost(T, E).
Observe costs calculated relevant context. attribute already
tested upper nodes, charge testing again. Similarly, attribute
group g already tested, apply group discount attributes
g. delayed attribute encountered, sum cost entire subtree.

3.2.2 Estimating Misclassification Costs
go estimating cost misclassification obvious. tree size
longer used heuristic predictive errors. Occams razor allows comparison
two consistent trees provides means estimating accuracy. Moreover, tree size
measured different currency accuracy hence cannot easily incorporated
cost function.
Rather using tree size, propose different estimator: expected error
(Quinlan, 1993). leaf training examples, misclassified,
expected error defined upper limit probability error, i.e., EE(m, s, cf ) =
bin (m, s), cf confidence level U bin upper limit confidence
Ucf
interval binomial distribution. expected error tree sum expected
errors leaves.
Originally, expected error used C4.5s error-based pruning predict whether
subtree performs better leaf. Although lacking theoretical basis, shown
experimentally good heuristic. ACT use expected error approximate
misclassification cost. Assume problem |C| classes misclassification cost
matrix . Let c class label leaf l. Let ml total number examples
l mil number examples l belong class i. penalties
predictive errors uniform (Mi,j = mc), estimated misclassification cost l
(l) = EE(ml , ml mc , cf ) mc.
mcost
l

problem nonuniform misclassification costs, mc replaced cost
actual errors leaf expected make. errors obviously unknown
learner. One solution estimate error type separately using confidence intervals
multinomial distribution multiply associated cost:
(l) =
mcost

X

mul
Ucf
(ml , mil , |C|) mc.

i6=c

approach, however, would result overly pessimistic approximation, mainly
many classes. Alternatively, compute expected error uniform
case propose replacing mc weighted average penalty classifying
instance c belongs another class. weights derived proportions
mil
ml mc using generalization Laplaces law succession (Good, 1965, chap. 4):
l

(l) = EE(ml , ml mc , cf )
mcost
l

X
i6=c



!

mil + 1
Mc,i .
ml mcl + |C| 1

Note problem C classes, average C 1 possible penalties
Mc,c = 0. Hence, problem two classes c1 , c2 leaf marked c1 , mc
8

fiAnytime Induction Low-cost, Low-error Classifiers

Procedure ACT-Choose-Attribute(E, A, r)
r = 0
Return EG2-Choose-Attribute(E, A)
Foreach
Foreach vi domain(a)
Ei {e E | a(e) = vi }
EG2(a, Ei , {a})
mini Total-Cost(T, Ei )
Repeat r 1 times
SEG2(a, Ei , {a})
mini min (mini , Total-Cost(T, Ei ))
P|domain(a)|
mini
totala Cost(a) + i=1
Return totala minimal
Figure 5: Attribute selection ACT
would replaced M1,2 . classifying new instance, expected misclassification
cost tree built examples sum expected misclassification costs
leaves divided m:
1 X

mcost(l),
mcost(T
)=

L set leaves . Hence, expected total cost classifying
single instance is:



total(T,
E) = tcost(T,
E) + mcost(T
).

alternative approach intend explore future work estimate cost
sampled trees using cost set-aside validation set. approach attractive
mainly training set large one afford setting aside significant part
it.
3.3 Choosing Split
decided sampler tree utility function, ready formalize
tree growing phase ACT. tree built top-down. procedure selecting
splitting test node listed Figure 5 illustrated Figure 6. give
detailed example ACT chooses splits explain split selection procedure
modified numeric attributes.
3.3.1 Choosing Split: Illustrative Examples
ACTs evaluation cost-senstive considers test error costs simultaneously
take account different error penalties. illustrate let us consider
two-class problem mc = 100$ (uniform) 6 attributes, a1 , . . . , a6 , whose costs
10$. training data contains 400 examples, 200 positive 200
negative.
9

fiEsmeir & Markovitch



G2
SE
st( 4.9
=

co
)

cost(EG2)
=4.7
cost(SEG2)
=5.1

cost(EG2)
=8.9

Figure 6: Attribute evaluation ACT. Assume cost current context
0.4. estimated cost subtree rooted therefore 0.4 + min(4.7, 5.1) +
min(8.9, 4.9) = 10.

Test costs
a1 10
a2 10
a3 10
a4 10
a5 10
a6 10
MC costs
FP 100
FN 100

r=1

T1

T2

a1
a3

5 +
95 EE = 7.3

-

a2
a4

a5

95 +
5
EE = 7.3

+

5 +
95 EE = 7.3

-

95 +
5
EE = 7.3

+

mcost (T1) = 7.3*4*100$ / 400 = 7.3$
tcost (T1) = 20$
total(T1) = 27.3$

0 +
50 EE = 1.4

-

a6

100 +
50 EE = 54.1

+

0 +
50 EE = 1.4

-

100 +
50 EE = 54.1

+

mcost (T2) = (1.4*2 + 54.1*2) * 100$ / 400 = 27.7$
tcost (T2) = 20$
total (T2) = 47.7$

Figure 7: Evaluation tree samples ACT. leftmost column defines costs: 6
attributes identical cost uniform error penalties. 1 sampled a1
2 a2 . EE stands expected error. total cost T1
lower, ACT would prefer split a1 .

Assume choose a1 a2 , r = 1. Let trees
Figure 7, denoted 1 2, sampled a1 a2 respectively. expected
error costs T1 T2 are:3

mcost(T
1) =


mcost(T
2) =

=

1
4 7.3
(4 EE (100, 5, 0.25)) 100$ =
100$ = 7.3$
400
400
1
(2 EE (50, 0, 0.25) 100$ + 2 EE (150, 50, 0.25) 100$)
400
2 1.4 + 2 54.1
100$ = 27.7$
400

test error costs involved, ACT considers sum. Since test
cost trees identical (20$), ACT would prefer split a1 . If, however, cost
3. example set cf 0.25, C4.5. Section 3.5 discuss tune cf .

10

fiAnytime Induction Low-cost, Low-error Classifiers

Test costs
a1 40
a2 10
a3 10
a4 10
a5 10
a6 10
MC costs
FP 100
FN 100

r=1

T1

T2

a1
a3

-

a4

a5

95 +
5
EE = 7.3

5 +
95 EE = 7.3

95 +
5
EE = 7.3

5 +
95 EE = 7.3

a2

-

+

+

mcost (T1) = 7.3*4*100$ / 400 = 7.3$
tcost (T1) = 50$
total(T1) = 57.3$

a6

-

100 +
50 EE = 54.1

0 +
50 EE = 1.4

100 +
50 EE = 54.1

0 +
50 EE = 1.4

-

+

+

mcost (T2) = (1.4*2 + 54.1*2) * 100$ / 400 = 27.7$
tcost (T2) = 20$
total (T2) = 47.7$

Figure 8: Evaluation tree samples ACT. leftmost column defines costs: 6
attributes identical cost (except expensive a1 ) uniform error
penalties. 1 sampled a1 2 a2 . total cost T2
lower, ACT would prefer split a2 .

Test costs
a1 10
a2 10
a3 10
a4 10
a5 10
a6 10
MC costs
FP 1
FN 199

r=1

T1

T2

a1
a3

5 +
95 EE = 7.3

-

a2
a4

a5

95 +
5
EE = 7.3

+

5 +
95 EE = 7.3

-

95 +
5
EE = 7.3

+

mcost (T1) = (7.3*2*1$ + 7.3*2*199$) / 400 = 7.3$
tcost (T1) = 20$
total (T1) = 27.3$

0 +
50 EE = 1.4

-

a6

100 +
50 EE = 54.1

+

0 +
50 EE = 1.4

-

100 +
50 EE = 54.1

+

mcost (T2) = (54.1*2*1$ + 1.4*2*199$) / 400 = 1.7$
tcost (T2) = 20$
total (T2) = 21.7$

Figure 9: Evaluation tree samples ACT. leftmost column defines costs: 6
attributes identical cost nonuniform error penalties. 1 sampled
a1 2 a2 . total cost T2 lower, ACT would prefer
split a2 .

a1 40$, Figure 8, tcost(T 1) would become 50$ total cost 1 would
become 57.3$, 2 would remain 47.7$. Hence, case ACT would split
a2 .
illustrate ACT handles nonuniform error penalties, let us assume cost
attributes 10$, cost false positive (FP ) 1$ cost
false negative (F N ) 199$. Let trees Figure 9, denoted 1 2, sampled
a1 a2 respectively. first example, misclassification costs play role
test costs trees same. Although average misclassification
11

fiEsmeir & Markovitch

cost also 100$, ACT evaluates trees differently:

mcost(T
1) =

=


mcost(T
2) =

=

1
(2 EE (100, 5, 0.25) 1$ + 2 EE (100, 5, 0.25) 199$)
400
2 7.3 1$ + 2 7.3.1 199$
= 7.3$
400
1
(2 EE (50, 0, 0.25) 199$ + 2 EE (100, 50, 0.25) 1$)
400
2 1.4 199$ + 2 54.1 1$
= 1.7$
400

Therefore, nonuniform setup, ACT would prefer a2 . makes sense
given setup prefer trees may result false positives reduce number
expensive false negatives.
3.3.2 Choosing Split Attributes Numeric
selection procedure formalized Figure 5 must modified slightly attribute numeric: rather iterating values attribute take, first pick
r tests (split points) highest information gain invoke EG2
split point. guarantees numeric nominal attributes get resources.
Chickering, Meek, Rounthwaite (2001) introduced several techniques generating
small number candidate split points dynamically little overhead. future
intend apply techniques select r points, evaluated
single invocation EG2.
3.4 Cost-Sensitive Pruning
Pruning plays important role decision tree induction. cost-insensitive environments,
main goal pruning simplify tree order avoid overfitting training
data. subtree pruned resulting tree expected yield lower error.
test costs taken account, pruning another important role: reducing
test costs tree. Keeping subtree worthwhile expected reduction misclassification costs larger cost tests subtree. misclassification
cost zero, makes sense keep split tree. If, hand, misclassification cost much larger test costs, would expect similar behavior
cost-insensitive setup.
handle challenge, propose novel approach cost-sensitive pruning.
error-based pruning (Quinlan, 1993), scan tree bottom-up. compare
expected total cost subtree leaf. leaf expected perform better,
subtree pruned.
cost subtree estimated described Section 3.2. Formally, let E
set training examples reach subtree , let size E. Assume
examples E belong default class.4 Let L set leaves .
4. misclassification costs uniform, default class majority class. Otherwise, class
minimizes misclassification cost node.

12

fiAnytime Induction Low-cost, Low-error Classifiers

prune leaf if:
X
1


mcost(l).
EE(m, s, cf ) mc tcost(T,
E) +



assumes uniform misclassification cost mc. case nonuniform penalties,
multiply expected error average misclassification cost.
alternative approach post-pruning early stopping growing phase.
example, one could limit depth tree, require minimal number examples
child (as C4.5), prevent splitting nodes splitting criterion fails exceed
predetermined threshold (as DTMC). Obviously, pre-pruning condition also
applied part post-pruning procedure. advantage post-pruning, however,
ability estimate effect split entire subtree it,
immediate successors (horizon effect).
Consider example 2-XOR problem b. Splitting neither b would
positive gain hence growing would stopped. pre-pruning allowed,
optimal tree would found would post-pruned utility
splits correctly measured. Frank (2000) reports comprehensive study pruning
decision trees, compared pre- post-pruning empirically cost-insensitive
setup. findings show advantage post-pruning variety UCI datasets
significant. pre-pruning computationally efficient, Frank concluded
that, practice, might viable alternative post-pruning. Despite results,
decided use post-pruning ACT, following reasons:
1. Several concepts represented UCI repository may appear real-world
problems. example, parity functions naturally arise real-world problems,
Drosophila survival concept (Page & Ray, 2003).
2. costs involved, horizon effect may appear frequently high
costs may hide good splits.
3. anytime setup user willing wait longer order obtain good tree.
Since post-pruning takes even less time induction single greedy tree,
extra cost post-pruning minor.
future plan add pre-pruning parameter allow early stopping
resources limited. Another interesting direction future work would postprune final tree pre-prune lookahead trees form samples. would
reduce runtime cost less accurate estimations utility candidate
split.
3.5 Setting Parameters ACT
addition r, sample size, ACT parameterized w, controls weight
test costs EG2, cf , confidence factor used pruning error
estimation. ICET tunes w cf using genetic search. ACT considered three different
alternatives: (1) keeping EG2s C4.5s default values w = 1 cf = 0.25, (2) tuning
13

fiEsmeir & Markovitch

values using cross-validation, (3) setting values priori, function
problem costs.
first solution simplest, exploit potential adapting
sampling mechanism specific problem costs. Although tuning values using
grid search would achieve good results, may costly terms runtime. example,
5 values parameter used 5-fold cross-validation, would need
run ACT 125 times sake tuning alone. anytime setup time could
invested invoke ACT larger r hence improve results. Furthermore,
algorithm would able output valid solution tuning stage finished.
Alternatively, could try tune parameters invoking much faster EG2,
results would good optimal values EG2 necessarily
good ACT.
third approach, chose experiments, set w cf advance,
according problem specific costs. w set inverse proportionally misclassification cost: high misclassification cost results smaller w, reducing effect attribute
costs split selection measure. exact formula is:
w = 0.5 + ex ,
x average misclassification cost (over non-diagnoal entries ) divided
C, cost take tests. Formally,
x=

P

Mi,j
.
(|C| 1) |C| C
i6=j

C4.5 default value cf 0.25. Larger cf values result less pruning. Smaller
cf values lead aggressive pruning. Therefore, ACT set cf value
range [0.2, 0.3]; exact value depends problem cost. test costs dominant,
prefer aggressive pruning hence low value cf . test costs negligible,
prefer prune less. value cf also used estimate expected error.
Again, test costs dominant, afford pessimistic estimate error,
misclassification costs dominant, would prefer estimate closer
error rate training data. exact formula setting cf is:
x1
cf = 0.2 + 0.05(1 +
).
x+1

4. Empirical Evaluation
conducted variety experiments test performance behavior ACT.
First introduce novel method automatic adaption existing datasets costsensitive setup. describe experimental methodology motivation. Finally
present discuss results.
4.1 Datasets
Typically, machine learning researchers use datasets UCI repository (Asuncion &
Newman, 2007). five UCI datasets, however, assigned test costs.5 include
5. Costs datasets assigned human experts (Turney, 1995).

14

fiAnytime Induction Low-cost, Low-error Classifiers

datasets experiments. Nevertheless, gain wider perspective,
developed automatic method assigns costs existing datasets. method
parameterized with:
1. cr, cost range.
2. g, number desired groups percentage number attributes.
problem |A| attributes, g |A| groups. probability attribute
1
, probability belong
belong groups g|A|+1
groups.
3. d, number delayed tests percentage number attributes.
4. , group discount percentage minimal cost group (to ensure
positive costs).
5. , binary flag determines whether costs drawn randomly, uniformly ( = 0)
semi-randomly ( = 1): cost test drawn proportionally information
gain, simulating common case valuable features tend higher costs.
case assume cost comes truncated normal distribution,
mean proportional gain.
Using method, assigned costs 25 datasets: 20 arbitrarily chosen UCI datasets6
5 datasets represent hard concepts used previous research. Appendix gives detailed descriptions datasets.
Due randomization cost assignment process, set parameters
defines infinite space possible costs. 25 datasets sampled space
4 times
cr = [1, 100], g = 0.2, = 0, = 0.8, = 1.
parameters chosen attempt assign costs manner similar
real costs assigned. total, 105 datasets: 5 assigned human experts
100 automatically generated costs.7
Cost-insensitive learning algorithms focus accuracy therefore expected perform well testing costs negligible relative misclassification costs. However,
testing costs significant, ignoring would result expensive classifiers. Therefore,
evaluating cost-sensitive learners requires wide spectrum misclassification costs.
problem 105, created 5 instances, uniform misclassification costs
mc = 100, 500, 1000, 5000, 10000. Later on, also consider nonuniform misclassification
costs.
4.2 Methodology
start experimental evaluation comparing ACT, given fixed resource allocation, several cost-sensitive cost-insensitive algorithms. Next compare
anytime behavior ACT ICET. Finally, evaluate algorithms
6. chosen UCI datasets vary size, type attributes, dimension.
7. additional 100 datasets available http://www.cs.technion.ac.il/esaher/publications/cost.

15

fiEsmeir & Markovitch

two modifications problem instances: random test cost assignment nonuniform
misclassification costs.
4.2.1 Compared Algorithms
ACT compared following algorithms:
C4.5 : cost-insensitive greedy decision tree learner. algorithm reimplemented following details (Quinlan, 1993) default parameters
used.
LSID3 : cost-insensitive anytime decision tree learner. uses extra time
induce trees higher accuracy. able, however, exploit additional allotted
time reduce classification costs.
IDX : greedy top-down learner prefers splits maximize
c (Norton, 1989).
algorithm take account misclassification costs. IDX implemented top C4.5, modifying split selection criteria.
2

CSID3 : greedy top-down learner prefers splits maximize Ic (Tan &
Schlimmer, 1989). algorithm take account misclassification costs.
CSID3 implemented top C4.5, modifying split selection criteria.
EG2 : greedy top-down learner prefers splits maximize

2I() 1

w

(cost()+1)
(Nunez, 1991). algorithm take account misclassification costs.
EG2 implemented top C4.5, modifying split selection criteria.

DTMC : DTMC implemented following original pseudo-code (Ling et al.,
2004; Sheng et al., 2006). However, original pseudo-code support continuous attributes multiple class problems. added support continuous
attributes, C4.5s dynamic binary-cut discretization, cost reduction
replacing gain ratio selecting cutting points. extension multiple class problems straightforward. Note DTMC post-prune trees
pre-prunes them.
ICET : ICET reimplemented following detailed description given
Turney (1995). verify results reimplementation, compared
reported literature. followed experimental setup
used 5 datasets. results indeed similar: basic version ICET
achieved average cost 50.8 reimplementation vs. 50 reported originally.
One possible reason slight difference may initial population
genetic algorithm randomized, genetic operators process
partitioning data training, validating, testing sets. paper, Turney
introduced seeded version ICET, includes true costs initial
population, reported perform better unseeded version. Therefore,
use seeded version comparison. parameters ICET
default ones.
16

fiAnytime Induction Low-cost, Low-error Classifiers

4.2.2 Normalized Cost
Turney (1995) points out, using average cost classification dataset
problematic because: (1) cost differences algorithms become relatively small
misclassification cost increases, (2) difficult combine results multiple
datasets fair manner (e.g., average), (3) difficult combine average
different misclassification costs. overcome problems, Turney suggests normalizing
average cost classification dividing standard cost. Let C cost
take tests. Let fi frequency class data. error response
always class therefore (1 fi ). standard cost defined
C + mini (1 fi ) maxi,j (Mi,j ) .
standard cost approximation maximal cost given problem.
consists two components: maximal test cost misclassification cost
classifier achieves baseline accuracy (e.g., majority-based classifier error
costs uniform). classifiers may perform even worse baseline
accuracy, standard cost strictly upper bound real cost.
experiments, however, exceeded.
4.2.3 Statistical Significance
problem 105, single 10-fold cross-validation experiment conducted.
partition train-test sets used compared algorithms. determine
statistical significance performance differences ACT, ICET, DTMC
used two tests:
Paired t-test = 5% confidence. problem 105
pair algorithms, 10 pairs results obtained 10-fold cross
validation runs. used paired t-test determine weather difference
two algorithms given problem significant (rejecting null hypothesis
algorithms differ performance). Then, count algorithm
many times significant winner.
Wilcoxon test (Demsar, 2006), compares classifiers multiple datasets
states whether one method significantly better ( = 5%).
4.3 Fixed-time Comparison
105 5 problem instances, ran different algorithms, including ACT
r = 5. chose r = 5 average runtime ACT would shorter ICET
problems. methods much shorter runtime due greedy nature.
Table 1 summarizes results.8 pair numbers represents average normalized
cost associated confidence interval ( = 5%). Figure 10 illustrates average results
plots normalized costs different algorithms misclassification costs.
Statistical significance test results ACT, ICET, DTMC given Table 2.
algorithms compared using t-test Wilcoxon test. table lists
8. full results available http://www.cs.technion.ac.il/esaher/publications/cost.

17

fiEsmeir & Markovitch

Table 1: Average cost classification percentage standard cost classification
different mc values. numbers represent average 105 datasets
associated confidence intervals ( = 5%).
mc
100
500
1000
5000
10000

C4.5
50.6
49.9
50.4
53.3
54.5

LSID3

4.2
4.2
4.6
5.9
6.4

45.3
43.0
42.4
43.6
44.5

IDX

3.7
3.9
4.5
6.1
6.6

34.4
42.4
47.5
58.1
60.8

CSID3

3.6
3.6
4.2
5.9
6.4

41.7
45.2
47.8
54.3
56.2

3.8
3.9
4.4
5.9
6.4

EG2
35.1
42.5
47.3
57.3
59.9

DTMC

3.6
3.6
4.3
5.9
6.4

14.6
37.7
47.1
57.6
59.5

1.8
3.1
3.8
5.2
5.6

ICET
24.3
36.3
40.6
45.7
47.1

3.1
3.1
3.9
5.6
6.0

ACT
15.2
34.5
39.1
41.5
41.4

1.9
3.2
4.2
5.7
6.0

Table 2: DTMC vs. ACT ICET vs. ACT using statistical tests. mc, first
column lists number t-test significant wins second column gives
winner, any, implied Wilcoxon test datasets = 5%.
test WINS
mc
100
500
1000
5000
10000

DTMC vs. ACT
14
9
7
7
6

3
29
45
50
56

W ilcoxon WINNER

ICET vs. ACT
4
5
12
15
7

54
23
24
21
24

DTMC vs. ACT

ICET vs. ACT

DTMC
ACT
ACT
ACT
ACT

ACT
ACT
ACT
ACT
-

number t-test wins algorithm 105 datasets, well winner,
any, Wilcoxon test applied.
misclassification cost relatively small (mc = 100), ACT clearly outperforms
ICET, 54 significant wins opposed ICETs 4 significant wins. significant
difference found remaining runs. setup ACT able produce
small trees, sometimes consisting one node; accuracy learned model ignored
setup. ICET, contrary, produced, datasets, larger
costly trees. DTMC achieved best results, outperformed ACT 14 times.
Wilcoxon test also indicates DTMC better ACT ACT better
ICET. investigation showed datasets ACT produced unnecessarily
larger trees. believe better tuning cf would improve ACT scenario
making pruning aggressive.
extreme, misclassification costs dominate (mc = 10000), performance DTMC worse ACT ICET. t-test indicates ACT
significantly better ICET 24 times significantly worse 7 times. According
Wilcoxon test = 5%, difference ACT ICET significant.
Taking > 5.05%, however, would turn result favor ACT. Observe DTMC,
winner mc = 100, becomes worst algorithm mc = 10000. One reason
18

fiAnytime Induction Low-cost, Low-error Classifiers

Average % Standard Cost

60

50

40

30
C4.5
LSID3
EG2
DTMC
ICET
ACT

20

10
100

1000
Misclassification Cost

10000

ACT Cost

Figure 10: Average normalized cost function misclassification cost

100

100

100

80

80

80

60

60

60

40

40

40

20

20

20

0

0
0

20

40
60
ICET Cost

80

100

0
0

20

40
60
ICET Cost

80

100

0

20

40
60
ICET Cost

80

100

Figure 11: Illustration differences performance ACT ICET mc =
100, 1000, 10000 (from left right). point represents dataset. x-axis
represents cost ICET y-axis represents ACT. dashed
line indicates equality. Points ACT performs better
ICET better.

phenomenon DTMC, introduced Ling et al. (2004), perform
post-pruning, although might improve accuracy domains.
two extremes less interesting: first could use algorithm
always outputs tree size 1 second could use cost-insensitive learners.
middle range, mc {500, 1000, 5000}, requires learner carefully balance
two types cost. cases ACT lowest average cost largest
number t-test wins. Moreover, Wilcoxon test indicates superior. ICET
second best method. reported Turney (1995), ICET clearly better
greedy methods EG2, IDX, CSID3.
Note EG2, IDX, CSID3, insensitive misclassification cost, produced trees values mc. trees, however, judged differently
change misclassification cost.
Figure 11 illustrates differences ICET ACT mc = 100, 1000, 10000.
point represents one 105 datasets. x-axis represents cost ICET
y-axis represents ACT. dashed line indicates equality. see,
19

fiEsmeir & Markovitch

100

Average Accuracy

90

80

70
C4.5
LSID3
EG2
DTMC
ICET
ACT

60

50
100

1000
Misclassification Cost

10000

Figure 12: Average accuracy function misclassification cost

majority points equality line, indicating ACT performs better.
mc = 10000 see points located close x-axis large x
value. points represent difficult domains, XOR, ICET could
learn ACT could.
4.4 Comparing Accuracy Learned Models
misclassification costs low, optimal algorithm would produce shallow
tree. misclassification costs dominant, optimal algorithm would produce
highly accurate tree. see, ACTs normalized cost increases increase
misclassification cost. relatively easy produce shallow trees, concepts
easily learnable even cost-insensitive algorithms fail achieve perfect accuracy
them. Hence, importance accuracy increases, normalized cost increases
predictive errors affect dramatically.
learn effect misclassification costs accuracy, compare
accuracy built trees different misclassification costs. Figure 12 shows results.
important property DTMC, ICET, ACT ability compromise accuracy needed. produce inaccurate trees accuracy insignificant much
accurate trees penalty error high. ACTs flexibility, however,
noteworthy: second least accurate method becomes accurate one.
Interestingly, accuracy extremely important, ICET ACT achieve even
better accuracy C4.5. reason non-greedy nature. ICET performs
implicit lookahead reweighting attributes according importance. ACT performs
lookahead sampling space subtrees every split. two, results
indicate ACTs lookahead efficient terms accuracy. DTMC less accurate
C4.5. reason different split selection criterion different pruning
mechanism.
comparison anytime cost insensitive algorithm LSID3, ACT produced less
accurate trees mc relatively low. mc set 5000, however, ACT
achieved comparable accuracy LSID3 slightly outperformed mc = 10000.
Statistical tests found differences accuracy two algorithms
20

fiAnytime Induction Low-cost, Low-error Classifiers

10
EG2
DTMC
ICET
ACT

84
8
Average Cost

Average Cost

82
80
EG2
DTMC
ICET
ACT

78
76

6

4

2

74
72

0
0

0.5

1

1.5

2
2.5
Time [sec]

3

3.5

4

4.5

0

0.2

0.4

0.6

0.8
1
Time [sec]

1.2

1.4

1.6

1.8

80
100
70

90
80
Average Cost

Average Cost

60
50
40
EG2
DTMC
ICET
ACT

30
20

EG2
DTMC
ICET
ACT

70
60
50
40
30

10

20
0
0

1

2
3
Time [sec]

4

5

0

1

2

3
Time [sec]

4

5

6

Figure 13: Average normalized cost function time (from top-left bottom-right)
Breast-cancer-20, Monks1, Multi-XOR, XOR5

case insignificant. ACTs small advantage datasets indicates that,
problems, expected error better heuristic tree size maximizing accuracy.
4.5 Comparison Anytime Behavior
ICET ACT, like typical anytime algorithms, perform better increased
resource allocation. ICET expected exploit extra time producing generations hence better tuning parameters final invocation EG2. ACT
use extra time acquire larger samples hence achieve better cost estimations.
examine anytime behavior ICET ACT, ran 4 problems,
namely Breast-cancer-20, Monks-1, Multi-XOR, XOR5, exponentially increasing
time allocation. mc set 5000. ICET run 2, 4, 8, . . . generations ACT
sample size 1, 2, 4, . . .. fixed-time comparison, used 4 instances
problem. Figure 13 plots results averaged 4 instances. also included
results greedy methods EG2 DTMC.
results show good anytime behavior ICET ACT: generally worthwhile allocate time. ACT dominates ICET four domains able
produce less costly trees shorter time.
One advantage ACT ICET able consider context
attribute judged. ICET, contrary, reassigns cost attributes globally:
21

fiEsmeir & Markovitch

Average % Standard Cost

60

50

DTMC

ICET

ACT

40

100
500
1000
5000
10000

30

20

10
100

DTMC
ICET
ACT
1000
Misclassification Cost

12.3
31.5
40.4
54.0
57.4

1.8
3.2
3.9
5.2
5.6

18.7
31.8
36.4
43.7
45.6

2.7
3.4
3.9
5.5
5.9

13.0
30.2
33.9
38.5
39.6

2.0
3.3
4.0
5.6
6.1

10000

Figure 14: Average cost test costs assigned randomly

attribute cannot assigned high cost one subtree low cost another. MultiXOR dataset exemplifies concept whose attributes important one sub-concept.
concept composed four sub-concepts, relies different attributes
(see Appendix details). expected, ACT outperforms ICET significantly
latter cannot assign context-based costs. Allowing ICET produce
generations (up 128) result trees comparable obtained ACT.
4.6 Random Costs
costs 100 105 datasets assigned using semi-random mechanism
gives higher costs informative attributes. ensure ACTs success due
particular cost assignment scheme, repeated experiments costs drawn
randomly uniformly given cost range cr, i.e., set 0. Figure 14 shows
results. see, ACT maintains advantage methods: dominates
along scale mc values.
4.7 Nonuniform Misclassification Costs
far, used uniform misclassification cost matrices, i.e., cost
error type identical. explained Section 3, ACT algorithm also handle
complex misclassification cost matrices penalty one type error might
higher penalty another type. next experiment examines ACT
nonuniform scenario. Let FP denote penalty false positive FN penalty
false negative. 2 classes, split classes 2 equal groups
according order (or randomly order exists). assign penalty FP
misclassifying instance belongs first group FN one belongs
second group.
obtain wide view, vary ratio FP FN also examine different
absolute values. Table 3 Figure 15 give average results. Table 4 lists number
t-test significant wins algorithm achieved. easy see ACT consistently
outperforms methods.
22

fiAnytime Induction Low-cost, Low-error Classifiers


8


4


2




2


4


8


= 500

C4.5
EG2
DTCM
ICET
ACT

29.2
30.1
12.4
23.3
11.9

34.2
33.0
20.3
27.0
18.5

41.3
37.2
29.8
31.5
27.2

49.9
42.5
37.7
36.3
34.5

43.6
39.3
32.5
34.2
29.1

39.0
37.5
22.9
31.8
20.4

36.3
36.8
15.8
29.2
13.8

= 5000

Table 3: Comparison C4.5, EG2, DTMC, ACT, ICET misclassification costs
nonuniform. FP denotes penalty false positive FN penalty
false negative. denotes basic mc unit.

C4.5
EG2
DTCM
ICET
ACT

27.0
30.9
13.8
21.4
12.9

31.3
35.2
23.6
25.6
19.1

39.2
43.1
38.0
32.7
28.8

53.3
57.3
57.6
45.7
41.5

44.0
47.7
42.5
37.4
31.1

39.0
42.4
29.3
32.8
22.5

36.3
39.7
20.1
29.8
14.6

FP
FN

Table 4: Comparing DTMC, ACT, ICET misclassification costs nonuniform.
F P/F N ratio, columns list number t-test significant wins
= 5%. FP denotes penalty false positive FN penalty
false negative. denotes basic mc unit.
= 500
F P/F N
0.125
0.25
0.5
1
2
4
8

DTMC vs. ACT
4
2
10
9
5
3
1

22
31
25
29
35
40
27

= 5000

ICET vs. ACT
11
7
7
5
1
0
4

52
49
42
23
47
72
72

DTMC vs. ACT
5
10
16
7
5
4
0

44
49
52
50
61
58
62

ICET vs. ACT
12
4
10
15
9
0
0

44
36
25
21
31
44
67

Interestingly, graphs slightly asymmetric. reason could
datasets, example medical ones, difficult reduce negative errors positive
ones, vice versa. similar phenomenon reported Turney (1995).
highest cost algorithms observed F P = F N because,
ratio FP FN extremely large extremely small, learner easily
build small tree whose leaves labeled class minimizes costs.
misclassification costs balanced, however, learning process becomes much
complicated.
23

fi50

60

45

55
Average % Standard Cost

Average % Standard Cost

Esmeir & Markovitch

40
35
30
25
C4.5
EG2
DTMC
ICET
ACT

20
15
10
/8

/4

/2
/
2/
Misclassification Cost FP/FN

50
45
40
35
30
25

C4.5
EG2
DTMC
ICET
ACT

20
15

4/

10
/8

8/

/4

/2
/
2/
Misclassification Cost FP/FN

4/

8/

Figure 15: Comparison C4.5, EG2, DTMC, ACT, ICET misclassification costs
nonuniform. misclassification costs represented pair (F P/F N ).
FP denotes penalty false positive FN penalty false
negative. denotes basic mc unit. figures plot average cost
function ratio FP FN, = 500 (left) = 5000
(right).

5. Related Work
addition works referred earlier paper, several related works warrant
discussion here.
Cost-sensitive trees subject many research efforts. Several works proposed learning algorithms consider different misclassification costs (Breiman, Friedman, Olshen, & Stone, 1984; Pazzani, Merz, Murphy, Ali, Hume, & Brunk, 1994; Provost
& Buchanan, 1995; Bradford, Kunz, Kohavi, Brunk, & Brodley, 1998; Domingos, 1999;
Elkan, 2001; Zadrozny, Langford, & Abe, 2003; Lachiche & Flach, 2003; Abe, Zadrozny, &
Langford, 2004; Vadera, 2005; Margineantu, 2005; Zhu, Wu, Khoshgoftaar, & Yong, 2007;
Sheng & Ling, 2007). methods, however, consider test costs hence
appropriate mainly domains test costs constraint.
Davis, Ha, Rossbach, Ramadan, Witchel (2006) presented greedy cost-sensitive
decision tree algorithm forensic classification: problem classifying irreproducible
events. setup, assume tests might used testing must
acquired hence charged classification.
One way exploit additional time searching less costly tree widen
search space. Bayer-Zubek Dietterich (2005) formulated cost-sensitive learning
problem Markov decision process (MDP), used systematic search algorithm
based AO* heuristic search procedure solve MDP. make AO* efficient,
algorithm uses two-step lookahead based heuristic. limited lookahead
informed immediate heuristics still insufficient complex domains might
cause search go astray (Esmeir & Markovitch, 2007a). algorithm shown
output better diagnostic policies several greedy methods using reasonable resources.
optimal solution, however, could always found due time memory limits.
nice property algorithm serve anytime algorithm computing
24

fiAnytime Induction Low-cost, Low-error Classifiers

best complete policy found far. anytime behavior, nevertheless, problematic
policies optimal respect train data tend overfit. result,
performance eventually start degrade.
Arnt Zilberstein (2005) tackled problem time cost sensitive classification
(TCSC). TCSC, utility labeling instance depends correctness
labeling, also amount time takes. Therefore total cost function
additional component, reflects time needed measure attribute. Typically,
super-linear form: cost quick result small fairly constant,
waiting time increases, time cost grows increasing rate. problem
complicated sequence time-sensitive classification instances considered,
time spent administering tests one case adversely affect costs future
instances. Arnt Zilberstein suggest solving problems extending decision
theoretic approach introduced Bayer-Zubek Dietterich (2005). work,
assume time takes administer test incorporated cost.
future, intend extend framework support time-sensitive classification,
individual cases sequences.
Fan, Lee, Stolfo, Miller (2000) studied problem cost-sensitive intrusion detection systems (IDS). goal maximize security minimizing costs.
prediction (action) cost. Features categorized three cost levels according
amount information needed compute values. reduce cost IDS, high
cost rules considered predictions low cost rules sufficiently
accurate.
Costs also involved learning phase, example acquisition
model learning. problem budgeted learning studied Lizotte, Madani,
Greiner (2003). cost associated obtaining attribute value
training example, task determine attributes test given budget.
related problem active feature-value acquisition. setup one tries reduce
cost improving accuracy identifying highly informative instances. Melville, SaarTsechansky, Provost, Mooney (2004) introduced approach instances
selected acquisition based accuracy current model confidence
prediction.
Greiner, Grove, Roth (2002) pioneers studying classifiers actively decide
tests administer. defined active classifier classifier given
partially specified instance, returns either class label strategy specifies
test performed next. Greiner et al. also analyzed theoretical aspects
learning optimal active classifiers using variant probably-approximately-correct
(PAC) model. showed task learning optimal cost-sensitive active classifiers
often intractable. However, task shown achievable active classifier
allowed perform (at most) constant number tests, limit provided
learning. setup proposed taking dynamic programming approach
build trees depth d.
setup assumed charged acquiring feature values test
cases. term test strategy (Sheng, Ling, & Yang, 2005) describes process feature
values acquisition: values query order. Several test strategies
studied, including sequential, single batch multiple batch (Sheng et al., 2006),
25

fiEsmeir & Markovitch

corresponds different diagnosis policy. strategies orthogonal
work assume given decision tree.
Bilgic Getoor (2007) tackled problem feature subset selection costs
involved. objective minimize sum information acquisition cost
misclassification cost. Unlike greedy approaches compute value features one
time, used novel data structure called value information lattice (VOILA),
exploits dependencies missing features makes possible share information value computations different feature subsets possible. VIOLA shown
empirically achieve dramatic cost improvements without prohibitive computational
costs comprehensive search.

6. Conclusions
Machine learning techniques increasingly used produce wide range classifiers complex real-world applications involve nonuniform testing misclassification costs. increasing complexity applications poses real challenge
resource management learning classification. work introduced novel
framework operating complex environments. framework four major
advantages:
uses non-greedy approach build decision tree therefore able overcome
local minima problems.
evaluates entire trees search; thus, adjusted cost scheme
defined trees.
exhibits good anytime behavior allows learning speed traded classification costs. many applications willing allocate time
would allocate greedy methods. proposed framework exploit extra
resources.
sampling process easily parallelized method benefit distributed computer power.
evaluate ACT designed extensive set experiments wide range
costs. Since publicly available cost-oriented datasets, designed
parametric scheme automatically assigns costs existing datasets. experimental
results show ACT superior ICET DTMC, existing cost-sensitive algorithms
attempt minimize test costs misclassification costs simultaneously. Significance
tests found differences statistically strong. ACT also exhibited good anytime
behavior: increase time allocation, cost learned models decreased.
ACT contract anytime algorithm requires sample size predetermined.
future intend convert ACT interruptible anytime algorithm adopting
IIDT general framework (Esmeir & Markovitch, 2007a). addition, plan apply
monitoring techniques (Hansen & Zilberstein, 2001) optimal scheduling ACT
examine strategies evaluating subtrees.
26

fiAnytime Induction Low-cost, Low-error Classifiers

Table 5: Characteristics datasets used

Dataset
Breast Cancer
Bupa
Car
Flare
Glass
Heart
Hepatitis
Iris
KRK
Monks-1
Monks-2
Monks-3
Multiplexer-20
Multi-XOR
Multi-AND-OR
Nursery
Pima
TAE
Tic-Tac-Toe
Titanic
Thyroid
Voting
Wine
XOR 3D
XOR-5

Instances
277
345
1728
323
214
296
154
150
28056
124+432
169+432
122+432
615
200
200
8703
768
151
958
2201
3772
232
178
200
200

Attributes
Nominal (binary) Numeric
9 (3)
0 (0)
6 (0)
10 (5)
0 (0)
8(4)
13(13)
0 (0)
6(0)
6 (2)
6 (2)
6 (2)
20 (20)
11 (11)
11 (11)
8(8)
0(0)
4(1)
9 (0)
3(2)
15(15)
16 (16)
0 (0)
0 (0)
10 (10)

0
5
0
0
9
5
6
4
0
0
0
0
0
0
0
0
8
1
0
0
5
0
13
6
0

Max attribute
domain

Classes

13
4
7
4
2
8
4
4
4
2
2
2
5
26
3
4
2
2
2

2
2
4
4
7
2
2
3
17
2
2
2
2
2
2
5
2
3
2
2
3
2
3
2
2

Acknowledgments
work partially supported funding EC-sponsored MUSCLE Network
Excellence (FP6-507752).

Appendix A. Datasets
Table 5 lists characteristics 25 datasets used. give detailed
description non-UCI datasets used experiments:
1. Multiplexer: multiplexer task used several researchers evaluating classifiers (e.g., Quinlan, 1993). instance series bits length + 2a ,
positive integer. first bits represent index remaining bits
label instance value indexed bit. experiments considered
20-Multiplexer (a = 4). dataset contains 500 randomly drawn instances.
2. Boolean XOR: Parity-like functions known problematic many learning
algorithms. However, naturally arise real-world data, Drosophila
survival concept (Page & Ray, 2003). considered XOR five variables five
additional irrelevant attributes.
27

fiEsmeir & Markovitch

3. Numeric XOR: XOR based numeric dataset used evaluate learning
algorithms (e.g., Baram, El-Yaniv, & Luz, 2003). example consists values
x coordinates. example labeled 1 product x positive,
1 otherwise. generalized domain three dimensions added irrelevant
variables make concept harder.
4. Multi-XOR / Multi-AND-OR: concepts defined 11 binary attributes.
cases target concept composed several subconcepts, first
two attributes determines considered. 10 attributes
used form subconcepts. Multi-XOR dataset, subconcept XOR,
Multi-AND-OR dataset, subconcept either OR.

References
Abe, N., Zadrozny, B., & Langford, J. (2004). iterative method multi-class costsensitive learning. Proceedings 10th ACM SIGKDD International Conference
Knowledge Discovery Data Mining (KDD-2004), Seattle, WA, USA.
Arnt, A., & Zilberstein, S. (2005). Learning policies sequential time cost sensitive
classification. Proceedings 1st international workshop Utility-based data
mining (UBDM05) held KDD05, pp. 3945, New York, NY, USA. ACM Press.
Asuncion, A., & Newman, D. (2007).
UCI machine learning repository.
University California, Irvine, School Information Computer Sciences.
http://www.ics.uci.edu/mlearn/MLRepository.html.
Baram, Y., El-Yaniv, R., & Luz, K. (2003). Online choice active learning algorithms.
Proceedings 20 International Conference Machine Learning (ICML-2003),
pp. 1926, Washington, DC, USA.
Bayer-Zubek, V., & Dietterich (2005). Integrating learning examples search
diagnostic policies. Artificial Intelligence, 24, 263303.
Bilgic, M., & Getoor, L. (2007). Voila: Efficient feature-value acquisition classification.
Proceedings 22nd National Conference Artificial Intelligence (AAAI-2007).
Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. (1987). Occams Razor.
Information Processing Letters, 24 (6), 377380.
Boddy, M., & Dean, T. L. (1994). Deliberation scheduling problem solving time
constrained environments. Artificial Intelligence, 67 (2), 245285.
Bradford, J., Kunz, C., Kohavi, R., Brunk, C., & Brodley, C. (1998). Pruning decision
trees misclassification costs. Proceedings 9th European Conference
Machine Learning (ECML-1998), pp. 131136.
Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification Regression
Trees. Wadsworth Brooks, Monterey, CA.
Chickering, D. M., Meek, C., & Rounthwaite, R. (2001). Efficient determination dynamic
split points decision tree. Proceedings 1st IEEE International Conference
28

fiAnytime Induction Low-cost, Low-error Classifiers

Data Mining (ICDM-2001), pp. 9198, Washington, DC, USA. IEEE Computer
Society.
Davis, J. V., Ha, J., Rossbach, C. J., Ramadan, H. E., & Witchel, E. (2006). Cost-sensitive
decision tree learning forensic classification. Proceedings 17th European
Conference Machine Learning (ECML-2006), pp. 622629, Berlin, Germany.
Demsar, J. (2006). Statistical comparisons classifiers multiple data sets. Journal
Machine Learning Research, 7, 130.
Domingos, P. (1999). Metacost: general method making classifiers cost-sensitive.
Proceedings 5th International Conference Knowledge Discovery Data
Mining (KDD1999), pp. 155164.
Elkan, C. (2001). foundations cost-sensitive learning. Proceedings 17th
International Joint Conference Artificial Intelligence (IJCAI-2001), pp. 973978,
Seattle, Washington, USA.
Esmeir, S., & Markovitch, S. (2006). decision tree learner plenty time.
Proceedings 21st National Conference Artificial Intelligence (AAAI-2006),
Boston, MA, USA.
Esmeir, S., & Markovitch, S. (2007a). Anytime learning decision trees. Journal Machine
Learning Research, 8.
Esmeir, S., & Markovitch, S. (2007b). Occams razor got sharper. Proceedings
20th International Joint Conference Artificial Intelligence (IJCAI-2007),
Hyderabad, India.
Fan, W., Lee, W., Stolfo, S. J., & Miller, M. (2000). multiple model cost-sensitive approach
intrusion detection. Proceedings 11th European Conference Machine
Learning (ECML-2000), pp. 142153, Barcelona, Catalonia, Spain.
Frank, E. (2000). Pruning Decision Trees Lists. Ph.D. thesis, Department Computer
Science, University Waikato.
Good, I. (1965). Estimation Probabilities: Essay Modern Bayesian Methods.
MIT Press, USA.
Greiner, R., Grove, A. J., & Roth, D. (2002). Learning cost-sensitive active classifiers.
Artificial Intelligence, 139 (2), 137174.
Hansen, E. A., & Zilberstein, S. (2001). Monitoring control anytime algorithms:
dynamic programming approach. Artificial Intelligence, 126 (1-2), 139157.
Hastie, T., Tibshirani, R., & Friedman, J. (2001). Elements Statistical Learning:
Data Mining, Inference, Prediction. New York: Springer-Verlag.
Hyafil, L., & Rivest, R. L. (1976). Constructing optimal binary decision trees NPcomplete. Information Processing Letters, 5 (1), 1517.
Lachiche, N., & Flach, P. (2003). Improving accuracy cost two-class multiclass probabilistic classifiers using roc curves. Proceedings 20th International
Conference Machine Learning (ICML-2003).
29

fiEsmeir & Markovitch

Lenert, L., & Soetikno, R. (1997). Automated computer interviews elicit utilities: Potential applications treatment deep venous thrombosis. American Medical
Informatics Association, 4 (1), 4956.
Ling, C. X., Yang, Q., Wang, J., & Zhang, S. (2004). Decision trees minimal costs.
Proceedings 21st International Conference Machine Learning (ICML-2004).
Lizotte, D. J., Madani, O., & Greiner, R. (2003). Budgeted learning naive bayes classifiers.
Proceedings 19th Conference Uncertainty Artificial Intelligence (UAI2003), Acapulco, Mexico.
Margineantu, D. (2005). Active cost-sensitive learning. Proceedings 19th International Joint Conference Artificial Intelligence (IJCAI-2005), Edinburgh, Scotland.
Melville, P., Saar-Tsechansky, M., Provost, F., & Mooney, R. J. (2004). Active feature acquisition classifier induction. Proceedings 4th IEEE International Conference
Data Mining (ICDM-2004), pp. 483486, Brighton, UK.
Norton, S. W. (1989). Generating better decision trees. Sridharan, N. S. (Ed.), Proceedings Eleventh International Joint Conference Artificial Intelligence, pp.
800805, Detroit, Michigan, USA.
Nunez, M. (1991). use background knowledge decision tree induction. Machine
Learning, 6, 231250.
Page, D., & Ray, S. (2003). Skewing: efficient alternative lookahead decision
tree induction. Proceedings 18th International Joint Conference Artificial
Intelligence (IJCAI-2003), Acapulco, Mexico.
Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., & Brunk, C. (1994). Reducing
misclassification costs: knowledge intensive approaches learning noisy data.
Proceedings 11th International Conference Machine Learning (ICML1994).
Provost, F., & Buchanan, B. (1995). Inductive policy: pragmatics bias selection.
Machine Learning, 20 (1-2), 3561.
Qin, Z., Zhang, S., & Zhang, C. (2004). Cost-sensitive decision trees multiple cost
scales. Lecture Notes Computer Scienc, AI 2004: Advances Artificial Intelligence,
Volume 3339/2004, 380390.
Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81106.
Quinlan, J. R. (1993). C4.5: Programs Machine Learning. Morgan Kaufmann, San
Mateo, CA.
Sheng, S., Ling, C. X., Ni, A., & Zhang, S. (2006). Cost-sensitive test strategies.
Proceedings 21st National Conference Artificial Intelligence (AAAI-2006),
Boston, MA, USA.
Sheng, S., Ling, C. X., & Yang, Q. (2005). Simple test strategies cost-sensitive decision
trees. Proceedings 9th European Conference Machine Learning (ECML2005), pp. 365376, Porto, Portugal.
30

fiAnytime Induction Low-cost, Low-error Classifiers

Sheng, V. S., & Ling, C. X. (2007). Roulette sampling cost-sensitive learning.
Proceedings 18th European Conference Machine Learning (ECML-2007),
pp. 724731, Warsaw, Poland.
Tan, M., & Schlimmer, J. C. (1989). Cost-sensitive concept learning sensor use approach recognition. Proceedings 6th International Workshop Machine
Learning, pp. 392395, Ithaca, NY.
Turney, P. (2000). Types cost inductive concept learning. Proceedings
Workshop Cost-Sensitive Learning held 17th International Conference
Machine Learning (ICML-2000), Stanford, CA.
Turney, P. D. (1995). Cost-sensitive classification: Empirical evaluation hybrid genetic
decision tree induction algorithm. Journal Artificial Intelligence Research, 2, 369
409.
Vadera, S. (2005). Inducing cost-sensitive non-linear decision trees. Technical report 03-052005, School Computing, Science Engineering, University Salford.
Zadrozny, B., Langford, J., & Abe, N. (2003). Cost-sensitive learning cost-proportionate
example weighting. Proceedings 3rd IEEE International Conference Data
Mining (ICDM-2003), Melbourne, Florida, USA.
Zhu, X., Wu, X., Khoshgoftaar, T., & Yong, S. (2007). empirical study noise
impact cost-sensitive learning. Proceedings 20th International Joint Conference Artificial Intelligence (IJCAI-2007), Hyderabad, India.

31

fiJournal Artificial Intelligence Research 33 (2008) 575-613

Submitted 4/08; published 12/08

Value Correlation
Itai Ashlagi

iashlagi@hbs.edu

Harvard Business School,
Harvard University,
Boston, MA, 02163,USA

Dov Monderer

dov@ie.technion.ac.il

Faculty Industrial Engineering Management,
Technion - Israel Institute Technology,
Haifa 32000, Israel

Moshe Tennenholtz

moshet@microsoft.com

Microsoft Israel R&D Center,
13 Shenkar St., Herzeliya 46725, Israel,
Faculty Industrial Engineering Management,
Technion - Israel Institute Technology,
Haifa 32000, Israel

Abstract
Correlated equilibrium generalizes Nash equilibrium allow correlation devices. Correlated equilibrium captures idea many systems exists trusted administrator recommend behavior set agents, enforce behavior.
makes solution concept appropriate study multi-agent systems
AI. Aumann showed example game, correlated equilibrium game
agents welfare (expected sum players utilities) greater welfare
mixed-strategy equilibria. Following idea initiated price anarchy literature
suggests study two major measures value correlation game
nonnegative payoffs:
1. ratio maximal welfare obtained correlated equilibrium maximal
welfare obtained mixed-strategy equilibrium. refer ratio mediation
value.
2. ratio maximal welfare maximal welfare obtained correlated equilibrium. refer ratio enforcement value.
work initiate study mediation enforcement values, providing
several general results value correlation captured concepts. also
present set results specialized case congestion games , class games
received lot attention recent literature.

1. Introduction
Much work area multi-agent systems adopts game-theoretic reasoning.
due fact many existing systems consist self-motivated participants,
attempts optimize performance. result Nash equilibrium,
central solution concept game theory, become major tool study analysis
multi-agent systems. Nash equilibrium captures multi-agent behavior stable
c
2008
AI Access Foundation. rights reserved.

fiAshlagi, Monderer, & Tennenholtz

unilateral deviations. Naturally, system fully controlled designer enforce
behaviors lead higher welfare one obtained fully decentralized system
agents behave selfishly follow Nash equilibrium. comparison
quantities studied title work price anarchy (Koutsoupias &
Papadimitriou, 1999; Roughgarden & Tardos, 2002; Christodoulou & Koutsoupias, 2005),
subject much interest computer science. However, fully controlled systems
versus fully uncontrolled systems two extreme points. acknowledged various
works AI (Shoham & Tennenholtz, 1995a, 1995b) one main practical approaches
dealing realistic systems consider systems limited centralized control.
Indeed, realistic systems designer recommend behavior;
distinguished strong requirement designer dictate behavior.
Correlated equilibrium, introduced Aumann (1974), famous game-theoretic
solution concept referring designer recommend enforce behavior.
game strategic form, correlated strategy probability distribution set
strategy profiles, strategy profile vector strategies, one player.
correlated strategy utilized follows: strategy profile selected according
distribution, every player informed strategy profile. selected
strategy player interpreted recommendation play. Correlated strategies
natural, since capture idea system administrator/reliable party
recommend behavior enforce it. Hence, correlated strategies make perfect sense
context congestion control, load balancing, trading, etc. correlated strategy
called correlated equilibrium better every player obey recommended
strategy believes players obey recommended strategies1 . Correlated
equilibrium makes perfect sense context work multi-agent systems AI
exists mediator recommend behavior agents.2 major potential
benefit mediator using correlated equilibrium attempt improve
welfare selfish players. paper, welfare obtained correlated strategy
defined expected sum utilities players, referred
welfare obtained correlated strategy.
striking example introduced Aumanns seminal paper (1974) two-player
two-strategy game, welfare obtained correlated equilibrium higher
welfare obtained every mixed-strategy equilibrium game. modification
Aumanns example serves us motivating example.
Aumanns Example:

1. Every correlated strategy defines Bayesian game private signal every player
recommended strategy. correlated equilibrium obeying recommended strategy every
player pure-strategy equilibrium Bayesian game.
2. use mediators obtaining desired behaviors, addition improving social welfare,
studied, (e.g., Monderer & Tennenholtz, 2004, 2006; Rozenfeld & Tennenholtz, 2007; Ashlagi,
Monderer, & Tennenholtz, 2008). However, mediators discussed work makes use
powerful capabilities making recommendation based probabilistic coin flips.

576

fiOn Value Correlation

b1

b2

a1 5

0
1

a2

4

0
1

4

5

result, Aumanns example suggests correlation may way improve
welfare still assuming players rational classical game-theoretic sense.3
game, three mixed-strategy equilibrium profiles. Two
obtained pure strategies, (a1 , b1 ), (a2 , b2 ). welfare purestrategy equilibrium profiles equals six. additional mixed-strategy equilibrium
every player chooses strategies equal probabilities. welfare
obtained profile equals 5 (= 14 (6 + 0 + 8 + 6)) every entry matrix
played probability 41 . Hence, maximal welfare mixed-strategy equilibrium
equals 6. Consider following correlated strategy: probability 1/3 assigned
every pure strategy profile (a1 , b2 ). correlated strategy correlated equilibrium.
Indeed, row player recommended play a1 knows player
recommended play b1 , therefore strictly prefers play a1 . row
player recommended play a2 conditional probability columns half,
therefore weakly prefers play a2 . Similar argument applied column player
shows correlated strategy indeed correlated equilibrium. welfare associated
1
correlated equilibrium equals 20
3 (= 3 (6 + 8 + 6)).
discussion suggests one may wish consider value correlation
games. order address challenge studying value correlation, tackle two
fundamental issues:
much society/system gain adding correlation device,
assume without device agents play mixed-strategy equilibrium.
much society/system loose fact correlation device
recommend (and enforce) course action?
Accordingly introduce two measures, mediation value enforcement value.
measures make sense mainly games nonnegative utilities, focus
paper.
mediation value measures ratio maximal welfare correlated
equilibrium maximal welfare mixed-strategy equilibrium. Notice higher
mediation value is, correlation helps. Hence, mediation value measures
value reliable mediator recommend play model
anarchy without mediator, anarchy defined situation
players use welfare-best mixed-strategy equilibrium, is, anarchy best
outcome reached rational selfish agents.4
3. advantages purely computational ones. recently shown, correlated equilibrium
computed polynomial time even structured representations games (Kakade, Kearns,
Langford, & Ortiz, 2003; Papadimitriou, 2005).
4. phenomenon multiple equilibria forces modeling choice concept anarchy, could
defined also welfare-worst mixed-strategy equilibrium, convex combination

577

fiAshlagi, Monderer, & Tennenholtz

Aummans example shown correlated equilibrium introduced
best correlated equilibrium, i.e., attains maximal welfare among correlated
equilibria game. Hence, mediation value Aumanns example 10
9 .
enforcement value measures ratio maximal welfare maximal
welfare correlated equilibrium. is, value center dictate
course play respect mediator use correlation devices equilibrium.
maximal welfare Aumanns example 8, enforcement value game equals
6
5.
paper establish general basic results concerning measures defined
above. consider mediation (enforcement) value classes games, defined
least upper bound mediation (enforcement) values games class.
first study general games. consider important class congestion games
(Rosenthal, 1973; Monderer & Shapley, 1996). Indeed, class games perhaps
applicable game theory CS synergy. particular, results regarding
price anarchy obtained congestion games. restrict study simple
congestion games.
Next summarize main results discuss related literature.
1.1 Main Results General Games
Aumanns example implies mediation value class two-player two-strategy
(22) games least 10/9. paper proved mediation value class
equals 4/3. Next, complex games studied. particular consider two possible
minimal extensions 2 2 games: Two-player games three strategies one
players two strategies other, three-player games two strategies
player. shown mediation value classes unbounded, i.e.,
equals . Consequently, mediation value unbounded classes larger games.5
interpreted positive result, showing extreme power correlation.
Considering enforcement value, proved equals classes 2 2
larger games. proof result uses games weakly dominant strategies.
show, however, enforcement value class three-player two-strategy games
without weakly dominated strategies also equals .
1.2 Main Results Simple Congestion Games
simple congestion game set facilities. Every facility j associated
nonnegative payoff function wj . Every player chooses facility, say facility j, receives
wj (k), k number players chose facility j.
completeness first deal simple case, exist two players,
show mediation value class simple congestion games two facilities
equals 4/3. general case, 2 facilities, proved
mediation value bounded 2. However, proved mediation
best-welfare worst-welfare mixed strategy equilibrium .This choice matter taste,
chose best option.
5. game larger game obtained adding players, and/or adding strategies
players .

578

fiOn Value Correlation

value equals 1 class simple congestion games non-increasing facility payoff
functions.
case two players, show mediation value unbounded
class games three players two facilities non-increasing payoffs.
contrast,
linear case, proved mediation value bounded

5+1

1.618 class games number players two facilities
2
non-increasing linear payoff functions. give example game class whose
mediation value equals 9/8, leaving open significant gap.
Additional special theorems proved simple congestion games symmetric
(identical) facilities; proved that, every n 4, mediation value higher 1
class two symmetric (identical) facilities non-increasing payoffs n players.
illustrates power correlation. Nevertheless, show every simple
congestion game number players number symmetric facilities,
facility payoff functions satisfy certain concavity requirement, best mixedstrategy equilibrium obtains maximal welfare, therefore mediation value
enforcement value game equal 1.
Finally, study enforcement value case, exist n players
symmetric facilities arbitrary facility payoff functions. characterize set
games enforcement value equals 1, result, determine situations
correlation allows obtaining maximal welfare.
1.3 Related Literature
end introduction discussion relevant issues price anarchy
literature, potential relationships concepts mediation value enforcement value.6 many situations natural deal nonnegative costs rather
utilities; indeed, literature price anarchy focused models.
translating definition price anarchy7 games utilities costs,
price anarchy utilities ratio maximal welfare minimal
welfare obtained mixed-strategy equilibrium. higher number is, value
center higher, center enforce course play. Hence, price anarchy
utilities measures value center respect anarchy, center
dictate play, anarchy measured worst social outcome reached rational selfish agents. Recently, Anshelevich, Dasgupta, Kleinberg, Tardos, Wexler,
Roughgarden (2004) defined price stability models costs.8 Accordingly, price
stability utilities, ratio maximal welfare maximal welfare
mixed-strategy equilibrium. relevant concept using correlated equilibrium models
costs defined independently Christodoulou Koutsoupias (2005),
referred price stability correlated equilibria.9 translated model
6. concept price anarchy received much attention recent computer science literature,
(e.g., Marvonicolas & Spirakis, 2001; Czumaj & Vocking, 2002; Roughgarden, 2002; Roughgarden &
Tardos, 2002).
7. price anarchy defined games costs ratio maximal cost mixedstrategy equilibrium minimal cost.
8. ratio minimal cost mixed strategy equilibrium minimal cost.
9. ratio minimal cost correlated equilibrium minimal cost.

579

fiAshlagi, Monderer, & Tennenholtz

utilities, price stability correlated equilibria utilities ratio
maximal welfare maximal welfare mixed-strategy equilibrium,
enforcement value.10 However, results proved one ratios one models
cannot translated results analogous ratio model. due
fact moving one model require multiplication
negative constant, e.g. -1, numbers also need shifted remain nonnegative;
needless say, corresponding ratios significantly changed shifting.11
return discussion Section 4.1.

2. Basic Definitions
finite game strategic form tuple = (N, (S )iN , (ui )iN ); N nonempty finite
set players. Unless otherwise specified assume N = {1, 2, ...., n}, n 1.
N , finite set strategies player i. Let = 1 2 n
set strategy profiles (n-tuples). element = (si )iN . N
let si = (s1 , ..., si1 , si+1 , ...sn ) denote strategies played everyone i. Thus
= (si , si ). player N , let ui : R utility function player i. ui (s)
utility player profile strategies played. called nonnegative
game utilities players nonnegative, i.e., ui : R+ every player i.
player also randomize among strategies using mixed strategy - distribution set strategies. finite set C, (C) denotes set probability
distributions C. Thus P = (S ) set mixed strategies player i. Let pi P
mixed strategy i. every si , pi (si ) probability player plays
strategy si pi . Every pure strategy si is, natural identification, mixed
strategy psi P

1 ti =

psi (t ) =
0 ti 6= si .
psi called pure strategy, si interchangeably called strategy pure strategy
(when identified psi ). Let P = P 1 P 2 P n set mixed strategy
profiles.
Let si , ti pure strategies player i. say si weakly dominates ti , ti
weakly dominated si si Si
ui (si , si ) ui (ti , si ),
least one inequality strict. say si strictly dominates ti , ti strictly
dominated si inequalities strict. ui (si , si ) = u(ti , si )
si Si , say si ti equivalent strategies player i.
(S) called correlated strategy. Every mixed strategy profile p P
interpreted correlated strategy p , every strategy profile S,
10. completeness, one define mediation value costs ratio minimal cost
mixed-strategy equilibrium minimal cost correlated equilibrium.
11. Interestingly, shown exist classes cost games price anarchy
bounded, price anarchy utilities analogous classes utility games unbounded.
class cost games analogous class utility games given Example 2 Section 4.1.2,
constitutes one example.

580

fiOn Value Correlation

Q
p (s) , ni=1 pi (si ). Whenever necessary identify p p . slight abuse
notation, every (S), denote ui () expected utility player
correlated strategy (S) played, is:
X
ui () =
ui (s)(s).
(1)
sS

Naturally, every p P denote ui (p) = ui (p ). Hence ui (p) expected utility
player mixed strategy profile p played.
say p P mixed-strategy equilibrium ui (pi , pi ) ui (pi , q ) every
player N every q P . Let p P mixed-strategy equilibrium. every
i, pi pure strategy, also call p pure-strategy equilibrium.
Definition 1 (Aumann, 1974, 1987) correlated strategy (S) correlated equilibrium N si , ti :
X
(si , si )[ui (si , si ) ui (si , ti )] 0.
(2)
si Si

Consider third party picks randomly pure-strategy profile respect
well-known correlated strategy , recommends privately every player play si .
left hand side (2) captures difference expected utility playing si , i.e.,
following recommendation third party, playing pure-strategy ti
given players follow recommendations. Hence, difference
nonnegative, player better playing si .
well-known easily verified every mixed-strategy
correlated
Pn equilibrium
(). value u()
equilibrium. every correlated strategy , let u() ,
u
i=1
called welfare . Let N () set mixed-strategy equilibria let
C() set correlated equilibria . define vC () vN () follows:
vC () , max{u() : C()},
vN () , max{u(p) : p N ()}.
Note vN () vC () well defined due compactness N () C()
respectively, continuity u. Define opt() (the maximal welfare) follows:
opt() , max{u() : (S)} = max{u(s) : S}.
mediation value nonnegative game defined follows:
V () ,

vC ()
.
vN ()

vN () = 0 vC () = 0, define V () 1. vN () = 0 vC () > 0,
V () defined . Denote EV () enforcement value nonnegative game
. is,
opt()
EV () ,
.
vC ()
581

fiAshlagi, Monderer, & Tennenholtz

vC () = 0 opt() = 0, define EV () 1. vC () = 0 opt() > 0,
EV () defined . Finally, class nonnegative games A, define
mediation value enforcement value class follows:
V (A) , sup V ();

EV (A) , sup EV ().





One tools need paper linear programming. game
strategic form, C() exactly set feasible solutions following linear program
Pb. Moreover, C() optimal solution Pb u() = vC ().

P

max sS (s)u(s)





s.t.
b
P : (s) 0
S,

P



sS (s) = 1,


P







si Si (s)[u (t , ) u (s , )] 0 N, (s , ) , 6= .
dual problem Pb one decision variable constraint
primal. Let
P
denote dual variable associated primal constraint sS (s) = 1. Let (ti |si )
denote dual variable associated primal constraint defined (si , ti ), is,
constraint
X
(s)[ui (ti , si ) ui (si , si )] 0,
si Si

let = (i )iN , = (i (ti |si ))(si ,ti )S , si 6=ti . dual problem
written follows:


min



s.t.
b:


(ti |si ) 0
N, (si , ti ) , ti 6= si ,


P
P





S.

{ti | ti 6=si } (t |s )[u (t , ) u (s , )] + u(s)
b feasible bounded, objective
well known problems Pb
values equal vC (). feasibility consequence existence mixed-strategy
equilibrium proved Nash (1951), fact every mixed-strategy equilibrium
also correlated equilibrium.12
also make use following notation definitions. Let G class
nonnegative games strategic form. m1 , m2 , ..., mn 1 denote Gm1 m2 mn G
class games n players |S | = mi every player i.

3. Results General Games
following two basic lemmas used proofs paper. proof
Lemma 1 follows directly Definition 1, proof Lemma 2 standard.
Therefore, proofs omitted.
12. elementary proof existence correlated equilibrium, use existence mixedstrategy equilibrium given Hart Schmeidler (1989).

582

fiOn Value Correlation

Lemma 1 Let game strategic form. Let si weakly dominated
ti , let si Si . (s) = 0 every correlated equilibrium
ui (ti , si ) > ui (si , si ).
Consequently, si strictly dominated strategy, (s) = 0 every correlated equilibrium
.
Next define extensions game adding dummy strategy one players,
adding dummy player. Let Gm1 m2 mn . game Gm1 mi1 (mi +1)mi+1 mn
extension adding dummy strategy player obtained adding
strategy player utility n players equal zero, player uses
new strategy.
game Gm1 m2 mn 1 extension adding dummy player, player
n + 1, obtained adding player n + 1 single strategy
utilities player zeros, utility n players remain . is,
ui (s, d) = ui (s) every S, denotes unique added strategy player n + 1.
game trivial extension sequence games,
= 0 , 1 , , =
k obtained k1 adding dummy player dummy strategy.
Lemma 2 Let trivial extension . Then, V () = V () EV () =
EV ().
3.1 Mediation Value
section show power correlation general games. start extending
Aumanns result power correlation 2 2 games.
3.1.1 Two-person two-strategy games
Aumanns example shows mediation value
show:

10
9

obtained 2 2 game.

Theorem 1 V (G22 ) = 43 .
following lemma needed proof Theorem 1:
Lemma 3 (Peeters & Potters, 1999) Let G22 . exist correlated equilibrium
, induced mixed-strategy equilibrium, least two pure-strategy
equilibria.
give proof Theorem 1 need following technical remarks, holds
rest paper:
Remark: games introduced figures, denote player 1,2,3 row,
column, matrix (if exist) players respectively. strategy profile players
utilities given left right player utility ith left payoff.
Proof Theorem 1: begin showing V (G22 ) 34 . Figure 1 describes
arbitrary game G22 .
583

fiAshlagi, Monderer, & Tennenholtz

1


a2

b1
a, b
m, n

b2
j, k
c,

Figure 1
Lemma 3, less two pure-strategy equilibrium profiles, V () = 1.
Therefore, assume without loss generality two pure-strategy
equilibrium profiles. However, four pure-strategy equilibrium profiles, mediation value equals 1. Therefore discuss cases either two
three pure-strategy equilibrium profiles.
Suppose possesses exactly three pure-strategy equilibrium profiles, without loss
generality let (a2 , b2 ) strategy profile equilibrium. Since (a1 , b1 ), (a2 , b1 )
(a1 , b2 ) pure-strategy equilibria, = b = k. (a2 , b2 )
equilibrium, c < j < n. c < j, Lemma 1, every correlated equilibrium C()
satisfies (a2 , b2 ) = 0, therefore V () = 1. Similarly, < n, (a2 , b2 ) = 0 implies
V () = 1.
Suppose exactly two pure-strategy equilibrium profiles . two equilibrium profiles may row, column, diagonal. Obviously,
proof case two pure-strategy equilibrium profiles row
column covered following proof, assumes two purestrategy equilibrium profiles first row. is, (a1 , b1 ) (a1 , b2 )
pure-strategy equilibria. assumption implies b = k. Observe strategy
profile player 1 plays strategy a1 probability one, player 2 plays
mixed strategy p2 P 2 , mixed-strategy equilibrium. Since exactly two purestrategy equilibria, must < c < j. < c < j, Lemma 1 every
correlated equilibrium C() satisfies (a2 , b1 ) = 0 (a2 , b2 ) = 0. Therefore
V () = 1. Suppose = a. Therefore c < j. Hence, Lemma 1 every correlated
equilibrium C() satisfies (a2 , b2 ) = 0. Since (a2 , b1 ) pure-strategy equilibrium, n < d. Since b = k n < d, Lemma 1, every correlated equilibrium
C() satisfies (a2 , b1 ) = 0. Therefore V () = 1. showed V () = 1
two pure-strategy equilibrium profiles, row
column.
proceed last case two pure-strategy equilibrium profiles
diagonal. Without loss generality let (a1 , b1 ) (a2 , b2 ) pure-strategy equilibrium
profiles. shown Peeters Potters (1999) c = j = b = k
= n, C() exactly convex hull N (). Hence, case extreme
point C() mixed-strategy equilibrium, therefore mediation value
equals 1. Therefore assume:
c > j, > m, b > k

> n.

(3)

both, u(a1 , b2 ) u(a2 , b1 ) smaller max{u(a1 , b1 ), u(a2 , b2 )}, V () = 1
proof completed. Therefore, without loss generality make following two
assumptions:
(A1) u(a1 , b1 ) u(a2 , b2 ). is, + b c + d.
584

fiOn Value Correlation

(A2) u(a2 , b1 ) u(a2 , b2 ). is, + n c + d.
Hence, set mixed-strategy equilibria is:
N () = {((1, 0), (1, 0)), ((0, 1), (0, 1)), ((

1

1

,
), (
,
))},
1+ 1+ 1+ 1+

bk
=
cj = dn . Note (3), positive.
continuing proof need following geometric characterization
C(). Peeters Potters (1999), C() polyhedron following five extreme
points , = 1, ..., 5:
!






1
1, 0
0, 0
(1+)(1+) , (1+)(1+)
,
1 =
, 2 =
, 3 =


0, 0
0, 1
, (1+)(1+)
(1+)(1+)

!

!
1

1
0
(1++) , (1++)
(1++) ,
4 =
, 5 =
,



0,
(1++
(1++) , (1++)

(j, k) denotes probability given strategy profile (aj , bk ) correlated equilibrium . is, (j, k)th entry (aj , bk ). agreement
identify mixed-strategy profiles correlated strategies observe set
mixed-strategy equilibrium precisely
N () = {1 , 2 , 3 }.

(4)

prove u() 43 vN () every correlated equilibrium C().
sufficient prove inequality holds extreme points C(). Since 1 , 2
3 mixed-strategy equilibria, u(i ) vN () = 1, 2, 3. Therefore suffices
prove inequality 4 5 .
first derive couple inequalities useful us. (A1) since
utilities nonnegative, c + c + d. Therefore, since < n < d,
obtain inequality
+ n 2(c + d).
(5)
Since (a1 , b1 ) (a2 , b2 ) pure-strategy equilibrium profiles, + n +
j + k c + b. (A2) since + n + d, c a. Hence j + k + b. Therefore,
(A1) obtain
j + k c + d.
(6)
Note inequality (6) implies u(4 ) vN () since (a2 , b2 ) pure-strategy
equilibrium.
remains show u(5 ) vN (). (A1), u(1 ) u(2 ). distinguish
following two cases:
Case 1: u(3 ) u(2 ).
(A1), u(1 ) u(2 ). Therefore, (4), vN () = u(3 ).
Hence,
+ b + (m + n) + (c + d)
1 + + +
u(5 )
=

.
vN ()
1 + +
+ b + (j + k) + (m + n) + (c + d)
585

fiAshlagi, Monderer, & Tennenholtz

Therefore, (j + k) 0,
u(5 )
1 + + +

.
vN ()
1 + +
z > 0, let f1 (z) =
Let K =
profiles,

m+ncd
c+djk .

1++z+z
1++z .

Hence, suffices show f1 () 43 .

Since (a1 , b1 ) (a2 , b2 ) pure-strategy equilibrium

+ n c c,

c + j k b.

Therefore (A1),
K 1.
Since u(3 ) c + d,
+ b + (j + k) + (m + n) (c + d)(1 + + ).
Therefore


(m + n c d) + + b c
.
c+djk

(7)

Equation (7) (A1), K. Since, K 1, . Note f1 (z)
non-decreasing z > 0. Therefore,
f1 () f1 () =
Since

1+2+ 2
1++ 2

=

(+1)2
(+1)2

1 + 2 + 2
.
1 + + 2

(8)

maximized > 0 = 1,
4
f1 () .
3

Case 2: u(3 ) < u(2 ).
(A1), u(1 ) u(2 ). Therefore, (4), vN () = u(2 ). Therefore,
u(5 )
+ b + (m + n) + (c + d)
=
.
vN ()
(1 + + )(c + d)
z > 0 let f2 (z) =

a+b+(m+n)+(c+d)z
.
(1++z)(c+d)

(9)

Hence, suffices show f2 () 43 .

case inequality (7) reversed.


(m + n c d) + + b c
,
c+djk



(m + n c d) + + b c
.
c+d

since j, k 0,

(10)

distinguish following two cases, noticing f2 (x) non-increasing
x > 0:
586

fiOn Value Correlation

Case 2.1: + b = c + d: case, f2 non-increasing (10) holds,
f2 ()

c + + (m + n) + (m + n c d) 2
(1 + +

(m+ncd) 2
)(c
c+d

+ d)

.

(11)

Set x = c + d. Therefore, (A2), exists 1 2 tx = + n.
(11)
f2 ()

1 + + (t 1) 2
4
x + tx + (t 1)x 2
=
,
2
2
x + x + (t 1)x
1 + + (t 1)
3

(12)

last inequality follows similar arguments following (8).
Case 2.2: + b < c + d:
Set x = + b. Therefore (A1), (A2) (5), > 1
1 k 2,
c + = tx + n = ktx.
(13)
Hence,
kt =

a+b+c+d
m+n

= + 1,
a+b
a+b

inequality follows since + n + b + c + d. Therefore


1
.
k1

(14)

Therefore
f2 ()

+ b + (m + n) + (c + d)[ (m+ncd)+a+bcd
]
c+d
(1 + +

(m+ncd)+a+bcd
)(c
c+d

+ d)

=

1 + kt + t(k 1) 2 +
1 + + t( + 1)(k 1)
=

2
+ + t(k 1) +
+ 2 (k 1) +

(15)

1 + 2 + 2
( + 1)2
4

,
1 + 2 (k 1) +
( + 1)2
3

(16)

right inequality (15) follows (14), left inequality (16) follows since 1 k 2, right inequality (16) follows since maximum
(+1)2
value (+1)
2 attained = 1.
showed mediation value bounded 43 . remains show
bound tight. show family games mediation value approaches
43 bound. Consider family games x shown Figure 2 (a variant Aumanns
example), x > 1.

587

fiAshlagi, Monderer, & Tennenholtz

1


a2

b1
x,1
x-1,x-1

b2
0,0
1,x

Figure 2
game strategy profiles (a1 , b1 ) (a2 , b2 ) pure-strategy equilibrium
profiles, u(a1 , b1 ) = u(a2 , b2 ) = x+1. exists one mixed-strategy equilibrium,
player assigns probability 0.5 strategies, yields
welfare lower x + 1. correlated strategy (S), strategy
profiles (a1 , b1 ) ,(a2 , b1 ) (a2 , b2 ) played equal probability 1/3 correlated
4x
4
equilibrium u() = 4x
3 . Hence V (x ) 3(x+1) . Therefore V (x ) 3
x . 2
3.1.2 General Games
Theorem 1 shows mediation value class nonnegative 2 2 games finite.
next theorem show mediation value unbounded move slightly
beyond 2 2 games. particular, one players 2-player game least
three strategies, remains two strategies, mediation value already
unbounded. Similarly, three players two strategies, mediation
value unbounded. 13
Theorem 2 V (Gm1 m2 ) = every m1 , m2 2 max(m1 , m2 ) 3.
Proof: Lemma 2 suffices prove V (G32 ) = . Let x, following
parametric G23 game Figure 3,
1


a2

b1
x, 1
0, z

b2
z, 1
z 1, z 1

b3
0, 0
1, z

Figure 3
z > 2 fixed, x > z 0 < < 0.5.
first show
N (x, ) = {((1, 0)(0, 1, 0)), ((0, 1), (0, 0, 1)), ((, 1 ), (

1
x
, 0,
))}.
1+x
1+x

standard check profile N (x, ) indeed mixed-strategy equilibrium,
player 1 plays pure-strategy equilibrium, mixed-strategy equilibrium
((1, 0)(0, 1, 0)) ((0, 1), (0, 0, 1)). show player 1 plays fully mixed
1
x
(that is, assigns positive probabilities a1 a2 .) , ((, 1 ), ( 1+x
, 0, 1+x
))
mixed-strategy equilibrium. Indeed, note player 2 plays pure strategy, player
1 never indifferent a1 a2 therefore player 1 always better deviating
pure strategy. player 2 assigns positive probability b1 b2 , player 1
better playing a1 . player 2 assigns positive probability b2 b3 ,
13. results presented paper showing mediation value may , also established
assume utilities uniformly bounded, e.g. utilities interval [0, 1].

588

fiOn Value Correlation

mixed-strategy player 1 play order player 2 indifferent b2 b3
(1/2, 1/2), player 2 better playing b1 . Similarly, shown
exist equilibrium player 1 fully mixes player 2 assigns positive
probabilities pure strategies. Suppose player 2 assigns positive probabilities
1
b1 b3 . probability player 2 assigns b1 higher (lower) 1+x
,
1
x
, 0, 1+x
) equilibrium
player 1 better playing a1 (a2 ). Hence, player 2 plays ( 1+x
player 1 fully mixes, therefore standard prove player 1 must play
(, 1 ).
Next show vN (x, ) z + 2 small enough . Notice welfare
pure-strategy equilibria z + 1. welfare mixed-strategy equilibrium ((, 1
1
x
), ( 1+x
, 0, 1+x
) is:
(x + 1 ) + (1 )(z ) + x(1 )(z + 1)
=
x+1
z z +

x
.
x+1

x
Note z z + x+1
z + 1 x 0.
proceed show net games, x, vC (x, ) 0 x .
Let correlated strategy. correlated equilibrium x, following
9 inequalities satisfied (in brackets relate inequality corresponding
inequality (2)):

1. (a1 , b1 )x + (a1 , b2 ) (a1 , b3 ) 0.

(i = 1, si = a1 , ti = a2 )

2. (a2 , b3 ) (a2 , b2 ) x(a2 , b1 ) 0.

(i = 1, si = a2 , ti = a1 )

3. (a1 , b1 )

(1)(a2 ,b1 )
.


4. (a1 , b1 )

(a2 ,b1 )
1 .

5. (a1 , b2 )

(1)(a2 ,b2 )
.


(i = 2, si = b1 , ti = b3 )

6. (a1 , b2 ) (a2 , b2 ).
7. (a2 , b3 )

(i = 2, si = b1 , ti = b2 )

(1)(a1 ,b3 )
.


(i = 2, si = b2 , ti = b1 )
(i = 2, si = b2 , ti = b3 )
(i = 2, si = b3 , ti = b1 )

8. (a2 , b3 ) (a1 , b3 ). (i = 2, si = b3 , ti = b2 )
P2 P3
j
9.
i=1
j=1 (a , b ) = 1.
Set (a1 , b1 ) = P
, (a2P
, b1 ) = 22 , (a1 , b2 ) = (1 ), (a1 , b3 ) = (a2 , b2 ) = 2 , (a2 , b3 ) =
2
1
3
1 (a , b ) i=1 2j=1 (ai , bj ) let x = 412 . Let 0. nine constraints
satisfied small enough . Note lim0 x(a1 , b1 ) = . Since V (x, ) x(a1 , b1 )
obtain V (x, ) 0. 2
Theorem 3 V (Gm1 mn ) = every n 3 every m1 , m2 , . . . , mn 2.
589

fiAshlagi, Monderer, & Tennenholtz

Proof: Lemma 2 suffices prove V (G222 ) = . Consider following
three player game (Figure 4):
a1
a2

b1
0, 0, 0
0, 0, 0

b2
2, 0, 0
1, 0, 1

a1
a2

c1

b1
4, 4, 0
5, 0, 0

b2
0, 0, 1
0, 3, 0
c2

Figure 4
show vN () = 0, is, welfare every mixed-strategy equilibrium
zero. addition construct correlated equilibrium, yields strictly
positive welfare. begin proving vN () = 0. Note pure-strategy
equilibrium profiles game (a1 , b1 , c1 ) (a2 , b1 , c1 ). Obviously, every strategy
profile players 2 3 play b1 c1 respectively, player 1 plays mixed
strategy, mixed-strategy equilibrium. next show mixedstrategy equilibria game. First show mixed-strategy
equilibria least one players plays pure strategy. verify
player:
1. Assume player 3 plays c2 probability one. 1 > p2 (b1 ) > 0, p1 (a2 ) = 1,
player 3 better playing c1 . p2 (b1 ) = 1, p1 (a2 ) = 1, player 2 better
playing b2 . p2 (b2 ) = 1, player 1 indifferent. p1 (a1 ) 0.5, player 2 better
playing b1 . p1 (a1 ) < 0.5, player 3 better playing c1 .
2. Assume player 3 plays c1 probability one. p2 (b2 ) > 0, p1 (a1 ) = 1,
player 3 better playing c2 .
3. Assume player 2 plays b1 probability one. Player 3 indifferent
strategies. p3 (c2 ) > 0, p1 (a2 ) = 1, player 2 better playing b2 . p3 (c1 ) =
1 dealt previous case. Assume player 2 plays b2 probability one.
p3 (c1 ) > 0, p1 (a1 ) = 1, player 3 better playing c2 .
4. Assume player 1 plays a1 probability one. p2 (b2 ) > 0, p3 (c2 ) = 1,
player 2 better playing b1 probability one. Assume player 1 plays a2
probability one. p3 (c2 ) > 0, p2 (b2 ) = 1, player 3 better playing c1
probability one.
Next prove exist completely mixed-strategy equilibrium, (an equilibrium every player assigns positive probability strategies). Suppose
negation case. Let ((p, 1 p), (q, 1 q), (h, 1 h)) completely
mixed-strategy equilibrium, 1 > p, q, h > 0. equilibrium properties, player
2 indifferent b1 b2 given players 1 3 play (p, 1 p) (h, 1 h)
respectively. Hence 4p(1 h) = 3(1 p)(1 h), implies p = 37 . Similarly, player
3 indifferent c1 c2 given players 1 2 play (p, 1 p) (q, 1 q)
respectively. Therefore (1 p)(1 q) = p(1 q) yielding p = 0.5, contradiction.
Therefore, exist completely mixed-strategy equilibrium.
proved vN () = 0. remains prove exists correlated equilibrium
strictly positive welfare; imply mediation value infinity. Let
590

fiOn Value Correlation

(S) following correlated strategy. (a1 , b2 , c1 ) = (a2 , b2 , c1 ) = (a1 , b1 , c2 ) =
(a2 , b1 , c2 ) = 0.25 S, (s) = 0. proceed prove
correlated equilibrium. Indeed, observe following inequalities, define
correlated equilibrium satisfied (as usual, brackets relate inequality
corresponding inequality (2)):
1. (a1 , b2 , c1 )(2 1) + (a1 , b1 , c2 )(4 5) 0.

(i = 1, si = a1 , ti = a2 )

2. (a2 , b2 , c1 )(1 2) + (a2 , b1 , c2 )(5 4) 0.

(i = 1, si = a2 , ti = a1 )

3. (a1 , b2 , c1 )(0 0) + (a2 , b2 , c1 )(0 0) 0.

(i = 2, si = b2 , ti = b1 )

4. (a1 , b1 , c2 )(4 0) + (a2 , b1 , c2 )(0 3) 0.

(i = 2, si = b1 , ti = b2 )

5. (a1 , b2 , c1 )(0 1) + (a2 , b1 , c1 )(1 0) 0.

(i = 3, si = c1 , ti = c2 )

6. (a1 , b1 , c2 )(0 0) + (a2 , b1 , c2 )(0 0) 0. (i = 3, si = c2 , ti = c1 )
2
proof Theorem 3 based showing exists three-player nonnegative game, utilities zero, yet welfare every mixed-strategy
equilibrium zero. next lemma shows phenomenon happen two
player game.
Lemma 4 Let Gnm , n, 1. vN () = 0 implies utilities zero.
is, = 1, 2,
ui (t1 , t2 ) = 0 t1 1 , t2 2 .

Proof: proof induction total number pure strategies n + game.
First note assertion holds games, Gnm n + = 2
case player exactly one strategy. Let k 2, assume assertion holds
every game, Gnm n + k.
Let Gnm game n + = k + 1 vN () = 0.
prove zero game. k + 1 3, exists least one player
one strategy; without loss generality, player 1 player.
Let p = (p1 , p2 ) mixed-strategy equilibrium , is, p N ().
vN () = 0, welfare p equals 0. is, u(p) = u1 (p) + u2 (p) = 0. utilities
nonnegative,
u1 (p) = 0 = u2 (p).
(17)
Let s1 arbitrary fixed strategy player 1 p1 (s1 ) > 0, let s2
arbitrary fixed strategy player 2 p2 (s2 ) > 0.
Claim 1: (i) u1 (t1 , s2 ) = 0 t1 1 . (ii) u2 (s1 , t2 ) = 0 t2 2 .
Proof Claim 1. Let t1 1 . Since (p1 , p2 ) mixed-strategy equilibrium
, u1 (t1 , p2 )
P
1
1
2
1
1
1
2
1
1
2
u (p , p ) = 0. Since u nonnegative, u (t , p ) = 0. Since u (t , p ) = t2 2 p2 (t2 )u1 (t1 , t2 ),
u1 nonnegative, p2 (s2 ) > 0, u1 (t1 , s2 ) = 0. proves (i). (ii) similarly proved. 2
Consider game G(n1)m obtained removing s1 1 ; game
strategy set player 1 1 = 1 \{s1 } strategy set player 2 remains 2 .
591

fiAshlagi, Monderer, & Tennenholtz

slight abuse notations utility functions also denoted u1 u2 . Claim
2: utilities zero.
Proof Claim 2 :
Assume negation utilities zero. induction hypothesis
exists mixed-strategy equilibrium (q 1 , q 2 ) N ( )
u(q 1 , q 2 ) > 0.

(18)

Extend q 1 mixed strategy r1 player 1 defining r1 (s1 ) = 0. Obviously,
ui (r1 , q 2 ) = ui (q 1 , q 2 ) = 1, 2 therefore (18) implies
u(r1 , q 2 ) > 0.

(19)

Since vN () = 0, (19) implies (r1 , q 2 ) mixed-strategy equilibrium . However, since r1 coincide q 1 1 , r1 (s1 ) = 0, (q 1 , q 2 ) mixed-strategy equilibrium
, player 2 profitable deviation q 2 player 1 uses r1 .
Therefore, player 1 must profitable deviation r1 player 2 uses q 2 .
particular, player 1 must pure-strategy profitable deviation. Since (q 1 , q 2 )
mixed-strategy equilibrium , potential pure-strategy profitable deviation
player 1 s1 . Therefore,
u1 (s1 , q 2 ) > u1 (r1 , q 2 ),

(20)

u1 (s1 , q 2 ) > 0.

(21)

implies particular
Next show (s1 , q 2 ) N (). Since (q 1 , q 2 ) mixed strategy equilibrium ,
u1 (q 1 , q 2 ) u1 (t1 , q 2 ) every t1 1 , t1 6= s1 . Since r1 (s1 ) = 0, u1 (r1 , q 2 ) u1 (t1 , q 2 )
every t1 1 , t1 6= s1 . Therefore, (20), s1 best-response q 2 . (ii) Claim
1, every pure-strategy player 2 best-response s1 hence, every mixedstrategy player 2 best response s1 , particular q 2 . Therefore, (s1 , q 2 )
indeed mixed-strategy equilibrium . Since vN () = 0, u1 (s1 , r2 ) = 0, contradicting
(21). complete proof Claim 2.2
Claim 2 (ii) Claim 1, (s1 , t2 ) N () every t2 2 . Since vN () = 0,
1
u(s , t2 ) = 0 every t2 2 . Hence, = 1, 2, ui (s1 , t2 ) = 0 every t2 2 .
ui identically zero , follows = 1, 2, ui (t1 , t2 ) = 0 every t1 1
every t2 2 .2
3.2 Enforcement Value
next theorem shows enforcement value unbounded even classes small
games.
Theorem 4 EV (Gm1 mn ) = every n 2 every m1 , m2 2.
Proof: Lemma 2 suffices prove theorem case n = 2, m1 = 2, m2 = 2.
Consider following parametric Prisoners Dilemma game x , x > 1, given Figure 5:

592

fiOn Value Correlation

1


a2

b1
1, 1
0, x + 1

b2
x + 1, 0
x, x

Figure 5
Lemma 1, every game unique correlated equilibrium (a1 , b1 ) whose welfare
2. However every x 1, opt(x ) = 2x. Therefore EV (x ) x . 2
proof Theorem 4 based constructing Prisoners Dilemma games
parameter x. particular every player weakly dominant strategy one
games. next theorem shows even ruling weakly dominant strategies,
enforcement value unbounded.
Theorem 5 EV ({| G222 , player weakly dominant strategy} = .
Proof: Consider family parametric games z, (Figure 6), z =
1


a2

a1
4 , 4 , 4
4 + , 4, 4

a2
4, 4 + , 4
0, 0, z

1


a2

a1

a1
4, 4, 4 +
0, z, 0

1
2

a2
z, 0, 0
0, 0, 0
a2

Figure 6
First observe opt(z, ) = z every 0 < 0.25. order prove result
b weak duality theorem every feasible solution (, )
use dual program D.
b satisfies vC (). Let x1 , x2 , x3 denote 1 (a1 |a2 ), 2 (a1 |a2 )
dual problem
3 (a1 |a2 ) respectively. Let y1 , y2 , y3 denote 1 (a2 |a1 ), 2 (a2 |a1 ) 3 (a2 |a1 ) respectively.
dual constraints written following way (recall z = 12 ):
2y1 + 2y2 + 2y3 + 12 3,
4y1 4y3 2x2 + 12 + ,
4y2 4y3 2x1 + 12 + ,
4y1 4y2 2x3 + 12 + ,
1
1
2 y1 + 4x2 + 4x3 + 2 ,


1
1
2 y2 + 4x1 + 4x3 + 2 ,


1
1
2 y3 + 4x1 + 4x2 + 2 ,


1
1
1
x1 + 2 x2 + 2 x3 + 0.
2


Set y1 = y2 = y3 = x1 = 0, = 1 , x2 = x3 = 412 , observe feasible
solution every sufficiently small > 0. Note EV (z, ) z . However z = 1
0, completes proof. 2
593

fiAshlagi, Monderer, & Tennenholtz

4. Simple Congestion Games
section explore mediation enforcement values simple congestion games.
first need notations definitions.
simple congestion form F = (N, M, (wj )jM ) defined follows. N nonempty
set players; keep assumption that, whenever convenient, N = {1, 2, . . . , n},
n = |N | number players. nonempty set facilities; Unless otherwise
specified assume = {1, 2, ..., m}. j , let wj Rn facility payoff
function, every 1 k n, wj (k) denotes payoff user facility j,
exactly k users. congestion form nonnegative every j , wj
nonnegative. Let Q class nonnegative simple congestion forms denote
Qnm Q class nonnegative simple congestion forms n players
facilities. Every simple congestion form F = (N, M, (wj )jM ) defines simple congestion
game F = (N, (S )iN , (ui )iN ), N set players, = every player
N , utility functions (ui )iN defined follows.
Let = . every = (A1 , A2 , ..., ) every j , let j (A) =
|{i N : Ai = j}| number users facility j. Define ui : R
ui (A) = wAi (Ai (A)).

(22)

say facility j non-increasing wj (k) non-increasing function k.
Define QN nm Qnm follows:
QN nm , {F Qnm | facilities F non-increasing}
call facility j linear exist constant dj wj (k + 1) wj (k) = dj
every k 1, wj (k) = dj k + j every k 1, j constant.
Let F simple congestion form n players facilities. congestion vector
= (n, m) isPan m-tuple = (j )jM , 1 , 2 , ..., Z (the set nonnegative
integers)
j=1 j = n. represents situation j players choose facility
j. Every strategy profile P uniquely determines congestion vector . Note




m2
1
j=1 j
n1 n
n m1
strategy profiles game F correspond
2
congestion vector , denote B set strategy profiles. Thus B =
{A S| = }. Given congestion vector ,
P strategy profiles B
welfare denote u(). u() = {jM | j >0} j wj (j ).
say congestion vector equilibrium every strategy profile B purestrategy equilibrium. congestion form called facility symmetric wj = wk j, k .
Obviously, facility symmetric congestion form induces symmetric game strategic form.
Let Inm Qnm defined
Inm = {F Qnm | F facility symmetric }.
Define nm Inm follows
nm = {F Inm | facilities F non-increasing}.
594

fiOn Value Correlation

4.1 Mediation Value
Although congestion games especially interesting number players large,
begin presenting results case two players, extending
upon results previous section. Following that, consider general
n-player case.
4.1.1 two-player case (n = 2)
Theorem 1 provides 43 tight upper bound mediation value class games
G22 . Hence, 43 obviously upper bound mediation value nonnegative simple
congestion games two players two facilities, i.e., games generated congestion
forms Q22 . turns bound tight:
Theorem 6 V ({F |F Q22 }) = 43 .
Proof: Let Fx , x > 1, following simple congestion form: = {a1 , a2 }, wa1 = (x, 0),
wa2 = (1, x 1) . games Fx , x > 1 obtained games, x defined
proof Theorem 1 (See Figure 2) renaming strategies player 2. Hence,
proved Theorem 1, V (Fx ) 34 x . 2
Next consider general case two players choose among
facilities:
Theorem 7 V ({F |F Q2m }) 2 every 2.
Proof: Let F Q2m . Rosenthal (1973) exists pure-strategy equilibrium F .
Let fixed pure-strategy equilibrium largest welfare,
u(A) u(D) every pure-strategy equilibrium D.

(23)

Let j k facilities players 1 2 choose respectively. prove
separation two cases, j = k j 6= k.
Case 1: j = k. prove theorem case separation cases wj (1) > wj (2)
wj (1) wj (2).
Case 1.1: wj (1) > wj (2). case show j strictly dominant strategy,
implies Lemma 1 V (F ) = 1. Indeed, let h 6= j facility. order
prove j strictly dominates h show following three inequalities
hold: wh (1) < wj (2), wh (2) < wj (1), wh (1) < wj (1). However, since wj (1) > wj (2),
first two inequalities proved. first prove wh (1) < wj (2): Since
pure strategy equilibrium players choose j, wh (1) wj (2). Suppose
negation wh (1) = wj (2). Since pure-strategy equilibrium, wj (2) wl (1)
every l 6= j, therefore, wh (1) wl (1) every l 6= j. If, addition, wj (1) wh (2),
strategy profile one player chooses h, player chooses j purestrategy equilibrium obtains larger welfare contradicting (23). Therefore,
wh (2) > wj (1). implies strategy profile players choose h
pure-strategy equilibrium obtains larger welfare A, contradiction
(23). Therefore wh (1) < wj (2).
595

fiAshlagi, Monderer, & Tennenholtz

next show wh (2) < wj (1). wh (2) wj (1) strategy profile
players choose h pure-strategy equilibrium 2wh (2) 2wj (1) > 2wj (2) contradicting
(23). completes proof Case 1.1.
Case 1.2: wj (1) wj (2). Let B pure strategy profile maximum
welfare obtained. is, u(B) = opt(F ). order prove theorem suffices
prove
u(B) 2u(A).
(24)
Suppose B players choose distinct facilities (one facilities j).
Since pure-strategy equilibrium, wl (1) wj (2) every l 6= j. Therefore, since
also wj (1) wj (2), u(B) u(A) particular (24) holds. Suppose two players
choose facility B, denote facility s. = j B = (24) holds.
Therefore assume 6= j. ws (2) wj (2), u(B) u(A), implies (24). Therefore
assume ws (2) > wj (2) particular ws (2) > wj (1). Since wj (2) wl (1)
every facility l, l 6= j, B pure-strategy equilibrium u(B) > u(A), contradicting
(23). Hence proof Case 1.2 complete.
Case 2: j 6= k. Let B arbitrary pure strategy profile maximum
welfare obtained. is, u(B) = opt(F ). suffices prove (24) holds.
Recall pure-strategy equilibrium. B player chooses different
facility, u(B) u(A) particular (24) holds. Therefore assume B
players choose facility l (l j k). claim
wl (2) max{wj (1), wk (1)}.

(25)

Indeed, (25) hold, B pure strategy equilibrium, player B
want deviate either j k, want deviate facility
pure strategy equilibrium. Since u(B) > u(A) contradicts (23). Since
u(B) = 2wl (2) max{wj (1), wk (1)} u(A), (25) implies (24). 2
Notice Theorems 6 7 imply correlation help congestion games
two players. next theorem shows correlation cannot help increasing social
welfare facilities non-increasing:
Theorem 8 V (QN 2m ) = 1 every 2.
Proof: Let F QN 2m . Let j wj (1) wl (1) l .
wj (2) > wl (1) l 6= j, j strictly dominant strategy, implies Lemma 1
V (F ) = 1. Suppose exist facility k,k 6= j wj (2) wk (1).
Choose k wk (1) maximal, wl (1) wk (1) every l, l 6= j.
Therefore, strategy profile one player chooses j player chooses
k pure strategy equilibrium obtains maximal welfare. existence
strategy profile implies V (F ) = 1. 2.
4.1.2 Simple congestion games n players
Theorems 2 3 Section 3.1.2 imply correlation unbounded value
considering arbitrary games. next theorem shows correlation similar effects
context simple congestion games. particular, show mediation value
unbounded presence additional player:
596

fiOn Value Correlation

Theorem 9 V ({F |F QN 32 }) = .
Proof: Consider following family forms F , 0 < 0.5: = {a1 , a2 }, wa1 =
(z, 4, 4 ), wa2 = (4 + , 0, 0), z = 12 . Observe games, F games
given proof Theorem 5 (Figure 6), monotonicity condition satisfied
every 0 < 0.5.
first show every sufficiently small , welfare every mixed-strategy
equilibrium lower 13. construct, every sufficiently small ,
correlated equilibria F welfare correlated equilibria approaches
infinity 0.
Note pure-strategy equilibria strategy profiles two players
play a1 one player plays a2 . welfare strategy profiles 12 + ,
less 13.
proceed deal mixed-strategy equilibria least one player
play pure strategy. equilibrium, player plays strategy a2 probability
one one players it, mixed-strategy equilibria
two players plays a1 probability one, i.e., equilibrium
pure.
Suppose player plays a1 probability one. Note utility matrix
two players given Figure 7.
1


a2

a1
4 , 4
4 + , 4

a2
4, 4 +
0, 0

Figure 7
Therefore standard check exists unique mixed-strategy equilibrium
one players play a1 probability 1, equilibrium,
player player 1 is:
2

2

((1, 0), (
,
), (
,
)).
2+ 2+ 2+ 2+
Since game symmetric, mixed-strategy equilibria three permutations
vector. welfare mixed-strategy equilibria
3(4 )(

2
2 2
2
) + 2(12 + )
) =
+z(
2
2+
(2 + )
2+
48 + 36 + (z + 4)2
(2 + )2

12.25,

0.
Consider completely mixed-strategy equilibrium. is, every player assigns positive
probabilities facilities. Denote equilibrium ((p, 1 p), (q, 1 q), (h, 1
h)) 0 < p, q, h < 1. player 1 indifferent a1 a2 given players 2
3 play (q, 1 q) (h, 1 h) respectively,
(4 )qh + 4(1 q)h + 4(1 h)q + z(1 h)(1 q) = (4 + )qh.
597

(26)

fiAshlagi, Monderer, & Tennenholtz

Note similar equalities hold players 2 3, q h exchanged p
respectively. every fixed h exists unique q solves equation (26). Therefore,
permuting names players, p = q = h. enables us reduce Equation (26)

(z 8 2)p2 + (8 2z)p + z = 0,
(27)
yields


2z 8 64 + 8z
p=
2z 16 4

(28)

solution (28) satisfy 0 < p < 1. Therefore, welfare
completely mixed-strategy equilibrium
3p(1 p)2 z + 3p2 (1 p)(12 + ) + p3 (12 3) =
(3z 24 5)p3 (6z 36 + 3)p2 + 3zp.

(29)

Let = (36 3)p2 (24 + 5)p3 B = 3zp3 6zp2 + 3zp. Hence (29) = + B.
show 12 B 0 0. Observe p 1 0. implies

12 0. Observe z = z. Thus, (28) small enough
p p =

1
2z 8z 4
2z

=1


8 43
.
2 z

simplicity set c =

3


8
2 .

Therefore

3

3

B 3z(1 cz 4 )3 6z(1 cz 4 )2 + 3z(1 cz 4 ) =
3

6

9

3

6

3

3z[1 3cz 4 + 3c2 z 4 c3 z 4 ] 6z[1 2cz 4 + c2 z 4 ] + 3z(1 cz 4 ) =
2

5

3c2 z 4 3c3 z 4 =
10

3c2 3c3 4 0
0.
completes proof vN (F ) 13 every sufficiently small enough .
complete proof theorem construct, every sufficiently small ,
correlated equilibria F welfare correlated equilibria approaches
infinity 0.
order correlated strategy correlated equilibrium F , following
inequalities satisfied (in brackets relate inequality corresponding
inequality (2)):14
1. 2(1, 1, 1) + 4(1, 2, 1) + 4(1, 1, 2) + z(1, 2, 2) 0.
2. 2(2, 1, 1) 4(2, 2, 1) 4(2, 1, 2) z(2, 2, 2) 0.
3. 2 + 4(2, 1, 1) + 4(1, 1, 2) + z(2, 1, 2) 0.

(i = 1, si = a2 , ti = a1 )

(i = 2, si = a1 , ti = a2 )

4. 2(1, 2, 1) 4(2, 2, 1) 4(1, 2, 2) z(2, 2, 2) 0.
5. 2(1, 1, 1) + 4(1, 2, 1) + 4(2, 1, 1) + z(2, 2, 1) 0.
14. Here, slightly abuse notation letting (i, j, k) = (ai , aj , ak ).

598

(i = 1, si = a1 , ti = a2 )

(i = 2, si = a2 , ti = a1 )
(i = 3, si = a1 , ti = a2 )

fiOn Value Correlation

6. 2(1, 1, 2) 4(1, 2, 2) 4(2, 1, 2) z(2, 2, 2) 0.
P8
7.
i=1 = 1.

(i = 3, si = a2 , ti = a1 )

sufficiently small , inequalities satisfied , (1, 1, 1) =
1
3
(2, 1, 2) = (2, 2, 2) = 0, (2, 1, 1) = (1, 1, 2) = 4 , (2, 2, 1) = (1, 2, 2) = 2 .
Note z(2, 2, 1) = 1 0, u() z(2, 2, 1). Therefore
u() 0. 2
conjecture Theorem 9 holds players and/or facilities. is,
V ({F |F QN nm }) = every n 3 every 2. One way prove
conjecture modify Lemma 2, used prove analogous extensions general
games (see e.g., Theorems 2 3).
next theorem deals linear facilities:
Theorem 10
V ({F |F QN n2 , facilities F linear})
every n 2,


= ( 5 + 1)/2.

(30)

following lemma (Schrijver, 1986, page 61) used proof Theorem 10.
Lemma 5 (Farkas Lemma) Let s, positive integers. Given matrix dimensions
vector b Rs , one one following systems solution:
(i)

Ax b,

x Rt ;

(ii)

yT = 0,

yT b > 0,

Rs+ ,

Rs+ denotes set nonnegative vectors Rs .
Proof Theorem 10: Consider nonnegative, non-increasing linear congestion
form = {f, g}, wf wg facility payoff functions f g
respectively, wf (k) = df k + f wg (k) = dg k + g . Obviously, df , dg 0. Denote
induced congestion game. Assume w.l.o.g.
wf (1) wg (1).

(31)

Denote k = (nk, k) congestion vector nk players choose f k players
choose g. Let , 0 n, largest integer equilibrium. Since
equilibrium, wg ( ) wg (n + 1), monotonicity condition,
wg (j) wg (n j + 1) j .
Recall welfare denoted u( ).
Claim 1: V ()) = 1, whenever {n, 0}.
599

(32)

fiAshlagi, Monderer, & Tennenholtz

Proof: Suppose = n. Since (0, n) equilibrium, wg (n) wf (1). Therefore, (31),
wg (n) wg (1). monotonicity condition, every k, u(k ) = (n k)wf (n k) +
kwg (k) (n k)wf (1) + kwg (1). Therefore, u(k ) nwg (n) = u(n ). Hence, maximal
welfare, opt() attained equilibrium n , implies mediation value
equals 1.
Suppose = 0. Since (n, 0) equilibrium, wf (n) wg (1). claim
wf (n) > wg (1).

(33)

Indeed, assume negation wf (n) = wg (1). Since (n 1, 1) equilibrium, either
wf (n 1) < wg (2) wg (1) < wf (n). Therefore, wf (n 1) < wg (2),
monotonicity condition, wf (n) < wg (1), yields contradiction. Therefore (33) holds,
implies choosing f strictly dominates choosing g . Therefore, Lemma 1,
every correlated equilibrium pure strategy profile, generates congestion
vector 0 . Hence, mediation value equals 1.2
Claim 1, rest proof assume w.l.og.
1 n 1.

(34)

Note wg (1) < wf (n), choosing f strictly dominates choosing g, implies
mediation value equals 1. Therefore assume w.l.o.g. rest proof
wg (1) wf (n).

(35)

following key claim proof.
Claim 2: u(j ) u( ) every j .
Proof: Let j < . Since facilities linear, wg (j) = wg ( ) + ( j)dg wf (n j) =
wf (n ) ( j)df . Hence,
u( ) u(j ) =
swg ( ) + (n )wf (n ) j(wg ( ) + ( j)dg ) (n j)(wf (n ) ( j)df ) =
wf (n )(j ) + wg ( )( j) j( j)dg + (n j)( j)df =
( j)(wg ( ) wf (n ) + (n j)df jdg )
( j)(wf (n + 1) wf (n ) + (n j)df jdg ),

(36)

last inequality follows since equilibrium. Since facilities linear,
RHS(36) = ( j)((n j 1)df jdg ) =
( j)(wf (1) wf (n j) wg (1) + wg (j + 1)).

(37)

Since j < suffices show (wf (1) wf (n j) wg (1) + wg (j + 1)) 0.
inequality follows (31) (32). 2
Next define auxiliary strategy profile. Denote q mixed-strategy profile
players 1, 2, . . . , n 1 choose f probability 1,
w (1)w (n)
+ 1 players choose g probability p = g (df +dfg ) , i.e., plays mixed
600

fiOn Value Correlation

strategy (1 p, p). (35), p 0. see p 1, note equilibrium,
wg (1) wf (n) wg (1) wg ( + 1) + wf (n ) wf (n) = sdg + sdf .
Claim 3: (i) q mixed strategy equilibrium; (ii) u(q) = nwf (n) + pdf ((n 1)( +
1) + ( + 1) ).
Proof: (i) first show ui (qi , f ) ui (qi , g) every player chooses f
probability 1. Let k = + 1. have, ui (qi , f ) ui (qi , g) =
k
X
k j
p (1 p)kj (wf (n j) wg (j + 1)) =
j
j=0

k
X
k j
p (1 p)kj (wf (1) (n j 1)df wg (1) + jdg ) =
j
j=0

wf (1) wg (1) (n 1)df + k pdf + k pdg .

(38)

Since wf (n) = wf (1) (n 1)df k > k 1,
RHS(38) = wf (n) wg (1) + (k 1)pdf + (k 1)pdg 0,
w (1)w (n)

g
f
last inequality follows since p 1, p = (k1)(d
.
f +dg )
next show every player plays mixed strategy (1 p, p) indifferent
f g. Observe ui (qi , f ) ui (qi , g) =


n
X
k1 j
p (1 p)k1j (wf (n j) wg (j + 1)) =
j
j=0


n
X
k1 j
p (1 p)k1j (wf (1) (n j 1)df wg (1) + jdg ) =
j
j=0

wf (1) wg (1) (n 1)df + (k 1)pdf + (k 1)pdg =
wf (n) wg (1) + (k 1)pdf + (k 1)pdg = 0.
(ii) Similar calculations part (i) yield expected payoff
n 1 players choose f probability one equals wf (n) + ( + 1)pdf ,
expected payoff + 1 players equals wf (n) + pdf . Therefore
u(q) = nwf (n) + pdf ((n 1)( + 1) + ( + 1) ). 2
proceed main proof. Define
Z = max{u( ), u(q)}.

(39)

u(k ) Z every 0 k n, opt() Z, therefore, vc () opt()
max{u( ), u(q)} implying V () . Therefore assume rest
proof exist integer k, 0 k n
u(k ) > Z..
601

(40)

fiAshlagi, Monderer, & Tennenholtz

b case, dual problem denoted
proceed utilizing dual program D.
b follows:



min





s.t.
b
: (f |g) 0
N,




(g|f ) 0
N,



P









(t |s )[u (t , ) u (s , )] + u(s) S,
every i, ti = f si = g, ti = g si = f .
Recall weak duality theorem, every feasible solution dual problem,
(, ), vC (). Therefore, order complete proof, suffices show
b = Z, Z defined
exists feasible solution dual problem
(39).
begin restricting range variables dual problem (D) .
c1 ) obtained (D)
b letting = Z, (g|f ) = 0
specifically, following system (D

x = (f |g) every N , i.e., x remains variable:


min





s.t.
c1 : = Z,




k(wf (n k + 1) wg (k))x u(k )




x 0.

k = 1, ..., n,

c1 defines feasible solution, (, ) (D)
Obviously, every optimal solution, x
c1 equivalent
= Z. Since = Z, existence optimal solution
c2 ):
existence feasible solution following system constraints, (D
(
c2 : k(wf (n k + 1) wg (k))x u(k )

x 0.

k = 1, ..., n,

c2 redundant, i.e., x satisfies
next show inequality x 0
first n constraints, x 0. Recall (40) exist integer 0 k n
u(k ) > Z. Since > 1, Claim 2, k > , particular, k 1. Since k > , k
equilibrium. Since , player f wish deviate g, case k
fewer players f . Therefore, k , deviation player g f
profitable, i.e., wf (n k + 1) wg (k) > 0, since k > 0, k(wf (n k + 1) wg (k)) > 0.
c2 u( ) > Z = , x > 0. Hence
b 2
Since x satisfies constraint k
k
equivalent
n
c3 : k(wf (n k + 1) wg (k))x u(k )

602

k = 1, ..., n.

fiOn Value Correlation

c3 solution x following system
Farkas Lemma (Lemma 5),
solution, = (y1 , y2 , . . . , yn ):
Pn

Pk=1 yk k(wf (n k + 1) wg (k)) = 0,
n
c
P1 :
k=1 yk (u(k ) ) > 0,


yk 0

k = 1, ..., n.

c1 solution. Note solution
Therefore, suffices show P
c
P1 , least one variable yk positive. Therefore, suffices toPprove
c1 , also probability vector, i.e., n yk = 1. Let
exist solution P
k=1
c1 . prove
probability vector satisfies first constraint P
satisfy second constraint, is, show
n
X

yk u(k ) .

(41)

k=1

Let random variable whichPyk = P (Y = k), k = 1, ..., n, recall
expected value satisfies E[Y ] = nk=1 kyk . first derive following useful
inequalities given Claim 4:
Claim 4:
(i)
E[Y ]

wg (1) wf (n) + df + dg
.
(df + dg )

(42)

(ii)
n
X

yk u(k ) = E[Y ](wf (1) wf (n)) + nwf (n).

k=1

c1 satisfied,
Proof: (i) Since first constraint P
0=

n
X

yk k(wf (n k + 1) wg (k)) =

k=1
n
X

yk k(wf (n) + (k 1)df wg (1) + (k 1)dg ) =

k=1

E[Y ](wf (n) wg (1) df dg ) + E(Y 2 )(df + dg ).
Since E[Y 2 ] E[Y ]2 ,
E[Y ](wf (n) wg (1) df dg ) + E[Y ]2 (df + dg ) 0.
Since E[Y ] > 0, dividing sides E[Y ] obtain (42).
603

(43)

fiAshlagi, Monderer, & Tennenholtz

(ii)
n
X

yk u(k ) =

k=1

n
X

yk (kwg (k) + (n k)wf (n k)) =

k=1
n
X

yk (kwf (n k + 1) + (n k)wf (n k)),

k=1

c1 satisfied y. Therefore,
last equality holds first constraint P
n
X

yk u(k ) =

k=1

wf (1)E[Y ] +

n
X

yk k(wf (1) (n k)df ) +

k=1
n
X

n
X

yk (n k)wf (n k) =

k=1

yk (n k)(wf (n k) kdf ) = wf (1)E[Y ] + wf (n)

k=1

n
X

yk (n k)

k=1

= E[Y ](wf (1) wf (n)) + nwf (n),
proves (43) holds. 2
proceed prove (41) holds. plugging (43) (41), equivalently
prove
E[Y ](wf (1) wf (n)) + nwf (n) ,
(44)
distinguish following two cases:
Case 1: p < 1/. First show
E[Y ] .
p < 1/, (wg (1) wf (n))/(df + dg ) /. Hence, (42), E[Y ]
Since = 1 + 1, = + . Since 1, (45) holds.

(45)



+ 1.

Suppose negation (44) hold, i.e., E[Y ](wf (1) wf (n)) + nwf (n) > .
Note = Z u( ). Therefore, E[Y ](wf (1) wf (n)) + nwf (n) > u( ).
definition u( ) linearity facilities,
E[Y ] >

wg ( ) + (n )wf (n ) nwf (n)
=
(n 1)df

wg ( ) + (n )(wf (n) + sdf ) nwf (n)
.
(n 1)df

(46)

Since equilibrium,
RHS(46)

wf (n + 1) + (n( 1) )wf (n) + (n ) df
.
(n 1)df

Since (n( 1) )wf (n) 0,
RHS(47)

(wf (n + 1) wf (n)) + (n ) df
=
(n 1)df
604

(47)

fiOn Value Correlation

( 1)df + (n ) df
(n 1)
=
= .
(n 1)df
n1
Therefore, (44) hold,
E[Y ] > ,

(48)

E[Y ] p + 1.

(49)

contradicting (45).
Case 2: p 1/.
(42) since

wg (1)wf (n)
(df +dg )

= p,

Suppose negation second constraint satisfied, i.e., RHS(43) = E[Y ](wf (1)
wf (n)) + nwf (n) > . Note u(q). Therefore
E[Y ] >

u(q)
.
(wf (1) wf (n)) + nwf (n)

Hence, part(ii) Claim 3 linearity facilities
E[Y ] >

nwf (n) + pdf ((n 1)( + 1) + ( + 1) ) nwf (n)
=
(n 1)df
n( 1)wf (n) + pdf ( + 1)(n 1)
.
(n 1)df

(50)

Since n( 1)wf (n) 0 p 1/
RHS(50) + 1.
Hence, E[Y ] > + 1 contradicting (49) since p 1.
completes proof theorem. 2
Theorem 10 derived, every n 2, upper bound mediation value
class games, ({F |F QN n2 , facilities F linear}). n = 2,
Theorem 8, know mediation value equals 1. Unfortunately, know
mediation value classes games n 3. However, example
shows that, n = 3, mediation value class least 89 ; Hence,
mediation value 1.125 1.618.
Example 1 Let n = 3, = {f, g}, wf = (24, 12, 0), wg = (8, 8, 8). easily
verified vN () = 32, obtained, e.g., pure-strategy equilibrium
two players choose f player chooses g. Consider correlated strategy ,
assigns probability 61 every strategy profile players choose
facility. easily verified correlated equilibrium welfare
36. Hence, mediation value least 36
32 .
605

fiAshlagi, Monderer, & Tennenholtz

next discussion useful recall price stability cost game
ratio minimal cost mixed-strategy equilibrium minimal cost,
price stability utilities utility game ratio maximal
welfare maximal welfare mixed-strategy equilibrium.
Let n, 2. Another open question us estimating mediation value, simple
congestion games n players, facilities, nonnegative, non-increasing,
linear facilities. One think upper bound class derived
results price stability analogous class congestion games
costs. Indeed, Christodoulou Koutsoupias (2005) proved price stability
class congestion games cost n players, facilities, nonnegative,
linear, non-decreasing cost functions bounded 1.6. result proven
price stability utilities class games would implied,
particular, mediation value class bounded 1.6,
mediation value cannot exceed price stability utilities.15 However, discussed
introduction, results price stability cost models cannot transformed
results price stability utilities utility models. illustrate this,
show next example price stability utilities class {F |F
QN 22 , facilities F linear} equals .16
Example 2 Let x, > 0 fixed. Let N = {1, 2}, = {f, g} every > ,
let wfd = (x + + , x + ), let wgd = (x, x), associated congestion game.
Since f strictly dominates g, strategy profile players choose f
mixed-equilibrium game . welfare obtained equilibrium 2x + 2.
strategy profile attaining maximal welfare one player chooses
different facility yielding welfare 2x + + d. Since 2x++d
2x+2 , price
stability {F |F QN 22 , facilities F linear} equals .
Theorems 9, Example 1 show correlation helpful context (even
non-increasing) congestion games. next theorem shows correlation helpful
even narrow class facility symmetric forms nonnegative non-increasing
facilities:
Theorem 11 V ({F |F n2 }) > 1 every n 4.
Proof: Let n 4. suffices prove exists F n2 V (F ) > 1.
Let 0 < < 1 fixed sufficiently small, consider following form, F : w1 =
w2 = (10n, 1, . . . , 1, 1 , 0). Note maximal welfare obtained strategy
profile exactly n 1 players choose facility, i.e., congestion
vector 1 = (1, n 1) 2 = (n 1, 1). Let L set strategy profiles
congestion vector 1 2 . Note exist exactly 2n strategy profiles L. Let
1
correlated strategy every strategy profile L played probability 2n
.
Since convex combination welfare maximizers, maximal welfare attained
15. Moreover, since 1.6 , result would saved us tedious proof Theorem 10.
16. proof Christodoulou Koutsoupias (2005) elegantly uses fact every congestion game
exact potential. known us wether potential approach could simplify proof
Theorem 10. However, turns technique applied directly setting
essentially bound total cost players every facility separately given profile.

606

fiOn Value Correlation

. claim correlated equilibrium. Indeed, (2), easily verified
F exists unique constraint satisfied order guarantee
correlated equilibrium. constraint is: 10n+((1)1)(n1)
0, indeed satisfied
2n
chose sufficiently small. Hence correlated equilibrium.
order prove V (F ) > 1, suffices prove mixed-strategy equilibrium obtains maximal welfare. First, note strategy profiles L obtain
maximal welfare. Hence, pure-strategy equilibrium obtains maximal welfare,
must belong L. However, claim every strategy profile L equilibrium;
Indeed player chooses facility chosen n 1 players better deviating
facility since utility increase .
Therefore, remains show every mixed-strategy equilibrium, p = (p1 , . . . , pn ),
least one player assigns positive probability facility, exists
least one pure strategy profile, s, L, played positive probability,
p (s) > 0, p correlated strategy associated p. Indeed, let p
mixed-strategy equilibrium, assume w.l.o.g. player assigns positive probability
facilities, is, pi (1), pi (2) > 0. Assume negation p (t) > 0 implies L,
let strategy profile p (s) > 0. Since pi (1), pi (2) > 0, p (1, si ) > 0
p (2, si ) > 0. Therefore, (1, si ) L (2, si ) L, impossible since n 4. 2
restrict assumptions Theorem 11 requiring concavity condition correlation cannot help anymore. first define concavity:
Let n 2. function v : {1, 2, ..., n} R+ concave every integer k, 2 k < n,
v(k + 1) v(k) v(k) v(k 1).
Theorem 12 Let n 2, let F nm . Define v(k) = kw(k) every
1 k n, w common facility payoff function, is, w = wj every
j . v concave, exists pure-strategy equilibrium F obtains
maximal welfare. Consequently, V (F ) = 1.
Proof: convenient also define v(0) = 0. first define operator congestion
vectors. Let = (1 , 2 , ..., ) congestion vector let j, l pair
distinct facilities. Let [j, l] congestion vector obtained replacing j
+
+
l j = j 2 l e l = b j 2 l c, respectively, bxc (dxe) denotes
Pm largest
(least) integer higher (lower) x. Observe u() =
i=1 v(i ),
therefore, concavity v,
u( [j, l]) u().

(51)

Next describe finite sequence congestion vectors, one
obtains maximal welfare last one also equilibrium. that, set k1 =
n
n
bm
c, k2 =
e, note every congestion vector coordinate either k1
k2 equilibrium.
Pick congestion vector obtains maximal welfare. equilibrium,
done. Otherwise, coordinates k1 k2 . particular, exist
two distinct coordinates j l j k1 , l k2 , least one inequality
strict. Construct = [j, l]. (51), u( ) u(), therefore, obtains
maximal welfare. equilibrium done. Otherwise let = repeat
607

fiAshlagi, Monderer, & Tennenholtz

process. sequence terminate, eventually reaches every
coordinates equals k1 k2 , i.e., reaches equilibrium contradiction. Therefore,
sequence terminates finite number stages, hence last congestion vector
sequence equilibrium attains maximal welfare. 2
4.2 Enforcement Value
already know construction proof Theorem 4 enforcement
value class = {x | x > 1} Prisoners Dilemma games described Figre 5
unbounded. easily verified every game class potential game,
therefore, (Monderer & Shapley, 1996), congestion game. Moreover, easily
verified game derived simple nonnegative congestion form
Q22 . Therefore, EV ({F |F Q22 ) = . However, noticed Monderer (2007),
every congestion game nonnegative utilities represented congestion
form nonnegative non-increasing facilities. particular, shown
Prisoners Dilemma games cannot represented simple congestion forms
nonnegative non-increasing facilities. next theorem shows even
facility payoff functions restricted non-increasing, enforcement value remains
unbounded two player games.
Theorem 13 EV ({F |F QN 2m }) = every 2.
Proof: Consider games ,d > 0, given Example 2. Since using facility f strictly
dominates using g, Lemma 1, strategy profile players choose f
d)
unique correlated equilibrium . proved Example 2, opt(
, .
vC (d )
proves theorem = 2. > 2 proof obtained, above, naturally
modifying games Example 2. 2
Since Theorem 13 deals two players, deduce enforcement value
unbounded also class games generated linear facilities QN 2m
two players, every non-increasing facility linear.
Note proof Theorem 13 utilizes games, posses strictly, particular,
weakly dominant strategies. next theorem deals games without weakly dominant
strategies.
Theorem 14
(i) EV ({F |F QN 22 , weakly dominant strategies}) = 1.
(ii) EV ({F |F QN 32 , weakly dominant strategies}) = .
Proof: Assertion (i) follows Theorem 8. Assertion (ii), proof follows
observing game proof Theorem 5 simple congestion game nonincreasing facilities. 2
next theorem shows enforcement value tends number
players tends even restricting facilities symmetric non-increasing.
Theorem 15 limn EV ({F |F n2 }) = .
608

fiOn Value Correlation

Proof: Consider following family forms Fn n2 , n 3: = {f, g} let

wf = wg = ( n, 1, 0, 0, .., 0). Observe congestion vector, n = (1, n 1), obtains

maximal welfare, equals n. Therefore, order prove theorem, suffices
show
(52)
vC (Fn ) 3 every n 3.
b
Let n 3 fixed. order prove (52), use dual program (D).
weak duality theorem, every feasible solution dual problem satisfies vC (Fn ).
Therefore, suffices prove dual problem feasible solution = 3.
b described right (40).
However, case, dual problem problem,
b , dual variables except identical
find feasible solution


value x, i.e., x = (f |g) = (g|f ) player i. restriction, dual
b reduces to:
program,


min





s.t.



x 0,



( n + n 1)x + n,





2x + 1,



nn + 0,
feasible solution, (x, ) = (1, 3). Therefore (52) holds. 2
Although enforcement value may unbounded facility symmetric
congestion forms, interesting characterize congestion games correlation enables get maximal welfare. done next theorem, first need
following notations. Let F congestion form, Let congestion vector
F . Let : one one function, i.e., permutation set facilities.
define congestion vector = ( )jM follows: ( )j = (j) every facility j.
Recall B theSset strategy profiles induce congestion vector .
define, L = B set strategy profiles induce permutation
congestion vector .
Let F Inm , is, facilities F symmetric. Therefore, u() = u( )
every permutation , and, addition, every pair strategy profiles A, B L ,
u(A) = u(B).
Theorem 16 Let n, 2, let F Inm . Then, vC (F ) = opt(F )
exist congestion vector = (1 , ..., ) correlated equilibrium C(F )
following two conditions hold:
1. u() = opt(F ).
2. distributed uniformly strategy profiles L ; is, (d) = (d)
every d, L , (d) = 0 every 6 L .
following lemma (Schrijver, 1986, page 61) used proof:
609

fiAshlagi, Monderer, & Tennenholtz

Lemma 6 (Variant Farkas Lemma) Let s, positive integers. Given matrix
dimensions vector b Rs , one one following systems
solution:
(i)

Ax = b,

(ii)

yT 0,

x 0,

x Rt ;

yT b < 0,

Rs .

Proof Theorem 16:17
Clearly, exist congestion vector correlated equilibrium C(F ),
satisfy conditions, vC (F ) = opt(F ). prove direction
need Claim 1 below.
Let w common facility payoff function, is, w = wj every j .
every congestion vector define Z() follows:
Pm
P
j=1 j
k6=j (w(j ) w(k + 1))
Z() =
.
(53)
m!n
Claim 1: Suppose vC (F ) = opt(F ). Let = (1 , ..., ) congestion vector
u() = opt(F ), let correlated strategy distributed uniformly elements
L . correlated equilibrium Z() 0.
Proof: Let D,i,j set strategy profiles player chooses facility j
congestion vector . D,i,j = {d : B , di = j}.

Qm nj Pl1 k 1k6=j
k=1
Let (j) = n1
, note |D,i,j | = (j) every
l=1
1
l 1l6=j
j
N.
Since distributed uniformly elements L , correlated equilibrium

P
Pm
k6=j (w(j ) w(k + 1))
j=1 (j)
0.
(54)
|L |
Hence, suffices show LHS(54) = Z(). follows since |L | = |B |m! =
n
n n1
nPm2 j
(k1
)
j=1
= nk . 2



m!

1
2
m1
(nk)
proceed proving remaining direction. Suppose vC (F ) = opt(F ),
assume contradiction exist congestion vector correlated
equilibrium C(F ) distributed uniformly elements L
u() = opt(F ). Recall every strategy profile S, congestion vector
induced
kd number players choose facility k d. Let =

{d:u( )=opt(F )} L . negation assumption, Claim 1 implies
Z( ) < 0 every D.

(55)

utilize Lemma 6, matter set J = |D|. Define matrix
size J n(m2 m) follows:
A(d, ijk) =

[w(jd ) w(kd + 1)]1ij (d)
,
m!n

17. proof technique inspired Nau McCardle (1990).

610

(56)

fiOn Value Correlation

row corresponds strategy profile column ijk corresponds
player chooses jth strategy (facility) deviates kth strategy (j 6= k), and:

1 di = j
1ij (d) =
0 otherwise.
Every row matrix corresponds strategy profile D, turn,
). Set b(d) = Z( ) every
corresponds congestion vector = (1d , ...,

D. Note column vector x = (1, 1, ..., 1) RJ satisfies (i) Lemma 6,
b = (b(d))dD . Therefore, Lemma 6, system (ii) Lemma 6 solution.
vC (F ) = opt(F ), exists correlated equilibrium, say , satisfying u() =
vC (F ) = opt(F ). Obviously, supported D, (d) = 0 every 6 D. Let
y(d) = (d) every D, let = (y(d))dD . Since correlated equilibrium
concentrated D, yT 0, since satisfy (ii),
yT b 0.

(57)

hand, since probability distribution D, (55) holds,
X
X
(d)b(d) < 0,
y(d)b(d) =
yT b =
dD

dD

contradicting (57). Therefore, negation assumption cannot hold, theorem
proved. 2
Theorem 16 shows symmetric congestion games, conditions
theorem, correlation helps obtaining maximal welfare. next example shows
exist games mixed-strategy equilibrium useful correlated
equilibrium:
Example 3 Let F I62 . Let wj = (1.5, 1, 4, 4.5, 4.5, 3) every j = 1, 2. easy
verify maximal welfare obtained strategy profile
L1 L2 , 1 = (3, 3) 2 = (1, 5). Let correlated strategy,
distributed uniformly L2 . checked correlated equilibrium. Hence,
Theorem 16, vC (F ) = opt(F ). hand, note 1 2
equilibrium. Hence, every mixed-strategy profile, profile obtain
maximal welfare played positive probability. Therefore mediation value
greater 1, i.e. best mixed-strategy equilibrium less useful best correlated
equilibrium.

5. Conclusion
work introduced studied two measures value correlation
strategic interactions: mediation value enforcement value. measures
complement existing measures appearing price anarchy literature, comparing maximal welfare (when agent behavior dictated) welfare obtained
Nash equilibrium (when agents selfish). Indeed, correlation captures many interesting situations, common computing systems e-commerce applications.
611

fiAshlagi, Monderer, & Tennenholtz

many systems reliable party advise agents behave enforce
behavior. gain may obtained capability major subject
study presented work. studied showed power approach,
general games context congestion games.

Acknowledgments
preliminary version paper appears proceedings 20th conference
Uncertainty Artificial Intelligence (UAI-05). thank German-Israeli Foundation
(GIF) financial support

References
Anshelevich, E., Dasgupta, A., Kleinberg, J., Tardos, E., Wexler, T., & Roughgarden,
T. (2004). Price Stability Network Design Fair Cost Allocation.
Proceedings 45th IEEE Symposium Foundations Computer Science,
(FOCS-04), pp. 5973.
Ashlagi, I., Monderer, D., & Tennenholtz, M. (2008). Mediators position auctions. appear Games Economic Behavior. shorter version appears Proceedings
8th ACM conference Electronic Eommerce.
Aumann, R. (1974). Subjectivity Correlation Randomized Strategies. Journal
Mathematical Economics, 1, 6796.
Aumann, R. (1987). Correlated Equilibrium Expression Bayesian Rationality.
Econometrica, 55, 118.
Christodoulou, G., & Koutsoupias, E. (2005). Price Anarchy Stability
Correlated Equilibria Linear Congestion Games. Proceedings 13th Annual
European Symposium, ESA 2005, pp. 5970.
Czumaj, A., & Vocking, B. (2002). Tight Bounds Worst Case Equilibria. Proceedings
13th Annual Symposium Discrete Algorithms, pp. 413420.
Hart, S., & Schmeidler, D. (1989). Existence Correlated Equilibria. Math. Oper. Res.,
14, 1825.
Kakade, S., Kearns, M., Langford, J., & Ortiz, L. (2003). Correlated equilibria graphical
games. Proceedings 4th ACM conference Electronic commerce, pp. 4247.
Koutsoupias, E., & Papadimitriou, C. (1999). Worst-Case Equilibria. Proceedings
16th Annual Symposium Theoretical Aspects Computer Science, pp. 404413.
Marvonicolas, M., & Spirakis, P. (2001). Price Selfish Routing. Proceedings
33rd Symposium Theory Computing, pp. 510519.
Monderer, D. (2007). Multipotential Games. Twentieth International joint conference
Artificial Intelligence (IJCAI-07) .
612

fiOn Value Correlation

Monderer, D., & Shapley, L. (1996). Potential Games. Games Economic Behavior, 14,
124143.
Monderer, D., & Tennenholtz, M. (2004). K-Implementation. Journal Artificial Intelligence Research (JAIR), 21, 3762.
Monderer, D., & Tennenholtz, M. (2006). Strong mediated equilibrium. Proceedings
AAAI.
Nash, J. (1951). Noncooperative Games. Ann. Math., 54, 286295.
Nau, R. F., & McCardle, K. F. (1990). Coherent Behavior Noncooperative Games.
Journal Economic Theory, 50, 424444.
Papadimitriou, C. (2001). Algorithms, Games, Internet. Proceedings 16th
Annual ACM Symposium Theoretical Aspects Computer Science, pp. 749753.
Papadimitriou, C. (2005). Computing correlated equilibria multi-player games. Proceedings thirty-seventh annual ACM symposium Theory computing, pp.
4956.
Peeters, R., & Potters, J. (1999). Structure Set Correlated Equilibria
Two-by-Two Bimatrix Games. Technical report, Tilburg - Center Economic
Research.
Rosenthal, R. (1973). Class Games Possessing Pure-Strategy Nash Equilibria. International Journal Game Theory, 2, 6567.
Roughgarden, T. (2002). Selfish Routing. PhD Thesis, Cornell University.
Roughgarden, T., & Tardos, E. (2002). Bad Selfish Routing?. Journal ACM,
49(2), 236259.
Rozenfeld, O., & Tennenholtz, M. (2007). Routing mediators. Proceedings 23rd
International Joint Conferences Artificial Intelligence(IJCAI-07), pp. 14881493.
Schrijver, A. (1986). Theory Linear Integer Programming. Wiley, New York.
Shoham, Y., & Tennenholtz, M. (1995a). Artificial Social Systems. Computers Artificial
Intelligence, 14, 533562.
Shoham, Y., & Tennenholtz, M. (1995b). Social Laws Artificial Agent Societies:
Off-Line Design. Artificial Intelligence, 73, 231252.

613

fiJournal Artificial Intelligence Research 33 (2008) 285-348

Submitted 4/08; published 11/08

Computational Logic Foundations KGP Agents
Antonis Kakas

antonis@ucy.ac.cy

Department Computer Science, University Cyprus
75 Kallipoleos Str., P.O. Box 537, CY-1678 Nicosia, Cyprus

Paolo Mancarella

paolo.mancarella@unipi.it

Dipartimento di Informatica, Universita di Pisa
Largo B. Pontecorvo, 3 - 56127 Pisa, Italy

Fariba Sadri

fs@doc.ic.ac.uk

Department Computing, Imperial College London
South Kensington Campus, London SW72AZ, UK

Kostas Stathis

kostas@cs.rhul.ac.uk

Department Computer Science, Royal Holloway
University London, Egham, Surrey TW20 0EX, UK

Francesca Toni

ft@doc.ic.ac.uk

Department Computing, Imperial College London
South Kensington Campus, London SW72AZ, UK

Abstract
paper presents computational logic foundations model agency called
KGP (Knowledge, Goals Plan) model. model allows specification
heterogeneous agents interact other, exhibit proactive
reactive behaviour allowing function dynamic environments adjusting
goals plans changes happen environments. KGP provides highly
modular agent architecture integrates collection reasoning physical capabilities, synthesised within transitions update agents state response reasoning,
sensing acting. Transitions orchestrated cycle theories specify order
transitions executed taking account dynamic context agent
preferences, well selection operators providing inputs transitions.

1. Introduction
widely acknowledged concept agency provides convenient powerful
abstraction describe complex software entities acting certain degree autonomy
accomplish tasks, often behalf user (Wooldridge, 2002). agent context
understood software component capabilities reacting, planning
(inter) acting achieve goals environment situated. paper,
present model agency, called KGP (Knowledge, Goals Plan). model
hierarchical highly modular, allowing independent specifications collection
reasoning physical capabilities, used equip agent intelligent decision making
adaptive behaviour. model particularly suited open, dynamic environments
agents adapt changes environment function
circumstances information incomplete.
c
2008
AI Access Foundation. rights reserved.

fiKakas, Mancarella, Sadri, Stathis & Toni

development KGP model originally motivated existing gap modal logic specifications (Rao & Georgeff, 1991) BDI agents (Bratman, Israel, &
Pollack, 1988) implementation (for example see issues raised Rao, 1996).
Another motivation development KGP comes participation SOCS
project (SOCS, 2007), need agent model satisfies several requirements. specifically, aimed agent model rich enough allow
intelligent, adaptive heterogeneous behaviour, formal could lent well
formal analysis, implementable way implementation sufficiently
close formal specification allow verification. Although several models agency
proposed, none satisfies requirements once.
bridge gap specification implementation KGP model based
computational logic (CL). focus work extend synthesise number
useful computational logic techniques produce formal executable specifications
agents. purpose, model integrates abductive logic programming (ALP) (Kakas,
Kowalski, & Toni, 1992), logic programming priorities (Kakas, Mancarella, & Dung,
1994; Prakken & Sartor, 1997) constraint logic programming (Jaffar & Maher, 1994).
techniques explored right, modular integration
within KGP model explores extensions each, well providing high level agent
reasoning capabilities.
KGP model provides hierarchical architecture agents. specifies collection
modular knowledge bases, formalised CL. knowledge bases support collection reasoning capabilities, planning, reactivity, goal decision,
given formal specifications. model also includes specification physical capabilities,
comprising sensing actuating. capabilities utilised within transitions,
model state agent changes result reasoning, sensing acting.
Transitions use selection operators providing inputs. control component, called
cycle theory, also formalised CL, specifies order transitions executed, depending environment, state agent, preferences agent.
cycle theory takes agent control beyond one-size-fits-all approach used
agent models, allows us specify agents different preferences profiles behaviour (Sadri & Toni, 2005). particular, whereas majority existing agent models
rely upon observe-plan-act, means cycle theory model behaviours
observe-revise goals-planact observe-plan-sense action preconditions-act
observe-plan-act-plan-act. provide one example cycle theory, refer
normal, allowing behaviours depending different circumstances (the environment agent situated preferences). Note also that, respect
agent models, KGP model allows agents revise goals life-time,
observing environment according two modalities: active passive observation.
agent built KGP architecture dynamically determines goals, plans (partially) achieve goals, interleaves planning action executions making
observations environment receiving messages agents, adapts
goals plans new information receives, changes observes, generates appropriate reactions.
number publications already described aspects (an initial version of)
KGP agents. precursor overall model described Kakas, Mancarella,
286

fiComputational Logic Foundations KGP Agents

Sadri, Stathis, Toni (2004b), planning component presented Mancarella,
Sadri, Terreni, Toni (2004), cycle theory developed Kakas, Mancarella,
Sadri, Stathis, Toni (2004a) implementation discussed Stathis et al.
(2004), Yip, Forth, Stathis, Kakas (2005), Bracciali, Endriss, Demetriou,
Kakas, Lu, Stathis (2006). paper, provide full formal specification
components KGP model, thus offering complete technical account
KGP one place. providing full formal specification, adjusted
developed model. particular, notion state definition novel,
reasoning capabilities simplified added, physical
capabilities extended (to include actuating) formally defined, transitions
selection operators formally defined full.
rest paper structured follows. Sections 2 3 give outline
model review background information necessary full description.
Sections 4, 5, 6 7, respectively, describe internal state KGP agents, reasoning physical capabilities, transitions. Section 8 describe selection
operators used cycle theory described Section 9. Following
detailed description KGP agents illustrate model series examples
Section 10, compare model others literature Section 11. Finally,
conclude paper Section 12.

2. KGP Model: Outline
Section give overview KGP agent model components,
provide informal examples functioning. model relies upon
internal (or mental) state, holding agent Knowledge base (beliefs), Goals (desires) Plans (intentions),
set reasoning capabilities,
set physical capabilities,
set transition rules, defining state agent changes, defined
terms capabilities,
set selection operators, enable provide appropriate inputs transitions,
cycle theory, providing control deciding transitions applied
when.
model defined modular fashion, different activities encapsulated
within different capabilities transitions, control separate module.
model also hierarchical structure, depicted Figure 1.
2.1 Internal State
tuple hKB0 , F, C, i, where:
287

fiKakas, Mancarella, Sadri, Stathis & Toni

CYCLE
THEORY

TRANSITIONS




SELECTION
OPERATORS


E
REASONING

CAPABILITIES

PHYSICAL CAPABILITIES

Figure 1: graphical overview KGP model
KB0 holds beliefs agent external world situated
(including past communications), well record actions already
executed.
F forest trees whose nodes goals, may executable not.
tree forest gives hierarchical presentation goals, tree represents
construction plan root tree. set leaves tree F
forms currently chosen plan achieving root tree. Executable goals
actions may physical, communicative, sensing. simplicity, assume
actions atomic duration. Non-executable goals may
mental sensing. non-executable mental goals may children, forming
(partial) plans them. Actions children tree F. goal
associated time variable, implicitly existentially quantified within overall
state serves two purposes: (1) indicating time goal achieved,
instantiated goal achieved appropriate time, (2) providing
unique identifier goal. remainder paper, often use
following terminology goals F, want emphasise role and/or
nature: roots trees F referred top-level goals, executable
goals referred actions, non-executable goals top-level
goals referred sub-goals. Top-level goals classified reactive
non-reactive, explained later. 1 Note top-level (reactive) goals
may actions.
1. Roughly speaking, reactive goals generated response observations, e.g. communications received
agents changes environment, example repair plans already
generated. Non-reactive goals, hand, chosen desires agent.

288

fiComputational Logic Foundations KGP Agents

C Temporal Constraint Store, namely set constraint atoms given
underlying constraint language. constrain time variables goals F.
example, may specify time window time action
instantiated, execution time.
set equalities instantiating time variables time constants. example,
time variables actions instantiated action execution time, records
instantiations kept .
2.2 Reasoning Capabilities
KGP supports following reasoning capabilities:
Planning, generates plans mental goals given input. plans consist
temporally constrained sub-goals actions designed achieving input goals.
Reactivity, used provide new reactive top-level goals, reaction
perceived changes environment current plans held agent.
Goal Decision, used revise non-reactive top-level goals, adapting
agents state changes environment.
Identification Preconditions Identification Effects actions, used
determine appropriate sensing actions checking whether actions may safely
executed (if preconditions known hold) whether recently executed
actions successful (by checking known effects hold).
Temporal Reasoning, allows agent reason evolving environment,
make predictions properties, including non-executable goals, holding
environment, based (partial) information agent acquires lifetime.
Constraint Solving, allows agent reason satisfiability
temporal constraints C .
concrete realisation KGP model provide paper, chosen
realise capabilities various extensions logic programming paradigm.
particular, use (conventional) logic programming Identification Preconditions
Effects, abductive logic programming constraints (see Section 3.2) Planning,
Reactivity Temporal Reasoning, logic programming priorities (see Section 3.3)
Goal Decision.
2.3 Physical Capabilities
addition reasoning capabilities, KGP agent equipped physical capabilities, linking agent environment, consisting
Sensing capability, allowing agent observe properties hold
hold, agents executed actions.
Actuating capability, executing (physical communicative) actions.
289

fiKakas, Mancarella, Sadri, Stathis & Toni

2.4 Transitions
state hKB0 , F, C, agent evolves applying transition rules, employ
capabilities follows:
Goal Introduction (GI), possibly changing top-level goals F, using Goal
Decision.
Plan Introduction (PI), possibly changing F C using Planning.
Reactivity (RE), possibly changing reactive top-level goals F C, using
Reactivity capability.
Sensing Introduction (SI), possibly introducing new sensing actions F checking
preconditions actions already F.
Passive Observation Introduction (POI), updating KB0 recording unsolicited information coming environment, using Sensing.
Active Observation Introduction (AOI), possibly updating KB0 , recording
outcome (actively sought) sensing actions, using Sensing.
Action Execution (AE), executing types actions consequence updating
KB0 , using Actuating.
State Revision (SR), possibly revising F, using Temporal Reasoning Constraint Solving.
2.5 Cycle Selection Operators
behaviour agent given application transitions sequences, repeatedly
changing state agent. sequences determined fixed cycles behaviour, conventional agent architectures, rather reasoning cycle theories.
Cycle theories define preference policies order application transitions,
may depend environment internal state agent. rely upon
use selection operators detecting transitions enabled inputs
be, follows:
action selection inputs AE; selection operator uses Temporal Reasoning
Constraint Solving capabilities;
goal selection inputs PI; selection operator uses Temporal Reasoning
Constraint Solving capabilities;
effect selection inputs AOI; selection operator uses Identification
Effect reasoning capability;
precondition selection inputs SI; selection operator uses Identification
Preconditions, Temporal Reasoning Constraint Solving capabilities;
290

fiComputational Logic Foundations KGP Agents

provision declarative control agents form cycle theories highly
novel feature model, could, principle, imported agent systems.
concrete realisation KGP model provide paper, chosen
realise cycle theories framework logic programming priorities
constraints (see Section 3.3) also use Goal Decision.
relationships capabilities, transitions selection operators
summarised Tables 2.5 2 below. Table 2.5 indicates capabilities (rows)
used transitions selection operators. Table 2 indicates selection
operators used compute possible inputs transitions cycle theory.

sensing
actuating
|=plan
|=pre
|=GD
|=react
|=T R
|=cs
|=ef f

AE
x
x

Transitions
AOI GI P OI
x
x

PI



SR

SI

Selection operators
fGS fAS fES fP

x
x

x

x
x

x
x

x
x
x

x
x

x

x
x

x

Table 1: tabular overview use capabilities transitions selection operators.
Here, |=plan , |=pre , |=GD , |=react , |=T R , |=cs |=ef f , stand for, respectively,
planning, identification preconditions, goal decision, reactivity, temporal reasoning, constraint solving identification effects (reasoning) capabilities,
fGS , fAS , fES , fP stand for, respectively, goal, action, effect precondition
selection operators.

AE
fGS
fAS
fES
fP

AOI

GI

P OI

PI
x



SR

SI

x
x
x

Table 2: tabular overview connections selection operators transitions,
required cycle theory. Here, fGS , fAS , fES , fP stand for, respectively,
goal, action, effect precondition selection operators.
provide components, though, introduce informally scenario
examples used illustrate technical details KGP agent
291

fiKakas, Mancarella, Sadri, Stathis & Toni

model throughout paper. full, formal presentation well additional
examples given throughout paper Section 10.
2.6 Examples
draw examples ubiquitous computing scenario call San
Vincenzo scenario, presented de Bruijn Stathis (2003) summarised follows.
businessman travels work purposes Italy and, order make trip easier,
carries personal communicator, namely device hybrid mobile phone
PDA. device businessmans KGP agent. agent considered
personal service agent (Mamdani, Pitt, & Stathis, 1999) (or psa short)
provides proactive information management flexible connectivity smart services
available global environment within businessman travels within.
2.6.1 Setting 1
businessmans psa requests San Vincenzo Station agent, svs, arrival time
train tr01 Rome. svs information answers
refusal. later, svs receives information arrival time tr01 train
Central Office agent, co. psa requests arrival time tr01 again, svs
accept request provide information.
first example requires one use Reactivity capability model rules interaction transition (a) achieve interaction amongst agents, (b) specify
dynamic adjustments agents behaviour changes, allowing different reactions
request, depending current situation agent. Here, interaction
form negotiation resources amongst agents, resources items information.
Thus, current situation agents amounts resources/information agents
currently own.
example also requires combination transitions RE, POI, AE achieve
expected agents behaviours, follows:
1. psa makes initial request applying AE
2. svs becomes aware request performing POI (and changing KB0 accordingly)
3. svs decides reply refusal performing (and adding corresponding
action plan F)
4. svs utters refusal performing AE
5. svs becomes aware, POI, arrival time (modifying KB0 accordingly)
6. psa makes second request applying AE
7. svs decides reply requested information performing (and adding
corresponding action plan F) communicates information
performing AE.
292

fiComputational Logic Foundations KGP Agents

sequence transitions given so-called normal cycle theory
see Section 9.
2.6.2 Setting 2
preparation businessmans next trip, psa aims getting plane ticket
Madrid Denver well obtaining visa USA. One possible way buy plane
tickets internet. Buying tickets way usually possible,
destinations (depending whether airlines flying destinations sell tickets
internet not) without internet connection. psa currently
connection, information Denver indeed destination tickets
bought online. plans buy ticket internet nonetheless, conditionally,
checks conditions executing planned action. successfully buying
ticket, psa focuses second goal, obtaining visa. achieved
applying USA embassy Madrid, application requires address
USA. address obtained arranging hotel Denver.
example illustrates form partial planning adopted KGP model
(where non-executable sub-goals well actions may part plans) shows
combination transition PI SI AE allows psa agent deal partial
information, generate conditional plans plans several layers, follows:
1. psa initially equipped top-level goals get ticket Denver
obtain visa (through earlier application GI)
2. PI first goal, psa adds partial plan F, buying ticket online
subject sub-goals internet connection available online
tickets bought Denver; sub-goals sensing goals
3. SI, sensing actions added F evaluate sensing sub-goals environment
4. sensing actions executed AE (and KB0 modified accordingly)
5. depending sensed values sensing sub-goals buying action may
may executed AE; let us assume remainder example
action executed
6. SR applied eliminate actions (since already executed), subgoals top-level goal getting ticket Denver (since achieved)
7. PI remaining top-level goal obtaining visa, psa adds plan fill
application form (action) acquiring residence address Denver (sub-goal)
8. action cannot executed, psa knows businessman resident
USA; PI introduces plan sub-goal booking hotel (action)
subgoal acquiring residence address Denver
9. AE executes booking action
293

fiKakas, Mancarella, Sadri, Stathis & Toni

10. AE executes action applying visa
11. SR eliminates actions (since already executed), sub-goal toplevel goal getting visa (since achieved).

3. Background
section give necessary background reasoning capabilities cycle
theory KGP agents, namely:
Constraint Logic Programming, pervasive whole model,
Abductive Logic Programming, heart Planning, Reactivity Temporal
Reasoning capabilities,
Logic Programming Priorities, heart Goal Decision capability
Cycle Theories.
3.1 Constraint Logic Programming
Constraint Logic Programming (CLP) (Jaffar & Maher, 1994) extends logic programming
constraint predicates processed ordinary logic programming predicates,
defined rules, checked satisfiability simplified means built-in,
black-box constraint solver. predicates typically used constrain values
variables conclusion rule take (together unification also
treated via equality constraint predicate). KGP model, constraints used
determine value time variables, goals actions, suitable temporal
constraint theory.
CLP framework defined structure < consisting domain D(<) set
constraint predicates includes equality, together assignment relations
D(<) constraint predicate. CLP, constraints built first-order
formulae usual way primitive constraints form c(t1 , . . . , tn ) c
constraint predicate symbol t1 , . . . , tn terms constructed domain, D(<),
values. rules constraint logic program, P , take form rules
conventional logic programming given
H L1 , . . . , Ln
H (ordinary) atom, L1 , . . . , Ln literals, n 0. Literals positive, namely
ordinary atoms, negative, namely form B, B ordinary atom,
constraint atoms <. negation symbol indicates negation failure (first
introduced Clark, 1978). variables H Li implicitly universally quantified,
scope entire rule. H called head (or conclusion) L1 , . . . , Ln called
body (or conditions) rule form above. n = 0, rule called fact.
valuation, , set variables mapping variables domain
D(<) natural extension maps terms D(<). valuation , set
variables appearing set constraints C, called <-solution C iff C, obtained
applying C, satisfied, i.e. C evaluates true given interpretation
294

fiComputational Logic Foundations KGP Agents

constraint predicates terms. denoted |=< C. set C called <-solvable
<-satisfiable, denoted |=< C, iff least one <-solution, i.e. |=< C
valuation .
One way give meaning constraint logic program P consider grounding program Herbrand base possible valuations, D(<),
constraint variables. rule, ground constraints C body evaluated true rule kept constraints C dropped, otherwise whole
rule dropped. Let ground(P ) resulting ground program. meaning P
given meaning |=LP ground(P ), many different possible
choices (Kakas, Kowalski, & Toni, 1998). resulting overall semantics constraint
logic program P referred |=LP (<) . precisely, given constraint logic
program P conjunction N C (where N conjunction non-constraint literals
C conjunction constraint atoms), remainder paper write
P |=LP (<) N C
denote exists ground substitution variables N C that:
|=< C
ground(P ) |=LP N .
3.2 Abductive Logic Programming Constraints
abductive logic program constraints tuple h<, P, A, Ii where:
< structure Section 3.1
P constraint logic program, namely set rules form
H L1 , . . . , Ln
Section 3.1
set abducible predicates language P . predicates
occurring head clause P (without loss generality, see (Kakas et al.,
1998)). Atoms whose predicate abducible referred abducible atoms
simply abducibles.
set integrity constraints, is, set sentences language P .
integrity constraints KGP model implicative form
L1 , . . . , Ln A1 . . . (n 0, > 0)
Li literals (as case rules) 2 , Aj atoms (possibly special
atom f alse). disjunction A1 . . . referred head constraint
conjunction L1 , . . . , Ln referred body. variables integrity
constraint implicitly universally quantified outside, except variables
occurring head, implicitly existentially quantified scope
head itself.
2. n = 0, L1 , . . . , Ln represents special atom true.

295

fiKakas, Mancarella, Sadri, Stathis & Toni

Given abductive logic program constraints h<, P, A, Ii formula (query)
Q, (implicitly existentially quantified) conjunction literals language
P , purpose abduction find (possibly minimal) set (ground) abducible
atoms which, together P , entails (an appropriate ground instantiation of) Q,
respect notion entailment language P equipped with,
extension P satisfies (see (Kakas et al., 1998) possible notions
integrity constraint satisfaction). Here, notion entailment combined
semantics |=LP (<) , discussed Section 3.1.
Formally, given query Q, set (possibly non-ground) abducible atoms,
set C (possibly non-ground) constraints, pair (, C) abductive answer (with
constraints) Q, respect abductive logic program constraints h<, P, A, Ii,
iff groundings variables Q, , C |=< C, holds
(i) P |=LP (<) Q,
(ii) P |=LP (<) I, i.e. B H I, P |=LP (<) B P |=LP (<) H.
Here, plays role earlier informal description abductive answer. Note
also that, (ii), integrity constraints classical implications.
Note also that, representing knowledge abductive logic program, one needs
decide go logic program, integrity constraints
abducibles. Intuitively, integrity constraints normative need
enforced, making sure head holds whenever body (by condition (ii)
above), whereas logic programming rules enable, help abducibles, derivation
given goals (by condition (i) above). Finally, abducibles chosen amongst literals
cannot derived means logic programming rules. paper, represent reactive constraints (that condition-action rules forcing reactive behaviour
agents) integrity constraints, thus extent addressing knowledge representation challenge posed abductive logic programming imposing sort structure
abductive logic programs use.
notion abductive answer extended take account initial set
(possibly non-ground) abducible atoms 0 initial set (possibly non-ground)
constraint atoms C0 . extension, abductive answer Q, respect
(h<, P, A, Ii, 0 , C0 )
pair (, C)
(i) 0 = {}
(ii) C C0 = {},
(iii) ( 0 , C C0 ) abductive answer Q respect h<, P, A, Ii (in
earlier sense).
worth noticing abductive answer (, C) query true respect
(h<, P, A, Ii, 0 , C0 )
296

fiComputational Logic Foundations KGP Agents

read fact abducibles 0 , along constraints
C0 C, guarantee overall consistency respect integrity constraints given
I. used specification capabilities KGP agents.
remainder paper, simplicity, omit < abductive logic
programs, written simply triples hP, A, Ii. addition, abductive logic
programs present KGP variants core event calculus (Kowalski & Sergot,
1986), define Section 5.1.1.
3.3 Logic Programming Priorities
purposes paper, logic program priorities constraint structure
<, referred , consists four parts:
(i) low-level basic part P , consisting logic program constraints; rule
P assigned name, term; e.g. one rule could
n(X, ) : p(X) q(X, ), r(Y )
name n(X, ) naming ground instance rule;
(ii) high-level part H, specifying conditional, dynamic priorities amongst rules P
H; e.g. one priority could
h(X) : m(X) n(X) c(X)
read: (some instance of) condition c(X) holds, (the corresponding instance of) rule named m(X) given higher priority (the
corresponding instance of) rule named n(X). rule named h(X);
(iii) auxiliary part A, constraint logic program defining (auxiliary) predicates
occurring conditions rules P, H conclusions rule
P H;
(iv) notion incompatibility which, purposes, assumed given
set rules defining predicate incompatible/2, e.g.
incompatible(p(X), p0 (X))
read: instance literal p(X) incompatible corresponding
instance literal p0 (X). assume incompatibility symmetric always
includes r incompatible r two rule names r, s. refer
set incompatibility rules I.
concrete LPP framework equipped notion entailment, denote |=pr , defined top underlying logic programming constraints
semantics |=LP (<) . defined differently different approaches LPP
share following pattern. Given logic program priorities = hP, H, A, Ii
conjunction ground (non-auxiliary) atoms, |=pr iff
(i) exists subset P 0 basic part P P 0 |=LP (<) ,
297

fiKakas, Mancarella, Sadri, Stathis & Toni

(ii) P 0 preferred wrt H subset P 00 P derives (under |=LP (<) )
conclusion incompatible, wrt I, .
framework way specifying meant one sub-theory P 0
preferred another sub-theory P 00 . example, existing literature (Kakas et al.,
1994; Prakken & Sartor, 1996; Kowalski & Toni, 1996; Kakas & Moraitis, 2003), |=pr
defined via argumentation. also approach adopt, relying notion
admissible argument sub-theory (i) consistent (does incompatible
conclusions) (ii) whose rules lower priority, respect high-level
part H theory, sub-theory incompatible conclusions
it. precise definition sets rules compared matter
choice specific framework LPP.
Given concrete definition admissible sub-theories, preference entailment,
|=pr , given by:
(i) exists (maximal) admissible sub-theory 0 0 |=LP (<) ,
(ii) incompatible exist admissible sub-theory
00 00 |=LP (<) .
first condition satisfied say theory credulously prefers possibly prefers . conditions satisfied say
theory sceptically prefers .

4. State KGP Agents
Section define formally concept state KGP agent. also introduce
notation use rest paper order refer state components.
necessary, also try exemplify discussion simple examples.
4.1 Preliminaries
KGP model assume (possibly infinite) vocabularies of:
fluents, indicated f, f 0 , . . .,
action operators, indicated a, a0 , . . .,
time variables, indicated , 0 , . . .,
time constants, indicated t, t0 , . . . , 1, 2, . . ., standing natural numbers (we also
often use constant indicate current time)
names agents, indicated c, c0 , . . . .
constants, ones mentioned above, normally indicated lower case
letters, e.g. r, r1 , . . .
298

fiComputational Logic Foundations KGP Agents

given constraint language, including constraint predicates <, , >, , =, 6=, respect structure < (e.g. natural numbers) equipped notion
constraint satisfaction |=< (see Section 3.1).
assume set fluents partitioned two disjoint sets:
mental fluents, intuitively representing properties agent able plan
satisfied, also observed,
sensing fluents, intuitively representing properties control
agent observed sensing external environment.
example, problem f ixed resource may represent mental fluents, namely
properties (given) problem fixed (given) resource obtained, whereas request accepted connection may represent sensing fluents, namely
properties request (given) resource accepted (given)
connection active. Note important distinguish mental sensing
fluents treated differently control agent: mental fluents need
planned for, whereas sensing fluents observed. clarified later
paper.
also assume set action operators partitioned three disjoint sets:
physical action operators, representing actions agent performs order
achieve specific effect, typically causes changes environment;
communication action operators, representing actions involve communications
agents;
sensing action operators, representing actions agent performs establish
whether fluent (either sensing fluent expected effect action)
holds environment, whether agent performed action.
example, sense(connection on, ) action literal representing act sensing whether network connection time , do(clear table, ) action literal representing physical action removing every item given table,
tell(c1 , c2 , request(r1 ), d, ) action literal representing communication action
expresses agent c1 requesting agent c2 resource r1 within dialogue
identifier d, time 3 .
fluent action operator associated arity: assume arity
greater equal 1, one argument (the last one, convention) always
time point given fluent holds given action takes place. time point
may time variable time constant. Given fluent f arity n > 0, refer
f (s1 , . . . , sn1 , x) f (s1 , . . . , sn1 , x), si constant x time
variable time constant (timed) fluent literals 4 . Given fluent literal `, denote `
3. role dialogue identifier become clearer Section 10. Intuitively, used link
communication actions occurring within dialogue.
4. Note represents classical negation. Negation failure occurs model within
knowledge bases agents, supporting reasoning capabilities cycle theory. negations
state understood classical negations.

299

fiKakas, Mancarella, Sadri, Stathis & Toni

complement, namely f (s1 , . . . , sn1 , x) ` f (s1 , . . . , sn1 , x), f (s1 , . . . , sn1 , x)
` f (s1 , . . . , sn1 , x). Examples fluent literals resource(pen, ), representing
certain resource pen obtained time , well (the ground)
on(box, table, 10), representing time 10 (a certain) box (a certain)
table.
Note assume fluent literals ground except time parameter.
allow us keep notation simpler highlight crucial role played
time parameter. Given simplification, often denote timed fluent literals simply
`[x].
Given action operator arity n > 0, refer a(s1 , . . . , sn1 , x), si
constant x time variable time constant, (timed) action literal. Similarly
case fluent literals, simplicity, assume timed action literals
ground except possibly time. Hence, often denote timed action literals
a[x].
adopt special syntax sensing actions, always form (x
either time variable time constant):
sense(f, x), f fluent,
sense(c : a, x), c name agent action operator.
first case, sensing action allows agent inspect external environment
order check whether fluent f holds time x sensing. second
case, sensing action allows agent determine whether, time x, another agent c
performed action a.
define formally concept state hKB0 , F, C, agent.
4.2 Forest: F
node tree F is:
either non-executable goal, namely (non-ground) timed fluent literal,
executable goal, namely (non-ground) timed action literal.
example tree F given Figure 2, p2 given problem
agent (c1 ) needs fix getting two resources r1 r2 , agent
already decided get r1 agent c2 already planned ask c2
communication action tell(c1 , c2 , request(r1 ), d, 4 ). example, San Vincenzo
scenario, p2 may transfer airport needs arranged, r1 may taxi, c2
taxi company, needed transportation train station, finally r2 may
train ticket.
Note time variable non-executable goals `[ ] actions a[ ] (any tree
in) F understood variable existentially quantified within whole state
agent. Whenever goal action introduced within state, time variable
understood distinguished, fresh variable, also serving identifier.
300

fiComputational Logic Foundations KGP Agents

problem f ixed(p2, 1 )





PPP

PP


P

PP
PP

resource(r1 , 2 )

resource(r2 , 3 )

tell(c1 , c2 , request(r1 ), d, 4 )

Figure 2: example tree F
indicated Section 2, roots trees referred top-level goals, executable
goals often called simply actions, non-executable goals may top-level goals subgoals. example, Figure 2, node identifier 1 top-level goal, nodes
identifiers 2 , 3 sub-goals node identifier 4 action.
Notation 4.1 Given forest F tree F:
node n , parent(n, ), children(n, ), ancestors(n, ), siblings(n, ),
descendents(n, ), indicate parent node n , children n ,
etc. leaf (n, ) value true n leaf , false otherwise.
node n F, parent(n, F), children(n, F), ancestors(n, F), siblings(n, F),
descendents(n, F), leaf (n, F) indicate parent(n, ) tree F
n occurs, etc. (T unique, due uniqueness time variable identifying
nodes).
nodes(T ) represent set nodes , nodes(F) represent set

nodes(F) = F nodes(T ).
Again, indicated Section 2, top-level goal tree F either
reactive non-reactive. see, Section 7, reactive top-level goals introduced state transition whereas non-reactive top-level goals introduced GI transition. example, F agent c1 may consist tree
Figure 2, root non-reactive goal, well tree root reactive goal (action)
301

fiKakas, Mancarella, Sadri, Stathis & Toni

tell(c1 , c2 , accept request(r3 ), d0 , 5 ). action may reply (planned agent c1 )
request resource r3 agent c2 (for example, San Vincenzo scenario, r3
may meeting requested colleague).
Notation 4.2 Given forest F
Rootsr (F) (resp. Rootsnr (F)) denote set reactive (resp. non-reactive)
top-level goals F
nodesr (F) (resp. nodesnr (F)) denote subset nodes(F) consisting nodes
trees whose root Rootsr (F) (resp. Rootsnr (F))
r(F) (resp. nr(F)) stands reactive (resp. non-reactive) part F, namely
set trees F whose root Rootsr (F) (resp. Rootsnr (F)).
Trivially, r(F) nr(F) disjoint, F= r(F) nr(F).
4.3 Temporal Constraint Store: C
set constraint atoms, referred temporal constraints, given underlying
constraint language. Temporal constraints refer time constants well time variables
associated goals (currently previously) state.
example, given forest tree Figure 2, C may contain 1 > 10, 1 20,
indicating top-level goal (of fixing problem p2) needs achieved within time
interval (10, 20], 2 < 1 , 3 < 1 , indicating resources r1 r2 need acquired
top-level goal deemed achieved, 4 < 2 , indicating
agent needs ask agent c2 first. Note need impose 2 3
executed order, namely C may contain neither 2 < 3 , 3 < 2 .
4.4 Agents Dynamic Knowledge Base: KB0
KB0 set logic programming facts state agent, recording actions
executed (by agent others) time execution, well
properties (i.e. fluents negation) observed time
observation. Formally, facts following forms:
executed(a, t) a[t] ground action literal, meaning action
executed agent time t.
observed(`, t) `[t] ground fluent literal, meaning ` observed
hold time t.
observed(c, a[t0 ], t) c agents name, different name agent
whose state defining, t0 time constants, a[t0 ] (ground) action
literal. means given agent observed time agent c
executed action time t0 5 .
5. see that, construction, always case t0 t. Note time executed
actions, t0 , time observation, t, typically different concrete implementation
KGP model, depend, example, time execution transitions within
operational trace agent.

302

fiComputational Logic Foundations KGP Agents

Note facts KB0 variable-free, time variables occur them. Facts
first kind record actions executed agent itself. Facts
second kind record observations made agent environment, excluding actions
executed agents, represented instead facts third kind.
example, action labelled 4 Figure 2 executed (by AE transition)
time 7 executed(tell(c1 , c2 , request(r1 ), d), 7) added KB0 . Moreover, if,
time 9, c1 observes (e.g. transition POI) resource r2 , observation
observed(have resource(r2 ), 9) added KB0 . Finally, KB0 may contain
observed(c2 , tell(c2 , c1 , request(r3 ), d0 , 1), 6)
represent agent c1 become aware, time 6, agent c2 requested,
earlier time 1, resource r3 c1 .
4.5 Instantiation Time Variables:
time variable occurring non-executable goal `[ ] action a[ ] F
instantiated time constant (e.g. action execution time), actual instantiation
= recorded component state agent. example, action
labelled 4 Figure 2 executed time 7, 4 = 7 added .
use allows one record instantiation time variables
time keeping different goals fluent distinguished. Clearly, time
variable exists one equality = .
Notation 4.3 Given time variable , denote ( ) time constant t, any,
= .
worth pointing valuation temporal constraint c C always
take equalities account. Namely, ground valuation temporal
variables c must agree temporal variables assigned .
example, given = { = 3} C = {1 > }, 1 = 10 suitable valuation,
whereas 1 = 1 not.

5. Reasoning Capabilities
section, give detailed specifications various reasoning capabilities, specified within framework ordinary logic programming (for Temporal Reasoning
Identification Preconditions Effects), Abductive Logic Programming Constraints (Section 3.2, Planning Reactivity), Logic Programming Priorities
Constraints (Section 3.3, Goal Decision), constraint programming (Section 3.1,
Constraint Solving).
reasoning capabilities defined means notion entailment respect
appropriate knowledge base (and time point now, appropriate), follows:
303

fiKakas, Mancarella, Sadri, Stathis & Toni

|=T R KBT R Temporal Reasoning, KBT R constraint logic program
variant framework Event Calculus (EC) reasoning actions,
events changes (Kowalski & Sergot, 1986) 6 ;
|=now
plan KBplan Planning, KBplan abductive logic program
constraints, extending KBT R ;
|=now
react KBreact Reactivity, KBreact extension KBplan , incorporating additional integrity constraints representing reactive rules;
|=pre KBpre , KBpre logic program contained KBT R ;
|=ef f KBef f , KBef f logic program contained KBT R ;
|=now
GD KBGD , KBGD logic program priorities constraints.
constraint solving capability defined terms entailment |=cs
basically |=< defined Section 3.1.
5.1 Temporal Reasoning, Planning, Reactivity, Identification Preconditions
Effects: EC-based Capabilities
reasoning capabilities specified within framework event calculus
(EC) reasoning actions, events changes (Kowalski & Sergot, 1986). Below,
first give core EC show use define various capabilities
section.
5.1.1 Preliminaries: Core Event Calculus
nutshell, EC allows one write meta-logic programs talk objectlevel concepts fluents, events (that interpret action operators) 7 , time points.
main meta-predicates formalism are:
holds at(F, ) - fluent F holds time ;
clipped(T1 , F, T2 ) - fluent F clipped (from holding holding) times
T1 T2 ;
declipped(T1 , F, T2 ) - fluent F declipped (from holding holding)
times T1 T2 ;
initially(F ) - fluent F holds initial time, say time 0;
happens(O, ) - operation happens time ;
initiates(O, T, F ) - fluent F starts hold operation time ;
6. sophisticated, abductive logic programming version |=T R KBT R given Bracciali
Kakas (2004).
7. section use original event calculus terminology events instead operators,
rest paper.

304

fiComputational Logic Foundations KGP Agents

terminates(O, T, F ) - fluent F ceases hold operation time .
Roughly speaking, last two predicates represent cause-effects links operations fluents modelled world. also use meta-predicate
precondition(O, F ) - fluent F one preconditions executability
operation O.
Fluent literals agents state mapped onto EC follows. EC-like representation fluent literal f [ ] (resp. f [ ]) agents state atom holds at(f, )
(resp. holds at(f, )). Moreover, arguments time variable need
considered, EC representation fluent literal f (x1 , . . . , xn , ) (resp. f (x1 , . . . , xn , ))
holds at(f (x1 , . . . , xn ), ) (resp. holds at(f (x1 , . . . , xn ), ). 8
Similarly, action literals state agent represented EC
straightforward way. Given action literal a[ ] EC representation happens(a, ).
arguments time considered, e.g. a(x1 , . . . , xn , ), EC representation given happens(a(x1 , . . . xn ), ).
remainder paper, abuse terminology, sometimes refer
f (x1 , . . . , xn ) f (x1 , . . . , xn ) interchangeably fluent literals fluents (although
strictly speaking fluent literals), a(x1 , . . . xn ) interchangeably action
literals action operators (although strictly speaking action literals).
EC allows one represent wide variety phenomena, including operations
indirect effects, non-deterministic operations, concurrent operations (Shanahan, 1997).
core EC use paper consists two parts: domain-independent rules
domain-dependent rules. basic domain-independent rules, directly borrowed
original EC, are:
holds at(F, T2 )
holds at(F, T2 )
holds at(F, )
holds at(F, )
clipped(T1 , F, T2 )
declipped(T1 , F, T2 )

happens(O, T1 ), initiates(O, T1 , F ),
T1 < T2 , clipped(T1 , F, T2 )
happens(O, T1 ), terminates(O, T1 , F ),
T1 < T2 , declipped(T1 , F, T2 )
initially(F ), 0 T, clipped(0, F, )
initially(F ), 0 T, declipped(0, F, )
happens(O, ), terminates(O, T, F ), T1 < T2
happens(O, ), initiates(O, T, F ), T1 < T2

domain-dependent rules define initiates, terminates, initially, e.g. case
setting 2.6.1 Section 2.6 may
initiates(tell(C, svs, inf orm(Q, I), D), T, inf o(svs, Q, I))
holds at(trustworthy(C), )
initially(have inf o(svs, arrival(tr01), I)
8. Note write holds at(f (x1 , . . . , xn ), ) instead holds at(f (x1 , . . . , xn ), ), done e.g.
Shanahan, 1997, want reason object-level properties true false
environment. use within meta-level axioms event calculus (see below) implement
persistence.

305

fiKakas, Mancarella, Sadri, Stathis & Toni

initially(trustworthy(co))
Namely, action agent C providing information concerning query Q
agent svs (the San Vincenzo station agent) initiates agent svs information
Q, provided C trustworthy. Moreover, initially agent co (the Central Office
agent) trustworthy, agent svs information arrival time tr01.
conditions rule defining initiates seen preconditions effects
operator tell take place. Preconditions executability operators specified
means set rules (facts) defining predicate precondition, e.g.
precondition(tell(svs, C, inf orm(Q, I), D), inf o(svs, Q, I))
namely precondition agent svs inform agent C Q svs indeed
information Q.
Notice presence language fluents negation, e.g. f f ,
poses problem inconsistencies, i.e. may case holds at(f, t)
holds at(f, t) derived axioms set events (i.e. given set
happens atoms). However, easily shown never case, provided
domain-dependent part contain two conflicting statements form
initially(f ) initially(f ) since inconsistencies cannot caused except initial
time point (see e.g. Miller & Shanahan, 2002, p. 459).
remainder paper assume domain-dependent part always
consistent agents.
allow agents draw conclusions contents KB0 , represents
narrative part agents knowledge, add domain-independent rules
following bridge rules:
holds at(F, T2 )
holds at(F, T2 )
happens(O, )
happens(O, )

observed(F, T1 ), T1 T2 , clipped(T1 , F, T2 )
observed(F, T1 ), T1 T2 , declipped(T1 , F, T2 )
executed(O, )
observed( , O[T ], )

Notice bridge rules make explicit translation state representation
EC representation fluents actions mentioned earlier section.
Note also assume fluent holds time observed hold.
choice dictated rationale observations considered reasoned
upon moment agent makes them. hand, actions agents
effect time executed 9 .
introduced ability reason narratives events observations,
need face problem inconsistency due conflicting observations, e.g. agent
may observe fluent negation hold time. done
9. time action unknown observation time, last rule may replaced
happens(O, ) observed( , O[ ], )
namely value fluent changed according observations moment observations
made.

306

fiComputational Logic Foundations KGP Agents

set initially atoms, assume external world consistent
too, i.e. never happen observed(f, t) observed(f, t) belong KB0 ,
fluent f time point t.
However, still need cope frame consistency problem, arises, e.g.
given observations observed(f, t) observed(f, t0 ), 6= t0 . issue analogous
case two different events happen time point initiate
terminate fluent. original EC suitable axioms predicates clipped
declipped added, given above, avoid fluent negation holding
time happening two events time. adopt
similar solution cope observations, namely adding following two axioms
domain-independent part:
clipped(T1 , F, T2 )
declipped(T1 , F, T2 )

observed(F, ), T1 < T2
observed(F, ), T1 < T2

solution may naive circumstances sophisticated solutions may
adopted, e.g. one proposed Bracciali Kakas (2004).
5.1.2 Temporal Reasoning
temporal reasoning capability invoked components KGP model
(namely Goal Decision capability, State Revision transition selection operators, see Section 7) prove disprove given (possibly temporally
constrained) fluent literal holds, respect given theory KBT R . purposes
paper KBT R EC theory composed domain-independent domaindependent parts given Section 5.1.1, narrative part given KB0 . Then,
given state S, fluent literal `[ ] possibly empty set 10 temporal constraints C,
temporal reasoning capability |=T R defined
|=T R `[ ] C iff KBT R |=LP (<) holds at(`, ) C.
example, given EC formulation Section 5.1.1 setting 2.6.1 Section 2.6,
state = hKB0 , F, C, agent svs contains
KB0 = {observed(co, tell(co, svs, inf orm(arrival(tr01), 18), d, 15), 17)},
|=T R inf o(svs, arrival(tr01), 18, ) > 20.
5.1.3 Planning
number abductive variants EC proposed literature deal
planning problems, e.g. one proposed Shanahan, 1989. Here, propose novel
variant, somewhat inspired E-language (Kakas & Miller, 1997), allow situated
agents generate partial plans dynamic environment.
refer KBplan = hPplan , Aplan , Iplan abductive logic program where:
10. remainder paper sets seen conjunctions, appropriate.

307

fiKakas, Mancarella, Sadri, Stathis & Toni

Aplan = {assume holds, assume happens}, namely consider two abducible predicates, corresponding assuming fluent holds action occurs, respectively, certain time point;
Pplan obtained adding core EC axioms narrative given KB0
following rules
happens(O, ) assume happens(O, )
holds at(F, ) assume holds(F, )
Iplan contains following set integrity constraints
holds at(F, ), holds at(F, ) f alse
assume happens(O, ), precondition(O, P ) holds at(P, )
assume happens(O, ), executed(O, ), time now(T 0 ) > 0
integrity constraints Iplan prevent generation (partial) plans
unfeasible. first integrity constraint makes sure plan generated entails
fluent negation hold time. second integrity constraint makes
sure that, plan requires action occur certain time point, goal
enforcing preconditions action hold time point taken account
plan. means that, preconditions already known hold,
plan need accommodate actions guarantee hold time
execution action. Finally, last integrity constraint forces assumed unexecuted
actions plan executable future, predicate time now( ) meant
return current time.
worth recalling that, concrete situations, Pplan Iplan also contain domaindependent rules constraints. Domain-dependent rules may needed define
initiates, terminates, initially precondition, may also contain additional
rules/integrity constraints expressing ramifications, e.g.
holds at(f, ) holds at(f1 , ) . . . holds at(fn , )
specific fluents domain. Moreover, integrity constraints may represent
specific properties actions fluents domain. example, domain-dependent
constraint could express two actions type cannot executed time,
e.g.
holds at(tell(c, X, accept request(R), D), ),
holds at(tell(c, X, ref use request(R), D), ) f alse
Intuitively, constructing (partial) plan goal (that given leaf node
current forest) amounts identifying actions sub-goals allowing achieve
goal, assuming nodes forest, executable non-executable,
feasible. Concretely, abductive logic program KBplan supports partial planning
follows. Whenever plan given goal requires agent execute action, a[ ] say,
corresponding atom assume happens(a, ) assumed, amounts intending
execute action (at concrete time instantiating ). hand,
plan given goal requires plan sub-goal, `[ ] say, corresponding atom
assume holds(`, ) may assumed, amounts setting requirement
planning needed sub-goal itself. Notice total plans taken
account, atoms form assume holds( , ) ever generated.
308

fiComputational Logic Foundations KGP Agents

KB
Formally, let KBplan
plan {time now(now)}, time constant (intuitively, time planning capability invoked). Then, planning capability
11 .
|=now
plan specified follows

Let = hKB0 , F, C, state, G = `[ ] mental goal labeling leaf node
tree F. Let also
CA = {assume happens(a, 0 ) | a[ 0 ] nodes(F)},
CG = {assume holds(`0 , 0 ) | `0 [ 0 ] nodes(F) \ {`[ ]}}

0 = CA CG
C0 = C .
Then,
S, G |=now
plan (Xs , C)
iff
Xs = {a[ 0 ] | assume happens(a, 0 ) } {`0 [ 0 ] | assume holds(`0 , 0 ) }
, , C ).
(, C) abductive answer holds at(`, ), wrt (KBplan
0
0
abductive answer exists, S, G |=now
,



used


indicate
failure
plan
(i.e. abductive answer exists).

example, consider setting 2.6.2 Section 2.6. domain-dependent part
KBplan agent psa (looking businessman scenario) contains
initiates(buy ticket online(F rom, o), T, ticket(F rom, o))
precondition(buy ticket online(F rom, o), available connection)
precondition(buy ticket online(F rom, o), available destination(T o))
goal G ticket(madrid, denver, ). Assume F consists single tree
consisting solely root G, thus CA = CG = {}. Then, S, G |=now
plan (Xs , C)
Xs = {buy ticket online(madrid, denver, 0 ),
available connection( 00 ), available destination(denver, 000 )}
C = { 0 < , 0 = 00 = 000 , 0 > now}.
5.1.4 Reactivity
capability supports reasoning reacting stimuli external environment
well decisions taken planning.
knowledge base KBreact supporting reactivity adopt extension knowledge
base KBplan follows. KBreact = hPreact , Areact , Ireact
Preact = Pplan
11. simplicity present case planning single goals only.

309

fiKakas, Mancarella, Sadri, Stathis & Toni

Areact = Aplan
Ireact = Iplan RR
RR set reactive constraints, form
Body Reaction, C

Reaction either assume holds(`, ), `[T ] timed fluent literal,
assume happens(a, ), a[T ] timed action literal, 12
Body non-empty conjunction items form (where `[X] timed fluent
literal a[X] timed action literal, X):
(i) observed(`, 0 ),
(ii) observed(c, a[T 0 ], 00 ),
(iii) executed(a, 0 ),
(iv) holds at(`, 0 ),
(v) assume holds(`, 0 ),
(vi) happens(a, 0 ),
(vii) assume happens(a, 0 ),
(viii) temporal constraints (some of) T, 0 , 00
contains least one item one (i), (ii) (iii).
C temporal constraints (some of) T, 0 , 00 .
integrity constraints abductive logic programming, variables Body
implicitly universally quantified whole reactive constraint, variables
Reaction, C occurring Body implicitly existentially quantified righthand
side reactive constraint. 13
Notice Body must contain least trigger, i.e. condition evaluated
KB0 . Intuitively, reactive constraint Body Reaction, C interpreted
follows: (some instantiation of) observations Body hold KB0 (some
corresponding instantiation of) remaining conditions Body hold, (the appropriate instantiation of) Reaction, associated (the appropriate instantiation of)
12. below, abuse notation, use notions timed fluent action literals liberally
allow non-ground, even though defined timed fluent action literals ground
except possibly time parameter.
13. Strictly speaking, syntactically reactive constraints integrity constraints (due presence
conjunction, represented ,, rather disjunction head). However, reactive constraint
Body Reaction, C transformed integrity constraint Body N ew new clause
N ew Reaction, C Preact . Thus, abuse notation, treat reactive constraints
integrity constraints.

310

fiComputational Logic Foundations KGP Agents

temporal constraints C, added F C, respectively. Notice Reaction
abducible planning performed reactivity capability.
theory KB
Formally, let KBreact
react {time now(now)}, time
constant (intuitively, time capability invoked). Then, reactivity capability |=now
react specified follows. Let = hKB0 , F, C, state. Let

CA = {assume happens(a, ) | a[ ] nodesnr (F)},
CG = {assume holds(`, ) | `[ ] nodesnr (F)}

0 = CA CG
C0 = C .
Then,
|=now
react (Xs , C)
iff
Xs = {a[ ] | assume happens(a, ) } {`[ ] | assume holds(`, ) }
, , C ).
(, C) abductive answer query true wrt (KBreact
0
0

abductive answer exists, |=react , used indicate failure
(i.e. abductive answer exists).

example, consider setting 2.6.1 Section 2.6, KBplan given Sections 5.1.1
5.1.3. Let RR agent svs consist of:
observed(C, tell(C, svs, request(Q), D, 0), ), holds at(have inf o(svs, Q, I), )
assume happens(tell(svs, C, inf orm(Q, I), D), 0 ), 0 >
observed(C, tell(C, svs, request(Q), D, 0), ), holds at(no inf o(svs, Q), )
assume happens(tell(svs, C, ref use(Q), D), 0 ), 0 >
Then, given = 30 = hKB0 , F, C,
KB0 = {observed(co, tell(co, svs, inf orm(arrival(tr01), 18), d1, 15), 17),
observed(psa, tell(psa, svs, request(arrival(tr01)), d2, 20), 22)}
obtain
|=now
react ({tell(svs, psa, inf orm(arrival(tr01), 18), d2, )}, > 30).
5.1.5 Identification Preconditions
capability used KGP agents determine preconditions executability
actions planned for. preconditions defined domain-dependent
part EC means set rules form precondition(O, F ), representing
fluent F precondition executability action action operator (see
5.1.1). Let KBpre subset KBT R containing rules defining precondition( , ).
311

fiKakas, Mancarella, Sadri, Stathis & Toni

identification preconditions capability |=pre specified follows. Given state
= hKB0 , F, C, timed action literal a[ ]
S, a[ ] |=pre Cs
iff
Cs =

V

{`[ ] | KBpre |=LP precondition(a, `)}14 .

5.1.6 Identification Effects
capability used KGP agents determine effects actions already
executed, order check whether actions successful. Note
actions may unsuccessful could executed, executed
expected effect. possible situations agent
full knowledge environment situated.
effects defined domain-dependent part EC means set
rules defining predicates initiates terminates. Let KBef f theory consisting
domain-dependent domain-independent parts EC, well narrative
part KB0 . Then, identification effects |=ef f specified follows. Given state
= hKB0 , F, C, action operator a[t],
S, a[t] |=ef f `
iff
` = f KBef f |=LP initiates(a, t, f )
` = f KBef f |=LP terminates(a, t, f )
5.2 Constraint Solving
Constraint Solving capability simply defined terms structure <
|=< notion presented Section 3.1. Namely, given state = hKB0 , F, C,
set constraints C:
|=cs C iff |=< C C;
exists total valuation S, |=cs C iff exists total valuation
|=< C C.
5.3 Goal Decision
Goal Decision reasoning capability allows agent decide, given time point,
(non-reactive) top-level goals pursued, go generate
plans aiming achieving them. generated goals goals current preferred
interest interest may change time.
14. assume

V

{} = true.

312

fiComputational Logic Foundations KGP Agents

Goal Decision capability operates according theory, KBGD , agent
represents goal preference policy. KBGD includes KBT R thus dynamic, observed
knowledge, KB0 , current state agent. KBGD expressed variant LPP
described Section 3.3, whereby rules lower basic part P LPP theory
form (T possibly empty sequence variables):
n(, ) : G[, ] B[T ], C[T ]

time variable, existentially quantified scope head rule
member ;
variables except universally quantified scope rule;
head G[, ] rule consists fluent literal conjoined (possibly
empty) set temporal constraints, represented h`[ ], C[, ]i;
B(T ) non-empty conjunction literals set auxiliary predicates
include atoms form holds at(`, 0 ), `[T 0 ] timed fluent literal,
atom time now(T 00 ) variables 0 , 00 ;
conditions rule constrained (possibly empty) temporal constraints
C[T ].
rule represents ground instances total valuation
variables satisfies constraints C[T ]. ground instance named
corresponding ground instance n(, ). Intuitively, conditions one rule
satisfied time grounds variable 00 current time
capability applied, goal head rule sanctioned one goals
agent would possibly prefer achieve time. decision whether
goal indeed preferred would depend high-level strategy part H KBGD ,
containing priority rules, described Section 3.3, rules lower-part
rules H. priority rules also include temporal atoms form
holds at(`, 0 ) atom time now(T 00 ) conditions.
accommodate form rules need extend notion incompatibility
defined conclusions h`( ), C[, ]i. simplify notation,
remainder often write h`( ), Ci instead h`( ), C[, ]i.
incompatibility defined different ways. example, (relatively) weak
notion incompatibility given follows. Two pairs h`1 (1 ), C1 h`2 (2 ), C2
incompatible iff every valuation C1 C2 satisfied, ground
instances `1 (1 ) `2 (2 ) incompatible. stronger notion would require
sufficient one valuation exist makes corresponding ground
literals incompatible.
theory KB
Let us denote KBGD
GD {time now(now)}, time
constant. Then, goal decision capability, |=now
GD , defined directly terms
preference entailment, |=pr , LPP (see Section 3.3), follows.
Given state = hKB0 , F, C, i,
|=now
GD Gs
313

fiKakas, Mancarella, Sadri, Stathis & Toni


Gs = {G1 , G2 , . . . , Gn }, n 0, Gi = h`i (i ), Ci = 1, . . . , n
iff Gs maximal set

KBGD
|=pr h`1 (1 ), C1 . . . h`n (n ), Cn i.

means new set goals Gs generated currently (sceptically) preferred
goal preference policy represented KBGD current information KB0 .
Note two goals Gs necessarily compatible other. two
special cases sceptically preferred goals time now. first one
concerns case goals currently sanctioned (lower-part)
KBGD . |=now
GD returns empty set goals (n = 0). second special
case occurs least two goals separately credulously preferred
goals incompatible other. |=now
GD , used
indicate failure identifying new goals pursued.
example, consider San Vincenzo scenario psa agent needs decide whether return home recharge battery. agents goals categorised
assigned priority according category possibly factors. KBGD
expressing given follows:
low-level part contains rules:

n(rh, 1 ) : hreturn home(1 ), {1 < 0 }i
holds at(f inished work, ),
holds at(at home, ),
time now(T ),
T0 = + 6
n(rb, 2 ) : hrecharge battery(2 ), {2 < 0 }i
holds at(low battery, ),
time now(T ),
T0 = + 2
auxiliary part contains, addition KBT R KB0 , following rules
specify category goal relative urgency categories:

typeof (return home, required)
typeof (recharge battery, operational)
urgent wrt type(operational, required)

314

fiComputational Logic Foundations KGP Agents

incompatibility part consists
incompatible(return home(T ), recharge battery(T ))
Namely, two goals pairwise incompatible, i.e. agent one
goals time.
high-level part contains following priority rule:

gd pref (X, ) : n(X, ) n(Y, ) typeof (X, XT ),
typeof (Y, ),
urgent wrt type(XT, ).
Then, = 1 current state = hKB0 , F, C, finished work
away home hold (by temporal reasoning) time now,
|=now
GD {hreturn home(1 ), {1 < 7}i}.
Suppose instead KB0 contains observed(low battery, 1). Then, using weak
notion incompatibility, requiring
every |=cs {1 < 7, 2 < 3}
holds incompatible(return home(1 ), recharge battery(2 ))
have:
|=now
GD {hreturn home(1 ), {1 < 7}i, hrecharge battery(2 ), {2 < 3}i}.
Indeed, = {1 = 3, 2 = 2}, incompatible(return home(3), recharge battery(2))
hold. However, using stronger notion incompatibility, requiring
exists |=cs {1 < 7, 2 < 3}
holds incompatible(return home(1 ), recharge battery(2 ))
have:
|=now
GD {hrecharge battery(2 ), {2 < 3}i}.
Suppose KBGD contains second operational goal hreplace part(3 ), {3 < 5}i
also sanctioned rule lower part time = 1. stronger
form incompatibility goal decision capability = 1 return
operational goals credulously preferred none sceptically preferred.
315

fiKakas, Mancarella, Sadri, Stathis & Toni

6. Physical Capabilities
addition reasoning capabilities defined far, agent equipped
physical capabilities allow experience world situated; world
consists agents and/or objects provide environment agents
interact communicate.
identify two types physical capabilities: sensing actuating. representing
capabilities abstract away sensors actuators agent would
typically rely upon access affect environment. also assume
sensors actuators part agents body, classify implementation
issue (Stathis et al., 2004).
physical sensing capability models way agent interacts external
environment order inspect it, e.g. find whether fluent holds
given time. hand, physical actuating capability models way
agent interacts external environment order affect it, physically executing
actions.
represent sensing physical capability agent function form:
sensing(L, t) = L0
where:
L (possibly empty) set
fluent literals f ,
terms form c : (meaning agent c performed action a),
sensed concrete time t,
L0 (possibly empty) set elements s0
s0 term f : v, f fluent v {true, f alse}, meaning fluent f
observed value v (namely true f alse) time t,

s0 term form c : a[t0 ], c agent name action,
meaning agent c performed action time t0 .
Note physical sensing requires time-stamp specify time
applied within transitions. Note also that, given non-empty set L, sensing(L, t) may
partial, e.g. fluent f L, neither f : true L0 , f : f alse L0 .
Similarly, represent physical actuating capability function
actuating(As, t) = As0
where:
set action literals {a1 , , }, n > 0, agent instructs body
actuate time t;
316

fiComputational Logic Foundations KGP Agents

As0 subset actions body actually managed perform.
meaning action belonging belonging As0 physical
actuators agents body able perform current situation. worth
pointing action belongs As0 necessarily mean effects
successfully reached. Indeed, preconditions executed
action (i) may wrongly believed agent true execution time (as
agents may interfered them) (ii) agent may unaware
preconditions. example, confirmed availability, agent may
booked hotel sending e-mail, (i) agent booked last available
room meanwhile, (ii) agent provide credit card number secure
booking. words, beliefs agent (as held KB0 ) may incorrect and/or
incomplete.
Section 7 Section 8 below, see AOI (Active Observation Introduction)
used check effects actions (identified fES effect selection operator,
turn using |=ef f reasoning capability) actions executed. Moreover, SI
(Sensing Introduction) used check preconditions actions (identified fP
precondition selection operator, turn using |=pre reasoning capability)
executed, make sure actions indeed executable. Overall, following
cases may occur:
action belongs As0 executed
preconditions held time execution effects hold environment execution;
preconditions wrongly believed hold time execution (because
agent partial knowledge environment KBplan incorrect)
consequence effects hold execution;
preconditions known hold time execution (e.g.
agent observed planned hold,
time -replan) consequence effects hold execution;
action belongs \ As0 executed (the body could execute
it).
actuating physical capability check preconditions/effects: left
capabilities called within transitions and/or transition invoking
actuating, show below. before, way body carry actions
implementation issue (Stathis et al., 2004).

7. Transitions
KGP model relies upon state transitions GI, PI, RE, SI, POI, AOI, AE, SR, defined
using following representation
(T)

hKB0 , F, C,
X

0
0
0
0
hKB0 , F , C ,
317

fiKakas, Mancarella, Sadri, Stathis & Toni

name transition, hKB0 , F, C, agents state transition applied, X input transition, time application
transition, hKB00 , F 0 , C 0 , 0 revised state, resulting application transition input X time state hKB0 , F, C, i. Please note transitions
modify components state. Also, transitions (namely GI,
RE, POI, SR) input X always empty omitted. transitions
(namely PI, SI, AOI, AE) input always non-empty (see Section 9) selected
appropriate selection operator (see Section 8).
define transition formally, defining hKB00 , F 0 , C 0 , 0 i. Note
assume transition takes care possible renaming time variables output
capabilities (if capability used transition), order guarantee
goal/action forest univocally identified time variable.
7.1 Goal Introduction
transition takes empty input. calls Goal Decision capability determine
new (non-reactive) top-level goals agent. capability returns set goals,
means circumstances possibly changed preferred top-level goals
agent transition reflect changing forest new state
consist one tree new (non-reactive) goal. hand, Goal Decision
capability return (non-reactive) goals (namely returns ) state left
unchanged, as, although goals current state longer sceptically preferred
may still credulously preferred and, since others replace them,
agent carry current plans achieve them.
(GI)

hKB0 , F, C,

hKB0 , F 0 , C 0 ,

where, given = hKB0 , F, C,
(i) |=now
GD ,
F0 = F
C0 = C
(ii) otherwise, |=now
GD Gs Gs 6= ,
F 0 defined follows:
nr(F 0 ) = {Tg[ ] | hg[ ], Gs} Tg[ ] tree consisting solely
root g[ ]
r(F 0 ) = {}
C 0 = {T C | h , Ci Gs}
transition drops (top-level) goals become semantically irrelevant (due
changed circumstances agent changes environment), replaces
new relevant goals. see, Section 7.8, goals also dropped
318

fiComputational Logic Foundations KGP Agents

book-keeping activities State Revision (SR) transition, transition
never add set goals.
Note that, GI replace whole forest old state new forest,
possible agent looses valuable information achieving goals,
one new preferred goals agent (or equivalent to) current goal.
effect though minimized calling (in cycle theory) GI transition
certain times, e.g. current goals achieved timed-out. Alternatively,
earlier formalisation GI transition could modified that, case (ii),
goals Gs already occur (modulo temporal variables associated temporal
constraints) roots (non-reactive) trees F, trees kept F 0 . simple way
characterise (some of) goals follows. Let

Xs = {hg[ ], C, = 0 |

hg[ ], Ci Gs,
g[ 0 ] Rootsnr (F)
|=cs C iff |=cs (C C { = 0 })}

Gs0 = {hg[ ], Ci |

hg[ ], C, = 0 Xs}

new constraints goals Gs0 equivalent old constraints C. example,
Gs may contain
G = hhave ticket(madrid, denver, 2 ), {2 < 12}i
ticket(madrid, denver, 1 ) Rootsnr (F) C = {1 < 12}.
Then, G definitely belongs Gs0 . Let
newC =

[

C { = 0 }.

h ,T C, = 0 iXs

Case (ii) redefined follows, using definitions Xs, Gs0 newC:
(ii0 ) otherwise, |=now
GD Gs Gs 6= , then, case |=cs C newC,
F 0 C 0 defined earlier case (ii), otherwise (if |=cs C newC):
F 0 defined follows:
nr(F 0 ) = {Tg[ ] | hg[ ], Gs \ Gs0 } F(Xs)
Tg[ ] tree consisting solely root g[ ]
F(Xs) set trees F roots goals form g[ 0 ]
hg[ ], , = 0 Xs
r(F 0 ) = {}
C 0 = C {T C | h , Ci Gs \ Gs0 } newC.
Note keep temporal constraints state, prior application GI,
force variables new goals remain state GI rewritten
using old identifiers goals.
319

fiKakas, Mancarella, Sadri, Stathis & Toni

7.2 Reactivity
transition takes empty input. calls Reactivity capability order determine
new top-level reactive goals state (if any), leaving non-reactive part unchanged.
new reactive goals exist, reactive part new state empty.
hKB0 , F, C,

hKB0 , F 0 , C 0 ,

(RE)
where, given = hKB0 , F, C, i:
(i) |=now
react ,
F 0 defined follows:
r(F 0 ) = {}
nr(F 0 ) = nr(F)
C0 = C

(ii) otherwise, |=now
react (X s, C),
F 0 defined follows:
nr(F 0 ) = nr(F)
r(F 0 ) = {Tx[ ] | x[ ] X s}
Tx[ ] tree consisting solely root x[ ]
C0 = C C
Note asymmetry case (ii) GI case (ii) RE, GI
eliminates reactive goals case, whereas leaves non-reactive goals unchanged.
Indeed, reactive goals may due choice specific non-reactive goals,
latter change former need re-evaluated. Instead, non-reactive goals affected
newly acquired reactive goals (that outcome enforcing reactive rules).
Note also case (ii), similarly GI, replaces whole (reactive) forest
old state new (reactive) forest, possible agent loses valuable
information achieving reactive goals, one new reactive goals
(or equivalent to) current goal. variant case (ii) RE, mirroring
variant given earlier GI using |=cs well, defined avoid problem.
7.3 Plan Introduction
transition takes input non-executable goal state (that selected
goal selection operator, see Section 8) produces new state calling agents
Planning capability, selected goal mental goal, simply introducing new
sensing action, goal sensing goal.
(PI)

hKB0 , F, C,
G

0
0
hKB0 , F , C ,

G input goal (selected planning tree F, thus leaf, see
Section 8)
320

fiComputational Logic Foundations KGP Agents

F 0 = (F \ {T | G leaf }) N ew
C0 = C C
N ew C obtained follows, hKB0 , F, C, i.
(i) G mental goal: let S, G |=now
plan P . Then,
either P =
N ew = {T } C = {},
P = (X s, C)
N ew = {T 0 } 0 obtained adding element X
child G.
(ii) G = `[ ] sensing goal, child goal G0 :
N ew = {T 0 } 0 (a node labelled by) sense(`, 0 ) new child G0
(here 0 new time variable)
C = { 0 }.
(iii) G = `[ ] sensing goal, root :
N ew = {T , 0 } 0 tree consisting solely root (labelled by) sense(`, 0 )
(here 0 new time variable)
C = { 0 }.
7.4 Sensing Introduction
transition takes input set fluent literals preconditions actions
state produces new state adding sensing actions leaves (appropriate)
trees forest component. Note that, SI invoked, input fluent literals
selected precondition selection operator, chosen amongst preconditions
actions already known true (see Section 8).
(SI)

hKB0 , F, C,
SP

0
0
hKB0 , F , C ,

SP non-empty set preconditions actions (in form pairs precondition,
action) trees F, where, given that:
- N ew = {h`[ ], A, sense(`, 0 )i | h`[ ], Ai SP 0 fresh variable}
- addSibling(T , A, SA) denotes tree obtained adding elements SA new
siblings tree leaf (A, )

F 0 = F \ {T | leaf (A, ) h`[ ], Ai SP s}
{addSibling(T , A, SA) | leaf (A, )
SA = {sense(`, 0 )|h`[ ], A, sense(`[ 0 ])i N ew}}
C 0 = C { 0 < | h`[ ], , sense(`[ 0 ])i N ew}
321

fiKakas, Mancarella, Sadri, Stathis & Toni

Basically, fluent literal selected precondition selection operator
precondition action A, new sensing action added sibling A,
constraint expressing sensing action must performed added
current set temporal constraints.
7.5 Passive Observation Introduction
transition updates KB0 adding new observed facts reflecting changes environment. observations deliberately made agent, rather,
forced upon agent environment. observations may properties
form positive negative fluents (for example battery running out) actions
performed agents (for example messages addressed agent).
hKB0 , F, C,

hKB00 , F, C,

(POI)
where, sensing(, now) = L,
KB00 = KB0

{observed(f, now) | f : true L}
{observed(f, now) | f : f alse L}
{observed(c, a[t], now) | c : a[t] L}.
7.6 Active Observation Introduction
transition updates KB0 adding new facts deliberately observed agent,
seeks establish whether given fluents hold given time. fluents
selected effect selection operator (see Section 8) given input transition.
Whereas POI decided agent (the agent interrupted forced
observation environment), AOI deliberate. Moreover, POI may observe fluents
actions, whereas AOI considers fluents (that effects actions executed
agent, see Section 8 Section 9).
(AOI)

hKB0 , F, C,
SF

0
hKB0 , F, C,

SF = {f1 , . . . , fn }, n > 0, set fluents selected actively sensed (by
effect selection operator), and, sensing(SF s, now) = L,
KB00 = KB0
{observed(f, now) | f : true L}
{observed(f, now) | f : f alse L}.
7.7 Action Execution
transition updates KB0 , recording execution actions agent. actions
executed selected action selection operator (see Section 8) prior
transition, given input transition.
322

fiComputational Logic Foundations KGP Agents

(AE)

hKB0 , F, C,
SAs

hKB00 , F, C, 0

SAs non-empty set actions selected execution (by action selection
operator),
let subset non-sensing actions SAs subset sensing
actions SAs;
let sensing(S 0 , now) = L0 , 0 = {f | sense(f, ) S}
let sensing(S 00 , now) = L00 , 00 = {c : | sense(c : a, ) S}
let actuating(A0 , now) = A00 , A0 = {a | a[ ] A}.
Then:
KB00 = KB0
{executed(a, now) | A00 }
{observed(f, now) | f : true L0 }
{observed(f, now) | f : f alse L0 }
{observed(c, a[t], now) | c : a[t] L00
|=cs C = sense(c : a, ) S}

0 = { = | a[ ] SAs A00 }
{ = | sense(f, ) SAs (f : ) L0 }
{ = | c : a[t] L00 |=cs C = sense(c : a, ) S}.
7.8 State Revision
SR transition revises state removing timed-out goals actions goals
actions become obsolete one ancestors already believed
achieved. make use following terminology.
Notation 7.1 Given state S, timed fluent literal `[ ], timed fluent literal action
operator x[ ], time-point now:
achieved(S, `[ ], now) stands
exists total valuation S, |=cs |=T R `[ ]
timed out(S, x[ ], now) stands
exists total valuation S, |=cs > now.
323

fiKakas, Mancarella, Sadri, Stathis & Toni

Then, specification transition follows.
(SR)

hKB0 , F, C,

hKB0 , F 0 , C,

F 0 set trees F pruned nodes(F 0 ) biggest subset
nodes(F) consisting goals/actions x[ ] tree F (here =
hKB0 , F, C, i):
(i) timed out(S, x[ ], now),
(ii) x action operator, case executed(x, t) KB0 ( = t) ,

(iii) x fluent literal, achieved(S, x[ ], now),
(iv) every y[ 0 ] siblings(x[ ], F)
either y[ 0 ] siblings(x[ ], F 0 ),
y[ 0 ] 6 siblings(x[ ], F 0 )
fluent literal achieved(S, y[ 0 ], now),
action literal executed(y, t) KB0 0 = ,

(v) x sensing action operator, x[ ] = sense(`, ),
either exists a[ 0 ] siblings(x[ ], F 0 ) ` precondition (i.e.
S, a[ 0 ] |=pre Cs `[ 0 ] Cs) < 0 C,
exists `[ 0 ] siblings(x[ ], F 0 ) ` sensing fluent <
0 C,
(vi) x[ ] top-level goal parent(x[ ], F) = P P nodes(F 0 ).
conditions specify SR keeps trees forest state. Intuitively, conditions may understood terms prevent remaining
trees:
condition (i) removes timed-out goals actions,
condition (ii) removes actions already executed,
condition (iii) removes goals already achieved,
condition (iv) removes goals actions whose siblings already timed
thus deleted, condition (i),
condition (v) removes sensing actions preconditions actions
deleted sensing goals deleted,
condition (vi) recursively removes actions goals whose ancestors removed.
following example illustrates SR used provide adjustment agents
goals plans light newly acquired information.
324

fiComputational Logic Foundations KGP Agents

7.9 Setting 3
agent psa goal museum ticket (state-run) museum
businessman wants visit, plan buy ticket. executing plan psa
observes European Heritage day (ehd short), via appropriate message
another agent mus (representing museum), stating state-run museums
Europe give free tickets anybody walking day. Then, psas goal
already achieved goal plan deleted state.
Let agents initial state hKB0 , F, C, with:
= { } = KB0
F

= {T }

C = {1 10, 2 = 3 , 3 < 1 }
consists top-level goal g1 = have(ticket, 1 ), two children,
g2 = money(2 ) a1 = buy(ticket, 3 ),

15

assuming KBT R contains
initiates(ehd, T, have(ticket))
initiates(buy(O), T, have(O))
precondition(buy(O), money).
remaining knowledge bases play useful role purposes
example, therefore considered empty. message museum
agent mus added KB0 via POI, e.g. time 6, following form:
observed(mus, ehd(5), 6)
i.e. time 6 observed time 5 mus announced state-run museums
Europe free day. Then, via SR, time 8 say, g1 , g2 a1 eliminated
F, g1 already achieved.

8. Selection Operators
KGP model relies upon selection operators:
fGS (goal selection, used provide input PI transition);
fP (precondition selection, used provide input SI transition);
fES (effect selection, used provide input AOI transition);
fAS (action selection, used provide input AE transition).
15. g1 a1 reactive not, matter example.

325

fiKakas, Mancarella, Sadri, Stathis & Toni

Selection operators defined terms (some the) capabilities (namely Temporal
Reasoning, Identification Preconditions Effects Constraint Solving).
high-level description, selection operators seen returning set
items given initial set satisfy certain number conditions. example,
given state hKB0 , F, C, i, goal selection operator returns set non-executable
goals trees F satisfy conditions; precondition selection operator returns
set pairs, consisting (i) timed fluent literal precondition
action tree F (ii) action, satisfying conditions; effect
selection operator returns set fluent literals effects actions already
executed (as recorded KB0 ) satisfy conditions; action selection operator
returns set actions trees F satisfy conditions.
selection operators formally defined below.
8.1 Goal Selection
Informally, set conditions goal selection operator follows. Given state
= hKB0 , F, C, time-point t, set goals selected fGS singleton set
consisting non-executable goal G tree F time t:
1. G timed out,
2. ancestor G timed out,
3. child ancestor G timed out,
4. neither G, ancestor G tree F already achieved.
5. G leaf
Intuitively, condition 1 ensures G already timed-out, conditions 2-3 impose
G belongs still feasible plan top-level goal F, condition 4 makes
sure considering G wasteful.
Note that, already mentioned Section 5.1.3, simplicity select single goal.
Formally, given state = hKB0 , F, C, time-point t, let G(S, t) set
non-executable goals `[ ] nodes(F) that:
1. timed out(S, `[ ], t)
2. timed out(S, G, t) G ancestors(`[ ], F),
3. timed out(S, X, t) X nodes(F) X child P
ancestors(`[ ], F)
4. achieved(S, G, t) G {`[ ]} ancestors(`[ ], F)
5. leaf (G, F)
Then, G(S, t) 6= {}:
fGS (S, t) = {G} G G(S, t).
Otherwise, fGS (S, t) = {}.
326

fiComputational Logic Foundations KGP Agents

8.2 Effect Selection
Informally, set conditions effect selection operator follows. Given state
= hKB0 , F, C, time-point t, fES selects fluents f f f one
effects action a[ ] recently executed.
Note f (or f ) may occur F could (observable)
effect executed action, necessarily goal action
contributes achieving. example, order check whether internet connection
available, agent may want observe access skype network even though
really interested opening browser (as needs browser order perform
booking online).
Formally, given state = hKB0 , F, C, time-point now, set (timed)
fluents selected fES set (timed) fluents f [ ] action
operator
1. executed(a, t0 ) KB0 , t0 = < t0 < now, sufficiently
small number (that left parameter here),
2. S, a[ ] |=ef f `, ` = f ` = f .
8.3 Action Selection
Informally, set conditions action selection operator follows. Given state
= hKB0 , F, C, time-point t, set actions selected fAS defined
follows. Let X (S, t) set actions trees F that:
1. executed,
2. ancestor timed out,
3. child ancestor timed out,
4. ancestor already satisfied,
5. precondition known false,
6. already executed.
fAS (S, t) X (S, t) actions fAS (S, t) executable concurrently
t.
Intuitively, conditions 2-4 impose belongs still feasible plan toplevel goals F. Note condition 1 definition X (S, t) logically redundant,
also re-imposed definition fAS (S, t). However, condition serves first
filter thus useful practice.
Formally, given state = hKB0 , F, C, i, time-point t, set actions
selected fAS defined follows. Let X (S, t) set actions a[ ] occurring
leaves trees F that:
327

fiKakas, Mancarella, Sadri, Stathis & Toni

1. exists total valuation S, |=cs = t,
2. timed out(S, G, t) G ancestors(a[ ], F),
3. timed out(S, X, t) X children(G, F) G ancestors(a[ ], F),
4. achieved(S, G, t) G ancestors(a[ ], F),
5. let S, a[ ] |=pre Cs Cs = `1 [ ] . . . `n [ ];
n > 0, = 1, . . . , n exists total valuation S, |=cs
= |=T R `i [ ],
6. exists t0 = t0 executed(a, t0 ) KB0 .
formalisation condition 6 allows instances action
executed. Then,
fAS (S, t) = {a1 [1 ], . . . , [m ]} X (S, t)
(where 0), exists total valuation variables C
S, |=cs 1 = . . . = t.
Note definition action selection operator extended take
account notion urgency respect temporal constraints. However,
extension beyond scope work.
8.4 Precondition Selection
Informally, set conditions precondition selection operator follows. Given
state = hKB0 , F, C, time-point t, set preconditions (of actions
F) selected fP set pairs hC, Ai (timed) preconditions C actions
nodes(F) that:
1. C precondition
2. C known true t,
3. one actions could selected execution fAS would called
current time.
reason selection operator returns pairs, rather simply preconditions,
transition SI, makes use outputs selection operator, needs
know actions associated preconditions. SI introduces sensing
actions precondition returned place sensing actions siblings
associated actions F, seen Section 7.4.
Formally, given state = hKB0 , F, C, time-point t, set preconditions
actions selected fP set pairs hC, Ai (timed) preconditions C actions
nodes(F) that:
1. = a[ ], S, a[ ] |=pre Cs C conjunct Cs,
328

fiComputational Logic Foundations KGP Agents

2. exists total valuation variables C S, |=cs =
|=T R C,
3. X (S, t), X (S, t) defined Section 8.3.

9. Cycle Theory
behaviour KGP agents results application transitions sequences,
repeatedly changing state agent. sequences fixed priori,
conventional agent architectures, determined dynamically reasoning
declarative cycle theories, giving form flexible control. Cycle theories given
framework Logic Programming Priorities (LPP) discussed Section 3.
9.1 Formalisation Cycle Theories.
use following new notations:
(S, X, 0 , t) represent application transition time state given
input X resulting state 0 ,
(S, X) represent transition potentially chosen next transition
state S, input X.
Recall that, transitions, X may empty set {}, indicated Section 7.
Formally, cycle theory Tcycle consists following parts.
initial part Tinitial , determines possible transitions agent could
perform starts operate. Concretely, Tinitial consists rules form
(S0 , X) C(S0 , X)
refer via name R0|T (S0 , X). rules sanction that, conditions
C hold initial state S0 initial transition could , applied state
S0 input X. example, rule
R0|GI (S0 , {}) : GI(S0 , {}) empty f orest(S0 )
sanctions initial transition GI, forest initial state S0
empty.
Note C(S0 , X) may empty, and, non-empty, C(S0 , X) may refer
current time via condition time now(t). example, rule
R0|P (S0 , G) : P I(S0 , G) Gs = fGS (S0 , t), Gs 6= {}, G Gs, time now(t)
sanctions initial transition PI, forest initial state S0
contains goal planned current time (in goal
selection operator picks goal).
basic part Tbasic determines possible transitions following given transitions,
consists rules form
0 (S 0 , X 0 ) (S, X, 0 , t), EC(S 0 , X 0 )
329

fiKakas, Mancarella, Sadri, Stathis & Toni

refer via name RT |T 0 (S 0 , X 0 ). rules sanction that, transition executed, starting time state resulting state 0 ,
conditions EC evaluated 0 satisfied, transition 0 could
next transition applied 0 , input X 0 .16 EC enabling conditions
determine 0 applied . also determine input X 0 0 ,
via calls selection operators. initial part Tcycle , EC may empty
and, not, may refer current time. example, rule
RAE|P (S 0 , G) : P I(S 0 , G) AE(S, As, 0 , t),
Gs = fGS (S 0 , t0 ), Gs 6= {}, G Gs, time now(t0 )
sanctions PI follow AE current time goal
current state selected goal selection function.
behaviour part Tbehaviour contains rules describing dynamic priorities amongst
rules Tbasic Tinitial . Rules Tbehaviour form
RT |T 0 (S, X 0 ) RT |T 00 (S, X 00 ) BC(S, X 0 , X 00 )
0 6= 00 , refer via name PTT0 00 . Recall RT |T 0 ()
RT |T 00 () (names of) rules Tbasic Tinitial . Note that, abuse notation,
could 0 case one rule used specify priority first
transition take place, words, priority rules Tinitial .
rules Tbehaviour sanction that, transition , conditions BC hold,
prefer next transition 0 00 . conditions BC behaviour
conditions give behavioural profile agent. example, rule

PGIT
0 : RT |GI (S, {}) RT |T 0 (S, X) empty f orest(S)

sanctions GI preferred transition transition
results state empty forest. components Tcycle ,
conditions BC may refer current time.
auxiliary part including definitions predicates occurring enabling
behaviour conditions.
incompatibility part, effect expressing one (instance a) transition
chosen one time.
Hence, Tcycle LPP-theory where: (i) P = Tinitial Tbasic , (ii) H = Tbehaviour .
9.2 Operational Trace
cycle theory Tcycle agent responsible behaviour, induces
operational trace agent, namely (typically infinite) sequence transitions
T1 (S0 , X1 , S1 , t1 ), . . . , Ti (Si1 , Xi , Si , ti ), Ti+1 (Si , Xi+1 , Si+1 , ti+1 ), . . .

16. Note order determine 0 possible transition , rule earlier form,
one needs know applied resulted state 0 . conveyed
choice name: RT |T 0 (S 0 , X 0 ). words, using Prolog notation, could represented
rule 0 (S 0 , X 0 ) ( , , 0 , ), EC(S 0 , X 0 ). Thus, rule Markovian.

330

fiComputational Logic Foundations KGP Agents

S0 given initial state;
1, ti given clock system (ti < ti+i );
(Tcycle Tbasic ) {time now(t1 )} |=pr T1 (S0 , X1 );
1
(Tcycle Tinitial ) {Ti (Si1 , Xi , Si , ti ), time now(ti+1 )} |=pr Ti+1 (Si , Xi+1 )
namely (non-final) transition sequence followed preferred transition,
specified Tcycle . If, stage, preferred transition determined |=pr
unique, choose one arbitrarily.
9.3 Normal Cycle Theory
normal cycle theory concrete example cycle theory, specifying pattern
operation agent prefers follow sequence transitions allows achieve
goals way matches expected normal behaviour. examples possible
cycle theories found literature (Kakas, Mancarella, Sadri, Stathis, & Toni,
2005; Sadri & Toni, 2006).
Basically, normal agent first introduces goals (if none start with) via GI,
reacts them, via RE, repeats process planning them, via PI,
executing (part of) chosen plans, via AE, revising state, via SR, goals
dealt (successfully revised away). point agent returns introducing
new goals via GI repeating process. Whenever process agent
interrupted via passive observation, via POI, chooses introduce new goals via
GI, take account changes environment. Whenever actions
unreliable, sense preconditions definitely need checked,
agent senses (via SI) executing action. Whenever actions
unreliable, sense effects definitely need checked, agent actively
introduces actions aim sensing effects, via AOI, executed
original actions. initially agent equipped goals, would plan
straightaway PI.
full definition normal cycle theory given appendix. used
provide control examples next section. Here, note that, although normal
cycle theory based classic observe-plan-act cycle agent control, generalises
several ways giving flexibility agent behaviour adapt changing
environment. example, goals agent need fixed dynamically
changed depending newly acquired information. Let us illustrates feature
brief example here. Suppose current state agent contains top-level nonreactive goal hreturn home(1 ), {1 < 7}i POI occurs adds observation
observed(low battery, 2) time 2. subsequent GI transition generated normal
cycle theory introduces new goal hrecharge battery(2 ), {2 < 3}i which, depending
details KBGD , either replaces previous goal adds additional goal.
normal cycle theory next choose PI transition new urgent
goal recharging battery.
331

fiKakas, Mancarella, Sadri, Stathis & Toni

10. Examples
section revisit examples introduced Section 2.6 used throughout
paper illustrate various components KGP model. Overall, aim
illustrate interplay transitions, interplay provides variety
behaviours afforded KGP model, including reaction observations, generation
execution conditional plans, dynamic adjustment goals plans.
Unless specified differently, assume Tcycle normal cycle theory
presented Section 9.3. provide domain-dependent definition auxiliary
part Tcycle explicitly, required.
10.1 Setting 1 Formalised
formalise initial state, knowledge bases behaviour svs Setting 1
described Section 2.6.1.
10.1.1 Initial State
simplicity, observations, goals plan svs assumed empty
initially. concretely let (initial) state svs
KB0 = { }
F

= {}

C = {}
= {}

10.1.2 Knowledge Bases
Following Section 5.1.4, formulate reactivity knowledge base agent svs terms
utterances query ref, ref use, inf orm inspired FIPA specifications communicative acts (FIPA, 2001a, 2001b). However, although use names
communicative acts FIPA specification, adopt mentalistic
svs formulated
semantic interpretation terms pre- post-conditions. Thus, KBreact
as:
observed(C, tell(C, svs, query ref (Q), D, 0), ), holds at(have inf o(Q, I), )
assume happens(tell(svs, C, inf orm(Q, I), D), 0 ), 0 >
observed(C, tell(C, svs, query ref (Q), D, 0), ), holds at(no inf o(Q), )
assume happens(tell(svs, C, ref use(Q), D), 0 ), 0 >
assume happens(tell(svs, C, inf orm(Q, I), D), ),
assume happens(tell(svs, C, ref use(Q), D), 0 )
f alse
assume happens(A, ), executable(A) f alse
executable(tell(svs, C, S, D)) C 6= svs
332

fiComputational Logic Foundations KGP Agents

initially(no inf o(arrival(tr01))
precondition(tell(svs, C, inf orm(Q, I), D), inf o(Q, I))
initiates(tell(C, svs, inf orm(Q, I), D), T, inf o(Q, I))
terminates(tell(C, svs, inf orm(Q, I), D), T, inf o(Q))
10.1.3 Behaviour
illustrate behaviour psa assume agent requests svs,
time 3, say, arrival time tr01. svs receives request psa time 5 arrival
time tr01. Via POI time 5 svs records KB0 :
observed(psa, tell(psa, svs, query ref (arrival(tr01)), d, 3), 5)
dialogue identifier. Then, via RE, time 7, say, svs modifies state
adding F tree rooted action a1 answer psa. action a1 refusal
represented as:
a1 = tell(svs, psa, ref use(arrival(tr01)), d, ),
temporal constraint > 7 added C.
refusal action generated via Reactivity capability svs
information requested arrival time. svs executes planned action a1 time
10, say, via AE transition, instantiating execution time, adding following record
KB0 :
executed(tell(svs, psa, ref use(arrival(tr01)), d), 10),
updating adding = 10 it.
Suppose svs makes two observations follows. time 17 svs receives
information arrival time (18) tr01 train co. Via POI, svs records
KB0 17 :
observed(co, tell(co, svs, inf orm(arrival(tr01), 18), d0 , 15), 17).
Assume time 25 svs receives another request psa arrival
time tr01 and, via POI, svs records KB0 :
observed(psa, tell(psa, svs, query ref (arrival(tr01)), d00 , 20), 25)
new dialogue identifier d00 . leads different answer svs query
psa. svs adds action state answer psa arrival time. done
via RE, say time 28. new tree added F rooted (reactive) action
tell(svs, psa, inf orm(arrival(tr01), 18), d00 , 0 ),
new temporal constraint 0 > 28 added C.
Via AE, svs executes action, instantiating execution time 30, say, adding
following record
17. d0 identifier dialogue within utterance performed, would typically
different earlier d.

333

fiKakas, Mancarella, Sadri, Stathis & Toni

executed(tell(svs, psa, inf orm(arrival(tr01), 18), d00 ), 30)
KB0 , adding 0 = 30 .
Eventually, SR clear planned (and executed) actions F component
state svs.
10.2 Setting 2 Formalised
formalise initial state, knowledge bases behaviour psa Setting 2
described Section 2.6.2.
10.2.1 Initial State
Let us assume initially state psa follows:
KB0 = { }
F

= {T1 , T2 }

C = {1 < 15, 2 < 15}
= {}
T1 T2 consist goals (respectively):
g1 = ticket(madrid, denver, 1 )
g2 = visa(usa, 2 ).
10.2.2 Knowledge Bases
psa
plan goal g1 , KBplan
contains:

initiates(buy ticket online(F rom, o), T, ticket(F rom, o))
precondition(buy ticket online(F rom, o), available connection)
precondition(buy ticket online(F rom, o), available destination(T o)).
psa
plan goal g2 , KBplan
contains:

initiates(apply visa(usa), T, visa(usa))
precondition(apply visa(usa), address(usa))
initiates(book hotel(L), T, address(usa)) holds(in(L, usa), ).
10.2.3 Behaviour
PI called state, time 2, say, generates partial plan goal,
changing state follows. goal g1 acquires three children T1 . are:
g11 = available connection(11 ),
g12 = available destination(denver, 12 ),
a13 = buy ticket online(madrid, denver, 13 ).
Also, consequently, set temporal constraints updated to:
C = {1 < 15, 2 < 15, 11 = 13 , 12 = 13 , 13 < 1 , 1 > 2}.
334

fiComputational Logic Foundations KGP Agents

action a13 generated action initiates goal g1 . Moreover, every plan
generated must satisfy integrity constraints KBplan . particular, precondition
actions tree already hold must generated sub-goals tree.
g11 g12 generated tree above.
via transition SI, following sensing actions added T1 siblings
action a13 18 :
a14 = sense(available connection, 14 )
a15 = sense(available destination(denver), 15 )
constraints
14 = 15 , 14 < 13
added C.
Then, via AE, two sensing actions executed (before original action a1 ),
KB0 updated result sensing follows. Suppose two actions
executed time 5. Consider first action senses fluent available connection.
fluent confirmed physical sensing capability, i.e. available connection : true
X
sensing({available connection, available destination}, 5) = X,
observed(available connection, 5) added KB0 . hand,
available connection : f alse
X above, observed(available connection, 5) added KB0 . cases
14 = 5 added .
neither cases occurs, i.e. sensing capability cannot confirm either
available connection available connection, fact added KB0 . Similarly
precondition, available destination. Let us assume step AE,
KB0 becomes
observed(available connection, 5)
observed(available destination(denver), 5)
AE execute original action a13 . Note agent might decide execute
action even one preconditions known satisfied sensing.
g1 achieved, SR eliminate a13 , a14 , a15 , g11 , g12 state.
resulting state, F = {T2 }, PI called, say time 6. results generating
partial plan g2 , changing state T2 root g2 children
a21 = apply visa(usa, 21 )
g22 = address(usa, 22 )
21 < 2 , 22 = 21 added C. Then, PI, say time 7, introduces
a23 = book hotel(denver, 23 )
18. assume auxiliary part Tcycle contains rule
unreliable pre(As) buy ticket online( , , )

335

fiKakas, Mancarella, Sadri, Stathis & Toni

child g22 T2 , adding 23 < 22 C. Then, AE time 8 executes a23 , adding
KB0 , AE time 9 executes a22 , also updating KB0 . Finally, SR eliminates
actions goals T2 returns empty F state.

11. Related Work
Many proposals exist models architectures individual agents based computational logic foundations (see e.g. survey Fisher, Bordini, Hirsch, & Torroni,
2007). proposals based logic programming, example IMPACT (Arisha, Ozcan, Ross, Subrahmanian, Eiter, & Kraus, 1999; Subrahmanian, Bonatti, Dix,
Eiter, Kraus, Ozcan, & Ross, 2000), AAA (Balduccini & Gelfond, 2008; Baral & Gelfond,
2001), DALI (Costantini & Tocchio, 2004), MINERVA (Leite, Alferes, & Pereira, 2002),
GOLOG (Levesque, Reiter, Lesperance, Lin, & Scherl, 1997), IndiGolog (De Giacomo,
Levesque, & Sardina, 2001). proposals based modal logic first-order logic
approaches, example BDI model (Bratman et al., 1988; Rao & Georgeff, 1997)
extensions deal normative reasoning (Broersen, Dastani, Hulstijn, Huang, &
van der Torre, 2001), Agent0 (Shoham, 1993), AgentSpeak (Rao, 1996) variants,
3APL (Hindriks, de Boer, van der Hoek, & Meyer, 1999) variants (Dastani, Hobo,
& Meyer, 2007).
high level comparison similarities objectives existing
computational logic models agency KGP, aim specifying knowledgerich agents certain desirable behaviours. also similarities finer
details KGP model related work, well differences.
feature KGP which, best knowledge, novel declarative
context-sensitive specification agents cycle. avoid static cycle control
(Rao & Georgeff, 1991; Rao, 1996), KGP relies upon cycle theory determines,
run time, given circumstances individual profile agent, next
step be. cycle theory sensitive solicited unsolicited information
agent receives environment, helps agent adapt behaviour
changes experiences. approach closest work 3APL (Hindriks
et al., 1999) extended Dastani, de Boer, Dignum, Meyer (2003), provides
meta-programming constructs specifying cycle agent goal selection,
plan expansion, execution, well if-then-else while-loop statements. Unlike
imperative constructs 3APL, KGP uses set selection operators extended
model different behaviours types agents. flexible ordering transitions
obtained using preference reasoning transitions applied specific
point time. preferences may change according external events changes
knowledge agent.
Another central distinguishing feature KGP model, comparison existing
models, including based logic programming, modular integration within
single framework abductive logic programming, temporal reasoning, constraint logic
programming, preference reasoning based logic programming priorities, order
support diverse collection capabilities. one specified declaratively
equipped provably correct computational counterpart (see Bracciali,
336

fiComputational Logic Foundations KGP Agents

Demetriou, Endriss, Kakas, Lu, Mancarella, Sadri, Stathis, Terreni, & Toni, 2004,
detailed discussion).
Compared existing logic programming approaches KGP two main similarities
MINERVA (Leite et al., 2002), architecture exploits computational logic
gives declarative operational semantics agents. Unlike KGP, MINERVA
agent consists several specialised, possibly concurrent, sub-agents performing various
tasks, relies upon MDLP (Multidimensional Dynamic Logic Programming) (Leite et al.,
2002). MDLP basic knowledge representation mechanism agent MINERVA,
based extension answer-set programming explicit rules updating
agents knowledge base. KGP instead integrate abductive logic programming
logic programming priorities combined temporal reasoning.
Closely related work KGP logic-based agent architecture reasoning
agents Baral Gelfond (2001). architecture assumes state agents
environment described set fluents evolve time terms transitions
labelled actions. agent also assumed capable correctly observing state
environment, performing actions, remembering history happened
it. agents knowledge base consists action description part specifying internal
agent transitions, domain specific generic KGP. knowledge
base also contains agent observes environment including actions,
KGPs KB0 . temporal aspects agent transitions specified action
language AL implemented A-Prolog, language logic programs answerset programming semantics. answer sets domain specific programs specified AL
correspond plans KGP hypothetical narratives abductive event calculus.
control agent based static observe-think-act cycle, instance KGP
cycle theories. recent refined account overall approach given rise
AAA Architecture, see (Balduccini & Gelfond, 2008) overview.
DALI (Costantini & Tocchio, 2004) logic programming language designed executable specification logical agents. Like KGP, DALI attempts provide constructs
represent reactivity proactivity agent using extended logic programs. DALI
agent contains reactive rules, events, actions aimed interacting external
environment. Behaviour (in terms reactivity proactivity) DALI agent triggered
different event types: external, internal, present, past events. events
actions time stamped record occur. External events like
observations KGP, past events like past observations. However, KGP
support internal events instead idea transitions called cycle
theory trigger reactive proactive behaviour.
IndiGolog (De Giacomo et al., 2001) high-level programming language robots
intelligent agents supports, like KGP, on-line planning, sensing plan execution
dynamic incompletely known environments. member Golog family
languages (Levesque et al., 1997) use Situation Calculus theory action perform
reasoning required executing program. Instead KGP model rely
abductive logic programming logic programming priorities combined temporal reasoning. Instead Situation Calculus KGP use Event Calculus
temporal reasoning, use Event Calculus prerequisite model
InterRaP (Muller, Fischer, & Pischel, 1998), replaced another temporal
337

fiKakas, Mancarella, Sadri, Stathis & Toni

reasoning framework, needed. Apart difference use Situation Event Calculi, IndiGolog goals cannot decided dynamically, whereas
KGP model change dynamically according specifications Goal Decision
capability.
obvious similarity KGP model BDI model (Bratman et al.,
1988) given correspondence KGPs knowledge, goals plan BDIs
beliefs, desires intentions, respectively. Apart fact BDI model
based modal logic, KGP knowledge (beliefs BDI) partitioned modules,
support various reasoning capabilities. KGP also tries bridge gap
specification practical implementation agent. gap criticized
BDI Rao (1996), developed AgentSpeak(L) language. computational
model AgentSpeak(L) formally studied dInverno Luck (1998),
recent implementations AgentSpeak interpreter incorporated Jason
platform (Bordini & Hubner, 2005). Like KGP implementation PROSOCS (Bracciali
et al., 2006), Jason implementation seeks narrow gap specification
executable BDI agent programs. Jason also extends BDI new features like belief
revision (Alechina, Bordini, Hubner, Jago, & Logan, 2006).
particular line work BDI Padgham Lambrix (2005), investigate
notion capability integrated BDI Logic Rao Georgeff (1991),
BDI agent reason capabilities. capability work
informally understood ability act rationally towards achieving particular goal,
sense abstract plan type believed achieve goal. Formally,
BDI logic Rao Georgeff extended incorporate modality capabilities
constrains agent goals intentions compatible agent believes
capabilities. set compatibility axioms presented detailing semantic
conditions capture desired inter-relationships among agents beliefs, capabilities,
goals, intentions. work also summarises extensions BDI model
implemented adapting BDI interpreter include capabilities, arguing
benefits extension original BDI Interpreter Rao Georgeff (1992).
KGP capabilities equate reasoning capabilities agent allow agent
plan actions given state, react incoming observations, decide upon
goals adopt. However, KGP, use capabilities level agents
domain specific knowledge guide agent determining whether rational
adopt particular goal.
issue separation specification implementation exists
KGP model Agent0 (Shoham, 1993), later refinement PLACA (Thomas,
1995). Two differences KGP Agent0 PLACA explicit
links exist KGP model amongst goals (in structuring forest
agent state) richer theories KGP specify priorities amongst potential
goals restricted temporal orderings. explicit links exploited
revising goals state, via Revision transition, light new information
passage time.
BOID architecture (Broersen et al., 2001) extends well known BDI model (Rao
& Georgeff, 1992) obligations, thus giving rise four main components representing
agent: beliefs, obligations, intentions desires. focus BOID find ways
338

fiComputational Logic Foundations KGP Agents

resolving conflicts amongst components. order define agent types,
including well known types agent theories realistic, selfish, social simple
minded agents. agent types differ give different priorities rules
four components. instance, simple minded agent gives higher priority
intentions, compared desires obligations, whereas social agent gives higher priority
obligations desires. use priorities propositional logic formulae specify
four components agent types.
existing KGP model already resolves conflicts BOID tries address. example, conflict belief prior intention, means
intended action longer executed due changes environment,
KGP agent notice give higher priority belief prior
intention, allowing agent effect retract intended action and, time permitting,
replan goals. KGP model also includes notion priority used Goal
Decision capability cycle theory controls behaviour agent.
KGP model also extended deal normative concepts, extended model
known N-KGP (Sadri, Stathis, & Toni, 2006). N-KGP common BOID
seeks extend KGP addition obligations. N-KGP model also
extends notion priorities incorporating amongst different types goals
actions. detailed comparison N-KGP related work presented Sadri, Stathis,
Toni (2006).
features included approaches absent
KGP model. BDI and, so, IMPACT system (Arisha et al., 1999; Subrahmanian
et al., 2000) allow agents knowledge bases representations knowledge
agents. systems allow agents degree introspection
ability reason agents beliefs reasoning. KGP model date
include features. IMPACT also allows incorporation legacy systems, possibly using diverse languages, richer knowledge base language including
deontic concepts probabilities. Similarly, 3APL, system based combination
imperative logic programming languages, includes optimisation component
absent KGP. component 3APL includes rules identify given
situation agent pursuing suboptimal plan, help agent find better way
achieving goals. 3APL also includes additional functionalities learning (van
Otterlo, Wiering, Dastani, & Meyer, 2003), model currently support.
2APL (Dastani et al., 2007) extension 3APL goals goal-plan rules well
external internal events. 2APL customisable (via graphical interface) cycle
fixed customised.

12. Conclusions
presented computational logic foundations KGP model agency.
model allows specification heterogeneous agents interact other,
exhibit proactive reactive behaviour allowing function dynamic
environments adjusting goals plans changes happen environments. KGP incorporates highly modular agent architecture integrates collection
339

fiKakas, Mancarella, Sadri, Stathis & Toni

reasoning sensing capabilities, synthesised within transitions, orchestrated cycle
theories take account dynamic context agent preferences.
formal specification KGP components within computational logic
major advantage facilitating formal analysis model direct verifiable
implementation. formal analysis started Sadri Toni (2006),
give formal analysis KGP agents exploring effectiveness terms goal
achievement, reactive awareness, impact reasoning capabilities towards
progress goal achievement. implementation precursor model, described
Kakas et al. (2004b), already developed within PROSOCS platform
Stathis et al. (2004) upon provably correct computational counterparts defined
component model given Kakas et al. (2004b). Concrete choices
computational counterparts described Bracciali et al. (2004). resulting
development framework allows deployment testing functionality earlier
variant KGP agents. Deployment agents relies upon agent template designed
Stathis et al. (2002), builds upon previous work head/body metaphor
described Steiner et al. (1991) Haugeneder et al. (1994), mind/body architecture introduced Bell (1995) recently used Huang, Eliens, de Bra (2001).
development platform applied number practical applications, and,
particular, ambient intelligence Stathis Toni (2004). Also, Sadri (2005)
provided guidelines specifying applications using KGP agents. Future work includes implementing deploying revised KGP model given paper: envisage
pose limited conceptual challenges, able capitalise experience
implementing deploying precursor model.
Sadri, Stathis, Toni (2006) explored precursor KGP agent
model augmented normative features allowing agents reason
choose social personal goals, prohibitions obligations. would
interesting continue work finalised KGP model given paper.
Sadri Toni (2005) developed number different profiles behaviour,
defined terms specific cycle theories, formally proved advantages given
circumstances. would interesting explore dimension further, characterise
different agent personalities provide guidance, formal properties,
type personality needed applications.
Future work also includes extending model incorporate (i) reasoning capabilities, including knowledge revision (e.g. Inductive Logic Programming),
sophisticated forms temporal reasoning, including identifying explanations unexpected
observations, (ii) introspective reasoning reasoning beliefs agents,
(iii) experimentation model via implementation, (iv) development
concurrent implementation.

Acknowledgments
work supported EU FET Global Computing Initiative, within SOCS
project (IST-2001-32530). wish thank colleagues SOCS useful discussions
development KGP. also grateful Chitta Baral anonymous
referees helpful comments earlier version paper.
340

fiComputational Logic Foundations KGP Agents

Appendix A. Normal Cycle Theory
give main parts normal Tcycle , exclude others, example
definitions incompatible auxiliary part, including definitions predicates
empty f orest, unreliable pre etc. details see (Kakas et al., 2005).
Tinitial : consists following rules:
R0|GI (S0 , {}) : GI(S0 , {}) empty f orest(S0 )
R0|AE (S0 , As) : AE(S0 , As) empty non executable goals(S0 ), = fAS (S0 , t),
6= {}, time now(t)
R0|P (S0 , G) : P I(S0 , G) Gs = fGS (S0 , t), Gs 6= {}, G Gs, time now(t)
Tbasic : consists following rules:
rules deciding might follow AE transition follows:
RAE|P (S 0 , G) : P I(S 0 , G) AE(S, As, 0 , t), Gs = fGS (S 0 , t0 ), Gs 6= {},
G Gs, time now(t0 )
0
0
RAE|AE (S , ) : AE(S 0 , As0 ) AE(S, As, 0 , t), As0 = fAS (S 0 , t0 ),
As0 6= {}, time now(t0 )
RAE|AOI (S 0 , F s) : AOI(S 0 , F s) AE(S, As, 0 , t), F = fES (S 0 , t0 ),
F 6= {}, time now(t0 )
RAE|SR (S 0 ) : SR(S 0 , {}) AE(S, As, 0 , t)
RAE|GI (S 0 , {}) : GI(S 0 , {}) AE(S, As, 0 , t)
Namely, AE could followed another AE, PI, AOI, SR,
GI, POI.
rules deciding might follow SR follows
RSR|P (S 0 , G) : P I(S 0 , G) SR(S, {}, 0 , t), Gs = fGS (S 0 , t0 ), Gs 6= {}, G Gs,
time now(t0 )
RSR|GI (S 0 , {}) : GI(S 0 , {}) SR(S, {}, 0 , t), Gs = fGS (S 0 , t0 ), Gs = {},
time now(t0 )
0
RSR|AE (S , As) : AE(S 0 , As) SR(S, {}, 0 , t), = fGS (S 0 , t0 ), 6= {},
time now(t0 )
Namely, SR followed PI GI AE, depending whether
goals plan state.
rules deciding might follow PI follows
RP I|AE (S 0 , As) : AE(S 0 , As) P I(S, G, 0 , t), = fAS (S 0 , t0 ), 6= {},
time now(t0 )
0
RP I|SI (S , P s) : SI(S 0 , P s) P I(S, G, 0 , t), P = fP (S 0 , t0 ), P 6= {}, time now(t0 )
second rule allow possibility sensing preconditions action
execution.
rules deciding might follow GI follows
RGI|RE (S 0 , {}) : RE(S 0 , {}) GI(S, {}, 0 , t)
RGI|P (S 0 , G) : P I(S 0 , G) GI(S, {}, 0 , t), Gs = fGS (S 0 , t0 ), Gs 6= {}, G Gs,
time now(t0 )
Namely, GI followed PI, goals plan for.
rules deciding might follow follows
341

fiKakas, Mancarella, Sadri, Stathis & Toni

RRE|P (S 0 , G) : P I(S 0 , G) RE(S, {}, 0 , t), Gs = fGS (S 0 , t0 ), Gs 6= {}, G Gs,
time now(t0 )
RRE|SI (S 0 , P s) : SI(S 0 , P s) RE(S, {}, 0 , t), P = fP (S 0 , t0 ), P 6= {},
time now(t0 )
rules deciding might follow SI follows
RSI|AE (S 0 , As) : AE(S 0 , As) SI(S, P s, 0 , t), = fAS (S 0 , t0 ), 6= {},
time now(t0 )
0
RSI|SR (S , {}) : SR(S 0 , {}) SI(S, P s, 0 , t)
rules deciding might follow AOI follows
RAOI|AE (S 0 , As) : AE(S 0 , As) AOI(S, F s, 0 , t), = fAS (S 0 , t0 ), 6= {},
time now(t0 )
0
RAOI|SR (S , {}) : SR(S 0 , {}) AOI(S, F s, 0 , t)
RAOI|SI (S 0 , P s) : SI(S 0 , P s) AOI(S, F s, 0 , t), P = fP (S 0 , t0 ), P 6= {},
time now(t0 )
rules deciding might follow POI follows
RP OI|GI (S 0 , {}) : GI(S 0 , {}) P OI(S, {}, 0 , t)
Tbehaviour : consists following rules:
GI given higher priority trees state:

PGIT
0 : RT |GI (S, {}) RT |T 0 (S, X) empty f orest(S)
transitions T, 0 , 0 6= GI, possibly 0 (indicating
trees initial state agent, GI first transition).
GI also given higher priority POI:
P OI : R
0
0
PGIT
P OI|GI (S , {}) RP OI|T (S, {}, )
transitions 6= GI.
GI, transition given higher priority:
GI
PRET
: RGI|RE (S, {}) RGI|T (S, X)
transitions 6= RE.
RE, transition PI given higher priority:
PPRE
: RRE|P (S, G) RRE|T (S, X)
transitions 6= P I.
PI, transition AE given higher priority, unless actions
actions selected execution whose preconditions unreliable need checking,
case SI given higher priority:
PI
PAET
: RP I|AE (S, As) RP I|T (S, X) unreliable pre(As)
transitions 6= AE.
PI
: RP I|SI (S, P s) RP I|AE (S, As) unreliable pre(As)
PSIAE
SI, transition AE given higher priority
SI
: RSI|AE (S, As) RSI|T (S, X)
PAET
transitions 6= AE.
AE, transition AE given higher priority
actions execute state, case either AOI SR given higher
priority, depending whether actions unreliable, sense
effects need checking, not:
342

fiComputational Logic Foundations KGP Agents

AE
PAET
: RAE|AE (S, As) RAE|T (S, X)
transitions 6= AE. Note that, definition Tbasic , transition AE applicable
still actions executed state.
AE
AE
PAOIT
: RAE|AOI (S, F s) RAE|T (S, X)) BCAOI|T
(S, F s, t), time now(t)
AE
transitions 6= AOI, behaviour condition BCAOI|T (S, F s, t) defined (in
auxiliary part) by:
AE
BCAOI|T
(S, F S, t) empty executable goals(S, t), unreliable ef f ect(S, t)
Similarly, have:
AE (S, t), time now(t)
AE
: RAE|SR (S, {}) RAE|T (S, X)) BCSR|T
PSRT
transitions 6= SR where:
AE (S, t) empty executable goals(S, t), unreliable ef f ect(S, t)
BCSR|T
Here, assume auxiliary part Tcycle specifies whether given set actions
contains unreliable action, sense expressed unreliable ef f ect, defines
predicate empty executable goals.

SR, transition PI higher priority:
PPSR
: RSR|P (S, G) RSR|T (S, X))
transitions 6= P I.
Note that, definition Tbasic , transition PI applicable still goals
plan state. actions goals left state, rule RGI|T
would apply.
initial state PI given higher priority:
PP0 : R0|P (S, G) R0|T (S, X)
transitions 6= P I. Note that, definition Tinitial below, transition PI
applicable initially goals plan initial state.

343

fiKakas, Mancarella, Sadri, Stathis & Toni

References
Alechina, N., Bordini, R. H., Hubner, J. F., Jago, M., & Logan, B. (2006). Belief Revision
AgentSpeak Agents. Nakashima, H., Wellman, M. P., Weiss, G., & Stone, P.
(Eds.), 5th International Joint Conference Autonomous Agents Multiagent
Systems (AAMAS 2006), pp. 12881290, Hakodate, Japan. ACM.
Arisha, K. A., Ozcan, F., Ross, R., Subrahmanian, V. S., Eiter, T., & Kraus, S. (1999).
IMPACT: Platform Collaborating Agents. IEEE Intelligent Systems, 14 (2),
6472.
Balduccini, M., & Gelfond, M. (2008). AAA Architecture: Overview. AAAI
Spring Symposium Architectures Intelligent Theory-Based Agents (AITA08).
Baral, C., & Gelfond, M. (2001). Reasoning agents dynamic domains. Logic-based
artificial intelligence, pp. 257279. Kluwer Academic Publishers, Norwell, MA, USA.
Bell, J. (1995). Planning Theory Practical Rationality. Proceedings AAAI95
Fall Symposium Rational Agency, pp. 14. AAAI Press.
Bordini, R. H., & Hubner, J. F. (2005). BDI Agent Programming AgentSpeak using Jason
(Tutorial Paper). Toni, F., & Torroni, P. (Eds.), Computational Logic MultiAgent Systems, 6th International Workshop, CLIMA VI, Lecture Notes Computer
Science, pp. 143164. Springer.
Bracciali, A., & Kakas, A. (2004). Frame consistency: Reasoning explanations.
Proceedings 10th International Workshop Non-Monotonic Reasoning
(NMR2004), Whistler BC, Canada.
Bracciali, A., Demetriou, N., Endriss, U., Kakas, A. C., Lu, W., Mancarella, P., Sadri, F.,
Stathis, K., Terreni, G., & Toni, F. (2004). KGP Model Agency Global
Computing: Computational Model Prototype Implementation. Priami, C., &
Quaglia, P. (Eds.), Global Computing, pp. 340367, Rovereto, Italy. Springer.
Bracciali, A., Endriss, U., Demetriou, N., Kakas, A. C., Lu, W., & Stathis, K. (2006).
Crafting Mind PROSOCS Agents. Applied Artificial Intelligence, 20 (2-4), 105
131.
Bratman, M., Israel, D., & Pollack, M. (1988). Plans resource-bounded practical reasoning. Computational Intelligence, 4.
Broersen, J., Dastani, M., Hulstijn, J., Huang, Z., & van der Torre, L. (2001). BOID
Architecture: conficts Beliefs, Obligations, Intentions Desires. Proceedings Fifth International Conference Autonomous Agents (Agents 2001), pp.
916. ACM Press, Montreal, Canada.
Clark, K. L. (1978). Negation Failure. Gallaire, H., & Minker, J. (Eds.), Logic
Data Bases, pp. 293322. Plenum Press.
Costantini, S., & Tocchio, A. (2004). DALI Logic Programming Agent-Oriented Language. Alferes, J. J., & Leite, J. A. (Eds.), Proceedings 9th European Conference Logics Artificial Intelligence, (JELIA 2004), Vol. 3229 Lecture Notes
Computer Science, pp. 685688. Springer.
344

fiComputational Logic Foundations KGP Agents

Dastani, M., de Boer, F., Dignum, F., & Meyer, J.-J. (2003). Programming Agent Deliberation: approach illustrated using 3APL Language. Autonomous Agents
Mult-agent Systems (AAMAS03), pp. 97104, Australia.
Dastani, M., Hobo, D., & Meyer, J.-J. (2007). Practical Extensions Agent Programming
Languages. Proceedings Sixth International Joint Conference Autonomous
Agents Multiagent Systems (AAMAS07). ACM Press.
de Bruijn, O., & Stathis, K. (2003). Socio-Cognitive Grids: Net Universal Human
Resource. Kameas, A., & Streitz, N. (Eds.), Proceedings Conference Tales
Disappearing Computer, pp. 211218, Santorini. CTI Press.
De Giacomo, G., Levesque, H. J., & Sardina, S. (2001). Incremental execution guarded
theories. ACM Transactions Computational Logic, 2 (4), 495525.
dInverno, M., & Luck, M. (1998). Engineering AgentSpeak(L): Formal Computational
Model. J. Log. Comput., 8 (3), 233260.
FIPA Communicative Act Library Specification (2001a). Experimental specification
XC00037H. Foundation Intelligent Physical Agents, http://www.fipa.org.
FIPA Query Interaction Protocol (2001b). Experimental specification XC00027F. Foundation Intelligent Physical Agents, http://www.fipa.org.
Fisher, M., Bordini, R., Hirsch, B., & Torroni, P. (2007). Computational Logics Agents:
Road Map Current Technologies Future Trends. Computational Intelligence,
23 (1), 6191.
Haugeneder, H., Steiner, D., & McCabe, F. (1994). IMAGINE: framework building
multi-agent systems. Deen, S. M. (Ed.), Proceedings 1994 International
Working Conference Cooperating Knowledge Based Systems (CKBS-94), pp. 31
64, DAKE Centre, University Keele, UK.
Hindriks, K. V., de Boer, F. S., van der Hoek, W., & Meyer, J. C. (1999). Agent programming 3APL. Autonomous Agents Multi-Agent Systems, 2(4), 357401.
Huang, Z., Eliens, A., & de Bra, P. (2001). Architecture Web Agents. Proceedings
EUROMEDIA01. SCS.
Jaffar, J., & Maher, M. (1994). Constraint logic programming: survey. Journal Logic
Programming, 19-20, 503582.
Kakas, A. C., Kowalski, R. A., & Toni, F. (1998). role abduction logic programming. Gabbay, D. M., Hogger, C. J., & Robinson, J. A. (Eds.), Handbook Logic
Artificial Intelligence Logic Programming, Vol. 5, pp. 235324. Oxford University
Press.
Kakas, A. C., Mancarella, P., & Dung, P. M. (1994). acceptability semantics logic
programs. Proceedings eleventh international conference Logic programming, pp. 504519, Cambridge, MA, USA. MIT Press.
Kakas, A. C., & Miller, R. (1997). simple declarative language describing narratives
actions. Logic Programming, 31.
345

fiKakas, Mancarella, Sadri, Stathis & Toni

Kakas, A. C., & Moraitis, P. (2003). Argumentation based decision making autonomous
agents. Rosenschein, J. S., Sandholm, T., Wooldridge, M., & Yokoo, M. (Eds.),
Proceedings Second International Joint Conference Autonomous Agents
Multiagent Systems (AAMAS-2003), pp. 883890, Melbourne, Victoria. ACM Press.
Kakas, A., Mancarella, P., Sadri, F., Stathis, K., & Toni, F. (2005). Declarative Agent
Control. Leite, J., & Torroni, P. (Eds.), CLIMA V: Computational Logic MultiAgent Systems, Vol. 3487 Lecture Notes Artificial Intelligence (LNAI), pp. 96
110. Springer Verlag.
Kakas, A. C., Kowalski, R. A., & Toni, F. (1992). Abductive Logic Programming. J. Log.
Comput., 2 (6), 719770.
Kakas, A. C., Mancarella, P., Sadri, F., Stathis, K., & Toni, F. (2004a). Declarative Agent
Control. Leite, J. A., & Torroni, P. (Eds.), Computational Logic Multi-Agent Systems, 5th International Workshop, CLIMA V, Vol. 3487 Lecture Notes Computer
Science, pp. 96110. Springer.
Kakas, A. C., Mancarella, P., Sadri, F., Stathis, K., & Toni, F. (2004b). KGP Model
Agency. de Mantaras, R. L., & Saitta, L. (Eds.), Proceedings 16th Eureopean
Conference Artificial Intelligence (ECAI 2004), pp. 3337. IOS Press.
Kowalski, R. A., & Sergot, M. (1986). logic-based calculus events. New Generation
Computing, 4 (1), 6795.
Kowalski, R., & Toni, F. (1996). Abstract argumentation. Artificial Intelligence Law
Journal, Special Issue Logical Models Argumentation, 4 (3-4), 275296. Kluwer
Academic Publishers.
Leite, J. A., Alferes, J. J., & Pereira, L. M. (2002). MIN ERVA: dynamic logic programming agent architecture. Intelligent Agents VIII: 8th International Workshop,
ATAL 2001, Seattle, WA, USA, Revised Papers, Vol. 2333 Lecture Notes Artificial Intelligence, pp. 141157.
Levesque, H. J., Reiter, R., Lesperance, Y., Lin, F., & Scherl, R. B. (1997). GOLOG:
logic programming language dynamic domains. Journal Logic Programming,
31 (1-3), 5983.
Mamdani, E. H., Pitt, J., & Stathis, K. (1999). Connected Communities Standpoint
Multi-agent Systems. New Generation Computing, 17 (4), 381393.
Mancarella, P., Sadri, F., Terreni, G., & Toni, F. (2004). Planning partially situated
agents. Leite, J. A., & Torroni, P. (Eds.), Computational Logic Multi-Agent Systems, 5th International Workshop, CLIMA V, Vol. 3487 Lecture Notes Computer
Science, pp. 230248. Springer.
Miller, R., & Shanahan, M. (2002). alternative formulations event calculus.
Kakas, A. C., & Sadri, F. (Eds.), Computational Logic: Logic Programming
Beyond - Essays Honour Robert A. Kowalski, Vol. 2408 Lecture Notes
Computer Science, pp. 452490. Springer.
Muller, J., Fischer, K., & Pischel, M. (1998). Pragmatic BDI Architecture. Huhns,
M. N., & Singh, M. P. (Eds.), Readings Agents, pp. 217225. Morgan Kaufmann
Publishers.
346

fiComputational Logic Foundations KGP Agents

Padgham, L., & Lambrix, P. (2005). Formalisations capabilities BDI-agents. Autonomous Agents Multi-Agent Systems, 10 (3), 249271.
Prakken, H., & Sartor, G. (1996). system defeasible argumentation, defeasible
priorities. International Conference Formal Applied Practical Reasoning,
Springer Lecture Notes AI 1085, pp. 510524.
Prakken, H., & Sartor, G. (1997). Argument-based extended logic programming defeasible priorities. Journal Applied Non-Classical Logics, 7 (1), 2575.
Rao, A. S. (1996). AgentSpeak(L): BDI agents speak logical computable language.
van Hoe, R. (Ed.), Agents Breaking Away, 7th European Workshop Modelling
Autonomous Agents Multi-Agent World, MAAMAW96, Eindhoven, Netherlands, January 22-25, 1996, Proceedings, Vol. 1038 Lecture Notes Computer
Science, pp. 4255. Springer-Verlag.
Rao, A. S., & Georgeff, M. P. (1991). Modeling Rational Agents within BDI-architecture.
Fikes, R., & Sandewall, E. (Eds.), Proceedings Knowledge Representation
Reasoning (KR&R-91), pp. 473484. Morgan Kaufmann Publishers.
Rao, A. S., & Georgeff, M. P. (1997). Modeling rational agents within BDI-architecture.
Huhns, M. N., & Singh, M. P. (Eds.), Readings Agents, pp. 317328. Morgan
Kaufmann Publishers, San Francisco, CA, USA.
Rao, A. S., & Georgeff, M. P. (1992). abstract architecture rational agents. Nebel,
B., Rich, C., & R.Swartout, W. (Eds.), 3rd International Conference Principles
Knowledge Representation Reasoning (KR92), pp. 439449, Cambridge, MA,
USA. Morgan Kaufmann.
Sadri, F. (2005). Using KGP model agency design applications (Tutorial Paper).
Toni, F., & Torroni, P. (Eds.), Computational Logic Multi-Agent Systems, 6th
International Workshop, CLIMA VI, Vol. 3900 Lecture Notes Computer Science,
pp. 165185. Springer.
Sadri, F., Stathis, K., & Toni, F. (2006). Normative KGP agents. Computational & Mathematical Organization Theory, 12 (2-3), 101126.
Sadri, F., & Toni, F. (2005). Variety behaviours profiles logic-based agents.
Toni, F., & Torroni, P. (Eds.), Computational Logic Multi-Agent Systems, 6th
International Workshop, CLIMA VI, Vol. 3900 Lecture Notes Computer Science,
pp. 206225. Springer.
Sadri, F., & Toni, F. (2006). Formal Analysis KGP agents. Fisher, M., van der Hoek,
W., Konev, B., & Lisitsa, A. (Eds.), Logics Artificial Intelligence, 10th European
Conference, JELIA 2006, Vol. 4160 Lecture Notes Computer Science, pp. 413
425. Springer.
Shanahan, M. (1997). Solving Frame Problem. MIT Press.
Shanahan, M. (1989). Prediction deduction explanation abduction. Proceedings
11th International Joint Conference Artificial Intelligence, pp. 10551060.
Shoham, Y. (1993). Agent-oriented programming. Artificial Intelligence, 60 (1), 5192.
347

fiKakas, Mancarella, Sadri, Stathis & Toni

SOCS (2007). Societies ComputeeS: computational logic model description,
analysis verification global open societies heterogeneous computees.
http://lia.deis.unibo.it/research/socs/.
Stathis, K., Child, C., Lu, W., & Lekeas, G. K. (2002). Agents Environments.
Tech. rep. Technical Report IST32530/CITY/005/DN/I/a1, SOCS Consortium, 2002.
IST32530/CITY/005/DN/I/a1.
Stathis, K., Kakas, A., Lu, W., Demetriou, N., Endriss, U., & Bracciali, A. (2004).
PROSOCS: platform programming software agents computational logic.
Muller, J., & Petta, P. (Eds.), Proceedings Agent Theory Agent Implementation (AT2AI-4 EMCSR2004 Session M), pp. 523528, Vienna, Austria.
Stathis, K., & Toni, F. (2004). Ambient Intelligence using KGP Agents. Markopoulos, P.,
Eggen, B., Aarts, E. H. L., & Crowley, J. L. (Eds.), Ambient Intelligence: Proceedings
Second European Symposium, EUSAI 2004, Vol. 3295 Lecture Notes Computer
Science, pp. 351362. Springer.
Steiner, D. E., Haugeneder, H., & Mahling, D. (1991). Collaboration knowledge bases
via knowledge based collaboration. Deen, S. M. (Ed.), CKBS-90 Proceedings
International Working Conference Cooperating Knowledge Based Systems, pp.
113133. Springer Verlag.
Subrahmanian, V. S., Bonatti, P., Dix, J., Eiter, T., Kraus, S., Ozcan, F., & Ross, R. (2000).
Heterogeneous Agent Systems. MIT Press/AAAI Press, Cambridge, MA, USA.
Thomas, S. R. (1995). PLACA agent programming language. Wooldridge, M. J., &
Jennings, N. R. (Eds.), Intelligent Agents, pp. 355370, Berlin. Springer-Verlag.
van Otterlo, M., Wiering, M., Dastani, M., & Meyer, J.-J. (2003). Characterization
Sapient Agents. Hexmoor, H. (Ed.), International Conference Integration
Knowledge Intensive Multi-Agent Systems (KIMAS-03), pp. 172177, Boston, Massachusetts. IEEE.
Wooldridge, M. (2002). Introduction Multiagent Systems. John Wiley & Sons.
Yip, A., Forth, J., Stathis, K., & Kakas, A. C. (2005). Software Anatomy KGP Agent.
Gleizes, M. P., Kaminka, G. A., Nowe, A., Ossowski, S., Tuyls, K., & Verbeeck, K.
(Eds.), EUMAS 2005 - Proceedings Third European Workshop Multi-Agent
Systems, pp. 459472. Koninklijke Vlaamse Academie van Belie voor Wetenschappen
en Kunsten.

348

fiJournal Artificial Intelligence Research 33 (2008) 79-107

Submitted 07/07; published 09/08

Use Automatically Acquired Examples
All-Nouns Word Sense Disambiguation
David Martinez

davidm@csse.unimelb.edu.au

University Melbourne
3010, Melbourne, Australia

Oier Lopez de Lacalle

oier.lopezdelacalle@ehu.es

University Basque Country
20018, Donostia, Basque Country

Eneko Agirre

e.agirre@ehu.es

University Basque Country
20018, Donostia, Basque Country

Abstract
article focuses Word Sense Disambiguation (WSD), Natural Language Processing task thought important many Language Technology
applications, Information Retrieval, Information Extraction, Machine Translation. One main issues preventing deployment WSD technology lack
training examples Machine Learning systems, also known Knowledge Acquisition Bottleneck. method shown work small samples words
automatic acquisition examples. previously shown one
promising example acquisition methods scales produces freely available database
150 million examples Web snippets polysemous nouns WordNet.
paper focuses issues arise using examples, alone addition
manually tagged examples, train supervised WSD system nouns. extensive
evaluation lexical-sample all-words Senseval benchmarks shows
able improve commonly used baselines achieve top-rank performance.
good use prior distributions senses proved crucial factor.

1. Introduction
paper devoted Word Sense Disambiguation (WSD) task Natural Language
Processing (NLP). goal task determine senses words
appear context. instance, given sentence took money bank.,
focus word bank, goal would identify intended sense,
context would financial sense, instead possibilities like edge
river sense. senses defined dictionary, knowledge-base ontology.
task defined intermediate step towards natural language understanding.
construction efficient algorithms WSD would benefit many NLP applications
Machine Translation (MT), Information Retrieval (IR) systems (Resnik, 2006).
instance, MT system translate previous example French, would
need choose among possible translations word bank. word
translated banque used financial sense (as example),
rive used edge river sense. See work Vickrey, Biewald,
c
2008
AI Access Foundation. rights reserved.

fiMartinez, Lopez de Lacalle & Agirre

Teyssier, Koller (2005) recent evaluation cross-lingual WSD MT. IR
engines, would also useful determine sense word query
order retrieve relevant documents, specially working multilingual documents
Cross-Language Information Retrieval (CLIR), IR scenarios recall key
performance factor, retrieving images captions. evidence favor
using WSD IR gathered lately (Kim, Seo, & Rim, 2004; Liu, Liu, Yu, & Meng,
2004; Stevenson & Clough, 2004; Vossen, Rigau, Alegra, Agirre, Farwell, & Fuentes, 2006).
WSD techniques also fill important role context Semantic Web.
Web grown focusing human communication, rather automatic processing.
Semantic Web vision automatic agents working information
Web semantic level, achieving interoperability use common terminologies
ontologies (Daconta, Obrst, & Smith, 2005). Unfortunately information
Web unstructured textual form. task linking terms texts
concepts reference ontology paramount Semantic Web.
Narrower domains like Biomedicine also calling WSD techniques. Unified
Medical Language System (UMLS) (Humphreys, Lindberg, Schoolman, & Barnett, 1998)
one extensive ontologies field, studies mapping terms medical
documents resource reported high levels ambiguity, calls WSD
technology (Weeber, Mork, & Aronson, 2001).
WSD received attention many groups researchers, general NLP books
dedicating separate chapters WSD (Manning & Schutze, 1999; Jurafsky & Martin, 2000;
Dale, Moisl, & Somers, 2000), special issues WSD NLP journals (Ide & Veronis,
1998; Edmonds & Kilgarriff, 2002), books devoted specifically issue (Ravin &
Leacock, 2001; Stevenson, 2003; Agirre & Edmonds, 2006). interested reader start
dedicated chapter Manning Schutze (1999) WSD book (Agirre
& Edmonds, 2006). widespread interest motivated Senseval initiative1 ,
joined different research groups common WSD evaluation framework since 1998.
goal follow example successful competitive evaluations, like DUC
(Document Understanding Conference) TREC (Text Retrieval Conference).
WSD systems classified according knowledge use build models, derived different resources like corpora, dictionaries, ontologies.
Another distinction drawn corpus-based systems, distinguishing
rely hand-tagged corpora (supervised systems), require resource (unsupervised systems). distinction important effort required
hand-tag senses high, would costly obtain tagged examples word
senses languages, estimations show (Mihalcea & Chklovski, 2003). spite
drawback (referred knowledge acquisition bottleneck), recent
efforts devoted improvement supervised systems, ones
obtain highest performance, even current low amounts training data.
systems rely sophisticated Machine Learning (ML) algorithms construct
models based features extracted training examples.
Alternatively, Senseval defines two kinds WSD tasks: lexical-sample all-words.
lexical-sample task systems need disambiguate specific occurrences handful
1. http://www.senseval.org

80

fiOn Use Automatically Acquired Examples All-Nouns WSD

words relatively large numbers training examples provided (more
100 examples cases). all-words task, training data provided, testing
done whole documents. Systems need tag content words occurring texts,
even small amounts external training data available.
analysis results English lexical-sample exercise third edition
Senseval (Mihalcea & Edmonds, 2004) suggested plateau performance
reached ML methods. task, systems relatively large amounts
training data, many systems top, performing close other.
systems able significantly improve baselines attained accuracies
70% (Mihalcea, Chklovski, & Killgariff, 2004).
case different all-words task (Snyder & Palmer, 2004), supervised
systems also performed best. used training examples Semcor (Miller, Leacock,
Tengi, & Bunker, 1993), sizable all-words sense-tagged corpus time
writing paper. scarcity examples use test documents corpora
unrelated Semcor heavily affected performance, systems scored
baseline method assigning frequent sense Semcor. order useful
NLP applications, WSD systems address knowledge acquisition bottleneck
(or least significant part) word types, evaluated all-words tasks.
Lexical-sample tasks useful evaluating WSD systems ideal conditions (i.e.
regarding availability training data), show systems scalable
words vocabulary. work use lexical-sample task order
adjust parameters system, main evaluation all-words task.
experiments designed accordingly: lexical-sample tests show empirical evidence
specific parameters, all-words evaluation compares systems state
art.
article, explore method alleviate knowledge acquisition bottleneck
large scale. use WordNet (Fellbaum, 1998) automatically acquire examples
Web. seminal work Leacock, Chodorow, Miller (1998) showed approach
promising, good results small sample nouns. works field
automatic acquisition examples focused exploring different approaches
acquisition process (Agirre, Ansa, Martinez, & Hovy, 2000; Mihalcea, 2002; Cuadros, Padro,
& Rigau, 2006), straightforward application WSD. explorations typically
required costly querying Web, thus tried limited number variations
handful words. approach different spirit: want go whole
process nouns, acquisition examples use WSD
thorough evaluation Senseval 2 lexical-sample Senseval 3 all-words datasets.
comes cost exploring different possibilities step,
advantage showing results extensive, limited small set
nouns.
reasons, given prior work acquisition techniques, use
efficient effective example acquisition method according independent experiments
performed Agirre et al. (2000) Cuadros et al. (2006). focus paper thus
issues arise using examples training data supervised ML
system. paper show automatically acquired examples effectively
81

fiMartinez, Lopez de Lacalle & Agirre

used without pre-existing data, deciding amount examples use
sense (the prior distribution) key issue.
objectives paper show existing methods acquire examples
Web scale-up nouns, study issues arise examples
used training data all-nouns WSD system. goal build stateof-the-art WSD system nouns using automatically retrieved examples.
Given cost large-scale example acquisition, decided limit scope
work nouns. think noun disambiguation useful tool
many applications, specially IR tasks mentioned above. method easily
adapted verbs adjectives (Cuadros et al., 2006), plan pursue line
future.
work reported partially published two previous conference papers.
method automatic acquisition examples described Agirre Lopez
de Lacalle (2004). first try application examples Word Sense Disambiguation presented Agirre Martinez (2004b). paper present global
view whole system, together thorough evaluation, shows
automatically acquired examples used build state-of-the-art WSD systems
variety settings.
article structured follows. introduction, related work knowledge acquisition bottleneck WSD described Section 2, focus automatic
example acquisition. Section 3 introduces method automatically build SenseCorpus,
automatically acquired examples WordNet senses. Section 4 describes experimental setting. Section 5 explores factors use SenseCorpus evaluates
lexical-sample task. final systems thoroughly evaluated all-nouns
task Section 6. Finally, Section 7 provides discussion, conclusions
work outlined Section 8.

2. Related Work
construction WSD systems applicable words goal many research initiatives. section describe related work looks ways
alleviate knowledge acquisition bottleneck using following techniques: bootstrapping, active learning, parallel corpora, automatic acquisition examples acquisition
topic signatures. Sections 5 6, evaluate proposed system public datasets,
review best performing systems literature.
Bootstrapping techniques consist algorithms learn instances labeled
data (seeds) big set unlabeled examples. Among approaches, highlight
co-training (Blum & Mitchell, 1998) derivatives (Collins & Singer, 1999; Abney,
2002). techniques appropriate WSD NLP tasks
wide availability untagged data scarcity tagged data. However,
systems shown perform well fine-grained WSD. well-known work,
Yarowsky (1995) applied iterative bootstrapping process induce classifier based
Decision Lists. minimum set seed examples, disambiguation results comparable
supervised methods obtained limited set binary sense distinctions,
success extended fine-grained senses.
82

fiOn Use Automatically Acquired Examples All-Nouns WSD

Recent work bootstrapping applied WSD also reported Mihalcea (2004)
Pham, Ng, Lee (2005). former, use unlabeled data significantly
increases performance lexical-sample system. latter, Pham et al. apply
WSD classifier all-words task Senseval-2, targeting words threshold
frequency Semcor WSJ corpora. observe slight increase accuracy
relying unlabeled data.
Active learning used choose informative examples hand-tagging, order
reduce manual cost. one works directly applied WSD, Fujii, Inui, Tokunaga, Tanaka (1998) used selective sampling acquisition examples
disambiguation verb senses, iterative process human taggers. informative
examples chosen following two criteria: maximum number neighbors unsupervised
data, minimum similarity supervised example set. Another active learning
approach Open Mind Word Expert (Mihalcea & Chklovski, 2003), project
collect sense-tagged examples Web users. system selects examples
tagged applying selective sampling method based two different classifiers, choosing
unlabeled examples disagreement. collected data used
Senseval-3 English lexical-sample task.
Parallel corpora another alternative avoid need hand-tagged data. Recently
Chan Ng (2005) built classifier English-Chinese parallel corpora. grouped
senses share Chinese translation, occurrences word
English side parallel corpora considered disambiguated sense
tagged appropriate Chinese translations. system successfully evaluated
all-words task Senseval-2. However, parallel corpora expensive resource
obtain target words. related approach use monolingual corpora second
language use bilingual dictionaries translate training data (Wang & Carroll,
2005). Instead using bilingual dictionaries, Wang Martinez (2006) applied machine
translation text snippets foreign languages back English achieved good results
English lexical-sample WSD.
automatic acquisition training examples, external lexical resource (WordNet,
instance) sense-tagged corpus used obtain new examples large
untagged corpus (e.g. Web). Leacock et al. (1998) present method obtain sensetagged examples using monosemous relatives WordNet. approach based
early work (cf. Section 3). algorithm, Leacock et al. (1998) retrieve number
examples per sense, give preference monosemous relatives consist
multiword containing target word. experiment evaluated 14 nouns
coarse sense-granularity senses. results showed monosemous
corpus provided precision close hand-tagged data.
Another automatic acquisition approach (Mihalcea & Moldovan, 1999) used information
WordNet (e.g. monosemous synonyms glosses) construct queries, later
fed Altavista2 search engine. Four procedures used sequentially, decreasing
order precision, increasing levels coverage. Results evaluated hand,
showing 91% examples correctly retrieved among set 1,080 instances
120 word senses. However, corpus resulting experiment used
2. http://www.altavista.com

83

fiMartinez, Lopez de Lacalle & Agirre

train real WSD system. Agirre Martinez (2000), early precursor work
presented here, tried apply technique train WSD system unsatisfactory
results. authors concluded examples correct,
somehow mislead ML classifier, providing biased features.
related work, Mihalcea (2002) generated sense tagged corpus (GenCor) using
set seeds consisting sense-tagged examples four sources: (i) Semcor, (ii) WordNet,
(iii) examples created using method above, (iv) hand-tagged examples
sources (e.g. Senseval-2 corpus). means iterative process, system obtained
new seeds retrieved examples. total, corpus 160,000 examples
gathered. However, evaluation carried lexical-sample task, showing
method useful subset Senseval-2 testing words (results 5 words
provided), without analysing sources performance gain. Even
work presented uses techniques, work seen extension
limited study, sense evaluate all-words tasks.
previous works focused use two different kinds techniques
automatic acquisition examples, namely, use monosemous relatives alone (Leacock
et al., 1998) use combination monosemous relatives glosses (Mihalcea
& Moldovan, 1999; Mihalcea, 2002). cases examples directly used feed
supervised ML WSD system, limited evaluation indication
methods scale-up. Unfortunately, direct comparison alternative methods
parameters automatically acquire examples WSD exists, see preference
use Web, existing corpora would contain occurrences monosemous
terms gloss fragments.
closely related area automatic acquisition examples WSD
enriching knowledge bases topic signatures. instance, Agirre et al. (2000)
Agirre, Ansa, Martinez, Hovy (2001) used combined monosemous-relatives plus
glosses strategy query Altavista, retrieve original documents build lists related
words word sense (so called topic signatures). topic signatures difficult
evaluate hand, applied context vectors WSD straightforward way.
Note authors train ML algorithm, rather combined examples
one vector per sense. showed using Web compared favorably using
fixed corpus, computationally costly: system first needs query search
engine retrieve original document order get example sense.
alternative, Agirre Lopez de Lacalle (2004) showed possible scale
gather examples nouns WordNet query limited using monosemous
relatives snippets returned Google used instead whole document.
point, Cuadros et al. (2006) set systematic framework evaluation
different parameters affect construction topic signatures, including
methods automatically acquire examples. study explores wide range querying
strategies (monosemous synonyms, monosemous relatives different distances, glosses,
combined using either operators) particular corpus (the British National Corpus) Web. best results obtained using Infomap3 British
National Corpus monosemous relatives method Web (Agirre & Lopez de
3. http://infomap-nlp.sourceforge.net

84

fiOn Use Automatically Acquired Examples All-Nouns WSD

Lacalle, 2004). Contrary method, Infomap returns lists related words,
thus used retrieve training examples. results confirmed
experiments reported Cuadros Rigau (2006).
all, literature shows using monosemous relatives snippets
Web (Agirre & Lopez de Lacalle, 2004) provides method automatically acquire examples
scales nouns WordNet, provides topic signatures better quality
alternative methods. explain examples acquired.

3. Building Sense-Tagged Corpus Nouns Automatically
order build corpus (which refer SenseCorpus) acquired 1,000
Google snippets monosemous noun WordNet 1.6 (including multiwords, e.g.
church building). Then, word sense ambiguous noun, gathered examples monosemous relatives (e.g. sense #2 church, gather examples
relative church building). way collect examples simply querying corpus
word string words (e.g. church building). method inspired
work Leacock et al. (1998) and, already mentioned Section 2, shown
efficient effective experiments topic signature acquisition.
basic assumption method given word sense target word,
monosemous synonym word sense, examples synonym
similar target word sense, could therefore used train
classifier target word sense. idea , lesser extent, applied
monosemous relatives, direct hyponyms, direct hypernyms, siblings, indirect
hyponyms, etc. expected reliability decreases distance hierarchy
monosemous relative target word sense.
actual method build SenseCorpus following. collected examples
Web monosemous relatives. relatives associated number
(type), correlates roughly distance target word, indicates
relevance: higher type, less reliable relative. Synonyms type 0, direct
hyponyms get type 1, distant hyponyms receive type number equal distance
target sense. Direct hypernyms get type 2, general
target sense, thus introduce noise direct hyponyms. also decided
include less reliable siblings, type 3. sophisticated schemes could tried,
using WordNet similarity weight distance target relative
word. However, chose approach capture notion distance simplicity,
avoid testing many parameters. sample monosemous relatives different
senses church, together sense inventory WordNet 1.7 shown Figure 1.
following subsections describe step step method construct
corpus. First explain acquisition highest possible amount examples per
sense, explain different ways limit number examples per sense
better performance.
3.1 Collecting Examples
method collect examples previously published (Agirre & Lopez de
Lacalle, 2004), comprises following steps:
85

fiMartinez, Lopez de Lacalle & Agirre

Sense inventory (church)
Sense 1: group Christians; group professing Christian doctrine belief.
Sense 2: place public (especially Christian) worship.
Sense 3: service conducted church.
Monosemous relatives different senses (of church)
Synonyms (Type 0): church building (sense 2), church service (sense 3) ...
Direct hyponyms (Type 1): Protestant Church (sense 1), Coptic Church (sense 1) ...
Direct hypernyms (Type 2): house prayer (sense 2), religious service (sense 3) ...
Distant hyponyms (Type 2,3,4...):
(sense 1)...

Greek Church (sense 1), Western Church

Siblings (Type 3): Hebraism (sense 2), synagogue (sense 2) ...

Figure 1: Sense inventory sample monosemous relatives WordNet 1.7 church.

1: query Google4 monosemous relatives sense, extract
snippets returned search engine. snippets used (up 1,000),
dropped next step.
2: try detect full meaningful sentences snippets contain target
word. first detect sentence boundaries snippet extract sentence
encloses target word. sentences filtered out, according following
criteria: length shorter 6 words, non-alphanumeric characters words
divided two, words uppercase lowercase.
3: automatically acquired examples contain monosemous relative target
word. order use examples train classifiers, monosemous relative (which
multiword term) substituted target word. case monosemous
relative multiword contains target word (e.g. Protestant Church church)
choose substitute, Protestant, instance, useful feature
first sense church. tried alternatives, Section 5 show
obtain slightly better results substitution applied multiwords.
4: given word sense, collect desired number examples (see following
section) order type: first retrieve examples type 0, type 1, etc.
type 3 necessary examples obtained. collect examples type 4
upwards. make distinctions relatives type. Contrary
Leacock et al. (1998) give preference multiword relatives containing
target word.
all, acquired around 150 million examples nouns WordNet using
technique, publicly available5 .
4. use off-line XML interface kindly provided Google research.
5. http://ixa.si.ehu.es/Ixa/resources/sensecorpus.

86

fiOn Use Automatically Acquired Examples All-Nouns WSD

3.2 Number Examples per Sense (Prior)
Previous work (Agirre & Martinez, 2000) reported distribution number
examples per word sense (prior short) strong influence quality
results. is, results degrade significantly whenever training testing samples
different distributions senses. also shown type-based approach
predicts majority sense word domain provide good performance
(McCarthy, Koeling, Weeds, & Carroll, 2004).
extracting examples automatically, decide many examples
use sense. order test impact prior, different settings
tried:
prior: take equal amount examples sense.
Web prior: take examples gathered Web.
Automatic ranking: number examples given ranking obtained following
method McCarthy et al. (2004).
Sense-tagged prior: take number examples proportional relative frequency word senses hand-tagged corpus.
first method assumes uniform priors. second assumes number
monosemous relatives occurrences correlated sense importance, is,
frequent senses would occurrences monosemous relatives. fourth
method uses information hand-tagged corpus, typically Semcor. Note
last kind prior requires hand-tagged data, rest (including third method
below) completely unsupervised.
third method sophisticated deserves clarification. McCarthy et al. (2004) present method acquire sense priors automatically domain
corpus. two-step process. first step corpus-based method, given
target word builds list contextually similar words (Lin, 1998) weights.
case, co-occurrence data gathered British National Corpus. instance,
given target word like authority, list topmost contextually similar words include government, police, official agency 6 . second step ranks senses
target word, depending scores WordNet-based similarity metric (Patwardhan
& Pedersen, 2003) relative list contextually similar words. Following
example, pairwise WordNet similarity authority government greater
sense 5 authority, evidence sense prominence corpus.
pairwise similarity scores added, yielding ranking 7 senses authority.
Table 2 shows column named Auto.MR normalized scores assigned
senses authority according technique.
Table 1 shows number examples per type (0,1,...) acquired church
following Semcor prior. last column gives number examples Semcor. Note
number examples sometimes smaller 1,000 (maximum number snippets
returned Google one query). due rare monosemous relatives,
6. Actual list words taken demo http://www.cs.ualberta.ca/~lindek/demos/depsim.htm.

87

fiMartinez, Lopez de Lacalle & Agirre

Sense
church#1
church#2
church#3
Overall

0
0
306
147
453

1
476
100
0
576

2
524
561
20
1,105

3
0
0
0
0

Total
1,000
967
167
2,134

Semcor
60
58
10
128

Table 1: Examples per type (0,1,2,3) acquired Web three senses
church following Semcor prior, total number examples Semcor.

Sense
authority#1
authority#2
authority#3
authority#4
authority#5
authority#6
authority#7
Overall

Semcor
#ex
18
5
3
2
1
1
0
30

%
60.0
16.7
10.0
6.7
3.3
3.3
0.0
100.0

Web PR
#ex
%
338
0.5
44932
66.4
10798
16.0
886
1.3
6526
9.6
72
0.1
4106
6.1
67657
100.0

Auto.
#ex
138
75
93
67
205
71
67
716

SenseCorpus
MR
Semcor PR
%
#ex
%
19.3
338
33.7
10.5
277
27.6
13.0
166
16.6
9.4
111
11.1
28.6
55
5.5
9.9
55
5.5
9.4
1
0.1
100.0
1003
100.0

Semcor
#ex
324
90
54
36
18
18
1
541

MR
%
59.9
16.6
10.0
6.7
3.3
3.3
0.2
100.0

Senseval
test
#ex
%
37
37.4
17
17.2
1
1.0
0
0.0
34
34.3
10
10.1
0
0.0
99
100.0

Table 2: Distribution examples senses authority different corpora. PR
(proportional) MR (minimum ratio) columns correspond different ways
apply Semcor prior.

usually caused sentence extraction filtering process, discards around 50%
snippets.
way apply prior straightforward. illustration, focus
Semcor prior. first approach Semcor prior, assigned 1,000 examples
major sense Semcor, gave senses proportion examples. call
method proportional (PR). cases number examples extracted
less expected distribution senses Semcor. result, actual number
examples available would follow desired distribution.
alternative, computed, word, minimum ratio (MR) examples
available given target distribution given number examples extracted
Web. observed last approach would reflect better original prior,
cost less examples.
Table 2 presents different distributions examples authority. see
Senseval-testing Semcor distributions, together total number examples
Web (Web PR); Semcor proportional distribution (Semcor PR) minimum
ratio (Semcor MR); automatic distribution minimum ratio (Auto MR).
Getting maximum one thousand examples per monosemous relative allows get
44,932 examples second sense (Web PR column), 72 sixth sense.
88

fiOn Use Automatically Acquired Examples All-Nouns WSD

Semcor
Word
art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
Average
Total

Web
prior
15,387
67,657
50,925
17,244
24,625
31,582
47,619
8,704
21,977
84,448
2,650
4,210
11,049
6,237
9,601
24,137
699,086

Automatic
prior
2,610
716
5,329
4,745
2,111
10,015
791
6,355
5,095
3,660
511
843
1,196
5,477
945
3,455
100,215

Semcor
prior
10,656
541
16,627
2,555
8,512
3,235
3,504
5,376
3,588
9,690
1,510
1,367
8,578
3,438
1,160
4,719
136,874

Semcor
Word
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew

Web
prior
20,874
6,682
16,714
12,161
100,109
648
608
32,553
34,968
33,055
10,315
5,361
10,356
10,767

Automatic
prior
277
2,730
1,846
884
6,385
464
608
9,813
8,005
2,877
2,176
2,657
3,081
8,013

Semcor
prior
2,209
1,531
1,248
2,959
7,855
287
594
24,746
4,264
2,152
2,059
2,458
2,175
2,000

Table 3: Number examples following different sense distributions Senseval-2
nouns. Minimum ratio applied Semcor automatic priors.

sixth sense single monosemous relative, rare word hits
Google, second sense many frequent monosemous relatives.
Regarding use minimum ratio, table illustrates MR allows better
approximate distribution senses Semcor: first sense7 60% Semcor,
gets 33.7% SenseCorpus proportional Semcor prior
338 examples SenseCorpus first sense. contrast SenseCorpus
minimum ratio using Semcor assign 59.9% examples first sense.
better approximation comes cost getting 541 examples authority, contrast
1,003 PR. Note authority occurs 30 times Semcor.
table also shows word distributions senses Semcor
Senseval-test important differences (sense 5 gets 3.3% 34.3% respectively), although frequent sense same. Web automatic distributions,
salient sense different Semcor, Web prior (Web PR column)
assigning 0.5% first sense. Note automatic method able detect
sense 5 salient test corpus, Semcor ranks 5th. general, distribution
discrepancies similar table observed words test
set.
conclude section, Table 3 shows number examples acquired automatically
word Senseval-2 lexical-sample following three approaches: Web prior,
Semcor prior minimum ratio, Automatic prior minimum ratio.
see retrieving examples (Web prior) get 24,137 examples average per
word; respectively 4,700 3,400 apply Semcor prior Automatic prior.
7. senses WordNet numbered according frequency Semcor, first sense
WordNet paramount frequent sense Semcor.

89

fiMartinez, Lopez de Lacalle & Agirre

3.3 Decision Lists
supervised learning method used measure quality corpus Decision Lists
(DL). simple method performs reasonably well comparison supervised
methods Senseval words (as illustrate Table 6.4), preliminary experiments showed perform better automatically retrieved examples
sophisticated methods like Support Vector Machines Vector Space Model. well
known learning methods perform differently according several conditions, showed
instance Yarowsky Florian (2003), analyzed depth performance
various learning methods (including DL) WSD tasks.
think main reason DL perform better preliminary experiments
SenseCorpus noisy corpus conflicting features. Decision Lists use
single powerful feature test context make predictions, contrast
ML techniques, could make perform better corpus. Specially
all-words task, hand-tagged examples per word cases, even
sophisticate ML algorithms cannot deal problem themselves.
best systems Senseval-3 lexical-sample rely complex kernel-based methods,
all-words task top systems find external ways deal sparseness
data apply well-known methods, memory based learning decision
trees (Mihalcea & Edmonds, 2004).
DL algorithm described Yarowsky (1994). method, sense sk
highest weighted feature fi selected, according log-likelihood (see Formula 1).
implementation, applied simple smoothing method: cases
denominator zero, use 0.1 denominator. roughly equivalent assigning
0.1 probability mass rest senses, shown effective enough
compared complex methods (Yarowsky, 1994; Agirre & Martinez, 2004a).
P r(sk |fi )
)
j6=k P r(sj |fi )

weight(sk , fi ) = log( P

(1)

3.4 Feature Types
feature types extracted context grouped three main sets:
Local collocations: bigrams trigrams formed words around target.
features constituted lemmas, word-forms, PoS tags8 . local features
formed previous/posterior lemma/word-form context.
Syntactic dependencies: syntactic dependencies extracted using heuristic patterns,
regular expressions defined PoS tags around target9 . following relations used: object, subject, noun-modifier, preposition, sibling.
Topical features: extract lemmas content words whole sentence
4-word window around target. also obtain salient bigrams context,
methods software described Pedersen (2001).
8. PoS tagging performed fnTBL toolkit (Ngai & Florian, 2001).
9. software kindly provided David Yarowskys group, Johns Hopkins University.

90

fiOn Use Automatically Acquired Examples All-Nouns WSD

complete feature set applied main experiments all-words Senseval3 corpus. However, initial experiments lexical-sample task local features
topical features (without salient bigrams) applied.

4. Experimental Setting
already noted introduction lexical-sample evaluations defined Senseval
realistic: relatively large amounts training examples available, drawn
corpus test examples, train test examples tagged
team. Besides, developing system handful words necessarily
show scalable. contrast, all-words evaluations provide training data.
Supervised WSD systems typically use Semcor (Miller et al., 1993) training.
corpus offers tagged examples open-class words occurring 350.000 word subset
balanced Brown corpus, tagged WordNet 1.6 senses. contrast lexicalsample, polysemous words like authority get handful examples (30
case, cf. Table 2). Note test examples (from Senseval) Semcor come
different corpora thus might related different domains, topics genres.
added difficulty posed fact tagged different teams
annotators distinct institutions.
mind, designed two sets experiments: first set performed
sample nouns (lexical-sample), used develop fine-tune method
basic aspects like effect kinds features importance prior.
use training examples, except measure impact priors. provide
comparison state-of-the-art systems.
second set experiment used show method scalable, useful
noun, performs state-of-the art WSD realistic setting. thus
selected apply WSD nouns running text (all-nouns). setting apply
best configurations obtained first set experiments, explore use
SenseCorpus alone, combined priors Semcor, also training data
Semcor. provide comparison results state-of-the-art systems.
lexical-sample evaluation, test part Senseval-2 English lexical-sample
task chosen, consisted instances 29 nouns, tagged WordNet 1.7 senses.
advantage corpus could focus word-set enough examples
testing. Besides, different corpus, therefore evaluation realistic
made using cross-validation Semcor. order factor pre-processing
focus WSD, test examples whose senses multiwords phrasal verbs
removed. Note problematic since efficiently detected
methods preprocess.
important note training part Senseval-2 lexical-sample used
construction systems, goal test performance could achieve
minimal resources (i.e. available word). relied Senseval-2
training prior preliminary experiments local/topical features, upperbound
compare performance types priors.
all-words evaluation relied Senseval-3 all-words corpus (Snyder &
Palmer, 2004). test data task consisted 5,000 words text. data
91

fiMartinez, Lopez de Lacalle & Agirre

extracted two Wall Street Journal articles one excerpt Brown Corpus.
texts represent three different domains: editorial, news story, fiction. Overall, 2,212
words tagged WordNet 1.7.1. senses (2,081 include multiwords).
these, 695 occurrences correspond polysemous nouns part multiwords,
comprise testing set.
rest Senseval participants, added difficulty WordNet versions
coincide. therefore used one freely available mappings WordNet
versions (Daude, Padro, & Rigau, 2000) convert training material Semcor
(tagged WordNet 1.6 senses) WordNet 1.7 WordNet 1.7.1 versions (depending
target corpus). preferred use mapping rather relying
available mappings converted Semcors. knowledge, comparative evaluation
among mappings performed, Daude et al. show mapping obtained
high scores extensive manual evaluation. Note versions Semcor
available Web (other original one, tagged WordNet 1.6) also
obtained using automatic mapping.
lexical-sample all-nouns settings, provide set baselines,
based frequent heuristic. heuristic known hard beat WSD,
specially unsupervised systems access priors, even
supervised systems all-nouns setting.

5. Lexical-Sample Evaluation
performed four sets experiments order study different factors, compare
performance state-of-the-art unsupervised systems Senseval-2 lexical-sample
task. First analyzed results systems using different sets local
topical features, well substituting multiwords. next experiments
devoted measure effect prior performance. that, compared
approach unsupervised systems participated Senseval-2. mentioned
introduction, results obtained lexical-sample evaluations realistic,
cannot expect hand-tagged data words target corpus.
reason report results supervised systems (which use training data).
next section all-nouns evaluation, realistic, compare supervised
systems
5.1 Local vs. Topical Features, Substitution
Previous work automatic acquisition examples (Leacock et al., 1998) reported
lower performance using local collocations formed PoS tags closed-class words.
contrast, Kohomban Lee (2005), related approach, used local features
WSD discriminated better senses. Given fact SenseCorpus
also constructed automatically, contradictory results previous
works, performed initial experiment comparing results using local features, topical
features, combination both. case used SenseCorpus Senseval training
prior, distributed according MR approach, always substituting target word.
results (per word overall) given Table 4.
92

fiOn Use Automatically Acquired Examples All-Nouns WSD

Local Feats.
Word
art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew
Overall

Coverage
94.4
93.4
98.3
100.0
100.0
73.5
100.0
100.0
88.7
98.6
100.0
100.0
98.2
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
98.3
79.5
93.0
100.0
100.0
100.0
96.7

Precision
57.4
51.2
53.0
81.2
88.7
54.0
56.5
67.7
51.1
60.2
87.5
89.3
29.1
82.5
55.1
19.0
73.4
96.3
80.4
43.2
36.8
80.6
44.4
44.7
37.1
62.5
74.2
53.9
81.5
58.5

Recall
54.2
47.8
52.1
81.2
88.7
39.7
56.5
67.7
45.3
59.4
87.5
89.3
28.6
82.5
55.1
19.0
73.4
96.3
80.4
43.2
36.8
80.6
44.4
43.9
29.5
58.1
74.2
53.9
81.5
56.5

Topical
Feats.
Recall
45.6
43.2
55.9
87.5
88.7
53.7
55.6
51.6
54.2
54.7
87.5
89.3
21.4
82.5
60.2
38.0
75.0
96.3
73.9
44.2
38.6
80.6
39.3
40.5
37.5
37.2
72.6
46.1
81.5
56.0

Combined
Subst.
Recall
47.0
46.2
57.2
85.0
88.7
55.9
56.5
54.8
56.1
56.8
87.5
89.3
21.4
82.5
60.2
39.0
75.0
96.3
73.9
43.8
39.5
80.6
40.7
40.5
37.1
38.4
74.2
48.7
81.5
57.0

Combined
Subst.
Recall
44.9
46.2
57.2
85.0
88.7
57.4
58.9
51.6
58.0
60.4
87.5
89.3
21.4
82.5
60.2
38.0
75.0
96.3
73.9
42.9
39.5
80.6
40.7
40.5
37.1
48.8
74.2
48.7
81.5
57.5

Table 4: Results per feature type (local, topical, combination), using SenseCorpus
Senseval-2 training prior (MR). Coverage precision given local
features (topical combination full coverage). Combination shown
substitution substitution options. best recall per word given
bold.

93

fiMartinez, Lopez de Lacalle & Agirre

experiment, observed local collocations achieved best precision overall, combination features obtained best recall. Local features achieve
58.5% precision 96.7% coverage overall10 , topical combined features
full-coverage. table shows clear differences results per word, fact
also known algorithms using real training data (Yarowsky & Florian, 2003).
variability another important factor focus all-words settings, large numbers
different words involved.
also show results substituting monosemous relative target
word monosemous relative multiword. see results mixed,
slight overall improvement choose substitute cases.
following experiments, chose work combination features
substitution, achieved best overall recall.
5.2 Impact Prior
order evaluate acquired corpus, first task analyze impact
prior. mentioned Section 3.2, training Decision Lists examples
SenseCorpus, need decide amount examples sense (what seen
estimation prior probabilities senses).
Table 5 shows recall11 attained DL four proposed methods
estimate priors target word, plus use training part Senseval-2 lexical
sample estimate prior. Note last estimation method realistic, one
cannot expect hand-tagged data words given target corpus,
thus taken upperbound. fact presented section completeness,
used comparison systems.
results show constant improvement less informative priors
informed ones. Among three unsupervised prior estimation methods, best results
obtained automatic ranking, worst uniform distribution (no prior
column), distribution examples returned SenseCorpus (Web prior)
middle. Estimating priors hand-tagged data improves results considerably,
even target corpus estimation corpus different (Semcor), best
results overall obtained priors estimated training part Senseval2 lexical-sample dataset. results word word show word behaves differently,
well-known behavior WSD. Note priors except informed
one number words performances 10%, might indicate DL trained
SenseCorpus sensitive badly estimated priors.
Table 6 shows overall results Table 5, together obtained using
prior (prior only). results show improvement attained training
SenseCorpus prominent unsupervised priors (from 6.5 19.7 percentage
points), lower improvements (around 2.0 percentage points) priors estimated
hand-tagged corpora. results show clearly acquired corpus use10. Note due sparse data problem, test examples might feature common
training data. cases DL algorithm return result, thus coverage
lower 100%
11. results following tables given recall, coverage always 100% precision
equals recall case.

94

fiOn Use Automatically Acquired Examples All-Nouns WSD

Unsupervised
Word
art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew
Overall


prior
34.0
20.9
24.7
36.7
61.3
42.2
40.3
43.8
44.3
15.3
52.1
92.9
19.6
58.8
27.2
11.3
57.8
70.4
24.3
51.7
39.5
80.6
21.9
36.8
26.3
44.8
74.2
38.6
70.4
38.0

Web
prior
61.1
22.0
52.1
18.8
62.9
28.7
1.6
62.1
52.8
2.2
16.7
89.3
26.8
73.8
51.0
8.0
37.5
7.4
79.3
50.8
39.5
80.6
44.4
47.4
9.1
18.6
66.1
52.6
85.2
39.8

Autom.
ranking
45.6
40.0
26.4
57.5
69.4
30.9
34.7
49.7
49.1
12.5
87.5
80.4
22.0
75.0
42.5
28.2
60.4
72.2
23.9
52.3
46.5
80.6
34.1
47.4
31.4
41.9
85.5
27.6
77.8
43.2

Minimally-Supervised
Semcor
prior
55.6
41.8
51.6
5.0
88.7
16.2
54.0
48.4
41.5
48.0
52.1
92.9
26.8
82.5
60.2
16.0
75.0
96.3
80.4
54.2
54.4
80.6
46.7
34.2
27.3
47.7
67.7
2.6
66.7
49.8

Senseval-2
prior
44.9
46.2
57.2
85.0
88.7
57.4
58.9
51.6
58.0
60.4
87.5
89.3
21.4
82.5
60.2
38.0
75.0
96.3
73.9
42.9
39.5
80.6
40.7
40.5
37.1
48.8
74.2
48.7
81.5
57.5

Table 5: Performance (recall) SenseCorpus 29 nouns Senseval-2 lexical-sample,
using different priors train DL. Best results word bold.

ful information word senses, estimation prior extremely
important.

Prior
prior
Web prior
autom. ranking
Semcor prior
Senseval2 prior

Type
unsupervised
minimallysupervised

prior
18.3
33.3
36.1
47.8
55.6

SenseCorpus
38.0
39.8
43.2
49.8
57.5

Diff.
+19.7
+6.5
+7.1
+2.0
+1.9

Table 6: Performance (recall) nouns Senseval-2 lexical-sample. row, results
given prior own, SenseCorpus using prior, difference
both.

95

fiMartinez, Lopez de Lacalle & Agirre

Method
SenseCorpus (Semcor prior)
UNED
SenseCorpus (Autom. prior)
Kenneth Litkowski-clr-ls
Haynes-IIT2
Haynes-IIT1

Type
minimallysupervised
unsupervised

Recall
49.8
45.1
43.3
35.8
27.9
26.4

Table 7: Results nouns best minimally supervised fully unsupervised systems (in bold) compared unsupervised systems took part Senseval-2
lexical-sample.

5.3 Comparison Systems
point, important compare performance DL-based approach
systems state art. section compare best unsupervised
system (the one using Automatic ranking) minimally unsupervised system (using
Semcor prior) systems participating Senseval-2 deemed unsupervised. order results systems, used resources available
Senseval-2 competition, answers participating systems different
tasks available12 . made possible compare results test data,
set nouns occurrences.
5 systems presented Senseval-2 lexical-sample task unsupervised,
WASP-Bench system relied lexicographers hand-code information semi-automatically
(Tugwell & Kilgarriff, 2001). system use training data, uses
manually coded knowledge think falls supervised category.
results 4 systems shown Table 7. classified
UNED system (Fernandez-Amoros, Gonzalo, & Verdejo, 2001) minimally supervised.
use hand-tagged examples training, heuristics applied
system rely prior information available Semcor. distribution senses
used discard low-frequency senses, also choose first sense back-off
strategy. conditions, minimally supervised system attains 49.8% recall,
nearly 5 points better.
rest systems fully unsupervised, perform significantly worse
unsupervised system.

6. Nouns Evaluation
explained introduction, main goal research develop WSD
system able tag nouns context, sample them. previous
section explored different settings system, adjusting according results
handful words lexical-sample task.
12. http://www.senseval.org

96

fiOn Use Automatically Acquired Examples All-Nouns WSD

section test SenseCorpus 695 occurrences polysemous nouns
present Senseval-3 all-words task, compare results performance
systems participated competition. also present analysis results
according frequency target nouns.
developed three different systems, based SenseCorpus, different requirements external information. less informed system unsupervised
system (called SenseCorpus-U), use hand-coded corpus prior extracted therein. system relies examples SenseCorpus following Automatic Ranking (McCarthy et al., 2004) train DL (see Section 3.2). following
system minimally-supervised (SenseCorpus-MS), sense uses priors
obtained Semcor define distribution examples SenseCorpus
fed DL. Lastly, informed system trains DL hand-tagged
examples Semcor SenseCorpus (following Semcor prior), known
SenseCorpus-S. three systems follow widely used distinction among unsupervised,
minimally-supervised supervised systems, compare similar
systems participated Senseval-3.
systems respond realistic scenarios. unsupervised system called
case languages all-words hand-tagged corpus exists, cases priors coming Semcor appropriate, domain-specific corpora. minimally
supervised system useful hand-tagged corpora,
indication distribution senses. Lastly, supervised system (SenseCorpus-S)
shows performance SenseCorpus currently available conditions English,
is, all-words corpus limited size available.
order measure real contribution SenseCorpus, compare three systems
following baselines: SenseCorpus-U vs. first sense according
automatically obtained ranking, SenseCorpus-MS vs. frequent sense Semcor,
SenseCorpus-S vs. Decision Lists trained Semcor. order judge
significance improvements, applied one-tail paired t-test.
6.1 Comparison Unsupervised Systems Senseval-3
systems participated all-words task three rely
hand-tagged corpora (not even estimating prior information). compare performance systems unsupervised system SenseCorpus-U Table 8. order
make fair comparison respect participants, removed answers
correctly guess lemma test instance (discarding errors pre-processing
Senseval-3 XML data).
see one participating systems automatic ranking McCarthy
et al. (2004) used baseline. Although able improve system,
results best unsupervised system (IRST-DDD-LSI) (Strapparava, Gliozzo, &
Giuliano, 2004). Surprisingly, unsupervised method able obtain better performance
dataset version relies Semcor frequencies (IRST-DDD-0, see next
subsection), discrepancy explained authors. reasons
remarkable results IRST-DDD-LSI clear, subsequent publications
authors shed light it.
97

fiMartinez, Lopez de Lacalle & Agirre

Code
IRST-DDD-LSI
SenseCorpus-U
AutoPS (Baseline)
DLSI-UA

Method
LSI
Decision Lists
Automatic Rank.
WordNet Domains

Attempt.
570
680
675
648

Prec.
64.6
45.5
44.6
27.8

Rec.
52.9
44.4
43.3
25.9

F
58.2
45.0
43.9
26.8

p-value
0.001

0.001
0.000

Table 8: Performance unsupervised systems participating Senseval-3 all-words
695 polysemous nouns, accompanied p-values one tailed paired t-test
respect unsupervised system (in bold).

Code
SenseCorpus-MS
MFS (Baseline)
IRST-DDD-00
Clr04-aw
KUNLP
IRST-DDD-09

Method
DL
MFS
Domain-driven
Dictionary clues
Similar relative WordNet
Domain-driven

Attempt.
695
695
669
576
628
346

Prec.
63.9
62.7
55.6
58.7
54.2
69.7

Rec.
63.9
62.7
53.5
48.6
49.0
34.7

F
63.9
62.7
54.5
53.2
51.5
46.3

p-value

0.044
0.000
0.000
0.000
0.000

Table 9: Performance minimally supervised systems participating Senseval-3 allwords 695 polysemous nouns, accompanied p-values one tailed
paired t-test respect SenseCorpus-MS (in bold).

improvement baseline lower lexical-sample case,
significant 0.99 level (significance 1p-value). order explore reasons
this, performed experiments separating words different sets according
frequency Semcor, reported Section 6.4.
6.2 Comparison Minimally Supervised Systems Senseval-3
four systems Senseval used Semcor estimate sense distribution,
without using examples word training. show performance
systems, together frequent sense baseline Table 9.
results show SenseCorpus examples able obtain best performance
kind systems, well rest. improvement Semcor MFS baseline
significant 0.96 level.
6.3 Comparison Supervised Systems Senseval-3
systems participated all-words task supervised systems
relied mainly Semcor. Table 10 present results top 10 competing systems
system, trained SenseCorpus Semcor. also include DL system
trained Semcor, baseline.
results show using SenseCorpus able obtain significant improvement
2.9% points F-score baseline. score places system second, close
98

fiOn Use Automatically Acquired Examples All-Nouns WSD

Code
SenseLearner
SenseCorpus-S
LCCaw
kuaw.ans
R2D2English
GAMBL-AW
upv-eaw.upv-eaw2
Meaning
upv-eaw.upv-eaw
Prob5
Semcor baseline
UJAEN2

Method
Syntactic Patterns
DL

Ensemble
Optim.,TiMBL
Ensemble

DL

Attempt.
695
695
695
695
695
695
695
695
695
691
695
695

Prec.
65.9
65.3
65.3
64.8
64.5
63.3
63.3
63.2
62.9
62.8
62.4
62.4

Rec.
65.9
65.3
65.3
64.7
64.5
63.3
63.3
63.2
62.9
62.4
62.4
62.4

F
65.9
65.3
65.3
64.7
64.5
63.3
63.3
63.2
62.9
62.6
62.4
62.4

p-value
0.313

0.166
0.115
0.054
0.013
0.014
0.009
0.007
0.007
0.006
0.002

Table 10: Performance top 10 supervised systems participating Senseval-3 allwords 695 polysemous nouns, accompanied p-values one tailed
paired t-test respect SenseCorpus-S (in bold).

best system all-nouns. statistical significance tests score 90%
top 4 systems, 95% rest systems. means system performs
similar top three systems, significantly better rest.
6.4 Analysis Performance Word Frequency
previous sections observed different words achieve different rates accuracy.
instance, lexical-sample experiments showed precision unsupervised
system ranged 12.5% 87.5% (cf. Table 5). Clearly, words
whose performance low using SenseCorpus. section, group
nouns Senseval-3 all-nouns task according frequency see whether
correlation frequency words performance system.
goal identify sets words disambiguated higher accuracy
method. process would allow us previously detect type words system
applied to, thus providing better tool work combination WSD systems
exploit properties language.
study, created separate word sets according frequency occurrence
Semcor. Table 11 shows different word-sets, frequency ranges, number
nouns range, average polysemy. see frequent words
tend also polysemous. case supervised systems, polysemy
number training examples tend compensate other, yielding good results
kinds words. is, polysemous words difficult disambiguate, also
examples train Semcor (Agirre & Martinez, 2000).
Table 12 shows results different frequency ranges top unsupervised systems Senseval-3, together method. see systems
performance low high-frequency range. best performing system (IRSTDDD-LSI) profits use threshold leaves many instances unanswered.
Regarding improvement SenseCorpus-U Automatic Ranking baseline (Au99

fiMartinez, Lopez de Lacalle & Agirre

Range
010
1120
2140
4160
6180
81100
101
Overall

#Nouns
207
101
89
88
54
31
125
695

Avg. Polysemy
3.6
5.1
6.1
6.6
6.9
9.3
9.6
5.4

Table 11: Number noun occurrences frequency ranges (in Semcor),
average polysemy.

010
1120
2140
4160
6180
81100
101
overall

DLSI-UA
Att.
F-sc.
188
35.98
96
34.50
75
15.82
82
19.97
54
22.20
31
9.70
122
24.30
648
26.83

IRST-DDD-LSI
Att.
F-sc.
195
67.13
91
69.77
81
57.65
75
55.21
50
57.69
19
36.02
59
35.85
570
58.22

SenseCorpus-U
Att.
F-sc.
198
62.77
98
58.90
89
25.50
85
42.84
54
35.80
31
23.70
125
29.60
680
45.00

AutoPS
Att.
F-sc
198
62.68
98
49.25
86
26.24
85
42.75
54
31.50
31
29.00
123
31.44
675
43.95

Table 12: Results unsupervised systems Senseval-3 words, evaluated
nouns Semcor frequency range. Att. stands number words
attempted range. Best F-score per system given bold.

toPS), best results obtained low-frequency range (0-20), baseline
scores 50-60% F-score range. results SenseCorpus lower baseline
words frequency higher 80. suggests system reliable
low-frequency words, simple threshold takes account frequency words
would indicative performance expect. behavior also apparent
unsupervised systems, shows weak spot kind
systems. think future research focus high frequency words.

7. Discussion
work implemented evaluated all-words WSD system nouns
able reach state-of-the-art performance three supervised, unsupervised
semi-supervised settings. produced different systems combining SenseCorpus
different priors actual examples Semcor. supervised system, trained
hand-tagged (Semcor) automatically obtained corpora, reaches F-score
65.3%, would rank second Senseval-3 all-nouns test data. semi-supervised
system, using priors Semcor manually-tagged examples, would rank first
100

fiOn Use Automatically Acquired Examples All-Nouns WSD

class, unsupervised system second. cases, SenseCorpus improves
baselines.
results remarkable. compare system came first
unsupervised supervised settings, see uses completely
different strategy. contrary, system, using primarily automatically acquired
examples, able perform top ranks three settings.
case, deep gap exists among following three kinds systems: (i) Supervised
systems specific training (e.g. Senseval lexical-sample systems), (ii) Supervised systems
all-words training (e.g. trained using Semcor), (iii) Unsupervised systems.
algorithm implemented all-words supervised system, unsupervised system. Although implementations obtain state-of-the-art performance
categories, different issues could addressed order close gaps,
make all-words unsupervised performance closer supervised systems.
identified three main sources error: low quality relatives applied
words, different distributions senses training testing data, low
performance high-frequency (and highly polysemous) words. examine
turn.
algorithm suffers noise introduced relatives far target
word, share local context it. Better filtering would required
alleviate problem, one way could retrieve examples
share part local context target word discard examples. Another
interesting aspect problem would identify type words achieve low
performance SenseCorpus. already observed high-frequency words obtain low
performance, another study performance according type relatives would
useful better application algorithm.
order deal words close WordNet relatives, another source
examples would use distributionally similar words. words would obtained
methods one presented Lin (1998), retrieved examples would
linked target senses using WordNet similarity package (Patwardhan & Pedersen,
2003).
second main problem systems rely automatic acquisition fact
sense distributions training test data different, seriously
affects performance. system relies automatically-obtained sense ranking
alleviate problem. However, words still get many examples senses
relevant domain. preliminary experiments, observed benefit using
heuristics filter senses, using number close relatives WordNet,
promising results.
Finally, third problem observed Section 6.4, fact high-frequency
words profit automatically acquired examples. unsupervised methods,
frequent (and polysemous) words get low performances, threshold-based
systems usually discard answering them. straightforward way improve F-score
system would apply threshold discard words apply another method
back-off strategy them.
all, detecting limitations system give us important clues work
towards accurate unsupervised all-words system. literature shows single
101

fiMartinez, Lopez de Lacalle & Agirre

unsupervised system able perform well words. able identify type
words suited different algorithms heuristics, integration
algorithms one single combined system could way go. instance, could
detect cases relatives target word different apply SenseCorpus
approach, cases automatic ranking enough evidence. also
observed simple heuristics number close relatives WordNet
successfully applied sets words. Meta-learning techniques (Vilalta & Drissi, 2002)
could useful exploit strengths unsupervised systems.

8. Conclusions Future Work
paper presents evidence showing proper use automatically acquired examples
allows state-of-the-art performance WSD nouns. gathered examples
nouns WordNet 1.6 resource called SenseCorpus, amounting 150 million examples,
made resource publicly available community.
used examples train supervised WSD system, variety settings:
own, combined prior information coming different sources, combined
training examples Semcor. Depending knowledge used, able build,
respectively, unsupervised system seen hand-labeled training data,
semisupervised one sees priors generic hand-labeled corpus (Semcor),
fully-supervised system also uses generic hand-labeled corpus (Semcor)
training data.
evaluation lexical-sample all-words settings shown SenseCorpus
improves commonly used baselines combinations, achieves state-of-the-art
performance all-words Senseval-3 evaluation set nouns. Previous work automatic example acquisition evaluated handful words. contrast
shown able scale nouns producing excellent results. way,
learned use prior senses crucial apply acquired examples
effectively.
discussion outlined different ways overcome limitations
system, proposed lines could improve significantly current performance.
Although recent literature shows unsupervised system performs
high precision words, believe different systems complement
other, usually perform well different sets words. meta-learning perspective, could build word-expert system able apply best knowledge
source problem: SenseCorpus, hand-tagged examples, simple heuristics,
unsupervised algorithms incorporated.
future work, aside proposed improvements, think would
interesting apply method testbeds. order applied, monosemous
relative method requires ontology raw corpus. resources found many
specific domains, Biomedicine, fine-grainedness WordNet,
could lead practical applications.
102

fiOn Use Automatically Acquired Examples All-Nouns WSD

Acknowledgments
work partially financed Ministry Education (KNOW project, ICT2007-211423) Basque Government (consolidated research groups grant, IT-397-07).
Oier Lopez de Lacalle supported PhD grant Basque Government. David
Martinez funded Australian Research Council, grant no. DP0663879.

References
Abney, S. (2002). Bootstrapping. Proceedings 40th Annual Meeting Association Computational Linguistics, ACL02, Philadelphia.
Agirre, E., Ansa, O., Martinez, D., & Hovy, E. (2000). Enriching large ontologies using
WWW. Proceedings Ontology Learning Workshop, organized ECAI,
Berlin (Germany).
Agirre, E., Ansa, O., Martinez, D., & Hovy, E. (2001). Enriching WordNet concepts
topic signatures. Procceedings SIGLEX workshop WordNet
Lexical Resources: Applications, Extensions Customizations. conjunction
NAACL.
Agirre, E., & Edmonds, P. (Eds.). (2006). Word Sense Disambiguation: Algorithms
Applications. Springer.
Agirre, E., & Lopez de Lacalle, O. (2004). Publicly available topic signatures WordNet nominal senses. Proceedings 4th International Conference Language
Resources Evaluation (LREC), Lisbon, Portugal.
Agirre, E., & Martinez, D. (2000). Exploring automatic word sense disambiguation Decision Lists Web. Procedings COLING 2000 Workshop Semantic
Annotation Intelligent Content, Luxembourg.
Agirre, E., & Martinez, D. (2004a). Smoothing word sense disambiguation. Proceedings Expaa Natural Language Processing (EsTAL), Alicante, Spain.
Agirre, E., & Martinez, D. (2004b). Unsupervised WSD based automatically retrieved
examples: importance bias. Proceedings Conference Empirical
Methods Natural Language Processing, Barcelona, Spain.
Blum, A., & Mitchell, T. (1998). Combining labeled unlabeled data co-training.
Proceedings 11h Annual Conference Computational Learning Theory, pp.
92100, New York. ACM Press.
Chan, Y., & Ng, H. (2005). Scaling word sense disambiguation via parallel texts.
Proceedings 20th National Conference Artificial Intelligence (AAAI 2005),
Pittsburgh, Pennsylvania, USA.
Collins, M., & Singer, Y. (1999). Unsupervised models named entity classification.
Proceedings Joint SIGDAT Conference Empirical Methods Natural
Language Processing Large Corpora, EMNLP/VLC99, College Park, MD,
USA.
103

fiMartinez, Lopez de Lacalle & Agirre

Cuadros, M., Padro, L., & Rigau, G. (2006). empirical study automatic acquisition
topic signatures. Proceedings Third International WordNet Conference, Jeju
Island (Korea).
Cuadros, M., & Rigau, G. (2006). Quality assessment large scale knowledge resources.
Proceedings 2006 Conference Empirical Methods Natural Language Processing, pp. 534541, Sydney, Australia. Association Computational Linguistics.
Daconta, M., Obrst, L., & Smith, K. (2005). Semantic Web: Guide Future
XML, Web Services, Knowledge Management. John Wiley & Sons.
Dale, R., Moisl, H., & Somers, H. (2000). Handbook Natural Language Processing. Marcel
Dekker Inc.
Daude, J., Padro, L., & Rigau, G. (2000). Mapping WordNets using structural information.
38th Anual Meeting Association Computational Linguistics (ACL2000),
Hong Kong.
Edmonds, P., & Kilgarriff, A. (2002). Natural Language Engineering, Special Issue Word
Sense Disambiguation Systems. No. 8 (4). Cambridge University Press.
Fellbaum, C. (1998). WordNet: Electronic Lexical Database. MIT Press.
Fernandez-Amoros, D., Gonzalo, J., & Verdejo, F. (2001). UNED systems Senseval2. Proceedings SENSEVAL-2 Workshop. conjunction ACL, Toulouse,
France.
Fujii, A., Inui, K., Tokunaga, T., & Tanaka, H. (1998). Selective sampling example-based
word sense disambiguation. Computational Linguistics, No. 24 (4), pp. 573598.
Humphreys, L., Lindberg, D., Schoolman, H., & Barnett, G. (1998). Unified Medical
Language System: informatics research collaboration. Journal American
Medical Informatics Association, 1 (5).
Ide, N., & Veronis, J. (1998). Introduction special issue word sense disambiguation:
state art. Computational Linguistics, 24 (1), 140.
Jurafsky, D., & Martin, J. (2000). Introduction Natural Language Processing, Computational Linguistics, Speech Recognition. Prentice-Hall, Upper Saddle River,
NJ 07458.
Kim, S.-B., Seo, H.-C., & Rim, H.-C. (2004). Information retrieval using word senses: root
sense tagging approach. SIGIR 04: Proceedings 27th annual international
ACM SIGIR conference Research development information retrieval, pp.
258265, New York, NY, USA. ACM.
Kohomban, U., & Lee, W. (2005). Learning semantic classes word sense disambiguation. Proceedings 43rd Annual Meeting Association Computational
Linguistics (ACL05).
Leacock, C., Chodorow, M., & Miller, G. A. (1998). Using corpus statistics WordNet
relations sense identification. Computational Linguistics, Vol. 24, pp. 147165.
Lin, D. (1998). Automatic retrieval clustering similar words. Proceedings
COLING-ACL, Montreal, Canada.
104

fiOn Use Automatically Acquired Examples All-Nouns WSD

Liu, S., Liu, F., Yu, C., & Meng, W. (2004). effective approach document retrieval
via utilizing WordNet recognizing phrases. SIGIR 04: Proceedings
27th annual international ACM SIGIR conference Research development
information retrieval, pp. 266272, New York, NY, USA. ACM.
Manning, C. D., & Schutze, H. (1999). Foundations Statistical Natural Language Processing. MIT Press, Cambridge, Massachusetts.
McCarthy, D., Koeling, R., Weeds, J., & Carroll, J. (2004). Finding predominant word senses
untagged text. Proceedings 42nd Annual Meeting Association
Computational Linguistics (ACL), Barcelona, Spain.
Mihalcea, R. (2002). Bootstrapping large sense tagged corpora. Proceedings
3rd International Conference Language Resources Evaluation (LREC), Las
Palmas, Spain.
Mihalcea, R. (2004). Co-training self-training word sense disambiguation. Proceedings Conference Natural Language Learning (CoNLL 2004), Boston,
USA.
Mihalcea, R., & Chklovski, T. (2003). Open Mind Word Expert: Creating large annotated
data collections Web users help. Proceedings EACL 2003 Workshop
Linguistically Annotated Corpora (LINC 2003), Budapest, Hungary.
Mihalcea, R., Chklovski, T., & Killgariff, A. (2004). Senseval-3 English lexical sample
task. Proceedings 3rd ACL workshop Evaluation Systems
Semantic Analysis Text (SENSEVAL), Barcelona, Spain.
Mihalcea, R., & Edmonds, P. (2004). Senseval-3, Third International Workshop
Evaluation Systems Semantic Analysis Text. Association Computational Linguistics.
Mihalcea, R., & Moldovan, D. (1999). automatic method generating sense tagged
corpora. Proceedings AAAI-99, Orlando, FL.
Miller, G. A., Leacock, C., Tengi, R., & Bunker, R. (1993). semantic concordance.
Proceedings ARPA Human Language Technology Workshop, pp. 303308,
Princeton, NJ. distributed Human Language Technology San Mateo, CA: Morgan
Kaufmann Publishers.
Ngai, G., & Florian, R. (2001). Transformation-Based Learning fast lane. Proceedings Second Conference North American Chapter Association
Computational Linguistics, pp. 4047, Pittsburgh, PA, USA.
Patwardhan, S., & Pedersen, T. (2003). cpan wordnet::similarity package.
http://search.cpan.org/author/SID/WordNet-Similarity-0.03/.



Pedersen, T. (2001). decision tree bigrams accurate predictor word sense.
Proceedings Second Meeting North American Chapter Association
Computational Linguistics (NAACL-01), Pittsburgh, PA.
Pham, T. P., Ng, H. T., & Lee, W. S. (2005). Word sense disambiguation semisupervised learning. Proceedings 20th National Conference Artificial
Intelligence (AAAI 2005), pp. 10931098, Pittsburgh, Pennsylvania, USA.
105

fiMartinez, Lopez de Lacalle & Agirre

Ravin, Y., & Leacock, C. (2001). Polysemy: Theoretical Computational Approaches.
Oxford University Press.
Resnik, P. (2006). Word sense disambiguation natural language processing applications.
Agirre, E., & Edmonds, P. (Eds.), Word Sense Disambiguation, chap. 11, pp. 299
337. Springer.
Snyder, B., & Palmer, M. (2004). English all-words task. Proceedings 3rd
ACL workshop Evaluation Systems Semantic Analysis Text (SENSEVAL), Barcelona, Spain.
Stevenson, M. (2003). Word Sense Disambiguation: Case Combining Knowledge
Sources. CSLI Publications, Stanford, CA.
Stevenson, M., & Clough, P. (2004). Eurowordnet resource cross-language information retrieval. Proceedings Fourth International Conference Language
Resources Evaluation, Lisbon, Portugal.
Strapparava, C., Gliozzo, A., & Giuliano, C. (2004). Pattern abstraction term similarity
word sense disambiguation: IRST Senseval-3. Proceedings 3rd ACL
workshop Evaluation Systems Semantic Analysis Text (SENSEVAL), Barcelona, Spain.
Tugwell, D., & Kilgarriff, A. (2001). WASP-Bench: lexicographic tool supporting word
sense disambiguation. Proceedings SENSEVAL-2 Workshop. conjunction
ACL-2001/EACL-2001, Toulouse, France.
Vickrey, D., Biewald, L., Teyssier, M., & Koller, D. (2005). Word-sense disambiguation
machine translation. Proceedings Human Language Technology Conference
Conference Empirical Methods Natural Language Processing.
Vilalta, R., & Drissi, Y. (2002). perspective view survey meta-learning. Artificial
Intelligence Review, No. 18 (2), pp. 7795.
Vossen, P., Rigau, G., Alegra, I., Agirre, E., Farwell, D., & Fuentes, M. (2006). Meaningful
results information retrieval MEANING project. Proceedings Third
International WordNet Conference, Jeju Island, Korea.
Wang, X., & Carroll, J. (2005). Word sense disambiguation using sense examples automatically acquired second language. Proceedings joint Human Language
Technologies Empirical Methods Natural Language Processing conference, Vancouver, Canada.
Wang, X., & Martinez, D. (2006). Word sense disambiguation using automatically translated sense examples. Proceedings EACL 2006 Workshop Cross Language
Knowledge Induction, Trento, Italy.
Weeber, M., Mork, J., & Aronson, A. (2001). Developing test collection biomedical
word sense disambiguation. Proceedings AMIA Symposium, pp. 746750.
Yarowsky, D. (1994). Decision lists lexical ambiguity resolution: Application accent
restoration Spanish French. Proceedings 32nd Annual Meeting
Association Computational Linguistics, pp. 8895, Las Cruces, NM.
106

fiOn Use Automatically Acquired Examples All-Nouns WSD

Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. Proceedings 33rd Annual Meeting Association Computational
Linguistics, pp. 189196, Cambridge, MA.
Yarowsky, D., & Florian, R. (2003). Evaluating sense disambiguation across diverse parameter spaces. Natural Language Engineering, 8 (2), 293310.

107

fiJournal Artificial Intelligence Research 33 (2008) 179-222

Submitted 01/08; published 10/08

Rigorously Bayesian Beam Model Adaptive Full
Scan Model Range Finders Dynamic Environments
Tinne De Laet
Joris De Schutter
Herman Bruyninckx

tinne.delaet@mech.kuleuven.be
joris.deschutter@mech.kuleuven.be
herman.bruyninckx@mech.kuleuven.be

Department Mechanical Engineering
Katholieke Universiteit Leuven
Celestijnenlaan 300B, box 2420, 3001 Heverlee, Belgium

Abstract
paper proposes experimentally validates Bayesian network model range
finder adapted dynamic environments. modeling assumptions rigorously explained, model parameters physical interpretation. approach results
transparent intuitive model. respect state art beam model
paper: (i) proposes different functional form probability range measurements
caused unmodeled objects, (ii) intuitively explains discontinuity encountered
state art beam model, (iii) reduces number model parameters, maintaining representational power experimental data. proposed beam model
called RBBM, short Rigorously Bayesian Beam Model. maximum likelihood
variational Bayesian estimator (both based expectation-maximization) proposed
learn model parameters.
Furthermore, RBBM extended full scan model two steps: first,
full scan model static environments next, full scan model general, dynamic
environments. full scan model accounts dependency beams adapts
local sample density using particle filter. contrast Gaussian-based state
art models, proposed full scan model uses sample-based approximation.
sample-based approximation enables handling dynamic environments capturing multimodality, occurs even simple static environments.

1. Introduction
probabilistic approach, inaccuracies embedded stochastic nature model,
particularly conditional probability density representing measurement process.
vital importance types inaccuracies affecting measurements incorporated probabilistic sensor model. Inaccuracies arise sensor limitations, noise,
fact complex environments represented perceived
limited way. dynamic nature environment particular important source
inaccuracies. dynamic nature results presence unmodeled possibly
moving objects people.
paper proposes probabilistic range finder sensor model dynamic environments.
Range finders, widely used mobile robotics, measure distances z objects
environment along certain directions relative sensor. derive sensor
c
2008
AI Access Foundation. rights reserved.

fiDe Laet, De Schutter & Bruyninckx

model form suitable mobile robot localization, i.e.: P (Z = z | X = x, = m)1 ,
Z indicates measured range, X position mobile robot (and
sensor mounted it), environment map. presented model however
useful applications range sensors well.
First, paper derives probabilistic sensor model one beam range finder,
i.e. beam model. particular, paper gives rigorously Bayesian derivation using Bayesian network model stating model assumptions giving physical
interpretation model parameters. obtained model named RBBM, short
Rigorously Bayesian Beam Model. innovations presented approach (i)
introduce extra state variables = positions unmodeled objects probabilistic sensor model P (z | x, m, a), (ii) marginalize extra state variables
total probability estimation. latter required extra variables
(exponentially!) increase computational complexity state estimation lot
applications estimating position unmodeled objects primary interest.
summary, marginalization avoids increase complexity infer probability
distributions P (x) P (m), maintaining modeling dynamic nature
environment.
paper furthermore presents maximum-likelihood variational Bayesian estimator (both based expectation-maximization) learn model parameters
RBBM.
Next, paper presents extension RBBM full scan model i.e.: P (z | , x, m)
z contain measured distances beam angles, respectively. full
scan model accounts dependency beams adapts local sample
density using particle filter. contrast Gaussian-based state art models,
proposed full scan model uses sample-based approximation. sample-based approximation allows us capture multi-modality full scan model, shown
occur even simple static environments.
1.1 Paper Overview
paper organized follows. Section 2 gives overview related work. Section 3
(i) presents Bayesian beam model range finders founded Bayesian networks,
RBBM, (ii) mathematically derives analytical formula probabilistic sensor model
clearly stating assumptions, (iii) provides useful insights obtained beam
model (iv) shows obtained analytical sensor model agrees proposed
Bayesian network. Section 4 presents maximum likelihood variational Bayesian
estimator (both based expectation-maximization) learn model parameters.
Section 5 model parameters RBBM learned experimental data
resulting model compared state art beam model proposed Thrun,
Burgard, Fox (2005), called Thruns model. Section 6 extends RBBM
adaptive full scan model dynamic environments. Section 7 discusses obtained
RBBM adaptive full scan model compares previously proposed range
finder sensor models.
1. simplify notation, explicit mention random variable probabilities omitted whenever
possible, replaced common abbreviation P (x) instead writing P (X = x).

180

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

2. Related Work
Three basic approaches deal dynamic environments exist literature (Fox,
Burgard, & Thrun, 1999; Thrun et al., 2005): state augmentation, adapting sensor
model outlier detection.
state augmentation latent states, e.g. position moving objects people environment, included estimated states. Wang, Thorpe, Thrun
(2003) developed algorithm SLAM DATMO, short SLAM detection
tracking moving objects. State augmentation however often infeasible since
computational complexity state estimation increases exponentially number
independent state variables estimate. closely related solution consists adapting
map according changes environment. Since approaches assume
environment almost static, unable cope real dynamics populated
environments (Fox et al., 1999). recent, related approach proposed Wolf
Sukhatme (2004) maintains two coupled occupancy grids environment, one
static map one moving objects, account environment dynamics.
Probabilistic approaches extent robust unmodeled dynamics, since
able deal sensor noise. approaches however, sensor noise
reflect real uncertainty due unmodeled dynamics environment. Therefore,
second approach dealing dynamic environments adapt sensor model
correctly reflect situations measurements affected unmodeled environment dynamics. Fox et al. (1999) show approaches capable model
noise average, and, approaches work reliably occasional sensor blockage,
inadequate situations fifty percent measurements
corrupted.
handle measurement corruption effectively, approach based outlier detection used. approach uses adapted sensor model, explained
previous paragraph. idea investigate cause sensor measurement
reject measurements likely affected unmodeled environment dynamics.
Hahnel, Schulz, Burgard (2003a) Hahnel, Triebel, Burgard, Thrun (2003b)
studied problem performing SLAM environments many moving objects using
EM algorithm filtering affected measurements. so, able
acquire maps environment conventional SLAM techniques failed. Fox et al.
(1999) propose two different kinds filters: entropy filter, suited arbitrary sensor, distance filter, designed proximity sensors. filters detect whether
measurement corrupted not, discard sensor readings resulting objects
contained map.
paper focuses (sonar laser) range finders, whose physical principle
emission sound light wave, followed recording echo. Highly accurate
sensor models would include physical parameters surface curvature material
absorption coefficient. parameters however difficult estimate robustly unstructured environments. Hence, literature typically relies purely basic geometric
models.
range finder sensor models available literature traditionally divided
three main groups: feature-based approaches, beam-based models correlation-based
181

fiDe Laet, De Schutter & Bruyninckx

methods. Feature-based approaches extract set features range scan match
features contained environmental model. Beam-based models, also known
ray cast models, consider distance measurement along beam separate range
measurement. models represent one-dimensional distribution distance
measurement parametric function, depends expected range measurement
respective beam directions. addition, models closely linked geometry physics involved measurement process. often result overly
peaked likelihood functions due underlying assumption independent beams.
last group range finder sensor models, correlation-based methods, build local maps
consecutive scans correlate global map. simple efficient likelihood
field models end point model (Thrun, 2001) related correlation-based methods. Plagemann, Kersting, Pfaff, Burgard (2007) nicely summarize advantages
drawbacks different range finder sensor models.
Range finder sensor models also classified according whether use discrete
geometric grids (Hahnel et al., 2003a, 2003b; Fox et al., 1999; Burgard, Fox, Hennig, &
Schmidt, 1996; Moravec, 1988) continuous geometric models (Thrun et al., 2005; Choset,
Lynch, Hutchinson, Kantor, Burgard, Kavraki, & Thrun, 2005; Pfaff, Burgard, & Fox,
2006). Moravec proposed non-Gaussian measurement densities discrete grid possible distances measured sonar; likelihood measurements computed
possible positions mobile robot given time. Even simplified models (Burgard
et al., 1996) approach turned computationally expensive real-time
application. Therefore, Fox et al. proposed beam model consisting mixture two
physical causes measurement: hit object map, object
yet modeled map. last cause accounts dynamic nature environment. analogous mixture (Thrun et al., 2005; Choset et al., 2005) adds two
physical causes: sensor failure unknown cause resulting max-range measurement random measurement, respectively. Thrun et al. Pfaff et al. use
continuous model, Choset et al. present discrete analog mixture, taking
account limited resolution range sensor. Pfaff et al. extend basic mixture
model use Monte Carlo localization. overcome problems due combination
limited representational power peaked likelihood accurate range finder,
propose adaptive likelihood model. likelihood model smooth global
localization peaked tracking.
Recently, different researchers tried tackle problems associated beam-based
models, caused independence assumptions beams. Plagemann et al. (2007)
propose sensor model full scan. model treats sensor modeling task
non-parametric Bayesian regression problem, solves using Gaussian processes.
claimed Gaussian beam processes combine advantages beam-based
correlation-based models. Due underlying assumption measurements
jointly Gaussian distributed, Gaussian beam processes suited take
account non-Gaussian uncertainty due dynamic nature environment.
alternative approach handle overly-peaked likelihood functions resulting
traditional beam models proposed Pfaff, Plagemann, Burgard (2007). locationdependent full scan model takes account approximation error sample-based
representation, explicitly models correlations individual beams introduced
182

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

pose uncertainty. measurements assumed jointly Gaussian distributed
Plagemann et al. proposed. Plagemann et al. represent covariance
matrix parametrized covariance function using Gaussian processes whose parameters
learned data, Pfaff et al. learn full covariance matrix less restrictive
manner. Despite modeled correlation beams, measurements still
assumed jointly Gaussian distributed, limits applicability dynamic
environments.
paper proposes rigorously Bayesian modeling probabilistic range sensor
beam model dynamic environments, referred RBBM. Similar work Thrun
et al. (2005) Pfaff et al. (2006) sensor model derived continuous geometry.
Unlike previous models Thrun et al. (2005), Pfaff et al. (2006), Fox et al. (1999)
Choset et al. (2005), mixture components founded Bayesian modeling.
modeling makes use probabilistic graphical models, case Bayesian networks.
graphical models provide simple way visualize structure probabilistic model,
used design motivate new models (Bishop, 2006). inspection
graph, insights model, including conditional independence properties obtained.
Next, inspired adaptive full scan models literature (Pfaff et al., 2006, 2007;
Plagemann et al., 2007), RBBM extended adaptive full scan model.
underlying sample-based approximation full scan model, contrast Gaussianbased approximation proposed Pfaff et al. (2007) Plagemann et al., enables handling
dynamic environments capturing multi-modality, occurs even simple static
environments.

3. Beam Model
model probabilistic beam model P (Z = z | X = x, = m) dynamic environments Bayesian network. introduce extra state variables = positions
unmodeled objects probabilistic sensor model P (z | x, m, a). prevent exponential increase computational complexity state estimation due extra
variables, variables marginalized total probability estimation.
marginalization:

P (z | x, m) =

Z

P (z | x, m, a) P (a) da,

(1)



avoids increasing complexity infer conditional probability distributions interest,
P (x) P (m), maintains modeling dynamic nature environment.
Section 3.1 explains extra state variables physically relevant, Section 3.3
explains marginalization extra state variables. Section 3.5 summarizes assumptions approximations. Finally, Section 3.6 provides useful insights obtained
beam model, called RBBM, derivation. Section 3.7 shows obtained analytical expression RBBM agrees proposed Bayesian network means
Monte Carlo simulation.
183

fiDe Laet, De Schutter & Bruyninckx

p

X

N



XN j
n
K

XKi
k


Zoccl

Z



Figure 1: Bayesian network probabilistic measurement model supplemented
deterministic parameters represented smaller solid nodes.
compact representation plates (the rounded rectangular boxes) used.
plate represents number, indicated lower right corner, independent
nodes single example shown explicitly.

184

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

3.1 Bayesian Model
Bayesian networks graphically represent probabilistic relationships variables
mathematical model, structure facilitate probabilistic inference computations
variables (Jensen & Nielsen, 2007; Neapolitan, 2004). Bayesian network defined
follows: (i) set nodes, associated random variable, connected directed
edges forming directed acyclic graph (DAG); (ii) discrete (continuous) random variable finite (infinite) set mutually exclusive states; (iii) random variable
parents B1 , . . . , BN conditional probability distribution P (A | B1 , . . . , Bn ) (known
conditional probability table case discrete variables). Although definition
Bayesian networks refer causality, requirement directed
edges represent causal impact, well-known way structuring variables reasoning
uncertainty construct graph representing causal relations (Jensen & Nielsen, 2007).
case graphical models also known generative models (Bishop, 2006), since
capture causal process generating random variables.
application, range sensor ideally measures z , distance closest
object map. unknown number n unmodeled objects, possibly preventing
measurement closest object map, however present environment.
Depending position jth unmodeled object along measurement beam, xN j ,
unmodeled object occludes map not. unmodeled object occludes
map located front closest object contained map. k total number
occluding objects n unmodeled objects. positions occluding
objects measurement beam denoted {xKi }i=1:k . map occluded

unmodeled object, range sensor ideally measure zoccl
= xKc , xKc position
closest occluding object.
following extra state variables, Eq. (1), included Bayesian model: N
discrete random variable indicating unknown number unmodeled objects
environment; XN j continuous random variable position jth unmodeled
object measurement beam; K discrete random variable indicating number
objects occluding measurement map; XKi continuous random variable

position ith occluding object measurement beam; Zoccl

continuous random variable indicating ideal range measurement closest occluding
object. Fig. 1 shows Bayesian network probabilistic range finder sensor model
variables Z, X occur probabilistic sensor model (defined


Section 1), extra variables N, XN = {XN j }j=1:n , K, XK = {XKi }i=1:k , Zoccl
model parameters p (defined Section 3.2).
directed edges graphical model represent causal relationships
variables. example, X unambiguously determine measured range Z
perfect sensor absence unmodeled occluding objects. number occluding
objects K depends total number N unmodeled objects positions XN
respect measurement beam. X also causal impact K: larger
expected measurement z , higher possibility one unmodeled objects
occluding modeled object corresponding expected measurement. positions
along measurement beam XK occluding objects equal positions
K N unmodeled objects occluding map. Therefore, random variables XK
185

fiDe Laet, De Schutter & Bruyninckx

P (n)
0.3
0.2
0.1

0 1 2 3 4 5 6 7 8 9 10

n

Figure 2: P (n) (Eq. (2)) p = 0.65.

influenced K also XN . Since K objects occluding map,
positions along measurement beam limited interval [0, z ], XK

causal dependency X . ideal measurement zoccl
occluding object

position occluding object closest sensor, Zoccl depends positions XK
occluding objects. Finally, measurement Z also depends ideal measurement

occluding object Zoccl
number occluding objects K. case occlusion

(k 1), zoccl ideally measured, else (no occlusion, k = 0) z ideally measured.
3.2 Conditional Probability Distributions
Inferring probability distribution extra state variables P (n) often
feasible. Marginalization extra state variables Z, X, M, N, XN , K, XK , Zoccl
avoids
increase complexity estimation problem, still takes account dynamic nature environment. Marginalization requires modelling conditional
probability tables conditional probability distributions (pdf ) random variable
conditionally parents.
First all, assumptions made P (n). Assume probability
number unmodeled objects decreases exponentially, i.e. P (n) given by:
P (n) = (1 p) pn ,

(2)

p measure degree appearance unmodeled objects. precisely, p
probability least one unmodeled object present. p indicated Fig. 1,
Fig. 2 shows resulting distribution P (n).
Secondly, assume nothing known priori position unmodeled
objects along measurement beam. Hence unmodeled objects position assumed
uniformly distributed measurement beam (Fig. 3):
(
1
xN j zmax
P (xN j ) = zmax
(3)
0
otherwise,
186

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

P (xN j | n)

u

1
zmax

z

0

zmax

z

Figure 3: P (xN j | n) (Eq. (3)).

P (k | n, x, m)
0.3
0.2
0.1

0 1 2 3 4 5 6 7 8 9 10

k

Figure 4: P (k | n, x, m) (Eq. (5)) n = 10 u = 0.25.

zmax maximum range range sensor.
Thirdly, assume positions unmodeled objects independent:
P (xN | n) =

n


P (xN j ) .

(4)

j=1

Next, expression needed conditional probability: P (k | n, xN , x, m), i.e.
probability k n unmodeled objects occluding map m. unmodeled
object occluding map located along measurement beam front
closest object map. straightforward show P (k | n, xN , x, m)
binomial distribution:
!


n

uk (1 u)nk k n
(5)
P (k | n, xN , x, m) =
k


0
otherwise,

n
u probability unmodeled object occluding map
=
k
n!
(nk)!k! number ways selecting k objects total n objects. Fig. 4 shows
187

fiDe Laet, De Schutter & Bruyninckx

XN 1

XN 2

XN 3

...

XN j

...

XK1

XK2

...

XKi

...

XN n2

XN n1

XN n

XKk

Figure 5: selection scheme, cross eliminates unmodeled object
occluding map.

binomial distribution. Since assumed positions unmodeled objects
uniformly distributed, u, probability unmodeled object occluding
map is:
u = P (xN j < z ) =

z
zmax

,

(6)

depicted Fig. 3.
Furthermore, analytical expression P (xK | xN , k) necessary. positions
occluding objects xK equal positions unmodeled objects xN
occluding map, shown Fig. 5. words, xKi equals xN j
unmodeled object occluding map, i.e. xN j z :

1


(xKi xN j ) = zmax
z (xKi xN j ) xN j z
P (xN j z )
P (xKi | xN j , k, x, m) =
(7)
0
otherwise,
Dirac function xKi occluding object corresponding xN j .
case occlusion, range sensor ideally measures distance closest occluding object xKc :


P (zoccl
| xK ) = (zoccl
xKc ) .

(8)

range finders truly quite deterministic since measurements great
extent explainable underlying physical phenomena specular reflections, inference,
... underlying phenomena complex therefore costly model. top
underlying phenomena additional uncertainty measurement due (i) uncertainty
sensor position, (ii) inaccuracies world model (iii) inaccuracies sensor
itself. far disturbances measurements due unmodeled objects environment included. capture additional uncertainty, additional measurement noise
added. taking account disturbances unmodeled objects, unexplainable
measurements sensor failures (Section 3.4), physical reason expect
mean value true measurements deviates expected measurement
true measurements distributed asymmetrically around mean. Therefore
188

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

symmetrical noise mean value zero added. Two facts justify modeling
measurement noise normal distribution: (i) normal distribution maximizes
information entropy among distributions known mean variance, making
natural choice underlying distribution data summarized terms sample mean
variance; (ii) underlying phenomena assumed small, independent
effect measurement, central limit theorem states certain conditions
(such independent identically distributed finite variance), sum
large number random variables approximately normally distributed. measurement noise modeled zero mean Gaussian standard deviation , conditional
, k) is:
probability P (z | x, m, zoccl
(
N (z; z , )
k = 0

P (z | x, m, zoccl
, k) =
(9)

N (z; zoccl , ) k 1,
, k) two main cases, first k = 0
conditional probability P (z | x, m, zoccl
occlusion present sensor observing map m, second case
k 1 sensor observes occluding object. included Bayesian
network Fig. 1.

3.3 Marginalization
section shows different steps needed marginalize extra state variables
Eq. (1), motivates approximation leads analytical sensor model.
product rule rewrites sensor model P (z | x, m) as:
P (z | x, m) =

P (z, x, m)
P (z, x, m)
=
,
P (x, m)
P (x) P (m)

(10)

since X independent. numerator obtained marginalizing joint
) x ,
probability whole Bayesian network pjoint = P (z, x, m, xN , n, xK , k, zoccl
N
:
n, xK , k zoccl
Z
XZ XZ

pjoint dxN dxK dzoccl
.
(11)
P (z, x, m) =

zoccl

k

xK

n

xN

Using chain rule factorize joint distribution making use conditional
dependencies Bayesian network (Fig. 1) yields:


| xK ) P (k | n, xN , x, m)
, k) P (zoccl
pjoint = P (z | x, m, zoccl

P (xK | xN , k, x, m) P (xN | n) P (n) P (x) P (m) .
Substituting (12) (11) (10) gives:
Z
Z
X

, k)
P (z | x, m, zoccl
P (z | x, m) =

zoccl

xK

k

X


P (zoccl
| xK )


P (k | n, x, m) P (n) P (xK | n, k, x, m) dxK dzoccl
,

n

189

(12)

(13)

fiDe Laet, De Schutter & Bruyninckx

P (xKi | x, m)
1
z

xKi

0

z

zmax

xKi

Figure 6: P (xKi | n, k, x, m) (Eq. (15))


P (xK | n, k, x, m) =

Z

P (xK | xN , k, x, m) P (xN | n) dxN .

(14)

xN

Since binomial distribution P (k | n, xN , x, m) Eq. (5) independent xN ,
moved integral xN (14), denoted P (k | n, x, m).
Marginalizing xN study integral xN Eq. (14) focus xN j ,
position one unmodeled object. Substituting (3) (7) results in:
Z z
1
zmax
(xKi xN j )
dxN j
P (xKi | n, k, x, m) =

zmax
xN j =0 z
(
1
xKi z
z
=
(15)
0
otherwise.
equation expresses xKi uniformly distributed conditioned n, k, x
shown Fig. 6. Since occluding objects considered independent:
( k
1
0 k : xKi z
z
(16)
P (xK | n, k, x, m) =
0
otherwise.
equation shows P (xK | n, k, x, m) independent n thus moved
summation n Eq. (13):
Z
X



,
(17)
| k, x, m) P (k | x, m) dzoccl
, k) P (zoccl
P (z | x, m, zoccl
P (z | x, m) =

zoccl

k


P (k | x, m) =

X

P (k | n, x, m) P (n) ,

(18)

n



P (zoccl
| n, k, x, m) =

Z

xK


P (zoccl
| xK ) P (xK | k, x, m) dxK .

190

(19)

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

Marginalizing n First focus summation n Eq. (18) substitute (2)
(5):
P (k | x, m) =




X
n
uk (1 u)nk (1 p) pn .
k

(20)

n=k

Appendix proves infinite sum simplifies to:

P (k | x, m) = 1 p pk ,

(21)



p =


.
1 (1 u) p

(22)

Marginalizing xK focus integral xK Eq. (19). Substituting (8)
equation results in:
Z


P (zoccl | k, x, m) =
(zoccl
xKc ) P (xKc | k) dxKc
xKc


| k, x, m) .
= P (xKc = zoccl

(23)


equation shows conditional probability P (zoccl
| k, x, m) represents prob , i.e. probability perfect measurement nearest occluding object zoccl

ability nearest occluding object located zoccl . case one

k objects along measurement beam located zoccl
measured,
objects along measurement beam located behind occluding object,
expressed probabilities:


P (zoccl
| k, x, m) =

k
X



P (xK6=i zoccl
| k, x, m) P (xKi = zoccl
| k, x, m) .

(24)

i=1

Since xK uniformly distributed [0, z ] shown Eq. (15), follows that:

| k, x, m) =
P (xKi = zoccl

P (xKi zoccl
| k, x, m) =

1
,
z

z zoccl
,

z

(25)
(26)

(24) written as:
P


(zoccl

1
| k, x, m) = k
z




z zoccl
z

k1

.

(27)


Marginalizing k obtaining expressions P (k | x, m) (Eq. (21)) P (zoccl
| k, x, m)
(Eq. (27)) turn attention summation k Eq. (17):
X



P (z | x, m, zoccl
, k) P (zoccl
| k, x, m) P (k | x, m) .
(28)
P (z, zoccl
| x, m) =
k

191

fiDe Laet, De Schutter & Bruyninckx

Split summation two parts: one k = 0, occlusion, one
, k) given
k 1, substitute expressions P (k | x, m) P (z | x, m, zoccl
Eq. (21) Eq. (9), respectively:


P (z, zoccl
| x, m) = N (z; z , ) P (zoccl
| k = , x, m) P (k = | x, m) +


N (z; zoccl
, ) P (zoccl
| k , x, m) P (k | x, m)



= N (z; z , ) P (zoccl | k = , x, m) p +


N (z; zoccl
, ) (zoccl
| x, m) ,




(zoccl

| x, m) =
=


P (zoccl

X
k=1

| k 1, x, m) P (k 1 | x, m)


P (zoccl
| k, x, m) 1 p pk .

(29)

(30)

Substituting (27) (30) results in:

(zoccl

| x, m) =


X
k=1

1
k
z




z zoccl
z

k1

simplified using Eq. (114) Appendix A:



(zoccl

| x, m) =
=

X
1
k
1 p p

z
p (1

h

z 1
Substituting (32) (29) gives:





k=1
p )


z zoccl
p
z




1 p pk ,


z zoccl
p
z

k1
(32)

i2 .




P (z, zoccl
| x, m) = N (z; z , ) P (zoccl
| k = , x, m) p +

N (z; zoccl
, )

(31)

z

h

( p ) p

. (33)
z zoccl

p

z



Marginalizing zoccl
Substituting (33) (17) shows integration zoccl
still carried out:
Z z
p






dzoccl
N (z; zoccl
, ) h
P (z | x, m) = (1 p )N (z; z , ) + p
.(34)
z zoccl
=

zoccl
z
p

z

first term right hand side Gaussian distribution around ideal measurement, multiplied probability occlusion (k = 0). second term
integration possible positions occluding object scaled Gaussian distribu ). scaling factor
tion centered ideal measurement occluding object (zoccl

represents probability occluding objects located zoccl
measured.
Eq. (20) Eq. (32) follows scaling factor written as:

(zoccl
| x, m) =

p (1 p)
h

i2 ,

zoccl
zmax 1 1 zmax
p
192

(35)

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

PSfrag
0.45

Finite sum
Approximation

0.5
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0

1

2

3

4

5

6

7

z

Figure 7: Comparison approximation (Eq.(36)) integral Eq. (34)
finite sum approximation small step size p = 0.8, zmax = 10, z = 5
= 0.15.

P (z | x, m)

Finite sum
Approximation

1.0

0.90
0.86

0.9

0.82

0.8
0.7

0.78

0.6
4.90 4.95 5.00 5.05 5.10

0.5
0.4
0.3
0.2
0.1
0

1

2

3

4

5

6

7

8

9

10

z

Figure 8: Comparison obtained RBBM P (z | x, m) (Eq. (37)) finite sum approximation Eq. (34) small step size p = 0.8, zmax = 10, z = 5
= 0.15.

193

fiDe Laet, De Schutter & Bruyninckx

independent z .
now, approximations made obtain Eq. (34) beam model
P (z | x, m). integral scaled Gaussian distributions however, cannot obtained analytically. Therefore, first approximation marginalization made
, )
neglecting noise range measurement case occlusion, i.e.: N (z; zoccl


(z zoccl ). Using approximation second term right hand side Eq. (34)
becomes:
p (1 p )
p (1 p)
(36)
i2 .
h

2 =

z
z
z
z 1 z p
p
zmax 1 1 zmax

Fig. 7 shows quality approximation integral Eq. ( 34) compared
finite sum approximation small step size. approximation introduces discontinuity
around z = z . Using proposed approximation integral resulting beam model
is:

(p )
(1 p ) N (z; z , ) + p
z z
z z

p

z
)]
[
(

z
P (z | x, m) =
(37)
(1 p ) N (z; z , )
otherwise,

shown Fig. 8.
RBBM written mixture two components:

P (z | x, m) = 1 Phit (z | x, m) + 2 Poccl (z | x, m) ,

(38)



(39)

1 = 1 p
2 = p



(40)



Phit (z | x, m) = N (z; z , )

1p
1
z 1 z z p 2
[ ( z )]
Poccl (z | x, m) =
0

(41)
0 z z

(42)

otherwise.

3.4 Extra Components

Occasionally, range finders produce unexplainable measurements, caused phantom readings sonars bounce walls, suffer cross-talk (Thrun et al., 2005). Furthermore additional uncertainty measurements caused (i) uncertainty
sensor position, (ii) inaccuracies world model (iii) inaccuracies sensor itself.
unexplainable measurements modeled using uniform distribution spread
entire measurement range [0, zmax ]:
(
1
0 z zmax ,
(43)
Prand (z | x, m) = zmax
0
otherwise.
Furthermore, sensor failures typically produce max-range measurements, modeled
point-mass distribution centered around zmax :
(
1 z = zmax ,
Pmax (z | x, m) = (zmax ) =
(44)
0 otherwise.
194

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

two extra components added Eq. (38), resulting final RBBM:
P (z | x, m) = 1 Phit (z | x, m) + 2 Poccl (z | x, m) + 3 Prand (z | x, m) + 4 Pmax (z | x, m) ,
(45)
3 4 probabilities range finder returns unexplainable measurement maximum reading, respectively. Furthermore,

1 = 1 p (1 3 4 )
(46)
2 = p (1 3 4 ),

(47)

Phit (z | x, m), Poccl (z | x, m), Prand (z | x, m) Pmax (z | x, m) given (41),
(42), (43) (44) respectively.
3.5 Assumptions Approximations
section summarizes assumptions approximations made arrive RBBM
Eq. (45).
Section 3.2 makes four assumptions:
(i) probability number unmodeled objects decreases exponentially, Eq. (2);
(ii) unmodeled objects position uniformly distributed measurement beam
(Fig. 3, Eq. (3));
(iii) positions unmodeled objects independent, Eq. (4);
(iv) measurement noise zero mean normally distributed standard deviation
(Eq. 9).
Furthermore, Section 3.3 makes one approximation obtain analytical expression
neglecting noise range measurement case occlusion (Eq. (34)).
3.6 Interpretation
following paragraphs give insights RBBM derivation.
mixture representation (45) shows four possible causes range measurement:
hit map, hit unmodeled object, unknown cause resulting
random measurement sensor failure resulting maximum reading measurement.
derivation Section 3.3 shows position occluding objects
uniformly distributed sensor ideally measured object environment
(Eq. (15), Fig. 6). perfectly reasonable considering assumption uniformly
distributed unmodeled objects.

Furthermore, insights provided concerning (zoccl
| x, m) (Eq. (35), Fig. 7),

probability occluding objects located zoccl
measured. First
all, probability independent location ideally measured object
environment (z ) (except probability zero z > z ). agrees intuition,
since one expects measurements caused occluding objects independent z ,
measurement case occlusion. Second, probability sensing unmodeled
objects decreases range, expected. easily explained following
thought experiment: two objects present likelihood perception
field range finder, first object closest range sensor, sensor
likely measure first object. measure second object, second object
195

fiDe Laet, De Schutter & Bruyninckx

present first object absent (Thrun et al., 2005). Moreover,
rate decrease likelihood sensing unmodeled objects dependent p,
degree appearance unmodeled objects.
probability measuring feature map, therefore integral
scaled Gaussian (1 p )Phit (z | x, m) (45), decreases expected range. easily
explained since probability map occluded decreases feature
located away.
Finally, discontinuity RBBM (Fig. 8) shown caused
approximation made (Section 3.5). Since state art range sensors accurate, neglecting measurement noise measurement occluding object
acceptable approximation. also shown experiments presented Section 5.
respect state art beam model Thrun et al. (2005), model proposed here, Eq. (45), has: (i) different functional form probability range measurements caused unmodeled objects, (ii) intuitive explanation discontinuity
encountered cited paper, (iii) reduction number model parameters.
Thrun et al. find Poccl (z | x, m) exponential distribution. exponential
distribution results following underlying assumptions (although revealed
authors): (i) unmodeled objects equally distributed environment (ii)
beam reflected constant probability range. last assumption equals
assuming probability unmodeled object located certain distance
constant. assumption fails capture number unmodeled object finite
probable limited number unmodeled objects huge
number them. also assume unmodeled objects equally distributed
environment (Eq. (3)), assume number unmodeled objects geometrically distributed (Eq. (2)) capturing finiteness number unmodeled objects
higher probability smaller number unmodeled objects. modeling
finiteness number unmodeled objects higher probability smaller
number unmodeled objects results quadratic decay Poccl (z | x, m), instead
exponential decay Poccl (z | x, m) found Thrun et al..
stated previous paragraph, discontinuity RBBM (Fig. 8) caused
approximation. Thruns model considers rate decay Poccl (z | x, m)
independent 2 , probability occlusion, shown
depend parameter p (Eq. (42), Eq. (47)). Therefore RBBM fewer
parameters Thruns model.
3.7 Validation
goal section show means Monte Carlo simulation2 RBBM,
Eq. (45), agrees Bayesian network Fig. 1. Monte Carlo simulation approximate inference method Bayesian networks. idea behind Monte Carlo simulation
draw random configurations network variables Z, X, , N , XN = {XN j }j=1:n ,

K, XK = {XKi }i=1:k Zoccl
sufficient number times. Random configurations selected ancestral sampling (Bishop, 2006), i.e. successively sampling
2. Monte Carlo simulation also known stochastic simulation Bayesian network literature
(Jensen & Nielsen, 2007).

196

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

P (z | x, m)
Finite sum
Approximation
Monte Carlo Simulation

1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

1

2

3

4

5

6

7

8

9

10

z

Figure 9: Comparison obtained RBBM P (z | x, m) (45), finite sum approximation
Eq. (34) small stepsize normalized histogram 500 samples
obtained Monte Carlo Simulation proposed Bayesian network (Fig. 1)
p = 0.8, zmax = 10, z = 5, = 0.15, 3 = 0.2 4 = 0.02.

states variables following causal model defined directed acyclic graph
Bayesian network.
Fig. 9 shows RBBM agrees Monte Carlo simulation 500 samples
proposed Bayesian network.

4. Variational Bayesian Learning Model Parameters
RBBM, Eq. (45), depends four independent model parameters:


= , p , 3 , 4 ,

(48)

zmax known sensor characteristic. set parameters clear physical
interpretation; standard deviation zero mean Gaussian measurement noise
Eq. (9) governing Phit (z | x, m) (Eq. (41)); p , defined Eq. (21), probability
map occluded (P (k 1 | x, m)); 3 4 probabilities range finder
returns unexplainable measurement (unknown cause) maximum reading (sensor
failure), respectively.
alternative non-minimal set parameters containing mixing coefficients =
[1 , 2 , 3 , 4 ] could used: = [m , ], provided constraint:
S=4
X

= 1,

s=1

197

(49)

fiDe Laet, De Schutter & Bruyninckx



1 , 2 , 3 , 4

Z


J

Figure 10: Graphical representation mixture measurement model (Eq. (45))
latent correspondence variable = {D1 , D2 , D3 , D4 } model parameters
= [m , 1 , 2 , 3 , 4 ].

taken account. set minimal parameters straightforwardly follows
non-minimal set since:
p =

2
,
1 3 4

(50)

seen Eq. (47).
physical interpretation parameters allows us initialize hand
plausible values. However, another, flexible way learn model parameters
actual data containing J measurements Z = {z }=1:J corresponding states X =
{x }=1:J map m. Furthermore, learning model parameters also validation
proposed analytical model: learning algorithm succeeds finding model
parameters resulting distribution gives good explanation data,
analytical model likely agree well reality.
paper two different estimators3 , maximum likelihood (ML) (Dempster, Laird,
& Rubin, 1977; McLachlan & Krishnan, 1997; Bishop, 2006) variational Bayesian
(VB) (Beal & Ghahramani, 2003; Bishop, 2006) estimator, presented learn model
parameters data. Section 4.1 derives maximum likelihood estimator,
known approach problem, reformulated RBBM. ML estimator
provides point estimates parameters leads overfitting since likelihood
function generally higher complex model structures. Therefore, propose
variational Bayesian (VB) estimator Section 4.2, new approach learning
parameters beam models. VB estimator fully Bayesian learning approach; priors
unknown parameters included, complex (overfitting) models punished,
full probability distribution parameters obtained.
3. paper approximately follows notation Bishop (2006).

198

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

4.1 Maximum Likelihood Learning
maximum likelihood estimator proposed identify model parameters
maximize likelihood data Z corresponding X map m:
= arg max log P (Z | X, m, ) .


(51)

using mixture representation RBBM (Eq. (45)), estimation problem
formulated finding ML estimates parameters = [m , ] provided
constraint Eq. (49) included. general known four
possible causes actually caused measurements. case ML estimation
problem difficult lacks closed-form solution. however, corresponding causes
measurements known, solution easily obtained closed form.
Therefore, introduce latent correspondence variable = [d1 , d2 , d3 , d4 ], representing
unknown cause, using 1-of-S representation. elements ds give probability
measurement result sth cause. graphical representation
mixture formulation including latent correspondence variable shown Fig. 10.
Although ML estimation problem lacks closed-form solution due unknown
correspondences, expectation-maximization approach (EM) solve problem
iterating expectation maximization step. expectation step calculates
expectation correspondence variables ds maximization step computes
model parameters expectations.
Algorithm 1 ML estimator model parameters
convergence criterion satisfied
z Z, = 1 : J, J = |Z|1

calculate zm
= [ 1 Phit (z | x , m) + 2 Poccl (z | x , m) + 3 Prand (z | x , m) +
4 Pmax (z | x , m)]1
(d1 ) = 1 Phit (z | x , m)
(d2 ) = 2 Poccl (z | x , m)
(d3 ) = 3 Prand (z | x , m)
(d4 ) = 4 Pmax (z | x , m)
end P
1 = J 1 P (d1 )
2 = J 1 P (d2 )
3 = J 1 P (d3 )
4 = J 1 (d4 )
p = 1324
rP
=



2
(d
P 1 )(z z )
(d
)
1


end
return = [m , p , 3 , 4 ]

199

fiDe Laet, De Schutter & Bruyninckx

marginal distribution correspondence variable specified terms
mixing coefficients that:
P (ds = 1) = ,
parameters must satisfy following two conditions:

0 1,
PS
s=1 = 1.

(52)

(53)

Since uses 1-of-S representation, marginal distribution written as:
P (d) =




sds .

(54)

s=1

EM-algorithm expresses complete-data log likelihood, i.e. log likelihood
observed latent variables:
J
X

(d1 (log 1 + log Phit (z | x , m)) +
log P Z, | X, , =
=1

d2 (log 2 + log Poccl (z | x , m)) +

d3 (log 3 + log Prand (z | x , m)) +
d4 (log 4 + log Pmax (z | x , m))) ,

(55)

Z = {z }=1:J vector containing observed data = {d } vector
containing matching correspondences.
Expectation step: Taking expectation complete-data log likelihood Eq. (55)
respect posterior distribution latent variables gives:


Q( , old ) = ED log P Z, | X, ,
=

J
X

( (d1 ) (log 1 + log Phit (z | x , m)) +

=1

(d2 ) (log 2 + log Poccl (z | x , m)) +
(d3 ) (log 3 + log Prand (z | x , m)) +
(d4 ) (log 4 + log Pmax (z | x , m))) ,

(56)

(ds ) = E [ds ] discrete posterior probability, responsibility (Bishop, 2006),
cause data point z . E-step, responsibilities evaluated using Bayes
theorem, takes form:
1 Phit (z | x , m)
,
Norm
2 Poccl (z | x , m)
(d2 ) = E [d ] =
,
Norm
3 Prand (z | x , m)
(d3 ) = E [d ] =
,
Norm
4 Pmax (z | x , m)
,
(d4 ) = E [d ] =
Norm
(d1 ) = E [d ] =

200

(57)
(58)
(59)
(60)

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

Norm normalization constant:
Norm = 1 Phit (z | x , m) + 2 Poccl (z | x , m) + 3 Prand (z | x , m) + 4 Pmax (z | x , m) .
(61)
Two measures derived responsibilities:
Js =

J
X

(ds ) ,

(62)

=1

zs =

J
1 X
(ds ) z ,
Js

(63)

=1

Js effective number data points associated cause s, zs mean
effective data points associated cause s.
Maximization step: M-step expected complete-data log likelihood Eq. (56)
maximized respect parameters = [m , ]:
new = arg max
Q( , old ).



(64)

Maximization respect using Lagrange multiplier enforce constraint
P
= 1 results in:
=

Js
,
J

(65)

effective fraction points data set explained cause s. Maximization
respect results in:
v
u
J
u1 X

=
d1 (z z )2 .
(66)
J1
=1

Algorithm 1 summarizes equations ML estimator, called ML-EM
algorithm.
4.2 Variational Bayesian Learning
ML estimator provides point estimates parameters sensitive
overfitting (Bishop, 2006). Therefore, propose variational Bayesian (VB) estimator,
new approach learning parameters beam models. VB estimator
fully Bayesian learning approach; priors unknown parameters included, complex
(overfitting) models punished, full probability distribution parameters
obtained. VB estimator little computational overhead compared
ML estimator (Bishop, 2006).
Bayesian approach attempts integrate possible values uncertain
quantities rather optimize ML approach (Beal, 2003; Beal & Ghahramani, 2003). quantity results integrating latent variables
201

fiDe Laet, De Schutter & Bruyninckx

R
parameters known marginal likelihood4 : P (Z) = P (Z | D, ) P (D, ) d(D, ),
P (D, ) prior latent variables parameters model. Integrating parameters penalizes models degrees freedom, since
models priori model larger range data sets. property Bayesian integrations
known Occams razor, since favors simpler explanations data complex
ones (Jeffreys & Berger, 1992; Rasmussen & Ghahramani, 2000).
Unfortunately, computing marginal likelihood, P (Z), intractable almost
models interest. variational Bayesian method constructs lower bound
marginal likelihood, attempts optimize bound using iterative scheme
intriguing similarities standard EM algorithm. emphasize similarity
ML-EM, algorithm based variational Bayesian inference called VB-EM.
introducing distribution Q latent variables complete log marginal
likelihood decomposed (Bishop, 2006):
log P (Z) = L (Q) + KL (Q||P ) ,

(67)



(68)


L (Q) =

Z

Q (D, ) log

P (Z, D, )
Q (D, )



(D, ) ,

KL (Q||P ) KL-divergence Q P . Since KL-divergence always
greater equal zero, L (Q) lower bound log marginal likelihood. Maximizing lower bound respect distribution Q (D, ) equivalent minimizing
KL-divergence. possible choice Q (D, ) allowed, maximum
lower bound would occur KL-divergence vanishes, i.e. Q (D, ) equal
posterior distribution P (D, | Z). Working true posterior distribution
however often intractable practice. One possible approximate treatment considers
restricted family distributions Q (D, ) seeks member family minimizing
KL-divergence. variational Bayesian treatment uses factorized approximation,
case latent variables parameter :
Q (D, ) = QD (D) Q () .

(69)

variational approach makes free form (variational) optimization L (Q) respect
distributions QD (D) Q (), optimizing respect factors
turn. general expressions optimal factors (Bishop, 2006):
log QD (D) = E [log P (Z, D, )] + C te ,
log Q ()

te

= ED [log P (Z, D, )] + C ,



(70)
(71)

indicates optimality. expressions give explicit solution factors,
optimal distribution one factors depends expectation computed
respect factor. Therefore iterative procedure, similar EM, cycles
factors replaces turn revised optimal estimate used.
4. avoid overloading notation conditioning map positions X = {x } associated
data Z = {z } explicitly written.

202

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

Introducing priors Since variational Bayesian approach fully Bayesian approach,
priors introduced parameters = [, , ]. Remark
variational Bayesian estimator standard deviation governing Phit (z | x, m)
(Eq. (41)) estimated also means, referred on. Since analysis
considerably simplified conjugate prior distributions used, Dirichlet prior chosen
mixing coefficients :
P () = Dir (|0 ) ,

(72)

well independent Gaussian-Wishart prior5 mean precision =
1 Gaussian distribution P

hit (z | x, m) (Eq. (41)):

P (, ) = N | , (m ) W (m |W , ) .

(73)

0 gives effective prior number observations associated component
mixture. Therefore, value 0 set small, posterior distribution mainly
influenced data rather prior.
Expectation step
written as:

Using conjugate priors, shown factor QD (D)

QD (D) =

J


d1 d2 d3 d4
r1
r2 r3 r4 ,

(74)

=1

quantities rs responsibilities analogous Eq. (57) given
by:
rs =


,
1 + 2 + 3 + 4

(75)


log 1 = E [log 1 ] + E [log Phit (z | x , m)] ,

(76)

log 2 = E [log 2 ] + E [log Poccl (z | x , m)] ,

(77)

log 3 = E [log 3 ] + E [log Prand (z | x , m)] ,

(78)

log 4 = E [log 4 ] + E [log Pmax (z | x , m)] .

(79)

equations rewritten as:
h




log 1 = E [log 1 ] + E [log |m |] log () E,m (z )T (z ) , (80)



log 2 = E [log 2 ] + log Poccl (z | x , m) ,
(81)
log 3 = E [log 3 ] + log Prand (z | x , m) ,

(82)

log 4 = E [log 4 ] + log Pmax (z | x , m) ,

(83)

5. parameters defined Bishop (2006).

203

fiDe Laet, De Schutter & Bruyninckx

expectations calculated follows:
E [log ] = (s ) (1 + 2 + 3 + 4 ) ,

+ log 2 + log |W |,
E [log |m |] =
2


h
E,m (z )T (z )

= 1 + (z )T W (z ) ,

(84)
(85)
(86)

digamma function.
Three measures derived responsibilities:
J
X

Js =

(87)

rs ,

=1

J
1 X
rs z ,
Js

zs =

(88)

=1

J
1 X
rs (z zs ) (z zs )T ,
Js

Cs =

(89)

=1

Js effective number data points associated cause s, zs mean
effective data points associated cause Cs covariance effective data
points associated cause s. Due similarity E-step EM-algorithm,
step calculating responsibilities variational Bayesian inference known
variational E-step.
Maximization step accordance graphical representation Fig. 10,
shown variational posterior Q () factorizes Q () Q (1 , )
first optimal factor given Dirichlet distribution:
Q () = Dir (|) ,

(90)

= 0 + Js .

(91)



second optimal factor given Gaussian-Wishart distribution:

Q (1 , ) = N |, (m ) W (m |W, ) ,

(92)



= 0 + J1 ,
1
(0 0 + J1 z1 ) ,
=

W 1 = W01 + J1 C1 +

(93)
(94)

0 J1
(z1 0 ) (z1 0 )T ,
0 + J1

= 0 + J1 .

(95)
(96)

204

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

update equations analogous M-step EM-algorithm maximum
likelihood solution therefore known variational M-step. variational M-step
computes distribution parameters (in conjugate family) rather point
estimate case maximum likelihood estimator. distribution
parameters allows us calculate predictive density P (z | Z).
Due use conjugate priors, integrals predictive density calculated analytically:



1 St z|, 1+
W, + 2 Poccl (z | x, m) + 3 Prand (z | x, m) + 4 Pmax (z | x, m)
P (z | Z) =
,
1 + 2 + 3
(97)
St (.) Students t-distribution. size J data large, Students
t-distribution approximates Gaussian predictive distribution rewritten as:
P (z | Z) =

1 N (z|, ) + Poccl (z | x, m) + Prand (z | x, m) + Pmax (z | x, m)
. (98)
1 + 2 + 3 + 4

point estimates desired parameters, maximum posteriori estimates
obtained follows:

+ + +
1

2

W
=
,
1+
2
.
=
1 3 4

= E [s ] =

p

(99)
(100)
(101)

Algorithm 2 summarizes equations VB-EM estimator.

5. Experiments
goal section threefold: (i) learn model parameters (Eq. (48))
RBBM (Eq. (45)) experimental data, (ii) compare results proposed
ML-EM VB-EM estimator (Section 4), (iii) compare results proposed
estimators learning approach Thruns model proposed Thrun et al. (2005).
end two experimental setups different application areas robotics used.
data first learning experiment gathered typical mobile robot application
robot equipped laser scanner travelling office environment.
data second learning experiment gathered typical industrial pickand-place operation human populated environment. laser scanner mounted
industrial robot make aware people unexpected objects robots
workspace.
see well learned model explains experiment, learned continuous pdf
P (z | x, m, ) Eq. (45) compared discrete pdf experimental
data (histogram) H (z). end, learned pdf first discretized using bins
{zf }f =1:F experimental pdf. quantize difference learned
205

fiDe Laet, De Schutter & Bruyninckx

Algorithm 2 Variational Bayesian estimator model parameters
convergence criterion satisfied
z Z, = 1 : J, J = |Z|1

calculate zm

1

1
1 = exp (1 ) (1 + 2 + 3 +
4 ) + 2 2 + log 2 + log |W | 2 log (2) . . .
21 1 + (z )T W (z )
2 = exp [ (2 ) (1 + 2 + 3 + 4 ) + log Poccl (z | x, m)]
3 = exp [ (3 ) (1 + 2 + 3 + 4 ) + log Prand (z | x, m)]
4 = exp [ (4 ) (1 + 2 + 3 + 4 ) + log Pmax (z | x, m)]
= 1 + 2 + 3 + 4
r1 = 1 1
r2 = 1 2
r3 = 1 3
r4 = 1 4
end
P
J1 = J=1 r1
PJ
J2 = =1 r2
P
J3 = J=1 r3
P
J4 = J=1 r4
P
z1 = J11 Jj=1 r1 z
P
C1 = J11 J=1 r1 (z z1 ) (z z1 )T ,
1 = 0 + J1 .
2 = 0 + J2 .
3 = 0 + J3 .
4 = 0 + J4 .
= 0 + J1
= 1 (0 0 + J1 z1 )
W 1 = W01 + J1 C1 +
= 0 + J1

0 J 1
0 +J1

(z1 0 ) (z1 0 )T

1
1 = 1 +2+
3 +4
2
2 = 1 +2 +3 +4
3
3 = 1 +2+
3 +4
4
4 = 1 +2 +3 +4
p = 1324
1

2

W
= 1+
end
return {1 , 2 , 3 , 4 , , , W, , = [, , p , 3 , 4 ]}

206

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

N

N

800

800

700

700

600

600

500

500

400

400

300

300

200

200

100

100

0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

4.5

5.0

z[m]

0

0.5

(a) Short range

1.0

1.5

2.0

2.5

3.0

3.5

4.0

4.5

5.0

z[m]

(b) Long range

Figure 11: Data second learning experiment reported Thrun et al. (2005).
data consist two series measurements obtained mobile robot traveling typical office environment. set measurements 10000
measurements centered around two different expected ranges selected.

experimental pdf two distance measures used: discrete KL-divergence:
d1 = KL (H||P )

F
X

H (zf ) log

f =1

H (zf )
,
P (zf | x, m, )

square root discrete Hellinger distance:
v
u F

uX
1
1 2
d2 = DH (H||P )
H (zf ) 2 P (zf | x, m, ) 2 .

(102)

(103)

f =1

latter known valid symmetric distance metric (Bishop, 2006).
5.1 First Learning Experiment
first learning experiment, experimental data reported Thrun et al. (2005) used.
data consists two series measurements obtained mobile robot traveling
typical office environment. set measurements, 10000 measurements
centered around two different expected ranges, selected. two obtained
sets different expected ranges shown Fig. 11. parameters learning
algorithms listed Table 1. Fig. 12 Table 2 show results ML-EM
VB-EM estimators RBBM compared results ML estimator Thruns
model (Thrun et al., 2005) two sets. results obtained running
learning algorithms 30 iteration steps.
207

fiDe Laet, De Schutter & Bruyninckx

ML-EM
RBBM
m,init = 0.5
pinit = 0.4
3,init = 0.2
4,init = 0.1

VB-EM
RBBM
3,init = 18
pinit = 13
init = 5000 4,init = 18
Winit = 12
0 = 5
init = xmp
W0 = 50
init = 100
0 = xmp
1,init = 58
0 = 100
1
2,init = 8
0 = 1

ML-EM
Thruns model
m,init = 0.5
zhit,init = 0.4
zshort,init = 0.3
zmax,init = 0.1
zrand,init = 0.2
short,init = 0.1

Table 1: EM-parameters first second learning experiment (all SI-units).
ML approaches, mean Phit (z | x, m) set xmp , i.e. probable
bin histogram training set H (z).

P (z | x, m)

P (z | x, m)

0.8

1.2
1.0
0.8

ML-EM RBBM
VB-EM RBBM
ML-EM Thruns model
Histogram Training set

0.7

ML-EM RBBM
VB-EM RBBM
ML-EM Thruns model
Histogram Training set

0.6
0.5
0.4

0.6

0.3

0.4

0.2
0.2

0.0

0.1
0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

4.5

5.0

z[m]

(a) Short range

0.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

4.5

5.0

(b) Long range

Figure 12: Comparison results ML-EM VB-EM estimators RBBM
results maximum likelihood estimator Thruns model (Thrun
et al., 2005) data Fig. 11.

proposed ML-EM VB-EM estimator outperform ML-EM estimator
Thruns model studied data sets. Despite reduced number parameters
RBBM compared Thruns model (Section 3.6), RBBM least
representational power.
208

z[m]

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

Experiment

short range
long range
average

ML-EM
RBBM
0.5295
0.4366
0.4830

d1 (Eq. (102))
VB-EM
ML-EM
RBBM Thruns model
0.5127
0.7079
0.4368
0.5852
0.4747
0.6465

ML-EM
RBBM
0.3166
0.1683
0.2425

d2 (Eq. (103))
VB-EM
ML-EM
RBBM Thruns model
0.2971
0.5629
0.2100
0.3481
0.2535
0.4555

Table 2: Discrete KL-divergence (d1 ) square root Hellinger distance (d2 ) first
learning experiment training set results ML-EM
VB-EM estimators RBBM ML-EM estimator Thruns model
(Thrun et al., 2005).

(a) Front view

(b) Side view

(c) Zoomed front view

Figure 13: Setup second learning experiment Sick LMS 200 laser scanner
mounted first axis industrial Kuka 361 robot.

5.2 Second Learning Experiment
data second learning experiment gathered execution typical
industrial pick-and-place operation human-populated environment. Sick LMS 200
laser scanner mounted first axis industrial Kuka 361 robot (Fig. 13).
laser scanner provides measurements robot environment therefore people
unexpected objects robots workspace. Processing measurements first
step towards making industrial robots aware possibly changing environment
moving robots cages.
209

fiDe Laet, De Schutter & Bruyninckx

Robot

4

selected ranges
cut region
environment

2

y[m]

0

-2

-4

-6
-6

-4

-2

0

x[m]

2

4

6

Figure 14: Map build robots static environment, i.e. without unexpected objects
people moving around, rotating first axis industrial robot.
safety reasons, people allowed move inside safety region (circle
radius 1m). Therefore, measurements smaller 1m discarded.
studied expected ranges second learning experiment range 3.0m
4.5m steps 0.1m indicated figure selected ranges
region.

first step, map (Fig. 14) build robots static environment, i.e. without
unexpected objects people moving around, rotating first axis industrial
robot. Next, robot performs pick-and-place operation number people
walking around random robot environment. Different sets measurements
acquired different number people. Similar first learning experiment,
measurements selected centered around different expected ranges acquired
data. studied expected ranges second learning experiment range 3.0m
4.5m steps 0.1m (Fig. 14). safety reasons, people allowed move closer
1m robot, i.e. safety region (Fig. 14). Therefore, measurements smaller
1m discarded.
data, model parameters learning using learning parameters
first learning experiment (Table. 1).
Table 3 shows Kullback Leibler divergence (Eq. (102)) Hellinger distance
(Eq. (103)) averaged studied expected range different set measurements
running ML-EM VB-EM estimators RBBM ML estimator
Thruns model (Thrun et al., 2005). results obtained running
learning algorithms 30 iteration steps.
210

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

Experiment
number
people
1
2
3
4
6
8
average

ML-EM
RBBM
1.7911
1.8002
1.7789
1.8277
1.8007
1.7676
1.7944

d1 (Eq. (102))
VB-EM
ML-EM
RBBM Thruns model
1.5271
1.9697
1.5172
1.9735
1.5199
1.9606
1.5140
1.9853
1.5168
1.9655
1.5157
1.9498
1.5185
1.9674

ML-EM
RBBM
5.6141
5.7038
5.6033
5.7563
5.6483
5.4989
5.6375

d2 (Eq. (103))
VB-EM
ML-EM
RBBM Thruns model
4.3449
6.5582
4.3334
6.6119
4.3468
6.5365
4.2972
6.6744
4.3126
6.5596
4.2843
6.4257
4.3199
6.5611

Table 3: Discrete KL-divergence (d1 ) square root Hellinger distance (d2 ) averaged
studied expected range different set measurements second
learning experiment. Distances training set results
ML-EM VB-EM estimators RBBM ML-EM estimator
Thruns model (Thrun et al., 2005). first column indicates number
people walking around environment particular set measurements.

proposed ML-EM VB-EM estimator outperform ML-EM estimator
Thruns model studied data sets. Despite reduced number parameters
RBBM compared Thruns model (Section 3.6), RBBM least
representational power.

6. Adaptive Full Scan Model
section extends RBBM adaptive full scan model dynamic environments;
adaptive, since automatically adapts local density samples using samplebased representations; full scan, since model takes account dependencies individual beams.
many applications using range finder, posterior approximated finite
set samples (histogram filter, particle filters). peaked likelihood function associated
range finder (small due accuracy) problematic using finite set
samples. likelihood P (z | x, m) evaluated samples, approximately
distributed according posterior estimate. Basic sensor models typically assume
estimate x map known exactly, is, assume one
samples corresponds true value. assumption, however, valid limit
infinitely many samples. Otherwise, probability value exactly corresponds
true location virtually zero. consequence, peaked likelihood functions
adequately model uncertainty due finite, sample-based representation
posterior (Pfaff et al., 2007). Furthermore, use basic range finder model typically
results even peaked likelihood models, especially using large number
beams per measurement, due multiplication probabilities. practice, problem
211

fiDe Laet, De Schutter & Bruyninckx

P (z | x, m)

P (z | x, m)

2.5

2.5
ML-EM RBBM
VB-EM RBBM
ML-EM Thruns model
Histogram Training set

2.0

2.0

1.5

1.5

1.0

1.0

0.5

0.5

0.0
1.0

2.0

3.0

4.0

5.0

6.0

7.0

ML-EM RBBM
VB-EM RBBM
ML-EM Thruns model
Histogram Training set

8.0

0.0

z[m] 1.0

2.0

(a) Short range, 3 people

2.5

ML-EM RBBM
VB-EM RBBM
ML-EM Thruns model
Histogram Training set

2.0

6.0

7.0

8.0

z[m]

1.5

1.0

1.0

0.5

0.5

4.0

5.0

6.0

7.0

8.0

z[m]

ML-EM RBBM
VB-EM RBBM
ML-EM Thruns model
Histogram Training set

2.0

1.5

3.0

5.0

P (z | x, m)

2.5

2.0

4.0

(b) Short range, 8 people

P (z | x, m)

0.0
1.0

3.0

8.0

0.0

z[m] 1.0

(c) Long range, 3 people

2.0

3.0

4.0

5.0

6.0

7.0

(d) Long range, 8 people

Figure 15: Comparison results ML-EM VB-EM estimators RBBM
results maximum likelihood estimator two different expected
ranges two different number people populating robot environment.

212

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

peaked likelihoods, dealt various ways: sub-sampling measurement (fewer
beams); introducing minimal likelihoods beams; inflating measurement uncertainty;
means regularization resulting likelihoods. solutions satisfactory however, since additional uncertainty due sample-based representation
known advance. additional uncertainty strongly varies number
samples uncertainty estimate (Pfaff et al., 2006). Fox (2003) proposes
dynamically adapt number samples means KLD sampling (KLD stands
Kullback-Leibler divergence). peaked likelihoods however, might result
huge number samples. Lenser Veloso (2000) Thrun, Fox, Burgard, Dellaert (2001) ensure critical mass samples located important parts
state space sampling observation model. Sampling observation model
however, often possible approximate inaccurate way. Pfaff et al. (2006)
introduced adaptive beam model dynamic environments, explicitly takes location uncertainty due sample-based representation account. compute
additional uncertainty due sample-based representation, using techniques
density estimation. evaluating likelihood function sample, consider
certain region around sample, depending sample density location. Then,
depending area covered sample, variance Gaussian, , governing
beam model Eq. (38), calculated sample. result, beam model
automatically adapts local density samples. location dependent model results smooth likelihood function global localization peaked function
position tracking without changing number samples.
Plagemann et al. (2007) Pfaff et al. (2007) showed considering region
around samples, individual beams become statistically dependent. degree dependency depends geometry environment size location
considered region. Beam models, RBBM, implicitly assume however
beams independent, is:
P (z | , x, m) =

B


P (zb | b , x, m) ,

(104)

b=1

z = {zb }b=1:B = {b }b=1:B vectors containing measured ranges
angles different beams respectively; zb range measured beam
angle b ; B total number beams P (zb | b , x, m) instance
RBBM (Eq. (45)). neglecting dependency beams, resulting likelihoods
P (z | , x, m) overly peaked. Models taking account dependencies
beams consider full range scan therefore called full scan models on.
full scan models proposed Plagemann et al. (2007) Pfaff et al. (2007)
assume beams range scan jointly Gaussian distributed. off-diagonal
elements covariance matrix associated Gaussian distribution represent
dependency. learn model parameters, methods draw samples region
around sample perform ray-casting using samples. Plagemann et al. train
Gaussian process models full scan, Pfaff et al. directly provide maximum
likelihood estimate mean covariance Gaussian.
Section 6.1 shows dependency beams may introduce multi-modality,
even simple static environments. Multi-variate Gaussian models proposed Plage213

fiDe Laet, De Schutter & Bruyninckx

mann et al. (2007) Pfaff et al. (2007) cannot handle multi-modality. Therefore,
new sample-based method obtaining adaptive full scan model beam model,
able handle multi-modality, proposed. Section 6.2 extends adaptive full scan model
dynamic environments taking account non-Gaussian model uncertainty.
6.1 Sample-based Adaptive Full Scan Model Static Environments
Plagemann et al. (2007) Pfaff et al. (2007) estimate full scan model, P (z | x, m)6 ,
based local environment U (x) exact estimate x:
Z
P (z | x, m) P (x | x) Phit (z | x, m) dx,
(105)
P (x | x) distribution representing probability x element
environment U (x), i.e: x U (x). environment U (x) modeled circular region
around x. Since section consider dynamics environment, one
component RBBM Eq. (37) used: Phit (z | x, m). marginalization
environment U (x) Eq.(105) introduces dependencies measurements zb
measurement vector z.
environment U (x), explained above, depends sample density around
sample x consideration. Pfaff et al. (2006) proposed use circular region
diameter dU (x) , weighted sum Euclidean distance angular
difference. Like Plagemann et al. (2007) Pfaff et al. (2007), approximation
likelihood estimated online sample x simulating L complete range
scans locations drawn U (x) using given map environment. Contrary
multivariate Gaussian approximation proposed Plagemann et al. Pfaff et al.,
propose sample-based approximation, able handle multi-modality. Sampling
environment U (x) immediately results sample-based approximation P (x | x):
L

P (x | x)

1X
x(l) ,
L

(106)

l=1

x(l) denotes delta-Dirac mass located x(l) , samples distributed
according P (x | x):
x(l) P (x | x) .

(107)

Using sample-based approximation P (x | x) likelihood Eq. (105) approximated as:
P (z | x, m)

L


1X
Phit z | x(l) , .
L

(108)

l=1

Since sample-based approximation calculated online, number samples
limited. used environment U (x) large, resulting approximation
6. simplify notation omitted P (z | , x, m) P (zb | b , x, m), respectively.

214

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

Figure 16: Panorama taken Sick LMS 200 range finder mounted Kuka 361
industrial robot. environment consists rectangular room
object (a Kuka KR 15/2 robot) middle. show even simple
static environment, presented sample-based full scan model outperforms
Gaussian-based state art models.

bad. smooth undesired bumpy behavior due limited number samples,
measurement noise governing Phit (z | x, m) Eq. (45), artificially increased depending
size U (x) multiplying factor:
q
1 + C dU (x) .
(109)
experiments, C set 20.

6.1.1 Experiment
simple environment consisting rectangular room object (a Kuka KR 15/2
robot) middle (Fig. 16) used show marginalization (even small)
U (x) obtain true likelihood introduces dependencies beams
also multi-modality. U (x) results local uncertainty x- yposition
0.01m rotational uncertainty 5 . obtain reference, Sick LMS 200 range
finder used take large number measurements (L = 1500) random locations
sampled U (x). allow exact positioning, Sick LMS 200 placed Kuka 361
industrial robot. Sick LMS 200 range finder connected laptop controls
motion Kuka 361 industrial robot network using Corba-facilities Open
Robot Control Software, Orocos (Bruyninckx, 2001; Soetens, 2006). simplified map
environment (Fig. 16) built simulate 150 complete range scans needed construct
full scan model. marginal P (zb | x, m) two selected beams studied
detail. marginal likelihoods selected beam using proposed sample-based approximation Eq. (108) Gaussian approximation proposed Pfaff et al. (2007),
compared Fig. 17(b)-17(c). histogram measurements selected beam
figure clearly shows multi-modality likelihood caused dependency
beams. contrast Gaussian-based state art full scan model, proposed sample-based approximation able handle multi-modality range finder
215

fiDe Laet, De Schutter & Bruyninckx

data. Fig. 17(d) shows difference beams experimentally obtained
cumulative marginal (L = 1500) Gaussian-based sample-based approximation
beams. mean difference experimental data sample-based approximation 2.8 times smaller difference Gaussian-based approximation,
even simple static environment Fig. 16 small U (x).
6.2 Sample-based Adaptive Full Scan Model Dynamic Environments
adaptive beam model proposed Pfaff et al. (2006) suited use dynamic
environments since uses four component mixture beam model (Thrun et al., 2005;
Choset et al., 2005). date however, adaptive full scan likelihood models Pfaff et al.
(2007) Plagemann et al. (2007) adapted dynamic environments.
assumption beams jointly Gaussian distributed, unable capture nonGaussian uncertainty due environment dynamics, prevents straightforward extension
dynamic environments. contrast, sample-based approximation full scan
likelihood, proposed Section 6.1, extended include environment dynamics.
end, replace Phit (z | x, m) Eq. (105) Eq. (108) full mixture Eq. (38).
6.2.1 Experiment
Fig. 18(a) Fig. 18(b) compare marginals selected beams (Fig. 17(a)) obtained
adaptive full scan model dynamic environments using proposed samplebased approximation Gaussian approximation proposed Pfaff et al. (2007).
contrast Gaussian-based state art full scan model, proposed sample-based
approximation able handle multi-modality range finder data. Fig. 18(c) shows
probability map adaptive full scan model (sample-based approximation) suited
dynamic environments example environment Fig. 16. probability map plots
P (z | x, m) function position map shows marginalization
environment U (x) sample Eq. (105) introduces dependency
beams also introduces multi-modality.

7. Discussion
paper proposed experimentally validated RBBM, rigorously Bayesian network model range finder adapted dynamic environments. modeling assumptions
rigorously explained, model parameters physical interpretation.
approach resulted transparent intuitive model. rigorous modeling revealed
underlying assumptions parameters. way clear physical interpretation parameters obtained providing intuition parameter choices. contrast model
Thrun et al. (2005), assumption underlying non-physical discontinuity
RBBM discovered. Furthermore, paper proposes different functional form
probability range measurements caused unmodeled objects Poccl (z | x, m) (Eq. (45)),
i.e. quadratic rather exponential proposed Thrun et al. Furthermore, compared
work Thrun et al. (2005), Choset et al. (2005), Pfaff et al. (2006) RBBM
depends fewer parameters, maintaining representational power experimental data. Bayesian modeling revealed rate decay Poccl (z | x, m)
216

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

5

4.5

4

3.5

3.5

3

2.5

P (zb | x, m)

y[m]

3

2.5

2

1.5

2

1.5

1

1

Environment
measurements
Samples U (x)
Example beams

0.5

0
2.5

Experimental data (1500 samples)
Sample-based approximation
Gaussian approximation (Pfaff et al., 2007)

4

2

1.5

1

0.5

0

0.5

1

1.5

2

0.5

0

2.5

0

1

x[m]
(a) Environment model

2

3

4

5

6

z[m]

(b) Marginal likelihood left beam
3

7

Experimental data (1500 samples)
Sample-based approximation
Gaussian approximation (Pfaff et al., 2007)

2.5

x 10

Sample-based approximation
Gaussian-based approximation (Pfaff et al., 2007)

6

2

Pc (z | x, m)

P (zb | x, m)

5

1.5

1

4

3

2

0.5

1

0

0

1

2

3

4

5

6

z[m]

(c) Marginal likelihood right beam

0

0

50

100

150

200
beam

250

300

350

400

(d) Difference experimental cumulative
marginal Gaussian sample-based approximations

Figure 17: Experimental results sample-based adaptive full scan model static environments. (a) models simple environment Fig. 16. range finder
located (0.15m, 0.75m). Samples U (x) (resulting local uncertainty x- yposition 0.01m rotational uncertainty
5 ) shown black dots, simulated measurements shown
grey. (b) (c) show marginal likelihood P (zb | x, m) two selected
beams together histogram experimentally recorded range finder
data, Gaussian-based approximation (L = 150) Pfaff et al. (2007),
sample-based approximation (L = 150) paper. (d) shows difference beams experimentally obtained cumulative marginal
(L = 1500) Gaussian-based sample-based approximation.
217

fiDe Laet, De Schutter & Bruyninckx

0.9

0.7

Large sample approximation
Sample-based approximation
Gaussian appr. (Pfaff, 2007)

0.8

Large sample approximation
Sample-based approximation
Gaussian appr. (Pfaff, 2007)

0.6
0.7
0.5

P (z | x, m)

P (z | x, m)

0.6

0.5

0.4

0.4

0.3

0.3
0.2
0.2
0.1
0.1

0

0

1

2

3

4

5

6

0

0

1

2

z[m]

3

4

5

z[m]

(a) Marginal likelihood left beam

(b) Marginal likelihood right beam

(c) Probability map P (z | x, m) sample-based
approximation

Figure 18: Results sample-based adaptive full scan model dynamic environments. (a)
(b) show marginal likelihood P (zb | x, m) two selected beams
Fig. 17(a) together Gaussian-based approximation (L = 150) Pfaff
et al. (2007) sample-based approximation (L = 150) extended
use dynamic environments. (c) shows probability map resulting
sample-based approximation. probability map shows P (z | x, m)
function x- position map.

218

6

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

probability occluded measurement 2 depend one parameter p . State art
sensor models however, assume independency two parameters. Finally, maximumlikelihood variational Bayesian estimator (both based expectation-maximization)
proposed learn model parameters RBBM. Learning model parameters
experimental data benefits RBBMs reduced number parameters. Using
two sets learning experiments different application areas robotics (one reported
Thrun et al. (2005)) RBBM shown explain obtained measurements
least well state art model Thrun et al.
Furthermore, paper extended RBBM adaptive full scan model two
steps: first, full scan model static environments next, full scan model
general, dynamic environments. full scan model adapts local sample density
using particle filter, accounts dependency beams. contrast
Gaussian-based state art models Plagemann et al. (2007) Pfaff et al.
(2007), proposed full scan model uses sample-based approximation, cope
dynamic environments multi-modality (which shown occur even
simple static environments).

Acknowledgments
authors thank anonymous reviewers thorough constructive reviews.
authors also thank Wilm Decre, Pauwel Goethals, Goele Pipeleers, Ruben Smits,
Bert Stallaert, Lieboud Van den Broeck, Marnix Volckaert Hans Wambacq participating experiments. authors gratefully acknowledge financial support
K.U.Leuvens Concerted Research Action GOA/05/10 Research Council K.U.Leuven,
CoE EF/05/006 Optimization Engineering (OPTEC). Tinne De Laet Doctoral Fellow
Fund Scientific ResearchFlanders (F.W.O.) Belgium.

Appendix A. Simplification Infinite Sum
goal appendix prove



X
X
n
nk
k
n
u (1 u)
(1 p) p ,
P (k | n, x, m) P (n) =
k
n

(110)

n=k

simplified to:

X
n


P (k | n, x, m) P (n) = 1 p pk ,


p = 1(1u)p
.
Expand Eq. (110) move terms summation that:


X
X
n!
nk
k k
[(1 u) p]
.
P (k | n, x, m) P (n) = (1 p) u p
(n k)!k!
n

(111)

(112)

n=k

Next introduce variables = n k e = (1 u) p:
X
n



X
(t + k)!
P (k | n, x, m) P (n) = (1 p) u p
e .
t!k!
k k

t=0

219

(113)

fiDe Laet, De Schutter & Bruyninckx

next step prove induction that:


X
1
(t + k)!
e =
.
k+1
t!k!
(1

e)
t=0

(114)

First, show equality holds k = 0:

X
t!
t=0

t!



e =


X

et ,

(115)

t=0

well-known geometric series, so:

X

et =

t=0

1
,
1e

(116)

showing equality (114) indeed holds k = 0. Next proved that, expression
holds k 1, also holds k. Introduce variable V solution infinity sum
k split (114) two parts:

X




X
(t + k)!
(t + k)! (t + k 1)! X (t + k 1)!
(117)
V =
e =

e ,
e +
t!k!
t!k!
t! (k 1)!
t! (k 1)!
t=0
t=0
{z
}
|t=0
1
(1e)k

fact used equality (114) holds k 1. Simplify first term
summation to:




X
(t + k)! (t + k 1)! X (t + k 1)!

te .
(118)
e =
t!k!
t! (k 1)!
t!k!
t=0

t=0

term summation = 0 equal zero. Hence, introduce variable = t1
simplify:




X
X
+1
(t + k)!
(t + k)! (t + k 1)!
+1 e
(119)
e =

t!k!
t! (k 1)!
(t + 1)!k!

t=0
=0


X
(t + k)!
= e
e
,
(120)
!k!

|t =0
{z
}
V

series V looking recognized. Substitute result
Eq. (117) that:
V = eV +

1
(1 e)k

.

(121)

Solving equation V gives:
V =

1
(1 e)k+1
220

,

(122)

fiRigorously Bayesian Beam Model Adaptive Full Scan Model

proving equality (114) holds k assumed hold k 1, closing proof
induction.
substitute Eq. (114) Eq. (113):
X

P (k | n, x, m) P (n) =

n

(1 p) uk pk
[1 (1 u) p]k+1

,

(123)

rewrite as:
X
n

p =


1(1u)p ,


P (k | n, x, m) P (n) = 1 p pk ,

(124)

exactly proved.

References
Beal, M. J. (2003). Variational algorithms approximate Bayesian inference. Ph.D. thesis,
University College London.
Beal, M. J., & Ghahramani, Z. (2003). variational bayesian em algorithm incomplete data: application scoring graphical model structures. Valencia
International Meeting Bayesian Statistics, Tenerife, Canary Islands, Spain.
Bishop, C. M. (2006). Pattern Recognition Machine Learning. Springer.
Bruyninckx, H. (2001). Open RObot COntrol Software. http://www.orocos.org/.
Burgard, W., Fox, D., Hennig, D., & Schmidt, T. (1996). Estimating absolute position
mobile robot using position probability grids. Proc. National Conference
Artificial Intelligence.
Choset, H., Lynch, K. M., Hutchinson, S., Kantor, G. A., Burgard, W., Kavraki, L. E., &
Thrun, S. (2005). Principles Robot Motion: Theory, Algorithms, Implementations. MIT Press.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood incomplete
data via EM algorithm (with discussion). Journal Royal Statistical Society
(Series B), 39, 138.
Fox, D. (2003). Adapting Sample Size Particle Filters KLD-Sampling.
International Journal Robotics Research, 22 (12), 9851003.
Fox, D., Burgard, W., & Thrun, S. (1999). Markov localization mobile robots dynamic
environments. Journal Artificial Intelligence Research, 11, 391427.
Hahnel, D., Schulz, D., & Burgard, W. (2003a). Mobile robot mapping populated environments sensor planning. Journal Advanced Robotics, 17 (7), 579597.
Hahnel, D., Triebel, R., Burgard, W., & Thrun, S. (2003b). Map building mobile robots
dynamic environments. Proceedings 2003 IEEE International Conference
Robotics Automation, pp. 15571569, Taipeh, Taiwan. ICRA2003.
Jeffreys, W., & Berger, J. (1992). Ockhams razor bayesian analysis. American Scientist,
80, 6472.
221

fiDe Laet, De Schutter & Bruyninckx

Jensen, F. V., & Nielsen, T. D. (2007). Bayesian Networks Decision Graphs. Springer.
Lenser, S., & Veloso, M. (2000). Sensor resetting localization poorly modelled mobile
robots. Proceedings 2000 IEEE International Conference Robotics
Automation, San Francisco, CA. ICRA2000.
McLachlan, G. J., & Krishnan, T. (1997). EM algorithm extensions. John Wiley
& Sons, New York, NY.
Moravec, H. P. (1988). Sensor fusion certainty grids mobile robots. AI Magazine, 9,
6174.
Neapolitan, R. E. (2004). Learning Bayesian Networks. Pearson Prentice Hall, New York,
NY.
Pfaff, P., Burgard, W., & Fox, D. (2006). Robust Monte-Carlo localization using adaptive
likelihood models. Christensen, H. (Ed.), European Robotics Symposium, Vol. 22,
pp. 181194, Palermo, Italy. Springer-Verlag Berlin Heidelberg, Germany.
Pfaff, P., Plagemann, C., & Burgard, W. (2007). Improved likelihood models probabilistic
localization based range scans. Proceedings 2007 IEEE/RSJ International
Conference Intelligent Robots Systems, San Diego, California. IROS2007.
Plagemann, C., Kersting, K., Pfaff, P., & Burgard, W. (2007). Gaussian beam processes:
nonparametric Bayesian measurement model range finders. Robotics: Science
Systems (RSS), Atlanta, Georgia, USA.
Rasmussen, C., & Ghahramani, Z. (2000). Occams razor. Advances Neural Information Processing 13, Denver, Colorado. MIT Press.
Soetens, P. (2006). Software Framework Real-Time Distributed Robot Machine Control. Ph.D. thesis, Department Mechanical Engineering, Katholieke Universiteit Leuven, Belgium. http://www.mech.kuleuven.be/dept/resources/docs/
soetens.pdf.
Thrun, S., Burgard, W., & Fox, D. (2005). Probabilistic Robotics. MIT Press.
Thrun, S. (2001). probabilistic online mapping algorithm teams mobile robots.
International Journal Robotics Research, 20 (5), 335363.
Thrun, S., Fox, D., Burgard, W., & Dellaert, F. (2001). Robust monte carlo localization
mobile robots. Artificial Intelligence, 128, 99141.
Wang, C.-C., Thorpe, C., & Thrun, S. (2003). Online simultaneous localization mapping detection tracking moving objects: Theory results ground
vehicle crowded urban areas. Proceedings 2003 IEEE International Conference Robotics Automation, Taipeh, Taiwan. ICRA2003.
Wolf, D. F., & Sukhatme, G. S. (2004). Mobile robot simultaneous localization mapping
dynamic environments. Proceedings 2004 IEEE International Conference
Robotics Automation, pp. 13011307, New Orleans, U.S.A. ICRA2004.

222

fiJournal Artificial Intelligence Research 33 (2008) 433-464

Submitted 07/08; published 11/08

Ordinal Bargaining Solution Fixed-Point Property
Dongmo Zhang
Yan Zhang

dongmo@scm.uws.edu.au
yan@scm.uws.edu.au

Intelligent Systems Laboratory
School Computing Mathematics
University Western Sydney, Australia

Abstract
Shapleys impossibility result indicates two-person bargaining problem
non-trivial ordinal solution traditional game-theoretic bargaining model. Although
result longer true bargaining problems two agents, none
well known bargaining solutions ordinal. Searching meaningful ordinal solutions,
especially bilateral bargaining problem, challenging issue bargaining
theory three decades. paper proposes logic-based ordinal solution
bilateral bargaining problem. argue bargaining problem modeled
terms logical relation players physical negotiation items, meaningful bargaining
solution constructed based ordinal structure bargainers preferences.
represent bargainers demands propositional logic bargainers preferences
demands total preorder.
show solution satisfies desirable logical
properties, individual rationality (logical version), consistency, collective rationality
well typical game-theoretic properties, weak Pareto optimality
contraction invariance. addition, players demand sets logically closed,
solution satisfies fixed-point condition, says outcome negotiation
result mutual belief revision. Finally, define various decision problems relation
bargaining model study computational complexity.

1. Introduction
Bargaining central research topic economics five decades
become interesting issue computer science recent years (Osborne & Rubinstein,
1990; Rosenschein & Zlotkin, 1994; Muthoo, 1999). ground-breaking paper, Nash
(1950) models bargaining problem pair (S, d), <2 subset twodimensional Euclidean space (feasible set), representing set utility pairs
derived bargainers preferences feasible outcomes, point designated
disagreement point. bargaining solution function assigns
bargaining problem (S, d) point (Thomson, 1994).
Nash bargaining model presumes bargainers preferences represented
von Neumann-Morgenstern utility, referred cardinal utility (Myerson, 1991).
assumption, two utility functions viewed one derived
affine positive transformation. Thus bargaining solution based
Nashs bargaining model invariant affine positive transformations.
However, traditional economic theory considers bargaining problems players preferences represented ordinal (Calvo & Peters, 2005). Therefore, ideally, bargaining
c
2008
AI Access Foundation. rights reserved.

fiZhang & Zhang

solution invariant order-preserving transformations utilities.
property referred ordinal invariance game-theoretic literature (Thomson,
1994). bargaining solution possesses property called ordinal solution.
Obviously, ordinal bargaining solutions desirable cardinal solutions
ordinal information players preferences easier elicit cardinal preferences
corresponding solutions robust (Sakovics, 2004; Calvo & Peters, 2005).
However, none well known bargaining solutions (Nash, 1950; Kalai & Smorodinsky,
1975; Kalai, 1977; Perles & Maschler, 1981) ordinal. fact, Shapley (1969) showed
two-person bargaining problem (bilateral bargaining) non-trivial
(i.e., strongly individual rational) ordinal solution1 . result generally referred
Shapleys impossibility result game-theoretic literature.
Shapleys negative result obviously discouraged investigation ordinal bargaining,
notwithstanding Shapley demonstrated ten years later ordinal solutions exist
three-person bargaining problem (Shubik, 1982). study ordinal bargaining
theory regain focus game theory recently. Kibris (2001) provided
axiomatic characterization ordinal solution three-person bargaining problem
based Nashs bargaining model2 . Safra Samet (2004) extended result
bargaining problems three players. Rubinstein et al. (1992) ONeill et
al. (2004) investigated ordinal bargaining problem varying Nashs bargaining model.
Calvo Perers (2005) explored problem ordinal bargaining least one
player cardinal. Nevertheless, problem ordinal bargaining still considered
unsolved problem. pierces work focus existence ordinal solutions.
None proposed solutions gains strong intuitive support. Looking meaningful
ordinal bargaining solutions still outstanding problem game theory (Sakovics, 2004).
show difficulty ordinal bargaining, let us consider simple typical bargaining
scenario:
Example 1 (Muthoo, 1999) Two players, B, bargain partition cake.
Let xi share cake percentage player (i = A, B). set possible
agreements represented = {(xA , xB ) : 0 xA 100 xB = 100 xA }.
xi [0, 100], ui (xi ) player utility obtaining share xi cake.
Assume player linear utility scale share, uA (xA ) = xA , player
B utility scale proportional square share, uB (xB ) = x2B . Failure agree rated 0 B. Consider two influential bargaining solutions: Nashs solution (Nash, 1950) Kalai-Smorodinskys solution (Kalai & Smorodinsky, 1975). easy calculate Nashs bargaining solution problem gives
outcome (33.3, 66.7) Kalai-Smorodinskys solution gives (38.2, 61.8). solutions
favor player B. player B less risk-averse (has concave utility)
player (has linear utility). Nashs solution Kalai-Smorodinskys solution,
risk-loving players advantage bargaining comparing risk-neutral risk-averse
players (see Roths book, 1979a, p.35-60). consider order-preserving transformation

(x) = x player Bs utility. transformed utility player B becomes linear.
1. See work Thomson (1994) easy proof.
2. original work formally published. Kibris (2004) gave brief note.

434

fiAn Ordinal Bargaining Solution Fixed-Point Property

new utility scales, Nashs solution Kalai-Smorodinskys solution give (50, 50)
outcome. means none solutions ordinally invariant.
example clearly shows non-linearity utility functions, expresses
risk posture player, determines outcomes bargaining collapses ordinal transformations. words, ordinal transformations filter useful information
expressible cardinal utility expressible ordinal utility. explains
resolution two-player bargaining problem made basis ordinal
utility alone .... satisfactory theory bilateral bargaining requires knowledge something ordering bargainers preferences (see Shubiks book, 1982,
p.94-98).
all, ordinal preference insufficient fully specify bargaining situation.
bargaining model must supply way express information additional ordinal preferences, bargainers attitude towards risk. article aims demonstrate
ordinal solution bilateral bargaining problem language logic used
express knowledge required modeling bargaining ordinal preferences.
recent years, studies logic-based frameworks bargaining negotiation
received considerable attention field artificial intelligence (AI) (Sycara, 1990;
Kraus, Sycara, & Evenchik, 1998; Parsons, Sierra, & Jennings, 1998; Zhang, Foo, Meyer,
& Kwok, 2004; Meyer, Foo, Kwok, & Zhang, 2004; Zhang, 2005, 2007). frameworks
utilize logical languages represent bargaining situations physical negotiation items,
bargaining conflicts, players beliefs mutual threats explicitly expressed,
differentiates traditional game-theoretic models. paper
propose ordinal solution bilateral bargaining problem based logical model
introduced Zhang Zhang (2006a).
organization paper following. Section 2 presents formal model
bargaining. use finite propositional language describe bargainers demands.
bargainers preferences demands sorted total preorder. bargaining
problem defined pair hierarchies two parties demand sets. construction
bargaining solution, presented Section 3, based idea party tries
maximize prior demands included final agreement keeping
outcome consistent. approach viewed extension Nebels prioritized
base revision two-agent setting (Nebel, 1992). Section 4 Section 5 devoted
discussions properties proposed solution. shall prove Section
4 solution satisfies desirable logical properties, logical version
Individual Rationality, Consistency Collective Rationality. extraordinarily,
shall show solution satisfies desirable fixed-point condition introduced
Zhang et al. (2004), says outcome bargaining result mutual
belief revision. Section 5 focuses discussion game-theoretic properties
solution. prove solution satisfies Weak Pareto Optimality, Restricted Symmetry
Contraction Invariance. Section 6 devoted discussion bargainers attitudes
towards risk represented model determine players bargaining power.
Section 7 investigates complexity issues related proposed model. consider four
major decision problems relation bargaining game provide computational
435

fiZhang & Zhang

complexity results. final two sections conclude work discussion related
work.
paper made self-contained. However, reader find basic knowledge belief revision game-theoretic bargaining theory helpful better
understanding concepts introduced paper. introductory survey
areas, see Gardenforss article (1992) Thomsons article (1994), respectively.

2. Representation Bargaining Problems
seen Example 1, ordinal solution bilateral bargaining problem
requires information addition ordinal preferences. order express extra information, model bargaining situation two aspects: physical bargaining terms,
described propositional logic, ordering bargainers preferences
bargaining terms, described total preorder. Since bilateral bargaining
challenging problem bargaining theory, shall restrict bargaining problem two players.
2.1 Preliminaries
assume party set negotiation items, referred demand set,
described finite propositional language L. language consists finite set
propositional variables propositional connectives , ,, standard
syntax semantics. logical closure operator Cn defined Cn(X) = { : X ` },
X set sentences. say X logically closed belief set X = Cn(X).
X two sets sentences, X + denotes Cn(X ).
Suppose X1 X2 two sets sentences. simplify exploration, use Xi
represent set among X1 X2 Xi one them. vector two
components, D1 D2 represent component D.
2.2 Bargaining Games
shown previous section, cardinal utility encodes two types information: bargainers attitude towards risk (via non-linearity utility functions) bargainers preferences
possible outcomes (via ordering utility values). One may ask whether theory
bargaining based purely ordinal information preferences. order investigate possibility, Osborne Rubinstein introduced different way represent
bargaining situation, preference information separated physical
bargaining terms (Osborne & Rubinstein, 1990). Precisely, define bargaining problem four-tuple (X, D, 1 , 2 ), X set feasible outcomes (in physical terms),
disagreement event, complete transitive reflexive ordering set
X D, representing bargainer preferences. shown Example 1, however, simply
describing physical bargaining terms without specifying relations suffice
lead ordinal solution (see also Osborne & Rubinsteins book, 1990, p.32).
paper, extend Osborne & Rubinsteins model way physical
negotiation items represented logical formulae.
436

fiAn Ordinal Bargaining Solution Fixed-Point Property

Definition 1 bargaining game pair ((X1 , 1 ), (X2 , 2 )), Xi (i = 1, 2)
logically consistent set sentences L complete transitive reflexive order
(total preorder weak order) Xi satisfies following logical constraint3 :
(LC) 1 , , n ` , k (1 k n) k .
call pair (Xi , ) prioritized demand set player i. , Xi ,
denotes 6i . denotes .
Intuitively, bargaining game formal representation bargaining situation
player describes demands logical formulae expresses preferences
demands total preorder. assume player consistent demands.
preference ordering player reflects degree entrenchment player
defends demands. logical constraint LC says demand logical
consequence demands 1 , , n , less entrenched
fail defend , least one dropped (otherwise
would lost ). easy see ordering similar Gadenfors
Makinsons epistemic entrenchment (Gardenfors & Makinson, 1988). fact, logical
constraint LC, introduced Zhang Foo (2001), actually combination
postulates EE2 EE3 epistemic entrenchment ordering.
Observation 1 Let total preorder X. logical constraint LC equivalent
conjunction following conditions:
1. ` , .
2. Either .
Proof: easy verify LC implies two conditions. prove
conditions imply LC induction n. Obviously, LC holds n = 1. Assume LC
holds n = l. Suppose 1 , , l1 , l , l+1 ` , 1 , , l1 , (l l+1 ) ` .
inductive assumption, either l l+1 k (1 k l 1)
k . l l+1 , condition 2 transitivity , either l
l+1 . Therefore case k (1 k l + 1) k .

remark preference ordering represents firmly agent entrenches
demands rather gain payoff4 . instance, suppose p1 represents
demand seller price good less $10 p2 denotes price
good less $8. Obviously seller could get higher payoff p1
p2 . However, since p1 implies p2 , entrench p2 less firmly p1 , i.e., p2 p1 ,
3. complete transitive reflexive order, i.e., total preorder weak order, satisfies following properties:
Completeness totality: .
Reflexivity: .
Transitivity: .
4. define bargainers gains Section 5.1.

437

fiZhang & Zhang

because, fails keep p1 , still bargain p2 loss p2 means loss
both.
another significant difference model bargaining gametheoretic model. game-theoretic model abstracts bargaining situation numerical
game. demands, preferences risk posture player represented utility
values. model, however, factors abstracted logical statements
ordering statement, i.e., prioritized demand set. Note endowed
word demand broad meaning. model, demand player
everything related negotiation player wants keep final
agreement. physical item player wants obtain party.
also piece knowledge, belief, goal, constraint even thread, whatever,
player wants keep agreement. Different logics rational agency,
BDI logics, distinguish knowledge, belief, goal real demands.
instance, logical tautology demand player player considers
included final agreement. Consider Example 1 again. Typical demands
player xA 40, xA 50, xA 60, on5 , mean player
wants get share cake less 40%, 50%, 60%, . Player Bs demands
opposite, say xB 40, xB 50, xB 60, . Besides real demands,
set domain constraints players demand sets, xA + xB 100,
xA xA z xB xB z whenever z. constraints link
demands players together, therefore, play important rule determination
bargaining outcomes. instance, cannot xA 55 xB 55
final agreement inconsistent constraints. One may wonder
player include constraints demand set. case,
player would accept unreasonable results, xA 55 xB 55.
words, idealize bargaining problem assuming two individuals highly
rational (see work Nash, 1950, p.155), assume constraints
background knowledge included demand set player (see discussions
example Section 6). Again, ordering demand sets reflect gains
player mentioned above. likely rational agent could give
highest priority mentioned constraints background knowledge since
might never going give fundamental rules.
following example shows model suitable discrete domain
problems.
Example 2 couple making family budget next year. husband wants
change car new fancy model domestic holiday. wife going
implement dream romantic trip Europe suggests redecorate kitchen.
know cant two holidays one year. also realize
cannot afford new car overseas holiday year without getting
loan bank. However, wife like idea borrowing money.
order represent situation logic, let c denote buy new car, stand
domestic holiday, overseas holiday, k kitchen redecoration l loan.
5. Note player may different demands different stages negotiation.

438

fiAn Ordinal Bargaining Solution Fixed-Point Property

(d o) means impossible domestic holiday overseas
holiday. statement (c o) l says want buy new car also
overseas holiday, get loan bank.
symbolization, express husbands demands following
set:
X1 = {c, d, (d o), (c o) l}
Similarly, wifes demands represented by:
X2 = {o, k, (d o), (c o) l, l}
Let us assume husbands preferences demands are:
(d o) 1 (c o) l 1 c 1
wifes preferences are:
(d o) 2 (c o) l 2 2 k 2 l
Note agents give common beliefs: (d o) (c o) l highest
priority. reflects couple rational. shall see common beliefs
play important role determination bargaining solution.
example, also see differences model Osborne
& Rubinsteins model (Osborne & Rubinstein, 1990). First, demand sets X1 X2
model represent players physical demands rather feasible outcomes (note
two players share feasible set X Osborne & Rubinsteins model).
model, feasible set generated demand sets procedure conflict
resolving (see Definition 2 Example 3). Secondly, representation
disagreements. negotiation ends disagreement, simply assume agreement empty. Thirdly, Osborne & Rubinsteins model, items X independent,
model, items players demand sets related
logical relations orderings (via LC). allows us represent bargainers attitude
towards risk.
2.3 Hierarchy Demands
construct bargaining solution, present basic properties prioritized
demand sets. Consider prioritized demand set (X, ) single agent. define
recursively hierarchy, {X j }+
j=1 , X respect ordering follows:
1. X 1 = { X : X( )}; 1 = X\X 1 .
2. X j+1 = { j : j ( )}; j+1 = j \X j+1 .
intuition behind construction following: stage construction,
collects maximal elements current demand set remove
set next stage construction. easy see exists number
439

fiZhang & Zhang

n X =

n


X j due logical constraint LC6 . Therefore demand set X

j=1

always written X 1 X n , X j X k = j 6= k (see Figure 1). Also
entrenched

X1

EAA
E
E
E
E

X2
...
Xn

least entrenched

Figure 1: hierarchy demand set.
X j X k , j < k. words, prioritized
demand set (X, ) uniquely determines partition X,

n


X j , total order

j=1

partition. Therefore, concept prioritized demand set equivalent concept
so-called nicely-ordered partition belief set7 , introduced Zhang Foo (Zhang
& Foo, 2001), demand set logically closed.
sequent, write X k denote

k


Xj.

j=1

2.4 Prioritized Base Revision
hierarchy demand set agent, able define belief
revision operator agent following Nebels prioritized base revision (Nebel, 1992)8 .
define revision function follows:
demand set (X, ) set F sentences,
def

X F =

\

(H + F ),

HXF

X F defined as: H X F
1. H X,
2. k (k = 1, 2, ), H X k maximal subset X k

k


(H X j ) F

j=1

consistent.

words, H maximal subset X consistent F gives priority
higher ranked items. following result used Section 4.
Lemma 1 (Nebel, 1992) X logically closed, satisfies AGM postulates.
6. Note X infinite set even though language finite.
7. nicely-ordered partition belief set K triple (K, , ), partition K
total order satisfies logical constraint LC. See work Zhang Foo (2001) p.540.
8. idea construction traced back Poole Brewkas approach default logic (Brewka,
1989; Poole, 1988).

440

fiAn Ordinal Bargaining Solution Fixed-Point Property

3. Bargaining Solution
Nashs bargaining model, bargaining solution defined function assigns
bargaining game point feasible set game. However,
simple definition model set possible agreements (feasible set)
given bargaining game. generate feasible set demand sets.
involves process conflict resolving.
3.1 Possible Deals
Whenever demands two agents conflict, least one agent make concession
order reach agreement. simple way making concession withdraw
number demands. sense, possible agreement pair subsets two players
original demand sets collection remaining demands consistent. Obviously
player would like keep many original demands possible. addition, player
give demand, player typically gives one lowest priority.
idea leads following definition possible deals.
Definition 2 Let G = ((X1 , 1 ), (X2 , 2 )) bargaining game. deal G pair
(D1 , D2 ) satisfying following conditions: = 1, 2,
1. Di Xi ;
2. X1 X2 Di ;
3. k (k = 1, 2, ), Di Xik maximal subset Xik

k


(Di Xij )

j=1

Di consistent.
{Xij }+
j=1 hierarchy Xi defined Section 2.3. set deals G
denoted (G), called feasible set game.
definition obviously analogue Nebels notion (see Section 2.4).
difference procedure maximization interactive two agents:
given one players demands, player always tops demands highest
prioritized items provided overall outcome consistent.
Since assumed X1 X2 consistent, (G) non-empty. Specifically, X1 X2 consistent, (G) = {(X1 , X2 )}.
Example 3 Consider bargaining game Example 2. According preference orderings couple, game three possible deals:
D1 = ({(d o), (c o) l, c, d}, {(d o), (c o) l, k, l}).
D2 = ({(d o), (c o) l, c}, {(d o), (c o) l, o, k}).
D3 = ({(d o), (c o) l}, {(d o), (c o) l, o, k, l}).
Therefore (G) = {D1 , D2 , D3 }.
441

fiZhang & Zhang

3.2 Core Agreement
shown generate possible deals, form set possible agreements,
i.e., feasible set, resolving conflicting demands. level
game-theoretic model that, define bargaining solution, select deal
possible deals. Obviously, demands two parties contradict,
multiple possible deals. Different deals would favor different parties. instance,
Figure 2, deal D0 favor player 1 D00 favor player 2. Therefore,
conflicts choosing outcomes still exist. major concern bargaining theory
measure balance gain negotiating party.


1
D0

X11

X21

...

...

X1k1

X2k1

X1k

X2k

X1k+1

X2k+1

...

...

P

PD00

Figure 2: Different deals favor different players.
Instead counting number demands party remain deal,
consider top block demands player keeps deal (the top levels demands
players demand hierarchy) ignore demands included
top blocks purpose measuring players gains.
Given deal D, shall use maximal number top levels demands
deal indicator players gain deal, i.e., max{k : Xik Di }
= 1, 2. instance, Figure 2, player 1 remains maximally top k 1 levels
demands deal D00 player 2 successfully gain top k + 1 levels
demands deal. deal D0 , players remain top k levels
demands.
order compare different deals, refer gain index deal gain
player whoever receives less deal, i.e., min{max{k : X1k D1 }, max{k : X2k
D2 }}, equivalently, max{k : X1k D1 X2k D2 }. instance, Figure 2,
gain index D0 k gain index D00 k 1. Therefore say D0
better D00 because, D0 , players remind least top k blocks
demand hierarchies D00 cant.
Formally, let
G
max
=



max

{k : X1k D1 X2k D2 }

(D1 ,D2 )(G)

G
max

(G) = {(D1 , D2 ) (G) : X1

442

G
max

D1 & X2

D2 }

(1)

(2)

fiAn Ordinal Bargaining Solution Fixed-Point Property

(G) collects best deals possible deals G. none
G
deals contain top max
levels demands players.
G
Note max may infinite X1 X2 consistent. Let
G
max

(1 , 2 ) = (X1

G
max

, X2

)

(3)

call = (1 , 2 ) core game. Therefore core contains top block
demands best deals contain, therefore included final agreement.
following lemma gives another way calculate core game.
G
Lemma 2 max
= max{k : X1k X2k (X1 X2 ) consistent}.

Proof: Let = max{k : X1k X2k (X1 X2 ) consistent}. easy show
G

G

X1 max X2 max (X1 X2 ) consistent (G) non-empty. Therefore
G
max
. hand, since X1 X2 (X1 X2 ) consistent, exists
deal (D1 , D2 ) (G) Xi Di X1 X2 Di = 1, 2. Thus
G . conclude = G .
max

max

3.3 Solution
first sight, bargaining solution easily defined function assigns
bargaining game G possible deal (G). likely, solution could select one
best deals (G). However, due multiplicity (G) (see Figure 3),
selection always feasible want solution symmetric player.
show difficulty, let us consider following example.
Example 4 Consider bargaining game G = ((X1 , 1 ), (X2 , 2 )), described three propositional variables p, q r, X1 = {{p}, {r}} X2 = {{q}, {r}} (note
demand sets represented form hierarchy p 1 r q 2 r (see Section
2.3)). easy know game two possible deals, ({p, r}, {q}) ({p}, {q, r}),
(G). However, none deals lead impartial solution. reasonable solution problem ({p}, {q}), take intersection
best deals player, respectively.
Base intuitive explanation, ready present bargaining
solution.
Definition 3 bargaining solution function F defined follows, maps
bargaining game G = ((X1 , 1 ), (X2 , 2 )) pair sets sentences:
def

F (G) = (

\

D1 ,

(D1 ,D2 )(G)

\

D2 )

(4)

(D1 ,D2 )(G)

(G) defined Equation (2).
better understanding construction solution, would like make
following remarks:
443

fiZhang & Zhang

X11

X21

...

...

G

G

X1 max
G



X1 max

1
...
0

X2 max
G

+1

+1

P


PD00
X2 max
iP 000
... P


Figure 3: Multiplicity best deals.
1. solution gives prediction bargaining outcome game. Given game
G, Fi (G) represents demands player successfully remain end
bargaining. also means demands accepted player.
Therefore final agreement bargaining defined F1 (G) F2 (G)9 .
2. Note Fi (G) Xi = 1, 2. Therefore solution means compromise
player. players may make concessions demands order
reach agreement. Obviously, conflict players original
demands, i.e., X1 X2 consistent, concession needed, is, Fi (G) = Xi
(i = 1, 2).
3. construction solution takes skeptical view sense that,
player, demand item included solution belongs best
deals. words, solution gives cautious prediction bargaining
outcome. result, solution necessarily deal multiple
elements (G).
4. solution unique bargaining game. Like bargaining problem
non-convex domain, scarify strict Pareto optimality gain uniqueness
(this reason take cautious prediction). However, show
solution weakly Pareto optimal (see Section 5.2 discussions).
5. bargaining model, specify disagreement points. fact, assume
solution gives empty agreement, i.e., F (G) = (, ), negotiation
reaches disagreement. words, (, ) default disagreement point
bargaining game.
Example 5 Continue Example 3. According hierarchies demand sets shown
Example 2, core game is:
({(d o), (c o) l, c}, {(d o), (c o) l, o})
9. Alternatively, define final agreement Cn(F1 (G) F2 (G)) consider outcome
negotiation contains logical consequences demands agreement. addition,
relation items agreement read rather or. words,
items agreement accepted players.

444

fiAn Ordinal Bargaining Solution Fixed-Point Property

Therefore (G) contains single deal, D2 (see Example 3). solution

F (G) = D2 = ({(d o), (c o) l, c}, {(d o), (c o) l, o, k})
words, couple agree upon commonsense one holiday
get loan want buy new car go overseas holiday.
husband accepts wifes suggestion holiday Europe wife agrees buying
new car.
consider following preference orderings:
(d o) 1 (c o) l 1 1 c
(d o) 2 (c o) l 2 2 k 2 l
Therefore demand hierarchies become:
X1 = {{(d o), (c o) l}, {d}, {c}}.
X2 = {{(d o), (c o) l}, {o}, {k}, {l}}.
Let G0 denote game. deals original hierarchy shown
Example 3. However solution game becomes ({(d o), (c o) l}, {(d
G0 = 2 three deals included (G0 ). Note
o), (c o) l, k}) max
final agreement include demands lead conflicting (d, o, l) keeps
demands lead conflicting (k). words, solution excludes
demands lead conflict keeps demands involved conflicts
even though low priorities.

4. Logical Properties Bargaining Solution
following two sections, discuss properties bargaining solution introduced previous section. According Zhang (2007), bargaining solution satisfies
axioms Collective Rationality, Scale Invariance, Symmetry Mutually Comparable
Monotonicity well basic assumptions Individual Rationality, Consistency Comprehensiveness logical version Kalai-Smorodinsky solution (Kalai &
Smorodinsky, 1975). Among properties, Collective Rationality, Individual Rationality
Consistency capture logical properties bargaining solution. Scale Invariance,
Symmetry Mutually Comparable Monotonicity reflect game-theoretic properties
bargaining solution. Comprehensiveness idealized assumption logic-based bargaining solution. Although significant difference bargaining solution
defined paper one work, two solutions share desirable
properties bargaining solutions.
4.1 Generic Properties Logic-Based Bargaining Solutions
easy see solution constructed previous section satisfies following
generic properties logic-based bargaining solution.
Theorem 1 bargaining game G = ((X1 , 1 ), (X2 , 2 )), let F (G) = (F1 (G), F2 (G)).

445

fiZhang & Zhang

1. F1 (G) X1 F2 (G) X2 .

(Individual Rationality)

2. F1 (G) F2 (G) consistent.

(Consistency)

3. X1 X2 consistent, Fi (G) = Xi i.

(Collective Rationality)

Proof: proofs properties straightforward definition bargaining solution (Definition 3).

Note logical version Individual Rationality (IR) different meaning
game-theoretic version. logical version IR means player concerns
demands, i.e., whether many demands included
final agreement. contrast, game-theoretic version IR concerns whether
player gain less disagreement point negotiation (see details
Section 5.1). two properties quite intuitive.
following example shows solution satisfy comprehensiveness,
requires Fi (G) implies Fi (G) (see work Zhang,
2007).
Example 6 Consider bargaining situation player 1s demand set X1 = {p, q}
player 2s demand set X2 = {p, r}, p, r, q propositional variables. Assume
player ranks demands level (i.e., demand sets singleton
partition). Based assumption, easy know solution game
({q}, {r}). Therefore, solution comprehensive (for instance, q p q F1 (G)
p 6 F1 (G)).
Since solution satisfy comprehensiveness, according Zhang (2007),
logical version Kalai-Smorodinsky solution. However, mean
solution less intuitive. Although comprehensiveness common restriction belief
revision game theory, means desirable property bargaining solution.
example, q r involved conflict underlying bargaining
game. Thus reasonable players keep irrelevant demands. addition,
solution syntax-dependent. represent demand set X1 = {p q}
X2 = {p r}, solution (, ).
4.2 Fixed-Point Property
Besides generic properties, solution possesses another extraordinary logical property:
fixed-point property. consider bargaining negotiation mutual persuasion: one
persuades accept demands. outcome negotiation result
mutual belief revision (Zhang et al., 2004). case, negotiation outcome
satisfy following fixed-point property.
Theorem 2 bargaining game G = ((X1 , 1 ), (X2 , 2 )), X1 X2 logically
closed, bargaining solution F (G) satisfies following fixed-point condition:
F1 (G) + F2 (G) = (X1 1 F2 (G)) (X2 2 F1 (G))

(5)

prioritized revision operator player (see definition Section 2.4).
446

fiAn Ordinal Bargaining Solution Fixed-Point Property

Assume X1 X2 two belief sets (so logically closed), representing belief
states two agents. Mutual belief revision agents means agent takes
part agents beliefs revise belief set. instance, 1 subset X1
2 subset X2 , X1 1 2 revised belief set player 1 accepts
player 2s beliefs 2 X2 2 1 resulting belief set player 2 accepting
1 . interaction belief revision continue reaches fixed point
beliefs common, i.e., (X1 1 2 ) (X2 2 1 ), exactly beliefs agents
mutually accept, is, 1 + 2 . Note agent uses way revision
rebuilt belief state. view bargaining mutual belief revision, agreement
bargaining, i.e., F1 (G) + F2 (G), exactly common demands agents accept
other, i.e., (X1 1 F2 (G)) (X2 2 F1 (G)). words, solution F (G)
fixed-point respect game G. theorem shows true
demand sets game logically closed.
show theorem, need technical lemmas.
Lemma 3 bargaining game G = ((X1 , 1 ), (X2 , 2 )),
1. F1 (G) X1 1 F2 (G);
2. F2 (G) X2 2 F1 (G).
Proof: According definition prioritized base revision, X1 1 F2 (G) =

Cn(H F2 (G)). H X1 F2 (G), deal (D1 , D2 ) (G)

HX1 F2 (G)

D1 = H. extend pair (H, F2 (G)) deal (H, D2 )
F2 (G) D2 . hand, since 1 F2 (G) consistent, 1 H,
(1 , 2 ) core G. Thus, 1 D1 2 D2 . follows (D1 , D2 ) (G).
Since F1 (G) D1 , F1 (G) H. conclude F1 (G) X1 1 F2 (G).
proof second statement similar.

lemma have,
1. F1 (G) + F2 (G) X1 1 F2 (G);
2. F1 (G) + F2 (G) X2 2 F1 (G).
Note lemma require demand sets X1 X2 logically
closed. However, without assumption, following lemmas hold.
Lemma 4 Let (1 , 2 ) core game G = ((X1 , 1 ), (X2 , 2 )). X1 X2
logically closed,
1. X1 1 F2 (G) = X1 1 (2 + (X1 X2 ));
2. X2 2 F1 (G) = X2 2 (1 + (X1 X2 ))
Proof: present proof first statement. second one similar. Firstly,
prove F2 (G) 1 + 2 + (X1 X2 ). X1 X2 consistent, result obviously
true. Therefore assume X1 X2 inconsistent.
447

fiZhang & Zhang

Assume F2 (G). 6 1 +2 +(X1 X2 ), {}1 2 (X1 X2 )
G

G

+1

+1

consistent. According Lemma 2, X1 max X2 max (X1 X2 ) inconsistent.
Since language finite X1 X2 logically closed, sets X1 X2 ,
G +1
G +1
X1 max
X2 max
logically closed (the latter two due LC). Therefore
set finite axiomatization. Let sentence 0 axiomatize X1 X2 , 1 axiomatize
G +1
G +1
X1 max 2 axiomatize X2 max . Thus 0 1 2 inconsistent. Notice
0 1 X1 0 2 X2 . follows (0 1 ) X1 (0 2 ) X2 .
Since {} 1 2 (X1 X2 ) consistent, deal (D1 , D2 ) (G)
{(0 1 )}1 (X1 X2 ) D1 {(0 2 )}2 (X1 X2 ) D2 . know
F2 (G), D1 + D2 . Thus 0 1 2 D1 + D2 , contradicts fact
D1 + D2 consistent. Therefore, shown F2 (G) 1 + 2 + (X1 X2 ).
prove X1 1 F2 (G) = X1 1 (2 + (X1 X2 )). Lemma 3,
1 + 2 X1 1 F2 (G). follows X1 1 F2 (G) = (X1 1 F2 (G)) + (1 + 2 ).
Furthermore, yield X1 1 F2 (G) = (X1 1 F2 (G)) + (1 + 2 ) + (X1 X2 )
X1 X2 F2 (G). Since F2 (G) 1 + 2 + (X1 X2 ). According AGM postulates, (X1 1 F2 (G)) + (1 + 2 + (X1 X2 )) = X1 1 (1 + 2 + (X1 X2 )).
Therefore X1 2 F2 (G) = X1 1 (1 + 2 + (X1 X2 )). addition, easy
prove 1 X1 1 (2 + (X1 X2 )). AGM postulates again,
X1 1 (2 + (X1 X2 )) = (X1 1 (2 + (X1 X2 )) + 1 = X1 1 (1 + 2 + (X1 X2 )).
Therefore X1 1 F2 (G) = X1 1 (2 + (X1 X2 )).

following lemma complete proof Theorem 2.
Lemma 5 X1 X2 logically closed,
(X1 1 F2 (G)) (X2 2 F1 (G)) F1 (G) + F2 (G).
Proof: Let (1 , 2 ) core G. Let
1
max

01 = X1

2
max

02 = X2

2
1
= max{k : 1 X2k
max
= max{k : X1k 2 (X1 X2 ) consistent} max
(X1 X2 ) consistent}.

Note cases max
exist, simply assume equals
0
+. claim X1 1 F2 (G) = 1 + F2 (G) X2 2 F1 (G) = 02 + F1 (G). shall
provide proof first statement. second one similar.
Firstly, according Lemma 2, 1 01 . Secondly, Lemma 4, X1 1 F2 (G) =
X1 1 (2 +(X1 X2 )). Therefore show X1 1 F2 (G) = 01 +F2 (G), need prove
X1 1 (2 +(X1 X2 )) = 01 +2 +(X1 X2 ). 2 +(X1 X2 ) F2 (G),
F2 (G) 1 + 2 + (X1 X2 ) 1 01 . construction prioritized revision,
easily verify 01 + 2 + (X1 X2 ) X1 1 (2 + (X1 X2 )). Therefore
show direction, i.e., X1 1 (2 + (X1 X2 )) 01 + 2 + (X1 X2 ).
01 = X1 , X1 (2 + (X1 X2 )) consistent. follows X1 1 (2 +
(X1 X2 )) X1 + (2 + (X1 X2 )) = 01 + 2 + (X1 X2 ), desired. 01 6= X1 ,
1 +1
1
, X1 max 2 (X1 X2 ) inconsistent.
according definition max
1
max
+1

Therefore exists X1

2 + (X1 X2 ). assume
448

fiAn Ordinal Bargaining Solution Fixed-Point Property

X1 1 (2 + (X1 X2 )). 6 01 + 2 + (X1 X2 ), {} 01 2 (X1 X2 )
1 +1
consistent. { } 01 2 (X1 X2 ). Notice X1 max .
exists H X1 (2 + (X1 X2 )) { } 01 H. Since
X1 1 (2 + (X1 X2 )) H logically closed, H, contradicts
consistency H (2 +(X1 X2 )). Therefore X1 1 (2 +(X1 X2 )) 01 +2 +(X1 X2 ).
Finally prove claim lemma. Let (X1 1 F2 (G)) (X2 2 F1 (G)).
(01 + F2 (G)) (01 + F2 (G)). 01 + F2 (G), exists sentence 2 F2 (G) ` 2 2 01 . Similarly, exists sentence 1
F1 (G) ` 1 1 02 . turns 1 2 01 02 .
Thus 1 2 X1 X2 . However, X1 X2 F1 (G) + F2 (G). follows
1 2 F1 (G) + F2 (G). Note 1 2 F1 (G) + F2 (G). Therefore conclude
F1 (G) + F2 (G).

Theorem 2 establishes link bargaining theory belief revision. link
helps us understand reasoning process behind bargaining. even interesting
extend result general multiagent case. However, main challenge
multiple agents mutually revise beliefs.

5. Game-theoretic Properties Bargaining Solution
game theory, properties considered important bargaining solution
include individual rationality, Pareto optimality, ordinal invariance (or scale invariance),
symmetry contraction independence. bargaining model, bargainers preferences
represented total preorder, order-preserving transformation preferences
change order preferences. Therefore solution satisfies ordinal invariance
trivially. section, examine properties list
bargaining solution. presenting results, first introduce concepts
necessary game-theoretic analysis bargaining.
5.1 Strategies Utilities
Two concepts play essential roles game-theoretic analysis bargaining: strategy
utility. Given bargaining game G = ((X1 , 1 ), (X2 , 2 )), strategy profile game
pair (S1 , S2 ) S1 X1 S2 X2 . strategy profile interpreted
pair proposals demands players course bargaining.
say strategy profile = (S1 , S2 ) compatible
1. X1 X2 S1 X1 X2 S2
2. S1 S2 consistent
Obviously deal game compatible strategy profile. bargaining solution
F (G) also compatible strategy profile G.
consider gains player strategy profile. Assume
strategy profile (S1 , S2 ) leads agreement, player payoff utility defined as:

449

fiZhang & Zhang

(

ui (Si ) =

max{k : Xik Si },
min{k : Xik = Xi },

Si 6= Xi ;
otherwise.

words, ui (S) counts number top block demands covered Si .
Note payoff count individual demands. Specifically define utility
default disagreement point (, ) (0, 0).
5.2 Pareto Optimality
Based definition, easy see solution satisfies individual rationality (in sense game theory) game G, ui (Fi (G)) 0 = 1, 2.
consider Pareto efficiency.
Pareto optimality one important properties bargaining solution.
call compatible strategy profile (S1 , S2 ) game (strictly) Pareto optimal
exist compatible strategy profile (S10 , S20 ) game either u1 (S10 )
u1 (S1 ) & u2 (S20 ) > u2 (S2 ) u1 (S10 ) > u1 (S1 ) & u2 (S20 ) u2 (S2 ).
compatible strategy profile (S1 , S2 ) game weakly Pareto optimal
exist another compatible strategy profile (S10 , S20 ) game u1 (S10 ) > u1 (S1 )
u2 (S20 ) > u2 (S2 ).
Theorem 3 bargaining game G, F (G) weakly Pareto optimal.
Proof: Suppose compatible strategy profile (S1 , S2 ) G u1 (S1 ) >
G
G . Since
u1 (F1 (G)) u2 (S2 ) > u2 (F2 (G)). u1 (S1 ) > max
u2 (S2 ) > max
u1 (S1 )
u2 (S2 )
(S1 , S2 ) compatible, X1
X2
(X1 X2 ) consistent. turns
k
k
G , contradicts Lemma 2. Therefore
max{k : X1 X2 (X1 X2 ) consistent} > max
F (G) weakly Pareto optimal.

Obviously, solution F satisfy strict Pareto optimality. instance,
solution G0 Example 5 (the second part example) strictly Pareto optimal
(but weakly Pareto optimal). problem solution nature
problem domain consider. well known game theory feasible set
bargaining game convex, guarantee unique bargaining solution
strictly Pareto optimal (Kaneko, 1980; Mariotti, 1996). Therefore, non-convex domain,
need trade-off uniqueness solutions strict Pareto optimality (Conley
& Wilkie, 1991, 1996; Mariotti, 1998; Xu & Yoshihara, 2006). using similar approach
introduced Zhang (2007), map logically represented bargaining game
numerically represented bargaining game. mapping, feasible set
corresponds logically represented bargaining game non-convex unless demand
sets logical bargaining game consistent. Since solution unique solution,
cannot expect strictly Pareto optimal.
5.3 Restricted Symmetry
game theory, bargaining game symmetric feasible set invariant
permutation point feasible set (Nash, 1950). However, concept symmetry easy extended logic-based bargaining models bargaining
450

fiAn Ordinal Bargaining Solution Fixed-Point Property

problem represented physical items. Permutation deals make sense.
One may wonder judge fairness bargaining without concept symmetry.
point view, thing fair outcome negotiation. outcome
negotiation relies bargaining power party. bargainer higher
negotiation power receives gains negotiation. However, reasonable
assume negotiation based fair bargaining procedure negotiation
protocol. construction bargaining solution meant capture idea fair
negotiation protocols. approach use paper similar idea bargaining
agenda (ONeill et al., 2004). consider negotiation process consists several
rounds stages. round, parties reach agreements new issues
considered previous rounds. assume party always place
higher wanted demands earlier rounds. demands mutually
accepted earlier rounds remain agreements negotiation
later rounds. new agreement reached, negotiation procedure
stops. process, negotiation always terminates level priority
demands players.
Theorem 4 bargaining game G = ((X1 , 1 ), (X2 , 2 )), X1 X2 logically
closed, natural number n
F(G) = (X1 (X1n + X2n + (X1 X2 )), X2 (X1n + X2n + (X1 X2 )))
G . case X n + X n = + .
Proof: fact, prove n = max
1
2
2
1
Obviously X1 X2 consistent, result trivial. Therefore assume
X1 X2 inconsistent. prove case F1 (G) = X1 (1 + 2 + (X1 X2 )).
proof part similar.
(D1 , D2 ) (G), 1 D1 2 D2 . prove X1 (1 +2 +
(X1 X2 )) D1 . case, exists sentence X1 (1 + 2 + (X1 X2 ))
6 D1 . one hand, X1 (1 +2 +(X1 X2 )) implies D1 D2 `
1 + 2 + (X1 X2 ) D1 + D2 . hand, 6 D1 implies
{} D1 D2 inconsistent due maximality deals. follows D1 D2 ` .
Therefore D1 D2 inconsistent, contradiction. proved deal
(D1 , D2 ) (G), X1 (1 + 2 + (X1 X2 )) D1 . Thus X1 (1 + 2 + (X1 X2 ))

D1 = F1 (G). proof Lemma 4 shown direction inclu-

(D1 ,D2 )(G)

sion F1 (G) X1 (1 +2 +(X1 X2 )) holds. Therefore F1 (G) = X1 (1 +2 +(X1 X2 )).

theorem shows termination negotiation, party remain
G , including common demands
demands level, i.e., max
logical consequences. However, solution seemingly excludes low ranked irrelevant
items. due assumption logical closedness demand sets. fact,
assumption, items irrelevant two statements , Xi ,
always Xi . assume logical closedness
general.
Another question may arise player could gain negotiation power
puts negotiation items earlier stages agenda (effectively
451

fiZhang & Zhang

could gain negotiation end disagreement). fact, true
natural risk breakdown taken account. player places conflictive
item earlier stage agenda, negotiation would terminate sooner. Therefore
ordering demands part strategy bargainer. discuss issue
separate section (see Section 6)
5.4 Contraction Independence
Contraction Independence, called Independence Irrelevant Alternatives (IIA), requires
alternative judged best compromise problem,
still judged best subproblem contains (Thomson, 1994). logic-based
bargaining model, alternatives explicitly given. However, easily define
concept subproblem terms bargainers prioritized demand sets.
bargaining game G0 = ((X10 , 01 ), (X20 , 02 )) subgame G = ((X1 , 1 ), (X2 , 2 )),
denoted G0 v G, = 1, 2,
1. Xi0 Xi ,
2. 0i =i (Xi0 Xi0 ),
3. Xi , Xi0 , Xi0 .
words, Xi0 upper segment Xi respect Xi hierarchy.
Theorem 5 Let G0 v G. F (G) strategy profile G0 , F (G0 ) = F (G).
Proof: First, since F (G) strategy profile G0 , X1 X2 Xi0 (i = 1, 2).
G0
G .
According definition subgame, easy show max
max
0
0
G
G
condition F (G) strategy profile G again, max = max , means
G0 G share core (1 , 2 ). deal (D1 , D2 ) (G), obviously
(D1 X10 , D2 X20 ) deal G0 (D1 X10 , D2 X20 ) (G0 ). follows
F1 (G0 ) D1 F2 (G0 ) D2 . Therefore F1 (G0 ) F1 (G) F2 (G0 ) F2 (G).
hand, (D10 , D20 ) (G0 ), extend deal (D1 , D2 )
G (D1 , D2 ) (G) G0 G share core X1 X2 Xi0
(i = 1, 2). Since F (G) strategy profile G0 , F1 (G) D1 X10 F2 (G)
D2 X20 . follows F1 (G) D10 F2 (G) D20 , implies F1 (G) F1 (G0 )
F2 (G) F2 (G0 ). conclude F (G0 ) = F (G).

Note claim theorem weaker Nashs IIA because,
two bargaining games G G0 , alternatives G0 subset alternatives
G guarantee G0 subgame G. However, pointed many
authors, Nashs IIA original form longer plausible assumption domain
non-convex bargaining problems (Conley & Wilkie, 1996; Mariotti, 1998; Zhang, 2007).

6. Bargaining Power Risk Posture
shown Section 1, representing bargainers preferences ordinal
automatically solve problem ordinal bargaining (see Example 1).
452

fiAn Ordinal Bargaining Solution Fixed-Point Property

ordinal preference much less expressive power cardinal utility. unable express risk posture player, determines players bargaining power. successful
solution ordinal bargaining problem supply alternative mean express
bargainers attitude towards risk. section, illustrate two case studies
problem solved using framework.
6.1 Case Study I: Bargainers Attitude towards Risk
Let us revisit bargaining game Example 4, demand sets X1 = {p, r}
X2 = {q, r}. Consider following variations preferences, reflect difference
players attitude towards risk (note different cases lead different games).
G1 : X1 = {{p}, {r}} X2 = {{q}, {r}}10
G2 : X2 = {{p, r}, {}} X2 = {{q}, {r}}
G3 : X3 = {{p}, {r}} X2 = {{q, r}, {}}
G4 : X4 = {{p, r}} X2 = {{q, r}}
easy calculate solutions games:
F (G1 ) = ({p}, {q})
F (G2 ) = ({p, r}, {q})
F (G3 ) = ({p}, {q, r})
F (G4 ) = (, )
G1 , players risk-averse, players rank conflicting item r
lowest priority. means incentive reach agreement.
G2 , player 1 aggressive since conflicting item highly ranked. find
player 1 game case. surprising. general, risk-averse player
would gain disadvantage negotiation comparing risk-lover (see Roths book, 1979b,
p.35-60). G3 symmetrical G2 . G4 , players aggressive, therefore game
ends disagreement.
example, clear see bargainers attitude towards risk specified
model. risk-averse player would give demands likely conflict
demands player relatively lower priorities agreement likely
reached. contrast, risk-loving player would firmly entrench conflicting
demands. Notice logic plays crucial role representation. Simply expressing
bargainers demands physical terms without specifying relation sufficient
lead ordinal solution. crucial specify logical relations demands
players. example, contradiction demands r r play main
role determination solutions. main difference model
game-theoretic models.
6.2 Case Study II: Discretization Numerical Games
mentioned introduction section, risk posture represented non-linearity
utility functions Nashs model. subsection, use example cake division
(Example 1) show non-linearity preferences represented model.
10. demand sets represented prioritized partitions. corresponding preference orderings
p 1 r q 2 r, respectively.

453

fiZhang & Zhang

represent bargaining problem logical form, need discretize domain.
Let pn = {Player receives less n percentages cake player B gets
remain}, n natural number 0 100. addition, following
constraints acknowledged players:
C = {pn+1 pn : n = 0, 1, 2, , 100}
says player 1 receives less n + 1 percents cake, must
receive less n percents.
demands two players represented
XA = C {p0 , p1 , p1 , , p100 }
XB = C {p101 , p100 , p99 , , p1 }
Assume player arranges demands according linear scale share.
hierarch demand set is11 :
{C, {p0 , p5 }, {p6 , p10 }, , {p96 , , p100 }}
Player B arranges demands according square share. hierarch
demand set is12 :
{C, {p101 , , p79 }, {p78 , , p70 }, , {p6 , p5 , p4 }, {p3 , p2 }, {p1 }}
According setting, solution bargaining game is:
(C {p34 , p33 , , p0 }, C {p42 , p43 , , p101 })
easy calculate players agree division cake 34 xA < 42
58 < xB 66.13 Therefore solution gives similar prediction game-theoretic
solutions. indicates risk posture players embedded
model. fact, easily see player B aggressive ranks higher
conflicting items relatively higher player does. instance, player B ranks
equal share (50/50) 7th level player ranks 11th level.
seen example, ordering demands reflect
players gains demands represents players preference retaining abandoning demands. another significant difference bargaining model
game-theoretic models.
One may ask whether player could get advantages cheating sense
agent knows demands ranking party, agent adjust
demand hierarchy order obtain better outcome. Yes, possible. tell
opposite want (your demands) release ranking
demands. Otherwise, lose bargaining power. reason attitude
towards risk encoded ranking demands.
11. indicates player ignores small differences divisions. instance, may consider
share 0-5% means him. real negotiation, player may request 100%,
95%, 5% sequence giving 5% round bargaining.
12. Player B claims share sequence 100%, 98%, 95%, dropping demand scale
square.
13. Note communication players. Therefore player may give
needed (similar sealed-bid auction).

454

fiAn Ordinal Bargaining Solution Fixed-Point Property

7. Computational Complexity
section, study computational properties bargaining solution developed earlier. assume readers familiar complexity classes P,
NP, coNP, P2 P2 = coP2 . class DP contains languages L
P
L = L1 L2 L1 NP L2 coNP. Also class Pk+1 = P k contains
languages recognizable polynomial time deterministic Turing machine
Pk oracle. particular, class P2 [O(logn)] contains languages recognizable
deterministic Turing machine O(logn) calls NP oracle14 P3 contains languages recognisable deterministic Turing machine P2 oracle. well known
P N P DP P2 P2 P3 , inclusions generally believed
proper (readers may refer work Papadimitriou, 1994, details).
Consider bargaining game G = ((X1 , 1 ), (X2 , 2 )) X1 X2 finite.
mentioned Section 2.3, always write Xi = Xi1 Xim , Xik Xil =
k 6= l. Also k < m, formula Xik , exist
Xil (k < l) . Therefore, convenience analysis, rest
section, specify bargaining game G = (X1 , X2 ), X1 =
X2 =

n

j=1




i=1

X1i

X2j , X11 , , X1m , X21 , , X2n partitions X1 X2 respectively

satisfy properties mentioned above.
following four major decision problems important order understand
computational properties bargaining game model developed previous
sections: let G = (X1 , X2 ) bargaining game, would like decide: (1) whether
given pair (D1 , D1 ) Di Xi (i = 1, 2) deal G; (2) whether given pair
propositional formulas (1 , 2 ) core G respectively; (3) whether given formula
derivable core G; (4) whether given strategy profile G solution
G, First, following result deciding deal given bargaining game.
Theorem 6 Let G = (X1 , X2 ) bargaining game, D1 X1 D2 X2 . Deciding
whether (D1 , D2 ) deal G DP-complete.
Proof: Membership proof. According Definition 2, decide whether (D1 , D2 ) deal
G, D1 (or D2 ), need check: (1) k = 1, , (or k 0 = 1, , n resp.),
whether D2

k


(D1 X1j ) (or D1

j=1

k0


(D2 X2j ) resp.) consistent; (2) checking whether

j=1

X1 X2 Di (i = 1, 2); (3) D1 D2 maximal subsets X1 X2
Sk
respectively. (1), observe k, set j=1
(D1 X1j ) computed
polynomial time, checking consistency D2

k


(D1 X1j ) NP.

j=1

D2 case. obvious see (2) done polynomial time. consider (3).
order check whether D1 D2 maximal subsets X1 X2 respectively
satisfying condition, consider complement problem: assume D1 (we
also assume D2 ) maximal subset X1 satisfying required conditions,
14. Note literatures, different notions used denote complexity class P N P [logn] ,
P||N P P
2 .

455

fiZhang & Zhang

exists k (X1 \ D1 ) D2

k


((D1 {}) X1k )

j=1

consistent. Clearly, guess k, formula interpretation S, check
whether model D2

k


((D1 {}) X1k ). Obviously, NP. original

j=1

problem coNP.
Hardness proof. known given propositional formulas 1 2 , deciding whether 1 satisfiable 2 unsatisfiable DP-complete (Papadimitriou, 1994).
Given two propositional formulas 1 2 , construct polynomial time transformation 1 satisfiability 2 unsatisfiability deal decision problem
game. simply define game G = (X1 , X2 ) = ({2 1 p}, {q} {p}),
p, q propositional atoms occurring 1 2 . Note X1 X2 = . Let
D1 = X1 D2 = {q}. show (D1 , D2 ) deal G, is, D2
maximal subset X2 X1 D2 consistent, 1 satisfiable 2
unsatisfiable.
() Clearly, 1 satisfiable 2 unsatisfiable, X1 D2 consistent,
X1 X2 consistent. (D1 , D2 ) deal G.
() Suppose 1 2 unsatisfiable. X1 consistent. 1
2 satisfiable, observed X1 X2 consistent. (D1 , D2 ) deal
G. Finally, suppose 1 unsatisfiable 2 satisfiable. case, X1 X2 still
consistent. means, (D1 , D2 ) deal G either.

see definition, core bargaining plays essential role
construction bargaining solution. following theorem provides complexity
result decision problem.
Theorem 7 Let G bargaining game. Deciding whether given pair sets propositional
formulas (1 , 2 ) core G DP-complete.




Proof: Membership proof. Let G = (X1 , X2 ), X1 = X1i , X1 = X1j .
outline algorithm check whether (1 , 2 ) core G: (1) check whether
k, 1 =

k

i=1

X1i 2 =

k

i=1

X2i , 1 6=

k+1

i=1

X1i 2 6=

k+1

i=1

X2i ; (2) check whether

1 2 consistent; (3) check 1 X1k+1 2 X2k+1 consistent. Clearly, (1)
done polynomial time, checking (2) NP checking (3) coNP.
problem DP.
Hardness proof. hardness proof similar described proof
Theorem 7 variations. Given two propositional formulas 1 2 , reduce
decision problem 1 satisfiability 2 unsatisfiability problem. Let
G = (X1 , X2 ), X1 = {2 1 p} {q}, X2 = {q} {p}, p, q
propositional atoms occurring 1 2 . specify pair sets formulas:
(1 , 2 ) = ({2 1 p}, {q}).
show (1 , 2 ) core G 1 satisfiable 2
unsatisfiable. Suppose 1 satisfiable 2 unsatifiable. 1 = {1 p, q},
consistent. hand, 1 {q} 2 {p} = {1 p, q, p},
consistent. (1 , 2 ) core G.
456

fiAn Ordinal Bargaining Solution Fixed-Point Property

prove direction. (1) 1 2 satisfiable. 1 2 =
{(2 1 )(2 p), q}, consistent. However, also see 1 {q}2 {p}
leat one model satisfies 2 , q p. implies (1 , 2 ) longer
core G. (2) 1 2 unsatisfiable. case, 1 2 consistent
more. (1 , 2 ) core G. (3) 1 unsatisfiable 2 satisfiable.
Again, situation, 1 2 longer consistent, hence core
G. completes proof.

Recall intuition behind core final agreement maximizes fairly
agents demands without violating overall consistency. interesting
know whether certain information derivable agents demands
final agreement. Let = (1 , 2 ) core bargaining game G. define C(G) `
1 ` 2 ` .
Theorem 8 Given bargaining game G propositional formula . Deciding whether
C(G) ` P2 [O(logn)]-complete.
Proof: Membership proof. outline algorithm deciding C(G) ` follows: (1)
compute core (1 , 2 ) G, (2) checking 1 ` 2 ` . definition,
= (1 , 2 ) core G iff 1 =

k

j=1

X1j , 2 =

k

j=2

X1j , k maximal

number makes 1 2 consistent. Clearly, k determinated binay search
O(logn) NP oracle calls. checking 1 ` 2 ` done two NP
oracle calls. problem P2 [O(logn)].
Hardness proof. reduce P2 [O(logn)]-complete PARITY B
(Kobler, Schoning,
& Wagner, 1987; Wagner, 1988) problem. instance PARITY B
set
propositional formulas 1 , , n satisfiable, j i, j
satisfiable. problem decide whether number satisfiable formulas
odd. Without loss generality, assume n even number. construct
polynomial time bargaining game G = (X1 , X2 ) follows:


n/2

X1 = X1 = {2 p} {4 3 p} {n n1 p},
n/2
X2 = X2 = {q1 } {q2 } {qn/2 },
p, q1 , , qn/2 propositional atoms occurring 1 , , n . Let = p.
show C(G) ` p odd number satisfiable formulas 1 , , n .
First, suppose k odd number, 1 , , k satisfiable, k+1 , , n satisfiable. observed =

[k/2]

j=1

X1j X2j = {2 p, 4 3 p, , k+1

k p} {q1 , , q[k/2] } consistent, {k+3 k+2 p} {q[k/2]+1 } con[k/2]


sistent. (

j=1

X1j ,

[k/2]

j=1

X2j ) core G. Also, since k+1 satisfiable, follows

k+1 k p reduced k p, contained

[k/2]

j=1

X1j . C(G) ` p.

Second, assume even number satisfiable formulas 1 , , n . Let k
457

fiZhang & Zhang

even number 1 , , k satisfiable k+1 , , n satisfiable.
case, observed =

k/2

j=1

X1j X2j = {2 p, 4 3 p, , k

k1 p} {q1 , , qk/2 } consistent, {k+2 k+1 p} {qk/2+1 }
consistent. (

k/2


j=1
k/2




j=1

X1j ,

k/2

j=1

X2j ) core G. obvious p cannot derived

X1j . is, C(G) 6` p. completes proof.



Finally, consider decision problem solution given bargaining game.
following theorem gives complexity upper bound.
Theorem 9 Let G bargaining game,and (S1 , S2 ) strategy profile G. Deciding
whether (S1 , S2 ) solution G P3 .
Proof: Definition 3, need check whether S1 =

(D1 ,D2 )(G)


(D1 ,D2 )(G)

D1 S2 =

D2 , (G) = {(D1 , D2 ) (G) : 1 D1 , 2 D2 }, (1 , 2 )

core G. Note simplicity, use notion core represent
solution.

first consider complement deciding S1 =
D1 : checking whether
S1 6=


(D1 ,D2 )(G)



(D1 ,D2 )(G)

D1 . Clearly, S1 6=


(D1 ,D2 )(G)

(D1 ,D2 )(G)

D1 iff (1) S1 6


(D1 ,D2 )(G)

D1 ; (2)

D1 6 S1 . first guess pair sets propositional formulas (1 , 2 ),

check core G. According Theorem 8, know P2 . Clearly,

(1) holds iff exists formula (a) S1 , (b) 6
D1 .
(D1 ,D2 )(G)

Further, (b) holds iff exists deal (D1 , D2 ) 1 D1 , 2 D2 6 D1 .
guess (D1 , D2 ), check S1 , (D1 , D2 ) deal containing
core, 6 D1 . Thorem 7 know checking (D1 , D2 ) deal done
two NP oracle calls. task (1) solved P2 .

hand, (2) holds iff exists
D1
(D1 ,D2 )(G)


6 S1 . solve task (2), consider complement: , 6 S1 ,

6
D1 . Since |X1 | + |X2 | formulas need check, checking
(D1 ,D2 )(G)

S1 done linear time. 6 S1 , need

check 6
D1 , which, shown above, done P2 . Therefore,
(D1 ,D2 )(G)

S1 , |X1 | + |X2 | checkings whether 6
P

P 2 = P3 . follows deciding whether S1 =
Consequently, original problem also P3 .




(D1 ,D2 )(G)

(D1 ,D2 )(G)

D1 ,

D1 P3 .


proof Theorem 10, observed computation bargaining
game different Nebels prioritized belief revision.
458

fiAn Ordinal Bargaining Solution Fixed-Point Property

Theorem 10 Let G bargaining game (S1 , S2 ) strategy profile G. Deciding
whether (S1 , S2 ) solution G DP-hard.
Proof: consider special game G = (X1 , X2 ) X1 = X11 X12 , X2 = X21 (i.e.
X2 singleton partition), X11 X21 consistent X1 X2 consistent.
case, know (X11 , X21 ) core G. question reduced decide
whether S2 = X2 , S1 maximal subset X1 containing X11 S1 X2
consistent. proof Theorem 3, easy see DP-hard.

Obviously, gap lower bound upper bound solution
decision problem. also sheds light bargaining solution cannot represented
terms traditional belief revision operators.
Theorem 11 Let G bargaining game (S1 , S2 ) strategy profile G. Deciding
whether (S1 , S2 ) solution G P2 [O(logn)], given set deals (G)
provided.
Proof: decide whether (S1 , S2 ) solution G, need following: (1)
compute core (1 , 2 ) G; (2) compute (G) (G) (1 , 2 ); (3) compute


D10 =
D1 D20 =
D2 ; (4) compare whether S1 = D10
(D1 ,D2 )(G)

(D1 ,D2 )(G)

S2 = D20 . Task (1) solved one P2 [O(logn)] oracle call; (1 , 2 ), task
(2) done polynomial time; tasks (3) (4) done polynomial time.
problem P2 [O(logn)].


8. Related Work
investigation ordinal bargaining theory diverges two directions. first direction
focuses existence ordinal solution Nash bargaining model.
mentioned introduction section, Shapley, Kibris, Safra Samet shown
solution bargaining problems three agents more, satisfies
ordinal invariance, symmetry Pareto optimality (Shubik, 1982; Ozgur Kibris, 2004;
Safra & Samet, 2004). result interesting shows difference
bilateral bargaining multilateral bargaining. However, solve problem
ordinal bargaining solution constructive and, importantly,
alternative mean offered facilitate representation bargainers risk attitude. Calvo
Perers investigated bargaining problems mixed players: cardinal players
ordinal players (Calvo & Peters, 2005). bargaining solution called utility invariant
ordinally invariant ordinal players cardinally invariant cardinal
players. proved solution satisfying utility invariance, Pareto optimality
individual rationality provided least one player cardinal. Obviously, result
peripheral utility invariant solution necessarily ordinal invariant.
direction investigation tries circumvent Shapleys impossibility result
altering Nash bargaining model. Rubinstein et al. reinterpreted Nash bargaining
solution respect ordinal preferences (Rubinstein et al., 1992). restating Nashs
459

fiZhang & Zhang

axioms, proved redefined solution, referred ordinal-Nash solution, satisfies Pareto optimality, symmetry contraction independence (ordinal invariance holds
trivially). However, result based assumption preference ordering
player complete, transitive continuous set finite lotteries topological space. unclear use specific preference language describe
players risk attitudes. important, advantages ordinal preferences, ease
elicitation robustness corresponded solutions, may lost preference
ordering extended space lotteries. ONeill et al. model bargaining situation
family Nash bargaining games, parameterized time (ONeill et al., 2004).
bargaining solution defined function specifies outcome time.
model, bargainers risk attitude expressed varying preferences
time, intuitive. Since solution longer single point function
time, construction proposed ordinal solution relies solution sets
simultaneous differential equations.
work developed based series previous work authors. fixed-point
condition discussed paper firstly proposed Zhang et al. (2004)15 . Zhang
Zhang (2006a, 2006b) presented logical solution bilateral bargaining problem based
ordinal preferences. However, solution satisfy fixed point condition.
Zhang (2008) showed revision solution satisfies fixed-point condition.
present paper develops systemizes solution discusses logical
game-theoretic properties.

9. Conclusion
presented bargaining solution bilateral bargaining problem based
logical representation bargaining demands ordinal representation bargainers
preferences. shown solution satisfies desirable logical properties,
individual rationality (logical version), consistency collective rationality,
desirable game-theoretic properties, weak Pareto optimality, restricted symmetry
contraction invariance. ordinal invariance game-theoretic version individual
rationality hold trivially. Due discrete nature logical representation, solution
(strictly) Pareto optimal satisfy symmetry. However, demand sets
two players logically closed, solution meets restricted version symmetry.
addition, demonstrated logical closedness assumption, outcome
negotiation result mutual belief revision terms Nebels prioritized belief
base revision. result established link bargaining theory belief revision.
link would play important role future research multiagent belief
revision logic-based bargaining theory. complexity analysis indicates
computation bargaining solution difficult prioritized belief base revision.
satisfactory model bargaining able encode key factors determine bargaining outcome, bargainers demands, preferences, attitudes towards
15. Jin et al. (2007) also introduced fixed-point condition negotiation functions, says
certain conditions, negotiating outcome negotiation generates outcome.
Obviously bargaining solution satisfies fixed-point condition outcome bargaining
consistent, remains negotiation.

460

fiAn Ordinal Bargaining Solution Fixed-Point Property

risk on. Cardinal utility specifies two sorts information: preference possible agreements (via ordering utility values) risk attitudes (via non-linearity
utility function). However, second sort information, determines players
bargaining power, may lost ordinal transformation. Meanwhile, bargaining
model based purely ordinal information preferences automatically solve
problem bargainers risk attitude even inexpressible model. Therefore ordinal bargaining theory must supply facility describe information
ordinal preferences, including risk attitudes. paper, specify bargaining
situation logical structure. Bargainers demands, goals beliefs described logical
statements. conflicts demands two players identified
consistency checking. importantly, bargainers attitudes towards risk expressible
model natural way: risk-averse player tends give conflicting demand
relatively lower priority agreement could likely reached risk-lover
would firmly entrench demands less care whether demands contradict
opponents.
issues worth investigation. Firstly, shown solution
satisfies set logical properties game-theoretic properties. interesting know
whether axiomatic system exactly characterizes solution. main
challenge construction solution syntax-dependent. simply
impose logical closedness demand sets, lose desirable properties,
inclusion irrelevant items computational results. apply
assumption, shall need axioms specify way logical representation.
words, axioms specify demand set represented syntactically.
Secondly, present work offers solution bilateral bargaining situations.
supply model bargaining agents. Therefore current framework
deal issues like demand formed?, demand ranked
higher another? bargain effectively?. interesting logic
agency used developed model bargaining agents logic interacts
logic bargaining.
Finally, issues computational complexity proposed solution remain
unsolved. shown paper membership checking solution DPhard P3 . clear yet upper bound lower bound gap
closed. think new complexity proof technique may needed challenge.

Acknowledgments
authors wish thank Norman Foo, Michael Thielscher anonymous reviewers
comments. work partially supported Australian Research Council
project LP0883646.

References
Brewka, G. (1989). Preferred subtheories: extended logical framework default reasoning. Proceedings 11th International Joint Conference Artificial Intelligence
(IJCAI), pp. 10431048.
461

fiZhang & Zhang

Calvo, E., & Peters, H. (2005). Bargaining ordinal cardinal players. Games
Economic Behavior, 52, 2033.
Conley, J. P., & Wilkie, S. (1991). bargaining problem without convexity: extending
Egalitarian Kalai-Smorodinsky solutions. Economics Letters, 36 (4), 365369.
Conley, J. P., & Wilkie, S. (1996). extension Nash bargaining solution nonconvex problems. Games Economic Behavior, 13, 2638.
Gardenfors, P. (1992). Belief revision: introduction. Belief Revision, pp. 120. Cambridge University Press.
Gardenfors, P., & Makinson, D. (1988). Revisions knowledge systems using epistemic
entrenchment. Proceedings Second Conference Theoretical Aspect Reasoning Knowledge (TARK88), pp. 8396.
Jin, Y., Thielscher, M., & Zhang, D. (2007). Mutual belief revision: semantics computation. Proceedings 22nd AAAI Conference Artificial Intelligence (AAAI07), pp. 440445.
Kalai, E. (1977). Proportional solutions bargaining situations: interpersonal utility comparisons. Econometrica, 45 (7), 16231630.
Kalai, E., & Smorodinsky, M. (1975). solutions Nashs bargaining problem. Econometrica, 43 (3), 513518.
Kaneko, M. (1980). extension Nash bargaining problem Nash social
welfare function. Theory Decision, 12, 135148.
Kobler, J., Schoning, U., & Wagner, K. (1987). difference truth-table hierarchies
NP. RAIRO - Theoretical Informatics Applications, 21, 41937.
Kraus, S., Sycara, K., & Evenchik, A. (1998). Reaching agreements argumentation:
logical model implementation. Artificial Intelligence, 104, 169.
Mariotti, M. (1996). Non-optimal Nash bargaining solutions. Economics Letters, 52, 1520.
Mariotti, M. (1998). Extending Nashs axioms nonconvex problems. Games Economic Behavior, 22, 377383.
Meyer, T., Foo, N., Kwok, R., & Zhang, D. (2004). Logical foundations negotiation:
strategies preferences. Proceedings 9th International Conference
Principles Knowledge Representation Reasoning (KR04), pp. 311318.
Muthoo, A. (1999). Bargaining Theory Applications. Cambridge University Press.
Myerson, R. B. (1991). Game Theory: Analysis Conflict. Harvard University Press.
Nash, J. (1950). bargaining problem. Econometrica, 18 (2), 155162.
Nebel, B. (1992). Syntax-based approaches belief revision. Gardenfors (Ed.), Belief
Revision, pp. 5288. Cambridge University Press.
ONeill, B., Samet, D., Wiener, Z., & Winter, E. (2004). Bargaining agenda. Games
Economic Behavior, 48, 139153.
Osborne, M. J., & Rubinstein, A. (1990). Bargaining Markets. Academic Press.
462

fiAn Ordinal Bargaining Solution Fixed-Point Property

Ozgur Kibris (2004). Ordinal invariance multicoalitional bargaining. Games Economic Behavior, 46, 7687.
Papadimitriou, C. (1994). Computational Complexity. Addison Wesley.
Parsons, S., Sierra, C., & Jennings, N. R. (1998). Agents reason negotiate
arguing. Journal Logic Computation, 8 (3), 261292.
Perles, M. A., & Maschler, M. (1981). super-additive solution Nash bargaining
game. International Journal Game Theory, 10, 163193.
Poole, D. (1988). logical framework default reasoning. Artif. Intell., 36 (1), 2747.
Rosenschein, J. S., & Zlotkin, G. (1994). Rules Encounter: Designing Conventions
Automated Negotiation among Computers. MIT Press.
Roth, A. E. (1979a). Axiomatic Models Bargaining. Springer-Verlag.
Roth, A. E. (1979b). impossibility result concerning n-person bargaining games. International Journal Game Theory, 8, 129132.
Rubinstein, A., Safra, Z., & Thomson, W. (1992). interpretation Nash bargaining solution extension non-expected utility preferences. Econometrica,
60 (5), 11711186.
Safra, Z., & Samet, D. (2004). ordinal solution bargaining problems many
players. Games Economic Behavior, 46, 129142.
Sakovics, J. (2004). meaningful two-person bargaining solution based ordinal preferences. Economics Bulletin, pp. 16.
Sharpley, L. S. (1969). Utility comparison theory games. Guilbaud, G. T.
(Ed.), La Deecision, pp. 251263. Paris: Editions du Centre National de la Recherche
Scientifique.
Shubik, M. (1982). Game Theory Social Sciences: Concepts Solutions. MIT
Press, Cambridge.
Sycara, K. P. (1990). Persuasive argumentation negotiation. Theory Decision, 28,
203242.
Thomson, W. (1994). Cooperative models bargaining. Aumann, R., & Hart, S. (Eds.),
Handbook Game Theory, Vol. 2, chap. 35, pp. 12371284. Elsevier.
Wagner, K. (1988). Bounded query computations. Proceedings 3rd Conference
Structure Complexity Theory, pp. 419437.
Xu, Y., & Yoshihara, N. (2006). Alternative characterizations three bargaining solutions
nonconvex problems. Games Economic Behavior, 57 (1), 8692.
Zhang, D. (2005). logical model Nash bargaining solution. Proceedings 19th
International Joint Conference Artificial Intelligence (IJCAI-05), pp. 983988.
Zhang, D. (2007). Reasoning bargaining situations. Proceedings 22nd AAAI
Conference Artificial Intelligence (AAAI-07), pp. 154159.
Zhang, D. (2008). fixed-point property logic-based bargaining solution. AI 2008,
pp. 3041. Springer.
463

fiZhang & Zhang

Zhang, D., & Foo, N. (2001). Infinitary belief revision. Journal Philosophical Logic,
30 (6), 525570.
Zhang, D., Foo, N., Meyer, T., & Kwok, R. (2004). Negotiation mutual belief revision,.
Proceedings 19th National Conference Artificial Intelligence (AAAI-04),
pp. 317322.
Zhang, D., & Zhang, Y. (2006a). computational model logic-based negotiation.
Proceedings 21st National Conference Artificial Intelligence (AAAI-06), pp.
728733.
Zhang, D., & Zhang, Y. (2006b). Logical properties belief-revision-based bargaining
solution. AI 2006: Advances Artificial Intelligence (AI-06), pp. 7989.

464

fiJournal Artificial Intelligence Research 33 (2008)

Submitted 12/07; published 10/08

Similarities Inference Game Theory
Machine Learning
Iead Rezek

i.rezek@imperial.ac.uk

Department Clinical Neurosciences, Imperial College
London, SW7 2AZ, UK

David S. Leslie

david.leslie@bristol.ac.uk

Department Mathematics, University Bristol
Bristol, BS8 1TW, UK

Steven Reece
Stephen J. Roberts

reece@robots.ox.ac.uk
sjrob@robots.ox.ac.uk

Department Engineering Science, University Oxford
Oxford, OX1 3PJ, UK

Alex Rogers
Rajdeep K. Dash
Nicholas R. Jennings

acr@ecs.soton.ac.uk
rkd@ecs.soton.ac.uk
nrj@ecs.soton.ac.uk

School Electronics Computer Science, University Southampton
Southampton, SO17 1BJ, UK

Abstract

paper, elucidate equivalence inference game theory machine
learning. aim establish equivalent vocabulary two
domains facilitate developments intersection fields, proof
usefulness approach, use recent developments field make useful
improvements other. specifically, consider analogies smooth
best responses fictitious play Bayesian inference methods. Initially, use
insights develop demonstrate improved algorithm learning games based
probabilistic moderation. is, integrating distribution opponent
strategies (a Bayesian approach within machine learning) rather taking simple empirical average (the approach used standard fictitious play) derive novel moderated
fictitious play algorithm show likely standard fictitious play
converge payoff-dominant risk-dominated Nash equilibrium simple coordination game. Furthermore consider converse case, show insights game
theory used derive two improved mean field variational learning algorithms.
first show standard update rule mean field variational learning analogous
Cournot adjustment within game theory. analogy fictitious play,
suggest improved update rule, show results fictitious variational play,
improved mean field variational learning algorithm exhibits better convergence
highly strongly connected graphical models. Second, use recent advance fictitious
play, namely dynamic fictitious play, derive derivative action variational learning algorithm, exhibits superior convergence properties canonical machine learning
problem (clustering mixture distribution).
c
2008
AI Access Foundation. rights reserved.

fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

1. Introduction
recently increasing interest research intersection game theory
machine learning (Shoham, Powers, & Grenager, 2007; Greenwald & Littman, 2007).
work motivated observation whilst two fields traditionally
viewed disparate research areas, actually great deal commonality
exploited within fields. example, insights
machine learning literature graphical models led development efficient
algorithms calculating Nash equilibria large multi-player games (Kearns, Littman, &
Singh, 2001). Similarly, development boosting algorithms within machine learning
facilitated regarding engaged zero-sum game base
learner (Freund & Schapire, 1997; Demiriz, Bennett, & Shawe-Taylor, 2002).
interdisciplinary inspiration promising, unless clearer understanding principal connections exist two disciplines, examples
remain isolated pieces research. Thus, general goal work explore
formal way commonalities game theory machine learning. order
so, consider important problem central fields; making
inferences based previous observations.
first consider game theory, problem occurs context inferring
correct strategy play opponent within repeated game. generally
termed learning games common approach use algorithm based fictitious
play (see Fudenberg & Levine, 1999). Here, show insight Bayesian inference
(a standard machine learning technique) allow us derive improved fictitious play
algorithm. specifically, show integrating distribution opponent
strategies (a standard approach within machine learning), rather taking simple
empirical average (the approach used within standard fictitious play algorithm),
derive novel moderated fictitious play algorithm. Moreover, go demonstrate
algorithm likely standard fictitious play converge payoffdominant risk-dominated Nash equilibrium simple coordination game1 .
second part paper, consider mean field variational learning algorithm, popular means making inferences within machine learning.
show analogies game theory allows us suggest two improved variational learning
algorithms. first show standard update rule mean field variational learning
analogous Cournot adjustment process within game theory. analogy fictitious play, suggest improved update rule, leads improved mean field
variational learning algorithm, term fictitious variational play. appealing
game-theoretic arguments, prove convergence procedure (in contrast standard
mean-field updates suffer thrashing behaviour (Wolpert, Strauss, & Rajnarayan,
2006) similar Cournot process), show algorithm exhibits better convergence highly strongly connected graphical models. Second, show recent
advance fictitious play, namely dynamic fictitious play (Shamma & Arslan, 2005),
used derive novel derivative action variational learning algorithm. demon1. note form Bayesian learning games known converge equilibrium (Kalai
& Lehrer, 1993). However, work players perform Bayesian calculations space
repeated game strategies, resulting extremely complex inference problems. contrast, consider
Bayesian extension fictitious play, using insights machine learning aid myopic decision-making.

260

fiOn Similarities Inference Game Theory Machine Learning

strate properties algorithm canonical machine learning problem (clustering
mixture distribution), show exhibits superior convergence properties
compared standard algorithm.
taken together, results suggest much gained closer
examination intersection game theory machine learning. end,
paper, present range insights allow us derive improved learning algorithms
fields. such, suggest initial first steps herald possibility
significant gains area exploited future.
remainder paper organised follows. section 2 discuss work related
interplay learning games machine learning. Then, section 3 discuss
techniques within machine learning used relation learning games. review
standard stochastic fictitious play algorithm, go derive evaluate
moderated fictitious play algorithm. change focus, section 4, show
techniques within game theory apply machine learning algorithms. Again, initially
review standard mean field variational learning algorithm, then, analogy
fictitious play Cournot adjustment, present section 4.2 fictitious variational
play algorithm. section 4.3 continue theme incorporate insights dynamic
fictitious play derive evaluate derivative action variational learning algorithm.
Finally, conclude discuss future directions section 5.

2. Related Work
topics inference game theory traditionally viewed separate research
areas, consequently little previous research exploited common features
achieve profitable cross-fertilisation.
One area progress made use concepts game theory
find optimum multi-dimensional function. context, Lambert, Epelman,
Smith (2005) used fictitious play optimisation heuristic players represent
single variable act independently optimise global cost function. analysis
restricts attention class objective functions products independent
variables, thus rather limited practice.
similar vein, Wolpert co-authors consider independent players who,
actions, attempting maximise global cost function (Lee & Wolpert, 2004;
Wolpert, 2004). body work, however, optimisation carried
respect joint distributions variables chosen players. mean-field
approach taken, resulting independent choices player; approach
similar flavour presented Section 4 article. However, paper
explicitly use advances theory learning games develop improved optimisation
algorithms.
context improving game-theoretical algorithms using techniques machine
learning statistics, Fudenberg Levine (1999) show fictitious play interpretation Bayesian learning procedure. However interpretation shows fictitious
play type plug-in classifier (Ripley, 2000), stop short using full
power Bayesian techniques improve method. contrast, article take
fully Bayesian approach deciding optimal action play game.
261

fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

articles cross-over attempted, overlap greatly
current article, include Demiriz et al. (2002) Freund Schapire (1997),
interpreted boosting algorithms zero sum games, Kearns et al. (2001)
consider use techniques graphical models (Jordan, Ghahramani, Jaakkola, &
Saul, 1997) help calculate equilibria graphical games.

3. Fictitious Play
Fictitious play important model learning games source algorithmic
developments presented later work. begin presenting notation terminology used standard game-theoretic representation fictitious play. reader
referred work Fudenberg Levine (1999) extensive discussion.
consider strategic-form games players indexed {1, . . . , I},
use index players player i. Si denote finite set
pure strategies (also known actions) available player i, set S1 S2 SI
pure strategy profiles players, Si set pure strategy profiles
players i. players pay-off function denoted Ri : R maps pure
strategy profiles real line, i.e. set actions selected players associated
real number.
simple model usually extended allow players use mixed strategies
(Si ), (Si ) denotes set probability distributions pure strategy set
Si . Hence probability distribution discrete space Si . Writing =
( 1 , 2 , . . . , ) probability distribution product individual
mixed strategies, , extend reward functions space mixed strategies
setting
Ri () = E Ri (S)
(1)
E denotes expectation respect pure strategy profile selected
according distribution . Similarly
Ri (S , ) = Ei Ri (S , )

(2)

Ei denotes expectation respect Si .
standard solution concept game theory Nash equilibrium. mixed
strategy profile
Ri () Ri ( , ) (Si ).

(3)

words, Nash equilibrium set mixed strategies player
increase expected reward unilaterally changing strategy.
players receive identical reward known partnership
game. case, players acting independently trying optimise global
objective function. special case important since corresponds distributed
optimisation problem, objective function represents reward function
game. Nash equilibrium impossible improve expected value objective
function changing probability distribution single player. Thus Nash equilibria
correspond local optima objective function.
262

fiOn Similarities Inference Game Theory Machine Learning

Fictitious play proceeds assuming repeated play game, every player
monitors action opponent. players continually update estimates
opponents mixed strategies taking empirical average past action choices
players. Given estimate play, player selects best response (i.e. action
maximizes expected reward given beliefs). Thus, time t, estimates
updated according


1
1

(4)
bi (ti )
t+1 = 1
ti +
t+1
t+1
bi (ti ), best response players empirical mixed strategies, satisfies
bi (ti ) argmax Ri (S , ti ).

(5)

Si

certain classes games, including partnership games mentioned previously, beliefs evolve according equation 4 known converge Nash equilibrium.
hand, also exist games non-convergence equation 4
shown Fudenberg Levine.
3.1 Stochastic Fictitious Play
Now, one objection fictitious play discontinuity best response function, means players almost always play pure strategies, even beliefs
converged Nash equilibrium mixed strategies. overcome problems, fictitious
play generalized stochastic fictitious play (see Fudenberg & Levine, 1999)
employs smooth best response function, defined
( ) = argmax Ri ( , ) + v ( )

(6)

(Si )

temperature parameter v smooth, strictly differentiable concave
function approaches boundary (Si ) slope v becomes infinite.
One popular choice smoothing function v entropy function, results
logistic choice function noise temperature parameter


1
1


R (S , )
(7)
( )(S ) = exp
Z

partition function Z ensures best response adds unity. Thus,
stochastic fictitious play, players choose every round action randomly selected using
smooth best response current estimate opponents probability play.
estimates process also known converge several classes games,
including partnership games (Hofbauer & Hopkins, 2005) discussed
section 4. Several extensions fictitious play introduced attempts
extend classes games convergence achieved, including weakened
fictitious play (Leslie & Collins, 2005; van der Genugten, 2000) dynamic fictitious
play (Shamma & Arslan, 2005). use extensions second part
paper improve convergence modifications variational learning.
263

fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

3.2 Moderated Fictitious Play
fictitious play, player learns using empirical average past action
choices players estimate current mixed strategy. estimate
thought maximum likelihood estimate (MLE) opponents mixed strategy
time assumption actions player selected using
multinomial distribution parameter
ti = ti = argmax





P (Sui ; )

(8)

u=0


P (Sui ; ) modelled product multinomial distributions P (Sui ; ) =
QI
j

j
corresponds
j=1,j6=i (Su ). Fudenberg Levine note choice
maximum posteriori estimate opponent mixed strategies Bayesian model.
However, machine learning perspective, logistic choice best response function, given equation 7, may viewed single layer neural network sigmoid
activation function (Bishop, 2006). Substituting unknown parameter, ti , equation
7 maximum likelihood estimate (or Bayesian point estimate) fails take
account anything known parameters distribution; classifiers using approach called plug-in classifiers (Ripley, 2000). Bayesian perspective,
better predictions obtained integrating parameters distribution thus
computing posterior predictive best response function (Gelman, Carlin, Stern, & Rubin,
2000). process known neural network literature moderation (MacKay,
1992).
suggests modification fictitious play term moderated fictitious play.
this, every player uses posterior predictive best response, obtained integrating
opponent mixed strategies, weighted posterior distribution given
previously observed actions. conventional choose use posterior mean
point estimate, strategy chosen player
Z

(9)
) ti
ti = (ti )P (ti | S1:t

) posterior probability opponents mixed strategies ti given
P (ti | S1:t

play time 1 t.
observed history S1:t
Since model observed pure strategies player j observations multinomial
random variable parameters j , place uniform Dirichlet prior, Dir( j ; j0 ),
j , parameters j0k = 1. posterior distribution ti therefore
product independent Dirichlet distributions,


P (ti | si
Dir(tj ; jt )
(10)
1:t ; ) =
j6=i

P
jt (sj ) = 1 + tu=1 I{sju = sj }, indicator function.
multiple approaches estimating integral equation 9. generally
applicable approach sample N opponent mixed strategies,
n , posterior
264

fiOn Similarities Inference Game Theory Machine Learning

distribution use Monte Carlo approximation integral equation 9, given
ti

N
1 X
(n ).
N n=1

(11)

investigate effect moderation also consider analytic expression ti
makes use two approximations. first approximates distribution equation
10 normal distribution
N (; )
(12)
mean vector

=
/t

(13)



1 (1 1 ) . . .
1 K
1

..
=

.

K 1
. . . K (1 K )

(14)

covariance matrix

P
K = |S |
=

k tk (Bernardo & Smith, 1994). second, given
MacKay (1992) case two action choices, approximates integral sigmoid

1

=
g

1 + exp

respect normal distribution, P (a) = N (a; m, 2 ) mean variance 2 ,
modified sigmoid


Z
1

P (a) g
g
()m
(15)



() =



2
1+
8

21

.

(16)

see equations 15 16 effect moderation scale high rewards
proportion variance (and thus uncertainty estimated opponent mixed
strategy) shift probability value action closer 0.5 (i.e. unity
zero 0.5). onset play little known opponent, playing
actions equal probability intuitively reasonable course action.
test general moderated fictitious play equation 9 Dirichlet posterior
distributions, investigate games varying degrees risk dominance, since
cases equilibrium selection strategies strongly dependent upon players beliefs
players probabilities action. compared probability
moderated stochastic fictitious play converging payoff dominated solution
games payoffs described payoff matrix


(1, 1) (0, r)
R=
(17)
(r, 0) (10, 10)
265

fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

Probability Convergence Moderated Fictitious Play
Fictitious Play
Moderated Play

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
25

20

15

10

5

0

Risk Parameter (r)

Figure 1: Probability convergence (with 95% confidence intervals) fictitious moderated play payoff-dominant risk-dominated equilibrium game
represented payoff matrix shown equation 17.

factor r determined degree risk dominance action pair 1/1
set range r = 1, 2 25.
game, clearly best players choose action 2 (since
receive reward 10). However many learning processes (see Fudenberg & Levine, 1999
Young, 1998, example) players see initial actions become convinced
opponent playing strategy means choosing action 2 bad (since
penalty playing 2 1 high). find taking uncertainty strategies
account start, use moderated fictitious play algorithm,
less likely get stuck playing action 1, convergence strategy (10, 10)
likely.
value r ran 500 plays game measured convergence rates
moderated stochastic fictitious play using matching initial conditions. algorithms used smooth best response function. Specifically, Boltzmann smooth best
responses temperature parameter = 0.1. present results Figure 1. Stochastic fictitious play converges (1,1) equilibrium games action (2,2)
risk dominated (i.e. r < 10). soon action (2/2) longer risk dominated
stochastic fictitious play converge. contrast, moderated play exhibits much
smoother overall convergence characteristic. Moderated play achieves convergence action
(2/2) much greater range values r, though varying degrees probability.
Thus risk dominated games examined (r = 25 10), moderated play
likely converge payoff-dominant equilibrium stochastic fictitious play.
266

fiOn Similarities Inference Game Theory Machine Learning

Thus, using insight machine learning, specifically, standard procedure Bayesian inference integrating distribution opponents strategies,
rather taking empirical average, able derive algorithm based
stochastic fictitious play smoother thus predictable convergence behaviour.

4. Variational Learning
shown first part paper insights machine learning used
derive improved algorithms fictitious play, consider converse case.
specifically, consider popular machine learning algorithm, mean field variational
learning algorithm, show viewed learning game players
receive identical rewards. proceed show insights game theory (specifically,
relating variational leaning Cournot process fictitious play) used derive
improved algorithms.
start reviewing mean field variational learning algorithm, first note
methods typically used infer probability distribution latent
(or hidden) variables, based evidence provided another set observable variables.
Computing probability distributions, however, requires integration step
frequently intractable (MacKay, 2003). tackle problem one use Markov chain
Monte Carlo methods (Robert & Casella, 1999) obtain asymptotically optimal results,
alternatively use approximate analytical methods, variational approaches,
faster algorithms preferred. Among variational methods, mean field variational
approach distribution estimation (Jordan et al., 1997) applied real world
problems ranging bioinformatics (Husmeier, Dybowski, & Roberts, 2004) finite
element analysis (Liu, Besterfield, & Belytschko, 1988), games.
4.1 Mean Field Variational Method
typical starting point variational learning distributional form model, postulated underlie experimental data generating processes (i.e. generative model).
distribution usually instantiated observations, D, defined
set latent variables indexed = 1, I. denote domain
latent variable Si , element si Si . Note that, ease exposition later
text, re-use newly define intend make connection earlier
definition strategy profile. often desire marginal distribution pi (Si )
latent variable i, taken set marginal distributions (Si ) .
absence detailed knowledge dependence independence
variables, define joint distribution p (S) set distributions S,
= S1 S2 SI profile domain latent variables. mathematical
convenience resort logarithm density
(S | D, ) , log (p(S | D, ))

(18)

parameterise distribution p .
Due intractability integrating equation 18 respect S, variational
learning methods (Jordan et al., 1997) approach problem finding distribution,
267

fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

q (S), minimises criterion
Z
Z
F = q(s)(s | D, ) + H (q)

(19)

H() entropy function
H (q) =

Z



Z

q(s) log q(s) s.

(20)

equivalent minimising
D(q||p) =

Z



Z

q(s) log



q(s)
p(s | D, )



ds

(21)

highlights fact variational cost function described equation 19
Kullback-Leibler (KL) divergence marginal log-likelihood log (p(D))
negative free energy (Jordan et al., 1997).
Within variational learning, mean field approach makes assumption
latent variables independent. Thus, distribution profile, q, latent variables
simplifies


q (si )
(22)
q(s) ,
i=1

qi

(Si ).



basis mean field assumption, optimal distribution
variable one minimises KL divergence equation 21, assuming
variables adopt distribution q (Si ), obtained partial
differentiation equation 21. model-free update equation q , assumptions (Haft, Hofmann, & Tresp, 1999), takes general form
Z

Z
1
@i @i
@i
@i

q (s )(s , | D; )
q (s ) exp
(23)

index set @i denotes Markov blanket variable (i.e. set nodes
neighbouring node i). 2 player game set consist opponent,
graphical game Markov blanket consists players affecting player actions (Kearns
et al., 2001; Pearl, 1988).
variational algorithm thus proceeds iterating update equation 23
KL divergence expressed equation 21 converged stationary value possibly
local extremum. case EM algorithm, one round updates equation 23
interlaced one round updates parameter . update equations
obtained differentiation equation 19 respect .
game-theoretic perspective, log-probability shown equation 18, instantiated observations D, interpreted global reward function
r(S | ) log (p(S | D, ))

(24)

parameterised . latent variables become synonymous players,
player strategy space consisting possible values mixed strategy
268

fiOn Similarities Inference Game Theory Machine Learning

q . Interpreted terms players, task probabilistic inference partnership
game, played players jointly nature (Grunwald & Dawid, 2004). total
expected reward
Z
Z
L() q(s) log (p(s | D, ))
(25)
given set mixed strategies, simply expected log-likelihood. Bayesian
setting, priors , global reward full log-posterior distribution
equivalent total expected reward marginal log-likelihood, evidence. p factorises
often represented graphical model. Within game-theoretic interpretation, graphical models seen players own, local, reward functions,
graphical games (Kearns et al., 2001) active research area implicitly
makes use analogy game theory graphical models.
4.2 Fictitious Variational Play
variational mean field algorithm, described previous section, suggests
mixed strategies maximise equation 24 determined iterating equation 23
gradually taking limit 0. analogous Cournot adjustment process
game theory literature, modification smooth best responses used
place best responses2 . However, well known shortcoming Cournot process
iteration best response often fail converge and, instead, exhibit cyclic
behaviour. Consider, instance, partnership game reward matrix


1 0
0 1



.

players commence play choosing different actions, Cournot process fails
converge iterated best responses exhibit cyclic solutions.
cyclic behaviour indeed observed variational mean field algorithm.
Whilst commonly reported, cycles also occur highly connected graphical models
(e.g. Markov Random Fields Coupled Hidden Markov Models), probability distribution approximated really support imposed independence
assumption mean field approach (i.e. random variables strongly instead
weakly connected). Clearly, especially latter case, even random initialisation cannot
avoid mean field algorithms problem convergence. phenomenon described
thrashing Wolpert et al.s (2006).
Example: simple example illustrates point. Suppose two observations,
= {y1 , y2 } y1 = 10 y2 = 10. know observation drawn
two-component mixture one dimensional normal distributions, mixing probability
0.5 variances set 1. means normal distributions chosen
2. Cournot adjustment process, players use strategies best response action
opponents used previous period. many cases, process converge (Fudenberg &
Levine, 1999).

269

fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

set {10, 10}. Therefore,
(1 , 2 | D) = log(0.5((d1 1 ) + (d1 2 )))
+ log(0.5((d2 1 ) + (d2 2 )))
(yi j ) i, j = 1, 2 denotes density yi j standard normal
distribution. symmetry, clear lsame l(10, 10) = l(10, 10) =
log (0) + log (20) 201.8379 ldif f erent l(10, 10) = l(10, 10) = 2 log(0.5((0) +
(20))) 3.2242. game theory perspective, means partnership game
playing simply given 2 2 matrix


201.8379 3.2242
.
3.2242 201.8379
choose two q-distributions equation 23 component mean, initialise
q (i = 10) = 0.9 = 1 q (i = 10) component index = {1, 2} (i.e.
marginal distribution places weight 0.9 mean 10), update simultaneously, distributions switch virtually mass 10 point.
shouldnt surprise, given clearly need 1 6= 2 . problem
components mean field approximation jump time. next step,
reverse move happen. time things get extreme, continuous cycling
occurs.
work Wolpert et al. (2006) suggested thrashing avoided
adjusting distributions toward best responses, instead setting best
responses. Here, use analogy game theory rigorously develop approach,
prove convergence. start noting fictitious play exactly this;
modifies Cournot process adjusting players mixed strategies towards smooth best
response, instead setting smooth best response. suggests fictitious playlike modification standard mean field algorithm, best responses computed
time mixed current distributions:




(26)
qt+1
= (1 )qti +
F qt
wherePt sequence satisfying
Robins-Monro conditions
(Bishop, 2006)

Pusual

2 < ,
denotes
best response


=
,
lim

=
0,

q


t=1
t=1
MF
function distribution given equation 23. call process variational
fictitious play.
game theory literature proves stochastic fictitious play partnership games
guaranteed converge Si finite (Leslie & Collins, 2005). allows us prove
following result:
Theorem: random variable discrete, variational fictitious play converges.
Proof: case discrete random variables, equation 26 defines generalised weakened
fictitious play process game player receives reward log(p(s | D, )).
270

fiOn Similarities Inference Game Theory Machine Learning

Standard Mean Field Variational Algorithm

Si1





E

p(si = 1)

1
0.8
0.6
0.4
0.2
0
1



2

3

4

5

6

Iteration

7

8

9

10

9

10

Fictitious Variational Play

(
0.9
p(E |S i1 , ) =
0.1

i1 6=
i1 =

p(si = 1)

1
0.8
0.6
0.4
0.2
0
1

2

(a)

3

4

5

6

Iteration

7

8

(b)

Figure 2: Comparison performance standard mean field variational algorithm
improved algorithm applied exemplar binary hidden Markov
model.

process known converge (Leslie & Collins, 2005).
Remark: equivalent theorem yet available continuous random variables,
similar approach may yield suitable results combined recent work Hofbauer
Sorin (2006).
Example: example change fictitious update scheme standard
variational algorithm, consider binary hidden Markov model shown Figure 2a.
model contains two latent nodes, indexed 1, jointly parent
observed variable E . latent variables take values i1 , {0, 1}. model
specified observation best explained (with probability 0.9; see Figure 2b),
two neighbouring states take different values. Further, joint prior states 1
uniform distribution. simplicity omit parameter , encodes
observation state probabilities prior distribution.
Consider initialising distributions q 1 (s1 = 1) q 2 (s2 = 1) close
1. result periodic flipping state probability distributions, q 1
q 2 , every update iteration mean field algorithm (top graph Figure 2b).
contrast, fictitious variational play scheme gradually forces algorithm
converge (bottom graph Figure 2b).
271

fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

Initialisation random reduces likelihood mean field algorithms failure
converge. However, empirically, could still observe cyclic solutions 50%
random starts hidden Markov model training procedure. contrast,
modified mean field variational algorithm converges reliably cases.
4.3 Derivative Action Variational Algorithm
previous sections shown that, viewed game theoretic perspective, smooth best response function equivalent mean field update rule
and, consequently, able apply results game theory derive improved
variational learning algorithm. section go use equivalence
incorporate recent development fictitious play, specifically dynamic fictitious play,
variational mean field method.
dynamic fictitious play (Shamma & Arslan, 2005) standard best response expression, shown equation 7, extended include additional term






(27)
qti + q .
dt
| {z }
new
additional derivative term acts anticipatory component opponents play
expected increase speed convergence partnership game considered
here. Note that, convergence, derivative terms become zero fixed points
exactly standard variational algorithm.
Based correspondence best response function fictitious play
model-free update expression variational learning section 4.1, incorporate
additional differential term update equation variational learning algorithm
derive derivative action variational algorithm (DAVA) displays improved convergence
properties compared standard variational algorithm.
illustrate procedure, consider case applying derivative action
variational algorithm problem performing clustering (i.e. learning labelling
mixture distribution order best fit experimental data set). chose clustering
represents canonical problem, widely reported literature machine learning.
consider standard mixture distribution consisting K one-dimensional Gaussian
distributions, N (d|k , k ) k = 1, . . . , K, given
p(d|) =

K
X

k N (d|k , k )

(28)

k=1

set distribution parameters {1 , 1 , 1 , . , K , , K , K }. Here, k k
mean precision Gaussian, k weighting within mixture.
usual approach learning distribution (Redner & Walker, 1984) assume
existence indicator (or labelling) variable, indexed takes values
{1, K}, sample, di , formulate complete likelihood




p(S , 1 , . . . , K , 1 , . . . , K | )

K


k=1

272

(S =k)

k

N (di |k , k )(S

=k)

p()

(29)

fiOn Similarities Inference Game Theory Machine Learning

p() denote parameter prior distributions. obtain analytic coupled update
equations member parameter set, {1 , 1 , 1 , , K , K , K }, use
model discussed Uedaa Ghahramani (2002), describes appropriate choice
approximating marginal posterior distribution parameter. evaluating
integral equation 23, replacing generic latent variables model
parameters indicator variables S, Uedaa Ghahramani show closed
form expression derived approximating marginal posterior distributions
parameters. compute approximate marginal posterior mean distribution,
example, set variables @i equation 23 become place holders k
{k , , 1 , , N }, respectively. words, compute posterior distribution
k , logarithm equation 23 must averaged respect distributions q(k ),
q(), q(S ), = 1, N .
computations result closed form solution marginal posterior
element parameter set. Thus, posterior means, k , normal distribution
q k , N (k |mk , k )

(30)

mean mk precision k
k =
=


ck bk dk + 0 0 k

P



ik

ik = q (S = k)
P
dk =
k

k = ck bk k + 0

0 0 are, respectively, mean precision Gaussian prior k .
posterior precisions Gamma distribution
q k , (k |bk , ck )

(31)


1
k + 0
2
1P
bk =
(d k )2 )+ 12 k k1 +0
2 k
0 0 are, respectively, shape precision parameters Gamma
prior precisions k . Finally, posterior mixture weights K-dimensional
Dirichlet distribution
q , Dir(|)
(32)
ck =


=

P



ik +0

0 parameter Dirichlet prior . Finally, distribution component labels si form



K
bk ck
1
1 (S =k) 12

2


k exp
k
(d mk ) +
(33)
q (S ) =
ZS
2
k
k=1

273

fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

normalising constant ZS computed finite set states .
values k k computed using relations
k =(k ) (

PK

l=1

l )

k =(ck ) + log(bk ).
digamma function.
noted earlier, update equations interpreted best response
particular parameter given values others. Thus, additional derivative term
seen dynamic fictitious play can, principle, included variational update
equations 30, 31, 32, 33. desired, however, obtain closed form solutions
modified best response functions, update equations, derivative term
incorporated discrete distribution, given equation 33, follows.
notational clarity add anticipatory component estimate
means, k , use update sk . is, consider
k
q term equation 33. Furthermore, approximate derivative
inclusion dt
k


discrete
difference distributions iterations 1
q
dt
k
k
.
q qtk qt1
dt

(34)

also possible implement smoothed version derivative provide added
robustness random fluctuations, done equivalent fictitious play algorithm described Shamma Arslan (2005). introduce derivative term
equation 33 update equation component labels becomes




q (S )

K


k=1

1

(S =k) 2
k
k




2
1
bk ck

(1 + ) mkt +
exp
2
kt


2
bk ck
1

+
mkt1 +
2
kt1

(35)

parameters qtk denoted mkt kt . differential coefficient, ,
allows us control degree differential term contributes toward overall
update equation.
order demonstrate empirically convergence properties derivative action
variational inference algorithm, compared algorithm using either update equation 33
update equation 35 test data. control problem setting, generated synthetic
test data drawing data points mixture 3 Gaussian distributions
applying non-linear mapping function3 . data shown Figure 3 along
optimal clustering solution.
performed 200 runs comparing standard variational algorithm (implemented
described Penny & Roberts, 2002) derivative action variational algorithm.
run, algorithms started identically randomly initialised
3. Equation 35 generalises two dimensions higher replacing quadratic term (di mkt )2
inner product (d~i
~ kt )T (d~i
~ kt ). assumes vector valued data samples, d~i i, Gaussian
distributed means
~ k homoscedastic precision k .

274

fiOn Similarities Inference Game Theory Machine Learning

(1)
(3)

(2)

(a)

(b)

Figure 3: Synthetic data (a) used empirical study convergence properties
derivative action variational inference algorithm optimal clustering solution (b) showing centres three Gaussians mixture.

set model parameters iteration measured value Kullback-Leibler
divergence equation 21. algorithms terminated 60 iterations (chosen
well number iterations required achieve relative change Kullback-Leibler
divergence less 105 ). differential coefficient, , equation 35 set
constant value throughout. illustrative purposes chose following values
coefficient: = 0.5, 1.0, 1.5, 2.0.
first consider difference convergence rate standard variational
algorithm derivative action variational algorithm. Due nature algorithms, different initial conditions lead different final clustering solutions. addition,
difference update rule two algorithms means that, although start
algorithms identical initial conditions, necessarily result identical final
clustering solutions. Thus, analyse difference Kullback-Leibler divergence
iteration, runs fact produce identical clustering solutions
termination. Figure 4 compare algorithms case = 0.5.
clearly seen, DAVA converges everywhere quickly standard algorithm.
experiments also indicate choice differential coefficient, ,
significant effect convergence behaviour DAVA. seen Figure 5
case = 1.0. Compared results Figure 4, magnitude difference Kullback-Leibler divergence much larger, indicating substantial increase
convergence rate.
comparing times algorithm reached equilibrium (indicated
relative change Kullback-Leibler divergence less 105 ), four values ,
275

fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

Relative Performance Derivative
Action Variational Algorithm, =0.50
0.2

10th Percentile
50th Percentile
90th Percentile

Difference KL divergence

0.15

0.1

0.05

0

0.05
0

10

20

30

40

50

60

Iteration

Figure 4: comparison standard derivative action variational algorithm
( = 0.5), Kullback-Leibler divergence values, obtained algorithms
every iteration, compared. Shown estimates 10th ,
50th (the median) 90th percentiles differences standard
KL derivative action KL, i.e. KLstandard (t) KLDAV (t) iteration
t. positive value suggests current solution standard algorithm
worse derivative action algorithm. zero value implies
solutions found algorithms identical. initialisation KL
differences zero algorithms identical initialisation conditions.

role differential coefficient improving convergence rates becomes apparent.
particular, Table 1 shows relative convergence rate improvement shown derivative action variational algorithm, compared standard variational algorithm.
results indicate median improvement much 28% = 2.0.
addition, value differential coefficient increases, variance convergence
rate increases, thus, widening gap best worst performance
improvements.
However, increasing variance imply DAVA converging inferior
solutions. seen Figure 6 shows analysis quality solutions
reached algorithm, 200 runs (not algorithms converged
clustering solution). moderate values ( 1.5) derivative component
assist finding better solution. indicated proportion positive KL
divergence differences (in Figure 6). Positive values imply standard algorithm often
finds worse solutions compared DAVA particular setting . Increasing value
276

fiOn Similarities Inference Game Theory Machine Learning

Relative Performance Derivative
Action Variational Algorithm, =1.00
0.2

10th Percentile
50th Percentile
90th Percentile

Difference KL divergence

0.15

0.1

0.05

0

0.05
0

10

20

30

40

50

60

Iteration

Figure 5: Estimates 10th , 50th (the median) 90th percentiles differences
Kullback-Leibler divergence values, iteration, 200 runs
standard derivative action variational algorithm ( = 1.0).

differential coefficient increases variance clustering solutions
generated algorithm.
evaluated performance DAVA algorithm synthetic data set,
investigated performance range values derivative coefficient, ,
consider realistic experimental setting apply algorithm medical magnetic
resonance data (see Figure 7). data consists 100100 pixel region slice
tumour patients brain. Data collected using T2 proton density (PD) spin
sequences (shown Figure 7a), used directly form two-dimensional feature
space.
10 component Gaussian mixture model fitted data space, before,
use DAVA algorithm derived earlier learn label appropriate mixture
distribution. Following synthetic data experiments, DAVA derivative coefficient
set 1.0. best compromise speed robustness
algorithm. let algorithms run 100 iterations measured KL divergence
iteration monitor convergence. Figure 7b shows resulting segmentation
DAVA algorithm.
dataset, algorithm converged K = 5 classes; identical Markov Chain
Monte Carlo clustering reported work Roberts, Holmes, Denison (2001).
segmentation clearly shows spatial structure incorporated segmen277

fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

Difference final KL divergences Standard
Derivative Action Variational Algorithm
1
=0.5
=1.0
=1.5
=2.0

0.9
0.8

Probability

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

<0

0

>0

Sign final differences (StandardDerivative Action)

Figure 6: Relative difference Kullback-Leibler divergence values equilibrium
standard derivative action variational algorithms derivative coefficients: = 0.5, 1.0, 1.5, 2.0.


0.5
1.0
1.5
2.0

10th percentile
0%
0%
-2%
8%

50th percentile
9%
19 %
22%
28%

90th percentile
21 %
39 %
46 %
53 %

Table 1: Relative convergence rate improvement derivation action variational algorithm standard variational algorithms.

tation process priori, algorithm approximately 1.5 times faster
standard segmentation algorithm4 .
summary, adding derivative term variational learning algorithm,
produced algorithm that, empirical studies, shows improved convergence rates compared standard variational algorithm. Indeed, convergence rate improvement
achieved applying additional derivative response term mean components
mixture model parameters only, and, thus, believe improvement
possible parameters treated similarly.
4. determine factor speedup two algorithms averaged KL divergence values
every iteration segmentation algorithm. Based mean KL divergence curve
iteration number algorithm converged could calculated. iteration
relative change KL divergence 2 successive iterations less 104 . ratio
convergence points DAVA standard algorithm produced indicator speed.

278

fiOn Similarities Inference Game Theory Machine Learning

Slice T2 magnetic resonance image tumour patients brain.

(a)
proton density image corresponding T2 MR image.

(b)
Figure 7: results segmentation using DAVA algorithm = 1.0.
data segmentation obtained approximately half time took
standard segmentation algorithm.

5. Conclusions Future Work
work shown equivalence game-theoretical learning partnership games variational learning algorithm common machine learning.
comparison, probabilistic inference using mean field variational learning seen
Cournot adjustment process partnership game. Likewise, smooth best response
function used players plug-in single layer classifier sigmoid activation function.
exploiting analysis insights derived it, able show
insights one area may applied derive improved fictitious play
variational learning algorithms. empirical comparisons, algorithms showed
improved convergence properties compared standard counterparts.
279

fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

Player 1s Estimate Opponent Strategy
1
C


0.8
0.6
0.4
0.2
0
0

50

Iterations

100

150

Expected Rewards
5

Player 1
Player 2

4
3
2
1
0
0

50

Iterations

100

150

Figure 8: Using dynamic logistic regression (DLR) adaptive estimation opponent
mixed strategies. game repeated prisoners dilemma one player changes
strategies always co-operate always defect midway game.
Using DLR model learning, player adapts changes based
uncertainty predicted observed opponent strategies.

believe initial results particularly exciting. Thus, whilst still remains
analysis performed (specifically, would like prove convergence moderated fictitious play compare performance variational algorithms large real
world data sets), work clearly shows value studying intersection machine
learning game theory.
One conclusion work almost machine learning algorithm put
use model players learning other. Consider, instance adaptive classification algorithms capable adapting changes learners environment.
game-theoretic methods tuned toward achieving equilibrium estimating,
example, Nash mixed strategy profiles. desirable stationary environments,
algorithms fail adapt non-stationary environments, consequently little practical use dynamic real-world applications. research presented paper
strongly suggests adaptive classifiers might prove useful.
proof concept, implemented dynamic logistic regression (DLR),
presented Penny Roberts (1999), model play one player (player 1)
game repeated prisoners dilemma. player (player 2) set play always
co-operate strategy first 75 rounds play. second set 75 rounds, player 2
set play always defect strategy. task player 1 detect change
opponents behaviour compute best responses according updated estimate
player 2s strategy. estimates player 1 player 2s strategy entire game
shown Figure 8, together players expected rewards. DLR adaptively
280

fiOn Similarities Inference Game Theory Machine Learning

changes one-step ahead predicted opponent strategy basis uncertainty
results incorporation recently observed opponent action (see Penny
& Roberts, 1999 details; observations considered work map directly
observed actions made opponent). decision action play follows
usual (i.e. compute best response according updated estimate using
equation 9).
two things would like point out. First, implemented here, input
required DLR simply recently observed opponent action, decision made
DLR action drawn best response function. However, DLR also allows
vector inputs, consequence, players made respond
opponents actions, also context application specific variables. Second,
DLR estimation described Penny Roberts (1999) fully Bayesian. Missing data
naturally embedded Bayesian estimation demonstrated Lowne,
Haw, Roberts (2006). Mapping fact back use DLR model play
implies players keep track opponents behaviour without need follow
observe every move. Missing observations, instance, could result increased
uncertainty opponents predicted play and, within, reversal toward appropriate
best response (as might existed onset play).
Similar ideas using dynamic, instead static, estimators opponent strategy
recently presented work Smyrnakis Leslie (2008). Extending
use machine learning techniques allow dynamic estimation strategies
environmental parameters allow game theoretical learning become generally
applicable real-world scenarios.

Acknowledgments
authors would like thank reviewers, whose suggestions led significant improvements content clarity final paper. research undertaken
part ARGUS II DARP ALADDIN projects. ARGUS II DARP (Defence
Aerospace Research Partnership) collaborative project involving BAE SYSTEMS,
QinetiQ, Rolls-Royce, Oxford University Southampton University, funded
industrial partners together EPSRC, MoD DTI. ALADDIN (Autonomous
Learning Agents Decentralised Data Information Systems) jointly funded
BAE Systems EPSRC (Engineering Physical Science Research Council) strategic
partnership (EP/C548051/1).

References
Bernardo, J. M., & Smith, A. F. M. (1994). Bayesian Theory. John Wiley Sons.
Bishop, C. M. (2006). Pattern Recognition Machine Learning. Oxford University Press,
Oxford.
Demiriz, A., Bennett, K. P., & Shawe-Taylor, J. (2002). Linear programming boosting via
column generation. Machine Learning, 46 (13), 225254.
281

fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

Freund, Y., & Schapire, R. E. (1997). decision-theoretic generalization on-line learning
application boosting. Journal Computer System Sciences, 55 (1),
119139.
Fudenberg, D., & Levine, D. K. (1999). Theory Learning Games. MIT Press.
Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2000). Bayesian Data Analysis.
Chapman & Hall/CRC.
Greenwald, A., & Littman, M. L. (2007). Introduction special issue learning
computational game theory. Machine Learning, 67 (1-2), 36.
Grunwald, P. D., & Dawid, A. P. (2004). Game Theory, Maximum Entropy, Minimum
Discrepancy Robust Bayesian Decision Theory. Annals Statistics, 32, 1367
1433.
Haft, M., Hofmann, R., & Tresp, V. (1999). Model-Independent Mean Field Theory
Local Method Approximate Propagation Information. Computation Neural
Systems, 10, 93105.
Hofbauer, J., & Hopkins, E. (2005). Learning perturbed asymmetric games. Games
Economic Behavior, 52, 133157.
Hofbauer, J., & Sorin, S. (2006). Best response dynamics continuous zero-sum games.
Discrete Continuous Dynamical Systems, B6, 215224.
Husmeier, D., Dybowski, R., & Roberts, S. J. (Eds.). (2004). Probabilistic Modeling
Bioinformatics Medical Informatics. Advanced Information Knowledge Processing. Springer Verlag.
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1997). Introduction
Variational Methods Graphical Models. Jordan, M. I. (Ed.), Learning
Graphical Models. Kluwer Academic Press.
Kalai, E., & Lehrer, E. (1993). Rational Learning Leads Nash Equilibrium. Econometrica,
61 (5), 10191045.
Kearns, M., Littman, M. L., & Singh, S. (2001). Graphical Models Game Theory.
Proceedings Seventeenth Conference Uncertainty Artificial Intelligence,
pp. 253260.
Lambert, T., Epelman, M. A., & Smith, R. L. (2005). Fictitious Play Approach
Large-Scale Optimization. Operations Research, 53 (3), 477489.
Lee, C. F., & Wolpert, D. H. (2004). Product Distribution Theory Control Multi-Agent
Systems. AAMAS 04: Proceedings Third International Joint Conference
Autonomous Agents Multiagent Systems, pp. 522529, New York, USA.
Leslie, D. S., & Collins, E. J. (2005). Generalised weakened fictitious play. Games
Economic Behavior, 56 (2), 285298.
Liu, W., Besterfield, G., & Belytschko, T. (1988). Variational approach probabilistic
finite elements. Journal Engineering Mechanics, 114 (12), 21152133.
Lowne, D., Haw, C., & Roberts, S. (2006). adaptive, sparse-feedback EEG classifier
self-paced BCI. Proceedings Third International Workshop BrainComputer Interfaces, Graz, Austria.
282

fiOn Similarities Inference Game Theory Machine Learning

MacKay, D. J. C. (1992). Evidence Framework Applied Classification Networks.
Neural Computation, 4 (5), 720736.
MacKay, D. J. C. (2003). Information Theory, Inference, Learning Algorithms. Cambridge University Press.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann Publishers.
Penny, W., & Roberts, S. (1999). Dynamic logistic regression. Proceedings International Joint Conference Neural Networks (IJCNN99), Vol. 3, pp. 1562 1567.
Penny, W., & Roberts, S. (2002). Bayesian Multivariate Autoregressive Models Structured Priors. IEE Proceedings Vision, Signal Image Processing, 149 (1), 3341.
Redner, R. A., & Walker, H. F. (1984). Mixture Densities, Maximum Likelihood
EM Algorithm. SIAM Review, 26 (2), 195239.
Ripley, B. (2000). Pattern Recognition Neural Networks. Cambridge University Press.
Robert, C. P., & Casella, G. (1999). Monte Carlo Statistical Methods. Springer-Verlag: New
York.
Roberts, S., Holmes, C., & Denison, D. (2001). Minimum Entropy data partitioning using
Reversible Jump Markov Chain Monte Carlo. IEEE Transactions Pattern Analysis
Machine Intelligence, 23 (8), 909915.
Shamma, J. S., & Arslan, G. (2005). Dynamic Fictitious Play, Dynamic Gradient Play
Distributed Convergence Nash Equilibria. IEEE Transactions Automatic
Control, 50 (3), 312327.
Shoham, Y., Powers, R., & Grenager, T. (2007). multi-agent learning answer,
question. Artificial Intelligence, 171 (7), 365377.
Smyrnakis, M., & Leslie, D. (2008). Stochastic Fictitious Play using particle Filters
update beliefs opponents strategies. Proceedings First International
Workshop Optimisation Multi-Agent Systems (OPTMAS) Seventh International Conference Autonomous Agents Multiagent Systems (AAMAS 2008).
Uedaa, N., & Ghahramani, Z. (2002). Bayesian model search mixture models based
optimizing variational bounds. Neural Networks, 15, 12231241.
van der Genugten, B. (2000). Weakened Form Fictitious Play Two-Person Zero-Sum
Games. International Game Theory Review, 2 (4), 307328.
Wolpert, D. H. (2004). Information Theory - Bridge Connecting Bounded Rational
Game Theory Statistical Physics. arXiv.org:cond-mat/0402508.
Wolpert, D., Strauss, C., & Rajnarayan, D. (2006). Advances Distributed Optimization
Using Probability Collectives. Advances Complex Systems, 9 (4), 383436.
Young, H. P. (1998). Individual Strategy Social Structure: Evolutionary Theory
Institutions. Princeton University Press.

283

fiJournal Artificial Intelligence Research 33 (2008) 109-147

Submitted 11/07; published 9/08

Networks Influence Diagrams: Formalism
Representing Agents Beliefs Decision-Making Processes
Yaakov Gal

MIT Computer Science Artificial Intelligence Laboratory
Harvard School Engineering Applied Sciences

Avi Pfeffer

gal@csail.mit.edu

avi@eecs.harvard.edu

Harvard School Engineering Applied Sciences

Abstract
paper presents Networks Influence Diagrams (NID), compact, natural
highly expressive language reasoning agents beliefs decision-making processes. NIDs graphical structures agents mental models represented
nodes network; mental model agent may use descriptions mental
models agents. NIDs demonstrated examples, showing used
describe conflicting cyclic belief structures, certain forms bounded rationality. opponent modeling domain, NIDs able outperform computational
agents whose strategies known advance. NIDs equivalent representation
Bayesian games compact structured formalism. particular, equilibrium definition NIDs makes explicit distinction agents
optimal strategies, actually behave reality.

1. Introduction
recent years, decision theory game theory profound impact design
intelligent systems. Decision theory provides mathematical language single-agent
decision-making uncertainty, whereas game theory extends language multiagent case. fundamental level, approaches provide definition means
build intelligent agent, equating intelligence utility maximization. Meanwhile, graphical languages Bayesian networks (Pearl, 1988) received much
attention AI allow compact natural representation uncertainty
many domains exhibit structure. formalisms often lead significant savings
representation inference time (Dechter, 1999; Cowell, Lauritzen, & Spiegelhater,
2005).
Recently, wide variety representations algorithms augmented graphical
languages able represent reason agents decision-making processes.
single-agent case, influence diagrams (Howard & Matheson, 1984) able represent
solve agents decision making problem using principles decision theory.
representation extended multi-agent case, decision problems
solved within game-theoretic framework (Koller & Milch, 2001; Kearns, Littman, &
Singh, 2001).
focus AI far classical, normative approach decision
game theory. classical approach, game specifies actions available
agents, well utilities associated possible set agents
c
!2008
AI Access Foundation. rights reserved.

fiGal & Pfeffer

actions. game analyzed determine rational strategies agents.
Fundamental approach assumptions structure game, including
agents utilities actions, known agents, agents beliefs
game consistent correct, agents reason game
way, agents rational choose strategy
maximizes expected utility given beliefs.
systems involving multiple, autonomous agents become ubiquitous, increasingly deployed open environments comprising human decision makers computer
agents designed represent different individuals organizations. Examples
systems include on-line auctions, patient care-delivery systems (MacKie-Mason,
Osepayshivili, Reeves, & Wellman, 2004; Arunachalam & Sadeh, 2005). settings
challenging assumptions made decision-making strategies
participants open environments. Agents may uncertain structure
game beliefs agents structure game; may use
heuristics make decisions may deviate optimal strategies (Camerer,
2003; Gal & Pfeffer, 2003b; Rajarshi, Hanson, Kephart, & Tesauro, 2001).
succeed environments, agents need make clear distinction
decision-making models, models others may using make decisions,
extent agents deviate models actually make decisions.
paper contributes language, called Networks Influence Diagrams (NID), makes
explicit different mental models agents may use make decisions. NIDs provide
clear compact representation reason agents beliefs
decision-making processes. allows multiple possible mental models deliberation
agents, uncertainty models agents using. recursive,
mental model agent may contain models mental models
agents, associated uncertainty. addition, NIDs allow agents beliefs form cyclic
structures, form, believe believe believe,..., cycle
explicitly represented language. NIDs also describe agents conflicting beliefs
other. example, one describe scenario two agents disagree
beliefs behavior third agent.
NIDs graphical language whose building blocks Multi Agent Influence Diagrams
(MAID) (Koller & Milch, 2001). mental model NID represented MAID,
models connected (possibly cyclic) graph. NID converted
equivalent MAID represent subjective beliefs agent game.
provide equilibrium definition NIDs combines normative aspects
decision-making (what agents do) descriptive aspects decision-making
(what agents expected do). equilibrium makes explicit distinction
two types strategies: Optimal strategies represent agents best course action given
beliefs others. Descriptive strategies represent agents may deviate
optimal strategy. classical approach game theory, normative aspect (what
agents do) descriptive aspect (what analysts agents expect
do), coincided. Identification two aspects makes sense agent
better optimize decisions relative model world. However,
open environments, important consider possibility agent deviating
rational strategy respect model.
110

fiNetworks Influence Diagrams

NIDs share relationship Bayesian game formalism, commonly used model
uncertainty agents payoffs economics (Harsanyi, 1967). formalism,
type possible payoff function agent may using. Although NIDs
representationally equivalent Bayesian games, argue compact,
succinct natural representation. Bayesian game converted NID
linear time. NID converted Bayesian game, size Bayesian
game may exponential size NID.
paper revised expanded version previous work (Gal & Pfeffer, 2003a,
2003b, 2004), organized follows: Section 2 presents syntax NID language,
shows build MAIDs order express structure holds
agents beliefs. Section 3 presents semantics NIDs terms MAIDs, provides
equilibrium definition NIDs. Section 4 provides series examples illustrating
representational benefits NIDs. shows agents construct belief hierarchies
others decision-making order represent agents conflicting incorrect belief
structures, cyclic belief structures opponent modeling. also shows certain forms
bounded rationality modeled making distinction agents models
deliberation way behave reality. Section 5 demonstrates NIDs model
believe believe type reasoning practice. describes NID able
outperform top programs submitted competition automatic rockpaper-scissors players, whose strategy known advance. Section 6 compares NIDs
several existing formalisms describing uncertainty decision-making processes.
provides linear time algorithm converting Bayesian games NIDs. Finally, Section 7
concludes presents future work.

2. NID Syntax
building blocks NIDs Bayesian networks (Pearl, 1988), Multi Agent Influence
Diagrams (Koller & Milch, 2001). Bayesian network directed acyclic graph
node represents random variable. edge two nodes X1 X2 implies
X1 direct influence value X2 . Let Pa(Xi ) represent set parent
nodes Xi network. node Xi contains conditional probability distribution
(CPD) domain value parents, denoted P (Xi | Pa(Xi )). topology
network describes conditional independence relationships hold domain
every node network conditionally independent non-descendants given
parent nodes. Bayesian network defines complete joint probability distribution
random variables decomposed product conditional probabilities
node given parent nodes. Formally,
P (X1 , . . . , Xn ) =

n
!
i=1

P (Xi | Pa(Xi ))

illustrate Bayesian networks following example.
Example 2.1. Consider two baseball team managers Alice Bob whose teams playing late innings game. Alice, whose team hitting, attempt advance runner
instructing steal base next pitch delivered. successful
111

fiGal & Pfeffer

steal result benefit hitting team loss pitching team, may
result runner thrown out, incurring large cost hitting team
benefit pitching team. Bob, whose team pitching, instruct team throw
pitch out, thereby increasing probability stealing runner thrown out.
However, throwing pitch incurs cost pitching team. decisions whether
steal pitch taken simultaneously team managers. Suppose
game tied, either Alices Bobs team leading score, identity
leading team known Alice Bob make decision.
Suppose Alice Bob using pre-specified strategies make decisions
described follows: Alice leading, instructs steal probability 0.75,
Bob calls pitch probability 0.90; Alice leading, instructs
steal probability 0.65, Bob calls pitch probability 0.50.
six random variables domain: Steal PitchOut represent decisions Alice
Bob; ThrownOut represents whether runner thrown out; Leader represents
identity leading team; Alice Bob represent utility functions Alice Bob.
Figure 1 shows Bayesian network scenario.
Leader

Steal

PitchOut
ThrownOut

Alice

Bob

Figure 1: Bayesian network Baseball Scenario (Example 2.1)
CPD associated node network represents probability distribution domain value parents. CPDs nodes Leader, Steal,
PitchOut, ThrownOut Bayesian network shown Table 1. example,
CPD ThrownOut, shown Table 1d, represents conditional probability distribution
P (ThrownOut | Steal, PitchOut). According CPD, Alice instructs runner
steal base 80% chance get thrown Bob calls pitch 60%
chance get thrown Bob remains idle. nodes Alice Bob deterministic CPDs, assigning utility agent joint value parent nodes Leader,
Steal, PitchOut ThrownOut. utility Alice shown Table 2. utility
Bob symmetric assigns negative value assigned Alices utility
value parent nodes. example, Alice leading, instructs runner
steal base, Bob instructs pitch out, runner thrown out, Alice incurs
utility 60, Bob incurs utility 60.1
1. Note Alice instruct steal base, runner cannot thrown out, utility
agents defined case.

112

fiNetworks Influence Diagrams

Leader
alice bob none
0.4
0.3
0.3
(a) node Leader

Leader
alice
bob

PitchOut
true f alse
0.90 0.10
0.50 0.50

(c) node PitchOut

Leader
alice
bob

Steal
true f alse
0.75 0.25
0.65 0.35

(b) node Steal

Steal
true
true
f alse
f alse

PitchOut
true
f alse
true
f alse

ThrownOut
true f alse
0.8
0.2
0.6
0.4
0
1
0
1

(d) node ThrownOut

Table 1: Conditional Probability Tables (CPDs) Bayesian network Baseball
Scenario (Example 2.1)

Leader
alice
alice
alice
alice
alice
alice
alice
alice
bob
bob
bob
bob
bob
bob
bob
bob

Steal
true
true
true
true
f alse
f alse
f alse
f alse
true
true
true
true
f alse
f alse
f alse
f alse

PitchOut
true
true
f alse
f alse
true
true
f alse
f alse
true
true
f alse
f alse
true
true
f alse
f alse

ThrownOut
true
f alse
true
f alse
true
f alse
true
f alse
true
f alse
true
f alse
true
f alse
true
f alse

Alice
60
110
80
110

10

0
90
110
100
110

20

0

Table 2: Alices utility (Example 2.1) (Bobs utility symmetric, assigns negative value
Alices value).

113

fiGal & Pfeffer

2.1 Multi-agent Influence Diagrams
Bayesian networks used specify agents play specific strategies,
capture fact agents free choose strategies, cannot
analyzed compute optimal strategies agents. Multi-agent Influence Diagrams
(MAID), address issues extending Bayesian networks strategic situations,
agents must choose values decisions maximize utilities, contingent
fact agents choosing values decisions maximize
utilities. MAID consists directed graph three types nodes: Chance
nodes, drawn ovals, represent choices nature, Bayesian networks. Decision nodes,
drawn rectangles, represent choices made agents. Utility nodes, drawn diamonds,
represent agents utility functions. decision utility node MAID associated
particular agent. two kinds edges MAID: Edges leading chance
utility nodes represent probabilistic dependence, manner edges
Bayesian network. Edges leading decision nodes represent information available
agents time decision made. domain decision node represents
choices available agent making decision. parents decision
nodes called informational parents. total ordering agents decisions,
earlier decisions informational parents always informational parents
later decisions. assumption known perfect recall forgetting. CPD
chance node specifies probability distribution domain value
parent nodes, Bayesian networks. CPD utility node represents deterministic
function assigns probability 1 utility incurred agent value
parent nodes.
MAID, strategy decision node Di maps value informational parents,
denoted pai , choice Di . Let Ci domain Di . choice decision
value Ci . pure strategy Di maps value informational
parents action ci Ci . mixed strategy Di maps value informational
parents distribution Ci . Agent free choose mixed strategy Di
makes decision. strategy profile set decisions MAID consists
strategies specifying complete plan action decisions set.
MAID Example 2.1 shown Figure 2. decision nodes Steal PitchOut
represent Alices Bobs decisions, nodes Alice Bob represent utilities.
CPDs chance node Leader ThrownOut described Tables 1a
1d.
MAID definition specify strategies decisions. need computed assigned process. strategy exists decision, relevant
decision node MAID converted chance node follows strategy.
chance node domain parent nodes domain informational parents decision node MAID. CPD chance node
equal strategy decision. say chance node Bayesian
network implements strategy MAID. Bayesian network represents complete
strategy profile MAID strategy decision MAID implemented
relevant chance node Bayesian network. say Bayesian network
implements strategy profile. Let represent strategy profile implements
114

fiNetworks Influence Diagrams

Leader

PitchOut

Steal
ThrownOut

Alice

Bob

Figure 2: MAID Baseball Scenario (Example 2.1)
decisions MAID. distribution defined Bayesian network denoted
P .
agents utility function specified aggregate individual utilities;
sum utilities incurred agent utility nodes associated
agent.
Definition 2.2. Let E set observed nodes MAID representing evidence
available let strategy profile decisions. Let U() set
utility nodes belonging . expected utility given E defined
"
"
"
U ( | E) =
E [U | E] =
P (u | E) u
U U()

U U() uDom(U )

Solving MAID requires computing optimal strategy profile decisions,
specified Nash equilibrium MAID, defined follows.
Definition 2.3. strategy profile decisions MAID Nash equilibrium
strategy component decision Di belonging agent MAID one
maximizes utility achieved agent, given strategy decisions
.
argmax U $i ,i % ()

(1)

Si

equilibrium strategies specify agent decision given
available information decision. MAID contains several sequential decisions,
no-forgetting assumption implies decisions taken sequentially
agent, previous decisions available observations agent reasons
future decisions.
MAID least one Nash equilibrium. Exact approximate algorithms
proposed solving MAIDs efficiently, way utilizes structure
network (Koller & Milch, 2001; Vickrey & Koller, 2002; Koller, Meggido, & von Stengel,
115

fiGal & Pfeffer

1996; Blum, Shelton, & Koller, 2006). Exact algorithms solving MAIDs decompose
MAID graph subsets interrelated sub-games, proceed find set
equilibria sub-games together constitute global equilibrium entire
game. case multiple Nash equilibria, algorithms select one
them, arbitrarily. MAID Figure 2 single Nash equilibrium,
obtain solving MAID: Alice leading, instructs runner steal base
probability 0.2, remain idle probability 0.8, Bob calls pitch
probability 0.3, remains idle probability 0.7. Bob leading, Alice instructs
steal probability 0.8, Bob calls pitch probability 0.5.
Bayesian network implements Nash equilibrium strategy profile
MAID queried predict likelihood interesting events. example,
query network Figure 2 find probability stealer get thrown
out, given agents strategies follow Nash equilibrium strategy profile, 0.57.
MAID converted extensive form game decision tree
vertex associated particular agent nature. Splits tree represent
assignment values chance decision nodes MAID; leaves tree
represent end decision-making process, labeled utilities incurred
agents given decisions chance node values instantiated along
edges path leading leaf. Agents imperfect information regarding actions
others represented set vertices cannot tell apart make
particular decision. set referred information set. Let decision
MAID belonging agent . one-to-one correspondence values
informational parents MAID information sets vertices
representing move decision D.
2.2 Networks Influence Diagrams
motivate NIDs, consider following extension Example 2.1.
Example 2.4. Suppose experts influence whether team
steal pitch out. social pressure managers follow advice
experts, managers decision turns wrong assign blame
experts. experts suggest Alice call steal, Bob call pitch
out. advice common knowledge managers. Bob may uncertain
whether Alice fact follow experts steal, whether ignore
play best-response respect beliefs Bob. quantify, Bob believes
probability 0.7, Alice follow experts, probability 0.3, Alice play
best-response. Alices beliefs Bob symmetric Bobs beliefs Alice:
probability 0.7 Alice believes Bob follow experts call pitch out,
probability 0.3 Alice believes Bob play best-response strategy respect
beliefs Alice. probability distribution variables example
remains shown Table 1.
NIDs build top MAIDs explicitly represent structure. Network Influence Diagrams (NID) directed, possibly cyclic graph, node MAID.
avoid confusion internal nodes MAID, call nodes NID
blocks. Let decision belonging agent block K, let agent. (In
116

fiNetworks Influence Diagrams

particular, may agent itself.) introduce new type node, denoted Mod[, D]
values range block L NID. Mod[, D] takes value L,
say agent block K modeling agent using block L make decision D.
means believes may using strategy computed block L make
decision D. duration paper, refer node Mod[, D] Mod
node agent decision clear context.
Mod node chance node like other; may influence, influenced
nodes K. required parent decision
informational parent decision. agents strategy
specify value Mod node. Every decision Mod[, D]
node agent makes decision block K, including agent owns
decision. CPD Mod[, D] assigns positive probability block L,
require exists block L either decision node chance node.
chance node L, means believes agent playing like automaton
L, using fixed, possibly mixed strategy D; decision node L, means
believes analyzing block L determine course action D. presentation
purposes, also add edge K L NID, labeled {, D}.
Leader

Mod[Alice, Steal]

Mod[Bob, PitchOut]

Mod[Bob, Steal]

Mod[Alice, PitchOut]

Steal

ThrownOut

PitchOut

Alice

Bob

(a) Top-level Block

Leader

Leader

Top-level

Steal

Bob,STEAL Alice,PITCHOUT

PitchOut


(b) block

(c) block P

P

(d) Baseball NID

Figure 3: Baseball Scenario (Example 2.1)
represent Example 2.4 NID described Figure 3. three blocks
NID. Top-level block, shown Figure 3a, corresponds interaction
Alice Bob free choose whether steal base call pitch out,
respectively. block identical MAID Figure 2, except decision node
includes Mod nodes agents. Block S, presented Figure 3b, corresponds
situation Alice follows expert recommendation instructs player steal.
117

fiGal & Pfeffer

Mod[Bob, Steal]
Top-level

0.3
0.7
(a) node
Mod[Bob, Steal]

Mod[Alice, PitchOut]
Top-level
P
0.3
0.7
(b) node
Mod[Alice, PitchOut]

Mod[Bob, PitchOut]
Top-level
1
(c) node
Mod[Bob, PitchOut]

Mod[Alice, Steal]
Top-level
1
(d) node
Mod[Alice, Steal]

Table 3: CPDs Top-level block NID Baseball Scenario (Example 2.1)

block, Steal decision replaced chance node, assigns probability
1 true value informational parent Leader. Similarly, block P, presented
Figure 3c, corresponds situation Bob instructs team pitch out.
block, PitchOut decision replaced chance node, assigns probability 1
true value informational parent Leader.
root NID Top-level block, example corresponds reality.
Mod nodes Top-level block capture agents beliefs decision-making
processes. node Mod[Bob, Steal] represents Bobs belief block Alice
using make decision Steal. CPD assigns probability 0.3 Top-level block,
0.7 block S. Similarly, node Mod[Alice, PitchOut] represents Alices beliefs
block Bob using make decision PitchOut. CPD assigns probability 0.3
Top-level block, 0.7 block P. shown Table 3.
important aspect NIDs allow agents express uncertainty
block using make decisions. node Mod[Alice, Steal]
Top-level block represents Alices beliefs block Alice using
make decision Steal. example, CPD node assigns probability 1
Top-level. Similarly, node Mod[Bob, PitchOut] represents Bobs beliefs
block using make decision PitchOut, assigns probability 1 Top-level
block. Thus, example, Bob Alice uncertain block
agent using make decision, block using.
However, could also envision situation agent unsure
decision-making. say Mod[, D] block K equals block L $= K,
owns decision D, agent modeling using block L make decision
D. Section 3.2 show allows capture interesting forms bounded
rational behavior. impose requirement exists cycle
edge includes label {, D}. words, cycle agent
modeling edge. cycle called self-loop. MAID
representation NID self-loop include cycle nodes representing
agents beliefs block NID.
future examples, use following convention: exists Mod[, D] node
block K (regardless whether owns decision) CPD Mod[, D] assigns
probability 1 block K, omit node Mod[, D] block description.
Top-level block Figure 3a, means nodes Mod[Alice, Steal]
Mod[Bob, PitchOut], currently appearing dashed ovals, omitted.
118

fiNetworks Influence Diagrams

3. NID Semantics
section provide semantics NIDs terms MAIDs. first show
NID converted MAID. define NID equilibrium terms Nash
equilibrium constructed MAID.
3.1 Conversion MAIDs
following process converts block K NID MAID fragment OK ,
connects form MAID representation NID. key construct process
use chance node DK MAID represent beliefs agent regarding
action chosen decision block K. value depends block
used model decision D, determined value Mod[, D] node.
1. block K NID, create MAID OK . chance utility node N
block K descendant decision node K replicated OK ,
agent , denoted NK . N descendant decision node K,
copied OK denoted N K . case, set NK = N K agent .
2. P parent N K, PK made parent NK OK . CPD
NK OK equal CPD N K.
3. decision K, create decision node BR[D]K OK , representing
optimal action decision. N chance decision node
informational parent K, belongs agent , NK made
informational parent BR[D]K OK .
4. create chance node DK OK agent . make Mod[, D]K parent
DK . decision belongs agent , make BR[D]K parent DK .
decision belongs agent $= , make DK parent DK .
5. assemble MAID fragments OK single MAID follows: add
edge DL DK L $= K L assigned positive probability Mod[, D]K ,
owns decision D. Note may agent, including itself.
6. set CPD DK multiplexer. owns CPD DK assigns
probability 1 BR[D]K Mod[, D]K equals K, assigns probability 1
DL Mod[, D]K equals L $= K. $= owns CPD DK assigns
probability 1 DK Mod[, D]K equals K, assigns probability 1 DL
Mod[, D]K equals L $= K.
explain, Step 1 process creates MAID fragment OK NID block.
nodes ancestors decision nodes representing events occur prior
decisions copied OK . However, events occur decisions taken may
depend actions decisions. Every agent NID may beliefs
actions events follow them, regardless whether agent owns
decision. Therefore, descendant nodes decisions duplicated agent
OK . Step 2 ensures two nodes connected original block K,
119

fiGal & Pfeffer

nodes representing agents beliefs OK also connected. Step 3 creates decision
node OK decision node block K belonging agent . informational
parents decision OK nodes represent beliefs
informational parents K. Step 4 creates separate chance node OK agent
represents belief decisions K. owns decision, node
depends decision node belonging . Otherwise, node depends beliefs
regarding action agent owns decision. case models
using different block make decisions, Step 5 connects MAID fragments
block. Step 6 determines CPDs nodes representing agents beliefs
others decisions. CPD ensures block used model decision
determined value Mod node. MAID obtained result
process complete description agents beliefs others decisions.
demonstrate process converting NID Example 2.4 MAID representation, shown Figure 4. First, MAID fragments three blocks Top-level, P,
created. node Leader appearing blocks Top-level, P, descendant decision. Following Step 1, created MAID fragments,
giving nodes LeaderT L , LeaderP LeaderS . Similarly, node Steal block
node PitchOut block P created MAID fragment, giving nodes
StealS PitchOutP . Also Step 1, nodes Mod[Alice, Steal]T L , Mod[Bob, Steal]T L ,
Mod[Alice, PitchOut]T L Mod[Bob, PitchOut]T L added MAID fragment
Top-level block.
Step 3 adds decision nodes BRT L [Steal] BRT L [PitchOut] MAID fragment
L
L
L
, PitchOutTAlice
, StealTAlice
Top-level block. Step 4 adds chance nodes PitchOutTBob
TL
StealBob MAID fragment Top-level block. nodes represent agents
beliefs block decisions decisions agents. exL
represents Bobs beliefs decision whether pitch out,
ample, PitchOutTBob
TL
PitchOutAlice represents Alices beliefs Bobs beliefs decision. Also followL
L
L
StealTAlice
StealTBob
added
ing Step 4, edges BRT L [PitchOut] PitchOutTBob
MAID fragment Top-level block. represent Bobs beliefs decision
L
L
StealTBob
added MAID fragment represent
block. edge StealTAlice
Bobs beliefs Alices decision Top-level block. also nodes representing
Alices beliefs Bobs decisions block.
L
L
PitchOutP PitchoutTAlice
added
Step 5, edges StealS StealTBob
MAID fragment Top-level block. allow Bob reason Alices decision
block S, Alice reason Bobs decision block P. action unifies
L
MAID fragments single MAID. parents StealTBob
Mod[Bob, Steal]T L , StealS
L
StealTAlice
. CPD multiplexer node determines Bobs prediction Alices
action: Mod[Bob, Steal]T L equals S, Bob believes Alice using block S,
action follow experts play strategy StealS . Mod[Bob, Steal]T L equals
Top-level block, Bob believes Alice using Top-level block,
Alices action respond beliefs Bob. situation similar Alices
L
decision StealTAlice
node Mod[Alice, Steal]T L following exception:

L
Mod[Alice, Steal] equals Top-level block, Alices action follows decision node
BRT L [Steal].
Appendix, prove following theorem.
120

fiNetworks Influence Diagrams

Theorem 3.1. Converting NID MAID introduce cycle resulting
MAID.
Alice

TL

BobTLBob

Bob

ModTL[Bob, Steal]

ModTL[Bob, PitchOut]

ThrownOutTLBob

StealTLBob

PitchOutTLBob


Lead
Bob
Leader

TL
BR [PitchOut]
StealS
P
Lead
Bob
Leader

TL
LeadBob
Leader
BRTL[Steal]

PitchOutP

Mod

TL

[Alice, PitchOut]

ModTL[Alice, Steal]

StealTLAlice

PitchOutTLAlice

ThrownOut

Alice

TL

TL
Alice

Bob

Alice

TL
Alice

Figure 4: MAID representation NID Example 2.4
conversion process implies, NIDs MAIDs equivalent expressive
power. However, NIDs provide several advantages MAIDs. NID block structure
makes explicit agents different beliefs decisions, chance variables utilities
world. mental model way agents reason decisions block. MAIDs
distinguish real world agents mental models world
other, whereas NIDs separate block mental model. Further, MAID,
nodes simply represent chance, decision utilities, inherently interpreted
terms beliefs. DK node MAID representation NID inherently
represent agent beliefs decision made mental model K,
ModK agent inherently represent mental model used make
decision. Indeed, mental models defined MAID. addition,
relationship MAID descendants decisions NK NK , sense
represent possibly different beliefs agents N .
121

fiGal & Pfeffer

Together NID construction process described above, NID blueprint
constructing MAID describes agents mental models. Without NID,
process becomes inherently difficult. Furthermore, constructed MAID may large
unwieldy compared NID block. Even simple NID Example 2.4, MAID
Figure 4 complicated hard understand.
3.2 Equilibrium Conditions
Section 2.1, defined pure mixed strategies decisions MAIDs. NIDs,
associate strategies decisions blocks appear. pure strategy
decision NID block K mapping informational parents
action domain D. Similarly, mixed strategy mapping
informational parents distribution domain D. strategy profile
NID set strategies decisions blocks NID.
Traditionally, equilibrium game defined terms best response strategies.
Nash equilibrium strategy profile agent best possibly can,
given strategies agents. Classical game theory predicts agents
play best response. NIDs, hand, allow us describe situations
agent deviates best response playing according decision-making
process. would therefore like equilibrium specify agents
do, also predict actually do, may different.
NID equilibrium includes two types strategies. first, called best response
strategy, describes agents do, given beliefs decision-making
processes agents. second, called actually played strategy, describes
agents actually according model described NID. two strategies
mutually dependent. best response strategy decision block takes
account agents beliefs actually played strategies decisions.
actually played strategy decision block mixture best response
decision block, actually played strategies decision blocks.
Definition 3.2. Let N NID let MAID representation N. Let
equilibrium M. Let node belonging agent block K N. Let parents
Pa. construction MAID representation detailed Section 3.1,
K
parents BR[D]K PaK
domains Pa Pa same. Let
K
BR[D]K (pa) denote mixed strategy assigned BR[D] PaK
equals pa.
K
best response strategy K, denoted (pa), defines function values
Pa distributions satisfy
K

(pa) BR[D]K (pa)

words, best response strategy MAID equilibrium
corresponding parents take values.
Definition 3.3. Let P denote distribution defined Bayesian network
implements . actually played strategy decision K owned
agent , denoted K
(pa), specifies function values Pa distributions
satisfy

K
K
(pa) P (D | pa)
122

fiNetworks Influence Diagrams

Note here, DK conditioned informational parents decision rather
parents. node represents beliefs decision K. Therefore, actually
played strategy K represents belief K, given informational
parents D.
Definition 3.4. Let MAID equilibrium. NID equilibrium corresponding
consists two strategy profiles , every decision every block K,
K best response strategy K, K actually played strategy


K.
example, consider constructed MAID baseball example Figure 4.
best response strategies NID equilibrium specify strategies nodes Steal
PitchOut Top-level block belong Alice Bob respectively. equilibrium MAID, best response strategy Steal Top-level block
strategy specified BRT L [Steal]. Similarily, best response strategy Pitchout
Top-level block strategy specified BRT L [Pitchout]. actually played
strategy Steal Top-level equal conditional probability distribution
L
given informational parent LeaderT L . Similarly, actually played strategy
StealTAlice
L
Pitchout equal conditional probability distribution PitchoutTBob
given
TL
informational parent Leader . Solving MAID yields following unique equilibrium:
NID Top-level block, CPD nodes Mod[Alice, Steal] Mod[Bob, Pitchout]
assigns probability 1 Top-level block, actually played best response strategies Bob Alice equal specified follows: Alice leading, Alice steals
base probability 0.56 Bob pitches probability 0.47. Bob leading,
Alice never steals base Bob never pitches out. turns experts may instruct Bob call pitch out, Alice considerably less likely steal base,
compared equilibrium strategy MAID Example 2.1, none
managers considered possibility advised experts. case
similar Bob.
natural consequence definition problem computing NID equilibria
reduces computing MAID equilibria. Solving NID requires convert
MAID representation solving MAID using exact approximate solution algorithms.
size MAID bounded size block times number blocks times
number agents. structure NID exploited MAID solution
algorithm (Koller & Milch, 2001; Vickrey & Koller, 2002; Koller et al., 1996; Blum et al.,
2006).

4. Examples
section, provide series examples demonstrating benefits NIDs
describing representing uncertainty decision-making processes wide variety
domains.
4.1 Irrational Agents
Since challenge notion perfect rationality foundation economic systems presented Simon (1955), theory bounded rationality grown different
123

fiGal & Pfeffer

directions. economic point view, bounded rationality dictates complete deviation utility maximizing paradigm, concepts optimization
objective functions replaced satisficing heuristics (Gigerenzer &
Selten, 2001). concepts recently formalized Rubinstein (1998).
traditional AI perspective, agent exhibits bounded rationality program solution constrained optimization problem brought limitations architecture
computational resources (Russell & Wefald, 1991). NIDs serve complement
two prevailing perspectives allowing control extent agents behaving
irrationally respect model.
Irrationality captured framework distinction best response
actually played strategies. Rational agents always play best response respect
models. rational agents, distinction normative behavior
prescribed agent NID block, descriptive prediction agent
actually would play using block. case, best response actually
played strategies agents equal. However, open systems, people
involved, may need model agents whose behavior differs best response
strategy. words, best response strategies actually played strategies
different. capture agent behaving (partially) irrationally decision
block K setting CPD Mod[, ] assign positive probability block
L $= K.
natural way express distinction NIDs use Mod
node. decision associated agent , use Mod[, ] describe
block actually uses make decision . block K, Mod[, ] equal K
probability 1, means within K, making decision according beliefs
block K, meaning rational; play best response strategies
agents, given beliefs. If, however, Mod[, ] assigns positive probability
block L K, means probability play
best response beliefs K, rather play strategy according
block L. case, say self-models block K. introduction actually played
strategies equilibrium definition represents another advantage NIDs MAIDs,
explicitly represent strategies agents may deviate optimal
strategies.
cases, making decision may lead agent behave irrationally viewing
future considerably positive light objectively likely. example,
person undergoing treatment disease may believe treatment stands better
chance success scientifically plausible. psychological literature, effect
referred motivational bias positive illusion (Bazerman, 2001). following
example shows, NIDs represent agents motivational biases compelling way,
making Mod nodes depend outcome decision nodes.
Example 4.1. Consider case toothpaste company whose executives faced
two sequential decisions: whether place advertisement magazine
leading brand, whether increase production brand. Based past analysis,
executives know without advertising, probability high sales brand
next quarter 0.5. Placing advertisement costs money, probability
high sales rise 0.7. Increasing production brand contribute profit
124

fiNetworks Influence Diagrams

sales high, hurt profit sales low due high cost storage space.
Suppose company executives wish consider possibility motivational
bias, placing advertisement inflate beliefs sales high
next quarter probability 0.9. may lead company increase production
brand warranted market consequently, suffer losses.
company executives wish compute best possible strategy two decisions
given fact attribute motivational bias.
NID describing situation shown Figure 5c. Top-level block Figure 5a
shows situation point view reality. includes two decisions, whether
advertise (Advertise) whether increase supply brand (Increase).
node Sales represents amount sales brand decision whether
advertise, node Profit represents profit company, depends
nodes Advertise, Increase Sales. CPD Sales Top-level block assigns
probability 0.7 high Advertise true 0.5 high Advertise f alse, described
Table 4a. utility values node Profit shown Table 4.1. example,
company advertises toothpaste, increases supply, sales high, receives
reward 70; company advertises toothpaste, increase supply,
sales low, receives reward 40. Block Bias, described Figure 5b, represents
companys biased model. Here, decision advertise replaced automaton
chance node assigns probability 1 Advertise = true. CPD Sales block Bias
assigns probability 0.9 high Advertise true 0.5 high Advertise f alse,
described Table 4b. Top-level block, following:
1. node Mod[Company, Advertise] assigns probability 1 Top-level block.
2. decision node Advertise parent node Mod[Company, Increase].
3. node Mod[Company, Increase] assigns probability 1 block Bias Advertise
true, assigns probability 0 block Bias Advertise f alse.
Intuitively, Step 1 captures companys beliefs biased makes
decision advertise. Step 2 allows companys uncertainty whether biased
depend decision advertise. Note example shows necessary
decision node depend agents beliefs past decision. Step 3 captures
companys beliefs may use block Bias make decision whether increase
supply, confident high sales.
Solving NID results following unique equilibrium: block Bias, companys actually played best response strategy increase supply,
optimal action advertises sales high. block Top-level, following: company chooses advertise, behave rationally, best response
actually played strategy increase supply; company chooses advertise, actually played strategy use block Bias increases supply,
best response strategy increase supply. Now, expected utility
company Top-level block higher chooses advertise. Therefore,
best response strategies decisions advertise increase supply.
Interestingly, company never biased, shown using backwards induction
125

fiGal & Pfeffer

optimal action first decision advertise. Thus, reasoning
possible irrational behavior second decision, company revised strategy
first decision.
Mod[Company, Advertise]

Advertise
Sales

Advertise
Mod[Company, Increase]

Sales

Increase

Prot

Increase

Prot

(a) Block Top-level

(b) Block Bias

Top-level

Company, INCREASE

Bias

(c) NID

Figure 5: Motivational Bias Scenario (Example 4.1)

Advertise
true
f alse

Sales
low high
0.3
0.7
0.5
0.5

(a) node Sales (Top-level
Block)

Advertise
true
f alse

Sales
low high
0.1
0.9
0.5
0.5

(b) node Sales (Bias Block)

Table 4: CPDs Top-level block Motivational Bias NID (Example 4.1)

Example 4.2. Consider following extension Example 2.4. Suppose
two successive pitches, pitch managers option steal pitch
out. Bob pitches first pitch, utility pitching second pitch
(regardless Alices action) decreases 20 units forfeited two pitches.
Bob believes probability 0.3, succumb social pressure second
pitch call pitch out. Bob would like reason possibility making
decision first pitch.
126

fiNetworks Influence Diagrams

Advertise
true
true
true
true
f alse
f alse
f alse
f alse

Increase
true
true
f alse
f alse
true
true
f alse
f alse

Sales
high
low
high
low
high
low
high
low

Profit
70
70
50
40
80
60
60
30

Table 5: Companys utility (node Profit) Top-level block Motivational Bias NID
(Example 4.1)
example, manager faced sequential decision problem: whether
steal pitch first second pitch. strategy second pitch relevant
strategy first pitch agent. Now, managers,
rational, could use backward induction compute optimal strategies first pitch,
working backwards second pitch. However, valid procedure
managers behave rationally second pitch. example above, Bob knows
strong pressure pitch second pitch wishes take
possibility account, making decision first pitch.
Mod[Bob, PitchOut2 ]
Top-level
L

0.7
0.3

Table 6: CPD Mod[Bob, PitchOut2 ] node Top-level block Irrational Agent Scenario
(Example 4.2)
model situation NID follows. Top-level block NID shown
Figure 6a. Here, decision nodes Steal1 PitchOut1 represent decisions Alice
Bob first pitch, nodes Steal2 Pitchout2 represent decisions
Alice Bob second pitch. nodes Leader, Steal1 , PitchOut1 ThrownOut1
informational parents decision nodes Steal2 PitchOut2 . expository
convenience, included edges leading node Leader utility nodes
block. Block L, shown Figure 6b, describes model second pitch
Bob succumbing social pressure pitches out, regardless leading.
represented block include chance node PitchOut2 equals true
probability 1 value Leader. node Mod[Bob, PitchOut2 ] assign probability
0.3 block L, 0.7 probability Top-level block, shown Table 4.1. node
Mod[Bob, PitchOut2 ] displayed Top-level block. convention, implies
CPD assigns probability 1 Top-level block, Bob reasoning
possibility behaving irrationally respect second pitch. way,
captured fact Bob may behave irrationally respect second pitch,
reasoning possibility making decision first pitch.
127

fiGal & Pfeffer

Leader

Steal1

Bob1

ThrownOut1
PitchOut1

Steal2

Alice1
Bob2

Mod[Bob, PitchOut2]

ThrownOut2

Leader
PitchOut2

Alice2

PitchOut2

(a) Block Top-level
Top level

Bob, PITCHOUT2

L

(c) Irrational NID

Figure 6: Irrational Agent Scenario (Example 4.2)

128

(b) Block L

fiNetworks Influence Diagrams

unique equilibrium NID. agents behave rationally first
decision actually played best response strategies equal, specified
follows: Alice steals base probability 0.49 leading, never steals base
Bob leading. Bob pitches probability 0.38 Alice leading pitches
probability 0.51 Bob leading. second pitch, Alice behaves rationally,
best response actually played strategy follows: steal base probability
0.42 Alice leading never steal base Bob leading. Bob may behave irrationally
second pitch: best response strategy pitch probability 0.2 Alice
leading, pitch probability 0.52 Bob leading; actually played strategy
pitch probability 0.58 Alice leading, probability 0.71 Bob
leading. Note Bob reasoning possible irrational behavior
second pitch, less likely pitch first pitch compared case
Bob completely rational (Example 2.4).
4.2 Conflicting Beliefs
traditional game theory, agents beliefs assumed consistent common prior
distribution, meaning beliefs agents others knowledge expressed
posterior probability distribution resulting conditioning common prior
agents information state. One consequence assumption agents beliefs
differ observe different information (Aumann & Brandenburger, 1995).
result led theoretic work attempted relax common prior assumption. Myerson
(1991) showed game inconsistent belief structure finite converted
new game consistent belief structures constructing utility functions
equivalent original game way assign expected utility
agents. However, new game include beliefs utility functions
fundamentally different original game exhibiting inconsistent belief structure.
summary economic philosophical ramifications relaxing common prior
assumption, see work Morris (1995) Bonanno Nehring (1999).
language allows us talk different mental models
agents world, different beliefs
structure game, natural relax common prior assumption within
NIDs preserving original structure game.
Example 4.3. Consider following extension baseball scenario Example 2.1.
probability runner thrown depends decisions
managers, also speed runner. Suppose fast runner thrown
0.4 probability Bob calls pitch out, 0.2 probability Bob
call pitch out. slow runner thrown 0.8 probability Bob calls
pitch out, 0.6 probability Bob call pitch out.
Now, Bob believes runner slow, unsure Alices beliefs regarding
speed runner. probability 0.8, Bob believes Alice thinks
stealer fast, probability 0.2 Bob believes Alice thinks stealer
slow. Assume distributions variables example described
Table 1.
129

fiGal & Pfeffer

example, Bob uncertain whether Alices beliefs speed runner
conflict own. NIDs allow express natural fashion two blocks
describe decision-making process, differ CPD assign
speed runner. use Mod node, NIDs specify agents
conflicting beliefs two blocks used Alice make decision,
according Bobs beliefs. NID blocks scenario presented Figure 7.

Mod[Bob, Steal]

Lead

Leader

Speed

Speed

Steal

Steal

PitchOut

Alice

PitchOut
ThrownOut

ThrownOut

Alice

Bob

(a) Top-level Block

Bob

(b) Block L

Top level
Bob,STEAL

L

(c) Conflicting Beliefs NID

Figure 7: Conflicting Beliefs Scenario (Example 4.3)
Top-level block, shown Figure 7a, Bob Alice decide whether pitch
steal base, respectively. block identical structure Top-level block
previous example, additional node Speed parent node ThrownOut,
representing fact speed runner affects probability runner
thrown out.
Top-level corresponds Bobs model, runner slow. CPD
node Speed assigns probability 1 slow block, shown Table 7a. Block
L, shown Figure 7b, represents identical decision-making process Top-level
block, except CPD Speed different: assigns probability 1 f ast, shown
Table 7b. complete NID shown Figure 7c. Bobs uncertainty Toplevel block Alices decision-making process represented node Mod[Bob, Steal],
whose CPD shown Table 7c. probability 0.8, Alice assumed using block
L, speed runner fast. probability 0.2, Alice assumed
using Top-level block, speed runner slow. Note
130

fiNetworks Influence Diagrams

Speed
f ast slow
0
1

Speed
f ast slow
1
0

(a) node Speed
(b) node Speed
(block Top-level) (block L)

Mod[Bob, Steal]
Top-level
L
0.2
0.8
(c) node
Mod[Bob, Steal] (block
Top-level)

Table 7: CPDs nodes Conflicting Beliefs NID (Example 4.3)
Top-level block, nodes Mod[Alice, Steal], Mod[Alice, PitchOut] Mod[Bob, PitchOut]
displayed. convention introduced earlier, nodes assign probability
1 Top-level block omitted Top-level block Figure 7a.
Interestingly, implies Alice knows runner slow, even though Bob believes
Alice believes runner fast. solving NID, get unique equilibrium.
agents rational, best response actually played strategies equal,
specified follows: block L, runner fast, Alice always steals base, Bob
always calls pitch out. Top-level block, Bob believes Alice uses block L
high probability, seals base. Top-level block speed runner
slow likely thrown out. Therefore, Bob pitch order maximize
utility given beliefs Alice. turn, Alice steal base Top-level
block speed runner slow block.
4.3 Collusion Alliances
situation agent modeling multiple agents, may important know
whether agents working together fashion. situations, models
agents make decisions may correlated, due possible collusion.
Example 4.4. voting game involves 3 agents Alice, Bob, Carol, voting one
chairperson committee. Alice incumbent, chairperson
vote ends draw. agent would like chairperson, receives
utility 2 case. Alice also receives utility 1 votes winner loses
election, wants look good. Bob Carol, meanwhile, dislike Alice
receive utility -1 Alice wins.
best interests agents Bob Carol coordinate, vote
person. Bob Carol indeed coordinate, Alices best interest vote
person vote for. However, Bob Carol mis-coordinate, Alice vote
remain chairperson. taking opponent modeling approach, Alice would
like model Bob Carol likely vote. Alice believes
probability 0.2, Bob Carol collude; probability 0.3, Bob Carol collude
vote Bob; probability 0.4, Bob Carol collude vote Carol. Also, Alice
believes collude, agents might renege vote
probability 0.1.
example easily captured NID. Top-level block shown Figure 8.
node Collude, three possible values: none indicating collusion;
131

fiGal & Pfeffer

Bob Carol indicating collusion vote Bob Carol respectively. decision nodes
A, B, C represent decisions Alice, Bob Carol, respectively. CPD Collude
presented Table 8a. nodes Mod[Alice, B] Mod[Alice, C], whose CPD shown
Table 8b 8c respectively, depend Collude. Collude none, Mod[Alice, B]
assign probability 1 Top-level block. Collude Bob, Mod[Alice, B] equal block
B describing automaton Bob Carol vote Bob. Collude Carol,
Mod[Alice, B] equal block C, Bob Carol vote Carol probability
0.9, block B probability 0.1. accounts possibility Bob
Carol agreed vote Carol, Bob might renege. CPD Mod[Alice, B]
similar, described Table 8b. CPD Mod[Alice, C] symmetric,
described Table 8c.
Collude

Mod[Alice, B]

Mod[Alice, C]



B

C

Alice

Bob

Caroll

Figure 8: Top-level block Collusion Scenario (Example 4.4)

Collude
none Bob Carol
0.2
0.3
0.5
(a) node Collude

Mod[Alice, B]
Top-level
B
C

none
1
0
0

Collude
Bob Carol
0
0
1
0
0.1
0.9

(b) node Mod[Alice, B]

Mod[Alice, C]
Top-level
B
C

none
1
0
0

Collude
Bob Carol
0
0
0.9
0.1
0
1

(c) node Mod[Alice, C]

Table 8: CPDs Top-level block Collusion Scenario (Example 4.4)
unique NID equilibrium example, agents rational actually
played best response strategies equal. equilibrium, Alice always votes
Carol believes Bob Carol likely collude vote Carol.
132

fiNetworks Influence Diagrams

turn, Carol votes herslef Bob probability 0.5, Bob always votes
himself. reneging, Bob gives chance win vote, case Carol
votes him.
Moving beyond example, one important issues multi-player games
alliances. players form alliance, act benefit alliance rather
purely self-interest. Thus agents beliefs alliance structure
affects models agents make decisions. agent make
decision situation, important able model uncertainty
alliance structure.
4.4 Cyclic Belief Structures
Cyclic belief structures important game theory, used model agents
symmetrically modeling other. used describe infinite regress
think think think... reasoning. Furthermore, cyclic belief structures
expressed economic formalisms, like Bayesian games, vital allow
NIDs order NIDs encompass Bayesian games. Cyclic belief structures naturally
captured NIDs including cycle NID graph.
Example 4.5. Recall Example 4.3, Alice Bob conflicting beliefs
speed runner. Suppose Bob believes runner slow,
probability 0.8, Alice believes runner fast, modeling Bob reasoning
Alices beliefs, on...
model scenario using cyclic NID described Figure 9c. Top-level
block, shown Figure 9b, Bob believes runner slow modeling Alice
using block L make decision. block L, Alice believes runner fast,
modeling Bob using Top-level block make decision. Bobs beliefs Alice
Top-level block represented CPD node Mod[Bob, Steal], shown Table
9c, assigns probability 1 block L.
block L, CPD Speed, shown Table 9b assigns probability 1 f ast. Alices
beliefs Bob block L represented CPD node Mod[Alice, PitchOut],
shown Table 9d, assigns probability 1 block L. Top-level block, CPD
Speed assigns probability 1 slow, shown Table 4.4a. NID equilibrium
scenario follows. blocks L Top-level, Alice steal base, Bob
pitch out, regardless leading.

5. Application: Opponent Modeling
cases, agents use rules, heuristics, patterns tendencies making decisions.
One main approaches game playing imperfect information opponent modeling, agents try learn patterns exhibited players react
model others. NIDs provide solid, coherent foundation opponent modeling.
Example 5.1. game RoShamBo (commonly referred Rock-Paper-Scissors),
players simultaneously choose one rock, paper, scissors. choose item,
result tie; otherwise rock crushes scissors, paper covers rock, scissors cut paper,
shown Table 10.
133

fiGal & Pfeffer

Speed
f ast slow
0
1

Speed
f ast slow
1
0

(a) node Speed
(b) node Speed
(block Top-level) (block L)

Mod[Bob, Steal]
Top-level
L
1
0
(c) node
Mod[Bob, Steal]
(block Top-level)

Mod[Alice, PitchOut]
Top-level
L
1
0
(d) node
Mod[Alice, PitchOut]
(block L)

Table 9: CPDs nodes Cyclic NID (Example 4.5)

Mod[Bob, Steal]

Mod[Alice, PitchOut]

Speed

Speed

Steal

Steal

PitchOut
ThrownOut

PitchOut
ThrownOut

Alice

Alice

Bob

(a) Block L

Bob

(b) Block Top-level

Top level
Bob,STEAL

Alice,PITCHOUT

L

(c) Cyclic NID

Figure 9: Cyclic Baseball Scenario (Example 4.5)

rock
paper
scissors

rock
(0, 0)
(1, 1)
(1, 1)

paper
(1, 1)
(0, 0)
(1, 1)

scissors
(1, 1)
(1, 1)
(0, 0)

Table 10: Payoff Matrix Rock-paper-scissors

134

fiNetworks Influence Diagrams

game single Nash equilibrium players play mixed strategy
{rock, paper, scissors} probability { 13 , 13 , 13 }. players deviate
equilibrium strategy, guaranteed expected payoff zero. fact,
easy verify player always plays equilibrium strategy guaranteed
get expected zero payoff regardless strategy opponent. words,
sticking equilibrium strategy guarantees lose match expectation,
also guarantees win it!
However, player try win game opponents playing suboptimally.
suboptimal strategy beaten, predicting next move opponent
employing counter-strategy. key predicting next move model
strategy opponent, identifying regularities past moves.
consider situation two players play repeatedly other.
player able pick tendencies suboptimal opponent, might able
defeat it, assuming opponent continues play suboptimally. recent competition (Billings, 2000), programs competed matches consisting 1000
games RoShamBo. one might expect, Nash equilibrium players came middle
pack broke even every opponent. turned task
modeling opponents strategy surprisingly complex, despite simple structure game itself. sophisticated players attempt counter-model
opponents, hide strategy avoid detection. winning program,
called Iocaine Powder (Egnor, 2000), beautiful job modeling opponents multiple levels. Iocaine Powder considered opponent might play randomly, according
heuristic, might try learn pattern used Iocaine Powder, might
play strategy designed counter Iocaine Powder learning pattern, several
possibilities.
5.1 NID Modeling Belief Hierarchies
Inspired Iocaine Powder, constructed NID player playing match
RoShamBo trying model opponent. Suppose Bob wishes model
Alices play using NID. block Top-level NID, shown Figure 10a, simply
MAID depicting RoShamBo round Bob Alice. players access
predictor P, algorithm able predict next move sequence probability
distribution possible moves. information available predictor
history past moves Alice Bob.
Alice may ignoring P, playing Nash Equilibrium strategy. Bob several
alternative models Alices decision. According block Automaton, shown Figure 10c,
Alice always follows signal P. block B1, shown Figure 10b, Bob modeling Alice
using block Automaton make decision. achieved setting CPD
Mod[Bob, Alice] block B1 assign probability 1 Automaton. analyze NID
rooted block B1 determine Bobs best response Alice. example, Bob thinks,
based history, P likely tell Alice play rock, Bob would play
paper. Let us denote strategy BR(P).
However, Alice also model Bob assigning probability 1 Mod[Alice, Bob] block
A1. way, Alice reasoning Bob modeling Alice following predictor P.
135

fiGal & Pfeffer

P

Mod[Alice, Bob]

alice

bob

bob

alice

Mod[Bob, Alice]

P

alice

bob

bob

alice

P

(a) Blocks Top-level, A1,A2

(b) Blocks B1,B2

Alice

(c) Block Automaton

Top-level

Bob,ALICE

A2

Bob, ALICE

Alice, BOB
Bob, ALICE
B2

Bob,ALICE

A1

Alice, BOB

B1
Bob, ALICE
Automaton

(d) RoShamBo NID

Figure 10: RoShamBo Scenario (Example 5.1)

136

fiNetworks Influence Diagrams

analyze NID originating block A1, shown Figure 10a, determine
Alices best-response Bobs model well Bobs best-response model
Alice. Since Alice believes Bob plays BR(P) result Bobs belief Alice plays
according P, therefore play best response BR(P), thereby double-guessing
Bob. Alices strategy block A1 denoted BR(BR(P)). Following example,
block A1 Alice play rock all, scissors, order beat Bobs play paper.
Similarly, block B2, Bob models Alice using block A1 make decisions,
block A2, Alice models Bob using block B2 make decision. Therefore, solving
NID originating block B2 results BR(BR(BR(P))) strategy Bob. would
prompt Bob play rock B2 example, order beat scissors. Lastly, solving
NID originating block A2 results BR(BR(BR(BR(P)))) strategy Alice.
would prompt Alice play paper block A2, order beat rock. Thus,
shown every instance predictor P, Alice might play one three possible
strategies. pure strategy choose rock, paper, scissors
given P, reasoning process terminates.
entire NID shown Figure 10d. block Top-level, Bob models Alice using
one several possible child blocks: block Automaton, Alice follows predictor;
block A1, Alice second-guessing predictor; block A2, Alice
triple-guessing predictor. Bobs uncertainty Alices decision-making processes
captured Mod[Bob, Alice] node block Top-level. Analyzing Top-level block
NID extract Bobs best response strategy given beliefs Alices decisionmaking processes.
use NID practice, necessary compute MAID equilibrium extract
Bobs best-response strategy Top-level block. end, need estimate
values NID parameters, represented unknown CPDs blocks,
solve NID. parameters include Mod[Bob, Alice], representing Bobs beliefs
Top-level block regarding block Alice using; node P, representing
distributions governing signals Alice Bob, respectively.2 end, use
on-line version EM algorithm tailored NIDs. begin
random parameter assignments unknown CPDs. revise estimate
parameters NID given observations round. Bob plays bestresponse strategy MAID representation NID given current parameter
setting. Interleaving learning using NID make decision helps Bob adapt
Alices possibly changing strategy.
5.2 Empirical Evaluation
evaluated NID agent ten top contestants first automatic
RoShamBo competition. agents used opponent modeling approach,
is, learned signal opponents play based history prior rounds.
Contestants roughly classified according three dimensions: type signal used
(probabilistic vs. deterministic); type reasoning used (pattern vs. meta-reasoners);
and, degree exploration versus exploitation model. Probabilistic agents
2. Technically, CPDs nodes representing prior history also missing. However,
observed decision-making point interaction CPDs affect players utilities.

137

fiGal & Pfeffer

estimated distribution strategies opponents deterministic agents
predicted opponents next move certainty. Pattern reasoners directly modeled
opponents playing according rule distribution, reason
possibility opponents modeling themselves. contrast, meta-reasoners
attempted double- triple-guess opponents play. Exploitative agents played best
response model opponents, explorative agents deviated, certain
conditions, best response strategy try learn different behavioral patterns
opponents. Iocaine Powder used strategy reverting Nash equilibrium
losing. made impossible evaluate whether NID model
could learn Iocaine powders reasoning process, turned strategy. Also, limited
contestants strategies depend last 100 rounds play, order allow fair
comparison NID agent used four rounds play. limit
four rounds originally designed use short history.
purpose show explicitly reasoning learning mental models make
difference, optimize learning model signal.
Figure 11 shows performance RoShamBo NID playing 10 matches
3,000 rounds contestant. overall standings determined ordering
total scores contestant rounds played (+1 winning round
contestant NID player; 1 losing round; 0 ties). Therefore, important
player maximize win weaker opponents, minimize loss
stronger opponents. x-axis includes contestant number y-axis describes
difference average score RoShamBo NID contestant; error
bars indicate single standard deviation difference.
shown figure, RoShamBo NID able defeat contestant
matches, including version Iocaine Powder. best performance NID
achieved playing pattern reasoners used deterministic signals (Contestants 3, 5
6). contestants directly predicted opponents play function
history, without reasoning opponents model themselves. Consequently,
difficult detect change strategies adaptive opponents,
RoShamBo NID. addition, use deterministic signals made harder
contestants capture probabilistic players like NID algorithm.
RoShamBo NID also outperformed contestants attempted trick
opponents, reasoning possibility opponents double- tripleguessing model (Contestants 4 1). shows NID able determine
level reasoning employed opponents.

6. Relationship Economic Models
section, describe relationship NIDs several existing formalisms
representing uncertainty decision-making processes. NIDs share close relationship Bayesian games (Harsanyi, 1967), game-theoretic framework representing
uncertainty players payoffs. Bayesian games capture beliefs agents
well define equilibrium assigns best response strategy
agent given beliefs. Bayesian games quite powerful ability describe belief
hierarchies cyclic belief structures.
138

fiNetworks Influence Diagrams

400

350

Average Score Difference

300

250

200

150

100

50

0

0

1

2

3

4

5
Contestant

6

7

Opponent type
Iocaine Powder
Probabilistic, Pattern, Exploitative
Deterministic, Pattern, Exploitative
Probabilistic, Meta, Exploitative
Probabilistic, Pattern, Exploitative

8

9

10

Number
1
2, 9
3, 6, 5
1, 4
7, 8

Figure 11: Difference average outcomes NID player opponents

Bayesian game, agent discrete type embodying private information.
Let N set agents. agent Bayesian game includes set possible types
Ti , set possible actions Ci , conditional distribution pi utility function ui . Let
= Ti let C = Ci . agent i, let Ti = j&=i Tj denote set
possible types agent i. probability distribution pi function
ti Ti , is, pi (.|ti ) specifies type ti Ti joint distribution
types agents. utility function ui function C real
numbers. standard assumption game, including agents strategies, utilities
type distributions, common knowledge agents.
solution concept commonly associated Bayesian games Bayesian
Nash equilibrium. equilibrium maps type mixed strategy actions
agents best response strategies agents, given beliefs
types. Notice Bayesian game, agents action depend types
types agents, unknown agent
analyzes game. assumed agent knows type, type
subsumes agents private information game begins. types
agents unknown, agent maximizes expected utility given distribution
types.
Let Ni denote agents Bayesian game apart agent i. Let (.|ti )
denote random strategy agent given type ti . Bayesian Nash equilibrium
139

fiGal & Pfeffer

mixed strategy profile agent type ti Ti
#
(.|ti ) argmax Ci ti Ti pi (ti |ti )
$%
&
#
cC
jNi j (cj |tj ) (ci )ui (t, c)

(2)

Bayesian games used extensively modeling interaction agents
private information, auction mechanisms (Myerson, 1991) used
express uncertainty agents decision-making models. general, Bayesian games
expressive NIDs. show, Bayesian game converted NID
time space linear size Bayesian game. Conversely, NID
converted Bayesian game, NID converted MAID,
turn converted extensive form game. extensive form game converted
normal form game trivial Bayesian game one type per agent.
However, worst case, size extensive form game exponential
number informational parents decision nodes MAID, size normal
form game exponential size extensive form game. course,
brute force conversion; compact conversions may possible.
consider formally question whether Bayesian games represented NIDs. idea align type Bayesian game decision
NID block. resulting best response strategy decision NID equilibrium
equal Bayes Nash equilibrium strategy type.
Definition 6.1. Let B Bayesian game N NID. say N equivalent
B exists injective mapping f types B (block,agent) pairs N ,
following conditions hold:
1. Bayesian Nash equilibrium B, exists NID equilibrium N ,
every type ti , f maps ti (K, ), best-response actually-played
strategies K equal (.|ti ).
2. NID equilibrium N , exists Bayesian Nash equilibrium B
every (K, ) image f , (.|ti ) ti = f 1 (K, ) equal
best-response actually-played strategies K.
following theorem proved Appendix 8.
Theorem 6.2. Every Bayesian game represented equivalent NID whose size
linear size Bayesian game.
section, use term Bayesian games specify representation
includes type distributions utility functions presented explicitly. NIDs enjoy
advantages fully specified Bayesian games graphical models typically enjoy
unstructured representations. general, NIDs may exponentially compact
Bayesian games Bayesian games require, every type every agent, full
joint distribution types agents. addition, utility function
Bayesian game specifies utility joint combination types actions every
player. distributions utility functions exponential number players.
NIDs, based MAIDs, type distributions decomposed
140

fiNetworks Influence Diagrams

product small conditional distributions, utility functions additively
decomposed sum small functions depend small number actions.
addition, Bayesian games representationally obscure. First, types Bayesian
games atomic entities capture information available agent single
variable. type used capture agents beliefs way world works
(including preferences), private information. example, poker,
players beliefs players tendency bluff knowledge cards
received captured type. believe two aspects fundamentally
different; one describes actual state world describes going
players head. Conflating two aspects leads confusion. NIDs, two aspects
differentiated. Private information world represented informational
parents, whereas mental models represented blocks.
Second, type Bayesian game decompose different aspects information
variables. Thus poker, hand must represented single variable, whereas
NIDs represented different variables representing cards. final point
Bayesian games uncertainty must folded utility functions
distribution agents types. Consider scenario two agents
conflicting beliefs chance variable, Example 4.3. NID,
separate block possible mental model differs CPD assignments
chance variable. contrast, type Bayesian game would sum
distribution chance variable. Looking Bayesian game, would know
whether reason different utility functions agent different beliefs
chance variable, whether due different preferences agent.
NIDs also exhibit relationship recent formalisms games awareness,
agents may unaware players strategies structure
game (Halpern & Rego, 2006; Feinberg, 2004). game description formalism shows
players awareness others strategies changes time. game awareness
includes set extensive form game descriptions, called augmented games, represent
analysts beliefs world, well separate descriptions game
may become true according agents subjective beliefs. analysts augmented game
considered actual description reality, subjective augmented game
differ analysts game agents utility functions, decisions, strategies
available agents decisions. history agent augmented game
sub-path tree leading node agent makes move. Awareness
modeled function maps agent-history pair one augmented game another
augmented game agent considers possible given history. Uncertainty
agents awareness augmented game quantified nature choose
move tree leading agents information sets. definition Nash equilibrium
extended include set strategies agent-game pairthat agent considers
possible, given history best-response strategies used agents
augmented game. formalism capture analysts model agents awareness
well agents model own, agents awareness.
fundamental differences NIDs games awareness. First, like
Bayesian games, equilibrium conditions representation allow agents
deviate best-response strategies. Second, require presence modeler
141

fiGal & Pfeffer

agent, reality, modeling uncertainty levels awareness agents.
NIDs allow modeler agent, require it. allows capture
situations agent certain knowledge reality, Baseball NID
Example 2.4. Third, augmented game awareness represented extensive form
game, shown above, may exponentially larger MAID used
represent decision-making model NID. Lastly, agents awareness others
strategies one type reasoning captured NID. types
reasoning processes described Section 4.
Lastly, Gmytrasiewicz Durfee (2001) developed framework representing
uncertainty decision-making using tree structure nodes consist payoff
matrices particular agent. Like Bayesian games, uncertainty folded payoff
matrices. agent maintains tree, representing model decision-making
processes used agents. Like traditional representations, language assumes
agents behave rationally. addition, assumes agent believes others use
fixed strategy, folded environment.

7. Conclusion
presented highly expressive language describing agents beliefs decisionmaking processes games. language graphical. model language
network interrelated models, mental model graphical model
game. agent one mental model may believe another agent (or possibly itself) uses
different mental model make decisions; may uncertainty mental
model used. presented semantics language terms multi-agent influence
diagrams. analyzed relationship language Bayesian games.
equally expressive, NIDs may exponentially compact.
showed language used describe agents play irrationally,
sense actual play correspond best possible response given
beliefs world agents. captured novel equilibrium
concept captures interaction agents actually
do. also showed express situations agents conflicting beliefs,
including situations agents common prior distribution
state world. Finally, showed capture cyclic reasoning patterns,
agents engage infinite chains think think think... reasoning.
vital question use language learn agents behavior reasoning
processes. shown, language used learn non-stationary strategies
rock-paper-scissors. work, shown models inspired
NIDs learn peoples play negotiation games (Gal, Pfeffer, Marzo, & Grosz, 2004; Gal
& Pfeffer, 2006). focus continuing work develop general method
learning models NIDs.

Acknowledgments
Thank much useful comments provided anonymous reviewers
editor. Thanks Barbara Grosz Whitman Richards invaluable guidance.
142

fiNetworks Influence Diagrams

Thanks Adam Juda reading prior draft work. work supported
NSF Career Award IIS-0091815 AFOSR contract FA9550-05-1-0321.

8. Appendix
Theorem 3.1: Converting NID MAID introduce cycle resulting
MAID.
Proof. First, let us ignore edges added step 5 construction, focus
MAID fragment OK constructed single block K. Since block acyclic,
number nodes block integers topological order. number nodes
OK follows. node N derives chance utility node N K, N
gets number N . node BR[D]K gets number D. node DK ,
owns D, gets number plus 1/3. node DK ,
D, gets number plus 2/3. construction, P parent N OK , P
lower number N .
let us consider entire constructed MAID O. Suppose, way contradiction,
cycle O. follows argument must consist entirely
edges fragments added step 5. Since edges emanate node
DK owns D, end node DL , nodes cycle must refer
decision D, must belong agent owns D. Thus cycle must form
DK1 , . . . , DKn , DK1 owns D. Since edge added DKi DKi+1
O, must modeling block Ki using block Ki+1 make decision D. Therefore
self-loop NID, contradiction.
Theorem 6.2: Every Bayesian game represented equivalent NID whose size
linear size Bayesian game.
Proof. Given Bayesian game B, construct NID N follows. set agents N
equal set agents B. type ti agent B corresponding
block N labeled ti . block ti contains decision node Dj utility node Uj
every agent j. Dj informational parents. domain Dj set choices Cj
agent j B. add new chance node Qi block ti whose domain set Ti .
node Mod[i, Dj ] j $= node Qi parent. parents Ui
decision nodes well node Qi . agent j $= i, Uj parent
Dj . agent j define distinguished action cj Cj .
set CPD nodes ti follows:
1. CPD Mod[i, Di ] assigns probability 1 ti .
2. CPD Qi assigns probability pi (ti | ti ), defined B, type profile
ti Ti .
3. CPD node Mod[i, Dj ] j $= assigns probability 1 block tj
jth element value parent node Qi equals tj . projects probability
distribution Qi B node Mod[i, Dj ] representing beliefs block
agent j using NID.
143

fiGal & Pfeffer

4. CPD Ui assigns probability 1 ui (t, c), defined B, given Qi equals
t, equals c.
5. CPD Uj assigns probability 1 utility 1 Dj = cj , probability 1
0 otherwise.
6. CPD Mod[j, Dk ], k, j $= i, assigns probability 1 ti .
construction accompanied injective mapping f maps type ti
(block,agent) pair (ti , i).
Let constructed MAID N. prove condition 1 Definition 6.1, let
Bayes Nash equilibrium B. agent, conditional probability distribution
( | ti ). define strategy profile follows. BR[Di ]ti = ( | ti ) decisions
owned agent i, BR[Dj ]ti assigns probability 1 cj j $= i.
claim following:
1. MAID equilibrium M, according Definition 2.3.
2. resulting NID equilibrium, best response strategy ti ( | ti ).
3. resulting NID equilibrium, actually played strategy best
response strategy.
Claim 3 true Mod[i, Di ] assigns probability 1 ti .
Note informational parents N. Therefore, definition NID
ti
= BR[Di ,ti ] = ( | ti ). Therefore, Claim 2
equilibrium, best response strategy

true.
prove Claim 1, note first block ti , utility node Uj , j $= i, fully
determined Dj , Dj sole parent Uj . Also, player j self-modeling
Dj , CPD node Mod[j, Dj ] assigns probability 1 ti . holds
M: decision node BR[Dj ]ti sole parent Ujti . Therefore, equilibrium
M, strategy BR[Dj ]ti assign probability 1 distinguished action cj
causes Ujti 1.
block ti , CPD Mod[i, Dj ] assigns probability 0 ti . means player
j using block ti make decision, according beliefs. Therefore, BR[Dj ]ti
independent Uiti , equilibrium strategies BR[Di ]ti independent
distinguished action chosen BR[Di ]tj .
definition MAID equilibrium Definition 2.3, strategy profile
equilibrium maximizes EU (i). need show maximizing equivalent
maximizing right hand side Equation 2. utility Uiti decision node
BR[Di ]ti every block ti . Let ctii denote choice agent decision BR[Di ]ti block
t"

ti . Let t(i denote block corresponding different type t(i agent i. Let cii choice
"
agent decision BR[Di ]ti block t(i ci choices decisions
"
BR[Di ]ti . construction M, Uiti d-separated BR[Di ]ti given BR[Di ]ti
BR[Di ]ti .
result, optimize Uiti separately utility nodes belonging
agent i, considering BR[Di ]ti . get utility given
144

fiNetworks Influence Diagrams

strategy profile written
E [Uiti ] =

"

iti (ctii )

"
ci


ci

(ci )

"

ui

P (Uiti = ui | ctii , ci ) utii

(3)

condition agent beliefs decisions agents block ti . Let
Mod[i, Di ]ti denote set nodes Mod[i, Dj ]ti j $= i, let tuple ti refer
block label profile blocks Ti . obtain
"

ci

iti (ctii )

"
ci

(ci )

"
ti

P (Mod[i, Di ]ti = ti )

"

ui

(utii | ctii , ci , ti ) utii

(4)

observe role Mod[i, Di ]ti determine choices decisions
BR[Di ]ti relevant utility player i. particular, Mod[i, Dj ]ti equal
tj , js choice block tj player needs consider makes decision

BR[Di ]ti . Let ci denote relevant choices BR[Di ]ti Mod[i, Di ]ti = ti .
Since choice variables irrelevant, marginalize obtain
#



ci

iti (ctii )

#






ci

#


(ci
)

#

ti

P (Mod[i, Di ]ti = ti )

(5)




P (Uiti = ui | ctii , ci
) utii



ui

Rearranging terms, rewrite Equation 5.
(
# '%
tj tj
ti = )
P
(Mod[i,

]

(c
)


ti
c
j&=i j
j

ti ti #




(ci ) uti P (Ui = ui | cii , ci
) ui

#

(6)







construction, P (Mod[i, Di ]ti = ti ) pi (ti | ti ) defined B, jj (cjj )
#
ti
) utii ui (t, c). therefore get
j (cj | tj ) defined B, uti P (Uiti = ui | ctii , ci


"
ti

pi (ti | ti )

"
c




!
j&=i



j (cj | tj ) (ci | ti )ui (t, c)

(7)

Therefore MAID equilibrium Bayesian Nash equilibrium
B. Claim 1 established therefore Condition 1 Definition 6.1 satisfied.
Finally, prove Condition 2, given NID equilibrium N construct MAID
equilibrium copying best response strategies, construct strategies
B exactly reverse manner above. previous reasoning applies reverse
show Bayes Nash equilibrium B best response actually played
strategies N equal .

145

fiGal & Pfeffer

References
Arunachalam, R., & Sadeh, N. M. (2005). supply chain trading agent competition.
Electronic Commerce Research Applications, 4, 6381.
Aumann, R., & Brandenburger, A. (1995). Epistemic conditions Nash equilibrium.
Econometrica, 63 (5), 11611180.
Bazerman, M. (2001). Judgment Managerial Decision Making. Wiley Publishers.
Billings, D. (2000). first international RoShamBo programming competition. International Computer Games Association Journal, 23 (1), 38.
Blum, B., Shelton, C. R., & Koller, D. (2006). continuation method Nash equilibria
structured games. Journal Artificial Intelligence Research, 25, 457502.
Bonanno, G., & Nehring, K. (1999). make sense common prior assumption
incomplete information. International Journal Game Theory, 28, 409434.
Camerer, C. F. (2003) Behavioral Game Theory. Experiments Strategic Interaction,
chap. 2. Princeton University Press.
Cowell, R. G., Lauritzen, S. L., & Spiegelhater, D. J. (2005). Probabilistic Networks
Expert Systems. Springer.
Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial
Intelligence, 113 (1-2), 4185.
Egnor, D. (2000). Iocaine Powder. International Computer Games Association Journal,
23 (1), 38.
Feinberg, Y. (2004). Subjective reasoning games unawareness. Tech. rep. 1875,
Stanford University.
Gal, Y., & Pfeffer, A. (2003a). language modeling agents decision making processes
games. Proc. 2nd International Joint Conference Autonomous Agents
Multi-agent Systems (AAMAS).
Gal, Y., & Pfeffer, A. (2003b). language opponent modeling repeated games.
Workshop Game Theory Decision Theory, AAMAS.
Gal, Y., & Pfeffer, A. (2004). Reasoning rationality belief. Proc. 3rd International Joint Conference Autonomous Agents Multi-agent Systems (AAMAS).
Gal, Y., & Pfeffer, A. (2006). Predicting peoples bidding behavior negotiation. Proc.
5th International Joint Conference Autonomous Agents Multi-agent Systems
(AAMAS).
Gal, Y., Pfeffer, A., Marzo, F., & Grosz, B. (2004). Learning social preferences games.
Proc. 19th National Conference Artificial Intelligence (AAAI).
Gigerenzer, G., & Selten, R. (Eds.). (2001). Bounded Rationality: Adaptive Toolbox.
MIT Press.
Gmytrasiewicz, P., & Durfee, E. H. (2001). Rational communication multi-agent environments. Autonomous Agents Multi-Agent Systems, 4 (3), 233272.
146

fiNetworks Influence Diagrams

Halpern, J., & Rego, L. (2006). Extensive games possibly unaware players. Proc.
5th International Joint Conference Autonomous Agents Multi-agent Systems
(AAMAS).
Harsanyi, J. C. (1967). Games incomplete information played Bayesian players.
Management Science, 14, 159182, 320334, 486502.
Howard, R. A., & Matheson, J. E. (1984). Influence diagrams. Readings Principles
Applications Decision Analysis, pp. 721762.
Kearns, M., Littman, M., & Singh, S. (2001). Graphical models game theory. Proc.
17th Conference Uncertainty Artificial Intelligence (UAI).
Koller, D., Meggido, N., & von Stengel, B. (1996). Efficient computation equilibria
extensive two-person games. Games Economic Behavior, 14 (2), 247259.
Koller, D., & Milch, B. (2001). Multi-agent influence diagrams representing solving
games. Proc. 17th International Joint Conference Artificial Intelligence (IJCAI).
MacKie-Mason, J. K., Osepayshivili, A., Reeves, D. M., & Wellman, M. P. (2004). Price
prediction strategies market-based scheduling. Proc. 18th International Conference Automated Planning Scheduling.
Morris, S. (1995). common prior assumption economic theory. Economic Philosophy,
pp. 227253.
Myerson, R. (1991). Game Theory. Harvard University Press.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.
Rajarshi, D., Hanson, J. E., Kephart, J. O., & Tesauro, G. (2001). Agent-human interactions
continuous double auction. Proc. 17th International Joint Conference
Artificial Intelligence (IJCAI).
Rubinstein, A. (1998). Modeling Bounded Rationality. MIT Press.
Russell, S., & Wefald, E. (1991). Right Thing: Studies Limited Rationality. MIT
Press.
Simon, H. A. (1955). behavioral model rational choice. Quarterly Journal Economics,
69, 99118.
Vickrey, D., & Koller, D. (2002). Multi-agent algorithms solving graphical games.
Proc. 18th National Conference Artificial Intelligence (AAAI).

147

fiJournal Artificial Intelligence Research 33 (2008) 3377

Submitted 09/07; published 09/08

ICE: Expressive Iterative Combinatorial Exchange
Benjamin Lubin
Adam I. Juda
Ruggiero Cavallo
Sebastien Lahaie
Jeffrey Shneidman
David C. Parkes

blubin@eecs.harvard.edu
adamjuda@post.harvard.edu
cavallo@eecs.harvard.edu
slahaie@eecs.harvard.edu
jeffsh@eecs.harvard.edu
parkes@eecs.harvard.edu

School Engineering Applied Sciences
Harvard University
Cambridge, 02138

Abstract
present design analysis first fully expressive, iterative combinatorial
exchange (ICE). exchange incorporates tree-based bidding language (TBBL)
concise expressive CEs. Bidders specify lower upper bounds TBBL
value different trades refine bounds across rounds. bounds allow price
discovery useful preference elicitation early rounds, allow termination
efficient trade despite partial information bidder valuations. computation
exchange carefully optimized exploit structure bid-trees avoid enumerating trades. proxied interpretation revealed-preference activity rule, coupled
simple linear prices, ensures progress across rounds. exchange fully implemented, give results demonstrating several aspects scalability economic
properties simulated bidding strategies.

1. Introduction
Combinatorial exchanges combine generalize two different mechanisms: double auctions
combinatorial auctions. double auction (DA), multiple buyers sellers trade
units identical good (McAfee, 1992). combinatorial auction (CA), single seller
multiple heterogeneous items sale (de Vries & Vohra, 2003; Cramton, Shoham,
& Steinberg, 2006). buyer CA may complementarities (I want B)
substitutabilities (I want B) goods, provided expressive
bidding language describe preferences. common goal design DAs
CAs implement efficient allocation, allocation maximizes
total social welfare.
combinatorial exchange (CE) (Parkes, Kalagnanam, & Eso, 2001) combinatorial
double auction brings together multiple buyers sellers trade multiple heterogeneous goods. CEs potential use wireless spectrum allocation (Cramton, Kwerel, &
Williams, 1998; Kwerel & Williams, 2002), airport takeoff landing slot allocation (Ball,
Donohue, & Hoffman, 2006; Vossen & Ball, 2006), financial markets (Saatcioglu,
Stallaert, & Whinston, 2001). domains incumbents property
rights, necessary facilitate complex multi-way reallocation resources. Another potential application domain CEs allocate resources shared distributed
c
2008
AI Access Foundation. rights reserved.

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

systems, PlanetLab (Fu, Chase, Chun, Schwab, & Vahdat, 2003). CEs also find
use task allocation robot teams, making potentially powerful tool multiagent systems community (Gerkey & Mataric, 2002; Bererton, Gordon, & Thrun, 2003;
Dias, Zlot, Kalra, & Stentz, 2006). Finally, CEs promise mechanisms expressive
sourcing multiple bid-takers, perhaps representing different profit centers within organization; see associated work expressive sourcing using one-sided CAs (Sandholm,
2007).
paper presents design first fully expressive, iterative combinatorial exchange (ICE). designing iterative exchange, share motivation earlier work
iterative CAs: wish mitigate elicitation costs focusing bidders, case
price discovery activity rules, values relevant trades. important
determining value even single potential trade challenging problem complex domains (Sandholm & Boutilier, 2006; Compte & Jehiel, 2007). Moreover,
bidders often wish reveal little information possible avoid leaking information
competitors. describing central design principles support ICE mechanism,
highlight following aspects:
bidder interacts ICE first defining structured representation valuation different trades. Defined tree-based bidding language (TBBL),
concisely defines set trades interest bidder. bidder must annotate
tree initial lower upper bounds value different trades.
lower upper bounds valuations allows exchange identify
provisional trade provisional payments round, generate provisional clearing price item market. round ICE, bidder
required tighten bounds TBBL bid make precise trade
preferred given current prices.
ICE hybrid demand-revealing process direct-revelation mechanism, simple (linear) prices guiding preference elicitation bids submitted
direct claims valuation functions TBBL language,
expressive bids finally used clear exchange.
ICE terminates, payment rule used determine payments made,
received, participant. suggesting payments defined way
seeks mitigate opportunities manipulation exchange, ICE agnostic
particular payment rule adopted. given rule, prices quoted
round defined part approximate payments, aggregated across
provisional trade suggested bidder. concreteness, adopt Threshold
rule (Parkes et al., 2001) defining final payments, minimizes ex post regret
truthful bidding across budget-balanced payment rules, holding bids
participants fixed; see also work Milgrom (2007). 1 say
1. aware existence mechanism design solutions approximately efficient, truthful
(i.e., truthful bidding dominant-strategy equilibrium) budget-balanced, sealed bid (i.e.,
non iterative) CEs. Nevertheless, true payment rules developed leveraged
directly within ICE would allow ICE inherit truthful bidding (i.e., revising TBBL bounds
remain consistent bidders true valuation) ex post Nash equilibrium, achieved
iterative Generalized Vickrey auctions (Mishra & Parkes, 2007).

34

fiICE: Iterative Combinatorial Exchange

incentive issues related payment rules important design successful CEs.
Rather, orthogonal design ICE main focus work.
propose novel activity rules, designed mitigate opportunities
strategic behavior.
highlight following technical contributions made work:
tree-based bidding language (TBBL) extends earlier CA bidding languages support bidders wish simultaneously buy sell, specification valuation
bounds, use generalized choose operators provide concise representations OR* LGB (Boutilier & Hoos, 2001; Nisan, 2006). TBBL
directly encoded within mixed-integer programming (MIP) formulation
winner determination problem.
Despite quoting prices items bundles items, ICE able converge
efficient trade straightforward (i.e., non-strategic) bidders. Efficiency
established duality theory prices sufficiently accurate. Otherwise,
direct proof based reasoning upper lower valuation bounds always
available, even combinatorics instance preclude duality-based proof.
Preference elicitation performed combination two novel activity rules.
first modified revealed-preference activity rule (MRPAR), requires
bidder make precise trade preferred round. second
delta improvement activity rule (DIAR), requires bidder refine bid
improve price accuracy prove improvement possible. coupled
together rules ensure useful progress towards determining efficient trade
made round.
summarize, three main reasons prefer explicit value representations
repeated demand reports context iterative CE: (a) provisional allocation
computed round 1, since upper lower bounds value available, (b)
combinatorics domain directly handled clearing exchange efficiency
limited adopting simple (linear) prices, (c) proofs (approximate) efficiency
available reasoning directly bounds valuations despite adopting simple
(linear) prices.
exchange fully implemented Java (with C-based MIP solver). present
scalability results showing performance across wide number bidders, goods valuation complexity well benchmarks provide qualitative understanding
characteristics mechanism. experimental results (with straightforward bidders)
show exchange quickly converges efficient trade, taking average
7 rounds example domain 100 goods 20 different types 8 bidders
valuation functions containing average 112 TBBL nodes. domain,
find bidders leave upwards 62% maximum attainable value undefined
efficient trade known, 56% final payments determined, indicating
bidders able leave large amounts value space unrefined. exchange
terminates problems average 8.5 minutes 3.2GHz dual-processor dualcore workstation 8GB memory. includes time winner determination,
pricing, activity rules, well time simulate agent bidding strategies.
35

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

1.1 Related Work
Many ascending-price one-sided CAs known literature (Parkes & Ungar, 2000a;
Wurman & Wellman, 2000; Ausubel & Milgrom, 2002; de Vries, Schummer, & Vohra,
2007; Mishra & Parkes, 2007). Direct elicitation approaches, bidders respond
explicit queries valuations, also proposed one-sided CAs (Conen &
Sandholm, 2001; Hudson & Sandholm, 2004; Lahaie & Parkes, 2004; Lahaie, Constantin, &
Parkes, 2005). particular relevance ascending CAs designed work
simple prices items (Dunford, Hoffman, Menon, Sultana, & Wilson, 2003; Kwasnica,
Ledyard, Porter, & DeMartini, 2005). computing (approximately competitive) linear
prices, generalize extend methods. Building work Rassenti, Smith,
Bulfin (1982), earlier papers consider bids bundles individually, find prices
exact winning bids minimize pricing error losing bids. Generalizing
TBBL expressive language, propose instead compute prices minimize
worst-case pricing error bidders (rather bids individual trades), considering
preferred trade consistent TBBL bid bidder. work
Dunford et al. (2003) Kwasnica et al. (2005) incorporate additional tie-breaking
stages, case lexicographically minimize error find prices closely
approximate provisional payments. latter step appears novel.
Linear prices important practical applications. prices adopted
FCC wireless spectrum auctions (Cramton, 2006), within clock auctions
procurement electricity generation (Cramton, 2003), essential part
proposed design airport landing slot auction Laguardia airport (Ball et al. 2007).
Linear competitive equilibrium prices exist two-sided markets indivisibilities
assignment problem agent buy sell single item (but may
interested multiple different items) (Shapley & Shubik, 1972). general linear,
competitive equilibrium prices exist combinatorial markets nonconvexities;
see work Kelso Crawford (1982), Bikhchandani Mamer (1997), Bikhchandani
Ostroy (2002), ONeill, Sotkiewicz, Hobbs, Rothkopf, Stewart (2005) related
discussions.
ICE proxied architecture sense bidders submit refine bounds
TBBL bids directly exchange, information used drive price dynamics
ultimately clear exchange. Earlier work considered proxied approaches,
application one-sided ascending-price CAs (Parkes & Ungar, 2000b; Ausubel &
Milgrom, 2002). Given focus simple, linear prices, ICE considered provide
two-sided generalization clock-proxy design Ausubel, Cramton, Milgrom,
initial stage linear price discovery followed best-and-final sealedbid stage (Ausubel et al., 2006). Activity rules shown important
practice. instance, Milgrom-Wilson activity rule requires bidder
active minimum percentage quantity spectrum eligible
bid critical component auction rules used FCC wireless spectrum
auctions (Milgrom, 2004). ICE adopts variation clock-proxy auctions revealedpreference activity rule.
well known exact efficiency together budget balance possible
Myerson-Satterthwaite impossibility result (Myerson & Satterthwaite, 1983). Given
36

fiICE: Iterative Combinatorial Exchange

this, Parkes et al. study sealed-bid combinatorial exchanges introduced Threshold
payment rule (Parkes et al., 2001); see work Milgrom (2007) Day Raghavan (2007) recent discussion. Double auctions truthful bidding dominant strategy equilibrium known unit demand settings (McAfee, 1992) also
slightly expressive domains (Babaioff & Walsh, 2005; Chu & Shen, 2007). However,
truthful, budget-balanced mechanisms useful efficiency properties known
general CE problem.
Voucher-based schemes proposed alternative method extend onesided CAs exchanges (Kwerel & Williams, 2002). mechanisms collect goods
sellers run one-sided auction sellers buy-back
goods vouchers used provide seller share revenue collected
goods. Although voucher-based schemes facilitate design exchanges
one-sided auction technology, ICE design offers nice advantage providing equal
symmetric expressiveness participants. aware previous studies
fully expressive iterative CEs. Smith, Sandholm, Simmons previously studied iterative CEs, handle limited expressiveness adopt direct-query based approach
enumerative internal data structure scale (Smith et al., 2002).
novel feature earlier design (not supported here) item discovery, items
available trade need known advance. Earlier work also considered sealed-bid
combinatorial exchanges purpose contingent trades financial markets, including
aspects expressiveness winner determination (Saatcioglu et al., 2001).
Several bidding languages CAs previously proposed, arguably
compelling allow bidders explicitly represent logical structure valuation goods via standard logical operators. refer logical bidding
languages (Nisan, 2006). Closest generality TBBL LGB language (Boutilier &
Hoos, 2001), allows arbitrarily nested levels, combining goods trades
standard propositional logic operators, also provides k-of operator, used represent
willingness pay k trades quantifies over; see also work Rothkopf, Pekec,
Harstad (1998) restricted tree-based bidding language. key insight, Boutilier
specifies MIP formulation Winner Determination (WD) using LGB , provides positive empirical performance results using commercial solver, suggesting computational
feasibility moving expressive logical language (Boutilier, 2002). TBBL shares
structural elements LGB language important differences semantics. LGB , semantics propositional logic, items
allocation able satisfy tree multiple places. Although make LGB especially
concise settings, semantics propose provide representational locality,
value one component tree understood independently rest
tree.
1.2 Outline
Section 2 introduces preliminary concepts, defining efficient trade competitive equilibrium prices. Section 3 defines sealed-bid CE, introducing TBBL providing MIP
used solve winner determination. Section 4 extends TBBL allow valuation
bounds defines MRPAR DIAR activity rules. main theoretical results
37

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

also described well method determine price feedback round. Section 5
gives number illustrative examples operation ICE. Section 6 presents main
experimental results. conclude Section 7. Appendix provides algorithm
two activity rules together details bidding logic used simulated
bidding agents.

2. Preliminaries
basic environment considers set bidders, N = {1, . . . , n}, interested
trading multiple units distinct, indivisible goods, set different types goods
denoted G = {1, . . . , m}. bidder initial endowment goods valuation
different trades. Let x0 = (x01 , . . . , x0n ) denote initial endowment goods,
x0i = (x0i1 , . . . , x0im ) x0ij Z+ indicate number units good type j G
initially held bidder N . trade = (1 , . . . , n ) denotes change allocation,
= (i1 , . . . , imP
)
ij Z denoting change number units item j
P
bidder i. Let = jG x0ij denote total supply exchange. write
denote bidder active trade, i.e., buys sells least one item.
2.1 Efficient Trade
bidder value vi (i ) R component trade . value positive
negative, represents change value final allocation x0i +
initial allocation x0i . valuation initial allocation information private
bidder, assume externalities, bidders value depends

individual trade. assume free disposal,
P vi (i ) vi (i ) trade


, i.e., ij ij j. Let v() = vi (i ).
Utility modeled quasi-linear, ui (i , p) = vi (i ) p trade payment
p R. implies bidders modeled risk neutral assumes
budget constraints. payment, p, negative, indicating bidder
may receive payment trade. use term payoff interchangeably utility.
quasi-linearity, Pareto optimal (i.e., efficient) trade maximize social
welfare, equivalent total increase value bidders due trade.
Given instance CE problem, defined tuple (v, x0 ), i.e., valuation profile
v = (v1 , . . . , vn ) initial allocation x0 = (x01 , . . . , x0n ), efficient trade , defined
follows:
Definition 1 Given CE instance (v, x0 ), efficient trade solves
X
max
vi (i )
(1 ,...,n )

s.t.

(1)



ij + x0ij 0,
X
ij = 0,

i, j

(2)

j

(3)



ij Z
Constraints (2) ensure bidder sells items initial allocation.
free disposal, impose strict balance supply demand goods
38

fiICE: Iterative Combinatorial Exchange

solution constraints (3), i.e., allocate unwanted items bidder. adopt
F(x0 ) denote set feasible trades, given constraints given initial
allocation x0 , Fi (x0 ) set feasible trades bidder i. Note valuation
function vi cannot explicitly represented value possible trade bidder i,
number trades scales O(sm ), maximal number units
item market different items. TBBL language (introduced
Section 3) leads concise formulation efficient trade problem mixed-integer
program.
initial allocation x0i may private agent i. assume throughout bidders
truthful revealing information, motivate supposing participants
cannot sell items actually (or pay suitably high penalty do).
2.2 Competitive Equilibrium Prices
Linear prices, = (1 , . . . , ), define
Pa price j good price bidder
trade defined p (i ) = j ij j = . prices play important role
ICE. particular interest set competitive equilibrium prices:
Definition 2 Linear prices competitive equilibrium (EQ) prices CE problem
(v, x0 ) feasible trade F(x0 ) that:
vi (i ) p (i ) vi (i ) p (i ),

Fi (x0 ),

(4)

every bidder i. say trade, , supported prices .
Theorem 1 (Bikhchandani & Ostroy, 2002) trade supported competitive equilibrium prices efficient trade.
practice, exact EQ prices unlikely exist. Instead, useful define
concept approximate EQ prices approximately efficient trade:
Definition 3 Linear prices -approximate competitive equilibrium (EQ) prices
CE problem (v, x0 ) R0 , feasible trade F(x0 ) that:
vi (i ) p (i ) + vi (i ) p (i ),

Fi (x0 ),

(5)

every bidder i.
-approximate EQ prices, trade every bidder within 0
maximizing utility. Furthermore, say trade z-approximate total value
trade within z total value efficient trade.
Theorem 2 trade supported -approximate EQ prices 2 min(M, n2 )approximate efficient trade.
Proof: Fix instance (v, x0 ) consider (, ). trade 6=
X
X
[vi (i ) p (i )],
[vi (i ) p (i ) + ]




39

(6)

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

-EQ prices
values P
prices zero bidders P
notP
participate
P

=
( ) = 0 (since
( ) =
p
p
p


trade.






=


P
Pi
P
P P
0 )). Then,
ij = 0, ij = 0 j, F(xP
= j j P
Pi j ij jP




(i ) + vi (i ). Fix := , efficient trade . Then, vi (i ) +
viP
vi (i ),
=

X

n
min(2A#(x0 ), n) min(2 min(M, n), n) = 2 min(M, )
2


(7)



A#(x0 ) maximal number bidders trade feasible trade given x0 .
second inequality follows bidders trade number
goods trade bidders market thus A#(x0 ) min(M, n).


3. Step One: TBBL-Based Sealed-Bid Combinatorial Exchange
first flesh details non-iterative, TBBL-based CE bidder
submits sealed bid TBBL language.
Bidding language. tree-based bidding language (TBBL) designed expressive
concise, entirely symmetric respect buyers sellers, easily provide
bidders buying selling goods; i.e., ranging simple swaps highly
complex trades. Bids expressed annotated bid trees, define bidders change
value possible trades. main feature TBBL general intervalchoose logical operator internal nodes coupled rich semantics propagating
values within tree. Leaves tree annotated traded items nodes
annotated changes values (either positive negative). TBBL designed
changes value expressed trades rather total value allocations.
Examples provided Figures 1 2.
Consider bid tree Ti bidder i. Let Ti denote node tree, let
vi () R denote value specified node (perhaps negative). Let Leaf (Ti ) Ti
subset nodes representing leaves Ti let Child () Ti denote children
node . nodes except leaves labeled interval-choose operator ICyx ().
leaf labeled buy sell, units qi (, j) Z good j associated
leaf , qi (, j ) = 0 otherwise. good j may simultaneously occur multiple
leaves tree, given semantics tree described below.
IC operator defines range number children be, must
be, satisfied node satisfied: ICyx () node (where x non-negative
integers) indicates bidder willing pay satisfaction least x
children. suitable values x operator include many
logical connectors. instance: ICnn () node n children equivalent
operator; ICn1 () equivalent operator; IC11 () equivalent XOR
operator.2
say satisfaction ICyx () node defined following two rules:
2. equivalence implies TBBL directly express XOR, XOR/OR languages (Nisan,
2006).

40

fiICE: Iterative Combinatorial Exchange

R1 Node ICyx () may satisfied least x children
satisfied.
R2 node satisfied, none children may satisfied.
One consider R1 first pass defines set candidates satisfaction.
candidate set refined R2. Besides defining value propagated, virtue
R2 logical operators act constraints trades acceptable provide
necessary sufficient conditions.3
Given tree Ti , (change in) value trade defined sum values
satisfied nodes, set satisfied nodes chosen provide maximal total
value. Let sat () {0, 1} denote whether node tree Ti bidder satisfied,
sat = {sat (), Ti }. solution sat valid tree Ti trade , written
sat valid (Ti , ), rules R1 R2 must hold internal nodes {Ti\Leaf (Ti )}
ICyx ():
x sat ()

X

sat ( ) sat ()

(8)

Child()

Equation (8) enforces interval-choose constraints, ensuring
less appropriate number children satisfied node satisfied.
constraint also ensures time node root satisfied, parent
also satisfied. require, sat valid (Ti , ), total increase quantity
item across satisfied leaves greater total number units awarded
trade:
X

qi (, j)sat () ij ,

j G

(9)

Leaf (Ti )

free disposal, allow trade assign additional units item overand-above required order activate leaves bid tree. works sellers
well buyers: sellers trade negative requires total number
items indicated sold tree least total number items traded away
bidder trade.
Given constraints, total value trade , given bid-tree Ti bidder i,
defined solution optimization problem:
vi (Ti , ) = max
sat

X

vi ()sat ()

(10)

Ti

s.t. (8), (9)
Example 1 Consider airline operating slot-controlled airport already owns
several morning landing slots, none evening. order expand business
airline wishes acquire least two possibly three evening slots. However,
needs offset cost purchase selling one morning slots. Figure 1 shows
TBBL valuation tree expressing kind swap.
41

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes



IC3
2

Buy 4pm $1

Buy 6pm $4

XOR

Buy 8pm $3

Sell 5am $-2

Sell 7am $-9

Sell 9am $-5

Figure 1: simple TBBL tree airline interested trading landing slots.
working numerous examples frequently found cumbersome
capture even simple trades languages specified values allocations, case
existing languages. Indeed, earlier work, demonstrate natural instances
TBBL exponentially concise OR* LGB (Cavallo et al. 2005).
fact, TBBLs conciseness incomparable OR* LGB extended simple
ways strictly dominate earlier languages.
Winner Determination. problem determining efficient trade given bids
called winner determination (WD) problem. WD problem CAs (and thus also
CEs) NP-hard (Rothkopf et al., 1998). approach adopt formulate problem mixed-integer program (MIP), solve branch-and-cut algorithms (Nemhauser & Wolsey, 1999). similar approach proved successful solving
WD problem CAs (de Vries & Vohra, 2003; Boutilier, 2002; Sandholm, 2006).
Given tree Ti , useful adopt notation denote node Ti
satisfied trade . formulate WD problem bid trees = (T1 , . . . , Tn )
initial allocation x0 :
WD(T, x0 ) : max
,sat

XX


vi ()sat ()

Ti

s.t. (2), (3)
sat valid (Ti , ),



sat () {0, 1}, ij Z,
sat = (sat 1 , . . . , sat n ). tree structure made explicit MIP formulation:
decision variables represent satisfaction nodes capture logic
TBBL language linear constraints; related approach approach
considered application LGB (Boutilier, 2002). this, O(nB + mn)
variables constraints, B maximal number nodes bid tree.
formulation determines trade simultaneously determining value bidders
activating nodes bid trees.
Payments. Given reported valuation functions v = (v1 , . . . , vn ) bidder,
Vickrey-Clarke-Groves (VCG) (e.g. Krishna, 2002) mechanism collects following pay3. R1 naturally generalizes approach taken LGB , internal node satisfied according
operator subset children satisfied. semantics LGB , however, treat logical
operators way specifying added value (positive negative) results attaining
combinations goods. use R2 also imposes constraints acceptable trades.

42

fiICE: Iterative Combinatorial Exchange

ments bidder:
pvcg,i = vi (i ) (V (v) Vi (v)),

(11)

efficient trade, V (v) reported value trade Vi (v)
reported value efficient trade economy without bidder i, vi =
(v1 , . . . , vi1 , vi+1 , . . . , vn ). Let us refer vcg,i = V (v) Vi (v) VCG discount.
problem VCG mechanism context CE may run budget deficit total payments negative. alternative payment method provided
Threshold rule (Parkes et al., 2001):
pthresh,i = vi (i ) thresh,i ,

(12)

discounts thresh,i P
picked minimize maxi (vcg,i thresh,i ) subject
thresh,i vcg,i thresh,i V (v). Threshold payments exactly budget
balanced minimize maximal deviation VCG outcome across balanced
rules.
Bidder 1
Bidder 2



IC3
1
XOR

Buy C $6
Buy $10

Sell $-4

Buy B $5

Sell C $-3

Sell B $-8

Figure 2: Two bidders three items {A, B, C}. efficient trade bidder 1 sell
buy C.

Example 2 Consider two bidders Figure 2. Bidder 1 potentially sell one
items (A B) get Bidder 2s item, C, right price. Bidder 2 interested
buying one Bidder 1s items also selling item. consider
possible trades: Bidder 1 trades C gets $2 value Bidder 2 gets $7.
Bidder 1 trades B C gets $-2 value Bidder 2 gets $2. trade occurs
bidders get $0 value. Therefore efficient trade swap C.
efficient trade creates surplus $9 removing either bidder results
null trade, bidders Vickrey discount $9. Thus use VCG payments,
Bidder 1 pays $2-$9=$-7 Bidder 2 pays $7-$9=$-2 exchange runs deficit.
Threshold payment rule chooses payments minimally deviate VCG
maintaining budget balance. minimization reduces discounts $4.50, thus
Bidder 1 pays $2-$4.50=$-2.50 Bidder 2 pays $7-$4.50=$2.50.

4. Step Two: Making Exchange Iterative
defined sealed-bid, TBBL-based exchange modify design make
iterative. Rather provide exact valuation interesting trades, bidder
43

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

Figure 3: ICE system overview
annotates single TBBL tree upper lower bounds valuation. ICE
mechanism proceeds rounds, illustrated Figure 3.
ICE proxied design bidder proxy facilitate valuation
refinement. round, bidder responds prices interacting proxy agent
order tighten bounds TBBL tree meet activity rules. exchange
chooses provisional valuation profile (denoted v = (v1 , . . . , vn ) figure),
valuation vi bidder picked fall within bidders current valuation bounds
(and tend towards lower valuation bound progress made towards determining
final trade). Then, exchange computes provisional trade checks whether
conditions moving last-and-final round satisfied. Approximate equilibrium
prices computed based valuation profile v trade new round
begins. last-and-final round, final payments trade computed terms
lower valuations; semantics lower bounds guarantee bidder
willing pay least amount (or receive payment amount) order
complete trade.
Let v v denote lower upper valuation functions reported bidder
particular round ICE, adopt WD(v) denote WD problem valuation
profile v = (v1 , . . . , vn ). ICE parameterized target approximation error (0, 1],
requires total value optimal trade given current lower-bound
valuation profile (i.e., solves WD(v)) close total value efficient trade :
P
v()

vi (i )
P
EFF() =
) = v( )
v
(


(13)

However, true valuation v thus trade uncertain within ICE thus
later introduce techniques estblish bound.
round, ICE goes following steps:
1. last-and-final round, implement trade solves WD(v)
collect Threshold payments defined valuations v. STOP.
ELSE,
44

fiICE: Iterative Combinatorial Exchange

2. Solve WD(v) obtain . Use valuation bounds prices determine lowerbound, eff , allocative efficiency EFF() . eff next
round designated last-and-final round.
3. Set [0, 1], tending 1 eff tends 1, provisional valuation profile
v = (v1 , . . . , vn ), vi (i ) = v (i )+(1)v (i ), expressed TBBL tree
value node Ti vi () = v () + (1 )v ().
4. Solve WD(v ) find provisional trade , determine Threshold payments
provisional valuation profile, v .

5. Compute linear prices, Rm
0 , approximate CE prices given valuations v
trade , breaking ties best approximate provisional Threshold payments
finally minimize difference price items.

6. Report (i , ) bidder N , whether next round last-andfinal.
transitioning next round, proxy agents responsible guiding bidders
make refinements lower- upper-bound valuations order meet activity
rules ensure progress towards efficient trade across rounds. follows, (a)
extend TBBL capture lower upper valuation bounds, (b) describe two activity
rules, (c) explain compute price feedback, (d) provide main theoretical results.
developing theoretical experimental results ICE assume straightforward
bidders, bidders refine upper lower bounds valuations keep true
valuation consistent bounds.
Extending TBBL. first extend TBBL allow bidder report lower upper
bound (v (), v ()) value node Ti , turn induces valuation
functions v (Ti , ) v (Ti , ), using exact semantics (10). bounds
trade interpreted bounding payment bidder considers acceptable.
bidder commits complete trade payment less equal lowerbound refuse complete trade payment greater upper-bound.
exact value, thus true willingness-to-pay, remains unknown except v () = v ()
nodes. say bid-tree Ti bidder well-formed v () v ()
nodes Ti . case also v (Ti , ) v (Ti , ) trades . refer
difference v () v () value uncertainty node . efficient trade
often determined partial information bidder valuations. Consider
following simple variant Example 2:
Example 3 structure bidders trees Figure 4 Example 2
nodes annotated bounds. Let x [3, 8] denote Bidder 1s true value buy
C [4, 1] denote Bidder 2s true value sell C. three feasible trades
are: (1) trade C, (2) trade B C, (3) trade. first trade already provably
efficient. Fixing x y, minimal value 4+9+xy least 5+7+xy,
value second trade. Moreover, worst-case value 4 + 9 + 3 4 0, value
null trade.
45

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

Bidder 1
Bidder 2



IC3
1
XOR

Buy C $8
$3
Buy $12
$9

Sell $-3
$-4

Sell B

Buy B $7
$3

Sell C $-1
$-4

$-5
$-10

Figure 4: Two bidders, partial value information defined bid tree. One
already prove efficient trade bidder 1 sell buy C.
4.1 Activity Rules
Activity rules used guide preference elicitation process round ICE.
Without activity rule, rational bidder would likely wait last moment
revise valuation information, free-riding price discovery enabled bids
participants. every bidder behave way exchange would reduce
sealed-bid mechanism lose desirable properties.4 Thus, activity rules critical
mitigating opportunities strategic behavior.5
ICE employs two activity rules. presenting activity rules, specify
explicit consequences failing meet activity rule. One simple possibility
default action automatically set upper valuation bound every node bid
tree maximum provisional price node6 lower-bound value
node. entirely analogous bidder ascending-clock auction stops
bidding price: permitted bid higher price future rounds.
Modified Revealed-Preference Activity Rule (MRPAR). first rule, MRPAR,
based simple idea. require bidders refine valuation bounds round,
trade optimal (i.e., maximizes surplus) bidder given
current prices possible valuations consistent bounds. MRPAR loosely
based around revealed-preference based activity rule, advocated clock-proxy
auction one-sided CA (Ausubel et al., 2006).
Let vi Ti TBBL tree Ti denote valuation vi consistent value bounds
tree. bounds tight everywhere, vi exactly valuation function
defined tree Ti . simple variant (RPAR), requires enough information
valuation bounds establish one trade weakly preferred trades
prices, i.e.
Fi (x0 ) s.t. vi (i ) p (i ) vi (i ) p (i ),

vi Ti , Fi (x0 )

(RPAR)

Note bidder always meet rule defining exact valuation vi tight
value bounds every node bid tree; case, trade arg maxi Fi (x0 ) [vi (i )
4. problem evocatively described snake grass problem. See Kwerels forward
Milgroms book (2004).
5. conflict assumption straightforward bidding: design strategic
case despite assuming straightforward bidding provide tractable theoretical experimental
analysis; moreover, presence activity rules helps motivate straightforward bidding.
6. provisional price node defined minimal total price across feasible trades
subtree rooted node satisfied.

46

fiICE: Iterative Combinatorial Exchange

p (i )] satisfies RPAR. say prices strict EQ prices (v , ) when:
vi (i ) p (i ) > vi (i ) p (i ),

Fi (x0 ) \ {i } ,

(14)

every bidder N .
Theorem 3 prices strict EQ prices provisional valuation profile v trade
, every bidder retains vi bid tree meeting RPAR, trade
efficient bidders straightforward.
Proof: Fix bidder i. Let denote trade satisfies RPAR. vi consistent
revised bid tree bidder i, have:
vi (i ) p (i ) vi (i ) p (i ),

Fi (x0 ).

(15)

Moreover, must = , vi (i ) p (i ) > vi (i ) p (i )
strictness prices. Instantiating RPAR trade, true valuations vi Ti
(since bidders straightforward), have:
vi (i ) p (i ) vi (i ) p (i ),

Fi (x0 ),

(16)

prices p EQ prices respect true valuations. efficiency claim
follows welfare theorem, Theorem 1.

particular, provisional trade efficient given strict EQ prices every bidder
meets rule without modifying bounds way. Strict EQ prices required
prevent problems involving ties:
Buyer

Seller

XOR

Buy $8



$4
Buy B $4
$2

$-6
Sell $-9
$-20

$-2
Sell B $-6
$-10

Figure 5: Example illustrate failure simple RPAR rule without strict EQ
prices. True values shown bold efficient outcome
trade.
Example 4 TBBL trees shown Figure 5 trade occur truthful
valuation (which indicated bold value bounds). However, suppose = 0
provisional valuations efficient traded. Prices = (6, 2) EQ
(but strict EQ) prices given v , buyer indifferent buying
buying B seller indifferent selling A, selling B, making sale.
buyer passes RPAR without changing bounds bounds already establish
(weakly) prefers B, prefers trade, possible valuations.
Similarly, seller passes RPAR without changing bounds bounds establish
weakly prefers trade selling combination B given current
prices. Thus, activity even though current provisional trade inefficient.
47

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

order better handle sorts ties, slightly strengthen RPAR modified
RPAR (MRPAR), requires exists Fi (x0 )
(i , , vi ) 0,
either =





(i , , vi )

> 0,

vi Ti , Fi (x0 )
vi

Ti .

(17)
(18)

(i , , vi ) = vi (i ) p (i ) (vi (i ) p (i )) denotes profit bidder
trade given vi prices . (17) RPAR additional requirements enforce
satisfying trade either strictly preferred . need show
strict preference prevents deadlock shown Example 4. seller shown
weak preference trading selling A. MRPAR, seller must also
show strictly prefers , case reducing upper-bounds
B, thus ensuring progress.
actual rule adopted ICE -MRPAR, parameterized accuracy parameter
0, providing relaxation MRPAR useful even exact
EQ prices defined respect ( , v ) round.
Definition 4 Given provisional trade , linear prices , accuracy parameter 0,
-MRPAR requires every bidder refines value bounds TBBL tree Ti
satisfies:
(i , , vi ) ,

vi Ti , Fi

(19)

or, Fi (x0 )
(i , , vi ) 0,

vi Ti , Fi (x0 )

(20)

(i , , vi )

vi

(21)

> ,

Ti

simple matter check -MRPAR reduces MRPAR = 0. Phrasing
description allow rule interpreted without relaxation,
-MRAPR requires bidder must adjust valuation bounds establish
provisional trade [within ] maximizing profit possible valuations (19),
trade satisfies RPAR (20) strictly preferred [by least ] provisional
trade (21). RPAR, one show bidder always meet -MRPAR (for
) defining exact valuation.7
Lemma 1 every bidder meets -MRPAR without precluding vi updated bid
tree, prices -approximate EQ prices respect provisional valuation profile
v trade , bidders straightforward, provisional trade
2 min(M, n2 )-approximate efficient trade.
7. Let vi denote valuation. -MRPAR satisfied via (19) arg maxi Fi (x0 ) [vi (i )
p ()] satisfy -MRPAR. satisfies (20) construction. Now, let denote trade









vi (i ) p (i ) > vi (
) p (i ) + . vi (i ) p (i ) vi (i ) p (i ) > vi (i ) p (i ) + ,
(21).

48

fiICE: Iterative Combinatorial Exchange

Proof: Fix bidder i. -EQ, (i , , vi ) Fi (x0 ). Consider
6= . vi remains bid tree, must (i , , vi )
-MRPAR cannot satisfied via (20) (21). Therefore, -MRPAR satisfied every
bidder via (19) provisional trade satisfying trade. Therefore prove
prices, , -approximate EQ prices valuations, including true valuation
since bidders straightforward within bounds. efficiency trade
follows Theorem 2.

turn provides simple proof efficiency ICE approximate CE
prices exist upon termination. Suppose ICE defined terminate soon prices
-accurate v retained bid tree bidders meeting activity rule,
quiescence reached bidder refines bounds meeting rule.
variation, provisional trade trade finally implemented.
Theorem 4 ICE -MRPAR 2 min(M, n2 )-efficient prices -accurate
respect (v , ) upon termination bidders straightforward.
Proof: ICE terminates either (a) prices -accurate v retained
bid tree bidders appeal directly Lemma 1, (b) bidder refines
bounds meeting -MRPAR, case vi remains space valuations consistent
bid tree bidder.

also following simple corollary, considers property ICE
domain approximately accurate EQ prices exist:
Corollary 1 ICE -MRPAR 2 min(M, n2 )-efficient -accurate competitive
equilibrium prices exist valuations valuation domain bidders
straightforward.
Specializing domains exact EQ prices exist (e.g., unit-demand preferences
assignment model Shapley Shubik, 1972; see also work Bikhchandani
Mamer, 1997) ICE MRPAR efficient straightforward bidders.
XOR

Buy $8
$2

Buy B $5
$4

XOR

Buy $8=v
$2=x

(a) Passes -MRPAR

Buy B $8=y
$4=w

(b) Fails -MRPAR

Figure 6: -MRPAR provisional trade Buy A, = 3, B = 4 = 2
Example 5 illustrate -MRPAR rule consider single bidder valuation tree
Figure 6(a). Suppose provisional trade allocates bidder, prices
= 3, B = 4 = 2. bidder satisfied -MRPAR guaranteed
$2-$3=$-1 payoff within possible $5-$4=$1 payoff B. consider
Figure 6(b), relaxed upper-bound buy B $8. bidder fails -MRPAR
guaranteed $-1 payoff within possible payoff B
$8-$4=$4. Let [x, v] [w, y] denote lower upper bounds, buy buy
B respectively, revised meeting rule. pass rule, bidder two choices:
49

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

Demonstrate best response. bidder need adjust x
make x 3 4 2 x 3; e.g., values x = $2, = $5 solve this,
Figure 6(a), many possibilities.
Demonstrate another trade (e.g., buy B) $2 better ,
i.e., w 4 > v 3 + 2 w v > 3, buy B weakly better null trade,
i.e., w 4 0. instance, bidders true values vA = $3, vB = $8
x 3 v w 8 rule cannot satisfied first case. But,
buyer establish buy B best-response, e.g., setting v = $4, w = $7,
v = $3, w = $6.
Remark: Computation Bidder Feedback. definition MRPAR naively
suggests checking compliance requires explicitly considering valuations vi Ti
trades Fi (x0 ). Fortunately, necessary. present Appendix
method check MRPAR given prices , provisional trade bid tree Ti solving three
MIPs. Moreover, explain solution MIPs also provides nice feedback
bidders. ICE automatically identify set nodes bidder needs increase
lower bound set nodes bidder needs decrease upper bound
meeting MRPAR.
Delta Improvement Activity Rule (DIAR). -MRPAR, quite possible
ICE get stuck, bidders satisfying activity rule without changing
bounds, prices less accurate (with respect ( , v )). Therefore,
need activity rule continue drive reduction value uncertainty, i.e.,
gap upper bound values lower bound values, even face inaccurate
prices, ideally way remains price-directed sense using prices
determine trades (and turn nodes TBBL trees) bidder
focused on.
introduce purpose second (and novel) activity rule (DIAR), fills
role requiring bidders reveal information improve price accuracy and,
limit, full information nodes matter. Defined way, DIAR rule
nicely complements -MRPAR rule. establish efficiency
provisional trade directly via valuation bounds, see Section 4.3,
actually need fully accurate prices order close exchange. Thus, DIAR
rule imply bidders reveal full information. Rather, presence DIAR
ensures good performance practice well good theoretical properties.
experiments enable DIAR rounds ICE, fires parallel -MRPAR.
practice, see progress refining valuation information occurs due
-MRPAR, progress early rounds occurs due -MRPAR. Experimental
support provided Section 6.8
providing specifics DIAR, identify node Ti bid tree
bidder interesting fixed instance (v, x0 ), node satisfied
feasible trade. following simple lemma:
8. variation way ICE defined, DIAR could used rounds price error
provisional valuation trade greater error associated -MRPAR.
-MRPAR sufficient approximate efficiency prices accurate enough.

50

fiICE: Iterative Combinatorial Exchange

Lemma 2 value uncertainty interesting nodes bid trees
biders, bidders straightforward, efficient.
Proof: value uncertainty thus exact information value interesting
nodes implies difference value exactly known pairs feasible
trades uninteresting nodes, either node never satisfied trade
(and thus value matter) node satisfied every trade thus actual
value matter defining difference value pairs trades.
difference value pairs trades important determining efficient trade.

DIAR focuses bidder particular interesting nodes correspond trades
pricing error large, error could still reduced refining
valuation bounds node. Given prices provisional trade , main focus
k
DIAR following upper-bound , amount prices might misprice
trade ki Fi (x0 ) respect bidder true valuation:
k

= max
[vi (ki ) p (ki ) (vi (i ) p (i ))]

vi Ti

(22)

call DIAR error trade ki , note depends current
prices well current bid tree provisional trade, true valuation
unknown center. DIAR error provides upper bound additional payoff
bidder could achieve trade ki trade . order trades, 1i , 2i , . . .,
1
1i maximal DIAR error, , = maxi Fi (x0 ) [vi (i ) p (i )
(vi (i ) p (i ))] pricing error respect provisional trade provisional
valuation profile. error pricing algorithm designed minimize
round, error used Theorem 2 reference -accurate prices.
Thus, see maximal DIAR error also bounds amount prices
1
approximate EQ prices, 0 bidders current prices
exact EQ prices respect ( , v ).
satisfy DIAR bidder must reduce DIAR error trade largest error
error reduced (some error may intrinsic given current prices
uncertainty bidders valuation), establish providing exact
value information throughout tree none DIAR error trades due
value uncertainty. Figure 7 illustrates difference MRPAR DIAR.
bidder satisfy MRPAR making clear lower bound payoff
trade greater upper bound trades, still leave large uncertainty
value. DIAR requires bidder also refine upper bound node
corresponds trade DIAR error (and thus potentially actual
approximation prices) large. rule illustrated Figure 8.
DIAR parameterized 0. refer formal rule -DIAR:
Definition 5 satisfy -DIAR given provisional trade prices , bidder must
modify valuation bounds to:
(a) reduce DIAR error trade, ji Fi (x0 ), least
k

(b) prove error cannot improved trades ki Fi (x0 ) 1 k < j,
k

(c) establish cannot improved trade ki Fi (x0 ).
51

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

Figure 7: Stylized effect MRPAR
DIAR bounds
trades

Figure 8: Trades bidder i, ordered
DIAR error reducing left
right. bidder must reduce,
least , DIAR error
trade greatest
error possible
prove (via valuation bounds)
impossible improve
trades larger error.

particular, even bidder case (c) above, still forced narrow
bounds progress made towards bounding efficiency. practice, define
parameter large start smaller later rounds.
XOR

$6
Buy $4
$2

$8
Buy B $5
$3

XOR

$10
Buy C $10
$4

$6
Buy $4
$2

(a) Fails DIAR

$7
Buy B $5
$3

Buy C

$10
$10
$9.01

(b) Passes DIAR

Figure 9: Respecting DIAR provisional trade Buy A, = 4, B = 5, C = 6
= 1
Example 6 Consider tree Figure 9(a) provisional trade buy A, prices
= ($4, $5, $6) DIAR parameter = 1. DIAR error trade, defined via
(22), listed decreasing order, are:
1

C = ($10 $6) ($2) = $6
2

B = ($8 $5) ($2) = $5
3

= ($0 $0) ($2) = $2
4

= ($2 $4) ($2) = $0,
$2 = $2 $4 worst-case profit provisional trade. Now, see
1
cannot made smaller lowering upper-bound leaf buy C bound
52

fiICE: Iterative Combinatorial Exchange

already tight truthful value $10. Instead bidder must demonstrate
decrease = 1 impossible raising lower bound buy C 9.01. However
2
decreased = 1, reducing upper-bound buy B 8 7, giving
us tree Figure 9(b).
Lemma 3 ICE incorporates DIAR, straightforward bidder must eventually reveal
complete value information interesting nodes bid tree 0.
Proof: Fix provisional trade consider trade, 1i Fi (x0 ) 6= , maximal
DIAR error. Continue assume straightforward bidders. Recall vi () denotes
bidders true value node TBBL tree. case analysis nodes Ti , meeting
DIAR rule trade 0 requires:
(i) Nodes 1i \ . Decrease upper-bound vi (), true value, reduce
error. Increase lower-bound vi () prove progress possible.
(ii) Nodes \ 1i . Increase lower-bound vi (), true value, reduce
error. Decrease upper-bound vi () prove progress
possible.
(iii) Nodes 1i . change required.
(iv) Nodes
/ 1i . change required.
Continue fix , consider impact DIAR 0
rule met successive trades, moving 1i 2i onwards. Eventually, value
bounds nodes
/ least one feasible trade driven truth
(i), value bounds nodes least one feasible trade
driven truth (ii). Noting null trade always feasible, bidder
ultimately reveal complete value information except nodes satisfied
feasible trade.

Putting together following simple theorem, considers convergence property ICE DIAR activity rule.
Theorem 5 ICE -DIAR rule terminate efficient trade
bidders straightforward 0.
Proof: Immediate Lemma 2 Lemma 3.

practice, use -MRPAR DIAR role DIAR ensure convergence instances exist good, supporting EQ prices. use
DIAR lead, case, full revelation bidder valuations
prove efficiency directly terms valuation bounds different trades (see Section 4.3).
Remark: Computation Bidder Feedback. present Appendix method
check -DIAR given prices , provisional trade , bidders bid tree past
round proposed new bid tree solving two MIPs. Moreover, solution
MIPs also provides nice feedback bidders. ICE automatically identify trade,
turn corresponding nodes bid tree, bidder must provide
information.
53

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

4.2 Generating Linear Prices
Given provisional trade , provisional valuations v , given provisional payments also determined (according payment rule, Threshold,
adopted exchange), approximate clearing prices computed round according following rules:
I: Accuracy (ACC). First, compute prices minimize maximal error
best-response constraints across bidders.
II: Fairness (FAIR). Second, break ties prefer prices minimize maximal
deviation provisional payments across bidders.
III: Balance (BAL). Third, break ties prefer prices minimize maximal
difference price across items.
Taken together, steps designed promote informativeness prices
driving progress across rounds. Balance well motivated domains items
likely similar value dissimilar, preferring prices similar across items
rejecting extremal prices. Note prices may ascend descend round
round general tend towards increasing accuracy, shall see
experimentally Section 6.
Buyer

Seller



Buy $8



Buy B $8

Sell $-6

Sell B $-6

Figure 10: simple example illustrate pricing. ACC prices AB $12 $16,
FAIR narrows $14 BAL requires = $7, B = $7
Example 7 Consider example Figure 10 one buyer interested buying AB
one seller interested selling AB. buyers sellers values item
8 -6 respectively. efficient outcome given values trade complete.
ACC requires 12 +B 16, thus allows range prices. Threshold payment
splits difference, buyer pays 14 seller FAIR adds constraint
+ B = 14. Finally, BAL requires = B = 7.
three stages occur turn. interest space, present
basic formulation Accuracy stage: define maximally accurate EQ prices first
considering following LP:

s.t.


acc
= min acc
,acc
X
X
j ij vi (i )
j ij + acc ,
vi (i )
j

j

acc 0,
j 0,

j G
54

i, Fi (x0 )

(23)

fiICE: Iterative Combinatorial Exchange

prices minimize maximal loss payoff across bidders trade compared trade bidder would prefer given provisional valuation v , i.e.,
minimize maximal value (i , , vi ), = arg maxi Fi (x0 ) [vi (i ) p (i )].
Prices solve LP refined lexicographically, fixing worst-case pricing error (ACC) working try additionally minimize next largest pricing
error on. Given maximally accurate prices, triggers series lexicographical refinements best approximate payments (FAIR) without reducing pricing
accuracy, eventually series lexicographical refinements try maximally balance prices across distinct items (BAL). addition improving quality
prices, process also ensures uniqueness prices.
Accuracy, Fairness Balance problems exponential number
constraints price accuracy constraints (23) (which carried forward
subsequent stages) defined trades Fi (x0 ) bidders i.
therefore infeasible even write problems down. Rather solve explicitly,
use constraint generation (e.g. Bertsimas & Tsitsiklis, 1997) dynamically generate
sufficient subset constraints. Constraint generation (CG) considers relaxed program
contains manageable subset constraints, solves optimality.
Given solution relaxed program, subproblem used either prove
solution optimal full program, find violated constraint full problem
introduced (now strengthened) relaxed program resolved. case
subproblem variation winner determination IP Section 3,
concisely formulated solved via branch-and-cut.9
4.3 Establishing Bounds Efficiency
Consider round ICE. round starts announcement prices, denote
, provisional trade. round ends every bidder met
-MRPAR -DIAR activity rules. question address is: established
efficiency trade defined lower-bound valuations end round?
perhaps unsurprising MRPAR sufficient provide efficiency claims
prices suitably accurate. interesting coupling MRPAR
DIAR ensures ICE converges provably efficient trade cases,
efficiency often established independently prices reasoning directly lower
upper valuation bounds. theoretical analysis convergence efficiency, assume
straightforward bidders, mean bidder always retains true valuation
within valuation bounds. (All results could equivalently phrased terms efficiency
claims respect reported valuations.)
closing round, ICE makes determination whether move
last-and-final round. Bidders notified occurs. last-and-final round
9. pricing step computationally intensive steps ICE therefore heavily optimized.
practice, found useful employ heuristics seed set constraints used CG.
also developed algorithmic techniques speed search appropriate set constraints
context lexicographic refinement: provisional Locking multiple lexicographic values
CG check, lazy constraint checks subset conditions CG routinely
checked, even though complete set eventually enforced. Please see technical report www.
eecs.harvard.edu/~blubin/ice complete details pricing method.

55

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

provides final opportunity bidders update lower valuation bound information
(without exceeding upper bounds). exchange finally terminates efficient
trade payments determined respect lower valuation bounds:
lower bounds considered beP
ultimate bid submitted bidder
ICE terminates. Let arg maxF (x0 ) v (i ) denote trade optimal given
lower bound valuations. explained Section 4, ICE parameterized target
approximation error, , providing lower-bound relative efficiency
efficient trade true valuations. challenge obtain useful bounds
relative efficiency EFF() trade . provide two methods, one price-based
uses duality theory second directly reasons bounds bidder
valuations. consider turn.
price-based proof efficiency. already seen Section 2.2 bound
efficiency provisional trade sometimes established via prices. provides
simple method establish bound efficiency trade . Fix 0. v
denoting provisional valuation profile start round t, corresponding
provisional trade, know
(a) bidders meet -MRPAR leaving v within bounds,
(b) prices -approximate EQ prices v ,
(c) equal , i.e., efficient trade given refined lower bound valuations,
trade
2 min(M, n2 )-approximation
efficient trade Theorem 2.
P
P
vi (i ) + 2 min(M, n2 ) vi (i ), then,
P
2 min(M, n2 )
2 min(M, n2 )
vi (i )
P
P
,

1



1

EFF() = P


maxF (x0 ) v ()
vi ( )
vi (i )

(24)

define price . Conditioned (ac) met, bound available,
satisfy price small enough parameter. bound available
set price := 0.
direct proof efficiency. also provide complementary, direct, method
establish relative efficiency working refined valuation bounds
end round t. First, given bid tree Ti , useful define perturbed valuation
respect trade , assigning following values node :

v () , sat (i )
vi () =
(25)
v () , otherwise,
sat (i ) node satisfied given tree Ti lower bound valuations
v nodes, given trade . valuation function vi associated TBBL tree Ti
defined minimize value nodes satisfied trade maximize value
nodes. concept, given valuation bounds, establish
following bound,




v()
v()
v ()
v()
,
(26)

min
= min
=
EFF() =




v( ) v T, F (x0 ) v ( )
F (x0 ) v( )
v()
56

fiICE: Iterative Combinatorial Exchange

Figure 11: Determining efficiency bound based lower upper valuations.

define direct . Notation v = (v1 , . . . , vn ), trade maximizes
P
vi (i ) across feasible trades. first inequality holds domain
minimization includes v trade = . first equality holds
6= , worst-case efficiency occurs value v selected minimize
value nodes \ , maximize value nodes \ , minimize value
shared nodes, . Whatever choice , valuation provided perturbed
valuation v. final equality, v() = v() definition, optimal trade
maximizes value denominator, i.e., trade . Figure 11 schematically
illustrates various trades values used bound, particular provides
graphical intuition v() v() v( ) v() = maxv [v ( ) v ()]
v( ) v().
Combining together. Given methods establish lower-bound eff =
max( price , direct ) relative efficiency trade . ICE defined move
last-and-final round either following hold:
(a) error bound eff
(b) trade even optimistic (i.e., upper-bound) valuations.
Combining Theorem 5, immediately get main result.
Theorem 6 ICE incorporates -MRPAR -DIAR bidders
straightforward, exchange terminates trade within target approximation error , 0 0.
use -DIAR sufficient establish result. However, use
prices MRPAR drives elicitation practice, particularly fix
-MRPAR tiny constant actual use. Empirical support this, along
quality price-based bound direct efficiency bounds, provided Section 6.
parameter -DIAR, find simple rule:
:=

1 X X v () v ()
,
2n
|Ti |


(27)

Ti

works well. tends towards zero value information revealed participants.
One last element design ICE precise method provisional
valuation profile v = v + (1 )v constructed. important
57

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

used determine provisional trade price feedback. simple approach works
well define := max(0.5, eff ). find lower bound 0.5 useful heuristic
early rounds eff likely small, making ICE adopt provisional valuation
middle valuation bounds much known. effect push
towards 1 thus v towards v efficiency bound eff improves.10

5. Illustrative Examples
section illustrate behavior exchange two simple examples.
examples provided give qualitative feel behavior. construct examples
populate ICE simple, automated bidding agents. agents use MIPguided heuristics minimize amount information revealed course passing
activity rules, maintaining true value within lower- upper-bounds
(i.e., act straightforward way). reluctance reveal information models
basic tenet design, costly participants refine reveal
information values different trades. detailed explanation operation
bidding agents provided Appendix.
section, also presenting main experimental results, move
last-and-final round. Rather, bidding agents programmed continue improve
bids past round efficiency already proved (and last-and-final
round would ordinarily declared), payments within desired accuracy
tolerance. avoid need program agents strategy bid
last-and-final round.
Prices
9.5

3.5

9


B
Efficient Allocation

8.5
3
8
2.5

7.5

Pessimistic
Alpha
Optimistic

2

Price

Average Allocation Value

Allocation Value
4

7
6.5

1.5

6
1
5.5
Provable
Efficiency

0.5
0
0

0.2

0.4
0.6
% Complete

Provable
Efficiency

5
0.8

4.5
0

1

(a) Allocative value

0.2

0.4
0.6
% Complete

0.8

1

(b) Prices

Figure 12: AgentA: $8, AgentB: B $8, AgentAB: B $10.
10. domains, may also important require payments (rather efficiency
trade ) accurate enough moving last-and-final round. bound payments
computed analogous way efficiency. Whether required practice likely
domain-specific depend, instance, whether payments tend accurate anyway
time trade approximately accurate, also impact strategic behavior.

58

fiICE: Iterative Combinatorial Exchange

Example 8 Consider market no-reserve seller two items B, three
buyers. AgentA demands value $8, AgentB demands B value $8,
AgentAB demands B value $10. Figure 12(a) shows quickly
exchange discovers correct trade. price $5 $8 accurate
situation, see prices Figure 12(b) quickly meet condition.
Fairness drives prices towards $6, eventual Threshold payments
AgentA AgentB. Balance ensures prices remain two items.

Prices

Allocation Value

25

0.7

20

0.5
0.4

15

Pessimistic
Alpha
Optimistic

0.3

Price

Average Allocation Value

0.6


B
Efficient Allocation

10

0.2

5
0.1
0
0

Provable
Efficiency

Provable
Efficiency
0.2

0.4
0.6
% Complete

0.8

0
0

1

(a) Allocative value

0.2

0.4
0.6
% Complete

0.8

1

(b) Prices

Figure 13: Seller -$10, Swapper: swap B $8, Buyer B $4
Example 9 Consider example Seller offering reserve $10, Swapper
willing pay $8 swap B A, Buyer willing pay $4 B.
complex example, takes 4 rounds, illustrated Figure 13(a), trade
found pessimistic trade. Revelation drives progress towards completed trade,
see Figure 13(b), reflected falling prices goods. Thus
see price feedback providing accurate information participants:
price eventually becomes low enough buying bidders actually want trade
occur also exchanges provisional trade switches. also worth
noting greater valuations Seller Swapper place good result net
higher price good B.

6. Experimental Analysis
section report results set experiments designed provide
proof-of-concept ICE. results illustrate scalability ICE realistic problem
sizes provide evidence effectiveness elicitation process techniques
bound efficiency provisional trade.
59

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

Implementation. First, brief aside experimental implementation. ICE approximately 20,000 lines extremely tight Java code, broken functional packages
described Table 1.11 prototype modular researchers may easily replace
components experimentation.12 ICEs complexity, essential
code constructed rigid hierarchy avoids obscuring high level logic behind
details generating, running integrating results MIPs. end,
system written series progressively abstractmini-languages
defines clean, understandable API next higher level logic. hierarchy provides
way hide extremely delicate steps needed handle numerical issues come
trying repeatedly solve coupled optimization problems, constraints
one problem may defined terms slightly inaccurate results earlier problem.
constraints presented paper must carefully relaxed monitored
order handle numerical precision issues. bottom hierarchy
MIP specification fed generalized back-end optimization solver interface13 (we
currently support CPLEX LGPL-licensed LPSolve), handles machine loadbalancing parallel MIP/LP solving. concurrent solving capability essential,
need handle tens thousands comparatively simple MIPs/LPs.
Component
Agent
Model
Bidding Language
Exchange Driver & Communication
Activity/Closing Rule Engines
WD Engine
Pricing Engine
MIP Builders
Framework & Instrumentation
JOpt
Instance Generator

Purpose
Strategic behavior information revelation decisions
XML support load goods true valuations
Implements TBBL
Controls exchange, coordinates agent behavior
MRPAR, DIAR Closing Rules
Logic WD
Logic three pricing stages
Translates engines optimization APIs
Wire components together & Gather data
Optimization API wrapping CPLEX
Random Problem Generator

Lines
2001
1353
2497
1322
1830
685
1317
2206
2642
2178
497

Table 1: Exchange components code breakdown

Experimental set-up. experiments, -parameter MRPAR set near
zero MRPAR DIAR activity rule fire every round. rule used
define -parameter DIAR exactly described Section 4.1. adopt
straightforward bidding agents employed Section 5 (see Appendix
details). simulation, adopt Threshold payment rule terminate ICE
per-agent error payment relative correct payment within 5% average
per-agent value efficient trade. typical instances, incurs additional 4
rounds beyond would required last-and-final round. timing
wall clock time, separately count large number parallel threads
execution system. experiments run dual-processor dual-core Pentium
11. Code size measured physical source line code (SLOC).
12. Please contact authors access source code.
13. http://www.eecs.harvard.edu/econcs/jopt

60

fiICE: Iterative Combinatorial Exchange

IV 3.2GHz 8GB memory CPLEX 10.1. results averaged 10 trials.
problem instances available http://www.eecs.harvard.edu/~blubin/ice.
instance generator begins generating set G good types. Next, j G
creates 1 copies good type, forming total potential supply market s|G|
goods (exactly many units supply depends precise structure bid trees).
unit assigned one bidders uniformly random. generator creates
bid tree Ti bidder recursively growing it, starting root adopting
two phases. tree depthLow, node receives number children drawn
uniform outDegreeLow outDegreeHigh (a percentage designated
leaves), resulting exponential growth number nodes phase.
width depth refer number nodes depth. point,
carefully control expected number children node order make
expected width conform triangle distribution depth depthLow depthMid
depthHigh: linearly increase expected width depth depthLow
depthMid fixed multiple () width depthLow, linearly decrease
expected width back zero depthHigh.14 provides complex deep trees without
inherently introducing exponential number nodes.
internal node must assigned parameters interval choose operator.
typically choose high-triangle distribution 1 number children
x low-triangle distribution 1 y. bias towards introduction
IC operators permit wide choice number children. internal node
also assigned bonus drawn according uniform distribution. leaf node assigned
buy node probability [0, 1], specific good type node
chosen among good types sale market. node assigned
quantity drawing low-triangle distribution 1 total number
existence.15 unit value node drawn specific buy distribution,
typically uniform, multiplied quantity assigned nodes bonus.
leaf nodes assigned sell nodes goods bonuses determined similarly,
time goods selected among previously assigned bidder.16
6.1 Experimental Results: Scalability
first set results present focuses computational properties ICE.
Figure 14 shows runtime performance system increase number
bidders holding parameters constant. example, 100 goods 20 types
traded bidders average 104 node trees. graph shows total
wall clock time parts system. see super-linear growth solve time
14. Note setting depthLow =depthMid =depthHigh one still grow full tree given depth
eliminating phase 2.
15. total number goods given type existence may actually available purchase
price given structure seller trees. Thus bias towards small quantities buy nodes large
quantities sell nodes produces interesting problem instances.
16. experiments, vary 2 |G| 128, 1 128, 2 |N | 20, 2 outDegreeLow 8,
2 outDegreeHigh 8, 2 depthLow 6, 2 depthMid 6, 2 depthHigh 8, set balanced buy
probability = 0.5, set width multiplier second phase = 2. examples,
buy node bonuses drawn uniformly [10, 100], sell nodes bonuses drawn uniformly
[100, 10] internal nodes bonuses uniformly [25, 25].

61

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

4

10

3000

Good Scalability (Mean 10 runs)

Agent Scalability (Mean 10 runs)

Concurrent CPU Time (s)

Concurrent CPU Time (s)

2500

2000

1500

1000

3

10

500

2

0
0

5

10

15

20

10 0
10

25

1

2

10

10

Agents

3

10

Good Types

Figure 14: Effect number bidders
run-time ICE

Figure 15: Effect number good
types run-time ICE

number bidders, constants growth markets large
numbers bidders efficiently solved (solving 20 bidders around 40 minutes).
error bars plots standard error statistic.
Figure 15 see effect varying number types goods (retaining
5 units good supply) computation time. example adopt
10 bidders, tree generation parameters. likely explanation eventual
concavity run-time performance suggested decrease average (item)
price upon termination ICE number types goods increased (see Figure 16).
average price provides good proxy competitiveness market. Adding
120
Mean Linear Price (Mean 10 runs)

Mean Linear Price

100

80

60

40

20

0 0
10

1

2

10

10

3

10

Goods

Figure 16: Effect number goods average item price upon termination
ICE.
62

fiICE: Iterative Combinatorial Exchange

4

4

10

10

3

3

10
Concurrent CPU Time (s)

Concurrent CPU Time (s)

10

2

10

1

10

2

10

1

10

Node Degree Scalability (Mean 10 runs)
0

10 0
10

Power law fit
1

0.4243 x

Tree Depth Scalability (Mean 10 runs)

1.59
0

2

10
10
Number Nodes Tree

10 0
10

3

10

Figure 17: Effect bid-tree size runtime ICE: Varying nodeout degree.

Power law fit
1

0.7553 x
2

10
10
Number Nodes Tree

1.45
3

10

Figure 18: Effect bid-tree size runtime ICE: Varying tree
depth.

goods problem initially make winner determination problem difficult,
large over-supply, point outcome easier determine.
Figures 17 18 illustrate change run time size bid trees.
use first phase tree-generator avoid confounding effects size
structural complexity. experiments, 100 goods 20 types traded 10
bidders. Figure 17 vary number children given node Figure 18
vary depth tree. Increasing branching factor and/or tree depth results
exponential growth tree size, necessarily corresponds exponential growth
runtime. However, account instead plotting number nodes
trees, see graphs indicate near-polynomial increase runtime
tree size. fit polynomial function data form = Axb , indicating
growth approximately degree 1.5 range tree sizes considered
experiments.
6.2 Empirical Results: Economic Properties
second set results present focus economic properties ICE:
efficiency trade across rounds, effectiveness preference elicitation, accuracy
stability prices. set experiments average 10 problem instances,
8 bidders, potential supply 100 goods 20 types, bid trees
average 104 nodes.
Figure 19 plots true efficiency trades computed pessimistic (lower bounds
v), provisional (-valuation v ) optimistic (upper bounds v) valuations across rounds.
graph follow, x-axis indicates number rounds completed
percentage total number rounds termination enables results
aggregated across multiple instances, different number total
63

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

100

10000

80

8000

70

7000

60
Mean
Provable 95%
Efficiency
SE

50
40
30

6000
5000
4000
Mean
Provable 95%
Efficiency
SE

3000
2000

Pessimistic
Alpha
Optimistic

20
10
0

MRPAR
DIAR

9000

Mean Slack Revealed

% Efficient

90

0.2

0.4
0.6
% Complete

0.8

1000
0
0

1

Figure 19: Efficiency optimistic, provisional, pessimistic trades
across rounds.

0.2

0.4
0.6
% Complete

0.8

1

Figure 20: Average reduction value uncertainty due rule.

rounds.17 vertical (dashed) line indicates average percentage complete
trade provably 95% efficient. exchange remains open past point payments
converge (and simulate outcome last-and-final round continuing
progress straightforward bidding agents). two lines either side represent
one standard error statistic.
Figure 19, see exchange quickly converges highly efficient trades, taking
average 6.8 rounds achieve efficiency. general, optimistic trade (i.e., computed
upper bounds v) higher (true) efficiency pessimistic one (i.e., computed
lower bounds v), efficiency provisional trade typically better
both. justifies design decision adopt provisional valuations provisional
trade driving exchange dynamics. also suggests exchanges traditional
paradigm improving bids (i.e., increasing lower bound claims valuations) would allow
little useful feedback early rounds: efficiency pessimistic tradeall would
available without information upper-bounds bidder valuationsis initially
poor.
Figure 20 shows average amount revelation caused MRPAR DIAR
round ICE. Revelation measured terms absolute tightening upper
lower bounds, summed across bid trees. MRPAR activity rule main driving
force behind revelation information vast majority revelation (in absolute
terms) occurs within first 25% rounds. DIAR plays role making progress towards
identifying efficient trade MRPAR substantially reduced value
uncertainty despite firing every round. One think MRPAR rockets
main engine, DIAR thruster mid-course correction. ICE determines efficient
17. data point represents average across 10 instances, determined averaging
underlying points neighborhood. Error-bars indicate standard error (SE) mean. Thus,
figures essentially histogram rendered line graph.

64

fiICE: Iterative Combinatorial Exchange

80

30
Price Volatility (Mean 10 runs)

% Regret Price (Mean 10 runs)

70

% Regret Price

% Difference final price

25
60
50
40
30

Mean
Provable 95%
Efficiency
SE

20

20

15

Mean
Provable 95%
Efficiency
SE

10

5

10
0
0

0.2

0.4
0.6
% Complete

0.8

0
0

1

Figure 21: Price trajectory: Closeness
prices round final
prices

0.2

0.4
0.6
% Complete

0.8

1

Figure 22: Regret best-response bidders due price inaccuracy relative final prices.

trade average node TBBL tree still retains gap upper lower
bounds value node equal around 62% maximum (true) value node
could contribute bidders value, roughly maximum marginal value contributed
node feasible trades. see ICE successful directing preference elicitation
information relevant determining efficient trade.
provide two different views effectiveness prices. Figure 21 shows
mean percentage absolute difference prices computed round
prices computed final round. Prices quickly converge. experiments
driven exchange beyond efficient solution order converge Threshold
payments, see price information already available point
efficiency. Figure 22 provides information quality price feedback. plot
regret, averaged across bidders runs, best-response trade determined
intermediate prices comparison best-response final prices, regret
defined terms lost payoff final prices. Define regret bidder
best response = arg maxi Fi (x0 ) [vi (i ) p (i )], prices , given final prices
, as:


) p ( )
v
(


100%.
Regreti (i , ) = 1
(28)
max vi (i ) p (i )
Fi (x0 )

payoff trade , evaluated prices , approaches bestresponse trade prices , Regreti (i , ) approaches 0%. Figure 22 plots average
regret across bidders function number rounds completed ICE. regret
low: 11.2% averaged across rounds efficient trade determined
7.0% averaged across rounds. regret falls across rounds also shows prices
become informative rounds proceed.
65

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

100
90
80

% Efficient

70
60
50
40
30
20
10
0
0

Closing Rule Prediction
Truth
0.2

0.4
0.6
% Complete

0.8

1

Figure 23: Comparison actual efficiency pessimistic trade direct
bound.
Finally, present experimental results relate two methods ICE employs
bound final efficiency pessimistic trade. total pricing error across
bidders round determined within pricing terms ( , v ), normalized
total true value efficient trade, already small (at 8.5%) initial rounds
falls around 3% final rounds ICE. suggests price-based bound
quite informative, although note defined terms error given ( , v )
immediately map price-based accuracy claim true valuations
current trade defined lower bound valuations. Figure 23 compares actual efficiency
pessimistic trade round estimated direct bound efficiency
available exchange. confirms direct bound reasonably tight,
effective bounding true efficiency regardless accuracy prices.

7. Conclusions
work designed implemented scalable highly expressive iterative combinatorial exchange. design includes many interesting features, including: new treebased language combinatorial exchange environments, new method construct approximate linear prices expressive languages, proxied architecture optimistic
pessimistic valuations coupled price-based activity rules drive preference elicitation, direct method estimate final efficiency trade terms valuation
bounds. adopting proxy agents receive direct, expressive claims upper
lower valuations bounds able form claims efficiency despite using
linear prices. bounds also allow good progress early rounds, even
efficient trade lower bound (pessimistic) values. Experimental results
automated, simple bidding agents indicate good behavior terms scalability
economic properties.
66

fiICE: Iterative Combinatorial Exchange

many intriguing opportunities future work. especially interesting
instantiate special-cases ICE design domains exist strategyproof,
static (two-sided) combinatorial market designs. would bring straightforward bidding
strategies ex post Nash equilibrium. example, possible integrate
methods trade-reduction (McAfee, 1992) generalizations (Babaioff & Walsh,
2005; Chu & Shen, 2007) domains restricted expressiveness. also consider
ICE combinatorial auction rather exchange, direct appeal VCG payments would provide incentive compatibility. two major directions future work
to: (a) modify design allow bidders refine structure, valuation
bounds TBBL tree, across rounds; (b) extend ICE work dynamic environment changing bidder population, instance maintaining linear price feedback
periodically clearing. Recent progress on-line mechanism design includes truthful,
dynamic double auctions simple expressiveness (Blum, Sandholm, & Zinkevich,
2006; Bredin, Parkes, & Duong, 2007), extend kind expressiveness
price sophistication present ICE; see work Parkes (2007) recent survey.
Lastly, incentive properties ICE much dependent payment rule used
argues analysis Threshold rule alternatives.

Acknowledgments
work supported part NSF grant IIS-0238147. earlier version paper
appeared Proc. 6th ACM Conference Electronic Commerce, 2005. TBBL language also described workshop paper (Cavallo et al., 2005). primary authors
paper Benjamin Lubin, Adam Juda David Parkes. Thanks anonymous
reviewers associate editor JAIR extremely helpful comments. Nick Elprin, Loizos
Michael Hassan Sultan contributed earlier versions work. thanks
students Al Roths class (Econ 2056) participated trial system
Cynthia Barnhart airline domain expertise. would also like thank Evan Kwerel
George Donohue early motivation encouragement. computation
used preparation manuscript performed Crimson Grid Harvard School Engineering Applied Sciences. Finally, papers genesis CS
286r Topics Interface Computer Science Economics taught Harvard
Spring 2004. Thanks students many early, innovative ideas.

Appendix A. Computation MRPAR
section show MRPAR computed solving sequence 3 MIPs.
begin considering special case = 0. general case follows almost immediately.
Define candidate passing trade, L
, as:
L
arg

max v (i ) p (i )

Fi (x0 )

breaking ties
(i) maximize v (i ) v (i )
(ii) favor
67

(29)

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

computed solving one MIP maximize v (i ) p (i ), followed
second MIP objective incorporated constraint v (i ) v (i )
becomes objective. Given perturbed valuation vi , defined respect trade L
(as
Section 4), define witness trade, U
,
as:

U
arg

max vi (i ) p (i ).

(30)

Fi (x0 )

found solving third MIP. Given prices , provisional trade
bid tree Ti , computational MRPAR rule (C-MRPAR) case = 0
defined as:
L
U
U
(1) v (L
) p (i ) vi (i ) p (i ) ,and

L
L


(2) L
= , v (i ) p (i ) > vi (i ) p (i )

establish C-MRPAR equivalent MRPAR, defined (19)(21).
Lemma 4 Given trades , prices , tree Ti , (i , , vi ) 0, vi Ti
v (i ) p (i ) vi (i ) p (i ), vi defined respect trade .
Proof: Direction () immediate since vi Ti . Consider direction () suppose,


contradiction, v (i ) p (i ) vi (i )
Pp (i ) exists vi Ti






vi (i ) p (i ) < vi (i ) p (i ). Subtract [vi () v ()] sides,

indicates node satisfied trade , get
X
X
X
X
v () p (i ) <
vi () +
vi ()
vi () +
X



X



vi ()



v () +

\i

X

vi () +

\i

X







\i

X

v (i ) p (i ) <

X

v () p (i ) <

v () +

\i

vi (i )

p



X

v () p (i )

(31)








vi () +

X

v () p (i ) (32)



(i ),

(33)

contradiction.



Lemma 5 Given trade , prices , tree Ti (i , , vi ) 0, vi Ti ,
U
Fi (x0 ), v (i ) p (i ) vi (U
) p (i ), vi defined respect
U
trade witness trade.
0
Proof: Direction () immediate since vi Ti U
Fi (x ). Consider direction

U
() suppose, contradiction, v (i ) p (i ) vi (i ) p (U
) exists

0




Fi (x ) vi Ti (i , , vi ) < 0. Lemma 4, means
v (i ) p (i ) < vi (i ) p (i ). But, contradiction
U
v (i ) p (i ) vi (U
) p (i )

=

max

0

Fi (x )

vi (i )

p

(34)


(i )



vi (i )

p



(i )

(35)


68

fiICE: Iterative Combinatorial Exchange

Theorem 7 C-MRPAR equivalent -MRPAR = 0.
Proof: Comparing (17) (18) C-MRPAR, given Lemmas 4 5,
left show sufficient check L
, candidate pass MRPAR.
is, need show Fi (x0 ) satisfies MRPAR L
satisfies
MRPAR. argue follows:
1. Trade must solve maxi Fi (x0 ) [v (i ) p (i )]. Otherwise,
v (i ) p (i ) > v (i ) p (i ). contradiction (17).
2. Trade must also break ties favor maximizing v (i ) v (i ). Otherwise,
profit v , v (i ) v (i ) > v (i ) v (i ).
implies v (i ) v (i ) > v (i ) v (i ), (i , , v ) > (i , , v ). But, since
profit v (i , , v ) = 0 (i , , v ) > 0.
contradiction (17).
3. Proceed case analysis. Either = , case done
explicitly selected candidate passing trade L
. case, let
L
denote

feasible
solutions

(29)

consider

difficult
case |L

| > 1.
L

argue satisfies MRPAR, trade L
,
6= . MRPAR, (i , , vi ) 0, vi Ti . particular,
vi (i )p (i ) vi (i )p (i ), vi defined respect , equivalently,
v (i ) p (i ) vi (i ) p (i ).

(36)

v (i ) p (i ) = v (i ) p (i ),

(37)

hand,


since L
. Taking (36) together (37), must satisfies
uncertain value nodes Ti also satisfied . Moreover, since v (i )
v (i ) = v (i ) v (i ), trades must satisfy exactly uncertain value
nodes. Finally, (37) profit fixed value nodes Ti must
trades. conclude profit vi Ti
current prices MRPAR satisfied either trade.


understand importance tie-breaking rule (i) selecting candidate
passing trade, L
, C-MRPAR, consider following example MRPAR = 0:
Example 10 bidder XOR(+A, +B) value 5 leaf +A value range
[5,10] leaf +B. Suppose prices currently 3 B = +B.
MRPAR rule satisfied market knows however remaining value
uncertainty +B resolved bidder always (weakly) prefer +B +A +B
. Notice +A +B pessimistic utility, +B satisfy
MRPAR. +B maximal value uncertainty, therefore selected +A
C-MRPAR.
69

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

understand importance selecting, evaluating, U
respect vi rather
v , consider following example (again = 0). illustrates role shared
uncertainty tree, occurs multiple trades share node uncertain
value value, although uncertain, resolved way trades.
Example 11 bidder XOR(+A, +B) value bounds [5, 10] root node
value 1 leaf +A. Suppose prices currently 3 B = +B.
MRPAR rule satisfied bidder strictly prefers +A +B, whichever way
uncertain value root node ultimately resolved. C-MRPAR selects L
buy
(L ) = 5 + 1 3 = 3. valuation v , witness trade buy B
)

p
A, payoff v (L



would selected payoff 10 3 = 7 seem violate MRPAR. But, whichever
way uncertain value root resolved affect +A +B way.
addressed setting vi () = v () = 5 root node, value adopted
determining payoff L
. Evaluated vi , witness buy (1)
C-MRPAR trivially satisfied (2) satisfied since 3 > 5 3 = 2.
-MRPAR > 0, adopt slight variation, -C-MRPAR procedure
defined as:
(1) Check (i , , vi ) vi Ti , Fi (x0 ) directly, application
Lemma 5 valuation vi defined respect trade , test
U
v (i ) p (i ) vi (U
) p (i )

(38)

(2) satisfied fall back C-MRPAR verify (20) (21),

candidate passing trade L
modified (29) drop tie-breaking favor
L

L
second step C-MRPAR modified require v (i ) p (i ) > vi (i )
p (i ) + , vi defined respect L
.
argument adopted proof Theorem 7 remains valid establishing

sufficient consider L
, defined -C-MRPAR, case pass
activity rule.

Appendix B. Computation DIAR
-DIAR rule verified solving two MIPs. first optimization problem
identifies trade maximal DIAR error current bounds refinement
improved error least :
Pi =

max [vi0 (i ) p (i ) (v 0i (i ) p (i ))]

Fi (x0 )

(39)

s.t. (vi0 (i ) p (i ) (v 0i (i ) p (i )))
= C +

max

Fi (x0 )

(vi1 (i ) p (i ) (v 1i (i ) p (i )))

(40)

vi0 (i )

(41)



p (i )

s.t. vi0 (i ) v 0i (i ) vi1 (i ) + v 1i (i ) ,
70

(42)

fiICE: Iterative Combinatorial Exchange

vi0 vi1 defined respect , v 0 v 1 represent valuations defined
bidders refinement respectively, C = v 0i (i ) p (i ). Note
problem could infeasible, case define Pi := .
second optimization identifies trade maximal DIAR error v 1 still
allows possibility valuation bounds provide error reduction v 0 :
Fi =

max [vi0 (i ) p (i ) (v 0i (i ) p (i ))]

Fi (x0 )

(43)

s.t. (vi0 (i ) p (i ) (v 0i (i ) p (i )))
(v 1i (i ) p (i ) (vi1 (i ) p (i )))
= C +

max vi0 (i ) p (i )

Fi (x0 )

s.t. vi0 (i ) v 0i (i ) v 1i (i ) + vi1 (i ) ,

(44)
(45)
(46)

vi defined respect , vi similarly defined respect .
second term (44) recognizes remains possible decrease value
new lower-bound v 1i (i ), increasing value new upper-bound v 1i (i )
except nodes shared , giving vi1 (i ). see (46) equivalent
to:
X
X
[v 1i () v 0i ()] ,
[v 0i () v 1i ()] +
\



\i

calculates amount refinement still possible service reducing
DIAR error. Note problem could infeasible, case define Fi := .
ultimately compare two solutions, bidder passes DIAR Pi Fi .

Appendix C. Automated Bidding Agents Bidder Feedback
bidding agents used simulation experiments designed minimize
amount information revealed order pass activity rules remaining
straightforward true valuation consistent lower upper valuations.
summarizing behavior bidding agents, three things explain: (a)
method adopt place last-and-final round; (b) feedback provided
ICE bidders meeting MRPAR DIAR; (c) logic followed
bidding agents. Rather define method bidding agents adjust bounds
last-and-final round, keep ICE open simulation past point would
ordinarily go last-and-final. Past point, bidding agents continue refine
bounds ICE terminates payments within desired accuracy.
bidding agent phase reduces uncertainty multiplicative factor
nodes active current provisional trade provisional trades
economies bidder removed. adopted simulation purposes only.
bidding agents operate loop, heuristically modifying valuation bounds
trying meet MRPAR DIAR querying proxy advice. proxy provides
guidance help bidding agent refine valuation meet activity
rule. MRPAR DIAR, optimization problems solved checking
whether bidder satisfied activity rule also provide information guide
71

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

U
bidder. First consider MRPAR recall L
candidate passing trade
witness trade. following lemma easy, stated without proof:

Lemma 6 MRPAR satisfied current valuation bounds, bidder must
U
increase lower bound least one node {L
\ }, decrease upper bound
U
L
least one node {i \ }, order meet activity rule.
simple bidder makes changes subset nodes, bidder
inquire passed activity rule. proxy respond yes
revise set nodes bidding agent refine valuation bounds.
similar functionality provided DIAR. time trade solves second MIP
(with DIAR error Fi ) provided feedback, together information much
bidder must either reduce error, constrain possibilities
trade, satisfy DIAR. bidding agent determine information nodes
must modify, much total, free decide much modify
node satisfy rule. key agent design following lemma:
Lemma 7 trade straightforward bidder passes MRPAR (for = 0) must
trade weakly preferred bidder trades true valuation.
Proof: contradiction. Suppose true valuation vi Ti trade meets MRPAR
weakly preferred trade true valuation prices . Then, exists
trade Fi (x0 ) (i , , vi ) > 0. But, contradiction MRPAR
since (i , , vi ) 0 vi Ti Fi (x0 ), including vi = vi = .
use observation define procedure UpdateMRPAR bidder
intelligently refine valuation bounds meet MRPAR. Let trade
hope pass MRPAR, define ui (i , ) = vi (i ) p (i ), ui (i , ) = v (i )
p (i ), ui (i , ) = vi (i ) p (i ), vi defined respect candidate passing
trade . high-level approach follows:
function UpdateMRPAR
arg maxi Fi (x0 ) ui (i , )
ui (i , ) < 0
reduce slack ui (i , )
end
U
arg maxi Fi (x0 ) ui (i , )
ui (i , ) < ui (U
, )
U
Heuristically reduce upper bounds U
\ ui (i , ) ui (i , )
remaining slack heuristically reduce lower bounds \ U

U
arg maxi Fi (x0 ) ui (i , )
end
6=
ui (i , ) ui (i , )
Heuristically reduce upper bounds \ ui (i , ) ui (i , )
remaining slack heuristically reduce lower bounds \
72

fiICE: Iterative Combinatorial Exchange

end
end
return
end function
bidding agent makes use couple optimization modalities exposed
proxy bidder. procedure first chooses preferred trade truth
trade pass MRPAR ; bidding agent requests proxy finds
trade solving MIP. trade negative profit, bidding agent attempts
demonstrate positive profit trade. Next, bidding agent enters loop, wherein
repeatedly requests proxy run MIP calculates witness trade U
respect
. long witness profit preferred
trade, bidding agent adjust bounds reverse mis-ordering. Lastly,
bidding agent must pass MRPAR, merely RPAR, bidding agent attempts
show strict preference identical.
meeting DIAR, bidding agent responds F 0 0 parameter
provided proxy follows. Let F trade chosen maximization
calculates F . high-level approach follows:
function updateDIAR
Proxy says still passed DIAR
F modified reduce DIAR error last round
Heuristically reduce upper-bound slack F \
Heuristically reduce lower-bound slack \ F
else
Heuristically reduce upper-bound slack \ F
Heuristically reduce lower-bound slack F \
end
end
end function
bidding agent attempts make current failing trade pass DIAR possible
reducing error respect trade. Otherwise, reduces bounds prove
DIAR could made pass trade loops next trade.

References
Ausubel, L., Cramton, P., & Milgrom, P. (2006). clock-proxy auction: practical
combinatorial auction design. Cramton et al. (Cramton et al., 2006), chap. 5.
Ausubel, L. M., & Milgrom, P. (2002). Ascending auctions package bidding. Frontiers
Theoretical Economics, 1, 142.
Babaioff, M., & Walsh, W. E. (2005). Incentive-compatible, budget-balanced, yet highly
efficient auctions supply chain formation. Decision Support Systems, 39, 123149.
Ball, M., Donohue, G., & Hoffman, K. (2006). Auctions safe, efficient, equitable
allocation airspace system resources. Cramton et al. (Cramton et al., 2006),
73

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

chap. 20.
Ball, M. O., Ausubel, L. M., Berardino, F., Cramton, P., Donohue, G., Hansen, M., &
Hoffman, K. (2007). Market-based alternatives managing congestion new yorks
laguardia airport. Proceedings AirNeth Annual Conference.
Bererton, C., Gordon, G., & Thrun, S. (2003). Auction mechanism design multi-robot
coordination. Proc. 17th Annual Conf. Neural Information Processing Systems
(NIPS03).
Bertsimas, D., & Tsitsiklis, J. (1997). Introduction Linear Optimization. Athena Scientific.
Bikhchandani, S., & Mamer, J. W. (1997). Competitive equilibrium exchange economy
indivisibilities. Journal Economic Theory, 74, 385413.
Bikhchandani, S., & Ostroy, J. M. (2002). package assignment model. Journal
Economic Theory, 107 (2), 377406.
Blum, A., Sandholm, T., & Zinkevich, M. (2006). Online algorithms market clearing.
Journal ACM, 53, 845879.
Boutilier, C. (2002). Solving concisely expressed combinatorial auction problems.
Proceedings 18th National Conference Artificial Intelligence, pp. 359366.
Boutilier, C., & Hoos, H. (2001). Bidding languages combinatorial auctions. Proc.
17th International Joint Conference Artificial Intelligence, pp. 11211217.
Bredin, J., Parkes, D. C., & Duong, Q. (2007). Chain: dynamic double auction framework
matching patient agents. Journal Artificial Intelligence Research, 30, 133179.
Cavallo, R., Parkes, D. C., Juda, A. I., Kirsch, A., Kulesza, A., Lahaie, S., Lubin, B.,
Michael, L., & Shneidman, J. (2005). TBBL: Tree-Based Bidding Language
Iterative Combinatorial Exchanges. Multidisciplinary Workshop Advances
Preference Handling (IJCAI).
Chu, L. Y., & Shen, Z. M. (2007). Truthful double auction mechanisms e-marketplace.
Operations Research. appear.
Compte, O., & Jehiel, P. (2007). Auctions information acquisition: Sealed-bid Dynamic Formats?. Rand Journal Economics, 38 (2), 355372.
Conen, W., & Sandholm, T. (2001). Preference elicitation combinatorial auctions..
Wellman, & Shoham (Wellman & Shoham, 2001), pp. 256259.
Cramton, P. (2003). Electricity Market Design: Good, Bad, Ugly.
Proceedings Hawaii International Conference System Sciences.
Cramton, P. (2006). Simultaneous ascending auctions. Cramton et al. (Cramton et al.,
2006), chap. 3.
Cramton, P., Kwerel, E., & Williams, J. (1998). Efficient relocation spectrum incumbents.
Journal Law Economics, 41, 647675.
Cramton, P., Shoham, Y., & Steinberg, R. (Eds.). (2006). Combinatorial Auctions. MIT
Press.
74

fiICE: Iterative Combinatorial Exchange

Day, R., & Raghavan, S. (2007). Fair payments efficient allocations public sector
combinatorial auctions. Management Science, 53 (9), 1389.
de Vries, S., Schummer, J., & Vohra, R. V. (2007). ascending Vickrey auctions
heterogeneous objects. Journal Economic Theory, 132, 95118.
de Vries, S., & Vohra, R. V. (2003). Combinatorial auctions: survey. Informs Journal
Computing, 15 (3), 284309.
Dias, M., Zlot, R., Kalra, N., & Stentz, A. (2006). Market-based multirobot coordination:
survey analysis. Proceedings IEEE, 94, 12571270.
Dunford, M., Hoffman, K., Menon, D., Sultana, R., & Wilson, T. (2003). Testing linear
pricing algorithms use ascending combinatorial auctions. Tech. rep., SEOR,
George Mason University. Submitted INFORMS J.Computing.
Fu, Y., Chase, J., Chun, B., Schwab, S., & Vahdat, A. (2003). Sharp: architecture
secure resource peering. Proceedings 19th ACM symposium Operating
systems principles, pp. 133148. ACM Press.
Gerkey, B. P., & Mataric, M. J. (2002). Sold!: Auction methods multi-robot coordination.
IEEE Transactions Robotics Automation, Special Issue Multi-robot Systems,
18, 758768.
Hudson, B., & Sandholm, T. (2004). Effectiveness query types policies preference
elicitation combinatorial auctions. Proc. 3rd Int. Joint. Conf. Autonomous
Agents Multi Agent Systems, pp. 386393.
Kelso, A. S., & Crawford, V. P. (1982). Job matching, coalition formation, gross
substitutes. Econometrica, 50, 14831504.
Krishna, V. (2002). Auction Theory. Academic Press.
Kwasnica, A. M., Ledyard, J. O., Porter, D., & DeMartini, C. (2005). new improved
design multi-object iterative auctions. Management Science, 51, 419434.
Kwerel, E., & Williams, J. (2002). proposal rapid transition market allocation
spectrum. Tech. rep., FCC Office Plans Policy.
Lahaie, S., Constantin, F., & Parkes, D. C. (2005). power demand queries
combinatorial auctions: Learning atomic languages handling incentives. Proc.
19th Int. Joint Conf. Artificial Intell. (IJCAI05).
Lahaie, S., & Parkes, D. C. (2004). Applying learning algorithms preference elicitation.
Proc. 5th ACM Conf. Electronic Commerce (EC-04), pp. 180188.
McAfee, R. P. (1992). dominant strategy double auction. J. Economic Theory, 56,
434450.
Milgrom, P. (2004). Putting Auction Theory Work. Cambridge University Press.
Milgrom, P. (2007). Package auctions package exchanges (2004 Fisher-Schultz lecture).
Econometrica, 75, 935966.
Mishra, D., & Parkes, D. C. (2007). Ascending price Vickrey auctions general valuations.
Journal Economic Theory, 132, 335366.
75

fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

Myerson, R. B., & Satterthwaite, M. A. (1983). Efficient mechanisms bilateral trading.
Journal Economic Theory, 28, 265281.
Nemhauser, G., & Wolsey, L. (1999). Integer Combinatorial Optimization. WileyInterscience.
Nisan, N. (2006). Bidding languages combinatorial auctions. Cramton et al. (Cramton
et al., 2006), chap. 9.
Nisan, N., Roughgarden, T., Tardos, E., & Vazirani, V. (Eds.). (2007). Algorithmic Game
Theory. Cambridge University Press.
ONeill, R. P., Sotkiewicz, P. M., Hobbs, B. F., Rothkopf, M. H., & Stewart, Jr., W. R.
(2005). Efficient market-clearing prices markets nonconvexities. European
Journal Operations Research, 164, 269285.
Parkes, D. C. (2007). On-line mechanisms. Nisan et al. (Nisan, Roughgarden, Tardos,
& Vazirani, 2007), chap. 16. appear.
Parkes, D. C., Kalagnanam, J. R., & Eso, M. (2001). Achieving budget-balance
Vickrey-based payment schemes exchanges. Proc 17th International Joint Conference Artificial Intelligence, pp. 11611168.
Parkes, D. C., & Ungar, L. H. (2000a). Iterative combinatorial auctions: Theory practice. Proc. 17th National Conference Artificial Intelligence (AAAI-00), pp.
7481.
Parkes, D. C., & Ungar, L. H. (2000b). Preventing strategic manipulation iterative
auctions: Proxy agents price-adjustment. Proc. 17th National Conference
Artificial Intelligence (AAAI-00), pp. 8289.
Rassenti, S. J., Smith, V. L., & Bulfin, R. L. (1982). combinatorial mechanism airport
time slot allocation. Bell Journal Economics, 13, 402417.
Rothkopf, M. H., Pekec, A., & Harstad, R. M. (1998). Computationally manageable combinatorial auctions. Management Science, 44 (8), 11311147.
Saatcioglu, K., Stallaert, J., & Whinston, A. B. (2001). Design financial portal. Communications ACM, 44, 3338.
Sandholm, T. (2006). Optimal winner determination algorithms. Cramton et al. (Cramton et al., 2006), chap. 14.
Sandholm, T. (2007). Expressive Commerce Application Sourcing:
Conducted $35 Billion Generalized Combinatorial Auctions. AI Magazine, 28 (3),
45.
Sandholm, T., & Boutilier, C. (2006). Preference elicitation combinatorial auctions.
Cramton et al. (Cramton et al., 2006), chap. 10.
Shapley, L. S., & Shubik, M. (1972). assignment game I: core. Int. Jounral
Game Theory, 1, 111130.
Smith, T., Sandholm, T., & Simmons, R. (2002). Constructing clearing combinatorial
exchanges using preference elicitation. AAAI-02 workshop Preferences AI
CP: Symbolic Approaches.
76

fiICE: Iterative Combinatorial Exchange

Vossen, T. W. M., & Ball, M. O. (2006). Slot trading opportunities collaborative ground
delay programs. Transportation Science, 40, 1528.
Wellman, M. P., & Shoham, Y. (Eds.). (2001). Proc. 3rd ACM Conf. Electronic Commerce (EC-01), New York, NY. ACM.
Wurman, P. R., & Wellman, M. P. (2000). AkBA: progressive, anonymous-price combinatorial auction. Proc. 2nd ACM Conf. Electronic Commerce (EC-00), pp.
2129.

77

fi
