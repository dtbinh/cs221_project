Journal Artificial Intelligence Research 33 (2008) 521-549Submitted 6/08; published 12/08Multiagent Reinforcement Learning AlgorithmNon-linear DynamicsSherief AbdallahSHERIEF. ABDALLAH @ BUID . AC . AEFaculty InformaticsBritish University DubaiUnited Arab Emirates(Fellow) School InformaticsUniversity EdinburghUnited KingdomVictor LesserLESSER @ CS . UMASS . EDUDepartment Computer ScienceUniversity Massachusetts AmherstUnited StatesAbstractSeveral multiagent reinforcement learning (MARL) algorithms proposed optimizeagents decisions. Due complexity problem, majority previously developedMARL algorithms assumed agents either knowledge underlying game (suchNash equilibria) and/or observed agents actions rewards received.introduce new MARL algorithm called Weighted Policy Learner (WPL), allowsagents reach Nash Equilibrium (NE) benchmark 2-player-2-action games minimumknowledge. Using WPL, feedback agent needs local reward (the agentobserve agents actions rewards). Furthermore, WPL assume agents knowunderlying game corresponding Nash Equilibrium priori. experimentally showalgorithm converges benchmark two-player-two-action games. also showalgorithm converges challenging Shapleys game previous MARL algorithms failedconverge without knowing underlying game NE. Furthermore, show WPLoutperforms state-of-the-art algorithms realistic setting 100 agents interactinglearning concurrently.important aspect understanding behavior MARL algorithm analyzing dynamics algorithm: policies multiple learning agents evolve time agentsinteract one another. analysis verifies whether agents using given MARLalgorithm eventually converge, also reveals behavior MARL algorithm priorconvergence. analyze algorithm two-player-two-action games show symbolically proving WPLs convergence difficult, non-linear nature WPLs dynamics,unlike previous MARL algorithms either linear piece-wise-linear dynamics. Instead,numerically solve WPLs dynamics differential equations compare solution dynamicsprevious MARL algorithms.1. Introductiondecision problem agent viewed selecting particular action given state.well-known simple example single-agent decision problem multi-armed bandit (MAB)problem: agent needs choose lever among set available levers. reward executing action drawn randomly according fixed distribution. agents goal choosec2008AI Access Foundation. rights reserved.fiA BDALLAH L ESSERlever (the action) highest expected reward. order so, agent samplesunderlying reward distribution action (by trying different actions observing resulting rewards). goal reinforcement learning algorithms general eventually stabilize(converge) strategy maximizes agents payoff. Traditional reinforcement learning algorithms (such Q-learning) guarantee convergence optimal policy stationary environment(Sutton & Barto, 1999), simply means reward distribution associated actionfixed change time.multiagent system, reward agent receives executing particular action dependsagents actions well. example, consider extending MAB problem multiagent case. reward agent gets choosing lever 1 depends lever agent Bchosen. agents B learning adapting strategies, stationary assumptionsingle-agent case violated (the reward distribution changing) therefore single-agentreinforcement learning techniques guaranteed converge. Furthermore, multi-agentcontext optimality criterion clear single agent case. Ideally, wantagents reach equilibrium maximizes individual payoffs. However, agentscommunicate and/or agents cooperative, reaching globally optimal equilibriumalways attainable (Claus & Boutilier, 1998). alternative goal pursue converging Nash Equilibrium (NE) (Bowling, 2005; Conitzer & Sandholm, 2007; Banerjee & Peng,2007), definition local maximum across agents (no agent better deviatingunilaterally NE).important aspect understanding behavior MARL algorithm analyzing dynamics algorithm: policies multiple learning agents evolve time interacting one another. analysis reveals whether agents using particular MARLalgorithm eventually converge, also points features MARL algorithmexhibited convergence period. Analyzing dynamics MARL algorithm evensimplest domains (particularly, two-player-two-action games) challenging task therefore performed few, relatively simple, MARL algorithms (Singh, Kearns, & Mansour,2000; Bowling & Veloso, 2002) restrictive subset games. analyzed dynamicseither linear piece-wise linear.Recently, several multi-agent reinforcement learning (MARL) algorithms proposedstudied (Claus & Boutilier, 1998; Singh et al., 2000; Peshkin, Kim, Meuleau, & Kaelbling,2000; Littman, 2001; Bowling & Veloso, 2002; Hu & Wellman, 2003; Bowling, 2005; Abdallah &Lesser, 2006; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007). MARL algorithmsassumed agent knew underlying game structure, Nash Equilibrium (NE) (Bowling &Veloso, 2002; Banerjee & Peng, 2007). even required knowing actions agentsexecuted rewards received (Hu & Wellman, 2003; Conitzer & Sandholm, 2007).assumptions restrictive open domains limited communication (such ebayPeer-to-Peer file-sharing) agent rarely knows agents existence let alone observingactions knowing actions affect agents local reward.hand, agents unaware underlying game (particularly NE)observing other, even simple game two players two actionschallenging. example, suppose Player 1 observes getting reward 10 executingfirst action reward 8 executing action 2. time passes, Player 2 changes policy,Player 1 observes switch reward associated action: action 1 reward7 action 2 reward 9. Note cases Player 1 unaware NE strategy522fiM ULTIAGENT L EARNING N - LINEAR DYNAMICSoblivious current policy Player 2. feedback Player 1 getting changereward function, turn depends Player 2s policy. situation appliesreverse Player 2.paper propose new MARL algorithm enables agents converge NashEquilibrium, benchmark games, assuming agent oblivious agents receivesone type feedback: reward associated choosing given action. new algorithmcalled Weighted Policy Learner WPL reasons become clear shortly. experimentally show WPL converges well-known benchmark two-player-two-action games.Furthermore, show WPL converges challenging Shapleys game, state-of-theart MARL algorithms failed converge (Bowling & Veloso, 2002; Bowling, 2005),1 unlike WPL.also show WPL outperforms state-of-the-art MARL algorithms (shorter time converge,better performance convergence, better average reward) realistic domain100 agents interacting learning one another. also analyze WPLs dynamics shownon-linear. non-linearity made attempt solve, symbolically, differentialequations representing WPLs dynamics unsuccessful. Instead, solve equations numericallyverify theoretical analysis experimental results consistent. comparedynamics WPL earlier MARL algorithms discuss interesting differences similarities.paper organized follows. Section 2 lays necessary background, including gametheory closely related MARL algorithms. Section 3 describes proposed algorithm. Section 4analyzes WPLs dynamics restricted class games compares previous algorithmsdynamics. Section 5 discusses experimental results. conclude Section 6.2. Backgroundsection introduce necessary background contribution. First, brief reviewrelevant game theory definitions given. review relevant MARL algorithms provided,particular focus gradient-ascent MARL (GA-MARL) algorithms closely relatedalgorithm.2.1 Game TheoryGame theory provides framework modeling agents interaction used previousresearchers order analyze convergence properties MARL algorithms (Claus & Boutilier,1998; Singh et al., 2000; Bowling & Veloso, 2002; Wang & Sandholm, 2003; Bowling, 2005;Conitzer & Sandholm, 2007; Abdallah & Lesser, 2006). game specifies, compact simplemanner, payoff agent depends agents actions. (normal form) gamedefined tuple hn, A1 , ..., , R1 , ..., Rn i, n number players2 game, Aiset actions available agent i, Ri : A1 ... < reward (payoff) agentreceive function joint action executed agents. game twoplayers, convenient define reward functions payoff matrix shown Table1. cell (i, j) matrix represents payoff received row player (Player 1)column player (Player 2), respectively, row player plays action column player1. Except MARL algorithms assumed agents know information underlying game (Hu & Wellman,2003; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007).2. use terms agent player interchangeably.523fiA BDALLAH L ESSERplays action j. Table 1 Table 2 provide example benchmark games used evaluatingprevious MARL algorithms (Bowling & Veloso, 2002; Bowling, 2005; Conitzer & Sandholm, 2007;Banerjee & Peng, 2007).Table 1: Benchmark 2-action games. coordination game two pure NE(s): [(0, 1)r , (0, 1)c ][(1, 0)r , (1, 0)c ]. matching-pennies tricky games one mixed NEactions played equal probability, NE=[( 21 , 12 )r , ( 21 , 12 )c ](a) coordinationa1a2a12,10,0(b) matching penniesa20,01,2HH1,-1-1,1-1,11,-1(c) trickya1a2a10,31,0a23,22,1(d) generalgamea1a22-player-2-actiona1r11 , c11r21 , c21a2r12 , c12r22 , c22Table 2: Benchmark 3-action games. games one mixed NE actions playedequal probability, NE= [( 13 , 13 , 13 )r , ( 31 , 13 , 13 )c ].(a) rock paper scissorsr1r2r3c10,01,-1-1,1c2-1,10,01,-1(b) Shapleysc31, -1-1,10,0r1r2r3c10,00,11,0c21,00,00,1c30,11,00,0policy (or strategy) agent denoted P D(Ai ), P D(Ai ) setprobability distributions actions Ai . probability choosing action ak accordingpolicy (ak ). policy deterministic pure probability playing one action 1probability playing actions 0, (i.e. k : (ak ) = 1 l 6= k : (al ) = 0),otherwise policy stochastic mixed.joint policy collection individual agents policies, i.e. = h1 , 2 , ..., n i,n number agents. convenience, joint policy usually expressed = hi , i,collection policies agents agent i.VLet variable Ai = {ha1 , ..., : aj Aj iP6= j}.P expected reward agent would get,agents follow joint policy , Vi (hi , i) = ai Ai ai Ai (ai )i (ai ).Ri (ai , ai ),i.e. reward averaged joint policy. joint policy Nash Equilibrium, NE, iffagent get higher expected reward changing policy unilaterally. formally, hi ,i) V (h , i). NE pure constituting policiesNE iff : Vi (hi ,pure. Otherwise NE called mixed stochastic. game least one Nash equilibrium,may pure (deterministic) equilibrium.Consider benchmark games Table 1. coordination game (Table 1(a))example games least one pure NE. matching pennies game (Table 1(b))example games pure NE mixed NE (where playerplays a1 a2 equal probability). Convergence GA-MARL algorithms games pure524fiM ULTIAGENT L EARNING N - LINEAR DYNAMICSNE easier games NE mixed (Singh et al., 2000; Bowling & Veloso, 2002;Zinkevich, 2003). tricky game similar matching pennies game onemixed NE (no pure NE), yet GA-MARL algorithms succeeded converging matchingpennies games, failing tricky game (Bowling & Veloso, 2002).Table 2 shows well-known 2-player-3-action benchmark games. Shapleys game, particular,received considerable attention MARL community (Bowling, 2005; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007) remains challenging state-of-the-art MARL algorithms despite apparent simplicity (and similarity rock-paper-scissors gamechallenging). proposed algorithm first MARL algorithm converge Shapleys gamewithout observing agents knowing underlying NE strategy.introducing MARL algorithms following section, two issues worth noting.first issue general assumption MARL context: agents play game repeatedlylarge number times. necessary assumption order agents gain experiencelearn. second issue games rewards, described section, deterministicgiven joint action. However, agents perspective, rewards stochasticrandomness caused agents actions system.2.2 Multiagent Reinforcement Learning, MARLEarly MARL algorithms based Q-learning algorithm (Sutton & Barto, 1999), therefore could learn deterministic policy (Claus & Boutilier, 1998), significantly limitingapplicability competitive partially observable domains. Joint Action Learner (Claus &Boutilier, 1998) example family algorithms also required knowledgeagents chosen actions.Another class MARL algorithms class Equilibrium learners Nash-Q (Hu &Wellman, 2003), OAL (Wang & Sandholm, 2003), AWESOME (Conitzer & Sandholm, 2007).algorithms assumed agent observed agents actions additionknowing underlying game.3 agent computed Nash Equilibria. purposelearning allow agents converge particular Nash Equilibrium (in case agents executeMARL algorithm).Observing agents knowing underlying game structure applicable openlarge domains, motivated development gradient ascent learners. Gradient ascentMARL algorithms (GA-MARL) learn stochastic policy directly following expected rewardgradient. ability learn stochastic policy particularly important world fullyobservable competitive nature. Consider example blind robot maze (i.e. robotcannot distinguish maze locations). deterministic policy (i.e. one always choosesparticular action everywhere) may never escape maze, stochastic policy choosesaction non-zero probability eventually escape maze.4 Similarly, competitivedomain stochastic policy may stable policy (e.g. Nash Equilibrium competitivegame). remainder section focuses family algorithms closely relatedproposed algorithm.3. Nash-Q require knowing underlying game required observing agents rewards additionchosen actions.4. Note uniformly random policy may optimal maze biased specific direction.525fiA BDALLAH L ESSERInfinitesimal Gradient Ascent algorithm (IGA) (Singh et al., 2000) generalization(Generalized IGA GIGA) (Zinkevich, 2003) proved converge games pure NE.However, algorithms failed converge games mixed NE, therefore maysuitable applications require mixed policy.Several modifications IGA GIGA proposed avoid divergence gamesmixed NE, including: IGA/PHC-WoLF (Bowling & Veloso, 2002), PHC-PDWoLF (Banerjee &Peng, 2003), GIGA-WoLF (Bowling, 2005). used form Win Learn Fastheuristics (Bowling & Veloso, 2002), whose purpose speedup learning agentworse NE policy (losing) slow learning agent better NEpolicy. main problem heuristic agent cannot know whether betterworse NE policy without knowing underlying game prior. Therefore, practicalimplementation WoLF heuristic needed use approximation methods predictingperformance agents NE policy. algorithm, called WPL, uses different heuristicrequire knowing NE policy consequently, show, converges trickygame Shapleys game, algorithms relying WoLF heuristic failed. Furthermore,show large-scale partially-observable domain (the distributed task allocation problem,DTAP) WPL outperforms state-of-the-art GA-MARL algorithm GIGA-WoLF.following section reviews detail well known GA-MARL algorithms IGA,IGA-WoLF, GIGA-WoLF use compare algorithm against.2.3 Gradient-Ascent MARL Algorithmsfirst GA-MARL algorithm whose dynamics analyzed Infinitesimal Gradient Ascent(IGA) (Singh et al., 2000). IGA simple gradient ascent algorithm agent updatespolicy follow gradient expected payoffs (or value function) Vi , illustratedfollowing equations.Vi ( )projection(i + it+1 )it+1it+1Parameter called policy-learning-rate approaches zero limit ( 0), henceword Infinitesimal IGA. Function projection projects updated policy spacevalid policies. original IGA paper (Singh et al., 2000) defined projection function (andtherefore IGA) case agent two actions choose from. general definitionprojection function later developed resulting algorithm called Generalized IGAGIGA (Zinkevich, 2003). generalized function projection(x) = argminx0 :valid(x0 ) |x x0 |,|x x0 | Euclidean distance x x0 . APvalid Ppolicy (over set actionsA) must satisfy two constraints: : 1 (a) 0 = aA (a) = 1.words, space valid policies simplex, line segment (0,1),(1,0)case two actions, triangular surface (1,0,0), (0,1,0), (0,0,1) case three actions,on. joint policy, , point simplex.quite possible (especially tryingPfollow approximate policy gradient) deviates 1 goes beyond simplex.generalized projection function projects invalid policy closest valid policy withinsimplex (Zinkevich, 2003).526fiM ULTIAGENT L EARNING N - LINEAR DYNAMICSIGA algorithm converge two-player-two-action games (Singh et al., 2000).Algorithm IGA-WoLF (WoLF stands Win Learn Fast) proposed (Bowling & Veloso,2002) order improve convergence properties IGA using two different learning ratesfollows. player getting average reward lower reward would get executingNE strategy, learning rate large. Otherwise (the player current policy betterNE policy), learning rate small. formally,Vi ()(a)(a)lose Vi (i , ) < Vi (i , )win otherwiseprojection(i + )NE policy agent lose > win learning rates. dynamicsIGA-WoLF analyzed proved converge 2-player-2-action games (Bowling &Veloso, 2002), briefly review Section 2.4. IGA-WoLF limited practical use, required agent know equilibrium policy (which means knowing underlying game).approximation IGA-WoLF proposed, PHC-WoLF, agents NE strategy approximated averaging agents policy time (Bowling & Veloso, 2002). approximatealgorithm, however, failed converge tricky game shown Table 1(c).GIGA-WoLF algorithm extended GIGA algorithm WoLF heuristic. GIGAWoLF kept track two policies z. Policy used select actions execution. Policy zused approximate NE policy. update equations GIGA-WoLF (Bowling, 2005,2004):t+1 = projection( + rt )zt+1t+1= projection( + r /3)||z t+1 z ||= min 1, t+1zt+1 = t+1 + t+1 (z t+1 t+1 )(1)(2)(3)(4)main idea variant WoLF heuristics (Bowling & Veloso, 2002). agent changespolicy faster agent performing worse policy z, i.e. V < V z . z movesslower , GIGA-WoLF uses z realize needs change current gradient direction.approximation allows GIGA-WoLF converge tricky game, Shapleys game(Bowling, 2005).following section briefly reviews analysis IGAs IGA-WoLFs dynamics, showingjoint policy two agents evolves time. build analysis Section 4analyze WPLs dynamics.2.4 Dynamics GA-MARL AlgorithmsDifferential equations used model dynamics IGA (Singh et al., 2000). simplifyanalysis, authors considered two-player-two-action games, also here.refer joint policy two players time probabilities choosing first action(pt , q ), 1 = (pt , 1 pt ) policy player 1 2 = (q , 1 q ) policy527fiA BDALLAH L ESSERplayer 2. notation omitted affect clarity (for example,considering one point time).IGAs update equations simplified (note rij cij payoffs rowcolumn players respectively):pt+1 = pt +Vr (pt , q )= pt + (Vr (1, q ) Vr (0, q ))pVr (1, q ) Vr (0, q ) = r11 q + r12 (1 q ) r21 q + r22 (1 q )= q (r11 r12 r21 + r22 ) + (r12 r22 )= u1 q + u2(5)similarly,q t+1 = q + (u3 pt + u4 )u1 , u2 , u3 , u4 game-dependent constants following values.u1 = r11 r12 r21 + r22u2 = r12 r22u3 = c11 c12 c21 + c22u4 = c21 c22analyzing IGA, original paper distinguished three types games dependingu1 u3 parameters: u1 u3 > 0, u1 u3 = 0, u1 u3 < 0. Figure 1(a), Figure 1(b), Figure1(c) taken (Bowling & Veloso, 2002) illustrate dynamics IGAthree cases. figure displays space joint policies, horizontal axis representpolicy row player, p, vertical axis represents policy column player q.5joint policy two agents evolves time following directed linesfigures. example, Figure 1(b) illustrates starting joint policy, two playerseventually evolve one two equilibriums, either bottom-left corner, upper-right corner.noted that, simplicity, dynamics shown Figure 1 unconstrained: effectprojection function (the simplex described Section 2.3) shown. IGAs analysis(Singh et al., 2000), effect projection function taken account consideringpossible locations simplex.first second cases, IGA converged. third case IGA oscillated aroundequilibrium strategy, without ever converging it. happens point time, unlessagents NE, one agent better changing policy closer NE strategy,agent better changing policy away NE strategy.roles switch one agents crosses NE strategy shown Figure 1(c).described Section 2.3, IGA-WoLF proposed (Bowling & Veloso, 2002) extension IGA ensures convergence third case. idea IGA-WoLF, described earlier,5. Note Figure 1(b) Figure 1(c) divided four quadrants A, B, C, clarification. gradientdirection quadrant illustrated arrows Figure 1(c).528fiM ULTIAGENT L EARNING N - LINEAR DYNAMICSFigure 1: illustration IGAs dynamics (Bowling & Veloso, 2002).two learning rates agent moving toward away NE. following equations capture dynamics IGA-WoLF, hp , q NE. equations usefactored form lose win .p =pq =qt1+ (u1 qt1t1+ (u3 pt1+ u2 )llose Vr (pt1 , q t1 ) < Vr (p , q t1 )lwin otherwise+ u4 )llose Vc (q t1 , pt1 ) < Vc (q , pt1 )lwin otherwisedynamics IGA-WoLF best illustrated visually Figure 2, taken (Bowling & Veloso, 2002). switching learning rates causes switching ellipsessmaller diameters, eventually leading convergence.main limitation IGA-WoLF assumed agent knows NE policy (neededswitch two modes learning rate). following section presents WPL,make assumption, expense complex dynamics showSection 4.3. Weighted Policy Learner (WPL)propose paper Weighted Policy Learner (WPL) algorithm, followingupdate equations:529fiA BDALLAH L ESSERFigure 2: illustration IGA-WoLFs dynamics case mixed NE (Bowling & Veloso, 2002).Vi ()(a)(a)(()(a)V(a) < 01 (a) otherwiseprojection(i + )projection function adopted GIGA (Zinkevich, 2003) minor modification:a :1 (a) (the modification ensures minimum amount exploration ). algorithm worksfollows. gradient particular action negative gradient weighted (a),otherwise (gradient positive) gradient weighted (1i (a)). effectively, probabilitychoosing good action increases rate decreases probability approaches 1 (orboundary simplex). Similarly, probability choosing bad action decreases ratealso decreases probability approaches zero.unless gradient direction changes, WPL agent decreases learning rate agentgets closer simplex boundary. means, 2-player-2-action game, WPL agent movetowards NE strategy (away simplex boundary) faster moving away NEstrategy (towards simplex boundary), NE strategy always inside simplex.WPL biased pure (deterministic) policies reach pure policylimit (because rate updating policy approaches zero). theoretical limitationlittle practical concern two reasons. first reason exploration: 100% pure strategiesbad prevent agents exploring actions (in open dynamic environmentreward action may change time).second reason action dominates another action (the case gameVi ()()pure NE), V(a) large enough (a) > 1. case WPL jumppure policy one step.6 Note Section 4, order simplify theoretical analysis, assume0. practical perspective, however, set value close 0, never 0. holds6. fact, WPL even go beyond simplex valid policies, projection function comes play530fiM ULTIAGENT L EARNING N - LINEAR DYNAMICSgradient-ascent MARL algorithms (Bowling & Veloso, 2002; Bowling, 2005; Banerjee &Peng, 2007).key differences IGA-WoLF WPLs update rules despite apparentsimilarity. algorithms two modes learning rates (corresponding two conditionspolicy update rule). However, IGA-WoLF needs know equilibrium strategy orderdistinguish two modes, unlike WPL needs know value action.Another difference IGA-WoLF used two fixed learning rates (l > w ) two modes,WPL uses continuous spectrum learning rates, depending current policy.understood definition WPLs update equations, includes additional(continuous) scaling factor: . particular feature causes WPLs dynamics non-linear,discuss following section.4. Analyzing WPLs DynamicsClaim 1 WPL non-linear dynamics.Proof.policies two agents following WPL expressed followst1+ (u3 pt1+ (u1 qq qt1+ u4 )1 q t1 u3 pt1 + u4 > 0q t1otherwise+ u2 )1 pt1 u1 q t1 + u2 > 0pt1otherwisep pt1Note u1 q t1 + u2 = Vr (1, q t1 ) Vr (0, q t1 ) Equation 5. followspt pt1=(t (t 1))(u1 qt1+ u2 )1 pt1 Vr (1, q t1 ) > Vr (0, q t1 )pt1otherwise1 q t1 Vc (1, pt1 ) > Vc (0, pt1 )q t1otherwiseanalogously column playerq q t1=(t (t 1))(u3 pt1+ u4 )0, equations become differential:q 0 (t) =(u3 pt1+ u4 )1 q t1 Vc (1, pt1 ) > Vc (0, pt1 )q t1otherwise531(6)fiA BDALLAH L ESSERp0 (t) =(u1 qt1+ u2 )1 pt1 Vr (1, q t1 ) > Vr (0, q t1 )pt1otherwise(7)Notice NE strategy appear WPLs equations, unlike IGA-WoLF. Furthermore, IGA IGA-WoLF needed take projection function account,safely ignore projection function analyzing dynamics WPL two-player-twoaction games. due way WPL scales learning rate using current policy.definition p0 (t), positive p0 (t) approaches zero p approaches one negative p0 (t)approaches zero p approaches zero. words, p (or q) approaches 0 1, learningrate approaches zero, therefore p (or q) never go beyond simplex valid policies.7observation become apparent Section 4.2 compare dynamics WPLdynamics IGA IGA-WoLF.Following IGA-WoLFs analysis (Bowling & Veloso, 2002), illustrated Figure 3,focus challenging case deterministic NE (the NE inside joint policyspace) analyze p q evolve time. case original IGA oscillatedshown Figure 1. important note, however, gradient ascent MARL algorithms(including WPL) converge 2x2 games cases (at least) one pure NE,dynamics loops eventually lead one pure equilibriums (Singh et al.,2000; Bowling & Veloso, 2002).solve WPLs differential equations period 0 4 Figure 3, assumingPlayer 2 starts NE policy q time 0 returns q time 4. prove that,period 0 4, Player 2s policy p(t) gets closer NE policy p , i.e. pmin2 pmin1 > 0Figure 3, induction next time period p get closer equilibrium on.readability, p q used instead p(t) q(t) remainder section.Without loss generality, assume u1 q + u2 > 0 iff q > q u3 p + u4 > 0 iff p < p .overall period 0 4 divided four intervals defined times 0, 1, 2, 3, 4.period corresponds one combination p q follows. first period 0 1,p < p q > q , therefore agent p better moving toward NE, agent q bettermoving away NE. differential equations solved dividing p0 q 0dp(1 p)(u1 q + u2 )=dq(1 q)(u3 p + u4 )separationZppmin1u3 p + u4dp =1pZqmaxqu1 q + u2dq1q1 pmin1=1 p1 qu1 (qmax q ) + (u1 + u2 )ln1 qmaxu3 (p pmin1 ) + (u3 + u4 )ln7. practice WPL still needs projection function must set small value strictly larger 0.532fiM ULTIAGENT L EARNING N - LINEAR DYNAMICSPolicyqqmax(p*,qmax)q*qmin(pmin2,q*)(pmin1,q*)pmax(pmax,q*)pp*pmin2pmin1T1T2T3timeT4Figure 3: illustration WPLs dynamics. figure left shows policies evolution time,figure right shows joint policy space.Similarly, period 1 2, p > p q > q1 p=1 pmaxqu1 (q qmax ) + u2 lnqmaxu3 (pmax p ) + (u3 + u4 )lnperiod 2 3, p > p q < qu3 (p pmax ) + u4 lnppmax= u1 (qmin q ) + u2 lnqminqfinally period 3 4, p < p q < qpmin2=p1 qminu1 (q qmin ) + (u1 + u2 )ln1 qu3 (pmin2 p ) + u4 ln4 non-linear equations (note existence x ln(x) equations)5 unknowns (pmin , pmin2 , pmax , qmin1 , qmax ), along inequalities governing constantsu1 , u2 , u3 , u4 .WPLs dynamics non-linear, could obtain closed-form solution therefore could formally prove WPLs convergence.8 Instead, solve equations numericallyfollowing section. Although numerical solution still formal proof, provides usefulinsights understanding WPLs dynamics.8. equations linear, could substituted unknowns terms pmin1 p2min easilydetermined whether pmin2 pmin1 > 0, similar IGA-WoLF.533fiA BDALLAH L ESSER4.1 Numerical Solutionused Matlab solve equations numerically. Figure 4 shows theoretical behaviorpredicted model matching-pennies game. clear resemblance actual(experimental) behavior game (Figure 5). Note time-scale horizontalaxes figures effectively same, displayed horizontal axisFigure 5 decision steps simulation. multiplied actual policy-learning-rate(the time step) used experiments, 0.001, axes become identical.10.90.80.7policy0.60.50.40.30.20.100246810timeFigure 4: Convergence WPL predicted theoretical model matching pennies game.1player1player20.8policy0.60.40.200200040006000800010000timeFigure 5: Convergence WPL experiments, using = 0.001 .Figure 6 plots p(t) versus q(t), game NE= (0.9, 0.9) (u1 = 0.5, u2 = 0.45, u3 =0.5, , u4 = 0.45) starting 157 different initial joint policies (40 initial policiesside joint policy space). Figure 7 plots p(t) q(t) time, showing convergence157 initial joint policies.repeated numerical solution 100 different NE(s) make 10x10 gridp-q space (and starting 157 boundary initial joint policies individual NE).WPL algorithm converges NE spiral fashion similar specific case Figure 6100 NE(s). Instead drawing 100 figures (one NE), Figure 8 plots merge100 figures compact way: plotting agents joint policy time 700 time 800 (whichenough convergence Figure 7 shows). two agents converge 100 NE cases,indicated centric points. Note algorithm converging, joint policiestrajectories time 700 occupy space, 157 initial joint policiespolicy space boundary.534fiM ULTIAGENT L EARNING N - LINEAR DYNAMICSqpFigure 6: illustration WPL convergence (0.9,0.9) NE p-q space: p horizontal axisq vertical axis.policytimeFigure 7: illustration WPL convergence (0.9,0.9) NE: p(t) (gray) q(t) (black) plottedvertical axis time (horizontal axis).Figure 8: illustration WPLs convergence 10x10 NE(s).535fiA BDALLAH L ESSER4.2 Comparing Dynamics IGA, IGA-WoLF, WPLdifferential equations modeling three algorithms, compare dynamicspoint main distinguishing characteristics WPL. Matlab used solvedifferential equations (of three algorithms) numerically. Figure 9 shows dynamicsgame u1u3 < 0 NE=(0.5,0.5). joint strategy moves clockwise direction.dynamics WPL close IGA-WoLF, IGA-WoLF converging bit faster (after onecomplete round around NE, IGA-WoLF closer NE WPL). still impressiveWPL comparable performance IGA-WoLF, since WPL require agents knowNE strategy underlying game priori, unlike IGA-WoLF.1IGAIGAWoLFWPL0.90.80.7player 20.60.50.40.30.20.1000.20.40.60.81player 1Figure 9: Dynamics IGA, IGA-WoLF, WPL game NE=(0.5,0.5).Figure 10 shows dynamics game u1u3 < 0 NE=(0.5,0.1). Three interestingregions figure designated A,B, C. Region shows IGA IGAWoLF dynamics discontinuous due effect projection function. WPL usessmooth policy weighting scheme, dynamics remain continuous. also true region B.region C, WPL initially deviates NE IGA, eventually converges well.reason NE, case, closer boundary, policy weighting makesvertical player move much slower pace moving downward (the right half)horizontal player.Figure 11 shows dynamics coordination game (Table 1(a)), starting initial jointpolicy (0.1,0.6). coordination game two NEs: (0,0) (1,1). algorithms convergecloser NE, (0,0), see IGA IGA-WoLF discontinuitydynamics, unlike WPL smoothly converge NE. Notice WPL converges pureNE limit, graph shows joint policy space, depiction time.continuity WPLs dynamics comes play target policy mixed. IGA,GIGA, GIGA-WoLF algorithms go extreme deterministic policies transientperiod prior convergence, cause ripple effect realistic settings large numberagents interacting asynchronously (e.g. network). WPL never reaches extremesexperimental results Section 5.4 show GIGA-WoLF takes significantly timeconverge compared WPL domain 100 agents.536fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS1IGAIGAWoLFWPL0.8player 20.60.40.20CB00.10.20.30.40.5player 10.60.70.80.91Figure 10: Dynamics IGA, IGA-WoLF, WPL game NE=(0.5,0.1). Three regionsparticular interest highlighted figure: A, B, C.1IGAIGAWoLFWPL0.90.80.7player 20.60.50.40.30.20.1000.10.20.30.40.50.6player 10.70.80.9Figure 11: Dynamics IGA, IGA-WoLF, WPL coordination game two NEs=(0,0)(1,1). Note IGAs dynamics exactly IGA-WoLFs dynamics case.following section presents experimental results benchmark 2-player-2-actiongames larger games involving actions agents.5. Experimental Resultssection divided three parts. Section 5.1 discusses main learning parameters needset practice. Section 5.2 presents experimental results 2x2 benchmark games.Section 5.3 presents experimental results domains larger number actions and/oragents.537fiA BDALLAH L ESSER5.1 Learning ParametersConducting experiments WPL involves setting two main parameters: policy-learning-rate,, value-learning-rate, . theory (as discussed Section 4) policy-learning-rate,, approach zero. practice, however, possible tried settingdifferent small values 0.01 0.00001. smaller is, longer take WPLconverge, smaller oscillations around NE become (and vice versa), similarprevious GA-MARL algorithms. reasonable value used experiments= 0.002.value-learning-rate used compute expected reward action time t,rt (a), known priori. common approach, used previous GA-MARLalgorithms also use here, using equation rt+1 (a) Rt + (1 )rt (a), Rtsample reward received time 0 1 (Sutton & Barto, 1999). tried threevalues : 0.01,0.1, 1.5.2 Benchmark 2x2 Gamesimplemented set previous MARL algorithms comparison: PHC-WoLF (the realisticimplementation IGA-WoLF), GIGA, GIGA-WoLF. experiments useddecaying rates. reason open system dynamics continuously change, onewould want learning continuous well. fixed exploration rate 0.1 (whichcomes play modified projection function Section 3), policy-learning-rate0.002, value-learning-rate 0.1 (unless otherwise specified).first set experiments show results applying algorithm three benchmarkgames described Table 1 10 simulation runs. Figure 12 plots (r1) (c1) (i.e. probability choosing first action row player column player respectively) time.matching pennies tricky games, initial joint policy ([0.1, 0.9]r , [0.9, 0.1]c ) (wealso tested 0.5,0.5 initial joint policy similar results) plot average across10 runs, standard deviation shown vertical bar. coordination game plotted singlerun two probable pure NEs averaging runs capture that. WPLconverges one NE strategies runs.Figure 13 shows convergence GIGA, PHC-WoLF, GIGA-WoLF algorithmscoordination game. expected, GIGA, PHC-WoLF, GIGA-WoLF converges one NEstrategies faster, WPL slightly biased pure strategies.Figure 14 shows convergence previous GA-MARL algorithms matching penniesgame. GIGA hopelessly oscillates around NE, expected. PHC-WoLF better, relatively high standard deviation across runs (when compared WPL). GIGA-WoLF comparableperformance WPL, GIGA-WoLF takes longer converge (the width oscillations aroundNE higher WPL).tricky game deserves attention one challenging two-player-two-actiongames (Bowling & Veloso, 2002). Figure 15 shows convergence PHC-WoLF, GIGA,GIGA-WoLF. GIGA-WoLF converges slower rate approach (Figure 12(c)).performance GIGA, PHC-WoLF, GIGA-WoLF conforms results reported previously authors (Bowling & Veloso, 2002; Bowling, 2005). remainder experiments (Section 5.3) focuses GIGA-WoLF performed best among previousGA-MARL algorithms.538fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS1player1player20.80.6policy0.40.20050000100000150000200000time(a) WPL: coordination game1player1player20.80.6policy0.40.20050000100000150000200000time(b) WPL: matching pennies game1player1player20.80.6policy0.40.20050000100000150000200000time(c) WPL: tricky gameFigure 12: Convergence WPL benchmark two-player-two-action games. figures plot probability playing first action player (vertical axis) time (horizontal axis).5.3 Games Larger 2x2Figures 16 17 plot policy row player time games Table 2WPL GIGA-WoLF initial joint strategy ([0.1, 0.8, 0.1]r , [0.8, 0.1, 0.1]c ) (wealso tried ([ 13 , 13 , 31 ]r , [ 31 , 13 , 13 ]c ), produced similar results), = 0.001, two values: 0.1 1. rock-paper-scissors game (Figure 16) GIGA-WoLF WPL converge= 0.1, WPL converges = 1. Shapleys game (Figure 17) GIGAWoLF keeps oscillating = 1 = 0.1 (GIGA-WoLFs performance gets worse539fiA BDALLAH L ESSER1player1player20.80.6policy0.40.20050000100000150000200000time(a) PHC-WoLF: coordination game1player1player20.80.6policy0.40.20050000100000150000200000time(b) GIGA: coordination game1player1player20.80.6policy0.40.20050000100000150000200000time(c) GIGA-WoLF: coordination gameFigure 13: Convergence previous GA-MARL algorithms (GIGA, PHC-WoLF, GIGA-WoLF)coordination game. figures plot probability playing first action player(vertical axis) time (horizontal axis).increases). WPL hand performs better increases converges Shapleys game= 1.believe reason small leads out-of-date reward estimate turnleads agents continuously chase NE without successfully converging. Smaller meanssamples contribute computed expected reward. Using samples estimate rewardmakes estimate accurate. However, time required get samples may factdegrade accuracy reward estimate, expected reward changes time.540fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS1player1player20.80.6policy0.40.20050000100000150000200000time(a) PHC-WoLF: matching game1player1player20.80.6policy0.40.20050000100000150000200000time(b) GIGA: matching game1player1player20.80.6policy0.40.20050000100000150000200000time(c) GIGA-WoLF: matching gameFigure 14: Convergence GA-MARL algorithms (GIGA, PHC-WoLF, GIGA-WoLF) matching pennies game. figures plot probability playing first action player(vertical axis) time (horizontal axis).Setting value always 1 result sub-optimal performance, illustratedfollowing experiment. Consider biased game shown Table 3. NE policy biasedgame mixed probabilities uniform across actions, unlike previous benchmark gamesmixed NE uniform probability across actions. set 0.01, WPL convergespolicy close NE policy shown Figure 18. = 1, WPL converged policyfar NE policy shown Figure 19.541fiA BDALLAH L ESSER1player1player20.80.6policy0.40.20050000100000150000200000time(a) PHC-WoLF: tricky game1player1player20.80.6policy0.40.20050000100000150000200000time(b) GIGA: tricky game1player1player20.80.6policy0.40.20050000100000150000200000time(c) GIGA-WoLF: tricky gameFigure 15: Convergence GA-MARL algorithms (GIGA, PHC-WoLF, GIGA-WoLF) trickygame. figures plot probability playing first action player (vertical axis)time (horizontal axis).Table 3: Biased game: NE=(0.15,0.85) & (0.85,0.15)a1a2a1 1.0,1.85 1.85,1.0a2 1.15,1.0 1.00,1.15542fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS11a0a0a10.8a10.8a20.60.6policypolicy0.40.40.20.20a20050000100000150000200000050000time100000150000200000time(b) WPL, = 0.1(a) GIGA-WoLF, = 0.111a0a0a10.8a10.8a20.6a20.6policypolicy0.40.40.20.20001000002000003000004000005000006000000time50000100000150000200000time(c) GIGA-WolF, = 1(d) WPL, = 1Figure 16: Convergence GIGA-WoLF WPL rock-paper-scissors game. figures plotprobability playing action first player (vertical axis) time (horizontal axis).understand effect = 1 case WPL, consider following scenariobiased game. Suppose policy column player fixed 0.7,0.3. value small,action value function approximates well expected value action. valuefirst row action (from row player perspective) = 0.71 + 0.31.85= 1.255. Similarly,value second row action = 0.7 1.15 + 0.31 = 1.105. Unless column player changespolicy, row player gradient clearly points toward increasing probability choosingfirst action (up 1). consider case = 1. case action value reflectslatest (sample) reward, average. case, probability choosing first action, p,increases average=0.7 [0.7 (1 1.15) p + 0.35 (1 1)] + 0.3[0.7 (1.85 1.15) (1 p) + 0.3 (1.85 1) (1 p)]=0.297p + 0.2235means row players policy effectively stabilize = 0 p = 0.75.words, possible players stabilize equilibrium NE. Noteproblem occurs mainly biased games. common benchmark games NE uniformacross actions pure, WPL still converge NE even = 1, shown Figure 16Figure 17.Tuning easy task expected reward dynamically changing (becauseagents changing policies concurrently). currently investigating extension543fiA BDALLAH L ESSER11a0a0a10.8a10.8a20.6a20.6policypolicy0.40.40.20.200050000100000150000200000050000time(a) GIGA-WolF, = 0.1100000150000200000time(b) WPL, = 0.111a0a0a10.8a10.8a20.60.6policypolicy0.40.40.20.20a20050000100000150000200000050000100000time150000200000time(c) GIGA-WolF, = 1(d) WPL, = 1Figure 17: Convergence GIGA-WoLF WPL Shapleys game. figures plot probabilityplaying action first player (vertical axis) time (horizontal axis).1player1player20.80.6policy0.40.2002000004000006000008000001e+006timeFigure 18: Convergence WPL fixed value-learning-rate 0.01 biased game. Horizontal axistime. Vertical axis probability first action player.WPL automatically recognizes oscillations adjusts value accordingly. Preliminaryresults promising.following section illustrates applicability algorithm applying WPLrealistic setting involving 100 agents.544fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS1player1player20.8policy0.60.40.2005000001e+0061.5e+0062e+0062.5e+0063e+006timeFigure 19: Convergence WPL fixed value-learning-rate 1.0 biased game. Horizontal axistime. Vertical axis probability first action player.5.4 Distributed Task Allocation Problem (DTAP)use simplified version distributed task allocation domain (DTAP) (Abdallah & Lesser,2007), goal multiagent system assign tasks agents servicetime task minimized. illustration, consider example scenario depicted Figure20. Agent A0 receives task T1, executed agents A0, A1, A2, A3,A4. agents agent A4 overloaded, therefore best option agent A0forward task T1 agent A2 turn forwards task left neighbor (A5) taskT1 reaches agent A4. Although agent A0 know A4 under-loaded (because agentA0 interacts immediate neighbors), agent A0 eventually learn (through experienceinteraction neighbors) sending task T1 agent A2 best action without evenknowing agent A4 exists.A1A0A5A2A3T1A4Figure 20: Task allocation using network agents.Q-learning appropriate setting due communication delay (which resultspartial observability). example, even two neighbors practically load, Q-learningassign incoming requests one neighbors feedback received later indicatingchange load. noted Q-learning successfully used packet routingdomain (Boyan & Littman, 1994; Dutta, Jennings, & Moreau, 2005), load balancing545fiA BDALLAH L ESSERmain concern (the main objective routing packet particular source particulardestination).let us define different aspects DTAP domain formally. time unit,agents make decisions regarding task requests received time unit. task,agent either execute task locally send task neighboring agent. agent decidesexecute task locally, agent adds task local queue, tasks executedfirst come first serve basis, unlimited queue length.agent physical location. Communication delay two agents proportionalEuclidean distance them, one time unit per distance unit. Agents interact via twotypes messages. REQUEST message hi, j, indicates request sent agent agentj requesting task . UPDATE message hi, j, T, Ri indicates feedback (reward signal)agent agent j task took R time steps complete (the time steps computed since agentreceived request).P main goal DTAP reduce total service time, averaged tasks, ST =ST (T ), set task requests received time period TSTtotal time task spends system. TST consists time routing task requestnetwork, time task request spends local queue, time actually executingtask.noted although underlying simulator different underlying states,deliberately made agent oblivious states. feedback agent gets (consistentinitial claim) reward. agents learn joint policy makes good compromisedifferent unobserved states (because agents distinguish states).evaluated WPLs performance using following setting.9 100 agents organized10x10 grid. Communication delay two adjacent agents two time units. Tasks arrive4x4 sub-grid center rate 0.5 tasks/time unit. agents execute task rate0.1 task/time unit (both task arrival service durations follow exponential distribution).Figure 21 shows results applying GIGA, GIGA-WoLF WPL using value-learning-rate1 policy-learning-rate 0.0001. GIGA fail converge, WPL GIGA-WoLFconverge. WPL converges faster better ATST: GIGA-WoLFs ATST converges around100 time units, WPLs ATST converges around 70 time units.believe GIGA-WoLFs slow convergence due way GIGA-WoLF works. GIGAWoLF relies learning slowly moving policy addition actual policy order approximate NE policy. requires time WPL algorithm. Furthermore, GIGA-WoLFsdynamics discontinuous prior convergence reach extreme deterministic policies evenNE policy mixed. large system, ripple effect slow system-wideconvergence. WPL, hand, continuous dynamics, allowing faster collective convergence.|T |6. Conclusion Future Workwork presents WPL, gradient ascent multiagent reinforcement learning algorithm (GAMARL) assumes agent neither knows underlying game observes agents.experimentally show WPL converges benchmark 2-player-2-action games. also show9. simulator available online http://www.cs.umass.edu/shario/dtap.html.546fiM ULTIAGENT L EARNING N - LINEAR DYNAMICS3000WPLGIGA2500GIGAWoLF2000ATST150010005000020000400006000080000100000timeFigure 21: ATST 10x10 grid different MARL algorithms. Horizontal axis time vertical axisATST.WPL converges Shapleys game none previous GA-MARL algorithms successfully converged. verify practicality algorithm distributed task allocation domainnetwork 100 agents. WPL outperforms state-of-the-art GA-MARL algorithmsspeed convergence expected reward. analyze dynamics WPL showcontinuous non-linear dynamics, previous GA-MARL algorithms discontinuous dynamics. show predicted theoretical behavior consistent experimentalresults.work briefly illustrated importance value-learning-rate affects convergence, particularly proposed algorithm WPL. Finding right balance theoreticallyanalyzing affects convergence interesting research questions. currently investigating extension WPL automatically finds good value-learning-rate. Preliminary experimental results promising.Another future direction considering extending theoretical analysis gamesactions players order verify experimental findings Shapleys gamedistributed task allocation domain. currently investigating alternative methodologiesanalyzing dynamics, including evolutionary dynamics (Tuyls, Hoen, & Vanschoenwinkel,2006) Lyapunov stability theory (Khalil, 2002).7. Acknowledgmentswork based previous conference publications (Abdallah & Lesser, 2006, 2008).ReferencesAbdallah, S., & Lesser, V. (2006). Learning task allocation game. Proceedings International Joint Conference Autonomous Agents Multiagent Systems, pp. 850857.Abdallah, S., & Lesser, V. (2007). Multiagent reinforcement learning self-organizationnetwork agents. Proceedings International Joint Conference AutonomousAgents Multiagent Systems.547fiA BDALLAH L ESSERAbdallah, S., & Lesser, V. (2008). Non-linear dynamics multiagent reinforcement learning algorithms. Proceedings International Joint Conference Autonomous AgentsMultiagent Systems, pp. 13211324.Banerjee, B., & Peng, J. (2003). Adaptive policy gradient multiagent learning. ProceedingsInternational Joint Conference Autonomous Agents Multi Agent Systems, pp.686692.Banerjee, B., & Peng, J. (2007). Generalized multiagent learning performance bound. Autonomous Agents Multiagent Systems, 15(3), 281312.Bowling, M. (2004). Convergence no-regret multiagent learning. Tech. rep., UniversityAlberta.Bowling, M. (2005). Convergence no-regret multiagent learning. ProceedingsAnnual Conference Advances Neural Information Processing Systems, pp. 209216.Bowling, M., & Veloso, M. (2002). Multiagent learning using variable learning rate. ArtificialIntelligence, 136(2), 215250.Boyan, J. A., & Littman, M. L. (1994). Packet routing dynamically changing networks:reinforcement learning approach. Proceedings Annual Conference AdvancesNeural Information Processing Systems, pp. 671678.Claus, C., & Boutilier, C. (1998). dynamics reinforcement learning cooperative multiagentsystems.. Proceedings National Conference Artificial intelligence/InnovativeApplications Artificial Intelligence, pp. 746752.Conitzer, V., & Sandholm, T. (2007). AWESOME: general multiagent learning algorithmconverges self-play learns best response stationary opponents. MachineLearning, 67(1-2), 2343.Dutta, P. S., Jennings, N. R., & Moreau, L. (2005). Cooperative information sharing improvedistributed learning multi-agent systems. Journal Artificial Intelligence Research, 24,407463.Hu, J., & Wellman, M. P. (2003). Nash Q-learning general-sum stochastic games. JournalMachine Learning Research, 4, 10391069.Khalil, H. K. (2002). Nonlinear Systems. Prentice-Hall, Upper Saddle River, NJ, USA.Littman, M. (2001). Value-function reinforcement learning Markov games. Cognitive SystemsResearch, 2(12), 5566.Peshkin, L., Kim, K.-E., Meuleau, N., & Kaelbling, L. P. (2000). Learning cooperate via policysearch. Proceedings Conference Uncertainty Artificial Intelligence, pp. 307314.Singh, S., Kearns, M., & Mansour, Y. (2000). Nash convergence gradient dynamics generalsum games. Proceedings Conference Uncertainty Artificial Intelligence, pp.541548.548fiM ULTIAGENT L EARNING N - LINEAR DYNAMICSSutton, R., & Barto, A. (1999). Reinforcement Learning: Introduction. MIT Press.Tuyls, K., Hoen, P. J., & Vanschoenwinkel, B. (2006). evolutionary dynamical analysismulti-agent learning iterated games. Autonomous Agents Multi-Agent Systems, 12(1),115153.Wang, X., & Sandholm, T. (2003). Reinforcement learning play optimal Nash equilibriumteam Markov games. Proceedings Annual Conference Advances NeuralInformation Processing Systems, pp. 15711578.Zinkevich, M. (2003). Online convex programming generalized infinitesimal gradient ascent..Proceedings International Conference Machine Learning, pp. 928936.549fiJournal Artificial Intelligence Research 33 (2008) 615-655Submitted 09/08; published 12/08Latent Relation Mapping Engine:Algorithm ExperimentsPeter D. Turneypeter.turney@nrc-cnrc.gc.caInstitute Information TechnologyNational Research Council CanadaOttawa, Ontario, Canada, K1A 0R6AbstractMany AI researchers cognitive scientists argued analogy corecognition. influential work computational modeling analogy-makingStructure Mapping Theory (SMT) implementation Structure Mapping Engine(SME). limitation SME requirement complex hand-coded representations.introduce Latent Relation Mapping Engine (LRME), combines ideasSME Latent Relational Analysis (LRA) order remove requirement handcoded representations. LRME builds analogical mappings lists words, usinglarge corpus raw text automatically discover semantic relations among words.evaluate LRME set twenty analogical mapping problems, ten based scientificanalogies ten based common metaphors. LRME achieves human-level performancetwenty problems. compare LRME variety alternative approachesfind able reach level performance.1. Introductionfaced problem, try recall similar problems facedpast, transfer knowledge past experience currentproblem. make analogy past situation current situation,use analogy transfer knowledge (Gentner, 1983; Minsky, 1986; Holyoak & Thagard,1995; Hofstadter, 2001; Hawkins & Blakeslee, 2004).survey computational modeling analogy-making, French (2002) citesStructure Mapping Theory (SMT) (Gentner, 1983) implementation StructureMapping Engine (SME) (Falkenhainer, Forbus, & Gentner, 1989) influentialwork modeling analogy-making. SME, analogical mapping : Bsource target B. source familiar, known, concrete,whereas target relatively unfamiliar, unknown, abstract. analogical mappingused transfer knowledge source target.Gentner (1983) argues two kinds similarity, attributional similarityrelational similarity. distinction attributes relations may understood terms predicate logic. attribute predicate one argument,large(X), meaning X large. relation predicate two arguments,collides with(X, ), meaning X collides .Structure Mapping Engine prefers mappings based relational similaritymappings based attributional similarity (Falkenhainer et al., 1989). example, SMEable build mapping representation solar system (the source)c2008National Research Council Canada. Reprinted permission.fiTurneyrepresentation Rutherford-Bohr model atom (the target). sun mappednucleus, planets mapped electrons, mass mapped charge. Notemapping emphasizes relational similarity. sun nucleus differentterms attributes: sun large nucleus small. Likewise,planets electrons little attributional similarity. hand, planets revolvearound sun like electrons revolve around nucleus. mass sun attractsmass planets like charge nucleus attracts charge electrons.Gentner (1991) provides evidence children rely primarily attributional similaritymapping, gradually switching relational similarity mature. usesterms mere appearance refer mapping based mostly attributional similarity, analogyrefer mapping based mostly relational similarity, literal similarity refermixture attributional relational similarity. Since use analogical mappings solveproblems make predictions, focus structure, especially causal relations,look beyond surface attributes things (Gentner, 1983). analogysolar system Rutherford-Bohr model atom illustrates importancegoing beyond mere appearance, underlying structures.Figures 1 2 show LISP representations used SME input analogysolar system atom (Falkenhainer et al., 1989). Chalmers, French,Hofstadter (1992) criticize SMEs requirement complex hand-coded representations.argue hard work done human creates high-levelhand-coded representations, rather SME.(defEntity sun :type inanimate)(defEntity planet :type inanimate)(defDescription solar-systementities (sun planet)expressions (((mass sun) :name mass-sun)((mass planet) :name mass-planet)((greater mass-sun mass-planet) :name >mass)((attracts sun planet) :name attracts-form)((revolve-around planet sun) :name revolve)((and >mass attracts-form) :name and1)((cause and1 revolve) :name cause-revolve)((temperature sun) :name temp-sun)((temperature planet) :name temp-planet)((greater temp-sun temp-planet) :name >temp)((gravity mass-sun mass-planet) :name force-gravity)((cause force-gravity attracts-form) :name why-attracts)))Figure 1: representation solar system SME (Falkenhainer et al., 1989).Gentner, Forbus, colleagues attempted avoid hand-codingrecent work SME.1 CogSketch system generate LISP representationssimple sketches (Forbus, Usher, Lovett, Lockwood, & Wetzel, 2008). Gizmo systemgenerate LISP representations qualitative physics models (Yan & Forbus, 2005).Learning Reader system generate LISP representations natural language text(Forbus et al., 2007). systems require LISP input.1. Dedre Gentner, personal communication, October 29, 2008.616fiThe Latent Relation Mapping Engine(defEntity nucleus :type inanimate)(defEntity electron :type inanimate)(defDescription rutherford-atomentities (nucleus electron)expressions (((mass nucleus) :name mass-n)((mass electron) :name mass-e)((greater mass-n mass-e) :name >mass)((attracts nucleus electron) :name attracts-form)((revolve-around electron nucleus) :name revolve)((charge electron) :name q-electron)((charge nucleus) :name q-nucleus)((opposite-sign q-nucleus q-electron) :name >charge)((cause >charge attracts-form) :name why-attracts)))Figure 2: Rutherford-Bohr model atom SME (Falkenhainer et al., 1989).However, CogSketch user interface requires person draws sketch identify basic components sketch hand-label terms knowledgebase derived OpenCyc. Forbus et al. (2008) note OpenCyc contains58,000 hand-coded concepts, added hand-coded concepts OpenCyc,order support CogSketch. Gizmo system requires user hand-code physicalmodel, using methods qualitative physics (Yan & Forbus, 2005). Learning Readeruses 28,000 phrasal patterns, derived ResearchCyc (Forbuset al., 2007). evident SME still requires substantial hand-coded knowledge.work present paper effort avoid complex hand-coded representations. approach combine ideas SME (Falkenhainer et al., 1989) LatentRelational Analysis (LRA) (Turney, 2006). call resulting algorithm Latent Relation Mapping Engine (LRME). represent semantic relation two termsusing vector, elements derived pattern frequencies large corpusraw text. semantic relations automatically derived corpus, LRMErequire hand-coded representations relations. needs list termssource list terms target. Given two lists, LRME uses corpusbuild representations relations among terms, constructs mappingtwo lists.Tables 1 2 show input output LRME analogy solarsystem Rutherford-Bohr model atom. Although human effort involvedconstructing input lists, considerably less effort SME requires input(contrast Figures 1 2 Table 1).Scientific analogies, analogy solar system RutherfordBohr model atom, may seem esoteric, believe analogy-making ubiquitousdaily lives. potential practical application work task identifyingsemantic roles (Gildea & Jurafsky, 2002). Since roles relations, attributes,appropriate treat semantic role labeling analogical mapping problem.example, Judgement semantic frame contains semantic roles judge,evaluee, reason, Statement frame contains roles speaker, addressee, message, topic, medium (Gildea & Jurafsky, 2002). task identifying617fiTurneySourceplanetattractsrevolvessungravitysolar systemmassTarget BrevolvesatomattractselectromagnetismnucleuschargeelectronTable 1: representation input LRME.Sourcesolar systemsunplanetmassattractsrevolvesgravityMappingTarget BatomnucleuselectronchargeattractsrevolveselectromagnetismTable 2: representation output LRME.semantic roles automatically label sentences roles, following examples (Gildea & Jurafsky, 2002):[Judge She] blames [Evaluee Government] [Reason failing enoughhelp].[Speaker We] talked [Topic proposal] [Medium phone].training set labeled sentences testing set unlabeled sentences,may view task labeling testing sentences problem creating analogicalmappings training sentences (sources) testing sentences (targets). Table 3 shows blames Government failing enough help. mightmapped blame company polluting environment. mappingfound, transfer knowledge, form semantic role labels, sourcetarget.SourceblamesgovernmentfailinghelpMappingTarget BblamecompanypollutingenvironmentTable 3: Semantic role labeling analogical mapping.Section 2, briefly discuss hypotheses behind design LRME.precisely define task performed LRME, specific form analogical mapping,618fiThe Latent Relation Mapping EngineSection 3. LRME builds Latent Relational Analysis (LRA), hence summarize LRASection 4. discuss potential applications LRME Section 5.evaluate LRME, created twenty analogical mapping problems, ten science analogy problems (Holyoak & Thagard, 1995) ten common metaphor problems (Lakoff &Johnson, 1980). Table 1 one science analogy problems. intended solutiongiven Table 2. validate intended solutions, gave colleagues liststerms (as Table 1) asked generate mappings lists. Section 6presents results experiment. Across twenty problems, average agreementintended solutions (as Table 2) 87.6%.LRME algorithm outlined Section 7, along evaluation twentymapping problems. LRME achieves accuracy 91.5%. differenceperformance human average 87.6% statistically significant.Section 8 examines variety alternative approaches analogy mapping task.best approach achieves accuracy 76.8%, approach requires hand-coded partof-speech tags. performance significantly LRME human performance.Section 9, discuss questions raised results precedingsections. Related work described Section 10, future work limitations consideredSection 11, conclude Section 12.2. Guiding Hypothesessection, list assumptions guided design LRME.results present paper necessarily require assumptions, mighthelpful reader, understand reasoning behind approach.1. Analogies semantic relations: Analogies based semantic relations(Gentner, 1983). example, analogy solar system Rutherford-Bohr model atom based similarity semantic relationsamong concepts involved understanding solar system semanticrelations among concepts involved Rutherford-Bohr model atom.2. Co-occurrences semantic relations: Two terms interesting, significant semantic relation tend co-occur within relativelysmall window (e.g., five words) relatively large corpus (e.g., 1010 words).interesting semantic relation causes co-occurrence co-occurrence reliableindicator interesting semantic relation (Firth, 1957).3. Meanings semantic relations: Meaning relations amongwords individual words. Individual words tend ambiguous polysemous.putting two words pair, constrain possible meanings. puttingwords sentence, multiple relations among words sentence,constrain possible meanings further. focus word pairs (or tuples), insteadindividual words, word sense disambiguation less problematic. Perhaps wordsense apart relations words (Kilgarriff, 1997).4. Pattern distributions semantic relations: many-to-many mapping semantic relations patterns two terms co-occur.example, relation CauseEffect(X, ) may expressed X causes ,619fiTurneyX, due X, X, on. Likewise, patternX may expression CauseEffect(X, ) (sick bacteria)OriginEntity(X, ) (oranges Spain). However, given X , statistical distribution patterns X co-occur reliable signaturesemantic relations X (Turney, 2006).extent LRME works, believe success lends support hypotheses.3. Taskpaper, examine algorithms generate analogical mappings. simplicity,restrict task generating bijective mappings; is, mappings injective(one-to-one; instance two terms source map termtarget) surjective (onto; source terms cover target terms;target term left mapping). assume entitiesmapped given input. Formally, input algorithms two sets terms,B.= {hA, Bi}(1)Since mappings bijective, B must contain number terms, m.= {a1 , a2 , . . . , }(2)B = {b1 , b2 , . . . , bm }(3)term, ai bj , may consist single word (planet) compound two words(solar system). words may part speech (nouns, verbs, adjectives, adverbs).output bijective mapping B.= {M : B}(4)(ai ) B(5)(A) = {M (a1 ), (a2 ), . . . , (am )} = B(6)algorithms consider accept batch multiple independent mappingproblems input generate mapping one output.= {hA1 , B1 , hA2 , B2 , . . . , hAn , Bn i}(7)= {M1 : A1 B1 , M2 : A2 B2 , . . . , Mn : Bn }(8)Suppose terms arbitrary order a.= ha1 , a2 , . . . ,mapping function : B, given a, determines unique ordering b B.620(9)fiThe Latent Relation Mapping Engineb = hM (a1 ), (a2 ), . . . , (am )i(10)Likewise, ordering b B, given a, defines unique mapping function . Sincem! possible orderings B, also m! possible mappings B. tasksearch m! mappings find best one. (Section 6 showsrelatively high degree consensus mappings best.)Let P (A, B) set m! bijective mappings B. (P stands permutation, since mapping corresponds permutation.)P (A, B) = {M1 , M2 , . . . , Mm! }(11)= |A| = |B|(12)m! = |P (A, B)|(13)following experiments, 7 average 9 most, m! usually around7! = 5, 040 9! = 362, 880. feasible us exhaustively search P (A, B).explore two basic kinds algorithms generating analogical mappings, algorithmsbased attributional similarity algorithms based relational similarity (Turney,2006). attributional similarity two words, sima (a, b) <, dependsdegree correspondence properties b. correspondenceis, greater attributional similarity. relational similarity twopairs words, simr (a : b, c : d) <, depends degree correspondencerelations : b c : d. correspondence is, greater relationalsimilarity. example, dog wolf relatively high degree attributional similarity,whereas dog : bark cat : meow relatively high degree relational similarity.Attributional mapping algorithms seek mapping (or mappings) maximizessum attributional similarities terms correspondingterms B. (When multiple mappings maximize sum, break tierandomly choosing one them.)= arg maxXsima (ai , (ai ))(14)P (A,B) i=1Relational mapping algorithms seek mapping (or mappings) Mr maximizessum relational similarities.Mr = arg maxXXsimr (ai : aj , (ai ) : (aj ))(15)P (A,B) i=1 j=i+1(15), assume simr symmetrical. example, degree relational similaritydog : bark cat : meow degree relational similaritybark : dog meow : cat.simr (a : b, c : d) = simr (b : a, : c)(16)also assume simr (a : a, b : b) interesting; example, may constantvalue b. Therefore (15) designed always less j.621fiTurneyLet scorer (M ) scorea (M ) defined follows.scorer (M ) =scorea (M ) =XXsimr (ai : aj , (ai ) : (aj ))i=1 j=i+1Xsima (ai , (ai ))(17)(18)i=1Mr may defined terms scorer (M ) scorea (M ).Mr = arg max scorer (M )(19)P (A,B)= arg max scorea (M )(20)P (A,B)Mr best mapping according simr best mapping according sima .Recall Gentners (1991) terms, discussed Section 1, mere appearance (mostly attributional similarity), analogy (mostly relational similarity), literal similarity (a mixtureattributional relational similarity). take Mr abstract model mapping based analogy model mere appearance. literal similarity,combine Mr , take care normalize scorer (M ) scorea (M )combine them. (We experiment combining Section 9.2.)4. Latent Relational AnalysisLRME uses simplified form Latent Relational Analysis (LRA) (Turney, 2005, 2006)calculate relational similarity pairs words. briefly describe pastwork LRA present LRME.LRA takes input set word pairs generates output relationalsimilarity simr (ai : bi , aj : bj ) two pairs input.= {a1 : b1 , a2 : b2 , . . . , : bn }(21)= {simr : <}(22)LRA designed evaluate proportional analogies. Proportional analogies form: b :: c : d, means b c d. example, mason : stone :: carpenter : woodmeans mason stone carpenter wood. mason artisan worksstone carpenter artisan works wood.consider proportional analogies special case bijective analogical mapping,defined Section 3, |A| = |B| = = 2. example, a1 : a2 :: b1 : b2 equivalentM0 (23).= {a1 , a2 } , B = {b1 , b2 } , M0 (a1 ) = b1 , M0 (a2 ) = b2 .definition scorer (M ) (17), following result M0 .622(23)fiThe Latent Relation Mapping Enginescorer (M0 ) = simr (a1 : a2 , M0 (a1 ) : M0 (a2 )) = simr (a1 : a2 , b1 : b2 )(24)is, quality proportional analogy mason : stone :: carpenter : wood givensimr (mason : stone, carpenter : wood).Proportional analogies may also evaluated using attributional similarity.definition scorea (M ) (18), following result M0 .scorea (M0 ) = sima (a1 , M0 (a1 )) + sima (a2 , M0 (a2 )) = sima (a1 , b1 ) + sima (a2 , b2 )(25)attributional similarity, quality proportional analogy mason : stone :: carpenter :wood given sima (mason, carpenter) + sima (stone, wood).LRA handles proportional analogies. main contribution LRME extendLRA beyond proportional analogies bijective analogies > 2.Turney (2006) describes ten potential applications LRA: recognizing proportionalanalogies, structure mapping theory, modeling metaphor, classifying semantic relations,word sense disambiguation, information extraction, question answering, automatic thesaurus generation, information retrieval, identifying semantic roles. Twoapplications (evaluating proportional analogies classifying semantic relations) experimentally evaluated, state-of-the-art results.Turney (2006) compares performance relational similarity (24) attributionalsimilarity (25) task solving 374 multiple-choice proportional analogy questionsSAT college entrance test. LRA used measure relational similarity varietylexicon-based corpus-based algorithms used measure attributional similarity.LRA achieves accuracy 56% 374 SAT questions, significantlydifferent average human score 57%. hand, best performanceattributional similarity 35%. results show attributional similarity betterrandom guessing, good relational similarity. result consistentGentners (1991) theory maturation human similarity judgments.Turney (2006) also applies LRA task classifying semantic relations nounmodifier expressions. noun-modifier expression phrase, laser printer,head noun (printer) preceded modifier (laser). task identify semanticrelation noun modifier. case, relation instrument;laser instrument used printer. set 600 hand-labeled noun-modifier pairsfive different classes semantic relations, LRA attains 58% accuracy.Turney (2008) employs variation LRA solving four different language tests,achieving 52% accuracy SAT analogy questions, 76% accuracy TOEFL synonymquestions, 75% accuracy task distinguishing synonyms antonyms, 77%accuracy task distinguishing words similar, words associated,words similar associated. core algorithm usedfour tests, tuning parameters particular test.5. Applications LRMESince LRME extension LRA, every potential application LRA also potentialapplication LRME. advantage LRME LRA ability handle bijective623fiTurneyanalogies > 2 (where = |A| = |B|). section, consider kindsapplications might benefit ability.Section 7.2, evaluate LRME science analogies common metaphors,supports claim two applications benefit ability handle larger setsterms. Section 1, saw identifying semantic roles (Gildea & Jurafsky, 2002)also involves two terms, believe LRME superior LRAsemantic role labeling.Semantic relation classification usually assumes relations binary; is,semantic relation connection two terms (Rosario & Hearst, 2001; Nastase& Szpakowicz, 2003; Turney, 2006; Girju et al., 2007). Yuret observed binary relations may linked underlying n-ary relations.2 example, Nastase Szpakowicz(2003) defined taxonomy 30 binary semantic relations. Table 4 shows six binary relations Nastase Szpakowicz (2003) covered one 5-ary relation,Agent:Tool:Action:Affected:Theme. Agent uses Tool perform Action. Somebodysomething Affected Action. whole event summarized Theme.Nastase Szpakowicz (2003)RelationExampleagentstudent protestpurposeconcert hallbeneficiarystudent discountinstrumentlaser printerobjectmetal separatorobject property sunken shipAgent:Tool:Action:Affected:ThemeAgent:ActionTheme:ToolAffected:ActionTool:AgentAffected:ToolAction:AffectedTable 4: six binary semantic relations Nastase Szpakowicz (2003)viewed different fragments one 5-ary semantic relation.SemEval Task 4, found easier manually tag datasets expandedbinary relations underlying n-ary relations (Girju et al., 2007). believeexpansion would also facilitate automatic classification semantic relations. resultsSection 9.3 suggest applications LRA discussed Section 4might benefit able handle bijective analogies > 2.6. Mapping Problemsevaluate algorithms analogical mapping, created twenty mapping problems,given Appendix A. twenty problems consist ten science analogy problems, basedexamples analogy science Chapter 8 Holyoak Thagard (1995), tencommon metaphor problems, derived Lakoff Johnson (1980).tables Appendix show intended mappings twenty problems. validate mappings, invited colleagues Institute InformationTechnology participate experiment. experiment hosted web server2. Deniz Yuret, personal communication, February 13, 2007. observation contextwork building datasets SemEval 2007 Task 4 (Girju et al., 2007).624fiThe Latent Relation Mapping Engine(only accessible inside institute) people participated anonymously, using webbrowsers offices. 39 volunteers began experiment 22went way end. analysis, use data 22 participantscompleted mapping problems.instructions participants Appendix A. sequence problemsorder terms within problem randomized separately participant,remove effects due order. Table 5 shows agreement intendedmapping mappings generated participants. Across twenty problems,average agreement 87.6%, higher agreement figures manylinguistic annotation tasks. agreement impressive, given participantsminimal instructions training.TypescienceanalogiescommonmetaphorsMappingA1A2A3A4A5A6A7A8A9A10M1M2M3M4M5M6M7M8M9M10Source Targetsolar system atomwater flow heat transferwaves soundscombustion respirationsound lightprojectile planetartificial selection natural selectionbilliard balls gas moleculescomputer mindslot machine bacterial mutationwar argumentbuying item accepting beliefgrounds building reasons theoryimpediments travel difficultiesmoney timeseeds ideasmachine mindobject ideafollowing understandingseeing understandingAverageAgreement90.986.981.879.079.297.474.788.184.383.693.596.187.9100.077.389.098.789.196.678.887.6788877789577676775867.0Table 5: average agreement intended mappings mappings22 participants. See Appendix details.column labeled gives number terms set source termsmapping problem (which equal number terms set target terms).average problem, = 7. third column Table 5 gives mnemonic summarizesmapping (e.g., solar system atom). Note mnemonic used inputalgorithms, mnemonic shown participants experiment.agreement figures Table 5 individual mapping problem averagesmappings problem. Appendix gives detailed view, showingagreement individual mapping mappings. twenty problems containtotal 140 individual mappings (20 7). Appendix shows every one 140625fiTurneymappings agreement 50% higher. is, every case, majorityparticipants agreed intended mapping. (There two cases agreementexactly 50%. See problems A5 Table 14 M5 Table 16 Appendix A.)select mapping chosen majority 22 participants,get perfect score twenty problems. precisely, try m! mappingsproblem, select mapping maximizes sum number participantsagree individual mapping mappings, score100% twenty problems. strong support intended mappingsgiven Appendix A.Section 3, applied Genters (1991) categories mere appearance (mostly attributional similarity), analogy (mostly relational similarity), literal similarity (a mixtureattributional relational similarity) mappings Mr , Mrbest mapping according simr best mapping according sima . twentymapping problems chosen analogy problems; is, intended mappingsAppendix meant relational mappings, Mr ; mappings maximize relationalsimilarity, simr . tried avoid mere appearance literal similarity.Section 7 use twenty mapping problems evaluate relational mappingalgorithm (LRME), Section 8 use evaluate several different attributionalmapping algorithms. hypothesis LRME perform significantly betterattributional mapping algorithms twenty mapping problems,analogy problems (not mere appearance problems literal similarity problems).expect relational attributional mapping algorithms would perform approximatelyequally well literal similarity problems, expect mere appearance problemswould favour attributional algorithms relational algorithms, testlatter two hypotheses, primary interest paper analogy-making.goal test hypothesis real, practical, effective, measurabledifference output LRME output various attributional mapping algorithms. skeptic might claim relational similarity simr (a : b, c : d)reduced attributional similarity sima (a, c) + sima (b, d); therefore relational mappingalgorithm complicated solution illusory problem. slightly less skeptical claimrelational similarity versus attributional similarity valid distinction cognitivepsychology, relational mapping algorithm capture distinction. testhypothesis refute skeptical claims, created twenty analogical mappingproblems, show LRME handles problems significantly bettervarious attributional mapping algorithms.7. Latent Relation Mapping EngineLatent Relation Mapping Engine (LRME) seeks mapping Mr maximizessum relational similarities.Mr = arg maxXXsimr (ai : aj , (ai ) : (aj ))(26)P (A,B) i=1 j=i+1search Mr exhaustively evaluating possibilities. Ties broken randomly. use simplified form LRA (Turney, 2006) calculate simr .626fiThe Latent Relation Mapping Engine7.1 AlgorithmBriefly, idea LRME build pair-pattern matrix X, rows correspondpairs terms columns correspond patterns. example, row xi: mightcorrespond pair terms sun : solar system column x:j might correspondpattern X centered . patterns, wild card, matchsingle word. value element xij X based frequency patternx:j , X instantiated terms pair xi: . example,take pattern X centered instantiate X : pair sun : solar system,pattern sun centered solar system , thus value elementxij based frequency sun centered solar system corpus. matrixX smoothed truncated singular value decomposition (SVD) (Golub & Van Loan,1996) relational similarity simr two pairs terms given cosineangle two corresponding row vectors X.detail, LRME takes input set mapping problems generatesoutput corresponding set mappings.= {hA1 , B1 , hA2 , B2 , . . . , hAn , Bn i}(27)= {M1 : A1 B1 , M2 : A2 B2 , . . . , Mn : Bn }(28)following experiments, twenty mapping problems (Appendix A) processedone batch (n = 20).first step make list R contains pairs terms input I.mapping problem hA, Bi I, add R pairs ai : aj , ai ajmembers A, 6= j, pairs bi : bj , bi bj members B, 6= j.|A| = |B| = m, m(m 1) pairs m(m 1) pairs B.3typical pair R would sun : solar system. allow duplicates R; R listpair types, pair tokens. twenty mapping problems, R list 1,694 pairs.pair r R, make list S(r) phrases corpus containpair r. Let ai : aj terms pair r. search corpus phrasesfollowing form:[0 1 words] ai [0 3 words] aj [0 1 words](29)ai : aj R, aj : ai also R, find phrases members pairsorders, S(ai : aj ) S(aj : ai ). search template (29) usedTurney (2008).following experiments, search corpus 5 1010 English words (about 280GB plain text), consisting web pages gathered web crawler.4 retrieve phrases3. m(m 1) here, m(m 1)/2, need pairs orders. wantcalculate simr one order pairs, always less j (26); however, ensuresimr symmetrical, (16), need make matrix X symmetrical, rowsmatrix orders every pair.4. corpus collected Charles Clarke University Waterloo. provide copiescorpus request.627fiTurneycorpus, use Wumpus (Buttcher & Clarke, 2005), efficient search enginepassage retrieval large corpora.51,694 pairs R, find total 1,996,464 phrases corpus, average1,180 phrases per pair. pair r = sun : solar system, typical phraseS(r) would sun centered solar system illustrates.Next make list C patterns, based phrases found. pairr R, r = ai : aj , found phrase S(r), replace ai Xreplace aj . remaining words may either left replacedwild card symbol . replace ai aj X, replaceremaining words wild cards leave are. n remainingwords s, ai aj replaced, generate 2n+1 patterns s, addpatterns C. add new patterns C; is, C list pattern types,pattern tokens; duplicates C.example, pair sun : solar system, found phrase sun centered solarsystem illustrates. replace ai : aj X : , X centeredillustrates. three remaining words, generate eight patterns,X illustrates, X centered , X illustrates, on.patterns added C. replace ai : aj : X, yielding centered Xillustrates. gives us another eight patterns, centered X . Thusphrase sun centered solar system illustrates generates total sixteen patterns,add C.revise R, make list pairs correspond rows frequencymatrix F. remove pairs R phrases found corpus,terms either order. Let ai : aj terms pair r. remover R S(ai : aj ) S(aj : ai ) empty. remove rowswould correspond zero vectors matrix F. reduces R 1,694 pairs 1,662pairs. Let nr number pairs R.Next revise C, make list patterns correspond columnsfrequency matrix F. following experiments, stage, C contains millionspatterns, many efficient processing standard desktop computer. needreduce C manageable size. select patterns sharedpairs. Let c pattern C. Let r pair R. phrase S(r),pattern generated identical c, say r onepairs generated c. sort patterns C descending order numberpairs R generated pattern, select top tnr patternssorted list. Following Turney (2008), set parameter 20; hence C reducedtop 33,240 patterns (tnr = 20 1,662 = 33,240). Let nc number patternsC (nc = tnr ).rows R columns C defined, build frequency matrixF. Let ri i-th pair terms R (e.g., let ri sun : solar system) let cjj-th pattern C (e.g., let cj X centered ). instantiate Xpattern cj terms ri ( sun centered solar system ). element fij Ffrequency instantiated pattern corpus.5. Wumpus developed Stefan Buttcher available http://www.wumpus-search.org/.628fiThe Latent Relation Mapping EngineNote need search corpus instantiated patternfij , order find frequency. process creating pattern, keep trackmany phrases generated pattern, pair. get frequency fijchecking record patterns generated ri .next step transform matrix F raw frequencies form Xenhances similarity measurement. Turney (2006) used log entropy transformation,suggested Landauer Dumais (1997). kind tf-idf (term frequencytimes inverse document frequency) transformation, gives weight elementsmatrix statistically surprising. However, Bullinaria Levy (2007) recentlyachieved good results new transformation, called PPMIC (Positive Pointwise MutualInformation Cosine); therefore LRME uses PPMIC. raw frequencies F usedcalculate probabilities, calculate pointwise mutual information(PMI) element matrix. element negative PMI set zero.fijpij = Pnr Pncj=1 fiji=1(30)Pncj=1 fijpi = Pnr Pnc(31)PnrfPncij= Pnr i=1(32)i=1pji=1j=1 fijj=1 fijpijpi pjpmiij = logpmiij pmiij > 0xij =0 otherwise(33)(34)Let ri i-th pair terms R (e.g., let ri sun : solar system) let cjj-th pattern C (e.g., let cj X centered ). (33), pij estimated probabilitypattern cj instantiated pair ri ( sun centered solar system ), piestimated probability ri , pj estimated probability cj . ri cjstatistically independent, pi pj = pij (by definition independence), thuspmiij zero (since log(1) = 0). interesting semantic relationterms ri , pattern cj captures aspect semantic relation,expect pij larger would ri cj indepedent; hence findpij > pi pj , thus pmiij positive. (See Hypothesis 2 Section 2.)hand, terms completely different domains may avoid other, casefind pmiij negative. PPMIC designed give high value xijpattern cj captures aspect semantic relation terms ri ; otherwise,xij value zero, indicating pattern cj tells us nothingsemantic relation terms ri .experiments, F density 4.6% (the percentage nonzero elements)X density 3.8%. lower density X due elements negative PMI,transformed zero PPMIC.629fiTurneysmooth X applying truncated singular value decomposition (SVD) (Golub& Van Loan, 1996). use SVDLIBC calculate SVD X.6 SVDLIBC designedsparse (low density) matrices. SVD decomposes X product three matricesUVT , U V column orthonormal form (i.e., columns orthogonalunit length, UT U = VT V = I) diagonal matrix singular values(Golub & Van Loan, 1996). X rank r, also rank r. Let k ,k < r, diagonal matrix formed top k singular values, let Uk Vkmatrices produced selecting corresponding columns U V. matrixUk k VkT matrix rank k best approximates original matrix X, senseminimizes approximation errors. is, X = Uk k VkT minimizes kX XkFmatrices X rank k, k . . . kF denotes Frobenius norm (Golub & VanLoan, 1996). may think matrix Uk k VkT smoothed compressed versionoriginal matrix X. Following Turney (2006), set parameter k 300.relational similarity simr two pairs R inner product twocorresponding rows Uk k VkT , rows normalized unit length.simplify calculations dropping Vk (Deerwester, Dumais, Landauer, Furnas, & Harshman,1990). take matrix Uk k normalize row unit length. Let Wresulting matrix. let Z WWT , square matrix size nr nr . matrix containscosines combinations two pairs R.mapping problem hA, Bi I, let : a0 pair terms let b : b0pair terms B. Suppose ri = : a0 rj = b : b0 , ri rji-th j-th pairs R. simr (a : a0 , b : b0 ) = zij , zij element i-throw j-th column Z. either : a0 b : b0 R, S(a : a0 ), S(a0 : a),S(b : b0 ), S(b0 : b) empty, set similarity zero. Finally, mappingproblem I, output map Mr maximizes sum relational similarities.Mr = arg maxXXsimr (ai : aj , (ai ) : (aj ))(35)P (A,B) i=1 j=i+1simplified form LRA used calculate simr differs LRA used Turney(2006) several ways. LRME, use synonyms generate alternate formspairs terms. LRME, morphological processing terms. LRME usesPPMIC (Bullinaria & Levy, 2007) process raw frequencies, instead log entropy.Following Turney (2008), LRME uses slightly different search template (29) LRMEsets number columns nc tnr , instead using constant. Section 7.2,evaluate impact two changes (PPMIC nc ), testedchanges, mainly motivated desire increased efficiencysimplicity.7.2 Experimentsimplemented LRME Perl, making external calls Wumpus searching corpusSVDLIBC calculating SVD. used Perl Net::Telnet package interprocess6. SVDLIBC work Doug Rohde available http://tedlab.mit.edu/dr/svdlibc/.630fiThe Latent Relation Mapping Enginecommunication Wumpus, PDL (Perl Data Language) package matrix manipulations (e.g., calculating cosines), List::Permutor package generate permutations(i.e., loop P (A, B)).ran following experiments dual core AMD Opteron 64 computer, running64 bit Linux. running time spent searching corpus phrases. took16 hours 27 minutes Wumpus fetch 1,996,464 phrases. remaining stepstook 52 minutes, SVD took 10 minutes. running time could cut halfusing RAID 0 speed disk access.Table 6 shows performance LRME baseline configuration. comparison,agreement 22 volunteers intended mapping copied Table 5.difference performance LRME (91.5%) human participants(87.6%) statistically significant (paired t-test, 95% confidence level).MappingA1A2A3A4A5A6A7A8A9A10M1M2M3M4M5M6M7M8M9M10AverageSource Targetsolar system atomwater flow heat transferwaves soundscombustion respirationsound lightprojectile planetartificial selection natural selectionbilliard balls gas moleculescomputer mindslot machine bacterial mutationwar argumentbuying item accepting beliefgrounds building reasons theoryimpediments travel difficultiesmoney timeseeds ideasmachine mindobject ideafollowing understandingseeing understandingAccuracyLRME Humans100.090.9100.086.9100.081.8100.079.071.479.2100.097.471.474.7100.088.155.684.3100.083.671.493.5100.096.1100.087.9100.0100.0100.077.3100.089.0100.098.760.089.1100.096.6100.078.891.587.6Table 6: LRME baseline configuration, compared human performance.Table 6, column labeled Humans average 22 people, whereas LRMEcolumn one algorithm (it average). Comparing average several scoresindividual score (whether individual human algorithm) may givemisleading impression. results individual person, typically several100% scores scores 55-75% range. average mapping problem seventerms. possible exactly one term mapped incorrectly;incorrect mappings, must two incorrect mappings. followsnature bijections. Therefore score 5/7 = 71.4% uncommon.631fiTurneyTable 7 looks results another perspective. column labeled LRME wronggives number incorrect mappings made LRME twenty problems.five columns labeled Number people N wrong show, various values N ,may 22 people made N incorrect mappings. average mapping problem,15 22 participants perfect score (N = 0); remaining 7 participants, 5made two mistakes (N = 2). Table 7 shows clearly Table 6 LRMEsperformance significantly different (individual) human performance. (For yetanother perspective, see Section 9.1).MappingA1A2A3A4A5A6A7A8A9A10M1M2M3M4M5M6M7M8M9M10AverageLRMEwrong000020204020000002001Number people N wrongN =0 N =1 N =2 N =3 N 4160420140503909229090410072320020080662130801110722130900170500190300140800220000901102150430210100180211190300130333150511788877789577676775867Table 7: Another way viewing LRME versus human performance.Table 8, examine sensitivity LRME parameter settings. first rowshows accuracy baseline configuration, Table 6. next eight rows showimpact varying k, dimensionality truncated singular value decomposition,50 400. eight rows show effect varying t, column factor,5 40. number columns matrix (nc ) given number rows (nr= 1,662) multiplied t. second last row shows effect eliminating singularvalue decomposition LRME. equivalent setting k 1,662, numberrows matrix. final row gives result PPMIC (Bullinaria & Levy,2007) replaced log entropy (Turney, 2006). LRME sensitivemanipulations: None variations Table 8 perform significantly differentlybaseline configuration (paired t-test, 95% confidence level). (This necessarily meanmanipulations effect; rather, suggests larger sample problemswould needed show significant effect.)632fiThe Latent Relation Mapping EngineExperimentbaseline configurationvarying kvaryingdropping SVDlog entropyk3005010015020025030035040030030030030030030030030016623002020202020202020205101520253035402020nc33,24033,24033,24033,24033,24033,24033,24033,24033,2408,31016,62024,93033,24041,55049,86058,17066,48033,24033,240Accuracy91.589.392.891.392.690.691.590.690.686.994.094.091.590.190.689.591.789.783.9Table 8: Exploring sensitivity LRME various parameter settings modifications.8. Attribute Mapping Approachessection, explore variety attribute mapping approaches twenty mappingproblems. approaches seek mapping maximizes sumattributional similarities.= arg maxXsima (ai , (ai ))(36)P (A,B) i=1search exhaustively evaluating possibilities. Ties broken randomly. use variety different algorithms calculate sima .8.1 Algorithmsfollowing experiments, test five lexicon-based attributional similarity measuresuse WordNet:7 HSO (Hirst & St-Onge, 1998), JC (Jiang & Conrath, 1997), LC (Leacock & Chodrow, 1998), LIN (Lin, 1998), RES (Resnik, 1995). five implementedPerl package WordNet::Similarity,8 builds WordNet::QueryData9 package. core idea behind treat WordNet graph measure semanticdistance two terms length shortest path graph.Similarity increases distance decreases.7. WordNet developed team Princeton available http://wordnet.princeton.edu/.8. Ted Pedersens WordNet::Similarity package http://www.d.umn.edu/tpederse/similarity.html.9. Jason Rennies WordNet::QueryData package http://people.csail.mit.edu/jrennie/WordNet/.633fiTurneyHSO works nouns, verbs, adjectives, adverbs, JC, LC, LIN, RESwork nouns verbs. used WordNet::Similarity try possible parts speechpossible senses input word. Many adjectives, true valuable,also noun verb senses WordNet, JC, LC, LIN, RES still ablecalculate similarity them. raw form word found WordNet,WordNet::Similarity searches morphological variations word.multiple similarity scores, multiple parts speech multiple senses, selecthighest similarity score. similarity score, word WordNet,JC, LC, LIN, RES could find alternative noun verb formadjective adverb, set score zero.also evaluate two corpus-based attributional similarity measures: PMI-IR (Turney,2001) LSA (Landauer & Dumais, 1997). core idea behind wordcharacterized company keeps (Firth, 1957). similarity two termsmeasured similarity statistical distributions corpus. used corpusSection 7 along Wumpus implement PMI-IR (Pointwise Mutual InformationInformation Retrieval). LSA (Latent Semantic Analysis), used onlinedemonstration.10 selected Matrix Comparison option General Reading1st year college (300 factors) topic space term-to-term comparison type. PMI-IRLSA work parts speech.eighth similarity measure based observation intended mappingsmap terms part speech (see Appendix A). Let POS(a) partof-speech tag assigned term a. use part-of-speech tags define measureattributional similarity, simPOS (a, b), follows.100 = b10 POS(a) = POS(b)(37)simPOS (a, b) =0 otherwisehand-labeled terms mapping problems part-of-speech tags (Santorini,1990). Automatic taggers assume words tagged embeddedsentence, terms mapping problems sentences, tagsambiguous. used knowledge intended mappings manually disambiguatepart-of-speech tags terms, thus guaranteeing corresponding termsintended mapping always tags.first seven attributional similarity measures above, created sevensimilarity measures combining simPOS (a, b). example, let simHSO (a, b)Hirst St-Onge (1998) similarity measure. combine simPOS (a, b) simHSO (a, b)simply adding them.simHSO+POS (a, b) = simHSO (a, b) + simPOS (a, b)(38)values returned simPOS (a, b) range 0 100, whereas values returnedsimHSO (a, b) much smaller. chose large values (37) getting POS tagsmatch weight similarity measures. manual POS tags10. online demonstration LSA work team University Colorado Boulder.available http://lsa.colorado.edu/.634fiThe Latent Relation Mapping Enginehigh weight simPOS (a, b) give unfair advantage attributional mappingapproach, relational mapping approach afford generous.8.2 ExperimentsTable 9 presents accuracy various measures attributional similarity.best result without POS labels 55.9% (HSO). best result POS labels 76.8%(LIN+POS). 91.5% accuracy LRME (see Table 6) significantly higher76.8% accuracy LIN+POS (and thus, course, significantly higher everything elseTable 9; paired t-test, 95% confidence level). average human performance 87.6%(see Table 5) also significantly higher 76.8% accuracy LIN+POS (paired t-test,95% confidence level). summary, humans LRME perform significantly bettervariations attributional mapping approaches tested.AlgorithmHSOJCLCLINRESPMI-IRLSAPOS (hand-labeled)HSO+POSJC+POSLC+POSLIN+POSRES+POSPMI-IR+POSLSA+POSReferenceHirst St-Onge (1998)Jiang Conrath (1997)Leacock Chodrow (1998)Lin (1998)Resnik (1995)Turney (2001)Landauer Dumais (1997)Santorini (1990)Hirst St-Onge (1998)Jiang Conrath (1997)Leacock Chodrow (1998)Lin (1998)Resnik (1995)Turney (2001)Landauer Dumais (1997)Accuracy55.954.748.548.243.854.439.644.871.173.669.576.871.672.865.8Table 9: accuracy attribute mapping approaches wide variety measuresattributional similarity.9. Discussionsection, examine three questions suggested preceding results.difference science analogy problems common metaphorproblems? advantage combining relational attributional mapping approaches? advantage relational mapping approach attributionalmapping approach?9.1 Science Analogies versus Common MetaphorsTable 5 suggests science analogies may difficult common metaphors.supported Table 10, shows agreement 22 participantsintended mapping (see Section 6) varies science problems metaphor635fiTurneyproblems. science problems lower average performance greater variationperformance. difference science problems metaphor problemsstatistically significant (paired t-test, 95% confidence level).Participant12345678910111213141516171819202122AverageStandard deviation2072.688.290.071.895.783.479.691.989.780.794.590.693.297.186.680.593.386.592.990.482.796.287.67.2Average Accuracy10 Science 10 Metaphor59.985.485.990.586.393.856.487.194.297.183.982.973.685.795.088.890.089.381.480.095.793.387.493.889.696.794.3100.088.584.880.280.789.996.778.994.296.089.884.196.774.990.594.997.584.690.710.85.8Table 10: comparison difficulty science problems versus metaphor problems 22 participants. numbers bold font scoresscores LRME.average science problem terms (7.4) average metaphor problem(6.6), might contribute difficulty science problems. However, Table 11shows clear relation number terms problem (mTable 5) level agreement. believe people find metaphor problemseasier science problems common metaphors entrenchedlanguage, whereas science analogies peripheral.Table 12 shows 16 algorithms studied perform slightly worse scienceproblems metaphor problems, difference statistically significant(paired t-test, 95% confidence level). hypothesize attributional mapping approaches performing well enough sensitive subtle differences scienceanalogies common metaphors.Incidentally, tables give us another view performance LRME comparison human performance. first row Table 12 shows performance LRME636fiThe Latent Relation Mapping EngineNum terms56789Agreement86.481.391.186.584.3Table 11: average agreement among 22 participants function numberterms problems.AlgorithmLRMEHSOJCLCLINRESPMI-IRLSAPOSHSO+POSJC+POSLC+POSLIN+POSRES+POSPMI-IR+POSLSA+POSAverageStandard deviation2091.555.954.748.548.243.854.439.644.871.173.669.576.871.672.865.861.414.7Average Accuracy10 Science 10 Metaphor89.893.157.454.357.452.149.647.546.749.739.048.649.559.237.341.942.147.466.975.278.169.270.868.268.884.870.372.965.779.969.162.459.962.915.015.3Table 12: comparison difficulty science problems versus metaphor problems 16 algorithms.science metaphor problems. Table 10, marked bold font caseshuman scores greater LRMEs scores. 20 problems, 8cases; 10 science problems, 8 cases; 10 metaphor problems, 10 cases. evidence LRMEs performancesignificantly different human performance. LRME near middle rangeperformance 22 human participants.9.2 Hybrid Relational-Attributional ApproachesRecall definitions scorer (M ) scorea (M ) given Section 3.637fiTurneyscorer (M ) =scorea (M ) =XXsimr (ai : aj , (ai ) : (aj ))i=1 j=i+1Xsima (ai , (ai ))(39)(40)i=1combine scores simply adding multiplying them, scorer (M )scorea (M ) may quite different scales distributions values; thereforefirst normalize probabilities.scorer (M )Mi P (A,B) scorer (Mi )(41)scorea (M )Mi P (A,B) scorea (Mi )(42)probr (M ) = Pproba (M ) = Pprobability estimates, assume scorer (M ) 0 scorea (M ) 0.necessary, constant value may added scores, ensure negative.combine scores adding multiplying probabilities.Mr+a = arg max probr (M ) + proba (M )(43)P (A,B)Mra = arg max probr (M ) proba (M )(44)P (A,B)Table 13 shows accuracy LRME combined LIN+POS (the best attributional mapping algorithm Table 9, accuracy 76.8%) HSO (the bestattributional mapping algorithm use manual POS tags, accuracy55.9%). try adding multiplying probabilities. own, LRMEaccuracy 91.5%. Combining LRME LIN+POS increases accuracy 94.0%,improvement statistically significant (paired t-test, 95% confidence level). Combining LRME HSO results decrease accuracy. decrease significantprobabilities multiplied (85.4%), significant probabilitiesadded (78.5%).summary, experiments show significant advantage combining LRMEattributional mapping. However, possible larger sample problems wouldshow significant advantage. Also, combination methods explored (additionmultiplication probabilities) elementary. sophisticated approach,weighted combination, may perform better.9.3 Coherent Relationshypothesize LRME benefits kind coherence among relations.hand, attributional mapping approaches involve kind coherence.638fiThe Latent Relation Mapping EngineComponentsRelational AttributionalLRMELIN+POSLRMELIN+POSLRMEHSOLRMEHSOCombinationadd probabilitiesmultiply probabilitiesadd probabilitiesmultiply probabilitiesAccuracy94.094.078.585.4Table 13: performance four different hybrids relational attributional mappingapproaches.Suppose swap two terms mapping. Let original mappinglet 0 new mapping, 0 (a1 ) = (a2 ), 0 (a2 ) = (a1 ), 0 (ai ) = (ai )> 2. attributional similarity, impact swap score mappinglimited. Part score affected.scorea (M ) = sima (a1 , (a1 )) + sima (a2 , (a2 )) +Xsima (ai , (ai ))(45)sima (ai , (ai ))(46)i=3scorea (M 0 ) = sima (a1 , (a2 )) + sima (a2 , (a1 )) +Xi=3hand, relational similarity, impact swap limitedway. change part mapping affects whole score. kind globalcoherence relational similarity lacking attributional similarity.Testing hypothesis LRME benefits coherence somewhat complicated,need design experiment coherence effect isolatedeffects. this, move terms outside accuracy calculation.Let : B one twenty mapping problems, intendedmapping = |A| = |B|. Let A0 randomly selected subset size m0 . Let B 0(A0 ), subset B maps A0 .A0(47)0B B0(48)0B = (A )m0 = A0 = B 00<m(49)(50)(51)two ways might use LRME generate mapping 0 : A0 B 0new reduced mapping problem, internal coherence total coherence.1. Internal coherence: select 0 based hA0 , B 0 alone.639fiTurneyA0 = {a1 , ..., am0 }(52)0B = {b1 , ..., bm0 }(53)m00 = arg maxm0X XP (A0 ,B 0 ) i=1 j=i+1simr (ai : aj , (ai ) : (aj ))(54)case, 0 chosen based relations internal hA0 , B 0 i.2. Total coherence: select 0 based hA, Bi knowledge 0must satisfy constraint 0 (A0 ) = B 0 . (This knowledge also embeddedinternal coherence.)= {a1 , ..., }(55)B = {b1 , ..., bm }P (A, B) = | P (A, B) (A0 ) = B 0XX0= arg maxsimr (ai : aj , (ai ) : (aj ))0(56)(57)(58)P 0 (A,B) i=1 j=i+1case, 0 chosen using relations internal hA0 , B 0relations hA, Bi external hA0 , B 0 i.Suppose calculate accuracy two methods based subproblem hA0 , B 0 i. first might seem advantage total coherence,must explore larger space possible mappings internal coherence (since|P 0 (A, B)| larger |P (A0 , B 0 )|), additional terms exploresinvolved calculating accuracy. However, hypothesize total coherencehigher accuracy internal coherence, additional external relationshelp select correct mapping.test hypothesis, set m0 3 randomly generated ten new reducedmapping problems twenty problems (i.e., total 200 new problems size3). average accuracy internal coherence 93.3%, whereas average accuracytotal coherence 97.3%. difference statistically significant (paired t-test, 95%confidence level).hand, attributional mapping approaches cannot benefit totalcoherence, connection attributes hA0 , B 0attributes outside. decompose scorea (M ) two independent parts.640fiThe Latent Relation Mapping EngineA00 = \ A00(59)00A=AP (A, B) = | P (A, B) (A0 ) = B 0X0 = arg maxsima (ai , (ai ))(60)0(61)(62)P 0 (A,B)= arg maxP 0 (A,B)Xaisima (ai , (ai )) +A0Xaisima (ai , (ai ))(63)A00two parts optimized independently. Thus terms externalhA0 , B 0 influence part 0 covers hA0 , B 0 i.Relational mapping cannot decomposed independent parts way,relations connect parts. gives relational mapping approaches inherentadvantage attributional mapping approaches.confirm analysis, compared internal total coherence using LIN+POS200 new problems size 3. average accuracy internal coherence88.0%, whereas average accuracy total coherence 87.0%. differencestatistically significant (paired t-test, 95% confidence level). (The reasondifference that, two mappings score, break ties randomly.causes random variation accuracy.)benefit coherence suggests make analogy mapping problems easierLRME adding terms. difficulty new terms cannot randomlychosen; must fit logic analogy overlap existing terms.course, important difference relational attributional mapping approaches. believe important difference relationsreliable general attributes, using past experiences makepredictions future (Hofstadter, 2001; Gentner, 2003). Unfortunately, hypothesis difficult evaluate experimentally hypothesis coherence.10. Related WorkFrench (2002) gives good survey computational approaches analogy-making,perspective cognitive science (where emphasis well computational systemsmodel human performance, rather well systems perform). samplesystems survey add mentioned.French (2002) categorizes analogy-making systems symbolic, connectionist, symbolicconnectionist hybrids. Gardenfors (2004) proposes another category representationalsystems AI cognitive science, calls conceptual spaces. spatial geometric systems common information retrieval machine learning (Widdows, 2004;van Rijsbergen, 2004). influential example Latent Semantic Analysis (Landauer &Dumais, 1997). first spatial approaches analogy-making began appear aroundtime Frenchs (2002) survey. LRME takes spatial approach analogy-making.641fiTurney10.1 Symbolic ApproachesComputational approaches analogy-making date back Analogy (Evans, 1964)Argus (Reitman, 1965). systems designed solve proportional analogies(analogies |A| = |B| = 2; see Section 4). Analogy could solve proportionalanalogies simple geometric figures Argus could solve simple word analogies.systems used hand-coded rules able solve limited range problemsdesigners anticipated coded rules.French (2002) cites Structure Mapping Theory (SMT) (Gentner, 1983) StructureMapping Engine (SME) (Falkenhainer et al., 1989) prime examples symbolicapproaches:SMT unquestionably influential work date modelinganalogy-making applied wide range contexts rangingchild development folk physics. SMT explicitly shifts emphasis analogymaking structural similarity source target domains. Twomajor principles underlie SMT:relation-matching principle: good analogies determined mappings relations attributes (originally identical predicatesmapped)systematicity principle: mappings coherent systems relationspreferred mappings individual relations.structural approach intended produce domain-independent mapping process.LRME follows principles. LRME uses relational similarity; attributional similarity involved (see Section 7.1). Coherent systems relations preferredmappings individual relations (see Section 9.3). However, spatial (statistical,corpus-based) approach LRME quite different symbolic (logical, hand-coded)approach SME.Martin (1992) uses symbolic approach handle conventional metaphors. Gentner,Bowdle, Wolff, Boronat (2001) argue novel metaphors processed analogies,conventional metaphors recalled memory without special processing. However,line conventional novel metaphor unclear.Dolan (1995) describes algorithm extract conventional metaphorsdictionary. semantic parser used extract semantic relations LongmanDictionary Contemporary English (LDOCE). symbolic algorithm finds metaphoricalrelations words, using extracted relations.Veale (2003, 2004) developed symbolic approach analogy-making, using WordNet lexical resource. Using spreading activation algorithm, achieved score43.0% set 374 multiple-choice lexical proportional analogy questions SATcollege entrance test (Veale, 2004).Lepage (1998) demonstrated symbolic approach proportional analogiesused morphology processing. Lepage Denoual (2005) apply similar approachmachine translation.642fiThe Latent Relation Mapping Engine10.2 Connectionist ApproachesConnectionist approaches analogy-making include ACME (Holyoak & Thagard, 1989)LISA (Hummel & Holyoak, 1997). Like symbolic approaches, systems use handcoded knowledge representations, search mappings takes connectionist approach, nodes weights incrementally updated time,system reaches stable state.10.3 Symbolic-Connectionist Hybrid Approachesthird family examined French (2002) hybrid approaches, containing elementssymbolic connectionist approaches. Examples include Copycat (Mitchell,1993) Tabletop (French, 1995). Much work Fluid Analogies ResearchGroup (FARG) concerns symbolic-connectionist hybrids (Hofstadter & FARG, 1995).10.4 Spatial ApproachesMarx, Dagan, Buhmann, Shamir (2002) present coupled clustering algorithm,uses feature vector representation find analogies collections text. example,given documents Buddhism Christianity, finds related terms, {school,Mahayana, Zen} Buddhism {tradition, Catholic, Protestant} Christianity.Mason (2004) describes CorMet system extracting conventional metaphorstext. CorMet based clustering feature vectors represent selectional preferencesverbs. Given keywords source domain laboratory target domain finance,able discover mappings liquid income container institution.Turney, Littman, Bigham, Shnayder (2003) present system solving lexicalproportional analogy questions SAT college entrance test, combines thirteendifferent modules. Twelve modules use either attributional similarity symbolicapproach relational similarity, one module uses spatial (feature vector) approachmeasuring relational similarity. module worked much bettermodules; therefore, studied detail Turney Littman (2005).relation pair words represented vector, elements patternfrequencies. similar LRME, one important difference TurneyLittman (2005) used fixed, hand-coded set 128 patterns, whereas LRME automaticallygenerates variable number patterns given corpus (33,240 patternsexperiments here).Turney (2005) introduced Latent Relational Analysis (LRA), examinedthoroughly Turney (2006). LRA achieves human-level performance set 374multiple-choice proportional analogy questions SAT college entrance exam. LRMEuses simplified form LRA. similar simplification LRA used Turney (2008),system processing analogies, synonyms, antonyms, associations. contributionLRME go beyond proportional analogies, larger systems analogical mappings.10.5 General Theories Analogy MetaphorMany theories analogy-making metaphor either involve computationsuggest general principles concepts specific particular computational643fiTurneyapproach. design LRME influenced several theories type (Gentner,1983; Hofstadter & FARG, 1995; Holyoak & Thagard, 1995; Hofstadter, 2001; Gentner,2003).Lakoff Johnson (1980) provide extensive evidence metaphor ubiquitouslanguage thought. believe system analogy-making ablehandle metaphorical language, ten analogy problems derivedLakoff Johnson (1980). agree claim metaphor merelyinvolve superficial relation couple words; rather, involves systematic setmappings two domains. Thus analogy problems involve larger sets words,beyond proportional analogies.Holyoak Thagard (1995) argue analogy-making central daily thought,especially finding creative solutions new problems. ten scientific analogiesderived examples analogy-making scientific creativity.11. Limitations Future WorkSection 4, mentioned ten applications LRA, Section 5 claimedresults experiments Section 9.3 suggest LRME may perform better LRAten applications, due ability handle bijective analogies > 2.focus future work testing hypothesis. particular, task semanticrole labeling, discussed Section 1, seems good candidate application LRME.input LRME simpler input SME (compare Figures 1 2Section 1 Table 1), still human effort involved creating input.LRME immune criticism Chalmers, French, Hofstadter (1992),human generates input work computer makesmappings, although trivial matter find right mapping 5,040 (7!)choices.future work, would like relax requirement hA, Bi must bijection(see Section 3), adding irrelevant words (distractors) synonyms. mappingalgorithm forced decide terms include mapping termsleave out.would also like develop algorithm take proportional analogy (m = 2)input (e.g., sun:planet::nucleus:electron) automatically expand larger analogy(m > 2, e.g., Table 2). is, would automatically search corpus new termsadd analogy.next step would give computer topic source domain (e.g.,solar system) topic target domain (e.g., atomic structure), let workrest own. might possible combining ideas LRME ideascoupled clustering (Marx et al., 2002) CorMet (Mason, 2004).seems analogy-making triggered people encounter problem(Holyoak & Thagard, 1995). problem defines target us, immediatelystart searching source. Analogical mapping enables us transfer knowledgesource target, hopefully leading solution problem. suggestsinput ideal analogical mapping algorithm would simply statement644fiThe Latent Relation Mapping Engineproblem (e.g., structure atom?). Ultimately, computer mightfind problems well. input would large corpus.algorithms considered perform exhaustive search setpossible mappings P (A, B). acceptable sets small, here,problematic larger problems. future work, necessary useheuristic search algorithms instead exhaustive search.takes almost 18 hours LRME process twenty mapping problems (Section 7).better hardware changes software, time could significantlyreduced. even greater speed, algorithm could run continuously, building largedatabase vector representations term pairs, ready create mappingssoon user requests them. similar vision Banko Etzioni (2007).LRME, like LRA LSA (Landauer & Dumais, 1997), uses truncated singular valuedecomposition (SVD) smooth matrix. Many algorithms proposedsmoothing matrices. past work LRA (Turney, 2006), experimentedNonnegative Matrix Factorization (NMF) (Lee & Seung, 1999), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999), Iterative Scaling (IS) (Ando, 2000), KernelPrincipal Components Analysis (KPCA) (Scholkopf, Smola, & Muller, 1997).interesting results small matrices (around 1000 2000), none algorithmsseemed substantially better truncated SVD, none scaled matrixsizes (1,662 33,240). However, believe SVD unique,future work likely discover smoothing algorithm efficient effectiveSVD. results Section 7.2 show significant benefit SVD. Table 8hints PPMIC (Bullinaria & Levy, 2007) important SVD.LRME extracts knowledge many fragments text. Section 7.1, notedfound average 1,180 phrases per pair. information 1,180phrases combined vector, represent semantic relation pair.quite different relation extraction (for example) Automatic Content Extraction(ACE) Evaluation.11 task ACE identify label semantic relation singlesentence. Semantic role labeling also involves labeling single sentence (Gildea & Jurafsky,2002).contrast LRME ACE analogous distinction cognitivepsychology semantic episodic memory. Episodic memory memoryspecific event ones personal past, whereas semantic memory memory basic factsconcepts, unrelated specific event past. LRME extracts relational informationindependent specific sentence, like semantic memory. ACE concernedextracting relation specific sentence, like episodic memory. cognition, episodicmemory semantic memory work together synergistically. experience event,use semantic memory interpret event form new episodic memory,semantic memory constructed past experiences, accumulatedepisodic memories. suggests synergy combining LRME-likesemantic information extraction algorithms ACE-like episodic information extractionalgorithms.11. ACE annual event began 1999. Relation Detection Characterization (RDC)introduced ACE 2001. information, see http://www.nist.gov/speech/tests/ace/.645fiTurney12. ConclusionAnalogy core cognition. understand present analogy past.predict future analogy past present. solve problems searchinganalogous situations (Holyoak & Thagard, 1995). daily language saturatedmetaphor (Lakoff & Johnson, 1980), metaphor based analogy (Gentner et al.,2001). understand human language, solve human problems, work humans,computers must able make analogical mappings.best theory analogy-making Structure Mapping Theory (Gentner, 1983),Structure Mapping Engine (Falkenhainer et al., 1989) puts much burdenanalogy-making human users (Chalmers et al., 1992). LRME attemptshift burden onto computer, remaining consistent generalprinciples SMT.shown LRME able solve bijective analogical mapping problemshuman-level performance. Attributional mapping algorithms (at least, triedfar) able reach level. supports SMT, claims relationsimportant attributes making analogical mappings.still much research done. LRME takes load humanuser, formulating input LRME easy. paper incremental steptowards future computers make surprising useful analogies minimalhuman assistance.AcknowledgmentsThanks colleagues Institute Information Technology participatingexperiment Section 6. Thanks Charles Clarke Egidio Terra corpus.Thanks Stefan Buttcher making Wumpus available giving advice use.Thanks Doug Rohde making SVDLIBC available. Thanks WordNet teamPrinceton University WordNet, Ted Pedersen WordNet::Similarity Perl package,Jason Rennie WordNet::QueryData Perl package. Thanks LSA teamUniversity Colorado Boulder use online demonstration LSA.Thanks Deniz Yuret, Andre Vellino, Dedre Gentner, Vivi Nastase, Yves Lepage, DiarmuidSeaghdha, Roxana Girju, Chris Drummond, Howard Johnson, Stan Szpakowicz,anonymous reviewers JAIR helpful comments suggestions.Appendix A. Details Mapping Problemsappendix, provide detailed information twenty mapping problems.Figure 3 shows instructions given participants experimentSection 6. instructions displayed web browsers. Tables 14, 15, 16,17 show twenty mapping problems. first column gives problem number(e.g., A1) mnemonic summarizes mapping (e.g., solar system atom).second column gives source terms third column gives target terms.mappings shown tables intended mappings. fourth columnshows percentage participants agreed intended mappings. example,646fiThe Latent Relation Mapping EngineSystematic Analogies MetaphorsInstructionspresented twenty analogical mapping problems, ten based scientificanalogies ten based common metaphors. typical problem look like this:horselegshaybraindung?????may click drop-down menus above, see options available.task construct analogical mapping; is, one-to-one mappingitems left items right. example:horselegshaycarwheelsgasolinebraindungdriverexhaustmapping expresses analogy horse car. horses legs likecars wheels. horse eats hay car consumes gasoline. horses brain controlsmovement horse like cars driver controls movement car. horsegenerates dung waste product like car generates exhaust waste product.duplicate items answers right-hand side.duplicates missing items (question marks), get error messagesubmit answer.welcome use dictionary work problems, would findhelpful.find instructions unclear, please continue exercise.answers twenty problems used standard evaluating outputcomputer algorithm; therefore, proceed confidentunderstand task.Figure 3: instructions participants experiment Section 6.647fiTurneyMappingA1solar systematomA2water flowheat transferA3wavessoundsA4combustionrespirationA5soundlightSourcesolar systemsunplanetmassattractsrevolvesgravityAverage agreement:waterflowspressurewater towerbucketfillingemptyinghydrodynamicsAverage agreement:wavesshorereflectswaterbreakwaterroughcalmcrashingAverage agreement:combustionfirefuelburninghotintenseoxygencarbon dioxideAverage agreement:soundlowhighechoesloudquiethornAverage agreement:Targetatomnucleuselectronchargeattractsrevolveselectromagnetismheattransferstemperatureburnerkettleheatingcoolingthermodynamicssoundswallechoesairinsulationloudquietvibratingrespirationanimalfoodbreathinglivingvigorousoxygencarbon dioxidelightredvioletreflectsbrightdimlensAgreement86.4100.095.586.490.995.581.890.986.495.586.472.772.795.595.590.986.986.477.395.595.581.863.6100.054.581.872.795.590.972.759.177.377.386.479.086.450.054.5100.090.977.395.579.2POSNNNNNNNNVBZVBZNNNNVBZNNNNNNVBGVBGNNNNSNNVBZNNNNJJJJVBGNNNNNNVBGJJJJNNNNNNJJJJVBZJJJJNNTable 14: Science analogy problems A1 A5, derived Chapter 8 HolyoakThagard (1995).648fiThe Latent Relation Mapping EngineMappingA6projectileplanetA7artificial selectionnatural selectionA8billiard ballsgas moleculesA9computermindA10slot machinebacterial mutationSourceprojectiletrajectoryearthparabolicairgravityattractsAverage agreement:breedsselectionconformanceartificialpopularitybreedingdomesticatedAverage agreement:ballsbilliardsspeedtablebouncingmovingslowfastAverage agreement:computerprocessingerasingwritereadmemoryoutputsinputsbugAverage agreement:slot machinesreelsspinningwinninglosingAverage agreement:TargetplanetorbitsunellipticalspacegravityattractsspeciescompetitionadaptationnaturalfitnessmatingwildmoleculesgastemperaturecontainerpressingmovingcoldhotmindthinkingforgettingmemorizeremembermemorymusclessensesmistakebacteriagenesmutatingreproducingdyingAgreement100.0100.0100.0100.0100.090.990.997.4100.059.159.177.354.595.577.374.790.972.781.895.577.386.4100.0100.088.190.995.5100.072.754.581.872.790.9100.084.368.272.786.490.9100.083.6POSNNNNNNJJNNNNVBZNNSNNNNJJNNVBGJJNNSNNNNNNVBGVBGJJJJNNVBGVBGVBVBNNNNSNNSNNNNSNNSVBGVBGVBGTable 15: Science analogy problems A6 A10, derived Chapter 8 HolyoakThagard (1995).649fiTurneyMappingM1warargumentM2buying itemaccepting beliefM3grounds buildingreasons theoryM4impediments traveldifficultiesM5moneytimeSourcewarsoldierdestroyfightingdefeatattacksweaponAverage agreement:buyermerchandisebuyingsellingreturningvaluableworthlessAverage agreement:foundationsbuildingssupportingsolidweakcrackAverage agreement:obstructionsdestinationroutetravellertravellingcompanionarrivingAverage agreement:moneyallocatebudgeteffectivecheapexpensiveAverage agreement:Targetargumentdebaterrefutearguingacceptancecriticizeslogicbelieverbeliefacceptingadvocatingrejectingtruefalsereasonstheoriesconfirmingrationaldubiousflawdifficultiesgoalplanpersonproblem solvingpartnersucceedingtimeinvestscheduleefficientquickslowAgreement90.9100.090.995.590.995.590.993.5100.090.995.5100.095.595.595.596.172.777.395.590.995.595.587.9100.0100.0100.0100.0100.0100.0100.0100.095.586.486.486.450.059.177.3POSNNNNVBVBGNNVBZNNNNNNVBGVBGVBGJJJJNNSNNSVBGJJJJNNNNSNNNNNNVBGNNVBGNNVBNNJJJJJJTable 16: Common metaphor problems M1 M5, derived Lakoff Johnson (1980).650fiThe Latent Relation Mapping EngineMappingM6seedsideasM7machinemindM8objectideaM9followingunderstandingM10seeingunderstandingSourceseedsplantedfruitfulfruitgrowwitherblossomAverage agreement:machineworkingturnedturnedbrokenpowerrepairAverage agreement:objectholdweighheavylightAverage agreement:followleaderpathfollowerlostwanderstwistedstraightAverage agreement:seeinglightilluminatingdarknessviewhiddenAverage agreement:TargetideasinspiredproductiveproductdevelopfailsucceedmindthinkingawakeasleepconfusedintelligencetherapyideaunderstandanalyzeimportanttrivialunderstandspeakerargumentlistenermisunderstooddigressescomplicatedsimpleunderstandingknowledgeexplainingconfusioninterpretationsecretAgreement90.995.581.895.581.8100.077.389.095.5100.0100.0100.0100.095.5100.098.790.981.881.895.595.589.1100.0100.0100.0100.086.490.995.5100.096.668.277.386.486.468.286.478.8POSNNSVBDJJNNVBVBVBNNVBGJJJJJJNNNNNNVBVBJJJJVBNNNNNNJJVBZJJJJVBGNNVBGNNNNJJTable 17: Common metaphor problems M6 M10, derived Lakoff Johnson(1980).651fiTurneyproblem A1, 81.8% participants (18 22) mapped gravity electromagnetism.final column gives part-of-speech (POS) tags source target terms.used Penn Treebank tags (Santorini, 1990). assigned tags manually.intended mappings tags chosen mapped terms tags.example, A1, sun maps nucleus, sun nucleus tagged NN.POS tags used experiments Section 8. POS tags used LRMEshown participants experiment Section 6.ReferencesAndo, R. K. (2000). Latent semantic space: Iterative scaling improves precision interdocument similarity measurement. Proceedings 23rd Annual ACM SIGIRConference Research Development Information Retrieval (SIGIR-2000), pp.216223.Banko, M., & Etzioni, O. (2007). Strategies lifelong knowledge extraction web.Proceedings 4th International Conference Knowledge Capture (K-CAP2007), pp. 95102.Bullinaria, J., & Levy, J. (2007). Extracting semantic representations word cooccurrence statistics: computational study. Behavior Research Methods, 39 (3),510526.Buttcher, S., & Clarke, C. (2005). Efficiency vs. effectiveness terabyte-scale information retrieval. Proceedings 14th Text REtrieval Conference (TREC 2005),Gaithersburg, MD.Chalmers, D. J., French, R. M., & Hofstadter, D. R. (1992). High-level perception, representation, analogy: critique artificial intelligence methodology. JournalExperimental & Theoretical Artificial Intelligence, 4 (3), 185211.Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A.(1990). Indexing latent semantic analysis. Journal American SocietyInformation Science (JASIS), 41 (6), 391407.Dolan, W. B. (1995). Metaphor emergent property machine-readable dictionaries. Proceedings AAAI 1995 Spring Symposium Series: RepresentationAcquisition Lexical Knowledge: Polysemy, Ambiguity Generativity, pp. 2732.Evans, T. (1964). heuristic program solve geometric-analogy problems. ProceedingsSpring Joint Computer Conference, pp. 327338.Falkenhainer, B., Forbus, K. D., & Gentner, D. (1989). structure-mapping engine:Algorithm examples. Artificial Intelligence, 41 (1), 163.Firth, J. R. (1957). synopsis linguistic theory 19301955. Studies LinguisticAnalysis, pp. 132. Blackwell, Oxford.Forbus, K., Usher, J., Lovett, A., Lockwood, K., & Wetzel, J. (2008). Cogsketch: Opendomain sketch understanding cognitive science research education.Proceedings Fifth Eurographics Workshop Sketch-Based Interfaces Modeling, Annecy, France.652fiThe Latent Relation Mapping EngineForbus, K. D., Riesbeck, C., Birnbaum, L., Livingston, K., Sharma, A., & Ureel, L. (2007).prototype system learns reading simplified texts. AAAI Spring SymposiumMachine Reading, Stanford University, California.French, R. (1995). Subtlety Sameness: Theory Computer Model AnalogyMaking. MIT Press, Cambridge, MA.French, R. M. (2002). computational modeling analogy-making. Trends CognitiveSciences, 6 (5), 200205.Gardenfors, P. (2004). Conceptual Spaces: Geometry Thought. MIT Press.Gentner, D. (1983). Structure-mapping: theoretical framework analogy. CognitiveScience, 7 (2), 155170.Gentner, D. (1991). Language career similarity. Gelman, S., & Byrnes, J.(Eds.), Perspectives Thought Language: Interrelations Development, pp.225277. Cambridge University Press.Gentner, D. (2003). smart. Gentner, D., & Goldin-Meadow, S. (Eds.),Language Mind: Advances Study Language Thought, pp. 195235.MIT Press.Gentner, D., Bowdle, B. F., Wolff, P., & Boronat, C. (2001). Metaphor like analogy.Gentner, D., Holyoak, K. J., & Kokinov, B. N. (Eds.), analogical mind: Perspectives Cognitive Science, pp. 199253. MIT Press, Cambridge, MA.Gildea, D., & Jurafsky, D. (2002). Automatic labeling semantic roles. ComputationalLinguistics, 28 (3), 245288.Girju, R., Nakov, P., Nastase, V., Szpakowicz, S., Turney, P., & Yuret, D. (2007). Semeval2007 task 04: Classification semantic relations nominals. ProceedingsFourth International Workshop Semantic Evaluations (SemEval 2007), pp.1318, Prague, Czech Republic.Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations (Third edition). JohnsHopkins University Press, Baltimore, MD.Hawkins, J., & Blakeslee, S. (2004). Intelligence. Henry Holt.Hirst, G., & St-Onge, D. (1998). Lexical chains representations context detectioncorrection malapropisms. Fellbaum, C. (Ed.), WordNet: ElectronicLexical Database, pp. 305332. MIT Press.Hofmann, T. (1999). Probabilistic Latent Semantic Indexing. Proceedings 22ndAnnual ACM Conference Research Development Information Retrieval (SIGIR 99), pp. 5057, Berkeley, California.Hofstadter, D. (2001). Epilogue: Analogy core cognition. Gentner, D., Holyoak,K. J., & Kokinov, B. N. (Eds.), Analogical Mind: Perspectives CognitiveScience, pp. 499538. MIT Press.Hofstadter, D., & FARG (1995). Fluid Concepts Creative Analogies: Computer ModelsFundamental Mechanisms Thought. Basic Books, New York, NY.653fiTurneyHolyoak, K., & Thagard, P. (1989). Analogical mapping constraint satisfaction. CognitiveScience, 13, 295355.Holyoak, K., & Thagard, P. (1995). Mental Leaps. MIT Press.Hummel, J., & Holyoak, K. (1997). Distributed representations structure: theoryanalogical access mapping. Psychological Review, 104, 427466.Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity based corpus statisticslexical taxonomy. Proceedings International Conference ResearchComputational Linguistics (ROCLING X), pp. 1933, Tapei, Taiwan.Kilgarriff, A. (1997). dont believe word senses. Computers Humanities, 31,91113.Lakoff, G., & Johnson, M. (1980). Metaphors Live By. University Chicago Press.Landauer, T. K., & Dumais, S. T. (1997). solution Platos problem: latent semantic analysis theory acquisition, induction, representation knowledge.Psychological Review, 104 (2), 211240.Leacock, C., & Chodrow, M. (1998). Combining local context WordNet similarityword sense identification. Fellbaum, C. (Ed.), WordNet: Electronic LexicalDatabase. MIT Press.Lee, D. D., & Seung, H. S. (1999). Learning parts objects nonnegative matrixfactorization. Nature, 401, 788791.Lepage, Y. (1998). Solving analogies words: algorithm. Proceedings 36thAnnual Conference Association Computational Linguistics, pp. 728735.Lepage, Y., & Denoual, E. (2005). Purest ever example-based machine translation: Detailedpresentation assessment. Machine Translation, 19 (3), 251282.Lin, D. (1998). information-theoretic definition similarity. Proceedings 15thInternational Conference Machine Learning (ICML-98).Martin, J. H. (1992). Computer understanding conventional metaphoric language. Cognitive Science, 16 (2), 233270.Marx, Z., Dagan, I., Buhmann, J., & Shamir, E. (2002). Coupled clustering: methoddetecting structural correspondence. Journal Machine Learning Research, 3,747780.Mason, Z. (2004). CorMet: computational, corpus-based conventional metaphor extraction system. Computational Linguistics, 30 (1), 2344.Minsky, M. (1986). Society Mind. Simon & Schuster, New York, NY.Mitchell, M. (1993). Analogy-Making Perception: Computer Model. MIT Press, Cambridge, MA.Nastase, V., & Szpakowicz, S. (2003). Exploring noun-modifier semantic relations.Fifth International Workshop Computational Semantics (IWCS-5), pp. 285301,Tilburg, Netherlands.Reitman, W. R. (1965). Cognition Thought: Information Processing Approach. JohnWiley Sons, New York, NY.654fiThe Latent Relation Mapping EngineResnik, P. (1995). Using information content evaluate semantic similarity taxonomy.Proceedings 14th International Joint Conference Artificial Intelligence(IJCAI-95), pp. 448453, San Mateo, CA. Morgan Kaufmann.Rosario, B., & Hearst, M. (2001). Classifying semantic relations noun-compoundsvia domain-specific lexical hierarchy. Proceedings 2001 ConferenceEmpirical Methods Natural Language Processing (EMNLP-01), pp. 8290.Santorini, B. (1990). Part-of-speech tagging guidelines Penn Treebank Project. Tech.rep., Department Computer Information Science, University Pennsylvania.(3rd revision, 2nd printing).Scholkopf, B., Smola, A. J., & Muller, K.-R. (1997). Kernel principal component analysis.Proceedings International Conference Artificial Neural Networks (ICANN1997), pp. 583588, Berlin.Turney, P. D. (2001). Mining Web synonyms: PMI-IR versus LSA TOEFL.Proceedings Twelfth European Conference Machine Learning (ECML-01),pp. 491502, Freiburg, Germany.Turney, P. D. (2005). Measuring semantic similarity latent relational analysis. Proceedings Nineteenth International Joint Conference Artificial Intelligence(IJCAI-05), pp. 11361141, Edinburgh, Scotland.Turney, P. D. (2006). Similarity semantic relations. Computational Linguistics, 32 (3),379416.Turney, P. D. (2008). uniform approach analogies, synonyms, antonyms, associations. Proceedings 22nd International Conference ComputationalLinguistics (Coling 2008), pp. 905912, Manchester, UK.Turney, P. D., & Littman, M. L. (2005). Corpus-based learning analogies semanticrelations. Machine Learning, 60 (13), 251278.Turney, P. D., Littman, M. L., Bigham, J., & Shnayder, V. (2003). Combining independentmodules solve multiple-choice synonym analogy problems. ProceedingsInternational Conference Recent Advances Natural Language Processing(RANLP-03), pp. 482489, Borovets, Bulgaria.van Rijsbergen, C. J. (2004). Geometry Information Retrieval. Cambridge UniversityPress, Cambridge, UK.Veale, T. (2003). analogical thesaurus. Proceedings 15th Innovative Applications Artificial Intelligence Conference (IAAI 2003), pp. 137142, Acapulco,Mexico.Veale, T. (2004). WordNet sits SAT: knowledge-based approach lexical analogy.Proceedings 16th European Conference Artificial Intelligence (ECAI 2004),pp. 606612, Valencia, Spain.Widdows, D. (2004). Geometry Meaning. Center Study LanguageInformation, Stanford, CA.Yan, J., & Forbus, K. D. (2005). Similarity-based qualitative simulation. Proceedings27th Annual Meeting Cognitive Science Society, Stresa, Italy.655fiJournal Artificial Intelligence Research 33 (2008) 149178Submitted 03/08; published 09/08Complexity Strategic Behavior Multi-Winner ElectionsReshef MeirAriel D. ProcacciaJeffrey S. RosenscheinAviv Zoharreshef24@cs.huji.ac.ilarielpro@cs.huji.ac.iljeff@cs.huji.ac.ilavivz@cs.huji.ac.ilSchool Engineering Computer ScienceHebrew University JerusalemAbstractAlthough recent years seen surge interest computational aspectssocial choice, specific attention previously devoted elections multiplewinners, e.g., elections assembly committee. paper, characterizeworst-case complexity manipulation control context four prominent multiwinner voting systems, different formulations strategic agents goal.1. IntroductionComputational aspects voting focus much interest, variety fields.multiagent systems, attention motivated applications well-studiedvoting systems1 method preference aggregation. instance, Ghosh, Mundhe,Hernandez, Sen (1999) designed automated movie recommendation system,conflicting preferences user may movies represented agents,movies suggested selected according voting scheme (in examplemultiple winners, several movies recommended user). general, candidatesvirtual election entities beliefs, joint plans (Ephrati & Rosenschein, 1997),schedules (Haynes, Sen, Arora, & Nadella, 1997).Different aspects voting rules explored computer scientists. issueparticularly well-studied manipulation. many settings, voter maybetter revealing preferences untruthfully. instance, real-life electionsvoter awards single point favorite candidate, may judged pointlessvote candidate appears, polls, sure loser, even candidatevoters truthful first choice.celebrated Gibbard-Satterthwaite Theorem (Gibbard, 1973; Satterthwaite, 1975)implies non-dictatorial voting scheme (i.e., single voteralways dictates outcome election), always exist elections voterimprove utility lying true preferences.2 Nevertheless,suggested bounded-rational agents may find hard determine exactly lieuse, thus may give manipulations altogether. words, computationalcomplexity may obstacle prevents strategic behavior. first address1. use terms voting schemes, voting rules, voting systems, voting protocols interchangeably.2. theorem also generalized multiple winner setting (Duggan & Schwartz, 2000).c2008AI Access Foundation. rights reserved.fiMeir, Procaccia, Rosenschein, & Zoharpoint Bartholdi, Tovey Trick (1989); Bartholdi Orlin (1991) later showedmanipulating important Single Transferable Vote (STV) voting rule N P-completeproblem.recently, shown voting protocols tweaked addingelimination preround, way makes manipulation hard (Conitzer & Sandholm, 2003).Conitzer, Sandholm, Lang (2007) studied setting entire coalitionmanipulators. setting, problem manipulation coalition N P-completevariety protocols, even number candidates constant.Another related issue received significant attention computational difficulty controlling election. Here, authority conducts elections attemptsachieve strategic results adding removing registered voters candidates.rationale work well: computationally hard determine improveoutcome election control, chairman might give cheating altogether.Bartholdi, Tovey Trick (1992) first analyzed computational complexity differentmethods controlling election Plurality Condorcet protocols.paper, augment classical problems manipulation control introducing multiple winners. Specifically, assume manipulator utility functioncandidates, manipulators goal achieve set winnerstotal utility threshold. study abovementioned problems respectfour simple important multi-winner voting schemes: SNTV, Bloc voting, Approval,Cumulative voting.paper proceeds follows. Section 2, describe voting rules question.Section 3, deal manipulation problems. Section 4, deal control problems.discuss related work Section 5, conclude Section 6.2. Multi-Winner Voting Schemessection present several multi-winner voting systems significance. Althoughdiscussion self-contained, interested readers find details article BramsFishburn (2002).Let set voters V = {v1 , v2 , . . . vn }; let set candidates C = {c1 , . . . cm }.Furthermore, assume k N candidates elected.Multi-winner voting rules differ single-winner ones propertiesexpected satisfy. major concern multi-winner elections proportional representation: faction consists fraction X population representedapproximately fraction X seats assembly. property satisfied(generalizations of) many rules usually considered respect single-winnerelections.Thus, examine four prevalent multi-winner voting rules. four,candidates given points voters, k candidates points winelection. schemes differ way points awarded candidates.Single Non-Transferable Vote (SNTV): voter gives one point favorite candidate.33. SNTV single winner elections also known Plurality.150fiComplexity Strategic Behavior Multi-Winner ElectionsBloc voting: voter gives one point k candidates.4Approval voting: voter approves disapproves candidate; approvedcandidate awarded one point, limit number candidatesvoter approve.Cumulative voting: allows voters express intensities preference, askingdistribute fixed number points among candidates. Cumulative votingespecially interesting, since encourages minority representation maximizessocial welfare (Brams & Fishburn, 2002).Scoring rules prominent family voting rules. voting rule familydefined vector integers~ = h1 , . . . , i, l l+1 l = 1, . . . , 1.voter reports ranking candidates, thus awarding 1 points top-rankedcandidate, 2 points second candidate, general l points candidateranked place l. Notice SNTV family scoring rules defined |C|vector h1, 0, . . . , 0i, Bloc family scoring rules defined |C|, kvector h1, . . . , 1, 0, . . . , 0i, number 1s k.3. Manipulationvoter considered manipulator, said vote strategically, voterreveals false preferences attempt improve outcome election. Settingsmanipulation possible avoided, since may lead socially undesirableoutcome emerging winner election. Therefore, computational resistancemanipulation considered advantage.classical formalization manipulation problem (Bartholdi et al., 1989),given set C candidates, set V voters, distinguished candidate p C.also full knowledge voters votes. asked whether possible castadditional vote, manipulators ballot, way makes p win election.generalizing problem k-winner case, several formulations possible.general formulation given following definition.Definition 3.1. Manipulation problem, given set C candidates,set V voters already cast vote, number winners k N, utilityfunction u : C Z, integer N.P asked whether manipulator castvote resulting election, cW u(c) t, W set winners,|W | = k.Notice number winners k parameter problem.Remark 3.2. manipulators utility function implicitly assumed additive. Oneconsider elaborate utility functions, ones investigated contextcombinatorial auctions, beyond scope paper.4. Bloc voting also known k-Approval.151fiMeir, Procaccia, Rosenschein, & ZoharRemark 3.3. make standard assumption tie-breaking adversarialmanipulator (Conitzer et al., 2007), i.e., several candidates perform equallywell election, ones lower utility manipulator elected.number winners k = 1, assumption equivalent formulatingmanipulation problems unique winner version, ballot must castway designated candidate strictly better rest (e.g., higher score).One might argue general formulation problem given makes manipulation harder. Indeed, manipulator might following, specific, goalsmind.1. manipulator specific candidate interested seeing amongwinners (constructive manipulation).2. manipulator specific candidate interested excludingset winners (destructive manipulation) (Conitzer et al., 2007).3. manipulator (additive) boolean-valued utility function candidates u : C {0, 1}.first second settings naturally special cases third, thirdspecial case Definition 3. intend explore foregoing formulationsManipulation problem. Notice one also consider goals, examplemanipulator favorite set candidates interested seeing them,many possible them, among winners. However, investigategoals insofar special cases boolean-valued utility function.Remark 3.4. Unless explicitly mentioned otherwise, usually assume general (additive)utility function, Definition 3.1.find convenient represent SNTV Bloc voting using common framework. consider l-Bloc voting rulesvoting rules every voter gives one pointexactly l candidates, l k. Notice SNTV l = 1, Bloc votingl = k. remind reader number winners k constant, ratherparameter Manipulation problem.Proposition 3.5. Let l {1, . . . , k}. Manipulation l-Bloc voting P.Proof. manipulator faced score awarded candidates votersV ; let s[c] total score candidate c. Order candidates score, lets0 score kth highest candidate. example, k = 3, |C| = = 4,initial scores 8, 5, 5, 3, s0 = 5. addition, let= {c C : s[c] > s0 }.Notice |A| k 1. LetB = {c C : s[c] = s0 }.B may large, particular candidates score, B = C.152fiComplexity Strategic Behavior Multi-Winner ElectionsNow, candidates least s0 + 2 initial points elected regardlessactions manipulator, manipulator award one pointcandidate. Candidates exactly s0 +1 points elected, unless candidateslower utility ultimately receive s0 + 1 points due manipulators voteas tiesbroken adversarially manipulator. Let us examine candidates lesss0 points. Notice candidates s[c] s0 2 lose case. Moreover,since ties broken adversarially, voting candidates s0 1 points cannot benefitmanipulator. conclude point, manipulator better makesure candidates B (with exactly s0 points) eventually elected highutility possible.Therefore, put simply, manipulators optimal strategy vote top(in terms utility) k |A| (top l l < k |A|) candidates B, thus guaranteeingcandidates among winners, cast remaining votes favorcandidates (which win anyway). discussion leads conclusionAlgorithm 1 decides Manipulation problem. Clearly computational complexityAlgorithm 1 Decides Manipulation problem l-Bloc voting1: procedure Manipulate-Bloc(V, C, k, u, t, l)2:s[c] |{v V : v votes candidate c}|3:s0 score kth highest candidate4:{c C : s[c] > s0 }|A| k 15:B {c C : s[c] = s0 }, ordered u(c) decreasing order6:l k |A|7:manipulator votes top l candidates B8:else9:manipulator votes top k |A| candidates B l + |A| k candidates10:end11:utility winners12:return true13:else14:return false15:end16: end procedurealgorithm polynomial input size.following immediate corollary:Corollary 3.6. Manipulation SNTV Bloc voting P.situation Approval voting dissimilar. Indeed, question is: votergain approving k candidates? priori, answer yes. However, givenvotes voters, clearly manipulator cannot gain approving candidatesk eventual winners election. hand, manipulator alsobenefit approving less k voters. Say manipulator approved l < k153fiMeir, Procaccia, Rosenschein, & Zoharvoters, candidates W , |W | = k, eventually election. Let Ccandidates manipulator approved, W = W \ C winnersmanipulator approve; W k l. manipulator approves l candidatesC well k l candidates W , set winners clearly still goingW . Therefore, manipulator better approve exactly k candidates;already demonstrated Proposition 3.5 accomplished optimallyefficiently. Therefore:Corollary 3.7. Manipulation Approval P.Remark 3.8. Formally, manipulation Approval subtle issue, since issue mayill-defined voters assumed linear preferences candidates.case, multiple sincere ballots (where approved candidates preferreddisapproved candidates). specific settings, voter cannot gain castinginsincere ballot (Endriss, 2007), always true. case, manipulationproblem according definition well-defined nontrivial Approval.contrast abovementioned three voting rules, Cumulative voting turnscomputationally hard manipulate general utility function.Proposition 3.9. Manipulation Cumulative voting N P-complete.implicit assumption made proposition number pointsdistributed constant, rather parameter Manipulation problemCumulative voting.proof Proposition 3.9 relies reduction one well-knownN P-complete problems, Knapsack problem.Definition 3.10. Knapsack problem, given set items = {a1 , . . . , },weight w(a) N value (a),P N.P capacity b N,asked whether subset aA (a) aA w(a) b.Proof Proposition 3.9. problem clearly N P.see Manipulation Cumulative voting N P-hard, prove Knapsackreduces problem. given input hA, w, , b, ti Knapsack, constructinstance Manipulation Cumulative voting follows.Let n = |A|. 2n voters, V = {v1 , . . . , v2n }, 3n candidates, C = {c1 , . . . , c3n },n winners. addition, voter may distribute b points among candidates.want voters V cast votes way following three conditionssatisfied:1. j = 1, . . . , n, cj b w(aj ) + 1 points.2. j = n + 1, . . . , 2n, cj b points.3. j = 2n + 1, . . . , 3n, cj exactly b points.easily done. Indeed, = 1, . . . , n, voter vi awards b w(ai ) + 1 pointscandidate ci , awards remaining w(ai ) 1 points candidate cn+i . Now,= 1, . . . , n, voter n + awards b points candidate c2n+i .154fiComplexity Strategic Behavior Multi-Winner Electionsdefine utility u candidates follows:((aj ) j = 1, . . . , nu(cj ) =0j = n + 1, . . . , 3ntransformation clearly polynomial time computable, remains verifyreduction. Assume subset total weight btotal value least t. Let C = {cj : aj }. manipulator awards w(aj ) pointscandidate c C , raising total score candidates b + 1. Since initiallycandidates b points, candidatesPc C amongP n winnersu(c)=election. total utility candidates is:aA (a) (sincecCj = 1, . . . , n, u(cj ) = (aj )).direction, assume manipulator able distribute b points waywinners election total utility least t. Recall initiallyleast n candidates b points utility 0, ties broken adversariallymanipulator. Therefore, must subset C C candidates ultimatelyscore least b+1, total utility least t. Let correspondingitems Knapsack instance, i.e., aj cj C . total weightitems b, b points distributed among candidates Cmanipulator, cj C initially b w(aj ) + 1 points. also holdstotal utility items exactly total utility candidates C , namelyleast t.next proposition gives negative answer question whether ManipulationCumulative voting still hard restricted formulations manipulatorsgoal, discussed beginning section. Indeed, put forward algorithmdecides problem boolean-valued utility function.Proposition 3.11. Manipulation Cumulative voting boolean-valued utilityfunction u : C {0, 1} P.Remark 3.12. result holds even number points distributed exponentialnumber voters candidates.Proof Proposition 3.11. Let s[c] score candidate c C manipulatorcast vote, [c] cs score manipulators vote taken account.Assume without loss generality s[c1 ] s[c2 ] . . . s[cm ]. Let = {d1 , d2 , . . .}set desirable candidates C u(d) = 1, assume sortednonincreasing scores.Informally, going find threshold thresh pushing candidatesthreshold guarantees victory. check whether possible distributeL points least candidates pass threshold, L number pointsavailable voter.Formally, consider Algorithm 2 (w.l.o.g. k |D| t, otherwise manipulationimpossible). algorithm clearly halts polynomial time. remains provecorrectness algorithm.155fiMeir, Procaccia, Rosenschein, & ZoharAlgorithm 2 Decides Manipulation Cumulative voting boolean-valued utility1: j max{j : |{c1 , c2 , . . . , cj1 } D| + k + 1 j cj/ D} j exists, sincecondition holds first candidate2: thresh s[cj ]Pt3:j=1 max{0, thresh + 1 s[dj ]}4: L5:return true6: else7:return false8: endLemma 3.13. Algorithm 2 correctly decides Manipulation Cumulative votingboolean-valued utility function.Proof. Denote W = {c1 , . . . , ck } k candidates highest score (sorted)manipulators vote, W final set k winners. threshold candidate cjpartitions W two disjoint subsets: Wu = {c1 , . . . , cj 1 }, Wd = {cj , . . . , ck }.maximality j , holds that:|Wu D| + |Wd | = |Wu D| + (k + 1 j ) = t.(1)Note exact number votes required push desirable candidatesthreshold. Now, must show manipulator cast vote waywinner set W satisfies |W D| if, if, L S.Suppose first L. clearly possible push desirable candidatesthresh. Wu threshold already; follows Wd replacedentirely desirable candidates.Let W = {w1 , . . . , wk } set new winners. particular, write W =Wu {wj , . . . , wk }. Wu contains |Wu D| desirable candidates, {wj , . . . , wk } consistspurely desirable candidates. Equation (1):|W D| = |Wu D| + |{wj , . . . , wk }|= |Wu D| + |Wd |=tConversely, suppose > L. must show manipulator cannot distribute Lpoints way candidates among winners.Clearly possibility push desirable candidates thresh. Considerballot cast manipulator, assume w.l.o.g. manipulator distributedpoints among candidates D. Denote new set winners W = Wu Wd ,Wu = {c C : [c] > thresh}Wd = {c C : [c] thresh}.claim|Wu D| = k t,156(2)fiComplexity Strategic Behavior Multi-Winner Elections= C \ D. Indeed, Equation (1)|Wu D| = k 1 + j ,therefore|Wu D| = |Wu D| = |Wu | |Wu D|= (j 1) (t k 1 + j )=ktfirst equality follows fact points distributed candidatesD.Denote F set candidates pushed threshold. Formally:F = {c : [c] > thresh s[c] thresh}Thus:Wu = Wu F.Let w new position candidate cj candidates sorted nonincreasing [c]. holdsw = j + |F |.claim|Wd D| 1.(3)Indeed,|Wu D| <|Wu D| + |F | = |Wu D| < = |Wu D| + k + 1|F | < k + 1w=j+ |F | <jj+k+1wk=k+1|Wd D| 1combining Equations (2) (3), finally obtain:|W D| = k |W D|= k (|Wu D| + |Wd D|)=t1<tproof Proposition 3.11 completed.157jcj Wk (k + 1)jfiMeir, Procaccia, Rosenschein, & ZoharRemark 3.14. proof shows manipulation Cumulative voting coalition(even weighted) voters, work Conitzer et al. (2007), tractableboolean-valued utility function. follows simply joining (weighted) score poolsvoters coalition.Remark 3.15. possible show number points distributedpolynomially bounded, manipulation Cumulative voting P even general utilityfunctions.4. Controlcontrol setting, assume authority controlling election (hereinafter,chairman) power tweak elections electorate slate candidatesway might change outcome. also form undesirable strategic behavior,part behind-the-scenes player supposed take active partelection.one setting, chairman might add remove voters support candidate,number voters add/remove without alerting attention actionslimited. problems formally defined follows:Definition 4.1. problem Control Adding Voters, given setC candidates, set V registered voters, set V unregistered voters, numberwinners k N, utility function u : C Z, integers r, N. askedwhether possible register r voters V resulting election,PcW u(c) t, W set winners, |W | = k.Definition 4.2. problem Control Removing Voters, given set Ccandidates, set V registered voters, number winners k N, utility functionu : C Z, integers r, N. asked whetherpossible removePr voters V resulting election, cW u(c) t, W setwinners, |W | = k.Another possible misuse chairmans authority tampering slatecandidates. Removing candidates obviously helpful, even adding candidatessometimes tip scales direction chairmans favorites.Definition 4.3. problem Control Adding Candidates, givenset C registered candidates, set C unregistered candidates, set V voters,number winners k, utility function u : C C Z, integers r, N. voterspreferences candidates C C . asked whether possiblePaddr candidates C C , resulting elections C C , cW u(c) t,W set winners, |W | = k.Definition 4.4. problem Control Removing Candidates, givenset C candidates, set V voters, number winners k, utility function u : C Z,integers r, N. asked whetherPit possible remove r candidatesC , resulting elections cW u(c) t, W set winners,|W | = k.158fiComplexity Strategic Behavior Multi-Winner Electionsclarification order. context scoring rules, assumptiontwo problems voters rankings candidates C C . Therefore,candidates added removed, voters preferences new set candidatesstill well-defined. goes Approval: voter approves disapprovesevery candidates C C . However, context Cumulative voting, problemscontrol adding/removing candidates well-defined. Indeed, one would requirespecification voters distribute points among every possible subsetcandidates, would require representation exponential size.5 Consequently,consider control adding removing candidates Cumulative voting.Remark 4.5. authors (e.g., Hemaspaandra et al., 2007b) considered typescontrol, control partitioning set voters. paper, restrictattention four types control mentioned above.Remark 4.6. Unless stated otherwise, assume ties broken adversariallychairman.before, unless explicitly mentioned otherwise, going assume generaladditive utility function, definitions.Remark 4.7. clear computational problems N P votingrules question. Therefore, N P-completeness proofs show hardness.4.1 Controlling Set VotersControl Adding Voters tractable SNTV, though procedure trivial.Bartholdi et al. (1992) showed control adding voters easy SNTVsingle winner, former result seen extension latter (to casemultiple winners general [additive] utility function).Proposition 4.8. Control Adding Voters SNTV P.Proof. describe algorithm, Control-SNTV, efficiently decides ControlSNTV. Informally, algorithm works follows. first calculates number pointsawarded candidates voters V . Then, stage, algorithm analyzeselection l top winners original election remain winners, attemptsselect k l winners way maximizes utility. done settingthreshold one point score (l + 1)-highest candidate; algorithmpushes scores potential winners threshold (see Figure 1 illustration).formal description Control-SNTV given Algorithm 3. procedure Pushworks follows: first parameter threshold thr, second parameternumber candidates pushed, pushN um. procedure also implicit accessinput Control-SNTV, namely parameters given Control instance. Pushreturns subset V V registered. say procedure pushes candidatec threshold exactly thr s[c] voters v V vote c registered.words, procedure registers enough voters V order ensure cs score5. possible imagine compact representations, beyond scope paper.159fiMeir, Procaccia, Rosenschein, & Zohar6032243220112050322222015Figure 1: left panel illustrates input Control problem SNTV.candidate represented circled numberthe utility candidate.location circle determines score candidate, based votersV . Let k = 5; winners blackened. Now, assume 6 votersV , 3 voting two bottom candidates, r = 3. chairmanaward 3 points candidate utility 5 score 0, wouldchange result election. Alternatively, chairman award 3points candidate utility 2 score 1, thus improving utility 1,seen right panel. election considered algorithml = 4, s[ci5 ] = 3, threshold 4.reaches thePthreshold. Push finds subset C candidates size pushN ummaximizes cC u(c), restriction candidates C simultaneouslypushed threshold registering subset V V s.t. |V | r. procedure returnssubset V .Now, assume procedure Push always correct (in maximizing utilityk l candidates able push threshold s[cl+1 ] + 1, registeringr voters) runs polynomial time. Clearly, Control-SNTV also runspolynomial time. Furthermore:Lemma 4.9. Control-SNTV correctly decides Control problem SNTV.Proof. Let W = {cj1 , . . . , cjk } k winners election takeaccount votes voters V (the original election), sorted descending score,candidates identical score, ascending utility. Let W = {cj1 , . . . , cjk }candidates controlled election maximum utility, sorted descendingscore, ascending utility; let [c] final score candidate c optimalelection. Let min smallest index cjmin/ W (w.l.o.g. min exists, otherwiseW = W done). holds candidates c W , [c] s[cjmin ]. Now,160fiComplexity Strategic Behavior Multi-Winner ElectionsAlgorithm 3 Decides Control problem SNTV.1: procedure Control-SNTV(C, V, V , k, u, r, t)2:s[c] |{v V : v votes candidate c}|3:Sort candidates descending scoreBreak ties ascending utility4:Let sorted candidates {ci1 , . . . , cim }5:l = 0, . . . , kFix l top winners6:V Push(s[cl+1 ] + 1, k l)Select winners; see details7:ul utility election V registered8:end9:maxl ul return true10:else11:return false12:end13: end procedureassume w.l.o.g. c W [c] = s[cjmin ] c W (and consequently,c = cjq q < min). Indeed, must hold u[c] u[cjmin ] (as tie-breakingadversarial chairman), indeed c/ W even though c W , chairmanmust registered voters vote c, although lower total utilitykeep unchanged.sufficient show one elections considered algorithmset winners utility least W . Indeed, let W = {cj1 , . . . , cjmin1 } W ;k min + 1 candidates c W \ W s[c] s[cjmin ] + 1. algorithm considerselection first min 1 winners, namely W , remain fixed, thresholds[cjmin ] + 1. Surely, possible push candidates W \ W threshold,election, winners would W . Since Push maximizes utilityk min + 1 candidates pushes threshold, utility returned Pushl = min 1 least large total utility winners W .remains explain procedure Push implemented run polynomialtime. Recall Knapsack problem; general formulation problemtwo resource types. item two weight measures, w1 (ai ) w2 (ai ),specify much resource consumes type, knapsack twocapacities: b1 b2 . requirement total amount resource firsttype consumed exceed b1 , total use resource secondtype exceed b2 . problem, often two dimensions,called Multidimensional Knapsack. Push essentially solves special case twodimensional knapsack problem, capacities b1 = r (the number voterschairman allowed register), b2 = pushN um (the number candidatespushed). threshold thr, candidate cj supported leastthr s[cj ] voters V , set w1 (aj ) = thr s[cj ], w2 (aj ) = 1, (aj ) = u(cj ).Multidimensional Knapsack problem solved time polynomialnumber items capacities knapsack (Kellerer, Pferschy, & Pisinger, 2004)161fiMeir, Procaccia, Rosenschein, & Zohar(via dynamic programming, example). Since case capacities bounded|V | m, Push designed run polynomial time.following lemma allows us extend results Control Adding VotersControl Removing Voters, vice versa. shall momentarily applySNTV, also prove useful later on.Lemma 4.10. Let R = hV, C, r, t, k, ui instance Control Removing Votersvoting rule, let = hV , V , C , r , , k , u instance ControlAdding Voters voting rule, that:C = Cr = r=Xu(c)cCk = |C| ku (c) = u(c)V = VLet U subset voters selected chairman V = V . Denote sU [c], sU [c]final score candidate c election obtained removing adding votersU , respectively. holdsc, c C, U V,sU [c] sU [c ] sU [c] sU [c ](4)R Control Removing Voters Control AddingVoters.Proof. condition (4) sU , sU holds k = |C| k winnersconstructed instance exactly |C| k losers original instance.6 is, Wwinners given instance W winners constructed instance,W = C \ W .follows that,XXXu(c) (t +u(c))u(c) =cWcCcC\W=Xu (c) +cC\W=Xu (c)Xu (c)cCcWPThus, choice subset U removed added, cW u(c) if,Pif, cW u (c) . conclude given instance yes instanceconstructed instance yes instance.6. statement also takes account tie-breaking scheme, candidates lower utilityoriginal instance candidates higher utility.162fiComplexity Strategic Behavior Multi-Winner ElectionsApplying lemma, easily obtain:Proposition 4.11. Control Removing Voters SNTV P.Proof. give polynomial time reduction Control Removing VotersSNTV Control Adding Voters SNTV, shown (Proposition 4.8) P. Given instance hV, C, r, t, k, ui Control Removing Voters,define equivalent instance hV , V , C , r , , k , u latter problem. wantuse Lemma 4.10, need define V correctly.voter v V = V , let f (v) C candidate voter v ranks first; f (v)gives relevant information voter vs ballot. ballots voters Vdefined following rule. candidate c C,|{v V : f (v ) = c}| = |V | |{v V : f (v) = c}|holds that:sU [c] = |{v V : f (v) = c}| |{v U : f (v) = c}|definition V ,sU [c] = |{v V : f (v ) = c}| + |{v U : f (v ) = c}|= |V | |{v V : f (v) = c}| + |{v U : f (v) = c}|= |V | (|{v V : f (v) = c}| |{v U : f (v) = c}|)= |V | sU [c]Hence c, c C, U V :sU [c] sU [c ] sU [c] sU [c ].implies conditions Lemma 4.10 hold, thus polynomial reductionControl Removing Voters SNTV Control Adding VotersSNTV. Since latter problem P, former.rest section prove control adding/removing voters hardthree voting rules consideration, even chairman simply wants includeexclude specific candidate. fact, statement includes 12 different results (3 votingrules adding/removing include/exclude). Instead proving result separately,use generic reductions. proof scheme follows: shall first establishControl Removing Voters hard Approval, Bloc, Cumulative voting,even chairman wants include candidate. use Lemma 4.10 showadding voters hard foregoing rules, even chairman wants excludecandidate. Finally, Lemma 4.17 give us six remaining results: removingexcluding, adding including. overall scheme illustrated Figure 2.following result known work Hemaspaandra et al. (2007b).Proposition 4.12. (Hemaspaandra et al., 2007b) Control Removing votersApproval N P-complete, even chairman trying make distinguished candidatewin single winner elections.163fiMeir, Procaccia, Rosenschein, & ZoharFigure 2: Scheme hardness proofs control adding/removing voterschairman wants include/exclude distinguished candidate, voting rules:Bloc, Approval, Cumulative voting.result, results appear later, stated single winnerelections, i.e., number winners k satisfies k = 1. Clearly hardnessproblem k = 1 implies hardness general formulation problemk also parameter.Proposition 4.13. Control Removing Voters Cumulative voting N P-complete,even chairman trying make distinguished candidate win single winner elections.Remark 4.14. instances control problems chairman simply wants include/exclude distinguished candidate, replace utility function u distinguished candidate p, i.e., u(p) = 1 (respectively, u(p) = 1) include (resp., exclude)u(c) = 0 candidates c. threshold = 1 (resp., = 0).Proof Proposition 4.13. use reduction Control Removing VotersApproval. Let hV, C, p, ri instance problem Approval, p Cdistinguished candidate (and k = 1). Define instance problem Cumulativevoting follows: pool points satisfies L = |C|; r = r; p = p; C = C C ,C contains |V | L candidates.construct set voters. voter v V , add V voterawards 1 point every candidate v approves, gives points (no164fiComplexity Strategic Behavior Multi-Winner ElectionsL ) distinct candidates C . V contains two voters;voters gives one point candidate C. Finally, let V = V V .Removing voters V cannot promote p , chairman may limit withoutloss generality removing voters V . Now, every candidate C onepoint, none beats candidate C (each least two points).Furthermore, every selection voters V corresponding voters Vremove, candidate C gets score new instance got originalinstance, plus 2. directly implies control possible constructed instance(of Cumulative voting) possible given instance (of Approval).order complete hardness proofs removing voters including distinguished candidate, give similar result Bloc. Notice, however, reductionconstructs instances multiple winners.Proposition 4.15. Control Removing Voters Bloc N P-complete, evenchairman simply wants include distinguished candidate among winners.Proof. prove proposition polynomial time reduction Control Removing Voters Approval. Let hV, C, p, k, ri instance latter problem (withk winners; possible let k = 1). Denote, usual, n = |V | = |C|. Define:C = C C C , C contains new candidates, C contains 10 k n newcandidates; p = p; k = k + m; r = r. need design new instancecandidates C among winners, candidates C amonglosers. define voter set accordingly.1. V contains one voter voter V (n total). v V , v gives pointcandidates C v approved. points go candidates C ,C .2. V contains r + 2 voters. voter gives point candidate C, givesk points candidates C (arbitrarily).3. V contains 4n voters. voter awards points C , k candidatesC (in way specified later).Finally let V = V V V .claim hV , C , p , k , r Control Removing Voters BlochV, C, p, k, ri Control Removing Voters Approval.candidate c, denote s[c], [c] total number votes c obtains originalconstructed instances, removing voters. Note whole set C gets(4n)k + nk votes, less size (10kn), possible scatter votesV candidate C 1 point. addition, candidateC least 4n points. also holdsc C, [c] = s[c] + r + 2,(from votes V , V ), thus:c C, 4n r > r + n + 2 [c] > r + 1.165fiMeir, Procaccia, Rosenschein, & Zoharconclude that:c C, c C , [c] < [c ] r,c C, c C , [c] > [c ] + r.Without loss generality chairman removes voters V (= V ), sincevotes may influence result elections. Denote sU [c], sU [c] scorecandidate c instances, removing subset U V = V . U|U | r holds that:c1 , c2 C, sU [c1 ] sU [c2 ] sU [c1 ] sU [c2 ].Moreover,c C, c C , sU [c] < sU [c ],c C, c C , sU [c] > sU [c ].words, candidates C among winners, whereas candidatesC among losers, ranking candidates C change.conclude k winners constructed instance exactly candidatesC together k winners given Approval instance. Thus control possibleoriginal instance Approval (i.e., possible make p win)possible constructed instance Bloc.promised, use Lemma 4.10 transform results three voting rulesControl (Include winner) Removing Voters Control (Exclude winner) AddingVoters.Proposition 4.16. Control Adding Voters Approval, Bloc, Cumulative voting N P-hard, even chairman simply wants exclude distinguished candidateset winners.Proof. prove proposition using Lemma 4.10 similar notations. Considerinstance Control Removing Voters chairman wants includecandidate. utility function u 1 p 0 otherwise, = 1. utilityfunction u constructed Lemma 4.10 -1 p 0 otherwise, threshold1 = 0; is, chairman wants exclude p. obtain desiredresult directly showing lemmas condition holds. words, must showthree voting rules question, given instance Control RemovingVoters, possible construct instance Control Adding Voterssatisfies condition (4).Approval Bloc: proof similar Proposition 4.11.voter Control Removing Voters instance, add voter V givespoints complement subset candidates. Formally, denote Cv C candidatesv awards points. v V , add v V , Cv = C \ Cv . Note166fiComplexity Strategic Behavior Multi-Winner Electionsconstruction creates legal Bloc instance since |Cv | = |C \ Cv | = |C| k = k ,thus(v V, |Cv | = k) v V , |Cv | = k .Let U V ; holds voting rules that:sU [c] = |{v V : c Cv }| |{v U : c Cv }|and,sU [c] = |{v V : c Cv }| + |{v U : c Cv }|= |{v V : c/ Cv }| + |{v U : c Cv }|= |V | |{v V : c Cv }| + |{v U : c Cv }|= |V | (|{v V : c Cv }| |{v U : c Cv }|)= |V | sU [c],thus:c, c C, U V,sU [c] sU [c ] sU [c] sU [c ].Cumulative voting: original voter, add new voter Vdistributes pool way complements originaldistribution. Formally, let svcPvscore voter v gives candidate c; v V,cC sc = L. point distributionnew voter v V svc = L svc , c C. Note legal Cumulativevoting instance, L = L(m 1):XX XL svc = mLsvc = mL L = L(m 1) = L .v V ,svc =cCcCcClook final score adding/removing voters U V :XXXsU [c] =svc =svcsvc .vVvV \UvUFurther,sU [c] =XsvcvV U=Xsvc +v V=XXvU(Lsvc )+vV= L|V |svcXvUXsvc+vV= L|V |svcXvV= L|V | sU [c]167XsvcvUsvcXvUsvc!fiMeir, Procaccia, Rosenschein, & ZoharTherefore, have:c, c C, U V,sU [c] sU [c ] sU [c] sU [c ].final generic transformation lemma shows constructive destructivesettings, i.e., including/excluding distinguished candidate, basically equivalentthree voting rules question. subtle point, shall deal formulateprove lemma, lemma reverses tie-breaking scheme: adversarial tiebreaking (as assumed far) becomes friendly tie-breaking, i.e., ties brokenfavor candidates higher utility; vice versa.Lemma 4.17. every instance Control Adding (resp., Removing) Votersadversarial tie-breaking W C made win adding (resp., removing)r voters, instance Control Adding (resp., Removing) Votersfriendly tie-breaking candidate set C C\W made win adding(resp., removing) r voters. Similarly, friendly tie-breaking transformed adversarial tiebreaking. statement holds Approval, Bloc, Cumulative Voting.Proof. clarity, prove lemma adding voters; proof removing voterspractically identical. also prove result Cumulative voting (with poolpoints size L), straightforward extend Approval Bloc, generallyreplacing L 1.Given instance hV, V , C, p, k, ri, let U V subset r voters addingvoters induces set winners W . Construct instance Control AddingVoters follows: set number winners k = |C| k, set pool pointsL = L(|C| 1). voter v V, V voter v gives L svccandidate c, svc score given c voter v. Define set V (resp., V )set containing voters corresponding voters V (resp., V ). Notev V V ,XcCsvc =XL svc = L|C|cCXsvc = L|C| L = L(|C| 1) = L .cCDenote V final set voters results adding voters U , Vcorresponding set voters constructed instance (i.e., v V v V ). Alsodenote s[c], s[c] score candidate c C original instance newinstance, adding voters.168fiComplexity Strategic Behavior Multi-Winner Electionsprevious proofs, get candidates order reversed. Formally,comparing score two candidates:XXs[c1 ] s[c2 ] =svc1svc2v V=Xv V(L svc1 )vVXvV= |V |LX(svc1 ) |V |L +vV=XvV(L svc2 )svc2XX(svc2 )vVsvc1vV= s[c2 ] s[c1 ]Therefore, since assumed tie-breaking scheme reversed, k candidateshighest score original instance k candidates lowest scoreconstructed instance, and, moreover, C \ W form k = k winners newinstance.7Lemma 4.17 allows us derive straightforward reductions versions Controlchairman wants include/exclude distinguished candidate: possibleinclude candidate given instance possible exclude candidateconstructed instance, vice versa. point address changetie-breaking scheme. Fortunately, easy obtain versions Propositions 4.12,4.13, 4.15 friendly tie-breaking, slight modifications proofs. Hence,lemma, applied three propositions, yields:Proposition 4.18. Control Removing Voters Bloc, Approval, Cumulativevoting N P-complete, even chairman simply wants exclude distinguishedcandidate.Recall Lemma 4.10 preserves tie-breaking scheme. Thus, obtainfriendly version Proposition 4.16. Then, Lemma 4.17 applied friendly Proposition 4.16 gives us final result section. Note constructive version (i.e.,including distinguished candidate) Control Adding Voters Approval already known N P-hard even single winner elections (Hemaspaandra et al., 2007b).Proposition 4.19. Control Adding Voters Approval (Hemaspaandra et al.,2007b), Bloc, Cumulative voting N P-complete, even chairman simply wantsinclude distinguished candidate.emerges issue worth clarifying. noted above, constructive version (i.e.,including distinguished candidate) Control Adding/Removing Voters Approval already known N P-hard even single winner elections (Hemaspaandraet al., 2007b). hand, Hemaspaandra et al. (2007b) show destructive7. precise, candidates utilities identical C \ W winners; nameswinners may change, irrelevant purposes.169fiMeir, Procaccia, Rosenschein, & Zoharversion problems (i.e., adding/removing voters excluding distinguishedcandidate) P k = 1. surprising results imply longer casenumber winners k also parameter.first may appear follows Lemma 4.17 constructive destructive versions problem equivalent, would seem contradictory. closerexamination reveals conflict: apply lemma constructiveresult k = 1, obtain reduction destructive version, k = |C| 1.4.2 Controlling Set Candidatesturn problem controlling set candidates. noted beginningsection, problem ill-defined comes Cumulative voting,following restrict SNTV, Bloc voting, Approval voting.Another subtle point problem destructive control removing candidatessingle winner elections, assumed chairman cannot remove distinguishedcandidate p. seem elegant way generalize assumptionmulti-winner elections. Hence, discuss control removing candidatesgoal exclude distinguished candidate.Now, recall Bartholdi et al. (1992) show control adding/removing candidates SNTV N P-complete, even single winner elections (in particular, evenchairman wants include single candidate among winners). Hemaspaandra etal. (2007b) extended result destructive control. Crucially, since results holdsingle winner elections, also apply Bloc voting, Bloc voting SNTVidentical k = 1. So, fact, Bartholdi et al. Hemaspaandra et al. togetherimply control adding candidates SNTV Bloc N P-complete, evenchairman wants include exclude distinguished candidate, control removing candidates SNTV Bloc N P-complete, even chairman wantsinclude distinguished candidate (again, discuss exclusion candidate).Although Approval voting seems complicated SNTV (or even Bloc), surprisingly much easier control tampering set candidates.Proposition 4.20. Control Adding Candidates Approval P,utility function.Interestingly, single winner setting, Approval immune control addingcandidates (Hemaspaandra et al., 2007b), i.e., possible get candidate electedadding candidates. However, multiple winner model clearly possiblegain utility adding candidates.Proof Proposition 4.20. actually solve following problem: chairmanadd exactly r candidates, way added candidates become winners,utility least t? Solving problem entails chairman also solve originalproblem, simply run algorithm every r r.First note candidate c C, C fixed number points s[c], regardlessparticipation candidate. chairman selects subset C , |D| = r,winners k candidates C highest score. Therefore,effective add candidates actually winners.170fiComplexity Strategic Behavior Multi-Winner ElectionsSay indeed W , W set winners. Regardless identitycandidates D, winners C exactly |C| r candidates highestscore C. Since r fixed (without loss generality) r k, utility winnersC also fixed, need optimize D, i.e., select best r candidatesC made winners.Lemma 4.21. Let C = {c1 , . . . , cm } sorted nonincreasing score, nondecreasingutility, i.e., s[cj ] s[cj+1 ] j = 1, . . . , 1, s[cj ] = s[cj+1 ] u(cj )u(cj+1 ). Let W set winners candidates C , |D| = r,added. W D, one following holds:1. s[d] > s[ckr+1 ].2. s[d] = s[ckr+1 ] u(d) < u(ckr+1 ).Proof. Assume first W . Among k candidates highest score, least rC. Thus, least r candidates among highest-scoring k candidates Cexcluded W ; candidates {ckr+1 , . . . , ck } certainly excluded setwinners. Since candidates lower score excluded first, equality brokenfavor candidates lower utility, either c W , s[ckr+1 ] < s[c],equality holds utility c lower; particular, true W .Conversely, suppose D, either condition 1 condition 2 holds.exactly k candidates preferred (by voting rule tie-breaking mechanism)ckr+1 : {c1 , . . . , ckr }, well candidates D. Thus k winners.Denote thresh(r) = s[ckr+1 ]. Lemma 4.21, sufficient consider candidates{c C : s[c] > thresh(r) s[c] = thresh(r) u(c) < u(ckr+1 )}.Clearly, possible achieve utility accomplished addingr candidates highest utility set.Proposition 4.22. Control Removing Candidates Approval P,utility function.Proof. actually easier problem problem control adding candidates,give simpler algorithm: First, sort candidates decreasing score, sortingcandidates tied scores increasing utilities. Next, remove r candidateslowest utility first k + r candidates. Finally, check total utilitywinner set reaches t.candidates ranked position k + r cannot elected therefore irrelevantchairmans perspective. additivity utility function, clearleaving k candidates highest utility maximizes total utility chairman.Thus, simple algorithm optimal, fails deduce controlimpossible.171fiMeir, Procaccia, Rosenschein, & Zohar5. Related WorkAcademic interest complexity manipulation voting initiated Bartholdi,Tovey Tricks (1989) seminal paper. authors suggested computational complexity may prevent manipulation practice, presented voting rule, namely SecondOrder Copeland, hard manipulate. paper examined single-winner elections,goal manipulator cast vote way makes given candidatewin election.Bartholdi Orlin (1991) later proved important Single Transferable Vote(STV) rule N P-hard manipulate. STV one prominent voting rulesliterature voting. proceeds rounds; first round, voter votescandidate ranks first. every subsequent round, candidate least numbervotes eliminated, votes voters voted candidate transferednext surviving candidate ranking.Conitzer Sandholm (2003) examined voting rules usually easy manipulatesingle-winner elections, induced hardness introducing notion preround.preround, candidates paired; candidates pair competeother. introduction preround make election N P-hard, #P-hard,PSPACE-hard, depending whether preround precedes, comes after, interleavedvoting rule, respectively. Elkind Lipmaa (2005a) generalized approachusing Hybrid voting rules, composed several base voting rules.authors also considered setting entire coalition manipulators.setting, standard formulation manipulation problem follows:given set votes cast, set manipulators. addition, votesweighted, e.g., voter weight k counts k voters voting identically. askedwhether manipulators cast vote way makes specific candidate winelection.Conitzer, Sandholm, Lang (2007) shown problem N P-hardvariety voting rules. Indeed, setting manipulators must coordinatestrategies, top taking weights account, manipulation made muchcomplicated. fact, problem complicated hardness results hold evennumber candidates constant. Hemaspaandra Hemaspaandra (2007)generalized last results exactly characterizing scoring rulesmanipulation N P-hard. Elkind Lipmaa (2005b) shown use one-wayfunctions make coalitional manipulation hard.However, recent papers argued worst-case computational hardness maysufficient prevent manipulation. Indeed, although hardness impliesproblem infinite number hard instances, may still case reasonable real-world distributions instances, problem easy solve.theme discussed Procaccia Rosenschein (2007b, 2007a, 2008), ConitzerSandholm (2006), Erdelyi et al. (2007).complexity control chairman received somewhat less attention,nevertheless much already known. Bartholdi, Tovey Trick (1992) studiedcomplexity seven different types control two voting rules (and single-winner setting): adding voters/candidates, deleting voters/candidates, partitioning voters,172fiComplexity Strategic Behavior Multi-Winner ElectionsManipulationRuleSNTVBlocApprovalCumulativeIn/Ex candidatePPPPBoolean utilityGeneral utilityPPPPPPPN P-cTable 1: complexity manipulation multi-winner electionstwo types candidate partitioning. authors reached conclusion differentvoting rules differ significantly terms resistance control. HemaspaandraHemaspaandra (2007b) extended results destructive control, chairmanwants candidate elected.Hemaspaandra, Hemaspaandra Rothe (2007a) contributed investigation examining twenty different types control. showed unique winnersetting, voting rule resistant twenty types. However, unclearwhether one natural single-winner voting rules property total resistance manipulation (though voting rules come close (Faliszewski, Hemaspaandra,Hemaspaandra, & Rothe, 2007)).Finally, structured setting, voters essentially preferences subsetscandidates, related recent work combinatorial voting (see, e.g., Lang (2007)references listed there).Note paper subsumes parts Procaccia, Rosenschein Zohar (2007)Meir, Procaccia Rosenschein (2008).6. Discussionanalysis paper focused complexity manipulating controlling four simple voting schemes often considered context multi-winnerelections: SNTV, Bloc, Approval, Cumulative voting. formulation computational problems, assumed manipulator (or chairman) additiveutility function candidates. distinguished three major cases: manipulator wants one candidate included set winners, excluded setwinners; manipulator binary utility function; manipulator generalutility function.point wish direct readers attention Table 1, summarizesresults regarding manipulation. left (respectively right) implication arrow tablemeans result cell implied result left (respectively right)cell. general utility function, manipulation hard Cumulative voting, easythree. results Cumulative voting, however, carrymanipulator binary utility function. Another interesting result,appear above, restrict boolean utility functions, scoringrule easily manipulated. result formally stated proven Appendix A.173fiMeir, Procaccia, Rosenschein, & ZoharControl Adding/Removing VotersRuleIn/Ex candidate Boolean utility General utilitySNTVP [2]PPBlocN P-cN P-cN P-cApprovalN P-c*N P-cN P-cCumulativeN P-cN P-cN P-cControl Adding/Removing CandidatesRuleIn/Ex candidate** Boolean utility General utilitySNTVN P-c [1,2]N P-cN P-cBlocN P-c [1,2]N P-cN P-cApprovalPPPCumulativeIrrelevantIrrelevantIrrelevantTable 2: complexity control adding/removing voters/candidates multi-winnerelections. * result known including candidate (Hemaspaandra et al.,2007b), even single-winner elections. However, k = 1, destructive controladding/removing voters Approval tractable, casek parameter. ** context control removing candidates,discuss case excluding candidate. [1] Bartholdi et al. (1992).[2] Hemaspaandra et al. (2007b).remains open question scoring rules (if any) N P-hard manipulategeneral utility functions.Table 2 summarizes results regarding control. Notice respect control,results true three types utility functions appear table. resultsimply control adding removing voters easy SNTV hard threerules. Surprisingly, situation turned head comes control addingremoving candidates: Approval voting easy control,rules hard. short, clear hierarchy resistance control. concludeone adopt voting rule ad hoc, depending whether control tamperingset voters, set candidates, major concern.Note types control, partitioning set voters setcandidates (Bartholdi et al., 1992), investigated paper.Finally, wish connect work ongoing discussion worst-case versusaverage-case complexity manipulation control elections. mentioned Section 5, strand recent research argues worst-case hardness strong enoughguarantee resistance strategic behavior, showing manipulation problemsknown N P-hard tractable according different average-case notions (Conitzer& Sandholm, 2006; Procaccia & Rosenschein, 2007a; Zuckerman et al., 2008; Erdelyi et al.,2007). However, research still highly inconclusive. Therefore, worst-case complexity manipulation control remains important benchmark comparing different174fiComplexity Strategic Behavior Multi-Winner Electionsvoting rules, still inspires considerable steadily growing body work (Conitzeret al., 2007; Hemaspaandra et al., 2007a, 2007b; Faliszewski et al., 2007).Acknowledgmentsauthors wish thank anonymous JAIR referees helpful comments.work partially supported grant #898/05 Israel Science Foundation. ArielProcaccia supported Adams Fellowship Program Israel Academy SciencesHumanities.Appendix A. Manipulating Scoring RulesSection 3, shown SNTV Bloc voting, scoring rules,easy manipulate general utility function. next proposition establishestrue scoring rule, boolean-valued utility function.Proposition A.1. Let P scoring rule defined parameters~ = h1 , . . . , i.Manipulation P boolean-valued utility function u : C {0, 1} P.Proof. Let~ = h1 , . . . , parameters scoring rule question. Denotescore candidate c C, manipulator cast vote, s[c]. Let Jmanipulators preference profile, given by:J = c j 1 c j 2 . . . cjSuppose candidate c C ranked place l manipulator, c = cjl . Denotefinal score candidate c, according manipulators profile J , by:sJ [c] = s[c] + lFinally, denote winner set results manipulators ballot J WJ .Lemma A.2. Given C C, |C| = k, possible determine polynomial timeexists J s.t. C = WJ .Proof. Denote C = {c1 . . . ck },C = C \ C = {c1 , . . . cmk },C , C sorted nondecreasing score s[c]. LetJ = c1 c2 . . . ck c1 . . . cmkpreference profile ranks players C first, giving points candidateslower initial score. Candidates C ranked next, rule applies.intuition would like candidates C high-as-possible,less balanced, score. Likewise, would like candidates C low-as-possiblebalanced score. strategy generalizes algorithm Bartholdi et al. (1989).175fiMeir, Procaccia, Rosenschein, & Zoharclaim exists J s.t. C = WJ C = WJ . C = WJobviously exists J s.t. C = WJ . Conversely, suppose exists J #C = WJ # . Without loss generality, holds (by adversarial tie breakingassumption)8c C , c C , sJ # [c ] < sJ # [c ].(5)argue possible obtain J J # iteratively transposing pairscandidates, without changing winner set. Indeed, distinguish three cases:1. j1 , j2 {1, 2, . . . , k} s[cj1 ] > s[cj2 ] , J # holds cj1 cj2 .Now, transpose rankings cj1 cj2 J # , i.e., consider preference profileidentical J # except places cj1 cj2 switched. DenoteW new set winners.score cj2 increased, certainly still W . Moreover, new final(possibly lower) score cj1 is:s[cj1 ] + j2 s[cj2 ] + j2 = sJ # [cj2 ](5) that:c C , sJ # [c ] < sJ # [cj2 ]Therefore, cj1 W even transposition. conclude still holdsC = W .2. j1 , j2 {1, 2, . . . , k} s[cj1 ] > s[cj2 ], J # holds cj1 cj2 .similar argument holds case.3. c C , c C J # holds c c . Clearly desirable candidate c rank higher transpose two candidates.Using three types transpositions, replace couple candidatesstep obtain J J # . step remains true C = W , thusC = WJ .Lemma A.3. Given C C, C k, possible determine polynomial timeexists J s.t. C WJ .Proof. Let C C, |C | = k < k. add C k k candidates Chighest score (according s[c]), denote new set size k C . AccordingLemma A.2, determine efficiently exists J C = WJ .argue enough check C . Indeed, assume JC WJ . Let c C \ WJ exists c WJ s[c ] < s[c]. Now,transpose, ranking J , candidates c c , clearly c becomes winner cbecomes loser. Therefore, possible make C set winners.8. Tie breaking works candidates utility 1 (which ones ultimately care about),favor candidates C utility 0. However, ease exposition, dealborderline cases here.176fiComplexity Strategic Behavior Multi-Winner Electionscomplete proof proposition, denote set candidates whoseutility 1. total utility least subset sizeincluded W . Let C candidates highest score s[c] D. similararguments before, subset cannot win subset size can.Lemma A.3 efficiently find whether possible include C W .ReferencesBartholdi, J., & Orlin, J. (1991). Single transferable vote resists strategic voting. SocialChoice Welfare, 8, 341354.Bartholdi, J., Tovey, C. A., & Trick, M. A. (1989). computational difficulty manipulating election. Social Choice Welfare, 6, 227241.Bartholdi, J., Tovey, C. A., & Trick, M. A. (1992). hard control election.Mathematical Computer Modelling, 16, 2740.Brams, S. J., & Fishburn, P. C. (2002). Voting procedures. Arrow, K. J., Sen, A. K., &Suzumura, K. (Eds.), Handbook Social Choice Welfare, chap. 4. North-Holland.Conitzer, V., & Sandholm, T. (2003). Universal voting protocol tweaks make manipulation hard. Proceedings International Joint Conference Artificial Intelligence (IJCAI), pp. 781788.Conitzer, V., & Sandholm, T. (2006). Nonexistence voting rules usually hardmanipulate. Proceedings 21st National Conference Artificial Intelligence(AAAI), pp. 627634.Conitzer, V., Sandholm, T., & Lang, J. (2007). elections candidateshard manipulate?. Journal ACM, 54, 133.Duggan, J., & Schwartz, T. (2000). Strategic manipulability without resoluteness sharedbeliefs: Gibbard-Satterthwaite generalized. Social Choice Welfare, 17 (1), 8593.Elkind, E., & Lipmaa, H. (2005a). Hybrid voting protocols hardness manipulation.16th Annual International Symposium Algorithms Computation, LectureNotes Computer Science, pp. 206215. Springer-Verlag.Elkind, E., & Lipmaa, H. (2005b). Small coalitions cannot manipulate voting. International Conference Financial Cryptography, Lecture Notes Computer Science.Springer-Verlag.Endriss, U. (2007). Vote manipulation presence multiple sincere ballots. Proceedings 11th Conference Theoretical Aspects Rationality Knowledge(TARK), pp. 125134.Ephrati, E., & Rosenschein, J. S. (1997). heuristic technique multiagent planning.Annals Mathematics Artificial Intelligence, 20, 1367.Erdelyi, G., Hemaspaandra, L. A., Rothe, J., & Spakowski, H. (2007). approximating optimal weighted lobbying, frequency correctness versus average-case polynomialtime. Tech. rep., arXiv:cs/0703097v1.177fiMeir, Procaccia, Rosenschein, & ZoharFaliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007). LlullCopeland voting broadly resist bribery control. Proceedings 22nd National Conference Artificial Intelligence (AAAI), pp. 724730.Ghosh, S., Mundhe, M., Hernandez, K., & Sen, S. (1999). Voting movies: anatomyrecommender system. Proceedings 3rd Annual Conference AutonomousAgents (AGENTS), pp. 434435.Gibbard, A. (1973). Manipulation voting schemes. Econometrica, 41, 587602.Haynes, T., Sen, S., Arora, N., & Nadella, R. (1997). automated meeting scheduling system utilizes user preferences. Proceedings 1st International ConferenceAutonomous Agents (AGENTS), pp. 308315.Hemaspaandra, E., & Hemaspaandra, L. (2007). Dichotomy voting systems. JournalComputer System Sciences, 73 (1), 7383.Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007a). Hybrid elections broadencomplexity-theoretic resistance control. Proceedings 20th InternationalJoint Conference Artificial Intelligence (IJCAI), pp. 13081314.Hemaspaandra, E., Hemaspaandra, L. A., & Rothe, J. (2007b). Anyone him:complexity precluding alternative. Artificial Intelligence, 171 (56), 255285.Kellerer, H., Pferschy, U., & Pisinger, D. (2004). Knapsack Problems. Springer.Lang, J. (2007). Vote aggregation combinatorial domains structured preferences.Proceedings 20th International Joint Conference Artificial Intelligence(AAAI), pp. 13661371.Meir, R., Procaccia, A. D., & Rosenschein, J. S. (2008). broader picture complexitystrategic behavior multi-winner elections. Proceedings 7th InternationalJoint Conference Autonomous Agents Multiagent Systems (AAMAS), pp. 991998.Procaccia, A. D., Rosenschein, J. S., & Zohar, A. (2007). Multi-winner elections: Complexity manipulation, control winner-determination. Proceedings 20thInternational Joint Conference Artificial Intelligence (IJCAI), pp. 14761481.Procaccia, A. D., & Rosenschein, J. S. (2007a). Average-case tractability manipulationelections via fraction manipulators. Proceedings 6th International JointConference Autonomous Agents Multiagent Systems (AAMAS), pp. 718720,Honolulu, Hawaii.Procaccia, A. D., & Rosenschein, J. S. (2007b). Junta distributions average-casecomplexity manipulating elections. Journal Artificial Intelligence Research, 28,157181.Satterthwaite, M. (1975). Strategy-proofness Arrows conditions: Existence correspondence theorems voting procedures social welfare functions. JournalEconomic Theory, 10, 187217.Zuckerman, M., Procaccia, A. D., & Rosenschein, J. S. (2008). Algorithms coalitionalmanipulation problem. Proceedings 19th ACM-SIAM Symposium DiscreteAlgorithms (SODA), pp. 277286, San Francisco, California.178fiJournal Artificial Intelligence Research 33 (2008) 403-432Submitted 07/08; published 11/08Computational Complexity DominanceConsistency CP-NetsJudy GoldsmithGOLDSMIT @ CS . UKY. EDUDepartment Computer ScienceUniversity KentuckyLexington, KY 40506-0046, USAJrme LangLANG @ IRIT. FRIRITUniversit de Toulouse, UPS31062 Toulouse Cedex, FranceMiroslaw TruszczynskiMIREK @ CS . UKY. EDUDepartment Computer ScienceUniversity KentuckyLexington, KY 40506-0046, USANic WilsonN . WILSON @4 C . UCC . IECork Constraint Computation CentreUniversity CollegeCork, IrelandAbstractinvestigate computational complexity testing dominance consistency CP-nets.Previously, complexity dominance determined restricted classesdependency graph CP-net acyclic. However, preferences interest definecyclic dependency graphs; modeled general CP-nets. main results, showdominance consistency general CP-nets PSPACE-complete.consider concept strong dominance, dominance equivalence dominance incomparability,several notions optimality, identify complexity corresponding decision problems. reductions used proofs STRIPS planning, thus reinforce earlierestablished connections areas.1. Introductionproblems eliciting, representing computing preferences multi-attribute domain arise many fields planning, design, group decision making. However,multi-attribute preference domain, computations may nontrivial, showCP-net representation. Natural questions arise preference domain are, item preferred one?, set preferences consistent? formally, set preferencesconsistent item preferred itself. assume preferences transitive,i.e., preferred , preferred , preferred .explicit representation preference ordering elements, also called outcomes,multi-variable domains exponentially large number attributes. Therefore, AI researchersdeveloped languages representing preference orderings succinct way. formalismCP-nets (Boutilier, Brafman, Hoos, & Poole, 1999) among popular ones. CP-netc2008AI Access Foundation. rights reserved.fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSONprovides succinct representation preference ordering outcomes terms local preferencestatements form p : xi > x j , xi , x j values variable X p logical condition.Informally, preference statement p : xi > x j means given p, xi strictly preferred x j ceterisparibus, is, things equal. meaning CP-net given certain ordering relation, called dominance, set outcomes, derived reading preferencestatements. one outcome dominates another, say dominant one preferred.Reasoning preference ordering (dominance relation) expressed CP-net fareasy. key problems include dominance testing consistency testing. first problem,given CP-net two outcomes , want decide whether dominates . secondproblem asks whether dominance cycle dominance ordering defined inputCP-net, is, whether outcome dominates (is preferred to) itself.study computational complexity two problems. results obtained priorwork concerned restricted classes CP-nets, requiring graph variable dependencies implied preference statements CP-net acyclic. certain assumptions,dominance-testing problem NP and, additional assumptions, even P (Domshlak& Brafman, 2002; Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004a). show complexity general case PSPACE-complete, holds even propositional case,exhibiting Section 4 PSPACE-hardness proof dominance testing.turn consistency testing. acyclic CP-nets guaranteed consistent,case general CP-nets (Domshlak & Brafman, 2002; Brafman & Dimopoulos, 2004).Section 5, show consistency testing hard dominance testing.following two sections study decision problems related dominance optimalityCP-nets. First, consider complexity deciding strict dominance, dominance equivalencedominance incomparability outcomes CP-net. Then, study complexity decidingoptimality outcomes, existence optimal outcomes, several notions optimality.prove hardness part results, first establish PSPACE-hardness problems related propositional STRIPS planning. show problems reducedCP-net dominance consistency testing exploiting connections actions STRIPSplanning preference statements CP-nets.complexity results paper address CP-nets whose dominance relation may containcycles. earlier work concentrated acyclic model. However, argued earlier,instance Domshlak Brafman (2002), acyclic CP-nets sufficiently expressive capture human preferences even simple domains.1 Consider, instance, dinerchoose either red white wine, either fish meat. Given red wine, prefer meat,conversely, given meat prefer red wine. hand, given white wine, prefer fish,conversely, given fish prefer white wine. gives consistent cyclic CP-net,acyclic CP-net giving rise preferences outcomes. So, cyclicity preferencevariables necessarily lead cyclic order outcomes.1. mean say cyclic CP-nets sufficient capture possible human preferences simple domainsobviously true. However, note every preference relation extends preference relation inducedCP-net possibly cyclic dependencies. property longer true cyclic dependenciesprecluded but, case binary variables, number linear orders extends acyclic CP-netexponentially smaller number linear orders (Xia, Conitzer, & Lang, 2008).404fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETSassume familiarity complexity class PSPACE. refer Papadimitriou(1994) details. particular, later use identities NPSPACE = PSPACE = coPSPACE.several places, consider versions decision problem, input instancesassumed additional property. problems usually formulated followingway: Q, given R2 . first note Q, given R problem Q R. Letus recall definition decision problem presented Ausiello et al. (1999). decisionproblem pair P = hIP ,YP IP set strings (formally, subset ,finite alphabet), decision problem P = hIP ,YP reads follows: given string x IP , decidewhether x YP . problem hIP ,YP complexity class C language YP C (thisdepend IP ). problem hIQ ,YQ reducible hIP ,YP polynomial-timefunction F (1) every x IQ , F(x) IP , (2) every x IQ , x YQF(x) YP . Thus, P decision problem Q, given R, IP set stringssatisfying R, YP set strings satisfying R Q. problems, grantedinput belongs R; solve check input string indeedelement R. problems Q, given R widespread literature. However,cases, R simple property, checked polynomial (and often linear) time,decide whether graph possesses Hamiltonian cycle, given every vertex degree3. Here, however, consider several problems Q, given R Rclass P (unless polynomial hierarchy collapses). However, said above, complexityrecognizing whether given string R matter. words, complexity Q,given R same, whether R recognized unit time PSPACE-complete.come back first problem appears paper (cf. proof Proposition 5).case consider complexity R greater complexity Q.part paper (up Section 5) extended version earlier conference publication(Goldsmith, Lang, Truszczynski, & Wilson, 2005). Sections 6 7 entirely new.2. Generalized Propositional CP-NetsLet V = {x1 , . . . , xn } finite set variables. variable x V , assume finite domainDx values. outcome n-tuple (d1 , . . . , dn ) Dx1 Dxn .paper, focus propositional variables: variables binary domains. Let Vfinite set propositional variables. every x V , set Dx = {x, x} (thus, overloadnotation write x variable one values). refer x x literals.Given literal l write l denote dual literal l. focus binary variables makespresentation clearer impact complexity results.also note case binary domains, often identify outcome setvalues (literals). fact, also often identify sets conjunctions elements.Sets (conjunctions) literals corresponding outcomes consistent complete.conditional preference rule (sometimes, preference rule rule) V expression p : l > l, l literal atom x V p propositional formula Vinvolve variable x.2. literature one often finds following formulation: Q, even R, exactlymeaning Q, given R. Specifically, saying Q NP-complete, even R, one means Q NP-complete,Q, given R NP-complete well.405fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSONrest paper, need refer two different languages: conditional preferencelanguage every (binary) variable x, conditional preference table x needs specifypreferred value x every possible assignment parent variables, generallanguage tables may incomplete (for values parents, preferred valuex may specified) and/or locally inconsistent (for values parents, table maycontain information x preferred information x preferred). calllanguages respectively CP-nets GCP-nets (for generalized CP-nets). Note GCP-netsnew, similar structures discussed (Domshlak, Rossi, Venable, & Walsh,2003). reason use terminology (CP-nets GCP-nets) twofold. First, evenassumptions completeness local consistency CP-nets sometimes relaxed,papers CP-nets make them. Second, could used CP-nets locally consistent,complete CP-nets instead GCP-nets CP-nets, felt notation simplertransparent.Definition 1 (Generalized CP-net) generalized CP-net C (for short, GCP-net) Vset conditional preference rules. x V define pC+ (x) pC (x), usually written just:p+ (x) p (x), follows: pC+ (x) equal disjunction p exists rulep : x > x C; pC (x) disjunction p exists rule p : x > x C.define associated directed graph GC (the dependency graph) V consist pairs (y, x)variables appears either p+ (x) p (x).complexity results also need following representation GCP-nets: GCPnet C said conjunctive form C contains rules p : l > l p (possiblyempty) conjunction literals. case formulas p (x), p+ (x) disjunctive normal form,is, disjunction conjunctions literals (including empty conjunction literals).GCP-nets determine transitive relation outcomes, interpreted terms preference.preference rule p : l > l represents statement given p holds, l preferred l ceterisparibus. intended meaning follows. outcome satisfies p l, preferredoutcome differs assigns l variable x. situation sayimproving flip sanctioned rule p : l > l.Definition 2 0 , . . . , sequence outcomes 1 next outcomesequence obtained previous one improving flip, say 0 , . . . ,improving sequence 0 GCP-net, dominates 0 , written 0 .Finally, GCP-net consistent outcome strictly preferred itself,is, .main objective paper establish complexity following two problemsconcerning notion dominance associated GCP-nets (sometimes restrictionsclass input GCP-nets).Definition 3GCP - DOMINANCE : given GCP-net C two outcomes , decide whether C,is, whether dominates C.GCP - CONSISTENCY : given GCP-net C, decide whether C consistent.406fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETSGCP-nets extend notion CP-nets (Boutilier et al., 1999). two propertiesGCP-nets essential linking two notions.Definition 4GCP-net C V locally consistent every x V , formula pC (x) pC+ (x) unsatisfiable. locally complete every x V , formula pC (x) pC+ (x) tautology.Informally, local consistency means outcome x preferredx x preferred x. Local completeness means that, every variable x, every outcomeeither x preferred x x preferred x.Definition 5 (Propositional CP-net) CP-net set V (propositional) variables locally consistent locally complete GCP-net V .easy decide whether GCP-net actually CP-net. fact, task coNPcomplete.Proposition 1 problem deciding, given GCP-net C, whether C CP-net coNPcomplete.Proof: Deciding whether GCP-net C CP-net consists checking local consistency localcompleteness. tasks amounts n validity tests (one variable). followsdeciding whether GCP-net CP-net intersection 2n problems coNP. Hence,coNP, itself. Hardness comes following reduction UNSAT. propositionalformula assign CP-net C(), defined set variables Var(){z}, z 6 Var(),following tables:+variable x 6= z: pC()(x) = ; pC()(x) = ;+(z) = .(z) = ; pC()pC()++(z) = . There(z) pC()(x) = ; moreover, pC()(x) pC()variable x 6= z, pC()+fore, C() locally consistent. Now, variable x 6= z, pC() (x) pC()(x) = .+Moreover, pC() (z) pC() (z) = . Thus, C() locally complete unsatisfiable.follows C() CP-net unsatisfiable.Many works CP-nets make use explicit conditional preference tables list every combination values parent variables (variables x depends) exactly once, combination designating either x x preferred.3 Clearly, CP-nets restricted senseregarded CP-nets sense that, every variable x, satisfy following condition:y1 , . . . , yk atoms appearing p+ (x) p (x) every completeconsistent conjunction literals {y1 , . . . , yn } appears disjunct exactly onep+ (x) p (x).3. exceptions. discussed instance Boutilier et al. (2004a) Section 6 paper.407fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSONembedding, concepts dominance consistency introduced GCP-netsgeneralize ones considered CP-nets defined Boutilier et al. (2004a).Problems CP - DOMINANCE CP - CONSISTENCY defined analogously Definition 3.paper interested complexity dominance consistency problems GCPnets CP-nets. Therefore, matter way nets (especially CP-nets,GCP-nets alternative proposals) represented important. representationCP-nets often compact one proposed Boutilier et al. (2004a), formulasp+ (x) p (x) implied conditional preference tables often given equivalent,exponentially smaller, disjunctive normal form representations. Thus, defining decisionproblem, critical specify way represent input instances, representation mayaffect complexity problem. Unless stated otherwise, assume GCP-nets (and thus,CP-nets) represented set preference rules, described Definition 1. Therefore,size GCP-net given total size formulas p (x), p+ (x), x V .note key property consistent GCP-nets, use several times laterpaper.Proposition 2 GCP-net C consistent locally consistent.Proof: C locally consistent exists variable x outcome satisfyingpC (x) pC+ (x). shown flipping x current value dual valueflipping back: since satisfies pC (x) pC+ (x), since pC (x) pC+ (x) involveoccurrences x, flips allowed.Finally, conclude section example illustrating notions discussed above.Example 1 Consider GCP-net C variables V = {x, y} four rules, defined follows:x : > y; x : > y; : x > x; : x > x. p+ (y) = x, p (y) = x, p+ (x) =p (x) = y. Therefore C locally consistent locally complete, CP-net.cycle dominance outcomes: x x x x x y,C inconsistent. shows consistency strictly stronger property localconsistency.3. Propositional STRIPS Planningsection derive technical results propositional STRIPS planning formbasis complexity results Sections 4 5. establish complexity plan existenceproblems propositional STRIPS planning restrictions input instances makeproblem use studies dominance consistency GCP-nets.Let V finite set variables. state V complete consistent set literalsV , often view conjunction members. state therefore equivalentoutcome, defined CP-nets context.Definition 6 (Propositional STRIPS planning) propositional STRIPS instance meantuple hV, 0 , , ACTi,1. V finite set propositional variables;408fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS2. 0 state V , called initial state;3. state called goal;44. ACT finite set actions, action ACT described consistent conjunction literals pre(a) (a precondition) consistent conjunction literals post(a) (apostcondition, effect).5action executable state |= pre(a). effect state , denoted eff (a, ),state containing literals variables mentioned post(a),literals post(a). assume action applied state,change state preconditions hold: 6|= pre(a) (given states complete,equivalent |= pre(a)) eff (a, ) = . assumption influence farcomplexity results concerned.PROPOSITIONAL STRIPS PLAN EXISTENCE problem, STRIPS PLAN short, decide whether given propositional STRIPS instance hV, 0 , , ACTi finite sequenceactions leading initial state 0 final state . sequence planhV, 0 , , ACTi. plan irreducible every one actions changes state.assume, without loss generality, action a, literal post(a) appears alsopre(a); otherwise omit literal post(a) without changing effect action;post(a) becomes empty conjunction, action omitted ACTeffect.following result due Bylander (1994).Proposition 3 (Bylander, 1994)STRIPS PLANPSPACE-complete.Typically, propositional STRIPS instances require goals states. Instead, goalsdefined consistent conjunctions literals need complete. setting,plan sequence actions leads start state state goal holds.restrict consideration complete goals. restriction effect complexity planexistence problem: remains PSPACE-complete goal-completeness restriction (Lang,2004).3.1 Acyclic STRIPSDefinition 7 (Acyclic sets actions) set actions ACT (we use notation Definition 6) acyclic state hV, , , ACTi non-empty irreducible plan,say, non-trivial directed cycles graph states induced ACT.establish complexity following problem:ACTION - SET ACYCLICITY :given set ACT actions, decide whether ACT acyclic.Proposition 4ACTION - SET ACYCLICITY PSPACE-complete.4. Note standard STRIPS goal partial state. point discussed Proposition 3.5. emphasize allow negative literals preconditions goals. definitions STRIPS allowthis. particular variant STRIPS sometimes called PSN (propositional STRIPS negation) literature.409fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSONProof: argument membership PSPACE standard; nevertheless give details.omit details proofs membership PSPACE. following nondeterministic algorithm decides ACT cycle:guess 0 ;:= 0 ;repeatguess action ACT ;:= eff (a, );:== 0 .algorithm works nondeterministic polynomial space (because need store 0 ,), shows ACTION - SET ACYCLICITY NPSPACE, therefore PSPACE,since NPSPACE = PSPACE. Thus, ACTION - SET ACYCLICITY coPSPACE, hence PSPACE,since coPSPACE = PSPACE.show complement ACTION - SET ACYCLICITY problem PSPACEhard reducing ACYCLIC STRIPS PLAN problem it.Let PE = hV, 0 , , ACTi instance ACYCLIC STRIPS PLAN problem. particular,ACT acyclic. Let new action defined pre(a) = post(a) = 0 . easysee ACT {a} acyclic exists plan PE. Thus, PSPACEhardness complement ACTION - SET ACYCLICITY problem follows Proposition5. Consequently, ACTION - SET ACYCLICITY problem coPSPACE-hard. Since PSPACE =coPSPACE, ACTION - SET ACYCLICITY problem PSPACE-hard, well.Next, consider STRIPS planning problem restricted instances acyclic setsactions. Formally, consider following problem:ACYCLIC STRIPS PLAN : Given propositional STRIPS instance hV, 0 , , ACTiACT acyclic 0 6= , decide whether plan hV, 0 , , ACTifirst problems form Q, given R encounter illustrateswell concerns discussed end introduction. Here, R set propositionalSTRIPS instances hV, 0 , , ACTi ACT acyclic, Q set instancesplan hV, 0 , , ACTi. Checking whether given propositional STRIPS instanceactually acyclic PSPACE-complete (this Proposition 4 states),matter comes solving ACYCLIC STRIPS PLAN: considering instance ACYCLICSTRIPS PLAN , already know acyclic (and reflected reduction below).Proposition 5ACYCLIC STRIPS PLANPSPACE-complete.Proof: argument membership PSPACE standard (cf. proof Proposition 4).prove PSPACE-hardness, first exhibit polynomial-time reduction F STRIPS PLAN. LetPE = hV, 0 , , ACTi instance STRIPS PLAN. idea behind reduction introducecounter, time action executed, counter incremented. counter maycount 2n , n = |V |, making use n additional variables. counter initialized410fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETS0. reaches 2n 1 longer incremented action executed. Hence,set actions resulting instance STRIPS PLAN acyclic: guaranteed produceinstance R. describe reduction, write V {x1 , . . . , xn }. define F(PE) = PE =hV , 0 , , ACT follows:V = {x1 , . . . , xn , z1 , . . . , zn }, zi new variables use implement counter;0 = 0 z1 zn ;= z1 zn ;action ACT, include ACT n actions ai , 1 n, that:pre(ai ) = pre(a) zi zi+1 znn 1 :post(ai ) = post(a) zi zi+1 zn ,pre(an ) = pre(a) zn= n :post(an ) = post(a) zn .Furthermore, include ACT n actions bi , 1 n, that:pre(bi ) = zi zi+1 znn 1 :post(bi ) = zi zi+1 zn ,pre(bn ) = zn= n :post(bn ) = zn .denote states V pairs (, k), state V k integer, 0k 2n 1. view k compact representation state variables z1 , . . . , zn : assumingbinary representation k d1 . . . dn (with dn least significant digit), k representsstate contains zi di = 1 zi , otherwise. instance, let V = {x1 , x2 , x3 }.V = {x1 , x2 , x3 , z1 , z2 , z3 }, state x1 x2 x3 z1 z2 z3 denoted (x1 x2 x3 , 5).note effect ai bi state (, k) either void, increments counter:eff (ai , (, k)) =(eff (a, ), k + 1) ai executable (, k)(, k)otherwiseeff (bi , (, k)) =(, k + 1) bi executable (, k)(, k)otherwiseNext, remark one ai one bi executable given state (, k).precisely,k < 2n 1, exactly one bi executable (, k); denote i(k) index bi(k)executable (, k) (this index depends k). also ai(k) executable(, k), provided executable .k = 2n 1, ai bi executable (, k).411fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSONshow PE acyclic. Assume irreducible plan hV , , , ACT i. Let= (, k). k < 2n 1, empty, since action ACT state either nonexecutable increments counter, irreducible plan contains actions whose effectnon-void. k = 2n 1, action ACT executable empty. Thus,exists non-empty irreducible plan hV , , , ACT i, holds . Therefore PEacyclic.claim plan PE plan PE . First, assumeplan PE. Let shortest plan PE let length (the number actionsused). 2n 1, since state along repeats (otherwise, shorter plans PEwould exist). Let 0 , 1 , . . . , = sequence states obtained executing . Letaction used transition k k+1 . Since k < 2n 1 (because 2n 1 k 1),exactly one i, 1 n, action ai applies state (, k) V . Replacingai yields plan started (0 , 0) leads (m , m) = (, m). Appendingplan appropriate actions bi increment counter 2n 1 yields plan PE . Conversely,plan PE , plan obtained removing actions form b j replacingaction ai plan PE, since ai effect V does. Thus, claimfollows.emphasize reduction F STRIPS PLAN ACYCLIC STRIPS PLAN (or, equivalently, STRIPS PLAN given ACTION - SET ACYCLICITY) works satisfies followingtwo conditions:1. every instance PE STRIPS PLAN, F(PE) instance ACYCLIC STRIPS PLAN (thisholds every PE, F(PE) acyclic);2. every PE STRIPS PLAN, F(PE) positive instance ACYCLICPE positive instance STRIPS PLAN.STRIPS PLAN3.2 Mapping STRIPS Plans Single-Effect STRIPS PlansVersions STRIPS PLAN ACYCLIC STRIPS PLAN problems important us allow actions exactly one literal postconditions input propositional STRIPSinstances. call actions single-effect actions.6 refer restricted problems SESTRIPS PLAN ACYCLIC SE STRIPS PLAN , respectively.prove PSPACE-hardness problems, describe mapping STRIPS instancessingle-effect STRIPS instances.7Consider instance PE = hV, 0 , , ACTi STRIPS PLAN problem, ACT necessarily acyclic. action ACT introduce new variable xa , whose intuitive meaningaction currently executed.Vset X = aACT xa . is, X conjunction negative literals additionalVvariables. addition, ACT set Xa = xa bACT{a} xb . defineinstance PE = hV , 0 , , S(ACT)i SE STRIPS PLAN problem follows:6. actions also called unary actions planning literature. stick terminology single-effectalthough less commonly used, simply explicit.7. PSPACE-completeness propositional STRIPS planning single-effect actions proved already Bylander(1994). However, deal acyclicity need give different reduction one used paper.412fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETSSet variables: V = V {xa : ACT};initial state: 0 = 0 X;goal state: = X;set actions: S(ACT) = {ai : ACT, = 1, . . . , 2|post(a)| + 1}.Let action ACT post(a) = l1 lq , l1 , . . . , lq literals.= 1, . . . , q, define action ai setting:pre(ai ) = pre(a) X li ; post(ai ) = xa .role ai enforce Xa holds ai successfully applied,way enable starting execution a, provided action currentlyexecuted, ith effect already true, precondition true.= q + 1, . . . , 2q, define action ai setting:pre(ai ) = Xa ; post(ai ) = li .role ai make ith effect true.Finally, define a2q+1 setting:pre(a2q+1 ) = Xa l1 lq ; post(a2q+1 ) = xa .Thus, a2q+1 designed X holds a2q+1 successfully applied; is, a2q+1closes execution a, thus allowing next action executed.Let sequence actions ACT. define S() sequence actions S(ACT)obtained replacing action a1 , . . . , a2q+1 , q = |post(a)|. considersequence actions S(ACT). Remove every action ai 6= 2|post(a)| + 1,replace actions form a2|post(a)|+1 a. denote resulting sequence actionsACT (). note (S()) = . following properties hold.Lemma 1 definitions,(i) plan PE S() plan PE ;(ii) irreducible plan PE () irreducible plan PE;(iii) ACT acyclic S(ACT) acyclic.Proof: (i) Let ACT action, let state let state obtainedapplying a. Let V -state obtained applying sequence actions ha1 , . . . , a2q+1(where q = |post(a)|) state X PE . show = X.note = 1, . . . , q, state X satisfy pre(ai ) sequenceactions ha1 , . . . , a2q+1 effect, state still X. happen, either doesntsatisfy pre(a), l1 , . . . , lq already hold post(a) holds . either case, = ,= X.413fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSONSuppose {1, . . . , q}, satisfy pre(ai ). first actioncauses xa hence Xa hold. applying actions aq+1 , . . . , a2q , l1 lq holds,post(a) holds. applying a2q+1 post(a) X hold. variable V changed,= X, required.Applying result iteratively implies plan PE S() plan PE .ai(ii) Let irreducible plan PE , every action changes state, impliesevery action performed state precondition true. show ()/ = 0,/ () = 0,/ too, assertion follows.plan PE. assume 6= 0.jWrite first action , ACT, let maximal initial subsequenceconsisting actions form ai . must j |post(a)|, since X holds 0 (byassumption above, action j apply) X inconsistent precondition ai> |post(a)|. Also, pre(a j ) l j hold 0 so, 0 well. Thus, 0 satisfies pre(a),applying changes state, since l j holds 0 post(a) |= l j . Let us denote stateresulting applying 0 . noted, 6= 0 ,Let state resulting applying 0 . goal state X holds .goal state 6= . Let bi action directly following last action .definition , 6= b. applying j , Xa holds, either Xa holds X holds. Thus,Xb hold, 6= b. Since bi changes state, must {1, . . . , |post(b)|}, X holdscase, too.Hence last action a2q+1 , q = |post(a)|. Since variables Vaffected actions ai appear literals post(a) since action a2q+1executed (otherwise would belong ), follows = X.Applying reasoning repeatedly, show applying () 0 yields ,action () changes state, () irreducible plan PE, non-emptynon-empty.(iii) Suppose ACT acyclic, exists state non-empty irreducible planPE = hV, , , ACTi. Then, (i), S() plan PE = hV , X, X, S(ACT )i.non-empty irreducible, changes state, S() also changes state, hencereduced non-empty irreducible plan PE . Therefore S(ACT) acyclic.Conversely, suppose S(ACT) acyclic. exists state non-emptyirreducible plan hV , , , S(ACT)i. first prove X holds state obtainedexecution plan./Suppose X holds state, let j first action . note 6= 0.assumption, X hold either applying j . Therefore q + 1 j 2q,q = |post(a)|. Since irreducible, j changes state. Thus, l j holds l j holdsstate resulting applying j .assumption, Xa holds applying j . Thus, next action, one,must also form ai q + 1 2q. Repeating argument implies actionsform ai q + 1 2q. Since set literals post(a) consistent, l jnever reset back l j . Thus, state resulting applying different ,contradiction.Thus, X holds state reached execution . Let us consider one state.written X, state V . cyclically permute generatenon-empty irreducible plan hV , X, X, S(ACT)i. part (ii), ( ) non-empty414fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETSirreducible plan hV, , , ACTi. Therefore ACT acyclic.Proposition 6SE STRIPS PLANACYCLIC SE STRIPS PLAN PSPACE-complete.Proof: Again, argument membership PSPACE standard. PSPACE-hardnessACYCLIC SE STRIPS PLAN shown reduction ACYCLIC STRIPS PLAN . construction shows STRIPS PLAN reducible SE STRIPS PLAN, thus SE STRIPS PLANPSPACE-complete.Let us consider instance PE = hV, 0 , , ACTi ACYCLIC STRIPS PLAN. define PE =hV , 0 , , S(ACT)i, Lemma 1(iii) instance ACYCLIC SE STRIPS PLAN problem. Lemma 1(i) (ii) exists plan PE exists plan PE .implies ACYCLIC SE STRIPS PLAN PSPACE-hard.4. Dominancegoal section prove GCP - DOMINANCE problem PSPACE-complete,complexity go even restrict class inputs CP-nets.use results propositional STRIPS planning Section 3 prove general GCP DOMINANCE problem PSPACE-complete. show complexity changerequire input GCP-net locally consistent locally complete.similarities dominance testing CP-nets propositional STRIPS planningfirst noted Boutilier et al. (1999). presented reduction, discussed later detailBoutilier et al. (2004a), dominance problem plan existence problem classpropositional STRIPS planning specifications consisting unary actions (actions singleeffects). prove results GCP - DOMINANCE GCP - CONSISTENCY problems constructing reduction direction.reduction much complex one used Boutilier et al. (1999), duefact CP-nets impose restrictions STRIPS planning. Firstly, STRIPS planning allowsmultiple effects, GCP-nets allow flips x > x x > x change value onevariable; constructed reduction STRIPS planning single-effect STRIPSplanning last section. Secondly, CP-nets impose two restrictions, local consistencylocal completeness, natural counterparts context STRIPS planning.dominance consistency problems consider, membership PSPACEdemonstrated similarly membership proof Proposition 4, namely considering nondeterministic polynomial space algorithms consisting repeatedly guessing appropriate improving flipsmaking use fact PSPACE = NPSPACE = coPSPACE. Therefore,provide arguments PSPACE-hardness problems consider.4.1 Dominance Generalized CP-Netsprove GCP - DOMINANCE problem PSPACE-complete reductionproblem SE STRIPS PLAN, know PSPACE-complete.415fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON4.1.1 APPING INGLE -E FFECT STRIPS P ROBLEMSP ROBLEMSGCP-N ETS OMINANCELet hV, 0 , , ACTi instance SE STRIPS PLAN problem. every action ACTdenote la unique literal postcondition a, is, post(a) = la . denotepre (a) conjunction literals pre(a) different la (we recall conventionadopted earlier, pre (a) contain la ). define ca conditional preference rulepre (a) : la > la define M(ACT) GCP-net C = {ca : ACT}, conjunctiveform.sequence states plan corresponds improving sequence 0 , leadsfollowing result.Lemma 2 notation,(i) non-empty irreducible plan hV, 0 , , ACTi dominates 0M(ACT);(ii) ACT acyclic M(ACT) consistent.Proof: first note following equivalence. Let action ACT, letdifferent outcomes (or, STRIPS setting, states). action applied yieldsrule ca sanctions improving flip . applied yieldssatisfies pre(a) differ literal la , satisfying la satisfyingla . satisfies pre (a) differ literal la , satisfyingla , satisfying la . This, turn, equivalent say rule ca sanctions improving flip.Proof (i): Suppose first exists non-empty irreducible plan a1 , . . . , hV, 0 , , ACTi.Let 0 , 1 , . . . , = corresponding sequence outcomes, and, = 1, . . . , m, action ai , applied state i1 , yields different state . equivalence,= 1, . . . , m, cai sanctions improving flip i1 , implies 0 , 1 , . . . ,improving flipping sequence M(ACT), therefore dominates 0 M(ACT).Conversely, suppose dominates 0 M(ACT), exists improving flippingsequence 0 , 1 , . . . , = , 1. = 1, . . . , m, let cai elementM(ACT) sanctions improving flip i1 . Then, equivalence,action ai , applied state i1 yields (which different i1 ), a1 , . . . ,non-empty irreducible plan hV, 0 , , ACTi.Proof (ii): ACT acyclic exists state non-empty irreducibleplan hV, , , ACTi. (i) exists outcome dominatesM(ACT), M(ACT) consistent.Theorem 1 GCP - DOMINANCE problem PSPACE-complete. Moreover, remainsrestrictions GCP-net consistent conjunctive form.Proof: PSPACE-hardness shown reduction ACYCLIC SE STRIPS PLAN (Proposition 6).Let hV, 0 , , ACTi instance ACYCLIC SE STRIPS PLAN problem. Lemma 2(ii),M(ACT) consistent GCP-net conjunctive form. Since 0 6= (imposed definition416fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETSproblem ACYCLIC SE STRIPS PLAN), plan hV, 0 , , ACTinon-empty irreducible plan hV, 0 , , ACTi, which, Lemma 2(i), dominates0 C.Theorem 1 implies PSPACE-completeness dominance general conditionalpreference language introduced Wilson (2004b), conditional preference rules written conjunctive form.4.2 Dominance CP-Netssection show GCP - DOMINANCE remains PSPACE-complete restrictionlocally consistent locally complete GCP-nets, is, CP-nets. refer restrictionGCP - DOMINANCE CP - DOMINANCE .Consistency GCP-net implies local consistency (Proposition 2). Therefore, reduction proof Theorem 1 (from ACYCLIC SE STRIPS PLAN GCP - DOMINANCE restrictedconsistent GCP-nets) also reduction GCP - DOMINANCE restricted locally consistentGCP-nets. PSPACE-hardness ACYCLIC SE STRIPS PLAN (Proposition 6) implies GCP DOMINANCE restricted locally consistent GCP-nets PSPACE-hard, and, fact, PSPACEcomplete since membership PSPACE easily obtained usual line argumentation.show PSPACE-hardness CP - DOMINANCE reduction GCP - DOMINANCEconsistent GCP-nets.4.2.1 APPING L OCALLY C ONSISTENT GCP-N ETSCP-N ETSLet C locally consistent GCP-net. Let V = {x1 , . . . , xn } set variables C. define/ define GCP-net C V ,V = V {y1 , . . . , yn }, {y1 , . . . , yn } V = 0.show CP-net. end, every z V define conditional preference rulesq+ (z) : z > z q (z) : z > z included C specifying formulas q+ (z) q (z).First, variable xi V , setq+ (xi ) = yi q (xi ) = yi .Thus, xi depends yi . also note formulas q+ (xi ) q (xi ) satisfy local consistency local completeness requirements.Next, variable yi , 1 n, defineei = (x1 y1 ) (xi1 yi1 ) (xi+1 yi+1 ) (xn yn ),fi+ = ei p+ (xi ) fi = ei p (xi ).Finally, defineq+ (yi ) = fi+ ( fi xi )q (yi ) = fi ( fi+ xi ).Thus, yi depends every variable V itself.note local consistency C, formulas fi+ fi , 1 n, unsatisfiable.Consequently, formulas q+ (yi ) q (yi ), 1 n, unsatisfiable. Thus, C locally consistent.417fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSONFinally, q+ (yi ) q (yi ) equivalent fi+ xi fi xi , tautology. Thus, C locallycomplete hence CP-net V .Let outcomes {x1 , . . . , xn } {y1 , . . . , yn }, respectively. denoteoutcome V obtained concatenating n-tuples . Conversely, every outcome Cwritten way.Let outcome V . define outcome {y1 , . . . , yn } obtainedreplacing every component form xi yi every component xi yi .every i, 1 n, |= ei .Let sequence 0 , . . . , outcomes V . Define L(s) sequence V outcomes: 0 0 , 0 1 , 1 1 , 1 2 , . . . , . Further, let sequence 0 , 1 , . . . , V outcomes 0 = = . Define L (t) sequence obtained projectingelement V iteratively removing elements sequencepredecessor (until two consecutive outcomes different).Lemma 3 definitions,(i) improving sequence C L(s) improving sequence C;(ii) improving sequence L (t) improving sequence ;(iii) C consistent C consistent.Proof: Let e = ni=1 (xi yi ). definitions arranged GCP-net CCP-net C following properties:(a) e hold outcome V , every improving flip applicable changesvalue variable xi yi xi yi holds flip.Indeed, let us assume improving flip outcome V .flip concerns variable xi , xi yi holds . Consequently, xi yi holds .Thus, let us assume flip concerns variable yi . ei holds then, since e not,xi yi holds . Thus, xi yi holds . ei hold neither fi+ fi does.Thus, xi (xi , respectively) holds , yi (yi , respectively) holds . Since flip concerns yi ,follows xi yi holds .(b) improving flip changes variable xi .Indeed, variable xi , since e holds , xi yi holds , too. Thus, improvingflip changes xi .(c) improving flip C changes variable yi outcomeimproving flip GCP-net C outcome changes variable xi . applyingimproving flip (changing variable yi ) , exactly one improving flip possible. changesxi results outcome , outcome V resulting applyingimproving flip changing variable xi .prove (c), let us first assume yi holds observe case xi holds, too. follows q+ (yi ) holds p+ (xi ) holds . Consequently, changingyi improving flip C changing xi improving flip C.argument case yi holds analogous (but involves q (yi ) p (xi )). Thus,first part (c) follows.V418fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETSLet outcome obtained applying improving flip xi . followsimproving flip changing value yi results outcome . outcome, (a),improving flip must concern x j j x j j holds flip. Since every j 6= i,x j j holds , improving flips concern either xi yi . local consistencyC , yi cannot flipped right back. Clearly, changing xi improving flip applied. discussion, improving flip applicable results outcome. proves second part (c).Proof (i): assertion follows iterative application (c).Proof (ii): Suppose improving sequence 0 , 1 , . . . , V -outcomes 0 == . Since e holds 0 , (b) implies first flip changes variable yi , (c)implies second flip changes variable xi make xi yi hold again. Hence 2 written. (c) improving flip C outcome changing variable xi , is, leading. Iterating process shows L (t) improving sequence .Proof (iii): Suppose C inconsistent. exists outcome improvingsequence C . (i), L(s) improving sequence , proving Cinconsistent.Conversely, suppose C inconsistent, exists improving sequence Coutcome itself. (a), improving flip applied outcome e holdincreases (by one) number xi yi holds. implies e must holdoutcome t, acyclic. Write outcome . cyclically permuteform improving sequence t2 itself. Part (ii) implies L (t2 ) improvingflipping sequence C itself, showing C inconsistent.Theorem 2 CP - DOMINANCE PSPACE-complete. holds even restrict CP-netsconsistent.Proof: use reduction PSPACE-hardness GCP - DOMINANCE problemGCP-nets restricted consistent (Theorem 1). Let C consistent, hence locallyconsistent, GCP-net V , let outcomes V . Consider CP-net Cvariables V constructed above. Lemma 3(i) (ii) imply dominates Cdominates C . Moreover, C consistent Lemma 3(iii). Consequently, hardness partassertion follows.Note PSPACE-hardness obviously remains require input outcomes different,reduction Theorem 1 uses pair different outcomes.Notice huge complexity gap problem deciding whether exists nondominated outcome, NP-complete (Domshlak et al., 2003, 2006).5. Consistency GCP-Netssection showSections 3 4.GCP - CONSISTENCY419problem PSPACE-complete, using resultsfiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSONTheorem 3GCP - CONSISTENCY PSPACE-complete. holds even restriction GCP-netsconjunctive form.Proof: PSPACE-hardness shown reduction ACTION - SET ACYCLICITY. apply function Section 3.2 followed Section 4.1. maps instances ACTION - SETACYCLICITY instances GCP - CONSISTENCY conjunctive form. Lemma 1(iii) Lemma2 (ii), instance ACTION - SET ACYCLICITY acyclic corresponding instanceGCP - CONSISTENCY consistent, proving result.show consistency testing remains PSPACE-complete CP-nets (GCP-netslocally consistent locally complete).Theorem 4CP - CONSISTENCYPSPACE-complete.Proof: use reduction GCP - CONSISTENCY restriction GCP-netconjunctive form. Let C GCP-net conjunctive form. define CP-net C follows. C conjunctive form, local consistency decided polynomial time, amountschecking consistency conjunction conjunctions literals. C locally consistentset C predetermined inconsistent locally consistent CP-net, exampleSection 2. Otherwise, C locally consistent C take CP-net constructedSection 4.2. mapping locally consistent GCP-nets CP-nets, described Section 4.2,preserves consistency (Lemma 3 (iii)). Since local inconsistency implies inconsistency (Proposition 2), GCP-net C consistent CP-net C consistent. Thus,PSPACE-hardness CP - CONSISTENCY problem follows Theorem 3.6. Additional Problems Related Dominance GCP-Netsproved main results consistency dominance GCP-nets, moveadditional questions concerning dominance relation. state them, introduceterminology.Let outcomes GCP-net C. say dominance-equivalent C,written C , = , C C . Next, dominance-incomparable C6= , C C . Finally, strictly dominates C 6C .Definition 8define following decision problems:SELF - DOMINANCE : given GCP-net C outcome , decide whether C , is, whetherdominates C.STRICT DOMINANCE : given GCP-net C outcomes , decide whether strictly dominates C.DOMINANCE EQUIVALENCE : given GCP-net C outcomes , decide whetherdominance-equivalent C.DOMINANCE INCOMPARABILITY : given GCP-net C outcomes , decide whetherdominance-incomparable C.420fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETSestablishing complexity problems, use polynomial-time reductionsproblem GCP - DOMINANCE. Let H GCP-net set variables V = {x1 , . . . , xn },let outcome. define GCP-net G = 1 (H, ) set variables W = V {y}setting conditions flips variables xi , = 1, . . . , n, follows:1. xi :+p+G (xi ) = pH (xi )pG (xi ) = pH (xi )2. xi :+p+G (xi ) = pH (xi )pG (xi ) = pH (xi )3. p+G (y) =4. pG (y) = .mapping 1 computed polynomial time. Moreover, one check Hlocally consistent GCP-net, 1 (H, ) also locally consistent. Finally, H CP-net, 1 (H, )CP-net, well.every V -outcome , let + = = y. note every W -outcomeform + . explain structure GCP-net G, point improvingflip G + + improving flip H (thus, G restrictedoutcomes form + forms copy GCP-net H). Moreover, improving flipG agrees exactly one variable xi does.Finally, improving flip moves outcomes different type transforms+ , + 6= .formalize useful properties GCP-net G = 1 (H, ). use notationintroduced above.Lemma 4 every V -outcome , G + and, 6= , + G + (in words, + dominatesevery W -outcome).Proof: Consider V -outcome 6= . C since, given y, changing literalform improving flip. definition, also CG (as 6= ). follows G + + G G + . Thus, assertionfollows.Lemma 5 arbitrary V -outcome different , following statements equivalent:1. H ;2. + G + ;3. + G + .421fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSONProof: Lemma 4, + G + . Thus, conditions (2) (3) equivalent.[(1)(2)] Clearly (recall discussion structure G), improving flipH, improving flip + + G. Thus, improvingsequence H , improving sequence G + + .[(2)(1)] Let us assume + G + , let us consider improving sequence minimum length+ + . minimality, internal element sequence + . Thus, internalelement equals either (as improving flip leads + ). Since improving flip+ requires = , outcomes sequence form + . droppingoutcome sequence, get improving flipping sequence H.Thus, H .Lemma 6 Let H consistent let different V -outcomes. Then, + G +H .Proof: Suppose exists improving sequence + itself. must outcomesequence form (otherwise, dropping every outcome yields improvingsequence H, contradicting consistency H). perform improving flipneed hold, implies + appears sequence. Thus, + G + .Lemma 5, H .Conversely, let us assume H . Lemma 5, + G + . Lemma 4, + G + .Thus, + G + .next construction similar. Let H GCP-net variables V = {x1 , . . . , xn }, letoutcome. define GCP-net F = 2 (H, ) follows. before, set W = V {y}set variables F. define conditions flips variables xi , = 1, . . . , n,follows:+1. p+G (xi ) = pH (xi )2. pG (xi ) = pH (xi )3. p+G (y) =4. pG (y) = .Informally, outcomes form + form F copy H. improving flipsoutcomes form . improving flip + and, every 6= ,+ . particular, F consistent 2 (H, ) consistent, mapping 2 computedpolynomial time also following property.Lemma 7 Let V -outcome different . following conditions equivalent:1. H2. strictly dominates F3. dominance-incomparable F.422fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETSProof: exists improving sequence first improving flip sequence changes + . Moreover, improving flip + = .Thus, F H . Since F three conditions equivalent.Proposition 7 following problems PSPACE-complete: SELF - DOMINANCE, STRICTINANCE , DOMINANCE EQUIVALENCE , DOMINANCE INCOMPARABILITY .DOM -Proof: four problems, membership proven easily problems earlier sections.PSPACE-hardness proofs, use problem CP - DOMINANCE versionrequired input CP-net consistent two input outcomes different. problemPSPACE-hard Theorem 2.Let H consistent CP-net set V variables, let two different V -outcomes.Lemma 5, H decided deciding problem DOMINANCE EQUIVALENCE ++ GCP-net 1 (H, ). Thus, PSPACE-hardness DOMINANCE EQUIVALENCEfollows.Next, equivalence Lemma 6, + G + H , holds due consistency H,shows problem SELF - DOMINANCE PSPACE-hard.Finally, Lemma 7, H decided either deciding problem STRICT DOMI NANCE outcomes 2 (H, ), deciding complement problem DOM INANCE INCOMPARABILITY GCP-net 2 (H, ). follows STRICT DOM INANCE DOMINANCE INCOMPARABILITY (the latter fact coPSPACE=PSPACE)PSPACE-complete.8Corollary 1 problems SELF - DOMINANCE DOMINANCE EQUIVALENCE PSPACE-complete restriction CP-nets. problems STRICT DOMINANCE DOMINANCE COMPARABILITY remain PSPACE-complete restriction consistent CP-nets.Proof: Since proof Proposition 7 H CP-net, claim first twoproblems follows remarks mapping 1 preserves property CP-net.last two problems, observe since H proof Proposition 7 assumedconsistent, F = 2 (H, ) consistent, too. Thus, also locally consistent mappingF F used proof Theorem 2 applies. particular, F consistent CP-netfollowing properties (implied Lemma 3):1. strictly dominates F strictly dominates F2. dominance-incomparable F dominance-incomparable F .Since F consistent CP-net, claim last two problems follows, too.8. STRICT DOMINANCE, result could also obtained simple corollary Theorem 2, sinceconsistent GCP-nets dominance equivalent strict dominance.423fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSON7. Problems Concerning Optimality GCP-Netsdominance relation C GCP-net C determines certain order relation, gives riseseveral notions optimality. introduce study complexity correspondingdecision problems.first observe dominance equivalence relation indeed equivalence relation (reflexive, symmetric transitive). Thus, partitions set outcomes non-empty equivalence classes, call dominance classes. denote dominance class outcomeGCP-net C []C .relation C induces set dominance classes strict order relation (a relationirreflexive transitive). Namely, define []C Cdc []C []C 6= []C (equivalently, 6C )C . One check definition relation Cdc dominance classes independentchoice representatives classes.Definition 9 (Non-dominated class, optimality GCP-nets) Let C GCP-net. dominanceclass []C non-dominated maximal strict order Cdc (there dominance class[]C []C Cdc []C ). dominance class dominating every dominance class []C ,[]C = []C []C Cdc []C .outcome weakly non-dominated belongs non-dominated class. weaklynon-dominated element dominance class, non-dominated.outcome dominating belongs dominating class. outcome stronglydominating dominating non-dominated.Outcomes weakly non-dominated, non-dominated, dominating strongly dominatingcapture notions optimality. context CP-nets, weakly non-dominated nondominated outcomes proposed studied (Brafman & Dimopoulos, 2004).referred weakly strongly optimal there. Similar notions optimality also studiedearlier problem defining winners partial tournaments (Brandt, Fischer, & Harrenstein,2007). study complexity problems decide whether given outcome optimalwhether optimal outcomes exist.First, note following general properties (simple consequences properties finite strictorders).Lemma 8 Let C GCP-net.1. exist non-dominated classes so, weakly non-dominated outcomes.2. Dominating outcomes nondominated outcomes weakly non-dominated.3. strongly dominating outcome dominating non-dominated.4. following conditions equivalent:(a) C unique non-dominated class;(b) C dominating outcome;(c) weakly non-dominated dominating outcomes C coincide.consistent GCP-nets two different notions optimality remain.424fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETSLemma 9 Let C consistent GCP-net. Then:1. dominance class singleton, C strict order, C Cdc coincide (moduloone-to-one onto correspondence 7 []C )2. weakly non-dominated outcome, non-dominated (weakly non-dominatednon-dominated outcomes coincide)3. dominating outcome, strongly dominating (strongly dominating dominatingoutcomes coincide).4. Finally, unique (weakly) non-dominated outcome strongly dominating.Next, observe concepts optimality introduced different. end,show GCP-nets single non-dominated class singleton, multiple non-dominatedclasses, singleton, single non-dominated class singleton,multiple non-dominated classes, containing one element. also show GCPnet two non-dominated classes, one singleton one consisting severaloutcomes.Example 2 Consider following GCP-net C two binary variables b: >: b > bGCP-net determines strict preorder dominance classes, {ab}maximal class (in fact, dominance classes singletons). Thus, ab non-dominateddominating so, strongly dominating.Example 3 Consider following GCP-net C two binary variables bb : >b : >: b > b: b > bGCP-net determines strict preorder, {ab} {ab} two different non-dominatedclasses. Thus, ab ab non-dominated dominating outcome.Example 4 Consider GCP-net variables a, b c, defined follows:: b > b: b > bb : >b : >ab : c > c425fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSONtwo dominance classes: Sc = {abc, abc, abc, abc} Sc = {abc, abc, abc, abc}. Everyoutcome Sc strictly dominates every outcome Sc , therefore, Sc unique non-dominatedclass every outcome Sc dominating. Sc singleton, nondominated outcomes (and so, strongly dominating outcome, either).Example 5 Let us remove GCP-net Example 4 preference statement ab : c > c.Sc Sc still two dominance classes, every outcome Sc incomparableoutcome Sc . Thus, Sc Sc non-dominated. Since two non-dominatedclasses, dominating outcome. Since class one element,non-dominated outcomes. outcomes weakly non-dominated, though.Example 6 Let us modify GCP-net Example 4 changing preference statement b : >bc : > a. dominance relation GCP-net satisfies following properties: (i)four outcomes Sc dominate other; (ii) abc abc abc abc; (iii) outcome Scdominates abc (and, fortiori, abc). One check five dominance classes: Sc , {abc},{abc}, {abc} {abc}. Two non-dominated: Sc {abc}. Since two nondominated classes, dominating outcome. hand, {abc} non-dominatedoutcome (a unique one).consider following decision problems corresponding notions optimalityintroduced.Definition 10given GCP-net C:WEAKLY NON - DOMINATED OUTCOME : given outcome , decide whether weakly nondominated CNON - DOMINATED OUTCOME : given outcome , decide whether non-dominated CDOMINATING OUTCOME : given outcome , decide whether dominating CSTRONGLY DOMINATING OUTCOME : given outcome , decide whether strongly dominating CEXISTENCE NON - DOMINATED OUTCOME : decide whether C non-dominated outcomeEXISTENCE DOMINATING OUTCOME : decide whether C dominating outcomeEXISTENCE STRONGLY DOMINATING OUTCOME : decide whether C strongly dominating outcome.hardness proofs, use reductions 1 2 , describedprevious section. note following additional useful properties GCP-net G = 1 (H, ).Lemma 10 arbitrary V -outcome different , following statements equivalent:1. + G +2. + weakly non-dominated G3. + dominating outcome G.426fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETSProof: Since + dominating G (Lemma 4), weakly non-dominated outcomes dominatingoutcomes coincide (Lemma 8). follows conditions (1)-(3) equivalent other.Proposition 8 following problems PSPACE-complete: WEAKLY NON - DOMINATED OUTCOME DOMINATING OUTCOME . result holds also problems restricted CP-nets.Proof: membership easy prove techniques similar used earlier.PSPACE-hardness proofs, use reductions CP - DOMINANCE consistent CPnets (in version two input outcomes different). Let H CP-net,two different V -outcomes. Lemmas 5 10, H decided deciding eitherproblems WEAKLY NON - DOMINATED OUTCOME DOMINATING OUTCOME GCPnet G = 1 (H, ) outcome + . observed earlier, H CP-net,G = 1 (H, ). Thus, second part assertion follows.Next, consider problem STRONGLY DOMINATING OUTCOME. exploitreduction F = 2 (H, ), discussed previous section. observe followingproperty F.Lemma 11 Let H GCP-net F = 2 (H, ). strongly dominating Fdominating H.Proof: Let us assume dominating H. definition F, follows everyV -outcome 6= , + F + F + . Since + F , dominating F. Sinceimproving flip leading , strongly dominating.Conversely, let us assume strongly dominating F let V -outcome different . Let us consider improving sequence + . outcomes sequencelast one, , form + . Moreover, outcome directly preceding+ . Dropping every outcome segment sequence + + yieldsimproving sequence H.following consequence result.Proposition 9 problem STRONGLYstricted CP-nets.DOMINATING OUTCOMEPSPACE-complete, even re-Proof: Let H CP-net (over set V variables) outcome. Lemma 11, problem DOMINATING OUTCOME decided deciding problem STRONGLY DOMINATINGOUTCOME F = 2 (H, ) . Thus, PSPACE-hardness STRONGLY DOMINATINGOUTCOME follows Proposition 8. membership PSPACE is, cases, standardomitted.Since H CP-net, locally consistent so, F locally consistent, too. proofCorollary 1 use mapping GCP-net F CP-net F defined Section 4.2. Lemma3, strongly dominating outcome F dominates every outcome form, strongly dominating outcome F , since F -outcomedominated outcome form (using rules q+ (xi ) = yi q (xi ) = yi ). Therefore427fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSONF decided decidingF . Thus, second part claim follows.STRONGLY DOMINATING OUTCOMENATING OUTCOMESTRONGLY DOMIproblem NON - DOMINATED OUTCOME easier. known P CP-nets (Brafman& Dimopoulos, 2004). result extends GCP-nets. Indeed, H GCP-net outcome,non-dominated improving flip applies . latter holdsevery variable x H, x (respectively, x) holds , p (x) (respectively, p+ (x))hold . Since conditions checked polynomial claim holdsfollowing result.Proposition 10 problem NON - DOMINATEDOUTCOMEGCP-nets P.Next, consider problems concerning existence optimal outcomes. Let HGCP-net set variables V = {x1 , . . . , xn }, let two different V -outcomes.every = 1, 2, . . . , n, define formulas follows. xi , conjunctionliterals , except instead xi take xi . Similarly, xi , conjunctionliterals , except instead xi take xi . Thus, outcome resultsliteral corresponding xi flipped dual.define GCP-net E = 3 (H, , ) taking W = V {y} set variables Edefining flipping conditions follows:+1. p+E (xi ) = (pH (xi ) y) (y )pE (xi ) = pH (xi )2. p+E (y) =3. pE (y) = .GCP-net 3 (H, , ) following properties. outcomes form + (= y)form copy H. improving flip outcome (= y). Next,improving flip outcome form . see this, let us assume flipexists concerns variable, say, xi . follows = . definition flipping conditions,improving flip involves xi impossible, contradiction. Thus, improvingflip leads originates + .also every outcome , E . follows factevery outcome , improving flip. Indeed,variable xi (i) xi false , (ii) flipping literal xi dual lead(that is, ). (For even = i, then, , 6= , exists 6=differ xi , xi satisfies (i) (ii).) Thus, flip variable improving.improving flips outcomes containing result one variable xi assignedtrue, thus status , E follows.Finally, E + and, every outcome , + E . leadsfollowing property E = 3 (H, , ).Lemma 12 Let H GCP-net let two different outcomes. H3 (H, , ) (strongly) dominating outcome.428fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETSProof: (Only if) Based earlier remarks, + E . Moreover, since H ,+ E + . addition, every different , + E E E + . Thus,dominating strongly dominating (the latter follows fact improving flipslead ).(If) Let us assume dominating (and so, argument applies also stronglydominating). improving sequence + . Let us consider shortestsequence. Clearly, + outcome sequence (as pointed out, improving flip outcome form possible). Moreover, definition3 (H, , ) fact considering shortest sequence + , every outcomesequence + + form + . dropping outcomes,get improving sequence .Proposition 11 problem EXISTENCE DOMINATING OUTCOME problem EXISTENCESTRONGLY DOMINATING OUTCOME PSPACE-complete, even restricted CP-nets.Proof: show hardness part only, membership part straightforward. prove hardness notice Lemma 12, given consistent CP-net H two outcomes , Hdecided deciding either problems EXISTENCE DOMINATING OUTCOMEEXISTENCE STRONGLY DOMINATING OUTCOME 3 (H, , ). prove second partassertion, note H consistent, E = 3 (H, , ) consistent, so, mappinglocally consistent GCP nets CP-nets applies. Let us denote result applying mapping E E . Then, using argument proof Proposition 9, E (strongly)dominating outcome E strongly dominating outcome. Thus, one decidewhether H consistent CP-net H deciding either problems EXISTENCE DOM INATING OUTCOME EXISTENCE STRONGLY DOMINATING OUTCOME E .also note problem EXISTENCEstandard complexity theory assumptions).NON - DOMINATED OUTCOMEProposition 12 problem EXISTENCE NON - DOMINATEDOUTCOMEeasier (underNP-complete.Proof: note case GCP-nets conjunctive form problem known NP-hard(Domshlak et al., 2003, 2006). Thus, problem NP-hard GCP-nets. membershipclass NP follows Proposition 10.restrict consistent GCP-nets, situation simplifies. First, recall (Lemma 9)GCP-net consistent weakly non-dominated non-dominated outcomes coincide,true dominating strongly dominating outcomes. Moreover, consistent GCP-nets,non-dominated outcomes exist (and so, corresponding decision problem trivially P). Thus,consistent GCP-nets consider problems DOMINATING OUTCOME EXISTENCEDOMINATING OUTCOME .Proposition 13 problems DOMINATING OUTCOMECOME restricted consistent GCP-nets coNP.429EXISTENCE DOMINATING OUT-fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSONProof: Using Lemmas 8 9, dominating outcome exists outcome6= non-dominated. Similarly, dominating outcome consistent GCP-netleast two non-dominated outcomes. Thus, guessing non-deterministicallyoutcome 6= , verifying non-dominated, non-deterministic polynomial-timealgorithm deciding complement problem DOMINATING OUTCOME. argumentproblem similar.know bounds Proposition 13 tight, is, whether two problemscoNP-complete. conjecture are.8. Concluding Remarksshown dominance consistency testing CP-nets PSPACE-complete. Alsoseveral related problems related dominance optimality CP-nets PSPACE-complete, too.repeated use reductions planning problems confirms importance structural similarity STRIPS planning reasoning CP-nets. suggests welldeveloped field planning algorithms STRIPS representations, especially unary operators(Brafman & Domshlak, 2003), could useful implementing algorithms dominanceconsistency CP-nets.theorems extend CP-nets non-binary domains, extensions variationsCP-nets, TCP-nets (Brafman & Domshlak, 2002; Brafman, Domshlak, & Shimony, 2006)allow explicit priority variables others, general languageconditional preferences (Wilson, 2004a, 2004b), conditional preference rules writtenconjunctive form.complexity result dominance also relevant following constrained optimisationproblem: given CP-net constraint satisfaction problem (CSP), find optimal solution (asolution CSP dominated solution CSP). computationally complex, intuitively complete algorithm involves many dominance checksdefinition dominance constraints allows dominance paths go outcomesviolating constraints (Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004b).9 problemchecking whether given solution CSP non-dominated seen PSPACE-completereduction CP-dominance uses CSP exactly two solutions.results reinforce need work finding special classes problems dominanceconsistency tested efficiently (Domshlak & Brafman, 2002; Boutilier et al., 2004a),incomplete methods checking consistency constrained optimisation (Wilson, 2004a,2006).Several open problems remain. know complexity deciding whether preference relation induced CP-net complete. know whether dominance consistencytesting remain PSPACE-complete number parents dependency graph boundedconstant. also know whether two problems remain PSPACE-completeCP-nets conjunctive form (the reduction used prove Theorems 2 4 yields CP-netsconjunctive form). Two additional open problems listed end Section 7.9. another possible definition, going outcomes violating constraints allowed (Prestwich,Rossi, Venable, & Walsh, 2005), dominance testing needed check whether given solution non-dominated.430fiT C OMPUTATIONAL C OMPLEXITY OMINANCE C ONSISTENCY CP-N ETSAcknowledgmentsJrme Langs new address is: LAMSADE, Universit Paris-Dauphine, 75775 Paris Cedex 16,France. authors grateful reviewers excellent comments, Pierre Marquishelpful discussions. work supported part NSF Grants ITR-0325063,IIS-0097278 KSEF-1036-RDE-008, ANR Project ANR05BLAN0384 PreferenceHandling Aggregation Combinatorial Domains, Science Foundation Ireland GrantsNo. 00/PI.1/C075 05/IN/I886, Enterprise Ireland Ulysses travel grant FR/2006/36.ReferencesAusiello, G., Crescenzi, P., Gambosi, G., Kann, V., Marchetti-Spaccamela, A., & Protasi, M. (1999).Complexity Approximation. Combinatorial optimization problems approximability properties. Springer Verlag.Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004a). CP-nets: toolrepresenting reasoning conditional ceteris paribus statements. Journal ArtificialIntelligence Research, 21, 135191.Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004b). Preference-based constrained optimization CP-nets. Computational Intelligence, 20(2), 137157.Boutilier, C., Brafman, R., Hoos, H., & Poole, D. (1999). Reasoning conditional ceteris paribuspreference statements. Proceedings UAI-99, pp. 7180.Brafman, R., Domshlak, C., & Shimony, E. (2006). graphical modeling preferenceimportance. Journal Artificial Intelligence Research, 25, 389424.Brafman, R., & Dimopoulos, Y. (2004). Extended semantics optimization algorithms CPnetworks. Computational Intelligence, 20(2), 218245.Brafman, R., & Domshlak, C. (2002). Introducing variable importance trade-offs CP-nets.Proceedings UAI-02, pp. 6976.Brafman, R., & Domshlak, C. (2003). Structure complexity planning unary operators.Journal Artificial Intelligence Research, 18, 315439.Brandt, F., Fischer, F., & Harrenstein, P. (2007). computational complexity choice sets.Proceedings TARK-07, pp. 8291.Bylander, T. (1994). computational complexity propositional STRIPS planning. ArtificialIntelligence, 69(12), 165204.Domshlak, C., & Brafman, R. (2002). CP-netsreasoning consistency testing. ProceedingsKR-02, pp. 121132.Domshlak, C., Prestwich, S., Rossi, F., Venable, K., & Walsh, T. (2006). Hard soft constraintsreasoning qualitative conditional preferences. Journal Heuristics, 12(4/5), 263285.Domshlak, C., Rossi, F., Venable, K., & Walsh, T. (2003). Reasoning soft constraintsconditional preferences: complexity results approximation techniques. ProceedingsIJCAI-03, pp. 215220.431fiG OLDSMITH , L ANG , RUSZCZY NSKI & W ILSONGoldsmith, J., Lang, J., Truszczynski, M., & Wilson, N. (2005). computational complexitydominance consistency CP-nets. Proceedings IJCAI-05, pp. 144149.Lang, J. (2004). Logical preference representation combinatorial vote. Annals MathematicsArtificial Intelligence, 42(1), 3771.Papadimitriou, C. (1994). Computational complexity. Addison-Wesley.Prestwich, S., Rossi, F., Venable, B., & Walsh, T. (2005). Constraint-based preferential optimization.Proceedings AAAI-05, pp. 461466.Wilson, N. (2004a). Consistency constrained optimisation conditional preferences.Proceedings ECAI-04, pp. 888892.Wilson, N. (2004b). Extending CP-nets stronger conditional preference statements. Proceedings AAAI-04, pp. 735741.Wilson, N. (2006). efficient upper approximation conditional preference. ProceedingsECAI-06, pp. 472476.Xia, L., Conitzer, V., & Lang, J. (2008). Voting multiattribute domains cyclic preferentialdependencies. Proceedings AAAI-08, pp. 202207.432fiJournal Artificial Intelligence Research 33 (2008) 551574Submitted 09/08; published 12/08Learning Reach Agreement Continuous Ultimatum GameSteven de JongSimon UyttendaeleSTEVEN . DEJONG @ MICC . UNIMAAS . NLMICC, Maastricht UniversityP.O. Box 616, 6200 MD Maastricht, NetherlandsKarl TuylsK . P. TUYLS @ TUE . NLEindhoven University TechnologyP.O. Box 513, 5600 MB Eindhoven, NetherlandsAbstractwell-known acting individually rational manner, according principles classical game theory, may lead sub-optimal solutions class problems named social dilemmas.contrast, humans generally much difficulty social dilemmas, ablebalance personal benefit group benefit. agents multi-agent systems regularly confronted social dilemmas, instance tasks resource allocation, agents maybenefit inclusion mechanisms thought facilitate human fairness. Although manymechanisms already implemented multi-agent systems context, application usually limited rather abstract social dilemmas discrete set available strategies(usually two). Given many real-world examples social dilemmas actually continuousnature, extend previous work general dilemmas, agents operate continuous strategy space. social dilemma study well-known Ultimatum Game,optimal solution achieved agents agree common strategy. investigate whetherscale-free interaction network facilitates agents reach agreement, especially presencefixed-strategy agents represent desired (e.g. human) outcome. Moreover, study influence rewiring interaction network. agents equipped continuous-actionlearning automata play large number random pairwise games order establish common strategy. experiments, may conclude results obtained discrete-strategygames generalized continuous-strategy games certain extent: scale-free interaction network structure allows agents achieve agreement common strategy, rewiringinteraction network greatly enhances agents ability reach agreement. However, alsobecomes clear alternative mechanisms, reputation volunteering, manysubtleties involved convincing beneficial effects continuous case.1. IntroductionSharing limited resources others common challenge individuals human societieswell agents multi-agent systems (Chevaleyre et al., 2006). Often, conflictinterest personal benefit group (or social) benefit. conflict prominentlypresent class problems named social dilemmas, individuals need considerpersonal benefit, also effects choices others, failure maylead sub-optimal solutions. dilemmas, classical game theory, assumes playersagents completely individually rational strategic circumstances, seems limited value(Gintis, 2001; Maynard-Smith, 1982), individually rational players socially conditioned.Humans hand generally show remarkable ability address social dilemmas, duec2008AI Access Foundation. rights reserved.fiD E J ONG , U YTTENDAELE & UYLStendency consider concepts fairness addition personal benefit (see, e.g.Dannenberg et al., 2007; Fehr & Schmidt, 1999; Gintis, 2001; Oosterbeek et al., 2004).prime example social dilemma modeled well-known Ultimatum Game (Guethet al., 1982).1 game, two agents bargain division amount R, obtainedoutsider. first agent proposes offer r2 second agent (e.g. receive $4$10). second agent accepts, agent gets share (i.e. first agent receives R r2 ,second receives r2 ); however, second agent rejects, agents left nothing.individually rational first agent would offer smallest amount possible, knowing secondagent choose obtaining amount accepting, nothing rejecting. Thus,accepting smallest amount possible individually rational response. contrast, humanplayers game hardly (if ever) offer less 20%, offer occurs,likely accepted (Bearden, 2001; Oosterbeek et al., 2004). Thus, individually rational playerplays proposer human player probably gain money.Researchers proposed various mechanisms may responsible emergencefair strategies human populations playing social dilemmas, well resistancestrategies invasion individually rational strategies (see, e.g. Fehr & Schmidt, 1999; Gintis, 2001; Nowak et al., 2000; Santos et al., 2006a). Often, mechanisms implemented multi-agent systems validation purposes, i.e. agents shown preferfair strategies individually rational ones, makes plausible humans actuallyaffected underlying mechanisms. However, argue multi-agent systems driven fairness mechanisms may used validate mechanisms, also allow agents actfair way real-world applications. Given agents often face tasks resource sharingallocation (if explicitly, implicitly, e.g. sharing limited computational resources),tasks regularly contain elements social dilemmas, important enable agentsact based individual rationality, also based fairness (Chevaleyre et al., 2006).Unfortunately, existing work usually introduces number abstractions allow resulting multi-agent systems applied realistic problems resource allocation.prominently, work focused social dilemmas discrete strategy sets (usually limitedtwo).2 abstraction simplifies dilemmas hand reflect potential realworld nature, since many dilemmas, especially related real-world resource allocation,continuum strategies (i.e. continuous strategy space) rather discrete setpure strategies. Moreover, social dilemmas continuous strategy spaces, qualificationscooperation defection, often used discrete social dilemmas, actually relative: certain strategy may seen cooperative (i.e. desirable) certain context, whereasmay either defective simply naive another one. clear dilemma may farcomplicated continuous strategy spaces, agents may need use different waydetermining whether behavior desirable.1. analogy Ultimatum Game social dilemmas, Public Goods Game, shownfull mathematical rigor (Sigmund et al., 2001). De Jong & Tuyls (2008) report preliminary results applyingmethodology described paper Public Goods Game.2. Theoretical work field evolutionary game theory occasionally limited discrete strategy sets. Worthmentioning work Peters (2000), introduces theoretical extension evolutionary stablestrategy concept (ESS) continuous strategy spaces, i.e. extended stability calculus. extension providestheoretical solution concept clarify egalitarian outcomes. However, concept shed lightlearning agents possibly achieve fair outcomes social dilemmas. work therefore complementarywork, aims mechanisms enabling agents find fair outcomes.552fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AMEpaper, generalize existing work achieving agreement, cooperation fairnesssocial dilemmas continuous strategy spaces, aim presenting methodologyallows agents reach satisfactory outcomes dilemmas, well real-world problemscontaining elements dilemmas. apply proposed methodology UltimatumGame, example social dilemma. game, population agents needs reach agreement (i.e. converge common strategy) order obtain satisfactory payoff. agreementspecifies populations cooperative, desirable strategy. precise nature agreementmay vary.3 population agents, strategy agreed upon sufficient numberagents successful, dictate culture, desirable strategy, population.wish agents learn human strategy, may introduce number simulated humanagents, i.e. agents play according desirable strategy. learning agentsable imitate strategy, even already reached agreement different, possiblyrelatively defective, strategy.remainder paper structured follows. First, 2, give brief overviewrelated work paper aims continue. Next, 3, discuss methodology, aimedestablishing agreement large populations learning agents. 4, present experimentsresults. 5, outline number alternative approaches proposed dilemmasdiscrete strategy sets, fail impress dilemma continuous strategy space. discusscase. Finally, conclude paper 6.2. Related Workwork basically builds upon two tracks existing work. give overviewtracks indicate related current work. extensive discussion,refer previous work (De Jong et al., 2008b).2.1 Learning Fairness BargainingDe Jong et al. (2008a) investigated behavior agents playing Ultimatum GameNash Bargaining Game continuous action learning automata. games, agentsinteracting time. Ultimatum Game, required extension twoplayers agents, one other, demanded portion reward hand. lastplayer received left. Homo Egualis utility function, developed Fehr & Schmidt(1999) Gintis (2001), used represent desired outcome, i.e. required minimal amountevery agent wished obtain. 100 agents able successfully find maintain agreements games. addition, observed solutions agreed upon correspondedsolutions agreed upon humans, reported literature. work, similarly use continuous action learning automata learn agreement Ultimatum Game. However, multi-agentsystem, organized network structure, efficiently populated much larger numberagents (e.g. thousands). contrast previous work, agents play pairwise games. Moreover,3. Note analogy humans, cultural background one primary influences constitutes fair,cooperative, desirable strategy. Although general tendency deviate pure individual rationalityfavor socially-aware strategies, exact implications vary greatly (Henrich et al., 2004; Oosterbeek et al.,2004; Roth et al., 1991). Ultimatum Game instance, actual amounts offered minimally acceptedvary 10% much 70%, depending various factors, amount bargain (Cameron,1999; De Jong et al., 2008c; Slonim & Roth, 1998; Sonnegard, 1996), culture (Henrich et al., 2004). Culturaldifferences may persist groups agents, also within groups (Axelrod, 1997).553fiD E J ONG , U YTTENDAELE & UYLSuse Homo Egualis utility function. Instead, desired, human-inspired outcomeoffered Homo Egualis utility function replaced (potentially) including agentsalways play according certain fixed strategy (i.e. simulated human players).2.2 Network TopologySantos et al. (2006b) investigated impact scale-free networks resulting strategies socialdilemmas. scale-free network used order randomly determine two agents (neighbors) would play together various social dilemmas, Prisoners DilemmaSnowdrift Game. agents limited two strategies, i.e., cooperate defect,initially equally probable. observed that, due scale-free network, defectors couldspread entire network games, network structures. authorsidentified topology network contributed observed maintained cooperation.subsequent research, Santos et al. (2006a) introduced rewiring network played many different social dilemmas, two strategies per agent. concluded ease(measure individuals inertia readjust ties) rewiring increasing ratecooperators efficiently wipe defectors. contrast work Santos et al. (2006a,b),used discrete strategy set, work uses continuous strategy space. requires another viewfairness, cooperation agreement, departing traditional view fairness achieveddriving agents (manually labeled) cooperative strategies. social dilemmasUltimatum Game, strategy agents may agree leads satisfactory outcomes.3. Methodologydiscussing methodology detail, first outline basic setting. continueexplaining continuous-action learning automata, central methodology. Next,discuss structure topology networks interaction use. discussagent types initial strategies agents. elaborate provide additionalpossibility rewiring connections agents. Finally, explain experimental setup.3.1 Basic Settingstudy large group adaptive agents, driven continuous action learning automata, playingUltimatum Game pairwise interactions. Pairs chosen according (scale-free) networkinteraction. Every agent randomly assigned role proposer responder UltimatumGame. Agents start different strategies. good performance, need convergeagreement playing many pairwise games, i.e. need learn common strategy.agents may fixed strategies; agents represent external strategy adaptiveagents need converge to, instance preference dictated humans. additionbasic setting, study influence adding option agents rewire networkinteraction response agent behaved defecting manner.3.2 Continuous Action Learning AutomataContinuous Action Learning Automata (CALA; Thathachar & Sastry, 2004) learning automatadeveloped problems continuous action spaces. CALA essentially function optimizers:every action continuous, one-dimensional action space A, receive feedback554fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME(x) goal optimize feedback. CALA proven convergence (local) optima,given feedback function (x) sufficiently smooth. advantage CALA manyreinforcement learning techniques (see, e.g. Sutton & Barto, 1998), necessarydiscretize continuous action spaces, actions simply real numbers.3.2.1 H OW CALA W ORKEssentially, CALA maintain Gaussian distribution actions pulled. contraststandard learning automata, CALA require feedback two actions, action correspondingmean Gaussian distribution, action corresponding sample x, takendistribution. actions lead feedback () (x), respectively, turn,feedback used update probability distributions . precisely, update formulaCALA written as:x= + (x)()()()2(x)()x= + ()1 K ( L )()(1)equation, represents learning rate; K represents large constant driving .variance kept threshold L keep calculations tractable even case convergence.4implemented using function:() = max (, L )(2)intuition behind update formula quite straightforward (De Jong et al., 2008a). Usingupdate formula, CALA rather quickly converge (local) optimum. multiple (e.g. n) learningautomata, every automaton receives feedback respect joint actions, respectively ()(x), = 1 , . . . , n x = x1 , . . . , xn . case, still convergence(local) optimum (Thathachar & Sastry, 2004).3.2.2 ODIFICATIONS CALA P URPOSESoutlined above, use CALA enable agents learn sensible proposer responder strategy Ultimatum Game. playing Ultimatum Game, two agents may agreeone two joint actions (i.e. obtain one high one low feedback),may even disagree (i.e. obtain two low feedbacks). situations needadditional attention, occurrence prevents CALA converging correct solutions.address situations, propose two domain-specific modifications update formulaCALA (De Jong et al., 2008a).5First, case joint actions yield feedback 0, CALA unable draw effectiveconclusions, even though may tried ineffective strategy thus actually4. used following settings initial experiments: = 0.02, K = 0.001 L = 107 .precise settings L decisive influence outcomes, although values may lead slowerconvergence. K chosen large, (rather vaguely) implied Thathachar & Sastry (2004), decreasesfast, i.e. usually CALA stop exploring sufficient solution found.5. Note modifications uncommon literature; see, e.g. work Selten & Stoecker (1986)learning direction theory. Grosskopf (2003) successfully applied directional learning setting UltimatumGame, focusing responder competition (which addressed paper).555fiD E J ONG , U YTTENDAELE & UYLSlearn. order counter problem, introduce driving force, allows agents updatestrategy even feedback received 0. driving force defined as:proposers: () = xiff () = (x) = 0(3)responders: () = xeffect modification, call zero-feedback avoidance (ZFA), agentplaying proposer learn offer more, agent playing responder acceptlower expectation. roles, lead probable agreement.Second, one joint action yields agreement, feedback 0, CALA may adaptstrategies drastically favor first joint action fact, shifts values greater109 observed (De Jong & Tuyls, 2008; De Jong et al., 2008a). tackle problem,restrict difference possible two feedbacks CALA receive everyiteration. precisely, empirically set:fififi () (x) fififi1(4)fifi()Thus, large difference feedback -action x-action, preservedirection indicated feedback, prevent automaton jump far direction.call modification strategy update limitation (SUL).3.3 Network Interactionscale-free network (Barabasi & Albert, 1999) network degree distribution followspower law. precisely, fraction P (k) nodes network k connectionsnodes goes large values k P (k) k . value constant typicallyrange 2 < < 3. Scale-free networks noteworthy many empirically observed networksappear scale-free, including world wide web, protein networks, citation networks, alsosocial networks. mechanism preferential attachment proposed explain power lawdegree distributions networks. Preferential attachment implies nodes prefer attachingnodes already large number neighbors, nodes smallnumber neighbors.Previous research indicated scale-free networks contribute emergence cooperation (Santos et al., 2006b). wish determine whether phenomenon still occurscontinuous strategy spaces therefore use scale-free topology interaction network, using Barabasi-Albert model. precisely, probability pi newly introduced nodeconnected existing node degree ki equal to:kipi = P(5)j kjconstruct network, two first nodes linked other,nodes introduced sequentially connected one existing nodes, using pi .way, newly introduced node probably connect heavily linked hub oneconnections. simulations, connect every new node one, two threeexisting ones (uniform probabilities). yields networks interaction realisticacyclic ones obtained always connecting new nodes one existing node. example,network modeling friendship network, avoiding cycles means assuming friendscertain person never friends other.556fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AME3.4 Agent Types Strategiesorder study agreement concerning common strategy emerges, need makeagents learn reach common strategy, starting situation absent (i.e.agents different strategies). Moreover, need study whether common strategyestablished example agents, whether robust agents use different,potentially relatively defective strategy.3.4.1 WO YPES AGENTSintroduce two types agents, i.e. dynamic strategy (DS) agents fixed strategy (FS) agents.DS agents learning agents. start certain predefined strategy allowedadapt strategy constantly, according learning mechanism learning automaton.Basically, agents similar used earlier work (De Jong et al., 2008a). FS agents(optional) good examples: model example strategy needs learned(other) agents system, therefore refuse adapt strategy.3.4.2 NE WO CALA PER AGENT ?outlined above, agent needs able perform two different roles Ultimatum Game, i.e. playing proposer well playing responder. words,agent one two distinct states, state requires learn different strategy. CALAstateless learners, agent therefore would require two CALA. Nonetheless, remainderpaper, equip every DS agent one CALA, representing agents proposerstrategy well responder strategy.choice one CALA motivated two observations, i.e. (1) human behavior, (2)initial experiments. First, human strategies often consistent, implying generallyaccept offers, reject offers lower (Oosterbeek et al., 2004), even highamounts stake (De Jong et al., 2008c; Sonnegard, 1996). Second, set initial experiments,observed agents using two CALA generally converge one single strategy anyway.illustration, three learning curves obtained fully connected network three agents playingUltimatum Game displayed Figure 1. clearly visible agents proposer strategies(bold lines) strongly attracted agents responder strategies (thin lines), especiallylowest responder strategies. presence FS agent offers 4.5 acceptsleast 1, first strategy immediately ignored favor (lower) second one. DSagents, strategies attracted lowest responder strategy present.6future work, study observation perspective evolutionary game theoryreplicator equations (Gintis, 2001; Maynard-Smith & Price, 1973). current paper,use observation justify abstraction, i.e. limit complexity agents equippingone CALA. CALA represents agents proposer strategy wellresponder strategy. updated agent plays proposer well playsresponder, according CALA update formula presented 3.2.1 modificationspresented 3.2.2. Thus, agents single CALA receive twice much feedback two separateCALA would. abstraction therefore increases efficiency learning process.6. agents quickly adapt strategies downward upward (Figure 1). Therefore, multiple (e.g. 10)DS agents learning (i.e. without FS agents), strategy usually converges 0. due artifactlearning process; two CALA trying learn others current strategy tend driven downward.557fi554.54.5443.53.53StrategyStrategyE J ONG , U YTTENDAELE & UYLS2.521.521.5110.50.500500 1000 1500 2000 2500 3000 3500 4000 4500Iteration500 1000 1500 2000 2500 3000 3500 4000 4500Iteration10(top-left) Two DS agents, one starting offering accepting 4.5, one starting offering accepting 0.01, learnplay optimally FS agent offering4.5 accepting 1. DS agents rather quickly learnoffer accept 1.987Strategy32.56(top-right) Two DS agents, starting offering 4.5accepting 1, learn play optimallyFS agent also offering 4.5 accepting 1. DSagents learn offer accept 1.543(bottom-left) Three DS agents, starting different initialstrategies (i.e. offering 9, 4.5, 1, accepting 3, 21, respectively), quickly learn single, similar strategy.210500 1000 1500 2000 2500 3000 3500 4000 4500IterationFigure 1: Evolving strategies fully connected network three agents. Proposal strategiesindicated bold line, response strategies indicated thin line. Agentsconverge situation two initial strategies become similar.3.4.3 AGENTS TRATEGIESsimulations, use two types DS agents one type FS agents. precisely,DSr agents learning agents start rational solution offering X N (0.01, 1) (andalso accepting amount more). DSh agents start human, fair solution, i.e.offering X N (4.5, 1) (and also accepting amount more). Since FS agentsexamples desired solution, equip fair, human-inspired solution see whetheragents able adapt solution. FS agents always offer 4.5, acceptoffer 4.5 more. agents limited strategies taken continuous intervalc = [0, 10], 10 chosen upper bound (instead common 1)common amount money needs shared Ultimatum Game. agents strategyfalls outside interval c, round strategy nearest value within interval.3.5 RewiringAgents play together based connections interaction network. Thus, order avoidplaying certain undesirable neighbor j, agent may decide break connection558fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AMEj create new link random neighbor j (Santos et al., 2006a).7 rewiring,use heuristic proposed Santos et al.: agents want disconnect (relative)defectors, prefer play relative cooperators. Thus, probability agent unwiresagent j, calculated as:si sjpr =(6)CHere, si sj agents current strategies (more precisely, agent responder strategyagent js proposer strategy), C amount stake Ultimatum Game, i.e. 10. Evenagents determine want unwire probability, may still allowedto, breaks last link one them. unwiring takes place, agent creates new wirerandom neighbor agent j.3.6 Experimental SetupUsing aforementioned types agents, need determine whether proposed methodologypossesses traits would like see. population said establishedsuccessful agreement manages reach common strategy incorporates preferencesgood examples, time discouraging agents try exploit dominantstrategy. Thus, population consisting DS agents, strategy shared (orall) agents leads good performance, since agents agree games, yielding average payoff5 per game per agent architecture able find common strategy.using DS well FS agents, FS agents dictate strategy DS agents convergeto, regardless whether start DSh DSr agents.order measure whether agents achieved satisfactory outcome, study four quantitiesrelated learning process final outcome, viz. (1) point convergence, (2)learned strategy, (3) performance (4) resulting network structure. briefly explainfour quantities below. general, remark every simulation lasts 3, 000 iterationsper agent, i.e. 3, 000n iterations n agents. repeat every simulation 50 times obtain reliableestimates quantities interest.3.6.1 P OINT C ONVERGENCEimportant quantity concerning agents learning process point convergence,which, present, tells us many games agents needed play order establish agreement. determine point convergence, calculate save average population strategyavg(t) pairwise game (i.e. iteration learning process). iterations,obtain ordered set averages, i.e. {avg(1), . . . , avg(T )}. Initially, average populationstrategy changes time, agents learning. certain point time t, agents stoplearning, result, average population strategy avg(t) change much anymore.estimate point t, i.e. point convergence, find lowest standarddeviation subset {avg(t), . . . , avg(T )} 103 . Subsequently, report numbergames per agent played iteration t, i.e. nt . experiments, every simulation repeated7. Note may also choose allow agent create new connection specific agents insteadrandom neighbors neighbor j. However, especially combination reputation (see 5.1), allows(relative) defectors quickly identify (relative) cooperators, may connectattempt exploit. Preliminary experiments shown behavior leads interaction network losingscale-freeness, may seriously impair emergence agreement.559fiD E J ONG , U YTTENDAELE & UYLSAvgStdConv000045000004035000030000250000020015000050Iteration10Strategy10987654321000000AvgStdConvPopulation strategy45000004035000030000250000020000001510500StrategyPopulation strategy109876543210IterationFigure 2: Two examples convergence point single run. graphs, displayaverage strategy population (bold line) well standard deviationaverage (thin line). dotted vertical line denotes convergence point, foundanalysis detailed text.50 times, resulting 50 convergence points. use box plot visualize distribution50 convergence points.8example, Figure 2 (left), see 17 FS agents, 17 DSh agents 16 DSr agentsconverge agreement, using rewiring. first 50, 000 games shown. additionbold line denoting average population strategy, also plot thinner line, denoting standarddeviation average. Using method outlined above, point convergence determinedaround 27, 500 games, i.e. approximately 550 games per agent necessary. Figure2 (right), show similar results 10 FS agents 40 DSr agents, using rewiring.Here, point convergence around 34, 000 games, i.e. approximately 680 games per agentnecessary, means learning reach agreement difficult.3.6.2 L EARNED TRATEGYestablished iteration agents converged, state averagelearned strategy precisely avg(t). repeat every simulation 50 times obtain reliable estimate average. again, results, use box plot visualize distributionaverage learned strategy.3.6.3 P ERFORMANCEmeasure performance, first allow agents learn playing 3, 000 Ultimatum Gameseach. Then, fix strategies DS agents. let every agent play proposerneighbors (one one), count number games successful.9 divide8. box plots, report average instead median, average informative quantity, e.g.comparing results existing work. may result box plots mid point located outside box.9. Note CALA update formula prevents agents converging exact strategy, standard deviationCALAs Gaussian kept artificially strictly positive. Therefore, noise strategies agentsconverged to. counter noise measuring performance, set responders strategies 99%actual strategies. Thus, agent strategy 4 propose 4 accept offer 3.96 more.560fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AMEnumber total number games played (i.e. twice number edges interactionnetwork). resulting number denotes performance, lies 0 (for utterly catastrophic) 1 (for complete agreement). Human players Ultimatum Game typically achieveperformance 0.80.9 (Fehr & Schmidt, 1999; Oosterbeek et al., 2004). again, 50repetitions lead 50 measures performance, displayed box plot results.3.6.4 R ESULTING N ETWORK TRUCTURESince network interaction may rewired agents satisfied neighbors, interested network structure resulting agents learning processes.examine network structure looking degree distribution nodes network (i.e.number neighbors agents). 50 repeated simulations, may draw single boxplot expressing average degree distribution.4. Experiments Resultspresent experiments results two subsections. First, study setup without rewiringsetup rewiring, varying number agents, keeping proportion DSh, DSrFS agents constant equal (i.e. 33% type agent). Second, studytwo setups various population sizes, time varying proportion FS agents,remainder population half DSh half DSr. general, remark every experimentreports results averaged 50 simulations. every simulation, allow agentsplay 3, 000n random games, n denotes number agents (i.e. population size).4.1 Varying Population Sizemany multi-agent systems, increasing number agents (i.e. population size) causesdifficulties. Many mechanisms work relatively low number agents stop workingwell high number agents, instance due computational complexity undesiredemergent properties. According previous research, issue scalability also appliestask learning social dilemmas. Indeed, previous research using evolutionary algorithms gamesdiscrete strategy sets mentions number games needed converge agreement(i.e. cooperation) may prohibitively large (Santos et al., 2006a).10Since agents learning continuous strategy spaces, may expect scalability issuewell. determine whether proposed methodology issue, vary populationsize 10 10, 000 (with steps between), keeping proportion FS,DSh DSr agents constant one-third each. study setup without rewiring wellsetup rewiring, determine (1) point convergence, i.e. number games per agentneeded reach convergence; (2) average learned strategy agents converged to; (3) finalperformance system; finally (4) resulting network structure. Especially firstthird quantities give indication scalability methodology.10. order limit time taken learning, Santos et al. (2006a) terminate learning process 108 iterations,using 103 agents, leading average (more than) 105 games per agent available. Still,high number games per agent, report agents occasionally converge.561fiD E J ONG , U YTTENDAELE & UYLSW ITH REWIRING300025002500Games per agentGames per agentN REWIRING3000200015001000200015001000500500001050100 200 500Number agents101000 1000050100 200 500 1000 10000Number agentsFigure 3: Games per agent convergence; without rewiring (left) rewiring (right).4.1.1 P OINT C ONVERGENCEsetup without rewiring (Figure 3, left) tends require games per agent total numberagents increases. certain point, i.e. around population size 200 agents, tendencystops, mainly average number games per agent approaches maximum, i.e. 3, 000games per agent. setup rewiring (same figure, right) convincingly outperforms one withoutrewiring, increasing population size hardly affects number games per agent requiredreach convergence. Independent population size, setup requires approximately 500games per agent converge. Note difference previous research (i.e. Santos et al., 2006a),reports requiring 105 games per agent (or more).4.1.2 L EARNED TRATEGYsetup without rewiring (Figure 4, left) average converges strategy offering wellaccepting around 3, 4.5 would required, 33% FS agents present populationplay strategy (i.e. 66% DS agents average strategy 2). increasingpopulation size, average strategy affected; however, becomescertainty established. again, setup rewiring (same figure, right) shows convincinglybetter results. Independent population size, learning agents converge desiredstrategy, i.e. 4.5.4.1.3 P ERFORMANCEsetup without rewiring (Figure 5, left), already saw average learned strategyDS agents good. Performance seriously affected; around 60%, indicatesDS agents ever agree FS agents. However, average performance influencedpopulation size. learned strategy, performance around 60% becomescertainly established. expected, setup rewiring (same figure, right) shows muchsatisfying results, i.e. generally 80% agreement. results actually positively affectedpopulation size, average performance increases increasing population.562fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AMEN REWIRINGW ITH REWIRING5Average converged strategyAverage converged strategy543214321001050100200500Number agents1000101000050100 200 500Number agents1000 10000Figure 4: Average learned strategy; without rewiring (left) rewiring (right).W ITH REWIRING10.90.80.70.60.50.40.30.20.10Performance convergencePerformance convergenceN REWIRING1050100200500Number agents10.90.80.70.60.50.40.30.20.10101000 1000050100 200 500Number agents1000 10000Figure 5: Final performance; without rewiring (left) rewiring (right).4.1.4 R ESULTING N ETWORK TRUCTURElook network structure resulting learning reach agreement, determine whetherstructure influenced population size. Obviously, setup without rewiring (Figure 6,left) display influence here, network static. setup rewiring (samefigure, right) shows interesting tendency. average degree resulting network stays low,maximum degree increases increasing population size. Clearly, populationsize increases, hubs scale-free network receive preferential attachment,correspondingly, less densely connected nodes become even less densely connected.examine number times agents actually rewire, find number generally lies1, 000, i.e. low percentage total number games played actually made agentsrewire random neighbor undesired proposer.563fiD E J ONG , U YTTENDAELE & UYLSW ITH REWIRING150120120Degree distributionDegree distributionN REWIRING150906030906030001050100 200 500Number agents101000 1000050100 200 500Number agents1000 10000Figure 6: Resulting network structure; without rewiring (left) rewiring (right).4.1.5 N C ONCLUSIONconclusion subsection, may state proposed methodology sufferingsevere scalability issues. setup include rewiring clearly outperformed oneinclude rewiring, neither setup without rewiring, setup rewiring, sufferseverely increasing number agents.4.2 Varying Proportion Good Examples (FS Agents)section, investigate behavior proposed methodology proportiongood examples population (i.e. FS agents strategy 4.5) varied. remainderpopulation consists DSh DSr agents equal proportions. experimentednumber population sizes, ranging 50 500.Since results population size rather similar, restrict graphicallyreporting analyzing results experiments 100 agents remaindersection. selection remaining results given Table 1. Specifically, setup without rewiring setup rewiring, report population size (Pop), percentage FSagents used (%FS), average number games per agent needed converge (Games), average learned strategy (Strat), average performance (Perf), finally, maximum numberconnections single agent agents network (Netw). discussbelow, results reported Table 1 population sizes 100 highly similarpopulation size 100 agents.4.2.1 P OINT C ONVERGENCEsetup without rewiring (Figure 7, left) requires games per agent converge,proportion FS agents reaches around 30%. Then, required number games decreasesagain, although great deal uncertainty. Introducing rewiring (same figure, right) yieldsmuch better results. number games required per agent hardly exceeds 700, numberdecreases steadily increasing proportion population FS agent.564fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AMEN REWIRINGPop% FS50W ITH REWIRINGGamesStratPerfNetwPop% FS0305080663.802,588.501,800.02259.860.012.874.124.470.630.590.700.87151516155003050802000305080671.302,796.851,354.80288.350.012.644.174.470.630.570.700.88181718182005000305080662.502,793.551,237.75264.600.012.854.184.470.640.590.690.8920202121500GamesStratPerfNetw639.38528.52485.60356.340.014.454.474.490.630.810.890.96223829230305080743.00540.40493.40382.200.014.454.474.490.620.870.910.97205228240305080650.20549.95498.00380.910.014.454.474.490.650.870.920.97601005535Table 1: Summary results experiments proportion FS agents varied.details additional results, see main text.W ITH REWIRING3000300025002500Games per agentGames per agentN REWIRING2000150010002000150010005005000000 10 20 30 40 50 60 70 80 90 100Percentage FS agents10 20 30 40 50 60 70 80 90 100Percentage FS agentsFigure 7: Games per agent convergence; without rewiring (left) rewiring (right).4.2.2 L EARNED TRATEGYInterestingly, population consisting DS agents tends converge offering acceptinglowest amount possible, setup use rewiring (Figure 8, left), wellsetup (same figure, right). explained 3, DS agents tend adaptstrategies downward easily upward. Thus, two DS agents approximatelystrategy, may slowly pull others strategy downward. many DS agents,probability happens increases. Adding FS agents population results differentbehavior two setups. setup without rewiring difficulties moving away lowestamount possible; sufficient number FS agents (i.e. 30% population)average learned strategy reflect DS agents move towards strategy dictated FSagents. rewiring, results convincingly better; even 10% FS agents, DS agentsaverage converge towards offering accepting amount dictated agents, i.e. 4.5.565fiD E J ONG , U YTTENDAELE & UYLSN REWIRINGW ITH REWIRING5Average converged strategyAverage converged strategy543214321000102030 40 50 60 70Percentage FS agents80090 10010 20 30 40 50 60 70 80 90 100Percentage FS agentsFigure 8: Average learned strategy; without rewiring (left) rewiring (right).W ITH REWIRINGPerformance convergencePerformance convergenceN REWIRING10.90.80.70.60.50.40.30.20.10010.90.80.70.60.50.40.30.20.10010 20 30 40 50 60 70 80 90 100Percentage FS agents10 20 30 40 50 60 70 80 90 100Percentage FS agentsFigure 9: Final performance; without rewiring (left) rewiring (right).4.2.3 P ERFORMANCEobservations concerning learned strategy, reported above, reflected performancecollective agents. setup without rewiring (Figure 9, left), performance decreasesinitially increasing proportion FS agents, DS agents refuse adapt dictatedstrategy. proportion FS agents becomes large enough, DS agents start pickingstrategy, resulting increasing performance. setup rewiring (same figure, right)better, performance increases increasing number FS agents. Even though averagelearned strategy close 4.5 every proportion FS agents, low proportions FS agentsstill display less performance higher proportions. may require additional explanation.Note box plot Figure 8 shows distribution average strategy 50 repeatedsimulations; i.e. show strategy distribution within single simulation.Thus, even though average strategy single simulation always close 4.5,still variance. low number FS agents, variance prominently caused inertia,566fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AMEW ITH REWIRINGDegree distributionDegree distributionN REWIRING1009080706050403020100010 20 30 40 50 60 70 80 90 100Percentage FS agents1009080706050403020100010 20 30 40 50 60 70 80 90 100Percentage FS agentsFigure 10: Resulting network structure; without rewiring (left) rewiring (right).i.e. DS agents directly connected FS agent, implies need learndesired strategy neighboring agents also learning. Especially rewiring,may result two agents playing together compatible neighbors,(yet) other.4.2.4 R ESULTING N ETWORK TRUCTUREClearly, network structure setup without rewiring (Figure 10, left) influencedvarying proportion FS agents. rewiring used (same figure, right), observeinteresting phenomenon, closely related observations 4.1. again, numbertimes agents actually rewire generally lies 1, 000. Even though low number,affect network structure useful way. low proportion FS agents, largetendency increased preferential attachment. 10% FS agents instance, singleagent connects 70 100 agents. increasing proportion FS agents,maximum degree network decreases, finally, closely resembles original network.Clearly, presence examples desired strategy, DS agents attempt connectagents provide examples. interesting useful emergent behavior.4.2.5 N C ONCLUSIONcompare results obtained population 100 agents resultspopulation sizes, reported Table 1, see highly similar. conclusionsubsection, may state setup using rewiring severe difficulties convergingdesired example proportion FS agents providing example low. for, e.g. halfpopulation consisting examples, half learn desired behavior. setupusing rewiring absolutely problems converging desired strategy, evenlow proportion FS agents. cases, completely omitting examples leads agentsconverging individually rational solution. caused artifact learning methodused, i.e. mentioned before, two CALA trying learn others strategy tend drivendownward lowest value allowed.567fiD E J ONG , U YTTENDAELE & UYLS5. Discussionresults presented previous section suggest mechanisms lead cooperative solutions social dilemmas discrete set strategies (e.g. scale-free networks rewiring),also lead agreement social dilemmas continuous strategy space. section, however, show trivial issue. precisely, discuss number mechanismsenhance agents abilities reach cooperation social dilemmas discrete strategy sets,directly enhance agents abilities reach agreement continuous strategy spaces.empirically analyze case.5.1 ReputationReputation one main concepts used behavioral economics explain fairnessemerges (e.g. Bowles et al., 1997; Fehr, 2004). Basically, assumed interactionspeople lead expectations concerning future interactions. expectations may positivenegative may kept oneself, actually shared peers.work closely related work, Nowak et al. (2000) show reputation deters agentsaccepting low offers Ultimatum Game, information spread, leadingagents also receiving low offers return. Then, agents refuse accept low offers,provide high offers. Thus, Nowak et al. argue population goes toward providingaccepting high offers. However, note shared strategy (i.e. agreement)Ultimatum Game yields expected payoff 50% amount stake agents. Thus,reputation may indeed help agents decide strategy play others, preferenceplaying cooperatively (i.e. providing high offers) directly result reputation.5.1.1 PREADING R EPUTATIONstudy effects reputation optionally adding second network system.interaction network, consider reputation network scale-free. contrastinteraction network however, reputation network assumed static, agents truthfulconcerning reputation, making unnecessary agents consider rewiring. Note two agentssharing reputation information may may connected well interaction network,consequence, two agents playing Ultimatum Game may may share reputationinformation other. effect, every Ultimatum Game, responding agent maybroadcast reputation information neighbors reputation network. information sentresponder concerns offer done proposer; informationguaranteed correct. Agents receive information probability:pij = 1H(7)Here, distance sender (potential) receiver j reputation network.Thus, reputation information may travel H hops, decreasing probability per hop.simulations, set H = 5. note relatively small networks, impliesreputation information essentially public.Note reputation information may helpful allow agents somethinginformation. work Nowak et al. (2000), instance, reputation others usedagents determine offer others. Given (1) observation reputation, used568fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AMEway, necessarily promote cooperative strategies (see above), (2) factalready use CALA determine agents offer other, want reputation affectsomething else agents strategies. discuss number ways agents may usereputation, taken literature, i.e. interacting preferred neighbor (below) usingreputation facilitate voluntary participation (5.3).5.1.2 U SING R EPUTATIONWithout reputation, agents play random neighbor interaction network. Reputationmay used make agents prefer interacting specific neighbors Chiang (2008) discussesstrategies fairness could evolve dominant agents allowed choose preferred partnersplay against. Chiang allows agents select partners helped agent previously.determine preferred partner, use heuristic proposed Santos et al. (2006a),i.e. agent prefers playing (relative) cooperators, help obtaining high payoffresponder. Thus, probability agent plays agent j Ni , Ni setagent neighbors, is:sj sipij = P(8)kNi skHere, si , sj sk agents current strategies (for agents i, estimatesbased reputation previous experience).two problems approach. First, number times agent receivesinformation agent j Ni may rather low, especially many agents. Even50 agents, observe around 25% reputation information received agents actually concerned one neighbors. problem may addressed making reputationnetwork identical interaction network (as neighbor relations networks identical). However, may seen considerable abstraction. Second, probability agentinformation concerning neighbors low, need specify default values s0j .Clearly, default value often wrong right, unless use centralized mechanismestimate by, instance, using current average population strategy,simulations.mechanism place, perform experiments 4, i.e. varypopulation size 10 10, 000 agents, proportion FS agents steps 10%.statistical analysis reveals significant difference setup uses reputation setupnot. analyze results, see that, expected, agents almost alwaysneed resort default values neighbors strategies. Thus, average, reputationsystem often change probabilities certain neighbors selected.5.2 Reputation Rewiringseen 4, rewiring works well without reputation (i.e. purely based agentsexperience). Adding reputation may beneficial agents, longer need interactallowed unwire. Thus, agents may increase preferencecertain others. Reputation information (i.e. amount offered certain agent) propagates(static) reputation network, allowing agents receiving information potentiallyunwire one neighbors consider neighbors behavior undesirable.rewiring mechanism used detailed 3 (i.e. Equation 6). allow responder569fiD E J ONG , U YTTENDAELE & UYLSUltimatum Game broadcast reputation information reputation network,maximum H = 5 hops.again, perform experiments 4, again, significantdifference main results. analyze number times agents actually rewired,find number average increases factor 2 respect setup reputationused (i.e. reported 4.3). However, increase increase performance. average, agents neighbors; thus, generally receive reputation information concerningneighbor that, absence reputation, would play soon anyway.5.3 VolunteeringAccording existing research human fairness (e.g. Boyd & Mathew, 2007; Hauert et al., 2007;Sigmund et al., 2001) mechanism volunteering may contribute reaching cooperationgames two strategies. mechanism volunteering consists allowing playersparticipate certain games, enabling fall back safe side income dependplayers strategies. risk-averse optional participation prevent exploitersgaining upper hand, left empty-handed cooperative players preferringparticipate. Clearly, side income must carefully selected, agents encouragedparticipate population sufficiently cooperative. Experiments show volunteering indeedallows collective players spend time happy state (Boyd & Mathew, 2007)players cooperative.biggest problem applying volunteering basically introduce yet anothersocial dilemma. agent may refrain participating make statementagent, may convince agent become social future, makestatement, agent must refuse expected positive payoff: Ultimatum Game randomlyassigned roles, expected payoff always positive. Nonetheless, study whether volunteeringpromotes agreement games continuous strategy spaces. use heuristicproposed Santos et al. (2006a), already applied various mechanismspaper: agent thinks agent j (relative) cooperator, agrees playing.agents agree, game played. prevent agents playing game (after all,agents see relative cooperator already playing strategy),introduce 10% probability games played anyway, even one agents wantto. Note reputation may used here, may allow agents estimate whether oneneighbors relative cooperator not, without play neighbor.Unfortunately, experimental results point agents using volunteering (with withoutreputation) severe difficulties establishing common strategy (Uyttendaele, 2008). result, measuring performance, see around 50% games played.games played, performance similar setup rewiring (e.g. 80%), mayexpected, two agents usually agree play strategies similar. reasonagents converge properly quite simple: avoid playing agentsdifferent them. Therefore, learn behave way similar others.5.4 General Discussiongeneral, may state mechanism rewiring, clearly find good balanceallowing agents play preferred neighbors one hand, forcing agents570fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AMElearn different hand. additions discussed allowagents selective, i.e. much influence play against.may interest individual agents, generally leads agents playing othersdifferent them, instead learning others, required obtain convergenceagreement.6. Conclusionpaper, argue mechanisms thought allow humans find fair, satisfactory solutionssocial dilemmas, may useful multi-agent systems, many multi-agent systems needaddress tasks contain elements social dilemmas, e.g. resource allocation (Chevaleyre et al.,2006). Existing work concerning (human-inspired) fairness multi-agent systems generally restricted discrete strategy sets, usually two strategies, one deemedcooperative (i.e. desirable). However, many real-world applications multi-agent systems posesocial dilemmas require strategy taken continuous strategy space, rather discrete strategy set. observed traditional concept cooperation trivially applicablecontinuous strategy spaces, especially since longer feasible manually label certainstrategy cooperative absolute manner. certain strategy may cooperative certainculture, whereas may defective naive another. Thus, cooperation relative ratherabsolute concept continuous strategy spaces.propose concept agreement (as introduced statistical physics; DallAsta et al.,2006) may used alternative cooperation. discuss emergence agreementcontinuous strategy spaces, using learning agents play pairwise Ultimatum Games, basedscale-free interaction network possibility rewire network. UltimatumGame, two agents agree offering agent offers least minimal amount satisfiesresponding agent (in case, two agents cooperate). Thus, population agentsagree many random pairwise games, agents converge strategy. Withoutexternal influences, shared strategy sufficient. external influences, e.g. preferencedictated humans, agents adapt dictated strategy, even already agreeingcompletely different strategy. propose methodology, based continuous-action learningautomata, interactions scale-free networks, rewiring networks, aimed allowingagents reach agreement. set experiments investigates usability methodology.conclusion, give four statements. (1) proposed methodology able establish agreement common strategy, especially agents given option rewire networkinteraction. Humans playing Ultimatum Game reach agreement approximately 80-90%(Oosterbeek et al., 2004). Without rewiring, agents worse (generally, 65% gamessuccessful); rewiring, well humans. Thus, games discrete strategy set,rewiring greatly enhances agents abilities reach agreement, without compromising scalefree network structure. indicates interactions scale-free networks, well rewiringnetworks, plausible mechanisms making agents reach agreement. (2) comparisonmethodologies reported related work (e.g. Santos et al., 2006b), methodology facilitatesconvergence low number games per agent needed (e.g. 500 instead 10,000).indicates continuous-action learning automata satisfactory approach aimingallowing agents learn relatively low number examples. (3) performancecollective seriously influenced size. clearly influenced characteristics571fiD E J ONG , U YTTENDAELE & UYLSscale-free, self-similar network. (4) Concepts reputation volunteering,reported facilitate cooperative outcomes discrete-strategy games, seem(additional) benefits continuous strategy spaces.Although Ultimatum Game one example social dilemma, core difficultypresent social dilemmas: selecting individually rational action, optimize onespayoff, actually may hurt payoff. Ultimatum Game, problem caused factmay play (as proposer) someone would rather go home empty-handedaccept deal perceived unfair. Similar fairness-related problems may arise variousinteractions, e.g. bargaining division reward, resource allocation (Chevaleyreet al., 2006; Endriss, 2008). Many multi-agent systems need allocate resources, explicitlyassigned task, implicitly, instance multiple agents share certain, limitedamount computation time. Thus, fair division important area research, recentlyreceiving increasing attention multi-agent systems community (Endriss, 2008). humansoften display adequate immediate ability come fair division acceptedthem, definitely pay allow agents learn imitate human strategies.paper, examined task may executed.Acknowledgmentsauthors wish thank anonymous referees valuable contributions. Steven de Jongfunded Breedtestrategie programme Maastricht University.ReferencesAxelrod, R. (1997). Dissemination Culture: Model Local Convergence GlobalPolarization. Journal Conflict Resolution, 41:203226.Barabasi, A.-L. Albert, R. (1999). Emergence scaling random networks. Science, 286:509512.Bearden, J. N. (2001). Ultimatum Bargaining Experiments: State Art. SSRN eLibrary.Bowles, S., Boyd, R., Fehr, E., Gintis, H. (1997). Homo reciprocans: Research InitiativeOrigins, Dimensions, Policy Implications Reciprocal Fairness. Advances ComplexSystems, 4:130.Boyd, R. Mathew, S. (2007). Narrow Road Cooperation. Science, 316:18581859.Cameron, L. (1999). Raising stakes ultimatum game: Evidence Indonesia. JournalEconomic Inquiry, 37:4759.Chevaleyre, Y., Dunne, P., Endriss, U., Lang, J., Lematre, M., Maudet, N., Padget, J., Phelps, S.,Rodriguez-Aguilar, J., Sousa, P. (2006). Issues Multiagent Resource Allocation. Informatica, 30:331.Chiang, Y.-S. (2008). Path Toward Fairness: Preferential Association Evolution Strategies Ultimatum Game. Rationality Society, 20(2):173201.572fiL EARNING R AGREEMENT C ONTINUOUS U LTIMATUM G AMEDallAsta, L., Baronchelli, A., Barrat, A., Loreto, V. (2006). Agreement dynamics smallworld networks. Europhysics Letters, 73(6):pp. 969975.Dannenberg, A., Riechmann, T., Sturm, B., Vogt, C. (2007). Inequity Aversion IndividualBehavior Public Good Games: Experimental Investigation. SSRN eLibrary.de Jong, S. Tuyls, K. (2008). Learning cooperate public-goods interactions. PresentedEUMAS08 Workshop, Bath, UK, December 18-19.de Jong, S., Tuyls, K., Verbeeck, K. (2008a). Artificial Agents Learning Human Fairness.Proceedings international joint conference Autonomous Agents Multi-AgentSystems (AAMAS08), pages 863870.de Jong, S., Tuyls, K., Verbeeck, K. (2008b). Fairness multi-agent systems. KnowledgeEngineering Review, 23(2):153180.de Jong, S., Tuyls, K., Verbeeck, K., Roos, N. (2008c). Priority Awareness: Towards Computational Model Human Fairness Multi-agent Systems. Adaptive Agents Multi-AgentSystems III. Adaptation Multi-Agent Learning, 4865:117128.Endriss, U. (2008). Fair Division. Tutorial International Conference Autonomous AgentsMulti-Agent Systems (AAMAS).Fehr, E. (2004). Dont lose reputation. Nature, 432:499500.Fehr, E. Schmidt, K. (1999). Theory Fairness, Competition Cooperation. QuarterlyJournal Economics, 114:817868.Gintis, H. (2001). Game Theory Evolving: Problem-Centered Introduction Modeling StrategicInteraction. Princeton University Press, Princeton, USA.Grosskopf, B. (2003). Reinforcement Directional Learning Ultimatum Game Responder Competition. Experimental Economics, 6(2):141158.Gueth, W., Schmittberger, R., Schwarze, B. (1982). Experimental Analysis UltimatumBargaining. Journal Economic Behavior Organization, 3 (4):367388.Hauert, C., Traulsen, A., Brandt, H., Nowak, M. A., Sigmund, K. (2007). Via freedomcoercion: emergence costly punishment. Science, 316:19051907.Henrich, J., Boyd, R., Bowles, S., Camerer, C., Fehr, E., Gintis, H. (2004). FoundationsHuman Sociality: Economic Experiments Ethnographic Evidence Fifteen Small-ScaleSocieties. Oxford University Press, Oxford, UK.Maynard-Smith, J. (1982). Evolution Theory Games. Cambridge University Press.Maynard-Smith, J. Price, G. R. (1973). logic animal conflict. Nature, 246:1518.Nowak, M. A., Page, K. M., Sigmund, K. (2000). Fairness versus reason UltimatumGame. Science, 289:17731775.573fiD E J ONG , U YTTENDAELE & UYLSOosterbeek, H., Sloof, R., van de Kuilen, G. (2004). Cultural Differences Ultimatum GameExperiments: Evidence Meta-Analysis. Experimental Economics, 7:171188.Peters, R. (2000). Evolutionary Stability Ultimatum Game. Group Decision Negotiation,9:315324.Roth, A. E., Prasnikar, V., Okuno-Fujiwara, M., Zamir, S. (1991). Bargaining MarketBehavior Jerusalem, Ljubljana, Pittsburgh, Tokyo: Experimental Study. AmericanEconomic Review, 81(5):106895.Santos, F. C., Pacheco, J. M., Lenaerts, T. (2006a). Cooperation Prevails IndividualsAdjust Social Ties. PLoS Comput. Biol., 2(10):12841291.Santos, F. C., Pacheco, J. M., Lenaerts, T. (2006b). Evolutionary Dynamics Social DilemmasStructured Heterogeneous Populations. Proc. Natl. Acad. Sci. USA, 103:34903494.Selten, R. Stoecker, R. (1986). End behavior sequences finite Prisoners Dilemma supergames : learning theory approach. Journal Economic Behavior & Organization, 7(1):4770.Sigmund, K., Hauert, C., Nowak, M. A. (2001). Reward punishment. ProceedingsNational Academy Sciences, 98(19):1075710762.Slonim, R. Roth, A. (1998). Learning high stakes ulitmatum games: experimentSlovak republic. Econometrica, 66:569596.Sonnegard, J. (1996). Determination first movers sequential bargaining games: experimental study. Journal Economic Psychology, 17:359386.Sutton, R. S. Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press,Cambridge, MA. Bradford Book.Thathachar, M. A. L. Sastry, P. S. (2004). Networks Learning Automata: TechniquesOnline Stochastic Optimization. Kluwer Academic Publishers, Dordrecht, Netherlands.Uyttendaele, S. (2008). Fairness agreement complex networks. Masters thesis, MICC,Maastricht University.574fiJournal Artificial Intelligence Research 33 (2008) 349-402Submitted 05/08; published 11/08Learning Partially Observable Deterministic Action ModelsEYAL @ ILLINOIS . EDUEyal AmirComputer Science DepartmentUniversity Illinois, Urbana-ChampaignUrbana, IL 61801, USAALLENC 256@ YAHOO . COMAllen Chang2020 Latham st., apartment 25Mountainview, CA 94040, USAAbstractpresent exact algorithms identifying deterministic-actions effects preconditionsdynamic partially observable domains. apply one know action model (theway actions affect world) domain must learn partial observations time.scenarios common real world applications. challenging AI taskstraditional domain structures underly tractability (e.g., conditional independence) fail(e.g., world features become correlated). work departs traditional assumptionspartial observations action models. particular, focuses problems actionsdeterministic simple logical structure observation models features observedfrequency. yield tractable algorithms modified problem domains.algorithms take sequences partial observations time input, output deterministic action models could lead observations. algorithms output onemodels (depending choice), exact model misclassified givenobservations. algorithms take polynomial time number time steps state featurestraditional action classes examined AI-planning literature, e.g., STRIPS actions.contrast, traditional approaches HMMs Reinforcement Learning inexact exponentially intractable domains. experiments verify theoretical tractability guarantees,show identify action models exactly. Several applications planning, autonomousexploration, adventure-game playing already use results. also promisingprobabilistic settings, partially observable reinforcement learning, diagnosis.1. IntroductionPartially observable domains common real world. involve situations onecannot observe entire state world. Many examples situations availablewalks life, e.g., physical worlds (we observe position items rooms),Internet (we observe web pages time), inter-personal communications (we observe state mind partners).Autonomous agents actions involve special kind partial observability domains.agents explore new domain (e.g., one goes building meets new person),limited knowledge action models (actions preconditions effects). action models change time, may depend state features. agents actintelligently, learn actions affect world use knowledge respondgoals.c2008AI Access Foundation. rights reserved.fiA MIR & C HANGLearning action models important goals change. agent acted while,use accumulated knowledge actions domain make better decisions. Thus, learningaction models differs Reinforcement Learning. enables reasoning actions insteadexpensive trials world.Learning actions effects preconditions difficult partially observable domains.difficulty stems absence useful conditional independence structures domains.fully observable domains include structures, e.g., Markov property (independencestate time + 1 state time 1, given (observed) state time t).fundamental tractable solutions learning decision making.partially observable domains structures fail (e.g., state world time + 1depends state time 1 observe state time t), complexapproximate approaches feasible path. reasons, much work farlimited fully observable domains (e.g., Wang, 1995; Pasula, Zettlemoyer, & Kaelbling, 2004),hill-climbing (EM) approaches unbounded error deterministic domains (e.g., Ghahramani, 2001; Boyen, Friedman, & Koller, 1999), approximate action models (Dawsey, Minsker,& Amir, 2007; Hill, Minsker, & Amir, 2007; Kuffner. & LaValle, 2000; Thrun, 2003).paper examines application old-new structure learning partially observabledomains, namely, determinism logical formulation. focuses deterministic domains tractable learning feasible, shows traditional assumption formdeterminism (the STRIPS assumption, generalized ADL, Pednault, 1989) leads tractablelearning state estimation. Learning domains immediate applications (e.g., exploration planning, Shahaf, Chang, & Amir, 2006; Chang & Amir, 2006) also servebasis learning stochastic domains. Thus, fundamental advance applicationstructure important opening field new approaches broader applicability.following details technical aspects advance.main contribution paper approach called SLAF (Simultaneous LearningFiltering) exact learning actions models partially observable deterministic domains.approach determines set possible transition relations, given execution sequence actionspartial observations. example, input could come watching another agent actwatching results actions execution. approach online, updatespropositional logical formula called Transition Belief Formula. formula represents possibletransition relations world states every time step. way, similar spirit Bayesianlearning HMMs (e.g., Ghahramani, 2001) Logical Filtering (Amir & Russell, 2003).algorithms present differ range applicability computationalcomplexity. First, present deduction-based algorithm applicable nondeterministiclearning problem, takes time worst-case exponential number domain fluents.Then, present algorithms update logical encoding consistent transition relationspolynomial time per step, limited applicability special classes deterministicactions.One set polynomial-time algorithms present applies action-learning scenariosactions ADL (Pednault, 1989) (with conditional effects) one followingholds: (a) action model already preconditions known, observe action failures (e.g.,perform actions domain), (b) actions execution always succeeds (e.g.,expert tutor performs actions).350fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSalgorithms output transition belief formula represents possible transition relationsstates partial observations state actions. updating componentformula separately linear time. Thus, updating transition belief formula everyaction execution observation takes linear time size input formula.Processing sequence action executions observations takes time O(T 2 n) case(b). main reason linear growth representation size transition beliefformula: time t, iterative process updates formula would process formulasize linear t.case (a) processing sequence length takes polynomial time O(T n k )),observe every feature domain every k steps expectation, fixed k. reasontransition belief formula kept k-CNF (k Conjunctive NormalV Form),W thussize O(nk ). (Recall propositional formula k-CNF, form im jk li,j ,every li,j propositional variable negation.) Case (b) takes time O(T n)assumption.Another set polynomial-time algorithms present takes linear time representationsize. case actions known injective, i.e., map states 1:1. There, boundcomputation time steps O(T nk ), approximate transition-belief formularepresentation k-CNF formula.contrast, work learning Dynamic Bayesian Networks (e.g., Boyen et al., 1999), reinforcement learning POMDPs (e.g., Littman, 1996), Inductive Logic Programming (ILP)(e.g., Wang, 1995) either approximate solution unbounded error deterministic domains,ntake time (22 ), inapplicable domains larger 10 features. algorithmsbetter respect, scale polynomially practically domains 100s featuresmore. Section 8 provides comparison works.conduct set experiments verify theoretical results. experiments showalgorithms faster better qualitatively related approaches. example,learn ADL actions effects domains > 100 features exactly efficiently.important distinction must made learning action models traditional creationAI-Planning operators. perspective AI Planning, action models resultexplicit modeling, taking account modeling decisions. contrast, learning action modelsdeducing possible transition relations compatible set partially observedexecution trajectories.particular, action preconditions typically used knowledge engineer controlgranularity action model leave aside specification unwanted cases. example,driving truck insufficient fuel one site another might generate unexpected situationsmodeller want consider, simple precondition used avoid considering case. intention paper mimic modeling perspective, insteadfind action models generate sound states starting sound state. Sound statestate system practice, namely, ones observations real executionsreflect.technical advance deterministic domains important many applicationsautomatic software interfaces, internet agents, virtual worlds, games. applications,robotics, human-computer interfaces, program machine diagnosis use deterministicaction models approximations. Finally, understanding deterministic case better help us351fiA MIR & C HANGdevelop better results stochastic domains, e.g., using approaches Boutilier,Reiter, Price (2001), Hajishirzi Amir (2007).following, Section 2 defines SLAF precisely, Section 3 provides deduction-based exactSLAF algorithm, Section 4 presents tractable action-model-update algorithms, Section 5 gives sufficient conditions algorithms keeping action-model representation compact (thus, overallpolynomial time), Section 7 presents experimental results.2. Simultaneous Learning Filtering (SLAF)Simultaneous Learning Filtering (SLAF) problem tracking dynamic systemsequence time steps partial observations, systems completedynamics initially. solution SLAF representation combinations action modelscould possibly given rise observations input, representationcorresponding states system may (after sequence time stepsgiven input occurs).Computing (the solution for) SLAF done recursive fashion dynamic programmingdetermine SLAF time step t+1 solution SLAF time t. sectiondefine SLAF formally recursive fashion.Ignoring stochastic information assumptions, SLAF involves determining set possibleways actions change world (the possible transition models, defined formally below)set states system might in. transition model determines set possible states,solution SLAF transition model associated possible states.define SLAF following formal tools, borrowing intuitions work Bayesianlearning Hidden Markov Models (HMMs) (Ghahramani, 2001) Logical Filtering (Amir &Russell, 2003).Definition 2.1 transition system tuple hP, S, A, Ri,P finite set propositional fluents;P ow(P) set world states.finite set actions;R transition relation (transition model).Thus, world state, S, subset P contains propositions true state (omittedpropositions false state), R(s, a, s0 ) means state s0 possible result actionstate s. goal paper find R, given known P, S, A, sequence actionspartial observations (logical sentences subset P).Another, equivalent, representation also use paper following.literal proposition, p P, negation, p. complete term P conjunctionliterals P every fluent appears exactly once. Every state corresponds completeterm P vice versa. reason, sometime identify state term. E.g.,states s1 , s2 , s1 s2 disjunction complete terms corresponding 1 , s2 , respectively.transition belief state set tuples hs, Ri state R transition relation.Let R = P ow(S S) set possible transition relations S, A. Let = R.hold transition belief state consider every tuple hs, Ri possible.352fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSFigure 1: Locked door unknown key domain.Example 2.2 Consider domain agent room locked door (see Figure 1).possession three different keys, suppose agent cannot tell observationkey opens door. goal agent unlock door.domain represented follows: let set variables defining state spaceP = {locked} locked true door locked. Let set states= {s1 , s2 } s1 = {locked} (the state door locked) s2 = {} (heredoor unlocked). Let = {unlock1 , unlock2 , unlock3 } three actions wherein agenttries unlocking door using three keys.Let R1 = {hs1 , unlock1 , s2 i, hs1 , unlock2 , s1 i, hs1 , unlock3 , s1 i} represent transition relation key 1 unlocks door keys not. Define R 2 R3 similarfashion (e.g., R2 key 2 unlocks door keys 1 3 not). transition belief staterepresents set possibilities consider consistent observations far. Considertransition belief state given = {hs1 , R1 i, hs1 , R2 i, hs1 , R3 i}, i.e., state worldfully known action model partially known.would like agent able open door despite knowing key opens it.this, agent learn actual action model (i.e., key opens door). general,learning action model useful achieving immediate goal, knowledgeuseful agent attempts perform tasks domain.2Definition 2.3 (SLAF Semantics) Let transition belief state. SLAFactions observations haj , oj i1jt defined1. SLAF [a]() ={hs0 , Ri | hs, a, s0 R, hs, Ri };2. SLAF [o]() = {hs, Ri | true s};3. SLAF [haj , oj iijt ]() =SLAF [haj , oj ii+1jt ](SLAF [oi ](SLAF [ai ]())).Step 1 progression a, Step 2 filtering o.Example 2.4 Consider domain Example 2.2. progression action unlock 1given SLAF [unlock1 ]() = {hs2 , R1 i, hs1 , R2 i, hs1 , R3 i}. Likewise, filteringobservation locked (the door became unlocked) given SLAF [locked]() = {hs 2 , R1 i}.2353fiA MIR & C HANGExample 2.5 slightly involved example following situation presented Figure 2.There, two rooms, light bulb, switch, action flipping switch, observation, E (we east room). real states world action, s2, s2 0 ,respectively (shown top part), known us.WestEastPSfrag replacementsWests2 = sw lit E=sw-on<s1,R1>1Easts20 = sw lit E<s1,R1><s3,R3><s1,R1><s3,R3><s2,R2><s2,R2>2Figure 2: Top: Two rooms flipping light switch. Bottom: SLAF semantics; progressingaction (the arrows map state-transition pairs) filtering observation(crossing pairs).bottom Figure 2 demonstrates knowledge evolves performing action sw-on.There, 1 = {hs1 , R1 i, hs2 , R2 i, hs3 , R3 i} s1 , R1 , s3 , R3 , s2 = {E}, R2 includes hs2 , sw-on, s02 (the identity full details R1 , R2 , R3 irrelevant here, omitthem). 2 resulting transition belief state action sw-on observation E: 2 =SLAF [sw-on, E](1 ). 2assume observations (and observation model relating observations state fluents)given us logical sentences fluents performing action. denotedo.approach transition belief states generalizes Version Spaces action models (e.g.,Wang, 1995) follows: current state, s, known, version spaces lattice containsset transition relations = {R | hs, Ri }. Thus, perspective version spaces,SLAF semantics equivalent set version spaces, one state might be.semantics also generalizes belief states: transition relation, R, known,belief state (set possible states) R = {s | hs, Ri } (read restricted R), LogicalFiltering (Amir & Russell, 2003) belief state action equal (thus, define as)F ilter[a]() = (SLAF [a]({hs, Ri | }))R .Thus, SLAF semantics equivalent holding set belief states, conditioned transitionrelation, similar saying transition relation R, belief state (set states) R .3. Learning Logical InferenceLearning transition models using Definition 2.3 directly intractable requires space (2 2 )many cases. reason explicit representation large set possibletransition-state pairs. Instead, section rest paper represent transition belief states compactly using propositional logic. many scenarios amountstructure exploited make propositional representation compact.|P|354fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELScombinatorial argument implies encoding compact sets. Nonetheless,motivated success propositional (logical) approaches logical filtering (Amir & Russell,2003; Shahaf & Amir, 2007) logical-database regression (Reiter, 1991, 2001), observepropositional logic represents compactly natural sets exponential size.section re-define SLAF operation propositional logical formulaspropositional formula output. SLAFs input propositional formula represents transitionbelief state, SLAF computes new transition belief formula input sequenceactions observations.want find algorithms SLAF manipulate input formula produce correctoutput. use general-purpose logical inference task section. later sectionssidestep expensive general-purpose inference, make assumptions lead tractable algorithms. rest paper focus deterministic transition relations, namely, transitionrelations partial functions (every action one outcome state every state).3.1 Representing Transition Relations Logicinitial algorithm solving SLAF (to presented momentarily) compactrepresentation transition belief states. present logical encoding transition belief statesfirst, define deduction-based algorithm next section.use following general terminology propositional logical languages (all terminological conventions apply without subscripts superscripts). L denotes vocabulary, i.e.,set propositional variables use present context. L denotes language, i.e., setpropositional sentences. , , script Greek letters stand propositional formulaslanguage present context. F, G also stand formulas, restricted context(see below). L() denotes vocabulary . L(L) denotes language built propositionsL using standard propositional connectives (, , ,...). L() shorthand L(L()).represent deterministic transition relations propositional vocabulary, L , whosepropositions form aFG , A, F literal P, G logical formula. Feffect aFG , G precondition aFG . proposition aFG takes truth value TRUE,intended meaning G holds present state, F holds stateresults executing a.let F P {p | p P} set effects, F , consider. let Gset preconditions, G, consider. rest section Section 4 assumeG represents single state S. Recall identify state complete termconjunction literals hold state. use representation states write Fsinstead aFG . Later build definition consider Gs general formulas.assumption (G now, stated above) conclude L O(2|P| 2|P||A|) propositional variables. prove fundamental results language set axioms,disregarding size language moment. Section 5 focuses decreasing languagesize computational efficiency.semantics vocabulary LA lets every interpretation (truth assignment), , LA correspond transition relation, RM . Every transition relation least one (possibly more)interpretation corresponds it, correspondence surjective (onto) injective(1-to-1). Every propositional sentence L(LA ) specifies set transition models follows:355fiA MIR & C HANGset models1 (satisfying interpretations) , I[] = {M interpretation LA | |= },specifies corresponding set transition relations, {RM | I[]}.Informally, assume propositions aFs 1 , ...aFs k LA take value TRUE ,propositions preconditionV take value FALSE. Then, R (with action0 a) takes0state state satisfies ik Fi , identical otherwise. exists (e.g.,Fi = Fj , i, j k), RM takes s0 (thus, executable accordingRM ).following paragraphs show interpretations L correspond transition relations.culminate precise definition correspondence formulas L(L P)transition belief states S.E NTERPRETATION LA C ORRESPONDS U NIQUE RANSITION R ELATIONEvery interpretations LA defines unique transition relation RM follows. Let interpretation LA . every state action either define unique state 0hs, a, s0 RM decide s0 hs, a, s0 RM .gives interpretation every proposition aFs , F fluent negation.fluent p P, [aps ] = [ap] = TRUE (M [] truth value according interpretation), decide s0 hs, a, s0 RM . Otherwise, defines0 = {p P | |= aps } {p | |= ap}left-hand side consider cases p P [a ps ] 6= [ap],right-hand side treat cases p P [aps ] = [ap] = F ALSE (thiscalled inertia p keeps previous value lack specifications). Put another way,0s0 [p] = [aps ] (s[p] [ap]), view interpretation P. RM well defined, i.e.,one RM every .E RANSITION R ELATIONLAL EAST NE C ORRESPONDING NTERPRETATIONpossible RM = RM 0 6= 0 . occurs two circumstances: (a) caseshs, a, s0 RM s, (b) [aps ] = [ap] = F ALSE (inertia)pp00[as ] = s[p], [as ] = s[p] (not inertia).example first circumstance, let p fluent, let interpretation0[aps ] = [ap] G. Define interpretation identical propositionsp pp0besides , follows. Define [as ] opposite truth assignment [aps ] (FALSEpinstead TRUE, TRUE instead FALSE). Define 0 [ap] = [as ].Then, RM = RM 0 map pairs s, way. particular, statecorresponds G, hs, a, s0 RM similarly hs, a, s0 RM 0 .Finally, every transition relation R least one interpretation R = R . seethis, define MR every hs, a, s0 R interpretation aps (p fluent) MR [aps ] = RU E iff/ s0 . Finally, s,p s0 . Also, hs, a, s0 define MR [ap] = F ALSE iff ppps0 , define MR [as ] = MR [as ] = RU E. Then, R = RMR .1. overload word model multiple related meanings. model refers satisfying interpretation logicalformula. Transition model defined Definition 2.1 transition relation transition system. Action modeldefine Introduction section well-defined specification actions preconditions effects.356fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSE RANSITION R ELATION EFINES F ORMULA LAEvery deterministic transition relation R defines logical formula whose set models mapR. many possible formulas, define general one (up logicalequivalence) make use inertia.Define h(R) follows.F00h0 (R) = {aFs , aF| LA , hs, a, R, |= F }pph1 (R) = {as | p P, S}W00h2 (R) = { pP (aps ap) | , hs, a, R}h(R) = h0 (R) h1 (R) h2h0 addresses fluent changes, h1 addresses fluent innertia (effectively disallowing innertiadefinition), h2 addresses conditions actions executable. Thus, h(R)includes model every interpretation satisfies RM = R requires inertiadefinition RM . represents R models satisfies RM = R.illuminating see modeling decisions (above throughout section) leadlast definition. one hand, choose every interpretation L correspondtransition relation (we simplify later arguments logical entailment). Consequently, associate interpretations [aFs ] = [aF] = F ALSE transition relations R(s, a, s0 ) keep value F fixed s, s0 (this inertia F a, s).hand, define h(R) above, choose axioms exclude models (thus,avoid models include inertia) simplifies later discussion learning algorithms.summary, consider every interpretation LA representing exactly one transition relation, consider set axioms defining R define directly, i.e., without inertia(without [aFs ] = [aF] = F ALSE).RANSITION B ELIEF TATES C ORRESPOND F ORMULAS LA PThus, everyW transition belief state define formula L(L P) correspondsit: h() = hs,Ri (s h(R)). formulas exist would characterize similar way,equivalent. stronger formulas L(L )|= h(R) h(R) 6|= every model, , satisfies RM = R.Similarly, every formula L(LA P) define transition belief state () = {hM P, RM | |= , }, i.e., state-transition pairs satisfy (M P restrictedP, viewed complete term P). say formula transition belief formula,h(()) (note: (T h()) = always holds).3.2 Transition-Formula Filteringsection, show computing transition belief formula SLAF [a]() successfulaction transition belief formula equivalent logical consequence finding operation.characterization SLAF consequence-finding permits using consequence findingalgorithm SLAF, important later paper proving correctnesstractable, specialized algorithms.Let CnL () denote set logical consequences restricted vocabulary L. is,LCn () contains set prime implicates contain propositions set L.357fiA MIR & C HANGRecall implicate formula clause entailed ( |= ). Recall prime implicate formula implicate subsumed (entailed) implicates.Consequence finding process computes CnL () input, . example, propositional resolution (Davis & Putnam, 1960; Chang & Lee, 1973) efficient consequence finderused properly (Lee, 1967; del Val, 1999) (Marquis, 2000, surveys results prime implicates consequence finding algorithms). Thus, Cn L () { L(L)| |= }.set propositions P, let P 0 represent set propositions every proposition primed (i.e., proposition f annotated become f 0 ). Typically, use primedfluent denote value unprimed fluent one step future taking action. Let[P 0 /P] denote formula , primed fluents replaced unprimed counterparts. example, formula (a b0 )[P 0 /P] equal b b P. (See Section 8discussion comparison relevant formal verification techniques.)following lemma shows logical equivalence existential quantification quantifiedboolean formulas consequence finding restricted vocabulary. Recall quantified booleanformulas (QBF) propositional formulas addition existential universal quantifierspropositions. Informally, QBF x. true given interpretationexists true/false valuation x makes true assignment. lemma proveuseful showing equivalence SLAF consequence-finding.Lemma 3.1 x. CnL()\{x} (), propositional logic formula propositional variable x.P ROOFSee Section B.1. 2lemma extends easily case multiple variables:Corollary 3.2 formula set propositional variables X, X. Cn L()\X ().present algorithm updating transition belief formulas whose output equivalentSLAF (when SLAF applied equivalent transition belief state). algorithm appliesconsequence finding input transition belief formula together set axioms definetransitions time steps. present set axioms first.deterministic (possibly conditional) action, a, action model (for time t) axiomatizedVTeff (a) = lF ,GG ((alG G) l0 )WV(1)l0lF (l ( GG (aG G)))first part (1) says assuming executes time t, causes l G holds, Gholds time t, l holds time + 1. second part says l holds execution,must alG holds also G holds current state. two parts similar(in fact, somewhat generalize) effect axioms explanation closure axioms used SituationCalculus (see McCarthy & Hayes, 1969; Reiter, 2001).Now, ready describe zeroth-level algorithm (SLAF 0 ) SLAF transitionbelief formula. Let L0 = P 0 LA vocabulary includes fluents time t+1 effectpropositions LA . Recall (Definition 2.3) SLAF two operations: progression (withaction) filtering (with observation). time apply progression given actioncurrent transition belief formula, , apply filtering current observations:358fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS0t+1 = SLAF0 [at , ot ](t ) = (CnL (t Teff (at )))[P 0 /P] ot(2)identical Definition 2.3 (SLAF semantics), replacing 1 2.0stated above, SLAF0 implement CnL () using consequence finding algorithmsresolution variants (e.g., Simon & del Val, 2001; McIlraith & Amir, 2001;Lee, 1967; Iwanuma & Inoue, 2002). following theorem shows formula-SLAF algorithm correct exact.Theorem 3.3 (Representation) transition belief formula, action,SLAF [a]({hs, Ri | hs, Ri satisfies }) ={hs, Ri | hs, Ri satisfies SLAF0 [a]()}P ROOFSee Section B.2. 2theorem allows us identify SLAF0 SLAF , throughout restpaper. particular, show polynomial-time algorithms SLAF special cases correctshowing output logically equivalent SLAF 0 .U SINGUTPUTSLAF0output algorithm SLAF transition belief formula logical formula. wayuse formula answering questions SLAF depends query formoutput formula. wish find transition model state possible, wish see|= , interpretation L = P LA output SLAF0 .answer found simple model-checking algorithm 2 . example, checkinterpretation satisfies logical formula assign truth values variables interpretation formula; computing truth value formula done linear time.Thus, type query SLAF takes linear time size output formulaSLAF final query propositional interpretation propositional formula.wish find transition model possible state possible,propositional satisfiability (SAT) solver algorithms (e.g., Moskewicz, Madigan, Zhao, Zhang, &Malik, 2001). Similarly, wish answer whether possible models satisfy propertyuse SAT solver.Example 3.4 Recall Example 2.4 discuss locked door three combinations. Let0 = locked, let 1 = SLAF0 [unlock2 , locked](0 ). wish find 1 implies tryingunlock door key 2 fails open it. equivalent asking models consistent1 give value TRUE unlock2lockedlocked .answer query taking SLAF0 output formula, 1 , checking 1unlock2lockedlocked SAT (has model). (Follows Deduction Theorem propositional logic:|= iff SAT.)One example application approach goal achievement algorithm Chang Amir(2006). relies SAT algorithms find potential plans given partial knowledge encodedtransition belief formula.2. model checking sense used Formal Verification literature. There, model transitionmodel, checking done updating formula OBDD transformations359fiA MIR & C HANGzeroth-level algorithm may enable compact representation, guaranteeit, guarantee tractable computation. fact, algorithm maintain compact representation tractable computation general. Deciding clause true result SLAFcoNP-hard similar decision problem Logical Filtering coNP-hard (Eiter & Gottlob, 1992; Amir & Russell, 2003) even deterministic actions. (The input representationproblems includes initial belief state formula CNF. input representation Filteringincludes propositional encoding CNF (known) transition relation.)Also, representation transition belief states uses poly(|P|) propositions grows exponentially (in number time steps |P|) starting transition belief states actionsequences, actions allowed nondeterministic 3 . question whether exponential growth must happen deterministic actions flat formula representations (e.g., CNF,DNF, etc.; see Darwiche & Marquis, 2002) open (logical circuits known give solutiondeterministic actions, representation given terms fluents time 0, Shahaf et al.,2006).4. Factored Formula UpdateUpdate representation hard must consider set interactions partsrepresentation. Operations like used SLAF 0 consider interactions, manipulatethem, add many interactions result. processing broken independentpieces, computation scales linearly number pieces (i.e., computation timetotal times takes piece separately). So, important find decompositionsenable independent pieces computation. Hereforth examine one type decomposition,namely, one follows logical connectives.Learning world models easier SLAF distributes logical connectives. function,f , distributes logical connective, {, , ...}, f ( ) f () f (). ComputationSLAF becomes tractable, distributes , . bottleneck computation casebecomes computing SLAF part separately.section examine conditions guarantee distribution, present linear-timealgorithm gives exact solution cases. also show algorithmgives weaker transition belief formula distribution possible.Distribution properties always hold SLAF follow set theoretical considerationsTheorem 3.3:Theorem 4.1 , transition belief formulas, action,SLAF [a]( ) SLAF [a]() SLAF [a]()|= SLAF [a]( ) SLAF [a]() SLAF [a]()P ROOFSee Appendix B.3. 2Stronger distribution properties hold SLAF whenever hold Logical Filtering.Theorem 4.2 Let 1 , 2 transition belief states.SLAF [a](1 2 ) = SLAF [a](1 ) SLAF [a](2 )3. follows theorem filtering Amir Russell (2003), even provide proper axiomatization(note axiomatization deterministic actions only).360fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSiff every RRRRF ilter[a](R1 2 ) = F ilter[a](1 ) F ilter[a](2 ).conclude following corollary Theorems 3.3, 4.2 theorems Amir Russell(2003).Corollary 4.3 , transition belief formulas, action, SLAF [a]( ) SLAF [a]()SLAF [a]() every relation R , one following holds:1. R maps states 1:12. R conditional effects, includes prime implicates, observefails3. state known R: one s, hs, Ri .Condition 2 combines semantics syntax. particularly useful correct computationSLAF later sections. states particular syntactic form (namely, togetherinclude joint prime implicates), simple enough (but necessarily 1:1),computation SLAF broken separate SLAF .Figure 3 presents Procedure Factored-SLAF, computes SLAF exactly conditions Corollary 4.3 hold. Consequently, Factored-SLAF returns exact solution wheneveractions known 1:1. actions conditional effects success/failureobserved, modified Factored-SLAF solve problem exactly (see Section 5).PROCEDURE Factored-SLAF(hai , oi i0<it ,)i, ai action, oi observation, transition belief formula.1. 1 do,(a) Set Step-SLAF(oi ,ai ,).(b) Eliminate subsumed clauses .2. Return .PROCEDURE Step-SLAF(o,a,)observation formula L(P), action, transition belief formula.1. literal, return oLiteral-SLAF(a,).2. = 1 2 , return Step-SLAF(o,a,1 )Step-SLAF(o,a,2 ).3. = 1 2 , return Step-SLAF(o,a,1 )Step-SLAF(o,a,2 ).PROCEDURE Literal-SLAF(a,)action, proposition Lt negation.01. Return CnL ( Teff (a))[P 0 /P ] .Figure 3: SLAF using distribution ,pre-compute (and cache) 2n possible responses Literal-SLAF, every time stepprocedure requires linear time representation size , transition belief formulatime step. significant improvement (super exponential) time takenstraightforward algorithm, (potentially exponential) time taken general-purposeconsequence finding used zeroth-level SLAF procedure above.Theorem 4.4 Step-SLAF(a, o, ) returns formula 0 SLAF [a, o]() |= 0 . every runLiteral-SLAF takes time c, Step-SLAF takes time O(||c). (recall || syntactic,representation size .) Finally, assume one assumptions Corollary 4.3,0 SLAF [a, o]().361fiA MIR & C HANGbelief-state formula transition belief formula effect propositions, i.e., includes fluent variables propositions form FG . identical traditionaluse term belief-state formula, e.g., (Amir & Russell, 2003). give closed-form solution SLAF belief-state formula (procedure Literal-SLAF Figure 3). makesprocedure Literal-SLAF tractable, avoiding general-purpose inference filtering single literal,also allows us examine structure belief state formulas detail.VTheorem 4.5 belief-state formula L(P), action a, Ca = GG,lF (alG alG ),G1 , ..., Gm G terms G Gi |= ,SLAF [a]()^_li(li aG) Cal1 ,...,lm F i=1VHere, l1 ,...,lm F means conjunction possible (combinations of) selections literalsF.P ROOFSee Appendix B.4. 2theorem significant says write result SLAFprescribed form. form still potentially exponential size, boils simplecomputations. proof follows straightforward (though little long) derivation possibleprime implicates SLAF [a](t ).consequence Theorem 4.5 implement Procedure Literal-SLAF usingequivalenceTheorem 4.5L(l) PSLAF [a](l)l SLAF [a](TRUE) otherwiseNotice computation Theorem 4.5 = l literal simple G 1 , ..., Gmcomplete terms L(P) include l. computation require general-purposeconsequence finder, instead need answer 2n+1 queries initialization phase, namely,storing table values SLAF [a](l) l = p l = p p P alsol = TRUE.general, could high 2|P| , number complete terms G, finding G1 , ..., Gmmay take time exponential |P|. Still, simplicity computation formula providebasic ingredients needed efficient computations following sections restricted cases.also give guidelines future developments SLAF algorithms.5. Compact Model RepresentationPrevious sections presented algorithms potentially intractable long sequences actionsobservations. example, Theorem 4.5, could high 2 |P| , number completeterms G. Consequently, clauses may exponential length (in n = |P|) maysuper-exponential number clauses result.section focus learning action models efficiently presence action preconditions failures. important agents partial domain knowledgetherefore likely attempt inexecutable actions.restrict attention deterministic actions conditional effects, provideoverall polynomial bound growth representation, size many steps,362fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELStime taken compute resulting model. class actions generalizes STRIPS (Fikes, Hart,& Nilsson, 1972), results apply large part AI-planning literature.give efficient algorithm learning non-conditional deterministic action effectspreconditions, well efficient algorithm learning actions effects presenceaction failures.5.1 Actions Limited Effectmany domains assume every action affects k fluents, small k > 0.also common assume actions STRIPS, may fail without us knowing,leaving world unchanged. assumptions together allow us progress SLAF limited(polynomial factor) growth formula size.use language similar one Section 3, uses action propositionsalG Gfluent term size k (instead fluent term size n G). Semantically,Vlal1 ...lk lk+1 ,...,ln all1 ...ln .Theorem 5.1 Let L(P) belief-state formula, STRIPS action k fluentsaffected precondition term. Let G k set terms k fluents L(P)consistent . Then,SLAF [a]()^k_li(li aG) Cai=1G1 , ..., Gk G kG1 ... Gk |=l1 , ..., lk FVHere, ... refers conjunction possible (combinations of) selections literalsF G1 , ..., Gk G k G1 ... Gk |= .P ROOFSee Section B.5. 2main practical difference theorem Theorem 4.5 smaller numberterms need checked practical computation. limited language enables entailslimited number terms play here. Specifically, k literalspreconditions, need check combinations k terms G 1 , ..., Gk G k , computationbounded O(exp(k)) iterations.proof uses two insights. First, one case change occurs, everyclause Theorem 4.5 subsumed clause entailed SLAF [a]( ), onealGi per literal li (i.e., li 6= lj 6= j) Gi fluent term (has disjunctions). Second, everyalG G term equivalent formula alGi Gi terms length k, affects kfluents.Thus, encode clauses conjunction using subset (extended) actioneffect propositions, alG , G term size k. O(nk ) terms, O(nk+1 )propositions. Every clause length 2k, identity clause determined2first half (the set action effect propositions). Consequently, SLAF [a]( ) takes O(nk +k k 2 )2space represent using O(nk +k ) clauses length 2k.363fiA MIR & C HANG5.2 Actions Conditional Effects: Revised LanguagethisSsection reformulate representation presented Section 3.1. LetL0f = aA {af , af , af , a[f ] , a[f ] } every f P. Let vocabulary formulas representing transition belief states defined L = P L0f . intuition behind propositionsvocabulary follows:Val causes l literal l. Formally, al sS als .Vaf keeps f . Formally, af sS ((s f ) afs ) ((s f ) af).V).(Thus, la[l] causes FALSE l. Formally, a[l] sS (s l) (als alprecondition executing a, must hold executes.)model transition belief formula L, valuation fluents P definesstate. valuation propositions L0f defines unconditional deterministic transitionrelation follows: action proposition af (af ) true action transitionrelation causes f (f ) hold executed. Action proposition f trueaction affect fluent f . Action proposition a[f ] (a[f ] ) true f (f )precondition a. assume existence logical axioms disallow inconsistentimpossible models. axioms are:1. af af af2. (af af ) (af af ) (af af )3. a[f ] a[f ]possible f P. first two axioms state every action model, exactly oneaf , af , af must hold (thus, causes f , negation, keeps f unchanged). last axiomdisallows interpretations a[f ] a[f ] hold. state axiomsneed represent constraints explicitly transition belief formula itself.use set theoretic propositional logic notations transition belief states interchangeably. Note vocabulary defined sufficient describing unconditionalSTRIPS action model, deterministic action model general.Example 5.2 Consider domain Example 2.2. transition belief state represented transition belief formula:locked((unlock1locked unlock2locked unlock3locked )(unlock1locked unlock2locked unlock3locked )(unlock1locked unlock2locked unlock3locked )).2provide axiomatization equivalent SLAF special case eff (1)notation above. P P 0 . Recall intend primed fluents representvalue fluent immediately action taken.364fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSeff (a)^Prea,f Effa,ff PPrea,f^(a[l] l)l{f,f }Effa,f^((al (af l)) l0 ) (l0 (al (af l))).l{f,f }Prea,f describes precondition action a. states literal l occurs preconditionliteral l must held state taking a. formula Eff a,f describes effectsaction a. states fluents taking action must consistent accordingaction model defined propositions af , af , af .show revised axiomatization action models, eff , leads equivalentdefinition SLAF within restricted action models.0Theorem 5.3 successful action a, SLAF [a]() Cn LP ( eff (a))[P 0 /P] .P ROOFSee Appendix B.6. 25.3 Always-Successful Non-Conditional Actionsready present algorithm learns effects actions conditionaleffects. algorithm allows actions preconditions fully known. Still,assumes filtered actions executed successfully (without failures), cannot effectivelylearn preconditions (e.g., would know knew originallypreconditions seeing sequence events). sequence actions might, example,generated expert agent whose execution traces observed.algorithm maintains transition belief formulas special fluent-factored form, defined below. maintaining formulas special form, show certain logical consequencefinding operations performed efficiently. formula fluent-factored, conjunction formulas f f concerns one fluent, f , action propositions.Also, every fluent, f , f conjunction positive element, negative element,neutral one f (f explf ) (f explf ) Af , explf , explf , Af formulae actionpropositions af , af , a[f ] , a[f ] , af (possibly multiple different actions). intuitionexplf explf possible explanations f true false, respectively.Also, Af holds knowledge actions effects preconditions f , knowledgedepend f current value. Note formula L(L f ) represented fluentfactored formula. Nonetheless, translation sometimes leads space blowup, maintainrepresentation form construction.new learning algorithm, AS-STRIPS-SLAF4 , shown Figure 4. simplify exposition,described case single action-observation pair, though obviousapply algorithm sequences actions observations. Whenever action taken, firstsubformulas Af , explf , explf f updated according steps 1.(a)-(c). Then,4. AS-STRIPS-SLAF extends AE-STRIPS-SLAF (Amir, 2005) allowing preconditions actions365fiA MIR & C HANGAlgorithm AS-STRIPS-SLAF[ha, oi]()Inputs:V Successful action a, observation term o, fluent-factored transition belief formula =f P f .Returns: Fluent-factored transition belief formula SLAF [ha, oi]()1. every f P(a) Set Af (a[f ] explf ) (a[f ] explf ) Af(b) Set explf (af (af a[f ] explf ))(c) Set explf (af (af a[f ] explf ))(d) |= f (f observed) seta , f (f >) (f ) Af explf(e) |= f set f (f ) (f >) Af explf [Note: 6|= f6|= f , nothing beyond earlier steps.]2. Simplify (e.g., eliminate subsumed clauses ).3. Returna. term (f >) new explf , (f ) new explf . appear without simplificationconform Step 1a algorithm. also emphasizes syntactic nature procedure,implicit logical simplification assumed.Figure 4: SLAF algorithm always successful STRIPS actions.observation received, f updated according observation according steps1.(d)-(e). Step 2 merely indicates implementations, likely simplificationprocedure used formula subsumption elimination. However, usesimplification procedure strictly necessary order theoretical guaranteesalgorithm hold.example, know nothing actions affect f (e.g., start exploration),f = (f RU E) (f RU E) RU E. representation, SLAF [a](f )conjunction (f explf )(f explf )Af computed step 1 Procedure AS-STRIPS-SLAF.similar formula holds observations.following theorem shows correctness algorithm. shows steps takenalgorithm produce result equivalent logical consequence-finding characterizationSLAF Theorem 5.3.Theorem 5.4 SLAF[ha, oi]() AS-STRIPS-SLAF[ha, oi]() fluent-factored formula ,successfully executed action a, observation term o.P ROOFSee Appendix B.7 2time space complexity procedure AS-STRIPS-SLAF given followingtheorem. time guarantee, shown procedure takes linear time size inputformula. condition algorithm receives observations often enoughspecifically366fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSevery fluent observed least every k calls procedureit possible showtransition belief formula remains k-CNF indefinitely (recall k-CNFfixed k, conjunction clauses size k). Thus, regardless length actionobservation input sequence, output AS-STRIPS-SLAF value throughoutcomputation k-CNF. amounts space guarantee size formula.Theorem 5.5 following true AS-STRIPS-SLAF:1. procedure takes linear time size input formula single action, observationpair input.2. every fluent every k steps observation fluent one steps,input formula k-CNF, resulting formula (after arbitrary numbersteps) k-CNF.3. input AS-STRIPS-SLAF fluent-factored, output.P ROOFSee Appendix B.8 2following corollary follows immediately above.Corollary 5.6 order process steps actions observations, AS-STRIPS-SLAF requires(T |P|) time. Additionally,every fluentis observed every k steps, resultingformula always size |P| |A|k .Corollary holds Theorem 5.5(2) guarantees bound size belief-stateformula point algorithm.5.4 Learning Actions May Failmany partially observable domains, decision-making agent cannot know beforehand whetheraction decides take fail succeed. section consider possible action failure,assume agent knows whether action attempts fails succeeds tryingaction.precisely, assume additional fluent OK observed agentOK true action succeeded. failed action, case, may viewedextra observation agent preconditions action met. is,action failure equivalent observation^(a[f ] f ) (a[f ] f ).(3)f PAction failures make performing SLAF operation considerably difficult. particular,observations form (3) cause interactions fluents value particular fluentmight longer depend action propositions fluent, action propositionsfluents well. Transition belief states longer represented convenient fluentfactored formulas cases, becomes difficult devise algorithms giveuseful time space performance guarantees.367fiA MIR & C HANGAlgorithm PRE-STRIPS-SLAF[a, o]()Inputs: Action observationterm o. transition belief formula following facV Wtored form: = j i,j , i,j fluent-factored formula.Returns: Filtered transition belief formula1. |= OK:W(a) Set F (li ) li literals appearing precondition,F (l) Vfluent-factored formula equivalent l (i.e., F (l) = ((l >) (l) >) f P ((f >) (f >) >))(b) Set i,j AS-STRIPS-SLAF[o](i,j ) i,j2. Else (o |= OK):(a) i,ji. Set i,j AS-STRIPS-SLAF[P ](i,j ), P preconditionii. Set i,j AS-STRIPS-SLAF[ha, oi](i,j )3. i,j factored Ai,j Bi,j Bi,j contains (and only) clauses containingfluent P.W exists B j, B i,j B, replaceWBj Ai,jj i,j4. Simplify i,j (e.g. remove subsumed clauses)5. ReturnFigure 5: Algorithm handling action failures preconditions known.shall demonstrate, action failures dealt tractably assume actionpreconditions known agent. is, agent must learn effects actionstake, need learn preconditions actions. particular, meansaction a, algorithm given access formula (more precisely, logical term) P describingprecondition action a. Clearly, algorithm need learn preconditionsactions, restrict action proposition vocabulary used describe belief statesones forms af , af , af , longer need action propositions forms a[f ]a[f ] .present procedure PRE-STRIPS-SLAF5 (Figure 5) performs SLAF transition beliefformulas presence action failures actions non-conditional effects. maintains transition beliefconjunctions disjunctions fluent-factored formulas (formulasV formulasWform = j i,j i,j fluent factored). Naturally, formulas supersetfluent-factored formulas.5. PRE-STRIPS-SLAF essentially identical CNF-SLAF (Shahaf et al., 2006)368fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSalgorithm operates follows: action executes successfully (and ensuing observation received), component fluent-factored formulas i,j filtered separatelyaccording AS-STRIPS-SLAF procedure action-observation pair (Step 2).hand, action fails, disjunction fluent-factored formulas appended transitionbelief formula (Step 1). component disjunction corresponds one possible reasons action failed (i.e., one literals occurring actions precondition). Finally,observations accumulated learning algorithm, collapses disjunctions fluent-factoredformulas occurring belief formula together (Step 3) simplifies generally (Step 4),decreasing total size formula. case AS-STRIPS-SLAF, simplificationsteps necessary order time space guarantees hold.proof correctness Algorithm PRE-STRIPS-SLAF relies distribution resultsSection 4, Theorem 4.1 Corollary 4.3.proceed show correctness PRE-STRIPS-SLAF. following theorem showsprocedure always returns filtered transition belief formula logically weakerexact result, always produces safe approximation. Additionally, theorem showsconditions Corollary 4.3, filtered transition belief formula exact result.Theorem 5.7 following true:1. SLAF[a, o]() |= PRE-STRIPS-SLAF[a, o]()2. PRE-STRIPS-SLAF[a, o]() SLAF[a, o]() Corollary 4.3 holds.P ROOFSee Appendix B.9. 2consider time space complexity algorithm. following theorem shows(1) procedure time efficient, (2) given frequent enough observations (as Theorem5.5), algorithm space efficient transition belief formula stays indefinitely compact.Theorem 5.8 following true PRE-STRIPS-SLAF:1. procedure takes time linear size formula single action, observation pairinput.2. every fluent observed every k steps input formula k-CNF,filtered formula k-CNF, maximum number literals actionprecondition.P ROOFSee Appendix B.10 2Therefore, get following corollary:Corollary 5.9 order process steps actions observations, PRE-STRIPS-SLAF requires(T |P|) time. every fluentobserved leastfrequently every k steps, resultingmkformula always size |P| |A|.6. Building Resultssection describe briefly one might extend approach include elaborateobservation model, bias, parametrized actions.369fiA MIR & C HANG6.1 Expressive Observation Modelobservation model use throughout paper simple: every state, fluentobserved value v, value current state. consider observationmodel general.observation model, O, set logical sentences relates propositions set Obsfluents P. Obs includes propositions appear P, independentprevious following state (times 1 + 1) given fluents time t.SLAF result conjoining CnLt (oO), i.e., finding prime implicatesconjoining . embed extension SLAF algorithms above,maintain structures algorithms use. k-CNF every stepobserve (at most) 1 variable, finding prime implicates easy. Embeddingtransition-belief formula done conjunction prime implicates formula,removal subsumed clauses. resulting formula still fluent factored, inputfluent factored. Then, algorithms remain applicable time complexity,replacing ot prime implicates ot Ot .Using Model algorithms described provide exact solution SLAF,tuples hs, Ri solution consistent observations. computesolution SLAF represented logical formula. use SAT solver (e.g., Moskewiczet al., 2001) answer queries formula, checking entails f , actionfluent f . would show consistent models action makes f value TRUE.number variables result formula always independent , linear |P|algorithms. Therefore, use current SAT solvers treat domains 1000features more.Preference Probabilistic Bias Many times information leads us preferpossible action models others. example, sometimes assume actions changefluents, suspect action (e.g., open-door) affect features (e.g.,position) normally. represent bias using preference model (e.g., McCarthy, 1986;Ginsberg, 1987) probabilistic prior transition relations (e.g., Robert, Celeux, & Diebolt,1993).add bias end SLAF computation, get exact solutioncompute effect bias together logical formula efficiently. Preferential biasesstudied fit easily result algorithms (e.g., use implementationsDoherty, Lukaszewicz, & Szalas, 1997, inference bias).Also, algorithms inference probabilistic bias logical sentences emergingused (Hajishirzi & Amir, 2007). There, challenge enumeratetentative models explicitly, challenge overcome success work HajishirziAmir (2007) similar task filtering. use algorithms apply probabilisticbias resulting logical formula.example, given probabilistic graphical model (e.g., Bayesian Networks) set propositional logical sentences, consider logical sentences observations. approach,logical sentence gives rise characteristic function (x ) 1x satisfies0 otherwise. conjunction clauses get set functions (one per clause). Thus,inference combined probabilistic-logical system probabilistic inference. example,370fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSone consider variable elimination (e.g., Dechter, 1999) additional potentialfunctions.Parametrized Actions many systems situations natural use parametrized actions.action schemas whose effect depend parameters, definition appliesidentically instantiations.example, move(b, x, y) action moves b position x position y,b, x, parameters action. common planning systems (e.g., STRIPS,PDDL, Situation Calculus). complete treatment parameterized actions outside scopepaper, give guidelines generalization current approach actions.Consider domain set fluent predicates universe named objects. propositional fluents defined domain ground instantiations predicate fluents. SLAFwork set propositional fluents instantiated actions mannerrest paper. action propositions a( X )lG instantiated every vector object namesX.)lG .different treatment comes additional axioms sayx,.a(x )lG a(Inference transition belief state axioms able join information collecteddifferent instantiations actions. expect thorough treatmentable provide efficient algorithms whose time complexity depend number actionschemas instead number instantiated actions.Several approaches already start address problem, including work Nance, Vogel,Amir (2006) filtering work Shahaf Amir (2006) SLAF.7. Experimental EvaluationPrevious sections discussed problem settings consider algorithms solutions. showed modifying traditional settings learning dynamic partially observabledomains important. Determinism alone lead tractability, additional assumptions simple, logical action structure bounded 0 frequency observations fluentsdo. Specifically, far showed time space computing SLAF length-T timesequence n fluents polynomial n.section considers practical considerations involved using SLAF procedures.particular, examines following questions:much time space SLAF computations take practice?much time required extract model logical formula resultSLAF procedures?quality learned model (taking arbitrary consistent model)? fartrue (generating) model?conditions algorithms correctness hold practice?learned model used successful planning execution? learningprocedures fit planning execution?implemented algorithms ran experiments AS-STRIPS-SLAF following domains taken 3rd International Planning Competition (IPC): Drivelog, Zenotravel,Blocksworld, Depots (details domains learning results appear Appendix C).371fiA MIR & C HANGexperiment involves running chosen algorithm sequence randomly generatedaction-observation sequences 5000 steps. Information recorded every 200 steps.random-sequence generator receives correct description domain, specifiedPDDL (Ghallab, Howe, Knoblock, McDermott, Ram, Veloso, Weld, & Wilkins, 1998; Fox & Long,2002) (a plannig-domain description language), size domain, starting state. (Thesize domain number propositional fluents it. set specificationnumber objects domain number arity predicates domain.) generatesvalid sequence actions observations domain starting state, i.e., sequenceconsistent input PDDL generator actions may fail (action failureconsistent PDDL action attempted state canot execute).experiments, chose observations follows: every time step select 10fluents uniformly random observe. applied additional restrictions (such making surefluent observed every fixed k steps).SLAF algorithm receives sequences actions observations, domaininformation otherwise (e.g., receive size domain, fluents, starting state,PDDL). starting knowledge algorithm empty knowledge, TRUE.domain ran algorithm different numbers propositional fluents (19 250fluents). collected time space taken SLAF computation plottedfunction input-sequence length (dividing total computation time steps).time space results shown Figures 6, 7, 8, 9. graphs broken differentdomains compare time space taken different domain sizes. time SLAF-timewithout CNF simplification (e.g. remove subsumed clauses)much time space SLAF computations take practice? answer firstquestion now. observe figures time per step remains relatively constantthroughout execution. Consequently, time taken perform SLAF different domains growslinearly number time steps. Also, see time SLAF grows domainsize, scales easily moderate domain sizes (1ms per step SLAF domains 200 fluents).much time required extract model logical formula resultSLAF procedures? SLAF procedures return logical formula sequence actionsobservations. need apply work extract candidate (consistent) modelformula. computation done SAT solver CNF formulas.quality learned model (taking arbitrary consistent model)? fartrue (generating) model? sometimes many possible models, littlebias must consider possible likely. decided introduce onebias, namely, actions instances actions schemas. Thus, actions assumedeffect parameters (or objects), given properties parameters. Thus,actions effects assumed independent identity parameter.So, vanilla implementation, propositions look like((STACK((STACK((STACK((STACKetc.EEG)G)B)A)CAUSESCAUSESCAUSESCAUSES(ON(ON(ON(ONEGG))E))B))A))372fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSSLAF Time/Step: Blocksworld Domain1.61.41.2Time (ms)119 fluents41 fluents71 fluents131 fluents209 fluents0.80.60.40.20200100018002600340042005000Input Sequence LengthSLAF Time/Step: Depots Domain1.61.41.2Time (ms)158 fluents94 fluents138 fluents190 fluents250 fluents0.80.60.40.20200100018002600340042005000Input Sequence LengthFigure 6: SLAF-time without CNF simplification domains Blocksworld DepotsInstead, replace ground propositions like schematized propositions:((STACK ?X ?Y) CAUSES (ON ?X ?Y))((STACK ?X ?Y) CAUSES (ON ?Y ?X))((STACK ?X ?Y) CAUSES (ON ?X ?Y))etc.Thus, belief-state formula looks something like:373fiA MIR & C HANGSLAF Time/Step: Driverlog Domain1.81.61.4Time (ms)1.231 fluents76 fluents122 fluents186 fluents231 fluents10.80.60.40.20200100018002600340042005000Input Sequence LengthSLAF Time/Step: Zeno-Travel Domain0.80.70.6Time (ms)0.558 fluents91 fluents134 fluents0.40.30.20.10200100018002600340042005000Input Sequence LengthFigure 7: SLAF-time without CNF simplification domains Driverlog Zeno-Travel(AND(AND(OR (ON E G)(OR ((STACK ?X ?Y) CAUSES (NOT (ON ?X ?Y)))(AND ((STACK ?X ?Y) KEEPS (ON ?X ?Y))(NOT ((STACK ?X ?Y) NEEDS (ON ?X ?Y))))))...374fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSSLAF Space: Blocksworld Domain300KSpace (#lisp symbols)250K200K19 fluents41 fluents71 fluents131 fluents209 fluents150K100K50KK200100018002600340042005000Input Sequence LengthSLAF Space: Depots Domain160K140KSpace (#lisp symbols)120K100K58 fluents94 fluents138 fluents190 fluents250 fluents80K60K40K20KK200100018002600340042005000Input Sequence LengthFigure 8: SLAF space domains Blocksworld Depotsexample fragment model (the complete output given Appendix C) consistenttraining dataBlocksworld domain:* 209 fluents* 1000 randomly selected actions* 10 fluents observed per step* "schematized" learning375converting CNFclause count: 235492variable count: 187adding clausescalling zchafffiA MIR & C HANGSLAF Space: Driverlog Domain250KSpace (#lisp symbols)200K150K31 fluents76 fluents122 fluents186 fluents231 fluents100K50KK200100018002600340042005000Input Sequence LengthSLAF Space: Zeno-Travel Domain60KSpace (#lisp symbols)50K40K58 fluents91 fluents134 fluents30K20K10KK200100018002600340042005000Input Sequence LengthFigure 9: SLAF space domains Driverlog Zeno-Travel* 1:1 precondition heuristicsparsing resultSLAF time: 2.203Inference time: 42.312Learned model:(UNSTACK NEEDS (NOT (CLEAR ?UNDEROB)))(UNSTACK NEEDS (CLEAR ?OB))(UNSTACK NEEDS (ARM-EMPTY))376fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS(UNSTACK(UNSTACK(UNSTACK(UNSTACK(UNSTACK(UNSTACK(UNSTACK(UNSTACK(UNSTACK(UNSTACK(UNSTACK(UNSTACK(UNSTACK...NEEDS (NOT (HOLDING ?OB)))NEEDS (ON ?OB ?UNDEROB))CAUSES (CLEAR ?UNDEROB))CAUSES (NOT (CLEAR ?OB)))CAUSES (NOT (ARM-EMPTY)))CAUSES (HOLDING ?OB))CAUSES (NOT (ON ?OB ?UNDEROB)))KEEPS (ON-TABLE ?UNDEROB))KEEPS (ON-TABLE ?OB))KEEPS (HOLDING ?UNDEROB))KEEPS (ON ?UNDEROB ?UNDEROB))KEEPS (ON ?OB ?OB))KEEPS (ON ?UNDEROB ?OB))Sometimes, multiple possible schematized propositions correspond groundaction proposition, case disjoin propositions together replacement(i.e., single ground propositional symbol gets replaced disjunction schema propositions).replacement simple procedure, one effective deriving information fewer steps also speeding model finding SLAF formula. implementedrun SLAF runs. One could SAT-solving portion algorithm,equivalent result.Regarding latter, ran scaling issues SAT solver (ZChaff , Moskewicz et al.,2001; Tang, Yinlei Yu, & Malik, 2004) common lisp compiler large experiments. gotaround issues applying replacement scheme above, thus reducing greatly numbervariables SAT solver handles.Another issue ran SAT solver tended choose models blank preconditions (the sequences used experiments include action failure, preconditions never eliminated algorithm). add bias extracted action model,added axioms following form:(or (not (,a causes ,f)) (,a needs (not ,f)))(or (not (,a causes (not ,f))) (,a needs ,f))axioms state action causes fluent f hold, requires f holdprecondition (similarly, analagous axiom f ). Intuitively, axioms causesat solver favor 1:1 action models. got idea heuristic work Wu,Yang, Jiang (2007), uses somewhat similar set axioms bias results termslearning preconditions. Clearly, axioms dont always hold, results, one seelearned preconditions often inaccurate.inaccuracies learned action models reasonable. example, fluentnever changes course action sequence, algorithm may infer arbitrary actioncauses fluent hold.conditions algorithms correctness hold practice? scenariosreport here, conditions guarantee correctness algorithms hold. experiments assumed main conditions algorithms hold, namely, actions377fiA MIR & C HANGdeterministic preconditions. enforce observations every fluentevery (fixed) k steps. latter condition necessery correctness algorithm,necessary guarantee polynomial-time computation. experiments verify necessary practice, indeed case algorithms polynomial-time guaranteemodified observationearlier work Hlubocky Amir (2004) included modified version AS-STRIPSSLAF architecture tested suite adventure-game-like virtual environmentsgenerated random. include arbitrary numbers places, objects various kinds,configurations settings those. There, agents task exit house, startingknowledge state space, available actions effects, characteristics objects.experiments show agent learns effects actions efficiently. agentmakes decisions using learned knowledge, inference resulting representation fast(a fraction second per SAT problem domains including 30 object, modes,locations).learned model used successful planning execution? learningprocedures fit planning execution? learned model used planningtranslating PDDL. However, model always correct one domain,plan may feasible may lead required goal. cases, interleaveplanning, execution, learning, described work Chang Amir (2006). There,one finds short plan consistent action model, executes plan, collects observations,applies SLAF those. plan failure detected (e.g., goal achieved),results Chang Amir (2006) guarantee joint planning-execution-learning procedurewould reach goal bounded amount time. bounded time fact linear lengthlongest plan needed reaching goal, exponential complexity actionmodel need learn.8. Comparison Related WorkHMMs (Boyen & Koller, 1999; Boyen et al., 1999; Murphy, 2002; Ghahramani, 2001) usedestimate stochastic transition model observations. Initially, expected comparework HMM implementation Murphy (2002), uses EM (a hill-climbing approach).Unfortunately, HMMs require explicit representation state space, smallest domain (31 features) requires transition matrix (231 )2 entries. prevents initializing HMMsprocedures current computer.Structure learning approaches Dynamic Bayes Nets (DBNs) (e.g., Ghahramani & Jordan,1997; Friedman, Murphy, & Russell, 1998; Boyen et al., 1999) use EM additional approximations (e.g., using factoring, variation, sampling), tractable. However, stilllimited small domains (e.g., 10 features , Ghahramani & Jordan, 1997; Boyen et al., 1999),also unbounded errors discrete deterministic domains, usable settings.simple approach learning transition models devised work HolmesCharles Lee Isbell (2006) deterministic POMDPs. There, transition observation modelsdeterministic. approach close represents hidden state possible models using finite structure, Looping Prediction Suffix Tree. structure seenrelated representation grow models relate action histories possibletransition models. work interactions realized recursive structure378fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELStransition-belief formula built AS-STRIPS-SLAF, e.g.,(af (af a[f ] explf ))explf refers similar formula created previous time step.main difference draw work Holmes CharlesLee Isbell (2006) latter refers states explicitly, whereas work refers features.Consequently, representation (provably) compact procedures scale largerdomains theoretically practice. Furthermore, procedure provably maintains reference possible models data insufficient determine single model, whereaswork Holmes Charles Lee Isbell (2006) focuses limit case enough information determining single consistent model. down-side, procedure considerstochasticity belief state, remains area development.similar relationship holds work Littman, Sutton, Singh (2002).work, model representation given size linear number states. model,predictive state representation (PSR), based action/observation histories predicts behavior based histories. work prefers low-dimensional vector basis instead featurebased representation states (one traditional hallmarks Knowledge Representationapproach). necessary correspondence basis vectors intuitive features real world necessarily. enables representation world closelybased behavior.Learning PSRs (James & Singh, 2004) nontrivial one needs find good lowdimensional vector basis (the stage called discovery tests). stage learning PSRs requiresmatrices size (n2 ), states spaces size n.work advances line work providing correct results time polylogarithmic number states. Specifically, work learns (deterministic) transition modelspolynomial time state features, thus taking time O(poly(log n)).Reinforcement Learning (RL) approaches (Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1996)compute mapping world states preferred actions. highly intractablepartially observable domains (Kaelbling, Littman, & Cassandra, 1998), approximation (e.g.,Kearns, Mansour, & Ng, 2000; Meuleau, Peshkin, Kim, & Kaelbling, 1999; Even-Dar, Kakade, &Mansour, 2005; McCallum, 1995) practical small domains (e.g., 10-20 features)small horizon time .contrast HMMs, DBNs, RL, algorithms exact tractable large domains (>300 features). take advantages properties common discrete domains, determinism,limited effects actions, observed failure.Previous work learning deterministic action models AI-Planning literature assumesfully observable deterministic domains. learn parametrized STRIPS actions using, e.g., version spaces (Gil, 1994; Wang, 1995), general classifiers (Oates & Cohen, 1996), hill-climbingILP (Benson, 1995). Recently, work Pasula et al. (2004) gave algorithm learns stochastic actions conditional effects. work Schmill, Oates, Cohen (2000) approximatespartial observability assuming world fully observable. apply partiallyobservable learning problems (sometimes) using space belief states instead world states,increases problem size exponentially, practical problem.Finally, recent research learning action models partially observable domains includesworks Yang, Wu, Jiang (2005) Shahaf Amir (2006). works Yang et al.379fiA MIR & C HANG(2005), example plan traces encoded weighted maximum SAT problem,candidate STRIPS action model extracted. general, may many possible action modelsgiven set example traces, therefore approach nature approximate (in contrastours, always identifies exact set possible action models). work also introducesadditional approximations form heuristic rules meant rule unlikely action models.work Shahaf Amir (2006) presents approach solving SLAF using logicalcircuit encodings transition belief states. approach performs tractable SLAF general deterministic models present Section 5, also requires SAT solverslogical circuits. SAT solvers optimized nowadays comparison CNF SATsolvers, overall performance answering questions SLAF lower ours.importantly, representation given Shahaf Amir (2006) grows (linearly) numbertime steps, still hinder long sequences actions observations. comparison,transition belief formula bounded size independent number time stepstrack.encoding language, LA typical methods software hardware verificationtesting. Relevant books methods (e.g., Clarke, Grumberg, & Peled, 1999) closely relatedrepresentation, results achieve applicable vice versa.main distinction draw work done Formal Methods (e.g., ModelChecking Bounded Model Checking) able conclude size bounds logical formulas involved computation. OBDDs used success Model Checking,CNF representations used success Bounded Model Checking, littlebounds sizes formulas theory practice. conditions available AI applicationsused current manuscript deliver bounds yield tractability scalability resultstheoretical practical significance.Interestingly, methods use Linear Temporal Logics (LTL) cannot distinguishhappen actually happens (Calvanese, Giacomo, & Vardi, 2002). Thus, cannotconsider causes occurence. method similar consider alternatefutures state explicitly. However, use extended language, namely L , makesalternatives explicit. allows us forego limitations LTL produce needed result.9. Conclusionspresented framework learning effects preconditions deterministic actionspartially observable domains. approach differs earlier methods focuses determining exact set consistent action models (earlier methods not). showed severalcommon situations done exactly time polynomial (sometime linear) number time steps features. add bias compute exact solution large domains(hundreds features), many cases. Furthermore, show number action-observationtraces must seen convergence polynomial number features domain.positive results contrast difficulty encountered many approaches learningdynamic models reinforcement learning partially observable domains.results presented promising many applications, including reinforcementlearning, agents virtual domains, HMMs. Already, work applied autonomousagents adventure games, exploration guided transition belief state compute380fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSinformation gain criteria. future plan extend results stochastic domains,domains continuous features.Acknowledgmentswish thank Megan Nance providing code samples sequence generator,also wish thank Dafna Shahaf encouraging collaboration enhanced development understanding results. first author also wishes acknowledge stimulatingdiscussion Brian Hlubocky related topics. wish acknowledge support DAFAir Force Research Laboratory Award FA8750-04-2-0222 (DARPA REAL program). secondauthor wishes acknowledge support University Illinois Urbana-Champaign, CollegeEngineering fellowship. Finally, first author acknowledges support joint Fellowship(2007-2008) Center Advanced Studies Beckman Institute UniversityIllinois Urbana-Champaign.Earlier versions results manuscript appeared conference proceedings (Amir,2005; Shahaf et al., 2006).Appendix A. Representation Domain Descriptionstransition relation RM interpretation LA defined Section 3.1 waysimilar interpretation Domain Descriptions. Domain Descriptions common methodspecifying structured deterministic domains (Fagin, Ullman, & Vardi, 1983; Lifschitz, 1990;Pednault, 1989; Gelfond & Lifschitz, 1998). methods equivalent contextsinclude Successor-State Axioms (Reiter, 2001) Fluent Calculus (Thielscher, 1998).relevance influence work merit separate exposition relationship work.domain description finite set effect rules form causes F G describe effects actions, F G state formulas (propositional combinations fluentnames). say F head effect G precondition rule. writecauses F , causes F TRUE. denote ED (a) set effect rules actionA. effect rule e, let Ge precondition Fe effect. Rule e active state s,|= Ge (s taken interpretation P).Every domain description defines unique transition relation R (s, a, s0 ) follows.VLet F (a, s) conjunction effects rules active a, i.e., {Fe | eED (a), |= Ge }. set F (a, s) = RU E rule active s.Let I(a, s) set fluents affected s, i.e., I(a, s) = {f P | eED (a) (s |= Ge ) (f/ L(Fe ))}.Define (recalling world states sets fluents)00 (s I(a, s)) = (s I(a, s))RD = hs, a,(4)s0 |= F (a, s)Thus, action effect FALSE s, cannot execute s.definition applies inertia (a fluent keeps value) fluents appear active rule.contexts useful specify inertia explicitly extra effect rules formkeeps f G, fluent f P. shorthand writing two rules causes f f Gcauses f f G. includes inertia (keeps) statements, saycomplete domain description.381fiA MIR & C HANGExample A.1 Consider scenario Figure 2 assume actions observations occurFigure 10. Actions assumed deterministic, conditional effects, preconditionsmust holds execute successfully. Then, every action affects every fluent either negatively,positively, all. Thus, every transition relation complete domain descriptionincludes rules form causes l keeps l, l fluent literal (a fluentnegation).Time stepActionLocationBulbSwitch1go-WE?sw2Elit?go-E3E?swsw-on4E?swgo-W5Elit?go-E6E?swFigure 10: action-observation sequence (table entries observations). Legend: E: east; E:west; lit: light on; lit: light off; sw: switch on; sw: switch off.Consequently, every transition relation R completely defined domain description(viewing tuple set elements)causes E, causes sw, causes lit,causes E causes sw causes litR9 keeps E8keeps litkeeps sw> go-W >><>=go-E >>>;: sw-on >Say initially know effects go-E, go-W, know sw-on does. Then,transition filtering starts product set R (of 27 possible relations) possible 2 3states. Also, time step 4 know world state exactly {E, lit, sw}. try sw-onget F ilter[sw-on](4 ) includes set transition relationstransition relations projecting state {E, lit, sw} appropriate choice S.receive observations o5 = E sw time step 5, 5 = F ilter[o5 ](F ilter[sw-on](4 ))removes transition belief state relations gave rise E sw. lefttransition relations satisfying one tuplessw-on causes litsw-on causes E,sw-on causes litsw-on causes swsw-on keeps Esw-on keeps litFinally, perform action go-W, update set states associated everytransition relation set pairs 5 . receive observations time step 6,conclude 6 = F ilter[o6 ](F ilter[go-W](5 )) =sw-on keeps E,sw-on causes E,++ *E*Esw-on causes sw,sw-on causes sw,litlit(5),,,sw-on causes lit,sw-on causes lit,swswgo-E...go-E...2382fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSAppendix B. ProofsB.1 Proof Lemma 3.1: Consequence Finding Existential QuantificationP ROOFConsider CNF form . Suppose clauses containing literal x x1 , . . . , x 1 , . . . , clauses. Suppose clauses containing literal xx 1 , . . . , x b . VSuppose clausescontaining x x 1 , . . . , c . noteVL()\{x}Cn() ( 1ic ) ( 1ia,1jb j ) (the formula produced addingresolvents variable x removing clauses belonging L(L()\{x})), sinceresolution complete consequence finding.Necessity. (x. |= CnL()\{x} ()) Consider model x.. definition,extended L() (i.e., assigning value m(x)) m() = 1. Extendcase. suppose contradiction model Cn L()\{x} (). cannotcase m(k ) = 0 k, m() = 0, contradiction. Then, must casem(i j ) = 0 1 1 j b. Therefore, m(i ) = 0 m(j ) = 0.model , m(x ) = 1 m(x j ) = 1. Thus either m(i ) = 1m(j ) = 1, contradiction.Sufficiency. (CnL()\{x} () |= x.) Consider model CnL()\{x} (). Supposecontradiction m(x.) = 0. is, extended L(), m() = 0. Now, extendL() m(x) = 0. cannot case m(k ) = 0 k since modelsCnL()\{x} (). m(x) = 1, cannot case m(x j ) = 0 j.Therefore m(x ) = 0 1 a. Therefore, m(i ) = 0. m(i j ) = 11 j n, must m(j ) = 1 j. alter m(x) = 1,satisfies , contradiction. 2B.2 Proof Theorem 3.3P ROOFsides equality relation sets state-transition-relation pairs. showtwo sets elements. show first left-hand side equalitycontained right-hand side.Take hs0 , Ri SLAF [a]({hs, Ri | hs, Ri satisfies }). Definition 2.3{s | hs, Ri satisfies } hs, a, s0 R. words,hs, Ri satisfies hs, a, s0 R.prove hs0 , Ri satisfies SLAF0 [a]() need show Teff (at ) modelRM = R s0 interprets P 0 . Let interprets P, s0 interprets P 0 , MRinterpreting LA previoussection. interpretationWsatisfy formulaVVone conjuncts lF ,GG ((alG G) l0 ) lF (l0 ( GG (alG G))) falsified.cannot case choice s.Assume contradiction (alG G) l0 ) fails l. Then, (alG G) hold l0FALSE. portion interprets LA built according MR , R. Since hs, R, s0know s0 satisfies l, construction MR . contradicts l 0 FALSE(M interprets P 0 according s0 ), therefore conclude (alG G) l0 ) every l0 .WSimilarly, assume contradiction (l 0 ( GG (alG G))) fails l. Then, l 0 holdss0 , als fails. Again, way constructed MR must als takes valuecorresponds l 0 truth value s0 . Thus, must als takes value TRUE ,done first direction.383fiA MIR & C HANGopposite direction (showing right-hand side contained left-hand side), takehs0 , Ri satisfies SLAF [a](). showhs0 , Ri SLAF [a]({hs, Ri | hs, Ri satisfies }).hs0 , Ri |= SLAF [a]() implies (Corollary 3.2) hs 0 , R, si |=Teff (a) (s0 , R, interpreting P 0 , LA , P, respectively). similar argument one givefirst part shows hs, a, s0 R, hs0 , Ri SLAF [a]({hs, Ri | hs, Ri satisfies }).2B.3 Proof Theorem 4.1: Distribution SLAF Connectivesfirst part, show sets models SLAF [a]( ) SLAF [a]()SLAF [a]() identical.Take model SLAF [a]( ). Let 0 model SLAF [a](M 0 ) =. Then, 0 model model . Without loss generalization, assumemodel . Thus, |= SLAF [a](), follows model SLAF [a]()SLAF [a]().direction, take model SLAF [a]() SLAF [a](). Then, modelSLAF [a]() model SLAF [a](). Without loss generalization assumemodel SLAF [a](). Take 0 model = SLAF [a](M 0 ). So, 0 |= .follows |= SLAF [a]( ).similar argument follows case. Take model SLAF [a]( ). Let 0model SLAF [a](M 0 ) = . Then, 0 model . Thus, |=SLAF [a]() |= SLAF [a](). follows model SLAF [a]()SLAF [a]().2B.4 Proof Theorem 4.5: Closed form SLAF belief state formulaP ROOF SKETCHfollow characterization offered Theorem 3.3 Formula (1).take Teff (a, t) resolve literals time t. resolution guaranteed generateset consequences equivalent CnLt+1 (t Teff (a, t)).VAssuming , Teff (a, t) logically equivalent Teff (a, t)| = lF ,GG,G|= ((alG Gt )Vlt+1 ) lF ,GG,G|= (lt+1 (Gt alG )). follows two observations. First, noticeimplies G G G 6|= get Gt alG (alG Gt ) lt+1 (theantecedent notWhold, formula true). Second, notice that,V second partoriginal Teff (a, t), ( GG,G|= (alG Gt )) equivalent (assuming ) ( GG,G|= (Gt alG )).Now, resolving literals time Teff (a, t)| considerresolutions clauses (Gt term) form alG Gt lt+1 clauses formlt+1 (Gt alG ) other. yields equivalent^_[ lit+1i=1G1 , ...,WGm G|= im Gil1 , ..., lm F_lialGi )](aGi=1eliminate Wliterals time resolve together sets clauses matching Gi |= Gi . formula encodes resulting clauses chosen384fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSliset G1 , ..., Gm chosen set literals l1 , ..., lm . reason including (alGi aGi )always choose clause Gi , li specific type (either one includes alGone produces alG.Finally, get formula theorem aFG aFG (G characterizes exactly onestate S), fact one set G1 , ..., Gm stronger rest (it entailsrest) G1 , ..., Gm complete terms. set complete fluent assignments Gsatisfy . 2B.5 Proof Theorem 5.1: Closed form k affected fluentsP ROOF SKETCHliteral l clause C conjunction Theorem 4.5, aggregateaction propositions single action proposition alGl , G disjunction completepreconditions l C (notice cannot ls negation C tautology).Lemma B.2 shows Gl equivalent fluent term. First prove restricted LemmaB.1.Wlit+1) clause formula Theorem 4.5, let Gl =Lemma B.1 Let C =aGi=1 (liW{Gi | li = l, Gi |= li }, literal l6 . Assume effect deterministicone case term precondition (if case hold, nothing changes). Then, G lequivalent term.P ROOFGl disjunction complete state terms Gi , represents set statescorresponds Gi s. intuition applyWlit+1 alGiCWVm i=1t+1( i=1 alGi ) (i=1 li )0lGl l CC 0 part C affect l. reason complete terms G knowlilalGi aGi . Thus, choice G includes conditions lchanges, assume precondition alGl .compute action model l updating copy G l . Let li fluent literal,set Glt = Gl .1. Gl2 Glt Gl2 |= li , terms Glt include li . Thus,Glt Glt li , alGl alGl li . Thus, add li conjunct Glt .2. Otherwise, Gl2 Glt Gl2 |= li , terms Glt include li .Thus, Glt Glt li , alGl alGl li . Thus, add li conjunct Glt .3. Otherwise, know li li value fluent l changes l =TRUE. Since assume value l changes single case preconditions(i.e., conditional effects - either succeeds change, fails change),li cannot part preconditions, i.e., every term G Glt replace lili vice versa, precondition would still entail l action. Thus,alGl alGl \{l } , Glt \ {li } result replacing li li TRUE.6. related literal l proof above.385fiA MIR & C HANGresult replacements term Glt consistent (it consistentoriginal Gl ) satisfies 1 case |= alGl alGl . 2Wlit+1cl =Lemma B.2 Let C =aGclause formula Theorem 4.5, let Gi=1 liW{Gi | li = l}, literal l. Assume effect deterministic one casecl equivalentterm precondition (if case hold, nothing changes). Then, either Gclterm, C subsumed another clause entailed formula Gequivalent term.Gl .clP ROOFConsider Gl Lemma B.1, let Gl1 complete fluent term GlllThus, G1 |= l. Let Gt term equivalent G according Lemma B.1.Clause C equivalent alGl al.... However, alGl already asserts change l lGl1result action a, alasserts change different condition l l. Thus,Gl1. get subsuming clause C 0 = C \ al. way1 case |= alGl alGlGl11remove C literals alGi Gi Gl .cl . However, clause Cprocess left clause C Gl Gform Theorem 5.1 Gi missing. representtheorem? must allow Gi missing.2Proof theorem continues Thus, representation C 0 (the new clause) takes space O(n2 )(each propositional symbol encoded O(n) space, assuming constant encoding everyfluent p P).However, number propositional symbols still large (there O(2 n ) fluent terms,encoding still requires O(2n n) (all preconditions effects) new propositional symbols.notice limit attention clauses C k literals l whose actionproposition alGl satisfies |= Gl l. k propositions C,VWlsay {alGi l }ik0 , C ( ki=1 alGi l ) aGk+1... im lit+1 , always subsumedlk+1Vl.lattersentencealways true, assume change( ki=1 alGi l ) aGk+1lk+1k fluents (aGk+1asserts lk+1 remains same, alGi l asserts li changeslk+1one conditions satisfied Gli ).Vl(which state k effects)Using clauses form ( ki=1 alGi l ) aGk+1llk+1lresolve away aGj l j > k every clause C original conjunction. Thus,jWVleft clauses form C ( ki=1 alGi l ) im lit+1 . Now, since choice literalsl1 , ..., lk independent rest clause, every polarityfluents, get clauses resolve (on combinations literals) (andsubsumed by)k_^lit+1(6)C ( alGi l )i=1386ikfiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSThus, get conjunction clauses form (6), G li (i k) fluent term. So,conjunction clauses Theorem 4.5 equivalent conjunction clausesclause 2k literals. Thus, space required represent clause 2kn.Finally, use fact every action dependent k fluents. Every proposition asserts no-change li equivalent conjunction literals stating nonepossible k preconditions consistent affect li . example alli1 ...lk lu impliesalli1 ...lk alli1 ...lk1 lu .... Similarly, one elements conjunction impliesalli1 ...lk lu . 2B.6 Proof Theorem 5.3: Equivalent Restricted Definition Action AxiomsP ROOFLet 0 = P.(eff (a)). claim successful actionSa, SLAF [a]()0[P 0 /P] . see this, consider model 0 . valuation fluents f P L0f definetransition relation R valuation fluents P 0 define state s0 hs0 , Ri 0 .definition 0 , hs0 , Ri 0 exists hs, Ri eff (a) satisfied. Finally note eff (a) satisfied preconditions action mets0 consistent effects action applied s. is, eff (a) satisfiedhs, a, s0 R. Together, observations Corollary 3.2 yield theorem. 2B.7 Proof Theorem 5.4: AS-STRIPS-SLAF CorrectP ROOFLet shorthand notation C() denote C() CnL ()[P 0 /P] .definition, SLAF [ha, oi]() SLAF [o](SLAF [a]()). Theorem 5.3,SLAF [a]() C( eff (a)). formula equivalent C( eff (a)) may generatedresolvingV fluents P (by following procedure proof Lemma 3.1). Suppose= f P f fluent-factored form. may rewrite C( eff (a)) as:0SLAF [a]()^f P^f P^f P^f PC(Prea,f ) C(Effa,f ) C(f )(7)C(Prea,f Effa,f )C(f Prea,f )C(f Effa,f )equivalence holds resolvents generated resolving literals P C(eff (a)) still generated formula. is, pair clauses possibly resolved together (where fluent P resolved out) eff (a) generate newconsequence C ( eff (a)) appear together one C () components (7).387fiA MIR & C HANGevery clause eff (a) contains one literal P, see possible consequencesgenerated.note Effa,f may rewritten follows:Effa,f^((al (af l)) l0 ) (l0 (al (af l)))(8)l{f,f }^(l (l0 al )) (l0 (al af ))l{f,f }straightforward verify equivalence rewritten formula original formula. Noteperforming rewriting, may discard clauses form f af af , musttrue every consistent model (given axioms described Section 5.2).consider consequences generated component (7). may computeconsequences performing resolution. C(Pre a,f ) a[f ] a[f ] ,may discard clauses inconsistent action models violate clause.definition fluent-factored formulas, C(f ) Af . Next, remaining componentscomputed straightforwardly:^C(Effa,f )(l0 al af )l{f,f }C(f Prea,f ) C(f ) C(Prea,f )^(a[l] expll )l{f,f }C(Prea,f Effa,f ) C(Prea,f ) C(Effa,f )^(l0 al a[l] )l{f,f }C(f Effa,f ) C(f ) C(Effa,f )^(l0 al expll )l{f,f }Finally, difficult see steps (a)-(c) procedure sets following formula (influent-factored form):^^SLAF [a]()Af(a[l] expll ) (l0 al (af a[l] expll )f Pl{f,f }Now, note SLAF [o](SLAF [a]()) SLAF [a]() o. Note term,SLAF [a]() made fluent-factored performing unit resolution. exactlysteps 1.(d)-(e) do. 2B.8 Proof Theorem 5.5: AS-STRIPS-SLAF ComplexityP ROOFConsider size formula returned AS-STRIPS-SLAF. Overview: noteformula i-CNF, integer > 0, filtered formula one step(i + 1)-CNF. Then, note every observation fluent f resets f -part belief stateformula 1-CNF (thus, = 1).Details: first part, call procedure appendsone literal existing clauses formula, new clauses length k + 1388fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSgenerated. Additionally, every fluent observed every k steps, transition beliefformula stays k-CNF (i.e., indefinitely compact). existing clauses may growlength (1 literal per timestep) augmented steps 1.(a)-(c), appropriate fluentobserved steps 1.(d)-(e), clauses stop growing. Finally, easy seesteps 1.(a)-(e) performed polynomial time. 2B.9 Proof Theorem 5.7: PRE-STRIPS-SLAF CorrectP ROOFConsider semantics SLAF filtering STRIPS action known precondition. case action failure, world filtered transition belief stateworld meet action precondition (and satisfies observation). Clearly, step 1algorithm performs filtering conjoining belief formula negation actionprecondition (converted logically equivalent disjunction fluent-factored formulas).case action success, filtering performed first removing worldssatisfy action precondition (so remaining worlds, action executable)filtering remaining worlds using algorithm AS-STRIPS-SLAF. Moreover, Theorem 4.1Corollary 4.3 follows filtering formula performed filteringsubformulas i,j separately. Furthermore, SLAF[ha, oi]() |= PRE-STRIPS-SLAF[ha, oi](),PRE-STRIPS-SLAF[ha, oi]() SLAF[ha, oi]() conditions Corollary 4.3.filtering subformula performed steps 2 3 algorithm.Finally, note steps 3 4 serve simplify belief formula produce logicallyequivalent formula. 2B.10 Proof Theorem 5.8: PRE-STRIPS-SLAF ComplexityP ROOFNote call AS-STRIPS-SLAF subformula takes time linear sizesubformula, steps involving AS-STRIPS-SLAF performed linear time.Thus total time complexity linear. Additionally, note every fluent observed everyk steps, every fluent-factored subformula i,j belief formula k-CNF,theorem AmirW (2005). action preconditions contain literals, disjunctionform j i,j contains disjuncts. Therefore, entire belief formula staysk-CNF, indefinitely. 2Appendix C. Experiments Outputsexperiments (Section 7) examine properties algorithms learning action models.show learning tractable exact. Appendix section bring generating modelslearned models detailed comparison reader. Recall algorithmsoutput representation set models possible given input. bringone satisfying model learned formula.experiments include following domains International Planning Competition(IPC): Drivelog, Zenotravel, Blocksworld, Depots.C.1 Driverlog DomainDriverlog domain following generating PDDL:(define (domain driverlog)389fiA MIR & C HANG(:requirements :typing)(:typeslocation locatable - objectdriver truck obj - locatable )(:predicates(at ?obj - locatable ?loc - location)(in ?obj1 - obj ?obj - truck)(driving ?d - driver ?v - truck)(path ?x ?y - location)(empty ?v - truck) )(:action LOAD-TRUCK:parameters(?obj - obj?truck - truck?loc - location):precondition(and (at ?truck ?loc) (at ?obj ?loc)):effect(and (not (at ?obj ?loc)) (in ?obj ?truck)))(:action UNLOAD-TRUCK:parameters(?obj - obj?truck - truck?loc - location):precondition(and (at ?truck ?loc) (in ?obj ?truck)):effect(and (not (in ?obj ?truck)) (at ?obj ?loc)))(:action BOARD-TRUCK:parameters(?driver - driver?truck - truck?loc - location):precondition(and (at ?truck ?loc) (at ?driver ?loc) (empty ?truck)):effect(and (not (at ?driver ?loc)) (driving ?driver ?truck)(not (empty ?truck))))(:action DISEMBARK-TRUCK:parameters(?driver - driver?truck - truck?loc - location):precondition(and (at ?truck ?loc) (driving ?driver ?truck)):effect(and (not (driving ?driver ?truck)) (at ?driver ?loc)(empty ?truck)))(:action DRIVE-TRUCK:parameters(?truck - truck?loc-from - location?loc-to location?driver - driver):precondition(and (at ?truck ?loc-from)(driving ?driver ?truck)(path ?loc-from ?loc-to)):effect(and (not (at ?truck ?loc-from)) (at ?truck ?loc-to)))(:action WALK:parameters(?driver - driver?loc-from - location?loc-to - location):precondition(and (at ?driver ?loc-from) (path ?loc-from ?loc-to)):effect(and (not (at ?driver ?loc-from)) (at ?driver ?loc-to))) )One learned model (one possible satisfying model formula) random-sequenceinput Driverlog domain following (brought together experimental parameters).Driverlog domain:* IPC3 problem 99* 231 fluents* 1000 randomly selected actions* 10 fluents observed per step390fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS* "schematized" learning* 1:1 precondition heuristics* Action distribution:((BOARD-TRUCK . 52) (DRIVE-TRUCK . 86) (DISEMBARK-TRUCK . 52)(WALK . 529) (UNLOAD-TRUCK . 139) (LOAD-TRUCK . 142))converting CNFclause count: 82338variable count: 210adding clausescalling zchaffparsing resultSLAF time: 2.469Inference time: 8.406Learned model:(WALK NEEDS (AT ?DRIVER ?LOC-FROM))(WALK NEEDS (NOT (AT ?DRIVER ?LOC-TO)))(WALK CAUSES (AT ?DRIVER ?LOC-TO))(WALK CAUSES (NOT (AT ?DRIVER ?LOC-FROM)))(WALK KEEPS (PATH ?LOC-FROM ?LOC-FROM))(WALK KEEPS (PATH ?LOC-TO ?LOC-TO))(WALK KEEPS (PATH ?LOC-TO ?LOC-FROM))(WALK KEEPS (PATH ?LOC-FROM ?LOC-TO))(DRIVE-TRUCK NEEDS (NOT (AT ?TRUCK ?LOC-TO)))(DRIVE-TRUCK NEEDS (AT ?TRUCK ?LOC-FROM))(DRIVE-TRUCK CAUSES (AT ?TRUCK ?LOC-TO))(DRIVE-TRUCK CAUSES (NOT (AT ?TRUCK ?LOC-FROM)))(DRIVE-TRUCK KEEPS (AT ?DRIVER ?LOC-TO))(DRIVE-TRUCK KEEPS (AT ?DRIVER ?LOC-FROM))(DRIVE-TRUCK KEEPS (DRIVING ?DRIVER ?TRUCK))(DRIVE-TRUCK KEEPS (PATH ?LOC-TO ?LOC-TO))(DRIVE-TRUCK KEEPS (PATH ?LOC-FROM ?LOC-FROM))(DRIVE-TRUCK KEEPS (PATH ?LOC-FROM ?LOC-TO))(DRIVE-TRUCK KEEPS (PATH ?LOC-TO ?LOC-FROM))(DRIVE-TRUCK KEEPS (EMPTY ?TRUCK))(DISEMBARK-TRUCK NEEDS (NOT (AT ?DRIVER ?LOC)))(DISEMBARK-TRUCK NEEDS (DRIVING ?DRIVER ?TRUCK))(DISEMBARK-TRUCK NEEDS (NOT (EMPTY ?TRUCK)))(DISEMBARK-TRUCK CAUSES (AT ?DRIVER ?LOC))(DISEMBARK-TRUCK CAUSES (NOT (DRIVING ?DRIVER ?TRUCK)))(DISEMBARK-TRUCK CAUSES (EMPTY ?TRUCK))(DISEMBARK-TRUCK KEEPS (AT ?TRUCK ?LOC))(DISEMBARK-TRUCK KEEPS (PATH ?LOC ?LOC))(BOARD-TRUCK NEEDS (AT ?DRIVER ?LOC))(BOARD-TRUCK NEEDS (NOT (DRIVING ?DRIVER ?TRUCK)))(BOARD-TRUCK NEEDS (EMPTY ?TRUCK))(BOARD-TRUCK CAUSES (NOT (AT ?DRIVER ?LOC)))(BOARD-TRUCK CAUSES (DRIVING ?DRIVER ?TRUCK))(BOARD-TRUCK CAUSES (NOT (EMPTY ?TRUCK)))(BOARD-TRUCK KEEPS (AT ?TRUCK ?LOC))(BOARD-TRUCK KEEPS (PATH ?LOC ?LOC))(UNLOAD-TRUCK NEEDS (NOT (AT ?OBJ ?LOC)))(UNLOAD-TRUCK NEEDS (IN ?OBJ ?TRUCK))(UNLOAD-TRUCK CAUSES (AT ?OBJ ?LOC))391fiA MIR & C HANG(UNLOAD-TRUCK CAUSES (NOT (IN ?OBJ ?TRUCK)))(UNLOAD-TRUCK KEEPS (AT ?TRUCK ?LOC))(UNLOAD-TRUCK KEEPS (PATH ?LOC ?LOC))(UNLOAD-TRUCK KEEPS (EMPTY ?TRUCK))(LOAD-TRUCK NEEDS (AT ?OBJ ?LOC))(LOAD-TRUCK NEEDS (NOT (IN ?OBJ ?TRUCK)))(LOAD-TRUCK CAUSES (NOT (AT ?OBJ ?LOC)))(LOAD-TRUCK CAUSES (IN ?OBJ ?TRUCK))(LOAD-TRUCK KEEPS (AT ?TRUCK ?LOC))(LOAD-TRUCK KEEPS (PATH ?LOC ?LOC))(LOAD-TRUCK KEEPS (EMPTY ?TRUCK))C.2 Zeno-Travel DomainZeno-Travel domain following generating PDDL:(define (domain zeno-travel)(:requirements :typing)(:types aircraft person city flevel - object)(:predicates (at ?x - (either person aircraft) ?c - city)(in ?p - person ?a - aircraft)(fuel-level ?a - aircraft ?l - flevel)(next ?l1 ?l2 - flevel))(:action board:parameters (?p - person ?a - aircraft ?c - city):precondition (and (at ?p ?c) (at ?a ?c)):effect (and (not (at ?p ?c)) (in ?p ?a)))(:action debark:parameters (?p - person ?a - aircraft ?c - city):precondition (and (in ?p ?a) (at ?a ?c)):effect (and (not (in ?p ?a)) (at ?p ?c)))(:action fly:parameters (?a - aircraft ?c1 ?c2 - city ?l1 ?l2 - flevel):precondition (and (at ?a ?c1) (fuel-level ?a ?l1) (next ?l2 ?l1)):effect (and (not (at ?a ?c1)) (at ?a ?c2) (not (fuel-level ?a ?l1))(fuel-level ?a ?l2)))(:action zoom:parameters (?a - aircraft ?c1 ?c2 - city ?l1 ?l2 ?l3 - flevel):precondition (and (at ?a ?c1) (fuel-level ?a ?l1) (next ?l2 ?l1)(next ?l3 ?l2) ):effect (and (not (at ?a ?c1)) (at ?a ?c2) (not (fuel-level ?a ?l1))(fuel-level ?a ?l3) ))(:action refuel:parameters (?a - aircraft ?c - city ?l - flevel ?l1 - flevel):precondition (and (fuel-level ?a ?l) (next ?l ?l1) (at ?a ?c)):effect (and (fuel-level ?a ?l1) (not (fuel-level ?a ?l)))))One learned model (one possible satisfying model formula) random-sequenceinput Zeno-Travel domain following (brought together experimental parameters).Zenotravel domain:392fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELS*******IPC3 problem 991 fluents, 21000 possible unique actions1000 actions learned action sequence5 observed fluents per step"schematized" learning1:1 precondition heuristicsAction distribution: ((ZOOM . 27) (FLY . 216) (REFUEL . 264)(BOARD . 249) (DEBARK . 244))converting CNFclause count: 71119variable count: 138adding clausescalling zchaffparsing resultSLAF time: 1.109Inference time: 11.015Learned model:(REFUEL NEEDS (FUEL-LEVEL ?A ?L))(REFUEL NEEDS (NOT (FUEL-LEVEL ?A ?L1)))(REFUEL CAUSES (NOT (FUEL-LEVEL ?A ?L)))(REFUEL CAUSES (FUEL-LEVEL ?A ?L1))(REFUEL KEEPS (NEXT ?L ?L))(REFUEL KEEPS (NEXT ?L ?L1))(REFUEL KEEPS (NEXT ?L1 ?L))(REFUEL KEEPS (NEXT ?L1 ?L1))(ZOOM NEEDS (NOT (FUEL-LEVEL ?A ?L3)))(ZOOM NEEDS (FUEL-LEVEL ?A ?L1))(ZOOM CAUSES (FUEL-LEVEL ?A ?L3))(ZOOM CAUSES (NOT (FUEL-LEVEL ?A ?L1)))(ZOOM KEEPS (FUEL-LEVEL ?A ?L2))(ZOOM KEEPS (NEXT ?L3 ?L3))(ZOOM KEEPS (NEXT ?L3 ?L2))(ZOOM KEEPS (NEXT ?L3 ?L1))(ZOOM KEEPS (NEXT ?L2 ?L3))(ZOOM KEEPS (NEXT ?L2 ?L2))(ZOOM KEEPS (NEXT ?L2 ?L1))(ZOOM KEEPS (NEXT ?L1 ?L3))(ZOOM KEEPS (NEXT ?L1 ?L2))(ZOOM KEEPS (NEXT ?L1 ?L1))(FLY NEEDS (NOT (FUEL-LEVEL ?A ?L2)))(FLY NEEDS (FUEL-LEVEL ?A ?L1))(FLY CAUSES (FUEL-LEVEL ?A ?L2))(FLY CAUSES (NOT (FUEL-LEVEL ?A ?L1)))(FLY KEEPS (NEXT ?L2 ?L2))(FLY KEEPS (NEXT ?L2 ?L1))(FLY KEEPS (NEXT ?L1 ?L2))(FLY KEEPS (NEXT ?L1 ?L1))(DEBARK NEEDS (IN ?P ?A))(DEBARK CAUSES (NOT (IN ?P ?A)))(BOARD NEEDS (NOT (IN ?P ?A)))(BOARD CAUSES (IN ?P ?A))393fiA MIR & C HANGC.3 Blocks-World DomainBlocksworld domain following generating PDDL:(define (domain blocksworld)(:requirements :strips)(:predicates (clear ?x - object)(on-table ?x - object)(arm-empty)(holding ?x - object)(on ?x ?y - object))(:action pickup:parameters (?ob - object):precondition (and (clear ?ob) (on-table ?ob) (arm-empty)):effect (and (holding ?ob) (not (clear ?ob)) (not (on-table ?ob))(not (arm-empty))))(:action putdown:parameters (?ob - object):precondition (holding ?ob):effect (and (clear ?ob) (arm-empty) (on-table ?ob)(not (holding ?ob))))(:action stack:parameters (?ob - object?underob - object):precondition (and (clear ?underob) (holding ?ob)):effect (and (arm-empty) (clear ?ob) (on ?ob ?underob)(not (clear ?underob)) (not (holding ?ob))))(:action unstack:parameters (?ob - object?underob - object):precondition (and (on ?ob ?underob) (clear ?ob) (arm-empty)):effect (and (holding ?ob) (clear ?underob) (not (on ?ob ?underob))(not (clear ?ob)) (not (arm-empty)))))One learned model (one possible satisfying model formula) random-sequenceinput Blocksworld domain following (brought together experimental parameters).Blocksworld domain:* 209 fluents* 1000 randomly selected actions* 10 fluents observed per step* "schematized" learning* 1:1 precondition heuristicsconverting CNFclause count: 235492variable count: 187adding clausescalling zchaffparsing resultSLAF time: 2.203Inference time: 42.312394fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSLearned model:(UNSTACK NEEDS (NOT (CLEAR ?UNDEROB)))(UNSTACK NEEDS (CLEAR ?OB))(UNSTACK NEEDS (ARM-EMPTY))(UNSTACK NEEDS (NOT (HOLDING ?OB)))(UNSTACK NEEDS (ON ?OB ?UNDEROB))(UNSTACK CAUSES (CLEAR ?UNDEROB))(UNSTACK CAUSES (NOT (CLEAR ?OB)))(UNSTACK CAUSES (NOT (ARM-EMPTY)))(UNSTACK CAUSES (HOLDING ?OB))(UNSTACK CAUSES (NOT (ON ?OB ?UNDEROB)))(UNSTACK KEEPS (ON-TABLE ?UNDEROB))(UNSTACK KEEPS (ON-TABLE ?OB))(UNSTACK KEEPS (HOLDING ?UNDEROB))(UNSTACK KEEPS (ON ?UNDEROB ?UNDEROB))(UNSTACK KEEPS (ON ?OB ?OB))(UNSTACK KEEPS (ON ?UNDEROB ?OB))(STACK NEEDS (CLEAR ?UNDEROB))(STACK NEEDS (NOT (CLEAR ?OB)))(STACK NEEDS (NOT (ARM-EMPTY)))(STACK NEEDS (HOLDING ?OB))(STACK NEEDS (NOT (ON ?OB ?UNDEROB)))(STACK CAUSES (NOT (CLEAR ?UNDEROB)))(STACK CAUSES (CLEAR ?OB))(STACK CAUSES (ARM-EMPTY))(STACK CAUSES (NOT (HOLDING ?OB)))(STACK CAUSES (ON ?OB ?UNDEROB))(STACK KEEPS (ON-TABLE ?UNDEROB))(STACK KEEPS (ON-TABLE ?OB))(STACK KEEPS (HOLDING ?UNDEROB))(STACK KEEPS (ON ?UNDEROB ?UNDEROB))(STACK KEEPS (ON ?OB ?OB))(STACK KEEPS (ON ?UNDEROB ?OB))(PUTDOWN NEEDS (NOT (CLEAR ?OB)))(PUTDOWN NEEDS (NOT (ON-TABLE ?OB)))(PUTDOWN NEEDS (NOT (ARM-EMPTY)))(PUTDOWN NEEDS (HOLDING ?OB))(PUTDOWN CAUSES (CLEAR ?OB))(PUTDOWN CAUSES (ON-TABLE ?OB))(PUTDOWN CAUSES (ARM-EMPTY))(PUTDOWN CAUSES (NOT (HOLDING ?OB)))(PUTDOWN KEEPS (ON ?OB ?OB))(PICKUP NEEDS (CLEAR ?OB))(PICKUP NEEDS (ON-TABLE ?OB))(PICKUP NEEDS (ARM-EMPTY))(PICKUP NEEDS (NOT (HOLDING ?OB)))(PICKUP CAUSES (NOT (CLEAR ?OB)))(PICKUP CAUSES (NOT (ON-TABLE ?OB)))(PICKUP CAUSES (NOT (ARM-EMPTY)))(PICKUP CAUSES (HOLDING ?OB))(PICKUP KEEPS (ON ?OB ?OB))395fiA MIR & C HANGC.4 Depot DomainDepot domain following generating PDDL:(define (domain Depot)(:requirements :typing)(:types place locatable - objectdepot distributor - placetruck hoist surface - locatablepallet crate - surface)(:predicates (at ?x - locatable ?y - place)(on ?x - crate ?y - surface)(in ?x - crate ?y - truck)(lifting ?x - hoist ?y - crate)(available ?x - hoist)(clear ?x - surface))(:action Drive:parameters (?x - truck ?y - place ?z - place):precondition (and (at ?x ?y)):effect (and (not (at ?x ?y)) (at ?x ?z)))(:action Lift:parameters (?x - hoist ?y - crate ?z - surface ?p - place):precondition (and (at ?x ?p) (available ?x) (at ?y ?p) (on ?y ?z)(clear ?y)):effect (and (not (at ?y ?p)) (lifting ?x ?y) (not (clear ?y))(not (available ?x)) (clear ?z) (not (on ?y ?z))))(:action Drop:parameters (?x - hoist ?y - crate ?z - surface ?p - place):precondition (and (at ?x ?p) (at ?z ?p) (clear ?z) (lifting ?x ?y)):effect (and (available ?x) (not (lifting ?x ?y)) (at ?y ?p)(not (clear ?z)) (clear ?y) (on ?y ?z)))(:action Load:parameters (?x - hoist ?y - crate ?z - truck ?p - place):precondition (and (at ?x ?p) (at ?z ?p) (lifting ?x ?y)):effect (and (not (lifting ?x ?y)) (in ?y ?z) (available ?x)))(:action Unload:parameters (?x - hoist ?y - crate ?z - truck ?p - place):precondition (and (at ?x ?p) (at ?z ?p) (available ?x) (in ?y ?z)):effect (and (not (in ?y ?z)) (not (available ?x)) (lifting ?x ?y))) )One learned model (one possible satisfying model formula) random-sequenceinput Depot domain following (brought together experimental parameters).Depots domain:* IPC3 problem 5* 250 fluents* 1000 randomly selected actions* 10 fluents observed per step* "schematized" learning* 1:1 precondition heuristicsconverting CNF396fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSclause count: 85359variable count: 236adding clausescalling zchaffparsing resultSLAF time: 2.797Inference time: 8.062Learned model:(UNLOAD NEEDS (IN ?Y ?Z))(UNLOAD NEEDS (NOT (LIFTING ?X ?Y)))(UNLOAD NEEDS (AVAILABLE ?X))(UNLOAD CAUSES (NOT (IN ?Y ?Z)))(UNLOAD CAUSES (LIFTING ?X ?Y))(UNLOAD CAUSES (NOT (AVAILABLE ?X)))(UNLOAD KEEPS (AT ?Z ?P))(UNLOAD KEEPS (AT ?Y ?P))(UNLOAD KEEPS (AT ?X ?P))(UNLOAD KEEPS (ON ?Y ?Y))(UNLOAD KEEPS (CLEAR ?Y))(LOAD NEEDS (NOT (IN ?Y ?Z)))(LOAD NEEDS (LIFTING ?X ?Y))(LOAD NEEDS (NOT (AVAILABLE ?X)))(LOAD CAUSES (IN ?Y ?Z))(LOAD CAUSES (NOT (LIFTING ?X ?Y)))(LOAD CAUSES (AVAILABLE ?X))(LOAD KEEPS (AT ?Z ?P))(LOAD KEEPS (AT ?Y ?P))(LOAD KEEPS (AT ?X ?P))(LOAD KEEPS (ON ?Y ?Y))(LOAD KEEPS (CLEAR ?Y))(DROP NEEDS (NOT (AT ?Y ?P)))(DROP NEEDS (NOT (ON ?Y ?Z)))(DROP NEEDS (LIFTING ?X ?Y))(DROP NEEDS (NOT (AVAILABLE ?X)))(DROP NEEDS (CLEAR ?Z))(DROP NEEDS (NOT (CLEAR ?Y)))(DROP CAUSES (AT ?Y ?P))(DROP CAUSES (ON ?Z ?Z))(DROP CAUSES (NOT (ON ?Z ?Z)))(DROP CAUSES (ON ?Z ?Y))(DROP CAUSES (NOT (ON ?Z ?Y)))(DROP CAUSES (ON ?Y ?Z))(DROP CAUSES (NOT (LIFTING ?X ?Y)))(DROP CAUSES (LIFTING ?X ?Z))(DROP CAUSES (NOT (LIFTING ?X ?Z)))(DROP CAUSES (AVAILABLE ?X))(DROP CAUSES (NOT (CLEAR ?Z)))(DROP CAUSES (CLEAR ?Y))(DROP KEEPS (AT ?Z ?P))(DROP KEEPS (AT ?X ?P))(DROP KEEPS (ON ?Z ?Z))(DROP KEEPS (ON ?Z ?Y))(DROP KEEPS (ON ?Y ?Y))(DROP KEEPS (LIFTING ?X ?Z))397fiA MIR & C HANG(LIFT NEEDS (AT ?Y ?P))(LIFT NEEDS (ON ?Y ?Z))(LIFT NEEDS (NOT (LIFTING ?X ?Y)))(LIFT NEEDS (AVAILABLE ?X))(LIFT NEEDS (NOT (CLEAR ?Z)))(LIFT NEEDS (CLEAR ?Y))(LIFT CAUSES (NOT (AT ?Y ?P)))(LIFT CAUSES (NOT (ON ?Y ?Z)))(LIFT CAUSES (ON ?Z ?Z))(LIFT CAUSES (NOT (ON ?Z ?Z)))(LIFT CAUSES (ON ?Z ?Y))(LIFT CAUSES (NOT (ON ?Z ?Y)))(LIFT CAUSES (LIFTING ?X ?Y))(LIFT CAUSES (LIFTING ?X ?Z))(LIFT CAUSES (NOT (LIFTING ?X ?Z)))(LIFT CAUSES (NOT (AVAILABLE ?X)))(LIFT CAUSES (CLEAR ?Z))(LIFT CAUSES (NOT (CLEAR ?Y)))(LIFT KEEPS (AT ?Z ?P))(LIFT KEEPS (AT ?X ?P))(LIFT KEEPS (ON ?Y ?Y))(LIFT KEEPS (ON ?Z ?Z))(LIFT KEEPS (ON ?Z ?Y))(LIFT KEEPS (LIFTING ?X ?Z))(DRIVE NEEDS (AT ?X ?Y))(DRIVE NEEDS (NOT (AT ?X ?Z)))(DRIVE CAUSES (NOT (AT ?X ?Y)))(DRIVE CAUSES (AT ?X ?Z))ReferencesAmir, E. (2005). Learning partially observable deterministic action models. Proc. NineteenthInternational Joint Conference Artificial Intelligence (IJCAI 05), pp. 14331439. International Joint Conferences Artificial Intelligence.Amir, E., & Russell, S. (2003). Logical filtering. Proc. Eighteenth International Joint ConferenceArtificial Intelligence (IJCAI 03), pp. 7582. Morgan Kaufmann.Benson, S. (1995). Inductive learning reactive action models. Proceedings 12th International Conference Machine Learning (ICML-95).Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming first-order MDPs.Proc. Seventeenth International Joint Conference Artificial Intelligence (IJCAI 01), pp.690697. Morgan Kaufmann.Boyen, X., Friedman, N., & Koller, D. (1999). Discovering hidden structure complex dynamicsystems. Proceedings 15th Conference Uncertainty Artificial IntelligenceUAI1999, pp. 91100. Morgan Kaufmann. Available http://www.cs.stanford.edu/ xb/uai99/.Boyen, X., & Koller, D. (1999). Approximate learning dynamic models. Kearns, M. S., Solla,S. A., & Kohn, D. A. (Eds.), Advances Neural Information Processing Systems 11: Proceedings 1998 ConferenceNIPS 1998, pp. 396402. Cambridge: MIT Press. Available http://www.cs.stanford.edu/ xb/nips98/.398fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSCalvanese, D., Giacomo, G. D., & Vardi, M. Y. (2002). Reasoning actions planningLTL action theories. Principles Knowledge Representation Reasoning: Proc. EighthIntl Conference (KR 2002), pp. 593602. Morgan Kaufmann.Chang, A., & Amir, E. (2006). Goal achievement partially known, partially observable domains.Proceedings 16th Intl Conf. Automated Planning Scheduling (ICAPS06).AAAI Press.Chang, C.-L., & Lee, R. C.-T. (1973). Symbolic Logic Mechanical Theorem Proving. AcademicPress.Clarke, E. M., Grumberg, O., & Peled, D. A. (1999). Model Checking. MIT Press.Darwiche, A., & Marquis, P. (2002). knowledge compilation map. Journal Artificial Intelligence Research, 17, 229264.Davis, M., & Putnam, H. (1960). computing procedure quantification theory. JournalACM, 7, 201215.Dawsey, W., Minsker, B., & Amir, E. (2007). Real-time assessment drinking water systems usingBayesian networks. World Environmental Water Resources Congress.Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial Intelligence,113(12), 4185.del Val, A. (1999). new method consequence finding compilation restricted language. Proc. National Conference Artificial Intelligence (AAAI 99), pp. 259264.AAAI Press/MIT Press.Doherty, P., Lukaszewicz, W., & Szalas, A. (1997). Computing circumscription revisited: reduction algorithm. Journal Automated Reasoning, 18(3), 297336.Eiter, T., & Gottlob, G. (1992). complexity propositional knowledge base revision, updates, counterfactuals. Artificial Intelligence, 57(2-3), 227270.Even-Dar, E., Kakade, S. M., & Mansour, Y. (2005). Reinforcement learning POMDPs.Proc. Nineteenth International Joint Conference Artificial Intelligence (IJCAI 05), pp.660665. International Joint Conferences Artificial Intelligence.Fagin, R., Ullman, J. D., & Vardi, M. Y. (1983). semantics updates databases.Proceedings Second ACM SIGACT-SIGMOD Symposium Principles DatabaseSystems, pp. 352365, Atlanta, Georgia.Fikes, R., Hart, P., & Nilsson, N. (1972). Learning executing generalized robot plans. ArtificialIntelligence, 3, 251288.Fox, M., & Long, D. (2002). PDDL2.1: extension PDDL expressing temporal planningdomains. http://www.dur.ac.uk/d.p.long/IPC/pddl.html. Used AIPS02 competition.Friedman, N., Murphy, K., & Russell, S. (1998). Learning structure dynamic probabilisticnetworks. Proc. Fourteenth Conference Uncertainty Artificial Intelligence (UAI 98).Morgan Kaufmann.Gelfond, M., & Lifschitz, V. (1998). Action languages. Electronic Transactions Artificial Intelligence (http://www.etaij.org), 3, nr 16.399fiA MIR & C HANGGhahramani, Z. (2001). introduction Hidden Markov Models Bayesian networks. International Journal Pattern Recognition Artificial Intelligence, 15(1), 942.Ghahramani, Z., & Jordan, M. I. (1997). Factorial hidden markov models. Machine Learning, 29,245275.Ghallab, M., Howe, A., Knoblock, C., McDermott, D., Ram, A., Veloso, M., Weld, D., & Wilkins,D. (1998). PDDL Planning Domain Definition Language, version 1.2. Tech. rep. CVCTR-98-003/DCS TR-1165, Yale center computational vision control.Gil, Y. (1994). Learning experimentation: Incremental refinement incomplete planning domains. Proceedings 11th International Conference Machine Learning (ICML-94),pp. 1013.Ginsberg, M. L. (1987). Readings Nonmonotonic Reasoning, chap. 1, pp. 123. Morgan Kaufmann, Los Altos, CA.Hajishirzi, H., & Amir, E. (2007). Stochastic filtering probabilistic action models. Proc. National Conference Artificial Intelligence (AAAI 07).Hill, D. J., Minsker, B., & Amir, E. (2007). Real-time Bayesian anomaly detection environmentalsensor data. 32nd Congress International Association Hydraulic EngineeringResearch (IAHR 2007).Hlubocky, B., & Amir, E. (2004). Knowledge-gathering agents adventure games. AAAI-04Workshop Challenges Game AI. AAAI Press.Holmes, M. P., & Charles Lee Isbell, J. (2006). Looping suffix tree-based inference partiallyobservable hidden state. Proceedings 23rd International Conference MachineLearning (ICML-06), pp. 409416. ACM Press.Iwanuma, K., & Inoue, K. (2002). Minimal answer computation sol. Logics ArtificialIntelligence: Proceedings Eighth European Conference, Vol. 2424 LNAI, pp. 245257. Springer-Verlag.James, M., & Singh, S. (2004). Learning discovery predictive state representations dynamical systems reset. Proceedings 21st International Conference MachineLearning (ICML-04), pp. 417424. ACM Press.Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting partiallyobservable stochastic domains. Artificial Intelligence, 101, 99134.Kearns, M., Mansour, Y., & Ng, A. Y. (2000). Approximate planning large POMDPs via reusabletrajectories. Proceedings 12th Conference Neural Information Processing Systems(NIPS98), published 1999, pp. 10011007. MIT Press.Kuffner., J. J., & LaValle, S. M. (2000). Rrt-connect: efficient approach single-query pathplanning.. IEEE International Conference Robotics Automation (ICRA), pp. 9951001.Lee, R. C.-T. (1967). Completeness Theorem Computer Program Finding TheoremsDerivable Given Axioms. Ph.D. thesis, University California, Berkeley.Lifschitz, V. (1990). semantics STRIPS. Allen, J. F., Hendler, J., & Tate, A. (Eds.),Readings Planning, pp. 523530. Morgan Kaufmann, San Mateo, California.400fiL EARNING PARTIALLY BSERVABLE ETERMINISTIC ACTION ODELSLittman, M. L. (1996). Algorithms sequential decision making. Ph.D. thesis, DepartmentComputer Science, Brown University. Technical report CS-96-09.Littman, M. L., Sutton, R., & Singh, S. (2002). Predictive representations state. Proceedings15th Conference Neural Information Processing Systems (NIPS01), published 2002.MIT Press.Marquis, P. (2000). Consequence finding algorithms. Gabbay, D., & Smets, P. (Eds.), Handbook Defeasible Reasoning Uncertainty Management Systems, Vol. 5: Algorithmsdefeasible uncertain reasoning. Kluwer.McCallum, R. A. (1995). Instance-based utile distinctions reinforcement learning hiddenstate. Proceedings 12th International Conference Machine Learning (ICML-95).Morgan Kaufmann.McCarthy, J. (1986). Applications Circumscription Formalizing Common Sense Knowledge.Artificial Intelligence, 28, 89116.McCarthy, J., & Hayes, P. J. (1969). philosophical problems standpoint artificial intelligence. Meltzer, B., & Michie, D. (Eds.), Machine Intelligence 4, pp. 463502.Edinburgh University Press.McIlraith, S., & Amir, E. (2001). Theorem proving structured theories. Proc. SeventeenthInternational Joint Conference Artificial Intelligence (IJCAI 01), pp. 624631. MorganKaufmann.Meuleau, N., Peshkin, L., Kim, K.-E., & Kaelbling, L. P. (1999). Learning finite-state controllerspartially observable environments. Proc. Fifteenth Conference Uncertainty ArtificialIntelligence (UAI 99). Morgan Kaufmann.Moskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: EngineeringEfficient SAT Solver. Proceedings 38th Design Automation Conference (DAC01).Murphy, K. (2002). Dynamic Bayesian Networks: Representation, Inference Learning. Ph.D.thesis, University California Berkeley.Nance, M., Vogel, A., & Amir, E. (2006). Reasoning partially observed actions. Proc. National Conference Artificial Intelligence (AAAI 06). AAAI Press.Oates, T., & Cohen, P. R. (1996). Searching planning operators context-dependentprobabilistic effects. Proc. National Conference Artificial Intelligence (AAAI 96), pp.863868. AAAI Press.Pasula, H. M., Zettlemoyer, L. S., & Kaelbling, L. P. (2004). Learning probabilistic relationalplanning rules. Proceedings 14th Intl Conf. Automated Planning Scheduling(ICAPS04). AAAI Press.Pednault, E. P. D. (1989). ADL: exploring middle ground STRIPS situationcalculus. Proc. First International Conference Principles Knowledge RepresentationReasoning (KR 89), pp. 324332.Reiter, R. (2001). Knowledge Action: Logical Foundations Describing ImplementingDynamical Systems. MIT Press.401fiA MIR & C HANGReiter, R. (1991). frame problem situation calculus: simple solution (sometimes)completeness result goal regression. Lifschitz, V. (Ed.), Artificial IntelligenceMathematical Theory Computation (Papers Honor John McCarthy), pp. 359380.Academic Press.Robert, C. P., Celeux, G., & Diebolt, J. (1993). Bayesian estimation hidden Markov chains:stochastic implementation. Statist. Prob. Letters, 16, 7783.Schmill, M. D., Oates, T., & Cohen, P. R. (2000). Learning planning operators real-world, partially observable environments. Proceedings 5th Intl Conf. AI PlanningScheduling (AIPS00), pp. 246253. AAAI Press.Shahaf, D., & Amir, E. (2006). Learning partially observable action schemas. Proc. NationalConference Artificial Intelligence (AAAI 06). AAAI Press.Shahaf, D., & Amir, E. (2007). Logical circuit filtering. Proc. Twentieth International Joint Conference Artificial Intelligence (IJCAI 07), pp. 26112618. International Joint ConferencesArtificial Intelligence.Shahaf, D., Chang, A., & Amir, E. (2006). Learning partially observable action models: Efficientalgorithms. Proc. National Conference Artificial Intelligence (AAAI 06). AAAI Press.Simon, L., & del Val, A. (2001). Efficient consequence-finding. Proc. Seventeenth InternationalJoint Conference Artificial Intelligence (IJCAI 01), pp. 359365. Morgan Kaufmann.Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: introduction. MIT Press.Tang, D., Yinlei Yu, D. R., & Malik, S. (2004). Analysis search based algorithms satisfiability quantified boolean formulas arising circuit state space diameter problems.Proceedings Seventh International Conference Theory Applications Satisfiability Testing (SAT2004).Thielscher, M. (1998). Introduction fluent calculus. Electronic Transactions ArtificialIntelligence (http://www.etaij.org), 3, nr 14.Thrun, S. (2003). Robotic mapping: survey. Exploring artificial intelligence new millennium, pp. 135. Morgan Kaufmann.Wang, X. (1995). Learning observation practice: incremental approach planning operator acquisition. Proceedings 12th International Conference Machine Learning(ICML-95), pp. 549557. Morgan Kaufmann.Wu, K., Yang, Q., & Jiang, Y. (2007). Arms: automatic knowledge engineering tool learningaction models ai planning. Knowledge Engineering Review, 22(2), 135152.Yang, Q., Wu, K., & Jiang, Y. (2005). Learning actions models plan examples incompleteknowledge.. Biundo, S., Myers, K. L., & Rajan, K. (Eds.), ICAPS, pp. 241250. AAAI.402fiJournal Artificial Intelligence Research 33 (2008) 223-258Submitted 5/08; published 10/08Completeness Performance APO AlgorithmTal GrinshpounAmnon Meiselsgrinshpo@cs.bgu.ac.ilam@cs.bgu.ac.ilBen-Gurion University NegevDepartment Computer ScienceBeer-Sheva, IsraelAbstractAsynchronous Partial Overlay (APO) search algorithm uses cooperative mediation solve Distributed Constraint Satisfaction Problems (DisCSPs). algorithmpartitions search different subproblems DisCSP. original proof completeness APO algorithm based growth size subproblems.present paper demonstrates expected growth subproblems occursituations, leading termination problem algorithm. problematic partsAPO algorithm interfere completeness identified necessarymodifications algorithm fix problematic parts given. resultingversion algorithm, Complete Asynchronous Partial Overlay (CompAPO), ensurescompleteness. Formal proofs soundness completeness CompAPOgiven. detailed performance evaluation CompAPO comparing DisCSPalgorithms presented, along extensive experimental evaluation algorithmsunique behavior. Additionally, optimization version algorithm, CompOptAPO,presented, discussed, evaluated.1. IntroductionAlgorithms solve Distributed Constraint Satisfaction Problems (DisCSPs) attemptachieve concurrency problem solving order utilize distributed natureproblems. Distributed backtracking, forms majority DisCSP algorithms,take many forms. Asynchronous Backtracking (ABT) (Yokoo, Durfee, Ishida, & Kuwabara,1998; Yokoo & Hirayama, 2000), Asynchronous Forward-Checking (AFC) (Meisels & Zivan, 2007), Concurrent Dynamic Backtracking (ConcDB) (Zivan & Meisels, 2006a)representative examples family distributed backtracking algorithms.algorithms maintain one partial solutions DisCSP attempt extendpartial solution complete one. ABT algorithm attempts achieve concurrencyasynchronously assigning values variables. AFC algorithm performs valueassignments synchronously, achieves concurrency performing asynchronous computation form forward checking. ConcDB algorithm concurrently attemptsextend multiple partial solutions, scanning different parts search space.completely different approach achieve concurrency merging partialsolutions complete one. inherent concurrency merging partial solutions makesfascinating paradigm solving DisCSPs. However, approach prone manyerrors deadlocks could prevent termination, failures could occur attemptmerge partial solutions. Consequently, hard develop algorithmc2008AI Access Foundation. rights reserved.fiGrinshpoun & Meiselscorrect well performing. recently published algorithm, AsynchronousPartial Overlay (APO) (Mailler, 2004; Mailler & Lesser, 2006), attempts solve DisCSPsmerging partial solutions. uses concept mediation centralize searchprocedure different parts DisCSP. Due unique approach, several researchersalready proposed changes modifications APO algorithm (Benisch & Sadeh,2006; Semnani & Zamanifar, 2007). Unfortunately, none studies examinedcompleteness APO. Additionally, distinctive behavior APO algorithm callsthorough experimental evaluation. present paper presents in-depth investigationcompleteness termination APO algorithm, constructs correct versionalgorithm CompAPO goes present extensive experimental evaluationcomplete APO algorithm.APO algorithm partitions agents groups attempt find consistentpartial solutions. partition mechanism dynamic search enables dynamicchange groups. key factor termination (and consequently completeness)APO algorithm presented original correctness proof (Mailler & Lesser, 2006)monotonic growth initially partitioned groups search. growth arisessubproblems overlap, allowing agents increase size subproblemssolve. discovered expected growth groups occursituations, leading termination problem APO algorithm. Nevertheless,unique way APO attempts solve DisCSPs encouraged us try fix it.termination problem APO algorithm shown section 4 constructing scenario leads infinite loop algorithms run (Grinshpoun, Zazon,Binshtok, & Meisels, 2007). running example essential understandingAPOs completeness problem, since algorithm complex. help understandproblem, full pseudo-code APO follows closely original presentationalgorithm (Mailler & Lesser, 2006) given. erroneous part proof APOscompleteness presented Mailler Lesser (2006) shown problematic partsalgorithm interfere completeness identified. Necessary modificationsalgorithm proposed, order fix problematic parts. resulting versionalgorithm ensures completeness, termed Complete Asynchronous PartialOverlay (CompAPO) (Grinshpoun & Meisels, 2007). Formal proofs soundnesscompleteness CompAPO presented.modifications CompAPO may potentially affect performance algorithm.Also, evaluation original APO algorithm (Mailler & Lesser, 2006), compared AWC algorithm (Yokoo, 1995), efficient DisCSP solver (Zivan,Zazone, & Meisels, 2007). Moreover, tests work Mailler Lesser (2006)performed relatively sparse problems, comparison AWCmade use problematic measures. extensive experimental evaluationCompAPO compares performance DisCSP search algorithms randomlygenerated DisCSPs. experiments show CompAPO performs significantly different DisCSP algorithms, surprising considering singular wayproblem solving.Asynchronous Partial Overlay actually family algorithms. completenesstermination problems presented corrected present study applymembers family. OptAPO algorithm (Mailler & Lesser, 2004; Mailler, 2004)224fiCompleteness Performance APO Algorithmoptimization version APO solves Distributed Constraint Optimization Problems(DisCOPs). present paper proposes similar modifications APO orderachieve completeness OptAPO. resulting CompOptAPO algorithm evaluatedextensively randomly generated DisCOPs.plan paper follows. DisCSPs presented briefly section 2. Section 3, gives short description APO algorithm along pseudo-code.infinite loop scenario APO described detail section 4 problemslead infinite looping analyzed section 5. Section 6 presents detailed solutionproblem forms CompAPO version algorithm, followed proofssoundness completeness CompAPO (section 7). optimization versionalgorithm, CompOptAPO, presented discussed section 8. extensive performance evaluation CompAPO CompOptAPO section 9. conclusionssummarized section 10.2. Distributed Constraint Satisfactiondistributed constraints satisfaction problem DisCSP, composed set k agentsA1 , A2 , ..., Ak . agent Ai contains set constrained variables xi1 , xi2 , ..., xini . Constraints relations R subsets Cartesian product domains constrained variables. set constrained variables xik , xjl , ..., xmn , domains valuesvariable Dik , Djl , ..., Dmn , constraint defined R Dik Djl ... Dmn .binary constraint Rij two variables xj xi subset Cartesianproduct domains Rij Dj Di . distributed constraint satisfaction problem(DisCSP), agents connected constraints variables belong different agents (Yokoo et al., 1998). addition, agent set constrained variables,i.e. local constraint network.assignment (or label) pair < var, val >, var variable agentval value var domain assigned it. compound label setassignments values set variables. solution DisCSP compound labelincludes variables agents, satisfies constraints. Agents checkassignments values non-local constraints communicating agentssending receiving messages.Current studies DisCSPs follow assumption agents hold exactly one variable (Yokoo & Hirayama, 2000; Bessiere, Maestre, Brito, & Meseguer, 2005). Accordingly,present study often uses variables name xi represent agent belongs (Ai ).addition, following common assumptions used present study:amount time passes sending receiving messagefinite.Messages sent agent Ai agent Aj received Aj order sent.3. Asynchronous Partial OverlayAsynchronous Partial Overlay (APO) algorithm solving DisCSPs appliescooperative mediation. pseudo-code Algorithms 1, 2, 3 follows closelypresentation APO work Mailler Lesser (2006).225fiGrinshpoun & MeiselsAlgorithm 1 APO procedures initialization local resolution.procedure initialize1: di random Di ;2: pi sizeof (neighbors) + 1;3: mi true;4: mediate false;5: add xi good list;6: send (init, (xi , pi , di , mi , Di , Ci )) neighbors;7: init list neighbors;received (init, (xj , pj , dj , mj , Dj , Cj ))1: add (xj , pj , dj , mj , Dj , Cj ) agent view ;2: xj neighbor xk good list3:add xj good list;4:add xl agent view xl/ good list connected good list;5:pi sizeof (good list);6: xj/ init list7:send (init, (xi , pi , di , mi , Di , Ci )) xj ;8: else9:remove xj init list;10: check agent view;received (ok?, (xj , pj , dj , mj ))1: update agent view (xj , pj , dj , mj );2: check agent view;procedure check agent view1: init list 6= mediate 6= false2:return;3: mi hasConf lict(xi );4: mi j(pj > pi mj == true)5:(di Di )(di agent view conflict) di conflicts exclusively lowerpriority neighbors6:di di ;7:send (ok?, (xi , pi , di , mi )) xj agent view;8:else9:mediate;10: else mi 6= mi11:mi mi ;12:send (ok?, (xi , pi , di , mi )) xj agent view;226fiCompleteness Performance APO AlgorithmAlgorithm 2 Procedures mediating APO session choosing solutionAPO mediation.procedure mediate1: pref erences ;2: counter 0;3: xj good list4:send (evaluate?, (xi , pi )) xj ;5:counter counter + 1;6: mediate true;received (wait!, (xj , pj ))1: update agent view (xj , pj );2: counter counter 1;3: counter == 0 choose solution;received (evaluate!, (xj , pj , labeled Dj ))1: record (xj , labeled Dj ) preferences;2: update agent view (xj , pj );3: counter counter 1;4: counter == 0 choose solution;procedure choose solution1: select solution using Branch Bound search that:2:1. satisfies constraints agents good list3:2. minimizes violations agents outside session4: satisfies constraints5:broadcast solution;6: xj agent view7:xj pref erences/ agent view8:dj violates xk xk9:send (init, (xi , pi , di , mi , Di , Ci )) xk ;10:add xk init list;11:send (accept!, (dj , xi , pi , di , mi )) xj ;12:update agent view xj ;13:else14:send (ok?, (xi , pi , di , mi )) xj ;15: mediate false;16: check agent view;227fiGrinshpoun & MeiselsAlgorithm 3 Procedures receiving APO session.received (evaluate?, (xj , pj ))1: mj true;2: mediate == true k(pk > pj mk == true)3:send (wait!, (xi , pi )) xj ;4: else5:mediate true;6:label Di names agents would violated settingdi d;7:send (evaluate!, (xi , pi , labeled Di )) xj ;received (accept!, (d, xj , pj , dj , mj ))1: di d;2: mediate false;3: send (ok?, (xi , pi , di , mi )) xj agent view;4: update agent view (xj , pj , dj , mj );5: check agent view;beginning problem solving, APO algorithm performs initializationphase, neighboring agents exchange data init messages (procedure initialize Algorithm 1). Following that, agents check agent view identify conflictsneighbors (procedure check agent view Algorithm 1).check, agent finds conflict one neighbors, expresses desireact mediator. case agent neighbors wish mediatewider view constraint graph itself, agent successfully assumesrole mediator.Using mediation (Algorithms 2, 3), agents solve subproblems DisCSPconducting internal Branch Bound search (procedure choose solution Algorithm 2). complete solution DisCSP, solutions subproblems mustcompatible. solutions overlapping subproblems conflicts, solving agentsincrease size subproblems work on. original paper (Mailler &Lesser, 2006) uses term preferences describe potential conflicts solutionsoverlapping subproblems. present paper use term external constraintsdescribe conflicts. detailed description APO algorithm foundwork Mailler Lesser (2006).4. Infinite Loop ScenarioConsider 3-coloring problem presented Figure 1 solid lines. agentassign one three available colors Red, Green, Blue. standard inequalityconstraints solid lines represent, four weaker constraints (diagonal dashed lines)added. dashed lines represent constraints allow combinations(Green,Green) (Blue,Blue) assigned agents. Ties priorities agentsbroken using anti-lexicographic ordering names.228fiCompleteness Performance APO AlgorithmA1RedA2RedA8GreenA3BlueA7BlueA4GreenA6RedA5RedFigure 1: constraints graph initial assignments.AgentA1A2A3A4A5A6A7A8ColorRRBGRRBGmim1m2m3m4m5m6m7m8=t=t=f=f=t=t=f=fdjd2d1d1d3d3d5d1d1values=R, d3=R, d3=R, d2=B, d5=B, d4=R, d7=R, d5=R, d7=B, d7 =B, d8 =G=B=R, d4 =G, d5 =R=R=G, d6 =R, d7 =B=B=R, d6 =R, d8 =G=Bmjm2m1m1m3m3m5m1m1values= t, m3 = f , m7 = f , m8 = f= t, m3 = f= t, m2 = t, m4 = f , m5 == f , m5 == f , m4 = f , m6 = t, m7 = f= t, m7 = f= t, m5 = t, m6 = t, m8 = f= t, m7 = fTable 1: Configuration 1.initial selection values agents depicted Figure 1. initial state,two constraints violated (A1 , A2 ) (A5 , A6 ). Assume agents A3 , A4 , A7 ,A8 first complete initialization phase exchanging init messagesneighbors (procedure initialize Algorithm 1). agents conflicts,therefore set mi false send ok? messages neighborsruns check agent view procedure (Algorithm 1). arrival ok?messages agents A3 , A4 , A7 , A8 , agents A1 , A2 , A5 , A6 accept last initmessages neighbors complete initialization phase. Agents A2A6 conflicts, complete check agent view procedure without mediatingchanging state. true, agent views A2 A6 , m1 = truem5 = true, respectively. neighbors higher priority agents A2 A6respectively. denote configuration 1 states agents pointprocessing present configuration Table 1.agents complete initializations, agents A1 A5 detectconflicts, neighbor higher priority wants mediate.Consequently, agents A1 A5 start mediation sessions, since cannot changecolor consistent state neighbors.229fiGrinshpoun & MeiselsAgentA1A2A3A4A5A6A7A8ColorGBRGRRBRmim1m2m3m4m5m6m7m8=f=f=f=f=t=t=f=fdj valuesd2 =B, d3 =R, d7 =B, d8 =Rd1 =G, d3 =Rd1 =G, d2 =B, d4 =G, d5 =R3 =B, d5 =R3 =B, d4 =G, d6 =R, d7 =Bd5 =R, d7 =Bd1 =G, d5 =R, d6 =R, 8 =Gd1 =G, d7 =Bmjm2m1m1m3m3m5m1m1values= f , m3 = f , m7 = f , m8 = f= f , m3 = f= f , m2 = f , m4 = f , m5 == f , m5 == f , m4 = f , m6 = t, m7 = f= t, m7 = f= f , m5 = t, m6 = t, m8 = f= f , m7 = fTable 2: Configuration 2 obsolete data agent views bold face.Let us first observe A1 mediation session. A1 sends evaluate? messages neighbors A2 , A3 , A7 , A8 (procedure mediate Algorithm 2). agents replyevaluate! messages (Algorithm 3). A1 conducts Branch Bound search findsolution satisfies constraints A1 , A2 , A3 , A7 , A8 , also minimizes external constraints (procedure choose solution Algorithm 2). example,A1 finds solution (A1 Green, A2 Blue, A3 Red, A7 Blue, A8 Red),satisfies internal constraints, minimizes zero external constraints. A1 sendsaccept! messages neighbors, informing solution. A2 , A3 , A7 , A8receive accept! messages send ok? messages new states neighbors (Algorithm 3). However, ok? messages A8 A7 A3 A4A5 delayed. Observe algorithm asynchronous naturally dealsscenarios.Concurrently mediation session A1 , agent A5 starts mediationsession. A5 sends evaluate? messages neighbors A3 , A4 , A6 , A7 . Let us assumemessage A7 delayed. A4 A6 receive evaluate? messages replyevaluate!, since know agents higher priority A5 wantmediate. A3 , A1 mediation session, replies wait!. denoteconfiguration 2 states agents point processing (see Table 2).A1 mediation session over, A7 receives delayed evaluate? messageA5 . Since A7 longer mediation session, expect mediation sessionnode higher priority A5 (see A7 view Table 2), agent A7 repliesevaluate!. Notice A7 view d8 obsolete (the ok? message A8 A7 stilldelayed). agent A5 receives evaluate! message A7 , continuemediation session involving agents A4 , A5 , A6 , A7 . Since ok? messages A3A4 A5 also delayed, agent A5 starts mediation session knowledgeagents A3 A8 updated (see bold-faced data Table 2).Agent A5 conducts Branch Bound search find solution satisfiesconstraints A4 , A5 , A6 , A7 , also minimizes external constraints.example, A5 finds solution (A4 Red, A5 Green, A6 Blue, A7 Red),satisfies internal constraints, minimizes zero external constraints (rememberA5 wrong data assignments A3 A8 ). A5 sends accept! messagesA4 , A6 , A7 , informing solution. agents receive messagessend ok? messages new states neighbors. now, delayed230fiCompleteness Performance APO AlgorithmA1GreenA2BlueA8RedA3RedA7RedA4RedA6BlueA5GreenFigure 2: graph configuration 3.AgentA1A2A3A4A5A6A7A8ColorGBRRGBRRmim1m2m3m4m5m6m7m8=f=f=t=t=f=f=t=tdjd2d1d1d3d3d5d1d1values=B, d3 =R, d7 =R, d8 =R=G, d3 =R=G, d2 =B, d4 =R, d5 =G=R, d5 =G=R, d4 =R, d6 =B, d7 =R=G, d7 =R=G, d5 =G, d6 =B, d8 =R=G, d7 =Rmjm2m1m1m3m3m5m1m1values= f , m3 = t, m7 = t, m8 == f , m3 == f , m2 = f , m4 = t, m5 = f= t, m5 = f= t, m4 = t, m6 = f , m7 == f , m7 == f , m5 = f , m6 = f , m8 == f , m7 =Table 3: Configuration 3.messages get destinations, two constraints violated (A3 ,A4 ) (A7 ,A8 ).Consequently, agents A3 , A4 , A7 , A8 want mediate, whereas agents A1 , A2 , A5 ,A6 wish mediate, since conflicts. denote configuration3 states agents A5 solution assigned delayed messagesarrived destinations (see Figure 2 Table 3).now, shown series steps led configuration 1 configuration3. careful look Figures 1 2 reveals configurations actually isomorphic.Consequently, next show similar series steps lead us right backconfiguration 1.Agents A3 A7 detect conflicts neighborhigher priority wants mediate. Consequently, agents A3 A7 start mediationsessions, since cannot change color consistent state neighbors.first observe A3 mediation session. A3 sends evaluate? messagesneighbors A1 , A2 , A4 , A5 . agents reply evaluate! messages. A3conducts Branch Bound search find solution satisfies constraintsA1 , A2 , A3 , A4 , A5 , also minimizes external constraints. Agent A3 findssolution (A1 Green, A2 Red, A3 Blue, A4 Green, A5 Red), satisfies231fiGrinshpoun & MeiselsAgentA1A2A3A4A5A6A7A8ColorGRBGRBBRmim1m2m3m4m5m6m7m8=f=f=f=f=f=f=t=tdj values2 =B, d3 =B, d7 =R, d8 =Rd1 =G, d3 =Bd1 =G, d2 =R, d4 =G, d5 =Rd3 =B, d5 =Rd3 =B, d4 =G, d6 =B, d7 =R5 =G, d7 =Rd1 =G, 5 =G, d6 =B, d8 =Rd1 =G, d7 =Rmjm2m1m1m3m3m5m1m1values= f , m3= f , m3= f , m2= f , m5= f , m4= f , m7= f , m5= f , m7= f,=f= f,=f= f,=t= f,=tm7 = t, m8 =m4 = f , m5 = fm6 = f , m7 =m6 = f , m8 =Table 4: Configuration 4 obsolete data agent views bold face.internal constraints, minimizes zero external constraints. A3 sends accept!messages neighbors, informing solution. A1 , A2 , A4 , A5 receiveaccept! messages send ok? messages new states neighbors.However, ok? messages A2 A1 A5 A6 A7 delayed.Concurrently mediation session A3 , agent A7 starts mediationsession. A7 sends evaluate? messages neighbors A1 , A5 , A6 , A8 . Let us assumemessage A1 delayed. A6 A8 receive evaluate? messages replyevaluate!, since know agents higher priority A7 wantmediate. A5 , A3 mediation session, replies wait!. denoteconfiguration 4 states agents point processing (see Table 4).A3 mediation session over, A1 receives delayed evaluate? messageA7 . Since A1 longer mediation session, expect mediation sessionnode higher priority A7 (see A1 view Table 4), agent A1 repliesevaluate!. Notice A1 view d2 obsolete (the ok? message A2 A1 stilldelayed). agent A7 receives evaluate! message A1 , continuemediation session involving agents A1 , A6 , A7 , A8 . Since ok? messages A5A6 A7 also delayed, agent A7 starts mediation session knowledgeagents A2 A5 updated (see bold-faced data Table 4).Agent A7 conducts Branch Bound search find solution satisfiesconstraints A1 , A6 , A7 , A8 , also minimizes external constraints.example, A7 finds solution (A1 Red, A6 Red, A7 Blue, A8 Green),satisfies internal constraints, minimizes zero external constraints (rememberA7 wrong data A2 A5 ). A7 sends accept! messages A1 , A6 , A8 ,informing solution. agents receive messages send ok? messagesnew states neighbors. now, delayed messages getdestination, two constraints violated (A1 ,A2 ) (A5 ,A6 ). Consequently, agentsA1 , A2 , A5 , A6 want mediate, whereas agents A3 , A4 , A7 , A8 wishmediate, since conflicts. Notice agents returnedexact states configuration 1 (see Figure 1 Table 1).cycle shown configuration 1 configuration 3continue indefinitely. example contradicts termination completenessAPO algorithm.232fiCompleteness Performance APO Algorithmnoted mention messages passed runningexample. mentioned messages important understandingexample, since example complicated enough. instance, agent A1 completesmediation session (before configuration 2 ), straightforward exchangemessages agents, mj values agents become correct (as presentedTable 2).5. Analyzing Problemsprevious section termination problem APO algorithm describedconstructing scenario leads infinite loop algorithms run. betterunderstand completeness problem APO, one must refer completeness proofAPO algorithm given Mailler Lesser (2006). proof basedincorrect assertion mediation session terminates three possible outcomes:1. solution external conflicts.2. solution exists.3. solution least one external violated constraint.first case, mediator presumably finds solution subproblem.second case, mediator discovers overall problem unsolvable. thirdcase, mediator adds agent (or agents) external conflicts foundgood list, used define future mediations. way, either solutionsolution found (first two cases), good list grows, consequently bringingproblem solving closer centralized solution (third case).However, infinite loop scenario section 4 shows assertion claimingthree cases cover possible outcomes mediation session incorrect.two possible reasons incorrectness. first reason possibilitymediator initiates partial mediation session without obtaining lock agentsgood list. second reason incorrect information external constraintsneighboring mediation sessions performed concurrently. reasons relateconcurrency mediation sessions.5.1 Partial Mediation Sessionsfirst reason incorrectness always growth assertion possibilitymediator initiates partial mediation session without obtaining lockagents good list. possibility occur earlier engagementsgood lists members mediation sessions. APOs code, agents sendwait! message.Let us consider partial mediation session. Assume mediator findssolution subproblem, external conflicts agents outsidemediation session. Assume also conflicts agents alreadymediators good list. Notice possible, since agents engagedmediation sessions earlier sent wait! messages mediator. presentmediation session falls case 3 original proof. However, apparent newagents added good list contradicting assertion.233fiGrinshpoun & MeiselsAnother possible outcome partial mediation sessions situation agentseveral agents entire graph good list try mediate, failget lock agents good list. Consequently, situation singleagent holds entire constraint graph, necessarily lead solution, dueoscillation.5.2 Neighboring Mediation Sessionssecond reason incorrectness assertion original proof (Mailler &Lesser, 2006) potential existence obsolete information external constraints.reason involves scenario two neighboring mediation sessions performed concurrently. mediation sessions scenario end finding solutionpresumably external conflicts, combination solutions causes newconflicts. case mediation sessions agents A1 A5 examplesection 4. scenario seemingly fits first case assertion,external conflicts found mediation sessions. Consequently, externalconflict-free partial solution found contradicting assertion. Furthermore, nonemediators increase good list. enables occurrence infinite loop,displayed section 4.6. Complete Asynchronous Partial Overlaytwo-part solution solves completeness problem APO presented. firstpart solution insures first reason incorrectness assertion (seesection 5.1) could occur. achieved preventing partial mediation sessionsgo without participation entire mediators good list. second partsolution addresses scenario two neighboring mediation sessions performedconcurrently. scenario, results mediation sessions create newconflicts. order ensure good lists grow rule infinite loop, secondpart solution makes sure least one good lists grows. Combinedfirst part insures mediation sessions involve entire good listsmediators, completeness APO secured.6.1 Preventing Partial Mediation Sessionsproposed algorithm disables initiation partial mediation sessions makingmediator wait obtains lock agents good list. Algorithm 4 presentschanges additions APO needed preventing partial mediation sessions.mediator receives wait! message least one agentsgood list, simply cancels mediation session (wait!, line 2) sets counterspecial value -1 (wait!, line 3). notify participants canceled mediationsession, mediator sends cancel! message participants (wait!, line 4).Upon receiving cancel! message, receiving agent updates agent view (cancel!,line 1) frees mediators lock (cancel!, line 2). However, agent still234fiCompleteness Performance APO AlgorithmAlgorithm 4 Preventing partial mediation sessions.received (wait!, (xj , pj ))1: update agent view (xj , pj );2: mediate false;3: counter 1;4: send (cancel!, (xi , pi )) xj good list;5: check agent view;received (evaluate!, (xj , pj , labeledDj ))1: update agent view (xj , pj );2: counter 6= 13:record (xj , labeledDj ) preferences;4:counter counter 1;5:counter = 0 choose solution;received (cancel!, (xj , pj ))1: update agent view (xj , pj );2: mediate false;3: check agent view;aware mediators willingness mediate. Consequently, join mediationsession lower priority agent. special value counter used mediatordisregard evaluate! messages arrive wait! message (that causes cancellation)due asynchronous message passing (evaluate!, line 2).cancellation mediation session upon receiving single wait! message introducesneed unique identification mediation sessions. Consider wait! messagemediator receives. Upon receiving message, mediator cancels mediation sessioncalls check agent view. may decide initiate new mediation session. However,might receive wait! message another agent corresponding previous, alreadycancelled, mediation session. Consequently, new mediation session would mistakenlycancelled too. prevent occurrence problem, unique id addedmediation session. way, mediator could disregard obsolete wait! evaluate!messages. unique identification mediation sessions removed pseudo-codeorder keep simple possible.approach may imply kind live-lock, repeatedly agent succeedsinitiating mediation sessions. However, live-lock cannot occur due prioritiesagents. Consider agent xp highest priority among agentswish mediate. case agent xp obtains lock agents good list,initiate mediation session live-lock. interesting situation agentxp fails get lock agents good list (receives least one wait! message).Even case agent xp eventually succeed initiating mediation session, sinceagents good list aware willingness mediate. agentsmoment locked mediators (both initiated mediation sessions mediation235fiGrinshpoun & Meiselssessions canceled) eventually freed either cancel! accept!messages. Since agents aware agent xp willingness mediate,join mediation session agent xp (unless xp informslonger wishes mediate). Consequently, agent xp eventually obtain lockagents good list contradicting implied live-lock.6.2 Neighboring Mediation SessionsSequential concurrent neighboring mediation sessions may result new conflictscreated without good lists growing. mediation sessions may leadinfinite loop depicted section 4. A7 configuration 2 example agentparticipates sequential neighboring mediation sessions (of mediators A1 A5 ).hand, A3 configuration 2 example agent whose neighborsincorrect view of, due concurrent mediation sessions.solution problem subsequent neighboring mediation sessions could obtained agent (for example, A7 configuration 2 ) would agree participate newmediation session agent view updated changes previousmediation session. achieved mediator sending entire solutionaccept! messages, instead specific dj s. Therefore, sending accept! messages(choose solution, line 11) Algorithm 2 changed following:send (accept!, (s, xi , pi , di , mi )) xj ;Upon receiving revised accept! message (Algorithm 5), agent updatesdk received solution accept dk agent view (accept!, lines1-3). Notice agent still send ok? messages neighbors (accept!, line 7),since neighbors necessarily involved mediation session.solution problem concurrent neighboring mediation sessions could obtained mediator informed post factum new conflict created dueconcurrent mediation sessions. manner, mediator add new conflictingagent good list. Algorithm 5 presents changes additions APOneeded handling concurrent neighboring mediation sessions.agent xi participating mediation session receives accept! messagemediator, keeps list neighbors (in constraint graph)included accept! message (not part mediation session), associatedmediator (accept!, lines 4-5). list named conc list, since contains agentspotentially involved concurrent mediation sessions.Upon receiving ok? message agent xj belonging conc list (ok?, line2), agent xi checks data received ok? message generates new conflictxj (ok?, line 3). new conflict generated, agent xj removedconc list (ok?, line 6). However, case conflict generated (ok?, lines 3-5), agent xiperceives agent xj involved concurrent mediation sessionscreated new conflicts. case, agent xi mediator add agent xj agent viewgood list. Hence, agent xi sends new add! message mediator (associatedagent xj conc list). mediator receives add! message adds agent xjagent view good list (add!, lines 1-2).236fiCompleteness Performance APO AlgorithmAlgorithm 5 Handling neighboring mediation sessions.received (accept!, (s, xj , pj , dj , mj ))1: xk agent view (starting xi )2:xk3:update agent view (xk , dk );4:else di generate conflict existing dk5:add (xk , xj ) conc list;6: mediate false;7: send (ok?, (xi , pi , di , mi )) xj agent view;8: update agent view (xj , pj , dj , mj );9: check agent view;received (ok?, (xj , pj , dj , mj ))1: update agent view (xj , pj , dj , mj );2: xj conc list3:dj generates conflict di4:tuple (xj , xk ) conc list5:send (add!, (xj )) xk ;6:remove tuples (xj , xk ) conc list;7: check agent view;received (add!, (xj ))1: send (init, (xi , pi , di , mi , Di , Ci )) xj ;2: add xj init list;slight problem solution, since may push problem solvingprocess become centralized. may happen ok? message agent xjgenerates new conflict may actually result later mediation sessionagent xj involved in. case, xj mediator already added agent xigood list. Adding agent xj good list xi mediator necessarycompleteness algorithm. lead faster convergence problemcentralized one. Nevertheless, experiments show effect growth good listsnegligible (see section 9.3).6.3 Preventing Busy-Waitinginsure partial mediation sessions occur, wait! message received mediator (Algorithm 4) causes cancel mediation session (section 6.1). cancellationsession immediately followed call check agent view (wait!, line 5).call likely result additional attempt agent start mediationsession, due high probability agents view change since previousmediation attempt. reasons failed previous mediation attempt may wellcause new mediation session succeed also. subsequent mediation attemptsmay occur several times mediation session succeeds mediator decides237fiGrinshpoun & Meiselsstop attempts. matter fact, mediator remains busy-waiting mode,either view changes, reasons mediation sessions failure longer valid.latter case enables mediation session take place.state busy-waiting adds unnecessary overhead computation loadproblem solving. particular, increases number sent messages. preventoverhead, mediating agent xm work interrupt-based manner ratherbusy-waiting manner. interrupt-based approach mediator notified (interrupted)reason previous mediation sessions failure longer valid. doneok? message sent mediator agent xw sent precedingwait! message, caused mediation session fail. agent xw sendok? message reason caused send wait! message becomesobsolete. Namely, one following occurs:mediation session xw involved over.agent higher priority xm longer wants mediate.init list xw emptied out.order remember agents notified (interrupted) oneinstances occurs, agent maintains list pending mediators called wait list.time agent sends wait! message mediator, adds mediator wait list.Whenever agent sends ok? messages, clears wait list.changes pseudo-code must applied order use interrupt-basedmethod. maintain wait list, following line added Algorithm 3line 3 evaluate? (inside statement):add xj wait list;Also, sending ok? messages entire agent view, done example procedurecheck agent view line 7 (Algorithm 1), following line added:empty wait list;Finally, need interrupt pending mediators whenever reason mediationsessions failure may longer valid. example, agent removedinit list (init, line 9) Algorithm 1, following lines need added (inside elsestatement):init list ==send (ok?, (xi , pi , di , mi )) xw wait list;empty wait list;lines handle case init list emptied out. Similar additions mustapplied deal mentioned cases. Applying interrupt-based methodrules need busy-waiting. Thus, call check agent view (wait!, line 5)discarded.238fiCompleteness Performance APO Algorithm7. Soundness Completenesssection show CompAPO sound complete. proofsfollow basic structure assumptions original APO proofs (Mailler & Lesser,2006). original completeness proof incorrect incompletenessoriginal algorithm. Consequently, use assertion discusseddetail section 3 played key role original (and incorrect) proofcompleteness (Mailler & Lesser, 2006). following lemmas needed proofssoundness completeness.Lemma 1 Links bidirectional. i.e. xi xj agent view eventually xjxi agent view.Proof (as appears work Mailler Lesser, 2006):Assume xi xj agent view xi agent view xj .order xi xj agent view, xi must received init messagepoint xj . two cases.Case 1: xj init list xi . case, xi must sent xj init messagefirst, meaning xj received init message therefore xi agent viewcontradiction.Case 2: xj init list xi . case, xi receives init messagexj , responds init message. means reliable communicationassumption holds, eventually xj receive xi init message add xi agent viewalso contradiction.Definition 1 agent considered stable state waiting messages,message ever reach it.Definition 2 deadlock state agent conflicts desiresmediate enters stable state.Lemma 2 deadlock cannot occur CompAPO algorithm.Proof:Assume agent xi enters deadlock. means agent xi desires mediate,stable state. consequence agent xi would able getlock agents good list.One possibility xi already invited members good list join mediation session sending evaluate? messages. finite time receive eitherevaluate! wait! messages agents good list. Depending replies,xi either initiates mediation session cancels it. Either way, xi stable statecontradicting assumption.possibility xi reach stage invites agentsjoin mediation session. happen, exists least one agent xjxi point view desires mediate (mj = true) higher priority xi(pj > pi ). two cases xj would mediate session included xi ,xi expecting to:239fiGrinshpoun & MeiselsCase 1: xi mj = true agent view actual value false.Assume xi mj = true agent view actually mj = false. wouldmean point xj changed value mj false without informing xiit. one place xj changes value mj check agent viewprocedure. Note procedure, whenever flag changes true false,agent sends ok? messages agents agent view. Since Lemma 1know xi agent view xj , agent xi must eventually received messageinforming mj = false, contradicting assumption.Case 2: xj believes xi mediating xi believe be.xj point view, mi = true pi > pj . previous case, know xjbelieves mi true (mi = true) must case. need showcondition pi > pj impossible. Assume xj believes pi > pj factpi < pj . means point xi sent message xj informing currentpriority pi . Since know priorities increase time (all good listsget larger), know pi pi (xj always correct value underestimatepi ). Since pi < pj pi pi pi < pj contradiction assumption.Definition 3 algorithm considered stable state agentsstable state.Theorem 1 CompAPO algorithm sound. i.e., reaches stable stateeither found answer solution exists.Proof:assume agents reach stable state, consider caseshappen.Case 1: agent conflicts. case, agents stable stateconflicts. means current value agent variablesatisfies constraints. Consequently, current values valid solutionoverall problem, CompAPO algorithm found answer.Case 2: solution message broadcast. case, least one agentfound subproblem solution, informed agentsbroadcasting solution message. Consequently, agent receives message(all agents) stops run reports solution exists.Case 3: agents conflicts. Let us consider agent xi conflict.Since conflict, xi desires mediate. able perform mediation sessionstable state contradiction assumption. Therefore, conditionxi remain stable state expecting mediation requesthigher priority agent xj send words, deadlocked.Lemma 2 cannot happen.Since cases 1 2 occur, CompAPO algorithm reaches stable stateeither found answer solution exists. Consequently, CompAPOalgorithm sound.240fiCompleteness Performance APO AlgorithmLemma 3 exist agents hold entire graph good list desiremediate, one agents perform mediation session.Proof:shall consider two cases one agent holds entiregraph good list desires mediate, several agents.Case 1: Consider agent xi agent holds entire graphgood list desires mediate. Since xi entire graph good listhighest possible priority. Moreover, agents aware xi priority (pi ) desiremediate (mi ) due ok? messages received xi containing information(xi sent ok? messages agents agent view, holds entire graph).Consequently, agent engage point on, mediation sessionxi s. Since mediation sessions finite new mediation sessions occur, agentxi eventually get lock agents perform mediation session.Case 2: several agents exist, tie priorities brokenagents index. Consider xi one highest index agents,apply proof case 1.Lemma 4 agent holding entire graph good list performs mediation session,algorithm reaches stable state.Proof:Consider mediator agent xi . Following first part CompAPOs solution(section 6.1), agent perform mediation session received evaluate! messages agents good list. Since xi holds entire graph good list,means agents graph sent evaluate! messages xi setmediate flags true. means xi completes search returns accept! messages solution, agent change assignment. Assumingcentralized internal solver xi uses sound complete, find solutionentire problem solution exists, alternatively conclude solution exists.solution exists, xi informs agents problem solvingterminates. Otherwise, agent receives accept! message xi containssolution entire problem. Consequently, agent conflicts algorithmreaches stable state.Lemma 5 Infinite value changes without mediation sessions cannot occur.Proof:proof focus line 6 check agent view procedure, placecode value changed without mediation session. reminder, noticeagents graph ordered priority (ties broken IDsagents).Consider agent lowest priority (xp1 ). Agent xp1 cannot changevalue, since line 5 check agent view procedure states order reachvalue change line 6, current value must conflict exclusively lower priority agents.Clearly impossible agent xp1 , lowest priority graph.241fiGrinshpoun & MeiselsNow, consider next agent ordering, xp2 . Agent xp2 change currentvalue conflict exclusively lower priority agents. lower priorityagent case xp1 . xp1 xp2 neighbors, agent xp2 cannot changevalue reason agent xp1 . Otherwise, agent xp2 know upto-date value agent xp1 finite time (any previously sent updates regarding xp1 valueeventually reach agent xp2 ), since proved value xp1 cannot changedwithout mediation session. xp2 up-to-date value lower priorityneighbors (only xp1 ), change value without mediation session.Eventually, neighbors xp2 updated final change value.general, agent xpi (including highest priority agent) finite timeup-to-date values lower priority agents. happens, changevalue without mediation session. Eventually, neighbors xpiupdated final change value. Thus, infinite value changes withoutmediation sessions cannot occur.proof implicitly relies fact ordering agents change.However, priority agent may change time. Nevertheless, prioritiesbounded size graph, number priority changes finite. provesvalue changes cannot indefinitely occur without mediation sessions.Lemma 6 point agent mediate desire mediate, algorithmreach stable state.Proof:shall consider two cases messages yet arriveddestinations, messages.Case 1: Consider case messages yet arriveddestination. agent desires mediate, mi false, meaningagent graph conflicts. Consequently, current state graph solutionsatisfies constraints, algorithm reaches stable state.Case 2: Consider case messages yet arriveddestinations. Eventually messages arrive. According assumptionlemma, arrival messages make agents desire mediate.Next, consider arrival type message show cannot lead infiniteexchange messages:evaluate?, evaluate!, wait!, cancel!: messages must belong obsoletemediation session, otherwise contradict assumption lemma. Accordingly,may result limited exchange messages (e.g., sending wait! line 3evaluate?). messages may lead call check agent viewprocedure.accept!: message cannot received without contradicting assumption,since receiving agent active mediation session receivingaccept! message.init: message part handshake two agents. Consequently,single additional init message sent. leads callcheck agent view procedure involved agents.242fiCompleteness Performance APO Algorithmadd!: message results sending single init message.ok?: message may result sending finite number add! messages.also leads call check agent view procedure.examining types messages, conclude message leadfinite exchange messages, finite number calls check agent viewprocedure. need show call check agent view cannot lead infiniteexchange messages.check agent view procedure 4 possible outcomes. may simply return (line2), change value variable (lines 6-7), mediate (line 9), update desire mediate (lines 11-12). According assumption lemma, cannot mediate. updatedesire mediate, means value mi true, updated true.Either way, contradiction assumption lemma. Consequently,possibilities simple return, change value variable. According Lemma 5, value changes cannot indefinitely occur without mediationsessions. Consequently, final messages eventually arrive destinations,first case proof hold.Definition 4 One says algorithm advances least one good lists grows.Lemma 7 every n mediation sessions, algorithm either advances reachesstable state.Proof:Consider mediation session agent xi . mediation session three possibleoutcomes solution satisfying constraints within good list, solution satisfyingconstraints within good list violations external constraints, solutionsatisfying constraints within good list external constraints.Case 1: solution satisfies constraints within xi good list exists, thereforeentire problem unsatisfiable. case, xi informs agentsproblem solving terminates.Case 2: xi finds solution satisfies constraints within good list violatesexternal constraints. case, xi adds agents external conflictsgood list. agents already xi good list, since mediation sessionincluded entire good list xi (according section 6.1). Consequently, xi good listgrows algorithm advances.Case 3: xi finds solution satisfies constraints within good listexternal constraints. Following second part CompAPOs solution (section 6.2), agentsxi good list maintain conc list, would notify xi add agents good listcase experience new conflicts due concurrent mediation sessions. case,xi would notified, good list would grow algorithm would advance.situation algorithm advance reach stable state,mediation sessions experience case 3, concurrent mediation sessionscreate new conflicts. case, n mediation sessions (equal overallnumber agents), agents would desire mediate. According Lemma 6,algorithm reaches stable state.243fiGrinshpoun & MeiselsLemma 8 exists group agents desire mediate, mediation sessioneventually occur.Proof:agent manage get lock agents good list (essentialmediation session occur) agents group sent evaluate? messagesgot least one wait! message each. case, consider xi highest priorityagent among group.wait! message agent xi received either agent membergroup agent outside group, currently involved another mediationsession. case agent (xj ) belongs group, xj also got wait! message(clearly, wait! message arrived xj sent wait! xi ). xj therefore cancelmediation session, wait xi next evaluate? message (since xj awarexi desire mediate pi highest priority among agents currently desiremediate). case xj belong group, mediation session xj involvedeventually terminate, xi get lock, unless xj higher priorityxi (pj > pi ) also xj desires mediate session terminates. case,xj eventually get lock reasons.Theorem 2 CompAPO algorithm complete. i.e., solution exists, algorithmfind it, solution exist, algorithm report fact.Proof:Since shown Theorem 1 whenever algorithm reaches stable state,problem solved finds subset variables unsatisfiableterminates, need show always reaches one two states finitetime.According Lemma 6, point time agent mediate desiremediate, algorithm reach stable state. According Lemma 8 existagents desire mediate, eventually mediation session occur. Lemmas 68 conclude possibility algorithm reach stable statecontinuous occurrences mediation sessions. According Lemma 7, every nmediation sessions, algorithm either advances reaches stable state. Consequently,algorithm either reaches stable state continuously advances.case algorithm continuously advances, good lists continuously grow.point, agents (eventually agents) hold entire graph good list.One agents eventually desire mediate (if not, according Lemma 6,algorithm reaches stable state). According Lemma 3, one agents performmediation session. According Lemma 4, algorithm reaches stable state.8. OptAPO Optimizing APODistributed Constraint Optimization Problems (DisCOPs) version distributed constraint problems, goal find optimal solution problem, rathersatisfying one. optimization problem, agent associates cost violatedconstraints maintains bounds costs order reach optimal solutionminimizes number violated constraints.244fiCompleteness Performance APO Algorithmnumber algorithms proposed last years solving DisCOPs.simplest algorithm Synchronous Branch Bound (SyncBB) (Hirayama &Yokoo, 1997), distributed version well-known centralized BranchBound algorithm. Another algorithm uses Branch Bound scheme Asynchronous Forward Bounding (AFB) (Gershman, Meisels, & Zivan, 2006), agentsperform sequential assignments propagated bounds checking early detection need backtrack. number algorithms use pseudo-tree derivedstructure DisCOP order improve process acquiring solutionoptimization problem. ADOPT (Modi, Shen, Tambe, & Yokoo, 2005)asynchronous algorithm assignments passed pseudo-tree. Agentscompute upper lower bounds possible assignments send costs parents pseudo-tree. costs eventually accumulated root agent. Anotheralgorithm exploits pseudo tree DPOP (Petcu & Faltings, 2005). DPOP,agent receives agents sons pseudo-tree, combinationspartial solutions sub-tree corresponding costs. agent calculatesgenerates possible partial solutions include partial solutions receivedsons assignments sends resulting combinations pseudotree. root agent receives information sons, produces optimalsolution propagates pseudo-tree rest agents.Another different approach implemented Optimal Asynchronous Partial Mediation (OptAPO) (Mailler & Lesser, 2004; Mailler, 2004) algorithm,optimization version APO algorithm. Differently APO, OptAPO algorithmintroduces second type mediation sessions called passive mediation sessions. goalpassive sessions update bounds costs without changing valuesvariables. sessions add parallelism algorithm accelerate distributioninformation. might solve many problems result incorrect information,discussed section 5.2. However, active mediation sessions also occur OptAPO.active sessions may consist parts good list (partial mediation sessions),result lead problems described section 5.1. Moreover, satisfiable problemalso solved OptAPO, returning zero optimal cost. Therefore, infinite loop scenario described section 4 also occur OptAPO, behaves like APOproblem satisfiable.OptAPO algorithm must corrected order aforementioned problemssolved. section 6 several modifications APO algorithm proposed.changes turn APO complete search algorithm CompAPO. Equivalent modificationsmust also applied OptAPO algorithm order ensure correctness. Interestingly, modifications APO procedures similar APO OptAPO.main differences APO OptAPO addition passive mediation sessions (procedure check agent view) OptAPO, internal searchmediators perform (procedure choose solution). However, neither procedureseffected modifications CompAPO. Thus, pseudo-code changes mustapplied OptAPO similar modifications CompAPO, thereforeomitted paper. performance resulting algorithm CompOptAPOevaluated section 9.5. full pseudo-code original OptAPO algorithmfound work Mailler Lesser (2004).245fiGrinshpoun & Meisels9. Experimental Evaluationoriginal (and incomplete) version APO algorithm evaluated MaillerLesser (2006). compared AWC algorithm (Yokoo, 1995),efficient DisCSP solver (Zivan et al., 2007). experiments performed 3-coloringproblems, subclass uniform random constraints problems. problemscharacterized small domain size, low constraints density, fixed constraints tightness(for characterization random CSPs see works Prosser, 1996 Smith, 1996).comparison APO AWC (Mailler & Lesser, 2006) made respectthree measures number sent messages, number cycles, serial runtime.number sent messages important widely accepted measure,measures problematic. cycle, incoming messages delivered,agent allowed process information, messages createdprocessing added outgoing queue delivered beginningnext cycle. meaning cycle APO mediation session possiblyinvolves entire graph takes single cycle. measure clearly problematic,since every centralized algorithm solves problem one cycle. Measuring serialruntime also adequate distributed CSPs, since take accountconcurrent computations problem solving. order measure concurrentruntime DisCSP algorithms implementation independent way, one needs countnon-concurrent constraint checks (NCCCs) (Meisels, Razgon, Kaplansky, & Zivan, 2002).measure gained global agreement DisCSP DisCOP community (Bessiereet al., 2005; Zivan & Meisels, 2006b) used present evaluation.modifications CompAPO, especially prevention partial mediationsessions (section 6.1) add synchronization algorithm, may tax heavilyperformance algorithm. Thus, important evaluate effect changescomparing CompAPO (incomplete) versions APO algorithm. Additionally,evaluate effectiveness interrupt-based method compared busy-waiting.9.1 Experimental Setupexperiments use simulator agents simulated threads,hold shared memory communicate message passing. networkconstraints, experiments, generated randomly selecting probabilityp1 constraint among pair variables probability p2 , occurrenceviolation among two assignments values constrained pair variables.uniform random constraints networks n variables, k values domain, constraintsdensity p1 tightness p2 commonly used experimental evaluations CSPalgorithms (Prosser, 1996; Smith, 1996).Experiments conducted several density values. setup included problemsgenerated 15 agents (n = 15) 10 values (k = 10). drew 100 different instancescombination p1 p2 . experiments agent holds singlevariable.246fiCompleteness Performance APO AlgorithmFigure 3: Mean NCCCs sparse problems (p1 = 0.1).Figure 4: Mean NCCCs medium density problems (p1 = 0.4).9.2 Comparison Algorithmsperformance CompAPO compared three asynchronous search algorithmswell known Asynchronous Backtracking (ABT) (Yokoo et al., 1998; Yokoo & Hirayama,2000), extremely efficient Asynchronous Forward-Checking Backjumping (AFCCBJ) (Meisels & Zivan, 2007), Asynchronous Weak Commitment (AWC) (Yokoo,1995), used original APO evaluation (Mailler & Lesser, 2006).Results presented three sets tests different values problem densitysparse (p1 = 0.1), medium (p1 = 0.4), dense (p1 = 0.7). sets value p2varies 0.1 0.9, cover ranges problem difficulty.247fiGrinshpoun & MeiselsFigure 5: Mean NCCCs dense problems (p1 = 0.7).Figure 6: Mean number messages medium density problems (p1 = 0.4).order evaluate performance algorithms, two independent measuresperformance used search effort form NCCCs communication loadform total number messages sent. Figures 3, 4, 5 present numberNCCCs performed CompAPO solving problems different densities. Figure 6shows total number messages sent problem solving process. figuresexhibit phase-transition phenomenon increasing values tightness, p2 , problemdifficulty increases, reaches maximum, drops back low value.termed easy-hard-easy transition hard problems (Prosser, 1996), observedDisCSPs (Meisels & Zivan, 2007; Bessiere et al., 2005).248fiCompleteness Performance APO Algorithmperformance CompAPO NCCCs turns poor phasetransition region compared asynchronous search algorithms. worst resultsproblems relatively sparse (Figures 3 4). However, even dense problemsABT AFC-CBJ clearly outperform CompAPO (Figure 5). comparingCompAPO AWC, results significantly different. AWC known perform bestsparse problems. Thus, like ABT AFC-CBJ clearly outperforms CompAPOproblems (Figure 3). medium density problems, AWC still performs betterCompAPO difference performances algorithms much smaller(Figure 4). dense problems, AWC performs extremely bad ten timesNCCCs CompAPO. results AWC omitted Figure 5, sincefinish running set tests reasonable time stop run 40hours.Notice scale Figure 4 different Figures 3 5. dueespecially poor performance APO around phase transition medium densityproblems. behavior untypical, since DisCSP algorithm suffer worstperformance around phase transition high density problems (Figure 5). factperformance CompAPO better high density problems mediumdensity ones explained faster convergence centralized solution denseproblems. problems around phase transition, CompAPO algorithm frequentlyreaches full centralization anyway. Thus, faster convergence centralized solutionactually improves performance algorithm.search effort performed agents running CompAPO extremely high,communication load system remains particularly low. seenFigure 6, medium density problems. Similar results achieved sparse denseproblems. surprising, since major part search effort carriedagents performing mediation sessions without need extensive exchangemessages.9.3 Comparison Versions APOSeveral versions APO algorithm proposed Benisch Sadeh (2006). Oneversions (APO-BT) uses simple backtracking mediation procedure, insteadBranch Bound originally proposed APO (APO-BB). performanceCompAPO compared two incomplete versions algorithm.modifications CompAPO, especially prevention partial mediationsessions (section 6.1) add synchronization algorithm. potential partial mediationsession must wait sessions end mediator able get lock entiregood list. synchronization may tax performance algorithm. Nevertheless,experiments show CompAPO actually performs slightly better APO-BBmeasured NCCCs (Figures 7 8). improved performance explainedbetter distribution data entire solution sent accept! message(section 6.2). Figure 9 shows effect CompAPOs modifications even greatercommunication load. substantial advantage CompAPO may explaineduse interrupt-based approach (section 6.3) helps performance eliminatingunnecessary overhead busy-waiting.249fiGrinshpoun & MeiselsFigure 7: Mean NCCCs medium density problems (p1 = 0.4).Figure 8: Mean NCCCs dense problems (p1 = 0.7).Figure 10 presents mean size largest mediation session occurring search,medium density problems (p1 = 0.4) 15 variables. average size largestmediation session around 12 (out maximum 15). occurs problemsphase transition region p2 0.5 0.6. Although number farmaximum 15, suggest considerable portion hard problemssolved without reaching full centralization.part code CompAPO solves neighboring mediation sessions problem (section 6.2) implies potential additional growth good lists (ok?, line 5),may result faster centralization problem solving. Nevertheless, Figure 10 clearlyshows CompAPO centralize faster original version APO (APOBB), except tight, unsolvable problems.250fiCompleteness Performance APO AlgorithmFigure 9: Mean number messages medium density problems (p1 = 0.4).Figure 10: Mean size largest mediation session (p1 = 0.4 n = 15).experiments show medium density problems, APO-BT version performspoorly respect NCCCs number sent messages comparisonAPO-BB CompAPO (Figures 7 9). reason ABO-BTs poor performanceeasily explained frequent convergence full centralization shown Figure 10. Nevertheless, APO-BT lower communication load APO-BB phasetransition. reason actually reason leads APO-BTs extensive search effort. prompt convergence full centralization yields high search effort(NCCCs), time may reduce communication load.Figure 8 shows dense problems APO-BT performs better APO-BBalmost CompAPO. supports results reported Benisch Sadeh(2006) dense random DisCSPs. paper also presents results structured251fiGrinshpoun & MeiselsFigure 11: Interrupt-based vs. busy-waiting (mean NCCCs p1 = 0.4).3-coloring problems, APO-BT outperformed APO-BB. Similar behaviorobserved experiments conducted sparser problems (Figure 7),suggests variance APO-BTs performance densityproblem structure.Benisch Sadeh propose additional version APO algorithm,mediation session selection rule inverse original selection rule (Benisch & Sadeh,2006). version called IAPO instructs agents choose smallest mediation sessionrather largest one. clear IAPO turned correct algorithm,since correctness proofs presented section 7 rely fact largest mediationsessions chosen. Consequently, evaluation IAPO omitted paper.9.4 Interrupt-Based Versus Busy-WaitingFigures 11 12 present two measures performance comparing different methodssynchronization needed order avoid conflicts concurrent mediationsessions interrupt-based busy-waiting (section 6.3). interrupt-based methodclearly outperforms busy-waiting harder problem instances. Predictably, differenceperformance pronounced measuring number messages (Figure 12).9.5 Evaluation CompOptAPOoriginal (and incomplete) version OptAPO algorithm evaluated MaillerLesser (2004). compared ADOPT algorithm (Modi et al., 2005),best DisCOP solver. Similarly original results APO (Mailler & Lesser,2006), comparison OptAPO ADOPT (Mailler & Lesser, 2004) maderespect three measures number sent messages, number cycles,serial runtime. reasons DisCSP algorithms, cycles serial runtimealso problematic measuring performance DisCOP algorithms. case252fiCompleteness Performance APO AlgorithmFigure 12: Interrupt-based vs. busy-waiting (mean number messages p1 = 0.4).CompAPO, CompOptAPO algorithm also evaluated counting NCCCsnumber sent messages.Distributed Optimization problems used following experiments randomMax-DisCSPs. Max-DisCSP subclass DisCOP constraint costs (weights)equal one (Modi et al., 2005). feature simplifies task generating random problems, since using Max-DisCSPs one decide costsconstraints. Max-CSPs commonly used experimental evaluations constraintoptimization problems (COPs) (Larrosa & Schiex, 2004). experimental evaluationsDisCOPs include graph coloring problems (Modi et al., 2005; Zhang, Xing, Wang, &Wittenburg, 2005), subclass Max-DisCSP. advantage using randomMax-DisCSP problems fact create evaluation framework knownexhibit phase-transition phenomenon centralized COPs. importantevaluating algorithms solving DisCOPs, enabling known analogy behavior centralized algorithms problem difficulty changes. problems solved sectionrandomly generated Max-DisCSP 10 agents (n = 10) 10 values (k = 10), constraint density either p1 = 0.4 p1 = 0.7, varying constraint tightness 0.4 p2 < 1.performance CompOptAPO compared three search algorithms Synchronous Branch Bound (SyncBB) (Hirayama & Yokoo, 1997), AFB (Gershman et al.,2006), ADOPT (Modi et al., 2005). ADOPT used original OptAPO evaluation (Mailler & Lesser, 2004).must noted experiments original OptAPO algorithm,experienced several runs algorithm failed advance reachsolution. shows termination problem OptAPO occurs practicetheory, scenario involves particular message delays one presentedsection 4 APO algorithm. Additionally, discovered OptAPO mayalways able report optimal cost (i.e., number broken constraints MaxDisCSP experiments). understand happen consider almost disjoint253fiGrinshpoun & MeiselsFigure 13: example 3-coloring problem.Figure 14: Mean NCCCs sparse optimization problems (p1 = 0.4).graph, one depicted Figure 13. example assume agents A1A2 conflicts. Consequently, knowledge regarding cost Fexchanged two groups, agent holds correct overall costproblem (F = 3). Nevertheless, OptAPO algorithm terminates,optimal solution. Thus, optimal value derived upon termination summingnumber broken constraints agents. result must divided twoaccount broken constraints counted involved agents.performance CompOptAPO NCCCs comparable DisCOP algorithms problems relatively loose (low p2 value), ADOPTalgorithm performing slightly better. case sparse dense problems(Figures 14 15, respectively). problems become tighter, CompOptAPO clearlyoutperforms ADOPT SyncBB. fact, ADOPT algorithm failed terminatereasonable time tight problems (p2 > 0.8 Figure 14 p2 > 0.6 Figure 15). However, tight problems AFB algorithm much faster CompOptAPO. Actually,AFB algorithm experiments managed terminate reasonabletime problems dense (p1 = 0.7) tight (p2 = 0.9).254fiCompleteness Performance APO AlgorithmFigure 15: Mean NCCCs dense optimization problems (p1 = 0.7).Figure 16: Mean number messages dense optimization problems (p1 = 0.7).Similarly CompAPO, communication load system remains particularlylow running CompOptAPO algorithm. seen Figure 16 denseproblems. Similar results observed sparse problems. surprising, sincemajor part search effort carried agents performing mediation sessionswithout need extensive exchange messages.255fiGrinshpoun & Meisels10. ConclusionsAPO search algorithm asynchronous distributed algorithm DisCSPs.algorithm partitions search different subproblems. subproblem solvedselected agent mediator. conflicts arise solution subproblemneighboring agents, conflicting agents added subproblem. Ideally,algorithm either leads compatible solutions constraining subproblems,growth subproblems whose solution incompatible neighboring agents. twooption situation used original APO paper (Mailler & Lesser, 2006) provetermination completeness algorithm.proof completeness APO algorithm presented Mailler Lesser(2006) based growth size subproblems. turns expectedgrowth groups occur situations, leading termination problemalgorithm. present paper demonstrates problem following exampleterminate. Furthermore, paper identifies problematic partsoriginal algorithm interfere completeness applies modifications solveproblematic parts. resulting CompAPO algorithm ensures completenesssearch. Formal proofs soundness completeness CompAPO presented.CompAPO algorithm forms class DisCSP search algorithms. contrast backtracking concurrent search processes, achieves concurrency solvingsubproblems concurrently. therefore interesting important evaluateperformance CompAPO compare DisCSP search algorithms.Asynchronous Partial Overlay actually family algorithms. completenesstermination problems presented corrected present study applymembers family. OptAPO algorithm (Mailler & Lesser, 2004; Mailler, 2004)optimization version APO solves Distributed Constraint Optimization Problems(DisCOPs). present paper shows similar modification ones madeAPO algorithm must also applied OptAPO order ensure correctness.changes call performance evaluation resulting CompOptAPO algorithm.experimental evaluation presented section 9 demonstratesperformance CompAPO poor compared asynchronous search algorithms.randomly generated DisCSPs runtime APO, measured NCCCs, longertwo orders magnitude ABT (Yokoo et al., 1998; Yokoo & Hirayama,2000) AFC-CBJ (Meisels & Zivan, 2007).total number messages sent CompAPO considerably smaller corresponding number ABT AFC-CBJ. clear result fact hard probleminstances tend solved small number mediators semi-centralized manner.runtime performance CompOptAPO better ADOPT (Modi et al.,2005) SyncBB (Hirayama & Yokoo, 1997) hard instances randomly generatedDisCOPs. Similarly DisCSP case, total number messages sent CompOptAPO considerably smaller corresponding number DisCOP algorithms.However, phase-transition region randomly generated DisCOPs, runtimeCompOptAPO longer order magnitude AFB (Gershmanet al., 2006).256fiCompleteness Performance APO AlgorithmReferencesBenisch, M., & Sadeh, N. (2006). Examining DCSP coordination tradeoffs. ProceedingsFifth International Joint Conference Autonomous Agents MultiagentSystems (AAMAS06), pp. 14051412. ACM.Bessiere, C., Maestre, A., Brito, I., & Meseguer, P. (2005). Asynchronous backtrackingwithout adding links: new member ABT family. Artificial Intelligence, 161:12, 724.Gershman, A., Meisels, A., & Zivan, R. (2006). Asynchronous forward-bounding distributed constraints optimization. Proc. ECAI-06, pp. 103107.Grinshpoun, T., & Meisels, A. (2007). CompAPO: complete version APO algorithm. Proceedings 2007 IEEE/WIC/ACM International ConferenceIntelligent Agent Technology (IAT 2007), pp. 370376.Grinshpoun, T., Zazon, M., Binshtok, M., & Meisels, A. (2007). Termination problemAPO algorithm. Proceedings Eighth International Workshop DistributedConstraint Reasoning (DCR07), pp. 113124.Hirayama, K., & Yokoo, M. (1997). Distributed partial constraint satisfaction problem.Proceedings Third International Conference Principles PracticeConstraint Programming (CP-97), pp. 222236.Larrosa, J., & Schiex, T. (2004). Solving weighted csp maintaining arc consistency.Artificial Intelligence, 159, 126.Mailler, R. (2004). mediation-based approach cooperative, distributed problem solving.Ph.D. thesis, University Massachusetts.Mailler, R., & Lesser, V. (2004). Solving distributed constraint optimization problems usingcooperative mediation. Proceedings Third International Joint ConferenceAutonomous Agents MultiAgent Systems (AAMAS04), pp. 438445. ACM.Mailler, R., & Lesser, V. (2006). Asynchronous partial overlay: new algorithm solvingdistributed constraint satisfaction problems. Journal Artificial Intelligence Research(JAIR), 25, 529576.Meisels, A., Razgon, I., Kaplansky, E., & Zivan, R. (2002). Comparing performancedistributed constraints processing algorithms. Proc. AAMAS-2002 WorkshopDistributed Constraint Reasoning DCR, pp. 8693.Meisels, A., & Zivan, R. (2007). Asynchronous forward-checking DisCSPs. Constraints,12 (1), 131150.Modi, P. J., Shen, W., Tambe, M., & Yokoo, M. (2005). Adopt: asynchronous distributedconstraints optimization quality guarantees. Artificial Intelligence, 161:1-2, 149180.Petcu, A., & Faltings, B. (2005). scalable method multiagent constraint optimization.Proceedings International Joint Conference Artificial Intelligence, pp.266271.257fiGrinshpoun & MeiselsProsser, P. (1996). empirical study phase transitions binary constraint satisfactionproblems. Artificial Intelligence, 81, 81109.Semnani, S. H., & Zamanifar, K. (2007). MaxCAPO: new expansion APO solvedistributed constraint satisfaction problems. Proceedings International Conference Artificial Intelligence Soft Computing (ASC 2007).Smith, B. M. (1996). Locating phase transition binary constraint satisfaction problems. Artificial Intelligence, 81, 155 181.Yokoo, M. (1995). Asynchronous weak-commitment search solving distributed constraint satisfaction problems. Proceedings First International ConferencePrinciples Practice Constraint Programming (CP-95), pp. 88 102.Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1998). Distributed constraint satisfaction problem: Formalization algorithms.. IEEE Trans. Data Kn. Eng.,10, 673685.Yokoo, M., & Hirayama, K. (2000). Algorithms distributed constraint satisfaction:review. Autonomous Agents Multi-Agent Systems, 3 (2), 185207.Zhang, W., Xing, Z., Wang, G., & Wittenburg, L. (2005). Distributed stochastic searchdistributed breakout: properties, comparison applications constraints optimization problems sensor networks. Artificial Intelligence, 161:1-2, 5588.Zivan, R., & Meisels, A. (2006a). Concurrent search distributed CSPs. Artificial Intelligence, 170 (4), 440461.Zivan, R., & Meisels, A. (2006b). Message delay DisCSP search algorithms. AnnalsMathematics Artificial Intelligence(AMAI), 46(4), 415439.Zivan, R., Zazone, M., & Meisels, A. (2007). Min-domain ordering asynchronous backtracking. Proceedings 13th International Conference Principles Practice Constraint Programming (CP-2007), pp. 758772.258fiJournal Artificial Intelligence Research 33 (2008) 465-519Submitted 05/08; published 12/08AND/OR Multi-Valued Decision Diagrams (AOMDDs)Graphical ModelsRobert MateescuMATEESCU @ PARADISE . CALTECH . EDUElectrical Engineering DepartmentCalifornia Institute TechnologyPasadena, CA 91125, USARina DechterDECHTER @ ICS . UCI . EDUDonald Bren School Information Computer ScienceUniversity California IrvineIrvine, CA 92697, USARadu MarinescuR . MARINESCU @4 C . UCC . IECork Constraint Computation CentreUniversity College Cork, IrelandAbstractInspired recently introduced framework AND/OR search spaces graphical models, propose augment Multi-Valued Decision Diagrams (MDD) nodes, ordercapture function decomposition structure extend compiled data structures general weighted graphical models (e.g., probabilistic models). present AND/OR Multi-ValuedDecision Diagram (AOMDD) compiles graphical model canonical form supports polynomial (e.g., solution counting, belief updating) constant time (e.g. equivalencegraphical models) queries. provide two algorithms compiling AOMDD graphicalmodel. first search-based, works applying reduction rules trace memoryintensive AND/OR search algorithm. second inference-based uses Bucket Eliminationschedule combine AOMDDs input functions via APPLY operator.algorithms, compilation time size AOMDD are, worst case, exponentialtreewidth graphical model, rather pathwidth known ordered binary decisiondiagrams (OBDDs). introduce concept semantic treewidth, helps explainsize decision diagram often much smaller worst case bound. provideexperimental evaluation demonstrates potential AOMDDs.1. Introductionpaper extends decision diagrams AND/OR multi-valued decision diagrams (AOMDDs)shows graphical models compiled data-structures. work presentedpaper based two existing frameworks: (1) AND/OR search spaces graphical models(2) decision diagrams.1.1 AND/OR Search SpacesAND/OR search spaces (Dechter & Mateescu, 2004a, 2004b, 2007) proven unifyingframework various classes search algorithms graphical models. main characteristicexploitation independencies variables search, provide exponentialspeedups traditional search methods viewed traversing structure.c2008AI Access Foundation. rights reserved.fiM ATEESCU , ECHTER & ARINESCUnodes capture problem decomposition independent subproblems, nodes represent branching according variable values. AND/OR spaces accommodate dynamic variableordering, however current work focuses static decomposition. Examples AND/ORsearch trees graphs appear later, example Figures 6 7.AND/OR search space idea originally developed heuristic search (Nilsson, 1980).context graphical models, AND/OR search (Dechter & Mateescu, 2007) also inspiredsearch advances introduced sporadically past three decades constraint satisfactionrecently probabilistic inference optimization tasks. Specifically, resemblespseudo tree rearrangement (Freuder & Quinn, 1985, 1987), adapted subsequently distributed constraint satisfaction Collin, Dechter, Katz (1991, 1999) recentlyModi, Shen, Tambe, Yokoo (2005), also shown related graph-based backjumping (Dechter, 1992). work extended Bayardo Miranker (1996) BayardoSchrag (1997) recently applied optimization tasks Larrosa, Meseguer, Sanchez(2002). Another version viewed exploring AND/OR graphs presented recently constraint satisfaction (Terrioux & Jegou, 2003b) optimization (Terrioux & Jegou,2003a). Similar principles introduced recently probabilistic inference, algorithm Recursive Conditioning (Darwiche, 2001) well Value Elimination (Bacchus, Dalmao, & Pitassi,2003b, 2003a), currently core advanced SAT solvers (Sang, Bacchus,Beame, Kautz, & Pitassi, 2004).1.2 Decision DiagramsDecision diagrams widely used many areas research, especially software hardwareverification (Clarke, Grumberg, & Peled, 1999; McMillan, 1993). BDD represents Booleanfunction directed acyclic graph two terminal nodes (labeled 0 1), every internalnode labeled variable exactly two children: low 0 high 1. isomorphicnodes merged, would full search tree, also called Shannon tree,usual full tree explored backtracking algorithm. tree ordered variables encounteredorder along every branch. compressed merging isomorphic nodes(i.e., label identical children), eliminating redundant nodes (i.e., whoselow high children identical). result celebrated reduced ordered binary decisiondiagram, OBDD short, introduced Bryant (1986). However, underlying structureOR, initial Shannon tree tree. AND/OR search trees reduced nodemerging redundant nodes elimination get compact search graph viewedBDD representation augmented nodes.1.3 Knowledge Compilation Graphical Modelspaper combine two ideas, creating decision diagram AND/OR structure, thus exploiting problem decomposition. detail, number values also increasedtwo constant. context constraint networks, decision diagrams usedrepresent whole set solutions, facilitating solutions count, solution enumeration queriesequivalence constraint networks. benefit moving structure AND/ORlower complexity algorithms size compiled structure. typically movesbounded exponentially pathwidth pw , characteristic chain decompositionslinear structures, exponentially bounded treewidth w , characteristic tree466fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSstructures (Bodlaender & Gilbert, 1991) (it always holds w pw pw w log n,n number variables model). cases, compactness result achievedpractice often far smaller bounds suggest.decision diagram offers compilation propositional knowledge-base. extensionOBDDs provided Algebraic Decision Diagrams (ADD) (Bahar, Frohm, Gaona, Hachtel,Macii, Pardo, & Somenzi, 1993), terminal nodes 0 1, take valuesarbitrary finite domain. knowledge compilation approach become important researchdirection automated reasoning past decade (Selman & Kautz, 1996; Darwiche & Marquis,2002; Cadoli & Donini, 1997). Typically, knowledge representation language compiledcompact data structure allows fast responses various queries. Accordingly, computationaleffort divided offline online phase work pushedoffline. Compilation also used generate compact building blocks used onlinealgorithms multiple times. Macro-operators compiled prior search viewedlight (Korf & Felner, 2002), graphical models building blocks functionswhose compact compiled representations used effectively across many tasks.one example, consider product configuration tasks imagine user chooses sequential options configure product. naive system, user would allowed choose validoption current level based initial constraints, either product configured,else, dead-end encountered, system would backtrack previous statecontinue there. would fact search space possible partial configurations. Needless say, would unpractical, would offer user guaranteefinishing limited time. system based compilation would actually build backtrack-freesearch space offline phase, represent compact manner. online phase,valid partial configurations (i.e., extended full valid configuration) allowed,depending query type, response time guarantees offered terms sizecompiled structure.Numerous examples, diagnosis planning problems, formulatedgraphical models could benefit compilation (Palacios, Bonet, Darwiche, & Geffner, 2005;Huang & Darwiche, 2005a). diagnosis, compilation facilitate fast detection possible faultsexplanations unusual behavior. Planning problems also formulated graphicalmodels, compilation would allow swift adjustments according changes environment.Probabilistic models one used types graphical models, basic querycompute conditional probabilities variables given evidence. compact compilationprobabilistic model would allow fast response queries incorporate evidence acquired time.example, two important tasks Bayesian networks computing probabilityevidence, computing maximum probable explanation (MPE). modelvariables become assigned (evidence), tasks performed time linear compilation size, practice many cases smaller upper-bound based treewidthpathwidth graph. Formal verification another example compilation heavily usedcompare equivalence circuit design, check behavior circuit. Binary DecisionDiagram (BDD) (Bryant, 1986) arguably widely known used compiled structure.contributions made paper knowledge compilation general decision diagrams particular following:1. formally describe AND/OR Multi-Valued Decision Diagram (AOMDD) provecanonical representation constraint networks, given pseudo tree.467fiM ATEESCU , ECHTER & ARINESCU2. extend AOMDD general weighted graphical models.3. give compilation algorithm based AND/OR search, saves trace memoryintensive search reduces one bottom pass.4. present APPLY operator combines two AOMDDs show complexityquadratic input, never worse exponential treewidth.5. give scheduling order building AOMDD graphical model startingAOMDDs functions based Variable Elimination algorithm.guarantees complexity exponential induced width (treewidth) alongordering.6. show AOMDDs relate various earlier recent compilation frameworks, providing unifying perspective methods.7. introduce semantic treewidth, helps explain compiled decision diagramsoften much smaller worst case bound.8. provide experimental evaluation new data structure.structure paper follows. Section 2 provides preliminary definitions, descriptionbinary decision diagrams Bucket Elimination algorithm. Section 3 gives overviewAND/OR search spaces. Section 4 introduces AOMDD discusses properties. Section5 describes search-based algorithm compiling AOMDD. Section 6 presents compilationalgorithm based Bucket Elimination schedule APPLY operation. Section 7 provesAOMDD canonical representation constraint networks given pseudo tree, Section8 extends AOMDD weighted graphical models proves canonicity. Section 9 tiescanonicity new concept semantic treewidth. Section 10 provides experimentalevaluation. Section 11 presents related work Section 12 concludes paper. proofsappear appendix.2. PreliminariesNotations reasoning problem defined terms set variables taking values finitedomains set functions defined variables. denote variables subsetsvariables uppercase letters (e.g., X, Y, . . .) values variables lower case letters (e.g.,x, y, . . .). Sets usually denoted bold letters, example X = {X1 , . . . , Xn } setvariables. assignment (X1 = x1 , . . . , Xn = xn ) abbreviated x = (hX1 , x1 i, . . . ,hXn , xn i) x = (x1 , . . . , xn ). subset variables Y, DY denotes Cartesian productdomains variables Y. projection assignment x = (x1 , . . . , xn ) subsetdenoted xY x[Y]. also denote = (or short) assignment valuesvariables respective domains. denote functions letters f , g, h etc.,scope (set arguments) function f scope(f ).2.1 Graphical ModelsEFINITION 1 (graphical model) graphical model 4-tuple, = hX, D, F, i, where:468fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS1. X = {X1 , . . . , Xn } finite set variables;2. = {D1 , . . . , Dn } set respective finite domains values;3. F = {f1 , . . . , fr } set positive real-valued discrete functions (i.e., domainslisted), defined subset variables Si X, called scope, denotedscope(fi ).Q P4. combination operator1 (e.g., { , , 1} product, sum, join), takeinput two (or more) real-valued discrete functions, produce another real-valued discretefunction.graphical model represents combination functions: ri=1 fi .Several examples graphical models appear later, example: Figure 1 shows constraintnetwork Figure 2 shows belief network.order define equivalence graphical models, useful introduce notionuniversal graphical model defined single function.EFINITION 2 (universal equivalent graphical model) Given graphical model=hX, D, F1 , universal equivalent model u(M) = hX, D, F2 = {fi F1 fi }, i.Two graphical models equivalent represent function. Namely,universal model.EFINITION 3 (weight full partial assignment) Given graphical model =hX, D, Fi, weight full assignment x = (x1 , . . . , xn ) defined w(x) =f F f (x[scope(f )]). Given subset variables X, weight partial assignmentcombination functions whose scopes included (denoted FY ) evaluatedassigned values. Namely, w(y) = f FY f (y[scope(f )]).Consistency graphical models, range functions special zero value 0absorbing relative combination operator (e.g., multiplication). Combining anything0 yields 0. 0 value expresses notion inconsistent assignments. primaryconcept constraint networks also defined relative graphical models0 element.EFINITION 4 (consistent partial assignment, solution) Given graphical model 0element, partial assignment consistent cost non-zero. solution consistent assignment variables.EFINITION 5 (primal graph) primal graph graphical model undirected graphvariables vertices edge connects two variables appear scopefunction.primal graph captures structure knowledge expressed graphical model.particular, graph separation indicates independency sets variables given assignmentsvariables. advanced algorithms graphical models exploit graphical structure,using heuristically good elimination order, tree decomposition similar method.use concept pseudo tree, resembles tree rearrangements introduced FreuderQuinn (1985):1. combination operator also defined axiomatically (Shenoy, 1992).469fiM ATEESCU , ECHTER & ARINESCUEEBFBGFCGC(a) Graph coloring problem(b) Constraint graphFigure 1: Constraint networkEFINITION 6 (pseudo tree) pseudo tree graph G = (X, E) rooted treeset nodes X, every arc E backarc (A path rooted tree startsroot ends one leaf. Two nodes connected backarc exists pathcontains both).use common concepts parameters graph theory, characterize connectivity graph, close tree chain. induced width graphical modelgoverns complexity solving Bucket Elimination (Dechter, 1999), also shownbound AND/OR search graph memory used cache solved subproblems (Dechter &Mateescu, 2007).EFINITION 7 (induced graph, induced width, treewidth, pathwidth) ordered graphpair (G, d), G = ({X1 , . . . , Xn }, E) undirected graph, = (X1 , . . . , Xn )ordering nodes. width node ordered graph number neighborsprecede ordering. width ordering d, denoted w(d), maximum widthnodes. induced width ordered graph, w (d), width induced ordered graphobtained follows: node, last first d, preceding neighbors connectedclique. induced width graph, w , minimal induced width orderings.induced width also equal treewidth graph. pathwidth pw graphtreewidth restricted class orderings correspond chain decompositions.Various reasoning tasks, queries defined graphical models. defined formally using marginalization operators projection, summation minimization.However, since goal present compilation graphical model independentqueries posed it, discuss tasks informal manner only.information see work Kask, Dechter, Larrosa, Dechter (2005).Throughout paper, use two examples graphical models: constraint networksbelief networks. case constraint networks, functions understood relations. words, functions (also called constraints) take two values, {0, 1},{f alse, true}. 0 value indicates corresponding assignment variables inconsistent (not allowed), 1 value indicates consistency. Belief networks examplegeneral case graphical models (also called weighted graphical models). functions caseconditional probability tables, values function real numbers interval [0, 1].470fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSExample 1 Figure 1(a) shows graph coloring problem modeled constraint network. Given map regions, problem color region one given colors {red,green, blue}, neighboring regions different colors. variables problemsregions, one domain {red, green, blue}. constraints relationdifferent neighboring regions. Figure 1(b) shows constraint graph, solution(A=red, B=blue, C=green, D=green, E=blue, F=blue, G=red) given Figure 1(a).detailed example given later Example 8.Propositional Satisfiability special case CSP propositional satisfiability (SAT). formula conjunctive normal form (CNF) conjunction clauses 1 , . . . , , clausedisjunction literals (propositions negations). example, = (P Q R)clause, P , Q R propositions, P , Q R literals. SAT problemdecide whether given CNF theory model, i.e., truth-assignment propositionsviolate clause. Propositional satisfiability (SAT) defined CSP,propositions correspond variables, domains {0, 1}, constraints represented clauses,example clause (A B) relation propositional variables allows tuples(A, B) except (A = 1, B = 0).Cost Networks immediate extension constraint networks cost networks setfunctions real-valued cost functions, primary task optimization. Also, GAI-nets(generalized additive independence, Fishburn, 1970) used represent utility functions.example cost functions appear Figure 19.EFINITIONP8 (cost network, combinatorial optimization) cost network 4-tuple,hX, D, C, i, X set variables X = {X1 , . . . , Xn }, associated setdiscrete-valued domains, = {D1 , . . . , Dn }, set cost functions C = {C1 , . . . , Cr }.Ci real-valued function defined subset variables Si X. combination operator,P. reasoning problem find minimum cost solution.Belief Networks (Pearl, 1988) provide formalism reasoning partial beliefs conditions uncertainty. defined directed acyclic graph vertices representing randomvariables interest (e.g., temperature device, gender patient, feature object, occurrence event). arcs signify existence direct causal influenceslinked variables quantified conditional probabilities attached cluster parentschild vertices network.QEFINITION 9 (belief networks) belief network (BN) graphical model P = hX, D, PG , i,X = {X1 , . . . , Xn } set variables domains = {D1 , . . . , Dn }. Given directed acyclic graph G X nodes, PG = {P1 , . . . , Pn }, Pi = {P (Xi | pa (Xi ) ) }conditional probability tables (CPTs short) associated Xi , pa(Xi )parents XiQacyclic graph G. belief network represents probability distribution X,P (x1 , . . . , xn ) = ni=1 P (xi |xpa(Xi ) ). evidence set e instantiated subset variables.formulated graphical model, functions F denote conditional probability tablesscopes functions determined directed acyclic graph G: functionQfi ranges variable Xi parents G. combination operator product, = .primal graph belief network (viewed undirected model) called moral graph.connects two variables appearing CPT.471fiM ATEESCU , ECHTER & ARINESCUSeasonSprinkler BWateringC RainBF WetnessG SlipperyCFG(a) Directed acyclic graph(b) Moral graphFigure 2: Belief networkExample 2 Figure 2(a) gives example belief network 6 variables, Figure 2(b)shows moral graph . example expresses causal relationship variables Season(A), configuration automatic sprinkler system (B), amount rain expected(C), amount manual watering necessary (D), wetness pavement (F )Whether pavement slippery (G). belief network expresses probability distribution P (A, B, C, D, F, G) = P (A) P (B|A) P (C|A) P (D|B, A) P (F |C, B) P (G|F ).Another example belief network CPTs appears Figure 9.two popular tasks belief networks defined below:EFINITION 10 (belief updating, probable explanation (MPE)) Given belief networkevidence e, belief updating task compute posterior marginal probability variableXi , conditioned evidence. Namely,XBel(Xi = xi ) = P (Xi = xi | e) =nP (xk , e|xpak ),{(x1 ,...,xi1 ,xi+1 ,...,xn )|E=e,Xi =xi } k=1normalization constant. probable explanation (MPE) task findcomplete assignment agrees evidence, highest probability amongassignments. Namely, find assignment (xo1 , . . . , xon )P (xo1 , . . . , xon ) = maxx1 ,...,xnnP (xk , e|xpak ).k=12.2 Binary Decision Diagrams ReviewDecision diagrams widely used many areas research represent decision processes.particular, used represent functions. Due fundamental importance Booleanfunctions, lot effort dedicated study Binary Decision Diagrams (BDDs),extensively used software hardware verification (Clarke et al., 1999; McMillan,1993). earliest work BDDs due Lee (1959), introduced binary-decision program, understood linear representation BDD (e.g., depth first search orderingnodes), node branching instruction indicating address next instruction 0 1 value test variable. Akers (1978) presented actual graphical472fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS00001111B00110011C01010101f(ABC)00010101BCC0C0(a) Table0BB10B01BC10(b) Unordered treeC00C10C101(c) Ordered treeFigure 3: Boolean function representationsrepresentation developed BDD idea. However, Bryant (1986) introducedcalled Ordered Binary Decision Diagram (OBDD). restricted order variables along path diagram, presented algorithms (most importantly apply procedure, combines two OBDDs operation) time complexity quadraticsizes input diagrams. OBDDs fundamental applications large binary functions,especially many practical cases provide compact representations.BDD representation Boolean function. Given B = {0, 1}, Boolean functionf : Bn B, n arguments, X1 , , Xn , Boolean variables, takes Booleanvalues.Example 3 Figure 3(a) shows table representation Boolean function three variables.explicit representation straightforward, also costly due exponentialrequirements. function also represented binary tree, shown Figure 3(b),exponential size number variables. internal round nodes representvariables, solid edges 1 (or high) value, dotted edges 0 (or low) value.leaf square nodes show value function assignment along path. treeshown 3(b) unordered, variables appear order along path.building OBDD, first condition variables appear order (A,B,C)along every path root leaves. Figure 3(c) shows ordered binary tree function.order imposed, two reduction rules transform decision diagramequivalent one:(1) isomorphism: merge nodes label children.(2) redundancy: eliminate nodes whose low high edges point node, connectparent removed node directly child removed node.Applying two reduction rules exhaustively yields reduced OBDD, sometimes denotedrOBDD. use OBDD assume completely reduced.Example 4 Figure 4(a) shows binary tree Figure 3(c) isomorphic terminal nodes(leaves) merged. highlighted nodes, labeled C, also isomorphic, Figure4(b) shows result merged. Now, highlighted nodes labeled C Bredundant, removing gives OBDD Figure 4(c).2.3 Bucket Elimination ReviewBucket Elimination (BE) (Dechter, 1999) well known variable elimination algorithm inference graphical models. describe using terminology constraint networks,473fiM ATEESCU , ECHTER & ARINESCUBBCCC01BCBCC0(a) Isomorphic nodesBC10(b) Redundant nodes1(c) OBDDFigure 4: Reduction rulesA:C1(AC)C2(AB)C3(ABE)BCC4(BCD)h4(A)B:C2(AB)E:C3(ABE)h3(AB)h2(AB)AB bucket-BABABEC:E(a) Constraint networkD:C1(AC)h1(BC)bucket-Abucket-EABABC bucket-CBCBCD bucket-DC4 (BCD)(b) execution(c) Bucket treeFigure 5: Bucket Eliminationalso applied graphical model. Consider constraint network R = hX, D, Ciordering = (X1 , X2 , . . . , Xn ). ordering dictates elimination order BE, lastfirst. variable associated bucket. constraint C placed bucketlatest variable d. Buckets processed Xn X1 eliminating bucket variable (theconstraints residing bucket joined together, bucket variable projected out)placing resulting constraint (also called message) bucket latest variable d.execution, renders network backtrack free, solution produced assigningvariables along d. also produce solutions count marginalization done summation(rather projection) functional representation constraints, join substitutedmultiplication.also constructs bucket tree, linking bucket Xi destination bucketmessage (called parent bucket). node bucket tree typically bucket variable,collection constraints, scope (the union scopes constraints). nodesbucket tree replaced respective bucket variables, easy see obtain pseudotree.Example 5 Figure 5(a) shows network four constraints. Figure5(b) shows executionBucket Elimination along = (A, B, E, C, D). buckets processed A.2 Figure5(c) shows bucket tree. pseudo tree corresponding order given Fig. 6(a).2. representation Figure 5 reverses top bucket processing described earlier papers (Dechter, 1999).474fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSProcedure GeneratePseudoTree(G, d)12345input : graph G = (X, E); order = (X1 , . . . , Xn )output : Pseudo treeMake X1 rootCondition X1 (eliminate X1 incident edges G). Let G1 , . . . , Gp resulting connectedcomponents G= 1 pTi = GeneratePseudoTree (Gi , d|Gi )Make root Ti child X16 return2.4 Orderings Pseudo TreesGiven ordering d, structural information captured primal graph scopesfunctions F = {f1 , . . . , fr } used create unique pseudo tree corresponds(Mateescu & Dechter, 2005). precisely bucket tree (or elimination tree),created (when variables processed reverse d). pseudo tree createdconditioning primal graph, processing variables order d, described ProcedureGeneratePseudoTree. following, d|Gi restriction order nodesgraph Gi .3. Overview AND/OR Search Space Graphical ModelsAND/OR search space recently introduced (Dechter & Mateescu, 2004a, 2004b, 2007)unifying framework advanced algorithmic schemes graphical models. main virtue consists exploiting independencies variables search, provide exponentialspeedups traditional search methods oblivious problem structure. Since AND/OR MDDsbased AND/OR search spaces need provide comprehensive overview sakecompleteness.3.1 AND/OR Search TreesAND/OR search tree guided pseudo tree primal graph. idea exploitproblem decomposition independent subproblems search. Assigning valuevariable (also known conditioning), equivalent graph terms removing variable (andincident edges) primal graph. partial assignment therefore lead decompositionresidual primal graph independent components, searched (or solved)separately. pseudo tree captures precisely decompositions given order variableinstantiation.EFINITION 11 (AND/OR search tree graphical model) Given graphical model =hX, D, Fi, primal graph G pseudo tree G, associated AND/OR search treealternating levels nodes. nodes labeled Xi correspondvariables. nodes labeled hXi , xi (or simply xi ) correspond value assignments.structure AND/OR search tree based . root node labeledroot . children node Xi nodes labeled assignments hXi , xi475fiM ATEESCU , ECHTER & ARINESCUB1BB0EE0C1C0 1E010 1(a) Pseudo tree0 10CE010 10 10 11CE010 10 10 1C010 10 10 1(b) Search treeFigure 6: AND/OR search treeconsistent assignments along path root. children nodehXi , xi nodes labeled children variable Xi pseudo tree .Example 6 Figure 6 shows example AND/OR search tree graphical model givenFigure 5(a), assuming tuples consistent, variables binary valued. tuplesinconsistent, paths tree exist. Figure 6(a) gives pseudo treeguides search, top bottom, indicated arrows. dotted arcs backarcsprimal graph. Figure 6(b) shows AND/OR search tree, alternating levels(circle) (square) nodes, structure indicated pseudo tree.AND/OR search tree traversed depth first search algorithm, thus using linearspace. already shown (Freuder & Quinn, 1985; Bayardo & Miranker, 1996; Darwiche, 2001;Dechter & Mateescu, 2004a, 2007) that:HEOREM 1 Given graphical model n variables, pseudo tree depth m,size AND/OR search tree based O(n k ), k bounds domains variables.graphical model treewidth w pseudo tree depth w log n, thereforeAND/OR search tree size O(n k w log n ).AND/OR search tree expresses set possible assignments problem variables(all solutions). difference traditional search space solution longerpath root leaf, rather tree, defined follows:EFINITION 12 (solution tree) solution tree AND/OR search tree contains root node.every node, contains one child nodes nodes containschild nodes, leaf nodes consistent.3.2 AND/OR Search GraphAND/OR search tree may contain nodes root identical subproblems. nodes saidunifiable. unifiable nodes merged, search space becomes graph. sizebecomes smaller expense using additional memory search algorithm. depth firstsearch algorithm therefore modified cache previously computed results, retrievenodes encountered again. notion unifiable nodes defined formally next.476fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSEFINITION 13 (minimal AND/OR graph, isomorphism) Two AND/OR search graphs G G0isomorphic exists one one mapping vertices G vertices G0vertex v, (v) = v 0 , v v 0 root identical subgraphs relative .AND/OR graph called minimal isomorphic subgraphs merged. Isomorphic nodes(that root isomorphic subgraphs) also said unifiable.shown Dechter Mateescu (2007) that:HEOREM 2 graphical model unique minimal AND/OR search graph relativepseudo-tree .minimal AND/OR graph graphical model G relative pseudo tree denotedMT (G). Note definition minimality used work Dechter Mateescu (2007)based isomorphism reduction. extend also including eliminationredundant nodes. previous theorem shows given AND/OR graph, mergeoperator fixed point, minimal AND/OR graph. show paperAOMDD canonical representation, namely two equivalent graphical modelsrepresented unique AOMDD given accept pseudo tree,AOMDD minimal terms number nodes.unifiable nodes identified based contexts. define graph basedcontexts nodes nodes, expressing set ancestor variablescompletely determine conditioned subproblem. However, shown using cachingbased contexts makes caching based contexts redundant vice versa,use caching. value assignment context X separates subproblem Xrest network.EFINITION 14 (OR context) Given pseudo tree AND/OR search space,context(X) = [X1 . . . Xp ] set ancestors X , ordered descendingly, connected primal graph X descendants X.EFINITION 15 (context unifiable nodes) Given AND/OR search graph, two nodes n1n2 context unifiable variable label X assignmentscontexts identical. Namely, 1 partial assignment variables along path n1 ,2 partial assignment variables along path n2 , restriction contextX same: 1 |context(X) = 2 |context(X) .depth first search algorithm traverses AND/OR search tree, modifiedtraverse graph, enough memory available. could allocate cache table variable X,scope table context(X). size cache table X therefore productdomains variables context. variable X, possible assignmentcontext, corresponding conditioned subproblem solved computedvalue saved cache table, whenever context assignment encountered again,value subproblem retrieved cache table. algorithm traversescalled context minimal AND/OR graph.EFINITION 16 (context minimal AND/OR graph) context minimal AND/OR graph obtained AND/OR search tree merging context unifiable nodes.477fiM ATEESCU , ECHTER & ARINESCURFGB[]CJK[C]H[C]L[CK][CH]N[CKL]B[CHA][CKLN]P[CKO]HECLE[CHAB]R[HAB]J[CHAE]F[AR][CEJ]G[AF][CD]KNP(a) Primal graph(b) Pseudo treeC00KHK0101LLLLH01010101010101010101NNNNNNNNBBBBBBBB01010101010101010E01EEE01EEE01EEEE01EEEE1E001RRR01RRRR1R0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 10 1 0 1 0 1 0 1 0 1PPPPPPP0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 10 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1PJJJJJJJJJJJJJJJJ0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 10 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 10 1 0 1 0 1 0 1FFFF0 1 0 1 0 1 0 10 1 0 1 0 1 0 1GGGG0 1 0 1 0 1 0 10 1 0 1 0 1 0 1(c) Context minimal graphFigure 7: AND/OR search graphalready shown (Bayardo & Miranker, 1996; Dechter & Mateescu, 2004a, 2007) that:HEOREM 3 Given graphical model M, primal graph G pseudo tree , sizecontext minimal AND/OR search graph based , therefore size minimal AND/ORsearch graph, O(n k wT (G) ), wT (G) induced width G depth first traversal, k bounds domain size.Example 7 Lets look impact caching size search space examining largerexample. Figure 7(a) shows graphical model binary variables Figure 7(b) pseudo treedrives AND/OR search. context node given square brackets. contextminimal graph given Figure 7(c). Note far smaller AND/OR search tree,28 = 256 nodes level alone (because depth 8 pseudo tree).shaded rectangles show size cache table, equal number nodesappear one. cache entry useful whenever one incoming edgesnode. Incidentally, caches useful (namely nodes one incomingarc), called dead caches (Darwiche, 2001), determined based pseudo478fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELStree inspection, therefore cache table need allocated them. context minimal graphalso explain execution along pseudo tree (or, equivalently, along depthfirst traversal order). buckets shaded rectangles, processing done bottom up.number possible assignments bucket equals number nodes appearit. message scope identical context bucket variable, messageidentical corresponding cache table. details relationship AND/ORsearch see work Mateescu Dechter (2005).3.3 Weighted AND/OR Graphsprevious subsections described structure AND/OR trees graphs. orderuse solve reasoning task, need define way using input function valuestraversal AND/OR graph. realized placing weights (or costs)OR-to-AND arcs, dictated function values. functions relevant contributeOR-to-AND arc weight, captured buckets relative pseudo tree:EFINITION 17 (buckets relative pseudo tree) Given graphical model = hX, D, F,pseudo tree , bucket Xi relative , denoted BT (Xi ), set functions whosescopes contain Xi included pathT (Xi ), set variables root Xi. Namely,BT (Xi ) = {f F|Xi scope(f ), scope(f ) pathT (Xi )}.function belongs bucket variable Xi iff scope fully instantiatedXi assigned. Combining values functions bucket, current assignment, gives weight OR-to-AND arc:EFINITION 18 (OR-to-AND weights) Given AND/OR graph graphical model M,weight w(n,m) (Xi , xi ) arc (n, m) Xi labels n xi labels m, combinationfunctions BT (Xi ) assigned values along current path node m, .Formally, w(n,m) (Xi , xi ) = f BT (Xi ) f (asgn(m )[scope(f )]).EFINITION 19 (weight solution tree) Given weighted AND/OR graph graphical modelM, given solution tree OR-to-AND set arcs arcs(t), weight definedw(t) = earcs(t) w(e).Example 8 start straightforward case constraint networks. Since functionstake values 0 1, combination product (join relations), follows ORto-AND arc weight 0 1. example given Figure 8. Figure 8(a) showsconstraint graph, 8(b) pseudo tree it, 8(c) four relations define constraintproblem. Figure 8(d) shows AND/OR tree traversed depth first search algorithmchecks consistency input functions (i.e., constraint propagation used).Similar OBDD representation, OR-to-AND arcs weight 0 denoted dottedlines, tree unfolded them, since contain solution. arcsweight 1 drawn solid lines.479fiM ATEESCU , ECHTER & ARINESCUBCFBE(a) Constraint graph00001111B00110011B00001111C RABC0111001101110110C00110011CEF(b) Pseudo treeRBCD011101100110011100001111B00110011E RABE011001110011011000001111E00110011F RAEF0011011101110110(c) Relations1101BB11110CCE110CECEE11100111110110100101010101010101F11 10 0101110010FF1 01 111111000FF1 10101011010101011(d) AND/OR treeFigure 8: AND/OR search tree constraint networksExample 9 Figure 9 shows weighted AND/OR tree belief network. Figure 9(a) showsdirected acyclic graph, dotted arc BC added moralization. Figure 9(b) shows pseudotree, 9(c) shows conditional probability tables. Figure 9(d) shows weighted AND/ORtree.constraint networks, move weighted AND/OR search treesweighted AND/OR search graphs merging unifiable nodes. case arc labelsalso considered determining unifiable subgraphs. yield context-minimal weightedAND/OR search graphs minimal weighted AND/OR search graphs.4. AND/OR Multi-Valued Decision Diagrams (AOMDDs)section begin describing contributions paper. context minimal AND/ORgraph (Definition 16) offers effective way identifying unifiable nodes execution search algorithm. Namely, context unifiable nodes discovered basedpaths root, without actually solving corresponding subproblems. However, merging based context complete, means may still exist unifiable nodessearch graph identical contexts. Moreover, nodes context480fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSP(A)01P(B | A)P(A).6.401P(C | A)01B=1.6.9B=0.4.1P(D | B,C)BBB0011CEEC(a) Belief networkC0101P(E | A,B)D=1.8.9.7.5D=0.2.1.3.5C=1.8.3C=0.2.7(b) Pseudo tree0011B0101E=0.4.5.7.2E=1.6.5.3.8(c) CPTs.6.401BB.400C.61E.2.8.5010.201.81.10.2.8.7010.91.30C0E.3.7.3101.7 .511EC.5.901E.4.1.6.2.501.20C.8.7.3101.81.10.91.30.7 .510.51(d) Weighted AND/OR treeFigure 9: Weighted AND/OR search tree belief networksminimal AND/OR graph may redundant, example set solutions rooted variable Xi dependant specific value assigned Xi (this situation detectable basedcontext). sometimes termed interchangeable values symmetrical values.overviewed earlier, Dechter Mateescu (2007, 2004a) defined complete minimal AND/ORgraph AND/OR graph whose unifiable nodes merged, Dechter Mateescu(2007) also proved canonicity non-weighted graphical models.paper propose augment minimal AND/OR search graph removing redundant variables common OBDD representation well adopt notational conventionscommon community. yields data structure call AND/OR BDD, exploitsdecomposition using nodes. present extension multi-valued variables yieldingAND/OR MDD AOMDD define general weighted graphical models. Subsequentlypresent two algorithms compiling canonical AOMDD graphical model: firstsearch-based, uses memory intensive AND/OR graph search generate context minimalAND/OR graph, reduces bottom applying reduction rules; second inferencebased, uses Bucket Elimination schedule combine AOMDDs initial functionsAPPLY operations (similar apply OBDDs). show, approachesworst case complexity AND/OR graph search context based caching, alsocomplexity Bucket Elimination, namely time space exponential treewidthproblem, O(n k w ). benefit generation schemes discussed.481fiM ATEESCU , ECHTER & ARINESCU1(a) OBDD2k(b) MDDFigure 10: Decision diagram nodes (OR)1(a) AOBDD2k(b) AOMDDFigure 11: Decision diagram nodes (AND/OR)4.1 AND/OR Search Graphs Decision DiagramsAND/OR search graph G graphical model = hX, D, F, represents setpossible assignments problem variables (all solutions costs). sense, Gviewed representing function f = fi F fi defines universal equivalent graphicalmodel u(M) (Definition 2). full assignment x = (x1 , . . . , xn ), x solution expressedtree tx , f (x) = w(tx ) = earcs(tx ) w(e) (Definition 19); otherwise f (x) = 0 (theassignment inconsistent). solution tree tx consistent assignment x read Glinear time following assignments root. x inconsistent, dead-endencountered G attempting read solution tree tx , f (x) = 0. Therefore, Gviewed decision diagram determines values f every complete assignment x.see process AND/OR search graph reduction rules similarcase OBDDs, order obtain representation minimal size. case OBDDs,node labeled variable name, example A, low (dotted line) high (solidline) outgoing arcs capture restriction function assignments = 0 = 1.determine value function, one needs follow either one (but both)outgoing arcs (see Figure 10(a)). straightforward extension OBDDs multi-valuedvariables (multi-valued decision diagrams, MDDs) presented Srinivasan, Kam, Malik,Brayton (1990), node structure use given Figure 10(b). outgoing arcassociated one k values variable A.paper generalize OBDD MDD representations demonstrated Figures 10(a)10(b) allowing outgoing arc arc. arc connects node setnodes, captures decomposition problem independent components. numberarcs emanating node two case AOBDDs (Figure 11(a)), domain sizevariable general case (Figure 11(b)). given node A, k arcsconnect possibly different number nodes, depending problem decomposes basedparticular assignment A. arcs depicted shaded sector connectsoutgoing lines corresponding independent components.482fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS12k(a) Nonterminal meta-node01(b) Terminal meta-node 0(c) Terminal meta-node 1Figure 12: Meta-nodesdefine AND/OR Decision Diagram representation based AND/OR search graphs.find useful maintain semantics Figure 11 especially need expressredundancy nodes, therefore introduce meta-node data structure, defines smallportions AND/OR graph, based node children:EFINITION 20 (meta-node) meta-node u AND/OR search graph either: (1)terminal node labeled 0 1, (2) nonterminal node, consists node labeledX (therefore var(u) = X) k children labeled x1 , . . . , xk correspond valueassignments X. node labeled xi stores list pointers child meta-nodes, denotedu.childreni . case weighted graphical models, node xi also stores OR-toAND arc weight w(X, xi ).rectangle Figure 12(a) meta-node variable A, domain size k. Notesimilar Figure 11, small difference information valuecorresponds outgoing arc stored nodes meta-node.showing weights figure. larger example AND/OR graph meta-nodesappears later Figure 16.terminal meta-nodes play role terminal nodes OBDDs. terminal metanode 0, shown Figure 12(b), indicates inconsistent assignments, terminal meta-node 1,shown figure 12(c) indicates consistent ones.AND/OR search graph viewed diagram meta-nodes, simply groupingnodes children, adding terminal meta-nodes appropriately.defined meta-nodes, easier see variable redundant respect outcome function based current partial assignment. variable redundantassignments leads set solutions.EFINITION 21 (redundant meta-node) Given weighted AND/OR search graph G representedmeta-nodes, meta-node u var(u) = X |D(X)| = k redundant iff:(a) u.children1 = . . . = u.childrenk(b) w(X, x1 ) = . . . = w(X, xk ).AND/OR graph G, contains redundant meta-node u, transformed equivalent graph G 0 replacing incoming arc u common list children u.children1 ,absorbing common weight w(X, x1 ) combination weight parent meta-nodecorresponding incoming arc, removing u outgoing arcs G.value X = x1 picked arbitrarily, isomorphic. u root483fiM ATEESCU , ECHTER & ARINESCUProcedure RedundancyReduction: AND/OR graph G; redundant meta-node u, var(u) = X; List meta-node parents u,denoted P arents(u).output : Reduced AND/OR graph G elimination u.1 P arents(u) empty2return independent AND/OR graphs rooted meta-nodes u.children1 , constant w(X, x1 )input3 forall v P arents(u) (assume var(v) == )4forall {1, . . . , |D(Y )|}5u v.childreni6v.childreni v.childreni \ {u}7v.childreni v.childreni u.children18w(Y, yi ) w(Y, yi ) w(X, x1 )9 remove u10 return reduced AND/OR graph GProcedure IsomorphismReduction: AND/OR graph G; isomorphic meta-nodes u v; List meta-node parents u, denotedP arents(u).output : Reduced AND/OR graph G merging u v.forall p P arents(u)u p.childrenip.childreni p.childreni \ {u}p.childreni p.childreni {v}input12345 remove u6 return reduced AND/OR graph Ggraph, common weight w(X, x1 ) stored separately constant. ProcedureRedundancyReduction formalizes redundancy elimination.EFINITION 22 (isomorphic meta-nodes) Given weighted AND/OR search graph G representedmeta-nodes, two meta-nodes u v var(u) = var(v) = X |D(X)| = kisomorphic iff:(a) u.childreni = v.childreni {1, . . . , k}(b) wu (X, xi ) = wv (X, xi ) {1, . . . , k}, (where wu , wv weights u v).Procedure IsomorphismReduction formalizes process merging isomorphic metanodes. Naturally, AND/OR graph obtained merging isomorphic meta-nodes equivalentoriginal one. define AND/OR Multi-Valued Decision Diagram:EFINITION 23 (AOMDD) AND/OR Multi-Valued Decision Diagram (AOMDD) weightedAND/OR search graph completely reduced isomorphic merging redundancy removal,namely:(1) contains isomorphic meta-nodes;(2) contains redundant meta-nodes.484fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS2k1Bc22kz11zkc(b) eliminating Bmeta-node(a) Fragment AOMDDFigure 13: Redundancy reduction12k12kB1C221C1Bk1e2kCk2k1(a) Fragment AOMDD2ke(b) merging isomorphic C meta-nodesFigure 14: Isomorphism reductionExample 10 Figure 13 shows example applying redundancy reduction rule portionAOMDD. left side, Figure 13(a), meta-node variable B redundant (wedont show weights OR-to-AND arcs, avoid cluttering figure). values{1, . . . , k} B lead set meta-nodes {c, d, . . . , y}, coupledarc. Therefore, meta-node B eliminated. result shown Figure 13(b),meta-nodes {c, d, . . . , y} z coupled arc outgoing = 1.Figure 14 show example applying isomorphism reduction rule. case,meta-nodes labeled C Figure 14(a) isomorphic (again, omit weights). resultmerging shown Figure 14(b).Examples AOMDDs appear Figures 16, 17 18. Note weight OR-toAND arc zero, descendant terminal meta-node 0. Namely, current pathdead-end, cannot extended solution, therefore linked directly 0.5. Using AND/OR Search Generate AOMDDsSection 4.1 described transform AND/OR graph AOMDD applyingreduction rules. Section 5.1 describe explicit algorithm takes input graphi485fiM ATEESCU , ECHTER & ARINESCUcal model, performs AND/OR search context-based caching obtain context minimalAND/OR graph, Section 5.2 give procedure applies reduction rules bottomobtain AOMDD.5.1 Algorithm AND/OR-S EARCH -AOMDDAlgorithm 1, called AND/OR-S EARCH -AOMDD, compiles graphical model AOMDD.memory intensive (with context-based caching) AND/OR search used create context minimal AND/OR graph (see Definition 16). input AND/OR-S EARCH -AOMDD graphicalmodel pseudo tree , also defines OR-context variable.variable Xi associated cache table, whose scope context Xi .ensures trace search context minimal AND/OR graph. list denoted LXi(see line 35), used variable Xi save pointers meta-nodes labeled Xi .lists used procedure performs bottom reduction, per layers AND/ORgraph (one layer contains nodes labeled one given variable). fringe searchmaintained stack called OPEN. current node (either node) denotedn, parent p, current path n . children current node denotedsuccessors(n). node n, Boolean attribute consistent(n) indicates current pathextended solution. information useful pruning search space.algorithm based two mutually recursive steps: Forward (beginning line 5)Backtrack (beginning line 29), call (or themselves) search terminates.forward phase, AND/OR graph expanded top down. two types nodes,OR, treated differently according semantics.node expanded, cache table variable checked (line 8). entrynull, link created already existing node roots graph equivalentcurrent subproblem. Otherwise, node expanded generating descendants.OR-to-AND weight (see Definition 18) computed line 13. value xi Xi checkedconsistency (line 14). least expensive check verify OR-to-AND weight non-zero.However, deterministic (inconsistent) assignments extracted form constraintnetwork. level constraint propagation performed step (e.g., look ahead, arcconsistency, path consistency, i-consistency etc.). computational overhead increase,hope pruning search space aggressively. note constraint propagationcrucial algorithm, complexity guarantees maintained even simpleweight check performed. consistent nodes added list successors n (line16), inconsistent ones linked terminal 0 meta-node (line 19).node n labeled hXi , xi expanded (line 20) based structure pseudotree. Xi leaf , n linked terminal 1 meta-node (line 22). Otherwise,node created child Xi (line 24).forward step continues long current node dead-end still unevaluatedsuccessors. backtrack phase triggered node empty set successors (line 29).Note that, successor processed, removed set successors line 42.backtrack reaches root (line 32), search complete, context minimal AND/OR graphgenerated, Procedure B OTTOM U P R EDUCTION called.backtrack step processes node (line 31), saves pointer cache,also adds pointer corresponding meta-node list LXi . consistent attribute486fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSAlgorithm 1: AND/OR EARCH - AOMDDinput : = hX, D, Fi; pseudo tree rooted X1 ; parents pai (OR-context) every variable Xi .output : AOMDD M.1 forall Xi XInitialize context-based cache table CacheXi (pai ) null entries23 Create new node t, labeled Xi ; consistent(t) true; push top OPEN4 OPEN 6=5n top(OPEN); remove n OPEN// Forward6successors(n)7n node labeled Xi// OR-expandCacheXi (asgn(n )[pai ]) 6= null8Connect parent n CacheXi (asgn(n )[pai ])9// Use cached pointer10111213141516171819elseforall xi DiCreate new node t, labeled hXi , xiw(X, xi )f (asgn(n )[pai ])f BT (Xi )hXi , xi consistent nconsistent(t) trueadd successors(n)elseconsistent(t) f alsemake terminal 0 child2021222324252627n node labeled hXi , xichildrenT (Xi ) ==make terminal 1 child nelseforall childrenT (Xi )Create new node t, labeledconsistent(t) f alseadd successors(n)282930313233Add successors(n) top OPENsuccessors(n) ==let p parent nn node labeled XiXi == X1Call BottomUpReduction procedure343536373839// Constraint Propagation// AND-expand// Backtrack// Search complete// begin reduction AOMDDCache(asgn(n )[pai ]) nAdd meta-node n list LXiconsistent(p) consistent(p) consistent(n)consistent(p) == f alseremove successors(p) OPENsuccessors(p)4041n node labeled hXi , xiconsistent(p) consistent(p) consistent(n);4243remove n successors(p)np487// Save cache// Check p dead-endfiM ATEESCU , ECHTER & ARINESCUProcedure BottomUpReduction: graphical model = hX, D, Fi; pseudo tree primal graph, rooted X1 ; Contextminimal AND/OR graph, lists LXi meta-nodes level Xi .output : AOMDD M.Let = {X1 , . . . , Xn } depth first traversal orderingn 1Let H hash table, initially emptyforall meta-nodes n LXiH(Xi , n.children1 , . . . , n.childrenki , wn (Xi , x1 ), . . . , wn (Xki , xki )) returns meta-nodepmerge n p AND/OR graphinput123456789101112else n redundanteliminate n AND/OR graphcombine weight parentelsehash n table H:H(Xi , n.children1 , . . . , n.childrenki , wn (Xi , x1 ), . . . , wn (Xki , xki )) n13 return reduced AND/OR graphparent p updated conjunction consistent(n). parent p becomesinconsistent, necessary check remaining successors (line 38). backtrackstep processes node (line 40), consistent attribute parent p updateddisjunction consistent(n).AND/OR search algorithm usually maintains value node, corresponding tasksolved. include values description AOMDD equivalentrepresentation original graphical model M. task solved traversalAOMDD. however user include information meta-nodes (e.g.,number solutions subproblem).5.2 Reducing Context Minimal AND/OR Graph AOMDDProcedure BottomUpReduction processes variables bottom relative pseudo tree .use depth first traversal ordering (line 1), bottom ordering good.outer loop (starting line 2) goes level context minimal AND/OR graph(where level contains nodes labeled variable, wordscontains meta-nodes variable). efficiency, ensure complexity guaranteesprove, hash table, initially empty, used level. inner loop (startingline 4) goes metanodes level, also saved (or pointers saved)list LXi . new meta-node n list LXi , line 5 hash table H checkedverify node isomorphic n already exists. hash table H already contains node p corresponding hash key (Xi , n.children1 , . . . , n.childrenki , wn (Xi , x1 ), . . . , wn (Xki , xki )),p n isomorphic merged. Otherwise, new meta-node n redundant,eliminated AND/OR graph. none previous two conditions met,new meta-node n hashed table H.488fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSGCBFEHBC(a)FEGH(b)Figure 15: (a) Constraint graph C = {C1 , . . . , C9 }, C1 = F H, C2 = H,C3 = B G, C4 = F G, C5 = B F , C6 = E, C7 = C E, C8 = C D,C9 = B C; (b) Pseudo tree (bucket tree) ordering = (A, B, C, D, E, F, G, H)Proposition 1 output Procedure BottomUpReduction AOMDD alongpseudo tree , namely resulting AND/OR graph completely reduced.Note explicated Procedure BottomUpReduction separately clarity. practice, actually included Algorithm AND/OR-S EARCH -AOMDD, reduction rulesapplied whenever search backtracks. maintain hash table variable, AND/OR search, store pointers meta-nodes. search backtracksnode, already check redundancy meta-node, also look hash tablecheck isomorphism. Therefore, reduction AND/OR graph doneAND/OR search, output AOMDD M.Theorem 3 Proposition 1 conclude:HEOREM 4 Given graphical model pseudo tree primal graph G, AOMDDcorresponding size bounded O(n k wT (G) ) computed AlgorithmAND/OR-S EARCH -AOMDD time O(n k wT (G) ), wT (G) induced width Gdepth first traversal , k bounds domain size.6. Using Bucket Elimination Generate AOMDDssection propose use Bucket Elimination (BE) type algorithm guide compilationgraphical model AOMDD. idea express graphical model functionsAOMDDs, combine APPLY operations based schedule. APPLYsimilar OBDDs (Bryant, 1986), adapted AND/OR search graphs.takes input two functions represented AOMDDs based pseudo tree, outputscombination initial functions, also represented AOMDD based pseudo tree.describe detail Section 6.2.start example based constraint networks. easier understandweights arcs 1 0, therefore depicted figures solid dashedlines, respectively.Example 11 Consider network defined X = {A, B, . . . , H}, DA = . . . = DH = {0, 1}constraints (where denotes XOR): C1 = F H, C2 = AH, C3 = ABG, C4 = F G,489fiM ATEESCU , ECHTER & ARINESCUm7m7001B010C0C101001C101BB00FC1B101F101F1BF0101BC0GE1000101m6G10H1001E1m3FH1Gm3m600BC0C0101100CC100FB010100G100100H01BF1G11FF1F01B1B1E11B0H1H10F1C0 1010m5C9C0E0C1000E1C101001G1010C0 1C6C710101010BH0 1110H1GEC3011G0GFF0BGHm1m11B0 1C8m21EE10C5m21m4m500 11m4C40H10F1F0 10 1GC1C2HFigure 16: Execution AOMDDs01B0B10B1BCC0C10C10C10F10F10F10C101E010E10G10G10H10CCF0EFH1EFFF1GGGGH01H1(a)G0(b)Figure 17: (a) final AOMDD; (b) OBDD correspondingC5 = B F , C6 = E, C7 = C E, C8 = C D, C9 = B C. constraint graphshown Figure 15(a). Consider ordering = (A, B, C, D, E, F, G, H). pseudo tree (orbucket tree) induced given Fig. 15(b). Figure 16 shows execution AOMDDsalong ordering d. Initially, constraints C1 C9 represented AOMDDs placedbucket latest variable d. scope original constraint always appears490fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSAlgorithm 2: BE-AOMDD: Graphical model = hX, D, Fi, X = {X1 , . . . , Xn }, F = {f1 , . . . , fr } ; order= (X1 , . . . , Xn )output : AOMDD representing fi1 = GeneratePseudoTree(G, d);2 1 r// place functions buckets3place Gfaomddbucketlatestvariableinput4 n 1message(Xi ) G1aomdd56bucket(Xi ) 6=7pick Gfaomdd bucket(Xi );8910// process buckets// initialize AOMDD 1 ;// combine AOMDDs bucket Xibucket(Xi ) bucket(Xi ) \ {Gfaomdd };message(Xi ) APPLY(message(Xi ), Gfaomdd )add message(Xi ) bucket parent Xi11 return message(X1 )path root leaf pseudo tree. Therefore, original constraint representedAOMDD based chain (i.e., branching independent components point).chain scope constraint, ordered according d. bi-valued variables,original constraints represented OBDDs, multiple-valued variables MDDs. Notedepict meta-nodes: one node two children, appear inside graynode. dotted edge corresponds 0 value (the low edge OBDDs), solid edge1 value (the high edge). redundancy notation, keeping value nodesarc-types (dotted arcs 0 solid arcs 1).scheduling used process buckets reverse order d. bucket processedjoining AOMDDs inside it, using APPLY operator. However, step eliminationbucket variable omitted want generate full AOMDD. example,messages m1 = C1 ./ C2 m2 = C3 ./ C4 still based chains, thereforeOBDDs. Note contain variables H G, eliminated. However,message m3 = C5 ./ m1 ./ m2 OBDD anymore. see followsstructure pseudo tree, F two children, G H. nodes correspondingF two outgoing edges value 1.processing continues manner. final output algorithm, coincidesm7 , shown Figure 17(a). OBDD based ordering shown Fig.17(b). Notice AOMDD 18 nonterminal nodes 47 edges, OBDD 27nonterminal nodes 54 edges.6.1 Algorithm BE-AOMDDAlgorithm 2, called BE-AOMDD, creates AOMDD graphical model using schedule APPLY operations. Given order variables, first pseudo tree created based,primal graph. initial function fi represented AOMDD, denoted Gfaomddplaced bucket. obtain AOMDD function, scope function orderedaccording d, search tree (based chain) represents fi generated, reducedProcedure BottomUpReduction. algorithm proceeds exactly like BE, difference combination functions realized APPLY algorithm, variables491fiM ATEESCU , ECHTER & ARINESCUeliminated carried destination bucket. messages buckets initializeddummy AOMDD 1, denoted G1aomdd , neutral combination.order create compilation graphical model based AND/OR graphs, necessarytraverse AND/OR graph top bottom up. similar inward outwardmessage passing tree decomposition. Note BE-AOMDD describes bottom traversalexplicitly, top phase actually performed APPLY operation. twoAOMDDs combined, top chain portion pseudo tree processed, remainingindependent branches attached participate newly restricted set solutions.amounts exchange information independent branches, equivalenttop phase.6.2 AOMDD APPLY Operationdescribe combine two AOMDDs. APPLY operator takes input twoAOMDDs representing functions f1 f2 returns AOMDD representing f1 f2 .OBDDs apply operator combines two input diagrams based variable ordering.Likewise, order combine two AOMDDs assume pseudo trees identical.condition satisfied two AOMDDs bucket BE-AOMDD. However,present version APPLY general, relaxing previous conditionidentical compatible pseudo trees. Namely, pseudo treeembedded. general, pseudo tree induces strict partial order variablesparent node always precedes child nodes.EFINITION 24 (compatible pseudo trees) strict partial order d1 = (X, <1 ) set Xconsistent strict partial order d2 = (Y, <2 ) set Y, x1 , x2 X Y,x1 <2 x2 x1 <1 x2 . Two partial orders d1 d2 compatible iff exists partialorder consistent both. Two pseudo trees compatible iff partial orders inducedvia parent-child relationship, compatible.simplicity, focus restricted notion compatibility, sufficientusing like schedule APPLY operator combine input AOMDDs (as describedSection 6). APPLY algorithm present extended general notioncompatibility.EFINITION 25 (strictly compatible pseudo trees) pseudo tree T1 set nodes X1embedded pseudo tree set nodes X X1 X T1 obtaineddeleting node X \ X1 connecting parent descendents. Twopseudo trees T1 T2 strictly compatible exists T1 T2embedded .Algorithm APPLY (algorithm 3) takes input one node Gfaomdd list nodesGgaomdd . Initially, node Gfaomdd root node, list nodes Ggaomdd factalso made one node, root. sometimes identify AOMDD rootnode. pseudo trees Tf Tg strictly compatible, target pseudo tree .list nodes Ggaomdd always special property: nodeancestor another (we refer variable meta-node). Therefore, list z1 , . . . , zm492fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSAPPLY (v1 ; z1 , . . . , zm )input : AOMDDs Gfaomdd nodes vi Ggaomdd nodes zj , based strictly compatible pseudotrees Tf , Tg embedded .var(v1 ) ancestor var(z1 ), . . . , var(zm ) .var(zi ) var(zj ) ancestor-descendant relation , 6= j.output : v1 (z1 . . . zm ), based .H1 (v1 , z1 , . . . , zm ) 6= null return H1 (v1 , z1 , . . . , zm );// cache(any v1 , z1 , . . . , zm 0) return 0(v1 = 1) return 1(m = 0) return v1// nothing combinecreate new nonterminal meta-node uvar(u) var(v1 ) (call Xi , domain Di = {x1 , . . . , xki } )j 1 kiu.childrenj// children j-th node u// assign weight v1wu (Xi , xj ) wv1 (Xi , xj )( (m = 1) (var(v1 ) = var(z1 ) = Xi ) )temp Children z1 .childrenj// combine input weightswu (Xi , xj ) wv1 (Xi , xj ) wz1 (Xi , xj )Algorithm 3:1234567891011121314else1516171819group nodes v1 .childrenj temp Children several {v 1 ; z 1 , . . . , z r }{v 1 ; z 1 , . . . , z r }APPLY(v 1 ; z 1 , . . . , z r )(y = 0)u.childrenj 0; break2021temp Children {z1 , . . . , zm }elseu.childrenj u.childrenj {y}222324(u.children1 = . . . = u.childrenki ) (wu (Xi , x1 ) = . . . = wu (Xi , xki ))promote wu (Xi , x1 ) parentreturn u.children1// redundancy2526(H2 (Xi , u.children1 , . . . , u.childrenki , wu (Xi , x1 ), . . . , wu (Xki , xki )) 6= null)return H2 (Xi , u.children1 , . . . , u.childrenki , wu (Xi , x1 ), . . . , wu (Xki , xki ))// isomorphism27 Let H1 (v1 , z1 , . . . , zm ) = u28 Let H2 (Xi , u.children1 , . . . , u.childrenki , w u (Xi , x1 ), . . . , w u (Xki , xki )) = u29 return u// add u H1// add u H2g expresses decomposition respect , nodes appear different branches.employ usual techniques OBDDs make operation efficient. First, onearguments 0, safely return 0. Second, hash table H1 used store nodesalready processed, based nodes (v1 , z1 , . . . , zr ). Therefore, never needmake multiple recursive calls arguments. Third, hash table H2 used detectisomorphic nodes. typically split separate tables variable. endrecursion, returning value, discover meta-node variable,children weights already created, dont need store simplyreturn existing node. fourth, end recursion discover createdredundant node (all children weights same), dont storeit, return instead one identical lists children, promote common weight.493fiM ATEESCU , ECHTER & ARINESCU00001111B00110011C01010101f(ABC)0001010100001111BCA100CA3*0A4B2A5B30BB11C000A1B1B1B41B011g(ABC)0001011001001010101B11BA2B001100110B6=BA2B2B701C0B50A4B61A411B410B01B71011Figure 18: Example APPLY operationNote v1 always ancestor z1 , . . . , zm . consider variableancestor itself. self explaining checks performed lines 1-4. Line 2 specificmultiplication, needs changed combination operations. algorithm createsnew meta-node u, whose variable var(v1 ) = Xi recall var(v1 ) highest (closest root)among v1 , z1 , . . . , zm . Then, possible value Xi , line 7, starts building listchildren.One important steps happens line 15. two lists meta-nodes, oneoriginal AOMDD f g, refer variables, appear .lists important property mentioned above, nodes ancestorsother. union two lists grouped maximal sets nodes, highest nodeset ancestor others. follows root node set belongs oneoriginal AOMDD, say v 1 f , others, say z 1 , . . . , z r g. example,suppose pseudo tree Fig. 15(b), two lists {C, G, H} f {E, F }g. grouping line 15 create {C; E} {F ; G, H}. Sometimes, maycase newly created group contains one node. means nothing joinrecursive calls, algorithm return, via line 4, single node. on, oneinput AOMDDs traversed, important complexity APPLY, discussedbelow.Example 12 Figure 18 shows result combining two Boolean functions operation(or product). input functions f g represented AOMDDs based chain pseudotrees, results based pseudo tree expresses decomposition variablesB instantiated. APPLY operator performs depth first traversal two inputAOMDDs, generates resulting AOMDD based output pseudo tree. Similarcase OBDDs, function AOMDD identified root meta-node. exampleinput meta-nodes labels (A1 , A2 , B1 , B2 , etc.). output meta-node labeled A2 B2494fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSroot diagram represents function obtained combining functions rooted A2B2 .6.3 Complexity APPLY BE-AOMDDprovide characterization complexity APPLY, based different criteria.following propositions inspired results govern OBDD apply complexity,adapted pseudo tree orderings.AOMDD along pseudo tree regarded union regular MDDs, restrictedfull path root leaf pseudo tree. Let path . Baseddefinition strictly compatible pseudo trees, corresponding paths Tf Tf Tg Tg .MDDs f g corresponding Tf Tg combined using regular MDDapply. process repeated every path . resulting MDDs, one pathneed synchronized common parts (on intersection paths). algorithmproposed processing once, depth first search traversal inputs. Basedconstruction, give first characterization complexity AOMDD APPLYgoverned complexity MDD apply.Proposition 2 Let 1 , . . . , l set paths enumerated left right let GfiGgi MDDs restricted path , size output AOMDD applyPP bounded| |G | n max |G | |G |. time complexity also bounded| |G ||G|Ggggfffn maxi |Gfi | |Ggi |.second characterization complexity given, similar MDD case, termstotal number nodes inputs:Proposition 3 Given two AOMDDs Gfaomdd Ggaomdd based strictly compatible pseudo trees,size output APPLY O(| Gfaomdd | | Ggaomdd |).detail previous proposition follows. Given AOMDDs Gfaomdd Ggaomdd ,based compatible pseudo trees Tf Tg common pseudo tree , define intersection pseudo tree Tf g obtained following two steps: (1) marksubtrees whose nodes belong either Tf Tg (the leaves subtreeleaves ); (2) remove subtrees marked step (1) . Steps (1) (2) applied(that is, recursively). part AOMDD Gfaomdd corresponding variables Tf gdenoted Gff g , similarly Ggaomdd denoted Ggf g .Proposition 4 time complexity|Gfaomdd | + |Ggaomdd |).APPLYsize output O(|Gff g | |Ggf g | +turn complexity BE-AOMDD algorithm. bucket associatedbucket pseudo tree. top chain bucket pseudo tree variable Xi containsvariables context(Xi ). variables appear bucket pseudo tree,associated buckets already processed. original functions belong bucketXi scope included context(Xi ), therefore associated AOMDDs based495fiM ATEESCU , ECHTER & ARINESCUchains. functions appear bucket Xi messages received independent branches below. Therefore, two functions bucket Xi share variablescontext(Xi ), forms top chain bucket pseudo tree. therefore characterizecomplexity APPLY terms treewidth, context size bucket variable.Proposition 5 Given two AOMDDs bucket BE-AOMDD, time space complexity APPLY exponential context size bucket variable(namely number variables top chain bucket pseudo tree).bound complexity BE-AOMDD output size:HEOREM 5 space complexity BE-AOMDD size output AOMDDO(n k w ), n number variables, k maximum domain size w treewidthbucket tree. time complexity bounded O(r k w ), r number initialfunctions.7. AOMDDs Canonical Representationswell known OBDDs canonical representations Boolean functions given orderingvariables (Bryant, 1986), namely strict ordering CNF specificationBoolean function yield identical OBDD, property extends MDDs (Srinivasanet al., 1990). linear ordering variables defines chain pseudo tree capturesstructure OBDD MDD. case AOBDDs AOMDDs, canonicityrespect pseudo tree, transitioning total orders (that correspond linear ordering)partial orders (that correspond pseudo tree ordering). one hand gain abilitycompact compiled structure, hand canonicity longer respectequivalent graphical models, relative graphical models consistentpseudo tree used. Specifically, start strict ordering generate chainAOMDD canonical relative equivalent graphical models. however wantexploit additional decomposition use partial ordering captured pseudo-tree createcompact AOMDD. AOMDD however canonical relative equivalent graphicalmodels accept pseudo tree guided AOMDD. general, AOMDDviewed flexible framework compilation allows partial total orderings.Canonicity restricted subset graphical models whose primal graph agrees partialorder relevant larger set orderings consistent pseudo-tree.following subsection discuss canonicity AOMDD constraint networks.case general weighted graphical models discussed Section 8.7.1 AOMDDs Constraint Networks Canonical Representationscase constraint networks straightforward, weights OR-to-ANDarcs 0 1. show equivalent constraint networks, admitpseudo tree , AOMDD based . start proposition help provemain theorem.Proposition 6 Let f function, always zero, defined constraint network X. Givenpartition {X1 , . . . , Xm } set variables X (namely, Xi Xj = , 6= j, X =496fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSi=1 X ), f = f1 . . . fm f = g1 . . . gm , scope(fi ) = scope(gi ) = X{1, . . . , m}, fi = gi {1, . . . , m}. Namely, f decomposedgiven partition, decomposition unique.ready show AOMDDs constraint networks canonical representationsgiven pseudo tree.HEOREM 6 (AOMDDs canonical given pseudo tree) Given constraint network,pseudo tree constraint graph, unique (up isomorphism) AOMDD represents it, minimal number meta-nodes.constraint network defined relations (or functions). exist equivalent constraintnetworks defined different sets functions, even different scope signatures.However, equivalent constraint networks define function, ask AOMDDdifferent equivalent constraint networks same. following corollary derivedimmediately Theorem 6.Corollary 1 Two equivalent constraint networks admit pseudo treeAOMDD based .8. Canonical AOMDDs Weighted Graphical ModelsTheorem 6 ensures AOMDD canonical constraint networks, namely functionstake values 0 1. proof relied fact OR-to-AND weights0 1, Proposition 6 ensured unique decomposition function definedconstraint network.section turn general weighted graphical models. first observe Proposition 6 longer valid general functions. valid solutions (having strictlypositive weight) weight decomposed one way product positiveweights.Therefore raise issue recognizing nodes root AND/OR graphs representuniversal function, even though graphical representation different. seeAOMDD weighted graphical model unique current definitions,slightly modify obtain canonicity again. note canonicity AOMDDsweighted graphical models (e.g., belief networks) far less crucial case OBDDsused formal verification. Even that, sometimes may useful eliminateredundant nodes, order maintain simpler semantics AND/OR graph representsmodel.loss canonicity AOMDD weighted graphical models happenweights OR-to-AND arcs, suggest possible way re-enforcing compactcanonical representation needed.Example 13 Figure 19 shows weighted graphical model, defined two (cost) functions,f (M, A, B) g(M, B, C). Assuming order (M,A,B,C), Figure 20 shows AND/OR searchtree left. arcs labeled function values, leaves show valuecorresponding full assignment (which product numbers arcs path).497fiM ATEESCU , ECHTER & ARINESCU00001111BBCC00110011B f(M,A,B)012150181204110061400001111B00110011C g(M,B,C)0315014112091150716Figure 19: Weighted graphical model010101B0B51820101CC50136 60140C3510170 6054 901400B4100C121B123CB610CC9157691571010101036 6070 6054 900B12518210101C6128 24501140B40C31B4C1228 2411064101CC12915761010136 60Figure 20: AND/OR search tree context minimal graphsee either value (0 1) gives rise function (because leaves twosubtrees values). However, two subtrees identified representingfunction usual reduction rules. right part figure shows context minimalgraph, compact representation subtree, share parts.would like case method recognizing left right subtreescorresponding = 0 = 1 represent function. normalizingvalues level, processing bottom up. Figure 21 left, values OR-to-ANDarcs normalized, variable, normalization constant promotedvalue. Figure 21 right, normalization constants promoted upwardsmultiplication. process change value full assignment, thereforeproduces equivalent graphs.see already nodes labeled C merged, producing graphFigure 22 left. Continuing process obtain AOMDD weighted graph,shown Figure 22 right.define AOMDD weighted graphical model follows:EFINITION 26 (AOMDD weighted graphical model) AOMDD weighted graphicalmodel AND/OR graph, meta-nodes, that: (1) meta-node, weights sum1; (2) root meta-node constant associated it; (3) completely reduced, namelyisomorphic meta-nodes, redundant meta-nodes.498fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS0101B0BB125182010183/805/814/261012/263/8641011014/2610126*5013/826*201107/1310B24*413*10024*61C5/81B8*18C12/260B8*1213 C5/8B1024 C10B4026 CC1013*401C6/13C3/815/807/13106/131Figure 21: Normalizing values bottom8441/201/210101B9600B130144101B521960B13014410C3/8017/130196/42201B52B96/2261130/22601C5/8226/422144/19652/19601C6/133/810C5/817/1306/131Figure 22: AOMDD weighted graphprocedure transforming weighted AND/OR graph AOMDD similarProcedure B OTTOM U P R EDUCTION Section 5. difference new layerprocessed, first meta-node weights normalized promoted parent,procedure continues usual reduction rules.HEOREM 7 Given two equivalent weighted graphical models accept common pseudo tree, normalizing arc values together exhaustive application reduction rules yieldsAND/OR graph, AOMDD based .Finite Precision Arithmetic implementation algorithm described section mayprove challenging machines used finite precision arithmetic. Since weightsreal-valued, repeated normalization may lead precision errors. One possible approach,also used experiments, define -tolerance, user defined sufficientlysmall , consider weights equal within other.9. Semantic Treewidthgraphical model represents universal function F = fi . function F may representeddifferent graphical models. Given particular pseudo tree , captures structuralinformation F , interested graphical models accept pseudo tree, namelyprimal graphs contain edges backarcs . Since size AOMDD Fbased bounded worst case induced width graphical model along ,define semantic treewidth be:499fiM ATEESCU , ECHTER & ARINESCU123BCCB1234CB4B1 31 42 43 14 14 2(a) two solutionsC1 21 42 12 33 23 44 14 31 21 32 12 32 43 13 23 44 24 3B C1 31 42 43 14 14 2B1 21 42 12 33 23 44 14 3C1 31 42 43 14 14 2(b) First modelB2 43 1BCB C1 44 1C1 34 2(c) Second modelFigure 23: 4-queen problemEFINITION 27 (semantic treewidth) semantic treewidth graphical model relativepseudo tree denoted swT (M), smallest treewidth taken models Requivalent M, accept pseudo tree . Formally, defined swT (M) =minR,u(R)=u(M) wT (R), u(M) universal function M, wT (R) inducedwidth R along . semantic treewidth graphical model, M, minimal semantictreewidth pseudo trees express universal function.Computing semantic treewidth shown NP-hard.3HEOREM 8 Computing semantic treewidth graphical model NP-hard.Theorem 8 shows computing semantic treewidth hard, likely actualcomplexity even higher. However, semantic treewidth explain sometimes minimalAND/OR graph OBDD much smaller exponential treewidth pathwidth upperbounds. many cases, could huge disparity treewidth semantictreewidth along .Example 14 Figure 23(a) shows two solutions 4-queen problem. problem expressed complete graph treewidth 3, given Figure 23(b). Figure 23(c) shows equivalentproblem (i.e., set solutions), treewidth 1. semantic treewidth4-queen problem 1.Based fact AOMDD canonical representation universal functiongraphical model, conclude size AOMDD bounded exponentiallysemantic treewidth along pseudo tree, rather treewidth given graphical modelrepresentation.Proposition 7 size AOMDD graphical model bounded O(n k swT (M )),n number variables, k maximum domain size swT (M) semantictreewidth along pseudo tree .3. thank David Eppstein proof.500fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSBBCCBCNP01(a) OBDD representation(b) Primal graph hiddenvariables M, N P .Figure 24: parity functionExample 15 Consider constraint network n variables every two variables constrained equality (X = ). One graph representation complete graph, another chainanother tree. problem specified complete graph, use linear order,OBDD linear size exists representation pathwidth 1(rather n).semantic treewidth yield much better upper bound AOMDD, alsobad bound. well known parity function n variables compact,chain-like OBDD representation. Yet, constraint network representation parity functionfunction (namely complete graph variables), whose treewidth semantictreewidth number variables, n. OBDD representation parity function suggestsaddition hidden variables simplify presentation. show example Figure24. left side, Figure 24(a) OBDD representation parity functionfour binary variables. graphical model would represent function complete graphfour variables. However, could add extra variables M, N P Figure 24(b), sometimescalled hidden variables, help decompose model. case form constrainttogether B represents parity B, namely = 1 B = 1,parity (XOR) operator. Similarly, N would capture parity C, Pwould capture parity N D, would also give parity initial four variables.two structures surprisingly similar. would interesting study connectionhidden variables compact AOBDDs, leave future work.10. Experimental Evaluationexperimental evaluation preliminary stages, results already encouraging. ran search-based compile algorithm, recording trace AND/OR search,reducing resulting AND/OR graph bottom up. results applied reduction isomorphism still kept redundant meta-nodes. implemented algorithmsC++ ran experiments 2.2GHz Intel Core 2 Duo 2GB RAM, running Windows.501fiM ATEESCU , ECHTER & ARINESCU10.1 Benchmarkstested performance search-based compilation algorithm random Bayesian networks, instances Bayesian Network Repository subset networks UAI06Inference Evaluation Dataset.Random Bayesian Networks random Bayesian networks generated using parameters(n, k, c, p), n number variables, k domain size, c number conditionalprobability tables (CPTs) p number parents CPT. structure networkcreated randomly picking c variables n and, each, randomly picking p parentspreceding variables, relative ordering. remaining n c variables called rootnodes. entries probability table generated randomly using uniform distribution,table normalized. also possible control amount determinismnetwork forcing percentage det CPTs 0 1 entries.Bayesian Network Repository Bayesian Network Repository4 contains collection beliefnetworks extracted various real-life domains often used benchmarking probabilistic inference algorithms.UAI06 Inference Evaluation Dataset UAI 2006 Inference Evaluation Dataset5 containscollection random well real-world belief networks used first UAI 2006Inference Evaluation contest. purpose selected subset networks derivedISCAS89 digital circuits benchmark.6 ISCAS89 circuits common benchmark usedformal verification diagnosis. circuits converted Bayesian networkremoving flip-flops buffers standard way, creating deterministic conditional probabilitytable gate, putting uniform distributions input signals.10.2 Algorithmsconsider two search-based compilation algorithms, denoted AOMDD-BCP AOMDDSAT, respectively, reduce context minimal AND/OR graph explored via isomorphism,exploiting determinism (if any) present network. approach take handlingdeterminism based unit resolution CNF encoding (i.e., propositional clauses) zeroprobability tuples CPTs. idea using unit resolution search Bayesian networks first explored Allen Darwiche (2003). AOMDD-BCP conservative appliesunit resolution node search graph, whereas AOMDD-SAT aggressivedetects inconsistency running full SAT solver. used zChaff SAT solver (Moskewicz,Madigan, Zhao, Zhang, & Malik, 2001) unit resolution well full satisfiability.comparison, also ran version AOMDD-BCP, called MDD-BCP.reference also report results obtained ACE7 compiler. ACE compiles Bayesiannetwork Arithmetic Circuit (AC) uses AC answer multiple queries respect network. arithmetic circuit representation equivalent AND/OR graphs(Mateescu & Dechter, 2007). time ACE compiler invoked, uses one two algorithmsbasis compilation. First, elimination order generated network4.5.6.7.http://www.cs.huji.ac.il/compbio/Repository/http://ssli.ee.washington.edu/bilmes/uai06InferenceEvaluationAvailable at: http://www.fm.vslib.cz/kes/asic/iscas/Available at: http://reasoning.cs.ucla.edu/ace502fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSNetwork(w*, h)(n, k)ACE#nodes timeMDD w/ BCPAOMDD w/ BCPAOMDD w/ SAT#meta #cm(OR) time#meta #cm(OR)time #meta #cm(OR)timeBayesian Network Repositoryalarm(4, 13) (37, 4)1,511 0.01 208,837 682,195 73.353204590.053204590.22cpcs54(14, 23) (54, 2)196,933 0.06- 65,158 66,4056.97 65,158 66,4056.97cpcs179(8, 14) (179, 4)67,919 0.059,990 32,185 46.56 9,990 32,185 46.56cpcs360b (20, 27) (360, 2) 5,258,826 1.72diabetes(4, 77) (413, 21) 7,615,989 1.81hailfinder (4, 16) (56, 11)8,815 0.012,0682,2020.34 1,8932,2021.48mildew(4, 13) (35, 100) 823,913 0.39- 73,666 110,284 1367.81 62,903 65,599 3776.82mm(20, 57) (1220, 2)47,171 1.49- 38,414 58,1444.54 30,274 52,523 99.55munin2(9, 32) (1003, 21) 2,128,147 1.91munin3(9, 32) (1041, 21) 1,226,635 1.27munin4(9, 32) (1044, 21) 2,423,009 4.44pathfinder (6, 11) (109, 63)18,250 0.05 610,854 1,303,682 352.186,984 16,267 30.71 2,265 15,963 50.36pigs(11, 26) (441, 3) 636,684 0.19- 261,920 294,101 174.29 198,284 294,101 1277.72water(10, 15) (32, 4)59,642 0.52 707,283 1,138,096 95.14 18,744 20,9262.02 18,503 19,2257.45UAI06 Evaluation DatasetBN 42(21, 62) (851, 2)4,860 1.35- 107,025 341,428 53.50 42,445 43,280 57.36BN 43(26, 65) (851, 2)10,373 1.62- 1,343,923 1,679,013 1807.63 313,388 314,669 434.38BN 44(25, 56) (851, 2)4,235 1.31- 155,588 187,589 20.90 47,222 48,540 66.09BN 45(22, 54) (851, 2)12,319 1.50- 390,795 487,593 68.81 126,182 126,929 177.50BN 46(20, 46) (851, 2)5,912 2.90 1,125,658 1,228,332 94.93 16,711 17,5321.31 7,3377,5135.54BN 47(39, 57) (632, 2)1,448 1.17 42,419 47,128 2.871,8732,6630.24 1,3032,6142.36BN 49(40, 60) (632, 2)1,408 1.16 18,344 19,251 1.321,2051,5390.199521,5151.34BN 51(41, 68) (632, 2)1,467 1.15 63,851 68,005 4.224,4425,2670.50 3,6535,1954.58BN 53(47, 87) (532, 2)1,357 0.91 14,210 19,162 1.494,8199,5610.74 1,3651,7191.36BN 55(49, 92) (532, 2)1,288 0.935,1686,088 0.571,9722,8160.267909040.75BN 57(49, 85) (532, 2)1,276 0.90 48,436 51,611 3.524,0365,0890.379621,2771.01BN 59(52, 87) (511, 2)1,749 0.93 332,030 353,720 25.61 22,963 29,1462.14 10,655 18,752 14.17BN 61(41, 64) (638, 2)1,411 1.10 20,459 20,806 1.451,2441,5890.17 1,0161,5281.37BN 63(53, 95) (511, 2)1,324 0.90 11,461 17,087 1.287,182 14,0481.07 1,4192,1771.69BN 65(56, 86) (411, 2)1,184 0.75- 20,764 23,1021.52 12,569 19,778 12.90BN 67(54, 88) (411, 2)1,031 0.74- 179,067 511,031 154.917161,1690.78Positive Random Bayesian Networks (n=75, k=2, p=2, c=65)r75-1(12, 22) (75, 2)67,737 0.31- 21,619 21,6192.59 21,619 21,6192.59r75-2(12, 23) (75, 2)46,703 0.29- 18,083 18,0831.88 18,083 18,0831.88r75-3(11, 26) (75, 2)53,245 0.30- 18,419 18,4191.86 18,419 18,4191.86r75-4(11, 19) (75, 2)28,507 0.298,3638,3631.16 8,3638,3631.16r75-5(13, 24) (75, 2)149,707 0.36- 42,459 42,4594.61 42,459 42,4594.61r75-6(14, 24) (75, 2)132,107 1.19- 62,621 62,6216.95 62,621 62,6216.95r75-7(12, 24) (75, 2)89,913 0.36- 21,583 21,5832.42 21,583 21,5832.42r75-8(14, 24) (75, 2)86,183 0.36- 49,001 49,0016.23 49,001 49,0016.23r75-9(11, 19) (75, 2)29,025 0.307,6817,6810.81 7,6817,6810.81r75-10(10, 24) (75, 2)20,291 0.285,9055,9050.63 5,9055,9050.63Deterministic Random Bayesian Networks (n=100, k=2, p=2, c=90) det = 25% CPTs containing 0 1 entriesr100d25-1 (13, 31) (100, 2)68,398 0.38- 34,035 34,0752.94 34,035 34,075 12.77r100d25-2 (16, 28) (100, 2) 150,134 0.46- 70,241 70,9317.72 70,241 70,931 27.17r100d25-3 (16, 29) (100, 2) 705,200 0.96- 134,079 135,203 13.80 134,079 135,203 50.51r100d25-4 (16, 31) (100, 2) 161,902 0.54- 79,366 79,4887.26 79,366 79,488 28.06r100d25-5 (16, 29) (100, 2) 185,348 0.53- 140,627 140,636 14.57 140,627 140,636 49.42r100d25-6 (18, 28) (100, 2) 148,835 0.66- 204,232 210,066 17.56 197,134 210,066 92.24r100d25-7 (16, 29) (100, 2) 264,629 0.60- 134,344 135,008 14.26 133,850 135,008 55.60r100d25-8 (17, 27) (100, 2)65,186 0.46- 36,857 36,8872.95 36,857 36,887 11.97r100d25-9 (14, 27) (100, 2) 140,014 0.40- 58,421 59,7916.88 58,172 59,791 23.21r100d25-10 (16, 27) (100, 2) 173,808 0.58- 69,110 69,1367.50 69,110 69,136 26.50Table 1: Results experiments 50 Bayesian networks 3 problem classes; w =treewidth, h = depth pseudo tree, n = number variables, k = domain size, timegiven seconds; bold types highlight best results across rows.503fiM ATEESCU , ECHTER & ARINESCUsufficiently small induced width, tabular variable elimination used basis.algorithm similar one discussed Chavira Darwiche (2007), uses tables represent factors rather ADDs. induced width large, logical model countingused basis. Tabular variable elimination typically efficient width small cannothandle networks width larger. Logical model counting, hand, incursoverhead tabular variable elimination, handle many networks larger treewidth.tabular variable elimination logical model counting produce ACs exploit local structure, leading efficient online inference. logical model counting invoked, proceedsencoding Bayesian network CNF (Chavira & Darwiche, 2005; Chavira, Darwiche, &Jaeger, 2006), simplifying CNF, compiling CNF d-DNNF, extracting ACcompiled d-DNNF. dtree CNF clauses drives compilation step.experiments report compilation time seconds (time), numbernodes context minimal graph explored (#cm), number meta-nodes resultingAOMDD (#meta), well size AC compiled ACE (#nodes). networkspecify number variables (n), domain size (k), induced width (w ) pseudo tree depth (h).- stands exceeding 2GB memory limit respective algorithm. best performancepoints highlighted.10.3 Evaluation Bayesian NetworksTable 1 reports results obtained experiments 50 Bayesian networks. AOMDDcompilers well ACE used min-fill heuristic (Kjaerulff, 1990) construct guidingpseudo tree dtree, respectively.10.3.1 BAYESIAN N ETWORKS R EPOSITORYsee ACE overall fastest compiler domain, outperforming AOMDD-BCPAOMDD-SAT several orders magnitude (e.g., mildew, pigs). However,diagrams compiled ACE AOMDD-BCP (resp. AOMDD-SAT) comparable size.cases, AOMDD-BCP AOMDD-SAT able compile much smaller diagramsACE. example, diagram produced AOMDD-BCP mildew network 13 timessmaller one compiled ACE. principle output produced ACE AOMDDsimilar guided pseudo tree/dtree. scheme viewedcompilation alternative (1) extends decision diagrams (2) mimics traces searchproperties may make representation accessible. compiler MDD-BCP ablecompile 3 14 test instances, sizes far larger producedAOMDD-BCP. instance, pathfinder network, AOMDD-BCP outputs decisiondiagram almost 2 orders magnitude smaller MDD-BCP.10.3.2 UAI06 DATASETUAI06 Dataset instances picked randomly 30 variables instantiatedevidence. see ACE best performing compiler dataset. AOMDD-BCPcompetitive ACE terms compile time 9 16 test instances. AOMDD-SATable compile smallest diagrams 6 networks (e.g., BN 47, BN 49, BN 55, BN 57,BN 61, BN 67). before, difference size compiled data-structures producesMDD-BCP AOMDD-BCP 2 orders magnitude favor latter.504fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELS10.3.3 R ANDOM N ETWORKSproblem instances denoted r75-1 r75-10 generated class randombelief networks parameters (n = 75, k = 2, p = 2, c = 65). Similarly, instances denotedr100d25-1 r100d25-10 belong class parameters (n = 100, k = 2, p = 2, c =90). latter case, det = 25% CPTs deterministic, namely contain 01 probability tuples. test instances compiled without evidence. seedomain AOMDD-BCP/AOMDD-SAT able compile smallest diagrams,average 2 times smaller produced ACE. However, ACE fastestcompiler. Notice compiler MDD-BCP ran memory test cases.10.4 Impact Variable Orderingtheory dictates, AOMDD size influenced quality guiding pseudo tree.addition min-fill heuristic also considered hypergraph heuristic constructspseudo tree recursively decomposing dual hypergraph associated graphical model.idea also explored Darwiche (2001) constructing dtrees guide ACE.Since min-fill hypergraph partitioning heuristics randomized (namely tiesbroken randomly), size AOMDD guided resulting pseudo tree may vary significantly one run next. Figure 25 displays AOMDD size using hypergraph min-fillbased pseudo trees 6 networks selected Table 1, 20 independent runs. also recordaverage induced width depth obtained pseudo trees (see header plotFigure 25). see two heuristics dominate other, namely variance outputsize quite significant cases.10.5 Memory UsageTable 2 shows memory usage (in MBytes) ACE, AOMDD-BCP AOMDD-SAT, respectively, Bayesian networks Table 1. see cases AOMDD based compilers require far less memory ACE. example, mildew network, AOMDDBCP AOMDD-SAT use 22 MB memory compile AND/OR decision diagram,ACE requires much 218 MB memory. Moreover, compiled AOMDDcase one order magnitude fewer nodes constructed ACE. comparingtwo AND/OR search-based compilers, observe networks significant amountdeterminism, UAI06 Evaluation dataset, AOMDD-SAT uses average twotimes less memory AOMDD-BCP. dramatic savings memory usage due aggressive constraint propagation employed AOMDD-SAT compared AOMDD-BCPseen BN 67 network. case, difference memory usage AOMDD-SATAOMDD-BCP 2 orders magnitude favor former.11. Related Workrelated work viewed along two directions: (1) work related AND/OR searchidea graphical models (2) work related compilation graphical models exploitsproblem structure.extensive discussion (1) provided previous work Dechter Mateescu(2007). Since focus paper, mention AND/OR idea origi505fiM ATEESCU , ECHTER & ARINESCUFigure 25: Effect variable ordering.506fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSNetworkACEAOMDD w/ BCPAOMDD w/ SAT#nodes memory (MB) #nodes memory (MB) #nodes memory (MB)Bayesian Network Repositoryalarm1,5110.003200.02063200.0206cpcs54196,9334.00 65,1583.4415 65,1583.4415cpcs17967,9195.009,9901.9263 9,9901.9263cpcs360b 5,258,826204.00diabetes7,615,989449.00hailfinder8,8150.002,0680.1576 1,8930.1740mildew823,913218.00 73,66622.5781 62,90322.1467mm47,171369.00 38,4141.5719 30,2741.3711munin22,128,147202.00munin31,226,635150.00munin42,423,009n/apathfinder18,25010.006,9840.6009 2,2650.3515pigs636,68431.00 261,92023.3761 198,28417.7096water59,642161.00 18,7441.09578 18,5031.3258UAI06 Evaluation DatasetBN 424,860n/a 107,0254.5622 42,4451.9323BN 4310,373n/a 1,343,92357.8422 313,38814.2828BN 444,235n/a 155,5886.5613 47,2222.1628BN 4512,319n/a 390,79517.9325 126,1825.7958BN 465,912n/a16,7110.6929 7,3370.3401BN 471,448n/a1,8730.0720 1,3030.0583BN 491,408n/a1,2050.04499520.0409BN 511,467n/a4,4420.1689 3,6530.1633BN 531,357n/a4,8190.1814 1,3650.0587BN 551,288n/a1,9720.07237900.0336BN 571,276n/a4,0360.14959620.0411BN 591,749n/a22,9630.8501 10,6550.4587BN 611,411n/a1,2440.0463 1,0160.0445BN 631,324n/a7,1820.2728 1,4190.0607BN 651,184n/a20,7640.7539 12,5690.5384BN 671,031n/a 179,0676.96037160.0304Positive Random Bayesian Networks parameters (n=75, k=2, p=2, c=65)r75-167,7371.00 21,6191.2503 21,6191.2503r75-246,7031.00 18,0830.9957 18,0830.9957r75-353,2451.00 18,4190.9955 18,4190.9955r75-428,5071.008,3630.5171 8,3630.5171r75-5149,7073.00 42,4592.3299 42,4592.3299r75-6132,1073.00 62,6213.4330 62,6213.4330r75-789,9132.00 21,5831.1942 21,5831.1942r75-886,1832.00 49,0012.8130 49,0012.8130r75-929,0251.007,6810.4124 7,6810.4124r75-1020,2911.005,9050.3261 5,9050.3261Deterministic Random Bayesian Networks parameters (n=100, k=2, p=2, c=90)r100d25-168,3985.00 34,0351.6290 34,0351.7149r100d25-2 150,13410.00 70,2413.6129 70,2413.7810r100d25-3 705,20040.00 134,0796.6372 134,0796.9873r100d25-4 161,90222.00 79,3663.8113 79,3664.0079r100d25-5 185,34815.00 140,6277.0839 140,6277.4660r100d25-6 148,83537.00 204,2329.1757 197,1349.6542r100d25-7 264,62919.00 134,3446.9619 133,8506.9961r100d25-865,18621.00 36,8571.6872 36,8571.8278r100d25-9 140,0146.00 58,4213.1058 58,1723.2055r100d25-10 173,80827.00 69,1103.5578 69,1103.6636Table 2: Memory usage MBytes ACE, AOMDD-BCP AOMDD-SAT 50 Bayesiannetworks Table 1. Bold types highlight best performance across rows. n/aindicates respective memory usage statistic available ACEs output.507fiM ATEESCU , ECHTER & ARINESCUnally developed heuristic search (Nilsson, 1980). mentioned introduction, AND/ORsearch graphical models based pseudo tree spans graph model, similartree rearrangement Freuder Quinn (1985, 1987). idea adapted distributedconstraint satisfaction Collin et al. (1991, 1999) recently Modi et al. (2005),also shown related graph-based backjumping (Dechter, 1992). work extendedBayardo Miranker (1996), Bayardo Schrag (1997) recently applied optimization tasks Larrosa et al. (2002). Another version viewed exploring AND/ORgraphs presented recently constraint satisfaction (Terrioux & Jegou, 2003b) optimization (Terrioux & Jegou, 2003a). Similar principles introduced recently probabilisticinference, algorithm Recursive Conditioning (Darwiche, 2001) well Value Elimination(Bacchus et al., 2003b, 2003a), currently core advanced SAT solvers (Sanget al., 2004).direction (2), various lines related research. formal verification literature,beginning work Bryant (1986) contains large number papers dedicatedstudy BDDs. However, BDDs fact structures (the underlying pseudo tree chain)take advantage problem decomposition explicit way. complexity boundsOBDDs based pathwidth rather treewidth.noted earlier, work Bertacco Damiani (1997) Disjoint Support Decomposition(DSD) related AND/OR BDDs various ways. main common aspect approaches show structure decomposition exploited BDD-like representation. DSDfocused Boolean functions exploit refined structural information inherent Boolean functions. contrast, AND/OR BDDs assume structure conveyedconstraint graph, therefore broadly applicable constraint expression alsographical models general. allow simpler higher level exposition yields graphbased bounds overall size generated AOMDD. full relationship twoformalisms studied further.McMillan (1994) introduced BDD trees, along operations combining them.2wcircuits bounded tree width, BDD trees linear space upper bound O(|g|2w2 ),|g| size circuit g (typically linear number variables) w treewidth.bound hides large constants claim linear dependence |g| w bounded.However, McMillan maintains input function CNF expression BDD-treesbounds AND/OR BDDs, namely exponential treewidth only.sketch short comparison McMillans BDD trees AOMMDs, considerexample simple pseudo tree root , left child right child .nodes may stand set variables. BDD trees, assignments groupedequivalence classes according cofactors generated remaining .example assignments 1 2 equivalent generate function .node represented BDD whose leaves cofactors. done .node represented matrix BDDs, column corresponds cofactorline cofactor . contrast, AOMDD represents node BDD whoseleaves cofactors (the number distinct functions ) cofactorroot decomposition (an node) . Moreover, representations (asdescendants different cofactor ) shared much possible goes .high level description, becomes slightly complicated redundant nodeseliminated, idea remains same.508fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSAND/OR structure restricted propositional theories similar deterministic decomposable negation normal form (d-DNNF) (Darwiche & Marquis, 2002; Darwiche, 2002).recently, Huang Darwiche (2005b) used trace DPLL algorithm generate OBDD,compared typical formal verification approach combining OBDDs inputfunction according schedule. structures investigated case still OR.idea extended present work AND/OR search compilation algorithm.McAllester, Collins, Pereira (2004) introduced case factor diagrams (CFD), subsume Markov random fields bounded tree width probabilistic context free grammars (PCFG).CFDs much related AND/OR graphs. CFDs target minimal representation,exploiting decomposition (similar nodes) also exploiting context sensitive information allowing dynamic ordering variables based context. CFDs eliminateredundant nodes, part cause use zero suppression. claimCFDs canonical forms, also description combine two CFDs.numerous variants decision diagrams designed represent integer-valuedreal-valued functions. comprehensive view refer reader survey DrechslerSieling (2001). Algebraic decision diagrams (ADDs) (Bahar et al., 1993) provide compilation general real-valued rather Boolean functions. main drawbacksize increases fast number terminals becomes large. several approachestry alleviate problem. However structure capture still OR,exploit decomposition. alternatives introduce edge values (or weights) enablesubgraph sharing. Edge-valued binary decision diagrams (EVBDDs) (Lai & Sastry, 1992)use additive weights, multiplicative weights also allowed called factoredEVBDDs (FEVBDDs) (Tafertshofer & Pedram, 1997). Another type BDDs called K*BMDs(Drechsler, Becker, & Ruppertz, 1996) also use integer weights, additive multiplicativeparallel. ADDs also extended affine ADDs (Sanner & McAllester, 2005),affine transformations achieve compression. result shown beneficialprobabilistic inference algorithms, tree clustering, still exploitstructure.recently, independently parallel work AND/OR graphs (Dechter & Mateescu, 2004a, 2004b), Fargier Vilarem (2004) Fargier Marquis (2006, 2007) proposed compilation CSPs tree-driven automata, many similarities work.main focus transition linear automata tree automata (similarAND/OR), possible savings tree-structured networks hyper-trees constraintsdue decomposition. compilation approach guided tree-decompositionguided variable-elimination based algorithms. well known Bucket Eliminationcluster-tree decomposition principle (Dechter & Pearl, 1989).Wilson (2005) extended OBDDs semi-ring BDDs. semi-ring treatment restrictedsearch spaces, allows dynamic variable ordering. otherwise similar aimscope AOMDD. restricting AOMDD graphs only, two closely related,except express BDDs using Shenoy-Shafer axiomatization centered twooperation combination marginalization rather semi-ring formulation. Minimalityformulation Wilson (2005) general allowing merging nodes different valuestherefore capture symmetries (called interchangeability).Another framework similar AOMDDs, became aware recently, Probabilistic Decision Graphs (PDG) Jaeger (2004). work preceded relevant work509fiM ATEESCU , ECHTER & ARINESCUdiscussed (Fargier & Vilarem, 2004; Wilson, 2005) went somewhat unnoticed, perhaps due notational cultural differences. however similar motivation, frameworkproposed algorithms. believe AND/OR framework accessible. define framework multi-valued domains, provide greater details algorithms complexity analysis,make explicit connection search frameworks, fully address issues canonicity wellprovide empirical demonstration. particular, claim canonicity PDGs similarone make AOMDDs weighted models, relative trees (or forests)represent given probability distribution.another line research Drechsler group (e.g. Zuzek, Drechsler, & Thornton,2000), use AND/OR graphs Boolean function representation, may seem similarapproach. However, semantics purpose AND/OR graphs different.constructed based technique recursive learning used perform Boolean reasoning,i.e. explore logic consequences given assumption based structure circuit,especially derive sets implicants. meaning case relatedmeaning gates/functions, case meaning related semanticfunctions. AND/OR enumeration tree results circuit according Zuzek et al.(2000) related AND/OR decomposition discuss.12. Conclusionpropose AND/OR multi-valued decision diagram (AOMDD), emerges studyAND/OR search spaces graphical models (Dechter & Mateescu, 2004a, 2004b; Mateescu &Dechter, 2005; Dechter & Mateescu, 2007) ordered binary decision diagrams (OBDDs) (Bryant,1986). data-structure used compile graphical model.Graphical models algorithms search-based compiled data-structures BDDsdiffer primarily choices time vs. memory. move regular searchspace AND/OR search space spectrum algorithms available improved timevs. memory decisions. believe AND/OR search space clarifies available choiceshelps guide user making informed selection algorithm would fit bestparticular query asked, specific input function available computational resources.contribution work is: (1) formally describe AOMDD provecanonical representation constraint network. (2) extend AOMDD general weightedgraphical models. (3) give compilation algorithm based AND/OR search, savestrace memory intensive search (the context minimal AND/OR graph), reducesone bottom pass. (4) describe APPLY operator combines two AOMDDsoperation show complexity quadratic input, never worse exponentialtreewidth. (5) give scheduling order building AOMDD graphical modelstarting AOMDDs functions based Variable Elimination algorithm.guarantees complexity exponential induced width (treewidth) alongordering. (6) show AOMDDs relate various earlier recent compilation frameworks,providing unifying perspective methods. (7) introduce semantic treewidth,helps explain compiled decision diagrams often much smaller worst casebound. Finally, (8) provide preliminary empirical demonstration power currentscheme.510fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSAcknowledgmentswork done Robert Mateescu Radu Marinescu University California, Irvine. authors would like thank anonymous reviewers constructivesuggestions improve paper, David Eppstein useful discussion complexity issues,Lars Otten Natasha Flerova comments final version manuscript. worksupported NSF grants IIS-0412854 IIS-0713118, initial part Radcliffe fellowship 2005-2006 (through partner program), Harvard undergraduate student John Cobb.AppendixProof Proposition 1Consider level variable Xi , meta-nodes list LXi . one passmeta-nodes LXi (the inner loop), two meta-nodes level XiAND/OR graph isomorphic, would merged line 6. Also,pass meta-nodes LXi redundant meta-nodes LXi eliminatedline 8. Processing meta-nodes level Xi create new redundant isomorphicmeta-nodes levels processed before. follows resulting AND/ORgraph completely reduced. 2Proof Theorem 4bound size follows directly Theorem 3. AOMDD size smallersize context minimal AND/OR graph, bounded O(n k wT (G) ). provetime bound, rely use hash table, assumption efficient implementation allows access time constant. time bound AND/OR-S EARCH -AOMDDO(n k wT (G) ), Theorem 3, takes time linear output (we assumeconstraint propagation performed search). Procedure B OTTOM U P R EDUCTION (procedure 1) takes time linear size context minimal AND/OR graph. Therefore, AOMDDcomputed time O(n k wT (G) ), result algorithm performsreduction search. 2Proof Proposition 2complexity OBDD (and MDD) apply known quadratic input. Namely,number nodes output product number nodes input. Therefore,number nodes appear along one path output AOMDD productnumber nodes input, along path, |Gfi | |Ggi |. Summing pathsgives result. 2Proof Proposition 3argument identical case MDDs. recursive calls APPLY lead combinationsone node Gfaomdd one node Ggaomdd (rather list nodes). numbertotal possible combinations O(| Gfaomdd | | Ggaomdd |). 2Proof Proposition 4recursive calls APPLY generate one meta-node output combination511fiM ATEESCU , ECHTER & ARINESCUnodes Gff g Ggf g . Lets look combinations nodes Gff g Ggaomdd \ Ggf g .meta-nodes Ggaomdd \ Ggf g participate combinations (lets call set A)levels (of variables) right Tf g . mechanicsrecursive calls APPLY. Whenever node f belongs Gff g combined nodeg belongs A, line 15 APPLY expands node f , node (or nodes)remain same. happen nodes f combinednode (or nodes) A, point APPLY simply copy remaining portionoutput Ggaomdd . size therefore proportional | Ggf g | (because layermetanodes immediately Ggf g ). similar argument valid symmetrical case.combinations nodes Ggaomdd \ Ggf g Ggaomdd \ Ggf g . bound followsarguments. 2Proof Proposition 5APPLY operation works constructing output AOMDD root leaves. first createsmeta-node root variable, recursively creates children metanodes using APPLYcorresponding children input. worst case happen outputreduced all, recursive call made possible descendant. correspondsunfolding full AND/OR search tree based context variables, exponentialcontext size. APPLY finishes context variables, arrives first branchingbucket pseudo tree, remaining branches independent. Similar case OBDDs,one function occupies single place memory, APPLY simply create linkcorresponding branches inputs (this happens line 4 APPLY algorithm).Therefore, time space complexity exponential context size. 2Proof Theorem 5space complexity governed BE. Since AOMDD never requires spacefull exponential table (or tree), follows BE-AOMDD needs space O(n k w ).size output AOMDD also bounded, per layers, number assignmentscontext layer (namely, size context minimal AND/OR graph). Therefore,context size bounded treewidth, follows output size O(n k w ).time complexity follows Proposition 5, fact number functionsbucket cannot exceed r, original number functions.2Proof Proposition 6suffices prove proposition = 2. general result obtained induction.essential function defined constraint network (i.e., values 0 1),function takes value 1 least one assignment. value 1 denotes consistent assignments (solutions), 0 denotes inconsistent assignments. Suppose f = f1 f2 . Lets denotex full assignment X, x1 x2 projection x X1 X2 , respectively.write x = x1 x2 (concatenation partial assignments). follows f (x) = f1 (x1 ) f2 (x2 ).Therefore, f (x) = 1, must f1 (x1 ) = 1 f2 (x2 ) = 1. claim x1 ,f1 (x1 ) = 1 exists x2 f (x1 x2 ) = 1. Suppose contradictionexist x1 f1 (x1 ) = 1 f (x1 x2 ) = 0 x2 . Since f always zero,512fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSfollows f2 always zero, therefore must x2 f2 (x2 ) = 1.leads contradiction, therefore functions f1 f2 uniquely defined f . 2Proof Theorem 6proof structural induction depth pseudo tree . follows canonicityproofs OBDDs (Bryant, 1986) MDDs (Srinivasan et al., 1990), extends linearorderings tree orderings capture function decomposition according pseudo tree .depth , along paths root leaf, actually size dependency set,set variables value function depends. Remember AOMDDAND/OR graph completely reduced. use word function, denoted f , referuniversal relation, characteristic function, defined constraint network.Assume depth 0. means function depend variable,must one constants 0 1. Suppose function constant 0. Then, mustAOMDD contain terminal meta-node 1, since nodes must reachablealong path, would mean function also evaluate 1. Suppose AOMDDcontains nonterminal meta-node, say labeled X, X take k different values. mustk children meta-nodes X terminal meta-node 0. oneterminal 0, AOMDD completely reduced. one 0, followsmeta-node labeled X redundant. Therefore, above, follows AOMDDrepresenting constant 0 made terminal 0. unique, contains smallestnumber nodes. similar argument applies constant 1.Now, suppose statement theorem holds constraint network admitspseudo tree depth strictly smaller p, constraint network pseudotree depth equal p, p > 0. Let X root , domain {x1 , . . . , xk }.denote fi , {1, . . . , k}, functions defined restricted constraint networkX = xi , namely fi = f |X=xi . Let Y1 , . . . , Ym children X . Suppose twoAOMDDs f , denoted G G 0 . show two AND/OR graphs isomorphic.functions fi decomposed according pseudo tree root X removed.fact forest independent pseudo trees (they share variables), rootedY1 , . . . , Ym . Based Proposition 6, unique decomposition fi = fiY1 . . . fiYm ,{1, . . . , k}. Based induction hypothesis, function fi j unique AOMDD.AND/OR graphs G G 0 , look subgraphs descending X = xi ,completely reduced define function, fi , therefore exists isomorphic mappingthem. Let v root metanode G v 0 root G 0 . claim G G 0isomorphic according following mapping:0v,u = v;(u) =(u), u subgraph rooted hX, xi i.prove this, show well defined, isomorphic mapping.meta-node u G contained subgraphs rooted hX, xi hX, xj i,AND/OR graphs rooted (u) j (u) isomorphic one rooted u, thereforeother. Since G 0 completely reduced, contain isomorphic subgraphs, therefore(u) = j (u). Therefore well defined.show bijection. show one-to-one, assume two distinct metanodes u1 u2 G, (u1 ) = (u2 ). Then, subgraphs rooted u1 u2 isomorphic513fiM ATEESCU , ECHTER & ARINESCUsubgraph rooted (u1 ), therefore other. Since G completely reduced, mustu1 = u2 . fact onto isomorphic mapping follows definitionfact onto new node root meta-node. Since AOMDDscontain one root meta-node (more one root would lead conclusion rootmeta-nodes isomorphic merged), conclude G G 0 isomorphic.Finally, show among AND/OR graphs representing f , AOMDDminimal number meta-nodes. Suppose G AND/OR graph represents f , minimalnumber meta-nodes, without AOMDD. Namely, completely reduced.reduction rule would transform G AND/OR graph smaller number meta-nodes,leading contradiction. Therefore, G must unique AOMDD represents f . 2Proof Corollary 1proof Theorem 6 rely scopes define constraint network. longnetwork admits decomposition induced pseudo tree , universal function definedconstraint network always AOMDD, therefore constraint networkequivalent admits also AOMDD. 2Proof Theorem 7constant associated root actually sum weights solutions.derived definition weighted AOMDD. weights meta-nodenormalized (they sum 1), therefore values computed node AND/OR searchalways 1 (when task computingP sum solution weights). Therefore, constantweighted AOMDD always x w(x) regardless graphical model. proveweighted AOMDDs canonical functions normalized.Assume two different weighted AOMDDs, denoted G 1 G 2 , normalized function f . Let root variable A, domain {a1 , . . . , ak }. Let x denote fullassignment variables. Similar argument root constant,Pmeta-nodes normalized weights, follows w1 (A, a1 ) = w2 (A, a1 ) = x|A=a1 f (x).superscript w1 w2 indicates AOMDD, summation possible assignments restricted = a1 . follows root meta-nodes identical. valueroot variable, restricted functions represented G 1 G 2 identical, recursivelyapply argument above.However, proof complete, discuss case restricted functiondecomposed independent functions, according pseudo tree. Suppose twoindependent components, rooted B C. one 0 function, followsentire function 0. prove meta-nodes B G 1 G 2 identical. Bone value b1 extendable solution, weight must 1 meta-nodes, meta-nodesidentical. B one value, suppose without loss generality weightsdifferent first value b1 ,w1 (B, b1 ) > w2 (B, b1 ).(1)Since f 6= 0, must value C = c1 B = b1 , C = c1 extended fullsolution. sum weights possible extensionsXf (x) = w1 (B, b1 ) w1 (C, c1 ) = w2 (B, b1 ) w2 (C, c1 ).(2)x|B=b1 ,C=c1514fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSEquations 1 2 fact weight non-zero, followsw1 (C, c1 ) < w2 (C, c1 ).(3)Equation 1, fact B one value fact weights Bnormalized, follows value b2w1 (B, b2 ) < w2 (B, b2 ).(4)Equations 3 4, followsw1 (B, b2 ) w1 (C, c1 ) < w2 (B, b2 ) w2 (C, c1 ).(5)However, sidesP Equation 5 represent sum weights solutions B =b2 , C = c1 , namely x|B=b2 ,C=c1 f (x), leading contradiction. Therefore, mustEquation 1 false. Continuing argument values B, follows metanodes B identical, similarly meta-nodes C identical.decomposition two components, argument applies, Bfirst component C meta-variable combines components.2Proof Theorem 8Consider well known NP-complete problem 3- COLORING: Given graph G,3-coloring G? Namely, color vertices using three colors, twoadjacent vertices different colors? reduce 3- COLORING problem computingsemantic treewidth graphical model. Let H graph 3-colorable, nontrivial semantic treewidth. easy build examples H. G 3-colorable, G Halso 3-colorable non-trivial semantic treewidth, adding G simplifytask describing colorings H. However, G 3-colorable, G H also3-colorable, semantic treewidth zero. 2Proof Proposition 7Since AOMDDs canonical representations graphical models, follows graphicalmodel actual semantic treewidth realized AOMDD M,therefore AOMDD bounded exponentially semantic treewidth. 2ReferencesAkers, S. (1978). Binary decision diagrams. IEEE Transactions Computers, C-27(6), 509516.Allen, D., & Darwiche, A. (2003). New advances inference recursive conditioning. Proceedings Nineteenth Conference Uncertainty Artificial Intelligence (UAI03), pp.210.Bacchus, F., Dalmao, S., & Pitassi, T. (2003a). Algorithms complexity results #SATBayesian inference. Proceedings 44th Annual IEEE Symposium FoundationsComputer Science (FOCS03), pp. 340351.515fiM ATEESCU , ECHTER & ARINESCUBacchus, F., Dalmao, S., & Pitassi, T. (2003b). Value elimination: Bayesian inference via backtracking search. Proceedings Nineteenth Conference Uncertainty ArtificialIntelligence (UAI03), pp. 2028.Bahar, R., Frohm, E., Gaona, C., Hachtel, G., Macii, E., Pardo, A., & Somenzi, F. (1993). Algebraic decision diagrams applications. IEEE/ACM International ConferenceComputer-Aided Design (ICCAD93), pp. 188191.Bayardo, R., & Miranker, D. (1996). complexity analysis space-bound learning algorithmsconstraint satisfaction problem. Proceedings Thirteenth National ConferenceArtificial Intelligence (AAAI96), pp. 298304.Bayardo, R. J., & Schrag, R. C. (1997). Using CSP look-back techniques solve real world SATinstances. Proceedings Fourteenth National Conference Artificial Intelligence(AAAI97), pp. 203208.Bertacco, V., & Damiani, M. (1997). disjunctive decomposition logic functions.IEEE/ACM International Conference Computer-Aided Design (ICCAD97), pp. 7882.Bodlaender, H. L., & Gilbert, J. R. (1991). Approximating treewidth, pathwidth minimumelimination tree height. Tech. rep., Utrecht University.Bryant, R. E. (1986). Graph-based algorithms Boolean function manipulation. IEEE Transactions Computers, 35, 677691.Cadoli, M., & Donini, F. M. (1997). survey knowledge compilation. AI Communications,10(3-4), 137150.Chavira, M., & Darwiche, A. (2005). Compiling Bayesian networks local structure.Proceedings Nineteenth International Joint Conference Artificial Intelligence (IJCAI05), pp. 13061312.Chavira, M., & Darwiche, A. (2007). Compiling Bayesian networks using variable elimination.Proceedings Twentieth International Joint Conference Artificial Intelligence (IJCAI07), pp. 24432449.Chavira, M., Darwiche, A., & Jaeger, M. (2006). Compiling relational Bayesian networks exactinference. International Journal Approximate Reasoning, 42(1-2), 420.Clarke, E., Grumberg, O., & Peled, D. (1999). Model Checking. MIT Press.Collin, Z., Dechter, R., & Katz, S. (1991). feasibility distributed constraint satisfaction.Proceedings Twelfth International Conference Artificial Intelligence (IJCAI91),pp. 318324.Collin, Z., Dechter, R., & Katz, S. (1999). Self-stabilizing distributed constraint satisfaction.Chicago Journal Theoretical Computer Science, 115, special issue self-stabilization.Darwiche, A. (2001). Recursive conditioning. Artificial Intelligence, 125(1-2), 541.516fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSDarwiche, A. (2002). logical approach factoring belief networks. ProceedingsEighth International Conference Principles Knowledge Representation Reasoning(KR02), pp. 409420.Darwiche, A., & Marquis, P. (2002). knowledge compilation map. Journal Artificial Intelligence Research (JAIR), 17, 229264.Dechter, R. (1992). Constraint networks. Encyclopedia Artificial Intelligence, 276285.Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial Intelligence,113, 4185.Dechter, R., & Mateescu, R. (2007). AND/OR search spaces graphical models. Artificial Intelligence, 171(2-3), 73106.Dechter, R., & Mateescu, R. (2004a). Mixtures deterministic-probabilistic networksAND/OR search space. Proceedings Twentieth Conference Uncertainty Artificial Intelligence (UAI04), pp. 120129.Dechter, R., & Mateescu, R. (2004b). impact AND/OR search spaces constraint satisfaction counting. Proceedings Tenth International Conference PrinciplesPractice Constraint Programming (CP04), pp. 731736.Dechter, R., & Pearl, J. (1989). Tree clustering constraint networks. Artificial Intelligence, 38,353366.Drechsler, R., Becker, B., & Ruppertz, S. (1996). K*BMDs: new data structure verification.Proceedings 1996 European conference Design Test (ED&TC96), pp. 28.Drechsler, R., & Sieling, D. (2001). Binary decision diagrams theory practice. InternationalJournal Software Tools Technology Transfer (STTT), 3(2), 112136.Fargier, H., & Marquis, P. (2006). use partially ordered decision graphs knowledgecompilation quantified boolean formulae. Proceedings Twenty-First NationalConference Artificial Intelligence (AAAI06), pp. 4247.Fargier, H., & Marquis, P. (2007). valued negation normal form formulas. ProceedingsTwentieth International Joint Conference Artificial Intelligence (IJCAI07), pp. 360365.Fargier, H., & Vilarem, M. (2004). Compiling CSPs tree-driven automata interactive solving. Constraints, 9(4), 263287.Fishburn, P. C. (1970). Utility Theory Decision Making. Wiley, NewYork.Freuder, E. C., & Quinn, M. J. (1985). Taking advantage stable sets variables constraintsatisfaction problems. Proceedings Ninth International Joint Conference ArtificialIntelligence (IJCAI85), pp. 10761078.Freuder, E. C., & Quinn, M. J. (1987). use lineal spanning trees represent constraintsatisfaction problems. Tech. rep. 87-41, University New Hampshire, Durham.517fiM ATEESCU , ECHTER & ARINESCUHuang, J., & Darwiche, A. (2005a). compiling system models faster scalable diagnosis. Proceedings 20th National Conference Artificial Intelligence (AAAI05),pp. 300306.Huang, J., & Darwiche, A. (2005b). DPLL trace: SAT knowledge compilation.Proceedings Nineteenth International Joint Conference Artificial Intelligence(IJCAI05), pp. 156162.Jaeger, M. (2004). Probabilistic decision graphs - combining verification AI techniquesprobabilistic inference. International Journal Uncertainty, Fuzziness KnowledgeBased Systems, 12, 1942.Kask, K., Dechter, R., Larrosa, J., & Dechter, A. (2005). Unifying cluster-tree decompositionsreasoning graphical models. Artificial Intelligence, 166 (1-2), 165193.Kjaerulff, U. (1990). Triangulation graph-based algorithms giving small total state space. Tech.rep., University Aalborg, Denmark.Korf, R., & Felner, A. (2002). Disjoint pattern database heuristics. Artificial Intelligence, 134(1-2),922.Lai, Y.-T., & Sastry, S. (1992). Edge-valued binary decision multi-level hierarchical verification.Proceedings Twenty Nineth Design Automation Conference, pp. 608613.Larrosa, J., Meseguer, P., & Sanchez, M. (2002). Pseudo-tree search soft constraints.Proceedings European Conference Artificial Intelligence (ECAI02), pp. 131135.Lee, C. (1959). Representation switching circuits binary-decision programs. Bell SystemTechnical Journal, 38, 985999.Mateescu, R., & Dechter, R. (2005). relationship AND/OR search variable elimination. Proceedings Twenty First Conference Uncertainty Artificial Intelligence(UAI05), pp. 380387.Mateescu, R., & Dechter, R. (2007). AND/OR multi-valued decision diagrams (AOMDDs)weighted graphical models. Proceedings Twenty Third Conference UncertaintyArtificial Intelligence (UAI07), pp. 276284.McAllester, D., Collins, M., & Pereira, F. (2004). Case-factor diagrams structured probabilisticmodeling. Proceedings Twentieth Conference Uncertainty Artificial Intelligence (UAI04), pp. 382391.McMillan, K. L. (1993). Symbolic Model Checking. Kluwer Academic.McMillan, K. L. (1994). Hierarchical representation discrete functions application modelchecking. Computer Aided Verification, pp. 4154.Modi, P. J., Shen, W., Tambe, M., & Yokoo, M. (2005). ADOPT: asynchronous distributed constraint optimization quality guarantees. Artificial Intelligence, 161, 149180.518fiAND/OR ULTI -VALUED ECISION IAGRAMS (AOMDD ) G RAPHICAL ODELSMoskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineeringefficient SAT solver. Proceedings Thirty Eighth Design Automation Conference, pp.530535.Nilsson, N. J. (1980). Principles Artificial Intelligence. Tioga, Palo Alto, CA.Palacios, H., Bonet, B., Darwiche, A., & Geffner, H. (2005). Pruning conformant plans counting models compiled d-DNNF representations. Proceedings 15th InternationalConference Planning Scheduling (ICAPS05), pp. 141150.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.Sang, T., Bacchus, F., Beame, P., Kautz, H., & Pitassi, T. (2004). Combining component cachingclause learning effective model counting. Proceedings Seventh InternationalConference Theory Applications Satisfiability Testing (SAT04).Sanner, S., & McAllester, D. (2005). Affine algebraic decision diagrams (AADDs) application structured probabilistic inference. Proceedings Nineteenth InternationalJoint Conference Artificial Intelligence (IJCAI05), pp. 13841390.Selman, B., & Kautz, H. (1996). Knowledge compilation theory approximation. JournalACM, 43(2), 193224.Shenoy, P. (1992). Valuation-based systems Bayesian decision analysis. Operations Research,40, 463484.Srinivasan, A., Kam, T., Malik, S., & Brayton, R. K. (1990). Algorithms discrete functionmanipulation. International conference CAD, pp. 9295.Tafertshofer, P., & Pedram, M. (1997). Factored edge-valued binary decision diagrams. FormalMethods System Design, 10(2-3), 243270.Terrioux, C., & Jegou, P. (2003a). Bounded backtracking valued constraint satisfactionproblems. Proceedings Ninth International Conference Principles PracticeConstraint Programming (CP03), pp. 709723.Terrioux, C., & Jegou, P. (2003b). Hybrid backtracking bounded tree-decomposition constraint networks. Artificial Intelligence, 146, 4375.Wilson, N. (2005). Decision diagrams computation semiring valuations. ProceedingsNineteenth International Joint Conference Artificial Intelligence (IJCAI05), pp.331336.Zuzek, A., Drechsler, R., & Thornton, M. (2000). Boolean function representation spectralcharacterization using AND/OR graphs. Integration, VLSI journal, 29, 101116.519fiJournal Artificial Intelligence Research 33 (2008) 1-31Submitted 05/08; published 09/08Anytime Induction Low-cost, Low-error Classifiers:Sampling-based ApproachSaher EsmeirShaul Markovitchesaher@cs.technion.ac.ilshaulm@cs.technion.ac.ilComputer Science DepartmentTechnionIsrael Institute TechnologyHaifa 32000, IsraelAbstractMachine learning techniques gaining prevalence production wide rangeclassifiers complex real-world applications nonuniform testing misclassificationcosts. increasing complexity applications poses real challenge resourcemanagement learning classification. work introduce ACT (anytimecost-sensitive tree learner), novel framework operating complex environments.ACT anytime algorithm allows learning time increased return lowerclassification costs. builds tree top-down exploits additional time resourcesobtain better estimations utility different candidate splits. Using samplingtechniques, ACT approximates cost subtree candidate split favorsone minimal cost. stochastic algorithm, ACT expected ableescape local minima, greedy methods may trapped. Experimentsvariety datasets conducted compare ACT state-of-the-art cost-sensitivetree learners. results show majority domains ACT produces significantlyless costly trees. ACT also exhibits good anytime behavior diminishing returns.1. IntroductionTraditionally, machine learning algorithms focused induction modelslow expected error. many real-word applications, however, several additional constraintsconsidered. Assume, example, medical center decided use machine learning techniques build diagnostic tool heart disease. comprehensibilitydecision tree models (Hastie, Tibshirani, & Friedman, 2001, chap. 9) makes preferred choice base tool. Figure 1 shows three possible trees. first tree(upper-left) makes decisions using results cardiac catheterization (heart cath).tree expected highly accurate. Nevertheless, high costs risks associated heart cath procedure make decision tree impractical. second tree(lower-left) dispenses need cardiac catheterization reaches decision basedsingle, simple, inexpensive test: whether patient complains chest pain.tree would highly accurate: people experience chest painindeed healthy. tree, however, distinguish costs different typeserrors. false positive prediction might result extra treatments, false negativeprediction might put persons life risk. Therefore, third tree (right) preferred, oneattempts minimize test costs misclassification costs simultaneously.c2008AI Access Foundation. rights reserved.fiEsmeir & Markovitchheart cathnormalchest painyesyesbloodpressurealertingbloodpressureyescardiacstressnormalnormalnormalnormalheart cathyesyesyesalertingyesnormalalertingnormalheart cathchest painyesalertingFigure 1: Three possible decision trees diagnosis heart diseases. upper-left treebases decision solely heart cath therefore accurate prohibitivelyexpensive. lower-left tree dispenses need heart cath reachesdecision using single, simple, inexpensive test: whether patientcomplains chest pain. tree would highly accuratedistinguish costs different error types. third (right-hand)tree preferable: attempts minimize test costs misclassification costssimultaneously.cost(a1-10) = $$a9a1001a7a101a6a90cost(a1-8) = $$cost(a9,10) = $$$$$$a10a911a400a4110Figure 2: Left: example difficulty greedy learners might face. Right: exampleimportance context-based feature evaluation.Finding tree lowest expected total cost least NP-complete.1cost insensitive case, greedy heuristic used bias search towards low-cost trees.Decision Trees Minimal Cost (DTMC), greedy method attempts minimize1. Finding smallest consistent tree, easier problem, NP-complete (Hyafil & Rivest, 1976).2fiAnytime Induction Low-cost, Low-error Classifierstypes costs simultaneously, recently introduced (Ling, Yang, Wang, &Zhang, 2004; Sheng, Ling, Ni, & Zhang, 2006). tree built top-down, greedy splitcriterion takes account testing misclassification costs used. basicidea estimate immediate reduction total cost split, prefersplit maximal reduction. split reduces cost training data,induction process stopped.Although efficient, DTMC approach trapped local minimumproduce trees globally optimal. example, consider concept costsdescribed Figure 2 (left). 10 attributes, a9 a10 relevant.cost a9 a10 , however, significantly higher others. high costs mayhide usefulness a9 a10 , mislead learner repeatedly splitting a18 ,would result large, expensive tree. problem would intensified a9a10 interdependent, low immediate information gain (e.g., a9 a10 ).case, even costs uniform, local measure might fail recognize relevancea9 a10 .DTMC appealing learning resources limited. However, requiresfixed runtime cannot exploit additional resources escape local minima. manyreal-life applications, willing wait longer better tree induced (Esmeir &Markovitch, 2006). example, importance model saving patients lives mayconvince medical center allocate 1 month learn it. Algorithms exploitadditional time produce better solutions called anytime algorithms (Boddy & Dean,1994).ICET algorithm (Turney, 1995) pioneer non-greedy search treeminimizes test misclassification costs. ICET uses genetic search produce newset costs reflects original costs contribution attributereducing misclassification costs. builds tree using EG2 algorithm (Nunez,1991) evolved costs instead original ones. EG2 greedy cost-sensitivealgorithm builds tree top-down evaluates candidate splits consideringinformation gain yield measurement costs. not, however, takeaccount misclassification cost problem.ICET shown significantly outperform greedy tree learners, producing treeslower total cost. ICET use additional time resources produce generationshence widen search space costs. genetic operations randomized,ICET likely escape local minima EG2 original costs mighttrapped. Nevertheless, two shortcomings limit ICETs ability benefit extra time.First, search phase, uses greedy EG2 algorithm build final tree.EG2 prefers attributes high information gain (and low test cost),usefulness highly relevant attributes may underestimated greedy measurecase hard-to-learn concepts attribute interdependency hidden. resultexpensive trees. Second, even ICET overcomes problem randomlyreweighting attributes, searches space parameters globally, regardlesscontext tree. imposes problem attribute important one subtreeuseless another. better understand shortcomings, consider concept describedtree Figure 2 (right). 10 attributes similar costs. value a1determines whether target concept a7 a9 a4 a6 . interdependencies result3fiEsmeir & Markovitchlow gain attributes. ICET assigns costs globally, attributessimilar costs well. Therefore, ICET able recognize one relevantcontext. irrelevant attributes cheaper, problem intensifiedmodel might end relying irrelevant attributes.Recently, introduced cost-insensitive LSID3 algorithm, induceaccurate trees allocated time (Esmeir & Markovitch, 2007a). algorithm evaluates candidate split estimating size smallest consistent treeit. estimation based sampling space consistent trees, sizesample determined advance according allocated time. LSID3 designed,however, minimize test misclassification costs. work build LSID3propose ACT, anytime cost-sensitive tree learner exploit additional timeproduce lower-cost trees. Applying sampling mechanism cost-sensitive setup,however, trivial imposes three major challenges: (1) produce sample,(2) evaluate sampled trees, (3) prune induced trees. Section3 show obstacles may overcome.Section 4 report extensive set experiments compares ACT severaldecision tree learners using variety datasets costs assigned human expertsautomatically. results show ACT significantly better majorityproblems. addition, ACT shown exhibit good anytime behavior diminishingreturns.2. Cost-Sensitive ClassificationOffline concept learning consists two stages: learning stage, set labeledexamples used induce classifier; classification stage, inducedclassifier used classify unlabeled instances. two stages involve different typescosts (Turney, 2000). primary goal work trade learning speedreduction test misclassification costs. make problem well defined, needspecify: (1) misclassification costs represented, (2) test costs calculated,(3) combine types cost.answer questions, adopt model described Turney (1995). problem|C| different classes, misclassification cost matrix |C| |C| matrix whose Mi,jentry defines penalty assigning class ci instance actually belongsclass cj . Typically, entries main diagonal classification cost matrix (no error)zero.classifying example e using tree , propagate e tree alongsingle path root one leaves. Let (T, e) set tests alongpath. denote cost() cost administering test . testing cost ePtherefore tcost(T, e) = cost(). Note use sets notation testsappear several times charged once. addition, model described Turney(1995) handles two special test types, namely grouped delayed tests.Grouped Tests. tests share common cost, would like chargeonce. Typically, test also extra (possibly different) cost. example, considertree path tests like cholesterol level glucose level. values measured,blood test needed. Taking blood samples measure cholesterol level clearly lowers4fiAnytime Induction Low-cost, Low-error Classifierscost measuring glucose level. Formally, test possibly belongs group.2first test group administered, charge full cost. anothertest group already administered earlier decision path,charge marginal cost.Delayed Tests. Sometimes outcome test cannot obtained immediately, e.g.,lab test results. tests, called delayed tests, force us wait outcomeavailable. Alternatively, Turney (1995) suggests taking account possible outcomes:delayed test encountered, tests subtree administeredcharged for. result delayed test available, prediction hand.One problem setup follows paths subtree, regardlessoutcome non-delayed costs. Moreover, possible distinguish delaysdifferent tests impose: example, one result might ready several minutesanother days. work handle delayed tests,explain ACT modified take account.test misclassification costs measured, important questionremains: combine them? Following Turney (1995), assumecost types given scale. general model would require utility functioncombines types. Qin, Zhang, Zhang (2004) presented method handletwo kinds cost scales setting maximal budget one kind cost minimizingone. Alternatively, patient preferences elicited summarized utilityfunction (Lenert & Soetikno, 1997).Note algorithm introduce paper adapted cost model.important property cost-sensitive setup maximizing generalization accuracy,goal existing learners, viewed special case: accuracyobjective, test costs ignored misclassification cost uniform.3. ACT AlgorithmACT, proposed anytime framework induction cost-sensitive decision trees, buildsrecently introduced LSID3 algorithm. LSID3 adopts general top-down schemeinduction decision trees (TDIDT): starts entire set training examples,partitions subsets testing value attribute, recursively buildssubtrees. Unlike greedy inducers, LSID3 invests time resources making better splitdecisions. every candidate split, LSID3 attempts estimate size resultingsubtree split take place. Following Occams razor (Blumer, Ehrenfeucht, Haussler, & Warmuth, 1987; Esmeir & Markovitch, 2007b), favors one smallestexpected size.estimation based biased sample space trees rooted evaluatedattribute. sample obtained using stochastic version ID3 (Quinlan, 1986),call SID3. SID3, rather choosing attribute maximizes informationgain (as ID3), choose splitting attribute semi-randomly. likelihoodattribute chosen proportional information gain. Due randomization,2. model test may belong single group. However, easy extend work allowtests belong several groups.5fiEsmeir & MarkovitchProcedure LSID3-Choose-Attribute(E, A, r)r = 0Return ID3-Choose-Attribute(E, A)ForeachForeach vi domain(a)Ei {e E | a(e) = vi }miniRepeat r timesSID3(Ei , {a})mini min (mini , Size(T ))P|domain(a)|totala i=1miniReturn totala minimalFigure 3: Attribute selection LSID3repeated invocations SID3 result different trees. candidate attribute a, LSID3invokes SID3 r times form sample r trees rooted a, uses size smallesttree sample evaluate a. Obviously, r larger, resulting size estimationsexpected accurate, improving final tree. Consider, example, 3-XORconcept several additional irrelevant attributes. LSID3 prefer one relevantattributes root, one trees samples relevant attributes mustsmallest. probability event increases increase sample size.LSID3 contract anytime algorithm parameterized r, sample size. Additionaltime resources utilized forming larger samples. Figure 3 lists procedureattribute selection applied LSID3. Let = |E| number examplesn = |A| number attributes. runtime complexity LSID3 O(rmn3 ). LSID3shown exhibit good anytime behavior diminishing returns. appliedhard concepts, produced significantly better trees ID3 C4.5.ACT takes sampling approach LSID3. three major componentsLSID3 need replaced order adapt cost-sensitive problems are: (1)sampling space trees, (2) evaluating tree, (3) pruning tree.3.1 Obtaining SampleLISD3 uses SID3 bias samples towards small trees. ACT, however, would likebias sample towards low-cost trees. purpose, designed stochastic versionEG2 algorithm, attempts build low cost trees greedily. EG2, treebuilt top-down, test maximizes ICF chosen splitting node, where,ICF () =2I() 1.(cost () + 1)winformation gain (as ID3). parameter w [0, 1] controls biastowards lower cost attributes. w = 0, test costs ignored ICF relies solely6fiAnytime Induction Low-cost, Low-error ClassifiersProcedure SEG2-Choose-Attribute(E, A)Foreach(a) Information-Gain(E, a)c (a) Cost(a)2I(a) 1p (a) (c(a)+1)wChoose attribute random A;attribute a, probabilityselecting proportional p (a)ReturnFigure 4: Attribute selection SEG2information gain. Larger values w strengthen effect test costs ICF.discuss setting value w Section 3.5.stochastic EG2 (SEG2), choose splitting attributes semi-randomly, proportionallyICF. SEG2 stochastic, expect able escape local minimaleast trees sample. Figure 4 formalizes attribute selection componentSEG2. obtain sample size r, ACT uses EG2 SEG2 r 1 times. EG2SEG2 given direct access context-based costs, i.e., attribute alreadytested, cost zero another attribute belongs grouptested, group discount applied.3.2 Evaluating SubtreeLSID3 cost-insensitive learning algorithm. such, main goal maximizeexpected accuracy learned tree. Occams razor states given two consistenthypotheses, smaller one likely accurate. Following Occams razor, LSID3uses tree size preference bias favors splits expected reduce finalsize.cost-sensitive setup, however, goal minimize expected total costclassification. Therefore, rather choosing attribute minimizes size,would like choose one minimizes total cost. Given decision tree, needcome procedure estimates expected cost using tree classifyfuture case. cost two components: test cost misclassification cost.3.2.1 Estimating Test CostsAssuming distribution future cases would similar learningexamples, estimate test costs using training data. Given tree, calculateaverage test cost training examples use estimate test cost newcases. (sub)tree built E, set training examples, denote averagecost traversing example Etcost(T, E) =1 Xtcost(T, e).eE7fiEsmeir & Markovitchestimated test cost unseen example e therefore tcost(T,e ) = tcost(T, E).Observe costs calculated relevant context. attribute alreadytested upper nodes, charge testing again. Similarly, attributegroup g already tested, apply group discount attributesg. delayed attribute encountered, sum cost entire subtree.3.2.2 Estimating Misclassification Costsgo estimating cost misclassification obvious. tree sizelonger used heuristic predictive errors. Occams razor allows comparisontwo consistent trees provides means estimating accuracy. Moreover, tree sizemeasured different currency accuracy hence cannot easily incorporatedcost function.Rather using tree size, propose different estimator: expected error(Quinlan, 1993). leaf training examples, misclassified,expected error defined upper limit probability error, i.e., EE(m, s, cf ) =bin (m, s), cf confidence level U bin upper limit confidenceUcfinterval binomial distribution. expected error tree sum expectederrors leaves.Originally, expected error used C4.5s error-based pruning predict whethersubtree performs better leaf. Although lacking theoretical basis, shownexperimentally good heuristic. ACT use expected error approximatemisclassification cost. Assume problem |C| classes misclassification costmatrix . Let c class label leaf l. Let ml total number examplesl mil number examples l belong class i. penaltiespredictive errors uniform (Mi,j = mc), estimated misclassification cost l(l) = EE(ml , ml mc , cf ) mc.mcostlproblem nonuniform misclassification costs, mc replaced costactual errors leaf expected make. errors obviously unknownlearner. One solution estimate error type separately using confidence intervalsmultinomial distribution multiply associated cost:(l) =mcostXmulUcf(ml , mil , |C|) mc.i6=capproach, however, would result overly pessimistic approximation, mainlymany classes. Alternatively, compute expected error uniformcase propose replacing mc weighted average penalty classifyinginstance c belongs another class. weights derived proportionsmilml mc using generalization Laplaces law succession (Good, 1965, chap. 4):l(l) = EE(ml , ml mc , cf )mcostlXi6=c!mil + 1Mc,i .ml mcl + |C| 1Note problem C classes, average C 1 possible penaltiesMc,c = 0. Hence, problem two classes c1 , c2 leaf marked c1 , mc8fiAnytime Induction Low-cost, Low-error ClassifiersProcedure ACT-Choose-Attribute(E, A, r)r = 0Return EG2-Choose-Attribute(E, A)ForeachForeach vi domain(a)Ei {e E | a(e) = vi }EG2(a, Ei , {a})mini Total-Cost(T, Ei )Repeat r 1 timesSEG2(a, Ei , {a})mini min (mini , Total-Cost(T, Ei ))P|domain(a)|minitotala Cost(a) + i=1Return totala minimalFigure 5: Attribute selection ACTwould replaced M1,2 . classifying new instance, expected misclassificationcost tree built examples sum expected misclassification costsleaves divided m:1 Xmcost(l),mcost(T)=L set leaves . Hence, expected total cost classifyingsingle instance is:total(T,E) = tcost(T,E) + mcost(T).alternative approach intend explore future work estimate costsampled trees using cost set-aside validation set. approach attractivemainly training set large one afford setting aside significant partit.3.3 Choosing Splitdecided sampler tree utility function, ready formalizetree growing phase ACT. tree built top-down. procedure selectingsplitting test node listed Figure 5 illustrated Figure 6. givedetailed example ACT chooses splits explain split selection proceduremodified numeric attributes.3.3.1 Choosing Split: Illustrative ExamplesACTs evaluation cost-senstive considers test error costs simultaneouslytake account different error penalties. illustrate let us considertwo-class problem mc = 100$ (uniform) 6 attributes, a1 , . . . , a6 , whose costs10$. training data contains 400 examples, 200 positive 200negative.9fiEsmeir & MarkovitchG2SEst( 4.9=co)cost(EG2)=4.7cost(SEG2)=5.1cost(EG2)=8.9Figure 6: Attribute evaluation ACT. Assume cost current context0.4. estimated cost subtree rooted therefore 0.4 + min(4.7, 5.1) +min(8.9, 4.9) = 10.Test costsa1 10a2 10a3 10a4 10a5 10a6 10MC costsFP 100FN 100r=1T1T2a1a35 +95 EE = 7.3-a2a4a595 +5EE = 7.3+5 +95 EE = 7.3-95 +5EE = 7.3+mcost (T1) = 7.3*4*100$ / 400 = 7.3$tcost (T1) = 20$total(T1) = 27.3$0 +50 EE = 1.4-a6100 +50 EE = 54.1+0 +50 EE = 1.4-100 +50 EE = 54.1+mcost (T2) = (1.4*2 + 54.1*2) * 100$ / 400 = 27.7$tcost (T2) = 20$total (T2) = 47.7$Figure 7: Evaluation tree samples ACT. leftmost column defines costs: 6attributes identical cost uniform error penalties. 1 sampled a12 a2 . EE stands expected error. total cost T1lower, ACT would prefer split a1 .Assume choose a1 a2 , r = 1. Let treesFigure 7, denoted 1 2, sampled a1 a2 respectively. expectederror costs T1 T2 are:3mcost(T1) =mcost(T2) ==14 7.3(4 EE (100, 5, 0.25)) 100$ =100$ = 7.3$4004001(2 EE (50, 0, 0.25) 100$ + 2 EE (150, 50, 0.25) 100$)4002 1.4 + 2 54.1100$ = 27.7$400test error costs involved, ACT considers sum. Since testcost trees identical (20$), ACT would prefer split a1 . If, however, cost3. example set cf 0.25, C4.5. Section 3.5 discuss tune cf .10fiAnytime Induction Low-cost, Low-error ClassifiersTest costsa1 40a2 10a3 10a4 10a5 10a6 10MC costsFP 100FN 100r=1T1T2a1a3-a4a595 +5EE = 7.35 +95 EE = 7.395 +5EE = 7.35 +95 EE = 7.3a2-++mcost (T1) = 7.3*4*100$ / 400 = 7.3$tcost (T1) = 50$total(T1) = 57.3$a6-100 +50 EE = 54.10 +50 EE = 1.4100 +50 EE = 54.10 +50 EE = 1.4-++mcost (T2) = (1.4*2 + 54.1*2) * 100$ / 400 = 27.7$tcost (T2) = 20$total (T2) = 47.7$Figure 8: Evaluation tree samples ACT. leftmost column defines costs: 6attributes identical cost (except expensive a1 ) uniform errorpenalties. 1 sampled a1 2 a2 . total cost T2lower, ACT would prefer split a2 .Test costsa1 10a2 10a3 10a4 10a5 10a6 10MC costsFP 1FN 199r=1T1T2a1a35 +95 EE = 7.3-a2a4a595 +5EE = 7.3+5 +95 EE = 7.3-95 +5EE = 7.3+mcost (T1) = (7.3*2*1$ + 7.3*2*199$) / 400 = 7.3$tcost (T1) = 20$total (T1) = 27.3$0 +50 EE = 1.4-a6100 +50 EE = 54.1+0 +50 EE = 1.4-100 +50 EE = 54.1+mcost (T2) = (54.1*2*1$ + 1.4*2*199$) / 400 = 1.7$tcost (T2) = 20$total (T2) = 21.7$Figure 9: Evaluation tree samples ACT. leftmost column defines costs: 6attributes identical cost nonuniform error penalties. 1 sampleda1 2 a2 . total cost T2 lower, ACT would prefersplit a2 .a1 40$, Figure 8, tcost(T 1) would become 50$ total cost 1 wouldbecome 57.3$, 2 would remain 47.7$. Hence, case ACT would splita2 .illustrate ACT handles nonuniform error penalties, let us assume costattributes 10$, cost false positive (FP ) 1$ costfalse negative (F N ) 199$. Let trees Figure 9, denoted 1 2, sampleda1 a2 respectively. first example, misclassification costs play roletest costs trees same. Although average misclassification11fiEsmeir & Markovitchcost also 100$, ACT evaluates trees differently:mcost(T1) ==mcost(T2) ==1(2 EE (100, 5, 0.25) 1$ + 2 EE (100, 5, 0.25) 199$)4002 7.3 1$ + 2 7.3.1 199$= 7.3$4001(2 EE (50, 0, 0.25) 199$ + 2 EE (100, 50, 0.25) 1$)4002 1.4 199$ + 2 54.1 1$= 1.7$400Therefore, nonuniform setup, ACT would prefer a2 . makes sensegiven setup prefer trees may result false positives reduce numberexpensive false negatives.3.3.2 Choosing Split Attributes Numericselection procedure formalized Figure 5 must modified slightly attribute numeric: rather iterating values attribute take, first pickr tests (split points) highest information gain invoke EG2split point. guarantees numeric nominal attributes get resources.Chickering, Meek, Rounthwaite (2001) introduced several techniques generatingsmall number candidate split points dynamically little overhead. futureintend apply techniques select r points, evaluatedsingle invocation EG2.3.4 Cost-Sensitive PruningPruning plays important role decision tree induction. cost-insensitive environments,main goal pruning simplify tree order avoid overfitting trainingdata. subtree pruned resulting tree expected yield lower error.test costs taken account, pruning another important role: reducingtest costs tree. Keeping subtree worthwhile expected reduction misclassification costs larger cost tests subtree. misclassificationcost zero, makes sense keep split tree. If, hand, misclassification cost much larger test costs, would expect similar behaviorcost-insensitive setup.handle challenge, propose novel approach cost-sensitive pruning.error-based pruning (Quinlan, 1993), scan tree bottom-up. compareexpected total cost subtree leaf. leaf expected perform better,subtree pruned.cost subtree estimated described Section 3.2. Formally, let Eset training examples reach subtree , let size E. Assumeexamples E belong default class.4 Let L set leaves .4. misclassification costs uniform, default class majority class. Otherwise, classminimizes misclassification cost node.12fiAnytime Induction Low-cost, Low-error Classifiersprune leaf if:X1mcost(l).EE(m, s, cf ) mc tcost(T,E) +assumes uniform misclassification cost mc. case nonuniform penalties,multiply expected error average misclassification cost.alternative approach post-pruning early stopping growing phase.example, one could limit depth tree, require minimal number exampleschild (as C4.5), prevent splitting nodes splitting criterion fails exceedpredetermined threshold (as DTMC). Obviously, pre-pruning condition alsoapplied part post-pruning procedure. advantage post-pruning, however,ability estimate effect split entire subtree it,immediate successors (horizon effect).Consider example 2-XOR problem b. Splitting neither b wouldpositive gain hence growing would stopped. pre-pruning allowed,optimal tree would found would post-pruned utilitysplits correctly measured. Frank (2000) reports comprehensive study pruningdecision trees, compared pre- post-pruning empirically cost-insensitivesetup. findings show advantage post-pruning variety UCI datasetssignificant. pre-pruning computationally efficient, Frank concludedthat, practice, might viable alternative post-pruning. Despite results,decided use post-pruning ACT, following reasons:1. Several concepts represented UCI repository may appear real-worldproblems. example, parity functions naturally arise real-world problems,Drosophila survival concept (Page & Ray, 2003).2. costs involved, horizon effect may appear frequently highcosts may hide good splits.3. anytime setup user willing wait longer order obtain good tree.Since post-pruning takes even less time induction single greedy tree,extra cost post-pruning minor.future plan add pre-pruning parameter allow early stoppingresources limited. Another interesting direction future work would postprune final tree pre-prune lookahead trees form samples. wouldreduce runtime cost less accurate estimations utility candidatesplit.3.5 Setting Parameters ACTaddition r, sample size, ACT parameterized w, controls weighttest costs EG2, cf , confidence factor used pruning errorestimation. ICET tunes w cf using genetic search. ACT considered three differentalternatives: (1) keeping EG2s C4.5s default values w = 1 cf = 0.25, (2) tuning13fiEsmeir & Markovitchvalues using cross-validation, (3) setting values priori, functionproblem costs.first solution simplest, exploit potential adaptingsampling mechanism specific problem costs. Although tuning values usinggrid search would achieve good results, may costly terms runtime. example,5 values parameter used 5-fold cross-validation, would needrun ACT 125 times sake tuning alone. anytime setup time couldinvested invoke ACT larger r hence improve results. Furthermore,algorithm would able output valid solution tuning stage finished.Alternatively, could try tune parameters invoking much faster EG2,results would good optimal values EG2 necessarilygood ACT.third approach, chose experiments, set w cf advance,according problem specific costs. w set inverse proportionally misclassification cost: high misclassification cost results smaller w, reducing effect attributecosts split selection measure. exact formula is:w = 0.5 + ex ,x average misclassification cost (over non-diagnoal entries ) dividedC, cost take tests. Formally,x=PMi,j.(|C| 1) |C| Ci6=jC4.5 default value cf 0.25. Larger cf values result less pruning. Smallercf values lead aggressive pruning. Therefore, ACT set cf valuerange [0.2, 0.3]; exact value depends problem cost. test costs dominant,prefer aggressive pruning hence low value cf . test costs negligible,prefer prune less. value cf also used estimate expected error.Again, test costs dominant, afford pessimistic estimate error,misclassification costs dominant, would prefer estimate closererror rate training data. exact formula setting cf is:x1cf = 0.2 + 0.05(1 +).x+14. Empirical Evaluationconducted variety experiments test performance behavior ACT.First introduce novel method automatic adaption existing datasets costsensitive setup. describe experimental methodology motivation. Finallypresent discuss results.4.1 DatasetsTypically, machine learning researchers use datasets UCI repository (Asuncion &Newman, 2007). five UCI datasets, however, assigned test costs.5 include5. Costs datasets assigned human experts (Turney, 1995).14fiAnytime Induction Low-cost, Low-error Classifiersdatasets experiments. Nevertheless, gain wider perspective,developed automatic method assigns costs existing datasets. methodparameterized with:1. cr, cost range.2. g, number desired groups percentage number attributes.problem |A| attributes, g |A| groups. probability attribute1, probability belongbelong groups g|A|+1groups.3. d, number delayed tests percentage number attributes.4. , group discount percentage minimal cost group (to ensurepositive costs).5. , binary flag determines whether costs drawn randomly, uniformly ( = 0)semi-randomly ( = 1): cost test drawn proportionally informationgain, simulating common case valuable features tend higher costs.case assume cost comes truncated normal distribution,mean proportional gain.Using method, assigned costs 25 datasets: 20 arbitrarily chosen UCI datasets65 datasets represent hard concepts used previous research. Appendix gives detailed descriptions datasets.Due randomization cost assignment process, set parametersdefines infinite space possible costs. 25 datasets sampled space4 timescr = [1, 100], g = 0.2, = 0, = 0.8, = 1.parameters chosen attempt assign costs manner similarreal costs assigned. total, 105 datasets: 5 assigned human experts100 automatically generated costs.7Cost-insensitive learning algorithms focus accuracy therefore expected perform well testing costs negligible relative misclassification costs. However,testing costs significant, ignoring would result expensive classifiers. Therefore,evaluating cost-sensitive learners requires wide spectrum misclassification costs.problem 105, created 5 instances, uniform misclassification costsmc = 100, 500, 1000, 5000, 10000. Later on, also consider nonuniform misclassificationcosts.4.2 Methodologystart experimental evaluation comparing ACT, given fixed resource allocation, several cost-sensitive cost-insensitive algorithms. Next compareanytime behavior ACT ICET. Finally, evaluate algorithms6. chosen UCI datasets vary size, type attributes, dimension.7. additional 100 datasets available http://www.cs.technion.ac.il/esaher/publications/cost.15fiEsmeir & Markovitchtwo modifications problem instances: random test cost assignment nonuniformmisclassification costs.4.2.1 Compared AlgorithmsACT compared following algorithms:C4.5 : cost-insensitive greedy decision tree learner. algorithm reimplemented following details (Quinlan, 1993) default parametersused.LSID3 : cost-insensitive anytime decision tree learner. uses extra timeinduce trees higher accuracy. able, however, exploit additional allottedtime reduce classification costs.IDX : greedy top-down learner prefers splits maximizec (Norton, 1989).algorithm take account misclassification costs. IDX implemented top C4.5, modifying split selection criteria.2CSID3 : greedy top-down learner prefers splits maximize Ic (Tan &Schlimmer, 1989). algorithm take account misclassification costs.CSID3 implemented top C4.5, modifying split selection criteria.EG2 : greedy top-down learner prefers splits maximize2I() 1w(cost()+1)(Nunez, 1991). algorithm take account misclassification costs.EG2 implemented top C4.5, modifying split selection criteria.DTMC : DTMC implemented following original pseudo-code (Ling et al.,2004; Sheng et al., 2006). However, original pseudo-code support continuous attributes multiple class problems. added support continuousattributes, C4.5s dynamic binary-cut discretization, cost reductionreplacing gain ratio selecting cutting points. extension multiple class problems straightforward. Note DTMC post-prune treespre-prunes them.ICET : ICET reimplemented following detailed description givenTurney (1995). verify results reimplementation, comparedreported literature. followed experimental setupused 5 datasets. results indeed similar: basic version ICETachieved average cost 50.8 reimplementation vs. 50 reported originally.One possible reason slight difference may initial populationgenetic algorithm randomized, genetic operators processpartitioning data training, validating, testing sets. paper, Turneyintroduced seeded version ICET, includes true costs initialpopulation, reported perform better unseeded version. Therefore,use seeded version comparison. parameters ICETdefault ones.16fiAnytime Induction Low-cost, Low-error Classifiers4.2.2 Normalized CostTurney (1995) points out, using average cost classification datasetproblematic because: (1) cost differences algorithms become relatively smallmisclassification cost increases, (2) difficult combine results multipledatasets fair manner (e.g., average), (3) difficult combine averagedifferent misclassification costs. overcome problems, Turney suggests normalizingaverage cost classification dividing standard cost. Let C costtake tests. Let fi frequency class data. error responsealways class therefore (1 fi ). standard cost definedC + mini (1 fi ) maxi,j (Mi,j ) .standard cost approximation maximal cost given problem.consists two components: maximal test cost misclassification costclassifier achieves baseline accuracy (e.g., majority-based classifier errorcosts uniform). classifiers may perform even worse baselineaccuracy, standard cost strictly upper bound real cost.experiments, however, exceeded.4.2.3 Statistical Significanceproblem 105, single 10-fold cross-validation experiment conducted.partition train-test sets used compared algorithms. determinestatistical significance performance differences ACT, ICET, DTMCused two tests:Paired t-test = 5% confidence. problem 105pair algorithms, 10 pairs results obtained 10-fold crossvalidation runs. used paired t-test determine weather differencetwo algorithms given problem significant (rejecting null hypothesisalgorithms differ performance). Then, count algorithmmany times significant winner.Wilcoxon test (Demsar, 2006), compares classifiers multiple datasetsstates whether one method significantly better ( = 5%).4.3 Fixed-time Comparison105 5 problem instances, ran different algorithms, including ACTr = 5. chose r = 5 average runtime ACT would shorter ICETproblems. methods much shorter runtime due greedy nature.Table 1 summarizes results.8 pair numbers represents average normalizedcost associated confidence interval ( = 5%). Figure 10 illustrates average resultsplots normalized costs different algorithms misclassification costs.Statistical significance test results ACT, ICET, DTMC given Table 2.algorithms compared using t-test Wilcoxon test. table lists8. full results available http://www.cs.technion.ac.il/esaher/publications/cost.17fiEsmeir & MarkovitchTable 1: Average cost classification percentage standard cost classificationdifferent mc values. numbers represent average 105 datasetsassociated confidence intervals ( = 5%).mc1005001000500010000C4.550.649.950.453.354.5LSID34.24.24.65.96.445.343.042.443.644.5IDX3.73.94.56.16.634.442.447.558.160.8CSID33.63.64.25.96.441.745.247.854.356.23.83.94.45.96.4EG235.142.547.357.359.9DTMC3.63.64.35.96.414.637.747.157.659.51.83.13.85.25.6ICET24.336.340.645.747.13.13.13.95.66.0ACT15.234.539.141.541.41.93.24.25.76.0Table 2: DTMC vs. ACT ICET vs. ACT using statistical tests. mc, firstcolumn lists number t-test significant wins second column giveswinner, any, implied Wilcoxon test datasets = 5%.test WINSmc1005001000500010000DTMC vs. ACT149776329455056W ilcoxon WINNERICET vs. ACT45121575423242124DTMC vs. ACTICET vs. ACTDTMCACTACTACTACTACTACTACTACT-number t-test wins algorithm 105 datasets, well winner,any, Wilcoxon test applied.misclassification cost relatively small (mc = 100), ACT clearly outperformsICET, 54 significant wins opposed ICETs 4 significant wins. significantdifference found remaining runs. setup ACT able producesmall trees, sometimes consisting one node; accuracy learned model ignoredsetup. ICET, contrary, produced, datasets, largercostly trees. DTMC achieved best results, outperformed ACT 14 times.Wilcoxon test also indicates DTMC better ACT ACT betterICET. investigation showed datasets ACT produced unnecessarilylarger trees. believe better tuning cf would improve ACT scenariomaking pruning aggressive.extreme, misclassification costs dominate (mc = 10000), performance DTMC worse ACT ICET. t-test indicates ACTsignificantly better ICET 24 times significantly worse 7 times. AccordingWilcoxon test = 5%, difference ACT ICET significant.Taking > 5.05%, however, would turn result favor ACT. Observe DTMC,winner mc = 100, becomes worst algorithm mc = 10000. One reason18fiAnytime Induction Low-cost, Low-error ClassifiersAverage % Standard Cost60504030C4.5LSID3EG2DTMCICETACT20101001000Misclassification Cost10000ACT CostFigure 10: Average normalized cost function misclassification cost100100100808080606060404040202020000204060ICET Cost8010000204060ICET Cost801000204060ICET Cost80100Figure 11: Illustration differences performance ACT ICET mc =100, 1000, 10000 (from left right). point represents dataset. x-axisrepresents cost ICET y-axis represents ACT. dashedline indicates equality. Points ACT performs betterICET better.phenomenon DTMC, introduced Ling et al. (2004), performpost-pruning, although might improve accuracy domains.two extremes less interesting: first could use algorithmalways outputs tree size 1 second could use cost-insensitive learners.middle range, mc {500, 1000, 5000}, requires learner carefully balancetwo types cost. cases ACT lowest average cost largestnumber t-test wins. Moreover, Wilcoxon test indicates superior. ICETsecond best method. reported Turney (1995), ICET clearly bettergreedy methods EG2, IDX, CSID3.Note EG2, IDX, CSID3, insensitive misclassification cost, produced trees values mc. trees, however, judged differentlychange misclassification cost.Figure 11 illustrates differences ICET ACT mc = 100, 1000, 10000.point represents one 105 datasets. x-axis represents cost ICETy-axis represents ACT. dashed line indicates equality. see,19fiEsmeir & Markovitch100Average Accuracy908070C4.5LSID3EG2DTMCICETACT60501001000Misclassification Cost10000Figure 12: Average accuracy function misclassification costmajority points equality line, indicating ACT performs better.mc = 10000 see points located close x-axis large xvalue. points represent difficult domains, XOR, ICET couldlearn ACT could.4.4 Comparing Accuracy Learned Modelsmisclassification costs low, optimal algorithm would produce shallowtree. misclassification costs dominant, optimal algorithm would producehighly accurate tree. see, ACTs normalized cost increases increasemisclassification cost. relatively easy produce shallow trees, conceptseasily learnable even cost-insensitive algorithms fail achieve perfect accuracythem. Hence, importance accuracy increases, normalized cost increasespredictive errors affect dramatically.learn effect misclassification costs accuracy, compareaccuracy built trees different misclassification costs. Figure 12 shows results.important property DTMC, ICET, ACT ability compromise accuracy needed. produce inaccurate trees accuracy insignificant muchaccurate trees penalty error high. ACTs flexibility, however,noteworthy: second least accurate method becomes accurate one.Interestingly, accuracy extremely important, ICET ACT achieve evenbetter accuracy C4.5. reason non-greedy nature. ICET performsimplicit lookahead reweighting attributes according importance. ACT performslookahead sampling space subtrees every split. two, resultsindicate ACTs lookahead efficient terms accuracy. DTMC less accurateC4.5. reason different split selection criterion different pruningmechanism.comparison anytime cost insensitive algorithm LSID3, ACT produced lessaccurate trees mc relatively low. mc set 5000, however, ACTachieved comparable accuracy LSID3 slightly outperformed mc = 10000.Statistical tests found differences accuracy two algorithms20fiAnytime Induction Low-cost, Low-error Classifiers10EG2DTMCICETACT848Average CostAverage Cost8280EG2DTMCICETACT78766427472000.511.522.5Time [sec]33.544.500.20.40.60.81Time [sec]1.21.41.61.880100709080Average CostAverage Cost605040EG2DTMCICETACT3020EG2DTMCICETACT7060504030102000123Time [sec]450123Time [sec]456Figure 13: Average normalized cost function time (from top-left bottom-right)Breast-cancer-20, Monks1, Multi-XOR, XOR5case insignificant. ACTs small advantage datasets indicates that,problems, expected error better heuristic tree size maximizing accuracy.4.5 Comparison Anytime BehaviorICET ACT, like typical anytime algorithms, perform better increasedresource allocation. ICET expected exploit extra time producing generations hence better tuning parameters final invocation EG2. ACTuse extra time acquire larger samples hence achieve better cost estimations.examine anytime behavior ICET ACT, ran 4 problems,namely Breast-cancer-20, Monks-1, Multi-XOR, XOR5, exponentially increasingtime allocation. mc set 5000. ICET run 2, 4, 8, . . . generations ACTsample size 1, 2, 4, . . .. fixed-time comparison, used 4 instancesproblem. Figure 13 plots results averaged 4 instances. also includedresults greedy methods EG2 DTMC.results show good anytime behavior ICET ACT: generally worthwhile allocate time. ACT dominates ICET four domains ableproduce less costly trees shorter time.One advantage ACT ICET able consider contextattribute judged. ICET, contrary, reassigns cost attributes globally:21fiEsmeir & MarkovitchAverage % Standard Cost6050DTMCICETACT401005001000500010000302010100DTMCICETACT1000Misclassification Cost12.331.540.454.057.41.83.23.95.25.618.731.836.443.745.62.73.43.95.55.913.030.233.938.539.62.03.34.05.66.110000Figure 14: Average cost test costs assigned randomlyattribute cannot assigned high cost one subtree low cost another. MultiXOR dataset exemplifies concept whose attributes important one sub-concept.concept composed four sub-concepts, relies different attributes(see Appendix details). expected, ACT outperforms ICET significantlylatter cannot assign context-based costs. Allowing ICET producegenerations (up 128) result trees comparable obtained ACT.4.6 Random Costscosts 100 105 datasets assigned using semi-random mechanismgives higher costs informative attributes. ensure ACTs success dueparticular cost assignment scheme, repeated experiments costs drawnrandomly uniformly given cost range cr, i.e., set 0. Figure 14 showsresults. see, ACT maintains advantage methods: dominatesalong scale mc values.4.7 Nonuniform Misclassification Costsfar, used uniform misclassification cost matrices, i.e., costerror type identical. explained Section 3, ACT algorithm also handlecomplex misclassification cost matrices penalty one type error mighthigher penalty another type. next experiment examines ACTnonuniform scenario. Let FP denote penalty false positive FN penaltyfalse negative. 2 classes, split classes 2 equal groupsaccording order (or randomly order exists). assign penalty FPmisclassifying instance belongs first group FN one belongssecond group.obtain wide view, vary ratio FP FN also examine differentabsolute values. Table 3 Figure 15 give average results. Table 4 lists numbert-test significant wins algorithm achieved. easy see ACT consistentlyoutperforms methods.22fiAnytime Induction Low-cost, Low-error Classifiers842248= 500C4.5EG2DTCMICETACT29.230.112.423.311.934.233.020.327.018.541.337.229.831.527.249.942.537.736.334.543.639.332.534.229.139.037.522.931.820.436.336.815.829.213.8= 5000Table 3: Comparison C4.5, EG2, DTMC, ACT, ICET misclassification costsnonuniform. FP denotes penalty false positive FN penaltyfalse negative. denotes basic mc unit.C4.5EG2DTCMICETACT27.030.913.821.412.931.335.223.625.619.139.243.138.032.728.853.357.357.645.741.544.047.742.537.431.139.042.429.332.822.536.339.720.129.814.6FPFNTable 4: Comparing DTMC, ACT, ICET misclassification costs nonuniform.F P/F N ratio, columns list number t-test significant wins= 5%. FP denotes penalty false positive FN penaltyfalse negative. denotes basic mc unit.= 500F P/F N0.1250.250.51248DTMC vs. ACT4210953122312529354027= 5000ICET vs. ACT1177510452494223477272DTMC vs. ACT51016754044495250615862ICET vs. ACT124101590044362521314467Interestingly, graphs slightly asymmetric. reason coulddatasets, example medical ones, difficult reduce negative errors positiveones, vice versa. similar phenomenon reported Turney (1995).highest cost algorithms observed F P = F N because,ratio FP FN extremely large extremely small, learner easilybuild small tree whose leaves labeled class minimizes costs.misclassification costs balanced, however, learning process becomes muchcomplicated.23fi50604555Average % Standard CostAverage % Standard CostEsmeir & Markovitch40353025C4.5EG2DTMCICETACT201510/8/4/2/2/Misclassification Cost FP/FN504540353025C4.5EG2DTMCICETACT20154/10/88//4/2/2/Misclassification Cost FP/FN4/8/Figure 15: Comparison C4.5, EG2, DTMC, ACT, ICET misclassification costsnonuniform. misclassification costs represented pair (F P/F N ).FP denotes penalty false positive FN penalty falsenegative. denotes basic mc unit. figures plot average costfunction ratio FP FN, = 500 (left) = 5000(right).5. Related Workaddition works referred earlier paper, several related works warrantdiscussion here.Cost-sensitive trees subject many research efforts. Several works proposed learning algorithms consider different misclassification costs (Breiman, Friedman, Olshen, & Stone, 1984; Pazzani, Merz, Murphy, Ali, Hume, & Brunk, 1994; Provost& Buchanan, 1995; Bradford, Kunz, Kohavi, Brunk, & Brodley, 1998; Domingos, 1999;Elkan, 2001; Zadrozny, Langford, & Abe, 2003; Lachiche & Flach, 2003; Abe, Zadrozny, &Langford, 2004; Vadera, 2005; Margineantu, 2005; Zhu, Wu, Khoshgoftaar, & Yong, 2007;Sheng & Ling, 2007). methods, however, consider test costs henceappropriate mainly domains test costs constraint.Davis, Ha, Rossbach, Ramadan, Witchel (2006) presented greedy cost-sensitivedecision tree algorithm forensic classification: problem classifying irreproducibleevents. setup, assume tests might used testing mustacquired hence charged classification.One way exploit additional time searching less costly tree widensearch space. Bayer-Zubek Dietterich (2005) formulated cost-sensitive learningproblem Markov decision process (MDP), used systematic search algorithmbased AO* heuristic search procedure solve MDP. make AO* efficient,algorithm uses two-step lookahead based heuristic. limited lookaheadinformed immediate heuristics still insufficient complex domains mightcause search go astray (Esmeir & Markovitch, 2007a). algorithm shownoutput better diagnostic policies several greedy methods using reasonable resources.optimal solution, however, could always found due time memory limits.nice property algorithm serve anytime algorithm computing24fiAnytime Induction Low-cost, Low-error Classifiersbest complete policy found far. anytime behavior, nevertheless, problematicpolicies optimal respect train data tend overfit. result,performance eventually start degrade.Arnt Zilberstein (2005) tackled problem time cost sensitive classification(TCSC). TCSC, utility labeling instance depends correctnesslabeling, also amount time takes. Therefore total cost functionadditional component, reflects time needed measure attribute. Typically,super-linear form: cost quick result small fairly constant,waiting time increases, time cost grows increasing rate. problemcomplicated sequence time-sensitive classification instances considered,time spent administering tests one case adversely affect costs futureinstances. Arnt Zilberstein suggest solving problems extending decisiontheoretic approach introduced Bayer-Zubek Dietterich (2005). work,assume time takes administer test incorporated cost.future, intend extend framework support time-sensitive classification,individual cases sequences.Fan, Lee, Stolfo, Miller (2000) studied problem cost-sensitive intrusion detection systems (IDS). goal maximize security minimizing costs.prediction (action) cost. Features categorized three cost levels accordingamount information needed compute values. reduce cost IDS, highcost rules considered predictions low cost rules sufficientlyaccurate.Costs also involved learning phase, example acquisitionmodel learning. problem budgeted learning studied Lizotte, Madani,Greiner (2003). cost associated obtaining attribute valuetraining example, task determine attributes test given budget.related problem active feature-value acquisition. setup one tries reducecost improving accuracy identifying highly informative instances. Melville, SaarTsechansky, Provost, Mooney (2004) introduced approach instancesselected acquisition based accuracy current model confidenceprediction.Greiner, Grove, Roth (2002) pioneers studying classifiers actively decidetests administer. defined active classifier classifier givenpartially specified instance, returns either class label strategy specifiestest performed next. Greiner et al. also analyzed theoretical aspectslearning optimal active classifiers using variant probably-approximately-correct(PAC) model. showed task learning optimal cost-sensitive active classifiersoften intractable. However, task shown achievable active classifierallowed perform (at most) constant number tests, limit providedlearning. setup proposed taking dynamic programming approachbuild trees depth d.setup assumed charged acquiring feature values testcases. term test strategy (Sheng, Ling, & Yang, 2005) describes process featurevalues acquisition: values query order. Several test strategiesstudied, including sequential, single batch multiple batch (Sheng et al., 2006),25fiEsmeir & Markovitchcorresponds different diagnosis policy. strategies orthogonalwork assume given decision tree.Bilgic Getoor (2007) tackled problem feature subset selection costsinvolved. objective minimize sum information acquisition costmisclassification cost. Unlike greedy approaches compute value features onetime, used novel data structure called value information lattice (VOILA),exploits dependencies missing features makes possible share information value computations different feature subsets possible. VIOLA shownempirically achieve dramatic cost improvements without prohibitive computationalcosts comprehensive search.6. ConclusionsMachine learning techniques increasingly used produce wide range classifiers complex real-world applications involve nonuniform testing misclassification costs. increasing complexity applications poses real challengeresource management learning classification. work introduced novelframework operating complex environments. framework four majoradvantages:uses non-greedy approach build decision tree therefore able overcomelocal minima problems.evaluates entire trees search; thus, adjusted cost schemedefined trees.exhibits good anytime behavior allows learning speed traded classification costs. many applications willing allocate timewould allocate greedy methods. proposed framework exploit extraresources.sampling process easily parallelized method benefit distributed computer power.evaluate ACT designed extensive set experiments wide rangecosts. Since publicly available cost-oriented datasets, designedparametric scheme automatically assigns costs existing datasets. experimentalresults show ACT superior ICET DTMC, existing cost-sensitive algorithmsattempt minimize test costs misclassification costs simultaneously. Significancetests found differences statistically strong. ACT also exhibited good anytimebehavior: increase time allocation, cost learned models decreased.ACT contract anytime algorithm requires sample size predetermined.future intend convert ACT interruptible anytime algorithm adoptingIIDT general framework (Esmeir & Markovitch, 2007a). addition, plan applymonitoring techniques (Hansen & Zilberstein, 2001) optimal scheduling ACTexamine strategies evaluating subtrees.26fiAnytime Induction Low-cost, Low-error ClassifiersTable 5: Characteristics datasets usedDatasetBreast CancerBupaCarFlareGlassHeartHepatitisIrisKRKMonks-1Monks-2Monks-3Multiplexer-20Multi-XORMulti-AND-ORNurseryPimaTAETic-Tac-ToeTitanicThyroidVotingWineXOR 3DXOR-5Instances277345172832321429615415028056124+432169+432122+432615200200870376815195822013772232178200200AttributesNominal (binary) Numeric9 (3)0 (0)6 (0)10 (5)0 (0)8(4)13(13)0 (0)6(0)6 (2)6 (2)6 (2)20 (20)11 (11)11 (11)8(8)0(0)4(1)9 (0)3(2)15(15)16 (16)0 (0)0 (0)10 (10)05009564000000008100501360Max attributedomainClasses13474284442225263422222447223172222225232232322Acknowledgmentswork partially supported funding EC-sponsored MUSCLE NetworkExcellence (FP6-507752).Appendix A. DatasetsTable 5 lists characteristics 25 datasets used. give detaileddescription non-UCI datasets used experiments:1. Multiplexer: multiplexer task used several researchers evaluating classifiers (e.g., Quinlan, 1993). instance series bits length + 2a ,positive integer. first bits represent index remaining bitslabel instance value indexed bit. experiments considered20-Multiplexer (a = 4). dataset contains 500 randomly drawn instances.2. Boolean XOR: Parity-like functions known problematic many learningalgorithms. However, naturally arise real-world data, Drosophilasurvival concept (Page & Ray, 2003). considered XOR five variables fiveadditional irrelevant attributes.27fiEsmeir & Markovitch3. Numeric XOR: XOR based numeric dataset used evaluate learningalgorithms (e.g., Baram, El-Yaniv, & Luz, 2003). example consists valuesx coordinates. example labeled 1 product x positive,1 otherwise. generalized domain three dimensions added irrelevantvariables make concept harder.4. Multi-XOR / Multi-AND-OR: concepts defined 11 binary attributes.cases target concept composed several subconcepts, firsttwo attributes determines considered. 10 attributesused form subconcepts. Multi-XOR dataset, subconcept XOR,Multi-AND-OR dataset, subconcept either OR.ReferencesAbe, N., Zadrozny, B., & Langford, J. (2004). iterative method multi-class costsensitive learning. Proceedings 10th ACM SIGKDD International ConferenceKnowledge Discovery Data Mining (KDD-2004), Seattle, WA, USA.Arnt, A., & Zilberstein, S. (2005). Learning policies sequential time cost sensitiveclassification. Proceedings 1st international workshop Utility-based datamining (UBDM05) held KDD05, pp. 3945, New York, NY, USA. ACM Press.Asuncion, A., & Newman, D. (2007).UCI machine learning repository.University California, Irvine, School Information Computer Sciences.http://www.ics.uci.edu/mlearn/MLRepository.html.Baram, Y., El-Yaniv, R., & Luz, K. (2003). Online choice active learning algorithms.Proceedings 20 International Conference Machine Learning (ICML-2003),pp. 1926, Washington, DC, USA.Bayer-Zubek, V., & Dietterich (2005). Integrating learning examples searchdiagnostic policies. Artificial Intelligence, 24, 263303.Bilgic, M., & Getoor, L. (2007). Voila: Efficient feature-value acquisition classification.Proceedings 22nd National Conference Artificial Intelligence (AAAI-2007).Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. (1987). Occams Razor.Information Processing Letters, 24 (6), 377380.Boddy, M., & Dean, T. L. (1994). Deliberation scheduling problem solving timeconstrained environments. Artificial Intelligence, 67 (2), 245285.Bradford, J., Kunz, C., Kohavi, R., Brunk, C., & Brodley, C. (1998). Pruning decisiontrees misclassification costs. Proceedings 9th European ConferenceMachine Learning (ECML-1998), pp. 131136.Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification RegressionTrees. Wadsworth Brooks, Monterey, CA.Chickering, D. M., Meek, C., & Rounthwaite, R. (2001). Efficient determination dynamicsplit points decision tree. Proceedings 1st IEEE International Conference28fiAnytime Induction Low-cost, Low-error ClassifiersData Mining (ICDM-2001), pp. 9198, Washington, DC, USA. IEEE ComputerSociety.Davis, J. V., Ha, J., Rossbach, C. J., Ramadan, H. E., & Witchel, E. (2006). Cost-sensitivedecision tree learning forensic classification. Proceedings 17th EuropeanConference Machine Learning (ECML-2006), pp. 622629, Berlin, Germany.Demsar, J. (2006). Statistical comparisons classifiers multiple data sets. JournalMachine Learning Research, 7, 130.Domingos, P. (1999). Metacost: general method making classifiers cost-sensitive.Proceedings 5th International Conference Knowledge Discovery DataMining (KDD1999), pp. 155164.Elkan, C. (2001). foundations cost-sensitive learning. Proceedings 17thInternational Joint Conference Artificial Intelligence (IJCAI-2001), pp. 973978,Seattle, Washington, USA.Esmeir, S., & Markovitch, S. (2006). decision tree learner plenty time.Proceedings 21st National Conference Artificial Intelligence (AAAI-2006),Boston, MA, USA.Esmeir, S., & Markovitch, S. (2007a). Anytime learning decision trees. Journal MachineLearning Research, 8.Esmeir, S., & Markovitch, S. (2007b). Occams razor got sharper. Proceedings20th International Joint Conference Artificial Intelligence (IJCAI-2007),Hyderabad, India.Fan, W., Lee, W., Stolfo, S. J., & Miller, M. (2000). multiple model cost-sensitive approachintrusion detection. Proceedings 11th European Conference MachineLearning (ECML-2000), pp. 142153, Barcelona, Catalonia, Spain.Frank, E. (2000). Pruning Decision Trees Lists. Ph.D. thesis, Department ComputerScience, University Waikato.Good, I. (1965). Estimation Probabilities: Essay Modern Bayesian Methods.MIT Press, USA.Greiner, R., Grove, A. J., & Roth, D. (2002). Learning cost-sensitive active classifiers.Artificial Intelligence, 139 (2), 137174.Hansen, E. A., & Zilberstein, S. (2001). Monitoring control anytime algorithms:dynamic programming approach. Artificial Intelligence, 126 (1-2), 139157.Hastie, T., Tibshirani, R., & Friedman, J. (2001). Elements Statistical Learning:Data Mining, Inference, Prediction. New York: Springer-Verlag.Hyafil, L., & Rivest, R. L. (1976). Constructing optimal binary decision trees NPcomplete. Information Processing Letters, 5 (1), 1517.Lachiche, N., & Flach, P. (2003). Improving accuracy cost two-class multiclass probabilistic classifiers using roc curves. Proceedings 20th InternationalConference Machine Learning (ICML-2003).29fiEsmeir & MarkovitchLenert, L., & Soetikno, R. (1997). Automated computer interviews elicit utilities: Potential applications treatment deep venous thrombosis. American MedicalInformatics Association, 4 (1), 4956.Ling, C. X., Yang, Q., Wang, J., & Zhang, S. (2004). Decision trees minimal costs.Proceedings 21st International Conference Machine Learning (ICML-2004).Lizotte, D. J., Madani, O., & Greiner, R. (2003). Budgeted learning naive bayes classifiers.Proceedings 19th Conference Uncertainty Artificial Intelligence (UAI2003), Acapulco, Mexico.Margineantu, D. (2005). Active cost-sensitive learning. Proceedings 19th International Joint Conference Artificial Intelligence (IJCAI-2005), Edinburgh, Scotland.Melville, P., Saar-Tsechansky, M., Provost, F., & Mooney, R. J. (2004). Active feature acquisition classifier induction. Proceedings 4th IEEE International ConferenceData Mining (ICDM-2004), pp. 483486, Brighton, UK.Norton, S. W. (1989). Generating better decision trees. Sridharan, N. S. (Ed.), Proceedings Eleventh International Joint Conference Artificial Intelligence, pp.800805, Detroit, Michigan, USA.Nunez, M. (1991). use background knowledge decision tree induction. MachineLearning, 6, 231250.Page, D., & Ray, S. (2003). Skewing: efficient alternative lookahead decisiontree induction. Proceedings 18th International Joint Conference ArtificialIntelligence (IJCAI-2003), Acapulco, Mexico.Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., & Brunk, C. (1994). Reducingmisclassification costs: knowledge intensive approaches learning noisy data.Proceedings 11th International Conference Machine Learning (ICML1994).Provost, F., & Buchanan, B. (1995). Inductive policy: pragmatics bias selection.Machine Learning, 20 (1-2), 3561.Qin, Z., Zhang, S., & Zhang, C. (2004). Cost-sensitive decision trees multiple costscales. Lecture Notes Computer Scienc, AI 2004: Advances Artificial Intelligence,Volume 3339/2004, 380390.Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81106.Quinlan, J. R. (1993). C4.5: Programs Machine Learning. Morgan Kaufmann, SanMateo, CA.Sheng, S., Ling, C. X., Ni, A., & Zhang, S. (2006). Cost-sensitive test strategies.Proceedings 21st National Conference Artificial Intelligence (AAAI-2006),Boston, MA, USA.Sheng, S., Ling, C. X., & Yang, Q. (2005). Simple test strategies cost-sensitive decisiontrees. Proceedings 9th European Conference Machine Learning (ECML2005), pp. 365376, Porto, Portugal.30fiAnytime Induction Low-cost, Low-error ClassifiersSheng, V. S., & Ling, C. X. (2007). Roulette sampling cost-sensitive learning.Proceedings 18th European Conference Machine Learning (ECML-2007),pp. 724731, Warsaw, Poland.Tan, M., & Schlimmer, J. C. (1989). Cost-sensitive concept learning sensor use approach recognition. Proceedings 6th International Workshop MachineLearning, pp. 392395, Ithaca, NY.Turney, P. (2000). Types cost inductive concept learning. ProceedingsWorkshop Cost-Sensitive Learning held 17th International ConferenceMachine Learning (ICML-2000), Stanford, CA.Turney, P. D. (1995). Cost-sensitive classification: Empirical evaluation hybrid geneticdecision tree induction algorithm. Journal Artificial Intelligence Research, 2, 369409.Vadera, S. (2005). Inducing cost-sensitive non-linear decision trees. Technical report 03-052005, School Computing, Science Engineering, University Salford.Zadrozny, B., Langford, J., & Abe, N. (2003). Cost-sensitive learning cost-proportionateexample weighting. Proceedings 3rd IEEE International Conference DataMining (ICDM-2003), Melbourne, Florida, USA.Zhu, X., Wu, X., Khoshgoftaar, T., & Yong, S. (2007). empirical study noiseimpact cost-sensitive learning. Proceedings 20th International Joint Conference Artificial Intelligence (IJCAI-2007), Hyderabad, India.31fiJournal Artificial Intelligence Research 33 (2008) 575-613Submitted 4/08; published 12/08Value CorrelationItai Ashlagiiashlagi@hbs.eduHarvard Business School,Harvard University,Boston, MA, 02163,USADov Mondererdov@ie.technion.ac.ilFaculty Industrial Engineering Management,Technion - Israel Institute Technology,Haifa 32000, IsraelMoshe Tennenholtzmoshet@microsoft.comMicrosoft Israel R&D Center,13 Shenkar St., Herzeliya 46725, Israel,Faculty Industrial Engineering Management,Technion - Israel Institute Technology,Haifa 32000, IsraelAbstractCorrelated equilibrium generalizes Nash equilibrium allow correlation devices. Correlated equilibrium captures idea many systems exists trusted administrator recommend behavior set agents, enforce behavior.makes solution concept appropriate study multi-agent systemsAI. Aumann showed example game, correlated equilibrium gameagents welfare (expected sum players utilities) greater welfaremixed-strategy equilibria. Following idea initiated price anarchy literaturesuggests study two major measures value correlation gamenonnegative payoffs:1. ratio maximal welfare obtained correlated equilibrium maximalwelfare obtained mixed-strategy equilibrium. refer ratio mediationvalue.2. ratio maximal welfare maximal welfare obtained correlated equilibrium. refer ratio enforcement value.work initiate study mediation enforcement values, providingseveral general results value correlation captured concepts. alsopresent set results specialized case congestion games , class gamesreceived lot attention recent literature.1. IntroductionMuch work area multi-agent systems adopts game-theoretic reasoning.due fact many existing systems consist self-motivated participants,attempts optimize performance. result Nash equilibrium,central solution concept game theory, become major tool study analysismulti-agent systems. Nash equilibrium captures multi-agent behavior stablec2008AI Access Foundation. rights reserved.fiAshlagi, Monderer, & Tennenholtzunilateral deviations. Naturally, system fully controlled designer enforcebehaviors lead higher welfare one obtained fully decentralized systemagents behave selfishly follow Nash equilibrium. comparisonquantities studied title work price anarchy (Koutsoupias &Papadimitriou, 1999; Roughgarden & Tardos, 2002; Christodoulou & Koutsoupias, 2005),subject much interest computer science. However, fully controlled systemsversus fully uncontrolled systems two extreme points. acknowledged variousworks AI (Shoham & Tennenholtz, 1995a, 1995b) one main practical approachesdealing realistic systems consider systems limited centralized control.Indeed, realistic systems designer recommend behavior;distinguished strong requirement designer dictate behavior.Correlated equilibrium, introduced Aumann (1974), famous game-theoreticsolution concept referring designer recommend enforce behavior.game strategic form, correlated strategy probability distribution setstrategy profiles, strategy profile vector strategies, one player.correlated strategy utilized follows: strategy profile selected accordingdistribution, every player informed strategy profile. selectedstrategy player interpreted recommendation play. Correlated strategiesnatural, since capture idea system administrator/reliable partyrecommend behavior enforce it. Hence, correlated strategies make perfect sensecontext congestion control, load balancing, trading, etc. correlated strategycalled correlated equilibrium better every player obey recommendedstrategy believes players obey recommended strategies1 . Correlatedequilibrium makes perfect sense context work multi-agent systems AIexists mediator recommend behavior agents.2 major potentialbenefit mediator using correlated equilibrium attempt improvewelfare selfish players. paper, welfare obtained correlated strategydefined expected sum utilities players, referredwelfare obtained correlated strategy.striking example introduced Aumanns seminal paper (1974) two-playertwo-strategy game, welfare obtained correlated equilibrium higherwelfare obtained every mixed-strategy equilibrium game. modificationAumanns example serves us motivating example.Aumanns Example:1. Every correlated strategy defines Bayesian game private signal every playerrecommended strategy. correlated equilibrium obeying recommended strategy everyplayer pure-strategy equilibrium Bayesian game.2. use mediators obtaining desired behaviors, addition improving social welfare,studied, (e.g., Monderer & Tennenholtz, 2004, 2006; Rozenfeld & Tennenholtz, 2007; Ashlagi,Monderer, & Tennenholtz, 2008). However, mediators discussed work makes usepowerful capabilities making recommendation based probabilistic coin flips.576fiOn Value Correlationb1b2a1 501a240145result, Aumanns example suggests correlation may way improvewelfare still assuming players rational classical game-theoretic sense.3game, three mixed-strategy equilibrium profiles. Twoobtained pure strategies, (a1 , b1 ), (a2 , b2 ). welfare purestrategy equilibrium profiles equals six. additional mixed-strategy equilibriumevery player chooses strategies equal probabilities. welfareobtained profile equals 5 (= 14 (6 + 0 + 8 + 6)) every entry matrixplayed probability 41 . Hence, maximal welfare mixed-strategy equilibriumequals 6. Consider following correlated strategy: probability 1/3 assignedevery pure strategy profile (a1 , b2 ). correlated strategy correlated equilibrium.Indeed, row player recommended play a1 knows playerrecommended play b1 , therefore strictly prefers play a1 . rowplayer recommended play a2 conditional probability columns half,therefore weakly prefers play a2 . Similar argument applied column playershows correlated strategy indeed correlated equilibrium. welfare associated1correlated equilibrium equals 203 (= 3 (6 + 8 + 6)).discussion suggests one may wish consider value correlationgames. order address challenge studying value correlation, tackle twofundamental issues:much society/system gain adding correlation device,assume without device agents play mixed-strategy equilibrium.much society/system loose fact correlation devicerecommend (and enforce) course action?Accordingly introduce two measures, mediation value enforcement value.measures make sense mainly games nonnegative utilities, focuspaper.mediation value measures ratio maximal welfare correlatedequilibrium maximal welfare mixed-strategy equilibrium. Notice highermediation value is, correlation helps. Hence, mediation value measuresvalue reliable mediator recommend play modelanarchy without mediator, anarchy defined situationplayers use welfare-best mixed-strategy equilibrium, is, anarchy bestoutcome reached rational selfish agents.43. advantages purely computational ones. recently shown, correlated equilibriumcomputed polynomial time even structured representations games (Kakade, Kearns,Langford, & Ortiz, 2003; Papadimitriou, 2005).4. phenomenon multiple equilibria forces modeling choice concept anarchy, coulddefined also welfare-worst mixed-strategy equilibrium, convex combination577fiAshlagi, Monderer, & TennenholtzAummans example shown correlated equilibrium introducedbest correlated equilibrium, i.e., attains maximal welfare among correlatedequilibria game. Hence, mediation value Aumanns example 109 .enforcement value measures ratio maximal welfare maximalwelfare correlated equilibrium. is, value center dictatecourse play respect mediator use correlation devices equilibrium.maximal welfare Aumanns example 8, enforcement value game equals65.paper establish general basic results concerning measures definedabove. consider mediation (enforcement) value classes games, definedleast upper bound mediation (enforcement) values games class.first study general games. consider important class congestion games(Rosenthal, 1973; Monderer & Shapley, 1996). Indeed, class games perhapsapplicable game theory CS synergy. particular, results regardingprice anarchy obtained congestion games. restrict study simplecongestion games.Next summarize main results discuss related literature.1.1 Main Results General GamesAumanns example implies mediation value class two-player two-strategy(22) games least 10/9. paper proved mediation value classequals 4/3. Next, complex games studied. particular consider two possibleminimal extensions 2 2 games: Two-player games three strategies oneplayers two strategies other, three-player games two strategiesplayer. shown mediation value classes unbounded, i.e.,equals . Consequently, mediation value unbounded classes larger games.5interpreted positive result, showing extreme power correlation.Considering enforcement value, proved equals classes 2 2larger games. proof result uses games weakly dominant strategies.show, however, enforcement value class three-player two-strategy gameswithout weakly dominated strategies also equals .1.2 Main Results Simple Congestion Gamessimple congestion game set facilities. Every facility j associatednonnegative payoff function wj . Every player chooses facility, say facility j, receiveswj (k), k number players chose facility j.completeness first deal simple case, exist two players,show mediation value class simple congestion games two facilitiesequals 4/3. general case, 2 facilities, provedmediation value bounded 2. However, proved mediationbest-welfare worst-welfare mixed strategy equilibrium .This choice matter taste,chose best option.5. game larger game obtained adding players, and/or adding strategiesplayers .578fiOn Value Correlationvalue equals 1 class simple congestion games non-increasing facility payofffunctions.case two players, show mediation value unboundedclass games three players two facilities non-increasing payoffs.contrast,linear case, proved mediation value bounded5+11.618 class games number players two facilities2non-increasing linear payoff functions. give example game class whosemediation value equals 9/8, leaving open significant gap.Additional special theorems proved simple congestion games symmetric(identical) facilities; proved that, every n 4, mediation value higher 1class two symmetric (identical) facilities non-increasing payoffs n players.illustrates power correlation. Nevertheless, show every simplecongestion game number players number symmetric facilities,facility payoff functions satisfy certain concavity requirement, best mixedstrategy equilibrium obtains maximal welfare, therefore mediation valueenforcement value game equal 1.Finally, study enforcement value case, exist n playerssymmetric facilities arbitrary facility payoff functions. characterize setgames enforcement value equals 1, result, determine situationscorrelation allows obtaining maximal welfare.1.3 Related Literatureend introduction discussion relevant issues price anarchyliterature, potential relationships concepts mediation value enforcement value.6 many situations natural deal nonnegative costs ratherutilities; indeed, literature price anarchy focused models.translating definition price anarchy7 games utilities costs,price anarchy utilities ratio maximal welfare minimalwelfare obtained mixed-strategy equilibrium. higher number is, valuecenter higher, center enforce course play. Hence, price anarchyutilities measures value center respect anarchy, centerdictate play, anarchy measured worst social outcome reached rational selfish agents. Recently, Anshelevich, Dasgupta, Kleinberg, Tardos, Wexler,Roughgarden (2004) defined price stability models costs.8 Accordingly, pricestability utilities, ratio maximal welfare maximal welfaremixed-strategy equilibrium. relevant concept using correlated equilibrium modelscosts defined independently Christodoulou Koutsoupias (2005),referred price stability correlated equilibria.9 translated model6. concept price anarchy received much attention recent computer science literature,(e.g., Marvonicolas & Spirakis, 2001; Czumaj & Vocking, 2002; Roughgarden, 2002; Roughgarden &Tardos, 2002).7. price anarchy defined games costs ratio maximal cost mixedstrategy equilibrium minimal cost.8. ratio minimal cost mixed strategy equilibrium minimal cost.9. ratio minimal cost correlated equilibrium minimal cost.579fiAshlagi, Monderer, & Tennenholtzutilities, price stability correlated equilibria utilities ratiomaximal welfare maximal welfare mixed-strategy equilibrium,enforcement value.10 However, results proved one ratios one modelscannot translated results analogous ratio model. duefact moving one model require multiplicationnegative constant, e.g. -1, numbers also need shifted remain nonnegative;needless say, corresponding ratios significantly changed shifting.11return discussion Section 4.1.2. Basic Definitionsfinite game strategic form tuple = (N, (S )iN , (ui )iN ); N nonempty finiteset players. Unless otherwise specified assume N = {1, 2, ...., n}, n 1.N , finite set strategies player i. Let = 1 2 nset strategy profiles (n-tuples). element = (si )iN . Nlet si = (s1 , ..., si1 , si+1 , ...sn ) denote strategies played everyone i. Thus= (si , si ). player N , let ui : R utility function player i. ui (s)utility player profile strategies played. called nonnegativegame utilities players nonnegative, i.e., ui : R+ every player i.player also randomize among strategies using mixed strategy - distribution set strategies. finite set C, (C) denotes set probabilitydistributions C. Thus P = (S ) set mixed strategies player i. Let pi Pmixed strategy i. every si , pi (si ) probability player playsstrategy si pi . Every pure strategy si is, natural identification, mixedstrategy psi P1 ti =psi (t ) =0 ti 6= si .psi called pure strategy, si interchangeably called strategy pure strategy(when identified psi ). Let P = P 1 P 2 P n set mixed strategyprofiles.Let si , ti pure strategies player i. say si weakly dominates ti , tiweakly dominated si si Siui (si , si ) ui (ti , si ),least one inequality strict. say si strictly dominates ti , ti strictlydominated si inequalities strict. ui (si , si ) = u(ti , si )si Si , say si ti equivalent strategies player i.(S) called correlated strategy. Every mixed strategy profile p Pinterpreted correlated strategy p , every strategy profile S,10. completeness, one define mediation value costs ratio minimal costmixed-strategy equilibrium minimal cost correlated equilibrium.11. Interestingly, shown exist classes cost games price anarchybounded, price anarchy utilities analogous classes utility games unbounded.class cost games analogous class utility games given Example 2 Section 4.1.2,constitutes one example.580fiOn Value CorrelationQp (s) , ni=1 pi (si ). Whenever necessary identify p p . slight abusenotation, every (S), denote ui () expected utility playercorrelated strategy (S) played, is:Xui () =ui (s)(s).(1)sSNaturally, every p P denote ui (p) = ui (p ). Hence ui (p) expected utilityplayer mixed strategy profile p played.say p P mixed-strategy equilibrium ui (pi , pi ) ui (pi , q ) everyplayer N every q P . Let p P mixed-strategy equilibrium. everyi, pi pure strategy, also call p pure-strategy equilibrium.Definition 1 (Aumann, 1974, 1987) correlated strategy (S) correlated equilibrium N si , ti :X(si , si )[ui (si , si ) ui (si , ti )] 0.(2)si SiConsider third party picks randomly pure-strategy profile respectwell-known correlated strategy , recommends privately every player play si .left hand side (2) captures difference expected utility playing si , i.e.,following recommendation third party, playing pure-strategy tigiven players follow recommendations. Hence, differencenonnegative, player better playing si .well-known easily verified every mixed-strategycorrelatedPn equilibrium(). value u()equilibrium. every correlated strategy , let u() ,ui=1called welfare . Let N () set mixed-strategy equilibria letC() set correlated equilibria . define vC () vN () follows:vC () , max{u() : C()},vN () , max{u(p) : p N ()}.Note vN () vC () well defined due compactness N () C()respectively, continuity u. Define opt() (the maximal welfare) follows:opt() , max{u() : (S)} = max{u(s) : S}.mediation value nonnegative game defined follows:V () ,vC ().vN ()vN () = 0 vC () = 0, define V () 1. vN () = 0 vC () > 0,V () defined . Denote EV () enforcement value nonnegative game. is,opt()EV () ,.vC ()581fiAshlagi, Monderer, & TennenholtzvC () = 0 opt() = 0, define EV () 1. vC () = 0 opt() > 0,EV () defined . Finally, class nonnegative games A, definemediation value enforcement value class follows:V (A) , sup V ();EV (A) , sup EV ().One tools need paper linear programming. gamestrategic form, C() exactly set feasible solutions following linear programPb. Moreover, C() optimal solution Pb u() = vC ().Pmax sS (s)u(s)s.t.bP : (s) 0S,PsS (s) = 1,Psi Si (s)[u (t , ) u (s , )] 0 N, (s , ) , 6= .dual problem Pb one decision variable constraintprimal. LetPdenote dual variable associated primal constraint sS (s) = 1. Let (ti |si )denote dual variable associated primal constraint defined (si , ti ), is,constraintX(s)[ui (ti , si ) ui (si , si )] 0,si Silet = (i )iN , = (i (ti |si ))(si ,ti )S , si 6=ti . dual problemwritten follows:mins.t.b:(ti |si ) 0N, (si , ti ) , ti 6= si ,PPS.{ti | ti 6=si } (t |s )[u (t , ) u (s , )] + u(s)b feasible bounded, objectivewell known problems Pbvalues equal vC (). feasibility consequence existence mixed-strategyequilibrium proved Nash (1951), fact every mixed-strategy equilibriumalso correlated equilibrium.12also make use following notation definitions. Let G classnonnegative games strategic form. m1 , m2 , ..., mn 1 denote Gm1 m2 mn Gclass games n players |S | = mi every player i.3. Results General Gamesfollowing two basic lemmas used proofs paper. proofLemma 1 follows directly Definition 1, proof Lemma 2 standard.Therefore, proofs omitted.12. elementary proof existence correlated equilibrium, use existence mixedstrategy equilibrium given Hart Schmeidler (1989).582fiOn Value CorrelationLemma 1 Let game strategic form. Let si weakly dominatedti , let si Si . (s) = 0 every correlated equilibriumui (ti , si ) > ui (si , si ).Consequently, si strictly dominated strategy, (s) = 0 every correlated equilibrium.Next define extensions game adding dummy strategy one players,adding dummy player. Let Gm1 m2 mn . game Gm1 mi1 (mi +1)mi+1 mnextension adding dummy strategy player obtained addingstrategy player utility n players equal zero, player usesnew strategy.game Gm1 m2 mn 1 extension adding dummy player, playern + 1, obtained adding player n + 1 single strategyutilities player zeros, utility n players remain . is,ui (s, d) = ui (s) every S, denotes unique added strategy player n + 1.game trivial extension sequence games,= 0 , 1 , , =k obtained k1 adding dummy player dummy strategy.Lemma 2 Let trivial extension . Then, V () = V () EV () =EV ().3.1 Mediation Valuesection show power correlation general games. start extendingAumanns result power correlation 2 2 games.3.1.1 Two-person two-strategy gamesAumanns example shows mediation valueshow:109obtained 2 2 game.Theorem 1 V (G22 ) = 43 .following lemma needed proof Theorem 1:Lemma 3 (Peeters & Potters, 1999) Let G22 . exist correlated equilibrium, induced mixed-strategy equilibrium, least two pure-strategyequilibria.give proof Theorem 1 need following technical remarks, holdsrest paper:Remark: games introduced figures, denote player 1,2,3 row,column, matrix (if exist) players respectively. strategy profile playersutilities given left right player utility ith left payoff.Proof Theorem 1: begin showing V (G22 ) 34 . Figure 1 describesarbitrary game G22 .583fiAshlagi, Monderer, & Tennenholtz1a2b1a, bm, nb2j, kc,Figure 1Lemma 3, less two pure-strategy equilibrium profiles, V () = 1.Therefore, assume without loss generality two pure-strategyequilibrium profiles. However, four pure-strategy equilibrium profiles, mediation value equals 1. Therefore discuss cases either twothree pure-strategy equilibrium profiles.Suppose possesses exactly three pure-strategy equilibrium profiles, without lossgenerality let (a2 , b2 ) strategy profile equilibrium. Since (a1 , b1 ), (a2 , b1 )(a1 , b2 ) pure-strategy equilibria, = b = k. (a2 , b2 )equilibrium, c < j < n. c < j, Lemma 1, every correlated equilibrium C()satisfies (a2 , b2 ) = 0, therefore V () = 1. Similarly, < n, (a2 , b2 ) = 0 impliesV () = 1.Suppose exactly two pure-strategy equilibrium profiles . two equilibrium profiles may row, column, diagonal. Obviously,proof case two pure-strategy equilibrium profiles rowcolumn covered following proof, assumes two purestrategy equilibrium profiles first row. is, (a1 , b1 ) (a1 , b2 )pure-strategy equilibria. assumption implies b = k. Observe strategyprofile player 1 plays strategy a1 probability one, player 2 playsmixed strategy p2 P 2 , mixed-strategy equilibrium. Since exactly two purestrategy equilibria, must < c < j. < c < j, Lemma 1 everycorrelated equilibrium C() satisfies (a2 , b1 ) = 0 (a2 , b2 ) = 0. ThereforeV () = 1. Suppose = a. Therefore c < j. Hence, Lemma 1 every correlatedequilibrium C() satisfies (a2 , b2 ) = 0. Since (a2 , b1 ) pure-strategy equilibrium, n < d. Since b = k n < d, Lemma 1, every correlated equilibriumC() satisfies (a2 , b1 ) = 0. Therefore V () = 1. showed V () = 1two pure-strategy equilibrium profiles, rowcolumn.proceed last case two pure-strategy equilibrium profilesdiagonal. Without loss generality let (a1 , b1 ) (a2 , b2 ) pure-strategy equilibriumprofiles. shown Peeters Potters (1999) c = j = b = k= n, C() exactly convex hull N (). Hence, case extremepoint C() mixed-strategy equilibrium, therefore mediation valueequals 1. Therefore assume:c > j, > m, b > k> n.(3)both, u(a1 , b2 ) u(a2 , b1 ) smaller max{u(a1 , b1 ), u(a2 , b2 )}, V () = 1proof completed. Therefore, without loss generality make following twoassumptions:(A1) u(a1 , b1 ) u(a2 , b2 ). is, + b c + d.584fiOn Value Correlation(A2) u(a2 , b1 ) u(a2 , b2 ). is, + n c + d.Hence, set mixed-strategy equilibria is:N () = {((1, 0), (1, 0)), ((0, 1), (0, 1)), ((11,), (,))},1+ 1+ 1+ 1+bk=cj = dn . Note (3), positive.continuing proof need following geometric characterizationC(). Peeters Potters (1999), C() polyhedron following five extremepoints , = 1, ..., 5:!11, 00, 0(1+)(1+) , (1+)(1+),1 =, 2 =, 3 =0, 00, 1, (1+)(1+)(1+)(1+)!!110(1++) , (1++)(1++) ,4 =, 5 =,0,(1++(1++) , (1++)(j, k) denotes probability given strategy profile (aj , bk ) correlated equilibrium . is, (j, k)th entry (aj , bk ). agreementidentify mixed-strategy profiles correlated strategies observe setmixed-strategy equilibrium preciselyN () = {1 , 2 , 3 }.(4)prove u() 43 vN () every correlated equilibrium C().sufficient prove inequality holds extreme points C(). Since 1 , 23 mixed-strategy equilibria, u(i ) vN () = 1, 2, 3. Therefore sufficesprove inequality 4 5 .first derive couple inequalities useful us. (A1) sinceutilities nonnegative, c + c + d. Therefore, since < n < d,obtain inequality+ n 2(c + d).(5)Since (a1 , b1 ) (a2 , b2 ) pure-strategy equilibrium profiles, + n +j + k c + b. (A2) since + n + d, c a. Hence j + k + b. Therefore,(A1) obtainj + k c + d.(6)Note inequality (6) implies u(4 ) vN () since (a2 , b2 ) pure-strategyequilibrium.remains show u(5 ) vN (). (A1), u(1 ) u(2 ). distinguishfollowing two cases:Case 1: u(3 ) u(2 ).(A1), u(1 ) u(2 ). Therefore, (4), vN () = u(3 ).Hence,+ b + (m + n) + (c + d)1 + + +u(5 )=.vN ()1 + ++ b + (j + k) + (m + n) + (c + d)585fiAshlagi, Monderer, & TennenholtzTherefore, (j + k) 0,u(5 )1 + + +.vN ()1 + +z > 0, let f1 (z) =Let K =profiles,m+ncdc+djk .1++z+z1++z .Hence, suffices show f1 () 43 .Since (a1 , b1 ) (a2 , b2 ) pure-strategy equilibrium+ n c c,c + j k b.Therefore (A1),K 1.Since u(3 ) c + d,+ b + (j + k) + (m + n) (c + d)(1 + + ).Therefore(m + n c d) + + b c.c+djk(7)Equation (7) (A1), K. Since, K 1, . Note f1 (z)non-decreasing z > 0. Therefore,f1 () f1 () =Since1+2+ 21++ 2=(+1)2(+1)21 + 2 + 2.1 + + 2(8)maximized > 0 = 1,4f1 () .3Case 2: u(3 ) < u(2 ).(A1), u(1 ) u(2 ). Therefore, (4), vN () = u(2 ). Therefore,u(5 )+ b + (m + n) + (c + d)=.vN ()(1 + + )(c + d)z > 0 let f2 (z) =a+b+(m+n)+(c+d)z.(1++z)(c+d)(9)Hence, suffices show f2 () 43 .case inequality (7) reversed.(m + n c d) + + b c,c+djk(m + n c d) + + b c.c+dsince j, k 0,(10)distinguish following two cases, noticing f2 (x) non-increasingx > 0:586fiOn Value CorrelationCase 2.1: + b = c + d: case, f2 non-increasing (10) holds,f2 ()c + + (m + n) + (m + n c d) 2(1 + +(m+ncd) 2)(cc+d+ d).(11)Set x = c + d. Therefore, (A2), exists 1 2 tx = + n.(11)f2 ()1 + + (t 1) 24x + tx + (t 1)x 2=,22x + x + (t 1)x1 + + (t 1)3(12)last inequality follows similar arguments following (8).Case 2.2: + b < c + d:Set x = + b. Therefore (A1), (A2) (5), > 11 k 2,c + = tx + n = ktx.(13)Hence,kt =a+b+c+dm+n= + 1,a+ba+binequality follows since + n + b + c + d. Therefore1.k1(14)Thereforef2 ()+ b + (m + n) + (c + d)[ (m+ncd)+a+bcd]c+d(1 + +(m+ncd)+a+bcd)(cc+d+ d)=1 + kt + t(k 1) 2 +1 + + t( + 1)(k 1)=2+ + t(k 1) ++ 2 (k 1) +(15)1 + 2 + 2( + 1)24,1 + 2 (k 1) +( + 1)23(16)right inequality (15) follows (14), left inequality (16) follows since 1 k 2, right inequality (16) follows since maximum(+1)2value (+1)2 attained = 1.showed mediation value bounded 43 . remains showbound tight. show family games mediation value approaches43 bound. Consider family games x shown Figure 2 (a variant Aumannsexample), x > 1.587fiAshlagi, Monderer, & Tennenholtz1a2b1x,1x-1,x-1b20,01,xFigure 2game strategy profiles (a1 , b1 ) (a2 , b2 ) pure-strategy equilibriumprofiles, u(a1 , b1 ) = u(a2 , b2 ) = x+1. exists one mixed-strategy equilibrium,player assigns probability 0.5 strategies, yieldswelfare lower x + 1. correlated strategy (S), strategyprofiles (a1 , b1 ) ,(a2 , b1 ) (a2 , b2 ) played equal probability 1/3 correlated4x4equilibrium u() = 4x3 . Hence V (x ) 3(x+1) . Therefore V (x ) 3x . 23.1.2 General GamesTheorem 1 shows mediation value class nonnegative 2 2 games finite.next theorem show mediation value unbounded move slightlybeyond 2 2 games. particular, one players 2-player game leastthree strategies, remains two strategies, mediation value alreadyunbounded. Similarly, three players two strategies, mediationvalue unbounded. 13Theorem 2 V (Gm1 m2 ) = every m1 , m2 2 max(m1 , m2 ) 3.Proof: Lemma 2 suffices prove V (G32 ) = . Let x, followingparametric G23 game Figure 3,1a2b1x, 10, zb2z, 1z 1, z 1b30, 01, zFigure 3z > 2 fixed, x > z 0 < < 0.5.first showN (x, ) = {((1, 0)(0, 1, 0)), ((0, 1), (0, 0, 1)), ((, 1 ), (1x, 0,))}.1+x1+xstandard check profile N (x, ) indeed mixed-strategy equilibrium,player 1 plays pure-strategy equilibrium, mixed-strategy equilibrium((1, 0)(0, 1, 0)) ((0, 1), (0, 0, 1)). show player 1 plays fully mixed1x(that is, assigns positive probabilities a1 a2 .) , ((, 1 ), ( 1+x, 0, 1+x))mixed-strategy equilibrium. Indeed, note player 2 plays pure strategy, player1 never indifferent a1 a2 therefore player 1 always better deviatingpure strategy. player 2 assigns positive probability b1 b2 , player 1better playing a1 . player 2 assigns positive probability b2 b3 ,13. results presented paper showing mediation value may , also establishedassume utilities uniformly bounded, e.g. utilities interval [0, 1].588fiOn Value Correlationmixed-strategy player 1 play order player 2 indifferent b2 b3(1/2, 1/2), player 2 better playing b1 . Similarly, shownexist equilibrium player 1 fully mixes player 2 assigns positiveprobabilities pure strategies. Suppose player 2 assigns positive probabilities1b1 b3 . probability player 2 assigns b1 higher (lower) 1+x,1x, 0, 1+x) equilibriumplayer 1 better playing a1 (a2 ). Hence, player 2 plays ( 1+xplayer 1 fully mixes, therefore standard prove player 1 must play(, 1 ).Next show vN (x, ) z + 2 small enough . Notice welfarepure-strategy equilibria z + 1. welfare mixed-strategy equilibrium ((, 11x), ( 1+x, 0, 1+x) is:(x + 1 ) + (1 )(z ) + x(1 )(z + 1)=x+1z z +x.x+1xNote z z + x+1z + 1 x 0.proceed show net games, x, vC (x, ) 0 x .Let correlated strategy. correlated equilibrium x, following9 inequalities satisfied (in brackets relate inequality correspondinginequality (2)):1. (a1 , b1 )x + (a1 , b2 ) (a1 , b3 ) 0.(i = 1, si = a1 , ti = a2 )2. (a2 , b3 ) (a2 , b2 ) x(a2 , b1 ) 0.(i = 1, si = a2 , ti = a1 )3. (a1 , b1 )(1)(a2 ,b1 ).4. (a1 , b1 )(a2 ,b1 )1 .5. (a1 , b2 )(1)(a2 ,b2 ).(i = 2, si = b1 , ti = b3 )6. (a1 , b2 ) (a2 , b2 ).7. (a2 , b3 )(i = 2, si = b1 , ti = b2 )(1)(a1 ,b3 ).(i = 2, si = b2 , ti = b1 )(i = 2, si = b2 , ti = b3 )(i = 2, si = b3 , ti = b1 )8. (a2 , b3 ) (a1 , b3 ). (i = 2, si = b3 , ti = b2 )P2 P3j9.i=1j=1 (a , b ) = 1.Set (a1 , b1 ) = P, (a2P, b1 ) = 22 , (a1 , b2 ) = (1 ), (a1 , b3 ) = (a2 , b2 ) = 2 , (a2 , b3 ) =2131 (a , b ) i=1 2j=1 (ai , bj ) let x = 412 . Let 0. nine constraintssatisfied small enough . Note lim0 x(a1 , b1 ) = . Since V (x, ) x(a1 , b1 )obtain V (x, ) 0. 2Theorem 3 V (Gm1 mn ) = every n 3 every m1 , m2 , . . . , mn 2.589fiAshlagi, Monderer, & TennenholtzProof: Lemma 2 suffices prove V (G222 ) = . Consider followingthree player game (Figure 4):a1a2b10, 0, 00, 0, 0b22, 0, 01, 0, 1a1a2c1b14, 4, 05, 0, 0b20, 0, 10, 3, 0c2Figure 4show vN () = 0, is, welfare every mixed-strategy equilibriumzero. addition construct correlated equilibrium, yields strictlypositive welfare. begin proving vN () = 0. Note pure-strategyequilibrium profiles game (a1 , b1 , c1 ) (a2 , b1 , c1 ). Obviously, every strategyprofile players 2 3 play b1 c1 respectively, player 1 plays mixedstrategy, mixed-strategy equilibrium. next show mixedstrategy equilibria game. First show mixed-strategyequilibria least one players plays pure strategy. verifyplayer:1. Assume player 3 plays c2 probability one. 1 > p2 (b1 ) > 0, p1 (a2 ) = 1,player 3 better playing c1 . p2 (b1 ) = 1, p1 (a2 ) = 1, player 2 betterplaying b2 . p2 (b2 ) = 1, player 1 indifferent. p1 (a1 ) 0.5, player 2 betterplaying b1 . p1 (a1 ) < 0.5, player 3 better playing c1 .2. Assume player 3 plays c1 probability one. p2 (b2 ) > 0, p1 (a1 ) = 1,player 3 better playing c2 .3. Assume player 2 plays b1 probability one. Player 3 indifferentstrategies. p3 (c2 ) > 0, p1 (a2 ) = 1, player 2 better playing b2 . p3 (c1 ) =1 dealt previous case. Assume player 2 plays b2 probability one.p3 (c1 ) > 0, p1 (a1 ) = 1, player 3 better playing c2 .4. Assume player 1 plays a1 probability one. p2 (b2 ) > 0, p3 (c2 ) = 1,player 2 better playing b1 probability one. Assume player 1 plays a2probability one. p3 (c2 ) > 0, p2 (b2 ) = 1, player 3 better playing c1probability one.Next prove exist completely mixed-strategy equilibrium, (an equilibrium every player assigns positive probability strategies). Supposenegation case. Let ((p, 1 p), (q, 1 q), (h, 1 h)) completelymixed-strategy equilibrium, 1 > p, q, h > 0. equilibrium properties, player2 indifferent b1 b2 given players 1 3 play (p, 1 p) (h, 1 h)respectively. Hence 4p(1 h) = 3(1 p)(1 h), implies p = 37 . Similarly, player3 indifferent c1 c2 given players 1 2 play (p, 1 p) (q, 1 q)respectively. Therefore (1 p)(1 q) = p(1 q) yielding p = 0.5, contradiction.Therefore, exist completely mixed-strategy equilibrium.proved vN () = 0. remains prove exists correlated equilibriumstrictly positive welfare; imply mediation value infinity. Let590fiOn Value Correlation(S) following correlated strategy. (a1 , b2 , c1 ) = (a2 , b2 , c1 ) = (a1 , b1 , c2 ) =(a2 , b1 , c2 ) = 0.25 S, (s) = 0. proceed provecorrelated equilibrium. Indeed, observe following inequalities, definecorrelated equilibrium satisfied (as usual, brackets relate inequalitycorresponding inequality (2)):1. (a1 , b2 , c1 )(2 1) + (a1 , b1 , c2 )(4 5) 0.(i = 1, si = a1 , ti = a2 )2. (a2 , b2 , c1 )(1 2) + (a2 , b1 , c2 )(5 4) 0.(i = 1, si = a2 , ti = a1 )3. (a1 , b2 , c1 )(0 0) + (a2 , b2 , c1 )(0 0) 0.(i = 2, si = b2 , ti = b1 )4. (a1 , b1 , c2 )(4 0) + (a2 , b1 , c2 )(0 3) 0.(i = 2, si = b1 , ti = b2 )5. (a1 , b2 , c1 )(0 1) + (a2 , b1 , c1 )(1 0) 0.(i = 3, si = c1 , ti = c2 )6. (a1 , b1 , c2 )(0 0) + (a2 , b1 , c2 )(0 0) 0. (i = 3, si = c2 , ti = c1 )2proof Theorem 3 based showing exists three-player nonnegative game, utilities zero, yet welfare every mixed-strategyequilibrium zero. next lemma shows phenomenon happen twoplayer game.Lemma 4 Let Gnm , n, 1. vN () = 0 implies utilities zero.is, = 1, 2,ui (t1 , t2 ) = 0 t1 1 , t2 2 .Proof: proof induction total number pure strategies n + game.First note assertion holds games, Gnm n + = 2case player exactly one strategy. Let k 2, assume assertion holdsevery game, Gnm n + k.Let Gnm game n + = k + 1 vN () = 0.prove zero game. k + 1 3, exists least one playerone strategy; without loss generality, player 1 player.Let p = (p1 , p2 ) mixed-strategy equilibrium , is, p N ().vN () = 0, welfare p equals 0. is, u(p) = u1 (p) + u2 (p) = 0. utilitiesnonnegative,u1 (p) = 0 = u2 (p).(17)Let s1 arbitrary fixed strategy player 1 p1 (s1 ) > 0, let s2arbitrary fixed strategy player 2 p2 (s2 ) > 0.Claim 1: (i) u1 (t1 , s2 ) = 0 t1 1 . (ii) u2 (s1 , t2 ) = 0 t2 2 .Proof Claim 1. Let t1 1 . Since (p1 , p2 ) mixed-strategy equilibrium, u1 (t1 , p2 )P1121112112u (p , p ) = 0. Since u nonnegative, u (t , p ) = 0. Since u (t , p ) = t2 2 p2 (t2 )u1 (t1 , t2 ),u1 nonnegative, p2 (s2 ) > 0, u1 (t1 , s2 ) = 0. proves (i). (ii) similarly proved. 2Consider game G(n1)m obtained removing s1 1 ; gamestrategy set player 1 1 = 1 \{s1 } strategy set player 2 remains 2 .591fiAshlagi, Monderer, & Tennenholtzslight abuse notations utility functions also denoted u1 u2 . Claim2: utilities zero.Proof Claim 2 :Assume negation utilities zero. induction hypothesisexists mixed-strategy equilibrium (q 1 , q 2 ) N ( )u(q 1 , q 2 ) > 0.(18)Extend q 1 mixed strategy r1 player 1 defining r1 (s1 ) = 0. Obviously,ui (r1 , q 2 ) = ui (q 1 , q 2 ) = 1, 2 therefore (18) impliesu(r1 , q 2 ) > 0.(19)Since vN () = 0, (19) implies (r1 , q 2 ) mixed-strategy equilibrium . However, since r1 coincide q 1 1 , r1 (s1 ) = 0, (q 1 , q 2 ) mixed-strategy equilibrium, player 2 profitable deviation q 2 player 1 uses r1 .Therefore, player 1 must profitable deviation r1 player 2 uses q 2 .particular, player 1 must pure-strategy profitable deviation. Since (q 1 , q 2 )mixed-strategy equilibrium , potential pure-strategy profitable deviationplayer 1 s1 . Therefore,u1 (s1 , q 2 ) > u1 (r1 , q 2 ),(20)u1 (s1 , q 2 ) > 0.(21)implies particularNext show (s1 , q 2 ) N (). Since (q 1 , q 2 ) mixed strategy equilibrium ,u1 (q 1 , q 2 ) u1 (t1 , q 2 ) every t1 1 , t1 6= s1 . Since r1 (s1 ) = 0, u1 (r1 , q 2 ) u1 (t1 , q 2 )every t1 1 , t1 6= s1 . Therefore, (20), s1 best-response q 2 . (ii) Claim1, every pure-strategy player 2 best-response s1 hence, every mixedstrategy player 2 best response s1 , particular q 2 . Therefore, (s1 , q 2 )indeed mixed-strategy equilibrium . Since vN () = 0, u1 (s1 , r2 ) = 0, contradicting(21). complete proof Claim 2.2Claim 2 (ii) Claim 1, (s1 , t2 ) N () every t2 2 . Since vN () = 0,1u(s , t2 ) = 0 every t2 2 . Hence, = 1, 2, ui (s1 , t2 ) = 0 every t2 2 .ui identically zero , follows = 1, 2, ui (t1 , t2 ) = 0 every t1 1every t2 2 .23.2 Enforcement Valuenext theorem shows enforcement value unbounded even classes smallgames.Theorem 4 EV (Gm1 mn ) = every n 2 every m1 , m2 2.Proof: Lemma 2 suffices prove theorem case n = 2, m1 = 2, m2 = 2.Consider following parametric Prisoners Dilemma game x , x > 1, given Figure 5:592fiOn Value Correlation1a2b11, 10, x + 1b2x + 1, 0x, xFigure 5Lemma 1, every game unique correlated equilibrium (a1 , b1 ) whose welfare2. However every x 1, opt(x ) = 2x. Therefore EV (x ) x . 2proof Theorem 4 based constructing Prisoners Dilemma gamesparameter x. particular every player weakly dominant strategy onegames. next theorem shows even ruling weakly dominant strategies,enforcement value unbounded.Theorem 5 EV ({| G222 , player weakly dominant strategy} = .Proof: Consider family parametric games z, (Figure 6), z =1a2a14 , 4 , 44 + , 4, 4a24, 4 + , 40, 0, z1a2a1a14, 4, 4 +0, z, 012a2z, 0, 00, 0, 0a2Figure 6First observe opt(z, ) = z every 0 < 0.25. order prove resultb weak duality theorem every feasible solution (, )use dual program D.b satisfies vC (). Let x1 , x2 , x3 denote 1 (a1 |a2 ), 2 (a1 |a2 )dual problem3 (a1 |a2 ) respectively. Let y1 , y2 , y3 denote 1 (a2 |a1 ), 2 (a2 |a1 ) 3 (a2 |a1 ) respectively.dual constraints written following way (recall z = 12 ):2y1 + 2y2 + 2y3 + 12 3,4y1 4y3 2x2 + 12 + ,4y2 4y3 2x1 + 12 + ,4y1 4y2 2x3 + 12 + ,112 y1 + 4x2 + 4x3 + 2 ,112 y2 + 4x1 + 4x3 + 2 ,112 y3 + 4x1 + 4x2 + 2 ,111x1 + 2 x2 + 2 x3 + 0.2Set y1 = y2 = y3 = x1 = 0, = 1 , x2 = x3 = 412 , observe feasiblesolution every sufficiently small > 0. Note EV (z, ) z . However z = 10, completes proof. 2593fiAshlagi, Monderer, & Tennenholtz4. Simple Congestion Gamessection explore mediation enforcement values simple congestion games.first need notations definitions.simple congestion form F = (N, M, (wj )jM ) defined follows. N nonemptyset players; keep assumption that, whenever convenient, N = {1, 2, . . . , n},n = |N | number players. nonempty set facilities; Unless otherwisespecified assume = {1, 2, ..., m}. j , let wj Rn facility payofffunction, every 1 k n, wj (k) denotes payoff user facility j,exactly k users. congestion form nonnegative every j , wjnonnegative. Let Q class nonnegative simple congestion forms denoteQnm Q class nonnegative simple congestion forms n playersfacilities. Every simple congestion form F = (N, M, (wj )jM ) defines simple congestiongame F = (N, (S )iN , (ui )iN ), N set players, = every playerN , utility functions (ui )iN defined follows.Let = . every = (A1 , A2 , ..., ) every j , let j (A) =|{i N : Ai = j}| number users facility j. Define ui : Rui (A) = wAi (Ai (A)).(22)say facility j non-increasing wj (k) non-increasing function k.Define QN nm Qnm follows:QN nm , {F Qnm | facilities F non-increasing}call facility j linear exist constant dj wj (k + 1) wj (k) = djevery k 1, wj (k) = dj k + j every k 1, j constant.Let F simple congestion form n players facilities. congestion vector= (n, m) isPan m-tuple = (j )jM , 1 , 2 , ..., Z (the set nonnegativeintegers)j=1 j = n. represents situation j players choose facilityj. Every strategy profile P uniquely determines congestion vector . Notem21j=1 jn1 nn m1strategy profiles game F correspond2congestion vector , denote B set strategy profiles. Thus B ={A S| = }. Given congestion vector ,P strategy profiles Bwelfare denote u(). u() = {jM | j >0} j wj (j ).say congestion vector equilibrium every strategy profile B purestrategy equilibrium. congestion form called facility symmetric wj = wk j, k .Obviously, facility symmetric congestion form induces symmetric game strategic form.Let Inm Qnm definedInm = {F Qnm | F facility symmetric }.Define nm Inm followsnm = {F Inm | facilities F non-increasing}.594fiOn Value Correlation4.1 Mediation ValueAlthough congestion games especially interesting number players large,begin presenting results case two players, extendingupon results previous section. Following that, consider generaln-player case.4.1.1 two-player case (n = 2)Theorem 1 provides 43 tight upper bound mediation value class gamesG22 . Hence, 43 obviously upper bound mediation value nonnegative simplecongestion games two players two facilities, i.e., games generated congestionforms Q22 . turns bound tight:Theorem 6 V ({F |F Q22 }) = 43 .Proof: Let Fx , x > 1, following simple congestion form: = {a1 , a2 }, wa1 = (x, 0),wa2 = (1, x 1) . games Fx , x > 1 obtained games, x definedproof Theorem 1 (See Figure 2) renaming strategies player 2. Hence,proved Theorem 1, V (Fx ) 34 x . 2Next consider general case two players choose amongfacilities:Theorem 7 V ({F |F Q2m }) 2 every 2.Proof: Let F Q2m . Rosenthal (1973) exists pure-strategy equilibrium F .Let fixed pure-strategy equilibrium largest welfare,u(A) u(D) every pure-strategy equilibrium D.(23)Let j k facilities players 1 2 choose respectively. proveseparation two cases, j = k j 6= k.Case 1: j = k. prove theorem case separation cases wj (1) > wj (2)wj (1) wj (2).Case 1.1: wj (1) > wj (2). case show j strictly dominant strategy,implies Lemma 1 V (F ) = 1. Indeed, let h 6= j facility. orderprove j strictly dominates h show following three inequalitieshold: wh (1) < wj (2), wh (2) < wj (1), wh (1) < wj (1). However, since wj (1) > wj (2),first two inequalities proved. first prove wh (1) < wj (2): Sincepure strategy equilibrium players choose j, wh (1) wj (2). Supposenegation wh (1) = wj (2). Since pure-strategy equilibrium, wj (2) wl (1)every l 6= j, therefore, wh (1) wl (1) every l 6= j. If, addition, wj (1) wh (2),strategy profile one player chooses h, player chooses j purestrategy equilibrium obtains larger welfare contradicting (23). Therefore,wh (2) > wj (1). implies strategy profile players choose hpure-strategy equilibrium obtains larger welfare A, contradiction(23). Therefore wh (1) < wj (2).595fiAshlagi, Monderer, & Tennenholtznext show wh (2) < wj (1). wh (2) wj (1) strategy profileplayers choose h pure-strategy equilibrium 2wh (2) 2wj (1) > 2wj (2) contradicting(23). completes proof Case 1.1.Case 1.2: wj (1) wj (2). Let B pure strategy profile maximumwelfare obtained. is, u(B) = opt(F ). order prove theorem sufficesproveu(B) 2u(A).(24)Suppose B players choose distinct facilities (one facilities j).Since pure-strategy equilibrium, wl (1) wj (2) every l 6= j. Therefore, sincealso wj (1) wj (2), u(B) u(A) particular (24) holds. Suppose two playerschoose facility B, denote facility s. = j B = (24) holds.Therefore assume 6= j. ws (2) wj (2), u(B) u(A), implies (24). Thereforeassume ws (2) > wj (2) particular ws (2) > wj (1). Since wj (2) wl (1)every facility l, l 6= j, B pure-strategy equilibrium u(B) > u(A), contradicting(23). Hence proof Case 1.2 complete.Case 2: j 6= k. Let B arbitrary pure strategy profile maximumwelfare obtained. is, u(B) = opt(F ). suffices prove (24) holds.Recall pure-strategy equilibrium. B player chooses differentfacility, u(B) u(A) particular (24) holds. Therefore assume Bplayers choose facility l (l j k). claimwl (2) max{wj (1), wk (1)}.(25)Indeed, (25) hold, B pure strategy equilibrium, player Bwant deviate either j k, want deviate facilitypure strategy equilibrium. Since u(B) > u(A) contradicts (23). Sinceu(B) = 2wl (2) max{wj (1), wk (1)} u(A), (25) implies (24). 2Notice Theorems 6 7 imply correlation help congestion gamestwo players. next theorem shows correlation cannot help increasing socialwelfare facilities non-increasing:Theorem 8 V (QN 2m ) = 1 every 2.Proof: Let F QN 2m . Let j wj (1) wl (1) l .wj (2) > wl (1) l 6= j, j strictly dominant strategy, implies Lemma 1V (F ) = 1. Suppose exist facility k,k 6= j wj (2) wk (1).Choose k wk (1) maximal, wl (1) wk (1) every l, l 6= j.Therefore, strategy profile one player chooses j player choosesk pure strategy equilibrium obtains maximal welfare. existencestrategy profile implies V (F ) = 1. 2.4.1.2 Simple congestion games n playersTheorems 2 3 Section 3.1.2 imply correlation unbounded valueconsidering arbitrary games. next theorem shows correlation similar effectscontext simple congestion games. particular, show mediation valueunbounded presence additional player:596fiOn Value CorrelationTheorem 9 V ({F |F QN 32 }) = .Proof: Consider following family forms F , 0 < 0.5: = {a1 , a2 }, wa1 =(z, 4, 4 ), wa2 = (4 + , 0, 0), z = 12 . Observe games, F gamesgiven proof Theorem 5 (Figure 6), monotonicity condition satisfiedevery 0 < 0.5.first show every sufficiently small , welfare every mixed-strategyequilibrium lower 13. construct, every sufficiently small ,correlated equilibria F welfare correlated equilibria approachesinfinity 0.Note pure-strategy equilibria strategy profiles two playersplay a1 one player plays a2 . welfare strategy profiles 12 + ,less 13.proceed deal mixed-strategy equilibria least one playerplay pure strategy. equilibrium, player plays strategy a2 probabilityone one players it, mixed-strategy equilibriatwo players plays a1 probability one, i.e., equilibriumpure.Suppose player plays a1 probability one. Note utility matrixtwo players given Figure 7.1a2a14 , 44 + , 4a24, 4 +0, 0Figure 7Therefore standard check exists unique mixed-strategy equilibriumone players play a1 probability 1, equilibrium,player player 1 is:22((1, 0), (,), (,)).2+ 2+ 2+ 2+Since game symmetric, mixed-strategy equilibria three permutationsvector. welfare mixed-strategy equilibria3(4 )(22 22) + 2(12 + )) =+z(22+(2 + )2+48 + 36 + (z + 4)2(2 + )212.25,0.Consider completely mixed-strategy equilibrium. is, every player assigns positiveprobabilities facilities. Denote equilibrium ((p, 1 p), (q, 1 q), (h, 1h)) 0 < p, q, h < 1. player 1 indifferent a1 a2 given players 23 play (q, 1 q) (h, 1 h) respectively,(4 )qh + 4(1 q)h + 4(1 h)q + z(1 h)(1 q) = (4 + )qh.597(26)fiAshlagi, Monderer, & TennenholtzNote similar equalities hold players 2 3, q h exchanged prespectively. every fixed h exists unique q solves equation (26). Therefore,permuting names players, p = q = h. enables us reduce Equation (26)(z 8 2)p2 + (8 2z)p + z = 0,(27)yields2z 8 64 + 8zp=2z 16 4(28)solution (28) satisfy 0 < p < 1. Therefore, welfarecompletely mixed-strategy equilibrium3p(1 p)2 z + 3p2 (1 p)(12 + ) + p3 (12 3) =(3z 24 5)p3 (6z 36 + 3)p2 + 3zp.(29)Let = (36 3)p2 (24 + 5)p3 B = 3zp3 6zp2 + 3zp. Hence (29) = + B.show 12 B 0 0. Observe p 1 0. implies12 0. Observe z = z. Thus, (28) small enoughp p =12z 8z 42z=18 43.2 zsimplicity set c =382 .Therefore33B 3z(1 cz 4 )3 6z(1 cz 4 )2 + 3z(1 cz 4 ) =3693633z[1 3cz 4 + 3c2 z 4 c3 z 4 ] 6z[1 2cz 4 + c2 z 4 ] + 3z(1 cz 4 ) =253c2 z 4 3c3 z 4 =103c2 3c3 4 00.completes proof vN (F ) 13 every sufficiently small enough .complete proof theorem construct, every sufficiently small ,correlated equilibria F welfare correlated equilibria approachesinfinity 0.order correlated strategy correlated equilibrium F , followinginequalities satisfied (in brackets relate inequality correspondinginequality (2)):141. 2(1, 1, 1) + 4(1, 2, 1) + 4(1, 1, 2) + z(1, 2, 2) 0.2. 2(2, 1, 1) 4(2, 2, 1) 4(2, 1, 2) z(2, 2, 2) 0.3. 2 + 4(2, 1, 1) + 4(1, 1, 2) + z(2, 1, 2) 0.(i = 1, si = a2 , ti = a1 )(i = 2, si = a1 , ti = a2 )4. 2(1, 2, 1) 4(2, 2, 1) 4(1, 2, 2) z(2, 2, 2) 0.5. 2(1, 1, 1) + 4(1, 2, 1) + 4(2, 1, 1) + z(2, 2, 1) 0.14. Here, slightly abuse notation letting (i, j, k) = (ai , aj , ak ).598(i = 1, si = a1 , ti = a2 )(i = 2, si = a2 , ti = a1 )(i = 3, si = a1 , ti = a2 )fiOn Value Correlation6. 2(1, 1, 2) 4(1, 2, 2) 4(2, 1, 2) z(2, 2, 2) 0.P87.i=1 = 1.(i = 3, si = a2 , ti = a1 )sufficiently small , inequalities satisfied , (1, 1, 1) =13(2, 1, 2) = (2, 2, 2) = 0, (2, 1, 1) = (1, 1, 2) = 4 , (2, 2, 1) = (1, 2, 2) = 2 .Note z(2, 2, 1) = 1 0, u() z(2, 2, 1). Thereforeu() 0. 2conjecture Theorem 9 holds players and/or facilities. is,V ({F |F QN nm }) = every n 3 every 2. One way proveconjecture modify Lemma 2, used prove analogous extensions generalgames (see e.g., Theorems 2 3).next theorem deals linear facilities:Theorem 10V ({F |F QN n2 , facilities F linear})every n 2,= ( 5 + 1)/2.(30)following lemma (Schrijver, 1986, page 61) used proof Theorem 10.Lemma 5 (Farkas Lemma) Let s, positive integers. Given matrix dimensionsvector b Rs , one one following systems solution:(i)Ax b,x Rt ;(ii)yT = 0,yT b > 0,Rs+ ,Rs+ denotes set nonnegative vectors Rs .Proof Theorem 10: Consider nonnegative, non-increasing linear congestionform = {f, g}, wf wg facility payoff functions f grespectively, wf (k) = df k + f wg (k) = dg k + g . Obviously, df , dg 0. Denoteinduced congestion game. Assume w.l.o.g.wf (1) wg (1).(31)Denote k = (nk, k) congestion vector nk players choose f k playerschoose g. Let , 0 n, largest integer equilibrium. Sinceequilibrium, wg ( ) wg (n + 1), monotonicity condition,wg (j) wg (n j + 1) j .Recall welfare denoted u( ).Claim 1: V ()) = 1, whenever {n, 0}.599(32)fiAshlagi, Monderer, & TennenholtzProof: Suppose = n. Since (0, n) equilibrium, wg (n) wf (1). Therefore, (31),wg (n) wg (1). monotonicity condition, every k, u(k ) = (n k)wf (n k) +kwg (k) (n k)wf (1) + kwg (1). Therefore, u(k ) nwg (n) = u(n ). Hence, maximalwelfare, opt() attained equilibrium n , implies mediation valueequals 1.Suppose = 0. Since (n, 0) equilibrium, wf (n) wg (1). claimwf (n) > wg (1).(33)Indeed, assume negation wf (n) = wg (1). Since (n 1, 1) equilibrium, eitherwf (n 1) < wg (2) wg (1) < wf (n). Therefore, wf (n 1) < wg (2),monotonicity condition, wf (n) < wg (1), yields contradiction. Therefore (33) holds,implies choosing f strictly dominates choosing g . Therefore, Lemma 1,every correlated equilibrium pure strategy profile, generates congestionvector 0 . Hence, mediation value equals 1.2Claim 1, rest proof assume w.l.og.1 n 1.(34)Note wg (1) < wf (n), choosing f strictly dominates choosing g, impliesmediation value equals 1. Therefore assume w.l.o.g. rest proofwg (1) wf (n).(35)following key claim proof.Claim 2: u(j ) u( ) every j .Proof: Let j < . Since facilities linear, wg (j) = wg ( ) + ( j)dg wf (n j) =wf (n ) ( j)df . Hence,u( ) u(j ) =swg ( ) + (n )wf (n ) j(wg ( ) + ( j)dg ) (n j)(wf (n ) ( j)df ) =wf (n )(j ) + wg ( )( j) j( j)dg + (n j)( j)df =( j)(wg ( ) wf (n ) + (n j)df jdg )( j)(wf (n + 1) wf (n ) + (n j)df jdg ),(36)last inequality follows since equilibrium. Since facilities linear,RHS(36) = ( j)((n j 1)df jdg ) =( j)(wf (1) wf (n j) wg (1) + wg (j + 1)).(37)Since j < suffices show (wf (1) wf (n j) wg (1) + wg (j + 1)) 0.inequality follows (31) (32). 2Next define auxiliary strategy profile. Denote q mixed-strategy profileplayers 1, 2, . . . , n 1 choose f probability 1,w (1)w (n)+ 1 players choose g probability p = g (df +dfg ) , i.e., plays mixed600fiOn Value Correlationstrategy (1 p, p). (35), p 0. see p 1, note equilibrium,wg (1) wf (n) wg (1) wg ( + 1) + wf (n ) wf (n) = sdg + sdf .Claim 3: (i) q mixed strategy equilibrium; (ii) u(q) = nwf (n) + pdf ((n 1)( +1) + ( + 1) ).Proof: (i) first show ui (qi , f ) ui (qi , g) every player chooses fprobability 1. Let k = + 1. have, ui (qi , f ) ui (qi , g) =kXk jp (1 p)kj (wf (n j) wg (j + 1)) =jj=0kXk jp (1 p)kj (wf (1) (n j 1)df wg (1) + jdg ) =jj=0wf (1) wg (1) (n 1)df + k pdf + k pdg .(38)Since wf (n) = wf (1) (n 1)df k > k 1,RHS(38) = wf (n) wg (1) + (k 1)pdf + (k 1)pdg 0,w (1)w (n)gflast inequality follows since p 1, p = (k1)(d.f +dg )next show every player plays mixed strategy (1 p, p) indifferentf g. Observe ui (qi , f ) ui (qi , g) =nXk1 jp (1 p)k1j (wf (n j) wg (j + 1)) =jj=0nXk1 jp (1 p)k1j (wf (1) (n j 1)df wg (1) + jdg ) =jj=0wf (1) wg (1) (n 1)df + (k 1)pdf + (k 1)pdg =wf (n) wg (1) + (k 1)pdf + (k 1)pdg = 0.(ii) Similar calculations part (i) yield expected payoffn 1 players choose f probability one equals wf (n) + ( + 1)pdf ,expected payoff + 1 players equals wf (n) + pdf . Thereforeu(q) = nwf (n) + pdf ((n 1)( + 1) + ( + 1) ). 2proceed main proof. DefineZ = max{u( ), u(q)}.(39)u(k ) Z every 0 k n, opt() Z, therefore, vc () opt()max{u( ), u(q)} implying V () . Therefore assume restproof exist integer k, 0 k nu(k ) > Z..601(40)fiAshlagi, Monderer, & Tennenholtzb case, dual problem denotedproceed utilizing dual program D.b follows:mins.t.b: (f |g) 0N,(g|f ) 0N,P(t |s )[u (t , ) u (s , )] + u(s) S,every i, ti = f si = g, ti = g si = f .Recall weak duality theorem, every feasible solution dual problem,(, ), vC (). Therefore, order complete proof, suffices showb = Z, Z definedexists feasible solution dual problem(39).begin restricting range variables dual problem (D) .c1 ) obtained (D)b letting = Z, (g|f ) = 0specifically, following system (Dx = (f |g) every N , i.e., x remains variable:mins.t.c1 : = Z,k(wf (n k + 1) wg (k))x u(k )x 0.k = 1, ..., n,c1 defines feasible solution, (, ) (D)Obviously, every optimal solution, xc1 equivalent= Z. Since = Z, existence optimal solutionc2 ):existence feasible solution following system constraints, (D(c2 : k(wf (n k + 1) wg (k))x u(k )x 0.k = 1, ..., n,c2 redundant, i.e., x satisfiesnext show inequality x 0first n constraints, x 0. Recall (40) exist integer 0 k nu(k ) > Z. Since > 1, Claim 2, k > , particular, k 1. Since k > , kequilibrium. Since , player f wish deviate g, case kfewer players f . Therefore, k , deviation player g fprofitable, i.e., wf (n k + 1) wg (k) > 0, since k > 0, k(wf (n k + 1) wg (k)) > 0.c2 u( ) > Z = , x > 0. Henceb 2Since x satisfies constraint kkequivalentnc3 : k(wf (n k + 1) wg (k))x u(k )602k = 1, ..., n.fiOn Value Correlationc3 solution x following systemFarkas Lemma (Lemma 5),solution, = (y1 , y2 , . . . , yn ):PnPk=1 yk k(wf (n k + 1) wg (k)) = 0,ncP1 :k=1 yk (u(k ) ) > 0,yk 0k = 1, ..., n.c1 solution. Note solutionTherefore, suffices show PcP1 , least one variable yk positive. Therefore, suffices toPprovec1 , also probability vector, i.e., n yk = 1. Letexist solution Pk=1c1 . proveprobability vector satisfies first constraint Psatisfy second constraint, is, shownXyk u(k ) .(41)k=1Let random variable whichPyk = P (Y = k), k = 1, ..., n, recallexpected value satisfies E[Y ] = nk=1 kyk . first derive following usefulinequalities given Claim 4:Claim 4:(i)E[Y ]wg (1) wf (n) + df + dg.(df + dg )(42)(ii)nXyk u(k ) = E[Y ](wf (1) wf (n)) + nwf (n).k=1c1 satisfied,Proof: (i) Since first constraint P0=nXyk k(wf (n k + 1) wg (k)) =k=1nXyk k(wf (n) + (k 1)df wg (1) + (k 1)dg ) =k=1E[Y ](wf (n) wg (1) df dg ) + E(Y 2 )(df + dg ).Since E[Y 2 ] E[Y ]2 ,E[Y ](wf (n) wg (1) df dg ) + E[Y ]2 (df + dg ) 0.Since E[Y ] > 0, dividing sides E[Y ] obtain (42).603(43)fiAshlagi, Monderer, & Tennenholtz(ii)nXyk u(k ) =k=1nXyk (kwg (k) + (n k)wf (n k)) =k=1nXyk (kwf (n k + 1) + (n k)wf (n k)),k=1c1 satisfied y. Therefore,last equality holds first constraint PnXyk u(k ) =k=1wf (1)E[Y ] +nXyk k(wf (1) (n k)df ) +k=1nXnXyk (n k)wf (n k) =k=1yk (n k)(wf (n k) kdf ) = wf (1)E[Y ] + wf (n)k=1nXyk (n k)k=1= E[Y ](wf (1) wf (n)) + nwf (n),proves (43) holds. 2proceed prove (41) holds. plugging (43) (41), equivalentlyproveE[Y ](wf (1) wf (n)) + nwf (n) ,(44)distinguish following two cases:Case 1: p < 1/. First showE[Y ] .p < 1/, (wg (1) wf (n))/(df + dg ) /. Hence, (42), E[Y ]Since = 1 + 1, = + . Since 1, (45) holds.(45)+ 1.Suppose negation (44) hold, i.e., E[Y ](wf (1) wf (n)) + nwf (n) > .Note = Z u( ). Therefore, E[Y ](wf (1) wf (n)) + nwf (n) > u( ).definition u( ) linearity facilities,E[Y ] >wg ( ) + (n )wf (n ) nwf (n)=(n 1)dfwg ( ) + (n )(wf (n) + sdf ) nwf (n).(n 1)df(46)Since equilibrium,RHS(46)wf (n + 1) + (n( 1) )wf (n) + (n ) df.(n 1)dfSince (n( 1) )wf (n) 0,RHS(47)(wf (n + 1) wf (n)) + (n ) df=(n 1)df604(47)fiOn Value Correlation( 1)df + (n ) df(n 1)== .(n 1)dfn1Therefore, (44) hold,E[Y ] > ,(48)E[Y ] p + 1.(49)contradicting (45).Case 2: p 1/.(42) sincewg (1)wf (n)(df +dg )= p,Suppose negation second constraint satisfied, i.e., RHS(43) = E[Y ](wf (1)wf (n)) + nwf (n) > . Note u(q). ThereforeE[Y ] >u(q).(wf (1) wf (n)) + nwf (n)Hence, part(ii) Claim 3 linearity facilitiesE[Y ] >nwf (n) + pdf ((n 1)( + 1) + ( + 1) ) nwf (n)=(n 1)dfn( 1)wf (n) + pdf ( + 1)(n 1).(n 1)df(50)Since n( 1)wf (n) 0 p 1/RHS(50) + 1.Hence, E[Y ] > + 1 contradicting (49) since p 1.completes proof theorem. 2Theorem 10 derived, every n 2, upper bound mediation valueclass games, ({F |F QN n2 , facilities F linear}). n = 2,Theorem 8, know mediation value equals 1. Unfortunately, knowmediation value classes games n 3. However, exampleshows that, n = 3, mediation value class least 89 ; Hence,mediation value 1.125 1.618.Example 1 Let n = 3, = {f, g}, wf = (24, 12, 0), wg = (8, 8, 8). easilyverified vN () = 32, obtained, e.g., pure-strategy equilibriumtwo players choose f player chooses g. Consider correlated strategy ,assigns probability 61 every strategy profile players choosefacility. easily verified correlated equilibrium welfare36. Hence, mediation value least 3632 .605fiAshlagi, Monderer, & Tennenholtznext discussion useful recall price stability cost gameratio minimal cost mixed-strategy equilibrium minimal cost,price stability utilities utility game ratio maximalwelfare maximal welfare mixed-strategy equilibrium.Let n, 2. Another open question us estimating mediation value, simplecongestion games n players, facilities, nonnegative, non-increasing,linear facilities. One think upper bound class derivedresults price stability analogous class congestion gamescosts. Indeed, Christodoulou Koutsoupias (2005) proved price stabilityclass congestion games cost n players, facilities, nonnegative,linear, non-decreasing cost functions bounded 1.6. result provenprice stability utilities class games would implied,particular, mediation value class bounded 1.6,mediation value cannot exceed price stability utilities.15 However, discussedintroduction, results price stability cost models cannot transformedresults price stability utilities utility models. illustrate this,show next example price stability utilities class {F |FQN 22 , facilities F linear} equals .16Example 2 Let x, > 0 fixed. Let N = {1, 2}, = {f, g} every > ,let wfd = (x + + , x + ), let wgd = (x, x), associated congestion game.Since f strictly dominates g, strategy profile players choose fmixed-equilibrium game . welfare obtained equilibrium 2x + 2.strategy profile attaining maximal welfare one player choosesdifferent facility yielding welfare 2x + + d. Since 2x++d2x+2 , pricestability {F |F QN 22 , facilities F linear} equals .Theorems 9, Example 1 show correlation helpful context (evennon-increasing) congestion games. next theorem shows correlation helpfuleven narrow class facility symmetric forms nonnegative non-increasingfacilities:Theorem 11 V ({F |F n2 }) > 1 every n 4.Proof: Let n 4. suffices prove exists F n2 V (F ) > 1.Let 0 < < 1 fixed sufficiently small, consider following form, F : w1 =w2 = (10n, 1, . . . , 1, 1 , 0). Note maximal welfare obtained strategyprofile exactly n 1 players choose facility, i.e., congestionvector 1 = (1, n 1) 2 = (n 1, 1). Let L set strategy profilescongestion vector 1 2 . Note exist exactly 2n strategy profiles L. Let1correlated strategy every strategy profile L played probability 2n.Since convex combination welfare maximizers, maximal welfare attained15. Moreover, since 1.6 , result would saved us tedious proof Theorem 10.16. proof Christodoulou Koutsoupias (2005) elegantly uses fact every congestion gameexact potential. known us wether potential approach could simplify proofTheorem 10. However, turns technique applied directly settingessentially bound total cost players every facility separately given profile.606fiOn Value Correlation. claim correlated equilibrium. Indeed, (2), easily verifiedF exists unique constraint satisfied order guaranteecorrelated equilibrium. constraint is: 10n+((1)1)(n1)0, indeed satisfied2nchose sufficiently small. Hence correlated equilibrium.order prove V (F ) > 1, suffices prove mixed-strategy equilibrium obtains maximal welfare. First, note strategy profiles L obtainmaximal welfare. Hence, pure-strategy equilibrium obtains maximal welfare,must belong L. However, claim every strategy profile L equilibrium;Indeed player chooses facility chosen n 1 players better deviatingfacility since utility increase .Therefore, remains show every mixed-strategy equilibrium, p = (p1 , . . . , pn ),least one player assigns positive probability facility, existsleast one pure strategy profile, s, L, played positive probability,p (s) > 0, p correlated strategy associated p. Indeed, let pmixed-strategy equilibrium, assume w.l.o.g. player assigns positive probabilityfacilities, is, pi (1), pi (2) > 0. Assume negation p (t) > 0 implies L,let strategy profile p (s) > 0. Since pi (1), pi (2) > 0, p (1, si ) > 0p (2, si ) > 0. Therefore, (1, si ) L (2, si ) L, impossible since n 4. 2restrict assumptions Theorem 11 requiring concavity condition correlation cannot help anymore. first define concavity:Let n 2. function v : {1, 2, ..., n} R+ concave every integer k, 2 k < n,v(k + 1) v(k) v(k) v(k 1).Theorem 12 Let n 2, let F nm . Define v(k) = kw(k) every1 k n, w common facility payoff function, is, w = wj everyj . v concave, exists pure-strategy equilibrium F obtainsmaximal welfare. Consequently, V (F ) = 1.Proof: convenient also define v(0) = 0. first define operator congestionvectors. Let = (1 , 2 , ..., ) congestion vector let j, l pairdistinct facilities. Let [j, l] congestion vector obtained replacing j++l j = j 2 l e l = b j 2 l c, respectively, bxc (dxe) denotesPm largest(least) integer higher (lower) x. Observe u() =i=1 v(i ),therefore, concavity v,u( [j, l]) u().(51)Next describe finite sequence congestion vectors, oneobtains maximal welfare last one also equilibrium. that, set k1 =nnbmc, k2 =e, note every congestion vector coordinate either k1k2 equilibrium.Pick congestion vector obtains maximal welfare. equilibrium,done. Otherwise, coordinates k1 k2 . particular, existtwo distinct coordinates j l j k1 , l k2 , least one inequalitystrict. Construct = [j, l]. (51), u( ) u(), therefore, obtainsmaximal welfare. equilibrium done. Otherwise let = repeat607fiAshlagi, Monderer, & Tennenholtzprocess. sequence terminate, eventually reaches everycoordinates equals k1 k2 , i.e., reaches equilibrium contradiction. Therefore,sequence terminates finite number stages, hence last congestion vectorsequence equilibrium attains maximal welfare. 24.2 Enforcement Valuealready know construction proof Theorem 4 enforcementvalue class = {x | x > 1} Prisoners Dilemma games described Figre 5unbounded. easily verified every game class potential game,therefore, (Monderer & Shapley, 1996), congestion game. Moreover, easilyverified game derived simple nonnegative congestion formQ22 . Therefore, EV ({F |F Q22 ) = . However, noticed Monderer (2007),every congestion game nonnegative utilities represented congestionform nonnegative non-increasing facilities. particular, shownPrisoners Dilemma games cannot represented simple congestion formsnonnegative non-increasing facilities. next theorem shows evenfacility payoff functions restricted non-increasing, enforcement value remainsunbounded two player games.Theorem 13 EV ({F |F QN 2m }) = every 2.Proof: Consider games ,d > 0, given Example 2. Since using facility f strictlydominates using g, Lemma 1, strategy profile players choose fd)unique correlated equilibrium . proved Example 2, opt(, .vC (d )proves theorem = 2. > 2 proof obtained, above, naturallymodifying games Example 2. 2Since Theorem 13 deals two players, deduce enforcement valueunbounded also class games generated linear facilities QN 2mtwo players, every non-increasing facility linear.Note proof Theorem 13 utilizes games, posses strictly, particular,weakly dominant strategies. next theorem deals games without weakly dominantstrategies.Theorem 14(i) EV ({F |F QN 22 , weakly dominant strategies}) = 1.(ii) EV ({F |F QN 32 , weakly dominant strategies}) = .Proof: Assertion (i) follows Theorem 8. Assertion (ii), proof followsobserving game proof Theorem 5 simple congestion game nonincreasing facilities. 2next theorem shows enforcement value tends numberplayers tends even restricting facilities symmetric non-increasing.Theorem 15 limn EV ({F |F n2 }) = .608fiOn Value CorrelationProof: Consider following family forms Fn n2 , n 3: = {f, g} letwf = wg = ( n, 1, 0, 0, .., 0). Observe congestion vector, n = (1, n 1), obtainsmaximal welfare, equals n. Therefore, order prove theorem, sufficesshow(52)vC (Fn ) 3 every n 3.bLet n 3 fixed. order prove (52), use dual program (D).weak duality theorem, every feasible solution dual problem satisfies vC (Fn ).Therefore, suffices prove dual problem feasible solution = 3.b described right (40).However, case, dual problem problem,b , dual variables except identicalfind feasible solutionvalue x, i.e., x = (f |g) = (g|f ) player i. restriction, dualb reduces to:program,mins.t.x 0,( n + n 1)x + n,2x + 1,nn + 0,feasible solution, (x, ) = (1, 3). Therefore (52) holds. 2Although enforcement value may unbounded facility symmetriccongestion forms, interesting characterize congestion games correlation enables get maximal welfare. done next theorem, first needfollowing notations. Let F congestion form, Let congestion vectorF . Let : one one function, i.e., permutation set facilities.define congestion vector = ( )jM follows: ( )j = (j) every facility j.Recall B theSset strategy profiles induce congestion vector .define, L = B set strategy profiles induce permutationcongestion vector .Let F Inm , is, facilities F symmetric. Therefore, u() = u( )every permutation , and, addition, every pair strategy profiles A, B L ,u(A) = u(B).Theorem 16 Let n, 2, let F Inm . Then, vC (F ) = opt(F )exist congestion vector = (1 , ..., ) correlated equilibrium C(F )following two conditions hold:1. u() = opt(F ).2. distributed uniformly strategy profiles L ; is, (d) = (d)every d, L , (d) = 0 every 6 L .following lemma (Schrijver, 1986, page 61) used proof:609fiAshlagi, Monderer, & TennenholtzLemma 6 (Variant Farkas Lemma) Let s, positive integers. Given matrixdimensions vector b Rs , one one following systemssolution:(i)Ax = b,(ii)yT 0,x 0,x Rt ;yT b < 0,Rs .Proof Theorem 16:17Clearly, exist congestion vector correlated equilibrium C(F ),satisfy conditions, vC (F ) = opt(F ). prove directionneed Claim 1 below.Let w common facility payoff function, is, w = wj every j .every congestion vector define Z() follows:PmPj=1 jk6=j (w(j ) w(k + 1))Z() =.(53)m!nClaim 1: Suppose vC (F ) = opt(F ). Let = (1 , ..., ) congestion vectoru() = opt(F ), let correlated strategy distributed uniformly elementsL . correlated equilibrium Z() 0.Proof: Let D,i,j set strategy profiles player chooses facility jcongestion vector . D,i,j = {d : B , di = j}.Qm nj Pl1 k 1k6=jk=1Let (j) = n1, note |D,i,j | = (j) everyl=11l 1l6=jjN.Since distributed uniformly elements L , correlated equilibriumPPmk6=j (w(j ) w(k + 1))j=1 (j)0.(54)|L |Hence, suffices show LHS(54) = Z(). follows since |L | = |B |m! =nn n1nPm2 j(k1)j=1= nk . 2m!12m1(nk)proceed proving remaining direction. Suppose vC (F ) = opt(F ),assume contradiction exist congestion vector correlatedequilibrium C(F ) distributed uniformly elements Lu() = opt(F ). Recall every strategy profile S, congestion vectorinducedkd number players choose facility k d. Let ={d:u( )=opt(F )} L . negation assumption, Claim 1 impliesZ( ) < 0 every D.(55)utilize Lemma 6, matter set J = |D|. Define matrixsize J n(m2 m) follows:A(d, ijk) =[w(jd ) w(kd + 1)]1ij (d),m!n17. proof technique inspired Nau McCardle (1990).610(56)fiOn Value Correlationrow corresponds strategy profile column ijk correspondsplayer chooses jth strategy (facility) deviates kth strategy (j 6= k), and:1 di = j1ij (d) =0 otherwise.Every row matrix corresponds strategy profile D, turn,). Set b(d) = Z( ) everycorresponds congestion vector = (1d , ...,D. Note column vector x = (1, 1, ..., 1) RJ satisfies (i) Lemma 6,b = (b(d))dD . Therefore, Lemma 6, system (ii) Lemma 6 solution.vC (F ) = opt(F ), exists correlated equilibrium, say , satisfying u() =vC (F ) = opt(F ). Obviously, supported D, (d) = 0 every 6 D. Lety(d) = (d) every D, let = (y(d))dD . Since correlated equilibriumconcentrated D, yT 0, since satisfy (ii),yT b 0.(57)hand, since probability distribution D, (55) holds,XX(d)b(d) < 0,y(d)b(d) =yT b =dDdDcontradicting (57). Therefore, negation assumption cannot hold, theoremproved. 2Theorem 16 shows symmetric congestion games, conditionstheorem, correlation helps obtaining maximal welfare. next example showsexist games mixed-strategy equilibrium useful correlatedequilibrium:Example 3 Let F I62 . Let wj = (1.5, 1, 4, 4.5, 4.5, 3) every j = 1, 2. easyverify maximal welfare obtained strategy profileL1 L2 , 1 = (3, 3) 2 = (1, 5). Let correlated strategy,distributed uniformly L2 . checked correlated equilibrium. Hence,Theorem 16, vC (F ) = opt(F ). hand, note 1 2equilibrium. Hence, every mixed-strategy profile, profile obtainmaximal welfare played positive probability. Therefore mediation valuegreater 1, i.e. best mixed-strategy equilibrium less useful best correlatedequilibrium.5. Conclusionwork introduced studied two measures value correlationstrategic interactions: mediation value enforcement value. measurescomplement existing measures appearing price anarchy literature, comparing maximal welfare (when agent behavior dictated) welfare obtainedNash equilibrium (when agents selfish). Indeed, correlation captures many interesting situations, common computing systems e-commerce applications.611fiAshlagi, Monderer, & Tennenholtzmany systems reliable party advise agents behave enforcebehavior. gain may obtained capability major subjectstudy presented work. studied showed power approach,general games context congestion games.Acknowledgmentspreliminary version paper appears proceedings 20th conferenceUncertainty Artificial Intelligence (UAI-05). thank German-Israeli Foundation(GIF) financial supportReferencesAnshelevich, E., Dasgupta, A., Kleinberg, J., Tardos, E., Wexler, T., & Roughgarden,T. (2004). Price Stability Network Design Fair Cost Allocation.Proceedings 45th IEEE Symposium Foundations Computer Science,(FOCS-04), pp. 5973.Ashlagi, I., Monderer, D., & Tennenholtz, M. (2008). Mediators position auctions. appear Games Economic Behavior. shorter version appears Proceedings8th ACM conference Electronic Eommerce.Aumann, R. (1974). Subjectivity Correlation Randomized Strategies. JournalMathematical Economics, 1, 6796.Aumann, R. (1987). Correlated Equilibrium Expression Bayesian Rationality.Econometrica, 55, 118.Christodoulou, G., & Koutsoupias, E. (2005). Price Anarchy StabilityCorrelated Equilibria Linear Congestion Games. Proceedings 13th AnnualEuropean Symposium, ESA 2005, pp. 5970.Czumaj, A., & Vocking, B. (2002). Tight Bounds Worst Case Equilibria. Proceedings13th Annual Symposium Discrete Algorithms, pp. 413420.Hart, S., & Schmeidler, D. (1989). Existence Correlated Equilibria. Math. Oper. Res.,14, 1825.Kakade, S., Kearns, M., Langford, J., & Ortiz, L. (2003). Correlated equilibria graphicalgames. Proceedings 4th ACM conference Electronic commerce, pp. 4247.Koutsoupias, E., & Papadimitriou, C. (1999). Worst-Case Equilibria. Proceedings16th Annual Symposium Theoretical Aspects Computer Science, pp. 404413.Marvonicolas, M., & Spirakis, P. (2001). Price Selfish Routing. Proceedings33rd Symposium Theory Computing, pp. 510519.Monderer, D. (2007). Multipotential Games. Twentieth International joint conferenceArtificial Intelligence (IJCAI-07) .612fiOn Value CorrelationMonderer, D., & Shapley, L. (1996). Potential Games. Games Economic Behavior, 14,124143.Monderer, D., & Tennenholtz, M. (2004). K-Implementation. Journal Artificial Intelligence Research (JAIR), 21, 3762.Monderer, D., & Tennenholtz, M. (2006). Strong mediated equilibrium. ProceedingsAAAI.Nash, J. (1951). Noncooperative Games. Ann. Math., 54, 286295.Nau, R. F., & McCardle, K. F. (1990). Coherent Behavior Noncooperative Games.Journal Economic Theory, 50, 424444.Papadimitriou, C. (2001). Algorithms, Games, Internet. Proceedings 16thAnnual ACM Symposium Theoretical Aspects Computer Science, pp. 749753.Papadimitriou, C. (2005). Computing correlated equilibria multi-player games. Proceedings thirty-seventh annual ACM symposium Theory computing, pp.4956.Peeters, R., & Potters, J. (1999). Structure Set Correlated EquilibriaTwo-by-Two Bimatrix Games. Technical report, Tilburg - Center EconomicResearch.Rosenthal, R. (1973). Class Games Possessing Pure-Strategy Nash Equilibria. International Journal Game Theory, 2, 6567.Roughgarden, T. (2002). Selfish Routing. PhD Thesis, Cornell University.Roughgarden, T., & Tardos, E. (2002). Bad Selfish Routing?. Journal ACM,49(2), 236259.Rozenfeld, O., & Tennenholtz, M. (2007). Routing mediators. Proceedings 23rdInternational Joint Conferences Artificial Intelligence(IJCAI-07), pp. 14881493.Schrijver, A. (1986). Theory Linear Integer Programming. Wiley, New York.Shoham, Y., & Tennenholtz, M. (1995a). Artificial Social Systems. Computers ArtificialIntelligence, 14, 533562.Shoham, Y., & Tennenholtz, M. (1995b). Social Laws Artificial Agent Societies:Off-Line Design. Artificial Intelligence, 73, 231252.613fiJournal Artificial Intelligence Research 33 (2008) 285-348Submitted 4/08; published 11/08Computational Logic Foundations KGP AgentsAntonis Kakasantonis@ucy.ac.cyDepartment Computer Science, University Cyprus75 Kallipoleos Str., P.O. Box 537, CY-1678 Nicosia, CyprusPaolo Mancarellapaolo.mancarella@unipi.itDipartimento di Informatica, Universita di PisaLargo B. Pontecorvo, 3 - 56127 Pisa, ItalyFariba Sadrifs@doc.ic.ac.ukDepartment Computing, Imperial College LondonSouth Kensington Campus, London SW72AZ, UKKostas Stathiskostas@cs.rhul.ac.ukDepartment Computer Science, Royal HollowayUniversity London, Egham, Surrey TW20 0EX, UKFrancesca Tonift@doc.ic.ac.ukDepartment Computing, Imperial College LondonSouth Kensington Campus, London SW72AZ, UKAbstractpaper presents computational logic foundations model agency calledKGP (Knowledge, Goals Plan) model. model allows specificationheterogeneous agents interact other, exhibit proactivereactive behaviour allowing function dynamic environments adjustinggoals plans changes happen environments. KGP provides highlymodular agent architecture integrates collection reasoning physical capabilities, synthesised within transitions update agents state response reasoning,sensing acting. Transitions orchestrated cycle theories specify ordertransitions executed taking account dynamic context agentpreferences, well selection operators providing inputs transitions.1. Introductionwidely acknowledged concept agency provides convenient powerfulabstraction describe complex software entities acting certain degree autonomyaccomplish tasks, often behalf user (Wooldridge, 2002). agent contextunderstood software component capabilities reacting, planning(inter) acting achieve goals environment situated. paper,present model agency, called KGP (Knowledge, Goals Plan). modelhierarchical highly modular, allowing independent specifications collectionreasoning physical capabilities, used equip agent intelligent decision makingadaptive behaviour. model particularly suited open, dynamic environmentsagents adapt changes environment functioncircumstances information incomplete.c2008AI Access Foundation. rights reserved.fiKakas, Mancarella, Sadri, Stathis & Tonidevelopment KGP model originally motivated existing gap modal logic specifications (Rao & Georgeff, 1991) BDI agents (Bratman, Israel, &Pollack, 1988) implementation (for example see issues raised Rao, 1996).Another motivation development KGP comes participation SOCSproject (SOCS, 2007), need agent model satisfies several requirements. specifically, aimed agent model rich enough allowintelligent, adaptive heterogeneous behaviour, formal could lent wellformal analysis, implementable way implementation sufficientlyclose formal specification allow verification. Although several models agencyproposed, none satisfies requirements once.bridge gap specification implementation KGP model basedcomputational logic (CL). focus work extend synthesise numberuseful computational logic techniques produce formal executable specificationsagents. purpose, model integrates abductive logic programming (ALP) (Kakas,Kowalski, & Toni, 1992), logic programming priorities (Kakas, Mancarella, & Dung,1994; Prakken & Sartor, 1997) constraint logic programming (Jaffar & Maher, 1994).techniques explored right, modular integrationwithin KGP model explores extensions each, well providing high level agentreasoning capabilities.KGP model provides hierarchical architecture agents. specifies collectionmodular knowledge bases, formalised CL. knowledge bases support collection reasoning capabilities, planning, reactivity, goal decision,given formal specifications. model also includes specification physical capabilities,comprising sensing actuating. capabilities utilised within transitions,model state agent changes result reasoning, sensing acting.Transitions use selection operators providing inputs. control component, calledcycle theory, also formalised CL, specifies order transitions executed, depending environment, state agent, preferences agent.cycle theory takes agent control beyond one-size-fits-all approach usedagent models, allows us specify agents different preferences profiles behaviour (Sadri & Toni, 2005). particular, whereas majority existing agent modelsrely upon observe-plan-act, means cycle theory model behavioursobserve-revise goals-planact observe-plan-sense action preconditions-actobserve-plan-act-plan-act. provide one example cycle theory, refernormal, allowing behaviours depending different circumstances (the environment agent situated preferences). Note also that, respectagent models, KGP model allows agents revise goals life-time,observing environment according two modalities: active passive observation.agent built KGP architecture dynamically determines goals, plans (partially) achieve goals, interleaves planning action executions makingobservations environment receiving messages agents, adaptsgoals plans new information receives, changes observes, generates appropriate reactions.number publications already described aspects (an initial version of)KGP agents. precursor overall model described Kakas, Mancarella,286fiComputational Logic Foundations KGP AgentsSadri, Stathis, Toni (2004b), planning component presented Mancarella,Sadri, Terreni, Toni (2004), cycle theory developed Kakas, Mancarella,Sadri, Stathis, Toni (2004a) implementation discussed Stathis et al.(2004), Yip, Forth, Stathis, Kakas (2005), Bracciali, Endriss, Demetriou,Kakas, Lu, Stathis (2006). paper, provide full formal specificationcomponents KGP model, thus offering complete technical accountKGP one place. providing full formal specification, adjusteddeveloped model. particular, notion state definition novel,reasoning capabilities simplified added, physicalcapabilities extended (to include actuating) formally defined, transitionsselection operators formally defined full.rest paper structured follows. Sections 2 3 give outlinemodel review background information necessary full description.Sections 4, 5, 6 7, respectively, describe internal state KGP agents, reasoning physical capabilities, transitions. Section 8 describe selectionoperators used cycle theory described Section 9. Followingdetailed description KGP agents illustrate model series examplesSection 10, compare model others literature Section 11. Finally,conclude paper Section 12.2. KGP Model: OutlineSection give overview KGP agent model components,provide informal examples functioning. model relies uponinternal (or mental) state, holding agent Knowledge base (beliefs), Goals (desires) Plans (intentions),set reasoning capabilities,set physical capabilities,set transition rules, defining state agent changes, definedterms capabilities,set selection operators, enable provide appropriate inputs transitions,cycle theory, providing control deciding transitions appliedwhen.model defined modular fashion, different activities encapsulatedwithin different capabilities transitions, control separate module.model also hierarchical structure, depicted Figure 1.2.1 Internal Statetuple hKB0 , F, C, i, where:287fiKakas, Mancarella, Sadri, Stathis & ToniCYCLETHEORYTRANSITIONSSELECTIONOPERATORSEREASONINGCAPABILITIESPHYSICAL CAPABILITIESFigure 1: graphical overview KGP modelKB0 holds beliefs agent external world situated(including past communications), well record actions alreadyexecuted.F forest trees whose nodes goals, may executable not.tree forest gives hierarchical presentation goals, tree representsconstruction plan root tree. set leaves tree Fforms currently chosen plan achieving root tree. Executable goalsactions may physical, communicative, sensing. simplicity, assumeactions atomic duration. Non-executable goals maymental sensing. non-executable mental goals may children, forming(partial) plans them. Actions children tree F. goalassociated time variable, implicitly existentially quantified within overallstate serves two purposes: (1) indicating time goal achieved,instantiated goal achieved appropriate time, (2) providingunique identifier goal. remainder paper, often usefollowing terminology goals F, want emphasise role and/ornature: roots trees F referred top-level goals, executablegoals referred actions, non-executable goals top-levelgoals referred sub-goals. Top-level goals classified reactivenon-reactive, explained later. 1 Note top-level (reactive) goalsmay actions.1. Roughly speaking, reactive goals generated response observations, e.g. communications receivedagents changes environment, example repair plans alreadygenerated. Non-reactive goals, hand, chosen desires agent.288fiComputational Logic Foundations KGP AgentsC Temporal Constraint Store, namely set constraint atoms givenunderlying constraint language. constrain time variables goals F.example, may specify time window time actioninstantiated, execution time.set equalities instantiating time variables time constants. example,time variables actions instantiated action execution time, recordsinstantiations kept .2.2 Reasoning CapabilitiesKGP supports following reasoning capabilities:Planning, generates plans mental goals given input. plans consisttemporally constrained sub-goals actions designed achieving input goals.Reactivity, used provide new reactive top-level goals, reactionperceived changes environment current plans held agent.Goal Decision, used revise non-reactive top-level goals, adaptingagents state changes environment.Identification Preconditions Identification Effects actions, useddetermine appropriate sensing actions checking whether actions may safelyexecuted (if preconditions known hold) whether recently executedactions successful (by checking known effects hold).Temporal Reasoning, allows agent reason evolving environment,make predictions properties, including non-executable goals, holdingenvironment, based (partial) information agent acquires lifetime.Constraint Solving, allows agent reason satisfiabilitytemporal constraints C .concrete realisation KGP model provide paper, chosenrealise capabilities various extensions logic programming paradigm.particular, use (conventional) logic programming Identification PreconditionsEffects, abductive logic programming constraints (see Section 3.2) Planning,Reactivity Temporal Reasoning, logic programming priorities (see Section 3.3)Goal Decision.2.3 Physical Capabilitiesaddition reasoning capabilities, KGP agent equipped physical capabilities, linking agent environment, consistingSensing capability, allowing agent observe properties holdhold, agents executed actions.Actuating capability, executing (physical communicative) actions.289fiKakas, Mancarella, Sadri, Stathis & Toni2.4 Transitionsstate hKB0 , F, C, agent evolves applying transition rules, employcapabilities follows:Goal Introduction (GI), possibly changing top-level goals F, using GoalDecision.Plan Introduction (PI), possibly changing F C using Planning.Reactivity (RE), possibly changing reactive top-level goals F C, usingReactivity capability.Sensing Introduction (SI), possibly introducing new sensing actions F checkingpreconditions actions already F.Passive Observation Introduction (POI), updating KB0 recording unsolicited information coming environment, using Sensing.Active Observation Introduction (AOI), possibly updating KB0 , recordingoutcome (actively sought) sensing actions, using Sensing.Action Execution (AE), executing types actions consequence updatingKB0 , using Actuating.State Revision (SR), possibly revising F, using Temporal Reasoning Constraint Solving.2.5 Cycle Selection Operatorsbehaviour agent given application transitions sequences, repeatedlychanging state agent. sequences determined fixed cycles behaviour, conventional agent architectures, rather reasoning cycle theories.Cycle theories define preference policies order application transitions,may depend environment internal state agent. rely uponuse selection operators detecting transitions enabled inputsbe, follows:action selection inputs AE; selection operator uses Temporal ReasoningConstraint Solving capabilities;goal selection inputs PI; selection operator uses Temporal ReasoningConstraint Solving capabilities;effect selection inputs AOI; selection operator uses IdentificationEffect reasoning capability;precondition selection inputs SI; selection operator uses IdentificationPreconditions, Temporal Reasoning Constraint Solving capabilities;290fiComputational Logic Foundations KGP Agentsprovision declarative control agents form cycle theories highlynovel feature model, could, principle, imported agent systems.concrete realisation KGP model provide paper, chosenrealise cycle theories framework logic programming prioritiesconstraints (see Section 3.3) also use Goal Decision.relationships capabilities, transitions selection operatorssummarised Tables 2.5 2 below. Table 2.5 indicates capabilities (rows)used transitions selection operators. Table 2 indicates selectionoperators used compute possible inputs transitions cycle theory.sensingactuating|=plan|=pre|=GD|=react|=T R|=cs|=ef fAExxTransitionsAOI GI P OIxxPISRSISelection operatorsfGS fAS fES fPxxxxxxxxxxxxxxxxTable 1: tabular overview use capabilities transitions selection operators.Here, |=plan , |=pre , |=GD , |=react , |=T R , |=cs |=ef f , stand for, respectively,planning, identification preconditions, goal decision, reactivity, temporal reasoning, constraint solving identification effects (reasoning) capabilities,fGS , fAS , fES , fP stand for, respectively, goal, action, effect preconditionselection operators.AEfGSfASfESfPAOIGIP OIPIxSRSIxxxTable 2: tabular overview connections selection operators transitions,required cycle theory. Here, fGS , fAS , fES , fP stand for, respectively,goal, action, effect precondition selection operators.provide components, though, introduce informally scenarioexamples used illustrate technical details KGP agent291fiKakas, Mancarella, Sadri, Stathis & Tonimodel throughout paper. full, formal presentation well additionalexamples given throughout paper Section 10.2.6 Examplesdraw examples ubiquitous computing scenario call SanVincenzo scenario, presented de Bruijn Stathis (2003) summarised follows.businessman travels work purposes Italy and, order make trip easier,carries personal communicator, namely device hybrid mobile phonePDA. device businessmans KGP agent. agent consideredpersonal service agent (Mamdani, Pitt, & Stathis, 1999) (or psa short)provides proactive information management flexible connectivity smart servicesavailable global environment within businessman travels within.2.6.1 Setting 1businessmans psa requests San Vincenzo Station agent, svs, arrival timetrain tr01 Rome. svs information answersrefusal. later, svs receives information arrival time tr01 trainCentral Office agent, co. psa requests arrival time tr01 again, svsaccept request provide information.first example requires one use Reactivity capability model rules interaction transition (a) achieve interaction amongst agents, (b) specifydynamic adjustments agents behaviour changes, allowing different reactionsrequest, depending current situation agent. Here, interactionform negotiation resources amongst agents, resources items information.Thus, current situation agents amounts resources/information agentscurrently own.example also requires combination transitions RE, POI, AE achieveexpected agents behaviours, follows:1. psa makes initial request applying AE2. svs becomes aware request performing POI (and changing KB0 accordingly)3. svs decides reply refusal performing (and adding correspondingaction plan F)4. svs utters refusal performing AE5. svs becomes aware, POI, arrival time (modifying KB0 accordingly)6. psa makes second request applying AE7. svs decides reply requested information performing (and addingcorresponding action plan F) communicates informationperforming AE.292fiComputational Logic Foundations KGP Agentssequence transitions given so-called normal cycle theorysee Section 9.2.6.2 Setting 2preparation businessmans next trip, psa aims getting plane ticketMadrid Denver well obtaining visa USA. One possible way buy planetickets internet. Buying tickets way usually possible,destinations (depending whether airlines flying destinations sell ticketsinternet not) without internet connection. psa currentlyconnection, information Denver indeed destination ticketsbought online. plans buy ticket internet nonetheless, conditionally,checks conditions executing planned action. successfully buyingticket, psa focuses second goal, obtaining visa. achievedapplying USA embassy Madrid, application requires addressUSA. address obtained arranging hotel Denver.example illustrates form partial planning adopted KGP model(where non-executable sub-goals well actions may part plans) showscombination transition PI SI AE allows psa agent deal partialinformation, generate conditional plans plans several layers, follows:1. psa initially equipped top-level goals get ticket Denverobtain visa (through earlier application GI)2. PI first goal, psa adds partial plan F, buying ticket onlinesubject sub-goals internet connection available onlinetickets bought Denver; sub-goals sensing goals3. SI, sensing actions added F evaluate sensing sub-goals environment4. sensing actions executed AE (and KB0 modified accordingly)5. depending sensed values sensing sub-goals buying action maymay executed AE; let us assume remainder exampleaction executed6. SR applied eliminate actions (since already executed), subgoals top-level goal getting ticket Denver (since achieved)7. PI remaining top-level goal obtaining visa, psa adds plan fillapplication form (action) acquiring residence address Denver (sub-goal)8. action cannot executed, psa knows businessman residentUSA; PI introduces plan sub-goal booking hotel (action)subgoal acquiring residence address Denver9. AE executes booking action293fiKakas, Mancarella, Sadri, Stathis & Toni10. AE executes action applying visa11. SR eliminates actions (since already executed), sub-goal toplevel goal getting visa (since achieved).3. Backgroundsection give necessary background reasoning capabilities cycletheory KGP agents, namely:Constraint Logic Programming, pervasive whole model,Abductive Logic Programming, heart Planning, Reactivity TemporalReasoning capabilities,Logic Programming Priorities, heart Goal Decision capabilityCycle Theories.3.1 Constraint Logic ProgrammingConstraint Logic Programming (CLP) (Jaffar & Maher, 1994) extends logic programmingconstraint predicates processed ordinary logic programming predicates,defined rules, checked satisfiability simplified means built-in,black-box constraint solver. predicates typically used constrain valuesvariables conclusion rule take (together unification alsotreated via equality constraint predicate). KGP model, constraints useddetermine value time variables, goals actions, suitable temporalconstraint theory.CLP framework defined structure < consisting domain D(<) setconstraint predicates includes equality, together assignment relationsD(<) constraint predicate. CLP, constraints built first-orderformulae usual way primitive constraints form c(t1 , . . . , tn ) cconstraint predicate symbol t1 , . . . , tn terms constructed domain, D(<),values. rules constraint logic program, P , take form rulesconventional logic programming givenH L1 , . . . , LnH (ordinary) atom, L1 , . . . , Ln literals, n 0. Literals positive, namelyordinary atoms, negative, namely form B, B ordinary atom,constraint atoms <. negation symbol indicates negation failure (firstintroduced Clark, 1978). variables H Li implicitly universally quantified,scope entire rule. H called head (or conclusion) L1 , . . . , Ln calledbody (or conditions) rule form above. n = 0, rule called fact.valuation, , set variables mapping variables domainD(<) natural extension maps terms D(<). valuation , setvariables appearing set constraints C, called <-solution C iff C, obtainedapplying C, satisfied, i.e. C evaluates true given interpretation294fiComputational Logic Foundations KGP Agentsconstraint predicates terms. denoted |=< C. set C called <-solvable<-satisfiable, denoted |=< C, iff least one <-solution, i.e. |=< Cvaluation .One way give meaning constraint logic program P consider grounding program Herbrand base possible valuations, D(<),constraint variables. rule, ground constraints C body evaluated true rule kept constraints C dropped, otherwise wholerule dropped. Let ground(P ) resulting ground program. meaning Pgiven meaning |=LP ground(P ), many different possiblechoices (Kakas, Kowalski, & Toni, 1998). resulting overall semantics constraintlogic program P referred |=LP (<) . precisely, given constraint logicprogram P conjunction N C (where N conjunction non-constraint literalsC conjunction constraint atoms), remainder paper writeP |=LP (<) N Cdenote exists ground substitution variables N C that:|=< Cground(P ) |=LP N .3.2 Abductive Logic Programming Constraintsabductive logic program constraints tuple h<, P, A, Ii where:< structure Section 3.1P constraint logic program, namely set rules formH L1 , . . . , LnSection 3.1set abducible predicates language P . predicatesoccurring head clause P (without loss generality, see (Kakas et al.,1998)). Atoms whose predicate abducible referred abducible atomssimply abducibles.set integrity constraints, is, set sentences language P .integrity constraints KGP model implicative formL1 , . . . , Ln A1 . . . (n 0, > 0)Li literals (as case rules) 2 , Aj atoms (possibly specialatom f alse). disjunction A1 . . . referred head constraintconjunction L1 , . . . , Ln referred body. variables integrityconstraint implicitly universally quantified outside, except variablesoccurring head, implicitly existentially quantified scopehead itself.2. n = 0, L1 , . . . , Ln represents special atom true.295fiKakas, Mancarella, Sadri, Stathis & ToniGiven abductive logic program constraints h<, P, A, Ii formula (query)Q, (implicitly existentially quantified) conjunction literals languageP , purpose abduction find (possibly minimal) set (ground) abducibleatoms which, together P , entails (an appropriate ground instantiation of) Q,respect notion entailment language P equipped with,extension P satisfies (see (Kakas et al., 1998) possible notionsintegrity constraint satisfaction). Here, notion entailment combinedsemantics |=LP (<) , discussed Section 3.1.Formally, given query Q, set (possibly non-ground) abducible atoms,set C (possibly non-ground) constraints, pair (, C) abductive answer (withconstraints) Q, respect abductive logic program constraints h<, P, A, Ii,iff groundings variables Q, , C |=< C, holds(i) P |=LP (<) Q,(ii) P |=LP (<) I, i.e. B H I, P |=LP (<) B P |=LP (<) H.Here, plays role earlier informal description abductive answer. Notealso that, (ii), integrity constraints classical implications.Note also that, representing knowledge abductive logic program, one needsdecide go logic program, integrity constraintsabducibles. Intuitively, integrity constraints normative needenforced, making sure head holds whenever body (by condition (ii)above), whereas logic programming rules enable, help abducibles, derivationgiven goals (by condition (i) above). Finally, abducibles chosen amongst literalscannot derived means logic programming rules. paper, represent reactive constraints (that condition-action rules forcing reactive behaviouragents) integrity constraints, thus extent addressing knowledge representation challenge posed abductive logic programming imposing sort structureabductive logic programs use.notion abductive answer extended take account initial set(possibly non-ground) abducible atoms 0 initial set (possibly non-ground)constraint atoms C0 . extension, abductive answer Q, respect(h<, P, A, Ii, 0 , C0 )pair (, C)(i) 0 = {}(ii) C C0 = {},(iii) ( 0 , C C0 ) abductive answer Q respect h<, P, A, Ii (inearlier sense).worth noticing abductive answer (, C) query true respect(h<, P, A, Ii, 0 , C0 )296fiComputational Logic Foundations KGP Agentsread fact abducibles 0 , along constraintsC0 C, guarantee overall consistency respect integrity constraints givenI. used specification capabilities KGP agents.remainder paper, simplicity, omit < abductive logicprograms, written simply triples hP, A, Ii. addition, abductive logicprograms present KGP variants core event calculus (Kowalski & Sergot,1986), define Section 5.1.1.3.3 Logic Programming Prioritiespurposes paper, logic program priorities constraint structure<, referred , consists four parts:(i) low-level basic part P , consisting logic program constraints; ruleP assigned name, term; e.g. one rule couldn(X, ) : p(X) q(X, ), r(Y )name n(X, ) naming ground instance rule;(ii) high-level part H, specifying conditional, dynamic priorities amongst rules PH; e.g. one priority couldh(X) : m(X) n(X) c(X)read: (some instance of) condition c(X) holds, (the corresponding instance of) rule named m(X) given higher priority (thecorresponding instance of) rule named n(X). rule named h(X);(iii) auxiliary part A, constraint logic program defining (auxiliary) predicatesoccurring conditions rules P, H conclusions ruleP H;(iv) notion incompatibility which, purposes, assumed givenset rules defining predicate incompatible/2, e.g.incompatible(p(X), p0 (X))read: instance literal p(X) incompatible correspondinginstance literal p0 (X). assume incompatibility symmetric alwaysincludes r incompatible r two rule names r, s. referset incompatibility rules I.concrete LPP framework equipped notion entailment, denote |=pr , defined top underlying logic programming constraintssemantics |=LP (<) . defined differently different approaches LPPshare following pattern. Given logic program priorities = hP, H, A, Iiconjunction ground (non-auxiliary) atoms, |=pr iff(i) exists subset P 0 basic part P P 0 |=LP (<) ,297fiKakas, Mancarella, Sadri, Stathis & Toni(ii) P 0 preferred wrt H subset P 00 P derives (under |=LP (<) )conclusion incompatible, wrt I, .framework way specifying meant one sub-theory P 0preferred another sub-theory P 00 . example, existing literature (Kakas et al.,1994; Prakken & Sartor, 1996; Kowalski & Toni, 1996; Kakas & Moraitis, 2003), |=prdefined via argumentation. also approach adopt, relying notionadmissible argument sub-theory (i) consistent (does incompatibleconclusions) (ii) whose rules lower priority, respect high-levelpart H theory, sub-theory incompatible conclusionsit. precise definition sets rules compared matterchoice specific framework LPP.Given concrete definition admissible sub-theories, preference entailment,|=pr , given by:(i) exists (maximal) admissible sub-theory 0 0 |=LP (<) ,(ii) incompatible exist admissible sub-theory00 00 |=LP (<) .first condition satisfied say theory credulously prefers possibly prefers . conditions satisfied saytheory sceptically prefers .4. State KGP AgentsSection define formally concept state KGP agent. also introducenotation use rest paper order refer state components.necessary, also try exemplify discussion simple examples.4.1 PreliminariesKGP model assume (possibly infinite) vocabularies of:fluents, indicated f, f 0 , . . .,action operators, indicated a, a0 , . . .,time variables, indicated , 0 , . . .,time constants, indicated t, t0 , . . . , 1, 2, . . ., standing natural numbers (we alsooften use constant indicate current time)names agents, indicated c, c0 , . . . .constants, ones mentioned above, normally indicated lower caseletters, e.g. r, r1 , . . .298fiComputational Logic Foundations KGP Agentsgiven constraint language, including constraint predicates <, , >, , =, 6=, respect structure < (e.g. natural numbers) equipped notionconstraint satisfaction |=< (see Section 3.1).assume set fluents partitioned two disjoint sets:mental fluents, intuitively representing properties agent able plansatisfied, also observed,sensing fluents, intuitively representing properties controlagent observed sensing external environment.example, problem f ixed resource may represent mental fluents, namelyproperties (given) problem fixed (given) resource obtained, whereas request accepted connection may represent sensing fluents, namelyproperties request (given) resource accepted (given)connection active. Note important distinguish mental sensingfluents treated differently control agent: mental fluents needplanned for, whereas sensing fluents observed. clarified laterpaper.also assume set action operators partitioned three disjoint sets:physical action operators, representing actions agent performs orderachieve specific effect, typically causes changes environment;communication action operators, representing actions involve communicationsagents;sensing action operators, representing actions agent performs establishwhether fluent (either sensing fluent expected effect action)holds environment, whether agent performed action.example, sense(connection on, ) action literal representing act sensing whether network connection time , do(clear table, ) action literal representing physical action removing every item given table,tell(c1 , c2 , request(r1 ), d, ) action literal representing communication actionexpresses agent c1 requesting agent c2 resource r1 within dialogueidentifier d, time 3 .fluent action operator associated arity: assume aritygreater equal 1, one argument (the last one, convention) alwaystime point given fluent holds given action takes place. time pointmay time variable time constant. Given fluent f arity n > 0, referf (s1 , . . . , sn1 , x) f (s1 , . . . , sn1 , x), si constant x timevariable time constant (timed) fluent literals 4 . Given fluent literal `, denote `3. role dialogue identifier become clearer Section 10. Intuitively, used linkcommunication actions occurring within dialogue.4. Note represents classical negation. Negation failure occurs model withinknowledge bases agents, supporting reasoning capabilities cycle theory. negationsstate understood classical negations.299fiKakas, Mancarella, Sadri, Stathis & Tonicomplement, namely f (s1 , . . . , sn1 , x) ` f (s1 , . . . , sn1 , x), f (s1 , . . . , sn1 , x)` f (s1 , . . . , sn1 , x). Examples fluent literals resource(pen, ), representingcertain resource pen obtained time , well (the ground)on(box, table, 10), representing time 10 (a certain) box (a certain)table.Note assume fluent literals ground except time parameter.allow us keep notation simpler highlight crucial role playedtime parameter. Given simplification, often denote timed fluent literals simply`[x].Given action operator arity n > 0, refer a(s1 , . . . , sn1 , x), siconstant x time variable time constant, (timed) action literal. Similarlycase fluent literals, simplicity, assume timed action literalsground except possibly time. Hence, often denote timed action literalsa[x].adopt special syntax sensing actions, always form (xeither time variable time constant):sense(f, x), f fluent,sense(c : a, x), c name agent action operator.first case, sensing action allows agent inspect external environmentorder check whether fluent f holds time x sensing. secondcase, sensing action allows agent determine whether, time x, another agent cperformed action a.define formally concept state hKB0 , F, C, agent.4.2 Forest: Fnode tree F is:either non-executable goal, namely (non-ground) timed fluent literal,executable goal, namely (non-ground) timed action literal.example tree F given Figure 2, p2 given problemagent (c1 ) needs fix getting two resources r1 r2 , agentalready decided get r1 agent c2 already planned ask c2communication action tell(c1 , c2 , request(r1 ), d, 4 ). example, San Vincenzoscenario, p2 may transfer airport needs arranged, r1 may taxi, c2taxi company, needed transportation train station, finally r2 maytrain ticket.Note time variable non-executable goals `[ ] actions a[ ] (any treein) F understood variable existentially quantified within whole stateagent. Whenever goal action introduced within state, time variableunderstood distinguished, fresh variable, also serving identifier.300fiComputational Logic Foundations KGP Agentsproblem f ixed(p2, 1 )PPPPPPPPPPresource(r1 , 2 )resource(r2 , 3 )tell(c1 , c2 , request(r1 ), d, 4 )Figure 2: example tree Findicated Section 2, roots trees referred top-level goals, executablegoals often called simply actions, non-executable goals may top-level goals subgoals. example, Figure 2, node identifier 1 top-level goal, nodesidentifiers 2 , 3 sub-goals node identifier 4 action.Notation 4.1 Given forest F tree F:node n , parent(n, ), children(n, ), ancestors(n, ), siblings(n, ),descendents(n, ), indicate parent node n , children n ,etc. leaf (n, ) value true n leaf , false otherwise.node n F, parent(n, F), children(n, F), ancestors(n, F), siblings(n, F),descendents(n, F), leaf (n, F) indicate parent(n, ) tree Fn occurs, etc. (T unique, due uniqueness time variable identifyingnodes).nodes(T ) represent set nodes , nodes(F) represent setnodes(F) = F nodes(T ).Again, indicated Section 2, top-level goal tree F eitherreactive non-reactive. see, Section 7, reactive top-level goals introduced state transition whereas non-reactive top-level goals introduced GI transition. example, F agent c1 may consist treeFigure 2, root non-reactive goal, well tree root reactive goal (action)301fiKakas, Mancarella, Sadri, Stathis & Tonitell(c1 , c2 , accept request(r3 ), d0 , 5 ). action may reply (planned agent c1 )request resource r3 agent c2 (for example, San Vincenzo scenario, r3may meeting requested colleague).Notation 4.2 Given forest FRootsr (F) (resp. Rootsnr (F)) denote set reactive (resp. non-reactive)top-level goals Fnodesr (F) (resp. nodesnr (F)) denote subset nodes(F) consisting nodestrees whose root Rootsr (F) (resp. Rootsnr (F))r(F) (resp. nr(F)) stands reactive (resp. non-reactive) part F, namelyset trees F whose root Rootsr (F) (resp. Rootsnr (F)).Trivially, r(F) nr(F) disjoint, F= r(F) nr(F).4.3 Temporal Constraint Store: Cset constraint atoms, referred temporal constraints, given underlyingconstraint language. Temporal constraints refer time constants well time variablesassociated goals (currently previously) state.example, given forest tree Figure 2, C may contain 1 > 10, 1 20,indicating top-level goal (of fixing problem p2) needs achieved within timeinterval (10, 20], 2 < 1 , 3 < 1 , indicating resources r1 r2 need acquiredtop-level goal deemed achieved, 4 < 2 , indicatingagent needs ask agent c2 first. Note need impose 2 3executed order, namely C may contain neither 2 < 3 , 3 < 2 .4.4 Agents Dynamic Knowledge Base: KB0KB0 set logic programming facts state agent, recording actionsexecuted (by agent others) time execution, wellproperties (i.e. fluents negation) observed timeobservation. Formally, facts following forms:executed(a, t) a[t] ground action literal, meaning actionexecuted agent time t.observed(`, t) `[t] ground fluent literal, meaning ` observedhold time t.observed(c, a[t0 ], t) c agents name, different name agentwhose state defining, t0 time constants, a[t0 ] (ground) actionliteral. means given agent observed time agent cexecuted action time t0 5 .5. see that, construction, always case t0 t. Note time executedactions, t0 , time observation, t, typically different concrete implementationKGP model, depend, example, time execution transitions withinoperational trace agent.302fiComputational Logic Foundations KGP AgentsNote facts KB0 variable-free, time variables occur them. Factsfirst kind record actions executed agent itself. Factssecond kind record observations made agent environment, excluding actionsexecuted agents, represented instead facts third kind.example, action labelled 4 Figure 2 executed (by AE transition)time 7 executed(tell(c1 , c2 , request(r1 ), d), 7) added KB0 . Moreover, if,time 9, c1 observes (e.g. transition POI) resource r2 , observationobserved(have resource(r2 ), 9) added KB0 . Finally, KB0 may containobserved(c2 , tell(c2 , c1 , request(r3 ), d0 , 1), 6)represent agent c1 become aware, time 6, agent c2 requested,earlier time 1, resource r3 c1 .4.5 Instantiation Time Variables:time variable occurring non-executable goal `[ ] action a[ ] Finstantiated time constant (e.g. action execution time), actual instantiation= recorded component state agent. example, actionlabelled 4 Figure 2 executed time 7, 4 = 7 added .use allows one record instantiation time variablestime keeping different goals fluent distinguished. Clearly, timevariable exists one equality = .Notation 4.3 Given time variable , denote ( ) time constant t, any,= .worth pointing valuation temporal constraint c C alwaystake equalities account. Namely, ground valuation temporalvariables c must agree temporal variables assigned .example, given = { = 3} C = {1 > }, 1 = 10 suitable valuation,whereas 1 = 1 not.5. Reasoning Capabilitiessection, give detailed specifications various reasoning capabilities, specified within framework ordinary logic programming (for Temporal ReasoningIdentification Preconditions Effects), Abductive Logic Programming Constraints (Section 3.2, Planning Reactivity), Logic Programming PrioritiesConstraints (Section 3.3, Goal Decision), constraint programming (Section 3.1,Constraint Solving).reasoning capabilities defined means notion entailment respectappropriate knowledge base (and time point now, appropriate), follows:303fiKakas, Mancarella, Sadri, Stathis & Toni|=T R KBT R Temporal Reasoning, KBT R constraint logic programvariant framework Event Calculus (EC) reasoning actions,events changes (Kowalski & Sergot, 1986) 6 ;|=nowplan KBplan Planning, KBplan abductive logic programconstraints, extending KBT R ;|=nowreact KBreact Reactivity, KBreact extension KBplan , incorporating additional integrity constraints representing reactive rules;|=pre KBpre , KBpre logic program contained KBT R ;|=ef f KBef f , KBef f logic program contained KBT R ;|=nowGD KBGD , KBGD logic program priorities constraints.constraint solving capability defined terms entailment |=csbasically |=< defined Section 3.1.5.1 Temporal Reasoning, Planning, Reactivity, Identification PreconditionsEffects: EC-based Capabilitiesreasoning capabilities specified within framework event calculus(EC) reasoning actions, events changes (Kowalski & Sergot, 1986). Below,first give core EC show use define various capabilitiessection.5.1.1 Preliminaries: Core Event Calculusnutshell, EC allows one write meta-logic programs talk objectlevel concepts fluents, events (that interpret action operators) 7 , time points.main meta-predicates formalism are:holds at(F, ) - fluent F holds time ;clipped(T1 , F, T2 ) - fluent F clipped (from holding holding) timesT1 T2 ;declipped(T1 , F, T2 ) - fluent F declipped (from holding holding)times T1 T2 ;initially(F ) - fluent F holds initial time, say time 0;happens(O, ) - operation happens time ;initiates(O, T, F ) - fluent F starts hold operation time ;6. sophisticated, abductive logic programming version |=T R KBT R given BraccialiKakas (2004).7. section use original event calculus terminology events instead operators,rest paper.304fiComputational Logic Foundations KGP Agentsterminates(O, T, F ) - fluent F ceases hold operation time .Roughly speaking, last two predicates represent cause-effects links operations fluents modelled world. also use meta-predicateprecondition(O, F ) - fluent F one preconditions executabilityoperation O.Fluent literals agents state mapped onto EC follows. EC-like representation fluent literal f [ ] (resp. f [ ]) agents state atom holds at(f, )(resp. holds at(f, )). Moreover, arguments time variable needconsidered, EC representation fluent literal f (x1 , . . . , xn , ) (resp. f (x1 , . . . , xn , ))holds at(f (x1 , . . . , xn ), ) (resp. holds at(f (x1 , . . . , xn ), ). 8Similarly, action literals state agent represented ECstraightforward way. Given action literal a[ ] EC representation happens(a, ).arguments time considered, e.g. a(x1 , . . . , xn , ), EC representation given happens(a(x1 , . . . xn ), ).remainder paper, abuse terminology, sometimes referf (x1 , . . . , xn ) f (x1 , . . . , xn ) interchangeably fluent literals fluents (althoughstrictly speaking fluent literals), a(x1 , . . . xn ) interchangeably actionliterals action operators (although strictly speaking action literals).EC allows one represent wide variety phenomena, including operationsindirect effects, non-deterministic operations, concurrent operations (Shanahan, 1997).core EC use paper consists two parts: domain-independent rulesdomain-dependent rules. basic domain-independent rules, directly borrowedoriginal EC, are:holds at(F, T2 )holds at(F, T2 )holds at(F, )holds at(F, )clipped(T1 , F, T2 )declipped(T1 , F, T2 )happens(O, T1 ), initiates(O, T1 , F ),T1 < T2 , clipped(T1 , F, T2 )happens(O, T1 ), terminates(O, T1 , F ),T1 < T2 , declipped(T1 , F, T2 )initially(F ), 0 T, clipped(0, F, )initially(F ), 0 T, declipped(0, F, )happens(O, ), terminates(O, T, F ), T1 < T2happens(O, ), initiates(O, T, F ), T1 < T2domain-dependent rules define initiates, terminates, initially, e.g. casesetting 2.6.1 Section 2.6 mayinitiates(tell(C, svs, inf orm(Q, I), D), T, inf o(svs, Q, I))holds at(trustworthy(C), )initially(have inf o(svs, arrival(tr01), I)8. Note write holds at(f (x1 , . . . , xn ), ) instead holds at(f (x1 , . . . , xn ), ), done e.g.Shanahan, 1997, want reason object-level properties true falseenvironment. use within meta-level axioms event calculus (see below) implementpersistence.305fiKakas, Mancarella, Sadri, Stathis & Toniinitially(trustworthy(co))Namely, action agent C providing information concerning query Qagent svs (the San Vincenzo station agent) initiates agent svs informationQ, provided C trustworthy. Moreover, initially agent co (the Central Officeagent) trustworthy, agent svs information arrival time tr01.conditions rule defining initiates seen preconditions effectsoperator tell take place. Preconditions executability operators specifiedmeans set rules (facts) defining predicate precondition, e.g.precondition(tell(svs, C, inf orm(Q, I), D), inf o(svs, Q, I))namely precondition agent svs inform agent C Q svs indeedinformation Q.Notice presence language fluents negation, e.g. f f ,poses problem inconsistencies, i.e. may case holds at(f, t)holds at(f, t) derived axioms set events (i.e. given sethappens atoms). However, easily shown never case, provideddomain-dependent part contain two conflicting statements forminitially(f ) initially(f ) since inconsistencies cannot caused except initialtime point (see e.g. Miller & Shanahan, 2002, p. 459).remainder paper assume domain-dependent part alwaysconsistent agents.allow agents draw conclusions contents KB0 , representsnarrative part agents knowledge, add domain-independent rulesfollowing bridge rules:holds at(F, T2 )holds at(F, T2 )happens(O, )happens(O, )observed(F, T1 ), T1 T2 , clipped(T1 , F, T2 )observed(F, T1 ), T1 T2 , declipped(T1 , F, T2 )executed(O, )observed( , O[T ], )Notice bridge rules make explicit translation state representationEC representation fluents actions mentioned earlier section.Note also assume fluent holds time observed hold.choice dictated rationale observations considered reasonedupon moment agent makes them. hand, actions agentseffect time executed 9 .introduced ability reason narratives events observations,need face problem inconsistency due conflicting observations, e.g. agentmay observe fluent negation hold time. done9. time action unknown observation time, last rule may replacedhappens(O, ) observed( , O[ ], )namely value fluent changed according observations moment observationsmade.306fiComputational Logic Foundations KGP Agentsset initially atoms, assume external world consistenttoo, i.e. never happen observed(f, t) observed(f, t) belong KB0 ,fluent f time point t.However, still need cope frame consistency problem, arises, e.g.given observations observed(f, t) observed(f, t0 ), 6= t0 . issue analogouscase two different events happen time point initiateterminate fluent. original EC suitable axioms predicates clippeddeclipped added, given above, avoid fluent negation holdingtime happening two events time. adoptsimilar solution cope observations, namely adding following two axiomsdomain-independent part:clipped(T1 , F, T2 )declipped(T1 , F, T2 )observed(F, ), T1 < T2observed(F, ), T1 < T2solution may naive circumstances sophisticated solutions mayadopted, e.g. one proposed Bracciali Kakas (2004).5.1.2 Temporal Reasoningtemporal reasoning capability invoked components KGP model(namely Goal Decision capability, State Revision transition selection operators, see Section 7) prove disprove given (possibly temporallyconstrained) fluent literal holds, respect given theory KBT R . purposespaper KBT R EC theory composed domain-independent domaindependent parts given Section 5.1.1, narrative part given KB0 . Then,given state S, fluent literal `[ ] possibly empty set 10 temporal constraints C,temporal reasoning capability |=T R defined|=T R `[ ] C iff KBT R |=LP (<) holds at(`, ) C.example, given EC formulation Section 5.1.1 setting 2.6.1 Section 2.6,state = hKB0 , F, C, agent svs containsKB0 = {observed(co, tell(co, svs, inf orm(arrival(tr01), 18), d, 15), 17)},|=T R inf o(svs, arrival(tr01), 18, ) > 20.5.1.3 Planningnumber abductive variants EC proposed literature dealplanning problems, e.g. one proposed Shanahan, 1989. Here, propose novelvariant, somewhat inspired E-language (Kakas & Miller, 1997), allow situatedagents generate partial plans dynamic environment.refer KBplan = hPplan , Aplan , Iplan abductive logic program where:10. remainder paper sets seen conjunctions, appropriate.307fiKakas, Mancarella, Sadri, Stathis & ToniAplan = {assume holds, assume happens}, namely consider two abducible predicates, corresponding assuming fluent holds action occurs, respectively, certain time point;Pplan obtained adding core EC axioms narrative given KB0following ruleshappens(O, ) assume happens(O, )holds at(F, ) assume holds(F, )Iplan contains following set integrity constraintsholds at(F, ), holds at(F, ) f alseassume happens(O, ), precondition(O, P ) holds at(P, )assume happens(O, ), executed(O, ), time now(T 0 ) > 0integrity constraints Iplan prevent generation (partial) plansunfeasible. first integrity constraint makes sure plan generated entailsfluent negation hold time. second integrity constraint makessure that, plan requires action occur certain time point, goalenforcing preconditions action hold time point taken accountplan. means that, preconditions already known hold,plan need accommodate actions guarantee hold timeexecution action. Finally, last integrity constraint forces assumed unexecutedactions plan executable future, predicate time now( ) meantreturn current time.worth recalling that, concrete situations, Pplan Iplan also contain domaindependent rules constraints. Domain-dependent rules may needed defineinitiates, terminates, initially precondition, may also contain additionalrules/integrity constraints expressing ramifications, e.g.holds at(f, ) holds at(f1 , ) . . . holds at(fn , )specific fluents domain. Moreover, integrity constraints may representspecific properties actions fluents domain. example, domain-dependentconstraint could express two actions type cannot executed time,e.g.holds at(tell(c, X, accept request(R), D), ),holds at(tell(c, X, ref use request(R), D), ) f alseIntuitively, constructing (partial) plan goal (that given leaf nodecurrent forest) amounts identifying actions sub-goals allowing achievegoal, assuming nodes forest, executable non-executable,feasible. Concretely, abductive logic program KBplan supports partial planningfollows. Whenever plan given goal requires agent execute action, a[ ] say,corresponding atom assume happens(a, ) assumed, amounts intendingexecute action (at concrete time instantiating ). hand,plan given goal requires plan sub-goal, `[ ] say, corresponding atomassume holds(`, ) may assumed, amounts setting requirementplanning needed sub-goal itself. Notice total plans takenaccount, atoms form assume holds( , ) ever generated.308fiComputational Logic Foundations KGP AgentsKBFormally, let KBplanplan {time now(now)}, time constant (intuitively, time planning capability invoked). Then, planning capability11 .|=nowplan specified followsLet = hKB0 , F, C, state, G = `[ ] mental goal labeling leaf nodetree F. Let alsoCA = {assume happens(a, 0 ) | a[ 0 ] nodes(F)},CG = {assume holds(`0 , 0 ) | `0 [ 0 ] nodes(F) \ {`[ ]}}0 = CA CGC0 = C .Then,S, G |=nowplan (Xs , C)iffXs = {a[ 0 ] | assume happens(a, 0 ) } {`0 [ 0 ] | assume holds(`0 , 0 ) }, , C ).(, C) abductive answer holds at(`, ), wrt (KBplan00abductive answer exists, S, G |=now,usedindicatefailureplan(i.e. abductive answer exists).example, consider setting 2.6.2 Section 2.6. domain-dependent partKBplan agent psa (looking businessman scenario) containsinitiates(buy ticket online(F rom, o), T, ticket(F rom, o))precondition(buy ticket online(F rom, o), available connection)precondition(buy ticket online(F rom, o), available destination(T o))goal G ticket(madrid, denver, ). Assume F consists single treeconsisting solely root G, thus CA = CG = {}. Then, S, G |=nowplan (Xs , C)Xs = {buy ticket online(madrid, denver, 0 ),available connection( 00 ), available destination(denver, 000 )}C = { 0 < , 0 = 00 = 000 , 0 > now}.5.1.4 Reactivitycapability supports reasoning reacting stimuli external environmentwell decisions taken planning.knowledge base KBreact supporting reactivity adopt extension knowledgebase KBplan follows. KBreact = hPreact , Areact , IreactPreact = Pplan11. simplicity present case planning single goals only.309fiKakas, Mancarella, Sadri, Stathis & ToniAreact = AplanIreact = Iplan RRRR set reactive constraints, formBody Reaction, CReaction either assume holds(`, ), `[T ] timed fluent literal,assume happens(a, ), a[T ] timed action literal, 12Body non-empty conjunction items form (where `[X] timed fluentliteral a[X] timed action literal, X):(i) observed(`, 0 ),(ii) observed(c, a[T 0 ], 00 ),(iii) executed(a, 0 ),(iv) holds at(`, 0 ),(v) assume holds(`, 0 ),(vi) happens(a, 0 ),(vii) assume happens(a, 0 ),(viii) temporal constraints (some of) T, 0 , 00contains least one item one (i), (ii) (iii).C temporal constraints (some of) T, 0 , 00 .integrity constraints abductive logic programming, variables Bodyimplicitly universally quantified whole reactive constraint, variablesReaction, C occurring Body implicitly existentially quantified righthandside reactive constraint. 13Notice Body must contain least trigger, i.e. condition evaluatedKB0 . Intuitively, reactive constraint Body Reaction, C interpretedfollows: (some instantiation of) observations Body hold KB0 (somecorresponding instantiation of) remaining conditions Body hold, (the appropriate instantiation of) Reaction, associated (the appropriate instantiation of)12. below, abuse notation, use notions timed fluent action literals liberallyallow non-ground, even though defined timed fluent action literals groundexcept possibly time parameter.13. Strictly speaking, syntactically reactive constraints integrity constraints (due presenceconjunction, represented ,, rather disjunction head). However, reactive constraintBody Reaction, C transformed integrity constraint Body N ew new clauseN ew Reaction, C Preact . Thus, abuse notation, treat reactive constraintsintegrity constraints.310fiComputational Logic Foundations KGP Agentstemporal constraints C, added F C, respectively. Notice Reactionabducible planning performed reactivity capability.theory KBFormally, let KBreactreact {time now(now)}, timeconstant (intuitively, time capability invoked). Then, reactivity capability |=nowreact specified follows. Let = hKB0 , F, C, state. LetCA = {assume happens(a, ) | a[ ] nodesnr (F)},CG = {assume holds(`, ) | `[ ] nodesnr (F)}0 = CA CGC0 = C .Then,|=nowreact (Xs , C)iffXs = {a[ ] | assume happens(a, ) } {`[ ] | assume holds(`, ) }, , C ).(, C) abductive answer query true wrt (KBreact00abductive answer exists, |=react , used indicate failure(i.e. abductive answer exists).example, consider setting 2.6.1 Section 2.6, KBplan given Sections 5.1.15.1.3. Let RR agent svs consist of:observed(C, tell(C, svs, request(Q), D, 0), ), holds at(have inf o(svs, Q, I), )assume happens(tell(svs, C, inf orm(Q, I), D), 0 ), 0 >observed(C, tell(C, svs, request(Q), D, 0), ), holds at(no inf o(svs, Q), )assume happens(tell(svs, C, ref use(Q), D), 0 ), 0 >Then, given = 30 = hKB0 , F, C,KB0 = {observed(co, tell(co, svs, inf orm(arrival(tr01), 18), d1, 15), 17),observed(psa, tell(psa, svs, request(arrival(tr01)), d2, 20), 22)}obtain|=nowreact ({tell(svs, psa, inf orm(arrival(tr01), 18), d2, )}, > 30).5.1.5 Identification Preconditionscapability used KGP agents determine preconditions executabilityactions planned for. preconditions defined domain-dependentpart EC means set rules form precondition(O, F ), representingfluent F precondition executability action action operator (see5.1.1). Let KBpre subset KBT R containing rules defining precondition( , ).311fiKakas, Mancarella, Sadri, Stathis & Toniidentification preconditions capability |=pre specified follows. Given state= hKB0 , F, C, timed action literal a[ ]S, a[ ] |=pre CsiffCs =V{`[ ] | KBpre |=LP precondition(a, `)}14 .5.1.6 Identification Effectscapability used KGP agents determine effects actions alreadyexecuted, order check whether actions successful. Noteactions may unsuccessful could executed, executedexpected effect. possible situations agentfull knowledge environment situated.effects defined domain-dependent part EC means setrules defining predicates initiates terminates. Let KBef f theory consistingdomain-dependent domain-independent parts EC, well narrativepart KB0 . Then, identification effects |=ef f specified follows. Given state= hKB0 , F, C, action operator a[t],S, a[t] |=ef f `iff` = f KBef f |=LP initiates(a, t, f )` = f KBef f |=LP terminates(a, t, f )5.2 Constraint SolvingConstraint Solving capability simply defined terms structure <|=< notion presented Section 3.1. Namely, given state = hKB0 , F, C,set constraints C:|=cs C iff |=< C C;exists total valuation S, |=cs C iff exists total valuation|=< C C.5.3 Goal DecisionGoal Decision reasoning capability allows agent decide, given time point,(non-reactive) top-level goals pursued, go generateplans aiming achieving them. generated goals goals current preferredinterest interest may change time.14. assumeV{} = true.312fiComputational Logic Foundations KGP AgentsGoal Decision capability operates according theory, KBGD , agentrepresents goal preference policy. KBGD includes KBT R thus dynamic, observedknowledge, KB0 , current state agent. KBGD expressed variant LPPdescribed Section 3.3, whereby rules lower basic part P LPP theoryform (T possibly empty sequence variables):n(, ) : G[, ] B[T ], C[T ]time variable, existentially quantified scope head rulemember ;variables except universally quantified scope rule;head G[, ] rule consists fluent literal conjoined (possiblyempty) set temporal constraints, represented h`[ ], C[, ]i;B(T ) non-empty conjunction literals set auxiliary predicatesinclude atoms form holds at(`, 0 ), `[T 0 ] timed fluent literal,atom time now(T 00 ) variables 0 , 00 ;conditions rule constrained (possibly empty) temporal constraintsC[T ].rule represents ground instances total valuationvariables satisfies constraints C[T ]. ground instance namedcorresponding ground instance n(, ). Intuitively, conditions one rulesatisfied time grounds variable 00 current timecapability applied, goal head rule sanctioned one goalsagent would possibly prefer achieve time. decision whethergoal indeed preferred would depend high-level strategy part H KBGD ,containing priority rules, described Section 3.3, rules lower-partrules H. priority rules also include temporal atoms formholds at(`, 0 ) atom time now(T 00 ) conditions.accommodate form rules need extend notion incompatibilitydefined conclusions h`( ), C[, ]i. simplify notation,remainder often write h`( ), Ci instead h`( ), C[, ]i.incompatibility defined different ways. example, (relatively) weaknotion incompatibility given follows. Two pairs h`1 (1 ), C1 h`2 (2 ), C2incompatible iff every valuation C1 C2 satisfied, groundinstances `1 (1 ) `2 (2 ) incompatible. stronger notion would requiresufficient one valuation exist makes corresponding groundliterals incompatible.theory KBLet us denote KBGDGD {time now(now)}, timeconstant. Then, goal decision capability, |=nowGD , defined directly termspreference entailment, |=pr , LPP (see Section 3.3), follows.Given state = hKB0 , F, C, i,|=nowGD Gs313fiKakas, Mancarella, Sadri, Stathis & ToniGs = {G1 , G2 , . . . , Gn }, n 0, Gi = h`i (i ), Ci = 1, . . . , niff Gs maximal setKBGD|=pr h`1 (1 ), C1 . . . h`n (n ), Cn i.means new set goals Gs generated currently (sceptically) preferredgoal preference policy represented KBGD current information KB0 .Note two goals Gs necessarily compatible other. twospecial cases sceptically preferred goals time now. first oneconcerns case goals currently sanctioned (lower-part)KBGD . |=nowGD returns empty set goals (n = 0). second specialcase occurs least two goals separately credulously preferredgoals incompatible other. |=nowGD , usedindicate failure identifying new goals pursued.example, consider San Vincenzo scenario psa agent needs decide whether return home recharge battery. agents goals categorisedassigned priority according category possibly factors. KBGDexpressing given follows:low-level part contains rules:n(rh, 1 ) : hreturn home(1 ), {1 < 0 }iholds at(f inished work, ),holds at(at home, ),time now(T ),T0 = + 6n(rb, 2 ) : hrecharge battery(2 ), {2 < 0 }iholds at(low battery, ),time now(T ),T0 = + 2auxiliary part contains, addition KBT R KB0 , following rulesspecify category goal relative urgency categories:typeof (return home, required)typeof (recharge battery, operational)urgent wrt type(operational, required)314fiComputational Logic Foundations KGP Agentsincompatibility part consistsincompatible(return home(T ), recharge battery(T ))Namely, two goals pairwise incompatible, i.e. agent onegoals time.high-level part contains following priority rule:gd pref (X, ) : n(X, ) n(Y, ) typeof (X, XT ),typeof (Y, ),urgent wrt type(XT, ).Then, = 1 current state = hKB0 , F, C, finished workaway home hold (by temporal reasoning) time now,|=nowGD {hreturn home(1 ), {1 < 7}i}.Suppose instead KB0 contains observed(low battery, 1). Then, using weaknotion incompatibility, requiringevery |=cs {1 < 7, 2 < 3}holds incompatible(return home(1 ), recharge battery(2 ))have:|=nowGD {hreturn home(1 ), {1 < 7}i, hrecharge battery(2 ), {2 < 3}i}.Indeed, = {1 = 3, 2 = 2}, incompatible(return home(3), recharge battery(2))hold. However, using stronger notion incompatibility, requiringexists |=cs {1 < 7, 2 < 3}holds incompatible(return home(1 ), recharge battery(2 ))have:|=nowGD {hrecharge battery(2 ), {2 < 3}i}.Suppose KBGD contains second operational goal hreplace part(3 ), {3 < 5}ialso sanctioned rule lower part time = 1. strongerform incompatibility goal decision capability = 1 returnoperational goals credulously preferred none sceptically preferred.315fiKakas, Mancarella, Sadri, Stathis & Toni6. Physical Capabilitiesaddition reasoning capabilities defined far, agent equippedphysical capabilities allow experience world situated; worldconsists agents and/or objects provide environment agentsinteract communicate.identify two types physical capabilities: sensing actuating. representingcapabilities abstract away sensors actuators agent wouldtypically rely upon access affect environment. also assumesensors actuators part agents body, classify implementationissue (Stathis et al., 2004).physical sensing capability models way agent interacts externalenvironment order inspect it, e.g. find whether fluent holdsgiven time. hand, physical actuating capability models wayagent interacts external environment order affect it, physically executingactions.represent sensing physical capability agent function form:sensing(L, t) = L0where:L (possibly empty) setfluent literals f ,terms form c : (meaning agent c performed action a),sensed concrete time t,L0 (possibly empty) set elements s0s0 term f : v, f fluent v {true, f alse}, meaning fluent fobserved value v (namely true f alse) time t,s0 term form c : a[t0 ], c agent name action,meaning agent c performed action time t0 .Note physical sensing requires time-stamp specify timeapplied within transitions. Note also that, given non-empty set L, sensing(L, t) maypartial, e.g. fluent f L, neither f : true L0 , f : f alse L0 .Similarly, represent physical actuating capability functionactuating(As, t) = As0where:set action literals {a1 , , }, n > 0, agent instructs bodyactuate time t;316fiComputational Logic Foundations KGP AgentsAs0 subset actions body actually managed perform.meaning action belonging belonging As0 physicalactuators agents body able perform current situation. worthpointing action belongs As0 necessarily mean effectssuccessfully reached. Indeed, preconditions executedaction (i) may wrongly believed agent true execution time (asagents may interfered them) (ii) agent may unawarepreconditions. example, confirmed availability, agent maybooked hotel sending e-mail, (i) agent booked last availableroom meanwhile, (ii) agent provide credit card number securebooking. words, beliefs agent (as held KB0 ) may incorrect and/orincomplete.Section 7 Section 8 below, see AOI (Active Observation Introduction)used check effects actions (identified fES effect selection operator,turn using |=ef f reasoning capability) actions executed. Moreover, SI(Sensing Introduction) used check preconditions actions (identified fPprecondition selection operator, turn using |=pre reasoning capability)executed, make sure actions indeed executable. Overall, followingcases may occur:action belongs As0 executedpreconditions held time execution effects hold environment execution;preconditions wrongly believed hold time execution (becauseagent partial knowledge environment KBplan incorrect)consequence effects hold execution;preconditions known hold time execution (e.g.agent observed planned hold,time -replan) consequence effects hold execution;action belongs \ As0 executed (the body could executeit).actuating physical capability check preconditions/effects: leftcapabilities called within transitions and/or transition invokingactuating, show below. before, way body carry actionsimplementation issue (Stathis et al., 2004).7. TransitionsKGP model relies upon state transitions GI, PI, RE, SI, POI, AOI, AE, SR, definedusing following representation(T)hKB0 , F, C,X0000hKB0 , F , C ,317fiKakas, Mancarella, Sadri, Stathis & Toniname transition, hKB0 , F, C, agents state transition applied, X input transition, time applicationtransition, hKB00 , F 0 , C 0 , 0 revised state, resulting application transition input X time state hKB0 , F, C, i. Please note transitionsmodify components state. Also, transitions (namely GI,RE, POI, SR) input X always empty omitted. transitions(namely PI, SI, AOI, AE) input always non-empty (see Section 9) selectedappropriate selection operator (see Section 8).define transition formally, defining hKB00 , F 0 , C 0 , 0 i. Noteassume transition takes care possible renaming time variables outputcapabilities (if capability used transition), order guaranteegoal/action forest univocally identified time variable.7.1 Goal Introductiontransition takes empty input. calls Goal Decision capability determinenew (non-reactive) top-level goals agent. capability returns set goals,means circumstances possibly changed preferred top-level goalsagent transition reflect changing forest new stateconsist one tree new (non-reactive) goal. hand, Goal Decisioncapability return (non-reactive) goals (namely returns ) state leftunchanged, as, although goals current state longer sceptically preferredmay still credulously preferred and, since others replace them,agent carry current plans achieve them.(GI)hKB0 , F, C,hKB0 , F 0 , C 0 ,where, given = hKB0 , F, C,(i) |=nowGD ,F0 = FC0 = C(ii) otherwise, |=nowGD Gs Gs 6= ,F 0 defined follows:nr(F 0 ) = {Tg[ ] | hg[ ], Gs} Tg[ ] tree consisting solelyroot g[ ]r(F 0 ) = {}C 0 = {T C | h , Ci Gs}transition drops (top-level) goals become semantically irrelevant (duechanged circumstances agent changes environment), replacesnew relevant goals. see, Section 7.8, goals also dropped318fiComputational Logic Foundations KGP Agentsbook-keeping activities State Revision (SR) transition, transitionnever add set goals.Note that, GI replace whole forest old state new forest,possible agent looses valuable information achieving goals,one new preferred goals agent (or equivalent to) current goal.effect though minimized calling (in cycle theory) GI transitioncertain times, e.g. current goals achieved timed-out. Alternatively,earlier formalisation GI transition could modified that, case (ii),goals Gs already occur (modulo temporal variables associated temporalconstraints) roots (non-reactive) trees F, trees kept F 0 . simple waycharacterise (some of) goals follows. LetXs = {hg[ ], C, = 0 |hg[ ], Ci Gs,g[ 0 ] Rootsnr (F)|=cs C iff |=cs (C C { = 0 })}Gs0 = {hg[ ], Ci |hg[ ], C, = 0 Xs}new constraints goals Gs0 equivalent old constraints C. example,Gs may containG = hhave ticket(madrid, denver, 2 ), {2 < 12}iticket(madrid, denver, 1 ) Rootsnr (F) C = {1 < 12}.Then, G definitely belongs Gs0 . LetnewC =[C { = 0 }.h ,T C, = 0 iXsCase (ii) redefined follows, using definitions Xs, Gs0 newC:(ii0 ) otherwise, |=nowGD Gs Gs 6= , then, case |=cs C newC,F 0 C 0 defined earlier case (ii), otherwise (if |=cs C newC):F 0 defined follows:nr(F 0 ) = {Tg[ ] | hg[ ], Gs \ Gs0 } F(Xs)Tg[ ] tree consisting solely root g[ ]F(Xs) set trees F roots goals form g[ 0 ]hg[ ], , = 0 Xsr(F 0 ) = {}C 0 = C {T C | h , Ci Gs \ Gs0 } newC.Note keep temporal constraints state, prior application GI,force variables new goals remain state GI rewrittenusing old identifiers goals.319fiKakas, Mancarella, Sadri, Stathis & Toni7.2 Reactivitytransition takes empty input. calls Reactivity capability order determinenew top-level reactive goals state (if any), leaving non-reactive part unchanged.new reactive goals exist, reactive part new state empty.hKB0 , F, C,hKB0 , F 0 , C 0 ,(RE)where, given = hKB0 , F, C, i:(i) |=nowreact ,F 0 defined follows:r(F 0 ) = {}nr(F 0 ) = nr(F)C0 = C(ii) otherwise, |=nowreact (X s, C),F 0 defined follows:nr(F 0 ) = nr(F)r(F 0 ) = {Tx[ ] | x[ ] X s}Tx[ ] tree consisting solely root x[ ]C0 = C CNote asymmetry case (ii) GI case (ii) RE, GIeliminates reactive goals case, whereas leaves non-reactive goals unchanged.Indeed, reactive goals may due choice specific non-reactive goals,latter change former need re-evaluated. Instead, non-reactive goals affectednewly acquired reactive goals (that outcome enforcing reactive rules).Note also case (ii), similarly GI, replaces whole (reactive) forestold state new (reactive) forest, possible agent loses valuableinformation achieving reactive goals, one new reactive goals(or equivalent to) current goal. variant case (ii) RE, mirroringvariant given earlier GI using |=cs well, defined avoid problem.7.3 Plan Introductiontransition takes input non-executable goal state (that selectedgoal selection operator, see Section 8) produces new state calling agentsPlanning capability, selected goal mental goal, simply introducing newsensing action, goal sensing goal.(PI)hKB0 , F, C,G00hKB0 , F , C ,G input goal (selected planning tree F, thus leaf, seeSection 8)320fiComputational Logic Foundations KGP AgentsF 0 = (F \ {T | G leaf }) N ewC0 = C CN ew C obtained follows, hKB0 , F, C, i.(i) G mental goal: let S, G |=nowplan P . Then,either P =N ew = {T } C = {},P = (X s, C)N ew = {T 0 } 0 obtained adding element Xchild G.(ii) G = `[ ] sensing goal, child goal G0 :N ew = {T 0 } 0 (a node labelled by) sense(`, 0 ) new child G0(here 0 new time variable)C = { 0 }.(iii) G = `[ ] sensing goal, root :N ew = {T , 0 } 0 tree consisting solely root (labelled by) sense(`, 0 )(here 0 new time variable)C = { 0 }.7.4 Sensing Introductiontransition takes input set fluent literals preconditions actionsstate produces new state adding sensing actions leaves (appropriate)trees forest component. Note that, SI invoked, input fluent literalsselected precondition selection operator, chosen amongst preconditionsactions already known true (see Section 8).(SI)hKB0 , F, C,SP00hKB0 , F , C ,SP non-empty set preconditions actions (in form pairs precondition,action) trees F, where, given that:- N ew = {h`[ ], A, sense(`, 0 )i | h`[ ], Ai SP 0 fresh variable}- addSibling(T , A, SA) denotes tree obtained adding elements SA newsiblings tree leaf (A, )F 0 = F \ {T | leaf (A, ) h`[ ], Ai SP s}{addSibling(T , A, SA) | leaf (A, )SA = {sense(`, 0 )|h`[ ], A, sense(`[ 0 ])i N ew}}C 0 = C { 0 < | h`[ ], , sense(`[ 0 ])i N ew}321fiKakas, Mancarella, Sadri, Stathis & ToniBasically, fluent literal selected precondition selection operatorprecondition action A, new sensing action added sibling A,constraint expressing sensing action must performed addedcurrent set temporal constraints.7.5 Passive Observation Introductiontransition updates KB0 adding new observed facts reflecting changes environment. observations deliberately made agent, rather,forced upon agent environment. observations may propertiesform positive negative fluents (for example battery running out) actionsperformed agents (for example messages addressed agent).hKB0 , F, C,hKB00 , F, C,(POI)where, sensing(, now) = L,KB00 = KB0{observed(f, now) | f : true L}{observed(f, now) | f : f alse L}{observed(c, a[t], now) | c : a[t] L}.7.6 Active Observation Introductiontransition updates KB0 adding new facts deliberately observed agent,seeks establish whether given fluents hold given time. fluentsselected effect selection operator (see Section 8) given input transition.Whereas POI decided agent (the agent interrupted forcedobservation environment), AOI deliberate. Moreover, POI may observe fluentsactions, whereas AOI considers fluents (that effects actions executedagent, see Section 8 Section 9).(AOI)hKB0 , F, C,SF0hKB0 , F, C,SF = {f1 , . . . , fn }, n > 0, set fluents selected actively sensed (byeffect selection operator), and, sensing(SF s, now) = L,KB00 = KB0{observed(f, now) | f : true L}{observed(f, now) | f : f alse L}.7.7 Action Executiontransition updates KB0 , recording execution actions agent. actionsexecuted selected action selection operator (see Section 8) priortransition, given input transition.322fiComputational Logic Foundations KGP Agents(AE)hKB0 , F, C,SAshKB00 , F, C, 0SAs non-empty set actions selected execution (by action selectionoperator),let subset non-sensing actions SAs subset sensingactions SAs;let sensing(S 0 , now) = L0 , 0 = {f | sense(f, ) S}let sensing(S 00 , now) = L00 , 00 = {c : | sense(c : a, ) S}let actuating(A0 , now) = A00 , A0 = {a | a[ ] A}.Then:KB00 = KB0{executed(a, now) | A00 }{observed(f, now) | f : true L0 }{observed(f, now) | f : f alse L0 }{observed(c, a[t], now) | c : a[t] L00|=cs C = sense(c : a, ) S}0 = { = | a[ ] SAs A00 }{ = | sense(f, ) SAs (f : ) L0 }{ = | c : a[t] L00 |=cs C = sense(c : a, ) S}.7.8 State RevisionSR transition revises state removing timed-out goals actions goalsactions become obsolete one ancestors already believedachieved. make use following terminology.Notation 7.1 Given state S, timed fluent literal `[ ], timed fluent literal actionoperator x[ ], time-point now:achieved(S, `[ ], now) standsexists total valuation S, |=cs |=T R `[ ]timed out(S, x[ ], now) standsexists total valuation S, |=cs > now.323fiKakas, Mancarella, Sadri, Stathis & ToniThen, specification transition follows.(SR)hKB0 , F, C,hKB0 , F 0 , C,F 0 set trees F pruned nodes(F 0 ) biggest subsetnodes(F) consisting goals/actions x[ ] tree F (here =hKB0 , F, C, i):(i) timed out(S, x[ ], now),(ii) x action operator, case executed(x, t) KB0 ( = t) ,(iii) x fluent literal, achieved(S, x[ ], now),(iv) every y[ 0 ] siblings(x[ ], F)either y[ 0 ] siblings(x[ ], F 0 ),y[ 0 ] 6 siblings(x[ ], F 0 )fluent literal achieved(S, y[ 0 ], now),action literal executed(y, t) KB0 0 = ,(v) x sensing action operator, x[ ] = sense(`, ),either exists a[ 0 ] siblings(x[ ], F 0 ) ` precondition (i.e.S, a[ 0 ] |=pre Cs `[ 0 ] Cs) < 0 C,exists `[ 0 ] siblings(x[ ], F 0 ) ` sensing fluent <0 C,(vi) x[ ] top-level goal parent(x[ ], F) = P P nodes(F 0 ).conditions specify SR keeps trees forest state. Intuitively, conditions may understood terms prevent remainingtrees:condition (i) removes timed-out goals actions,condition (ii) removes actions already executed,condition (iii) removes goals already achieved,condition (iv) removes goals actions whose siblings already timedthus deleted, condition (i),condition (v) removes sensing actions preconditions actionsdeleted sensing goals deleted,condition (vi) recursively removes actions goals whose ancestors removed.following example illustrates SR used provide adjustment agentsgoals plans light newly acquired information.324fiComputational Logic Foundations KGP Agents7.9 Setting 3agent psa goal museum ticket (state-run) museumbusinessman wants visit, plan buy ticket. executing plan psaobserves European Heritage day (ehd short), via appropriate messageanother agent mus (representing museum), stating state-run museumsEurope give free tickets anybody walking day. Then, psas goalalready achieved goal plan deleted state.Let agents initial state hKB0 , F, C, with:= { } = KB0F= {T }C = {1 10, 2 = 3 , 3 < 1 }consists top-level goal g1 = have(ticket, 1 ), two children,g2 = money(2 ) a1 = buy(ticket, 3 ),15assuming KBT R containsinitiates(ehd, T, have(ticket))initiates(buy(O), T, have(O))precondition(buy(O), money).remaining knowledge bases play useful role purposesexample, therefore considered empty. message museumagent mus added KB0 via POI, e.g. time 6, following form:observed(mus, ehd(5), 6)i.e. time 6 observed time 5 mus announced state-run museumsEurope free day. Then, via SR, time 8 say, g1 , g2 a1 eliminatedF, g1 already achieved.8. Selection OperatorsKGP model relies upon selection operators:fGS (goal selection, used provide input PI transition);fP (precondition selection, used provide input SI transition);fES (effect selection, used provide input AOI transition);fAS (action selection, used provide input AE transition).15. g1 a1 reactive not, matter example.325fiKakas, Mancarella, Sadri, Stathis & ToniSelection operators defined terms (some the) capabilities (namely TemporalReasoning, Identification Preconditions Effects Constraint Solving).high-level description, selection operators seen returning setitems given initial set satisfy certain number conditions. example,given state hKB0 , F, C, i, goal selection operator returns set non-executablegoals trees F satisfy conditions; precondition selection operator returnsset pairs, consisting (i) timed fluent literal preconditionaction tree F (ii) action, satisfying conditions; effectselection operator returns set fluent literals effects actions alreadyexecuted (as recorded KB0 ) satisfy conditions; action selection operatorreturns set actions trees F satisfy conditions.selection operators formally defined below.8.1 Goal SelectionInformally, set conditions goal selection operator follows. Given state= hKB0 , F, C, time-point t, set goals selected fGS singleton setconsisting non-executable goal G tree F time t:1. G timed out,2. ancestor G timed out,3. child ancestor G timed out,4. neither G, ancestor G tree F already achieved.5. G leafIntuitively, condition 1 ensures G already timed-out, conditions 2-3 imposeG belongs still feasible plan top-level goal F, condition 4 makessure considering G wasteful.Note that, already mentioned Section 5.1.3, simplicity select single goal.Formally, given state = hKB0 , F, C, time-point t, let G(S, t) setnon-executable goals `[ ] nodes(F) that:1. timed out(S, `[ ], t)2. timed out(S, G, t) G ancestors(`[ ], F),3. timed out(S, X, t) X nodes(F) X child Pancestors(`[ ], F)4. achieved(S, G, t) G {`[ ]} ancestors(`[ ], F)5. leaf (G, F)Then, G(S, t) 6= {}:fGS (S, t) = {G} G G(S, t).Otherwise, fGS (S, t) = {}.326fiComputational Logic Foundations KGP Agents8.2 Effect SelectionInformally, set conditions effect selection operator follows. Given state= hKB0 , F, C, time-point t, fES selects fluents f f f oneeffects action a[ ] recently executed.Note f (or f ) may occur F could (observable)effect executed action, necessarily goal actioncontributes achieving. example, order check whether internet connectionavailable, agent may want observe access skype network even thoughreally interested opening browser (as needs browser order performbooking online).Formally, given state = hKB0 , F, C, time-point now, set (timed)fluents selected fES set (timed) fluents f [ ] actionoperator1. executed(a, t0 ) KB0 , t0 = < t0 < now, sufficientlysmall number (that left parameter here),2. S, a[ ] |=ef f `, ` = f ` = f .8.3 Action SelectionInformally, set conditions action selection operator follows. Given state= hKB0 , F, C, time-point t, set actions selected fAS definedfollows. Let X (S, t) set actions trees F that:1. executed,2. ancestor timed out,3. child ancestor timed out,4. ancestor already satisfied,5. precondition known false,6. already executed.fAS (S, t) X (S, t) actions fAS (S, t) executable concurrentlyt.Intuitively, conditions 2-4 impose belongs still feasible plan toplevel goals F. Note condition 1 definition X (S, t) logically redundant,also re-imposed definition fAS (S, t). However, condition serves firstfilter thus useful practice.Formally, given state = hKB0 , F, C, i, time-point t, set actionsselected fAS defined follows. Let X (S, t) set actions a[ ] occurringleaves trees F that:327fiKakas, Mancarella, Sadri, Stathis & Toni1. exists total valuation S, |=cs = t,2. timed out(S, G, t) G ancestors(a[ ], F),3. timed out(S, X, t) X children(G, F) G ancestors(a[ ], F),4. achieved(S, G, t) G ancestors(a[ ], F),5. let S, a[ ] |=pre Cs Cs = `1 [ ] . . . `n [ ];n > 0, = 1, . . . , n exists total valuation S, |=cs= |=T R `i [ ],6. exists t0 = t0 executed(a, t0 ) KB0 .formalisation condition 6 allows instances actionexecuted. Then,fAS (S, t) = {a1 [1 ], . . . , [m ]} X (S, t)(where 0), exists total valuation variables CS, |=cs 1 = . . . = t.Note definition action selection operator extended takeaccount notion urgency respect temporal constraints. However,extension beyond scope work.8.4 Precondition SelectionInformally, set conditions precondition selection operator follows. Givenstate = hKB0 , F, C, time-point t, set preconditions (of actionsF) selected fP set pairs hC, Ai (timed) preconditions C actionsnodes(F) that:1. C precondition2. C known true t,3. one actions could selected execution fAS would calledcurrent time.reason selection operator returns pairs, rather simply preconditions,transition SI, makes use outputs selection operator, needsknow actions associated preconditions. SI introduces sensingactions precondition returned place sensing actions siblingsassociated actions F, seen Section 7.4.Formally, given state = hKB0 , F, C, time-point t, set preconditionsactions selected fP set pairs hC, Ai (timed) preconditions C actionsnodes(F) that:1. = a[ ], S, a[ ] |=pre Cs C conjunct Cs,328fiComputational Logic Foundations KGP Agents2. exists total valuation variables C S, |=cs =|=T R C,3. X (S, t), X (S, t) defined Section 8.3.9. Cycle Theorybehaviour KGP agents results application transitions sequences,repeatedly changing state agent. sequences fixed priori,conventional agent architectures, determined dynamically reasoningdeclarative cycle theories, giving form flexible control. Cycle theories givenframework Logic Programming Priorities (LPP) discussed Section 3.9.1 Formalisation Cycle Theories.use following new notations:(S, X, 0 , t) represent application transition time state giveninput X resulting state 0 ,(S, X) represent transition potentially chosen next transitionstate S, input X.Recall that, transitions, X may empty set {}, indicated Section 7.Formally, cycle theory Tcycle consists following parts.initial part Tinitial , determines possible transitions agent couldperform starts operate. Concretely, Tinitial consists rules form(S0 , X) C(S0 , X)refer via name R0|T (S0 , X). rules sanction that, conditionsC hold initial state S0 initial transition could , applied stateS0 input X. example, ruleR0|GI (S0 , {}) : GI(S0 , {}) empty f orest(S0 )sanctions initial transition GI, forest initial state S0empty.Note C(S0 , X) may empty, and, non-empty, C(S0 , X) may refercurrent time via condition time now(t). example, ruleR0|P (S0 , G) : P I(S0 , G) Gs = fGS (S0 , t), Gs 6= {}, G Gs, time now(t)sanctions initial transition PI, forest initial state S0contains goal planned current time (in goalselection operator picks goal).basic part Tbasic determines possible transitions following given transitions,consists rules form0 (S 0 , X 0 ) (S, X, 0 , t), EC(S 0 , X 0 )329fiKakas, Mancarella, Sadri, Stathis & Tonirefer via name RT |T 0 (S 0 , X 0 ). rules sanction that, transition executed, starting time state resulting state 0 ,conditions EC evaluated 0 satisfied, transition 0 couldnext transition applied 0 , input X 0 .16 EC enabling conditionsdetermine 0 applied . also determine input X 0 0 ,via calls selection operators. initial part Tcycle , EC may emptyand, not, may refer current time. example, ruleRAE|P (S 0 , G) : P I(S 0 , G) AE(S, As, 0 , t),Gs = fGS (S 0 , t0 ), Gs 6= {}, G Gs, time now(t0 )sanctions PI follow AE current time goalcurrent state selected goal selection function.behaviour part Tbehaviour contains rules describing dynamic priorities amongstrules Tbasic Tinitial . Rules Tbehaviour formRT |T 0 (S, X 0 ) RT |T 00 (S, X 00 ) BC(S, X 0 , X 00 )0 6= 00 , refer via name PTT0 00 . Recall RT |T 0 ()RT |T 00 () (names of) rules Tbasic Tinitial . Note that, abuse notation,could 0 case one rule used specify priority firsttransition take place, words, priority rules Tinitial .rules Tbehaviour sanction that, transition , conditions BC hold,prefer next transition 0 00 . conditions BC behaviourconditions give behavioural profile agent. example, rulePGIT0 : RT |GI (S, {}) RT |T 0 (S, X) empty f orest(S)sanctions GI preferred transition transitionresults state empty forest. components Tcycle ,conditions BC may refer current time.auxiliary part including definitions predicates occurring enablingbehaviour conditions.incompatibility part, effect expressing one (instance a) transitionchosen one time.Hence, Tcycle LPP-theory where: (i) P = Tinitial Tbasic , (ii) H = Tbehaviour .9.2 Operational Tracecycle theory Tcycle agent responsible behaviour, inducesoperational trace agent, namely (typically infinite) sequence transitionsT1 (S0 , X1 , S1 , t1 ), . . . , Ti (Si1 , Xi , Si , ti ), Ti+1 (Si , Xi+1 , Si+1 , ti+1 ), . . .16. Note order determine 0 possible transition , rule earlier form,one needs know applied resulted state 0 . conveyedchoice name: RT |T 0 (S 0 , X 0 ). words, using Prolog notation, could representedrule 0 (S 0 , X 0 ) ( , , 0 , ), EC(S 0 , X 0 ). Thus, rule Markovian.330fiComputational Logic Foundations KGP AgentsS0 given initial state;1, ti given clock system (ti < ti+i );(Tcycle Tbasic ) {time now(t1 )} |=pr T1 (S0 , X1 );1(Tcycle Tinitial ) {Ti (Si1 , Xi , Si , ti ), time now(ti+1 )} |=pr Ti+1 (Si , Xi+1 )namely (non-final) transition sequence followed preferred transition,specified Tcycle . If, stage, preferred transition determined |=prunique, choose one arbitrarily.9.3 Normal Cycle Theorynormal cycle theory concrete example cycle theory, specifying patternoperation agent prefers follow sequence transitions allows achievegoals way matches expected normal behaviour. examples possiblecycle theories found literature (Kakas, Mancarella, Sadri, Stathis, & Toni,2005; Sadri & Toni, 2006).Basically, normal agent first introduces goals (if none start with) via GI,reacts them, via RE, repeats process planning them, via PI,executing (part of) chosen plans, via AE, revising state, via SR, goalsdealt (successfully revised away). point agent returns introducingnew goals via GI repeating process. Whenever process agentinterrupted via passive observation, via POI, chooses introduce new goals viaGI, take account changes environment. Whenever actionsunreliable, sense preconditions definitely need checked,agent senses (via SI) executing action. Whenever actionsunreliable, sense effects definitely need checked, agent activelyintroduces actions aim sensing effects, via AOI, executedoriginal actions. initially agent equipped goals, would planstraightaway PI.full definition normal cycle theory given appendix. usedprovide control examples next section. Here, note that, although normalcycle theory based classic observe-plan-act cycle agent control, generalisesseveral ways giving flexibility agent behaviour adapt changingenvironment. example, goals agent need fixed dynamicallychanged depending newly acquired information. Let us illustrates featurebrief example here. Suppose current state agent contains top-level nonreactive goal hreturn home(1 ), {1 < 7}i POI occurs adds observationobserved(low battery, 2) time 2. subsequent GI transition generated normalcycle theory introduces new goal hrecharge battery(2 ), {2 < 3}i which, dependingdetails KBGD , either replaces previous goal adds additional goal.normal cycle theory next choose PI transition new urgentgoal recharging battery.331fiKakas, Mancarella, Sadri, Stathis & Toni10. Examplessection revisit examples introduced Section 2.6 used throughoutpaper illustrate various components KGP model. Overall, aimillustrate interplay transitions, interplay provides varietybehaviours afforded KGP model, including reaction observations, generationexecution conditional plans, dynamic adjustment goals plans.Unless specified differently, assume Tcycle normal cycle theorypresented Section 9.3. provide domain-dependent definition auxiliarypart Tcycle explicitly, required.10.1 Setting 1 Formalisedformalise initial state, knowledge bases behaviour svs Setting 1described Section 2.6.1.10.1.1 Initial Statesimplicity, observations, goals plan svs assumed emptyinitially. concretely let (initial) state svsKB0 = { }F= {}C = {}= {}10.1.2 Knowledge BasesFollowing Section 5.1.4, formulate reactivity knowledge base agent svs termsutterances query ref, ref use, inf orm inspired FIPA specifications communicative acts (FIPA, 2001a, 2001b). However, although use namescommunicative acts FIPA specification, adopt mentalisticsvs formulatedsemantic interpretation terms pre- post-conditions. Thus, KBreactas:observed(C, tell(C, svs, query ref (Q), D, 0), ), holds at(have inf o(Q, I), )assume happens(tell(svs, C, inf orm(Q, I), D), 0 ), 0 >observed(C, tell(C, svs, query ref (Q), D, 0), ), holds at(no inf o(Q), )assume happens(tell(svs, C, ref use(Q), D), 0 ), 0 >assume happens(tell(svs, C, inf orm(Q, I), D), ),assume happens(tell(svs, C, ref use(Q), D), 0 )f alseassume happens(A, ), executable(A) f alseexecutable(tell(svs, C, S, D)) C 6= svs332fiComputational Logic Foundations KGP Agentsinitially(no inf o(arrival(tr01))precondition(tell(svs, C, inf orm(Q, I), D), inf o(Q, I))initiates(tell(C, svs, inf orm(Q, I), D), T, inf o(Q, I))terminates(tell(C, svs, inf orm(Q, I), D), T, inf o(Q))10.1.3 Behaviourillustrate behaviour psa assume agent requests svs,time 3, say, arrival time tr01. svs receives request psa time 5 arrivaltime tr01. Via POI time 5 svs records KB0 :observed(psa, tell(psa, svs, query ref (arrival(tr01)), d, 3), 5)dialogue identifier. Then, via RE, time 7, say, svs modifies stateadding F tree rooted action a1 answer psa. action a1 refusalrepresented as:a1 = tell(svs, psa, ref use(arrival(tr01)), d, ),temporal constraint > 7 added C.refusal action generated via Reactivity capability svsinformation requested arrival time. svs executes planned action a1 time10, say, via AE transition, instantiating execution time, adding following recordKB0 :executed(tell(svs, psa, ref use(arrival(tr01)), d), 10),updating adding = 10 it.Suppose svs makes two observations follows. time 17 svs receivesinformation arrival time (18) tr01 train co. Via POI, svs recordsKB0 17 :observed(co, tell(co, svs, inf orm(arrival(tr01), 18), d0 , 15), 17).Assume time 25 svs receives another request psa arrivaltime tr01 and, via POI, svs records KB0 :observed(psa, tell(psa, svs, query ref (arrival(tr01)), d00 , 20), 25)new dialogue identifier d00 . leads different answer svs querypsa. svs adds action state answer psa arrival time. donevia RE, say time 28. new tree added F rooted (reactive) actiontell(svs, psa, inf orm(arrival(tr01), 18), d00 , 0 ),new temporal constraint 0 > 28 added C.Via AE, svs executes action, instantiating execution time 30, say, addingfollowing record17. d0 identifier dialogue within utterance performed, would typicallydifferent earlier d.333fiKakas, Mancarella, Sadri, Stathis & Toniexecuted(tell(svs, psa, inf orm(arrival(tr01), 18), d00 ), 30)KB0 , adding 0 = 30 .Eventually, SR clear planned (and executed) actions F componentstate svs.10.2 Setting 2 Formalisedformalise initial state, knowledge bases behaviour psa Setting 2described Section 2.6.2.10.2.1 Initial StateLet us assume initially state psa follows:KB0 = { }F= {T1 , T2 }C = {1 < 15, 2 < 15}= {}T1 T2 consist goals (respectively):g1 = ticket(madrid, denver, 1 )g2 = visa(usa, 2 ).10.2.2 Knowledge Basespsaplan goal g1 , KBplancontains:initiates(buy ticket online(F rom, o), T, ticket(F rom, o))precondition(buy ticket online(F rom, o), available connection)precondition(buy ticket online(F rom, o), available destination(T o)).psaplan goal g2 , KBplancontains:initiates(apply visa(usa), T, visa(usa))precondition(apply visa(usa), address(usa))initiates(book hotel(L), T, address(usa)) holds(in(L, usa), ).10.2.3 BehaviourPI called state, time 2, say, generates partial plan goal,changing state follows. goal g1 acquires three children T1 . are:g11 = available connection(11 ),g12 = available destination(denver, 12 ),a13 = buy ticket online(madrid, denver, 13 ).Also, consequently, set temporal constraints updated to:C = {1 < 15, 2 < 15, 11 = 13 , 12 = 13 , 13 < 1 , 1 > 2}.334fiComputational Logic Foundations KGP Agentsaction a13 generated action initiates goal g1 . Moreover, every plangenerated must satisfy integrity constraints KBplan . particular, preconditionactions tree already hold must generated sub-goals tree.g11 g12 generated tree above.via transition SI, following sensing actions added T1 siblingsaction a13 18 :a14 = sense(available connection, 14 )a15 = sense(available destination(denver), 15 )constraints14 = 15 , 14 < 13added C.Then, via AE, two sensing actions executed (before original action a1 ),KB0 updated result sensing follows. Suppose two actionsexecuted time 5. Consider first action senses fluent available connection.fluent confirmed physical sensing capability, i.e. available connection : trueXsensing({available connection, available destination}, 5) = X,observed(available connection, 5) added KB0 . hand,available connection : f alseX above, observed(available connection, 5) added KB0 . cases14 = 5 added .neither cases occurs, i.e. sensing capability cannot confirm eitheravailable connection available connection, fact added KB0 . Similarlyprecondition, available destination. Let us assume step AE,KB0 becomesobserved(available connection, 5)observed(available destination(denver), 5)AE execute original action a13 . Note agent might decide executeaction even one preconditions known satisfied sensing.g1 achieved, SR eliminate a13 , a14 , a15 , g11 , g12 state.resulting state, F = {T2 }, PI called, say time 6. results generatingpartial plan g2 , changing state T2 root g2 childrena21 = apply visa(usa, 21 )g22 = address(usa, 22 )21 < 2 , 22 = 21 added C. Then, PI, say time 7, introducesa23 = book hotel(denver, 23 )18. assume auxiliary part Tcycle contains ruleunreliable pre(As) buy ticket online( , , )335fiKakas, Mancarella, Sadri, Stathis & Tonichild g22 T2 , adding 23 < 22 C. Then, AE time 8 executes a23 , addingKB0 , AE time 9 executes a22 , also updating KB0 . Finally, SR eliminatesactions goals T2 returns empty F state.11. Related WorkMany proposals exist models architectures individual agents based computational logic foundations (see e.g. survey Fisher, Bordini, Hirsch, & Torroni,2007). proposals based logic programming, example IMPACT (Arisha, Ozcan, Ross, Subrahmanian, Eiter, & Kraus, 1999; Subrahmanian, Bonatti, Dix,Eiter, Kraus, Ozcan, & Ross, 2000), AAA (Balduccini & Gelfond, 2008; Baral & Gelfond,2001), DALI (Costantini & Tocchio, 2004), MINERVA (Leite, Alferes, & Pereira, 2002),GOLOG (Levesque, Reiter, Lesperance, Lin, & Scherl, 1997), IndiGolog (De Giacomo,Levesque, & Sardina, 2001). proposals based modal logic first-order logicapproaches, example BDI model (Bratman et al., 1988; Rao & Georgeff, 1997)extensions deal normative reasoning (Broersen, Dastani, Hulstijn, Huang, &van der Torre, 2001), Agent0 (Shoham, 1993), AgentSpeak (Rao, 1996) variants,3APL (Hindriks, de Boer, van der Hoek, & Meyer, 1999) variants (Dastani, Hobo,& Meyer, 2007).high level comparison similarities objectives existingcomputational logic models agency KGP, aim specifying knowledgerich agents certain desirable behaviours. also similarities finerdetails KGP model related work, well differences.feature KGP which, best knowledge, novel declarativecontext-sensitive specification agents cycle. avoid static cycle control(Rao & Georgeff, 1991; Rao, 1996), KGP relies upon cycle theory determines,run time, given circumstances individual profile agent, nextstep be. cycle theory sensitive solicited unsolicited informationagent receives environment, helps agent adapt behaviourchanges experiences. approach closest work 3APL (Hindrikset al., 1999) extended Dastani, de Boer, Dignum, Meyer (2003), providesmeta-programming constructs specifying cycle agent goal selection,plan expansion, execution, well if-then-else while-loop statements. Unlikeimperative constructs 3APL, KGP uses set selection operators extendedmodel different behaviours types agents. flexible ordering transitionsobtained using preference reasoning transitions applied specificpoint time. preferences may change according external events changesknowledge agent.Another central distinguishing feature KGP model, comparison existingmodels, including based logic programming, modular integration withinsingle framework abductive logic programming, temporal reasoning, constraint logicprogramming, preference reasoning based logic programming priorities, ordersupport diverse collection capabilities. one specified declarativelyequipped provably correct computational counterpart (see Bracciali,336fiComputational Logic Foundations KGP AgentsDemetriou, Endriss, Kakas, Lu, Mancarella, Sadri, Stathis, Terreni, & Toni, 2004,detailed discussion).Compared existing logic programming approaches KGP two main similaritiesMINERVA (Leite et al., 2002), architecture exploits computational logicgives declarative operational semantics agents. Unlike KGP, MINERVAagent consists several specialised, possibly concurrent, sub-agents performing varioustasks, relies upon MDLP (Multidimensional Dynamic Logic Programming) (Leite et al.,2002). MDLP basic knowledge representation mechanism agent MINERVA,based extension answer-set programming explicit rules updatingagents knowledge base. KGP instead integrate abductive logic programminglogic programming priorities combined temporal reasoning.Closely related work KGP logic-based agent architecture reasoningagents Baral Gelfond (2001). architecture assumes state agentsenvironment described set fluents evolve time terms transitionslabelled actions. agent also assumed capable correctly observing stateenvironment, performing actions, remembering history happenedit. agents knowledge base consists action description part specifying internalagent transitions, domain specific generic KGP. knowledgebase also contains agent observes environment including actions,KGPs KB0 . temporal aspects agent transitions specified actionlanguage AL implemented A-Prolog, language logic programs answerset programming semantics. answer sets domain specific programs specified ALcorrespond plans KGP hypothetical narratives abductive event calculus.control agent based static observe-think-act cycle, instance KGPcycle theories. recent refined account overall approach given riseAAA Architecture, see (Balduccini & Gelfond, 2008) overview.DALI (Costantini & Tocchio, 2004) logic programming language designed executable specification logical agents. Like KGP, DALI attempts provide constructsrepresent reactivity proactivity agent using extended logic programs. DALIagent contains reactive rules, events, actions aimed interacting externalenvironment. Behaviour (in terms reactivity proactivity) DALI agent triggereddifferent event types: external, internal, present, past events. eventsactions time stamped record occur. External events likeobservations KGP, past events like past observations. However, KGPsupport internal events instead idea transitions called cycletheory trigger reactive proactive behaviour.IndiGolog (De Giacomo et al., 2001) high-level programming language robotsintelligent agents supports, like KGP, on-line planning, sensing plan executiondynamic incompletely known environments. member Golog familylanguages (Levesque et al., 1997) use Situation Calculus theory action performreasoning required executing program. Instead KGP model relyabductive logic programming logic programming priorities combined temporal reasoning. Instead Situation Calculus KGP use Event Calculustemporal reasoning, use Event Calculus prerequisite modelInterRaP (Muller, Fischer, & Pischel, 1998), replaced another temporal337fiKakas, Mancarella, Sadri, Stathis & Tonireasoning framework, needed. Apart difference use Situation Event Calculi, IndiGolog goals cannot decided dynamically, whereasKGP model change dynamically according specifications Goal Decisioncapability.obvious similarity KGP model BDI model (Bratman et al.,1988) given correspondence KGPs knowledge, goals plan BDIsbeliefs, desires intentions, respectively. Apart fact BDI modelbased modal logic, KGP knowledge (beliefs BDI) partitioned modules,support various reasoning capabilities. KGP also tries bridge gapspecification practical implementation agent. gap criticizedBDI Rao (1996), developed AgentSpeak(L) language. computationalmodel AgentSpeak(L) formally studied dInverno Luck (1998),recent implementations AgentSpeak interpreter incorporated Jasonplatform (Bordini & Hubner, 2005). Like KGP implementation PROSOCS (Braccialiet al., 2006), Jason implementation seeks narrow gap specificationexecutable BDI agent programs. Jason also extends BDI new features like beliefrevision (Alechina, Bordini, Hubner, Jago, & Logan, 2006).particular line work BDI Padgham Lambrix (2005), investigatenotion capability integrated BDI Logic Rao Georgeff (1991),BDI agent reason capabilities. capability workinformally understood ability act rationally towards achieving particular goal,sense abstract plan type believed achieve goal. Formally,BDI logic Rao Georgeff extended incorporate modality capabilitiesconstrains agent goals intentions compatible agent believescapabilities. set compatibility axioms presented detailing semanticconditions capture desired inter-relationships among agents beliefs, capabilities,goals, intentions. work also summarises extensions BDI modelimplemented adapting BDI interpreter include capabilities, arguingbenefits extension original BDI Interpreter Rao Georgeff (1992).KGP capabilities equate reasoning capabilities agent allow agentplan actions given state, react incoming observations, decide upongoals adopt. However, KGP, use capabilities level agentsdomain specific knowledge guide agent determining whether rationaladopt particular goal.issue separation specification implementation existsKGP model Agent0 (Shoham, 1993), later refinement PLACA (Thomas,1995). Two differences KGP Agent0 PLACA explicitlinks exist KGP model amongst goals (in structuring forestagent state) richer theories KGP specify priorities amongst potentialgoals restricted temporal orderings. explicit links exploitedrevising goals state, via Revision transition, light new informationpassage time.BOID architecture (Broersen et al., 2001) extends well known BDI model (Rao& Georgeff, 1992) obligations, thus giving rise four main components representingagent: beliefs, obligations, intentions desires. focus BOID find ways338fiComputational Logic Foundations KGP Agentsresolving conflicts amongst components. order define agent types,including well known types agent theories realistic, selfish, social simpleminded agents. agent types differ give different priorities rulesfour components. instance, simple minded agent gives higher priorityintentions, compared desires obligations, whereas social agent gives higher priorityobligations desires. use priorities propositional logic formulae specifyfour components agent types.existing KGP model already resolves conflicts BOID tries address. example, conflict belief prior intention, meansintended action longer executed due changes environment,KGP agent notice give higher priority belief priorintention, allowing agent effect retract intended action and, time permitting,replan goals. KGP model also includes notion priority used GoalDecision capability cycle theory controls behaviour agent.KGP model also extended deal normative concepts, extended modelknown N-KGP (Sadri, Stathis, & Toni, 2006). N-KGP common BOIDseeks extend KGP addition obligations. N-KGP model alsoextends notion priorities incorporating amongst different types goalsactions. detailed comparison N-KGP related work presented Sadri, Stathis,Toni (2006).features included approaches absentKGP model. BDI and, so, IMPACT system (Arisha et al., 1999; Subrahmanianet al., 2000) allow agents knowledge bases representations knowledgeagents. systems allow agents degree introspectionability reason agents beliefs reasoning. KGP model dateinclude features. IMPACT also allows incorporation legacy systems, possibly using diverse languages, richer knowledge base language includingdeontic concepts probabilities. Similarly, 3APL, system based combinationimperative logic programming languages, includes optimisation componentabsent KGP. component 3APL includes rules identify givensituation agent pursuing suboptimal plan, help agent find better wayachieving goals. 3APL also includes additional functionalities learning (vanOtterlo, Wiering, Dastani, & Meyer, 2003), model currently support.2APL (Dastani et al., 2007) extension 3APL goals goal-plan rules wellexternal internal events. 2APL customisable (via graphical interface) cyclefixed customised.12. Conclusionspresented computational logic foundations KGP model agency.model allows specification heterogeneous agents interact other,exhibit proactive reactive behaviour allowing function dynamicenvironments adjusting goals plans changes happen environments. KGP incorporates highly modular agent architecture integrates collection339fiKakas, Mancarella, Sadri, Stathis & Tonireasoning sensing capabilities, synthesised within transitions, orchestrated cycletheories take account dynamic context agent preferences.formal specification KGP components within computational logicmajor advantage facilitating formal analysis model direct verifiableimplementation. formal analysis started Sadri Toni (2006),give formal analysis KGP agents exploring effectiveness terms goalachievement, reactive awareness, impact reasoning capabilities towardsprogress goal achievement. implementation precursor model, describedKakas et al. (2004b), already developed within PROSOCS platformStathis et al. (2004) upon provably correct computational counterparts definedcomponent model given Kakas et al. (2004b). Concrete choicescomputational counterparts described Bracciali et al. (2004). resultingdevelopment framework allows deployment testing functionality earliervariant KGP agents. Deployment agents relies upon agent template designedStathis et al. (2002), builds upon previous work head/body metaphordescribed Steiner et al. (1991) Haugeneder et al. (1994), mind/body architecture introduced Bell (1995) recently used Huang, Eliens, de Bra (2001).development platform applied number practical applications, and,particular, ambient intelligence Stathis Toni (2004). Also, Sadri (2005)provided guidelines specifying applications using KGP agents. Future work includes implementing deploying revised KGP model given paper: envisagepose limited conceptual challenges, able capitalise experienceimplementing deploying precursor model.Sadri, Stathis, Toni (2006) explored precursor KGP agentmodel augmented normative features allowing agents reasonchoose social personal goals, prohibitions obligations. wouldinteresting continue work finalised KGP model given paper.Sadri Toni (2005) developed number different profiles behaviour,defined terms specific cycle theories, formally proved advantages givencircumstances. would interesting explore dimension further, characterisedifferent agent personalities provide guidance, formal properties,type personality needed applications.Future work also includes extending model incorporate (i) reasoning capabilities, including knowledge revision (e.g. Inductive Logic Programming),sophisticated forms temporal reasoning, including identifying explanations unexpectedobservations, (ii) introspective reasoning reasoning beliefs agents,(iii) experimentation model via implementation, (iv) developmentconcurrent implementation.Acknowledgmentswork supported EU FET Global Computing Initiative, within SOCSproject (IST-2001-32530). wish thank colleagues SOCS useful discussionsdevelopment KGP. also grateful Chitta Baral anonymousreferees helpful comments earlier version paper.340fiComputational Logic Foundations KGP AgentsAppendix A. Normal Cycle Theorygive main parts normal Tcycle , exclude others, exampledefinitions incompatible auxiliary part, including definitions predicatesempty f orest, unreliable pre etc. details see (Kakas et al., 2005).Tinitial : consists following rules:R0|GI (S0 , {}) : GI(S0 , {}) empty f orest(S0 )R0|AE (S0 , As) : AE(S0 , As) empty non executable goals(S0 ), = fAS (S0 , t),6= {}, time now(t)R0|P (S0 , G) : P I(S0 , G) Gs = fGS (S0 , t), Gs 6= {}, G Gs, time now(t)Tbasic : consists following rules:rules deciding might follow AE transition follows:RAE|P (S 0 , G) : P I(S 0 , G) AE(S, As, 0 , t), Gs = fGS (S 0 , t0 ), Gs 6= {},G Gs, time now(t0 )00RAE|AE (S , ) : AE(S 0 , As0 ) AE(S, As, 0 , t), As0 = fAS (S 0 , t0 ),As0 6= {}, time now(t0 )RAE|AOI (S 0 , F s) : AOI(S 0 , F s) AE(S, As, 0 , t), F = fES (S 0 , t0 ),F 6= {}, time now(t0 )RAE|SR (S 0 ) : SR(S 0 , {}) AE(S, As, 0 , t)RAE|GI (S 0 , {}) : GI(S 0 , {}) AE(S, As, 0 , t)Namely, AE could followed another AE, PI, AOI, SR,GI, POI.rules deciding might follow SR followsRSR|P (S 0 , G) : P I(S 0 , G) SR(S, {}, 0 , t), Gs = fGS (S 0 , t0 ), Gs 6= {}, G Gs,time now(t0 )RSR|GI (S 0 , {}) : GI(S 0 , {}) SR(S, {}, 0 , t), Gs = fGS (S 0 , t0 ), Gs = {},time now(t0 )0RSR|AE (S , As) : AE(S 0 , As) SR(S, {}, 0 , t), = fGS (S 0 , t0 ), 6= {},time now(t0 )Namely, SR followed PI GI AE, depending whethergoals plan state.rules deciding might follow PI followsRP I|AE (S 0 , As) : AE(S 0 , As) P I(S, G, 0 , t), = fAS (S 0 , t0 ), 6= {},time now(t0 )0RP I|SI (S , P s) : SI(S 0 , P s) P I(S, G, 0 , t), P = fP (S 0 , t0 ), P 6= {}, time now(t0 )second rule allow possibility sensing preconditions actionexecution.rules deciding might follow GI followsRGI|RE (S 0 , {}) : RE(S 0 , {}) GI(S, {}, 0 , t)RGI|P (S 0 , G) : P I(S 0 , G) GI(S, {}, 0 , t), Gs = fGS (S 0 , t0 ), Gs 6= {}, G Gs,time now(t0 )Namely, GI followed PI, goals plan for.rules deciding might follow follows341fiKakas, Mancarella, Sadri, Stathis & ToniRRE|P (S 0 , G) : P I(S 0 , G) RE(S, {}, 0 , t), Gs = fGS (S 0 , t0 ), Gs 6= {}, G Gs,time now(t0 )RRE|SI (S 0 , P s) : SI(S 0 , P s) RE(S, {}, 0 , t), P = fP (S 0 , t0 ), P 6= {},time now(t0 )rules deciding might follow SI followsRSI|AE (S 0 , As) : AE(S 0 , As) SI(S, P s, 0 , t), = fAS (S 0 , t0 ), 6= {},time now(t0 )0RSI|SR (S , {}) : SR(S 0 , {}) SI(S, P s, 0 , t)rules deciding might follow AOI followsRAOI|AE (S 0 , As) : AE(S 0 , As) AOI(S, F s, 0 , t), = fAS (S 0 , t0 ), 6= {},time now(t0 )0RAOI|SR (S , {}) : SR(S 0 , {}) AOI(S, F s, 0 , t)RAOI|SI (S 0 , P s) : SI(S 0 , P s) AOI(S, F s, 0 , t), P = fP (S 0 , t0 ), P 6= {},time now(t0 )rules deciding might follow POI followsRP OI|GI (S 0 , {}) : GI(S 0 , {}) P OI(S, {}, 0 , t)Tbehaviour : consists following rules:GI given higher priority trees state:PGIT0 : RT |GI (S, {}) RT |T 0 (S, X) empty f orest(S)transitions T, 0 , 0 6= GI, possibly 0 (indicatingtrees initial state agent, GI first transition).GI also given higher priority POI:P OI : R00PGITP OI|GI (S , {}) RP OI|T (S, {}, )transitions 6= GI.GI, transition given higher priority:GIPRET: RGI|RE (S, {}) RGI|T (S, X)transitions 6= RE.RE, transition PI given higher priority:PPRE: RRE|P (S, G) RRE|T (S, X)transitions 6= P I.PI, transition AE given higher priority, unless actionsactions selected execution whose preconditions unreliable need checking,case SI given higher priority:PIPAET: RP I|AE (S, As) RP I|T (S, X) unreliable pre(As)transitions 6= AE.PI: RP I|SI (S, P s) RP I|AE (S, As) unreliable pre(As)PSIAESI, transition AE given higher prioritySI: RSI|AE (S, As) RSI|T (S, X)PAETtransitions 6= AE.AE, transition AE given higher priorityactions execute state, case either AOI SR given higherpriority, depending whether actions unreliable, senseeffects need checking, not:342fiComputational Logic Foundations KGP AgentsAEPAET: RAE|AE (S, As) RAE|T (S, X)transitions 6= AE. Note that, definition Tbasic , transition AE applicablestill actions executed state.AEAEPAOIT: RAE|AOI (S, F s) RAE|T (S, X)) BCAOI|T(S, F s, t), time now(t)AEtransitions 6= AOI, behaviour condition BCAOI|T (S, F s, t) defined (inauxiliary part) by:AEBCAOI|T(S, F S, t) empty executable goals(S, t), unreliable ef f ect(S, t)Similarly, have:AE (S, t), time now(t)AE: RAE|SR (S, {}) RAE|T (S, X)) BCSR|TPSRTtransitions 6= SR where:AE (S, t) empty executable goals(S, t), unreliable ef f ect(S, t)BCSR|THere, assume auxiliary part Tcycle specifies whether given set actionscontains unreliable action, sense expressed unreliable ef f ect, definespredicate empty executable goals.SR, transition PI higher priority:PPSR: RSR|P (S, G) RSR|T (S, X))transitions 6= P I.Note that, definition Tbasic , transition PI applicable still goalsplan state. actions goals left state, rule RGI|Twould apply.initial state PI given higher priority:PP0 : R0|P (S, G) R0|T (S, X)transitions 6= P I. Note that, definition Tinitial below, transition PIapplicable initially goals plan initial state.343fiKakas, Mancarella, Sadri, Stathis & ToniReferencesAlechina, N., Bordini, R. H., Hubner, J. F., Jago, M., & Logan, B. (2006). Belief RevisionAgentSpeak Agents. Nakashima, H., Wellman, M. P., Weiss, G., & Stone, P.(Eds.), 5th International Joint Conference Autonomous Agents MultiagentSystems (AAMAS 2006), pp. 12881290, Hakodate, Japan. ACM.Arisha, K. A., Ozcan, F., Ross, R., Subrahmanian, V. S., Eiter, T., & Kraus, S. (1999).IMPACT: Platform Collaborating Agents. IEEE Intelligent Systems, 14 (2),6472.Balduccini, M., & Gelfond, M. (2008). AAA Architecture: Overview. AAAISpring Symposium Architectures Intelligent Theory-Based Agents (AITA08).Baral, C., & Gelfond, M. (2001). Reasoning agents dynamic domains. Logic-basedartificial intelligence, pp. 257279. Kluwer Academic Publishers, Norwell, MA, USA.Bell, J. (1995). Planning Theory Practical Rationality. Proceedings AAAI95Fall Symposium Rational Agency, pp. 14. AAAI Press.Bordini, R. H., & Hubner, J. F. (2005). BDI Agent Programming AgentSpeak using Jason(Tutorial Paper). Toni, F., & Torroni, P. (Eds.), Computational Logic MultiAgent Systems, 6th International Workshop, CLIMA VI, Lecture Notes ComputerScience, pp. 143164. Springer.Bracciali, A., & Kakas, A. (2004). Frame consistency: Reasoning explanations.Proceedings 10th International Workshop Non-Monotonic Reasoning(NMR2004), Whistler BC, Canada.Bracciali, A., Demetriou, N., Endriss, U., Kakas, A. C., Lu, W., Mancarella, P., Sadri, F.,Stathis, K., Terreni, G., & Toni, F. (2004). KGP Model Agency GlobalComputing: Computational Model Prototype Implementation. Priami, C., &Quaglia, P. (Eds.), Global Computing, pp. 340367, Rovereto, Italy. Springer.Bracciali, A., Endriss, U., Demetriou, N., Kakas, A. C., Lu, W., & Stathis, K. (2006).Crafting Mind PROSOCS Agents. Applied Artificial Intelligence, 20 (2-4), 105131.Bratman, M., Israel, D., & Pollack, M. (1988). Plans resource-bounded practical reasoning. Computational Intelligence, 4.Broersen, J., Dastani, M., Hulstijn, J., Huang, Z., & van der Torre, L. (2001). BOIDArchitecture: conficts Beliefs, Obligations, Intentions Desires. Proceedings Fifth International Conference Autonomous Agents (Agents 2001), pp.916. ACM Press, Montreal, Canada.Clark, K. L. (1978). Negation Failure. Gallaire, H., & Minker, J. (Eds.), LogicData Bases, pp. 293322. Plenum Press.Costantini, S., & Tocchio, A. (2004). DALI Logic Programming Agent-Oriented Language. Alferes, J. J., & Leite, J. A. (Eds.), Proceedings 9th European Conference Logics Artificial Intelligence, (JELIA 2004), Vol. 3229 Lecture NotesComputer Science, pp. 685688. Springer.344fiComputational Logic Foundations KGP AgentsDastani, M., de Boer, F., Dignum, F., & Meyer, J.-J. (2003). Programming Agent Deliberation: approach illustrated using 3APL Language. Autonomous AgentsMult-agent Systems (AAMAS03), pp. 97104, Australia.Dastani, M., Hobo, D., & Meyer, J.-J. (2007). Practical Extensions Agent ProgrammingLanguages. Proceedings Sixth International Joint Conference AutonomousAgents Multiagent Systems (AAMAS07). ACM Press.de Bruijn, O., & Stathis, K. (2003). Socio-Cognitive Grids: Net Universal HumanResource. Kameas, A., & Streitz, N. (Eds.), Proceedings Conference TalesDisappearing Computer, pp. 211218, Santorini. CTI Press.De Giacomo, G., Levesque, H. J., & Sardina, S. (2001). Incremental execution guardedtheories. ACM Transactions Computational Logic, 2 (4), 495525.dInverno, M., & Luck, M. (1998). Engineering AgentSpeak(L): Formal ComputationalModel. J. Log. Comput., 8 (3), 233260.FIPA Communicative Act Library Specification (2001a). Experimental specificationXC00037H. Foundation Intelligent Physical Agents, http://www.fipa.org.FIPA Query Interaction Protocol (2001b). Experimental specification XC00027F. Foundation Intelligent Physical Agents, http://www.fipa.org.Fisher, M., Bordini, R., Hirsch, B., & Torroni, P. (2007). Computational Logics Agents:Road Map Current Technologies Future Trends. Computational Intelligence,23 (1), 6191.Haugeneder, H., Steiner, D., & McCabe, F. (1994). IMAGINE: framework buildingmulti-agent systems. Deen, S. M. (Ed.), Proceedings 1994 InternationalWorking Conference Cooperating Knowledge Based Systems (CKBS-94), pp. 3164, DAKE Centre, University Keele, UK.Hindriks, K. V., de Boer, F. S., van der Hoek, W., & Meyer, J. C. (1999). Agent programming 3APL. Autonomous Agents Multi-Agent Systems, 2(4), 357401.Huang, Z., Eliens, A., & de Bra, P. (2001). Architecture Web Agents. ProceedingsEUROMEDIA01. SCS.Jaffar, J., & Maher, M. (1994). Constraint logic programming: survey. Journal LogicProgramming, 19-20, 503582.Kakas, A. C., Kowalski, R. A., & Toni, F. (1998). role abduction logic programming. Gabbay, D. M., Hogger, C. J., & Robinson, J. A. (Eds.), Handbook LogicArtificial Intelligence Logic Programming, Vol. 5, pp. 235324. Oxford UniversityPress.Kakas, A. C., Mancarella, P., & Dung, P. M. (1994). acceptability semantics logicprograms. Proceedings eleventh international conference Logic programming, pp. 504519, Cambridge, MA, USA. MIT Press.Kakas, A. C., & Miller, R. (1997). simple declarative language describing narrativesactions. Logic Programming, 31.345fiKakas, Mancarella, Sadri, Stathis & ToniKakas, A. C., & Moraitis, P. (2003). Argumentation based decision making autonomousagents. Rosenschein, J. S., Sandholm, T., Wooldridge, M., & Yokoo, M. (Eds.),Proceedings Second International Joint Conference Autonomous AgentsMultiagent Systems (AAMAS-2003), pp. 883890, Melbourne, Victoria. ACM Press.Kakas, A., Mancarella, P., Sadri, F., Stathis, K., & Toni, F. (2005). Declarative AgentControl. Leite, J., & Torroni, P. (Eds.), CLIMA V: Computational Logic MultiAgent Systems, Vol. 3487 Lecture Notes Artificial Intelligence (LNAI), pp. 96110. Springer Verlag.Kakas, A. C., Kowalski, R. A., & Toni, F. (1992). Abductive Logic Programming. J. Log.Comput., 2 (6), 719770.Kakas, A. C., Mancarella, P., Sadri, F., Stathis, K., & Toni, F. (2004a). Declarative AgentControl. Leite, J. A., & Torroni, P. (Eds.), Computational Logic Multi-Agent Systems, 5th International Workshop, CLIMA V, Vol. 3487 Lecture Notes ComputerScience, pp. 96110. Springer.Kakas, A. C., Mancarella, P., Sadri, F., Stathis, K., & Toni, F. (2004b). KGP ModelAgency. de Mantaras, R. L., & Saitta, L. (Eds.), Proceedings 16th EureopeanConference Artificial Intelligence (ECAI 2004), pp. 3337. IOS Press.Kowalski, R. A., & Sergot, M. (1986). logic-based calculus events. New GenerationComputing, 4 (1), 6795.Kowalski, R., & Toni, F. (1996). Abstract argumentation. Artificial Intelligence LawJournal, Special Issue Logical Models Argumentation, 4 (3-4), 275296. KluwerAcademic Publishers.Leite, J. A., Alferes, J. J., & Pereira, L. M. (2002). MIN ERVA: dynamic logic programming agent architecture. Intelligent Agents VIII: 8th International Workshop,ATAL 2001, Seattle, WA, USA, Revised Papers, Vol. 2333 Lecture Notes Artificial Intelligence, pp. 141157.Levesque, H. J., Reiter, R., Lesperance, Y., Lin, F., & Scherl, R. B. (1997). GOLOG:logic programming language dynamic domains. Journal Logic Programming,31 (1-3), 5983.Mamdani, E. H., Pitt, J., & Stathis, K. (1999). Connected Communities StandpointMulti-agent Systems. New Generation Computing, 17 (4), 381393.Mancarella, P., Sadri, F., Terreni, G., & Toni, F. (2004). Planning partially situatedagents. Leite, J. A., & Torroni, P. (Eds.), Computational Logic Multi-Agent Systems, 5th International Workshop, CLIMA V, Vol. 3487 Lecture Notes ComputerScience, pp. 230248. Springer.Miller, R., & Shanahan, M. (2002). alternative formulations event calculus.Kakas, A. C., & Sadri, F. (Eds.), Computational Logic: Logic ProgrammingBeyond - Essays Honour Robert A. Kowalski, Vol. 2408 Lecture NotesComputer Science, pp. 452490. Springer.Muller, J., Fischer, K., & Pischel, M. (1998). Pragmatic BDI Architecture. Huhns,M. N., & Singh, M. P. (Eds.), Readings Agents, pp. 217225. Morgan KaufmannPublishers.346fiComputational Logic Foundations KGP AgentsPadgham, L., & Lambrix, P. (2005). Formalisations capabilities BDI-agents. Autonomous Agents Multi-Agent Systems, 10 (3), 249271.Prakken, H., & Sartor, G. (1996). system defeasible argumentation, defeasiblepriorities. International Conference Formal Applied Practical Reasoning,Springer Lecture Notes AI 1085, pp. 510524.Prakken, H., & Sartor, G. (1997). Argument-based extended logic programming defeasible priorities. Journal Applied Non-Classical Logics, 7 (1), 2575.Rao, A. S. (1996). AgentSpeak(L): BDI agents speak logical computable language.van Hoe, R. (Ed.), Agents Breaking Away, 7th European Workshop ModellingAutonomous Agents Multi-Agent World, MAAMAW96, Eindhoven, Netherlands, January 22-25, 1996, Proceedings, Vol. 1038 Lecture Notes ComputerScience, pp. 4255. Springer-Verlag.Rao, A. S., & Georgeff, M. P. (1991). Modeling Rational Agents within BDI-architecture.Fikes, R., & Sandewall, E. (Eds.), Proceedings Knowledge RepresentationReasoning (KR&R-91), pp. 473484. Morgan Kaufmann Publishers.Rao, A. S., & Georgeff, M. P. (1997). Modeling rational agents within BDI-architecture.Huhns, M. N., & Singh, M. P. (Eds.), Readings Agents, pp. 317328. MorganKaufmann Publishers, San Francisco, CA, USA.Rao, A. S., & Georgeff, M. P. (1992). abstract architecture rational agents. Nebel,B., Rich, C., & R.Swartout, W. (Eds.), 3rd International Conference PrinciplesKnowledge Representation Reasoning (KR92), pp. 439449, Cambridge, MA,USA. Morgan Kaufmann.Sadri, F. (2005). Using KGP model agency design applications (Tutorial Paper).Toni, F., & Torroni, P. (Eds.), Computational Logic Multi-Agent Systems, 6thInternational Workshop, CLIMA VI, Vol. 3900 Lecture Notes Computer Science,pp. 165185. Springer.Sadri, F., Stathis, K., & Toni, F. (2006). Normative KGP agents. Computational & Mathematical Organization Theory, 12 (2-3), 101126.Sadri, F., & Toni, F. (2005). Variety behaviours profiles logic-based agents.Toni, F., & Torroni, P. (Eds.), Computational Logic Multi-Agent Systems, 6thInternational Workshop, CLIMA VI, Vol. 3900 Lecture Notes Computer Science,pp. 206225. Springer.Sadri, F., & Toni, F. (2006). Formal Analysis KGP agents. Fisher, M., van der Hoek,W., Konev, B., & Lisitsa, A. (Eds.), Logics Artificial Intelligence, 10th EuropeanConference, JELIA 2006, Vol. 4160 Lecture Notes Computer Science, pp. 413425. Springer.Shanahan, M. (1997). Solving Frame Problem. MIT Press.Shanahan, M. (1989). Prediction deduction explanation abduction. Proceedings11th International Joint Conference Artificial Intelligence, pp. 10551060.Shoham, Y. (1993). Agent-oriented programming. Artificial Intelligence, 60 (1), 5192.347fiKakas, Mancarella, Sadri, Stathis & ToniSOCS (2007). Societies ComputeeS: computational logic model description,analysis verification global open societies heterogeneous computees.http://lia.deis.unibo.it/research/socs/.Stathis, K., Child, C., Lu, W., & Lekeas, G. K. (2002). Agents Environments.Tech. rep. Technical Report IST32530/CITY/005/DN/I/a1, SOCS Consortium, 2002.IST32530/CITY/005/DN/I/a1.Stathis, K., Kakas, A., Lu, W., Demetriou, N., Endriss, U., & Bracciali, A. (2004).PROSOCS: platform programming software agents computational logic.Muller, J., & Petta, P. (Eds.), Proceedings Agent Theory Agent Implementation (AT2AI-4 EMCSR2004 Session M), pp. 523528, Vienna, Austria.Stathis, K., & Toni, F. (2004). Ambient Intelligence using KGP Agents. Markopoulos, P.,Eggen, B., Aarts, E. H. L., & Crowley, J. L. (Eds.), Ambient Intelligence: ProceedingsSecond European Symposium, EUSAI 2004, Vol. 3295 Lecture Notes ComputerScience, pp. 351362. Springer.Steiner, D. E., Haugeneder, H., & Mahling, D. (1991). Collaboration knowledge basesvia knowledge based collaboration. Deen, S. M. (Ed.), CKBS-90 ProceedingsInternational Working Conference Cooperating Knowledge Based Systems, pp.113133. Springer Verlag.Subrahmanian, V. S., Bonatti, P., Dix, J., Eiter, T., Kraus, S., Ozcan, F., & Ross, R. (2000).Heterogeneous Agent Systems. MIT Press/AAAI Press, Cambridge, MA, USA.Thomas, S. R. (1995). PLACA agent programming language. Wooldridge, M. J., &Jennings, N. R. (Eds.), Intelligent Agents, pp. 355370, Berlin. Springer-Verlag.van Otterlo, M., Wiering, M., Dastani, M., & Meyer, J.-J. (2003). CharacterizationSapient Agents. Hexmoor, H. (Ed.), International Conference IntegrationKnowledge Intensive Multi-Agent Systems (KIMAS-03), pp. 172177, Boston, Massachusetts. IEEE.Wooldridge, M. (2002). Introduction Multiagent Systems. John Wiley & Sons.Yip, A., Forth, J., Stathis, K., & Kakas, A. C. (2005). Software Anatomy KGP Agent.Gleizes, M. P., Kaminka, G. A., Nowe, A., Ossowski, S., Tuyls, K., & Verbeeck, K.(Eds.), EUMAS 2005 - Proceedings Third European Workshop Multi-AgentSystems, pp. 459472. Koninklijke Vlaamse Academie van Belie voor Wetenschappenen Kunsten.348fiJournal Artificial Intelligence Research 33 (2008) 79-107Submitted 07/07; published 09/08Use Automatically Acquired ExamplesAll-Nouns Word Sense DisambiguationDavid Martinezdavidm@csse.unimelb.edu.auUniversity Melbourne3010, Melbourne, AustraliaOier Lopez de Lacalleoier.lopezdelacalle@ehu.esUniversity Basque Country20018, Donostia, Basque CountryEneko Agirree.agirre@ehu.esUniversity Basque Country20018, Donostia, Basque CountryAbstractarticle focuses Word Sense Disambiguation (WSD), Natural Language Processing task thought important many Language Technologyapplications, Information Retrieval, Information Extraction, Machine Translation. One main issues preventing deployment WSD technology lacktraining examples Machine Learning systems, also known Knowledge Acquisition Bottleneck. method shown work small samples wordsautomatic acquisition examples. previously shown onepromising example acquisition methods scales produces freely available database150 million examples Web snippets polysemous nouns WordNet.paper focuses issues arise using examples, alone additionmanually tagged examples, train supervised WSD system nouns. extensiveevaluation lexical-sample all-words Senseval benchmarks showsable improve commonly used baselines achieve top-rank performance.good use prior distributions senses proved crucial factor.1. Introductionpaper devoted Word Sense Disambiguation (WSD) task Natural LanguageProcessing (NLP). goal task determine senses wordsappear context. instance, given sentence took money bank.,focus word bank, goal would identify intended sense,context would financial sense, instead possibilities like edgeriver sense. senses defined dictionary, knowledge-base ontology.task defined intermediate step towards natural language understanding.construction efficient algorithms WSD would benefit many NLP applicationsMachine Translation (MT), Information Retrieval (IR) systems (Resnik, 2006).instance, MT system translate previous example French, wouldneed choose among possible translations word bank. wordtranslated banque used financial sense (as example),rive used edge river sense. See work Vickrey, Biewald,c2008AI Access Foundation. rights reserved.fiMartinez, Lopez de Lacalle & AgirreTeyssier, Koller (2005) recent evaluation cross-lingual WSD MT. IRengines, would also useful determine sense word queryorder retrieve relevant documents, specially working multilingual documentsCross-Language Information Retrieval (CLIR), IR scenarios recall keyperformance factor, retrieving images captions. evidence favorusing WSD IR gathered lately (Kim, Seo, & Rim, 2004; Liu, Liu, Yu, & Meng,2004; Stevenson & Clough, 2004; Vossen, Rigau, Alegra, Agirre, Farwell, & Fuentes, 2006).WSD techniques also fill important role context Semantic Web.Web grown focusing human communication, rather automatic processing.Semantic Web vision automatic agents working informationWeb semantic level, achieving interoperability use common terminologiesontologies (Daconta, Obrst, & Smith, 2005). Unfortunately informationWeb unstructured textual form. task linking terms textsconcepts reference ontology paramount Semantic Web.Narrower domains like Biomedicine also calling WSD techniques. UnifiedMedical Language System (UMLS) (Humphreys, Lindberg, Schoolman, & Barnett, 1998)one extensive ontologies field, studies mapping terms medicaldocuments resource reported high levels ambiguity, calls WSDtechnology (Weeber, Mork, & Aronson, 2001).WSD received attention many groups researchers, general NLP booksdedicating separate chapters WSD (Manning & Schutze, 1999; Jurafsky & Martin, 2000;Dale, Moisl, & Somers, 2000), special issues WSD NLP journals (Ide & Veronis,1998; Edmonds & Kilgarriff, 2002), books devoted specifically issue (Ravin &Leacock, 2001; Stevenson, 2003; Agirre & Edmonds, 2006). interested reader startdedicated chapter Manning Schutze (1999) WSD book (Agirre& Edmonds, 2006). widespread interest motivated Senseval initiative1 ,joined different research groups common WSD evaluation framework since 1998.goal follow example successful competitive evaluations, like DUC(Document Understanding Conference) TREC (Text Retrieval Conference).WSD systems classified according knowledge use build models, derived different resources like corpora, dictionaries, ontologies.Another distinction drawn corpus-based systems, distinguishingrely hand-tagged corpora (supervised systems), require resource (unsupervised systems). distinction important effort requiredhand-tag senses high, would costly obtain tagged examples wordsenses languages, estimations show (Mihalcea & Chklovski, 2003). spitedrawback (referred knowledge acquisition bottleneck), recentefforts devoted improvement supervised systems, onesobtain highest performance, even current low amounts training data.systems rely sophisticated Machine Learning (ML) algorithms constructmodels based features extracted training examples.Alternatively, Senseval defines two kinds WSD tasks: lexical-sample all-words.lexical-sample task systems need disambiguate specific occurrences handful1. http://www.senseval.org80fiOn Use Automatically Acquired Examples All-Nouns WSDwords relatively large numbers training examples provided (more100 examples cases). all-words task, training data provided, testingdone whole documents. Systems need tag content words occurring texts,even small amounts external training data available.analysis results English lexical-sample exercise third editionSenseval (Mihalcea & Edmonds, 2004) suggested plateau performancereached ML methods. task, systems relatively large amountstraining data, many systems top, performing close other.systems able significantly improve baselines attained accuracies70% (Mihalcea, Chklovski, & Killgariff, 2004).case different all-words task (Snyder & Palmer, 2004), supervisedsystems also performed best. used training examples Semcor (Miller, Leacock,Tengi, & Bunker, 1993), sizable all-words sense-tagged corpus timewriting paper. scarcity examples use test documents corporaunrelated Semcor heavily affected performance, systems scoredbaseline method assigning frequent sense Semcor. order usefulNLP applications, WSD systems address knowledge acquisition bottleneck(or least significant part) word types, evaluated all-words tasks.Lexical-sample tasks useful evaluating WSD systems ideal conditions (i.e.regarding availability training data), show systems scalablewords vocabulary. work use lexical-sample task orderadjust parameters system, main evaluation all-words task.experiments designed accordingly: lexical-sample tests show empirical evidencespecific parameters, all-words evaluation compares systems stateart.article, explore method alleviate knowledge acquisition bottlenecklarge scale. use WordNet (Fellbaum, 1998) automatically acquire examplesWeb. seminal work Leacock, Chodorow, Miller (1998) showed approachpromising, good results small sample nouns. works fieldautomatic acquisition examples focused exploring different approachesacquisition process (Agirre, Ansa, Martinez, & Hovy, 2000; Mihalcea, 2002; Cuadros, Padro,& Rigau, 2006), straightforward application WSD. explorations typicallyrequired costly querying Web, thus tried limited number variationshandful words. approach different spirit: want go wholeprocess nouns, acquisition examples use WSDthorough evaluation Senseval 2 lexical-sample Senseval 3 all-words datasets.comes cost exploring different possibilities step,advantage showing results extensive, limited small setnouns.reasons, given prior work acquisition techniques, useefficient effective example acquisition method according independent experimentsperformed Agirre et al. (2000) Cuadros et al. (2006). focus paper thusissues arise using examples training data supervised MLsystem. paper show automatically acquired examples effectively81fiMartinez, Lopez de Lacalle & Agirreused without pre-existing data, deciding amount examples usesense (the prior distribution) key issue.objectives paper show existing methods acquire examplesWeb scale-up nouns, study issues arise examplesused training data all-nouns WSD system. goal build stateof-the-art WSD system nouns using automatically retrieved examples.Given cost large-scale example acquisition, decided limit scopework nouns. think noun disambiguation useful toolmany applications, specially IR tasks mentioned above. method easilyadapted verbs adjectives (Cuadros et al., 2006), plan pursue linefuture.work reported partially published two previous conference papers.method automatic acquisition examples described Agirre Lopezde Lacalle (2004). first try application examples Word Sense Disambiguation presented Agirre Martinez (2004b). paper present globalview whole system, together thorough evaluation, showsautomatically acquired examples used build state-of-the-art WSD systemsvariety settings.article structured follows. introduction, related work knowledge acquisition bottleneck WSD described Section 2, focus automaticexample acquisition. Section 3 introduces method automatically build SenseCorpus,automatically acquired examples WordNet senses. Section 4 describes experimental setting. Section 5 explores factors use SenseCorpus evaluateslexical-sample task. final systems thoroughly evaluated all-nounstask Section 6. Finally, Section 7 provides discussion, conclusionswork outlined Section 8.2. Related Workconstruction WSD systems applicable words goal many research initiatives. section describe related work looks waysalleviate knowledge acquisition bottleneck using following techniques: bootstrapping, active learning, parallel corpora, automatic acquisition examples acquisitiontopic signatures. Sections 5 6, evaluate proposed system public datasets,review best performing systems literature.Bootstrapping techniques consist algorithms learn instances labeleddata (seeds) big set unlabeled examples. Among approaches, highlightco-training (Blum & Mitchell, 1998) derivatives (Collins & Singer, 1999; Abney,2002). techniques appropriate WSD NLP taskswide availability untagged data scarcity tagged data. However,systems shown perform well fine-grained WSD. well-known work,Yarowsky (1995) applied iterative bootstrapping process induce classifier basedDecision Lists. minimum set seed examples, disambiguation results comparablesupervised methods obtained limited set binary sense distinctions,success extended fine-grained senses.82fiOn Use Automatically Acquired Examples All-Nouns WSDRecent work bootstrapping applied WSD also reported Mihalcea (2004)Pham, Ng, Lee (2005). former, use unlabeled data significantlyincreases performance lexical-sample system. latter, Pham et al. applyWSD classifier all-words task Senseval-2, targeting words thresholdfrequency Semcor WSJ corpora. observe slight increase accuracyrelying unlabeled data.Active learning used choose informative examples hand-tagging, orderreduce manual cost. one works directly applied WSD, Fujii, Inui, Tokunaga, Tanaka (1998) used selective sampling acquisition examplesdisambiguation verb senses, iterative process human taggers. informativeexamples chosen following two criteria: maximum number neighbors unsuperviseddata, minimum similarity supervised example set. Another active learningapproach Open Mind Word Expert (Mihalcea & Chklovski, 2003), projectcollect sense-tagged examples Web users. system selects examplestagged applying selective sampling method based two different classifiers, choosingunlabeled examples disagreement. collected data usedSenseval-3 English lexical-sample task.Parallel corpora another alternative avoid need hand-tagged data. RecentlyChan Ng (2005) built classifier English-Chinese parallel corpora. groupedsenses share Chinese translation, occurrences wordEnglish side parallel corpora considered disambiguated sensetagged appropriate Chinese translations. system successfully evaluatedall-words task Senseval-2. However, parallel corpora expensive resourceobtain target words. related approach use monolingual corpora secondlanguage use bilingual dictionaries translate training data (Wang & Carroll,2005). Instead using bilingual dictionaries, Wang Martinez (2006) applied machinetranslation text snippets foreign languages back English achieved good resultsEnglish lexical-sample WSD.automatic acquisition training examples, external lexical resource (WordNet,instance) sense-tagged corpus used obtain new examples largeuntagged corpus (e.g. Web). Leacock et al. (1998) present method obtain sensetagged examples using monosemous relatives WordNet. approach basedearly work (cf. Section 3). algorithm, Leacock et al. (1998) retrieve numberexamples per sense, give preference monosemous relatives consistmultiword containing target word. experiment evaluated 14 nounscoarse sense-granularity senses. results showed monosemouscorpus provided precision close hand-tagged data.Another automatic acquisition approach (Mihalcea & Moldovan, 1999) used informationWordNet (e.g. monosemous synonyms glosses) construct queries, laterfed Altavista2 search engine. Four procedures used sequentially, decreasingorder precision, increasing levels coverage. Results evaluated hand,showing 91% examples correctly retrieved among set 1,080 instances120 word senses. However, corpus resulting experiment used2. http://www.altavista.com83fiMartinez, Lopez de Lacalle & Agirretrain real WSD system. Agirre Martinez (2000), early precursor workpresented here, tried apply technique train WSD system unsatisfactoryresults. authors concluded examples correct,somehow mislead ML classifier, providing biased features.related work, Mihalcea (2002) generated sense tagged corpus (GenCor) usingset seeds consisting sense-tagged examples four sources: (i) Semcor, (ii) WordNet,(iii) examples created using method above, (iv) hand-tagged examplessources (e.g. Senseval-2 corpus). means iterative process, system obtainednew seeds retrieved examples. total, corpus 160,000 examplesgathered. However, evaluation carried lexical-sample task, showingmethod useful subset Senseval-2 testing words (results 5 wordsprovided), without analysing sources performance gain. Evenwork presented uses techniques, work seen extensionlimited study, sense evaluate all-words tasks.previous works focused use two different kinds techniquesautomatic acquisition examples, namely, use monosemous relatives alone (Leacocket al., 1998) use combination monosemous relatives glosses (Mihalcea& Moldovan, 1999; Mihalcea, 2002). cases examples directly used feedsupervised ML WSD system, limited evaluation indicationmethods scale-up. Unfortunately, direct comparison alternative methodsparameters automatically acquire examples WSD exists, see preferenceuse Web, existing corpora would contain occurrences monosemousterms gloss fragments.closely related area automatic acquisition examples WSDenriching knowledge bases topic signatures. instance, Agirre et al. (2000)Agirre, Ansa, Martinez, Hovy (2001) used combined monosemous-relatives plusglosses strategy query Altavista, retrieve original documents build lists relatedwords word sense (so called topic signatures). topic signatures difficultevaluate hand, applied context vectors WSD straightforward way.Note authors train ML algorithm, rather combined examplesone vector per sense. showed using Web compared favorably usingfixed corpus, computationally costly: system first needs query searchengine retrieve original document order get example sense.alternative, Agirre Lopez de Lacalle (2004) showed possible scalegather examples nouns WordNet query limited using monosemousrelatives snippets returned Google used instead whole document.point, Cuadros et al. (2006) set systematic framework evaluationdifferent parameters affect construction topic signatures, includingmethods automatically acquire examples. study explores wide range queryingstrategies (monosemous synonyms, monosemous relatives different distances, glosses,combined using either operators) particular corpus (the British National Corpus) Web. best results obtained using Infomap3 BritishNational Corpus monosemous relatives method Web (Agirre & Lopez de3. http://infomap-nlp.sourceforge.net84fiOn Use Automatically Acquired Examples All-Nouns WSDLacalle, 2004). Contrary method, Infomap returns lists related words,thus used retrieve training examples. results confirmedexperiments reported Cuadros Rigau (2006).all, literature shows using monosemous relatives snippetsWeb (Agirre & Lopez de Lacalle, 2004) provides method automatically acquire examplesscales nouns WordNet, provides topic signatures better qualityalternative methods. explain examples acquired.3. Building Sense-Tagged Corpus Nouns Automaticallyorder build corpus (which refer SenseCorpus) acquired 1,000Google snippets monosemous noun WordNet 1.6 (including multiwords, e.g.church building). Then, word sense ambiguous noun, gathered examples monosemous relatives (e.g. sense #2 church, gather examplesrelative church building). way collect examples simply querying corpusword string words (e.g. church building). method inspiredwork Leacock et al. (1998) and, already mentioned Section 2, shownefficient effective experiments topic signature acquisition.basic assumption method given word sense target word,monosemous synonym word sense, examples synonymsimilar target word sense, could therefore used trainclassifier target word sense. idea , lesser extent, appliedmonosemous relatives, direct hyponyms, direct hypernyms, siblings, indirecthyponyms, etc. expected reliability decreases distance hierarchymonosemous relative target word sense.actual method build SenseCorpus following. collected examplesWeb monosemous relatives. relatives associated number(type), correlates roughly distance target word, indicatesrelevance: higher type, less reliable relative. Synonyms type 0, directhyponyms get type 1, distant hyponyms receive type number equal distancetarget sense. Direct hypernyms get type 2, generaltarget sense, thus introduce noise direct hyponyms. also decidedinclude less reliable siblings, type 3. sophisticated schemes could tried,using WordNet similarity weight distance target relativeword. However, chose approach capture notion distance simplicity,avoid testing many parameters. sample monosemous relatives differentsenses church, together sense inventory WordNet 1.7 shown Figure 1.following subsections describe step step method constructcorpus. First explain acquisition highest possible amount examples persense, explain different ways limit number examples per sensebetter performance.3.1 Collecting Examplesmethod collect examples previously published (Agirre & Lopez deLacalle, 2004), comprises following steps:85fiMartinez, Lopez de Lacalle & AgirreSense inventory (church)Sense 1: group Christians; group professing Christian doctrine belief.Sense 2: place public (especially Christian) worship.Sense 3: service conducted church.Monosemous relatives different senses (of church)Synonyms (Type 0): church building (sense 2), church service (sense 3) ...Direct hyponyms (Type 1): Protestant Church (sense 1), Coptic Church (sense 1) ...Direct hypernyms (Type 2): house prayer (sense 2), religious service (sense 3) ...Distant hyponyms (Type 2,3,4...):(sense 1)...Greek Church (sense 1), Western ChurchSiblings (Type 3): Hebraism (sense 2), synagogue (sense 2) ...Figure 1: Sense inventory sample monosemous relatives WordNet 1.7 church.1: query Google4 monosemous relatives sense, extractsnippets returned search engine. snippets used (up 1,000),dropped next step.2: try detect full meaningful sentences snippets contain targetword. first detect sentence boundaries snippet extract sentenceencloses target word. sentences filtered out, according followingcriteria: length shorter 6 words, non-alphanumeric characters wordsdivided two, words uppercase lowercase.3: automatically acquired examples contain monosemous relative targetword. order use examples train classifiers, monosemous relative (whichmultiword term) substituted target word. case monosemousrelative multiword contains target word (e.g. Protestant Church church)choose substitute, Protestant, instance, useful featurefirst sense church. tried alternatives, Section 5 showobtain slightly better results substitution applied multiwords.4: given word sense, collect desired number examples (see followingsection) order type: first retrieve examples type 0, type 1, etc.type 3 necessary examples obtained. collect examples type 4upwards. make distinctions relatives type. ContraryLeacock et al. (1998) give preference multiword relatives containingtarget word.all, acquired around 150 million examples nouns WordNet usingtechnique, publicly available5 .4. use off-line XML interface kindly provided Google research.5. http://ixa.si.ehu.es/Ixa/resources/sensecorpus.86fiOn Use Automatically Acquired Examples All-Nouns WSD3.2 Number Examples per Sense (Prior)Previous work (Agirre & Martinez, 2000) reported distribution numberexamples per word sense (prior short) strong influence qualityresults. is, results degrade significantly whenever training testing samplesdifferent distributions senses. also shown type-based approachpredicts majority sense word domain provide good performance(McCarthy, Koeling, Weeds, & Carroll, 2004).extracting examples automatically, decide many examplesuse sense. order test impact prior, different settingstried:prior: take equal amount examples sense.Web prior: take examples gathered Web.Automatic ranking: number examples given ranking obtained followingmethod McCarthy et al. (2004).Sense-tagged prior: take number examples proportional relative frequency word senses hand-tagged corpus.first method assumes uniform priors. second assumes numbermonosemous relatives occurrences correlated sense importance, is,frequent senses would occurrences monosemous relatives. fourthmethod uses information hand-tagged corpus, typically Semcor. Notelast kind prior requires hand-tagged data, rest (including third methodbelow) completely unsupervised.third method sophisticated deserves clarification. McCarthy et al. (2004) present method acquire sense priors automatically domaincorpus. two-step process. first step corpus-based method, giventarget word builds list contextually similar words (Lin, 1998) weights.case, co-occurrence data gathered British National Corpus. instance,given target word like authority, list topmost contextually similar words include government, police, official agency 6 . second step ranks sensestarget word, depending scores WordNet-based similarity metric (Patwardhan& Pedersen, 2003) relative list contextually similar words. Followingexample, pairwise WordNet similarity authority government greatersense 5 authority, evidence sense prominence corpus.pairwise similarity scores added, yielding ranking 7 senses authority.Table 2 shows column named Auto.MR normalized scores assignedsenses authority according technique.Table 1 shows number examples per type (0,1,...) acquired churchfollowing Semcor prior. last column gives number examples Semcor. Notenumber examples sometimes smaller 1,000 (maximum number snippetsreturned Google one query). due rare monosemous relatives,6. Actual list words taken demo http://www.cs.ualberta.ca/~lindek/demos/depsim.htm.87fiMartinez, Lopez de Lacalle & AgirreSensechurch#1church#2church#3Overall00306147453147610005762524561201,10530000Total1,0009671672,134Semcor605810128Table 1: Examples per type (0,1,2,3) acquired Web three senseschurch following Semcor prior, total number examples Semcor.Senseauthority#1authority#2authority#3authority#4authority#5authority#6authority#7OverallSemcor#ex1853211030%60.016.710.06.73.33.30.0100.0Web PR#ex%3380.54493266.41079816.08861.365269.6720.141066.167657100.0Auto.#ex1387593672057167716SenseCorpusMRSemcor PR%#ex%19.333833.710.527727.613.016616.69.411111.128.6555.59.9555.59.410.1100.01003100.0Semcor#ex32490543618181541MR%59.916.610.06.73.33.30.2100.0Sensevaltest#ex%3737.41717.211.000.03434.31010.100.099100.0Table 2: Distribution examples senses authority different corpora. PR(proportional) MR (minimum ratio) columns correspond different waysapply Semcor prior.usually caused sentence extraction filtering process, discards around 50%snippets.way apply prior straightforward. illustration, focusSemcor prior. first approach Semcor prior, assigned 1,000 examplesmajor sense Semcor, gave senses proportion examples. callmethod proportional (PR). cases number examples extractedless expected distribution senses Semcor. result, actual numberexamples available would follow desired distribution.alternative, computed, word, minimum ratio (MR) examplesavailable given target distribution given number examples extractedWeb. observed last approach would reflect better original prior,cost less examples.Table 2 presents different distributions examples authority. seeSenseval-testing Semcor distributions, together total number examplesWeb (Web PR); Semcor proportional distribution (Semcor PR) minimumratio (Semcor MR); automatic distribution minimum ratio (Auto MR).Getting maximum one thousand examples per monosemous relative allows get44,932 examples second sense (Web PR column), 72 sixth sense.88fiOn Use Automatically Acquired Examples All-Nouns WSDSemcorWordartauthoritybarbumchairchannelchildchurchcircuitdaydetentiondykefacilityfatiguefeelingAverageTotalWebprior15,38767,65750,92517,24424,62531,58247,6198,70421,97784,4482,6504,21011,0496,2379,60124,137699,086Automaticprior2,6107165,3294,7452,11110,0157916,3555,0953,6605118431,1965,4779453,455100,215Semcorprior10,65654116,6272,5558,5123,2353,5045,3763,5889,6901,5101,3678,5783,4381,1604,719136,874SemcorWordgriphearthholidayladymaterialmouthnationnaturepostrestraintsensespadestressyewWebprior20,8746,68216,71412,161100,10964860832,55334,96833,05510,3155,36110,35610,767Automaticprior2772,7301,8468846,3854646089,8138,0052,8772,1762,6573,0818,013Semcorprior2,2091,5311,2482,9597,85528759424,7464,2642,1522,0592,4582,1752,000Table 3: Number examples following different sense distributions Senseval-2nouns. Minimum ratio applied Semcor automatic priors.sixth sense single monosemous relative, rare word hitsGoogle, second sense many frequent monosemous relatives.Regarding use minimum ratio, table illustrates MR allows betterapproximate distribution senses Semcor: first sense7 60% Semcor,gets 33.7% SenseCorpus proportional Semcor prior338 examples SenseCorpus first sense. contrast SenseCorpusminimum ratio using Semcor assign 59.9% examples first sense.better approximation comes cost getting 541 examples authority, contrast1,003 PR. Note authority occurs 30 times Semcor.table also shows word distributions senses SemcorSenseval-test important differences (sense 5 gets 3.3% 34.3% respectively), although frequent sense same. Web automatic distributions,salient sense different Semcor, Web prior (Web PR column)assigning 0.5% first sense. Note automatic method able detectsense 5 salient test corpus, Semcor ranks 5th. general, distributiondiscrepancies similar table observed words testset.conclude section, Table 3 shows number examples acquired automaticallyword Senseval-2 lexical-sample following three approaches: Web prior,Semcor prior minimum ratio, Automatic prior minimum ratio.see retrieving examples (Web prior) get 24,137 examples average perword; respectively 4,700 3,400 apply Semcor prior Automatic prior.7. senses WordNet numbered according frequency Semcor, first senseWordNet paramount frequent sense Semcor.89fiMartinez, Lopez de Lacalle & Agirre3.3 Decision Listssupervised learning method used measure quality corpus Decision Lists(DL). simple method performs reasonably well comparison supervisedmethods Senseval words (as illustrate Table 6.4), preliminary experiments showed perform better automatically retrieved examplessophisticated methods like Support Vector Machines Vector Space Model. wellknown learning methods perform differently according several conditions, showedinstance Yarowsky Florian (2003), analyzed depth performancevarious learning methods (including DL) WSD tasks.think main reason DL perform better preliminary experimentsSenseCorpus noisy corpus conflicting features. Decision Lists usesingle powerful feature test context make predictions, contrastML techniques, could make perform better corpus. Speciallyall-words task, hand-tagged examples per word cases, evensophisticate ML algorithms cannot deal problem themselves.best systems Senseval-3 lexical-sample rely complex kernel-based methods,all-words task top systems find external ways deal sparsenessdata apply well-known methods, memory based learning decisiontrees (Mihalcea & Edmonds, 2004).DL algorithm described Yarowsky (1994). method, sense skhighest weighted feature fi selected, according log-likelihood (see Formula 1).implementation, applied simple smoothing method: casesdenominator zero, use 0.1 denominator. roughly equivalent assigning0.1 probability mass rest senses, shown effective enoughcompared complex methods (Yarowsky, 1994; Agirre & Martinez, 2004a).P r(sk |fi ))j6=k P r(sj |fi )weight(sk , fi ) = log( P(1)3.4 Feature Typesfeature types extracted context grouped three main sets:Local collocations: bigrams trigrams formed words around target.features constituted lemmas, word-forms, PoS tags8 . local featuresformed previous/posterior lemma/word-form context.Syntactic dependencies: syntactic dependencies extracted using heuristic patterns,regular expressions defined PoS tags around target9 . following relations used: object, subject, noun-modifier, preposition, sibling.Topical features: extract lemmas content words whole sentence4-word window around target. also obtain salient bigrams context,methods software described Pedersen (2001).8. PoS tagging performed fnTBL toolkit (Ngai & Florian, 2001).9. software kindly provided David Yarowskys group, Johns Hopkins University.90fiOn Use Automatically Acquired Examples All-Nouns WSDcomplete feature set applied main experiments all-words Senseval3 corpus. However, initial experiments lexical-sample task local featurestopical features (without salient bigrams) applied.4. Experimental Settingalready noted introduction lexical-sample evaluations defined Sensevalrealistic: relatively large amounts training examples available, drawncorpus test examples, train test examples taggedteam. Besides, developing system handful words necessarilyshow scalable. contrast, all-words evaluations provide training data.Supervised WSD systems typically use Semcor (Miller et al., 1993) training.corpus offers tagged examples open-class words occurring 350.000 word subsetbalanced Brown corpus, tagged WordNet 1.6 senses. contrast lexicalsample, polysemous words like authority get handful examples (30case, cf. Table 2). Note test examples (from Senseval) Semcor comedifferent corpora thus might related different domains, topics genres.added difficulty posed fact tagged different teamsannotators distinct institutions.mind, designed two sets experiments: first set performedsample nouns (lexical-sample), used develop fine-tune methodbasic aspects like effect kinds features importance prior.use training examples, except measure impact priors. providecomparison state-of-the-art systems.second set experiment used show method scalable, usefulnoun, performs state-of-the art WSD realistic setting. thusselected apply WSD nouns running text (all-nouns). setting applybest configurations obtained first set experiments, explore useSenseCorpus alone, combined priors Semcor, also training dataSemcor. provide comparison results state-of-the-art systems.lexical-sample evaluation, test part Senseval-2 English lexical-sampletask chosen, consisted instances 29 nouns, tagged WordNet 1.7 senses.advantage corpus could focus word-set enough examplestesting. Besides, different corpus, therefore evaluation realisticmade using cross-validation Semcor. order factor pre-processingfocus WSD, test examples whose senses multiwords phrasal verbsremoved. Note problematic since efficiently detectedmethods preprocess.important note training part Senseval-2 lexical-sample usedconstruction systems, goal test performance could achieveminimal resources (i.e. available word). relied Senseval-2training prior preliminary experiments local/topical features, upperboundcompare performance types priors.all-words evaluation relied Senseval-3 all-words corpus (Snyder &Palmer, 2004). test data task consisted 5,000 words text. data91fiMartinez, Lopez de Lacalle & Agirreextracted two Wall Street Journal articles one excerpt Brown Corpus.texts represent three different domains: editorial, news story, fiction. Overall, 2,212words tagged WordNet 1.7.1. senses (2,081 include multiwords).these, 695 occurrences correspond polysemous nouns part multiwords,comprise testing set.rest Senseval participants, added difficulty WordNet versionscoincide. therefore used one freely available mappings WordNetversions (Daude, Padro, & Rigau, 2000) convert training material Semcor(tagged WordNet 1.6 senses) WordNet 1.7 WordNet 1.7.1 versions (dependingtarget corpus). preferred use mapping rather relyingavailable mappings converted Semcors. knowledge, comparative evaluationamong mappings performed, Daude et al. show mapping obtainedhigh scores extensive manual evaluation. Note versions Semcoravailable Web (other original one, tagged WordNet 1.6) alsoobtained using automatic mapping.lexical-sample all-nouns settings, provide set baselines,based frequent heuristic. heuristic known hard beat WSD,specially unsupervised systems access priors, evensupervised systems all-nouns setting.5. Lexical-Sample Evaluationperformed four sets experiments order study different factors, compareperformance state-of-the-art unsupervised systems Senseval-2 lexical-sampletask. First analyzed results systems using different sets localtopical features, well substituting multiwords. next experimentsdevoted measure effect prior performance. that, comparedapproach unsupervised systems participated Senseval-2. mentionedintroduction, results obtained lexical-sample evaluations realistic,cannot expect hand-tagged data words target corpus.reason report results supervised systems (which use training data).next section all-nouns evaluation, realistic, compare supervisedsystems5.1 Local vs. Topical Features, SubstitutionPrevious work automatic acquisition examples (Leacock et al., 1998) reportedlower performance using local collocations formed PoS tags closed-class words.contrast, Kohomban Lee (2005), related approach, used local featuresWSD discriminated better senses. Given fact SenseCorpusalso constructed automatically, contradictory results previousworks, performed initial experiment comparing results using local features, topicalfeatures, combination both. case used SenseCorpus Senseval trainingprior, distributed according MR approach, always substituting target word.results (per word overall) given Table 4.92fiOn Use Automatically Acquired Examples All-Nouns WSDLocal Feats.WordartauthoritybarbumchairchannelchildchurchcircuitdaydetentiondykefacilityfatiguefeelinggriphearthholidayladymaterialmouthnationnaturepostrestraintsensespadestressyewOverallCoverage94.493.498.3100.0100.073.5100.0100.088.798.6100.0100.098.2100.0100.0100.0100.0100.0100.0100.0100.0100.0100.098.379.593.0100.0100.0100.096.7Precision57.451.253.081.288.754.056.567.751.160.287.589.329.182.555.119.073.496.380.443.236.880.644.444.737.162.574.253.981.558.5Recall54.247.852.181.288.739.756.567.745.359.487.589.328.682.555.119.073.496.380.443.236.880.644.443.929.558.174.253.981.556.5TopicalFeats.Recall45.643.255.987.588.753.755.651.654.254.787.589.321.482.560.238.075.096.373.944.238.680.639.340.537.537.272.646.181.556.0CombinedSubst.Recall47.046.257.285.088.755.956.554.856.156.887.589.321.482.560.239.075.096.373.943.839.580.640.740.537.138.474.248.781.557.0CombinedSubst.Recall44.946.257.285.088.757.458.951.658.060.487.589.321.482.560.238.075.096.373.942.939.580.640.740.537.148.874.248.781.557.5Table 4: Results per feature type (local, topical, combination), using SenseCorpusSenseval-2 training prior (MR). Coverage precision given localfeatures (topical combination full coverage). Combination shownsubstitution substitution options. best recall per word givenbold.93fiMartinez, Lopez de Lacalle & Agirreexperiment, observed local collocations achieved best precision overall, combination features obtained best recall. Local features achieve58.5% precision 96.7% coverage overall10 , topical combined featuresfull-coverage. table shows clear differences results per word, factalso known algorithms using real training data (Yarowsky & Florian, 2003).variability another important factor focus all-words settings, large numbersdifferent words involved.also show results substituting monosemous relative targetword monosemous relative multiword. see results mixed,slight overall improvement choose substitute cases.following experiments, chose work combination featuressubstitution, achieved best overall recall.5.2 Impact Priororder evaluate acquired corpus, first task analyze impactprior. mentioned Section 3.2, training Decision Lists examplesSenseCorpus, need decide amount examples sense (what seenestimation prior probabilities senses).Table 5 shows recall11 attained DL four proposed methodsestimate priors target word, plus use training part Senseval-2 lexicalsample estimate prior. Note last estimation method realistic, onecannot expect hand-tagged data words given target corpus,thus taken upperbound. fact presented section completeness,used comparison systems.results show constant improvement less informative priorsinformed ones. Among three unsupervised prior estimation methods, best resultsobtained automatic ranking, worst uniform distribution (no priorcolumn), distribution examples returned SenseCorpus (Web prior)middle. Estimating priors hand-tagged data improves results considerably,even target corpus estimation corpus different (Semcor), bestresults overall obtained priors estimated training part Senseval2 lexical-sample dataset. results word word show word behaves differently,well-known behavior WSD. Note priors except informedone number words performances 10%, might indicate DL trainedSenseCorpus sensitive badly estimated priors.Table 6 shows overall results Table 5, together obtained usingprior (prior only). results show improvement attained trainingSenseCorpus prominent unsupervised priors (from 6.5 19.7 percentagepoints), lower improvements (around 2.0 percentage points) priors estimatedhand-tagged corpora. results show clearly acquired corpus use10. Note due sparse data problem, test examples might feature commontraining data. cases DL algorithm return result, thus coveragelower 100%11. results following tables given recall, coverage always 100% precisionequals recall case.94fiOn Use Automatically Acquired Examples All-Nouns WSDUnsupervisedWordartauthoritybarbumchairchannelchildchurchcircuitdaydetentiondykefacilityfatiguefeelinggriphearthholidayladymaterialmouthnationnaturepostrestraintsensespadestressyewOverallprior34.020.924.736.761.342.240.343.844.315.352.192.919.658.827.211.357.870.424.351.739.580.621.936.826.344.874.238.670.438.0Webprior61.122.052.118.862.928.71.662.152.82.216.789.326.873.851.08.037.57.479.350.839.580.644.447.49.118.666.152.685.239.8Autom.ranking45.640.026.457.569.430.934.749.749.112.587.580.422.075.042.528.260.472.223.952.346.580.634.147.431.441.985.527.677.843.2Minimally-SupervisedSemcorprior55.641.851.65.088.716.254.048.441.548.052.192.926.882.560.216.075.096.380.454.254.480.646.734.227.347.767.72.666.749.8Senseval-2prior44.946.257.285.088.757.458.951.658.060.487.589.321.482.560.238.075.096.373.942.939.580.640.740.537.148.874.248.781.557.5Table 5: Performance (recall) SenseCorpus 29 nouns Senseval-2 lexical-sample,using different priors train DL. Best results word bold.ful information word senses, estimation prior extremelyimportant.PriorpriorWeb priorautom. rankingSemcor priorSenseval2 priorTypeunsupervisedminimallysupervisedprior18.333.336.147.855.6SenseCorpus38.039.843.249.857.5Diff.+19.7+6.5+7.1+2.0+1.9Table 6: Performance (recall) nouns Senseval-2 lexical-sample. row, resultsgiven prior own, SenseCorpus using prior, differenceboth.95fiMartinez, Lopez de Lacalle & AgirreMethodSenseCorpus (Semcor prior)UNEDSenseCorpus (Autom. prior)Kenneth Litkowski-clr-lsHaynes-IIT2Haynes-IIT1TypeminimallysupervisedunsupervisedRecall49.845.143.335.827.926.4Table 7: Results nouns best minimally supervised fully unsupervised systems (in bold) compared unsupervised systems took part Senseval-2lexical-sample.5.3 Comparison Systemspoint, important compare performance DL-based approachsystems state art. section compare best unsupervisedsystem (the one using Automatic ranking) minimally unsupervised system (usingSemcor prior) systems participating Senseval-2 deemed unsupervised. order results systems, used resources availableSenseval-2 competition, answers participating systems differenttasks available12 . made possible compare results test data,set nouns occurrences.5 systems presented Senseval-2 lexical-sample task unsupervised,WASP-Bench system relied lexicographers hand-code information semi-automatically(Tugwell & Kilgarriff, 2001). system use training data, usesmanually coded knowledge think falls supervised category.results 4 systems shown Table 7. classifiedUNED system (Fernandez-Amoros, Gonzalo, & Verdejo, 2001) minimally supervised.use hand-tagged examples training, heuristics appliedsystem rely prior information available Semcor. distribution sensesused discard low-frequency senses, also choose first sense back-offstrategy. conditions, minimally supervised system attains 49.8% recall,nearly 5 points better.rest systems fully unsupervised, perform significantly worseunsupervised system.6. Nouns Evaluationexplained introduction, main goal research develop WSDsystem able tag nouns context, sample them. previoussection explored different settings system, adjusting according resultshandful words lexical-sample task.12. http://www.senseval.org96fiOn Use Automatically Acquired Examples All-Nouns WSDsection test SenseCorpus 695 occurrences polysemous nounspresent Senseval-3 all-words task, compare results performancesystems participated competition. also present analysis resultsaccording frequency target nouns.developed three different systems, based SenseCorpus, different requirements external information. less informed system unsupervisedsystem (called SenseCorpus-U), use hand-coded corpus prior extracted therein. system relies examples SenseCorpus following Automatic Ranking (McCarthy et al., 2004) train DL (see Section 3.2). followingsystem minimally-supervised (SenseCorpus-MS), sense uses priorsobtained Semcor define distribution examples SenseCorpusfed DL. Lastly, informed system trains DL hand-taggedexamples Semcor SenseCorpus (following Semcor prior), knownSenseCorpus-S. three systems follow widely used distinction among unsupervised,minimally-supervised supervised systems, compare similarsystems participated Senseval-3.systems respond realistic scenarios. unsupervised system calledcase languages all-words hand-tagged corpus exists, cases priors coming Semcor appropriate, domain-specific corpora. minimallysupervised system useful hand-tagged corpora,indication distribution senses. Lastly, supervised system (SenseCorpus-S)shows performance SenseCorpus currently available conditions English,is, all-words corpus limited size available.order measure real contribution SenseCorpus, compare three systemsfollowing baselines: SenseCorpus-U vs. first sense accordingautomatically obtained ranking, SenseCorpus-MS vs. frequent sense Semcor,SenseCorpus-S vs. Decision Lists trained Semcor. order judgesignificance improvements, applied one-tail paired t-test.6.1 Comparison Unsupervised Systems Senseval-3systems participated all-words task three relyhand-tagged corpora (not even estimating prior information). compare performance systems unsupervised system SenseCorpus-U Table 8. ordermake fair comparison respect participants, removed answerscorrectly guess lemma test instance (discarding errors pre-processingSenseval-3 XML data).see one participating systems automatic ranking McCarthyet al. (2004) used baseline. Although able improve system,results best unsupervised system (IRST-DDD-LSI) (Strapparava, Gliozzo, &Giuliano, 2004). Surprisingly, unsupervised method able obtain better performancedataset version relies Semcor frequencies (IRST-DDD-0, see nextsubsection), discrepancy explained authors. reasonsremarkable results IRST-DDD-LSI clear, subsequent publicationsauthors shed light it.97fiMartinez, Lopez de Lacalle & AgirreCodeIRST-DDD-LSISenseCorpus-UAutoPS (Baseline)DLSI-UAMethodLSIDecision ListsAutomatic Rank.WordNet DomainsAttempt.570680675648Prec.64.645.544.627.8Rec.52.944.443.325.9F58.245.043.926.8p-value0.0010.0010.000Table 8: Performance unsupervised systems participating Senseval-3 all-words695 polysemous nouns, accompanied p-values one tailed paired t-testrespect unsupervised system (in bold).CodeSenseCorpus-MSMFS (Baseline)IRST-DDD-00Clr04-awKUNLPIRST-DDD-09MethodDLMFSDomain-drivenDictionary cluesSimilar relative WordNetDomain-drivenAttempt.695695669576628346Prec.63.962.755.658.754.269.7Rec.63.962.753.548.649.034.7F63.962.754.553.251.546.3p-value0.0440.0000.0000.0000.000Table 9: Performance minimally supervised systems participating Senseval-3 allwords 695 polysemous nouns, accompanied p-values one tailedpaired t-test respect SenseCorpus-MS (in bold).improvement baseline lower lexical-sample case,significant 0.99 level (significance 1p-value). order explore reasonsthis, performed experiments separating words different sets accordingfrequency Semcor, reported Section 6.4.6.2 Comparison Minimally Supervised Systems Senseval-3four systems Senseval used Semcor estimate sense distribution,without using examples word training. show performancesystems, together frequent sense baseline Table 9.results show SenseCorpus examples able obtain best performancekind systems, well rest. improvement Semcor MFS baselinesignificant 0.96 level.6.3 Comparison Supervised Systems Senseval-3systems participated all-words task supervised systemsrelied mainly Semcor. Table 10 present results top 10 competing systemssystem, trained SenseCorpus Semcor. also include DL systemtrained Semcor, baseline.results show using SenseCorpus able obtain significant improvement2.9% points F-score baseline. score places system second, close98fiOn Use Automatically Acquired Examples All-Nouns WSDCodeSenseLearnerSenseCorpus-SLCCawkuaw.ansR2D2EnglishGAMBL-AWupv-eaw.upv-eaw2Meaningupv-eaw.upv-eawProb5Semcor baselineUJAEN2MethodSyntactic PatternsDLEnsembleOptim.,TiMBLEnsembleDLAttempt.695695695695695695695695695691695695Prec.65.965.365.364.864.563.363.363.262.962.862.462.4Rec.65.965.365.364.764.563.363.363.262.962.462.462.4F65.965.365.364.764.563.363.363.262.962.662.462.4p-value0.3130.1660.1150.0540.0130.0140.0090.0070.0070.0060.002Table 10: Performance top 10 supervised systems participating Senseval-3 allwords 695 polysemous nouns, accompanied p-values one tailedpaired t-test respect SenseCorpus-S (in bold).best system all-nouns. statistical significance tests score 90%top 4 systems, 95% rest systems. means system performssimilar top three systems, significantly better rest.6.4 Analysis Performance Word Frequencyprevious sections observed different words achieve different rates accuracy.instance, lexical-sample experiments showed precision unsupervisedsystem ranged 12.5% 87.5% (cf. Table 5). Clearly, wordswhose performance low using SenseCorpus. section, groupnouns Senseval-3 all-nouns task according frequency see whethercorrelation frequency words performance system.goal identify sets words disambiguated higher accuracymethod. process would allow us previously detect type words systemapplied to, thus providing better tool work combination WSD systemsexploit properties language.study, created separate word sets according frequency occurrenceSemcor. Table 11 shows different word-sets, frequency ranges, numbernouns range, average polysemy. see frequent wordstend also polysemous. case supervised systems, polysemynumber training examples tend compensate other, yielding good resultskinds words. is, polysemous words difficult disambiguate, alsoexamples train Semcor (Agirre & Martinez, 2000).Table 12 shows results different frequency ranges top unsupervised systems Senseval-3, together method. see systemsperformance low high-frequency range. best performing system (IRSTDDD-LSI) profits use threshold leaves many instances unanswered.Regarding improvement SenseCorpus-U Automatic Ranking baseline (Au99fiMartinez, Lopez de Lacalle & AgirreRange010112021404160618081100101Overall#Nouns20710189885431125695Avg. Polysemy3.65.16.16.66.99.39.65.4Table 11: Number noun occurrences frequency ranges (in Semcor),average polysemy.010112021404160618081100101overallDLSI-UAAtt.F-sc.18835.989634.507515.828219.975422.20319.7012224.3064826.83IRST-DDD-LSIAtt.F-sc.19567.139169.778157.657555.215057.691936.025935.8557058.22SenseCorpus-UAtt.F-sc.19862.779858.908925.508542.845435.803123.7012529.6068045.00AutoPSAtt.F-sc19862.689849.258626.248542.755431.503129.0012331.4467543.95Table 12: Results unsupervised systems Senseval-3 words, evaluatednouns Semcor frequency range. Att. stands number wordsattempted range. Best F-score per system given bold.toPS), best results obtained low-frequency range (0-20), baselinescores 50-60% F-score range. results SenseCorpus lower baselinewords frequency higher 80. suggests system reliablelow-frequency words, simple threshold takes account frequency wordswould indicative performance expect. behavior also apparentunsupervised systems, shows weak spot kindsystems. think future research focus high frequency words.7. Discussionwork implemented evaluated all-words WSD system nounsable reach state-of-the-art performance three supervised, unsupervisedsemi-supervised settings. produced different systems combining SenseCorpusdifferent priors actual examples Semcor. supervised system, trainedhand-tagged (Semcor) automatically obtained corpora, reaches F-score65.3%, would rank second Senseval-3 all-nouns test data. semi-supervisedsystem, using priors Semcor manually-tagged examples, would rank first100fiOn Use Automatically Acquired Examples All-Nouns WSDclass, unsupervised system second. cases, SenseCorpus improvesbaselines.results remarkable. compare system came firstunsupervised supervised settings, see uses completelydifferent strategy. contrary, system, using primarily automatically acquiredexamples, able perform top ranks three settings.case, deep gap exists among following three kinds systems: (i) Supervisedsystems specific training (e.g. Senseval lexical-sample systems), (ii) Supervised systemsall-words training (e.g. trained using Semcor), (iii) Unsupervised systems.algorithm implemented all-words supervised system, unsupervised system. Although implementations obtain state-of-the-art performancecategories, different issues could addressed order close gaps,make all-words unsupervised performance closer supervised systems.identified three main sources error: low quality relatives appliedwords, different distributions senses training testing data, lowperformance high-frequency (and highly polysemous) words. examineturn.algorithm suffers noise introduced relatives far targetword, share local context it. Better filtering would requiredalleviate problem, one way could retrieve examplesshare part local context target word discard examples. Anotherinteresting aspect problem would identify type words achieve lowperformance SenseCorpus. already observed high-frequency words obtain lowperformance, another study performance according type relatives woulduseful better application algorithm.order deal words close WordNet relatives, another sourceexamples would use distributionally similar words. words would obtainedmethods one presented Lin (1998), retrieved examples wouldlinked target senses using WordNet similarity package (Patwardhan & Pedersen,2003).second main problem systems rely automatic acquisition factsense distributions training test data different, seriouslyaffects performance. system relies automatically-obtained sense rankingalleviate problem. However, words still get many examples sensesrelevant domain. preliminary experiments, observed benefit usingheuristics filter senses, using number close relatives WordNet,promising results.Finally, third problem observed Section 6.4, fact high-frequencywords profit automatically acquired examples. unsupervised methods,frequent (and polysemous) words get low performances, threshold-basedsystems usually discard answering them. straightforward way improve F-scoresystem would apply threshold discard words apply another methodback-off strategy them.all, detecting limitations system give us important clues worktowards accurate unsupervised all-words system. literature shows single101fiMartinez, Lopez de Lacalle & Agirreunsupervised system able perform well words. able identify typewords suited different algorithms heuristics, integrationalgorithms one single combined system could way go. instance, coulddetect cases relatives target word different apply SenseCorpusapproach, cases automatic ranking enough evidence. alsoobserved simple heuristics number close relatives WordNetsuccessfully applied sets words. Meta-learning techniques (Vilalta & Drissi, 2002)could useful exploit strengths unsupervised systems.8. Conclusions Future Workpaper presents evidence showing proper use automatically acquired examplesallows state-of-the-art performance WSD nouns. gathered examplesnouns WordNet 1.6 resource called SenseCorpus, amounting 150 million examples,made resource publicly available community.used examples train supervised WSD system, variety settings:own, combined prior information coming different sources, combinedtraining examples Semcor. Depending knowledge used, able build,respectively, unsupervised system seen hand-labeled training data,semisupervised one sees priors generic hand-labeled corpus (Semcor),fully-supervised system also uses generic hand-labeled corpus (Semcor)training data.evaluation lexical-sample all-words settings shown SenseCorpusimproves commonly used baselines combinations, achieves state-of-the-artperformance all-words Senseval-3 evaluation set nouns. Previous work automatic example acquisition evaluated handful words. contrastshown able scale nouns producing excellent results. way,learned use prior senses crucial apply acquired exampleseffectively.discussion outlined different ways overcome limitationssystem, proposed lines could improve significantly current performance.Although recent literature shows unsupervised system performshigh precision words, believe different systems complementother, usually perform well different sets words. meta-learning perspective, could build word-expert system able apply best knowledgesource problem: SenseCorpus, hand-tagged examples, simple heuristics,unsupervised algorithms incorporated.future work, aside proposed improvements, think wouldinteresting apply method testbeds. order applied, monosemousrelative method requires ontology raw corpus. resources found manyspecific domains, Biomedicine, fine-grainedness WordNet,could lead practical applications.102fiOn Use Automatically Acquired Examples All-Nouns WSDAcknowledgmentswork partially financed Ministry Education (KNOW project, ICT2007-211423) Basque Government (consolidated research groups grant, IT-397-07).Oier Lopez de Lacalle supported PhD grant Basque Government. DavidMartinez funded Australian Research Council, grant no. DP0663879.ReferencesAbney, S. (2002). Bootstrapping. Proceedings 40th Annual Meeting Association Computational Linguistics, ACL02, Philadelphia.Agirre, E., Ansa, O., Martinez, D., & Hovy, E. (2000). Enriching large ontologies usingWWW. Proceedings Ontology Learning Workshop, organized ECAI,Berlin (Germany).Agirre, E., Ansa, O., Martinez, D., & Hovy, E. (2001). Enriching WordNet conceptstopic signatures. Procceedings SIGLEX workshop WordNetLexical Resources: Applications, Extensions Customizations. conjunctionNAACL.Agirre, E., & Edmonds, P. (Eds.). (2006). Word Sense Disambiguation: AlgorithmsApplications. Springer.Agirre, E., & Lopez de Lacalle, O. (2004). Publicly available topic signatures WordNet nominal senses. Proceedings 4th International Conference LanguageResources Evaluation (LREC), Lisbon, Portugal.Agirre, E., & Martinez, D. (2000). Exploring automatic word sense disambiguation Decision Lists Web. Procedings COLING 2000 Workshop SemanticAnnotation Intelligent Content, Luxembourg.Agirre, E., & Martinez, D. (2004a). Smoothing word sense disambiguation. Proceedings Expaa Natural Language Processing (EsTAL), Alicante, Spain.Agirre, E., & Martinez, D. (2004b). Unsupervised WSD based automatically retrievedexamples: importance bias. Proceedings Conference EmpiricalMethods Natural Language Processing, Barcelona, Spain.Blum, A., & Mitchell, T. (1998). Combining labeled unlabeled data co-training.Proceedings 11h Annual Conference Computational Learning Theory, pp.92100, New York. ACM Press.Chan, Y., & Ng, H. (2005). Scaling word sense disambiguation via parallel texts.Proceedings 20th National Conference Artificial Intelligence (AAAI 2005),Pittsburgh, Pennsylvania, USA.Collins, M., & Singer, Y. (1999). Unsupervised models named entity classification.Proceedings Joint SIGDAT Conference Empirical Methods NaturalLanguage Processing Large Corpora, EMNLP/VLC99, College Park, MD,USA.103fiMartinez, Lopez de Lacalle & AgirreCuadros, M., Padro, L., & Rigau, G. (2006). empirical study automatic acquisitiontopic signatures. Proceedings Third International WordNet Conference, JejuIsland (Korea).Cuadros, M., & Rigau, G. (2006). Quality assessment large scale knowledge resources.Proceedings 2006 Conference Empirical Methods Natural Language Processing, pp. 534541, Sydney, Australia. Association Computational Linguistics.Daconta, M., Obrst, L., & Smith, K. (2005). Semantic Web: Guide FutureXML, Web Services, Knowledge Management. John Wiley & Sons.Dale, R., Moisl, H., & Somers, H. (2000). Handbook Natural Language Processing. MarcelDekker Inc.Daude, J., Padro, L., & Rigau, G. (2000). Mapping WordNets using structural information.38th Anual Meeting Association Computational Linguistics (ACL2000),Hong Kong.Edmonds, P., & Kilgarriff, A. (2002). Natural Language Engineering, Special Issue WordSense Disambiguation Systems. No. 8 (4). Cambridge University Press.Fellbaum, C. (1998). WordNet: Electronic Lexical Database. MIT Press.Fernandez-Amoros, D., Gonzalo, J., & Verdejo, F. (2001). UNED systems Senseval2. Proceedings SENSEVAL-2 Workshop. conjunction ACL, Toulouse,France.Fujii, A., Inui, K., Tokunaga, T., & Tanaka, H. (1998). Selective sampling example-basedword sense disambiguation. Computational Linguistics, No. 24 (4), pp. 573598.Humphreys, L., Lindberg, D., Schoolman, H., & Barnett, G. (1998). Unified MedicalLanguage System: informatics research collaboration. Journal AmericanMedical Informatics Association, 1 (5).Ide, N., & Veronis, J. (1998). Introduction special issue word sense disambiguation:state art. Computational Linguistics, 24 (1), 140.Jurafsky, D., & Martin, J. (2000). Introduction Natural Language Processing, Computational Linguistics, Speech Recognition. Prentice-Hall, Upper Saddle River,NJ 07458.Kim, S.-B., Seo, H.-C., & Rim, H.-C. (2004). Information retrieval using word senses: rootsense tagging approach. SIGIR 04: Proceedings 27th annual internationalACM SIGIR conference Research development information retrieval, pp.258265, New York, NY, USA. ACM.Kohomban, U., & Lee, W. (2005). Learning semantic classes word sense disambiguation. Proceedings 43rd Annual Meeting Association ComputationalLinguistics (ACL05).Leacock, C., Chodorow, M., & Miller, G. A. (1998). Using corpus statistics WordNetrelations sense identification. Computational Linguistics, Vol. 24, pp. 147165.Lin, D. (1998). Automatic retrieval clustering similar words. ProceedingsCOLING-ACL, Montreal, Canada.104fiOn Use Automatically Acquired Examples All-Nouns WSDLiu, S., Liu, F., Yu, C., & Meng, W. (2004). effective approach document retrievalvia utilizing WordNet recognizing phrases. SIGIR 04: Proceedings27th annual international ACM SIGIR conference Research developmentinformation retrieval, pp. 266272, New York, NY, USA. ACM.Manning, C. D., & Schutze, H. (1999). Foundations Statistical Natural Language Processing. MIT Press, Cambridge, Massachusetts.McCarthy, D., Koeling, R., Weeds, J., & Carroll, J. (2004). Finding predominant word sensesuntagged text. Proceedings 42nd Annual Meeting AssociationComputational Linguistics (ACL), Barcelona, Spain.Mihalcea, R. (2002). Bootstrapping large sense tagged corpora. Proceedings3rd International Conference Language Resources Evaluation (LREC), LasPalmas, Spain.Mihalcea, R. (2004). Co-training self-training word sense disambiguation. Proceedings Conference Natural Language Learning (CoNLL 2004), Boston,USA.Mihalcea, R., & Chklovski, T. (2003). Open Mind Word Expert: Creating large annotateddata collections Web users help. Proceedings EACL 2003 WorkshopLinguistically Annotated Corpora (LINC 2003), Budapest, Hungary.Mihalcea, R., Chklovski, T., & Killgariff, A. (2004). Senseval-3 English lexical sampletask. Proceedings 3rd ACL workshop Evaluation SystemsSemantic Analysis Text (SENSEVAL), Barcelona, Spain.Mihalcea, R., & Edmonds, P. (2004). Senseval-3, Third International WorkshopEvaluation Systems Semantic Analysis Text. Association Computational Linguistics.Mihalcea, R., & Moldovan, D. (1999). automatic method generating sense taggedcorpora. Proceedings AAAI-99, Orlando, FL.Miller, G. A., Leacock, C., Tengi, R., & Bunker, R. (1993). semantic concordance.Proceedings ARPA Human Language Technology Workshop, pp. 303308,Princeton, NJ. distributed Human Language Technology San Mateo, CA: MorganKaufmann Publishers.Ngai, G., & Florian, R. (2001). Transformation-Based Learning fast lane. Proceedings Second Conference North American Chapter AssociationComputational Linguistics, pp. 4047, Pittsburgh, PA, USA.Patwardhan, S., & Pedersen, T. (2003). cpan wordnet::similarity package.http://search.cpan.org/author/SID/WordNet-Similarity-0.03/.Pedersen, T. (2001). decision tree bigrams accurate predictor word sense.Proceedings Second Meeting North American Chapter AssociationComputational Linguistics (NAACL-01), Pittsburgh, PA.Pham, T. P., Ng, H. T., & Lee, W. S. (2005). Word sense disambiguation semisupervised learning. Proceedings 20th National Conference ArtificialIntelligence (AAAI 2005), pp. 10931098, Pittsburgh, Pennsylvania, USA.105fiMartinez, Lopez de Lacalle & AgirreRavin, Y., & Leacock, C. (2001). Polysemy: Theoretical Computational Approaches.Oxford University Press.Resnik, P. (2006). Word sense disambiguation natural language processing applications.Agirre, E., & Edmonds, P. (Eds.), Word Sense Disambiguation, chap. 11, pp. 299337. Springer.Snyder, B., & Palmer, M. (2004). English all-words task. Proceedings 3rdACL workshop Evaluation Systems Semantic Analysis Text (SENSEVAL), Barcelona, Spain.Stevenson, M. (2003). Word Sense Disambiguation: Case Combining KnowledgeSources. CSLI Publications, Stanford, CA.Stevenson, M., & Clough, P. (2004). Eurowordnet resource cross-language information retrieval. Proceedings Fourth International Conference LanguageResources Evaluation, Lisbon, Portugal.Strapparava, C., Gliozzo, A., & Giuliano, C. (2004). Pattern abstraction term similarityword sense disambiguation: IRST Senseval-3. Proceedings 3rd ACLworkshop Evaluation Systems Semantic Analysis Text (SENSEVAL), Barcelona, Spain.Tugwell, D., & Kilgarriff, A. (2001). WASP-Bench: lexicographic tool supporting wordsense disambiguation. Proceedings SENSEVAL-2 Workshop. conjunctionACL-2001/EACL-2001, Toulouse, France.Vickrey, D., Biewald, L., Teyssier, M., & Koller, D. (2005). Word-sense disambiguationmachine translation. Proceedings Human Language Technology ConferenceConference Empirical Methods Natural Language Processing.Vilalta, R., & Drissi, Y. (2002). perspective view survey meta-learning. ArtificialIntelligence Review, No. 18 (2), pp. 7795.Vossen, P., Rigau, G., Alegra, I., Agirre, E., Farwell, D., & Fuentes, M. (2006). Meaningfulresults information retrieval MEANING project. Proceedings ThirdInternational WordNet Conference, Jeju Island, Korea.Wang, X., & Carroll, J. (2005). Word sense disambiguation using sense examples automatically acquired second language. Proceedings joint Human LanguageTechnologies Empirical Methods Natural Language Processing conference, Vancouver, Canada.Wang, X., & Martinez, D. (2006). Word sense disambiguation using automatically translated sense examples. Proceedings EACL 2006 Workshop Cross LanguageKnowledge Induction, Trento, Italy.Weeber, M., Mork, J., & Aronson, A. (2001). Developing test collection biomedicalword sense disambiguation. Proceedings AMIA Symposium, pp. 746750.Yarowsky, D. (1994). Decision lists lexical ambiguity resolution: Application accentrestoration Spanish French. Proceedings 32nd Annual MeetingAssociation Computational Linguistics, pp. 8895, Las Cruces, NM.106fiOn Use Automatically Acquired Examples All-Nouns WSDYarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. Proceedings 33rd Annual Meeting Association ComputationalLinguistics, pp. 189196, Cambridge, MA.Yarowsky, D., & Florian, R. (2003). Evaluating sense disambiguation across diverse parameter spaces. Natural Language Engineering, 8 (2), 293310.107fiJournal Artificial Intelligence Research 33 (2008) 179-222Submitted 01/08; published 10/08Rigorously Bayesian Beam Model Adaptive FullScan Model Range Finders Dynamic EnvironmentsTinne De LaetJoris De SchutterHerman Bruyninckxtinne.delaet@mech.kuleuven.bejoris.deschutter@mech.kuleuven.beherman.bruyninckx@mech.kuleuven.beDepartment Mechanical EngineeringKatholieke Universiteit LeuvenCelestijnenlaan 300B, box 2420, 3001 Heverlee, BelgiumAbstractpaper proposes experimentally validates Bayesian network model rangefinder adapted dynamic environments. modeling assumptions rigorously explained, model parameters physical interpretation. approach resultstransparent intuitive model. respect state art beam modelpaper: (i) proposes different functional form probability range measurementscaused unmodeled objects, (ii) intuitively explains discontinuity encounteredstate art beam model, (iii) reduces number model parameters, maintaining representational power experimental data. proposed beam modelcalled RBBM, short Rigorously Bayesian Beam Model. maximum likelihoodvariational Bayesian estimator (both based expectation-maximization) proposedlearn model parameters.Furthermore, RBBM extended full scan model two steps: first,full scan model static environments next, full scan model general, dynamicenvironments. full scan model accounts dependency beams adaptslocal sample density using particle filter. contrast Gaussian-based stateart models, proposed full scan model uses sample-based approximation.sample-based approximation enables handling dynamic environments capturing multimodality, occurs even simple static environments.1. Introductionprobabilistic approach, inaccuracies embedded stochastic nature model,particularly conditional probability density representing measurement process.vital importance types inaccuracies affecting measurements incorporated probabilistic sensor model. Inaccuracies arise sensor limitations, noise,fact complex environments represented perceivedlimited way. dynamic nature environment particular important sourceinaccuracies. dynamic nature results presence unmodeled possiblymoving objects people.paper proposes probabilistic range finder sensor model dynamic environments.Range finders, widely used mobile robotics, measure distances z objectsenvironment along certain directions relative sensor. derive sensorc2008AI Access Foundation. rights reserved.fiDe Laet, De Schutter & Bruyninckxmodel form suitable mobile robot localization, i.e.: P (Z = z | X = x, = m)1 ,Z indicates measured range, X position mobile robot (andsensor mounted it), environment map. presented model howeveruseful applications range sensors well.First, paper derives probabilistic sensor model one beam range finder,i.e. beam model. particular, paper gives rigorously Bayesian derivation using Bayesian network model stating model assumptions giving physicalinterpretation model parameters. obtained model named RBBM, shortRigorously Bayesian Beam Model. innovations presented approach (i)introduce extra state variables = positions unmodeled objects probabilistic sensor model P (z | x, m, a), (ii) marginalize extra state variablestotal probability estimation. latter required extra variables(exponentially!) increase computational complexity state estimation lotapplications estimating position unmodeled objects primary interest.summary, marginalization avoids increase complexity infer probabilitydistributions P (x) P (m), maintaining modeling dynamic natureenvironment.paper furthermore presents maximum-likelihood variational Bayesian estimator (both based expectation-maximization) learn model parametersRBBM.Next, paper presents extension RBBM full scan model i.e.: P (z | , x, m)z contain measured distances beam angles, respectively. fullscan model accounts dependency beams adapts local sampledensity using particle filter. contrast Gaussian-based state art models,proposed full scan model uses sample-based approximation. sample-based approximation allows us capture multi-modality full scan model, shownoccur even simple static environments.1.1 Paper Overviewpaper organized follows. Section 2 gives overview related work. Section 3(i) presents Bayesian beam model range finders founded Bayesian networks,RBBM, (ii) mathematically derives analytical formula probabilistic sensor modelclearly stating assumptions, (iii) provides useful insights obtained beammodel (iv) shows obtained analytical sensor model agrees proposedBayesian network. Section 4 presents maximum likelihood variational Bayesianestimator (both based expectation-maximization) learn model parameters.Section 5 model parameters RBBM learned experimental dataresulting model compared state art beam model proposed Thrun,Burgard, Fox (2005), called Thruns model. Section 6 extends RBBMadaptive full scan model dynamic environments. Section 7 discusses obtainedRBBM adaptive full scan model compares previously proposed rangefinder sensor models.1. simplify notation, explicit mention random variable probabilities omitted wheneverpossible, replaced common abbreviation P (x) instead writing P (X = x).180fiRigorously Bayesian Beam Model Adaptive Full Scan Model2. Related WorkThree basic approaches deal dynamic environments exist literature (Fox,Burgard, & Thrun, 1999; Thrun et al., 2005): state augmentation, adapting sensormodel outlier detection.state augmentation latent states, e.g. position moving objects people environment, included estimated states. Wang, Thorpe, Thrun(2003) developed algorithm SLAM DATMO, short SLAM detectiontracking moving objects. State augmentation however often infeasible sincecomputational complexity state estimation increases exponentially numberindependent state variables estimate. closely related solution consists adaptingmap according changes environment. Since approaches assumeenvironment almost static, unable cope real dynamics populatedenvironments (Fox et al., 1999). recent, related approach proposed WolfSukhatme (2004) maintains two coupled occupancy grids environment, onestatic map one moving objects, account environment dynamics.Probabilistic approaches extent robust unmodeled dynamics, sinceable deal sensor noise. approaches however, sensor noisereflect real uncertainty due unmodeled dynamics environment. Therefore,second approach dealing dynamic environments adapt sensor modelcorrectly reflect situations measurements affected unmodeled environment dynamics. Fox et al. (1999) show approaches capable modelnoise average, and, approaches work reliably occasional sensor blockage,inadequate situations fifty percent measurementscorrupted.handle measurement corruption effectively, approach based outlier detection used. approach uses adapted sensor model, explainedprevious paragraph. idea investigate cause sensor measurementreject measurements likely affected unmodeled environment dynamics.Hahnel, Schulz, Burgard (2003a) Hahnel, Triebel, Burgard, Thrun (2003b)studied problem performing SLAM environments many moving objects usingEM algorithm filtering affected measurements. so, ableacquire maps environment conventional SLAM techniques failed. Fox et al.(1999) propose two different kinds filters: entropy filter, suited arbitrary sensor, distance filter, designed proximity sensors. filters detect whethermeasurement corrupted not, discard sensor readings resulting objectscontained map.paper focuses (sonar laser) range finders, whose physical principleemission sound light wave, followed recording echo. Highly accuratesensor models would include physical parameters surface curvature materialabsorption coefficient. parameters however difficult estimate robustly unstructured environments. Hence, literature typically relies purely basic geometricmodels.range finder sensor models available literature traditionally dividedthree main groups: feature-based approaches, beam-based models correlation-based181fiDe Laet, De Schutter & Bruyninckxmethods. Feature-based approaches extract set features range scan matchfeatures contained environmental model. Beam-based models, also knownray cast models, consider distance measurement along beam separate rangemeasurement. models represent one-dimensional distribution distancemeasurement parametric function, depends expected range measurementrespective beam directions. addition, models closely linked geometry physics involved measurement process. often result overlypeaked likelihood functions due underlying assumption independent beams.last group range finder sensor models, correlation-based methods, build local mapsconsecutive scans correlate global map. simple efficient likelihoodfield models end point model (Thrun, 2001) related correlation-based methods. Plagemann, Kersting, Pfaff, Burgard (2007) nicely summarize advantagesdrawbacks different range finder sensor models.Range finder sensor models also classified according whether use discretegeometric grids (Hahnel et al., 2003a, 2003b; Fox et al., 1999; Burgard, Fox, Hennig, &Schmidt, 1996; Moravec, 1988) continuous geometric models (Thrun et al., 2005; Choset,Lynch, Hutchinson, Kantor, Burgard, Kavraki, & Thrun, 2005; Pfaff, Burgard, & Fox,2006). Moravec proposed non-Gaussian measurement densities discrete grid possible distances measured sonar; likelihood measurements computedpossible positions mobile robot given time. Even simplified models (Burgardet al., 1996) approach turned computationally expensive real-timeapplication. Therefore, Fox et al. proposed beam model consisting mixture twophysical causes measurement: hit object map, objectyet modeled map. last cause accounts dynamic nature environment. analogous mixture (Thrun et al., 2005; Choset et al., 2005) adds twophysical causes: sensor failure unknown cause resulting max-range measurement random measurement, respectively. Thrun et al. Pfaff et al. usecontinuous model, Choset et al. present discrete analog mixture, takingaccount limited resolution range sensor. Pfaff et al. extend basic mixturemodel use Monte Carlo localization. overcome problems due combinationlimited representational power peaked likelihood accurate range finder,propose adaptive likelihood model. likelihood model smooth globallocalization peaked tracking.Recently, different researchers tried tackle problems associated beam-basedmodels, caused independence assumptions beams. Plagemann et al. (2007)propose sensor model full scan. model treats sensor modeling tasknon-parametric Bayesian regression problem, solves using Gaussian processes.claimed Gaussian beam processes combine advantages beam-basedcorrelation-based models. Due underlying assumption measurementsjointly Gaussian distributed, Gaussian beam processes suited takeaccount non-Gaussian uncertainty due dynamic nature environment.alternative approach handle overly-peaked likelihood functions resultingtraditional beam models proposed Pfaff, Plagemann, Burgard (2007). locationdependent full scan model takes account approximation error sample-basedrepresentation, explicitly models correlations individual beams introduced182fiRigorously Bayesian Beam Model Adaptive Full Scan Modelpose uncertainty. measurements assumed jointly Gaussian distributedPlagemann et al. proposed. Plagemann et al. represent covariancematrix parametrized covariance function using Gaussian processes whose parameterslearned data, Pfaff et al. learn full covariance matrix less restrictivemanner. Despite modeled correlation beams, measurements stillassumed jointly Gaussian distributed, limits applicability dynamicenvironments.paper proposes rigorously Bayesian modeling probabilistic range sensorbeam model dynamic environments, referred RBBM. Similar work Thrunet al. (2005) Pfaff et al. (2006) sensor model derived continuous geometry.Unlike previous models Thrun et al. (2005), Pfaff et al. (2006), Fox et al. (1999)Choset et al. (2005), mixture components founded Bayesian modeling.modeling makes use probabilistic graphical models, case Bayesian networks.graphical models provide simple way visualize structure probabilistic model,used design motivate new models (Bishop, 2006). inspectiongraph, insights model, including conditional independence properties obtained.Next, inspired adaptive full scan models literature (Pfaff et al., 2006, 2007;Plagemann et al., 2007), RBBM extended adaptive full scan model.underlying sample-based approximation full scan model, contrast Gaussianbased approximation proposed Pfaff et al. (2007) Plagemann et al., enables handlingdynamic environments capturing multi-modality, occurs even simple staticenvironments.3. Beam Modelmodel probabilistic beam model P (Z = z | X = x, = m) dynamic environments Bayesian network. introduce extra state variables = positionsunmodeled objects probabilistic sensor model P (z | x, m, a). prevent exponential increase computational complexity state estimation due extravariables, variables marginalized total probability estimation.marginalization:P (z | x, m) =ZP (z | x, m, a) P (a) da,(1)avoids increasing complexity infer conditional probability distributions interest,P (x) P (m), maintains modeling dynamic nature environment.Section 3.1 explains extra state variables physically relevant, Section 3.3explains marginalization extra state variables. Section 3.5 summarizes assumptions approximations. Finally, Section 3.6 provides useful insights obtainedbeam model, called RBBM, derivation. Section 3.7 shows obtained analytical expression RBBM agrees proposed Bayesian network meansMonte Carlo simulation.183fiDe Laet, De Schutter & BruyninckxpXNXN jnKXKikZocclZFigure 1: Bayesian network probabilistic measurement model supplementeddeterministic parameters represented smaller solid nodes.compact representation plates (the rounded rectangular boxes) used.plate represents number, indicated lower right corner, independentnodes single example shown explicitly.184fiRigorously Bayesian Beam Model Adaptive Full Scan Model3.1 Bayesian ModelBayesian networks graphically represent probabilistic relationships variablesmathematical model, structure facilitate probabilistic inference computationsvariables (Jensen & Nielsen, 2007; Neapolitan, 2004). Bayesian network definedfollows: (i) set nodes, associated random variable, connected directededges forming directed acyclic graph (DAG); (ii) discrete (continuous) random variable finite (infinite) set mutually exclusive states; (iii) random variableparents B1 , . . . , BN conditional probability distribution P (A | B1 , . . . , Bn ) (knownconditional probability table case discrete variables). Although definitionBayesian networks refer causality, requirement directededges represent causal impact, well-known way structuring variables reasoninguncertainty construct graph representing causal relations (Jensen & Nielsen, 2007).case graphical models also known generative models (Bishop, 2006), sincecapture causal process generating random variables.application, range sensor ideally measures z , distance closestobject map. unknown number n unmodeled objects, possibly preventingmeasurement closest object map, however present environment.Depending position jth unmodeled object along measurement beam, xN j ,unmodeled object occludes map not. unmodeled object occludesmap located front closest object contained map. k total numberoccluding objects n unmodeled objects. positions occludingobjects measurement beam denoted {xKi }i=1:k . map occludedunmodeled object, range sensor ideally measure zoccl= xKc , xKc positionclosest occluding object.following extra state variables, Eq. (1), included Bayesian model: Ndiscrete random variable indicating unknown number unmodeled objectsenvironment; XN j continuous random variable position jth unmodeledobject measurement beam; K discrete random variable indicating numberobjects occluding measurement map; XKi continuous random variableposition ith occluding object measurement beam; Zocclcontinuous random variable indicating ideal range measurement closest occludingobject. Fig. 1 shows Bayesian network probabilistic range finder sensor modelvariables Z, X occur probabilistic sensor model (definedSection 1), extra variables N, XN = {XN j }j=1:n , K, XK = {XKi }i=1:k , Zocclmodel parameters p (defined Section 3.2).directed edges graphical model represent causal relationshipsvariables. example, X unambiguously determine measured range Zperfect sensor absence unmodeled occluding objects. number occludingobjects K depends total number N unmodeled objects positions XNrespect measurement beam. X also causal impact K: largerexpected measurement z , higher possibility one unmodeled objectsoccluding modeled object corresponding expected measurement. positionsalong measurement beam XK occluding objects equal positionsK N unmodeled objects occluding map. Therefore, random variables XK185fiDe Laet, De Schutter & BruyninckxP (n)0.30.20.10 1 2 3 4 5 6 7 8 9 10nFigure 2: P (n) (Eq. (2)) p = 0.65.influenced K also XN . Since K objects occluding map,positions along measurement beam limited interval [0, z ], XKcausal dependency X . ideal measurement zoccloccluding objectposition occluding object closest sensor, Zoccl depends positions XKoccluding objects. Finally, measurement Z also depends ideal measurementoccluding object Zocclnumber occluding objects K. case occlusion(k 1), zoccl ideally measured, else (no occlusion, k = 0) z ideally measured.3.2 Conditional Probability DistributionsInferring probability distribution extra state variables P (n) oftenfeasible. Marginalization extra state variables Z, X, M, N, XN , K, XK , Zocclavoidsincrease complexity estimation problem, still takes account dynamic nature environment. Marginalization requires modelling conditionalprobability tables conditional probability distributions (pdf ) random variableconditionally parents.First all, assumptions made P (n). Assume probabilitynumber unmodeled objects decreases exponentially, i.e. P (n) given by:P (n) = (1 p) pn ,(2)p measure degree appearance unmodeled objects. precisely, pprobability least one unmodeled object present. p indicated Fig. 1,Fig. 2 shows resulting distribution P (n).Secondly, assume nothing known priori position unmodeledobjects along measurement beam. Hence unmodeled objects position assumeduniformly distributed measurement beam (Fig. 3):(1xN j zmaxP (xN j ) = zmax(3)0otherwise,186fiRigorously Bayesian Beam Model Adaptive Full Scan ModelP (xN j | n)u1zmaxz0zmaxzFigure 3: P (xN j | n) (Eq. (3)).P (k | n, x, m)0.30.20.10 1 2 3 4 5 6 7 8 9 10kFigure 4: P (k | n, x, m) (Eq. (5)) n = 10 u = 0.25.zmax maximum range range sensor.Thirdly, assume positions unmodeled objects independent:P (xN | n) =nP (xN j ) .(4)j=1Next, expression needed conditional probability: P (k | n, xN , x, m), i.e.probability k n unmodeled objects occluding map m. unmodeledobject occluding map located along measurement beam frontclosest object map. straightforward show P (k | n, xN , x, m)binomial distribution:!nuk (1 u)nk k n(5)P (k | n, xN , x, m) =k0otherwise,nu probability unmodeled object occluding map=kn!(nk)!k! number ways selecting k objects total n objects. Fig. 4 shows187fiDe Laet, De Schutter & BruyninckxXN 1XN 2XN 3...XN j...XK1XK2...XKi...XN n2XN n1XN nXKkFigure 5: selection scheme, cross eliminates unmodeled objectoccluding map.binomial distribution. Since assumed positions unmodeled objectsuniformly distributed, u, probability unmodeled object occludingmap is:u = P (xN j < z ) =zzmax,(6)depicted Fig. 3.Furthermore, analytical expression P (xK | xN , k) necessary. positionsoccluding objects xK equal positions unmodeled objects xNoccluding map, shown Fig. 5. words, xKi equals xN junmodeled object occluding map, i.e. xN j z :1(xKi xN j ) = zmaxz (xKi xN j ) xN j zP (xN j z )P (xKi | xN j , k, x, m) =(7)0otherwise,Dirac function xKi occluding object corresponding xN j .case occlusion, range sensor ideally measures distance closest occluding object xKc :P (zoccl| xK ) = (zocclxKc ) .(8)range finders truly quite deterministic since measurements greatextent explainable underlying physical phenomena specular reflections, inference,... underlying phenomena complex therefore costly model. topunderlying phenomena additional uncertainty measurement due (i) uncertaintysensor position, (ii) inaccuracies world model (iii) inaccuracies sensoritself. far disturbances measurements due unmodeled objects environment included. capture additional uncertainty, additional measurement noiseadded. taking account disturbances unmodeled objects, unexplainablemeasurements sensor failures (Section 3.4), physical reason expectmean value true measurements deviates expected measurementtrue measurements distributed asymmetrically around mean. Therefore188fiRigorously Bayesian Beam Model Adaptive Full Scan Modelsymmetrical noise mean value zero added. Two facts justify modelingmeasurement noise normal distribution: (i) normal distribution maximizesinformation entropy among distributions known mean variance, makingnatural choice underlying distribution data summarized terms sample meanvariance; (ii) underlying phenomena assumed small, independenteffect measurement, central limit theorem states certain conditions(such independent identically distributed finite variance), sumlarge number random variables approximately normally distributed. measurement noise modeled zero mean Gaussian standard deviation , conditional, k) is:probability P (z | x, m, zoccl(N (z; z , )k = 0P (z | x, m, zoccl, k) =(9)N (z; zoccl , ) k 1,, k) two main cases, first k = 0conditional probability P (z | x, m, zocclocclusion present sensor observing map m, second casek 1 sensor observes occluding object. included Bayesiannetwork Fig. 1.3.3 Marginalizationsection shows different steps needed marginalize extra state variablesEq. (1), motivates approximation leads analytical sensor model.product rule rewrites sensor model P (z | x, m) as:P (z | x, m) =P (z, x, m)P (z, x, m)=,P (x, m)P (x) P (m)(10)since X independent. numerator obtained marginalizing joint) x ,probability whole Bayesian network pjoint = P (z, x, m, xN , n, xK , k, zocclN:n, xK , k zocclZXZ XZpjoint dxN dxK dzoccl.(11)P (z, x, m) =zocclkxKnxNUsing chain rule factorize joint distribution making use conditionaldependencies Bayesian network (Fig. 1) yields:| xK ) P (k | n, xN , x, m), k) P (zocclpjoint = P (z | x, m, zocclP (xK | xN , k, x, m) P (xN | n) P (n) P (x) P (m) .Substituting (12) (11) (10) gives:ZZX, k)P (z | x, m, zocclP (z | x, m) =zocclxKkXP (zoccl| xK )P (k | n, x, m) P (n) P (xK | n, k, x, m) dxK dzoccl,n189(12)(13)fiDe Laet, De Schutter & BruyninckxP (xKi | x, m)1zxKi0zzmaxxKiFigure 6: P (xKi | n, k, x, m) (Eq. (15))P (xK | n, k, x, m) =ZP (xK | xN , k, x, m) P (xN | n) dxN .(14)xNSince binomial distribution P (k | n, xN , x, m) Eq. (5) independent xN ,moved integral xN (14), denoted P (k | n, x, m).Marginalizing xN study integral xN Eq. (14) focus xN j ,position one unmodeled object. Substituting (3) (7) results in:Z z1zmax(xKi xN j )dxN jP (xKi | n, k, x, m) =zmaxxN j =0 z(1xKi zz=(15)0otherwise.equation expresses xKi uniformly distributed conditioned n, k, xshown Fig. 6. Since occluding objects considered independent:( k10 k : xKi zz(16)P (xK | n, k, x, m) =0otherwise.equation shows P (xK | n, k, x, m) independent n thus movedsummation n Eq. (13):ZX,(17)| k, x, m) P (k | x, m) dzoccl, k) P (zocclP (z | x, m, zocclP (z | x, m) =zocclkP (k | x, m) =XP (k | n, x, m) P (n) ,(18)nP (zoccl| n, k, x, m) =ZxKP (zoccl| xK ) P (xK | k, x, m) dxK .190(19)fiRigorously Bayesian Beam Model Adaptive Full Scan ModelMarginalizing n First focus summation n Eq. (18) substitute (2)(5):P (k | x, m) =Xnuk (1 u)nk (1 p) pn .k(20)n=kAppendix proves infinite sum simplifies to:P (k | x, m) = 1 p pk ,(21)p =.1 (1 u) p(22)Marginalizing xK focus integral xK Eq. (19). Substituting (8)equation results in:ZP (zoccl | k, x, m) =(zocclxKc ) P (xKc | k) dxKcxKc| k, x, m) .= P (xKc = zoccl(23)equation shows conditional probability P (zoccl| k, x, m) represents prob , i.e. probability perfect measurement nearest occluding object zocclability nearest occluding object located zoccl . case onek objects along measurement beam located zocclmeasured,objects along measurement beam located behind occluding object,expressed probabilities:P (zoccl| k, x, m) =kXP (xK6=i zoccl| k, x, m) P (xKi = zoccl| k, x, m) .(24)i=1Since xK uniformly distributed [0, z ] shown Eq. (15), follows that:| k, x, m) =P (xKi = zocclP (xKi zoccl| k, x, m) =1,zz zoccl,z(25)(26)(24) written as:P(zoccl1| k, x, m) = kzz zocclzk1.(27)Marginalizing k obtaining expressions P (k | x, m) (Eq. (21)) P (zoccl| k, x, m)(Eq. (27)) turn attention summation k Eq. (17):XP (z | x, m, zoccl, k) P (zoccl| k, x, m) P (k | x, m) .(28)P (z, zoccl| x, m) =k191fiDe Laet, De Schutter & BruyninckxSplit summation two parts: one k = 0, occlusion, one, k) givenk 1, substitute expressions P (k | x, m) P (z | x, m, zocclEq. (21) Eq. (9), respectively:P (z, zoccl| x, m) = N (z; z , ) P (zoccl| k = , x, m) P (k = | x, m) +N (z; zoccl, ) P (zoccl| k , x, m) P (k | x, m)= N (z; z , ) P (zoccl | k = , x, m) p +N (z; zoccl, ) (zoccl| x, m) ,(zoccl| x, m) ==P (zocclXk=1| k 1, x, m) P (k 1 | x, m)P (zoccl| k, x, m) 1 p pk .(29)(30)Substituting (27) (30) results in:(zoccl| x, m) =Xk=11kzz zocclzk1simplified using Eq. (114) Appendix A:(zoccl| x, m) ==X1k1 p pzp (1hz 1Substituting (32) (29) gives:k=1p )z zocclpz1 p pk ,z zocclpzk1(32)i2 .P (z, zoccl| x, m) = N (z; z , ) P (zoccl| k = , x, m) p +N (z; zoccl, )(31)zh( p ) p. (33)z zocclpzMarginalizing zocclSubstituting (33) (17) shows integration zocclstill carried out:Z zpdzocclN (z; zoccl, ) hP (z | x, m) = (1 p )N (z; z , ) + p.(34)z zoccl=zocclzpzfirst term right hand side Gaussian distribution around ideal measurement, multiplied probability occlusion (k = 0). second termintegration possible positions occluding object scaled Gaussian distribu ). scaling factortion centered ideal measurement occluding object (zocclrepresents probability occluding objects located zocclmeasured.Eq. (20) Eq. (32) follows scaling factor written as:(zoccl| x, m) =p (1 p)hi2 ,zocclzmax 1 1 zmaxp192(35)fiRigorously Bayesian Beam Model Adaptive Full Scan ModelPSfrag0.45Finite sumApproximation0.50.350.30.250.20.150.10.0501234567zFigure 7: Comparison approximation (Eq.(36)) integral Eq. (34)finite sum approximation small step size p = 0.8, zmax = 10, z = 5= 0.15.P (z | x, m)Finite sumApproximation1.00.900.860.90.820.80.70.780.64.90 4.95 5.00 5.05 5.100.50.40.30.20.1012345678910zFigure 8: Comparison obtained RBBM P (z | x, m) (Eq. (37)) finite sum approximation Eq. (34) small step size p = 0.8, zmax = 10, z = 5= 0.15.193fiDe Laet, De Schutter & Bruyninckxindependent z .now, approximations made obtain Eq. (34) beam modelP (z | x, m). integral scaled Gaussian distributions however, cannot obtained analytically. Therefore, first approximation marginalization made, )neglecting noise range measurement case occlusion, i.e.: N (z; zoccl(z zoccl ). Using approximation second term right hand side Eq. (34)becomes:p (1 p )p (1 p)(36)i2 .h2 =zzzz 1 z ppzmax 1 1 zmaxFig. 7 shows quality approximation integral Eq. ( 34) comparedfinite sum approximation small step size. approximation introduces discontinuityaround z = z . Using proposed approximation integral resulting beam modelis:(p )(1 p ) N (z; z , ) + pz zz zpz)][(zP (z | x, m) =(37)(1 p ) N (z; z , )otherwise,shown Fig. 8.RBBM written mixture two components:P (z | x, m) = 1 Phit (z | x, m) + 2 Poccl (z | x, m) ,(38)(39)1 = 1 p2 = p(40)Phit (z | x, m) = N (z; z , )1p1z 1 z z p 2[ ( z )]Poccl (z | x, m) =0(41)0 z z(42)otherwise.3.4 Extra ComponentsOccasionally, range finders produce unexplainable measurements, caused phantom readings sonars bounce walls, suffer cross-talk (Thrun et al., 2005). Furthermore additional uncertainty measurements caused (i) uncertaintysensor position, (ii) inaccuracies world model (iii) inaccuracies sensor itself.unexplainable measurements modeled using uniform distribution spreadentire measurement range [0, zmax ]:(10 z zmax ,(43)Prand (z | x, m) = zmax0otherwise.Furthermore, sensor failures typically produce max-range measurements, modeledpoint-mass distribution centered around zmax :(1 z = zmax ,Pmax (z | x, m) = (zmax ) =(44)0 otherwise.194fiRigorously Bayesian Beam Model Adaptive Full Scan Modeltwo extra components added Eq. (38), resulting final RBBM:P (z | x, m) = 1 Phit (z | x, m) + 2 Poccl (z | x, m) + 3 Prand (z | x, m) + 4 Pmax (z | x, m) ,(45)3 4 probabilities range finder returns unexplainable measurement maximum reading, respectively. Furthermore,1 = 1 p (1 3 4 )(46)2 = p (1 3 4 ),(47)Phit (z | x, m), Poccl (z | x, m), Prand (z | x, m) Pmax (z | x, m) given (41),(42), (43) (44) respectively.3.5 Assumptions Approximationssection summarizes assumptions approximations made arrive RBBMEq. (45).Section 3.2 makes four assumptions:(i) probability number unmodeled objects decreases exponentially, Eq. (2);(ii) unmodeled objects position uniformly distributed measurement beam(Fig. 3, Eq. (3));(iii) positions unmodeled objects independent, Eq. (4);(iv) measurement noise zero mean normally distributed standard deviation(Eq. 9).Furthermore, Section 3.3 makes one approximation obtain analytical expressionneglecting noise range measurement case occlusion (Eq. (34)).3.6 Interpretationfollowing paragraphs give insights RBBM derivation.mixture representation (45) shows four possible causes range measurement:hit map, hit unmodeled object, unknown cause resultingrandom measurement sensor failure resulting maximum reading measurement.derivation Section 3.3 shows position occluding objectsuniformly distributed sensor ideally measured object environment(Eq. (15), Fig. 6). perfectly reasonable considering assumption uniformlydistributed unmodeled objects.Furthermore, insights provided concerning (zoccl| x, m) (Eq. (35), Fig. 7),probability occluding objects located zocclmeasured. Firstall, probability independent location ideally measured objectenvironment (z ) (except probability zero z > z ). agrees intuition,since one expects measurements caused occluding objects independent z ,measurement case occlusion. Second, probability sensing unmodeledobjects decreases range, expected. easily explained followingthought experiment: two objects present likelihood perceptionfield range finder, first object closest range sensor, sensorlikely measure first object. measure second object, second object195fiDe Laet, De Schutter & Bruyninckxpresent first object absent (Thrun et al., 2005). Moreover,rate decrease likelihood sensing unmodeled objects dependent p,degree appearance unmodeled objects.probability measuring feature map, therefore integralscaled Gaussian (1 p )Phit (z | x, m) (45), decreases expected range. easilyexplained since probability map occluded decreases featurelocated away.Finally, discontinuity RBBM (Fig. 8) shown causedapproximation made (Section 3.5). Since state art range sensors accurate, neglecting measurement noise measurement occluding objectacceptable approximation. also shown experiments presented Section 5.respect state art beam model Thrun et al. (2005), model proposed here, Eq. (45), has: (i) different functional form probability range measurements caused unmodeled objects, (ii) intuitive explanation discontinuityencountered cited paper, (iii) reduction number model parameters.Thrun et al. find Poccl (z | x, m) exponential distribution. exponentialdistribution results following underlying assumptions (although revealedauthors): (i) unmodeled objects equally distributed environment (ii)beam reflected constant probability range. last assumption equalsassuming probability unmodeled object located certain distanceconstant. assumption fails capture number unmodeled object finiteprobable limited number unmodeled objects hugenumber them. also assume unmodeled objects equally distributedenvironment (Eq. (3)), assume number unmodeled objects geometrically distributed (Eq. (2)) capturing finiteness number unmodeled objectshigher probability smaller number unmodeled objects. modelingfiniteness number unmodeled objects higher probability smallernumber unmodeled objects results quadratic decay Poccl (z | x, m), insteadexponential decay Poccl (z | x, m) found Thrun et al..stated previous paragraph, discontinuity RBBM (Fig. 8) causedapproximation. Thruns model considers rate decay Poccl (z | x, m)independent 2 , probability occlusion, showndepend parameter p (Eq. (42), Eq. (47)). Therefore RBBM fewerparameters Thruns model.3.7 Validationgoal section show means Monte Carlo simulation2 RBBM,Eq. (45), agrees Bayesian network Fig. 1. Monte Carlo simulation approximate inference method Bayesian networks. idea behind Monte Carlo simulationdraw random configurations network variables Z, X, , N , XN = {XN j }j=1:n ,K, XK = {XKi }i=1:k Zocclsufficient number times. Random configurations selected ancestral sampling (Bishop, 2006), i.e. successively sampling2. Monte Carlo simulation also known stochastic simulation Bayesian network literature(Jensen & Nielsen, 2007).196fiRigorously Bayesian Beam Model Adaptive Full Scan ModelP (z | x, m)Finite sumApproximationMonte Carlo Simulation1.00.90.80.70.60.50.40.30.20.1012345678910zFigure 9: Comparison obtained RBBM P (z | x, m) (45), finite sum approximationEq. (34) small stepsize normalized histogram 500 samplesobtained Monte Carlo Simulation proposed Bayesian network (Fig. 1)p = 0.8, zmax = 10, z = 5, = 0.15, 3 = 0.2 4 = 0.02.states variables following causal model defined directed acyclic graphBayesian network.Fig. 9 shows RBBM agrees Monte Carlo simulation 500 samplesproposed Bayesian network.4. Variational Bayesian Learning Model ParametersRBBM, Eq. (45), depends four independent model parameters:= , p , 3 , 4 ,(48)zmax known sensor characteristic. set parameters clear physicalinterpretation; standard deviation zero mean Gaussian measurement noiseEq. (9) governing Phit (z | x, m) (Eq. (41)); p , defined Eq. (21), probabilitymap occluded (P (k 1 | x, m)); 3 4 probabilities range finderreturns unexplainable measurement (unknown cause) maximum reading (sensorfailure), respectively.alternative non-minimal set parameters containing mixing coefficients =[1 , 2 , 3 , 4 ] could used: = [m , ], provided constraint:S=4X= 1,s=1197(49)fiDe Laet, De Schutter & Bruyninckx1 , 2 , 3 , 4ZJFigure 10: Graphical representation mixture measurement model (Eq. (45))latent correspondence variable = {D1 , D2 , D3 , D4 } model parameters= [m , 1 , 2 , 3 , 4 ].taken account. set minimal parameters straightforwardly followsnon-minimal set since:p =2,1 3 4(50)seen Eq. (47).physical interpretation parameters allows us initialize handplausible values. However, another, flexible way learn model parametersactual data containing J measurements Z = {z }=1:J corresponding states X ={x }=1:J map m. Furthermore, learning model parameters also validationproposed analytical model: learning algorithm succeeds finding modelparameters resulting distribution gives good explanation data,analytical model likely agree well reality.paper two different estimators3 , maximum likelihood (ML) (Dempster, Laird,& Rubin, 1977; McLachlan & Krishnan, 1997; Bishop, 2006) variational Bayesian(VB) (Beal & Ghahramani, 2003; Bishop, 2006) estimator, presented learn modelparameters data. Section 4.1 derives maximum likelihood estimator,known approach problem, reformulated RBBM. ML estimatorprovides point estimates parameters leads overfitting since likelihoodfunction generally higher complex model structures. Therefore, proposevariational Bayesian (VB) estimator Section 4.2, new approach learningparameters beam models. VB estimator fully Bayesian learning approach; priorsunknown parameters included, complex (overfitting) models punished,full probability distribution parameters obtained.3. paper approximately follows notation Bishop (2006).198fiRigorously Bayesian Beam Model Adaptive Full Scan Model4.1 Maximum Likelihood Learningmaximum likelihood estimator proposed identify model parametersmaximize likelihood data Z corresponding X map m:= arg max log P (Z | X, m, ) .(51)using mixture representation RBBM (Eq. (45)), estimation problemformulated finding ML estimates parameters = [m , ] providedconstraint Eq. (49) included. general known fourpossible causes actually caused measurements. case ML estimationproblem difficult lacks closed-form solution. however, corresponding causesmeasurements known, solution easily obtained closed form.Therefore, introduce latent correspondence variable = [d1 , d2 , d3 , d4 ], representingunknown cause, using 1-of-S representation. elements ds give probabilitymeasurement result sth cause. graphical representationmixture formulation including latent correspondence variable shown Fig. 10.Although ML estimation problem lacks closed-form solution due unknowncorrespondences, expectation-maximization approach (EM) solve problemiterating expectation maximization step. expectation step calculatesexpectation correspondence variables ds maximization step computesmodel parameters expectations.Algorithm 1 ML estimator model parametersconvergence criterion satisfiedz Z, = 1 : J, J = |Z|1calculate zm= [ 1 Phit (z | x , m) + 2 Poccl (z | x , m) + 3 Prand (z | x , m) +4 Pmax (z | x , m)]1(d1 ) = 1 Phit (z | x , m)(d2 ) = 2 Poccl (z | x , m)(d3 ) = 3 Prand (z | x , m)(d4 ) = 4 Pmax (z | x , m)end P1 = J 1 P (d1 )2 = J 1 P (d2 )3 = J 1 P (d3 )4 = J 1 (d4 )p = 1324rP=2(dP 1 )(z z )(d)1endreturn = [m , p , 3 , 4 ]199fiDe Laet, De Schutter & Bruyninckxmarginal distribution correspondence variable specified termsmixing coefficients that:P (ds = 1) = ,parameters must satisfy following two conditions:0 1,PSs=1 = 1.(52)(53)Since uses 1-of-S representation, marginal distribution written as:P (d) =sds .(54)s=1EM-algorithm expresses complete-data log likelihood, i.e. log likelihoodobserved latent variables:JX(d1 (log 1 + log Phit (z | x , m)) +log P Z, | X, , ==1d2 (log 2 + log Poccl (z | x , m)) +d3 (log 3 + log Prand (z | x , m)) +d4 (log 4 + log Pmax (z | x , m))) ,(55)Z = {z }=1:J vector containing observed data = {d } vectorcontaining matching correspondences.Expectation step: Taking expectation complete-data log likelihood Eq. (55)respect posterior distribution latent variables gives:Q( , old ) = ED log P Z, | X, ,=JX( (d1 ) (log 1 + log Phit (z | x , m)) +=1(d2 ) (log 2 + log Poccl (z | x , m)) +(d3 ) (log 3 + log Prand (z | x , m)) +(d4 ) (log 4 + log Pmax (z | x , m))) ,(56)(ds ) = E [ds ] discrete posterior probability, responsibility (Bishop, 2006),cause data point z . E-step, responsibilities evaluated using Bayestheorem, takes form:1 Phit (z | x , m),Norm2 Poccl (z | x , m)(d2 ) = E [d ] =,Norm3 Prand (z | x , m)(d3 ) = E [d ] =,Norm4 Pmax (z | x , m),(d4 ) = E [d ] =Norm(d1 ) = E [d ] =200(57)(58)(59)(60)fiRigorously Bayesian Beam Model Adaptive Full Scan ModelNorm normalization constant:Norm = 1 Phit (z | x , m) + 2 Poccl (z | x , m) + 3 Prand (z | x , m) + 4 Pmax (z | x , m) .(61)Two measures derived responsibilities:Js =JX(ds ) ,(62)=1zs =J1 X(ds ) z ,Js(63)=1Js effective number data points associated cause s, zs meaneffective data points associated cause s.Maximization step: M-step expected complete-data log likelihood Eq. (56)maximized respect parameters = [m , ]:new = arg maxQ( , old ).(64)Maximization respect using Lagrange multiplier enforce constraintP= 1 results in:=Js,J(65)effective fraction points data set explained cause s. Maximizationrespect results in:vuJu1 X=d1 (z z )2 .(66)J1=1Algorithm 1 summarizes equations ML estimator, called ML-EMalgorithm.4.2 Variational Bayesian LearningML estimator provides point estimates parameters sensitiveoverfitting (Bishop, 2006). Therefore, propose variational Bayesian (VB) estimator,new approach learning parameters beam models. VB estimatorfully Bayesian learning approach; priors unknown parameters included, complex(overfitting) models punished, full probability distribution parametersobtained. VB estimator little computational overhead comparedML estimator (Bishop, 2006).Bayesian approach attempts integrate possible values uncertainquantities rather optimize ML approach (Beal, 2003; Beal & Ghahramani, 2003). quantity results integrating latent variables201fiDe Laet, De Schutter & BruyninckxRparameters known marginal likelihood4 : P (Z) = P (Z | D, ) P (D, ) d(D, ),P (D, ) prior latent variables parameters model. Integrating parameters penalizes models degrees freedom, sincemodels priori model larger range data sets. property Bayesian integrationsknown Occams razor, since favors simpler explanations data complexones (Jeffreys & Berger, 1992; Rasmussen & Ghahramani, 2000).Unfortunately, computing marginal likelihood, P (Z), intractable almostmodels interest. variational Bayesian method constructs lower boundmarginal likelihood, attempts optimize bound using iterative schemeintriguing similarities standard EM algorithm. emphasize similarityML-EM, algorithm based variational Bayesian inference called VB-EM.introducing distribution Q latent variables complete log marginallikelihood decomposed (Bishop, 2006):log P (Z) = L (Q) + KL (Q||P ) ,(67)(68)L (Q) =ZQ (D, ) logP (Z, D, )Q (D, )(D, ) ,KL (Q||P ) KL-divergence Q P . Since KL-divergence alwaysgreater equal zero, L (Q) lower bound log marginal likelihood. Maximizing lower bound respect distribution Q (D, ) equivalent minimizingKL-divergence. possible choice Q (D, ) allowed, maximumlower bound would occur KL-divergence vanishes, i.e. Q (D, ) equalposterior distribution P (D, | Z). Working true posterior distributionhowever often intractable practice. One possible approximate treatment considersrestricted family distributions Q (D, ) seeks member family minimizingKL-divergence. variational Bayesian treatment uses factorized approximation,case latent variables parameter :Q (D, ) = QD (D) Q () .(69)variational approach makes free form (variational) optimization L (Q) respectdistributions QD (D) Q (), optimizing respect factorsturn. general expressions optimal factors (Bishop, 2006):log QD (D) = E [log P (Z, D, )] + C te ,log Q ()te= ED [log P (Z, D, )] + C ,(70)(71)indicates optimality. expressions give explicit solution factors,optimal distribution one factors depends expectation computedrespect factor. Therefore iterative procedure, similar EM, cyclesfactors replaces turn revised optimal estimate used.4. avoid overloading notation conditioning map positions X = {x } associateddata Z = {z } explicitly written.202fiRigorously Bayesian Beam Model Adaptive Full Scan ModelIntroducing priors Since variational Bayesian approach fully Bayesian approach,priors introduced parameters = [, , ]. Remarkvariational Bayesian estimator standard deviation governing Phit (z | x, m)(Eq. (41)) estimated also means, referred on. Since analysisconsiderably simplified conjugate prior distributions used, Dirichlet prior chosenmixing coefficients :P () = Dir (|0 ) ,(72)well independent Gaussian-Wishart prior5 mean precision =1 Gaussian distribution Phit (z | x, m) (Eq. (41)):P (, ) = N | , (m ) W (m |W , ) .(73)0 gives effective prior number observations associated componentmixture. Therefore, value 0 set small, posterior distribution mainlyinfluenced data rather prior.Expectation stepwritten as:Using conjugate priors, shown factor QD (D)QD (D) =Jd1 d2 d3 d4r1r2 r3 r4 ,(74)=1quantities rs responsibilities analogous Eq. (57) givenby:rs =,1 + 2 + 3 + 4(75)log 1 = E [log 1 ] + E [log Phit (z | x , m)] ,(76)log 2 = E [log 2 ] + E [log Poccl (z | x , m)] ,(77)log 3 = E [log 3 ] + E [log Prand (z | x , m)] ,(78)log 4 = E [log 4 ] + E [log Pmax (z | x , m)] .(79)equations rewritten as:hlog 1 = E [log 1 ] + E [log |m |] log () E,m (z )T (z ) , (80)log 2 = E [log 2 ] + log Poccl (z | x , m) ,(81)log 3 = E [log 3 ] + log Prand (z | x , m) ,(82)log 4 = E [log 4 ] + log Pmax (z | x , m) ,(83)5. parameters defined Bishop (2006).203fiDe Laet, De Schutter & Bruyninckxexpectations calculated follows:E [log ] = (s ) (1 + 2 + 3 + 4 ) ,+ log 2 + log |W |,E [log |m |] =2hE,m (z )T (z )= 1 + (z )T W (z ) ,(84)(85)(86)digamma function.Three measures derived responsibilities:JXJs =(87)rs ,=1J1 Xrs z ,Jszs =(88)=1J1 Xrs (z zs ) (z zs )T ,JsCs =(89)=1Js effective number data points associated cause s, zs meaneffective data points associated cause Cs covariance effective datapoints associated cause s. Due similarity E-step EM-algorithm,step calculating responsibilities variational Bayesian inference knownvariational E-step.Maximization step accordance graphical representation Fig. 10,shown variational posterior Q () factorizes Q () Q (1 , )first optimal factor given Dirichlet distribution:Q () = Dir (|) ,(90)= 0 + Js .(91)second optimal factor given Gaussian-Wishart distribution:Q (1 , ) = N |, (m ) W (m |W, ) ,(92)= 0 + J1 ,1(0 0 + J1 z1 ) ,=W 1 = W01 + J1 C1 +(93)(94)0 J1(z1 0 ) (z1 0 )T ,0 + J1= 0 + J1 .(95)(96)204fiRigorously Bayesian Beam Model Adaptive Full Scan Modelupdate equations analogous M-step EM-algorithm maximumlikelihood solution therefore known variational M-step. variational M-stepcomputes distribution parameters (in conjugate family) rather pointestimate case maximum likelihood estimator. distributionparameters allows us calculate predictive density P (z | Z).Due use conjugate priors, integrals predictive density calculated analytically:1 St z|, 1+W, + 2 Poccl (z | x, m) + 3 Prand (z | x, m) + 4 Pmax (z | x, m)P (z | Z) =,1 + 2 + 3(97)St (.) Students t-distribution. size J data large, Studentst-distribution approximates Gaussian predictive distribution rewritten as:P (z | Z) =1 N (z|, ) + Poccl (z | x, m) + Prand (z | x, m) + Pmax (z | x, m). (98)1 + 2 + 3 + 4point estimates desired parameters, maximum posteriori estimatesobtained follows:+ + +12W=,1+2.=1 3 4= E [s ] =p(99)(100)(101)Algorithm 2 summarizes equations VB-EM estimator.5. Experimentsgoal section threefold: (i) learn model parameters (Eq. (48))RBBM (Eq. (45)) experimental data, (ii) compare results proposedML-EM VB-EM estimator (Section 4), (iii) compare results proposedestimators learning approach Thruns model proposed Thrun et al. (2005).end two experimental setups different application areas robotics used.data first learning experiment gathered typical mobile robot applicationrobot equipped laser scanner travelling office environment.data second learning experiment gathered typical industrial pickand-place operation human populated environment. laser scanner mountedindustrial robot make aware people unexpected objects robotsworkspace.see well learned model explains experiment, learned continuous pdfP (z | x, m, ) Eq. (45) compared discrete pdf experimentaldata (histogram) H (z). end, learned pdf first discretized using bins{zf }f =1:F experimental pdf. quantize difference learned205fiDe Laet, De Schutter & BruyninckxAlgorithm 2 Variational Bayesian estimator model parametersconvergence criterion satisfiedz Z, = 1 : J, J = |Z|1calculate zm111 = exp (1 ) (1 + 2 + 3 +4 ) + 2 2 + log 2 + log |W | 2 log (2) . . .21 1 + (z )T W (z )2 = exp [ (2 ) (1 + 2 + 3 + 4 ) + log Poccl (z | x, m)]3 = exp [ (3 ) (1 + 2 + 3 + 4 ) + log Prand (z | x, m)]4 = exp [ (4 ) (1 + 2 + 3 + 4 ) + log Pmax (z | x, m)]= 1 + 2 + 3 + 4r1 = 1 1r2 = 1 2r3 = 1 3r4 = 1 4endPJ1 = J=1 r1PJJ2 = =1 r2PJ3 = J=1 r3PJ4 = J=1 r4Pz1 = J11 Jj=1 r1 zPC1 = J11 J=1 r1 (z z1 ) (z z1 )T ,1 = 0 + J1 .2 = 0 + J2 .3 = 0 + J3 .4 = 0 + J4 .= 0 + J1= 1 (0 0 + J1 z1 )W 1 = W01 + J1 C1 += 0 + J10 J 10 +J1(z1 0 ) (z1 0 )T11 = 1 +2+3 +422 = 1 +2 +3 +433 = 1 +2+3 +444 = 1 +2 +3 +4p = 132412W= 1+endreturn {1 , 2 , 3 , 4 , , , W, , = [, , p , 3 , 4 ]}206fiRigorously Bayesian Beam Model Adaptive Full Scan ModelNN80080070070060060050050040040030030020020010010000.51.01.52.02.53.03.54.04.55.0z[m]00.5(a) Short range1.01.52.02.53.03.54.04.55.0z[m](b) Long rangeFigure 11: Data second learning experiment reported Thrun et al. (2005).data consist two series measurements obtained mobile robot traveling typical office environment. set measurements 10000measurements centered around two different expected ranges selected.experimental pdf two distance measures used: discrete KL-divergence:d1 = KL (H||P )FXH (zf ) logf =1H (zf ),P (zf | x, m, )square root discrete Hellinger distance:vu FuX11 2d2 = DH (H||P )H (zf ) 2 P (zf | x, m, ) 2 .(102)(103)f =1latter known valid symmetric distance metric (Bishop, 2006).5.1 First Learning Experimentfirst learning experiment, experimental data reported Thrun et al. (2005) used.data consists two series measurements obtained mobile robot travelingtypical office environment. set measurements, 10000 measurementscentered around two different expected ranges, selected. two obtainedsets different expected ranges shown Fig. 11. parameters learningalgorithms listed Table 1. Fig. 12 Table 2 show results ML-EMVB-EM estimators RBBM compared results ML estimator Thrunsmodel (Thrun et al., 2005) two sets. results obtained runninglearning algorithms 30 iteration steps.207fiDe Laet, De Schutter & BruyninckxML-EMRBBMm,init = 0.5pinit = 0.43,init = 0.24,init = 0.1VB-EMRBBM3,init = 18pinit = 13init = 5000 4,init = 18Winit = 120 = 5init = xmpW0 = 50init = 1000 = xmp1,init = 580 = 10012,init = 80 = 1ML-EMThruns modelm,init = 0.5zhit,init = 0.4zshort,init = 0.3zmax,init = 0.1zrand,init = 0.2short,init = 0.1Table 1: EM-parameters first second learning experiment (all SI-units).ML approaches, mean Phit (z | x, m) set xmp , i.e. probablebin histogram training set H (z).P (z | x, m)P (z | x, m)0.81.21.00.8ML-EM RBBMVB-EM RBBMML-EM Thruns modelHistogram Training set0.7ML-EM RBBMVB-EM RBBMML-EM Thruns modelHistogram Training set0.60.50.40.60.30.40.20.20.00.10.51.01.52.02.53.03.54.04.55.0z[m](a) Short range0.00.51.01.52.02.53.03.54.04.55.0(b) Long rangeFigure 12: Comparison results ML-EM VB-EM estimators RBBMresults maximum likelihood estimator Thruns model (Thrunet al., 2005) data Fig. 11.proposed ML-EM VB-EM estimator outperform ML-EM estimatorThruns model studied data sets. Despite reduced number parametersRBBM compared Thruns model (Section 3.6), RBBM leastrepresentational power.208z[m]fiRigorously Bayesian Beam Model Adaptive Full Scan ModelExperimentshort rangelong rangeaverageML-EMRBBM0.52950.43660.4830d1 (Eq. (102))VB-EMML-EMRBBM Thruns model0.51270.70790.43680.58520.47470.6465ML-EMRBBM0.31660.16830.2425d2 (Eq. (103))VB-EMML-EMRBBM Thruns model0.29710.56290.21000.34810.25350.4555Table 2: Discrete KL-divergence (d1 ) square root Hellinger distance (d2 ) firstlearning experiment training set results ML-EMVB-EM estimators RBBM ML-EM estimator Thruns model(Thrun et al., 2005).(a) Front view(b) Side view(c) Zoomed front viewFigure 13: Setup second learning experiment Sick LMS 200 laser scannermounted first axis industrial Kuka 361 robot.5.2 Second Learning Experimentdata second learning experiment gathered execution typicalindustrial pick-and-place operation human-populated environment. Sick LMS 200laser scanner mounted first axis industrial Kuka 361 robot (Fig. 13).laser scanner provides measurements robot environment therefore peopleunexpected objects robots workspace. Processing measurements firststep towards making industrial robots aware possibly changing environmentmoving robots cages.209fiDe Laet, De Schutter & BruyninckxRobot4selected rangescut regionenvironment2y[m]0-2-4-6-6-4-20x[m]246Figure 14: Map build robots static environment, i.e. without unexpected objectspeople moving around, rotating first axis industrial robot.safety reasons, people allowed move inside safety region (circleradius 1m). Therefore, measurements smaller 1m discarded.studied expected ranges second learning experiment range 3.0m4.5m steps 0.1m indicated figure selected rangesregion.first step, map (Fig. 14) build robots static environment, i.e. withoutunexpected objects people moving around, rotating first axis industrialrobot. Next, robot performs pick-and-place operation number peoplewalking around random robot environment. Different sets measurementsacquired different number people. Similar first learning experiment,measurements selected centered around different expected ranges acquireddata. studied expected ranges second learning experiment range 3.0m4.5m steps 0.1m (Fig. 14). safety reasons, people allowed move closer1m robot, i.e. safety region (Fig. 14). Therefore, measurements smaller1m discarded.data, model parameters learning using learning parametersfirst learning experiment (Table. 1).Table 3 shows Kullback Leibler divergence (Eq. (102)) Hellinger distance(Eq. (103)) averaged studied expected range different set measurementsrunning ML-EM VB-EM estimators RBBM ML estimatorThruns model (Thrun et al., 2005). results obtained runninglearning algorithms 30 iteration steps.210fiRigorously Bayesian Beam Model Adaptive Full Scan ModelExperimentnumberpeople123468averageML-EMRBBM1.79111.80021.77891.82771.80071.76761.7944d1 (Eq. (102))VB-EMML-EMRBBM Thruns model1.52711.96971.51721.97351.51991.96061.51401.98531.51681.96551.51571.94981.51851.9674ML-EMRBBM5.61415.70385.60335.75635.64835.49895.6375d2 (Eq. (103))VB-EMML-EMRBBM Thruns model4.34496.55824.33346.61194.34686.53654.29726.67444.31266.55964.28436.42574.31996.5611Table 3: Discrete KL-divergence (d1 ) square root Hellinger distance (d2 ) averagedstudied expected range different set measurements secondlearning experiment. Distances training set resultsML-EM VB-EM estimators RBBM ML-EM estimatorThruns model (Thrun et al., 2005). first column indicates numberpeople walking around environment particular set measurements.proposed ML-EM VB-EM estimator outperform ML-EM estimatorThruns model studied data sets. Despite reduced number parametersRBBM compared Thruns model (Section 3.6), RBBM leastrepresentational power.6. Adaptive Full Scan Modelsection extends RBBM adaptive full scan model dynamic environments;adaptive, since automatically adapts local density samples using samplebased representations; full scan, since model takes account dependencies individual beams.many applications using range finder, posterior approximated finiteset samples (histogram filter, particle filters). peaked likelihood function associatedrange finder (small due accuracy) problematic using finite setsamples. likelihood P (z | x, m) evaluated samples, approximatelydistributed according posterior estimate. Basic sensor models typically assumeestimate x map known exactly, is, assume onesamples corresponds true value. assumption, however, valid limitinfinitely many samples. Otherwise, probability value exactly correspondstrue location virtually zero. consequence, peaked likelihood functionsadequately model uncertainty due finite, sample-based representationposterior (Pfaff et al., 2007). Furthermore, use basic range finder model typicallyresults even peaked likelihood models, especially using large numberbeams per measurement, due multiplication probabilities. practice, problem211fiDe Laet, De Schutter & BruyninckxP (z | x, m)P (z | x, m)2.52.5ML-EM RBBMVB-EM RBBMML-EM Thruns modelHistogram Training set2.02.01.51.51.01.00.50.50.01.02.03.04.05.06.07.0ML-EM RBBMVB-EM RBBMML-EM Thruns modelHistogram Training set8.00.0z[m] 1.02.0(a) Short range, 3 people2.5ML-EM RBBMVB-EM RBBMML-EM Thruns modelHistogram Training set2.06.07.08.0z[m]1.51.01.00.50.54.05.06.07.08.0z[m]ML-EM RBBMVB-EM RBBMML-EM Thruns modelHistogram Training set2.01.53.05.0P (z | x, m)2.52.04.0(b) Short range, 8 peopleP (z | x, m)0.01.03.08.00.0z[m] 1.0(c) Long range, 3 people2.03.04.05.06.07.0(d) Long range, 8 peopleFigure 15: Comparison results ML-EM VB-EM estimators RBBMresults maximum likelihood estimator two different expectedranges two different number people populating robot environment.212fiRigorously Bayesian Beam Model Adaptive Full Scan Modelpeaked likelihoods, dealt various ways: sub-sampling measurement (fewerbeams); introducing minimal likelihoods beams; inflating measurement uncertainty;means regularization resulting likelihoods. solutions satisfactory however, since additional uncertainty due sample-based representationknown advance. additional uncertainty strongly varies numbersamples uncertainty estimate (Pfaff et al., 2006). Fox (2003) proposesdynamically adapt number samples means KLD sampling (KLD standsKullback-Leibler divergence). peaked likelihoods however, might resulthuge number samples. Lenser Veloso (2000) Thrun, Fox, Burgard, Dellaert (2001) ensure critical mass samples located important partsstate space sampling observation model. Sampling observation modelhowever, often possible approximate inaccurate way. Pfaff et al. (2006)introduced adaptive beam model dynamic environments, explicitly takes location uncertainty due sample-based representation account. computeadditional uncertainty due sample-based representation, using techniquesdensity estimation. evaluating likelihood function sample, considercertain region around sample, depending sample density location. Then,depending area covered sample, variance Gaussian, , governingbeam model Eq. (38), calculated sample. result, beam modelautomatically adapts local density samples. location dependent model results smooth likelihood function global localization peaked functionposition tracking without changing number samples.Plagemann et al. (2007) Pfaff et al. (2007) showed considering regionaround samples, individual beams become statistically dependent. degree dependency depends geometry environment size locationconsidered region. Beam models, RBBM, implicitly assume howeverbeams independent, is:P (z | , x, m) =BP (zb | b , x, m) ,(104)b=1z = {zb }b=1:B = {b }b=1:B vectors containing measured rangesangles different beams respectively; zb range measured beamangle b ; B total number beams P (zb | b , x, m) instanceRBBM (Eq. (45)). neglecting dependency beams, resulting likelihoodsP (z | , x, m) overly peaked. Models taking account dependenciesbeams consider full range scan therefore called full scan models on.full scan models proposed Plagemann et al. (2007) Pfaff et al. (2007)assume beams range scan jointly Gaussian distributed. off-diagonalelements covariance matrix associated Gaussian distribution representdependency. learn model parameters, methods draw samples regionaround sample perform ray-casting using samples. Plagemann et al. trainGaussian process models full scan, Pfaff et al. directly provide maximumlikelihood estimate mean covariance Gaussian.Section 6.1 shows dependency beams may introduce multi-modality,even simple static environments. Multi-variate Gaussian models proposed Plage213fiDe Laet, De Schutter & Bruyninckxmann et al. (2007) Pfaff et al. (2007) cannot handle multi-modality. Therefore,new sample-based method obtaining adaptive full scan model beam model,able handle multi-modality, proposed. Section 6.2 extends adaptive full scan modeldynamic environments taking account non-Gaussian model uncertainty.6.1 Sample-based Adaptive Full Scan Model Static EnvironmentsPlagemann et al. (2007) Pfaff et al. (2007) estimate full scan model, P (z | x, m)6 ,based local environment U (x) exact estimate x:ZP (z | x, m) P (x | x) Phit (z | x, m) dx,(105)P (x | x) distribution representing probability x elementenvironment U (x), i.e: x U (x). environment U (x) modeled circular regionaround x. Since section consider dynamics environment, onecomponent RBBM Eq. (37) used: Phit (z | x, m). marginalizationenvironment U (x) Eq.(105) introduces dependencies measurements zbmeasurement vector z.environment U (x), explained above, depends sample density aroundsample x consideration. Pfaff et al. (2006) proposed use circular regiondiameter dU (x) , weighted sum Euclidean distance angulardifference. Like Plagemann et al. (2007) Pfaff et al. (2007), approximationlikelihood estimated online sample x simulating L complete rangescans locations drawn U (x) using given map environment. Contrarymultivariate Gaussian approximation proposed Plagemann et al. Pfaff et al.,propose sample-based approximation, able handle multi-modality. Samplingenvironment U (x) immediately results sample-based approximation P (x | x):LP (x | x)1Xx(l) ,L(106)l=1x(l) denotes delta-Dirac mass located x(l) , samples distributedaccording P (x | x):x(l) P (x | x) .(107)Using sample-based approximation P (x | x) likelihood Eq. (105) approximated as:P (z | x, m)L1XPhit z | x(l) , .L(108)l=1Since sample-based approximation calculated online, number sampleslimited. used environment U (x) large, resulting approximation6. simplify notation omitted P (z | , x, m) P (zb | b , x, m), respectively.214fiRigorously Bayesian Beam Model Adaptive Full Scan ModelFigure 16: Panorama taken Sick LMS 200 range finder mounted Kuka 361industrial robot. environment consists rectangular roomobject (a Kuka KR 15/2 robot) middle. show even simplestatic environment, presented sample-based full scan model outperformsGaussian-based state art models.bad. smooth undesired bumpy behavior due limited number samples,measurement noise governing Phit (z | x, m) Eq. (45), artificially increased dependingsize U (x) multiplying factor:q1 + C dU (x) .(109)experiments, C set 20.6.1.1 Experimentsimple environment consisting rectangular room object (a Kuka KR 15/2robot) middle (Fig. 16) used show marginalization (even small)U (x) obtain true likelihood introduces dependencies beamsalso multi-modality. U (x) results local uncertainty x- yposition0.01m rotational uncertainty 5 . obtain reference, Sick LMS 200 rangefinder used take large number measurements (L = 1500) random locationssampled U (x). allow exact positioning, Sick LMS 200 placed Kuka 361industrial robot. Sick LMS 200 range finder connected laptop controlsmotion Kuka 361 industrial robot network using Corba-facilities OpenRobot Control Software, Orocos (Bruyninckx, 2001; Soetens, 2006). simplified mapenvironment (Fig. 16) built simulate 150 complete range scans needed constructfull scan model. marginal P (zb | x, m) two selected beams studieddetail. marginal likelihoods selected beam using proposed sample-based approximation Eq. (108) Gaussian approximation proposed Pfaff et al. (2007),compared Fig. 17(b)-17(c). histogram measurements selected beamfigure clearly shows multi-modality likelihood caused dependencybeams. contrast Gaussian-based state art full scan model, proposed sample-based approximation able handle multi-modality range finder215fiDe Laet, De Schutter & Bruyninckxdata. Fig. 17(d) shows difference beams experimentally obtainedcumulative marginal (L = 1500) Gaussian-based sample-based approximationbeams. mean difference experimental data sample-based approximation 2.8 times smaller difference Gaussian-based approximation,even simple static environment Fig. 16 small U (x).6.2 Sample-based Adaptive Full Scan Model Dynamic Environmentsadaptive beam model proposed Pfaff et al. (2006) suited use dynamicenvironments since uses four component mixture beam model (Thrun et al., 2005;Choset et al., 2005). date however, adaptive full scan likelihood models Pfaff et al.(2007) Plagemann et al. (2007) adapted dynamic environments.assumption beams jointly Gaussian distributed, unable capture nonGaussian uncertainty due environment dynamics, prevents straightforward extensiondynamic environments. contrast, sample-based approximation full scanlikelihood, proposed Section 6.1, extended include environment dynamics.end, replace Phit (z | x, m) Eq. (105) Eq. (108) full mixture Eq. (38).6.2.1 ExperimentFig. 18(a) Fig. 18(b) compare marginals selected beams (Fig. 17(a)) obtainedadaptive full scan model dynamic environments using proposed samplebased approximation Gaussian approximation proposed Pfaff et al. (2007).contrast Gaussian-based state art full scan model, proposed sample-basedapproximation able handle multi-modality range finder data. Fig. 18(c) showsprobability map adaptive full scan model (sample-based approximation) suiteddynamic environments example environment Fig. 16. probability map plotsP (z | x, m) function position map shows marginalizationenvironment U (x) sample Eq. (105) introduces dependencybeams also introduces multi-modality.7. Discussionpaper proposed experimentally validated RBBM, rigorously Bayesian network model range finder adapted dynamic environments. modeling assumptionsrigorously explained, model parameters physical interpretation.approach resulted transparent intuitive model. rigorous modeling revealedunderlying assumptions parameters. way clear physical interpretation parameters obtained providing intuition parameter choices. contrast modelThrun et al. (2005), assumption underlying non-physical discontinuityRBBM discovered. Furthermore, paper proposes different functional formprobability range measurements caused unmodeled objects Poccl (z | x, m) (Eq. (45)),i.e. quadratic rather exponential proposed Thrun et al. Furthermore, comparedwork Thrun et al. (2005), Choset et al. (2005), Pfaff et al. (2006) RBBMdepends fewer parameters, maintaining representational power experimental data. Bayesian modeling revealed rate decay Poccl (z | x, m)216fiRigorously Bayesian Beam Model Adaptive Full Scan Model54.543.53.532.5P (zb | x, m)y[m]32.521.521.511EnvironmentmeasurementsSamples U (x)Example beams0.502.5Experimental data (1500 samples)Sample-based approximationGaussian approximation (Pfaff et al., 2007)421.510.500.511.520.502.501x[m](a) Environment model23456z[m](b) Marginal likelihood left beam37Experimental data (1500 samples)Sample-based approximationGaussian approximation (Pfaff et al., 2007)2.5x 10Sample-based approximationGaussian-based approximation (Pfaff et al., 2007)62Pc (z | x, m)P (zb | x, m)51.514320.5100123456z[m](c) Marginal likelihood right beam0050100150200beam250300350400(d) Difference experimental cumulativemarginal Gaussian sample-based approximationsFigure 17: Experimental results sample-based adaptive full scan model static environments. (a) models simple environment Fig. 16. range finderlocated (0.15m, 0.75m). Samples U (x) (resulting local uncertainty x- yposition 0.01m rotational uncertainty5 ) shown black dots, simulated measurements showngrey. (b) (c) show marginal likelihood P (zb | x, m) two selectedbeams together histogram experimentally recorded range finderdata, Gaussian-based approximation (L = 150) Pfaff et al. (2007),sample-based approximation (L = 150) paper. (d) shows difference beams experimentally obtained cumulative marginal(L = 1500) Gaussian-based sample-based approximation.217fiDe Laet, De Schutter & Bruyninckx0.90.7Large sample approximationSample-based approximationGaussian appr. (Pfaff, 2007)0.8Large sample approximationSample-based approximationGaussian appr. (Pfaff, 2007)0.60.70.5P (z | x, m)P (z | x, m)0.60.50.40.40.30.30.20.20.10.1001234560012z[m]345z[m](a) Marginal likelihood left beam(b) Marginal likelihood right beam(c) Probability map P (z | x, m) sample-basedapproximationFigure 18: Results sample-based adaptive full scan model dynamic environments. (a)(b) show marginal likelihood P (zb | x, m) two selected beamsFig. 17(a) together Gaussian-based approximation (L = 150) Pfaffet al. (2007) sample-based approximation (L = 150) extendeduse dynamic environments. (c) shows probability map resultingsample-based approximation. probability map shows P (z | x, m)function x- position map.2186fiRigorously Bayesian Beam Model Adaptive Full Scan Modelprobability occluded measurement 2 depend one parameter p . State artsensor models however, assume independency two parameters. Finally, maximumlikelihood variational Bayesian estimator (both based expectation-maximization)proposed learn model parameters RBBM. Learning model parametersexperimental data benefits RBBMs reduced number parameters. Usingtwo sets learning experiments different application areas robotics (one reportedThrun et al. (2005)) RBBM shown explain obtained measurementsleast well state art model Thrun et al.Furthermore, paper extended RBBM adaptive full scan model twosteps: first, full scan model static environments next, full scan modelgeneral, dynamic environments. full scan model adapts local sample densityusing particle filter, accounts dependency beams. contrastGaussian-based state art models Plagemann et al. (2007) Pfaff et al.(2007), proposed full scan model uses sample-based approximation, copedynamic environments multi-modality (which shown occur evensimple static environments).Acknowledgmentsauthors thank anonymous reviewers thorough constructive reviews.authors also thank Wilm Decre, Pauwel Goethals, Goele Pipeleers, Ruben Smits,Bert Stallaert, Lieboud Van den Broeck, Marnix Volckaert Hans Wambacq participating experiments. authors gratefully acknowledge financial supportK.U.Leuvens Concerted Research Action GOA/05/10 Research Council K.U.Leuven,CoE EF/05/006 Optimization Engineering (OPTEC). Tinne De Laet Doctoral FellowFund Scientific ResearchFlanders (F.W.O.) Belgium.Appendix A. Simplification Infinite Sumgoal appendix proveXXnnkknu (1 u)(1 p) p ,P (k | n, x, m) P (n) =kn(110)n=ksimplified to:XnP (k | n, x, m) P (n) = 1 p pk ,p = 1(1u)p.Expand Eq. (110) move terms summation that:XXn!nkk k[(1 u) p].P (k | n, x, m) P (n) = (1 p) u p(n k)!k!n(111)(112)n=kNext introduce variables = n k e = (1 u) p:XnX(t + k)!P (k | n, x, m) P (n) = (1 p) u pe .t!k!k kt=0219(113)fiDe Laet, De Schutter & Bruyninckxnext step prove induction that:X1(t + k)!e =.k+1t!k!(1e)t=0(114)First, show equality holds k = 0:Xt!t=0t!e =Xet ,(115)t=0well-known geometric series, so:Xet =t=01,1e(116)showing equality (114) indeed holds k = 0. Next proved that, expressionholds k 1, also holds k. Introduce variable V solution infinity sumk split (114) two parts:XX(t + k)!(t + k)! (t + k 1)! X (t + k 1)!(117)V =e =e ,e +t!k!t!k!t! (k 1)!t! (k 1)!t=0t=0{z}|t=01(1e)kfact used equality (114) holds k 1. Simplify first termsummation to:X(t + k)! (t + k 1)! X (t + k 1)!te .(118)e =t!k!t! (k 1)!t!k!t=0t=0term summation = 0 equal zero. Hence, introduce variable = t1simplify:XX+1(t + k)!(t + k)! (t + k 1)!+1 e(119)e =t!k!t! (k 1)!(t + 1)!k!t=0=0X(t + k)!= ee,(120)!k!|t =0{z}Vseries V looking recognized. Substitute resultEq. (117) that:V = eV +1(1 e)k.(121)Solving equation V gives:V =1(1 e)k+1220,(122)fiRigorously Bayesian Beam Model Adaptive Full Scan Modelproving equality (114) holds k assumed hold k 1, closing proofinduction.substitute Eq. (114) Eq. (113):XP (k | n, x, m) P (n) =n(1 p) uk pk[1 (1 u) p]k+1,(123)rewrite as:Xnp =1(1u)p ,P (k | n, x, m) P (n) = 1 p pk ,(124)exactly proved.ReferencesBeal, M. J. (2003). Variational algorithms approximate Bayesian inference. Ph.D. thesis,University College London.Beal, M. J., & Ghahramani, Z. (2003). variational bayesian em algorithm incomplete data: application scoring graphical model structures. ValenciaInternational Meeting Bayesian Statistics, Tenerife, Canary Islands, Spain.Bishop, C. M. (2006). Pattern Recognition Machine Learning. Springer.Bruyninckx, H. (2001). Open RObot COntrol Software. http://www.orocos.org/.Burgard, W., Fox, D., Hennig, D., & Schmidt, T. (1996). Estimating absolute positionmobile robot using position probability grids. Proc. National ConferenceArtificial Intelligence.Choset, H., Lynch, K. M., Hutchinson, S., Kantor, G. A., Burgard, W., Kavraki, L. E., &Thrun, S. (2005). Principles Robot Motion: Theory, Algorithms, Implementations. MIT Press.Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood incompletedata via EM algorithm (with discussion). Journal Royal Statistical Society(Series B), 39, 138.Fox, D. (2003). Adapting Sample Size Particle Filters KLD-Sampling.International Journal Robotics Research, 22 (12), 9851003.Fox, D., Burgard, W., & Thrun, S. (1999). Markov localization mobile robots dynamicenvironments. Journal Artificial Intelligence Research, 11, 391427.Hahnel, D., Schulz, D., & Burgard, W. (2003a). Mobile robot mapping populated environments sensor planning. Journal Advanced Robotics, 17 (7), 579597.Hahnel, D., Triebel, R., Burgard, W., & Thrun, S. (2003b). Map building mobile robotsdynamic environments. Proceedings 2003 IEEE International ConferenceRobotics Automation, pp. 15571569, Taipeh, Taiwan. ICRA2003.Jeffreys, W., & Berger, J. (1992). Ockhams razor bayesian analysis. American Scientist,80, 6472.221fiDe Laet, De Schutter & BruyninckxJensen, F. V., & Nielsen, T. D. (2007). Bayesian Networks Decision Graphs. Springer.Lenser, S., & Veloso, M. (2000). Sensor resetting localization poorly modelled mobilerobots. Proceedings 2000 IEEE International Conference RoboticsAutomation, San Francisco, CA. ICRA2000.McLachlan, G. J., & Krishnan, T. (1997). EM algorithm extensions. John Wiley& Sons, New York, NY.Moravec, H. P. (1988). Sensor fusion certainty grids mobile robots. AI Magazine, 9,6174.Neapolitan, R. E. (2004). Learning Bayesian Networks. Pearson Prentice Hall, New York,NY.Pfaff, P., Burgard, W., & Fox, D. (2006). Robust Monte-Carlo localization using adaptivelikelihood models. Christensen, H. (Ed.), European Robotics Symposium, Vol. 22,pp. 181194, Palermo, Italy. Springer-Verlag Berlin Heidelberg, Germany.Pfaff, P., Plagemann, C., & Burgard, W. (2007). Improved likelihood models probabilisticlocalization based range scans. Proceedings 2007 IEEE/RSJ InternationalConference Intelligent Robots Systems, San Diego, California. IROS2007.Plagemann, C., Kersting, K., Pfaff, P., & Burgard, W. (2007). Gaussian beam processes:nonparametric Bayesian measurement model range finders. Robotics: ScienceSystems (RSS), Atlanta, Georgia, USA.Rasmussen, C., & Ghahramani, Z. (2000). Occams razor. Advances Neural Information Processing 13, Denver, Colorado. MIT Press.Soetens, P. (2006). Software Framework Real-Time Distributed Robot Machine Control. Ph.D. thesis, Department Mechanical Engineering, Katholieke Universiteit Leuven, Belgium. http://www.mech.kuleuven.be/dept/resources/docs/soetens.pdf.Thrun, S., Burgard, W., & Fox, D. (2005). Probabilistic Robotics. MIT Press.Thrun, S. (2001). probabilistic online mapping algorithm teams mobile robots.International Journal Robotics Research, 20 (5), 335363.Thrun, S., Fox, D., Burgard, W., & Dellaert, F. (2001). Robust monte carlo localizationmobile robots. Artificial Intelligence, 128, 99141.Wang, C.-C., Thorpe, C., & Thrun, S. (2003). Online simultaneous localization mapping detection tracking moving objects: Theory results groundvehicle crowded urban areas. Proceedings 2003 IEEE International Conference Robotics Automation, Taipeh, Taiwan. ICRA2003.Wolf, D. F., & Sukhatme, G. S. (2004). Mobile robot simultaneous localization mappingdynamic environments. Proceedings 2004 IEEE International ConferenceRobotics Automation, pp. 13011307, New Orleans, U.S.A. ICRA2004.222fiJournal Artificial Intelligence Research 33 (2008) 433-464Submitted 07/08; published 11/08Ordinal Bargaining Solution Fixed-Point PropertyDongmo ZhangYan Zhangdongmo@scm.uws.edu.auyan@scm.uws.edu.auIntelligent Systems LaboratorySchool Computing MathematicsUniversity Western Sydney, AustraliaAbstractShapleys impossibility result indicates two-person bargaining problemnon-trivial ordinal solution traditional game-theoretic bargaining model. Althoughresult longer true bargaining problems two agents, nonewell known bargaining solutions ordinal. Searching meaningful ordinal solutions,especially bilateral bargaining problem, challenging issue bargainingtheory three decades. paper proposes logic-based ordinal solutionbilateral bargaining problem. argue bargaining problem modeledterms logical relation players physical negotiation items, meaningful bargainingsolution constructed based ordinal structure bargainers preferences.represent bargainers demands propositional logic bargainers preferencesdemands total preorder.show solution satisfies desirable logicalproperties, individual rationality (logical version), consistency, collective rationalitywell typical game-theoretic properties, weak Pareto optimalitycontraction invariance. addition, players demand sets logically closed,solution satisfies fixed-point condition, says outcome negotiationresult mutual belief revision. Finally, define various decision problems relationbargaining model study computational complexity.1. IntroductionBargaining central research topic economics five decadesbecome interesting issue computer science recent years (Osborne & Rubinstein,1990; Rosenschein & Zlotkin, 1994; Muthoo, 1999). ground-breaking paper, Nash(1950) models bargaining problem pair (S, d), <2 subset twodimensional Euclidean space (feasible set), representing set utility pairsderived bargainers preferences feasible outcomes, point designateddisagreement point. bargaining solution function assignsbargaining problem (S, d) point (Thomson, 1994).Nash bargaining model presumes bargainers preferences representedvon Neumann-Morgenstern utility, referred cardinal utility (Myerson, 1991).assumption, two utility functions viewed one derivedaffine positive transformation. Thus bargaining solution basedNashs bargaining model invariant affine positive transformations.However, traditional economic theory considers bargaining problems players preferences represented ordinal (Calvo & Peters, 2005). Therefore, ideally, bargainingc2008AI Access Foundation. rights reserved.fiZhang & Zhangsolution invariant order-preserving transformations utilities.property referred ordinal invariance game-theoretic literature (Thomson,1994). bargaining solution possesses property called ordinal solution.Obviously, ordinal bargaining solutions desirable cardinal solutionsordinal information players preferences easier elicit cardinal preferencescorresponding solutions robust (Sakovics, 2004; Calvo & Peters, 2005).However, none well known bargaining solutions (Nash, 1950; Kalai & Smorodinsky,1975; Kalai, 1977; Perles & Maschler, 1981) ordinal. fact, Shapley (1969) showedtwo-person bargaining problem (bilateral bargaining) non-trivial(i.e., strongly individual rational) ordinal solution1 . result generally referredShapleys impossibility result game-theoretic literature.Shapleys negative result obviously discouraged investigation ordinal bargaining,notwithstanding Shapley demonstrated ten years later ordinal solutions existthree-person bargaining problem (Shubik, 1982). study ordinal bargainingtheory regain focus game theory recently. Kibris (2001) providedaxiomatic characterization ordinal solution three-person bargaining problembased Nashs bargaining model2 . Safra Samet (2004) extended resultbargaining problems three players. Rubinstein et al. (1992) ONeill etal. (2004) investigated ordinal bargaining problem varying Nashs bargaining model.Calvo Perers (2005) explored problem ordinal bargaining least oneplayer cardinal. Nevertheless, problem ordinal bargaining still consideredunsolved problem. pierces work focus existence ordinal solutions.None proposed solutions gains strong intuitive support. Looking meaningfulordinal bargaining solutions still outstanding problem game theory (Sakovics, 2004).show difficulty ordinal bargaining, let us consider simple typical bargainingscenario:Example 1 (Muthoo, 1999) Two players, B, bargain partition cake.Let xi share cake percentage player (i = A, B). set possibleagreements represented = {(xA , xB ) : 0 xA 100 xB = 100 xA }.xi [0, 100], ui (xi ) player utility obtaining share xi cake.Assume player linear utility scale share, uA (xA ) = xA , playerB utility scale proportional square share, uB (xB ) = x2B . Failure agree rated 0 B. Consider two influential bargaining solutions: Nashs solution (Nash, 1950) Kalai-Smorodinskys solution (Kalai & Smorodinsky, 1975). easy calculate Nashs bargaining solution problem givesoutcome (33.3, 66.7) Kalai-Smorodinskys solution gives (38.2, 61.8). solutionsfavor player B. player B less risk-averse (has concave utility)player (has linear utility). Nashs solution Kalai-Smorodinskys solution,risk-loving players advantage bargaining comparing risk-neutral risk-averseplayers (see Roths book, 1979a, p.35-60). consider order-preserving transformation(x) = x player Bs utility. transformed utility player B becomes linear.1. See work Thomson (1994) easy proof.2. original work formally published. Kibris (2004) gave brief note.434fiAn Ordinal Bargaining Solution Fixed-Point Propertynew utility scales, Nashs solution Kalai-Smorodinskys solution give (50, 50)outcome. means none solutions ordinally invariant.example clearly shows non-linearity utility functions, expressesrisk posture player, determines outcomes bargaining collapses ordinal transformations. words, ordinal transformations filter useful informationexpressible cardinal utility expressible ordinal utility. explainsresolution two-player bargaining problem made basis ordinalutility alone .... satisfactory theory bilateral bargaining requires knowledge something ordering bargainers preferences (see Shubiks book, 1982,p.94-98).all, ordinal preference insufficient fully specify bargaining situation.bargaining model must supply way express information additional ordinal preferences, bargainers attitude towards risk. article aims demonstrateordinal solution bilateral bargaining problem language logic usedexpress knowledge required modeling bargaining ordinal preferences.recent years, studies logic-based frameworks bargaining negotiationreceived considerable attention field artificial intelligence (AI) (Sycara, 1990;Kraus, Sycara, & Evenchik, 1998; Parsons, Sierra, & Jennings, 1998; Zhang, Foo, Meyer,& Kwok, 2004; Meyer, Foo, Kwok, & Zhang, 2004; Zhang, 2005, 2007). frameworksutilize logical languages represent bargaining situations physical negotiation items,bargaining conflicts, players beliefs mutual threats explicitly expressed,differentiates traditional game-theoretic models. paperpropose ordinal solution bilateral bargaining problem based logical modelintroduced Zhang Zhang (2006a).organization paper following. Section 2 presents formal modelbargaining. use finite propositional language describe bargainers demands.bargainers preferences demands sorted total preorder. bargainingproblem defined pair hierarchies two parties demand sets. constructionbargaining solution, presented Section 3, based idea party triesmaximize prior demands included final agreement keepingoutcome consistent. approach viewed extension Nebels prioritizedbase revision two-agent setting (Nebel, 1992). Section 4 Section 5 devoteddiscussions properties proposed solution. shall prove Section4 solution satisfies desirable logical properties, logical versionIndividual Rationality, Consistency Collective Rationality. extraordinarily,shall show solution satisfies desirable fixed-point condition introducedZhang et al. (2004), says outcome bargaining result mutualbelief revision. Section 5 focuses discussion game-theoretic propertiessolution. prove solution satisfies Weak Pareto Optimality, Restricted SymmetryContraction Invariance. Section 6 devoted discussion bargainers attitudestowards risk represented model determine players bargaining power.Section 7 investigates complexity issues related proposed model. consider fourmajor decision problems relation bargaining game provide computational435fiZhang & Zhangcomplexity results. final two sections conclude work discussion relatedwork.paper made self-contained. However, reader find basic knowledge belief revision game-theoretic bargaining theory helpful betterunderstanding concepts introduced paper. introductory surveyareas, see Gardenforss article (1992) Thomsons article (1994), respectively.2. Representation Bargaining Problemsseen Example 1, ordinal solution bilateral bargaining problemrequires information addition ordinal preferences. order express extra information, model bargaining situation two aspects: physical bargaining terms,described propositional logic, ordering bargainers preferencesbargaining terms, described total preorder. Since bilateral bargainingchallenging problem bargaining theory, shall restrict bargaining problem two players.2.1 Preliminariesassume party set negotiation items, referred demand set,described finite propositional language L. language consists finite setpropositional variables propositional connectives , ,, standardsyntax semantics. logical closure operator Cn defined Cn(X) = { : X ` },X set sentences. say X logically closed belief set X = Cn(X).X two sets sentences, X + denotes Cn(X ).Suppose X1 X2 two sets sentences. simplify exploration, use Xirepresent set among X1 X2 Xi one them. vector twocomponents, D1 D2 represent component D.2.2 Bargaining Gamesshown previous section, cardinal utility encodes two types information: bargainers attitude towards risk (via non-linearity utility functions) bargainers preferencespossible outcomes (via ordering utility values). One may ask whether theorybargaining based purely ordinal information preferences. order investigate possibility, Osborne Rubinstein introduced different way representbargaining situation, preference information separated physicalbargaining terms (Osborne & Rubinstein, 1990). Precisely, define bargaining problem four-tuple (X, D, 1 , 2 ), X set feasible outcomes (in physical terms),disagreement event, complete transitive reflexive ordering setX D, representing bargainer preferences. shown Example 1, however, simplydescribing physical bargaining terms without specifying relations sufficelead ordinal solution (see also Osborne & Rubinsteins book, 1990, p.32).paper, extend Osborne & Rubinsteins model way physicalnegotiation items represented logical formulae.436fiAn Ordinal Bargaining Solution Fixed-Point PropertyDefinition 1 bargaining game pair ((X1 , 1 ), (X2 , 2 )), Xi (i = 1, 2)logically consistent set sentences L complete transitive reflexive order(total preorder weak order) Xi satisfies following logical constraint3 :(LC) 1 , , n ` , k (1 k n) k .call pair (Xi , ) prioritized demand set player i. , Xi ,denotes 6i . denotes .Intuitively, bargaining game formal representation bargaining situationplayer describes demands logical formulae expresses preferencesdemands total preorder. assume player consistent demands.preference ordering player reflects degree entrenchment playerdefends demands. logical constraint LC says demand logicalconsequence demands 1 , , n , less entrenchedfail defend , least one dropped (otherwisewould lost ). easy see ordering similar GadenforsMakinsons epistemic entrenchment (Gardenfors & Makinson, 1988). fact, logicalconstraint LC, introduced Zhang Foo (2001), actually combinationpostulates EE2 EE3 epistemic entrenchment ordering.Observation 1 Let total preorder X. logical constraint LC equivalentconjunction following conditions:1. ` , .2. Either .Proof: easy verify LC implies two conditions. proveconditions imply LC induction n. Obviously, LC holds n = 1. Assume LCholds n = l. Suppose 1 , , l1 , l , l+1 ` , 1 , , l1 , (l l+1 ) ` .inductive assumption, either l l+1 k (1 k l 1)k . l l+1 , condition 2 transitivity , either ll+1 . Therefore case k (1 k l + 1) k .remark preference ordering represents firmly agent entrenchesdemands rather gain payoff4 . instance, suppose p1 representsdemand seller price good less $10 p2 denotes pricegood less $8. Obviously seller could get higher payoff p1p2 . However, since p1 implies p2 , entrench p2 less firmly p1 , i.e., p2 p1 ,3. complete transitive reflexive order, i.e., total preorder weak order, satisfies following properties:Completeness totality: .Reflexivity: .Transitivity: .4. define bargainers gains Section 5.1.437fiZhang & Zhangbecause, fails keep p1 , still bargain p2 loss p2 means lossboth.another significant difference model bargaining gametheoretic model. game-theoretic model abstracts bargaining situation numericalgame. demands, preferences risk posture player represented utilityvalues. model, however, factors abstracted logical statementsordering statement, i.e., prioritized demand set. Note endowedword demand broad meaning. model, demand playereverything related negotiation player wants keep finalagreement. physical item player wants obtain party.also piece knowledge, belief, goal, constraint even thread, whatever,player wants keep agreement. Different logics rational agency,BDI logics, distinguish knowledge, belief, goal real demands.instance, logical tautology demand player player considersincluded final agreement. Consider Example 1 again. Typical demandsplayer xA 40, xA 50, xA 60, on5 , mean playerwants get share cake less 40%, 50%, 60%, . Player Bs demandsopposite, say xB 40, xB 50, xB 60, . Besides real demands,set domain constraints players demand sets, xA + xB 100,xA xA z xB xB z whenever z. constraints linkdemands players together, therefore, play important rule determinationbargaining outcomes. instance, cannot xA 55 xB 55final agreement inconsistent constraints. One may wonderplayer include constraints demand set. case,player would accept unreasonable results, xA 55 xB 55.words, idealize bargaining problem assuming two individuals highlyrational (see work Nash, 1950, p.155), assume constraintsbackground knowledge included demand set player (see discussionsexample Section 6). Again, ordering demand sets reflect gainsplayer mentioned above. likely rational agent could givehighest priority mentioned constraints background knowledge sincemight never going give fundamental rules.following example shows model suitable discrete domainproblems.Example 2 couple making family budget next year. husband wantschange car new fancy model domestic holiday. wife goingimplement dream romantic trip Europe suggests redecorate kitchen.know cant two holidays one year. also realizecannot afford new car overseas holiday year without gettingloan bank. However, wife like idea borrowing money.order represent situation logic, let c denote buy new car, standdomestic holiday, overseas holiday, k kitchen redecoration l loan.5. Note player may different demands different stages negotiation.438fiAn Ordinal Bargaining Solution Fixed-Point Property(d o) means impossible domestic holiday overseasholiday. statement (c o) l says want buy new car alsooverseas holiday, get loan bank.symbolization, express husbands demands followingset:X1 = {c, d, (d o), (c o) l}Similarly, wifes demands represented by:X2 = {o, k, (d o), (c o) l, l}Let us assume husbands preferences demands are:(d o) 1 (c o) l 1 c 1wifes preferences are:(d o) 2 (c o) l 2 2 k 2 lNote agents give common beliefs: (d o) (c o) l highestpriority. reflects couple rational. shall see common beliefsplay important role determination bargaining solution.example, also see differences model Osborne& Rubinsteins model (Osborne & Rubinstein, 1990). First, demand sets X1 X2model represent players physical demands rather feasible outcomes (notetwo players share feasible set X Osborne & Rubinsteins model).model, feasible set generated demand sets procedure conflictresolving (see Definition 2 Example 3). Secondly, representationdisagreements. negotiation ends disagreement, simply assume agreement empty. Thirdly, Osborne & Rubinsteins model, items X independent,model, items players demand sets relatedlogical relations orderings (via LC). allows us represent bargainers attitudetowards risk.2.3 Hierarchy Demandsconstruct bargaining solution, present basic properties prioritizeddemand sets. Consider prioritized demand set (X, ) single agent. definerecursively hierarchy, {X j }+j=1 , X respect ordering follows:1. X 1 = { X : X( )}; 1 = X\X 1 .2. X j+1 = { j : j ( )}; j+1 = j \X j+1 .intuition behind construction following: stage construction,collects maximal elements current demand set removeset next stage construction. easy see exists number439fiZhang & Zhangn X =nX j due logical constraint LC6 . Therefore demand set Xj=1always written X 1 X n , X j X k = j 6= k (see Figure 1). AlsoentrenchedX1EAAEEEEX2...Xnleast entrenchedFigure 1: hierarchy demand set.X j X k , j < k. words, prioritizeddemand set (X, ) uniquely determines partition X,nX j , total orderj=1partition. Therefore, concept prioritized demand set equivalent conceptso-called nicely-ordered partition belief set7 , introduced Zhang Foo (Zhang& Foo, 2001), demand set logically closed.sequent, write X k denotekXj.j=12.4 Prioritized Base Revisionhierarchy demand set agent, able define beliefrevision operator agent following Nebels prioritized base revision (Nebel, 1992)8 .define revision function follows:demand set (X, ) set F sentences,defX F =\(H + F ),HXFX F defined as: H X F1. H X,2. k (k = 1, 2, ), H X k maximal subset X kk(H X j ) Fj=1consistent.words, H maximal subset X consistent F gives priorityhigher ranked items. following result used Section 4.Lemma 1 (Nebel, 1992) X logically closed, satisfies AGM postulates.6. Note X infinite set even though language finite.7. nicely-ordered partition belief set K triple (K, , ), partition Ktotal order satisfies logical constraint LC. See work Zhang Foo (2001) p.540.8. idea construction traced back Poole Brewkas approach default logic (Brewka,1989; Poole, 1988).440fiAn Ordinal Bargaining Solution Fixed-Point Property3. Bargaining SolutionNashs bargaining model, bargaining solution defined function assignsbargaining game point feasible set game. However,simple definition model set possible agreements (feasible set)given bargaining game. generate feasible set demand sets.involves process conflict resolving.3.1 Possible DealsWhenever demands two agents conflict, least one agent make concessionorder reach agreement. simple way making concession withdrawnumber demands. sense, possible agreement pair subsets two playersoriginal demand sets collection remaining demands consistent. Obviouslyplayer would like keep many original demands possible. addition, playergive demand, player typically gives one lowest priority.idea leads following definition possible deals.Definition 2 Let G = ((X1 , 1 ), (X2 , 2 )) bargaining game. deal G pair(D1 , D2 ) satisfying following conditions: = 1, 2,1. Di Xi ;2. X1 X2 Di ;3. k (k = 1, 2, ), Di Xik maximal subset Xikk(Di Xij )j=1Di consistent.{Xij }+j=1 hierarchy Xi defined Section 2.3. set deals Gdenoted (G), called feasible set game.definition obviously analogue Nebels notion (see Section 2.4).difference procedure maximization interactive two agents:given one players demands, player always tops demands highestprioritized items provided overall outcome consistent.Since assumed X1 X2 consistent, (G) non-empty. Specifically, X1 X2 consistent, (G) = {(X1 , X2 )}.Example 3 Consider bargaining game Example 2. According preference orderings couple, game three possible deals:D1 = ({(d o), (c o) l, c, d}, {(d o), (c o) l, k, l}).D2 = ({(d o), (c o) l, c}, {(d o), (c o) l, o, k}).D3 = ({(d o), (c o) l}, {(d o), (c o) l, o, k, l}).Therefore (G) = {D1 , D2 , D3 }.441fiZhang & Zhang3.2 Core Agreementshown generate possible deals, form set possible agreements,i.e., feasible set, resolving conflicting demands. levelgame-theoretic model that, define bargaining solution, select dealpossible deals. Obviously, demands two parties contradict,multiple possible deals. Different deals would favor different parties. instance,Figure 2, deal D0 favor player 1 D00 favor player 2. Therefore,conflicts choosing outcomes still exist. major concern bargaining theorymeasure balance gain negotiating party.1D0X11X21......X1k1X2k1X1kX2kX1k+1X2k+1......PPD00Figure 2: Different deals favor different players.Instead counting number demands party remain deal,consider top block demands player keeps deal (the top levels demandsplayers demand hierarchy) ignore demands includedtop blocks purpose measuring players gains.Given deal D, shall use maximal number top levels demandsdeal indicator players gain deal, i.e., max{k : Xik Di }= 1, 2. instance, Figure 2, player 1 remains maximally top k 1 levelsdemands deal D00 player 2 successfully gain top k + 1 levelsdemands deal. deal D0 , players remain top k levelsdemands.order compare different deals, refer gain index deal gainplayer whoever receives less deal, i.e., min{max{k : X1k D1 }, max{k : X2kD2 }}, equivalently, max{k : X1k D1 X2k D2 }. instance, Figure 2,gain index D0 k gain index D00 k 1. Therefore say D0better D00 because, D0 , players remind least top k blocksdemand hierarchies D00 cant.Formally, letGmax=max{k : X1k D1 X2k D2 }(D1 ,D2 )(G)Gmax(G) = {(D1 , D2 ) (G) : X1442GmaxD1 & X2D2 }(1)(2)fiAn Ordinal Bargaining Solution Fixed-Point Property(G) collects best deals possible deals G. noneGdeals contain top maxlevels demands players.GNote max may infinite X1 X2 consistent. LetGmax(1 , 2 ) = (X1Gmax, X2)(3)call = (1 , 2 ) core game. Therefore core contains top blockdemands best deals contain, therefore included final agreement.following lemma gives another way calculate core game.GLemma 2 max= max{k : X1k X2k (X1 X2 ) consistent}.Proof: Let = max{k : X1k X2k (X1 X2 ) consistent}. easy showGGX1 max X2 max (X1 X2 ) consistent (G) non-empty. ThereforeGmax. hand, since X1 X2 (X1 X2 ) consistent, existsdeal (D1 , D2 ) (G) Xi Di X1 X2 Di = 1, 2. ThusG . conclude = G .maxmax3.3 Solutionfirst sight, bargaining solution easily defined function assignsbargaining game G possible deal (G). likely, solution could select onebest deals (G). However, due multiplicity (G) (see Figure 3),selection always feasible want solution symmetric player.show difficulty, let us consider following example.Example 4 Consider bargaining game G = ((X1 , 1 ), (X2 , 2 )), described three propositional variables p, q r, X1 = {{p}, {r}} X2 = {{q}, {r}} (notedemand sets represented form hierarchy p 1 r q 2 r (see Section2.3)). easy know game two possible deals, ({p, r}, {q}) ({p}, {q, r}),(G). However, none deals lead impartial solution. reasonable solution problem ({p}, {q}), take intersectionbest deals player, respectively.Base intuitive explanation, ready present bargainingsolution.Definition 3 bargaining solution function F defined follows, mapsbargaining game G = ((X1 , 1 ), (X2 , 2 )) pair sets sentences:defF (G) = (\D1 ,(D1 ,D2 )(G)\D2 )(4)(D1 ,D2 )(G)(G) defined Equation (2).better understanding construction solution, would like makefollowing remarks:443fiZhang & ZhangX11X21......GGX1 maxGX1 max1...0X2 maxG+1+1PPD00X2 maxiP 000... PFigure 3: Multiplicity best deals.1. solution gives prediction bargaining outcome game. Given gameG, Fi (G) represents demands player successfully remain endbargaining. also means demands accepted player.Therefore final agreement bargaining defined F1 (G) F2 (G)9 .2. Note Fi (G) Xi = 1, 2. Therefore solution means compromiseplayer. players may make concessions demands orderreach agreement. Obviously, conflict players originaldemands, i.e., X1 X2 consistent, concession needed, is, Fi (G) = Xi(i = 1, 2).3. construction solution takes skeptical view sense that,player, demand item included solution belongs bestdeals. words, solution gives cautious prediction bargainingoutcome. result, solution necessarily deal multipleelements (G).4. solution unique bargaining game. Like bargaining problemnon-convex domain, scarify strict Pareto optimality gain uniqueness(this reason take cautious prediction). However, showsolution weakly Pareto optimal (see Section 5.2 discussions).5. bargaining model, specify disagreement points. fact, assumesolution gives empty agreement, i.e., F (G) = (, ), negotiationreaches disagreement. words, (, ) default disagreement pointbargaining game.Example 5 Continue Example 3. According hierarchies demand sets shownExample 2, core game is:({(d o), (c o) l, c}, {(d o), (c o) l, o})9. Alternatively, define final agreement Cn(F1 (G) F2 (G)) consider outcomenegotiation contains logical consequences demands agreement. addition,relation items agreement read rather or. words,items agreement accepted players.444fiAn Ordinal Bargaining Solution Fixed-Point PropertyTherefore (G) contains single deal, D2 (see Example 3). solutionF (G) = D2 = ({(d o), (c o) l, c}, {(d o), (c o) l, o, k})words, couple agree upon commonsense one holidayget loan want buy new car go overseas holiday.husband accepts wifes suggestion holiday Europe wife agrees buyingnew car.consider following preference orderings:(d o) 1 (c o) l 1 1 c(d o) 2 (c o) l 2 2 k 2 lTherefore demand hierarchies become:X1 = {{(d o), (c o) l}, {d}, {c}}.X2 = {{(d o), (c o) l}, {o}, {k}, {l}}.Let G0 denote game. deals original hierarchy shownExample 3. However solution game becomes ({(d o), (c o) l}, {(dG0 = 2 three deals included (G0 ). Noteo), (c o) l, k}) maxfinal agreement include demands lead conflicting (d, o, l) keepsdemands lead conflicting (k). words, solution excludesdemands lead conflict keeps demands involved conflictseven though low priorities.4. Logical Properties Bargaining Solutionfollowing two sections, discuss properties bargaining solution introduced previous section. According Zhang (2007), bargaining solution satisfiesaxioms Collective Rationality, Scale Invariance, Symmetry Mutually ComparableMonotonicity well basic assumptions Individual Rationality, Consistency Comprehensiveness logical version Kalai-Smorodinsky solution (Kalai &Smorodinsky, 1975). Among properties, Collective Rationality, Individual RationalityConsistency capture logical properties bargaining solution. Scale Invariance,Symmetry Mutually Comparable Monotonicity reflect game-theoretic propertiesbargaining solution. Comprehensiveness idealized assumption logic-based bargaining solution. Although significant difference bargaining solutiondefined paper one work, two solutions share desirableproperties bargaining solutions.4.1 Generic Properties Logic-Based Bargaining Solutionseasy see solution constructed previous section satisfies followinggeneric properties logic-based bargaining solution.Theorem 1 bargaining game G = ((X1 , 1 ), (X2 , 2 )), let F (G) = (F1 (G), F2 (G)).445fiZhang & Zhang1. F1 (G) X1 F2 (G) X2 .(Individual Rationality)2. F1 (G) F2 (G) consistent.(Consistency)3. X1 X2 consistent, Fi (G) = Xi i.(Collective Rationality)Proof: proofs properties straightforward definition bargaining solution (Definition 3).Note logical version Individual Rationality (IR) different meaninggame-theoretic version. logical version IR means player concernsdemands, i.e., whether many demands includedfinal agreement. contrast, game-theoretic version IR concerns whetherplayer gain less disagreement point negotiation (see detailsSection 5.1). two properties quite intuitive.following example shows solution satisfy comprehensiveness,requires Fi (G) implies Fi (G) (see work Zhang,2007).Example 6 Consider bargaining situation player 1s demand set X1 = {p, q}player 2s demand set X2 = {p, r}, p, r, q propositional variables. Assumeplayer ranks demands level (i.e., demand sets singletonpartition). Based assumption, easy know solution game({q}, {r}). Therefore, solution comprehensive (for instance, q p q F1 (G)p 6 F1 (G)).Since solution satisfy comprehensiveness, according Zhang (2007),logical version Kalai-Smorodinsky solution. However, meansolution less intuitive. Although comprehensiveness common restriction beliefrevision game theory, means desirable property bargaining solution.example, q r involved conflict underlying bargaininggame. Thus reasonable players keep irrelevant demands. addition,solution syntax-dependent. represent demand set X1 = {p q}X2 = {p r}, solution (, ).4.2 Fixed-Point PropertyBesides generic properties, solution possesses another extraordinary logical property:fixed-point property. consider bargaining negotiation mutual persuasion: onepersuades accept demands. outcome negotiation resultmutual belief revision (Zhang et al., 2004). case, negotiation outcomesatisfy following fixed-point property.Theorem 2 bargaining game G = ((X1 , 1 ), (X2 , 2 )), X1 X2 logicallyclosed, bargaining solution F (G) satisfies following fixed-point condition:F1 (G) + F2 (G) = (X1 1 F2 (G)) (X2 2 F1 (G))(5)prioritized revision operator player (see definition Section 2.4).446fiAn Ordinal Bargaining Solution Fixed-Point PropertyAssume X1 X2 two belief sets (so logically closed), representing beliefstates two agents. Mutual belief revision agents means agent takespart agents beliefs revise belief set. instance, 1 subset X12 subset X2 , X1 1 2 revised belief set player 1 acceptsplayer 2s beliefs 2 X2 2 1 resulting belief set player 2 accepting1 . interaction belief revision continue reaches fixed pointbeliefs common, i.e., (X1 1 2 ) (X2 2 1 ), exactly beliefs agentsmutually accept, is, 1 + 2 . Note agent uses way revisionrebuilt belief state. view bargaining mutual belief revision, agreementbargaining, i.e., F1 (G) + F2 (G), exactly common demands agents acceptother, i.e., (X1 1 F2 (G)) (X2 2 F1 (G)). words, solution F (G)fixed-point respect game G. theorem shows truedemand sets game logically closed.show theorem, need technical lemmas.Lemma 3 bargaining game G = ((X1 , 1 ), (X2 , 2 )),1. F1 (G) X1 1 F2 (G);2. F2 (G) X2 2 F1 (G).Proof: According definition prioritized base revision, X1 1 F2 (G) =Cn(H F2 (G)). H X1 F2 (G), deal (D1 , D2 ) (G)HX1 F2 (G)D1 = H. extend pair (H, F2 (G)) deal (H, D2 )F2 (G) D2 . hand, since 1 F2 (G) consistent, 1 H,(1 , 2 ) core G. Thus, 1 D1 2 D2 . follows (D1 , D2 ) (G).Since F1 (G) D1 , F1 (G) H. conclude F1 (G) X1 1 F2 (G).proof second statement similar.lemma have,1. F1 (G) + F2 (G) X1 1 F2 (G);2. F1 (G) + F2 (G) X2 2 F1 (G).Note lemma require demand sets X1 X2 logicallyclosed. However, without assumption, following lemmas hold.Lemma 4 Let (1 , 2 ) core game G = ((X1 , 1 ), (X2 , 2 )). X1 X2logically closed,1. X1 1 F2 (G) = X1 1 (2 + (X1 X2 ));2. X2 2 F1 (G) = X2 2 (1 + (X1 X2 ))Proof: present proof first statement. second one similar. Firstly,prove F2 (G) 1 + 2 + (X1 X2 ). X1 X2 consistent, result obviouslytrue. Therefore assume X1 X2 inconsistent.447fiZhang & ZhangAssume F2 (G). 6 1 +2 +(X1 X2 ), {}1 2 (X1 X2 )GG+1+1consistent. According Lemma 2, X1 max X2 max (X1 X2 ) inconsistent.Since language finite X1 X2 logically closed, sets X1 X2 ,G +1G +1X1 maxX2 maxlogically closed (the latter two due LC). Thereforeset finite axiomatization. Let sentence 0 axiomatize X1 X2 , 1 axiomatizeG +1G +1X1 max 2 axiomatize X2 max . Thus 0 1 2 inconsistent. Notice0 1 X1 0 2 X2 . follows (0 1 ) X1 (0 2 ) X2 .Since {} 1 2 (X1 X2 ) consistent, deal (D1 , D2 ) (G){(0 1 )}1 (X1 X2 ) D1 {(0 2 )}2 (X1 X2 ) D2 . knowF2 (G), D1 + D2 . Thus 0 1 2 D1 + D2 , contradicts factD1 + D2 consistent. Therefore, shown F2 (G) 1 + 2 + (X1 X2 ).prove X1 1 F2 (G) = X1 1 (2 + (X1 X2 )). Lemma 3,1 + 2 X1 1 F2 (G). follows X1 1 F2 (G) = (X1 1 F2 (G)) + (1 + 2 ).Furthermore, yield X1 1 F2 (G) = (X1 1 F2 (G)) + (1 + 2 ) + (X1 X2 )X1 X2 F2 (G). Since F2 (G) 1 + 2 + (X1 X2 ). According AGM postulates, (X1 1 F2 (G)) + (1 + 2 + (X1 X2 )) = X1 1 (1 + 2 + (X1 X2 )).Therefore X1 2 F2 (G) = X1 1 (1 + 2 + (X1 X2 )). addition, easyprove 1 X1 1 (2 + (X1 X2 )). AGM postulates again,X1 1 (2 + (X1 X2 )) = (X1 1 (2 + (X1 X2 )) + 1 = X1 1 (1 + 2 + (X1 X2 )).Therefore X1 1 F2 (G) = X1 1 (2 + (X1 X2 )).following lemma complete proof Theorem 2.Lemma 5 X1 X2 logically closed,(X1 1 F2 (G)) (X2 2 F1 (G)) F1 (G) + F2 (G).Proof: Let (1 , 2 ) core G. Let1max01 = X12max02 = X221= max{k : 1 X2kmax= max{k : X1k 2 (X1 X2 ) consistent} max(X1 X2 ) consistent}.Note cases maxexist, simply assume equals0+. claim X1 1 F2 (G) = 1 + F2 (G) X2 2 F1 (G) = 02 + F1 (G). shallprovide proof first statement. second one similar.Firstly, according Lemma 2, 1 01 . Secondly, Lemma 4, X1 1 F2 (G) =X1 1 (2 +(X1 X2 )). Therefore show X1 1 F2 (G) = 01 +F2 (G), need proveX1 1 (2 +(X1 X2 )) = 01 +2 +(X1 X2 ). 2 +(X1 X2 ) F2 (G),F2 (G) 1 + 2 + (X1 X2 ) 1 01 . construction prioritized revision,easily verify 01 + 2 + (X1 X2 ) X1 1 (2 + (X1 X2 )). Thereforeshow direction, i.e., X1 1 (2 + (X1 X2 )) 01 + 2 + (X1 X2 ).01 = X1 , X1 (2 + (X1 X2 )) consistent. follows X1 1 (2 +(X1 X2 )) X1 + (2 + (X1 X2 )) = 01 + 2 + (X1 X2 ), desired. 01 6= X1 ,1 +11, X1 max 2 (X1 X2 ) inconsistent.according definition max1max+1Therefore exists X12 + (X1 X2 ). assume448fiAn Ordinal Bargaining Solution Fixed-Point PropertyX1 1 (2 + (X1 X2 )). 6 01 + 2 + (X1 X2 ), {} 01 2 (X1 X2 )1 +1consistent. { } 01 2 (X1 X2 ). Notice X1 max .exists H X1 (2 + (X1 X2 )) { } 01 H. SinceX1 1 (2 + (X1 X2 )) H logically closed, H, contradictsconsistency H (2 +(X1 X2 )). Therefore X1 1 (2 +(X1 X2 )) 01 +2 +(X1 X2 ).Finally prove claim lemma. Let (X1 1 F2 (G)) (X2 2 F1 (G)).(01 + F2 (G)) (01 + F2 (G)). 01 + F2 (G), exists sentence 2 F2 (G) ` 2 2 01 . Similarly, exists sentence 1F1 (G) ` 1 1 02 . turns 1 2 01 02 .Thus 1 2 X1 X2 . However, X1 X2 F1 (G) + F2 (G). follows1 2 F1 (G) + F2 (G). Note 1 2 F1 (G) + F2 (G). Therefore concludeF1 (G) + F2 (G).Theorem 2 establishes link bargaining theory belief revision. linkhelps us understand reasoning process behind bargaining. even interestingextend result general multiagent case. However, main challengemultiple agents mutually revise beliefs.5. Game-theoretic Properties Bargaining Solutiongame theory, properties considered important bargaining solutioninclude individual rationality, Pareto optimality, ordinal invariance (or scale invariance),symmetry contraction independence. bargaining model, bargainers preferencesrepresented total preorder, order-preserving transformation preferenceschange order preferences. Therefore solution satisfies ordinal invariancetrivially. section, examine properties listbargaining solution. presenting results, first introduce conceptsnecessary game-theoretic analysis bargaining.5.1 Strategies UtilitiesTwo concepts play essential roles game-theoretic analysis bargaining: strategyutility. Given bargaining game G = ((X1 , 1 ), (X2 , 2 )), strategy profile gamepair (S1 , S2 ) S1 X1 S2 X2 . strategy profile interpretedpair proposals demands players course bargaining.say strategy profile = (S1 , S2 ) compatible1. X1 X2 S1 X1 X2 S22. S1 S2 consistentObviously deal game compatible strategy profile. bargaining solutionF (G) also compatible strategy profile G.consider gains player strategy profile. Assumestrategy profile (S1 , S2 ) leads agreement, player payoff utility defined as:449fiZhang & Zhang(ui (Si ) =max{k : Xik Si },min{k : Xik = Xi },Si 6= Xi ;otherwise.words, ui (S) counts number top block demands covered Si .Note payoff count individual demands. Specifically define utilitydefault disagreement point (, ) (0, 0).5.2 Pareto OptimalityBased definition, easy see solution satisfies individual rationality (in sense game theory) game G, ui (Fi (G)) 0 = 1, 2.consider Pareto efficiency.Pareto optimality one important properties bargaining solution.call compatible strategy profile (S1 , S2 ) game (strictly) Pareto optimalexist compatible strategy profile (S10 , S20 ) game either u1 (S10 )u1 (S1 ) & u2 (S20 ) > u2 (S2 ) u1 (S10 ) > u1 (S1 ) & u2 (S20 ) u2 (S2 ).compatible strategy profile (S1 , S2 ) game weakly Pareto optimalexist another compatible strategy profile (S10 , S20 ) game u1 (S10 ) > u1 (S1 )u2 (S20 ) > u2 (S2 ).Theorem 3 bargaining game G, F (G) weakly Pareto optimal.Proof: Suppose compatible strategy profile (S1 , S2 ) G u1 (S1 ) >GG . Sinceu1 (F1 (G)) u2 (S2 ) > u2 (F2 (G)). u1 (S1 ) > maxu2 (S2 ) > maxu1 (S1 )u2 (S2 )(S1 , S2 ) compatible, X1X2(X1 X2 ) consistent. turnskkG , contradicts Lemma 2. Thereforemax{k : X1 X2 (X1 X2 ) consistent} > maxF (G) weakly Pareto optimal.Obviously, solution F satisfy strict Pareto optimality. instance,solution G0 Example 5 (the second part example) strictly Pareto optimal(but weakly Pareto optimal). problem solution natureproblem domain consider. well known game theory feasible setbargaining game convex, guarantee unique bargaining solutionstrictly Pareto optimal (Kaneko, 1980; Mariotti, 1996). Therefore, non-convex domain,need trade-off uniqueness solutions strict Pareto optimality (Conley& Wilkie, 1991, 1996; Mariotti, 1998; Xu & Yoshihara, 2006). using similar approachintroduced Zhang (2007), map logically represented bargaining gamenumerically represented bargaining game. mapping, feasible setcorresponds logically represented bargaining game non-convex unless demandsets logical bargaining game consistent. Since solution unique solution,cannot expect strictly Pareto optimal.5.3 Restricted Symmetrygame theory, bargaining game symmetric feasible set invariantpermutation point feasible set (Nash, 1950). However, concept symmetry easy extended logic-based bargaining models bargaining450fiAn Ordinal Bargaining Solution Fixed-Point Propertyproblem represented physical items. Permutation deals make sense.One may wonder judge fairness bargaining without concept symmetry.point view, thing fair outcome negotiation. outcomenegotiation relies bargaining power party. bargainer highernegotiation power receives gains negotiation. However, reasonableassume negotiation based fair bargaining procedure negotiationprotocol. construction bargaining solution meant capture idea fairnegotiation protocols. approach use paper similar idea bargainingagenda (ONeill et al., 2004). consider negotiation process consists severalrounds stages. round, parties reach agreements new issuesconsidered previous rounds. assume party always placehigher wanted demands earlier rounds. demands mutuallyaccepted earlier rounds remain agreements negotiationlater rounds. new agreement reached, negotiation procedurestops. process, negotiation always terminates level prioritydemands players.Theorem 4 bargaining game G = ((X1 , 1 ), (X2 , 2 )), X1 X2 logicallyclosed, natural number nF(G) = (X1 (X1n + X2n + (X1 X2 )), X2 (X1n + X2n + (X1 X2 )))G . case X n + X n = + .Proof: fact, prove n = max1221Obviously X1 X2 consistent, result trivial. Therefore assumeX1 X2 inconsistent. prove case F1 (G) = X1 (1 + 2 + (X1 X2 )).proof part similar.(D1 , D2 ) (G), 1 D1 2 D2 . prove X1 (1 +2 +(X1 X2 )) D1 . case, exists sentence X1 (1 + 2 + (X1 X2 ))6 D1 . one hand, X1 (1 +2 +(X1 X2 )) implies D1 D2 `1 + 2 + (X1 X2 ) D1 + D2 . hand, 6 D1 implies{} D1 D2 inconsistent due maximality deals. follows D1 D2 ` .Therefore D1 D2 inconsistent, contradiction. proved deal(D1 , D2 ) (G), X1 (1 + 2 + (X1 X2 )) D1 . Thus X1 (1 + 2 + (X1 X2 ))D1 = F1 (G). proof Lemma 4 shown direction inclu-(D1 ,D2 )(G)sion F1 (G) X1 (1 +2 +(X1 X2 )) holds. Therefore F1 (G) = X1 (1 +2 +(X1 X2 )).theorem shows termination negotiation, party remainG , including common demandsdemands level, i.e., maxlogical consequences. However, solution seemingly excludes low ranked irrelevantitems. due assumption logical closedness demand sets. fact,assumption, items irrelevant two statements , Xi ,always Xi . assume logical closednessgeneral.Another question may arise player could gain negotiation powerputs negotiation items earlier stages agenda (effectively451fiZhang & Zhangcould gain negotiation end disagreement). fact, truenatural risk breakdown taken account. player places conflictiveitem earlier stage agenda, negotiation would terminate sooner. Thereforeordering demands part strategy bargainer. discuss issueseparate section (see Section 6)5.4 Contraction IndependenceContraction Independence, called Independence Irrelevant Alternatives (IIA), requiresalternative judged best compromise problem,still judged best subproblem contains (Thomson, 1994). logic-basedbargaining model, alternatives explicitly given. However, easily defineconcept subproblem terms bargainers prioritized demand sets.bargaining game G0 = ((X10 , 01 ), (X20 , 02 )) subgame G = ((X1 , 1 ), (X2 , 2 )),denoted G0 v G, = 1, 2,1. Xi0 Xi ,2. 0i =i (Xi0 Xi0 ),3. Xi , Xi0 , Xi0 .words, Xi0 upper segment Xi respect Xi hierarchy.Theorem 5 Let G0 v G. F (G) strategy profile G0 , F (G0 ) = F (G).Proof: First, since F (G) strategy profile G0 , X1 X2 Xi0 (i = 1, 2).G0G .According definition subgame, easy show maxmax00GGcondition F (G) strategy profile G again, max = max , meansG0 G share core (1 , 2 ). deal (D1 , D2 ) (G), obviously(D1 X10 , D2 X20 ) deal G0 (D1 X10 , D2 X20 ) (G0 ). followsF1 (G0 ) D1 F2 (G0 ) D2 . Therefore F1 (G0 ) F1 (G) F2 (G0 ) F2 (G).hand, (D10 , D20 ) (G0 ), extend deal (D1 , D2 )G (D1 , D2 ) (G) G0 G share core X1 X2 Xi0(i = 1, 2). Since F (G) strategy profile G0 , F1 (G) D1 X10 F2 (G)D2 X20 . follows F1 (G) D10 F2 (G) D20 , implies F1 (G) F1 (G0 )F2 (G) F2 (G0 ). conclude F (G0 ) = F (G).Note claim theorem weaker Nashs IIA because,two bargaining games G G0 , alternatives G0 subset alternativesG guarantee G0 subgame G. However, pointed manyauthors, Nashs IIA original form longer plausible assumption domainnon-convex bargaining problems (Conley & Wilkie, 1996; Mariotti, 1998; Zhang, 2007).6. Bargaining Power Risk Postureshown Section 1, representing bargainers preferences ordinalautomatically solve problem ordinal bargaining (see Example 1).452fiAn Ordinal Bargaining Solution Fixed-Point Propertyordinal preference much less expressive power cardinal utility. unable express risk posture player, determines players bargaining power. successfulsolution ordinal bargaining problem supply alternative mean expressbargainers attitude towards risk. section, illustrate two case studiesproblem solved using framework.6.1 Case Study I: Bargainers Attitude towards RiskLet us revisit bargaining game Example 4, demand sets X1 = {p, r}X2 = {q, r}. Consider following variations preferences, reflect differenceplayers attitude towards risk (note different cases lead different games).G1 : X1 = {{p}, {r}} X2 = {{q}, {r}}10G2 : X2 = {{p, r}, {}} X2 = {{q}, {r}}G3 : X3 = {{p}, {r}} X2 = {{q, r}, {}}G4 : X4 = {{p, r}} X2 = {{q, r}}easy calculate solutions games:F (G1 ) = ({p}, {q})F (G2 ) = ({p, r}, {q})F (G3 ) = ({p}, {q, r})F (G4 ) = (, )G1 , players risk-averse, players rank conflicting item rlowest priority. means incentive reach agreement.G2 , player 1 aggressive since conflicting item highly ranked. findplayer 1 game case. surprising. general, risk-averse playerwould gain disadvantage negotiation comparing risk-lover (see Roths book, 1979b,p.35-60). G3 symmetrical G2 . G4 , players aggressive, therefore gameends disagreement.example, clear see bargainers attitude towards risk specifiedmodel. risk-averse player would give demands likely conflictdemands player relatively lower priorities agreement likelyreached. contrast, risk-loving player would firmly entrench conflictingdemands. Notice logic plays crucial role representation. Simply expressingbargainers demands physical terms without specifying relation sufficientlead ordinal solution. crucial specify logical relations demandsplayers. example, contradiction demands r r play mainrole determination solutions. main difference modelgame-theoretic models.6.2 Case Study II: Discretization Numerical Gamesmentioned introduction section, risk posture represented non-linearityutility functions Nashs model. subsection, use example cake division(Example 1) show non-linearity preferences represented model.10. demand sets represented prioritized partitions. corresponding preference orderingsp 1 r q 2 r, respectively.453fiZhang & Zhangrepresent bargaining problem logical form, need discretize domain.Let pn = {Player receives less n percentages cake player B getsremain}, n natural number 0 100. addition, followingconstraints acknowledged players:C = {pn+1 pn : n = 0, 1, 2, , 100}says player 1 receives less n + 1 percents cake, mustreceive less n percents.demands two players representedXA = C {p0 , p1 , p1 , , p100 }XB = C {p101 , p100 , p99 , , p1 }Assume player arranges demands according linear scale share.hierarch demand set is11 :{C, {p0 , p5 }, {p6 , p10 }, , {p96 , , p100 }}Player B arranges demands according square share. hierarchdemand set is12 :{C, {p101 , , p79 }, {p78 , , p70 }, , {p6 , p5 , p4 }, {p3 , p2 }, {p1 }}According setting, solution bargaining game is:(C {p34 , p33 , , p0 }, C {p42 , p43 , , p101 })easy calculate players agree division cake 34 xA < 4258 < xB 66.13 Therefore solution gives similar prediction game-theoreticsolutions. indicates risk posture players embeddedmodel. fact, easily see player B aggressive ranks higherconflicting items relatively higher player does. instance, player B ranksequal share (50/50) 7th level player ranks 11th level.seen example, ordering demands reflectplayers gains demands represents players preference retaining abandoning demands. another significant difference bargaining modelgame-theoretic models.One may ask whether player could get advantages cheating senseagent knows demands ranking party, agent adjustdemand hierarchy order obtain better outcome. Yes, possible. tellopposite want (your demands) release rankingdemands. Otherwise, lose bargaining power. reason attitudetowards risk encoded ranking demands.11. indicates player ignores small differences divisions. instance, may considershare 0-5% means him. real negotiation, player may request 100%,95%, 5% sequence giving 5% round bargaining.12. Player B claims share sequence 100%, 98%, 95%, dropping demand scalesquare.13. Note communication players. Therefore player may giveneeded (similar sealed-bid auction).454fiAn Ordinal Bargaining Solution Fixed-Point Property7. Computational Complexitysection, study computational properties bargaining solution developed earlier. assume readers familiar complexity classes P,NP, coNP, P2 P2 = coP2 . class DP contains languages LPL = L1 L2 L1 NP L2 coNP. Also class Pk+1 = P k containslanguages recognizable polynomial time deterministic Turing machinePk oracle. particular, class P2 [O(logn)] contains languages recognizabledeterministic Turing machine O(logn) calls NP oracle14 P3 contains languages recognisable deterministic Turing machine P2 oracle. well knownP N P DP P2 P2 P3 , inclusions generally believedproper (readers may refer work Papadimitriou, 1994, details).Consider bargaining game G = ((X1 , 1 ), (X2 , 2 )) X1 X2 finite.mentioned Section 2.3, always write Xi = Xi1 Xim , Xik Xil =k 6= l. Also k < m, formula Xik , existXil (k < l) . Therefore, convenience analysis, restsection, specify bargaining game G = (X1 , X2 ), X1 =X2 =nj=1i=1X1iX2j , X11 , , X1m , X21 , , X2n partitions X1 X2 respectivelysatisfy properties mentioned above.following four major decision problems important order understandcomputational properties bargaining game model developed previoussections: let G = (X1 , X2 ) bargaining game, would like decide: (1) whethergiven pair (D1 , D1 ) Di Xi (i = 1, 2) deal G; (2) whether given pairpropositional formulas (1 , 2 ) core G respectively; (3) whether given formuladerivable core G; (4) whether given strategy profile G solutionG, First, following result deciding deal given bargaining game.Theorem 6 Let G = (X1 , X2 ) bargaining game, D1 X1 D2 X2 . Decidingwhether (D1 , D2 ) deal G DP-complete.Proof: Membership proof. According Definition 2, decide whether (D1 , D2 ) dealG, D1 (or D2 ), need check: (1) k = 1, , (or k 0 = 1, , n resp.),whether D2k(D1 X1j ) (or D1j=1k0(D2 X2j ) resp.) consistent; (2) checking whetherj=1X1 X2 Di (i = 1, 2); (3) D1 D2 maximal subsets X1 X2Skrespectively. (1), observe k, set j=1(D1 X1j ) computedpolynomial time, checking consistency D2k(D1 X1j ) NP.j=1D2 case. obvious see (2) done polynomial time. consider (3).order check whether D1 D2 maximal subsets X1 X2 respectivelysatisfying condition, consider complement problem: assume D1 (wealso assume D2 ) maximal subset X1 satisfying required conditions,14. Note literatures, different notions used denote complexity class P N P [logn] ,P||N P P2 .455fiZhang & Zhangexists k (X1 \ D1 ) D2k((D1 {}) X1k )j=1consistent. Clearly, guess k, formula interpretation S, checkwhether model D2k((D1 {}) X1k ). Obviously, NP. originalj=1problem coNP.Hardness proof. known given propositional formulas 1 2 , deciding whether 1 satisfiable 2 unsatisfiable DP-complete (Papadimitriou, 1994).Given two propositional formulas 1 2 , construct polynomial time transformation 1 satisfiability 2 unsatisfiability deal decision problemgame. simply define game G = (X1 , X2 ) = ({2 1 p}, {q} {p}),p, q propositional atoms occurring 1 2 . Note X1 X2 = . LetD1 = X1 D2 = {q}. show (D1 , D2 ) deal G, is, D2maximal subset X2 X1 D2 consistent, 1 satisfiable 2unsatisfiable.() Clearly, 1 satisfiable 2 unsatisfiable, X1 D2 consistent,X1 X2 consistent. (D1 , D2 ) deal G.() Suppose 1 2 unsatisfiable. X1 consistent. 12 satisfiable, observed X1 X2 consistent. (D1 , D2 ) dealG. Finally, suppose 1 unsatisfiable 2 satisfiable. case, X1 X2 stillconsistent. means, (D1 , D2 ) deal G either.see definition, core bargaining plays essential roleconstruction bargaining solution. following theorem provides complexityresult decision problem.Theorem 7 Let G bargaining game. Deciding whether given pair sets propositionalformulas (1 , 2 ) core G DP-complete.Proof: Membership proof. Let G = (X1 , X2 ), X1 = X1i , X1 = X1j .outline algorithm check whether (1 , 2 ) core G: (1) check whetherk, 1 =ki=1X1i 2 =ki=1X2i , 1 6=k+1i=1X1i 2 6=k+1i=1X2i ; (2) check whether1 2 consistent; (3) check 1 X1k+1 2 X2k+1 consistent. Clearly, (1)done polynomial time, checking (2) NP checking (3) coNP.problem DP.Hardness proof. hardness proof similar described proofTheorem 7 variations. Given two propositional formulas 1 2 , reducedecision problem 1 satisfiability 2 unsatisfiability problem. LetG = (X1 , X2 ), X1 = {2 1 p} {q}, X2 = {q} {p}, p, qpropositional atoms occurring 1 2 . specify pair sets formulas:(1 , 2 ) = ({2 1 p}, {q}).show (1 , 2 ) core G 1 satisfiable 2unsatisfiable. Suppose 1 satisfiable 2 unsatifiable. 1 = {1 p, q},consistent. hand, 1 {q} 2 {p} = {1 p, q, p},consistent. (1 , 2 ) core G.456fiAn Ordinal Bargaining Solution Fixed-Point Propertyprove direction. (1) 1 2 satisfiable. 1 2 ={(2 1 )(2 p), q}, consistent. However, also see 1 {q}2 {p}leat one model satisfies 2 , q p. implies (1 , 2 ) longercore G. (2) 1 2 unsatisfiable. case, 1 2 consistentmore. (1 , 2 ) core G. (3) 1 unsatisfiable 2 satisfiable.Again, situation, 1 2 longer consistent, hence coreG. completes proof.Recall intuition behind core final agreement maximizes fairlyagents demands without violating overall consistency. interestingknow whether certain information derivable agents demandsfinal agreement. Let = (1 , 2 ) core bargaining game G. define C(G) `1 ` 2 ` .Theorem 8 Given bargaining game G propositional formula . Deciding whetherC(G) ` P2 [O(logn)]-complete.Proof: Membership proof. outline algorithm deciding C(G) ` follows: (1)compute core (1 , 2 ) G, (2) checking 1 ` 2 ` . definition,= (1 , 2 ) core G iff 1 =kj=1X1j , 2 =kj=2X1j , k maximalnumber makes 1 2 consistent. Clearly, k determinated binay searchO(logn) NP oracle calls. checking 1 ` 2 ` done two NPoracle calls. problem P2 [O(logn)].Hardness proof. reduce P2 [O(logn)]-complete PARITY B(Kobler, Schoning,& Wagner, 1987; Wagner, 1988) problem. instance PARITY Bsetpropositional formulas 1 , , n satisfiable, j i, jsatisfiable. problem decide whether number satisfiable formulasodd. Without loss generality, assume n even number. constructpolynomial time bargaining game G = (X1 , X2 ) follows:n/2X1 = X1 = {2 p} {4 3 p} {n n1 p},n/2X2 = X2 = {q1 } {q2 } {qn/2 },p, q1 , , qn/2 propositional atoms occurring 1 , , n . Let = p.show C(G) ` p odd number satisfiable formulas 1 , , n .First, suppose k odd number, 1 , , k satisfiable, k+1 , , n satisfiable. observed =[k/2]j=1X1j X2j = {2 p, 4 3 p, , k+1k p} {q1 , , q[k/2] } consistent, {k+3 k+2 p} {q[k/2]+1 } con[k/2]sistent. (j=1X1j ,[k/2]j=1X2j ) core G. Also, since k+1 satisfiable, followsk+1 k p reduced k p, contained[k/2]j=1X1j . C(G) ` p.Second, assume even number satisfiable formulas 1 , , n . Let k457fiZhang & Zhangeven number 1 , , k satisfiable k+1 , , n satisfiable.case, observed =k/2j=1X1j X2j = {2 p, 4 3 p, , kk1 p} {q1 , , qk/2 } consistent, {k+2 k+1 p} {qk/2+1 }consistent. (k/2j=1k/2j=1X1j ,k/2j=1X2j ) core G. obvious p cannot derivedX1j . is, C(G) 6` p. completes proof.Finally, consider decision problem solution given bargaining game.following theorem gives complexity upper bound.Theorem 9 Let G bargaining game,and (S1 , S2 ) strategy profile G. Decidingwhether (S1 , S2 ) solution G P3 .Proof: Definition 3, need check whether S1 =(D1 ,D2 )(G)(D1 ,D2 )(G)D1 S2 =D2 , (G) = {(D1 , D2 ) (G) : 1 D1 , 2 D2 }, (1 , 2 )core G. Note simplicity, use notion core representsolution.first consider complement deciding S1 =D1 : checking whetherS1 6=(D1 ,D2 )(G)(D1 ,D2 )(G)D1 . Clearly, S1 6=(D1 ,D2 )(G)(D1 ,D2 )(G)D1 iff (1) S1 6(D1 ,D2 )(G)D1 ; (2)D1 6 S1 . first guess pair sets propositional formulas (1 , 2 ),check core G. According Theorem 8, know P2 . Clearly,(1) holds iff exists formula (a) S1 , (b) 6D1 .(D1 ,D2 )(G)Further, (b) holds iff exists deal (D1 , D2 ) 1 D1 , 2 D2 6 D1 .guess (D1 , D2 ), check S1 , (D1 , D2 ) deal containingcore, 6 D1 . Thorem 7 know checking (D1 , D2 ) deal donetwo NP oracle calls. task (1) solved P2 .hand, (2) holds iff existsD1(D1 ,D2 )(G)6 S1 . solve task (2), consider complement: , 6 S1 ,6D1 . Since |X1 | + |X2 | formulas need check, checking(D1 ,D2 )(G)S1 done linear time. 6 S1 , needcheck 6D1 , which, shown above, done P2 . Therefore,(D1 ,D2 )(G)S1 , |X1 | + |X2 | checkings whether 6PP 2 = P3 . follows deciding whether S1 =Consequently, original problem also P3 .(D1 ,D2 )(G)(D1 ,D2 )(G)D1 ,D1 P3 .proof Theorem 10, observed computation bargaininggame different Nebels prioritized belief revision.458fiAn Ordinal Bargaining Solution Fixed-Point PropertyTheorem 10 Let G bargaining game (S1 , S2 ) strategy profile G. Decidingwhether (S1 , S2 ) solution G DP-hard.Proof: consider special game G = (X1 , X2 ) X1 = X11 X12 , X2 = X21 (i.e.X2 singleton partition), X11 X21 consistent X1 X2 consistent.case, know (X11 , X21 ) core G. question reduced decidewhether S2 = X2 , S1 maximal subset X1 containing X11 S1 X2consistent. proof Theorem 3, easy see DP-hard.Obviously, gap lower bound upper bound solutiondecision problem. also sheds light bargaining solution cannot representedterms traditional belief revision operators.Theorem 11 Let G bargaining game (S1 , S2 ) strategy profile G. Decidingwhether (S1 , S2 ) solution G P2 [O(logn)], given set deals (G)provided.Proof: decide whether (S1 , S2 ) solution G, need following: (1)compute core (1 , 2 ) G; (2) compute (G) (G) (1 , 2 ); (3) computeD10 =D1 D20 =D2 ; (4) compare whether S1 = D10(D1 ,D2 )(G)(D1 ,D2 )(G)S2 = D20 . Task (1) solved one P2 [O(logn)] oracle call; (1 , 2 ), task(2) done polynomial time; tasks (3) (4) done polynomial time.problem P2 [O(logn)].8. Related Workinvestigation ordinal bargaining theory diverges two directions. first directionfocuses existence ordinal solution Nash bargaining model.mentioned introduction section, Shapley, Kibris, Safra Samet shownsolution bargaining problems three agents more, satisfiesordinal invariance, symmetry Pareto optimality (Shubik, 1982; Ozgur Kibris, 2004;Safra & Samet, 2004). result interesting shows differencebilateral bargaining multilateral bargaining. However, solve problemordinal bargaining solution constructive and, importantly,alternative mean offered facilitate representation bargainers risk attitude. CalvoPerers investigated bargaining problems mixed players: cardinal playersordinal players (Calvo & Peters, 2005). bargaining solution called utility invariantordinally invariant ordinal players cardinally invariant cardinalplayers. proved solution satisfying utility invariance, Pareto optimalityindividual rationality provided least one player cardinal. Obviously, resultperipheral utility invariant solution necessarily ordinal invariant.direction investigation tries circumvent Shapleys impossibility resultaltering Nash bargaining model. Rubinstein et al. reinterpreted Nash bargainingsolution respect ordinal preferences (Rubinstein et al., 1992). restating Nashs459fiZhang & Zhangaxioms, proved redefined solution, referred ordinal-Nash solution, satisfies Pareto optimality, symmetry contraction independence (ordinal invariance holdstrivially). However, result based assumption preference orderingplayer complete, transitive continuous set finite lotteries topological space. unclear use specific preference language describeplayers risk attitudes. important, advantages ordinal preferences, easeelicitation robustness corresponded solutions, may lost preferenceordering extended space lotteries. ONeill et al. model bargaining situationfamily Nash bargaining games, parameterized time (ONeill et al., 2004).bargaining solution defined function specifies outcome time.model, bargainers risk attitude expressed varying preferencestime, intuitive. Since solution longer single point functiontime, construction proposed ordinal solution relies solution setssimultaneous differential equations.work developed based series previous work authors. fixed-pointcondition discussed paper firstly proposed Zhang et al. (2004)15 . ZhangZhang (2006a, 2006b) presented logical solution bilateral bargaining problem basedordinal preferences. However, solution satisfy fixed point condition.Zhang (2008) showed revision solution satisfies fixed-point condition.present paper develops systemizes solution discusses logicalgame-theoretic properties.9. Conclusionpresented bargaining solution bilateral bargaining problem basedlogical representation bargaining demands ordinal representation bargainerspreferences. shown solution satisfies desirable logical properties,individual rationality (logical version), consistency collective rationality,desirable game-theoretic properties, weak Pareto optimality, restricted symmetrycontraction invariance. ordinal invariance game-theoretic version individualrationality hold trivially. Due discrete nature logical representation, solution(strictly) Pareto optimal satisfy symmetry. However, demand setstwo players logically closed, solution meets restricted version symmetry.addition, demonstrated logical closedness assumption, outcomenegotiation result mutual belief revision terms Nebels prioritized beliefbase revision. result established link bargaining theory belief revision.link would play important role future research multiagent beliefrevision logic-based bargaining theory. complexity analysis indicatescomputation bargaining solution difficult prioritized belief base revision.satisfactory model bargaining able encode key factors determine bargaining outcome, bargainers demands, preferences, attitudes towards15. Jin et al. (2007) also introduced fixed-point condition negotiation functions, sayscertain conditions, negotiating outcome negotiation generates outcome.Obviously bargaining solution satisfies fixed-point condition outcome bargainingconsistent, remains negotiation.460fiAn Ordinal Bargaining Solution Fixed-Point Propertyrisk on. Cardinal utility specifies two sorts information: preference possible agreements (via ordering utility values) risk attitudes (via non-linearityutility function). However, second sort information, determines playersbargaining power, may lost ordinal transformation. Meanwhile, bargainingmodel based purely ordinal information preferences automatically solveproblem bargainers risk attitude even inexpressible model. Therefore ordinal bargaining theory must supply facility describe informationordinal preferences, including risk attitudes. paper, specify bargainingsituation logical structure. Bargainers demands, goals beliefs described logicalstatements. conflicts demands two players identifiedconsistency checking. importantly, bargainers attitudes towards risk expressiblemodel natural way: risk-averse player tends give conflicting demandrelatively lower priority agreement could likely reached risk-loverwould firmly entrench demands less care whether demands contradictopponents.issues worth investigation. Firstly, shown solutionsatisfies set logical properties game-theoretic properties. interesting knowwhether axiomatic system exactly characterizes solution. mainchallenge construction solution syntax-dependent. simplyimpose logical closedness demand sets, lose desirable properties,inclusion irrelevant items computational results. applyassumption, shall need axioms specify way logical representation.words, axioms specify demand set represented syntactically.Secondly, present work offers solution bilateral bargaining situations.supply model bargaining agents. Therefore current frameworkdeal issues like demand formed?, demand rankedhigher another? bargain effectively?. interesting logicagency used developed model bargaining agents logic interactslogic bargaining.Finally, issues computational complexity proposed solution remainunsolved. shown paper membership checking solution DPhard P3 . clear yet upper bound lower bound gapclosed. think new complexity proof technique may needed challenge.Acknowledgmentsauthors wish thank Norman Foo, Michael Thielscher anonymous reviewerscomments. work partially supported Australian Research Councilproject LP0883646.ReferencesBrewka, G. (1989). Preferred subtheories: extended logical framework default reasoning. Proceedings 11th International Joint Conference Artificial Intelligence(IJCAI), pp. 10431048.461fiZhang & ZhangCalvo, E., & Peters, H. (2005). Bargaining ordinal cardinal players. GamesEconomic Behavior, 52, 2033.Conley, J. P., & Wilkie, S. (1991). bargaining problem without convexity: extendingEgalitarian Kalai-Smorodinsky solutions. Economics Letters, 36 (4), 365369.Conley, J. P., & Wilkie, S. (1996). extension Nash bargaining solution nonconvex problems. Games Economic Behavior, 13, 2638.Gardenfors, P. (1992). Belief revision: introduction. Belief Revision, pp. 120. Cambridge University Press.Gardenfors, P., & Makinson, D. (1988). Revisions knowledge systems using epistemicentrenchment. Proceedings Second Conference Theoretical Aspect Reasoning Knowledge (TARK88), pp. 8396.Jin, Y., Thielscher, M., & Zhang, D. (2007). Mutual belief revision: semantics computation. Proceedings 22nd AAAI Conference Artificial Intelligence (AAAI07), pp. 440445.Kalai, E. (1977). Proportional solutions bargaining situations: interpersonal utility comparisons. Econometrica, 45 (7), 16231630.Kalai, E., & Smorodinsky, M. (1975). solutions Nashs bargaining problem. Econometrica, 43 (3), 513518.Kaneko, M. (1980). extension Nash bargaining problem Nash socialwelfare function. Theory Decision, 12, 135148.Kobler, J., Schoning, U., & Wagner, K. (1987). difference truth-table hierarchiesNP. RAIRO - Theoretical Informatics Applications, 21, 41937.Kraus, S., Sycara, K., & Evenchik, A. (1998). Reaching agreements argumentation:logical model implementation. Artificial Intelligence, 104, 169.Mariotti, M. (1996). Non-optimal Nash bargaining solutions. Economics Letters, 52, 1520.Mariotti, M. (1998). Extending Nashs axioms nonconvex problems. Games Economic Behavior, 22, 377383.Meyer, T., Foo, N., Kwok, R., & Zhang, D. (2004). Logical foundations negotiation:strategies preferences. Proceedings 9th International ConferencePrinciples Knowledge Representation Reasoning (KR04), pp. 311318.Muthoo, A. (1999). Bargaining Theory Applications. Cambridge University Press.Myerson, R. B. (1991). Game Theory: Analysis Conflict. Harvard University Press.Nash, J. (1950). bargaining problem. Econometrica, 18 (2), 155162.Nebel, B. (1992). Syntax-based approaches belief revision. Gardenfors (Ed.), BeliefRevision, pp. 5288. Cambridge University Press.ONeill, B., Samet, D., Wiener, Z., & Winter, E. (2004). Bargaining agenda. GamesEconomic Behavior, 48, 139153.Osborne, M. J., & Rubinstein, A. (1990). Bargaining Markets. Academic Press.462fiAn Ordinal Bargaining Solution Fixed-Point PropertyOzgur Kibris (2004). Ordinal invariance multicoalitional bargaining. Games Economic Behavior, 46, 7687.Papadimitriou, C. (1994). Computational Complexity. Addison Wesley.Parsons, S., Sierra, C., & Jennings, N. R. (1998). Agents reason negotiatearguing. Journal Logic Computation, 8 (3), 261292.Perles, M. A., & Maschler, M. (1981). super-additive solution Nash bargaininggame. International Journal Game Theory, 10, 163193.Poole, D. (1988). logical framework default reasoning. Artif. Intell., 36 (1), 2747.Rosenschein, J. S., & Zlotkin, G. (1994). Rules Encounter: Designing ConventionsAutomated Negotiation among Computers. MIT Press.Roth, A. E. (1979a). Axiomatic Models Bargaining. Springer-Verlag.Roth, A. E. (1979b). impossibility result concerning n-person bargaining games. International Journal Game Theory, 8, 129132.Rubinstein, A., Safra, Z., & Thomson, W. (1992). interpretation Nash bargaining solution extension non-expected utility preferences. Econometrica,60 (5), 11711186.Safra, Z., & Samet, D. (2004). ordinal solution bargaining problems manyplayers. Games Economic Behavior, 46, 129142.Sakovics, J. (2004). meaningful two-person bargaining solution based ordinal preferences. Economics Bulletin, pp. 16.Sharpley, L. S. (1969). Utility comparison theory games. Guilbaud, G. T.(Ed.), La Deecision, pp. 251263. Paris: Editions du Centre National de la RechercheScientifique.Shubik, M. (1982). Game Theory Social Sciences: Concepts Solutions. MITPress, Cambridge.Sycara, K. P. (1990). Persuasive argumentation negotiation. Theory Decision, 28,203242.Thomson, W. (1994). Cooperative models bargaining. Aumann, R., & Hart, S. (Eds.),Handbook Game Theory, Vol. 2, chap. 35, pp. 12371284. Elsevier.Wagner, K. (1988). Bounded query computations. Proceedings 3rd ConferenceStructure Complexity Theory, pp. 419437.Xu, Y., & Yoshihara, N. (2006). Alternative characterizations three bargaining solutionsnonconvex problems. Games Economic Behavior, 57 (1), 8692.Zhang, D. (2005). logical model Nash bargaining solution. Proceedings 19thInternational Joint Conference Artificial Intelligence (IJCAI-05), pp. 983988.Zhang, D. (2007). Reasoning bargaining situations. Proceedings 22nd AAAIConference Artificial Intelligence (AAAI-07), pp. 154159.Zhang, D. (2008). fixed-point property logic-based bargaining solution. AI 2008,pp. 3041. Springer.463fiZhang & ZhangZhang, D., & Foo, N. (2001). Infinitary belief revision. Journal Philosophical Logic,30 (6), 525570.Zhang, D., Foo, N., Meyer, T., & Kwok, R. (2004). Negotiation mutual belief revision,.Proceedings 19th National Conference Artificial Intelligence (AAAI-04),pp. 317322.Zhang, D., & Zhang, Y. (2006a). computational model logic-based negotiation.Proceedings 21st National Conference Artificial Intelligence (AAAI-06), pp.728733.Zhang, D., & Zhang, Y. (2006b). Logical properties belief-revision-based bargainingsolution. AI 2006: Advances Artificial Intelligence (AI-06), pp. 7989.464fiJournal Artificial Intelligence Research 33 (2008)Submitted 12/07; published 10/08Similarities Inference Game TheoryMachine LearningIead Rezeki.rezek@imperial.ac.ukDepartment Clinical Neurosciences, Imperial CollegeLondon, SW7 2AZ, UKDavid S. Lesliedavid.leslie@bristol.ac.ukDepartment Mathematics, University BristolBristol, BS8 1TW, UKSteven ReeceStephen J. Robertsreece@robots.ox.ac.uksjrob@robots.ox.ac.ukDepartment Engineering Science, University OxfordOxford, OX1 3PJ, UKAlex RogersRajdeep K. DashNicholas R. Jenningsacr@ecs.soton.ac.ukrkd@ecs.soton.ac.uknrj@ecs.soton.ac.ukSchool Electronics Computer Science, University SouthamptonSouthampton, SO17 1BJ, UKAbstractpaper, elucidate equivalence inference game theory machinelearning. aim establish equivalent vocabulary twodomains facilitate developments intersection fields, proofusefulness approach, use recent developments field make usefulimprovements other. specifically, consider analogies smoothbest responses fictitious play Bayesian inference methods. Initially, useinsights develop demonstrate improved algorithm learning games basedprobabilistic moderation. is, integrating distribution opponentstrategies (a Bayesian approach within machine learning) rather taking simple empirical average (the approach used standard fictitious play) derive novel moderatedfictitious play algorithm show likely standard fictitious playconverge payoff-dominant risk-dominated Nash equilibrium simple coordination game. Furthermore consider converse case, show insights gametheory used derive two improved mean field variational learning algorithms.first show standard update rule mean field variational learning analogousCournot adjustment within game theory. analogy fictitious play,suggest improved update rule, show results fictitious variational play,improved mean field variational learning algorithm exhibits better convergencehighly strongly connected graphical models. Second, use recent advance fictitiousplay, namely dynamic fictitious play, derive derivative action variational learning algorithm, exhibits superior convergence properties canonical machine learningproblem (clustering mixture distribution).c2008AI Access Foundation. rights reserved.fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings1. Introductionrecently increasing interest research intersection game theorymachine learning (Shoham, Powers, & Grenager, 2007; Greenwald & Littman, 2007).work motivated observation whilst two fields traditionallyviewed disparate research areas, actually great deal commonalityexploited within fields. example, insightsmachine learning literature graphical models led development efficientalgorithms calculating Nash equilibria large multi-player games (Kearns, Littman, &Singh, 2001). Similarly, development boosting algorithms within machine learningfacilitated regarding engaged zero-sum game baselearner (Freund & Schapire, 1997; Demiriz, Bennett, & Shawe-Taylor, 2002).interdisciplinary inspiration promising, unless clearer understanding principal connections exist two disciplines, examplesremain isolated pieces research. Thus, general goal work exploreformal way commonalities game theory machine learning. orderso, consider important problem central fields; makinginferences based previous observations.first consider game theory, problem occurs context inferringcorrect strategy play opponent within repeated game. generallytermed learning games common approach use algorithm based fictitiousplay (see Fudenberg & Levine, 1999). Here, show insight Bayesian inference(a standard machine learning technique) allow us derive improved fictitious playalgorithm. specifically, show integrating distribution opponentstrategies (a standard approach within machine learning), rather taking simpleempirical average (the approach used within standard fictitious play algorithm),derive novel moderated fictitious play algorithm. Moreover, go demonstratealgorithm likely standard fictitious play converge payoffdominant risk-dominated Nash equilibrium simple coordination game1 .second part paper, consider mean field variational learning algorithm, popular means making inferences within machine learning.show analogies game theory allows us suggest two improved variational learningalgorithms. first show standard update rule mean field variational learninganalogous Cournot adjustment process within game theory. analogy fictitious play, suggest improved update rule, leads improved mean fieldvariational learning algorithm, term fictitious variational play. appealinggame-theoretic arguments, prove convergence procedure (in contrast standardmean-field updates suffer thrashing behaviour (Wolpert, Strauss, & Rajnarayan,2006) similar Cournot process), show algorithm exhibits better convergence highly strongly connected graphical models. Second, show recentadvance fictitious play, namely dynamic fictitious play (Shamma & Arslan, 2005),used derive novel derivative action variational learning algorithm. demon1. note form Bayesian learning games known converge equilibrium (Kalai& Lehrer, 1993). However, work players perform Bayesian calculations spacerepeated game strategies, resulting extremely complex inference problems. contrast, considerBayesian extension fictitious play, using insights machine learning aid myopic decision-making.260fiOn Similarities Inference Game Theory Machine Learningstrate properties algorithm canonical machine learning problem (clusteringmixture distribution), show exhibits superior convergence propertiescompared standard algorithm.taken together, results suggest much gained closerexamination intersection game theory machine learning. end,paper, present range insights allow us derive improved learning algorithmsfields. such, suggest initial first steps herald possibilitysignificant gains area exploited future.remainder paper organised follows. section 2 discuss work relatedinterplay learning games machine learning. Then, section 3 discusstechniques within machine learning used relation learning games. reviewstandard stochastic fictitious play algorithm, go derive evaluatemoderated fictitious play algorithm. change focus, section 4, showtechniques within game theory apply machine learning algorithms. Again, initiallyreview standard mean field variational learning algorithm, then, analogyfictitious play Cournot adjustment, present section 4.2 fictitious variationalplay algorithm. section 4.3 continue theme incorporate insights dynamicfictitious play derive evaluate derivative action variational learning algorithm.Finally, conclude discuss future directions section 5.2. Related Worktopics inference game theory traditionally viewed separate researchareas, consequently little previous research exploited common featuresachieve profitable cross-fertilisation.One area progress made use concepts game theoryfind optimum multi-dimensional function. context, Lambert, Epelman,Smith (2005) used fictitious play optimisation heuristic players representsingle variable act independently optimise global cost function. analysisrestricts attention class objective functions products independentvariables, thus rather limited practice.similar vein, Wolpert co-authors consider independent players who,actions, attempting maximise global cost function (Lee & Wolpert, 2004;Wolpert, 2004). body work, however, optimisation carriedrespect joint distributions variables chosen players. mean-fieldapproach taken, resulting independent choices player; approachsimilar flavour presented Section 4 article. However, paperexplicitly use advances theory learning games develop improved optimisationalgorithms.context improving game-theoretical algorithms using techniques machinelearning statistics, Fudenberg Levine (1999) show fictitious play interpretation Bayesian learning procedure. However interpretation shows fictitiousplay type plug-in classifier (Ripley, 2000), stop short using fullpower Bayesian techniques improve method. contrast, article takefully Bayesian approach deciding optimal action play game.261fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jenningsarticles cross-over attempted, overlap greatlycurrent article, include Demiriz et al. (2002) Freund Schapire (1997),interpreted boosting algorithms zero sum games, Kearns et al. (2001)consider use techniques graphical models (Jordan, Ghahramani, Jaakkola, &Saul, 1997) help calculate equilibria graphical games.3. Fictitious PlayFictitious play important model learning games source algorithmicdevelopments presented later work. begin presenting notation terminology used standard game-theoretic representation fictitious play. readerreferred work Fudenberg Levine (1999) extensive discussion.consider strategic-form games players indexed {1, . . . , I},use index players player i. Si denote finite setpure strategies (also known actions) available player i, set S1 S2 SIpure strategy profiles players, Si set pure strategy profilesplayers i. players pay-off function denoted Ri : R maps purestrategy profiles real line, i.e. set actions selected players associatedreal number.simple model usually extended allow players use mixed strategies(Si ), (Si ) denotes set probability distributions pure strategy setSi . Hence probability distribution discrete space Si . Writing =( 1 , 2 , . . . , ) probability distribution product individualmixed strategies, , extend reward functions space mixed strategiessettingRi () = E Ri (S)(1)E denotes expectation respect pure strategy profile selectedaccording distribution . SimilarlyRi (S , ) = Ei Ri (S , )(2)Ei denotes expectation respect Si .standard solution concept game theory Nash equilibrium. mixedstrategy profileRi () Ri ( , ) (Si ).(3)words, Nash equilibrium set mixed strategies playerincrease expected reward unilaterally changing strategy.players receive identical reward known partnershipgame. case, players acting independently trying optimise globalobjective function. special case important since corresponds distributedoptimisation problem, objective function represents reward functiongame. Nash equilibrium impossible improve expected value objectivefunction changing probability distribution single player. Thus Nash equilibriacorrespond local optima objective function.262fiOn Similarities Inference Game Theory Machine LearningFictitious play proceeds assuming repeated play game, every playermonitors action opponent. players continually update estimatesopponents mixed strategies taking empirical average past action choicesplayers. Given estimate play, player selects best response (i.e. actionmaximizes expected reward given beliefs). Thus, time t, estimatesupdated according11(4)bi (ti )t+1 = 1ti +t+1t+1bi (ti ), best response players empirical mixed strategies, satisfiesbi (ti ) argmax Ri (S , ti ).(5)Sicertain classes games, including partnership games mentioned previously, beliefs evolve according equation 4 known converge Nash equilibrium.hand, also exist games non-convergence equation 4shown Fudenberg Levine.3.1 Stochastic Fictitious PlayNow, one objection fictitious play discontinuity best response function, means players almost always play pure strategies, even beliefsconverged Nash equilibrium mixed strategies. overcome problems, fictitiousplay generalized stochastic fictitious play (see Fudenberg & Levine, 1999)employs smooth best response function, defined( ) = argmax Ri ( , ) + v ( )(6)(Si )temperature parameter v smooth, strictly differentiable concavefunction approaches boundary (Si ) slope v becomes infinite.One popular choice smoothing function v entropy function, resultslogistic choice function noise temperature parameter11R (S , )(7)( )(S ) = expZpartition function Z ensures best response adds unity. Thus,stochastic fictitious play, players choose every round action randomly selected usingsmooth best response current estimate opponents probability play.estimates process also known converge several classes games,including partnership games (Hofbauer & Hopkins, 2005) discussedsection 4. Several extensions fictitious play introduced attemptsextend classes games convergence achieved, including weakenedfictitious play (Leslie & Collins, 2005; van der Genugten, 2000) dynamic fictitiousplay (Shamma & Arslan, 2005). use extensions second partpaper improve convergence modifications variational learning.263fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings3.2 Moderated Fictitious Playfictitious play, player learns using empirical average past actionchoices players estimate current mixed strategy. estimatethought maximum likelihood estimate (MLE) opponents mixed strategytime assumption actions player selected usingmultinomial distribution parameterti = ti = argmaxP (Sui ; )(8)u=0P (Sui ; ) modelled product multinomial distributions P (Sui ; ) =QIjjcorrespondsj=1,j6=i (Su ). Fudenberg Levine note choicemaximum posteriori estimate opponent mixed strategies Bayesian model.However, machine learning perspective, logistic choice best response function, given equation 7, may viewed single layer neural network sigmoidactivation function (Bishop, 2006). Substituting unknown parameter, ti , equation7 maximum likelihood estimate (or Bayesian point estimate) fails takeaccount anything known parameters distribution; classifiers using approach called plug-in classifiers (Ripley, 2000). Bayesian perspective,better predictions obtained integrating parameters distribution thuscomputing posterior predictive best response function (Gelman, Carlin, Stern, & Rubin,2000). process known neural network literature moderation (MacKay,1992).suggests modification fictitious play term moderated fictitious play.this, every player uses posterior predictive best response, obtained integratingopponent mixed strategies, weighted posterior distribution givenpreviously observed actions. conventional choose use posterior meanpoint estimate, strategy chosen playerZ(9)) titi = (ti )P (ti | S1:t) posterior probability opponents mixed strategies ti givenP (ti | S1:tplay time 1 t.observed history S1:tSince model observed pure strategies player j observations multinomialrandom variable parameters j , place uniform Dirichlet prior, Dir( j ; j0 ),j , parameters j0k = 1. posterior distribution ti thereforeproduct independent Dirichlet distributions,P (ti | siDir(tj ; jt )(10)1:t ; ) =j6=iPjt (sj ) = 1 + tu=1 I{sju = sj }, indicator function.multiple approaches estimating integral equation 9. generallyapplicable approach sample N opponent mixed strategies,n , posterior264fiOn Similarities Inference Game Theory Machine Learningdistribution use Monte Carlo approximation integral equation 9, giventiN1 X(n ).N n=1(11)investigate effect moderation also consider analytic expression timakes use two approximations. first approximates distribution equation10 normal distributionN (; )(12)mean vector=/t(13)1 (1 1 ) . . .1 K1..=.K 1. . . K (1 K )(14)covariance matrixPK = |S |=k tk (Bernardo & Smith, 1994). second, givenMacKay (1992) case two action choices, approximates integral sigmoid1=g1 + exprespect normal distribution, P (a) = N (a; m, 2 ) mean variance 2 ,modified sigmoidZ1P (a) gg()m(15)() =21+821.(16)see equations 15 16 effect moderation scale high rewardsproportion variance (and thus uncertainty estimated opponent mixedstrategy) shift probability value action closer 0.5 (i.e. unityzero 0.5). onset play little known opponent, playingactions equal probability intuitively reasonable course action.test general moderated fictitious play equation 9 Dirichlet posteriordistributions, investigate games varying degrees risk dominance, sincecases equilibrium selection strategies strongly dependent upon players beliefsplayers probabilities action. compared probabilitymoderated stochastic fictitious play converging payoff dominated solutiongames payoffs described payoff matrix(1, 1) (0, r)R=(17)(r, 0) (10, 10)265fiRezek, Leslie, Reece, Roberts, Rogers, Dash & JenningsProbability Convergence Moderated Fictitious PlayFictitious PlayModerated Play10.90.80.70.60.50.40.30.20.102520151050Risk Parameter (r)Figure 1: Probability convergence (with 95% confidence intervals) fictitious moderated play payoff-dominant risk-dominated equilibrium gamerepresented payoff matrix shown equation 17.factor r determined degree risk dominance action pair 1/1set range r = 1, 2 25.game, clearly best players choose action 2 (sincereceive reward 10). However many learning processes (see Fudenberg & Levine, 1999Young, 1998, example) players see initial actions become convincedopponent playing strategy means choosing action 2 bad (sincepenalty playing 2 1 high). find taking uncertainty strategiesaccount start, use moderated fictitious play algorithm,less likely get stuck playing action 1, convergence strategy (10, 10)likely.value r ran 500 plays game measured convergence ratesmoderated stochastic fictitious play using matching initial conditions. algorithms used smooth best response function. Specifically, Boltzmann smooth bestresponses temperature parameter = 0.1. present results Figure 1. Stochastic fictitious play converges (1,1) equilibrium games action (2,2)risk dominated (i.e. r < 10). soon action (2/2) longer risk dominatedstochastic fictitious play converge. contrast, moderated play exhibits muchsmoother overall convergence characteristic. Moderated play achieves convergence action(2/2) much greater range values r, though varying degrees probability.Thus risk dominated games examined (r = 25 10), moderated playlikely converge payoff-dominant equilibrium stochastic fictitious play.266fiOn Similarities Inference Game Theory Machine LearningThus, using insight machine learning, specifically, standard procedure Bayesian inference integrating distribution opponents strategies,rather taking empirical average, able derive algorithm basedstochastic fictitious play smoother thus predictable convergence behaviour.4. Variational Learningshown first part paper insights machine learning usedderive improved algorithms fictitious play, consider converse case.specifically, consider popular machine learning algorithm, mean field variationallearning algorithm, show viewed learning game playersreceive identical rewards. proceed show insights game theory (specifically,relating variational leaning Cournot process fictitious play) used deriveimproved algorithms.start reviewing mean field variational learning algorithm, first notemethods typically used infer probability distribution latent(or hidden) variables, based evidence provided another set observable variables.Computing probability distributions, however, requires integration stepfrequently intractable (MacKay, 2003). tackle problem one use Markov chainMonte Carlo methods (Robert & Casella, 1999) obtain asymptotically optimal results,alternatively use approximate analytical methods, variational approaches,faster algorithms preferred. Among variational methods, mean field variationalapproach distribution estimation (Jordan et al., 1997) applied real worldproblems ranging bioinformatics (Husmeier, Dybowski, & Roberts, 2004) finiteelement analysis (Liu, Besterfield, & Belytschko, 1988), games.4.1 Mean Field Variational Methodtypical starting point variational learning distributional form model, postulated underlie experimental data generating processes (i.e. generative model).distribution usually instantiated observations, D, definedset latent variables indexed = 1, I. denote domainlatent variable Si , element si Si . Note that, ease exposition latertext, re-use newly define intend make connection earlierdefinition strategy profile. often desire marginal distribution pi (Si )latent variable i, taken set marginal distributions (Si ) .absence detailed knowledge dependence independencevariables, define joint distribution p (S) set distributions S,= S1 S2 SI profile domain latent variables. mathematicalconvenience resort logarithm density(S | D, ) , log (p(S | D, ))(18)parameterise distribution p .Due intractability integrating equation 18 respect S, variationallearning methods (Jordan et al., 1997) approach problem finding distribution,267fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jenningsq (S), minimises criterionZZF = q(s)(s | D, ) + H (q)(19)H() entropy functionH (q) =ZZq(s) log q(s) s.(20)equivalent minimisingD(q||p) =ZZq(s) logq(s)p(s | D, )ds(21)highlights fact variational cost function described equation 19Kullback-Leibler (KL) divergence marginal log-likelihood log (p(D))negative free energy (Jordan et al., 1997).Within variational learning, mean field approach makes assumptionlatent variables independent. Thus, distribution profile, q, latent variablessimplifiesq (si )(22)q(s) ,i=1qi(Si ).basis mean field assumption, optimal distributionvariable one minimises KL divergence equation 21, assumingvariables adopt distribution q (Si ), obtained partialdifferentiation equation 21. model-free update equation q , assumptions (Haft, Hofmann, & Tresp, 1999), takes general formZZ1@i @i@i@iq (s )(s , | D; )q (s ) exp(23)index set @i denotes Markov blanket variable (i.e. set nodesneighbouring node i). 2 player game set consist opponent,graphical game Markov blanket consists players affecting player actions (Kearnset al., 2001; Pearl, 1988).variational algorithm thus proceeds iterating update equation 23KL divergence expressed equation 21 converged stationary value possiblylocal extremum. case EM algorithm, one round updates equation 23interlaced one round updates parameter . update equationsobtained differentiation equation 19 respect .game-theoretic perspective, log-probability shown equation 18, instantiated observations D, interpreted global reward functionr(S | ) log (p(S | D, ))(24)parameterised . latent variables become synonymous players,player strategy space consisting possible values mixed strategy268fiOn Similarities Inference Game Theory Machine Learningq . Interpreted terms players, task probabilistic inference partnershipgame, played players jointly nature (Grunwald & Dawid, 2004). totalexpected rewardZZL() q(s) log (p(s | D, ))(25)given set mixed strategies, simply expected log-likelihood. Bayesiansetting, priors , global reward full log-posterior distributionequivalent total expected reward marginal log-likelihood, evidence. p factorisesoften represented graphical model. Within game-theoretic interpretation, graphical models seen players own, local, reward functions,graphical games (Kearns et al., 2001) active research area implicitlymakes use analogy game theory graphical models.4.2 Fictitious Variational Playvariational mean field algorithm, described previous section, suggestsmixed strategies maximise equation 24 determined iterating equation 23gradually taking limit 0. analogous Cournot adjustment processgame theory literature, modification smooth best responses usedplace best responses2 . However, well known shortcoming Cournot processiteration best response often fail converge and, instead, exhibit cyclicbehaviour. Consider, instance, partnership game reward matrix1 00 1.players commence play choosing different actions, Cournot process failsconverge iterated best responses exhibit cyclic solutions.cyclic behaviour indeed observed variational mean field algorithm.Whilst commonly reported, cycles also occur highly connected graphical models(e.g. Markov Random Fields Coupled Hidden Markov Models), probability distribution approximated really support imposed independenceassumption mean field approach (i.e. random variables strongly insteadweakly connected). Clearly, especially latter case, even random initialisation cannotavoid mean field algorithms problem convergence. phenomenon describedthrashing Wolpert et al.s (2006).Example: simple example illustrates point. Suppose two observations,= {y1 , y2 } y1 = 10 y2 = 10. know observation drawntwo-component mixture one dimensional normal distributions, mixing probability0.5 variances set 1. means normal distributions chosen2. Cournot adjustment process, players use strategies best response actionopponents used previous period. many cases, process converge (Fudenberg &Levine, 1999).269fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jenningsset {10, 10}. Therefore,(1 , 2 | D) = log(0.5((d1 1 ) + (d1 2 )))+ log(0.5((d2 1 ) + (d2 2 )))(yi j ) i, j = 1, 2 denotes density yi j standard normaldistribution. symmetry, clear lsame l(10, 10) = l(10, 10) =log (0) + log (20) 201.8379 ldif f erent l(10, 10) = l(10, 10) = 2 log(0.5((0) +(20))) 3.2242. game theory perspective, means partnership gameplaying simply given 2 2 matrix201.8379 3.2242.3.2242 201.8379choose two q-distributions equation 23 component mean, initialiseq (i = 10) = 0.9 = 1 q (i = 10) component index = {1, 2} (i.e.marginal distribution places weight 0.9 mean 10), update simultaneously, distributions switch virtually mass 10 point.shouldnt surprise, given clearly need 1 6= 2 . problemcomponents mean field approximation jump time. next step,reverse move happen. time things get extreme, continuous cyclingoccurs.work Wolpert et al. (2006) suggested thrashing avoidedadjusting distributions toward best responses, instead setting bestresponses. Here, use analogy game theory rigorously develop approach,prove convergence. start noting fictitious play exactly this;modifies Cournot process adjusting players mixed strategies towards smooth bestresponse, instead setting smooth best response. suggests fictitious playlike modification standard mean field algorithm, best responses computedtime mixed current distributions:(26)qt+1= (1 )qti +F qtwherePt sequence satisfyingRobins-Monro conditions(Bishop, 2006)Pusual2 < ,denotesbest response=,lim=0,qt=1t=1MFfunction distribution given equation 23. call process variationalfictitious play.game theory literature proves stochastic fictitious play partnership gamesguaranteed converge Si finite (Leslie & Collins, 2005). allows us provefollowing result:Theorem: random variable discrete, variational fictitious play converges.Proof: case discrete random variables, equation 26 defines generalised weakenedfictitious play process game player receives reward log(p(s | D, )).270fiOn Similarities Inference Game Theory Machine LearningStandard Mean Field Variational AlgorithmSi1Ep(si = 1)10.80.60.40.20123456Iteration78910910Fictitious Variational Play(0.9p(E |S i1 , ) =0.1i1 6=i1 =p(si = 1)10.80.60.40.2012(a)3456Iteration78(b)Figure 2: Comparison performance standard mean field variational algorithmimproved algorithm applied exemplar binary hidden Markovmodel.process known converge (Leslie & Collins, 2005).Remark: equivalent theorem yet available continuous random variables,similar approach may yield suitable results combined recent work HofbauerSorin (2006).Example: example change fictitious update scheme standardvariational algorithm, consider binary hidden Markov model shown Figure 2a.model contains two latent nodes, indexed 1, jointly parentobserved variable E . latent variables take values i1 , {0, 1}. modelspecified observation best explained (with probability 0.9; see Figure 2b),two neighbouring states take different values. Further, joint prior states 1uniform distribution. simplicity omit parameter , encodesobservation state probabilities prior distribution.Consider initialising distributions q 1 (s1 = 1) q 2 (s2 = 1) close1. result periodic flipping state probability distributions, q 1q 2 , every update iteration mean field algorithm (top graph Figure 2b).contrast, fictitious variational play scheme gradually forces algorithmconverge (bottom graph Figure 2b).271fiRezek, Leslie, Reece, Roberts, Rogers, Dash & JenningsInitialisation random reduces likelihood mean field algorithms failureconverge. However, empirically, could still observe cyclic solutions 50%random starts hidden Markov model training procedure. contrast,modified mean field variational algorithm converges reliably cases.4.3 Derivative Action Variational Algorithmprevious sections shown that, viewed game theoretic perspective, smooth best response function equivalent mean field update ruleand, consequently, able apply results game theory derive improvedvariational learning algorithm. section go use equivalenceincorporate recent development fictitious play, specifically dynamic fictitious play,variational mean field method.dynamic fictitious play (Shamma & Arslan, 2005) standard best response expression, shown equation 7, extended include additional term(27)qti + q .dt| {z }newadditional derivative term acts anticipatory component opponents playexpected increase speed convergence partnership game consideredhere. Note that, convergence, derivative terms become zero fixed pointsexactly standard variational algorithm.Based correspondence best response function fictitious playmodel-free update expression variational learning section 4.1, incorporateadditional differential term update equation variational learning algorithmderive derivative action variational algorithm (DAVA) displays improved convergenceproperties compared standard variational algorithm.illustrate procedure, consider case applying derivative actionvariational algorithm problem performing clustering (i.e. learning labellingmixture distribution order best fit experimental data set). chose clusteringrepresents canonical problem, widely reported literature machine learning.consider standard mixture distribution consisting K one-dimensional Gaussiandistributions, N (d|k , k ) k = 1, . . . , K, givenp(d|) =KXk N (d|k , k )(28)k=1set distribution parameters {1 , 1 , 1 , . , K , , K , K }. Here, k kmean precision Gaussian, k weighting within mixture.usual approach learning distribution (Redner & Walker, 1984) assumeexistence indicator (or labelling) variable, indexed takes values{1, K}, sample, di , formulate complete likelihoodp(S , 1 , . . . , K , 1 , . . . , K | )Kk=1272(S =k)kN (di |k , k )(S=k)p()(29)fiOn Similarities Inference Game Theory Machine Learningp() denote parameter prior distributions. obtain analytic coupled updateequations member parameter set, {1 , 1 , 1 , , K , K , K }, usemodel discussed Uedaa Ghahramani (2002), describes appropriate choiceapproximating marginal posterior distribution parameter. evaluatingintegral equation 23, replacing generic latent variables modelparameters indicator variables S, Uedaa Ghahramani show closedform expression derived approximating marginal posterior distributionsparameters. compute approximate marginal posterior mean distribution,example, set variables @i equation 23 become place holders k{k , , 1 , , N }, respectively. words, compute posterior distributionk , logarithm equation 23 must averaged respect distributions q(k ),q(), q(S ), = 1, N .computations result closed form solution marginal posteriorelement parameter set. Thus, posterior means, k , normal distributionq k , N (k |mk , k )(30)mean mk precision kk ==ck bk dk + 0 0 kPikik = q (S = k)Pdk =kk = ck bk k + 00 0 are, respectively, mean precision Gaussian prior k .posterior precisions Gamma distributionq k , (k |bk , ck )(31)1k + 021Pbk =(d k )2 )+ 12 k k1 +02 k0 0 are, respectively, shape precision parameters Gammaprior precisions k . Finally, posterior mixture weights K-dimensionalDirichlet distributionq , Dir(|)(32)ck ==Pik +00 parameter Dirichlet prior . Finally, distribution component labels si formKbk ck11 (S =k) 122k expk(d mk ) +(33)q (S ) =ZS2kk=1273fiRezek, Leslie, Reece, Roberts, Rogers, Dash & Jenningsnormalising constant ZS computed finite set states .values k k computed using relationsk =(k ) (PKl=1l )k =(ck ) + log(bk ).digamma function.noted earlier, update equations interpreted best responseparticular parameter given values others. Thus, additional derivative termseen dynamic fictitious play can, principle, included variational updateequations 30, 31, 32, 33. desired, however, obtain closed form solutionsmodified best response functions, update equations, derivative termincorporated discrete distribution, given equation 33, follows.notational clarity add anticipatory component estimatemeans, k , use update sk . is, considerkq term equation 33. Furthermore, approximate derivativeinclusion dtkdiscretedifference distributions iterations 1qdtkk.q qtk qt1dt(34)also possible implement smoothed version derivative provide addedrobustness random fluctuations, done equivalent fictitious play algorithm described Shamma Arslan (2005). introduce derivative termequation 33 update equation component labels becomesq (S )Kk=11(S =k) 2kk21bk ck(1 + ) mkt +exp2kt2bk ck1+mkt1 +2kt1(35)parameters qtk denoted mkt kt . differential coefficient, ,allows us control degree differential term contributes toward overallupdate equation.order demonstrate empirically convergence properties derivative actionvariational inference algorithm, compared algorithm using either update equation 33update equation 35 test data. control problem setting, generated synthetictest data drawing data points mixture 3 Gaussian distributionsapplying non-linear mapping function3 . data shown Figure 3 alongoptimal clustering solution.performed 200 runs comparing standard variational algorithm (implementeddescribed Penny & Roberts, 2002) derivative action variational algorithm.run, algorithms started identically randomly initialised3. Equation 35 generalises two dimensions higher replacing quadratic term (di mkt )2inner product (d~i~ kt )T (d~i~ kt ). assumes vector valued data samples, d~i i, Gaussiandistributed means~ k homoscedastic precision k .274fiOn Similarities Inference Game Theory Machine Learning(1)(3)(2)(a)(b)Figure 3: Synthetic data (a) used empirical study convergence propertiesderivative action variational inference algorithm optimal clustering solution (b) showing centres three Gaussians mixture.set model parameters iteration measured value Kullback-Leiblerdivergence equation 21. algorithms terminated 60 iterations (chosenwell number iterations required achieve relative change Kullback-Leiblerdivergence less 105 ). differential coefficient, , equation 35 setconstant value throughout. illustrative purposes chose following valuescoefficient: = 0.5, 1.0, 1.5, 2.0.first consider difference convergence rate standard variationalalgorithm derivative action variational algorithm. Due nature algorithms, different initial conditions lead different final clustering solutions. addition,difference update rule two algorithms means that, although startalgorithms identical initial conditions, necessarily result identical finalclustering solutions. Thus, analyse difference Kullback-Leibler divergenceiteration, runs fact produce identical clustering solutionstermination. Figure 4 compare algorithms case = 0.5.clearly seen, DAVA converges everywhere quickly standard algorithm.experiments also indicate choice differential coefficient, ,significant effect convergence behaviour DAVA. seen Figure 5case = 1.0. Compared results Figure 4, magnitude difference Kullback-Leibler divergence much larger, indicating substantial increaseconvergence rate.comparing times algorithm reached equilibrium (indicatedrelative change Kullback-Leibler divergence less 105 ), four values ,275fiRezek, Leslie, Reece, Roberts, Rogers, Dash & JenningsRelative Performance DerivativeAction Variational Algorithm, =0.500.210th Percentile50th Percentile90th PercentileDifference KL divergence0.150.10.0500.050102030405060IterationFigure 4: comparison standard derivative action variational algorithm( = 0.5), Kullback-Leibler divergence values, obtained algorithmsevery iteration, compared. Shown estimates 10th ,50th (the median) 90th percentiles differences standardKL derivative action KL, i.e. KLstandard (t) KLDAV (t) iterationt. positive value suggests current solution standard algorithmworse derivative action algorithm. zero value impliessolutions found algorithms identical. initialisation KLdifferences zero algorithms identical initialisation conditions.role differential coefficient improving convergence rates becomes apparent.particular, Table 1 shows relative convergence rate improvement shown derivative action variational algorithm, compared standard variational algorithm.results indicate median improvement much 28% = 2.0.addition, value differential coefficient increases, variance convergencerate increases, thus, widening gap best worst performanceimprovements.However, increasing variance imply DAVA converging inferiorsolutions. seen Figure 6 shows analysis quality solutionsreached algorithm, 200 runs (not algorithms convergedclustering solution). moderate values ( 1.5) derivative componentassist finding better solution. indicated proportion positive KLdivergence differences (in Figure 6). Positive values imply standard algorithm oftenfinds worse solutions compared DAVA particular setting . Increasing value276fiOn Similarities Inference Game Theory Machine LearningRelative Performance DerivativeAction Variational Algorithm, =1.000.210th Percentile50th Percentile90th PercentileDifference KL divergence0.150.10.0500.050102030405060IterationFigure 5: Estimates 10th , 50th (the median) 90th percentiles differencesKullback-Leibler divergence values, iteration, 200 runsstandard derivative action variational algorithm ( = 1.0).differential coefficient increases variance clustering solutionsgenerated algorithm.evaluated performance DAVA algorithm synthetic data set,investigated performance range values derivative coefficient, ,consider realistic experimental setting apply algorithm medical magneticresonance data (see Figure 7). data consists 100100 pixel region slicetumour patients brain. Data collected using T2 proton density (PD) spinsequences (shown Figure 7a), used directly form two-dimensional featurespace.10 component Gaussian mixture model fitted data space, before,use DAVA algorithm derived earlier learn label appropriate mixturedistribution. Following synthetic data experiments, DAVA derivative coefficientset 1.0. best compromise speed robustnessalgorithm. let algorithms run 100 iterations measured KL divergenceiteration monitor convergence. Figure 7b shows resulting segmentationDAVA algorithm.dataset, algorithm converged K = 5 classes; identical Markov ChainMonte Carlo clustering reported work Roberts, Holmes, Denison (2001).segmentation clearly shows spatial structure incorporated segmen277fiRezek, Leslie, Reece, Roberts, Rogers, Dash & JenningsDifference final KL divergences StandardDerivative Action Variational Algorithm1=0.5=1.0=1.5=2.00.90.8Probability0.70.60.50.40.30.20.10<00>0Sign final differences (StandardDerivative Action)Figure 6: Relative difference Kullback-Leibler divergence values equilibriumstandard derivative action variational algorithms derivative coefficients: = 0.5, 1.0, 1.5, 2.0.0.51.01.52.010th percentile0%0%-2%8%50th percentile9%19 %22%28%90th percentile21 %39 %46 %53 %Table 1: Relative convergence rate improvement derivation action variational algorithm standard variational algorithms.tation process priori, algorithm approximately 1.5 times fasterstandard segmentation algorithm4 .summary, adding derivative term variational learning algorithm,produced algorithm that, empirical studies, shows improved convergence rates compared standard variational algorithm. Indeed, convergence rate improvementachieved applying additional derivative response term mean componentsmixture model parameters only, and, thus, believe improvementpossible parameters treated similarly.4. determine factor speedup two algorithms averaged KL divergence valuesevery iteration segmentation algorithm. Based mean KL divergence curveiteration number algorithm converged could calculated. iterationrelative change KL divergence 2 successive iterations less 104 . ratioconvergence points DAVA standard algorithm produced indicator speed.278fiOn Similarities Inference Game Theory Machine LearningSlice T2 magnetic resonance image tumour patients brain.(a)proton density image corresponding T2 MR image.(b)Figure 7: results segmentation using DAVA algorithm = 1.0.data segmentation obtained approximately half time tookstandard segmentation algorithm.5. Conclusions Future Workwork shown equivalence game-theoretical learning partnership games variational learning algorithm common machine learning.comparison, probabilistic inference using mean field variational learning seenCournot adjustment process partnership game. Likewise, smooth best responsefunction used players plug-in single layer classifier sigmoid activation function.exploiting analysis insights derived it, able showinsights one area may applied derive improved fictitious playvariational learning algorithms. empirical comparisons, algorithms showedimproved convergence properties compared standard counterparts.279fiRezek, Leslie, Reece, Roberts, Rogers, Dash & JenningsPlayer 1s Estimate Opponent Strategy1C0.80.60.40.20050Iterations100150Expected Rewards5Player 1Player 243210050Iterations100150Figure 8: Using dynamic logistic regression (DLR) adaptive estimation opponentmixed strategies. game repeated prisoners dilemma one player changesstrategies always co-operate always defect midway game.Using DLR model learning, player adapts changes baseduncertainty predicted observed opponent strategies.believe initial results particularly exciting. Thus, whilst still remainsanalysis performed (specifically, would like prove convergence moderated fictitious play compare performance variational algorithms large realworld data sets), work clearly shows value studying intersection machinelearning game theory.One conclusion work almost machine learning algorithm putuse model players learning other. Consider, instance adaptive classification algorithms capable adapting changes learners environment.game-theoretic methods tuned toward achieving equilibrium estimating,example, Nash mixed strategy profiles. desirable stationary environments,algorithms fail adapt non-stationary environments, consequently little practical use dynamic real-world applications. research presented paperstrongly suggests adaptive classifiers might prove useful.proof concept, implemented dynamic logistic regression (DLR),presented Penny Roberts (1999), model play one player (player 1)game repeated prisoners dilemma. player (player 2) set play alwaysco-operate strategy first 75 rounds play. second set 75 rounds, player 2set play always defect strategy. task player 1 detect changeopponents behaviour compute best responses according updated estimateplayer 2s strategy. estimates player 1 player 2s strategy entire gameshown Figure 8, together players expected rewards. DLR adaptively280fiOn Similarities Inference Game Theory Machine Learningchanges one-step ahead predicted opponent strategy basis uncertaintyresults incorporation recently observed opponent action (see Penny& Roberts, 1999 details; observations considered work map directlyobserved actions made opponent). decision action play followsusual (i.e. compute best response according updated estimate usingequation 9).two things would like point out. First, implemented here, inputrequired DLR simply recently observed opponent action, decision madeDLR action drawn best response function. However, DLR also allowsvector inputs, consequence, players made respondopponents actions, also context application specific variables. Second,DLR estimation described Penny Roberts (1999) fully Bayesian. Missing datanaturally embedded Bayesian estimation demonstrated Lowne,Haw, Roberts (2006). Mapping fact back use DLR model playimplies players keep track opponents behaviour without need followobserve every move. Missing observations, instance, could result increaseduncertainty opponents predicted play and, within, reversal toward appropriatebest response (as might existed onset play).Similar ideas using dynamic, instead static, estimators opponent strategyrecently presented work Smyrnakis Leslie (2008). Extendinguse machine learning techniques allow dynamic estimation strategiesenvironmental parameters allow game theoretical learning become generallyapplicable real-world scenarios.Acknowledgmentsauthors would like thank reviewers, whose suggestions led significant improvements content clarity final paper. research undertakenpart ARGUS II DARP ALADDIN projects. ARGUS II DARP (DefenceAerospace Research Partnership) collaborative project involving BAE SYSTEMS,QinetiQ, Rolls-Royce, Oxford University Southampton University, fundedindustrial partners together EPSRC, MoD DTI. ALADDIN (AutonomousLearning Agents Decentralised Data Information Systems) jointly fundedBAE Systems EPSRC (Engineering Physical Science Research Council) strategicpartnership (EP/C548051/1).ReferencesBernardo, J. M., & Smith, A. F. M. (1994). Bayesian Theory. John Wiley Sons.Bishop, C. M. (2006). Pattern Recognition Machine Learning. Oxford University Press,Oxford.Demiriz, A., Bennett, K. P., & Shawe-Taylor, J. (2002). Linear programming boosting viacolumn generation. Machine Learning, 46 (13), 225254.281fiRezek, Leslie, Reece, Roberts, Rogers, Dash & JenningsFreund, Y., & Schapire, R. E. (1997). decision-theoretic generalization on-line learningapplication boosting. Journal Computer System Sciences, 55 (1),119139.Fudenberg, D., & Levine, D. K. (1999). Theory Learning Games. MIT Press.Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2000). Bayesian Data Analysis.Chapman & Hall/CRC.Greenwald, A., & Littman, M. L. (2007). Introduction special issue learningcomputational game theory. Machine Learning, 67 (1-2), 36.Grunwald, P. D., & Dawid, A. P. (2004). Game Theory, Maximum Entropy, MinimumDiscrepancy Robust Bayesian Decision Theory. Annals Statistics, 32, 13671433.Haft, M., Hofmann, R., & Tresp, V. (1999). Model-Independent Mean Field TheoryLocal Method Approximate Propagation Information. Computation NeuralSystems, 10, 93105.Hofbauer, J., & Hopkins, E. (2005). Learning perturbed asymmetric games. GamesEconomic Behavior, 52, 133157.Hofbauer, J., & Sorin, S. (2006). Best response dynamics continuous zero-sum games.Discrete Continuous Dynamical Systems, B6, 215224.Husmeier, D., Dybowski, R., & Roberts, S. J. (Eds.). (2004). Probabilistic ModelingBioinformatics Medical Informatics. Advanced Information Knowledge Processing. Springer Verlag.Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1997). IntroductionVariational Methods Graphical Models. Jordan, M. I. (Ed.), LearningGraphical Models. Kluwer Academic Press.Kalai, E., & Lehrer, E. (1993). Rational Learning Leads Nash Equilibrium. Econometrica,61 (5), 10191045.Kearns, M., Littman, M. L., & Singh, S. (2001). Graphical Models Game Theory.Proceedings Seventeenth Conference Uncertainty Artificial Intelligence,pp. 253260.Lambert, T., Epelman, M. A., & Smith, R. L. (2005). Fictitious Play ApproachLarge-Scale Optimization. Operations Research, 53 (3), 477489.Lee, C. F., & Wolpert, D. H. (2004). Product Distribution Theory Control Multi-AgentSystems. AAMAS 04: Proceedings Third International Joint ConferenceAutonomous Agents Multiagent Systems, pp. 522529, New York, USA.Leslie, D. S., & Collins, E. J. (2005). Generalised weakened fictitious play. GamesEconomic Behavior, 56 (2), 285298.Liu, W., Besterfield, G., & Belytschko, T. (1988). Variational approach probabilisticfinite elements. Journal Engineering Mechanics, 114 (12), 21152133.Lowne, D., Haw, C., & Roberts, S. (2006). adaptive, sparse-feedback EEG classifierself-paced BCI. Proceedings Third International Workshop BrainComputer Interfaces, Graz, Austria.282fiOn Similarities Inference Game Theory Machine LearningMacKay, D. J. C. (1992). Evidence Framework Applied Classification Networks.Neural Computation, 4 (5), 720736.MacKay, D. J. C. (2003). Information Theory, Inference, Learning Algorithms. Cambridge University Press.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann Publishers.Penny, W., & Roberts, S. (1999). Dynamic logistic regression. Proceedings International Joint Conference Neural Networks (IJCNN99), Vol. 3, pp. 1562 1567.Penny, W., & Roberts, S. (2002). Bayesian Multivariate Autoregressive Models Structured Priors. IEE Proceedings Vision, Signal Image Processing, 149 (1), 3341.Redner, R. A., & Walker, H. F. (1984). Mixture Densities, Maximum LikelihoodEM Algorithm. SIAM Review, 26 (2), 195239.Ripley, B. (2000). Pattern Recognition Neural Networks. Cambridge University Press.Robert, C. P., & Casella, G. (1999). Monte Carlo Statistical Methods. Springer-Verlag: NewYork.Roberts, S., Holmes, C., & Denison, D. (2001). Minimum Entropy data partitioning usingReversible Jump Markov Chain Monte Carlo. IEEE Transactions Pattern AnalysisMachine Intelligence, 23 (8), 909915.Shamma, J. S., & Arslan, G. (2005). Dynamic Fictitious Play, Dynamic Gradient PlayDistributed Convergence Nash Equilibria. IEEE Transactions AutomaticControl, 50 (3), 312327.Shoham, Y., Powers, R., & Grenager, T. (2007). multi-agent learning answer,question. Artificial Intelligence, 171 (7), 365377.Smyrnakis, M., & Leslie, D. (2008). Stochastic Fictitious Play using particle Filtersupdate beliefs opponents strategies. Proceedings First InternationalWorkshop Optimisation Multi-Agent Systems (OPTMAS) Seventh International Conference Autonomous Agents Multiagent Systems (AAMAS 2008).Uedaa, N., & Ghahramani, Z. (2002). Bayesian model search mixture models basedoptimizing variational bounds. Neural Networks, 15, 12231241.van der Genugten, B. (2000). Weakened Form Fictitious Play Two-Person Zero-SumGames. International Game Theory Review, 2 (4), 307328.Wolpert, D. H. (2004). Information Theory - Bridge Connecting Bounded RationalGame Theory Statistical Physics. arXiv.org:cond-mat/0402508.Wolpert, D., Strauss, C., & Rajnarayan, D. (2006). Advances Distributed OptimizationUsing Probability Collectives. Advances Complex Systems, 9 (4), 383436.Young, H. P. (1998). Individual Strategy Social Structure: Evolutionary TheoryInstitutions. Princeton University Press.283fiJournal Artificial Intelligence Research 33 (2008) 109-147Submitted 11/07; published 9/08Networks Influence Diagrams: FormalismRepresenting Agents Beliefs Decision-Making ProcessesYaakov GalMIT Computer Science Artificial Intelligence LaboratoryHarvard School Engineering Applied SciencesAvi Pfeffergal@csail.mit.eduavi@eecs.harvard.eduHarvard School Engineering Applied SciencesAbstractpaper presents Networks Influence Diagrams (NID), compact, naturalhighly expressive language reasoning agents beliefs decision-making processes. NIDs graphical structures agents mental models representednodes network; mental model agent may use descriptions mentalmodels agents. NIDs demonstrated examples, showing useddescribe conflicting cyclic belief structures, certain forms bounded rationality. opponent modeling domain, NIDs able outperform computationalagents whose strategies known advance. NIDs equivalent representationBayesian games compact structured formalism. particular, equilibrium definition NIDs makes explicit distinction agentsoptimal strategies, actually behave reality.1. Introductionrecent years, decision theory game theory profound impact designintelligent systems. Decision theory provides mathematical language single-agentdecision-making uncertainty, whereas game theory extends language multiagent case. fundamental level, approaches provide definition meansbuild intelligent agent, equating intelligence utility maximization. Meanwhile, graphical languages Bayesian networks (Pearl, 1988) received muchattention AI allow compact natural representation uncertaintymany domains exhibit structure. formalisms often lead significant savingsrepresentation inference time (Dechter, 1999; Cowell, Lauritzen, & Spiegelhater,2005).Recently, wide variety representations algorithms augmented graphicallanguages able represent reason agents decision-making processes.single-agent case, influence diagrams (Howard & Matheson, 1984) able representsolve agents decision making problem using principles decision theory.representation extended multi-agent case, decision problemssolved within game-theoretic framework (Koller & Milch, 2001; Kearns, Littman, &Singh, 2001).focus AI far classical, normative approach decisiongame theory. classical approach, game specifies actions availableagents, well utilities associated possible set agentsc!2008AI Access Foundation. rights reserved.fiGal & Pfefferactions. game analyzed determine rational strategies agents.Fundamental approach assumptions structure game, includingagents utilities actions, known agents, agents beliefsgame consistent correct, agents reason gameway, agents rational choose strategymaximizes expected utility given beliefs.systems involving multiple, autonomous agents become ubiquitous, increasingly deployed open environments comprising human decision makers computeragents designed represent different individuals organizations. Examplessystems include on-line auctions, patient care-delivery systems (MacKie-Mason,Osepayshivili, Reeves, & Wellman, 2004; Arunachalam & Sadeh, 2005). settingschallenging assumptions made decision-making strategiesparticipants open environments. Agents may uncertain structuregame beliefs agents structure game; may useheuristics make decisions may deviate optimal strategies (Camerer,2003; Gal & Pfeffer, 2003b; Rajarshi, Hanson, Kephart, & Tesauro, 2001).succeed environments, agents need make clear distinctiondecision-making models, models others may using make decisions,extent agents deviate models actually make decisions.paper contributes language, called Networks Influence Diagrams (NID), makesexplicit different mental models agents may use make decisions. NIDs provideclear compact representation reason agents beliefsdecision-making processes. allows multiple possible mental models deliberationagents, uncertainty models agents using. recursive,mental model agent may contain models mental modelsagents, associated uncertainty. addition, NIDs allow agents beliefs form cyclicstructures, form, believe believe believe,..., cycleexplicitly represented language. NIDs also describe agents conflicting beliefsother. example, one describe scenario two agents disagreebeliefs behavior third agent.NIDs graphical language whose building blocks Multi Agent Influence Diagrams(MAID) (Koller & Milch, 2001). mental model NID represented MAID,models connected (possibly cyclic) graph. NID convertedequivalent MAID represent subjective beliefs agent game.provide equilibrium definition NIDs combines normative aspectsdecision-making (what agents do) descriptive aspects decision-making(what agents expected do). equilibrium makes explicit distinctiontwo types strategies: Optimal strategies represent agents best course action givenbeliefs others. Descriptive strategies represent agents may deviateoptimal strategy. classical approach game theory, normative aspect (whatagents do) descriptive aspect (what analysts agents expectdo), coincided. Identification two aspects makes sense agentbetter optimize decisions relative model world. However,open environments, important consider possibility agent deviatingrational strategy respect model.110fiNetworks Influence DiagramsNIDs share relationship Bayesian game formalism, commonly used modeluncertainty agents payoffs economics (Harsanyi, 1967). formalism,type possible payoff function agent may using. Although NIDsrepresentationally equivalent Bayesian games, argue compact,succinct natural representation. Bayesian game converted NIDlinear time. NID converted Bayesian game, size Bayesiangame may exponential size NID.paper revised expanded version previous work (Gal & Pfeffer, 2003a,2003b, 2004), organized follows: Section 2 presents syntax NID language,shows build MAIDs order express structure holdsagents beliefs. Section 3 presents semantics NIDs terms MAIDs, providesequilibrium definition NIDs. Section 4 provides series examples illustratingrepresentational benefits NIDs. shows agents construct belief hierarchiesothers decision-making order represent agents conflicting incorrect beliefstructures, cyclic belief structures opponent modeling. also shows certain formsbounded rationality modeled making distinction agents modelsdeliberation way behave reality. Section 5 demonstrates NIDs modelbelieve believe type reasoning practice. describes NID ableoutperform top programs submitted competition automatic rockpaper-scissors players, whose strategy known advance. Section 6 compares NIDsseveral existing formalisms describing uncertainty decision-making processes.provides linear time algorithm converting Bayesian games NIDs. Finally, Section 7concludes presents future work.2. NID Syntaxbuilding blocks NIDs Bayesian networks (Pearl, 1988), Multi Agent InfluenceDiagrams (Koller & Milch, 2001). Bayesian network directed acyclic graphnode represents random variable. edge two nodes X1 X2 impliesX1 direct influence value X2 . Let Pa(Xi ) represent set parentnodes Xi network. node Xi contains conditional probability distribution(CPD) domain value parents, denoted P (Xi | Pa(Xi )). topologynetwork describes conditional independence relationships hold domainevery node network conditionally independent non-descendants givenparent nodes. Bayesian network defines complete joint probability distributionrandom variables decomposed product conditional probabilitiesnode given parent nodes. Formally,P (X1 , . . . , Xn ) =n!i=1P (Xi | Pa(Xi ))illustrate Bayesian networks following example.Example 2.1. Consider two baseball team managers Alice Bob whose teams playing late innings game. Alice, whose team hitting, attempt advance runnerinstructing steal base next pitch delivered. successful111fiGal & Pfeffersteal result benefit hitting team loss pitching team, mayresult runner thrown out, incurring large cost hitting teambenefit pitching team. Bob, whose team pitching, instruct team throwpitch out, thereby increasing probability stealing runner thrown out.However, throwing pitch incurs cost pitching team. decisions whethersteal pitch taken simultaneously team managers. Supposegame tied, either Alices Bobs team leading score, identityleading team known Alice Bob make decision.Suppose Alice Bob using pre-specified strategies make decisionsdescribed follows: Alice leading, instructs steal probability 0.75,Bob calls pitch probability 0.90; Alice leading, instructssteal probability 0.65, Bob calls pitch probability 0.50.six random variables domain: Steal PitchOut represent decisions AliceBob; ThrownOut represents whether runner thrown out; Leader representsidentity leading team; Alice Bob represent utility functions Alice Bob.Figure 1 shows Bayesian network scenario.LeaderStealPitchOutThrownOutAliceBobFigure 1: Bayesian network Baseball Scenario (Example 2.1)CPD associated node network represents probability distribution domain value parents. CPDs nodes Leader, Steal,PitchOut, ThrownOut Bayesian network shown Table 1. example,CPD ThrownOut, shown Table 1d, represents conditional probability distributionP (ThrownOut | Steal, PitchOut). According CPD, Alice instructs runnersteal base 80% chance get thrown Bob calls pitch 60%chance get thrown Bob remains idle. nodes Alice Bob deterministic CPDs, assigning utility agent joint value parent nodes Leader,Steal, PitchOut ThrownOut. utility Alice shown Table 2. utilityBob symmetric assigns negative value assigned Alices utilityvalue parent nodes. example, Alice leading, instructs runnersteal base, Bob instructs pitch out, runner thrown out, Alice incursutility 60, Bob incurs utility 60.11. Note Alice instruct steal base, runner cannot thrown out, utilityagents defined case.112fiNetworks Influence DiagramsLeaderalice bob none0.40.30.3(a) node LeaderLeaderalicebobPitchOuttrue f alse0.90 0.100.50 0.50(c) node PitchOutLeaderalicebobStealtrue f alse0.75 0.250.65 0.35(b) node StealStealtruetruef alsef alsePitchOuttruef alsetruef alseThrownOuttrue f alse0.80.20.60.40101(d) node ThrownOutTable 1: Conditional Probability Tables (CPDs) Bayesian network BaseballScenario (Example 2.1)LeaderalicealicealicealicealicealicealicealicebobbobbobbobbobbobbobbobStealtruetruetruetruef alsef alsef alsef alsetruetruetruetruef alsef alsef alsef alsePitchOuttruetruef alsef alsetruetruef alsef alsetruetruef alsef alsetruetruef alsef alseThrownOuttruef alsetruef alsetruef alsetruef alsetruef alsetruef alsetruef alsetruef alseAlice601108011010090110100110200Table 2: Alices utility (Example 2.1) (Bobs utility symmetric, assigns negative valueAlices value).113fiGal & Pfeffer2.1 Multi-agent Influence DiagramsBayesian networks used specify agents play specific strategies,capture fact agents free choose strategies, cannotanalyzed compute optimal strategies agents. Multi-agent Influence Diagrams(MAID), address issues extending Bayesian networks strategic situations,agents must choose values decisions maximize utilities, contingentfact agents choosing values decisions maximizeutilities. MAID consists directed graph three types nodes: Chancenodes, drawn ovals, represent choices nature, Bayesian networks. Decision nodes,drawn rectangles, represent choices made agents. Utility nodes, drawn diamonds,represent agents utility functions. decision utility node MAID associatedparticular agent. two kinds edges MAID: Edges leading chanceutility nodes represent probabilistic dependence, manner edgesBayesian network. Edges leading decision nodes represent information availableagents time decision made. domain decision node representschoices available agent making decision. parents decisionnodes called informational parents. total ordering agents decisions,earlier decisions informational parents always informational parentslater decisions. assumption known perfect recall forgetting. CPDchance node specifies probability distribution domain valueparent nodes, Bayesian networks. CPD utility node represents deterministicfunction assigns probability 1 utility incurred agent valueparent nodes.MAID, strategy decision node Di maps value informational parents,denoted pai , choice Di . Let Ci domain Di . choice decisionvalue Ci . pure strategy Di maps value informationalparents action ci Ci . mixed strategy Di maps value informationalparents distribution Ci . Agent free choose mixed strategy Dimakes decision. strategy profile set decisions MAID consistsstrategies specifying complete plan action decisions set.MAID Example 2.1 shown Figure 2. decision nodes Steal PitchOutrepresent Alices Bobs decisions, nodes Alice Bob represent utilities.CPDs chance node Leader ThrownOut described Tables 1a1d.MAID definition specify strategies decisions. need computed assigned process. strategy exists decision, relevantdecision node MAID converted chance node follows strategy.chance node domain parent nodes domain informational parents decision node MAID. CPD chance nodeequal strategy decision. say chance node Bayesiannetwork implements strategy MAID. Bayesian network represents completestrategy profile MAID strategy decision MAID implementedrelevant chance node Bayesian network. say Bayesian networkimplements strategy profile. Let represent strategy profile implements114fiNetworks Influence DiagramsLeaderPitchOutStealThrownOutAliceBobFigure 2: MAID Baseball Scenario (Example 2.1)decisions MAID. distribution defined Bayesian network denotedP .agents utility function specified aggregate individual utilities;sum utilities incurred agent utility nodes associatedagent.Definition 2.2. Let E set observed nodes MAID representing evidenceavailable let strategy profile decisions. Let U() setutility nodes belonging . expected utility given E defined"""U ( | E) =E [U | E] =P (u | E) uU U()U U() uDom(U )Solving MAID requires computing optimal strategy profile decisions,specified Nash equilibrium MAID, defined follows.Definition 2.3. strategy profile decisions MAID Nash equilibriumstrategy component decision Di belonging agent MAID onemaximizes utility achieved agent, given strategy decisions.argmax U $i ,i % ()(1)Siequilibrium strategies specify agent decision givenavailable information decision. MAID contains several sequential decisions,no-forgetting assumption implies decisions taken sequentiallyagent, previous decisions available observations agent reasonsfuture decisions.MAID least one Nash equilibrium. Exact approximate algorithmsproposed solving MAIDs efficiently, way utilizes structurenetwork (Koller & Milch, 2001; Vickrey & Koller, 2002; Koller, Meggido, & von Stengel,115fiGal & Pfeffer1996; Blum, Shelton, & Koller, 2006). Exact algorithms solving MAIDs decomposeMAID graph subsets interrelated sub-games, proceed find setequilibria sub-games together constitute global equilibrium entiregame. case multiple Nash equilibria, algorithms select onethem, arbitrarily. MAID Figure 2 single Nash equilibrium,obtain solving MAID: Alice leading, instructs runner steal baseprobability 0.2, remain idle probability 0.8, Bob calls pitchprobability 0.3, remains idle probability 0.7. Bob leading, Alice instructssteal probability 0.8, Bob calls pitch probability 0.5.Bayesian network implements Nash equilibrium strategy profileMAID queried predict likelihood interesting events. example,query network Figure 2 find probability stealer get thrownout, given agents strategies follow Nash equilibrium strategy profile, 0.57.MAID converted extensive form game decision treevertex associated particular agent nature. Splits tree representassignment values chance decision nodes MAID; leaves treerepresent end decision-making process, labeled utilities incurredagents given decisions chance node values instantiated alongedges path leading leaf. Agents imperfect information regarding actionsothers represented set vertices cannot tell apart makeparticular decision. set referred information set. Let decisionMAID belonging agent . one-to-one correspondence valuesinformational parents MAID information sets verticesrepresenting move decision D.2.2 Networks Influence Diagramsmotivate NIDs, consider following extension Example 2.1.Example 2.4. Suppose experts influence whether teamsteal pitch out. social pressure managers follow adviceexperts, managers decision turns wrong assign blameexperts. experts suggest Alice call steal, Bob call pitchout. advice common knowledge managers. Bob may uncertainwhether Alice fact follow experts steal, whether ignoreplay best-response respect beliefs Bob. quantify, Bob believesprobability 0.7, Alice follow experts, probability 0.3, Alice playbest-response. Alices beliefs Bob symmetric Bobs beliefs Alice:probability 0.7 Alice believes Bob follow experts call pitch out,probability 0.3 Alice believes Bob play best-response strategy respectbeliefs Alice. probability distribution variables exampleremains shown Table 1.NIDs build top MAIDs explicitly represent structure. Network Influence Diagrams (NID) directed, possibly cyclic graph, node MAID.avoid confusion internal nodes MAID, call nodes NIDblocks. Let decision belonging agent block K, let agent. (In116fiNetworks Influence Diagramsparticular, may agent itself.) introduce new type node, denoted Mod[, D]values range block L NID. Mod[, D] takes value L,say agent block K modeling agent using block L make decision D.means believes may using strategy computed block L makedecision D. duration paper, refer node Mod[, D] Modnode agent decision clear context.Mod node chance node like other; may influence, influencednodes K. required parent decisioninformational parent decision. agents strategyspecify value Mod node. Every decision Mod[, D]node agent makes decision block K, including agent ownsdecision. CPD Mod[, D] assigns positive probability block L,require exists block L either decision node chance node.chance node L, means believes agent playing like automatonL, using fixed, possibly mixed strategy D; decision node L, meansbelieves analyzing block L determine course action D. presentationpurposes, also add edge K L NID, labeled {, D}.LeaderMod[Alice, Steal]Mod[Bob, PitchOut]Mod[Bob, Steal]Mod[Alice, PitchOut]StealThrownOutPitchOutAliceBob(a) Top-level BlockLeaderLeaderTop-levelStealBob,STEAL Alice,PITCHOUTPitchOut(b) block(c) block PP(d) Baseball NIDFigure 3: Baseball Scenario (Example 2.1)represent Example 2.4 NID described Figure 3. three blocksNID. Top-level block, shown Figure 3a, corresponds interactionAlice Bob free choose whether steal base call pitch out,respectively. block identical MAID Figure 2, except decision nodeincludes Mod nodes agents. Block S, presented Figure 3b, correspondssituation Alice follows expert recommendation instructs player steal.117fiGal & PfefferMod[Bob, Steal]Top-level0.30.7(a) nodeMod[Bob, Steal]Mod[Alice, PitchOut]Top-levelP0.30.7(b) nodeMod[Alice, PitchOut]Mod[Bob, PitchOut]Top-level1(c) nodeMod[Bob, PitchOut]Mod[Alice, Steal]Top-level1(d) nodeMod[Alice, Steal]Table 3: CPDs Top-level block NID Baseball Scenario (Example 2.1)block, Steal decision replaced chance node, assigns probability1 true value informational parent Leader. Similarly, block P, presentedFigure 3c, corresponds situation Bob instructs team pitch out.block, PitchOut decision replaced chance node, assigns probability 1true value informational parent Leader.root NID Top-level block, example corresponds reality.Mod nodes Top-level block capture agents beliefs decision-makingprocesses. node Mod[Bob, Steal] represents Bobs belief block Aliceusing make decision Steal. CPD assigns probability 0.3 Top-level block,0.7 block S. Similarly, node Mod[Alice, PitchOut] represents Alices beliefsblock Bob using make decision PitchOut. CPD assigns probability 0.3Top-level block, 0.7 block P. shown Table 3.important aspect NIDs allow agents express uncertaintyblock using make decisions. node Mod[Alice, Steal]Top-level block represents Alices beliefs block Alice usingmake decision Steal. example, CPD node assigns probability 1Top-level. Similarly, node Mod[Bob, PitchOut] represents Bobs beliefsblock using make decision PitchOut, assigns probability 1 Top-levelblock. Thus, example, Bob Alice uncertain blockagent using make decision, block using.However, could also envision situation agent unsuredecision-making. say Mod[, D] block K equals block L $= K,owns decision D, agent modeling using block L make decisionD. Section 3.2 show allows capture interesting forms boundedrational behavior. impose requirement exists cycleedge includes label {, D}. words, cycle agentmodeling edge. cycle called self-loop. MAIDrepresentation NID self-loop include cycle nodes representingagents beliefs block NID.future examples, use following convention: exists Mod[, D] nodeblock K (regardless whether owns decision) CPD Mod[, D] assignsprobability 1 block K, omit node Mod[, D] block description.Top-level block Figure 3a, means nodes Mod[Alice, Steal]Mod[Bob, PitchOut], currently appearing dashed ovals, omitted.118fiNetworks Influence Diagrams3. NID Semanticssection provide semantics NIDs terms MAIDs. first showNID converted MAID. define NID equilibrium terms Nashequilibrium constructed MAID.3.1 Conversion MAIDsfollowing process converts block K NID MAID fragment OK ,connects form MAID representation NID. key construct processuse chance node DK MAID represent beliefs agent regardingaction chosen decision block K. value depends blockused model decision D, determined value Mod[, D] node.1. block K NID, create MAID OK . chance utility node Nblock K descendant decision node K replicated OK ,agent , denoted NK . N descendant decision node K,copied OK denoted N K . case, set NK = N K agent .2. P parent N K, PK made parent NK OK . CPDNK OK equal CPD N K.3. decision K, create decision node BR[D]K OK , representingoptimal action decision. N chance decision nodeinformational parent K, belongs agent , NK madeinformational parent BR[D]K OK .4. create chance node DK OK agent . make Mod[, D]K parentDK . decision belongs agent , make BR[D]K parent DK .decision belongs agent $= , make DK parent DK .5. assemble MAID fragments OK single MAID follows: addedge DL DK L $= K L assigned positive probability Mod[, D]K ,owns decision D. Note may agent, including itself.6. set CPD DK multiplexer. owns CPD DK assignsprobability 1 BR[D]K Mod[, D]K equals K, assigns probability 1DL Mod[, D]K equals L $= K. $= owns CPD DK assignsprobability 1 DK Mod[, D]K equals K, assigns probability 1 DLMod[, D]K equals L $= K.explain, Step 1 process creates MAID fragment OK NID block.nodes ancestors decision nodes representing events occur priordecisions copied OK . However, events occur decisions taken maydepend actions decisions. Every agent NID may beliefsactions events follow them, regardless whether agent ownsdecision. Therefore, descendant nodes decisions duplicated agentOK . Step 2 ensures two nodes connected original block K,119fiGal & Pfeffernodes representing agents beliefs OK also connected. Step 3 creates decisionnode OK decision node block K belonging agent . informationalparents decision OK nodes represent beliefsinformational parents K. Step 4 creates separate chance node OK agentrepresents belief decisions K. owns decision, nodedepends decision node belonging . Otherwise, node depends beliefsregarding action agent owns decision. case modelsusing different block make decisions, Step 5 connects MAID fragmentsblock. Step 6 determines CPDs nodes representing agents beliefsothers decisions. CPD ensures block used model decisiondetermined value Mod node. MAID obtained resultprocess complete description agents beliefs others decisions.demonstrate process converting NID Example 2.4 MAID representation, shown Figure 4. First, MAID fragments three blocks Top-level, P,created. node Leader appearing blocks Top-level, P, descendant decision. Following Step 1, created MAID fragments,giving nodes LeaderT L , LeaderP LeaderS . Similarly, node Steal blocknode PitchOut block P created MAID fragment, giving nodesStealS PitchOutP . Also Step 1, nodes Mod[Alice, Steal]T L , Mod[Bob, Steal]T L ,Mod[Alice, PitchOut]T L Mod[Bob, PitchOut]T L added MAID fragmentTop-level block.Step 3 adds decision nodes BRT L [Steal] BRT L [PitchOut] MAID fragmentLLL, PitchOutTAlice, StealTAliceTop-level block. Step 4 adds chance nodes PitchOutTBobTLStealBob MAID fragment Top-level block. nodes represent agentsbeliefs block decisions decisions agents. exLrepresents Bobs beliefs decision whether pitch out,ample, PitchOutTBobTLPitchOutAlice represents Alices beliefs Bobs beliefs decision. Also followLLLStealTAliceStealTBobaddeding Step 4, edges BRT L [PitchOut] PitchOutTBobMAID fragment Top-level block. represent Bobs beliefs decisionLLStealTBobadded MAID fragment representblock. edge StealTAliceBobs beliefs Alices decision Top-level block. also nodes representingAlices beliefs Bobs decisions block.LLPitchOutP PitchoutTAliceaddedStep 5, edges StealS StealTBobMAID fragment Top-level block. allow Bob reason Alices decisionblock S, Alice reason Bobs decision block P. action unifiesLMAID fragments single MAID. parents StealTBobMod[Bob, Steal]T L , StealSLStealTAlice. CPD multiplexer node determines Bobs prediction Alicesaction: Mod[Bob, Steal]T L equals S, Bob believes Alice using block S,action follow experts play strategy StealS . Mod[Bob, Steal]T L equalsTop-level block, Bob believes Alice using Top-level block,Alices action respond beliefs Bob. situation similar AlicesLdecision StealTAlicenode Mod[Alice, Steal]T L following exception:LMod[Alice, Steal] equals Top-level block, Alices action follows decision nodeBRT L [Steal].Appendix, prove following theorem.120fiNetworks Influence DiagramsTheorem 3.1. Converting NID MAID introduce cycle resultingMAID.AliceTLBobTLBobBobModTL[Bob, Steal]ModTL[Bob, PitchOut]ThrownOutTLBobStealTLBobPitchOutTLBobLeadBobLeaderTLBR [PitchOut]StealSPLeadBobLeaderTLLeadBobLeaderBRTL[Steal]PitchOutPModTL[Alice, PitchOut]ModTL[Alice, Steal]StealTLAlicePitchOutTLAliceThrownOutAliceTLTLAliceBobAliceTLAliceFigure 4: MAID representation NID Example 2.4conversion process implies, NIDs MAIDs equivalent expressivepower. However, NIDs provide several advantages MAIDs. NID block structuremakes explicit agents different beliefs decisions, chance variables utilitiesworld. mental model way agents reason decisions block. MAIDsdistinguish real world agents mental models worldother, whereas NIDs separate block mental model. Further, MAID,nodes simply represent chance, decision utilities, inherently interpretedterms beliefs. DK node MAID representation NID inherentlyrepresent agent beliefs decision made mental model K,ModK agent inherently represent mental model used makedecision. Indeed, mental models defined MAID. addition,relationship MAID descendants decisions NK NK , senserepresent possibly different beliefs agents N .121fiGal & PfefferTogether NID construction process described above, NID blueprintconstructing MAID describes agents mental models. Without NID,process becomes inherently difficult. Furthermore, constructed MAID may largeunwieldy compared NID block. Even simple NID Example 2.4, MAIDFigure 4 complicated hard understand.3.2 Equilibrium ConditionsSection 2.1, defined pure mixed strategies decisions MAIDs. NIDs,associate strategies decisions blocks appear. pure strategydecision NID block K mapping informational parentsaction domain D. Similarly, mixed strategy mappinginformational parents distribution domain D. strategy profileNID set strategies decisions blocks NID.Traditionally, equilibrium game defined terms best response strategies.Nash equilibrium strategy profile agent best possibly can,given strategies agents. Classical game theory predicts agentsplay best response. NIDs, hand, allow us describe situationsagent deviates best response playing according decision-makingprocess. would therefore like equilibrium specify agentsdo, also predict actually do, may different.NID equilibrium includes two types strategies. first, called best responsestrategy, describes agents do, given beliefs decision-makingprocesses agents. second, called actually played strategy, describesagents actually according model described NID. two strategiesmutually dependent. best response strategy decision block takesaccount agents beliefs actually played strategies decisions.actually played strategy decision block mixture best responsedecision block, actually played strategies decision blocks.Definition 3.2. Let N NID let MAID representation N. Letequilibrium M. Let node belonging agent block K N. Let parentsPa. construction MAID representation detailed Section 3.1,Kparents BR[D]K PaKdomains Pa Pa same. LetKBR[D]K (pa) denote mixed strategy assigned BR[D] PaKequals pa.Kbest response strategy K, denoted (pa), defines function valuesPa distributions satisfyK(pa) BR[D]K (pa)words, best response strategy MAID equilibriumcorresponding parents take values.Definition 3.3. Let P denote distribution defined Bayesian networkimplements . actually played strategy decision K ownedagent , denoted K(pa), specifies function values Pa distributionssatisfyKK(pa) P (D | pa)122fiNetworks Influence DiagramsNote here, DK conditioned informational parents decision ratherparents. node represents beliefs decision K. Therefore, actuallyplayed strategy K represents belief K, given informationalparents D.Definition 3.4. Let MAID equilibrium. NID equilibrium correspondingconsists two strategy profiles , every decision every block K,K best response strategy K, K actually played strategyK.example, consider constructed MAID baseball example Figure 4.best response strategies NID equilibrium specify strategies nodes StealPitchOut Top-level block belong Alice Bob respectively. equilibrium MAID, best response strategy Steal Top-level blockstrategy specified BRT L [Steal]. Similarily, best response strategy PitchoutTop-level block strategy specified BRT L [Pitchout]. actually playedstrategy Steal Top-level equal conditional probability distributionLgiven informational parent LeaderT L . Similarly, actually played strategyStealTAliceLPitchout equal conditional probability distribution PitchoutTBobgivenTLinformational parent Leader . Solving MAID yields following unique equilibrium:NID Top-level block, CPD nodes Mod[Alice, Steal] Mod[Bob, Pitchout]assigns probability 1 Top-level block, actually played best response strategies Bob Alice equal specified follows: Alice leading, Alice stealsbase probability 0.56 Bob pitches probability 0.47. Bob leading,Alice never steals base Bob never pitches out. turns experts may instruct Bob call pitch out, Alice considerably less likely steal base,compared equilibrium strategy MAID Example 2.1, nonemanagers considered possibility advised experts. casesimilar Bob.natural consequence definition problem computing NID equilibriareduces computing MAID equilibria. Solving NID requires convertMAID representation solving MAID using exact approximate solution algorithms.size MAID bounded size block times number blocks timesnumber agents. structure NID exploited MAID solutionalgorithm (Koller & Milch, 2001; Vickrey & Koller, 2002; Koller et al., 1996; Blum et al.,2006).4. Examplessection, provide series examples demonstrating benefits NIDsdescribing representing uncertainty decision-making processes wide varietydomains.4.1 Irrational AgentsSince challenge notion perfect rationality foundation economic systems presented Simon (1955), theory bounded rationality grown different123fiGal & Pfefferdirections. economic point view, bounded rationality dictates complete deviation utility maximizing paradigm, concepts optimizationobjective functions replaced satisficing heuristics (Gigerenzer &Selten, 2001). concepts recently formalized Rubinstein (1998).traditional AI perspective, agent exhibits bounded rationality program solution constrained optimization problem brought limitations architecturecomputational resources (Russell & Wefald, 1991). NIDs serve complementtwo prevailing perspectives allowing control extent agents behavingirrationally respect model.Irrationality captured framework distinction best responseactually played strategies. Rational agents always play best response respectmodels. rational agents, distinction normative behaviorprescribed agent NID block, descriptive prediction agentactually would play using block. case, best response actuallyplayed strategies agents equal. However, open systems, peopleinvolved, may need model agents whose behavior differs best responsestrategy. words, best response strategies actually played strategiesdifferent. capture agent behaving (partially) irrationally decisionblock K setting CPD Mod[, ] assign positive probability blockL $= K.natural way express distinction NIDs use Modnode. decision associated agent , use Mod[, ] describeblock actually uses make decision . block K, Mod[, ] equal Kprobability 1, means within K, making decision according beliefsblock K, meaning rational; play best response strategiesagents, given beliefs. If, however, Mod[, ] assigns positive probabilityblock L K, means probability playbest response beliefs K, rather play strategy accordingblock L. case, say self-models block K. introduction actually playedstrategies equilibrium definition represents another advantage NIDs MAIDs,explicitly represent strategies agents may deviate optimalstrategies.cases, making decision may lead agent behave irrationally viewingfuture considerably positive light objectively likely. example,person undergoing treatment disease may believe treatment stands betterchance success scientifically plausible. psychological literature, effectreferred motivational bias positive illusion (Bazerman, 2001). followingexample shows, NIDs represent agents motivational biases compelling way,making Mod nodes depend outcome decision nodes.Example 4.1. Consider case toothpaste company whose executives facedtwo sequential decisions: whether place advertisement magazineleading brand, whether increase production brand. Based past analysis,executives know without advertising, probability high sales brandnext quarter 0.5. Placing advertisement costs money, probabilityhigh sales rise 0.7. Increasing production brand contribute profit124fiNetworks Influence Diagramssales high, hurt profit sales low due high cost storage space.Suppose company executives wish consider possibility motivationalbias, placing advertisement inflate beliefs sales highnext quarter probability 0.9. may lead company increase productionbrand warranted market consequently, suffer losses.company executives wish compute best possible strategy two decisionsgiven fact attribute motivational bias.NID describing situation shown Figure 5c. Top-level block Figure 5ashows situation point view reality. includes two decisions, whetheradvertise (Advertise) whether increase supply brand (Increase).node Sales represents amount sales brand decision whetheradvertise, node Profit represents profit company, dependsnodes Advertise, Increase Sales. CPD Sales Top-level block assignsprobability 0.7 high Advertise true 0.5 high Advertise f alse, describedTable 4a. utility values node Profit shown Table 4.1. example,company advertises toothpaste, increases supply, sales high, receivesreward 70; company advertises toothpaste, increase supply,sales low, receives reward 40. Block Bias, described Figure 5b, representscompanys biased model. Here, decision advertise replaced automatonchance node assigns probability 1 Advertise = true. CPD Sales block Biasassigns probability 0.9 high Advertise true 0.5 high Advertise f alse,described Table 4b. Top-level block, following:1. node Mod[Company, Advertise] assigns probability 1 Top-level block.2. decision node Advertise parent node Mod[Company, Increase].3. node Mod[Company, Increase] assigns probability 1 block Bias Advertisetrue, assigns probability 0 block Bias Advertise f alse.Intuitively, Step 1 captures companys beliefs biased makesdecision advertise. Step 2 allows companys uncertainty whether biaseddepend decision advertise. Note example shows necessarydecision node depend agents beliefs past decision. Step 3 capturescompanys beliefs may use block Bias make decision whether increasesupply, confident high sales.Solving NID results following unique equilibrium: block Bias, companys actually played best response strategy increase supply,optimal action advertises sales high. block Top-level, following: company chooses advertise, behave rationally, best responseactually played strategy increase supply; company chooses advertise, actually played strategy use block Bias increases supply,best response strategy increase supply. Now, expected utilitycompany Top-level block higher chooses advertise. Therefore,best response strategies decisions advertise increase supply.Interestingly, company never biased, shown using backwards induction125fiGal & Pfefferoptimal action first decision advertise. Thus, reasoningpossible irrational behavior second decision, company revised strategyfirst decision.Mod[Company, Advertise]AdvertiseSalesAdvertiseMod[Company, Increase]SalesIncreaseProtIncreaseProt(a) Block Top-level(b) Block BiasTop-levelCompany, INCREASEBias(c) NIDFigure 5: Motivational Bias Scenario (Example 4.1)Advertisetruef alseSaleslow high0.30.70.50.5(a) node Sales (Top-levelBlock)Advertisetruef alseSaleslow high0.10.90.50.5(b) node Sales (Bias Block)Table 4: CPDs Top-level block Motivational Bias NID (Example 4.1)Example 4.2. Consider following extension Example 2.4. Supposetwo successive pitches, pitch managers option steal pitchout. Bob pitches first pitch, utility pitching second pitch(regardless Alices action) decreases 20 units forfeited two pitches.Bob believes probability 0.3, succumb social pressure secondpitch call pitch out. Bob would like reason possibility makingdecision first pitch.126fiNetworks Influence DiagramsAdvertisetruetruetruetruef alsef alsef alsef alseIncreasetruetruef alsef alsetruetruef alsef alseSaleshighlowhighlowhighlowhighlowProfit7070504080606030Table 5: Companys utility (node Profit) Top-level block Motivational Bias NID(Example 4.1)example, manager faced sequential decision problem: whethersteal pitch first second pitch. strategy second pitch relevantstrategy first pitch agent. Now, managers,rational, could use backward induction compute optimal strategies first pitch,working backwards second pitch. However, valid proceduremanagers behave rationally second pitch. example above, Bob knowsstrong pressure pitch second pitch wishes takepossibility account, making decision first pitch.Mod[Bob, PitchOut2 ]Top-levelL0.70.3Table 6: CPD Mod[Bob, PitchOut2 ] node Top-level block Irrational Agent Scenario(Example 4.2)model situation NID follows. Top-level block NID shownFigure 6a. Here, decision nodes Steal1 PitchOut1 represent decisions AliceBob first pitch, nodes Steal2 Pitchout2 represent decisionsAlice Bob second pitch. nodes Leader, Steal1 , PitchOut1 ThrownOut1informational parents decision nodes Steal2 PitchOut2 . expositoryconvenience, included edges leading node Leader utility nodesblock. Block L, shown Figure 6b, describes model second pitchBob succumbing social pressure pitches out, regardless leading.represented block include chance node PitchOut2 equals trueprobability 1 value Leader. node Mod[Bob, PitchOut2 ] assign probability0.3 block L, 0.7 probability Top-level block, shown Table 4.1. nodeMod[Bob, PitchOut2 ] displayed Top-level block. convention, impliesCPD assigns probability 1 Top-level block, Bob reasoningpossibility behaving irrationally respect second pitch. way,captured fact Bob may behave irrationally respect second pitch,reasoning possibility making decision first pitch.127fiGal & PfefferLeaderSteal1Bob1ThrownOut1PitchOut1Steal2Alice1Bob2Mod[Bob, PitchOut2]ThrownOut2LeaderPitchOut2Alice2PitchOut2(a) Block Top-levelTop levelBob, PITCHOUT2L(c) Irrational NIDFigure 6: Irrational Agent Scenario (Example 4.2)128(b) Block LfiNetworks Influence Diagramsunique equilibrium NID. agents behave rationally firstdecision actually played best response strategies equal, specifiedfollows: Alice steals base probability 0.49 leading, never steals baseBob leading. Bob pitches probability 0.38 Alice leading pitchesprobability 0.51 Bob leading. second pitch, Alice behaves rationally,best response actually played strategy follows: steal base probability0.42 Alice leading never steal base Bob leading. Bob may behave irrationallysecond pitch: best response strategy pitch probability 0.2 Aliceleading, pitch probability 0.52 Bob leading; actually played strategypitch probability 0.58 Alice leading, probability 0.71 Bobleading. Note Bob reasoning possible irrational behaviorsecond pitch, less likely pitch first pitch compared caseBob completely rational (Example 2.4).4.2 Conflicting Beliefstraditional game theory, agents beliefs assumed consistent common priordistribution, meaning beliefs agents others knowledge expressedposterior probability distribution resulting conditioning common prioragents information state. One consequence assumption agents beliefsdiffer observe different information (Aumann & Brandenburger, 1995).result led theoretic work attempted relax common prior assumption. Myerson(1991) showed game inconsistent belief structure finite convertednew game consistent belief structures constructing utility functionsequivalent original game way assign expected utilityagents. However, new game include beliefs utility functionsfundamentally different original game exhibiting inconsistent belief structure.summary economic philosophical ramifications relaxing common priorassumption, see work Morris (1995) Bonanno Nehring (1999).language allows us talk different mental modelsagents world, different beliefsstructure game, natural relax common prior assumption withinNIDs preserving original structure game.Example 4.3. Consider following extension baseball scenario Example 2.1.probability runner thrown depends decisionsmanagers, also speed runner. Suppose fast runner thrown0.4 probability Bob calls pitch out, 0.2 probability Bobcall pitch out. slow runner thrown 0.8 probability Bob callspitch out, 0.6 probability Bob call pitch out.Now, Bob believes runner slow, unsure Alices beliefs regardingspeed runner. probability 0.8, Bob believes Alice thinksstealer fast, probability 0.2 Bob believes Alice thinks stealerslow. Assume distributions variables example describedTable 1.129fiGal & Pfefferexample, Bob uncertain whether Alices beliefs speed runnerconflict own. NIDs allow express natural fashion two blocksdescribe decision-making process, differ CPD assignspeed runner. use Mod node, NIDs specify agentsconflicting beliefs two blocks used Alice make decision,according Bobs beliefs. NID blocks scenario presented Figure 7.Mod[Bob, Steal]LeadLeaderSpeedSpeedStealStealPitchOutAlicePitchOutThrownOutThrownOutAliceBob(a) Top-level BlockBob(b) Block LTop levelBob,STEALL(c) Conflicting Beliefs NIDFigure 7: Conflicting Beliefs Scenario (Example 4.3)Top-level block, shown Figure 7a, Bob Alice decide whether pitchsteal base, respectively. block identical structure Top-level blockprevious example, additional node Speed parent node ThrownOut,representing fact speed runner affects probability runnerthrown out.Top-level corresponds Bobs model, runner slow. CPDnode Speed assigns probability 1 slow block, shown Table 7a. BlockL, shown Figure 7b, represents identical decision-making process Top-levelblock, except CPD Speed different: assigns probability 1 f ast, shownTable 7b. complete NID shown Figure 7c. Bobs uncertainty Toplevel block Alices decision-making process represented node Mod[Bob, Steal],whose CPD shown Table 7c. probability 0.8, Alice assumed using blockL, speed runner fast. probability 0.2, Alice assumedusing Top-level block, speed runner slow. Note130fiNetworks Influence DiagramsSpeedf ast slow01Speedf ast slow10(a) node Speed(b) node Speed(block Top-level) (block L)Mod[Bob, Steal]Top-levelL0.20.8(c) nodeMod[Bob, Steal] (blockTop-level)Table 7: CPDs nodes Conflicting Beliefs NID (Example 4.3)Top-level block, nodes Mod[Alice, Steal], Mod[Alice, PitchOut] Mod[Bob, PitchOut]displayed. convention introduced earlier, nodes assign probability1 Top-level block omitted Top-level block Figure 7a.Interestingly, implies Alice knows runner slow, even though Bob believesAlice believes runner fast. solving NID, get unique equilibrium.agents rational, best response actually played strategies equal,specified follows: block L, runner fast, Alice always steals base, Bobalways calls pitch out. Top-level block, Bob believes Alice uses block Lhigh probability, seals base. Top-level block speed runnerslow likely thrown out. Therefore, Bob pitch order maximizeutility given beliefs Alice. turn, Alice steal base Top-levelblock speed runner slow block.4.3 Collusion Alliancessituation agent modeling multiple agents, may important knowwhether agents working together fashion. situations, modelsagents make decisions may correlated, due possible collusion.Example 4.4. voting game involves 3 agents Alice, Bob, Carol, voting onechairperson committee. Alice incumbent, chairpersonvote ends draw. agent would like chairperson, receivesutility 2 case. Alice also receives utility 1 votes winner loseselection, wants look good. Bob Carol, meanwhile, dislike Alicereceive utility -1 Alice wins.best interests agents Bob Carol coordinate, voteperson. Bob Carol indeed coordinate, Alices best interest voteperson vote for. However, Bob Carol mis-coordinate, Alice voteremain chairperson. taking opponent modeling approach, Alice wouldlike model Bob Carol likely vote. Alice believesprobability 0.2, Bob Carol collude; probability 0.3, Bob Carol colludevote Bob; probability 0.4, Bob Carol collude vote Carol. Also, Alicebelieves collude, agents might renege voteprobability 0.1.example easily captured NID. Top-level block shown Figure 8.node Collude, three possible values: none indicating collusion;131fiGal & PfefferBob Carol indicating collusion vote Bob Carol respectively. decision nodesA, B, C represent decisions Alice, Bob Carol, respectively. CPD Colludepresented Table 8a. nodes Mod[Alice, B] Mod[Alice, C], whose CPD shownTable 8b 8c respectively, depend Collude. Collude none, Mod[Alice, B]assign probability 1 Top-level block. Collude Bob, Mod[Alice, B] equal blockB describing automaton Bob Carol vote Bob. Collude Carol,Mod[Alice, B] equal block C, Bob Carol vote Carol probability0.9, block B probability 0.1. accounts possibility BobCarol agreed vote Carol, Bob might renege. CPD Mod[Alice, B]similar, described Table 8b. CPD Mod[Alice, C] symmetric,described Table 8c.ColludeMod[Alice, B]Mod[Alice, C]BCAliceBobCarollFigure 8: Top-level block Collusion Scenario (Example 4.4)Colludenone Bob Carol0.20.30.5(a) node ColludeMod[Alice, B]Top-levelBCnone100ColludeBob Carol00100.10.9(b) node Mod[Alice, B]Mod[Alice, C]Top-levelBCnone100ColludeBob Carol000.90.101(c) node Mod[Alice, C]Table 8: CPDs Top-level block Collusion Scenario (Example 4.4)unique NID equilibrium example, agents rational actuallyplayed best response strategies equal. equilibrium, Alice always votesCarol believes Bob Carol likely collude vote Carol.132fiNetworks Influence Diagramsturn, Carol votes herslef Bob probability 0.5, Bob always voteshimself. reneging, Bob gives chance win vote, case Carolvotes him.Moving beyond example, one important issues multi-player gamesalliances. players form alliance, act benefit alliance ratherpurely self-interest. Thus agents beliefs alliance structureaffects models agents make decisions. agent makedecision situation, important able model uncertaintyalliance structure.4.4 Cyclic Belief StructuresCyclic belief structures important game theory, used model agentssymmetrically modeling other. used describe infinite regressthink think think... reasoning. Furthermore, cyclic belief structuresexpressed economic formalisms, like Bayesian games, vital allowNIDs order NIDs encompass Bayesian games. Cyclic belief structures naturallycaptured NIDs including cycle NID graph.Example 4.5. Recall Example 4.3, Alice Bob conflicting beliefsspeed runner. Suppose Bob believes runner slow,probability 0.8, Alice believes runner fast, modeling Bob reasoningAlices beliefs, on...model scenario using cyclic NID described Figure 9c. Top-levelblock, shown Figure 9b, Bob believes runner slow modeling Aliceusing block L make decision. block L, Alice believes runner fast,modeling Bob using Top-level block make decision. Bobs beliefs AliceTop-level block represented CPD node Mod[Bob, Steal], shown Table9c, assigns probability 1 block L.block L, CPD Speed, shown Table 9b assigns probability 1 f ast. Alicesbeliefs Bob block L represented CPD node Mod[Alice, PitchOut],shown Table 9d, assigns probability 1 block L. Top-level block, CPDSpeed assigns probability 1 slow, shown Table 4.4a. NID equilibriumscenario follows. blocks L Top-level, Alice steal base, Bobpitch out, regardless leading.5. Application: Opponent Modelingcases, agents use rules, heuristics, patterns tendencies making decisions.One main approaches game playing imperfect information opponent modeling, agents try learn patterns exhibited players reactmodel others. NIDs provide solid, coherent foundation opponent modeling.Example 5.1. game RoShamBo (commonly referred Rock-Paper-Scissors),players simultaneously choose one rock, paper, scissors. choose item,result tie; otherwise rock crushes scissors, paper covers rock, scissors cut paper,shown Table 10.133fiGal & PfefferSpeedf ast slow01Speedf ast slow10(a) node Speed(b) node Speed(block Top-level) (block L)Mod[Bob, Steal]Top-levelL10(c) nodeMod[Bob, Steal](block Top-level)Mod[Alice, PitchOut]Top-levelL10(d) nodeMod[Alice, PitchOut](block L)Table 9: CPDs nodes Cyclic NID (Example 4.5)Mod[Bob, Steal]Mod[Alice, PitchOut]SpeedSpeedStealStealPitchOutThrownOutPitchOutThrownOutAliceAliceBob(a) Block LBob(b) Block Top-levelTop levelBob,STEALAlice,PITCHOUTL(c) Cyclic NIDFigure 9: Cyclic Baseball Scenario (Example 4.5)rockpaperscissorsrock(0, 0)(1, 1)(1, 1)paper(1, 1)(0, 0)(1, 1)scissors(1, 1)(1, 1)(0, 0)Table 10: Payoff Matrix Rock-paper-scissors134fiNetworks Influence Diagramsgame single Nash equilibrium players play mixed strategy{rock, paper, scissors} probability { 13 , 13 , 13 }. players deviateequilibrium strategy, guaranteed expected payoff zero. fact,easy verify player always plays equilibrium strategy guaranteedget expected zero payoff regardless strategy opponent. words,sticking equilibrium strategy guarantees lose match expectation,also guarantees win it!However, player try win game opponents playing suboptimally.suboptimal strategy beaten, predicting next move opponentemploying counter-strategy. key predicting next move modelstrategy opponent, identifying regularities past moves.consider situation two players play repeatedly other.player able pick tendencies suboptimal opponent, might abledefeat it, assuming opponent continues play suboptimally. recent competition (Billings, 2000), programs competed matches consisting 1000games RoShamBo. one might expect, Nash equilibrium players came middlepack broke even every opponent. turned taskmodeling opponents strategy surprisingly complex, despite simple structure game itself. sophisticated players attempt counter-modelopponents, hide strategy avoid detection. winning program,called Iocaine Powder (Egnor, 2000), beautiful job modeling opponents multiple levels. Iocaine Powder considered opponent might play randomly, accordingheuristic, might try learn pattern used Iocaine Powder, mightplay strategy designed counter Iocaine Powder learning pattern, severalpossibilities.5.1 NID Modeling Belief HierarchiesInspired Iocaine Powder, constructed NID player playing matchRoShamBo trying model opponent. Suppose Bob wishes modelAlices play using NID. block Top-level NID, shown Figure 10a, simplyMAID depicting RoShamBo round Bob Alice. players accesspredictor P, algorithm able predict next move sequence probabilitydistribution possible moves. information available predictorhistory past moves Alice Bob.Alice may ignoring P, playing Nash Equilibrium strategy. Bob severalalternative models Alices decision. According block Automaton, shown Figure 10c,Alice always follows signal P. block B1, shown Figure 10b, Bob modeling Aliceusing block Automaton make decision. achieved setting CPDMod[Bob, Alice] block B1 assign probability 1 Automaton. analyze NIDrooted block B1 determine Bobs best response Alice. example, Bob thinks,based history, P likely tell Alice play rock, Bob would playpaper. Let us denote strategy BR(P).However, Alice also model Bob assigning probability 1 Mod[Alice, Bob] blockA1. way, Alice reasoning Bob modeling Alice following predictor P.135fiGal & PfefferPMod[Alice, Bob]alicebobbobaliceMod[Bob, Alice]PalicebobbobaliceP(a) Blocks Top-level, A1,A2(b) Blocks B1,B2Alice(c) Block AutomatonTop-levelBob,ALICEA2Bob, ALICEAlice, BOBBob, ALICEB2Bob,ALICEA1Alice, BOBB1Bob, ALICEAutomaton(d) RoShamBo NIDFigure 10: RoShamBo Scenario (Example 5.1)136fiNetworks Influence Diagramsanalyze NID originating block A1, shown Figure 10a, determineAlices best-response Bobs model well Bobs best-response modelAlice. Since Alice believes Bob plays BR(P) result Bobs belief Alice playsaccording P, therefore play best response BR(P), thereby double-guessingBob. Alices strategy block A1 denoted BR(BR(P)). Following example,block A1 Alice play rock all, scissors, order beat Bobs play paper.Similarly, block B2, Bob models Alice using block A1 make decisions,block A2, Alice models Bob using block B2 make decision. Therefore, solvingNID originating block B2 results BR(BR(BR(P))) strategy Bob. wouldprompt Bob play rock B2 example, order beat scissors. Lastly, solvingNID originating block A2 results BR(BR(BR(BR(P)))) strategy Alice.would prompt Alice play paper block A2, order beat rock. Thus,shown every instance predictor P, Alice might play one three possiblestrategies. pure strategy choose rock, paper, scissorsgiven P, reasoning process terminates.entire NID shown Figure 10d. block Top-level, Bob models Alice usingone several possible child blocks: block Automaton, Alice follows predictor;block A1, Alice second-guessing predictor; block A2, Alicetriple-guessing predictor. Bobs uncertainty Alices decision-making processescaptured Mod[Bob, Alice] node block Top-level. Analyzing Top-level blockNID extract Bobs best response strategy given beliefs Alices decisionmaking processes.use NID practice, necessary compute MAID equilibrium extractBobs best-response strategy Top-level block. end, need estimatevalues NID parameters, represented unknown CPDs blocks,solve NID. parameters include Mod[Bob, Alice], representing Bobs beliefsTop-level block regarding block Alice using; node P, representingdistributions governing signals Alice Bob, respectively.2 end, useon-line version EM algorithm tailored NIDs. beginrandom parameter assignments unknown CPDs. revise estimateparameters NID given observations round. Bob plays bestresponse strategy MAID representation NID given current parametersetting. Interleaving learning using NID make decision helps Bob adaptAlices possibly changing strategy.5.2 Empirical Evaluationevaluated NID agent ten top contestants first automaticRoShamBo competition. agents used opponent modeling approach,is, learned signal opponents play based history prior rounds.Contestants roughly classified according three dimensions: type signal used(probabilistic vs. deterministic); type reasoning used (pattern vs. meta-reasoners);and, degree exploration versus exploitation model. Probabilistic agents2. Technically, CPDs nodes representing prior history also missing. However,observed decision-making point interaction CPDs affect players utilities.137fiGal & Pfefferestimated distribution strategies opponents deterministic agentspredicted opponents next move certainty. Pattern reasoners directly modeledopponents playing according rule distribution, reasonpossibility opponents modeling themselves. contrast, meta-reasonersattempted double- triple-guess opponents play. Exploitative agents played bestresponse model opponents, explorative agents deviated, certainconditions, best response strategy try learn different behavioral patternsopponents. Iocaine Powder used strategy reverting Nash equilibriumlosing. made impossible evaluate whether NID modelcould learn Iocaine powders reasoning process, turned strategy. Also, limitedcontestants strategies depend last 100 rounds play, order allow faircomparison NID agent used four rounds play. limitfour rounds originally designed use short history.purpose show explicitly reasoning learning mental models makedifference, optimize learning model signal.Figure 11 shows performance RoShamBo NID playing 10 matches3,000 rounds contestant. overall standings determined orderingtotal scores contestant rounds played (+1 winning roundcontestant NID player; 1 losing round; 0 ties). Therefore, importantplayer maximize win weaker opponents, minimize lossstronger opponents. x-axis includes contestant number y-axis describesdifference average score RoShamBo NID contestant; errorbars indicate single standard deviation difference.shown figure, RoShamBo NID able defeat contestantmatches, including version Iocaine Powder. best performance NIDachieved playing pattern reasoners used deterministic signals (Contestants 3, 56). contestants directly predicted opponents play functionhistory, without reasoning opponents model themselves. Consequently,difficult detect change strategies adaptive opponents,RoShamBo NID. addition, use deterministic signals made hardercontestants capture probabilistic players like NID algorithm.RoShamBo NID also outperformed contestants attempted trickopponents, reasoning possibility opponents double- tripleguessing model (Contestants 4 1). shows NID able determinelevel reasoning employed opponents.6. Relationship Economic Modelssection, describe relationship NIDs several existing formalismsrepresenting uncertainty decision-making processes. NIDs share close relationship Bayesian games (Harsanyi, 1967), game-theoretic framework representinguncertainty players payoffs. Bayesian games capture beliefs agentswell define equilibrium assigns best response strategyagent given beliefs. Bayesian games quite powerful ability describe beliefhierarchies cyclic belief structures.138fiNetworks Influence Diagrams400350Average Score Difference300250200150100500012345Contestant67Opponent typeIocaine PowderProbabilistic, Pattern, ExploitativeDeterministic, Pattern, ExploitativeProbabilistic, Meta, ExploitativeProbabilistic, Pattern, Exploitative8910Number12, 93, 6, 51, 47, 8Figure 11: Difference average outcomes NID player opponentsBayesian game, agent discrete type embodying private information.Let N set agents. agent Bayesian game includes set possible typesTi , set possible actions Ci , conditional distribution pi utility function ui . Let= Ti let C = Ci . agent i, let Ti = j&=i Tj denote setpossible types agent i. probability distribution pi functionti Ti , is, pi (.|ti ) specifies type ti Ti joint distributiontypes agents. utility function ui function C realnumbers. standard assumption game, including agents strategies, utilitiestype distributions, common knowledge agents.solution concept commonly associated Bayesian games BayesianNash equilibrium. equilibrium maps type mixed strategy actionsagents best response strategies agents, given beliefstypes. Notice Bayesian game, agents action depend typestypes agents, unknown agentanalyzes game. assumed agent knows type, typesubsumes agents private information game begins. typesagents unknown, agent maximizes expected utility given distributiontypes.Let Ni denote agents Bayesian game apart agent i. Let (.|ti )denote random strategy agent given type ti . Bayesian Nash equilibrium139fiGal & Pfeffermixed strategy profile agent type ti Ti#(.|ti ) argmax Ci ti Ti pi (ti |ti )$%&#cCjNi j (cj |tj ) (ci )ui (t, c)(2)Bayesian games used extensively modeling interaction agentsprivate information, auction mechanisms (Myerson, 1991) usedexpress uncertainty agents decision-making models. general, Bayesian gamesexpressive NIDs. show, Bayesian game converted NIDtime space linear size Bayesian game. Conversely, NIDconverted Bayesian game, NID converted MAID,turn converted extensive form game. extensive form game convertednormal form game trivial Bayesian game one type per agent.However, worst case, size extensive form game exponentialnumber informational parents decision nodes MAID, size normalform game exponential size extensive form game. course,brute force conversion; compact conversions may possible.consider formally question whether Bayesian games represented NIDs. idea align type Bayesian game decisionNID block. resulting best response strategy decision NID equilibriumequal Bayes Nash equilibrium strategy type.Definition 6.1. Let B Bayesian game N NID. say N equivalentB exists injective mapping f types B (block,agent) pairs N ,following conditions hold:1. Bayesian Nash equilibrium B, exists NID equilibrium N ,every type ti , f maps ti (K, ), best-response actually-playedstrategies K equal (.|ti ).2. NID equilibrium N , exists Bayesian Nash equilibrium Bevery (K, ) image f , (.|ti ) ti = f 1 (K, ) equalbest-response actually-played strategies K.following theorem proved Appendix 8.Theorem 6.2. Every Bayesian game represented equivalent NID whose sizelinear size Bayesian game.section, use term Bayesian games specify representationincludes type distributions utility functions presented explicitly. NIDs enjoyadvantages fully specified Bayesian games graphical models typically enjoyunstructured representations. general, NIDs may exponentially compactBayesian games Bayesian games require, every type every agent, fulljoint distribution types agents. addition, utility functionBayesian game specifies utility joint combination types actions everyplayer. distributions utility functions exponential number players.NIDs, based MAIDs, type distributions decomposed140fiNetworks Influence Diagramsproduct small conditional distributions, utility functions additivelydecomposed sum small functions depend small number actions.addition, Bayesian games representationally obscure. First, types Bayesiangames atomic entities capture information available agent singlevariable. type used capture agents beliefs way world works(including preferences), private information. example, poker,players beliefs players tendency bluff knowledge cardsreceived captured type. believe two aspects fundamentallydifferent; one describes actual state world describes goingplayers head. Conflating two aspects leads confusion. NIDs, two aspectsdifferentiated. Private information world represented informationalparents, whereas mental models represented blocks.Second, type Bayesian game decompose different aspects informationvariables. Thus poker, hand must represented single variable, whereasNIDs represented different variables representing cards. final pointBayesian games uncertainty must folded utility functionsdistribution agents types. Consider scenario two agentsconflicting beliefs chance variable, Example 4.3. NID,separate block possible mental model differs CPD assignmentschance variable. contrast, type Bayesian game would sumdistribution chance variable. Looking Bayesian game, would knowwhether reason different utility functions agent different beliefschance variable, whether due different preferences agent.NIDs also exhibit relationship recent formalisms games awareness,agents may unaware players strategies structuregame (Halpern & Rego, 2006; Feinberg, 2004). game description formalism showsplayers awareness others strategies changes time. game awarenessincludes set extensive form game descriptions, called augmented games, representanalysts beliefs world, well separate descriptions gamemay become true according agents subjective beliefs. analysts augmented gameconsidered actual description reality, subjective augmented gamediffer analysts game agents utility functions, decisions, strategiesavailable agents decisions. history agent augmented gamesub-path tree leading node agent makes move. Awarenessmodeled function maps agent-history pair one augmented game anotheraugmented game agent considers possible given history. Uncertaintyagents awareness augmented game quantified nature choosemove tree leading agents information sets. definition Nash equilibriumextended include set strategies agent-game pairthat agent considerspossible, given history best-response strategies used agentsaugmented game. formalism capture analysts model agents awarenesswell agents model own, agents awareness.fundamental differences NIDs games awareness. First, likeBayesian games, equilibrium conditions representation allow agentsdeviate best-response strategies. Second, require presence modeler141fiGal & Pfefferagent, reality, modeling uncertainty levels awareness agents.NIDs allow modeler agent, require it. allows capturesituations agent certain knowledge reality, Baseball NIDExample 2.4. Third, augmented game awareness represented extensive formgame, shown above, may exponentially larger MAID usedrepresent decision-making model NID. Lastly, agents awareness othersstrategies one type reasoning captured NID. typesreasoning processes described Section 4.Lastly, Gmytrasiewicz Durfee (2001) developed framework representinguncertainty decision-making using tree structure nodes consist payoffmatrices particular agent. Like Bayesian games, uncertainty folded payoffmatrices. agent maintains tree, representing model decision-makingprocesses used agents. Like traditional representations, language assumesagents behave rationally. addition, assumes agent believes others usefixed strategy, folded environment.7. Conclusionpresented highly expressive language describing agents beliefs decisionmaking processes games. language graphical. model languagenetwork interrelated models, mental model graphical modelgame. agent one mental model may believe another agent (or possibly itself) usesdifferent mental model make decisions; may uncertainty mentalmodel used. presented semantics language terms multi-agent influencediagrams. analyzed relationship language Bayesian games.equally expressive, NIDs may exponentially compact.showed language used describe agents play irrationally,sense actual play correspond best possible response givenbeliefs world agents. captured novel equilibriumconcept captures interaction agents actuallydo. also showed express situations agents conflicting beliefs,including situations agents common prior distributionstate world. Finally, showed capture cyclic reasoning patterns,agents engage infinite chains think think think... reasoning.vital question use language learn agents behavior reasoningprocesses. shown, language used learn non-stationary strategiesrock-paper-scissors. work, shown models inspiredNIDs learn peoples play negotiation games (Gal, Pfeffer, Marzo, & Grosz, 2004; Gal& Pfeffer, 2006). focus continuing work develop general methodlearning models NIDs.AcknowledgmentsThank much useful comments provided anonymous reviewerseditor. Thanks Barbara Grosz Whitman Richards invaluable guidance.142fiNetworks Influence DiagramsThanks Adam Juda reading prior draft work. work supportedNSF Career Award IIS-0091815 AFOSR contract FA9550-05-1-0321.8. AppendixTheorem 3.1: Converting NID MAID introduce cycle resultingMAID.Proof. First, let us ignore edges added step 5 construction, focusMAID fragment OK constructed single block K. Since block acyclic,number nodes block integers topological order. number nodesOK follows. node N derives chance utility node N K, Ngets number N . node BR[D]K gets number D. node DK ,owns D, gets number plus 1/3. node DK ,D, gets number plus 2/3. construction, P parent N OK , Plower number N .let us consider entire constructed MAID O. Suppose, way contradiction,cycle O. follows argument must consist entirelyedges fragments added step 5. Since edges emanate nodeDK owns D, end node DL , nodes cycle must referdecision D, must belong agent owns D. Thus cycle must formDK1 , . . . , DKn , DK1 owns D. Since edge added DKi DKi+1O, must modeling block Ki using block Ki+1 make decision D. Thereforeself-loop NID, contradiction.Theorem 6.2: Every Bayesian game represented equivalent NID whose sizelinear size Bayesian game.Proof. Given Bayesian game B, construct NID N follows. set agents Nequal set agents B. type ti agent B correspondingblock N labeled ti . block ti contains decision node Dj utility node Ujevery agent j. Dj informational parents. domain Dj set choices Cjagent j B. add new chance node Qi block ti whose domain set Ti .node Mod[i, Dj ] j $= node Qi parent. parents Uidecision nodes well node Qi . agent j $= i, Uj parentDj . agent j define distinguished action cj Cj .set CPD nodes ti follows:1. CPD Mod[i, Di ] assigns probability 1 ti .2. CPD Qi assigns probability pi (ti | ti ), defined B, type profileti Ti .3. CPD node Mod[i, Dj ] j $= assigns probability 1 block tjjth element value parent node Qi equals tj . projects probabilitydistribution Qi B node Mod[i, Dj ] representing beliefs blockagent j using NID.143fiGal & Pfeffer4. CPD Ui assigns probability 1 ui (t, c), defined B, given Qi equalst, equals c.5. CPD Uj assigns probability 1 utility 1 Dj = cj , probability 10 otherwise.6. CPD Mod[j, Dk ], k, j $= i, assigns probability 1 ti .construction accompanied injective mapping f maps type ti(block,agent) pair (ti , i).Let constructed MAID N. prove condition 1 Definition 6.1, letBayes Nash equilibrium B. agent, conditional probability distribution( | ti ). define strategy profile follows. BR[Di ]ti = ( | ti ) decisionsowned agent i, BR[Dj ]ti assigns probability 1 cj j $= i.claim following:1. MAID equilibrium M, according Definition 2.3.2. resulting NID equilibrium, best response strategy ti ( | ti ).3. resulting NID equilibrium, actually played strategy bestresponse strategy.Claim 3 true Mod[i, Di ] assigns probability 1 ti .Note informational parents N. Therefore, definition NIDti= BR[Di ,ti ] = ( | ti ). Therefore, Claim 2equilibrium, best response strategytrue.prove Claim 1, note first block ti , utility node Uj , j $= i, fullydetermined Dj , Dj sole parent Uj . Also, player j self-modelingDj , CPD node Mod[j, Dj ] assigns probability 1 ti . holdsM: decision node BR[Dj ]ti sole parent Ujti . Therefore, equilibriumM, strategy BR[Dj ]ti assign probability 1 distinguished action cjcauses Ujti 1.block ti , CPD Mod[i, Dj ] assigns probability 0 ti . means playerj using block ti make decision, according beliefs. Therefore, BR[Dj ]tiindependent Uiti , equilibrium strategies BR[Di ]ti independentdistinguished action chosen BR[Di ]tj .definition MAID equilibrium Definition 2.3, strategy profileequilibrium maximizes EU (i). need show maximizing equivalentmaximizing right hand side Equation 2. utility Uiti decision nodeBR[Di ]ti every block ti . Let ctii denote choice agent decision BR[Di ]ti blockt"ti . Let t(i denote block corresponding different type t(i agent i. Let cii choice"agent decision BR[Di ]ti block t(i ci choices decisions"BR[Di ]ti . construction M, Uiti d-separated BR[Di ]ti given BR[Di ]tiBR[Di ]ti .result, optimize Uiti separately utility nodes belongingagent i, considering BR[Di ]ti . get utility given144fiNetworks Influence Diagramsstrategy profile writtenE [Uiti ] ="iti (ctii )"cici(ci )"uiP (Uiti = ui | ctii , ci ) utii(3)condition agent beliefs decisions agents block ti . LetMod[i, Di ]ti denote set nodes Mod[i, Dj ]ti j $= i, let tuple ti referblock label profile blocks Ti . obtain"ciiti (ctii )"ci(ci )"tiP (Mod[i, Di ]ti = ti )"ui(utii | ctii , ci , ti ) utii(4)observe role Mod[i, Di ]ti determine choices decisionsBR[Di ]ti relevant utility player i. particular, Mod[i, Dj ]ti equaltj , js choice block tj player needs consider makes decisionBR[Di ]ti . Let ci denote relevant choices BR[Di ]ti Mod[i, Di ]ti = ti .Since choice variables irrelevant, marginalize obtain#ciiti (ctii )#ci#(ci)#tiP (Mod[i, Di ]ti = ti )(5)P (Uiti = ui | ctii , ci) utiiuiRearranging terms, rewrite Equation 5.(# '%tj tjti = )P(Mod[i,](c)ticj&=i jjti ti #(ci ) uti P (Ui = ui | cii , ci) ui#(6)construction, P (Mod[i, Di ]ti = ti ) pi (ti | ti ) defined B, jj (cjj )#ti) utii ui (t, c). therefore getj (cj | tj ) defined B, uti P (Uiti = ui | ctii , ci"tipi (ti | ti )"c!j&=ij (cj | tj ) (ci | ti )ui (t, c)(7)Therefore MAID equilibrium Bayesian Nash equilibriumB. Claim 1 established therefore Condition 1 Definition 6.1 satisfied.Finally, prove Condition 2, given NID equilibrium N construct MAIDequilibrium copying best response strategies, construct strategiesB exactly reverse manner above. previous reasoning applies reverseshow Bayes Nash equilibrium B best response actually playedstrategies N equal .145fiGal & PfefferReferencesArunachalam, R., & Sadeh, N. M. (2005). supply chain trading agent competition.Electronic Commerce Research Applications, 4, 6381.Aumann, R., & Brandenburger, A. (1995). Epistemic conditions Nash equilibrium.Econometrica, 63 (5), 11611180.Bazerman, M. (2001). Judgment Managerial Decision Making. Wiley Publishers.Billings, D. (2000). first international RoShamBo programming competition. International Computer Games Association Journal, 23 (1), 38.Blum, B., Shelton, C. R., & Koller, D. (2006). continuation method Nash equilibriastructured games. Journal Artificial Intelligence Research, 25, 457502.Bonanno, G., & Nehring, K. (1999). make sense common prior assumptionincomplete information. International Journal Game Theory, 28, 409434.Camerer, C. F. (2003) Behavioral Game Theory. Experiments Strategic Interaction,chap. 2. Princeton University Press.Cowell, R. G., Lauritzen, S. L., & Spiegelhater, D. J. (2005). Probabilistic NetworksExpert Systems. Springer.Dechter, R. (1999). Bucket elimination: unifying framework reasoning. ArtificialIntelligence, 113 (1-2), 4185.Egnor, D. (2000). Iocaine Powder. International Computer Games Association Journal,23 (1), 38.Feinberg, Y. (2004). Subjective reasoning games unawareness. Tech. rep. 1875,Stanford University.Gal, Y., & Pfeffer, A. (2003a). language modeling agents decision making processesgames. Proc. 2nd International Joint Conference Autonomous AgentsMulti-agent Systems (AAMAS).Gal, Y., & Pfeffer, A. (2003b). language opponent modeling repeated games.Workshop Game Theory Decision Theory, AAMAS.Gal, Y., & Pfeffer, A. (2004). Reasoning rationality belief. Proc. 3rd International Joint Conference Autonomous Agents Multi-agent Systems (AAMAS).Gal, Y., & Pfeffer, A. (2006). Predicting peoples bidding behavior negotiation. Proc.5th International Joint Conference Autonomous Agents Multi-agent Systems(AAMAS).Gal, Y., Pfeffer, A., Marzo, F., & Grosz, B. (2004). Learning social preferences games.Proc. 19th National Conference Artificial Intelligence (AAAI).Gigerenzer, G., & Selten, R. (Eds.). (2001). Bounded Rationality: Adaptive Toolbox.MIT Press.Gmytrasiewicz, P., & Durfee, E. H. (2001). Rational communication multi-agent environments. Autonomous Agents Multi-Agent Systems, 4 (3), 233272.146fiNetworks Influence DiagramsHalpern, J., & Rego, L. (2006). Extensive games possibly unaware players. Proc.5th International Joint Conference Autonomous Agents Multi-agent Systems(AAMAS).Harsanyi, J. C. (1967). Games incomplete information played Bayesian players.Management Science, 14, 159182, 320334, 486502.Howard, R. A., & Matheson, J. E. (1984). Influence diagrams. Readings PrinciplesApplications Decision Analysis, pp. 721762.Kearns, M., Littman, M., & Singh, S. (2001). Graphical models game theory. Proc.17th Conference Uncertainty Artificial Intelligence (UAI).Koller, D., Meggido, N., & von Stengel, B. (1996). Efficient computation equilibriaextensive two-person games. Games Economic Behavior, 14 (2), 247259.Koller, D., & Milch, B. (2001). Multi-agent influence diagrams representing solvinggames. Proc. 17th International Joint Conference Artificial Intelligence (IJCAI).MacKie-Mason, J. K., Osepayshivili, A., Reeves, D. M., & Wellman, M. P. (2004). Priceprediction strategies market-based scheduling. Proc. 18th International Conference Automated Planning Scheduling.Morris, S. (1995). common prior assumption economic theory. Economic Philosophy,pp. 227253.Myerson, R. (1991). Game Theory. Harvard University Press.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.Rajarshi, D., Hanson, J. E., Kephart, J. O., & Tesauro, G. (2001). Agent-human interactionscontinuous double auction. Proc. 17th International Joint ConferenceArtificial Intelligence (IJCAI).Rubinstein, A. (1998). Modeling Bounded Rationality. MIT Press.Russell, S., & Wefald, E. (1991). Right Thing: Studies Limited Rationality. MITPress.Simon, H. A. (1955). behavioral model rational choice. Quarterly Journal Economics,69, 99118.Vickrey, D., & Koller, D. (2002). Multi-agent algorithms solving graphical games.Proc. 18th National Conference Artificial Intelligence (AAAI).147fiJournal Artificial Intelligence Research 33 (2008) 3377Submitted 09/07; published 09/08ICE: Expressive Iterative Combinatorial ExchangeBenjamin LubinAdam I. JudaRuggiero CavalloSebastien LahaieJeffrey ShneidmanDavid C. Parkesblubin@eecs.harvard.eduadamjuda@post.harvard.educavallo@eecs.harvard.eduslahaie@eecs.harvard.edujeffsh@eecs.harvard.eduparkes@eecs.harvard.eduSchool Engineering Applied SciencesHarvard UniversityCambridge, 02138Abstractpresent design analysis first fully expressive, iterative combinatorialexchange (ICE). exchange incorporates tree-based bidding language (TBBL)concise expressive CEs. Bidders specify lower upper bounds TBBLvalue different trades refine bounds across rounds. bounds allow pricediscovery useful preference elicitation early rounds, allow terminationefficient trade despite partial information bidder valuations. computationexchange carefully optimized exploit structure bid-trees avoid enumerating trades. proxied interpretation revealed-preference activity rule, coupledsimple linear prices, ensures progress across rounds. exchange fully implemented, give results demonstrating several aspects scalability economicproperties simulated bidding strategies.1. IntroductionCombinatorial exchanges combine generalize two different mechanisms: double auctionscombinatorial auctions. double auction (DA), multiple buyers sellers tradeunits identical good (McAfee, 1992). combinatorial auction (CA), single sellermultiple heterogeneous items sale (de Vries & Vohra, 2003; Cramton, Shoham,& Steinberg, 2006). buyer CA may complementarities (I want B)substitutabilities (I want B) goods, provided expressivebidding language describe preferences. common goal design DAsCAs implement efficient allocation, allocation maximizestotal social welfare.combinatorial exchange (CE) (Parkes, Kalagnanam, & Eso, 2001) combinatorialdouble auction brings together multiple buyers sellers trade multiple heterogeneous goods. CEs potential use wireless spectrum allocation (Cramton, Kwerel, &Williams, 1998; Kwerel & Williams, 2002), airport takeoff landing slot allocation (Ball,Donohue, & Hoffman, 2006; Vossen & Ball, 2006), financial markets (Saatcioglu,Stallaert, & Whinston, 2001). domains incumbents propertyrights, necessary facilitate complex multi-way reallocation resources. Another potential application domain CEs allocate resources shared distributedc2008AI Access Foundation. rights reserved.fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkessystems, PlanetLab (Fu, Chase, Chun, Schwab, & Vahdat, 2003). CEs also finduse task allocation robot teams, making potentially powerful tool multiagent systems community (Gerkey & Mataric, 2002; Bererton, Gordon, & Thrun, 2003;Dias, Zlot, Kalra, & Stentz, 2006). Finally, CEs promise mechanisms expressivesourcing multiple bid-takers, perhaps representing different profit centers within organization; see associated work expressive sourcing using one-sided CAs (Sandholm,2007).paper presents design first fully expressive, iterative combinatorial exchange (ICE). designing iterative exchange, share motivation earlier workiterative CAs: wish mitigate elicitation costs focusing bidders, caseprice discovery activity rules, values relevant trades. importantdetermining value even single potential trade challenging problem complex domains (Sandholm & Boutilier, 2006; Compte & Jehiel, 2007). Moreover,bidders often wish reveal little information possible avoid leaking informationcompetitors. describing central design principles support ICE mechanism,highlight following aspects:bidder interacts ICE first defining structured representation valuation different trades. Defined tree-based bidding language (TBBL),concisely defines set trades interest bidder. bidder must annotatetree initial lower upper bounds value different trades.lower upper bounds valuations allows exchange identifyprovisional trade provisional payments round, generate provisional clearing price item market. round ICE, bidderrequired tighten bounds TBBL bid make precise tradepreferred given current prices.ICE hybrid demand-revealing process direct-revelation mechanism, simple (linear) prices guiding preference elicitation bids submitteddirect claims valuation functions TBBL language,expressive bids finally used clear exchange.ICE terminates, payment rule used determine payments made,received, participant. suggesting payments defined wayseeks mitigate opportunities manipulation exchange, ICE agnosticparticular payment rule adopted. given rule, prices quotedround defined part approximate payments, aggregated acrossprovisional trade suggested bidder. concreteness, adopt Thresholdrule (Parkes et al., 2001) defining final payments, minimizes ex post regrettruthful bidding across budget-balanced payment rules, holding bidsparticipants fixed; see also work Milgrom (2007). 1 say1. aware existence mechanism design solutions approximately efficient, truthful(i.e., truthful bidding dominant-strategy equilibrium) budget-balanced, sealed bid (i.e.,non iterative) CEs. Nevertheless, true payment rules developed leverageddirectly within ICE would allow ICE inherit truthful bidding (i.e., revising TBBL boundsremain consistent bidders true valuation) ex post Nash equilibrium, achievediterative Generalized Vickrey auctions (Mishra & Parkes, 2007).34fiICE: Iterative Combinatorial Exchangeincentive issues related payment rules important design successful CEs.Rather, orthogonal design ICE main focus work.propose novel activity rules, designed mitigate opportunitiesstrategic behavior.highlight following technical contributions made work:tree-based bidding language (TBBL) extends earlier CA bidding languages support bidders wish simultaneously buy sell, specification valuationbounds, use generalized choose operators provide concise representations OR* LGB (Boutilier & Hoos, 2001; Nisan, 2006). TBBLdirectly encoded within mixed-integer programming (MIP) formulationwinner determination problem.Despite quoting prices items bundles items, ICE able convergeefficient trade straightforward (i.e., non-strategic) bidders. Efficiencyestablished duality theory prices sufficiently accurate. Otherwise,direct proof based reasoning upper lower valuation bounds alwaysavailable, even combinatorics instance preclude duality-based proof.Preference elicitation performed combination two novel activity rules.first modified revealed-preference activity rule (MRPAR), requiresbidder make precise trade preferred round. seconddelta improvement activity rule (DIAR), requires bidder refine bidimprove price accuracy prove improvement possible. coupledtogether rules ensure useful progress towards determining efficient trademade round.summarize, three main reasons prefer explicit value representationsrepeated demand reports context iterative CE: (a) provisional allocationcomputed round 1, since upper lower bounds value available, (b)combinatorics domain directly handled clearing exchange efficiencylimited adopting simple (linear) prices, (c) proofs (approximate) efficiencyavailable reasoning directly bounds valuations despite adopting simple(linear) prices.exchange fully implemented Java (with C-based MIP solver). presentscalability results showing performance across wide number bidders, goods valuation complexity well benchmarks provide qualitative understandingcharacteristics mechanism. experimental results (with straightforward bidders)show exchange quickly converges efficient trade, taking average7 rounds example domain 100 goods 20 different types 8 biddersvaluation functions containing average 112 TBBL nodes. domain,find bidders leave upwards 62% maximum attainable value undefinedefficient trade known, 56% final payments determined, indicatingbidders able leave large amounts value space unrefined. exchangeterminates problems average 8.5 minutes 3.2GHz dual-processor dualcore workstation 8GB memory. includes time winner determination,pricing, activity rules, well time simulate agent bidding strategies.35fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes1.1 Related WorkMany ascending-price one-sided CAs known literature (Parkes & Ungar, 2000a;Wurman & Wellman, 2000; Ausubel & Milgrom, 2002; de Vries, Schummer, & Vohra,2007; Mishra & Parkes, 2007). Direct elicitation approaches, bidders respondexplicit queries valuations, also proposed one-sided CAs (Conen &Sandholm, 2001; Hudson & Sandholm, 2004; Lahaie & Parkes, 2004; Lahaie, Constantin, &Parkes, 2005). particular relevance ascending CAs designed worksimple prices items (Dunford, Hoffman, Menon, Sultana, & Wilson, 2003; Kwasnica,Ledyard, Porter, & DeMartini, 2005). computing (approximately competitive) linearprices, generalize extend methods. Building work Rassenti, Smith,Bulfin (1982), earlier papers consider bids bundles individually, find pricesexact winning bids minimize pricing error losing bids. GeneralizingTBBL expressive language, propose instead compute prices minimizeworst-case pricing error bidders (rather bids individual trades), consideringpreferred trade consistent TBBL bid bidder. workDunford et al. (2003) Kwasnica et al. (2005) incorporate additional tie-breakingstages, case lexicographically minimize error find prices closelyapproximate provisional payments. latter step appears novel.Linear prices important practical applications. prices adoptedFCC wireless spectrum auctions (Cramton, 2006), within clock auctionsprocurement electricity generation (Cramton, 2003), essential partproposed design airport landing slot auction Laguardia airport (Ball et al. 2007).Linear competitive equilibrium prices exist two-sided markets indivisibilitiesassignment problem agent buy sell single item (but mayinterested multiple different items) (Shapley & Shubik, 1972). general linear,competitive equilibrium prices exist combinatorial markets nonconvexities;see work Kelso Crawford (1982), Bikhchandani Mamer (1997), BikhchandaniOstroy (2002), ONeill, Sotkiewicz, Hobbs, Rothkopf, Stewart (2005) relateddiscussions.ICE proxied architecture sense bidders submit refine boundsTBBL bids directly exchange, information used drive price dynamicsultimately clear exchange. Earlier work considered proxied approaches,application one-sided ascending-price CAs (Parkes & Ungar, 2000b; Ausubel &Milgrom, 2002). Given focus simple, linear prices, ICE considered providetwo-sided generalization clock-proxy design Ausubel, Cramton, Milgrom,initial stage linear price discovery followed best-and-final sealedbid stage (Ausubel et al., 2006). Activity rules shown importantpractice. instance, Milgrom-Wilson activity rule requires bidderactive minimum percentage quantity spectrum eligiblebid critical component auction rules used FCC wireless spectrumauctions (Milgrom, 2004). ICE adopts variation clock-proxy auctions revealedpreference activity rule.well known exact efficiency together budget balance possibleMyerson-Satterthwaite impossibility result (Myerson & Satterthwaite, 1983). Given36fiICE: Iterative Combinatorial Exchangethis, Parkes et al. study sealed-bid combinatorial exchanges introduced Thresholdpayment rule (Parkes et al., 2001); see work Milgrom (2007) Day Raghavan (2007) recent discussion. Double auctions truthful bidding dominant strategy equilibrium known unit demand settings (McAfee, 1992) alsoslightly expressive domains (Babaioff & Walsh, 2005; Chu & Shen, 2007). However,truthful, budget-balanced mechanisms useful efficiency properties knowngeneral CE problem.Voucher-based schemes proposed alternative method extend onesided CAs exchanges (Kwerel & Williams, 2002). mechanisms collect goodssellers run one-sided auction sellers buy-backgoods vouchers used provide seller share revenue collectedgoods. Although voucher-based schemes facilitate design exchangesone-sided auction technology, ICE design offers nice advantage providing equalsymmetric expressiveness participants. aware previous studiesfully expressive iterative CEs. Smith, Sandholm, Simmons previously studied iterative CEs, handle limited expressiveness adopt direct-query based approachenumerative internal data structure scale (Smith et al., 2002).novel feature earlier design (not supported here) item discovery, itemsavailable trade need known advance. Earlier work also considered sealed-bidcombinatorial exchanges purpose contingent trades financial markets, includingaspects expressiveness winner determination (Saatcioglu et al., 2001).Several bidding languages CAs previously proposed, arguablycompelling allow bidders explicitly represent logical structure valuation goods via standard logical operators. refer logical biddinglanguages (Nisan, 2006). Closest generality TBBL LGB language (Boutilier &Hoos, 2001), allows arbitrarily nested levels, combining goods tradesstandard propositional logic operators, also provides k-of operator, used representwillingness pay k trades quantifies over; see also work Rothkopf, Pekec,Harstad (1998) restricted tree-based bidding language. key insight, Boutilierspecifies MIP formulation Winner Determination (WD) using LGB , provides positive empirical performance results using commercial solver, suggesting computationalfeasibility moving expressive logical language (Boutilier, 2002). TBBL sharesstructural elements LGB language important differences semantics. LGB , semantics propositional logic, itemsallocation able satisfy tree multiple places. Although make LGB especiallyconcise settings, semantics propose provide representational locality,value one component tree understood independently resttree.1.2 OutlineSection 2 introduces preliminary concepts, defining efficient trade competitive equilibrium prices. Section 3 defines sealed-bid CE, introducing TBBL providing MIPused solve winner determination. Section 4 extends TBBL allow valuationbounds defines MRPAR DIAR activity rules. main theoretical results37fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkesalso described well method determine price feedback round. Section 5gives number illustrative examples operation ICE. Section 6 presents mainexperimental results. conclude Section 7. Appendix provides algorithmtwo activity rules together details bidding logic used simulatedbidding agents.2. Preliminariesbasic environment considers set bidders, N = {1, . . . , n}, interestedtrading multiple units distinct, indivisible goods, set different types goodsdenoted G = {1, . . . , m}. bidder initial endowment goods valuationdifferent trades. Let x0 = (x01 , . . . , x0n ) denote initial endowment goods,x0i = (x0i1 , . . . , x0im ) x0ij Z+ indicate number units good type j Ginitially held bidder N . trade = (1 , . . . , n ) denotes change allocation,= (i1 , . . . , imP)ij Z denoting change number units item jPbidder i. Let = jG x0ij denote total supply exchange. writedenote bidder active trade, i.e., buys sells least one item.2.1 Efficient Tradebidder value vi (i ) R component trade . value positivenegative, represents change value final allocation x0i +initial allocation x0i . valuation initial allocation information privatebidder, assume externalities, bidders value dependsindividual trade. assume free disposal,P vi (i ) vi (i ) trade, i.e., ij ij j. Let v() = vi (i ).Utility modeled quasi-linear, ui (i , p) = vi (i ) p trade paymentp R. implies bidders modeled risk neutral assumesbudget constraints. payment, p, negative, indicating biddermay receive payment trade. use term payoff interchangeably utility.quasi-linearity, Pareto optimal (i.e., efficient) trade maximize socialwelfare, equivalent total increase value bidders due trade.Given instance CE problem, defined tuple (v, x0 ), i.e., valuation profilev = (v1 , . . . , vn ) initial allocation x0 = (x01 , . . . , x0n ), efficient trade , definedfollows:Definition 1 Given CE instance (v, x0 ), efficient trade solvesXmaxvi (i )(1 ,...,n )s.t.(1)ij + x0ij 0,Xij = 0,i, j(2)j(3)ij ZConstraints (2) ensure bidder sells items initial allocation.free disposal, impose strict balance supply demand goods38fiICE: Iterative Combinatorial Exchangesolution constraints (3), i.e., allocate unwanted items bidder. adoptF(x0 ) denote set feasible trades, given constraints given initialallocation x0 , Fi (x0 ) set feasible trades bidder i. Note valuationfunction vi cannot explicitly represented value possible trade bidder i,number trades scales O(sm ), maximal number unitsitem market different items. TBBL language (introducedSection 3) leads concise formulation efficient trade problem mixed-integerprogram.initial allocation x0i may private agent i. assume throughout bidderstruthful revealing information, motivate supposing participantscannot sell items actually (or pay suitably high penalty do).2.2 Competitive Equilibrium PricesLinear prices, = (1 , . . . , ), definePa price j good price biddertrade defined p (i ) = j ij j = . prices play important roleICE. particular interest set competitive equilibrium prices:Definition 2 Linear prices competitive equilibrium (EQ) prices CE problem(v, x0 ) feasible trade F(x0 ) that:vi (i ) p (i ) vi (i ) p (i ),Fi (x0 ),(4)every bidder i. say trade, , supported prices .Theorem 1 (Bikhchandani & Ostroy, 2002) trade supported competitive equilibrium prices efficient trade.practice, exact EQ prices unlikely exist. Instead, useful defineconcept approximate EQ prices approximately efficient trade:Definition 3 Linear prices -approximate competitive equilibrium (EQ) pricesCE problem (v, x0 ) R0 , feasible trade F(x0 ) that:vi (i ) p (i ) + vi (i ) p (i ),Fi (x0 ),(5)every bidder i.-approximate EQ prices, trade every bidder within 0maximizing utility. Furthermore, say trade z-approximate total valuetrade within z total value efficient trade.Theorem 2 trade supported -approximate EQ prices 2 min(M, n2 )approximate efficient trade.Proof: Fix instance (v, x0 ) consider (, ). trade 6=XX[vi (i ) p (i )],[vi (i ) p (i ) + ]39(6)fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes-EQ pricesvalues Pprices zero bidders PnotPparticipateP=( ) = 0 (since( ) =ppptrade.=PPiPP P0 )). Then,ij = 0, ij = 0 j, F(xP= j j PPi j ij jP(i ) + vi (i ). Fix := , efficient trade . Then, vi (i ) +viPvi (i ),=Xnmin(2A#(x0 ), n) min(2 min(M, n), n) = 2 min(M, )2(7)A#(x0 ) maximal number bidders trade feasible trade given x0 .second inequality follows bidders trade numbergoods trade bidders market thus A#(x0 ) min(M, n).3. Step One: TBBL-Based Sealed-Bid Combinatorial Exchangefirst flesh details non-iterative, TBBL-based CE biddersubmits sealed bid TBBL language.Bidding language. tree-based bidding language (TBBL) designed expressiveconcise, entirely symmetric respect buyers sellers, easily providebidders buying selling goods; i.e., ranging simple swaps highlycomplex trades. Bids expressed annotated bid trees, define bidders changevalue possible trades. main feature TBBL general intervalchoose logical operator internal nodes coupled rich semantics propagatingvalues within tree. Leaves tree annotated traded items nodesannotated changes values (either positive negative). TBBL designedchanges value expressed trades rather total value allocations.Examples provided Figures 1 2.Consider bid tree Ti bidder i. Let Ti denote node tree, letvi () R denote value specified node (perhaps negative). Let Leaf (Ti ) Tisubset nodes representing leaves Ti let Child () Ti denote childrennode . nodes except leaves labeled interval-choose operator ICyx ().leaf labeled buy sell, units qi (, j) Z good j associatedleaf , qi (, j ) = 0 otherwise. good j may simultaneously occur multipleleaves tree, given semantics tree described below.IC operator defines range number children be, mustbe, satisfied node satisfied: ICyx () node (where x non-negativeintegers) indicates bidder willing pay satisfaction least xchildren. suitable values x operator include manylogical connectors. instance: ICnn () node n children equivalentoperator; ICn1 () equivalent operator; IC11 () equivalent XORoperator.2say satisfaction ICyx () node defined following two rules:2. equivalence implies TBBL directly express XOR, XOR/OR languages (Nisan,2006).40fiICE: Iterative Combinatorial ExchangeR1 Node ICyx () may satisfied least x childrensatisfied.R2 node satisfied, none children may satisfied.One consider R1 first pass defines set candidates satisfaction.candidate set refined R2. Besides defining value propagated, virtueR2 logical operators act constraints trades acceptable providenecessary sufficient conditions.3Given tree Ti , (change in) value trade defined sum valuessatisfied nodes, set satisfied nodes chosen provide maximal totalvalue. Let sat () {0, 1} denote whether node tree Ti bidder satisfied,sat = {sat (), Ti }. solution sat valid tree Ti trade , writtensat valid (Ti , ), rules R1 R2 must hold internal nodes {Ti\Leaf (Ti )}ICyx ():x sat ()Xsat ( ) sat ()(8)Child()Equation (8) enforces interval-choose constraints, ensuringless appropriate number children satisfied node satisfied.constraint also ensures time node root satisfied, parentalso satisfied. require, sat valid (Ti , ), total increase quantityitem across satisfied leaves greater total number units awardedtrade:Xqi (, j)sat () ij ,j G(9)Leaf (Ti )free disposal, allow trade assign additional units item overand-above required order activate leaves bid tree. works sellerswell buyers: sellers trade negative requires total numberitems indicated sold tree least total number items traded awaybidder trade.Given constraints, total value trade , given bid-tree Ti bidder i,defined solution optimization problem:vi (Ti , ) = maxsatXvi ()sat ()(10)Tis.t. (8), (9)Example 1 Consider airline operating slot-controlled airport already ownsseveral morning landing slots, none evening. order expand businessairline wishes acquire least two possibly three evening slots. However,needs offset cost purchase selling one morning slots. Figure 1 showsTBBL valuation tree expressing kind swap.41fiLubin, Juda, Cavallo, Lahaie, Shneidman & ParkesIC32Buy 4pm $1Buy 6pm $4XORBuy 8pm $3Sell 5am $-2Sell 7am $-9Sell 9am $-5Figure 1: simple TBBL tree airline interested trading landing slots.working numerous examples frequently found cumbersomecapture even simple trades languages specified values allocations, caseexisting languages. Indeed, earlier work, demonstrate natural instancesTBBL exponentially concise OR* LGB (Cavallo et al. 2005).fact, TBBLs conciseness incomparable OR* LGB extended simpleways strictly dominate earlier languages.Winner Determination. problem determining efficient trade given bidscalled winner determination (WD) problem. WD problem CAs (and thus alsoCEs) NP-hard (Rothkopf et al., 1998). approach adopt formulate problem mixed-integer program (MIP), solve branch-and-cut algorithms (Nemhauser & Wolsey, 1999). similar approach proved successful solvingWD problem CAs (de Vries & Vohra, 2003; Boutilier, 2002; Sandholm, 2006).Given tree Ti , useful adopt notation denote node Tisatisfied trade . formulate WD problem bid trees = (T1 , . . . , Tn )initial allocation x0 :WD(T, x0 ) : max,satXXvi ()sat ()Tis.t. (2), (3)sat valid (Ti , ),sat () {0, 1}, ij Z,sat = (sat 1 , . . . , sat n ). tree structure made explicit MIP formulation:decision variables represent satisfaction nodes capture logicTBBL language linear constraints; related approach approachconsidered application LGB (Boutilier, 2002). this, O(nB + mn)variables constraints, B maximal number nodes bid tree.formulation determines trade simultaneously determining value biddersactivating nodes bid trees.Payments. Given reported valuation functions v = (v1 , . . . , vn ) bidder,Vickrey-Clarke-Groves (VCG) (e.g. Krishna, 2002) mechanism collects following pay3. R1 naturally generalizes approach taken LGB , internal node satisfied accordingoperator subset children satisfied. semantics LGB , however, treat logicaloperators way specifying added value (positive negative) results attainingcombinations goods. use R2 also imposes constraints acceptable trades.42fiICE: Iterative Combinatorial Exchangements bidder:pvcg,i = vi (i ) (V (v) Vi (v)),(11)efficient trade, V (v) reported value trade Vi (v)reported value efficient trade economy without bidder i, vi =(v1 , . . . , vi1 , vi+1 , . . . , vn ). Let us refer vcg,i = V (v) Vi (v) VCG discount.problem VCG mechanism context CE may run budget deficit total payments negative. alternative payment method providedThreshold rule (Parkes et al., 2001):pthresh,i = vi (i ) thresh,i ,(12)discounts thresh,i Ppicked minimize maxi (vcg,i thresh,i ) subjectthresh,i vcg,i thresh,i V (v). Threshold payments exactly budgetbalanced minimize maximal deviation VCG outcome across balancedrules.Bidder 1Bidder 2IC31XORBuy C $6Buy $10Sell $-4Buy B $5Sell C $-3Sell B $-8Figure 2: Two bidders three items {A, B, C}. efficient trade bidder 1 sellbuy C.Example 2 Consider two bidders Figure 2. Bidder 1 potentially sell oneitems (A B) get Bidder 2s item, C, right price. Bidder 2 interestedbuying one Bidder 1s items also selling item. considerpossible trades: Bidder 1 trades C gets $2 value Bidder 2 gets $7.Bidder 1 trades B C gets $-2 value Bidder 2 gets $2. trade occursbidders get $0 value. Therefore efficient trade swap C.efficient trade creates surplus $9 removing either bidder resultsnull trade, bidders Vickrey discount $9. Thus use VCG payments,Bidder 1 pays $2-$9=$-7 Bidder 2 pays $7-$9=$-2 exchange runs deficit.Threshold payment rule chooses payments minimally deviate VCGmaintaining budget balance. minimization reduces discounts $4.50, thusBidder 1 pays $2-$4.50=$-2.50 Bidder 2 pays $7-$4.50=$2.50.4. Step Two: Making Exchange Iterativedefined sealed-bid, TBBL-based exchange modify design makeiterative. Rather provide exact valuation interesting trades, bidder43fiLubin, Juda, Cavallo, Lahaie, Shneidman & ParkesFigure 3: ICE system overviewannotates single TBBL tree upper lower bounds valuation. ICEmechanism proceeds rounds, illustrated Figure 3.ICE proxied design bidder proxy facilitate valuationrefinement. round, bidder responds prices interacting proxy agentorder tighten bounds TBBL tree meet activity rules. exchangechooses provisional valuation profile (denoted v = (v1 , . . . , vn ) figure),valuation vi bidder picked fall within bidders current valuation bounds(and tend towards lower valuation bound progress made towards determiningfinal trade). Then, exchange computes provisional trade checks whetherconditions moving last-and-final round satisfied. Approximate equilibriumprices computed based valuation profile v trade new roundbegins. last-and-final round, final payments trade computed termslower valuations; semantics lower bounds guarantee bidderwilling pay least amount (or receive payment amount) ordercomplete trade.Let v v denote lower upper valuation functions reported bidderparticular round ICE, adopt WD(v) denote WD problem valuationprofile v = (v1 , . . . , vn ). ICE parameterized target approximation error (0, 1],requires total value optimal trade given current lower-boundvaluation profile (i.e., solves WD(v)) close total value efficient trade :Pv()vi (i )PEFF() =) = v( )v((13)However, true valuation v thus trade uncertain within ICE thuslater introduce techniques estblish bound.round, ICE goes following steps:1. last-and-final round, implement trade solves WD(v)collect Threshold payments defined valuations v. STOP.ELSE,44fiICE: Iterative Combinatorial Exchange2. Solve WD(v) obtain . Use valuation bounds prices determine lowerbound, eff , allocative efficiency EFF() . eff nextround designated last-and-final round.3. Set [0, 1], tending 1 eff tends 1, provisional valuation profilev = (v1 , . . . , vn ), vi (i ) = v (i )+(1)v (i ), expressed TBBL treevalue node Ti vi () = v () + (1 )v ().4. Solve WD(v ) find provisional trade , determine Threshold paymentsprovisional valuation profile, v .5. Compute linear prices, Rm0 , approximate CE prices given valuations vtrade , breaking ties best approximate provisional Threshold paymentsfinally minimize difference price items.6. Report (i , ) bidder N , whether next round last-andfinal.transitioning next round, proxy agents responsible guiding biddersmake refinements lower- upper-bound valuations order meet activityrules ensure progress towards efficient trade across rounds. follows, (a)extend TBBL capture lower upper valuation bounds, (b) describe two activityrules, (c) explain compute price feedback, (d) provide main theoretical results.developing theoretical experimental results ICE assume straightforwardbidders, bidders refine upper lower bounds valuations keep truevaluation consistent bounds.Extending TBBL. first extend TBBL allow bidder report lower upperbound (v (), v ()) value node Ti , turn induces valuationfunctions v (Ti , ) v (Ti , ), using exact semantics (10). boundstrade interpreted bounding payment bidder considers acceptable.bidder commits complete trade payment less equal lowerbound refuse complete trade payment greater upper-bound.exact value, thus true willingness-to-pay, remains unknown except v () = v ()nodes. say bid-tree Ti bidder well-formed v () v ()nodes Ti . case also v (Ti , ) v (Ti , ) trades . referdifference v () v () value uncertainty node . efficient tradeoften determined partial information bidder valuations. Considerfollowing simple variant Example 2:Example 3 structure bidders trees Figure 4 Example 2nodes annotated bounds. Let x [3, 8] denote Bidder 1s true value buyC [4, 1] denote Bidder 2s true value sell C. three feasible tradesare: (1) trade C, (2) trade B C, (3) trade. first trade already provablyefficient. Fixing x y, minimal value 4+9+xy least 5+7+xy,value second trade. Moreover, worst-case value 4 + 9 + 3 4 0, valuenull trade.45fiLubin, Juda, Cavallo, Lahaie, Shneidman & ParkesBidder 1Bidder 2IC31XORBuy C $8$3Buy $12$9Sell $-3$-4Sell BBuy B $7$3Sell C $-1$-4$-5$-10Figure 4: Two bidders, partial value information defined bid tree. Onealready prove efficient trade bidder 1 sell buy C.4.1 Activity RulesActivity rules used guide preference elicitation process round ICE.Without activity rule, rational bidder would likely wait last momentrevise valuation information, free-riding price discovery enabled bidsparticipants. every bidder behave way exchange would reducesealed-bid mechanism lose desirable properties.4 Thus, activity rules criticalmitigating opportunities strategic behavior.5ICE employs two activity rules. presenting activity rules, specifyexplicit consequences failing meet activity rule. One simple possibilitydefault action automatically set upper valuation bound every node bidtree maximum provisional price node6 lower-bound valuenode. entirely analogous bidder ascending-clock auction stopsbidding price: permitted bid higher price future rounds.Modified Revealed-Preference Activity Rule (MRPAR). first rule, MRPAR,based simple idea. require bidders refine valuation bounds round,trade optimal (i.e., maximizes surplus) bidder givencurrent prices possible valuations consistent bounds. MRPAR looselybased around revealed-preference based activity rule, advocated clock-proxyauction one-sided CA (Ausubel et al., 2006).Let vi Ti TBBL tree Ti denote valuation vi consistent value boundstree. bounds tight everywhere, vi exactly valuation functiondefined tree Ti . simple variant (RPAR), requires enough informationvaluation bounds establish one trade weakly preferred tradesprices, i.e.Fi (x0 ) s.t. vi (i ) p (i ) vi (i ) p (i ),vi Ti , Fi (x0 )(RPAR)Note bidder always meet rule defining exact valuation vi tightvalue bounds every node bid tree; case, trade arg maxi Fi (x0 ) [vi (i )4. problem evocatively described snake grass problem. See Kwerels forwardMilgroms book (2004).5. conflict assumption straightforward bidding: design strategiccase despite assuming straightforward bidding provide tractable theoretical experimentalanalysis; moreover, presence activity rules helps motivate straightforward bidding.6. provisional price node defined minimal total price across feasible tradessubtree rooted node satisfied.46fiICE: Iterative Combinatorial Exchangep (i )] satisfies RPAR. say prices strict EQ prices (v , ) when:vi (i ) p (i ) > vi (i ) p (i ),Fi (x0 ) \ {i } ,(14)every bidder N .Theorem 3 prices strict EQ prices provisional valuation profile v trade, every bidder retains vi bid tree meeting RPAR, tradeefficient bidders straightforward.Proof: Fix bidder i. Let denote trade satisfies RPAR. vi consistentrevised bid tree bidder i, have:vi (i ) p (i ) vi (i ) p (i ),Fi (x0 ).(15)Moreover, must = , vi (i ) p (i ) > vi (i ) p (i )strictness prices. Instantiating RPAR trade, true valuations vi Ti(since bidders straightforward), have:vi (i ) p (i ) vi (i ) p (i ),Fi (x0 ),(16)prices p EQ prices respect true valuations. efficiency claimfollows welfare theorem, Theorem 1.particular, provisional trade efficient given strict EQ prices every biddermeets rule without modifying bounds way. Strict EQ prices requiredprevent problems involving ties:BuyerSellerXORBuy $8$4Buy B $4$2$-6Sell $-9$-20$-2Sell B $-6$-10Figure 5: Example illustrate failure simple RPAR rule without strict EQprices. True values shown bold efficient outcometrade.Example 4 TBBL trees shown Figure 5 trade occur truthfulvaluation (which indicated bold value bounds). However, suppose = 0provisional valuations efficient traded. Prices = (6, 2) EQ(but strict EQ) prices given v , buyer indifferent buyingbuying B seller indifferent selling A, selling B, making sale.buyer passes RPAR without changing bounds bounds already establish(weakly) prefers B, prefers trade, possible valuations.Similarly, seller passes RPAR without changing bounds bounds establishweakly prefers trade selling combination B given currentprices. Thus, activity even though current provisional trade inefficient.47fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkesorder better handle sorts ties, slightly strengthen RPAR modifiedRPAR (MRPAR), requires exists Fi (x0 )(i , , vi ) 0,either =(i , , vi )> 0,vi Ti , Fi (x0 )viTi .(17)(18)(i , , vi ) = vi (i ) p (i ) (vi (i ) p (i )) denotes profit biddertrade given vi prices . (17) RPAR additional requirements enforcesatisfying trade either strictly preferred . need showstrict preference prevents deadlock shown Example 4. seller shownweak preference trading selling A. MRPAR, seller must alsoshow strictly prefers , case reducing upper-boundsB, thus ensuring progress.actual rule adopted ICE -MRPAR, parameterized accuracy parameter0, providing relaxation MRPAR useful even exactEQ prices defined respect ( , v ) round.Definition 4 Given provisional trade , linear prices , accuracy parameter 0,-MRPAR requires every bidder refines value bounds TBBL tree Tisatisfies:(i , , vi ) ,vi Ti , Fi(19)or, Fi (x0 )(i , , vi ) 0,vi Ti , Fi (x0 )(20)(i , , vi )vi(21)> ,Tisimple matter check -MRPAR reduces MRPAR = 0. Phrasingdescription allow rule interpreted without relaxation,-MRAPR requires bidder must adjust valuation bounds establishprovisional trade [within ] maximizing profit possible valuations (19),trade satisfies RPAR (20) strictly preferred [by least ] provisionaltrade (21). RPAR, one show bidder always meet -MRPAR (for) defining exact valuation.7Lemma 1 every bidder meets -MRPAR without precluding vi updated bidtree, prices -approximate EQ prices respect provisional valuation profilev trade , bidders straightforward, provisional trade2 min(M, n2 )-approximate efficient trade.7. Let vi denote valuation. -MRPAR satisfied via (19) arg maxi Fi (x0 ) [vi (i )p ()] satisfy -MRPAR. satisfies (20) construction. Now, let denote tradevi (i ) p (i ) > vi () p (i ) + . vi (i ) p (i ) vi (i ) p (i ) > vi (i ) p (i ) + ,(21).48fiICE: Iterative Combinatorial ExchangeProof: Fix bidder i. -EQ, (i , , vi ) Fi (x0 ). Consider6= . vi remains bid tree, must (i , , vi )-MRPAR cannot satisfied via (20) (21). Therefore, -MRPAR satisfied everybidder via (19) provisional trade satisfying trade. Therefore proveprices, , -approximate EQ prices valuations, including true valuationsince bidders straightforward within bounds. efficiency tradefollows Theorem 2.turn provides simple proof efficiency ICE approximate CEprices exist upon termination. Suppose ICE defined terminate soon prices-accurate v retained bid tree bidders meeting activity rule,quiescence reached bidder refines bounds meeting rule.variation, provisional trade trade finally implemented.Theorem 4 ICE -MRPAR 2 min(M, n2 )-efficient prices -accuraterespect (v , ) upon termination bidders straightforward.Proof: ICE terminates either (a) prices -accurate v retainedbid tree bidders appeal directly Lemma 1, (b) bidder refinesbounds meeting -MRPAR, case vi remains space valuations consistentbid tree bidder.also following simple corollary, considers property ICEdomain approximately accurate EQ prices exist:Corollary 1 ICE -MRPAR 2 min(M, n2 )-efficient -accurate competitiveequilibrium prices exist valuations valuation domain biddersstraightforward.Specializing domains exact EQ prices exist (e.g., unit-demand preferencesassignment model Shapley Shubik, 1972; see also work BikhchandaniMamer, 1997) ICE MRPAR efficient straightforward bidders.XORBuy $8$2Buy B $5$4XORBuy $8=v$2=x(a) Passes -MRPARBuy B $8=y$4=w(b) Fails -MRPARFigure 6: -MRPAR provisional trade Buy A, = 3, B = 4 = 2Example 5 illustrate -MRPAR rule consider single bidder valuation treeFigure 6(a). Suppose provisional trade allocates bidder, prices= 3, B = 4 = 2. bidder satisfied -MRPAR guaranteed$2-$3=$-1 payoff within possible $5-$4=$1 payoff B. considerFigure 6(b), relaxed upper-bound buy B $8. bidder fails -MRPARguaranteed $-1 payoff within possible payoff B$8-$4=$4. Let [x, v] [w, y] denote lower upper bounds, buy buyB respectively, revised meeting rule. pass rule, bidder two choices:49fiLubin, Juda, Cavallo, Lahaie, Shneidman & ParkesDemonstrate best response. bidder need adjust xmake x 3 4 2 x 3; e.g., values x = $2, = $5 solve this,Figure 6(a), many possibilities.Demonstrate another trade (e.g., buy B) $2 better ,i.e., w 4 > v 3 + 2 w v > 3, buy B weakly better null trade,i.e., w 4 0. instance, bidders true values vA = $3, vB = $8x 3 v w 8 rule cannot satisfied first case. But,buyer establish buy B best-response, e.g., setting v = $4, w = $7,v = $3, w = $6.Remark: Computation Bidder Feedback. definition MRPAR naivelysuggests checking compliance requires explicitly considering valuations vi Titrades Fi (x0 ). Fortunately, necessary. present Appendixmethod check MRPAR given prices , provisional trade bid tree Ti solving threeMIPs. Moreover, explain solution MIPs also provides nice feedbackbidders. ICE automatically identify set nodes bidder needs increaselower bound set nodes bidder needs decrease upper boundmeeting MRPAR.Delta Improvement Activity Rule (DIAR). -MRPAR, quite possibleICE get stuck, bidders satisfying activity rule without changingbounds, prices less accurate (with respect ( , v )). Therefore,need activity rule continue drive reduction value uncertainty, i.e.,gap upper bound values lower bound values, even face inaccurateprices, ideally way remains price-directed sense using pricesdetermine trades (and turn nodes TBBL trees) bidderfocused on.introduce purpose second (and novel) activity rule (DIAR), fillsrole requiring bidders reveal information improve price accuracy and,limit, full information nodes matter. Defined way, DIAR rulenicely complements -MRPAR rule. establish efficiencyprovisional trade directly via valuation bounds, see Section 4.3,actually need fully accurate prices order close exchange. Thus, DIARrule imply bidders reveal full information. Rather, presence DIARensures good performance practice well good theoretical properties.experiments enable DIAR rounds ICE, fires parallel -MRPAR.practice, see progress refining valuation information occurs due-MRPAR, progress early rounds occurs due -MRPAR. Experimentalsupport provided Section 6.8providing specifics DIAR, identify node Ti bid treebidder interesting fixed instance (v, x0 ), node satisfiedfeasible trade. following simple lemma:8. variation way ICE defined, DIAR could used rounds price errorprovisional valuation trade greater error associated -MRPAR.-MRPAR sufficient approximate efficiency prices accurate enough.50fiICE: Iterative Combinatorial ExchangeLemma 2 value uncertainty interesting nodes bid treesbiders, bidders straightforward, efficient.Proof: value uncertainty thus exact information value interestingnodes implies difference value exactly known pairs feasibletrades uninteresting nodes, either node never satisfied trade(and thus value matter) node satisfied every trade thus actualvalue matter defining difference value pairs trades.difference value pairs trades important determining efficient trade.DIAR focuses bidder particular interesting nodes correspond tradespricing error large, error could still reduced refiningvaluation bounds node. Given prices provisional trade , main focuskDIAR following upper-bound , amount prices might mispricetrade ki Fi (x0 ) respect bidder true valuation:k= max[vi (ki ) p (ki ) (vi (i ) p (i ))]vi Ti(22)call DIAR error trade ki , note depends currentprices well current bid tree provisional trade, true valuationunknown center. DIAR error provides upper bound additional payoffbidder could achieve trade ki trade . order trades, 1i , 2i , . . .,11i maximal DIAR error, , = maxi Fi (x0 ) [vi (i ) p (i )(vi (i ) p (i ))] pricing error respect provisional trade provisionalvaluation profile. error pricing algorithm designed minimizeround, error used Theorem 2 reference -accurate prices.Thus, see maximal DIAR error also bounds amount prices1approximate EQ prices, 0 bidders current pricesexact EQ prices respect ( , v ).satisfy DIAR bidder must reduce DIAR error trade largest errorerror reduced (some error may intrinsic given current pricesuncertainty bidders valuation), establish providing exactvalue information throughout tree none DIAR error trades duevalue uncertainty. Figure 7 illustrates difference MRPAR DIAR.bidder satisfy MRPAR making clear lower bound payofftrade greater upper bound trades, still leave large uncertaintyvalue. DIAR requires bidder also refine upper bound nodecorresponds trade DIAR error (and thus potentially actualapproximation prices) large. rule illustrated Figure 8.DIAR parameterized 0. refer formal rule -DIAR:Definition 5 satisfy -DIAR given provisional trade prices , bidder mustmodify valuation bounds to:(a) reduce DIAR error trade, ji Fi (x0 ), leastk(b) prove error cannot improved trades ki Fi (x0 ) 1 k < j,k(c) establish cannot improved trade ki Fi (x0 ).51fiLubin, Juda, Cavallo, Lahaie, Shneidman & ParkesFigure 7: Stylized effect MRPARDIAR boundstradesFigure 8: Trades bidder i, orderedDIAR error reducing leftright. bidder must reduce,least , DIAR errortrade greatesterror possibleprove (via valuation bounds)impossible improvetrades larger error.particular, even bidder case (c) above, still forced narrowbounds progress made towards bounding efficiency. practice, defineparameter large start smaller later rounds.XOR$6Buy $4$2$8Buy B $5$3XOR$10Buy C $10$4$6Buy $4$2(a) Fails DIAR$7Buy B $5$3Buy C$10$10$9.01(b) Passes DIARFigure 9: Respecting DIAR provisional trade Buy A, = 4, B = 5, C = 6= 1Example 6 Consider tree Figure 9(a) provisional trade buy A, prices= ($4, $5, $6) DIAR parameter = 1. DIAR error trade, defined via(22), listed decreasing order, are:1C = ($10 $6) ($2) = $62B = ($8 $5) ($2) = $53= ($0 $0) ($2) = $24= ($2 $4) ($2) = $0,$2 = $2 $4 worst-case profit provisional trade. Now, see1cannot made smaller lowering upper-bound leaf buy C bound52fiICE: Iterative Combinatorial Exchangealready tight truthful value $10. Instead bidder must demonstratedecrease = 1 impossible raising lower bound buy C 9.01. However2decreased = 1, reducing upper-bound buy B 8 7, givingus tree Figure 9(b).Lemma 3 ICE incorporates DIAR, straightforward bidder must eventually revealcomplete value information interesting nodes bid tree 0.Proof: Fix provisional trade consider trade, 1i Fi (x0 ) 6= , maximalDIAR error. Continue assume straightforward bidders. Recall vi () denotesbidders true value node TBBL tree. case analysis nodes Ti , meetingDIAR rule trade 0 requires:(i) Nodes 1i \ . Decrease upper-bound vi (), true value, reduceerror. Increase lower-bound vi () prove progress possible.(ii) Nodes \ 1i . Increase lower-bound vi (), true value, reduceerror. Decrease upper-bound vi () prove progresspossible.(iii) Nodes 1i . change required.(iv) Nodes/ 1i . change required.Continue fix , consider impact DIAR 0rule met successive trades, moving 1i 2i onwards. Eventually, valuebounds nodes/ least one feasible trade driven truth(i), value bounds nodes least one feasible tradedriven truth (ii). Noting null trade always feasible, bidderultimately reveal complete value information except nodes satisfiedfeasible trade.Putting together following simple theorem, considers convergence property ICE DIAR activity rule.Theorem 5 ICE -DIAR rule terminate efficient tradebidders straightforward 0.Proof: Immediate Lemma 2 Lemma 3.practice, use -MRPAR DIAR role DIAR ensure convergence instances exist good, supporting EQ prices. useDIAR lead, case, full revelation bidder valuationsprove efficiency directly terms valuation bounds different trades (see Section 4.3).Remark: Computation Bidder Feedback. present Appendix methodcheck -DIAR given prices , provisional trade , bidders bid tree pastround proposed new bid tree solving two MIPs. Moreover, solutionMIPs also provides nice feedback bidders. ICE automatically identify trade,turn corresponding nodes bid tree, bidder must provideinformation.53fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes4.2 Generating Linear PricesGiven provisional trade , provisional valuations v , given provisional payments also determined (according payment rule, Threshold,adopted exchange), approximate clearing prices computed round according following rules:I: Accuracy (ACC). First, compute prices minimize maximal errorbest-response constraints across bidders.II: Fairness (FAIR). Second, break ties prefer prices minimize maximaldeviation provisional payments across bidders.III: Balance (BAL). Third, break ties prefer prices minimize maximaldifference price across items.Taken together, steps designed promote informativeness pricesdriving progress across rounds. Balance well motivated domains itemslikely similar value dissimilar, preferring prices similar across itemsrejecting extremal prices. Note prices may ascend descend roundround general tend towards increasing accuracy, shall seeexperimentally Section 6.BuyerSellerBuy $8Buy B $8Sell $-6Sell B $-6Figure 10: simple example illustrate pricing. ACC prices AB $12 $16,FAIR narrows $14 BAL requires = $7, B = $7Example 7 Consider example Figure 10 one buyer interested buying ABone seller interested selling AB. buyers sellers values item8 -6 respectively. efficient outcome given values trade complete.ACC requires 12 +B 16, thus allows range prices. Threshold paymentsplits difference, buyer pays 14 seller FAIR adds constraint+ B = 14. Finally, BAL requires = B = 7.three stages occur turn. interest space, presentbasic formulation Accuracy stage: define maximally accurate EQ prices firstconsidering following LP:s.t.acc= min acc,accXXj ij vi (i )j ij + acc ,vi (i )jjacc 0,j 0,j G54i, Fi (x0 )(23)fiICE: Iterative Combinatorial Exchangeprices minimize maximal loss payoff across bidders trade compared trade bidder would prefer given provisional valuation v , i.e.,minimize maximal value (i , , vi ), = arg maxi Fi (x0 ) [vi (i ) p (i )].Prices solve LP refined lexicographically, fixing worst-case pricing error (ACC) working try additionally minimize next largest pricingerror on. Given maximally accurate prices, triggers series lexicographical refinements best approximate payments (FAIR) without reducing pricingaccuracy, eventually series lexicographical refinements try maximally balance prices across distinct items (BAL). addition improving qualityprices, process also ensures uniqueness prices.Accuracy, Fairness Balance problems exponential numberconstraints price accuracy constraints (23) (which carried forwardsubsequent stages) defined trades Fi (x0 ) bidders i.therefore infeasible even write problems down. Rather solve explicitly,use constraint generation (e.g. Bertsimas & Tsitsiklis, 1997) dynamically generatesufficient subset constraints. Constraint generation (CG) considers relaxed programcontains manageable subset constraints, solves optimality.Given solution relaxed program, subproblem used either provesolution optimal full program, find violated constraint full problemintroduced (now strengthened) relaxed program resolved. casesubproblem variation winner determination IP Section 3,concisely formulated solved via branch-and-cut.94.3 Establishing Bounds EfficiencyConsider round ICE. round starts announcement prices, denote, provisional trade. round ends every bidder met-MRPAR -DIAR activity rules. question address is: establishedefficiency trade defined lower-bound valuations end round?perhaps unsurprising MRPAR sufficient provide efficiency claimsprices suitably accurate. interesting coupling MRPARDIAR ensures ICE converges provably efficient trade cases,efficiency often established independently prices reasoning directly lowerupper valuation bounds. theoretical analysis convergence efficiency, assumestraightforward bidders, mean bidder always retains true valuationwithin valuation bounds. (All results could equivalently phrased terms efficiencyclaims respect reported valuations.)closing round, ICE makes determination whether movelast-and-final round. Bidders notified occurs. last-and-final round9. pricing step computationally intensive steps ICE therefore heavily optimized.practice, found useful employ heuristics seed set constraints used CG.also developed algorithmic techniques speed search appropriate set constraintscontext lexicographic refinement: provisional Locking multiple lexicographic valuesCG check, lazy constraint checks subset conditions CG routinelychecked, even though complete set eventually enforced. Please see technical report www.eecs.harvard.edu/~blubin/ice complete details pricing method.55fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkesprovides final opportunity bidders update lower valuation bound information(without exceeding upper bounds). exchange finally terminates efficienttrade payments determined respect lower valuation bounds:lower bounds considered bePultimate bid submitted bidderICE terminates. Let arg maxF (x0 ) v (i ) denote trade optimal givenlower bound valuations. explained Section 4, ICE parameterized targetapproximation error, , providing lower-bound relative efficiencyefficient trade true valuations. challenge obtain useful boundsrelative efficiency EFF() trade . provide two methods, one price-baseduses duality theory second directly reasons bounds biddervaluations. consider turn.price-based proof efficiency. already seen Section 2.2 boundefficiency provisional trade sometimes established via prices. providessimple method establish bound efficiency trade . Fix 0. vdenoting provisional valuation profile start round t, correspondingprovisional trade, know(a) bidders meet -MRPAR leaving v within bounds,(b) prices -approximate EQ prices v ,(c) equal , i.e., efficient trade given refined lower bound valuations,trade2 min(M, n2 )-approximationefficient trade Theorem 2.PPvi (i ) + 2 min(M, n2 ) vi (i ), then,P2 min(M, n2 )2 min(M, n2 )vi (i )PP,11EFF() = PmaxF (x0 ) v ()vi ( )vi (i )(24)define price . Conditioned (ac) met, bound available,satisfy price small enough parameter. bound availableset price := 0.direct proof efficiency. also provide complementary, direct, methodestablish relative efficiency working refined valuation boundsend round t. First, given bid tree Ti , useful define perturbed valuationrespect trade , assigning following values node :v () , sat (i )vi () =(25)v () , otherwise,sat (i ) node satisfied given tree Ti lower bound valuationsv nodes, given trade . valuation function vi associated TBBL tree Tidefined minimize value nodes satisfied trade maximize valuenodes. concept, given valuation bounds, establishfollowing bound,v()v()v ()v(),(26)min= min=EFF() =v( ) v T, F (x0 ) v ( )F (x0 ) v( )v()56fiICE: Iterative Combinatorial ExchangeFigure 11: Determining efficiency bound based lower upper valuations.define direct . Notation v = (v1 , . . . , vn ), trade maximizesPvi (i ) across feasible trades. first inequality holds domainminimization includes v trade = . first equality holds6= , worst-case efficiency occurs value v selected minimizevalue nodes \ , maximize value nodes \ , minimize valueshared nodes, . Whatever choice , valuation provided perturbedvaluation v. final equality, v() = v() definition, optimal trademaximizes value denominator, i.e., trade . Figure 11 schematicallyillustrates various trades values used bound, particular providesgraphical intuition v() v() v( ) v() = maxv [v ( ) v ()]v( ) v().Combining together. Given methods establish lower-bound eff =max( price , direct ) relative efficiency trade . ICE defined movelast-and-final round either following hold:(a) error bound eff(b) trade even optimistic (i.e., upper-bound) valuations.Combining Theorem 5, immediately get main result.Theorem 6 ICE incorporates -MRPAR -DIAR biddersstraightforward, exchange terminates trade within target approximation error , 0 0.use -DIAR sufficient establish result. However, useprices MRPAR drives elicitation practice, particularly fix-MRPAR tiny constant actual use. Empirical support this, alongquality price-based bound direct efficiency bounds, provided Section 6.parameter -DIAR, find simple rule::=1 X X v () v (),2n|Ti |(27)Tiworks well. tends towards zero value information revealed participants.One last element design ICE precise method provisionalvaluation profile v = v + (1 )v constructed. important57fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkesused determine provisional trade price feedback. simple approach workswell define := max(0.5, eff ). find lower bound 0.5 useful heuristicearly rounds eff likely small, making ICE adopt provisional valuationmiddle valuation bounds much known. effect pushtowards 1 thus v towards v efficiency bound eff improves.105. Illustrative Examplessection illustrate behavior exchange two simple examples.examples provided give qualitative feel behavior. construct examplespopulate ICE simple, automated bidding agents. agents use MIPguided heuristics minimize amount information revealed course passingactivity rules, maintaining true value within lower- upper-bounds(i.e., act straightforward way). reluctance reveal information modelsbasic tenet design, costly participants refine revealinformation values different trades. detailed explanation operationbidding agents provided Appendix.section, also presenting main experimental results, movelast-and-final round. Rather, bidding agents programmed continue improvebids past round efficiency already proved (and last-and-finalround would ordinarily declared), payments within desired accuracytolerance. avoid need program agents strategy bidlast-and-final round.Prices9.53.59BEfficient Allocation8.5382.57.5PessimisticAlphaOptimistic2PriceAverage Allocation ValueAllocation Value476.51.5615.5ProvableEfficiency0.5000.20.40.6% CompleteProvableEfficiency50.84.501(a) Allocative value0.20.40.6% Complete0.81(b) PricesFigure 12: AgentA: $8, AgentB: B $8, AgentAB: B $10.10. domains, may also important require payments (rather efficiencytrade ) accurate enough moving last-and-final round. bound paymentscomputed analogous way efficiency. Whether required practice likelydomain-specific depend, instance, whether payments tend accurate anywaytime trade approximately accurate, also impact strategic behavior.58fiICE: Iterative Combinatorial ExchangeExample 8 Consider market no-reserve seller two items B, threebuyers. AgentA demands value $8, AgentB demands B value $8,AgentAB demands B value $10. Figure 12(a) shows quicklyexchange discovers correct trade. price $5 $8 accuratesituation, see prices Figure 12(b) quickly meet condition.Fairness drives prices towards $6, eventual Threshold paymentsAgentA AgentB. Balance ensures prices remain two items.PricesAllocation Value250.7200.50.415PessimisticAlphaOptimistic0.3PriceAverage Allocation Value0.6BEfficient Allocation100.250.100ProvableEfficiencyProvableEfficiency0.20.40.6% Complete0.8001(a) Allocative value0.20.40.6% Complete0.81(b) PricesFigure 13: Seller -$10, Swapper: swap B $8, Buyer B $4Example 9 Consider example Seller offering reserve $10, Swapperwilling pay $8 swap B A, Buyer willing pay $4 B.complex example, takes 4 rounds, illustrated Figure 13(a), tradefound pessimistic trade. Revelation drives progress towards completed trade,see Figure 13(b), reflected falling prices goods. Thussee price feedback providing accurate information participants:price eventually becomes low enough buying bidders actually want tradeoccur also exchanges provisional trade switches. also worthnoting greater valuations Seller Swapper place good result nethigher price good B.6. Experimental Analysissection report results set experiments designed provideproof-of-concept ICE. results illustrate scalability ICE realistic problemsizes provide evidence effectiveness elicitation process techniquesbound efficiency provisional trade.59fiLubin, Juda, Cavallo, Lahaie, Shneidman & ParkesImplementation. First, brief aside experimental implementation. ICE approximately 20,000 lines extremely tight Java code, broken functional packagesdescribed Table 1.11 prototype modular researchers may easily replacecomponents experimentation.12 ICEs complexity, essentialcode constructed rigid hierarchy avoids obscuring high level logic behinddetails generating, running integrating results MIPs. end,system written series progressively abstractmini-languagesdefines clean, understandable API next higher level logic. hierarchy providesway hide extremely delicate steps needed handle numerical issues cometrying repeatedly solve coupled optimization problems, constraintsone problem may defined terms slightly inaccurate results earlier problem.constraints presented paper must carefully relaxed monitoredorder handle numerical precision issues. bottom hierarchyMIP specification fed generalized back-end optimization solver interface13 (wecurrently support CPLEX LGPL-licensed LPSolve), handles machine loadbalancing parallel MIP/LP solving. concurrent solving capability essential,need handle tens thousands comparatively simple MIPs/LPs.ComponentAgentModelBidding LanguageExchange Driver & CommunicationActivity/Closing Rule EnginesWD EnginePricing EngineMIP BuildersFramework & InstrumentationJOptInstance GeneratorPurposeStrategic behavior information revelation decisionsXML support load goods true valuationsImplements TBBLControls exchange, coordinates agent behaviorMRPAR, DIAR Closing RulesLogic WDLogic three pricing stagesTranslates engines optimization APIsWire components together & Gather dataOptimization API wrapping CPLEXRandom Problem GeneratorLines200113532497132218306851317220626422178497Table 1: Exchange components code breakdownExperimental set-up. experiments, -parameter MRPAR set nearzero MRPAR DIAR activity rule fire every round. rule useddefine -parameter DIAR exactly described Section 4.1. adoptstraightforward bidding agents employed Section 5 (see Appendixdetails). simulation, adopt Threshold payment rule terminate ICEper-agent error payment relative correct payment within 5% averageper-agent value efficient trade. typical instances, incurs additional 4rounds beyond would required last-and-final round. timingwall clock time, separately count large number parallel threadsexecution system. experiments run dual-processor dual-core Pentium11. Code size measured physical source line code (SLOC).12. Please contact authors access source code.13. http://www.eecs.harvard.edu/econcs/jopt60fiICE: Iterative Combinatorial ExchangeIV 3.2GHz 8GB memory CPLEX 10.1. results averaged 10 trials.problem instances available http://www.eecs.harvard.edu/~blubin/ice.instance generator begins generating set G good types. Next, j Gcreates 1 copies good type, forming total potential supply market s|G|goods (exactly many units supply depends precise structure bid trees).unit assigned one bidders uniformly random. generator createsbid tree Ti bidder recursively growing it, starting root adoptingtwo phases. tree depthLow, node receives number children drawnuniform outDegreeLow outDegreeHigh (a percentage designatedleaves), resulting exponential growth number nodes phase.width depth refer number nodes depth. point,carefully control expected number children node order makeexpected width conform triangle distribution depth depthLow depthMiddepthHigh: linearly increase expected width depth depthLowdepthMid fixed multiple () width depthLow, linearly decreaseexpected width back zero depthHigh.14 provides complex deep trees withoutinherently introducing exponential number nodes.internal node must assigned parameters interval choose operator.typically choose high-triangle distribution 1 number childrenx low-triangle distribution 1 y. bias towards introductionIC operators permit wide choice number children. internal nodealso assigned bonus drawn according uniform distribution. leaf node assignedbuy node probability [0, 1], specific good type nodechosen among good types sale market. node assignedquantity drawing low-triangle distribution 1 total numberexistence.15 unit value node drawn specific buy distribution,typically uniform, multiplied quantity assigned nodes bonus.leaf nodes assigned sell nodes goods bonuses determined similarly,time goods selected among previously assigned bidder.166.1 Experimental Results: Scalabilityfirst set results present focuses computational properties ICE.Figure 14 shows runtime performance system increase numberbidders holding parameters constant. example, 100 goods 20 typestraded bidders average 104 node trees. graph shows totalwall clock time parts system. see super-linear growth solve time14. Note setting depthLow =depthMid =depthHigh one still grow full tree given deptheliminating phase 2.15. total number goods given type existence may actually available purchaseprice given structure seller trees. Thus bias towards small quantities buy nodes largequantities sell nodes produces interesting problem instances.16. experiments, vary 2 |G| 128, 1 128, 2 |N | 20, 2 outDegreeLow 8,2 outDegreeHigh 8, 2 depthLow 6, 2 depthMid 6, 2 depthHigh 8, set balanced buyprobability = 0.5, set width multiplier second phase = 2. examples,buy node bonuses drawn uniformly [10, 100], sell nodes bonuses drawn uniformly[100, 10] internal nodes bonuses uniformly [25, 25].61fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes4103000Good Scalability (Mean 10 runs)Agent Scalability (Mean 10 runs)Concurrent CPU Time (s)Concurrent CPU Time (s)2500200015001000310500200510152010 01025121010Agents310Good TypesFigure 14: Effect number biddersrun-time ICEFigure 15: Effect number goodtypes run-time ICEnumber bidders, constants growth markets largenumbers bidders efficiently solved (solving 20 bidders around 40 minutes).error bars plots standard error statistic.Figure 15 see effect varying number types goods (retaining5 units good supply) computation time. example adopt10 bidders, tree generation parameters. likely explanation eventualconcavity run-time performance suggested decrease average (item)price upon termination ICE number types goods increased (see Figure 16).average price provides good proxy competitiveness market. Adding120Mean Linear Price (Mean 10 runs)Mean Linear Price100806040200 010121010310GoodsFigure 16: Effect number goods average item price upon terminationICE.62fiICE: Iterative Combinatorial Exchange4410103310Concurrent CPU Time (s)Concurrent CPU Time (s)10210110210110Node Degree Scalability (Mean 10 runs)010 010Power law fit10.4243 xTree Depth Scalability (Mean 10 runs)1.59021010Number Nodes Tree10 010310Figure 17: Effect bid-tree size runtime ICE: Varying nodeout degree.Power law fit10.7553 x21010Number Nodes Tree1.45310Figure 18: Effect bid-tree size runtime ICE: Varying treedepth.goods problem initially make winner determination problem difficult,large over-supply, point outcome easier determine.Figures 17 18 illustrate change run time size bid trees.use first phase tree-generator avoid confounding effects sizestructural complexity. experiments, 100 goods 20 types traded 10bidders. Figure 17 vary number children given node Figure 18vary depth tree. Increasing branching factor and/or tree depth resultsexponential growth tree size, necessarily corresponds exponential growthruntime. However, account instead plotting number nodestrees, see graphs indicate near-polynomial increase runtimetree size. fit polynomial function data form = Axb , indicatinggrowth approximately degree 1.5 range tree sizes consideredexperiments.6.2 Empirical Results: Economic Propertiessecond set results present focus economic properties ICE:efficiency trade across rounds, effectiveness preference elicitation, accuracystability prices. set experiments average 10 problem instances,8 bidders, potential supply 100 goods 20 types, bid treesaverage 104 nodes.Figure 19 plots true efficiency trades computed pessimistic (lower boundsv), provisional (-valuation v ) optimistic (upper bounds v) valuations across rounds.graph follow, x-axis indicates number rounds completedpercentage total number rounds termination enables resultsaggregated across multiple instances, different number total63fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes1001000080800070700060MeanProvable 95%EfficiencySE504030600050004000MeanProvable 95%EfficiencySE30002000PessimisticAlphaOptimistic20100MRPARDIAR9000Mean Slack Revealed% Efficient900.20.40.6% Complete0.81000001Figure 19: Efficiency optimistic, provisional, pessimistic tradesacross rounds.0.20.40.6% Complete0.81Figure 20: Average reduction value uncertainty due rule.rounds.17 vertical (dashed) line indicates average percentage completetrade provably 95% efficient. exchange remains open past point paymentsconverge (and simulate outcome last-and-final round continuingprogress straightforward bidding agents). two lines either side representone standard error statistic.Figure 19, see exchange quickly converges highly efficient trades, takingaverage 6.8 rounds achieve efficiency. general, optimistic trade (i.e., computedupper bounds v) higher (true) efficiency pessimistic one (i.e., computedlower bounds v), efficiency provisional trade typically betterboth. justifies design decision adopt provisional valuations provisionaltrade driving exchange dynamics. also suggests exchanges traditionalparadigm improving bids (i.e., increasing lower bound claims valuations) would allowlittle useful feedback early rounds: efficiency pessimistic tradeall wouldavailable without information upper-bounds bidder valuationsis initiallypoor.Figure 20 shows average amount revelation caused MRPAR DIARround ICE. Revelation measured terms absolute tightening upperlower bounds, summed across bid trees. MRPAR activity rule main drivingforce behind revelation information vast majority revelation (in absoluteterms) occurs within first 25% rounds. DIAR plays role making progress towardsidentifying efficient trade MRPAR substantially reduced valueuncertainty despite firing every round. One think MRPAR rocketsmain engine, DIAR thruster mid-course correction. ICE determines efficient17. data point represents average across 10 instances, determined averagingunderlying points neighborhood. Error-bars indicate standard error (SE) mean. Thus,figures essentially histogram rendered line graph.64fiICE: Iterative Combinatorial Exchange8030Price Volatility (Mean 10 runs)% Regret Price (Mean 10 runs)70% Regret Price% Difference final price2560504030MeanProvable 95%EfficiencySE202015MeanProvable 95%EfficiencySE10510000.20.40.6% Complete0.8001Figure 21: Price trajectory: Closenessprices round finalprices0.20.40.6% Complete0.81Figure 22: Regret best-response bidders due price inaccuracy relative final prices.trade average node TBBL tree still retains gap upper lowerbounds value node equal around 62% maximum (true) value nodecould contribute bidders value, roughly maximum marginal value contributednode feasible trades. see ICE successful directing preference elicitationinformation relevant determining efficient trade.provide two different views effectiveness prices. Figure 21 showsmean percentage absolute difference prices computed roundprices computed final round. Prices quickly converge. experimentsdriven exchange beyond efficient solution order converge Thresholdpayments, see price information already available pointefficiency. Figure 22 provides information quality price feedback. plotregret, averaged across bidders runs, best-response trade determinedintermediate prices comparison best-response final prices, regretdefined terms lost payoff final prices. Define regret bidderbest response = arg maxi Fi (x0 ) [vi (i ) p (i )], prices , given final prices, as:) p ( )v(100%.Regreti (i , ) = 1(28)max vi (i ) p (i )Fi (x0 )payoff trade , evaluated prices , approaches bestresponse trade prices , Regreti (i , ) approaches 0%. Figure 22 plots averageregret across bidders function number rounds completed ICE. regretlow: 11.2% averaged across rounds efficient trade determined7.0% averaged across rounds. regret falls across rounds also shows pricesbecome informative rounds proceed.65fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkes1009080% Efficient7060504030201000Closing Rule PredictionTruth0.20.40.6% Complete0.81Figure 23: Comparison actual efficiency pessimistic trade directbound.Finally, present experimental results relate two methods ICE employsbound final efficiency pessimistic trade. total pricing error acrossbidders round determined within pricing terms ( , v ), normalizedtotal true value efficient trade, already small (at 8.5%) initial roundsfalls around 3% final rounds ICE. suggests price-based boundquite informative, although note defined terms error given ( , v )immediately map price-based accuracy claim true valuationscurrent trade defined lower bound valuations. Figure 23 compares actual efficiencypessimistic trade round estimated direct bound efficiencyavailable exchange. confirms direct bound reasonably tight,effective bounding true efficiency regardless accuracy prices.7. Conclusionswork designed implemented scalable highly expressive iterative combinatorial exchange. design includes many interesting features, including: new treebased language combinatorial exchange environments, new method construct approximate linear prices expressive languages, proxied architecture optimisticpessimistic valuations coupled price-based activity rules drive preference elicitation, direct method estimate final efficiency trade terms valuationbounds. adopting proxy agents receive direct, expressive claims upperlower valuations bounds able form claims efficiency despite usinglinear prices. bounds also allow good progress early rounds, evenefficient trade lower bound (pessimistic) values. Experimental resultsautomated, simple bidding agents indicate good behavior terms scalabilityeconomic properties.66fiICE: Iterative Combinatorial Exchangemany intriguing opportunities future work. especially interestinginstantiate special-cases ICE design domains exist strategyproof,static (two-sided) combinatorial market designs. would bring straightforward biddingstrategies ex post Nash equilibrium. example, possible integratemethods trade-reduction (McAfee, 1992) generalizations (Babaioff & Walsh,2005; Chu & Shen, 2007) domains restricted expressiveness. also considerICE combinatorial auction rather exchange, direct appeal VCG payments would provide incentive compatibility. two major directions future workto: (a) modify design allow bidders refine structure, valuationbounds TBBL tree, across rounds; (b) extend ICE work dynamic environment changing bidder population, instance maintaining linear price feedbackperiodically clearing. Recent progress on-line mechanism design includes truthful,dynamic double auctions simple expressiveness (Blum, Sandholm, & Zinkevich,2006; Bredin, Parkes, & Duong, 2007), extend kind expressivenessprice sophistication present ICE; see work Parkes (2007) recent survey.Lastly, incentive properties ICE much dependent payment rule usedargues analysis Threshold rule alternatives.Acknowledgmentswork supported part NSF grant IIS-0238147. earlier version paperappeared Proc. 6th ACM Conference Electronic Commerce, 2005. TBBL language also described workshop paper (Cavallo et al., 2005). primary authorspaper Benjamin Lubin, Adam Juda David Parkes. Thanks anonymousreviewers associate editor JAIR extremely helpful comments. Nick Elprin, LoizosMichael Hassan Sultan contributed earlier versions work. thanksstudents Al Roths class (Econ 2056) participated trial systemCynthia Barnhart airline domain expertise. would also like thank Evan KwerelGeorge Donohue early motivation encouragement. computationused preparation manuscript performed Crimson Grid Harvard School Engineering Applied Sciences. Finally, papers genesis CS286r Topics Interface Computer Science Economics taught HarvardSpring 2004. Thanks students many early, innovative ideas.Appendix A. Computation MRPARsection show MRPAR computed solving sequence 3 MIPs.begin considering special case = 0. general case follows almost immediately.Define candidate passing trade, L, as:Largmax v (i ) p (i )Fi (x0 )breaking ties(i) maximize v (i ) v (i )(ii) favor67(29)fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkescomputed solving one MIP maximize v (i ) p (i ), followedsecond MIP objective incorporated constraint v (i ) v (i )becomes objective. Given perturbed valuation vi , defined respect trade L(asSection 4), define witness trade, U,as:Uargmax vi (i ) p (i ).(30)Fi (x0 )found solving third MIP. Given prices , provisional tradebid tree Ti , computational MRPAR rule (C-MRPAR) case = 0defined as:LUU(1) v (L) p (i ) vi (i ) p (i ) ,andLL(2) L= , v (i ) p (i ) > vi (i ) p (i )establish C-MRPAR equivalent MRPAR, defined (19)(21).Lemma 4 Given trades , prices , tree Ti , (i , , vi ) 0, vi Tiv (i ) p (i ) vi (i ) p (i ), vi defined respect trade .Proof: Direction () immediate since vi Ti . Consider direction () suppose,contradiction, v (i ) p (i ) vi (i )Pp (i ) exists vi Tivi (i ) p (i ) < vi (i ) p (i ). Subtract [vi () v ()] sides,indicates node satisfied trade , getXXXXv () p (i ) <vi () +vi ()vi () +XXvi ()v () +\iXvi () +\iX\iXv (i ) p (i ) <Xv () p (i ) <v () +\ivi (i )pXv () p (i )(31)vi () +Xv () p (i ) (32)(i ),(33)contradiction.Lemma 5 Given trade , prices , tree Ti (i , , vi ) 0, vi Ti ,UFi (x0 ), v (i ) p (i ) vi (U) p (i ), vi defined respectUtrade witness trade.0Proof: Direction () immediate since vi Ti UFi (x ). Consider directionU() suppose, contradiction, v (i ) p (i ) vi (i ) p (U) exists0Fi (x ) vi Ti (i , , vi ) < 0. Lemma 4, meansv (i ) p (i ) < vi (i ) p (i ). But, contradictionUv (i ) p (i ) vi (U) p (i )=max0Fi (x )vi (i )p(34)(i )vi (i )p(i )(35)68fiICE: Iterative Combinatorial ExchangeTheorem 7 C-MRPAR equivalent -MRPAR = 0.Proof: Comparing (17) (18) C-MRPAR, given Lemmas 4 5,left show sufficient check L, candidate pass MRPAR.is, need show Fi (x0 ) satisfies MRPAR LsatisfiesMRPAR. argue follows:1. Trade must solve maxi Fi (x0 ) [v (i ) p (i )]. Otherwise,v (i ) p (i ) > v (i ) p (i ). contradiction (17).2. Trade must also break ties favor maximizing v (i ) v (i ). Otherwise,profit v , v (i ) v (i ) > v (i ) v (i ).implies v (i ) v (i ) > v (i ) v (i ), (i , , v ) > (i , , v ). But, sinceprofit v (i , , v ) = 0 (i , , v ) > 0.contradiction (17).3. Proceed case analysis. Either = , case doneexplicitly selected candidate passing trade L. case, letLdenotefeasiblesolutions(29)considerdifficultcase |L| > 1.Largue satisfies MRPAR, trade L,6= . MRPAR, (i , , vi ) 0, vi Ti . particular,vi (i )p (i ) vi (i )p (i ), vi defined respect , equivalently,v (i ) p (i ) vi (i ) p (i ).(36)v (i ) p (i ) = v (i ) p (i ),(37)hand,since L. Taking (36) together (37), must satisfiesuncertain value nodes Ti also satisfied . Moreover, since v (i )v (i ) = v (i ) v (i ), trades must satisfy exactly uncertain valuenodes. Finally, (37) profit fixed value nodes Ti musttrades. conclude profit vi Ticurrent prices MRPAR satisfied either trade.understand importance tie-breaking rule (i) selecting candidatepassing trade, L, C-MRPAR, consider following example MRPAR = 0:Example 10 bidder XOR(+A, +B) value 5 leaf +A value range[5,10] leaf +B. Suppose prices currently 3 B = +B.MRPAR rule satisfied market knows however remaining valueuncertainty +B resolved bidder always (weakly) prefer +B +A +B. Notice +A +B pessimistic utility, +B satisfyMRPAR. +B maximal value uncertainty, therefore selected +AC-MRPAR.69fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkesunderstand importance selecting, evaluating, Urespect vi ratherv , consider following example (again = 0). illustrates role shareduncertainty tree, occurs multiple trades share node uncertainvalue value, although uncertain, resolved way trades.Example 11 bidder XOR(+A, +B) value bounds [5, 10] root nodevalue 1 leaf +A. Suppose prices currently 3 B = +B.MRPAR rule satisfied bidder strictly prefers +A +B, whichever wayuncertain value root node ultimately resolved. C-MRPAR selects Lbuy(L ) = 5 + 1 3 = 3. valuation v , witness trade buy B)pA, payoff v (Lwould selected payoff 10 3 = 7 seem violate MRPAR. But, whicheverway uncertain value root resolved affect +A +B way.addressed setting vi () = v () = 5 root node, value adopteddetermining payoff L. Evaluated vi , witness buy (1)C-MRPAR trivially satisfied (2) satisfied since 3 > 5 3 = 2.-MRPAR > 0, adopt slight variation, -C-MRPAR proceduredefined as:(1) Check (i , , vi ) vi Ti , Fi (x0 ) directly, applicationLemma 5 valuation vi defined respect trade , testUv (i ) p (i ) vi (U) p (i )(38)(2) satisfied fall back C-MRPAR verify (20) (21),candidate passing trade Lmodified (29) drop tie-breaking favorLLsecond step C-MRPAR modified require v (i ) p (i ) > vi (i )p (i ) + , vi defined respect L.argument adopted proof Theorem 7 remains valid establishingsufficient consider L, defined -C-MRPAR, case passactivity rule.Appendix B. Computation DIAR-DIAR rule verified solving two MIPs. first optimization problemidentifies trade maximal DIAR error current bounds refinementimproved error least :Pi =max [vi0 (i ) p (i ) (v 0i (i ) p (i ))]Fi (x0 )(39)s.t. (vi0 (i ) p (i ) (v 0i (i ) p (i )))= C +maxFi (x0 )(vi1 (i ) p (i ) (v 1i (i ) p (i )))(40)vi0 (i )(41)p (i )s.t. vi0 (i ) v 0i (i ) vi1 (i ) + v 1i (i ) ,70(42)fiICE: Iterative Combinatorial Exchangevi0 vi1 defined respect , v 0 v 1 represent valuations definedbidders refinement respectively, C = v 0i (i ) p (i ). Noteproblem could infeasible, case define Pi := .second optimization identifies trade maximal DIAR error v 1 stillallows possibility valuation bounds provide error reduction v 0 :Fi =max [vi0 (i ) p (i ) (v 0i (i ) p (i ))]Fi (x0 )(43)s.t. (vi0 (i ) p (i ) (v 0i (i ) p (i )))(v 1i (i ) p (i ) (vi1 (i ) p (i )))= C +max vi0 (i ) p (i )Fi (x0 )s.t. vi0 (i ) v 0i (i ) v 1i (i ) + vi1 (i ) ,(44)(45)(46)vi defined respect , vi similarly defined respect .second term (44) recognizes remains possible decrease valuenew lower-bound v 1i (i ), increasing value new upper-bound v 1i (i )except nodes shared , giving vi1 (i ). see (46) equivalentto:XX[v 1i () v 0i ()] ,[v 0i () v 1i ()] +\\icalculates amount refinement still possible service reducingDIAR error. Note problem could infeasible, case define Fi := .ultimately compare two solutions, bidder passes DIAR Pi Fi .Appendix C. Automated Bidding Agents Bidder Feedbackbidding agents used simulation experiments designed minimizeamount information revealed order pass activity rules remainingstraightforward true valuation consistent lower upper valuations.summarizing behavior bidding agents, three things explain: (a)method adopt place last-and-final round; (b) feedback providedICE bidders meeting MRPAR DIAR; (c) logic followedbidding agents. Rather define method bidding agents adjust boundslast-and-final round, keep ICE open simulation past point wouldordinarily go last-and-final. Past point, bidding agents continue refinebounds ICE terminates payments within desired accuracy.bidding agent phase reduces uncertainty multiplicative factornodes active current provisional trade provisional tradeseconomies bidder removed. adopted simulation purposes only.bidding agents operate loop, heuristically modifying valuation boundstrying meet MRPAR DIAR querying proxy advice. proxy providesguidance help bidding agent refine valuation meet activityrule. MRPAR DIAR, optimization problems solved checkingwhether bidder satisfied activity rule also provide information guide71fiLubin, Juda, Cavallo, Lahaie, Shneidman & ParkesUbidder. First consider MRPAR recall Lcandidate passing tradewitness trade. following lemma easy, stated without proof:Lemma 6 MRPAR satisfied current valuation bounds, bidder mustUincrease lower bound least one node {L\ }, decrease upper boundULleast one node {i \ }, order meet activity rule.simple bidder makes changes subset nodes, bidderinquire passed activity rule. proxy respond yesrevise set nodes bidding agent refine valuation bounds.similar functionality provided DIAR. time trade solves second MIP(with DIAR error Fi ) provided feedback, together information muchbidder must either reduce error, constrain possibilitiestrade, satisfy DIAR. bidding agent determine information nodesmust modify, much total, free decide much modifynode satisfy rule. key agent design following lemma:Lemma 7 trade straightforward bidder passes MRPAR (for = 0) musttrade weakly preferred bidder trades true valuation.Proof: contradiction. Suppose true valuation vi Ti trade meets MRPARweakly preferred trade true valuation prices . Then, existstrade Fi (x0 ) (i , , vi ) > 0. But, contradiction MRPARsince (i , , vi ) 0 vi Ti Fi (x0 ), including vi = vi = .use observation define procedure UpdateMRPAR bidderintelligently refine valuation bounds meet MRPAR. Let tradehope pass MRPAR, define ui (i , ) = vi (i ) p (i ), ui (i , ) = v (i )p (i ), ui (i , ) = vi (i ) p (i ), vi defined respect candidate passingtrade . high-level approach follows:function UpdateMRPARarg maxi Fi (x0 ) ui (i , )ui (i , ) < 0reduce slack ui (i , )endUarg maxi Fi (x0 ) ui (i , )ui (i , ) < ui (U, )UHeuristically reduce upper bounds U\ ui (i , ) ui (i , )remaining slack heuristically reduce lower bounds \ UUarg maxi Fi (x0 ) ui (i , )end6=ui (i , ) ui (i , )Heuristically reduce upper bounds \ ui (i , ) ui (i , )remaining slack heuristically reduce lower bounds \72fiICE: Iterative Combinatorial Exchangeendendreturnend functionbidding agent makes use couple optimization modalities exposedproxy bidder. procedure first chooses preferred trade truthtrade pass MRPAR ; bidding agent requests proxy findstrade solving MIP. trade negative profit, bidding agent attemptsdemonstrate positive profit trade. Next, bidding agent enters loop, whereinrepeatedly requests proxy run MIP calculates witness trade Urespect. long witness profit preferredtrade, bidding agent adjust bounds reverse mis-ordering. Lastly,bidding agent must pass MRPAR, merely RPAR, bidding agent attemptsshow strict preference identical.meeting DIAR, bidding agent responds F 0 0 parameterprovided proxy follows. Let F trade chosen maximizationcalculates F . high-level approach follows:function updateDIARProxy says still passed DIARF modified reduce DIAR error last roundHeuristically reduce upper-bound slack F \Heuristically reduce lower-bound slack \ FelseHeuristically reduce upper-bound slack \ FHeuristically reduce lower-bound slack F \endendend functionbidding agent attempts make current failing trade pass DIAR possiblereducing error respect trade. Otherwise, reduces bounds proveDIAR could made pass trade loops next trade.ReferencesAusubel, L., Cramton, P., & Milgrom, P. (2006). clock-proxy auction: practicalcombinatorial auction design. Cramton et al. (Cramton et al., 2006), chap. 5.Ausubel, L. M., & Milgrom, P. (2002). Ascending auctions package bidding. FrontiersTheoretical Economics, 1, 142.Babaioff, M., & Walsh, W. E. (2005). Incentive-compatible, budget-balanced, yet highlyefficient auctions supply chain formation. Decision Support Systems, 39, 123149.Ball, M., Donohue, G., & Hoffman, K. (2006). Auctions safe, efficient, equitableallocation airspace system resources. Cramton et al. (Cramton et al., 2006),73fiLubin, Juda, Cavallo, Lahaie, Shneidman & Parkeschap. 20.Ball, M. O., Ausubel, L. M., Berardino, F., Cramton, P., Donohue, G., Hansen, M., &Hoffman, K. (2007). Market-based alternatives managing congestion new yorkslaguardia airport. Proceedings AirNeth Annual Conference.Bererton, C., Gordon, G., & Thrun, S. (2003). Auction mechanism design multi-robotcoordination. Proc. 17th Annual Conf. Neural Information Processing Systems(NIPS03).Bertsimas, D., & Tsitsiklis, J. (1997). Introduction Linear Optimization. Athena Scientific.Bikhchandani, S., & Mamer, J. W. (1997). Competitive equilibrium exchange economyindivisibilities. Journal Economic Theory, 74, 385413.Bikhchandani, S., & Ostroy, J. M. (2002). package assignment model. JournalEconomic Theory, 107 (2), 377406.Blum, A., Sandholm, T., & Zinkevich, M. (2006). Online algorithms market clearing.Journal ACM, 53, 845879.Boutilier, C. (2002). Solving concisely expressed combinatorial auction problems.Proceedings 18th National Conference Artificial Intelligence, pp. 359366.Boutilier, C., & Hoos, H. (2001). Bidding languages combinatorial auctions. Proc.17th International Joint Conference Artificial Intelligence, pp. 11211217.Bredin, J., Parkes, D. C., & Duong, Q. (2007). Chain: dynamic double auction frameworkmatching patient agents. Journal Artificial Intelligence Research, 30, 133179.Cavallo, R., Parkes, D. C., Juda, A. I., Kirsch, A., Kulesza, A., Lahaie, S., Lubin, B.,Michael, L., & Shneidman, J. (2005). TBBL: Tree-Based Bidding LanguageIterative Combinatorial Exchanges. Multidisciplinary Workshop AdvancesPreference Handling (IJCAI).Chu, L. Y., & Shen, Z. M. (2007). Truthful double auction mechanisms e-marketplace.Operations Research. appear.Compte, O., & Jehiel, P. (2007). Auctions information acquisition: Sealed-bid Dynamic Formats?. Rand Journal Economics, 38 (2), 355372.Conen, W., & Sandholm, T. (2001). Preference elicitation combinatorial auctions..Wellman, & Shoham (Wellman & Shoham, 2001), pp. 256259.Cramton, P. (2003). Electricity Market Design: Good, Bad, Ugly.Proceedings Hawaii International Conference System Sciences.Cramton, P. (2006). Simultaneous ascending auctions. Cramton et al. (Cramton et al.,2006), chap. 3.Cramton, P., Kwerel, E., & Williams, J. (1998). Efficient relocation spectrum incumbents.Journal Law Economics, 41, 647675.Cramton, P., Shoham, Y., & Steinberg, R. (Eds.). (2006). Combinatorial Auctions. MITPress.74fiICE: Iterative Combinatorial ExchangeDay, R., & Raghavan, S. (2007). Fair payments efficient allocations public sectorcombinatorial auctions. Management Science, 53 (9), 1389.de Vries, S., Schummer, J., & Vohra, R. V. (2007). ascending Vickrey auctionsheterogeneous objects. Journal Economic Theory, 132, 95118.de Vries, S., & Vohra, R. V. (2003). Combinatorial auctions: survey. Informs JournalComputing, 15 (3), 284309.Dias, M., Zlot, R., Kalra, N., & Stentz, A. (2006). Market-based multirobot coordination:survey analysis. Proceedings IEEE, 94, 12571270.Dunford, M., Hoffman, K., Menon, D., Sultana, R., & Wilson, T. (2003). Testing linearpricing algorithms use ascending combinatorial auctions. Tech. rep., SEOR,George Mason University. Submitted INFORMS J.Computing.Fu, Y., Chase, J., Chun, B., Schwab, S., & Vahdat, A. (2003). Sharp: architecturesecure resource peering. Proceedings 19th ACM symposium Operatingsystems principles, pp. 133148. ACM Press.Gerkey, B. P., & Mataric, M. J. (2002). Sold!: Auction methods multi-robot coordination.IEEE Transactions Robotics Automation, Special Issue Multi-robot Systems,18, 758768.Hudson, B., & Sandholm, T. (2004). Effectiveness query types policies preferenceelicitation combinatorial auctions. Proc. 3rd Int. Joint. Conf. AutonomousAgents Multi Agent Systems, pp. 386393.Kelso, A. S., & Crawford, V. P. (1982). Job matching, coalition formation, grosssubstitutes. Econometrica, 50, 14831504.Krishna, V. (2002). Auction Theory. Academic Press.Kwasnica, A. M., Ledyard, J. O., Porter, D., & DeMartini, C. (2005). new improveddesign multi-object iterative auctions. Management Science, 51, 419434.Kwerel, E., & Williams, J. (2002). proposal rapid transition market allocationspectrum. Tech. rep., FCC Office Plans Policy.Lahaie, S., Constantin, F., & Parkes, D. C. (2005). power demand queriescombinatorial auctions: Learning atomic languages handling incentives. Proc.19th Int. Joint Conf. Artificial Intell. (IJCAI05).Lahaie, S., & Parkes, D. C. (2004). Applying learning algorithms preference elicitation.Proc. 5th ACM Conf. Electronic Commerce (EC-04), pp. 180188.McAfee, R. P. (1992). dominant strategy double auction. J. Economic Theory, 56,434450.Milgrom, P. (2004). Putting Auction Theory Work. Cambridge University Press.Milgrom, P. (2007). Package auctions package exchanges (2004 Fisher-Schultz lecture).Econometrica, 75, 935966.Mishra, D., & Parkes, D. C. (2007). Ascending price Vickrey auctions general valuations.Journal Economic Theory, 132, 335366.75fiLubin, Juda, Cavallo, Lahaie, Shneidman & ParkesMyerson, R. B., & Satterthwaite, M. A. (1983). Efficient mechanisms bilateral trading.Journal Economic Theory, 28, 265281.Nemhauser, G., & Wolsey, L. (1999). Integer Combinatorial Optimization. WileyInterscience.Nisan, N. (2006). Bidding languages combinatorial auctions. Cramton et al. (Cramtonet al., 2006), chap. 9.Nisan, N., Roughgarden, T., Tardos, E., & Vazirani, V. (Eds.). (2007). Algorithmic GameTheory. Cambridge University Press.ONeill, R. P., Sotkiewicz, P. M., Hobbs, B. F., Rothkopf, M. H., & Stewart, Jr., W. R.(2005). Efficient market-clearing prices markets nonconvexities. EuropeanJournal Operations Research, 164, 269285.Parkes, D. C. (2007). On-line mechanisms. Nisan et al. (Nisan, Roughgarden, Tardos,& Vazirani, 2007), chap. 16. appear.Parkes, D. C., Kalagnanam, J. R., & Eso, M. (2001). Achieving budget-balanceVickrey-based payment schemes exchanges. Proc 17th International Joint Conference Artificial Intelligence, pp. 11611168.Parkes, D. C., & Ungar, L. H. (2000a). Iterative combinatorial auctions: Theory practice. Proc. 17th National Conference Artificial Intelligence (AAAI-00), pp.7481.Parkes, D. C., & Ungar, L. H. (2000b). Preventing strategic manipulation iterativeauctions: Proxy agents price-adjustment. Proc. 17th National ConferenceArtificial Intelligence (AAAI-00), pp. 8289.Rassenti, S. J., Smith, V. L., & Bulfin, R. L. (1982). combinatorial mechanism airporttime slot allocation. Bell Journal Economics, 13, 402417.Rothkopf, M. H., Pekec, A., & Harstad, R. M. (1998). Computationally manageable combinatorial auctions. Management Science, 44 (8), 11311147.Saatcioglu, K., Stallaert, J., & Whinston, A. B. (2001). Design financial portal. Communications ACM, 44, 3338.Sandholm, T. (2006). Optimal winner determination algorithms. Cramton et al. (Cramton et al., 2006), chap. 14.Sandholm, T. (2007). Expressive Commerce Application Sourcing:Conducted $35 Billion Generalized Combinatorial Auctions. AI Magazine, 28 (3),45.Sandholm, T., & Boutilier, C. (2006). Preference elicitation combinatorial auctions.Cramton et al. (Cramton et al., 2006), chap. 10.Shapley, L. S., & Shubik, M. (1972). assignment game I: core. Int. JounralGame Theory, 1, 111130.Smith, T., Sandholm, T., & Simmons, R. (2002). Constructing clearing combinatorialexchanges using preference elicitation. AAAI-02 workshop Preferences AICP: Symbolic Approaches.76fiICE: Iterative Combinatorial ExchangeVossen, T. W. M., & Ball, M. O. (2006). Slot trading opportunities collaborative grounddelay programs. Transportation Science, 40, 1528.Wellman, M. P., & Shoham, Y. (Eds.). (2001). Proc. 3rd ACM Conf. Electronic Commerce (EC-01), New York, NY. ACM.Wurman, P. R., & Wellman, M. P. (2000). AkBA: progressive, anonymous-price combinatorial auction. Proc. 2nd ACM Conf. Electronic Commerce (EC-00), pp.2129.77fi