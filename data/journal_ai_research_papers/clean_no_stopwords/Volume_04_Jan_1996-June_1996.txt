Journal Artificial Intelligence Research 4 (1996) 365396Submitted 4/95; published 5/96Adaptive Problem-Solving Large-ScaleScheduling Problems: Case StudyGRATCH@ISI.EDUJonathan GratchUniversity Southern California, Information Sciences Institute4676 Admiralty Way, Marina del Rey, CA 90292, USASTEVE.CHIEN@JPL.NASA.GOVSteve ChienJet Propulsion Laboratory, California Institute Technology4800 Oak Grove Drive, M/S 5253660, Pasadena, CA, 911098099AbstractAlthough scheduling problems NP-hard, domain specific techniques perform wellpractice quite expensive construct. adaptive problem-solving, domain specificknowledge acquired automatically general problem solver flexible control architecture.approach, learning system explores space possible heuristic methods one well-suitedeccentricities given domain problem distribution. article, discussapplication approach scheduling satellite communications. Using problem distributionsbased actual mission requirements, approach identifies strategies decreaseamount CPU time required produce schedules, also increase percentage problemssolvable within computational resource limitations.1.Introductionmaturation automated problem-solving research come grudging abandonmentsearch domain-independent problem solver. General problem-solving tasks like planningscheduling provably intractable. Although heuristic methods effective many practicalsituations, ever growing body work demonstrates narrowness specific heuristic strategies(e.g., Baker, 1994, Frost & Dechter, 1994, Kambhampati, Knoblock & Yang, 1995, Stone, Veloso& Blythe, 1994, Yang & Murray, 1994). Studies repeatedly show strategy excels onetask perform abysmally others. negative results entirely discreditdomain-independent approaches, suggest considerable effort expertise required findacceptable combination heuristic methods, conjecture generally published accountsreal-world implementations (e.g., Wilkins, 1988). specificity heuristic methodsespecially troubling consider problem-solving tasks frequently change time.Thus, heuristic problem solver may require expensive tune-ups character applicationchanges.Adaptive problem solving general method reducing cost developing maintaining effective heuristic problem solvers. Rather forcing developer choose specific heuristicstrategy, adaptive problem solver adjusts idiosyncrasies application.1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiGRATCH & CHIENseen natural extension principle least commitment (Sacerdoti, 1977). solvingproblem, one commit particular solution path one information distinguishpath alternatives. Likewise, faced entire distribution problems,makes sense avoid committing particular heuristic strategy one make informeddecision strategy performs better distribution. adaptive problem solver embodiesspace heuristic methods, settles particular combination methodsperiod adaptation, system automatically acquires information particular distribution problems associated intended application.previous articles, Gratch DeJong presented formal characterization adaptiveproblem solving developed general method transforming standard problem solveradaptive one (Gratch & DeJong, 1992, Gratch & DeJong, 1996). primary purpose articletwofold: illustrate efficacy learning approaches solving real-world problem solvingtasks, build empirical support specific learning approach advocate. reviewing basic method, describe application development large-scale schedulingsystem National Aeronautics Space Administration (NASA). applied adaptiveproblem solving approach scheduling system developed separate research group, without knowledge adaptive techniques. scheduler included expert-crafted schedulingstrategy achieve efficient scheduling performance. automatically adapting scheduling system distribution scheduling problems, adaptive approach resulted significant improvement scheduling performance expert strategy: best adaptation found machinelearning exhibited seventy percent improvement scheduling performance (the average learnedstrategy resulted fifty percent improvement).2.Adaptive Problem Solvingadaptive problem solver defers selection heuristic strategy informationgathered performance specific distribution tasks. needapproach predicated claim difficult identify effective heuristic strategypriori. claim means proven, considerable evidence that, leastclass heuristics proposed till now, one collection heuristic methodssuffice. example, Kambhampati, Knoblock, Yang (1995) illustrate planning heuristicsembody design tradeoffs heuristics reduce size search space typically increase costnode, vice versa desired tradeoff varies different domains. Similarobservations made context constraint satisfaction problems (Baker, 1994, Frost& Dechter, 1994). inherent difficulty recognizing worth (or lack worth) controlknowledge termed utility problem (Minton, 1988) studied extensivelymachine learning community (Gratch & DeJong, 1992, Greiner & Jurisca, 1992, Holder, 1992,Subramanian & Hunter, 1992). case utility problem determining worth heuristicstrategy specific problem distribution.2.1 Formulation Adaptive problem solvingdiscussing approaches adaptive problem solving, formally state common definitiontask (as proposed Gratch & DeJong, 1992, Greiner & Jurisca, 1992, Laird, 1992,Subramanian & Hunter, 1992). Adaptive problem solving requires flexible problem solver,366fiADAPTIVE PROBLEM SOLVINGmeaning problem solver possesses control decisions may resolved alternative ways.Given flexible problem solver, PS, several control points, CP1 ...CPn (where control pointCPi corresponds particular control decision), set alternative heuristic methodscontrol point, {Mi,1 ...Mi,k ,},1 control strategy defines specific method every control point (e.g.,STRAT = <M1,3 ,M2,6 ,M3,1 ,...>). control strategy determines overall behavior problemsolver. Let PSSTRAT problem solver operating particular control strategy.quality problem solving strategy defined terms decision-theoretic notionexpected utility. Let U(PSSTRAT, d), real valued utility function measure goodnessbehavior problem solver specific problem d. generally, expected utilitydefined formally distribution problems D:E D[U(PS STRAT)]U(PSSTRAT, d) probability(d)dDgoal adaptive problem solving expressed as: given problem distribution D, findcontrol strategy space possible strategies maximizes expected utility problemsolver. example, PRODIGY planning system (Minton, 1988), control points include:select operator use achieve goal; select variable bindings instantiateoperator; etc. method operator choice control point might set control rulesdetermine operators use achieve various goals. strategy PRODIGY would setcontrol rules default methods every control point (e.g., one operator choice, onebinding choice, etc.). Utility might defined function time construct plan givenplanning problem.2.2 Approaches Adaptive Problem SolvingThree potentially complementary approaches adaptive problem solving discussedliterature. first, call syntactic approach, preprocess problem-solving domainefficient form, based solely domains syntactic structure. example, EtzionisSTATIC system analyzes portion planing domains deductive closure conjecture set searchcontrol heuristics (Etzioni, 1990). Dechter Pearl describe class constraint satisfactiontechniques preprocess general class problems efficient form (Dechter & Pearl,1987). recent work focused recognizing structural properties influenceeffectiveness different heuristic methods (Frost & Dechter, 1994, Kambhampati, Knoblock &Yang. 1995, Stone, Veloso & Blythe, 1994). goal research provide problem solveressentially big lookup table, specifying heuristic strategy use basedeasily recognizable syntactic features domain. later approach seems promising, workarea still preliminary focused primarily artificial applications. disadvantagepurely syntactic techniques ignore potentially important source information,distribution problems. Furthermore, current syntactic approaches problem specificparticular, often unarticulated, utility function (usually problem-solving cost). example,allowing utility function user specified parameter would require significantproblematic extension methods.second approach, call generative approach, generate custom-made heuristics response careful, automatic, analysis past problem-solving attempts. Generative ap1. Note method may consist smaller elements method may set control rulescombination heuristics.367fiGRATCH & CHIENproaches consider structure domain, also structures arise problemsolver interacting specific problems domain. approach exemplified SOAR(Laird, Rosenbloom & Newell, 1986) PRODIGY/EBL (Minton, 1988). techniques analyzepast problem-solving traces conjectures heuristic control rules response specific problemsolving inefficiencies. approaches effectively exploit idiosyncratic structure domain careful analysis. limitation approaches typically focused generating heuristics response particular problems well addressedissue adapting distribution problems2. Furthermore, syntactic approaches, thusfar directed towards specific utility function.final approach call statistical approach. techniques explicitly reasonperformance different heuristic strategies across distribution problems. generallystatistical generate-and-test approaches estimated average performance different heuristics random set training examples, explore explicit space heuristics greedysearch techniques. Examples systems COMPOSER (Gratch & DeJong, 1992), PALO (Greiner & Jurisca, 1992), statistical component MULTI-TAC (Minton, 1993). Similar approachesalso investigated operations research community (Yakowitz & Lugosi, 1990).techniques easy use, apply variety domains utility functions, provide strongstatistical guarantees performance. limited, however, computationally expensive, require many training examples identify strategy, face problems localmaxima. Furthermore, typically leave user conjecture space heuristic methods(see Minton, 1993 notable exception).article, adopt statistical approach adaptive problem solving due generalityease use. particular use COMPOSER technique adaptive problem solving (Gratch& DeJong, 1992, Gratch & DeJong, 1996), reviewed next section. implementation incorporates novel features address computational expense method. Ideally,however, adaptive problem solver would incorporate form methods.end investigating incorporate methods adaptation current research.3.COMPOSERCOMPOSER embodies statistical approach adaptive problem solving. turn problem solveradaptive problem solver, developer required specify utility function, representativesample training problems, space possible heuristic strategies. COMPOSER adaptsproblem solver exploring space heuristics via statistical hillclimbing search. searchspace defined terms transformation generator takes strategy generates settransformations it. example, one simple transformation generator returns single methodmodifications given strategy. Thus transformation generator defines space possibleheuristic strategies non-deterministic order space may searched. COMPOSERsoverall approach one generate test hillclimbing. Given initial problem solver,transformation generator returns set possible transformations control strategy.statistically evaluated expected distribution problems. transformation adopted2. generative approaches trained problem distribution, learning typically occurswithin context single problem. systems often learn knowledge helpfulparticular problem decreases utility overall, necessitating use utility analysis techniques.368fiADAPTIVE PROBLEM SOLVINGincreases expected performance solving problems distribution. generatorconstructs set transformations new strategy on, climbing gradient expectedutility values.Formally, COMPOSER takes initial problem solver, PS0 , identifies sequence problemsolvers, PS0 , PS1 , ... subsequent PS higher expected utility probability 1(where > 0 userspecified constant). transformation generator, TG, functiontakes problem solver returns set candidate changes. Apply(t, PS) function takestransformation, TG(PS) problem solver returns new problem solver resulttransforming PS t. Let Uj (PS) denote utility PS problem j. change utilitytransformation provides jth problem, called incremental utility transformation,denoted Uj (t|PS). difference utility solving problem without transformation. COMPOSER finds problem solver high expected utility identifyingtransformations positive expected incremental utility. expected incremental utility estimated averaging sample randomly drawn incremental utility values. Given sample n values, average sample denoted Un (t|PS). likely difference averagetrue expected incremental utility depends variance distribution, estimatedsample sample variance 2n(t|PS), size sample, n. COMPOSER provides statistical technique determining sufficient examples gathered decide, error ,expected incremental utility transformation positive negative. COMPOSERpresumes relevant distributions normally distributed, COMPOSER requires estimate incremental utility based minimum number samples n0 determinedapplication. algorithm summarized Figure 1.COMPOSERs technique applicable cases following conditions apply:1. control strategy space structured facilitate hillclimbing search. general, spacestrategies large make exhaustive search intractable. COMPOSER requirestransformation generator structures space sequence search steps, relativelytransformations step. Section 5.1 discuss techniques incorporating domainspecific information structuring control strategy space.2. large supply representative training problems adequate samplingproblems used estimate expected utility various control strategies.3. Problems solved sufficiently low cost resources estimating expected utilityfeasible.4. sufficient regularity domain cost learning good strategyamortized gains solving many problems.4.Deep Space NetworkDeep Space Network (DSN) multi-national collection ground-based radio antennasresponsible maintaining communications research satellites deep space probes. DSNOperations responsible scheduling communications large growing numberspacecraft. already complex scheduling problem becoming challenging yearbudgetary pressures limit construction new antennas. result, DSN Operations turned369fiGRATCH & CHIENGiven: PSold , TG(), , examples, n0[1] PS := PSold ; := TG(PS);[3]Repeatn := 0; i:=0; := Bound(, |T|);{Find next transformation}[2] < |examples| {Hillclimb long data possible transformations}[4]n := n+1; := i+1; steptaken := FALSE;[5]: Get Ui (|PS) {Observe incremental utility values ith problem}[6]significant :+2(|PS)n: n w n0 n2[Q()] 2U n(|PS){Collect transformations reached statistical significance.}[7]:+ significant : U n(|PS) 0[8]significant : U n(|PS) u 0 {Adopt increases expected utility}{Discard trans. decrease expeced utility}[9]PS + Apply(x significant : significant U n(x|PS) u U n(y|PS) , PS)[10]:= TG(PS); n := 0;[11]:= Bound(, ||); steptaken :=TRUE;steptaken T= i=|examples|;Return: PSRBound(, |T|) :+ ,|T|Q() :+ x1 2e0.5y 2dy +2xFigure 1: COMPOSER algorithmincreasingly towards intelligent scheduling techniques way increasing efficiencynetwork utilization. part ongoing effort, Jet Propulsion Laboratory (JPL)given responsibility automating scheduling 26-meter sub-net; collection26-meter antennas Goldstone, CA, Canberra, Australia Madrid, Spain.section discuss application adaptive problem-solving techniques development prototype system automated scheduling 26-meter sub-net. first discussdevelopment basic scheduling system discuss adaptive problem solving enhanced schedulers effectiveness.4.1 Scheduling ProblemScheduling DSN 26-meter subnet viewed large constraint satisfaction problem.satellite set constraints, called project requirements, define communication needs.typical project specifies three generic requirements: minimum maximum numbercommunication events required fixed period time; minimum maximum duration370fiADAPTIVE PROBLEM SOLVINGcommunication events; minimum maximum allowable gap communication events. example, Nimbus-7, meteorological satellite, must least four 15-minutecommunication slots per day, slots cannot greater five hours apart. Projectrequirements determined project managers tend invariant across lifetimespacecraft.addition project requirements, constraints associated various antennas.First, antennas limited resource two satellites cannot communicate given antennatime. Second, satellite communicate given antenna certain times, depending orbit brings within view antenna. Finally, antennas undergo routinemaintenance cannot communicate satellite times.Scheduling done weekly basis. weekly scheduling problem defined three elements: (1) set satellites scheduled, (2) constraints associated satellite,(3) set time periods specifying temporal intervals satellite legally communicateantenna week. time period tuple specifying satellite, communicationtime interval, antenna, (1) time interval must satisfy communication durationconstraints satellite, (2) satellite must view antenna interval. Antenna maintenance treated project time periods constraints. Two time periods conflictuse antenna overlap temporal extent. valid schedule specifies non-conflicting subset possible time periods projects requirements satisfied.automated scheduler must generate schedules quickly scheduling problems frequentlyover-constrained (i.e., project constraints combined allowable time periods producesset constraints unsatisfiable). occurs, DSN Operations must go complex cycle negotiating project managers reduce requirements. goal automatedscheduling provide system relatively quick response time human user may interact scheduler perform reasoning assist negotiation process. Ultimately, goal automate negotiation process well, place even greater demandsscheduler response time (Chien & Gratch, 1994). reasons, focus developmentupon heuristic techniques necessarily uncover optimal schedule, rather produceadequate schedules quickly.4.2 LR-26 SchedulerLR-26 heuristic scheduling approach DSN scheduling developed Jet PropulsionLaboratory (Bell & Gratch, 1993).3 LR-26 based 01 integer linear programming formulationscheduling problem (Taha, 1982). Scheduling cast problem finding assignmentinteger variables maximizes value objective function subject set linearconstraints. particular, time periods treated 0-1 integer variables: 0 (or OUT) timeperiod excluded schedule; 1 (or IN) included. objective maximizenumber time periods schedule solution must satisfy project requirementsantenna constraints (expressed sets linear inequalities). typical scheduling problemformulation 700 variables 1300 constraints.operations research, integer programs solved variety techniques including branchand-bound search, gomory method (Kwak & Schniederjans, 1987), Lagrangian relaxation3.LR-26 stands Lagrangian Relaxation approach scheduling 26-meter sub-net.371fiGRATCH & CHIEN(Fisher, 1981). artificial intelligence problems generally solved constraint propagationsearch techniques (e.g., Dechter, 1992, Mackworth, 1992). address complexity scheduling problem LR-26 uses hybrid approach combines Lagrangian relaxation constraint propagation search. Lagrangian relaxation divide-and-conquer method which, given decompositionscheduling problem set easier sub-problems, coerces sub-problems solvedway frequently result global solution. One specifies problem decompositionidentifying subset problem constraints that, removed, result one independentcomputationally easy sub-problems.4 problematic constraints relaxed, meaninglonger act constraints instead added objective function way (1)incentive satisfying relaxed constraints solving sub-problems and, (2) bestsolution relaxed problem, satisfies relaxed constraints, guaranteed best solution original problem. Furthermore, relaxed objective function parameterized setweights (one relaxed constraint). systematically changing weights (therebymodulating incentives satisfying relaxed constraints) global solution often found.Even weight search produce global solution, make solution sub-problems sufficiently close global solution global solution discovered substantiallyreduced constraint propagation search.DSN domain, scheduling problem decomposed scheduling antenna independently. Specifically, constraints associated complete problem divided twogroups: refer single antenna, mention multiple antennas. laterrelaxed resulting single-antenna sub-problems solved time linear numbertime periods associated antenna (see below). LR-26 solves complete problem firsttrying coerce global solution performing search space weights then, failsproduce solution, resorting constraint propagation search space possible schedules.4.2.1SCHEDULESdescribe formalization problem. Let P set projects, set antennas,= {0,..,10080}, V enumeration, V={0, 1, *}, denoting whether time period excludedschedule (0), included (1), uncommitted. Note P, A, M, specified advanceV determined scheduler initially always uncommitted. Let PAMMVdenote set possible time periods week, given time period specifies project,antenna start end communication event, respectively. given S, defineproject(s), antenna(s), start(s), end(s), value(s) denote corresponding elements s.also define length(s) = end(s) start(s) simplify subsequent notation.ground schedule assignment 0 (excluded) 1 (included) time period S.seen application function G maps element 0 1. denoteG. partial schedule refers schedule subset time periods committed,denote via mapping function maps elements 0, 1, *. partial schedule corresponds set possible ground schedules (i.e., result forcing uncommitted time period either schedule). denote M. define particularpartial schedule 0 denote completely uncommitted partial schedule (with time periods assigned value *).4. problem consists independent sub-problems global objective function maximizedfinding maximal solution sub-problem isolation.372fiADAPTIVE PROBLEM SOLVING4.2.2CONSTRAINTSscheduler must identify ground schedule satisfies set project antennaconstraints, formalize.Project Requirements. project pn P associated set constraints called projectrequirements. constraints processed translated simple linear inequalitieselements S. complete set project requirements, denoted PR, union requirementsindividual projects. requirement expressed integer linear inequality:pr j PR 5i,j@ value(s i) w b ji,j@ value(s i) w b jai represents weighting factor indicating degree ith time period (if included)contributes satisfying particular requirement. example, requirement project, p,must least 100 minutes communication time week expressed:[I(project(s) + p) @ length(s)] @ value(s) w 100.sSI(project(s)) equals one belongs project; otherwise zero. Note time periodszero weight play role explicitly mentioned actual constraint representation.Constraints length individual time periods represented similarly:length(s) w 15efficiency, however, time periods satisfy unary inequalities simplyeliminated preprocessing step.5Antenna Constraints. three antennas constraint two projects useantenna time. translated set linear inequalities ACa,for antennafollows:ACa = {si + sj 1 | si sj antenna(si )=antenna(sj )=a[start(si )..end(si )][start(sj )..end(sj )] }4.2.3PROBLEM FORMULATIONscheduling objective used LR-26 find ground schedule, denoted S*,maximizes number time periods schedule subject project antenna constraints:6Problem:DSNFind:Subject to:S* + arg maxGs 0ZG +value(s)sS g(1)AC1 AC2 AC3 PR5. Note inherent limitation formalization scheduler cannot entertain variablelength communication events communication events must discretized finite set fixed lengthintervals.373fiGRATCH & CHIENZ G value objective function ground schedule arg max denotesargument leads maximum.Lagrangian relaxation, certain constraints folded objective function standardized fashion. intuition add factor objective function negative iffrelaxed constraint unsatisfied. constraint form ai si b, u[ai si b] addedobjective function, u non-negative weighting factor. Likewise, constraintform ai si b, u[bai si ] added. LR-26, project requirements relaxed:Problem:Find:*(u) =DSN(u)(2)arg max Z G(u) + Z G )G0Subject to:uj aij @ value(si) * bj) uj bj aij @ value(si)PR vGAC1 AC2 AC3PR wGZs (u) relaxed objective function u vector non-negative weights length |PR|(one relaxed constraint). Note defines space relaxed solutions dependweight vector u. Let Z* denote value optimal solution original problem(Definition 1), let Z*(u) denote value optimal solution relaxed problem (Definition2) particular weight vector u. weight vector u, Z*(u) shown upper boundvalue Z*. Thus, relaxed solution satisfies original problem constraints,guaranteed optimal solution original problem. Lagrangian relaxation proceedsincrementally tightening upper bound (by adjusting weight vector) hope identifyingglobal solution. global solution cannot always identified manner, completescheduler must combine Lagrangian relaxation form search.4.2.4SEARCHsolution cannot found weight adjustment, LR26 resorts basic refinement search(Kambhampati, Knoblock & Yang, 1995) (or split-and-prune search (Dechter & Pearl, 1987))space partial schedules. search paradigm partial schedule recursively refined (split)set specific partial schedules. context DSN scheduling problem, refinementcorresponds forcing uncommitted time periods schedule. partial schedule wouldpruned ground schedules violate constraints. scheduler applied recursivelyrefined partial schedule satisfactory ground schedule found schedulespruned.refinement refined propagating local consequence new commitment.variable set particular value, individual constraint references variableanalyzed determine time period would forced schedule resultassignment. LR26 performs partial constraint propagation, complete propagationcomputationally expensive. Specifically, constraint C1 references time periods s2, s4 s5,6.might correspond desire maintain maximum downlink flexibility.374fiADAPTIVE PROBLEM SOLVINGs2 assigned value, LR26 analyzes C1 see new assignment determines value s4 and/s5. If, example, s4 constrained take particular value, triggers analysisconstraints contain s4. viewed performing arcconsistency (Dechter, 1992).constraint propagation may possible show refinement contains validground schedule. case partial schedule may pruned search.LR-26 augments basic refinement search Lagrangian relaxation heuristically reducecombinatorics problem. difficulty refinement search may performconsiderable (and poorly directed) search tree refinements identify single satisficingsolution. optimal solution sought, every leaf search tree must examined.7 contrast, searching space relaxed solutions partial schedule, one sometimesidentify best schedule without refinement search. Even possible, Lagrangianrelaxation heuristically identifies small set problematic constraints, focusing subsequent refinement search. Thus, performing search space relaxed solutions step,augmented search method significantly reduce depth breadth refinement search.augmented procedure works extent efficiently solve relaxed solutions, ideally allowing algorithm explore several points space weight vectors steprefinement search. LR-26 solves relaxed problems linear time, O(|AC1 AC2 AC3 |). see this,note time period appears exactly one antenna. Thus, Zs (u) broken sumthree objective functions, containing time periods associated particular antenna. Furthermore, relaxed objective function reexpressed weighted sumtime periods antenna, unrelaxed constraints simple pairwise exclusionconstraints individual time periods. Combine fact time periods partiallyordered start time problem simplifies identifying nonexclusive sequencetime periods maximum cumulative weight. easily formulated solved dynamic programming problem (see Bell & Gratch, 1993 details).augmented refinement search performed LR-26 summarized Figure 24.2.5PERFORMANCE TRADEOFFSPerhaps difficult decisions constructing scheduler involve flesh detailssteps 1,2, 3, 4. constraint satisfaction operations research literatures proposedmany heuristic methods steps. Unfortunately, due heuristic nature, clearcombination methods best suits scheduling problem. power heuristic methoddepends subtle factors difficult assess advance. Additionally, consideringmultiple methods, one consider interactions methods.LR-26 key interaction arises tradeoff amount weight vector search vs.refinement search performed scheduler (as determined Step 2). step refinement search, scheduler opportunity search space relaxed solutions. Spendingeffort weight search reduce amount subsequent refinement search.point savings reduced refinement search may overwhelmed cost performing7. Partial schedules may also pruned, branch-and-bound search, shown containlower value solutions partial schedules. practice LR-26 run satisficing mode, meaningsearch terminates soon ground schedule found (not necessarily optimal) satisfiesproblem constraints.375fiGRATCH & CHIENLR-26 Scheduler(1)(2)(3)(4)Agenda := {S 0};AgendaSelect partial schedule Agenda; Agenda:=Agenda{S}Weight search S*(u) S;S*(u) satisfies project requirements (PR)Return S*(u);ElseSelect constraint c PR satisfied S*(u);Refine {S i}, SG satisfies c{S i} = S;Perform constraint propagationAgenda := Agenda{S i};Figure 2: basic LR-26 refinement search method.weight search. classic example utility problem, difficult see bestresolve tradeoff without intimate knowledge form distribution scheduling problems.Another important issue improving scheduling efficiency choice heuristic methodscontrolling direction refinement search (as determined steps 1, 3, 4). Oftenmethods stated general principles (e.g., first instantiate variables maximally constrainrest search space, Dechter, 1992, p. 277) may many ways realizeparticular scheduler domain. Furthermore, almost certainly interactionsmethods used different control points make difficult construct good overall strategy.tradeoffs conspire make manual development evaluation heuristics tedious, uncertain, time consuming task requires significant knowledge domain scheduler. case LR-26, initial control strategy identified hand, requiring significant cycletrial-and-error evaluation developer small number artificial problems. Eveneffort, resulting scheduler still expensive use, motivating us try adaptive techniques.5.Adaptive Problem Solving Deep Space Networkdeveloped adaptive version scheduler, Adaptive LR-26, attempt improveperformance.8 Rather committing particular combination heuristic strategies, AdaptiveLR-26 embodies adaptive problem solving solution. scheduler provided variety heuristicmethods, and, period adaptation, settles particular combination heuristics suitsactual distribution scheduling problems domain.perform adaptive problem solving, must formally specify three things: transformationgenerator defines space legal heuristic control strategies; utility function capturespreferences strategies control grammar; representative sample training problems. describe elements relate DSN scheduling problem.5.1 Transformation Generatordescription LR-26 Figure 2 highlights four points non-determinism respectscheduler performs refinement search. fully instantiate scheduler must specify:8.system also referred name DSN-COMPOSER (Gratch, Chien & DeJong, 1993).376fiADAPTIVE PROBLEM SOLVINGway ordering elements agenda, weight search method, method selecting constraint,method generating spanning set refinements satisfy constraint. alternativeways resolving four decisions specified control grammar, describe.grammar defines space legal search control strategies available adaptive problemsolver.5.1.1SELECT PARTIAL SCHEDULEfirst decision refinement search choose partial schedule agenda.selection policy defines character search. Maintaining agenda stack implementsdepth-first search. Sorting agenda value function implements best-first search.Adaptive LR-26 restrict space methods variants depth-first search. time setrefinements created (Decision 4), added front agenda. Search always proceedsexpanding first partial schedule agenda. Heuristics act ordering refinementsadded agenda. grammar specifies several ordering heuristics, sometimes calledvalue ordering heuristics, lookahead schemes constraint propagation literature (Dechter,1992, Mackworth, 1992). methods entertained refinement construction,detailed description delayed section.Look-ahead schemes decide refine partial schedules. Look-back schemes handle reverse decision whenever scheduler encounters dead end must backtrackanother partial schedule. Standard depth-first search performs chronological backtracking, backingrecent decision. constraint satisfaction literature explored several heuristicalternatives simple strategy, including backjumping (Gaschnig, 1979), backmarking (Haralick& Elliott, 1980), dynamic backtracking (Ginsberg, 1993), dependency-directed backtracking(Stallman & Sussman, 1977) (see Backer & Baker, 1994, Frost Dechter, 1994, recentevaluation methods). currently investigating look-back schemes controlgrammar discussed article.5.1.2SEARCH RELAXED SOLUTIONnext dimension flexibility weight-adjusting methods search space possiblerelaxed solutions given partial schedule. general goal weight search findrelaxed solution closest true solution sense many constraints satisfiedpossible. achieved minimizing value Z*(u) respect u.popular method searching space called subgradient-optimization (Fisher, 1981).standard optimization method repeatedly changes current u directiondecreases Z*(u). Thus step i, ui+1 = ui + ti di ti step size di directional vectorweight space. method expensive guaranteed converge minimum Z*(u)certain conditions (Held & Karp, 1970). less expensive technique, withoutconvergence guarantee, consider one weight time finding improving direction.Thus ui+1 = ui + ti di di directional vector zeroes one location. methodcalled dual-descent. methods, weights adjusted changerelaxed solution: S*(ui ) = S*(ui+1 ).better relaxed solutions create greater reduction amount subsequent refinement search, unclear tradeoff two search spaces lies. Perhapsunnecessary spend much time improving relaxed schedules. Thus radical, extremely377fiGRATCH & CHIENefficient, approach settle first relaxed solution found. call first-solution method. moderate approach perform careful weight search beginning refinementsearch (where much gained reducing subsequent refinement search) perform restricted first-solution search deeper refinement search tree. truncated-dual-descent method performs dual-descent initial refinement search node usesfirst-solution method rest refinement search.control grammar includes four methods performing weight space search (Figure 3).2a: Subgradient-optimization2b: Dual-descent2c: Truncated-dual-descent2d: First-solutionFigure 3: Weight Search Methods5.1.3SELECT CONSTRAINTscheduler cannot find relaxed solution solves original problem, must breakcurrent partial schedule set refinements explore non-deterministically. AdaptiveLR-26, task creating refinements broken two decisions: selecting unsatisfiedconstraint (Decision 3), creating refinements make progress towards satisfying selectedconstraint (Decision 4). Lagrangian relaxation simplifies first decision identifying smallsubset constraints appear problematic. However, still leaves problem choosing oneconstraint subset base subsequent refinement.common wisdom search community choose constraint maximallyconstrains rest search space, idea minimize size subsequent refinement search allow rapid pruning partial schedule unsatisfiable. Therefore, controlgrammar incorporates several alternative heuristic methods locally assessing factor. Givencommon wisdom heuristic, include small number methods violateintuition. methods functions look local constraint graph topology return value constraint. Constraints ranked value highest valueconstraint chosen. control grammar implements primary secondary sortconstraints. Constraints primary value ordered secondary value.sake simplicity discuss measures constraints form b. (Analogous measures defined forms.) first define measures time periods. Measuresconstraints functions measures time periods participate constraint.Measures Time Periods. unforced time period one neither schedule(value(s)=*). conflictedness unforced time period (with respect current partialschedule) number unforced time periods forced forcedschedule (because participate antenna constraint s). time period already forcedcurrent partial schedule, count toward ss conflictedness. Forcing time periodhigh conflictedness schedule result many constraint propagations, reducesnumber ground schedules refinement.gain unforced time period (with respect current partial schedule) numberunsatisfied project constraints participates in. Preferring time periods high gainmake progress towards satisfying many project constraints simultaneously.378fiADAPTIVE PROBLEM SOLVINGloss unforced time period (with respect current partial schedule) combinationgain conflictedness. Loss sum gain unforced time period forcedforced schedule. Time period high loss best avoided prevent progress towards satisfying many project constraints.illustrate measures, consider simplified scheduling problem Figure 4.P1P2Project RequirementsP 1 : 1 + s2 + s3 2s1s2s3s4P 2 : 2 + s3 + s4 2Antenna ConstraintsA1: s1 + s3 1A2: s2 + s4 1A1A2Figure 4: simplified DSN scheduling problem based four time periods.two project constraints, two antenna constraints. example, P1 signifiesleast two first three time periods must appear schedule, A1 signifieseither s1 s3 may appear schedule, both. solution, s2s3 appear schedule.respect initial partial schedule (with none time periods forced either out)conflictedness s2 one, appears one antenna constraint (A2). subsequently,s4 forced out, conflictedness s2 drops zero, conflictedness computedunforced time periods. initial gain s2 two, appears project constraints. gaindrops one s3 s4 forced schedule, P2 becomes satisfied. initial losss2 sum gain time periods conflicting (s4). gain s4 one (it appearsP2) loss s2 one.Measures Constraints. Constraint measures (with respect partial schedule) definedfunctions measures unforced time periods participate constraint.functions max, min, total defined. Thus, total-conflictedness sumconflictedness unforced time periods mentioned constraint, max-gainmaximum gains unforced time periods. Thus, constraints defined above,initial total-conflictedness P1 conflictedness s1, s2 s3, 1 + 1 + 1 = 3. initialmaxgain constraint P1 maximum gains s1, s2, s3 max{1,2,2} = 2.also define two constraint measures. unforced-periods constraint (with respectpartial schedule) simply number unforced time periods mentionedconstraint. Preferring constraint small number unforced time periods restricts numberrefinements must considered, refinements consider combinations time periods forceschedule order satisfy constraint. Thus, initial unforced-periods P1 three(s1, s2, s3).379fiGRATCH & CHIENsatisfaction-distance constraint (with respect partial schedule) heuristic measurenumber time periods must forced order satisfy constraint. measure heuristic account dependencies time periods imposed antennaconstraints. initial satisfaction-distance P1 two two time periods must forcedconstraint satisfied.Given constraint measures, constraints ordered measure worth.example may prefer constraints high total conflictedness, denoted prefer-total-conflictedness. possible combinations seem meaningful control grammar Adaptive LR-26 implements nine constraint ordering heuristics (Figure 5).3a:3b:3c:3d:3e:Prefer-max-gainPrefer-total-gainPenalize-max-lossPenalize-max-conflictednessPrefer-total-conflictedness3f: Penalize-total-conflictedness3g: Prefer-min-conflictedness3h: Penalize-unforced-periods3i: Penalize-satisfaction-distanceFigure 5: Constraint Selection Methods5.1.4REFINE PARTIAL SCHEDULEGiven selected constraint, scheduler must create set refinements make progress towardssatisfying it. constraint form b time periods left-hand-side mustforced schedule constraint satisfied. Thus, refinements constructedidentifying set ways force time periods partial schedulerefinements form spanning set: {S i} = S. refinements ordered addedagenda. Again, simplicity restrict discussion constraints form b.Basic Refinement Method. basic method refining partial schedule takeunforced time period mentioned constraint create refinement time period vjforced schedule. Thus, constraints defined above, would three refinementsconstraint P1, one s1 forced in: one s2 forced in, one s3 forced in.refinement refined performing constraint propagation (arc consistency) determine local consequences new restriction. Thus, every time period conflictsvj forced refined partial schedule, turn may force time periods included, forth. process, refinements may recognized inconsistent (containground solutions) pruned search space (for efficiency, constraint propagationperformed partial schedules removed agenda).set refinements created, ordered value ordering heuristicplaced agenda. constraint ordering heuristics, common wisdomcreating value ordering heuristics: prefer refinements maximized number future optionsavailable future assignments (Dechter & Pearl, 1987, Haralick & Elliott, 1980). controlgrammar implements several heuristic methods using measures time periods createdrefinement. example, one way keep options available prefer forcing time periodminimal conflictedness. common wisdom heuristic, also incorporate methodviolates it. control grammar includes five value ordering heuristics derived380fiADAPTIVE PROBLEM SOLVINGmeasures time periods (Figure 6), last method, arbitrary, uses orderingtime periods appear constraint.1a: Prefer-gain1b: Penalize-loss1c: Penalize-conflictedness1d: Prefer-conflictedness1e: ArbitraryFigure 6: Value Ordering MethodsSystematic Refinement Method. basic refinement method one unfortunate propertymay limit effectiveness. search resulting refinement method unsystematicsense McAllester Rosenblitt (1991). means redundancyset refinements: j. Unsystematic search inefficient total size refinementsearch space greater systematic (non-redundant) refinement method used.may may disadvantage practice scheduling complexity driven sizesearch space actually explored (the effective search space) rather total size. Nevertheless,good reason suspect systematic method lead smaller effective search spaces.systematic refinement method chooses time period helps satisfy selected constraintforms spanning set two refinements: one time period forced onetime period forced out. refinements guaranteed non-overlapping. systematicmethod incorporated control grammar uses value ordering heuristic choose unforced time period use. two refinements ordered based makes immediate progress towards satisfying constraint (e.g., s=1 first constraints form b). controlgrammar includes basic systematic refinement methods (Figure 7).4a: Basic-Refinement4b: Systematic-RefinementFigure 7: Refinement Methodsproblem specified Figure 4, systematically refining constraint P1, one would usevalue ordering method select among time periods s1, s2, s3. s2 selected, two refinements would proposed, one s2 forced one s2 forced out.control grammar summarized Figure 8. original expert control strategy developedLR-26 particular point control space defined grammar: value ordering methodarbitrary (1e); weight search dual-descent (2b); primary constraint ordering penalize-unforced-periods (3h); secondary constraint ordering, thus primary ordering; basic refinement method used (4a).5.1.5META-CONTROL KNOWLEDGEconstraint grammar defines space close three thousand possible control strategies.quality strategy must assessed respect distribution problems, thereforeprohibitively expensive exhaustively explore control space: taking significant numberexamples (say fifty) strategies cost 5 CPU minutes per problem would requireapproximately 450 CPU days effort.381fiGRATCH & CHIENCONTROL STRATEGY :=VALUE ORDERINGWEIGHT SEARCH METHODPRIMARY CONSTRAINT ORDERINGSECONDARY CONSTRAINT ORDERINGREFINEMENT METHODVALUE ORDERINGWEIGHT SEARCH METHODPRIMARY CONSTRAINT ORDERINGSECONDARY CONSTRAINT ORDERINGREFINEMENT METHOD:= {1a, 1b, 1c, 1d,1e}:= {2a, 2b, 2c, 2d}:= {3a, 3b, 3c, 3d, 3e, 3f, 3g, 3h, 3i}:= {3a, 3b, 3c, 3d, 3e, 3f, 3g, 3h, 3i}:= {4a, 4b}Figure 8: Control grammar Adaptive LR-26COMPOSER requires transformation generator specify alternative strategies, explored via hillclimbing search. case, obvious way proceed consider single method changes given control strategy. However cost searching strategy space qualityfinal solution depend large extent hillclimbing proceeds, obvious way needbest. Adaptive LR-26, augment control grammar domain-specificknowledge help organize search. knowledge includes, example, prior expectationcertain control decisions would interact, likely importance different control decisions. intent meta-control knowledge reduce branching factor controlstrategy search improve expected utility locally optimal solution found. approachled layered search strategy space. control decision assigned level.control grammar search evaluating combinations methods single level, adoptingbest combinations, moving onto next level. organization shown below:Level 0:Level 1:Level 2:Level 3:{Weight search method}{Refinement method}{Secondary constraint ordering, Value ordering}{Primary constraint ordering}weight search refinement control points separate, seem relatively independentcontrol points, terms effect overall strategy. clearlyinteraction weight search, refinement construction, control points,good selection methods pricing alternative construction perform well acrossordering heuristics. primary constraint ordering method relegated last leveleffort made optimizing decision expert strategy LR-26, believedunlikely default strategy could improved.Given transformation generator, Adaptive LR-26 performs hillclimbing across levels.first entertains weight adjustment methods, alternative construction methods, combinations secondary constraint sort child sort methods, finally primary constraint sort methods.choice made given previously adopted methods.layered search viewed consequence asserting certain types relations control points. Independence relations indicate cases utility methods onecontrol point roughly independent methods used control points. Dominance rela-382fiADAPTIVE PROBLEM SOLVINGtions indicate changes utility changing methods one control point much largerchanges utility another control point. Finally, inconsistency relations indicatemethod M1 control point X inconsistent method M2 control point Y. meansstrategy using methods control points need considered.5.2 EXPECTED UTILITYpreviously mentioned, chief design requirement LR-26 scheduler produce solutions(or prove none exist) efficiently. behavioral preference expressed utilityfunction related computational effort required solve problem. effort produceschedule increases, utility scheduler problem decrease. paper,characterize preference defining utility negative CPU time requiredscheduler problem. Thus, Adaptive LR-26 tunes strategies minimize average timegenerate schedule (or prove one exist). utility functions could entertained.fact, recent research focused measures schedule quality (Chien & Gratch, 1994).5.3 Problem DistributionAdaptive LR-26 needs representative sample training examples adaptation phase.Unfortunately, DSN Operations recently begun maintain database schedulingproblems machine readable format. ultimately allow scheduler tuneactual problem distribution, small body actual problems available timeevaluation. Therefore, resorted means create reasonable problem distribution.constructed augmented set training problems syntactic manipulation set realproblems. Recall scheduling problem composed two components: set project requirements, set time periods. time periods change across scheduling problems,organize real problems set tuples, one project, containing weeklyblocks time periods associated (one entry week project scheduled). setaugmented scheduling problems constructed taking cross product tuples. Thus,weekly scheduling problem defined combining one weeks worth time periodsproject (time periods different projects may drawn different weeks), well projectrequirements each. simple procedure defines set 6600 potential scheduling problems.Two concerns led us use subset augmented problems. First, significant percentage augmented problems appeared much harder solve (or prove unsatisfiable)real problems (on almost half constructed problems scheduler terminate, evenlarge resource bounds). hard problems exist unexpected scheduling NPhard, however, frequency augmented sample seems disproportionately high. Second,existence hard problems raises secondary issue best terminate search. standard approach impose arbitrary resource bound declare problem unsatisfiablesolution found within bound. Unfortunately raises issue sized boundreasonable. could resolved adding resource bound control grammar, however, point project settled simpler approach. address previousconcern excluding augmented problem distribution problems seem fundamentally intractable. means practice exclude problems couldsolved large set heuristic methods within five minute resource bound, determina-383fiGRATCH & CHIENtion discussed Appendix A. results reduced set three thousand scheduling problems.use resource bound problematic evaluating power learning technique.noted Segre, Elkan, Russell (1991), learning system greatly improves problem solving performance given resource bound may perform quite differently different resource bound. researchers suggest statistical analysis methods assessing significancefactor (e.g., see Etzioni Etzioni, 1994). study, however, address issueresults might change given different resource bounds. note COMPOSERs statisticalproperties suggest problem solving performance worse learning, whateverresource bound, performance improvement many vary considerably. give leastinsight generality adaptive problem solving, include secondary set evaluationsbased 6600 augmented problems (including fundamentally intractable ones).6.Empirical Evaluationconjecture Adaptive LR26 improve performance basic scheduler.broken two separate claims. First, claim modifications suggestedcontain useful transformations (it possible improve scheduler). Second, claimAdaptive LR26 identify transformations (and avoid harmful ones) requestedlevel probability. first claim solely based intuitions; second supportedstatistical theory underlies COMPOSER approach. usefulness COMPOSER dependsability COMPOSER go beyond simply improving performance identifying strategiesrank highly judged respect whole space possible strategies. third claim,therefore, Adaptive LR-26 find better strategies simply picked best largenumber randomly selected strategies. Besides testing three claims, also interestedthree secondary questions: quickly technique improve expected utility (e.g.,many examples required make statistical inferences?); Adaptive LR-26 improve numberproblems solved (or proved unsatisfiable) within resource bound; sensitiveeffectiveness adaptive problem solving changes distribution problems.6.1 Methodologyevaluation influenced stochastic nature adaptive problem solving. adaptation,Adaptive LR-26 guided random selection training examples according problemdistribution. result random factor, system exhibit different behavior differentruns system. runs system may learn high utility strategies; runsrandom examples may poorly represent distribution system may adopt transformationsnegative utility. Thus, evaluation directed assessing expected performanceadaptive scheduler averaging results multiple experimental trials.experiments, scheduler allowed adapt 300 scheduling problems drawn randomly problem distribution described above. expected utility learned strategiesassessed independent test set 1000 test examples drawn randomly complete setthree thousand. adaptation rate assessed recording strategy learned Adaptive LR-26every 20 examples. Thus see result learning twenty examples, fortyexamples, etc. measure statistical error technique (the probability adopting trans-384fiLR-26706050Avg. Solution Timeseconds per prob.Summary Results8040Adaptive LR-263020100030 60 90 120 150 180 210 240 270 300Examples Training SetLR-26avg. across trialsAdaptiveLR-26StatisticalErrorRateSolution Rate% solvable probs.Average Solution Time (CPU seconds)ADAPTIVE PROBLEM SOLVING8040best strategy24worst strategy55predicted5%observed3%LR-26avg. across trialsAdaptiveLR-26Dist. 179%95%best strategy97%worst strategy86%Figure 9. Learning curve showing performance function number training examplestable experimental results.formation negative incremental utility) performing eighty runs system eighty distinct training sets drawn randomly problem distribution. measure distributional sensitivity technique evaluating adaptive scheduler second distribution problems.Recall purposely excluded inherently difficult scheduling problems augmented setproblems. added, excluded problems make adaptation difficult strategylikely provide noticeable improvement within five minute resource bound. secondevaluation includes difficult problemsthird evaluation assesses relative quality strategies identified Adaptive LR-26compared strategies strategy space. inferred comparing expected utility learned strategies several strategies drawn randomly space. also provides opportunity assess quality expert strategy, thus give sense challenging improve it.COMPOSER, statistical component adaptive scheduler, two parameters governbehavior. parameter specifies acceptable level statistical error (this chancetechnique adopt bad transformation reject good one). Adaptive LR-26, setstandard value 5%. COMPOSER bases statistical inferences minimum n0 examples.Adaptive LR-26, n0 set empirically determined value fifteen.6.2 Overall Results DSN DISTRIBUTIONFigure 9 summarizes results adaptive problem solving constructed DSN problemdistribution. results support two primary claims. First, system learned search controlstrategies yielded significant improvement performance. Adaptive problem solving reducedaverage time solve problem (or prove unsatisfiable) 80 40 seconds (a 50%385fiGRATCH & CHIENimprovement). Second, observed statistical error fell well within predicted bound. 370transformations adopted across eighty trials, 3% decreased expected utility.Due stochastic nature adaptive scheduler, different strategies learned different trials. learned strategies produced least improvement performance. beststrategies required 24 seconds average solve problem (an improvement 70%).fastest adaptations occurred early adaptation phase performance improvements decreased steadily throughout. took average 62 examples adopt transformation. Adaptive LR-26 showed improvement non-adaptive scheduler terms numberproblems could solved (or proven unsatisfiable) within resource bound. LR-26 unablesolve 21% scheduling problems within resource bound. One adaptive strategy substantially reduced number 3%.analysis learned strategies revealing. performance improvement (aboutone half) traced modifications LR-26s weight search method. rest improvements divided equally among changes heuristics value ordering, constraint selection,refinement. expected, changes primary constraint ordering degraded performance.top three strategies illustrated Figure 10.1) Value ordering:Weight search:Primary constraint ordering:Secondary constraint ordering:Refinement method:penalize-conflictedness (1c)first-solution (2d)penalize-unforced-periods (3h)prefer-total-conflictedness (3e)systematic-refinement (4b)2) Value ordering:Weight search:Primary constraint ordering:Secondary constraint ordering:Refinement method:prefer-gain (1a)first-solution (2d)penalize-unforced-periods (3h)prefer-total-conflictedness (3e)systematic-refinement (4b)3) Value ordering:Weight search:Primary constraint ordering:Secondary constraint ordering:Refinement method:penalize-conflictedness (1c)first-solution (2d)penalize-unforced-periods (3h)penalize-satisfaction-distance (3i)systematic-refinement (4b)Figure 10: three highest utility strategies learned Adaptive LR-26.weight search, learned strategies used first-solution method (2d). seemsthat, least domain problem distribution, reduction refinement search spaceresults better relaxed solutions offset additional cost weight search.scheduler did, however, benefit reduction size results systematic refinement method.386fi140LR-26Adaptive LR-26120100AdaptiveLR-26LR-26avg. across trials156best strategy133worst strategy150predicted5%observed6%LR-26avg. across trials51%best strategy57%worst strategy51%StatisticalError Rate8060402000 30 60 90 120150180210240270300Dist. 1Summary ResultsAvg. Timeseconds per prob.160Solution Rate% solvableAverage Solution Time (CPU seconds)ADAPTIVE PROBLEM SOLVINGAdaptiveLR-2614654%Examples Training SetFigure 11. Learning curves table experimental results showing performanceaugmented distribution (including intractable problems).interestingly, Adaptive LR-26 seems rediscovered common wisdom heuristic constraint-satisfaction search. exploring new refinements, often suggested choseleast constrained value constrained constraint. best learned strategies followadvice worst strategies violate it. best strategy, time period lowest conflictedness least constraining (in sense tend produce least constraint propagations) thus produces least commitments resulting partial schedule. argument, constraint highest total conflicted tend hardest satisfy.6.3 Overall Results FULL AUGMENTED DISTRIBUTIONFigure 11 summarizes results augmented distribution. expected, distributionproved challenging adaptive problem solving. Nevertheless, modest performanceimprovements still possible, lending support claimed generality adaptive problemsolving approach. Learned strategies reduced average solution time 156 146 seconds (an6% improvement). best learned strategies required 133 seconds average solve problem(an improvement 15%). observed statistical accuracy significantly differtheoretically predicted bound, although slightly higher expected: 397 transformationsadopted across trials, 6% produced decrease expected utility. introductiondifficult problems resulted higher variance distribution incremental utility valuesreflected higher sample complexity: average 118 examples adopt transformation.improvement noted supposedly intractable problems. One strategy learnedAdaptive LR-26 increased number problems could processed within resource bound51% 57%.One interesting result evaluation that, unlike previous evaluation, best learnedstrategies use truncated-dual-descent weight search method (the strategies similar alongcontrol dimensions). illustrates even modest changes distribution problems387fiGRATCH & CHIENinfluence design tradeoffs associated problem solver: case, changing tradeoffweight refinement search.6.4 Quality Learned strategiesthird claim that, practice, COMPOSER identify strategies rank highly judgedrespect whole strategy space. secondary question well expert strategyperform. improvements Adaptive LR-26 little significance expert strategyperforms worse strategies space. Alternatively, expert strategy extremelygood, improvement compelling.way assessing claims estimate probability selecting high utility strategygiven choose randomly one three strategy spaces: space possible strategies(expressible transformation grammar), space strategies produced Adaptive LR-26,trivial space containing expert strategy. corresponds problem estimatingprobability density function (p.d.f.) space: p.d.f., f(x), associated random variablegives probability instance variable value x. specifically want estimate density functions, fs (u), probability randomly selecting strategy spaceexpected utility u.use non-parametric density estimation technique called kernel method estimate fs (u)(as Smyth, 1993). estimate density function whole space, randomly selectedtested thirty strategies. learned strategies used estimate density learnedspace. (In cases, five percent data withheld estimate bandwidth parameter usedkernel method.) p.d.f. associated single expert strategy estimated using normal model fit 1000 test examples previous evaluation.6.4.1DSN DISTRIBUTIONFigure 12 illustrates results DSN distribution. evaluation learned strategiessignificantly outperformed randomly selected strategies. Thus, one would select testmany strategies random finding one comparable expected utility one foundAdaptive LR-26. results also indicate expert strategy already good strategy (asindicated relative positions peaks expert random strategy distributions),indicating improvement due Adaptive LR-26 significant non-trivial.results provide additional insight Adaptive LR-26s learning behavior. p.d.flearned strategies contains several peaks, graphically illustrates different local maxima existproblem. Thus, may benefit running system multiple times choosingbest strategy. also suggests techniques designed avoid local maxima would beneficial.6.4.2FULL AUGMENTED DISTRIBUTIONFigure 13 illustrates results full augmented distribution. results similar DSNdistribution: learned strategies outperformed expert strategy turnoutperformed randomly selected strategies. data shows expert strategysignificantly better randomly selected strategies. Together, two evaluations supportclaim Adaptive LR-26 selecting high performance strategies. Even though expert strategyquite good compared complete strategy space, adaptive algorithm ableimprove expected problem solving performance.388fiADAPTIVE PROBLEM SOLVINGImproved PerformanceProbability0.1400.120Expert Strategy0.1000.080Learned Strategies0.0600.040Random Strategies0.0200020406080100120140160180Negative Expected UtilityFigure 12: DSN Distribution. graph shows probability obtainingstrategy particular utility, given chosen (1) set strategies,(2) set learned strategies, (3) expert strategy.ProbabilityImproved Performance0.1200.100Expert Strategy0.0800.060Learned Strategies0.040Random Strategies0.02000306090120150180210240270Negative Expected UtilityFigure 13: Full augmented distribution. graph shows probability obtaining strategy particular utility, given chosen (1) setstrategies, (2) set learned strategies, (3) expert strategy.389fiGRATCH & CHIEN7.Future Workresults applying adaptive approach deep space network scheduling promising.hope build success number ways. discuss directions relatethree basic approaches adaptive problem solving: syntactic, generative, statistical.7.1 Syntactic ApproachesSyntactic approaches attempt identify control strategies analyzing structure domainproblem solver. LR-26, use meta-control knowledge seen syntactic approach;although unlike syntactic approaches attempt identify specific combination heuristicmethods, meta-knowledge (dominance indifference relations) acts constraintspartially determine strategy. advantage weakening syntactic approachlends natural complementary interaction statistical approaches: structuralinformation restricts space reasonable strategies, explored statisticaltechniques. important question concerning knowledge extent contributesuccess evaluations, and, interestingly, could information derivedautomatically structural analysis domain problem solver. currentlyperforming series experiments address former question. step towards resolvingsecond question would evaluate context LR-26 structural relationshipssuggested recent work area (Frost & Dechter, 1994, Stone, Veloso & Blythe, 1994).7.2 Generative ApproachesAdaptive LR-26 uses non-generative approach conjecturing heuristics. experiencescheduling domain indicates performance adaptive problem solving inextricably tiedtransformations given expense processing examples. inductivelearning technique relies good attributes, COMPOSER effective, must exist goodmethods control points make strategy. Generative approaches could improveeffectiveness Adaptive LR-26. Generative approaches dynamically construct heuristic methodsresponse observed problem-solving inefficiencies. advantage waiting inefficienciesobserved twofold. First, exploration strategy space much focusedconjecturing heuristics relevant observed complications. Second, conjecturedheuristics tailored much specifically characteristics observedcomplications.previous application COMPOSER achieved greater performance improvements Adaptive LR-26, part exploited generative technique construct heuristics (Gratch & DeJong, 1992). Ongoing research directed towards incorporating generative methods AdaptiveLR-26. preliminary work analyzes problem-solving traces induce good heuristic methods.constraint value ordering metrics discussed Section 5.1.3 used characterizesearch node. information fed decision-tree algorithm, tries induce effectiveheuristic methods. generated methods evaluated statistically.7.3 Statistical ApproachesFinally directions future work devoted towards enhancing power basicstatistical approach, Adaptive LR-26 particular, statistical approaches general.390fiADAPTIVE PROBLEM SOLVINGscheduler, two important considerations: enhancing control grammarexploring wider class utility functions. Several methods could added control grammar.example, informal analysis empirical evaluations suggests scheduler couldbenefit look-back scheme backjumping (Gaschnig, 1979) backmarking (Haralick& Elliott, 1980). would also like investigate adaptive problem solving methodologyricher variety scheduling approaches, besides integer programming. Among wouldpowerful bottleneck centered techniques (Biefeld & Cooper, 1991), constraint-basedtechniques (Smith & Cheng, 1993), opportunistic techniques (Sadeh, 1994), reactive techniques(Smith, 1994) powerful backtracking techniques (Xiong, Sadeh & Sycara, 1992).current evaluation scheduler focused problem solving time utility metric,future work consider improve aspects schedulers capabilities. example,choosing another utility function could guide Adaptive LR-26 towards influencing aspectsLR-26s behavior as: increasing amount flexibility generated schedules, increasing robustness generated schedules, maximizing number satisfied project constraints,reducing implementation cost generated schedules. alternative utility functionsgreat significance provide much greater leverage impacting actual operations.example, finding heuristics reduce DSN schedule implementation costs 3% wouldmuch greater impact reducing automated scheduler response time 3%. preliminary work focused improving schedule quality (Chien & Gratch, 1994).generally, several ways improve statistical approach embodied COMPOSStatistical approaches involve two processes, estimating utility transformations exploring space strategies. process estimating expected utilities enhancedefficient statistical methods (Chien, Gratch & Burl, 1995, Moore & Lee, 1994, Nelson & Matejcik,1995), alternative statistical decision requirements (Chien, Gratch & Burl, 1995) complexstatistical models weaken assumption normality (Smyth & Mellstrom, 1992). processexploring strategy space improved terms efficiency susceptibilitylocal maxima. Moore Lee propose method called schemata search help reduce combinatorics search. Problems local maxima mitigated, albeit expensively, consideringk-wise combinations heuristics (as MULTI-TAC) level 2 Adaptive LR-26s search),standard numerical optimization approaches repeating hillclimbing search several timesdifferent start points.ER.One final issue expense processing training examples. LR-26 domain costgrows linearly number candidates hillclimbing step. badcomplexity standpoint, pragmatic concern. proposals reduceexpense gathering statistics. previous work (Gratch & DeJong, 1992) exploited propertiestransformations gather statistics single solution attempt. system requiredheuristic methods act pruning refinements guaranteed unsatisfiable. GreinerJurisica (1992) discuss similar technique eliminates restriction providing upper lower bounds incremental utility transformations. Unfortunately, neither approachescould applied LR-26 devising methods reduce processing cost important directionfuture work.391fiGRATCH & CHIEN8.ConclusionsAlthough many scheduling problems intractable, actual sets constraints problemdistributions, heuristic solutions provide acceptable performance. frequent difficultydetermining appropriate heuristic methods given problem class distribution challengingprocess draws upon deep knowledge domain problem solver used. Furthermore,problem distribution changes time future, one must manually re-evaluateeffectiveness heuristics.Adaptive problem solving general approach reducing developmental burden.paper described application adaptive problem solving, using LR26 scheduling systemCOMPOSER machine learning system, automatically learn effective scheduling heuristicsDeep Space Network communications scheduling. demonstrating applicationtechniques real-world application problem, paper makes several contributions. First,provides example wide range heuristics integrated flexible problem-solving architecture providing adaptive problem-solving system rich control space search.Second, demonstrates difficulties local maxima large search spaces entailedrich control space tractably explored. Third, successful application COMPOSER statistical techniques demonstrates real-world applicability statistical assumptions underlyingCOMPOSER approach. Fourth, significantly, paper demonstrates viabilityadaptive problem solving. strategies learned adaptive problem solving significantly outperformed best human expert derived solution.Appendix A. Determination Resource boundgood CPU bound characterize intractable problems characteristicincreasing bound little effect proportion problems solvable. orderdetermine resource bound define intractable DSN scheduling problems empiricallyevaluated likely LR26 able solve problem various resource bounds.Informally, experimented find bound 5 CPU minutes. formally verified boundtaking problems solvable within resource bound 5 CPU minutes, allowing LR26additional CPU hour attempt solve problem, observing affected solution rate.expected, even allocating significant CPU time, LR26 able solve manyproblems. Figure 14 shows cumulative percentage problems solved;solvable within 5 minute CPU bound. curve shows even another CPU hour (perproblem!), 12% problems became solvable. graph also shows 95%confidence intervals cumulative curve. light results, fact one learnedstrategy able increase 18% percentage problems solvable within resource boundeven impressive. effect, learning strategy greater impact allocating anotherCPU hour per problem.AcknowledgementsPortions work performed Jet Propulsion Laboratory, California InstituteTechnology, contract National Aeronautics Space Administration portionsBeckman Institute, University Illinois National Science Foundation GrantNSFIRI9209394.392fiADAPTIVE PROBLEM SOLVINGProbability1.00.80.60.40.200300600900120015001800210024002700300033003600SecondsFigure 14: Given problem cannot solved five minutes, show probabilitysolved hour time (with 95% confidence intervals).ReferencesBaker, A. (1994). Hazards Fancy Backtracking. Proceedings AAAI94.Bell, C., & Gratch, J. (1993). Use Lagrangian Relaxation Machine Learning TechniquesSchedule Deep Space Network Data Transmissions. Proceedings 36th Joint NationalMeeting Operations Research Society America, Institute Management Sciences.Biefeld, E., & Cooper, L. (1991). Bottleneck Identification Using Process Chronologies. Proceedings IJCAI91.Chien, S. & Gratch, J. (1994). Producing Satisficing Solutions Scheduling Problems: IterativeConstraint Relaxation Approach. Proceedings Second International ConferenceArtificial Intelligence Planning Systems.Chien, S., Gratch, J. & Burl, M. (1995). Efficient Allocation Resources HypothesisEvaluation: Statistical Approach. Institute Electrical Electronics Engineers Transactions Pattern Analysis Machine Intelligence 17(7), 652665.Dechter, R. & Pearl, J. (1987). NetworkBased Heuristics ConstraintSatisfaction Problems.Artificial Intelligence 34(1) 138.Dechter, R. (1992). Constraint Networks. Encyclopedia Artificial Intelligence, Stuart C. Shapiro (ed.).393fiGRATCH & CHIENEtzioni, E. (1990). Prodigy/EBL Works. Proceedings AAAI90.Etzioni. O. & Etzioni, R. (1994). Statistical Methods Analyzing Speedup Learning Experiments.Machine Learning 14(3), 333347.Fisher, M. (1981). Lagrangian Relaxation Method Solving Integer Programming Problems.Management Science 27 (1) 118.Frost, D. & Dechter, R. (1994). Search Best Constraint Satisfaction Search. ProceedingsAAAI94.Gaschnig, J. (1979). Performance Measurement Analysis Certain Search Algorithms. Technical Report CMUCS79124, CarnegieMellon University.Ginsberg, M. (1993). Dynamic Backtracking. Journal Artificial Intelligence Research 1, 2546.Gratch, J. & DeJong, G. (1992). COMPOSER: Probabilistic Solution Utility Problem SpeedLearning. Proceedings AAAI92.Gratch, J., Chien, S., & DeJong, G. (1993). Learning Search Control Knowledge Deep Space Network Scheduling. Proceedings Ninth International Conference Machine Learning.Gratch, J. & DeJong, G. (1996). Decisiontheoretic Approach Adaptive Problem Solving. Artificial Intelligence (to appear, Winter 1996).Greiner, R. & Jurisica, I. (1992). Statistical Approach Solving EBL Utility Problem. Proceedings AAAI92.Haralick, R. & Elliott, G. (1980). Increasing Tree Search Efficiency Constraint Satisfaction Problems. Artificial Intelligence 14, 263313.Held, M. & Karp, R. (1970). Traveling Salesman Problem Minimum Spanning Trees. Operations Research 18, 11381162.Holder, L. (1992). Empirical Analysis General Utility Problem Machine Learning. Proceedings AAAI92.Kambhampati, S., Knoblock, C. & Yang, Q. (1995). Planning Refinement Search: UnifiedFramework Evaluating Design Tradeoffs Partial Order Planning. Artificial Intelligence:Special Issue Planning Scheduling 66, 167238.Kwak, N. & Schniederjans, M. (1987). Introduction Mathematical Programming, New York:Robert E. Krieger Publishing.Laird, J., Rosenbloom, P. & Newell, A. (1986). Universal Subgoaling Chunking: AutomaticGeneration Learning Goal Hierarchies. Norwell, MA: Kluwer Academic Publishers.394fiADAPTIVE PROBLEM SOLVINGLaird, P. (1992). Dynamic Optimization. Proceedings Ninth International ConferenceMachine Learning.Mackworth, A. (1992). Constraint Satisfaction. Encyclopedia Artificial Intelligence, Stuart C.Shapiro (ed.).McAllester, D. & Rosenblitt, D. (1991). Systematic Nonlinear Planning. ProceedingsAAAI91.Minton, S. (1988). Learning Search Control Knowledge: ExplanationBased Approach, Norwell, MA: Kluwer Academic Publishers.Minton, S. (1993). Integrating Heuristics Constraint Satisfaction Problems: Case Study.Proceedings AAAI93.Moore, A. & Lee, M. (1994). Efficient Algorithms Minimizing Cross Validation Error. Proceedings Tenth International Conference Machine Learning.Nelson, B. & Matejcik, F. (1995). Using Common Random Numbers IndifferenceZone Selection Multiple Comparisions Simulation. Management Science.Sacerdoti, E. (1977). Structure Plans Behavior. New York: American Elsevier.Sadeh, N. (1994). Microopportunistic Scheduling: Microboss Factory Scheduler. Intelligent Scheduling. San Mateo, CA: Morgan Kaufman.Segre, A., Elkan, C., & Russell, A. (1991). Critical Look Experimental Evaluations EBL.Machine Learning 6(2).Smith, S. & Cheng, C. (1993). Slackbased Heuristics Constraintsatisfaction Scheduling.Proceedings AAAI93.Smith, S. (1994). OPIS: Methodology Architecture Reactive Scheduling. IntelligentScheduling. San Mateo, CA: Morgan Kaufman.Smyth, P. & Mellstrom, J. (1992). Detecting Novel Classes Applications Fault Diagnosis.Proceedings Ninth International Conference Machine Learning.Smyth, P. (1993). Probability Density Estimation Local Basis Function Neural Networks. Computational Learning Theory Natural Learning Systems 2.Stallman, R. & Sussman, G. (1977). Forward Reasoning DependencyDirected BacktrackingSystem ComputerAided Circuit Analysis. Artificial Intelligence 9, 2, 135196.Stone, P., Veloso, M., & Blythe, J. (1994). Need Different DomainIndependent Heuristics.Proceedings Second International Conference Artificial Intelligence PlanningSystems.395fiGRATCH & CHIENSubramanian, D. & Hunter, S. (1992). Measuring Utility Design Provably Good EBL Algorithms. Proceedings Ninth International Conference Machine Learning.Taha, H. (1982). Operations Research, Introduction, Macmillan Publishing Co.Wilkins, D. (1988). Practical Planning: Extending Classical Artificial Intelligence PlanningParadigm. San Mateo, CA: Morgan Kaufman.Xiong, Y., Sadeh, N., & Sycara, K. (1992). Intelligent Backtracking Techniques Job Shop Scheduling. Proceedings Third International Conference Knowledge RepresentationReasoning.Yakowitz, S. & Lugosi, E. (1990). Random Search Presence Noise, Application Machine Learning. Society Industrial Applied Mathematics Journal Scientific Statistical Computing 11, 4, 702712.Yang, Q. & Murray, C. (1994). Evaluation Temporal Coherence Heuristic PartialOrderPlanning. Computational Intelligence 10, 2.396fiff fi!"$#%$&('*),+--.0/1''0243'652B789:;%$<=+8+>-026?$@9#& <A.0>-.CEDGFAHJILKMCEFNILHPO*QRDLFNSUTVDGFXWEYZO[O]\_^`YbaZO]IcFAdfed`gTVFNDhHijFNDLklK0OcHPmRInd`\WEDoK^qpre0Dhd`msEtVu"vwu"xjy*u"xNz|{}u"~%~|"1| G|"r "n8(orw(!V8*wq0wq;;4ww 6f ,r8w,Ar1Vqu"~,u"vZru|u"t u!u||||||1 ||8;N= 4n=8w4E"q=wf,,!=6 r;n=w[;"(V !80 0$ $q V`$ Z q 4w !;`!M $ $c $ }1!6 ;V G=% $`w6%4wE $A% 611(` V$ 1N%$ 4w $ q"*1"V 10|ZN1 (1%4wE $;; (8$ 84w 1;|610L 0 Vq $ $ ;c`VwV$ 6 !L V$ $EA!;,%L `( $ }% A0$ E6 $ $8$V" * ;1%wEV $$ Ef$c Z$nVVEw 0 $ 0w V q0 (1(4qw 1[! lq ,"wq ,w w0 !0 l0$ Efo1 0qw V"AZ$4$ 1;%E V $fE q4$ Nf ` `w }oV*V4w f!V ; $Z$ $o`wl$4wo[ 8E; V$, Vo`V$ r0w ! ! [E $8 0w w $ |180V` !VN0$,w$V rw0 |611$ w ( $6 $ AV60$fiff ff!#"%$'& fi()+*(!fi,( &-/.10324065870fi9;:=<?>A@CBfiD;E<?>GFC.?FHB'9;><?>GIJ<LKM<?>A2;EGFNDO0fi>2;FOPQ2R0'58DO0fi> DOFTSG2%.?FHB'9 ><1>GIUV-WSG9;0fi2 0fi2YXZS<LDTB[.FOP B'@\S.?F]065^2;E<LK<LK_.?FHB'9;><?>GI2;0N9;FHDO0fiI3><1`TFRE B'> abc79Y<?2;2;FT>dD E B'9BfiDO2;FT9OKe59;03@fD .gB3K;Kh<ji FHakFOPlB6@\S.?FHKTUm0fi> DOFTSG2n.?FHB'9;><?>GI<LK2 EGFoKYpGqGrYFHDO20'5sB6>%<?>Z2;FT> KtF2;EGFT0fi9;FT2t<gDTBu.vKY2;p aX%pG> aFT92;EGF> B'@RF0'5nwyx9;0fiq B6q.1X-SGSG9;0[PZ<?@]B'2 FO.1Xzm0fi9;9;FHDO2R{|x-m}o~FHB69;><?>GIZJKt0DTB[.j.?FHakq FHDTB'p KtF]2;EGFR.?FHB'9;>GFT9M<LKM9;FHfip<?9;FHaJ0fi>.?X2;0.1FHB69;>B6>B'SGSG9 0HP<?@]B'2Y<?0fi>J2;02;EGFC2B'9 IfiFT24DO0fi> DOFTSG2%7<?2;EdBNE<?IfiEkSG9;0fiq B6q<.j<?2YXA{hB[.j<gB6>Z2HeHfi'G}OUE<LK+9Y<LD;EB6> a\Ifi9 0u7<1>GIMq 0GaX\0'5#:>G0u7.1FHaI3FKY2;p a3<?FHK+2;EGFoS0ZK;Kh<?q<j.j<?2|XN065v.?FHB'9 ><1>GICB'SGSG9;0[PZ<?@]B62Y<?0fi> K2;0DO0fi> DOFTSG2OK<?>Va3<j#FT9;FT>Z29 FTSG9;FHKYFT>2B'2Y<?0fi> K_pG> aFT9uB69Y<?0fip K.?FHB'9;><?>GI=SG9;0fi2;0GDO0'.LKTUC{;fiFTF\B'2B'9B6rtB6>#Hfilfi-o>2;EG0fi>Xn<?IfiIZKy#HfifilZ039FHB69;> KoB6`O<19OB'><H3'M50fi9B%qG9;0ZBfiaR<?>2;9;0Gap DO2Y<?0fi>#Uj}>N2;E<LKS B6S FT9HG7FB'9 FDO0fi> DOFT9;>GFHaN7<12 EB\a3<jsFT9 FT>Z2:3<?> aN0'5.?FHB'9;><?>GINDTB[.j.1FHadwTKtS FTFHapGSN.?FHB'9 >Gb<?>GI k7E<LD EaFHB[.LK\7<?2;Ez<?@\SG9;0'fi<?>GId2;EGFDO03@\SGpG2B'2Y<?0fi> Bu.oFOD <?FT> DOX0'5_BSG9;0fiq.?FT@KY0'.?fiFT9C7<?2;EFOPGS FT9Y<?FT> DOFfiU>GF0'52;EGF=@]B[<?>a3<j#FT9;FT> DOFHKCq FT2Y7FTFT>A2 EGFDO0fi> DOFTSG2R.?FHB'9;><?>GIAB'> ad2 EGFKYS FTFHapGS.?FHB'9;><?>GI<LKM2;E B'2Hs<?>d2;EGF%.LB'2;2;FT9[#<?2M<LKM2;EGFT0fi9;FT2Y<LDTB[.j.?XkS 0ZK;K|<1q.?FN2;0KY0'.?fiFC2;EGFCSG9;0fiq.?FT@]KM0fiSG2Y<?@]B[.j.?Xp Kh<?>GINB%qG9;pG2;FTb5c0fi9DOFMSG9;0fiq.?FT@Kt0'.?fiFT9HUe0'7^FT3FT9HSG9;0fiq.?FT@Kt0'.?fi<?>GIC7<?2;EG0fipG2.?FHB'9;><?>GIC<LKoxnbcE B'9a<?>A@\0KY2\0'52;EGFHKYFa0fi@CB[<?> KTB'> akEGFT> DOFN<LK<?@\SG9BfiDO2Y<LDTB[.<?>A@\0KY2]DTBfiKYFHKyU EGFN9;0'.?F=0'5o.?FHB'9 ><1>GIDTB'>q FCKYFTFT>BfiKe<?@\SG9;0'fi<?>GIN2;EGF%FOD <?FT> DOX065^BCqG9;pG2;FTb5c0fi9DOF%SG9;0fiq.?FT@fKY0'.?fiFT9qXB3DTfip<?9Y<?>GIKt0fi@\FwTDO0fi>2;9;06.v:>G0u7.1FHaI3FH%2;E B'2n<gKp KYFO5p.2;0%Ifip<LaFSG9;0fiq.?FT@KY0'.?3<?>GI\<?>C5c9;p<?2Y5p.a3<19 FHDO2Y<?0fi> KTU >B%DO0fi>GbDOFTSG2e.1FHB69;><?>GIN2BfiKY:Gq FO5c0fi9;F_.?FHB'9;><?>GI 2;EGFT9;F_<LK>G0fi2FT>G0fipGIfiE<?>50fi9 @]B'2Y<?0fi>=2;0D .LBfiK Kh<j5XB'>=FOPlB6@\S.?F+--8.rfi|fi"$##G <r <+[8ff:(19#& # fi%&# #c<'fiG 3TGHH^''GGG;GTfiTCZRG;G;TcfiOfifiTC;GfiG3\;GTHGC?H';?G\G fifi' 3^3TOH;+;4_G;G;TcfiOG;fi?TY'?fiT[[1nL^tYj# [j?TGfio;O3;%g6;+1OGu1H3?]_[; 6]'3HG;fiG?Tt'?fi?G\O ?THGT;[fi TTC'ZNY TOH;|t THG=?H';?GtQY T]H;OY? H?d;GCOQTY?\TZO[]fi 1GC?H' 1G?;TO';G;fi%4y4|?;fi#HfifiQ8' #M%|#u1O ZYT?Z3veTOjHfi3 O?GNY^%';G\YG;fi%?GTfiGHTfi hLT;GfiC[?N'ntZ% 6L81;TfiO'Y?fi#e13TN;GO ?Y?fi';GfiC[?' NY6 G'NO'?'?;Tfi[LT'fifiGfiOfi\?T;M?cfi;]'t13fi=Gu;t'?fi4'NY'?u'?G;fi?T=THj?N\ZGC' ' T' 61C'oY'?3?GG fi?T]%'oY% 'jL%?;Tfi6Y?fi#\';GNRH' 1T?3Go\T GQGT_'GH'; '%GTH;COl'1G\;6\?? Y' OHy Y; ]t'?GY?fi ;%;GHYe? Y' OHy ' R fiYHCfiC;GHYY61Gt13 GjgG%YT'GTGYLYYLT_; 'jjT '?] 3L;3?VY'?fiMcG;G;%G;fi?T]TMJ;LYT Yfi# G?H' 1GG;GOH; fieGO? H?\G;'fifiGOfi\GG't13 [#O ?T Ofi^Gt1O \H;Ot1o G?;?Y?fiC T? \3G6\Tfi;GT;fiZOcT;Y?G Gcfi;C[lTO[jg;ML';TYHOY?fi yNH;tT OfiG^3gj?fifiG+?H';?GCG;fifiO';%T H3? Gc'j?u1G]'GGT 4Ofi |gT\= g3;6fi]u1 yt ;k; 'Hfi VfiC[??k;G] Lfi;8gMG';;YH;+'\O ?Tu13fiY?;G o6;+1;T;Ht;H%?]8?H';?G\[?fifiY?;G fi Go Lfi;'[?fifit1 G; ''3H^3?GGG;GYH vT6Y?fi'\fiC[?[\c;fi ;G Lfi fiOjfi;'R1+? Y6 OH' ;GG fi?T] ?4 '3][?\';GO?Y61Gt13 TZ' _G;G OHfi#3G;GG'?Tu13fiY?;Gfi;Gfi][?#j YTfi[;G^ '\?? Y6 OH' GO1Y'?GY?fiL[4O; L[ ;'?+?4 G^G QOH;yfifi?\ GO16 YT OfifiOfi Y; OY?G'O ?TG;31Tt'?fiTcfio G?GGG4fiC[?T6 \OfiRGG'Y?fi [j?=?Z;OfiO'?fi;Le ' TH%';?;T;HY H?]O '%??G\;GOfi 3?Y?fi G TL;NY %?H';?G%L+]fioO3\GG'Y?fi u?\ON 1TZh?G\;'\?? Y6 OH^6 ]Y61Gt13 G;HYTGj HR3;][6\Tfi; '^T'G G;H3;YG fiLYH' CG YGT;3gtHNcfi;C6nYTHGN?H';?G GT;'\?He'Y TOH;hcG;fi?TY613?G' G u3LHZ;H3;GT' AdYH' AG fifi' ;HYHOY?fiO?fiGc'\Tfi;g fiYHN3Yfi\'fiGG;T3?fi fi \;T 3;;HCH'Yj?TM|o6't'CfiT ujHfi3fi ff#3T [j[fiGHOGM\T;GGG'' [?Q|g%' |1L'%;N ''?H';?G ?; 'e^fi?N;H3?;;G_?H';GT;%fiG;GGo'6GG;[Z?]' O1O3;;HOG;31TWY'?fiT?;?fiG;3 'j?Yfi' Y%fie??H';?G %;H31 ];GM?H';GT%;C NY TOH;|fi'ZY6Y?fi ';\G fi?T3LY;t1GGt13GGG'N;\ G1H6;GTHGT;o [fi TTtfi\fi;GT';;TRG;_3;][j TY THG?H';?GNfi ? 3GT#v[fi GO1GT?Z tfi#H3fiGfi fiGGO']'L'NeG;TH [fi'TfiTH3\ZY+'#;GHYe3;][j H'Y?fi n'sYTHG\?H' 1GM Yo\H3YG;'G;fi?T\Y613?G% TYcfi;]6 Ot ;]3;GG4T'GGHOG ' H=?Y'?3?G%G;fi?T 3GT#H3 fi;GMZG% T'GjvT'Y?fi MfiG?6 YTY?G3GT;k;;O?GT4? Y3eHfifiO \ Oj?TfiN; 6;GHYR\HfiYG;HM';\ ZGfiO[?GH%;YOcvfiecfiG G't13%3;fiG Y;GT3;'YTHG\?H' 1G\OfiR ''?;%;G' [?GhL'Ofi OTG1H6;?G\?N;G?H';?GR6\Tfi; G6?'?G\;GY6 G'GfiOtgO?NOfi\?O1YC;GTfi;fi Yo Gofit;TfiYo3YZRG;fiYLOfi\?OZ?YNfifiG\H3YG;' TYcfi;]6 Ofi %;H31 ]NY TOH;hc^YTHG?H';?G=G;fifi6;C;HY?_1k\G;31TY'?fiT+GZYfit;TfiYfiY\G;fiYLO3\?OZ?YMgn T; T; '%; 6'v;GeGfi?H';?G%G;G;TcfiOG;fi?TY'?fiTH=fi;T'fiT[H;G1H6;?G?YOj% YOfi tG\fi?\ '?Gfi%L['\3GZ'Y?\%' R 61Gfi%L[G4T'Ol'R1Hyoefi; 'H fiTO33?G];CfiGoO ?Y?fi# ;GY6 G'Rcfi;]6^Ofi\j?TfiGY?% H'Y?fi3+?Zfi=GG;'jj?G ' ?\G;'fiT\T+?;G'' \fiL V G\G;fifiO'WL GVGfifi ujc3?H';?GG;GOH;YHM HT' t];GT\Gfi 'Gfi;G3YZRG;fiYLOfi\?OZ?YN'G fi?TY613?Ge'^T3TH\fi;YfiGLYYLT' H!fi"$#&%('fi)*fi+,#-'fi*fi).0/1%('fi243!%('456..0798fi6;:9.*fi'fi<fi=><9?@BACEDGFHAI@JfiCEAKCMLIDONECPLIQ9FR@BACED,LINTSAQGFTU9VXWYL0FJ9LCENTSZL[\^]IL[U9LINTSAQYLCE\_VMAQ9FRSZ`a\^CE\`bLFH[\LICEQaSQfiK,JfiCEAc]0S>`a\`dNEWfi\^efSD JfiCEAI]\gNXWfi\LFTeaD JfiNEANhS>ViVMAD Ja[\MjkSNTe AI@-JfiCEA0KCPLIDl\Mjfi\VMUfiNTSAQ-mnoWaU9F^p0qr\is9\M[tS\^]\NEW9LINSD JfiCEAI]0SuQfiKNXWfi\_qoACPFTNXcvV^LFT\LFTeaD JfiNEANTSZVfVMAD Ja[\MjkSNTewA@xJfiCEA0sa[u\^DyFhAI[]SQfiKwV^LIJfiNEUfiCX\FL,VMAKQaSNTS]\M[eSQkNX\^CE\FTNTSQfiKb@zA0CED{AI@|FhJ9\^\`aUfiJ}[u\LCEQaSQfiK9m,~g[NEWfiAUfiK0WLIQaeY`a\VMCX\LFT\dSuQLFTeaD JfiNEANTSZV VMA0D Ja[\MjkSNTebS>FU9FT\M@BUa[xLIQ9`dSQaNE\^CE\FTNTSQfiK9pSuQbNEWaSZFoJ9LIJ\^Cqo\qoSt[[s9\_VMA0Q9VM\^CEQfi\`qoSNEW,[\LICXQaSuQfiK,AI@J9AI[eaQfiADSZL[cBNTSD \JfiCEAsa[\^DFhAI[]\^CPF@BACg`aADGL!SuQ9FHqgWaSZVXWV^LQGAQa[e,s9\FTAI[]\`dSQ,\MjJAQfi\^QkNhS>L![NTSD \iSuQ,NEWfi\qoACPFTNoV^LFT\qoSNEWfiAUfiN|[u\LCEQaSQfiK9mQ\VMNTSAQ(pqr\fSQaNECEAfi`aU9VM\ NEWfi\ JfiCE\M[tSDSQ9LICTS\FA@rJfiCEAsa[\^DFTAI[]SQfiK;LIQ9`4~[\LICEQaSQfiK9m Q\VMNTSAQ fipqo\SQkNXCEA`aU9VM\oAUfiC@BACEDGL[I@BCPLID \^qoACXg@BACFTJ9\^\`aUfiJf[\LICEQaSQfiK9mCMLq|SuQfiKiAQ_JfiCTSACCX\FTUa[NPFSQY~c[\LICXQaSuQfiKpfiqo\JfiCEAI]\1LK\^Qfi\^CPL[-NEWfi\^ACX\^DOSZ`a\^QkNTSt@BeSQfiKwVMAQ9`0SNTSAQ9FFTUaVXS\^QaNgNEAGL![[AIqFTU9VXW[\LICEQaSQfiK9m Q\VMNTSAQpqr\,LIJfiJa[eYA0UfiC@BCPLID \^qoACENXA,[u\LCEQaSQfiKVMA0QkNECXAI[xCXUa[u\FLIQ9`}`a\FEVMCTSs9\LIQSD Ja[\^D \^QaNPLINTSAQLIQ9`}\MjfiJ9\^CTSD \^QaNPL[oCE\FTUa[NPFSQNEWfi\GFTeaDs9AI[tSZVdSQkNE\^K0CPLINTSAQ`aADGLSQ-m Q\VMNhSuA0Qpqo\_LIJfiJa[ewAUfiCi@zCMLID \^qoACE NXA [\LICEQaSQfiKDGL0VMCEAcBAJ\^CPLINEACMFHSQwNEWfi\ `aADGL!SuQbAI@rSKWaNUfi^M[\m Q\VMNTSAQ(pqr\,`0S>FXVMU9FEFAUfiCqoACXSQ4CE\M[ZLINTSAQ}NEAGJfiCX\^]SAU9F@BACEDGL[tSLINTSAQ9FA@|FTJ\^\`aUfiJ4[\LICEQaSQfiK9mQ\VMNTSAQ}kpqr\_`0SZFEVMU9FXFFTAD \@zUfiNEUfiCX\_\MjfiNE\^Q9FRSAQ9FNEA A0UfiCg@BCPLID \^qoACXp0SQ9VX[U9`0SuQfiKb[\LICEQaSQfiK,@BCEADUfiQ9FTAI[]\`JfiCEAsa[\^DGFLIQ9`,\Mjfi\^CPVXSZFT\F^mi\VMNTSAQY VMA0Q9VX[uU9`a\FNXWfi\J9LIJ9\^Cm09vRvHvfiNEWfiAUfiN[AkFXFAI@K\^Qfi\^CPL[tSNTepkqo\gLFEFhUfiD \!firNEAs\gNEWfi\L![uJfiW9Ls9\^NoAI@NXWfi\o[ZLIQfiKU9LIK\iAI@FTNPLINE\`a\FEVMCTSJfiNTSAQ9FpLIQ9`U9FT\d@zA0CrNXWfi\_FT\^NgAI@saSQ9LICEewFTNEChSuQfiKaFoAI@[u\^QfiK0NEWYm~Tk^tl_SZFLiNEUfiJa[\_TP MPpqgWfi\^CE\pa4 SZFLFh\^NAI@B0B^^pSZFJfiCEAVM\`aUfiCX\NEACE\VMAKQaS^\LFhUfis9FT\^NAI@FTNPLNE\FSQwLFNXWfi\ra^BB^^pLIQ9`SZFLFT\^NAI@xMfiTBXI!EMIIkpqgWfi\^CE\\LVXW4SZF_L,JfiCEAfiVM\`aUfiCE\qgWaSZVXW4NPLI\FL,FTNPLNE\SQL0FoSQfiJfiUfiNLQ9`wA0UfiNEJfiUfiNPFLIQfiANXWfi\^C_FTNPLINE\L[ZFTASQrmnoWfi\4VMADsaSuQ9LNTSAQAI@KAaL[ZFGLIQ9`AJ\^CPLINEA0CPFfSZFV^L[t[\`NEWfi\}vfiP oP0BZ}AI@bm~BIT,S>FL FT\^NoAI@`aADGL!SuQ9F`a\M9Qfi\`AI]\^CiNEWfi\_FELID \_Fh\^NgAI@FTNPLINE\Fm\`a\^QfiANE\gNXWfi\CE\FhUa[uNoAI@-LIJfiJa[eSQfiKGLQ AJ9\^CMLINEACoNEALfFTNPLINE\gsae TIPm~Tk^tS>FoLfFTNPLINE\f;rm~JfiCEAsa[\^D{S>F^tkHSt@NEWfi\^CX\S>FL FT\0Ufi\^Q9VM\_A@xAJ\^CPLINEA0CPFHlp-p9PpLIQ9`LFT\0Ufi\^Q9VM\AI@FTNPLINX\Fop9pfiMpFhU9VEW,NEW9LINoLxf^pvs-@BACoL[t[&@BCEADiNEApkh^9PpLIQ9`Vx FXLINTSZFR9\FHNEWfi\gKAaL[&dm QGNXWaS>FoV^LFh\pkSZFrLd^tkB,^Pk9MAI@pfiLIQ9`,SZFNXWfi\taAI@NEWfi\_FTAI[UfiNTSAQ;FT\0Ufi\^Q9VM\_mn|Wfi\a^t^^SZFLFTeaQkNMLVMNTSZVHD \LFTUfiCE\iAI@9NEWfi\VMADdJa[u\MjaSNTeA@&LJfiCEAsa[\^DFTU9VXWGLFSNPF[\^QfiKNXWqgWfi\^Qb\^Q9VMA`a\`SuQbsaSQ9LICEem @SZFLIQwLICEsaSNECMLICEe JfiCEA0sa[u\^DSQ44paNEWfi\^QSNPFgFRS^\k0SZFmANTSZVM\NEW9LNA0UfiC`aA0DGLSQFTJ\VXStV^LINTSAQSZFQfiA0NGLFf\MjJa[tSZVXSNwLFfNEWfi\w`aADGL!SuQNXWfi\^ACEeU9FT\`}SQNTekJaSZV^L[FhJ9\^\`aUfiJ[\LICEQaSQfiKfJfiCEAKCPLDGF[tS\_ $ SQkNEA0Q-pk aPmnoWfi\oAJ9\^CPLNEACPFQfi\^\`fQfiANs9\g`a\FEVMChSus\`fSQNEWfi\ _M @BACEDGL[tSZFTDwpLQ9`KAkL![>FQfi\^\`fQfiANs9\H[AKISZV^L[fi@BACEDUa[ZLF^m Qf@ LVMNpNEWfi\^eQfi\^\`dQfiANs\1`a\VX[ZLICPLNTS]\M[e CE\^JfiCE\FT\^QaNE\`,LINL[t[vpfisfiUfiND,Lefs9\`a\FEVMCTSs9\`,sae JfiCEAfiVM\`aUfiCE\FHqgWfiAkFh\gCEUfiQNTSD \iSZFCX\LFTAQ9LIsa[e sAUfiQ9`a\`mnoWaU9F^paAUfiC [u\LCEQaSQfiK@zCMLID \^qoACEfCE\UaSCE\FHNEWfi\i[\LICEQaSQfiK NE\VXWfiQaSZUfi\FNEA s\_D A0CE\SQ9`a\^J9\^Q9`a\^QaNAI@xNXWfi\_AJ\^CPLINEACoCE\^JfiCX\FT\^QkNMLINTSAQNEW9LIQbNEWfi\NECPL0`0SuNhSuA0Q9L[D \^NEWfiAfi`fiF^mn|WaS>FL[t[AIqFgVXWfiAkAkFSuQfiKdNEWfi\A0J9\^CPLINXACoCE\^JfiCE\FT\^QaNPLINTSAQ,qgWaSZVXWSZFos9\FhN|FhUaSuNX\`NEANEWfi\`aADGLSQCPLNEWfi\^CoNEW9LIQs9\MSQfiKVMAQ9FTNECMLSQfi\`saeGNEWfi\LFEFTUfiDdJfiNTSAQ9FoAI@NEWfi\[\LICEQaSQfiK,NE\VEWfiQaSZ0Ufi\mQNEWfi\FTJ9\^\`aUfiJb[\LICEQaSQfiKGFTefiFTNE\^D,FrFhNEU9`0S\`saeGNEWfi\\MjfiJ9\^CTSD \^QaNPL[xVMA0D DUfiQaSuNTepfiNXWfi\KAaL[ZFgLIQ9`AJ\^CPLINEACMFLCE\U9FTU9L[t[eJ9LICPLIDd\^NE\^CTS^\`mnoWfi\FT\wFheFTNX\^DGFL[ZFTA;[\LICEQVMAQkNXCEAI[gCEUa[\F LIQ9`DGLVMCEAcAJ\^CPLINEACMF_q|SuNXWJ9LICPL\^NE\^CPF^m \LCEQaSQfiKJ9LICMLID \^NE\^CTS^\`CEUa[\F,LIQ9`DGLVMCXAcBAJ9\^CMLINEACPFfDGL\FfSNJ9AaFEFRSsa[\gNEALIJfiJa[efNEWfi\^DNXAgJfiCEAsa[\^DGFAI@9LICEsaSNECMLICEe_FRS^\m~QfiA0NEWfi\^CxL0`a]!LIQaNPLIK\A@9J9LICPLID \^NX\^CTSLINTSAQfffifi!!"#$%&('*),+-.!/0&212&3)45)$67.!880134)$+-'$.!9:-<;$=013->;$-?@=;@'A&3BC-@134D9E.!F04G)H&39:-'I!JK+-;$-<-.C?,+.!88012&(?.!)H&36CFL/0&3FM'N)$+8.!;O.P9:-)$-;O'N)$6DM&2Q*-;$-F0).!;,RC=9:-F0)O'SUTVF0WX6C;$)$=F.!),-@1Y4CI+6!J-B-;IC8.!;@.!9:-)$-;H&3Z.!)[&Y6F\.1]'[6&3F?@;$-.C'H-')$+-^?@6C9:8=)@.!)H&36CF._1?@6`'[)a6!W&3F'H)O.!F0)H&(.!)H&3FRb),+-^6C8-;@.!)$6C;O'dce6C;f;$=013-'@gOSihj+-Fj)$+-^F0=9L/-;a6!W8.!;O.P9:-)$-;O'5?.!Fk/l-m.P;$/0&3)$;O.!;H&2134n+0&YR+*Io)$+-p&YF'[)O.!F0)H&(.!)H&36CFn8;$6C/013-9q&]'pr7sute?@6C9:8013-)$-p&YF R-F-;O._1eSv F-JK._4),6),+-6C;$-)H&(?._12134f12&Y9\&Y)V)$+0&('K?@6`'H)<&(')$65=88-;$tX/l6C=FMw)$+-7F0=9L/-;K6PWU8.!;O.!9p-)$-;O'o6PWU)$+6C8l-;O.!)$6C;@'I9f.C?@;$6CtX6C8l-;O.!)$6;O'I.!FM5?@6CF0)$;$6!13tX;$=013-'N)$67.G?@6CF'H)@.!F`)_SUx+0&('y-F'H=;$-'N)$+.!)y/6C)$+5)$+-)H&39:WX6C;G&3F'H)O.!F0)H&(.!)H&36CF .!FMz)$+-pF`=9L/-;56!W{M&2Q*-;$-F0)&3F'H)O.PF`)H&(.!)[&Y6F'.P;$-L86!1340F6C9L&(._1('5&3F^)$+-L13-FRC),+6!WU),+-D'H)O.!)$-M0-'$?@;H&38)H&36CF*S|A)&('860'$'A&3/013-D)$6:-@})$-FMf6C=;V;$-'H=013)O')$6:'H=?,+a8.!;O.P9:-)$-;H&3Z-M~M069E._&3F'J&3)$+'H6C9p-'H=0&3)O.!/013-z;$-'H),;H&(?@)H&36CF'f6CFb),+-F0=9L/-;f6!WD8.!;O.P9:-)$-;O'p6C;f)$+-@&3;f&YF0)$-;OM0-8l-FM0-F?,&3-'cxN.!9L/-CINrK-J-@121eI>6`'[-F`/013606C9~IKC0gOSL|AFzW.?@)I*6C=;L.!88012&(?.!)H&36CF 6!WK)$+-LWX6C;$9f._1UWX;O.!9p-J6;$)$6z)$+-~'H409L/6!12&(?f&3F0)$-RC;O.P)H&36CFbM06C9E._&3FjM06`-'5&3F0BC6!13BC-~.!Fn&39:8012&(?,&3)a8.!;O.!9p-)$-;:)$+.!):M0-F6C),-'L)$+'H=/l-@}8;$-','A&36CF6PWD)$+-k?@=;$;$-F0)a-@}8;$-'$'A&36CF),6kJK+0&(?,+.!Fj6C8l-;O.!)$6C;f&('a.!88012&3-MSV6!J-BC-;IWX6C;'A&39:8012&(?,&3)4L6!W-@}86`'A&3)H&36CF*I!J->?@=;$;,-F`)H134;$-'[)$;H&(?@)*6C=;*),+-6C;$-)H&(?._1CWX;O.!9p-J6;$K)$6KF6F8.!;O.!9:-),-;H&3Z-M6C8l-;O.!)$6C;@'SH`2_D W6;.zM06C9E._&3Fbq&('L.zM0-)$-;$9L&3F0&('H)H&(?d8;$6CRC;O.P9)$+.!)5)O.!C-'L.C'ff&3F8=)E.8;$6C/013-9~IlCI.!FMf6=)$8=)O'<&3)O'K'[6!13=)H&36CF~'H-=-F?@-&2W'H=?,+a-@}`&('H)O'I6C;)$+-D'H8l-?,&].1u'H409L/6!1!K&2Wo&3)M060-'KF6C)-@}0&('H)S`,C`]feCOL &(':.z'H-):6PW78;$6C/013-9'H6P1YB-;O'S^|W &(':.z'H8.C?@-a6PW7+`4086)$+-'H-'I<)$+;$-'H),;H&(?@)H&36CFE6PW ),6D8;$6C/013-9E'<6!WU'A&3Z-Lbf&('?._1213-Ma. eC@ 6!W*+`4086)$+-'H-'.!FM5&('M0-F6C)$-Mp/`4K SK6C;$9E.1134CIW6;7-BC-;,4 ^w I)$+-;$-&('7.f?@6C;$;$-'[86CFM&3FRa8;$6C/013-9'H6P1YB-; _~~V 'H=?,+z)$+.!)cHgo cg*&2W{NCbw.!FMp=FM0-@F-Mf6C)$+-;$J&('H-CS<x+- 2@CC@(0CLC 6; CLC6!W>.L+04`8l6C)$+-'A&('G'H8.C?@- &('7M0-@F-Mw)$6L/-136CRff V .!FMp&('GM0-F6C)$-Ma/04a`Awc K gOS:GC<e>$C* !*XN>C*<el|eFf)$+0&(''H-?@)H&36CF*I0J-7M0-'$?@;H&3/l-6C=;o13-.!;$F0&3FRLWX;O.!9p-J6;$SNy&3;O'H)I)$+-7M06C9E.&YFa'H8l-?,&?.P)H&36CFa&('oRP&YB-F)$6a)$+-513-.!;$F-;_Sx+-p)$-.C?,+-;)$+-F^'H-@13-?@)O'D.!Fn.!;$/0&3)$;O.!;,4a8;$6C/013-9M&('H)$;[&Y/=)[&Y6F.!FMz.f8;$6/01Y-9'H6!13BC-;_Sbh^-z.C'$'H=9p-a)$+.!):)$+-;,-a&('E.P)L13-.C'H)f6CF-~8;$6/01Y-9'H6!13BC-;p&3Fb)$+-w+`4086)$+-'A&('E'[8.C?@-~6!W)$+-L13-.!;,F-;D)$+.!)G&('WX=F?@)H&36CF._12134^-C=0&3B!._13-F`):),6a)$+-:)$-.C?,+-;'G8;,6C/013-9'H6!13BC-;_I&eS-CS3Iy6CF-:JK+0&(?,+6C=)$8=)@'>),+-D'$.!9:-D'H6P1Y=)[&Y6F~.C')$+-G)$-.C?,+-;'<8;$6C/013-9'H6!13BC-;K6FE-.C?,+a8;$6C/013-9~S<h^-?._121'H=?,+~.8;$6C/013-9'H6!13BC-;&3Fw)$+-G13-.!;$F-;'V+04`8l6C)$+-'A&('G'H8.C?@-CI. XH`2C2_x{+-:13-.!;$F0&3FR ._13RC6C;H&3)$+9+.C'L.C??@-'$'L)$6^.!F 6;O.C?,13-~?._1213-Mj v7!> tes< v7*< )L-.C?,+?._121eIG v7!> tes> v7*< ;O.!FM06C9\1Y4b?,+606`'H-'L.z8;$6C/013-9&3Fb)$+-~?@=;$;,-F`):M06C9E.&YF*IK'[6!13BC-'5&3)='A&3FR:)$+-G)$-.C?,+-;'u8;$6C/013-9'H6!13BC-;I.!FMp;$-)$=;,F'>),+-3l!X__!XA!*o8._&3;I0JK+0&(?,+f&('?.113-M.!F !CV2 X(l(0fK&]'5.a'H-)6!WK'H=?$+)$;O._&3F0&3FR~-@}.!9:8013-'SLh^-:.C','H=9:-L)$+.!)G&2WK)$+8;$6C/013-9&('KF6C)K'H6P1YB!.!/013-L/`4f)$+-D),-.C?$+-;_'<8;$6C/013-9'[6!13BC-;I0&3)K6C=),8=)O')$+-8._&3;L3!!Xz_GOS|HM0-._12134CIU),+-R6`._1U6!WK'H8--M0=81Y-.P;$F0&3FR~&('G)$6pFM^.f)O.P;$RC-)K8;$6/01Y-9'H6!13BC-;G&3F^)$+-513-.!;$F-;_'+04`8l6C)$+-'A&('G'H8.C?@-CS>V6!J-B-;I0)$+0&('<&('KF6C)K._13JK._4'<8l6`'$'A&3/013-L/-?.!='H-56C=;K9:6M0-@1U6!Wy1Y-.P;$F0&3FRa;$-@12&Y-'6CFz;O.!FM069L134?,+6`'H-F~),;O._&3F0&3FR~-@}.!9:8013-'SDV-F?@-CIJ-\.1136!J)$+-513-.!;$F0&3FR~._13RC6C;[&Y),+9)$6E6=)$8=).!F^.!88;$6_}0&39E.!)$-@134~?@6C;$;,-?@)8;$6C/013-9'H6!13BC-;J&3)$+^.p+0&3RC+^8;$6C/.P/0&12&3)H4^._W),-;ff'H--@&3FR~.f;$-.'H6CF.!/013F0=9/l-;:6!WK-@}.!9:8013-'Skx{+-a8;$6C/013-9'H6!13BC-;LF--M'L6F01Y4n)$6z/-a.!88;$6_}0&39E.!)$-@134 ?@6C;$;$-?@)5&3F )$+'H-F'H-5)$+.!)V&3)79E._4pW.&1o)$6:8;,6M0=?@-:.:?@6C;,;$-?@)ff'[6!13=)H&36CFwW6C;G.:8;$6/01Y-9J&3)$+z.E'H9f._1218;$6/.!/0&21&3)H4-BC-F^),+6C=RC+^)$+-p)$-.C?,+-;'H=??@--M'&3F'[6!13BC&3FRw&3)Sfh^-E.!;$-pF6J;,-.CM04z)$6aWX6C;$9E.1134^M0-@F-f6C=;9:6M0-@1U6!WN13-.!;$F0&3FRSfifffi ffff"!$#&%'#($!*),+.-/021ff34658789;:=<>5&?@/?BAC6CEDFGAH0ICE/46-J5K-L1/02134E5K789L:*MN34O/P:@C'7Q/SRTD3:@/58-U 58-/9;VWAX3ff789;CY?Y5&?Z?[AX/\EC^]P_`5 MaMN3ff4b/-JV@D3:@/58-dc>ePUf_`/-V@\Y9L35K\ECg3hMfi/iAj4T3;kY0ICl:mDff5&?Y7Q465QkYFL7Q583-onO_/-JD/-JVp7Q/4Q1Cl7qAj4T3;kG0rCl:=?Y30Is'Cl4"teu]P_vw <x7Q/SyCG?g/S?z58-SAjFL7^789;CO?BAC6\l5 {^\6/ff7Q5K3-|3TMz/D3:@/58-}c~e|Uf_:z/58:zFL:Aj4T3;kY0IC':>?Y5&YCb_"/ffC'4E43ff4AX/4T/:@C'7QCl4bE_^/-D/\63-G{"DCl-\EC^A/4T/:@C'7QC'4ff;w <:@/V\6/080`a^ZRB.^"a|_"a9;58\Y94Cl7QF;4E-?CY/ff:Aq0rCG?oB6tqW`MN34@cu_fia9;Cl4TCp5&?p\Y9L3S?NCl-`58789PAj4T3;kW/;kG5K0I587QVpnOq^MY43ff:789;COAj4T3;kG0IC':?YC'7.789;CP-JF;:pkWC'4@3TMo34T/\l0ICP\6/080?3TMo<f:@F?Y7zkWC@A30IV-J3:@5K/ff05K-789;C:@/SS5K:@FL:Aj4T3;kG0IC':f?Y5&YCo_P _dff_z/ff-D789;C0IC'-L17893TM5K78?g58-SAjFL785K78?@46FL--J5K-L17Q5K:@Cp:@Fff?N7kWCZAX30IVS-3ff:z58/0$58-/080a789;CZAj4TC'sl5K3ffF?AX/4T/:@C'7QCl4?@/-DP/ffFYAAC'4@k3FL-DO.3-789;Cp4EF;-J-58-;17Q5K:@CY?g3hMAj4T361ff4T/:O?g5K-c3-58-SAjFL78?g3TM.?Y5&YCb/-D/080qAq43Lk/;kG5K0I587QV}D5?Y7Q465QkYFL7Q5K3ff-?znf3slC'4@ _.`5K789Aj4T3;kW/;kG5K0I587QV}/7.0ICE/S?N7;w MN34/0K0aceU6`PW_j<3ffF;7AjFL78?g/ZAj4T3E14T/:tJ789;/7/EAAj4T3SS5K:@/7QCG?"]58-}789;Ci?NCl-?YCb789L/7aSnO"E_a9;C'4TC.a2t Tq,tb/-DptZ/-JDw 789;C'4TC}5?P/@{SC6DA30IVS-3:@58/0?NFL\Y9789L/78_MY34/:@/SS5K:@FL:~Aj4T3;kY0ICl:?Y5NCp_z:@/SS5K:@FL:?N30IFL7Q5K3ff-}0IC'-L1789g^_b _b _/-D789;C@FYAAC'4gkW3FL-D..3-|789;CAj4T3614T/:O?.5K-cP_`5 M<x3FL7AjFL78?bt _789;C@46FL-7Q58:zC@3TMbtJ5?zkW3F;-JDCEDdkGVzOaEE wYSWHYL&;SPTLKJ'LKESpY^q*S&@I}Q6SpY^SfiaLYlIIz6l'LELr&pSfiZS6SESSSLYrI@6'ffL6a&'SL&l&T&KgQET&S"Wp;Lff'[K@TEo@gpW'YZKiQYpYY6TS&T&`@SLgpYqaBpTKELKS'L^'WY`olgJbZ;S@.T'S6Wff6S}$^L&bW'L&YpYLS$^ESYqL&YTS&Y$WSqKMYFL-\'7Q583-J/0K0IV'L&YL@.'ffY'WL&YTS&YELKT6&|Z&'SY';LW'[KTELTphffY'L'}hgS^&'SYbYdSW'L&oSZ.6YWL&YhS&YbSL&ESTI&dW&;TE&;6Sff&@&PpgpZ6l&L&ESL'G`B^LKJ.&pL[&L&zWLLKI.ESYL&YTS&Y^&6hEr"LL'^E;KT&}Z&'SYlLL'ZTETgpi8&'SL&PI&Y6SWJ&'SL&}@TZohYE'[QL`LYJYLY;@S^WSKESg6L&ffKThhffn@WL&YTS&Y}t ffoLW}&'SWYPWlKl;&oSWP}6YpffLYTS&YlrgWYLEWS}TS&T&^&LrI&Tu&'OS.Y`Y'T'WpL&Yo@SL'[&n@S&EP6L&@L&Yo^SW"ESL@&oW;TY$ffYpTpYT&p' ol@JY'TYL6ShffX@&'SY.ol r`@&'SWHSlL&oSE&}EW'EbffLYhS&Y'YEq"OL&P'ffLWSTWL&Y hS&Y^K^&'SW'}S&'TZ^&}@ffSLIr&T^;&lI&qYZKZ.W'L&YpYL.S.&'S'uL&YTS&Yzh@ dW&YL'YSWgLKKL'Lp&[KTT&P^&fiTffLuP&|T&pgJS&;&}SST&S6SYY6Y&W&ffL&Y=[ Y;hS&T&p&YW$S&;ffY6T'SjWYgWErKSLIIKTS6SpYWY6`SL;SZY^`ffoT&pbY'L'QEX'ET&pWfiLl&oJY6Sff6YLKT6SpYYBEff6bW.T&p.QE'ET&p&ff&ffL"JY6SEW=.ffLYTS&ff&T&p^[&E}WLT&pKpTffpYL&W'L& l&ffT& YSPE'E'&pSL'YTg.ffY6SW6^S.T'@gff BPY"ff6Y^.'ffL&.L&SZ@LzJYgSffY6SWbE'ET&fiiS&L@Kl`&SST&.6SpYYEYYffYW.T&piff"E'ET&}p[&S&OY6^oloJzSWLW6SpZffL'qbTY'L}ffL&Y';^&'SY'WL&YhS&Y`K^&'T'E^@gLEBSL&^YQ6E}WL&YTS&Y'LKK}ffLllIKSL&W}&'SWYPEQ}W}&'SWLYGS-3ff7WW'T'E.@L&YfTS&Y6&O&'SY'ZLL'OTE.Ifffififfff!"#$"%&'(*),+.- &/102)- 3,&(4065 78$"%&9%;:.'),$*%&ff0<5=002'>,?&)A@B$*%&- &ff>("7&(>(*&C02D'').02&ffEF$*)G+&H&IJ?*5 &7.$ffK0<5 7?&L &>("&),7.- :F/F&ff>,0MD(25 7NO&IJ?*5 &7?:8+.:P?);>A(402&0"?>Q- &ff0R0MD?"%8>0S("D77.5 7NG5 7F'T)- :;7)/U5=>Q-V$25 /F&,WYXZ0 L &0">Q5=E[&ff>(2-\5 &(ffK L &C>(*&7),$?),7?&("7&ffE L 5 $"%]/F),("&(*&^7&ffE]7),$25 ),70_)@`&IJ?*5 &7?:,K02D?*%J>0S5 /F'(")35a7N$"%&O$25 /F&U?),/F'.- &b.5a$2:c)@d'("),+.- &/e02)- 35a7N[@f("),/hgUikjTlmd$")8gnikjoQm4K5 7p$"%.5=09'>'&(ffWX9- $"%),DN,% L &c$"("&ff>$"&ffEq>r'("),+.- &/s02)- 3,&(8>0F0<5 /F'.- :t>uE.&$*&("/U5 7.5=02$25=?v'("),N(4>/w$"%>$F/]>'0'("),+.- &/80x$")F02)- D$25 ),70K$2:.'.5=?>Q-\-a:]5 $C?)70<5=02$40)@S$ L )U?),/F')7&7;$40zy6>[E.&ff?*-{>A(4>$25 3,&O("&'(*&ff02&7;$>$25 ),7)@C02),/[&8|5 7Eu)@C?),7.$"(")A-|.7) L -a&ffE.N&pik>J@}D7?$M5a)7mZ$*%>$P02'T&ff?*5\^&ff0 L %.5=?"%q)'&(4>$*),(G),(~),'&(>$"),(02&ffD&7?&8$*)J>''.- :5 7>]NA5a3&702$4>$*&,KV>7E>75a7.$"&("'(*&$"&(G$"%>A$OD02&ff0H$"%&8?),7.$"(")A-6|.7) L - &ffE.N,&F$")02)- 3,&U>7.:8'(")+.-a&/5 7p$25 /F&~')- :.7),/U5=>Q-S5 7c5 $40H0<5 &,W5 7?&U$"%&O5 7.$"&("'("&$*&(5=0D02D> -- :^b&ffEKT$"%&%.:;'T),$"%&ff0<5=0O02'>,?&U)@'("),+.- &/02)- 3,&(0CE5 ("&ff?$2- :u?),("(*&ff02'),7E0H$*)J>]%.:;'T),$"%&ff0<5=0O02'>?&P)A@R');0"0k5a+.- &?),7.$"(")-|;7) L - &ffE.N,&,WSXO0"02D/U5 7NF$*%>$R$"%&(*&5=0>7]&IJ?*5 &7;$$4>(*N,&$R'(")+.-a&/h02)A-a3&(R5 7c$"%&H%.:;'T),$"%&ff0<5=0U02'>,?&])@'("),+.- &/02)- 3,&(40Z5 /F'.-\5a&ff0[$"%>$~$"%&("&U5=0U>$4>("N&$H@}D7?$25 ),75 7$*%&J?),(*("&ff02')7E5a7N%.:;'T),$"%&ff0<5=0F02'>?&8)@C?),7.$"(")A-R|;7) L - &ffE.N,&,W,'&&ffE.D'-a&ff>A("7.5 7Nu)@C>c%.:;'T),$"%&ff0<5=0F0M'>,?&8)A@x$>("N,&$'("),+.- &/0M)- 3,&(40F?>7q+T&p>,?*%.5a&3&ffEq+.:TXOR- &ff>(*7.5a7N)@Z$*%&J?),(*("&ff02')7E5a7N%.:;'T),$"%&ff0<5=0]02'>,?&)@?),7.$"(")A-6|.7) L - &ffE.N,&,WG9) L &3,&(QK L &PE.)c%>Q3,&F>7v>,EE5 $25 ),7>Q-'("),+.- &/e)@?),7.3,&("$25 7Nc'(")+.-a&/[02)- D$25 ),7'>Q5 (40U)@H$"%&c$4>("N&$P'(*),+.- &/02)- 3,&(F$*)u&b>A/F'.- &ff0U)@O$"%&c$4>("N&$P?),7.$"(")-|.7) L - &ffE.N,&,W&~$4>|,&O>,E.3 >A7;$4>AN,&O)@d$"%&PE.),/]>Q5 702'T&ff?*5^V?>A$25 ),7ikE.&^7.5 $25 ),7v)@6N);>Q-=0>7E),'&(4>A$"),(40mS5 7E.)A5a7N$"%.5=0H?),7;3&(40<5 ),7`W&7?&F02'T&&ffE.D'p- &ff>(*7.5a7N[5 7v),D(@f(4>/F& L ),("|]?),70<5=02$409)@d$ L )F02$"&'0zy5 (402$QK$"%&'("),+.- &/F0M)- D$25 ),7p'>Q5 (40x)@$"%&O$4>A("N,&$R'(*),+.- &/102)- 3,&(C0M%),D.-=E]+&G?),7.3,&("$"&ffE]$*)P&b>/F'.- &ff0)A@$"%&$4>(*N,&$G?),7.$"(")-|;7) L - &ffE.N,&]D0<5 7Nv$"%&JE.)/8>Q5 70M'&ff?*5\^V?>$25 ),7`W,&ff?),7EKd$"%&F&b>/F'.- &ff0U)A@x$>("N,&$?),7.$"(")-|;7) L - &ffE.N,&H/UD02$+&HN,&7&(4>Q-\5 &ffEcD0k5a7N]02),/F&9@}D7?$M5a)7]-a&ff>A("7.5 7N80"?*%&/F&,K>7EF$*%&O("&ff02D.- $/UD02$+T&O'.-aDNN,&ffE5a7.$")F$*%&H5a7.$"&("'(*&$"&(C$*)n?(*&ff>$"&G>7p>''(*)ffb.5 /8>$"&O'(*),+.- &/02)A-a3&(ffW),(0k5a/['.-5=?*5 $2:p)@S&b'T);0<5 $25 ),7`KT$"%.5=0_@}(>/F& L ),("|F>,0"0MD/F&ff0_$"%>$$"%&O/8>Qb.5 /UD/'(*),+.- &/10<5 &Pj5=0N5 3,&7`W),(>UNA5a3&7c'("),+.- &/E5=02$*(25 +D$25 ),7`K$*%.5{09?>7p>Q-=02)F+T&G&ff>,0<5\- :c&ff02$25 /8>A$"&ffE[@}(*),/&b>/['.-a&ff0+.:c$"%&P02$>7E>(4E'(")?&ffE.D(*&P)A@Y02$>("$25 7N L 5 $"%0<5 &cU>7E]5 $"&(4>A$25 3,&- :E.),D+.-\5 7Nc5 $O>7Ec3,&(M5@f:5a7N5 $ L 5 $"%v>F02D.IJ?*5 &7.$2- :]-{>A("N,&P0M&$)@(4>A7E.),/U- :cN,&7&(4>$*&ffE]'("),+.- &/80ikC>A$4>(4>AM>7`KQ,,.m4W]` ff{`V2;zQQffdv{ ?)70<5=02$"&7.$C nUf2,;H,9V\H ,ffffQ4~ ffffk j ,Gp;[fk=T=.U,9V\Z fQ fk< j5 /U5\-=>($*)P/]>7;:]XR- &ff>("7.5 7N8>Q- N,),(M5a$*%/80K.$"%&G02'T&&ffE.D']-a&ff>A("7.5 7NJ>Q- N,)(25 $"%/80 L &Z?),70k5{E.&()L ,("|v+.:&Ic?*5a&7.$2- :^-a$*&(25 7N$*%&8%.:;'T),$"%&ff0<5=0U02'>,?&[@f),(P>'("),+.- &/02)- 3,&( L %.5{?*%5=0U?),70<5=02$"&7.$L 5 $"%F$*%&$"(4>Q5 7.5 7NF0">/F'.- &,WR_&@f),("& L &'(")3,&$*%&),("&/80d>+T),D$R'>A("$25=?D.-=>(%;:.'),$*%&ff02&ff0R02'>?&ff0K L &^(402$G0M$4>$"&U>]N,&7&(4>Q-R$"%&)("&/ L %.5=?"%r5{0~>cE5 ("&ff?$P?),70M&ff,D&7?&])@$*%&P(*&ff02D.- $40H5 7TXOR- &ff>(*7.5a7N)@S^7.5 $"&G%.:;'T),$"%&ff0<5=0H02'>,?&ff0Hi2d- D/F&(QKVY%("&7.@}&D?*%.$ffKH>D0"0<- &(ffK >("/UD$"%`Kff,.m4WU .VQ`Q "UfffAk,,U,F,Tp"u;*,;{8<,pMU,\,U,kfUV2;zQQff*,U,P=8=]`ffR. *H,\P,\p",.4= j q;ffu,,4; G[4,zr\,;F ,=;, ]c6 G~f zG;~4Q 4,f,2Chcw j dR,FG=AV.4xf\U,=O;O,Q,G,]Q2,f,*P8~4,\4QP . m,- 7C9u- 7 f2,;Fz ,9V\z4~fffPfO=f=U,\,U,= j C C R,TF=;~{C2O;U,,U,4Q 4f=,,];Uf2,=T=.] z4R~,;;UOV2;z\ff,\QffH=] ;p{G, fffT p;Ff2,;nfi fffififfff!" #$"%&('*),+-/.1032546472)98;:<)<=?>@=BA' =C.@+EDF8.@+ =;2>@=;GH.I2J.K)<2!L-5>@.KDFMBNE4F-5>O+EPQLR2.@+ =8(DF8O8KL-MB=8TS8K=;.U8V250BM 2'E.@>@254W>@NE47=8X-5'YCGZ-MB>I2[32L=;>B-5.@2>U8T\,]=^D$YE=;'E.KD603P_8`NEabMID7=;'Q.VMB2'YD7.KD72'89.@2cN-5>U-5'E.@=;=V8KL=;=YEN L47=-5>@'ED7' cdDe'C=-MI+?250f.I+ =8K=X.K)<2V+QPEL2.I+ =8(DF8g8KL-MB=8;\hji1k9lmonprqsptvu wpyx/nwzgn{zslR|} ' =~),-P.I2 NED64FYO=BaOMIDe=;'E.,L >@2E4e=;G8K2547=;>U8DF8<EPV47=-5>@'ED7' cZMB2'E.@>@254o>@NE47=8,-8D7'!,D7.UMI+ =B464:.@c25: <-5' =;>3(Ds:Ej2><D7'9XV;VD7'E.@2':oU\<2'E.@>@2/4>@NE47=8>@=YENMB=18K=-5>UMI+OQP8K=B47=MB.KD7' c: >I=K=MB.KD7' cV2>r2>UYE=;>KD7' cV2L=;>B-5.@2>U8r-5L L >@2L >`D$-/.@=B47P\j&s'.@+EDF8,8K=MB.KD72')<=gMB2'8(DFYE=;><47=-5>I'EDe' c250MB2'E.@>@254>@NE47=8.@+-5.,8K=B47=MB.-/L L >@2L >KDF-5.@=~2L=;>U-5.I2>U8<.@21-5L LE47PD7'-c5D7=;'8K.B-5.@=\f35oQoRg5y$`77EMB2'E.@>@254W>@NE47=~DF8~-VL-D7>1@^UUU:),+ =;>@=O~YE=8IMB>KD7=89.@+ =V8K=;.g2/0L >@2E47=;G8`.U-5.@=8<2'?),+EDFMI+.@+EDF8>@NE47=8K=B47=MB.U8.@+ =~2LR=;>U-5.@2>,E\,^jDF8,M;-4647=Y?.I+ =VB(2/0E\]=?-8@8KN G1=O.I+-5.V.@+ =?8K=B47=MB.@[s8K=;.U8d250~2L=;>B-5.@2>U8250XYE2GZ-De'8De'v-5>@=OYE=8@MB>KD7=YD7'8`2G1=4F-5' cN-5c=bd\Z]=ZMB2'8(DFYE=;>Z-+QPEL2.I+ =8(DF88`L-MB=Z2/0L >I2E47=;G8K2547=;>U8T:),+ =;>@==;=;>@P!L >@2E4e=;G8K2547=;>~MB2'8(DF8K.B8,250-8K=;.,2508K=B47=MB.@[s8`=;.U8<D7'1:ff2' =~032>9=-M@+C2L=;>B-5.@2><D7'C.@+ =YE2G-D7'\,o=;.X<1R=.@+ =X8K=B47=MB.@[s8`=;.U8,>I=8K.@>KDFMB.@=Y.@2VL >@2E47=;GZ82508De;=*W\+ =1+QPEL2.@+ =8(DF8V8KL-MB=1N8K=8-A =Y.I2.U-4j2>UYE=;>KD7' c!25=;>~.@+ =12L=;>U-5.I2>U89250,.@+ =ZYE2GZ-D7'\<+EDF8<2>BYE=;>KD7' cXDF8<N8`=Y.@2X>@=8`2547=XMB2'E DFMB.U8<R=;.)<=;=;'O-5L LE46DFM;-5E47=2L=;>U-5.I2>U8j)+ =;'OG12>@=9.@+-5'2' =8K=B47=MB.@[s8K=;.<MB2'E.U-D7'8j.@+ =c5D7=;'1L >@2E47=;G?\W&('1),+-5.W032546472)98;:)<D7.@+ 2N .j472Q8@8W250oc=;' =;>U-46D7.KP:E)=,-8I8KN G1=.@+-5.<.@+ =~2L=;>U-5.I2>U8<-5>@=~'EN GV=;>@=YN8(D7' cZ.@+EDF8<2>UYE=;>KD7' c\<De=;'-L >@2E47=;G-5'YO-18K=;.,2/0MB2'E.@>@254>@NE47=8;:-L >@2E47=;GH8K2547=;>D7'JLEDFM@ 8.@+ =147=-8`.X'EN GR=;>@=Y2LR=;>U-5.@2>~)+ 2Q8K=O8K=B47=MB.@[s8K=;.1MB2'E.U-D7'8.@+ =1L >I2E47=;G?:j-5'YJ-/L LE4D7=8^D7.\O<+EDF8^D$8>@=;LR=-5.@=Y!N 'E.KD64.I+ =ZL >@2E4e=;GDF88`2547=Y2>~' 28K=B47=MB.@[s8K=;.MB2'E.U-D7'8.@+ =MBN >@>I=;'Q.,L >I2E47=;G?: D7'!),+EDFMI+M;-8K=:R.@+ =~L >@2E4e=;G8`2547=;><0s-D64$8K8K=;=XyD7cN >@=Z/U\&0.@+ =G1=;GV=;>B8K+ED7L?D7'C.@+ =X8K=B47=MB.@[s8K=;.U89M;-5'OR=MI+ =M@=YbD7'?L2/4ePE' 2GVDF-4j.KD7G1=:R.@+ =;'O.@+EDF8L >@2E47=;G8`2547=;>>@N '8rD7'C.KD7G1=XLR2547PQ' 2GD$-4fD7'?5-5>KD72N8L-5>U-5Gd=;.@=;>U8;\25)X:E)<=X-5>@=9>@=-YEP.@2Z8K.U-/.@=-/'YOL >@25=9.@+ =~GZ-D7'C.@+ =;2>@=;G2/0.@+EDF8,8`=MB.KD72'\+ =X8K.U-5.@=;Gd=;'Q.-5'YCL >@2E2502/0.@+EDF89.@+ =;2>@=;GM;-5'!R=YE=;>`De=YC0>I2GL >I=;D72N8^>@=8KNE47.U892'C47=-5>@'ED7' c8`=;.U8,)De.I+2' =;[8(DFYE=YO=;>I>@2>~9-5.U-/>U-5@-5':EU\y]=~L >@25=9D7.<03>@2G.I+ =9A>U8K.,L >KD7'MID7LE47=89032>,MB2G1LE47=;.@=;' =8@8T\=;.?YE=;' 2.@=J-8K=;.O25018K=;'E.@=;'MB=8;:9=-MI+2/0),+EDFMI+>@=;L >@=8K=;'E.U8O-8K=;.O2/0L >@2E47=;GZ8bD7'.@+ =YE2GZ-De'\<+ =;>@=VDF8-'-5.@N >U-4L-/>@.KDF-42>UYE=;>`De' cC25=;>~.@+ =V=B47=;G1=;'Q.B8X2/0YE=BA' =YQP!.@+ =!BG12>@=8KLR=MIDAMd.@+-5'>I=B4$-/.KD72'\ 8K=;'E.@=;'MB=VDF8dd(B 1Q?-5' 2.@+ =;>9D60,.@+ =18K=;.~>@=;L >I=8K=;'Q.I=Y!QP.@+ =^A>U8K.g8K=;'E.@=;'MB=^D$89-18KN 8`=;.,250j.@+-5.,>@=;L >I=8K=;'Q.I=YOEPZ.@+ =V8K=MB2'YC8K=;'Q.I=;'MB=\]=XYE=BA' =1-OV5;(B ZRK67;3K X250X-J8`=;.?250~L >@2E47=;GZ8dD7'.@2JR=-8K=;'E.@=;'MB=CDe'),+EDFMI+>@=;L >@=8`=;'Q.U8.@+ =~G12Q8K.98KL=MID6AMV8KN L=;>B8K=;.g2/0\fiff!ff ("+$. 0/ /fiff )#(ff ff' "*$ &%*,- "V3/ V5BTU;;VCs U 6UURFE? UQFQ IQ;;7E(B, <K T; @ I5 1C;6Bs; ;V!Q51VK T6 XO Q<; TX ; ;6K ;65Vo3FV9 X;W rK ;6X~ jQKX$g E EVV5W(B <$;3FZZ<FrU IXB E3 Od 5V3V@@BQ _!QXF<B ITQB FC3FV9 RVFO4e2c o7X 5VZ1324fi57698:<;*69==>@?BAC6EDE69F969GH69IJhji KJRLNQ)MEO9kPRlNQSm KTOVU)W9XTYHZ[W'\ ]fiZ_^<\ `*]fiaEZ[W9b<\ `Ecd]fefiW9\ gT`EZn Oo hjixwprn qsuOh[zo Othjivw{}|m ~Ef'l7}PNL#[H9wfi9<}fi#[fi*9}l3@dH~Tm _T9HmlVpqqss~lSTm79p7~TfmP#LNO iQ7PkJRU)Q)W'kXp}YHZ&m W9\ ]fiZ^\ `*]fiaEZ[W'b\ `Ecd]fefiW9\ gT`EZ9H$q}9H*fiE*Efi<HENHKTL7L9<*w[(H99HEfEE qd*9H*TVN9E9HT9d<$fi9fiC@fi"@RH)9H<9*H*9H*H*TdE9EE9fiHdHTHEfi[EfiH97dH@H[E$$*E9dfi9dH*EHH_*fi*H<E9}}H9w[HV"EH*99E*H<E[HTdEER97@f}<H99T99*9fi[TU7W9$X_YZ[W9fiH\ ]fi*Z_^H\ `*]fiEaE$Z[W9b9\ `Ecd]fejW'\ g`E@Z79EH9 *&9H$0fi*THN@@HE$wHE9990HTfiH_H*9RH*[EH9*fH99HfiTE09 9HET[@HH9*T[*)*T[Cw9fi*9EHS*9wT(fiH9[E9<<CE<7*9fiH*$*T$EdH9 H9*<H*rH9<<<EH9H9*E$*Efi9H[TH@@fiE$T@99uEHE9}39<fiH9H9$U7fiW9@XfiYHZ&W9\ ]fiZ)^\ `*]fi\*`9E*ZXT`EZfi w<9EH$fi*#fiEHE[Tfi<fiHH99*9*$T9N`EX`E9Zf\ *`*9fi<<*fiH H@E99HTEfif@H9dfiH_fi*C$E99<"*9EHfi['fiE[E$9fid9*fiEU7<W99X*YfZ[W9\ ]fiZ_^\ `*]fi\ `9 HZ_<X`EZ*@VHV@9*TEHf$ETE~T-9*H$fi<3[EENH'0fi9V9H99HH*H<9fi*EN0`EX`E9Zf99\ H*`d*VH9ffifi*[H|#~Tf[99[HVfi9H99*EEH99HTE9H0H9_fi*fid9~T<*(9**T9w[ *wE99Efi9EHE99H99H)U9WXH[Z'W\fi]_Z<^\*`fi]\`_ZXE`ZTU7W9XYHZ&W99\ ]fiZ^\ f`*]fi}\ `EZX`EZ9T99**H9H<*TfiE *<#[H9H*[fifiHE**}$wVH 9HTE@9fi9HEEr$HE99H[fiH9*9E}H9H9*fiE9fiE99H<**f_C[EH$fiH[TT9HH99H<f*EEfiE9Hfi9##H*H*$9H<[E'dfi099fi99H$*HE@dH9$E"EEHfiEfiEH*$*H[HHHE93fififffi" ! #%$&(')$+*-,.)/102,3 45076fi3 89453 8":;07."8%0<>= ('-?A@9BDC)BDEGFIHKJLBDMANOBDP BDQR $TS <>=UV9W MXFZY[)\K]_^ Fa`fiB"b"b)b1BDcedTf<>= < ? <gh2<i $kjH5[lmNDB)b"b"bnB9jH5[omN W1p Ydqfrfisut $#%? < ! =avwyx V9zV9{D|%W1V}VO~|%TV"pfi{VOV"OWn75V9WD sr)$ $ g ? l KH c pfi ]] zyz l N? < $R $TS >< =# gh2h U% y7 U W1ppfiWD|)z KH BO NDf> kFa?%$ =UV9W Fm["")BD[)9%B"b)b"b5BD[)"OjHK[)"ONFXjHK[)"mN YdjHK[)99NFjHK[)9)N Y[)"9H5N1db"b"bjHK[ " NFjHK[ " N Y[ HDb)b"b5HK[ " HKNnN-b"b)b N1dqf$ = &rfisut $#%? < ! =I-wy pz5W1{1OW|%{np"~T|%W5pzp%(VOV"OWn75V9WD sr!-^Fa`?%)! 'S c&!HK[ \ NF 8%."8%0:fi3 98%H1jHK[ \ N1NDf!'-?"('? W1V}{1pfiTV95p%V9{p%({nV `} W1WnVyV"|;{1zV" HK[fiND$ = &*-,."/10,3 45063 89453 8":%07.)8%0{1VA z|)p{5W1Gp{yOpzqWn{1p%-{nTVV"|%{1zTzp eV9{D|%W1pfi{ |%T29|%W5pzq+W1VWnV"|1V9{) pz2TV9{| H 5 N {1pTV9 Tn+2}5p%V"+q+W1V5V"fiV9zOV F9[)""BO[)9"B"bbbBD[)"5 TW1VWnV"|1V9{)VLV"|;{1zTz|)p{5W1znTV" zW1V5V9W jHK[ " N V9zW1VV"|;{1zTz|fip{5W1V9zV9{D|9V"W1T25V9W"WnVAV"|%{1zV"5VOV"OW17V9Wp% [ "znTV" zOV|{1pTV95p;fiV9{DzW1VqTpWnV"2}5|OV}znfizW1VWD|;{1V9W{1pfiTV95p%V9{|) |)A5VW1VV"|5WzTAV9{1V"pV9{D|;W1p{ pT5V5VOV"OW175V9WOpzTWD|)zAW1V+{1pTV9Wp%p W1|;WW1VWD|%{1fiV9W5VOV"OW175V9WDAp;pfiV9{D|%Wnp{D [%l)B"b)b"b1BO[""nl TpzpWAOpzqWO|)z +p{1V9p%V9{)zOVW1VV"|;{1zV9{yz}W1VApT5W5V"n-LV9zV9{D|)"|%W5pzp%W1V5V9W}p;VO~|%TV"9 WnVV"|%{nzV"5VOV"OW175V9WOp%}peV9{D|%W1p{O [%l"B)b"b"bnBD[)"nl A5WLV5V9WDLp% WnVOp{1{1V"5epzfizWD|%{1fiV9WVOV"OWn5V9WDmq|%zAV9zOV}TpzpfiWOpzTWD|)z V9zOV [)" W1VV"|fi5WzTAV9{1V"LpV9{O|%W1p{ pT5V5VOV"OW175V9WOpzTWD|)z |;z eVVOV"OWnV"kqW1VAV"|%{nzV"{1pfiTV95p;fiV9{uWnp5p%V zOV [ " HKN5p%V" Wn|uV"V9zOVp% V9zW1V"nW1|%z qWnV}W1V"|nV9{"TqLzTOW5VTqepW1V"29WV5p%V" W1W1V1|%V5V"fiV9zOVqW1VAV"|%{nzV"{1pfiTV95p;fiV9{"V9zOV V5p%V"z qW1VyV"|%{1zV"{1pTV95p%V9{""fifi9"" 1fiff ff!!" !#$ %& !'(!!" !#)' %*+ !,-%#!".%#! !#$ 0/ 1$!! !234 657895;:!<>=#?#@$'ABfiCDFEG+HIKJMLC!N JMLC+CODPEfiQ DPRST>U&VXWZYI[A\]FUfiLA^_DPEY>U`RTI[;YaDFLbE7Lc+U`TI[AY>LTICUeEL;fZC#gL;fhY>g+[;YY>gUCi[;jkc]PUCODPl`UGDPEKLSTm[n]PRLTaDFYigjoDpCmC#SqrJiDPU`EYm^sLTt]PUu[;T>EDPERr[AEr[;ccTiLuvwDPjx[;Y>U$cT>Lb\]FU`jyC#L;]PzU`Tu Q LTGcT>L\]PU`jxC{L;^|CODPl`Ux@}LT~]PUuC>C`Uu[bJ>g(C#U`Yk{bJ`[;E \UxJ>gLC#U`EDFEh 5fm[nCDFEUuJMY#DPLE L;^GY>gUr[3]FRbLT#DPY>gjK;DFE+JMUY>gU`T>UK[;TiUrff}LcU`TI[;Y>LbTIC`0Y>gU7ESj\U`TkL;^bDpC#YaDFE+JMYC#UM]PUuJMY>wC#U`Y~Y>Sc]PUuC`+[;E+"gU`E+JMUfiY>gUGESj$\+U`T~L;^_bDCaY#DPE+JMYecT>Lb\]FU`jC#LA]FzbU`TICtY>g+[;YmJ`[;E\+UJMLE+C#YiT>S+JMY>UuDPEUuJMYaDFLbErDC7 5 7mU`E+JMU7 5 ff|]FLbR 5 x~U`E+JMUx\(gU`LT>U`jkY>gUC>[;jkc]PU7CODPl`UR;DPzU`EDFEY>gU[n]PRLTaDFYigjoDpCeC#Sq7JiDFU`EY~^LTt]PUu[;T>EDPER+;DPE+JMUG]PLR 5 ADpCc+LA]FELj$Dp[n]!DPE@0bD^_jkU`j$\+U`TMC#gDPc7DPE7Y>gUfiC#U`YICXDPEK 5 [;E+"Y>gUfijkLC#YC#c+UuJiD%JRU`EU`TI[3]9DPlu[;YaDFLbE+CL;^YigUKC#U`YIC"L;^UMv+[;jkc]PUuCJ`[;E\LY>g.\UJMLjkcSYiUu}DPEhc+L;]PELj$Dp[n]fiY#DPjkUY>gU`E!u> `Inu;n; T>SE+CtDPE7c+L;]PELj$Dp[n]_YaDFj"U[C-fUM]]|~U`E+JMU\xgU`LT>U`jbDPYDpC[$C#c+U`UuSc]PUu[;T>EDPER7[n]PRLT#DPY>gj^LbT| DPEKLY>U~Y>g+[;YYigUe[;\+L;zU~Y>gU`LT>U`jJ`[;Ex[3]CaL\UCaYI[;Y>Uu$S+CDFER$Y>gUGLEw]DPEUfij$DpC#YI[;bU`ws\+LSE+"jkLUM]DPE.fmgDpJ>g.YigUk]PUu[;T>EU`T)DPE+JMT>U`jkU`EYI[n]]P.Sc%[AY>UuCk[7gc+LY>gUuCDCfmgU`EU`zU`T$DPYkJ`[;EELYkC#L;]PzU7[EU`fY>TI[3DFEDPERcT>L\]PU`jDPE(Y>gUkC>[AjkU$f[nK[CGYigUkY>Uu[JigU`TLUuCDUPfmgU`EU`zbU`TfiY>gU$]PUu[;T>EU`T)jx[;UuCG[j$DpC#YM[;UuDPY>Y#]PUuC#Y>LbEUubIgDpCGbDPUM]C$[7CO]DPRgY#]P jkLT>U$RU`EU`TI[3]T>UuC#S]PYY>g+[AEgU`LT>U`j\+UuJ`[AS+C#U+SE+U`TGY>gU$C>[;jkUJMLE+bDPY#DPLE+CGLA^Y>gDpC~Y>gU`LT>U`jYigUESj\U`TeLA^j$DpC#YM[;UuCL;^-Y>gUG]PUu[;T>EU`TDPE.Y>gUf|LbTIC#Y>wJ`[CaU$DpCcL;]PELbj$Dp[n]]F\+LSE+Uu^sLT$[;E}[;T>\DPY>TI[;TiJigL;DpJMU7L;^eYiTI[nDPEDPER(UMv6[Ajkc]PUuC`DUP-ELbYEUuJMUuCiC>[;T#D]PRbU`EU`TI[;Y>Uu}S+CODPER[vUu(cTiL\+[;\D]DFY#bDpC#Y>TaDF\SYaDFLbEgUxj$DpC#YM[;U`ws\+LbSE+[n]PRLTaDFYigjxC$J`[;E.\+UKJMLEzU`T>YiUu}Y>L \+[;YIJigWfiw]PUu[;TiEDFER.[n]PRLTaDFYigjxC{DFE[C#Y>TI[nDPRgY#^sLT>fm[;TIfm[nDPY>Ya]FUuCaY>LEUubI_70_;uP`.`X0%p7+`%nuUkEL;fJMLE+CODpU`Tk[;E[;cc]DpJ`[;Y#DPLE}L;^egU`LT>U`jyY>L7Y>gUxLbjx[nDPELA^C#j$\+L;]DpJ$DPEY>U`RTI[AY#DPLE[ C{f[bCLEU"DPEY>gUmcT>LRbTI[;jyDPYIJigUM]9]mU`Y$[n]Pu&M UfD]]~CagL3fgL;fY>gDpC$J`[;E(\UUMqrJiDPU`EY#]P}DFj"c]FU`j"U`EY>UuS+CODPER[ C#Y>TI[3DFRbgY#^sLT>fm[;TM[Acc]9DpJ`[;YaDFLbEL;^gU`LT>U`j^LTk[ C#S\+C#U`Y$L;^mCLjx[nDPEtLE+CODpU`TeYigUJi][bC>CmLA^XC#j$\+L;]DpJ{DPEY>U`RbTI[n]pCY>g+[;YGJ`[;EK\UC#L;]PzUu\Y>gU$C#YI[;E+[;TM"DFEY>U`RTM[;Y#DPLELcU`TI[;Y>LTMC`!U`YG \+U)Y>gUC#U`Y~L;^XLbjx[nDPE+Cmf~gLC#ULbc+U`TI[;YiLTIC[;T>U$UuC>JMT#DP\+Uu\7T>S]PUuCeC#S+Jig[bCtDPEQ DPRST>U$&%[AE+7fmgLCaUcT>L\]PU`jxCfiJ`[AEK\+U$UuC>JMT#DP\+Uu\K[;ESE+[;j$\DPRSLS+CfiJMLEY>UMvY^TiU`URTI[Ajkjx[;TC#S+J>gK[bCmC#gL;fmEDPE Q DPRST>Ufi*U`Y.\+Uk[;ECaU`EY>U`EY#Dp[n]-^sLT>jK6DUP[C#Y>T#DPER7L;^Y>U`T>j$DPE+[n]pC)[;E+z3[;T#Dp[;\]PUuC`L;^Y>gU$RTI[Ajkjx[;TL;^ Q DFRbST>Ufi*xU`T#DPz3[A\]FU)^sT>LjY>gUC#YI[;TiYCaj$\+LA]fi;;;0WCaU`EY>U`EY#Dp[n]!^sLT>jU`ELbY>UuCY>gUC#U`YL;^cT>L\]PU`jxC)U`T#DPz3[;\]PU^sT>LjS+CODPERKYigUkcT>LS+JMY#DPLE+CfiL;^ 7tLE+CODpU`T[gcLY>gUuCODpC$C#c+[JMU"L;^3`fi6b6(36(b+ ++fffi fffifi !" #$%!'&( fffi) # !+*" ,-fiff#$%!.0/214365$798:;/<14365$798>=$# !"?/21@365 7A8>B #$%!/21@365 7A8>DCE%FHGJIKLffNM9*O# !QPK #RTS) U ; VMW fffiPU#$EM4EEX#$!QP#RTSYZM4[]\_^#ff-PKNM4`Aa[,-PCFHGbI<c;FIKLHLdffeDc;FfI<DgfihLjfLk lmLn;Lofpqf rst\_u v;wyxElfz|{}u wA~Y~wOyau xb^;xbw9~yx;x\i^yxbu wA~@\_a ^;wya _xb[;wya _xb[@a_ xbwA[;a[x#[@xA_x `AyW[@xb9[O~wxwyxb;wyx [@xb^yx ffy;x[@xb^xb^@\~$a-wy[|ay;xu-w9~Y~w2"xff~^"ya[@;ayf~y;xm;$y? @W- b@ W @f\i(~^~;;wya ;w@\~yx 6- bffwya-v;@\_^;xYyf~`Aa Y;v;x [Oy;x,;a2~Y[@xba;wa _xb[b\[~E_x ~wy^\_^;u~%iu-a w@\_y;>6a w \_^Qxfw9[@Y^;xbx ~Qxb:xAf^\i\ia-^f[b{; 7K5367@595\[~^a-w9xbwyx ywyxbxQ;xbwyx~$#^;a;x [~wyx~fxA_x ;x,~w@\~_x [a-w#yxbwff\_^f~$[a2y;xu-w9~Y~w f~^fQy;xffwyaa O\[~fxA_x Qy;x,[9~wy[@fffaW(Qa wxba%-xbw H\~^;a;xf~-[,`\wyxb^H $ @9H;xb^ H $ $y9Hffvf[@x~;wya;vf`A@\_a ^ay;xu w9~Y~w YO;x[yw@\_^;ua[@,a[Ea ;9~$\_^;x wyx ~ -\_^;uy;xix ~$ x [eay;xf~w9[x#ywxbxOwa _xA6aw@\_u <\[O`b~$_x y;xff5$Nay;xywyxbxeM4a-m`Awya6N_~^h rp%rP9)y;xu w9~Y~w\[v;^f~ff\iu-v;a vf[b;y;xb^6a wOxb xbwy[@xb^yxb^f`Ax\``b~^fxEu xb^;xbw9~yx y;xu w9~Y~wy;xbwyx\[~Ev;^\ v;xEf~w9[@xywyxbx\`yQ \_xA;[yf~[xb^yxb^f`Ax <O\[Oywyxbx\[`b~$_x y;xf~wA[@x#a)yf~[@xb^yxb^f`Ax{>AAa~ffwyxbx\[~^a wAxbwyx [@v;;wyxbx[@vf`yf~eM4~P~$);x^;a;x [#~^fx u x [Oa~wyx\i^E;MWHP|y;xwyaa 2a\[<\_^ ;~^fM4`$Pt\N~^;a;x\[\_^ y;xb^\_9[<f~wyxb^~^fE\_9[O[]\_\_^;u[\_^}~wyx~$[@aff\_^]^v\i\i-xAi ~#`b~\[ta ;9~$\_^;x E;wyv;^\_^;uy;xO[@v;;ywyxbx [|wyaa yx ev;^fxbw2[@a YxO[@xA_x `Ayx e\_^yxbw^f~$^;a;x [\_^y;xf~w9[@xYwyxbx~^f~ \_^;uy;a[@x^;ax [e\_9[e_x ~ -x [bQ\i^f`Ax;xu wA~Y~w\[Ev;^;~ff\_u v;a vf[b~$;xu xb^;xbw9~$\_ ~@\_a ^f[eM6\_^CffPa2~^xA?~ixff`Aa wywyx [fa ^fyaffy;x-\_xA;[a|%~w@\_a vf[`b~f[<a);x#f~wA[@x#af~OxA?~ix 6)y;xbwx~wyx@Oa,`b~f[O ~^fb<a-w~ef~w9[@xywyxbx[@vf`yf~b\[#~%[a~`b~a f;xb^ [O-\_xA\[Ya-wyx[@fx `\m`fff~^b [b\_^Qyf~y;x[@xba[xb^yxb^f`Ax [xbw@\_%~ix6wya :y;xY-\_xA"aOy;xEa-wyYxbw\[~[@v;f[xbaOy;xY`Aa wywyx [@a ^f-\_^;u"[@xbxbw@\_%~_x6wyay;xY-\_xA"aOy;xff~yxbw ff"xY[y~$yf~ \[eYa wyx[fx `\m`Yf~^(be\_^"y\[`b~ [x \[E[@yw@\`A@_Ya wx[@fx `\m`Eyf~^b\ \[Ya wyx[@x `\m`f~^b~^f E b\_ xb^"@Oaa wEYa wyxfff~wA[@xYywyxbx [6a wEy;x[y~x,u-w9~Y~w Hy;x8ffb3]5A 9AM4;P<\[xAf^;x ~ [~[@v;;ywyxbxff\`"\[E~`b~a~$2y;xf~w9[@xffywyxbx [[vf`y"f~^;aa y;xbw`Aa Ya ^"`b~6a wYy;x [@xQywyxbx [E\[[@yw@\`A@_Ya wyx[@x `\m`\_^f`Axy;x`b~f[ay;xf~w9[xywyxbxa,~^xA?~Y_x`Aa wywx [@fa ^fya~$2a[y[]\__xu xb^;xbw9~$\_ ~@\_a ^f[Eayf~xA?~ix\_^"a v;wEfa ;x []\[,[f~ `AxCYty;x-\ixAa|y;x;ay;x#f~w9[@xywyxbx [a~[@xbOa;wya-ixb[`Aa-wywyx [@a ^f;[OyaEy;x#;ayf~O[@xbaxA?~Y_x [bx^;ax [y`Aw@\_fxy;x 6- b~$_u w@\_y;\``Aa-Y;v;yx [;x#;a~[xb2aHxA?~ix [`Aa Y;v;\i^;uQy;xY;a2y;xA\_wf~wA[@xffywyxbx [bffO;xY~$_u w@\_y;:\[eya~w9`a^y;x [x,f~w9[@x%fiH;f-b; O+;;;y;]_#]_fffi f (]_fi ;]_#!#"&%fi ;$"4ifi' )(+*-,.0/-132 ,4..,.5-,. 6.b87:9;7@ fi f =fi < 7>-. fi=? *-7@ fi f fi=< 7>-.ff7A@ fi . 9=BC5 ? .7,.. <])B0* ? 7 9-. fi *f ?)D0< , fi BE7>-.F, fiGfi 7IH$_4? * -_-(J9e fi G.ff9 E)79<])K ? _-(iL7>-.ffMON-P < 9 fi ?)D< 7>-. 9=,.9 ?Q? 5-,. 6.b87O_R9 ?Q? 7>-.ff5 9;,9A.ff7,.. iS. T97 ?)D 7>-.9=BC.ff5 fi ])7@ fi f 9 U7>-.A),F5 9=,.bG79 ? ,4.I9+ _4? * G.I:VP fi f]WG.,IH <Xfi ,Y. T9=BZ5 ? .+H7> 9=7Y7>-.5-, fi (+,:9=B+F(\[.b]74>-. <Xfi=?Q?)fi @O_-(^7A@ fi . 9=BC5 ? . V ' )(+*-,.0/A> fi #@ _74>-. fi=? *-@7 fi f fi=< 74>-.ff7`@ fi 5-, fi K ? .B ia7>-. <bfi ,B fi=< 7,.. cVV ]_#0 Cegi f:h ]_# 0 Cegi f:j :fi ; Cegi flkfi f#m 4n+Vfi ;0]_# Cegi f:hfi ; 0(4i Cegi flp ]_(]_# Zebi f j ]_ :fi ;#B A.7' , fi Bq7>-. A.07A@ fi . T9;BC5 ? . H 7>-.!5-, fi-.IG*-,.^rs-tIuvws-x yAv{z+x |yAx |I}=v{tI|=v3(+b. -.,:9=7. 74>-.~5-, fi K ? .% ]_#fi ;4i <bfi , fi 5.,:9=7 fi ,-HI9 7>-. 4i-( ? .7 fi ,A.A7 % GH % ]_ 8Hfi ; #8H <bfi , fi 5.,:9=7 fi 9, F/-H TH 9 a!,. A5 .7 \[. ?\D V2_>-.~MON- fi=< 7>-ff. ]_-( ? .7 fi 6.97 9;,.ff7>-.. 9=BC5 ? . &7>-.B A. ? [+. cV2>-.5 9=9, 6.7,.. fi=< 7>-.7A@ fi 5-, fi K ? .B ]_#& 9 fffi ;e4i. A> fi @ _ ' )(+*-,.^-V2>-.JMON-P fi;< 74>-.7A@ fi 5 9=9, A.L7,.. !9=,.CBL9=,+.IOO@ )7><bfi , fi 5.,:9=7 fi ,~9;,7@, W9 -( ? . cV2_>-. ). ? fi=< 7>-.MON-PH =X AbI Hfi ,,. 65 fi ; 7 fi 7>-.C;* W*-.JMON- fi=<7>-.ff7A@ fi . T9=BZ5 ? . cV|=t|=vX}x |fi BC5-*-74. C7>-.aMON- fi;< B fi ,.7> 9 . 9=BC5 ? . _,.BCb. G7:9 ?Q?)D K ,.5.I9=7.I ?)D- _-(74>-.~MON-P fi=< 74>-.ff5 9=9, A.ff7,4.. fi=< 7>-.*-,,b. G7FMRN- fi ,_7>-. 9, 67F5-, fi K ? .B^F9 L7>-. -. -75-, fi K ? .BVR.9=,4. fi @,.I9 7 fi A7:9=7.ff9 U5-, fi [+.74>-. <bfi=?Q?\fi @O_-(L7>-. fi ,.BSV0Gn Ow8Sl~6I b bw+l:!]I W8Q!lA+w8ab+ !l=CFA]+CQ ] + bclA +A+00+L I_4aw8S84+w8cl^{+ aA0nA8QlI4+g lw8Lw8!IQ IwIwlA+LY)ff:+c++0+waWaw800Ib=A++0+w^ G=ff`GQI+QII_wqw8+&::A IbQl_+WTl8Q&`GQI!wwG++0+WTw8^w8IA)-:Q::WGC++:ww8l+_ WL]. A> fi @7> 9=70rs-tIu4vWs-x yAvz+x |yAx |I}=v{tI|=v 9 ? .I9=, i-(R9 ? ( fi @, )7>-B <Xfi ,ff _K A> fi O@ _-(7> 9=77>-.fi - )@7 fi f fi;< 2>-. fi ,.B > fi=? V3R.ff9 ? ,.I9+ 9 A*-BC.IL7>-. 9, A7fi - )@7 fi fi=< 2_>-. fi ,.B;H 9;BC. ?)D 7>-.!. A74b.. fi=<fi BC5 ? .7.!5-, fi K ? .B fi=? [+.9, <_SVfi#T--L --T-RTR+++ -a +---_a_3= _:=3=#ff`=fi=+fi__3==! "3=$3 ffA3=fi3 fffi`=%&'(*),+-".0/21"34(*),5'67),5"+980:"/<;>="?A@CBD80:"/E8GF@</H'ICJLKM,/ONP3 N#),5EQSRTQ'UPVQWIX5'6EY @N'QZRTN#),5[Q\VQ]^:"/^;>="?_@XB`IENG/P8@CBaK".0@7bMc/PJ9Nd) ND-"5) e7-"/b\/ IC-'NG/fg)hNi-"5'ICJSb),+-"@-'NPjDkEND6/ON .G),b\/O6L/OIC.&Ml),/P.Om80:"/n;>="o IC5pb\/ @JqK"-"80/O6r)c5s8G),JL/SMt),5"/OIC.u)c5s80:"/S5-"J<b\/P.2@CB/HvIXJLKM,/ON2IC5'6p8:"/ZNw)cxP/ONy@CB80:"/K'IC.N&/z80.0/P/ONPj{=C),5 /pK'IC.N#),5"+pB|@.L-"5'ICJSb),+-"@-'N @58/H}8SB|.0/P/z+.ICJLJnIC.N IC5{b'/W6@5"/r)c5{8G),JL/~SG U w IX.GM,/Pm'OC mC80:"/E;>="?@CBIyNG/P8@CBK".0@bM,/PJnN IC59b'/B@-"5'6Z),5nK\@CM,5"@7JS) IM8G),JL/j4]:-'N80:"/2NG/ @75'6 @5'67),8G),@5@CBi]^:"/P@.0/PJy) NN0IC8G) N#'/O6INF/MlM$j]^:"/8:)c.6 @5'67),8G),@5L@CB]:"/P@.0/PJEIMhN&@E:"@CM 6"N4N#),5 /JL/PJSb\/P.NG:),KL),5nN&/Mc/ 8$NG/P8N @7.0.0/ONGK\@5'6"N80@K'IX.N#),5"+F:) :) NnIX5 ~Sw U K".0@bM,/PJWj(*),5'IMtMcmNw)c5 /p80:"/z5-"JSb'/P.9@CB[NG/P580/P58G) IMB|@.0JnNy@CBM,/P5"+80: ) NEIC8EJL@N&8[9B|@.ENG@JL/ @5'NG8IC58ECmvMc@7+uv)hN@XB80:"/S@.6/P.E@XB ~Sw mN0IX8G) N#B7),5"+z80:"/ING8 @75'67)c8&)c@75@XBi]:"/P@7.0/PJ"j4/P5 /E),8B@XMlM,@CFN8:'IC8:'INIqNGK'/P/O6-"KM,/OIC.05),5"+WIM,+@.&)c8:"J),5Wjfi"'7P"OOCC"""0",d"p''P'q}X`hO"z"O " G$ PG OX$OCC'p0"ra"0 v G$7 PGC "7 CE|G" CC0y,L,PLP0O>,9"07Ct,O\^9C'{0O&0Oc0"GS'Ct ,0PXG,An,"p""0O0#,'0'GXc7O_C0P"PCOs'#,"0"WCLnCZc*,"0p'"9,'7n'PX0, ,'7c"p0"G,*,"<"'X'WGq<7tP0P&hXG,C'W#,Ltt CG,>'PX0P C,7c">"0,P'# G9 20PLC,"s0",0P72#,AC'_#,Lt |,"0"p0OG,WqS'0#,, $p0 }\PG,LPO L0G"qOW'C"Z&C,"G,'090"S"0,Pn2X02"0C7ZXn"0P0'0O "PODSt,00Xl,O9"'PG,LPC&c7 ,0z7tP0P0,c"LGPP,L,PLP0Os0"L0O "Py'#,"GP2 ^G, 00C40,OP $ 0"LO "P 00Cfffi0,O90"Pn&c7OP0z,OC"Offp"nC|P"PX0O{vCqcOr'#,"0"COC7 P0"&c"0'C 7L""0OS0"LGLG\ P"PtOCG,2GPq[",PnP!4 G'CL, '# GO9"0,PC'0"L\OGE'&E'PX0E0WC",p'CE"0,PW#0 9n,O {'PC0SS,L'pC"t C,0nX'X0S2C""0O0#,!" P0"p0O "P#|P"PC0OGC,"G,L"07cP 0't, 7'# G9Lpt GW2'CCLPPGP\PC0Pff"P0"'CXLP0PP"70O0", C&c7%w$ G"'""0O0wc7'i& X( 9,hC"t,O"0"OG2'cP"0"E"07C,OC0'0"2G, #$ GPO G,'Or,W0"E"P,'G G,^""0,PGC,P ,n0"'"O# &' C00G"LOZ02PL,CS'&#| P0P0"z}"0O#,00Pz00C') &"'""0O0#,'qsC",,"s0"z'PC0 Cc 0",OC0"P* EG, #$ GPyC0L","OpLGEG\ l 7P"PtOCG, 0"L0,c"vXL,OPC'#, <\00"E,OC0"O"07cP GC,PEC'r0"2X0P"07cP GC,PPL,Cz0"<CL,&c"0q0PGE"2""0O0#,p00P\z0"<C"LP,"2"0 4 4"P0P "\0",OC0"O9"07cP GC,P O" '# G0P ,0z0"0,,"L'CL,OP P E"0"P70P++"+X"l C,S0Lh'CXLP0PG POzncp t$>L0,"O>0"nG"G0P p,0PC&c7"0,Pny0'C '# G0O> G"nu "0" E' P C'WG7LE0G,"7LP0G |" G,' * #W'C0& C'O z"0,P D0"|Ctc ," 70W "PEOcP ^7G, 0Oz" |0S,C'S, tdc "XC'q,'P'P'PG, |0t"P "X,.1Z[ff324.- /10%6597 8 +:z<;->=@?BACEDGF 8 wcH-I <-IE+:z<;KJL-GMGF 8 #,N-9I <-I3+:z<;KJL-4F 8 #,O-I -I3+:gP; QRS | 0PO W0,c"9'CL,'"<G"G0PT^0OG0OrL&PU*ffZ0OG"07cP9P"0OG"0,Pn4P0<h&gG, 0O'#,"z0"S0CLE,,"W7h&0G,""G,LPG,"Or'3| 00OG"0,PV"0OW7 00 &cW&C,Ozz0"E,OC"P4 ,GC,"G,v G,znC "O0'C40"2O "P ESL70 '&P0C&c7W nD LO&"G,"W P " z0'X "G,"0"E"07cP9+pC00O Oz<*7X ,,P""0O0wc7'O ,0"7""c0P#,K$ G| OE"L,OC0"PS0p#,Lt 0"q0OG,y ,0PC&c7p,0"gCLY p0'CE0"90O "POP%*," gG" u0"L\P PC9 0"90OGE",Pn 00 G,T&C,O |0 "L0OGSGPPC7OCP,,"_0G 9,0OX,'G90"p"S'Pz <0c,"'CL,OP"P007'XP"0"2GC'"XnP7 CG,9,0P0h^z'g# 0"ELOCD",OCc"\X ^]<_1ffP0O ,90O,"Z7P\ C'GEO" ,0,W 0,,"SvXL,OP40c,"L'CL,, $ 4C,O^ G02nXn&nt0,,"L"'PG,P OP\O 0OG'7'7c"0L7"E'PX0C"t CG,`a3b>cWd\eBf6gih3d@jWkmlnf6oKp q^rmrms#tuomvw*omd@kKs#x>kmld/ermsEwny6d)z+o{f6pomkmd|q^jWs^x}kmldKd@pBk~f6rmd(jsEzOq|f6pWwd@g#q#vomdUq3'zOq^pBUermsEwny6d)z+of6psEvrKj*s3zq|f6pj*s4p*s3kKl q@3d+gy6sEomd@jxs3rmzoms^y6v*k~f6s3p*o@}q^p*j`w 9o~f6p*g@dOkmld+kmd|q^gild@r{f6o/kmsOwd\ rmomkKkmriq|f6p*d@jv*o~f6pl q^p*jnomdy6d)g@kmd@jd@*q#z+eny6d)o@'f6k.f6oOkmd@jBf6s3v*o+kmsWjsWkmlnf6o(s3pqyq^rmEdHjsEzOq|f6pffbYld+y6d#q#rmpnf6p*Yed@r~xsErmzOq^p*g@d4s#xKkml*domBomkmd@zf6opsEk{omd@p*o~f6k~f63d(kms\kmlBf6o{gils#f6g@d/s^x>kml*dKermsEwny6d@zjnf6omkmr~f6w*vk~f6sEpb*fi/`/N ffffff>%>ff>100Average accuracy8060Accuracy4020005101520Number training examples25309ff#R4'*#<%3#ffmffY>34/O`'#YB#^ffH>E#ff@3>E<B@ffff3#BH<WL^WY*LY4>'ff\i/'ff'BOGffG9ff`ff3#ffmffGBE#/i43#GKR@*ffB>3}ffGBE#E^>#<Bff*\H@ffRiK%ff3#ffffGBE#ff3/`ffH#Hff3BW#W|*E^u@>ff33<E@B+#>^@>ff3H@@ffffGB##EBG#B#B<`>3^*ffu#3^B>3%O@*E^4i%#<W@*3|'(3ff><BW^W*^<Kff3#mff>BE#ffE+>1mff#B>ff4@>B*<*#<1EYB+ff#'} m'R'*ff@{ffB} fi>B#W+W`ffH#ff#@Y|u#>4@E#*#H#B#*|BRE<Yffff*3#ffE#3#B# *m*##*B<OB^U#H`ff R RGBOL*Um*##HB`Eff+>ff><*fiR`WGRK{i+< 6/>ff9#*##*H#Y3<i*ff^(# !@'3>*34>Y*B{>#3O*O#H3<W><@ff>B\i"{ff<$ #KB3ffG#Om*#^*(#U|*BG>`#3O *#@Y><@ff>B&%>@#WE@ff('*),+.-ff/0/0/0-1)324+#`#B#*@B<H@3#5)'OB#6)37K4#{E$ 8:9<;Hm*#^ff<ff>= @?:AB?:CDEDGF HBIKJ1ALNMOA.P3CRQSDTI.L#41 U*3(9RY>BE^ffK= ff`Hm*#^UO>3|uL^H HKff<^>\*^#N>1(><BGB><BRHff#B+m*##4 *W V4ff||YX,Z[\PE$ +<6/GWH%1 >Y<(1#ffE<*3Y>R^< <'>*B>|Y#1 U*3YW<##ff#><@ffW3<`3ffY<B#3#*3E<m#ff #@>><@ff%/#>U3<ff^]9ffE <*3ffYGR#<)ffU>Bff*<u<ff>ff "{ff<_#.B3ffi("{ff<5#KB3ff{#Y1 U*3HO<uffGBE#ff4ffa`bRY c.d.e$fR{BEff`@<B>B>ffHff<ff^Y>R)| O#>W@ff{@%ff^Y>R)| O#1<}V4@UK XffZ[\<U<1 g>*Yff^HffBBEff@ff.<*3ff1>R#<@B*hn@B@<*3ff1>R#<@O i1O<^U<B<#|>#u<ff`>Bjlk,mfinoqprBs3oqt,t,uwvyxo.z.oq{qoq|}oq~ff0_ffK<<T611NO.3R3GB.03}1ff130q5B}q^3:,.}q,G}q1 ,1.31.}05qBff06 w:,.}q}lff0qwEw:q106.qllq,.06}.:,.}q}^.affGq&,.q},$.$q},1,W&ffK:}},.w}qE.qN ,Eff:,.}q}w0 BRwq310q}q3>Y0q31&Ew3EffG0>,1 }.0&B.,.q}@BK30q}.K3K}qE.qKffN}&}.w3Eff,1 R}.GG0N^q3} q^ff0>w1GBY0K}3q},3K.B:0R$q31&E}3q},3R},R00$,}q},w.G,wq1&llq,S}q30wENq@3lT0,1 }.0:@.K30q .}q&:,.}q},S5}q 6}.&}q6RffYE}.E, >0q.0 ffK.},}10,ff,ff,}OKE$q q0>ffG0l&q$0,.}q3N}^},1B03$q3>}qqqRffEw.}03,ff0}.W, 1q,: ffG.,G$}q 3^.0..01.},0R,q363K.}13q*33}K}@,} ..051.}.>B3K.&.0..0E3ff0 $E}3EffG0,1 R}.0NB :,.}q}wK3q6 qq0R}6@3q3KffG003y>}q>K30qB,,ffKY1}K.0E aK.0>.@1}R}035 WqRffK.}$q3}q51.0q N}3q},3R$q&,.q},0 }q6..BK30q.}q>}.@}3q},36}q10 R0alff0q,31} $0}q>:Ea1.0q .@q R<}.}E,w}q5B,.W:3G&1q,1:&G$q3q33w0NN, .0l.0 K._$q3}}q:,.}q},,ff,}16,B^}q10Rl.ff0q,3 ,ff,} < .>}q:,.}q} N}q6lff0q^}q6:,.}q, 6}q}Bq(}q ,00qK.} .}.K.}3,a}a,B5q Rff.ff0q,ff,ff 1} W YENqq},qq.NT0@.E,N}q^B1}:K.0q}B3}>.>q}E1q}1q . }.E,ff&Bq.w,6}YTff06Ew3EffG0,1 R}.0. }E,}q$3K.}0Rq}}q30a (B1}K.0W&}^,BK 3$N03R>K.: .}0}K}W}*3.}.,.q}$llq,Sff0$E}Ew3EffG0,1B R.0w0},,1w}.>31.},Bq31.@1.,&1.} ff,ff} < *3,333w}< ,ff,} SR,ff33}.W,}q&1}B:K.0$q}3}R01&}q$.ff0q,w.:,.}q,$$q}q0qwE.}}3wK.}330(}q10 .ff0q,60q0Eff^K}.(q6q>lff0q,._B}q3,.q},*}3q,3R,>w0}(RK}q ff0}.E,N31.}0Rq}}q3}&qa}531.}6,q31BR.0l.0K.}a}3.3,GE6},qqR, _3}3E1,w0}0Kwq},q}1q>}.w}.E,}q&1:K.0&q}3WN31. 3EffG0,1 R}.0 ff0}.6}.E,}q B3K.50Rqa1}>K}.61Kff00>qq},qqR&1}3 ^3ffG61}6K.0^qBq},qq.,G1}K.0 w0}qq}qE.}10K3,a,}q},NE.03Y}q3^3 ,} 1}}1.0q0&:} R0Eff$K.^w0}qq6. RK0q>,.Kq.$a00q}@W&}q,BK qw,}q},}Bw}6>.}$31,}010K.3}}q10fflff0q,3.q00q1R W w$q3}(E }qalff0q.:,.}q} 0qK}>1:}.q0Bq}q61 G}q:,.}q},66}qqY6ff>qff6}q10^BRfflff0q,^qqa}q.qGE3.0}q1}}q3ffGwGG3Rff0}q10Rff.ff0q,^ff0q>q:,.}q} R}q63a.}q .qGE3.0}q61} 3..0..0 q}03 E.0,01}.qGE3.0&}>q}B3 .03fffffiRSTff Bff ,3 B6 "! .3Ka#a6,.:R3G%R$ ffW6^>::BWB,1'&5y#ff,^%$(31:RS)$>%$R%*$ 3O1,:60/ #+#, 03,1EBEG61KNq.3G.- E%1# :1Rff<@<<$BT:<#q _S6K'3 '&54.687:9%;#<>=?9;A@B C?D<FE>;AB G:C?9#a%$R^6ff :R3G& - <%$12,%R$ H 1:W6ffK%26& ,I8JfiKfiLNMPORQAS(TAUM QATAS(VWXORQAYZ8ORQ[#\#V#V]3^A\`_3V#TAQAaAba3cdfefig h?iAjlkPe:im.n8o:p%q#r>s?pqAtu v?wrFx>qAu y:v?pz|{ dlk~}.#A#'ff)"fi?FR?`Age.}? e:g k jgi' z|{)>#A?:ffA?AF%Ffi?A?l#fi?FR?(#fiA>fi?FA>?f FH#3FR`>{ jHg k}fidlk~}fi?FA>?{ jm.n?o:pq#r>s?pqRt#u v?wrFx>qAu y:v?pAA(0#8`#5')#?ff#)0 >80A ?A.A'' ffA ff3#'>fi?`F#A 2:>?0#'fi## #A 5'ff)5 ? ###F#3)?#?fffi? fi )5 :.' )? #))3 'ffA >. >fi? # '#A#'ff)AfiA) >305>A#?ff? f HAA)fiA(A# 5A>3)fi #A (~A0#F?'ff#2>? ffA3>.?ffA #>? ffA Afi?`##F#3)?# ff ?ffA Afi?Afi.5?>A5fi`3:#.A 2:8:ffAfi8fi#' . A.88A?A5 fiA?A5A#'ff)?3 AfiA)A ?">))A.#F#3)#0F#A ?`A ?ffA ?A#?'ff A) 8fi#'A) #3ff`fffi? fi ffff) Afi#A) 5'ff?A' fi AA#'ff) fi)?3>.A5'# 0>) :ff'fi#52?'ff` fi>3fiA#') HAFfi?AfiA# fi#A 2:88Afi:F) ?A' )? H A) >3A) #3fi fi5##F#)?(`>fi#A)3>fi0.55>fi? #)?#0>fi#A .3A ffA AFfi?AfiA#.A 2: 88Afi ( ?A' fi #3)?#>fiA .AA A#'ff) A)fiAAA F) ffA A# :AXA (PA#F?'ff ff H )fi:A.Ffi?Afi#'3:#fi#AfiA ':(8:ffAfi #)?>fiA 3?5fiA ff' :(>?05A0 :> )A fi#A?ffA53 #3A?3?# 5'ff)5 ? ?~Afi#A) ff. ?~3' fi Afi#A)##ff. ff '3> #A)0# ?>fi?`# ff '35)'fi'5'A A#'ff)">) ff fi )5 (#AA)# 0 ff3A).? )A')"?ff#) ff A'2#Afi >3?Afffi? ) >fi#A)fiA#'ff)">?ff#) 0fi )5 'Afffi'A.?3?# ') ?(Afi#A)3fi)?3> )3>fiA##F#3)?# ?:fffi'fi fi'Afffi? ))#)32 'ff . >Afi3?.A) #F?'ff3?5) )?:>?ffA AA'3fi' AfiA) 'fi fffi? 'ffA 5)A):fffi#) :? ffA5'ff)5 ? ?AfiA)H##ff#lff #HA?8fi )5? 3fififf fififififi!fi"#%$&('*)+-,/.0 132.4+653)+7983&&(:<;>=?@A&(B*C?83&8ED!&FGA$&@!HI=;>J&KL&MNF4OQP*R>&8SD!TQ;>KG@!&OU&KVDWFRXR>YLZ?*;[R6\LD!$&O]FG@AT^DWFZ*R>&_`#aTb8c;>OQP*R[;XdIYQD!$&EP@!&83&K*DWFD3;>TKaef&gF8A83?OQ&gD!$F4D`D!$&gP@!T=@WFOh;i8j=;>J&KUD!$&gK*?O(Zk&@TddI&FD!?@!&8mlnejFK\oDA$&QKV?ObZk&@(T4dE\;683Dp;qKGDrds&FDA?@!&QJFRq?&8Uteufg$*;6GA$vD!T=&D!$&@(\*&DA&@!Ob;>K&wD!$&P@!TZ*R>&Ox8c;>y&bTd`T?@Eds@WF4OQ&fT@!z _{jTD!$wT4d|D!$&83&(GF4K}Z&b&83Dp;qOFD!&\dI@!TO~&MNFOUP*Rq&8^F4DgD!$&(GT*83DTd|FmR[;>D!D3R>&(F\\;>D3;>TKFRGTOUP*Rq&M*;>D3YwTdDA$&SRq&F4@!K*;>K=FR>=T@3;>D!$O}_ua '*)+-, .0 132.+59)+3bWW 79 W C l tk*&D !/u!Wcu*( 7 W l tNC*46j ^|SH-j {<|D!TQTZDWF;qKL73 W CWf;>D!$D!$&^K&MD%;qDA&@WFD3;>TK&D W>>>3PP*R>Y}D!$&^TP&@WFDAT@g83&?&KG&bD!TbD!$&^P@ATZ*R>&Omo[^aa aobak [ 79kCW%&GT=K*;>y&bD!$&^D!&@!Ob;>KFDp;qK=PT4;qK*DW8jdIT@gOFG@!THITPk&@WFD!T@W8Pk&@WFD!T@;>K\*&M] [mg[^aa l <VJFRq?&bTdudI&FD!?@!& Td|[&D w wZk&D!$&8pO]FR[R>&83Dg;>K*D!&=&@8_/D_- e e e eigL3 ;i8E&OQPD3Y3 q p ! >>iW !P q zO]FG@!TQDFZ*R>&'*)+-,/.0 132.+53)+:<;>=?@!&(B | &@p;iFR`F@8c;>K= R>=T@3;>D!$O}_#%$&b;6\*&F}Zk&$*;>K\D!$&]'*)+c, .0 132.+653)+j;68(89;qOUP*Rq&_-DbGTR[R>&GDW8QF}8p?*GA;>&KVDbK*?O(Zk&@TdgD!@F;>K*;qK=P@!TZ*R>&O]8mFK\D!$&;>@83TR>?D3;>TK8?8c;>K= jSH-| {a|_u#aTw&FGA$LD!@WF;>K*;>K=wP@!TZ*R>&Oek;>DFPP*R[;>&8S;>DW8m83TR>?D3;>TKo83&?&KG&QTZDWF;>K*;>K=}DA$&r8p&?&KG&QT4d|;>KVDA&@!OQ&\;6FD!&Q83DWF4D!&8kmD!$@!T?=$L_;>KG&;>D;68bz*KTfgKoD!$F4DD!$&w83TR>?D3;>TK8bDAT}P@!TZ*R>&O]8bF@A&]=&K&@FD!&\?8c;>K=DA$&]O]FG@!T}P@!TZ*Rq&O83TR>J&@uf;>D!$QFSz*KTfEK^dI&FD!?@!&jT@W\*&@3;>K=eD!$&g83T4Rq?Dp;qTKr8p&?&KG&gOb?83DuZ&gFSGTOQPkTV8c;>D3;>TKQTd8p&J&@WFRO]FG@ATHITP&@FD!T@W8_`-DSZ@!&Fz8D!$*;68S83TR>?D3;>TK;>KVD!TU;>DW8gGTK8pD3;>D!?&KVDSO]FG@ATV8 9 dIT@g&FGA$wdI&FD!?@A&Z*YQ@!&GT=K*;>y;>K=QD!$&S&F@3R[;>&83D;>K*D!&@!OQ&\;6FDA&83DWFD!&8;>Kwfg$*;6G!$wD!$&E@W83D ds&F4D!?@!&8`TZDWF;>KwD!$&;>@=TVFRJFRq?&8_]#$&bO]FG@ATV8^F@!&Q83D!T@!&\;qKD!$&QFPP@!TP@3;6FDA&]G&R[Ri8mTdD!$&bO]FG@!THIDWFZ*R>&b?K*Rq&8A8DA$&rG&R[R68$FJ&FR>@!&F\*YZ&&KR[R>&\ZVYP@!&J;qT?8cR>YRq&F4@!K&\O]FG@!THITP&@WFDAT@W8_#%$&^@!&83?*R>DT4dnD!$*;688p&GD3;>TKGFKwKTfZk&(83DFD!&\FK\wP@!TJ&\ _fiN|a|NoNofffifi "!$#"&%('*)*+-,/.10 24365 798;:&<>=?8@:A0BCD)FE"GH'*I'*IKJALNMOGHPRQSTLNUV'UWG-STLXGHEYIZ'$IJAGHS[JH\4]EX'$)*+KP_^`\HEbadce'*Igfhc9ij' ^kHl>GHS$SnmH\HPffGH'*IUo'*IhadcpGHE"LoU`LqEX'*GHS$STrWmLYsX\HPRQt\U`GKuNSvL>%('$)*+-E"LNUwQLYs)()F\Wxzy{KlRfhc|';U)*+KLU`Lq)ff\"^}GHS*S(Q E6\u`STLqP~U\SvLqEUuGU`LYm\HIIZ\HIE"LXmHKIZmHGHIZ)DPffGHsE"\]6)FGKuNSTL`U%('*)*+)*+L^LXGH)FE6Lff\HE"mHLEY'*IKJffxp^\EomH\HPffGH'*IUo'*Iga c yjGHImKl>)*+KLWIPWuLEo\"^WmH'U`)F'$IZsq)^`LXGH)FKE"LgqGHSTL`U7';UWu\HImLYm-uNrAG>Qt\HSTrI\HPff'*GHS^Is)F'$\HI?\"^DPffGM';]PDPQE"\Ku`STLqPU'Lz5(lX};H8;K`:`Y8T;6HzH"AAx|HY`Yqq>;:A;:XqH6@:HY`;YqbFH ;HRb"`Hgeff$H"!*H F#" q:;:";WV365Z6CHff;:o";WRD8;Wo6@:48@zHZ`YH$(:qgKff(:ffgK;ff8;`:Ho}gHX;:e-hHX}Y8;A$H:qK"R`:Xfhc$A}"`o8;:Hff$8;F";WH8;`1"8;H`YN"!$#"Hq9z"Hq(H`Y4H(""q`:X(`XY(Z`6b``:h4:Kff"Y* 0 qq[ $ 0 K`qX`HXt4: * 0 ;q t* 0 NKqq"`>hHXg" R;:XWffHh;:$-"`O8v8;qXHWZK8;FHAFqHY`";:Hqq"Y59XqXAW:H`Y4HgHhHXHFH`XH:-FqqoffHH$:`Z`:`:K>H:o8;qqqq`:z`:XH@6qzffhHXHFX8;oHZ`"H:-wHXq 6 D&Zo$`:"$`8jgXHq"H:;:hHX;:egY`DH8;`"8;H`qogHXY8;H D"q`"YH`j8@`"8;H`$bH"qH:Ao::q::hHXffY48@H:g:h""q`:Xo4"8;";H:AR- " Hbff"XRXbH8$gZq::qb;:XD8T8&hXKq"`:;:}h8;";H:bWX;:@:H8;`h>WXHqX"8;XYXqKhw[ "!O4$#"6Wff$H"!$H F#" v8T8Zo8;ff`X"8;";H:b(8T8q"zH8;`hb6@:g8;q:qphHX?Y8;H`:XAH8;`"8;H`WHWKg8;q:;:8;HH";XH:$"`:K@-X;:@:A8@H;:X>hHXjXYXqWgow[H "!O*#"8;hYhoXHq"ZH:;:hgHXK;:eY`DgHXY8;HbAH:8;};:$g8;q:qhHXY48@ff;HffOv8X"8;";H:;H`:KqH`W$g`:"W-:qXqegHXHFHZ`Yff;:YH`(hHX>Y8;H:`H`b``:g8;q:q b;:X`&5A&:g7DX48@:``zW"D5 7hHX`::K""`ffbq"ghHXKX8?gff$;:;:8;q:q?hHXY8;HoR`:Xffff:KffZ`T``:KhXhX8;qzH&H8;`d"8;H`Yz4`:Zo8;q:qFH1W;H`:Y`hHXgY8;>$z`FZH:q<H $R$zffXqX";HoHq6"HX>j8@q4:`q`:XH `H`H ,.0 2 3"578;:<z=?8;:B0 CbX8@qR$"A@`:`:"8;q:T8T;6Hff$gqH""`H ;H}4WA::;:";WDw[H "!O*#"$WZH:qKff3",AO5ZCY`$>ff8;`:Hb8;H:Hq"W"48@@:};:WYO@:;:}8@H(:?W`Y4W``Yff-WX:qeZHH@:X},.ff3 q2 8;:h0B CY(A:";WAow[H "!O*#"b48@:Hff$8R@:eO8v8zhqH;qpYW``Y``:X@Vff}`q8;q:@:8;HH";/FHadc;:-fhc(RjH(``"&Z bw[H "!O*#"HXt8;;Y9"`"$8tqXWKT8v;"HKH8;`W"HX"X$O8v8;&;bDXHq(&ZH`:"$8T8@XtZH:`:KO8:ff`b"8;";H:;:KZ8;K:ff$8D;`hHX}X8;HH`"$8D(Y;:}qH;qg4W-qH`W"8;";H:3w$q-A&zwjR&bj?C`:-ZV`@q;:-h6@:48@ff8@`d8;H`q H;;:8;}hHXY8;H}"$q$gq;`W`:AK8;`";:-q`A"qYFHAfififf!#"%$!&'$!(fi)"#*$!"*$,+-$.0/!)12+-345!$67.8$!9:.0;=<?>)9@+-3fi%>=!#"$!&'$(fi)"A*%$!">fi!+9$!*4fi))9@(")/B0$!3fi+C.0;.0)"9)DEGFH>:BI+H)9fi+-3")+J*>fi*J*>)")BK+?7.0<?7;L+MN#"$1*AO4:.P)QB09*%>)?.0)"9)"7RS++-)"A%>+-(fi!#)T<?>:BK%>BK+?#$!9fi+CBK+-*%)96*T<B0*>U7.V.W*>)1+X$.03*-B0$!9fi+5!)9)"AO*)DY*>:3fi+JZ[O"EMF>:BK+\(("%$6!%>]%.0$6+-)#.0;YB09:*)5!"A*%)+*>)^ .0)"9)"#_9fiD`*>) ^ *)!%>)"A_,9fiD`4"-B0956+?$3"a+-;+-*)b%.P$:+-)"\*%$,*>)1("%)/!B0$!3fi+QB0,(:.0),)9:*A*-B0$!9fi+$ZM39fi+-3(fi)"%/!BK+-)D@+-(c))D:3(U.0)"%9:BP95=+-3fi%>d!+fe8gNhjikml87B0"AD`)*\7.nE0opq!rs:tAEFu$=+-))1*>)NB0,(c$!"*A9fi#)v$ZG*%>)j4c$/)1")w3:B0"),)9:*o8#$!9fi+CBKD:)"1<Q>fi*?>fi((c)9fi+TBVZ*>)N*)!%>)"3fi+-)+N+-$!,)NZ'$!"b$ZH!D:xBI+%+CB04:.P)U+X)"A%>y.P5$!"-B0*>z*$=5B0/!)Y9@$!(*-B07.+-$O.P3*XBP$9{*$=)/!)";@|}B05!>:*~ 3#.0)Y("$!4:.0)B0*TBI+v!+X!)D*$U+-$.0/!)!E2'Z?*>),("$4:.P)Y+1"),%>$6+-)9{39:BVZ'$!"v.0;{"#9fiD:$!v.0;!oB0*fBI+>:B05!>:.0;=39:.VBP)#.P;=*%>fi*7.V.8*>)+-)$(*-B07.+X$.03*-B0$!9fi+?9Y4fi)1D:)"-B0/!)D2Z'"$!9:;+CB095.0)!#"$&'*A4:.0)!EF>)UDBV]#3:.0*-;$Z?fi9fiDB095$!(*-B07.a+X$.03*-B0$!9fi+NZ'$!"v*>)U5)9)"A7.VB0*-B0$!9$Za|BP5>6* ~ 3#.0)"53)+?)/!)9,$!")1+X*"$!95.0;U5:7B09fi+-*\*%>fi*?(fi$:++CB04:BV.B0*-;Z$"?4:B05!5!)"(3#.0)+Nkm?*%9)"a"v3*>8opq!rs:tAE8F>:BK+M+-35!5)+-*A+*>fiO*8*>)G*%)!>)"uBK+89$*8Z'"))G*%$?3fi+-)O96;("$4:.P)+X$.0/!B095$!"W+X)"A%>1,)*>$*$U+-$O.P/)("$4:.P)Y+oBVZ\*>)x.P)O"9:B095d>fi+*$=4c)]+-3fi#)+%+CZ3:.nEUQ$<)/!)"ocBVZ?*>)v.0)"9)"NBK+v7.V.P$<)D*$@!+-w3)"-B0)+o}BnE)!E0oJ!+-@*>)Y*)!%>)"j*%$d+-$.0/!)=")#Z'3:.V.0;D:)+CB05!9)D("$!4:.0)+o*>)]+mBP*%3fi*-B0$!9BI+DBV8)")96*7E\F>)9@*>)N.0)"9)"19@!+-=*>)N*)!%>)"\*%$]+-$O.P/)j2("$!4:.0)D:)+CB05!9)D{+-(c)%BV7.V.P;+-$*>fi*QB0*A+1+X$.03*-B0$!9{<$!3:.KD`fi*1Y(fi"*-BK#3:.K"j#)#.V.JBP9*>)v!#"$!&'*#4:.0)!E1C9UZn!#*ocB09{$3")#L(c)"-B0,)96*D:)+#"-B04c)DBP9*>),9)#*j+X)#*-B0$!98o8*>)2*)!%>)"13fi+-)+N+-)"A%>{$!9:.0;@*$d+X$.0/!),*>)+X34("$!4:.0)+N*>fi*#$!""%)+-(fi$!9fiD2*$TB09fiDB0/!BKD:3fi7.M#)#..K+JB09Y*>)?#"$!&'*A4:.0)!E}n9fi+X*)!D2$ZBP9:*)"("%)*-B095j*%>:BI+J!+M*%>)?*)!%>)")9fi+-3"-B095,*%>fi**>)")TBK+,+CB095.0)!#"$&'*A4:.0)<?>:BK>YBK+?#$9fi+CBK+-*)9:*<HBP*%>U7.V.B0*A+Q+-$.03*-B0$!9fi+o<)9*>:B09U$OZ*>)N*)!%>)"\*%$,4fi)T-3fi+-*\*%>)j+-)"#>("$!5!"AO<?>:BK>@+-$.0/!)+Q*>),+-34("$!4:.0)+E?B0/!)9@("$!4:.0)Uou*>)N.0)"9)"jD:)#$,(fi$6+X)+TB0*TB096*$U+X34("$!4:.0)+1O9fiD@*"-B0)+T*$=3fi+-)v*>),7.0")D:;U.0)"%9)D!#"%$!&'$!(fi)"#*$!"A+B09=B0*A+H!#"$!&'*#4:.0)1*$,+-$.0/!)N*>)UEM>)9)/!)"Tv(fi"*-BK#3:.K"+-34("$!4:.0)D:$:)+9$!*?>fi7/!),#$!"%")+-(fi$9fiDBP95Y!#"%$!&'$!(fi)"#*$!"BP9`B0*A+*A4:.0)!o:B0*\+mBP2(:.P;U..K+Q*>) ^ *)>)"A_x*$,+-$.0/!)B0*14:;{+-)"A%>{9fiD{+-*$")+T*>)+-$O.P3*XBP$9@B09@B0*A+*AO4:.P)!EYF>:BK+TBK+1O9fi7.0$!5!$!3fi+f*$U!+-B095U,)v4fi)"#+->:B0(w3)"-B0)+MB09Y$!9)T$Z8\95.03:B098RS+J,$D:)#.K+G$OZ ~ .0)"9:B095=knT95.03:B098o8pq!r!rLtAEBP*%>*>:BK+J,)v4fi)"#+->:B0(w3)";,$D:)#.[o7B0*uBK+89$Q.0$!95!)"}")w!3:B0")DN*>fiO*8*>)")BI+}Q("$!4:.0)+X$.0/!)"8B09N*>)M.0)"9)"7RS+8>:;:(fi$!*>)+mBI++-(fi!#)N<?>:BK>BK+T#$!9fi+CBK+-*)9:*<B0*>U*%>)1*)!%>)"RS+Q+-$.03*-B0$!9fi+ETF>)")BK+7.K+-$Y9$,5!3fiO"A9:*))1*>fiO*?*>).0)"9)",9fiD@*%>),*)!%>)"1("$D:3fi#)Y*>)+2)+-$.03*-B0$!9fi+N$!9{"#9fiD:$!("%$!4:.0)+EYn9Z[#*o8*>:BK+aBI+,$6+X*.VBP)#.P;@9$!*Q*>)j!+X)!ofi4c)3fi+-)v*>).0)"%9)"3fi+-)+?*%>)1!#"%$!&'*A4:.0)1*%$,("$D:3fi#)1B0*A+T+-$.03*-B0$!9fi+9fiD2*>)?*)>)"J7;j9$*EM$!"J)#2(:.P)!oBVZW*>)Q*)!%>)"7.0<?7;L+fi9fiD+G*%>)a+->$"*)+-*G+-$O.P3*XBP$9*$v("$!4:.0)b46;=+-)"#>8o*%>)9YBP*#+\+-$O.P3*XBP$9fi+?*$v"AO9fiD:$!v.0;U>$:+-)9=("$!4:.0)+T")T.VB0!)#.0;U*$24fi)j+X>$!"*)"*>fi9=*%>$6+-)("$D:3fi#)D=4:;*%>).0)"9)"EQW}[@fififi2LOIn9*>:BK+?+-)#*-B0$!98oc<)TBV..03fi+-*%"A*)v9U((:.VBK*-B0$!9@$ZM*>)*>)$!";Y*$v*>)v|}B05!>:* ~ 3#.0)jD:$7B098EC 9|}B05!>:* ~ 3#.0)!o!.0)*QmoVo6o:9fiD,f")(")+-)9:*G*>)Q("-B0vB0*-B0/!)\$(fi)"A*%$!"A+}$Z,$/!B095jf*-BV.0)\"-B05!>:*o.0)#Z*7o63(8oc9fiD=D:$<?9"%)+-(fi)#*-B0/!)#.0;!E@!#"$6+")T")(")+X)96*)DY!++-*"XBP95:+!D:)T3(=$ZW*>)+X)?.0)**)"#+E$"W)#fi,(:.0)!o!*%>)+-*"XBP95 ^ :_Q")(")+-)9:*A+MD:$<?9fZ$.V.0$<)D14:;"-B05!>:*EM$!"}9$!*AO*-B0$!9fi7.6)!+X)!o7Z)*%3")+kn*-BV.0)+#tO")?.K4c)#.0)D=Z'"$!N*$,rv+-*A9fiDB095vZ'$!"*>)4:.K9=9fiD=uZ'$!"*-BV.P)vEM"%$!*>)1"5!32)96*$ZM*>)1("%)/!B0$!3fi++-)#*XBP$98oB0*BK++-)"-BK7.V.0;dD:)#$,(fi$6+%4:.0)1Z'$!"?*%>)Z')*3")$!"#D:)"-B095]v*%>"$!35!>@rEG!#"%$@- }")(")+-)9:*A+Q*>)j+-)w3)9fi#)1$OZ,$/!)+H9))D:)DU*%$,5!)*?*>)1[Kv*-BV.0)1*%$,*>)15$67.8(fi$6+mBP*XBP$9Z'"$!B0*A+#3"")9:*?(fi$6+mBP*XBP$9,o:<?>:BV.0)1("%)+-)"/BP952*>)\(c$6+CB0*-B0$!9fi+Q$Z7.V.8(")/B0$!3fi+*-BV.0)+B09fi%.03fiDB095=*>)4:.K9EfiGY8Ga{{!!!fi@fi!Ifi61384725dr6182(a)452876(f)rdlu7(b)urdl131862345(c)7druuldrdluuldrurdllurd31475628314655(e)23487(d)u0!,\{}0!:M#0v!:0fi=%06%,K,-!:7K\N!#'!fiAO!A-0!1%1-fi#C0!v-!67KH%!uKu0,:0,6%!}:#-0cv0,0!fiM-#-0!8'%MKGOxO:K:0!#%!'!fi#!?:KYKJPfiC,0Y%xX0!\%\?-!670fi-J0?K-0\fiXfiff HP%UfiAnK-#fi#-K--K0fi#,%1!%?!AKT-A%fi!-vfiffi-}?#:}!#!'#:0GfiJ0!}0,:0,6#-0!,!#fi7V v:0A}K--0fi#-0!Yfi ,!%GfiN0%6fiN,?%P:-# xO60%}?-fi:0:0{-T-fi%!Ofi!ff#"!$%?:K%@0@0{fiXfiK-{2L:[ !0!Qfi#0=0!=v-0-0!fi'&H1fi:A=c6C0-0!)(m+*P{u0!Y2-:AQ0:0-K7-AO1fi1c6A`fi6mPXP(' *?%-:AT,!:7-A%!,,-V0c6C0-0!fiv26vfi%@, @2-VPY:jcAT0!6-#-?:K{,!%-,Yfi./L0Qfi#j%j%!P#'-V0,fi:C0-0!@KT6vc!vK0jX0vfi:C0-0!IT:vfi21/cfiU-Y!8?v#6%\-V0vfi6mPXPUKT:vfi@3 0!,%!:004.fi!}!1%!!Ab0:5'!v=!#%!'!fi#!fiO1-0!N.fiA-v-!6[Y[\,0!',A:-0Iv`!!)6/0.fiA-N-0A75!2,:K/5'!c6C0-0!98=`0AN!67Gc6C0-0!GY-v0@fiN2K170!+=!#%!'!fi#!:);< =?>A@CB+DFE]0G Pvfi-NfiO1!#%H 8:0{2fi6C0-0!I(- *\0J.fi!!nv06K5L!#!'!cA!M%fiMA75!}X0,?fi2?:K/521#0J!67fic6C0-0!fiM-xPfiv:KN!#%!'!fi#!Nfi!N!,70!+fi0G0,fi-ff @fi2.fifixfivL#MHNPOQPRTSVUWCUYX[Z\RT]_^X`a-b'ZdcCe'^Zd^OCUR Ufb'XOSge'^ R O^VaRTOh\ZdXaY^^gi\Sb'^OCUWUYj^UY^ R Sgj^aWaRTUYj^aHUYj7R Olk b'mb'OknUYj^^OCUfb'aY^X e'`Ufb'X_OJUYXpRlcaYXqCe'^ZrUYX\UYj^de'^ RTaYO^atsujCb'Sjvb'U jX_`#e'hUYj^O c7R ^b'OCUYXZ\RTSVaYXTwPXc7^VaRTUYXa WHhCb'aY^SUfe'x kTb'm_^ b'UO^sZ\RTSVaYXTwPXc7^VaRTUYXa QX_`OhqCx ^yRTaYSjKMnzuj^x R aY^ b'ZdcCe'x UYX_aY^hqCxUYj^e'^ R aYO^Va{b'Ob'U Z|RTSaYX wfUR q#e'^_Mdzj#bRmX b'h UYj^ ^afb}Re/c~RTa b'Ok UY^cfisujCb'SjsGX`Ce'hq7^tO^^Vh^hbQHUYj^[UY^ R Sgj^acaYX_kaR Zb Xc~RTC`^M~fiG/HKC/dn77/// /HC7F+~P\///P7 |?TH7[d//,d-/fiy}7H /+/ T/,~-,gHC[gt7/+tT+-C_7 KT+/ /dH,7 -,gG_ut K +PTJ__7+fi+gHfi /lH,g?!|-,7_gGPt/gt/ /C/dg/ K/y} /+//7/ /C+ /\+-,gH\7HJ7T+//CK ,[l/C/C+Gd\Tfi H|VH\7HH?HT/ /\+-,gH/d ?J-,7_V-_?++nK ?d|- K /K -HK/P7\ /KY_7+7H TH2d+HCuF KA +P+///K_7AC7)HTC,P7+[HTHK/ /++,C|/t/K_7r|K / |T!+\ +/H7+{Hg/ d//Y_7+nHCd_~+/H7+l?!T7+7nK 7H+dH7+Ctd/T_+/7Hl7J+ldC Hg/ / /+YKP y//yK7C\{77+ / K+CC\d/+_|u/7T!C_7Kt7CKC C, _~+/?C_Cdd/C\H_\+C//_H/7_9+CK9+ C ~~J9-gP+J7pT/?79d/?7T+/9+CJ+PTT+//C_7\C/+?, _+/+C[nCH- G/ +PH yC\ H7 /d_TC\ /CK CPCH/H7K H7+d|T7H?+C/_K2V/7_d/~|/?TH7\ /C 7+/K, yKP[,n2 /+ [/C+~?T+Hd / /CTCPCH/H/K\+P lH ?+/C\TH7? /7J+/C+/_1008060Accuracy402000510152025Number Training Examples303540u/K\G7 +/?//KYHC_[7T+//K_7K /CdH7n/_7H/7?+CP72|K?HV+l7[ /7|/ngHHtH /[7 +/ / K[Pd/y/-g /HC7/+/H+ /{ +/g 7-C7Hd7/ d/C) /+Cr K+T[-C_7 K_FP# /n//K,P_/yK )+J 9/HCCHPv}) H7/JK 7\ /7P g/Hdl+TJ7H H7 /?/T+CC7//KH27G/d 9T_+/Hd /?7t++7 +/-CCH H7THH ?T/!KC C, y//,Pl+/H7+ C/H7Hl /7 +/-CG/K/ CCHv / -CY7H~fifffi!"$#&%')(+*,(+(+'.- /102/35467)08693;:<->=?@BADCEAGFIHKJMLN2HPOAGQPAROSDTAVUAGAGFSXWYAGZ[W]\QP^_S`J2W]QaS+^&AGZB\QcbedBfQc\f\gdhADijHP\jCGS+fHPkQPAldKfAGADiIkfmJMADS+QPFInMFopq\QAErS+^&fIJMAN9s\OAGFmtauDvvwexS+FSJyLIzGADdBSj{`|\+JMkHKnM\Fj}$S+HPOjs~SCPOInMFog^&ADCPOSFIn dh^SFi_dKO\+ZdHPOS+H\QPoIS+FInMzEnyFofiHPOAVdK\JykHhny\FdB\+W$HPOAHPQaS`nMFInMFo&fQP\UIJyAG^fidBnMFSRHPQcAGAVS+FifiQPADdKHPQhn CEHhnyFofiHPOAVdKADSQaCPOfi\+W$HPOAfQP\UIJMAG^dK\+JMTAGQHP\&HPOIndHPQPAGAny^fQP\+TADdHcOAfAGQKW]\QP^_SFCEAV\+WHPOARfQP\UIJMAG^dK\+JMTAGQnyFmHPOAdhAGFdKAV\+WQPADiIkCcnMFoVHcOA~FIk^lUAGQ$\+W2F\eiIADd$dhADS+QaCcOADiVZnyHcO_SOInMoOfQP\US+UInJnMHLp\+ZBAGTAGQDNs\OAGF dQcADdKkIJMHad$iI\F\HokS+QaSFgHPAGABS+FlAGTAGFIHPkS`JCE\FgTAGQPoAGFCEABHP\S+Fj]DD9fQP\UIJyAG^dh\+JMTAGQDN+UkH$\FIJMLlHP\S+FV\fHhny^fiS`JfQP\UIJMAG^dK\+JMTAGQVSCcOInMAGTS+UIJMAfiUgLmQPADdKHPQKnCEHKnMFojHPOAfidKADS+QaCcOHP\_HcOAHcQPAGAl\+WdK\JykHhny\Fd\+W~HPOAlHPQES`nMFInyFofQP\UIJMAG^_dp~LjiIAEFInyFofiJMADS+QcFInyFoSdBfQP\iIkCcnyFojSf\+JMLIF\^lnS`JM]HKnM^&AlfQP\UIJMAG^dK\+JMTAGQSd\ff\gdhADiHP\d)nM^&fIJMLlQPkFFInMFoVW.SdKHcAGQHPOSFVHPOAB\QKnMo+nMFS`JfQc\UIJMAG^dK\+JMTAGQ`N+Z~ABOS`TA^&\QPAdKHPQhnyFoAGFgHCE\FinMHKnM\Fd\FdhkCGCEADdPd)W]kIJ~JMADS+QcFInyFonyF\kQWYQaS^&AGZB\QPb2p_q\QVAErS+^&fIJMANnMFiI\^_S`nMFdJnMbAfiHPOAjnMoOIH}kzGzEJyAN|\+JMkHKnM\F}$SHPOs~SCcOInMFoZBnJJVfQc\eiIkCEASFAErf\FAGFIHKnS`JJMLJS+QPoAmHPQPAGA\+WdK\+JMkHKnM\FdNd)nMFCEAADSCcOdK\+JMkHKnM\FoAGFAGQES+HPADijUILHcOAV^_SCEQc\]HaS+UIJMAnddKHc\QPADiSdSfS+HPOnyFHPOARHPQPAGAp2ADS+QPFInMFodKkCcOmJ SQPoAHPQPAGADd>ZBnJJFAGADiRAErf\FAGFIHKnS`JJMLlJS+QPoAFgk^lUAGQ$\WAErS+^&fIJMADd$S+FiRAEref9\FAGFIHKnS`JJyLRJM\FoQPkFFInMFoRHKnM^&Ap.FQPAGHPQP\gdhfADCEHDN+HPOIndn dF\HdKkQPfQhn dnyFoRUADCGS+kdhAdK\+JMkHKnM\FfifS+HPO&CGSCcOInMFondSZBADS+bJMADS+QPFInMFol^&AGHPO\iHPOS+HiI\IADdF\HSdPdKk^&AR\QAErfIJM\+nMHS+FILdKHPQckCEHPkQPAnMFHPOAlfQP\UIJMAG^dKfSCEApnMHPOAGQVS_iI\^_S`nMFOSddK\^AdKHPQPkCEHckQPAS+FiOAGFCEAdnyoFInCGSFgHdKf9AGADiIkffin dBSCcOInMAGTS+UIJMARUgLlAErfIJy\nyHhnyFoXnyHnMFjdK\^&ABJMADS+QcFInyFoS`JMo\QhnyHcO^N\QnMHViI\IADdF\HOS`TASFgLdKHcQPkCEHPkQPANnMFZOIn CcOCGSdKAJyADSQPFInMFoCGS+Fm\FIJMLOS`TAJnM^lnMHPADiUAGFAEH`pARUAEJnMAGTA&HPOS+HHPOAVQP\JyAR\+W~SlHPOAG\QPLj\+WdKfAGADiIkfJMADS+QcFInyFofindHP\jin dhHKnMFokIndKOUAGHKZBAGAGFHPOADdKAHKZB\CGSdKADdNeSFi&fQP\+TniIAJMADS+QPFInMFo_SJyo\QKnMHPO^_dW]\Q~CGSdhADd$nMFjZOInCcOd)nMoFInCGS+FIHdKfAGADiIkfdS+QPASCcOInMAGTS+UIJMApBOAT+S`Jn inMHKLj\+W$HPOIndBoAGFAGQaSJ^&AGHPO\iI\+JM\oLndS`JMQPADSiILfiU\QcFA\kHUIL_HPOAQhn CcOjU\iIL\+WQcADdKkIJMHadRnMFCE\^&fkHaSHKnM\FS`JBJMADS+QPFInMFoHPOAG\QcLJnyHcAGQaS+HPkQPAfinMFHPOACE\FgHPAErHV\WCE\FCEAGfHlJMADS+QPFInMFopkQS`nM^ndBHc\&HPQaS+FdWYAGQBHPOIndB^&AGHcO\eiI\+JM\oL_HP\&dKf9AGADiIkfjJMADS+QPFInMFoNniIAGFgHhnW]LjfQP\UIJMAG^<iI\^_S`nMFdBW]\QZOInCcOAE ADCEHKnMTAdhfAGADiIkfjndf\gdcd)nMUIJyAN SFifiUkInJidKf9AGADiIkfJMADS+QcFInyFojS`JMo\QhnyHcO^_dW]\QHPOAG^pkQZ~\QPbZSdV\QKnMo+nMFS`JJyLSny^ADiS+HRWY\Qc^_S`JnMzEnyFoSWY\QP^\+WrfIJS+FS+HKnM\F.BSdhADi2ADS+QcFInyFot~x$tSiIAGfS`JJn.N2uDvvuDSIxap ~jCE\FdKHPQPkCEHadSfQP\I\+W\+W2O\ZSfQP\UIJMAG^1nd$dK\+JMTADinyFlHPOABHPQES`nMFInyFoAErS+^&fIJMAkd)nMFoSF;AErefIJnCcnMHmWY\Qc^\+WXiI\^fiS`nMFHcOAG\QPLNS+FiHPOAGF;oAGFAGQES`JnyzGADdS+FiHPQES+Fd)W]\QP^_dHPOS+HfQP\I\+WHP\_SjCE\FgHPQc\+JQPkIJMAl\QS&^_SCEQP\]\f9AGQaS+HP\Q`NZOInCPOmndKkdKHKnADimUgLmHPOAV\QhnyonyFSJ~iI\^_S`nMFHPOAG\QcLtKjnMHaCcOAEJJ.NAEJJyAGQ`NADiS+QPKsBS+UAEJJn.NuDvAD+\Fom\g\FAGLNBuDvnMFIHP\F Nu`vv|OS`TJnMb2NuDvvIxapnybA|\JykHhny\F_}$S+HPOs~SCcOInyFo9NgBnd$SZBADS+bJMADS+QPFInMFoV^AGHPO\ei2NSFinMF&oAGFAGQaS`J.NCGS+FF\H$U9AAErfADCEHPADiHP\ny^fQP\+TAHPOABfAGQhWY\Qc^_S+FCEAp.FiIAGADi2NIHPOAQcADdKkIJMHad>nMF&HPOAdKf9AGADiIkf&JMADS+QcFInyFoJnMHPAGQaS+HPkQcAdKkooADdKHBHPOS+H~CE\kIJifiJMADSiHP\&fQP\UIJyAG^dK\+JMTAGQEdBZOInCPOS+QPA^lkCcOj^&\QPA&DDHPOS+FHPOA_\QKnMo+nMFS`J~fQP\UIJyAG^dK\JyTAGQadtnyFIHP\F NuDvvHPzEnM\FIn.NBuDvvIxEp\+ZBAGTAGQDN iIAGfAGFinMFo\FHPOAdKHcQPkCEHPkQPAfi\+WHPOAjfQc\UIJMAG^dKfSCEAjkdKADi2NBS+FiHPOAjZS`LmnMFZOInCcOB$d&iI\^_S`nMFHPOAG\QPLnCE\iIADi_S+FiRkdKADi2NnMH$ndf\gdPdnyUIJMAW]\QBHP\JMADS+QPF_dKkCGCEADdcd)WYkIJJMLnyFjdK\^AdnyHckS+HKnM\FdGp$q\Q$AErS+^fIJyAN$HczEny\FIndKO\+ZBADifiHPOS+HnyFHPOAVXGldhLedKHcAG^NB$ddKkCGCEADdcdOInyFoADd\FnMHadSUInJnMHKLHP\XFiCE\FdKHES+FgHc.d)nMzGAVF\FQPADCEkQEd)nMTAVfQP\I\+W.dBHPOS+HdKO\+Z;HPOS+HCcO\g\gdnyFojdK\^A\fAGQaSHP\QadnMFCEAGQPHaS`nMFdhHaS+HPADdndS`JMZS`Led>USifit)\QSJyZS`Ldo\g\i9x$tHPzEnM\FIn.NuDvvIxap|kCcO_CE\FdKHaSFgHP.dnyzGAfQP\g\WdQPADdhkIJyHnMF_CE\FdKHaSFgHPd)nMzGAjCE\FgHcQP\+JQPkIJMADdGNZOInCcOS+QPARnMFAEref9AGFd)nMTAHP\m^_S+HaCcO p&WHPOAGQPAXn dlSFInyHcAdKAGHR\+WdKkCcOCE\FIHPQP\+JQPkIJMADd$HcOS+HCGSFQcADiIkCEAHPOABFgk^lU9AGQ$\+WdKHES+HPADd>AErefS+FiIADinyFfQP\UIJMAG^dK\JyTnMFoWYQc\^S+FVAErf\FAGFgHKnS`JW]kFCEHKnM\F\+W$HPOAldKHaS+HPAd)nMzGAlHP\_Slf9\+JMLgF\^lnS`JW]kFCEHKnM\F N9HPOAfQP\UIJMAG^dK\+JMTnyFoCGSFjUAVokS+QaS+FIHPAGADiHP\fiHaS+bAR\FIJMLf9\+JMLgF\^Xn SJHKnM^&Afit$HPzEnM\FIn.N$uDvvN}QP\f\Id)nMHKnM\FwpMupMNfopluDwIxap$HPzEnM\FIn.d\QKnMo+nMFS`JdKLdKHPAG^|gPsAErefIJM\+nMHPADiHcOIn dfidKHPQckCEHPkQaS`JW]ADS+HPkQcA\+WHPOAfQc\UIJMAG^dKfSCEAHc\JMADS+QPFAECcnMAGFgHfQP\UIJMAG^dK\+JMTAGQadZBnMHPO\kHkd)nMFoSFgLAErS+^&fIJMADdltHPzEnM\FIn.NuDvvIxapRdKkUdhADkAGFIHdKLdKHPAG^CGSJJMADifi2GDDB~++PmcKDlE+&IMDPIGgh l P IyGfiP EI`MD P EMIE&IMDl ~ c+MD$MfiG EK c IK K KMfiM c_+KMfi +IGP PXyD _ c ] E+P aR EI P cIyDfi P B c MD cIy +K E BEM MD P _ EG) KD ] lK+MKM P $ EMI KG E+PDK B P+MD jKE+&IMDI jPlMD Pj lK K KMy ] Pfi+KMIM E cM)MB EMI _ PDPgRDPyX hK ] PIMDI hP P E a) mEeI+KM PDKIMa BM _ P ]a IMDl + I++KM BhD2D cIy G+ KDficVMD cj cMGg+ Il` ]KM&R P IMG h+ IM PMGK h hy I_`M&E IyE~h&K P Ec PK &$K K` E&IP Iy+I G+KM R G PMD PIM _ c ] E+P G+ fiKDPEI`M _K EDPM;I_`MR $ EM EG_`E)M hD5 G`MD g G` + K` $ a)M fi + G & aIK I` P_ IM j. KPG_ +VK EDP ]I yD Pfi Pg)Mg` aI hy RM&I cMK + G XI IM l K g K _ $ & PK EDc) EcM) ` aXG~`$E+Iy g $ EM g $ P GcKM I+E P E9g)MKM GcKM & I+ jKM E c Egyhy GPKM & I++KMD fi+ E c E9g)MKM +K PG PDhGgaKM` K E IMDBM j Vh K`I E&9gP IM VI_`Mfifi`MI cDK IM ]K EDcMMD PIM fi_ P ] a+c EycG ED G BGG h $ ay l+g _` hy _c B P E)KM fi l E+P aKDmM 2 &9 a+P K` $ E)M PV +PfiRMK EE IM fiMD cIy & +IK KjEePGIX aP aR P`Kj RP&MaMD cIy m&G RMD PIM j& +IK g BMD lEa P EP & a+c IIDV _ EDPP aPG_hE ` aG`IK DhaB + lM EG`MBBB EDPP _c EDPP&I c a+K E PG PDhGgPDfi a+c aP& Iy VKGDI)M _ c ] E+P BM YDP P aI KM [_ I_`M;K K`E&9gP IMfilK cMGIcjM l j+ P K+P EMKMXP+I m_ P ]9 a+P&fi`M g`$ g P EDP l K YD+c Pl aI KM jgEM &l E BM lKfi`I _ P ] aPBM K& Mj $Ky + ) yIPIG`ID K 9DjG e GMD B+ $ a)M yD Pl E P E]D+P P aI KM `M BM & _ ca IM ) ID c$cMD P& _ PRa IME+M& E+M& >)M jIMKMIMfiE+IyDcP+ +c& ]D+P c + E PDK9P + G E+M& Iym I&G e B g ] g EM BMlIP) IMc EK P E&I_y ] GEll)MDMIP P ]D+P c + E +GDP j. G P& c + E P EKGDIMD cIy` hy lMD Pj E P E ]D+P c aI hy + _ P ]a IM P E&IMD&K+MKM EK c EPD P +_ P ]E IM+IGP EKM h K`E&9gP IM BM E+&IMDB G a` EI hPG El fi P ]a IyD~a+IPGD 2 P$I g`KaP+R B E+P cMgK P$P+KDKR P9 PKMDB G a` 9 +IB M_ 9lD)M!"#-$/.%3$:C .,*F3fi3=K.%,* :ffSF*(hgie 44Zo4):s3"3/.%K feB*OffSF,.K4. 0.$Q+$$AE"F2.%Q34,,4%.Q.,W437. % . 1n.4,*,4Q4%WQ:L(MON_P:m,4%23.%BQ)..3 $R.34u*BvQ49$/.n$).4.%4)J$/.*. 0:3-3,rTU,3$6.%R.3Q0..aYE"F9ff.3<E>!"6.%n$Q/./.W$3W,4%K,#4. 01.%Qi43 iffEA4jj. % ..3>.r6A3.B4RI.%/.,j. .343$"!"%4i R. 01ff6%% .%Q+* 34K3_EY37$J03144"]z)..(X. 0/t4aYE"F>3K _e 4K]#* 4n J0i4/./.%Ql.9.4B4,fiGAE"Ffi$,4:$Z,*+$.hgie 4Q:@Q3j$3.).).3Kff)..^$"L(MON:P_mv.%Q|O.XB:4(<;<a.14(.Q444Kf. .. ,4O$7j.a4 ,*40"!"3WAE"Fj8!Qfi*,).%GAE"Frq+.%.%.3$3n.%:ff6}.44..33374!#*"K.%BfiHI.(pgyz}~3/.%4_B40E`(i!",XTU |{f!")..B*1,* J4R4 ,*3(4%n3).lYE"F>3xw .3,4f$j]ff68K%4Q.,@T U6VffS!"2kgf$.W3$).%@.'45B7L(MON_PG:]3).,QBQ4.8ffSF393,0%9. .. 0!29.,fi4 ,*,5TU dcjRb.f. % .4,,44)Z\[/.@).. 0. 0,4,#:,3Q% ,3 fi$3HI.fiff+*8D$2>ff6 ?:.1,Q 7LXMWN:P33%36.%731L(MON:P$1.,*).34&('),./.4%4G.).#*B. 0;<$3)..3+$. 0`'* #4#RK.%Q4}(=Yih@ d#A%/%h""S#_%}|h%h@n%n )#h#6#S>#S%h63h3, h33%"Sh`h#>|hd%r+%%3|h#9h@3S#A3#)W>3#x%W+%}B ||h#h3h+fipxAG(A x7+x7 )i)b)l%%lj5,-)I 1j ) ,,)x1 )31,@A)#/KnR#6K,5_3fi#,,)K2% 6"/#d3)-6#( 15z3ZZ,SGY):63B R%)A)+b 3A>% B Yv"K6fi@R )63%6WKR )^,Wl 63,(,:R#a1,1O)fi5z`,,j%f%lI%,zR ) BB )O D/#2 -)zKZ,9z)%) ,I-6Kfi3)))%,WK1GWR#,v3K)j% ,fi)3",@)1 %,3,:Z/3,j:I%,z@ ) ,,G )63%%,z%@bY)K 1563Y)"3,z 6b%GR#W))zSB,W%%WZ("") n JZ/%,n ),,)Y3 )63%6@/O,3r,z%,G/#("DBvW3D3=5 Bfiy,3/63R}% BAK)_B) (6 ,z%) -1 )35:,3,W")3>R,jK7<8)B _R#, 31 ,i,j5,)1#% B#K67 3R3/393)9zG6# )9%bO"il ,,^)6+B zfi1 /fifi3,B 6`,9 ,v3)<)`,%,ZZBvR W%)3n ,5_%_ ) ,,))/i)%j)3@R,fi,3%B8/-) %/#K/`/V6%,fi3)fffi,)7i1+,)3/W) 87ZZ%Z,_,83_v:3,:,3,9 3)`R3B )7%R B3,#1A%i63)`G3,fi/O,iROZ"B:`,Z3,i,13%_%,)%R3) 19fi 3):,7\W3"6#,,i,RR 63/)3631 @13j3%,R3,+,v,R#(,3,",jO,j%)3ZZB@3fi"G363 j3)91nr,i3, h3Z,3nn-`,:ZbKW63%33, ,aG,3A63)S%R3,)"b%l3%W3v) A71%6#,,j31,1Z5 R3) O:,",13}fi ()R5^_3%z%l3)8#Z-1/)3W _ z- Yb:)fi-W z3)3v6}>v)3/ /35",fi ,O)3%B3,:)@v,i)3,:/#A,1Z,1: ,)@")#O1)6#,7fi@33z1 // ,)3fi3G)6+BV1 /:/:%)319 /9RKfi1fi%%b3""i#/9G8z3^,G1%fi, R3,<)%1 B/,,)-7G3Gfi / ,)V z r1 z#"/:v/zZ) %G,%,!)#Z,63B) )3>)3 3z%BG`,Z3z5R2B%R@ )3,DR#Z31 z:3n%6#,,)89:"6}d"j3%I ,l%):,6:b%9R#7:5j)G6#,rW ,3,W,Bi B-) fi) %d3)7KR31)KBGj zK6%/#=3)#,`/"fi zK6b%j3)fi ,fi3)#,Sb3d%WR#,fK)-bZfi)zSB,G/1v,13)GR _,,8fi"j B)-31 b}A3"1 dR#15W)G-R3,:,3B )n3)y,,j5)K)j16+,v1 =GGv)W)fi:)%G,:,3,)}jO xK1,>13-,z%,iG3i863RKbK3):zY6 /r)3r1 ,W@`,3,)3>R ,b%13:)"3/3D,IO )63>,13:,) n@36#Z zA3(%$#3 BO 5/#ZB1%i//)KWR %,@,) R-z:)O3n p%zO2#fi#AR#)O33)-,)63)%,1 KJK6(Y>j1R#ZB) %,v63)%B%-/,,)}/fi )/#Z,fi,)%3,R "1B&)363ZZ,S':/(>%GzWK6J/zfi_ ) BB f#Z,1G ,:,3%("ZZ)#:jz3B W,1) _3)iDB zO) _ ) z/#") )3,1,j63%)363ZZ,Sfi /6Yv x B-WKb O3d,@/" ,jK 1!(=/`,3,,1: 16K,(3)O) n %))r%)6 %3 (,3,6X/%_3Y)W 3z%Br3)R )63%6#/@ ,W)%6%z)+*,fi-$.0/21435.066798;:<.>=>.0?0.0@A.0BCEDAF5FHGIFJ>DLKMNCEOD!FQPRKSTF5U2KQVW!XYUZC[J4KQ\]^_0`0DQJ>a%J>UbScJ>U%JdU2egfh_0Ub\AF5DjiQkllmbn o0DAFGIF5Ub\jJ>UZFQpqJ>arobstFhuHM0F5DAFJe0e4STU0v%JGwStU0vdstFxDAFeb_0U2e0J>Ub\Ha%JKQDOyEDA_bsTFKQDAFJ>\AFG9GIF5o2JdDLJ>`bSsSt\IVO>Cz\AM0FGIODA\9o0DAFG{F5U|\9STUO_0Dha%JKQDAOy\LJ>`bsTFG5]\AM0F5DAF5`bVNFQp0o2OU0F5Ub\IScJssTVgGIo F5Fe4StU0vN_0oxo0DAO`bsTF5a}GIO>sT~STU0v2WzXY\u9O_bsce`2FSTU|\AF5DFGI\ISTU0v\OHFQpobsTODFuHJVGO>C!\ADLJ>U2GwCODaZSTU0vebO4a%JSTU\AM0F5O4DISTFG9STUJxuHJV%\M2J>\HGIF5o2J>DLJ>`bSsST\IVSGho0DAFGIF5D~FeODHKQDAFJd\AFe$WH_0DCDLJdaF5u9ODAK5J>o0\A_0DAFG`2O\AMF5aobSTDIScK5JsJ>U2eFQpobscJ>U2Jd\ISTOU0yE`2JGIFeGIo2F5Feb_0osTFJ>DAUbSTU0vaF5\AM0yO0e0GrStUJ"_0UbSCODaa%J>U0U0F5DW;H_0D2QjHGIV0GI\AF5aSGebFGwStv4U0FeJCE\AF5D5b5]9uHMbScKMSGebFGAKQDIST` Fe JGZJ>UQF5aobSTDIScK5Js9sTFJ>DAUbSTU0vGIVG{\AF5a%iIST\LKM0FQssHF5\J+s[W]HklnL]J>U2e"O_0DZa%JKQDOyE\LJ>`bsTFsTFJ>DAU0F5DxScGZGYSTaZSsJdD\AO&ZZ]uhMbSKM"SGZebFGAKQD{St` Fe JGxJ>UQFQpobscJ>U2Jd\ISTOU0yE`2JGIFe"sTFJ>DAU0F5DQiIJSTDLeF5\<J+s[W]0klbnQWFH~4STF5u \AM0FGIo2F5Feb_0orsTFJ>DAUbSTU0vZo0DAO`bsTF5aJGzO4U0FHO>C22U2e4STU0vJKsTO|G{FJ>o0o0DAOpbStaJ>\ISTOUO>C!\AM0F\QJ>DAvF5\9o0DAO`bsTF5aGIO>sT~F5DhCDOaFQpqJ>arobstFGOdC\AM2Jd\<o0DO`bsTF5aGIO>sT~F5DJ>U2e\AM0FebOa%J+StUGIo2FKS2yK5J>\ISTOU`bVFQPRKSTF5U|\IsTVGIFJdDLKAMbSTU0v\AM0FNMbV|o O\AM0FGYScGHGIo2J4KQFNO>Co0DO`bsTF5aGIOdst~4F5DLG5WF5U0F5DLJ+ssTV\AM0F5DFJ>DAF\Iu9OSTU2e0GO>C<KQO4U2GI\ADLJSTUb\LG9O` F5VFe`|V\M0Fo0DAO`bsTF5aGIOdst~4F5DLG9STU\AM0FxM|Vbo2O\M0FGYScGNGIo2JKQFWNHU0FZ4STU2eJ>DAFN\M0FGIF5a%J>Ub\IScKNKQOU2GI\ADQJSTU|\LGuHMbScKMJ>DFO` F5VFe`bVJssebOa%JSTU2GSTU\AM0FNaF5\LJ>yebO4a%JSTUW!0ODHFQp0yJ>aobsTF]0GIF5DIScJsebFKQOaro2O|GAJd`bSsST\IVScG9GI_2KMRJZKQOU2G{\ADLJSTUb\W!9M0FHO\AM0F5D94STU2e%J>DFGIVbUb\LJKQ\IScKHKQOU2G{\ADLJSTUb\LGOU\AM0FG{\ADA_2KQ\A_0DAFNOdC\AM0FN\QJ>DAvF5\9o0DAO`bsTF5aG{O>sT~F5DWjqOD9FQpqJ>arobstF] \AM0FKQOU2G{\ADLJSTUb\LG9\AM2J>\9\AM0FN\QJ>DAvF5\o0DAO`bsTF5aGIO>sT~F5DScGODAv|JdUbSt5FeJGJaJKQDAOyE\LJd`bstFhOD<JG9JxGIF5\jO>C!KQOU|\DAO>sTyEDA_bsTFGu9ST\AMsTFQCE\9M2J>U2e%GwSebFGuHMbScKMJ>DFG{F5U|\AF5Ub\IScJs!CEODAa%GOdChJv4DLJ>aa%JdDJ>DFFQpqJdaobsTFGNO>CGIVbU|\QJKQ\IScKKQOU2GI\DLJSTU|\QG5W9M0F%GIVbU0y\LJKQ\{SKxJ>U2eG{F5a%J>Ub\IScKgKQO4U2GI\ADLJSTUb\LG9OU\AM0F\QJ>DAvF5\Ho0DO`bsTF5aGIO>sT~F5DM0FQsto"`bSJ4GH\AM0FsTFJ>DAU0F5D] STU\AM2J>\\AM0F5VrSTao0DAO>~FST\LGHJ>`bSsST\IV\AOZvF5U0F5DLJsST5FxCDAO4aJZGIaJssU|_0aZ` F5DHO>C!\ADLJSTUbSTU0vFQp2J>aobsTFG5WhF5U0F5DLJssTVGIo FJ>STU0v2]$\M0FgGIF5a%JdU|\IScKxKQOU2GI\ADLJ+StUb\hSGNG{\ADAOU0vF5DStU<GIV0GI\AF5a%GJ>U2e\AM0FgGIVbUb\LJKQ\IScKKQO4U2GI\ADLJSTUb\ScGHGI\ADOU0vF5DSTUF5aobSTDIScK5JssTFJ>DAUbSTU0vGIVG{\AF5a%G5W!F5o2F5U2e4STU0vOU\AM0FGI\DA_2KQ\A_0DAFO>C\AM0FG{FN\wu9OxSTU2e0G9O>CKQOU2GI\DLJSTU|\QG5]>\AM0F9sTFJ>DAU0F5Da%JVgJebO4o0\jJN~+JdDISTF5\wVxO>CGIFJ>DLKMGI\ADLJd\AF5v>STFGz\AO2U2eJ>UJ>o0o0DAOp|STa%Jd\ISTOU\AO\AM0FN\LJdDAvF5\jo0DO`bsTF5aGIOdst~4F5DjSTU\AM0FNM|Vbo2O4\AM0FGYScGHGIo2JKQFWXUv4F5U0F5DLJs]2JssGIo2F5Feb_0osTFJ>DAUbSTU0v%G{VGI\F5a%GJGAG{_0aF\AM2Jd\H\AM0FQSTDDF5o0DAFGIF5Ub\LJ>\ISTOU2Js!GI\ADA_2KQ\A_0DFGa%J4KQDAO|G5]2KQO4U|\ADO>sDA_bsTFG5]$ODHuHM2Jd\AF5~F5DHFQscGIFxJ>DAFJebF4_2J>\AFr\AOGI_2K5KSTU2KQ\IsTV&DF5o0DAFGIF5Ub\\AM0F%KQO4U|\ADO>sbU0O>u9sTFebvFU0F5FebFe\AOFQPKStF5Ub\IsTV&GIOdst~4F\AM0Fo0DAO`bsTF5a%GhSTU\M0FQStDebO4a%JSTUWXU&Je0e4ST\ISTOU\AOGIVbU|\QJKQ\IScKZJ>U2eGIF5a%J>Ub\IScKx`bSJ4GIFGuHMbScKAMDAFG{\ADIScKQ\N\AM0FMbV|o O\AM0FGYScGGIo2J4KQFO>Co0DAO4`bstF5aGIOdst~4F5DLG5]!JstFJdDAUbSTU0v&GIV0GI\AF5aaStv4M|\ZJscGIOStU2KQO4DAo2ODQJ>\AFo0DAFQCF5DF5U2KQF`bScJGIFG5]CO4DNFQpqJ>arobstF]zo0DAFQCEF5DgG{M0ODA\AF5DNDA_bsTFG5]ODD_bstFGxebF5DIST~FeCDAO4aGIM0O4DA\AF5DNFQp0obsJdU2J>\ISTOU2G5WScJGGIo FKS2FG\AM0FKQOU2e4ST\ISTOU2G_0U2ebF5DjuhMbSKMrstFJdDAUbSTU0vG{_2K5KQF5Fe0G<JdU2eJscGIOo0DO+~4ScebFG!\AM0Fw_2GI\{SK5Jd\ISTOUCEOD\AM0FsTFJ>DAUbSTU0vJsTvO4DIST\AM0aW^o F5Feb_0osTFJ>DAUbSTU0vGIVG{\AF5a%GjG{OaF5\ISTaFG9GI_bF5D!CEDAOa}uhM2J>\jM2JG` F5F5URK5JssTFe\AM0FQ_0\ISsST\wVro0DAO`0ysTF5a]"uHMbScKAMScG\AM0FSTU0FQPRKSTF5U2KQVO>C\M0FstFJdDAU0Feo0DAO4`bstF5aGIO>sT~F5DK5J>_2G{Fe`bV&\AM0Fo0DO>sSCEF5DLJ>\{StO4UO>C!sTFJ>DAU0FeKQOUb\ADAOds|U0O>u9sTFebvFZuHMbScKMSG\O|OFQpo F5U2GYST~F\AOr_2GIFiwStUb\AOU]zkll4bnLWH_0DNJ>o0o0DAO|JKMGI_0vv4FGI\LGN\M2J>\\AM0F_0\ISsST\wVo0DAO`bsTF5aK5JdU&` FRGIOdst~4Fe"StUGIOarFRK5JG{FG`bV&KQOU2G{\ADLJSTUbSTU0v\AM0F%\QJ>DAvF5\o0DAO`bsTF5aG{O>sT~F5DGIOZ\AM2J>\ST\HOUbsTVrsTFJ>DAU2G9FQPRKSTF5Ub\HCEODAaG!O>CjKQOUb\ADAO>s$|U0O>u9sTFebvFZio0DOo2F5DIsTVrSTU2ebFQpFea%JKQDOyEOo2F5DQJ>\AODLGhODKQOUb\ADAOdsDA_bsTFGQnNJ>U2e_2GIFGN\M0F5aStUJKQOUb\ADAO>ssTFeCJGIMbSTOUWr^>STU2KQF%\AM0Fr_0\ISsST\IVo0DAO`bsTF5aSG_0U2GIO>sT~+Jd`bstFSTUv4F5U0F5DLJsHiwSTU|\OU]!kllnL]2O4_0DJ>o0o0DAO|JKM&GI_0v4vFGI\LGJuHJV\AOScebF5U|\{SCEV\AM0FK5JG{FGSTUuHMbScKMST\QZ` FgGIO>sT~FeJ>U2eo0DAFKScGIFQsTVKM2J>DLJ4KQ\AF5DIST5F\AM0F5aWM2JdDLebOUiLkllbnFQp0\AF5U2e0GO_0Dju9ODg\O\AM0FHDFQStUbCEODLKQF5arF5U|\sTFJ>DAUbSTU0vo0DAO4`bstF5auHM0F5DF\AM0FhvO|JsScGx\AOsTFJ>DAUJ>UFQPRKSTF5U|\JKQ\{StO4UGI\DLJ>\AF5vV]SWFWT]!Ja%J>o0obSTU0vCDAO4aGIF5U2GIO4DAVStU0o0_0\QGZ\AOJ4KQ\ISTOU2G5]STUJGI\OKM2JGI\IScKebOa%JSTUWffHUbsSTF\AM0F\IVbobScK5JsDFQStUbCEODLKQF5arF5U|\rsTFJ>DAUbSTU0vJ+stv4ODIST\AM0a%GruHM0F5DAFJKQ\ISTOUGI\ADQJ>\AF5v>STFGjJdDAF9sTFJ>DAU0FerSTU2e4STDAFKQ\IsTV`bVZsTFJ>DAUbSTU0vZ~+JsT_0FCE_0U2KQ\ISTOU2G9O>~F5D<G{\LJ>\AFGzOD9GI\LJ>\F5yJKQ\ISTOUo2JSTDLGJ>FQsT`bsSTU0v2]! ST\A\Aa%JdU]O|ODF]!kllbnQ]0M0F5DAF\AM0FJ>o0o0DO|JKMScG\AOsTFJ>DU\M0F5ae4StDFKQ\IsTV`|VF5ayobSTDIScK5JsvF5U0F5DQJsStJd\ISTOUO>C!JKQ\ISTOUGIF4_0F5U2KQFG9O`2GIF5D~FerCDOaJZbU0O>u9sTFebvFJ>`bsTFN\AFJKM0F5DW M2JdDLebOUGIM0O>uG\AM2J>\9JKQ\ISTOUGI\DLJ>\AF5v>STFGDAF5o0DAFGIF5Ub\AFeJG9GIV0GI\AF5a%GO>Czo2J>DLJ>aF5\F5DIST5Feo0DAO0eb_2KQ\ISTOUDA_bsTFG9u9ST\AM+fiq0j000j4q0+q42020002I%+20Q24TIT29>A9QRT5|{ttdA2>bTHTbQ> 9 2YTZA5b ITA0 wtffc>AA2>Nd fi IhcI0b t"AQTbQQ55|T>AbT0 |zcAT|IQT>00A" !bT%>AZA0ZA05 #N4QIT&IAQ>A LdA05NA2>AT>A"A0Z0IT%<IL>A%$H0T|5AIITQgd2 0ZAbc>00|cZA2> &0b TA0 0AA5bgQtbELQ5r5|ZT>bt5A000'(fi)2AIQ+*-,!>A ."/01 TAA>..29AA>2bQ*43NdQtbT5/bT2h02>AIcbc>N4 btI69TA0bT5%7h05AA0{L>AhcHbT2>Ict42I5 +dbt8:9 ;=<?>@<5ABDCFE.AHG>0bT0J9AKLNbc>0bT&b4%T2ZM2AA0%bT00N9IcO&<r05A!520AMbTLhAIc05bb2A0wN{2QH2>9T2 2bP2LILb5HQdIT2!0A4c5>A 05A>AZ%d|2T5 tM2AbT22 2Q50T>AbTA42! >bTT&Q2AIAR2Q0Lb%+t2hc'|0SHA2xT|LQL>bT6MS2YT5=UVbc>2A2>9Z24! 5200bQAtS}054T20>TbEA%dIT.qW 2! >bT$A0T>A05NZb tS9|ITL!SHZ0AbT5%ANA09A05b2>0L00! A52YTAN0<2dL4d5Z25QIbTXH05ITY>bt.&HV:fi 0b%5+Z5V"[zA52ZI 50T>AbTr5A02>!T>A2A0Lw\4bQ2bYTITbTE9bd0btrEA 2! >bTHd25Z25LIbT]XH05IT(9fi 0bb52Z&*^9fi 25S>T>T=H[V)bQ2bYTITbTg0LYQTbQ2bIN 4|TbA0g 5>HM0|+x>20ITZTIQIT2NTA0K_`acbdfe7g 0AHL>]A0ZT>bt{t0 05A9050A72P 2Zh505L>ITr>{5>2 |YTIH2! >bT=hS<45t00A0Hr5g 5LIbT6X05ITH2P 24TNbIjI2jP505Q\dIT%d{5j>$q! >rbtc=b0SHA2lk,=E2>Lm(S2Awt5"JHV 0nXH05ITS|AYTbT&l2P 2ffD505L+>{t4T2QA55bLT|c5{E4tch05A05TA5LT 0Q424t{t4 cAQT>>|A0Rbxbd! 52Abc9AA+\E{trb4%T27h05AgQ{t42>02b5A5ZTb{Ic>2bc>0bT>20! 0IT>AhtbA5IT"I0S<"A2>r0<4c%d0bc5>bTA0M0254cIffI5AITtpHbcAff0%>A0590AScbH{> 0IT29AZ0AbT5%q(rs> *ut|405v"[0xw2" O.yUV|59QA002M 025 4cII5ITH05AxA0I> 0IT2>A4505L>A|I>QA0AHL>zM1TL5Y H[0{tTbA.y0 b52+Y O|UVvk>LdLS }A>NVUQS 904THQ 0L{j25M 955A0IM <9 !0AA5~ 0A05$cM 0NM 00bTAhA0zT>A05!I5>Q>2bZT0|I5!05LcI ff]2IQ b<M 000A4bt5NA2>0QT&I> ZA0Z0A4bt5NA2>2> 0Qt05 0TA0b%+t.bc!c55Ar2YTZdHA0 !05LcI0Z |5P22>j052>xA !0j |bT69TA.$A 2Ibg >cT| L>IT4j 5A5bIc XH2>IT2~ 0NT>A05hcH XbTAAQ5 xT2>Tb0Zc<{tr%>29TA"2>Tb0Zc<0Z25x>< !05LcI k>Q>LS }A>VU0AS 92>HA0ZQ24TIT2Q bRT5|E9T>AbTrI> 0AbT5%N<tAbcH2d25N>AbRT5bHET>AbTEA !5Q{N5<QY$H09>$A0HA2+T5>v02Q 025 4cI%{250rtdAbTNczA0bTITbthT%S 90A4bt5KLM 025R cII 50T>btx92M 2TAQ 0NA2>H0NA05 #HId\ 0{t42>jQYcIA5b)9TAZYT>T0A4bt5I> 59TA0|b24A0YcHI2Q 0NAI4%A2d<b<hQ 2ccNA2dN 5A0bT5T0%b%T2" rbTITbTI> 0IT2[tS V Wq !q>rbtA05A>A72M 2+T>|ZAH0A!A4AN0 #'!>RQA4 0 b>2ZA05H%Z2H%d|H>)P!btbcQT LA0Rd2I52Q>A4A05H052M 0AZA2dg<0RId\ 0{t42Z>HA0!2>bT>%Q2YcIA5b9TA&wtdt0AbT5I> 5" $A0ZT>A05x2NAbcb05AxL>R 50bT5I> 5gQ42YcIA5b9TA> 5&I5N><{> 0IT2NN04 L"A05<4L02AIQTQI> 0IT2x>H0A4bt5TQ 2A6HA2dg>TIZ0rL> 5N0bT5 {> 5{fiOH)-+SS" 7hV" VR.M xMH%QS K lMffS \H= ] lY@J" j 6& J= H6MR"MR=Sff\cMM"ff@j M)7{ SNR]"VS l"j S+KN SRj+SjfSfJV@\) lS6RS7RSqQSHMxH M{Z&7j 6'\ R xMRSJ{\ RVSSM KHxVS7S)OH S+RMK"6SD pV"7 FH" . SM "qHMV" Sff"H\MS h7R S7MhJ6R\S5hSMSM ]Hj5YS))"7KJM@\Q\HH"j\M ]HS" =YKSH"j J )MOvRj :@]HV"j SM @?S \hHjKM MS KH\K7j NH 6M "QS h=SS.:"xq@^SHM.&{7@qS.+hRKM@\UMS DMHS & lMMzM@ RVS:j5)R\SS 7@fj ff\6RJMHM]Q ffV@ )ffHqH7lSH MS @S6QS ]S:& (SjSN \ @R DMRMpMD HSff\K SS" "M VM "M@\Q\H]" \D ~7ScMRVDMR MS ffHK)"cH6RMjvR @J]Q\Q\FQS 5S j@M SR7S7" 7"U?V"j )-j 6M@\Q\H)""HMjvMS H"j MY"j l QUMRS)@7RVSK\S6M {.ZHM .=.5Y5{.fl5M7j "VS )MM SR\xf7=)RJV"OHNS=) H]=@\" SR\5(x7& )h '7MV@SVQ&Svxf JRq? @M S)5 S==H6" YMxRv@M c D'S7"SSV~O7JQlM" lx7MHSR FMS V~ Sff\h"M)J@\+\6H7OVMSHHQM SQ\H S= ?7 xVSHV.HfRS 5S)ff OS]QO R\@q"@(\HMSM@\Q\H7)RR K RVSff@+Sh7HJ\R@+j. %SSM HM Oxc xS M"&\@ ]SRM@HllMS)p{zRnRS KS )SxlMcM xl " Q\RSSlHj.'HhS+S=S) .Ql@ SKHxVSfRUR lVS )RD6Mx S6SR6M H jSMM~f)6x]l"]Q{hc6M j@M (c@'"jM6 RUpYp)7SNUff5)JRJ:S " f \RMR)MVSM 5S" zMS H\xqS RFVRMS ]%\ffSJ{SfMS SJ"Mff+HVM=f]x ff)KVM]@ff6QRpl@M "R\cHSM ]Sx( YY fiS6R)6@+"{{)MSV@xq fiSfi fi5lfi"!#"$% "#"!&'(")+*+, - & &.0/"-210& #""3"453067980:;"<>=@? ACBDAE? 798 F GIHJ: K9: LNM80:O5:K9:;QPF = ?ASRTPVUW7X79UWHYP[Z98 F]\^HXUW;`_a79bJPc? ;QPF deP[ZX<gf@O[:;"; F]h HYP[LQ80:H:80: LQUWj:; Fh 79UWj U Pc;Q79? ; F^=@? Pc7kZX8"UJOlOFnme:H9;"UWopG]UJOqOF^BH9Ar:;0LsGIHYPcULPc7YP[KWF:;0LtM80:u;0LQHk:v UL"LQoxwy? H%Ar:;QozPc;7XUWH9UKY7YPc;"{pLP[K9ZJ|0K9K}Pc? ;0K%?;~7X8QP5KE79? QP[Z=e80:;"<"KE79?+sPqOlOqP[:A M? 8"UW; F v ? ;QP80:HkLQ?; Fh 79UWj U Pc;Q79? ; F^:;0Lx798"U>H9UWjPcUWeUWHkK?uw(798QP[K%0:UWHw? H7X8"UJPVH7X8"? H9? |"{ 8p:u;0L8"UJOcQwy|QOZJ? AAEUW;7kKF:u;0L79?GI: LQAr:u:B<<:Hk:u|Ewy?H^8"UWHDZW:H9UJw|QO"H9?Q?wH9U: LPc;"{02"900"B;"{Oc|QPV; F Rk Qka^|"UWHYPcUKn:;0LZJ? ;0ZJUW"7TOcU:H9;QPc;"{0Tg [J J0F" kF0""B;798"?;o F cF0maPc{ {KFdk Qk TQ 0 @ kk[Q2 J JM:A"HYP[LQ{ UT;QPcj UWHkK}Pc7YoGIH9UK9KFdTUWI? HX<F0dTmaOc|"AUWHF]B5F _I8"H9UW;QwUW|0Z98Q7F@BcF :|0K9KOVUWHFR5F z:H9A|"798 F ]k QkU:uH9;0:QPqOqPV7Yo:u;0L2798"U:";QPc<QYM8"UWH9j ?;"UW;"< P[KnLPcAUW;0K}Pc? ; TJ0 ]YnagF"kF0 "mo O[:;0LQUWHF=n@k QkM? AEQOVUJQPc7Yo>H9UKY|QOc7kKTw? HnKYUWHYP[:OLQUZJ? A?K9:QPqOqPc7o ;zekJJ [QnY(u[ IWWYkn nJ klq 0JF" @ 0h:;>?KYU FMBBDBBDeGIH9UKXKWM80:O[: KX:;QPFG]cF_I7XbJPV?;QPFT\cF ? |";Q7FI"k "uk>RUW79UZJ7YPc;"{:u;0L+UJ"QOc?Pc7YPc;"{LQUZJ? AE0?K9:uQPlOcPV7Yo2Pc;z|"L":7XU{ Hk:"80KW;2k0k 0 e WWYk% zek0qrY elJY0 N 0 JW 0F " "M:A"HYP[LQ{ U F BM? 8"UW; FQk QkDK}Pc;"{rLP[KY7XHYPc"|"7YPc? ;"wyHXUWUnOcU:H9;QPc;"{798"UW?H9o79?:;0:OcobWUK?Oc|"7YPc? ;0:798ZW: ZX8QPV;"{AUZ980:u;QP5KArKWn^ J0qlkF0kF" "RU? ;"{F cF0 ?Q? ;"UWo F v J QJ_]"QO[:;0:7YPc? ;20: KYULOcU:H9;QPc;"{0eB;+:Oc79UWH9;0:7PVjUjPVUWnguW0J J0FF "_:uHYOcUWo F"]kQQkB;+UJZXPcUW;Q7ZJ? ;Q79UJ"79wyHXUWU0:uHkK}Pc;"{>:Oc{ ? HYPc798"ANc> Q0J ["IgFYQkF" "_I7XbJPV?;QPF\ J QJB~KY79HX|0ZJ79|"Hk:O7X8"UW? H9on?w0UJ"QO[:;0:7YPc? ;"0: KULOcU:H9;QPc;"{00nJ k0[qq 0JFJkF" "(:uH9UWo F 5Fu? 8";0KY? ; F0RkQQkT TQX 0Ek[quC^Q[ N Qk JYDk Tq0kW "f"H9UWUWA%:;DHJ:7kZX8 F"cFpRU?;"{0FQJ QJIu"]B"H9? 0:QPqOqP[KY7YP[Z^KY?uOV|"7PV?;7X?^798"U|"7PlOqPc7Yo"H9?QOVUWAPV;>K0UWULQ|""OcU:H9;QPc;"{0e};+YkJk[Q^Ynn [0 WWYk nJ e k0[qq 0JF" 0 ""h:;>?KYU FMBBDBBD^GaH9UK9KWDHXUJPV;"UWHF v Qk "uk0f@Pc;0LPc;"{7X8"U^? "7YPcAr:OLQUWHPVj:7YPc? ;>KY7XHk:79UW{ oPc;:nH9ULQ|";0L":u;7<;"?eOcULQ{ U^0:KYUnk [0J0qlkF0 JkF0 "DHXUJPV;"UWHF v cF t|"HP5KP5ZW:FJ"k QkB~KY7J:7YP[KY7YP[ZW:O:""HX?: ZX87X?(KY?uOVjPc;"{798"U_me%|"7YPqOqPV7Yo"HX? QOcUWA>;eYJkJ ]Yn 0 Y0J(k [ uk0[qq kFJ" 0J""h:;?QKYU FMBQBBDB^GIH9UKXKWfi"0W fffi" 9!#"$%'&)(*%+,-!.0/1+2+143065,)78+9)8+0:<;=>?@4&)@:ff';BA:ffC@DC&4A(E6FG9AHIJ):ffKLM8+IN:AO:ff):AK(PQ(8ffLRTS"!U5PWVYX=Z[\0\]+^`_'a<bcZ*de0_$fg\X_)h+fg^`Z+_$h+ijZ+^`_)f![Z+_d\X=\_$[0\kZ+_mlnX0fg^ oY[^Qhie0_)fg\iQip^pa+\_$[0\ff99Urqs+s<t)qs'14uk;CC8ff;wvx5vx8C>':ffzyn:ff&4LgIN:ffU&9;{):Y|B!}|k:ff&UYuJT.0/1+1+~430;C{x78+I94A4;K8<L4A87C%(J8+=AQ@94AQ:ff4>)lnX0fg^ oY[^Qhie0_)fg\iQip^pa+\_$[0\)+.~ff,430~+~t~+ffs$:ff&)(C(PA)uJ$.0/1+2+1430r"r:ff4>z78+ff=&)7;=+78+)79;0(Yx(*;CC&)7;C&0:A@48IN:)(T?h+['^`_)\nU\h+X_^Q_4a+rqt4s'8+9O7C8<LM;n-ApAIN:ffUB-n.0/14q1430e0_)fgX=Z]'[fg^QZ+_fgZl'fgZ+Jh+fgh U4\0Z+XUh+_'a'ha+\bNh+_$]Z+O4fgh+fg^QZ_)!Fn@@Q(=8+g(PAK+yn:ffA4Ap>)"!+"O;;CIN:ffU+vff"!+v8'8++FJ.0/1+1430+4Lg8+07I4;!A:ffC4>)EF(=&+K+jZ+4X_)h+iUZ*dlnX0fg^ oY[^Qhire0_)fg\iQip^pa+\_)[\\b\0h+X=[')~+4qt~+2+yk:ffC)(v-:ff0:ff4,'BJff.0/11ffs30l_e_)fgX=Z]+4[fg^`Z+_JfgZJZ+O4fgh+fg^`Z+_)hi4\h+X_)^`_'aU'\Z+X0ff{v50 C(C(Y:ffIJ=Q@4>++vFyk{):ff0@48Un.0/1+1+30"r:ff4>W;C8z;:ff%+:+7;=8+)(J5PVYX=Z[0\\0]^Q_4affbZ=dnh+fg^`Z+_)hiZ+_d\X=\_)[\Z_lnX0fg^ oY[^Qhi)e_)fg\i`ip^Ha\_$[0\+8+C;=AQ:ff)@rryk8+=LCrn.0/1+2430v?:7C8+g8+9$0:ff;C8+(EY:6:ff%zI;C{8@mLg8+A:ffC4>)lnXfg^ oY[^`h+ire_)fg\i`ip^Ha\_$[0\O4ff+t)q+q4"U:0@r4-+84(='4A848+Ix++|YApA,+FJ.0/1+2+30{4&%>kN(*8':ffE {:ff):<;C8+IJK8<LO:>+:AA:<C4>NI7{):ff4Q(=Ixc?h+['^Q_$\B\h+X_)^`_'aOffU//t's'"$;C;=A(=;8++|B.0/1+22430":ffC4>?&4Q7C%AK{CCA:ff4;:<;C;C=&;C(n:ff$8+&)@rEkFAH:ff;C{C(={8ffAQ@:A>+8+=;C{IxTWh+['^`_)\BU\h+X0_$^Q_4a+$+)~+2+t/2"$;C;CIm:ffU)v"!$Y:+(C(:ff)@40:4FJyn:ffA4AH>)U"))./1+1+43"r:ff4>69)8ffApQ7(9):ffC;=Q:ApAK8+)(=Cff:ff4Ax4+C8+I6';0(Ex'7:AH>&9U5,VX=Z[\0\]+^`_'affbmZ=df`4\xe_)fg\X_)hfg^QZ+_$h+i!Wh+[4^Q_$\\h+X_)^`_'amZ_d\X=\_$[0\+99U)+~t4q'v8+C>':<yn:ff&4LgIN:ffUv';C8U4)4.0/1+1+30&):ff4;=;0:ff;*Y(=&4A;0(78+)74>B;C{&;=pAp;=KJ8ffLr94AQ:ff):ff;*8g):+(=@BA:ffC4>)lnX0fg^ oY[^Qhi)e_)fg\i`ip^Ha\_$[0\+U.=~ff,430)+t+1/+v;07C{ApA, UykApAnUyk@:ff=Y:ff)ApAp,).0/1+2+30R94AQ:ff):ff;*8?):(=@W>+0:AH:ff;*8UEF&4HLgK>mBWh+['^`_)\BU\h+X0_$^Q_4a+rffsqt2+v;07C{ApA, ';C>+8ff'4S:ff,,n$.0/1+2430r"r:ffC4>4KJ9$=I';:ff;=8+UE!Fn7+&4=>z:ff)@C)4>9C8+4AI(=8<A>W{&=Q(=;=Q7(5Pv7{):AQ(=%,Yn:ffC)8AHA,-v;07C{ApA,.R@(30$?h+['^Q_$\nU\h+X0_$^Q_4a+99UU/+tr/1+ 8+>':)!:A86FA;C8)F|k:ff;0:ff0:<*:<USk./1+24q+3A:ffC4>ST848ffA:ffLg&)7;=8+)(5,VYX=Z[\\0]+^`_'a<bZ=dxf`'\/1<QWlJ!OffZffb^Q4#Z+_U4\0Z+XZ=dTZ+O'fg^`_'a99Ur~+1+tffs)ffFnYvC(C(|k:ff;0:ff0:<*:<U'Sk$.0/1+2+130!mA:ffC4>mLgC8+I07Q(=(!5PVYX=Z[0\\0]^Q_4affbZ=df`'\~<l_)_)4h+izZ+Xb0'ZZ+_Zr'fgh+fg^`Z+_$h+iUU\h+X_)^`_'aU'\Z+Xff99Urq~t24q'$':ff4;0:6C&+rFJfimU+++)W)+)fffi!#""$' $%'+& )'( ff+n* -,. N/ ffU'0 ff% 213& 5476kff0ff0<*<U+98;:<13=)?>@>@ACB$0DD0:#E#&513EF.g0-/+13E#&+2G5H. &+7>I1ffCA5()J,LK#-NOPRQff? ff!$3 ffRfiTSU3P3?ff=5U= WVXY?Z$V [ k6 \k6 2]+& ff%'0JB1213+^ % CJ6 C8`!_ )^Aa& 7b 0+4Y 0Rc 'd )-/eAgf-hiG6 513E )&->I1j.k& nC0?A 5(\= 2&]>a13/TH lT_ #!m,J nK#o?M-NpPqrQff ff!UL3 ffrfiLSUPs?o?ff5=5U= tuv3Z9tv-Y ff6 ]5w1 <131 UC0f+& Q> ffffr< C%'+& 2'( <xn* -,. N/ ffUyff51 5Jc 8{z ff2/j, 2U|% 0D4u 0C}~A ff<A 5m( H|5+& 21=H H&->I, AI+& T.k& 2|51p`+1 21 ffHoAa& q&-.2|5z1 [-w =5,5^3^>I1+AHA 4C0f 0]>a+1 jPJ nK?M-NpPqr !#SUPs?ffjLp ?!Qff?!!@@-=5U= UuDOZ -vY B|A@Q> <1>I=5|QA BJ6 ff%'+& 2'( ffn* -,. N/ ffUy#1<5<+d Ck4 : <13)= ?>@>@,A $B 8y#& fff -(->@AI&->I)& C)0 $0+4u 0ff:#|513& 2dw (,Ag<1<T13/r=A Agf >H=ff131<,5=T>I1 ff5w5(R&-.('& ?>ff<1f&/+=&HA AI+& m2,>I1H C,J 'K?M-NPQff ff!4fiffSU3P3?ff+ff,A ,J 0?>I+dy7AI1=H ffny 0Dv 0mC1 <CA 5(x<1f)AHAI+& '>@A=H H L3fiff + U 4t 05Y+Y ?Z5Y-Vuy#,ffH2H1>,> R)0 #8!B ffy 0+4[ 0F6 =5= 2&?AIN/ ffA 5(&= AIN/ >=&->@Agf)AI1Hx.k+& T)= ffCA >>IdF&]ffH1 2ff -]>I1=H 2&5f2)| *H AfO<&N/ ?A ffH CPJ \K#-N#PiQff? ff!5 ff7SUPs?o?ff+p ?!Qff?!!@@-=5U= UXDD?OZ +X )V %'+& '21 ?,> C4UJ60)| ?>@AIrG 9 04X 06pf3,A 5J( 21f, HAIn1 ffff<RA 21 ffAI1f+& fff13= HE#A 2|+15=Q> ff)ffAI+& 5w )] H1<j>I1 ff5w5$( ffRfiff $ff +t ?ZffvX0,5] 0-m/ ffQA ffU$Jc ff8, ')1 C)0 0+Y 0%\1 H, 5(q, A>@A ?d ffff< 2|51<1HoAa( &.= 2&-ff -]>Id(&&5<_ #?>I(& 2|5/qH jPJ K#o?M-NP+TQffff !3 ffrfiff 'SU3P3=5U= 5VYu?ZVt[ 6]1 <131 U0f& Q> ffffr< $%'+& )'( ffn* -,.km/ ffU:<13)= ?>@>@A #B T0+40T6. +& 2N/ ?>@AI^ ffAI+& &-.O15=> <)ffAI+& 5w )] H1<N/ f 2&w &=ff1 0<2+& >I1 ffCA 5)( LPJKNPjQff !C5O3P3j'p ?!Q! !@ff=5U= u ?u?Z9uYY0d94< 513+d 6,ff*H C0?>@QA ff%'+& 24( ffxn* ,.kN/ <U:<13)= ?>@>@A B 4+3r] 9m1 ffCA 5(jE#A 2|jA ffH2f 2, 0]>ak1 2|513+& Aa1H ffPJ KNPOfi5)NO" =5U= ff[-VV-Z9[-VD 54U|Af -($& 5Jo!m C%'+& 2'( <xn* -,. N/ ffU:<13)= ?>@>@A B 4+4Y6 2|513& 2d&-.C, ffH,5=ff1 )AH1<H=131<,5=+>I1 ffA 5$( C,J 'KNUPOp ff!SU3P3?ffpp ?!ffQff! !@-=5U= $YY ?Z9Yt-$V 5'0 ff\-&H+1 C4UJ6 56 66 JO!B 21H2H:-/j]ff+1 5% '13E#1>,> 5J6 58;y#&H1 ]>I&&/ ffB ff)0 $0+4X 0C:#|51 = 2&]>I13/`&-.19=1 ffHAI1f2|, 5G5H ffff<HOH&->I, AI+& ']md 21=H CAgf 5(T15= 21H2HAI1 51H2H L3 ffRfi+ ff o4t 0+Y +?Z9tVD?>@QA ff4ff!m rD-V C6 2|513& 2d+&-!. 2|51 >I1 ffC)-]>I+1 pS7jff 5NpPpj!S Wffff 0+ffOt-V-Z+3Vkff0ff0<*<Uk+ffr?< )fiJournal Artificial Intelligence Research 4 (1996) 77-90Submitted 10/95; published 3/96Improved Use Continuous Attributes C4.5J. R. QuinlanBasser Department Computer ScienceUniversity Sydney, Sydney Australia 2006quinlan@cs.su.oz.auAbstractreported weakness C4.5 domains continuous attributes addressedmodifying formation evaluation tests continuous attributes. MDL-inspiredpenalty applied tests, eliminating consideration alteringrelative desirability tests. Empirical trials show modifications leadsmaller decision trees higher predictive accuracies. Results also confirm newversion C4.5 incorporating changes superior recent approaches use globaldiscretization construct small trees multi-interval splits.1. Introductionempirical learning systems given set pre-classified cases, describedvector attribute values, construct mapping attribute valuesclasses. attributes used describe cases grouped continuous attributes,whose values numeric, discrete attributes unordered nominal values.example, description person might include weight kilograms, value73.5, color eyes whose value one `brown', `blue', etc.C4.5 (Quinlan, 1993) one system learns decision-tree classifiers. Severalauthors recently noted C4.5's performance weaker domains preponderance continuous attributes learning tasks mainly discrete attributes.example, Auer, Holte, Maass (1995) describe T2, system searches goodtwo-level decision trees, comment:\The accuracy T2's trees rivalled surpassed C4.5's 8 [15] datasets,including one datasets continuous attributes."Discussing effect replacing continuous attributes discrete attributes, whosevalues corresponds interval continuous attribute, Dougherty, Kohavi, Sahami (1995) write:\C4.5's performance significantly improved two datasets : : : usingentropy discretization method significantly degrade dataset.: : : conjecture C4.5 induction algorithm taking full advantagepossible local discretization."paper explores new version C4.5 changes relative desirability usingcontinuous attributes. Section 2 sketches current system, following sectiondescribes modifications. Results comprehensive set trials, reported Section 4,show modifications lead trees smaller accurate. Section 5c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiQuinlancompares performance new version results obtained two alternativemethods exploiting continuous attributes quoted above.2. Constructing Decision TreesC4.5 uses divide-and-conquer approach growing decision trees pioneeredHunt co-workers (Hunt, Marin, & Stone, 1966). brief descriptionmethod given here; see Quinlan (1993) complete treatment.following algorithm generates decision tree set cases:satisfies stopping criterion, tree leaf associatedfrequent class D. One reason stopping contains casesclass, criteria also formulated (see below).test mutually exclusive outcomes T1; T2 ; : : : ; Tk used partitionsubsets D1 ; D2 ; : : : ; Dk , Di contains cases outcome Ti .tree test root one subtree outcome Ticonstructed applying procedure recursively cases Di .Provided cases identical attribute values belong differentclasses, test produces non-trivial partition eventually lead singleclass subsets above. However, expectation smaller trees preferable (beingeasier understand often accurate predictors), family possible tests examined one chosen maximize value splitting criterion. defaulttests considered C4.5 are:A=? discrete attribute A, one outcome value A.continuous attribute A, two outcomes, true false. findthreshold maximizes splitting criterion, cases sortedvalues attribute give ordered distinct values v1 ,v2 ,: : : ,vN . Every pair adjacent values suggests potential threshold = (vi + vi+1 )=2 correspondingpartition D.1 threshold yields best value splitting criterionselected.default splitting criterion used C4.5 gain ratio, information-based measuretakes account different numbers (and different probabilities) test outcomes. LetC denote number classes p(D; j ) proportion cases belongj th class. residual uncertainty class case belongsexpressedCXInfo(D) = , p(D; j ) log2 (p(D; j ))j=11. Fayyad Irani (1992) prove that, convex splitting criteria information gain,necessary examine thresholds. cases value vi adjacent value vi+1 belongclass, threshold cannot lead partition maximum valuecriterion.78fiImproved Use Continuous Attributes C4.5corresponding information gained test k outcomesGain(D; ) = Info (D) ,k jD jXInfo (D ) :i=1jDjinformation gained test strongly affected number outcomesmaximal one case subset Di . hand, potentialinformation obtained partitioning set cases based knowing subset Dicase falls; split informationk jD jXlog jDi jSplit(D; ) = ,2 jD jjDji=1tends increase number outcomes test. gain ratio criterion assessesdesirability test ratio information gain split information.gain ratio every possible test determined and, among least average gain,split maximum gain ratio selected.situations, every possible test splits subsets classdistribution. tests zero gain, C4.5 uses additional stoppingcriterion.recursive partitioning strategy results trees consistenttraining data, possible. practical applications data often noisy { attributevalues incorrectly recorded cases misclassified. Noise leads overly complextrees attempt account anomalies. systems prune initial tree,identifying subtrees contribute little predictive accuracy replacingleaf.3. Modified Assessment Continuous Attributesreturn selection threshold continuous attribute A.N distinct values set cases D, N , 1 thresholds couldused test A. threshold gives unique subsets D1 D2 valuesplitting criterion function threshold. ability choose thresholdmaximize value gives continuous attribute advantage discreteattribute (which similar parameter adjusts partition D), alsocontinuous attributes fewer distinct values D. is, choicetest biased towards continuous attributes numerous distinct values.paper proposes correction bias consists two modifications C4.5.first these, inspired Minimum Description Length principle (Rissanen, 1983),adjusts apparent information gain test continuous attribute. Discussionchange prefaced brief introduction MDL.Following Quinlan Rivest (1989), let sender receiver possess orderedlist cases training data showing case's attribute values. sender alsoknows class case belongs must transmit informationreceiver. first encodes sends theory classify cases. Sincetheory might imperfect, sender must also identify exceptions theory79fiQuinlanoccur training cases state classes predicted theorycorrected. total length transmission thus number bits requiredencode theory (the theory cost) plus bits needed identify correctexceptions (the exceptions cost). sender may choice among several alternativetheories, simple leaving many errors corrected otherselaborate accurate. MDL principle may stated as: Choose theoryminimizes sum theory exceptions costs.MDL thus provides framework trading complexity theoryaccuracy training data D. exceptions cost associated set casesasymptotically equivalent jDj Info (D), jDj Gain (D; ) measures reductionexceptions cost partitioned test . Partitioning way, however,requires transmission complex theory includes definition . Whereastest A=? discrete attribute specified nominating attribute involved,test must also include threshold t; N , 1 possible thresholds A,take additional log2 (N , 1) bits.2 first modification \charge" increasedcost associated test continuous attribute apparent gain achievedtest, reducing (per-case) information Gain (D; ) log2 (N , 1)/jDj.test continuous attribute numerous distinct values less likelymaximum value splitting criterion among family possible tests,less likely selected. Further, thresholds continuous attributeadjusted gain less zero, attribute effectively ruled out.consequences first change thus re-ranking potential tests possibleexclusion them.second change much straightforward. Recall gain ratio criteriondivides apparent gain information available split. latter variesfunction threshold maximal many casesbelow. gain ratio criterion used select t, effect penalty describedalso vary t, least impact divides cases equally. seemsunnecessary complication, threshold chosen instead maximize gain.threshold chosen, however, final selection attribute usedtest still made basis gain ratio criterion using adjusted gain.4. Empirical Evaluationeffects changes assessed empirically series \before after"experiments substantial number learning tasks. Release 7 C4.5 (abbreviatedRel 7) modified produce new version (Rel 8). systemsapplied twenty databases UCI Repository involve continuous attributes,either alone combination discrete attributes. summary characteristicsdata sets appears Appendix A. following experiments, versions2. Even convex splitting criterion satisfies requirements Fayyad Irani (1992),cannot use number N potentially gain-maximizing thresholds instead greater number Npossible thresholds. Since receiver knows cases' attribute values classes,cannot determine whether cases two adjacent values belong class.message must consequently identify chosen threshold among possible thresholds.080fiImproved Use Continuous Attributes C4.5Table 1: Results using modified (Rel 8 ) previous (Rel 7 ) C4.5.annealautobreast-wcoliccredit-acredit-gdiabetesglassheart-cheart-hhepatitishypoirislaborlettersegmentsicksonarvehiclewaveformaverageRel 87.67 .1217.7 .55.26 .1915.0 .214.7 .228.4 .325.4 .332.5 .823.0 .521.5 .220.4 .6.48 .014.80 .1719.1 1.012.0 .03.21 .081.34 .0325.6 .727.1 .427.3 .3Error RateTree SizeRel 7w/d/l ratio Rel 8Rel 77.49 .16 3/2/5 1.02 75.2 .7 70.1 1.123.8 .6 10/0/0 .74 63.7 .4 62.9 .55.29 .09 5/1/4 .99 25.0 .5 20.3 .515.1 .45/2/3 .99 9.7 .2 20.0 .515.8 .37/1/2 .93 33.2 1.1 57.3 1.228.9 .35/1/4 .98 124 2155 228.3 .3 10/0/0 .90 44.0 1.6 128.2 1.832.1 .54/1/5 1.01 45.7 .4 51.3 .424.9 .48/0/2 .92 39.9 .4 45.3 .321.6 .54/0/6 1.00 19.1 .6 29.7 1.221.7 .86/1/3 .94 17.8 .3 15.5 .4.49 .02 6/3/1 .97 27.5 .1 25.3 .14.87 .20 3/3/4 .99 8.5 .09.3 .116.7 .91/2/7 1.15 7.0 .37.3 .112.2 .0 10/0/0 .98 2328 4 2370 43.77 .07 9/1/0 .85 82.9 .5 83.5 .61.29 .03 2/1/7 1.04 50.8 .5 51.5 .528.4 .68/0/2 .90 28.4 .2 33.1 .529.1 .3 10/0/0 .93 135 2181 128.1 .66/2/2 .97 44.6 .4 49.2 .4.96ratio1.071.011.23.49.58.80.34.89.88.641.151.09.91.96.98.99.99.86.75.91.88C4.5 run default settings parameters; attempt madetune either system tasks.4.1 Initial experimentsTable 1 displays results first trials, consisting ten complete ten-fold crossvalidations3 task. figure shown system mean tencross-validation results error rates tree sizes refer C4.5's pruned trees;standard error mean appears small font. column headed `w/d/l' showsnumber complete cross-validations Rel 8 gives lower error rate, errorrate, higher error rate Rel 7. figures `ratio' present results Rel 8divided corresponding figure Rel 7.overall averages foot table indicate, trees produced Rel 8trials 4% accurate 12% smaller generated Rel 7. Rel 83. ten-fold cross-validation performed dividing data ten blocks cases similarsize class distribution. block turn, decision tree constructed remaining nineblocks tested unseen cases hold-out block.81fiQuinlanless accurate Rel 7 four twenty tasks; smallest data set (labor,57 cases), however, trees produced Rel 8 substantially less accurate.pruned trees generated Rel 8 tasks great deal smaller Rel 7counterparts { diabetes particularly notable example.recommend use unpruned trees constructed initially C4.5 but,sake completeness, corresponding figures unpruned trees alsodetermined. average ratio error rate Rel 8 Rel 7 0.95,ratio tree size 0.94. unpruned trees, then, modifications incorporatedRel 8 lead 5% reduction error 6% reduction size.4.2 Adding irrelevant attributespractical applications, unlikely analyst would knowingly add irrelevantattributes data! However, even attribute relevant partstree might quite irrelevant others. bias towards continuous attributes inherentRel 7 implies system occasionally select test irrelevant continuousattribute preference tests relevant discrete attributes.explore potential deficiency, twenty data sets modified adding irrelevant attributes. Ten continuous attributes, uniformly distributedrandom values x, 0 x < 1. (Since order values continuous attributeimportant, distribution values matter { use another distributionGaussian N (0; 1) produce comparable results.) Kohavi (personalcommunication, 1995) points out, unfair compare Rel 8 Rel 7 data setsirrelevant continuous-valued attributes added, since modificationsincorporated Rel 8 make less likely choose tests involving continuous attributes.circumvent problem, ten discrete attributes ten equiprobable valuesadded, giving twenty irrelevant attributes all. experiments repeatedenlarged data sets, results shown Table 2.results highlight effects modifications implemented Rel 8. Additionirrelevant attributes increases error Rel 7 trees average 12%,much smaller impact produced Rel 8. head-to-head comparisonaltered data sets, presented table, shows pruned trees found Rel 810% lower error average, also great deal smaller. split randomcontinuous attribute unlikely generate sucient gain \pay for" threshold,tests tend filtered Rel 8 Rel 7. Consequently, Rel 7prone split data (uselessly) random attribute, leading larger treeshigher error rates.4.3 Ablation experimentseffects modifications implemented Rel 8 factored choosing (slightly)different thresholds using gain rather gain ratio, excluding attributesthreshold gives sucient gain offset penalty, re-ranking potential tests penalizing involve continuous attributes. ascertain contributions each, twointermediate versions C4.5 constructed:82fiImproved Use Continuous Attributes C4.5Table 2: Results addition irrelevant attributes.Rel 8anneal+7.72 .23auto+18.7 .5breast-w+ 5.69 .11colic+15.1 .2credit-a+ 13.6 .3credit-g+ 28.5 .3diabetes+ 26.9 .3glass+37.0 .5heart-c+22.6 .7heart-h+ 20.3 .4hepatitis+ 19.1 .6hypo+.47 .02iris+5.67 .15labor+19.1 .8letter+12.7 .1segment+ 3.91 .09sick+1.61 .05sonar+25.5 .8vehicle+28.7 .3waveform+ 30.1 .7averageError RateRel 7w/d/l8.13 .18 9/0/126.0 .7 10/0/06.17 .13 8/0/220.1 .3 10/0/016.4 .3 10/0/032.4 .4 10/0/030.3 .5 10/0/035.9 .83/0/730.3 .4 10/0/024.9 .5 10/0/023.9 .7 10/0/0.49 .02 7/2/15.73 .41 4/0/624.6 .79/1/013.3 .1 10/0/03.85 .05 4/0/61.57 .05 4/1/529.3 .69/1/028.8 .25/3/228.0 .64/0/6ratio.95.72.92.75.83.88.891.03.75.82.80.96.99.78.951.011.02.87.991.08.90Tree SizeRel 8Rel 7 ratio74.3 1.1 84.0 1.6 .8863.4 .7 62.3 .6 1.0216.8 .4 25.0 .4.678.9 .2 39.9 1.1 .2234.7 .7 58.4 .9.60111 3174 2.6443.6 2.1 115.5 1.8 .3831.0 .6 46.2 .9.6724.9 .8 52.0 .5.4819.9 .5 32.0 .9.625.6 .4 20.4 .6.2827.8 .2 25.9 .2 1.087.6 .19.7 .1.796.8 .2 10.6 .1.642300 3 2372 6.9769.2 .6 88.2 .6.7837.1 .8 54.8 .7.6820.1 .4 34.0 .5.59109 1162 1.6727.9 1.0 48.9 .5.57.667G differs Rel 7 threshold chosen maximize informationgain rather gain ratio;7GS also chooses thresholds gain; gain best threshold lesspenalty log2 (N , 1)/jDj, however, test excluded.difference 7GS Rel 8 latter's application penaltydetermining relative desirability possible tests.trials repeated using cross-validation blocks intermediate versions. Average error rates, tree sizes, ratios (again computed respectRel 7) presented Table 3 summarized graphically Figure 1.Selection thresholds gain rather gain ratio (7G) little impact {average error rate tree size ratios respect Rel 7 close one.non-trivial changes tasks, however; instance, error ratesegment data considerably lower trees found breast-w task noticeablylarger.83fiQuinlanTable 3: Results intermediate systems 7G 7GS.7Ganneal7.73 .16auto23.0 .9breast-w 5.21 .23colic15.0 .4credit-a 14.7 .3credit-g 29.7 .3diabetes 27.1 .4glass30.9 .5heart-c25.0 .4heart-h 22.2 .4hepatitis 22.0 .8hypo.49 .02iris4.93 .23labor18.8 1.1letter12.3 .0segment 3.39 .08sick1.34 .02sonar28.8 1.2vehicle28.1 .2waveform 28.1 .8averageError Rateratio 7GS1.03 7.62 .16.97 22.7 .8.98 5.32 .17.99 15.0 .3.93 14.1 .21.03 29.1 .2.96 25.2 .3.96 31.3 .71.01 23.8 .51.03 20.9 .31.01 21.4 .61.00 .50 .011.01 4.80 .171.13 19.5 1.01.00 12.2 .0.90 3.36 .071.04 1.34 .021.02 26.6 1.1.97 27.6 .31.00 27.3 .61.00ratio1.02.951.01.99.891.01.89.97.96.97.981.02.991.171.00.891.04.94.95.97.987G73.9 .759.5 .924.4 .320.2 .550.1 1.0148 2127 250.3 .344.5 .830.2 1.117.3 .425.7 .28.5 .17.8 .12327 383.7 .350.4 .332.6 .4178 146.8 .4Tree Sizeratio7GS1.05 73.4 .7.95 59.0 .91.21 24.1 .51.01 17.9 .6.88 38.3 1.0.96 138 2.99 45.4 2.0.98 46.2 .6.98 42.2 .91.02 19.3 .71.12 15.2 .41.02 27.0 .1.92 8.5 .01.06 7.6 .1.98 2330 31.00 82.8 .3.98 51.4 .3.98 28.5 .3.99 145 2.95 44.4 .61.00Error Rateratio1.05.941.19.90.67.89.35.90.93.65.981.07.921.04.98.991.00.86.80.90.90Tree SizeRel 77G7GSRel 8.9.951.0.8Figure 1: Summary ablation results.84.91.0fiImproved Use Continuous Attributes C4.5Use penalty filter tests continuous attributes (7GS) produces noticeable differences. Ruling tests continuous attributes accountsreduction tree size observed Rel 8. cases, trees markedly smaller{ diabetes data, 7GS trees average one-third sizeproduced Rel 7. change also accounts half Rel 8's improvementerror rate, diabetes data providing greatest change 7G.Finally, use Rel 8 penalty re-rank attributes yields improvement error rate small decrease average tree size. re-ranking maybeneficial even attributes continuous: average error rate Rel 81% lower 7GS nine tasks kind, two 7GSgive lower error rate Rel 8.5. Related Researchsection examines two alternative methods utilizing continuous attributesmentioned introduction, compares empirically C4.5 Rel 8.5.1 Global discretizationDougherty et al. (1995) consider various ways converting continuous attributediscrete one dividing values intervals, becomes separate valuereplacement discrete attribute. method found give best results, entropydiscretization, first investigated Catlett (1991) means reducing timerequired construct tree. Fayyad Irani (1993) subsequently introduced cleverrefinement led final form used Dougherty et al. experimentsreported here.find set intervals, training cases first sorted value continuous attribute question. procedure outlined Section 2 used find thresholdmaximizes information gain. process repeated correspondingsubsets cases attribute values t. (Since cases reordered,need re-sorted, source reduced learning times.) wthresholds found, continuous attribute mapped discrete attribute w+1values, one interval.stopping criterion required prevent process resulting largenumber intervals (which could become numerous training cases valuesattribute distinct). Catlett uses four-pronged heuristic criterion, FayyadIrani developed elegant test based MDL principle (Section 3). viewdiscretization rule classifying theory uses single attribute associatesclass interval. Introduction additional threshold, increasing complexitydiscretization rule, allowed greater theory coding costoffset consequent reduction exceptions cost. scheme generally leadsthresholds regions cases' classes vary much finer divisionsrequired.Similar experiments described Dougherty et al. carriedlearning tasks Section 4. trial, training data used find discretization85fiQuinlanTable 4: Comparison C4.5 using global discretization (Discr ).annealautobreast-wcoliccredit-acredit-gdiabetesglassheart-cheart-hhepatitishypoirislaborlettersegmentsicksonarvehiclewaveformaverageRel 87.67 .1217.7 .55.26 .1915.0 .214.7 .228.4 .325.4 .332.5 .823.0 .521.5 .220.4 .6.48 .014.80 .1719.1 1.012.0 .03.21 .081.34 .0325.6 .727.1 .427.3 .3Error RateDiscrw/d/l9.48 .14 10/0/023.8 .69/1/05.38 .15 6/0/415.1 .16/2/214.0 .10/1/928.1 .45/1/425.5 .35/0/528.4 .31/0/921.7 .62/1/720.8 .43/0/719.6 .83/1/6.72 .03 10/0/05.47 .29 6/3/120.0 .96/0/421.1 .0 10/0/05.65 .10 10/0/02.14 .03 10/0/024.6 .73/1/631.5 .5 10/0/026.5 .64/0/6ratio.81.74.98.991.051.01.991.141.061.041.04.67.88.96.57.57.631.04.861.03.90Tree SizeRel 8Discr75.2 .7 68.1 .563.7 .4 94.8 1.825.0 .5 19.9 .59.7 .27.8 .233.2 1.1 22.3 .6124 282 144.0 1.6 19.6 .745.7 .4 35.8 .339.9 .4 25.9 .419.1 .69.7 .617.8 .3 11.5 .527.5 .1 45.1 .38.5 .06.2 .17.0 .35.2 .12328 4 9600 1282.9 .5 296.4 2.650.8 .5 32.8 .428.4 .2 28.6 .5135 2175 244.6 .4 42.2 .8ratio1.11.671.251.231.491.502.251.281.541.971.55.611.361.34.24.281.55.99.781.061.20rules convert every continuous attribute discrete attribute. C4.54 invoked findtree evaluated test data, using discretization intervals foundtraining data. before, data set subjected ten cross-validations usingblocks cases previously.Results trials, summarized Table 4, show comments Doughertyet al. quoted introduction apply Rel 8. Discretization leads improvedaccuracy eight tasks degradation 12 them. improvementsmodest, however, several tasks exhibit marked increase error; average valueerror ratio indicates strong advantage local threshold selection employedRel 8 global thresholding used discretization.Kohavi (personal communication, 1996) suggests might \middle ground"thresholds determined locally subsets cases relatively small,point subsequent possible thresholds would found using discretization strategyabove. Evidence support idea provided Figure 2 where, task,error ratio appears Table 4 plotted size data set (on logarithmic4. Since continuous attributes, Rel 7 Rel 8 give identical results discretized tasks.86fiImproved Use Continuous Attributes C4.5size2000010001000.50.751.01.25ratioFigure 2: Effect discretization vs data set size.scale). clear trend shows global discretization degrades performance datasets become larger, beneficial tasks fewer cases.5.2 Multi-threshold splitscontrast, T2 (Auer et al., 1995) determines thresholds locally allows valuescontinuous attribute partitioned multiple intervals. intervalsfound heuristically recursive application binary splitting, above. Instead,thorough exploration carried find set intervals minimizeserror training set. (The default value C +1 C classesdata.) Search intervals expensive, T2 restricts decision trees two levelstests (in spirit one-level decision \stumps" described Holte, 1993)second level employs non-binary splits continuous attributes. Within restrictedtheory language, however, T2 guaranteed find tree misclassifiestraining cases possible.Even so, computational cost T2 using default value proportionalC 4 (C +1)2 a2 , number attributes (Auer, personal communication, 1996).example, time required process small auto data set six classes 25attributes four orders magnitude greater needed C4.5. effectivelyrules trials T2 learning tasks used above, specificallyfour classes. remaining 14 tasks, experiments following patternusing cross-validation blocks carried reportedTable 5. T2 produces trees error rates much lower generated Rel 8two tasks, slightly lower two more, higher remaining ten. ectedaverage error ratio, trials still favor C4.5 Rel 8 overall. (Had possible runtasks larger numbers classes, T2's restricted theory language would perhapscaused even noticeable increase error.)87fiQuinlanTable 5: Comparison T2.breast-wcoliccredit-acredit-gdiabetesheart-cheart-hhepatitisirislaborsicksonarvehiclewaveformaverageRel 85.26 .1915.0 .214.7 .228.4 .325.4 .323.0 .521.5 .220.4 .64.80 .1719.1 1.01.34 .0325.6 .727.1 .427.3 .3Error RateTree SizeT2w/d/l ratio Rel 8T2ratio4.06 .09 0/0/10 1.30 25.0 .5 10.0 .0 2.5016.2 .2 10/0/0.92 9.7 .2 15.5 .2 .6316.6 .2 10/0/0.89 33.2 1.1 46.1 .4 .7232.2 .2 10/0/0.88 124 249 1 2.5124.9 .23/0/7 1.02 44.0 1.6 11.5 .0 3.8126.8 .6 10/0/0.86 39.9 .4 20.5 .0 1.9426.1 .3 10/0/0.82 19.1 .6 16.3 .3 1.1824.8 .3 10/0/0.82 17.8 .3 13.7 .2 1.304.60 .35 3/1/6 1.04 8.5 .0 12.0 .0 .7115.3 1.6 3/0/7 1.25 7.0 .3 14.9 .1 .472.21 .01 10/0/0.61 50.8 .5 12.0 .0 4.2328.4 .78/0/2.90 28.4 .2 11.1 .0 2.5638.1 .3 10/0/0.71 135 216 0 8.4635.2 .6 10/0/0.78 44.6 .4 13.9 .0 3.21.912.44worth noting T2's trees much smaller found C4.5 { lesshalf size, average. despite fact tests T2 one outcome(for unknown values) corresponding tests C4.5.6. Conclusionresults Section 4 show straightforward changes C4.5's use continuousattributes lead overall improvement performance twenty learning tasksinvestigated here.5 pruned trees substantially smaller somewhat accurate,especially presence irrelevant attributes. tasks representative selectionUCI Repository involve continuous attributes, similar learning tasksalso benefit. course, C4.5's performance domains continuous attributesalso improved complementary ways, selecting attributes (John,Kohavi, & P eger, 1994), exploring space parameter settings (Kohavi & John, 1995),generating multiple classifiers (Breiman, 1996; Freund & Schapire, 1996).Comparisons well-known global discretization scheme, systemcarries thorough search space two-level decision trees, also favormodified C4.5. However, suggest ways system might improved.Non-binary splits continuous attributes make trees easier understand also seemlead accurate trees domains. would also interesting investigate5. files necessary update C4.5 Release 5 (available Quinlan, 1993) new Release 8obtained anonymous ftp ftp.cs.su.oz.au, file pub/ml/patch.tar.Z.88fiImproved Use Continuous Attributes C4.5Kohavi's suggestion use discretization within tree local number trainingcases small.another tack, C4.5 option affects tests discrete attributes. Insteaddefault, value attribute associated separate subtree,values grouped subsets one tree formed subset. Many possible subsetsexplored, many possible thresholds continuous attribute considered.argument application penalty tests continuous attributes would seemapply also subset tests.Appendix A. Description learning tasksAbbrevDomainCases Classesannealautobreast-wcoliccredit-acredit-gdiabetesglassheart-cheart-hhepatitishypoirislaborlettersegmentsicksonarvehiclewaveformannealing processes898auto insurance205breast cancer (Wisc)699horse colic368credit screening (Aust)690credit screening (Ger)1000Pima diabetes768glass identification214heart disease (Clev)303heart disease (Hun)294hepatitis prognosis155hypothyroid diagnosis3772iris classification150labor negotiations57letter identification20000image segmentation2310sick euthyroid3772sonar classification208silhouette recognition846waveform differentiation 300662222262225322672243AttributesCont Discr92915109{1012697138{9{85856137224{8816{19{72260{18{21{Acknowledgementsresearch made possible grant Australian Research Council.T2 system programmed Peter Auer made available comparisonsRob Holte. Thanks Thierry Van de Merckt comments results ledablation experiments. also grateful suggestions regarding paper's contentpresentation made Ron Kohavi, Usama Fayyad, Pat Langley. UCI DataRepository owes existence David Aha Patrick Murphy. breast cancer data(breast-w) provided Repository Dr William H. Wolberg.89fiQuinlanReferencesAuer, P., Holte, R. C., & Maass, W. (1995). Theory application agnostic paclearning small decision trees. Proceedings Twelfth International ConferenceMachine Learning, pp. 21{29. San Francisco: Morgam Kaufmann.Breiman, L. (1996). Bagging predictors. Machine Learning, (to appear).Catlett, J. (1991). changing continuous attributes ordered discrete attributes.Kodratoff, Y. (Ed.), Proceedings European Working Session Learning { EWSL-91,pp. 164{178. Berlin: Springer Verlag.Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised unsupervised discretizationcontinuous features. Proceedings Twelfth International Conference MachineLearning, pp. 194{202. San Francisco: Morgan Kaufmann.Fayyad, U. M., & Irani, K. B. (1992). handling continuous-valued attributesdecision tree generation. Machine Learning, 8, 87{102.Fayyad, U. M., & Irani, K. B. (1993). Multi-interval discretization continuous-valuedattributes classification learning. Proceedings Thirteenth International JointConference Artificial Intelligence, pp. 1022{1027. San Francisco: Morgan Kaufmann.Freund, Y., & Schapire, R. E. (1996). decision-theoretic generalization on-line learningapplication boosting. Unpublished manuscript, available authors'home pages (\http://www.research.att.com/orgs/ssr/people/fyoav,schapireg").Holte, R. C. (1993). simple classification rules perform well commonly useddatasets. Machine Learning, 11, 63{91.Hunt, E. B., Marin, J., & Stone, P. J. (1966). Experiments Induction. New York:Academic Press.John, G. H., Kohavi, R., & P eger, K. (1994). Irrelevant features subset selectionproblem. Proceedings Eleventh International Conference Machine Learning, pp.121{129. San Francisco: Morgan Kaufmann.Kohavi, R., & John, G. H. (1995). Automatic parameter selection minimizing estimatederror. Proceedings Twelfth International Conference Machine Learning, pp.304{312. San Francisco: Morgan Kaufmann.Quinlan, J. R. (1993). C4.5: Programs Machine Learning. San Mateo: Morgan Kaufmann.Quinlan, J. R., & Rivest, R. L. (1989). Inferring decision trees using minimum description length principle. Information Computation, 80, 227{248.Rissanen, J. (1983). universal prior integers estimation minimum descriptionlength. Annals Statistics, 11, 416{431.90fiJournal Artificial Intelligence Research 4 (1996) 237-285Submitted 9/95; published 5/96Reinforcement Learning: SurveyLeslie Pack KaelblingMichael L. LittmanComputer Science Department, Box 1910, Brown UniversityProvidence, RI 02912-1910 USAlpk@cs.brown.edumlittman@cs.brown.eduAndrew W. MooreSmith Hall 221, Carnegie Mellon University, 5000 Forbes AvenuePittsburgh, PA 15213 USAawm@cs.cmu.eduAbstractpaper surveys field reinforcement learning computer-science perspective. written accessible researchers familiar machine learning.historical basis field broad selection current work summarized.Reinforcement learning problem faced agent learns behaviortrial-and-error interactions dynamic environment. work describedresemblance work psychology, differs considerably details useword \reinforcement." paper discusses central issues reinforcement learning,including trading exploration exploitation, establishing foundations fieldvia Markov decision theory, learning delayed reinforcement, constructing empiricalmodels accelerate learning, making use generalization hierarchy, copinghidden state. concludes survey implemented systems assessmentpractical utility current methods reinforcement learning.1. IntroductionReinforcement learning dates back early days cybernetics work statistics,psychology, neuroscience, computer science. last five ten years, attractedrapidly increasing interest machine learning artificial intelligence communities.promise beguiling|a way programming agents reward punishment withoutneeding specify task achieved. formidable computationalobstacles fulfilling promise.paper surveys historical basis reinforcement learning currentwork computer science perspective. give high-level overview fieldtaste specific approaches. is, course, impossible mention importantwork field; taken exhaustive account.Reinforcement learning problem faced agent must learn behaviortrial-and-error interactions dynamic environment. work describedstrong family resemblance eponymous work psychology, differs considerablydetails use word \reinforcement." appropriately thoughtclass problems, rather set techniques.two main strategies solving reinforcement-learning problems. firstsearch space behaviors order find one performs well environment.approach taken work genetic algorithms genetic programming,c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiKaelbling, Littman, & MooreRrBFigure 1: standard reinforcement-learning model.well novel search techniques (Schmidhuber, 1996). second usestatistical techniques dynamic programming methods estimate utility takingactions states world. paper devoted almost entirely second settechniques take advantage special structure reinforcement-learningproblems available optimization problems general. yet clearset approaches best circumstances.rest section devoted establishing notation describing basicreinforcement-learning model. Section 2 explains trade-off explorationexploitation presents solutions basic case reinforcement-learningproblems, want maximize immediate reward. Section 3 considersgeneral problem rewards delayed time actions crucialgaining them. Section 4 considers classic model-free algorithms reinforcementlearning delayed reward: adaptive heuristic critic, TD() Q-learning. Section 5demonstrates continuum algorithms sensitive amount computationagent perform actual steps action environment. Generalization|thecornerstone mainstream machine learning research|has potential considerablyaiding reinforcement learning, described Section 6. Section 7 considers problemsarise agent complete perceptual access stateenvironment. Section 8 catalogs reinforcement learning's successful applications.Finally, Section 9 concludes speculations important open problemsfuture reinforcement learning.1.1 Reinforcement-Learning Modelstandard reinforcement-learning model, agent connected environmentvia perception action, depicted Figure 1. step interaction agentreceives input, i, indication current state, s, environment; agentchooses action, a, generate output. action changes stateenvironment, value state transition communicated agentscalar reinforcement signal, r. agent's behavior, B , choose actions tendincrease long-run sum values reinforcement signal. learntime systematic trial error, guided wide variety algorithmssubject later sections paper.238fiReinforcement Learning: SurveyFormally, model consistsdiscrete set environment states, ;discrete set agent actions, A;set scalar reinforcement signals; typically f0; 1g, real numbers.figure also includes input function , determines agent viewsenvironment state; assume identity function (that is, agent perceivesexact state environment) consider partial observability Section 7.intuitive way understand relation agent environmentfollowing example dialogue.Environment: state 65. 4 possible actions.Agent:I'll take action 2.Environment: received reinforcement 7 units. state15. 2 possible actions.Agent:I'll take action 1.Environment: received reinforcement -4 units. state65. 4 possible actions.Agent:I'll take action 2.Environment: received reinforcement 5 units. state44. 5 possible actions.......agent's job find policy , mapping states actions, maximizeslong-run measure reinforcement. expect, general, environmentnon-deterministic; is, taking action state two differentoccasions may result different next states and/or different reinforcement values.happens example above: state 65, applying action 2 produces differing reinforcements differing states two occasions. However, assume environmentstationary; is, probabilities making state transitions receiving specificreinforcement signals change time.1Reinforcement learning differs widely studied problem supervised learning several ways. important difference presentation input/output pairs. Instead, choosing action agent told immediate rewardsubsequent state, told action would best long-terminterests. necessary agent gather useful experience possible systemstates, actions, transitions rewards actively act optimally. Another differencesupervised learning on-line performance important: evaluation systemoften concurrent learning.1. assumption may disappointing; all, operation non-stationary environments onemotivations building learning systems. fact, many algorithms described later sectionseffective slowly-varying non-stationary environments, little theoretical analysisarea.239fiKaelbling, Littman, & Mooreaspects reinforcement learning closely related search planning issuesartificial intelligence. AI search algorithms generate satisfactory trajectorygraph states. Planning operates similar manner, typically within constructcomplexity graph, states represented compositionslogical expressions instead atomic symbols. AI algorithms less generalreinforcement-learning methods, require predefined model state transitions,exceptions assume determinism. hand, reinforcement learning,least kind discrete cases theory developed, assumesentire state space enumerated stored memory|an assumptionconventional search algorithms tied.1.2 Models Optimal Behaviorstart thinking algorithms learning behave optimally,decide model optimality be. particular, specifyagent take future account decisions makes behavenow. three models subject majority workarea.finite-horizon model easiest think about; given moment time,agent optimize expected reward next h steps:hXE ( rt) ;t=0need worry happen that. subsequent expressions,rt represents scalar reward received steps future. model usedtwo ways. first, agent non-stationary policy; is, one changestime. first step take termed h-step optimal action.defined best action available given h steps remaining actgain reinforcement. next step take (h , 1)-step optimal action,on, finally takes 1-step optimal action terminates. second, agentreceding-horizon control, always takes h-step optimal action. agentalways acts according policy, value h limits far ahead lookschoosing actions. finite-horizon model always appropriate. many casesmay know precise length agent's life advance.infinite-horizon discounted model takes long-run reward agent account, rewards received future geometrically discounted accordingdiscount factor , (where 0 < 1):1XE ( trt) :t=0interpret several ways. seen interest rate, probability livinganother step, mathematical trick bound infinite sum. model conceptually similar receding-horizon control, discounted model mathematicallytractable finite-horizon model. dominant reason wide attentionmodel received.240fiReinforcement Learning: SurveyAnother optimality criterion average-reward model, agent supposedtake actions optimize long-run average reward:h1Xrt) :limE(h!1 ht=0policy referred gain optimal policy; seen limiting caseinfinite-horizon discounted model discount factor approaches 1 (Bertsekas, 1995).One problem criterion way distinguish two policies,one gains large amount reward initial phasesnot. Reward gained initial prefix agent's life overshadowedlong-run average performance. possible generalize model takesaccount long run average amount initial reward gained.generalized, bias optimal model, policy preferred maximizes long-runaverage ties broken initial extra reward.Figure 2 contrasts models optimality providing environmentchanging model optimality changes optimal policy. example, circlesrepresent states environment arrows state transitions.single action choice every state except start state, upper leftmarked incoming arrow. rewards zero except marked.finite-horizon model h = 5, three actions yield rewards +6:0, +0:0, +0:0,first action chosen; infinite-horizon discounted model = 0:9,three choices yield +16:2, +59:0, +58:5 second action chosen;average reward model, third action chosen since leadsaverage reward +11. change h 1000 0.2, second actionoptimal finite-horizon model first infinite-horizon discounted model;however, average reward model always prefer best long-term average. Sincechoice optimality model parameters matters much, important choosecarefully application.finite-horizon model appropriate agent's lifetime known; one important aspect model length remaining lifetime decreases,agent's policy may change. system hard deadline would appropriately modeledway. relative usefulness infinite-horizon discounted bias-optimal modelsstill debate. Bias-optimality advantage requiring discount parameter;however, algorithms finding bias-optimal policies yet well-understoodfinding optimal infinite-horizon discounted policies.1.3 Measuring Learning Performancecriteria given previous section used assess policies learnedgiven algorithm. would also like able evaluate quality learning itself.several incompatible measures use.Eventual convergence optimal. Many algorithms come provable guar-antee asymptotic convergence optimal behavior (Watkins & Dayan, 1992).reassuring, useless practical terms. agent quickly reaches plateau241fiKaelbling, Littman, & Moore+2Finite horizon, h=4+10Infinite horizon, =0.9+11Average rewardFigure 2: Comparing models optimality. unlabeled arrows produce reward zero.99% optimality may, many applications, preferable agentguarantee eventual optimality sluggish early learning rate.Speed convergence optimality. Optimality usually asymptotic result,convergence speed ill-defined measure. practical speedconvergence near-optimality. measure begs definition nearoptimality sucient. related measure level performance given time,similarly requires someone define given time.noted another difference reinforcement learningconventional supervised learning. latter, expected future predictive accuracy statistical eciency prime concerns. example, well-knownPAC framework (Valiant, 1984), learning period mistakescount, performance period do. framework providesbounds necessary length learning period order probabilisticguarantee subsequent performance. usually inappropriate viewagent long existence complex environment.spite mismatch embedded reinforcement learning train/testperspective, Fiechter (1994) provides PAC analysis Q-learning (describedSection 4.2) sheds light connection two views.Measures related speed learning additional weakness. algorithmmerely tries achieve optimality fast possible may incur unnecessarilylarge penalties learning period. less aggressive strategy taking longerachieve optimality, gaining greater total reinforcement learning mightpreferable.Regret. appropriate measure, then, expected decrease reward gaineddue executing learning algorithm instead behaving optimallybeginning. measure known regret (Berry & Fristedt, 1985). penalizesmistakes wherever occur run. Unfortunately, results concerningregret algorithms quite hard obtain.242fiReinforcement Learning: Survey1.4 Reinforcement Learning Adaptive ControlAdaptive control (Burghes & Graham, 1980; Stengel, 1986) also concerned algorithms improving sequence decisions experience. Adaptive control muchmature discipline concerns dynamic systems states actions vectors system dynamics smooth: linear locally linearizable arounddesired trajectory. common formulation cost functions adaptive controlquadratic penalties deviation desired state action vectors. importantly,although dynamic model system known advance, must estimated data, structure dynamic model fixed, leaving model estimationparameter estimation problem. assumptions permit deep, elegant powerfulmathematical analysis, turn lead robust, practical, widely deployed adaptivecontrol algorithms.2. Exploitation versus Exploration: Single-State CaseOne major difference reinforcement learning supervised learningreinforcement-learner must explicitly explore environment. order highlightproblems exploration, treat simple case section. fundamental issuesapproaches described will, many cases, transfer complex instancesreinforcement learning discussed later paper.simplest possible reinforcement-learning problem known k-armed banditproblem, subject great deal study statistics appliedmathematics literature (Berry & Fristedt, 1985). agent room collectionk gambling machines (each called \one-armed bandit" colloquial English). agentpermitted fixed number pulls, h. arm may pulled turn. machinesrequire deposit play; cost wasting pull playing suboptimalmachine. arm pulled, machine pays 1 0, according underlyingprobability parameter pi , payoffs independent events pi unknown.agent's strategy be?problem illustrates fundamental tradeoff exploitation exploration.agent might believe particular arm fairly high payoff probability;choose arm time, choose another one less informationabout, seems worse? Answers questions depend long agentexpected play game; longer game lasts, worse consequencesprematurely converging sub-optimal arm, agent explore.wide variety solutions problem. consider representativeselection them, deeper discussion number important theoretical results,see book Berry Fristedt (1985). use term \action" indicateagent's choice arm pull. eases transition delayed reinforcement modelsSection 3. important note bandit problems fit definitionreinforcement-learning environment single state self transitions.Section 2.1 discusses three solutions basic one-state bandit problemformal correctness results. Although extended problems real-valuedrewards, apply directly general multi-state delayed-reinforcement case.243fiKaelbling, Littman, & MooreSection 2.2 presents three techniques formally justified, wideuse practice, applied (with similar lack guarantee) general case.2.1 Formally Justified Techniquesfairly well-developed formal theory exploration simple problems.Although instructive, methods provides scale well complexproblems.2.1.1 Dynamic-Programming Approachagent going acting total h steps, use basic Bayesian reasoningsolve optimal strategy (Berry & Fristedt, 1985). requires assumed priorjoint distribution parameters fpig, natural piindependently uniformly distributed 0 1. compute mapping beliefstates (summaries agent's experiences run) actions. Here, belief staterepresented tabulation action choices payoffs: fn1; w1; n2; w2; : : :; nk ; wk gdenotes state play arm pulled ni times wi payoffs.write V (n1; w1; : : :; nk ; wk ) expected payoff remaining, given total h pullsavailable,use remaining pulls optimally.Pni = h, remaining pulls, V (n1 ; w1; : : :; nk ; wk ) = 0.basis recursive definition. know V value belief states pullsremaining, compute V value belief state + 1 pulls remaining:V (n1; w1; : : :; nk ; wk) = maxi E= maxi"#Future payoff agent takes action i,acts optimally remaining pulls !iV (n1; wi; : : :; ni + 1; wi + 1; : : :; nk ; wk)+(1 , i)V (n1 ; wi; : : :; ni + 1; wi; : : :; nk ; wk )posterior subjective probability action paying given ni , wiprior probability. uniform priors, result beta distribution, =(wi + 1)=(ni + 2).expense filling table V values way attainable belief stateslinear number belief states times actions, thus exponential horizon.2.1.2 Gittins Allocation IndicesGittins gives \allocation index" method finding optimal choice actionstep k-armed bandit problems (Gittins, 1989). technique appliesdiscounted expected reward criterion. action, consider number timeschosen, n, versus number times paid off, w. certain discount factors,published tables \index values," (n; w) pair n w. Lookindex value action i, (ni ; wi). represents comparative measurecombined value expected payoff action (given history payoffs) valueinformation would get choosing it. Gittins shown choosingaction largest index value guarantees optimal balance explorationexploitation.244fiReinforcement Learning: Surveya=1a=0123N-1N2Nr=123N-1N+3N+2N+1N+2N+1a=1a=012N-1N2N2N-1N+3r=0Figure 3: Tsetlin automaton 2N states. top row shows state transitionsmade previous action resulted reward 1; bottomrow shows transitions reward 0. states left half figure,action 0 taken; right, action 1 taken.guarantee optimal exploration simplicity technique(given table index values), approach holds great deal promise usecomplex applications. method proved useful application robotic manipulationimmediate reward (Salganicoff & Ungar, 1995). Unfortunately, one yetable find analog index values delayed reinforcement problems.2.1.3 Learning Automatabranch theory adaptive control devoted learning automata, surveyedNarendra Thathachar (1989), originally described explicitly finite stateautomata. Tsetlin automaton shown Figure 3 provides example solves2-armed bandit arbitrarily near optimally N approaches infinity.inconvenient describe algorithms finite-state automata, move madedescribe internal state agent probability distribution accordingactions would chosen. probabilities taking different actions would adjustedaccording previous successes failures.example, stands among set algorithms independently developedmathematical psychology literature (Hilgard & Bower, 1975), linear reward-inactionalgorithm. Let pi agent's probability taking action i.action ai succeeds,pi := pi + ff(1 , pi)pj := pj , ffpj j 6=action ai fails, pj remains unchanged (for j ).algorithm converges probability 1 vector containing single 1rest 0's (choosing particular action probability 1). Unfortunately, alwaysconverge correct action; probability converges wrong onemade arbitrarily small making ff small (Narendra & Thathachar, 1974).literature regret algorithm.245fiKaelbling, Littman, & Moore2.2 Ad-Hoc Techniquesreinforcement-learning practice, simple, ad hoc strategies popular.rarely, ever, best choice models optimality used, mayviewed reasonable, computationally tractable, heuristics. Thrun (1992) surveyedvariety techniques.2.2.1 Greedy Strategiesfirst strategy comes mind always choose action highest estimated payoff. aw early unlucky sampling might indicate best action'sreward less reward obtained suboptimal action. suboptimal actionalways picked, leaving true optimal action starved data superioritynever discovered. agent must explore ameliorate outcome.useful heuristic optimism face uncertainty actions selectedgreedily, strongly optimistic prior beliefs put payoffs strong negativeevidence needed eliminate action consideration. still measurabledanger starving optimal unlucky action, risk made arbitrarily small. Techniques like used several reinforcement learning algorithmsincluding interval exploration method (Kaelbling, 1993b) (described shortly), exploration bonus Dyna (Sutton, 1990), curiosity-driven exploration (Schmidhuber, 1991a),exploration mechanism prioritized sweeping (Moore & Atkeson, 1993).2.2.2 Randomized StrategiesAnother simple exploration strategy take action best estimated expectedreward default, probability p, choose action random. versionsstrategy start large value p encourage initial exploration, slowlydecreased.objection simple strategy experiments non-greedy actionlikely try promising alternative clearly hopeless alternative.slightly sophisticated strategy Boltzmann exploration. case, expectedreward taking action a, ER(a) used choose action probabilistically accordingdistributionER(a)=TP (a) = P e ER(a )=T :2A e00temperature parameter decreased time decrease exploration.method works well best action well separated others, suffers somewhatvalues actions close. may also converge unnecessarily slowly unlesstemperature schedule manually tuned great care.2.2.3 Interval-based TechniquesExploration often ecient based second-order informationcertainty variance estimated values actions. Kaelbling's interval estimationalgorithm (1993b) stores statistics action ai : wi number successes ninumber trials. action chosen computing upper bound 100 (1 , ff)%246fiReinforcement Learning: Surveyconfidence interval success probability action choosing actionhighest upper bound. Smaller values ff parameter encourage greater exploration.payoffs boolean, normal approximation binomial distributionused construct confidence interval (though binomial used smalln). payoff distributions handled using associated statisticsnonparametric methods. method works well empirical trials. also relatedcertain class statistical techniques known experiment design methods (Box &Draper, 1987), used comparing multiple treatments (for example, fertilizersdrugs) determine treatment (if any) best small set experimentspossible.2.3 General Problemsmultiple states, reinforcement still immediate,solutions replicated, state. However, generalization required,solutions must integrated generalization methods (see section 6);straightforward simple ad-hoc methods, understood maintaintheoretical guarantees.Many techniques focus converging regime exploratoryactions taken rarely never; appropriate environment stationary.However, environment non-stationary, exploration must continue take place,order notice changes world. Again, ad-hoc techniques modifieddeal plausible manner (keep temperature parameters going 0; decaystatistics interval estimation), none theoretically guaranteed methodsapplied.3. Delayed Rewardgeneral case reinforcement learning problem, agent's actions determineimmediate reward, also (at least probabilistically) next stateenvironment. environments thought networks bandit problems,agent must take account next state well immediate rewarddecides action take. model long-run optimality agent using determinesexactly take value future account. agentable learn delayed reinforcement: may take long sequence actions, receivinginsignificant reinforcement, finally arrive state high reinforcement. agentmust able learn actions desirable based reward take placearbitrarily far future.3.1 Markov Decision ProcessesProblems delayed reinforcement well modeled Markov decision processes (MDPs).MDP consistsset states ,set actions A,247fiKaelbling, Littman, & Moorereward function R : ! <,state transition function : SA ! (S ), member (S ) probabilitydistribution set (i.e. maps states probabilities). write (s; a; s0)probability making transition state state s0 using action a.state transition function probabilistically specifies next state environmentfunction current state agent's action. reward function specifies expectedinstantaneous reward function current state action. model Markovstate transitions independent previous environment states agent actions.many good references MDP models (Bellman, 1957; Bertsekas, 1987; Howard,1960; Puterman, 1994).Although general MDPs may infinite (even uncountable) state action spaces,discuss methods solving finite-state finite-action problems. section 6,discuss methods solving problems continuous input output spaces.3.2 Finding Policy Given Modelconsider algorithms learning behave MDP environments, explore techniques determining optimal policy given correct model. dynamicprogramming techniques serve foundation inspiration learning algorithms follow. restrict attention mainly finding optimal policiesinfinite-horizon discounted model, algorithms analogs finitehorizon average-case models well. rely result that, infinite-horizondiscounted model, exists optimal deterministic stationary policy (Bellman, 1957).speak optimal value state|it expected infinite discounted sumreward agent gain starts state executes optimal policy.Using complete decision policy, written!1XV (s) = maxE t=0 rt:optimal value function unique defined solution simultaneousequations01@V (s) = maxR(s; a) +X2S(s; a; s0)V (s0)A ; 8s 2 ;(1)0assert value state expected instantaneous reward plusexpected discounted value next state, using best available action. Givenoptimal value function, specify optimal policy01X@(s) = arg max(s; a; s0)V (s0)A :R(s; a) +2S03.2.1 Value IterationOne way, then, find optimal policy find optimal value function.determined simple iterative algorithm called value iteration shownconverge correct V values (Bellman, 1957; Bertsekas, 1987).248fiReinforcement Learning: SurveyVinitialize ( ) arbitrarilyloop policy good enoughlooploops2Sa2AQ(s; a) := R(s; a) + Ps 2S (s; a; s0)V (s0)V (s) := maxa Q(s; a)0end loopend loopobvious stop value iteration algorithm. One important resultbounds performance current greedy policy function Bellman residualcurrent value function (Williams & Baird, 1993b). says maximum differencetwo successive value functions less , value greedy policy,(the policy obtained choosing, every state, action maximizes estimateddiscounted reward, using current estimate value function) differs valuefunction optimal policy 2 =(1 , ) state. provideseffective stopping criterion algorithm. Puterman (1994) discusses another stoppingcriterion, based span semi-norm, may result earlier termination. Anotherimportant result greedy policy guaranteed optimal finite numbersteps even though value function may converged (Bertsekas, 1987).practice, greedy policy often optimal long value function converged.Value iteration exible. assignments V need done strict ordershown above, instead occur asynchronously parallel provided valueevery state gets updated infinitely often infinite run. issues treatedextensively Bertsekas (1989), also proves convergence results.Updates based Equation 1 known full backups since make use information possible successor states. shown updates formQ(s; a) := Q(s; a) + ff(r + maxQ(s0; a0) , Q(s; a))0also used long pairing updated infinitely often, s0 sampleddistribution (s; a; s0), r sampled mean R(s; a) bounded variance,learning rate ff decreased slowly. type sample backup (Singh, 1993) criticaloperation model-free methods discussed next section.computational complexity value-iteration algorithm full backups, periteration, quadratic number states linear number actions. Commonly, transition probabilities (s; a; s0) sparse. average constantnumber next states non-zero probability cost per iteration linearnumber states linear number actions. number iterations requiredreach optimal value function polynomial number states magnitudelargest reward discount factor held constant. However, worst casenumber iterations grows polynomially 1=(1 , ), convergence rate slowsconsiderably discount factor approaches 1 (Littman, Dean, & Kaelbling, 1995b).249fiKaelbling, Littman, & Moore3.2.2 Policy Iterationpolicy iteration algorithm manipulates policy directly, rather finding indirectly via optimal value function. operates follows:choose arbitrary policyloop0:= 0compute value function policysolve linear equations:V (s) = R(s; (s)) + Ps 2S (s; (s); s0)V (s0)00(s) := arg maxa (R(s; a) + Ps 2S (s; a; s0)V(s0))= 0improve policy state:0value function policy expected infinite discounted rewardgained, state, executing policy. determined solving setlinear equations. know value state current policy,consider whether value could improved changing first action taken. can,change policy take new action whenever situation. stepguaranteed strictly improve performance policy. improvementspossible, policy guaranteed optimal.Since jAjjSj distinct policies, sequence policies improvesstep, algorithm terminates exponential number iterations (Puterman, 1994). However, important open question many iterations policy iterationtakes worst case. known running time pseudopolynomialfixed discount factor, polynomial bound total size MDP (Littmanet al., 1995b).3.2.3 Enhancement Value Iteration Policy Iterationpractice, value iteration much faster per iteration, policy iteration takes feweriterations. Arguments put forth effect approach betterlarge problems. Puterman's modified policy iteration algorithm (Puterman & Shin, 1978)provides method trading iteration time iteration improvement smoother way.basic idea expensive part policy iteration solving exact valueV . Instead finding exact value V , perform steps modifiedvalue-iteration step policy held fixed successive iterations.shown produce approximation V converges linearly . practice,result substantial speedups.Several standard numerical-analysis techniques speed convergence dynamicprogramming used accelerate value policy iteration. Multigrid methodsused quickly seed good initial approximation high resolution value functioninitially performing value iteration coarser resolution (Rude, 1993). State aggregation works collapsing groups states single meta-state solving abstractedproblem (Bertsekas & Casta~non, 1989).250fiReinforcement Learning: Survey3.2.4 Computational ComplexityValue iteration works producing successive approximations optimal value function.iteration performed O(jAjjS j2) steps, faster sparsitytransition function. However, number iterations required grow exponentiallydiscount factor (Condon, 1992); discount factor approaches 1, decisions mustbased results happen farther farther future. practice, policyiteration converges fewer iterations value iteration, although per-iteration costsO(jAjjS j2 + jS j3) prohibitive. known tight worst-case bound availablepolicy iteration (Littman et al., 1995b). Modified policy iteration (Puterman & Shin,1978) seeks trade-off cheap effective iterations preferredpractictioners (Rust, 1996).Linear programming (Schrijver, 1986) extremely general problem, MDPssolved general-purpose linear-programming packages (Derman, 1970; D'Epenoux,1963; Hoffman & Karp, 1966). advantage approach commercial-qualitylinear-programming packages available, although time space requirementsstill quite high. theoretic perspective, linear programming knownalgorithm solve MDPs polynomial time, although theoretically ecientalgorithms shown ecient practice.4. Learning Optimal Policy: Model-free Methodsprevious section reviewed methods obtaining optimal policy MDPassuming already model. model consists knowledge state transition probability function (s; a; s0) reinforcement function R(s; a). Reinforcementlearning primarily concerned obtain optimal policy modelknown advance. agent must interact environment directly obtaininformation which, means appropriate algorithm, processed produceoptimal policy.point, two ways proceed.Model-free: Learn controller without learning model.Model-based: Learn model, use derive controller.approach better? matter debate reinforcement-learningcommunity. number algorithms proposed sides. question alsoappears fields, adaptive control, dichotomy directindirect adaptive control.section examines model-free learning, Section 5 examines model-based methods.biggest problem facing reinforcement-learning agent temporal credit assignment.know whether action taken good one, might farreaching effects? One strategy wait \end" reward actions takenresult good punish result bad. ongoing tasks, dicultknow \end" is, might require great deal memory. Instead,use insights value iteration adjust estimated value state based251fiKaelbling, Littman, & MoorervAHCRLFigure 4: Architecture adaptive heuristic critic.immediate reward estimated value next state. class algorithmsknown temporal difference methods (Sutton, 1988). consider two differenttemporal-difference learning strategies discounted infinite-horizon model.4.1 Adaptive Heuristic Critic TD()adaptive heuristic critic algorithm adaptive version policy iteration (Barto,Sutton, & Anderson, 1983) value-function computation longer implemented solving set linear equations, instead computed algorithm calledTD(0). block diagram approach given Figure 4. consists two components: critic (labeled AHC), reinforcement-learning component (labeled RL).reinforcement-learning component instance k-armed bandit algorithms, modified deal multiple states non-stationary rewards. insteadacting maximize instantaneous reward, acting maximize heuristic value,v, computed critic. critic uses real external reinforcement signallearn map states expected discounted values given policy executedone currently instantiated RL component.see analogy modified policy iteration imagine componentsworking alternation. policy implemented RL fixed critic learnsvalue function V policy. fix critic let RL component learnnew policy 0 maximizes new value function, on. implementations,however, components operate simultaneously. alternating implementationguaranteed converge optimal policy, appropriate conditions. WilliamsBaird explored convergence properties class AHC-related algorithmscall \incremental variants policy iteration" (Williams & Baird, 1993a).remains explain critic learn value policy. define hs; a; r; s0iexperience tuple summarizing single transition environment.agent's state transition, choice action, r instantaneous rewardreceives, s0 resulting state. value policy learned using Sutton's TD(0)algorithm (Sutton, 1988) uses update ruleV (s) := V (s) + ff(r + V (s0) , V (s)) :Whenever state visited, estimated value updated closer r + V (s0 ),since r instantaneous reward received V (s0) estimated value actuallyoccurring next state. analogous sample-backup rule value iteration|thedifference sample drawn real world rather simulatingknown model. key idea r + V (s0 ) sample value V (s),252fiReinforcement Learning: Surveylikely correct incorporates real r. learning rate ff adjustedproperly (it must slowly decreased) policy held fixed, TD(0) guaranteedconverge optimal value function.TD(0) rule presented really instance general classalgorithms called TD(), = 0. TD(0) looks one step ahead adjustingvalue estimates; although eventually arrive correct answer, take quiteso. general TD() rule similar TD(0) rule given above,V (u) := V (u) + ff(r + V (s0) , V (s))e(u) ;applied every state according eligibility e(u), ratherimmediately previous state, s. One version eligibility trace definede(s) =Xk=1( )t,k s;sk, s;sk =(1 = sk .0 otherwiseeligibility state degree visited recent past;reinforcement received, used update states recentlyvisited, according eligibility. = 0 equivalent TD(0). = 1,roughly equivalent updating states according number timesvisited end run. Note update eligibility online follows:((s) + 1 = current state .e(s) := ee(s)otherwisecomputationally expensive execute general TD(), though oftenconverges considerably faster large (Dayan, 1992; Dayan & Sejnowski, 1994).recent work making updates ecient (Cichosz & Mulawka, 1995)changing definition make TD() consistent certainty-equivalentmethod (Singh & Sutton, 1996), discussed Section 5.1.4.2 Q-learningwork two components AHC accomplished unified mannerWatkins' Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992). Q-learningtypically easier implement. order understand Q-learning, developadditional notation. Let Q(s; a) expected discounted reinforcement taking actionstate s, continuing choosing actions optimally. Note V (s) valueassuming best action taken initially, V (s) = maxa Q (s; a). Q (s; a)hence written recursivelyQ(s; a) = R(s; a) +X2S0(s; a; s0) maxQ(s0; a0) :0Note also that, since V (s) = maxa Q (s; a), (s) = arg maxa Q (s; a)optimal policy.Q function makes action explicit, estimate Q values online using method essentially TD(0), also use define policy,253fiKaelbling, Littman, & Mooreaction chosen taking one maximum Q valuecurrent state.Q-learning ruleQ(s; a) := Q(s; a) + ff(r + maxQ(s0; a0) , Q(s; a)) ;0hs; a; r; s0i experience tuple described earlier. action executedstate infinite number times infinite run ff decayed appropriately,Q values converge probability 1 Q (Watkins, 1989; Tsitsiklis, 1994; Jaakkola,Jordan, & Singh, 1994). Q-learning also extended update states occurredone step previously, TD() (Peng & Williams, 1994).Q values nearly converged optimal values, appropriateagent act greedily, taking, situation, action highest Q value.learning, however, dicult exploitation versus exploration trade-offmade. good, formally justified approaches problem general case;standard practice adopt one ad hoc methods discussed section 2.2.AHC architectures seem dicult work Q-learning practicallevel. hard get relative learning rates right AHC twocomponents converge together. addition, Q-learning exploration insensitive:is, Q values converge optimal values, independent agentbehaves data collected (as long state-action pairs tried oftenenough). means that, although exploration-exploitation issue must addressedQ-learning, details exploration strategy affect convergencelearning algorithm. reasons, Q-learning popular seemseffective model-free algorithm learning delayed reinforcement. not,however, address issues involved generalizing large state and/or actionspaces. addition, may converge quite slowly good policy.4.3 Model-free Learning Average Rewarddescribed, Q-learning applied discounted infinite-horizon MDPs. alsoapplied undiscounted problems long optimal policy guaranteed reachreward-free absorbing state state periodically reset.Schwartz (1993) examined problem adapting Q-learning average-rewardframework. Although R-learning algorithm seems exhibit convergence problemsMDPs, several researchers found average-reward criterion closer trueproblem wish solve discounted criterion therefore prefer R-learningQ-learning (Mahadevan, 1994).mind, researchers studied problem learning optimal averagereward policies. Mahadevan (1996) surveyed model-based average-reward algorithmsreinforcement-learning perspective found several diculties existing algorithms.particular, showed existing reinforcement-learning algorithms average reward(and dynamic programming algorithms) always produce bias-optimal policies. Jaakkola, Jordan Singh (1995) described average-reward learning algorithmguaranteed convergence properties. uses Monte-Carlo component estimateexpected future reward state agent moves environment.254fiReinforcement Learning: Surveyaddition, Bertsekas presents Q-learning-like algorithm average-case reward newtextbook (1995). Although recent work provides much needed theoretical foundationarea reinforcement learning, many important problems remain unsolved.5. Computing Optimal Policies Learning Modelsprevious section showed possible learn optimal policy without knowingmodels (s; a; s0) R(s; a) without even learning models en route. Althoughmany methods guaranteed find optimal policies eventually uselittle computation time per experience, make extremely inecient use datagather therefore often require great deal experience achieve good performance.section still begin assuming don't know models advance,examine algorithms operate learning models. algorithmsespecially important applications computation considered cheapreal-world experience costly.5.1 Certainty Equivalent Methodsbegin conceptually straightforward method: first, learn Rfunctions exploring environment keeping statistics resultsaction; next, compute optimal policy using one methods Section 3.method known certainty equivlance (Kumar & Varaiya, 1986).serious objections method:makes arbitrary division learning phase acting phase.gather data environment initially? Random explorationmight dangerous, environments immensely inecient methodgathering data, requiring exponentially data (Whitehead, 1991) systeminterleaves experience gathering policy-building tightly (Koenig &Simmons, 1993). See Figure 5 example.possibility changes environment also problematic. Breakingagent's life pure learning pure acting phase considerable riskoptimal controller based early life becomes, without detection, suboptimalcontroller environment changes.variation idea certainty equivalence, model learned continuallyagent's lifetime and, step, current model used computeoptimal policy value function. method makes effective use available data,still ignores question exploration extremely computationally demanding,even fairly small state spaces. Fortunately, number model-basedalgorithms practical.5.2 DynaSutton's Dyna architecture (1990, 1991) exploits middle ground, yielding strategieseffective model-free learning computationally ecient255fiKaelbling, Littman, & Moore123.......nGoalFigure 5: environment, due Whitehead (1991), random exploration would taketake O(2n ) steps reach goal even once, whereas intelligent exploration strategy (e.g. \assume untried action leads directly goal") wouldrequire O(n2 ) steps.certainty-equivalence approach. simultaneously uses experience build model (T^R^ ), uses experience adjust policy, uses model adjust policy.Dyna operates loop interaction environment. Given experience tuplehs; a; s0; ri, behaves follows:Update model, incrementing statistics transition s0 actionreceiving reward r taking action state s. updated models T^R^ .Update policy state based newly updated model using ruleX^00 0^Q(s; a) := R(s; a) +0(s; a; ) maxQ(s ; ) ;0version value-iteration update Q values.Perform k additional updates: choose k state-action pairs random updateaccording rule before:Q(sk ; ak ):=R^(sk ; ak ) +X^0(sk ; ak; s0) maxQ(s0; a0) :0Choose action a0 perform state s0, based Q values perhaps modifiedexploration strategy.Dyna algorithm requires k times computation Q-learning per instance,typically vastly less naive model-based method. reasonable valuek determined based relative speeds computation taking action.Figure 6 shows grid world cell agent four actions (N, S, E,W) transitions made deterministically adjacent cell, unless block,case movement occurs. see Table 1, Dyna requires ordermagnitude fewer steps experience Q-learning arrive optimal policy.Dyna requires six times computational effort, however.256fiReinforcement Learning: SurveyFigure 6: 3277-state grid world. formulated shortest-path reinforcementlearning problem, yields result reward 1 givengoal, reward zero elsewhere discount factor used.Q-learningDynaprioritized sweepingSteps Backupsconvergenceconvergence531,00062,00028,000531,0003,055,0001,010,000Table 1: performance three algorithms described text. methods usedexploration heuristic \optimism face uncertainty": statepreviously visited assumed default goal state. Q-learning usedoptimal learning rate parameter deterministic maze: ff = 1. Dynaprioritized sweeping permitted take k = 200 backups per transition.prioritized sweeping, priority queue often emptied backupsused.257fiKaelbling, Littman, & Moore5.3 Prioritized Sweeping / Queue-DynaAlthough Dyna great improvement previous methods, suffers relativelyundirected. particularly unhelpful goal reachedagent stuck dead end; continues update random state-action pairs, ratherconcentrating \interesting" parts state space. problems addressedprioritized sweeping (Moore & Atkeson, 1993) Queue-Dyna (Peng & Williams,1993), two independently-developed similar techniques. describeprioritized sweeping detail.algorithm similar Dyna, except updates longer chosen randomvalues associated states (as value iteration) instead state-action pairs(as Q-learning). make appropriate choices, must store additional informationmodel. state remembers predecessors: states non-zero transitionprobability action. addition, state priority, initially setzero.Instead updating k random state-action pairs, prioritized sweeping updates k stateshighest priority. high-priority state s, works follows:Remember current value state: Vold = V (s).Update state's value^V (s) := maxR(s; a) +X^(s; a; s0)V (s0)!:0Set state's priority back 0.Compute value change = jVold , V (s)j.Use modify priorities predecessors s.updated V value state s0 changed amount ,immediate predecessors s0 informed event. state existsaction T^(s; a; s0) 6= 0 priority promoted T^(s; a; s0), unlesspriority already exceeded value.global behavior algorithm real-world transition \surprising"(the agent happens upon goal state, instance), lots computation directedpropagate new information back relevant predecessor states. realworld transition \boring" (the actual result similar predicted result),computation continues deserving part space.Running prioritized sweeping problem Figure 6, see large improvementDyna. optimal policy reached half number steps experienceone-third computation Dyna required (and therefore 20 times fewer stepstwice computational effort Q-learning).258fiReinforcement Learning: Survey5.4 Model-Based MethodsMethods proposed solving MDPs given model used context modelbased methods well.RTDP (real-time dynamic programming) (Barto, Bradtke, & Singh, 1995) anothermodel-based method uses Q-learning concentrate computational effort areasstate-space agent likely occupy. specific problemsagent trying achieve particular goal state reward everywhere else 0.taking account start state, find short path start goal,without necessarily visiting rest state space.Plexus planning system (Dean, Kaelbling, Kirman, & Nicholson, 1993; Kirman,1994) exploits similar intuition. starts making approximate version MDPmuch smaller original one. approximate MDP contains set states,called envelope, includes agent's current state goal state, one.States envelope summarized single \out" state. planningprocess alternation finding optimal policy approximate MDPadding useful states envelope. Action may take place parallel planning,case irrelevant states also pruned envelope.6. Generalizationprevious discussion tacitly assumed possible enumerate stateaction spaces store tables values them. Except small environments,means impractical memory requirements. also makes inecient use experience.large, smooth state space generally expect similar states similar values similar optimal actions. Surely, therefore, compact representationtable. problems continuous large discrete state spaces;large continuous action spaces. problem learning large spaces addressedgeneralization techniques, allow compact storage learned informationtransfer knowledge \similar" states actions.large literature generalization techniques inductive concept learningapplied reinforcement learning. However, techniques often need tailored specificdetails problem. following sections, explore application standardfunction-approximation techniques, adaptive resolution models, hierarchical methodsproblem reinforcement learning.reinforcement-learning architectures algorithms discussed includedstorage variety mappings, including ! (policies), ! < (value functions),! < (Q functions rewards), ! (deterministic transitions),! [0; 1] (transition probabilities). mappings, transitionsimmediate rewards, learned using straightforward supervised learning,handled using wide variety function-approximation techniques supervisedlearning support noisy training examples. Popular techniques include various neuralnetwork methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991).CMAC (Albus, 1981), local memory-based methods (Moore, Atkeson, & Schaal, 1995),generalizations nearest neighbor methods. mappings, especially policy259fiKaelbling, Littman, & Mooremapping, typically need specialized algorithms training sets input-output pairsavailable.6.1 Generalization Inputreinforcement-learning agent's current state plays central role selection rewardmaximizing actions. Viewing agent state-free black box, descriptioncurrent state input. Depending agent architecture, output eitheraction selection, evaluation current state used select action.problem deciding different aspects input affect value outputsometimes called \structural credit-assignment" problem. section examinesapproaches generating actions evaluations function description agent'scurrent state.first group techniques covered specialized case rewarddelayed; second group generally applicable.6.1.1 Immediate Rewardagent's actions uence state transitions, resulting problem becomesone choosing actions maximize immediate reward function agent's currentstate. problems bear resemblance bandit problems discussed Section 2except agent condition action selection current state.reason, class problems described associative reinforcement learning.algorithms section address problem learning immediate booleanreinforcement state vector valued action boolean vector.algorithms used context delayed reinforcement, instance,RL component AHC architecture described Section 4.1. alsogeneralized real-valued reward reward comparison methods (Sutton, 1984).CRBP complementary reinforcement backpropagation algorithm (Ackley & Littman,1990) (crbp) consists feed-forward network mapping encoding stateencoding action. action determined probabilistically activationoutput units: output unit activation yi , bit action vector value1 probability yi , 0 otherwise. neural-network supervised training procedureused adapt network follows. result generating action r = 1,network trained input-output pair hs; ai. result r = 0,network trained input-output pair hs; ai, = (1 , a1; : : :; 1 , ).idea behind training rule whenever action fails generate reward,crbp try generate action different current choice. Althoughseems like algorithm might oscillate action complement,happen. One step training network change action slightly sinceoutput probabilities tend move toward 0.5, makes action selectionrandom increases search. hope random distribution generateaction works better, action reinforced.ARC associative reinforcement comparison (arc) algorithm (Sutton, 1984)instance ahc architecture case boolean actions, consisting two feed260fiReinforcement Learning: Surveyforward networks. One learns value situations, learns policy.simple linear networks hidden units.simplest case, entire system learns optimize immediate reward. First,let us consider behavior network learns policy, mapping vectordescribing 0 1. output unit activation yi , a, action generated,1 + > 0, normal noise, 0 otherwise.adjustment output unit is, simplest case,e = r(a , 1=2) ;first factor reward received taking recent action secondencodes action taken. actions encoded 0 1, , 1=2 alwaysmagnitude; reward action sign, action 1made likely, otherwise action 0 be.described, network tend seek actions given positive reward. extendapproach maximize reward, compare reward baseline, b.changes adjustmente = (r , b)(a , 1=2) ;b output second network. second network trained standardsupervised mode estimate r function input state s.Variations approach used variety applications (Anderson, 1986;Barto et al., 1983; Lin, 1993b; Sutton, 1984).REINFORCE Algorithms Williams (1987, 1992) studied problem choosing actions maximize immedate reward. identified broad class update rules perform gradient descent expected reward showed integrate rulesbackpropagation. class, called reinforce algorithms, includes linear reward-inaction(Section 2.1.3) special case.generic reinforce update parameter wij writtenwij = ffij (r , bij ) @w@ ln(gj )ijffij non-negative factor, r current reinforcement, bij reinforcement baseline,gi probability density function used randomly generate actions based unitactivations. ffij bij take different values wij , however, ffijconstant throughout system, expected update exactly directionexpected reward gradient. Otherwise, update half space gradientnecessarily direction steepest increase.Williams points choice baseline, bij , profound effectconvergence speed algorithm.Logic-Based Methods Another strategy generalization reinforcement learningreduce learning problem associative problem learning boolean functions.boolean function vector boolean inputs single boolean output. Takinginspiration mainstream machine learning work, Kaelbling developed two algorithmslearning boolean functions reinforcement: one uses bias k-DNF drive261fiKaelbling, Littman, & Mooregeneralization process (Kaelbling, 1994b); searches space syntacticdescriptions functions using simple generate-and-test method (Kaelbling, 1994a).restriction single boolean output makes techniques dicult apply.benign learning situations, possible extend approach use collectionlearners independently learn individual bits make complex output.general, however, approach suffers problem unreliable reinforcement:single learner generates inappropriate output bit, learners receive lowreinforcement value. cascade method (Kaelbling, 1993b) allows collection learnerstrained collectively generate appropriate joint outputs; considerablyreliable, require additional computational effort.6.1.2 Delayed RewardAnother method allow reinforcement-learning techniques applied large statespaces modeled value iteration Q-learning. Here, function approximator usedrepresent value function mapping state description value.Many reseachers experimented approach: Boyan Moore (1995) usedlocal memory-based methods conjunction value iteration; Lin (1991) used backpropagation networks Q-learning; Watkins (1989) used CMAC Q-learning; Tesauro (1992,1995) used backpropagation learning value function backgammon (describedSection 8.1); Zhang Dietterich (1995) used backpropagation TD() learn goodstrategies job-shop scheduling.Although positive examples, general unfortunate interactions function approximation learning rules. discrete environmentsguarantee operation updates value function (accordingBellman equations) reduce error current value functionoptimal value function. guarantee longer holds generalization used.issues discussed Boyan Moore (1995), give simple examples valuefunction errors growing arbitrarily large generalization used value iteration.solution this, applicable certain classes problems, discourages divergence permitting updates whose estimated values shown near-optimalvia battery Monte-Carlo experiments.Thrun Schwartz (1993) theorize function approximation value functionsalso dangerous errors value functions due generalization becomecompounded \max" operator definition value function.Several recent results (Gordon, 1995; Tsitsiklis & Van Roy, 1996) show appropriate choice function approximator guarantee convergence, though necessarilyoptimal values. Baird's residual gradient technique (Baird, 1995) provides guaranteedconvergence locally optimal solutions.Perhaps gloominess counter-examples misplaced. Boyan Moore (1995)report counter-examples made work problem-specific hand-tuningdespite unreliability untuned algorithms provably converge discrete domains.Sutton (1996) shows modified versions Boyan Moore's examples convergesuccessfully. open question whether general principles, ideally supported theory,help us understand value function approximation succeed. Sutton's com262fiReinforcement Learning: Surveyparative experiments Boyan Moore's counter-examples, changes four aspectsexperiments:1. Small changes task specifications.2. different kind function approximator (CMAC (Albus, 1975)) weakgeneralization.3. different learning algorithm: SARSA (Rummery & Niranjan, 1994) instead valueiteration.4. different training regime. Boyan Moore sampled states uniformly state space,whereas Sutton's method sampled along empirical trajectories.intuitive reasons believe fourth factor particularly important,careful research needed.Adaptive Resolution Models many cases, would like partitionenvironment regions states considered purposeslearning generating actions. Without detailed prior knowledge environment,dicult know granularity placement partitions appropriate.problem overcome methods use adaptive resolution; course learning,partition constructed appropriate environment.Decision Trees environments characterized set boolean discretevalued variables, possible learn compact decision trees representing Q values.G-learning algorithm (Chapman & Kaelbling, 1991), works follows. starts assumingpartitioning necessary tries learn Q values entire environmentone state. parallel process, gathers statistics based individualinput bits; asks question whether bit b state descriptionQ values states b = 1 significantly different Q valuesstates b = 0. bit found, used split decision tree. Then,process repeated leaves. method able learn smallrepresentations Q function presence overwhelming number irrelevant,noisy state attributes. outperformed Q-learning backpropagation simple videogame environment used McCallum (1995) (in conjunction techniquesdealing partial observability) learn behaviors complex driving-simulator.cannot, however, acquire partitions attributes significant combination(such needed solve parity problems).Variable Resolution Dynamic Programming VRDP algorithm (Moore, 1991)enables conventional dynamic programming performed real-valued multivariatestate-spaces straightforward discretization would fall prey curse dimensionality. kd-tree (similar decision tree) used partition state space coarseregions. coarse regions refined detailed regions, parts statespace predicted important. notion importance obtained running \mental trajectories" state space. algorithm proved effective numberproblems full high-resolution arrays would impractical.disadvantage requiring guess initially valid trajectory state-space.263fiKaelbling, Littman, & Moore(a)(b)(c)GGGGoalStartFigure 7: (a) two-dimensional maze problem. point robot must find pathstart goal without crossing barrier lines. (b) path takenPartiGame entire first trial. begins intense exploration findroute almost entirely enclosed start region. eventually reachedsuciently high resolution, discovers gap proceeds greedily towardsgoal, temporarily blocked goal's barrier region. (c)second trial.PartiGame Algorithm Moore's PartiGame algorithm (Moore, 1994) another solutionproblem learning achieve goal configurations deterministic high-dimensionalcontinuous spaces learning adaptive-resolution model. also divides environmentcells; cell, actions available consist aiming neighboring cells(this aiming accomplished local controller, must provided partproblem statement). graph cell transitions solved shortest paths onlineincremental manner, minimax criterion used detect group cellscoarse prevent movement obstacles avoid limit cycles. offendingcells split higher resolution. Eventually, environment divided enoughchoose appropriate actions achieving goal, unnecessary distinctions made.important feature that, well reducing memory computational requirements,also structures exploration state space multi-resolution manner. Given failure,agent initially try something different rectify failure, resortsmall local changes qualitatively different strategies exhausted.Figure 7a shows two-dimensional continuous maze. Figure 7b shows performancerobot using PartiGame algorithm first trial. Figure 7c showssecond trial, started slightly different position.fast algorithm, learning policies spaces nine dimensions lessminute. restriction current implementation deterministic environmentslimits applicability, however. McCallum (1995) suggests related tree-structuredmethods.264fiReinforcement Learning: Survey6.2 Generalization Actionsnetworks described Section 6.1.1 generalize state descriptions presentedinputs. also produce outputs discrete, factored representation thus couldseen generalizing actions well.cases actions described combinatorially, importantgeneralize actions avoid keeping separate statistics huge number actionschosen. continuous action spaces, need generalization evenpronounced.estimating Q values using neural network, possible use either distinctnetwork action, network distinct output action.action space continuous, neither approach possible. alternative strategy usesingle network state action input Q value output. Trainingnetwork conceptually dicult, using network find optimal actionchallenge. One method local gradient-ascent search actionorder find one high value (Baird & Klopf, 1993).Gullapalli (1990, 1992) developed \neural" reinforcement-learning unit usecontinuous action spaces. unit generates actions normal distribution; adjustsmean variance based previous experience. chosen actionsperforming well, variance high, resulting exploration range choices.action performs well, mean moved direction variance decreased,resulting tendency generate action values near successful one. methodsuccessfully employed learn control robot arm many continuous degreesfreedom.6.3 Hierarchical MethodsAnother strategy dealing large state spaces treat hierarchylearning problems. many cases, hierarchical solutions introduce slight sub-optimalityperformance, potentially gain good deal eciency execution time, learning time,space.Hierarchical learners commonly structured gated behaviors, shown Figure 8.collection behaviors map environment states low-level actionsgating function decides, based state environment, behavior'sactions switched actually executed. Maes Brooks (1990) usedversion architecture individual behaviors fixed priorigating function learned reinforcement. Mahadevan Connell (1991b) useddual approach: fixed gating function, supplied reinforcement functionsindividual behaviors, learned. Lin (1993a) Dorigo Colombetti (1995,1994) used approach, first training behaviors training gatingfunction. Many hierarchical learning methods cast framework.6.3.1 Feudal Q-learningFeudal Q-learning (Dayan & Hinton, 1993; Watkins, 1989) involves hierarchy learningmodules. simplest case, high-level master low-level slave. masterreceives reinforcement external environment. actions consist commands265fiKaelbling, Littman, & Mooreb1b2gb3Figure 8: structure gated behaviors.give low-level learner. master generates particular commandslave, must reward slave taking actions satisfy command, evenresult external reinforcement. master, then, learns mapping statescommands. slave learns mapping commands states external actions.set \commands" associated reinforcement functions established advancelearning.really instance general \gated behaviors" approach, slaveexecute behaviors depending command. reinforcement functionsindividual behaviors (commands) given, learning takes place simultaneouslyhigh low levels.6.3.2 Compositional Q-learningSingh's compositional Q-learning (1992b, 1992a) (C-QL) consists hierarchy basedtemporal sequencing subgoals. elemental tasks behaviors achieverecognizable condition. high-level goal system achieve set conditions sequential order. achievement conditions provides reinforcementelemental tasks, trained first achieve individual subgoals. Then, gatingfunction learns switch elemental tasks order achieve appropriate high-levelsequential goal. method used Tham Prager (1994) learn controlsimulated multi-link robot arm.6.3.3 Hierarchical Distance GoalEspecially consider reinforcement learning modules part larger agent architectures, important consider problems goals dynamically inputlearner. Kaelbling's HDG algorithm (1993a) uses hierarchical approach solving problems goals achievement (the agent get particular state quicklypossible) given agent dynamically.HDG algorithm works analogy navigation harbor. environmentpartitioned (a priori, recent work (Ashar, 1994) addresses case learningpartition) set regions whose centers known \landmarks." agent266fiReinforcement Learning: Surveyoffice1/52/52/5hallhall+100printerFigure 9: example partially observable environment.currently region goal, uses low-level actions move goal.not, high-level information used determine next landmark shortestpath agent's closest landmark goal's closest landmark. Then, agent useslow-level information aim toward next landmark. errors action cause deviationspath, problem; best aiming point recomputed every step.7. Partially Observable Environmentsmany real-world environments, possible agent perfectcomplete perception state environment. Unfortunately, complete observabilitynecessary learning methods based MDPs. section, consider caseagent makes observations state environment, observationsmay noisy provide incomplete information. case robot, instance,might observe whether corridor, open room, T-junction, etc.,observations might error-prone. problem also referred problem\incomplete perception," \perceptual aliasing," \hidden state."section, consider extensions basic MDP framework solvingpartially observable problems. resulting formal model called partially observableMarkov decision process POMDP.7.1 State-Free Deterministic Policiesnaive strategy dealing partial observability ignore it. is,treat observations states environment try learnbehave. Figure 9 shows simple environment agent attempting getprinter oce. moves oce, good chance agentend one two places look like \hall", require different actionsgetting printer. consider states same, agent cannotpossibly behave optimally. well do?resulting problem Markovian, Q-learning cannot guaranteed converge. Small breaches Markov requirement well handled Q-learning,possible construct simple environments cause Q-learning oscillate (Chrisman &267fiKaelbling, Littman, & MooreLittman, 1993). possible use model-based approach, however; act accordingpolicy gather statistics transitions observations, solveoptimal policy based observations. Unfortunately, environmentMarkovian, transition probabilities depend policy executed, newpolicy induce new set transition probabilities. approach may yield plausibleresults cases, again, guarantees.reasonable, though, ask optimal policy (mapping observationsactions, case) is. NP-hard (Littman, 1994b) find mapping, evenbest mapping poor performance. case agent trying getprinter, instance, deterministic state-free policy takes infinite number stepsreach goal average.7.2 State-Free Stochastic Policiesimprovement gained considering stochastic policies; mappingsobservations probability distributions actions. randomnessagent's actions, get stuck hall forever. Jaakkola, Singh, Jordan (1995)developed algorithm finding locally-optimal stochastic policies, findingglobally optimal policy still NP hard.example, turns optimal stochastic policyp agent,2 0:6 weststatelookslikehall,goeastprobability2,pprobability 2 , 1 0:4. policy found solving simple (in case)quadratic program. fact simple example produce irrational numbersgives indication dicult problem solve exactly.7.3 Policies Internal Stateway behave truly effectively wide-range environments use memoryprevious actions observations disambiguate current state. varietyapproaches learning policies internal state.Recurrent Q-learning One intuitively simple approach use recurrent neural network learn Q values. network trained using backpropagation time (orsuitable technique) learns retain \history features" predict value.approach used number researchers (Meeden, McGraw, & Blank, 1993; Lin& Mitchell, 1992; Schmidhuber, 1991b). seems work effectively simple problems,suffer convergence local optima complex problems.Classifier Systems Classifier systems (Holland, 1975; Goldberg, 1989) explicitlydeveloped solve problems delayed reward, including requiring short-termmemory. internal mechanism typically used pass reward back chainsdecisions, called bucket brigade algorithm, bears close resemblance Q-learning.spite early successes, original design appear handle partially observed environments robustly.Recently, approach reexamined using insights reinforcementlearning literature, success. Dorigo comparative study Q-learningclassifier systems (Dorigo & Bersini, 1994). Cliff Ross (1994) start Wilson's zeroth268fiReinforcement Learning: SurveySEbFigure 10: Structure POMDP agent.level classifier system (Wilson, 1995) add one two-bit memory registers. findthat, although system learn use short-term memory registers effectively,approach unlikely scale complex environments.Dorigo Colombetti applied classifier systems moderately complex problemlearning robot behavior immediate reinforcement (Dorigo, 1995; Dorigo & Colombetti,1994).Finite-history-window Approach One way restore Markov property allowdecisions based history recent observations perhaps actions. LinMitchell (1992) used fixed-width finite history window learn pole balancing task.McCallum (1995) describes \utile sux memory" learns variable-width windowserves simultaneously model environment finite-memory policy.system excellent results complex driving-simulation domain (McCallum,1995). Ring (1994) neural-network approach uses variable history window,adding history necessary disambiguate situations.POMDP Approach Another strategy consists using hidden Markov model (HMM)techniques learn model environment, including hidden state, usemodel construct perfect memory controller (Cassandra, Kaelbling, & Littman, 1994;Lovejoy, 1991; Monahan, 1982).Chrisman (1992) showed forward-backward algorithm learning HMMs couldadapted learning POMDPs. He, later McCallum (1993), also gave heuristic statesplitting rules attempt learn smallest possible model given environment.resulting model used integrate information agent's observationsorder make decisions.Figure 10 illustrates basic structure perfect-memory controller. componentleft state estimator, computes agent's belief state, b functionold belief state, last action a, current observation i. context, beliefstate probability distribution states environment, indicating likelihood,given agent's past experience, environment actually states.state estimator constructed straightforwardly using estimated world modelBayes' rule.left problem finding policy mapping belief states action.problem formulated MDP, dicult solve using techniquesdescribed earlier, input space continuous. Chrisman's approach (1992)take account future uncertainty, yields policy small amount computation. standard approach operations-research literature solve269fiKaelbling, Littman, & Mooreoptimal policy (or close approximation thereof) based representation piecewiselinear convex function belief space. method computationally intractable,may serve inspiration methods make approximations (Cassandraet al., 1994; Littman, Cassandra, & Kaelbling, 1995a).8. Reinforcement Learning ApplicationsOne reason reinforcement learning popular serves theoretical toolstudying principles agents learning act. unsurprising alsoused number researchers practical computational tool constructingautonomous systems improve experience. applicationsranged robotics, industrial manufacturing, combinatorial search problemscomputer game playing.Practical applications provide test ecacy usefulness learning algorithms.also inspiration deciding components reinforcement learningframework practical importance. example, researcher real robotic taskprovide data point questions as:important optimal exploration? break learning period exploration phases exploitation phases?useful model long-term reward: Finite horizon? Discounted?Infinite horizon?much computation available agent decisionsused?prior knowledge build system, algorithms capableusing knowledge?Let us examine set practical applications reinforcement learning, bearingquestions mind.8.1 Game PlayingGame playing dominated Artificial Intelligence world problem domain ever sincefield born. Two-player games fit established reinforcement-learningframework since optimality criterion games one maximizing rewardface fixed environment, one maximizing reward optimal adversary(minimax). Nonetheless, reinforcement-learning algorithms adapted workgeneral class games (Littman, 1994a) many researchers used reinforcementlearning environments. One application, spectacularly far ahead time,Samuel's checkers playing system (Samuel, 1959). learned value function representedlinear function approximator, employed training scheme similar updatesused value iteration, temporal differences Q-learning.recently, Tesauro (1992, 1994, 1995) applied temporal difference algorithmbackgammon. Backgammon approximately 1020 states, making table-based reinforcement learning impossible. Instead, Tesauro used backpropagation-based three-layer270fiReinforcement Learning: SurveyTrainingGamesHiddenUnits300,00080TD 2.0800,00040TD 2.11,500,00080BasicTD 1.0ResultsPoorLost 13 points 51gamesLost 7 points 38gamesLost 1 point 40gamesTable 2: TD-Gammon's performance games top human professional players.backgammon tournament involves playing series games points oneplayer reaches set target. TD-Gammon none tournaments camesuciently close considered one best players world.neural network function approximator value functionBoard Position ! Probability victory current player:Two versions learning algorithm used. first, call Basic TDGammon, used little predefined knowledge game, representationboard position virtually raw encoding, suciently powerful permit neuralnetwork distinguish conceptually different positions. second, TD-Gammon,provided raw state information supplemented number handcrafted features backgammon board positions. Providing hand-crafted featuresmanner good example inductive biases human knowledge tasksupplied learning algorithm.training learning algorithms required several months computer time,achieved constant self-play. exploration strategy used|the system alwaysgreedily chose move largest expected probability victory. naive exploration strategy proved entirely adequate environment, perhaps surprisinggiven considerable work reinforcement-learning literature producednumerous counter-examples show greedy exploration lead poor learning performance. Backgammon, however, two important properties. Firstly, whatever policyfollowed, every game guaranteed end finite time, meaning useful rewardinformation obtained fairly frequently. Secondly, state transitions sucientlystochastic independent policy, states occasionally visited|a wronginitial value function little danger starving us visiting critical part statespace important information could obtained.results (Table 2) TD-Gammon impressive. competed toplevel international human play. Basic TD-Gammon played respectably,professional standard.271fiFigure 11: Schaal Atkeson's devil-sticking robot. tapered stick hit alternatelytwo hand sticks. task keep devil stick fallingmany hits possible. robot three motors indicated torquevectors 1; 2; 3.Although experiments games cases produced interesting learningbehavior, success close TD-Gammon repeated. gamesstudied include Go (Schraudolph, Dayan, & Sejnowski, 1994) Chess (Thrun,1995). still open question success TD-Gammonrepeated domains.8.2 Robotics Controlrecent years many robotics control applications usedreinforcement learning. concentrate following four examples, althoughmany interesting ongoing robotics investigations underway.1. Schaal Atkeson (1994) constructed two-armed robot, shown Figure 11,learns juggle device known devil-stick. complex non-linear controltask involving six-dimensional state space less 200 msecs per control decision. 40 initial attempts robot learns keep juggling hundredshits. typical human learning task requires order magnitude practiceachieve proficiency mere tens hits.juggling robot learned world model experience, generalizedunvisited states function approximation scheme known locally weightedregression (Cleveland & Delvin, 1988; Moore & Atkeson, 1992). trial,form dynamic programming specific linear control policies locally lineartransitions used improve policy. form dynamic programmingknown linear-quadratic-regulator design (Sage & White, 1977).272fiReinforcement Learning: Survey2. Mahadevan Connell (1991a) discuss task mobile robot pushes largeboxes extended periods time. Box-pushing well-known dicult roboticsproblem, characterized immense uncertainty results actions. Q-learningused conjunction novel clustering techniques designed enablehigher-dimensional input tabular approach would permitted. robotlearned perform competitively performance human-programmed solution. Another aspect work, mentioned Section 6.3, pre-programmedbreakdown monolithic task description set lower level taskslearned.3. Mataric (1994) describes robotics experiment with, viewpoint theoretical reinforcement learning, unthinkably high dimensional state space, containingmany dozens degrees freedom. Four mobile robots traveled within enclosure collecting small disks transporting destination region.three enhancements basic Q-learning algorithm. Firstly, pre-programmed signals called progress estimators used break monolithic task subtasks.achieved robust manner robots forced useestimators, freedom profit inductive bias provided.Secondly, control decentralized. robot learned policy independentlywithout explicit communication others. Thirdly, state space brutallyquantized small number discrete states according values small number pre-programmed boolean features underlying sensors. performanceQ-learned policies almost good simple hand-crafted controllerjob.4. Q-learning used elevator dispatching task (Crites & Barto, 1996).problem, implemented simulation stage, involved fourelevators servicing ten oors. objective minimize average squaredwait time passengers, discounted future time. problem poseddiscrete Markov system, 1022 states even simplified versionproblem. Crites Barto used neural networks function approximationprovided excellent comparison study Q-learning approachpopular sophisticated elevator dispatching algorithms. squared waittime controller approximately 7% less best alternative algorithm(\Empty System" heuristic receding horizon controller) less halfsquared wait time controller frequently used real elevator systems.5. final example concerns application reinforcement learning oneauthors survey packaging task food processing industry.problem involves filling containers variable numbers non-identical products.product characteristics also vary time, sensed. Dependingtask, various constraints placed container-filling procedure.three examples:mean weight containers produced shift mustmanufacturer's declared weight W .273fiKaelbling, Littman, & Moorenumber containers declared weight must less P %.containers may produced weight W 0.tasks controlled machinery operates according various setpoints.Conventional practice setpoints chosen human operators, choiceeasy dependent current product characteristics currenttask constraints. dependency often dicult model highly non-linear.task posed finite-horizon Markov decision task statesystem function product characteristics, amount time remainingproduction shift mean wastage percent declared shiftfar. system discretized 200,000 discrete states local weightedregression used learn generalize transition model. Prioritized sweeping used maintain optimal value function new piece transitioninformation obtained. simulated experiments savings considerable,typically wastage reduced factor ten. Since systemdeployed successfully several factories within United States.interesting aspects practical reinforcement learning come lightexamples. striking cases, make real system work provednecessary supplement fundamental algorithm extra pre-programmed knowledge.Supplying extra knowledge comes price: human effort insight requiredsystem subsequently less autonomous. also clear tasksthese, knowledge-free approach would achieved worthwhile performance withinfinite lifetime robots.forms pre-programmed knowledge take? included assumptionlinearity juggling robot's policy, manual breaking task subtaskstwo mobile-robot examples, box-pusher also used clustering techniqueQ values assumed locally consistent Q values. four disk-collecting robotsadditionally used manually discretized state space. packaging example far fewerdimensions required correspondingly weaker assumptions, there, too, assumption local piecewise continuity transition model enabled massive reductionsamount learning data required.exploration strategies interesting too. juggler used careful statistical analysis judge profitably experiment. However, mobile robot applicationsable learn well greedy exploration|always exploiting without deliberate exploration. packaging task used optimism face uncertainty. Nonestrategies mirrors theoretically optimal (but computationally intractable) exploration,yet proved adequate.Finally, also worth considering computational regimes experiments.different, indicates differing computational demandsvarious reinforcement learning algorithms indeed array differing applications.juggler needed make fast decisions low latency hit,long periods (30 seconds more) trial consolidate experiencescollected previous trial perform aggressive computation necessaryproduce new reactive controller next trial. box-pushing robot meant274fiReinforcement Learning: Surveyoperate autonomously hours make decisions uniform length controlcycle. cycle suciently long quite substantial computations beyond simple Qlearning backups. four disk-collecting robots particularly interesting. robotshort life less 20 minutes (due battery constraints) meaning substantialnumber crunching impractical, significant combinatorial search wouldused significant fraction robot's learning lifetime. packaging task easyconstraints. One decision needed every minutes. provided opportunitiesfully computing optimal value function 200,000-state system everycontrol cycle, addition performing massive cross-validation-based optimizationtransition model learned.great deal work currently progress practical implementationsreinforcement learning. insights task constraints produceimportant effect shaping kind algorithms developed future.9. Conclusionsvariety reinforcement-learning techniques work effectively varietysmall problems. techniques scale well larger problems.researchers done bad job inventing learning techniques,dicult solve arbitrary problems general case. order solve highlycomplex problems, must give tabula rasa learning techniques begin incorporatebias give leverage learning process.necessary bias come variety forms, including following:shaping: technique shaping used training animals (Hilgard & Bower, 1975);teacher presents simple problems solve first, gradually exposes learnercomplex problems. Shaping used supervised-learning systems,used train hierarchical reinforcement-learning systems bottom(Lin, 1991), alleviate problems delayed reinforcement decreasingdelay problem well understood (Dorigo & Colombetti, 1994; Dorigo, 1995).local reinforcement signals: Whenever possible, agents given reinforcementsignals local. applications possible compute gradient,rewarding agent taking steps gradient, rather achievingfinal goal, speed learning significantly (Mataric, 1994).imitation: agent learn \watching" another agent perform task (Lin, 1991).real robots, requires perceptual abilities yet available.another strategy human supply appropriate motor commands robotjoystick steering wheel (Pomerleau, 1993).problem decomposition: Decomposing huge learning problem collection smallerones, providing useful reinforcement signals subproblems powerful technique biasing learning. interesting examples robotic reinforcementlearning employ technique extent (Connell & Mahadevan, 1993).exes: One thing keeps agents know nothing learning anythinghard time even finding interesting parts space; wander275fiKaelbling, Littman, & Moorearound random never getting near goal, always \killed" immediately.problems ameliorated programming set \re exes" causeagent act initially way reasonable (Mataric, 1994; Singh, Barto,Grupen, & Connolly, 1994). exes eventually overriddendetailed accurate learned knowledge, least keep agent alivepointed right direction trying learn. Recent work Millan (1996)explores use exes make robot learning safer ecient.appropriate biases, supplied human programmers teachers, complex reinforcementlearning problems eventually solvable. still much work done manyinteresting questions remaining learning techniques especially regarding methodsapproximating, decomposing, incorporating bias problems.AcknowledgementsThanks Marco Dorigo three anonymous reviewers comments helpedimprove paper. Also thanks many colleagues reinforcement-learningcommunity done work explained us.Leslie Pack Kaelbling supported part NSF grants IRI-9453383 IRI9312395. Michael Littman supported part Bellcore. Andrew Moore supportedpart NSF Research Initiation Award 3M Corporation.ReferencesAckley, D. H., & Littman, M. L. (1990). Generalization scaling reinforcement learning. Touretzky, D. S. (Ed.), Advances Neural Information Processing Systems2, pp. 550{557 San Mateo, CA. Morgan Kaufmann.Albus, J. S. (1975). new approach manipulator control: Cerebellar model articulationcontroller (cmac). Journal Dynamic Systems, Measurement Control, 97, 220{227.Albus, J. S. (1981). Brains, Behavior, Robotics. BYTE Books, Subsidiary McGrawHill, Peterborough, New Hampshire.Anderson, C. W. (1986). Learning Problem Solving Multilayer ConnectionistSystems. Ph.D. thesis, University Massachusetts, Amherst, MA.Ashar, R. R. (1994). Hierarchical learning stochastic domains. Master's thesis, BrownUniversity, Providence, Rhode Island.Baird, L. (1995). Residual algorithms: Reinforcement learning function approximation. Prieditis, A., & Russell, S. (Eds.), Proceedings Twelfth InternationalConference Machine Learning, pp. 30{37 San Francisco, CA. Morgan Kaufmann.Baird, L. C., & Klopf, A. H. (1993). Reinforcement learning high-dimensional, continuous actions. Tech. rep. WL-TR-93-1147, Wright-Patterson Air Force Base Ohio:Wright Laboratory.276fiReinforcement Learning: SurveyBarto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamicprogramming. Artificial Intelligence, 72 (1), 81{138.Barto, A. G., Sutton, R. S., & Anderson, C. W. (1983). Neuronlike adaptive elementssolve dicult learning control problems. IEEE Transactions Systems, Man,Cybernetics, SMC-13 (5), 834{846.Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ.Berenji, H. R. (1991). Artificial neural networks approximate reasoning intelligentcontrol space. American Control Conference, pp. 1075{1080.Berry, D. A., & Fristedt, B. (1985). Bandit Problems: Sequential Allocation Experiments.Chapman Hall, London, UK.Bertsekas, D. P. (1987). Dynamic Programming: Deterministic Stochastic Models.Prentice-Hall, Englewood Cliffs, NJ.Bertsekas, D. P. (1995). Dynamic Programming Optimal Control. Athena Scientific,Belmont, Massachusetts. Volumes 1 2.Bertsekas, D. P., & Casta~non, D. A. (1989). Adaptive aggregation infinite horizondynamic programming. IEEE Transactions Automatic Control, 34 (6), 589{598.Bertsekas, D. P., & Tsitsiklis, J. N. (1989). Parallel Distributed Computation: Numerical Methods. Prentice-Hall, Englewood Cliffs, NJ.Box, G. E. P., & Draper, N. R. (1987). Empirical Model-Building Response Surfaces.Wiley.Boyan, J. A., & Moore, A. W. (1995). Generalization reinforcement learning: Safelyapproximating value function. Tesauro, G., Touretzky, D. S., & Leen, T. K.(Eds.), Advances Neural Information Processing Systems 7 Cambridge, MA.MIT Press.Burghes, D., & Graham, A. (1980). Introduction Control Theory including OptimalControl. Ellis Horwood.Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally partiallyobservable stochastic domains. Proceedings Twelfth National ConferenceArtificial Intelligence Seattle, WA.Chapman, D., & Kaelbling, L. P. (1991). Input generalization delayed reinforcementlearning: algorithm performance comparisons. Proceedings International Joint Conference Artificial Intelligence Sydney, Australia.Chrisman, L. (1992). Reinforcement learning perceptual aliasing: perceptualdistinctions approach. Proceedings Tenth National Conference ArtificialIntelligence, pp. 183{188 San Jose, CA. AAAI Press.277fiKaelbling, Littman, & MooreChrisman, L., & Littman, M. (1993). Hidden state short-term memory.. PresentationReinforcement Learning Workshop, Machine Learning Conference.Cichosz, P., & Mulawka, J. J. (1995). Fast ecient reinforcement learning truncated temporal differences. Prieditis, A., & Russell, S. (Eds.), ProceedingsTwelfth International Conference Machine Learning, pp. 99{107 San Francisco,CA. Morgan Kaufmann.Cleveland, W. S., & Delvin, S. J. (1988). Locally weighted regression: approachregression analysis local fitting. Journal American Statistical Association,83 (403), 596{610.Cliff, D., & Ross, S. (1994). Adding temporary memory ZCS. Adaptive Behavior, 3 (2),101{150.Condon, A. (1992). complexity stochastic games. Information Computation,96 (2), 203{224.Connell, J., & Mahadevan, S. (1993). Rapid task learning real robots. Robot Learning.Kluwer Academic Publishers.Crites, R. H., & Barto, A. G. (1996). Improving elevator performance using reinforcementlearning. Touretzky, D., Mozer, M., & Hasselmo, M. (Eds.), Neural InformationProcessing Systems 8.Dayan, P. (1992). convergence TD() general . Machine Learning, 8 (3), 341{362.Dayan, P., & Hinton, G. E. (1993). Feudal reinforcement learning. Hanson, S. J., Cowan,J. D., & Giles, C. L. (Eds.), Advances Neural Information Processing Systems 5San Mateo, CA. Morgan Kaufmann.Dayan, P., & Sejnowski, T. J. (1994). TD() converges probability 1. Machine Learning, 14 (3).Dean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. (1993). Planning deadlinesstochastic domains. Proceedings Eleventh National Conference ArtificialIntelligence Washington, DC.D'Epenoux, F. (1963). probabilistic production inventory problem. ManagementScience, 10, 98{108.Derman, C. (1970). Finite State Markovian Decision Processes. Academic Press, New York.Dorigo, M., & Bersini, H. (1994). comparison q-learning classifier systems.Animals Animats: Proceedings Third International ConferenceSimulation Adaptive Behavior Brighton, UK.Dorigo, M., & Colombetti, M. (1994). Robot shaping: Developing autonomous agentslearning. Artificial Intelligence, 71 (2), 321{370.278fiReinforcement Learning: SurveyDorigo, M. (1995). Alecsys AutonoMouse: Learning control real robotdistributed classifier systems. Machine Learning, 19.Fiechter, C.-N. (1994). Ecient reinforcement learning. Proceedings SeventhAnnual ACM Conference Computational Learning Theory, pp. 88{97. AssociationComputing Machinery.Gittins, J. C. (1989). Multi-armed Bandit Allocation Indices. Wiley-Interscience seriessystems optimization. Wiley, Chichester, NY.Goldberg, D. (1989). Genetic algorithms search, optimization, machine learning.Addison-Wesley, MA.Gordon, G. J. (1995). Stable function approximation dynamic programming. Prieditis, A., & Russell, S. (Eds.), Proceedings Twelfth International ConferenceMachine Learning, pp. 261{268 San Francisco, CA. Morgan Kaufmann.Gullapalli, V. (1990). stochastic reinforcement learning algorithm learning real-valuedfunctions. Neural Networks, 3, 671{692.Gullapalli, V. (1992). Reinforcement learning application control. Ph.D. thesis,University Massachusetts, Amherst, MA.Hilgard, E. R., & Bower, G. H. (1975). Theories Learning (fourth edition). Prentice-Hall,Englewood Cliffs, NJ.Hoffman, A. J., & Karp, R. M. (1966). nonterminating stochastic games. ManagementScience, 12, 359{370.Holland, J. H. (1975). Adaptation Natural Artificial Systems. University MichiganPress, Ann Arbor, MI.Howard, R. A. (1960). Dynamic Programming Markov Processes. MIT Press,Cambridge, MA.Jaakkola, T., Jordan, M. I., & Singh, S. P. (1994). convergence stochastic iterativedynamic programming algorithms. Neural Computation, 6 (6).Jaakkola, T., Singh, S. P., & Jordan, M. I. (1995). Monte-carlo reinforcement learningnon-Markovian decision problems. Tesauro, G., Touretzky, D. S., & Leen, T. K.(Eds.), Advances Neural Information Processing Systems 7 Cambridge, MA.MIT Press.Kaelbling, L. P. (1993a). Hierarchical learning stochastic domains: Preliminary results.Proceedings Tenth International Conference Machine Learning Amherst,MA. Morgan Kaufmann.Kaelbling, L. P. (1993b). Learning Embedded Systems. MIT Press, Cambridge, MA.Kaelbling, L. P. (1994a). Associative reinforcement learning: generate test algorithm.Machine Learning, 15 (3).279fiKaelbling, Littman, & MooreKaelbling, L. P. (1994b). Associative reinforcement learning: Functions k-DNF. MachineLearning, 15 (3).Kirman, J. (1994). Predicting Real-Time Planner Performance Domain Characterization.Ph.D. thesis, Department Computer Science, Brown University.Koenig, S., & Simmons, R. G. (1993). Complexity analysis real-time reinforcementlearning. Proceedings Eleventh National Conference Artificial Intelligence,pp. 99{105 Menlo Park, California. AAAI Press/MIT Press.Kumar, P. R., & Varaiya, P. P. (1986). Stochastic Systems: Estimation, Identification,Adaptive Control. Prentice Hall, Englewood Cliffs, New Jersey.Lee, C. C. (1991). self learning rule-based controller employing approximate reasoningneural net concepts. International Journal Intelligent Systems, 6 (1), 71{93.Lin, L.-J. (1991). Programming robots using reinforcement learning teaching.Proceedings Ninth National Conference Artificial Intelligence.Lin, L.-J. (1993a). Hierachical learning robot skills reinforcement. ProceedingsInternational Conference Neural Networks.Lin, L.-J. (1993b). Reinforcement Learning Robots Using Neural Networks. Ph.D. thesis,Carnegie Mellon University, Pittsburgh, PA.Lin, L.-J., & Mitchell, T. M. (1992). Memory approaches reinforcement learning nonMarkovian domains. Tech. rep. CMU-CS-92-138, Carnegie Mellon University, SchoolComputer Science.Littman, M. L. (1994a). Markov games framework multi-agent reinforcement learning. Proceedings Eleventh International Conference Machine Learning,pp. 157{163 San Francisco, CA. Morgan Kaufmann.Littman, M. L. (1994b). Memoryless policies: Theoretical limitations practical results.Cliff, D., Husbands, P., Meyer, J.-A., & Wilson, S. W. (Eds.), AnimalsAnimats 3: Proceedings Third International Conference SimulationAdaptive Behavior Cambridge, MA. MIT Press.Littman, M. L., Cassandra, A., & Kaelbling, L. P. (1995a). Learning policies partiallyobservable environments: Scaling up. Prieditis, A., & Russell, S. (Eds.), Proceedings Twelfth International Conference Machine Learning, pp. 362{370 SanFrancisco, CA. Morgan Kaufmann.Littman, M. L., Dean, T. L., & Kaelbling, L. P. (1995b). complexity solvingMarkov decision problems. Proceedings Eleventh Annual ConferenceUncertainty Artificial Intelligence (UAI{95) Montreal, Quebec, Canada.Lovejoy, W. S. (1991). survey algorithmic methods partially observable Markovdecision processes. Annals Operations Research, 28, 47{66.280fiReinforcement Learning: SurveyMaes, P., & Brooks, R. A. (1990). Learning coordinate behaviors. Proceedings EighthNational Conference Artificial Intelligence, pp. 796{802. Morgan Kaufmann.Mahadevan, S. (1994). discount discount reinforcement learning: casestudy comparing R learning Q learning. Proceedings Eleventh International Conference Machine Learning, pp. 164{172 San Francisco, CA. MorganKaufmann.Mahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms,empirical results. Machine Learning, 22 (1).Mahadevan, S., & Connell, J. (1991a). Automatic programming behavior-based robotsusing reinforcement learning. Proceedings Ninth National ConferenceArtificial Intelligence Anaheim, CA.Mahadevan, S., & Connell, J. (1991b). Scaling reinforcement learning robotics exploiting subsumption architecture. Proceedings Eighth InternationalWorkshop Machine Learning, pp. 328{332.Mataric, M. J. (1994). Reward functions accelerated learning. Cohen, W. W., &Hirsh, H. (Eds.), Proceedings Eleventh International Conference MachineLearning. Morgan Kaufmann.McCallum, A. K. (1995). Reinforcement Learning Selective Perception HiddenState. Ph.D. thesis, Department Computer Science, University Rochester.McCallum, R. A. (1993). Overcoming incomplete perception utile distinction memory.Proceedings Tenth International Conference Machine Learning, pp. 190{196 Amherst, Massachusetts. Morgan Kaufmann.McCallum, R. A. (1995). Instance-based utile distinctions reinforcement learninghidden state. Proceedings Twelfth International Conference Machine Learning, pp. 387{395 San Francisco, CA. Morgan Kaufmann.Meeden, L., McGraw, G., & Blank, D. (1993). Emergent control planning autonomous vehicle. Touretsky, D. (Ed.), Proceedings Fifteenth Annual MeetingCognitive Science Society, pp. 735{740. Lawerence Erlbaum Associates, Hillsdale, NJ.Millan, J. d. R. (1996). Rapid, safe, incremental learning navigation strategies. IEEETransactions Systems, Man, Cybernetics, 26 (3).Monahan, G. E. (1982). survey partially observable Markov decision processes: Theory,models, algorithms. Management Science, 28, 1{16.Moore, A. W. (1991). Variable resolution dynamic programming: Eciently learning action maps multivariate real-valued spaces. Proc. Eighth International MachineLearning Workshop.281fiKaelbling, Littman, & MooreMoore, A. W. (1994). parti-game algorithm variable resolution reinforcement learning multidimensional state-spaces. Cowan, J. D., Tesauro, G., & Alspector, J.(Eds.), Advances Neural Information Processing Systems 6, pp. 711{718 San Mateo,CA. Morgan Kaufmann.Moore, A. W., & Atkeson, C. G. (1992). investigation memory-based function approximators learning control. Tech. rep., MIT Artifical Intelligence Laboratory,Cambridge, MA.Moore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learningless data less real time. Machine Learning, 13.Moore, A. W., Atkeson, C. G., & Schaal, S. (1995). Memory-based learning control.Tech. rep. CMU-RI-TR-95-18, CMU Robotics Institute.Narendra, K., & Thathachar, M. A. L. (1989). Learning Automata: Introduction.Prentice-Hall, Englewood Cliffs, NJ.Narendra, K. S., & Thathachar, M. A. L. (1974). Learning automata|a survey. IEEETransactions Systems, Man, Cybernetics, 4 (4), 323{334.Peng, J., & Williams, R. J. (1993). Ecient learning planning within Dyna framework. Adaptive Behavior, 1 (4), 437{454.Peng, J., & Williams, R. J. (1994). Incremental multi-step Q-learning. ProceedingsEleventh International Conference Machine Learning, pp. 226{232 San Francisco,CA. Morgan Kaufmann.Pomerleau, D. A. (1993). Neural network perception mobile robot guidance. KluwerAcademic Publishing.Puterman, M. L. (1994). Markov Decision Processes|Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY.Puterman, M. L., & Shin, M. C. (1978). Modified policy iteration algorithms discountedMarkov decision processes. Management Science, 24, 1127{1137.Ring, M. B. (1994). Continual Learning Reinforcement Environments. Ph.D. thesis,University Texas Austin, Austin, Texas.Rude, U. (1993). Mathematical computational techniques multilevel adaptive methods. Society Industrial Applied Mathematics, Philadelphia, Pennsylvania.Rumelhart, D. E., & McClelland, J. L. (Eds.). (1986). Parallel Distributed Processing:Explorations microstructures cognition. Volume 1: Foundations. MITPress, Cambridge, MA.Rummery, G. A., & Niranjan, M. (1994). On-line Q-learning using connectionist systems.Tech. rep. CUED/F-INFENG/TR166, Cambridge University.282fiReinforcement Learning: SurveyRust, J. (1996). Numerical dynamic programming economics. Handbook Computational Economics. Elsevier, North Holland.Sage, A. P., & White, C. C. (1977). Optimum Systems Control. Prentice Hall.Salganicoff, M., & Ungar, L. H. (1995). Active exploration learning real-valuedspaces using multi-armed bandit allocation indices. Prieditis, A., & Russell, S.(Eds.), Proceedings Twelfth International Conference Machine Learning,pp. 480{487 San Francisco, CA. Morgan Kaufmann.Samuel, A. L. (1959). studies machine learning using game checkers. IBMJournal Research Development, 3, 211{229. Reprinted E. A. FeigenbaumJ. Feldman, editors, Computers Thought, McGraw-Hill, New York 1963.Schaal, S., & Atkeson, C. (1994). Robot juggling: implementation memory-basedlearning. Control Systems Magazine, 14.Schmidhuber, J. (1996). general method multi-agent learning incremental selfimprovement unrestricted environments. Yao, X. (Ed.), Evolutionary Computation: Theory Applications. Scientific Publ. Co., Singapore.Schmidhuber, J. H. (1991a). Curious model-building control systems. Proc. InternationalJoint Conference Neural Networks, Singapore, Vol. 2, pp. 1458{1463. IEEE.Schmidhuber, J. H. (1991b). Reinforcement learning Markovian non-Markovianenvironments. Lippman, D. S., Moody, J. E., & Touretzky, D. S. (Eds.), AdvancesNeural Information Processing Systems 3, pp. 500{506 San Mateo, CA. MorganKaufmann.Schraudolph, N. N., Dayan, P., & Sejnowski, T. J. (1994). Temporal difference learningposition evaluation game Go. Cowan, J. D., Tesauro, G., & Alspector,J. (Eds.), Advances Neural Information Processing Systems 6, pp. 817{824 SanMateo, CA. Morgan Kaufmann.Schrijver, A. (1986). Theory Linear Integer Programming. Wiley-Interscience, NewYork, NY.Schwartz, A. (1993). reinforcement learning method maximizing undiscounted rewards. Proceedings Tenth International Conference Machine Learning,pp. 298{305 Amherst, Massachusetts. Morgan Kaufmann.Singh, S. P., Barto, A. G., Grupen, R., & Connolly, C. (1994). Robust reinforcementlearning motion planning. Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.),Advances Neural Information Processing Systems 6, pp. 655{662 San Mateo, CA.Morgan Kaufmann.Singh, S. P., & Sutton, R. S. (1996). Reinforcement learning replacing eligibility traces.Machine Learning, 22 (1).283fiKaelbling, Littman, & MooreSingh, S. P. (1992a). Reinforcement learning hierarchy abstract models.Proceedings Tenth National Conference Artificial Intelligence, pp. 202{207San Jose, CA. AAAI Press.Singh, S. P. (1992b). Transfer learning composing solutions elemental sequentialtasks. Machine Learning, 8 (3), 323{340.Singh, S. P. (1993). Learning Solve Markovian Decision Processes. Ph.D. thesis, Department Computer Science, University Massachusetts. Also, CMPSCI TechnicalReport 93-77.Stengel, R. F. (1986). Stochastic Optimal Control. John Wiley Sons.Sutton, R. S. (1996). Generalization Reinforcement Learning: Successful Examples UsingSparse Coarse Coding. Touretzky, D., Mozer, M., & Hasselmo, M. (Eds.), NeuralInformation Processing Systems 8.Sutton, R. S. (1984). Temporal Credit Assignment Reinforcement Learning. Ph.D. thesis,University Massachusetts, Amherst, MA.Sutton, R. S. (1988). Learning predict method temporal differences. MachineLearning, 3 (1), 9{44.Sutton, R. S. (1990). Integrated architectures learning, planning, reacting basedapproximating dynamic programming. Proceedings Seventh InternationalConference Machine Learning Austin, TX. Morgan Kaufmann.Sutton, R. S. (1991). Planning incremental dynamic programming. ProceedingsEighth International Workshop Machine Learning, pp. 353{357. MorganKaufmann.Tesauro, G. (1992). Practical issues temporal difference learning. Machine Learning, 8,257{277.Tesauro, G. (1994). TD-Gammon, self-teaching backgammon program, achieves masterlevel play. Neural Computation, 6 (2), 215{219.Tesauro, G. (1995). Temporal difference learning TD-Gammon. CommunicationsACM, 38 (3), 58{67.Tham, C.-K., & Prager, R. W. (1994). modular q-learning architecture manipulator task decomposition. Proceedings Eleventh International ConferenceMachine Learning San Francisco, CA. Morgan Kaufmann.Thrun, S. (1995). Learning play game chess. Tesauro, G., Touretzky, D. S., &Leen, T. K. (Eds.), Advances Neural Information Processing Systems 7 Cambridge,MA. MIT Press.284fiReinforcement Learning: SurveyThrun, S., & Schwartz, A. (1993). Issues using function approximation reinforcementlearning. Mozer, M., Smolensky, P., Touretzky, D., Elman, J., & Weigend, A.(Eds.), Proceedings 1993 Connectionist Models Summer School Hillsdale, NJ.Lawrence Erlbaum.Thrun, S. B. (1992). role exploration learning control. White, D. A., &Sofge, D. A. (Eds.), Handbook Intelligent Control: Neural, Fuzzy, AdaptiveApproaches. Van Nostrand Reinhold, New York, NY.Tsitsiklis, J. N. (1994). Asynchronous stochastic approximation Q-learning. MachineLearning, 16 (3).Tsitsiklis, J. N., & Van Roy, B. (1996). Feature-based methods large scale dynamicprogramming. Machine Learning, 22 (1).Valiant, L. G. (1984). theory learnable. Communications ACM, 27 (11),1134{1142.Watkins, C. J. C. H. (1989). Learning Delayed Rewards. Ph.D. thesis, King's College,Cambridge, UK.Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8 (3), 279{292.Whitehead, S. D. (1991). Complexity cooperation Q-learning. ProceedingsEighth International Workshop Machine Learning Evanston, IL. Morgan Kaufmann.Williams, R. J. (1987). class gradient-estimating algorithms reinforcement learningneural networks. Proceedings IEEE First International ConferenceNeural Networks San Diego, CA.Williams, R. J. (1992). Simple statistical gradient-following algorithms connectionistreinforcement learning. Machine Learning, 8 (3), 229{256.Williams, R. J., & Baird, III, L. C. (1993a). Analysis incremental variants policyiteration: First steps toward understanding actor-critic learning systems. Tech. rep.NU-CCS-93-11, Northeastern University, College Computer Science, Boston, MA.Williams, R. J., & Baird, III, L. C. (1993b). Tight performance bounds greedy policiesbased imperfect value functions. Tech. rep. NU-CCS-93-14, Northeastern University, College Computer Science, Boston, MA.Wilson, S. (1995). Classifier fitness based accuracy. Evolutionary Computation, 3 (2),147{173.Zhang, W., & Dietterich, T. G. (1995). reinforcement learning approach job-shopscheduling. Proceedings International Joint Conference Artificial Intellience.285fiJournal Artificial Intelligence Research 4 (1996) 397-417Submitted 12/95; published 6/96Experimental Evidence UtilityOccam's RazorGeoffrey I. Webbwebb@deakin.edu.auSchool Computing MathematicsDeakin UniversityGeelong, Vic, 3217, Australia.Abstractpaper presents new experimental evidence utility Occam's razor.systematic procedure presented post-processing decision trees produced C4.5.procedure derived rejecting Occam's razor instead attending assumption similar objects likely belong class. increases decisiontree's complexity without altering performance tree training datainferred. resulting complex decision trees demonstrated have,average, variety common learning tasks, higher predictive accuracy lesscomplex original decision trees. result raises considerable doubt utilityOccam's razor commonly applied modern machine learning.1. Introductionfourteenth century William Occam stated \plurality assumed without necessity". principle since become known Occam's razor. Occam's razororiginally intended basis determining one's ontology. However, modern timeswidely reinterpreted adopted epistemological principle|a meansselecting alternative theories well ontologies. Modern reinterpretationsOccam's razor widely employed classification learning. However, utilityprinciple subject widespread theoretical experimental attack. paperadds debate providing experimental evidence utilitymodern interpretation Occam's razor. evidence takes form systematic procedure adding non-redundant complexity classifiers manner demonstratedfrequently improve predictive accuracy.modern interpretation Occam's razor characterized \of two hypotheses H H0 , explain E, simpler preferred" (Good, 1977).However, specify aspect theory measured simplicity.Syntactic, semantic, epistemological pragmatic simplicity alternative criteriaemployed Bunge (1963). practice, common use Occam's razormachine learning seeks minimize surface syntactic complexity. interpretationpaper addresses.assumed Occam's razor usually applied expectationapplication will, general, lead particular form advantage. widelyaccepted articulation precisely Occam's razor applied advantagesexpected application classification learning. However, literaturecontain two statements seem capture least one widely adopted approachc 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiWebbprinciple. Blumer, Ehrenfeucht, Haussler, Warmuth (1987) suggest wieldOccam's razor adopt goal discovering \the simplest hypothesis consistentsample data" expectation simplest hypothesis \perform wellobservations taken source". Quinlan (1986) states\Given choice two decision trees, correcttraining set, seems sensible prefer simpler one groundslikely capture structure inherent problem. simpler tree wouldtherefore expected classify correctly objects outside training set."statements would necessarily accepted proponents Occam'srazor, capture form Occam's razor paper seeks address|a learningbias toward classifiers minimize surface syntactic complexity expectationmaximizing predictive accuracy.statements Occam's razor restrict classifierscorrectly classify objects training set. Many modern machine learning systemsincorporate learning biases tolerate small levels misclassification training data(Clark & Niblett, 1989; Michalski, 1984; Quinlan, 1986, 1990, example). context,extending scope definition beyond decision trees classifiers general,seems reasonable modify Quinlan's (1986) statement (above)Given choice two plausible classifiers perform identicallytraining set, simpler classifier expected classify correctly objectsoutside training set.referred Occam thesis.concept identical performance training set could defined many differentways. might tempting opt definition requires identical error ratestwo classifiers applied training set. less strict interpretation might allow twoclassifiers differing error rates long difference within statisticalconfidence limit. However, maximize applicability results, paper adoptstrict interpretation identical performance|that every object trainingset, classifiers provide classification o.noted Occam thesis claiming two classifiersequal empirical support least complex always greater predictive accuracypreviously unseen objects. Rather, claiming frequently lesscomplex higher predictive accuracy.paper first examines arguments Occam thesis.presents new empirical evidence thesis. evidence acquired usinglearning algorithm post-processes decision trees learnt C4.5. post-processordeveloped rejecting Occam thesis instead attending assumptionsimilarity predictive class. post-processor systematically adds complexitydecision trees without altering performance training data. demonstratedlead increase predictive accuracy previously unseen objects range`real-world' learning tasks. evidence taken incompatible Occam thesis.398fiFurther Experimental Evidence Utility Occam's Razor2. Previous Theoretical Experimental Workprovide context new evidence Occam thesis, worth brieexamining previous relevant theoretical experimental work. relevant, outlineprovided reasons contribution may failed persuadeside debate.2.1 Law Conservation Generalization Performanceconservation law generalization performance (Schaffer, 1994) proves learningbias outperform bias space possible learning tasks1. followsOccam's razor valuable learning bias, subsetpossible learning tasks. might argued set `real-world' learning taskssubset.paper predicated accepting proposition set `real-world' learningtasks distinguished set possible learning tasks respects renderconservation law inapplicable. Rao, Gordon, Spears (1995) argue caselearning tasks universe uniformly distributed across spacepossible learning tasks.so? One argument support proposition follows.`Real-world' learning tasks defined people use machine learning systems.end, task constructors sought ensure independent variables(class attributes) related dependent variables (other attributes) wayscaptured within space classifiers made available learning system.Actual machine learning tasks drawn randomly space possible learningtasks. human involvement formulation problems ensures this.simple thought experiment support proposition, consider learning taskclass attribute generated random number generator wayrelates attributes. majority machine learning researchers wouldslightest disconcerted systems failed perform well trained data.example, consider learning task class attribute simple countnumber missing attribute values object. Assume learning tasksubmitted system, C4.5 (Quinlan, 1993), develops classifiersmechanism testing classification whether attribute value missing. Again,majority machine learning researchers would unconcerned systems failedperform well circumstances. Machine learning simply unsuited tasks.knowledgeable user would apply machine learning data, leastexpectation obtaining useful classifier therefrom.paper explores applicability Occam thesis `real-world' learning tasks.2.2 Theoretical Objections Occam Thesismachine learning systems explicitly implicitly employ Occam's razor. additionalmost universal use machine learning, principle Occam's razor widely1. law proved discrete valued learning tasks, reason believealso apply continuous valued tasks399fiWebbaccepted general scientific practice. persisted, despite Occam's razorsubjected extensive philosophical, theoretical empirical attack, suggestsattacks found persuasive.philosophical front, summarize Bunge (1963), complexity theory(classifier) depends entirely upon language encoded. claimacceptability theory depends upon language happens expressedappears indefensible. Further, obvious theoretical relationship syntactic complexity quality theory, possibility worldintrinsically simple use Occam's razor enables discovery intrinsicsimplicity. However, even world intrinsically simple, reasonsimplicity correspond syntactic simplicity arbitrary language.merely state less complex explanation preferable specifycriterion preferable. implicit assumption underlying much machine learning research appears that, things equal, less complex classifiers be,general, accurate (Blumer et al., 1987; Quinlan, 1986). Occam thesispaper seeks discredit.straight-forward interpretation, syntactic measure used predictexpected accuracy appears absurd. two classifiers identical meaning (such20AGE40 POS 20AGE30 30AGE40 POS)possible accuracies differ, matter greatly complexities differ.simple example highlights apparent dominance semantics syntaxdetermination predictive accuracy.2.3 Previous Experimental Evidence Occam Thesisempirical front, number recent experimental results appeared con ictOccam thesis. Murphy Pazzani (1994) demonstrated number artificial classification learning tasks, simplest consistent decision trees lower predictiveaccuracy slightly complex consistent trees. experimentation, however,showed results dependent upon complexity target concept.bias toward simplicity performed well target concept best describedsimple classifier bias toward complexity performed well target conceptbest described complex classifier (Murphy, 1995). addition, simplest classifiersobtained better average (over consistent classifiers) predictive accuracydata augmented irrelevant attributes attributes strongly correlated targetconcept, required classification.Webb (1994) presented results suggest wide range learning tasksUCI repository learning tasks (Murphy & Aha, 1993), relative generalityclassifiers better predictor classification performance relative surfacesyntactic complexity. However, could argued results demonstratestrategy selecting simplest pair theories lead maximizationpredictive accuracy, demonstrate selecting simplest availabletheories would fail maximize predictive accuracy.Schaffer (1992, 1993) shown pruning techniques reduce complexitydecreasing resubstitution accuracy sometimes increase predictive accuracy sometimes400fiFurther Experimental Evidence Utility Occam's Razordecrease predictive accuracy inferred decision trees. However, proponent Occamthesis could explain results terms positive effect application Occam'srazor (the reduction complexity) counter-balanced negative effectreduction empirical support (resubstitution accuracy).Holte, Acker, Porter (1989) shown specializing small disjuncts (ruleslow empirical support) exclude areas instance space occupied trainingobjects frequently decreases error rate unseen objects covered disjuncts.specialization involves increasing complexity, might viewed contraryOccam thesis. However, research shows total error rates classifiersdisjuncts embedded increases disjuncts specialized.proponent Occam thesis could thus dismiss relevance former resultsarguing thesis applies complete classifiers elementsclassifiers.2.4 Theoretical Experimental Support Occam Thesistheoretical experimental objections Occam thesis existsbody apparent theoretical empirical support.Several attempts made provide theoretical support Occam thesismachine learning context (Blumer et al., 1987; Pearl, 1978; Fayyad & Irani, 1990).However, proofs apply equally systematic learning bias favors smallsubset hypothesis space. Indeed, argued equally supportpreference classifiers high complexity (Schaffer, 1993; Berkman & Sandholm, 1995).Holte (1993) compared learning simple classification rules use sophisticated learner complex decision trees. found that, number tasks UCIrepository machine learning datasets (Murphy & Aha, 1993), simple rules achievedaccuracies within percentage points complex trees. could consideredsupportive Occam thesis. However, case simple rules outperformcomplex decision trees. demonstrated exist yet anotherlearning bias consistently outperformed studied.final argument might considered support Occam thesismajority machine learning systems employ form Occam's razor appear perform well practice. However, demonstrated even betterperformance would obtained Occam's razor abandoned.3. New Experimental Evidence Occam Thesistheoretical experimental objections Occam thesis appeargreatly diminished machine learning community's use Occam's razor. paperseeks support objections Occam thesis robust general experimentalcounter-evidence. end presents systematic procedure increasing complexity inferred decision trees without modifying performance training data.procedure takes form post-processor decision trees produced C4.5 (Quinlan, 1993). application procedure range learning tasks UCIrepository learning tasks (Murphy & Aha, 1993) demonstrated result, average,401fiWebbincreased predictive accuracy inferred decision trees applied previouslyunseen data.3.1 Theoretical Basis Decision Tree Post-processorsimilarity assumption common assumption machine learning|that objectssimilar high probability belonging class (Rendell & Seshu, 1990).techniques described rely upon assumption theoretical justificationrather upon Occam thesis.Starting similarity assumption, machine learning viewed inferencesuitable similarity metric learning task. decision tree viewedpartitioning instance space. partition, represented leaf, containsobjects similar relevant respects thus expected belongclass.raises issue similarity measured. Instance-based learning methods (Aha, Kibler, & Albert, 1991) tend map instance space onto ndimensional geometric space employ geometric distance measures withinspace measure similarity. approach problematic number grounds.First, assumes underlying metrics different attributes commensurable.possible determine priori whether difference five years age signifiesgreater lesser difference similarity difference one inch height? Second,assumes possible provide priori definitions similarity respectsingle attribute. one really make universal prescription value 16 alwayssimilar value 2 value 64? never caserelevant similarity metric based log2 surface value, case 16 wouldsimilar 64 2?wish employ induction learn classifiers expressed particular languagewould appear forced assume language question mannercaptures relevant aspect similarity. potential leaf decision tree presentsplausible similarity metric (all objects fall within leaf similar respect).Empirical evaluation (the performance leaf training set) usedinfer relevance similarity metric induction task hand. leaf l coverslarge number objects class c classes, provides evidencesimilarity respect tests define l predictive c.Figure 1 illustrates simple instance space partition C4.5 (Quinlan, 1993)imposes thereon. Note C4.5 forms nodes continuous attributes, B ,consist test cut value x. test takes form x. respectFigure 1 one cut, value 5 attribute A.C4.5 infers relevant similarity metric relates attribute only. partition(shown dashed line) placed value 5 attribute A. However, oneaccept Occam thesis, accept similarity assumption, reasonbelieve area instance space B > 5 5 (lightly shadedFigure 1) belong class + (as determined C4.5) rather class {.C4.5 uses Occam thesis justify termination partitioning instancespace soon decision tree accounts adequately training set. consequence,402fiFurther Experimental Evidence Utility Occam's Razor.................... { {.................... { {.......... {+{+{+1 2 3 4 5 6 7 8 9 10Figure 1: simple instance space109876B54321large areas instance space occupied objects training set mayleft within partitions similarity assumption provides little support.example, respect Figure 1, could argued relevant similarity metricrespect region 5 B > 5 similarity respect B . Withinentire instance space, objects values B > 5 belong class {. fiveobjects. contrast, three objects values 5 provideevidence objects area instance space belong class +.tests represents plausible similarity metric basis available evidence. Thus,object within region similar plausible respect three positive fivenegative objects. objects similar relevant respects high probabilitybelonging class, information available plausibleobject similar three positive five negative objects, would appearprobable object negative positive.disagreement C4.5 similarity assumption case contrastswith, example, area instance space 5 B < 1. region,similarity assumption suggests C4.5's partition appropriate plausiblesimilarity metrics indicate object region similar positive objectsonly2 .post-processor developed research analyses decision trees produced C4.5order identify regions|those occupied objects training setevidence (in terms similarity assumption) favoring relabeling2. provide example implausible similarity metric, consider similarity metric definedroot node, everything similar. plausible great leveldissimilarity classes respect metric. relevant similarity metric,distribution training examples representative distribution objects domainwhole, similarity assumption would violated, similar objects would probability0.58 belonging class. probability calculated follows. probabilitiesobject + { 0.3 0.7 respectively. object + probability belongingclass another object similar 0.3. object { probabilitybelonging class another object similar 0.7. Thus, probabilityobject belonging class another similar object 0:3 0:3 + 0:7 0:7 = 0:58. numbersinvolved simple example are, course, small reach conclusion high levelconfidence|the example intended illustrative only.403fiWebbdifferent class assigned C4.5. regions identified, new branchesadded decision tree, creating new partitionings instance space. treesmust provide identical performance respect training set regionsinstance space occupied objects training set affected.dicult see plausible metric complexity could interpret additionbranches increasing complexity tree.end result post-processor adds complexity decision tree withoutaltering tree applies training data. Occam thesis predicts will,general, lower predictive accuracy similarity assumption predicts will,general, increase predictive accuracy. seen, latter prediction consistentexperimental evidence former not.3.2 Post-processorprocess could applied continuous discrete attributes,current implementation addresses continuous attributes.post-processor operates examining leaf l tree turn. l,attribute considered turn. a, possible thresholdsregion instance space occupied objects l explored. First, minimum(min) maximum (max) determined values possible objectsreach l. l lies branch split thresholdsplit provides upper limit (max) values l. lies > branch,threshold provides lower limit (min). node lie branch,max = 1. node lie > branch, min = ,1. objectstraining set values within range min::max consideredfollowing operations.value observed training set attribute within allowable rangeoutside actual range values objects l, evidence evaluatedsupport reclassifying region threshold. level supportgiven threshold evaluated using Laplacian accuracy estimate (Niblett & Bratko, 1986).leaf relates binary classification (an object belongs class questionnot), binary form Laplace used. threshold attribute leaf l,evidence support labeling partition class n maximum valueancestor node x l formulaP +1+2number objects x min < t; P numberobjects belong class n.evidence support labeling partition threshold calculated identicallyexception objects < max instead considered.maximum evidence new labeling exceeds evidence current labelingregion, new branch added appropriate threshold creating new leaf nodelabeled appropriate class.addition evidence favor current labeling gathered above, evidence support current labeling region calculated using Laplace accuracy404fiFurther Experimental Evidence Utility Occam's Razorestimate considering objects leaf, number objects leafP number objects belong class node labeled.approach ensures new partitions define true regions. is,attribute value v possible partition v unless possibleobjects domain values greater v objects values lessequal v reach node partitioned (even though objectstraining set fall within new partition). particular, ensures new cutssimple duplications existing cuts ancestors current node. Thus, everymodification adds non-redundant complexity tree.algorithm presented Figure 2. implemented modificationC4.5 release 6, called C4.5X. source code modifications availableon-line appendix paper.C4.5X, multiple sets values equally satisfy specified constraintsmaximize Laplace function, values na nb deeper tree selectedcloser root and, single node, preference values aa ab dependsupon order attributes definition data preference values vavb dependent upon data order. selection strategies side effectimplementation system. reason believe experimental resultswould differ general strategies used select competing constraints.default, C4.5 develops two decision trees time run, unprunedpruned (simplified) decision tree. C4.5X produces post-processed versionstrees.3.3 Evaluationevaluate post-processor applied datasets containing continuous attributesUCI machine learning repository (Murphy & Aha, 1993) held (dueprevious machine learning experimentation) local repository Deakin University.datasets believed broadly representative repositorywhole. experimentation eleven data sets, two additional data sets, sickeuthyroid discordant results, retrieved UCI repository addedstudy order investigate specific issues, discussed below.resulting thirteen datasets described Table 1. second column containsnumber attributes object described. Next proportioncontinuous. fourth column indicates proportion attribute valuesdata missing (unknown). fifth column indicates number objectsdata set contains. sixth column indicates proportion belongclass represented objects within data set. final column indicatesnumber classes data set describes. Note glass type dataset usesFloat/Not Float/Other three class classification rather commonly used sixclass classification.data set divided training evaluation sets 100 times. trainingset consisted 80% data, randomly selected. evaluation set consistedremaining 20% data. C4.5 C4.5X applied resulting 1300(13 data sets 100 trials) training evaluation set pairs.405fiWebbLet cases(n) denote set training examples reach node n.Let value(a; x) denote value attribute training example x.Let pos(X; c) denote number objects class c set training examples X.Let Laplace(X; c) = ( +2)+1 X set training examples, jX j number trainingexamples c class.Let upperlim(n; a) denote minimum value cut attribute ancestor node nn lies branch. cut, upperlim(n; a) = 1. determinesupper bound values may reach n.Let lowerlim(n; a) denote maximum value cut attribute ancestor node nn lies > branch. cut, lowerlim(n; a) = ,1. determineslower bound values may reach n.post-process leaf l dominated class c1. Find valuesn : n ancestor l: continuous attributev : 9x : x 2 cases(n ) & v = value(a ; x) & v min(v : 9y : 2 cases(l) & v =value(a ; y)) & v > lowerlim(l; )c : c classmaximize L = Laplace(fx : x 2 cases(n ) & value(a ; x) v & value(a ; x) >lowerlim(l; )g; c ).2. Find valuesn : n ancestor l: continuous attributev : 9x : x 2 cases(n ) & v = value(a ; x) & v > max(v : 9y : 2 cases(l) & v =value(a ; y)) & v upperlim(l; )c : c classmaximize L = Laplace(fx : x 2 cases(n ) & value(a ; x) > v & value(a ; x)upperlim(l; )g; c ).3. L > Laplace(cases(l); c) & L L(a) c 6= ci. replace l node n test v .ii. set branch n lead new leaf class c .iii. set > branch n lead l.else L > Laplace(cases(l); c)(b) c 6= ci. replace l node n test v .ii. set > branch n lead new leaf class c .iii. set branch n lead l.pos X;cjX jbbbbbbbbbbbbbbbbbbbbbbbbbbbFigure 2: C4.5X post-processing algorithm406fiFurther Experimental Evidence Utility Occam's RazorTable 1: UCI data sets used experimentation%%No. contin%No. common No.NameAttrs. uous missing objects class classesbreast cancer Wisconsin9100<1699662Cleveland heart disease1346<1303542credit rating15401690562discordant results292463772982echocardiogram683374682glass type91000214403hepatitis19326155792Hungarian heart disease134620295642hypothyroid292463772924iris41000150333new thyroid51000215703Pima indians diabetes81000768652sick euthyroid292463772942Table 2 summarizes percentage predictive accuracy obtained unpruned decision trees generated C4.5 C4.5X. presents mean (x) standarddeviation (s) set 100 trials respect data set C4.5C4.5X along results two-tailed matched pairs t-test comparing means.twelve thirteen data sets C4.5X obtained higher mean accuracy C4.5.remaining data set, hypothyroid, C4.5 obtained higher mean predictive accuracyC4.5CS (albeit small margin|measured two decimal places respective mean accuracies 99.51 99.46, respectively). nine data sets advantage towardC4.5X statistically significant 0.05 level (p 0:05), although advantagerespect discordant results data small apparent measured onedecimal place (measured two decimal places values 98.58 98.62 respectively).advantage toward C4.5 hypothyroid data also statistically significant0.05 level. differences mean predictive accuracy Hungarian heart disease,new thyroid sick euthyroid data sets significant 0.05 level.Table 3 uses format Table 2 summarize predictive accuracy obtainedpruned decision trees generated C4.5 C4.5X. twelve datasets C4.5X obtained higher mean predictive accuracy C4.5. remaining dataset, hypothyroid, C4.5 obtained higher mean predictive accuracy, althoughmagnitude difference small apparent level precisiondisplayed (measured two decimal places mean accuracies 99.51 99.46).six data sets advantage toward C4.5X statistically significant 0.05level, although difference apparent precision two decimal placesdiscordant results data (99.81 99.82, respectively). advantage toward C4.5hypothyroid data also statistically significant 0.05 level. differences407fiWebbTable 2: Percentage predictive accuracy unpruned decision trees.Namebreast cancer WisconsinCleveland heart diseasecredit ratingdiscordant resultsechocardiogramglass typehepatitisHungarian heart diseasehypothyroidirisnew thyroidPima indians diabetessick euthyroidC4.5x94.172.882.298.672.074.079.677.099.595.489.970.298.71.85.03.40.59.87.07.15.30.23.44.23.50.5C4.5Xxp94.4 1.7 {3.2 0.00274.4 4.8 {6.1 0.00083.0 3.3 {7.6 0.00098.6 0.5 {5.4 0.00073.5 10.2 {2.8 0.00775.3 7.2 {4.2 0.00080.8 6.9 {3.3 0.00177.4 5.2 {1.8 0.08299.5 0.2 4.4 0.00095.7 3.5 {2.2 0.02890.1 4.3 {1.0 0.30271.3 3.6 {8.1 0.00098.7 0.5 {0.0 0.963Table 3: Percentage accuracy pruned decision trees.Namebreast cancer WisconsinCleveland heart diseasecredit ratingdiscordant resultsechocardiogramglass typehepatitisHungarian heart diseasehypothyroidirisnew thyroidPima indians diabetessick euthyroidC4.5x95.174.184.198.874.274.479.979.299.595.489.672.298.71.75.33.20.49.36.96.24.90.23.64.23.50.4C4.5Xx95.274.884.698.875.175.480.779.499.595.789.872.898.71.75.33.20.49.86.96.24.80.23.74.23.50.4p{2.0 0.051{3.7 0.000{5.3 0.000{2.6 0.010{1.6 0.1180{3.3 0.001{3.0 0.003{1.0 0.3105.4 0.000{1.6 0.109{0.8 0.451{5.9 0.000{0.7 0.480breast cancer Wisconsin, echocardiogram, Hungarian heart disease, iris, new thyroidsick euthyroid statistically significant 0.05 level.completing experimentation initial eleven data sets, resultshypothyroid data stood stark contrast ten. raisedpossibility might distinguishing features hypothyroid data408fiFurther Experimental Evidence Utility Occam's Razoraccounted difference performance. Table 1 indicates data set clearlydistinguishable ten initial data sets following six respects|attributes;containing greater proportion discrete attributes (which directly addressedC4.5X);containing objects;greater proportion objects belong common class;classes;producing decision trees extremely high predictive accuracy without post-processing.explore issues discordant results sick euthyroid data sets retrievedUCI repository added study. data sets identicalhypothyroid data set exception different class attribute. threedata sets contain objects, described attributes. additiondiscordant results sick euthyroid data little illuminate issue however.three data sets changes accuracy small magnitude. hypothyroidsignificant advantage C4.5. sick euthyroid significant advantageeither system. discordant results data significant advantage C4.5X.question whether distinguishing feature hypothyroid dataexplains observed results remains unanswered. investigation issue liesbeyond scope current paper remains interesting direction future research.results suggest C4.5X's post-processing frequently increases predictiveaccuracy type data found UCI repository. (Of twenty-sixcomparisons, significant increase fifteen significant decreasetwo. sign test reveals rate success significant 0.05 level,p = 0:001.)Tables 4 5 summarize number nodes decision trees developed. Table 4addresses unpruned decision trees Table 5 addresses pruned decision trees. postprocessing modification replaces single leaf split two leaves. onemodification performed per leaf original tree. data sets postprocessed decision trees significantly complex original decision trees.cases post-processing increased mean number nodes decision treesapproximately 50%. demonstrates post-processing causing substantialchange.4. Discussionprimary objective research discredit Occam thesis.end uses post-processor disregards Occam thesis instead theoreticallyfounded upon similarity assumption. Experimentation post-processor409fiWebbTable 4: Number nodes unpruned decision trees.C4.5C4.5XNamexxpbreast cancer Wisconsin 38.1 6.0 64.0 10.3 {51.5 0.000Cleveland heart disease 66.7 7.1 100.2 11.3 {61.9 0.000credit rating117.6 18.1 177.9 28.4 {44.2 0.000discordant results64.0 10.6 85.2 16.2 {33.3 0.000echocardiogram15.4 4.1 22.1 6.3 {26.1 0.000glass type43.0 5.2 69.7 8.4 {57.2 0.000hepatitis24.5 4.2 34.8 6.0 {49.1 0.000Hungarian heart disease 62.1 7.5 94.8 13.0 {50.1 0.000hypothyroid29.4 4.4 47.5 7.1 {57.8 0.000iris9.0 1.9 16.0 4.0 {31.5 0.000new thyroid14.7 2.4 23.4 3.8 {41.5 0.000Pima indians diabetes 164.8 10.8 238.8 16.3 {108.9 0.000sick euthyroid71.7 6.6 111.4 12.1 {65.8 0.000Table 5: Number nodes pruned decision trees.C4.5C4.5XNamexxbreast cancer Wisconsin 19.2 5.0 33.1Cleveland heart disease 44.6 8.3 68.3credit rating51.2 14.8 78.4discordant results24.9 5.6 32.5echocardiogram10.4 3.0 14.8glass type36.6 5.5 61.0hepatitis13.7 4.8 19.8Hungarian heart disease 26.8 11.4 41.2hypothyroid23.6 2.9 37.1iris8.2 1.9 14.8new thyroid14.1 2.7 22.5Pima indians diabetes 112.0 16.4 163.9sick euthyroid46.5 5.8 72.68.612.824.28.84.89.56.617.35.63.94.324.08.7{34.9{43.6{25.8{21.1{21.0{48.5{30.7{22.1{46.7{30.3{36.9{62.5{76.7p0.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.000demonstrated possible develop systematic procedures that, range `realworld' learning tasks increase predictive accuracy inferred decision trees resultchanges substantially increase complexity without altering performanceupon training data.is, general, dicult attack Occam thesis due absence widelyagreed formulation thereof. However, far apparent Occam thesis might410fiFurther Experimental Evidence Utility Occam's Razor........... . . .{. . . . . . { {.................... { {.......... {+{+{+1 2 3 4 5 6 7 8 9 10Figure 3: Modified simple instance space109876B54321recast accommodate experimental results provide practical learningbias.4.1 Directions Future Researchimplications research reach beyond relevance Occam's razor. postprocessor appears practical utility increasing quality inferred decision trees.However, objective research improve predictive accuracy ratherdiscredit Occam thesis, post-processor would modified number ways.first modification would enable addition multiple partitions singleleaf original tree. C4.5X selects single modificationmaximum support. design decision originated desire minimize likelihoodperforming modifications decrease accuracy. principle, however, wouldappear desirable select modifications strong support,could inserted tree order level supporting evidence.Even greater increases accuracy might expected one removed constraintpost-processing alter performance decision tree respecttraining set. case, new partitions may well found employ objectsregions instance space provide evidence support adding partitionscorrect misclassifications small numbers objects leaf node original tree.similarity assumption would provide strong evidence repartitioning.situation would occur, example, respect learning problem illustratedFigure 1, additional object class { attribute values A=2 B=9.illustrated Figure 3. case C4.5 would still create indicated partitions.However, C4.5X would unable relabel area containing additional object dueconstraint alter performance original decision tree respecttraining set. Thus addition object prevents C4.5X relabeling shadedregion even though, basis similarity assumption, improves evidencesupport relabeling.extended post-processor would encourage following model inductive inference decision trees. role C4.5 (or similar system) would identify clusters411fiWebbobjects within instance space grouped single leaf node.second stage would analyze regions instance space lie outside clustersorder allocate classes regions. Current decision tree learners, motivatedOccam thesis, ignore second stage, leaving regions outside identified clustersassociated whatever classes assigned by-product clusteridentification process.4.2 Related Researchnumber researchers developed learning systems viewed consideringevidence neighboring regions instance space order derive classificationswithin regions instance space occupied examples trainingset. Ting (1994) explicitly, examining training set directly exploreneighborhood object classified. system uses instance based learningclassification within nodes decision tree low empirical support (small disjuncts).number systems also viewed considering evidence neighboringregions classification. systems learn apply multiple classifiers (Ali,Brunk, & Pazzani, 1994; Nock & Gascuel, 1995; Oliver & Hand, 1995). context,point within region instance space occupied training objectslikely covered multiple leaves rules. these, leaf rule greatestempirical support used classification.C4.5X uses two distinct criteria evaluating potential splits. standard C4.5 stagetree induction employs information measure select splits. post-processor usesLaplace accuracy estimate. Similar uses dual criteria investigated elsewhere.Quinlan (1991) employs Laplace accuracy estimate considering neighboring regionsinstance space estimate accuracy small disjuncts. Lubinsky (1995) Brodley(1995) employ resubstitution accuracy select splits near leaves inductiondecision trees.adding split leaf, C4.5X specializing respect class leaf(and generalizing respect class new leaf). Holte et al. (1989) explorednumber techniques specializing small disjuncts. C4.5X differs leavescandidates specialization, low empirical support. differsmanner selects specialization perform considering evidencesupport alternative splits rather strength evidence supportindividual potential conditions current disjunct.4.3 Bias Versus VarianceBreiman, Friedman, Olshen, Stone (1984) provide analysis complexity induction terms trade-off bias variance. classifier partitions instancespace regions. regions large, degree fit accurate partitioning instance space poor, increasing error rates. effect called bias.regions small, probability individual regions labeledwrong class increased. effect, called variance, also increases error rates. Accordinganalysis, due variance, fine partitioning instance space tends increase412fiFurther Experimental Evidence Utility Occam's Razorerror rate while, due bias, coarse partitioning also tends increase errorrate.Increasing complexity decision tree creates finer partitionings instancespace. analysis used argue addition undue complexitydecision trees ground increase variance hence error rate.However, success C4.5X decreasing error rate demonstratessuccessfully managing bias/variance trade-off introduces complexitydecision tree. using evidence neighboring regions instance space, C4.5Xsuccessful increasing error rate resulting variance lower ratedecreases error rate resulting bias. success C4.5X demonstratesadding undue complexity C4.5's decision trees.4.4 Minimum Encoding Length InductionMinimum encoding length approaches perform induction seeking theory enablescompact encoding theory available data. Two key approachesdeveloped, Minimum Message Length (MML) (Wallace & Boulton, 1968)Minimum Description Length (MDL) (Rissanen, 1983). approaches admit probabilistic interpretations. Given prior probabilities theories data, minimizationMML encoding closely approximates maximization posterior probability (Wallace & Freeman, 1987). MDL code length defines upper bound \unconditionallikelihood" (Rissanen, 1987).two approaches differ MDL employs universal prior, Rissanen (1983)explicitly justifies terms Occam's razor, MML allows specification distinctappropriate priors induction task. However, practice, default prior usuallyemployed MML, one appears also derive justification Occam's razor.Neither MDL MML default prior would add complexity decision treejustified solely basis evidence neighboring regionsinstance space. evidence study presented herein appears supportpotential desirability so. casts doubt upon utility universalprior employed MDL default prior usually employed MML, leastrespect use maximizing predictive accuracy.noted, however, probabilistic interpretation minimumencoding length techniques indicates encoding length minimization represents maximization posterior probability unconditional likelihood. Maximizationfactors necessarily directly linked maximizing predictive accuracy.4.5 Appropriate Application Grafting Pruningimportant note although paper calls question value learningbiases penalize complexity, way provide support learning biasesencourage complexity sake. C4.5X grafts new nodes onto decision treeempirical support so.results way argue appropriate use decision tree pruning.generate pruned trees, C4.5 removes branches statistical estimates upperbounds error rates indicate increase branch removed.413fiWebbcould argued C4.5 reduces complexity empirical supportso. interesting note eight thirteen data sets examined, C4.5X'spost-processing pruned trees resulted higher average predictive accuracypost-processing unpruned trees. results suggest pruning graftingplay valuable role applied appropriately.5. Conclusionpaper presents systematic procedure adding complexity inferred decision treeswithout altering performance training data. procedure demonstrated lead increases predictive accuracy range learning tasks appliedpruned unpruned trees inferred C4.5. one thirteen learningtasks examined procedure lead statistically significant loss accuracycase magnitude difference mean accuracy extremely small.face it, provides strong experimental evidence Occam thesis.post-processing technique developed rejecting Occam thesis instead attending similarity assumption|that similar objects high probabilitybelonging class.procedure developed constrained need ensure revised decisiontree performed identically original decision tree respect training data.constraint arose desire obtain experimental evidence Occamthesis. possible constraint removed, basic techniques outlinedpaper could result even greater improvements predictive accuracy reportedherein.research considered one version Occam's razor favors minimizationsyntactic complexity expectation tend increase predictive accuracy.interpretations Occam's razor also possible, one minimizesemantic complexity. others (Bunge, 1963) provided philosophical objectionsformulations Occam's razor, paper sought investigate them.version Occam's razor examined research used widely machinelearning apparent success. objections principle substantiated research raise question, apparent successawed? Webb (1994) suggests apparent success principle duemanner syntactic complexity usually associated relevant qualitiesinferred classifiers generality prior probability. thesis accepted onekey challenges facing machine learning understand deeper qualitiesemploy understanding place machine learning sounder theoretical footing.paper offers small contribution direction demonstrating minimizationsurface syntactic complexity not, itself, general maximize predictive accuracyinferred classifiers.nonetheless important realize that, thrust paper notwithstanding,Occam's razor often useful learning bias employ. frequently good pragmatic reasons preferring simple hypothesis. simple hypothesisgeneral easier understand, communicate employ. preference simple414fiFurther Experimental Evidence Utility Occam's Razorhypotheses cannot justified terms expected predictive accuracy may justifiedpragmatic grounds.Acknowledgementsresearch supported Australian Research Council. gratefulCharlie Clelland, David Dowe, Doug Newlands, Ross Quinlan anonymous reviewersextremely valuable comments paper benefited greatly.ReferencesAha, D. W., Kibler, D., & Albert, M. K. (1991). Instance-based learning algorithms.Machine Learning, 6, 37{66.Ali, K., Brunk, C., & Pazzani, M. (1994). learning multiple descriptions concept.Proceedings Tools Artificial Intelligence New Orleans, LA.Berkman, N. C., & Sandholm, T. W. (1995). minimized decision tree:re-examination. Technical report 95-20, University Massachusetts Amherst,Computer Science Department, Amherst, Mass.Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. (1987). Occam's Razor.Information Processing Letters, 24, 377{380.Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). ClassificationRegression Trees. Wadsworth International, Belmont, Ca.Brodley, C. E. (1995). Automatic selection split criterion tree growing basednode selection. Proceedings Twelth International Conference MachineLearning, pp. 73{80 Taho City, Ca. Morgan Kaufmann.Bunge, M. (1963). Myth Simplicity. Prentice-Hall, Englewood Cliffs, NJ.Clark, P., & Niblett, T. (1989). CN2 induction algorithm. Machine Learning, 3,261{284.Fayyad, U. M., & Irani, K. B. (1990). minimized decision tree?AAAI-90: Proceedings Eighth National Conference Artificial Intelligence, pp.749{754 Boston, Ma.Good, I. J. (1977). Explicativity: mathematical theory explanation statisticalapplications. Proceedings Royal Society London Series A, 354, 303{330.Holte, R. C. (1993). simple classification rules perform well commonly useddatasets. Machine Learning, 11 (1), 63{90.Holte, R. C., Acker, L. E., & Porter, B. W. (1989). Concept learning problemsmall disjuncts. Proceedings Eleventh International Joint ConferenceArtificial Intelligence, pp. 813{818 Detroit. Morgan Kaufmann.415fiWebbLubinsky, D. J. (1995). Increasing performance consistency classification treesusing accuracy criterion leaves. Proceedings Twelth InternationalConference Machine Learning, pp. 371{377 Taho City, Ca. Morgan Kaufmann.Michalski, R. S. (1984). theory methodology inductive learning. Michalski,R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning: ArtificialIntelligence Approach, pp. 83{129. Springer-Verlag, Berlin.Murphy, P. M. (1995). empirical analysis benefit decision tree size biasesfunction concept distribution. Tech. rep. 95-29, Department InformationComputer Science, University California, Irvine.Murphy, P. M., & Aha, D. W. (1993). UCI repository machine learning databases.[Machine-readable data repository]. University California, Department Information Computer Science, Irvine, CA.Murphy, P. M., & Pazzani, M. J. (1994). Exploring decision forest: empirical investigation Occam's razor decision tree induction. Journal Artificial IntelligenceResearch, 1, 257{275.Niblett, T., & Bratko, I. (1986). Learning decision rules noisy domains. Bramer,M. A. (Ed.), Research Development Expert Systems III, pp. 25{34. CambridgeUniversity Press, Cambridge.Nock, R., & Gascuel, O. (1995). learning decision committees. ProceedingsTwelth International Conference Machine Learning, pp. 413{420 Taho City, Ca.Morgan Kaufmann.Oliver, J. J., & Hand, D. J. (1995). pruning averaging decision trees. ProceedingsTwelth International Conference Machine Learning, pp. 430{437 Taho City,Ca. Morgan Kaufmann.Pearl, J. (1978). connection complexity credibility inferredmodels. International Journal General Systems, 4, 255{264.Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81{106.Quinlan, J. R. (1990). Learning logical definitions relations. Machine Learning, 5,239{266.Quinlan, J. R. (1991). Improved estimates accuracy small disjuncts. MachineLearning, 6, 93{98.Quinlan, J. R. (1993). C4.5: Programs Machine Learning. Morgan Kaufmann, LosAltos.Rao, R. B., Gordon, D., & Spears, W. (1995). every generalization action reallyequal opposite reaction? Analysis conservation law generalization performance. Proceedings Twelth International Conference MachineLearning, pp. 471{479 Taho City, Ca. Morgan Kaufmann.416fiFurther Experimental Evidence Utility Occam's RazorRendell, L., & Seshu, R. (1990). Learning hard concepts constructive induction:Framework rationale. Computational Intelligence, 6, 247{270.Rissanen, J. (1983). universal prior integers estimation minimum descriptionlength. Annals Statistics, 11, 416{431.Rissanen, J. (1987). Stochastic complexity. Journal Royal Statistical Society SeriesB, 49 (3), 223{239.Schaffer, C. (1992). Sparse data effect overfitting avoidance decision treeinduction. AAAI-92: Proceedings Tenth National Conference ArtificialIntelligence, pp. 147{152 San Jose, CA. AAAI Press.Schaffer, C. (1993). Overfitting avoidance bias. Machine Learning, 10, 153{178.Schaffer, C. (1994). conservation law generalization performance. Proceedings1994 International Conference Machine Learning San Mateo, Ca. MorganKaufmann.Ting, K. M. (1994). problem small disjuncts: remedy decision trees.Proceedings Tenth Canadian Conference Artificial Intelligence, pp. 63{70.Morgan Kaufmann,.Wallace, C. S., & Boulton, D. M. (1968). information measure classification. Computer Journal, 11, 185{194.Wallace, C. S., & Freeman, P. R. (1987). Estimation inference compact coding.Journal Royal Statistical Society Series B, 49 (3), 240{265.Webb, G. I. (1994). Generality significant complexity: Toward alternativesOccam's razor. Zhang, C., Debenham, J., & Lukose, D. (Eds.), AI'94 { Proceedings Seventh Australian Joint Conference Artificial Intelligence, pp. 60{67Armidale. World Scientific.417fiA Divergence CriticBundy, A., van Harmelen, F., Horn, C., & Smaill, A. (1990). Oyster-Clam system.Stickel, M. (Ed.), 10th International Conference Automated Deduction, pp. 647{648. Springer-Verlag. Lecture Notes Artificial Intelligence No. 449.Dershowitz, N., & Pinchover, E. (1990). Inductive Synthesis Equational Programs.Proceedings 8th National Conference AI, pp. 234{239. American AssociationArtificial Intelligence.Hermann, M. (1989). Crossed term rewriting systems. CRIN report 89-R-003, Centre deRecherche en Informatique de Nancy.Ireland, A. (1992). Use Planning Critics Mechanizing Inductive Proof. Proceedings LPAR'92, Lecture Notes Artificial Intelligence 624. Springer-Verlag. Alsoavailable Research Report 592, Dept AI, Edinburgh University.Ireland, A., & Bundy, A. (1992). Using failure guide inductive proof. Tech. rep., Dept.Artificial Intelligence, University Edinburgh. Available Edinburgh DAIResearch Paper 613.Kirchner, H. (1987). Schematization infinite sets rewrite rules. Applicationdivergence completion processes. Proceedings RTA'87, pp. 180{191.Protzen, M. (1992). Disproving conjectures. Kapur, D. (Ed.), 11th ConferenceAutomated Deduction, pp. 340{354. Springer Verlag. Lecture Notes ComputerScience No. 607.Thomas, M., & Jantke, K. (1989). Inductive Inference Solving Divergence KnuthBendix Completion. Proceedings International Workshop AII'89, pp. 288{303.Thomas, M., & Watson, P. (1993). Solving divergence Knuth-Bendix completionenriching signatures. Theoretical Computer Science, 112, 145{185.Walsh, T. (1994). divergence critic. Bundy, A. (Ed.), 12th Conference AutomatedDeduction, pp. 14{25. Springer Verlag. Lecture Notes Artificial Intelligence No.814.Walsh, T., Nunes, A., & Bundy, A. (1992). use proof plans sum series. Kapur,D. (Ed.), 11th Conference Automated Deduction, pp. 325{339. Springer Verlag.Lecture Notes Computer Science No. 607. Also available Edinburgh DAIResearch Paper 563.Yoshida, T., Bundy, A., Green, I., Walsh, T., & Basin, D. (1994). Coloured rippling:extension theorem proving heuristic. Cohn, A. (Ed.), Proceedings ECAI-94,pp. 85{89. John Wiley. Also available Edinburgh DAI Research Paper 779.235fiWalshway. types divergence could perhaps recognizeddivergence critic. research needed identify divergence patterns, isolatecauses propose ways fixing them. research may take advantage closelinks divergence patterns particular types generalization. instance,may possible identify specific divergence patterns need generalize commonsubterms theorem proved.Acknowledgementsresearch supported Human Capital Mobility Postdoctoral Fellowship.wish thank: Adel Bouhoula Michael Rusinowitch invaluable assistanceSpike; Pierre Lescanne inviting visit Nancy researchperformed; David Basin, Alan Bundy, Miki Hermann, Andrew Ireland, MichaelRusinowitch comments questions; reviewers comments suggestions; members Eureca Protheo groups INRIA; membersDReaM group Edinburgh MRG groups Trento Genova.ReferencesAubin, R. (1976). Mechanizing Structural Induction. Ph.D. thesis, University Edinburgh.Basin, D., & Walsh, T. (1992). Difference matching. Kapur, D. (Ed.), 11th ConferenceAutomated Deduction, pp. 295{309. Springer Verlag. Lecture Notes ComputerScience No. 607. Also available Edinburgh DAI Research Paper 556.Basin, D., & Walsh, T. (1993). Difference unification. Proceedings 13th IJCAI.International Joint Conference Artificial Intelligence. Also available TechnicalReport MPI-I-92-247, Max-Planck-Institute fur Informatik.Basin, D., & Walsh, T. (1994). Termination orderings rippling. Bundy, A. (Ed.), 12thConference Automated Deduction, pp. 466{483. Springer Verlag. Lecture NotesArtificial Intelligence No. 814.Bouhoula, A., Kounalis, E., & Rusinowitch, M. (1992). Spike: theorem-prover.Proceedings LPAR'92, Lecture Notes Artificial Intelligence 624. Springer-Verlag.Bouhoula, A., & Rusinowitch, M. (1995a). Implicit induction conditional theories. Journal Automated Reasoning, 14 (2), 189{235.Bouhoula, A., & Rusinowitch, M. (1995b). Spike user manual.. INRIA LorraineCRIN, 615 rue du Jardin Botanique, Villers-les-Nancy, France. Availableftp://ftp.loria.fr/pub/loria/protheo/softwares/Spike/.Boyer, R., & Moore, J. (1979). Computational Logic. Academic Press. ACM monographseries.Bundy, A., Stevens, A., van Harmelen, F., Ireland, A., & Smaill, A. (1993). Rippling:heuristic guiding inductive proofs. Artificial Intelligence, 62, 185{253. Alsoavailable Edinburgh DAI Research Paper No. 567.234fiA Divergence Criticrev(qrev(x; nil)) = xrev(qrev(x; cons(y; nil) )) = cons(y; x)rev(qrev(x; cons(z; cons(y; nil)) )) = cons(z; cons(y; x))...annotated sequence unique maximal difference match. annotations suggestneed wave rule,rev(qrev(X; cons(Y; nil) )) = cons(Y; rev(qrev(X; nil))) :rule allows proof go without divergence. comparison, specificgeneralization seems unable identify rule. specific generalizationleft hand side sequence gives term rev(qrev(X; Z)) (or, ignoring first termsequence, rev(qrev(X; cons(Y; Z)))). specific generalization cannot, however,identify useful pattern, rev(qrev(X; cons(Y; nil))).Nqthm contains simple test divergence based subsumption. instance,example 13 last section, Nqthm unable simplify following subgoalstep case proof,(EQUAL (ROT (LENGTH X) (APPEND X (LIST Z)))(CONS Z (ROT (LENGTH X) X))))Note lemma speculated divergence critic. Nqthm generalizes (LENGTHX) subgoal giving false conjecture,(EQUAL (ROT (APPEND X (LIST Z)))(CONS Z (ROT X))))several attempts induction generalization, Nqthm realizes proofdiverging since subgoal subsumed parent. proof therefore loop,Nqthm gives up. attempt made analyse failed proof attempt identifystarted go wrong. addition, subsumption weak test divergence, muchweaker tests based difference matching generalization. subsumption testrecognizes divergence small number failed examples last section.10. Conclusionsdescribed divergence critic, computer program attempts identify diverging proof attempts propose lemmas generalizations overcomedivergence. divergence critic proved successful; enables system Spikeprove many theorems definitions alone. divergence critic's successlargely attributed power rippling heuristic. heuristic originallydeveloped proofs using explicit induction since found several applications.Difference matching used identify accumulating term structure causing divergence. Lemmas generalizations proposed ripple term structure233fiWalshdivergence critic described works implicit (and explicit) induction setting.Second, divergence critic automatically invoked must identify prooffailing. Third, divergence critic less specialized. last two differences ectfact critics Clam usually associated failure particular preconditionheuristic. divergence pattern can, comparison, arise many differentreasons: need generalize variables apart, generalize common subterms, addlemma, etc. Fourth, divergence critic must use difference matching annotate terms;Clam, terms usually already appropriately annotated. Finally, divergence criticless tightly coupled theorem prover's inference rules heuristics. critictherefore exploit strengths prover without needing reason complexrules heuristics used. instance, divergence critic diculty identifyingdivergence complex situations like nested mutual inductions. critic also benefitspowerful simplification rules used Spike.Divergence studied quite extensively completion procedures. Twomain novelties critic described use difference matching identifydivergence, use rippling speculation lemmas overcome divergence.Dershowitz Pinchover, comparison, use specific generalization identify divergence patterns critical pairs produced completion (Dershowitz & Pinchover, 1990).Kirchner uses generalization modulo equivalence relation recognise divergencepatterns (Kirchner, 1987); meta-rules synthesized describe infinite familiesrules common structure. Thomas Jantke use generalization inductiveinference recognize divergence patterns replace infinite sequences critical pairsfinite number generalizations (Thomas & Jantke, 1989). Thomas Watson usegeneralization replace infinite set rules finite complete set enrichedsignature (Thomas & Watson, 1993).Generalization modulo equivalence enables complex divergence patters identified. However, general undecidable. specific generalization, comparison,limited. cannot recognize divergence patterns give nested wave-fronts like,s( s(x) + x) :addition, specific generalization cannot identify term structure wave-holes.example, consider divergence sequence equations produced Spike attemptsprove example 25 Section 8,rev(qrev(x; nil)) = xrev(qrev(x; cons(y; nil))) = cons(y; x)rev(qrev(x; cons(z; cons(y; nil)))) = cons(z; cons(y; x))...Divergence analysis identifies term structure accumulating within accumulator argumentqrev ,232fiA Divergence CriticUnfortunately heuristics instantiating right hand side speculated lemmasstrong enough suggest rule,X + (Y + Z ) = + (X + Z )rule, Spike finds proof commutativity multiplication without diculty.diculties speculating rule arise wave-front stuck similarposition sides equality. clues therefore suggest rippletop term tree.example 33, divergence critic proposes lemma one needed. Spikeable find proof theorem definitions alone using 16 inductions. Threeinductions equations,0+x = xs(0) + x = s(x)s(s(0)) + x = s(s(x))sequence equations satisfies divergence critic's preconditions. critic thereforeproposes wave rules moving accumulating successor functions first argumentposition +. Although proposed lemmas necessary, either give much shortersimpler proof needing 7 inductions.Example 34 lemma speculated example 24. Divergence analysis Spike'sattempt prove theorem identifies term structure accumulating second (aliasaccumulator) argument qrev . first two lemmas proposed removing termstructure use subsumed recursive definition qrev . thirdlemma also fails prevent divergence. lemma simplifies two element lists secondargument position qrev . However, divergence still occur prover cannot simplifylists occur second argument position qrev contain 3 elements.Divergence overcome introduce derived function appending onto endlist. used simplify terms list arbitrary size occurssecond argument position qrev . example, simplify rule,qrev(X; ) = app(qrev(X; nil); )Unfortunately, append occur specification theorem dicultfind heuristic would speculate rule.9. Related WorkCritics monitoring construction proofs first proposed Ireland Clamprover (Ireland, 1992). framework, failure one proof methods automaticallyinvokes critic. Various critics explicit induction developed speculatemissing lemmas, perform generalizations, look suitable case splits, etc. rippling playscentral role Clam's proof methods, many heuristics similar described(Ireland & Bundy, 1992). are, however, several significant differences. First,231fiWalshTheorem31s(x) y=y+(x y)32x y=y x33 x+(y+(z+(v+w))) = w+(x+(y+(z+v)))Lemmas speculatedTime/s{n/as(X)+Y=X+s(Y)8.0s(X)+Y=s(X+Y)17.7s(X)+Y=X+s(Y)34 qrev(qrev(x,[y]),z)=y ::qrev(qrev(x,[]),z)qrev(Y,X ::Z)) = qrev(X ::Y,Z)9.6qrev(qrev(X,Y ::Z),W)=qrev(qrev(Y ::X,Z),W)qrev(qrev(X,Y ::[Z]),W)=Z::qrev(qrev(X,[Y]),W)Table 2: divergence critic's failures.diverges, generating following sequence equations,s(y) + (x + (x y)) = s(y) + (y + (x y))s(s(y)) + (x + (x + (x y))) = s(x) + (s(y) + (x + (x y))s(s(s(y))) + (x + (x + (x + (x y)))) = s(x) + (s(s(y)) + (x + (x + (x y))))...Divergence analysis left hand sides equations suggests need ruleform,s(Y ) + (X + Z ) = F (Y + Z )Unfortunately heuristics lemma speculation suciently strong suggestsuitable instantiation F (for example, z : s(X + z )). lemma rather complexresult two overlapping divergence patterns. annotations consideredseparately, suggest rules,s(X ) + = s(X + )+ (X + Z ) = X + (Y + Z )two rules, Spike finds proof without diculty.Example 32 commutativity multiplication. divergence critic identifiesdivergence pattern proposes transverse wave rule,s(X ) + = X + s(Y )However, Spike unable prove commutativity multiplication additionrule. proof attempt somewhat simpler contains diverging sequenceequations,x + (y + (x + (x y))) = + (x + (x + (x y)))x + (y + (x + (x + (x y))) ) = + (x + (x + (x + (x y))))x + (y + (x + (x + (x + (x y)))) ) = + (x + (x + (x + (x + (x y)))))...230fiA Divergence Criticspeculate non-theorems. research optimal strength generalizationheuristics would valuable.Example 24 disappointment; lemma proposed fixes divergencedicult proved automatically, even assistance divergence critic.See example 34 end section details. Example 25 discusseddetail related work Section 9 demonstrates superiority differencematching generalization techniques divergence analysis. Examples 26 28 requirelittle discussion. Finally, examples 29 30 demonstrate critic copedivergence moderately complex theories containing conditional equations.results pleasing. Using divergence critic, 30 theorems listed (withexception 24) proved definitions alone. provide indicationdiculty theorems, Nqthm system (Boyer & Moore, 1979),perhaps best known explicit induction theorem prover, unable provehalf theorems definitions alone. precise, Nqthm failed 5, 6, 7, 8, 9,11, 12, 13, 14, 15, 18, 19, 21, 22, 24, 25, 26, 27 28. course, additionsimple lemmas, Nqthm able prove theorems. Indeed, many cases, Nqthmneeds lemmas proposed divergence critic required Spike.suggests divergence critic especially tied particular prover usedeven implicit induction setting.test hypothesis, presented output diverging proof attempt Nqthmcritic. chose commutativity multiplication perhaps simplesttheorem causes Nqthm diverge. critic proposed lemma,(EQUAL (TIMES (ADD1 X)) (PLUS (TIMES X))))TIMES PLUS primitives Nqthm's logic recursively defined firstarguments. exactly lemma needed Nqthm prove commutativitymultiplication. Nqthm fails many examples similar reasons Spike,divergence analysis identifies appropriate lemma. supports suggestiondivergence critic likely useful wide variety provers.divergence critic several limitations. Recognizing divergence is, general,undecidable since reduces halting problem. divergence critic thereforesometimes fail identify diverging proof attempt. addition, critic sometimesidentify \divergence" pattern proof attempt diverging. Even divergence correctly identified, critic sometimes fail speculate appropriatelemma. Finally, critic speculates wave-rules. Whilst many theories contain largenumber wave-rules, often useful fixing divergence, typeslemma needed.Table 2 lists four theorems divergence critic fails. problemsrepresentative different ways critic fail. two main causefailure overlapping divergence patterns, inability heuristics speculateappropriate right hand side lemma. times speculate lemmasfind proof theorem.Example 31 commuted version recursive definition multiplication (defined recursively second argument position). Spike's attempt prove theorem229fiWalshexamples 6 7, optimal rules fixing divergence. Nevertheless, either proposed rules fix divergence proved without dicultySpike. Example 9 similar example 8.Examples 10 12 require little comment. example 13, proposed lemmadicult proved automatically. However, divergence critic able identifycause diculty propose lemma allows proof go (example15). example 14, speculated lemma optimal. simpler lemma speculatedexample 13 would adequate prove theorem without divergence. speculatedlemma optimal divergence critic attempts ripple accumulatingterm structure two functors, len rot top term tree. However,sucient problem ripple one functor, rot.Examples 16 19 straightforward require discussion. example 20,critic identifies two separate divergence patterns. overcome divergence, first lemmaplus one second third therefore needed. first divergence patternoccurs sequence subgoals,len(rev(x)) = 0 + len(x)len( app(rev(x); cons(y; nil)) ) = s(0 + len(x))len( app(app(rev(x); cons(y; nil)); cons(z; nil)) ) = s(s(0 + len(x)))...Term structure accumulating second argument append. term structureremoved first rule,len( app(X; cons(Y; nil)) ) = s(len(X ))second divergence pattern occurs sequence subgoals,s(x) + len(y) = s(x + len(y))s(s(x)) + len(y) = s(s(x + len(y)))s(s(s(x))) + len(y) = s(s(s(x + len(y))))...Term structure accumulating first argument +. removed onesecond third rules,s(X ) + = s(X + )s(X ) + = X + s(Y )Examples 21 23 reasonably straightforward. lemma speculated example22 special case associativity append. powerful generalization heuristicscould speculated associativity append. However, heuristics would also228fiA Divergence Criticcauses divergence current release. speculated lemmas do, however, simplifyproof. Example 4 used text illustrate generalization heuristics. secondlemma example 5 perhaps little surprising,len(app(X; (cons(W; cons(Z; ) )))) = s(len(app(X; cons(W; )))) :Although complex first lemma, nearly good fixing divergence.example 6, lemma proposed,even( s(s(X )) + ) = even(X + )optimal. is, simplest possible lemma fixes divergence. fixdivergence, merely need one rules, s(X ) + = s(X + ) s(X ) + = X +s(Y ). Either ripple successor functions accumulating first argumentposition +. divergence critic attempts construct lemma ripple two successorfunctions across first second argument positions +. Unfortunately,critic fails find appropriate instantiation right hand side lemma.critic instead proposes rule move two successor functions top termwave-front peter out. Example 7 similar example 6.Examples 8 10 demonstrate critic cope divergence theories involving mutual recursion. example 8, Spike attempts prove induction equations,evenm(x + x)oddm(s(x) + x)evenm (s(s(x)) + x)oddm(s(s(s(x))) + x)evenm (s(s(s(s(x)))) + x)=====...truetruetruetruetruecritic identifies two inter-linking divergence patterns,evenm (x + x) = trueevenm ( s(s(x)) + x) = trueevenm ( s(s(s(s(x)))) + x) = trueoddm(s(x) + x) = trueoddm( s(s(s(x))) + x) = trueoddm( s(s(s(s(s(x))))) + x) = true......critic therefore proposes rules ripple accumulating term structuretop term peters out,evenm( s(s(X )) + ) = evenm (X + )oddm( s(s(X )) + ) = oddm(X + )227fiWalsh1Theorems(x)+x=s(x+x)Lemmas speculatedTime/ss(X)+Y=s(X+Y)7.8s(X)+Y=X+s(Y)2dbl(x)=x+x $s(X)+Y=s(X+Y)8.2dbl(0)=0, dbl(s(x))=s(s(dbl(x)))s(X)+Y=X+s(Y)3len(x @ y)=len(y @ x)len(X @ (Z ::Y))=s(len(X @ Y))3.6len(X @ (Z ::Y))=len((W ::X) @ Y)4len(x @ y)=len(x)+len(y)s(X)+Y=s(X+Y)7.2s(X)+Y=X+s(Y)5len(x @ x)=dbl(len(x))len(X @ (Z ::Y))=s(len(X @ Y))11.6len(X @ (W ::Z ::Y))=s(len(X @ (W ::Y)))6even(x+x)even(s(s(X))+Y)=even(X+Y)5.47odd(s(x)+x)odd(s(s(X))+Y)=odd(X+Y)16.08evenm (x+x)evenm (s(s(X))+Y)=evenm (X+Y)28.4oddm (s(s(X))+Y)=oddm (X+Y)9oddm (s(x)+x)evenm (s(s(X))+Y)=evenm (X+Y)65.5oddm (s(s(X))+Y)=oddm (X+Y)10evenm (x) ! half(x)+half(x)=xs(X)+Y=s(X+Y)6.0s(X)+Y=X+s(Y)11half(x+x)=xs(s(X))+Y=X+s(s(Y))11.1half(s(s(X))+Y)=half(X+Y)12half(s(x)+x)=xs(s(X))+Y=X+s(s(Y))31.0half(s(s(X))+Y)=half(X+Y)13rot(len(x),x)=xrot(len(X),X @ [Y])=Y::rot(len(X),X)2.414len(rot(len(x),x))=len(x)len(rot(X,Z @ [Y]))=s(len(rot(X,Z)))4.815rot(len(x),x @ [y])=y ::rot(len(x),x)(X @ [Y])@ Z=X @ (Y ::Z)86.3rot(len(X),(X @ [Y])@ Z)=Y ::rot(len(X),X @ Z)16len(rev(x))=len(x)len(X @ [Y])=s(len(X))2.017rev(rev(x))=xrev(X @ [Y])=Y::rev(X)1.218rev(rev(x) @ [y])=y ::xrev(X @ [Y])=Y::rev(X)16.019rev(rev(x) @ [y])=y ::rev(rev(x))rev(X @ [Y])=Y::rev(X)18.620len(rev(x @ y))=len(x)+len(y)len(X @ [Y])=s(len(X))10.0s(X)+Y=s(X+Y)s(X)+Y=X+s(Y)21len(qrev(x,[]))=len(x)len(qrev(X,Z ::Y))=s(len(qrev(X,Y)))2.222qrev(x,y)=rev(x) @(X @ [Y])@ Z=X @ (Y ::Z)3.423len(qrev(x,y))=len(x)+len(y)s(X)+Y=s(X+Y)12.0s(X)+Y=X+s(Y)24qrev(qrev(x,[]),[])=xqrev(qrev(X,[Y]),Z)=Y ::qrev(qrev(X,[]),Z)5.025rev(qrev(x,[]))=xrev(qrev(X,[Y]))=Y ::rev(qrev(X,[]))5.826qrev(rev(x),[])=xqrev(X @ [Y],Z)=Y::qrev(X,Z)5.227nth(i,nth(j,x))=nth(j,nth(i,x))nth(s(I),nth(J,Y ::X))=nth(I,nth(J,X))7.428 nth(i,nth(j,nth(k,x)))=nth(k,nth(j,nth(i,x)))nth(s(I),nth(J,Y ::X))=nth(I,nth(J,X))7.629len(isort(x))=len(x)len(insert(Y,X))=s(len(X))2.030sorted(isort(x))sorted(insert(Y,X))=sorted(X)114sorted(insert(Y,insert(Z,X)))=sorted(X)Table 1: lemmas speculated divergence critic.Notes: :: written infix cons, @ infix append, [] nil, [x] cons(x,nil).addition, even defined s(s(x)) recursion, evenm mutual recursion oddm ,rot(n; l) rotates list l n elements.226fiA Divergence Criticcritic successful identifying divergence proposing appropriate lemmasgeneralizations significant number theorems. Divergence analysis quickexamples. divergence pattern recognized usually less second.time spent looking generalizations refuting over-generalizationsconjecture disprover. usually takes 1 100 seconds. Additional heuristicspreventing over-generalization ecient implementation conjecturedisprover would speed critic considerably.8. ResultsTable 1 lists 30 theorems cause Spike diverge lemmas speculateddivergence critic analysing diverging proof attempts. problems providerepresentative sample type theorems cause divergenceidentified appropriate lemma generalization speculated. Many problemscome Clam library corpus. Part table appeared (Walsh, 1994).Times divergence critic speculate lemmas average 10runs Sun 4 running Quintus 3.1.1.Spike's proof attempt diverges example given definitions alone.30 cases, critic quickly able suggest lemma overcomes divergence.multiple lemmas proposed (with exception 20) onesucient fix divergence. every case (except 13 24) lemmas proposedsuciently simple proved automatically without introducing fresh divergence.majority cases, lemmas proposed optimal; is, simplest possiblelemmas fix divergence. cases lemma optimal, usuallyslightly complex simplest lemma fixes divergence. manyexamples, lemmas conjectured divergence analysis quicklyrejected conjecture disprover. example, example 16, divergence analysispetering heuristic suggest rule,## len( app(X; cons(Y; nil)) ) = len(X ) ##However, refuted exhaustive normalization using ground terms X .case, cancellation heuristic identifies required lemma,len( app(X; cons(Y; nil)) ) = s(len(X )) :examples deserve additional comment. example 1, divergence criticidentifies successor functions accumulating first argument position +.critic speculates lemma moving successor functions either topterm (so immediate cancellation occur) onto second argument position(so simplification recursive definition + occur). first lemmaspeculated fact generalization theorem proved. Example 2 simpleprogram verification problem taken Dershowitz Pinchover (1990). forwarddirection theorem discussed introduction. Similar divergence occursexample 1 and, generalization, lemmas speculated.Example 3 caused divergence beta-version Spike available summer1994. proof rules Spike since strengthened example longer225fiWalsh% compiling file /home/dream5/tw/work/Spike/diverge/data.double.x+x% data.double.x+x compiled module user, 0.233 sec 1,612 bytes| ?- speculate.Equations input:double(x1)=x1+x1s(x1+x1)=s(x1)+x1s(s(x1+x1))=s(s(x1))+x1s(s(s(x1+x1)))=s(s(s(x1)))+x1Lemmas speculated:s(x1)+x1=s(x1+x1)s(x1)+x99=s(x1+x99)s(x99)+x1=s(x99+x1)s(x99)+x100=s(x99+x100)s(x1)+x1=x1+s(x1)s(x1)+x99=x1+s(x99)s(x99)+x1=x99+s(x1)s(x99)+x100=x99+s(x100)Deleting lemmas subsumed:s(x1)+x99=s(x1+x99)s(x1)+x99=x1+s(x99)Merging remaining lemmas:s(x1)+x99=s(x1+x99)s(x1)+x99=x1+s(x99)yes| ?-Figure 4: Example output divergence critic.Figure 1 gives divergence critic's output problem discussed introduction.Either proposed lemmas used rewrite rule adequate fix divergence.addition, proposed lemmas suciently simple proved automatically withoutintroducing fresh divergence. first lemma rewrite rule moving accumulatingsuccessor functions first argument position + top term tree.second lemma transverse wave rule discussed Section 6 moving accumulatingsuccessor functions first argument position + second argument position.224fiA Divergence CriticPreconditions:1. sequence equations si = tiprover attempts prove induction (i = 0, 1 ...);2. exists (non trivial) G; H j ,maximal difference match sj = G(Uj ; Acc)sj+1 = G( H (Uj ) ; Acc).Postconditions:1. critic proposes rule form,G( H (U0) ; Acc) = G(U0; F (Acc) )2. F instantiated fertilization simplificationheuristics;3. lemma generalized using (augmented) primary terms equality heuristics;4. Generalized lemmas filtered type checkerconjecture disprover;5. several lemmas suggested, critic deletessubsumed.Figure 3: Speculation transverse wave rules.annotations. could also speculate hybrid wave rules ripple part wave-frontacross part term tree. However, rules appear rare. addition,hybrid wave rules often decomposed pair wave rules, onemoves wave-fronts term tree, another moves wave-frontsacross.7. Implementationdivergence critic described previous sections implemented Prolog.system consists 787 lines code defining approximate 100 different Prolog predicates. recently cut version incorporated directly within Spikesystem written Caml Light (Bouhoula & Rusinowitch, 1995b). outputSpike parsed generate input critic. input consists of: equationsprover attempts prove induction; sort information (for type checkerdifference matcher); recursive argument positions (for constructing primary terms);rewrite rules defining theory (used conjecture disprover).223fiWalshrule allows proof go without divergence.Speculated transverse wave rules generalized using extended primary termsheuristic described Section 5. divergence critic also generalizes transverse waverules means equality heuristic. heuristic attempts cancel equal outermostfunctors possible. example, consider theorem,8x; : (x + y) , x =addition defined recursively second argument position subtractiondefined rewrite rules,X ,0 = X0,X = 0s(X) , s(Y) = X , Y:Spike's attempt prove theorem diverges generating (amongst others) goals,(x + ) , x =(s(x) + ) , x = s(y )(s(s(x)) + ) , x = s(s(y ))...Divergence analysis identifies accumulating term structure within equations,(x + ) , x =( s(x) + ) , x = s(y)( s(s(x)) + ) , x = s(s(y ))...unique maximal difference match. annotations suggest needtransverse rule,( s(X ) + ) , X = (X + s(Y ) ) , X:equality heuristic deletes equal outermost function, z : z , X . givesgeneral lemma,s(X ) + = X + s(Y ) :speculated lemmas filtered type checker ensure erasurewell typed. Speculated lemmas also filtered conjecture disprover guardover-generalization.actions critic summarized Figure 3. specification preconditionspostconditions uses second order variables limited manner. implementation merely requires second order matching first order difference matching.preconditions postconditions easily generalised include multiple nested222fiA Divergence Criticqrev(a; b) = app(rev(a); b)qrev(a; cons(c; b) ) = app( app(rev(a); cons(c; nil)) ; b)qrev(a; cons(c; cons(d; b)) ) = app( app(app(rev(a); cons(c; nil)); cons(d; nil)) ; b)...unique maximal difference match. Rather move accumulating termstructure right hand side equations top term, much simplermove accumulating term structure first onto second argumentoutermost append. critic therefore proposes transverse wave rule, preservesskeleton moves difference onto different argument position. example,rule form,app( app(rev(A); cons(C; nil)) ; B) = app(rev(A); F (B) ):moving difference onto another argument position, difference may change syntactically. right hand side lemma therefore partially determined.instantiate F , critic uses two heuristics: fertilization simplification.fertilization heuristic uses matching find instantiation F enablesimmediate fertilization. case, matching universally quantified variable binduction hypothesis suggests,app( app(rev(A); cons(C; nil)) ; B) = app(rev(A); cons(C; B) ):Finally critic generalizes lemma using extended primary term heuristic(i.e., augmenting recursive positions wave-hole positions). gives rule,app( app(A; cons(C; nil)) ; B) = app(A; cons(C; B) ):exactly rule needed Spike complete proof. addition, simpleenough proved without divergence; true ungeneralized rule.heuristic used instantiate right hand side speculated lemmasimplification heuristic. heuristic uses regular matching find instantiationF enable wave-front simplified using one recursive definitions.Consider dbl theorem introduction. Divergence analysis identifies successor functions accumulating first argument position +. accumulating termstructure either moved top term tree alternatively onto secondargument position + using transverse wave rule form,s(X ) + = X + F (Y ) :right hand side transverse wave rule instantiated simplification heuristic.wave-front right hand side simplified rewrite rule recursivelydefining + F instantiated z : s(z ). is, rule,s(X ) + = X + s(Y ) :221fiWalshf0; s(Y)g cover set natural numbers, two rules merged give,sorted( insert(Y; X) ) = sorted(X):6. Transverse Wave Ruleslemmas speculated far moved accumulating term structure directly topterm removed cancellation petering out. alternative wayremoving accumulating term structure move onto another argument position where:either removed matching \sink", universally quantified variableinduction hypothesis; moved upwards rewriting recursivedefinitions. Annotated rewrite rules preserve skeleton move wave-frontsacross argument positions called transverse wave rules (Bundy et al., 1993).Theorems involving functions accumulators provide rich source examplesrewrite rules prevent divergence.Consider, example, theorem correctness tail recursive list reversal,8a; b : qrev(a; b) = app(rev(a); b)b universally quantified, rev naive list reversal using append,qrev tail recursive list reversal building reversed list second argument position.functions defined rewrite rules,rev(nil)rev(cons(H; T))qrev(nil; R)qrev(cons(H; T); R)====nilapp(rev(T); cons(H; nil))Rqrev(T; cons(H; R)):Spike's attempt prove theorem diverges generating following sequence equa-tions prover attempts show induction,qrev(a; b) = app(rev(a); b)qrev(a; cons(c; b)) = app(app(rev(a); cons(c; nil)); b)qrev(a; cons(c; cons(d; b))) = app(app(app(rev(a); cons(c; nil)); cons(d; nil)); b)...Difference matching identifies term structure accumulating within equationscausing divergence,220fiA Divergence Critics(0) + len(b) = s(len(b))s(s(0)) + len(b) = s(s(len(b)))...Difference matching identifies term structure causing divergence,0 + len(b) = len(b)s(0) + len(b) = s(len(b))s(s(0)) + len(b) = s(s(len(b)))...unique maximal difference match. annotations suggest needwave rule,s(0) + len(B) = s(0 + len(B)) :set candidate terms generalization constructed computing intersectionprimary terms two sides rule. case, primary terms lefthand side set fs(0) + len(B); len(B); Bg, primary terms right handside set fs(0 + len(B)); 0 + len(B); len(B); Bg. intersection primary termsthus set flen(B); Bg. critic picks members intersection generalize newvariables. Picking B justs gives equivalent lemma renaming variables. Pickinglen(B) gives generalization,s(0) + = s(0 + Y) :reason considering primary terms recursive definitions typicallyprovide wave rules removing term structure accumulates positions.addition primary terms, divergence critic therefore also considers positionswave-holes (but wave-fronts) skeleton lemma speculated.motivation extension speculated lemma allow accumulating termstructure moved wave-hole positions; positions therefore also candidates generalization. Positions wave-fronts included since wantspeculate lemma move term structure positions.instance, wave-hole first argument + last example,0 also included intersection set candidate terms generalization. Picking 0generalize gives,s(X) + = s(X + Y) :speculated lemma general possible. rule allows proof gowithout divergence.critic also heuristic merging speculated lemmas. instance,theorem sorted(isort(x)), critic speculates several rules including,sorted( insert(0; X) ) = sorted(X)sorted( insert(s(Y); X) ) = sorted(X)219fiWalsh1. critic proposes rule form,G( H (U0) ) = F (G(U0))2. F instantiated cancellation peteringheuristics;3. Lemmas filtered type checker conjecture disprover;4. several lemmas suggested, critic deletessubsumed.Figure 2: Postconditions divergence critic5. Generalizationmajor cause divergence need generalize. lemmas proposedcritic fix divergence, attempting prove lemmas causefresh divergence. addition, several speculated lemmas sometimes replacedsingle generalization. Generalized lemmas also lead shorter, elegantnatural proofs. critic therefore attempts generalize lemma speculated, usingconjecture disprover guard over-generalization.main heuristic used generalization extension primary term heuristic(Aubin, 1976). primary terms terms encountered term exploredroot leaves ignoring non-recursive argument positions functions.notion recursive argument position used critic defined BouhoulaRusinowitch (1995a) used Spike performing inductions.Consider, example, theorem,8a; b : len(a) + len(b) = len(app(a; b))+ defined recursively second argument, len app definedmeans rewrite rules,len(nil)len(cons(H; T))app(nil; T)app(cons(H; T); R)====0s(len(T))cons(H; app(T; R)):problem taken Clam library corpus (Bundy et al., 1990). Spike's attemptprove theorem diverges. One sequences equations generated is,0 + len(b) = len(b)218fiA Divergence CriticSpike's diverging attempt prove theorem generates equations,nth(s(i); nth(j; x)) = nth(s(j ); nth(i; x))nth(s(s(i)); nth(j; cons(y; x))) = nth(s(j ); nth(i; x))nth(s(s(s(i))); nth(j; cons(z; cons(y; x)))) = nth(s(j ); nth(i; x))...Divergence analysis identifies term structure accumulating two different places,nth(s(i); nth(j; x)) = nth(s(j ); nth(i; x))nth( s(s(i)) ; nth(j; cons(y; x) )) = nth(s(j ); nth(i; x))nth( s(s(s(i))) ; nth(j; cons(z; cons(y; x)) )) = nth(s(j ); nth(i; x))...unique maximal difference match. divergence pattern suggests needrewrite rule form,nth( s(I ) ; nth(J; cons(Y; X ) )) = F (nth(I; nth(J; X ))) :petering heuristic instantiates F identity function z : z giving rule,nth( s(I ) ; nth(J; cons(Y; X) )) = nth(I; nth(J; X )):rule allows proof go without divergence.Since erasure wave rule must properly typed, sort information usedprune inappropriate instantiations F . speculated lemmas therefore filteredtype checker. Speculated lemmas also filtered conjecture disprover.con uent set rewrite rules exists ground terms, exhaustive normalizationrepresentative set ground instances equations used filter nontheorems. Alternatively, prover could used filter non-theorems. Unlikemany induction theorem provers, Spike refute conjectures since inferencerules refutationally complete conditional theories axioms groundconvergent defined functions completely defined free constructors (Bouhoula &Rusinowitch, 1995a). techniques disproving conjectures described Protzen(1992).critic's lemma speculation summarized Figure 2 (using variablenames preconditions). specification uses second order variableslimited manner. First order difference matching merely required construct lemmas.preconditions, specification postconditions easily extendeddeal multiple nested wave-fronts (as nth(i; nth(j; l)) = nth(j; nth(i; l))example). Since rules proposed critic move wave-fronts top term,usually introduce fresh divergence rare cases cancellation fertilizationfails. unlikely since cancellation petering heuristics attempt ensureprecisely cancellation fertilization take place.217fiWalshdivergence pattern suggests F instantiated z : s(z) enable immediate cancellation. Thus, required, cancellation heuristic suggests rule,s(X ) + = s(X + ) :heuristic used instantiate right hand side speculated lemmaspetering out. moving differences top term, may disappearaltogether. Consider, example, theorem,8l : sorted(isort(l)) = trueisort insertion sort sorted true iff list sorted order. definedconditional rewrite rules,sorted(nil) = truesorted(cons(X; nil)) = trueX < ! sorted(cons(X; cons(Y; Z ))) = sorted(cons(Y; Z ))isort(nil) = nilisort(cons(X; )) = insert(X; isort(Y ))insert(X; Z ), inserts element X list Z order, X <defined rewrite rules,0 < X = trues(X ) < 0 = falses(X ) < s(Y ) = X <insert(X; nil) = cons(X; nil)X < ! insert(X; cons(Y; Z )) = cons(X; cons(Y; Z )):(X < ) ! insert(X; cons(Y; Z )) = cons(Y; insert(X; Z ))Divergence analysis Spike's attempt prove theorem suggests needrule form,sorted( insert(Y; X) ) = F (sorted(X) :petering heuristic instantiates F identity function z : z. gives rule,sorted( insert(Y; X) ) = sorted(X):rule allows proof go without divergence.complex example, consider theorem,8i; j; l : nth(i; nth(j; l)) = nth(j; nth(i; l))nth defined rewrite rules,nth(0; L) = Lnth(N; nil) = nilnth(s(N ); cons(H; )) = nth(N; ):216fiA Divergence Criticnested annotations. allows critic recognise multiple sources divergenceequation. Techniques identify accumulating term structure specificgeneralization (Dershowitz & Pinchover, 1990) cannot cope divergence patternsgive rise nested annotations (see Section 9 details).specification preconditions left length sequence undefined.sequence length 2, critic preemptive. is, propose lemmaanother induction attempted divergence begins. short sequence risksidentifying divergence none exists. hand using long sequence expensive test allows prover waste time diverging proof attempts. Empirically,good compromise appears look sequences length 3. cheaptest reliable. identify accumulating term structure, appears sucientuse ground difference matching alpha conversion variable names. existsfast polynomial algorithm perform difference matching based upon grounddifference matching algorithm using dynamic programming (Basin & Walsh, 1993). Sinceskeleton must well typed (along erasure), algorithm extended usesort information prune potential difference matches.4. Lemma SpeculationOne way removing accumulating nested term structure propose wave rulemoves difference top term leaving skeleton unchanged. hopeeither cancel wave-fronts side equalitydisappear process moved. dbl theorem, generalization(which discussed next section) divergence pattern suggests rule form,s(X ) + = F (X + )F second order variable need instantiate. Instantiating F ultimately dicult synthesis problem hope heuristics worktime. Two heuristics used divergence critic instantiate Fcancellation petering out.cancellation heuristic uses difference matching identify term structure accumulating opposite side sequence would allow cancellation occur. Failingthat, cancellation heuristic looks suitable term structure cancel newsequence (the original sequence usually divergence pattern step case, whilstnew sequence usually divergence pattern base case). dbl example, successorfunctions accumulate top left hand side diverging equations,s(x + x) = s(x) + xs(s(x + x)) = s(s(x)) + xs(s(s(x + x))) = s(s(s(x))) + x...215fiWalshcritic attempts find accumulating nested term structuresequence causing divergence. case, successor functions accumulatingfirst argument +. identify accumulating term structure, critic usesdifference matching. Difference matching successive equations gives annotated sequence,s(x + x) = s(x) + xs(s(x + x)) = s(s(x)) + xs(s(s(x + x))) = s(s(s(x))) + x...unique maximal difference match.critic tries speculate lemma used rewrite rule moveaccumulating nested term structure way. case, critic speculatesrule moving successor function first argument +. is, rule,s(X ) + = s(X + ) :rule, Spike able prove dbl theorem without divergence. addition,rule suciently simple proved without assistance. heuristics usedcritic perform lemma speculation described detail next twosections.divergence analysis performed critic summarised Figure 1. analysing1. sequence equations si = tiprover attempts prove induction (i = 0, 1 ...);2. exists (non trivial) G; H j ,maximal difference match sj = G(Uj ), sj +1 =G( H (Uj ) ).Figure 1: Preconditions divergence criticdivergence, consider equations prover attempts prove induction. includes equation induction proof succeeds oftensuggest useful patterns. \non-trivial" wish exclude z : z, identity substitution. H thus accumulating nested term structure appears causingdivergence. dbl example, H z : s(z), G z : z + x, U0 s(x). AlthoughG H second order variables, second order nature divergence analysislimited. Indeed, implementation critic merely requires first order differencematching polynomial. simplicity, preconditions ignore orientationequations. addition, preconditions easily generalised include multiple214fiA Divergence CriticRippling several desirable properties. highly goal directed, manipulatingdifferences induction hypothesis induction conclusion.annotations restrict application rewrite rules, rippling also involves littlesearch. Difference matching rippling proved useful domains outside explicitinduction. example, used sum series (Walsh, Nunes, & Bundy, 1992)prove limit theorems (Yoshida, Bundy, Green, Walsh, & Basin, 1994). restpaper, show difference matching rippling also useful identifyingcorrecting divergence prover neither uses explicit rules induction usesannotations control rewriting.3. Divergence Analysisinitial problem recognizing proof diverging. Various properties rewriterules identified cause divergence like, example, forwards backwardscrossed systems (Hermann, 1989). However, properties fail capture divergingrewrite systems since problem is, general, undecidable. divergence critic insteadstudies proof attempt looking patterns divergence; attempt made analyserewrite rules structures give rise divergence. advantageapproach critic need know details rewrite rules applied,type induction performed, control structure used prover. criticthus recognise divergence patterns arising complex mutual multiple inductionslittle diculty divergence patterns arising simple straightforwardinductions. disadvantage approach critic identify \divergence"pattern none exists. Fortunately, cases appear rare, evenoccur, critic usually suggests lemma generalization gives shorterelegant proof (see Section 8 example).illustrate ideas behind critic's divergence analysis, consider theoremintroduction,8n : dbl(n) = n + n:divergence critic first partitions sequence equations prover attemptsprove induction. necessary since several diverging sequences may interleavedprover's output. Several heuristics used reduce number partitionsconsidered. useful heuristic parentage sequence partitionedequation derived previous one. is, equations liesingle branch proof tree. particular, base case step case inductionpartitioned different sequences. heuristics used include:function constant symbols occur one equation occur next equationpartition, weights equations partition form simple arithmeticprogression. case, single open branch proof tree,s(x + x) = s(x) + xs(s(x + x)) = s(s(x)) + xs(s(s(x + x))) = s(s(s(x))) + x...213fiWalshannotated term r. Difference matching unitary. is, two termsone difference match. example, s(s(x)) s( s(x) ) difference matchess(s(x)) s(x). number difference matches reduced computemaximal difference match wave-fronts high possible term tree.formal definition well founded ordering annotated terms givenBasin Walsh (1994).aim rippling rewrite annotated induction conclusion skeleton,induction hypothesis, preserved differences, wave-fronts movedharmless places (for example, top term). rewriting succeeds,able appeal induction hypothesis. rewrite annotated inductionconclusion, use following annotated rewrite rules, wave rules:dbl( s(X ) ) = s(s(dbl(X )))X + s(Y ) = s(X + )s(X ) + = s(X + )(1)(2)(3)first two annotated rewrite rules derived recursive definitions dbl+ whilst second derived lemma proposed end introduction.annotated rewrite rules preserves skeleton term rewritten,moves wave-fronts higher term tree. Wave rules guarantee this: wave ruleannotated rewrite rule identical skeleton left right hand sidesmoves wave-fronts well founded direction like, instance, top term tree(Basin & Walsh, 1994).Rippling left hand side annotated induction conclusion using (1) yields,s(s(dbl(x))) = s(x) + s(x) :rippling right hand side (2) gives,s(s(dbl(x))) = s( s(x) + x) :Finally rippling (3) right hand side yields,s(s(dbl(x))) = s(s(x + x)) :wave-fronts top term, successfully rippled sidesequality. appeal induction hypothesis left hand side giving,s(s(x + x)) = s(s(x + x)) :simple identity proof complete. Note complete proof,needed rewrite lemma, (3). aim divergence critic describedpaper propose lemmas.212fiA Divergence CriticSection 2, describe difference matching rippling, two key ideas heartdivergence critic. outline difference matching identifies accumulatingterm structure causing divergence (Section 3). Section 4 6, showlemmas speculated \ripple" term structure way. Section 5,describe heuristics used generalizing lemmas. Finally, implementationresults described Sections 7 8.2. Difference matching ripplingRippling powerful heuristic developed Edinburgh proving theorems involvingexplicit induction (Bundy, Stevens, van Harmelen, Ireland, & Smaill, 1993) implemented Clam theorem prover (Bundy, van Harmelen, Horn, & Smaill, 1990).step case inductive proof, induction conclusion typically differs inductionhypothesis addition constructors destructors. Rippling uses annotationsmark differences applies annotated rewrite rules remove them.simple example, consider theorem discussed introduction.step case, induction hypothesis is,dbl(x) = x + xinduction conclusion is,dbl(s(x)) = s(x) + s(x):\difference match" induction conclusion induction hypothesis (Basin& Walsh, 1992), obtain following annotated induction conclusion,dbl( s(x) ) = s(x) + s(x) :annotation consists wave-front, box wave-hole, underlined term. Wavefronts always one functor thick (Basin & Walsh, 1994). is, every wave-frontone immediate subterm annotated wave-hole. make presentation simpler,display adjacent wave-fronts merged. Thus, s(s(x)) syntactic sugarannotated term, s( s(x) ) . Wave-fronts also include arrows indicatewhether moving towards top term tree towards leaves.extension can, however, safely ignored here.skeleton annotated term formed deleting everything appearswave-front wave-hole. erasure annotated terms formeddeleting annotations terms contain. case, skeletonannotated induction conclusion identical induction hypothesis, erasureannotated induction conclusion unannotated induction conclusion. Differencematching guarantees this; is, difference matching induction conclusioninduction hypothesis annotates induction conclusion skeleton matchesinduction hypothesis.Formally, r difference match substitution iff (skeleton(r)) =erase(r) = skeleton(r) erase(r) build skeleton erasure211fiWalshalpha convert variable names necessary. Rewriting induction conclusionrecursive definitions dbl + gives,s(s(dbl(x))) = s(s(x) + x):outermost successor functions either side equality cancelled,s(dbl(x)) = s(x) + x:prover fertilizes induction hypothesis left hand side,s(x + x) = s(x) + x:equation cannot simplified another induction performed. Unfortunately,generates diverging sequence subgoals,s(x + x)s(s(x + x))s(s(s(x + x)))s(s(s(s(x + x))))s(s(s(s(s(x + x)))))=====...s(x) + xs(s(x)) + xs(s(s(x))) + xs(s(s(s(x)))) + xs(s(s(s(s(x))))) + xproblem prover repeatedly tries induction x unable simplifysuccessor functions introduces first argument position +. proofgo without divergence rewrite rule,s(X ) + = s(X + ):rule \ripples" accumulating successor functions first argument position +.rewrite rule derived lemma,8x; : s(x) + = s(x + y):commuted version recursive definition addition is, coincidently,generalization first subgoal. lemma proved without divergenceinduction variable, occurs second argument position +.paper describe simple \divergence critic", computer program attemptsautomate process. divergence critic identifies proof attempt divergingmeans \difference matching" procedure. critic proposes lemmasgeneralizations hopefully allow proof go without divergence. Althoughcritic designed work prover Spike, also work inductionprovers (Walsh, 1994). Spike rewrite based theorem prover first order conditionaltheories. contains powerful rules case analysis, simplification implicit inductionusing notion test set. Unfortunately, case inductive theoremprovers, attempts prove many theorems diverge without appropriate generalizationaddition suitable lemma.210fiJournal Artificial Intelligence Research 4 (1996) 209-235Submitted 1/96; published 4/96Divergence Critic Inductive Prooftoby@itc.itToby WalshIRST, Location Pante di PovoI38100 Trento, ITALYAbstractInductive theorem provers often diverge. paper describes simple critic, computer program monitors construction inductive proofs attempting identifydiverging proof attempts. Divergence recognized means \difference matching"procedure. critic proposes lemmas generalizations \ripple" differences away proof go without divergence. critic enablestheorem prover Spike prove many theorems completely automatically definitions alone.1. IntroductionTwo key problems inductive theorem proving proposing lemmas generalizations.prover's divergence often suggests user appropriate lemma generalizationenable proof go without divergence. simple example, considertheorem,8n : dbl(n) = n + n:part simple program verification problem (Dershowitz & Pinchover, 1990).Addition doubling defined recursively means rewrite rules,X+0 = XX + s(Y ) = s(X + )dbl(0) = 0dbl(s(X )) = s(s(dbl(X )))s(X ) represents successor X (that is, X + 1). adopted Prologconvention writing meta-variables like X upper case.theorem prover Spike (Bouhoula, Kounalis, & Rusinowitch, 1992) fails provetheorem. proof attempt begins simple one step induction x. basecase trivial. step case, induction hypothesis is,dbl(x) = x + xinduction conclusion is,dbl(s(x)) = s(x) + s(x):ease presentation, variables paper are, here, sometimes renamedintroduced Spike. effect results prover divergence criticc 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiJournal Artificial Intelligence Research 4 (1996) 129-145Submitted 11/95; published 3/96Active Learning Statistical ModelsDavid A. CohnZoubin GhahramaniMichael I. JordanCenter Biological Computational LearningDept. Brain Cognitive SciencesMassachusetts Institute TechnologyCambridge, 02139 USAcohn@harlequin.comzoubin@cs.toronto.edujordan@psyche.mit.eduAbstractmany types machine learning algorithms, one compute statistically \optimal" way select training data. paper, review optimal data selectiontechniques used feedforward neural networks. showprinciples may used select data two alternative, statistically-based learning architectures: mixtures Gaussians locally weighted regression. techniquesneural networks computationally expensive approximate, techniquesmixtures Gaussians locally weighted regression ecient accurate. Empirically, observe optimality criterion sharply decreases number trainingexamples learner needs order achieve good performance.1. Introductiongoal machine learning create systems improve performancetask acquire experience data. many natural learning tasks, experiencedata gained interactively, taking actions, making queries, experiments.machine learning research, however, treats learner passive recipient dataprocessed. \passive" approach ignores fact that, many situations,learner's powerful tool ability act, gather data, uence worldtrying understand. Active learning study use ability effectively.Formally, active learning studies closed-loop phenomenon learner selecting actions making queries uence data added training set. Examplesinclude selecting joint angles torques learn kinematics dynamics robotarm, selecting locations sensor measurements identify locate buried hazardouswastes, querying human expert classify unknown word natural languageunderstanding problem.actions/queries selected properly, data requirements problemsdecrease drastically, NP-complete learning problems become polynomial computation time (Angluin, 1988; Baum & Lang, 1991). practice, active learning offersgreatest rewards situations data expensive dicult obtain,environment complex dangerous. industrial settings training point may takedays gather cost thousands dollars; method optimally selecting pointscould offer enormous savings time money.c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiCohn, Ghahramani & Jordannumber different goals one may wish achieve using active learning. One optimization, learner performs experiments find set inputsmaximize response variable. example optimization problem wouldfinding operating parameters maximize output steel mill candy factory.extensive literature optimization, examining cases learnerprior knowledge parameterized functional form cases learnerknowledge; latter case generally greater interest machine learningpractitioners. favored technique kind optimization usually form response surface methodology (Box & Draper, 1987), performs experiments guidehill-climbing input space.related problem exists field adaptive control, one must learn controlpolicy taking actions. control problems, one faces complication valuespecific action may known many time steps taken. Also, control(as optimization), one usually concerned performing well learningtask must trade exploitation current policy exploration may improveit. subfield dual control (Fe'ldbaum, 1965) specifically concerned findingoptimal balance exploration control learning.paper, restrict examining problem supervised learning:based set potentially noisy training examples = f(xi; yi )gmi=1, xi 2 Xyi 2 , wish learn general mapping X ! . robot control, mapping maystate action ! new state; hazard location may sensor reading ! target position.contrast goals optimization control, goal supervised learningable eciently accurately predict given x.active learning situations, learner responsible acquiring trainingset. Here, assume iteratively select new input x~ (possibly constrainedset), observe resulting output y~, incorporate new example (~x; y~) trainingset. contrasts related work Plutowski White (1993), concernedfiltering existing data set. case, x~ may thought query, experiment,action, depending research field problem domain. questionconcerned choose x~ try next.many heuristics choosing x~, including choosing places don'tdata (Whitehead, 1991), perform poorly (Linden & Weber, 1993),low confidence (Thrun & Moller, 1992), expect change model (Cohn,Atlas, & Ladner, 1990, 1994), previously found data resulted learning(Schmidhuber & Storck, 1993). paper consider one may select x~statistically \optimal" manner classes machine learning algorithms. firstbrie review statistical approach applied neural networks, describedearlier work (MacKay, 1992; Cohn, 1994). Then, Sections 3 4 consider twoalternative, statistically-based learning architectures: mixtures Gaussians locallyweighted regression. Section 5 presents empirical results applying statistically-basedactive learning architectures. optimal data selection neural networkcomputationally expensive approximate, find optimal data selectiontwo statistical models ecient accurate.130fiActive Learning Statistical Models2. Active Learning { Statistical Approachbegin defining P (x; ) unknown joint distribution x , P (x)known marginal distribution x (commonly called input distribution).denote learner's output input x, given training set y^(x; D).1 writeexpected error learner follows:ZhxET (^y (x; D) , (x))2 jx P (x)dx;(1)ET [] denotes expectation P (y jx) training sets D. expectationinside integral may decomposed follows (Geman, Bienenstock, & Doursat, 1992):hhET (^y (x; D) , y(x))2 jx = E (y (x) , E [y jx])2+ (EDh [^y (x; D)] , E [y jx])2+ED (^y (x; D) , ED [^y(x; D)])2(2)ED [] denotes expectation training sets remaining expectationsright-hand side expectations respect conditional density P (y jx).important remember case active learning, distribution maydiffer substantially joint distribution P (x; ).first term Equation 2 variance given x | noisedistribution, depend learner training data. second termlearner's squared bias, third variance; last two terms comprisemean squared error learner respect regression function E [y jx].second term Equation 2 zero, say learner unbiased. shall assumelearners considered paper approximately unbiased; is,squared bias negligible compared overall mean squared error. Thusfocus algorithms minimize learner's error minimizing variance:hy2^ y2^(x) = ED (^y (x; D) , ED [^y (x; D)])2 :(3)(For readability, drop explicit dependence x | unless denoted otherwise, y^ y2^ functions x D.) active learning setting, chosenx-component training set D; indicate rewriting Equation 3Ey2^ = (^y , hy^i)2 ;hi denotes ED [] given fixed x-component D. new input x~ selectedqueried, resulting (~x; y~) added training set, y2^ change.denote expectation (over values y~) learner's new varianceEh~y2^ = ED[(~x;y~) y2^jx~ :(4)1. present equations univariate setting. results paper apply equally multivariate case.131fiCohn, Ghahramani & Jordan2.1 Selecting Data Minimize Learner Variancepaper consider algorithms active learning select data attemptminimize value Equation 4, integrated X . Intuitively, minimization proceedsfollows: assume estimate y2^, variance learner x. If,new input x~, knew conditional distribution P (~y jx~), could computeestimate learner's new variance x given additional example x~.true distribution P (~y jx~) unknown, many learning architectures let us approximategiving us Destimatesmean variance. Using estimated distribution y~,E2estimate ~y^ , expected variance learner querying x~.EGiven estimate ~y2^ , applies given x given query x~, mustintegrate x input distribution compute integrated average variancelearner.D Epractice, compute Monte Carlo approximation integral, evaluating ~y2^ number reference points drawn according P (x). queryingx~ minimizes average expected variance reference points, solidstatistical basis choosing new examples.2.2 Example: Active Learning Neural Networksection review use techniques Optimal Experiment Design (OED)minimize estimated variance neural network (Fedorov, 1972; MacKay, 1992; Cohn,1994). assume given learner y^ = fw^ (), training set = f(xi ; yi)gmi=1parameter vector estimate w^ maximizes likelihood measure given D. If,example, one assumes data produced process whose structure matchesnetwork, noise process outputs normal independentlyidentically distributed, negative log likelihood w^ given proportional2 = m1Xi=1(yi , y^(xi ))2 :maximum likelihood estimate w^ minimizes 2.estimated output variance networky2^ 2!@ y^(x) @ 2 2 ,1 @ y^(x) ; (MacKay, 1992)@w@w2@wtrue variance approximated second-order Taylor series expansion around2. estimate makes assumption @ y^=@w locally linear. Combinedassumption P (yDjx)Eis Gaussian constant variance x, one derive closedform expression ~y2^ . See Cohn (1994) details.practice, @ y^=@w may highly nonlinear, P (y jx) may far Gaussian;spite this, empirical results show works well problems (Cohn, 1994).advantage grounded statistics, optimal given assumptions.Furthermore, expectation differentiable respect x~. such, applicablecontinuous domains continuous action spaces, allows hillclimbing find x~minimizes expected model variance.132fiActive Learning Statistical Modelsneural networks, however, approach many disadvantages. additionrelying simplifications assumptions hold approximately, processcomputationally expensive. Computing variance estimate requires inversion jwjjwjmatrix new example, incorporating new examples network requiresexpensive retraining. Paass Kindermann (1995) discuss Markov-chain based samplingapproach addresses problems. rest paper, considertwo \non-neural" machine learning architectures much amenable optimaldata selection.3. Mixtures Gaussiansmixture Gaussians model powerful estimation prediction techniqueroots statistics literature (Titterington, Smith, & Makov, 1985); has, lastyears, adopted researchers machine learning (Cheeseman et al., 1988; Nowlan,1991; Specht, 1991; Ghahramani & Jordan, 1994). model assumes dataproduced mixture N multivariate Gaussians gi, = 1; :::; N (see Figure 1).context learning random examples, one begins producing joint densityestimate input/output space X based training set D. EM algorithm(Dempster, Laird, & Rubin, 1977) used eciently find locally optimal fitGaussians data. straightforward compute y^ given x conditioningjoint distribution x taking expected value.y12gg2g31xFigure 1: Using mixture Gaussians compute y^. Gaussians model datadensity. Predictions made mixing conditional expectationsGaussian given input x.One benefit learning mixture Gaussians fixed distinctioninputs outputs | one may specify subset input-output dimensions,compute expectations remaining dimensions. one learned forward modeldynamics robot arm, example, conditioning outputs automatically givesmodel arm's inverse dynamics. mixture model, also straightforwardcompute mode output, rather mean, obviates manyproblems learning direct inverse models (Ghahramani & Jordan, 1994).133fiCohn, Ghahramani & JordanGaussian gi denote input/output means x;i y;i vari2 , 2 xy;i respectively. express probabilityances covariances x;iy;ipoint (x; ), given gi11,1(5)P (x; ji) = p exp , 2 (x , ) (x , )2 ji jdefined"x = xy#"= x;iy;i#"#2xy;i := x;i2xy;i y;ipractice, true means variances unknown, estimated datavia EM algorithm. (estimated) conditional variance given x22 , xy;i :y2jx;i = y;i2x;iconditional expectation y^i variance y2^;i given x are:2!y2jx;i(x,)xy;ix;i2y^i = y;i + 2 (x , x;i ); y^;i = n 1 + 2:(6)x;ix;iHere, ni amount \support" Gaussian gi training data.computedni =Xj =1P (xj ; yj ji) :k=1 P (xj ; yj jk)PNexpectations variances Equation 6 mixed according probabilitygi responsible x, prior observing :hi hi (x) = PNP (xji) ;j =1 P (xjj )"2#(x,)1x;i:(7)P (xji) = q 2 exp , 222x;ix;iinput x then, conditional expectation y^ resulting mixture variancemay written:NN h2i 22!XX(x,)yjx;ix;i2y^ = h y^ ; =1+;i=1y^i=1ni2x;iassumed y^i independent calculating y2^. termscomputed eciently closed form. also worth noting y2^ one manyvariance measures might interested in. If, example, mapping stochasticallymultivalued (that is, Gaussians overlapped significantly x dimension), maywish prediction y^ ect likely value. case, y^ would mode,preferable measure uncertainty would (unmixed) variance individualGaussians.134fiActive Learning Statistical Models3.1 Active Learning Mixture Gaussianscontext active learning, assuming input distribution P (x) known.mixture Gaussians, one interpretation assumption know x;i2 Gaussian. case, application EM estimate y;i , 2 ,x;iy;ixy;i .Generally however, knowing input distribution correspond knowing2 Gaussian. may simply know, example, P (x)actual x;i x;iuniform, approximated set sampled inputs. cases, must use2 addition parameters involving . simply estimateEM estimate x;i x;ivalues training data, though, estimating joint distributionP (~x; ji) instead P (x; yji). obtain proper estimate, must correct Equation 5follows:(8)P (x; yji) = P (~x; ji) PP ((~xxjjii)) :Here, P (~xji) computed applying Equation 7 given mean x variancetraining data, P (xji) computed applying equation using meanx variance set reference data drawn according P (x).goal inD activelearning minimize variance, selectexamplesEtrainingE22x~ minimize ~y^ . mixture Gaussians, compute ~y^ eciently.model's estimated distribution y~ given x~ explicit:P (~y jx~) =NXh~i P (~yjx~; i) =NX~hi N (^yi (~x); y2jx;i(~x));i=1i=1h~ hi (~x), N (; 2) denotes normal distribution mean variance2. Given this, model change gi separately, calculating expectedvariance given new point sampled P (~y jx~; i) weight change ~hi . newexpectations combine form learner's new expected varianceEN h2i ~y2jx;iE2!X(x,)x;i2~y^ =1+(9)2~x;ii=1 ni + hiexpectation computed exactly closed form:E22 + (^2~2EEEn~h(~x),)ny;iyjx~;i2 = y;i +2 , xy;i ;~y;i;~y2jx;i = ~y;i22~~x;ini + hi(ni + hi )EE~n2~h22 (~x , x;i )22~xy;i = ni xy;i~ + ni hi (~x , x;i)(^~yi (~2x) , y;i ) ;~xy;i= h~xy;i i2 + yjx~;i ~ 4:ni + hi(ni + hi )(ni + hi )2 , must take accountIf, discussed earlier, also estimating x;i x;i2effect new example estimates, must replace x;i x;iequations2~ (~x , x;i )2~2 = ni x;i + ni h:~x;i = ni x;i +~hi x~ ; ~x;ini + hini + ~hi(ni + ~hi )2135fiCohn, Ghahramani & Jordanuse Equation 9 guide active learning. evaluating expected new variancereference set given candidate x~, select x~ giving lowest expected modelvariance. Note high-dimensional spaces, may necessary evaluate excessivenumber candidate points get good coverage potential queryEspace. cases,ecient differentiate Equation 9 hillclimb @ ~y2^ =@ x~ find locallymaximal x~. See, example, (Cohn, 1994).4. Locally Weighted RegressionModel-based methods, neural networks mixture Gaussians, use databuild parameterized model. training, model used predictionsdata generally discarded. contrast, \memory-based" methods non-parametricapproaches explicitly retain training data, use time prediction needsmade. Locally weighted regression (LWR) memory-based method performsregression around point interest using training data \local" point.One recent study demonstrated LWR suitable real-time control constructingLWR-based system learned dicult juggling task (Schaal & Atkeson, 1994).xFigure 2: locally weighted regression, points weighted proximity currentx question using kernel. regression computed using weightedpoints.consider form locally weighted regression variant LOESSmodel (Cleveland, Devlin, & Grosse, 1988). LOESS model performs linear regressionpoints data set, weighted kernel centered x (see Figure 2). kernelshape design parameter many possible choices: original LOESSmodel uses \tricubic" kernel; experiments used Gaussianhi(x) h(x , xi ) = exp(,k(x , xi )2);k smoothing parameter. Section 4.1 describe several methodsautomatically setting k.136fiActive Learning Statistical Modelskernel wide includes nonlinear regionkernel rightkernel narrow excludes linear regionxFigure 3: estimator variance minimized kernel includes many trainingpoints accommodated model. linear LOESS modelshown. large kernel includes points degrade fit; small kernelneglects points increase confidence fit.Pbrevity, drop argument x hi (x), define n = hi .write estimated means covariances as:PPP2= hi xi ; 2 = hi (xi , x ) ; = hi(xi , x)(yi , )xxyxnnnP22= inhiyi ; y2 = hi (yni , ) ; y2jx = y2 , xy2 :xPuse data covariances express conditional expectations estimatedvariances:y2jx X 2 (x , x )2 X 2 (xi , x )2 !xy2y^ = + (x , ); =h +h(10)x2xy^n24.1 Setting Smoothing Parameter kx2x2number ways one set k, smoothing parameter. method usedCleveland et al. (1988) set k reference point predictedpredetermined amount support, is, k set n close target value.disadvantage requiring assumptions noise smoothnessfunction learned. Another technique, used Schaal Atkeson (1994), sets kminimize crossvalidated error training set. disadvantage techniqueassumes distribution training set representative P (x),may active learning situation. third method, also described SchaalAtkeson (1994), set k minimize estimate y2^ reference points.k decreases, regression becomes global. total weight n increase (whichdecreases y2^ ), conditional variance y2jx (which increases y2^).value k, two quantities balance produce minimum estimated variance (seeFigure 3). estimate computed arbitrary reference points domain,137fiCohn, Ghahramani & Jordanuser option using either different k reference point singleglobal k minimizes average y2^ reference points. Empirically, foundvariance-based method gave best performance.4.2 Active Learning Locally Weighted RegressionEmixture Gaussians, want select x~ minimize ~y2^ . this,must estimate mean variance P (~y jx~). locally weightedDregression,E22explicit: mean y^(~x) variance yjx~ . estimate ~y^ also explicit.Defining h~ weight assigned x~ kernel compute expectationsexactly closed form. LOESS model, learner's expected new varianceE~y2jx "X 2 ~2 (x , ~x)2 X 2 (xi , ~x )2 ~ 2 (~x , ~x )2 !#2h + h + ~ 2hi ~ 2 + h ~ 2: (11)~y^ =(n + ~h)2xxxPPPPNote that, since h2i (xi , x )2 = h2i x2i + 2x h2i , 2x h2i xi ,Pthe new expectationPEquation 11 may eciently computed caching values h2i x2i h2i xi .Eobviates need recompute entire sum new candidate point.component expectations Equation 11 computed follows:E222~2EEE~nh+(^(~x),)nyjx~~y2 = y~ +~y2jx = ~y2 , ~xy2 ;;2~n+h(n + h)x~~~x = nx +~hx~ ; h~xy = nxy~ + nh(~x , x)(^y~(~x2 ) , ) ;n+hn+h(n + h)2 ~ 2 2 x , x )22E2~2 = h~ i2 + n h yjx~ (~~x2 = nx~ + nh(~x ,~x2) ;~xy:xyn+h(n + h)(n + h~ )4mixture Gaussians, use expectation Equation 11 guideactive learning.5. Experimental Resultsexperimental testbed, used \Arm2D" problem described Cohn (1994).task learn kinematics toy 2-degree-of-freedom robot arm (see Figure 4).inputs joint angles (1 ; 2), outputs Cartesian coordinatestip (X1; X2). One implicit assumptions models describednoise Gaussian output dimensions. test robustness algorithmassumption, ran experiments using noise, using additive Gaussian noise outputs,using additive Gaussian noise inputs. results comparable;report results using additive Gaussian noise inputs. Gaussian input noisecorresponds case arm effectors joint angle sensors noisy, resultsnon-Gaussian errors learner's outputs. input distribution P (x) assumeduniform.compared performance variance-minimizing criterion comparinglearning curves learner using criterion one learning random138fiActive Learning Statistical Models(x1 ,x2 )21Figure 4: arm kinematics problem. learner attempts predict tip position givenset joint angles (1 ; 2).samples. learning curves plot mean squared error variance learnertraining set size increases. curves created starting initial sample,measuring learner's mean squared error estimated variance set \reference"points (independent training set), selecting adding new example trainingset, retraining learner augmented set, repeating.step, variance-minimizing learner chose set 64 unlabeled referencepoints drawn input distributionP (x). selected query x~ = (1 ; 2)E2estimated would minimize ~yjx reference set. experiments reported here,best x~ selected another set 64 \candidate" points drawn randomiteration.25.1 Experiments Mixtures Gaussiansmixtures Gaussians model, three design parameters mustconsidered | number Gaussians, initial placement, number iterations EM algorithm. set parameters optimizing learnerusing random examples, used settings learner using varianceminimization criterion. Parameters set follows: Models fewer Gaussiansobvious advantage requiring less storage space computation. Intuitively, smallmodel also advantage avoiding overfitting, thought occursystems extraneous parameters. Empirically, increased number Gaussians,generalization improved monotonically diminishing returns (for fixed training set sizenumber EM iterations). test error larger models generally matchedsmaller models small training sets (where overfitting would concern),continued decrease large training sets smaller networks \bottomed out."therefore preferred larger mixtures, report results mixtures 60Gaussians. selected initial placement Gaussians randomly, chosen uniformlysmallest hypercube containing current training examples. arbitrarily choseff2. described earlier, could also selected queries hillclimbing @ ~y2 x =@ x~ ; lowdimensional problem computationally ecient consider random candidate set.j139fiCohn, Ghahramani & Jordanidentity matrix initial covariance matrix. learner surprisingly sensitivenumber EM iterations. examined range 5 40 iterations EM algorithmper step. Small numbers iterations (5-10) appear insucent allow convergencelarge training sets, large numbers iterations (30-40) degraded performance smalltraining sets. ideal training regime would employ form regularization, wouldexamine degree change iterations detect convergence; experiments,however, settled fixed regime 20 iterations per step.1randomvariance1randomvariance0.30.30.10.1MSEVar0.030.030.010.010.0030.0030.00150 100 150 200 250 300 350 400 450 50050 100 150 200 250 300 350 400 450 500Figure 5: Variance MSE learning curves mixture 60 Gaussians trainedArm2D domain. Dotted lines denote standard error average 10 runs,started one initial random example.Figure 5 plots variance MSE learning curves mixture 60 Gaussianstrained Arm2D domain 1% input noise added. estimated model varianceusing variance-minimizing criterion significantly better learner selecting data random. mean squared error, however, exhibits even greater improvement,error consistently 1=3 randomly sampling learner.5.2 Experiments LOESS RegressionLOESS, design parameters size shape kernel. describedearlier, arbitrarily chose work Gaussian kernel; used variance-basedmethod automatically selecting kernel size.case LOESS, variance MSE learner using varianceminimizing criterion significantly lower learner selecting data randomly.worth noting Arm2D domain, form locally weighted regression alsosignificantly outperforms mixture Gaussians neural networks discussedCohn (1994).140fiActive Learning Statistical Models100.001randomvariancerandomvariance10.00040.1MSEVar0.00020.010.00010.0015e-050.000150 100 150 200 250 300 350 400 450 500training set size50 100 150 200 250 300 350 400 450 500training set sizeFigure 6: Variance MSE learning curves LOESS model trained Arm2D domain. Dotted lines denote standard error average 60 runs, startedsingle initial random example.5.3 Computation TimeOne obvious concern criterion described computational cost. situations obtaining new examples may take days cost thousands dollars,clearly wise expend computation ensure examples useful possible.situations, however, new data may relatively inexpensive, computationalcost finding optimal examples must considered.Table 1 summarizes computation times two learning algorithms discussedpaper.3 Note that, mixture Gaussians, training time depends linearlynumber examples, prediction time independent. Conversely, locallyweighted regression, \training time" per se, cost additional examplesaccrues predictions made using training set.training time incurred mixture Gaussians may make infeasibleselecting optimal action learning actions realtime control, certainly fast enoughused many applications. Optimized, parallel implementations also enhanceutility.4 Locally weighted regression certainly fast enough many control applications,may made faster still optimized, parallel implementations. worth noting3. times reported \per reference point" \per candidate per reference point"; overall time mustcomputed number candidates reference points examined. case LOESSmodel, example, 100 training points, 64 reference points 64 candidate points, timerequired select action would (58 + 0:16 100) 4096seconds, 0.3 seconds.4. worth mentioning approximately half training time mixture Gaussians spentcomputing correction factor Equation 8. Without correction, learner still computes P (yjx),modeling training set distribution rather reference distribution.found however, problems examined, performance \uncorrected" learnersdiffer appreciably \corrected" learners.141fiCohn, Ghahramani & JordanTrainingEvaluating Reference Evaluating CandidatesMixture 3:9 + 0:05m sec 15000 sec1300 secLOESS 92 + 9:7m sec58 + 0:16m secTable 1: Computation times Sparc 10 function training set size m. Mixturemodel 60 Gaussians trained 20 iterations. Reference times per referencepoint; candidate times per candidate point per reference point.that, since prediction speed learners depends training set size, optimaldata selection doubly important, creates parsimonious training set allowsfaster predictions future points.6. DiscussionMixtures Gaussians locally weighted regression two statistical models offerelegant representations ecient learning algorithms. paper shownalso offer opportunity perform active learning ecient statisticallycorrect manner. criteria derived computed cheaply and, problemstested, demonstrate good predictive power. industrial settings, gathering singledata point may take days cost thousands dollars, techniques describedpotential enormous savings.paper, considered function approximation problems. Problemsrequiring classification could handled analogously appropriate models.learning classification mixture model, one would select examples maximizediscriminability Gaussians; locally weighted regression, one would use logisticregression instead linear one considered (Weisberg, 1985).future work proceed several directions. important active biasminimization. noted Section 2, learner's error composed biasvariance. variance-minimizing strategy examined ignores bias component,lead significant errors learner's bias non-negligible. Workprogress examines effective ways measuring optimally eliminating bias (Cohn, 1995);future work examine jointly minimize bias variance producecriterion truly minimizes learner's expected error.Another direction future research derivation variance- (and bias-) minimizing techniques statistical learning models. particular interest classmodels known \belief networks" \Bayesian networks" (Pearl, 1988; Heckerman,Geiger, & Chickering, 1994). models advantage allowing inclusiondomain knowledge prior constraints still adhering statistically sound framework. Current research belief networks focuses algorithms ecient inferencelearning; would important step derive proper criteria learning activelymodels.142fiActive Learning Statistical ModelsAppendix A. NotationXxy^xiyix~y~y2^D~y2^ E~y2^P (x)Generalinput spaceoutput spacearbitrary point input spacetrue output value corresponding input xpredicted output value corresponding input x\input" part example\output" part examplenumber examples training setspecified input query(possibly yet known) output query x~estimated variance y^new variance y^, example (~x; y~) addedexpected value ~y2^(known) natural distribution xww^fw^ ()S2Neural Networkweight vector neural networkestimated \best" w given training setfunction computed neural network given w^average estimated noise data, used estimate y2Nginix;iy;i2x;i2y;ixy;iy2jx;iP (x; ji)P (xji)hi~hiMixture Gaussianstotal number GaussiansGaussian numbertotal point weighting attributed Gaussianestimated x mean Gaussianestimated mean Gaussianestimated x variance Gaussianestimated variance Gaussianestimated xy covariance Gaussianestimated variance Gaussian i, given xjoint distribution input-output pair given Gaussiandistribution x given Gaussianweight given point attributed Gaussianweight new point (~x; y~) attributed Gaussiankhinx~hLocally Weighted Regressionkernel smoothing parameterweight given example kernel centered xsum weights given points kernelmean inputs, weighted kernel centered xmean outputs, weighted kernel centered xweight new point (~x; y~) given kernel centered x143fiCohn, Ghahramani & JordanAcknowledgementsDavid Cohn's current address is: Harlequin, Inc., One Cambridge Center, Cambridge,02142 USA. Zoubin Ghahramani's current address is: Department Computer Science,University Toronto, Toronto, Ontario M5S 1A4 CANADA. work fundedNSF grant CDA-9309300, McDonnell-Pew Foundation, ATR Human Information Processing Laboratories Siemens Corporate Research. deeply indebted MichaelTitterington Jim Kay, whose careful attention continued kind help allowed usmake several corrections earlier version paper.ReferencesAngluin, D. (1988). Queries concept learning. Machine Learning, 2, 319{342.Baum, E., & Lang, K. (1991). Neural network algorithms learn polynomial timeexamples queries. IEEE Trans. Neural Networks, 2.Box, G., & Draper, N. (1987). Empirical model-building response surfaces. Wiley.Cheeseman, P., Self, M., Kelly, J., Taylor, W., Freeman, D., & Stutz, J. (1988). Bayesianclassification. AAAI 88, 7th National Conference Artificial Intelligence,pp. 607{611. AAAI Press.Cleveland, W., Devlin, S., & Grosse, E. (1988). Regression local fitting. JournalEconometrics, 37, 87{114.Cohn, D. (1994). Neural network exploration using optimal experiment design. Cowan,J., Tesauro, G., & Alspector, J. (Eds.), Advances Neural Information ProcessingSystems 6. Morgan Kaufmann. Expanded version available MIT AI Lab memo1491 anonymous ftp publications.ai.mit.edu.Cohn, D. (1995). Minimizing statistical bias queries. AI Lab memo AIM1552, Massachusetts Institute Technology. Available anonymous ftppublications.ai.mit.edu.Cohn, D., Atlas, L., & Ladner, R. (1990). Training connectionist networks queriesselective sampling. Touretzky, D. (Ed.), Advances Neural Information ProcessingSystems 2. Morgan Kaufmann.Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization active learning.Machine Learning, 5 (2), 201{221.Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete datavia EM algorithm. J. Royal Statistical Society Series B, 39, 1{38.Fedorov, V. (1972). Theory Optimal Experiments. Academic Press.Fe'ldbaum, A. A. (1965). Optimal control systems. Academic Press, New York, NY.144fiActive Learning Statistical ModelsGeman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks bias/variancedilemma. Neural Computation, 4, 1{58.Ghahramani, Z., & Jordan, M. (1994). Supervised learning incomplete data viaEM approach. Cowan, J., Tesauro, G., & Alspector, J. (Eds.), Advances NeuralInformation Processing Systems 6. Morgan Kaufmann.Heckerman, D., Geiger, D., & Chickering, D. (1994). Learning Bayesian networks:combination knowledge statistical data. Tech report MSR-TR-94-09, Microsoft.Linden, A., & Weber, F. (1993). Implementing inner drive competence ection.Roitblat, H. (Ed.), Proceedings 2nd International Conference SimulationAdaptive Behavior. MIT Press, Cambridge, MA.MacKay, D. J. (1992). Information-based objective functions active data selection.Neural Computation, 4 (4), 590{604.Nowlan, S. (1991). Soft competitive adaptation: Neural network learning algorithms basedfitting statistical mixtures. Tech report CS-91-126, Carnegie Mellon University.Paass, G., & Kindermann, J. (1995). Bayesian query construction neural network models.Tesauro, G., Touretzky, D., & Leen, T. (Eds.), Advances Neural InformationProcessing Systems 7. MIT Press.Pearl, J. (1988). Probablistic Reasoning Intelligent Systems. Morgan Kaufmann.Plutowski, M., & White, H. (1993). Selecting concise training sets clean data. IEEETransactions Neural Networks, 4, 305{318.Schaal, S., & Atkeson, C. (1994). Robot juggling: implementation memory-basedlearning. Control Systems, 14, 57{71.Schmidhuber, J., & Storck, J. (1993). Reinforcement driven information acquisition nondeterministic environments. Tech report, Fakultat fur Informatik, Technische Universitat Munchen.Specht, D. (1991). general regression neural network. IEEE Trans. Neural Networks,2 (6), 568{576.Thrun, S., & Moller, K. (1992). Active exploration dynamic environments. Moody,J., Hanson, S., & Lippmann, R. (Eds.), Advances Neural Information ProcessingSystems 4. Morgan Kaufmann.Titterington, D., Smith, A., & Makov, U. (1985). Statistical Analysis Finite MixtureDistributions. Wiley.Weisberg, S. (1985). Applied Linear Regression. Wiley.Whitehead, S. (1991). study cooperative mechanisms faster reinforcement learning.Technical report CS-365, University Rochester, Rochester, NY.145fiff fi"!$#&%'(()*,+#'.-/+0)+1234$56'0'87(92:;3!5$97()<>=6?A@BDCD=FEG=$HI?AJKL$?FBMKNOEG@D?PEGQRCSHI=6?TBU=6@BDVXWY=&Z$K?AJK0L$?TBMKNOEG@NP[\VX=$BU@]NP[\^_J?P`G@U=6@acbMdUeUfg>hjilkTmnkoiepbrqMsptuf2vwbMieUxyz|{,}~Iy| { {,|p }|IeUdriSUo{ X|y| { {,|p }|$jM,o2o6$A. 2 cXI$|2& 0&8u$"$|&w IX6\$2Ac, 26 U$ j> X" w|U8G >oIXw0 8u2,0/u2$ G,Xj$ jA "G 2U"U Gp"I0 8u",$6, 2 XG,uTopA ,$ $$ "pXo,u& G$YuAo,u&Io jo,u&G6w0/u 0$$ $PuIG |0T /u 0I" 6cX0uG0&p P,XT 0 0XpuT,8uu,,u$o c &$M0/u 0$X2j,TX w2I>$,0G ,uP TGT p06uI c$,jw GU j juj XA0 8u2, jG >A0n jAou Xu w,"8u, co AuG,$,,U 08uA co,I0 cp$ YAu$ j w,c6 , $0/uTG,u,$wMT$$cPA w,G, ,$0 T&, r$cAFP0G,0T ,wI Gu j " PIT0/u2 $uuI T,/u 06fffiff "!$#%&#(')*)+, .-/fi01!$23 4$'"457689*;:< "!$#%&#(')*)+, =' ?>@';BA, "C'#, EFA7';5#G 4H(@, @9,'4F';9IFA&#%4J:ff#G&)K C L4((4J;:NMO4( ( @' P &';QR7')?DM9 4CS?T= U, ( 9 V@, :WC##G@FA&#%VQ4FA&9 @,)M9 V@';9 9;:$FAMO4( ( X' @ & +;:YFA ;D';( URZ')M9 4CS\[&#*, 4(F' ]^4F7M7M74`_Ya'#GQC cb-ed2]Yb-/fCg-/d2(2]Yb-/f # -/d2(2]Yb-/f ) -/d2(2';4hMO4( ( R7')M9 4i' jb-/f&-ed2(2k5b-/f + -ed2(2k5b-/f 9 -ed2(2I';4h &';( *RZ')*M94S 'ml AC nFA+4(oqpsrtb-/d2k;-/b-/f g -/uv2(2?w b-/u2(2Bx4'y4(9,( & z ?{)*M9 4H';9 9|MO4( ( Q' E ` &'}( XRD')M9 4CS?~"hFA7';IFA 44(IC' a674C `'}4='*74(C#G,M( & a}:|FA?C U{ (C#%4C]9 C'#( a:ff#G&)FA4(IR7')M9 4CS l A4$, ( & J}:9,'45';9FA&#G 4Y 4|'N:W#()\;:19 C'#( , +:ff#G&)R7')M9 4CS[&#'J)+&#GR(C 4( +, 5#G& Qfi01!]_|h#%5:WC#^(@-/1'Z#' X C#G45/]1C;7>@9 ( Qh"';7]1C;2SC'#( , +:ff#G&)RZ')M9 4=)+C' 4)+7:/V{ H'?FA&#GVJ(67#G, )+#G, Q';&#%' _" FAFA RZ')*M94S l A_Y")';, N&M7C#('}( & 4, ?fiff1!U:/&#)+78C';( & ?;:'YFA&#GV'#GCvCG ,/' *ff7F B/vS1C C#('}9 '; h4F#GC FAC 4'"FA#GV5A7';1 4&"_YC'v]_A 9Y4FMO,';9 '}( &_YC';C 4'.FA&#GVJFA7';" 4I(4(F#G l A4(M7C#('; 4& 9 VQ)';4(C 4(?_" FA, `'.&CvCG /GFS l A4I 4='*#G9,';( & J4(F'}(, _AC H4&)+9,'4(?4)+&#%C C#';91FA7' J4&)+FAC#9{'4(S50ff$G5,CCffG55/G$55,/0tG5,B(CffG5F/G$5B5,B//0/5'((0)pfi|fi75 5 T0 U 2ff4$ 3!fffi!25;fiY,77<($7vXE7^F7GX.(,7GFCC(; G@&G7C%h(j,ff1\G*F7F7( &/; (HC}F7F77 C}1, C}( &aQ, C;( &aG,;( .7}B&G&7Q;" 7H0F?F7F7( &n&G7CvYhFHF7;I,(+ h+&G?CC(;F7U+&7 t; C("+&GIF7F7N{*C;(IF7F7.Uff*FI{* C;H&G7C \ .&G"CC;F7H C; U, +Q {7} +&%CC(;157Q G,;( ?(X7;G&7t^7 q L +(" ,(F7fiff C; Q, iUFF7GG7CGC3F7F7( & hFJ+(F;F X0j7G( C,F7F7X7 ICGC; C;, C;( & HJ7 =C"(E,(";(F FE@G,7;;"F@; F 0aF7(1%{}( +{* C;@ i7G7CF7y, C}( & OFQGi77 7=7%& 7G7G &Y, C;( &Q(JF;& H7CG t}( & %&h,(X;7COCGC;*7G& 7G7G & !#"$%'&!", C;( &F& UBH; 17CG t}( & G&fiff( F,Q.CC(; GH&G7C75CGhGGY77G&;"(*CC(; # )C;( &U&Y57,; # )C;( &"+<%(h77G&}BUCC(}# )+&IFO,; * )+%',.-%'&!%'-/0",C1 (@7Q?7 (C(h5?,X75; ,UF =77C`.CG Q+CH &^& (C(! 2F;|^77G;ByC7JF(}m7;a(43Y7 7 2 56^&C 87:9&!5`ei";, C+C( &N< ;Z>=77,G@ ?@C,yFhF757( &HG7CY,HF &( AB,CD1!,C$E9:F@"$9HGBI ";YCJ,( B , FCI,(X m57F7+XUJ v&I,(F3F;+LKN Ov(QPRM OIjSKTM N OQ PUM Ov+"CU 7&+IF7F7+XU17hSVKqU"F7F7a hYC;C^F7U, C;( &1 W 7GFCIa 5YC( F ;F7; F( |QY7IF7F7+ 7 t; CCCF&+FXG" C; HZ ;; CC^H(&c77G&;CC(; # )H&+57,; # )8G<$XG+ ,Cc^J X5Q77G&;YQ" 7a&C(" F{LF Q7OCY^CGaFQ&CJ Z#:"[G<$\0!,C!"*%^]<"$%'9,_J,7GFC"(" 5B#:"[G<$7CC(; # )C;( &I; ;"YY(hCC; * )C( &( 7,;C CC; * )C}( &C;(ICC(; # )C;( &? ((I ,(YCG<%(7 (C(jQ F,567 5@7` 5aC7B"7G;UF7;J< (+(c b ,(h7}=* C;(CC(}# )C;77C57F7( &/` de=7<^? iJ,(J BnF7F7++; {(h,fbPa BnF7F7.mj; $FC,(+F7}i; (UF7F7+H; Y{(+{fb"HX< ;7 +C7*CC(; # )mQ5,UF,` dJ=h g &7G|EIF;+F` de=a "'7<;7 C"IEY& L& aFGE&( ((, & E&U{(Pff(C;1"YQ+ &7 & 7HFJ7( ( *< ;Z*+,(QF7(C@F;J(C7(;(` de=@ C;aF7(CJ"7;"Y&F;,Q?FGXF}{,H+&G57J&,(v&5Y(77%&;&57F7( &* ;&;,J XF}( }(&Gi < ;7 b&( ( F,(kjfiKlM N m&I PnM m"` KoM N p(I PnM p7FCJF+` dJ=Jba fiN q(I PnM Ov"Y,(r N Ov(I PnM Ov; +(C+Y+&GY77G&7G,;}$IC}(CC(}# )C;@ b"C7h7 &7@@ F, 2 i77%&;17C(J 7&+F7F7.j E WI|5Y< ;7 h; (+F;"CFIF7F7( &JG7C I7G( C,G H7F;( ;(&%iICY&( 7Ce :7/sHGt%X&Y,(! $,(I HCU7hG(Q" FyFC+( CuwvCxy{z}|~zii|~z~:X:zX!{X[fi[Xk@:X<h0.<<}}[y!yJhh@zi|z}[rH{y{zQh<zX1v!vC0z:`:z~[z}|~h:X:{fiH|~zEHz}cHhzQzE<[X::z}{z}|~h:XH|`h[[z@:X<h<<}}{:zH8[ @:z:<}!z:I':!1::E<:}``h!z1:<}Xy[[y+{:k@:X<h<Ht}![zzz}@XyI :z~[z}|~!`|~zw{!v!vCxy{z}|~z`H|~z}XHczXzz~z:`:z}{z}|~h:Xe[z}|X[X{I{XHJH{1*[HiX:!#<E {:Hz~s0:tu:}v<{fiCCHf@@@X<[@'@C@@Z@[@0H8ss@'w[@'@C<sHs4Bss< [J@s@s#0c# #@<s#8H[+s4~0s@s@s#0 0@B1s0c@[B}ss*#[H*ss@<EQ<<0@s#.**Q@ssf0scrs<seE#ha## <s#104s<s#s<k[Jh1s#@J0Qs##[#0[1B<#s~<s s<H#0frs<ss# <[Q<s<>@ *s* [H*s@s@es#*#0 e@[Jss*f@<Z@H*8sH<` .0cs:s< @<H ~Q1s#@:s@*<E#[HEs[# [#0sfi@s@r[fiJ@@[s[#08s#<0s>E<H*s[<s<E <sQ#B< ~@[<k ##B@s@1<<srQs*<ks##<r#iY<s T#r< ~@[#0[*#[#*k<@s*[[#Qs T#fi< ~[@<`ir 8s#<01s E[[H `B<s< rs c@J< ~[@#0s rs .s4+`J [ rs Q#J[#` fis#c@H*H*<*8sH0@s<H*s <0s<ss se<0#Hs<` Hh:s[ <sH<firs#<@!r< ~:[@[H*s [B<< 8s#<0sE[Q[#Js[@[BsJ<s*H<s<Qrs#t+@H< ~[:@[#0 s<J:sJ<0s<s < ~[@H[#0@s#c0@B@s<#0)* ,+-9?:?@ A:CBfffi: @ I: B KJfifffi"$!#&%'(fi.0/1 2$.3#%&465#%&/87"ff9"ff;: <:ff>)1,+-D9=@ E9FBG9H@ E9FB"ffML,)ffLff=9N1[fis1s s[##0@w`<[#1s#*#0i*e<4101@<s*@+ssk[FO s[64P s*#[H*sEQB<[#s##[#0*#[sJH@[<00@s ss{*<@H[<<0@rs#t 8a>s<fk0*s:~s@0a#<sZ<[#0se[s@0[@s##[#0F. 0s#[H0#[cs* [H*ss@s@s#*#0 <#@[<s0@sss{*<@[#@<HJ[H#0&R HS-5/$#%&/3.3%K.TU/$!# WV0#%&'(XTff@0Ssk#[1s* [H*sfis#[#4#1C@[c.J[<ca<<[# #0s>@!a<Z[<<0@< 1s<4#<[#04 ~`o@ *1s[# [#0s`@sc:sJ<0s<sc[0<a<<[# #0 s<s<#<J@s:~s X<es@<@ss [fis<<#0 <[Zff}Zs#@a<B+#[H*+[:1[Js4<0#Hs< ss ~<0#s<4[#[csH[# #0ss40[<:@<<[# [#0sE@s##< H[<+[fis<1@<Js*#0@fic@~0 @#s0s#ss4@<B< s<e@<e0@a1[#k@*Hs0s#8@<B<s0@[<c[fis[i<s<Js81s<H#<<4s0@[<Q[ 0+<s< sss041<ts ~`f<:#< [HH*sc 0e<s<![s[s<s<fi[#Q0<1< sH<*@! @@sCc [<@ LBsss0s@<: ~Qs#@w 40<[<EZ<ts1rs @ ss s[<s< [<@*#[##[s<0[H*#*H[#1@@c8:s@s@4[rk<s @[c#is<8s<s<e[s10EQs*f[s sf[<# ##[ s<H[##<<s< #[s`<1fis <s<+<c*#[#*f<@s*[[#1 s[fi<sH<Csfis:s<@sc@0H <si#<s#[[#QJ:sJs0<sH< Csis<B [0sIB<0sH*@BsEss*s0@[<E[ 01<sH<@sr[#k@ [H#0kHs0@[<[ s[<s<\> ]^ _ ]` ba> E)gih j @k1lllmk j_ngc]rF&sx{?y v &sx{?zF&sx _YL> c` ad QFeGLfJpjrFtsu hwv &sx3k8y_zF&sx`aEJ?JfiffffL<Lqb5'mS-5#JsE<01s@#0c[@@<Is[##rs@seB@}|` as#Bs0@[<`<s<#<re[C# @}| B0@< s0@[<pC 0I[<k[s<H@Br 1rs<s*s* [H*ss= `h8Qs80[<Q@<<[# [#0sI eh8Q [#sc<s*H?~8:@<<s#[[@+0s[s< sJs8[# <0@s1s1sBc[fi0@Q[rC0Qs1[J[<#EB##@:s><:s*Hs@c@< #0[esss<I} 8:s##{ss#1s#0s <J@H*H*sB<@s"cC1srs<0[H*JsB<3fi>``6m`ux q`1K6H3636,b363` }-`;;``6$`F6 -KTF6 , -q?-Gt`63 -&33 F6 , -q?;<6H__6 63\,M;Y6b`p,Yfi3-$`6 -6?$686 YK8`3$& ;_G6fi&-\,>K`,>8`,_}MK?6 ?m3,<36m3K3-8,66,>-6?6-86m -6`63 -&t3?36*t`-i86 F6-86m -6C`63 -&3 36mMF1A3686 8I6 f6`*,<86_3631`636`6f363>Y6 3H3-8,\`63 -68`8686C3- C?3-F`6$6;fi,<K_K6 K386 >>-86K66D`6f`,3H-\$6D``68`6 -fK63-636m6&b3`uC,K, 8&_`u `-66K66`\E}<36Y6 3A ,K?6? 3m8 3,m -6mF3C,-6`6A6$633?,3636I6I$6*86Y`6A6AH\6`}`, -,_-\\`-K366366IF6 -Y6 3II`fi&-`6IX&&`3$fiCU6`u3DHF\3`,m3- ,86 Y386 K K86C36m63*6f36F`686 fM;fib,C6 Km3_,`t`63m6&t3363_Ct<,_>>-6,8fi-`6,u;fi3$66f86 ?` 63- f63386 16 3D1`b`138`,C _F6 Km?86,8`<86Y_}$6G66`,K38 33*u63 m3,;3-6$_mU\`>63?, `6Hm`38,m66G6m_3f3&-H86 f833,,mE,-`H6m,386 C A`f8`I6`-`,I3``6 863;K6 , u>>&-`6f?`33K`6,;,;86,mU`686 >6G f*m68`86?`6,<,;86?\-C6386 K$`,>C YH86 K`36 31K `\8`,86`6,< >F6 *F1F`?u6?m*m-\K3-6 -f-6>86?3, -f`3>3f`6f\6`36H ,bF6 -C63>`$63fH,-\6`3 `6;6K`6f\6`3$3EF `3`f86 \,?86F6X,``6m33 - `6\-6F\ -86`$`,?86`33 u6 DF6 , -&-t`63m6&t3363fF3`3 F6 3I86[3- 63,X;m_G6\,3`33 u6 q 6Ff$ 3 F366`m6-86H3- 63[,Y[6 ,m6bu-3U6C Hf`33 u6 XY6386H-6t`63m6&t336F6 36$6C`63 -&t3f36= 3, Hf`63 -&3=`0--`6q663`f_3\6 ,m663 FH`63 -&3=`0--`6E66,K 3`F`A6, 16?3- 1<,?>K <$6 633 -I`;386 K`63G6 C86K6m,u`63 -,;86C3- 63C,<;fi``6>`63`,6`I[`mmA6>D-\f3,\,Y3\86I`3m63f,K-6I6-86m -6`63 -&t3*36m*6m,,`f-`33Y$636m63F6I3-`686 [,G;fi;,;K6FY`,_8636m3G3-8,6 36,86K`6\663Y,xt`63m63F363fi ;$6fi,FF1AF,>-,`3 ,_ -6}63 &b<` 6 EF6 \8, -6\,K M-m\86F6-`,F *3`63mI`\`63 -&3-t`63m6fiff ff!" #$ %&' #&$ $( &)+* , - ff.' /$0,fi12#3 45/$+$6 /$2#*7098:,28; #$6<=8;,fi# 0ff ##>5$6&"6 -6&1@?&$6,fi) 0,AA$6%ff%ff'/$A0BCED6) 0,GFIH"JKL=MONP NQR9STKLUP NQ&NVMWR )XFYEJ KL=MONVP NQR9STKL=Q&NVMONPZR[L $6\#]Z 71?ff^ /) ?$#)_8;,fi71a`=) .$ /1<ff12b/ $ R cF H 2 d,fi#ff*/0$A8eFYA &)fFYA )fFY22 d,fi#ff*/0$A8gF H )9F H h(0 #"F H )fFY,fi(ff/B . ff%ffi!\#b/* ff0$#5j 512# &$6 /$'FAHk( lnm'`k8n$6.$(o7FIH N F Yp h(.*/0,>$6&'lqm'rd8n$6#$U2 0ff &# KL=MONVP NfiQRkSsKLUtuNv Nfiw(R > + 0ff# ,ff!+ +*70,<UB/00,fi ff%x. /$6iCyZzzfi{|W}~A|Wn|Wn}n6Z}&nUn~G}nnAn|W}&n|C~XWW|OO=}nU}&nUn~n[nW;EW#E[&WZfi#CZ=WOZEWOnOf=7OXn&ZWCCnOOO&A;ZnWXWZZnO;qOZ7OZ&dAWkUu2COWZ=#uVdWuWZ_XZ;.OO&"0(fiC[7OZ&5(U+O=O=WWc#CufiWCOZ WWWZ\56 nk9Z #W7uWCfuO55VO#=d#OnZ;ZW@a@[nW2uWW+&;OfCO;finOZ;Cufi#Z9Z:uOZC#^7nWCOn&"&A7O9fieO[27nWZWZW+n#WuW';A#Z7iZZ+finOZ;Cufi#ZZ=WOZ 'nOZ;Cufi#Z+Z=WOZW#i7ufZ;uACA[uAWnO;i;CO+ZCOZO=O9&7Wn&ZnZZCO9ZCu7#=nO;C^7O&A@7n@+ZAXOZZOWn Wd+;&2WOi;&iOf#Z=&iuC[+n&uC#OnOOOIiZnu+OWAZu9"OEWOsW#W7^finOZ;Cufi#ZWO:niWk2+ZCu7#=nOcZCO;Z[n;O;C_&'ZC;OZWOOCufiZC;OZeC&Z7ZZ:i;&iOO=_&&@7O\;C#n#Z_:uOCn&WZA0nW#;O=WZk27O&s7n"Wu9OiZ9&+Z:uOZEn&XC&Z/nZZ=&;;&;CnOn=O;;&;C C[O:O=O7O;9A;77OZC#ui=Wf&'CnAZ7O;"Cgn;2W;i&AI7n&"\finOZ;Cufi#ZZ=WO/&=WOCn&u;";W675ufi0sfiWfinfi7O;+ZZ;C+Ii;'n/O[Cc+&e/OZCOZOd2OZZ2kC27O\n/OiiOO&2i[OnZ=7uOW;WW='7nO;7O;C 5ZW='2f#/\E/WOW#2Cu7OZ nWOaZWA;&CI6fi ff;U #W;W&C5XkfiOWfi ff;U #W;[7OOZC&;C &"WXC; ZA; OiZA&'i;&;dOiZ_;"7&ZE&A/OnO;W#7i;_qnWuZEn; nOZ;C&@7Ou[i;&;W 26# ; d;^Z=WOIi/EOOcW;;WfuOZC"cC#OZC&iu;;iWOW 26+ W;[9Z:uOfIi/OCOnWiiu;i&;7; GiZ;7O9n/O;n#WOWaZ=WOO+9n/OifWO&6!;9Z=WOW(2O"$#nOW7Onu;;W[i;&;=%fWO"&fnOW/OOZC&iu;;i+=9 O[O#Z=WOW"O;ZE#n#ZWd[ZCu7&n;Z;Cn;dnOWZW'"(*)+-,/.102.43/,65 Z87 nW&=nnWZ&A7O\#fiC#nd;WW;W7O; u'iuWOZO:9;=< 7 i+/OZ"&i'Z=WOZ"O;ZEu[ZCO7OZZEfi#C 7O[n[nW;+=7f O*: /;WWC C> ;?< 7 id7O\ZA&@&;AI@ C9Z=WOZ""O;Z_Wn[ZCO/OZZ9fi#C 7O\nW;=B97'fi7O;EnWZA2*nZ7nnnWC[XWO;7u#&=nnWnZB79"WOZCOin9/OZ=WO/&=WOCn&uC9WOD@IC_=WOCnW$> n&Z C_7O;:7f[X[A;;O&n/Of7nZ=O#=OW;7CO[WO&;;#C#n#AO9Z:uOZ2E7nO7nO;C:^Oi;&iOXWOE#Z=&;W\=O;i;C(*)+-,/.102.43/,GFZH WOI Z=WOZ+WOEs[ZI&eZ=WOZ'X\7 7n&! = ; =[=I_nOWZ&CKJLI_A7O#ZO;[97nO;7O;CNM97OZ7n&:$MOPIERQS WODI W#= ; T[=.U WV2O fiff W;kAJXaWOYIZJ[9fi\ ] OU/W6 <_^ `H ;6= afnnOWZ&Acb defO@ZW#[CnZg& i+&;f^[CnZg&A9f0\ g C67uU <h^ fi! i6= !_nnOWZE&aZb di_Caj2"klb d6EuOYI WYfi\ ] OU/W6 <_^ nVC fiff W; nob diaWOIpb dG9`H ;6= -IK# iu`ff + GWnOWZ&mZb d:qBEuesrtj2"kCb di_f WO*I u#WV2O `ff W;# ;Wfiff [[uob d:qYaWOYIKb d:q*f'v?wyx{zR|h}2~u]Wi~fi}Hn:~uh2zRW~fizRWhfifiu~fizR_BzB1~fi}a}Wmsna!h2~fi_W$h_~~fi}m=hH1_fifiu~fizR_/RW~fi=zsTvh=$Rm2{}zRux/WhR2a=v_=$mRwCfaRm}WfizRzRh-~ax/WhR2mR=_u{f1zR~/Wf~fi}fz2~fizR~fizR_"~aWzRW/Wa}_G1zR|n|Wh`~fih|W!~fi}hl=z}_Rw=fia1h_uycs[fi p="CnW$%fa"]_* %1$]1]aas_*nBnhgZ:%n2$ Ym!a=2h!=au]/mna="]=atn!1Wy=hgna=/1$]1]/ %n2$ Yt18na=2_!==su]/mahg2i"/%m"= i" "=W n2* : %y!Y===S_==y%C]g21"]]2_]G=a=2[=1h=2_1B=_1_]%Bnn"hg{fffiffc-as=1_=$"B1]=$1="t{{n2%n=$n] _B!" #$"* -(/. +*,*=* -(/0 21 43 & +*,*=* -3-5 +**,2* 6387 "as=1_=% =:'& )( & +*,*=]= 8_ny :9==N2:-n$n_hgW_]<; gCBt_22_m=y2 $_= > (/.@?n/3A52 n2 n$=_ _ '&/B (/.ffCD 21DB 3A5?_ ;$]:2]g=NFE G"H ! IJLK ! M{u'&BN1m!8]gh2_] (O. 3A5 :ng%_"8nC]]_2_]P ! IJLK !RQ #SIyfim'&!BN1:a_nyL9== 2-2*nU81!W1 VN1n=*na'&!Xa_1"]]=]A*1Y=h]2u'& 1 V g:]_B2]]=1 V C_nyL 9== 2D1Y=h]2HAY& B21gh_=fffiff[Z\%{=$KY =_]; ^(=1_8e;$]!2]]= 2fgIashH_!n-si2""=1hP]!$*$hg8]gh2_f& ?**,*_?"(O7F`abdc Y!2n=hf]__=__1"n2"lW__]n_]kjlfffifflnmo ! IJLK ! Mfp =1h=l &B=hmfN21 2 n:]]_2_g=_]=Y=_Y& %N1 :n2#S ! MpgJL Q ! ums1 g"%D=_g28*=_! &sW]]_2_]:=y=Y1*n:=n==hgfffiffqr%{=!S::_=/=_="P8=1_/m ! K I$/6 Lg!sgh_t2=$A=1h=Wuv& ?,***w? u'x sWT n!2=6u . ]:=]n2!1:!"=_g2=1_=1 u & ?*,**_? u .zy & mfiun=B$2]_]%=]_h2a:]_C|{~}a*fffiff%{=i$$_=am=_=Bo$=1_O Cn"W2:=]_h !Q g I"K: ]__2C{ff:*/!Z]"*n_]{ W2"=gh_l =_s n=n|{ }!Y= =n]2- =X|h!]2i!2X[6 @c, f22h]:9==6H2__ff!= 2,Mr ! oE ! - ! 2I@fgJL Q ! E !@I Q ,g JL Q !>8 flP!^ '2 ! ! fpIfN@I Q SMg ! NIfegJL Q ! B2/ Q g6fpks! ! !!]g!1$n"_]"H=2 ]=u==]_!=_=h2L 92_]81"]]2_]*=a=2=1_=*1N_2_$$C=_1_][[nn"_]{S"!$=X|%{= @c,gU@ c 2=Yh!]2@ iH2YBWR @ c>fiMMw+SSS^SzSMYSS|S^Sw< Sz^SzSMP~,Mff<ffff|^~~,M+-,M_2v,,LG2d_2PL^Wn r|ffDWLRRP^p@wS>6Y__L-@S:Lff fiLfiLNfi RPfi ^_@L @/"'Sk fiL"@G+|MO|ffGGW/.0+1.w'M"! L#s,:%$ 4 &(' R*)+,-)2 ,~4365 fi 7 ) +| fffi8 & ffPDNS, ) & 5 fi 7 ) SDS'@_LfiL"fiL__8ff4fiL%LL> % )"'SRfiL@)"S *)+, :9; " <) * _D"WLfi Lfi W=fiL__:>fiffM7 <)+,) 5 fi ?fi: : . +, .@'fi kS@A_B 7eC "7fiLLRkD7w" FE G _HffMS<^:L:^HKJ ff ffMLON PWS__QRPSfiLR>Ffi LT_"ff~<" FE %P>fiLQRP &U RPVWHXXXYWP[Z7ff\^]nKff"NSGP_fiLW'` "'S"k">PbafffiL" Fc_DS"de? K_ :fi 2"'S""v__ Lfi " Fc _S fi@EfMD$RLffS"Ww_gP &U ih4Wj4DS^S^k &l U ij4Y:m l U ij4WhpY S^S^qfffi 7U ij Wh7~S^ SDqGQ _LL> r @'_w:ds LRk'"S^rfi8 & ff^"PLfi '"R^LL2"S BtS^_"PS"Cff D@fiL_ ) S2SP)S'Pf fi _"7 Gff "un iwpY_m l xn ih7WyffN7fi=-S^..vl ijW U ijWFS"zq/6*w "Cfi 7 fi 7c P__ U ijWFn4{ iw7WS"|7% fi RLKJ ff ffM}ON ~,~_DS7fiL_6w_R wff;jVWXHXXWjZ<^LfiL_fi 7>Ffi LKSF fi 7c fi v~Asff h V WXHXXWh Z fi:wfi 7_"_FFSFfi 7cMfi v~ z~jVeh"VWXHXXWjZhKZ4DLfi ^L:z LH Sz >~ ~ DQA~ fiLPR@ffffDS _","S^Lfi N 5 ^u :_Y:fiLfi ~A@G+< ,/ _Pp@RL'ffffWv,n nr_R,L'DMwRL$P2 ,~43'7fiLp5 SS_ fiLNv"K _LcL jdVh"VWHXXHXWj"ZphKZp_QR r _fffiL%LL^@"kfiLP" "S^"@%fiL7v"S^ | +| _7 /"@Lfi e7~ff7 -"S^^ _ _P"S'fi 7 rPD"*_"_h"VWHXXHXWhKZS@% fiff '"w w"_ S@fi_^LHffS'"fi @ fi 7cDfi^Lff7SR_@7 'Ch ^j ff7 DF,M\rffM7 +[ p@@n@|ffD 67fi%`7Y4O -6H_DS4dH0 d[[ [:g0K4 4H [d[b Cb[7/=pd_%=A7^7p77D%76p77`7C*7===p77pFC%`===p7HK;`otd<?7^v"<D7`F^eC=ppR*_" ip:Fz_ifi ffKiR*_8_ip: F^477*D7==4*! "DK 8_p: e#$%&z_4/ D7==pffi) (* +*,- "D_8_ip: F^477D7==/.04'R D"6F14273^77=37 _;H =-54!67ff )(Kfi8Kp R =-"F 14273%7YY9K^7,K;:[<=*F=^76_%H =-><ff iff@?A8*7 _1R<F 14 273 47=3 7z&*F=>p4= 4!67R!ffB?=C"F!?3ffB iR*p^EGFH; :[pe> 7`7 H Dz FH 9=7:F`=pJIgp_Si==!K%*HFD4!67R!ffB?=C 4`7 H KMLR7NOI07P%8;K477RQ`7D7S FH 9=7F`=pd FH ; :[pF%Mpz`7 H F` pF%TSp/7Y FH 9=7*C%`_7 ==! K_8VUW<X7S7> F<H 3 =7D`SY5LRi `U;7XZ.\[]N^7OI_`]P7UW<XTSUWaX; bc 4dFY3 T7d7==pC=Y7===p7S7-7F:`===p7SipA` H YpeF^e? F<H YW :[7F7=%>7(7YK`>`; bFKHpHK;`o4&d**C` H Hgf&"< F<H YW:[7F%pz_hji, "<7=` H K7kG^_gklfnmgip:FOmoh77/%H&kv=p74HAffB?=qffB iRC$h H 7f>s r H ^ kg===0t? ff uppH4A ffB?=) qff@ ix5vxwzy|{sh H 7fMz_ _%,k fJkvipF7C=Y7k zsh H 7f>} H C=7 kv=&~>t "- ffB?=) qffB iR>C$h H 7f>78mMfkp FOmohr H -k=;C=ppF ffB | *>% "H ff@?) qffB ixKvyz@{h H 7fM`_8%>, VklfJk ip: FD"`=C=pkSu^ h H 7f>iAAD*>%Ye*Sh67r H 7fM`^_"77= pzr/=H 7 F<H H (D7 F<H 3 =7-`7 Hf`= 8k K7m F"z& Apru G7 *>VhA7%,kI0mzA7p77Y;7 >FA7Y7r7/7! DYC4p77ALR=% P HH 77LRpe% P=%% H 7H %%>"B *_`= :Fp"F=S F<H C; :pFYC=peF^AG` H ! KK;`o<d<1-;` H Y<7nfg- F<H YW:[7F>p_R%ipSF(7YH 7sh0_7F%2p="DA=_7C=Y7-7DSper`===p/h H 7f&`z_777FF^KfMS== O?3ffB x i-"7 H =O" 7, H H =M`D7!/=C=7 p/ e`7F`76C F<H YW:[7FA%r`7`6[g% H H =6,%=`797= H` H ^C7D6=F<H 3=7 `Y` H YS=p@7=/7 H 7`YB C7_ FH ; :[pF:7F E 78 F<H C;:[pFTf6=SC=7D_7D7SF`pF7 H Kf=*`=p7YG FH 3 =7D`Y`^]fiB5BB!-JBeBB77!~ *|B7!ABAsAz&7%B@3e7xe$%7zB7z|3%7!|zBTB7B7e@ se=%B&e@7eB/7 3@A3Bs%7!OBA&A&B!A7N7@9xe%7T;*eA!T|z!B/7V3%7!T-%7%=73e =793!97eT A3B&733!3eRxee V@eAe7O773BV~&%7R*eA!7z33ue zBA7;%7! W3!VB!7!39!a3eNj7!T%7%&73eu@7We!33!3eN57!z%7%&797u&3=g7B73=)BA%9@9B7 3BA3BC/%7 *7A!z33e %!@B-3!B!7!a93!977!&739!a3eB7/Z~%7O *7A!z337l-3!@!7!33!3e7!A!3Bx-!=793!97B77 fiz;!A!@@7eB B7N%A&;!A!@;*eA!A@9@O/B%agz3Ne73e7!7!7+797ugz/!BNV7%7!B$MeNB!ANZ73/&79!V~%7+77V3euuz39se3%7 B7z!Ae%7%&73eB3B!N@B%7!Ae@eA3B!z7!zBB977V=9!aV@!7!33!3e7B7eA!33!,3e77!&733!3eu|A%3B3B33Bu3!79!aB!7!a93!977B7e!%a93!9777!/A3BN&733!3eu7A%73/%73/%ABn,BNA!n!%&=BA3|B73Ot%7BAe797u/ssR0B*$ATzBBea!@3e//%7%&73euu!TB!7!33!3e7 7!T%7%&797,B!/3!7e!73B3#@O3BBA!@ B|7=A%73zY3B%@u s*B73V7TB337*-,!/.10!3243B35467276e98;:=<7>5@?AB!,!CBEDF5-xff fi !"$#&%(') +H5G +se< ,A+K]J L33 45M,&N&O!AP?05=F8-0e735e@%7&eB7,%7!c9 !B57]TN%%7#$5V_9=9;3e/B7B77%7<73e/xez%773!7 sBB!A/Z73VJaB73@7z!AB99@9t7B%/B7%agB7eaBVB7/xee/B7eaB!Y3B%@B7397%773!7&aBNuu77!#,B3e7eAN%7&tA!@ !@!B73!B7&39=B!A%a97BAxeAB#A79,xT3*A3!&7]_%z%7+eTZ79V@73V&73%7 73ea9$@7VN,x!R%7BA%B7BA3NBBA!R Q1S77 %7+e7!AB*-,!/.10,9423Bu3562e67X8Y:=<eff5@?ZABT!,!CBEDF5-xff fiUTV !"W%M'=') +H5G +se< ,A+K]J L33 (N&O O-A[?V\5=E8@&^]9_a`Zbffc>cffcbd_efGJs+S3%7e7T3BT@!7!33fiBCz%%7B70B+%B7BA3@BA! ug_ h_ `+i c>c>c _ e $%7!j_kl_ 7B!amon@mlp|q]_9Wrhst%7t%W_ kr/xez@!A;mtn&mVp|7!txezB!;mtn&mVp|C%7!A3vu k %7w_ k u k GtrMB7!u k e73aV7]@AB73j_ k Vz3;u lu9` cffc>c uKe%7!(_vux_y`Pu ` c>cffc _euezGHrO!7g_ Hr/ev_M3&/aY5ts{\ ff "|ff} ~C)+XC=>XC9dffX@)FffvffX LK}79X))X[CMCE9Eff)[)9;9+>1;ffY &E97WEff)[}=[d4>YX))>d)1>ff(d&)X&)[=d)C&[79X&C)[)[[yC)d&;9E[)L+X>Y)))&d)C@}X++>C)d&XWX&d)CX>ffX[dvC9ff[Z} 9X))X[CX++>1>@W>=[d) E9)[}Xfi&/ovHff@dd@d9$C9794@[>[M4-tW9E[971C;>9a9+>[;$97z$ [Pw7P&fi7g>[9z1[;7@ff !#"$%&!'(')*+!ff;1(9,-.0/1"2%&/35467!-"8)*+!:9Yd96>4! ;7C94! <;=>?/@BACd ff d1>9W9D, =4 @*,FE 4 C G(9D,<4 yE$>fi g(zd@)H fi *[9>Pfi&;9d ff #7C9$[I 9 ff ff >)J LKd1dMhNK)7>[MPOCQQKSTR M^+>[MU7R K;L 9@d9# KxWdd ff W VX >4fiCad>&d4>XY[Z!\^]_Z`$acb'd1efhgZkjmlnZ3\ko#pQqrqcfj *sutwvJxy{zv*|<}~|!|!v#&xh=KO3r!vJ~v_JvJWLLhxv6TTC:xhv_v5vHxh|L=~ 7h<]_\[\o c >H,(_JJ(WA2(tC7j4M91>>9[fi&H9d ff -d( [jd4 [PJ- , (J_J'(Wj[jdjW[7 ff (-< , (_JJ( 4[Wd;W79< L :. ) :/7M74d9@ 79d >&&>9g;d?fi C9:, E JJ E gF 5MF*@ vC>H , (_JJ(w4$d><^K W1Gz9H*(:@ d>fi #; ff 7a9L>H , (J_J'(W- , ; E J_ E ;) 4d9;9d ;u+)7#7*r ad1T^[ad 4[ 4[Q< g9 7a>9& 1>9w 1W[ ff & [d> ff 7[&fi Wff6 c 7>[Q#&cr ;<c(ad?ofi ga(9: <Q C MO= W ff d(d9#& NM)7@>[u*.#=7@4>[6r1CZjC967M 4& 77 X[W< -G.n, E JJ EL 74>[3NL[[ GaI >Y9->? , (J_J'(W A&;-; ff 7 9FCWJ zd>[;-CZ(d9#S;[+9 Hfi5 , ;m E J_ E ;[ , E _J E # , ,mE J_ E v> TMwGz9 *ML# 7P@ &C97 9CW>9#$fi >[*07P W1Ga9, <4+$7a7 # [>[!4jtW9v>vC&1a[W 1>v4[_>yvJ|xhJvz!1^x~ E7>j9dd ff 7fi$ZjC 9$947[7=H fi4g4z974>W;9 nV7[Q7? fiYW9C>[dPwff $G6t9&~!*hxvJ|FE>9;[d[>J [fi&1ffg4$[9dd ff 7jjQ49>[CJfi& ff >47&>[ >$>9E>>9 >97z>&[94$dC ff 7 7 ff +dW99+d#fi&I V? fi$Qff h-47*7 >JKZ:jwfiJf\wj (W vJ:>H , (J_J'(W&#AzMCz97[ >9>>CMzWv-8v_|x_vx~#>4MMh-*Q 7 E >?&L,J(J_J'(W& AHJJfi-ff fifififf fifi"!$#&%(' )%+*,!+-.%+''/!10)2!4356#&%879';:'<#&='>)?@A!17+)?%#B!#B=C'D%+-.#B!(#E>FG-%+'%/)IHJ)%+#LKM-7N@' 7$-?O).%+'!-PQR )J%+'%JFFG-%+'OUWVYXZ\[^]`_ _`_+]1ZIabO)?@cdVYXfeg[^]`_ _`_+]1eih(bSOj!4#&%>' )2%+*!+-k%+''l!;0)!4)?mon -PXqp"Z [sr Xftue [ ] _ _`_+]1tue h bfv] _`_ _9]fpjZ awr Xftue [ ] _`_ _+];tue h bv;b4#&%/) R ' )%+!$x' ?' 7+) R #&y )!+#&-?A-2PzU{J?@' 73 5 D|%+-A'=' 79*~}?#B!'l%+'!I-P QR )CJ%+'%>0)%>) R ' )%!4xC' ?' 7+) R #&y )!+#&-?J?@' 7\3 5 #?6S,A-7N'-f=C' 7Do#LP' ) Q 0lZo#&%w)/-7+? QR )J%+'/)?@,' ) Q 0\e|#&%w)$^" x79-J?@ R #&!+' 7+) R p#"S'SDq)x7N-J?@)2!+-E>vDC!10' ?!10#&% R ' )%+!/xC' ?' 7+) R #&y )!+#&-?6# RBR #&!+%+' R P8) R %-',)O-7+? QR )CJ%+'S, QQ -79@#?x R *Du#LP8c#&%I)\}?#&!+'\%+'!-PF-%+#&!+#&='\x79-J?@ R #&!+' 7+) R %`Ds!;0' ?A'=' 79*k}?#&!+'>%+'!(-2Pg-7+? QR )J%+'%I0)%4) R ' )%!/x' ?' 7+) R #&y )!#B-?J?@' 73 5 #?,OSs>$`A~MMM\k MM-f'l!1J7?kPj7N-EY%1J%;JE>F!+#&-?!+-k!10'l#E>F R # Q )!+#&-?-79@' 7SAM?6!10#&%I%+' Q !+#&-?'># R&R @#&% Q J%+%mon+% DC#?>!10'$?'<!%+' Q !+#&-?I'w0)?@ R '(n%`Ss-7u-7+? QR )J%+'%`D!;0'wmon/KMHqJ'%!+#&-?l0)2%z) R 79' )@*'' ?k)?%+' 79'@O?'x)2!+#&=' R *O*\Jxx R '!+-?,)?@k('/)'@!4pN vSm'!Z\[,V>p"p"v+v >p"vDZV>p"p"v+v, >p"vD([,V>pjpjv+v >p"v\)?@VW>p" p"v+v>p"vS0' ?,'I0) =C'/-!10([8 VXfZ\[]1Z b>)?@OVXfZ[^];Z bSj!w#&%(?-!=' 79*I@#L Q J R !!+-%+''$!10)!!;0' 79'w)79'(?-4E-79'%1FG' Q #L} Q -7+? QR )J%+'%!10)C?>([)?@\ !;0)!#E>F R *-C!10Z\[()?@Z SI-7$[^(?-O79'%+- R =C' ?!-Pz([$#&!10A#&!+%+' R P#E>F R #&'%lZ)C?@?- QR )J%+'\!10)!$#B%F79-FG' 7 R *l%;J%1JEI'@C*>([%+!+# R&R #E>F R #B'%/Z[z)?@OZ Su-7 |'=' 79*,79'%+- R =C' ?!-Ps #&!10\#&!+%+' R P#&%8)=f)79#)?C!-Pu D)C?@O?- QR )J%+'!10)!#&%(F79-FG' 7 R *,%1J%1JEI'@A*, %+!+# R&R #E.F R #&'%IZ [ )C?@Z0J%w[w)?@A)79'I-!101EI#?#E>) R& x' ?' 7+) R #&y )!#B-?%4J?@' 7#E.F R # Q )!#B-?-PXfZ\[ ];Z bS #? Q '([z)C?@l )7N'8?-! R -x# Q ) R&R *l'HJ#&=f) R ' ?C!8J?@' 7#E>F R # Q )!+#&-?D!10' 79'#&%?->mon/|-PuXfZ[^];Z b$#?\OS-'=' 7D!10'Pj) Q !!10)!!10' 79'#&%?->mon/|-2PXZ\[^];Z bw#?>@-'%?-!EI' )?I!10)2!zZ\[z)C?@Z0)`='w?->mon|#?D%+#? Q '8)I-7+? R )?xJ)2x'w#&%)/E-79'(79'%+!179# Q !+'@\%1F) Q '$!10)?.) QR )J%1) RR )?xJ)2x'SM?.PM) Q ! D#&!#&%%10-w?\*\Jxx R '!+-?\)?@Ou)x'Ip9 vu!10)!V>p"up"v+vl>pj pjv+vi>p"v#B%)?kmon/-2PiZ\[()?@Z#?S-7!10#&%/79' )2%+-?D#&!/E>)`*,'I-79!10Cw0# R '(P"-7!;0'4mon!+- Q -?%#B@' 7) QR )J%1) RoR )?xJ)x'#?%+!+' )@,-Pu-? R *O-7+? QR )J%+'%?!;0'l?'<!I%1J%' Q !+#&-?Du'>%10-!;0)!/)?C*O}?#&!+',%'!/-P QR )J%+'%Iw0# Q 0 Q -?!;)#?%4)! R ' )2%+!-?'I?-?CKM!1)J!+- R -Cx-J%PMJ? Q !+#&-?CK"PM79'' QR )CJ%+'D0)2%8)?kmon/#?\S8?#E>EI'@#)!+' Q -79- RBR )7N*,-P!10#&%79'%1J R !#B%!10''<#&%+!+' ? Q '$-Ps)?Imon/-Ps)?C*(}?#&!+'$%+'!u-PPjJ? Q !#B-?CK"Pj79'' QR )CJ%+'% S?>-J7uJ%;)x'-P!10'-79@oD)\PMJ? Q !+#&-?CK"PM79'' QR )CJ%+'4E>)`* Q -?!1)2#? Q -?%+!1)?C!+% D'=' ?!10-Jx0 Q -?%!1)?C!+%)79'(%+-E'!+#EI'%%+'' ?k)%PMJ? Q !+#&-?%w-2P)79#&!9*GS\Czo oW-7E-79'SQR )J%+'>#&% "L^9;$#P#&!(@-'%/?-! Q -?!1)2#?PjJ? Q !+#&-?k%+*EI- R %(-P)79#&!9*-!+'>!10)2!4) Q R )J%',#B%PMJ? Q !+#&-?CK"PM79'',#L#B!>0)%I@' F!10SO? Q )2%+'l-2Pg%'!+%/-P QR )J%+'%.w0# Q 0) R&RQ -?C!1)#?.PMJ? Q !+&# -?,%+*EI- R % DG!10'4monKMHJ'%+!+#&-?k79' E>)#?%$-F' ?S|/oos o.AsoA ffM?!10#&%l%;J%+' Q !+#&-?D',# R&R %10- !;0)!l)?C*}?#&!+'k%+'!lU-P QR )J%'% Q -?C!1)#?#?x)! R ' )%+!\-?'?-?CKM!1)CJ!+- R -x-J%gPjJ? Q !+#&-?CK"PM79'' QR )CJ%+'D0)2%w)?,monu#?\S\Czo m'!>'O) QR )CJ%+'Dz[^]`_ _`_+]1Ga~) RBR @#B%!+#? Q !l=)79#) R '%>#?\D)?@ fi )k%+'!I-P!+' 7+E% S0' ?!;0';\ -P8YS7S! Sfi #&%wp4]fivV X k6V Xf [[ ] _`_ _9]1 aMa b]fi"!$#%'&'(*),+.-0/1&'#%324657#98;:'<>=?A@*BCDBFEHGJI6KMLONQPCRBTSBCVUXWZY\[JY\]_^*`baQNdc\egfhjiTkl>ll0k.hAm>^onqp9rsp.BTtJPNAuTv$rw*p0BTp>L"t@*BxMt@*Byz'{T|}z3~j{T| PN1c?`CT`t`'Knqp1cjkKse;Ah kKsl>llTAh kKsT`P*CBTr*vBL*nN_heJ3F*1rx*J ef3k^L>t@*BxRAhk0AefJ*F.TkJff00TkJ.'ff*0TkJ07ff00^`_$_ BTtBr,x*nt.BAp0BTt"PN_uTv$rw*p0BTp>Lrx*ff'Bjr'PvqBpw'*p0t.ntw*t0nqPxNPC`@*Bxt@*B |>{>| PNUFsnqpAt@*Bjp0BTtPN1rvqv_t0BC0pdQnx*uTv$w*'n$x*opw'*t0BC0p PuTuw'C0CVnx*n$xo`t0BC0p0BTtPNAUZp0P*BJ;nqpJr,x*nqt0Bp0BTtPNCVPw'x*Zt0BC0p` PCjn$x*p0trx*uTBLt@*B9t0BC.\p0BTtPNeJ7Tk7kJ7k'k,>30"Uffef73kk^nqpA ef3kk*Tkk>^`w'Cd'B,x*nqt0nqPxZPN rFt.BC0\p0BTtjuTPC0CVBTp'Px*'pRt0PF?A@'rta'BTp0tr v$'w*nqp0toDWLWurvqvprn$x*n$Jrv"t0BC0gp0BTt>$`aHx;@*nqp'B,x*nqt0nqPxL"nN;nqp9rs'PvqBgpw'*p0t.ntw*t0nqPxNQP*Crp0BTtRPNAuTv$rw*p0BTpMef kl>ll0k ^?`CT`t`*p0PBjPt@*BC"p0BTtPNuTv$rw*p0BTpOL't@*Bxr |{>| PNOnqprR,x*nqt0BRp0BTtPNt0BC0p?A@*nqu@uTPxtrn$x*pt@*Bn$x*n$Jrvt0BC0p0BTtPNUFZrprpw'*p0BTt`p0n$x*J@*nqpAx*Pt0nqPxPNt0BC.p0BTtL'@*B'B,x*BTp9 y,qy~}|yz rpNQPvvqP?pnNhrx* rCVBAuTv$rw*p0BTprx*FnqpArRt0BC.p0BTt"PN"f ^UJp0PB'PvqBpw'*p0t.ntw*t0nqPxo?`C`t`'f>hJ^L*t@*Bxh y,qyT{?`C`t`'nNAhk0A e `An$J*vqnqurt0nqPxnqpj'BTuTnq3r*vqBL?"BrBCt@'rxFvqPnqurvn$J*vqnqurt0nqPxMrx*p0tCVPx*BCt@'rxpw'*pw'J*t0nqPx`"a'BTp0trR v$'w*nqp0tJDWL3WnqSBTpt@*BCDBTpw*vtt@'rtrxU,x*nt.Bp0BTtPNuTv$rw*p0BTp@'rp1rvBrp0t1Bx*BC.rvqnrt0nqPx9w'x*'BCn$J*vqnqurt0nqPx?`CT`t>`rxUt.BC0p.BTt1`P?"BTSBCTLrpj@*BJrvqp0Pox*Pt0BTpLn$J*vqnqurt0nqPxnqpx*PtAtC0rx*p0nqt0nqSBJrx*Z@*Bx*uTBJx*Ptrw'rp.nP*CD'BCT`j@*BCDBNPCDBnqt'P*BTpx*Pt3tRnxt0PFPw'CdBx*BC0rvNC0rBT?P*C0F@*BCDB` PCt@*nqpCVBrp0PxL?"B?nqvqv x*Ptj'np.uw*p0pntdNw*vqvqU@*BCDBL3rx*FNPCt@*BjprBCVBrp0Px?"B@'r>SBx*Ptn$x*uTv$w*'BTrCDP?NPC"n$J*vqnqurt0nqPxsn$xor*vqB9W`BTtw*px*P?bBTn$x?nqt@jt@*B 'CVPPN'PN3Pw'CCDBTpw*vqtuTPx*uTBC.x*nx*jt@*BBT*nqp0t0Bx*uTB"PN _ a0p>`"Px*p.n'BCh6eJ_k7kJ'kk*rx* L9rx*J rpr'PSB`@*BxRh e rx*Jrvqp0PAhk0 e Lp0n$x*uTB nqpArJCDBTp0PvSBxt"PN1J *TkkT_Jk kTrx*oJk *TkTJkk *0TL?A@*nquT@rCDBn$xAhk.AT` p?"B1?nqvqv*p@*P?Zn$xt@*Bx*BT*tvqBJJrLt@*nqp@*Pvq'pn$xBx*BC0rv_nN,h e rx*hnqp"Nw'x*uTt0nqPxNCDBTBL7t@*Bx?"BjurxFCDBTp0tCVnuTtrt0t0Bxt0nqPxt0PJt@*BjCVPw'x*Fn$x*p0trx*uTBTpPNhnx*p.trxt0n$rt0BTt0PJt.BC0p"n$xFt@*BRt0BC0p0BTtPN Up0PBO`@*BA'CDP*PNPN BJrRw*p0BTp"t@*BNQPvqvqP?n$x*n'Br`1"Px*p0nq'BC"rR'BCDnqSrt.nP*xPNrjuTv$rw*p0BNCDPrjp.BTt cPNCVPw'x*JuTv$rw*p0BTp`'w'''Pp0Bjp.PBPN_t@*BAuTv$rw*p.BTpn$xFcuTPxtrn$xJt0BC.px*Ptr'BrCDn$x*n$x`1@*BxrxUFvqnqt0BC0rvpuTPxtrn$x*n$x*Ft@*BTp0BRt0BC0pn$xcw*p0tBCDBTp.PvqSBTr>?r>UJn$xFt@*BR'BCDnqSrt0nqPx`@*nqpBrx*pt@'rtjnN?BJCDB*v$ruTBFrvqv1t@*BJt0BC0pjn$xt@*BJ'BCDnqSrt0nqPxt@'rtrCDBx*Ptjn$xL_Up.PBPt@*BCt0BC0ELt@*BxRt@*BCDBTpw*vqt?nqvqv3'Brx*Pt@*BC'BCDnqSrt0nqPxPN,` P*C_BT'rJ*vqBLt@*B"vqBNQtPN'3w'CDBp@*P?pAr'BCDnqSrt0nqPxPNvqBx*t@WjPN19`@*Bt0BC.T nxt@*B'rCDBxtuTvrw*p0BTpA'P*BTpjx*PtAr'BrCn$xo`aQN?"BCDB*v$ruTBjt@*npt0BC0UJt@*BjuTPx*p.trxt 7L*t@*BCDBTpw*vqtnqpArx*Pt@*BC'BCDnqSrt.nP*xFPN1CDnq@tPNt@*B3w'CDB`$Z $ * $'Z * $ffffffffff fiff fi$'nqw'CDBC0rx*pVNQPC.n$x*Jt@*BjvqBNQt'BCDnqSrt0nqPxFU*nqBTv'pjt@*BCDnq@t'BCDnqSrt0nqPx>| h 9} *z3~>|yz TD~q} {| fh^ } z| |{>|1 j/.10 $0 2034"!$#65!%#7!&8:<;<=0J}s~>} {T 9} >\{ {T|y| |yzz h ehk0A e'#5&(9*) +$#,#0-!fi> ?%@ ACBDE?%FG?%HG@GI<J6K@LBGJMGF A"@GFGNODEHG?%@LBG?PACBQ%R%?TSTJ@GI<JK@LBGJMGFUWVYXZX [\^]T_T`aTbcedgf hOikj6dmlonqpsr%aTtWikj6dmlonqpuf h(vwsx'yucqzGr<{'cEdgf h(vws|s}E~Lydgf h(v'Zc<rmG|^]T6v`Erer'To~%~%%%x'zTc<avw`Erer%TC~%~%%%x%C~WzT`b<rLocE`~P'{P`~PTY|1_GGGG~%Ccv`EaT~%rr%TC~%~%%%xzTc<avw`maT~%rr'To~%~%%%|_T`aTbcdf hvwsx`e,~%~LyE"~PEzTc~P7c</-zGrLzTc<7ccP`ooraT`ocEoc~LP~PGaTt`aTor%aTbc~Ldx%TbzzGrL1f h(vw|mzTck_GGTGTC`~TaEzTc~Pc<xzTc<ccT`oor*tGc<7`{rLC`~Ta27~P~LErbr%TocxTbzzGrLWvws|_T`aTbc`P7~TGaTtxTCWrLo~cP~PGaTtxo~yczGrY{%cvws|zT``T`c-zGrL~PaTb~Pa'rL`aToc<o~PnW|cEucr%ar%CT`or%7oc<o`anr'aTtckGc~PTrL`aTct*7~Tg%7c<TrLb`aTc{'c<7Cc<o`abr%TCcu`aykzT`b$z-`uaT~%`anWx%'m|1}~%oczGr1o`aTbcqc<rLb$zbr'Tock`a`rqT7~PGaTt`aTor'aTbc~L1zTc2GaTbo`~Pa',27ccbr%TocdxZc{%c<7br'Tocm`a`rLo~r-P7~TGaTt`aTor%aTbc~L1d|W}E~Ly`e`c<rLoo~occzGrLzTcr%mc7c<TrLbc<mc<a'm~Lkoc<oW`azTctGc<7`{rLo`~Pa~27~P7cTom`artGc<`{LrLo`~Pa~Lk~P]j7<pqc<rLbz7co~%To`~Paooc<`azTctGc<7`{rLo`~Pa27~Pb<r%arLo~cb<r%o`ct~PT`azTcmtGc<7`{LrLo`~Pa*~P4oc<ou`axo`aTbczTcWr%cWoc<ok`ar'7c7c<TrLbct'zTcWr%mcxr'aTtj,%pzTceCc<om`a^zGrr'7cWaT~%`angj,r'aTtzTc<aTbcr%7ce7c<TrLbct%ptG~aT~%r%Gc<r%`azTcb~PaTbTo`~Pa~LszTcetGc<7`{LrLo`~PaZ|_T`aTbczTc<7cm`Wr-tGc<7`{ro`~Pa~u~P4yczGr<{'coce~LEP~PGaTt`aTCr%aTbcm~Ludr%aTtrL1Cc<ome`af hxZr%aTtzTc<aTbcr%7cmoc<oW`anWxo~f hvws|e`Wr^ikj6dmlonqp|Wc<aTbcikj6dmlonqp1f h(vws|Zc<-rb<r%aGaT~%c%c<aTc<or`ct^o~zTcb<rLCcykzTc<7cdb~Ta%r`aTGaTbo`~PaoG~%~Lr'7`'x r'LcWdhj,sj, plPpj,GlpEr%aTtvhj,sj,TplTpj,lsj,Tpopj7~TzTcmcr'Tc%`{%c<a~TaZ|'~Lu,tGcor%2qT`o<x<'%%p|WzTc<anhlsj,TpW`WzTcmoc<Coce~Lvr%aTtycWzGrY{%cWdf hvxG%c`qb<r%aGcWCcc<azGrL1ikj6dmlonqpEf hv|EzTcmr%7PGmc<a'kToct`azTcmG7c{P`~PTc<rtG~TcaT~'1y~PozTc<7c%xGGcb<r%TocetG`c<7c<a'oc<om`a-o~PcEP7~PGaTt`aTor%aTbceaTcctaT~%7crLoco~tG`c<7c<a'e{r%`r'TcY|k~PcGr%Tc%xZ`azTcWP~PGaTt`aTor'aTbcj%jPplPpj ljPpop~L1dxycmb<r%aGaT~%oToW7c<TrLbcj,Tpk'o~Pmc~%zTc<qoc<Cx~TqzTc<azTc7cTo`aTbr'Tocy~PTtaT~%Gcmr%a`aTor%aTbc~Lsd|azTcm~%zTc<kzGr%aTtxc<r-b<r%aGcm%c<aTc<orL`cto~r*<~1br'TocW`aTooc<rt~uro`aT%cbr%Toc%|k`roc~LGaTbo`~Pa',27ccbr%Toc<xsd`r%ar%oT`Cr%7br%TCc%x1r%aTtw`r_GL~%c<GToC`To`~Pam,~Pd(y||<|Lex'zTc<amyckzGrY{%czGrL1f h^d(`ikj,elCnkpf h^dws|sEzTcqG7~P~`urLm~'o`oc<orLzTcer%mcWrLkr'G~L{%c%|zT`m7cTW`T`czGrWf hd`7ct Tb`TcC~r%a`T`b<rLo`~Paikj,"lonkpWf hdwOGc7ycc<aP7~TGaTtbr%Toc<|1_T`aTbc%x 'zTcqaTcPc<rGx'`T`b<ro`~Pacycc<a-P7~PGaTt-br'Toc`tGcb`t r%Tc%x`,~%~LyEqzGrLEf h^d`EtGcb`t r%Tc`ab<rocW^`E2GaTbo`~Pa',27cc%|ZT<<Ef h^dsP7 fffi<'1%fiY'P6,q$YsP7 fffiY%qff<7qeUWVYXZX [r%aTt!GcWzTceoc~LrP7~PGaTtrLC~Pm~Pbb<GC7`aT`a^r%aTtd|ZcEdh}~yb~PaTo`tGc<EzTcq~%~LyE`aTCrLoc<mc<a'o<xTykzT`bzb<r%acWzT~LykacGT`{rLc<a'<|j7<pf h^d|#" $lol%$G`eGaTrLC`r%Tc%|#" $ lol%$GmzGrLkaT~c<oGor%aTtm~TtGc|j'&Pp}~GTocq~L(`er%aEc<CGor%aTtm~TtGc~L1#" $ l ol$j,%pj,%p)*)P|fi+,.-/1012 3547698:01-/ff;=<?>@-BADC1EFGH IJDKLMON?KQPMSRTKQPMN?KQUVMXWVY!GH ISZ\[I]_^@R`P ZbacKQdH1eJ fgihjISIk:LlVmUMSRon pqJ rSIseutwv9ZxKQUMN?Ky^\MSk1zIH1e{IBKLMON?K'^\MSRK'^\M|pwvX}1ISrSpw}ffeW twIBW~ISre v9Ipwv5J pw9Ik@v9ZKLM:pwvo}1ISrSpw}ffeW twIebvzISttQRjjff\o5\ '\c :\i.To@QS:S ffQ'~`S%TwV@wV%'\SwuGH ITaQZtwtwZuzp.J f!v9IS1 IJ rSIBZbatwI] ] euvT]TZ\[IZ\[otwISv9vaQZtwtwZzvH IB1eb99I[9JZuaQ}1ISv9eV]Xt.]T1 pwv9vKLlVlMjhjI] ] eTL9ZhjI] ] eLPK'v7pq]pt.e[7ZchjI]] eXU~R.L9ZohjI] ] eoU1R.LPZua5Q}1ISv9%e]otq] pwv9kLllVUMSR(GH I[IH Ifpw{ISveT1[Z\Zba(Zua(H IIS pwv99IJ rSIZuaetwIeuv7fIJ I[9eutwpweu9pwZ\J1J }1I[G|p.] twpwreu9pwZ\JZua:eJVYs5J p7Iv9ISoZuaXKQJ ZXJ ISrSISv9v%e[pwtwYa1J rS9pwZ\JVQa[OISIMorStqeV v9ISvR#IreJ!eu}u9 v9%H I1[Z ZuapqJv rSHezeY!%H1euzI reJ v9IBpw9Z!ISv9%eW twpv%H#H IsIS pv79IJ rSIBZbaeVJ#hoZbaeVJY5J pw9Isv9ISZuarSt.e v9ISvrSZ\JVeup.J p.J febtwIeuv9Z\J IJ Z JVe 9ZtwZfVZ\ va1J rS9pwZ\JVQa[OISIrSt.e v9IR@#:i9sff'QB@1Q\QV7\VBVSS%:_71ox7BSV'\Swffsw 7XS7#qu%SyT'Sff'QS|X ffw.SQ'1XOc\@'j'%QV5\'\S1DXK'SMTXK' %Mj.off'QS|wVjffhjISXsW1I%H I] eu pq]eutj}1I HZuaH I9I[7]TvpqJrSt.e v9ISvopqJRQayZtwtwZuzva[Z\]ihjI] ]eLH1euoKQeVJ }H IJ rSIBeutwv9Z MoreJ1J ZVrSZ\J%eup.J!7I[9]TvoZua:}1I Hf\[OIeu9I[H1eJ!@kJ Z\[o1[IS}1pwreu9ISvka1J rS9pwZ\J vZ\[rSZ\J v9eJV9vZVH I[H1eJH Zv9Ip.JRGH ITv9ISoZuatwpw9I[9ebtvzXH pwrHreJW1ITrSZ\J v9%[9 rS9IS}a[Z\]1[IS}1pwreu9ISvp.JTeVJ }a[Z\]9I[9]TvZbaff}1I HTeu]Zv9(orSZ\J v7pv79p.J fZua1a1J rS7pZ J v:eJ }TrSZ\J v9eJV9vp.JeVJ }{ue[p.eW twISvp.Jskjpwvo5J p7IRIJ rSITH ITv9ISoZua:rSt.e v9ISvzXH pwrHreJW~IrSZ J v9[9 rS9IS}a[Z\]H Zv7Itpw9I[7eutwvXpwv5J p7IeuvzIStwtQR: pwveTv1W v9ISZbaH pwvov9ISk v9Z pvoe5J pw9Iv9ISZuarSt.e v7ISvR@#17wV7S\@Q'1%wVS ST'\1 179917 '\sQ%_c iiujS9~7 %\wSVff.VfffffiXK'TM%:'\?K'pw}1Z ISvcJ Zo] eu99I[|9ZTzXH pwrSH9I[9]TvjffhjIS9% 9H {ue[p.eW twISv~9~ e[I ] e1~IS}xWYkjeuvtwZ\J f!euvH ISYeV[I ] e11IS}9Z9I[7]TvXp.JXMSRn1111ZVv9IXXKy%M:ff%99 R:GH IJ oK'7XM@9\RhjIS W1ITeJ!hnZuaoK'%XMKQJ ZV9IB%H1eu!]T v9TW1Isa1J rS9pwZ\JVQa[OISIMSR#GH IJaQZ\[IS{I[OYL "$#%"'&jkH I[Ie[(*)v rSHH1eu%%(*),+-)RBGH pwv ]TIeJ vH1eu%(*)./+).DeJ }xH IJ rS!(0)1-)2kjaQZ\[oIS{I[YL "/#3"/&jR:GH I[OI%ayZ\[OI DXK'T9oMSRn p.J rSI k1z54 J Zuz a[OZ\]ihjI] ] eLoH1ebrSZ\J v9eVJ9ve1~Ie[p.J f p.Ji]T v9eutwv9ZBe1~Ie[p.J!RGH pwv]TIeJ v%H1euXpwveB6n 4bZtwI]_v1W v99pw 9pwZ\JaQZ\[o_zR`[SRRj RXGH IJa[Z\]_hjI] ] esUz74\J ZuzXK'T9oMX skH IJ rSR 851[OH I[9]TZ\[OIkjv9p.J rSipwveJhnZua(XKy%MSkeutwtrSZ\J v9%eJ7v:p.J ebtv7ZeV11Ie[|p.JsBk H IJ rSIeutwtffrSZ\J v9%eJ7v:p.J ]T v9e1~Ie[|p.JBR:GH vpwveutwv9Ze 6n 4uZVtI] v1W v99pw 9pwZ\JayZ [zR`[SRR~ \RGH I[I%aQZ\[_WYhjI] ] eP1RdZ\J v7p}1I[ '9 KQ:ff;M=<>9 K?:ff;1@MXeJ } <A@KCBXMR!D|ZH eJ }x p.] twY#H rSt.e v7I9 KQ:ff;ME<F9 KC;1%: MSG@KCH%MSRJIZuz J Z9IH1eb(X9 KQ:ff;ME<F9 KC:@;1%ffMSG@KCBoMjeutwv9Zp.] twpwISv BRGH pwvcH ZVt}1vayZ\[|rSt.e v9ISvpqJfIJ I[9eutQk IS{IJsp.J%H Ic1[ISv7IJ rSIZuaW1ebKr 4Vf\[Z\1J} 4 J ZztIS}1fVILNM*OfiPRQTSRUGVXWQTY6QTZ6S6[N\.]*SV6\1^6YRU=S6Y6_`WZ6QTSV6QUGVbaTcTQ-d-\2S6[N\1]0SV6\1^6YRUe fhgi-jk-j*l-mnojNp7p7q%rostujNvxw7yTjNk-jNvzq{n}|~6-mr2kmi-rosszj**mzrokffj k-nowk-j*j* mi-j s6j**r2qnNqsGj ,i-jNvxjTq k- qTvxj?6k-*mzrokuCvxj*jqTk-erosjNp7-mxwTfbj5k-j*j* mi-j,yTjNk-jNvGqnJNqszj mz!6vxtTj mi-j j*lroszmGjNk-*jq76rk-j**mzrokfhXX 7 uRGNoTu***0.-T TRTx *CTTTxTK76T3TReG!*N!NoTu**Nff2 TR% .N /JR6666Tszj qTk- |qTk-noj*m! ~6j7q p%-6j*nhe N f%-rk-*jqTk-bqTvxjszmqTk-Rquvx6ro*j*qu6qTvxmN|6mi-j!*n2qT-sGj rs j*6-rotqnojNkummzmi-j=}vGp!-n2q ! C.,i-jNvjff ! 6jNk-Tmzj*sffmi-j,6k-rotTjNvsqnonow66qTkumzrRj* *nqu-szj % fhgi-rosp%jNqTk-smi6qmrosqp%-6j*nR-vqp6j*n{EfJ6vxmi-jNvzpvxjT|R ros qnoszq7p%-6j*nEe=|6sz7rom}Tnonos?vx-pemi6qm ros q7p%-6j*nfhgi--s5e |i-jNk-*j fbffjNqTk6vxtTj mi-j%j*l-rsGmzjNk-*j%qTkffqukTwk-romzj%szj*m `*n2qT-sGj*s ,i-roKi*kumqr2k-sqmnojNqszmhk-jk-kumqT-mzTnoTyT--sqTk-=?6k-*mGr-kuC?vxj*j*nqu-szjTfk=q*mjffNqTk6vxtTjsGp%j*mi-r2k-y szmvk-yTjNv*|k6qTp%j*now7mi6qmffmi-rosros,q%N.TTzf6gi-rosffros,qTk7hmi6q{mrosk-Tmffk-now7r2p7-norj*|J~6-mq*m6qnonow*KK-%~uw qTkTwTmi-jNvyTjNk-jNvzqnoroNqmzrokEu2NRJj*m`~6j!q*n2qT-sqnn2qTk-y6qyuj!qTk-~j7qk-romzj%s6~-szj*m hfff5kr2kros Nqnonoj*qN.T3Er2k|6r / }vj*tTjNvxw yTjNk-jNvzq{nroNqmGr-k b/6k-6jNvr2p7-noroNqmzrokJfTmzj mi6qmr!ros qTk`{=qszj*m7*kumqr2k-r2k-y`qm7nojNqszmk-jk--kumqT-mzTnoTyu-s%6k-*mzrokuC?vj*j*n2qT-szjT|3mi-jNkX~TwjNp7p7qX ros7romzszj*n,6k-*mzrokuCvxj*jT|ff~6j*NqT-szj rom%si--no`r2p7-now`mi-j?6k-*mGr-kuvxj*j *n2qT-szj.s rkfRvr2k-szmqTk-*jT| CR C rosqTk`{5C ff F C 6 fi h z qTk- CR 6 fumzj mi6qmmi-rsros,6vxjNvxnows6~-s6p%j*~uw7mi-jK *-|T,i-ro*irs CR C fi f kpqNwszp%j*mzr2p%j*s ~6jmi-j jNp7-mxw*n2qT-szj,|}vj*lqTp-njrE fi f**" !{# %$& !T( ',* )+%? ,b Gb/uT*T u. --/ -- 22-R0 1C*Nx,NoTu**JKxT /TRCu1u oKTR T&1xCTCufi -Tu*NC.T& 1Kx,oTT*..-Nx%/ 2*.7N.T4 3ff x,.7JR`Jj*mff*% 565%5z 76,~j5q5k-rmGj,szj*m3*n2qT-sGj*svxp|us-Kimi6qm*kTmqr2k-sqm3njNq{szmk-jk-kumqT-mzTnoTyT--sff?6k-*mGr-kuC?vxj*j*n2qT-szjTfffbj NqTk q{szs6p%jromi--mnoTszsyujNk-jNvzqnoromw mi6qm,*kumqr2k-sk-%mqu-mzTnoTyTroj*sNfhJj*9m 8~6j q& :{TnojNp s6~-szmzrom-mzrok}-vh.| ; % < 6 5%5%5G <>= 5~6j mi-jmGjNvzpszj*m ~u?w 8hA| @ B *% 565%5z = ~j7qszj*m tqTvxr2qT~-noj*s7qTk-DC N6 5%5%5 ~j7mi-j%szj*mqnonyTjNk-jNvGqnorNq{mzrok-s `6k-6jNvr2p7-noroNqmzrokr2kEfhTmzj mi6q{m C|6sz C ros5k-TmjNp7-mxwTf-r2k-*jjNq*i*n2qT-szj r2Ek C p%-szmffr2p7-now mi-j 6k-*mzrokuC?vj*j*n2qT-szj.s r2k |-romff}unnosffvxp'JjNpp7q mi6qmH * @ H @ 565%5ursqnon p%jNp%~jNvxs C qTvxj 6k-*mzrokuCvxj*jT4f F3w JjNp7p7q G6|6mi-j szj*m C 6Iq=k-romzj,sGj*m{*n2qT-sGj*sNf-r2k-*Jj C6rs3k-romzjT|6mi-j,szj*mff H L K @ sffrosq{nsGk-romzjTfERvEszr2p7-nor*romxwT|nj*mH K @ 65%565 H # @ ~jmi-j szj*mEq{nn6rsGmzr2k-*m H L K @ sfj*m K ~jquk! H K @ |0}-vj*tTjNvwJ NOPNQJ|s-*imi6qmhJ *% 565%5z qTvxj3szmqTk-Rquvx6ro*j*qT6qTvmNfv j*tTjNvxw`NSRTNVU |mi-j%mzjNvGpszj*mX W~u?w 8Xros%szpj!sGj*m % < WZYN% 565%5z < WZ[z]\^;|s-*imi6qJm _a`IRcb*f vxpJjNp7peq d6|Jffj%i6qNtujmi6qm K W |6Cvj*tTjNvxw NfONfQquk- NgR]NfU|h%i%ifij9kml.n&o&prqtsfiuvwo&l.n(xzy|{}le~I&6r%r/L9 |LLc^/JVV"%%/#(r%9&6. ^zB%eD.&rrrrJ%cfiJcff"%r%ZS%rZ&Zc.&cL^#//Affc#%cmrLrcS&c99#P/.%f.r/.%ZET.r%Z.TZEVJ&/Ar/D&L# % JV# /rEr.AXEct 9 /9L#fi/wceX.%r%fic%ct# * L&r&%*&r&rBr%r/* r%t&cE9mr/#/r/&r%r/.}4fim/&mc/?&6.Jr.cLmr(B%B(6I //}"?}*t"%mtZ6rT.mBA/&cA%9J/r fffi % 6 }%!"#Z/ %$w r&Z '()/+*,-. +B/f&etrefi/Jcw>&r/0>Z/e/m.rfi/m rT/cmrJ.r./%.r..A&AffcL1r%Zfi./L2(#rrcmr/].&//mrecB.rfi.../.>Z 3wr#&Z/rrLr/%E.r%#LX&//mcAAXc 2 49%.Z.&r9*fi &//mcAXwI/ wm/6B(rE/Zm.I]rermcLmr/D/.%grrrIrD&&%65%&.rSm%&Z/rm.r&.ELrr>&r/&&.tr (rEDr/&cr]r&&%cc&mre&r?cfitrrJ%c%/AS6]r/.%Zr//fi&mr/.]]mtrr&%cr0>/&r c%?/m.rfi/% r]6I/%8 7c &.c&ff9B9mr/Tr%ZT.]rtr/].%c/Z/9c/m.r/&cP%.;:&%&6.&" <A%Z%.fi>=m] r&&%9ctrfi5%&.rEm%&Z/r% rrL49rE/%&4rr&&%9cr0>/rcc%.&r/fir0>Z/#/m.rfi/4rr]c&trfi. rLL&r&%Zfi.r&mr(&mr/J..%cBmr%c.rr%r/c..%c/&rcc%r/%JJ&tr/DmE% #c %&/cr%Br.?r5L%fi&.rm.fi%&Z/cfirr% rfi.r/.}&trA@ CB DrL.rEB F F//HGJIffB K KfiX&HzLeNMPO r%E9&6.J G QI(@@SRTO& G G(U #.r G(U VLf G r9/.%ZWMf.}/c.&%#.cSX %&rr 0Y4Lr%r(&ZZ\[/Jr.r%m%Z/firD/rfi/r%r/c r/Z%^]/r/%r/ 0>c& /rm.Zff0(r/r#&&%ffrwr#&trc/ 0>c&&/ V#X&//mccfi/L}r%JJ/%&cLJ/ 0>c&fic/AL.E/m.r#r/r/cr/S.rr%r/Erfi #&_ ` emr/e E&//mc4cLL /r&%Iff@bO#r%Zc@rr0>.r...rr&r/0>Z//m.rfi. r%D&//mc9A c*Sw..%c/&rcc%.JfiA@dDZ/c.%fir/Xr#V&X/ e 0>&c/AJ/ 0>c&ficc@]B%B(6gfC,) @E @8JSh!Zij#>(6ekl ZX& T$ & &ZX '()/V/& _m/.>\npo rqAstT9Eptvufft/wm>r&Z/rX&r//4&Zc./Dr'ff8/&} cL.DA9m c4/.%Etr/gc/m.r/J/cmrmr]c%crr0>.r...rrL>&r/0>Z//r.J>rJ&r//9LL/rzr _ `yxzr9rc.Z&EP9rc//>Z rc#r/I.Afi r]r&Zrccr&Z/r//B/&r/LrJAXwff{&|&}fi~j;H!!!& ff!e!6!!H!ff!jv>>8!&eTff!e!_b)()ffzz> bff! 8> & ff8>8>ffPi&\p>>m_>>>!> ji>8>jc&Wp\%8H&VffHff> mJ 8>j-+_>"Wi >\>ij&ff>W !&\iP>&ff8>> -eH W!v>;!V!&\!8> E&Vv8> 8>!8>!>&A 8 -Cff &&kHy _!ff>>V>!V!&ffH>>!V!&ff&AH8b>&jr H>j W8b>!.ffffH> &!H Wkv>gez>ij ffAff > W-> &!v >>> >! !&ff >b >8Wp>b(.>i 8SH &&& k!> Hffffp!>j HffcH A!SWPb\H!y & Vg H> SV&!V S!>b8 ff.i_!bkH&Vff. ff&&&_V>cff > -> v!W>>i;+j> &ff#Vpm!> . 8 >>i&ffji>WH! > >b_! >.&Sff8>8>c 8> ( &Y_ff_!&H !> 8> fiv&fibH>S > > ffH>ff>!(ym>c!.ffHff-H> >& !ff8j> !ffr&SS !>&V>+>W!\cffHm_ V&ji>> g8 >E!\ffH> .& !zV i&A&\>V V &&T >y>iVV&>!ffy > & 8>V6#&V&> ;8v!\!\.>&iS >8ff#&c> v_!ffH> AV!>_!V_m >ffff&&H>&v6 ffH_!ff>Sp"!r#6$fi;&%fi&VffH!ff!>ffH!>!V V> 8>b fi&%fivff!ff!\i! . !>&'>S>&!>p(!QV_! 8ffpj!)!J>E V>-k!>_!V_m >ffff&&!!!k>+>!V!&ff!j> >_jp8 +*>>+>!V!&Pff!!ij8> ff8> 8& ff>-ff>!C+E,z!_>&Vff_!!!y>&H+-S!j> ! !&ffHj V!i8HV _! .k &&.0/%y>&V>->!V!&ff!ij8> \V!ff&Vk >V/_> >_j&pff8> c>! !&ff &ff1H32 54p 6*7N 98: >v>b>!k&ff!8> i&jffb!ffV&kbV ; 1 y>W \!8> b 8> -> &kV > &\iH>k(&j +\%>Hj Hff&ff&h <ff => ? H>&_>#&&j kffffp!> yff% ff8> A@ B,@ =DCFE G V&V!&y>>+\i! >8> c!ff>&Hc &-H>&ff PgV>Hc &ff.i_!JS I&>!>8> H8S>&ffi!!&!!>!>S>V> S!>_ffffbpiff8&ff!<!> I!!&>ji>>E>-!>H!y>! >c>+y &_> j&> Ij&> ff_>p Yff!r!> > >-_> &ffp!H!\+_>pffz>H ffz 8>j ff8>8>ff&ffj>H>m>>m!> >H_pS!>_>K>zKNMPOQpv R(S8TN(U- VXWZY[!]\^Y_`%a _bdce_dafa _gdhibj_ hiYlknmpo rq5c"_tsng(q[uY"c Y%[+vFoJ L`%a _bdc cwo xFvy +z_g|{c}`vg|[u_q~gc_[aY_lc [wvg|Y)g|vgAF[u_bj[uva v9hivdbcno big|`%[uqvgAoexFYYff`%a _bdceYz[iYg[jYWnnmvFoqgV]qpc`vyfbi[u_j\ Yk%%fin5AAjf^9wA(|)A+(|(&+t(AfA| ffww|wA|nA^Fi%09n%j%9l %l9 ijAjA%j %l^j<9iF%l9 ^w 0 %l^jj%n+5jj5j %l9"d%j%9l %l9 ij09 9"0 5j9 " jffi&A9l ^ , "&jj%"AjjtjF%l9 9A 5l %l9 iL|j ff njt(Al|l j%09w%j%^l %09 iLA tj9 )AFA ff d%%Fj 9%jj"A,0ZFn" 5j9 tdjff inAj9juwAAAAd9"Al+n"A%dtijAd9 9 " dffj %Xj)Fnj jl9 j l j0 j<AA9Altw5ffj "%j Lj )"5ffj jX tj% i"d%Fn ffAljffAl"jff^9 5ffj <%+AjAj ffj5ffj j% ij ij ijl5< i^F0 %l^jd5jffjAw0$ j&6t jj ijtFijAF jjj9 ^jjA0tj" j^9%j l$d< lw j 9AF9 9j L+|i, 5j9 nj el L idl5Aj 9 ij%jjt j 99%j lff ,n9^ Li%Aj 99 iL(j %jwj%l^j%D idl5jtln %l9nij"jidj^ ijAj 9 idu6D5j9LjAFjltlFi#Dj5lu"tln % Aj j^9%j lffA j %l9 %9)ADdwl %A)Al$fj^^ jdltjAl$(A <jAj i<lj" 5j9 5^%wj 9D"9j(DA &AFt9 jwF%l^iuiLj ,A 99Aff F j,0wn99%AleFij 5jll9 j9j& ij9 A%j ,fF9FiDA%d ui<i%d%"jfDFn)9j 9dFfF9FjFA%ui9j5l%,j%& %XA%l9 jlAlj %l9%j%9l %l9 iX 9j9j9Aj 9 ij <j %,jj^A5ffj ]dd&j% ui9j5<j 5ffjAdffwjfffl Al jjniF%l^ 9+A 5l %l9 Dj9w "L j+ j dffAjL(j<j^5ffj dffj%j9jA0w t5ffj d<Aj<flj (iAFfi+ iAFfffiL%l9td%j%9l %l9 idjiF%09 9nA 5l %l9 ifF9FjFA%dlAAAd9djLtDt 5j9 %wj%ffdAnAAFij %5fjA5jffj"l nj)L tjF%l9ii,j Alw ftjw 5j9wj99%lff0j^j LlnA9fj)9j %l^t 5j9"j j "A dj9 djLlwAj < dj9AA%dFAj^AF<^fj9j 95 %"Xjj%Aj w5%l9<l"A 5l %l9 j%F< jXAFij %<t%F<n<%lffLlA9j95j ffLt Aj l %d^ 5j9An"AjA "jAj %Dl AAd9(%F56Dinl+ 5j^AFi%F j<f"dF^j(FA AdF%jj%jj9j )LAjll %9j" 5j9j e tjAjjl$tjL%tj n60 tltj9 <5jADillwL%ffffAAA9ijDdF5^<lj 9& 5j9 <j F9dj(FA PdAF%fftj% %dF 3ff<Alt95j ) "u lt"FiL%ffAlnA 5l %l9 ilAjA%w5ffj %l9 5ffj d)+tijnn"A%dtju lt5j<F j2"!$# %'&&'&(% *),+.- %'&&'&(% .)/! -10 &'&& 03'4'5)-fi67ff89;:=<>7ff?@7ffA@8@B'CED8:@CGF@?9H8@?@IJ<>A@7ff8:@79;:LKffMff7,N,CO8@B'CGDP8:@CGF@?9Q*RTS,UVSWYX[Z(\^]`_EabSdcfeSgUdhjilkm._nc=o1prqs'tfovu(sxwzy'{|wff}~wff{T{|wff},ws*s'Hjs''sEtsJ otEss2sP~tE~ wj E.o>,l@,|,.,,',ff|@*, Off,;.OJff','((r@ff(|',Off.;lH@,(|nOff,('@,';*ff@ff|x@ff(ff(,>@Ozj@,(|jOff, ,@ffjffx1ff'.,|( ,;('>,|j*'ff,>@, (^`@,(| Off,;@@(@'|ffff|'O.,||'(||O>*O(;ffTU V'UTGV[[Z(@bb'_GeS(ab'VebV'S=Uhh(ceb'_OU chV'S,SLeffTfaPSa`c SVr xp },y'tEffssy'{|wff}~wff{{|wffff,}wPs*ffffs'suP.E>T{EywfftGg~ w{wttGys'(x.^(zOff,,@ff j'ff@,ffjff@,|l,jff**@*,rO, ';Off,Off,,@ffffd(O,.,2@,|>;zOff,( ,, ^(zOff,;.|((j'ff'ff*,@,ff'*,|((`>;.Off,(>@>x @Ol^''ff,(,*ff Off,(j@2,;g;,@((^(Off,( ,L,ff @ff2lxOld,|>(O*Hn,,2@'@(|ffx@O||'(|rO,(;'lff','(||'(|TOT@|j .nffn(.n .dd(.d @j .(.d ff,. ,(Y. *>,'rH ff,r ff, ,'*|2,l^(Off,(z , @* '` j ff,L lz^',,'|.,gx@>j ( Ozlfiff ffff !"# fi%$& ff(' *),+.-#/ff, |';,L| ;,ff'@ff @.,(|T , s'{|wfftE's ,|';,L| 'L*.',',( 0ff,@,10,,>|@ffL'ff ,; (J*@j|j((2,(n, @@';rff,O(|,=, ((|T>|^@;',(|';2','(||'(|,z@,@'*O.,|'(|[O;(L;*zff|ff' 0,@,20,>|@ff3 5476 J,g,,.((| '*'((|';,,2@,@'O(|ffjO.,||'(|TO;',>||`@ff,j@,|', '@@,;,l1O(|ffjO.,||'(|rff,z@,|(|ff|ff' 98^ ; ; :,<=>= @T :??ff ^jff|ff>,ff||^, @,;,l` @.,;,O(|ff(l 0@,@0,,>|@ffA3 (z@|((O,,| Hn"@ ,(|r @.,(|Tj>||`'|,5BC8 @.,;, 6 DFEG >K ~}u~},*s~ Off,(*>|z@>( 0@,A0,>|@ffM3Hc_Ob'_UJc IX Off,(jL,'x| . ;(|,(|N.,@3.O PNRQ$ * ,'!BCQS6d|j, O.,||'(|@,;ffff,BCOT6@|ffzff |;ff.,|(2@@,@UVVVU WUa'_b'_U.c q st w u(s*y'{|wff}~s~jwffEtElsP~@sytt 3 Z 7[3u;s2w ~s't y'{|wff}~sP~Pxs XK ~}@u~},2sP~V UTUh ,;@',jff||^,(|((''ff(,z' ,,|'';:' L8 @* >|@^(\3j>,' |* ;(|,(|]N,@^32O PN7QN,@^ 3 PNQ(?ff >,' |* ;(|,(|]_`fiacbFdegf;h(hijh(hlk(bFd,mnh(fpoqhlr,bgos,fut,vbghlbgwxvRh(wzy{egw|hl}bFv~hlrwmry{egw|hl}bFvRxbgjf;oci#ki|hlrnfuksbFfukfuv,hsftv,bghlbgwvPwkfuegijhlbg|folm,dnolm#hlbgwvRbFvyegwjhl}bFv~;_du~i|ofri|f#o(fpfuvbFvPf;;hlbgwxvR,`,,fixfi\l^,MR|x|x]||n^%,R> .%(,M .%(# 7A];,(!jCC((!S|x,]n;l!>1j&7fiC^,A^lx.(>c>&(.ln^;xRM{^|^,;5|xx|x,l;(>cn((>jxz#|xxx|n,R,;^n,>,]|Mjn,nx5n 7*,;PJ,p>x,;\|5x,ncMF^ x^];MC>x,;|7Cx,RnTM7F^ F^%]|,,|R 7 ,>|;^AjMx]Jn q\7,;\n,#x|7|;&5,,x>x,cj\cM xC@nxnMAnC>\x, R^|n,;|n,]S;7>|Rx7xnfiff >x,Ac52;nx7xRqx,!#" % $&*'|#"P !(*),+9 -x'% $(),+9% -fixj 7^Cq|n.|zjC.nAc5.;nx5x\0/x]x 732xAjn >|n!|]x,x4!{x% ]c5n1|,|45,xx!,x;|nC\6n7% !T|x!\86nzn|,;x,xnnx9{MPn A,M^#n " xM|,!^]^:97] "$#|,;97] <5,M ,^|0% !*$R; >|n42n >=2 |>]9>,; \L]-@?(A+9 =|,J7^CB#@j]jxnM2c57#zAj ]7!1,2>@|%||C|,A|,;RRxR,Cnnx; %MnzRx#D!%|jR!^^xnx!,{],;lFER];xx x,|R; 1AC55 % !8(. -fixG' $8(. -fixHBz^nc|^xj|R\{,x|5||>I=fic|x,xn*!,$,5^,J9 !njxnx|Mn|x|n7AcMJn;nx5xS/^,,xncjPc5.n<;nx5x\.x,*5,7,,x (]|{c,nx!|7cMLK8Mx,^^,xC7n nx,nfi>x,;xL &N 'OPOOQ'NHRA\!x%xMnSffP U VWXZYT[8fiK\x,xC !,nxjR9#jnC,,|>;RCx,nPl(FT\.0)H\|,^x9{xx,,'OPOOQ'jZ^xnc>x,n,nxnq |] Cfi|1x ^,S7LZ_M `L,M Z_5W `Z_H(a)^ N @b OOPO b NR`Z_,(c)HN (COPOO(a)HNHR5 C],\jn9>2c5RnM|%x\M>x ()HN (>OPOO(0)HNHR7 'POOO'n( ^ (d)HN (>OOO(d)HNRjMnxAAc5Sn<2;nC>Mx7 Re(*)HN (fOOOg(*)HNHR |>xq5x ;n|c5Cx >P|#5,MA||nq,nx{|McMJn ;nx5xSxA|>ChHijlkmnfiompPo>qHr@mstunwvutyxnfiotyz|{Sp#}<{S~<mfik>mfivynfiotImlP r<vutysnfiotyz|{A|>x,;AxAj\xx,An FM* Cnx,; >\x%|Cx,\ F7]q|R;,xx*|] 7 ;,C5|xx,nF/ Mx,#xR|^ x!CC>zun6n5| A|7 |JnRxM4/Hxx^|RAn|\l|fi;g ;|#xZTwPFQIfifi.d| /Hl| V/3Q#ffg8M8TYT1 fiKMCA/71P*7 fiU .a/HP[&M M>*fi4g Z R /7fiA@Ua@lTTUGUPL8Q#cPPQXZTZa<HCgXTTfgQf8TQgg[fi*1QX.XT1TQ1>381QTglP@1;L P fiXTZ .>,& fiffXT X.Z< .ffTy;#V*1ff:g<aQ Q!C"#%$'&(*)+,.-/102(*)3y3u* 4,XTZX d8TQgg[.XTX#TT|#TggQgI8T Q Z T65Qg'@T[*0# 8TZZTXS 8TZTQT7@Tg1 ZQI<Q 86 9|8.QQ:3u>XTg 8*#Z lQQ#QgZXXZ84XT@ 8TgQQTfHT; 5y 8TgQQT.Q<TgQgT@T TQQ,XV gTT4ZZXTQ*Q 8ZQQP< @T*TQTZ<TH; QQgQgH##gZ=,> Tg?8@ T<Tg# TTlTBTQTlg#XT 8gQQT0T1T; 5y 8TgQQTd gQZTQggQg#TggQg> T*QQ@ITQf4TggI8XyTQg08P,>Tgd#; CZ 8 Tggg0XTdXD 5yE 9F*a6 9|8#TQgAHG. XT0T| TgTAQfXTgQc1Tg>XX TXTQgQgT 8glQ4Q TQIQ #TggQgg1QQggQ% TQg gg@#T1QX> CCTTICTT&8. 8QPG.XT.XTT|*48Q.yXT.X; T.@yIHXT< 8TgQQTfgQHTQgTT<TX TXTQgZg<Z*QXQ <8g#TggQgT8g#TggQgg<XQZZ4QXQ <#Q TgTJX TXTQgg4TgQgT@PT@ 8gQPJfi*gg,#TgQg*.0TgdEXT 8gQQTZ<g4TgQgT.0XTZQTZ7yTQg; 5yZ 8TQK.TLwgQ @#TggQggQH8TQggQgT7TTH 8gQH#yTQg; 5y@T3yZQQg7T<THX|**TH<g<g% 37STTQ*Q.TQ@M.QXT@TQgSXT0@Q .#TggQgfigQ*TQgTT4Tg 8gQ '8#gZgQd% G.XTXTT|@1*d@8#QAQ8Q0 g TgQgTdg@T 8gQ**>#Q0Q#XT.*> CQ*TQgg0NH TX#TQg+>P%)(RQI,.SUT%V<SUWSX)ZY>/lf*Tg g[CfQ1X8C=4,Q3%QX5.K4ATgQ4TXT4SHXTS#QP4TgT>Q## 18.XT1G\S^]_SX`;SX)+SU/K.^a#Lcbde>e>f>gh*QT0QgfigQTT8gXZ*3u'ilTgQTjblkH|mgRn_oXp;qsrutX>Zn { w{ Uf>>f;E>fK Zg<4 QPvxwzy{:|*}~sw rfi }~ d> w ~ ^HTfi 5[* @ 5bde>>;g=hEK44<'a.g}> w r |*}~^w r'>oUp7{ r^>o w r_ {D} { }>yw ~a.I@P7H@*QTTTTibde>e>>gK XTl4TX@gQEH3y } r D{ { p w ~>}dv {7E>v n_o vx{ _oU vxw} oX j}>w v } { { {} oI vxw r w n_o vx{w6~s{ oXr {= n <nt>>h>>E>'.8%>fi[>^c>XUHfi; shX@d>>>^_=^^;^[=^ E sh_>D;xz>dDD>*xx*x>>c;>>fffi^>[ ^> >_>.[^ c *d>>*fi^E s7 ;^?^ ^ E^h^_U>*dfiDx!>U_Ux6sXD#"uD>dff$s&%>('*)>*^' >^[fi3 E 4 ^5 ^:[ >^s= =s^6E sh+ ,E>x.-fiZd>>;0/1^E2 fifi< E=^ >>?^ x>>xzA?@ >xd>xz;X.B%>x fi*_CsddD_67>>;d78$s:9&;E>xu:zAE>8$^>xXh<;c;>Fd [fi>3 2 G%B [U3H IFK>J fiL: NMOPJ > ;xh d>^' %_K7^ x8Q *sfidsd>>SRUhT Dffs$ U=VIs u?>K70W<c< _Xxz>R.[MZ *4, s^*- Ffi\/ cd>) >&] >1c< xXu_[T^^$ _>d ;N=7 : >1<_s xdsd>>1` zK>7 z;aT^s$ bH >Ffi>dfi@ D=Q *c %d>8Q d?Ee z>*fh =; ^ N ^> 4=.\ E g * ^ RShff>H *fiXj fik Xd;>>l*` K>7 >x>d*^zsd>z;x^s^' Fd [fi>3 2 G%B [X3h [h%En^ ;6>x H >>>u> ^ X>^ [4 fi^ a^ * ;. ;^I[=^ E[FE, ;xfUj [.Fd s^ 7 dD_6>7 >; d8s$ >%;% 7>XN^ >pofofo]9&1; 1< E8^ >qj` K>7 >x>Id>1<_^ x?h9 U_shXXE' >) % DF4X3tMuY >&k @ >>>u#:v Xw u7 s[ 6V^ [zx>>K7:E X*d rU+ [&ksd sx*h[ + i> &Uj :ffs$ XjD>DX>FR?T^s$ D>6;x.E 8s$ IE7 :>K7dy<h< D>xz;<E fd H s^shfi3>3 sh>hX >>>u ^X^; >3 Fd 3fi >=[fiX3 fi3;3 sh>hXC%Y *6>DN7^ x8Q M*sd^;>^*%B ; U>ffisc9 _hc>>E =f?- E H *d>^' ^X^; fi>3 :Fd 3* ;=[fi3N{|/1^ *4=>^7 ^ ^fi3>3 sh^h[*Mzfi^ DX>* fijsd^;>^~}E N>) >1S>) >\ %d>;>fY>fi[^X^ >3 =Fd 3fi > fi3>3 shhhfi3>3 shhh[NMX fiX3 &fiC%*muXDN7^ x8Q *^zd^;>^*%B ; h>Xc9 Dhh>>>;fd\ .=d>s' ^gz ._ _F E X?6c ^ ;^ skd ^xhfi3>3 shh[.M %X>3 >ffiC%*mu>_D6>7 z;>%d 8s$ 8$ _UxDX>x>X;t7>p 6s$ 6I< >_N7^ x8Q M*^zdsd>>fi I> %B >xX;>?yE@c9 Cfi>7 Xh*I) DcX'0h EaMZ s^6h ^hc n Ix F^:E> _fiX3^ =Er=L ^ fi/ >>>u_^FHX>3 ^ E DE sH[ >3 F 3* > ^ =>hy=CY *mdD_6>7 >;d:8s$ U>% 472o^ d6< D>7>iE z;dh9 D>> *_>DX> o0PX9 ^6 ^hhD_>.[^ ><d;>>UUd> [>x>: >1 h = ^>^s>LMF^fi^d\^fi3Xhh Z[*M ?k;F-fi*d>)>t/^fi_^D=^ s' ^* ['[^X^>3>Fd 3* ;=[fi3N{thE>^lE [E 2fi-ME H cCY*m#7Q;XDz'DN7^x8QM*sd^;>^hU);fi) fi X>7 G ffiNNpsFFF8ffF=FNFFFF*p>fifirF=bF=FN.6r&*dSbfiiy*2rfiNfiffi#fiii*&lb*8N1 NCI=g*K I*FI_(.6r&FddN4IFbS*C2C8#I8*X4 KfiC8# bnIIN6Ff&d fiffpISFr*FF4fifi4p4.6r&tdKd.4FIFffGF4 fi?fip[*rfiNfiffidfipXi*&dfffi=K2 NCI=g*IN6tFn_Nfirfir&.N^?r|24fiG*fiff^?tf4ffUNCUFpi4(*4dyiXffFC.^C.tb 46 68W8* 8y*4 Fffff4K #Ab*=Kff6N8fi..CNUIC*4 ?Cb[=:?6C y8K NCI=g*IN6*F&KISF FF4rfi .4rNyffFfiFNf?4ff!^ifi *pdi*Fifiii2dfis fiaFiX:p ififf F4PXi*2*p0firIffffi=K[6NC8fiK INt _Fy*fiff Cf?bd# "fiifii*fi2r4i*sXW4ffi fipfiFfiff p: fiff*4ff*dfiffi*&G fiffi*&$ &Ctff.6NfiC8%~p*8y4i*4228*& .Cf( ':)f* ,+_I 8ffff.F&IS(#F204ffiFFfir4NF1. -2t4/ '(ff&GfiNfiffiU C4fiff?^ fiff*4ff*ffptyffinP4Ffi4?F( -ffi4p4>F0lN.^.?^Nr|211F*fifi4fifiK3 & 5 4U4X*ffa6 "*iififfafi fi*fi7 "fiiififf:ff*dfiffi?4 8_fi2*Fpi*4I_f*NfiNr*z??ffF 0.tCFff yI 66=48*9ff8:fi; S6fbnIIN6b*8NffK=<fi=ff.!&>@?*X . fi/ '?&IC*4Cb[=P?6C y8& KC8g*IN6F&NISF. FF4rfi .4rNACBCAfiJournal Artificial Intelligence Research 4 (1996) 1-18Submitted 9/95; published 1/96Design Experimental Analysis AlgorithmsTemporal ReasoningPeter van BeekDennis W. ManchakDepartment Computing Science, University AlbertaEdmonton, Alberta, Canada T6G 2H1vanbeek@cs.ualberta.cadmanchak@vnet.ibm.comAbstractMany applications|from planning scheduling problems molecular biology|rely heavily temporal reasoning component. paper, discuss designempirical analysis algorithms temporal reasoning system based Allen's uentialinterval-based framework representing temporal information. core systemalgorithms determining whether temporal information consistent, and, so,finding one scenarios consistent temporal information. Twoimportant algorithms tasks path consistency algorithm backtrackingalgorithm. path consistency algorithm, develop techniques resultten-fold speedup already highly optimized implementation.backtracking algorithm, develop variable value ordering heuristics shownempirically dramatically improve performance algorithm. well, showpreviously suggested reformulation backtracking search problem reducetime space requirements backtracking search. Taken together, techniquesdevelop allow temporal reasoning component solve problems practicalsize.1. IntroductionTemporal reasoning essential part many artificial intelligence tasks. desirable,therefore, develop temporal reasoning component useful across applications.applications, planning scheduling, rely heavily temporal reasoning component success application depend eciencyunderlying temporal reasoning component. paper, discuss design empirical analysis two algorithms temporal reasoning system based Allen's (1983)uential interval-based framework representing temporal information. two algorithms, path consistency algorithm backtracking algorithm, important twofundamental tasks: determining whether temporal information consistent, and, so,finding one scenarios consistent temporal information.stress designing algorithms robust ecient practice.path consistency algorithm, develop techniques result ten-foldspeedup already highly optimized implementation. backtracking algorithm,develop variable value ordering heuristics shown empirically dramaticallyimprove performance algorithm. well, show previously suggestedreformulation backtracking search problem (van Beek, 1992) reduce timespace requirements backtracking search. Taken together, techniques developc 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fivan Beek & ManchakRelationSymbol Inversexbbix meetsmix overlapsoix startssixdix finishesffix equaleqeqMeaningxxxxxxxFigure 1: Basic relations intervalsallow temporal reasoning component solve problems realistic size. partevidence support claim, evaluate techniques improving algorithmslarge problem arises molecular biology.2. Representing Temporal Informationsection, review Allen's (1983) framework representing relations intervals. discuss set problems chosen test algorithms.2.1 Allen's frameworkthirteen basic relations hold two intervals (see Figure 1; Allen,1983; Bruce, 1972). order represent indefinite information, relation twointervals allowed disjunction basic relations. Sets used listdisjunctions. example, relation fm,o,sg events B representsdisjunction, (A meets B) _ (A overlaps B) _ (A starts B): Let set basicrelations, fb,bi,m,mi,o,oi,s,si,d,di,f,fi,eqg. Allen allows relation two eventssubset .use graphical notation vertices represent events directed edgeslabeled sets basic relations. graphical convention, never show edges(i; i), show edge (i; j ), show edge (j; i). edgeexplicit knowledge relation labeled ; convention edgesalso shown. call networks labels arbitrary subsets , intervalalgebra IA networks.Example 1. Allen Koomen (1983) show IA networks used non-linearplanning concurrent actions. example representing temporal information usingIA networks, consider following blocks-world planning problem. three blocks,A, B, C. initial state, three blocks table. goal state2fiAlgorithms Temporal Reasoningsimply tower blocks B B C. associate states, actions,properties intervals hold over, immediately writefollowing temporal information.Initial ConditionsGoal ConditionsInitial fdg Clear(A)Goal fdg On(A,B)Initial fdg Clear(B)Goal fdg On(B,C)Initial fdg Clear(C)action called \Stack". effect stack action On(x; ): block xtop block . action successfully executed, conditions Clear(x)Clear(y ) must hold: neither block x block block them. Planning introducestwo stacking actions following temporal constraints.Stacking ActionStacking ActionStack(A,B) fbi,mig InitialStack(B,C) fbi,mig InitialStack(A,B) fdg Clear(A)Stack(B,C) fdg Clear(B)Stack(A,B) ffg Clear(B)Stack(B,C) ffg Clear(C)Stack(A,B) fmg On(A,B)Stack(B,C) fmg On(B,C)graphical representation IA network planning problem shownFigure 2a. Two fundamental tasks determining whether temporal informationconsistent, and, so, finding one scenarios consistent temporalinformation. IA network consistent exists mappingreal interval (u) event vertex u network relations events satisfied (i.e., one disjuncts satisfied). example, considersmall subnetwork Figure 2a consisting events On(A,B), On(B,C), Goal.subnetwork consistent demonstrated assignment, (On(A,B)) = [1; 5],(On(B,C)) = [2; 5], (Goal) = [3; 4]. change subnetwork insistOn(A,B) must On(B,C), mapping would exist subnetworkwould inconsistent. consistent scenario IA network non-disjunctive subnetwork (i.e., every edge labeled single basic relation) consistent.planning example, finding consistent scenario network corresponds findingordering actions accomplish goal stacking three blocks. Oneconsistent scenario reconstructed qualitative mapping shown Figure 2b.Example 2. Golumbic Shamir (1993) discuss IA networks usedproblem molecular biology: examining structure DNA organism (Benzer, 1959). intervals IA network represent segments DNA. Experimentsperformed determine whether pair segments either disjoint intersects.Thus, IA networks result contain edges labeled disjoint (fb,big), intersects(fm,mi,o,oi,s,si,d,di,f,fi,eqg), , set basic relations|which indicates experiment performed. IA network consistent, evidence hypothesisDNA linear structure; inconsistent, DNA nonlinear (it forms loops,example). Golumbic Shamir (1993) show determining consistency restricted version IA networks NP-complete. show problems ariseapplication often solved quickly practice.3fivan Beek & Manchak@@@=@R1PP -HYHHPHPPPPPq J]JJ ,HHHHHJJ ,,Z}ZZJ ,HHH ZHH(a) IA network block-stacking example:fbi,migfdgfdg1Initialfdgfbi,migfdg2Clear(A)3Clear(B)4Clear(C)5Stack(A,B)ffgfdgffgfmg7On(A,B)8On(B,C)fmgPiP)PPfdgfdg9Goal6Stack(B,C)(b) Consistent scenario:InitialStack(B,C)GoalStack(A,B)Clear(C)On(B,C)Clear(B)On(A,B)Clear(A)Figure 2: Representing qualitative relations intervals2.2 Test problemstested well heuristics developed improving path consistency backtracking algorithms perform test suite problems.purpose empirically testing algorithms determine performancealgorithms proposed improvements \typical" problems. twoapproaches: (i) collect set \benchmark" problems representative problemsarise practice, (ii) randomly generate problems \investigate algorithmicperformance depends problem characteristics ... learn predict algorithmperform given problem class" (Hooker, 1994).IA networks, existing collection large benchmark problems actuallyarise practice|as opposed to, example, planning toy domain blocksworld. start collection, propose IA network 145 intervals aroseproblem molecular biology (Benzer, 1959, pp. 1614-15; see Example 2, above).proposed benchmark problem strictly speaking temporal reasoning problem4fiAlgorithms Temporal Reasoningintervals represent segments DNA, intervals time. Nevertheless,formulated temporal reasoning problem. value benchmark problemarose real application. refer problem Benzer's matrix.addition benchmark problem, paper use two models random IAnetwork, denoted B(n) S(n; p), evaluate performance algorithms, nnumber intervals, p probability (non-trivial) constraint twointervals. Model B(n) intended model problems arise molecular biology (asestimated problem discussed Benzer, 1959). Model S(n; p) allows us studyalgorithm performance depends important problem characteristic sparsenessunderlying constraint graph. models, course, allow us study algorithmperformance depends size problem.B(n), random instances generated follows.Step 1. Generate \solution" size n follows. Generate n real intervals randomlygenerating values end points intervals. Determine IA network determining, pair intervals, whether two intervals either intersect disjoint.Step 2. Change constraints edges trivial constraint settinglabel , set 13 basic relations. represents case experimentperformed determine whether pair DNA segments intersect disjoint.Constraints changed percentage non-trivial constraints (approximately6% intersects 17% disjoint) distribution graph similarBenzer's matrix.S(n; p), random instances generated follows.Step 1. Generate underlying constraint graph indicating possible (n2)edges present. Let edge present probability p, independently presenceabsence edges.Step 2. edge occurs underlying constraint graph, randomly chose labeledge set possible labels (excluding empty label) labelchosen equal probability. edge occur, label edge , set13 basic relations.Step 3. Generate \solution" size n follows. Generate n real intervals randomlygenerating values end points intervals. Determine consistent scenariodetermining basic relations satisfied intervals. Finally, add solutionIA network generated Steps 1{2.Hence, consistent IA networks generated S(n; p). omit Step 3,shown analytically empirically almost different possibleIA networks generated distribution inconsistent inconsistencyeasily detected path consistency algorithm. avoid potential pitfall, testalgorithms consistent instances problem. method appears generatereasonable test set temporal reasoning algorithms problems range easyhard. found, example, instances drawn S(n; 1=4) hard problemsbacktracking algorithms solve, whereas values p either side (S(n; 1=2)S(n; 1=8)) problems easier.5fivan Beek & Manchak3. Path Consistency AlgorithmPath consistency transitive closure algorithms (Aho, Hopcroft, & Ullman, 1974; Mackworth, 1977; Montanari, 1974) important temporal reasoning. Allen (1983) showspath consistency algorithm used heuristic test whether IA networkconsistent (sometimes algorithm report information consistentreally not). path consistency algorithm useful also backtracking searchconsistent scenario used preprocessing algorithm (Mackworth, 1977;Ladkin & Reinefeld, 1992) algorithm interleaved backtracking search (see next section; Nadel, 1989; Ladkin & Reinefeld, 1992). section,examine methods speeding path consistency algorithm.idea behind path consistency algorithm following. Choose threevertices i, j , k network. labels edges (i; j ) (j; k) potentiallyconstrain label edge (i; k) completes triangle. example, considerthree vertices Stack(A,B), On(A,B), Goal Figure 2a. Stack(A,B) fmgOn(A,B) On(A,B) fdig Goal deduce Stack(A,B) fbg Goal thereforechange label edge , set basic relations, singletonset fbg. perform deduction, algorithm uses operations set intersection(\) composition () labels checks whether C = C \ C C , Clabel edge (i; k). C updated, may constrain labels, (i; k)added list processed turn, provided edge already list.algorithm iterates changes possible. unary operation, inverse,also used algorithm. inverse label inverse elements(see Figure 1 inverses basic relations).designed experimentally evaluated techniques improving eciencypath consistency algorithm. starting point variation Allen's (1983) algorithmshown Figure 3. implementation algorithm ecient, intersectioncomposition operations labels must ecient (Steps 5 & 10). Intersectionmade ecient implementing labels bit vectors. intersection two labelssimply logical two integers. Composition harder make ecient.Unfortunately, impractical implement composition two labels using tablelookup table would need size 213 213, 213 possible labels.experimentally compared two practical methods compositionproposed literature. Allen (1983) gives method composition uses tablesize 13 13. table gives composition basic relations (see Allen, 1983,table). composition two labels computed nested loop formsunion pairwise composition basic relations labels. Hogge (1987) givesmethod composition uses four tables size 27 27, 27 26, 26 27, 26 26.composition two labels computed taking union results four arrayreferences (H. Kautz independently devised similar scheme). experiments,implementations two methods differed composition computed.both, list, L, edges processed implemented using first-in, first-out policy(i.e., stack).also experimentally evaluated methods reducing number composition operations need performed. One idea examined improving eciencyikik6ikijjkikfiAlgorithms Temporal Reasoning(C; n)Path-Consistency1. L f(i; j ) j 1 < j ng2. (L empty)3. select delete (i; j ) L4.k 1 n, k 6= k 6= j5.C \ C C6.(t 6= C )7.C8.CInverse(t)9.LL [ f(i; k )g10.C\CC11.(t 6= C )12.C13.CInverse(t)14.LL [ f(k; j )gikijjkkiijikikkikjkjkjjkFigure 3: Path consistency algorithm IA networksavoid computation predicted result constrainlabel edge completes triangle. Three cases identified shownFigure 4. Another idea examined, first suggested Mackworth (1977, p. 113),order edges processed affect eciency algorithm.reason following. edge appear list, L, edges processedmany times progressively gets constrained. number times particular edgeappears list reduced good ordering. example, consider edges(3; 1) (3; 5) Figure 2a. process edge (3; 1) first, edge (3; 2) updatedfo,oi,s,si,d,di,f,fi,eqg added L (k = 2 Steps 5{9). process edge(3; 5), edge (3; 2) updated fo,s,dg added L second time. However,process edge (3; 5) first, (3; 2) immediately updated fo,s,dgadded L once.Three heuristics devised ordering edges shown Figure 9. edgesassigned heuristic value processed ascending order. new edgeadded list (Steps 9 & 14), edge inserted appropriate spot accordingnew heuristic value. little work ordering heuristics path consistencyalgorithms. Wallace Freuder (1992) discuss ordering heuristics arc consistencyalgorithms, closely related path consistency algorithms. Two heuristicscannot applied context heuristics assume constraint satisfaction problemfinite domains, whereas IA networks examples constraint satisfaction problemsinfinite domains. third heuristic (due B. Nudel, 1983) closely correspondscardinality heuristic.experiments performed Sun 4/25 12 megabytes memory.report timings rather measure number iterations believegives accurate picture whether results practical interest. Care7fivan Beek & Manchakcomputation, C \ C C , skipped known resultcomposition constrain label edge (i; k):a. either C C equal , result composition thereforeconstrain label edge (i; k). Thus, Step 1 Figure 3, edgeslabeled added list edges process.b. condition,ikijijjkjk(b 2 Cij ^bi 2 C ) _ (bi 2 Cij ^jkb 2 C ) _ (d 2 Cjkij ^di 2 C );jktrue, result composing C C . condition quickly testedusing bit operations. Thus, condition true Step 5, Steps 5{9skipped. similar condition formulated tested Step 10.c. point computation C C determined resultaccumulated far would constrain label C , rest computationskipped.ijjkijjkikFigure 4: Skipping techniquestaken always start base implementation algorithm addenough code implement composition method, new technique, heuristicevaluating. well, every attempt made implement method heuristiceciently could.Given implementations, Hogge's method composition foundecient Allen's method benchmark problem random instances(see Figures 5{8). much surprising. However, addition skippingtechniques, two methods became close eciency. skipping techniques sometimesdramatically improved eciency methods. ordering heuristics improveeciency, although results less dramatic. cardinality heuristicconstraintedness heuristic also tried ordering edges. foundcardinality heuristic costly compute weight heuristicperform it. constraintedness heuristic reduced number iterations provedcostly compute. illustrates balance must struck effectivenessheuristic additional overhead heuristic introduces.S(n; p), skipping techniques weight ordering heuristic together resultten-fold speedup already highly optimized implementation using Hogge'smethod composition. largest improvements eciency occur IA networkssparse (p smaller). encouraging appears problems ariseplanning molecular biology also sparse. B(n) Benzer's matrix, speedupapproximately four-fold. Perhaps importantly, execution times reported indicatepath consistency algorithm, even though O(n3 ) algorithm, usedpractical-sized problems. Figure 8, show well algorithms scale up.8fiAlgorithms Temporal ReasoningAllen137.7Hogge10.3Allen+skip5.7Hogge+skip4.0Hogge+skip+weight2.7Figure 5: Effect heuristics time (sec.) path consistency algorithms appliedBenzer's matrixtime (sec.)100101AllenHoggeAllen+skipHogge+skipHogge+skip+weight0.15075100n125150Figure 6: Effect heuristics average time (sec.) path consistency algorithms.data point average 100 tests random instances IA networks drawnB(n); coecient variation (standard deviation / average) set100 tests bounded 0.20seen algorithm includes weight ordering heuristic performs others.However, algorithm requires much space largest problem able solve500 intervals. algorithms included skipping techniquesable solve much larger problems running space (up 1500 intervals)constraint time took solve problems.9fivan Beek & Manchak100time (sec.)101AllenHoggeAllen+skipHogge+skipHogge+skip+weight0.11/81/41/2p3/41Figure 7: Effect heuristics average time (sec.) path consistency algorithms.data point average 100 tests random instances IA networks drawnS(100; p); coecient variation (standard deviation / average)set 100 tests bounded 0.2590008000S(n,1/4):Allen+skipHogge+skipHogge+skip+weight7000time (sec.)6000B(n):Allen+skipHogge+skipHogge+skip+weight500040003000200010000100200300400500600n7008009001000Figure 8: Effect heuristics average time (sec.) path consistency algorithms.data point average 10 tests random instances IA networks drawnS(n; 1=4) B(n); coecient variation (standard deviation / average) set 10 tests bounded 0.3510fiAlgorithms Temporal Reasoning4. Backtracking AlgorithmAllen (1983) first propose backtracking algorithm (Golomb & Baumert,1965) could used find consistent scenario IA network. worst case,backtracking algorithm take exponential amount time complete. worstcase also applies Vilain Kautz (1986, 1989) show finding consistentscenario NP-complete IA networks. spite worst case estimate, backtrackingalgorithms work well practice. section, examine methods speedingbacktracking algorithm finding consistent scenario present results wellalgorithm performs different classes problems. particular, compare eciencyalgorithm two alternative formulations problem: one previouslyproposed others one proposed (van Beek, 1992). also improveeciency algorithm designing heuristics ordering instantiationvariables ordering values domains variables.starting point, modeled backtracking algorithm LadkinReinefeld (1992) results experimentation suggests successfulfinding consistent scenarios quickly. Following Ladkin Reinefeld algorithmfollowing characteristics: preprocessing using path consistency algorithm, static orderinstantiation variables, chronological backtracking, forward checking pruningusing path consistency algorithm. chronological backtracking, search reachesdead end, search simply backs next recently instantiated variabletries different instantiation. Forward checking (Haralick & Elliott, 1980) techniquedetermined recorded instantiation current variable restrictspossible instantiations future variables. technique viewed hybridtree search consistency algorithms (see Nadel, 1989; Nudel, 1983). (See Dechter, 1992,general survey backtracking.)4.1 Alternative formulationsLet C matrix representation IA network, C label edge (i; j ).traditional method finding consistent scenario IA network searchsubnetwork network C that,(a) C ,(b) jS j = 1, i; j ,(c) consistent.find consistent scenario simply search different possible 's satisfyconditions (a) (b)|it simple matter enumerate them|until find onealso satisfies condition (c). Allen (1983) first propose using backtracking searchsearch potential 's.alternative formulation based results two restricted classes IA networks,denoted SA networks NB networks. IA networks, relation twointervals subset , set thirteen basic relations. SA networks(Vilain & Kautz, 1986), allowed relations two intervals subsetstranslated, using relations f<, , =, >, , 6=g, conjunctionsijijijij11fivan Beek & Manchakrelations endpoints intervals. example, IA network Figure 2aalso SA network. specific example, interval relation \A fbi,mig B"expressed conjunction point relations, (B, < B+ ) ^ (A, < A+ ) ^ (A, B+ );A, A+ represent start end points interval A, respectively. (See Ladkin& Maddux, 1988; van Beek & Cohen, 1990, enumeration allowed relationsSA networks.) NB networks (Nebel & Burckert, 1995), allowed relationstwo intervals subsets translated, using relations f<,, =, >, , 6=g, conjunctions Horn clauses express relationsendpoints intervals. set NB relations strict superset SA relations.alternative formulation follows. describe method terms SAnetworks, method applies NB networks. idea that, rathersearch directly consistent scenario IA network previous work, firstsearch something general: consistent SA subnetwork IA network.is, use backtrack search find subnetwork network C that,(a)Sij Cij(b)Sij(c),allowed relation SA networks, i; j ,consistent.previous work, search alternative singleton labelings edge, i.e.,jS j = 1. key idea proposal decompose labels largestpossible sets basic relations allowed SA networks searchdecompositions. considerably reduce size search space. example,suppose label edge fb,bi,m,o,oi,sig. six possible ways labeledge singleton label: fbg, fbig, fmg, fog, foig, fsig, two possible wayslabel edge decompose labels largest possible sets basic relationsallowed SA networks: fb,m,og fbi,oi,sig. another example, considernetwork shown Figure 2a. searching alternative singleton labelings,worst case size search space C12 C13 C89 = 314 (the edges labeledmust included calculation). decomposing labels largestpossible sets basic relations allowed SA networks searchingdecompositions, size search space 1, backtracking necessary (in general,search is, course, always backtrack free).test whether instantiation variable consistent instantiations pastvariables possible instantiations future variables, use incremental pathconsistency algorithm (in Step 1 Figure 3 instead initializing L edges,initialized single edge changed). result backtracking algorithmconsistent SA subnetwork IA network, report IA network inconsistent.backtracking completes, solution SA network found using fastalgorithm given van Beek (1992).ij4.2 Ordering heuristicsBacktracking proceeds progressively instantiating variables. consistent instantiationexists current variable, search backs up. order variables12fiAlgorithms Temporal ReasoningWeight. weight heuristic estimate much label edge restrictlabels edges. Restrictiveness measured basic relation successively composing basic relation every possible label summing cardinalitiesresults. results suitably scaled give table shown below.relation b bi mi oi si di f fi eqweight 3 3 2 2 4 4 2 2 4 3 2 2 1weight label sum weights elements. example, weightrelation fm,o,sg 2 + 4 + 2 = 8.Cardinality. cardinality heuristic variation weight heuristic. Here,weight every basic relation set one.Constraint. constraintedness heuristic estimate much change labeledge restrict labels edges. determined follows. Supposeedge interested (i; j ). constraintedness label edge (i; j )sum weights labels edges (k; i) (j; k), k = 1; :::; n; k 6= i; k 6= j .intuition comes examining path consistency algorithm (Figure 3) wouldpropagate change label C . see C composed C (Step 5)C (Step 10), k = 1; :::; n; k 6= i; k 6= j .ijijkijkFigure 9: Ordering heuristicsinstantiated order values domains tried possibleinstantiations greatly affect performance backtracking algorithm variousmethods ordering variables (e.g. Bitner & Reingold, 1975; Freuder, 1982; Nudel,1983) ordering values (e.g. Dechter & Pearl, 1988; Ginsberg et al., 1990; Haralick& Elliott, 1980) proposed.idea behind variable ordering heuristics instantiate variables firstconstrain instantiation variables most. is, backtracking searchattempts solve highly constrained part network first. Three heuristicsdevised ordering variables (edges IA network) shown Figure 9.alternative formulation, cardinality redefined count decompositions ratherelements label. variables put ascending order. experimentsordering static|it determined backtracking search startschange search progresses. context, cardinality heuristic similarheuristic proposed Bitner Reingold (1975) studied Purdom (1983).idea behind value ordering heuristics order values domainsvariables values likely lead solution tried first. Generally,done putting values first constrain choices variables least.propose novel technique value ordering based knowledge structuresolutions. idea first choose small set problems class problems,find consistent scenario instance without using value ordering.set solutions, examine solutions determine values domains13fivan Beek & Manchak120100SISAtime (sec.)80604020050100150n200250Figure 10: Effect decomposition method average time (sec.) backtracking algorithm. data point average 100 tests random instances IAnetworks drawn B(n); coecient variation (standard deviation /average) set 100 tests bounded 0.15likely appear solution values least likely. informationused order values subsequent searches solutions problemsclass problems. example, five problems generated using model S(100; 1=4)consistent scenarios found using backtracking search variable orderingheuristic constraintedness/weight/cardinality. rounding two significant digits,relations occurred solutions following frequency,relationb, bi d, di o, oivalue (10) 1900 240 220eq53m, mi f, fi s, si2015 14example using information order values domain, supposelabel edge fb,bi,m,o,oi,sig. decomposing labels singleton labels,would order values domain follows (most preferred first): fbg, fbig, fog,foig, fmg, fsig. decomposing labels largest possible sets basicrelations allowed SA networks, would order values domainfollows: fb,m,og, fbi,oi,sig, since 1900 + 20 + 220 > 1900 + 220 + 14. techniqueused whenever something known structure solutions.4.3 Experimentsexperiments performed Sun 4/20 8 megabytes memory.first set experiments, summarized Figure 10, examined effect problemformulation execution time backtracking algorithm. implemented three14fiAlgorithms Temporal Reasoning10000Random value ordering, RandomHeuristic value ordering, randomRandom value ordering, best heuristicHeuristic value ordering, best heuristicvariablevariablevariablevariableorderingorderingorderingorderingtime10001001001020304050test60708090100Figure 11: Effect variable value ordering heuristics time (sec.) backtrackingalgorithm. curve represents 100 tests random instances IA networksdrawn S(100; 1=4) tests ordered time taken solveinstance. backtracking algorithm used SA decomposition method.versions algorithm identical except one searched singletonlabelings (denoted hereafter Figure 10 SI method) two searcheddecompositions labels largest possible allowed relations SA networks NB networks, respectively. methods solved set randomproblems drawn B(n) also applied Benzer's matrix (denoted +Figure 10). problem, amount time required solve given IA network recorded. mentioned earlier, IA network preprocessed pathconsistency algorithm backtracking search. timings include preprocessingtime. experiments indicate speedup using SA decomposition methodthree-fold SI method. well, SA decomposition methodable solve larger problems running space (n = 250 versus n = 175).NB decomposition method gives exactly result SA methodproblems structure constraints. also tested three methodsset random problems drawn S(100; p), p = 1; 3=4; 1=2, 1=8.experiments, SA NB methods consistently twice fast SI method.well, NB method showed advantage SA method problems.surprising branching factor, hence size search space, smallerNB method SA method.second set experiments, summarized Figure 11, examined effectexecution time backtracking algorithm heuristically ordering variablesvalues domains variables backtracking search begins. variableordering, six permutations cardinality, constraint, weight heuristics tried15fivan Beek & Manchakprimary, secondary, tertiary sorting keys, respectively. basis comparison,experiments included case heuristics. Figure 11 shows approximate cumulativefrequency curves experimental results. Thus, example, readcurve representing heuristic value ordering best heuristic variable orderingapproximately 75% tests completed within 20 seconds, whereas random valuevariable ordering approximately 5% tests completed within 20 seconds.also read curves 0, 10, : : : , 100 percentiles data sets (wherevalue median 50th percentile value 50th test). curvestruncated time = 1800 (1/2 hour), backtracking search abortedtime limit exceeded.experiments found S(100; 1=4) represents particularly dicult classproblems different heuristics resulted dramatically different performance, heuristic case also different heuristics.value ordering, best heuristic variable ordering combination constraintedness/weight/cardinality constraintedness primary sorting keyremaining keys used break subsequent ties. Somewhat surprisingly, best heuristicvariable ordering changes heuristic value ordering incorporated. combination weight/constraintedness/cardinality works much better. heuristic togethervalue ordering particularly effective \ attening out" distribution allowing much greater number problems solved reasonable amount time.S(100; p), p = 1; 3=4; 1=2, 1=8, problems much easier threehundreds tests completed within 20 seconds. problems, heuristic usedresult significantly different performance.summary, experiments indicate changing decomposition methodable solve larger problems running space (n = 250 vs n = 175machine 8 megabytes; see Figure 10). experiments also indicate good heuristicorderings essential able find consistent scenario IA networkreasonable time. good heuristic ordering able solve much larger problemsrunning time (see Figure 11). experiments also provide additional evidenceecacy Ladkin Reinefeld's (1992, 1993) algorithm. Nevertheless, evenimprovements, problems still took considerable amount time solve.consideration, surprising. all, problem known NP-complete.5. ConclusionsTemporal reasoning essential part tasks planning scheduling. paper, discussed design empirical analysis two key algorithms temporalreasoning system. algorithms path consistency algorithm backtracking algorithm. temporal reasoning system based Allen's (1983) interval-based frameworkrepresenting temporal information. emphasis make algorithmsrobust ecient practice problems vary easy hard. path consistency algorithm, bottleneck performing composition operation. developedmethods reducing number composition operations need performed.methods result almost order magnitude speedup already highlyoptimized implementation algorithm. backtracking algorithm, developed16fiAlgorithms Temporal Reasoningvariable value ordering heuristics showed alternative formulationproblem considerably reduce time taken find solution. techniques allowinterval-based temporal reasoning system applied larger problems performeciently existing applications.ReferencesAho, A. V., Hopcroft, J. E., & Ullman, J. D. (1974). Design Analysis ComputerAlgorithms. Addison-Wesley.Allen, J. F. (1983). Maintaining knowledge temporal intervals. Comm. ACM, 26,832{843.Allen, J. F., & Koomen, J. A. (1983). Planning using temporal world model. ProceedingsEighth International Joint Conference Artificial Intelligence, pp. 741{747Karlsruhe, West Germany.Benzer, S. (1959). topology genetic fine structure. Proc. Nat. Acad. Sci. USA,45, 1607{1620.Bitner, J. R., & Reingold, E. M. (1975). Backtrack programming techniques. Comm. ACM,18, 651{655.Bruce, B. C. (1972). model temporal references application questionanswering program. Artificial Intelligence, 3, 1{25.Dechter, R. (1992). local global consistency. Artificial Intelligence, 55, 87{107.Dechter, R., & Pearl, J. (1988). Network-based heuristics constraint satisfaction problems. Artificial Intelligence, 34, 1{38.Freuder, E. C. (1982). sucient condition backtrack-free search. J. ACM, 29, 24{32.Ginsberg, M. L., Frank, M., Halpin, M. P., & Torrance, M. C. (1990). Search lessons learnedcrossword puzzles. Proceedings Eighth National Conference ArtificialIntelligence, pp. 210{215 Boston, Mass.Golomb, S., & Baumert, L. (1965). Backtrack programming. J. ACM, 12, 516{524.Golumbic, M. C., & Shamir, R. (1993). Complexity algorithms reasoningtime: graph-theoretic approach. J. ACM, 40, 1108{1133.Haralick, R. M., & Elliott, G. L. (1980). Increasing tree search eciency constraintsatisfaction problems. Artificial Intelligence, 14, 263{313.Hogge, J. C. (1987). TPLAN: temporal interval-based planner novel extensions. Department computer science technical report UIUCDCS-R-87, University Illinois.Hooker, J. N. (1994). Needed: empirical science algorithms. Operations Research,42, 201{212.17fivan Beek & ManchakLadkin, P., & Reinefeld, A. (1992). Effective solution qualitative interval constraintproblems. Artificial Intelligence, 57, 105{124.Ladkin, P., & Reinefeld, A. (1993). symbolic approach interval constraint problems.Calmet, J., & Campbell, J. (Eds.), Artificial Intelligence Symbolic MathematicalComputing, Springer Lecture Notes Computer Science 737. Springer-Verlag.Ladkin, P. B., & Maddux, R. D. (1988). binary constraint networks. Technical report,Kestrel Institute, Palo Alto, Calif.Mackworth, A. K. (1977). Consistency networks relations. Artificial Intelligence, 8,99{118.Montanari, U. (1974). Networks constraints: Fundamental properties applicationspicture processing. Inform. Sci., 7, 95{132.Nadel, B. A. (1989). Constraint satisfaction algorithms. Computational Intelligence, 5,188{224.Nebel, B., & Burckert, H.-J. (1995). Reasoning temporal relations: maximaltractable subclass Allen's interval algebra. J. ACM, 42, 43{66.Nudel, B. (1983). Consistent-labeling problems algorithms: Expected-complexitiestheory-based heuristics. Artificial Intelligence, 21, 135{178.Purdom, Jr., P. W. (1983). Search rearrangement backtracking polynomial averagetime. Artificial Intelligence, 21, 117{133.van Beek, P. (1992). Reasoning qualitative temporal information. Artificial Intelligence, 58, 297{326.van Beek, P., & Cohen, R. (1990). Exact approximate reasoning temporalrelations. Computational Intelligence, 6, 132{144.Vilain, M., & Kautz, H. (1986). Constraint propagation algorithms temporal reasoning.Proceedings Fifth National Conference Artificial Intelligence, pp. 377{382Philadelphia, Pa.Vilain, M., Kautz, H., & van Beek, P. (1989). Constraint propagation algorithmstemporal reasoning: revised report. Weld, D. S., & de Kleer, J. (Eds.), ReadingsQualitative Reasoning Physical Systems, pp. 373{381. Morgan Kaufmann.Wallace, R. J., & Freuder, E. C. (1992). Ordering heuristics arc consistency algorithms.Proceedings Ninth Canadian Conference Artificial Intelligence, pp. 163{169 Vancouver, B.C.18fiJournal Artificial Intelligence Research 4 (1996) 419-443Submitted 2/96; published 6 /96Principled Approach Towards SymbolicGeometric Constraint SatisfactionSanjay BhansaliBHANSALI@EECS.WSU.EDUSchool EECS, Washington State UniversityPullman, WA 99164-2752Glenn A. KramerGAK@EIT.COMEnterprise Integration Technologies, 800 El Camino RealMenlo Park, CA 94025Tim J. HoarTIMHOAR@MICROSOFT.COMMicrosoft CorporationOne Microsoft Way, 2/2069Redmond, WA 98052Abstractimportant problem geometric reasoning find configuration collectiongeometric bodies satisfy set given constraints. Recently, suggestedproblem solved efficiently symbolically reasoning geometry. approach, calleddegrees freedom analysis, employs set specialized routines called plan fragmentsspecify change configuration set bodies satisfy new constraintpreserving existing constraints. potential drawback, limits scalability approach,concerned difficulty writing plan fragments. paper address limitationshowing plan fragments automatically synthesized using first principlesgeometric bodies, actions, topology.1. Introductionimportant problem geometric reasoning following: given collection geometricbodies, called geoms, set constraints them, find configuration i.e., position,orientation, dimension geoms satisfies constraints. Solving problemintegral task many applications like constraint-based sketching design, geometricmodeling computer-aided design, kinematics analysis robots mechanisms(Hartenberg & Denavit, 1964), describing mechanical assemblies.General purpose constraint satisfaction techniques well suited solving constraintproblems involving complicated geometry. techniques represent geoms constraintsalgebraic equations, whose real solutions yield numerical values describing desiredconfiguration geoms. equation sets highly non-linear highly coupledgeneral case require iterative numerical solutions techniques. Iterative numerical techniquesparticularly efficient problems stability robustness (Press,Flannery, Teukolsky & Vetterling, 1986). many tasks (e.g., simulation optimizationmechanical devices) equations solved repeatedly makes compiled solutiondesirable. theory, symbolic manipulation equations often yield non-iterative, closedform solution. found, closed-form solution executed efficiently.1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiBHANSALI, KRAMER & HOARHowever, computational intractability symbolic algebraic solution equations rendersapproach impractical (Kramer, 1992; Liu & Popplestone, 1990).earlier work Kramer describes system called GCE uses alternative approachcalled degrees freedom analysis (1992, 1993). approach based symbolic reasoninggeometry, rather equations, shown efficient systems basedalgebraic equation solvers. approach uses two models. symbolic geometric model usedreason symbolically assemble geoms satisfy constraintsincrementally. "assembly plan" thus developed used guide solution complexnonlinear equations - derived second, numerical model - highly decoupled, stylizedmanner.GCE system used analyze problems domain kinematics shownperform kinematics simulation complex mechanisms (including Stirling engine,elevator door mechanism, sofa-bed mechanism) much efficiently pure numericalsolvers (Kramer, 1992). GCE subsequently integrated commercial systemcalled BravoTM Applicon used drive 2D sketcher (Brown-Associates, 1993).Several academic systems currently using degrees freedom analysisapplications like assembly modeling (Anantha, Kramer & Crawford, 1992), editinganimating planar linkages (Brunkhart, 1994), feature-based design (Salomons, 1994; Shah &Rogers, 1993).GCE employs set specialized routines called plan fragments create assembly plan.plan fragment specifies change configuration geom using fixed setoperators available degrees freedom, new constraint satisfiedpreserving prior constraints geom. assembly plan completedconstraints satisfied degrees freedom reduced zero. approachcanonical: constraints may satisfied order; final status geom termsremaining degrees freedom (p. 80-81, Kramer, 1992). algorithm findingassembly procedure time complexity O(cg) c number constraintsg number geoms (p. 139, Kramer, 1992).Since crux problem-solving taken care plan fragments, successapproach depends ones ability construct complete set plan fragments meetingcanonical specification. number plan fragments needed grows geometricallynumber geoms constraints increase. Worse, complexity planfragments increases exponentially since various constraints interact subtle ways creatinglarge number special cases need individually handled. potentially seriouslimitation extending degrees freedom approach. paper address problemshowing plan fragments automatically generated using first principlesgeoms, actions, topology.approach based planning. Plan fragment generation reduced planningproblem considering various geoms invariants describing state.Operators actions, rotate, change configuration geoms, therebyviolating achieving constraint. initial state specified set existinginvariants geom final state additional constraints satisfied. plansequence actions applied initial state achieves final state.formulation, one could presumably use classical planner, STRIPS (Fikes& Nilsson, 1971), automatically generate plan-fragment. However, operatorsdomain parametric operators real-valued domain. Thus, search space consistsinfinite number states. Even real-valued domain discretized considering realvalued intervals still large search space finding plan satisfies420fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTIONspecified constraints would intractable problem. approach uses loci information(representing set points satisfy constraints) reason effects variousoperators thus reduces search problem problem topology, involving reasoningintersection various loci.issue faced using conventional planner frame problem: determineproperties relationships change result action. typical solution useassumption: action modify property relationship unless explicitly statedeffect action. approach works well one knows priori possibleconstraints invariants might interest relatively constraints get affectedaction - true case. use novel scheme representing effectsactions. based reifying (i.e., treating first class objects) actions addition geometricentities invariant types. associate, pair geom invariants, set actionsused achieve preserve invariant geom. Whenever new geominvariant type introduced corresponding rules actions achieve/preserveinvariants added. Since many invariant types actionsdomain, scheme results simpler rules. Borgida, Mylopoulos & Reiter (1993) proposesimilar approach reasoning program specifications. unique feature workuse geometric-specific matching rules determine two general actionsachieve/preserve different constraints reformulated less general action.Another shortcoming using conventional planner difficulty representingconditional effects operators. GCE operations effect depends type geomwell particular geometry. example, action translating body intersectiontwo lines plane would normally reduce bodys translational degrees freedomzero; however, two lines happen coincide body still retains one degreetranslational freedom two lines parallel coincide action fails.situations called degeneracies. One approach handling degeneracies usereactive planner dynamically revises plan run-time. However, could resultunacceptable performance many real-time applications. approach makes possible precompile potential degeneracies plan. achieve dividing planningalgorithm two phases. first phase skeletal plan generated works normalcase second phase, skeletal plan refined take care singularitiesdegeneracies. approach similar idea refining skeletal plans MOLGEN(Friedland, 1979) idea critics HACKER (Sussman, 1975) fix known bugsplan. However, skeletal plan refinement MOLGEN essentially consisted instantiatingpartial plan work specific conditions, whereas method complete plan worksnormal case extended handle special conditions like degeneracies singularities.1.1 Plan Fragment Example.use simple example plan fragment specification illustrate approach.Domains mechanical CAD computer-based sketching rely heavily complexcombinations relatively simple geometric elements, points, lines, circlessmall collection constraints coincidence, tangency, parallelism. Figure 1illustrates fairly complex mechanisms (all implemented GCE) using simple geomsconstraints.421fiBHANSALI, KRAMER & HOARAutomobile suspensionElevator DoorsStirling EngineFigure 1. Modeling complex mechanisms using simple geoms constraints. constraintsneeded model joints mechanisms solvable using degrees freedom approach.example problem illustrated Figure 2 specified follows:Geom-type: circleName: $cInvariants: (fixed-distance-line $c $L1 $dist1 BIAS_COUNTERCLOCKWISE)To-be-achieved: (fixed-distance-line $c $L2 $dist2 BIAS_CLOCKWISE)example, variable-radius circle $c1 prior constraint specifying circlefixed distance $dist1 left fixed line $L1 (or alternatively, line drawn parallel$L1 distance $dist1 center $c tangent counterclockwise directioncircle). new constraint satisfied circle fixed distance $dist2right another fixed line $L2.$L2$L2$c$c$dist2$L1$dist1$L1Figure 2. Example problem (initial state)1We use following conventions: symbols preceded $ represent constants, symbols preceded ?represent variables, expressions form (>> parent subpart) denote subpart compound term, parent.422fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTIONsolve problem, three different plans used: (a) translate circlecurrent position position touches two lines $L2 $L1 shown figure(b) scale circle keeping point contact $L1 fixed, touches $L2 (c)scale translate circle touches $L2 $L1.action sequences constitute one plan fragment usedsituation would available GCE plan-fragment library. Noteplan fragments would applicable certain situations. example, $L1 $L2parallel, single translation never achieve constraints, plan-fragment (a)would applicable. paper show plan-fragmentsautomatically synthesized reasoning fundamental principles.rest paper organized follows: Section 2 gives architectural overviewsystem built synthesize plan fragments automatically detailed description variouscomponents. Section 3 illustrates plan fragment synthesis process using exampleFigure 2. Section 4 describes results current implementation system. Section5 relates approach work geometric constraint satisfaction. Section 6 summarizesmain results suggests future extensions work.2. Overview System ArchitectureFigure 3 gives overview architecture system showing various knowledgecomponents plan generation process. knowledge represented system broadlycategorized Geom knowledge-base contains knowledge specific particulargeometric entities Geometry knowledge-base independent particular geomsreused generating plan fragments geom.Knowledge ComponentsGeometry knowledge-baseGeom knowledge-baseGeomsActionsInvariantsAction Matching RulesAction RulesLociReformulation RulesSignaturesMeasurementsPlan fragmentspecificationPlannerPhasePrioritization StrategySkeletalPlanPlannerPhase IIPlan fragmentFigure 3. Architectural overview plan fragment generator2.1 Geom Knowledge-basegeom specific knowledge-base decomposed seven knowledgecomponents.423fiBHANSALI, KRAMER & HOAR2.1.1 ACTIONSdescribe operations performed geoms. GCE domain, three actionssuffice change configuration body arbitrary configuration: (translate g v)denotes translation geom g vector v; (rotate g pt ax amt) denotes rotationgeom g, around point pt, axis ax, angle amt; (scale g pt amt) ggeom, pt point geom, amt scalar. semantics scale operation dependstype geom; example, circle, scale indicates change radiuscircle line-segment denotes change line-segments length. Pt pointgeom fixed (e.g., center circle).2.1.2 INVARIANTSdescribe constraints solved geoms. initial version systemdesigned generate plan fragments variable-radius circle variable length linesegment fixed workplane, constraints distances geoms points,lines, geoms workplane. seven invariant types representconstraints. Examples two invariants are:(Invariant-point g pt glb-coords) specifies point pt geom gcoincident global coordinates glb-coords,(Fixed-distance-point g pt dist bias) specifies geom g lies fixeddistance dist point pt; bias either BIAS_INSIDE BIAS_OUTSIDEdepending whether g lies inside outside circle radius dist around point pt.2.1.3 LOCIrepresent sets possible values geom parameter, position pointgeom. various kinds loci grouped either 1d-locus (representable setparametric equations one parameter) 2d-locus (representable set parametricequations two variables). For, example line 1d locus specified (make-line-locusthrough-pt direc) represents infinite line passing through-ptdirection direc. loci represented system include rays, circles, parabolas, hyperbolas,ellipses.2.1.4 MEASUREMENTSused represent computation function, object, relationshipobjects. terms mapped set service routines get called planfragments. example measurement term is: (0d-intersection 1d-locus1 1d-locus2).represents intersection two 1d-loci. normal case, intersection two 1dimensional loci point. However, may singular cases, example, twoloci happen coincide; case intersection returns one locus insteadpoint. may also degenerate cases, example, two loci intersect;case, intersection undefined. exceptional conditions also representedmeasurement type used second phase plan generation processelaborate skeletal plan (see Section 3.3).424fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION2.1.5 GEOMSobjects interest solving geometric constraint satisfaction problems. Examplesgeoms lines, line-segments, circles, rigid bodies. Geoms degrees freedomsallow vary location size. example, 3D-space circle variableradius, three translational, two rotational, one dimensional degree freedom.configuration variables geom defined minimal number real-valuedparameters required specify geometric entity space unambiguously. Thus, circlesix configuration variables (three center, one radius, two planenormal). addition, representation geom includes following:name: unique symbol identify geom;action-rules: set rules describe invariants geompreserved achieved actions (see below);invariants: set current invariants geom;invariants-to-be-achieved: set invariants need achievedgeom.2.1.6 ACTION RULESaction rule describes effect action invariant. two facts interestplanner constructing plan: (1) achieve invariant using action (2)choose actions preserve many existing invariants possible. general,several ways achieve invariant several actions preserve invariant.intersection two sets actions set feasible solutions. system, effectactions represented part geom-specific knowledge form Action rules, whereasknowledge compute intersections two sets actions representedgeometry-specific knowledge (since depend particular geom acted on).action rule consists three-tuple (pattern, to-preserve, to-[re]achieve). Patterninvariant term interest; to-preserve list actions taken without violatingpattern invariant; to-[re]achieve list actions taken achieve invariantre-achieve existing invariant clobbered earlier action. actions statedgeneral form possible. matching rules Geometry Knowledge baseused obtain general unifier two actions. example action rule,associated variable-radius circle geoms is:pattern: (1d-constrained-point ?circle (>> ?circle CENTER) ?1dlocus)to-preserve: (scale ?circle (>> ?circle CENTER) ?any)(translate ?circle (v- (>> ?1dlocus ARBITRARY-POINT)(>> ?circle CENTER))to-[re]achieve: (translate ?circle (v- (>> ?1dlocus ARBITRARY-POINT)(>> ?circle CENTER))(AR-1)action rule used preserve achieve constraint center circle geomlie 1d locus. two actions may performed without violating constraint:(1) scale circle center. would change radius circle positioncenter remains hence 1d-constrained-point invariant preserved. (2)425fiBHANSALI, KRAMER & HOARtranslate circle vector goes current center arbitrary point 1dimensional locus ((v- b) denotes vector point b point a). achieve invariantone action may performed: translate circle center moves currentposition arbitrary position 1-dimensional locus.2.1.7 SIGNATUREScompleteness, necessary exist plan fragment possible combinationconstraints geom. However, many cases, two constraints describesituation geom (in terms degrees freedom). example, constraintsground two end-points line-segment constraints ground direction, length,one end-point line-segment reduce degrees freedom line-segmentzero hence describe situation. order minimize number plan fragmentsneed written, desirable group sets constraints describe situationequivalence classes represent equivalence class using canonical form.state geom, terms prior constraints it, summarized signature.signature scheme geom set canonical signatures plan fragments needwritten. Kramers earlier work (1993) signature scheme determined manuallyexamining signature obtained combining constraint types designating oneset equivalent signatures canonical. approach allows us construct signaturescheme geom automatically using reformulation rules (described shortly).reformulation rule rewrites one constraints simpler form. signature schemeobtained first generating possible combinations constraint types yield setpossible signatures. signatures reduced using reformulation rulessignature reduced simplest form. set (unique) signatures left constitutesignature scheme geom.example, consider set constraint types variable radius circle. signaturegeom represented tuple <Center, Normal, Radius, FixedPts, FixedLines> where:Center denotes invariants center point either Free (i.e.,constraint center point), L2 (i.e., center point constrained 2dimensional locus), L1 (i.e., center point constrained 1-dimensionallocus), Fixed.Normal denotes invariant normal plane circleeither Free, L1, Fixed (in 2D always fixed).Radius denotes invariant radius either Free Fixed.FixedPts denotes number Fixed-distance-point invariants either 0,1,2.FixedLines denotes number Fixed-distance-line invariants either0,1, 2.L2 L1 denote 2D 1D locus respectively. assume 2D geometry, L2 invariantCenter redundant, Normal always Fixed. 3 x 1 x 2 x 3 x 3 = 54possible signatures geom. However, several describe situation.example, signature:<Center-Free,Radius-Free, FixedPts-0,FixedLines-2>describes circle constrained specific distances two fixed lines,rewritten to:426fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION<Center-L1, Radius-Free,FixedPts-0,FixedLines-0>describes circle constrained 1-dimensional locus (in case angularbisector two lines). Using reformulation rules, derive signature scheme variableradius circles consisting 10 canonical signatures given below:<Center-Free,Radius-Free, FixedPts-0,FixedLines-0><Center-Free,Radius-Free, FixedPts-0,FixedLines-1><Center-Free,Radius-Free, FixedPts-1,FixedLines-0><Center-Free,Radius-Fixed, FixedPts-0,FixedLines-0><Center-L1,Radius-Free, FixedPts-0,FixedLines-0><Center-L1,Radius-Free, FixedPts-0,FixedLines-1><Center-L1,Radius-Free, FixedPts-1,FixedLines-0><Center-L1,Radius-Fixed, FixedPts-0,FixedLines-0><Center-Fixed,Radius-Free, FixedPts-0,FixedLines-0><Center-Fixed,Radius-Fixed, FixedPts-0,FixedLines-0>Similarly, number signatures line-segments reduced 108 19 usingreformulation rules.2.2 Geometry Specific Knowledgegeometry specific knowledge organized three different kinds rules.2.2.1 MATCHING RULESused match terms using geometric properties. planner employs unificationalgorithm match actions determine whether two actions common unifier. However,standard unification algorithm sufficient purposes, since purely syntacticuse knowledge geometry. illustrate this, consider following twoactions:(rotate $g $pt1 ?vec1 ?amt1),(rotate $g $pt2 ?vec2 ?amt2).first term denotes rotation fixed geom $g, around fixed point $pt1arbitrary axis arbitrary amount. second term denotes rotation geomaround different fixed point $pt2 rotation axis amount unspecified before.Standard unification fails applied terms binding variablesmakes two terms syntactically equal2. However, resorting knowledge geometry,match two terms yield following term:(rotate $g $pt1 (v- $pt2 $pt1) ?amt1)denotes rotation geom around axis passing points $pt1 $pt2.point around body rotated point axis (here arbitrarily chosenone fixed points, $pt1) amount rotation anything.planner applies matching rules match outermost expression term first;rule applies, tries subterms term, on. none matching rules apply,2 Specifically, unification fails tries unify $pt1 $pt2.427fiBHANSALI, KRAMER & HOARalgorithm degenerates standard unification. matching rules also conditionsattached them. condition boolean function; however, parttend simple type checks.2.2.2 REFORMULATION RULESmentioned earlier, several ways specify constraints restrict degreesfreedom geom. GCE, plan fragments indexed signatures summarizeavailable degrees freedom geom. reduce number plan fragments needwritten indexed, desirable reduce number allowable signatures.accomplished set invariant reformulation rules used rewrite pairsinvariants geom equivalent pair simpler invariants (using well-foundedordering). equivalence means two sets invariants produce rangemotions geom. reduces number different combinations invariantsplan fragments need written. example invariant reformulation following:(fixed-distance-line ?c ?l1 ?d1 BIAS_COUNTERCLOCKWISE)(fixed-distance-line ?c ?l2 ?d2 BIAS_CLOCKWISE)(RR-1)(1d-constrained-point ?c (>> ?c center) (angular-bisector(make-displaced-line ?l1 BIAS_LEFT ?d1)(make-displaced-line ?l2 BIAS_RIGHT ?d2)BIAS_COUNTERCLOCKWISEBIAS_CLOCKWISE))rule takes two invariants: (1) geom fixed distance left given line,(2) geom fixed distance right given line. reformulation producesinvariant geom lies angular bisector two lines parallel two givenlines specified distance them. Either two original invariants conjunctionnew one equivalent original set invariants.Besides reducing number plan fragments, reformulation rules also help simplifyaction rules. Currently action rules (for variable radius circles line-segments) usesingle action preserve achieve invariant. restrict allowable signaturesgeom, possible create examples need sequence (more one) actionsrule achieve invariant, need complex conditions need checkeddetermine rule applicability. Allowing sequences conditionals rules increasescomplexity rules pattern matcher. makes difficult verifycorrectness rules reduces efficiency pattern matcher.Using invariant reformulation rules allows us limit action rules containsingle action. Unfortunately, seems still need conditions achieve certain invariants.example, consider following invariant variable radius circle:(fixed-distance-point ?circle ?pt ?dist BIAS_OUTSIDE)states circle, ?circle distance ?dist point ?pt lie outsidecircle around ?pt radius ?dist. One action may taken achieve constraint is:(scale ?circle(>> ?circle center)428fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION(minus (>> (v- (>> ?circle center) ?pt)magnitude)?dist)is, scale circle setting radius distance center point ?ptminus scalar amount ?dist (see Figure 4). However, action achieves constraintcircle happens lie outside circular region radius ?dist center ?pt.$C$cFigure 4. geom $c scaled touch $C center $c lies shaded region.Therefore, need pre-condition rule checks indeed case. Noteaction necessary completeness (otherwise planner would able solvecertain cases solution). Instead allowing conditional rules, use rules withoutcondition second phase plan generation check seeexceptions. Thus, example, exception would detected since third argumentscale operation returns negative number considered exception condition scaleoperation.2.2.3 PRIORITIZING STRATEGYGiven set invariants achieved geom, planner generally creates multiplesolutions. valid solutions absence exception conditions yieldconfiguration geom. However, plan fragments contain redundant actionsequences (e.g., two consecutive translations). Moreover, geom constrainedexception conditions, plan fragments able provide solutionwhereas others not. prioritization strategy used prioritize skeletal planfragments plan fragments least redundancy flexibility chosen.Eliminating plan fragments redundant actions turns straightforward.assume one degree dimensional freedom geometric body.assumption proved 1 translation, 1 rotation, 1 scale sufficient changeconfiguration object arbitrary configuration 3D space. Therefore, planfragment contains one instance action type contains redundanciesrewritten equivalent plan fragment eliminating redundant actions, combining twoaction single composite action. example, consider following pairtranslations geom:(translate $g ?vec)429fiBHANSALI, KRAMER & HOAR(translate $g (v- ?to 2 (>> $g center)))?vec represents arbitrary vector ?to2 represents arbitrary position. ?to2independent positional parameter geom, first translate action redundantremoved. Hence plan fragments contain redundant actionseliminated.prioritize remaining plan fragments following principle used:Prefer solutions subsume alternative solution.rationale principle permits greater flexibility solving constraintsexception conditions. example, suppose two solutions circle geom:Solution 1: Translate circle center lies fixed position 1dimensional locus.Solution 2: Translate circle center lies arbitrary point 1dimensional locus; scale fixed amount (which functionposition arbitrary point).first solution subsumed second solution since always choosearbitrary point Solution 2 fixed position specified Solution 1 (the scale operationcase leaves dimension circle unchanged). Therefore Solution 2 preferredSolution 1.subsumption relation imposes partial order set skeletal plan fragments.prioritization strategy selects maximal elements partial order. runtimetried turn one yields solution.3.0 Plan Fragment Generationplan fragment generation process divided two phases (Figure 1). first phasespecification plan fragment taken input, planner used generate setskeletal plans. form input second phase chooses oneskeletal plans elaborates take care singularities degeneracies. outputphase complete plan fragments.3.1 Phaseskeletal plan generated using breadth-first search process. Figure 5 gives general formsearch tree produced planner. first action typically reformulationplanner uses reformulation rules rewrite geom invariants canonical form. Next,planner searches actions produce state least 1 invariant Preservedlist preserved least 1 action To-be-achieved (TBA) list achieved. preservedachieved invariants pushed Preserved list, clobbered unachievedinvariants pushed TBA list child state.strategy produce intermediate nodes search tree might clobberone preserved invariant without achieving new invariant might produce stateidentical parent state terms invariants Preserved430fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTIONPreserved: PTBA:ReformulatePreserved: PTBA:Action 1Preserved: P1TBA: A1Action 3Action 2Preserved: P2TBA: A2Preserved: P3TBA: A3ActionsPreserved: P +TBA: nilFigure 5. Overview search tree produced plannerTBA list. initial state geom may arbitrary configuration(among set allowable configurations) may necessary first move geomalternative allowable configuration find optimal solution.illustrate need, consider example Figure 6. example, one priorconstraint variable radius circle geom: center lies 1-dimensional locus. newconstraint achieved is: geom lie fixed distance line. orderachieve constraint one following two actions may taken: 1) scale(a)(b) Scale(c) Translate(d) Translate & scaleFigure 6. Example illustrate need actions produce state equivalent parent state.431fiBHANSALI, KRAMER & HOARcircle fixed distance line (Figure 6b), 2) translate circlenew position 1-dimensional locus touches line (Figure 6c). However,infinite number additional solutions consisting combinations scale translation(Figure 6d). solutions derived planner first changes configurationgeom preserves existing invariant without achieving new invariant (i.e.,scale arbitrary amount translate arbitrary point 1-dimensional locus)followed action achieves new invariant. Therefore planner also creates childstates identical parent state terms invariants Preserved TBA lists.planner iteratively expands leaf node search tree one followingtrue:1. node represents solution; is, TBA list nil.2. node represents cycle; is, invariants Preserved TBA listsidentical one ancestor nodes.node marked terminal search tree pruned point. leaf nodesmarked terminal, search terminates. planner collects terminal nodessolutions. plan-steps solution nodes represents skeletal planfragment. multiple skeletal plan fragments obtained planner, onechosen using prioritizing rule described earlier passed second phase planfragment generation.3.2 Phase I: Exampleuse example Section 1 illustrate Phase planner. planner beginsattempting reformulate given constraints. uses reformulation rule RR-1 described earlierrepeated convenience:(fixed-distance-line ?c ?l1 ?d1 BIAS_COUNTERCLOCKWISE)(fixed-distance-line ?c ?l2 ?d2 BIAS_CLOCKWISE)(RR-1)(1d-constrained-point ?c (>> ?c center) (angular-bisector(make-displaced-line ?l1 BIAS_LEFT ?d1)(make-displaced-line ?l2 BIAS_RIGHT ?d2)BIAS_COUNTERCLOCKWISEBIAS_CLOCKWISE))iiL2iiiivL1Figure 7. Four possible angular bisectors two lines L1 L2. bias symbols L1L2 corresponding ray (i) BIAS_COUNTERCLOCKWISE & BIAS_CLOCKWISE,respectively.432fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTIONrule two measurement terms: make-displaced-line angular-bisector.Make-displaced-line takes three arguments: line, l, bias symbol indicating whetherdisplaced line left right l, distance, d. returns line parallelgiven line l distance left right line depending bias. Angular-bisectortakes two lines, l1 l2, two bias symbols returns one four rays bisectslines l1 l2 depending bias symbols (see Figure 7). reformulation, statesearch tree shown Figure 8. reformulation rules applicable point.Preserved: (fixed-distance-line $c $L1 $dist1 BIAS_COUNTERCLOCKWISE)TBA:(fixed-distance-line $c $L2 $dist2 BIAS_CLOCKWISE)ReformulationPreserved: (fixed-distance-line $c $L1 $dist1 BIAS_COUNTERCLOCKWISE)TBA:(1d-constrained-point $c(>> $c CENTER)(angular-bisector(make-displaced-line $L1 $BIAS_LEFT $dist1)(make-displaced-line $L2 $BIAS_RIGHT $dist2)BIAS_COUNTERCLOCKWISEBIAS_CLOCKWISE))Figure 8. Search tree reformulating invariantsNext, planner searches actions achieve new invariant preserveexisting invariant both. describe steps involved finding actions satisfymaximal number constraints (in case, two). planner first finds actionsachieve 1d-constrained-point invariant examining action rules associatedvariable-circle geom. action rule AR-1 contains pattern matches 1d-constrainedpoint invariant:pattern: (1d-constrained-point ?circle (>> ?circle center) ?1dlocus)to-preserve: (scale ?circle (>> ?circle center) ?any)(translate ?circle (v- (>> ?1dlocus arbitrary-point)(>> ?circle center))to-[re]achieve: (translate ?circle (v- (>> ?1dlocus arbitrary-point)(>> ?circle center))following bindings:(AR-1){?circle = $c, ?1d-locus = (angular-bisector (make-displaced -line ...) ...)}Substituting bindings obtain following action:433fiBHANSALI, KRAMER & HOAR(translate $c (v- (>> (angular-bisector (make-displaced-line $L1 BIAS_LEFT $dist1)(make-displaced-line $L2 BIAS_RIGHT $dist2)arbitrary-point)(>> $c center)))(a1)taken achieve constraint. Similarly, planner finds actionspreserve fixed-distance-line invariant. relevant action rule following:pattern: (fixed-distance-line ?circle ?line ?distance)(AR-2)to-preserve: (translate ?circle (v- (>> (make-line-locus (>> ?circle center)(>> ?line direction))arbitrary-point)(>> ?circle center))to-[re]achieve: (translate ?circle (v- (>> (make-displaced-line?lineBIAS_LEFT(plus ?distance (>> ?circle radius)))arbitrary-point)(>> ?circle center)))relevant action appropriate substitutions is:(translate $c (v- (>> (make-line-locus(>> $c center)(>> L1 direction))arbitrary-point)(>> $c center))(a2)Now, find action preserves preserved invariant achieves TBAinvariant, planner attempts match preserving action (a2) achieving action (a1).two actions match using standard unification, match employing followinggeometry-specific matching rule:# move arbitrary point two# different loci, move point# intersection two loci(v- (>> $1d-locus1 arbitrary-point) $to)(v- (>> $1d-locus2 arbitrary-point) $to)(v- (0d-intersection $1d-locus1 $1d-locus2) $to)yield following action:(translate $c (v- (0d-intersection (angular-bisector(make-displaced-line ...) ...)(make-line-locus (>> $c center) (>> $L1 direction))(>> $c CENTER)))action moves circle point shown Figure 9 achieves constraints.simple one-step plan constitutes skeletal plan fragment.434fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION$L2$cangular-bisectormake-line-locus$dist2$dist1$L1Figure 9.denotes point circle moved.two actions generated planner first iteration. Oneachieves new constraint clobbers prior invariant. moves circleanother configuration without achieving new constraint preserving prior constraint.first action produces terminal state since constraints achieved.Hence search tree pruned point. However, planner continues searchalternative solutions expanding two nodes. two iterations followingsolutions obtained:1. Translate intersection angular-bisector make-line-locus.2. Translate arbitrary point angular-bisector, followed translationintersection point.3. Translate arbitrary point make-line-locus, followed translationintersection point.4. Translate arbitrary point angular-bisector scale.stage first phase plan fragment generation terminated skeletalplan fragments passed second phase planner.3.3 Phase II: Elaboration Skeletal Plan Fragmentpurpose Phase 2 planning i) select one skeletal plan fragments, ii)elaborate generate desirable configuration geomconstrained well handle exception conditions.3.3.1 SELECTION SKELETAL PLAN FRAGMENTStwo primary considerations selecting skeletal plan fragment reduce redundantactions plan increase generality plan. considerations used formulateprioritization strategy described Section 2. strategy implemented lookup tableassigns weights various plan fragments. plan fragments maximal weightsselected elaboration Phase 2. Readers interested implementation detailsreferred (Hoar, 1995).3.3.2 PLAN FRAGMENT ELABORATIONPlan fragment elaboration refines skeletal plan fragment two ways. First, refines actions435fiBHANSALI, KRAMER & HOARconstrained (e.g., translate arbitrary point locus) appropriateinstantiation unconstrained parameters (e.g., selecting specific point locus). Second,handles exception conditions result constrained over-constrained systems.action refinement exception handling treated using common technique.Plan elaboration based "principle least motion": multiplesolutions problem choose solution minimizes total amount perturbation(motion) system. Implementing principle requires definition motion function,CA,G action, A, geom type, G. example, translation geom, motionfunction, CT,circle could square displacement center geominitial final position. also need motion summation function, G sums motionproduced individual actions geom G. example summation functionnormal addition operator: plus. total motion produced geom computed usingsummation function motion functions action- geom pairs.plan fragment constrained, expression representing total motionwould contain one variables representing ungrounded parameters geom.Formal optimization techniques, based finite difference methods, used obtain valuesparameters would minimize motion function. However, use efficient,algorithm based hill-climbing guarantee optimality yields good resultspractice. use heuristic algorithm justified many interactive applications likesketching, fast, sub-optimal solution preferable computationally expensive,optimal one.algorithm begins segmenting continuous loci discrete intervals.systematically searches resultant, discrete n-dimensional space. algorithm first findslocal minima along one dimension holding variables constant values.holds first variable minimum value found searches lower local minima alongsecond dimension on. Although algorithm guarantee finding globaleven local minima, efficient yields good results practice. implementedalgorithm somewhat complex simple description above; detailsfound elsewhere (Hoar, 1995).Exception conditions handled using technique above. Exceptionconditions identified service routine returns set solutions solution (e.g.,routine compute intersection two 1-dimensional loci returns 1-dimensional locusnil). Multiple solutions represent constrained system requires search amongset solutions returned. conditions handled exactly described previousparagraph. no-solution exception occurs, system aborts plan fragment printsdiagnostic message explaining constraint could solved.3.4 Phase II: ExampleFour skeletal plan fragments generated first phase planner (Section 3.2). Usingrule eliminating redundant translations given earlier, second third plan fragmentsreduced single translation plan fragments equivalent first plan fragment.leaves two distinct plan fragment solutions consider.Using prioritizing rule, system concludes first plan fragment consistingsingle translation subsumed second plan fragment consisting translationscale. Thus, second plan fragment chosen preferred solution.plan fragment deterministic since contains action translates circle436fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTIONgeom arbitrary point angular-bisector. Therefore, system inserts iterative loopcomputes amount motion circle various points angular bisector,breaking loop finds minima. Similarly, service routine mayreturn exception, system inserts case statement contains loop handlesituations one solution returned. Online Appendix 1 contains completeexample plan fragment generated system.4.0 Resultsplan fragment generator described implemented using CLOS (Common LispObject System). implemented parts geometric constraint engine (GCE) describedKramer C++ XMotif based graphical user interface. also writtentranslator translates synthesized plan fragments C++. complete plan fragmentlibrary representative geom (line segment) synthesized integratedconstraint engine. Using able successfully demonstrate solutionseveral geometric constraints. present evaluation system.primary contribution research novel geometric constraint satisfactionapproach. perspective constraint satisfaction techniques, novel featureapproach - degrees freedom analysis - already described earlier workssecond author (Kramer, 1992, 1993). goal research develop automatedtechniques enable degrees freedom approach scale reducing amounteffort needed creating plan fragment libraries. Hence, evaluation basedsuccessful automating plan fragment synthesis process.used plan fragment generator described automatically synthesize planfragments two representative geoms -- line-segments circles -- 2D. seventypes constraints thirty four rules system (12 action rules line-segments, 8 actionrules circles, 7 Reformulation rules, 7 Matching rules). Using rulessuccessfully generated skeletal plan fragments various combinations constraints linesegments (249) circles (50). largest search tree produced planner orderhundred nodes takes minutes Macintosh Quadra. evaluationpurposes, present data one representative geom - line segment.4.1 Programming EffortFigure 10 shows number lines code comprising current system. areas solidrepresent code written manually. includes 5000 lines CLOS codeplan fragment synthesizer, 5400 lines C/C++ user interface, 3300 lines C/C++support routines. hatched area represent code synthesized planfragment generator. represents 27000 lines C++ code (for plan fragments linesegment geom). size synthesized plan fragment (about 121 lines average) much lessplan fragments written manually (in C) original version GCE. Thus, usingautomated plan fragment generator considerably reduced amount programming.reduction ratio 5:1 good indicator reduction programming effort,subject criticism since compares code two different programming languagescomprising different degrees difficulty.accurate evaluation obtained comparing total effort required writing planfragments manually total effort required synthesizing using437fiBHANSALI, KRAMER & HOAR12% (CLOS)13% (C/C++)67%(C++)User Interface8% (C/C++)Support routinesGeneratorPlan FragmentsFigure 10. Lines code different parts systemtechnique described paper. extremely difficult, impossible,controlled experimental setting number factors cost involved. bestdone compare empirical data based experience developing system.following table shows effort person days developing plan fragment libraryline-segment geom using technique.Plan Fragment GeneratorManuallyResearch900Development150498Total210498Table 1. Effort (in person-days) creating plan fragmentseffort involved writing plan fragments manually, use conservative estimate2 person days plan fragment3. table shows using plan fragment generatorobtained 58% reduction effort creating plan fragment library. testingdebugging time ignored assumed cases (althoughbelieve time much manually generated plan fragments).4.2 Scalabilitymuch stronger evidence support technique obtained look effort3 estimate based effort required developing plan fragment library GCE wellexperimental data obtained two graduate students write plan fragments manually.438fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTIONrequired extending plan fragment library adding features (e.g., new kindsgeoms constraints). evaluate scalability approach, decided extend planfragments 3D geoms added degrees rotational translational freedom.extension done manually would significant exercise software maintenance sincerequires changes plan fragment library. Using plan fragment generatorneeded revise rules used planner make changes support routines. Sincesupport routines written manually, cost modifyapproaches, effort needed rewrite rules relevant. took 1 weekeffort rewrite debug action rules synthesize complete plan fragment library3D, link successfully constraint engine. significant resultdemonstrating technique used scale degrees freedom analysiscomplex geoms geometries.4.3 Correctnessimportant issue ignored far is: one verify correctnesscompleteness plan fragment generator? done extensive testing evaluationplan fragments synthesized plan fragment generator. Table 2 summarizes results.Number plan fragment specificationsSpecs. solutions:Completenesssolution existsMissing rulessymbolic solutionTotalPlan fragments errors:CorrectnessFaults due errors logicSupport routine errorsTotal249651328005656Table 2. Completeness Correctness synthesized plan fragmentseighty plan fragment specifications planner failed producesolution. sixty five specifications, solutions general case --specifications represent overconstrained problems, constraining one end point linesegment one-dimensional locus previous constraints already reducedend-points translational degrees freedom zero. action planner takecases check new constraint already satisfied. Thirteen casessolutions two missing rules: one action rule, one reformulation rule. tworules added thirteen specifications solved. Finally, two planfragments planner failed produce analytical solution. cases shownFigure 11. solve problems need reformulation rule reformulates existinginvariant constraint endpoint $lseg curve $L3. Instead representingcomplex 1-dimensional (and higher dimensional) loci like $L3, assume constraintengine would call numerical solver computes solution iteratively. alternative wouldextend set support routines handle complex loci intersections.439fiBHANSALI, KRAMER & HOAR.$L2$P$L3$lseg$L1Figure 11. Example problem generated symbolic solution. $lseg line-segmentconstrained one end-point $L1, fixed length, tangentcircle centered $P. new constraint end-point $lseg $L2.check correctness plan fragments, exhaustive evaluationplan fragments. seen Table 2, code synthesized perfect.20% plan fragments function correctly. analyzed reasonsfailure manually inspecting plan fragments. significant finding nonefailures due logical errors plan fragments. words skeletal planfragments generated Phase correct complete. failuresbugs mathematical support routines called plan fragments.instances failures traced bugs implementing Phase 2 plan fragment: eitherselecting wrong skeletal plan fragment computing least motion correctly.expected first version automatically generated plan fragments completelybug-free. Indeed, high percentage plan fragments function correctly (almost 80%)positive result reflects significant increase quality correspondingdecrease maintenance effort building geometric constraint satisfaction systems usingapproach.5.0 Related WorkGeometric constraint satisfaction old problem. Probably first application problemconstraint-based sketching Sketchpad program developed Sutherland (1963).Sketchpad program based constraint relaxation limited problemsmodeled point variables.field mechanical design, graph based approach constraint satisfactiondescribed Serrano (1987). Serranos approach constraints modeled usingconstraint network; constraint satisfaction engine finds values constrained variablessatisfy constraints network using constraint propagation techniques. approachidentifies loops cycles network, collapses supernodes, appliesconventional sequential local propagation. approach uses numerical iterative techniquesproblems stability. computational advantage approach reducesequations tightly coupled.commercial systems kinematics analysis based numerical iterativetechniques algebraic techniques combination two. Although approachesprinciple robust, several shortcomings make inappropriate real-timeapplications.Among non-commercial systems, notable new approach constraint based sketching440fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTIONJuno-2 developed DEC-SRC (Heydon & Nelson, 1994). Constraints Juno-2specified using expressive, declarative constraint language seems powerful enoughexpress constraints arise practice. Juno-2 uses combination symbolicnumerical techniques solve geometric constraints efficiently. key difference Juno2 degrees freedom approach Juno-2 symbolic reasoning donedomain equations. example, Juno-2 uses symbolic techniques like local propagation,unpacking, unification closure reduce number unknowns system equations.equations solved Newtons method. degrees freedom analysis, symbolicreasoning done domain geometry rather equations.Geometric constraints also arise robotics, primary issues concernedfinding physically realizable path space robot manipulator partassembly. fundamental analytical tool solving motion planning problems roboticsconfiguration space framework (Lozano-Perez, 1983). configuration space approach,problem planning motion part space obstacles transformedequivalent simpler problem planning motion point space enlargedconfiguration-space obstacles. Degrees freedom analysis finesses problem since usesnotion incremental assembly metaphor solving geometric constraint systems.physical meaning ascribed objects move need- factor quite important real-world assembly problem arising robotics.use plan guide solution complicated non-linear equations arisingformulating solving problems algebraically.6.0 Conclusionsdescribed plan fragment generation methodology synthesize plan fragmentsgeometric constraint satisfaction systems reasoning first principles geometricentities, actions, topology. technique used successfully synthesize planfragments realistic set constraints geoms. may seem substituted onehard task - writing complete set correct plan fragments various combinations geomsconstraints - even harder task: creating knowledge base rules automateprocess. rules difficult write found necessary spendeffort debugging rules. However, estimate total effort write debug rulesstill order magnitude less writing debugging manually written plan fragmentcode. future work investigate approach scales complex constraintsgeometries.Another useful extension work would concerned pushing automation onelevel automatically acquire types knowledge simpler buildingblocks. example, technique automatically synthesizing least motion functiondescription geometry would useful.method plan fragment generation divided two disjoint phases.alternative method would explore two phases interleaved. One possibilitydegeneracy redundant constraint, planner could reformulateproblem removing redundant constraint re-synthesize skeletal plan fragmentnew set constraints. resultant plan would form part original planfragment deal degenerate cases. words, plan fragments would generatedon-the-fly needed constraint solver.441fiBHANSALI, KRAMER & HOARAcknowledgmentsthank Qiqing Xia helped implementing parts system described paper.also acknowledge support resources provided School ElectricalEngineering Computer Science, Washington State University. work originatedfirst author Knowledge Systems Laboratory, Stanford University, secondauthor Schlumberger Laboratory Computer Science, Austin.ReferencesAnantha, R., Kramer, G., & Crawford, R. (1992). architecture represent over, under,fully constrained assemblies. Proceedings ASME Winter Annual Meeting, 233-244.Borgida, A., Mylopoulos, J., & Reiter, R. (1993). ... nothing else changes: frame problemprocedure specifications. Proceedings 15th International ConferenceSoftware Engineering, Baltimore, MD.Brown-Associates. (1993). Applicons GCE: Strong Technical Framework. Brown AssociatesInc.Brunkhart, M. W. (1994). Interactive geometric constraint systems. Masters thesis, TR No.CSD-94-808, Department EE&CS, University California, Berkeley.Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: new approach applicatiion theoremproving problem solving. Artificial Intelligence, 2, 198-208.Friedland, P. E. (1979). Knowledge-based experiment design molecular genetics. Tech. reportCSD-79-771, Department Computer Science, Stanford University.Hartenberg, R. S., & Denavit, J. (1964). Kinematic Synthesis Linkages. New York: McGrawHill.Heydon, A., & Nelson, G. (1994). Juno-2 constraint-based drawing editor. SRC Researchreport 131a, Digital Systems Research Center, Palo Alto, CA.Hoar, T. (1995). Automatic program synthesis geometric constraint satisfaction. MastersThesis, School EECS, Washington State University.Kramer, G. A. (1992). Solving Geometric Constraint Systems: Case Study Kinematics.Cambridge, MA: MIT Press.Kramer, G. A. (1993). geometric constraint engine. Artificial Intelligence, 58(1-3), 327-360.Liu, Y., & Popplestone, R. J. (1990, ). Symmetry constraint inference assembly planning:automatic assembly configuration specification. Proceedings AAAI-90, Boston, MA,1038-1044.Lozano-Perez, T. (1983). Spatial planning: configuration space approach. IEEE TransactionsComputers, C-32, 108-120.442fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTIONPress, W. H., Flannery, B. P., Teukolsky, S. A., & Vetterling, W. T. (1986). Numerical Recipes:Art Scientific Computing. Cambridge, England: Cambridge University Press.Salomons, O. (1994). Computer support design mechanical products. Ph.D. Thesis,Universiteit Twente, Netherlands.Serrano, D. (1987). Constraints conceptual design. Ph.D. thesis, Massachusetts InstituteTechnology.Shah, J. J., & Rogers, M. T. (1993). Assembly modeling extension feature-based design.Research Engineering Design, 5, 218-237.Sussman, G. J. (1975). Computer Model Skill Acquisition. New York: American Elsevier.Sutherland, I. E. (1963). Sketchpad, man-machine graphical communication system. Ph.D.Thesis, Massachusetts Institute Technology.443fiJournal Artificial Intelligence Research 4 (1996) 477-507Submitted 9/95; published 6/96Partially Controlled Multi-Agent SystemsRonen I. Brafmanbrafman@cs.ubc.caComputer Science DepartmentUniversity British ColumbiaVancouver, B.C., Canada V6L 1Z4Moshe Tennenholtzmoshet@ie.technion.ac.ilIndustrial Engineering ManagementTechnion - Israel Institute TechnologyHaifa 32000, IsraelAbstractMotivated control theoretic distinction controllable uncontrollableevents, distinguish two types agents within multi-agent system: controllableagents , directly controlled system's designer, uncontrollable agents ,designer's direct control. refer systems partiallycontrolled multi-agent systems, investigate one might uence behavioruncontrolled agents appropriate design controlled agents. particular,wish understand problems naturally described terms, methodsapplied uence uncontrollable agents, effectiveness methods,whether similar methods work across different domains. Using game-theoretic framework,paper studies design partially controlled multi-agent systems two contexts:one context, uncontrollable agents expected utility maximizers,reinforcement learners. suggest different techniques controlling agents'behavior domain, assess success, examine relationship.1. Introductioncontrol agents central research topic two engineering fields: Artificial Intelligence (AI) Discrete Events Systems (DES) (Ramadge & Wonham, 1989). Oneparticular area fields concerned multi-agent environments;examples include work distributed AI (Bond & Gasser, 1988), work decentralizedsupervisory control (Lin & Wonham, 1988). fields developedtechniques incorporated particular assumptions models. Hence,natural techniques assumptions used one field may adoptedmay lead new insights field.difference AI work multi-agent systems, work decentralized discreteevent systems distinguishes controllable uncontrollable events. Controllableevents events directly controlled system's designer, uncontrollable events directly controlled system's designer. Translating terminology context multi-agent systems, introduce distinction twotypes agents: controllable agents , directly controlled system's designer,uncontrollable agents , designer's direct control. leadsc 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiBrafman & Tennenholtznaturally concept partially controlled multi-agent system (PCMAS)following design challenge: ensuring agents system behave appropriatelyadequate design controllable agents. believe many problemsnaturally formulated instances PCMAS design. goal characterize importantinstances design problem, examine tools used solve it,assess effectiveness generality tools.distinguishes partially controlled multi-agent systems AI context similar models DES structural assumptions make uncontrolled agentsinvolved. Unlike typical DES models concerned physical processes devices, AI particularly interested self-motivated agents, two concrete examplesrational agents, i.e., expected utility maximizers, learning agents, e.g., reinforcement learners. Indeed, examples constitute two central models self-motivatedagents game theory decision theory, referred educative evolutive models(e.g., see Gilboa & Matsui, 1991). special nature uncontrollable agentsspecial structure uncontrollable events induce differentiates PCMAScorresponding models DES literature. difference raises new questionssuggests new perspective design multi-agent systems. particular, callstechniques designing controllable agents that, exploiting structural assumptions,uence behavior uncontrollable agents lead system desiredbehavior.order understand issues, study two problems stated solvedadopting perspective PCMAS design; problemsinterest large community. problems goal uencebehavior agents control. exert uence indirectlychoosing suitable behaviors agents direct control. one case,attempt uence behavior rational agents, case, tryuence learning agents.first study concerned enforcement social laws. numberagents designed different designers work within shared environment, beneficialimpose certain constraints behavior, that, overall, system functionbetter. example, Shoham Tennenholtz (1995) show imposing certain \traclaws," considerably simplify task motion planning robot, stillenabling ecient motions. Indeed, see later, conventions heartmany coordination techniques multi-agent systems. Yet, without suitable mechanisms,rational agents may incentive follow conventions. show how,certain cases, use perspective partially controlled multi-agent systemsstructural assumption rationality enforce conventions.second study involves two-agent system consisting teacher student.teacher knowledgeable agent, student agent learningbehave domain. goal utilize teacher (which control)improve behavior student (which controlled us). Hence,instance partially controlled multi-agent systems structural assumptionuncontrolled agent employs particular learning algorithm.studies presented paper suggest techniques achieving satisfactory systembehavior design controllable agents, relevant, techniques478fiOn Partially Controlled Multi-Agent Systemsexperimentally assessed. Beyond formulation solution two interesting problems multi-agent system design, paper suggests general perspective certaindesign problems. Although feel still premature draw general conclusionpotential general theory PCMAS design, certain concepts, punishmentreward, suggest central area.paper organized follows: Section 2, describe problem enforcingsocial behavior multi-agent systems. Section 3 describe standard game-theoreticmodel problem suggest mechanism threats punishments generaltool class problems. Issues pertain design threats punishmentsdiscussed Section 4. Section 5 introduces second case study PCMAS design:embedded teaching reinforcement learners. context, teacher learnerembedded shared environment teacher serving controller whoseaim direct learner desired behavior. formal model problemintroduced Section 6. Section 7, show derive optimal teaching policies (undercertain assumptions) viewing teaching Markov decision process. effectivenessdifferent teaching policies studied experimentally Section 8. Finally, Section 9,examine relationship methods used two domainspossibility general methodology designing partially controlled multi-agent systems.conclude Section 10, summary discussion related work.2. Enforcement Social Behaviorsection introduce problem enforcement social laws multi-agentcontext. proposed solution falls naturally PCMAS design perspectivetake. Here, explain motivate particular problem social law enforcementapproach solution. Sections 3 4 formalize investigate approachframework general game-theoretic model.use following scenario illustrate problem:hired design new working environment artificialagents. Part job involves designing number agents usemaintain warehouse. agents, designed different designers,using warehouse obtain equipment. make sure different agentsdesigned different designers operate eciently environment,choose introduce number social laws, is, constraints behavioragents, help agents coordinate activities domain.rules include number `trac laws', regulating motion domain,well law specifies every tool used agent mustreturned designated storage area. robots programmed followlaws, expect others so. laws quite successful, allow ecient activity warehouse, new designer arrives.Pressed corporate bosses deliver better performance, decidesexploit rules. designs agent locally maximize performance,regardless social laws. do?479fiBrafman & Tennenholtzmulti-participant environments, one above, agent mightdynamic goals, interested finding ways agents coexistachieving goals. Several approaches coordination agent activity discusseddistributed systems DAI literature. examples are: protocols reachingconsensus (Dwork & Moses, 1990), rational deals negotiations (Zlotkin & Rosenschein,1993; Kraus & Wilkenfeld, 1991; Rosenschein & Genesereth, 1985), organizational structures (Durfee, Lesser, & Corkill, 1987; Fox, 1981; Malone, 1987), social laws (Moses& Tennenholtz, 1995; Shoham & Tennenholtz, 1995; Minsky, 1991; Briggs & Cook, 1995).methods, behavior agent predetermined prescribedcertain stage, example, content deal reached, outcomenegotiation process completed, social law instituted. workrelies assumption agents follow prescribed behaviors, e.g., obeylaw stick agreement. assumption central successmethods. However, makes agents follow rules vulnerable rational agentperforms local maximization payoff, exploiting knowledge others followrules. example, new designer may program robot return tools,saving time required so, thus causing agents fail tasks.Despite somewhat futuristic avor (although instances shared environmentsbeginning appear cyberspace), scenario useful illustrating vulnerabilitypopular coordination mechanism appearing multi-agent literaturewithin AI (e.g., see Bond & Gasser, 1988) assume agents involvedfully rational. aside, note that, case, actually need attribute muchintelligence agents themselves, sucient assume designersdesign way maximizes utility, disregarding utilityagents.order handle problem need modify existing design paradigms.adopting perspective partially controlled multi-agent systems, obtain one possiblehandle problem, requires making following basic assumption:original designer, scenario, controls number reliable agents.1basic idea reliable agents designed punish agentsdeviate desirable social standard. punishment mechanism `hardwired' (unchangeable) common-knowledge. agents controlledoriginal designer aware punishment possibility. punishmentmechanism well designed, deviations social standard become irrational.result, deviation actually occur punishment actually executed! Hence,making agents bit sophisticated, prevent temptation breakingsocial laws.suggested solution adopt perspective partially controlled multi-agent systems. agents controllable, others uncontrollable assumedadopt basic model expected utility maximization. punishment mechanism(part of) control strategy used uence behavior uncontrolledagents.1. ease exposition, assume reliable agents follow designer's instructions; assumenon-malicious failures, crash failures, possible.480fiOn Partially Controlled Multi-Agent Systems3. Dynamic Game Theoretic Modelsection introduce basic game-theoretic model, use studyproblem enforcement social behavior solution. Later on, Sections 5{8,model used study embedded teaching. wish emphasize modeluse common model representing emergent behavior population2(e.g., Huberman & Hogg, 1988; Kandori, Mailath, & Rob, 1991; Altenberg & Feldman,1987; Gilboa & Matsui, 1991; Weidlich & Haag, 1983; Kinderman & Snell, 1980).Definition 1 k-person game g defined k-dimensional matrix size n1nk , nm number possible actions (or strategies) m'th agent. entriesvectors length k real numbers, called payoff vectors. joint strategytuple (i1; i2; : : :; ik ), 1 j k, case 1 ij nj .Intuitively, dimension matrix represents possible actions one kplayers game. Following convention used game theory, often use termstrategy place action . Since dimensions matrix n1 nk , i'thagent ni possible strategies choose from. j 'th component vector residing(i1; i2; : : :; ik ) cell (i.e., Mi1 ;i2 ;:::;ik ) represents feedback player j receivesplayers' joint strategy (i1; i2; : : :; ik ), is, agent m's strategy im1 k. Here, use term joint strategy refer combined choicestrategies agents.Definition 2 n-k-g iterative game consists set n agents given k persongame g . game g played repetitively unbounded number times. iteration,random k-tuple agents play instance game, members k-tupleselected uniform distribution set agents.Every iteration n-k-g game represents local interaction k agents. agentsplay particular iteration game must choose strategy useinteraction; agent use different strategies different interactions. outcomeiteration represented payoff vector corresponding agents' joint strategy.Intuitively, payoff tells us good outcome joint behavior pointview agent. Many situations represented n-k-g game, example,\trac" aspect multi-agent system represented n-k-g game,time number agents meet intersection. encounter instancegame agents choose number strategies, e.g., move ahead, yield.payoff function gives utility set strategies. example, timetwo agents meet agents choose move ahead, collision occurs payoffslow.Definition 3 joint strategy game g called ecient sum players' payoffsmaximal.2. paper use term emergent behavior classical mathematical-economics interpretation:evolution behavior based repetitive local interactions (usually pairs of) agents,agent may change strategy following interactions based feedback received previousinteractions.481fiBrafman & TennenholtzHence, eciency one global criterion judging \goodness" outcomessystem's perspective, unlike single payoffs describe single agent's perspective.3Definition 4 Let fixed joint strategy given game g, payoff pi(s) playeri; instance g joint strategy s0 played, pi (s) pi (s0 ) sayi's punishment w.r.t. pi (s) , pi (s0 ), otherwise say benefit w.r.t.pi(s0) , pi (s).Hence, punishment benefit w.r.t. joint strategy measure gain (benefit)loss (punishment) agent somehow change joint behavior agentss0 .current discussion punishment benefit always respect chosenecient solution.designers multi-agent system, would prefer ecient possible.cases entails behavior sense unstable, is, individual agentsmay locally prefer behave differently. Thus, agents may need constrained behaveway locally sub-optimal. refer constraints excludepossible behaviors social laws .Due symmetry system assumption agentsrational utility additive (i.e., utility two outcomes sumutilities), clear agent's expected payoff higher one obtainedusing strategies giving ecient solution. Thus, clear case ecientsolution fair, sense agents get least could lawexisted, solution provide better expected payoff.However, good intentions designer creating environment beneficialparticipating agents, may backfire. social law provides information behavioragents conforming it, information agents (or respective designers)use increase expected payoff.Example 1 Assume playing n-2-g game g prisoner's dilemma,represented strategic form following matrix.agent 2agent 1121(2,2) (-10,10)2(10,-10) (-5,-5)ecient solution game obtained players play strategy 1. Assumesolution chosen original designer, followed agentscontrol.designer new agent function environment social lawobeyed may tempted program agent conform chosen law. Instead,program agent play strategy maximizes expected outcome, strategy3. Addition payoffs utilities across agents dangerous practice. However, particular model,shown system joint-strategies always ecient maximizes agent's expectedcumulative rewards.482fiOn Partially Controlled Multi-Agent Systems#2. new agent obtain payoff 10 playing one `good' agents.Thus, even though social law accepted order guarantee payoff 2agent, `good' agents obtain payoff -10 playing non-conformingagents. Note new designer exploits information strategies `good' players,dictated social law. agents controlled new designer uncontcolableagents; behavior dictated original designer.Agents conforming social law referred malicious agents . orderprevent temptation exploit social law, introduce number punishingagents , designed initial designer, play `irrationally' detect behaviorconforming social law, attempting minimize payoff malicious agents.knowledge future participants punishment policy would deter deviations eliminate need carrying out. Hence, punishing behavior usedthreat aimed deterring agents violating social law. threat (part of)control strategy adopted controllble agents order uence behaviorunconrollable agents. Notice control strategy relies structural assumptionunconrollable agents expected utility maximizers.define minimized malicious payoff minimal expected payoff malicious players guaranteed punishing agents. punishment exists ,minimized malicious payoff lower expected payoff obtained playing accordingsocial law. strategy guarantees malicious agents expected payoff lowerone obtained playing according social law called punishing strategy .Throughout section following section make natural assumptionexpected payoff malicious agents playing greaterone obtained ecient solution4 .Example 1 (continued) Example 1, punishment would simply play strategy2 on. may cause payoff punishing agent decrease, wouldguarantee malicious agent obtains payoff better -5 playing punishingagent. many non-malicious agents punishing, malicious agents' expected payoffwould decrease become smaller payoff guaranteed social law. Strategy2 would punishing strategy.4. Design Punishmentsprevious section described general model multi-agent interaction showedperspecive partially controlled multi-agent systems leads one possible solutionproblem enforcing social behavior setting, via idea threatspunishments. proceed examine issue punishment design.assume p agents designer controls either abilityobserve instances game occur, informed outcomegames. c additional agents conform law (that is, play strategiesentailed chosen ecient solution), malicious agents, boundlaw.4. assumptions may treated similarly.483fiBrafman & Tennenholtzwould like answer questions as: game offer ability punish?minimized malicious payoff? optimal ratio p; c; m?difference different social laws?Example 1 (continued) Consider Example 1 again. observedcause expected maximal loss malicious agents 7 (= 2 , (,5)). occurspunishing agents play strategy 2. gain malicious agent makesplaying agent following social law 8 (= 10 , 2). order punishingstrategy effective, must case expected payoff malicious agentgreater expected payoff obtained following social law. orderachieve this, must ensure ratio punishing/conforming agentsmalicious agent sucient encounters punishing agents. case, assuming2 deviators meet expected benefit 0 recalling agent equallylikely meet agent, need pc > 87 make incentive deviate negative.Implementing punishment approach requires complex behavior. agentsmust able detect deviations well switch new punishing strategy.whole behavior viewed new, complex, social law. callscomplex agents carry out, makes programming task harder.Clearly, would like minimize number complex agents, keepingbenefit malicious behavior negative. Here, major question ratiobenefit deviation prospective punishment.seen example, larger punishment, smaller numbersophisticated punishing agents needed. Therefore, would like findstrategies minimize malicious agent's payoff. order requireadditional definitions.Definition 5 two person game g zero-sum game every joint strategyplayers, sum players' payoffs 0.Hence, zero-sum game, win/win situations, larger payoff oneagent, smaller payoff agent. convention, payoff matrix twoperson zero-sum game mention payoffs player 1.Definition 6 Let g two person game. Let Pig (s,t) payoff player g (where2 f1; 2g) strategies played player 1 2 respectively. projectedgame, gp , following two person zero-sum game: strategies playersg , payoff matrix P gp (s; t) = ,P2g (s,t). Define transposed game g , g ,game g roles players change.projected game, first agent's payoff equals negated value second agent'spayoff original game. Thus, game ects desire lower payoffssecond player original game.give general result two-person game, g (with number strategies).make use following standard game-theoretic definition:484fiOn Partially Controlled Multi-Agent SystemsDefinition 7 Given game g, joint strategy players Nash equilibriumg whenever player takes action different action , payoff givenplayers play higher payoff given everybody plays .is, strategy Nash equilibrium game agent obtain better payoffunilaterally changing behavior agents play according .Nash-equilibrium central notion theory non-cooperative games (Luce &Raiffa, 1957; Owen, 1982; Fudenberg & Tirole, 1991). result, notion well studiedunderstood, reducing new concepts basic concept may quite usefuldesign perspective. particular, Nash-equilibrium always exists finite games,payoffs prescribed Nash-equilibria given zero-sum game uniquely defined.show:Theorem 1 Given n-2-g iterative game, minimized malicious payoff achievedplaying strategy player 1 prescribed Nash equilibrium projected game gp,playing player 1 (in g ), strategy player 1 prescribed Nash equilibriumprojected game (g )p, playing player 2 (in g ).5Proof: Assume punishing agent plays role player 1. player 1 adoptsstrategy prescribed Nash-equilibrium player 2 get better payoffone guaranteed since deviation player 2 improve situation (bydefinition Nash-equilibrium). hand, player 1 cause harmharm obtained playing strategy . see this, assume player 1 usesarbitrary strategy s, player 2 adopts strategy prescribed . outcomeplayer 1 higher one guaranteed playing Nash-equilibrium(by definition Nash-equilibrium). addition, due factzero-sum game implies outcome player 2 lower oneguaranteed player 1 would play according . case punishing agentplayer 2 treated similarly.Example 1 (continued) Continuing prisoner's dilemma example, gp wouldagent 2agent 1 1 21-2 -10210 5Nash equilibrium attained playing strategies yielding 5. example,(g )p = gp. Therefore, punishing strategies strategy # 2 case.Corollary 1 Let n-2-g iterative game, p punishing agents. Let v v'payoffs Nash equilibria gp gpT respectively (which, case, uniquelydefined). Let b,b' maximal payoffs player 1 obtain g g respectively,5. Notice that, cases, strategies prescribed original game determined strategiesplayer 1 Nash-Equilibria projected games.485fiBrafman & Tennenholtzassuming player 2 obeying social law. Let e e' payoffs player 1 2,respectively, g , players play according ecient solution prescribedsocial law. Finally, assume expected benefit two malicious agents meet0. necessary sucient condition existence punishing strategy(n,1,p) (b + b0) , p (v + v 0 ) < (e + e0 ).n,1n,1Proof: expected payoff obtained malicious agent encountering law-abiding agent b+2b , expected payoff encountering punishing agent ,(v2+v ) .order test conditions existence punishing strategy would needconsider best case scenario point view malicious agent; casenon-punishing agents law-abiding agents. order obtain expected utilitymalicious agent make average quantities taking accountproportion law-abiding punishing agents population. gives usexpected utility malicious agent (2(n,n1,,1)p) (b + b0) , 2(np,1) (v + v 0). definition,punishing strategy exists expected utility lower expectedutility guaranteed social law. Since expected utility guaranteedsocial law e+2e , get desired result.value punishment, (v+2v ) above, independent ecient solutionchosen, e + e0 identical ecient solutions, definition. However, b + b0 dependschoice ecient solution. number solutions exist, minimizingb + b0 important consideration design social law, affects incentive`cheat'.0000Example 2 Let's look slightly different version prisoner's dilemma. gamematrixagent 2agent 1121(0,0) (-10,10)2(10,-10) (-5,-5)3 ecient solutions, given joint strategies (1,1), (1,2), (2,1).case (1,1) b+b'=20 (gained playing strategy 2 instead 1). case(2,1) (1,2) b+b'=5.Clearly, incentive deviate social law prescribing strategies (1,1)social law prescribing (2,1) (1,2).summarize, preceding discussion suggests designing number punishing agents,whose behavior punishment mode prescribed Theorem 1 case n-2-g games.ensuring sucient number agents take away incentive deviatesocial laws. Hence, given malicious agents rational, follow socialnorm, consequently, need utilize punishment mechanism.observed different social laws leading solutions equally ecient differentproperties comes punishment design. Consequently, assumptionwould like minimize number punishing agents guaranteeing ecient486fiOn Partially Controlled Multi-Agent Systemssolution participants, choose ecient solution minimizes valueb + b0.5. Embedded Teachingsection move second study PCMAS design problem; now,uncontrollable agent reinforcement learner. choice arbitrary; rationalagents reinforcement learners two major types agents studied mathematicaleconomics, decision theory, game theory. also types agents discussedwork DAI concerned self-motivated agents (e.g., Zlotkin & Rosenschein,1993; Kraus & Wilkenfeld, 1991; Yanco & Stein, 1993; Sen, Sekaran, & Hale, 1994).agent's ability function environment greatly affected knowledgeenvironment. special cases, design agents sucient knowledgeperforming task (Gold, 1978), but, general, agents must acquire information on-lineorder optimize performance, i.e., must learn. One possible approachimproving performance learning algorithms employing teacher. example,Lin (1992) uses teaching example improve performance agents, supplyingexamples show task achieved. Tan's work (1993) alsoviewed form teaching agents share experiences. methods nontrivial form communication perception required. strive model broad notionteaching encompasses behavior improve learning agent's performance.is, wish conduct general study partially controlled multi-agent systemsuncontrollable agent runs learning algorithm. time, wantmodel clearly delineate limits teacher's (i.e., controlling agent's) abilityuence student.Here, propose teaching approach maintains situated \spirit" much likereinforcement learning (Sutton, 1988; Watkins, 1989; Kaelbling, 1990), callembedded teaching . embedded teacher simply \knowledgeable" controlled agentsituated student shared environment. Her6 goal lead studentadopt specific behavior. However, teacher's ability teach restrictednature environment share: repertoire actions limited,may also lack full control outcome actions. example, considertwo mobile robots without means direct communication. Robot 1 familiarsurroundings, Robot 2 not. situation, Robot 1 help Robot 2 reachgoal certain actions, blocking Robot 2 headed wrongdirection. However, Robot 1 may limited control outcomeinteraction uncertainty behavior Robot 2 control uncertainty.Nevertheless, Robot 2 specific structure, learner obeying learning scheme,attempt control indirectly choice actions Robot 1.76. differentiate teacher student, use female pronouns former male pronounslatter.7. general, fact agent controllable imply perfectly control outcomeactions, choice. Hence, robot may controllable sense, running programsupplied us, yet move-forward command may always desired outcome.487fiBrafman & Tennenholtzfollows, goal understand embedded teacher help studentadopt particular behavior. address number theoretical questions relatingproblem, experimentally explore techniques teaching two typesreinforcement learners.6. Basic Teaching Settingconsider teacher student repeatedly engage joint activity.student prior knowledge pertaining activity, teacher understandsdynamics. model, teacher's goal lead student adopt particularbehavior interactions. example, teacher student meet occasionallyroad teacher wants teach student drive right side. perhaps,teacher student share resource, CPU time, goal teachjudicious use resource. model encounters 2-2-g iterative games.capture idea teacher knowledgeable student, assumeknows structure game, i.e., knows payoff function,recognizes actions taken play. hand, student knowpayoff function, although perceive payoff receives. paper, makesimplifying assumptions teacher student two actionschoose outcome depends choice actions. Furthermore,excluding study Section 8.4, ignore cost teaching, hence, omitteacher payoff description.8 provides basic setting takefirst step towards understanding teaching problem.9teaching model concisely modeled 2 2 matrix. teacher's actionsdesignated II , student's actions designated numbers 12. entry corresponds joint action represents student's payoffjoint action played. suppose matrix Figure 1,wish teach student use action 1. stage, assume studentalways receives better payoff following action 1 learn play it.see situations teaching trivial. Assume first row dominates second row, i.e., > c b > d. case, student naturally prefertake action 1, teaching challenging, although might useful speedinglearning process. example, , c > b , d, matrix B Figure 1, teachermake advantage action 1 noticeable student always playing actionI.suppose one > c b > holds. case, teaching still easy.use basic teaching strategy, call preemption . preemption teacherchooses action makes action 1 look better action 2. example,situation described matrix C Figure 1, teacher always choose action .8. case could made inherent value teaching, may appropriate forumairing views.9. fact, idea consider basic embedded teaching setting already challenging. later see, basic setting closely related fundamental issue non-cooperativegames.488fiOn Partially Controlled Multi-Agent SystemsIIII1 b1 6 52 c(A)II1 5 12 2 6(C)2 1 2(B)II1 3 -22 5 6(D)II1 5 -102 10 -5(E)Figure 1: Game matrices A, B, C, D, E. teacher's possible actions II ,student's possible actions 1 2.Next, assume c greater b, matrix Figure 1.Regardless action teacher chooses, student receives higher payoffplaying action 2 (since minf5; 6g > maxf3; ,2g). Therefore, matter teacherdoes, student learn prefer action 2. Teaching hopeless situation.types interactions isomorphic case c > > > b,matrix E Figure 1. still challenging situation teacher action 2dominates action 1 (because 10 > 5 ,5 > ,10). Therefore, preemption cannot work.teaching strategy exists, complex always choosing action.Since seems challenging teaching situation, devote attentionteaching reinforcement learner choose action 1 class games.turns situation quite important game-theory multi-agentinteraction. projection famous game, prisoner's dilemma, discussedprevious sections. general, represent prisoner's dilemma usingfollowing game matrix:teacherstudent Coop Defectstudent CoopCoop (a,a) (b,c) commonly Coop (a,a)Defect (c,b) (d,d)Defect (c,-c)teacherDefect(-c,c)(d,d)c > > > b. actions prisoner's dilemma called Cooperate (Coop)Defect; identify Coop actions 1 , Defect actions 2 II .prisoner's dilemma captures essence many important social economic situations;particular, encapsulates notion cooperation. thus motivated enormous discussion among game-theorists mathematical economists (for overview, see Eatwell,Milgate, & Newman, 1989). prisoner's dilemma, whatever choice one player,second player maximize payoff playing Defect. thus seems \rational"player defect. However, players defect, payoffs much worsecooperate.489fiBrafman & Tennenholtzexample, suppose two agents given $10 moving object.agent perform task alone, take amount time energyvalue $20. However, together, effort make valued $5. getfollowing instance prisoner's dilemma:Agent 1Agent 2 MoveRestMove(5,5) (-10,10)Rest (10,-10) (0,0)experimental part study, teacher's task teach studentcooperate prisoner's dilemma. measure success teaching strategylooking cooperation rate induces students period time, is,percentage student's actions Coop. experimental results presentedpaper involving prisoner's dilemma respect following matrix:TeacherStudent Coop DefectCoop (10,10) (-13,13)Defect (13,-13) (-6,-6)observed qualitatively similar results instantiations prisoner'sdilemma, although precise cooperation rate varies.7. Optimal Teaching Policiesprevious section concentrated modeling teaching context instancepartially controlled multi-agent system, determining particular problemsinteresting. section start exploring question teacherteach. First, define optimal policy is. Then, define Markov decisionprocesses (MDP) (Bellman, 1962), show certain assumptions teachingviewed MDP. allow us tap vast knowledge accumulatedsolving problems. particular, use well known methods, valueiteration (Bellman, 1962), find optimal teaching policy.start defining optimal teaching policy. teaching policy functionreturns action iteration; possibly, may depend complete historypast joint actions. \right" definition optimal policy, teacher'smotivation may vary. However, paper, teacher's objective maximizenumber iterations student's action \good", Coop prisoner'sdilemma. teacher know precise number iterations playing,slightly prefers earlier success later success.formalized follows: Let u(a) value teacher places student'saction, a, let teacher's policy, assume induces probability distributionPr;k set possible student actions time k. define value strategy1Xval( ) = kEk (u)k=0490fiOn Partially Controlled Multi-Agent SystemsEk (u) expected value u:Ek (u) =X Pra2As;k (a) u(a)Here, student's set actions. teacher's goal find strategymaximizes val(), discounted expected value student's actions. example,case prisoner's dilemma, could= fCoop,Defectg u(Coop) = 1 u(Defect) = 0.Next, define MDPs. MDP, decision maker continually movingdifferent states. point time observes current state, receives payoff(which depends state), chooses action. action current statedetermine (perhaps stochastically) next state. goal maximize functionpayoffs. Formally, MDP four-tuple hS; A; P; ri, state-space,decision-maker's set possible actions, P : ! [0; 1] probabilitytransition states given decision-maker's action, r : ! < rewardfunction. Notice given initial state 2 , policy decision maker , Pinduces probability distribution Ps;;k , Ps;;k (s0 ) probabilitykth state obtained s0 current state s.0-optimal policy MDP policy maximizes statediscounted sum expected values payoffs received future states, startings, i.e.,1XX0k( Ps;;k (s0 ) r(s0))k=02S0Although may immediately obvious, single policy maximizing discounted sumsstarting state exists, well-known ways finding policy.experiments use method based value-iteration (Bellman, 1962).suppose student set possible states, set actions, teacher's set actions . Moreover, suppose followingproperties satisfied:(1) student's new state function old state current joint-action,denoted : ! ;(2) student's action stochastic function current state, probabilitychoosing state (s; a);(3) teacher knows student's state. (The natural way happenteacher knows student's initial state, function , outcome game,uses simulate agent.)Notice assumptions teaching policy function :know student's next action function next state. knowstudent's next state function current state, current action, teacher'scurrent action. Hence, next action function current state action,well teacher's current action. However, know student's current actionfunction current state. Hence, student's next action functioncurrent state teacher's current action. implies knowledgeteacher needs optimally choose current action student's current state,491fiBrafman & Tennenholtzadditional information redundant cannot improve success. generally,repeat line reasoning indefinitely future, see teacher'spolicy function student's state: function .possible see makings following MDP.Given observation three assumptions, see that, indeed, teacher'spolicy induces probability distribution set possible student actions time k.implies definition val makes sense here.Define teacher's MDP TMDP= h; At; P; U i,XP (s; s0; at) def=2As(s; as) ; (s;as;at)0(i;j defined 1 = j , 0 otherwise). is, probability transitions0 sum probabilities student's actions inducetransition. reward function expected value u:XU (s) def=as2As(s; as) u(as)Theorem 2 optimal teaching policy given 0 optimal policy TMDP.Proof: definition, 0 optimal policy TMDP policy 2maximizesis,1XXk( P0k=02s;;k (s0 ) U (s0 ))01XXk( Pk=0However, equal()00s;;k (s ) (20Xas2As(s0 ; as) u(as)))1XX X (s0; ) Pkk=00as2As 200s;;k (s ) u(as )know Ps;;k (s0 ) probability s0 state student timek, given teacher uses current state s. Hence,X (s0; ) P200s;;k (s )probability action taken student time k given initial(current) state s. Upon examination, see (*) identical val( ).optimal policy used teaching, teacher possess sucient information determine current state student. even case,allows us calculate upper bound success val( ) teaching policy .number property learning algorithm, measures degree uenceagent given student.492fiOn Partially Controlled Multi-Agent Systems8. Experimental Studysection describe experimental study embedded teaching. First, definelearning schemes considered, then, describe set results obtained using computersimulations.8.1 Learning Schemesexperiment two types students: One uses reinforcement learning algorithmviewed Q-learning one state, uses Q-learning. choosingparameters students tried emulate choices made reinforcement learningliterature.first student, call Blind Q-learner (BQL), perceive rewards,cannot see teacher acted remember past actions. keepsone value action, example, q (Coop) q (Defect) case prisoner'sdilemma. update rule following: performed action received rewardRqnew (a) = (1 , ff) qold(a) + ff Rparameter ff, learning-rate, fixed (unless stated otherwise) 0:1 experiments. wish emphasize although BQL bit less sophisticated \real"reinforcement learners discussed AI literature (which defined below), popular powerful type learning rule, much discussed used literature(Narendra & Thathachar, 1989).second student Q-learner (QL). observe teacher's actionsnumber possible states. QL maintains Q-value state-action pair.states encode recent experiences, i.e., past joint actions. update rule is:qnew (s; a) = (1 , ff) qold (s; a) + ff (R + V (s0))R reward received upon performing state s; s0 state studentfollowing performance s; called discount factor, 0:9, unlessotherwise noted; V (s0) current estimate value best policy s0 ,defined maxa2As q (s0 ; a). Q-values initially set zero.student's update rule tells us Q-values change result new experiences. must also specify Q-values determine behavior. QLBQL students choose actions based Boltzmann distribution. distributionassociates probability Ps (a) performance action state (P (a)BQL).exp(q (a)=T )q(s; a)=T )defP(QL)P()=(BQL)Ps(a) def= P exp(exp(0q (s; )=T )2A2A exp(q (a0)=T )called temperature . Usually, one starts high value ,makes action choice random, inducing exploration part student.slowly reduced, making Q-values play greater role student's choice action.use following schedule: (0) = 75 (n +1) = (n) 0:9+0:05. schedulecharacteristic properties fast initial decay slow later decay. also experimentfixed temperature.00493fiBrafman & TennenholtzApproximately Optimal Policy110000 iterations5000 iterations1000 iterations100 iterations0.8Fraction CoopsFraction Coops1Two Q-learners0.60.40.210000 iterations5000 iterations1000 iterations100 iterations0.80.60.40.20001234 5 6 7Temperature89 1001234 5 6 7Temperature89 10Figure 2: Fraction Coops function temperature approximately optimalpolicy (left) \teaching" using identical Q-learner (right). curvecorresponds Coop rate fixed number iterations. approx.optimal policy curves 1000, 5000 10000 iterations nearly identical.8.2 Blind Q-LearnersMotivated discussion Section 6 concentrate sectionfollowing section teaching context prisoner's dilemma. Section 8.4discuss another type teaching setting. section describes experimental resultsBQL. examined policy approximates optimal policy, two teachingmethods rely student model.8.2.1 Optimal PolicyFirst show BQLs fit student model Section 7. state space, useset possible assignment Q-values. continuous subspace <2,discretize (in order able compute policy), obtaining state spaceapproximately 40,000 states. Next, notice transitions stochastic functioncurrent state (current Q-values) teacher's action. see notice Q-valueupdates function current Q-value payoff; payoff functionteacher's student's actions; student's actions stochastic functioncurrent Q-value. left side Figure 2 see success teaching using policygenerated using dynamic programming solve optimization problem. curverepresents fraction Coops function temperature fixed numberiterations. values means 100 experiments.8.2.2 Two Q-Learnersalso ran experiments two identical BQLs. viewed \teaching" usinganother Q-learner. results shown right side Figure 2. temperaturesoptimal strategy performs better Q-learning \teaching" strategy. facttemperatures 1.0 less success rate approaches 1 beneficial lateradd temperature decay. However, also see inherent limit ability494fiOn Partially Controlled Multi-Agent SystemsTit-For-Tat110000 iterations5000 iterations1000 iterations100 iterations0.8Fraction CoopsFraction Coops12-Tit-For-Tat0.60.40.210000 iterations5000 iterations1000 iterations100 iterations0.80.60.40.20001234 5 6 7Temperature89 1001234 5 6 7Temperature89 10Figure 3: Fraction Coops function temperature teaching strategy basedTFT (left) 2TFT (right).affect behavior higher temperatures. interesting phenomenon phasetransition observed around = 2:5. qualitative explanation phenomenonhigh temperature adds randomness student's choice action, makesprobabilities P (a) less extreme. Consequently, ability predict student's behaviorlessens, probability choosing good action. However, randomnessserves lower success rate initially, also guarantees level effective cooperation,approach 0.5 temperature increases. Finally, notice although(Coop,Coop) seems like best joint-action pair agents, two interacting Q-learnersnever learn play joint strategy consistently, although approach 80% Coopslow temperatures.8.2.3 Teaching Without Modelteacher precise model student, cannot use techniquesSection 7 derive optimal policy; models, assume teacher\observe" student's current state (i.e. knows student's Q-values).therefore explore two teaching methods exploit knowledge gamefact student BQL.methods motivated basic strategy countering student's move.basic idea try counter good actions student action leadhigh payoff, counter bad actions action give low payoff.Ideally, would like play Coop student plays Coop, Defectstudent plays Defect. course, don't know action student choose,try predict past actions.assume Q-values change little one iteration other,student's likely action next game action tookrecent game. Therefore, saw student play Coop previous turn, playCoop . Similarly, teacher follow Defect student Defect495fiBrafman & TennenholtzFraction Coops timeFraction Coops10.90.80.7approximately optimalQ-learningTit-For-Tat2-Tit-For-Tat0.60.520004000 6000Iterations800010000Figure 4: Fraction Coops function time BQL using temperature decayscheme Section 8.1. Teaching strategies shown: approximately optimal strategy, Q-learning, TFT, 2TFT.part. strategy, called Tit-For-Tat (TFT short), well known (Eatwell et al., 1989).experiments show successful teaching BQL (see Figure 3).also experimented variant TFT, call 2TFT. strategyteacher plays Defect observing two consecutive Defects part student.motivated observation certain situations better let studentenjoy free lunch (that is, match Defect Coop) make Coop look badhim, may cause Q-value Coop low unlikelytry again. Two consecutive Defects indicate probability student playingDefect next quite high. results, shown Figure 3, indicate strategy workedbetter TFT, ranges temperature, better Q-learning. However,general, TFT 2TFT gave disappointing results.10Finally, Figure 4 shows performance four teaching strategies discussedfar incorporate temperature decay. see optimal policysuccessful. explained before, teaching easier student predictable,case temperature lower. temperature decay student spendstime relatively low temperature behaves similarly case fixed,low temperature. initial high-temperature phase could altered behavior,observe effects.8.3 Teaching Q-LearnersUnlike BQL, Q-learners (QL) number possible states encode joint actionsprevious games played. QL memory one four possible states, correspondingfour possible joint actions prisoner's dilemma; QL memorystates, encoding sequence joint actions.complex learning architectures structure, brings certainproblems. One possible problem may structure \teaching-resistant."10. sense use identical Q-learner implies model student, TFT2TFT make use model.496fiOn Partially Controlled Multi-Agent SystemsTit-For-TatTwo Q-learners1Fraction CoopsFraction Coops10.80.60.410000 iterations5000 iterations1000 iterations100 iterations0.210000 iterations5000 iterations1000 iterations100 iterations0.80.60.40.20001234 5 6 7Temperature89 1001234 5 6 7Temperature89 10Figure 5: curve shows fraction Coops QL function temperaturefixed number iterations TFT used teach (left) identical Q-learner used teach (right). Values means 100 experiments.real threat added computational complexity. mentioned, approximateoptimal teaching policy BQL compute space approximately 40,000discretized states. representing state BQL requires two numbers, oneQ-value, representing state QL states requires 2m + 1 numbers:one Q-value state/action pair, one encoding current state. sizecorresponding discretized state-space teacher's Markov decision process growsexponentially m. simplest case memory one (a student four states)would 1018 states. Since solving problem 40,000 states took 12 hourssun sparcstation-10, able approximate optimal teaching policieseven simplest QL.lost. structure may mean complexity, also meansproperties exploit. reach surprisingly good results exploiting structureQ-learners. Moreover, using teaching method introduced previoussection. However, QL method takes new meaning suggests familiarnotions reward punishment. Interestingly, one may recall punishmentmajor tool approach enforcement social behavior.choosing actions, QLs \care" immediate rewards, alsocurrent action's effect future rewards. makes suitable rewardpunishment scheme. idea following: suppose QL something \bad" (Defectcase). Although cannot reliably counter move move lowerreward, punish later choosing action always gives negativepayoff, matter student plays. achieve following student's DefectDefect teacher. immediate reward obtained QL playing Defectmay high, also learn associate subsequent punishment Defect action.Thus, may locally beneficial perform Defect, may able makelong-term rewards Defect less desirable. Similarly, follow student's Coopreward form Coop teacher, since guarantees positive payoffstudent.497fiBrafman & TennenholtzFraction Coops timeFraction Coops1Tit-For-TatQ-learning0.90.80.70.60.520004000 6000Iterations800010000Figure 6: Fraction Coops QL function time temperature decay TFTQ-learning teaching strategies.suggests using Tit-For-Tat again. Notice BQLs, TFT cannot understoodreward/punishment strategy BQLs care immediate outcomeaction; value associate action weighted average immediatepayoffs generated playing action.Figure 5 see success rates TFT function temperature, wellrates Q-learning teaching strategy. latter case, teacher identicalstudent. apparent TFT extremely successful, especially higher temperatures.Interestingly, behavior quite different two QLs. Indeed, examinebehavior two QLs, see that, lesser extent, phase change noticedBQLs still exists. obtain completely different behavior TFT used: Coop levelsincrease temperature, reaching almost 100% 3.0. Hence, see TFT worksbetter student Q-learner exhibits certain level experimentation. Indeed,examine success teaching strategies low temperature, seeQ-learning performs better TFT. explains behavior TFT QLtemperature decay introduced, described Figure 6. figure, QL seemseffective TFT. probably result fact experimentstudent's temperature quite low time.experiments QL remembers last joint action. experimentedQL memory performance worse. explained follows.QL memory one more, problem fully observable Markov decision processteacher plays TFT, TFT deterministic function previous joint action.know Q-learning converges optimal policy conditions (Watkins &Dayan, 1992). Adding memory effectively adds irrelevant attributes, which, turn,causes slower learning rate. also examined whether 2TFT would successfulagents memory two. results shown here, success rateconsiderably lower TFT, although better two QLs.TFT performed well teaching strategy, explained motivation using it.want produce quantitative explanation, one used predictsuccess vary various parameters, payoff matrix.498fiOn Partially Controlled Multi-Agent SystemsCoop rates function DIFFraction Coops10.80.60.40.20-20 -1001020 30DIF405060Figure 7: Coop rates function DIF = + b + (a + c) , (c + + (b + d)). means100 experiments, 10000 iterations each. Student's memory 1.Let student's payoff matrix matrix Figure 1; let p probabilitystudent plays Coop, let q = 1 , p probability student playsDefect. probabilities function student's Q-values (see descriptionSection 8.1). Let us assume probabilities p q change considerablyone iteration next. seems especially justified learning rate, ff, small.Given information, student's expected reward playing Coop?TFT, teacher's current action student's previous action, also assumeteacher play Coop probability p. Thus, student's expected payoffplaying Coop (p + q b). Since Q-learners care discounted future reward(not current reward), happens next also important. Since assumedstudent cooperated, teacher cooperate next iteration, stillassume p probability student cooperate next, student's expectedpayoff next step (p + q c). ignore higher order terms expected rewardplaying Coop becomes: p + q b + (p + q c): expected reward Defect thus:p c + q + (p b + q d): Therefore, TFT succeed teaching strategy when:p + q b + (p + q c) > p c + q + (p b + q d):Since initially p = q = 0:5, behavior stage p q approximately equal determine whether TFT succeeds, attempt predictsuccess TFT based whether:DIF = + b + (a + c) , [(c + + (b + d))] 0test hypothesis ran TFT number matrices using Q-learners differentdiscount factors. results Figure 7 show fraction Coops 10000 iterationsfunction DIF teacher using TFT, temperature decay. seeDIF reasonable predictor success. 0, almost rates20%, 8 rates 65%. However, 0 8 successful.499fiBrafman & Tennenholtz8.4 Teaching Design ToolSection 6 identified class games challenging teach, previoussections mostly devoted exploring teaching strategies gamesstudent Q-learner. One assumptions made teacher tryingoptimize function student's behavior careorder achieve optimal behavior. However, often teacher would like maximizefunction depends behavior student's behavior.case, even simple games discussed Section 6 pose challenge.section, examine basic coordination problem, block pushing,objective teaching, teaching essential obtaining good results.aim section demonstrate point, hence value understandingembedded teaching. results show teaching strategy achieves muchbetter performance naive teaching strategy leads behavior much bettertwo reinforcement learners.Consider two agents must push block far possible along given pathcourse 10,000 time units. time unit agent push block alongpath, either gently (saving energy) hard (spending much energy). blockmove iteration c x h + (2 , x) h units desired direction, h; c > 0constants x number agents push hard. iteration, agentspaid according distance block pushed. Naturally, agents wish worklittle possible paid much possible, payoff iterationfunction cost pushing payment received. assume agent prefersblock pushed hard least one agents (guaranteeing reasonablepayment), agent also prefers agent one pushing hard.denote two actions gentle hard, get related game describedfollows:hard gentlehard (3,3) (2,6)gentle (6,2) (1,1)Notice game falls category games teaching easy.teacher cares student learn push hard, simply pushgently. However, teacher actually trying maximize distanceblock moved, teaching strategy may optimal. Notice20000 instances hard push; naive teaching strategy mentioned yield10000 instances hard push. order increase number, needcomplex teaching strategy.results use BQL ff = 0:001. Consider following strategyteacher: push gently K iterations, start push hard. see,right selection K , obtain desired behavior. student push hardtime, total number hard push instances improve dramatically.Figure 8, x coordinate corresponds parameter K , coordinatecorresponds number hard push instances occur 10000 iterations.results obtained average results 50 trials.500fiOn Partially Controlled Multi-Agent Systems170001600015000140001300012000110002000400060008000Figure 8: Teaching push hard: number hard push instances student 10000iterations function number iterations teacherpush hard (avg. 50 trials).see Figure 8, eciency system non-monotonicthreshold K . behavior obtain appropriate selection K much betterwould obtained naive teaching strategy. interestingnote existence sharp phase transition performance neighborhoodoptimal K . Finally, mention agents reinforcement learners,get 7618 instances \push hard", much worse obtainedknowledgeable agent utilizes knowledge uence behavioragent.9. Towards General Theorytwo case studies presented paper raise natural question whether general,domain independent techniques PCMAS design exist, whether learnedtools case studies. believe still premature say whethergeneral theory PCMAS design emerge; requires much additional work. Indeed,given considerable differences exist two domains exploredpaper, given large range multi-agent systems agents envisioned,doubt existence common low-level techniques PCMAS design. Even withinclass rational agents investigated, agents differ considerablyphysical, computational, memory capabilities, approach decision making(e.g., expected utility maximization, maximization worst-case outcomes, minimizationregret). Similarly, problem social-law enforcement take different forms,example, malicious agents could cooperate among other. However,abstract view taken, certain important unifying concepts appear, namely, punishmentreward.Punishment reward abstract descriptions two types high-level feedbackscontrollable agents provide uncontrollable agents. Although punishmentreward take different form meaning two domains, cases, uncon501fiBrafman & Tennenholtztrollable agents seem \care" controllable agent's reaction action.see cases, controllable agents uence uncontrollable agents'perception worthiness actions. precise manner controllableagents affect perception differs, cases utilizes inherent aspectuncertainty uncontrollable agent's world model. case rational agents, despiteperfect knowledge dynamics world, uncertainty remains regardingoutcome non-malicious agents' actions. fixing certain course actioncontrollable agents, uence malicious agents' perception outcomeactions. case learning agent, one affect perception student'saction affecting basic world model. Hence, seems high-level approach PCMAS design two stages: First, analyze factors uence uncontrollableagent's perception actions. Next, analyze ability control factors.retrospect, implicit approach. study social-law enforcement,used projected game find agent's perception actionchanged used indirect mechanism threats enforce perception desired.study embedded teaching, started analysis different gamespossibility affecting agent's perception action games. Next, triedprovide perception. case BQL students, controllable teachercomplete control elements determine student's perceptionrandom nature student's action. Yet, try somehow affect them.case Q-learners, direct control available factors determiningstudent's perception. Yet, teacher could control aspects perception,found sucient.One might ask representative studies general PCMAS domains,therefore, relevant insight may provide. chosen two domainsbelief represent key aspects types agents studied AI.AI, study dynamic agents act improve state. agents likelyuse information revise assessment state world, much like learningagents, need make decisions based current information, much likeexpected utility maximizers studied. Hence, typical multi-agent systems studiedAI include agents exhibit one properties.punishment rewards provide conceptual basis designing controllable agents, MDPs supply natural model many domains. particular, MDPssuitable uncertainty exists, stemming either agents' choicesnature. showed Section 7, least principle, use established techniquesobtain strategies controllable agents problem phrased Markovdecision process. Using MDP perspective cases would require sophisticates tools number important challenges must met first: (1) assumptionsagent's state fully observable environment's state fully observableunrealistic many domains. assumptions invalid, obtain partiallyobservable Markov decision process (POMDP) (Sondik, 1978). Unfortunately, althoughPOMDPs used principle obtain ideal policy agents, current techniques solving POMDPs limited small problems. Hence, practice oneresort heuristic punishment reward strategies. (2) Section 7502fiOn Partially Controlled Multi-Agent Systemsone controlling agent. poses natural challenge generalizing tools techniquesMDPs distributed decision making processes.10. Summary Related Workpaper introduces distinction controllable uncontrollable agentsconcept partially controlled multi-agent systems. provides two problems multi-agentsystem design naturally fall framework PCMAS design suggests concretetechniques uencing behavior uncontrollable agents domains.work contributes AI research introducing exploring promising perspectivesystem design contributes DES research considering two types structuralassumptions agents, corresponding rational learning agents.application approach enforcement social behavior introduces newtool design multi-agent systems, punishment threats. used notioninvestigated part explicit design paradigm. Punishment, deterrence, threatsstudied political science (Dixit & Nalebuff, 1991; Schelling, 1980); yet,difference line work (and related game-theoretic models), consider casedynamic multi-agent system concentrate punishment design issues,question minimizing number reliable agents needed control system. Unlikemuch work multi-agent systems, assume agents rational agentslaw-abiding. Rather, assumed designer control agentsdeviations social laws uncontrolled agents need rational.Notice behavior controllable agents may considered irrational cases;however, eventually lead desired behavior agents. approachesnegotiations viewed incorporating threats. particular, RosenscheinGenesereth (1985) consider mechanism making deals among rational agents, agentsasked offer joint strategy followed agents declare movewould take agreement joint strategy. latter move viewedthreat describing implications refusing agent's suggested joint strategy.example, prisoner's dilemma setting agent may propose joint cooperationthreaten defecting otherwise. work first part paper could viewedexamining threat could credible effective particular contextiterative multi-agent interactions.part study, proposed embedded teaching situated teaching paradigmsuitable modeling wide range teaching instances. modeled teacherstudent players iterated two-player game. concentrated particulariterative game, showed challenging game type. model,dynamics teacher-student interaction made explicit, clearly delineatedlimits placed teacher's ability uence student. showeddetailed model student, optimal teaching policies theoretically generatedviewing teaching problem Markov decision process. performanceoptimal teaching policy serves bound agent's ability uence student.examined ability teach two types reinforcement learners. particular,showed optimal policy cannot used, use TFT teaching method.case Q-learners policy successful. Consequently, proposed model503fiBrafman & Tennenholtzexplains success. Finally, showed even games teachingchallenging, nevertheless quite useful. Moreover, objectivesimply teaching student, even simpler domains present non-trivialchoices. future hope examine learning architectures see whetherlessons learned domain generalized, whether use methodsaccelerate learning domains.number authors discussed reinforcement learning multi-agent systems.Yanco Stein (1993) examine evolution communication among cooperative reinforcement learners. Sen et al. (1994) use Q-learning induce cooperation twoblock pushing robots. Matraic (1995) Parker (1993) consider use reinforcementlearning physical robots. consider features real robots, discussedpaper. Shoham Tennenholtz (1992) examine evolution conventionssociety reinforcement learners. Kittock (1994) investigates effects societal structure multi-agent learning. Littman (1994) develops reinforcement learning techniquesagents whose goals opposed, Tan (1993) examines benefit sharing informationamong reinforcement learners. Finally, Whitehead (1991) shown n reinforcementlearners observe everything decrease learning time factorn. However, work concerned teaching, questionmuch uence one agent another. Lin (1992) explicitly concernedteaching way accelerating learning enhanced Q-learners. uses experience replay supplies students examples task achieved. remarkedearlier, teaching approach different ours, since teachers embeddedstudent's domain. Within game theory extensive body work triesunderstand evolution cooperation iterated prisoner's dilemma findgood playing strategies (Eatwell et al., 1989). work playersknowledge, teaching issue.Last least, work important links work conditioning especiallyoperant conditioning psychology (Mackintosh, 1983). conditioning experimentsexperimenter tries induce changes subjects arranging certain relationshipshold environment, explicitly (in operant conditioning) reinforcingsubjects' actions. framework controlled agent plays similar roleexperimenter. work uses control-theoretic approach related problem,applying two basic AI contexts.main drawback case studies simple domains conducted. typical initial exploration new problems, future work tryremove limiting assumptions models incorporate. example,embedded teaching context, assumed uncertainty outcome joint action. Similarly, model multi-agent interaction Section 3symmetric, assuming agents play k roles game, equallylikely play role, etc. Another assumption made malicious agents\loners" acting own, opposed team agents. Perhaps importantly,future work identify additional domains naturally described termsPCMAS formalize general methodology solving PCMAS design problems.504fiOn Partially Controlled Multi-Agent SystemsAcknowledgementsgrateful Yoav Shoham members Nobotics group Stanfordinput, anonymous referees productive comments suggestions.especially grateful James Kittock comments help improvingpresentation paper. research supported fund promotionresearch Technion, NSF grant IRI-9220645, AFOSR grant AF F49620-941-0090.ReferencesAltenberg, L., & Feldman, M. W. (1987). Selection, generalized transmission,evolution modifier genes. i. reduction principle. Genetics, 559{572.Bellman, R. (1962). Dynamic Programming. Princeton University Press.Bond, A. H., & Gasser, L. (1988). Readings Distributed Artificial Intelligence. AblexPublishing Corporation.Briggs, W., & Cook, D. (1995). Flexible Social Laws. Proc. 14th International JointConference Artificial Intelligence, pp. 688{693.Dixit, A. K., & Nalebuff, B. J. (1991). Thinking strategically : competitive edgebusiness, politics, everyday life. Norton, New York.Durfee, E. H., Lesser, V. R., & Corkill, D. D. (1987). Coherent Cooperation Among Communicating Problem Solvers. IEEE Transactions Computers, 36, 1275{1291.Dwork, C., & Moses, Y. (1990). Knowledge Common Knowledge Byzantine Environment: Crash Failures. Information Computation, 88 (2), 156{186.Eatwell, J., Milgate, M., & Newman, P. (Eds.). (1989). New Palgrave: Game Theory.W.W.Norton & Company, Inc.Fox, M. S. (1981). organizational view distributed systems. IEEE Trans. Sys., Man.,Cyber., 11, 70{80.Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.Gilboa, I., & Matsui, A. (1991). Social stability equilibrium. Econometrica, 59 (3),859{867.Gold, M. (1978). Complexity Automaton Identificaion Given Data. InformationControl, 37, 302{320.Huberman, B. A., & Hogg, T. (1988). Behavior Computational Ecologies.Huberman, B. A. (Ed.), Ecology Computation. Elsevier Science.Kaelbling, L. (1990). Learning embedded systems. Ph.D. thesis, Stanford University.505fiBrafman & TennenholtzKandori, M., Mailath, G., & Rob, R. (1991). Learning, Mutation Long EquilibriaGames. Mimeo. University Pennsylvania, 1991.Kinderman, R., & Snell, S. L. (1980). Markov Random Fields Applications.American Mathematical Society.Kittock, J. E. (1994). impact locality authority emergent conventions.Proceedings Twelfth National Conference Artificial Intelligence (AAAI '94),pp. 420{425.Kraus, S., & Wilkenfeld, J. (1991). Function Time Cooperative Negotiations.Proc. AAAI-91, pp. 179{184.Lin, F., & Wonham, W. (1988). Decentralized control coordination discrete-eventsystems. Proceedings 27th IEEE Conf. Decision Control, pp. 1125{1130.Lin, L. (1992). Self-improving reactive agents based reinforcement learning, planning,teaching. Machine Learning, 8 (3{4).Littman, M. (1994). Markov games framework multi-agent reinforcement learning.Proc. 11th Int. Conf. Mach. Learn.Luce, R. D., & Raiffa, H. (1957). Games Decisions- Introduction Critical Survey.John Wiley Sons.Mackintosh, N. (1983). Conditioning Associative Learning. Oxford University Press.Malone, T. W. (1987). Modeling Coordination Organizations Markets. ManagementScience, 33 (10), 1317{1332.Mataric, M. J. (1995). Reward Functions Accelerating Learning. Proceedings11th international conference Machine Learning, pp. 181{189.Minsky, N. (1991). imposition protocols open distributed systems. IEEETransactions Software Engineering, 17 (2), 183{195.Moses, Y., & Tennenholtz, M. (1995). Artificial social systems. Computers ArtificialIntelligence, 14 (6), 533{562.Narendra, K., & Thathachar, M. A. L. (1989). Learning Automata: Introduction.Prentice Hall.Owen, G. (1982). Game Theory (2nd Ed.). Academic Press.Parker, L. E. (1993). Learning Cooperative Robot Teams. Proceedings IJCAI-93Workshop Dynamically Interacting Robots.Ramadge, P., & Wonham, W. (1989). Control Discrete Event Systems. ProceedingsIEEE, 77 (1), 81{98.Rosenschein, J. S., & Genesereth, M. R. (1985). Deals Among Rational Agents. Proc.9th International Joint Conference Artificial Intelligence, pp. 91{99.506fiOn Partially Controlled Multi-Agent SystemsSchelling, T. (1980). Strategy Con ict. Harvard University Press.Sen, S., Sekaran, M., & Hale, J. (1994). Learning coordinate without sharing information.Proc. AAAI-94, pp. 426{431.Shoham, Y., & Tennenholtz, M. (1992). Emergent Conventions Multi-Agent Systems:initial experimental results observations. Proc. 3rd International Conference Principles Knowledge Representation Reasoning, pp. 225{231.Shoham, Y., & Tennenholtz, M. (1995). Social Laws Artificial Agent Societies: Off-lineDesign. Artificial Inteligence, 73.Sondik, E. J. (1978). optimal control partially observable markov processesinfinite horizon: Discounted costs. Operations Research, 26 (2).Sutton, R. (1988). Learning predict method temporal differences. Mach. Lear.,3 (1), 9{44.Tan, M. (1993). Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents.Proceedings 10th International Conference Machine Learning.Watkins, C. (1989). Learning Delayed Rewards. Ph.D. thesis, Cambridge University.Watkins, C., & Dayan, P. (1992). Q-learning. Machine Learning, 8 (3/4), 279{292.Weidlich, W., & Haag, G. (1983). Concepts Models Quantitative Sociology;Dynamics Interacting Populations. Springer-Verlag.Whitehead, S. (1991). complexity analysis cooperative mechanisms reinforcementlearning. Proceedings AAAI-91, pp. 607{613.Yanco, H., & Stein, L. (1993). Adaptive Communication Protocol CooperatingMobile Robots. Animal Animats: Proceedings Second InternationalConference Simulation Adaptive Behavior, pp. 478{485.Zlotkin, G., & Rosenschein, J. S. (1993). Domain Theory Task Oriented Negotiation.Proc. 13th International Joint Conference Artificial Intelligence, pp. 416{422.507fifffi! #"$ % '&)(+*', --.0/1,32-54678FHGJILKNMPO:K=ILQSR9:;<= >?,,3@-0A:BDC;%&>E(@-.T+MPUWVYX[Z]\^V_G`FHGJVbaWO:cWdfegTbGihjO:cWI?M1O:V_ckVL\lTbcWTbGJImQSnSVodoO!KpFHGJVod_GJILhqZrostvu#wyxzw|{^}ff~{1L$J0!v^L!0_5bo$ Y0vN^[ ?05^mE$ffE?0vgE^Pv:5LD D?^3^5^o_=3:D3:y^ o! ^:y^::5DE^3^3o^So33o3DE[ !^3_5Do^S0o3D3DL!^5L3: 3ff:5?:^ :D3oPD[#3v ?:D3^?D Y^S L033503:^ $ o^3: :5LD 0 :m=b!)330!L:3^Y3ff !3LD3D:!3^^^# 5^o$ : ::5N#3^^3:035=:)3 5m 5D5= 5^!!b3:1151 !3LD3?!!15)3: _55#:5)^ JDN5#!^mffff^ff':5^3^5^o5 0:$ _5:3^33[L:3^W3 !3LD3^^!:5D^E3^5^o:3y3:3:3 _3:off'L^3)3::^b3oD:^o!L: !Wm!^N^3:5L^ o^!:5D^^b3^5^o:fffifi "!$#&%')(*+-,.-/01.24356 (*27fi84fi9/- :fi 6;.<=/2fi9/-.>(?@AfiB(*2<C(D FE /-.G24fi- .5H&fiGI.@/@fi9/-6KJMLONP./Gfi 6RQTSVUUW=0Gfi 6RQXfi9fifi 6Y(? 6;TSVUUW=0Z\[: ] fi^_H.`D5%'1 fi9/-YfiGa. b/-c(?fiIF2c&fffi9/-4fidJAeI -6/-+.=/f/-, G/-&fi Aff2--/- P.2J.g<NP-/-a.fi_hi$/-.Gfijfi)Pfilk:VmXfi-nM1oqp5ff2/HcSVUrsMtcuv3fi 4H\SVUUwMtcN> /OoxQ8yH:SVUUMSz0Z|{j, d.</2fi9/-}(Afffi9/-2ebn~Cfi9-n%': fi9/-;fic(fiI2K ~Xfi9-nHSVUr0cfi.6~X,.fi%': @/@2I.ff/-b3.fi9/-~X,.fiHSVUrr0Hv\J_fi26/-ff2.fi9/-Y 6 /- ZK{j, ff(*2^/-, ^-/-I 6FL(X/-ff2<.fi9/-Y(B!=#&%':lZ'5ZbHcp52,2ffLco-/-HBSVUUW=0jjfiO&2/2fi=//-.Z{CmXOfi2-\(jB!=#&%'\/-,.fi9/\JA,.fi43+mm:Z4Z/Z/-ff_fi9/-Yfi92/-, ^-9E 4fi6fiffL5<fi 6fiff /2fi9J.j2fiXlN> /oQ8H SVUUMStFNP /o`f6F2-, lH SVUUMSz0Zhi+(fiff/HMN> /fi.6^Q8TSVUUMSz0e43</-,.fi9/+(j fi9/-;fi)a_ b/-<(?fiIF2).-Afi9/-6=/-/-, <2M(C/-,-LHv/-,(?1fi$LdfiffL5efiyHAfi-65E 6.ff2b3fi9/- jm>b/-,dfi92J.b/@fi9-L-ff/-d2I c(D6R$I.ff2/-ff2<.fi9/-ZX{j, :$3ff2-e(f/-, P2-I /H.lZ'ZbH_(ffi2fi/-ff2<.fi9/-j(?GfiB2I 6$I.ff2H/-, Yb/11fiffL5H, 6 bLI. 6 ffG/-, fi--I /-O/-,.fi9/1/-, )2fic 5E.I .6 ff2 5%bZN> /ffi.6e`f6F-2, &TSVUUMSz0f-/2fi9J.-,)fi.fiI.B-I b/-e/-ff2.fi9/-\(?-9E 4fi6+fiff /2fi9J_2fiH&/-, >/-\m:Z4Z/ZA/-, +`v2K-ff/-R2I HAmP,.2,R-ff/-:/-, +(*/-@/b/-fffi(XfiMI ff-LZ[fI .6 ff2 ^1fiRfi9J_27fi"(?2(D/-ff2<.fi9/-OmP, ,dfi92-1fi1-$Yfi:fi< 5E6fi9/-6Ofi9/-X-ff/-6AH fi ]5.fi 6Z'5ZbH lN> /CoQlH&SVUUW=0Z{Be/@24fi9/jfi-e 5E6fi9/-6+fi9/-<H~C,.fiTSVUrr0B$/@2M6 6efij$62n5 4mP)fi~X,.fiA%' @/@2I.ff/-b3v fi9/-AZ- +~X,.fiA%' @/@ff/-b3 fi9/-AH dfi922,.2fTSVUUs0X-, Vm6</-,.fi9/X/-, /- (fiffLMb/ Lfi 6}fiff /2fi9J.b/ L2V3M6.fid^_ff/-^,.fi9fiff/-ff284fi9/-(jfi\/-,.fi9/)/-ff2<.fi9/-^(?fi2I .6K$I ffZJW = +vXz, --. v %% !> 5 > b0 P !fi< ;!%&%^&0%|% 0 >fiO.5M .Fj.G.-Kff.29.b<)_ - 1 ff. -fff5b^Pb-O)2<$-G-A5 +- ff*2j 245 :ff.29.b+<4cAX9- ff )Aff2-<Aj P_-._9Aff1- ff;O<ff- $.?\ 45 7-ff2<.9-dP-Y2@Aff\-O- ^v2O-ff-2 D$ -.db@-<-<$-< ?279-+A-2b. 2)@.b)$-R X_9--457DP 7f9F- K G_9-C 2V7-)Aff55- 1-.ffj G-)&1ff 29_y- -:- b-P9:). 7- .1-.9P-.:2_A 2C-ff2<.9- e:4- cD2<-ff-y2.jj c ^A--K> .\- 2R^X4O-.9 2*9-ff..<<vj$ P791=@2M G- - Cv9.@i=ff P ffGvjX 2V-KAff 29. -KAff5.@i=fffP R@249-\-.+=ff224-Rlv+ff5K <ff 29.Vi2 ff)-dA^>2 ff-4X --. -FFff.29.b) ff }-;29Yz_TT_=ff?_}O$-9-7- -K - +A@- <ff- $.O* 45 -ff2.9-yv ffc 2 G9 .OF2-.b-e-R 2<c?27.7 2.<:; F -.242 5i_9-- 94X- V-.9C- P_ ?$*d?5)4ffzGvff7 2.47AG*27 7^_$-e$<4 XfyFFff 29.1 2j X.245 j+*-P$.ff2:lF &. 7$.ff2-.9P4yAG.ff- @Xff2A7- KFC7- Glj 2$$.M'.K$C 4Pb-OX.A'X -@2-be 9-Y &<-..@Aff-y-.c.2$?G- 4>>-_9>- ffd $.bKB?- :42:f 9-yP. b-F29 2< 2VM.c-.ff- +?.245 1-ff_9-+BB=&'MM.. +- e22 b-P 1 YfF222 fff5+ Yff 29.).2<1j2ff <C- cff-.$ R\?A?y9^-)X 4be4ff2<e-F4j_2&- j<ff- $+A> .^fF2- "TVMz _bc- P.-j&-M\+ -7=-?279-A. db4>P-< $-*<-.-\9-->D- : P -\-ff2.9-KG$Be- -F- D2>-ff-\ 2Vff4=- ^.-.992bA- V-.9@5@-<_-)e- X 2 < v_9 45 G-.b29.D?2749-e.<=29-*P 2.<K 5 < - >24-. 5j.7_9Aff^+2 ?VP j.K M2ff- $2 -<-ff2<2.92&T5ff-) 7e- - CffMb 7 Kff 29.b 792G 2-$-A5ff-.G$M& $=2FG-ff2.9-bc ff. b- Xfff.29.bXiFff-)-.-ff..b- 92+$-9-Y7<ff- y?1 2VM.-ff_9-Afff.&fiFff--<c. -.9+bA+j G_9Aff1: 5- .R2ffM-Rff2-YG d922 2lV!" #j <?4> 29-;PDA.-AK?V|v2R@5=2$} --.+-_9^-@2@29-- +Pb-O^49.b2Bff@-ffP2ff.2-$->)929.FP - ff@@2 X2ff 22=G @2$--ff2<\ 29- 4&%>9-@M)A:9^*- .-M('*),+-)/.5Ob-ffD\b- ff)9-0'B 12 )4343432)51$6.>+ 9-R9-87!'" 14 )4343432)51$6.GOM.1:9<; .> M.b= 1?9@> ;-fP ff2 = M.$-b.e4ff+-<OAff2.9.:. V:X-.929.c$ -2 y?1 ;M.- $_b-92-4 2F @?.?>. - $BAz> M.b= 1&9C> ;-Cj. E D)E FGfbP>29-@_9_1.FG PG. @2 HJ9L> K7P.2<b-bvGl 5- _X.ff1.T2@M DO& -OM7+ N) j)_ b--ffPf 2OQPSR 4 )434343T) R U 3V5WTXfiY Z\[^]`_ba`cedgfZihj_balknml_o[`ap[`qrsfafZ\ktvu[Ic_owY Z\[`c`Z\k`h xy{zb|~}@^y{}i5zovM|~njMij:zoozb|~5M Mvn/(Q445zovizb|~~4^i4i4ffnozb|~Mo4iMizog4iM|~4^(&|/,2|i4|~4in\M,ni|~n4G}`MTnni,Mn~4s|~Mn4-|" ,4,nol|~zony{zb|~}ffgi,^44il,MTziEM5MooG4~,zb4M*noo2y:4iMG~l~|~zb|~l|~zonL4&$524442/i$`nyG:i4M|~^{g|~}{4^Mozb|EM,&G<5i44,vl5jiMM"4|~4&|~nzoiMo-|~}is"i,zbn|~zonini5 zb|~MMin4l442,My{zo|~}v&^",|~}i4nl|~4Mi/yG~l`/|~zb|~l|~zon4n|~}i4|~}i:Mi~y,EM,sMi|,Mzoi45n0|~}i:i4|~zonn!{/44~:5y{}i5g-M-|~zb4T |~}iGn,z`o4iM|j^44l~,zo:zoiM zo/|,Mi4M"4ni,zoi|~}iGi5nM5M}ifiMi~y&|~|~}ifi^i~<!ezo&?Tfi2Mvn"Ti ,4,nol|~zon",4~nl|~zonvMin4-|~4y{zb|~}G}M i,^44il,M ~}2y|~}i4M~,4|~i4~n*n"Ti ,4,nol|~zonyGff,}i^n~Mi,nMM ~4&M"|~z4G|~}iG g4no|~zoneGE TM5 }izog~4&M"|~z4gzo{|~lMzo"|~,i,|,|~zonfinGM&~|ffnizb|~zoni4ff-|~izb|~zo4bn|~}iG 4no|~zonng&i,nMMfii4iM|~4^fi5z|~}ii5/|M,iff|~}4M~fiMi|,Mzoi4fi^e,M4zi&|~}izoozo2|~znn2M5}p4EMi~ffnfi y{zb|~}Mp4^izbnMo4i4M|:zo{4ni~|/,i|~4eM:nooTy{44o2yff^M"|~zo42:&2444T/"M:~(,4E|~zon(/\v!n-^44l~,zivzofi}2^zi,4,ni4"|~z -4M|-44lszo&|~}i}i2MnM-(4Mi~|~}4Mi(|~}iM,iE444T/M!"$#&%4M('M|~}~y{zo~Miz|~}i4M&|~}iEM5ig)+*5, 'zo)-Mi/. 444T(.10 ly{zb|~}2 ,ffMoI|~}i4Mi,4n3fi y:zb|~}(}i2M(/\vn"*ly:zb|~} .*?5 4 4442(4 j|~}4Mip|~}ivEM,6&444$/79 8;: = < 0?> @{!p /5*y{}i5ff@zos|~}i,|:nn,zo4{n.`{jzoff5 4 &p44M/4 5!Mi&24442/5E,4~}n,z`o44zo`Mobn*|~}iEno2y{zoB5~,(,4"$#DC9*E"GF$CHJI6%&,Mii4~(|~}|ff|~}4-`Mozb|e|~}i4M,en$ fiG!44n4G|~}i,MM|~}inffn|~}iLK:,i5Mi(izb,~M&2444T/M O*4444TPO!MQ &GROp44RO!5M{~ii|~zn(/^vnS` 444$POVU5NM 444T/MsTM{~zo/|~zoi|Gii|~zon(/\vno Mi)S4WM{~(|~5X4ff2 |4i ^44l,{zo/4nNM}~niii4~gn "Tl" ,4~nl|~zonyff 2 |4Ig g~4M-|~zo4gnooTy{g5n$fiY ZR!{ p44&: /5y{}i57e^M"|~zo42Mo|~}ff54M5zo4gn|~}iffM,iE^"Tl" ,4~nl|~zon(zog4no|~nibEM:-i5zo4}2^zovM|~,zo|~zoii,zbn|~zon[(M|4G}M G,-44l,zoiM|i4zoQ}`MMzol`izb|~(i,zoM|~zon* s?4ni~4^i4i4M:|~}iM|~zonnzolizo|~Tvi,zbn|~zon<zo\(]\fi^_V`Aa;bcedV`fcghJikjJlemjGn;okpqrVgqptsvuwVxeoxeojzy{(hJ|Vleq}+~hJ{kiPwqo=iPpnTh$~ffiPq{}xegVjiPxeh$gTh$~ o|tqGjJoPqiPwqLghJiPxh$gh$~pq{xeJjiPxeh$g)xeoh$~{xjJlx}ythJ{PijJgqJs[uwq{(q~hJ{qJAmq{q~9q{wq{qiPhjJg/jJliPq{(gVjiPxqpqrVgVxiPxeh$g6h$~wVjJgo3yV{h qpVf{q1$xqgB| nBkj{(wxehJ{(x$$(;mwq{(qiPwVq1of|i={qqooqpffiPh{(qoPh$lqgqjiPxq6lexiPq{jJleo7j{q|VxeleixegkjiPhJyf5phmg/mjn$th$goPi={iPxegiPwqx{7|{(jJg(wqo7xgyj{(jJlelqls7oLjh$goPq;qgqJViPwq})jJxegpVq{x$jiPxeh$gxeoxegrVgxiPqxe~3ji7leqGjJo=ih$gqLh$~iPwqoqoPf|i={(qqoxeoxegfrgxiPqJsuq{}xegVjiPxeh$gh$~ opqytqgpoh$giPwVqoPqleqiPxeh$g{leqJshJ{xego=ijJgVqJfiPwqLy{h$J{jJ}NiPq{}xegVjiPqoxe~iPwqk{h$leh$oPqleqiPxeh$gZ{lqJmwxe(wwVh h$oPqo)iPwVq/leq~iP}h$oPilexiPq{(jJl7h$~j q{n$xeooPqps[1fi iPwqy{h$J{jJ}ph;qoghJiiPq{}xegVjiPq1xe~iPwqoPqleqiPxh$g{lqm7wxew(wh;h$oPqoiPwq{xe$w iP}h$o=ilexiPq{jJlh$~6jz Vq{Pnxeo)oPqpsqoPwVjJllh$gVoPxepq{)iPwqE$qgq{(jJlxeGjiPxeh$gh$~ffiPwqE{h$leh$zoPqlqiPxeh$g{lqiPhyV{h$J{(jJ}oh$g ijJxegxegV6h$gVo=i={(jJxeg iPofmwxw)pqljGn;oiPwVqoqleqiPxeh$gh$~3y{xe}xiPxqh$gVo=i={(jJxeg iPojJo~h$lelhGmo/iPwq/leq~iP}h$o=ilexiPq{(jJlh$~Lj q{nm7wxewxeoghJijy{xe}xiPxqxegq;VjJlexinxowh$oqgtshJ{6oPxe}yVlexexin$mqh$g iPxeg VqiPh{q~9q{iPhiPwxoLoPqleqiPxeh$g{(leqjJoiPwqPEP$f,sgG i={qqiPwVjixeohJ|VijJxegqp|;nVoPxeg6iPwq{h$leh$oPqlqiPxeh$g{lqxoGjJleleqp ,V i={qqJsuhy{hGqiPq{(}xegjiPxeh$gh$~leh$$xey{h$J{(jJ}oJoPxij|Vlq~9giPxeh$go[~{h$}J{h$gpjiPh$}oiPhLgjiPf{(jJlg;}B|tq{o GjJlleqpleqql})jyyVxg$o,m7xelel|qLVoPqptsqi7pVqghJiPqLiPwqLq{P|{(jJgVp)|jJoPqh$~s[9RvAf V; $1V9 ~9hJ{ffBxeoj~gViPxeh$g[~{h$})gVjiPf{jJltg }B|tq{oGsiPhRleqql})jyyVxgxeoqAiPqgpqpiPhgqjiPqpJ{(h$gpjiPh$}o| n?[$ DsqphghJigVqqpiPhqAiPqgpiPwxeoghJiPxeh$gjJleoPhiPh)h$go=i={(jJxeg iPot|tqGjJoPqffiPwqn{(qy{qoPqg iiPq{}xgVjiPxegjiPh$}xejJiPxeh$gVoshGmqq{GghJiPqiPwji7iPwqy{qoPqgVqh$~h$go=i={jJxeg,iPoxegj;q{Pnxeg qgqoiPq{}xegVjiPxh$gtf|tqGjJoPqJ~9hJ{xego=ijJgVqJVjpq{x$jiPxeh$g)rVgVxiPqln~jJxeleox~3jJggVojiPxeo=rj|leq7h$gVo=i={(jJxeg ixeooPqlqiPqptsfffif{B}qiPwh pmxelel|tq|jJoPqph$giPwqgVhJiPxeh$goBh$~jJnAlexexinjJgpzjJqyij|Vxelexei5n$mwVxewj{q)oPqpiPhwj{(jJiPq{xeqj6ljJoPoh$~iPq{}xegVjiPxegy{h$J{(jJ}oms?{s?isfjJgj{|Vxi={(j{PnjJgpiPwq{h$leh$BoPqlqiPxeh$g{lqJ1{(qo=ytqiPxqln$s giPwxeooPqiPxeh$gZm1qk{qGjJleliPwq/pVqrVgxiPxeh$gh$~jJn;lexxi5n$7jJgVpoPh$}qoPq~9l{qoPVliPo~{(h$} kj{(wxehJ{(xt$$(m7wxeleqjJqyij|Vxelxi5n)mxellV|tqLpxeooPoPqpxe"g !fqiPxeh$$g #As7yijJgpZqq} $;o=iPpfniPq{}xegVjiPxeh$gzh$~ oms?{s?isjJgZj{P|xi={(j{PnoPqleqiPxeh$g{leqJsuwqn)xeg i={h;pqiPwq~9h$lelehmxegBqleqjJg io=nAg,ijJiPxghJiPxeh$gts[9&%tv(' *)$fA,+.-./,-A*03 y{(h$J{(jJ} !xeo21GD4365e2575$ tx~)~9hJ{jJlelJ{h$gVp/xegoPijJgqo98 ;:=<2>2?2?2?@>A:6B h$~ljJoPqoh$~ m1q6wVjGqffiPwji 8 DC :ffE wh$lpo~9hJ{jJleGl FIHJHLKos?is :ffE xo7ghJijh$gVo=i={(jJxeg xeo621&xe~[iPwq{q6qfxeo=iPojleqql})jyyVxegVos?isxeo7jJn;lexms?{Gs?isfDs~jy{h$J{(jJ} xeojJn;lxeJiPwqgjJlel3J{(h$gp q{(xeqo7wjqh$glnrgxiPqpVq{x$jiPxeh$gojJgpEwqgqiPq{}xegVjiPqJsu3hqAiPqgpiPwxeo{qoliiPhLgh$gA5J{h$gVp6;q{xqo$iPwq~h$llehGmxg7gVhJiPxeh$gh$~|th$gpVqpgqoPoxeooqpts[9&%tXAM /ON;f PON -Q) 3qit|tqBjleqql[}jyyxegAs7)|th$gpqpEms?{s?is?xe~3~hJ{7qq{PnEUHVFWHXKViPwqLoPqiVq{PnSRYR E [Z :ff\E :6\E xeojJ{h$gVpxego=ijJgqLh$~ :6E]^`_*a:=<T>2?2?2?T>A:6B xeofib6c.degfihgjk=lcnmfffihoqpfrdghSdgsutvlhwlc.owxzyGdDjwfr{$b6c.dgjgc.ogm6|}r~ww}iAOOA}r2AwO`qnnn(}2~("qnnn2 9nOn222n(@"AgO4OLO.2r}rvn(qOO"(Jqwnn2nAuwO~qnizwn}iAn(}iqA}rqn~6YTY2Qw2A}rqO~ww}iA=O}r(On62q(~(q.Aw}r~(2~(nin2~nOGnq*nwffAIAw6q~A~A}rw}rr}iq.wqnwn(}rn.n~AATOw~A}rnwO~I2qn~(wA}in2A}rqwOw(O}r"J2quwrA(g`OA(}rTA}rq7`(n}rO`}@qq`}i(~A2O(4Or}r29An2qn2nvqA`u}rgA}rqUYTY2gO"Ag}i`A~A2r2A}rq"(nrO4Q*g&rAO*gqg@n wQnL.QQ*znA$}~2`zgqQY@Y2}Or}iA~Iqw@nn`}iOA}qn~}rJ(Iwn}iAOw(qO`O}r~IzDq}rOrnO.gQw(}r2~(A`u}rgA}rnUYTY2wJg*i29`SOL2 wAO qg Tv;(SVAO.g(2wfiff.2 O2`gTnA(qv 4 (Og7OO 4(O.gA2wUOgu6ng*[2 `J4zgQwAqO fiff.2 ff.A!q"J# O$wQ%&%'(")* , +I 92 .-/r(0%1%32 * (+42O O2`52w76 v(O.gA-/r(8%1%w 9 "92`zgqQ:`q n2O(2~<;>=4On?;A@}iqrrT9~IAwA(}rwA}rww(qO`O~2q}rn2}n}iAO.2r}rn(qOOu~On AwOO.2r}rJw(qO`O~"QwASgO~4ww}iA4qgTn7`2}rOn qni}r}i}r~9qnnn299OA}r2Awvn2Sn2A}rq O~vww}iAO}rr(}r~9O~(~An2 w2O(2B;A@un2~nOnqrwOv}rw~(On2OAnn`qO`OCHJILK8M!EgGFT!EDQNFI!ED8MPwGFQM}r~A(}rwA}nugqnnn(}ngI}iI}r~=nOvO2r}O}rnw}rnUr2ww}rnOWn`T.}rnzO2r}2}i}~W(TA}ivn(22~A~20R v(2An`TOnTSVU.(n(?WXSv22O(AOJ@qY =4O$,AnO`qnqLn(2~A2Q(A}rq qUO(}qn~A2(nn}Qn2~4O2qn~`nA}rnJr2ng}rnq~2w9qrr@}rn~(2A}rq4}rrrn~A`A2~6n@O}A(2~AA}rnw(Owr2 }ruwqnuqwOAqn}r=(TO~(qn}rnTO$O(O}r22On}ruwr22A2O~vO"O.2r}rn`qO`O"[Z]\4_^a`'bgdc8efhg5ejilknmpogle"qwrLr.~O`r }r~O`JnA}rqqI4n`Owr2 }r n`(OO(O(~Qwznqn(}u}iA}rOA}rqw~} "~(}rgrUO(r~A2Oz}rn~(Ow2v}rr~A~Aq6@tsuq`?v9`u2qn~(}rn~A}ruwr=(~(}rqq6An}r~9n(Ow2wUO2(wOA}@xtxq` n(z(A(2wQ!r~zyn8{@'|*OnA(2zn~} }D(2Qvq~(}iA}rqn~ 'Qjuq u(wrOU wrL rTO22}rAn@OnOAnwQ! r"Oq$qnUqGAw2~Aq~A}iA}rqw~2.Ow$}iITO$T2(q qnq~(}iA}rqAOnOAn@ nw(Owr22qn~A}~A~9q~A22}.}rnzq~A~A}rwr2qwqA}rqn~2D}OigAwq~AOw(O}rn2"(q An}n}iA}O~A}rAwA}rq`O(}rn~A2n2n2qq~A~A}iwu@2~2=9$ OuwrqOS}rn}iA}O~(}iAwA}rq}r~q}i2}r}rq(zq@O~ r}D@xOq}i2~92On~(OQ(n(2~(2(A}rqzq.An}~n`Owr2TOn~qw(Onq~2qnn}rA}rqn~2hv9(O(JnAAnn(Owr2 n~A}rwu AAQOnvvT2~2~A}iAgA}rq"TOr2wrn~Pfi~1bpcqr0~t1P5t/~L(Y!~( lLLhttP&~L!pt[YLL.YLLlL~t!7l!.L!.,jtP[l [L !Y"Y>~L ~5~~lL~t :!YY [L Y#>~L ~5t~L p YLY /"Yh>LPa~~!~lL~t a!at~5tajatY[YY [L 5!Y5"Y#~L ~L~lLt 0~Y~tP[[t[TY1l!#L!jL~~L#YL7L~~LY4LLlLtjY LpLLlLtY!Y~Ptj!"YPp~5L5: ? YL?twLlj5!!L!lL~tjjL~LhY!wYjY!Y~YaL.t~~<~fi&08j:%?LtN:'P4 .fffiffff!"ff$#:'Pffj' &) ('+*& ,/.00 &/)(213&4:ff' &fi5 0 (fi:'6.87-94) 15' &8') &8':'6.87-940 (') &8fi(;< : /' &>=:'Pffj' &) ('&/.@1A&B:ff' &fi5 0 (fiC 78 95*78$ '0 &/) (fiD &/.D &8 ':'$:ffj' &/D (') &8E=:'F 75G,H.' &/ IJ') &8K:'$:ffj' &/ 150 I8') &8>=:'F 75G,H.' &/ IJ') &8K:'$:ffj' &/ 15F Mff8') &8':'$:ffjF 150 I8') &8>=:'O.87-90' (') &/K868PJ.0'/(&8 E=C78- 95*7P '' &) (fiA *- ,/.0' &) (2QO') &/ 4R=SFT6UfiVXW/Y8Z[]\^`_baGWcd[]\He fH[gY\hYikjlaG\ffaGW/effmon>Y^ff[gp VXW/Y^W/ecXqrtsGuwvffx8x6y8z{G|}~'d)8v8-}ff~-vffx~'XD)8Er ruh|/8H~~)J-G~D@DH-~Ox>/ff2)Bv8-}ff~-vffx~2DJ-)ff/v8-}ff~-vffx~D@DH)ff/v8-}ff~-vffx~6x>D/ff)ff/>'b v5z~' O50O20@05 ffg80 5tffFt't 8)'-)))'>A8uF))FuF)DFAOuFl$tAff3uF'5t' uF'5A)-uF 5'O6G ggO r ff '/ 0l00b 5 0Og r 0JOOgffOE0bg]0 0g ff 0g 'O 0 v/G}" O'']OE"O gEJ '0]ffgg ]GO0] 0g 0ff'O 0g |-H8H "O 0g gJ '0]ffg w ]GO0] 0gff0 0 ggt"g gGOJ g "F"5 ' 0' G O50O@0lgO 0E0gg0 50'"g0 ] $gb5 gg) u g s85uO-s86AdJuOGs8/'t58 '-uO-s8ff8G> uO" r""g g >05'6 FuFg g >05't'5 FuFg g >05'-'55 FuF500"05F-t> uO8"3/3- uO-""gb )$0FO00 fffi`g /Ogg Eff'O+5F 0O 0 O)0g ff8ff'O00 8 k ff @J 'gO KOffg0Og 0' 5g) ffO 0 O)0g "Off' 'O gg g0g]0 0g0gX0lO 0'OO 0 "5' g"!O 0# ' v/G}~Fvx~z%$80 ffOOG0 >]0 -}'x &)( '] 0g X]0 "]0 )@+* , .-0 /2 1/GA , u)uF3l500 "0g4 0 65 bO G 0g bff]0 g ' 0798:;<;>=)?A@ B C+DE=GFHJILKMHJBNPOD0lF8g X0O0g @ G0OO$t0'k 0g QR 0]) 00OO0g`ff'6GO`8 d0o50g ` /OggO] 50g ` OOff ffgg] .STVU 'O0F JXr W W8ru g 0 +5bff'6/g 0'k 0g ffQMYRZ 0 U ' g 0OO0g gDK0ffgo0O0g @$'O g0ff50g 0 0"]00 0O 'O g0+' ''5'Xr W W [GuF\/OggO] OO ffg] ggO offgff g40+ g6"g 0O0g 0ff'6/5'l 0g 08 g5F6/g 0' g 0g $ ]QMYRZ b 0 U ' 0OgO0g gD5 0) 0' g 0g g wff' "g0 'O0JO$00 U ' g0OgO0 ff U FO0' DXr W W8ru g-)F 050 OO ffg' E g^`_2afibdcfe>gh)i"jfe<ik)lnmpo"l korqtsunqpvxwyl kzmp{)v|q`un}~vxl k)w)o"mpo"l kz)qpvxwzmpldw)v k)v|un x"or)l n`un}qxZv>xv )mmp{fumxlnunl )kfw~o"kfqmunk)xv#'xxx'p9~l Muxrun)qpvnmp{)v#mpvxqm V 9JorqAv ln}vxwl k))kYmpo"mp{)v)qpm"ompv `unM {)o"`{uno""qx{)o"qo"qqp)xo"vxkYmqpo"k)xvnw))vmpl~mp{)vAl "l qvx"vx mpo"l k\)"vnfrompv `un"qunmpv o"" k)lnmsZvtqpvxrvx mpvxwAlxl }~f<mpv ])uxrunqpq4l }~lw)vx"ql |){fv vt'un""vxw>p' "Xp''o"qfqpvxw{)vl ""l o"k)6k)lnmpo"l korq)qpvxw{fv~p `.'.nzl +unko"kmpv )v mumpo"l k\mpluqpv m#l `vxrumpo"l k)qx<w)vxkflnmpvxwds~J forqmp{)vqpv ml ffumpl }ql M6{fu'o"kfmp{)vxo4vxrumporl k)q4o"k+6Yff2Ln)f> Y<Y>f v mz svmp{)v"v'unqpmqpv mPl Avxumpo"l k)qqxmxmp{)vvxrumporl k)ql lxx<p`o"k)6o"kyk)vxJumpvxwumpl }quvork+Aunk)wyo"unkvxrvx}~vxkYml lxx<qorkmp{)v{)v'unwl +u~xun)qpvnmp{)vxkun"mp{)vvxrumporl k)qtlYxx<o"k)o"k\mp{)vsZlw<dl Amp{fumxrun)qpv|uvo"k+ vsvmp{)vqv m#l 9xun)qpvxq#o"k {)l qv{)v'unwxl kmunork)quvxumpo"l k\l }z #l u}lYwfvxZl ffo"q%>x. 2po"M Ap o"qu|}lYwfvxl ] 2|M.`6Yff2LrY n<)< M| v mfsZvu6"v Jvxff}6u)fo"k)ln unk)w"v mtdsvunko"kYmpv p)v mumporl kl ||o"q` E ' fffir fi fi9n~or9\orqtuqpZvxxoun"o xvxwz}~lw)vxl unk)wlntun"nl )kfwo"k)qpmunk)xvxq xxxXp l 9xrun)qpvxq#l 9 v{fu'Jvmp{um| ){)l rw)qln%9 x !#"%$& ( '2) 2`A o"qv Jv pzq'mx) orq+kflnmuxl kfqm`uno"kYmx {)v vJ`` E xo"Momorqunxxv )musf"v 'mxfqpl }v#"v JvxM}6u)fo"kfunk)wdo"kYmpv p)`v mumpo"l k* u )`l n`un} o"qdunxxv )mus"vnmp{)vxkv Jv pnl )kf,w +Y)v {funqdl kffkfompv J. -0/ 1 w)v umpo"l k)qx{)vxk)xvommpv }o"kfumpvxqx~ldv>mpvxk)wmp{)o"qvxq)mmplkfl k 1 nl )k)2w +Y)v `o"vxqxunqlnmp{)vun >x"o"~'unqpvnmp{)vl ""l o"k)k)lnmpo"l kl sl )kfw)vxw)k)vxqpqo"q4fqpvxw6Yff2L 35 4<7 68-<>Y,976f : v mdAsv~udrv Jvx}6u)fo"kf6unkfw"v msZv~u qvxxorun"o xvxw}lYw)vx l ff|< ;=+Yfv p>? xxx2p orq4sl )k)w)vxwz 'mxff<<unk)w <+o"lnv Jv p@B > C 4 xxx2p< nl )k)wo"k)qmunk)xvl xxxXp unk)wxxxXpo"qfkfompvn;)m+unkfwffvxw<vxqp`{)o)l'Jvmp{fumlnunkunxxv )mus"vfl n`un}\v Jv p|sl )k)w)vxEw +Y)v {unql k)"fk)o"mpv4w)v umpo"l k)q XmxJmp{)v49`l "l qpvx"vx mporl k|)"vunkfwk)vxJumpo"l kunq]fk)ompv4uno"")vn{fv4xl kJv qvl mp{)o"qvxq)m4{)l "w)q {)vxk F{fun(k Gqxl k)qm) mpo"Jvk)vxJumpo"l kdorq)qpvxw Hu{fo"lno.I KJLJLMJ`O Nffoqmx vln}unro xvmp{fvxl k)xv )ml mpv }o"kfumpo"l k 'mxfmp{)v9`l "l |qpvx"vx mpo"l kd)"vn6Yff2LQ P> /YS RUT< 7 - K -0V976) : W -)X8> <K ;Y+Y)v zorq~[ Z [ \ %x` ].0U/1'mx~to"un"]ompqwfv umpo"l k)quv|fkfompvn^;fl n`un} o"q| _ Z[ \ % 8 ]~orAv Jv pnl )kfw +Y)v o"q"vxrm 1 mpv `}~o"kumpo"k) 'mx)|`ba c28 e Lg fih xffj`|nGJ`` E xf Q ] n nd ' >j| 7 k>ml!kf nUfimprq> G! snxt nfK -S/ \ `< Zxnu> `nfEn<nv nmp7 k>Ypl!kfx`En w9)%. fi`ba c28 e L x=h xffj`| [ Z [ \ %x` ]f5 ] n^fiprq< q>x~5 zJr 6 ! sn'LJf ]zB {~n~ >p' "Xp''> Z fi fiy|u} rJ E xfir fi fi])Zn O} ~ Z ! snxt n^l!kfxt n>{> p7 k>Ypfir fi fiA)n ,> _ Z[ \ % J ]fi* kmp{)vl "rl o"kfqpvx mpo"l k6unk~unxxv )musfrv)`l n`un}mp{fum+ln`}un"go xvxqfffrunk)k)ork)to"k|mp{)vsf"l5 >q ln"wo"q4 oJvxkLtKfi 8dd@U0<SLSddffuUUm ddd[Idyduy[%[y(WffdWL0Q0Edg7005 gQ8ffO7t(7L0 5g0550@(!!.W5LVL2Q!0!!L(L55dm.U!t705y7Q0@50@5g7!L0LStW5LX52d7!LSLStW5gL(77gLSQ00Q7g!j.7ff5LU!%05 L005Q055b7gL7Q5085QWdQKL7 yyoty55Q7 U[[7 WdU!fi ffQt UQ <d ^ L U.[_78LbyS t5d#QL L[_7 7dQ ^y LO[<fi WdU L& U ff. 75 !O0yLK00QL7o7#"%$&(')')*+'(,=yLK50W00tU7Q(%!L055Lb75!705!L500tL7t77U709 8Lt70d050yLK0!g70Q!!- $./02134.5$ 6<880cbbpcqrpLStfi: g700b50<;O85=>7Qg?@BABCDFEGBH AIKJ LE(M N@MOQPCD)RTSE@QC@BU2JVN@QW%R(MPU XQCPYE)JZN@BW[ML%EBR)M@BABCDFE)J L%EM\N@MQ]N@QW^[MOQPCDR_@BABCDFE)J L%EM\N@Ma`bEMY]c^RdSPU XQCPYE)JZN@fiMLER_@BABCDFE)J L%EM\N@Ma`bEMY]gfhi@kjgf%h@E^RTSE@QC@BU2JVN@l R(MIUIBn%UA2JVN@l)Ma`bEBR(MPU XQCPYE)JZN@Fl)MY]gfhi@kjgL%E^R(M@BABCDFE)J]gfhi@kjgL%E^[M N@fiMY]aN@ljo`%bBEi^fiMfh@pEBR_E@QC@BU2J]JVC2MqFl R(MJnrMqQsYR(M JhMqt%R^pRuSOBvK]owKMxfiMAfiMa@QHwfiJVCYR(M@BHwfiJn)R)Ma@BHw[JhR^IpUiIQnYUAJ\qFlMOFR)MIpUiIQnYUAJ\qQs2MOFR)MIpUiIQnYUAJ\qBtMOFR_zIpUiIQnYUAJ\LfiMQ]{L|jo}B^R~S_Z??e??Zqrfi%BY%QVpzpiQY\fiQgkg~piQY\fifi+YYY\Y% r T<u\Y4 \T QF) TY\9 p YZFQ%YZ\+K\Y\%\\%\4K+%Y44Y[\%YZY\ B \Y\YY Y\ Q Y\\% YcYZ u%T+ \YQQZ\\\Y%Y[akY\k9Yc%\kk\YZZY\%Z\\%YkY\{\Q %\Y Q%YZ\|Y\ p cVpYYY%\<+cY%QZ\TYZ|%ZQuY\YZ2%\Y%Y\\Y%\%\BY%<\%Q%QZ\Kr%\< p %V \\ %\\YYZY\%\Y%%Y%QZ\9r %\YY %gfiY)+Fr%\%Y%)fiY ()(\%\%[Vi (iZFZ VpZi p24 ZZZ Zi Y4YY\4Y\\Y\\Y+Z\)Y\+ \YB{ p24c\%()([+\ %\Y\YkYY VFF|Y %[Yr\Y\Y\%\\%|\F\YYVB\\FQr %\ )T2V (BF) %\ Q [pBF) %\ YcdFQ)\( BQff fi ff %T(2VB(B%2VBF) ff g%[ YaBifi ff Bp2V2 (r Y( ff %puK KfififiVY(fi))[ ffpiQYF)piQY2 F)piQYFZpzpiQY\fiQ{|oB~VpzpiQY\fiQgkg~piQY\fiFFY YuZ Y((+(~Y Y\Y\Y\QYYZZ! Y%Y%") B )YYVB\Y\Z\ \u B Q\ZY49\YQ[\Y YYY\$#fiY\Y YFFKY4\[\ \KY+Z\)YBFFr\fi\<Yi (iZpZ Zi \%FF|Y YrY\Yr\YYY%Y% '&'()&* , +(-Y %/.0% - %%2134/3 & , +F %/.% 3 * 46571 , +(-p98B: %;.<33>=4 *? &@ fi8 7ACB[DIKJLJFE % + %GDdHD%8 %fiMONPQRTSUWVYX6N[Z\RTS]^R_PS`PacbdX6SX6N]egf7PhUR_iCMONPUN]ZOjk lm6n6o7prqsrmutwv,x7yzy9{;|k}<lsrlG~Hv9k xk~<r|G~L6d[['; vfin[yr|Kyvy|KyvfiLyh6|kGyh;yh$y9/ymuyls97vfin|KylsKv|KylsKvfi|K{ W vfi{/| [[[ lsrl g[ uL6n/mv vfi{/| |d [ u `O/G[ 9 [[['vfi{/|rx k x>k [[ [G6[ 9 >[[\ 66 _v,lsLl\n6m6[v L vfi{/|> ||d [ kk[ _ [vpyyKp| [[ FvpLKp|[g _ [[ [[ [ [$ [ /}\ _9_}}7lmn/o7prqsLmtwv,'yOyK|lm6n/o7p6v,'yOyKHyK|}__ }pln/l v,x|Ck>x't' mv,xyz[|Ckzg [x`p lv,z[|)` L6c; F7 w6 ) c}t' [r h|Ky9)h|K 6| [[sLt7v hu K97 [ [ kk [9[ _ [g H kk 9 u67uk lm6n/o7pLqsrmtwv,xyzhy9{/|kHyvr|k lm6n6o7p/v,xyzhy9{y|kdyv|k lmn/o7p/v,xyzhy9{y|k77[[ [ F[7d ;[[[k {kvfi6|H[lm6n6o7prqsrmtwv,xp6yxl)y n6o7|pln/l vpl |Kylmn/o7p/v,xp6yKpl)y pl n/o7|Kh|Kvr|Y HHk lm6n/o7pLqsrmtwv,xp6yxl)y n/o7|k[dk}pln/l vpl |KO [plk lm6n/o7pLqsrmtwv,xp6yxl)y n6o7|k[ [ F[[lm6n6o7p/v,xp6yKplKy)n/m6[v L vv pl ||}k lm6n6o7p/v,xp6yKpl)y pl n/o7|k[py nKlk nKlp|\pln/l vplKr|Kyth|K\[k2pln6l vpl |kt' mvpl)yv|Y H _pr|Kylm6n6o7p/v ;n Klk xpyKpl)y plK;k py9nKlpu|Kp6y n;Klk n;Klp|kk lm6n/o7p6v,xpyKplKyfiffk2pln6l vpl)r|ky[fi!"$#&%('*)+, -/.*0132 %54 2$627-698:;2*6< 0=-7, 0=->2@? ) ,AB,DCFEHGIEHJIGI. % 27-K$698:;2 ) ,MLh 2VGI- % 8:;2 )jN OPP "$Q>RTS>UVSXW , YZ2@-90-9G % 2@-[K ) 6CFEHGIEHJIG@. % 72 -[K$698:;2 ) \^]_UR@ 72 -[Ka`cbedgf 2@-[KH`i- wy=I0*.*l % b^mnG7o % <D27-K, 8:;2@? )9)_zQk" =I0.$l % b^mnGfio % <D2@-[K, 8:;27? )9) Yp=[0.*l % bqmnG7o % 8:r2 )9)s Kj UR@u@R -rvx-9v-Fw{=[0.*l % bHmG7o % 8:r2 )9)\e]_URIkR@|"kR, -/.*0132 %54 2$627-698:;2*6< 0=-7, 0=->2@? ) ,AB, -/.*0132 % < 0=-@, 4 2@?}627-6<D27-[K, 8:;27?}6k0=->2 ) ,ML~ ]UR P k""$3|"_S>UR"S>URIu@! QkR@Q_"$xFQQ>#a!Vfi\3I$e/}7S>U!QQ>R@uIS>"$ RaS9k" u@R^ykQ9S!S>R@VS>!"$ u7R@ OP u@u@R P Sk f S/ "$S>URq"S>!"$QT"$uIu@u@S/yu@u@R P Sk f S/$\_RQ>U" S>UVS OP u@u@R P Sk f S/ P k"7R@Qn#a"kR P uIS>!u7S>""$S>Uyu@u@R P Sk f S/a|" P k"7R@S S>RIk#aVS>"$a"$xQ@\N R@uIS>"$\ K Ru@!#S>UVS3X"kRIS>" P k"fi*RR@S S>RIk#q!VS>"$"$*gg Sg!QQ u@R@SS>" P "7*Ru@u@R P Sk f S/"$tS>UR P V>S@"$x*yu7R@qxF^uIu@u@S/"$S>UR_kR@Q9S"$S>URP k"$[#y\3RIS QR[ P !!HU" RV>k*RTS>"aS>UQu@"$u@ Qk"$\Q9S *ggQ P V>S>S>"$R@S>"S " P V>S>Q7+ OPP RI P V>S Q@u@"$QkQ9S>"$Fu@! Q>R@Q K ) 6@L@L@Lfi6k ) " RI P VkS Qk@u@"$Q>Q9S>!"$S>UR^kR@Q9ST"$_*gg\]_UQ P V>S>S>"${QQ ukUS>UVS"kR@!VS>"$RIR@"u@u QX \n]_U!Q"$ P V>S>S>"$a"$ P k"$#QRIR@ f P VukU"!#aR@Q>Q>F% Kfi$ )Q_"$!" Q7\N 7S>UVS>@$5}Q5ka!3S_"u@u kQS>URXUR7y"$VSR7Q9S"$RX"$S>Q_u@! Q>R@QS>UtVS5I9$QX5ka!S>QR@!VS>"$Q_RIR@\F|tx5>$x*g P k"$#>It9$9!3"R@!VS>"$RItR@"u@u kQ_! \R@"S>R@ fN" R[S>R@Q ! ZRIR@QR kR@!VS>!"$Q P "$Q>Qk f Q>!nS>URkR@VS>"$QRIR@kR7\"Q>Sku@R S>U R P k"$[# fi+fiffR[S>R@QS>UR P k"$#+[R Ru@"$QkRIS>UR P k"$[#xFe" f SkR@k"$# f TR@RIS>S>URS>RI[Q3RIR@\RTu7S>UQ" P RIVS>!"$ I>I[ RIR@Q|"$" @Q \F|tx fi] UR$ @9@"!$#9*rR@"S>R@ f &% QS>UR P k"$[# " f Sk R@k"$# f qR@RIS>xS>URu@ Q>R@Q"$S>URX!S>RIQRIR@\')(+*fi,fi-/.0132465879-;:<132>=@?>1A.2B.CEDF792G79-/=GHJIK.L4G1AMN,fi-/.4-/=:fiOP;QRFSAT;UVXWT;Y[Z\;S]K^fi_`WT;aN^bcWdRXZea;ZgfT;Z[aNWUhWdiQkj9Z\Vl;Z[TN^b<mn^<_hSU8Vl;Zpo;RXQ@qRrWsutGvwxPSATGWyy3z@\|{}ZJo;R)Qkj9ZJVlGWdV~||KJSUpWY[Y[ZgoGVXWdiGyAZWT;a6VlWdVeuSUpWYgz/Y[yASAY\WT;aSAT6a;Q@SAT;qEVlGWdVc{ZlGWkj9ZpVQJVXWd9ZcYkWdRXZpVlGWdV8Vl;ZFV{QyAZgj9Z[ysWdo;oGSATGq@U<;UZ[aWdRXZFRXZ[yWdVZ[aizWY[Q@T;aGS3VSAQ@T\/TGWsZ[y3zJVlGWdV]$QRZgj9ZgRzqRXQ@GT;aSATGUVXWT;Y[Z\UXW[zv_k)bd\Q@]8WY[yW;UZQ@]8^fi_k\]QRZgj9ZgRzyAS3VZgR)WyfiY[Q@TVXWSAT;Z[aSATWT;aa;ZgfT;Z[aSATJ\dVlGZfiyAZgj9Z[ysWdo;oSAT;q}Q@]>SAUKT;QVKqR)ZkWdVZgRVlWTeVl;ZyAZgj9Z[ysWdo;oGST;qQ@]8xl;SUeY[Q@T;a;S3VSQ@TSAUSAsEo|QRXVXWT VpVQBZ[T;U>RXZEyAZ[]V"VZgRXsESTGWdVSAQ@TxP;QRSAT;UVXWT;Y[Z\Y[Q@TGUSAa;ZgRVl;Zo;RXQ@qRrWs^XLv|NLKXLvXLK@WT;aEVXWd9Zc^<_} ) WTGa @) x}l;Z[T^<_8Zr/VZ[T;a;U}J\/^fi_Lm6SAU}WY[Y[ZgoGVXWdiGyAZF{exRkxV[xVl;ZFyAZgj9Z[ysWdo;oGSAT;q 9 \SUWYgz/Y[yASAY{xRkxV[xVl;ZEyAZgj9Z[y}sWdo;oSAT;q \iG>V^SAUT;QVyAZ[]V"VZgRXsSATGWdVSAT;q/xQ/\Vl;ZUVZgoGU{ZWdoGoGyASAZ[anVQG9WdRXZNUX;sEsWdRXSA[Z[aSATnVl;Z]Q@yAyQk{hSATGqBaGZgfGT;S3VSAQ@TQ@]>o>WY[Y[ZgoGVXWdiGSAyAS3Vz@\>VlGWdVhY)lGWdR)WYgVZgRXSA[Z[UFyAZ[]V"VZgRXsSATGWdVSAT;qo;RXQ@qR)WsU[xP;QRFWyZgj9Z[ysWdo;oSAT;q;WT;aWo;RXQ@qRrWsuJ\>VlGZrg)k";|J\>a;Z[T;QVZ[a; \;SAU}Vl;ZyAZgj9Z[ysWdo;oGST;q]$QRh a;ZgfGTGZ[aiz J$|Ax" tG G@t>>|9>d ZgVfii|Z6WyAZgj9Z[ycsWdo;oGSAT;q]$QRN^xZgVNiZBU[xV[x^^fi_ ]$QRhUQ@sZ`^fi_k\WT;ayAZgV8EiZeWTNSATVZgRo;RXZgVXWdVSAQ@TQ@]K^mJx<^SAUpdG9Xr;fffiES]Vl;Zc]$Q@yAyAQ+{hSAT;qJY[Q@T;aGS3VSAQ@T;U8l;Q@yaxh^ _ Zr/VZ[T;a;Uhxh^mn SAUhWY[Y[Zgo;VXWdiyAZc{exRkxV[x>WT;aNxh SUhWYgzY[ySAYp{exRkxV[x>xh]$QRhZgj9ZgRzNqR)Q@;T;aSAT;UVXWT;Y[Zcv8_kkQ@]WY[yW;UXZcQ@]^fi_k\;]$QRhZgj9ZgRz\S]KSAU8aGZgfGT;Z[aNSATWTGaSAUT;QVhWY[Q@TGUVR)WSATV[\WT;aS]_g ! \;{hl;ZgRXZc _kk !WdRXZcVl;Q@UXZyAS3VZgRrWyAUWsEQ@TGq8_[+ {`l;Q@UZR)Z[yWdVSAQ@T;UQY[Y[>RhST^mnJ\%VlGZ[T n#" $xoGRXQ@qR)WsSAUpdG9Xr S]Vl;ZgRXZpZr>SAUV;\; WT;aEU[xV[x>^SU};o>WY[Y[Zgo;VXWdiGyAZF{exR+xV[xK>\;J\>Gx&' iUZgRj9ZFVlGWdV8izVXWd/SAT;qJ]QRVl;ZpZ[sEo;VzEUZgVQ@]Y[yWGUZ[U[\/{}ZcQiGVXWSATVl;ZpQRXSAq@SATWya;ZgfT;S3VSAQ@TQ@]WY[Y[Zgo;VXWdiGSAyS3Vz@x(FZr/V[\;{}ZSAT VR)Qa;GY[Z`VlGZeT;QVSAQ@TNQ@]dG / fi )fi+*[Gg-,x$|/." tG103254#6/7698:2;w ZgVe^iZ>o>WY[Y[Zgo;VXWdiyAZJ{exRkxV[xfi\_ + x}SAU8>o>iQ@;T;aGZ[aS]]$QRhZgj9ZgRz<)=3> ? $@ $@ _ +$@;WT;aB\KWT;a6yAZgVVl;ZcUZgVSAUWqRXQ@;TGaSAT;UVXWT;Y[ZcQ@]<WT;a@ BA@)CSAUfGTGS3VZ\>{hl;ZgRXZc @ + @ C WdRXZcVl;ZeyS3VZgR)WyAU8Q@] @ _ + @ED _ {hl;Q@UZpRXZ[yWdVSAQ@T;UQY[Y[>RhSAT^FHGIFmnJx&fiJLKMON5P1Q!RM#QSUT+V:W)X1YZW\[]V_^]`1Vab[]`cd[ec:f!fhg7ijlk#m7noX1YZW)p!q:cd[]p!V3T^V3rBc:TLs#t1n uvV3sT1X1YXxwys1YZW)zxcdW)Y|{ T1p/[]Y:}BaY^]`c:f!ft1W)VlqIY[]`cd[c+gIi jlk#m7noX1YZW)p/q3cd[]p!V3T~V3rc:Ts#t1n uvV3sT1X1YXw5s1YZW]z~V3T7[)c:pT1^|V3T1f/zs1t#n uV3s1T1X1YX~wysYZW)p!Y^c:T1XaYx^)`c:f!fc:^)^]Vypcd[]Yaep/[]`Yc:)`X1YZW)p!q:cd[]p!V3TV3r\[]`Yw5s1YZW]zc~X1Y^)YT1X1p!T1)`c:pTpT[]`1YxaYf!fnrEV3s1T1X1YX~^]YZ[;V3r$tc:p/W)^V3rs1f/[]p^]YZ[]^eV3rTcd[]s#WHc:fTysuvYZWH^aep![]`[]`1Yf!Y-#p!V3:WHcdt `1p!V:W)X1YZW;Yc:f!f[]`cd[c#^]YYY:O/Y^]`Vaep/[]:l3I3p!^c+s1TV:W)X1YZW)YXV3f!fYZ[]p!V3Tp!Ta`1p!)`[]`YTysuvYZWV3reV5s#W]WHYT1Y^V3reYc:H`Yf!YxYT7[p!^V3s1Ty[]YX1V:W)c:f!f/z3$cs1f/[]p!^)YZ[V3reTcd[]s1WHc:fTysuvYZWH^;p!^crEs1T1Z[]p!V3TrW)V3[]`Y^]YZ[_|V3rT cd[]s#WHc:f$T5s1uYZW)^[]Vp/[]^]Yfroh3p!q5p!T[]`Ys1f/[]p/t f!p!p/[zV3rYc:H`Tcd[]s#W-c:fT5s1uYZWe\`YT []`YV:W)XYZW)p!T1lBV3Ts1f/[]p^]YZ[]^\p!^X1YZ{T1YXc:^[]`1Y[WHc:T1^]p![]p/qIYfV3^]s#W)YV3r[]`1YW)YZtfc:YxYT7[V3rcTcd[]s#WHc:fBT5s1uYZWeaep/[]`c:Tyz{ T1p/[]YTysuvYZWtV3^]^]p/uf!zLYZW)V7V3rT cd[]s#WHc:fT5s1uYZW)^[]`cd[|cdW)Y^]c:f!f!YZWes1TX1YZW|OpT1Y_p!^aYffnorV3sT1X1YX []`1Yp!T1X1s1YXV:W)X1YZW)pT1lBp!^c:f!^]VaYf!fnorEV3s1T1XYX1V:W^]p_tfp!p/[ozaY;^]`c:ffV3xp/[p!T[]`1Y;^)Ywys1Yf[]`1Y|^]s#u^)ZW)p/t1[OOrW)V3lp/[]`+c:Ts#t#n uV3s1T1X1YX_wys1YZW)zOaYc:^]^]V5pcd[]Y|ct c:p/Wo ): )/ff ): /ff| ): V3rs1f/[]p^]YZ[]^5ae`1YZW)Y|rEV:Wct1W)V3:W-c:c:T1Xc:TLp!Ty[]YZW]t1W)YZ[)cd[]p!V3TL/ff )3 ~bHI):7 ff l7 ff):):ae`1YZWHY| l cdW)Y|[]`1V3^)Y;f!p/[]YZWHc:f^V3rBae`1V3^]Y|WHYfcd[]p!V3T1^V5s#Wep!Tc:T1X+ 7 ff):):p!^[]`1Yxc#p!s1V3r\ffae`1p!H`Lp!^u5zxV3TyqIYTy[]p!V3T~p!rffp!^[]`1YY_t1[z_^]YZ[HHYc:f!f[]`cd[[]`Y+f!Y-OpV3:WHcdt`1p!V:W)XYZWV3Ttc:p/W)^V3rs1f/[]p!^]YZ[]^-;p!^XYZ{T1YXu5z ] LH p/~Yp![]`1YZW\; V:W\ bc:T1X\`YT+aY|c:Tt1W)VlqIY|[]`1Y;rEV3f!f!Vlaep!T1W)Y^]sf/[fffifi fi7fifi fi fi # $fi % fi~I5dEO3dIHH /B1 ; hh; :h5 5vh# 3 g7ijlk1m 7 - d3ZH : #+ ~d 3 7-Z1ofi! "fi& fi '()*+hYZ[), .- 0/ uvY|cg7ijlk1mInoX1YZWHp/q:cd[]pV3TrEV:Wp!T Y|tW)VqIYu5zp!TX1s1Z[]p!V3TxV3T/ p^s1t#n uV3s1T1X1YXOc:T1Xx[]`cd[\p!rp![p^[]`YWHY^]V3f/qIYTy[\V3rcwysYZW]z2/435-uyz[]`1Y^]Yf!YZ[]p!V3T+V3r1 []`cd[e0_cf!p![]YZWHc:fvae`1p!H`+p!^T1V:[cV3T1^[WHc:p!Ty[#[]`1YTLo2#/ H: o0/635-- ):1V:W[]`1Yu c:^]Y;c:^]Y 1 3aY`cqIY[]`cd[| - p!^es#t#n uV3s1T1X1YXu5zLc:^]^]s1t1[]p!V3T87 Vla V3T1^)p!X1YZW3:c1X)^#13V]^][`c|[][1`)W]^/f|[1`3V!f1X^Er:VW3\`^06/53;!p^##nu3V111XYXOs#t1tV3^]Y191;:[]`cd[[]`1Y\W)Y^)V3f/qIYT7[V3r0/635- p!^$X1YZ{ T1YX_c:T1X_[]`cd[[]`1Ye^]Yf!YZ[]YXxf!p/[]YZWHc:fy^)cze5p!^T1V:[cV3T1^[WHc:p!Ty[[$rV3f!fVae^$rW)V3[]`YrEc:Z[[]`cd[0/635$- p!^s#t#n uV3s1TX1YXc:T1XrW)V3[]`1YX1YZ{ T1p/[]p!V3TV3rvs#t#nc:YZt1[)cdu p!f!p/[z`1YZW)Yc:f!^]VV3T1X1p/[]pV3T=~< p!^s^]YX ;[]`cd[2/ p!^s1t#n uV3s1T1X1YX>7 Y-5[aYx^]`1Vla []` cd[o2O/ ): p!^^]c:f!f!YZW[]`c:To2/435H- ): pT[]`1YLf!Y-#p!V3:WHcdt`p!V:W)X1YZWSUre[]`Y+W)Yfcd[]p!V3T^]z5uV3fV3r Vys1W)^p!Tb[]`1YT[]`1Y{1W)^[V3tV3T1YTy[;V3r\o0#/ ): uYV3xY^^]c:f!f!YZWuYc:s1^]YV3r\V3T1X1p/[]pV3T=y?@ []`1YZW)aep!^]Y:hpr$[]`1YWHYfcd[]p!V3T^zOuV3fV3rbV5s#W)^p!T []`1YT[]`Y{1W)^[;V3tV3T1YT7[|V3ro / ):X1V5Y^eT1V:[p!T1ZW)Yc:^]Y;uvYc:s^]Y;V3rBV3T1X1p/[]pV3T3ae`p!f!Y[]`1Y;^]YV3TXV3T1YuvYV3xY^^]c:f!f!YZWuYc:s1^]Y;V3rV3T1X1p![]p!V3T 5\`YV3T1f!s1^]pV3TrV3f!fVae^|rW)V3 []`1Y_rEc:Z[[]`cd[;[]`1Yf!Y-#p!V3:WHcdt `1p!V:W)X1YZWHp!T1p!^|aYf!fnrEV3s1T1X1YXc:TXrW)V3[]`1YrEc:Z[[]`cd[ pTcX1YZWHp/q:cd[]pV3TcV3T1^][WHc:p!Ty[ec:TuY;V3T^]Ys#[]p/qIYf/z+^]YfYZ[]YXV3T1f/zc{T1p![]Y|Tys1uYZWV3rh[]pY^BACDFEHG JILKMONPRQSUTHT5VWTUXZY[]\$^`_4a jyj"b ^dcRegf g4bhji c:f!fnlk Q6mLnoqprsmUt5Q4u []`1Yt1W)V3:WHc: V:u#nvk Q6mLnoqprsmUt5Q4u u5z+X1Yf!YZ[]p!T1[]`1Y;fc:s1^)Y^FwIc:T1X3H Y;tW)VqIY[]`cd[ PRQ4SxTHT5VTxX p!^lk Q6mLnoqprsmUt5Q4u 5c:T1X_XYZ{T1YXLc:^\p!Tx[]`1Y|Y-1c:tf!Y^V3rBOYZ[]pV3T1^yA5/[)c:p!T1YXrWHV3s#t#nc:YZt[)cduf!Ya;ffWff[# nz{}|fi~$R8d"d;``.U`~R;"RUH5WUU0yl$UqsU54R 2RR4 d)%R`" U yRR$""qddl RglsR$R FW"''"F`xHHWxj$%`Ryl6LLsx56y)`'%"%`gFU0y Rg`y)`'%" %`gyl$Uqsx56 $"R%R "RR>` RU}5} q '*4% } $ } U } } }g"} `%dR5"%%`g! '*4% } yRU}5} " } F " } J " $ #"R%R "RR>` RU } } 8 $"}RU } } 4 *}U 4 ldx " } 5 "y`W"'`> "g%g`%R0%Rs6R `"%RR %"`" Rg` B Rddl `g`l"lFW"'R%W" R RyRff fi!"$#&%('"$)+*,#.-0/1'2)3%4%5)'76839:!;# <=+7>?A@B# <=#6C#D-$%5%5)'76839E:FGIHKJMLONPRQTSVUWYXZX[LOS]\N^`_bac_8\:de ' R%%" 2' %0ddl `g`l>R6FR F`%%`W.%RR6"q"q% `g%"HJ%`%%")"R Df%RsR %" `66`R>)'1gddl Rg` "ihB%g%% !d `"xR" %`g)' RR 56%l.R"d sRl.R" 8g%Rddl `g`l"]jFR)6$RRF'1g>ddl `g`l Rg`! """RR%)Rdl Rg` "]kq` "4)2gR`)'1gdRl Rg` %`W$Ry%RR6"H"W* % `g%" "q"dR s`R"l &monqpirtsu 7vxwyz7{}|~ 7vx.pypx( Z%"R g% " g`= J $g% "6 RR; 8%`R$R"%Rs g`5yRW"'`F`" H'5xHx>``R8` %"` g%Rq8"R%%%"%Ry"RR}q74])]3C")ZV])$ `73 "C]3 "Cy$ "C7]5fi~[OZ3[4C)u71[I4])y3I"C7xZOVb])[]7I"C7xZOVcC]7+O3"q(E"q"O0O5+5+uy+OOO+[71"u 7$$ uOIC73 "uZcO$C)C)7$$ "O8IC]O8I77 7$$ "OcIC])ZOVcu]u71[I"uOc8u73 "uZcI8]Cy3 1[3V4]Oc81[3V4]O8b3ZOV"C7+514O443"B:+0~0OO++517OO"D"OqOEOOO ]~+O+O"Oq $3"D "1"Ox144O5"3" "O?fffifi`(O4D+O [K)i"OYD+O" O44E?+D71"O?K +510+O:+C"O O" fi! # "tO"O14O3] CO""O4KDO434+ + +[B71" OO"O$tE"%O"DOD+" ]"O71"".+71"&')( u fiy5+O"+4BD3"*+')( u fiDu"O40D+?O,*+' fiuq"O0+ 1B"O5"1.*KO+ +E4:+Z O4"/'0( u fi1"OqO4""4BD1"D2 * K3+"4:+O"(D+O"t 5"O]"Ot4D1" `OOBOO"$E;"O(D+O"` +O"O;"O0&+BDD14D1"76$836$9:83;<E"0D+O" 3 +O? > =(+""13Ett.4 "1(D? +O@153 7+"O1fi}+""O" =++[71".4A".+(+"T4D1"DBC6DE ERDR"4~ +OBFHGJI7RE4O ++[~1" "1qK $4?+(E" OOL"tO?4D1"DMFHG O7O +"O$1"N OG FHG4@OG FHG I! 4O("O4D TOO~ tOD4 OO+ ZD+P "O(4D1"D,OGQR8OOqB"4C 6$836$9H8H;DB"(: S["q:`O+4+O7bZCO&:T+CZ ]OTycO3C bHT+7 ODO3+"4O"~"O;71"TZ5:t?q"O0+ 1~IIZ4"4+5+UWVYX: Z?[VD? [+"5B71"O< :+O"uy+OqO+]\(H $JUWVYX^ Z?[V.O [T(. _8"4BD1"O[> +4O(" O5K $"O5"OEt"O +O417EbD`}7Oa _" =~"BBOVOUWVYX:Z?[VD?[Y"1}(+4I"O=B"FHG 6 b 3 3 ff c C"O=4+5+ O"""OV"OKD+O" 3 3"O(O+e $"O f O3 OO:H $Et(O+(+t="7+"~"=:= =(+"K+" "1"O+"TZg _b+O417Eb OO+OET"?4K $BD _8"5B71""O?4"q+3"O+4q1"h=(H $"""3 :+DEuO UeVEX: ZO[VDO [ Tc~"ZO4+5+ i)jKO"D"OCuy+OO}D+O" y+kimlO"""O]"O;4q+OD+O" n o=}+"K"1i)jqp +3 r7: S["OOimlK:+O4s iml4+O"E" +s &"O4(E"~4.1"&b36$836$9H8H; ctffuKvfiwmx<yRz!{|!}4~2xC5{|gPg{y!|ky!, | x< q?y+} {wmx<y!}!x<!mbc)PgffRmCN,PC!P Cff Jff C ffH )<C3,C@g CPffff+ff:+ H C C3@Hffqq:<3 R%R@5 ff^ff:+ffPYPKe W3YH- P7P# ff+ff:+ff R3eC H3 PCsH, P-3 3<Pg P)C3eP5OWYK??)e<<!EeCffK<Cd3YH- P@POWYJ^?YO>>g335hCK ^km##33HC < 5>H<3gC733^P C$H3-C!CPsffYDC73P P%1NH CP1R1RC307C3PC,d d, 3+P)51m eq- 3^P)HJ.? ff+Pff ffR @ff-ROJ EY:7P#W, YogC33HC qd3P HC-P3CH: 23P^+PCCNCCCP:C33! C qCsEPJeC.CPP)K:R3 P2e3K YdHP3PC3C# 3C3eC3C3eP H NCN3P ffP#)Y E !A<Rfiff PCffPff !#"$&%(')+*-, ./021435(14687q3 C@:9<;=7q OEeP-NH 1P3YPC 3s 3> @?5BA@ + 5e:<3 W C> 7:<3 W C> 1 :R3 D7 1E 5 ?PCe 03P Pg7eCff H Y3P0N 5^J H!CPNH CP7GF+3CKNPCNK H PRCC ,3PC HffCIHR KJ@WC@C3 3#CsK $?LJffMON QPY NCPffR &S2UT V0S-XW0&S2BY)&)Z &S-[T )&S-\YKR@:<3 2 NCPff]_^`fiacbQdegfhjiQdhkml&n2oUp qXln-oXrtsul&n2oBvql&n2oUpwvxzy{}|~/D|&II/& g{jQ@IgxzyIz{j| l&n-oUp u/{j| sul&ml&n2ogo[pk ln-ortsml&n2oBv_Qc@cgQc_(+j{|suln-o\v_l n-oUp kml&n2oBv QcU {j|kml&n2oUp qXln-o\vql&n2oUpwv{jQjj j{ z{j|<|/~&yQI~yQ Dfi gIQ|zUx yg|yK{j~I~I+Q& Q{jj{G{}|&{jI&+Q{j{j$#/g=&+2}~I{jz{j~IQ{G{j#I|IQ= GI|IG/mB2j lg( 2Qg - g +KXj+(Im QQ{j +|+~I}|I||IIXut|j+c{}+Q&+& {jUw{j|Q(&2IGjI2 {jyjjD{j~I{j{j|<yjDz gGI|zD{j|D~I~I+& 2jIm cgD{}|D~+g~I}{j~IBDD+(+c_{j|&~Ififf pm~I}|&B D+(+}{{j|<Q+QIc{j:Q{j|zD~IQ|_{j{}t Dy+& &y|&j{G+j|zQ Oy|_I} {j|zg~I~ID{}yQI ff"!# =_+D&(yQjxDyI&I QIIt|&{GQjI|DuQ{GG{j|+|{j|z2{G&| D{jy$y/j{j~I_ 2y{j~K_+&{j%$fi'&)( *(,+-*(/.0,$1&32 425+-426.0{87[I{jy+&)(/ *( + 09$:&32 42 + 0;& g 2|Q& {}<|&<$ j|&OQI j{j~I_ Qy{}~&Q+&{j2{j&|<mG{j|+=| 0 >(/?2XOcQ@( +2 + B( .fiC 2 . ED~I|{}+<y&{jQjFG &IH0KJMLFN OOP&&8QRHTS JMLFN VU W 8QRHTS JMLXN YVU W 8QRHTS JMLXN VU W 0Z[]\ gY^ V_ 8`ba (Vc-dfeghg Q(&2IGjI2 >H _Q4c g-kjQ+BlmeI onI_ qp r + g ng csoc Hhg ZcQ g "d/cQ Q 4cg-kjQ+ g od2 g dEg )u *ccvsI + KyQ|+z&I} {}||I&(/yQ {j| G~I~I+& Q}/I[2xDy&g{j|K|{}{j yQtcxDyI&I~I+yQ K~I|&{j+K0yx JMLFN OD{jKyQ{jz y+&D{j|<0 JMLXN gI|/G &IH0KJZLFN OD{j|KwE&IH0KJZLFN Q D|y y2 G &IH 0KJMLFN OD{}||/jj+myQ G &IHj{j~I_ QyQ{j~&+|jjD|I # y&I {j|g~I~I{jyI=yQ&|D~IIOG &IH 0 JZLFN I~II|z|/jj+<I~|u~I{G{j{jy&I {j|gz~I~I{j8yIytQ&|/~IKG &IH{Z| }fi~VY8Y8"X"YY< 6 k~YYYoZF4FM<XX84X'X" 4T4X @X<5X4FoF4TXX 84XV' F8X4,M-4X%*5XEXV"4%4/ M*F4XX5X q4XITKZF BFoMFMF5F 4X;X 84XX 4 48MXX4BFoF4fiXX4X'khZm"XVYYyXX-Yy ]M]4 - fmm-8"KFoM- % X4oF"o6-44 ,ZXFZFX444'X F5FXq4B84@F4Z5F'F>9oY-44 fiMXFZFX E ff6 fiE ff6qYfiqY ff65V fffiE! "#%$ffY&fi' V ffE! (# ff)$ffY&*-,+-/.-/01.2%3F4o XmZ-4X<9-V8@144>TMXFZF5 XM*4X*76859fi6 ff:<;8:>=?ff856 :<;<:A=Bff6CXfi@6 ;6 :<;<>D:>=$ff5@ X YX 6 ;5 :E;A(:<;&FDE;G$A(:<;&FHE;G$YIfifi@ YX 6 ;5 :E;A"E;&FDE;G$85JEK D#<;<ffE;L$6MEff<#<;>"N?;HF<;G$YM6 ;6 :<;<>D#<;E;&FD<;L$6E"HF%#$Cfi@E A(#PF(Q$YCfi@ 'E Q &FoM- 4fifX om-Y%R -/.-/0S.S2"( TUffV4 RWXR V RZYR @N[ V\5^ ]EU @ U_KU @ R%WXR @ Ra`bR _ Ra` V R _ RcdR @ feg_ R `R @N[ V\^ ]EUff] U_!U @ RW R _ RL`R \ [ - h _KU @ RW R _ Ra`F @4y4o4ZoM-4XjW iLklmnoi lpqkrnji lpqk nsimft(mfuHt(vF"4<wyx{zfi|~}!%)>!ALa>I("HLqffN<^Bff!HE/P <a &ZLq{fffNE^BHKHNO oE% ?{d oEZf(fH(r//SSA(ff~&aNG1^y>SPh fffs>KGP > qff >> > Psr qhPr >S1 ff sa& ff)MP K>L {Psr &{9 > , >7!ff> > q GO>~ A>< {~ CKE^BG GH!HNy{o G") G!~EMo &b ,~Eg o~E EZo >?% Ea > > 7 CKff^Bff!HNy9 > , >7!ff> > q K O>~ A>< {7 /S/S% N HAyy&/S/SSA^NG yHNE^B K DNHNyN "^j K 8EE &Hs ?EZ r>Sy N,M!>~ KK >Sh K rE> rEZ N "" N rE)"NX MIEZ > "N IE ?>X <Z >~ CKE^BG MH!HNy>>)h > !S q Pr!1P/S/S)>~Kqr ?1 MHDyah%y"rq > 7 {P f(fH(r rS% > > ! >%M% &ab EL%)X &b <a9(ffffX//SSA(ffX aZ> , >7!ff>>Ih!HNKff^Bff!HNy AyG> A>< { N<^Bff!HEy<L &b Ea X %Zff fiEbg &Z >>Ih!HNKfi !#"%$#&'(*)+,",-/.01,&23"450687:9,;=<?>A@#@B C!D%EGF HJIKHLDNMOQP*RTS,UWV X UZY[&\%],^ RJ_`\/^TU?aZRT[b\%P ^?c#_dU+Se?YUf[&PgRTU?hX eiRTU?jkRTS UfP \RT[&\%P ^d\%lmeaZna?o&[ba?[RKn*eP,jkea?a?UZV,Rpeiq,[&o&[RJn%cqn*rsUNeP ^ \%lteuVeiXTRT[RT[b\%Ps\%l0RTS U+V Xp\%hXverw[&PxRT\8eP=]#V,VGUZXyeP j=e/o&\_`UZXV1eiXTR?z!{|U}[&PgR~Xp\j ] a?U?jsRTS UP \RT[&\%P=\%lt]#V !eP,jk_`UNei=]#V#Jea?a?UZV,Rpeiq,[&o&[RJn%c#_WS UZXpUfRTS,U}]#V VUZXdVeiXTRy\%l0RTS UfV Xp\%hX er[&^dV Xp\YU?P=RT\qUea?a?UZV Rpeiq,obU/eP,j3RTS Uuo&\N_dUZXfVeiXTR+eaZna?o&[&azOQP'\Xpj UZXWRT\*R~XpUNeiR}eo&^T\RTS Uua?\%PgYUZXp^TUaNe^TUc[zUzc1RTS U]#V VUZXVeiXpRqGU?[bP h}eaZna?o&[baWeP j8RTS UWo&\N_dUZXVeiXpRea?a?UZV Rpeiq,o&Uc_`UW[&PxR~X \gj ],a?UP \N_RTS,UyP \RT[&\%P\%lGo&\_ea?a?UZV Rpeiq,[bo&[RKn%zt{|Udl\%o&o&\N_RTS U`^~R~Xp] aZRT] XpU\%l#RTS,UV X UZY[&\%],^^TU?aZRT[&\%P,^? X ^~R?ceWrs\RT[Y%eiRT[&P hUv erV,obU![&^V XpU?^pU?PxRTU?jz :UvR?c _dUj UZ,P UfRTS,U}P \RT[&\%P3\%lto&\_Jea?a?UZV Rpeiq,[&ob[RKn=eP j=V X \NYU^T\%rsU+XpU?^T] o&RT^?zm[bP,eo&on%c_dUeiV V,on*RTS [&^P \RT[b\%P=RT\8RTS U}V,Xp\%hX er\%lm\%]#XWUv erVo&UzkW0k,mx,#X eiVS^~R~X ] aZRT]#XpU?^eiXpU*],^TU?j[bP:Ol\X/rsePgnAeiV V,o&[&aNeiRT[b\%P ^?ct^T] apSe^uRTS UX UZV XpU?^TU?PgRpeiRT[&\%P\%lXpUvbo eiRT[&\%P,^?c^T[RT],eiRT[&\%P,^:\X}V X \q,o&U?rs^}^TU?U8Uzhzc X eiR~\c!%% zuy_`\sRKnV,[&aNeot\VUZX eiRT[&\%P,^+\%PAhXveiV,S ^eiXpU !*+%pN pZ'%Z=Ngv%c,eP ju!/ ,LQv,s?fJTN `p}Qv#ZZZzyS UfV,Xp\%hX er ,b0sqGU?ob\N_] ^pU?^q\RTS=RTS U?^TU\VUZX eiRT[b\%P ^yRT\^p\%oYU}RTS Ul\%o&o&\_W[&P hV X \q,o&U?rz[YU?P*RK_d\P \gj,U?^!t?~[&PeuhX eiV,S,c,P jeuP \j U3RTS,eiR`j \U?^ P \R qU?o&\%P h}RT\ePxneaZna?o&[&aWVeiRTS[&PsklbXp\%r RT\ z yS UfV Xp\%hXverN ,%ka?\%P ^T[b^~RT^y\%l0RTS Ua?obe] ^pU?^?|, ~ tQ Q1g N,,TGQ# ~mN, TG Q1%xTQ#~!Q1# g~ Gme] h%rsU?PxRTU?j_W[RTSRTS UV Xp\%hX er,0 |\%l!RTS,UV X UZY[&\%],^W^TU?aZRT[&\%PzfWS UX U?obeiRT[&\%P [&^+^~VU?av[ ,U?j e^kRTS U'P U?heiRT[&\%P\%lfffi cW_WS,UZXpUfi i%~ ~Qs[&^*R~Xp] U'[&l}RTS UZX U[&^=ePeaZna?o&[&aVeiRTS\%luRTS UhX eiV,S a?\%P,P U?aZRT[&P hRTS UAP \j U?^k|eP j eP ja?\%PgRpe[&P [&P,hz \X[&P,^~RpeP a?Uc,v v0Qt Z Qtp WS \%obj ^}t[&h%]#XpUffx zq^TUZXTYUtRTS,eiRm ,b0f[&^P \RRTUZXprs[&P,eiRT[bP hl\X[&P ^TRpeP a?UcRTS Ug] UZXpny%x1vt Z !T,S,e^ePk[&P#,P,[RTUyj UZXp[&YeiRT[&\%Ps\q Rpe[&P,U?j*qna \g\%^p[&P h/e^`[bP#V,]#R`a?obe] ^TULeuYeiXp[ePxRd\%lpRTS U:a?oe] ^TUeP jqgnA^TU?o&U?aZRT[bP heo_e?n^u[&RT^}Xp[&h%SgRTr*\%^TRfob[RTUZX eoz:\_`UZYUZX* ,bx%[b^o&U?lRQLRTUZXpr*[bP,eiRT[&P hzOQP\Xpj UZXRT\=V Xp\YURTS [&^fX U?^T] oR}] ^T[bP h3ea?a?UZV Rpeiq,[&ob[RKn fUZ,P [RT[&\%!P z c0_dUP U?U?jART\,P j|ekrs\gj,U?om\%lN,,bx%RTS,eiR}[&^}eo&^T\kesr*\j U?o\%"l $#&%'K ,b0, cG_WS [&a S[&^+X eiRTS,UZXfj,)[ (a?] oR?zf+\RTUeo&^T\RTS,eiR:RTS \RT[&\%P,^W\%lm_`UNei]#V# eP j]#V#Jea?a?UZV Rpeiq,[bo&[RKnkj \P,\R:S U?oVRT\^T[&r*V,o&[blbn/RTS U}V,Xp\g\%lJ*z :\_UZYUZXNcm_`UaNeP^~V,ob[RN ,%[&PARJ_`\^T]#q V X \%hX er*^N,+ a?\%P ^T[&^TRT[&P hk\%l`RTS Ua?obe] ^pU=}eP j +a?\%P ^T[b^~RT[&P h\%lRTS UyXpU?^TR`\%lGRTS U:V Xp\%hXverzm:\RTU+RTS,ei*R +mWUvRTU?P,j^ +NzyS UZXpU?l\XpUc[&P\X j UZX RT\^TS \_RTS,eiR: ,bx%k[b^`obU?lbRQLRTUZXprs[&P,eiRT[bP hc[&Rd[b^^T.] (a?[&U?PgRdRT\8V Xp\YUfRTS,ei/R + 1R +0 + [&^yeaZna?o&[&ac#RTS,ei2[&^yea?a?UZV Rpeiqo&Uc1eP j3RTS,eiRyRTS U}a?\XTX U?^~V\%P j [&P h/o&UZYU?orseiV V[&P h%^ eiX U^T] [RpeiqonXpU?oeiRTU?jz4365 87'9 s:0<;\Xprkeo&on%c_dU}[&PgR~Xp\gj,] a?UfRTS U}l\%o&o&\N_:[&P h/P \RT[&\%P3\%lmo&\_Jea?a?UZV Rpeiq,[&ob[RKn%z=<> >fi?A@CBD8EFHGCB.FbcIKJHLNM.OQPffRS*TUV WYX[Z\<] \QW&\&^)^ Z\<]Q_ \&^`] \QW<_ \&^ Z\QZ_[\&^ WY\<]Q_)_baffcCdNeHfghjikKl1mbn&mpolrqsutwvQxyNzC{}|~8~N8PPffeHP}PejYCJ4LpdO/'P2PffgP/dNe4MgQPgg`rpdO*gdNP 8CfeHPP/JHPOOQPQYJ4dNdNJ4g/Q<[4b[`1J4cPffpdNeHeHd /JHL dNfCJ)JHdNgcdNe4fS//P$Pfg//JHg/ eHJ `O `1./J4g/ PQYCeHP`O `1. fR/pdO2P}PO LOQdNMfjJHgQ P, \ \dNfi e4MgPdN .bdO2P}PO .JHJ4g2fPCPfJ4fAJHgd dNgO<JHcPOQdNLO$JHg}Q$[ J4cPO<PffP$JHgCfAg`JHg2e4d PQYeHP`O `1.f/J)ccCP,dJ4dNdNQ*JHgfCPCPfjg"J4cP,OQPJHdNMggP JHdN OQPCeb JHL` Q` /K \ \ JHg,LOQdNMfAJ4gQ ffP dNCfj <1 \fffifi/cPO<Pff < \ \ YOQPffcPeHJ)PO<eHgdN1 \ &\ p /cdNgPO<Pe4YJHdNg2d M.O/J4dOQd }PAcPAeHdNLNMPAdN cPdO<P pdOeHd [dNMfPf 8MPOQJHPg *Pggd J4YP,J)c\&)^`_adN'MCe)JHgPg/J)c pdOeHd [dNMfPf 8MPO J)O XQX )^`_OQdNLO$fJHPOQOQPQYJHdNr]$Z X Z K` \&\ Z ` a<\YOQPffcPffe4J)PO<eHgdN,cdNgPOQPe4YJ4dNgd MO,JH')^`ff_C i8#"}o%&i$& qs(')Y"}8 Y*C ~,ff+.-8N)/'bN,ff 10}Q$2[p Q4b[[13.b`6C"N fiQ< 'N#[$ Q4b!5 ! 6u/cPO<Pff < \ \798,8fi:<;>=?@BACDFE ;6GH@BA.IKJ.@L=AM=NPOQE ;>IRSUTV=WCR@LXY:<;>=C;>IG<Z[ \}]9]_^(` ac bRy dfe6gihhKjUkLl4lmknPkLo/pqgPrmhsrmbRpqrthKjuavb6d(h3gid(nxwzywz{}|vb6d~g#d%hK6d%g#d~eRop3(d(lM}i3 |vkBrm bacbRdj/hKoLoh9|vkLRg#d(lm6oBrFkLlvpURkBgid(~rc(hK6lmd(6d(6(d}hKjavb6d(h3gid(nlFzy1p36Y>yyW,WR,Lz(W \K&] ff \] \_3 / ]K_ ##_ ( U ^} ] ^ < ##_ ( `kLp3llv oLh9jRhK|coLoLh,p3|v(/(l(Md~y e6WriBpqRoLd3)9y<6 hKR6/&likL 6ffd~Kg&rmbR>dHe6gi>hKK3Rg) p3n 3,K6>R63 Rz,R6R d& lmb6Kh,|zKrmbR #pqKrcyVrmacbRdb6d(e6gihKrmb63g)dp3e6n gih9hK6je6Rgih/&( d(d(ffK6 lyvavb6de6gihK3g#p3n K #d)>rmd(66l ,6Rzyvavb6de6gihK3g#p3n K # 96V kLlcp3~(okL|yg9yr(yRrmb6doLd~ d(onpqe6eRkLRmW, ff K V _ , wzyyvkav b6dF6e6p3gin hK3eRg)op3dn yw 96jh3gVVpq rmhKkl<np3l (|v(d~kBe6rmbripqgiRd(oLo/dvpqrm|kLhKyg9 yr(y . p36 1{ rm b6dvkL rm{d~gie6gid~ri{qpqp3rmRkLhK u { |vmWkL9rmb ff.K6 Vd R R d(_ p3 l, p3R |vkBrmb {6l9yr(y/ mW9 K {p3R {R {p3R pqg#d}p3lc d(jh3gidu 6p3n eoLdQywK#y>yQFhK6lmkL6d~gvp13gihK66 kLRlrip36(dmW, ff K V _ mW9 ff K V _hKj #y<avb6d(mW, ff K V _ , w 9 mW9 ff K V _6d(hK 6pq 6rm6hKkLhKd(RMlmkL{ 6p3kBlcd~rPgMjffp3bRrmkLp3b6oL.ldgipdzR Rd~6gmRkB rmhKd 6 6 6d~ g#l(y 9 Rrg#Rd( d3){F |vkBrmb p36lm_| d~g ff ) ,y Qh3rmkLd((9dYp36rmlmbRd pqrkLlRoLlmh,kL|F6)! +*-,,!fiff" "$# &%('/. &0," "$# &%1fiff-$24365FPOfiG HAI87 97;:<7>=?= 87 >@<7A= 97 !@<7>= 87 B@C@ EDKJL JL NMQ8RTSVUXW>YBZ\[^]\[`_a[bdcfe[hgjikgB[l^m&n\bpoqWAeY1rstWAgfiuvmwn\xyYBma[`nkLlPd lmR66nkBrmnkhK6pqgilQk (wzd(y {kL Qp3d 66kBzrmkLy hK e6gih,y k6 dd(oLp h,|nPy d~rmb6 hYrmb6jkLh3lPglmeRd(gi~h9rm>kLhKkL6 1{F|od(djr_ 6grmd~#lgirPn6kLRklmpq(rm6khKlmYl hKp3jH63p p3zri p3 K{Rd(|vlYb6p3k6ib.hKjg#p9|c p3l rmlQbRpqhKr j kLrm6b6(kLh3lcgmne h3d~g#rmpqb6rmhd( ly rmb6Qdd)>6r(h3{rmkL| hKd}6lkLz| rdQgihbR6p96 (dQd}kp rnPgihd~6rm6bR(hzd(6PhKokLhK13rmbRdvjh3e6gvg#e6d~g#h9kLhK>RkL6lH1lmd(oLd(~j/rmr_kLhKrm6d~l9giynkLRkRpqp3rmkLoLhKoBK {| dKkB d}p3Yd)6p3nPeRoLdQkL h3gi6d~gcrmh kLoLoL6lmrg#pqrmdQrmb6dnPd~rmbRhz6hKohK3Ky6 6 R > 6 . RklckLRl(yPr(yRpd(kBnrmbRpd~.g kLnp3d)olmrmd~d(r R6l hKj (vhop336gclmd(kl (dhKj d~gilil(pyr(y j/h3ginlHp3p3~(oLkce6gihK3g)p3np36zy _j d)rmd(6Rl crmb6d(zK{|pz~}$M`{C8~\~qfi CGI C fi HAqq<CIC1 )HC<d'B!Hk<fi?A>aB<B1qP!A1yB1AC> dd\!!C1dyKt ddABAK1A6A !$C<!y> <?!A!B!/AA!A\^T!A6!tB9^`<A f \ ddA; AAAdaB<B1qP!BhK1AAC/A <yKt vAAB1A ddN!TN\!A1< dd~!!CA1\!A!6BF!AAA!!Ak 8!6B++^9\AA!A(/!9!KA qt1A?>6A ) (1/A \ ( AA ddf \ AA~!A!!C!tAKA?1K1)BK>/!p A!/!KAC A9!A8>/ABt;ak1dqAt!\AKd!KCA8Afiff !/!TA!A1(AkCA!!(1A1!A1AdAtd!AdCC6p6?A1AATA(/! XC>AK<(!A /!!C!q;ak1Kd>q8<!t!A!AA!/!fqB !AdKAA6!A A!A!<d(B$A ?AT6><8+y1(A&!A1A!!qA9A!!d)BKCAAA!K^!fi^A!/K11d>h61qA!AqA<)qA!(/1(BA(8>>&!A1(wL"!$#&%('*)+',-%/.103254687:9<; fi=?>-@$@ 7 8ACBDFEHG8EIA$JFKML dBh1AA!NN<&O ! K\yqd!3k?Ad^6!A HPd!KCAh6\PQ RTSVU+W-XZYH[\[\ZW-ff]Y?^_`aSb3Tb*ff]b\Adc`ch)eYI -f)HPhA qAAAq AAqhgCq!!h!AA!A)`!8 A!qB$8Aq/ia\ZjlkmOjnporq 1s)pO)HP tvuIk/w`<A~A yxf \w`<A ? \qPq!? >z|{+zfi}~y-`-H~C`8:8--HfiH~fihvRfi}~--~-Qfi|I"CfiCCmC\|H"*Im88ITfifi`C:pImR*ICQFHCTfifi`:1/V-fi3|CCI3:V:MCT88ITC-`3yC`:I?C:C:TCfi3`:lImfilCTm-HmfiCh:p"I$fi*I"::h1IC8C1T|ICC3|mT|-R:&CaC:IIfifi|mTHT|1CIp3::C|mT|R-C:C:Ch8C1T|fi-:&CC:I\ICTIIFI3C|II1"I?3MCIp3:MCfifiI`CI:3fiIlCTfiC::ITC-`yCIfi&8\CvTC3C\CC-|::(CT8CITCfi""C:Ca|CI-Ifi(* :fiCTIHICQC:I\I+"":&C|Hfi`fiT88ITC-ICQC:I\I +\CC :\:CCTfiMTH"CfiC&H8:dyHeIfiC$TC\T:C+C+Z* e\13IpyC(: +\ IC:( :\ $\1T`H`:Z*e- ++ZC &H 8C:* Ifi8HNdy `*:3|fi :\-TH-ffQTfiC: fiyfi|H3ITy3Md|dC3CI"Cfi|-V:|I|ICICCC:II+":&1|ICQCC:I\I+\:\-IC13I3"yCCICTp8\-I::8pTC$`3I:QfiT$I`:THTfimm|ICTfifipIfiT$I(1I:8pTC$(CTC"C:1::TC-r`T`H`C8fiC:I\IpC$C"-::Q8fiC:I\I1:`3ICh1:`-h1|IC:mfiyC`:IC:88+m|883|ITC-`ICTfifi`:CIfiTI3:C\fiI1I\3C:|CCT-C`:1|I3--fiCImCTC\IC"RI:!#"$!&%'/1)(+*-,/.102.43658769'7:.8;=<?>A@=7/;CBEDCFGHJIfi`K$LNMRC|O QPSRSRSRQPUTp|d-ITHTOVXWZY P L\[ ]_^` Uacb ed |`CCfUa`CTUa&IghacbiQj2kfilnmEo8p?q=rcsEotruAvxwzyE{Ew}|~{=z-wSffwS{C-cc=wcwz/wSO6=E|c{=n /t 6O /NSS/Z4 6\{==w}|c{wzE-wz-6|c:{ /t } / SS6O &:cc2Sew2hX|cfz?S~|cx=wS{OyE{=OEwS-wS4:{==|~{=ecwz/wShff6='|c{== Q =wz-|cwE-Q/wE6X|cSSwz=-6EcwQS?cwz/wSEO6=E|c{E= /K{E{|~{CwzE-wz-6|c:{2/2e_|~{===z|c:{8tE:-wE6 :|cx=wzyE{EwS&wz/wz {=t=4:weE62 |c=wzy'{=wS#wz/wz eff |ch|cfz8Sc|c {En#gwz/wz eO |chU|cSSwzE-6Ecw g|) eXJ f=wS{/ffh cU |~Kz8Sc|cg=wS{=-w= 6 =wzyE{Ewcwz/wSO6=E|c{E= 6 &K cU4QSUcU hX|~z8Sc|cQS4 6 {=SS#c-:={=|c{=-{ESwSQSSSQ :S~=wS:UcU &wz/wz eN |~ |c=wzy'{=wS|c{U=wS{6 g U 6U|~z8Sc|c}=wS{=w 6K=wzyE{Ew}ffcwz/wSO6=E|c{=\h 6 #UcU hQS)|_ egwS|)=wzUcU h|cSSwzE-6EcwneQS wSS|#c|cSwSff=wS26 {=6 |c{OE|cfQw-wzf264wx6gyE{=wSS|#c|cSwSO?=wSE26:U h&4 {=n4wSS|~c|cSwSffff=wS26 :UcU hSS#cf:={=J|c{E-{=SwS SSSS2 X :S~=wSg:hcU{=#xwz/wz \ |c |cg=wzyE{=wS|c{nU)Ug=wS{6 g 6 Q|c|KEc8-:={=|c{E-{=SwS+ QSSSQ X :ES~EwS:EUcU{E}&wz/wz|c |cf=wzyE{EwS|c{h=wS{ 6 g 6g4Q/w ffZ /t C6wE-Q/w'6=|cffwz=}|cSwSz |_w '6X|ccwS#wz-ff|c{E6|c{=g|c=Ewf62/wwzE|c6=Ec|cQ6'cwfU=wQU|)\{E:{8-:={=Ewz-|cwS w}=we=we-|~:|c{E{=|~:{:4:={=EwS={=wS?ffg=g{=wStwS-=| =|cK|cffwgQS==w|~{CwzE-wz-6|c:{-wS-=)|c{=#-:=wffwz=?4fffiff fi! #" %$$ '& ( )*,+- +/.1032452672 98 *,+;:@? ?<$ =1 >> CB f CB SS S2B CB |cf B -:= {E|c{=-{=Sw:C${=2/tED SS #4O#$&'tt=4:w'6Ew6|)|c:{ SSS2 :X/t{=2/t 6w=-|c{=wS\=|c{=EwwzEO:vxwzyE{=|)|c:{ ? 8 Uwz SSS2 X f=wS{|~XE }2 6 x|ch&wz/wz =w-wz|cKyE{E|)wFHG ?,I$JKMLON )QP;P(QRS-T6VU8;6W6VU-XP8 4Y6[Z[6[Z[( *OXQSSSQ ]\ E /8 C 8 *+x2/8 C 8 4 -T(&5678 ZV*- +)w R^ZV*_`6VU-bac-S6VU,(S{S+H]|(@df} eg-ihC*]Z[6[Z[( *kj2ml,2Xn-S6$o&p-q8T& ( )*,+-p+srS)]-54^t%0324Y2u672t C6v8 *,+f2 8 C6,2U-5*x-5y -54^tHz +-54^ZVyQ8~6[Zi( *x(@d$ZuRWhC*]Z[6-g8 *,+Z[6Xp(~*678 ZV*ffRH( *t& ( )*,+-p+r5)-Y4Z7-RY24(S(@d52 wSQ~=E62/t C62/tSS26==4:={E=wSEwz$}$QSSS2^$ wg=wyE{=wEw tEcwfe$ => @?~A ? > - $ =1> ] SSSQ$ $ => / $ $ =1> / :8=)|cwz|)Z#nE-: {EZ{|c{Cwz-=-wz-6|c:{ $ ~'<$ = SSSQ <$ ==SSfi3,,#;]Cff~ffM,,k%;;gM3,,,3]Y^^S555 X Qp% ]M MX~f]~ p5Q ~]b55ff#MC]p~MXpMkQW]T~`~E]5 p5]`V~MMM]g 5Y M~sMMM] ^Q sQ]MSQ M~~ M#kY[O9,V,TEkE~V,vW] kMq!]T]^~ ^ OMX]5 YpM5c]M^vV bs ^Qv ]vb']5XW ]k]]g]] ,Qq]5b] #5M~s c Y5MM%,Q vvup~W gC% M]H ]M]^~ ^b`]Y]]Yp% ]p~ ^ ]M^V g]~5k]1v ^Q bc]]5bWs ]5~v] 5W ] Qcfi ff ]p M]5k^~op5k ]T M] ]]5C ,Q]'] 35M~`Y5MMT,Q Mup~ WT v, ] ]'Q^55~p M]M]kQMS %~]T~f ]^]]55 fM~]ps ^bcfM~ffpsfi ! #"i $&%"i "i %'&"!"i(! '&"i "!")`]~M]q]p~ ^ * 5~] q~ ]T5 ]p5,+-fi.0/#132546187946:32546:87;=<>@?fiA3BDC /&:2548EGF;54HI-*JK/#1K2K461879461L4M:2fi;4+- NO/M1P4M:2546:;4+-.9/M1K254Q1704M:L46:87;SR,+-fi.0/#132546187946:L4M:T;U<> HVI*-J5/#13254Q1794Q1P46:;SRW ,+-(NX/#1L4EYE[ZL4Q\F^]:32FL4M:7*;_<N- N8`-fi+9/M1P48EGZL4Q\FT;4+- NO/M1P4M:2546:87*;R,+-(NX/#1L4EYE[ZL4Q\F^]:32FL4EfiEGZ46\FX][:7F*;_<> NT-(N`*-fi+0/#1L4EGZ46\YF;4N- N8`-fi+9/M1P46:32fi;54+- NO/M1P4M:2546:87*;R,+-(NX/#1L4EGF4E[F;=<aRb ?YA3BDC /#:L4M:T;c<@RdVe fbfigih*jkYlmon*j8mp*qor*sut#v*wxpy&z{Dy|D}~*T)K=v*wywq|(t#ozUV*9Mfi6*6)os*w*wUY_t[Oz}r8t#rT|Dqoq!w'8qors#!wV|Ds&ws*zDy#y&ws#p3z*o{t#zt#vwq|Drs#ws |DXOq|Drs#w Ows&y&!KwsXt#vwV|Ds#wvwy&wt#vwy&ws|zfi*wt#vT|(tzYwszDtKwqoz{t#z_|DfiU|DqooxpT|(t#vo=y&z}9t#zfiy&sMtPt#vwy&wq|(t#z=#Vosrs&w_t#z*s&r&vU|,*zfiwD*w'Yt9t#vwzYw|D_t#vwzDy#y&wsMpKz{i|(y&s|(y&wwqowt#wy&z}t#vw{Dy|(p*vK^rs&o{t#vw,y&wq|(t#ozT*|Dqoq!OV=osV|Dqqowy&wry&s#!wq!czt#vwy&ws#r*q!t#o{{Dy|(p*vKOq|Drs#wws#y!Kwst#v*wuT*|Dq0s#!t#r*|(t#oz5Kvwy&wzfit&|Doszqo,zYwst#v*|(tKwqoz{t#zs#z}wz!t#s|Dqoop|(t#vs^yz}@9t#zLvwy&wq|(t#zx96*6vzqosOot#vw{Dy|(p*vosLzDt&|Dwy&z}t#vw{Dy|(pTv*Ywqowt#o{u|Dqoqt#vw|(y&sXzt&|Do*o{t#v*wzYw^z*tos^y&wr8ys#!wq!w*wxYt#v*wq|Drs#ws*^|D3|DszwOzrqow'pKwtTs#wy#wt#v*|(tYrwy&wszt#v*wzDy}aV9Mfi 6O|Doq)zDywwy#x9Mfi6*wpy&zwt#v*|(tK*suqowt6t#wy&}o*|(t#o*{Y_rs&o{izr8yKzDtMt#z}[rp_}wt#v*zfiKKV|DiKwpT|(y#t#ot#ozwoxt#v8ywwpT|(y#t#sULos^t#v*wpy&z{Dy|D}~*SzS9|D}p*qwfiU0zs#sMt#szPt#vwq|Drs&wsS9z^Kp*qor*st#v*wq|Drs#ws p*pz~VT3USzs#sMt#szSt#vwq|Drs&ws 3T|D*,Oz9KTKoswV|DsMit#z&vw&,t#v*|(tSos|DqooD,zDy&wzwyV5Sw't#wsS(3|D0OwV|(q!iw'Yt#w*s$yV$t98zOwV|D_|(ppTq!it#vwKzDtMt#z}[rp|(pp*y&z|D&vt#zz*sMtMy&rt|xqowwq9}x|(pp*{Q6fiD |D*,|Diofit#wy#py&wt&|(t#z QMD Lv*wpy&zYz5py&zYwws|DszqqozVsVU os|Dwp*t&|(*qow$y$t 8 |D {!wio8|D}p*qowfiU0 osO|DYqo$yV$t 8 wTw|DsX8|D}p*qowfi)zDyM3V|D|Dxs$t96*6* fizDy&wzwyV5q|Drs#w zX~V*Tis&|(t#sM*wst#v*wuz*!t#oziywq|(t#o{t#v*wt[OzxqowwqS}|(p8pTo{szDy&*wyt#zw**w 8 8 D| Ow|(ppTq!pKzofit^Ozs#wyOt#vwqowwq5}x|(pp*{9Mfi6*6fiKG0)*6|D*iqowt96*6*8qoosMt#s|D*iw!t#vwy^T(5zDy*$#V(,#P G968 xqoosMt|D, )fi G KG08 fi fftoswV|DsM,t#zx&v*w#t#v*|(t |D |(y&wsMpKw|Dqo w}zfiwqsz0P |DO0(ywsMpKwt#!wq! t^y&w}x|DosPt#z&vw&t#vwt#wsMt#soxpKzofit#s^|Do^zs#owy|{Dy&zro*sMt&|DwV*9Mfi6*6KG0)* (#V(9MfiM96*96*68M9Mfi6*68fffi!#"!$&%'(*)+#"-,fi.-!"/!0213'("4'(,457689$4:;!$!,!)<=fi>@?BAC+DFEHG4IKJ(EMLG4INLKOP QSRT*UV?fiW V8X WZY4?fiWZY X P []\M^`_*P Y!? Pacb7dfeM^gPihkj4VmlonpU Y4?fiWBq]rsA P []\NtP QSRT*UV?fiW V8X WZY4?fiWZY X P []\^`_*P Y!? PacbM^uPivw9RxNUVP QSRT*UV?fiW V8X WZY4?fiWZY X P []\^`_*P Y!? Pacb7dzP Y|{ PKacX}^gP QSR~FUV?fiW V8X W VWZY4?BA P [fiyWZY4?fiWZYA P [fiy C[ y+ ][ \ P ^hkj!VmlnpU Y4?fiWBq|rsAW QSR~FUV WZY!?fiWZYACHG4EKYI]*Y4?INE=}-*m=fiELG!INL fiPPPLKY4?^qBr!I]*Y4? P @C G*EKmP QSRT*UV?fiW V8X WZY4?fiWZY X P []\=fi* *EI]=fi**g^`_*P Y!? Pacb7df_*P PacbM^uP Q(RT*UV4LI]*KEQSRT*UV=fi>?fiW V8X WZY4WZY X P []\ C?fiW V8X WZY!WZYAh+vw9RxNUV?fiW V8X W VWZYAX AC+DFEHG4IKJ(EMLG4INLKOP QSRT*UV?fiW V8X WZY4WZYA P []\^`_*P PBacb^gPivw9RxNUV3 EJ(ELG4INLLG*E*EKEK4KE=fi>LG*ELG*=fi-LLE 4INL =fiCDQSRT4UV ?fiW V8X WBq]rWZYAACLK EK>sLZLELEIhkj!VmlnpU4INL =fi=fi 2*=?fiW V8X W V?fiWBq8rsAWZYA P [fiy C7>**4I]EK|LI L=fi4INI]|LEKE EK>sLZU LIN(E>s=] *LI]4KELG*E3|*E=fi4fiE@G*=p28-]LG !INmE@E*=]m=fiEK& E2ELG*=*3>s=]4=J *2LE]I]|LGEKmEKLL=EK= -L =fi2G*EKE2ELG*=*3K=fiLGK=fi*L4L J(E*EK(INL =fi;I]*= =fi7E EKL =fi E]C4 *E}LG*E*=]L =fi4=fi>@I]KKE*LI ! L&I]*I] LfiCG*E/*=J *EI2=]E*I]L 4=|=fi>LEKG* |*E3>=]@LEEKmCpDFEG!IKJ(E4INL =fi=fi>IfiEK*EI =fi *=N!INL =fim(G4EELG4EEKI]|L4>s=]INL =fi*EK*LINLEKLG*EE EJfiI]*KE=fi>9LG*E2ELG4=|* EI]*p=fi>9=fi2EE*I] EKKfiG*=!INL K IN+LG4INLEK= -L =fi2I]4fi2EK|LEKLGG4I]K=fi*L*L (J E3*EK(INL =fi*+m=E>EK*=fi*fiG;L=>s=]I E3I]* EK2EK|L |LEEKL 4H4= 4 EK *=fi=fi*=]L=fi EI]=fi *C(DFE=fiE L=K=fi* *4E LGI]c= EJ]INL =fi=fiE INLEK=]9Cc4LI]*pEK-EKGU ?fi]SA SL=|*4KEKI2=* INkIN**=(I]G>=]+*=J *MI]KKE*LI 4 L=fi> =fi 4=fi]I]K CE]C#SLG*E*=7*=]L*EI*=fi]I]2EKI]*ELG*=fi]I]K=fi|LI *7*EK(INLEKINL=fiKC++=J *LELkI]=fi*|LL==fi>+LG4EK=fiE L =fi*=fi>*I3=|*E =fi>4LG*E@K=fi EL =fiM=fi>4I*=fi]I]L *=fi]I]KC4LI]4kEK*EKGE EKL =fi; E]-G*EEM=fi4ELEKL=2 2 s> 7LG*E*=|=fi> * *I]I]m=fi 4 ]E =fi #4;I]}K=fiK L@LI]94 *2=*E*=*=]LLI]E LG 3*= 4 EKCLG!INmEEG4IJ(E*=J *EKFI]I LE4INL J(EIKF=fi>@*=J *LE4INL =fi=fi>k|K9fi =fi;I7=|* IN+IKfi!4 *HLG*E*=]L =fi=fi>pI]KKE*LI 4 Lfi*EKEK2IINLG*E #4INL =fi/MCiCiLKCLG*E= =fiLL EEKI]SL*EK2EK|LI IK* *LG*EELG*=|4= =fi]*LINLEK*>=]INL =fiEKL =fiC(44+4`48(G pEKEINGI]+!INL }**m=]LEK 7LG*E*L I]EKEINGL fi= 2fi? eU =fi24 =fiX AC =fi (E@L=LG4I]-SI]-LLEK>=]+*==fi>*EI] *3LG p!INmE(ML =fi>!*L+>=]+*=]m=fiZ*2LG*E7L**=fi>I]I]*I]KKE*LI 4 E7*=fi]I]K-I]-/8EK* ->=] EI]I]|L K* =fi*KI]@E I]LG*EI]*=fiS=fi*EK>EEKEK>=]*EK> *fifiEKL =fi*I]*2K=fi2EK|L=fi2I]EINE +J(E =fi=fi>kLG !INmECNfi;4*4-&*Z*(44#fffi @"!$#%&!'&ff)(+*-,.0/1,23,465798;:&2=<>:&?@AB798C2EDfi>FNHGGJILKGMG*K>#NO#+PQ&RSUTV#% 2"!$#%&!'&ff)ff) W"%X'&WYZW"%J'N[#W]\^'_($`B![aJcbVdB,)ef:A4g2H5hi:jk :[D8;lnmi4:[DJ45?o?p8C2EfiD >qSFsrutNv wKfxE*Ky#i{zNYB!$([|"~}oz!$#Laf W"%[!$ff) W'N[ #W=#n%W"!'&3"!# #% "!$#%&!'&ff)(Wmi4:l,$,$&8C2ED&:6j7CdB,g2H7><>:&2Lj3:2UbVdf,$:&4[,798;l$5hf@f,[l7C1:6jn<>:&?@AB7,41H:6jg79.5&4[,E+#;TV1I&M !*H&MJILKf& "!$W"%!i+!$'&%f*K's!'&#C#W"Ymfi+&RSoT#% "!$#%&!'&ff)ff) W"%'&W"YW#L~Y%-!"!$([W|$'N[ #W]bVdB,eB:&Af4g2H5h:6j k :[DJ89lQmi4:$D465&?p?o8C2E(D NqSF&rt]v wx&GKLRJ!'N6J#gVMJ1mo k /m46:[D465&?p?p82Dj:&44798 l8;5hg27,Lhh8D,2Hlg,+Y"Y" ($#Wf([|'&WH}o>JQi#W"(6!$`"[ aJnW"%J'N[ #WH'&($Y#W/[|"o#ffH [YY'N$'sH'&($&yWmi4:l,$,$&8C2ED&:6jn7Cdf,p7CdXg27~<>:&2Lj>5&2H)"&?@3:2 k :[D8;lQm46:[DJ45&?p?p8C2EfiD 9*]gKIE's!$!NJx&J k :[DJ89l_5&2H_o575S$5s,+|'NNy%J'N[ #W'&(o'& `"!$]84&GKGJEzW`"ffz!$([K( "~p}B$|B!$J]+}o#>}#&![]w&RS!$ff) W'N[#WU#i #% !$#%&!'&ff)(\-1|^W"aJ!6WY" W"%( [#&![pbVdf,neB:&Af4235h:j k :[DJ89lQmi4:[DJ45?o?p8C2EfiD >qSFsrutNv Kf&M}!$([|"#~#[ ]1-~Jx!$ff) W'N[ #W#!$1!$[ W"%feB:&Af4235h~:js?^$:h8;l<>:&?@AB75798;:&2*-fiMKMa'&W"(KBJy%J'N[ #W'&(C'& `B!$i'&('&W)'N*"!$#J'&|[#H[|"1|'&W"('&W"Yff)Y"!$ff)#]8"!#&ffWmi4:Llg,$,$&82D&p:6jQ7Cdf,-tJ2Hg27,42357989:2H5h"&?y@f:s8CAf?:?y -*H&GKfxEQ#'& ([1Pn#{f!$%#]Kfi MJ #% 3'&([YcL'&`" `"(^#QaJWS[(L*o,./1,23,45J7989:&2<>:?y@HAf798C2E(D H- HMJxLKJIEfi's!$$|"#&!$ JIFff)[|"#EY# #%&U#&!7"!$#aW"%;[!$ff) W'N[#W #1%W"!'& #% 2"!$#%&!g'&ffX(Wmi4:Llg,$,$&82D&:j7Cdf,q7Cd g2H7,4g2H5798;:&2H5hieB:&8C27X<>:2Lj,46,2Hl,:24798 l8;5h>g27,hCh8D,2Hlg,gew<yB F 4*GJI&MKGMJxEfi's!$$|"#&!$ "MJW[!$ff) W'N[ #Wo#%W!'&" #% @"!#%&!'&ffX(Q!Li#W"(6!$`"[aJiW"%J'N[#W]bVdB,-ef:&Af4g2H5h:j k :$D8;lQm46:[D465&?p?p82ED(tf qB3MKfii's![|Efi"#>'J(z1MJ=f#ff)| #([#]| L'&"!$#&HffX(-!#ff's! ['&] |W [%W"& 5JldS8C23,g27,hCh8DE,2HlK, - RJMGKfI&JE([([#Wf-VJ1m4g823l8@Hh,-:j4798 l8;5h"g27,Lhh8D,2Hlg,"!$ W"%![+!$'&%f[!$ W"%BT+#Hf|'N!# V&RSbVdB,4g7:jmi4:h:[DJfi_uz!$([(L[|"($'&W"Y-]# W|o#fiJournal Artificial Intelligence Research 4 (1996) 37{59Submitted 9/95; published 2/96Logarithmic-Time Updates QueriesProbabilistic NetworksArthur L. DelcherComputer Science Department, Loyola College MarylandBaltimore, MD 21210Adam J. Grovedelcher@cs.loyola.edugrove@research.nj.nec.comNEC Research InstitutePrinceton, NJ 08540Simon Kasifkasif@cs.jhu.eduJudea Pearlpearl@lanai.cs.ucla.eduDepartment Computer Science, Johns Hopkins UniversityBaltimore, MD 21218Department Computer Science, University CaliforniaLos Angeles, CA 90095AbstractTraditional databases commonly support ecient query update proceduresoperate time sublinear size database. goal papertake first step toward dynamic reasoning probabilistic databases comparableeciency. propose dynamic data structure supports ecient algorithmsupdating querying singly connected Bayesian networks. conventional algorithm,new evidence absorbed time O(1) queries processed time O(N ), Nsize network. propose algorithm which, preprocessing phase,allows us answer queries time O(log N ) expense O(log N ) time per evidenceabsorption. usefulness sub-linear processing time manifests applicationsrequiring (near) real-time response large probabilistic databases. brie discusspotential application dynamic probabilistic reasoning computational biology.1. IntroductionProbabilistic (Bayesian) networks increasingly popular modeling techniqueused successfully numerous applications intelligent systems real-time planning navigation, model-based diagnosis, information retrieval, classification, Bayesianforecasting, natural language processing, computer vision, medical informatics computational biology. Probabilistic networks allow user describe environment using\probabilistic database" consists large number random variables, corresponding important parameter environment. random variables couldfact hidden may correspond unknown parameters (causes) uenceobservable variables. Probabilistic networks quite general store informationprobability failure particular component computer system, prob c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiDelcher, Grove, Kasif & Pearlability page computer cache requested near future, probabilitydocument relevant particular query, probability amino-acidsubsequence protein chain folding alpha-helix conformation.applications mind include networks dynamically maintainedkeep track probabilistic model changing system. instance, consider taskautomated detection power-plant failures. might repeat cycle consistsfollowing sequence operations: First perform sensing operations. operationscause updates performed specific variables probabilistic database. Basedevidence estimate (query) probability failure certain sites. precisely,query probability distribution random variables measure probabilityfailure sites based evidence. Since plant requires constant monitoring,must repeat cycle sense/evaluate frequent basis.conventional (non-probabilistic) database tracking plant's state wouldappropriate here, possible directly observe whether failureoccur. hand, probabilistic \database" based Bayesian networkuseful operations|update query|can performed quickly.real-time near real-time often necessary, question extremelyfast reasoning probabilistic networks important.Traditional (non-probabilistic) databases support ecient query update proceduresoften operate time sublinear size database (e.g., using binary search). goal paper take step toward systems performdynamic probabilistic reasoning (such probability event given setobservations) time sublinear size probabilistic network. Typically,sublinear performance complex networks attained using parallelism. paperrelies preprocessing.Specifically, describe new algorithms performing queries updates beliefnetworks form trees (causal trees, polytrees join trees). define two naturaldatabase operations probabilistic networks.1.Update-Node: Perform sensory input, modify evidence leaf node (singlevariable) network absorb evidence network.2.Query-Node: Obtain marginal probability distribution valuesarbitrary node (single variable) network.standard algorithms introduced Pearl (1988) perform Query-Node operation O(1) time although evidence absorption, i.e., Update-Node operation, takesO(N ) time N size network. Alternatively, one assumeUpdate-Node operation takes O(1) time (by simply recording change) QueryNode operation takes O(N ) time (evaluating entire network).paper describe approach perform queries updates O(log N )time. significant systems since improve ability systemrespond change encountered O(N ) time O(log N ). approachbased preprocessing network using form node absorption carefully structuredway create hierarchy abstractions network. Previous uses node absorptiontechniques reported Peot Shachter (1991).38fiQueries & Updates Probabilistic Networksnote measuring complexity terms size network, N ,overlook important factors. Suppose variable network domainsize k less. many purposes, k considered constant. Nevertheless,algorithms consider slowdown power k, becomesignificant practice unless N large. Thus careful state slowdownexists.Section 2 considers case causal trees, i.e., singly connected networksnode one parent. standard algorithm (see Pearl, 1988) must use O(k2 N )time either updates retrieval, although one operations doneO(1) time. discuss brie Section 2.1, also straightforward variantalgorithm takes O(k2D) time queries updates, heighttree.present algorithm takes O(k3 log N ) time updates O(k2 log N )time queries causal tree. course represent tremendous speedup,especially large networks. algorithm begins polynomial-time preprocessingstep (linear size network), constructing another data structure (whichprobabilistic tree) supports fast queries updates. techniques usemotivated earlier algorithms dynamic arithmetic trees, involve \caching" sucient intermediate computations update phase querying also relativelyeasy. note, however, substantial interesting differencesalgorithm probabilistic networks arithmetic trees. particular,apparent later, computation probabilistic trees requires bottom-up top-downprocessing, whereas arithmetic trees need former. Perhaps even interesting relevant probabilistic operations different algebraic structurearithmetic operations (for instance, lack distributivity).Bayesian trees many applications literature including classification.instance, one popular methods classification Bayes classifiermakes independence assumption features used perform classification(Duda & Hart, 1973; Rachlin, Kasif, Salzberg, & Aha, 1994). Probabilistic treesused computer vision (Hel-Or & Werman, 1992; Chelberg, 1990), signal processing(Wilsky, 1993), game playing (Delcher & Kasif, 1992), statistical mechanics (Berger& Ye, 1990). Nevertheless, causal trees fairly limited modeling purposes. Howeversimilar structures, called join trees, arise course one standard algorithmscomputing arbitrary Bayesian networks (see Lauritzen Spiegelhalter, 1988). Thusalgorithm join trees potential relevance many networks trees.join trees special structure, allow optimization basiccausal-tree algorithm. elaborate Section 5.Section 6 consider case arbitrary polytrees. give O(log N ) algorithm updates queries, involves transforming polytree join tree,using results Sections 2 5. join tree polytree particularlysimple form, giving algorithm updates take O(kp+3 log N ) time queriesO(kp+2 log N ), p maximum number parents node. Althoughconstant appears large, must noted original polytree takes O(kp+1 N ) spacemerely represent, conditional probability tables given explicit matrices.39fiDelcher, Grove, Kasif & PearlU,@MV jU ,,,,,V@@ X jU@@R@X,@jX ,,,@ Z jX@R@ZFigure 1: segment causal tree.Finally, discuss specific modelling application computational biology probabilistic models used describe, analyze predict functional behavior biological sequences protein chains DNA sequences (see Delcher, Kasif, Goldberg,Hsu, 1993 references). Much information computational biology databasesnoisy. However, number successful attempts build probabilistic modelsmade. case, use probabilistic tree depth 300 consists 600 nodesmatrices conditional probabilities 2 2. tree used model dependenceprotein's secondary structure chemical structure. detailed descriptionproblem experimental results given Delcher et al. (1993). problemobtain effective speed-up factor 10 perform update comparedstandard algorithm. Clearly, getting order magnitude improvement responsetime probabilistic real-time system could tremendous importance future usesystems.2. Causal Treesprobabilistic causal tree directed tree node represents discrete randomvariable X , directed edge annotated matrix conditional probabilitiesjX (associated edge X ! ). is, x possible value X; Y;(x; )th component jX Pr(Y = jX = x). tree represents jointprobability distribution product space variables; detailed definitionsdiscussion see Pearl (1988). Brie y, idea consider product, nodes,conditional probability node given parents. example, Figure 1implied distribution is:Pr(U = u; V = v; X = x; = y; Z = z ) =Pr(U = u) Pr(V = v jU = u) Pr(X = xjU = u) Pr(Y = jX = x) Pr(Z = z jX = x):Given particular values u; v; x; y; z; conditional probabilities readappropriate matrices . One advantage product representation40fiQueries & Updates Probabilistic Networksconcise. example, need four matrices unconditional probability U ,size square largest variable's domain size. contrast,general distribution N variables requires exponential (in N ) representation.course, every distribution represented causal tree. turnsproduct decomposition implied tree corresponds particular patternconditional independencies often hold (if perhaps approximately) realapplications. Intuitively speaking, Figure 1 implied independenciesconditional probability U given V , X , Z depends values VX ; probability given U , V , X , Z depends X . Independenciessort arise many reasons, instance causal modeling interactionsvariables. refer reader Pearl (1988) details related modelingindependence assumptions using graphs.following, make several assumptions significantly simplify presentation, sacrifice generality. First, assume variable rangessame, constant, number values k.1 follows marginal probability distributionvariable viewed k-dimensional vector, conditional probabilitymatrix jX square k k matrix. common case binary randomvariables (k = 2); distribution values (TRUE, FALSE) (p; 1 , p)probability p.next assumption tree binary, complete, node 02 children. tree converted form, doubling numbernodes. instance, suppose node p children c1 ; c2; c3 original tree.create another \copy" p, p0, rearrange tree two children pc1 p0, two children p0 c2 c3. constrain p0 alwaysvalue p simply choosing identity matrix conditional probability tablep p0 . distribution represented new tree effectivelyoriginal. Similarly, always add \dummy" leaf nodes necessary ensurenode two children. explained introduction, interested processescertain variables' values observed, upon wish condition. finalassumption observed evidence nodes leaves tree. Again,possible \copy" nodes add dummy nodes, restrictive.product distribution alluded corresponds distribution variablesprior observations. practice, interested conditional distribution,simply result conditioning observed evidence (which, earlierassumption, corresponds seeing values leaf nodes). Thus, non-leaf nodeX interested conditional marginal probability X , i.e., k-dimensionalvector:Bel (X ) = Pr(X j evidence values):main algorithmic problem compute Bel (X ) (non-evidence) node Xtree given current evidence. well known probability vector Bel (X )computed linear time (in size tree) popular algorithm based1. assumption nonrestrictive add \dummy" values variable's range,given conditional probability 0. Nevertheless, may computational advantageallowing different variable domain sizes. changes required permit dicult, sincecomplicate presentation somewhat omit them.41fiDelcher, Grove, Kasif & Pearlfollowing equation:Bel (X ) = Pr(X j evidence) = ff (X ) (X )ff normalizing constant, (X ) probability evidence subtreenode X given X , (X ) probability X given evidence resttree. interpret equation, note X = (x1; x2; : : :; xk ) (Y = y1 ; y2 ; : : :; yk )two vectors define operation component-wise product (pairwisedyadic product vectors):X = (x1y1; x2y2; : : :; xkyk ):usefulness (X ) (X ) derives fact computed recursively, follows:1. X root node, (X ) prior probability X .2. X leaf node, (X ) vector 1 ith position (where ith valueobserved) 0 elsewhere. value X observed, (X )vector consisting 1's.23. Otherwise, if, shown Figure 1, children node X Z , siblingV parent U , have:(X ) = (MY jX (Y )) (MZjX (Z ))(X ) = MX jU (U ) (MV jU (V ))presentation technique follows Pearl (1988). However, usesomewhat different notation don't describe messages sent parents successors, rather discuss direct relations among vectors terms simplealgebraic equations. take advantage algebraic properties equationsdevelopment.easy see equations evaluated time proportionalsize network. formal proof given Pearl (1988).Theorem 1: belief distribution every variable (that is, marginal probabilitydistribution variable, given evidence) causal tree evaluatedO(k2N ) time N size tree. (The factor k2 due multiplicationmatrix vector must performed node.)theorem shows possible perform evidence absorption O(N ) time,queries constant time (i.e., retrieving previously computed values lookuptable). next sections show perform queries updatesworst-case O(log N ) time. Intuitively, recompute marginal distributionsupdate, rather make small number changes, sucient, however,compute value variable logarithmic delay.2. set 1 components corresponding possible values|this especially usefulobserved variable part joint-tree clique (Section 5). general, (X ) thoughtlikelihood vector X given observations X .42fiQueries & Updates Probabilistic Networks2.1 Simple Preprocessing Approachobtain intuition new approach begin simple observation.Consider causal tree depth D. node X tree initially compute(X ) vector. vectors left uncomputed. Given update node , calculaterevised (X ) vectors nodes X ancestors tree. clearlydone time proportional depth tree, i.e., O(D). rest informationtree remains unchanged. consider Query-Node operation node Vtree. obviously already accurate (V ) vector every node treeincluding V . However, order compute (V ) vector need compute(Y ) vectors nodes V tree multiply appropriatevectors kept current. means compute accurate (V ) vectorneed perform O(D) work well. Thus, approach don't perform completeupdate every (X ) (X ) vector tree.Lemma 2: Update-Node Query-Node operations causal tree performed O(k2 D) time depth tree.implies tree balanced, operations done O(log N )time. However, important applications trees balanced (e.g., modelstemporal sequences, Delcher et al., 1993). obvious question therefore is: Given causaltree produce equivalent balanced tree 0? answer questionappears dicult, possible use sophisticated approach produce datastructure (which causal tree) process queries updates O(log N ) time.approach described subsequent sections.2.2 Dynamic Data Structure Causal Treesdata structure allow ecient incremental processing probabilistic tree =T0 sequence trees, T0; T1; T2; : : :; Ti; : : :; Tlog N . Ti+1 contractedversion Ti, whose nodes subset Ti . particular, Ti+1 containhalf many leaves predecessor.defer details contraction process next section. However, one keyidea maintain consistency, sense Bel (X ); (X ); (X ) givenvalues trees X appears. choose conditional probabilitymatrices contracted trees (i.e., trees T0 ) ensure this.Recall equations form(X ) = (MY jX (Y )) (MZjX (Z ))(X ) = MX jU (U ) (MV jU (V ))Z children X , X right child U , V X 's sibling (Figure 1).However, equations convenient form following notationalconventions helpful. First, let Ai (x) (resp., Bi (x)) denote conditionalprobability matrix X X 's left (resp., right) child tree Ti. Noteidentity children differ tree tree, X 's original childrenmight removed contraction process. One advantage new notation43fiDelcher, Grove, Kasif & PearlujTi, @,@,@vjexjRake, @,@p p zjp p(e; x))ujTi+1, @,@,@vjp p zjp pFigure 2: effect operation Rake (e; x). e must leaf, z may mayleaf.explicit dependence identity children suppressed. Next, suppose X 'sparent Ti u. let Ci (x) denote either Ai (u) Bi (u), Di(x) denote eitherBi (u) Ai(u) , depending whether X right left child, respectively, U .necessary keep careful track correspondences, simply noteequations become:3(x) = Ai(x) (y) Bi (x) (z)(x) = Di(x) ((u) Ci(x) (v))next section describe preprocessing step creates dynamic datastructure.2.3RakeOperationbasic operation used contract tree Rake removes leafparent tree. effect operation tree shown Figure 2.define algebraic effect operation equations associated tree.Recall want define conditional probability matrices raked treedistribution remaining variables unchanged. achieve substitutingequations (x) (x) equations (u), (z ), (v ). following,important note (u), (z ) (v ) unaffected rake operation.following, let Diagff denote diagonal matrix whose diagonal entriescomponents vector ff. derive algebraic effect rake operation follows:(u) = Ai (u) (v) Bi (u) (x)= Ai (u) (v ) Bi (u) (Ai (x) (e) Bi (x) (z ))= Ai (u) (v ) Bi (u) DiagA (x)(e) Bi (x) (z )= Ai (u) (v ) Bi (u) DiagA (x)(e) Bi (x) (z )= Ai+1 (u) (v ) Bi+1 (u) (z )Ai+1 (u) = Ai (u) Bi+1 (u) = Bi (u) DiagA (x)(e) Bi (x). (Of course, caseleaf raked right child generates analogous equations.) Thus, defining3. Throughout, assume lower precedence matrix multiplication (indicated ).44fiQueries & Updates Probabilistic NetworksAi+1(u) Bi+1 (u) way, ensure values raked tree identicalcorresponding values original tree. yet enough, mustcheck values similarly preserved. two values could possibly change(z ) (v ), check both. former, must(z) = Di(z) ((x) Ci(z) (e))= Di+1 (z ) ( (u) Ci+1(z ) (v )) :substituting (x) algebraic manipulation, see assuredCi+1(z) = Ci(x) Di+1(z) = Di(z) DiagC (z)(e) Di(x). However recall that, definition, Ci+1 (z ) = Ai+1 (u) Ci (x) = Ai (u), Ci+1 (z ) = Ci(x) follows. Furthermore,Di+1(z) = Bi+1(u)= (Bi (u) DiagA (x)(e) Bi (x))= Bi (x) DiagA (x)(e) Bi (u)= Di(z ) DiagC (z)(e) Di (x)required.(v ) necessary verify(v) = Di(v) ((u) Ci(v) (x))= Di+1 (v ) ( (u) Ci+1 (v ) (z )) :substituting (x), shown true Di+1(v ) = Di (v ) = Ai (u) =Ai+1(u) Ci+1(v) = Ci(v) DiagA (x)(e) Bi(x) = Bi+1(u). identities followdefinition, done.Beginning given tree = T0, successive tree constructed performingsequence rakes, rake away half remaining evidence nodes.specifically, let Contract operation apply Rake operation everyleaf causal tree, left-to-right order, excluding leftmost rightmostleaf. Let fTig set causal trees constructed Ti+1 causal tree generatedTi single application Contract. following result proved using easyinductive argument:Theorem 3: Let T0 causal tree size N . number leaves Ti+1 equalhalf leaves Ti (not counting two extreme leaves) starting T0,O(log N ) applications Contract, produce three-node tree: root,leftmost leaf rightmost leaf.observations process:1. complexity Contract linear size tree. Additionally, log N applications Contract reduce set tree equations single equation involvingroot O(N ) total time.2. total space store sets equations associated fTi g0ilog Ntwice space required store equations T0.45fiDelcher, Grove, Kasif & Pearl3. equation Ti+1 also store equations describe relationshipconditional probability matrices Ti+1 matrices Ti . Noticethat, even though Ti+1 produced Ti series rake operations, matrixTi+1 depends directly matrices present Ti. would caseattempted simultaneously rake adjacent children.regard equations part Ti+1. So, formally speaking fTig causal treesaugmented auxiliary equations. contracted trees describesprobability distribution subset first set variables consistentoriginal distribution.note ideas behind Rake operation originally developed MillerReif (1985) context parallel computation bottom-up arithmetic expressiontrees (Kosaraju & Delcher, 1988; Karp & Ramachandran, 1990). contrast, usingcontext incremental update query operations sequential computing.similar data structure independently proposed Frederickson (1993)context dynamic arithmetic expression trees, different approach incrementalcomputing arithmetic trees developed Cohen Tamassia (1991).important interesting differences arithmetic expression-tree caseown. arithmetic expressions computation done bottom-up. However, probabilistic networks -messages must passed top-down. Furthermore, arithmetic expressionstwo algebraic operations allowed, typically require distributivity oneoperation other, analogous property hold us. respects approach substantial generalization previous work, remainingconceptually simple practical.3. Example: Chainobtain intuition algorithms, sketch generate utilizeTi; 0 log N equations perform -value queries updates O(log N )time N = 2L + 1 node chain length L. Consider chain length 4 Figure 3,trees generated repeated application Contract chain.equations correspond contracted trees figure follows (ignoring trivial equations). Recall Ai (xj ) matrix associated left edgerandom variable xj Ti.(x1)(x2)(x3)(x4)====A0(x1) (e1) B0(x1) (x2)A0(x2) (e2) B0(x2) (x3)A0(x3) (e3) B0(x3) (x4)A0(x4) (e4) B0(x4) (e5)9>>>>>>=>>>B0 (x1) DiagA0 (x2)(e2) B0(x2) >>>B0 (x3) DiagA0 (x4)(e4) B0(x4) ;(x1) = A1(x1) (e1) B1(x1) (x3)(x3) = A1(x3) (e3) B1(x3) (e5)B1(x1) =B1(x3) =9>>>=>>>;46T0T1fiQueries & Updates Probabilistic NetworksT0 :xm1em1?T1 :xm1em1?T2 :xm1xm - xm3 - xm4 - e5m- 2em2em3?em4??xm - em5- 3em3?em- 5em1?Figure 3: simple chain example.(x1) = A2(x1) (e1) B2(x1) (e5)9>>=>>;T2B2(x1) = B1 (x1) DiagA (x )(e ) B1(x3)listed matrices because, example, constant.consider query operation x2 . Rather performing standard computationfind level x2 \raked". Since occurred level 0, obtainequation(x2) = A0(x2) (e2) B0(x2) (x3)Thus must compute (x3), find x3 \raked". happenedlevel 1. However, level equation associated x3 is:(x3) = A1(x3) (e3) B1(x3) (e5)means need follow chain. general chain N nodesanswer query node chain evaluating log N equations instead Nequations.consider update e4 . Since e4 raked immediately, first modifyequationB1(x3) = B0(x3) DiagA (x )(e ) B0(x4)first level e4 occurs right-hand side. Since B1 (x3) affectedchange e4 , subsequently modify equationB2(x1) = B1(x1) DiagA (x )(e ) B1(x3)14733044133fiDelcher, Grove, Kasif & Pearlsecond level. general, clearly need update log N equations; i.e., oneper level. generalize example describe general algorithms queriesupdates causal trees.3.1 Performing Queries Updates Ecientlysection shall show utilize contracted trees Ti; 0 log Nperform queries updates O(log N ) time general causal trees. shall showlogarithmic amount work necessary sucient compute enough informationdata structure update query value.3.2 Queriescompute (x) node x following. first locate ind (x),defined highest level x appears Ti . equation (x)form:(x) = Ai(x) (y) Bi(x) (z)z left right children, respectively, x Ti.Since x appear Ti+1 , raked level equations, impliesone child (we assume z ) leaf. therefore need compute (y ),done recursively. instead raked leaf, would compute (z ) recursively.either case O(1) operations done addition one recursive call,value higher level equations. Since O(log N ) levels, operationsmatrix vector multiplications, procedure takes O(k2 log N ) time. function-Query (x) given Figure 4.3.3 Updatesdescribe update operations modify enough information datastructure allow us query vectors vectors eciently. importantlyreader note update operation try maintain correctvalues. sucient ensure that, x, matrices Ai(x) Bi (x) (andthus also Ci (x) Di(x)) always date.update value evidence node, simply changing valueleaf e. level equations, value (e) appear twice:-equation e's parent -equation e's sibling Ti. edisappears, say level i, value incorporated one constant matrices Ai+1 (u)Bi+1 (u) u grandparent e Ti . constant matrix turn affectsexactly one constant matrix next higher level, on. Since effectlevel computed O(k3 ) time (due matrix multiplication) O(log N )levels equations, update accomplished O(k3 log N ) time. constant k3actually pessimistic, faster matrix multiplication algorithms exist.update procedure given Figure 5. Update initially called Update((E ) =e; i) E leaf, level raked, e new evidence.operation start sequence O(log N ) calls function -Update (X = Term; i)change propagate log N equations.48fiQueries & Updates Probabilistic NetworksFUNCTION -Query (x)look equation associated (x) Tind (x).Case 1: x leaf. equation form: (x) = e e known.case return e.Case 2: equation associated (x) form(x) = Ai(x) (y) Bi (x) (z)z leaf therefore (z ) known. case returnAi(X ) -Query (y) Bi (X ) (z)case leaf analogous.Figure 4: Function compute value node.3.4 Queriesrelatively easy use similar recursive procedure perform (x) queries. Unfortunately, approach yields O(log2N )-time algorithm simply use recursioncalculate terms calculate terms using earlier procedure.O(log N ) recursive calls calculate values, defined equationalso involves term taking O(log N ) time compute.achieve O(log N ) time, shall instead implement (x) queries defining procedure Calc (x; i) returns triple vectors hP; L; Ri P = (x), L = (y )R = (z ) z left right children, respectively, x Ti.compute (x) node x following. Let = ind (x). equation(x) Ti form:(x) = Di(x) ((u) Ci(x) (v))u parent x Ti v sibling. call procedure Calc (u; + 1)return triple h (u); (v); (x)i, immediately compute (x)using equation.Procedure Calc (x; i) implemented following fashion.Case 1: Ti 3-node tree x root, children x leaves, hencevalues known, (x) given sequence prior probabilities x.Case 2: x appear Ti+1 , one x's children leaf, say e rakedlevel i. Let z child. call Calc (u; + 1), u parentx Ti, receive back h(u); (z); (v)i h(u); (v); (z)i according whether x49fiDelcher, Grove, Kasif & PearlFUNCTION -Update (Term = Value; i)1. Find (at one) equation Ti , defining Ai Bi , Termappears right-hand side; let Term0 matrix defined equation(i.e., left-hand side).2. Update Term0; let Value new value.3. Call -Update (Term0 = Value; + 1) recursively.Figure 5: update procedure.left right child u Ti (and v u's child). compute (x)(u) (v ), (e) (z ), return necessary triple.Specifically,(x) =(Di(x) ((u) Ai+1 (u) (v))Di(x) ((u) Bi+1 (u) (v))choice depends whether x right left child, respectively, u Ti.Case 3: x appear Ti+1, call Calc (x; + 1). returns correctvalue (x). child z x Ti remains child x Ti+1 , also returnscorrect value (z ). z child x occur Ti+1 , mustcase z raked level one z 's children, say e, leaf letchild q . situation Calc (x; + 1) returned value (q )compute(z) = Ai(z) (e) Bi (z) (q)return value.three cases, constant amount work done addition single recursivecall uses equations higher level. Since O(log N ) levels equations,requiring matrix vector multiplication, total work done O(k2 log N ).4. Extended Examplesection illustrate application algorithms specific example. Considersequence contracted trees shown Figure 6. Corresponding trees50fiQueries & Updates Probabilistic Networksxl1T0 :xl####2xl4xl6e1xl8e2e4e8xl2xl5xl7xl3ZZcccc, @@@,,xl1T1 :ZZZe6xl4e9xl6e7e5e1e3T2:xl1e1e3T3: xl1xl4e5e9e5Figure 6: Example tree contraction.51e1e9e7e9fiDelcher, Grove, Kasif & Pearlequations following:T0 :(x1) = A0(x1 ) (x2 ) B0 (x1 ) (x3 )...(x2) = D0 (x2) ((x1) C0(x2) (x3 ))...T1 :(x1) = A1(x1 ) (x2 ) B1 (x1 ) (e9 )...(x2) = D1 (x2) ((x1) C1(x2) (e9 ))...T2 :(x1) = A2(x1 ) (x4 ) B2 (x1 ) (e9 )...(x4) = D2 (x4) ((x1) C2(x4) (e9 ))...T3 :(x1) = A3(x1 ) (e1 ) B3 (x1) (e9 )consider, instance, effect update e2 . Since raked immediately,new value (e2) incorporated in:B1 (x6 ) = B0 (x6 ) DiagA0 (x8 ) (e2) B0 (x8 )subsequent Rake operations know A2(x4 ) depends B1 (x6), A3 (x1)depends A2 (x4), must also update values follows:A2 (x4 ) = A1 (x4) DiagB1 (x6 ) (e3 ) A1 (x6)A3 (x1 ) = A2 (x1) DiagB2 (x4 ) (e5 ) A2 (x4)Finally, consider query x7 . Since x7 raked together e5 T0 , followsteps outlined generate following calls: Calc (x7; 0), Calc (x4; 1),Calc (x4; 2), Calc (x1; 3). provides us (x7). case, (x7)particularly easy compute since x7 's children leaf nodes. simplycompute (x7) (x7) normalize, giving us conditional marginal distributionBel (x7) required.5. Join TreesPerhaps best-known technique computing arbitrary (i.e., singly-connected)Bayesian networks uses idea join trees (junction trees) (Lauritzen & Spiegelhalter,1988). many ways join tree thought causal tree, albeit one somewhatspecial structure. Thus algorithm previous section applied. However,structure join tree permits optimization, describe section.becomes especially relevant next section, use join-tree techniqueshow O(log N ) updates queries done arbitrary polytrees. reviewjoin-trees utility extremely brief quite incomplete; clear expositionssee, instance, Spiegelhalter et al. (1993) Pearl (1988).Given Bayesian network, first step towards constructing join-tree moralizenetwork: insert edges every pair parents common node, treat52fiQueries & Updates Probabilistic Networksedges graph undirected (Spiegelhalter et al., 1993). resulting undirectedgraph called moral graph. interested undirected graphs chordal :every cycle length 4 contain chord (i.e., edge two nodesnon-adjacent cycle). moral graph chordal, necessary addedges make so; various techniques triangulation stage known (for instance,see Spiegelhalter et al., 1993).p probability distribution represented Bayesian network G = (V; E ),= (V; F ) result moralizing triangulating G, then:1. jV j cliques,4 say C1; : : :; CjV j.2. cliques ordered > 1 j (i) <Ci \ Cj(i) = Ci \ (C1 [ C2 [ : : : [ Ci,1:)tree formed treating cliques nodes, connecting node Ci\parent" Cj (i), called join tree.3. p =p(CijCj(i))4. p(CijCj (i)) = p(CijCj (i) \ Ci )2 3, see direct edges away \parent" cliques,resulting directed tree fact Bayesian causal tree represent originaldistribution p. true matter form original graph. course,price cliques may large, domain size (the number possible valuesclique node) exponential size. technique guaranteedecient.use Rake technique Section 2 directed join tree withoutmodification. However, property 4 shows conditional probability matricesjoin tree special structure. use gain eciency.following, let k domain size variables G usual. Let n maximumsize cliques join tree; without loss generality assume cliquessize (because add \dummy" variables). Thus domain sizeclique K = kn . Finally, let c maximum intersection size clique parent(i.e., jCj (i) \ Cij) L = kc .standard algorithm, would represent p(CijCj (i)) K K matrix, MC jC .However, p(Ci jCj (i) \ Ci) represented smaller L K matrix, MC jC \C .property 4 above, MC jC identical MC jC \C , except many rows repeated.Thus K L matrix Jj (i)j (i)j (i)MC jC = J MC jCj (i)j (i)j (i)\Ci :(J actually simple matrix whose entries 0 1, exactly one 1 per row; howeveruse fact.)4. clique maximal completely-connected subgraph.53fiDelcher, Grove, Kasif & Pearlclaim that, case join trees, following true. First, matricesAi Bi used Rake algorithm stored factored form, producttwo matrices dimension K L L K respectively. So, instance, factor AiAli Ari . never need explicitly compute, store, full matrices.seen, claim true = 0 matrices factor way. proof> 1 uses inductive argument, illustrate below. second claim that,matrices stored factored form, matrix multiplications usedRake algorithm one following types: 1) L K matrix times K L matrix,2) L K matrix times K K diagonal matrix, 3) L L matrix times L Kmatrix, 4) L K matrix times vector.prove claims consider, instance, equation defining Bi+1 terms lowerlevel matrices. Section 2, Bi+1 (u) = Bi (u) DiagA (x)(e) Bi (x): But, assumption,is:(Bil (u) Bir (u)) Diag(A (x)A (x))(e) (Bil (x) Bil (x));which, using associativity, clearly equivalenthBil (u) ((Bir (u) DiagA (x)(A (x)(e))) Bil(x)) Bil (x) :However, every multiplication expression one forms stated earlier. IdentifyingBil+1 (u) Bil(u) Bir+1 (u) bracketed part expression proves case,course case rake left child (so Ai+1 (u) updated) analogous.Thus, even using straightforward technique matrix multiplication, costupdating Bi+1 O(KL2) = O(kn+2c ). contrasts O(K 3) factormatrices, may represent worthwhile speedup c small. Note overall timeupdate using scheme O(kn+2c log N ). Queries, involve matrixvector multiplication, require O(kn+c log N ) time.many join trees difference N log N unimportant,clique domain size K often enormous dominates complexity. Indeed, K Lmay large cannot represent required matrices explicitly. course,cases technique little offer. cases benefitsworthwhile. important general class so, immediatereason presenting technique join trees, case polytrees.lrlr6. Polytreespolytree singly connected Bayesian network; drop assumption Section 2node one parent. Polytrees offer much exibility causaltrees, yet well-known process update query O(N ) time,causal trees. reason polytrees extremely popular class networks.suspect possible present O(log N ) algorithm updates queriespolytrees, direct extension ideas Section 2. Instead propose differenttechnique, involves converting polytree join tree using ideaspreceding section. basis simple observation join treepolytree already chordal. Thus (as show detail below) little lost consideringjoin tree instead original polytree. specific property polytreesrequire following. omit proof well-known proposition.54fiQueries & Updates Probabilistic NetworksProposition 4: moral graph polytree P = (V; E ) chordal,set maximal cliques ffv g [ parents (v ) : v 2 V g.Let p maximum number parents node. proposition, everymaximal clique join tree p +1 variables, domain size nodejoin tree K = kp+1 . may large, recall conditional probabilitymatrix original polytree, variable p parents, K entries anyway sincemust give conditional distribution every combination node's parents. Thus Kreally measure size polytree itself.follows proposition perform query updatepolytrees time O(K 3 log N ), simply using algorithm Section 2 directedjoin tree. But, noted Section 5, better. Recall savings dependc, maximum size intersection node parent join tree.However, join tree formed polytree, two cliques sharesingle node. follows immediately Proposition 4, two cliquesone node common must either two nodes share one parent,else node one parents share yet another parent. Neitherconsistent network polytree. Thus complexity bounds Section 5,put c = 1. follows process updates O(Kk2c log N ) = O(kp+3 log N )time queries O(kp+2 log N ).7. Application: Towards Automated Site-Specific Muta-Genesisexperiment commonly performed biology laboratories procedureparticular site protein changed (i.e., single amino-acid mutated)tested see whether protein settles different conformation. many cases,overwhelming probability protein change secondary structure outsidemutated region. process often called muta-genesis. Delcher et al. (1993) developedprobabilistic model protein structure basically long chain. lengthchain varies 300{500 nodes. nodes network either protein-structurenodes (PS-nodes) evidence nodes (E-nodes). PS-node network discreterandom variable Xi assumes values corresponding descriptors secondary sequencestructure: helix, sheet coil. PS-node model associates evidence nodecorresponds occurrence particular subsequence amino acids particularlocation protein.model, protein-structure nodes finite strings alphabet fh; e ; c g.example string hhhhhh string six residues ff-helical conformation,eecc string two residues fi -sheet conformation followed two residues foldedcoil. Evidence nodes nodes contain information particular regionprotein. Thus, main idea represent physical statistical rules formprobabilistic network.first set experiments converged following model that, clearlybiologically naive, seems match prediction accuracy many existing approachesneural networks. network looks like set PS-nodes connected chain.node connect single evidence node. experiments PS-nodes stringslength two three alphabet fh; e ; c g evidence nodes strings55fiDelcher, Grove, Kasif & Pearlcc?GSch?SAhh?Figure 7: Example causal tree model using pairs, showing protein segment GSATcorresponding secondary structure cchh.length set amino acids. following example clarifies representation.Assume string amino acids GSAT. model string network comprisedthree evidence nodes GS, SA, three PS-nodes. network shown Figure 7.correct prediction assign values cc, ch, hh PS-nodes shownfigure.probabilistic model, test robustness proteinwhether small changes protein affect structure certain critical sitesprotein. experiments, probabilistic network performs \simulated evolution"protein, namely simulator repeatedly mutates region chain testswhether designated sites protein coiled helix predictedremain conformation. main goal experiment test stable bonds faraway mutated location affected. previous results (Delcher et al., 1993)support current thesis biology community, namely local distant changesrarely affect structure.algorithms presented previous sections paper perfectly suitedtype application predicted generate factor 10 improvementeciency current brute-force implementation presented Delcher et al. (1993)change propagated throughout network.8. Summarypaper proposed several new algorithms yield substantial improvementperformance probabilistic networks form causal trees. updating proceduresabsorb sucient information tree query procedure computecorrect probability distribution node given current evidence. addition,procedures execute time O(log N ), N size network. algorithmsexpected generate orders-of-magnitude speed-ups causal trees contain longpaths (not necessarily chains) matrices conditional probabilitiesrelatively small. currently experimenting approach singly connectednetworks (polytrees). likely dicult generalize techniques generalnetworks. Since known general problem inference probabilistic networksNP -hard (Cooper, 1990), obviously possible obtain polynomial-time incremental56fiQueries & Updates Probabilistic Networkssolutions type discussed paper general probabilistic networks.natural open question extending approach developed paper dynamicoperations probabilistic networks addition deletion nodes modifyingmatrices conditional probabilities (as result learning).would also interesting investigate practical logarithmic-time parallel algorithms probabilistic networks realistic parallel models computation. Onemain goals massively parallel AI research produce networks perform real-timeinference large knowledge-bases eciently (i.e., time proportional depthnetwork rather size network) exploiting massive parallelism. JerryFeldman pioneered philosophy context neural architectures (see StanfillWaltz, 1986, Shastri, 1993, Feldman Ballard, 1982). achieve type performance neural network framework, typically postulate parallel hardwareassociates processor node network typically ignores communication requirements. careful mapping parallel architectures one indeed achieve ecientparallel execution specific classes inference operations (see Mani Shastri, 1994,Kasif, 1990, Kasif Delcher, 1992). techniques outlined paper presentedalternative architecture supports fast (sub-linear time) response capabilitysequential machines based preprocessing. However, approach obviously limitedapplications number updates queries time constant. One wouldnaturally hope develop parallel computers support real-time probabilistic reasoninggeneral networks.AcknowledgementsSimon Kasif's research Johns Hopkins University sponsored part NationalScience foundation Grants No. IRI-9116843, IRI-9223591 IRI-9220960.ReferencesBerger, T., & Ye, Z. (1990). Entropic aspects random fields trees. IEEE Trans.Information Theory, 36 (5), 1006{1018.Chelberg, D. M. (1990). Uncertainty interpretation range imagery. Proc. Intern.Conference Computer Vision, pp. 654{657.Cohen, R. F., & Tamassia, R. (1991). Dynamic trees applications. Proceedings2nd ACM-SIAM Symposium Discrete Algorithms, pp. 52{61.Cooper, G. (1990). computational complexity probabilistic inference using bayesbelief networks. Artificial Intelligence, 42, 393{405.Delcher, A., & Kasif, S. (1992). Improved decision making game trees: Recoveringpathology. Proceedings 1992 National Conference Artificial Intelligence.Delcher, A. L., Kasif, S., Goldberg, H. R., & Hsu, B. (1993). Probabilistic prediction protein secondary structure using causal networks. Proceedings 1993 InternationalConference Intelligent Systems Computational Biology, pp. 316{321.57fiDelcher, Grove, Kasif & PearlDuda, R., & Hart, P. (1973). Pattern Classification Scene Analysis. Wiley, New York.Feldman, J. A., & Ballard, D. (1982). Connectionist models properties. CognitiveScience, 6, 205{254.Frederickson, G. N. (1993). data structure dynamically maintaining rooted trees.Proc. 4th Annual Symposium Discrete Algorithms, pp. 175{184.Hel-Or, Y., & Werman, M. (1992). Absolute orientation uncertain data: unifiedapproach. Proc. Intern. Conference Computer Vision Pattern Recognition,pp. 77{82.Karp, R. M., & Ramachandran, V. (1990). Parallel algorithms shared-memory machines.Van Leeuwen, J. (Ed.), Handbook Theoretical Computer Science, pp. 869{941.North-Holland.Kasif, S. (1990). parallel complexity discrete relaxation constraint networks.Artificial Intelligence, 45, 275{286.Kasif, S., & Delcher, A. (1994). Analysis local consistency parallel constraint networks.Artificial Intelligence, 69.Kosaraju, S. R., & Delcher, A. L. (1988). Optimal parallel evaluation tree-structuredcomputations raking. Reif, J. H. (Ed.), VLSI Algorithms Architectures:Proceedings 1988 Aegean Workshop Computing, pp. 101{110. Springer Verlag.LNCS 319.Lauritzen, S., & Spiegelhalter, D. (1988). Local computations probabilities graphicalstructures applications expert systems. J. Royal Statistical Soc. Ser. B,50, 157{224.Mani, D., & Shastri, L. (1994). Massively parallel reasoning large knowledgebases. Tech. rep., Intern. Computer Science Institute.Miller, G. L., & Reif, J. (1985). Parallel tree contraction application. Proceedings26th IEEE Symposium Foundations Computer Science, pp. 478{489.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.Peot, M. A., & Shachter, R. D. (1991). Fusion propagation multiple observationsbelief networks. Artificial Intelligence, 48, 299{318.Rachlin, J., Kasif, S., Salzberg, S., & Aha, D. (1994). Towards better understandingmemory-based bayesian classifiers. Proceedings Eleventh InternationalConference Machine Learning, pp. 242{250 New Brunswick, NJ.Shastri, L. (1993). computational model tractable reasoning: Taking inspirationcognition. Proceeding 1993 Intern. Joint Conference Artificial Intelligence.AAAI.58fiQueries & Updates Probabilistic NetworksSpiegelhalter, D., Dawid, A., Lauritzen, S., & Cowell, R. (1993). Bayesian analysis expertsystems. Statistical Science, 8 (3), 219{283.Stanfill, C., & Waltz, D. (1986). Toward memory-based reasoning. CommunicationsACM, 29 (12), 1213{1228.Wilsky, A. (1993). Multiscale representation markov random fields. IEEE Trans. SignalProcessing, 41, 3377{3395.59fiJournal Artificial Intelligence Research 4 (1996) 91-128Submitted 6/95; published 3/96Quantum Computing Phase TransitionsCombinatorial SearchTad HoggXerox Palo Alto Research Center3333 Coyote Hill Road Palo Alto, CA 94304 USAhogg@parc.xerox.comAbstractintroduce algorithm combinatorial search quantum computers capable significantly concentrating amplitude solutions NP search problems,average. done exploiting aspects problem structure usedclassical backtrack methods avoid unproductive search choices. quantum algorithmmuch likely find solutions simple direct use quantum parallelism. Furthermore, empirical evaluation small problems shows quantum algorithm displaysphase transition behavior, location, seen many previouslystudied classical search methods. Specifically, dicult problem instances concentratednear abrupt change underconstrained overconstrained problems.1. IntroductionComputation ultimately physical process (Landauer, 1991). is, practicerange physically realizable devices determines computable resources,computer time, required solve given problem. Computing machines exploitvariety physical processes structures provide distinct trade-offs resourcerequirements. example development parallel computers trade-offoverall computation time number processors employed. Effective usetrade-off require algorithms would inecient implemented serially.Another example given hypothetical quantum computers (DiVincenzo, 1995).offer potential exploiting quantum parallelism trade computation timeuse coherent interference among many different computational paths. However,restrictions physically realizable operations make trade-off dicult exploitsearch problems, resulting algorithms essentially equivalent inecient methodgenerate-and-test. Fortunately, recent work factoring (Shor, 1994) shows betteralgorithms possible. continue line work introducing new quantum algorithm particularly dicult combinatorial search problems.algorithm represents substantial improvement quantum computers, particularlyinecient classical search method, memory time requirements.evaluating algorithms, computational complexity theory usually focusesscaling behavior worst case. particular theoretical concern whether searchcost grows exponentially polynomially. However, many practical situations, typicalaverage behavior interest. especially true many instancessearch problems much easier solve suggested worst case analyses.fact, recent studies revealed important regularity class search problems.Specifically, wide variety search methods, hard instances rarec 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiHoggalso concentrated near abrupt transitions problem behavior analogous physical phasetransitions (Hogg, Huberman, & Williams, 1996). exhibit concentration hardinstances search algorithm must exploit problem constraints prune unproductivesearch choices. Unfortunately, easy within range allowable quantumcomputational operations. thus interest see results generalize quantumsearch methods well.paper, new algorithm evaluated empirically determine average behavior. algorithm also shown exhibit phase transition, indicating indeedmanaging to, effect, prune unproductive search. leaves future work analysisworst case performance.paper organized follows. First discuss combinatorial search problemsphase transitions hard problem instances concentrated. Second,brief summary quantum computing, new quantum search algorithm motivateddescribed. fact, number natural variants general algorithm.Two evaluated empirically exhibit generality phase transitionperformance. Finally, important caveats implementation quantumcomputers open issues presented.2. Combinatorial SearchCombinatorial search among hardest common computational problems: solutiontime grow exponentially size problem (Garey & Johnson, 1979). Examples arise scheduling, planning, circuit layout machine vision, name areas.Many examples viewed constraint satisfaction problems (CSPs) (Mackworth, 1992). given set n variables assigned b possiblevalues. problem find assignment variable together satisfyspecified constraints. instance, consider small scheduling problem selecting onetwo periods teach two classes taught person.regard class variable time slot value, i.e., n = b = 2.constraints two classes assigned time.Fundamentally, combinatorial search problem consists finding combinationsdiscrete set items satisfy specified requirements. number possible combinations consider grows rapidly (e.g., exponentially factorially) numberitems, leading potentially lengthy solution times severely limiting feasible sizeproblems. example, number possible assignments constraint problembn , grows exponentially problem size (given number variablesn).exponentially large number possibilities appears time requiredsolve problems must grow exponentially, worst case. However manyproblems easy verify solution fact correct. problems form wellstudied class NP problems: informally say hard solve easy check.One well-studied instance graph coloring, variables represent nodes graph,values colors nodes constraints pair nodes linkededge graph must different colors. Another example propositional satisfiability(SAT), variables take logical values true false, assignment must92fiQuantum Computing Phase Transitions Combinatorial Searchsatisfy specified propositional formula involving variables. examplesinstances particularly dicult NP problems known class NP-complete searchproblems (Garey & Johnson, 1979).2.1 Phase TransitionsMuch theoretical work NP search problems examines worst case behavior.Although search problems hard, worst case, great dealindividual variation problems among different search methods. numberrecent studies NP search problems focused regularities typical behavior (Cheeseman, Kanefsky, & Taylor, 1991; Mitchell, Selman, & Levesque, 1992; Williams &Hogg, 1994; Hogg et al., 1996; Hogg, 1994). work identified number commonbehaviors. Specifically, large problems, parameters characterizing structuredetermine relative diculty wide variety common search methods, average.Moreover, changes parameters give rise transitions, becoming abruptlarger problems, analogous phase transitions physical systems. case,transition underconstrained overconstrained problems, hardest casesconcentrated transition region. One powerful result work concentration hard cases occurs parameter values wide range searchmethods. is, behavior property problems rather detailssearch algorithm.understood viewing search making series choices solutionfound. overall search usually relatively easy (i.e., require steps) eithermany choices leading solutions else choices lead solutionsrecognized quickly such, unproductive search avoided. Whethercondition holds turn determined tightly constrained problem is.constraints almost choices good ones, leading quickly solution.many constraints, hand, good choices bad onesrecognized quickly violating constraints much time wastedconsidering them. two cases hard problems: enough constraintsgood choices rare enough bad choices usually recognizedlot additional search.detailed analysis suggests series transitions (Hogg & Williams, 1994).constraints, average search cost scales polynomially. constraintsadded, transition exponential scaling. rate growth exponentialincreases transition region described reached. Beyond point,concentration hard problems, growth rate decreases. Eventually, highlyconstrained problems, search cost grows polynomially size.2.2 Combinatorial Search Spacegeneral view combinatorial search problem consists N items1requirement find solution, i.e., set L<N items satisfies specified conditionsconstraints. conditions turn described collection nogoods, i.e., sets1. CSPs, items possible variable-value pairs.93fiHogg{1,2,3,4}{1,2}{1,2,3}{1,3,4}{2,3,4}{1,2,4}{2,3}{1,3}{3,4}{2,4}{1}{2}{3}{4}{1,4}{}Figure 1: Structure set lattice problem four items. subsets f1; 2; 3; 4ggrouped levels size lines drawn set immediatesupersets subsets. bottom lattice, level 0, represents single setsize zero, four points level 1 represent four singleton subsets, etc.items whose combination inconsistent given conditions. contextdefine good set items consistent constraints problem.also say set complete L items, smaller sets partial incomplete.Thus solution complete good set. addition, partial solution incomplete goodset.key property makes set representation conceptually useful setnogood, supersets. sets, grouped size set linkedimmediate supersets subsets, form lattice structure. structure N = 4shown Fig. 1. sayNNi =(1)sets size level lattice. described below, various pathslattice levels near bottom solutions, level L, used createquantum interference basis search algorithm.example, consider problem N = 4 L = 2, suppose constraintseliminate items 1 3. sets fg, f2g, f4g partial goods, f1gf3g partial nogoods. Among 6 complete sets, f2,4g good otherssupersets f1g f3g hence nogood.94fiQuantum Computing Phase Transitions Combinatorial Searchsearch problems studied here, nogoods directly specified problemconstraints small sets items, e.g., size two three. hand,number items size solutions grow problem size. givesnumber small nogoods, i.e., near bottom lattice. Examples problems include binary constraint satisfaction, graph coloring propositional satisfiabilitymentioned above.CSPs, items possible variable-value pairs problem. ThusCSP n variables b values N = nb items2 . solution consistsassignment variable satisfies whatever constraints given problem.Thus solution consists set L = n items. terms general frameworkcombinatorial search constraint satisfaction problems also contain numberproblem-independent necessary nogoods, namelyb corresponding givingvariable two different values. n 2 necessary nogoods. nontrivialsearch must b2, restrict attention case LN=2.requirement important allowing construction quantum search method described below.Another example given simple CSP consisting n = 2 variables (v1 v2)take one b = 2 values (1 2) single constrainttwo variables take distinct values, i.e., v1 6= v2 . Hence N = nb = 4 variablevalue pairs v1 = 1; v1 = 2; v2 = 1; v2 = 2 denote items 1; 2; 3; 4 respectively.corresponding lattice given Fig. 1. nogoods problem?First due explicit constraint two variables distinctvalues: fv1 = 1; v2 = 1g fv1 = 2; v2 = 2g f1; 3g f2; 4g. addition,necessary nogoods implied requirement variable takes unique valueset giving multiple assignments variable necessarily nogood, namelyfv1 = 1; v1 = 2g fv2 = 1; v2 = 2g f1; 2g f3; 4g. Referring Fig. 1, seefour nogoods force sets size 3 4 nogood too. However, sets size zeroone goods remaining two sets size two: f2; 3g f1; 4g correspondingfv1 = 2; v2 = 1g fv1 = 1; v2 = 2g solutions problem.Search methods use various strategies examining sets lattice. instance,methods simulated annealing (Kirkpatrick, Gelatt, & Vecchi, 1983), heuristic repair (Minton, Johnston, Philips, & Laird, 1992) GSAT (Selman, Levesque, & Mitchell,1992) move among complete sets, attempting find solution series small changessets. Generally search techniques continue indefinitely problemsolution thus never show problem insoluble. methods calledincomplete. methods, search repeated, different initial conditionsmaking different random choices, either solution found specified limitnumber trials reached. latter case, one cannot distinguish problemsolution series unlucky choices soluble problem. searchtechniques attempt build solutions starting smaller sets, often process extending consistent set either solution found consistent extensionspossible. latter case search backtracks previous decision point tries2. lattice sets also represent problems variable different number assignedvalues.95fiHogganother possible extension choices remain. recording pending choicesdecision point, backtrack methods determine problem insoluble, i.e.,complete systematic search methods.description highlights two distinct aspects search procedure: generalmethod moving among sets, independent particular problem, testing procedure checks sets consistency particular problem's requirements. Often,heuristics used make search decisions depend problem structure hopingidentify changes likely lead solution avoid unproductive regionssearch space. However, conceptually aspects separated, casequantum search algorithm presented below.3. Quantum Search Methodssection brie describes capabilities quantum computers, straightforward attempts exploit capabilities search particularly effective,motivates describes new search algorithm.3.1 Overview Quantum Computersbasic distinguishing feature quantum computer (Benioff, 1982; Bernstein & Vazirani, 1993; Deutsch, 1985, 1989; Ekert & Jozsa, 1995; Feynman, 1986; Jozsa, 1992; Kimber,1992; Lloyd, 1993; Shor, 1994; Svozil, 1995) ability operate simultaneouslycollection classical states, thus potentially performing many operations time classical computer would one. Alternatively, quantum parallelism viewedlarge parallel computer requiring hardware needed single processor.hand, range allowable operations rather limited.describe concretely, adopt conventional ket notation quantummechanics (Dirac, 1958, section 6) denote various states3. is, use jffi denotestate computer described ff. low level description, state classicalcomputer described values bits. instance n bits,N = 2n possible states machine, associated numberss1 = 0; : : :; sN = 2n , 1. say computer state jsi valuesbits correspond number , 1. commonly, computer described termshigher level constructs formed groups bits, integers, character strings, setsaddresses variables program. example, state could arisesearch jfv1 = 1; v2 = 1g; soln = Falsei corresponding set assignments variablesCSP value false program variable soln, e.g., used represent whethersolution found. higher level descriptions, often aspectscomputer's state, e.g., stack pointers values various iteration counters,explicitly mentioned.states presented far, bit higher-level construct definite value,apply classical quantum computers. However, quantum computersfar richer set possible states. Specifically, js1 i; : : :; jsN possible states3. ket notation conceptually similar use boldface denote vectors distinguishscalars.96fiQuantum Computing Phase Transitions Combinatorial Searchclassical computer, possible states corresponding quantumlinearP js computersuperpositions states, i.e., states form jsi =complexnumber called amplitude associated state jsi i. physical interpretationamplitudes comes measurement process. measurement madequantum computer state jsi, e.g., determine result computationrepresented particular configuration bits register, one possible classicalstates obtained. Specifically, classical state jsi obtained probability j j2.Furthermore, measurement process changes state computer exactly matchresult. is, measurement said collapse original superpositionnew superposition consisting single classical state (i.e., amplitude returnedstate 1 amplitudes zero). means repeated measurements alwaysreturn result.important consequence interpretation results fact probabilitiesmust sum one. Thus amplitudes superposition states must satisfynormalization conditionX 2j ij = 1(2)Another consequence full state quantum computer, i.e., superposition,observable quantity. Nevertheless, changing amplitude associateddifferent classical states, operations superposition affect probabilityvarious states observed. possibility crucial exploiting quantumcomputation, makes potentially powerful probabilistic classical machines,choices program made randomly.superpositions also viewed vectors space whose basis individual classical states jsi component vector along ith basis elementspace. state vector also specified components ( 1; : : :; N )basis understood context. inner product two vectors= PNi=1 denotes complex conjugate . matrix notation,also written treated column vector row vectorgiven transpose entries changed complex conjugate values.vectors, normalization condition amounts requiring = 1.complete overview quantum computers, remains describe superpositions used within program. addition measurement process describedabove, two types operations performed superposition states.first type run classical programs machine, second allows creating manipulating amplitudes superposition. cases, keyproperty superposition linearity: operation superposition states givessuperposition operation acting states individually. describedbelow, property, combined normalization condition, greatly limits rangephysically realizable operations.first case, quantum computer perform classical program providedreversible, i.e., final state contains enough information recover initial state. Oneway achieve retain initial input part output. illustratelinearity operations, consider reversible classical computation states, e.g.,f (si ) produces new state given input one. applied superposition97fiHoggPstates, result f (jsi) =jf (si )i. reversibility required? Supposeprocedure f reversible, i.e., maps least two distinct states result.example, suppose f (s1 ) = f (s2 ) = s3 . superposition jsi = p12 (js1 + js2 i)plinearity requires f (jsi) = p12 (jf (s1 )i + jf (s2 )i) giving 2js3 i, superpositionviolates normalization condition. Thus irreversible classical operation physically realizable superposition, i.e., cannot used quantum parallelism.contrast use computations individual states, second type operationmodifies amplitude various states within superposition. is, startingjsi =Pjk sk operation, denoted U, creates new superposition js0i = U jsi = P j0 jsj i.operations linear respect superpositions,new amplitudesPexpressed terms original ones j0 = k Ujk k , matrix notation0 = U . is, linearity means operation changing amplitudesrepresented matrix. satisfy normalization condition, Eq. 2, matrix must( 0)y 0 = 1. terms matrix U condition becomes41 = (U )y(U ) = U yU(3)must hold initial state vector = 1. see impliesmatrix U yU , suppose = e^j = (: : :; 0; 1; 0; : : :) jth unit vector,corresponding superposition jsj amplitudes zero except j = 1.case yA = Ajj must equal one Eq. 3. is, diagonal elementsU yU must equal one. = p12 (^ej + e^k ) j 6= k,yA = 1 (^ej + ^ek )A(^ej + ^ek )2(4)= 12 [Ajj + Akk + Ajk + Akj ]must equal one Eq. 3, already know diagonal terms equal one. Thusconclude Ajk = ,Akj . similar argument using = p12 (^ej + ie^k ), superpositionimaginary value second amplitude, gives Ajk = Akj . Together conditionsmean identity matrix, U yU = , i.e., matrix U must unitaryoperate superpositions. Moreover, condition sucient make initial statesatisfy Eq. 3. shows restriction linear unitary operations arises directlylinearity quantum mechanics Eq. 2, normalization condition probabilities.class unitary matrices includes permutations, rotations arbitrary phase changes(i.e., diagonal matrices element diagonal complex numbermagnitude equal one).Reversible classical programs, unitary operations superpositions measurement process basic ingredients used construct program quantumcomputer. used search algorithm described below, program consistsfirst preparing initial superposition states, operating states seriesunitary matrices conjunction classical program evaluate consistency4.Uytranspose U elements changed complex conjugates.98,Ujk = (Ukj ) .fiQuantum Computing Phase Transitions Combinatorial Searchvarious states respect search requirements, making measurementobtain definite final answer. amplitudes superposition measurement made determine probability obtaining solution. overall structureprobabilistic Monte Carlo computation (Motwani & Raghavan, 1995)trial probability get solution, guarantee. means searchmethod incomplete: find solution one exists never guarantee solutiondoesn't exist.alternate conceptual view quantum programs provided path integral approach quantum mechanics (Feynman, 1985). view, final amplitudegiven state obtained weighted sum possible paths produce state.way, various possibilities involved computation interfereother, either constructively destructively. differs classical combinationprobabilities different ways reach outcome (e.g., used probabilisticalgorithms): probabilities simply added, giving possibility interference. Interference also seen classical waves, sound ripples surface water.systems lack capability quantum parallelism. various formulationsquantum mechanics, involving operators, matrices sums paths equivalentsuggest different intuitions constructing possible quantum algorithms.3.2 Example: One-Bit Computersimple example ideas given single bit. case two possibleclassical states j0i j1i corresponding values 0 1, respectively, bit.defines two dimensional vector space superpositions quantum bit.number proposals implementing quantum bits, i.e., devices whose quantum mechanicalproperties controlled produce desired superpositions two classical values. Oneexample (DiVincenzo, 1995; Lloyd, 1995) atom whose ground state correspondsvalue 0 excited state value 1. use lasers appropriate frequenciesswitch atom two states create superpositions two classicalstates. ability manipulate quantum superpositions demonstrated smallcases (Zhu, Kleiman, Li, Lu, Trentelman, & Gordon, 1995). Another possibilityuse atomically precise manipulations (DiVincenzo, 1995) using scanning tunnelingatomic force microscope. possibility precise manipulation chemical reactionsalso demonstrated (Muller, Klein, Lee, Clarke, McEuen, & Schultz, 1995).also number proposals investigation (Barenco, Deutsch, & Ekert, 1995;Sleator & Weinfurter, 1995; Cirac & Zoller, 1995), including possibility multiplesimultaneous quantum operations (Margolus, 1990).simple computation quantum bit logical operation, i.e., NOT(j0i) =j1i NOT(j1i) = j0i. operator simply exchanges state vector's components:01NOT( 0j0i + 1j1i) = 0j1i + 1j0i9910(5)fiHogg0 1operation also represented multiplication permutation matrix 1 0 .Another operator given rotation matrixcos , sinU () = sin cos(6)used create superpositions single classical states, e.g.,1111U 4 0 U 4 j0i = p (j0i + j1i) p 1(7)22rotation matrix also used illustrate interference, important wayquantum computers differ probabilistic classical algorithms. First, considerclassical algorithm two methods generating random bits, R0 (producing \0"probability 3=4) R1 (producing \0" probability 1=4). Suppose \0" representsfailure (e.g., probabilistic search find solution) \1" representssuccess. Finally, let classical algorithm consist selecting one methods use,probability p pick R0 . overall probability obtain \0" finalresult 34 p + 14 (1 , p)(8)Pclassical = 41 + p2best done choose p = 0, giving probability 1=4 failure.quantum analog simple calculation obtained rotation = 3 .Starting individual classical states gives superpositions1 1 p3 !U 3 0 =2 10 1 ,1U= p(9)312 3correspond generators R0 R1 respectively, respectiveprobabilities 3=4 1=4 produce \0" measured. Starting insteadsuperposition two classical states, cossin , corresponds step classicalalgorithm generator R0, isselectedprobability p = cos2 . resulting statecosapplying rotation, U 3 sin , probabilityp2Pquantum = 41 + cos2 , 43 sin (2)p= Pclassical , 43 sin (2)(10)produce \0" value. case minimum value probability obtain\0" 1=4 fact made equal 0 choice = 3 . caseamplitudes two original states exactly cancel other, example destructiveinterference.final example, illustrating limits operations superpositions, considersimple classical program sets bit value one. is, SET(j0i) = j1i100fiQuantum Computing Phase Transitions Combinatorial SearchSET(j1i) = j1i. operation isnot reversible: knowing result determineoriginal input. linearity, SET p12 (j0i + j1i) = p12 (SET(j0i) + SET(j1i)), turnpp12 2j1i = 2j1i. state violates normalization condition. Thus seeclassical operation physically realizable quantum computer. Similarly, anothercommon classical operation, making copy bit, also ruled (Svozil, 1995), formingbasis quantum cryptography (Bennett, 1992).3.3 Approaches Searchdevice consisting n quantum bits allows operations superpositions 2n classicalstates. ability operate simultaneously exponentially large number stateslinear number bits basis quantum parallelism. particular, repeating operation Eq. 7 n times, different bit, gives superposition equalamplitudes 2n states.first sight quantum computers would seem ideal combinatorial search problems class NP. problems, ecient procedure f (s) takespotential solution set determines whether fact solution, exponentially many potential solutions, fact solutions. s1 ; : : :; sNpotential sets consider, quickly form superposition p1N (js1i + : : : + jsN i)simultaneously evaluate f (sP) states, resulting superpositionsets evaluation, i.e., p1N jsi ; soln = f (si )i. jsi ; soln = f (si )i representsclassical search state considering set si along variable soln whose value truefalse according result evaluating consistency set respectproblem requirements. point quantum computer has, sense, evaluatedpossible sets determined solutions. Unfortunately, make measurement system, get set equal probability 1=N unlikelyobserve solution. thus better slow classical search method randomgenerate-and-test sets randomly constructed tested solution found.Alternatively, obtain solution high probability repeating operationO(N ) times, either serially (taking long time) multiple copies device (requiring large amount hardware energy if, say, computation done using multiplephotons). shows trade-off time energy (or physical resources),conjectured apply generally solving search problems (Cerny, 1993),also seen trade-off time number processors parallel computers.useful combinatorial search, can't evaluate various sets insteadmust arrange amplitude concentrated solution sets greatly increaseprobability solution observed. Ideally would done mappinggives constructive interference amplitude solutions destructive interference nonsolutions. Designing maps complicated fact must linear unitaryoperators described above. Beyond physical restriction, algorithmiccomputational requirement: mapping eciently computable (DiVincenzo &Smolin, 1994). example, map cannot require priori knowledge solutions(otherwise constructing map would require first search). computationalrequirement analogous restriction search heuristics: useful, heuristicmust take long time compute. requirements mapping trade101fiHoggother. Ideally one would like find way satisfy mapcomputed polynomial time give, worst, polynomially small probability getsolution problem soluble. One approach arrange constructive interferencesolutions nonsolutions receive random contributions amplitude.random contributions effective complete destructive interference,easier construct form basis recent factoring algorithm (Shor, 1994) wellmethod presented here.Classical search algorithms suggest ways combine use superpositionsinterference. include local repair styles search complete assignmentsmodified, backtracking search, solutions built incrementally. Using superpositions, many possibilities could simultaneously considered. However searchmethods priori specification number steps required reach solutionunclear determine enough amplitude might concentrated solutionstates make measurement worthwhile. Since measurement process destroyssuperposition, possible resume computation point measurement made produce solution. subtle problem arisesdifferent search choices lead solutions differing numbers steps. Thus one would alsoneed maintain amplitude already solution states search continues.dicult due requirement reversible computations.may fruitful investigate approaches further, quantum methodproposed based instead breadth-first search incrementally buildssolutions. Classically, methods maintain list goods given size. step,list updated include goods one additional variable. Thus step i, listconsists sets size used create new list sets size + 1. CSPn variables, ranges 0 n , 1, completing n steps listcontain solutions problem. Classically, useful method findingsingle solution list partial assignments grows exponentially numbersteps taken. quantum computer, hand, handle lists readilysuperpositions. method described below, superposition step consistssets size i, consistent ones, i.e., sets level lattice.question make final measurement computation requires exactlyn steps. Moreover, opportunity use interference concentrate amplitudetoward goods. done changing phase amplitudes corresponding nogoodsencountered moving lattice.division search methods general strategy (e.g., backtrack)problem specific choices, quantum mapping described general matrixcorresponds exploring possible changes partial sets, separate, particularlysimple, matrix incorporates information problem specific constraints.complex maps certainly possible, simple decomposition easier designdescribe. decomposition, dicult part quantum mapping independentdetails constraints particular problem. suggests possibilityimplementing special purpose quantum device perform general mapping.constraints specific problem used adjust phases described below,comparatively simple operation.102fiQuantum Computing Phase Transitions Combinatorial Searchconstraint satisfaction problems, simple alternative representation full latticestructure use partial assignments only, i.e., sets variable-value pairsvariable once. first sight might seem better removesconsideration necessary nogoods hence increases proportion complete setssolutions. However, case number sets function level latticedecreases reaching solution level, precluding simple form unitary mappingdescribed quantum search algorithm. Another representation avoidsproblem consider assignments single order variables (selected randomlyuse heuristics). version set lattice previously usedtheoretical analyses phase transitions search (Williams & Hogg, 1994). mayuseful explore quantum search, unlikely effective.fixed ordering sets become nogood last steps,resulting less opportunity interference based nogoods focus solutions.3.4 Motivationmotivate mapping described below, consider idealized version. showspaths lattice tend interfere destructively nonsolution states, providedconstraints small.idealized map simply maps set lattice equally supersets nextlevel, introducing random phases sets found nogood. discussionconcerned relative amplitude solutions nogoods ignore overallnormalization. Thus instance, N = 6, state jf1; 2gi map unnormalizedsuperposition four supersets size 3, namely state jf1; 2; 3gi + : : : + jf1; 2; 6gi.mapping, good level j receive equal contribution jsubsets prior level. Starting amplitude 1 level 0 gives amplitudej ! goods level j. particular, L! solutions.compare contribution nogoods, average? dependmany subsets nogoods also. simple case comparison setslattice nogood (starting level k given size constraints,e.g., k = 2 problems binary constraints). Let rj expected valuemagnitude amplitude sets level j. set level k rk = k! (andzero phase) smaller subsets goods. set level j>k sumj contributions (nogood) subsets, giving total contribution(s) =jXm=1(sm )eim(11)sm subsets size j , 1 phases randomly selected.(sm ) expected magnitude rj ,1 phase combinedgive new random phase . Ignoring variation magnitude amplitudeslevel gives+*Xjp(12)rj = rj,1e = rj,1 jm=1103fiHoggsum j random phases equivalent unbiased prandom walk (Karlinp &j.Thusr=rTaylor, 1975)junitstepsexpectednetdistancejk j !=k!prj = j !k! j>k.crude argument gives rough estimate relative probabilities solutionscompared complete nogoods. Suppose one solution. relativeprobability L!2 . nogoods relative probability (NL , 1)rL2 NLL!k! NLgiven Eq. 1. interesting scaling regime L = N=b fixed b, correspondingvariety well-studied constraint satisfaction problems. gives!NL!Psoln= ln N k! b ln N + O(N )(13)ln PnogoodLgoes infinity problems get large enhancement solutionsenough compensate rareness among sets solution level.main limitation argument assuming subsets nogood alsonogood. many nogoods, case, resulting less opportunitycancellation phases. worst situation respect subsets goods.could constraints large, i.e., don't rule states manyitems included. Even small constraints, could happen occasionally duepoor ordering choice adding items sets, hence suggesting lattice restrictedassignments single order much less effective canceling amplitude nogoods.problems considered here, small constraints, large nogood cannotmany good subsets nogood means small subset violates (small) constrainthence subsets obtained removing one element still contain bad subsetgiving nogood. fact, numerical experiments (with class unstructuredproblems described below) show mapping effective canceling amplitudenogoods. Thus assumptions made simplified argument seem providecorrect intuitive description behavior.Still assumption many nogood subsets underlying argument suggest extreme cancellation derived least apply problem manylarge partial solutions. gives simple explanation diculty encounteredfull map described phase transition point: transition associatedproblems relatively many large partial solutions complete solutions. Henceexpect relatively less cancellation least nogoods solution levellower overall probability find solution.discussion suggests mapping sets supersets along random phasesintroduced inconsistent set greatly decrease contribution nogoodssolution level. However, mapping physically realizableunitary. example, mapping level 1 2 N = 3 takes statesjf1gi; jf2gi; jf3gi jf1; 2gi; jf1; 3gi; jf2; 3gi matrix01 1 01= @1 0 1A(14)0 1 1Here, first column means state jf1gi contributes equally jf1; 2gi jf1; 3gi,supersets, gives contribution jf2; 3gi. see immediately columns104fiQuantum Computing Phase Transitions Combinatorial Searchthispmatrix orthogonal, though easily normalized dividing entries2. generally, mapping takes set level N , sets oneelement. corresponding matrix one column i{set one row(i +1)-set. column exactly N , 1's (corresponding supersetsgiven i{set) remaining entries 0. Two columns singlenonzero value common (and two corresponding i{sets onevalues common: way share superset common).means N gets large, columns matrix almost orthogonal (providedi<N=2, case interest here). fact used obtain unitary matrixfairly close M.3.5 Search Algorithmgeneral idea mapping introduced move much amplitude possiblesupersets (just classical breadth-first search, increments partial sets give supersets).combined problem specific adjustment phases based testing partialstates consistency (this corresponds diagonal matrix thus particularly simplerequire mixing amplitudes different states). specificmethods used described section.3.5.1 Problem-Independent Mappingtake advantage potential cancellation amplitude nogoods describedneed unitary mapping whose behavior similar ideal mapping supersets.two general ways adjust ideal mapping sets supersets (mixturestwo approaches possible well). First, keep amplitudelevel lattice instead moving amplitude next level. allowsusing ideal map described (with suitable normalization) gives excellentdiscrimination solutions nonsolutions, unfortunately much amplitudereaches solution level. surprising: use random phases cancel amplitudenogoods doesn't add anything solutions (because solutions supersetnogood hence cannot receive amplitude them). Hence best, evennogoods cancel completely, amplitude solutionsrelative number among complete sets, i.e., small. Thus random phases preventmuch amplitude moving nogoods high lattice, instead contributingsolutions amplitude simply remains lower levels lattice. Hencebetter chance random selection finding solution (but, solution found,instead getting nogood solution level, likely get smaller setlattice). Thus must arrange amplitude taken nogoods contribute insteadgoods. requires map take amplitude sets supersets,least extent.second way fix nonunitary ideal map move amplitude also nonsupersets. move amplitude solution level. allows canceledamplitude nogoods go goods, also vice versa, resulting less effectiveconcentration solutions. done unitary matrix close possiblenonunitary ideal map supersets, also relatively simple form.105fiHogggeneral question given k linearly independent vectors dimensional space,km, find k orthonormal vectors space close possible k original ones.Restricting attention subspace defined original vectors, obtained5using singular value decomposition (Golub & Loan, 1983) (SVD) matrixwhose columns k given vectors. Specifically, decomposition = AyB ,diagonal matrix containing singular values Ay Borthonormal columns. real matrix M, matrices decompositionalso real-valued. matrix U = AyB orthonormal columns closest setorthogonal vectors accordingFrobenius matrix norm. is, choice UPminimizes jU , j2 rs jUrs , Mrs j2 among unitary matrices. construction failsk>m since m{dimensional space cannot orthogonal vectors. Hencerestrict consideration mappings lattice levels level + 1least many sets level i, i.e., Ni Ni+1 . Obtaining solution requires mappinglevel L so, Eq. 1, restricts consideration problems LdN=2e.example, mapping level 1 2 N = 3 given Eq. 14 singularvalue decomposition = AyB decomposition given explicitly0 p1 , p1BB p13 p1 2B = @ 32p131010 p13 p13 p13 1200CBp1p1 Cq6 2 CA@ 00 10 01 AB@ q02 , 12 21 CA, 33 , p6 , p6p16p10closest unitary matrix(15)0 2 2 ,1 11U = AyB = 3 @ 2 ,1 2(16),1 2 2gives set orthonormal vectors close original map, one mightconcerned requirement compute SVD exponentially large matrices.Fortunately, however, resulting matrices particularly simple structureentries depend overlap sets. Thus write matrix elementsform Urfi = ajr\fi j (r (i+1)-subset, fi i-subset). overlap jr \ fi j rangesfi r 0 overlap. Thus instead exponentially manydistinct values, + 1, linear number. exploited givesimpler method evaluating entries matrix follows.get expressions values given N since resulting columnvectors orthonormal. Restricting attention real values, givesXi1 = U yU ffff = nk a2kk=0nk = ki +N 1,,i k(17)(18)5. thank J. Gilbert pointing technique, variant orthogonal Procrustes problem (Golub & Loan, 1983).106fiQuantum Computing Phase Transitions Combinatorial Searchnumber ways pick r specified overlap. off-diagonal terms,suppose jff \ fi j = p<i then, real values matrix elements,Xi0 = U yU fffi =n(jkp)aj akj;k=0(p) =njkX , p p , p N , 2i + px k,x x j ,x i+1,j,k+x(19)(20)number sets r required overlaps ff fi , i.e., jr \ ffj = kijr \ fij = j i. sum, x number items set r commonff fi . Together give + 1 equations values a0; : : :; ai, readilysolved numerically6 . multiple solutions system quadratic equations,representing possible unitary mapping. unique one closestideal mapping supersets, given SVD. solution use quantumsearch algorithm7 , although possible solution, conjunction variouschoices phases, performs better. Note number values equations growslinearly level lattice, even though number sets level growsexponentially. necessary distinguish values different levels lattice,use a(ki) mean value ak mapping level + 1.example Eq. 14, N = 3 = 1, 1 = a20 + 2a21 Eq. 170 = 2a0a1 + a21 Eq. 19. solution unitarity conditions closest Eq. 14a0 = , 13 ; a1 = 23 corresponding Eq. 16.normalized version ideal map a(ii) = p1ni = pN1 ,i values equalzero. actual values a(ki) fairly close (confirming ideal mapclose orthogonal already), alternate sign. illustrate behavior, usefulconsider scaled values b(ki) (,1)k a(i,i)k pni,k , ni,k evaluated using Eq. 18.behavior values N = 10 shown Fig. 2. Note b(0i) close one,decreases slightly higher levels lattice (i.e., larger values) considered:ideal mapping closer orthogonal low levels lattice.Despite simple values example Eq. 16, ak values generalappear simple closed form expression. suggested obtaining exact solutions Eqs. 17 19 using Mathematica symbolic algebra program (Wolfram, 1991).cases gives complicated expressions involving nested roots. Since expressions could simplify, ak values also checked close rational numberswhether roots single variable polynomials low degree8 . Neither simplificationfound apply.Finally note mapping describes sets levelmapped next level. full quantum system also perform mapping6. High precision values obtained FindRoot function Mathematica.7. values given Online Appendix 1.8. Using Mathematica function Rationalize package NumberTheory`Recognize`.107fiHogg10.50.20.10.0501234kFigure 2: Behavior b(ki) vs. k log scale N = 10. three curves show values= 4 (black), 3 (dashed) 2 (gray).remaining sets lattice. changing map step, setssimply left unchanged, need map sets level + 1identity mapping orthogonal map level i. orthogonal setmapping partly back level partly remaining sets level + 1 suitablethis: application amplitude level + 1 map used hencedoesn't matter mapping used. However, choice part overallmapping remains degree freedom could perhaps exploited minimize errorsintroduced external noise.3.5.2 Phases Nogoodsaddition general mapping one level next, problem-specificaspect algorithm, namely choice phases nogood sets level.ideal case described above, random phases given nogood, resultinggreat deal cancellation nogoods solution level. reasonablechoice unitary mapping, policies possible well. example, one couldsimply invert phase nogood9 (i.e., multiply amplitude -1). choicedoesn't work well idealized map supersets but, shown below, helpfulunitary map. motivated considering coecients shown Fig. 2.Specifically, nogood encountered first time path lattice,would like cancel phase supersets time enhance amplitudesets likely lead solutions. a(i,i)1 negative, inverting phase tendadd sets differ one element nogood. least avoidviolating small constraint produced nogood set, hence may contributeeventually sets lead solutions.Moreover, one could use information sets next level decidephase: currently described, computation makes use testing9. thank J. Lamping suggesting this.108fiQuantum Computing Phase Transitions Combinatorial Searchconsistency sets solution level itself, hence completely ineffective problemstest requires complete set. Perhaps better would mark state nogoodconsistent extensions one item (this simple check since numberextensions grows linearly problem size). Another possibility phaseadjusted based many constraints violated, could particularlyappropriate partial constraint satisfaction problems (Freuder & Wallace, 1992)optimization searches.3.5.3 Summarysearch algorithm starts evenly dividing amplitude among goods low levelK lattice. convenient choice binary CSPs start level K = 2,number sets proportional N 2. level K L , 1, adjustphases states depending whether good nogood map nextlevel. Thus ff(j ) represents amplitude set ff level j,(j +1) = X U (j )rff ff ffrff=Xka(kj )Xjr\ffj=kff(j )ff(21)ff phase assigned set ff testing whether nogood, finalinner sum sets ff k items common r. is, ff = 1 ffgood set. nogoods, ff = ,1 using phase inversion method, ff = eiuniformly selected [0; 2 ) using random phase method. Finallymeasure state, obtaining complete set. set solution probabilitypsoln =X fifififi(L) fi2fi(22)sum solution sets, depending particular problem methodselecting phases.computational resources required algorithm? storage requirementsquite modest: N bits produce superposition 2N states, enough representpossible sets lattice structure. Since trial algorithm givessolution probability psoln , average need repeated 1=psoln timesfind solution. cost trial consists time required constructinitial superposition evaluate mapping step level Ksolution level L, total L , K<N=2 mappings. initialstate consistssets size K, polynomial number (i.e., N K ) hencecost construct initial superposition relatively modest. mappingone level next need produced series elementary operationsdirectly implemented physical devices. Determining required numberoperations remains open question, though particularly simple structurematrices require involved computations also able exploitspecial purpose hardware. rate, mapping independent structureproblem cost affect relative costs different problem structures.Finally, determining phases use nogood sets involves testing setsconstraints, relatively rapid operation NP search problems. Thus examine109fiHoggcost search algorithm depends problem structure, key quantitybehavior psoln .3.6 Example Quantum Searchillustrate algorithm's operation behavior, consider small case N = 3map starting level K = 0 going level L = 2. Suppose f3gsupersets nogoods. begin amplitude empty set, i.e.,state j;i. map level 0 1 gives equal amplitude singleton sets,producing p13 (jf1gi + jf2gi + jf3gi). introduce phase nogood set, givingp1 jf1gi + jf2gi + ei jf3gi . Finally use Eq. 16 map sets level 2, giving3final statep1 4 , ei jf1; 2gi + 1 + 2ei jf1; 3gi + 1 + 2ei jf2; 3gi(23)3 3level, set f1,2g good, i.e., solution. Note algorithm makeuse testing states solution level consistency.probability obtain solution final measurement made determinedamplitude solution set, case Eq. 22 becomesfifi 1fififi2 1fipsoln = fi p 4 , e fi = 27 (17 , 8 cos )3 3(24)see effect different methods selecting phase nogoods.phase selected randomly, psoln = 1727 = 0:63 average value coszero. Inverting phase nogood, i.e., using = , gives psoln = 2527 = 0:93.probabilities compare 1/3 chance selecting solution random choice.case, optimal choice phase obtained simple inversion.However true general: picking phases optimally require knowledgesolutions hence feasible mapping. Note also even optimal choicephase doesn't guarantee solution found.4. Average Behavior Algorithmsection, behavior quantum algorithm evaluated two classes combinatorial search problems. first class, unstructured problems, used examinephase transition particularly simple context using random inverted phasesnogoods. second class, random propositional satisfiability (SAT), evaluatesrobustness algorithm problems particular structure.classical simulation algorithm explicitly compute amplitude setslattice solution level mapping levels. Unfortunately,results exponential slowdown compared quantum implementation severelylimits feasible size classical simulations. Moreover, determining expectedbehavior random phase method requires repeating search number timesproblem (10 tries experiments reported here). limits feasibleproblem size.110fiQuantum Computing Phase Transitions Combinatorial Searchsimple check numerical errors calculation, recorded totalnormalization sets solution level. double precision calculations SunSparc10, experiments reported typically norm 1 within times10,11. indication execution time unoptimized C++ code, single trialproblem N = 14 16, L = N=2, required 70 1000 seconds,respectively. uses direct evaluation map one level next givenEq. 21. substantial reduction compute time possible exploiting simplestructure mapping give recursive evaluation10 . additional improvementpossible exploiting fact amplitudes real using methodinverts phases nogoods. reduced execution time 1 6 seconds pertrial N 14 16, respectively.4.1 Unstructured Problemsexamine typical behavior quantum search algorithm respect problemstructure, need suitable class problems. particularly important averagecase analyses since one could inadvertently select class search problems dominatedeasy cases. Fortunately observed concentration hard cases near phase transitionsprovides method generate hard test cases.phase transition behavior seen variety search problem classes.select particularly simple class problems supposing constraints specifynogoods randomly level 2 lattice. corresponds binary constraint satisfactionproblems (Prosser, 1996; Smith & Dyer, 1996), ignores detailed structurenogoods imposed requirement variables unique assignment. ignoringadditional structure, able test wider range number specified nogoodsproblems would case considering constraint satisfaction problems.lack additional structure also likely make asymptotic behavior readilyapparent small problem sizes feasible classical simulation.Furthermore, since quantum search algorithm appropriate soluble problems, restrict attention random problems solution. could obtainedrandomly generating problems rejecting solution (as determinedusing complete classical search method). However, overconstrained problems soluble ones become quite rare dicult find method. Instead, generateproblems prespecified solution. is, randomly selecting nogoods addproblem, pick nogoods subsets prespecified solution set.always produces problems least one solution. Although problems tendbit easier randomly selected soluble problems, nevertheless exhibitconcentration hard problems location general randomproblems (Cheeseman et al., 1991; Williams & Hogg, 1994). quantum search startedlevel 2 lattice.10. thank S. Vavasis suggesting improvement classical simulation algorithm.111fiHoggcost25201510512345Figure 3: solid curves show classical backtrack search cost randomly generatedproblems prespecified solution function fi = m=N N = 10(gray) 20 (black) L = N=2. number nogoods selectedlevel 2 search lattice. cost average number backtrack steps,starting empty set, required find first solution problem,averaged 1000 problems. error bars indicate standard deviationestimate average value, cases smaller sizeplotted points. comparison, dashed curves show probabilitysolution randomly generated problems specified fi value,ranging 1 left 0 right.4.1.1 Theoryclass problems, phase transition behavior illustrated Fig. 3. Specifically,shows cost solve problem simple chronological backtrack search.cost given terms number search nodes considered solution found.minimum cost, search proceeds directly solution backtrackL + 1. parameter distinguishing underconstrained overconstrained problemsratio fi number nogoods level 2 given constraints numberitems N.Even relatively small problems, peak average search cost evident.Moreover, peak near transition region random problems11 changemostly soluble mostly insoluble. simple, approximate, theoretical valuelocation transition given point expected number solutionsequal one (Smith & Dyer, 1996; Williams & Hogg, 1994). Applying classproblems considered straightforward. Specifically, NL complete setsproblem, given Eq. 1. particular set size L good, i.e., solution,none nogoods selected problem subset s. Hence probability11. is, problems generated random selection nogoods without regard whethersolution.112fiQuantum Computing Phase Transitions Combinatorial Searchsolution given( N ),( L )22L = (mN )(25)2N2 sets size 2 choose nogoods specified directlyconstraints. average number solutions Nsoln = NL L. set= fiN L = N=b, large N becomes11ln N N h+ fi ln 1 ,(26)solnbb2h(x) ,x ln x , (1 , x) ln (1 , x). predicted transition point12 given(27)ficrit = , lnh(1(1,=b1)=b2)ficrit = 2:41 case considered (i.e., b = 2). closely matcheslocation peak search cost problems prespecified solution,shown Fig. 3, 20% larger location step solubility13 .Furthermore, theory predicts regime polynomial average cost sucientlyconstraints (Hogg & Williams, 1994). determined conditionexpected number goods level lattice monotonically increasing. Repeatingargument smaller levels lattice, find condition holds2(28)fipoly = b 2,b 1 ln(b , 1)fipoly = 0 b = 2.estimates approximate, indicate class randomsoluble problems defined behaves qualitatively quantitatively respecttransition behavior variety other, perhaps realistic, problem classes.close correspondence theory (derived limit large problems), suggestsobserving correct transition behavior even relatively small problems.Moreover approximate theoretical argument suggests average costgeneral classical search methods scales exponentially size problemfull range fi>0. Thus provides good test case average behaviorquantum algorithm. final observation, important obtain sucient numbersamples, especially near transition region. considerable variationproblems near transition, specifically highly skewed distribution numbersolutions. region, problems solutions extremely many:enough fact give substantial contribution average number solutions eventhough problems quite rare.12. differs slightly results problems specified structure nogoods,explicitly removing necessary nogoods consideration (Smith & Dyer, 1996; Williams & Hogg,1994).13. particularly large error theory: better problems larger constraintsallowed values per variable.113fiHogg<T>35302520151051243567Figure 4: Expected number trials hT find solution vs. fi random problemsprespecified solution binary constraints, using random phases nogoods.solid curve N = 10, 100 samples per point. gray curveN = 20 10 samples per point (but additional samples used aroundpeak). error bars indicate standard error estimate hT i.4.1.2 Phase Transitionsee problem structure affects search algorithm, evaluate psoln , probabilityfind solution problems different structures, ranging underconstrainedoverconstrained. Low values probability indicate relatively harder problems.expected number repetitions search required find solution given= 1=psoln. results shown Figs. 4 5 different ways introducingphases nogood sets. see general easy-hard-easy pattern cases. Anothercommon feature phase transitions increased variance around transition region.quantum search property well, shown Fig. 6.4.1.3 Scalingimportant question behavior search method average performancescales problem size. examine question, consider scaling fixed fi .shown Figs. 7 8 algorithms using random inverted phases nogoods,respectively. help identify likely scaling, show results log plot(where straight lines correspond exponential scaling) log-log plot (where straightlines correspond power-law polynomial scaling).dicult make definite conclusions results two reasons. First,variation behavior different problems gives statistical uncertainty estimatesaverage values, particularly larger sizes fewer samples available.standard errors estimates averages indicated error bars figures(though cases, errors smaller size plotted points). Second,scaling behavior could change larger cases considered. caveats mind,figures suggest psoln remains nearly constant underconstrained problems, eventhough fraction complete sets solutions decreasing exponentially.114fiQuantum Computing Phase Transitions Combinatorial Search<T>151051243567Figure 5: Expected number trials hT find solution vs. fi random problemsprespecified solution binary constraints, using inverted phases nogoods.solid curve N = 10, 1000 samples per point. gray curveN = 20 100 samples per point (but additional samples used aroundpeak). error bars indicate standard error estimate hT i.dev(T)403020101243567Figure 6: Standard deviation number trials find solution N = 20function fi . black curve random phases assigned nogoods,gray one inverting phases.115fi0.50.50.30.3p(soln)p(soln)Hogg0.20.10.050.20.10.058101214N161820810121416 18 20N0.50.50.30.3p(soln)p(soln)Figure 7: Scaling probability find solution using random phase method,fi 1 (solid), 2 (dashed), 3 (gray) 4 (dashed gray). shown loglog-log scales (left right plots, respectively).0.20.10.0580.20.10.0510 12 14 16 18 20 22 24N8101214 16 18 20 2224NFigure 8: Scaling probability find solution using phase inversion method,fi 1 (solid), 2 (dashed), 3 (gray) 4 (dashed gray). shown loglog-log scales (left right plots, respectively).behavior also seen overlap curves small fi Figs. 4 5. problemsconstraints, psoln appears decrease polynomially size problem,i.e., curves closer linear log-log plots log plots. confirmedquantitatively making least squares fit values seeing residualsfit power-law smaller exponential fit. interesting observationcomparing two phase choices scaling qualitatively similar, even thoughphase inversion method performs better. suggests detailed values phasechoices critical scaling behavior, particular high precision evaluationphases required. Finally note illustration averagescaling leaves open behavior worst case instances.underconstrained cases Figs. 7 8 small additional differencecases even odd number variables. due oscillationsamplitude goods level lattice, discussed fully contextSAT problems below.116fiQuantum Computing Phase Transitions Combinatorial Searchratio51041031021011081012141618202224NFigure 9: Scaling ratio probability find solution using quantum algorithm probability find solution random selection solutionlevel, using phase inversion method, fi 1 (solid), 2 (dashed), 3 (gray)4 (dashed gray). curves close linear log scale indicating exponential improvement direct selection among complete sets,higher enhancement problems constraints.-1p(soln)10-210-310-410810121416N18202224Figure 10: Comparison scaling probability find solution quantum algorithm using phase inversion method (dashed curve) random selectionsolution level (solid curve) fi = 2.Another scaling comparison see much algorithm enhances probabilityfind solution beyond simple quantum algorithm evaluating complete setsmaking measurement. shown Fig. 9, quantum algorithm appearsgive exponential improvement concentration amplitude solutions.explicit view difference behavior shown Fig. 10 fi = 2. figure,dashed curve shows behavior psoln phase inversion method, identicalfi = 2 curve Fig. 8.117fiHogg4.2 Random 3SATexperiments leave open question additional problem structure might affectscaling behaviors. universality phase transition behavior searchmethods suggests average behavior algorithm alsowide range problems, useful check empirically. end algorithmapplied satisfiability (SAT) problem. constraint satisfaction problem consistspropositional formula n variables requirement find assignment (truefalse) variable makes formula true. Thus b = 2 assignmentsvariable N = 2n possible variable-value pairs. consider well-studiedNP-complete 3SAT problem formula conjunction c clauses,disjunction 3 (possibly negated) variables.SAT problem readily represented nogoods lattice sets (Williams &Hogg, 1994). described Sec. 2.2, n necessary nogoods, size 2.addition, distinct clause proposition gives single nogood size 3. casethus additional interest specified nogoods two sizes. evaluatingquantum algorithm, start level 3 lattice. Thus smallest casephase choices uence result n = 5.generate random problems given number clauses selecting numberdifferent nogoods size 3 among sets already excluded necessary nogoods14. random 3SAT, hard problems concentrated near transition (Mitchell et al., 1992) c = 4:2n. Finally, among randomly generatedproblems, use fact solution15 . Using randomly selectedsoluble problems results somewhat harder problems using prespecified solution.Like studies need examine many goods nogoods lattice (Schrag &Crawford, 1996), results restricted much smaller problems studiesrandom SAT. Consequently, transition region rather spread out. Furthermore,additional structure necessary nogoods larger size constraints,compared previous class problems, makes likely larger problemsrequired see asymptotic scaling behavior. However, least asymptoticbehaviors observed (Crawford & Auton, 1993) persist quite accurately evenproblems small n = 3, indication scaling behaviorquestion small problems considered here.4.2.1 Phase Transitionbehavior algorithm function ratio clauses variables shownFig. 11 using phase inversion method. shows phase transition behavior.Comparing Fig. 5, also shows class random 3SAT problems harder,average, quantum algorithm class unstructured problems.14. differs slightly studies random 3SAT allowing duplicate clauses propositional formula.15. values c=n small problems examined here, enough soluble instances randomlygenerated need rely prespecified solution eciently find soluble test problems.118fiQuantum Computing Phase Transitions Combinatorial Search<T>70605040302010246c/n8Figure 11: Average number tries find solution quantum search algorithmrandom 3SAT function c=n, using phase inversion method.curves correspond n = 5 (black) n = 10 (gray).0.2p(soln)p(soln)0.20.10.050.020.010.10.050.025670.018n910115678910 11nFigure 12: Scaling probability find solution, using phase inversion method,function number variables random 3SAT problems. curvescorrespond different clause variable ratios: 2 (dashed), 4 (solid), 6 (gray)8 (gray, dashed). shown log log-log scales (left rightplots, respectively).4.2.2 Scalingscaling probability find solution shown Fig. 12 using phase inversion method. limited experiments random phase method showedbehavior seen unstructured class problems: somewhat worse performancesimilar scaling behavior. results less clear-cut Fig. 8. c=n = 2results consistent either polynomial exponential scaling. problemsconstraints, exponential scaling somewhat better fit.addition general scaling trend, also noticeable difference behaviorcases even odd number variables. due behavioramplitude step lattice. Instead monotonic decrease concentrationamplitude goods, oscillatory behavior amplitude alternates119fiHoggprobability10.950.90.8534681012levelFigure 13: Probability goods (i.e., consistent sets) function level lattice3SAT problems constraints. shows behavior n equal 9(gray dashed), 10 (black dashed), 11 (gray) 12 (black). problem,final probability level n probability solution obtainedquantum algorithm.dispersing focused goods different levels. extreme examplebehavior shown Fig.fi 13fifor 3SAT problems constraints, i.e., c = 0. Specifically,2Plevel shows fifi s(i)fifi sum sets level latticeconsistent, which, problems constraints, assignments variables.probability good would found algorithm terminatedlevel gives indication well algorithm concentrates amplitude amongconsistent states. case, expanded search space quantum algorithm resultsslightly worse performance random selection among complete assignments(all solutions case). search starts amplitude goodslevel 3. total probability goods alternately decreases increasesmap proceeds solution level. Cases even number variables (the blackcurves figure) end step decreases probability goods, resultingrelatively lower performance compared odd variable cases (gray curves). Althoughmight suggest improvement even n cases starting level 2 ratherlevel 3, fact turns case: starting level 2 gives essentiallybehavior upper levels starting search level 3 lattice dueone oscillation intermediate levels takes 2 steps complete. Increasing valuec=n, i.e., examining SAT problems constraints, reduces extent oscillations,particularly higher levels lattice, eventually results monotonic decreaseprobability search moves lattice. Nevertheless, problemsconstraints existence oscillations gives rise observed difference behaviorcases even odd number variables. oscillations also seenunderconstrained cases unstructured problems Figs. 7 8.Fig. 13 shows oscillatory behavior decreases larger problems, alsosuggests may appropriate choices phases. Specifically, maypossible obtain greater concentration amplitude solutions allowing120fiQuantum Computing Phase Transitions Combinatorial Search3ratio102101105678n91011Figure 14: Scaling ratio probability find solution using quantumalgorithm probability find solution random selection solutionlevel function number variables random 3SAT problemsclause variable ratio equal 4. solid dashed curves correspondusing phase inversion random phase methods, respectively.black curves compare random selection among complete sets, graycompare selection among complete assignments. curvesclose linear log scale indicating exponential improvementdirect selection among complete sets.dispersion nogoods intermediate levels lattice using initial conditionamplitude nogoods. so, would represent new policy selectingphases takes account problem-independent structure necessary nogoods.would somewhat analogous focusing light lens: paths many directionsmodified lens cause convergence single point.definite results obtained improvement random selection. Specifically, Fig. 14 shows exponential improvement phase inversion randomphase methods, corresponding behavior unstructured problems Fig. 9. Similarimprovement seen values c=n well: Fig. 9 highly constrainedproblems give larger improvements. stringent comparison random selectionamong complete assignments (i.e., variable given single value) ratheramong complete sets variable-value pairs. also shown Fig. 14, appearinggrow exponentially well. particularly significant quantum algorithmuses larger search space containing necessary nogoods. Another view comparison given Fig. 15, showing probabilities find solution quantumsearch random selection among complete assignments. concluderesults additional structure necessary nogoods constraints different sizesqualitatively similar unstructured random problems detailed comparisonscaling behaviors requires examining larger problem sizes.121fiHogg0.2p(soln)0.10.050.020.010.0054576891011nFigure 15: Comparison scaling probability find solution quantum algorithm using phase inversion method (solid curve) random selectionamong complete assignments (gray curve) c=n = 4.5. Discussionsummary, introduced quantum search algorithm evaluated averagebehavior range small search problems. appears increase amplitudesolution states exponentially compared evaluating measuring quantum superposition potential solutions directly. Moreover, method exhibits transitionbehavior, associated concentration hard problems, seen many classicalsearch methods. thus extends range methods phenomenon applies.importantly, indicates algorithm effectively exploiting structuresearch problems as, say, classical backtrack methods, prune unproductive search directions. thus major improvement simple applications quantum computingsearch problems behave essentially classical generate-and-test, methodcompletely ignores possibility pruning hence doesn't exhibit phase transition.transition behavior readily understood problems near transition pointmany large partial goods lead solutions (Williams & Hogg, 1994). Thusrelatively high proportion paths lattice appear goodquite eventually give deadends. choice phases based detecting nogoodsable work paths near solution level hence give lesschance cancel move amplitude paths fact lead solutions.Hence problems many large partial goods likely prove relatively dicultquantum algorithms operate distinguishing goods nogoods various sizes.remain many open questions. algorithm, division problem{independent mapping lattice simple problem-specific adjustmentphases allows range policies selecting phases. would useful understand effect different policies hope improving concentration amplitudesolutions. example, use phases two distinct jobs: first, keep amplitude moving along good sets rather diffusing nogoods, second,122fiQuantum Computing Phase Transitions Combinatorial Searchdeadend reached (i.e., good set good supersets) send amplitudedeadend promising region search space, possibly fardeadend occurred. goals, keeping amplitude concentrated one handsending away other, extent contradictory. Thus may proveworthwhile consider different phase choice policies two situations. Furthermore,mapping lattice motivated classical methods incrementally buildsolutions moving sets supersets lattice. Instead using unitary mapsstep close possible classical behavior, approaches couldallow significant spreading amplitude intermediate levels latticeconcentrate solutions last steps. may prove fruitful consider another type mapping based local repair methods moving among neighbors completesets. case, sets evaluated based number constraints violateappropriate phase selection policy could depend number, rather whetherset inconsistent not. possibilities may also suggest new probabilistic classicalalgorithms might competitive existing heuristic search methods.new example search method exhibiting transition behavior, workraises issues prior studies phenomenon. instance, extentbehavior apply realistic classes problems, clusteringinherent situations involving localized interactions (Hogg, 1996). dicultcheck empirically due limitation small problems feasible classicalsimulation algorithm. However observation behavior persists manyclasses problems search methods suggests widely applicable.also interest see phase transition phenomena appear quantum searchalgorithms, observed optimization searches (Cheeseman et al., 1991; Pemberton& Zhang, 1996; Zhang & Korf, 1996; Gent & Walsh, 1995). may also transitionsunique quantum algorithms, example required coherence time sensitivityenvironmental noise.specific instances algorithm presented here, also remainingissues. important one cost mapping one level next termsbasic operations might realized hardware, although simple structurematrices involved suggest costly. scaling behavioralgorithm larger cases also interest, perhaps approached examiningasymptotic nature matrix coecients Eqs. 17 19.important practical question physical implementation quantum computers general (Barenco et al., 1995; Sleator & Weinfurter, 1995; Cirac & Zoller, 1995),requirements imposed algorithm described here. implementationquantum computer need deal two important diculties (Landauer, 1994).First, defects construction device. Thus even ideal designexactly produces desired mapping, occasional manufacturing defects environmentalnoise introduce errors. thus need understand sensitivity algorithm'sbehavior errors mappings. main diculty likely problemindependent mapping one level lattice next, since choice phasesproblem-specific part doesn't require high precision. context notestandard error correction methods cannot used quantum computers lightrequirement operations reversible. also need address extent123fiHoggerrors minimized first place, thus placing less severe requirementsalgorithm. Particularly relevant respect possibility drastically reducing defects manufactured devices atomically precise control hardware (Drexler, 1992;Eigler & Schweizer, 1990; Muller et al., 1995; Shen, Wang, Abeln, Tucker, Lyding, Avouris,& Walkup, 1995). also uniquely quantum mechanical approaches controllingerrors (Berthiaume, Deutsch, & Jozsa, 1994) based partial measurements state.work could substantially extend range ideal quantum algorithmspossible implement.second major diculty constructing quantum computers maintaining coherence superposition states long enough complete computation. Environmentalnoise gradually couples state device, reducing coherence eventuallylimiting time superposition perform useful computations (Unruh, 1995;Chuang, La amme, Shor, & Zurek, 1995). effect, coupling environmentviewed performing measurement quantum system, destroying superpositionstates. problem particularly severe proposed universal quantum computersneed maintain superpositions arbitrarily long times. method presentedhere, number steps known advance could implemented special purpose search device (for problems given size) rather program runninguniversal computer. Thus given achievable coherence time would translate limitfeasible problem size. extent limit made larger feasiblealternative classical search methods, quantum search could useful.open question greatest theoretical interest whether algorithm simplevariants concentrate amplitude solutions suciently give polynomial,rather exponential, decrease probability find solution NP searchproblem small constraints. especially interesting since class problemsincludes many well-studied NP-complete problems graph coloring propositionalsatisfiability. Even worst case, may averageclasses otherwise dicult real-world problems. means clearextent quantum coherence provides powerful computational behavior classicalmachines, recent proposal rapid factoring (Shor, 1994) encouraging indicationcapabilities.subtle question along lines average scaling behaves awaytransition region hard problems. particular, quantum algorithms expandrange polynomially scaling problems seen highly underconstrained overconstrained instances? so, would provide class problems intermediate dicultyquantum search exponentially faster classical methods, average.highlights importance broadening theoretical discussions quantum algorithmsinclude typical average behaviors addition worst case analyses. generally,differences phase transition behaviors location comparedusual classical methods? questions, involving precise location transitionpoints, currently well understood even classical search algorithms. Thus comparison behavior quantum algorithm may help shed light naturevarious phase transitions seem associated intrinsic structuresearch problems rather specific search algorithms.124fiQuantum Computing Phase Transitions Combinatorial SearchAcknowledgementsthank John Gilbert, John Lamping Steve Vavasis suggestions comments work. also benefited discussions Peter Cheeseman, ScottClearwater, Bernardo Huberman, Kimber, Colin Williams, Andrew Yao MichaelYoussefmir.ReferencesBarenco, A., Deutsch, D., & Ekert, A. (1995). Conditional quantum dynamics logicgates. Physical Review Letters, 74, 4083{4086.Benioff, P. (1982). Quantum mechanical hamiltonian models Turing machines. J. Stat.Phys., 29, 515{546.Bennett, C. H. (1992). Quantum cryptography: Uncertainty service privacy.Science, 257, 752{753.Bernstein, E., & Vazirani, U. (1993). Quantum complexity theory. Proc. 25th ACMSymp. Theory Computation, pp. 11{20.Berthiaume, A., Deutsch, D., & Jozsa, R. (1994). stabilization quantum computations. Proc. Workshop Physics Computation (PhysComp94), pp.60{62 Los Alamitos, CA. IEEE Press.Cerny, V. (1993). Quantum computers intractable (NP-complete) computing problems.Physical Review A, 48, 116{119.Cheeseman, P., Kanefsky, B., & Taylor, W. M. (1991). really hard problemsare. Mylopoulos, J., & Reiter, R. (Eds.), Proceedings IJCAI91, pp. 331{337 SanMateo, CA. Morgan Kaufmann.Chuang, I. L., La amme, R., Shor, P. W., & Zurek, W. H. (1995). Quantum computers,factoring decoherence. Science, 270, 1633{1635.Cirac, J. I., & Zoller, P. (1995). Quantum computations cold trapped ions. PhysicalReview Letters, 74, 4091{4094.Crawford, J. M., & Auton, L. D. (1993). Experimental results cross-over pointsatisfiability problems. Proc. 11th Natl. Conf. Artificial Intelligence(AAAI93), pp. 21{27 Menlo Park, CA. AAAI Press.Deutsch, D. (1985). Quantum theory, Church-Turing principle universal quantum computer. Proc. R. Soc. London A, 400, 97{117.Deutsch, D. (1989). Quantum computational networks. Proc. R. Soc. Lond., A425, 73{90.Dirac, P. A. M. (1958). Principles Quantum Mechanics (4th edition). Oxford.DiVincenzo, D. P. (1995). Quantum computation. Science, 270, 255{261.125fiHoggDiVincenzo, D. P., & Smolin, J. (1994). Results two-bit gate design quantum computers. Proc. Workshop Physics Computation (PhysComp94), pp.14{23 Los Alamitos, CA. IEEE Press.Drexler, K. E. (1992). Nanosystems: Molecular Machinery, Manufacturing, Computation. John Wiley, NY.Eigler, D. M., & Schweizer, E. K. (1990). Positioning single atoms scanning tunnellingmicroscope. Nature, 344, 524{526.Ekert, A., & Jozsa, R. (1995). Shor's quantum algorithm factorising numbers. Rev.Mod. Phys. appear.Feynman, R. P. (1986). Quantum mechanical computers. Foundations Physics, 16,507{531.Feynman, R. P. (1985). QED: Strange Theory Light Matter. Princeton Univ.Press, NJ.Freuder, E. C., & Wallace, R. J. (1992). Partial constraint satisfaction. Artificial Intelligence, 58, 21{70.Garey, M. R., & Johnson, D. S. (1979). Guide Theory NP-Completeness. W.H. Freeman, San Francisco.Gent, I. P., & Walsh, T. (1995). TSP phase transition. Tech. rep. 95-178, Dept.Computer Science, Univ. Strathclyde.Golub, G. H., & Loan, C. F. V. (1983). Matrix Computations. John Hopkins UniversityPress, Baltimore, MD.Hogg, T. (1994). Phase transitions constraint satisfaction search. World Wide Webpage URL ftp://parcftp.xerox.com/pub/dynamics/constraints.html.Hogg, T. (1996). Refining phase transitions combinatorial search. Artificial Intelligence, 81, 127{154.Hogg, T., Huberman, B. A., & Williams, C. (1996). Phase transitions searchproblem. Artificial Intelligence, 81, 1{15.Hogg, T., & Williams, C. P. (1994). hardest constraint problems: double phasetransition. Artificial Intelligence, 69, 359{377.Jozsa, R. (1992). Computation quantum superposition. Proc. PhysicsComputation Workshop. IEEE Computer Society.Karlin, S., & Taylor, H. M. (1975). First Course Stochastic Processes (2nd edition).Academic Press, NY.Kimber, D. (1992). introduction quantum computation. Tech. rep., Xerox PARC.126fiQuantum Computing Phase Transitions Combinatorial SearchKirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization simulated annealing.Science, 220, 671{680.Landauer, R. (1991). Information physical. Physics Today, 44 (5), 23{29.Landauer, R. (1994). quantum mechanically coherent computation useful? Feng, D. H.,& Hu, B.-L. (Eds.), Proc. Drexel-4 Symposium Quantum Nonintegrability.International Press.Lloyd, S. (1993). potentially realizable quantum computer. Science, 261, 1569{1571.Lloyd, S. (1995). Quantum-mechanical computers. Scientific American, 273 (4), 140{145.Mackworth, A. (1992). Constraint satisfaction. Shapiro, S. (Ed.), Encyclopedia Artificial Intelligence, pp. 285{293. Wiley.Margolus, N. (1990). Parallel quantum computation. Zurek, W. H. (Ed.), Complexity,Entropy Physics Information, pp. 273{287. Addison-Wesley, New York.Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing con icts:heuristic repair method constraint satisfaction scheduling problems. ArtificialIntelligence, 58, 161{205.Mitchell, D., Selman, B., & Levesque, H. (1992). Hard easy distributions SATproblems. Proc. 10th Natl. Conf. Artificial Intelligence (AAAI92), pp.459{465 Menlo Park. AAAI Press.Motwani, R., & Raghavan, P. (1995). Randomized Algorithms. Cambridge University Press.Muller, W. T., Klein, D. L., Lee, T., Clarke, J., McEuen, P. L., & Schultz, P. G. (1995).strategy chemical synthesis nanostructures. Science, 268, 272{273.Pemberton, J. C., & Zhang, W. (1996). Epsilon-transformation: Exploiting phase transitions solve combinatorial optimization problems. Artificial Intelligence, 81, 297{325.Prosser, P. (1996). empirical study phase transitions binary constraint satisfactionproblems. Artificial Intelligence, 81, 81{109.Schrag, R., & Crawford, J. (1996). Implicates prime implicates random 3-SAT.Artificial Intelligence, 81, 199{222.Selman, B., Levesque, H., & Mitchell, D. (1992). new method solving hard satisfiabilityproblems. Proc. 10th Natl. Conf. Artificial Intelligence (AAAI92), pp.440{446 Menlo Park, CA. AAAI Press.Shen, T. C., Wang, C., Abeln, G. C., Tucker, J. R., Lyding, J. W., Avouris, P., & Walkup,R. E. (1995). Atomic-scale desorption electronic vibrational excitationmechanisms. Science, 268, 1590{1592.127fiHoggShor, P. W. (1994). Algorithms quantum computation: Discrete logarithms factoring. Goldwasser, S. (Ed.), Proc. 35th Symposium FoundationsComputer Science, pp. 124{134. IEEE Press.Sleator, T., & Weinfurter, H. (1995). Realizable universal quantum logic gates. PhysicalReview Letters, 74, 4087{4090.Smith, B. M., & Dyer, M. E. (1996). Locating phase transition binary constraintsatisfaction problems. Artificial Intelligence, 81, 155{181.Svozil, K. (1995). Quantum computation complexity theory I. Bulletin EuropeanAssociation Theoretical Computer Sciences, 55, 170{207.Unruh, W. G. (1995). Maintaining coherence quantum computers. Physical Review A,41, 992.Williams, C. P., & Hogg, T. (1994). Exploiting deep structure constraint problems.Artificial Intelligence, 70, 73{117.Wolfram, S. (1991). Mathematica: System Mathematics Computer (2ndedition). Addison-Wesley, NY.Zhang, W., & Korf, R. E. (1996). unified view complexity transitions travellingsalesman problem. Artificial Intelligence, 81, 223{239.Zhu, L., Kleiman, V., Li, X., Lu, S. P., Trentelman, K., & Gordon, R. J. (1995). Coherentlaser control product distribution obtained photoexcitation HI. Science,270, 77{80.128fiJournal Artificial Intelligence Research 4 (1996) 287{339Submitted 1/96; published 5/96Planning Contingencies: Decision-based ApproachLouise Pryorlouisep@aisb.ed.ac.ukGregg Collinscollins@ils.nwu.eduDepartment Artificial Intelligence, University Edinburgh80 South BridgeEdinburgh EH1 1HN, ScotlandInstitute Learning Sciences, Northwestern University1890 Maple AvenueEvanston, IL 60201, USAAbstractfundamental assumption made classical AI planners uncertaintyworld: planner full knowledge conditions planexecuted outcome every action fully predictable. planners cannottherefore construct contingency plans, i.e., plans different actions performeddifferent circumstances. paper discuss issues arise representationconstruction contingency plans describe Cassandra, partial-order contingencyplanner. Cassandra uses explicit decision-steps enable agent executing plandecide plan branch follow. decision-steps plan result subgoals acquireknowledge, planned way subgoals. Cassandra thusdistinguishes process gathering information process making decisions.explicit representation decisions Cassandra allows coherent approachproblems contingent planning, provides solid base extensions usedifferent decision-making procedures.1. IntroductionMany plans use everyday lives specify ways coping various problemsmight arise execution. words, incorporate contingency plans .contingencies involved plan often made explicit plan communicatedanother agent, e.g., \try taking Western Avenue, it's blocked use Ashland,"\crank lawnmower twice, still doesn't start jiggle spark plug." Socalled classical planners 1 cannot construct plans sort, due primarily reliancethree perfect knowledge assumptions:1. planner full knowledge initial conditions planexecuted, e.g., whether Western Avenue blocked;2. actions fully predictable outcomes, e.g., cranking lawnmower definitely either work work;1. category includes systems strips (Fikes & Nilsson, 1971), hacker (Sussman, 1975), noah(Sacerdoti, 1977) molgen (Stefik, 1981a, 1981b). Recent classical planners include tweak (Chapman, 1987), snlp (McAllester & Rosenblitt, 1991) ucpop (Penberthy & Weld, 1992). termdue Wilkins (1988).c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiPryor & Collins3. change world occurs actions performed planner, e.g., nobodyelse use car empty gas tank.assumptions world totally predictable ; need contingencyplans.perfect knowledge assumptions idealization planning context intended simplify planning process. allow development planning algorithmsprovable properties completeness correctness. Unfortunately,domains realistic: mostly, world extent unpredictable. Relying perfect knowledge assumptions unpredictable world may provecost-effective planner's uncertainty domain small, cost recovering failure low. general, however, may lead planner forgo optionswould available potential problems anticipated advance.example, assumption weather sunny, forecast, may neglecttake along umbrella; forecast later turns erroneous, impossibleuse umbrella stay dry. cost recovering failure high, failingprepare possible problems advance expensive mistake. order avoidmistakes sort, autonomous agent complex domain must able makeexecute contingency plans.Recently, number researchers begun investigating possibilityrelaxing perfect knowledge assumptions staying close frameworkclassical planning (Etzioni, Hanks, Weld, Draper, Lesh, & Williamson, 1992; Peot & Smith,1992; Pryor & Collins, 1993; Draper, Hanks, & Weld, 1994a; Goldman & Boddy, 1994a).work embodied Cassandra,2 contingency planner whose plans followingfeatures:include specific decision steps determine possible coursesaction pursue;Information gathering steps distinct decision-steps;circumstances possible perform action distinguishednecessary perform it.1.1 Issues Contingency Plannercontingency planner must able construct plans expected succeeddespite unknown initial conditions uncertain outcomes nondeterministic actions.effective contingency planner must possess following capabilities:must able anticipate outcomes nondeterministic actions;must able recognize uncertain outcome threatens achievementgoal;must able make contingency plans possible outcomes varioussources uncertainty affect given plan;2. Cassandra Trojan prophet fated believed accurately predicted futuredisasters. earlier version Cassandra described (Pryor & Collins, 1993).288fiPlanning Contingencies: Decision-based Approachmust able schedule sensing actions detect occurrence particularcontingency;must produce plans executed correctly regardless contingencyarises.design Cassandra addresses issues. However, several issuesaddressed:considered problem determining whether worth planningparticular outcome;Cassandra probabilistic planner: cannot make use informationlikelihood otherwise events;ignored possibility interleaving planning execution (but see Section 7.4);Cassandra handle exogenous events;version Cassandra described cannot solve Moore's bomb toiletproblem (McDermott, 1987): find plans involve decidingcourses action succeed different contingencies (but see Section 6.5.5).Cassandra assumes sources uncertainty possible outcomes known,plans affect achievement goals. firmly classicalplanning mold: job construct plans guaranteed achieve goals.decide plan, plan for. Moreover, although believe Cassandrasound complete, systematic. addition, current implementationslow practical use.1.2 Note Terminologyword conditional used variety senses literature. avoid usealtogether, except describing work authors use specializedsenses: example, conditional actions conditioning Peot Smith (1992).use term contingency plan refer plan contains actions may mayactually executed, depending circumstances hold time. useterm context-dependent refer action effects depend contextaction performed.1.3 Outlinepaper present Cassandra, describe algorithm detail, discussapproach takes important issues contingency planning, show handlesvariety example problems.start describing structure Cassandra's plans. Section 2 describesCassandra represents actions, including uncertain outcomes; explains system289fiPryor & Collinslabels allows determination alternative courses actioncontingency plan pursued; introduces notion explicit decision steps.Section 3 brie describes basic planning algorithm absence uncertainty.Section 4 explains algorithm extended handle uncertain outcomes actions.particular, structure Cassandra's decisions considered, problemsensuring soundness plan constructed. resulting algorithm describeddetail properties discussed Section 5.Section 6 consider issues arise contingency planning. Section 7describes related work planning uncertainty. Finally, Section 8 summarizescontributions work discusses limitations.2. Cassandra's Plan RepresentationCassandra's representation contingency plans three major components:action representation supports uncertain outcomes;plan schema;system labels keeping track elements plan relevantcontingencies.components described remainder section.2.1 Action RepresentationCassandra's action representation modified form strips operator (Fikes & Nilsson, 1971). consists preconditions executing action effects maybecome true result executing it, standard strips operator. syntaxused ucpop (Penberthy & Weld, 1992). ucpop, action effectscomplex standard strips effects: may associated set secondary preconditions , govern occurrence effect (Pednault, 1988, 1991).Secondary preconditions allow representation context-dependent effects actions,i.e., effects depend upon context action executed. use secondary preconditions critical Cassandra's ability represent uncertain effects,hence nondeterministic actions, discuss Section 2.1.1.Figure 1 shows simplified operator schema action making selectionsoft-drink machine (the effects describing \make another selection" indicator lightturned omitted). operator describes two possible effects carryingaction: effect acquiring soda, depends secondary preconditionsoda selected type available; effect \make another selection"indicator light come on, depends secondary precondition sodaselected type available. effects depend upon preconditions moneyentered machine machine plugged in.2.1.1 Representing Uncertain Effectsuncertain effect Cassandra context-dependent effect unknown precondition , i.e., precondition planner neither knowingly perceive deliberately affect.290fiPlanning Contingencies: Decision-based ApproachAction:(make-selection ?machine ?selection)Preconditions:Effects:(:and (money-entered ?machine)(plugged-in ?machine))(:when(available ?machine ?selection):effect (:and (dispensed ?selection)(:not (money-entered ?machine))))(:when(:not (available ?machine ?selection)):effect (another-selection-indicator-on ?machine))secondary preconditionsecondary preconditionFigure 1: Simplified representation operating vending machineexample, malfunctioning soft-drink machine may operate intermittently; planner aware intermittent functioning, unaware conditions governbehavior, correct functioning device depends upon unknown precondition. point view planner, uncertain effect nondeterministic;planner cannot tell advance whether occur. Clearly, definition fundamentally subjective: another planner better information might able specifyprecisely conditions device functions properly, example knewinternal mechanism machine worked. another example, considerhappens coin tossed: principle, given perfect knowledge forcesdistances involved, would possible predict outcome. practice, knowledgeunavailable effect action uncertain. principle, would possiblespecify conditions would lead coin landing tails up; practice,conditions unknown.interesting note circumstances might possible plannerlearn predict outcomes hitherto regarded uncertain: example,learned soda machine worked. \Unknown" refers current situation.representation would facilitate learning, would simply involve learning newsecondary preconditions rather whole new action representation.Unknown preconditions play syntactic role normal preconditions withinoperator schema; represented expressions formed using pseudo-predicate:unknown. effect secondary precondition type occurcertain contexts cannot distinguished planner contextsoccur.Figure 2 depicts simplified example operator uncertain effect|it represents action operating soft-drink machine intermittently fails dispensesoda despite operated correctly. operator two uncertain effects, onesoda dispensed, soda dispensed.Clearly, uncertainty respect effects stems single underlying source, namely uncertainty whether machine malfunction.effect, two unknown preconditions operator represent alternative resultsunderlying source uncertainty. relationship ected two arguments291fiPryor & CollinsAction:Preconditions:Effects:(enter-selection ?machine)(:and (money-entered ?machine)(plugged-in ?machine))(:when (:and (available ?machine ?selection)(:unknown ?ok T)):effect (dispensed ?selection))(:when (:and (available ?machine ?selection)(:unknown ?ok F)):effect (:not (dispensed ?selection)))(:when (available ?machine ?selection):effect (:not (money-entered ?machine)))(:when (:not (available ?machine ?selection)):effect (another-selection-indicator-on ?machine))uncertain effectuncertain effectFigure 2: Operating faulty soft-drink machine:unknown pseudo-predicate, first designates source uncertaintyassociated, second designates particular outcomeuncertainty represents. possible contexts effectively partitioned setequivalence classes, context class producing outcomeuncertainty. outcome used label equivalence class. conditionform (:unknown ?class outcome) true actual context classdesignated outcome.Notice instantiation operator introduce new source uncertainty,means first argument unknown precondition must representedvariable operator schema. Cassandra binds variable unique identifier (i.e.,skolem constant) operator instantiated.Cassandra's representation assumed different sources uncertaintyindependent other. source uncertainty linked uncertain outcomesone operator, single operator may introduce number sourcesuncertainty, may number outcomes. source uncertaintyexhaustive set mutually exclusive outcomes, unique name.2.1.2 Representing Sources Uncertaintykey element Cassandra's design use single format represent sourcesuncertainty affect planning. particular, uncertainty assumed manifestuncertain effects planning operators, outlined above. Uncertainty initialconditions handled within format treating initial conditions thougheffects phantom \start step" action. treatment initial conditions,initially developed reasons unrelated problem representing uncertainoutcomes, common snlp family planners Cassandra belongs.Cassandra's formulation ignores uncertainty might stem outside interferenceexecution agent's plans, except inasmuch represented292fiPlanning Contingencies: Decision-based Approachincomplete knowledge initial conditions. is, course, limitation classicalplanners general; change world assumed caused directly actionsagent.2.2 Basic Plan RepresentationCassandra's plan representation extension used ucpop (Penberthy & Weld,1992) snlp (McAllester & Rosenblitt, 1991; Barrett, Soderland, & Weld, 1991),turn derived representation used nonlin (Tate, 1977). plan representedschema following components:set steps ;set anticipated effects steps;set links relating effects steps produce consume (a stepconsumes effect requires effect achieve one preconditions).Note links effect denote protection intervals , i.e., intervals particularconditions must remain true order plan work properly.set variable bindings instantiating operator schema;partial ordering steps;set open conditions , i.e., unestablished goals;set unsafe links , i.e., links conditions could falsifiedeffects plan.plan complete contains open conditions unsafe links.2.3 Representing Contingenciescontingency plan intended achieve goal regardless foreseeablecontingencies associated actually arise execution. construct valid contingency plan, planner must able enumerate contingencies. setforeseeable contingencies computed sources uncertainty associated plan. effect, contingency one possible set outcomes relevantsources uncertainty.2.3.1 Contingency LabelsKeeping track whether plan achieves goal every contingency somewhatcomplex process. Cassandra, like cnlp, uses system labels accomplish necessarybookkeeping (Peot & Smith, 1992). goal, step, effect Cassandra's plan labeledindicate contingencies element participates:Goals labeled indicate contingencies must achieved;Effects labeled indicate contingencies expected occur,i.e., contingencies goals satisfy arise;293fiPryor & CollinsSteps labeled indicate contingencies must performed, i.e.,union contingencies effects expected occur.preconditions effect become new goals, labels correspondlabels effect give rise them.general, assumed particular step could executed contingency,albeit possibly purpose. However, sometimes necessary rule particular stepparticular contingency means preventing interference plancontingency. example, consider plan achieve goal coin headsup, first action toss coin (see Section 4.2.3 detailed discussionplan). one contingency coin lands heads up, actions required.another contingency, coin lands tails must turned order goalachieved. clear, however, turning action must performedfirst contingency: would mean goal coin headsachieved. Cassandra, ruling steps accomplished associating negative labelsplan steps indicate contingencies steps executed.Peot Smith (1992) call process conditioning .addition, every step depends, directly indirectly, particular outcomegiven source uncertainty ruled every contingency involves alternativeoutcome source uncertainty. discuss reason restrictiondetail below.Cassandra's labeling system thus provides clear guidance agent executingplan, simply performs steps whose positive labels ect actual circumstances hold execution. Steps neither positive negative labels involvingcurrent contingency affect goals, guaranteed executable.contrast, agent executing plan produced cnlp guided reason labels attached steps. cnlp's plans, action need executed least one goalsrepresented reason labels feasible. agent must therefore methoddeciding top-level goals feasible. assume done comparingcontext labels top-level goal (which labeled representeddummy actions) circumstances actually hold. Cassandra's method thussimpler: agent simply uses positive labels plan steps instead usinglabels attached step indicate goals whose context labels must analyzed.general principles label propagation Cassandra are:Positive labels, denote plan element concerned contributes goalachievement contingency, propagate along causal links subgoalsplan elements establish them;Negative labels, denote plan element concerned would prevent goalachievement contingency, propagate along causal links effects planelements establish.details given Section 5.1.4.294fiPlanning Contingencies: Decision-based ApproachKEYLinkWesternDriveWesternBelmontWestern ChecktrafficWesternTake WesternEvanstonknowtrafficstatusconditionAlternativecontrol flowDecideTake BelmontAshlandBelmontAshlandTake AshlandEvanstonFigure 3: plan includes decision-step2.3.2 Representing DecisionsPlanning seen process deciding advance done(Collins, 1987). need contingency plans arises necessary decisions cannotmade advance missing information (see Section 6.4). decisions cannotmade advance, must made plan executed. agent executingcontingency plan must point decide possible courses actionpursue, words branch take.Previous work effect assumed agent execute stepsconsistent contingency actually holds (Warren, 1976; Peot & Smith, 1992).However, determination steps consistent cannot (by definition) madeadvance; order know contingency holds execution, agent executingplan must general gather information decision based. ensureviable plan, planner must able guarantee steps required gatherinformation con ict required carry rest plan. Therefore,planner must general able include information gathering steps, wellsteps support decision making, plan constructing. Cassandra achievesrepresenting decisions explicitly plan steps. preconditions decisionsteps include goals possession information relevant making decision;scheduling actions obtain information thus handled normal planning process.instance, consider contingency plan alluded above: \try taking Western Avenue, it's blocked use Ashland ." execution plan, agent mustpoint decide branch plan execute. decision-step casewould precondition knowing whether Western Avenue blocked not,would cause planner schedule information-gathering action check tracstatus Western. operation might turn precondition Western, achieved traveling junction Western Belmont.decision taken, agent either take Western Evanston continue alongBelmont Ashland.Assuming goal plan Evanston, final plan might depictedFigure 3. Note control ow decision represented heavy lines. Solid linesdiagram represent links, action tail link achieving precondition295fiPryor & Collinsaction head link. plan, agent take Western Evanstonone contingency, take Belmont Ashland Ashland Evanstonother.3Notice order determine appropriate precondition given decision-step,planner must way determining exactly need know ordermake decision execution time. somewhat complex determination dependspart decision-making process carried out. Cassandra, decisionsmodeled evaluation set condition-action rules form:condition 1 contingency 1condition 2 contingency 2...condition n contingency npossible outcome given uncertainty gives rise decision rule; conditiondecision-rule specifies set effects agent test order determinewhether execute contingency plan outcome. example, decision-rulesdriving plan example would look like this:Western Avenue blockedexecute contingency using AshlandWestern Avenue blocked execute contingency using WesternCassandra's derivation inference rules decisions explained detail Section 4.preconditions decision-step goals know truth values conditionsdecision-rules: thus knowledge goals (McCarthy & Hayes, 1969; Pryor, 1995)(see Section 6.4). goals treated way preconditionsstep. Cassandra thus requires special provisions allow constructioninformation-gathering plans.explicit representation decision-steps provides basis supporting alternativedecision procedures. Cassandra's basic model decision procedure quitesimple, complex decision procedures supported within framework (oneprocedure described Section 6.5.5). example, model could changeddifferential-diagnosis procedure. representation decision procedures templatesway actions represented templates would allow planner choosealternative methods making decision way choosealternative methods achieving subgoal. even better approach might formulateexplicit goal make correct decision, allow system construct planachieve goal using inferential operators. However, would effect requiregoals operators stated meta-language describing preconditionsresults operators. yet addressed possibility detail.Cassandra's separation gathering information making decisionsallows one information-gathering step serve several decisions. allows exible useinformation-gathering actions; effective difference actionsaction may appear plan.3. Appendix shows plans Cassandra constructs examples described paper.plan Section A.1.296fiPlanning Contingencies: Decision-based ApproachNew step Add plan new step effect establish open condition. Addstep preconditions secondary preconditions effect open conditions. open conditionbecomes completed link.Reuse step Make open condition complete link effect existing plan step. Addsecondary preconditions effect open conditions.Figure 4: Resolving open conditions3. Planning Without Contingenciessection brie review basic planning algorithm Cassandra based.follows closely used ucpop (Penberthy & Weld, 1992), turn basedsnlp (McAllester & Rosenblitt, 1991). principal difference ucpop snlpuse secondary preconditions (see Collins & Pryor, 1992).Cassandra attempt construct contingency plan encountersuncertainty. point, constructs plan much mannerplanners snlp family. fact, uncertainty ever introduced plan,Cassandra effectively function ucpop would circumstances.Planning proceeds alternation two processes: resolving open conditionsprotecting unsafe links . processes involves choice methods, maytherefore give rise several alternative ways extend current plan. possibleextensions constructed, best-first search algorithm guides planner's explorationspace partial plans.initial plan consists two steps: start step, preconditionsinitial conditions effects, goal step, goal conditions preconditionseffects. planner attempts modify initial plan complete :i.e., open conditions unsafe links.3.1 Resolving Open Conditionsplanning process driven need satisfy open conditions, initiallysimply input goals. course planning satisfy open condition, new subgoalsmay generated; added set open conditions. plannerestablish open condition one two ways: introducing new step plan,reusing existing step making use one effects (see Figure 4). secondarypreconditions effect establishes condition become open conditions. newstep added, preconditions step become open conditions well. Finally,time open condition established, link added plan protect newlyestablished condition.One way establishing condition simply notice condition trueinitial state. initial conditions treated results start operator,always part plan, method treated establishment reusingexisting step; indeed, simplification motivation representing initialconditions way.297fiPryor & Collinslink establishing condition Cond unsafe effect Eff plan (other effectSourceEff establishes Cond (possible) effect GoalEff either established disabledlink) following properties:Unification One postconditions Eff possibly unify either Cond negation;Ordering step produces Eff can, according partial order, occur stepproduces GoalEff step produces SourceEff.unsafe link may resolved one three ways:Ordering Modify ordering steps plan ensure step producing Eff occurs eitherstep produces SourceEff step produces GoalEff;Separation Modify variable bindings plan ensure threatening effect Eff cannot factunify threatened condition Cond;Preservation Introduce new open condition plan disable Eff. new open conditionnegation one Eff's secondary preconditions.Figure 5: Unsafe links3.2 Protecting Unsafe LinksWhenever open condition established, links plan may jeopardized eithernew step threatens existing link, new link threatenedexisting step. situations link unsafe shown Figure 5. general,link considered unsafe effect plan could possibly interferecondition established link.three general methods protecting threatened link (see Figure 5). First,ordering used constrain threatening action occur either beginningend threatened link. Second, threatening effect threatenedlink separated imposing constraints variables involved effectcannot unified established condition. Third, link preservedgenerating new subgoal disable effect threatens link.4. Contingency PlanningCassandra proceeds described previous section either plan completeduncertainty introduced. section describes uncertainties introducedhandled.example plan involving uncertainty, let us consider version Moore'sclassic \bomb toilet" problem (McDermott, 1987), goal bombdisarmed , initial conditions bomb package1 bomb package2 .uncertainty case lies initial conditions: depending outcomeuncertainty, start operator either effect bomb package1effect bomb package2 .4.1 ContingenciesUncertainty introduced plan open condition plan achieveduncertain effect, i.e., effect unknown precondition. bomb-in-the-toilet298fiPlanning Contingencies: Decision-based ApproachKEYMovepackage1Packatoi geletStartLinkconditionLinkuncertaineffectconditionDunkpackage1Bombpackage1BombdisarmedEndFigure 6: introduction uncertainty planexample, instance, Cassandra may achieve condition bomb disarmed selectingdunk operator, preconditions package toilet , bombpackage . condition bomb package established identifyingbomb package1 , effect start operator. However,condition uncertain, determined noting unknown precondition.Cassandra attempt deal uncertainty introducing new contingency (ornew contingencies) plan. state plan introductionuncertainty illustrated Figure 6.4.1.1 Introducing ContingenciesCassandra notices uncertainty current plan becomes dependent upon particular outcome uncertainty use uncertain effect, i.e., effectunknown precondition specifies outcome uncertainty. planCassandra built point effect plan branch outcome.Since branches must also constructed possible outcomes uncertainty,Cassandra makes copy overall goal possible outcome uncertainty,copy carrying label indicating outcome uncertainty mustachieved. thus effectively splits plan set branches, one possibleoutcome uncertainty.4planning otherwise identical goals, Cassandra must make certainelement branch goal one outcome relies different outcomeuncertainty. words, goal, subgoals, may achieved effectdepends, directly indirectly, outcome uncertainty onegoal's label. described above, Cassandra achieves using system negativelabels indicating contingencies particular plan elements must excluded.4. alternative method would split plan two branches, regardless number outcomes.case, one branch would associated given outcome uncertainty,would associated possible outcomes uncertainty. effectively senspoperates (Etzioni et al., 1992).299fiPryor & CollinsMovepackage1Bomb 1packageStartKEYPackagtoil e1etBombpackagine22ageack iletPMovepackage2IIDunkpackage1BombdisarmeLinkconditionLinkuncertaineffectconditionEndDunkpackage2Bomb eddisarmIIIIVElement label classespackage1 contingencyIIpackage1 contingencypackage2 contingencyIIIpackage2 contingencypackage1 contingencyIVpackage2 contingencyFigure 7: contingency plan disarm bombbomb-in-the-toilet example, plan made dependent upon uncertainoutcome bomb package1 , new copy top level goal bomb disarmed addedset open conditions. new copy given label indicating belongscontingency bomb package2 .5 existing top level goalsubgoals labeled indicate belong contingency bombpackage1 . effect bomb package1 , action dunk package1 , effectsaction dunk package1 labeled indicate cannot play rolecontingency bomb package2 .Notice action move package1 , although plays role plan contingency bomb package1 , fact depend upon bombpackage1 . could principle made part plan disarming bombcontingency bomb package2 , prove useful anything.indicated fact negative label package2 contingency.Cassandra attempts achieve new open condition bomb disarmed , maychoose dunk operator (notice prohibited using effectsexisting dunk operator). new instance dunk operator turn gives risesubgoal bomb package dunked. achievedidentification effect bomb package2 . plan thus constructed depictedFigure 7 (the decision-step omitted clarity) listed Section A.2.4.1.2 Uncertainties Multiple OutcomesAlthough algorithm described deal uncertainties numberpossible outcomes, far discussed examples two possible outcomes.fact, two-outcome uncertainties suce describe majority problems5. Note describing contingency way clarity exposition. actual labelconstructed described Section 2.3.1.300fiPlanning Contingencies: Decision-based ApproachPackage location1PickuppackageDrive ?carlocation1StartotRob tion1locaKEYLinkLinkuncertaineffectDecideconditionconditionAlternativecontrol flowBIncompleteportionplanFigure 8: partial plan pick packageconsidered. Indeed, technically, situation could described terms numbertwo-outcome uncertainties. However, hard think situations mightnaturally represented terms source uncertainty two outcomes.example, suppose planner interested getting hold particular objectsituation object known one three places. case,start pseudo-operator would naturally represented three uncertain effects(one possible location object) associated alternative outcomessingle source uncertainty. Cassandra's plan acquiring object would involvethree contingencies, one possible location.4.1.3 Multiple Sources Uncertaintyplan may involve two sources uncertainty, case planone set branches. example, suppose Cassandra given goal pickingpackage one two locations, one two cars availableuse. uncertainty regarding location package encountered firstconstruction plan, Cassandra respond building plan involvingtwo contingencies, one location. Call contingencies B (see Figure 8Section A.3).point construction plan contingency A, Cassandraencounter uncertainty concerning car available make currentplan dependent upon one particular outcome uncertainty. Since new sourceuncertainty arises context planning contingency A, contingency effectbifurcated two contingencies: A1 , package location 1 car 1301fiPryor & CollinsPackage location1car1 availableDrive car1location1Robotlocation1Package location1Startcar2 availableDrive car2location11DecideRobot n1ioloca2LinkuncertaineffectDecidePickuppackageKEYLinkPickuppackageconditionconditionAlternativecontrol flowBIncompleteportionplanFigure 9: plan two sources uncertaintyavailable; A2 , package location 1 car 2 available). Cassandramust replace existing contingency labels contingency A1 labels. mustintroduce new copy top-level goal labeled contingency A2 .Note Cassandra must plan scratch achieve top-level goal contingencyA2 , spite fact already viable plan goal contingency A1 .necessary situations may encountered successful plansinvolve using different methods achieve goal two contingencies. example,extreme differences two cars might necessitate different plans driving(e.g., detailed representation situation presented here,differences might affect routes cars could driven placescould parked). Cassandra must therefore consider possible ways achievegoal contingency A2 search completion plan. particular car usedfact affect driving plan, one path search space resultisomorphic contingency plans A1 A2 (see Figure 9 Section A.4).reasoning applies extension plan deal contingency B .cannot assumed priori plan contingency B way resembleplan constructed contingency A. interesting consequence302fiPlanning Contingencies: Decision-based Approachuncertainty concerning availability cars necessarily arise given plancontingency B . example, location package contingency B closeenough agent could get without using car, final plan mightthree contingencies: A1 (location 1 car 1), A2 (location 1 car 2), B (location2, foot).Cassandra may, course, produce extension plan car usedcontingency B well, case encounter uncertainty associatedlocation car, proceed bifurcate contingency B donepreviously contingency A. limit, plan involve one contingency everymember cross product possible outcomes relevant uncertainties. However,important note every member cross-product set must appearcontingency, since, shown, uncertainties may arise given particularoutcomes uncertainties.4.2 Decision-stepsCassandra encounters new source uncertainty adds decision-step planrepresent act determining path plan followedexecution. following ordering constraints added plan time:decision-step must occur step uncertainty associated;decision-step must occur step precondition whose achievementdepends particular outcome uncertainty.4.2.1 Formulating Decision-rulesdecision-step operational, must effective procedureagent executing plan determine decision make. Cassandra, actiondeciding contingency execute modeled evaluation set conditionaction rules form:condition 1 contingency 1condition 2 contingency 2condition 3 contingency 3...Cassandra annotates decision-step plan set rules usedmake decision. executing agent make decision evaluatingrules comes decision-step course executing plan. orderevaluate decision-rule, executing agent must able determine whether rule'santecedent holds. preconditions decision-step must thus include goals knowcurrent status condition appears antecedent rule condition.preconditions decision-step become open conditions plan waypreconditions step.intended effect evaluating decision-rules choose appropriate contingency given outcome particular uncertainty, conditions diagnosticparticular outcomes uncertainty. executing agent cannot, course, directly303fiPryor & Collinsdetermine outcome uncertainty, must infer presence absenceeffects depend upon outcome.straightforward approach constructing antecedent conditionsdecision-rule would analyze plan operators identify effects couldexpected result given outcome uncertainty, make conditionconjunction effects. However, turns overkill. fact,necessary check effects given outcome uncertainty actuallyused establish preconditions contingency associated outcome .words, necessary verify contingency plan can, fact, succeed.interesting consequence executing agent might, principle, end selectingcontingency plan even though outcome uncertainty oneplan associated. Notice would cause problem executionplan, since would occur conditions plan's success met.fact, shall see, Cassandra depends effect certain circumstances.antecedent condition decision-rule thus conjunction direct effectsparticular outcome used establish preconditions contingency planoutcome. Decision-rules constructed incrementally plan elaborated.discuss Cassandra's construction rules detail Section 4.2.3 below.approach used formulating Cassandra's decision-rules consistentMorgenstern's observation agent execute plan \make sure"events plan executable (Morgenstern, 1987).4.2.2 Adding Decision-rule Examplebomb-in-the-toilet example, Cassandra introduce decision-step determinewhether bomb package1 . uncertainty initial conditions,decision constrained occur start step. must also occureither dunk actions, since depend upon particular outcomes uncertainty.decide step precondition know whether bomb package1 .actions available would allow determine this|X-raying box,example|Cassandra achieve precondition one actions, decidebasis branch plan execute.4.2.3 Cassandra Constructs Decision-rulespoint planning process Cassandra constructs decision-rule,one precondition plan known depend upon particular outcome uncertainty gave rise decision: namely, one led Cassandra discoveringuncertainty first place. decision-rule set Cassandra initially builds thuslooks like this:effect 1 contingency 1contingency 2contingency 3...construction plan, Cassandra must modify initial rule set timeeffect depending directly source uncertainty used establish open condition304fiPlanning Contingencies: Decision-based ApproachAction:(toss-coin ?coin)Preconditions:(holding ?agent ?coin)Effects:(:when (:unknown ?U H):effect (:and (flat ?coin)(heads ?coin)))(:when (:unknown ?U T):effect (:and (flat ?coin)(tails ?coin)))(:when (:unknown ?U E):effect (on-edge ?coin)))uncertain effectuncertain effectuncertain effectFigure 10: Representing action tossing coinplan. particular, Cassandra must determine contingency opencondition resides, conjoin effect existing antecedent decision-rulecontingency.Consider, example, happens coin tossed. might say theorythree possible outcomes action: coin land heads up;tails up; edge (Figure 10). Suppose Cassandra given goalcoin at. established using at-heads effect tossing it. Sinceuncertain effect, Cassandra introduces two new contingencies plan, oneoutcome coin lands tails up, another outcome landsedge.introduction contingencies mandates introduction decision-stepwhose initial rule set looks like this:6(flat coin) [U1: H] rule heads[U1: T] rule tails[U1: E] rule edgetime, new open condition (know-if (flat coin)) introduced precondition decision-step, new goal conditions introduced must achievedcontingencies [U1: T] [U1: E]. Cassandra next establishes goal condition contingency [U1: T] using at-tails effect toss step. decision-rules associatedtails contingency thus modified follows:(flat coin) [U1: H] rule heads(flat coin) [U1: T] rule tails[U1: E] rule edgeFinally, goal condition established contingency [U1:E] introducing newstep, tip, plan. precondition tip step coin edge,established on-edge effect toss action. Since effect depends directly6. Assuming ?U, variable representing source uncertainty, instantiated U1.305fiPryor & Collinsupon uncertainty U1, decision-rule edge contingency modified includecondition:(flat coin)[U1: H] rule heads(flat coin)[U1: T] rule tails(on-edge coin) [U1: E] rule edgeSince plan complete, final set decision-rules (see Section A.5). Noticerules discriminate heads-up outcome tails-up outcome.fact, either outcome do, reason make discrimination. planexecuted either conditions depends solely upon order agentexecuting plan chooses evaluate decision-rules.7somewhat complex problem arises give Cassandra goalcoin flat heads-up. case effects established using tossaction. lead introduction two new contingencies plan, onecoin lands tails up, one lands edge. Although Cassandracould establish (flat coin) tails-up case, would fail complete plan,coin would heads-up. However, turn-over action used,leaving coin flat heads-up given flat tails-up begin with.point decision-rules follows:(and (flat coin) (heads-up coin)) [U1: H] rule heads(and (flat coin) (tails-up coin)) [U1: T] rule tails[U1: E] rule edgeCassandra must plan goal outcome coin landsedge. effects established result tip action. However,result heads-up uncertain effect tip action, since coin might easilyland tails up. Cassandra must therefore add another new contingency coinlands tails tipped. instance, goal established usingturn-over action, tails-up precondition action establisheduncertain result tip action. final decision-rule set first decisionfollows:(and (flat coin) (heads-up coin)) [U1: H] rule heads(and (flat coin) (tails-up coin)) [U1: T] rule tails(on-edge coin)[U1: E] rule edgeon-edge contingency pursued, another decision, stemming uncertainresult tip, must added plan. name second source uncertainty U2,rules decision are:(heads-up coin) [U2: H](tails-up coin) [U2: T]plan depicted Figure 11 shown Section A.6.7. obvious extension Cassandra would construction post-processor spots decision-rulesdiscriminate particular sets outcomes, prunes plan remove super uouscontingencies. Note cannot determined plan complete whether conditionpertains.306fiPlanning Contingencies: Decision-based ApproachflatheadsflattailsheadsDecideflatTipcoinDecideKEYtailsconditionLink uncertain effectadedgeLinkEndheadsTosscoinTurn coinTurn coinconditionflatAlternative control flowIncomplete portion planFigure 11: plan two decisionsKick doorlock brokenOpendooropLinkDecideconditionEndStartlockKEYenintactPicklocknpeAlternativecontrol flowdoorunlockedOpendoorFigure 12: Opening door4.2.4 Decision-rules Unsafe Linksfact Cassandra allows decision-rules fully differentiate outcomes uncertainty raises somewhat subtle issue. Consider partial planopening locked door shown Figure 12. action kicking door has, let us say,two possible outcomes, one lock broken one agent's footbroken. plan contingency lock broken simply open door.plan alternative contingency pick lock open door.Since second plan depend causally outcome uncertainty (theagent's foot broken order pick lock open door),decision-rules based discussion would be:307fiPryor & Collins(lock-broken)[O: L][O: F]rule lock brokenrule foot brokenNotice case pick action depends lock intact,action may effect lock longer intact. words, kickaction potentially clobbers precondition pick. However, planner arguablyignore clobbering, two actions belong different contingencies.valid, though, structure decision-rules guarantees agentchoose execute contingency involving pick outcome kicklock broken. decision-rules clearly enforce this. solutioncase augment decision-rule contingency lock brokentest whether lock fact intact. results following decision-rules (the planshown Section A.7):kick(lock-broken)(not (lock-broken))[O: L][O: F]rule lock brokenrule foot brokenCassandra augments decision-rules way whenever direct effect uncertaintycould clobber link different contingency.5. Contingency Planning Algorithmsection give details Cassandra's algorithm. properties consideredSection 6.5.1 Plan Elementsplan consists steps, effects, links (some may unsafe), open conditions,variable bindings, partial ordering, contingency labels. plan completeopen conditions unsafe links.5.1.1 Steps Effectsplan step Step represents action. may enabling preconditions. leastone effect Eff. instantiation operator.plan step may decision-step Decide. decision-step enabling preconditionsform (know-if Cond) condition Cond. Decide also set decision-rules.effect Eff represents results action. attached step Step, representing action. may secondary preconditions. least one postconditionCond, condition becomes true result executing Step secondarypreconditions hold.5.1.2 Links Open Conditionslink represents causal dependency plan, specifying condition Cond established effect Eff, Cond postcondition. Eff secondary preconditionsSecPre result step Step. link supports step SupStep effect SupEffcondition Cond one of:308fiPlanning Contingencies: Decision-based Approachenabling precondition SupStep;secondary precondition effect SupEff result SupStep;negation secondary precondition effect result SupStep, thuspreserving link.link unsafe contingency Conting required effectClobberEff postcondition ClobberCond (the clobbering condition) resulting stepClobberStep that:Either ClobberCond unify Cond;Cond form (know-if KnowCond) ClobberCond unify KnowCond;Step ClobberStep occur steps Step SupStep;Effect ClobberEff occur contingency Conting.open condition (an unachieved subgoal) represented Cassandra incompletelink, i.e., link missing information effect establishes it.5.1.3 Bindings OrderingsPlan bindings (codesignation constraints) specify relationships variablesconstants. following relationships possible:Two variables may codesignate;variable may designate constant;variable may constrained designate constant;Two variables may constrained codesignate.ordering constrains order two steps respect other, stepS1 must precede step S2 (S1 < S2).5.1.4 Contingency LabelsEvery step, effect open condition partial plan two sets contingency labelsattached it. interests brevity, also refer labels link; case,mean labels step effect link establishes.contingency label two parts: symbol representing source uncertainty,symbol representing possible outcome source uncertainty. Positive contingency labels denote circumstances plan element must necessarilyoccur; negative contingency labels denote circumstances plan element cannotmust occur.Contingency labels must propagated plan. general, positive contingency labels propagated goals effects establish them, negativecontingency labels propagated steps effects result them.details follows:309fiPryor & CollinsPlan (PartList)1. Choose partial plan Plan PartList;2. Plan complete, finish;3. unsafe link Unsafe:resolve (Plan, Unsafe) add resulting plans PartList;Return step 1;4. open condition Open:establish (Plan, Open) add resulting plans PartList;Return step 1.Figure 13: Top level planning algorithmstep inherits positive labels effects result it;step inherits negative labels effects establish enabling preconditions;effect inherits positive labels steps whose enabling preconditionsestablishes;effect inherits positive labels effects whose secondary preconditionsestablishes;effect inherits negative labels step results;effect inherits negative labels effects establish secondary preconditions;open condition inherits positive labels step effect requiredestablish.Cassandra's system label propagation based cnlp complex.Indeed, rather complex would like. complexity mandatedneed deal operators involve multiple context-dependent effects,result step effects necessarily share labels.5.2 Algorithmplanning process starts constructing partial plan consisting two steps:initial step preconditions initial conditions effects;goal step effects goal conditions enabling preconditions.plan added (initially empty) list partial plans PartList. Planningproceeds shown Figure 13.remains describe threats unsafe links resolved openconditions established.310fiPlanning Contingencies: Decision-based ApproachResolve (Plan, Unsafe)1. Initialize list NewPlans;2. unification clobbering condition ClobberCond condition Cond established linkUnsafe involves adding codesignation constraints bindings Plan:Make possible modification bindings Plan ensures ClobberCond cannotunify Cond;Add resulting partial plan NewPlans;3. clobbering step ClobberStep precede step Step establishes Unsafe:Add ordering ensure ClobberStep precedes Step;Add resulting partial plan NewPlans;4. step SupStep supported Unsafe precede ClobberStep:Add ordering ensure SupStep precedes ClobberStep;Add resulting partial plan NewPlans;5. Prevent clobbering effect ClobberEff occurring contingency Conting linkUnsafe unsafe:one of:(a) Add negation secondary preconditions ClobberEff open conditionpositive contingency label Conting;(b) Add Conting negative contingency labels ClobberStep;(c) Add Conting negative contingency labels effect SupEff step SupStepUnsafe supports;appropriate modify relevant decision-rule discussed Section 4.2.4;Add orderings ensure step ClobberStep occurs steps Step SupStep;Propagate labels appropriate;Add resulting partial plan NewPlans;6. Return NewPlans.Figure 14: Resolving threats5.2.1 Resolving Threats Unsafe LinksFigure 14 shows threats resolved. methods shown steps 2, 3, 4standard methods found snlp ucpop; often termed separation, demotion,promotion respectively. say methods step 5 disable threat.methods steps 5a 5b ensure threatening effect occur givencontingency. method step 5a modification standard method found ucpopplanners use secondary preconditions. Essentially, idea preventeffect occurring ensuring context occurs cannot hold.method 5b prevents effect occurring contingency forbidding executionstep produces it. method step 5c notes established step effectcannot occur given contingency. techniques result inconsistent labelingplan element (so that, example, cannot occur every contingencyrequired) resulting partial plan abandoned, represents dead endsearch space.311fiPryor & Collins5.2.2 Establishing Open ConditionsFigure 15 shows procedure used. Procedure EstablishPre shows methods addingnew step reusing existing step; essentially methods used ucpopextended ect need check propagate contingency labels.Procedure EstablishUnk shows methods adding new decision reusing existingdecision specific Cassandra. issues involved discussed Section 4.2.6. Issues Contingency PlanningCassandra partial order planner directly descended ucpop, sound, complete, systematic|all plans produced ucpop guaranteed achieve goals,plan ucpop find it, ucpop never revisits partial plan.section discuss properties related issues context contingency planning.6.1 SoundnessUcpop's soundness depends perfect knowledge assumptions discussed Section 1.particular, ucpop's plans sound initial conditions fully specified,possible effects actions specified operators represent them.uncertainties involved plan, Cassandra equivalent ucpop thereforeconstructs sound plans.uncertainties involved plan, longer assumed initialconditions effects actions fully specified. Indeed, uncertainties ariseassumptions violated. However, assumptions adapted accountpresence uncertainty: would possible, example, insist possibleinitial conditions action effects specified. Cassandra's representation, meansevery source uncertainty must specified use unknown secondarypreconditions, every possible outcome source uncertainty must specified.conjecture Cassandra sound conditions. proof would followprocedure adding new goals whenever new source uncertaintyencountered ensures every goal achieved every possible outcome uncertainty.6.2 Completenessconjecture Cassandra complete limited sense that, sound planform construct, Cassandra find it. believe simpleextension ucpop's completeness. uncertainties involved, Cassandraalways find plan way ucpop. introduction source uncertaintyplan leads addition new contingent goals. Cassandra find plannew goals appropriate contingency. Thus, goal indeedachieved every contingency, Cassandra find plan achieves it, longway determining contingency holds.example, plan disarm bomb described Section 4.1 reliesmethod determining package bomb in. McDermott's presentationexample, two packages indistinguishable, point exampleillustrate nonetheless plan succeed disarming bomb, namely,312fiPlanning Contingencies: Decision-based ApproachEstablish (Plan, Open)1. open condition type :unknown EstablishPre (Plan, Open) return resulting listplans;2. open condition type :unknown source uncertainty Uncertainty outcome OutcomeEstablishUnk (Plan, Open, Uncertainty, Outcome) return resulting list plans.EstablishPre (Plan, Open)1. Initialize list NewPlans;2. effect Eff resulting step Step PlanEff occur every contingency Open must establishedEff precede step SupStep Open required supportpostcondition EffCond Eff unify condition Cond Open requiredestablish:Complete link Open using Eff establishing effect;Add resulting partial plan NewPlans;3. operator effect Eff postcondition EffCond unify Cond:Instantiate new step Step;Complete link Open using Eff establishing effect;Add enabling preconditions Step open conditions;Add resulting partial plan NewPlans;4. plan NewPlans:Add ordering ensure Step precedes SupStep;Add bindings necessary ensure EffCond unifies Cond;Add secondary preconditions SecPre Eff open conditions;Propagate labels appropriate;5. Return NewPlans.EstablishUnk (Plan, Open, Uncertainty,Outcome)1. Initialize list NewCPlans;2. Uncertainty new source uncertainty plan:Add new decision-step DecStep uncertainty Uncertainty;Add new top-level goals open conditions appropriate labels;Add resulting partial plan NewCPlans;3. Uncertainty existing source uncertainty plan:Find existing decision-step DecStep uncertainty Uncertainty;Add resulting partial plan NewCPlans;4. plan NewCPlans:Modify decision-rule DecStep Outcome include Cond antecedent;Add (know-if Cond) open condition required establish DecStep;Add orderings ensure DecStep precedes SupStep;Propagate labels appropriate;5. Return NewCPlans.Figure 15: Establishing open conditions313fiPryor & Collinsdunking packages (McDermott, 1987). algorithm described previous sectioncannot find plan situation impossible achieve preconditionsdecision-step determines package dunk. Section 6.5.5 discussexample detail describe simple extension Cassandra allows correctplan (to dunk packages) found.Ucpop's completeness, like soundness, depends perfect knowledge assumptions discussed Section 1. Cassandra's completeness depends three extensionsassumptions:sources uncertainty specified;specified outcomes exhaustive;actions available allow determination outcome uncertainty, even indirectly.Unfortunately, conditions necessary sucient. Cassandrafind plans actions uses determine contingency interfereachievement goal. instance, might dropping action availablewould detonate bomb inside package dropped. certainly actionallows determination outcome uncertainty, sound planmakes use it.order useful notion Cassandra's completeness, must therefore specify form plans construct. problem common provingcompleteness planner: example, claim snlp, say, incompletecannot find plan bomb-in-the-toilet problem. say insteadvalid plan form construct. fairly simple specify formplans snlp construct: consist partially ordered sequences steps,executed. introduction contingencies makes descriptionCassandra's plans rather complex; yet formalize description,actively working direction. Informally, Cassandra construct plansevery source uncertainty include step decide one relevant plan branches.extension Cassandra solves bomb-in-the-toilet problemconstruct plans meet criterion.6.3 SystematicityUcpop systematic: never visit partial plan twice searching. Cas-sandra, described paper, systematic; may visit partial planssearch space once. Consider plan disarm bomb discussedSection 4.1. plan, two different ways establishing goal disarmbomb: dunking package1 , dunking package2 . Cassandra initially chooseeither way establishing goal, leading case introduction contingencynecessity replanning achieve goal contingency. searchpaths arrive final plan, search systematic.Cassandra could made systematic insisting handling contingenciescertain order, search path uses order treated dead end.314fiPlanning Contingencies: Decision-based ApproachHowever, extension added currently debatedesirability systematicity. example, Langley (1992) argues non-systematicsearch method, iterative sampling, often better systematic method, depth-firstsearch, problems multiple solutions deep solution paths. PeotSmith (1992) observe performance non-systematic version snlp betteroriginal systematic version. ascribed behavior factexploring duplicate plans consumed less overhead ensuring systematicity.6.4 Knowledge Goalsagent executing contingency plans must able acquire information actualstate world determine possible courses action pursue.system constructs contingency plans must able plan informationacquisition: general, acquisition process may arbitrarily complex (Pryor & Collins,1991).early uential discussion goals possess knowledge worldMcCarthy Hayes (1969). Since then, various theories developedaccount (e.g., Moore, 1985; Haas, 1986; Morgenstern, 1987; Steel, 1995).common thread work knowledge goals arise need specifyactions performed; words, need make actions operational . Work area whole concentrated able describerepresent knowledge goals, largely ignored issues involved building plannersconstruct plans containing them.structure Cassandra based notion knowledge goals ariseneed make decisions actions performed (Pryor, 1995). view,planning process deciding advance done (Collins,1987). world conforming perfect knowledge assumptions classical planningalways possible world totally predictable, plans therefore needcontain knowledge goals. However, assumptions relaxed maypossible make decisions advance information necessary makeavailable planner. information may unavailable either planner'slimited knowledge world events nondeterministically causeconditions affect decisions yet occurred. cases maypossible planner determine decision must made even though cannottime actually make it. case planner defer decision: plan makefuture, necessary information available. Part planacquire information; plan thus contains knowledge goals.Cassandra's use \unknown" preconditions indicate nondeterminism thus crucialpart mechanism. Cassandra, knowledge goals arise result deferring decisions. deferred decisions represented explicitly plans, arisedirectly incompleteness Cassandra's knowledge world, whethereffects nondeterministic actions incompletely specified initial conditions.forms uncertainty handled way: Cassandra recognized need defer decision, reason deferral important exceptinasmuch results incomplete knowledge world.315fiPryor & Collinsview knowledge goals arising deferred decisions basically consistentview needed order make actions operational, differstraditional view knowledge goals directly preconditions physical actions,instead preconditions actions make decisions. example, McCarthyHayes consider problem combination safe: commonly held actionopening safe precondition know combination. Cassandra, however,goal knowing combination would arise subgoal deciding plan branchfollow, would branch possible combination.8 branches wouldarise Cassandra's incomplete knowledge world: initial conditionsplan executed fully specified.Cassandra uses variant syntactic approach proposed Haas (1986) representknowledge goals, limiting knowledge goals form know-if(fact). turnsadequate if, assume, possible outcomes given uncertainty known.general, representation used Cassandra, based strips representation adddelete lists, less powerful logics proposed either Morgenstern Haas.6.5 Miscellaneous Issues Contingency PlanningCassandra's approach raises number questions concerning desired behaviorcontingency planner, many obvious answers. section brieconsider issues raised.6.5.1 Dependence Outcomes Superfluous Contingenciesfact contingency plan assumes particular outcome uncertainty meanscannot depend upon different outcome uncertainty. Cassandraenforce constraint plan must causally depend upon outcome assumes.instance, example described Section 2.3.2, plan take Ashlandactually depend Western blocked; could executed successfully regardlesslevel trac Western.observation raises interesting question: plan contingency turnsdepend outcome uncertainty gave rise it, wouldobviate need plans alternative contingencies? instance, example,might seem sensible execute plan use Ashland regardless whether Westernblocked. might thus seem planner edit plan wayeliminate apparently super uous contingencies. However, easily shownversion plan involve dependence outcome uncertaintygenerated elsewhere search space. example, would meanplanner would fact consider plan simply involved taking Ashland. searchheuristics penalize plans involving contingencies appropriately planpreferred contingency plan, things equal.8. raises obvious question whether planning advance every possibility sensiblething do. See Section 7.4 discussion issue.316fiPlanning Contingencies: Decision-based Approach6.5.2 One-sided Contingenciespreceding discussion notwithstanding, plan involving contingencies alwayssuperior plan involving contingency. planner might fact constructplan like Western/Ashland one. take clear-cut example, suppose Pat needs$50 bet horse. might try borrow $50 Chris, outcomeaction uncertain|Chris might refuse. Alternatively, could rob convenience store.robbery plan would (we shall stipulate) involve uncertainties, bad planreasons. would better first try borrow $50 Chris, then,fails, rob convenience store. Cassandra could generate plan. order makeprefer plan contingency-free alternative, however, search metric wouldtake account estimated costs various actions, perform something akinexpected value computation. (See, example, Feldman & Sproull, 1977; Haddawy &Hanks, 1992, discussions decision-theoretic measures applied planning.) orderexecute plan properly, would also necessary way knowingborrowing plan preferred robbery plan possible executeeither them.6.5.3 Identical Branchespossible single plan could work well several different outcomesuncertainty. instance, suppose action asking Chris $50 three possibleoutcomes: either Pat gets money Chris happy (at opportunityfavor); Pat gets money Chris unhappy (at obligedfavor); Pat get money all. Pat constructs plan triesborrow $50 Chris bet horse, then, assuming plan dependupon Chris's happiness (which might, example, Pat needed get ride trackChris), plan work either \get money + Chris happy" outcome\get money + Chris unhappy" outcome.Cassandra could find plan, would effect find twice|onceoutcome uncertainty|and would still require decision-step discriminateoutcomes. inecient two ways: extra search time requiredfind essentially plan twice wasted, effort put makingunnecessary decision. looking ways avoid former problem. lattercould solved post-processor would \merge" identical contingency plans,implemented technique.6.5.4 Branch Mergingpossible construct plan branches split reunite. instance,consider Western/Ashland plan again. context goal getEvanston arises might obligation deliver toast dinner heldEvanston restaurant. contingency due uncertainty trac Western Avenuewould case seem affect portion plan concerned gettingEvanston; probably little bearing wording toast, choice wine,on. natural way frame plan might thus assume regardless317fiPryor & Collinscontingency carried out, planner eventually arrive certain locationEvanston, point single plan developed achieve final goal.Constructing plan way would result compact plan description,might thus reduce effort needed construct plan avoiding, example,construction multiple copies subplan. considering methodsbranch re-merging might achieved, methods considered far seemcomplicate planning process considerably.6.5.5 Fail-safe Planningdiscussed Section 6.2, Cassandra's operation relies able determine,even indirectly, outcome uncertainty. However, may alwayspossible, necessary precondition existence viable plan.bomb-in-the-toilet problem, example, valid plan Cassandra cannot find:dunk packages.suggests method constructing plans face uncertaintyoutcome uncertainty cannot determined|what one might call fail-safe plans.Whenever uncertainty arises principle possible might non-contingentplan would achieve goal whatever outcome uncertainty. findplan, planner must construct version contingency plan actionscontingency branches arising uncertainty executed unconditionally.Cassandra extended way, adding new type decision, oneexecute branches parallel (Collins & Pryor, 1995). plan containingdecision sound none actions must performed achieve goalone contingency interfere actions must performedcontingency, ability perform actions independent outcomeuncertainty. conditions clearly hold bomb-in-the-toilet problem.Cassandra reason possibility labeling scheme distinguishesactions must performed given contingency needperformed. possible execute branches actions branch mayperformed (but need not) branches.parallel decision added plan extended version Cassandra, newgoals added usual way labeling handled differently. branchesseparated, Cassandra longer reason causal links one branchaffected actions another branch.6.5.6 Contingent FailureCassandra produce plan possible achieve goal planpossible contingencies. Often, however, goal cannot fact achieved outcomeunderlying uncertainty. Consider, instance, Peot Smith's example tryingget ski resort car, road leading resort either clear blockedsnowdrifts (Peot & Smith, 1992). road clear, goal achieved,blocked, plans doomed failure.planner expected recognize impossibility achieving goalgeneral case (Chapman, 1987). However, possible approach suggested Peot318fiPlanning Contingencies: Decision-based ApproachSmith. could introduce alternative method resolving open goal conditions: simplyassume goal question fails.undesirable method resolving open goal conditions subgoal factachievable, theory plans involving contingent failure consideredplanner failed find plan goals achieved. sometimespossible, general problem determining whether successful planundecidable. may always partial plans involve goal failurecannot completed. example, partial plan modified may becomecomplex, resolution open condition involving introductionunachieved subgoals. case, plans involving contingent failure never consideredunless ranked plans involve contingent failure. ordergenerally useful, approach must weakened: instead considering goal failureavenues attack failed, apply high fixed penalty plans involvingfailed goals. aim would fix penalty high enough contingent failure wouldapply genuine cases goals unachievable. However, would necessityheuristic approach completeness would lost.7. Related WorkCassandra constructed using ucpop (Penberthy & Weld, 1992) platform. Ucpoppartial order planner handles actions context-dependent effects universally quantified preconditions effects. Ucpop extension snlp (Barrett et al.,1991; McAllester & Rosenblitt, 1991) uses subset Pednault's adl representation(Pednault, 1989).early contingency planner Warren's warplan-c (1976). Contingency planningless abandoned mid seventies early nineties,9 sensp(Etzioni et al., 1992) cnlp (Peot & Smith, 1992). sensp cnlp memberssnlp family: sensp is, like Cassandra, based ucpop, cnlp based directlysnlp. C-buridan (Draper et al., 1994a; Draper, Hanks, & Weld, 1994b), probabilisticcontingency planner, based probabilistic planner buridan (Kushmerick, Hanks,& Weld, 1995) (which based snlp) cnlp. Plinth (Goldman & Boddy,1994a, 1994b) total-order planner based McDermott's Pedestal (1991),strongly uenced cnlp treatment contingency plans.Warplan-c, unlike planners considered here, use strips-basedaction representation, based predicate calculus. could handle actionstwo possible outcomes, merge resulting plan branches.Sensp also differs planners considered here. represents uncertaintyuse run-time variables, distinguished ordinary variables treatedconstants whose values yet known. sensp plan branches ariseintroduction information-gathering steps bind run-time variables. Sensp handlesplan branching constructing separate plans achieve goal particularcontingency. combines separate plans later stage, keeping branchestotally separate. Sensp thus considers contingency branches separately, rather9. Neither noah (Sacerdoti, 1977) Interplan (Tate, 1975) explicitly addressed issues uncertainty,although tackled problems involving (Collins & Pryor, 1995).319fiPryor & Collinsparallel. Actions achieve knowledge goals may preconditions sensp:restriction required order maintain completeness.surprisingly, Cassandra, cnlp, c-buridan, lesser extent Plinth,many respects similar. except Plinth use basic snlp algorithm, useextended strips representations. Cassandra differs cnlp Plinth principallyway uncertainty represented (Section 7.1); difference important implications handling knowledge goals (Section 7.2). principal differenceCassandra c-buridan lies latter's use probabilities (Section 7.3).Contingency planning one approach problem planning uncertainty. aim contingency planning construct single plan succeedcircumstances: essentially extension classical planning.approaches planning uncertainty share aim: probabilistic plannersaim construct plans high probability succeeding (Section 7.3); systemsinterleave planning execution attempt plan fully advance (Section 7.4).approaches possible address problem determining contingencies planned for, currently possible Cassandra. thirdapproach reactive planning, behavior controlled set reactionrules (Section 7.5).7.1 Representation Uncertaintycnlp Plinth, uncertainty represented combination uncertain outcomes nondeterministic actions effects observing outcomes. threevalued logic used: postcondition action may true , false , unknown .example, action tossing coin might postcondition unk(side-up ?x). Special conditional actions , unknown precondition several mutuallyexclusive sets postconditions, used observe results nondeterministic actions. example, operator observe results tossing coin mightprecondition unk(side-up ?x) three possible outcomes: (side-up heads),(side-up tails), (side-up edge).Cnlp thus spreads representation uncertainty action whose execution produces uncertainty action observes result. consequencecnlp cannot use observation action observe results differentactions. example, would require different actions observe results tossingcoin (which three possible outcomes) tipping coin landed edge(which two possible outcomes).Plinth, notion conditional action extended cover action (notobservation actions) nondeterministic effects planner's world model .example, image-processing domain operator remove noise image maymay succeed. However, outcome evident soon applied,special observation action required.cnlp Plinth, information-gathering actions included plan wheneveraction uncertain effects occurs. necessary uncertainty actuallyrepresented information-gathering action rather action actually320fiPlanning Contingencies: Decision-based Approachproduces uncertainty. Knowledge goals thus represented explicitly twosystems.representation used cnlp Plinth arises desire use \singlemodel world, representing planner's state knowledge, rathercomplex formalization including epistemic ground formulas" (Goldman & Boddy,1994b). operator therefore represents effects execution underlying action planner's knowledge world, effectsactual state world. is, course, important represent actions affectplanner's world model, believe also important represent affectworld. all, purpose reasoning actions achieve goals world,planner's world model. particular, execution nondeterministic action actual effects, although may indeed unknown planner,occurred cannot altered. Cassandra's representation ects this: indeed, Cassandra reason possible effects without scheduling observation actions.means extension Cassandra can, example, solve original bomb-in-thetoilet problem, possible actions resolve uncertaintypackage contains bomb: bomb's state represented planner'sworld model stage beginning, known armed,end, packages dunked known safe.implication method representing uncertainty dicultyrepresenting actions whose uncertain effects cannot determined executionsingle action. Consider, example, malfunctioning soda machine oneindicator lights cannot make change, another lights runproduct requested. Suppoe that, functioning correctly, twoindicators light simultaneously. malfunctions, must kicked makework. Observing either light enough determine uncertain effect(working properly malfunctioning) occurred.7.2 Knowledge Goalsmethod representing uncertainty cnlp Plinth important implicationsknowledge goals handled plans.acquisition information planning task like (Pryor & Collins, 1991,1992; Pryor, 1994). general, sequence actions required achieve given knowledgegoal may arbitrarily complex. example, action observe tossed coin mightrequire observer appropriate location; cases, mightseveral different possible methods information gathering, involving perception,involving reasoning, combination. contingency planner, whose plansnecessarily involve achievement knowledge goals, must therefore able planfully generally information gathering.confusion source uncertainty observation uncertain resultslimits ways knowledge goals achieved cnlp Plinth: mustachieved special observation actions specify uncertain outcomes.result representation terms planner's world model, meansrepresent effects actions (except ag unknown)321fiPryor & Collinsplanner observed (or otherwise incorporated world model).discussion issue Goldman Boddy (1994b) explicitly exclude knowledgegoals consideration. point out, planning uncertainty requiresdistinction made actual state world planner's knowledgeit. order plan effectively knowledge goals, must represented.done Cassandra separating representation uncertainty representationinformation-gathering. effect results deterministically action, Cassandrareasons need observe it, forms part world model.uncertain effect, hand, incorporated unconditionally Cassandra'sworld model; noted possibly true, (if necessary) Cassandra setssubgoal determine whether indeed true.Sensp, uses uwl representation goals actions, three differentkinds precondition used represent information goals either alonecombination (Etzioni et al., 1992). well satisfy preconditions, may achievedactions observation, uwl hands-off preconditions indicatingvalue propositions must changed order achieve subgoal, find-outpreconditions. latter ways similar preconditions know-if propositionsCassandra. precondition (find-out (P . v)) tells planner ascertainwhether P truth value v. certain circumstances type preconditionmay achieved action changes value P. Knowledge goals may thusrepresented find-out preconditions satisfy preconditions (often used conjunctionhands-off preconditions). Etzioni et al. argue knowledge goalsachieved actions change value proposition questionchange required another purpose plan. believe unnecessarylimitation, circumstances enforcement actions may best wayachieving knowledge goals.7.3 Probabilistic Decision-theoretic Planningconstructing plans, Cassandra recognizes presence uncertaintyextent. planners specifically address issues probability: example, buridanconstructs plans whose probability achieving goal given threshold (Kushmerick et al., 1995); Drips uses utility different possible outcome variousplans choose one highest expected utility (Haddawy & Suwandi, 1994).Neither buridan drips constructs contingency plans, i.e., plans involve alternative courses action performed different circumstances. C-buridan, basedburidan, constructs contingency plans likely succeed (Draper et al., 1994b,1994a). represents extension cnlp direction decision-theoretic planning.Probabilistic planners use information probabilities possible uncertainoutcomes construct plans likely succeed. Cassandra, hand, cannotuse information constructs plans guaranteed succeed. Probabilisticplanning, relies explicit probabilities, less powerfuldeterministic contingency planning performed Cassandra. Cassandra cannot useinformation probabilities construct plans circumstancesinformation available. example, order solve bomb-in-the-toilet problem,322fiPlanning Contingencies: Decision-based Approachc-buridan would information, least make assumption,probabilities bomb package. Whatever assumptions made mightturn wrong, thus invalidating basis plan.believe would possible build probabilistic planner using ideasc-buridan Cassandra. explicit representation decisions Cassandra,planner would provide excellent opportunity investigating use differentdecision procedures. C-buridan relies full knowledge probabilitiestime constructs plans. knowledge, like other, may availableplan executed. would relatively simple add decision proceduresCassandra's decision representation depend information probabilities, e.g.,follow particular course action probability given outcome exceedscertain value. introduction decision procedures might, course, resultintroduction knowledge goals determine probabilities, possibly leading eventuallysystem would construct plans perform empirical studies determine probabilities.problem associated contingency planning branch merging, i.e.,determination whether two steps separate branches treated step.C-buridan performs full merging: effect probabilistic algorithmbased. Adding capability Cassandra area future work. majorproblem encountered considering branch merging identify variablesdifferent branches other: c-buridan's representations include variables,problem arise. may cause diculties adaptation c-buridan'smerging mechanism Cassandra's use.advantage combining probabilistic planning contingency planning resulting ability judge whether worth planning given contingency. Onelimitations Cassandra present form requirement every possible contingency planned for. complex situations makes resulting plans cumbersome.Moreover, Cassandra's performance deteriorates number distinct branchesplan. cost determining presence particular branch would significantly change probability plan's success might well much less costconstructing branch. interesting issue considered future.7.4 Interleaving Planning ExecutionAlthough Cassandra's plans may include sensing actions, course actionactually executed depending results actions, Cassandrainterleave planning execution. Plans fully specified executed.circumstances clearly inecient. Consider, example, Cassandraconstructs plan open combination safe (see Section 6.4). requires prior knowledgepossible combinations, constructs plan branch combination.obvious alternative would construct plan fully specifiedinformation-gathering step, execute plan stage and, informationgathered, construct rest plan.10 could done Cassandraintroducing another type decision procedure, planning achieve goal,assuming would always possible find plan achieve goal. strong10. See Section 8.2 discussion issue alternative approach.323fiPryor & Collinsassumption, would certainly valid cases problem opening safe.area future work. Interleaving planning execution way wouldadvantage would necessary plan contingencies actuallyarise. would however lose advantages planning advance. example,possible interference actions performed information gatheringmight missed, leading planner find suboptimal plans. Indeed, sensing actionsmay general change world, executing full construction viable planmight unfortunate result making achievement goal impossible.Planners interleave planning execution include ipem (Ambros-Ingerson & Steel,1988), xii (Golden, Etzioni, & Weld, 1994) Sage (Knoblock, 1995). three usebasic interleaving technique: planning possible stepsexecuted. thus set decide advance exactly planningnecessary, plans include explicit provision planning.effects different interleaving strategies investigated design bump (Olawsky& Gini, 1990). Continue Elsewhere strategy much preplanning possibleperformed; Stop Execute strategy, goals defined terms sensor readingsexecuted soon encountered. found neither strategyclear advantage other, strategies sometimes produced planssuboptimal might fail.7.5 Reactive Planningdifferent approach problem planning uncertainty taken reactiveplanning paradigm. approach, specific sequence actions planned advance.contingency planning, planner given set initial conditions goal.However, instead producing plan branches, produces set condition-actionrules: example, universal plans (Schoppers, 1987) Situated Control Rules (SCRs)(Drummond, 1989).theory, reactive planning system handle exogenous events well uncertaineffects unknown initial conditions: possible provide reaction rule everypossible situation may encountered, whether circumstances wouldlead envisaged. contrast, contingency planner Cassandra cannothandle exogenous events cannot predict them. Cassandra contingency planners focus planning effort circumstances predicted possible (or likely,case probabilistic contingency planner c-buridan).would possible represent Cassandra's contingency plans sets conditionaction rules, using causal links preconditions specify conditionsaction performed. However, reasoning required execution timeuse reaction rules required execute contingency plan. Instead simplyexecuting next step plan, reasoning branch points, use reactionrules requires evaluation conditions every cycle order select relevant rule.8. Discussiondescribed Cassandra, partial-order contingency planner represent uncertain outcomes construct contingency plans outcomes. design Cassandra324fiPlanning Contingencies: Decision-based Approachbased coherent view issues arising planning uncertainty. recognizesthat, uncertain world, distinction must drawn actual stateworld planner's model it; instantiates intuitively natural accountknowledge goals exist arise; bases treatment plan branchingrequirements agent execute plan. result, Cassandra explicitlyplans gather information allows information-gathering actions fully general.coherence design provides solid base advanced capabilitiesuse varying decision-making procedures.8.1 Contributionsprincipal contribution work lies explicit representation decision stepsimplications handling knowledge goals. Cassandra is, believe,first planner decisions represented explicit actions plansconstructs. Cassandra's knowledge goals arise specifically need decidealternative courses action, preconditions decision actions. Cassandra thusconsistent view planning process making decisions advance.view, contingency plans plans defer decisions informationbased available (Pryor, 1995). Different plan branches corresponddifferent decision outcomes.use explicit decision steps, Cassandra distinguishes sensinginformation-gathering actions one hand, decision making other. Oneimportant reason making distinction decision may dependone piece information, available performing different actions. addition,separating information-gathering decision-making provides basis introducing alternative methods making decisions. example, extension Cassandra describedSection 6.5.5 introduces type decision directs executing agent performbranches resulting given source uncertainty, allows constructionplans succeed situations way telling actual outcome (e.g., bomb-in-the-toilet problem). believe explicit representationdifferent methods making decisions important direction future research.knowledge goals arise preconditions decisions Cassandra, needknow whether particular plan branch work distinguished need knowactual outcome uncertainty. Cassandra plan determine outcomes unlessrelevant achievement otherwise goals. Moreover, Cassandratreat knowledge goals special cases: plans achieve may complex plansachieve goals. well planning achieve knowledge goals arisepreconditions decisions, Cassandra also produce plans top-level knowledge goals.Two features Cassandra worth noting: exibility afforded labelingscheme; potential learning adaptation afforded representationuncertainty.Cassandra's labeling scheme, although complex, allows agent executing plandistinguish three classes action: must executed givencontingency; must not; whose execution affect achievement325fiPryor & Collinsgoal contingency.11 feature paves way extension describedallows Cassandra build plans requiring execution branches resultingsource uncertainty.Cassandra's representation makes assumptions intrinsic nature uncertainty. unknown precondition simply denotes information contextproduce particular effect action available planner. mayinformation principle unknowable (in domains involving quantum effects,example); much likely uncertainty results limitationsplanner information available it. general, agent operating real-worlddomain much effective learn improve performance adaptchanging conditions. use unknown preconditions represent uncertainty meanscircumstances would relatively simple incorporate results learning adaptation planner's domain knowledge. example, planner mightdiscover predict certain outcomes: could change unknown preconditionsones ecting new knowledge. If, hand, discovered predictedeffects consistently failing occur, could change relevant preconditionsunknown ones.8.2 LimitationsCassandra one increasing number planners aim extend techniquesclassical planning realistic domains. Cassandra designed operate domainstwo three principal constraints observed classical planners relaxed:namely, allow non-deterministic actions incomplete knowledge initial conditions. Cassandra is, however, subject third constraint, changes take placeexcept result actions specified plan. clearly limits effectivenessmany real-world domains. Moreover, limits extent nondeterminismincompleteness knowledge handled. Cassandra's plans necessarilyachieve goals sources uncertainty ignored, possible outcomesspecified.Cassandra cannot make use information likely particular outcomes are,unlike probabilistic decision-theoretic planners; cannot plan interleave planningexecution; provide reaction rules possible circumstances.solve problems valid plans involving ways discriminatingpossible outcomes; algorithm given cannot solve original version bombin-the-toilet problem, although extension described Section 6.5.5 (Collins& Pryor, 1995).algorithm described paper two major practical limitations: first,plans produces often complex necessary; second, time takenproduce plans precludes use except simple problems.complexity Cassandra's plans results necessity planning everycontingency lack branch merging. example, suppose opencombination safe could obtain money pay evening out. Cassandra's11. agents make use information, guarantee third type stepactually executable.326fiPlanning Contingencies: Decision-based Approachplan goal enjoying evening would one branch possible safecombination. branch would start actions open safe,different combination, would continue actions going restaurantmovies, say, would identical branch. simpler plan wouldmerge separate branches safe opened. consideration methodsbranch merging area future work (see Sections 6.5.4 7.3).circumstances, example, plan complexity could reduceduse run-time variables, introduced ipem (Ambros-Ingerson& Steel, 1988) used sensp (Etzioni et al., 1992) (see Section 7).uncertainty value action parameter takes (which case openingcombination safe) would possible use run-time variable represent parameter, obviating need separate plan branches. Implementing strategy wouldrequire effective methods determining effects uncertainty limitedparameter values. general, notion indicates possible approach problembranch merging: taking least commitment approach variable binding,way least commitment approach taken step ordering partial orderplanner. would allow concept \conditional" variable binding: variablebinding could labeled required forbidden given contingency.analyzed complexity Cassandra's algorithm, believeexponential. effect multiple plan branches, whose presenceincreases number steps plan also increases number potentialinteractions number ways resolving them. Certainly, subjective impressionCassandra runs even slowly planners snlp family. Effectivedomain-independent search control heuristics dicult find, many (toy)domains used Cassandra even problem-specific heuristics hard comeby.8.3 ConclusionCassandra planning system based firmly classical planning paradigm. Manystrengths weaknesses classical planning systems. example,believe certain circumstances plans valid guaranteedfind valid plan one exists. However, techniques uses valid limitedcircumstances, computational complexity make direct scaling unlikelyfeasible.view, principal strengths Cassandra arise explicit representationdecisions plans. shown use decisions provides natural accountknowledge goals arise planning process. also sketcheddecisions used basis extensions provide added functionality. newtype decision allows fail-safe plans, provide method solving problemsbomb-in-the-toilet problem (Section 6.5.5); another type decision may provideeffective method interleaving planning execution (Section 7.4).believe use explicit decision procedures enable extensionrange applicability techniques classical planning. general, idea constructingsingle plan succeed circumstances is, feel, unlikely productive:327fiPryor & Collinsreal world complex uncertain enough trying predict behavior detailsimply impossible. However, use decision procedures that, example, involveprobabilistic techniques interleave planning execution, appears likely provideexible framework that, although inevitably sacrificing completeness correctness,provide basis effective, practical planning real world.Appendix A. Cassandra's Plansappendix shows plans constructed Cassandra examples bodypaper. plan consists initial conditions, plan steps goals. initial conditionsshown top plan. unknown shown dependingparticular contingency. plan steps shown next. shown numberdenoting order plan. numbers parentheses show ordersteps added plan. right step contingency labels.brevity, individual effects step always omitted links establishstep's enabling secondary preconditions often omitted.Finally, bottom plan come goal conditions. goal stated first,contingency goal shown links establish it. usual, contingencylabels right.A.1 Plan Get Evanstonplan shown Figure 3 discussed Section 2.3.2. Note decision-stepsingle active decision-rule. situation discussed comments onesided contingencies Section 6.5: route using Western quicker clear,Ashland route slower always possible.Initial:[TRAFFIC0S: GOOD] (NOT (TRAFFIC-BAD))[TRAFFIC0S: BAD] (TRAFFIC-BAD)(AND (AT START) (ROAD WESTERN) (ROAD BELMONT) (ROAD ASHLAND))Step1 (4): (GO-TO-WESTERN-AT-BELMONT)YES: [TRAFFIC0S: GOOD BAD](AND (NOT (AT START)) (ON WESTERN) (ON BELMONT))0 -> (AT START)Step2 (3): (CHECK-TRAFFIC-ON-WESTERN)(KNOW-IF (TRAFFIC-BAD))1 -> (ON WESTERN)Step3 (2): (DECIDE TRAFFIC0S)(and (NOT (TRAFFIC-BAD))) => [TRAFFIC0S: GOOD](and) => [TRAFFIC0S: BAD]2 -> (KNOW-IF (TRAFFIC-BAD))Step4 (6): (TAKE-BELMONT)YES: [TRAFFIC0S: BAD]: [TRAFFIC0S: GOOD](AND (NOT (ON WESTERN)) (ON ASHLAND))1 -> (ON BELMONT)328fiPlanning Contingencies: Decision-based ApproachStep5 (5): (TAKE-ASHLAND)YES: [TRAFFIC0S: BAD]: [TRAFFIC0S: GOOD](AT EVANSTON)4 -> (ON ASHLAND)Step: [TRAFFIC0S: GOOD]6 (1): (TAKE-WESTERN)YES: [TRAFFIC0S: GOOD]: [TRAFFIC0S: BAD](AT EVANSTON)1 -> (ON WESTERN)0 -> (NOT (TRAFFIC-BAD))Goal:: [TRAFFIC0S: BAD]: [TRAFFIC0S: BAD](AT EVANSTON)GOAL5 -> (AT EVANSTON)YES: [TRAFFIC0S: BAD]: [TRAFFIC0S: GOOD]6 -> (AT EVANSTON)YES: [TRAFFIC0S: GOOD]: [TRAFFIC0S: BAD]GOALComplete!A.2 Disarming Bombplan shown Figures 6 7 discussed Section 4.1.1. Notemoving steps dunking steps always possible, necessary oneoutcome uncertainty. fail-safe plan (see Section 6.2) therefore possible.Initial:[UNK0S: O2] (CONTAINS PACKAGE-2 BOMB)[UNK0S: O1] (CONTAINS PACKAGE-1 BOMB)(AND (AT PACKAGE-1 RUG) (AT PACKAGE-2 RUG))Step1 (5): (X-RAY PACKAGE-2)(KNOW-IF (CONTAINS PACKAGE-2 BOMB))Step2 (3): (X-RAY PACKAGE-1)(KNOW-IF (CONTAINS PACKAGE-1 BOMB))Step3 (2): (DECIDE UNK0S)(and (CONTAINS PACKAGE-2 BOMB)) => [UNK0S: O2](and (CONTAINS PACKAGE-1 BOMB)) => [UNK0S: O1]1 -> (KNOW-IF (CONTAINS PACKAGE-2 BOMB))2 -> (KNOW-IF (CONTAINS PACKAGE-1 BOMB))Step4 (7): (MOVE RUG TOILET PACKAGE-1)YES: [UNK0S: O1](AND (NOT (AT PACKAGE-1 RUG)) (AT PACKAGE-1 TOILET))0 -> (AT PACKAGE-1 RUG)Step5 (6): (MOVE RUG TOILET PACKAGE-2)YES: [UNK0S: O2](AND (NOT (AT PACKAGE-2 RUG)) (AT PACKAGE-2 TOILET))0 -> (AT PACKAGE-2 RUG)Step6 (4): (DUNK PACKAGE-2)(WET PACKAGE-2)YES: [UNK0S: O2]329fiPryor & Collins5 -> (AT PACKAGE-2 TOILET)(DISARMED BOMB)0 -> (CONTAINS PACKAGE-2 BOMB)Step7 (1): (DUNK PACKAGE-1)(WET PACKAGE-1)4 -> (AT PACKAGE-1 TOILET)(DISARMED BOMB)0 -> (CONTAINS PACKAGE-1 BOMB)Goal:: [UNK0S: O1]YES: [UNK0S: O1]: [UNK0S: O2](DISARMED BOMB)GOAL6 -> (DISARMED BOMB)YES: [UNK0S: O2]: [UNK0S: O1]7 -> (DISARMED BOMB)YES: [UNK0S: O1]: [UNK0S: O2]GOALComplete!A.3 Fetching Packageplan Figure 8, discussed Section 4.1.3, involves one source uncertaintyhence contains one decision-step. two possible ways achieving goal,one outcome uncertainty.Initial:(AVAILABLE CAR-1)[LOC0S: B] (PACKAGE-AT LOCATION-2)[LOC0S: A] (PACKAGE-AT LOCATION-1)(AND (IS-CAR CAR-1) (IS-CAR CAR-2) (LOCATION LOCATION-1)(LOCATION LOCATION-2))Step1 (2): (ASK-ABOUT-PACKAGE)(KNOW-IF (PACKAGE-AT LOCATION-2))0 -> (LOCATION LOCATION-2)(KNOW-IF (PACKAGE-AT LOCATION-1))0 -> (LOCATION LOCATION-1)Step2 (1): (DECIDE LOC0S)(and (PACKAGE-AT(and (PACKAGE-AT1 -> (KNOW-IF1 -> (KNOW-IFLOCATION-2)) => [LOC0S: B]LOCATION-1)) => [LOC0S: A](PACKAGE-AT LOCATION-2))(PACKAGE-AT LOCATION-1))Step3 (4): (DRIVE CAR-1 LOCATION-1)(AT LOCATION-1)0 -> (AVAILABLE CAR-1)YES: [LOC0S: A]Step4 (3): (DRIVE CAR-1 LOCATION-2)(AT LOCATION-2)0 -> (AVAILABLE CAR-1)YES: [LOC0S: B]Goal:(AND (AT ?LOC) (PACKAGE-AT ?LOC))330fiPlanning Contingencies: Decision-based ApproachGOALYES: [LOC0S: B]4 -> (AT LOCATION-2)0 -> (PACKAGE-AT LOCATION-2)GOAL: [LOC0S: A]YES: [LOC0S: A]3 -> (AT LOCATION-1)0 -> (PACKAGE-AT LOCATION-1): [LOC0S: B]Complete!A.4 Fetching Another Packageplan Figure 9, discussed Section 4.1.3, two sources uncertainty twodecision-steps. four possible ways achieving goal, one combinationoutcomes two sources uncertainty.Initial:Step(AND[CAR0S: C2] (AVAILABLE[CAR0S: C1] (AVAILABLE[LOC0S: B] (PACKAGE-AT[LOC0S: A] (PACKAGE-AT(IS-CAR CAR-1) (IS-CAR(LOCATION LOCATION-2))CAR-2)CAR-1)LOCATION-2)LOCATION-1)CAR-2) (LOCATION LOCATION-1)1 (5): (ASK-ABOUT-CAR)YES: [LOC0S: B](KNOW-IF (AVAILABLE CAR-2))0 -> (IS-CAR CAR-2)(KNOW-IF (AVAILABLE CAR-1))0 -> (IS-CAR CAR-1)Step2 (4): (DECIDE CAR0S)YES: [LOC0S: B](and (AVAILABLE CAR-2)) => [CAR0S: C2](and (AVAILABLE CAR-1)) => [CAR0S: C1]1 -> (KNOW-IF (AVAILABLE CAR-2))1 -> (KNOW-IF (AVAILABLE CAR-1))Step3 (2): (ASK-ABOUT-PACKAGE)YES: [CAR0S: C2 C1](KNOW-IF (PACKAGE-AT LOCATION-2))0 -> (LOCATION LOCATION-2)(KNOW-IF (PACKAGE-AT LOCATION-1))0 -> (LOCATION LOCATION-1)Step4 (1): (DECIDE LOC0S)(and (PACKAGE-AT(and (PACKAGE-AT3 -> (KNOW-IF3 -> (KNOW-IFYES: [CAR0S: C2 C1]LOCATION-2)) => [LOC0S: B]LOCATION-1)) => [LOC0S: A](PACKAGE-AT LOCATION-2))(PACKAGE-AT LOCATION-1))Step5 (8): (DRIVE CAR-2 LOCATION-1)YES: [LOC0S: A][CAR0S: C2]331fiPryor & Collins: [CAR0S: C1](AT LOCATION-1)0 -> (AVAILABLE CAR-2)Step: [CAR0S: C1]6 (6): (DRIVE CAR-2 LOCATION-2)YES: [LOC0S: B][CAR0S: C2]: [CAR0S: C1](AT LOCATION-2)0 -> (AVAILABLE CAR-2)Step: [CAR0S: C1]7 (7): (DRIVE CAR-1 LOCATION-1)YES: [LOC0S: A][CAR0S: C1]: [CAR0S: C2](AT LOCATION-1)0 -> (AVAILABLE CAR-1)Step: [CAR0S: C2]8 (3): (DRIVE CAR-1 LOCATION-2)YES: [LOC0S: B][CAR0S: C1]: [CAR0S: C2](AT LOCATION-2)0 -> (AVAILABLE CAR-1)Goal:: [CAR0S: C2](AND (AT ?LOC) (PACKAGE-AT ?LOC))GOAL5 -> (AT LOCATION-1)0 -> (PACKAGE-AT LOCATION-1)YES: [LOC0S: A][CAR0S: C2]: [CAR0S: C1]: [LOC0S: B]6 -> (AT LOCATION-2)0 -> (PACKAGE-AT LOCATION-2)YES: [LOC0S: B][CAR0S: C2]: [CAR0S: C1]: [LOC0S: A]8 -> (AT LOCATION-2)0 -> (PACKAGE-AT LOCATION-2)YES: [LOC0S: B][CAR0S: C1]: [CAR0S: C2]: [LOC0S: A]7 -> (AT LOCATION-1)0 -> (PACKAGE-AT LOCATION-1)YES: [LOC0S: A][CAR0S: C1]: [CAR0S: C2]: [LOC0S: B]GOALGOALGOALComplete!A.5 Tossing CoinSection 4.2.3 described plan ending coin. decision plandistinguish coin landing heads-up tails-up|the decision rulesambiguous.Initial:(HOLDING-COIN)Step1 (2): (TOSS-COIN)(AND (NOT (HOLDING-COIN)) (ON-TABLE))0 -> (HOLDING-COIN)Step2 (4): (INSPECT-COIN)(AND (KNOW-IF (FLAT-COIN)) (KNOW-IF (HEADS-UP))(KNOW-IF (TAILS-UP)) (KNOW-IF (ON-EDGE)))332fiPlanning Contingencies: Decision-based ApproachStep3 (3): (DECIDE UNK2S)(and (FLAT-COIN)) => [UNK2S: H](and (FLAT-COIN)) => [UNK2S: T](and (ON-EDGE)) => [UNK2S: E]2 -> (KNOW-IF (FLAT-COIN))2 -> (KNOW-IF (ON-EDGE))Step4 (1): (TIP-COIN)YES: [UNK2S: E]: [UNK2S: H T](FLAT-COIN)1 -> (ON-EDGE)Goal:: [UNK2S: H T](FLAT-COIN)GOAL1 -> (FLAT-COIN)YES: [UNK2S: T]: [UNK2S: H E]1 -> (FLAT-COIN)YES: [UNK2S: H]: [UNK2S: E]4 -> (FLAT-COIN)YES: [UNK2S: E]: [UNK2S: H T]GOALGOALComplete!A.6 Tossing Another Coinplan Figure 11 two decisions unambiguous decision-rules. fourways achieving goal plan, two sources uncertainty.Initial:(HOLDING-COIN)Step1 (1): (TOSS-COIN)(AND (NOT (HOLDING-COIN)) (ON-TABLE) (KNOW-IF (FLAT-COIN))(KNOW-IF (HEADS-UP)) (KNOW-IF (TAILS-UP)) (KNOW-IF (ON-EDGE)))0 -> (HOLDING-COIN)Step2 (2): (DECIDE TOSS1S)(and (FLAT-COIN)(HEADS-UP)(and (ON-EDGE)(and (FLAT-COIN)(TAILS-UP)1 -> (KNOW-IF1 -> (KNOW-IF1 -> (KNOW-IF1 -> (KNOW-IFStep) => [TOSS1S: H]) => [TOSS1S: E]) => [TOSS1S: T](ON-EDGE))(FLAT-COIN))(TAILS-UP))(HEADS-UP))3 (4): (TIP-COIN)YES: [TOSS1S: E]333fiPryor & Collins: [TOSS1S: H](AND (FLAT-COIN) (KNOW-IF (HEADS-UP)) (KNOW-IF (TAILS-UP)))1 -> (ON-EDGE): [TOSS1S: H T]Step4 (5): (DECIDE TIP4S)YES: [TOSS1S: E]: [TOSS1S: H](and (TAILS-UP)) => [TIP4S:(and (HEADS-UP)) => [TIP4S:3 -> (KNOW-IF (TAILS-UP))3 -> (KNOW-IF (HEADS-UP))Step5 (3): (TURN-OVER)(HEADS-UP)1 -> (TAILS-UP): [TOSS1S: H E]6 (6): (TURN-OVER)YES: [TOSS1S: E][TIP4S: T]: [TOSS1S: H][TIP4S: H]: [TOSS1S: H]3 -> (FLAT-COIN)(HEADS-UP)3 -> (TAILS-UP)Goal:H]: [TOSS1S: H]: [TOSS1S: H]YES: [TOSS1S: T]: [TOSS1S: E H]: [TOSS1S: H E]1 -> (FLAT-COIN)StepT]: [TOSS1S: H][TIP4S: H](AND (FLAT-COIN) (HEADS-UP))GOAL3 -> (FLAT-COIN)6 -> (HEADS-UP)YES: [TOSS1S: E][TIP4S: T]: [TOSS1S: H]: [TOSS1S: H][TIP4S: H]3 -> (FLAT-COIN)3 -> (HEADS-UP)YES: [TOSS1S: E][TIP4S: H]: [TOSS1S: H]: [TOSS1S: H T][TIP4S: T]1 -> (FLAT-COIN)5 -> (HEADS-UP)YES: [TOSS1S: T]: [TOSS1S: H E]: [TOSS1S: E H]1 -> (FLAT-COIN)1 -> (HEADS-UP)YES: [TOSS1S: H]: [TOSS1S: E]: [TOSS1S: E]GOALGOALGOALComplete!A.7 Opening DoorSection 4.2.4 described plan opening locked door without key; depictedFigure 12. plan Cassandra produces situation shown here. Eventhough preconditions pick step depend effect kick step, formercannot performed lock broken result kicking door. decision-rulesect dependence.334fiPlanning Contingencies: Decision-based ApproachInitial:(LOCK-INTACT)Step1 (2): (KICK)Step2 (4): (LOOK)(AND (KNOW-IF (LOCKED)) (KNOW-IF (LOCK-INTACT))(KNOW-IF (FOOT-BROKEN)))Step3 (3): (DECIDE KICK2S)(and ((LOCK-INTACT))) => [KICK2S: F](and (NOT (LOCKED))) => [KICK2S: L]2 -> (KNOW-IF (LOCKED))Step4 (6): (PICK)YES: [KICK2S: F]: [KICK2S: L](NOT (LOCKED))0 -> (LOCK-INTACT)Step: [KICK2S: L]5 (5): (OPEN-DOOR)YES: [KICK2S: F]: [KICK2S: L](OPEN)4 -> (NOT (LOCKED))Step: [KICK2S: L]6 (1): (OPEN-DOOR)YES: [KICK2S: L]: [KICK2S: F](OPEN)1 -> (NOT (LOCKED))Goal:: [KICK2S: F](OPEN)GOAL5 -> (OPEN)YES: [KICK2S: F]: [KICK2S: L]6 -> (OPEN)YES: [KICK2S: L]: [KICK2S: F]GOALComplete!AcknowledgementsThanks Dan Weld Tony Barrett supplying ucpop code, Mark Peot RobertGoldman comments earlier drafts, Fitzgerald many useful discussions,anonymous reviewers constructive helpful criticism. Muchwork performed first author student Institute LearningSciences, Northwestern University. work supported part AFOSRgrant number AFOSR-91-0341-DEF. Institute Learning Sciences established1989 support Andersen Consulting, part Arthur Andersen WorldwideOrganization. Institute receives additional support Ameritech North WestWater, Institute Partners, IBM.335fiPryor & CollinsReferencesAllen, J., Hendler, J., & Tate, A. (Eds.). (1990). Readings Planning. Morgan Kaufmann,San Mateo, CA.Ambros-Ingerson, J., & Steel, S. (1988). Integrating planning, execution, monitoring.Proceedings Seventh National Conference Artificial Intelligence, pp. 83{88St Paul, MN. AAAI. Also (Allen, Hendler, & Tate, 1990).Barrett, A., Soderland, S., & Weld, D. S. (1991). Effect step-order representationsplanning. Technical report 91-05-06, Department Computer Science Engineering, University Washington, Seattle.Chapman, D. (1987). Planning conjunctive goals. Artificial Intelligence, 32, 333{377.Also (Allen et al., 1990).Collins, G. C. (1987). Plan creation: Using strategies blueprints. Technical reportYALEU/CSD/RR 599, Department Computer Science, Yale University.Collins, G., & Pryor, L. (1992). Achieving functionality filter conditions partialorder planner. Proceedings Tenth National Conference Artificial Intelligence, pp. 375{380 San Jose, CA. AAAI.Collins, G., & Pryor, L. (1995). Planning uncertainty: key issues. ProceedingsFourteenth International Joint Conference Artificial Intelligence, pp. 1567{1573 Montreal, Canada. IJCAI.Draper, D., Hanks, S., & Weld, D. (1994a). probabilistic model action leastcommitment planning information gathering. Proceedings Tenth Conference Uncertainty Artificial Intelligence, pp. 178{186 Seattle, WA. MorganKaufmann.Draper, D., Hanks, S., & Weld, D. (1994b). Probabilistic planning information gathering contingent execution. Proceedings Second International ConferenceArtificial Intelligence Planning Systems, pp. 31{36 Chicago, IL. AAAI Press.Drummond, M. (1989). Situated control rules. Proceedings First InternationalConference Principles Knowledge Representation Reasoning, pp. 103{113Toronto. Morgan Kaufmann.Etzioni, O., Hanks, S., Weld, D., Draper, D., Lesh, N., & Williamson, M. (1992). approach planning incomplete information. Proceedings Third International Conference Knowledge Representation Reasoning, pp. 115{125 Boston,MA. Morgan Kaufmann.Feldman, J. A., & Sproull, R. F. (1977). Decision theory artificial intelligence II:hungry monkey. Cognitive Science, 1, 158{192. Also (Allen et al., 1990).Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: new approach applicationtheorem proving problem solving. Artificial Intelligence, 2, 189{208. Also (Allenet al., 1990).336fiPlanning Contingencies: Decision-based ApproachGolden, K., Etzioni, O., & Weld, D. (1994). Omnipotence without omniscience: Ecientsensor management planning. Proceedings Twelfth National ConferenceArtificial Intelligence, pp. 1048{1054. AAAI Press.Goldman, R. P., & Boddy, M. S. (1994a). Conditional linear planning. ProceedingsSecond International Conference Artificial Intelligence Planning Systems, pp.80{85 Chicago, IL. AAAI Press.Goldman, R. P., & Boddy, M. S. (1994b). Representing uncertainty simple planners.Proceedings Fourth International Conference Principles KnowledgeRepresentation Reasoning, pp. 238{245 Bonn. Morgan Kaufmann.Haas, A. R. (1986). syntactic theory belief action. Artificial Intelligence, 28,245{292.Haddawy, P., & Hanks, S. (1992). Representations decision-theoretic planning: Utilityfunctions deadline goals. Proceedings Third International ConferencePrinciples Knowledge Representation Reasoning, pp. 71{82 Boston, MA.Morgan Kaufmann.Haddawy, P., & Suwandi, M. (1994). Decision-theoretic refinement planning using inheritance abstraction. Proceedings Second Internatinal Conference ArtificialPlanning Systems, pp. 266{271 Chicago. AAAI Press.Knoblock, C. (1995). Planning, executing, sensing, replanning information gathering. Proceedings Fourteenth International Joint Conference ArtificialIntelligence, pp. 1686{1693 Montreal. IJCAI.Kushmerick, N., Hanks, S., & Weld, D. (1995). algorithm probabilistic planning.Artificial Intelligence, 76, 239{286.Langley, P. (1992). Systematic nonsystematic search strategies. ProceedingsFirst International Conference Artificial Intelligence Planning Systems, pp. 145{152 College Park, Maryland. Morgan Kaufmann.McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. ProceedingsNinth National Conference Artificial Intelligence, pp. 634{639 Anaheim, CA.AAAI.McCarthy, J., & Hayes, P. J. (1969). philosophical problems standpointartificial intelligence. Meltzer, B., & Michie, D. (Eds.), Machine Intelligence 4, pp.463{502. Edinburgh University Press. Also (Allen et al., 1990).McDermott, D. (1987). critique pure reason. Computational Intelligence, 3, 151{160.McDermott, D. (1991). Regression planning. International Journal Intelligent Systems,6 (4), 357{416. Also available Yale TR YALEU/CSD/RR 752.Moore, R. C. (1985). formal theory knowledge action. Hobbs, J. R., & Moore,R. C. (Eds.), Formal Theories Commonsense World. Ablex, Norwood, NJ. Also(Allen et al., 1990).337fiPryor & CollinsMorgenstern, L. (1987). Knowledge preconditions actions plans. ProceedingsTenth International Joint Conference Artificial Intelligence, pp. 867{874Milan. IJCAI.Olawsky, D., & Gini, M. (1990). Deferred planning sensor use. ProceedingsWorkshop Innovative Approaches Planning, Scheduling Control, pp. 166{174 San Diego, CA. DARPA.Pednault, E. P. D. (1988). Extending conventional planning techniques handle actionscontext-dependent effects. Proceedings Seventh National ConferenceArtificial Intelligence, pp. 55{59 St Paul, MN. AAAI.Pednault, E. P. D. (1989). ADL: Exploring middle ground STRIPSsituation calculus. Proceedings First International Conference PrinciplesKnowledge Representation Reasoning, pp. 324{332. Morgan Kaufmann.Pednault, E. P. D. (1991). Generalizing nonlinear planning handle complex goalsactions context-dependent effects. Proceedings Twelfth InternationalJoint Conference Artificial Intelligence, pp. 240{245 Sydney, Australia. IJCAI.Penberthy, J. S., & Weld, D. S. (1992). UCPOP: sound, complete, partial order planner ADL. Proceedings Third International Conference KnowledgeRepresentation Reasoning, pp. 103{114 Boston, MA. Morgan Kaufmann.Peot, M. A., & Smith, D. E. (1992). Conditional nonlinear planning. ProceedingsFirst International Conference Artificial Intelligence Planning Systems, pp.189{197 College Park, Maryland. Morgan Kaufmann.Pryor, L. (1994). Opportunities planning unpredictable world. Technical report 53,Institute Learning Sciences, Northwestern University.Pryor, L. (1995). Decisions, decisions: Knowledge goals planning. Hallam, J. (Ed.),Hybrid Problems, Hybrid Solutions (Proceedings AISB-95), Frontiers ArtificialIntelligence Applications, pp. 181{192. IOS Press, Amsterdam.Pryor, L., & Collins, G. (1991). Information-gathering planning task: position paper.Notes AAAI workshop Knowledge-Based Construction ProbabilisticDecision Models, pp. 101{105 Anaheim, CA. AAAI.Pryor, L., & Collins, G. (1992). Planning perceive: utilitarian approach. Workingnotes AAAI Spring Symposium: Control Selective Perception, pp. 113{122Stanford, CA. AAAI.Pryor, L., & Collins, G. (1993). Cassandra: Planning contingencies. Technical report 41, Institute Learning Sciences, Northwestern University.Sacerdoti, E. (1977). structure plans behavior. American Elsevier, New York.Schoppers, M. J. (1987). Universal plans reactive robots unpredictable environments.Proceedings Tenth International Joint Conference Artificial Intelligence,pp. 1039{1046 Milan. IJCAI.338fiPlanning Contingencies: Decision-based ApproachSteel, S. (1995). Knowing how: semantic approach. Hallam, J. (Ed.), Hybrid Problems,Hybrid Solutions (Proceedings AISB-95), Frontiers Artificial IntelligenceApplications, pp. 193{202. IOS Press, Amsterdam.Stefik, M. (1981a). Planning constraints (MOLGEN: Part 1). Artificial Intelligence,16, 111{140. Also (Allen et al., 1990).Stefik, M. (1981b). Planning constraints (MOLGEN: Part 2). Artificial Intelligence,16, 141{170.Sussman, G. J. (1975). computer model skill acquisition. American Elsevier, NewYork.Tate, A. (1975). Using goal structure direct search problem solver. Ph.D. thesis,University Edinburgh.Tate, A. (1977). Generating project networks. Proceedings Fifth InternationalJoint Conference Artificial Intelligence, pp. 888{893 Cambridge, MA. IJCAI. Also(Allen et al., 1990).Warren, D. (1976). Generating conditional plans programs. ProceedingsSummer Conference Artificial Intelligence Simulation Behaviour, pp.344{354 Edinburgh. AISB.Wilkins, D. E. (1988). Practical Planning: Extending Classical AI Planning Paradigm.Morgan Kaufmann, San Mateo, CA.339fiJournal Artificial Intelligence Research 4 (1996) 147-179Submitted 3/95; published 4/96Iterative Optimization SimplificationHierarchical ClusteringsDoug FisherDepartment Computer Science, Box 1679, Station BVanderbilt University, Nashville, TN 37235 USAdfisher@vuse.vanderbilt.eduAbstractClustering often used discovering structure data. Clustering systems differobjective function used evaluate clustering quality control strategy usedsearch space clusterings. Ideally, search strategy consistently constructclusterings high quality, computationally inexpensive well. general,cannot ways, partition search system inexpensivelyconstructs `tentative' clustering initial examination, followed iterative optimization,continues search background improved clusterings. Given motivation,evaluate inexpensive strategy creating initial clusterings, coupled severalcontrol strategies iterative optimization, repeatedly modifies initialclustering search better one. One methods appears novel iterativeoptimization strategy clustering contexts. clustering constructedjudged analysts { often according task-specific criteria. Several authors abstracted criteria posited generic performance task akin pattern completion,error rate completed patterns used `externally' judge clustering utility.Given performance task, adapt resampling-based pruning strategies used supervised learning systems task simplifying hierarchical clusterings, thus promisingease post-clustering analysis. Finally, propose number objective functions, basedattribute-selection measures decision-tree induction, might perform wellerror rate simplicity dimensions.1. IntroductionClustering often used discovering structure data. Clustering systems differobjective function used evaluate clustering quality control strategy used searchspace clusterings. Ideally, search strategy consistently construct clusterings high quality, computationally inexpensive well. Given combinatorialcomplexity general clustering problem, search strategy cannot computationally inexpensive give guarantee quality discovered clusterings acrossdiverse set domains objective functions. However, partition searchinitial clustering inexpensively constructed, followed iterative optimizationprocedures continue search background improved clusterings. allowsanalyst get early indication possible presence form structure data,search continue long seems worthwhile. seems primary motivationbehind design systems Autoclass (Cheeseman, Kelly, Self, Stutz, Taylor,& Freeman, 1988) Snob (Wallace & Dowe, 1994).paper describes evaluates three strategies iterative optimization, one inspired iterative `seed' selection strategy Cluster/2 (Michalski & Stepp, 1983a,c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiFisher1983b), one common form optimization iteratively reclassifies single observations, third method appears novel clustering literature. latter strategyinspired, part, macro-learning strategies (Iba, 1989) { collections observationsreclassified en masse, appears mitigate problems associated local maximameasured objective function. evaluation purposes, couple strategies simple, inexpensive procedure used Cobweb (Fisher, 1987a, 1987b)system Anderson Matessa (1991), constructs initial hierarchical clustering. iterative optimization strategies, however, paired methodsconstructing initial clusterings.clustering constructed judged analysts { often accordingtask-specific criteria. Several authors (Fisher, 1987a, 1987b; Cheeseman et al., 1988; Anderson & Matessa, 1991) abstracted criteria generic performance taskakin pattern completion, error rate completed patterns used`externally' judge utility clustering. systems, objective functionselected performance task mind. Given performance taskadapt resampling-based pruning strategies used supervised learning systems tasksimplifying hierarchical clusterings, thus easying post-clustering analysis. Experimentalevidence suggests hierarchical clusterings greatly simplified increasepattern-completion error rate.experiments clustering simplification suggest `external' criteria simplicityclassification cost, addition pattern-completion error rate, judging relativemerits differing objective functions clustering. suggest several objective functionsadaptations selection measures used supervised, decision-tree induction,may well dimensions simplicity error rate.2. Generating Hierarchical ClusteringsClustering form unsupervised learning partitions observations classesclusters (collectively, called clustering). objective function quality measure guidessearch, ideally clustering optimal measured objective function.hierarchical-clustering system creates tree-structured clustering, sibling clusterspartition observations covered common parent. section brie summarizessimple strategy, called hierarchical sorting, creating hierarchical clusterings.2.1 Objective Functionassume observation vector nominal values, Vij along distinct variables,Ai . measure category utility (Gluck & Corter, 1985; Corter & Gluck, 1992),X X[P (A = V jC )2 , P (A = V )2];CU (Ck ) = P (Ck )ij kijjand/or variants used extensively system known Cobweb (Fisher, 1987a)many related systems (Gennari, Langley, & Fisher, 1989; McKusick & Thompson, 1990;Iba & Gennari, 1991; McKusick & Langley, 1991; Reich & Fenves, 1991; Biswas, Weinberg,& Li, 1994; De Alte Da Veiga, 1994; Kilander, 1994; Ketterlin, Gancarski, & Korczak,1995). measure rewards clusters, Ck , increase predictability variable values148fiOptimization Hierarchical Clusteringswithin Ck (i.e., P (Ai = Vij jCk )) relative predictability population whole(i.e., P (Ai = Vij )). favoring clusters increase predictability (i.e., P (Ai = Vij jCk ) >P (Ai = Vij )), also necessarily favor clusters increase variable value predictiveness(i.e., P (Ck jAi = Vij ) > P (Ck )).Clusters many variable values predictable cohesive. Increases predictability stem shared variable values observations within cluster. clusterwell-separated decoupled clusters many variable values predictivecluster. High predictiveness stems differences variable values sharedmembers one cluster shared observations another cluster. general principle clustering increase similarity observations within clusters (i.e.,cohesion) decrease similarity observations across clusters (i.e., coupling).Category utility similar form Gini Index, used supervisedsystems construct decision trees (Mingers, 1989b; Weiss & Kulikowski, 1991).Gini Index typically intended address issue well values variable, Ai ,predict priori known class labels supervised context. summation Gini Indicesected CU addresses extent cluster predicts values variables.CU rewards clusters, Ck , reduce collective impurity variables.Fisher's (1987a) CobwebPsystem, CU used measure quality partitiondata, PU (fC1; C2; : : :CN g) = k CU (Ck )=N average category utility clusterspartition. Sections 3.5 5.2 note nonoptimalities measure partitionquality, suggest alternatives. Nonetheless, measure commonly used,take opportunity note problems, none techniques describetied measure.2.2 Structure ClustersCobweb, Autoclass (Cheeseman et al., 1988), systems (Anderson &Matessa, 1991), assume clusters, Ck , described probabilistically:variable value associated conditional probability, P (Ai = Vij jCk ), ectsproportion observations Ck exhibit value, Vij , along variable Ai . fact,variable value actually associated number observations clustervalue; probabilities computed `on demand' purposes evaluation.Probabilistically-described clusters arranged tree form hierarchical clusteringknown probabilistic categorization tree. set sibling clusters partitionsobservations covered common parent. single root cluster, identicalstructure clusters, covering observations containing frequency information necessary compute P (Ai = Vij )'s required category utility. Figure 1 givesexample probablistic categorization tree (i.e., hierarchical clustering)node cluster observations summarized probabilistically. Observations leavesdescribed three variables: Size, Color, Shape.2.3 Hierarchical Sortingstrategy initial clustering sorting, term adapted psychologicaltask requires subjects perform roughly procedure describe(Ahn & Medin, 1989). Given observation current partition, sorting evaluates149fiFisherSizeShapeColorsma 0.50squ 0.50blu 0.25med 0.25 lar 0.25sph 0.50gre 0.25 red 0.50P(C1jroot)=0.50sma 1.00squ 1.00blu 0.50 gre 0.50sma 1.00squ 1.00blu 1.00P(C3jC1)=0.50P(root)=1.0P(C2jroot)=0.50med 0.50 lar 0.50sph 1.00red 1.00sma 1.00squ 1.00gre 1.00P(C4jC1)=0.50med 1.00sph 1.00red 1.00P(C5jC2)=0.50lar 1.00sph 1.00red 1.00P(C6jC2)=0.50Figure 1: probabilistic categorization tree.quality new clusterings result placing observation existingclusters, quality clustering results creating new clustercovers new observation; option yields highest quality score (e.g., using PU )selected. clustering grows incrementally new observations added.procedure easily incorporated recursive loop builds tree-structuredclusterings: given existing hierarchical clustering, observation sorted relativetop-level partition (i.e., children root); existing child root choseninclude observation, observation sorted relative children node,serves root recursive call. leaf reached, tree extendeddownward. maximum height tree bounded, thus limiting downwardgrowth fixed depth. Figure 2 shows tree Figure 1 two new observationsadded it: one observation extends left subtree downward, secondmade new leaf deepest, existing level right subtree.sorting strategy identical used Anderson Matessa (1991). children cluster partition observations covered parent, thoughmeasure, PU , used guide sorting differs Anderson Matessa. observations stored singleton clusters leaves tree. hierarchical-sortbased strategies augment basic procedure manner described Section 3.3 (Fisher,1987a; Hadzikadic & Yun, 1989; Decaestecker, 1991).150fiOptimization Hierarchical ClusteringsSizeShapeColorP(C1jroot)=0.50sma 1.00squ 0.67blu 0.33 gre 0.67sma 1.00sma 1.00sqr 0.50squ 1.00gre 1.00blu 1.00P(C3jC1)=0.33sma 0.50squ 0.33blu 0.17med 0.33sph 0.33gre 0.33lar 0.17pyr 0.33red 0.50P(root)=1.0P(C2jroot)=0.50med 0.67 lar 0.33sph 0.67 pyr 0.33red 1.00pyr 0.33P(C4jC1)=0.67med 1.00lar 1.00med 1.00pyr 0.50pyr 1.00sph 1.00sph 1.00red 1.00red 1.00red 1.00P(C5jC2)=0.33 P(C7jC2)=0.33 P(C6jC2)=0.33sma 1.00squ 1.00gre 1.00P(C7|C4)=0.50New Objectsma 1.00pyr 1.00gre 1.00New ObjectP(C8|C4)=0.50Figure 2: updated probabilistic categorization tree.3. Iterative OptimizationHierarchical sorting quickly constructs tree-structured clustering, one typically nonoptimal. particular, control strategy suffers ordering effects: differentorderings observations may yield different clusterings (Fisher, Xu, & Zard, 1992).Thus, initial clustering phase, (possibly oine) process iterative optimizationseeks uncover better clusterings.3.1 Seed Selection, Reordering, ReclusteringMichalski Stepp's (1983a) Cluster/2 seeks optimal K-partitioning data.first step selects K random `seed' observations data. seeds `attractors'around K clusters grown remaining data. Since seed selectiongreatly impact clustering quality, Cluster/2 selects K new seeds `centroids'K initial clusters. Clustering repeated new seeds. process iteratesimprovement quality generated clusterings.151fiFisherFunction ORDER(Root)Root leaf Return(observations covered Root)Else Order children Root coveringobservations covering least.child, Ck , Root (in order) Lk ORDER(Ck )L MERGE(fLk jlist objects constructed ORDER(Ck )g)Return(L)Table 1: procedure creating `dissimilarity' ordering data.Ordering effects sorting related effects arise due differing fixed-K seedselections: initial observations ordering establish initial clusters `attract'remaining observations. general, sorting performs better initial observationsdiverse areas observation-description space, since facilitates establishment initial clusters ect different areas. Fisher, Xu, Zard (1992) showedordering data consecutive observations dissimilar based Euclidean distance led good clusterings. Biswas et al. (1994) adapted technique Iteratesystem similar results. cases, sorting used PU score described previously.procedure presumes observations appear dissimilar Euclidean distancetend placed different clusters using objective function. Taking leadCluster/2, measure-independent idea first sorts using random data ordering,extracts biased `dissimilarity' ordering hierarchical clustering, sorts again.function Table 1 outlines reordering procedure. recursively extracts listobservations probable (i.e., largest) cluster least probable,merges (i.e., interleaves) lists, exiting recursive call { step,element probable cluster placed first, followed element secondprobable, forth. Whatever measure guides clustering, observations differing clusters judged dissimilar measure. Thus, measure-independentprocedure returns measure-dependent dissimilarity ordering placing observationsdifferent clusters back-to-back.Following initial sorting, extract dissimilarity ordering, recluster, iterate,improvement clustering quality.3.2 Iterative Redistribution Single Observationscommon long-known form iterative optimization moves single observationscluster cluster search better clustering (Duda & Hart, 1973). basic strategyused one form another numerous sort-based algorithms well (Fisheret al., 1992). idea behind iterative redistribution (Biswas, Weinberg, Yang, & Koller,1991) simple: observations single-level clustering `removed' originalcluster resorted relative clustering. cluster contains one observation,cluster `removed' single observation resorted. process continuestwo consecutive iterations yield clustering.152fiOptimization Hierarchical ClusteringsHHHHHHHH#cc#,,@@cccc##,,##,@J@@@@,@JJJJJB0D0JKTJ JJ....F....C0B0CELD0G H....EK L....F....G H....JJFigure 3: Hierarchical redistribution: left subfigure indicates cluster Jremoved descendent B , thus producing D0 B 0 ,resorted relative children root (A). rightmost figureshows J placed new child C . Fisher (1995). Figure reproduced permission Proceedings First International Conferencec 1995 American AssociationKnowledge Discovery Data Mining, CopyrightArtificial Intelligence.Isodata algorithm (Duda & Hart, 1973) determines target cluster observation, actually change clustering targets observationsdetermined; point, observations moved targets, thus alteringclustering. limit sequential version, also described Duda Hart(1973), moves observation target identified sorting.strategy conceptually simple, limited ability overcome localmaxima { reclassification particular observation may true directionbetter clustering, may perceived objective function appliedclustering results resorting single observation.3.3 Iterative Hierarchical Redistributioniterative optimization strategy appears novel clustering literature iterativehierarchical redistribution. strategy rationalized relative single-observation iterative redistribution: even though moving set observations one cluster anothermay lead better clustering, movement single observation may initially reduceclustering quality, thus preventing eventual discovery better clustering. response, hierarchical redistribution considers movement observation sets, representedexisting clusters hierarchical clustering.Given existing hierarchical clustering, recursive loop examines sibling clustershierarchy depth-first fashion. set siblings, inner, iterative loop examinessibling, removes current place hierarchy (along subtree),resorts cluster relative entire hierarchy. Removal requires various153fiFishercounts ancestor clusters decremented. Sorting removed cluster done basedcluster's probabilistic description, requires minor generalization proceduresorting individual observations: rather incrementing certain variable value counts1 cluster ect addition new observation, `host' cluster's variablevalue counts incremented corresponding counts cluster classified.cluster may return original place hierarchy, Figure 3 illustrates, maysorted entirely different location.inner loop reclassifies sibling set, repeats two consecutive iterations lead set siblings. recursive loop turns attentionchildren remaining siblings. Eventually, individual observations represented leaves resorted (relative entire hierarchy) changesone iteration next. Finally, recursive loop may applied hierarchyseveral times, thus defining outermost (iterative) loop terminates changesoccur one pass next.one modification basic strategy implemented reasons cost:change subtree pass outermost loop hierarchy,subsequent passes attempt redistribute clusters subtree unlesscluster (from location hierarchy) placed subtree, thuschanging subtree's structure. addition, cases PU scores obtainedplacing cluster, C (typically singleton cluster), either two hostssame. cases, algorithm prefers placement C original host onecandidates high PU score. policy avoids infinite loops stemmingties PU score.sum, hierarchical redistribution takes large steps search better clustering. Similar macro-operator learners (Iba, 1989) problem-solving contexts, movingobservation set cluster bridges distant points clustering space, desirablechange made would otherwise viewed desirable redistribution limited movement individual observations. redistribution increasinglysmaller, granular clusters (terminating individual observations) serves increasingly refine clustering.large extent hierarchical redistribution inspired Fisher's (1987a) Cobwebsystem, fundamentally hierarchical-sort-based strategy. However, Cobwebaugmented operators merging, splitting, promotion. Merging combines two siblingclusters hierarchical clustering increases quality partitionclusters members; splitting remove cluster promote childrennext higher partition; distinct promotion operator promote individual clusternext higher level. fact, could regarded `iterative optimization' operators,keeping Cobweb's cognitive modeling motivations, cost applying`amortized' time: many observations sorted, cluster may migrate one parthierarchical clustering another collective repeated applicationmerging, splitting, promotion. similar view expressed McKusick Langley(1991), whose Arachne system differs Cobweb, part, way exploitspromotion operator. Unfortunately, Cobweb, lesser extent Arachne,merging, splitting, promotion applied locally migration hierarchylimited practice. contrast, hierarchical redistribution resorts cluster, regardless154fiOptimization Hierarchical Clusteringsinitial location tree, root entire tree, thus vigorouslypursuing migration globally evaluating merits moves.1idea hierarchical redistribution also closely related strategies foundBridger (Reich & Fenves, 1991) Hierarch (Nevins, 1995) systems. particular,Bridger identifies `misplaced' clusters hierarchical clustering using criterion specified,part, domain expert, whereas hierarchical redistribution simply uses objectivefunction. Bridger misplaced cluster removed (together subtree),cluster/subtree resorted single unit; rather, observations coveredcluster resorted individually. approach captures, part, idea hierarchicalredistribution, though resorting individual observations may escape local optimaextent hierarchical redistribution.Given existing hierarchical clustering new observation, Hierarch conductsbranch-and-bound search clustering, looking cluster `best matches'observation. best host found, clusters `vicinity' best hostreclassified using branch-and-bound respect entire hierarchy. clustersneed singletons, reclassification spawn reclassificationstermination condition reached.unclear Hierarch's procedure scales large data sets; number experimental trials size test data sets considerably less describe shortly.Nonetheless, importance bridging distant regions clustering space reclassifying observation sets en masse made explicit. Like Cobweb, Hierarch incremental,changes hierarchy triggered along path classifies new observation,changes may move many observations simultaneously, thus `amortizing' costoptimization time. contrast, hierarchical redistribution motivated philosophy sorting (or method) produce tentative clusteringdata quickly, followed iterative optimization procedures background reviseclustering intermittently. hierarchical redistribution ects many ideasimplemented Hierarch, Cobweb, related systems, appears novel iterativeoptimization strategy decoupled particular initial clustering strategy.3.4 Comparisons Iterative Optimization Strategiessection compares iterative optimization strategies two experimental conditions.first condition, random ordering observations generated hierarchicallysorted. optimization strategies applied independently resultanthierarchical clustering. experiments assume primary goal clusteringdiscover single-level partitioning data optimal quality. Thus, objectivefunction score first-level partition taken important dependent variable.independent variable height initially-constructed clustering; effectgranularity clusters used hierarchical redistribution. hierarchical clus1. Considering global changes also motivated redistribution individual observations Iterate.Nevins (1995) notes commentary experimental comparisons Iterate Cobweb(Fisher et al., 1992), even global movement single observations typically perform welllocal movement sets observations simultaneously, implemented Cobweb's mergingsplitting operators.155fiFishersortSoybean (small)reorder/resort(47 obs, 36 vars) iter. redist.hier. redist.sortSoybean (large)reorder/resort(307 obs, 36 vars) iter. redist.hier. redist.sortHousereorder/resort(435 obs, 17 vars) iter. redist.hier. redist.sortMushroomreorder/resort(1000 obs, 23 vars) iter. redist.hier. redist.Random1.53 (0.11)1.61 (0.02)1.54 (0.10)1.60 (0.05)0.89 (0.08)0.97 (0.04)0.92 (0.07)1.06 (0.02)1.22 (0.30)1.66 (0.09)1.24 (0.28)1.68 (0.00)1.10 (0.13)1.10 (0.08)1.10 (0.12)1.27 (0.00)Similarity1.08 (0.18)1.56 (0.08)1.34 (0.20)1.50 (0.08)0.66 (0.14)0.96 (0.05)0.84 (0.10)1.06 (0.01)0.83 (0.16)1.57 (0.18)1.06 (0.19)1.68 (0.00)0.73 (0.22)1.16 (0.08)0.95 (0.19)1.24 (0.10)Table 2: Iterative optimization strategies initial clusterings generated sorting random similarity ordered observations. Tree height 2. Averages standarddeviations PU scores 20 trials.tering height 2 corresponds single level partition data depth 1 (the rootdepth 0), leaves corresponding individual observations depth 2.addition experiments clusterings derived sorting random initial orderings,redistribution strategy tested exceptionally poor initial clusterings generatednonrandom orderings. `dissimilarity' orderings lead good clusterings, `similarity' orderings lead poor clusterings (Fisher et al., 1992). Intuitively, similarity orderingsamples observations within region data description space samplingobservations differing regions. reordering procedure Section 3.1 easily modified produce similarity orderings ranking set siblings hierarchical clusteringleast probable, appending rather interleaving observation listsdiffering clusters algorithm pops recursive levels. similarity orderingproduced applying procedure initial clustering produced earlier sortrandom ordering. Another clustering produced sorting similarity-ordereddata, three iterative optimization strategies applied independently.advocate one build clusterings similarity orderings practice, experimentsorderings better test robustness various optimization strategies.Table 2 shows results experiments random similarity orderings datafour databases UCI repository.2 results assume initial clusteringheight 2 (i.e., top-level partition + observations leaves). cell represents average2. reduceddata set.mushroomdata set obtained randomly selecting 1000 observations original156fiOptimization Hierarchical Clusteringsstandard deviation 20 trials. first cell (labeled `sort') domainmean PU scores initially obtained sorting. Subsequent rows domain ectmean scores obtained reordering/resorting procedure Section 3.1, iterativeredistribution single observations described Section 3.2, hierarchical redistributiondescribed Section 3.3.main findings ected Table 2 are:1. Initial hierarchical sorting random input reasonably well; PU scorescase closer scores optimized trees, poorest scores obtainedsorting similarity orderings. weakly suggests initial sortingrandom input takes substantial step space clusterings towards discoveryfinal structure.2. Hierarchical redistribution achieves highest mean PU score randomsimilarity case 3 4 domains. small soybean domain exception.3. House domain (random similarity case) Mushroom domain (randomcase only), standard deviation PU scores clusterings optimized hierarchicalredistribution 0.00, indicating always constructed level-1 partitionsPU score 20 trials.4. Reordering reclustering comes closest hierarchical redistribution's performancecases, bettering Small Soybean domain.5. Single-observation redistribution modestly improves initial sort, substantially worse two optimization methods.Note initial hierarchical clusterings height 2, differenceiterative hierarchical redistribution redistribution single observations hierarchical redistribution considers `merging' clusters partition (by reclassifying onerespect others) prior redistributing single observations passhierarchy.Section 3.3 suggested expected benefits hierarchical redistribution mightgreater deeper initial trees granular clusters. Table 3 shows resultsdomains initial orderings tree height 4 hierarchical redistribution; reader's convenience also repeat results Table 2 hierarchicalredistribution tree height 2. moving height 2 4, modest improvement small Soybean domain (particularly Similarity orderings),slight improvement large Soybean domain Mushroom domain Similarity orderings.3 improvements modest, moving height 4 trees leadsnear identical performance random similarity ordering conditions. suggestshierarchical redistribution able effectively overcome disadvantage initiallypoor clusterings.Experiments reorder/resort iterative distribution single observations alsovaried respect tree height (e.g., height 3). methods,3. standard deviation 0:00 indicates standard deviation non-0, observable2nd decimal place rounding.157fiFisherRandomheight 2height 4Soybean (small) 1.60 (0.05) 1.62 (0.00)Soybean (large) 1.06 (0.02) 1.07 (0.02)House1.68 (0.00) 1.68 (0:00)Mushroom1.27 (0.00) 1.27 (0.00)Similarityheight 2height 41.50 (0.08) 1.62 (0.00)1.06 (0.01) 1.07 (0.01)1.68 (0.00) 1.68 (0:00)1.24 (0.10) 1.27 (0.00)Table 3: Hierarchical redistribution initial clusterings generated sorting randomsimilarity ordered observations. Results shown tree heights 2 (copiedTable 2) 4. Averages standard deviations PU scores 20 trials.deepest set clusters initial hierarchy leaves, taken initialpartition. Reordering/resorting scores remained roughly height 2 condition, clusterings produced single-observation redistribution PU scoresconsiderably worse given Table 2.also recorded execution time method. Table 4 shows time requiredmethod seconds.4 particular, domain, Table 4 lists mean timeinitial sorting, mean additional time optimization method. Ironically,experiments demonstrate even though hierarchical redistribution `bottoms-out'single-observation form redistribution, former consistently faster lattertrees height 2 { reclassifying cluster simultaneously moves set observations,would otherwise repeatedly evaluated redistribution individuallyincreased time stabilization.5Table 4 assumes tree constructed initial sorting bounded height 2. Table 5gives time requirements hierarchical sorting hierarchical redistributioninitial tree bounded height 4. tree gets deeper cost hierarchicalredistribution grows substantially, comparison performance height 24 trees Table 3 suggests, drastically diminishing returns terms partitionquality. Importantly, limited experiments trees height 2, 3, 4 indicatecost hierarchical redistribution comparable cost reorder/resort greater treeheights significantly less expensive single-observation redistribution. dicultgive cost analysis hierarchical redistribution (and methods matter),since bounds loop iterations probably depend nature objective function.Suce say number nodes subject hierarchical redistributiontree covering n observations bounded 2n , 1; may n leavesn , 1 internal nodes given internal node less 2 children.iterative optimization occur background, real-time response important,cluster quality paramount, probably worth applying hierarchical redis4. Routines implemented SUN Common Lisp, compiled, run SUN 3/60.5. Similar timing results occur computational contexts well. Consider relationinsertion sort Shell sort. Shell sort's final `pass' table insertion sort limitedmoving table elements consecutive table locations time. large eciency advantageShell Sort stems fact previous passes table moved elements large distances,thus final pass, table nearly sorted.158fiOptimization Hierarchical ClusteringssortSoybean (small)reorder/resort(47 obs, 36 vars)iter. redist.hier. redist.sortSoybean (large)reorder/resort(307 obs, 36 vars) iter. redist.hier. redist.sortHousereorder/resort(435 obs, 17 vars) iter. redist.hier. redist.sortMushroomreorder/resort(1000 obs, 23 vars) iter. redist.hier. redist.Random6.98 (1.43)14.82 (2.60)9.00 (5.94)6.99 (1.28)50.62 (6.11)141.36 (46.99)166.53 (55.53)79.00 (19.23)34.99 (7.55)87.78 (23.94)177.75 (94.53)55.90 (11.92)111.47 (19.19)301.34 (100.56)162.58 (85.20)91.87 (29.50)Similarity7.21 (1.31)18.27 (6.00)15.51 (7.72)8.87 (3.58)54.09 (13.25)153.22 (43.59)307.59 (160.66)87.27 (19.64)39.15 (7.60)97.63 (29.54)320.43 (124.78)73.54 (10.05)119.33 (25.86)391.80 (211.54)390.11 (191.62)151.45 (48.89)Table 4: Time requirements (in seconds) hierarchical sorting iterative optimizationinitial clusterings generated sorting random similarity ordered observations. Tree height 2. Averages standard deviations 20 trials.tribution deeper trees; consistent philosophy behind systemsAutoclass Snob. domains examined here, however, seem costeffective optimize trees height greater 4. Thus, adopt tree construction strategy builds hierarchical clustering three levels time (with hierarchicalredistribution) experiments Section 4.3.5 Discussion Iterative Optimization Methodsexperiments demonstrate relative abilities three iterative optimization strategies,coupled PU objective function hierarchical sorting generateinitial clusterings. reorder/resort optimization strategy Section 3.1 makessense sorting primary clustering strategy, optimization techniquesstrongly tied particular initial clustering strategy. example, hierarchicalredistribution also applied hierarchical clusterings generated agglomerativestrategy (Duda & Hart, 1973; Everitt, 1981; Fisher et al., 1992), uses bottom-upprocedure construct hierarchical clusterings repeatedly `merging' observationsresulting clusters all-inclusive root cluster generated. Agglomerative methodssuffer ordering effects, greedy algorithms, susceptiblelimitations local decision making generally, would thus likely benefit iterativeoptimization.159fiFisherSoy (small) sorthier.Soy (large) sorthier.Housesorthier.Mushroom sorthier.redist.redist.redist.redist.Randomheight 2height 46.98 (1.4)18 (2)6.99 (1.3)94 (28)50.62 (6.1) 142 (10)79.00 (19.2) 436 (139)34.99 (7.6) 104 (9)55.90 (11.9) 355 (71)111.47 (19.2) 407 (64)91.87 (29.5) 1288 (458)Similarityheight 2height 47.21 (1.3)21 (2)8.87 (3.6)133 (28)54.09 (13.3) 152 (11)87.27 (19.6) 576 (260)39.15 (7.6) 120 (12)73.54 (10.1) 425 (105)119.33 (25.9) 443 (65)151.45 (48.9) 1368 (335)Table 5: Time requirements (in seconds) hierarchical sorting hierarchical redistribution initial clusterings generated sorting random similarity orderedobservations. Results shown tree heights 2 (copied Table 4) 4.Averages standard deviations 20 trials.addition, three optimization strategies applied regardless objective function. Nonetheless, relative benefits methods undoubtedly varies objective function. example, PU function undesirable characteristicmay, particular circumstances, view two partitions close formseparated `cliff' (Fisher, 1987b; Fisher et al., 1992). Consider partitionobservations involvingtwo, roughly equal-sized clusters; PU score formP2PU (fC1; C2g) = [ k=1 CU (Ck )]=2. create partition three clusters removing single observationfrom, say C2, creating new singleton cluster, C3P30PU (fC1; C2; C3g) = [ k=1 CU (Ck )]=3. relatively large, CU (C3)small score due term, P (C3) = 1=M (see Section 2.1). takingaverage CU score clusters, difference PU (fC1; C2g) PU (fC1; C20 ; C3g)may quite large, even though differ placement one observation. Thus,limiting experiments PU function may exaggerate general advantage hierarchical redistribution relative two optimization methods. statementsimultaneously positive statement robustness hierarchical redistributionface objective function cliffs, negative statement PU definingdiscontinuities. Nonetheless, PU variants adopted systems fallwithin Cobweb family (Gennari et al., 1989; McKusick & Thompson, 1990; Reich &Fenves, 1991; Iba & Gennari, 1991; McKusick & Langley, 1991; Kilander, 1994; Ketterlinet al., 1995; Biswas et al., 1994). Section 5.2 suggests alternative objective functions.Beyond nonoptimality PU , findings taken beststrategies engineered particular clustering system.could introduce forms randomization systematic variation three strategies. example, Michalski Stepp's seed-selection methodology inspires reordering/resorting, Michalski Stepp's approach selects `border' observationsselection `centroids' fails improve clustering quality one iteration next;160fiOptimization Hierarchical Clusteringsexample kind systematic variations one might introduce pursuitbetter clusterings. contrast, Autoclass may take large heuristically-guided `jumps'away current clustering. approach might be, fact, somewhat less systematic(but equally successful) variation macro-operator theme inspired hierarchical redistribution, similar Hierarch's approach well. Snob (Wallace & Dowe, 1994)employs variety search operators, including operators similar Cobweb's mergesplit (though without restrictions local application), random restart clustering process new seed observations, `redistribution' observations.6 fact,user program Snob's search strategy using differing primitive search operators.case, systems Cluster/2, Autoclass, Snob simply `give up'fail improve clustering quality one iteration next.Snob illustrates, one strategies might combined advantage.additional example, Biswas et al. (1994) adapt Fisher, Xu, Zard's (1992) dissimilarityordering strategy preorder observations prior clustering. sorting using PU ,Iterate system applies iterative redistribution single observations using categorymatch measure Fisher Langley (1990).combination preordering iterative redistribution appears yield good resultsIterate. results reorder/resort suggest preordering primarily responsible quality benefits simple sort, relative contribution Iterate'sredistribution operator certain since differs respects redistribution technique described paper.7 However, use three different measures {distance, PU , category match { clustering may unnecessary adds undesirable coupling design clustering algorithm. If, example, one wantsexperiment merits differing objective functions, undesirable worry`compatibility' function two measures. contrast, reordering/resorting generalizes Fisher et al.'s (1992) ordering strategy; generalizationiterative redistribution strategy describe assume auxiliary measures beyond objective function. fact, Fisher (1987a, 1987b), evaluation Iterate's clusteringsmade using measures variable value predictability P (Ai = Vij jCk ), predictivenessP (Ck jAi = Vij ), product. clear system need exploit several related,albeit different measures generation evaluation clusterings; undoubtedlysingle, carefully selected objective function used exclusively clustering.Reordering/resorting iterative redistribution single observations could combined manner similar Iterate's exploitation certain specializationsprocedures. results suggest reordering/resorting would put clustering good`ballpark', iterative redistribution would subsequently make modest refinements.combined strategies, sense conducted inverse `ablation' study,evaluating individual strategies isolation. limited number domains exploredSection 3.4, however, appears dicult better hierarchical redistribution.Finally, experiments applied various optimization techniques datasorted. may desirable apply optimization procedures intermittent pointssorting. may improve quality final clusterings using reordering/resorting6. Importantly, Snob (and Autoclass) assumes probabilistic assignment observations clusters.7. Iterate uses measure redistribution (Fisher & Langley, 1990) probably smoothes `cliffs',uses Isodata, non-sequential version redistribution.161fiFisherredistribution single observations, well reduce overall cost constructingfinal optimized clusterings using methods, including hierarchical redistribution,already appears quite well quality dimension. fact, Hierarchviewed performing something akin restricted form hierarchical redistributionobservation. probably extreme { iterative optimization performedoften, resultant cost outweigh savings gleaned maintaining relatively welloptimized trees throughout sorting process. Utgoff (1994) makes similar suggestionintermittent restructuring decision trees incremental, supervised induction.4. Simplifying Hierarchical Clusteringshierarchical clustering grown arbitrary height. structure data,ideally top layers clustering ect structure (and substructure onedescends hierarchy). However, lower levels clustering may ect meaningfulstructure. result overfitting, one finds supervised induction well.Inspired certain forms retrospective (or post-tree-construction) pruning decisiontree induction, use resampling identify `frontiers' hierarchical clusteringgood candidates pruning. Following initial hierarchy construction iterativeoptimization, simplification process final phase search spacehierarchical clusterings intended ease burden data analyst.4.1 Identifying Variable Frontiers ResamplingSeveral authors (Fisher, 1987a; Cheeseman et al., 1988; Anderson & Matessa, 1991) motivate clustering means improving performance task akin pattern completion,error rate completed patterns used `externally' judge utilityclustering. Given probablistic categorization tree type assumed, newobservation unknown value variable classified hierarchy usingsmall variation hierarchical sorting procedure described earlier.8 Classificationterminated selected node (cluster) along classification path, variable valuehighest probability cluster predicted unknown variable value newobservation. Naively, classification might always terminate leaf (i.e., observation),leaf's value along specified variable would predicted variable valuenew observation. use simple resampling strategy known holdout (Weiss& Kulikowski, 1991) motivated fact variable might better predictedinternal node classification path. identification ideal-prediction frontiersvariable suggests pruning strategy hierarchical clusterings.Given hierarchical clustering validation set observations, validation setused identify appropriate frontier clusters prediction variable. Figure 4illustrates preferred frontiers two variables may differ, clusters withinfrontier may different depths. variable, Ai , objects validationset classified hierarchical clustering value variable Ai`masked' purposes classification; cluster encountered classification8. Classification identical sorting except observation added clusteringstatistics node encountered sorting permanently updated ect newobservation.162fiOptimization Hierarchical Clusteringsfrontier A1A2A3........Figure 4: Frontiers three variables hypothetical clustering. Fisher (1995).Figure reproduced permission Proceedings First Internationalc 1995 AmericanConference Knowledge Discovery Data Mining, CopyrightAssociation Artificial Intelligence.observation's value Ai compared probable value Ai cluster;same, observation's value would correctly predictedcluster. count correct predictions variable clustermaintained. Following classification variables observations validationset, preferred frontier variable identified maximizes number correctcounts variable. simple, bottom-up procedure insures numbercorrect counts node variable's frontier greater equal sumcorrect counts variable set mutually-exclusive, collectively-exhaustivedescendents node.Variable-specific frontiers enable number pruning strategies. example, nodelies frontier every variable offers apparent advantage terms patterncompletion error rate; node probably ects meaningful structure (anddescendents) may pruned. However, analyst focusing attention subsetvariables, frontiers might exibly exploited pruning.4.2 Experiments Validationtest validation procedure's promise simplifying hierarchical clusterings,data sets used optimization experiments Section 3.4 randomly dividedthree subsets: 40% training, 40% validation, 20% test. hierarchicalclustering first constructed sorting training set randomized order. hierarchy optimized using iterative hierarchical redistribution. Actually, costconsiderations, hierarchy constructed several levels time. hierarchy initiallyconstructed height 4, deepest level set training observations.hierarchy optimized using hierarchical redistribution. Clusters bottommost level(i.e., 4) removed children level 3 clusters, subset training observations163fiFisherSoybean (small)LeavesAccuracyAve. Frontier SizeSoybean (large)LeavesAccuracyAve. Frontier SizeHouseLeavesAccuracyAve. Frontier SizeMushroomLeavesAccuracyAve. Frontier SizeUnvalidatedValidated18.00 (0.00)0.85 (0.01)18.00 (0.00)13.10 (1.59)0.85 (0.01)2.75 (1.17)122.00 (0.00) 79.10 (5.80)0.83 (0.02) 0.83 (0.02)122.00 (0.00) 17.01 (4.75)174.00 (0.00) 49.10 (7.18)0.76 (0.02) 0.81 (0.01)174.00 (0.00) 9.90 (5.16)400.00 (0.00) 96.30 (11.79)0.80 (0.01) 0.82 (0.01)400.00 (0.00) 11.07 (4.28)Table 6: Characteristics optimized clusterings validation. Averagestandard deviations 20 trials.covered cluster level 3 hierarchically sorted height 4 tree optimized.roots subordinate clusterings substituted cluster depth3 original tree. process repeated clusters level 3 subordinatetrees subsequent trees thereafter decomposition possible. finalhierarchy, constant-bounded height, decomposes entire training setsingleton clusters, containing single training observation. validation setused identify variable frontiers within entire hierarchy.testing validated clustering, variable test observation maskedturn; classification reaches cluster frontier masked variable,probable value predicted value observation; proportion correctpredictions variable test set recorded. comparative purposes,also use test set evaluate predictions stemming unvalidated tree,variable predictions made leaves (singleton clusters) tree.Table 6 shows results 20 experimental trials using optimized, unvalidatedvalidated clusterings generated described random orderings. first rowdomain lists average number leaves (over 20 experimental trials)unvalidated validated trees. unvalidated clusterings decompose training datasingle-observation leaves { number leaves equals number training observations.validated clustering, assume clusters pruned lie frontiersvariables. Thus, leaf validated clustering cluster (in original clustering)frontier least one variable, none descendent clusters (inoriginal clustering) frontier variable. example, assume164fiOptimization Hierarchical Clusteringstree Figure 4 covers data described terms variables A1 , A2, A3,number leaves validated clustering would 7.Prediction accuracies second row domain entry mean proportioncorrect predictions variables 20 trials. Predictions generated leaves(singleton clusters) unvalidated hierarchical clusterings appropriate variablefrontiers validated clusterings. cases, validation/pruning substantially reducesclustering size diminish accuracy.number leaves validated case, described it, assumescoarse pruning strategy; necessarily discriminate clustering uniformlydeep frontiers one single deep frontiers. suggestedexible pruning `attention' strategies might possible analyst focusingone variables. specify strategies, statistic given row3 domain entry suggests clusterings rendered considerably simplerforms analyst's attention selective. Row 3 average number frontierclusters per variable. average variables experimental trials.9validated tree Figure 4 average frontier size (1 + 4 + 6)=3 = 3:67.Intuitively, frontier cluster variable `leaf' far prediction variableconcerned. `frontier size' unvalidated clusterings simply given numberleaves, since variable predictions made unvalidated case.results suggest attention selective, partial clustering capturesstructure involving selected variables presented analyst simplified form.4.3 Discussion Validationresampling-based validation method inspired earlier work Fisher (1989),identified variable frontiers within strict incremental (i.e., sorting) context { separatevalidation set reserved, rather training set used identifying variablefrontiers well. particular, training observation hierarchically sorted usingCobweb, observation's variable values predicted `correct' countsnode updated correctly anticipated variables. Fisher (1989) variablevalues masked sorting { knowledge variable value usedsorting, thus helping guide classification, validation. addition, hierarchychanged sorting/validation. incremental strategy led desirable resultsterms pattern-completion error rate, likely variable frontiers identifiedincremental method less desirable frontiers identified holdout,strictly segregate training validation sets observations. addition Fisher(1989), work variable frontiers traced back ideas Lebowitz (1982)Kolodner (1983), directly Fisher (1987b), Fisher Schlimmer (1988),Reich Fenves (1991), use different method identify somethingsimilar spirit frontiers defined here.method validation pruning inspired retrospective pruning strategiesdecision tree induction reduced error pruning (Quinlan, 1987, 1993; Mingers,1989a). Bayesian clustering system Autoclass (Cheeseman et al., 1988),9. `standard deviations' given Row 3 actually mean standard deviationsfrontier sizes individual variables.165fiFisherminimum message length (MML) approach adopted Snob (Wallace & Dowe, 1994),expansion hierarchical clustering mediated tradeoff prior beliefexistence structure evidence data structure.detail fundamental tradeoff, suce say expansion hierarchicalclustering cease along path evidence structure datainsucient face prior bias. Undoubtedly, Bayesian MML approachesadapted identify variable-specific frontiers, thus used kind exiblepruning focusing strategies implied. fact, something similarintent implemented Autoclass (Hanson, Stutz, & Cheeseman, 1991) wayreducing cost clustering system: variables covary may `blocked',sense treated one. version Autoclass searches space hierarchicalclusterings, blocks variables assigned particular clusters hierarchy.interpretation assignments cluster `inherits' variable value distributionsvariable blocks assigned cluster's ancestors. Inversely, basic idea oneneed proceed cluster determine value distributions variables assignedcluster.experimental results suggest utility resampling validation, identification variable frontiers, pruning. However, procedure described methodper se clustering available data, since requires validation set heldinitial hierarchy construction.10 several options seem worthyexperimental evaluation adapting validation strategy tool simplificationhierarchical clusterings. One strategy would hold validation set, clustertraining set, identify variable frontiers validation set, sort validationset relative clustering. single holdout methodology problems, however,reasons similar identified single holdout supervised settings (Weiss &Kulikowski, 1991).better strategy might one akin n-fold-cross-validation: hierarchical clusteringconstructed available data, observation removed,11 usedvalidation respect variable, observation reinstated originallocation (together original variable value statistics clusters along pathlocation).5. General Discussionevaluation various strategies discussed paper ect two paradigmsvalidating clusterings. Internal validation concerned evaluating meritscontrol strategy searches space clusterings: evaluating extent searchstrategy uncovers clusterings high quality measured objective function. Internalvalidation focus Section 3.4. External validation concerned determiningutility discovered clustering relative performance task. noted10. purposes evaluating merits validation strategy terms error rate, also heldseparate test set. demonstrated point, however, would require separate testset held using resampling validation strategy.11. observation physically removed, variable value statistics clusters lie along pathroot observation decremented.166fiOptimization Hierarchical ClusteringsSoybean (small)LeavesAccuracyAve. Frontier SizeSoybean (large)LeavesAccuracyAve. Frontier SizeHouseLeavesAccuracyAve. Frontier SizeMushroomLeavesAccuracyAve. Frontier SizeUnoptimizedUnvalidated ValidatedOptimizedUnvalidated Validated18.00 (0.00)0.84 (0.18)18.00 (0.00)18.00 (0.00)0.85 (0.01)18.00 (0.00)15.35 (1.81)0.85 (0.01)3.97 (1.62)13.10 (1.59)0.85 (0.01)2.75 (1.17)122.00 (0.00) 88.55 (4.46)0.82 (0.02) 0.82 (0.02)122.00 (0.00) 24.74 (7.52)122.00 (0.00) 79.10 (5.80)0.83 (0.02) 0.83 (0.02)122.00 (0.00) 17.01 (4.75)174.00 (0.00) 68.95 (8.15)0.76 (0.02) 0.81 (0.02)174.00 (0.00) 17.72 (7.81)174.00 (0.00) 49.10 (7.18)0.76 (0.02) 0.81 (0.01)174.00 (0.00) 9.90 (5.16)400.00 (0.00) 145.50 (20.64) 400.00 (0.00) 96.30 (11.79)0.80 (0.01) 0.82 (0.01)0.80 (0.01) 0.82 (0.01)400.00 (0.00) 22.85 (8.75)400.00 (0.00) 11.07 (4.28)Table 7: Characteristics unoptimized optimized clusterings validation. Average standard deviations 20 trials.several authors point minimization error rate pattern completion genericperformance task motivates choice objective function. External validationfocus Section 4.2.section explores validation issues closely, identifies error rate simplicity (or `cost') necessary external criteria discriminating clustering utility, suggestsnumber alternative objective functions might usefully compared usingcriteria, speculates external validation criteria (taken collectively) ectreasonable criteria data analysts may use judge utility clusterings.5.1 Closer Look External Validation CriteriaIdeally, clustering quality measured objective function well correlatedclustering utility determined performance task: higher qualityclustering judged objective function, greater performance improvement(e.g., reduction error rate), lower quality, less performance improves.However, several authors (Fisher et al., 1992; Nevins, 1995; Devaney & Ram, 1993)pointed PU scores seem well-correlated error rates. precisely,hierarchical clusterings (constructed hierarchical sorting) top-level partitionlow PU score lead roughly error rates hierarchies top-levelpartition high PU score, variable-value predictions made leaves (singletonclusters). Apparently, even poor partitions level measured PU , test167fiFisherobservations classified similar observations leaves hierarchicalclustering. Pattern-completion error rate circumstances seems insucientdiscriminate might otherwise consider good poor clusterings.work simplification Section 4 suggests addition error rate, mightchoose judge competing hierarchical clusterings based simplicity similarlyintended criterion. error rate simplicity used judge classifiers supervisedcontexts. seen holdout used substantially `simplify' hierarchicalclustering. question ask whether hierarchical clusteringsoptimized relative PU simplified substantially unoptimized clusteringsdegradation pattern-completion error rate?answer question repeated validation experiments Section 4.2second experimental condition: hierarchical clusterings constructed similarityorderings observations using hierarchical sorting. saw Section 3.4 similarityorderings tend result clusterings judged poor PU function. optimizehierarchies using hierarchical optimization. Table 7 shows accuracies, numberleaves, average frontier sizes, unoptimized hierarchies constructed similarityorderings case subjected holdout-based validationcase not. results given heading `Unoptimized'.convenience, copy results Table 6 heading `Optimized'.optimized case, identifying exploiting variable frontiers unoptimizedclusterings appears simplify clustering substantially degradation error rate.interest here, however, optimized clusterings simplified substantiallygreater extent unoptimized clusterings degradation error rate.Thus far, focused criteria error rate simplicity, manyapplications, real interest simplicity stems broader interest minimizingexpected cost exploiting clustering classification: expect simplerclusterings lower expected classification costs. view various distinctionsunvalidated/validated unoptimized/optimized clusterings terms expectedclassification cost. Table 8 shows additional data obtained experimentsvalidation. particular, table shows:Leaves (L) mean number leaves (over 20 trials) validation (assuming coarse pruning strategy described Section 4.2) optimizedunoptimized cases (copied Table 7).EPL mean total path length (over 20 trials). total path length unvalidatedtree, leaf corresponds single observation, sum depthsleaves tree. case validated tree, leaf may cover multipleobservations, contribution leaf total path length depthleaf times number observations leaf.Depth (D) average depth leaf tree, computed EPLL .pBreadth (B)logaverage branching factor tree. Given B = L, B = LLB=mm.168fiOptimization Hierarchical ClusteringsSoybean (small)LeavesEPLDepthBreadthCostSoybean (large)LeavesEPLDepthBreadthCostHouseLeavesEPLDepthBreadthCostMushroomLeavesEPLDepthBreadthCostUnoptimizedUnvalidatedValidatedOptimizedUnvalidatedValidated18.00 (0.00)40.90 (3.64)2.273.578.1015.35 (1.81)31.90 (6.94)2.083.727.7418.00 (0.00)54.20 (4.74)3.012.617.8613.10 (1.59)34.50 (6.49)2.632.667.00122.00 (0.00)437.20 (34.74)3.583.8213.6888.55 (4.46)280.40 (28.07)3.174.1113.03122.00 (0.00)657.65 (28.38)5.392.4413.1579.10 (5.80)380.65 (43.63)4.812.4811.93174.00 (0.00)664.65 (41.16)3.823.8614.7568.95 (8.15)196.20 (35.32)2.854.4212.60174.00 (0.00)1005.10 (27.42)5.782.4414.1049.10 (7.18)217.25 (39.75)4.422.4110.65400.00 (0.00)2238.20 (123.63)5.602.9216.35145.50 (20.64)660.90 (117.86)4.543.0013.62400.00 (0.00)2608.85 (56.01)6.522.5116.3796.30 (11.79)503.40 (72.22)5.232.3912.50Table 8: Cost characteristics unoptimized optimized clustering validation. Average standard deviations 20 trials. Characteristicsed computed mean values `Leaves' EPL.Cost (C) expected cost classifying observation root leaf termsnumber nodes (clusters) examined classification. levelexamine cluster select best. Thus, cost product numberlevels number clusters per level. C = B D.Table 8 illustrates expected cost classification less optimized clusteringsunoptimized clusterings unvalidated validated cases. However,results taken grain salt, simply estimatedvalues. particular, expressed cost terms expected number nodeswould need examined classification. implicit assumption costexamination constant across nodes. fact, cost per examination roughly constant(per domain) across nodes implementation many others: node,variables examined. Consider measure cost, least cost (unvalidated)169fiFisherclustering one splits observations thirds node, thus forming balancedternary tree, regardless form structure data.course, tree reasonably capture structure data, mightexpect ected error rate and/or post-validation simplicity. Nonetheless,probably better measures cost available. particular, Gennari (1989) observedclassifying observation, evaluating objective function proper subsetvariables often sucient categorize observation relative clusterwould selected evaluation occurred variables. idealcircumstances, clusters partition well separated (decoupled), testing`critical' variables may sucient advance classification.Gennari implemented focusing algorithm sequentially evaluated objectivefunction variables, one additional variable time least `critical',categorization respect one clusters could made unambiguously.Using Gennari's procedure, examination cost constant across nodes.12 CarnesFisher (Fisher, Xu, Carnes, Reich, Fenves, Chen, Shiavi, Biswas, & Weinberg, 1993) adaptedGennari's procedure good effect diagnosis task, intent minimizenumber probes necessary diagnose fault. Gennari offers principledfocusing strategy used conjunction objective function, generalidea focusing selected features classification traced back Unimem(Lebowitz, 1982, 1987) Cyrus (Kolodner, 1983).results Table 8 illustrate form expected classification-cost analysis,might also measured cost time directly using test set. fact, comparisonstime requirements sorting random similarity ordering conditionsTables 4 5 suggest cost differences good poor clusterings termstime well. Regardless form analysis, however, seems desirable oneexpress branching factor cost terms number variables need testedassuming focusing strategy Gennari's. likely tend makebetter distinctions clusterings.5.2 Evaluating Objective Functions: Getting Bang Buckresults Section 5.1 suggest PU function useful identifying structuredata: clusterings optimized relative function simpler accurateclusterings optimized relative function. Thus, PU leads somethingreasonable along error rate simplicity dimensions, objective functionsbetter job along dimensions? Based earlier discussion limitationsPU , notably averaging CU clusters partition introduced `cliffs'space partitions, likely better objective functions found. example,might consider Bayesian variants like found Autoclass (Cheeseman et al., 1988)Anderson Matessa's (1991) system, closely related MML approach Snob(Wallace & Dowe, 1994). evaluate alternative measures here,suggest number candidates.12. fact, cost constant across observations, even classified along exactlypath { number variables one need test depends observation's values along previouslyexamined variables.170fiOptimization Hierarchical ClusteringsSection 2.1 noted CU function could viewed summation GiniIndices, measured collective impurity variables conditioned cluster membership. Intuition may helped information-theoretic analog CU (Corter& Gluck, 1992):P (Ck )X X[P (Aj= Vij jCk ) log2 P (Ai = Vij jCk ) , P (Ai = Vij ) log2 P (Ai = Vij )]:information-theoretic analog understood summation information gainvalues, information gain often used selection criterion decision tree induction(Quinlan, 1986): clustering analog rewards clusters, Ck , maximize suminformation gains individual variables, Ai .Gini Information Gain measures often-used bases selection measures decision tree induction. used measure expected decrease impurityuncertainty class label, conditioned knowledge given variable's value.clustering context, interested decrease impurity variable's valueconditioned knowledge cluster membership { thus, use summation suitableGini Indices alternatively, information gain scores. However, well knowncontext decision tree induction, measures biased select variableslegal values. Thus, various normalizations measures different measuresaltogether, devised.clustering adaptation measures normalizaPNtion also necessary, since k=1 CU alone information-theoretic analog favorclustering greatest cardinality, data partitioned singleton clusters,one observation. Thus, PU normalizes sum Gini indices averaging.general observation many selection measures used decision tree inductionadapted objective functions clustering. number selectionmeasures suggest candidates clustering, normalizationprincipled averaging. Two candidates Quinlan's (1986) Gain RatioLopez de Mantaras' (1991) normalized information gain.13Pj P (Ai=Vij ) Pk [P (Ck jAi =Vij ) log P (Ck jAi =Vij ),P (Ck )log P (Ck )]P(Quinlan, 1986), j P (Ai =Vij ) log P (Ai =Vij )Pj P (Ai=Vij ) Pk [P (Ck jAi =Vij ) log P (Ck jAi =Vij ),P (Ck )log P (Ck )]PP(Lopez de Mantaras, 1991), j k P (Ck ^Ai =Vij )log P (Ck ^Ai =Vij )222222derive two objective functions clustering:Pi Pk P (Ck ) Pj [P (Ai=Vij jCk )logP P (Ai =Vij jCk ),P (Ai =Vij ) log P (Ai=Vij )]2, k P (Ck )log2 P (Ck )2Pi Pk P (Ck ) Pj [P (PAi=VPij jCk )log P (Ai =Vij jCk ),P (Ai =Vij ) log P (Ai=Vij )]2, k j P (Ai =Vij ^Ck )log2 P (Ai =Vij ^Ck )213. Jan Hajek independently pointed relationship C U measure Gini Index,made suggestions one might select one another normalizations above.171fiFisherlatter clustering variations defined Fisher Hapanyengwi (1993).nonsystematic experimentation Lopez de Mantaras' normalized information gain variant suggests mitigates problems associated PU , though conclusionsmerits must await experimentation. general, wealth promisingobjective functions based decision tree selection measures might consider.described two, others Fayyad's (1991) ORT function.relationship supervised unsupervised measures also pointedcontext Bayesian systems (Duda & Hart, 1973). Consider Autoclass (Cheeseman et al., 1988), searches probable clustering, H , given availabledata set, { i.e., clustering highest P (H jD) / P (DjH )P (H ). independence assumptions made Autoclass, computation P (DjH ) includes easilyseen mechanisms simple Bayes classifier used supervised contexts.compared proposed derivations decision tree selection measuresBayesian/MML measures Autoclass Snob yet, proposed patterncompletion error rate, simplicity, classification cost external, objective criteriacould used comparisons. advantage Bayesian MML approachesthat, proper selection prior biases, require separate strategy (e.g.,resampling) pruning, strategies adapted variable frontier identification. Rather, objective function used cluster formation serves cease hierarchicaldecomposition well. Though know experimental studies BayesianMML techniques along accuracy cost dimensions outlined here, expectwould perform quite well.5.3 Final Comments External Validation Criteriaproposal external validation criteria clustering error rate classificationcost stem larger, often implicit, long-standing bias AI learningsystems serve ends artificial autonomous agent. Certainly, Cobwebfamily systems trace ancestry systems Unimem (Lebowitz, 1982)Cyrus (Kolodner, 1983) autonomous agency primary theme,Fisher (1987a); Anderson Matessa's (1991) work expresses similar concerns.short, view clustering means organizing memory observationsautonomous agent begs question agent's tasks memory organizationintended support? Pattern completion error rate simplicity/cost seem obviouscandidate criteria.However, underlying assumption article criteria also appropriate externally validating clusterings used data analysis contexts, clusteringexternal human analyst, nonetheless exploited analyst purposeshypothesis generation. Traditional criteria cluster evaluation contexts includemeasures intra-cluster cohesion (i.e., observations within clusters similar) inter-cluster coupling (i.e., observations differing clusters dissimilar).criteria proposed article traditional criteria certainly related. Considerfollowing derivation portion category utility measure, beginsexpected number variable values correctly predicted given predictionguided clustering fC1; C2; :::; CN g:172fiOptimization Hierarchical ClusteringsE (# Pcorrect variable predictionsjfC1 ; C2; :::; CN g)= Pk P (Ck )EP(# correct variable predictionsjCk )= Pk P (Ck ) Pi EP(# correct predictions variable Ai jCk)= Pk P (Ck ) Pi Pj P (Ai = Vij jCk )E (# times Vij correct prediction Ai jCk )= Pk P (Ck ) Pi Pj P (Ai = Vij jCk ) P (Ai = Vij jCk )= k P (Ck ) j P (Ai = Vij jCk )2final steps derivation assume variable value predicted probabilityP (Ai = Vij jCk) probability prediction correct { i.e.,derivation category utility assumes probability matching prediction strategy (Gluck &Corter, 1985; Corter & Gluck, 1992).14 favoring partitions improve prediction alongmany variables, hierarchical clustering using category utility tends result hierarchiesvariable frontiers, described Section 4.1, near top clustering;tends reduce post-validation classification cost.Thus, category utility motivated measure rewards cohesion withinclusters decoupling across clusters noted Section 2.1, measure motivateddesire reduce error rate (and indirectly, classification cost). general, measuresmotivated desire reduce error rate also favor cohesion decoupling; stemstwo aspects pattern-completion task (Lebowitz, 1982; Medin, 1983). First,assign observation cluster based known variable values observation,best facilitated variable value predictiveness high across many variables (i.e.,clusters decoupled).15 assigned observation cluster, use cluster'sdefinition predict values variables known observation'sdescription. process successful many variables predictable clusters(i.e., clusters cohesive). fact, designing measures cohesion decouplingmind undoubtedly results useful clusterings purposes pattern completion, whetherexplicit goal designer.external validation criteria error rate cost well correlated traditionalcriteria cohesion coupling, use former criteria all? part,stems AI machine learning bias systems designed evaluatedspecific performance task mind. addition, however, plethora measuresassessing cohesion coupling found, system assessed relativevariant. variation make dicult assess similarities differences acrosssystems. article suggests pattern-completion error rate cost relatively unbiasedalternatives comparative studies. Inversely, use direct measures errorrate classification cost (e.g., using holdout) `objective function' guide searchspace clusterings? expensive. Thus, use cheaply computedobjective function designed external error rate cost evaluation mind;undoubtedly, objective function ects cohesion coupling.14. Importantly, prediction Cobweb actually performed using probability maximizing strategy {frequent value variable cluster always predicted. Fisher (1987b) discusses advantageconstructing clusters implicit probability matching strategy, even cases clustersexploited probability maximizing strategy.15. MML Bayesian approaches Snob Autoclass support probabilistic assignment observations clusters, importance decoupling cohesion remain.173fiFishercourse, computed error rate identified variable frontiers given simplifiedperformance task: variable independently masked predicted test observations. unreasonable generic method computing error rate, differentdomains may suggest different computations, since often many variables simultaneouslyunknown and/or analyst may interested subset variables. addition,proposed simplicity (i.e., number leaves) expected classification costexternal validation criteria. Section 5.1 suggests one latter criteria probablynecessary, addition error rate, discriminate `good' `poor' clusterings judgedobjective function. general, desirable realizations error rate, simplicity,cost likely vary domain interpretation tasks analyst.short, analyst's task largely one making inferences clustering,error-rate cost components (i.e., information analyst gleanclustering much work required part analyst extractinformation). probably case expressed componentsprecisely way cognitively-implemented analyst. Nonetheless,article others (Fisher, 1987a; Cheeseman et al., 1988; Anderson & Matessa, 1991)viewed attempts formally, tentatively describe analyst's criteria clusterevaluation, based criteria might prescribe autonomous, artificial agentconfronted much task.5.4 Issuesmany important issues clustering address depth. Onepossible advantage overlapping clusters (Lebowitz, 1987; Martin & Billman, 1994).assumed tree-structured clusterings, store observationone cluster, clusters related proper subset-of relation one descendspath tree. many cases, lattices (Levinson, 1984; Wilcox & Levinson, 1986;Carpineto & Romano, 1993), generally, directed acyclic graphs (DAG) maybetter representation scheme. structures allow observation includedmultiple clusters, one cluster need subset another. such, maybetter provide analyst multiple perspectives data. example, animalspartitioned clusters corresponding mammals, birds, reptiles, etc., maypartitioned clusters corresponding carnivores, herbivores, omnivores. tree wouldrequire one partitions (e.g., carnivore, etc.) `subordinate' (e.g.,mammals, birds, etc.); Classes subordinate partition would necessarily `distributed'across descendents (e.g., carnivorous-mammal, omnivorous-mammal, carnivorous-reptile,etc.) top level clusters, ideally would represent clusters partition.DAG allows perspectives coexist relative equality, thus making perspectivesexplicit analyst.also assumed variables nominally valued. numerousadaptations basic PU function, functions, discretization strategies accommodate numeric variables (Michalski & Stepp, 1983a, 1983b; Gennari et al., 1989; Reich& Fenves, 1991; Cheeseman et al., 1988; Biswas et al., 1994). basic sorting procedureiterative optimization techniques used data described whole partnumerically-valued variables regardless approach one takes. identification174fiOptimization Hierarchical Clusteringsnumeric variable frontiers using holdout done using mean value variable node generating predictions, identifying variable's frontier setclusters collectively minimize measure error mean-squared error.6. Concluding Remarkspartitioned search space hierarchical clusterings threephases. phases, together opinion desirable characteristicsdata analysis standpoint, (1) inexpensive generation initial clustering suggests form structure data (or absence), (2) iterative optimization (perhapsbackground) clusterings better quality, (3) retrospective simplification generated clusterings. evaluated three iterative optimization strategies operateindependent objective function. these, varying degrees, inspired previousresearch, hierarchical redistribution appears novel iterative optimization techniqueclustering; also appears quite well.Another novel aspect work use resampling means validating clusters simplifying hierarchical clusterings. experiments Section 5 indicateoptimized clusterings provide greater data compression unoptimized clusterings.surprising, given PU compresses data reasonable manner; whether`optimally' though another issue.made several recommendations research.1. suggested experiments alternative objective functions, including BayesianMML measures, inspired variable-selection measures decision tree induction.2. may cost quality benefits applying optimization strategies intermittent points hierarchical sorting.3. holdout method identifying variable frontiers pruning suggests strategyakin n-fold-cross validation clusters data, still identifyingvariable frontiers facilitating pruning.4. Analyses classification cost purposes external validation probably bestexpressed terms expected number variables using focusing methodGennari's.sum, paper proposed criteria internal external validation,made experimental comparisons various approaches along dimensions. Ideally, researchers explore objective functions, search control strategies, pruningtechniques, kind experimental comparisons (particularly along external criteriaerror rate, simplicity, classification cost) de rigueur comparisonssupervised systems, become prominent unsupervised contexts.175fiFisherAcknowledgementsthank Sashank Varma, Arthur Nevins, Diana Gordon comments paper.reviewers editor supplied extensive helpful comments. work supportedgrant NAG 2-834 NASA Ames Research Center. abbreviated discussionarticle's results appear Fisher (1995), published AAAI Press.ReferencesAhn, W., & Medin, D. L. (1989). two-stage categorization model family resemblancesorting.. Proceedings Eleventh Annual Conference Cognitive ScienceSociety, pp. 315{322. Ann Arbor, MI: Lawrence Erlbaum.Anderson, J. R., & Matessa, M. (1991). iterative Bayesian algorithm categorization.Fisher, D., Pazzani, M., & Langley, P. (Eds.), Concept formation: KnowledgeExperience Unsupervised Learning. San Mateo, CA: Morgan Kaufmann.Biswas, G., Weinberg, J., & Li, C. (1994). Iterate: conceptual clustering methodknowledge discovery databases. Braunschweig, B., & Day, R. (Eds.), InnovativeApplications Artificial Intelligence Oil Gas Industry. Editions Technip.Biswas, G., Weinberg, J. B., Yang, Q., & Koller, G. R. (1991). Conceptual clusteringexploratory data analysis. Proceedings Eighth International MachineLearning Workshop, pp. 591{595. San Mateo, CA: Morgan Kaufmann.Carpineto, C., & Romano, G. (1993). Galois: order-theoretic approach conceptualclustering. Proceedings Tenth International Conference Machine Learning,pp. 33{40. Amherst, MA: Morgan Kaufmann.Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. (1988). AutoClass:Bayesian classification system. Proceedings Fifth International MachineLearning Conference, pp. 54{64. Ann Arbor, MI: Morgan Kaufmann.Corter, J., & Gluck, M. (1992). Explaining basic categories: feature predictabilityinformation. Psychological Bulletin, 111, 291{303.De Alte Da Veiga, F. (1994). Data Analysis Biomedical Research: Novel Methodological Approach Implementation Conceptual Clustering Algorithm (inPortuguese). Ph.D. thesis, Universidade de Coimbra, Unidade de Biomatematica eInformatica Medica da Faculdade de Medicina.Decaestecker, C. (1991). Description contrasting incremental concept formation. Kodratoff, Y. (Ed.), Machine Learning { EWSL-91, No. 482, Lecture Notes ArtificialIntelligence, pp. 220{233. Springer-Verlag.Devaney, M., & Ram, A. (1993). Personal communication, oct. 1993..Duda, R. O., & Hart, P. E. (1973). Pattern Classification Scene Analysis. New York,NY: Wiley Sons.176fiOptimization Hierarchical ClusteringsEveritt, B. (1981). Cluster Analysis. London: Heinemann.Fayyad, U. (1991). Induction Decision Trees Multiple Concept Learning. Ph.D.thesis, University Michigan, Ann Arbor, MI: Department Computer ScienceEngineering.Fisher, D. (1995). Optimization simplification hierarchical clusterings. ProceedingsFirst International Conference Knowledge Discovery Data Mining, pp.118{123. Menlo Park, CA: AAAI Press.Fisher, D., & Hapanyengwi, G. (1993). Database management analysis tools machineinduction. Journal Intelligent Information Systems, 2, 5{38.Fisher, D., Xu, L., Carnes, J., Reich, Y., Fenves, S., Chen, J., Shiavi, R., Biswas, G., &Weinberg, J. (1993). Applying AI clustering engineering tasks. IEEE Expert, 8,51{60.Fisher, D., Xu, L., & Zard, N. (1992). Ordering effects clustering. ProceedingsNinth International Conference Machine Learning, pp. 163{168. San Mateo, CA:Morgan Kaufmann.Fisher, D. H. (1987a). Knowledge acquisition via incremental conceptual clustering. Machine Learning, 2, 139{172.Fisher, D. H. (1987b). Knowledge Acquisition via Incremental Conceptual Clustering. Ph.D.thesis, University California, Irvine, CA: Department Information ComputerScience.Fisher, D. H. (1989). Noise-tolerant conceptual clustering. Proceedings International Joint Conference Artificial Intelligence, pp. 825{830. Detroit, MI: MorganKaufmann.Fisher, D. H., & Langley, P. (1990). structure formation natural categories.Bower, G. H. (Ed.), Psychology Learning Motivation, Vol. 25. San Diego,CA: Academic Press.Fisher, D. H., & Schlimmer, J. (1988). Concept simplification prediction accuracy.Proceedings Fifth International Conference Machine Learning, pp. 22{28.Ann Arbor, MI: Morgan Kaufmann.Gennari, J. (1989). Focused concept formation. Proceedings Sixth InternationalWorkshop Machine Learning, pp. 379{382. San Mateo, CA: Morgan Kaufmann.Gennari, J., Langley, P., & Fisher, D. (1989). Models incremental concept formation.Artificial Intelligence, 40, 11{62.Gluck, M. A., & Corter, J. E. (1985). Information, uncertainty, utility categories.Proceedings Seventh Annual Conference Cognitive Science Society, pp.283{287. Hillsdale, NJ: Lawrence Erlbaum.177fiFisherHadzikadic, M., & Yun, D. (1989). Concept formation incremental conceptual clustering.Proceedings International Joint Conference Artificial Intelligence, pp. 831{836. San Mateo, CA: Morgan Kaufmann.Hanson, R., Stutz, J., & Cheeseman, P. (1991). Bayesian classification correlationinheritance. Proceedings 12th International Joint Conference ArtificialIntelligence, pp. 692{698. San Mateo, CA: Morgan Kaufmann.Iba, G. (1989). heuristic approach discovery macro operators. Machine Learning,3, 285{317.Iba, W., & Gennari, J. (1991). Learning recognize movements. Fisher, D., Pazzani, M.,& Langley, P. (Eds.), Concept Formation: Knowledge Experience UnsupervisedLearning. San Mateo, CA: Morgan Kaufmann.Ketterlin, A., Gancarski, P., & Korczak, J. (1995). Hierarchical clustering compositeobjects variable number components. Preliminary papers FifthInternational Workshop Artificial Intelligence Statistics, pp. 303{309.Kilander, F. (1994). Incremental Conceptual Clustering On-Line Application. Ph.D.thesis, Stockholm University, Stockholm, Sweden: Department Computer Systems Sciences.Kolodner, J. L. (1983). Reconstructive memory: computer model. Cognitive Science, 7,281{328.Lebowitz, M. (1982). Correcting erroneous generalizations. Cognition Brain Theory,5, 367{381.Lebowitz, M. (1987). Experiments incremental concept formation: Unimem. MachineLearning, 2, 103{138.Levinson, R. (1984). self-organizing retrieval system graphs. ProceedingsNational Conference Artificial Intelligence, pp. 203{206. San Mateo, CA: MorganKaufmann.Lopez de Mantaras, R. (1991). distance-based attribute selection measure decisiontree induction. Machine Learning, 6, 81{92.Martin, J., & Billman, D. (1994). Acquiring combining overlapping concepts. MachineLearning, 16, 121{155.McKusick, K., & Langley, P. (1991). Constraints tree structure concept formation.Proceedings International Joint Conference Artificial Intelligence, pp.810{816. San Mateo, CA: Morgan Kaufmann.McKusick, K., & Thompson, K. (1990). Cobweb/3: portable implementation (Tech. Rep.No. FIA-90-6-18-2). Moffett Field, CA: AI Research Branch, NASA Ames ResearchCenter.178fiOptimization Hierarchical ClusteringsMedin, D. (1983). Structural principles categorization. Tighe, T., & Shepp, B.(Eds.), Perception, Cognition, Development, pp. 203{230. Hillsdale, NJ: LawrenceErlbaum.Michalski, R. S., & Stepp, R. (1983a). Automated construction classifications: conceptualclustering versus numerical taxonomy. IEEE Transactions Pattern AnalysisMachine Intelligence, 5, 219{243.Michalski, R. S., & Stepp, R. (1983b). Learning observation: conceptual clustering.Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning:Artificial Intelligence Approach. San Mateo, CA: Morgan Kaufmann.Mingers, J. (1989a). empirical comparison pruning methods decision-tree induction. Machine Learning, 4, 227{243.Mingers, J. (1989b). empirical comparison selection measures decision-tree induction. Machine Learning, 3, 319{342.Nevins, A. J. (1995). branch bound incremental conceptual clusterer. MachineLearning, 18, 5{22.Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81{106.Quinlan, J. R. (1987). Simplifying decision trees. International Journal Man-machineStudies, 27, 221{234.Quinlan, J. R. (1993). C4.5: Programs Machine Learning. San Mateo, CA: MorganKaufmann.Reich, Y., & Fenves, S. (1991). formation use abstract concepts design.Fisher, D., Pazzani, M., & Langley, P. (Eds.), Concept Formation: KnowledgeExperience Unsupervised Learning. San Mateo, CA: Morgan Kaufmann.Utgoff, P. (1994). improved algorithm incremental induction decision trees.Proceedings Eleventh International Conference Machine Learning, pp. 318{325. San Mateo, CA: Morgan Kaufmann.Wallace, C. S., & Dowe, D. L. (1994). Intrinsic classification MML - Snob program.Proceedings 7th Australian Joint Conference Artificial Intelligence, pp.37{44. UNE, Armidale, NSW, Australia: World Scientific.Weiss, S., & Kulikowski, C. (1991). Computer Systems Learn. San Mateo, CA: MorganKaufmann.Wilcox, C. S., & Levinson, R. A. (1986). self-organized knowledge base recall, design,discovery organic chemistry. Pierce, T. H., & Hohne, B. A. (Eds.), ArtificialIntelligence Applications Chemistry. Washington, DC: American Chemical Society.179fiJournal Artificial Intelligence Research 4 (1996) 61|76Submitted 11/95; published 3/96Mean Field Theory Sigmoid Belief NetworksLawrence K. SaulTommi JaakkolaMichael I. JordanCenter Biological Computational LearningMassachusetts Institute Technology79 Amherst Street, E10-243Cambridge, 02139lksaul@psyche.mit.edutommi@psyche.mit.edujordan@psyche.mit.eduAbstractdevelop mean field theory sigmoid belief networks based ideas statisticalmechanics. mean field theory provides tractable approximation true probability distribution networks; also yields lower bound likelihood evidence. demonstrate utility framework benchmark problem statistical pattern recognition|theclassification handwritten digits.1. IntroductionBayesian belief networks (Pearl, 1988; Lauritzen & Spiegelhalter, 1988) provide rich graphicalrepresentation probabilistic models. nodes networks represent random variables,links represent causal uences. associations endow directed acyclic graphs (DAGs)precise probabilistic semantics. ease interpretation afforded semantics explainsgrowing appeal belief networks, widely used models planning, reasoning,uncertainty.Inference learning belief networks possible insofar one eciently compute (orapproximate) likelihood observed patterns evidence (Buntine, 1994; Russell, Binder, Koller,& Kanazawa, 1995). exist provably ecient algorithms computing likelihoods beliefnetworks tree chain-like architectures. practice, algorithms also tend performwell general sparse networks. However, networks nodes many parents,exact algorithms slow (Jensen, Kong, & Kjaefulff, 1995). Indeed, large networksdense layered connectivity, exact methods intractable require summingexponentially large number hidden states.One approach dealing networks use Gibbs sampling (Pearl, 1988),stochastic simulation methodology roots statistical mechanics (Geman & Geman, 1984).approach paper relies different tool statistical mechanics|namely, mean fieldtheory (Parisi, 1988). mean field approximation well known probabilistic modelsrepresented undirected graphs|so-called Markov networks. example, Boltzmannmachines (Ackley, Hinton, & Sejnowski, 1985), mean field learning rules shown yieldtremendous savings time computation sampling-based methods (Peterson & Anderson,1987).main motivation work extend mean field approximation undirectedgraphical models directed counterparts. Since belief networks transformed Markovnetworks, mean field theories Markov networks well known, natural asknew framework required all. reason probabilistic models compactrepresentations DAGs may unwieldy representations undirected graphs. shall see,avoiding complexity working directly DAGs requires extension existing methods.paper focus sigmoid belief networks (Neal, 1992), resulting meanfield theory straightforward. networks binary random variables whose localc 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiSaul, Jaakkola, & Jordanconditional distributions based log-linear models. develop mean field approximationnetworks use compute lower bound likelihood evidence. method appliesarbitrary partial instantiations variables networks makes restrictionsnetwork topology. Note lower bound available, learning procedure maximizelower bound; useful true likelihood cannot computed eciently. similarapproximation models continous random variables discussed Jaakkola et al (1995).idea bounding likelihood sigmoid belief networks introduced relatedarchitecture known Helmholtz machine (Hinton, Dayan, Frey, & Neal 1995). fundamentaladvance work establish framework approximation especially conducivelearning parameters layered belief networks. close connection ideamean field approximation statistical mechanics, however, developed.paper hope elucidate connection, also convey senseapproximations likely generate useful lower bounds while, time, remaininganalytically tractable. develop perhaps simplest approximation beliefnetworks, noting sophisticated methods (Jaakkola & Jordan, 1996a; Saul & Jordan, 1995)also available. emphasized approximations form required handlemultilayer neural networks used statistical pattern recognition. networks, exactalgorithms hopelessly intractable; moreover, Gibbs sampling methods impractically slow.organization paper follows. Section 2 introduces problems inferencelearning sigmoid belief networks. Section 3 contains main contribution paper:tractable mean field theory. present mean field approximation sigmoid beliefnetworks derive lower bound likelihood instantiated patterns evidence. Section 4looks mean field algorithm learning parameters sigmoid belief networks.algorithm, give results benchmark problem pattern recognition|the classificationhandwritten digits. Finally, section 5 presents conclusions, well future issues research.2. Sigmoid Belief Networksgreat virtue belief networks clearly exhibit conditional dependenciesunderlying probability model. Consider belief network defined binary random variables= (S1 ; S2 ; : : : ; SN ). denote parents Si pa(Si ) fS1 ; S2; : : : Si,1 g; smallestset nodesP (Si jS1; S2 ; : : : ; Si,1) = P (Sijpa(Si )):(1)sigmoid belief networks (Neal, 1992), conditional distributions attached nodebased log-linear models. particular, probability ith node activated given10XJij Sj + hi ;P (Si = 1jpa(Si )) = @j(2)Jij hi weights biases network,1(z ) =(3)1 + e,zsigmoid function shown Figure 1. sigmoidbelief networks, Jij = 0 Sj 62 pa(Si );moreover, Jij = 0 j since network's structure directed acyclic graph.sigmoid function eq. (2) provides compact parametrization conditional probabilitydistributions1 eq. (2) used propagate beliefs. particular, P (Sijpa(Si )) depends pa(Si )sum weighted inputs, weights may viewed parameters1. relation noisy-OR models discussed appendix A.62fiMean Field Theory Sigmoid Belief Networks10.80.6(z)0.40.206420z246Figure 1: Sigmoid function (z ) = [1 + e,z ],1. z sum weighted inputs node ,P (S = 1jz ) = (z ) conditional probability node activated.logistic regression (McCullagh & Nelder, 1983). conditional probability distribution Si maysummarized as:hPexpJ+hSiijjhjPi:(4)P (Sijpa(Si )) =1 + exp j Jij Sj + hiNote substituting Si = 1 eq. (4) recovers result eq. (2). Combining eqs. (1) (4),may write joint probability distribution variables network as:P (S )=P (Si jpa(Si ))i98 h< exp Pj Jij Sj + hi Si ==: 1 + exp hPj Jij Sj + hii ; :(5)(6)denominator eq. (6) ensures probability distribution normalized unity.turn problem inference sigmoid belief networks. Absorbing evidence dividesunits belief network two types, visible hidden. visible units (or \evidencenodes") instantiated values; hidden unitsnot. possible ambiguity, use H V denote subsets hiddenvisible units. Using Bayes' rule, inference done conditional distributionP (H; V )P (H jV ) =;(7)P (V )XP (V ) =P (H; V )(8)Hlikelihood evidence V . principle, likelihood may computed summing2jH j configurations hidden units. Unfortunately, calculation intractable large,densely connected networks. intractability presents major obstacle learning parametersnetworks, nearly procedures statistical estimation require frequent estimateslikelihood. calculations exact probabilistic inference beset diculties.63fiSaul, Jaakkola, & JordanUnable compute P (V ) work directly P (H jV ), resort approximationstatistical physics known mean field theory.3. Mean Field Theorymean field approximation appears multitude guises physics literature; indeed,\almost old statistical mechanics" (Itzykson & Drouffe, 1991). Let us brie explainacquired name ubiquitous. physical models described Markov networks,variablesP Si represent localized magnetic moments (e.g., sites crystal lattice),sums j Jij Sj + hi represent local magnetic fields. Roughly speaking, certain cases centrallimit theorem may applied sums, useful approximation ignore uctuationsfields replace mean value|hence name, \mean field" theory.models, excellent approximation; others, poor one. simplicity, however,widely used first step understanding many types physical phenomena.Though explains philological origins mean field theory, fact many waysderive amounts approximation (Parisi, 1988). paper presentformulation appropriate inference learning graphical models. particular, viewmean field theory principled method approximating intractable graphical modeltractable one. done via variational principle chooses parameters tractablemodel minimize entropic measure error.basic framework mean field theory remains directed graphs, thoughfound necessary introduce extra mean field parameters addition usual ones.Markov networks, one finds set nonlinear equations mean field parameterssolved iteration. practice, found iteration converge fairly quickly scalewell large networks.Let us return problem posed end last section. foundmany belief networks, intractable decompose joint distribution P (S ) = P (H jV )P (V ),P (V ) likelihood evidence V . purposes probabilistic modeling, meanfield theory two main virtues. First, provides tractable approximation, Q(H jV ) P (H jV ),conditional distributions required inference. Second, provides lower boundlikelihoods required learning.Let us first consider origin lower bound. Clearly, approximating distributionQ(H jV ), equality:ln P (V ) = lnXP (H; V )H= lnXHQ(H jV )P (H; V )Q(H jV )(9):(10)obtain lower bound, apply Jensen's inequality (Cover & Thomas, 1991), pushinglogarithm sum hidden states expectation:XV)ln P (V ) Q(H jV ) ln PQ((H;:(11)H jV )Hstraightforward verify difference left right hand side eq. (11)Kullback-Leibler divergence (Cover & Thomas, 1991):Q(H jV )XKL(QjjP ) = Q(H jV ) ln P (H jV ) :(12)HThus, better approximation P (H jV ), tighter bound ln P (V ).64fiMean Field Theory Sigmoid Belief NetworksAnticipating connection statistical mechanics, refer Q(H jV ) mean fielddistribution. natural divide calculation bound two components,particular averages approximating distribution. components mean fieldentropy energy; overall bound given difference:ln P (V ) ,XHQ(H jV ) ln Q(H jV )!, ,XH!Q(H jV ) ln P (H; V ) :(13)terms physical interpretations. first measures amount uncertainty meanfield distribution follows standard definition entropy. second measures averagevalue2 , ln P (H; V ); name \energy" arises interpreting probability distributionsbelief networks Boltzmann distributions3 unit temperature. case, energynetwork configuration given (up constant) minus logarithm probabilityBoltzmann distribution. sigmoid belief networks, energy form, ln P (H; V ) = ,XijJij Si Sj,X2013X 4Xhi Si +ln 1 + exp @ Jij Sj + hi A5 ;j(14)follows eq. (6). first two terms equation familiar Markov networkspairwise interactions (Hertz, Krogh, & Palmer, 1991); last term peculiar sigmoid beliefnetworks. Note overall energy neither linear function weights polynomialfunction units. price pay sigmoid belief networks identifying P (H jV )Boltzmann distribution log-likelihood P (V ) partition function. Noteidentification made implicitly form eqs. (7) (8).bound eq. (11) valid probability distribution Q(H jV ). make use it,however, must choose distribution enables us evaluate right hand side eq. (11).Consider factorized distributionQ(H jV ) =2HSi (1 , )1,Si ;(15)binary hidden units fSi gi2H appear independent Bernoulli variables adjustablemeans . mean field approximation obtained substituting factorized distribution,eq. (15), true Boltzmann distribution, eq. (7). may seem approximation replacesrich probabilistic dependencies P (H jV ) impoverished assumption complete factorizability. Though true degree, reader keep mind valueschoose fi gi2H (and hence statistics hidden units) depend evidence V .best approximation form, eq. (15), found choosing mean values, figi2H ,minimize Kullback-Leibler divergence, KL(QjjP ). equivalent minimizing gaptrue log-likelihood, ln P (V ), lower bound obtained mean field theory.2. similar average performed E-step EM algorithm (Dempster, Laird, & Rubin, 1977); differenceaverage performed mean field distribution, Q(H jV ), rather true posterior,P (H jV ). related discussion, see Neal & Hinton (1993).3. terminology follows. Let denote degrees freedom statistical mechanical system. energysystem, E (S ), real-valued function degrees freedom, Boltzmann distributione,fiE (S)P (S ) = P ,fiE (S)Sedefines probabilitydistributionover possible configurationsof . parameter fi inverse temperature;serves calibrate energy scale fixed unity discussion belief networks. Finally,sum denominator|known partition function|ensures Boltzmann distribution normalizedunity.65fiSaul, Jaakkola, & Jordanmean field bound log-likelihood may calculated substituting eq. (15) righthand side eq. (11). result calculationXln P (V )Jij jXij,+Xhi ,XPln 1 + ejJij Sj +hi(16)[i ln + (1 , ) ln(1 , )] ;hi indicates expectation value mean field distribution, eq. (15). termsfirst line eq. (16) represent mean field energy, derived eq. (14); secondrepresent mean field entropy. slight abuse notation, defined mean valuesvisible units; course set instantiated values 2 f0; 1g.Note compute average energyP mean field approximation, must findexpected value hln [1 + ez ]i, zi = j Jij Sj + hi sum weighted inputs ithunit belief network. Unfortunately, even mean field assumption hiddenunits uncorrelated, average simple closed form. term arisemean field theory Markov networks pairwise interactions; again, peculiar sigmoidbelief networks.principal, average may performed enumerating possible states pa(Si ).result calculation, however, would extremely unwieldy function parametersbelief network. ects fact general, sigmoid belief network definedweights Jij equivalent Markov network N th order interactions pairwise ones.avoid complexity, must develop mean field theory works directly DAGs.handle expected value hln [1 + ez ]i distinguishes mean field theoryprevious ones. Unable compute term exactly, resort another bound. Noterandom variable z real number , equality:ffhln[1 + ez ]i = ln ez e,z (1 + ez )E= hz + ln[e,z + e(1,)z ] :(17)(18)upper bound right hand side applying Jensen's inequality opposite directionbefore, pulling logarithm outside expectation:Ehln[1 + ez ]i hz + ln e,z + e(1,)z :(19)Setting = 0 eq. (19) gives standard bound: hln(1 + ez )i lnh1 + ez i. tighter bound(Seung, 1995) obtained, however, allowing non-zero values . illustratedFigure 2 special case z Gaussian distributed random variable zero meanunit variance. bound eq. (19) two useful properties state without proof:(i) right hand side convex function ; (ii) value minimizes functionoccurs interval 2 [0; 1]. Thus, provided possible evaluate eq. (19) different values, tightest bound form found simple one-dimensional minimization.bound put immediate use attaching extra mean field parameterunit belief network. upper bound intractable terms mean fieldenergyln 1 + ePJjij+hj10EX@ Jij j + hi + ln e, z + e(1, )z ;j66(20)fiMean Field Theory Sigmoid Belief Networks10.950.9bound0.85exact0.80.7500.20.40.60.81Figure 2: Bound eq. (19) case z normally distributed zero meanunit variance.case,exact result hln(1 + ez )i = 0:806; bound givesn 1 2 In1 (1this2min ln[e 2 + e 2 ,) ] = 0:818. standard bound Jensen's inequality occurs= 0 gives 0:974.Pzi = j Jij Sj + hi. expectations inside logarithm evaluated exactlyfactorial distribution, eq. (15); example,Y,he, z = e, h1 , j + j e, J :(21)ijjholds he(1,i )zi i.similar resultThough averages tractable, tend writefollows. reader, however, keep mind averagespresent diculty; simply averages products independent random variables,opposed sums.Assembling terms eqs. (16) (20) gives lower bound ln P (V ) LV ,LV =Xij,Jij j +XX , zlne01X @Xhi ,Jij j + hiE Xj(1, )z+e+(22)[i ln + (1 , ) ln(1 , )] ;log-likelihood evidence V . far specified parameters fi gi2Hfi g; particular, bound eq. (22) valid choice parameters. naturally seekvalues maximize right hand side eq. (22). Suppose fix mean values fi gi2Hask parameters fig yield tightest possible bound. Note right handside eq. (22) couple terms belong different units network.minimization fi g therefore reduces N independent minimizations interval [0; 1].done number standard methods (Press, Flannery, Teukolsky, & Vetterling,1986).choose means, set gradients bound respect fi gi2H equal zero.end, let us define intermediate matrix:KijE= , @@ ln e, z + e(1, )z ;j67(23)fiSaul, Jaakkola, & JordanSiFigure 3: Markov blanket unitparents children.Siincludes parents children, wellzi weighted sum inputs ith unit. Note Kij zero unless Sj parentSi ; words, connectivity weight matrix Jij . Within mean fieldapproximation, Kij measures parental uence Sj Si given instantiated evidence V .degree correlation (positive negative) measured relative parents Si .matrix elements K may evaluated expanding expectations eq. (21); fullderivation given appendix B. Setting gradient @ LV =@i equal zero gives final meanfield equation:01= @ hi +Xj[Jij j + Jji(j , j ) + Kji ]A ;(24)() sigmoid function. argument sigmoid function may viewedeffective input ith unit belief network. effective input composed termsunit's Markov blanket (Pearl, 1988), shown Figure 3; particular, terms takeaccount unit's internal bias, values parents children, and, matrixKji, values children's parents. solving equations iteration, valuesinstantiated units propagated throughout entire network. analogous propagationinformation occurs exact algorithms (Lauritzen & Spiegelhalter, 1988) compute likelihoodsbelief networks.factorized approximation true posterior exact, mean field equationsset parameters figi2H values make approximation accurate possible.turn translates tightest mean field bound log-likelihood. overall procedurebounding log-likelihood thus consists two alternating steps: (i) update fi g fixed fi g;(ii) update figi2H fixed fi g. first step involves N independent minimizationsinterval [0; 1]; second done iterating mean field equations. practice, stepsrepeated mean field bound log-likelihood converges4 desired degree accuracy.quality bound depends two approximations: complete factorizabilitymean field distribution, eq. (15), logarithm bound, eq. (19). reliable approximations belief networks? study question, performed numerical experimentsthree layer belief network shown Figure 4. advantage working smallnetwork (2x4x6) true likelihoods computed exact enumeration. consideredparticular event units bottom layer instantiated zero. event,compared mean field bound likelihood true value, obtained enumerating4. shown asychronous updates mean field parameters lead monotonic increases lowerbound (just case Markov networks).68fiMean Field Theory Sigmoid Belief NetworksFigure 4: Three layer belief network (2x4x6) top-down propagation beliefs. modelimages handwritten digits section 4, used 8x24x64 networks unitsbottom layer encoded pixel values 8x8 bitmaps.450040004000350035003000mean field approximation3000uniform approximation25002500200020001500150010001000500500000.010.020.030.040.05relative error loglikelihood0.06010.070.500.51relative error loglikelihood1.5Figure 5: Histograms relative error log-likelihood 10000 randomly generated three layernetworks. left: relative error mean field approximation; right:relative error states bottom layer assumed occur equal probability.log-likelihood computed event nodes bottom layerinstantiated zero.states top two layers. done 10000 random networks whose weights biasesuniformly distributed -1 1. Figure 5 (left) shows histogram relative errorlog likelihood, computed LV = ln P (V ) , 1; networks, mean relative error 1.6%.Figure 5 (right) shows histogram results assuming states bottom layeroccur equal probability; case relative error computed (ln2,6 )= ln P (V ) , 1.\uniform" approximation, root mean square relative error 22.6%. large discrepancy results suggests mean field theory provide useful lower boundlikelihood certain belief networks. course, ultimately matters behavior meanfield theory networks solve meaningful problems. subject next section.4. LearningOne attractive use sigmoid belief networks perform density estimation high dimensionalinput spaces. problem parameter estimation: given set patterns particularunits belief network, find set weights Jij biases hi assign high probabilitypatterns. Clearly, ability compute likelihoods lies crux algorithmlearning parameters belief networks.69fiSaul, Jaakkola, & Jordantrue loglikelihoodlower boundtrue loglikelihoodlower boundtraining timetraining timeFigure 6: Relationship true log-likelihood lower bound learning. Onepossibility (at left) increase together. true log-likelihooddecreases, closing gap bound. latter viewedform regularization.Mean field algorithms provide strategy discovering appropriate values Jij hi withoutresort Gibbs sampling. Consider, instance, following procedure. patterntraining set, solve mean field equations fi; g compute associated boundlog-likelihood, LV . Next, adapt weights belief network gradient ascent5 meanfield bound,Jij =hi =@ LV@Jij@ LV;@hi(25)(26)suitably chosen learning rate. Finally, cycle patterns training set,maximizing likelihoods6 fixed number iterations one detects onsetoverfitting (e.g., cross-validation).procedure uses lower bound log-likelihood cost function training beliefnetworks (Hinton, Dayan, Frey, & Neal, 1995). fact lower bound loglikelihood, rather upper bound, course crucial success learning algorithm.Adjusting weights maximize lower bound affect true log-likelihood two ways(see Figure 6). Either true log-likelihood increases, moving direction bound,true log-likelihood decreases, closing gap two quantities. purposesmaximum likelihood estimation, first outcome clearly desirable; second, though lessdesirable, also viewed positive light. case, mean field approximation actingregularizer, steering network toward simple, factorial solutions even expense lowerlikelihood estimates.tested algorithm building maximum-likelihood classifier images handwrittendigits. data consisted 11000 examples handwritten digits [0-9] compiled U.S. PostalService Oce Advanced Technology. examples preprocessed produce 8x8 binaryimages, shown Figure 7. digit, divided available data training set700 examples test set 400 examples. trained three layer network7 (see5. Expressions gradients LV given appendix B.6. course, one also incorporate prior distributions weights biases maximize approximationlog posterior probability training set.7. many possible architectures could chosen purpose density estimation; used layerednetworks permit comparison previous benchmarks data set.70fiMean Field Theory Sigmoid Belief NetworksFigure 7: Binary images handwritten digits: two five.01234567890123456789388 2201300400 393 0001006012 376 13040 13 0024 373 0 12 00630020 383 0122 10021 13 0 377 2041142016 386 0000100000 388 3819170711 369 4040000085 383Table 1: Confusion matrix digit classification. entry ith row j th column countsnumber times digit classified digit j .Figure 4) digit, sweeping training set five times learning rate = 0:05.networks 8 units top layers, 24 units middle layer, 64 units bottomlayer, making far large treated exact methods.training, classified digits test set network assignedhighest likelihood. Table 1 shows confusion matrix ij th entry counts numbertimes digit classified digit j . 184 errors classification (out possible 4000),yielding overall error rate 4.6%. Table 2 gives performance various algorithmspartition data set. Table 3 shows average log-likelihood score networkdigits test set. (Note scores actually lower bounds.) scoresnormalized network zero weights biases (i.e., one 8x8 patternsequally likely) would receive score -1. expected, digits relatively simple constructions(e.g., zeros, ones, sevens) easily modeled rest.measures performance|error rate log-likelihood score|are competitive previously published results (Hinton, Dayan, Frey, & Neal, 1995) data set. successalgorithm arms strategy maximizing lower bound utility mean fieldapproximation. Though similar results obtained via Gibbs sampling, seems requireconsiderably computation methods based maximizing lower bound (Frey, Dayan, &Hinton, 1995).71fiSaul, Jaakkola, & Jordanalgorithmclassification errornearest neighbor6.7%back-propagation5.6%wake-sleep4.8%mean field4.6%Table 2: Classification error rates data set handwritten digits. first three reportedHinton et al (1995).digit log-likelihood score0-0.4471-0.2962-0.6363-0.5834-0.5745-0.5656-0.5157-0.4348-0.5699-0.495-0.511Table 3: Normalized log-likelihood score network digits test set. obtainraw score, multiply 400 64 ln2. last row shows score averaged acrossdigits.5. DiscussionEndowing networks probabilistic semantics provides unified framework incorporatingprior knowledge, handling missing data, performing inference uncertainty. Probabilisticcalculations, however, quickly become intractable, important develop techniquesapproximate probability distributions exible manner. especially true networksmultilayer architectures large numbers hidden units. Exact algorithms Gibbs samplingmethods generally practical networks; approximations required.paper developed mean field approximation sigmoid belief networks.computational tool, mean field theory two main virtues: first, provides tractableapproximation conditional distributions required inference; second, provides lowerbound likelihoods required learning.problem computing exact likelihoods belief networks NP-hard (Cooper, 1990);true approximating likelihoods within guaranteed degree accuracy (Dagum &Luby, 1993). follows one cannot establish universal guarantees accuracy meanfield approximation. certain networks, clearly, mean field approximation bound fail|itcannot capture logical constraints strong correlations uctuating units. preliminaryresults, however, suggest worst-case results apply belief networks.worth noting, moreover, qualifications apply Markov networks,domain, mean field methods already well-established.72fiMean Field Theory Sigmoid Belief Networksidea bounding likelihood sigmoid belief networks introduced relatedarchitecture known Helmholtz machine (Hinton, Dayan, Neal, & Zemel, 1995). formalismpaper differs number respects Helmholtz machine. importantly,enables one compute rigorous lower bound likelihood. cannot saidwake-sleep algorithm (Frey, Hinton, & Dayan, 1995), relies sampling-based methods,heuristic approximation Dayan et al (1995), guarantee rigorous lower bound.Also, mean field theory|which takes place \recognition model" Helmholtzmachine|applies generally sigmoid belief networks without layered structure. Moreover,places restrictions locations visible units; may occur anywhere withinnetwork|an important feature handling problems missing data. course, advantagesaccrued without extra computational demands complicated learning rules.recent work builds theory presented here, begun relax assumptioncomplete factorizability eq. (15). general, one would expect sophisticated approximationsBoltzmann distribution yield tighter bounds log-likelihood. challengefind distributions allow correlations hidden units remaining computationallytractable. tractable, mean choice Q(H jV ) must enable one evaluate (orleast upper bound) right hand side eq. (13). Extensions kind include mixture models(Jaakkola & Jordan, 1996) and/or partially factorized distributions (Saul & Jordan, 1995)exploit presence tractable substructures original network. approach paperwork simplest mean field theory computationally tractable, clearlybetter results obtained tailoring approximation problem hand.Appendix A. Sigmoid versus Noisy-ORsemantics sigmoid function similar, identical, noisy-OR gates (Pearl,1988) commonly found belief network literature. Noisy-OR gates use weightsnetwork represent independent causal events. case, probability unit Siactivated givenP (Si = 1jpa(Si )) = 1 , (1 , pij )S(27)jjpij probability Sj = 1 causes Si = 1 absence causal events.define weights noisy-OR belief network ij = , ln(1 , pij ), follows01Xp(Si jpa(Si )) = @ij Sj ;j(28)(z ) = 1 , e,z(29)noisy-OR gating function. Comparing sigmoid function, eq. (3), seemodel P (Si jpa(Si )) monotonically increasing function sum weighted inputs.main difference noisy-OR networks, weights ij constrained positiveunderlying set probabilities, pij . Recently, Jaakkola Jordan (1996b) developed meanfield approximation noisy-OR belief networks.Appendix B. GradientsPprovide expressions gradients appear eqs. (23), (25) (26). usual, letzi = j Jij Sj + hi denote sum inputs unit Si . factorial distribution, eq. (15),73fiSaul, Jaakkola, & Jordancompute averages:he, z = e, h1 , j + j e, J ;he(1, )z =(30)ijje(1,i )hiYhj1 , j + j e(1, )Jij(31):unit network, let us define quantity(1,i )zi(32)= he,hze + e(1,i )z :Note lies zero one. definition, write matrix elementseq. (23) as:(1 , )(1 , e, J ) + (1 , e(1, )J ) :(33)Kij =1 , j + j e, J1 , j + j e(1, )Jgradients eqs. (25) (26) found similar means. weights,ijijij,i Jij@ LV@Jijij(1,i )Jij, )i j ei(1 , )j e= ,(i , )j + (1,,J1 , j + j e1 , j + j e(1, )JLikewise, biases,@ LV= , :@hijij:(34)(35)Finally, note one may obtain simpler gradients expense introducing weaker boundeq. (19). advantageous speed computation importantquality bound. experiments paper used bound eq. (19).Acknowledgementsespecially grateful P. Dayan, G. Hinton, B. Frey, R. Neal, H. Seung sharing earlyversions manuscripts providing many stimulating discussions work.paper also improved greatly comments several anonymous reviewers. facilitate comparisons similar methods, results reported paper used images preprocessedUniversity Toronto. authors acknowledge support NSF grant CDA-9404932, ONRgrant N00014-94-1-0777, ATR Research Laboratories, Siemens Corporation.ReferencesAckley, D., Hinton, G., & Sejnowski, T. (1985) learning algorithm Boltzmann machines.Cognitive Science, 9, 147{169.Buntine, W. (1994) Operations learning graphical models. Journal Artificial IntelligenceResearch, 2, 159-225.Cooper, G. (1990) Computational complexity probabilistic inference using Bayesian belief networks. Artificial Intelligence, 42, 393{405.Cover, T., & Thomas, J. (1991) Elements Information Theory. New York: John Wiley & Sons.Dagum, P., & Luby, M. (1993) Approximately probabilistic reasoning Bayesian belief networksNP-hard. Artificial Intelligence, 60, 141{153.74fiMean Field Theory Sigmoid Belief NetworksDayan, P., Hinton, G., Neal, R., & Zemel, R. (1995) Helmholtz machine. Neural Computation,7, 889{904.Dempster, A., Laird, N., Rubin, D. (1977) Maximum likelihood incomplete data viaEM algorithm. Journal Royal Statistical Society B39, 1{38.Frey, B., Hinton, G., & Dayan, P. (1995) wake-sleep algorithm learn good density estimators? D. Touretzky, M. Mozer, M. Hasselmo (eds). Advances Neural InformationProcessing Systems: Proceedings 1995 Conference.Geman, S., & Geman, D. (1984) Stochastic relaxation, Gibbs distributions, Bayesian restoration images. IEEE Transactions Pattern Analysis Machine Intelligence, 6, 721{741.Hertz, J., Krogh, A., Palmer, R. G. (1991) Introduction Theory Neural Computation.Redwood City, CA: Addison-Wesley.Hinton, G., Dayan, P., Frey, B., & Neal, R. (1995) wake-sleep algorithm unsupervised neuralnetworks. Science, 268, 1158{1161.Itzykson, C., & Drouffe, J.M. (1991). Statistical Field Theory. Cambridge: Cambridge UniversityPress.Jaakkola, T., Saul, L., & Jordan, M. (1995) Fast learning bounding likelihoods sigmoid-typebelief networks. D. Touretzky, M. Mozer, M. Hasselmo (eds). Advances Neural InformationProcessing Systems: Proceedings 1995 Conference.Jaakkola, T., & Jordan, M. (1996a) Mixture model approximations belief networks. Manuscriptpreparation.Jaakkola, T., & Jordan, M. (1996b) Computing upper lower bounds likelihoods intractablenetworks. Submitted.Jensen, C. S., Kong, A., & Kjaerulff, U. (1995) Blocking Gibbs sampling large probabilisticexpert systems. International Journal Human Computer Studies. Special Issue Real-WorldApplications Uncertain Reasoning.Lauritzen, S., & Spiegelhalter, D. (1988). Local computations probabilities graphical structures application expert systems. Journal Royal Statistical Society B, 50, 157{224.McCullagh, P., & Nelder, J. A. (1983) Generalized Linear Models. London: Chapman Hall.Neal, R. (1992) Connectionist learning belief networks. Artificial Intelligence, 56, 71{113.Neal, R., & Hinton, G. (1993) new view EM algorithm justifies incrementalvariants. Submitted publication.Parisi, G. (1988) Statistical Field Theory. Redwood City, CA: Addison-Wesley.Pearl, J. (1988) Probabilistic Reasoning Intelligent Systems. San Mateo, CA: Morgan Kaufmann.Peterson, C., & Anderson, J.R. (1987) mean field theory learning algorithm neural networks.Complex Systems, 1, 995{1019.Press, W. H., Flannery, B. P., Teukolsky, S.A., & Vetterling, W. T. (1986) Numerical Recipes.Cambrige: Cambridge University Press.Russell, S., Binder, J., Koller, D., & Kanazawa, K. (1995). Local learning probabilistic networkshidden variables. Proceedings IJCAI{95.75fiSaul, Jaakkola, & JordanSaul, L., & Jordan, M. (1995) Exploiting tractable substructures intractable networks. D.Touretzky, M. Mozer, M. Hasselmo (eds). Advances Neural Information Processing Systems:Proceedings 1995 Conference.Seung, H. (1995). Annealed theories learning. J.-H. Oh, C. Kwon, S. Cho, eds. NeuralNetworks: Statistical Mechanics Perspective, Proceedings CTP-PRSRI Joint WorkshopTheoretical Physics. Singapore, World Scientific.76fifffi! #"$ %'&)(+*, --.0/1, -3254.C6789: ;=<>-?!@A8 %&;=B0>-.D)EE3FHGJILKNMJONDPORQSDUTWVXMZY1[0\:]_^`IbadcfegY1DUMJONDPOihjIlkb[\nmoapIbklapVXTW]q[Y1rtsvuwMJVXTx[\nmoapDP^yDPapDUMJ\:DP]z|{~}ff$}}{H~ffgpbp$P0=0) Jw030lH'010Hj0)ff+yp~SyH7L!~7L`3y`P5! 7`l5ybyg3y3yZX:3Sff~7+`#y~=0ff35y33`~y+305770!U~0737+y~Sff!7Syyy``g`7y!ff`Zyybb7y!ff|J373y!37`bH`H|JH`Zw37#yH03XyLJ!X`3Z! 7:7~3PZ)y`$3y7Z53:5`y$P5yH7y`3!Sy)3yy`yw~X0L!SJHffL`3Z7`37yZ=37 573:`Xy3$pN :)U~ lP#: $ 1fffi!"$#%&'(*),+-(./012'3.(4(.53.%!%)63.763."%8:9;%5'%5+<)=>)?=A@Bfi6)'#3*#C3.D)63*E(*/03.6+B3GF-3*)H/JI&:K(*LNMOPQKRTSU'%)6VWK/LNMOPXKRSU'%)6VWK/L SY=%76ZLT[]\^)_/LYMOO Ma`284bff6c^d e-3.)?@B)_fgJ#%<'(*)Afgc)6#h)6+<A)=5!i6+B3GF-j%53.7)=3kl#%&'(*)l+aE 3.#%l!7(.3.>@-(*73.%<7>)3*Z8ZmY8n"%8*LU3.fg^W %afo)=>)N6)'#%p)g>^#'(*)Lp#'(*)q>r%27(.(*/?!+(*s/#L 6)'#%p)t>^%5(.(*/!%)!+(.a/#L fg,fgp)u)67(.'#iv6:)6^3kg)g7+(*a/#wjx)=%l3k%27>)3*y)=>)Y:)6z3.76)'#%D)L%)=K'u+3.%"j)=%l6)'#%D)^#%<'(*)NaEr)=%l e-3.)3k%"7#'(*)u#%&'(*)8{ +B3GF|3*)_/}3.;~3.!+B)p);6'%?q+LB@'%)^)r)=?(*/%L#~>),(*6),3.6!j>++(k3.>)3*^%)r>3.(./i)=?!)r3.7+|)D)^%8-b$)=%(*"(T#%53.13*)r7/LB<3.)L@B5)=y6i)=>)j$!5"%(t'(.53.l+#C3ky3*)+D)#(q(.afY++|#)6y6)>)6?(.fI& >WKWLMOO`28Bb)=%66z+z7a/c@|,@-6#}$6!@3k;+3.3.+(*t"'(.>)3."j=af%e-3.)Y!"7'(*N>A)67@B,6(*E#8(.63.h)=%y>++(.3.>)3.J#%73.L:(.3.Wc7K#%(N@-6##3.>"3.?5 F"'>)3*LT+c+-(./h&'#7D)(r(*8N$K#%(r@#o#3.>"%3.i'6(."3.(;#%3*+)3*7)=%%5(@B=E 3*'%t!+B%p)gZ#%E 3.^)6")=%ufz3*)=y(*"3k(-#%3*+)3*5)=%;)'(.(*/!@-6E#c@B=E 3*'%a8 9;%r)63*g)6!'!r7(-@|=E 3*'%guu7p/5!+B%p)u+B3*@-(*8 #3.>"%3.q6+B# )6j?6)tZ7+|D)tgfz=3k=7)=6r7(./7'!+%)3*z(*#)6y3k3.)6/8%4/c)6$7(.>"p'@BuT+|3*@(*;#3.>"6z3.N@)3.%#8b(-(.3*<Y6!^7+|D)t>^(*4(.3.>@-(*u)=7)=8pSUl(.3k73.>)6Y(* +(.'3*@(*N#3.>"6%A}"3*El)=%A7(./i'!+)3*N<Y(.3.>@-(*,!+B%D)z=3*"=%u+23*3*)H/8bc F"'%>)3.y)WKu3*)N3.g<)63.!+B3*@-(*u)6!=3*El(k(U)=%,#%3*"y"(.89r<)6c%1#3.6)3.%"'3.=57l3.!+B)p)u"(.u<(*Y3.!+B)p)N%8SU76)6')Y)=%l@B6)N+B3*@-(*^ F-"'%>)3*N"(ku)=%c=EA)6!@|;+6p)6#u#%<'(*)gfz3*)=c#3GBp)g+#3k%"!)67)=%3*^#%3*>@-3.(.3*)H/8, --. ~ %%-!;3X;N=0gp!fi9Pq~8! %&%_ &%%0;fig%,-Y?*?t<,.rg.G*0.d%!%6.?6.%B$*G*i.gu!6gr%!%6.r*.Na;|i|Dn%*%2***5*67**ff&.<2*642.>.6pk667.j*.&,.*42Z.**%&*q..A&gY> |D2%76^>%t<g>r*5.7hK6.7Nky,..az.?66%6!,%2.%j!%7%&*N.N6c67p6B%,*$ %!%7%6...-;k6B.$%??q*.*}%&*r*..?.%<7>.fi.?6h60p6N%i%>*ff^K6*Tzsta4%c<.%<7>.J*6*r.!%7 J.ff%1*kY.%>tY.j!7>7.j .ff.%<7>.yg6jB^&..?_6BGp%,.N%?gay6.?>B%,&uB6y66.%cz*-Y<ZY.r.,.~6>~p66,6}%jN~B*?60>%z*Cff%afi*g>7D>*ZU<?k6:<?>yp6 _%B%pC%56!pAN%7A!%d.p*C |..%}.azAkl}kz&*u%%A76Y.<t>uT%,6.%%>Yglg.}..,65-%<.z1>>r.*szYN6y6puc.%<7>.<$%c.%>ih%.y2fik%27>*% 7k.*Zfi$p>BA&gY> |u%A%%^Y%2*Bij>>.pu4%7<.N..,.czk6.%ff>|yikyB**qr*%%~*oA%&*c*.$6p6.C.,>k*A>B l6>*5*.C766UkA>h%aYK>zz%Y>25.q!%>*.*?K67*!%7k%, *q.p*4z66*?%0Y.6K6***r!>*-..*H%o<K6*>D6k%7>Yl<l.5>*cgz>1.p65pz%K>6*KY^5}>B>^k66Dr%&*u%.-n*-%*zz%666.Y->^.z>k->*-B6;%c 6*;>,.BYk^.;66..y.?.$>;>B;tj.*Yz*7%&*z%l% %.66,T 6*Y.u%,6y%**;rk%27>*N.t**jp6*>.Yz%%N2yk%27>*5%.5|^>*6y%66sckK6*Kzz%Y>.. >B .7 >^%%_%7<. .p6ApK* k4pq%%6ysr7*.;7u^*%>*$u<k.%%!>~6D60k$.r>B,z..|j~~K60*.?5;z*d_g_KBA^%>*:Y.!>?kC!>2.6^*%?>..AB tc>c!662.*}.J}6B77}%^.J%%a t$>1!166..}.g%i%rk*a>*62>i6Y2%^<?.r^k$%27%&*^*.>|tj>7.utr7* %>.5 &..u5%Y*T^.56B!6% _%5%&*.Y*6y*.> d.fi6z.<%1 6.6o67p.yl%&*5*k6g.G<%~67p.i<fi;.%Zz 2.*HT >:q .6 U >T.<2*62Nn* 6fi.%D*h6.z>2z%1%!%6..*7>%-*.Z%l%%5*0u 6*HAklg.G %az~>Ag.G<%67p.!67.!!*K6!.p**.0KB6k*UYk.j.d}6}.fi%B ^ag|.Yku%_g*%1pc76!%Nk1.cy.-%lg.G<%.*uBl!-%6}.cB* %7.|.7%..4%% 6U %q>BkZ<.*szakK*?ug K.fiz-**;g.G<%77Dkt<N*.Y7tz*5_g?HpBtZ%>*izk5kq5y%fit%D$:--.it -- gYlG%du-%-q.it%DD%KD%%-*?* dffgfi ff0ff! %ff6 %5ff| 26ff %!%* $6"!g 2 ff# %ff $7ff $kff &%'(<ff )?. * yff+*g. ,<$ % 67ff .<ff /%ff )7B*ff 0 1$ ff 0ff ff 2"34!N5# %ff $7ff $kff &6879ff9:7<;lff =*Nff A$~ >=6? q@ %ff )A34!N B5DCr< &6(79ff9FE@GIH+JK$MLff679ff9ffN ;K$O0P< *Q /$g% * A:R| KSJ ff $ $ >2*g T:% uff $ UuV<ff). * W q.$ X6AB M6 ( # * AY6 %Z7ff [6* 5ff\%/B 6 $@ u@ >:ff 2]M ! Nff+$< $ S6ff 0 % *Z^? \ 7._ !ff ?> @0? ff 2* O0%` *aT@ 8 .$ * U ff 6ffb0 % , 6c@B )ff$%B*g. ,<$ %b$.$ . $ # *EW...$6 (%r :$ ?]B<*g NffZ@ N $$ ff K$ @0b!ff ZMu %ff [.ZJ x* 0ff6 @0@ # * feb)%< *]S .%1*tff KMBff 1d!Z[-* :>2<ff .0 % @0V*g. ,<$ %$. * $I<ff ($ ).ff )Lb$ ff 0ff ff A(IB >:% 2.g # * ^N"? M0 6 (. * $V6Cr<?ff WH+4K$MLff%':ff$M*g II7ff Ud.3,C;W5hHiJ))$MLff679ff9jU;K # * 2kz$ 6. 6l*tff Tff$.%nIoWprqssgtuXv+wSxzyzqy|{8q}h~&xiff@fvbIffqxSy(qyv++A v+ ~&}h3J$ ffB * ;q6}*ff0 c$ ff0ff ffD $)MNff*Nff8%P<ff )]$_ff<<ffK*]% z% U) ff z$ ff| )* d6 6`gnff6s% +$ ff| )* 6 AUff I$K6$ }>^%b.ff+%0* m)0 &$%bM>B ff %ff6]%0. }>-&ff..$ 23J*tT% 0 * _;K6io%%ff6 P.ff) 34MM $0;1%0 * &%ff V U?*$[*g[*]..86 !7,$67.ZK%7W6 W $ UuW6Nff$ ffB *$2 *6:ff!>A%1uff._0ff $-$ff$ffI% PK% 7:}*ff0 1$ ff0ff2 ff2Y "? c> $6&\<ff /T<z* *$0 " $ 6 * fi-@ B 66@6"<ff6$ff$ W34N! i5;C&6_79ff9FE;.ffu[K$ABgffi. k.$MM? z%ff[*:/g* [7` ff 7ff:bff6%fi.ff0 [$ ff0ff2 ff2b? i|fi%%z37<l; ff$*t W6b7ffUd-3,rC5H+JK$MLff6+79ff9jU;KZ6 ff$:6 $* $ff(M>*V!%+65ffU6-ffa3g;? )* $ffXg* .,<$%67ffc34( L>:?$$Tg6$79ff9:7<;K@]6i$$ ffK^ ffyB?:** yfftffy^*$7. cff8c )MH$S )M.M K$ rffM*t /6@0Pc> V *ZVff+%c<ff )>|`? PN%<6 >5d6 &XX+ ) <ff Z6 7BU7 FM 0b>fi2Y6 $> ^6zff46 2 /ff$Y7d6 \%<:$$ %)!ff ff6$t* P k&%" .r ff%cc> *@0Wg* K>% 0 6 $ $*J rA% W!%ff6 $db $6 )rffzyffd%%ff6 [1*]Fz3JU;KO0d67$>-F=6 AYff8 *.*]f%l?* !ff$$0@l 4WMff VfiI8`JQi" : _ ff J ) KM" 4 [+: ff <P cU ff4J :VU ) b Bh 2 - : J 4 P ffff,$gAK `z3JU;fiI8`JQi @/Z B J 44ff F : z3 ; Uff :B2BM 4 B]-fffiB@ff"4KfiF)FSgJ<BJJ@bK@K/V$+,-J:.bJJfiff Kc 4 MffM!V#"cB4M%$W'&(\f)@Mff,(*,+F-J.I0/*1+32"547698;:72'=<>4"b@).S"B?@r A"B6C* + 2"54DFEHGJI?KLMNE,OGQPM;R0SUT%P?KWV,XKWT%R0K3SNYZM\[#L]SN^NLMN_ X%T%`KLMNT%P?abKL P?KI(P?KW_cMNTfiI]GQRPdN`KWT%SNI?KW`eOgfih :kj,2'l4GnmoOGQP3R0SUTpI]MNGHTKW`qGQTMNEHErMNT%P?abKL@P?KI]PsSNY etusv K>P?KWR0SUT#`_cMwSNLsP]KW_cMNTpI]GHRPxYSNLxK0y1I?KWT%`KW`EJSN^UGQR>[%LSN^NLMN_cPd,azKWEHEn{|YSUX#T%`KW`}P?KW_cMNTfiI]GQRPdGQPxMNTGQT v KL]KWTpI]EQ~qP?NK[%I]GQRMNEzP?KW_cMNTfiI]GQRP=I v MIL]KYL)MNGQT%P;YL]SU_`LMa@GQT^R0SUT%REHX%P]GJSUT%P;a v KWTKNKLI v KLKcGQPM[-SNI?KWTpI]GHMNErR0SUT1GQR0I t%usv K=SNLGJ^UGQT%MNEZYSNL_X%EQMI]GJSUT}SNYazKWEQEJ{|YSUX%T#`KW`P?KW_cMNTfiI]GQRP3YSNLs^NKWTKLMNEEQSN^UGQR>[%L]S{^NLMN_cPz,~c;KWEQ`KLWdp@SUP]PbMNT%`1R v EQGJ[%Y 2WN14 GQP #MNP]KW`cSUT}MR0KL]I]MNGQT[#ML]I]GQMNE#_eS,`%KWE tp L~,_X%P]GQT#P?,GL]KWR0SUT%P]I?LX%R0I?KW`v GHP3`K0T%GJI]GQSUTGQT{|MNEQXKW`EQSN^UGQR 2' L]~1_X%P]GQT%P],Gd WNp4)tZu@v K;YSNL_X%EQMI]GQSUT}X#P]GQT^MNToMNTfiI]Gn{_eSUTSNI?SUTKeSN[-KLMI?SNL=axMNP;#L)P?I\^UGQNKWT5fi~bML)MNE MNT%`1X%LM v _cMNT%GQMNT 2WN14 YSNL=^NKWT%KLMNEEJSN^UGQR[%LSN^NLMN_cP;I?SN^NKI v KLla@GJI vR0SNL]L]KWP?[-SUT%`#GQT^`%K0T%GJI]GJSUTiYSNL`KYMNX#EJI>EQSN^UGQR tusv KeP?I?LMNGQ^ v I?YSNL?{axML`K0y,I?KWT%PGJSUTSNY7I v GQP\YSNL_lX#EQMI]GJSUT 2 L]KWP?[-KWR0I]GJNKWEJ~Nd-I v KL]KWP?I?LGHR0I]GJSUTSNY(I v Ke`KY'MNX%EJI\EQSN^UGQR`K0T%Gn{I]GJSUT 4 I?SK0y,I?KWT%`%KW`qEJSN^UGQR;[#L]SN^NLMN_cP3I v MIsa@GQEQEZ-K=GQTfiI?L]S,`#X%R0KW`TSaabMNP@X%P?KW`,~}P]KNKLMNEkMNXI v SNLPdK ^ t2 xMLMNEr>KWEQYSUT%`d WN% GQYP]R v GJI?Nd WNNU4)tn3 SNI?K;I v MI3GQTcI v GQP7[M[ZKLzazK>a@GQEHESUT%EJ~cR0SUT%PGQ`KLv KEHGJI?KLMNEQPbI v MI@MLKI?LXKGQTI v KR0SNL]L]KWP?[-SUT%`#GQT^e{|MNEQX%KW`P?KW_cMNTfiI]GQRPGJNKMNT%P?abKLP?KIP]KW_cMNTpI]GHRPeI v KabKWEQEn{|YSUX%T%`KW`P?KW_cMNTfiI]GQRPYSNLeK0y1I?KWT%`KW`EQSN^UGQR[%L]SN^NLMN_PcGQP#MNP?KW`}SUTI v K>SN[-KLMI?SNL *,+ SabKNKLWdI v K;SN[-KLMI?SNLsGQPxX%P?KW`GQT}MI?SNI]MNEQEJ~`%GJmZKL]KWTfiIbabM~ 1GQT%R0K *,+GQP\MNTpI]Gn{_cSUTSNI?SUTKI v KY'X%T%R0I]GJSUTi +62*,+(4 GQP\_eSUTSNI?SUTK D>RR0SNL`%GQT^I?SI v KYMN_eSUX#P\T#MNP?I?KL?{u MLP?1G3I v KSNL]KW_ 2'u ML)P?,G|d WNNU4 KNKL~_eSUTSNI?SUTKSN[-KLMI?SNL v MNPMEJKWMNP]Ie%y1[ZSUGHTpI t(usv KqP?KISNYabKWEQEn{|YSUX%T%`KW`R0SUT#REQX%P]GJSUT#PcSNYM[%L]SN^NLMN_ dz`%KWTSNI?KW`l 2'l4 dbGHP`K0T%KW`I?SZKiI v GQPEJKWMNP?I%y1[-SUGQTpI\SNYz +zt-usv K%y1[-SUGQTfiI\RMNT5-KM[#[%L]SUMNR v KW`5YL]SU_-KWEJSa,~iGQI?KLMI]GQT^ + SUT5I v KKW_e[%I~P?KI t% TRMNP?K GQPbT#GJI?K;I v GQPsGJI?KL)MI]GJSUTGQP3^UX%ML)MNTpI?KKW`qI?SMNR0I]X%MNEQEJ~}L]KWMNR v v K;#y,[-SUGQTfiIusv KGQTfiI]X%GJI]GJSUT-K v GQT#`I v GQPsX%P?K=SNY v KSN[-KLMI?SNL\GQP@MNP@YSUEQEJSa@Pa v KWT%KNKL *,+ GHPsM[%[#EQGQKW`I?SMP?KI3SNYEQGJI?KLMNEQP " 1TSa\TI?SeZK>I?LXK=GJIb[%L]S1`%X%R0KWPbI v K=P?KI3SNYMNEQE-EQGJI?KLMNEHP7I v MIsMLK;P]I]GQEQE[ZSNI?KWTfiI]GQMNEQEQ~`KLGQM#EQK D@[%[EJ~,GHT^GQIcI?SP]X%R v MP]KISNY;[-SNI?KWTpI]GQMNEHEJ~`%KLGJM#EJKEQGJI?KLMNEHPcGJIe[%L]S1`%X%R0KWPMP?KISNYEQGJI?KL)MNEQP1TSa@TI?So-KI?LX%KNd SNYI?KWTEQML^NKLeI v MNTI v KSNLGJ^UGQT#MNEzP?KI "t ,I]ML]I]GQT%^ia@GQI v v KKW_e[%I~P?KI@MNT#`GJI?KLMI]GQT%^cX%TpI]GHE-I v K=%y1[-SUGQTpI3GQPsL]KWMNR v KW`qI v X#Ps[%L]S1`%X%R0KWP3MP?KIsSNYI?LXKEHGJI?KLMNEQP t1 I@RMNT-KP v Sa@TI v MI=KNKL]~azKWEHEn{|YSUX#T%`KW`R0SUT%REQX%P]GQSUTGHP>MR0SUT%REHX%P]GJSUT5X%T%`%KL>I v KMNT#P?azKLP]KI;P]KW_cMNTpI]GHRPKWEQEn{|YSUX%T%`%KW`P?KW_cMNTfiI]GQRP@RMNTv X%P3-K=1GJKabKW`MNP@MNTM[#[%L]SWyGQ_cMI]GQSUTSNYMNT#P?azKL\P?KI@P?KW_cMNTfiI]GQRPTYSNL]I]X%T%MI?KWEJ~GJII]XLT%PSUX%I v MI YSNLz_cMNTp~[%L]SN^NL)MN_cPI v KsP?KI SNYZazKWEHEn{|YSUX#T%`KW`R0SUT%REQX%PGJSUT%PGQPK0y1I?L]KW_eKWEJ~}P]_cMNEQEZMNT%`}[%L]S,GQ`%KWPzMNKL]~[ZS,SNL3M[%[%L]Sy1GH_cMI]GJSUTSNYgMNT%P?abKLsP?KI3P?KW_cMNTfiI]GQRP t# SUT#P]GQ`KLv KYSUEQEJSa@GQT^[%L]SN^NL)MN_ v GQR vv MNPsMNEQP?Se-KKWT`#GQP]RX%P]P]KW`,~xMLMNEMNT%`i;KWEJYSUT#` 2WNfi44ssff]U43eff]4@ff3)N1|s);g|b'k|W?;30|)=)ZQ?=s||||x|()?)|s|@|?|(|]b'|)0|)3(-)0]|)fi13-?|b?b?| )| | ?>N |W?\;)k)|)fifi7Q}zpU#,pz%U%#piQ}z11#bsnfiff !!""$#%!#&!ff(')!ff'ff *,+!.-/ff('!#% 1032457689 :;!<ff=?>@BA!(fiff( C<D' EF<ffA<!"G($=H>I@IE( "J!#%fiLKNMO#%P(<'!ff,EF!) O<QR<7ST!'ffU'ffVff(E(+JEF'ff('!WXffF'!#% AC'P(!')(')Y )-AN(#%3Z['&#%\] .^ _89<!"a`8!<ff(!'&W, b",'(acCdL<!"adeSJ'ff9+!E(\J *f<QEF'ff ff9!E(g)e'W,#%!"J')(')!ff&"ihjQS<X#%*,+[) *, P(<QE(-k+[<')El') EF<&ffl'ffL+!EFmYQ<Q\J) UnE(*o(! V*R!'#V#%!P E(+J<QE((ff1^(Ep!) ff9C<V+!E(WEF<*qK,A!<!"_;S(! E( fi'ff<Ql) <ffr+!EFPTnEl <#F(r#%*,+[) *, P(<QE(-/') EF<&ff9ff gEp!) ff<QE( U"! n <Q "s\P-ut1v?5wKyx8pAiE( ,Kyx#%!ff('&ff (ff1^(! {z ff( EF'#%(|$EFJ) ff1'{K,AD'7S} S)Ai(ff ,l')(! W<Q(')a<ff<'&E(~ r(!'&ffN#<ff !!"! "rff *R<P('#ff#%!#J"ffN['Ttyv?5wKyx8pS ~ Tff(!!"g\ \PY3')!ffT(!<Qff(!#p<Off(')(!<Q('){'ffL? Jff L<EF<QE( V'*b')('WR#<ff SDNO(,#% EF<QEF-A'L#<\ fi %3+ #% "(!<Qy*R<#%*R*,Jff !ff g3e) "W y\J<ff( ff'&[W'Y gEF'ff ff(!#Fa!!"ff('E( "O\] J<Y3')ES!EL'!ff (<J#% A!<ffff(!*R <g3ml) "W \[<ff l#%P(<'!ff^'EF*R<Q('),(J<Q?\J')Ep"!ffHEF*b<)-fiZJ-,<!"R+W!'&!ffCEF*R<&)"i}AJ %;+!EF ff(ff "$<ff(! rff TWE(!!"$'&!ff (<!#% ff(y)e'WVEFJ) yff(#F*b<Q(<3hjm8ni58C[(cCi58pFdp>[5i8_8cC5i8C[i58pw]vJ>v?5i8ff(ff(!*,EF(E?(!<Q(l;e) "W l\J<ff L#%(<'Jff?(L'EF*R<Q(')R(!<Q^ .-b'ff?<r+W!'\J')Ep"iSe')')(ELni5n1m@!89ELcCi5nLe@89nmffE(*ff( EF'#%EFJ) ff'X(g3e"W ,\J<ff( U R<QE( R'(,ff(<*R Rff(')(!<Q('){<ffyl')({KNMQh !!""ff *R<P('#ffg"; ffyy"EF<m<P-z "<ff(')\J |fi#%!#&!ff(')iA;'7S} S3<U#%J#!ff(')O"EF'Y "bE(*<UEF!) l')(^ <QOW<Q(')$'(\;"!-A3<QL<7S19<9 ,ff(e(J<Q9<V*R'&E?E( EF*U!<Q(')(! !3+'P?+ EF<Q E#<XmY EF#%*R y(!'ff'P ) EF<Q\J L <Q3ff(ff<J"/) <"!ff *fi!#p$\ EE( ffF!)(ffSJ!ffF'"E(y)e'WV+ EF<Q E0!2 5u8Hty 5wKyy8EF fity 5wr8^"ff(! g*R'!'*b<]ff( T' EF<ff#)ff "a!!"E9(5w#&<ff(ff('#<n8^Ep!) ff9VS[ty 5wr8'ff9(;!ff' fit1v?5wr8^')(!9(gE( :;!')E( *R 9C)W'#<D#ff "!ff(ffS!e"%[!2 5{8H0325n0!2 5{8 8W<'^ V') EF<Q V(U *,+!I-kff 1 O\J(<'a(! fi !!""k#%!#!ffF')!ffl?<b+JE(WEF<*oK!'&#FX g'"! Ufi 5wKfi8pSJff('"EH(! l % #%(ffi(!'ff?*R;"!')[#<Q(')RE %3<*R+J) KTMQS0 2 4 5768?mJpcCJFdmS! rjm8'ffy#%P(<'"u'(Xm[pc[FdmE( "!!#%g9KTMU<J"{(P!ff 2 4 57681fedmSN3'!#% Rdfi'ffy<ff X(,!') Ep<i#%(<'! "a'/<&D<!ff Eff (fflCKNM1EL<Q+!+!E(m3'&*R<Q(')$<#%(!<-X#%'&!#'"ffl')($<!ff Eff ff( *R<('&#ff'X(!'&ff#<ff~ (^ .- %<*,+J) \(i5nLe@8<!"cCi5nLe@8<QE( +!E(eY<Q\J kE(*(6eE( "!J#%C(g3e) "W fi\J<ff S[lm EA](!'ffl!<ff'3Z[J#% ya! (El<,Ep!) rl#%3(<'!'&WV(y^ <QXW<Q(')aN! yN(ff( yI') Ep<ff9'O(y\;"-O'ff!ff "X ,+JE(;"J!#% L0 2 5768EgSiU % #%y?(V#%3Z['#%('&WO'&nEp*R<Q(')<Q\]!L I-]ff1ZJ-3'WO<Q\J''I-/'ffL(;!ffL +!)3#<?<!"G"; ffrV!<mY (O"!'ff(<ff( E(!ffr#%Jff :P! !#% fffi'fi!<ffU'(! bEF')W'&!<CEF*fiJ<Q(')!!""Xff *R<P('#ffSfiy[fifiV!!y (!QU(,!FQ Vg;!)Q)Pr k(p)!RlrqPrr%P(sQ(sgw!(u(Jg (,/(J!%g,DQ(J FQ(,s;!)Q)P^ ,!O(J?F $)9P9RQgPJ]F!%ll(u! O;U! fi(!OFQ , {XQ!J)sJF(U($%[J)()L^CX(,$;;)J() F!O,!(pRU)(,F(9Q()iC,(!p!(FRby!(3!!%L%,J)R(Q(J)F^ F!On9(!^( X(!1[ !(9%!J)()9J()$(p g(!rFQ J(;J!%,(r%J!()J9(!$(yF!gJH[mne[CF G Pfiff ff ff w!" ff#$ %I& '!y)(^* +- ,/.10 ,/.2gi4 36587u/!O! ,9.:0;- ,/.pC(;!V ;=<?>%@BA0f ;D<C >%@BA(F E((!fi(X((J)fi)eRR!Q )HGlFR! fiFmk(!Q1(UFQ 1!F;!J%lO!PP ((!)(J7})](!Q! RP(k (]y;^/l/Q!!FJRQ()XC! l RP(gJH[mne[LKT FM 6 I& N P?ff QCNS RU TV,w$ .UFU * $ !B ff#W %I& '!Q6ffYXZ [] W\N \ ]]!=(^ 1![ !Bff# _^a`$b c!-6ff=( (FBZ d]!=(^ 1!? !& QJ &!6edwJ fe]^a`$b ,wg .10RlU Th,w$ .gi4 3651!(()() F)3),(Q(&JX!9J,! Q7;ikjmlD(!(&)^J ^ TJ, n;1((!,]F ( Tl!3,!.r TJ%!(( !(J(g!)(RJV(()J&)I,%FJR RF(!)(o,Zp1)nJqsr(p!) uJvvwx.pN(ef(!Q,) FQ(u G($,JI(R!,!(3!!%Ou) F- Tz{ RU TV,w$ .fi)(3O% b(e(!Q|0}RlU Th,wg .,Ji ,/.10}RlU TV,w$ .pr:R /Q(J FQ(J ^O J ((!R/ 0~RlU TV,w$ .pB n3&!%X 0~R k!mW 0 @U] n3&!%;bd ((!R!() R^%J( P^1! R1 ,w .10W ,wW @.p3((,9.k1 ,w$ > .0y ,w .FLRrU GEy(!fi((lH(!fiJQ1bFnFU!Q(a(p!!l Xfi%P!)P!( k!(&b(,,! !U%! FJQ((1^J!Q kp!)l^V&D(a(a(VF!Dp!)!r I; !fi(%[!)(!(?.^LFQ FC1bJU%p!)m(PJ)F9(!QTQ;)Q a!(%!!()!^]y% iJ7})$Q(J FQ((NFJ)9)(OQX!Q()k%[!k1 ,w$ .N ,a ,w$ ..R!W ,wg .: , ,w$ ..p 7ue ;!)Q)(FJQF% FO tr;!XJ ;O(g;!Q()!,/.1 ,w @ .,9.W ,w @ .Fg @(g F!Q /;O(,^V(pk X(V (Q(Rg^!((!%LUR F!Q()Rp!QF% FO tQ(C J $a(y)ml,()ihTne[N1 F/N?ff $ey !M ff# wI !|=V N c!bXm!&# (FBZ{ bFRP`W @ ,wg .m #-\!WffB #6 /& g! ,9.ff e96 &_\+ J %e #- { ; <C >@-A&fiY]Y4xSfN__41"Wfi]-]fN]xx]4VxOYf-Y&]O1&V_OYON6Y &6O&$H"[fODP&B[9kD? %B k %B kZP gfFYckfF[6gYfOQ4kPff_Q_YO[$c&JxY6J&fJO]P]OQ 6\J]J6fcd1JJ]J:YYYOY6&\&J4J"]JVYJO9m:YY_46]&PYY4OY&[&JYJ1Y&Jdk4O_YJH4&\&6JYf][6fVff&OOf4Q:1h]Yc]:ff&kf41:YD:VJY]:cYU9N]Yf-c m1JdJ&"1:4&-6YfYQJff&Y&gPOo4fYYO fid4f6&\&"4f&Yc-6YOJ&"1YMdJ&f fY V$dY"&NW64OPYJ--YQJh1Q&YJJh4&\&6JYJ"Q6YOJ&xYfOJO P]& 4 16YOk4QJJY]6YOOWY fUY&[&JD& ]64OOYQY6f6OO&JOfMY66 fW fi ]&6 f&W16YfJY fiYc[4YOH!$ Y"Y]P_$? Y1OcxJYYJQJYf"]x Ox6&YJJhW YOO YNx V&6f-JcOHx1P6YQPY xcYc"Y4fYYJ]66]J#%$'&(#)%$*#= &(#]&6+%,P646Q&&6\YJ&.-Yf]Q]Q4OJM1 0/ JxOYJ6YfJBV4cf&OO!"Y[4YO fi f4ff21JOYc[4YO/Yg3YYQ 54QMYg YOJ f 6-PQY]fU YOJ ]]JJ6cOYJ Y6!]$6Jf YWYJffYO4fJ OQf/7c 6Jf]O/Q"]&\J6O4Og6YOJJdZ O? YOJOg1JVfg]8JYJY4OOY&29]4fSJYJF\k6f6YOJJZ O6YOJfgYc6]Q_d:<;=]VJ]YdJJd10/ ?Q]&JYJH J64Yf&F]x1OJDVcQ6YfJWYYYJofY$]:4B6YO66]JOo46Q&&6$[6YfYQJg]hJ$]]JJ66fOMY66f4OYkM1YfJVYOgYQJ \f6YfJ1MYcYD/Y46Q&& O&J4Q[$J 6]JYh4JY]POf4OQ&>YQJJ]_4OJk1P6YQ"4?f.fJfQff&OO9 f$Y@]-]P[Y4O fifQff&OOh9",A'& fiBC8D8D8DEfi FB G;='H C8D8D8DC <;='HJI_YJ"Y fi x AK& fiB88D8D8DEfi FB <;=LH 88D8D8DM <;='HJI k ,N8OPRQTSVURSVWXS YZQJ[WM\^]SVQT[V\`_aXJb[VUaXJ]cQJd2] e6UYZQJb`b^QaefSVW2U_gQJdCh@Ugd8SV\`QidLSVW2XTS SVW2URjk\^dM\`j'XJb[Vl2b`Ug]XT[VUSVWMUmM[VUnYZUo[c[VUapLQid2Uo]gOqErfis:tuvxw<yz{|~}@{ C"3C"}@%?L6<3a6C{"|{|TL"aaC{RB|T3{|T38C3{}~"|{|JKC"88|'"'"{ C'"{'3Ca{""8|Ja"""8:8{| a|{""}88@<B33{"%?:{"a{6{ {{M?8@B}83"|T88|CB}@{|T3{+a{6"C0B~|K"B|{}T{3{MKkn?{B|T"}83<3C8':x'a{@aC":CZV{"""CC}88"c:x: k"'88|%8}@{"}8B3{|TC"k}@{B}8"3{+"|T{|T338C+""|T{}T0'::: a{""|88|C"}@CRa{~a{3}@{<}@KC"88|"6{3BK3{{B|T"}83<38?"RB|Ta@B%L"?LC>}@{G}@.8|L"3CEGk}@{G}@xa88|J"C8g"~TB3C:}8axn?{|J"3CC"C}@{B3CC |+3a8|T:C3|kCB8}8G"6@V}@{<}@8L:{<}@+{>"+6C|CB8|a8{:CZV{""C}@{"}8B3{""}@{BaaC 8B"{{?C}@CT|T3{a{ {|"a"}@ffB|C}@{""3{{ {L{ K|J"3C"{8|T3B3+{||J"38CaC"}@{<}@B}833|C{3C0g"}8af"|88|C"}@{|J3{+KGTB3Gx8C}@aC06:{B8|{3{MK"|T{|TCkGk.<'8|Tff'n 6@Vk}@{<}@'68:8C K" BL{ff@B}83k"|88|C"}@%{|T3{3C"|C}@CC"}@a{ ?: a{K"<3 8|T.a{K}@{BLK~B|T0{ {|:{K"|T"}83B3CCC}T""}83BffCB%B%:}8"<3"|88||C|T"{"33k:+|T|">`"B}83{}@B33C"?a{%"3{K8|3a8|T?8Cff%}8"{>3{"8|?6L"8|T3C0"x{{EK">n?{|T"3C:@"3B"Z68|Ckn 6x{}@{<}@8Gk'<:C"{"Ca:|T"C|{}@{B3CC|L{E:88|C"B}8{{"{:|T"8CL{8|:B}@'8|TL}8""?ff"|C}@.6@V}@{<}@80}@{|TaKC8|TG}8a8C{}@{<}@"|T"ffCf6>B"3|C}@8BV<BCf{>@aaC"}@{"B33{"|T"C8%EK"+ |n 6@V}@{<}@'|8"8'{|T"3CkZaff:C38|K"8CK {|ka#J8C' BBa# 8C'L8|ff#""|T8aCK#fCB"3|C}@kn 6@V}@{<}@'k B'6@V}@{<}@k'|T8"C"na8K{|T"C8 x|TB3a8K"K"E%a{6C a{}8}@{" {|':CZV{""CaC }8BaC"B|{}T|K<a8{G'|T"CK"}J|K"""8CaC >Ka8{63a8|T{EK%a{~6a|TL{a:BR"?n?{~6CR{B}@{<}@|{"a{ 8Mc2:{~|T"3CRCffG:}@{<}@.{<6{n 6@V."G@VTJ :{B8|K{{EK"B|{|T T3 .{BZ<}83{{.8Cfi?""BfR<<?<<B:'Z<<?2Bfiffff ! "#fi$ff&%'(*),+-+/.103254.626798 :;.=<?>>@=)BADCEF@=7&G;.79+H..5A2A3I KJML )N.O:3NP.5@ERQS.5TU8VW2XI3EUNP.5@=7&798 :;.=<?>>&2A3IY27Z8[:;.=<?>&@=)BACER@=7]\*70327^ERQ_A3)7^QS)BTU42G`TU.aG 870.1E L :TRER@EU7 L .5@P032A3EFQ L Q)b/+H.5TRTU<?bc)Bd3A`I.5IeQS. L 2A 7ER@Q2TU)BA.f`>97gEFQ703ERQ"hDERA3IK)b@=)BADCEF@=7fi70327fi+/.i7SN8j7S)6Q)BTU4.^G[8j703.^.=k[:TRER@EU7":3N.bl.N.5A3@=.^EFAbc)N L 27EU)BAWf>mAn)BdN/.=k2 L :`TU. +gERTRT3G.&d3QS.5In7S)aI.N]EU4. f[(g)7S.70327"A),+o70.&2:3:`TREF@27EU)BA1)b I.bc.527Q2A3In70.N.gERQpA)iI32Aq.NH70327H2rTREU7S.NP2T`I.bc.527EFAq L EUqB0 7!G;.5@=) L .&I.NPEU42G`TU.TR27S.N5fs&.5A.NP2TFTU8V27Z8[:;.=<?>>t@=)BADCER@=7uG;.79+H..5Awv 2A3Iav J +fiN7fQS) L ."d3A3I3.bc.527S.5IwNPd`TU.5Qt)b`70.":3N)qNP2 L \+gERTFTBG;./Q)BTU4.5IERAnbl2,4)BdN)b70.&:3N.bl.NN.5InNPd3TU.V[Q2581v V )BA3TR81EUbW2:3:`TU8DERAq^v .=k@TRd3I.5Q/2A8nbldNP70.NH:;)BQQPEUG`ERTRER7Z8i)bI.NPER4[ERA3qa2Axv <ZI.bc.527EFAqnTREU7S.N]2Tyf(*)7S.n70327i.4.N8Y7Z8[:;.=<?>@=)BACER@=7^@2AzG;.O7dN]A.5IzERA 7S){2{I3EUNP.5@=7_7Z8[:.=<?>P>@=)BACER@=7_G[8e2 J A)BAD<.5|[d3EU42TU.5A7}~\fiN.N.:`N.5QS.5A 727EU)BAe)b!70.aNPd3TU.5Q ff EUb!.52@]0Y@=)BADCER@=7EFAqnNPd`TU.ivjERQgNP.:`TR2@=.5IG 8EU7Q*Q. L E<A)N L 2Ttbl)N L $ 70.5Ae2TRTu@=)BACER@=7Q&G;.5@=) L .r798 :;.=<?>>*@=)BADCER@=7Q2A3I2N.w70 d`Q*2 L .5A`2G`TU.a7S)K@=)BACER@=7N.5QS)BTFd7EU)BAK70N)Bd3qB0X:3N.bl.N.5A3@=.^ERA3bc)N L 27EU)BAWfbc7S.Nn703ERQ L )7EU427ERAqYI3ERQP@d3QQEU)BATU.7Od3Qw:3N.5Q.5A7w70.XA.+I.=A3EU7EU)BA3Q5fdNw7SN.527 L .5A71)b:3NPER)NPEU7EU.5Q&ERQG`2QS.5Ie)BA2K+H.52h.5A3EFAqX)bH703.1A)7EU)BAe)b/{<ZQ2bl.5A.5QQfW>mA[.5@=7fuj+/.1@=)BA3QEFI.N.5IY2NPd3TR.nvx2Qwx<ZQ2bc.K+g03.5A.4.Nw70.N.KERQwA):`N) )bbl)NO2TFEU7S.NP2T/I.bc.527EFAqvbcNP) L 703. L )BA3)7S)BA3ER@@=)Bd3A 7S.N:`2N7Q^)bHx<Zd3A3I.bl.527S.5IzNPd`TU.5Qf;(g)+ERA70.1@=)BA 7S.=k[7r)b/2K:`NPEU)NPEU7ER.5ITU)qBEF@w:3N)qNP2 L +/.+gERTFTW@=)BA3QERI.N*21NPd3TR.^v12Qgx<ZQ2bl.rEUb70.N.aERQfiA)jQd3@]0{:3N)[)bbcNP) )BA)7S)BA`ER@i@=)Bd3A 7S.N:`2NP7Q mw]?ce3=5 )b703.^{<Zd3A`I.bc.527S.5IxNPd3TU.5Q5f3fi0.^QdGQS.77S)1G;.^d3QS.5I{I3.:.5A`I3Q")BAX70.^N]d3TU.v12A3I@=)BA3QEFQS7Q&)bp70)BQS.wNPd`TU.5Q&70327_2N.OA)7XSI) L ERA327S.5I`6G[8{vDf;>A 7d3EU7ER4.5TU8VvtEFQ&I3) L ERA327S.5IG[8xv6EUv ERQ Jm \ph[A)+gAj7S)wG.TU.5QQH:3N.bl.NN.5I67032Ajvr2A`I J B\/I.bl.527S.5Ij+g0.5AjviERQ"2:3:TREU.5I17S)q.703.N+gEU70NPd3TR.5Q70327i2TUN.52I38032,4.nG;..5Ae.5QS72G`TREFQ0.5Ie7S){G;.O{<ZQP2bc.f J B\^ERQ_A.5@=.5QQP2N8e7S) L 2h.nQd3N.w70327.=kD:`TRER@EU7":3NP.bc.N.5A`@=.^ERAbl)N L 27EU)BAXERQd`QS.5IX70._NPERqB07+"258V2@@=)NPI3ERAqn7S)1)BdNgI3EFQ@d3QQER)BAn)b f>7ERQ)G 4DEU)Bd3Q70327+g0.5A3.4.N70.N.ERQA):3N)[)b3bl)N2I.bl.527ERAq_TRER7S.NP2TblN) L 2TRT {<Zd`A3I.bl.527S.5INPd3TR.5Q6703.N.Y@2AG;.eA3)QPd3@P0o:3NP) )b^blN) L 2Qd3G`QS.7K)bi70.5Q.YNPd3TU.5Qf"gd`TU.5Qj70327X+H.NP.Y{<ZQ2bl.2@@=)NPI3EFAq_7S)_)BdN!.52NPTFEU.NI.=A3EU7ER)BAr70[d3QN. L 2ERAw7S)^G;."{<ZQ2bl.f*.N.g2N.70.:`N.5@ERQS.I.=A`EU7EU)BA3Q ffXclo JMwS x \ ]_*M]MMU1BM"SSa a5m&~MZ=S~PWa5m[R] vw 1"tDHm]DR^BwcZX= vK =? rnZPijXH fiJ v\ clDi5v 3x J vB\ %x J v \H{ 1_SJMo vB\ M5Bl v R(*)7S.i70327 6X/ fiJ v\ERQ L )BA)7S)BA3ER@^EFAXG;)70X2A`I f3Y.r@2AA),+I.=A._703.^{<ZQ2bl.iN]d3TU.5QERA3I`d3@=7EU4.5TU8 ffXcl JMwS x \ P^g]ylMR,POUyHSBma wm*~MZ=S~="tD*5m6DU=nm *Z{*^* J r\ fiFny!Pfi=lU =p*^* J r\&R ;DS"*=g![vw v / M5ZP{=w^SJM*"jX/ U[J v\S\5;ym]?fiP]?Pfiff Pff ]9? ?m! m"W]P;#??ymP?fi]Z$=mg?&%tm'?(*)+,.-./901fi24365798;:<>=?@A?CBD?EGFIHCDJK@ML@MHCHNOHP=QSRTNU=WVHCXOY[Z\=WL6=?=WLN^]_NOL`EbaWcdDH@Mef=WL\?CBNOHL=?CNU=WL\g4@ANOLS?hC=iejV]k@!D9L6@"gZ\=WL6=?=WLjNO]l=m@"hD?=h>n;o"qfp rsutSv#wxKyx{z;w}|}~"d*`SGu;.{*OfiUC;CW _E6" W9njo"q p K[*PCuP"{#"M"9 f*Ik"6njoMq p {EP}l>>o"p *HQ@"J{=hC@Ag4@le6@k;L6@>?CB6@`{mjhNU=hNU?CNO"@Me;g@MX^XFJK=WVjLe6@Mef]k=WLj]"XOVHCNU=WLjHP=J\ie6@ML6=?@Me o"p *6DH?CB6@\XU@MDH?ljim=WNOLS?>=J4n o"q p aJ4DmhC=hDZe6=i@MH9L6=?]k=WLS?CDNOLmhC@"J{@"hC@ML]k@N^L6JK=hZD?CNU=WLD?DXOXNa'@aUNUJd?CB6@HYiZQ=WX!e6=S@MHL6=?Dmjm@MDhNOLb`?CB6@L@"gH@MZDLS?CNO]"H]k=WNOL]"NOe@MHgANU?CBHCNOLj]k@fNOL?CBD?d]"DH@lL6=[hVjXU@A]"DLe6=WZNOLjD?@>DL6=?CB6@"hdhVjXU@aW L?CB6@@ML6@"hDX]"DH@6HCNOL]k@A?CB6@L@"ge6@kLNU?CNU=WL\=JEFIHDJK@ML6@MHHPNOH#g4@MD@"h?CBDL\?CB@4=WL@_VH@Me`@MDhXONO@"hPNOLi@M]k?"aSg4@_ZDMY`BDMR@>Z\=hC@!EFIHDJK@!hVjXU@MHDLeJ{=h_?CBNOHdhC@MDHC=WL=Q?CDNOLGZ\=hC@]k=WL]"XOVjHCNU=WLHd?CBDLRiNOD\nq a_B6@9J{=WXOXU=fig>NOL6`hC@MHCVXO?dN^H!?CBSVjH!=QSRTNU=WVH rzjPz;fixKyx{z;w~"#*`SG``[.{*OfiuUC>;Ckk.M"! {I"E[in {Ednjo"q p {Ek6hC=WZ?CBN^HDLe?CB6@lZ\=WL6=?=WLN^]"NU?IY=JQ=?CB=m@"hD?=hHdNU?JK=WXOXO=figAH4NOZZ\@MejNOD?@MXUY`?CBD?A *[o"p *a'@MXOXUFJK=WVLje6@MeH@MZDLS?CNO]"HBDH9H=WZ@"?CNOZ\@MH9Q@"@ML]khNU?CN^]"NU"@MeJK=h9Q@MNOL6?=i=g@MDDLeZN^HCHCNOL6NOLS?@MLe6@Me]k=WLj]"XOVHCNU=WLjH"a_B6@mjhC=m=WHCNU?CNU=WLHCB=figAH[?CBD?[g4@f]"DLH?hC@ML6?CB6@MLb?CB6@f=Qj?CDNOL6@MehC@MHVXU?CHQiYDeejNOL6\De6@MiVD?@[mh@"JK@"hC@MLj]k@NOL6J{=hZD?CNU=WLaH_D`jhH?_HCNOZ\m;XU@>@k6DZ\mjXO@XU@"?AVH!]k=WLHNOe6@"h_?CB6@9J{=WXOXU=gANOL6`mh=hDZ r# r ! !r ;r@;hH?DmmjXOYn o"qp ?=?CB6@@MZ\mj?IY&H@"?"ac4@MHCN^e6@MH?CB6@NOLH?CDL]k@MH=J9?CB6@?hDLHNU?CNURTNU?IYDLeDLS?CNFHYTZZ\@"?hCYHC]B6@MZD\?CBD?Ag@[N^Z\mjXONO]"NO?CXUYDHCHCVZ@9=WLXUYflNOHANOLG# o" p *fia@[?CBiVH!=Q?CDN^L4M>#k_{#_M@AL6@kT?Dmm;XUYn;o"q p ?=kaTTNOL]k@d#!Pg4@_BDMR@>#!GGW {fial>>o" p *HCNOLj]k@*M#kde6=i@MH_L6=?Ae6@"J{@MD?_lDLeug4@=Q?CDN^L& _{ CfiV6h?CB6@"hNU?@"hD?CNU=WL=Jn o"qp YiNU@MX^eHPL6=[L@"gXONU?@"hDXOH"WNa'@a6_NOH?CB@>XU@MDH?Tm=WN^L?"aW<>=?@>?CBjD?_NOHL6=?D]k=WL]"X^VHCNU=WLuVLje6@"hd?CB6@=hNUWN^LDXg@MX^XFJK=WVjLe6@MeH@MZDLS?CNO]"H"a@L6@kT?lHCB6=fig?CBD?l?CB6@mhC=hDZHl DLe eNOHC]"VjHCH@Me@MDhXONU@"hlDhC@`BDLeXU@MeDH9NOLS?@MLe6@Mea>@"hC@[NOH! rfffifi fifi!#"%$'&&)(*+,.-/fi10203fi'42fi560fi7!8:9.9<;=fi:>?@9-7 fi'A!8fiBCD0:4!E090F;=9G0IH Jfffi!=9K /fi,)0L9!8279-7LM!K0:!% HN;+20#027fiHM4'K:O4927fifi74fiPJ :74:J7:fi.Q<C;=fi'KR HN7fi,)K0fiSJ/fi49.7/0:9.T!82/9.-/UNVfffiW497!8L/fiXfi'!K0:!%Y7fi'Z;+27fi/fi[)fi027fi498fi!8Jff9 L/,A!%0897,:HM/fi,)0fi'ML0fiK\:!#/fi:[)fi]_^`9Ba96/fi`027:!1J :74:J7:fiLM9-/J/J/9.42A97fi1;E9.-7L2'[)fi09;=fi'KR)fiC027fi790:9b9 cb>d!K@fi7fi!8!+fi[)fi -/8027fiX'6eIC027fi1:7/-74X0L[.fif/fiXY70:9.]g/-7:fifh;=9-7Lb2'[)fi09PVfffif497!8L/fifiZWBafiBVfffi39\i1j\;+27fi/fi[)fi+@9Kkfi'42b;=fiRJ/fi49 :0:9TlffmnEof9 hp oPrq sft "?ikuwv3xPyzAu+{ |_}~7."h)*8*Xg 9Kp r cTg7;+2/fifi#o\o1: of:!KK09B7A: o+\ff6fi\_]1OO? OO WCM@\aO\O?\]7]\_`]fC]aEffa+Z<GPOffC+Zff+bf?)C` 68_\ 6W ffW DI6 f ?K??WFbI _ A=D_??\\_8ff fafbM6 %6 8_P 6+bfD fC+ K?Cffd M6ff E ` d?D P DID6) \ 6 ff ? DKFD N ??D\ff ffAM? 6 ff\ff6b?K K?Zd ?C ?ff)fC]aEffaDOff#b\ K f ?W ff ? #AM % ?)b ? DF6 Z '6`\6) + ff +?a ffA\_ ? f D_ 8_W 6W +Af) fC++Z#AMAD % ?) + \_ ? 6 f ff ' 6D`D? # ffC? 6 M6ff P = ?3a +ZfD fa+ K6% C 6ff7ff ' ff ' ??? K = ?S ? 6)6 ZC ? D?C6 K?DA ff \?AP? E N?<DID ff ` K Kff D%ff _ ?a 6ff ffK ?d\wK6' 6 ? P W ffM ? ' d\fCff+bfWOff +bf+Zfffa+Off fa+?M Kff? ] ' ffDK ' N 6?@ Dd Z dZ ?XGCfiff ff?f'6 ff G' # C ff< 6?ff6\6 ` \G ffff ff? 6 CK 6'6 ?\K6ff K 6\ d?3'Tff K ?D6ff ` 1'6D+ ffff? ' ? % C k6ff7ff_ a' ??d K N ff 6'6= KffZ DK ffDC'6DK ? DK6 ?)D ff? ? DI6PD\K6 ff = 6 'P ?\ 'Z !A? #"W_ DD?ff "P` "$"%/ '&)( '6D\ D6 dw `` = DKF6 w ?\ E K'6 ?d DKff8_ff ffD ?\* DffD6 D?NF Dff? Dff?? ,'6D\K X ? D6 ? E DI 6 +d?\ + ffD d\ 6' W6 ffM ff ED6 w`\6 ? "$" + 6? 6) 6)S? ?D ?? Dff1 ff0fiE24 36 5 DN298; : 8 _ ff 6 K6)D_ff6 ) K?D \ 6WD ?b 6D ?` ?\b D/ff.O ??' 7.'"$"<>=fi?1@BACEDGFHJI$KLNMLNO$PRQTSKUPRQBLWV/XZYW[]\^K_PRQBLa`PRQBLNObQSKTcd/PRQBLfeO4HgKhNHieTjiL`klfimRn6oGprqm9s;tvu's!w)HJxL>I$eORL>h9L>cL>Kh9LPy`-jJSrMzI!IR{BeTe]`O4PyL>c}|~PRQBLfQHiw)QLNOS{PRQB`O4HiP~[K}`){OzhNSIyLfPRQBLWVBXYQSI$QHJw)QBLNO$S{BPRQB`O4HJP~}IRHJKh9LHiP$HJIkL>cBLNO4SjfijJS>Mf[QLSrxSHJjgS'|TjiLHJKBk`O4S'PRHi`)KZhNSKKHJh9L>ji~_|LORLNeORL>IRL>KPyL>cHJK`){BOES'eeOR`)ShQ[ `_S'LPRQLWL9SeTjJLfIy`)LNMzQS'P$I4QB`ORPyLNOzM1Lf{IyLEPRQLaKB`PRS'PRHi`)Kz>NNN>R]N u$ NNNNrN u$SIzSKS'||ORLNx/HJS'PRHi`)K}k`O$PRQBLfO{jiL!>NNN>R]N u$ NNNNrN u$ N u ;MzQBLNO4L HJIEPRQBLh9`)eTjiL>L>KPf`k dH[L[ Hik HJIWSKS'Py`)SKc Hik - [V/{Th4QO4{TjiL>IEPRQ{Ih9`ORORL>IRe]`)KTcPy`IyL>HK`O4Sj`O>dHik dK`O4SjcBLNkvS{jiPRIEHgK$L>HiPyLNO>IacBLNkS{TjiPEji`w)HJhUv$L>HJPyLNO>d> [L{IyLPRQBLwO4`){KcaHgKIyPRSKh9L>Ifi`kBPRQBLk`)jJji`MzHJKBw$KTSL>caO{jiL>IPy`bORLNeORL>IRL>KPPRQBLO4L>jiLNxSKPS'O4PRHJhNjiL`kPRQBLf^zdBPRQBLWV/XZYadGL9_`)IRPyLNO4Hi`OWv dTSKc_fiL9UVB{BeLNO4Hi`OfvV [ QBLEIy~W|`)jJI! SKTc_S'ORLfeGS'O4SLNPyLNO4I!k`OzO{jiLEKSL>INfa>]N) ; >G]>) ;-v $v >rGrrBJ ] NNU)9) > vr v ; 9 vBQ L-k`)jJjJ`rMzHJKw_kSh9PRIS'ORL-/KB`MzKS'|`){BPWPRQBL-hNSIyL}SKcS'O4L-ORLNeO4L>IyL>KPyL>cSIaO{jiL>IEMzHJPRQB`){BPf|`/cB~vSKc_MbHiPRQB`){BP$KSL]'r>BJ] N>U)9) > far9 EWLNPNIhNSjgjPRQBLS'|`rxL!IyLNP`kTjJHJPyLNO4SjJI[PyLNOS'PyL>cS'eeTjJHJhNS'PRHJ`)Kf`kTGN ~/HiL>jJcIfiPRQBLk`)jJji`MzHJKBwIyL>{BL>Kh9L`kjJHiPyLNO4SjIyLNPRIvHJK_L>ShQhNSIyL ] v NQBLHiPyLNO4S'PRHi`)KeOR`/c{h9L>IbKB`6KBLNMORL>IR{TjiPRI!|L>IRHJcBL>IzPRQBLakvSh9PRI^SjiORL>ScB~Uh9`)KPRSHJKBL>cZHJKPRQBLaeO4`wO4S_[QBL6ORL>SIy`)KHJIaPRQS'PE$SKcV/XY|Tji`/hRL>Sh4Q`PRQLNO>dSKTcPRQS'PKB`eORLNkLNORL>Kh9L}HJKBk`O4S'PRHi`)KHJIzeOR`/c{h9L>c_IRHJKTh9LaSjJIy`6PRQBLfORL>jiLNx'SKPbHJKIRPRSKh9L>I$`kfiL9`)IRPyLNO4Hi`ObSKTcL9ZVB{BeLNO4Hi`O!|Tji`/hR}L>ShQ`PRQBLNO>[ QBLUIRHiPR{TS'PRHi`)Kh4QTSKBwL>I-HikM1LSccHJKk`O4-S'PRHi`)KPyL>jJjJHJKw{TIQB`rMh9`)K/GHJh9PRI-|]LNPMLNL>KPRQBLjJS'PyPyLNObPM1`-S'ORLfPy`|LEORL>IR`)jixL>c[TYIRIR{LM1LaSccUPRQBLfk`)jJji`rMbHJKBwHJKBk`O4S'PRHi`)Kfa -faN'$y4v;^49E1$;br9$]Nv4ff/$ fiNfi "!"#%$'&()"!"()*+-,/.0#12435"*"+6#%$'&(78($9,:<;=>=-?0;A@BDCAEGFHBDI?KJL;M%MN;=<E%FOQPR?TS9U?TFVW?AXZ\[]_^a`b-YcdYfehgjiWkKlmljn-opbqrckKljlsitYfehguntiY{z|[b qwckKlmluitYfexgKn-opbYcdYfehgmiWk0lmljnyv 6Z ^`8Yfehgho}kKljlsi v k0lmlho~YexgmyY{|[Y[z ^` "v {TRTIEGPm?WCAQMN?HFEGVW?TMNE%M%MGUPRBRCBR?TPI;=_EGF;UCD;CAVIVW;F"E%VWBD?TPR;M%UBDEN;FPDBRCBR?OEN?TPVCAF@?PR?TVE1?TQ?TVM%CCBDENA?TMNAA@9QPDE%Q"MN0CAPDPR?DBDEGFO0D?TMN?CAFBD?JL?D?TFVW?TPCAQ;FO0BDI?E%FA;MNA?TVW;F"E%VWBE%FOQtUMN?TPf'w-ddTI?jBDE%Q?VW;QMN?WENBa;AJ=?TMGM1dJ;UF?TPR?TrCAFBDE%VPsJL;AsCwOA?TF?CAMfMN;AOEGV0D;AOAtCA@?0SUCACBDE%V0EGFBDI?mPDEN?m;AJ qD;9;AJ=CAPOENA?TF@'~E%BRBR?A??TFCD?TPUMNB/CBRBREN@"UBR?THBR;rJL;MN)MN;AD?0E%FqE%P<9F;=FBR;c CCAMfs?TM%J;FTA8 nc TA) n <E%PCAFCAMN9PE%PE%P@CAPR?T;Fs;=M%E%FOQCAF4uCAM%M%EN? P?TPDUMNB=I??@mPCBDE%P"C@EGM%ENBK;AJ;AtFQVM%CAUPR?TPVCAF@?-BR?TPRBR?TE%FM%E%F?TCBDE%?c s;=M%E%FOKuCAM%MGEN?T)TA8 nFas;=MGE%FOrCAFuCAM%M%E%?T PCD;CAVI4ENBE%P<CAVWBDUCAM%MNHCwE%FE%rCAM{Q;9?TM;AJCr<;AFBDI?;ADHBDICBsE%PVW;QUBR?TQE%FM%E%F?TC BDE%Q?A)E%FVW?-EGFE%CAM9Q;)?TM%P ;AJ"<;AFBDI?;AE%?TP C??TSUE%8CAMN?TFBfBR;KVMN;PDUD?TPf;AJUM%?TPf=ENBDI;UBF?OCBDE%;FQBDI?/D?TPUMNBE%PEN?TVWBDMNjC"M%E%VC@MN?-BR;0=-?TM%M1dJL;UF?TPR?TrCAFBDE%VPJ;AOA?TF?CAMMN;AOE%VuD;AOACAP)B/CAM%PR;C"M%EN?TP-BR;=-?TM%M1dJL;UF?TPR?TCAFBDE%VPJL;A/?W)BR?TF?THMN;AOEGVuD;AOACAPPDE%FVW?JL;ABDI?VW;QUBDCBDEN;F;AJBDI?M%?TCAPRB9?Tj;E%FB;AJD?TPR?TVWBDENA?TMN0{BDI?VW;QMN?TQ?TFBDCDmMGENBR?CAM%PCAFV CAFH@?K)EN?=-?TCAP/B=;'E%PRBDE%FVWB-CBR;PTv;ArBDI?HVW;QMN?WENBCAFCAMN)PDE%Pj;AJ;UtEN;AENBDEN?TC;CAVIMN?B~@?BDI?HF9Uj@?;AJtUMN?TPq[ci n PRBRCAENOIBRJL;AD=-CtE%Q"MN?TQ?TFBDCBDEN;F=;UM%Q;)?TME%FCAF;UBR?uM%;;A4CAF4BDI?jVW;Q"UBDCBDEN;F4;AJ Yfgs0 E%FaCAFE%FF?<MN;9;A;ADBDUFCBR?TMNAf=?VCAFVW;m@"E%F?rBDI?wB=;MN;9;APE%FBR;4CPE%FOMN?'MN;;A=I;PR?'@;9EGPm?W)?TVUBR?TCBE%FCEN;AE%BDEN?T4D;AOACABDI?CM%EGVCBDEN;F;AJ6"Yfgs0 OAD;=<PwQ;F;ABR;FE%VCAM%MN=E%BDICAFp"OAD;=<PYfg<m?D?0EGP-CQF;F?BR?EGFE%PRBDE%VCAMNOA;AENBDIJL;A/VW;QUBDEGFOjBDI?0MN?TCAPRBQ;PRBw>BDE%?TP I?D?TCAPR;F~E%PBDICBQ;F;ABR;FE%VCAM%MN=<ENBDI)?TH;E%FB-;AJ"Xm{9f8~Tf{hE%;AENBDEN?TMN;AOE%VuD;AOACAqx[cifBDI?0M%?TCAPRB)?TH;E%FB-;AJ"Y{X [}9uX [}9JL;A/ [n =ENBDIKBR;;N9 {Z PDUVIBDICBE%JBDI??0E%P/CUM%?Kl c N9N9 % c nRn;9?TP/F;AB/?J?TCBBDI?TFX [} {Z tYX [l c nZ?TMGPR?KD?BDUF Y{fffi?TFJ;A)[fiff!#"$&%('*)+-,/.0"214301#25)+6)*%87#9:9<;>=5?221#1!$@52$-9:?A'*BC52'-DE*#'+,F#$@)*BCG#9:HI'*"J=6E&?29:#' #'*)*"JK;9:B:'&%#1L'+5M=N"JE#OQPR%SKF5T1HM5=U)*%V=5E+;W9:5X5,4B:' @YT#$-?2)+#1Z"J)[85'+)]\^)*B_[8#']"21`)*%-E*V"JE*8"J)a[85'+)\bE&?29:#')*%2"J)c%A"#Gd)+5LKe$&%#$&f#1^=5Eg'*"J)*B_'+="$@)*B:5Z5=)*%eB:=_;W$@521ABC)*BC5QOQPR%6B:=_;W$@521ABC)*BC54BC)*'+#9:=$-"QDh"$-$@5E&12B_iI)+5!)*%ME*#'&?29C)*'c5=kj5l7k9:B_i`"A1bm"9:9:BC-E#DKFe$&%2#$*f#1nB:/9:B:2#"JEc)*B:[6o7p-#1)+5Z#'+)*"JK9:B:'*%rq sutLvuwCxTy+z {wCxTy-|}~S7k%AB:$&%bB:XG59CG#'8)*%2$@5[8,A?)*"J)*BC5n5=]"4[dB:2B:[6"9[851#95=])*%[855)+5AB:$V$@5?2)+-E&,A"JE*)*'5=3 0Q }OF46)*%#!%2"lG6)+5p#9:B:[6B_2"J)+g)*%8E?29C#'15[6B:2"J)+#1`KTHL}=5E&[3vlw:xXyR"21L$@5[8,A?)+g"25)*%-Ek[6B:AB:[6"9F[851#9F)+5d'+-a7 %-)*%-ER}VB:'R1-=#"J)+#1QOI5E*g,AE*#$-B:'+#9CHD2j5u7k9:B:i6"A1`m]"9:9_BC-ER'*%5u7)*%2"J)R)*%2g-#1#1L)*B:[8aB:'R9_B:#"JEkB:M)*%2gX?A[gKF-E5=,2E&5,5'&BC)*BC52"92$@5A'+)*")*'#OPk%2B:'T?2[cKF-EU[6"lHdKFkiE*#"J)+-E)*%A"d\!B:6,AE&B:2$-BC,9COk5u7U-G-ElD2'*B:2$@9:BC)+-E"9:')*%A"J)125S5)"J,2,F#"JEB:e)*%%#"1 5="cE&?29C][S?2'+)K=N"9:'+]B:d)*%2][dB:2B:[6"9A[85T12#97$-"#9:B:[dB:2"J)+S)*%#["$-$@5E&12B_i9CHL"21!75E*f`7kBC)*%!"p'+-)5=E&?29C#')*%2"J)a%A"'"J)g[85'+)\b9:BC)+-E"9:'-OFPR%2B:'9C#"12'R)+5d"p5lG-E"9:9)*B:[6a$@5[8,A9C@YBC)WH 5=6|\#~O)'*%5?29:1IKFS[6#)*BC52#1QDQ%5l7-G-E#D)*%2"J)12?2c)+5p)*%8?2'*S5=UE&?29CS'*$&%2#[6"J)*"p=5E)+E"2'*BC)*BCGBC)H"21`")*BC;W'+HT[d[8-)+E*Hp5=R,2E&BC5EBC)*BC-#1p,2E&5iE&"[6'$-"LKFc$@5A'*B:1-E&"JK9CHp9:"JE*i-E)*%2"I$@5E&E*#'+,F521;B:i?A,2E&BC5EBC)*BC-#1`,2E*5iE&"[d'-OPR%d)+E&"2'*BC)*B:GTBC)HI'*$%#[6"D=5EcB:2'+)*"2$@D%2"'e b B:2'*)*"2$@#'-OB:[8,9C#[8#X)*"J)*BC5I'&%5?29:1QD)*%-E*-=5E*DFKFcKA"'*#1!5`"4"J,A,2E*5"$&%`7k%2-E*8B:2'+)*"2$@#']"JE*V529:Hpi#;-E&"J)+#147 %#4"$@)*?2"9:9CH!-#1#1D5Eg545)*%-EaKA?AB:9C)]B:`)+#$&%22B_X?#'])*%A"J)g%2"2129:c)+E&"A'*BC)*BCGBC)WH`"21"X)*B<;W'+H[6[8-)+E*HO?2$&%p)+#$%22B:T?#'R"JE&K-H5A1p)*%g'*$@5,F5=)*%2B:',"J,-ElO8!AWW4u A(ff6)*%2B:'U'+#$@)*BC5e7U 7kB:9:9B_G#'+)*B:i"J)+ )*% E*#9:"J)*BC5d5=Q5?EU[65T12BC$-"J)*BC585=Q7#9:9<;>=5?221#1d'+#[6"X)*B:$-')+5]"2'+7-E'+-)'+#[6"X)*B:$-'|>m#9C=521cBC=N'*$%2BC)+D#J~OB:A$@5?E"J,2,2E*5"$%c%2"A129C#'"c@YT)+#A1#19:"i?A"Ji6B:!7k%2B:$%!$@-E*)*"B:Z'+H[cK59_'"JE&8iBCG#Z"M,A"JE*)*B_$-?29:"JE],2E*@;W1@#1![8#"2B:ip"M)*%5E&5?i%B:XG#'+)*BCi"J)*BC5L5=)*%2B:'E*#9_"J)*BC52'*%2B:,MB_'529CH ,F5'*'*BCKA9:"J=)+-E "e$@5E*E*#'+,F5212B_iV@YT)+#2'&BC5p5=h"2'+7-E'+-)R'+#[d")*B:$-'R)+56,2E&BC5EBC)*BC-#1 9C5iB:$],2E*5iE"[6'%2"'KF-#p1@2#1QO4g"JE*a5),A9_"22B:ic)+5dB:X)+E*5J;12?2$@"21V1-=#21S'*?2$&%V"S@Y)+#2'*BC5VB:c)*%2B:',A"J,F-E#Ok-G-E*)*%2#9C#'*'-DJ7$-"ViB:G'*5[8U,AE*#9:B:[6B_2"JE*HE*#'*?A9C)*'%-E&OXI5E*k,2E&#$-B:'+#9CHD77kB:9:9'*%25l7)*%A"J)U)*%$@52$-9:?2'&BC52'h,2E&5T12?A$@#16B:65?2E,2E*5,F5'*"9"JE*$@5E*E*#$@)V7RE&)-O"`,A"JE*)*B_$-?29:"JEc'&?KQ$-9:"'*'a5="2'+7-E8'+-)*'-D)*%p'+5J;W$-"9:9C#1/,2E&B:5E&BC)WHX;>,2E*#'*-E*GTB_i"2'+7-E'+-)*'-OpXu-3&VeC*k+*+JShJZJU@a#-ffk3SUJd:-}a2#---#*X- l#---#-& cT: S36IR-Jk}6R*l@W&g8:}V}l{|N~Wa l2#---u*Ag }6_k-&W*MVpXu-|N3V+\XtLu~88cJNN:l*<&+*+JS^JU@c--R+ 36c&JC&aJNc*#@lX ^-JV-J-a}V}u{|N~gg--|N3]Mqestz {Ah|}~+~-&k}2PR%pB:X)*?2BC)*B:5ZKF#%2B:21^)*%p1@ABC)*BC5/B:'S)*%M=59:9:5l7kB:2io7k%2#-G-E6"`E&?29C }`B_'cE&-KA?)+)+#1rB:r""2'+7-E8'+-)VKA?)SBC)*'cE*-KA?)+)*"9B:'S'+59C#9CH4K"'+#1(5/E&?A9C#'c125[6B:2"J)+#1^KXH(}7kBC)*%^E*#'+,F#$@)S)+5!ufiff fi!#"%$&ff'(ff)ff*,+- .0/1325467198:+;,.<-1>=?19*@.A19-#B)CEDGF13HI;+85J-1>2K.0/J8:*,8L*NM)J;6*@.0J7;+O;,=&.0/13*PM(*,JQ6*@B671SR201>=T1>2019+HI1J+=T;,25UV*@.0J;+W*,+-YXZ201\[A19HI.0]^.0/1*,+8AF1>2:8A1>.>_` 13H>*,+O+;PFa80/;bFcHI;,202019HI.0+19858;,=d;42K*@RR20;*,H5/WFK20.>_R25J7;,2eJ7.fCNR20198A1>20MJ+gh*,+80F1>2K801>.08>_ij(klmknPo?pboTkqsrGtvu>wyx{z}|~AffE(V5u^3,ew7bu0E 5,<A0,A@VmS>,|xSST:u%?S5@w@?u0T,?e@Tw0u>u>eb?)O@uI9u>wTSZ~Vij(kk5 /1SR20;);,=yJ8:85JUVJ6*@2K.A;N.0/1HI;,202019HI.0+19808<R20;);,=y;,= Sv F25.>_*,+8AF1>28A1>.L8A19UV*,+.0JH>8| 25;,R;85J7.0J7;+Y _mKg*,JQ+.0/1R20;,R;80J7.0J7;+J8.A25JM)J*,6Q67C80*@.0J8Z19-FK/19+1>M,1>2V.0/1>201WJ8+;R25J7;,25J7.CR2019801>20M)JQ+g3*,+8AF1>2801>.>;,2mJ8.0/180J+g61:R2eJ7;,25J7.CVR20198A1>25M)J+g3*,+80F1>28A1>.>_ ` 1SUV*9CN.0/1>201>=T;,201*,80804Uh1.0/*@.1>M,1>20CR2eJ7;,25J7.C^R2019801>20M)JQ+gh*,+8AF1>2K8A1>.;,= ~ J8%HI;+85J8A.A19+.>_+E.0/1JQ+-4HI.0J7M,1S8A.A1>RF1h80/;bF.0/*@.:=T;,2*,+*@20BJ7.A25*@25CWR25J7;,25J.fCR20198A1>25M)J+gN*,+8AF1>28A1>.LD*25461<VJQ8%+;,.K-1>=T19*@.A19- JQ+WDF:/19+1>M,1>2K > |xSgJM,19+W.0/*@.KJ8*^8A1>.;,=y6J7.A1>25*,6Q8.A2541J+DS_25;U.0/J8J.=?;6Q67;PFK8.0/*@.3 &| > |x3A HI;+.0*,J+8;+67C#6QJ7.A1>25*,68L.A2541VJ+*,66yR25J7;,25J7.CR2019801>20M)JQ+gh*,+8AF1>2K8A1>.08>_1>. ~< B1N-1I+19-*,83JQ+<1>=_y | .0/1WJ+-4HI.0JM,1^-1I+J.0J7;+;,=Ef80*@=?19+19808 *,+-*,80804Uh1J7.J8N*,672019*,-CY+;PF:+.0/*@..0/1E2e467198^JQ+ ~<? *@201 +;,.N-1>=T19*@.A19-J+DS_C-1I+J7.0J7;+ zI>>>b0ff>,wP>>>>P>,we{~: J cJ8+;,.-1>=T19*@.A19-BC A|~ aSN| _ ` 1-J80.0J+g4J85/^.F;^H>*,80198>*,801E@ >>>P0 DdJ+HI1D}*,+- N J8UV;+;,.A;+JHhJ+B;,.0/J+-JHI198LF1^/*PM,1|L~N| A|~ E| _ /1>201>=?;,251%H>*,++;,.mB1-1>=T19*@.A19-NJQ+VD80J+HI1DaJ8%R25J;,25J7.fCNR20198A1>25M)J+g_*,801#)9>>>90 DSJ+HI1O.0/1OR251>20194J80J7.A198h;,=LH>*,++;,.NB1E-1>2eJ7M,19-Y=T20;U ~L .0/18A1>. | HI;+ff.0*,JQ+8;+67C2e4671983-1>=?19*@.A19-B)C Z|~Ld *,6;+1,_&J+HI1DJ8*,+*,+8AF1>2h8A1>..0/198A125467198OH>*,+.EB1HI;+ff.0*,JQ+19-cJ+ ~< _ /1>251>=?;,201 A|~Lz A|~L | *,+.0/)48L A|~< A|~ 7) | _J+HI1BCO*,85804UhR.0J;+ODGJ8KHI;+80J8A.A19+.KF13*,6Q8A;N/*PM,1A|~ yz &|~ *,+-O.0/1>201>=T;,201hH>*,++;,.B1-1>=T19*@.A19-EJQ+WD_` 1/*PM,1L801>19+N.0/*@.;42%*@RR20;*,H5/J8&g4*@25*,+.A1>19-.A;3R20;)-4HI1;+67CVHI;+H>6480J7;+8&HI;+.0*,J+19-J+*,66R2eJ7;,25J7.CR2019801>20M)JQ+g*,+8AF1>2&8A1>.08>_ ` 1<H>*,+V*,68A;S*,8AS.0/1;,RR;80J7.A1)4198A.0J7;+(gJM,19+^*R*@25.0JH>46*@2*,+8AF1>2m8A1>.yDS@JQ8J.d*,6F*PC)8dR;8080JB671&.A;L;,B.0*,JQ+hD | ;,29,Uh;,201R2519H>J8A1967C,@*804R1>258A1>.;,=DHI;+ff.0*,J+J+g*,--J.0J7;+*,6%R251>=?1>2019+HI1EJ+=T;,25UV*@.0J7;+ .0/20;4g/R25J7;,25J.0J7>19-F196Q6 =?;4+-19-8A19U^*,+ff.0JH>8NB)C*,--J+g*,-19)4*@.A1R251>=?1>2019+HI1J+=T;,25UV*@.0J7;+/1*,+8AF1>2.A;^.0/J8)4198A.0J7;+OJQ8+;_ /12019*,8A;+ J8%.0/*@.=T;,2:85*@,1;,=d.A25*,HI.0*@BJ6QJ7.fCF1*,67F*9C8HI;+80JQ-1>280J+g61<2e467198FK/19+E-1>.A1>25UVJ+J+g3Of80*@=T19+19808:J+;42K*@RR20;*,H5/_ K1>201SJ8%*,+O1 *,UhR671,% ,w%K ,w%wh w/JQ8mR20;,g,2e*,U /*,8.F;*,+8AF1>2%8A1>.08 fiz ffbb *,+- fiz ffP(0 _ ;+80J-1>2 _ &M,19+J=F1*,-.0/13R201>=T1>2019+HI1J+=T;,25UV*@.0J7;+E.0/*@.LB;,.0/ *,+- *@2013R201>=T1>202019- .A;N19*,He/;,= *,+-F1h*@2014+*@B671%.A;S-1>2eJ7M,1 *,+- _);,2JQ+8A.0*,+HI1, J8m+;,.Of80*@=T1:B19H>*,4801KJ7.08&/19*,-^-;)198+;,.&-1>=T19*@. _+O;,25-1>2K.A;N-1>25J7M,1 J.F;46-OB1+19HI198085*@20CW.A;^.0*@,13.0/1R;8085J7BJ6J.fCV;,=y8A1>.08:;,=y25467198 | /1>201*,+- 9 -1>=?19*@.0J+g 6719808R201>=T1>202019-8A1>.08S;,=25467198 | /1>251 *,+-P JQ+ff.A;E*,H>HI;4+ff.>_d:6.0/;4g/.0/J8%JQ8R;8580J7B671<J+R25JQ+H>J7R671:J7.%F;46-H>6719*@2567C6719*,-W.A;VJ+.A25*,HI.0*@BJ6QJ7.fC80J+HI1J+.0/1F;,258A.H>*,801*,+fi"!#$%#'&)(+*,#-$.0/1324!576)-/683&)64!592:-$,;%6=<!-$,+>@?$*%AB&8!C/1EDF?%D)G%>HJI(;A(K#LM-@&82:*D&)*N/O(K,+(;&QP683%.R6S&8!!T?$(;L?U*E$2:(+D45V!2S<7?O*N&W(+6WL*(+#$%>U*#O>M<X68&)(+D:GY&8!T!-2.X!2)ZD3*-&)(;!-$6S*N$O2)!*D:?C5[!2&)?$(+6\2:%*68!#H]_^X`CaObQcdaOegfihkjlmcnoeqpEhknorbtsouNvwhknJux 3A32:*,*NO$2)!*D:?$%6T&82)%*N&)(+#Ly$2:35V32)%#OD%6T(+#z&)?{D!#'&8 &Y!5=,+!L(+DU$2)!L2:*.R.|(+#L}?O*%A~/13%#>%6)D2F(;/1%>E(K#B&)?0,+(;&832F*N&)-2)H{<7(K,+,1#!<g>O(+6)D3-$6)6?$!<m&)?3PB2)%,K*N&8&8!R!-29O2)!1!6)*,H!<*,K68G(_*#$> x *>2:(o% $2:!"!6)%>T&8!D!#$6:(+>3272F-$,;%6<7(+&)?#3L*N&)(;!#U(+#T&)?$=?%*>M*67D3$&)(;!#O6&8!=.R!2)7L%#32F*,$2:-$,+%6*#$>R&8!=L(;A7&)?$%.?$(+L?32$2:(+!2:(;&QPH_%D:?O#$(+D3*,+,;P'&)?$(+6o(K6*D:?$(+3A%>/P=*W2)%>#O(;&)(;!#=!5*#$68<32683&)63H&&)-2:#$6!-&&)?$*N&&)?$!2F(;L(+#$*,*#$68<32683&)62)%.|*(+#Z*#$68<32683&)6*D3D!2:>$(K#L&8!&)?#3<>"#$(;&)(;!#4<7?%#$3A32o&)?3PX*N2)7D!#O6)(+68&8%#&3H9?$9.R*(+#R*DF?$(;3A%.X%#&J(+6&)?$*N&$2)!L2F*.R6W<7?!68X6)(+#$L,;=*#$6)<J32683&(+6S(+#OD!#$6)(+68&8%#&W/"%D!.RZD!#O6)(+68&8%#&(+#&)?$Z#$3<6)%.R*#'&)(KD363H9?R*NO$2)!*D:?@D3*#?O*N2:>$,;PM/"|A(;3<%>~*6=*6)*N&)(+6)5[*D&8!2)PC&82)%*N&).R%#'&4!5\$2)35[32)%#$D%65[!2=6)3A32:*,2)%*68!#O63NH9O2)35V32:%#$D%6=*N2:|(+.XO,K(+D3(;&*#$>?O(;L?$,;PM2)%68&82:(+D&8%>&)?*6)P.R.R3&82:(+DX&82)%*N&).R%#'&4!51!6)(;&)(;A*#O>Y#3L*N&)(;A4(+#5[!2:.R*N&)(;!#B683%.R6-$#)-$68&)(%>H7(+&9(+69>$(ED3-$,;&&8!|6830?!<$5[!27(+#$6)&)*#$D D3O&)(;!#$69!5D3$&)(;!#$69D3*#/12)3$2)%68%#&8%>H95[3<32D!#$D3,+-O6)(;!#$6*N2)=!/$&)*(+#%>Y&)?$*#Y(+#T&)?$!2:(;L(+#O*,"*#O68<J32W683&768%.|*#'&)(+D36%D!#&82:*N2)PB&8!<?$*N&9!#<!-$,+>B 1%D&9<7?%#TO2)35V32:%#$D%69*N2)&)*NG%#M(+#&8!R*D3D!-$#'&3H&(+631&)?32)35[!2).X!2)X2:%*68!#$*N/O,;Z&8!YA (;3< !<*,+6)G(*#O> x *>2:(6S*N$O2)!*D:?{*6*TD!#&82:(;/-&)(;!#&8!|(+#OD!#$6)(+68&8%#ODP?$*#$>O,+(+#L42:*N&)?329&)?O*#T$2)35[32)%#$D0?O*#$>$,+(+#$LH#{*N$$2)!*DF?{&)?$*N&(+6D3,+!6832(+#C68O(;2F(;&7&8!Y!-$2:6(+6W!2:>32)%>{,+!L(+D4$2)!L2:*.R.|(+#L{-OD3D3*N5[-2:2:(3!#$O7-$,+,+!%FH #E!2:>32)%>T,+!L(+DW$2)!L2:*.(+6\*Z683&!5D!.X1!#%#'&)6\5[!2:.R(K#L4*#T(K#$?32:(&)*#$D7?O(;32:*N2:DF?'PH'J*D:?RD!.R"!#$%#'&oD!#O6)(+68&)6!51*683&!5"2:-$,+%63H9?9(+#$?$32:(;&)*#$D?$(;32F*N2:D:?P4(K6-$68%>&8!683&8&),;9D!#(+D&)6*.X!#$L2:-$,;%632:-$,+%6,+!<32o(+#4&)?9?$(;32F*N2:D:?P=?$*A9$2)35[32)%#$D\!A32o&)?!689?O(;L?32-R(+#4&)?9?O(;32:*N2:DF?'P46)(+#$D\&)?5[!2:.X32*N2)D!#$6)(+>32:%>4.R!2)9681%D3(DNH #!&)(;!#Z!51*68&)*N/O,;.X! >%,5[!29!2:>32)%>,;!L(+DS$2)!L2F*.R69D3*#Y/1>#$%>6830\-$D3D3*N5-2)2:(13&*,H;k%O5[!29&)?=>3&)*(+,+6FFH9?$32)E*N2:E&t<J!~.|*(+#}>O("32:%#$D%64/"3&t<J3%#y!2F>32)%>y,;!L(+D$2)!L2:*.|64*#O>}!-$24 &8%#$6)(+!#}!5<%,+,w5[!-$#$>%>B68%.R*#&)(+D363NH9!2F>32)%>Z,;!L(+Do$2)!L2:*.R6-$6)o!#$,;P!#$G (+#$>0!5O#3L*N&)(;!#&)?>$(K68&)(+#$D&)(;!#/"3&t<J3%#X#$3L*N&)(;!#*675[*(K,+-2)S*#$>YD3,+*6:6)(+D3*,#3L*N&)(+!#Y(+69#!& $2)%6:6)(;/O,;(+#B&)?$,+*#L-$*NLH9&)?$4O2)35V32:%#$D%6S!5!2:>32:%>U,+!L(+D4$2)!L2:*.R6S*N2)Z$2:%>#%>M&)?2)!-L?U&)?X(+#O?32:(;&)*#$D4?$(32F*N2:D:?PO&)?32:0(+6#!X<\*%PT!5>32:(;A (+#LXD!#&8&Q>$3"%#O>%#'&7$2)35[32)%#$D%69>P #$*.R(KD3*,+,;PH(+#$*,K,;P0<7<!-$,+>R,K(;G&8!=.X%#&)(;!#*#R*N$$2)!*DF?|2:%D%#'&),;PZ$2:%68%#'&8%>|/PXo2:*NGG%#E*#$> x *N2)&8!2%FHJ7?3P &8%#$>S-$#L6R*N2:L-$.X%#&Z6)P68&8%.68&QP ,;T2)%D!#$6)&82:-$D&)(;!#y!5W,;!L(+DE$2)!L2:*.|.R(+#LS-$#L7% R<7(;&)?*{$2)35[32)%#$DY?$*#O>$,+(+#L{.X3&)?$!>&)?O*N&|(+6XA32)PD3,;!6)Y&8!!-2F63H9?$(+6X(K6R#!&*68&8!#$(K6)?$(+#LR6:(+#$D*6&)?Z*-&)?!2:671!(+#'&7!-&3&)?$%(;2*N$O2)!*D:?U(+69/*68%>!#z8-$#-/O,+(+6:?%>B(+>%*6!5IW32:?O*N2:>}2)3<7G* H#5[*D&3(;&=<*64*O2)%,+(+.R(K#$*N2)PYA32:6:(;!#!5&)?O(+6O*N132=&)?$*N&4,+%>~&8!U&)?%(;25[!2:.=-O,+*N&)(;!#H%fiJ$$O'M+EJ O9Uo+EJ''O'{X$:%8%'8%{KM)O+WON13SC 8%$)+CJ;+4$::RW7;)UQTQ1%S$3N);73:O)3V3:%$9+[:RN);ZX$:$;%3X1\ $)%)8%XKZ)9;+3+$$N9$+8%$)+X+3)Z$83$VO:)+3N$OK+3N);$3\%R$88:N8%RO)+SXXO;\[);3)%8O+19=|+M$)NXoSN$$::{+)$N+8E)$+Q1=+[:RN);+8Q$3"%O%'W$Y3T1)%)%1T$T$3:;%Y $RK3+;:%+w[$$%y8%R)+3X+O3:;Z8XYF%9O:$N')N%%\X3)KX%)%8ONO;0$3K$);$7N)0$)+%1ST)=)3$O)=O$;);B$:3V3)%O0+[:RN);3|RN)783J1O3+$);O$)+$3:NO;4+N)3%'oJW$%W)7'M)339$U)$+S3))+O;+W)Z)%N8%8=)NRJJ%+;wV$O%8%|')+30$US$)18%8%$);BJ)%8$+$Z31+B1; R+")+R9$4)KXO;4$C$N):)3$:%8%')N)+C)X;3XO;Z$+)3O)8%U+C %3B)3%R8R+$O+3N8S)$N9%3F+;%N);BJ%KwVO$%B8%R)+39RO) +X3mN88:)+X$:R+8"3tJ3%@ $)%):;%%)4$~E3;%$~7+)U$=1307K'83)%))+Y18%)+N$OK+3N);$3{oOMQOkOzO_$+K;08E)$YF\3 )% W;""kffW:$fi%)o:N%3%W ;833%$ZtJ$' X$)3[3)3%[+'83:%8)+SRX%)%;OK78+XO)\) $+;t)$+\ON13%OJ\N:W;W%+V$} "!F#k+oO):RRK7$=$7;%$J)3O)%8%)N);%$&(')+* ,(&/.102&4357698)&:3)/,(;<;5=*"3?>A@CB=DEGF(HJI AK\N:W;O:$R$KAL42 J!FMS$+;t|13Q3%Y;83:ON);8%R)+3\_++W$)NFRJOOX8$+7VFR++)|3 N8)&J6JOPQ*R7-SOUTV&):WYX:Z[&Q\]&(*N0^&435_6`8)&43),;;<5=*"3,* acbd&*;c&(* &Re&(*576dfUg4,CXh&(*5=*"3$:K3%)39N G4i A!FUjW$$+XO:;:;)+%$81%3lk"3;QE83$;9++Nm 8)&J6JOM$no0^P/p9q@hrsB&():W"$F+3%)39N4 %!F^u7%8$+$=N1$F;:;);%K|3$;o++NGv8m)/& 6JOGp`p1p1PQq@hrsB^wGg:,R_R7-Sg3\$33N):;M#3$yx0;Mzu$+;iU {!F)NO;TX %+Z$@)%;RRO)N);y[++W$):R|+47;)B+$3F;)$S$E8:3N);8RN$1%N9+| :$}k# +o):R|+W7K+; }~SK+;3%^$M K"!Fv#_+%N)KX|;:+)$RS[08%8)+$T)|)N)+/kNO+++QO)1););$1FBV:4$+NU$&(')+* ,(-&/.102&4357698)&:3)/,(;<;5=*"3i {F I[KS$[_ H!F9733$)N++;tRN)$X%)$+)o$$$R%')O);KR$X$8$+:%8$+B$C;+=$)FRR+ ]8m)/& 6JOm>R=ZVPQ*R7-SOo$&(5=*Rd?&* .hgh)/gh* 6Qg&(*p1)+R75 o6 57,(PQ* Rg-=-5S3"gh* 6Qg3W%;[$1};#_;):$+8 L4y (A!F#k+0O):RS7+)M3+:)+33N);d8m)/& 6JOJR=ZPQ* R7-lO?&* .hgh)/gh* 6Qgc&(*V02&4357698)&43),;;<5=*"3fis1:} Q^S:(="(N( |17 oh_(%QhS"%Q]JAi1m/ +4(/([+7_QhMMs4s :h(:c/4shsS Q`h:(::s/[h// |4sh: =ss/4 /sJJ11`QY4[_ 9S%sQ_} JQ`^94:c`US4|sh4ShUhUh%/7_(?7="(^=4QS/}U Q s4shS:h4:S4 m/ Y1=Qe ( ?( 4 +V+7 oh_(%Q =S"h Qh^=4QS/%M Qs(4S1s h(:(4Sc4:css4hUCy7oQQ= + (4 [` 4+hhe7_( s2is4sh: [4S y2(^(|:(4Sfffi}" } Q4s(:4|?4sh:S y4h=h4:s4sc/U/G h 9[(4SY^ 2`Qff^C11"[++yQ(" 2M Q ^4_:( Uh( A=" eA17ch7M4s :hG11c/hQ|yQ(" ?SM (4/ Q!4sv4 (4S Ghh Sh(| Sh("A#$`:44hi(h( S&% '% c:S:S4 h JJUQhQ%77 (? hh/h Q<`d( (^(*V) (:A="ey:#[% 4/[eC (AQ2Uso es </ c"4hyhs ?U4<4s4s4h+e(s c44(S4 cA4hh$G, A(eQJ(+c7_:Ah-/.103C2 AJAACsy:#[% 4/[e(9% JQi4(SU/ A4hMm5/4 4S4:h^h Uh%/7_( ?7="(sQAffC/h 9 (AQ6S=Ush7Sm4 /sso17 h_(%Qhl[ +78- Q [}(:/_9" Q' (/4+e4sh4h4hffGA4h4 9S4^(h(4ShA (+ (/1]=sc7_Q( :J[([s4h/h;% `1%d Q ^]=hc77h QshQ7e( +~oS4c U / (::=U< /`44%hG"/ 2s4h/h;% CC%SYsc/2J9CdSYU:>% J" JQ' /h43? (h:( A%@ /:h4h /: 4 /4(/hQh hS | ss/sh:S4 m/ J3AB =21CED/F (%+(F(| sh ;dSs:h S:SGsJQ?s`o Hes / A4h=o h:GS4:ch (+ (/d=2I-ffJ.>-+C2 G( Y[(S//hh 1` JQ i(44U4 cA4hc=/:s4c"/ (o 44 4%Ks U7oQ4_`Qs2CC[(^[:hm7<L`"([ ` SGh4s^}h4 S:2MS2 `S=h4 h Q / c"4h=9S<4QcUS4hSh(4Sy m/ (= [/ :( ?(Jhh/h Q+7 oh_( QhS"%Q 2e%(M9Nfi