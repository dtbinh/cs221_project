Journal Artificial Intelligence Research 4 (1996) 365396

Submitted 4/95; published 5/96

Adaptive Problem-Solving Large-Scale
Scheduling Problems: Case Study
GRATCH@ISI.EDU

Jonathan Gratch
University Southern California, Information Sciences Institute
4676 Admiralty Way, Marina del Rey, CA 90292, USA

STEVE.CHIEN@JPL.NASA.GOV

Steve Chien
Jet Propulsion Laboratory, California Institute Technology
4800 Oak Grove Drive, M/S 5253660, Pasadena, CA, 911098099

Abstract
Although scheduling problems NP-hard, domain specific techniques perform well
practice quite expensive construct. adaptive problem-solving, domain specific
knowledge acquired automatically general problem solver flexible control architecture.
approach, learning system explores space possible heuristic methods one well-suited
eccentricities given domain problem distribution. article, discuss
application approach scheduling satellite communications. Using problem distributions
based actual mission requirements, approach identifies strategies decrease
amount CPU time required produce schedules, also increase percentage problems
solvable within computational resource limitations.

1.

Introduction

maturation automated problem-solving research come grudging abandonment
search domain-independent problem solver. General problem-solving tasks like planning
scheduling provably intractable. Although heuristic methods effective many practical
situations, ever growing body work demonstrates narrowness specific heuristic strategies
(e.g., Baker, 1994, Frost & Dechter, 1994, Kambhampati, Knoblock & Yang, 1995, Stone, Veloso
& Blythe, 1994, Yang & Murray, 1994). Studies repeatedly show strategy excels one
task perform abysmally others. negative results entirely discredit
domain-independent approaches, suggest considerable effort expertise required find
acceptable combination heuristic methods, conjecture generally published accounts
real-world implementations (e.g., Wilkins, 1988). specificity heuristic methods
especially troubling consider problem-solving tasks frequently change time.
Thus, heuristic problem solver may require expensive tune-ups character application
changes.
Adaptive problem solving general method reducing cost developing maintaining effective heuristic problem solvers. Rather forcing developer choose specific heuristic
strategy, adaptive problem solver adjusts idiosyncrasies application.

1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiGRATCH & CHIEN

seen natural extension principle least commitment (Sacerdoti, 1977). solving
problem, one commit particular solution path one information distinguish
path alternatives. Likewise, faced entire distribution problems,
makes sense avoid committing particular heuristic strategy one make informed
decision strategy performs better distribution. adaptive problem solver embodies
space heuristic methods, settles particular combination methods
period adaptation, system automatically acquires information particular distribution problems associated intended application.
previous articles, Gratch DeJong presented formal characterization adaptive
problem solving developed general method transforming standard problem solver
adaptive one (Gratch & DeJong, 1992, Gratch & DeJong, 1996). primary purpose article
twofold: illustrate efficacy learning approaches solving real-world problem solving
tasks, build empirical support specific learning approach advocate. reviewing basic method, describe application development large-scale scheduling
system National Aeronautics Space Administration (NASA). applied adaptive
problem solving approach scheduling system developed separate research group, without knowledge adaptive techniques. scheduler included expert-crafted scheduling
strategy achieve efficient scheduling performance. automatically adapting scheduling system distribution scheduling problems, adaptive approach resulted significant improvement scheduling performance expert strategy: best adaptation found machine
learning exhibited seventy percent improvement scheduling performance (the average learned
strategy resulted fifty percent improvement).
2.

Adaptive Problem Solving

adaptive problem solver defers selection heuristic strategy information
gathered performance specific distribution tasks. need
approach predicated claim difficult identify effective heuristic strategy
priori. claim means proven, considerable evidence that, least
class heuristics proposed till now, one collection heuristic methods
suffice. example, Kambhampati, Knoblock, Yang (1995) illustrate planning heuristics
embody design tradeoffs heuristics reduce size search space typically increase cost
node, vice versa desired tradeoff varies different domains. Similar
observations made context constraint satisfaction problems (Baker, 1994, Frost
& Dechter, 1994). inherent difficulty recognizing worth (or lack worth) control
knowledge termed utility problem (Minton, 1988) studied extensively
machine learning community (Gratch & DeJong, 1992, Greiner & Jurisca, 1992, Holder, 1992,
Subramanian & Hunter, 1992). case utility problem determining worth heuristic
strategy specific problem distribution.
2.1 Formulation Adaptive problem solving
discussing approaches adaptive problem solving, formally state common definition
task (as proposed Gratch & DeJong, 1992, Greiner & Jurisca, 1992, Laird, 1992,
Subramanian & Hunter, 1992). Adaptive problem solving requires flexible problem solver,

366

fiADAPTIVE PROBLEM SOLVING

meaning problem solver possesses control decisions may resolved alternative ways.
Given flexible problem solver, PS, several control points, CP1 ...CPn (where control point
CPi corresponds particular control decision), set alternative heuristic methods
control point, {Mi,1 ...Mi,k ,},1 control strategy defines specific method every control point (e.g.,
STRAT = <M1,3 ,M2,6 ,M3,1 ,...>). control strategy determines overall behavior problem
solver. Let PSSTRAT problem solver operating particular control strategy.
quality problem solving strategy defined terms decision-theoretic notion
expected utility. Let U(PSSTRAT, d), real valued utility function measure goodness
behavior problem solver specific problem d. generally, expected utility
defined formally distribution problems D:
E D[U(PS STRAT)]

U(PS

STRAT

, d) probability(d)

dD

goal adaptive problem solving expressed as: given problem distribution D, find
control strategy space possible strategies maximizes expected utility problem
solver. example, PRODIGY planning system (Minton, 1988), control points include:
select operator use achieve goal; select variable bindings instantiate
operator; etc. method operator choice control point might set control rules
determine operators use achieve various goals. strategy PRODIGY would set
control rules default methods every control point (e.g., one operator choice, one
binding choice, etc.). Utility might defined function time construct plan given
planning problem.
2.2 Approaches Adaptive Problem Solving
Three potentially complementary approaches adaptive problem solving discussed
literature. first, call syntactic approach, preprocess problem-solving domain
efficient form, based solely domains syntactic structure. example, Etzionis
STATIC system analyzes portion planing domains deductive closure conjecture set search
control heuristics (Etzioni, 1990). Dechter Pearl describe class constraint satisfaction
techniques preprocess general class problems efficient form (Dechter & Pearl,
1987). recent work focused recognizing structural properties influence
effectiveness different heuristic methods (Frost & Dechter, 1994, Kambhampati, Knoblock &
Yang. 1995, Stone, Veloso & Blythe, 1994). goal research provide problem solver
essentially big lookup table, specifying heuristic strategy use based
easily recognizable syntactic features domain. later approach seems promising, work
area still preliminary focused primarily artificial applications. disadvantage
purely syntactic techniques ignore potentially important source information,
distribution problems. Furthermore, current syntactic approaches problem specific
particular, often unarticulated, utility function (usually problem-solving cost). example,
allowing utility function user specified parameter would require significant
problematic extension methods.
second approach, call generative approach, generate custom-made heuristics response careful, automatic, analysis past problem-solving attempts. Generative ap1. Note method may consist smaller elements method may set control rules
combination heuristics.

367

fiGRATCH & CHIEN

proaches consider structure domain, also structures arise problem
solver interacting specific problems domain. approach exemplified SOAR
(Laird, Rosenbloom & Newell, 1986) PRODIGY/EBL (Minton, 1988). techniques analyze
past problem-solving traces conjectures heuristic control rules response specific problemsolving inefficiencies. approaches effectively exploit idiosyncratic structure domain careful analysis. limitation approaches typically focused generating heuristics response particular problems well addressed
issue adapting distribution problems2. Furthermore, syntactic approaches, thus
far directed towards specific utility function.
final approach call statistical approach. techniques explicitly reason
performance different heuristic strategies across distribution problems. generally
statistical generate-and-test approaches estimated average performance different heuristics random set training examples, explore explicit space heuristics greedy
search techniques. Examples systems COMPOSER (Gratch & DeJong, 1992), PALO (Greiner & Jurisca, 1992), statistical component MULTI-TAC (Minton, 1993). Similar approaches
also investigated operations research community (Yakowitz & Lugosi, 1990).
techniques easy use, apply variety domains utility functions, provide strong
statistical guarantees performance. limited, however, computationally expensive, require many training examples identify strategy, face problems local
maxima. Furthermore, typically leave user conjecture space heuristic methods
(see Minton, 1993 notable exception).
article, adopt statistical approach adaptive problem solving due generality
ease use. particular use COMPOSER technique adaptive problem solving (Gratch
& DeJong, 1992, Gratch & DeJong, 1996), reviewed next section. implementation incorporates novel features address computational expense method. Ideally,
however, adaptive problem solver would incorporate form methods.
end investigating incorporate methods adaptation current research.
3.

COMPOSER

COMPOSER embodies statistical approach adaptive problem solving. turn problem solver
adaptive problem solver, developer required specify utility function, representative
sample training problems, space possible heuristic strategies. COMPOSER adapts
problem solver exploring space heuristics via statistical hillclimbing search. search
space defined terms transformation generator takes strategy generates set
transformations it. example, one simple transformation generator returns single method
modifications given strategy. Thus transformation generator defines space possible
heuristic strategies non-deterministic order space may searched. COMPOSERs
overall approach one generate test hillclimbing. Given initial problem solver,
transformation generator returns set possible transformations control strategy.
statistically evaluated expected distribution problems. transformation adopted
2. generative approaches trained problem distribution, learning typically occurs
within context single problem. systems often learn knowledge helpful
particular problem decreases utility overall, necessitating use utility analysis techniques.

368

fiADAPTIVE PROBLEM SOLVING

increases expected performance solving problems distribution. generator
constructs set transformations new strategy on, climbing gradient expected
utility values.
Formally, COMPOSER takes initial problem solver, PS0 , identifies sequence problem
solvers, PS0 , PS1 , ... subsequent PS higher expected utility probability 1
(where > 0 userspecified constant). transformation generator, TG, function
takes problem solver returns set candidate changes. Apply(t, PS) function takes
transformation, TG(PS) problem solver returns new problem solver result
transforming PS t. Let Uj (PS) denote utility PS problem j. change utility
transformation provides jth problem, called incremental utility transformation,
denoted Uj (t|PS). difference utility solving problem without transformation. COMPOSER finds problem solver high expected utility identifying
transformations positive expected incremental utility. expected incremental utility estimated averaging sample randomly drawn incremental utility values. Given sample n values, average sample denoted Un (t|PS). likely difference average
true expected incremental utility depends variance distribution, estimated
sample sample variance 2n(t|PS), size sample, n. COMPOSER provides statistical technique determining sufficient examples gathered decide, error ,
expected incremental utility transformation positive negative. COMPOSER
presumes relevant distributions normally distributed, COMPOSER requires estimate incremental utility based minimum number samples n0 determined
application. algorithm summarized Figure 1.
COMPOSERs technique applicable cases following conditions apply:
1. control strategy space structured facilitate hillclimbing search. general, space
strategies large make exhaustive search intractable. COMPOSER requires
transformation generator structures space sequence search steps, relatively
transformations step. Section 5.1 discuss techniques incorporating domain
specific information structuring control strategy space.
2. large supply representative training problems adequate sampling
problems used estimate expected utility various control strategies.
3. Problems solved sufficiently low cost resources estimating expected utility
feasible.
4. sufficient regularity domain cost learning good strategy
amortized gains solving many problems.
4.

Deep Space Network

Deep Space Network (DSN) multi-national collection ground-based radio antennas
responsible maintaining communications research satellites deep space probes. DSN
Operations responsible scheduling communications large growing number
spacecraft. already complex scheduling problem becoming challenging year
budgetary pressures limit construction new antennas. result, DSN Operations turned

369

fiGRATCH & CHIEN

Given: PSold , TG(), , examples, n0
[1] PS := PSold ; := TG(PS);
[3]

Repeat

n := 0; i:=0; := Bound(, |T|);

{Find next transformation}

[2] < |examples| {Hillclimb long data possible transformations}
[4]

n := n+1; := i+1; steptaken := FALSE;

[5]

: Get Ui (|PS) {Observe incremental utility values ith problem}

[6]

significant :+


2(|PS)
n

: n w n0 n

2
[Q()] 2
U n(|PS)


{Collect transformations reached statistical significance.}

[7]

:+ significant : U n(|PS) 0

[8]

significant : U n(|PS) u 0 {Adopt increases expected utility}

{Discard trans. decrease expeced utility}

[9]

PS + Apply(x significant : significant U n(x|PS) u U n(y|PS) , PS)

[10]

:= TG(PS); n := 0;

[11]

:= Bound(, ||); steptaken :=TRUE;

steptaken T= i=|examples|;

Return: PS

R

Bound(, |T|) :+ ,
|T|

Q() :+ x

1 2e

0.5y 2

dy +
2

x

Figure 1: COMPOSER algorithm

increasingly towards intelligent scheduling techniques way increasing efficiency
network utilization. part ongoing effort, Jet Propulsion Laboratory (JPL)
given responsibility automating scheduling 26-meter sub-net; collection
26-meter antennas Goldstone, CA, Canberra, Australia Madrid, Spain.
section discuss application adaptive problem-solving techniques development prototype system automated scheduling 26-meter sub-net. first discuss
development basic scheduling system discuss adaptive problem solving enhanced schedulers effectiveness.
4.1 Scheduling Problem
Scheduling DSN 26-meter subnet viewed large constraint satisfaction problem.
satellite set constraints, called project requirements, define communication needs.
typical project specifies three generic requirements: minimum maximum number
communication events required fixed period time; minimum maximum duration

370

fiADAPTIVE PROBLEM SOLVING

communication events; minimum maximum allowable gap communication events. example, Nimbus-7, meteorological satellite, must least four 15-minute
communication slots per day, slots cannot greater five hours apart. Project
requirements determined project managers tend invariant across lifetime
spacecraft.
addition project requirements, constraints associated various antennas.
First, antennas limited resource two satellites cannot communicate given antenna
time. Second, satellite communicate given antenna certain times, depending orbit brings within view antenna. Finally, antennas undergo routine
maintenance cannot communicate satellite times.
Scheduling done weekly basis. weekly scheduling problem defined three elements: (1) set satellites scheduled, (2) constraints associated satellite,
(3) set time periods specifying temporal intervals satellite legally communicate
antenna week. time period tuple specifying satellite, communication
time interval, antenna, (1) time interval must satisfy communication duration
constraints satellite, (2) satellite must view antenna interval. Antenna maintenance treated project time periods constraints. Two time periods conflict
use antenna overlap temporal extent. valid schedule specifies non-conflicting subset possible time periods projects requirements satisfied.
automated scheduler must generate schedules quickly scheduling problems frequently
over-constrained (i.e., project constraints combined allowable time periods produces
set constraints unsatisfiable). occurs, DSN Operations must go complex cycle negotiating project managers reduce requirements. goal automated
scheduling provide system relatively quick response time human user may interact scheduler perform reasoning assist negotiation process. Ultimately, goal automate negotiation process well, place even greater demands
scheduler response time (Chien & Gratch, 1994). reasons, focus development
upon heuristic techniques necessarily uncover optimal schedule, rather produce
adequate schedules quickly.
4.2 LR-26 Scheduler
LR-26 heuristic scheduling approach DSN scheduling developed Jet Propulsion
Laboratory (Bell & Gratch, 1993).3 LR-26 based 01 integer linear programming formulation
scheduling problem (Taha, 1982). Scheduling cast problem finding assignment
integer variables maximizes value objective function subject set linear
constraints. particular, time periods treated 0-1 integer variables: 0 (or OUT) time
period excluded schedule; 1 (or IN) included. objective maximize
number time periods schedule solution must satisfy project requirements
antenna constraints (expressed sets linear inequalities). typical scheduling problem
formulation 700 variables 1300 constraints.
operations research, integer programs solved variety techniques including branchand-bound search, gomory method (Kwak & Schniederjans, 1987), Lagrangian relaxation
3.

LR-26 stands Lagrangian Relaxation approach scheduling 26-meter sub-net.

371

fiGRATCH & CHIEN

(Fisher, 1981). artificial intelligence problems generally solved constraint propagation
search techniques (e.g., Dechter, 1992, Mackworth, 1992). address complexity scheduling problem LR-26 uses hybrid approach combines Lagrangian relaxation constraint propagation search. Lagrangian relaxation divide-and-conquer method which, given decomposition
scheduling problem set easier sub-problems, coerces sub-problems solved
way frequently result global solution. One specifies problem decomposition
identifying subset problem constraints that, removed, result one independent
computationally easy sub-problems.4 problematic constraints relaxed, meaning
longer act constraints instead added objective function way (1)
incentive satisfying relaxed constraints solving sub-problems and, (2) best
solution relaxed problem, satisfies relaxed constraints, guaranteed best solution original problem. Furthermore, relaxed objective function parameterized set
weights (one relaxed constraint). systematically changing weights (thereby
modulating incentives satisfying relaxed constraints) global solution often found.
Even weight search produce global solution, make solution sub-problems sufficiently close global solution global solution discovered substantially
reduced constraint propagation search.
DSN domain, scheduling problem decomposed scheduling antenna independently. Specifically, constraints associated complete problem divided two
groups: refer single antenna, mention multiple antennas. later
relaxed resulting single-antenna sub-problems solved time linear number
time periods associated antenna (see below). LR-26 solves complete problem first
trying coerce global solution performing search space weights then, fails
produce solution, resorting constraint propagation search space possible schedules.
4.2.1

SCHEDULES

describe formalization problem. Let P set projects, set antennas,
= {0,..,10080}, V enumeration, V={0, 1, *}, denoting whether time period excluded
schedule (0), included (1), uncommitted. Note P, A, M, specified advance
V determined scheduler initially always uncommitted. Let PAMMV
denote set possible time periods week, given time period specifies project,
antenna start end communication event, respectively. given S, define
project(s), antenna(s), start(s), end(s), value(s) denote corresponding elements s.
also define length(s) = end(s) start(s) simplify subsequent notation.
ground schedule assignment 0 (excluded) 1 (included) time period S.
seen application function G maps element 0 1. denote
G. partial schedule refers schedule subset time periods committed,
denote via mapping function maps elements 0, 1, *. partial schedule corresponds set possible ground schedules (i.e., result forcing uncommitted time period either schedule). denote M. define particular
partial schedule 0 denote completely uncommitted partial schedule (with time periods assigned value *).
4. problem consists independent sub-problems global objective function maximized
finding maximal solution sub-problem isolation.

372

fiADAPTIVE PROBLEM SOLVING

4.2.2

CONSTRAINTS

scheduler must identify ground schedule satisfies set project antenna
constraints, formalize.
Project Requirements. project pn P associated set constraints called project
requirements. constraints processed translated simple linear inequalities
elements S. complete set project requirements, denoted PR, union requirements
individual projects. requirement expressed integer linear inequality:
pr j PR 5



i,j

@ value(s i) w b j





i,j

@ value(s i) w b j



ai represents weighting factor indicating degree ith time period (if included)
contributes satisfying particular requirement. example, requirement project, p,
must least 100 minutes communication time week expressed:

[I(project(s) + p) @ length(s)] @ value(s) w 100.
sS

I(project(s)) equals one belongs project; otherwise zero. Note time periods
zero weight play role explicitly mentioned actual constraint representation.
Constraints length individual time periods represented similarly:
length(s) w 15

efficiency, however, time periods satisfy unary inequalities simply
eliminated preprocessing step.5
Antenna Constraints. three antennas constraint two projects use
antenna time. translated set linear inequalities ACa,for antenna
follows:
ACa = {si + sj 1 | si sj antenna(si )=antenna(sj )=a
[start(si )..end(si )][start(sj )..end(sj )] }
4.2.3

PROBLEM FORMULATION

scheduling objective used LR-26 find ground schedule, denoted S*,
maximizes number time periods schedule subject project antenna constraints:6
Problem:

DSN

Find:
Subject to:

S* + arg max

Gs 0



ZG +

value(s)
sS g



(1)

AC1 AC2 AC3 PR

5. Note inherent limitation formalization scheduler cannot entertain variable
length communication events communication events must discretized finite set fixed length
intervals.

373

fiGRATCH & CHIEN

Z G value objective function ground schedule arg max denotes
argument leads maximum.
Lagrangian relaxation, certain constraints folded objective function standardized fashion. intuition add factor objective function negative iff
relaxed constraint unsatisfied. constraint form ai si b, u[ai si b] added
objective function, u non-negative weighting factor. Likewise, constraint
form ai si b, u[bai si ] added. LR-26, project requirements relaxed:
Problem:
Find:
*(u) =






DSN(u)
(2)

arg max Z G(u) + Z G )
G

0

Subject to:

uj aij @ value(si) * bj) uj bj aij @ value(si)

PR v






G

AC1 AC2 AC3

PR w



G



Zs (u) relaxed objective function u vector non-negative weights length |PR|
(one relaxed constraint). Note defines space relaxed solutions depend
weight vector u. Let Z* denote value optimal solution original problem
(Definition 1), let Z*(u) denote value optimal solution relaxed problem (Definition
2) particular weight vector u. weight vector u, Z*(u) shown upper bound
value Z*. Thus, relaxed solution satisfies original problem constraints,
guaranteed optimal solution original problem. Lagrangian relaxation proceeds
incrementally tightening upper bound (by adjusting weight vector) hope identifying
global solution. global solution cannot always identified manner, complete
scheduler must combine Lagrangian relaxation form search.
4.2.4

SEARCH

solution cannot found weight adjustment, LR26 resorts basic refinement search
(Kambhampati, Knoblock & Yang, 1995) (or split-and-prune search (Dechter & Pearl, 1987))
space partial schedules. search paradigm partial schedule recursively refined (split)
set specific partial schedules. context DSN scheduling problem, refinement
corresponds forcing uncommitted time periods schedule. partial schedule would
pruned ground schedules violate constraints. scheduler applied recursively
refined partial schedule satisfactory ground schedule found schedules
pruned.
refinement refined propagating local consequence new commitment.
variable set particular value, individual constraint references variable
analyzed determine time period would forced schedule result
assignment. LR26 performs partial constraint propagation, complete propagation
computationally expensive. Specifically, constraint C1 references time periods s2, s4 s5,
6.

might correspond desire maintain maximum downlink flexibility.

374

fiADAPTIVE PROBLEM SOLVING

s2 assigned value, LR26 analyzes C1 see new assignment determines value s4 and/
s5. If, example, s4 constrained take particular value, triggers analysis
constraints contain s4. viewed performing arcconsistency (Dechter, 1992).
constraint propagation may possible show refinement contains valid
ground schedule. case partial schedule may pruned search.
LR-26 augments basic refinement search Lagrangian relaxation heuristically reduce
combinatorics problem. difficulty refinement search may perform
considerable (and poorly directed) search tree refinements identify single satisficing
solution. optimal solution sought, every leaf search tree must examined.7 contrast, searching space relaxed solutions partial schedule, one sometimes
identify best schedule without refinement search. Even possible, Lagrangian
relaxation heuristically identifies small set problematic constraints, focusing subsequent refinement search. Thus, performing search space relaxed solutions step,
augmented search method significantly reduce depth breadth refinement search.
augmented procedure works extent efficiently solve relaxed solutions, ideally allowing algorithm explore several points space weight vectors step
refinement search. LR-26 solves relaxed problems linear time, O(|AC1 AC2 AC3 |). see this,
note time period appears exactly one antenna. Thus, Zs (u) broken sum
three objective functions, containing time periods associated particular antenna. Furthermore, relaxed objective function reexpressed weighted sum
time periods antenna, unrelaxed constraints simple pairwise exclusion
constraints individual time periods. Combine fact time periods partially
ordered start time problem simplifies identifying nonexclusive sequence
time periods maximum cumulative weight. easily formulated solved dynamic programming problem (see Bell & Gratch, 1993 details).
augmented refinement search performed LR-26 summarized Figure 2
4.2.5

PERFORMANCE TRADEOFFS

Perhaps difficult decisions constructing scheduler involve flesh details
steps 1,2, 3, 4. constraint satisfaction operations research literatures proposed
many heuristic methods steps. Unfortunately, due heuristic nature, clear
combination methods best suits scheduling problem. power heuristic method
depends subtle factors difficult assess advance. Additionally, considering
multiple methods, one consider interactions methods.
LR-26 key interaction arises tradeoff amount weight vector search vs.
refinement search performed scheduler (as determined Step 2). step refinement search, scheduler opportunity search space relaxed solutions. Spending
effort weight search reduce amount subsequent refinement search.
point savings reduced refinement search may overwhelmed cost performing
7. Partial schedules may also pruned, branch-and-bound search, shown contain
lower value solutions partial schedules. practice LR-26 run satisficing mode, meaning
search terminates soon ground schedule found (not necessarily optimal) satisfies
problem constraints.

375

fiGRATCH & CHIEN

LR-26 Scheduler
(1)
(2)

(3)
(4)

Agenda := {S 0};
Agenda
Select partial schedule Agenda; Agenda:=Agenda{S}
Weight search S*(u) S;
S*(u) satisfies project requirements (PR)
Return S*(u);
Else
Select constraint c PR satisfied S*(u);
Refine {S i}, SG satisfies c
{S i} = S;
Perform constraint propagation
Agenda := Agenda{S i};
Figure 2: basic LR-26 refinement search method.

weight search. classic example utility problem, difficult see best
resolve tradeoff without intimate knowledge form distribution scheduling problems.
Another important issue improving scheduling efficiency choice heuristic methods
controlling direction refinement search (as determined steps 1, 3, 4). Often
methods stated general principles (e.g., first instantiate variables maximally constrain
rest search space, Dechter, 1992, p. 277) may many ways realize
particular scheduler domain. Furthermore, almost certainly interactions
methods used different control points make difficult construct good overall strategy.
tradeoffs conspire make manual development evaluation heuristics tedious, uncertain, time consuming task requires significant knowledge domain scheduler. case LR-26, initial control strategy identified hand, requiring significant cycle
trial-and-error evaluation developer small number artificial problems. Even
effort, resulting scheduler still expensive use, motivating us try adaptive techniques.
5.

Adaptive Problem Solving Deep Space Network

developed adaptive version scheduler, Adaptive LR-26, attempt improve
performance.8 Rather committing particular combination heuristic strategies, Adaptive
LR-26 embodies adaptive problem solving solution. scheduler provided variety heuristic
methods, and, period adaptation, settles particular combination heuristics suits
actual distribution scheduling problems domain.
perform adaptive problem solving, must formally specify three things: transformation
generator defines space legal heuristic control strategies; utility function captures
preferences strategies control grammar; representative sample training problems. describe elements relate DSN scheduling problem.
5.1 Transformation Generator
description LR-26 Figure 2 highlights four points non-determinism respect
scheduler performs refinement search. fully instantiate scheduler must specify:
8.

system also referred name DSN-COMPOSER (Gratch, Chien & DeJong, 1993).

376

fiADAPTIVE PROBLEM SOLVING

way ordering elements agenda, weight search method, method selecting constraint,
method generating spanning set refinements satisfy constraint. alternative
ways resolving four decisions specified control grammar, describe.
grammar defines space legal search control strategies available adaptive problem
solver.
5.1.1

SELECT PARTIAL SCHEDULE

first decision refinement search choose partial schedule agenda.
selection policy defines character search. Maintaining agenda stack implements
depth-first search. Sorting agenda value function implements best-first search.
Adaptive LR-26 restrict space methods variants depth-first search. time set
refinements created (Decision 4), added front agenda. Search always proceeds
expanding first partial schedule agenda. Heuristics act ordering refinements
added agenda. grammar specifies several ordering heuristics, sometimes called
value ordering heuristics, lookahead schemes constraint propagation literature (Dechter,
1992, Mackworth, 1992). methods entertained refinement construction,
detailed description delayed section.
Look-ahead schemes decide refine partial schedules. Look-back schemes handle reverse decision whenever scheduler encounters dead end must backtrack
another partial schedule. Standard depth-first search performs chronological backtracking, backing
recent decision. constraint satisfaction literature explored several heuristic
alternatives simple strategy, including backjumping (Gaschnig, 1979), backmarking (Haralick
& Elliott, 1980), dynamic backtracking (Ginsberg, 1993), dependency-directed backtracking
(Stallman & Sussman, 1977) (see Backer & Baker, 1994, Frost Dechter, 1994, recent
evaluation methods). currently investigating look-back schemes control
grammar discussed article.
5.1.2

SEARCH RELAXED SOLUTION

next dimension flexibility weight-adjusting methods search space possible
relaxed solutions given partial schedule. general goal weight search find
relaxed solution closest true solution sense many constraints satisfied
possible. achieved minimizing value Z*(u) respect u.
popular method searching space called subgradient-optimization (Fisher, 1981).
standard optimization method repeatedly changes current u direction
decreases Z*(u). Thus step i, ui+1 = ui + ti di ti step size di directional vector
weight space. method expensive guaranteed converge minimum Z*(u)
certain conditions (Held & Karp, 1970). less expensive technique, without
convergence guarantee, consider one weight time finding improving direction.
Thus ui+1 = ui + ti di di directional vector zeroes one location. method
called dual-descent. methods, weights adjusted change
relaxed solution: S*(ui ) = S*(ui+1 ).
better relaxed solutions create greater reduction amount subsequent refinement search, unclear tradeoff two search spaces lies. Perhaps
unnecessary spend much time improving relaxed schedules. Thus radical, extremely

377

fiGRATCH & CHIEN

efficient, approach settle first relaxed solution found. call first-solution method. moderate approach perform careful weight search beginning refinement
search (where much gained reducing subsequent refinement search) perform restricted first-solution search deeper refinement search tree. truncated-dual-descent method performs dual-descent initial refinement search node uses
first-solution method rest refinement search.
control grammar includes four methods performing weight space search (Figure 3).
2a: Subgradient-optimization
2b: Dual-descent

2c: Truncated-dual-descent
2d: First-solution

Figure 3: Weight Search Methods

5.1.3

SELECT CONSTRAINT

scheduler cannot find relaxed solution solves original problem, must break
current partial schedule set refinements explore non-deterministically. Adaptive
LR-26, task creating refinements broken two decisions: selecting unsatisfied
constraint (Decision 3), creating refinements make progress towards satisfying selected
constraint (Decision 4). Lagrangian relaxation simplifies first decision identifying small
subset constraints appear problematic. However, still leaves problem choosing one
constraint subset base subsequent refinement.
common wisdom search community choose constraint maximally
constrains rest search space, idea minimize size subsequent refinement search allow rapid pruning partial schedule unsatisfiable. Therefore, control
grammar incorporates several alternative heuristic methods locally assessing factor. Given
common wisdom heuristic, include small number methods violate
intuition. methods functions look local constraint graph topology return value constraint. Constraints ranked value highest value
constraint chosen. control grammar implements primary secondary sort
constraints. Constraints primary value ordered secondary value.
sake simplicity discuss measures constraints form b. (Analogous measures defined forms.) first define measures time periods. Measures
constraints functions measures time periods participate constraint.
Measures Time Periods. unforced time period one neither schedule
(value(s)=*). conflictedness unforced time period (with respect current partial
schedule) number unforced time periods forced forced
schedule (because participate antenna constraint s). time period already forced
current partial schedule, count toward ss conflictedness. Forcing time period
high conflictedness schedule result many constraint propagations, reduces
number ground schedules refinement.
gain unforced time period (with respect current partial schedule) number
unsatisfied project constraints participates in. Preferring time periods high gain
make progress towards satisfying many project constraints simultaneously.

378

fiADAPTIVE PROBLEM SOLVING

loss unforced time period (with respect current partial schedule) combination
gain conflictedness. Loss sum gain unforced time period forced
forced schedule. Time period high loss best avoided prevent progress towards satisfying many project constraints.
illustrate measures, consider simplified scheduling problem Figure 4.

P1

P2
Project Requirements
P 1 : 1 + s2 + s3 2

s1

s2

s3

s4

P 2 : 2 + s3 + s4 2

Antenna Constraints
A1: s1 + s3 1
A2: s2 + s4 1

A1

A2

Figure 4: simplified DSN scheduling problem based four time periods.
two project constraints, two antenna constraints. example, P1 signifies
least two first three time periods must appear schedule, A1 signifies
either s1 s3 may appear schedule, both. solution, s2
s3 appear schedule.
respect initial partial schedule (with none time periods forced either out)
conflictedness s2 one, appears one antenna constraint (A2). subsequently,
s4 forced out, conflictedness s2 drops zero, conflictedness computed
unforced time periods. initial gain s2 two, appears project constraints. gain
drops one s3 s4 forced schedule, P2 becomes satisfied. initial loss
s2 sum gain time periods conflicting (s4). gain s4 one (it appears
P2) loss s2 one.
Measures Constraints. Constraint measures (with respect partial schedule) defined
functions measures unforced time periods participate constraint.
functions max, min, total defined. Thus, total-conflictedness sum
conflictedness unforced time periods mentioned constraint, max-gain
maximum gains unforced time periods. Thus, constraints defined above,
initial total-conflictedness P1 conflictedness s1, s2 s3, 1 + 1 + 1 = 3. initial
maxgain constraint P1 maximum gains s1, s2, s3 max{1,2,2} = 2.
also define two constraint measures. unforced-periods constraint (with respect
partial schedule) simply number unforced time periods mentioned
constraint. Preferring constraint small number unforced time periods restricts number
refinements must considered, refinements consider combinations time periods force
schedule order satisfy constraint. Thus, initial unforced-periods P1 three
(s1, s2, s3).

379

fiGRATCH & CHIEN

satisfaction-distance constraint (with respect partial schedule) heuristic measure
number time periods must forced order satisfy constraint. measure heuristic account dependencies time periods imposed antenna
constraints. initial satisfaction-distance P1 two two time periods must forced
constraint satisfied.
Given constraint measures, constraints ordered measure worth.
example may prefer constraints high total conflictedness, denoted prefer-total-conflictedness. possible combinations seem meaningful control grammar Adaptive LR-26 implements nine constraint ordering heuristics (Figure 5).
3a:
3b:
3c:
3d:
3e:

Prefer-max-gain
Prefer-total-gain
Penalize-max-loss
Penalize-max-conflictedness
Prefer-total-conflictedness

3f: Penalize-total-conflictedness
3g: Prefer-min-conflictedness
3h: Penalize-unforced-periods
3i: Penalize-satisfaction-distance

Figure 5: Constraint Selection Methods

5.1.4

REFINE PARTIAL SCHEDULE

Given selected constraint, scheduler must create set refinements make progress towards
satisfying it. constraint form b time periods left-hand-side must
forced schedule constraint satisfied. Thus, refinements constructed
identifying set ways force time periods partial schedule
refinements form spanning set: {S i} = S. refinements ordered added
agenda. Again, simplicity restrict discussion constraints form b.
Basic Refinement Method. basic method refining partial schedule take
unforced time period mentioned constraint create refinement time period vj
forced schedule. Thus, constraints defined above, would three refinements
constraint P1, one s1 forced in: one s2 forced in, one s3 forced in.
refinement refined performing constraint propagation (arc consistency) determine local consequences new restriction. Thus, every time period conflicts
vj forced refined partial schedule, turn may force time periods included, forth. process, refinements may recognized inconsistent (contain
ground solutions) pruned search space (for efficiency, constraint propagation
performed partial schedules removed agenda).
set refinements created, ordered value ordering heuristic
placed agenda. constraint ordering heuristics, common wisdom
creating value ordering heuristics: prefer refinements maximized number future options
available future assignments (Dechter & Pearl, 1987, Haralick & Elliott, 1980). control
grammar implements several heuristic methods using measures time periods created
refinement. example, one way keep options available prefer forcing time period
minimal conflictedness. common wisdom heuristic, also incorporate method
violates it. control grammar includes five value ordering heuristics derived

380

fiADAPTIVE PROBLEM SOLVING

measures time periods (Figure 6), last method, arbitrary, uses ordering
time periods appear constraint.
1a: Prefer-gain
1b: Penalize-loss
1c: Penalize-conflictedness

1d: Prefer-conflictedness
1e: Arbitrary

Figure 6: Value Ordering Methods

Systematic Refinement Method. basic refinement method one unfortunate property
may limit effectiveness. search resulting refinement method unsystematic
sense McAllester Rosenblitt (1991). means redundancy
set refinements: j. Unsystematic search inefficient total size refinement
search space greater systematic (non-redundant) refinement method used.
may may disadvantage practice scheduling complexity driven size
search space actually explored (the effective search space) rather total size. Nevertheless,
good reason suspect systematic method lead smaller effective search spaces.
systematic refinement method chooses time period helps satisfy selected constraint
forms spanning set two refinements: one time period forced one
time period forced out. refinements guaranteed non-overlapping. systematic
method incorporated control grammar uses value ordering heuristic choose unforced time period use. two refinements ordered based makes immediate progress towards satisfying constraint (e.g., s=1 first constraints form b). control
grammar includes basic systematic refinement methods (Figure 7).
4a: Basic-Refinement

4b: Systematic-Refinement
Figure 7: Refinement Methods

problem specified Figure 4, systematically refining constraint P1, one would use
value ordering method select among time periods s1, s2, s3. s2 selected, two refinements would proposed, one s2 forced one s2 forced out.
control grammar summarized Figure 8. original expert control strategy developed
LR-26 particular point control space defined grammar: value ordering method
arbitrary (1e); weight search dual-descent (2b); primary constraint ordering penalize-unforced-periods (3h); secondary constraint ordering, thus primary ordering; basic refinement method used (4a).
5.1.5

META-CONTROL KNOWLEDGE

constraint grammar defines space close three thousand possible control strategies.
quality strategy must assessed respect distribution problems, therefore
prohibitively expensive exhaustively explore control space: taking significant number
examples (say fifty) strategies cost 5 CPU minutes per problem would require
approximately 450 CPU days effort.

381

fiGRATCH & CHIEN

CONTROL STRATEGY :=
VALUE ORDERING
WEIGHT SEARCH METHOD
PRIMARY CONSTRAINT ORDERING
SECONDARY CONSTRAINT ORDERING
REFINEMENT METHOD
VALUE ORDERING
WEIGHT SEARCH METHOD
PRIMARY CONSTRAINT ORDERING
SECONDARY CONSTRAINT ORDERING
REFINEMENT METHOD

:= {1a, 1b, 1c, 1d,1e}
:= {2a, 2b, 2c, 2d}
:= {3a, 3b, 3c, 3d, 3e, 3f, 3g, 3h, 3i}
:= {3a, 3b, 3c, 3d, 3e, 3f, 3g, 3h, 3i}
:= {4a, 4b}

Figure 8: Control grammar Adaptive LR-26
COMPOSER requires transformation generator specify alternative strategies, explored via hillclimbing search. case, obvious way proceed consider single method changes given control strategy. However cost searching strategy space quality
final solution depend large extent hillclimbing proceeds, obvious way need
best. Adaptive LR-26, augment control grammar domain-specific
knowledge help organize search. knowledge includes, example, prior expectation
certain control decisions would interact, likely importance different control decisions. intent meta-control knowledge reduce branching factor control
strategy search improve expected utility locally optimal solution found. approach
led layered search strategy space. control decision assigned level.
control grammar search evaluating combinations methods single level, adopting
best combinations, moving onto next level. organization shown below:
Level 0:
Level 1:
Level 2:
Level 3:

{Weight search method}
{Refinement method}
{Secondary constraint ordering, Value ordering}
{Primary constraint ordering}

weight search refinement control points separate, seem relatively independent
control points, terms effect overall strategy. clearly
interaction weight search, refinement construction, control points,
good selection methods pricing alternative construction perform well across
ordering heuristics. primary constraint ordering method relegated last level
effort made optimizing decision expert strategy LR-26, believed
unlikely default strategy could improved.
Given transformation generator, Adaptive LR-26 performs hillclimbing across levels.
first entertains weight adjustment methods, alternative construction methods, combinations secondary constraint sort child sort methods, finally primary constraint sort methods.
choice made given previously adopted methods.
layered search viewed consequence asserting certain types relations control points. Independence relations indicate cases utility methods one
control point roughly independent methods used control points. Dominance rela-

382

fiADAPTIVE PROBLEM SOLVING

tions indicate changes utility changing methods one control point much larger
changes utility another control point. Finally, inconsistency relations indicate
method M1 control point X inconsistent method M2 control point Y. means
strategy using methods control points need considered.
5.2 EXPECTED UTILITY
previously mentioned, chief design requirement LR-26 scheduler produce solutions
(or prove none exist) efficiently. behavioral preference expressed utility
function related computational effort required solve problem. effort produce
schedule increases, utility scheduler problem decrease. paper,
characterize preference defining utility negative CPU time required
scheduler problem. Thus, Adaptive LR-26 tunes strategies minimize average time
generate schedule (or prove one exist). utility functions could entertained.
fact, recent research focused measures schedule quality (Chien & Gratch, 1994).
5.3 Problem Distribution
Adaptive LR-26 needs representative sample training examples adaptation phase.
Unfortunately, DSN Operations recently begun maintain database scheduling
problems machine readable format. ultimately allow scheduler tune
actual problem distribution, small body actual problems available time
evaluation. Therefore, resorted means create reasonable problem distribution.
constructed augmented set training problems syntactic manipulation set real
problems. Recall scheduling problem composed two components: set project requirements, set time periods. time periods change across scheduling problems,
organize real problems set tuples, one project, containing weekly
blocks time periods associated (one entry week project scheduled). set
augmented scheduling problems constructed taking cross product tuples. Thus,
weekly scheduling problem defined combining one weeks worth time periods
project (time periods different projects may drawn different weeks), well project
requirements each. simple procedure defines set 6600 potential scheduling problems.
Two concerns led us use subset augmented problems. First, significant percentage augmented problems appeared much harder solve (or prove unsatisfiable)
real problems (on almost half constructed problems scheduler terminate, even
large resource bounds). hard problems exist unexpected scheduling NPhard, however, frequency augmented sample seems disproportionately high. Second,
existence hard problems raises secondary issue best terminate search. standard approach impose arbitrary resource bound declare problem unsatisfiable
solution found within bound. Unfortunately raises issue sized bound
reasonable. could resolved adding resource bound control grammar, however, point project settled simpler approach. address previous
concern excluding augmented problem distribution problems seem fundamentally intractable. means practice exclude problems could
solved large set heuristic methods within five minute resource bound, determina-

383

fiGRATCH & CHIEN

tion discussed Appendix A. results reduced set three thousand scheduling problems.
use resource bound problematic evaluating power learning technique.
noted Segre, Elkan, Russell (1991), learning system greatly improves problem solving performance given resource bound may perform quite differently different resource bound. researchers suggest statistical analysis methods assessing significance
factor (e.g., see Etzioni Etzioni, 1994). study, however, address issue
results might change given different resource bounds. note COMPOSERs statistical
properties suggest problem solving performance worse learning, whatever
resource bound, performance improvement many vary considerably. give least
insight generality adaptive problem solving, include secondary set evaluations
based 6600 augmented problems (including fundamentally intractable ones).
6.

Empirical Evaluation

conjecture Adaptive LR26 improve performance basic scheduler.
broken two separate claims. First, claim modifications suggested
contain useful transformations (it possible improve scheduler). Second, claim
Adaptive LR26 identify transformations (and avoid harmful ones) requested
level probability. first claim solely based intuitions; second supported
statistical theory underlies COMPOSER approach. usefulness COMPOSER depends
ability COMPOSER go beyond simply improving performance identifying strategies
rank highly judged respect whole space possible strategies. third claim,
therefore, Adaptive LR-26 find better strategies simply picked best large
number randomly selected strategies. Besides testing three claims, also interested
three secondary questions: quickly technique improve expected utility (e.g.,
many examples required make statistical inferences?); Adaptive LR-26 improve number
problems solved (or proved unsatisfiable) within resource bound; sensitive
effectiveness adaptive problem solving changes distribution problems.
6.1 Methodology
evaluation influenced stochastic nature adaptive problem solving. adaptation,
Adaptive LR-26 guided random selection training examples according problem
distribution. result random factor, system exhibit different behavior different
runs system. runs system may learn high utility strategies; runs
random examples may poorly represent distribution system may adopt transformations
negative utility. Thus, evaluation directed assessing expected performance
adaptive scheduler averaging results multiple experimental trials.
experiments, scheduler allowed adapt 300 scheduling problems drawn randomly problem distribution described above. expected utility learned strategies
assessed independent test set 1000 test examples drawn randomly complete set
three thousand. adaptation rate assessed recording strategy learned Adaptive LR-26
every 20 examples. Thus see result learning twenty examples, forty
examples, etc. measure statistical error technique (the probability adopting trans-

384

fiLR-26

70
60
50

Avg. Solution Time
seconds per prob.

Summary Results
80

40

Adaptive LR-26

30
20
10
0
0

30 60 90 120 150 180 210 240 270 300
Examples Training Set

LR-26
avg. across trials
Adaptive
LR-26

Statistical
Error
Rate
Solution Rate
% solvable probs.

Average Solution Time (CPU seconds)

ADAPTIVE PROBLEM SOLVING

80
40

best strategy

24

worst strategy

55

predicted

5%

observed

3%

LR-26
avg. across trials
Adaptive
LR-26

Dist. 1

79%
95%

best strategy

97%

worst strategy

86%

Figure 9. Learning curve showing performance function number training examples
table experimental results.
formation negative incremental utility) performing eighty runs system eighty distinct training sets drawn randomly problem distribution. measure distributional sensitivity technique evaluating adaptive scheduler second distribution problems.
Recall purposely excluded inherently difficult scheduling problems augmented set
problems. added, excluded problems make adaptation difficult strategy
likely provide noticeable improvement within five minute resource bound. second
evaluation includes difficult problems
third evaluation assesses relative quality strategies identified Adaptive LR-26
compared strategies strategy space. inferred comparing expected utility learned strategies several strategies drawn randomly space. also provides opportunity assess quality expert strategy, thus give sense challenging improve it.
COMPOSER, statistical component adaptive scheduler, two parameters govern
behavior. parameter specifies acceptable level statistical error (this chance
technique adopt bad transformation reject good one). Adaptive LR-26, set
standard value 5%. COMPOSER bases statistical inferences minimum n0 examples.
Adaptive LR-26, n0 set empirically determined value fifteen.
6.2 Overall Results DSN DISTRIBUTION
Figure 9 summarizes results adaptive problem solving constructed DSN problem
distribution. results support two primary claims. First, system learned search control
strategies yielded significant improvement performance. Adaptive problem solving reduced
average time solve problem (or prove unsatisfiable) 80 40 seconds (a 50%

385

fiGRATCH & CHIEN

improvement). Second, observed statistical error fell well within predicted bound. 370
transformations adopted across eighty trials, 3% decreased expected utility.
Due stochastic nature adaptive scheduler, different strategies learned different trials. learned strategies produced least improvement performance. best
strategies required 24 seconds average solve problem (an improvement 70%).
fastest adaptations occurred early adaptation phase performance improvements decreased steadily throughout. took average 62 examples adopt transformation. Adaptive LR-26 showed improvement non-adaptive scheduler terms number
problems could solved (or proven unsatisfiable) within resource bound. LR-26 unable
solve 21% scheduling problems within resource bound. One adaptive strategy substantially reduced number 3%.
analysis learned strategies revealing. performance improvement (about
one half) traced modifications LR-26s weight search method. rest improvements divided equally among changes heuristics value ordering, constraint selection,
refinement. expected, changes primary constraint ordering degraded performance.
top three strategies illustrated Figure 10.
1) Value ordering:
Weight search:
Primary constraint ordering:
Secondary constraint ordering:
Refinement method:

penalize-conflictedness (1c)
first-solution (2d)
penalize-unforced-periods (3h)
prefer-total-conflictedness (3e)
systematic-refinement (4b)

2) Value ordering:
Weight search:
Primary constraint ordering:
Secondary constraint ordering:
Refinement method:

prefer-gain (1a)
first-solution (2d)
penalize-unforced-periods (3h)
prefer-total-conflictedness (3e)
systematic-refinement (4b)

3) Value ordering:
Weight search:
Primary constraint ordering:
Secondary constraint ordering:
Refinement method:

penalize-conflictedness (1c)
first-solution (2d)
penalize-unforced-periods (3h)
penalize-satisfaction-distance (3i)
systematic-refinement (4b)

Figure 10: three highest utility strategies learned Adaptive LR-26.

weight search, learned strategies used first-solution method (2d). seems
that, least domain problem distribution, reduction refinement search space
results better relaxed solutions offset additional cost weight search.
scheduler did, however, benefit reduction size results systematic refinement method.

386

fi140

LR-26

Adaptive LR-26

120
100

Adaptive
LR-26

LR-26
avg. across trials

156

best strategy

133

worst strategy

150

predicted

5%

observed

6%

LR-26
avg. across trials

51%

best strategy

57%

worst strategy

51%

Statistical
Error Rate

80
60
40
20
0
0 30 60 90 120150180210240270300

Dist. 1

Summary Results
Avg. Time
seconds per prob.

160

Solution Rate
% solvable

Average Solution Time (CPU seconds)

ADAPTIVE PROBLEM SOLVING

Adaptive
LR-26

146

54%

Examples Training Set

Figure 11. Learning curves table experimental results showing performance
augmented distribution (including intractable problems).
interestingly, Adaptive LR-26 seems rediscovered common wisdom heuristic constraint-satisfaction search. exploring new refinements, often suggested chose
least constrained value constrained constraint. best learned strategies follow
advice worst strategies violate it. best strategy, time period lowest conflictedness least constraining (in sense tend produce least constraint propagations) thus produces least commitments resulting partial schedule. argument, constraint highest total conflicted tend hardest satisfy.
6.3 Overall Results FULL AUGMENTED DISTRIBUTION
Figure 11 summarizes results augmented distribution. expected, distribution
proved challenging adaptive problem solving. Nevertheless, modest performance
improvements still possible, lending support claimed generality adaptive problem
solving approach. Learned strategies reduced average solution time 156 146 seconds (an
6% improvement). best learned strategies required 133 seconds average solve problem
(an improvement 15%). observed statistical accuracy significantly differ
theoretically predicted bound, although slightly higher expected: 397 transformations
adopted across trials, 6% produced decrease expected utility. introduction
difficult problems resulted higher variance distribution incremental utility values
reflected higher sample complexity: average 118 examples adopt transformation.
improvement noted supposedly intractable problems. One strategy learned
Adaptive LR-26 increased number problems could processed within resource bound
51% 57%.
One interesting result evaluation that, unlike previous evaluation, best learned
strategies use truncated-dual-descent weight search method (the strategies similar along
control dimensions). illustrates even modest changes distribution problems

387

fiGRATCH & CHIEN

influence design tradeoffs associated problem solver: case, changing tradeoff
weight refinement search.
6.4 Quality Learned strategies
third claim that, practice, COMPOSER identify strategies rank highly judged
respect whole strategy space. secondary question well expert strategy
perform. improvements Adaptive LR-26 little significance expert strategy
performs worse strategies space. Alternatively, expert strategy extremely
good, improvement compelling.
way assessing claims estimate probability selecting high utility strategy
given choose randomly one three strategy spaces: space possible strategies
(expressible transformation grammar), space strategies produced Adaptive LR-26,
trivial space containing expert strategy. corresponds problem estimating
probability density function (p.d.f.) space: p.d.f., f(x), associated random variable
gives probability instance variable value x. specifically want estimate density functions, fs (u), probability randomly selecting strategy space
expected utility u.
use non-parametric density estimation technique called kernel method estimate fs (u)
(as Smyth, 1993). estimate density function whole space, randomly selected
tested thirty strategies. learned strategies used estimate density learned
space. (In cases, five percent data withheld estimate bandwidth parameter used
kernel method.) p.d.f. associated single expert strategy estimated using normal model fit 1000 test examples previous evaluation.
6.4.1

DSN DISTRIBUTION

Figure 12 illustrates results DSN distribution. evaluation learned strategies
significantly outperformed randomly selected strategies. Thus, one would select test
many strategies random finding one comparable expected utility one found
Adaptive LR-26. results also indicate expert strategy already good strategy (as
indicated relative positions peaks expert random strategy distributions),
indicating improvement due Adaptive LR-26 significant non-trivial.
results provide additional insight Adaptive LR-26s learning behavior. p.d.f
learned strategies contains several peaks, graphically illustrates different local maxima exist
problem. Thus, may benefit running system multiple times choosing
best strategy. also suggests techniques designed avoid local maxima would beneficial.
6.4.2

FULL AUGMENTED DISTRIBUTION

Figure 13 illustrates results full augmented distribution. results similar DSN
distribution: learned strategies outperformed expert strategy turn
outperformed randomly selected strategies. data shows expert strategy
significantly better randomly selected strategies. Together, two evaluations support
claim Adaptive LR-26 selecting high performance strategies. Even though expert strategy
quite good compared complete strategy space, adaptive algorithm able
improve expected problem solving performance.

388

fiADAPTIVE PROBLEM SOLVING

Improved Performance

Probability

0.140
0.120

Expert Strategy

0.100
0.080

Learned Strategies

0.060
0.040

Random Strategies
0.020
0
0

20

40

60

80

100

120

140

160

180

Negative Expected Utility

Figure 12: DSN Distribution. graph shows probability obtaining
strategy particular utility, given chosen (1) set strategies,
(2) set learned strategies, (3) expert strategy.

Probability

Improved Performance
0.120
0.100

Expert Strategy
0.080
0.060

Learned Strategies
0.040

Random Strategies
0.020
0
0

30

60

90

120

150

180

210

240

270

Negative Expected Utility

Figure 13: Full augmented distribution. graph shows probability obtaining strategy particular utility, given chosen (1) set
strategies, (2) set learned strategies, (3) expert strategy.

389

fiGRATCH & CHIEN

7.

Future Work

results applying adaptive approach deep space network scheduling promising.
hope build success number ways. discuss directions relate
three basic approaches adaptive problem solving: syntactic, generative, statistical.
7.1 Syntactic Approaches
Syntactic approaches attempt identify control strategies analyzing structure domain
problem solver. LR-26, use meta-control knowledge seen syntactic approach;
although unlike syntactic approaches attempt identify specific combination heuristic
methods, meta-knowledge (dominance indifference relations) acts constraints
partially determine strategy. advantage weakening syntactic approach
lends natural complementary interaction statistical approaches: structural
information restricts space reasonable strategies, explored statistical
techniques. important question concerning knowledge extent contribute
success evaluations, and, interestingly, could information derived
automatically structural analysis domain problem solver. currently
performing series experiments address former question. step towards resolving
second question would evaluate context LR-26 structural relationships
suggested recent work area (Frost & Dechter, 1994, Stone, Veloso & Blythe, 1994).
7.2 Generative Approaches
Adaptive LR-26 uses non-generative approach conjecturing heuristics. experience
scheduling domain indicates performance adaptive problem solving inextricably tied
transformations given expense processing examples. inductive
learning technique relies good attributes, COMPOSER effective, must exist good
methods control points make strategy. Generative approaches could improve
effectiveness Adaptive LR-26. Generative approaches dynamically construct heuristic methods
response observed problem-solving inefficiencies. advantage waiting inefficiencies
observed twofold. First, exploration strategy space much focused
conjecturing heuristics relevant observed complications. Second, conjectured
heuristics tailored much specifically characteristics observed
complications.
previous application COMPOSER achieved greater performance improvements Adaptive LR-26, part exploited generative technique construct heuristics (Gratch & DeJong, 1992). Ongoing research directed towards incorporating generative methods Adaptive
LR-26. preliminary work analyzes problem-solving traces induce good heuristic methods.
constraint value ordering metrics discussed Section 5.1.3 used characterize
search node. information fed decision-tree algorithm, tries induce effective
heuristic methods. generated methods evaluated statistically.
7.3 Statistical Approaches
Finally directions future work devoted towards enhancing power basic
statistical approach, Adaptive LR-26 particular, statistical approaches general.

390

fiADAPTIVE PROBLEM SOLVING

scheduler, two important considerations: enhancing control grammar
exploring wider class utility functions. Several methods could added control grammar.
example, informal analysis empirical evaluations suggests scheduler could
benefit look-back scheme backjumping (Gaschnig, 1979) backmarking (Haralick
& Elliott, 1980). would also like investigate adaptive problem solving methodology
richer variety scheduling approaches, besides integer programming. Among would
powerful bottleneck centered techniques (Biefeld & Cooper, 1991), constraint-based
techniques (Smith & Cheng, 1993), opportunistic techniques (Sadeh, 1994), reactive techniques
(Smith, 1994) powerful backtracking techniques (Xiong, Sadeh & Sycara, 1992).
current evaluation scheduler focused problem solving time utility metric,
future work consider improve aspects schedulers capabilities. example,
choosing another utility function could guide Adaptive LR-26 towards influencing aspects
LR-26s behavior as: increasing amount flexibility generated schedules, increasing robustness generated schedules, maximizing number satisfied project constraints,
reducing implementation cost generated schedules. alternative utility functions
great significance provide much greater leverage impacting actual operations.
example, finding heuristics reduce DSN schedule implementation costs 3% would
much greater impact reducing automated scheduler response time 3%. preliminary work focused improving schedule quality (Chien & Gratch, 1994).
generally, several ways improve statistical approach embodied COMPOSStatistical approaches involve two processes, estimating utility transformations exploring space strategies. process estimating expected utilities enhanced
efficient statistical methods (Chien, Gratch & Burl, 1995, Moore & Lee, 1994, Nelson & Matejcik,
1995), alternative statistical decision requirements (Chien, Gratch & Burl, 1995) complex
statistical models weaken assumption normality (Smyth & Mellstrom, 1992). process
exploring strategy space improved terms efficiency susceptibility
local maxima. Moore Lee propose method called schemata search help reduce combinatorics search. Problems local maxima mitigated, albeit expensively, considering
k-wise combinations heuristics (as MULTI-TAC) level 2 Adaptive LR-26s search),
standard numerical optimization approaches repeating hillclimbing search several times
different start points.
ER.

One final issue expense processing training examples. LR-26 domain cost
grows linearly number candidates hillclimbing step. bad
complexity standpoint, pragmatic concern. proposals reduce
expense gathering statistics. previous work (Gratch & DeJong, 1992) exploited properties
transformations gather statistics single solution attempt. system required
heuristic methods act pruning refinements guaranteed unsatisfiable. Greiner
Jurisica (1992) discuss similar technique eliminates restriction providing upper lower bounds incremental utility transformations. Unfortunately, neither approaches
could applied LR-26 devising methods reduce processing cost important direction
future work.

391

fiGRATCH & CHIEN

8.

Conclusions

Although many scheduling problems intractable, actual sets constraints problem
distributions, heuristic solutions provide acceptable performance. frequent difficulty
determining appropriate heuristic methods given problem class distribution challenging
process draws upon deep knowledge domain problem solver used. Furthermore,
problem distribution changes time future, one must manually re-evaluate
effectiveness heuristics.
Adaptive problem solving general approach reducing developmental burden.
paper described application adaptive problem solving, using LR26 scheduling system
COMPOSER machine learning system, automatically learn effective scheduling heuristics
Deep Space Network communications scheduling. demonstrating application
techniques real-world application problem, paper makes several contributions. First,
provides example wide range heuristics integrated flexible problem-solving architecture providing adaptive problem-solving system rich control space search.
Second, demonstrates difficulties local maxima large search spaces entailed
rich control space tractably explored. Third, successful application COMPOSER statistical techniques demonstrates real-world applicability statistical assumptions underlying
COMPOSER approach. Fourth, significantly, paper demonstrates viability
adaptive problem solving. strategies learned adaptive problem solving significantly outperformed best human expert derived solution.

Appendix A. Determination Resource bound
good CPU bound characterize intractable problems characteristic
increasing bound little effect proportion problems solvable. order
determine resource bound define intractable DSN scheduling problems empirically
evaluated likely LR26 able solve problem various resource bounds.
Informally, experimented find bound 5 CPU minutes. formally verified bound
taking problems solvable within resource bound 5 CPU minutes, allowing LR26
additional CPU hour attempt solve problem, observing affected solution rate.
expected, even allocating significant CPU time, LR26 able solve many
problems. Figure 14 shows cumulative percentage problems solved;
solvable within 5 minute CPU bound. curve shows even another CPU hour (per
problem!), 12% problems became solvable. graph also shows 95%
confidence intervals cumulative curve. light results, fact one learned
strategy able increase 18% percentage problems solvable within resource bound
even impressive. effect, learning strategy greater impact allocating another
CPU hour per problem.

Acknowledgements
Portions work performed Jet Propulsion Laboratory, California Institute
Technology, contract National Aeronautics Space Administration portions
Beckman Institute, University Illinois National Science Foundation Grant
NSFIRI9209394.

392

fiADAPTIVE PROBLEM SOLVING

Probability

1.0

0.8

0.6

0.4

0.2

0
0

300

600

900

1200

1500

1800

2100

2400

2700

3000

3300

3600

Seconds

Figure 14: Given problem cannot solved five minutes, show probability
solved hour time (with 95% confidence intervals).

References
Baker, A. (1994). Hazards Fancy Backtracking. Proceedings AAAI94.
Bell, C., & Gratch, J. (1993). Use Lagrangian Relaxation Machine Learning Techniques
Schedule Deep Space Network Data Transmissions. Proceedings 36th Joint National
Meeting Operations Research Society America, Institute Management Sciences.
Biefeld, E., & Cooper, L. (1991). Bottleneck Identification Using Process Chronologies. Proceedings IJCAI91.
Chien, S. & Gratch, J. (1994). Producing Satisficing Solutions Scheduling Problems: Iterative
Constraint Relaxation Approach. Proceedings Second International Conference
Artificial Intelligence Planning Systems.
Chien, S., Gratch, J. & Burl, M. (1995). Efficient Allocation Resources Hypothesis
Evaluation: Statistical Approach. Institute Electrical Electronics Engineers Transactions Pattern Analysis Machine Intelligence 17(7), 652665.
Dechter, R. & Pearl, J. (1987). NetworkBased Heuristics ConstraintSatisfaction Problems.
Artificial Intelligence 34(1) 138.
Dechter, R. (1992). Constraint Networks. Encyclopedia Artificial Intelligence, Stuart C. Shapiro (ed.).

393

fiGRATCH & CHIEN

Etzioni, E. (1990). Prodigy/EBL Works. Proceedings AAAI90.
Etzioni. O. & Etzioni, R. (1994). Statistical Methods Analyzing Speedup Learning Experiments.
Machine Learning 14(3), 333347.
Fisher, M. (1981). Lagrangian Relaxation Method Solving Integer Programming Problems.
Management Science 27 (1) 118.
Frost, D. & Dechter, R. (1994). Search Best Constraint Satisfaction Search. Proceedings
AAAI94.
Gaschnig, J. (1979). Performance Measurement Analysis Certain Search Algorithms. Technical Report CMUCS79124, CarnegieMellon University.
Ginsberg, M. (1993). Dynamic Backtracking. Journal Artificial Intelligence Research 1, 2546.
Gratch, J. & DeJong, G. (1992). COMPOSER: Probabilistic Solution Utility Problem Speed
Learning. Proceedings AAAI92.
Gratch, J., Chien, S., & DeJong, G. (1993). Learning Search Control Knowledge Deep Space Network Scheduling. Proceedings Ninth International Conference Machine Learning.
Gratch, J. & DeJong, G. (1996). Decisiontheoretic Approach Adaptive Problem Solving. Artificial Intelligence (to appear, Winter 1996).
Greiner, R. & Jurisica, I. (1992). Statistical Approach Solving EBL Utility Problem. Proceedings AAAI92.
Haralick, R. & Elliott, G. (1980). Increasing Tree Search Efficiency Constraint Satisfaction Problems. Artificial Intelligence 14, 263313.
Held, M. & Karp, R. (1970). Traveling Salesman Problem Minimum Spanning Trees. Operations Research 18, 11381162.
Holder, L. (1992). Empirical Analysis General Utility Problem Machine Learning. Proceedings AAAI92.
Kambhampati, S., Knoblock, C. & Yang, Q. (1995). Planning Refinement Search: Unified
Framework Evaluating Design Tradeoffs Partial Order Planning. Artificial Intelligence:
Special Issue Planning Scheduling 66, 167238.
Kwak, N. & Schniederjans, M. (1987). Introduction Mathematical Programming, New York:
Robert E. Krieger Publishing.
Laird, J., Rosenbloom, P. & Newell, A. (1986). Universal Subgoaling Chunking: Automatic
Generation Learning Goal Hierarchies. Norwell, MA: Kluwer Academic Publishers.

394

fiADAPTIVE PROBLEM SOLVING

Laird, P. (1992). Dynamic Optimization. Proceedings Ninth International Conference
Machine Learning.
Mackworth, A. (1992). Constraint Satisfaction. Encyclopedia Artificial Intelligence, Stuart C.
Shapiro (ed.).
McAllester, D. & Rosenblitt, D. (1991). Systematic Nonlinear Planning. Proceedings
AAAI91.
Minton, S. (1988). Learning Search Control Knowledge: ExplanationBased Approach, Norwell, MA: Kluwer Academic Publishers.
Minton, S. (1993). Integrating Heuristics Constraint Satisfaction Problems: Case Study.
Proceedings AAAI93.
Moore, A. & Lee, M. (1994). Efficient Algorithms Minimizing Cross Validation Error. Proceedings Tenth International Conference Machine Learning.
Nelson, B. & Matejcik, F. (1995). Using Common Random Numbers IndifferenceZone Selection Multiple Comparisions Simulation. Management Science.
Sacerdoti, E. (1977). Structure Plans Behavior. New York: American Elsevier.
Sadeh, N. (1994). Microopportunistic Scheduling: Microboss Factory Scheduler. Intelligent Scheduling. San Mateo, CA: Morgan Kaufman.
Segre, A., Elkan, C., & Russell, A. (1991). Critical Look Experimental Evaluations EBL.
Machine Learning 6(2).
Smith, S. & Cheng, C. (1993). Slackbased Heuristics Constraintsatisfaction Scheduling.
Proceedings AAAI93.
Smith, S. (1994). OPIS: Methodology Architecture Reactive Scheduling. Intelligent
Scheduling. San Mateo, CA: Morgan Kaufman.
Smyth, P. & Mellstrom, J. (1992). Detecting Novel Classes Applications Fault Diagnosis.
Proceedings Ninth International Conference Machine Learning.
Smyth, P. (1993). Probability Density Estimation Local Basis Function Neural Networks. Computational Learning Theory Natural Learning Systems 2.
Stallman, R. & Sussman, G. (1977). Forward Reasoning DependencyDirected Backtracking
System ComputerAided Circuit Analysis. Artificial Intelligence 9, 2, 135196.
Stone, P., Veloso, M., & Blythe, J. (1994). Need Different DomainIndependent Heuristics.
Proceedings Second International Conference Artificial Intelligence Planning
Systems.

395

fiGRATCH & CHIEN

Subramanian, D. & Hunter, S. (1992). Measuring Utility Design Provably Good EBL Algorithms. Proceedings Ninth International Conference Machine Learning.
Taha, H. (1982). Operations Research, Introduction, Macmillan Publishing Co.
Wilkins, D. (1988). Practical Planning: Extending Classical Artificial Intelligence Planning
Paradigm. San Mateo, CA: Morgan Kaufman.
Xiong, Y., Sadeh, N., & Sycara, K. (1992). Intelligent Backtracking Techniques Job Shop Scheduling. Proceedings Third International Conference Knowledge Representation
Reasoning.
Yakowitz, S. & Lugosi, E. (1990). Random Search Presence Noise, Application Machine Learning. Society Industrial Applied Mathematics Journal Scientific Statistical Computing 11, 4, 702712.
Yang, Q. & Murray, C. (1994). Evaluation Temporal Coherence Heuristic PartialOrder
Planning. Computational Intelligence 10, 2.

396

fi
ff fi


!"$#%$&('*),+--.0/1''0243'652

B

789:;%$<=+8+>-026?$@9
#& <A.0>-.

CEDGFAHJILKMCEFNILHPO*QRDLFNSUTVDGFXWEYZO[O]\_^`YbaZO]IcFAdfed`g
TVFNDhH

ijFNDLklK0OcHPmRInd`\

WEDoK^qpre0Dhd`m

sEtVu"vwu"xjy*u"xNz|{}u"~%~

|"1| G|"r "

n8(orw(!V8*
wq0wq;;4ww 6f ,
r8w,Ar1V
qu"~,u"vZru|u"t u!u

||||||1 ||

8;N= 4n=8w4E"
q=wf,,!=6 r;n
=w[;"

(V !
80 0$ $q V`$ Z q 4w !;`!M $ $
c $ }1!6 ;V G=% $`w6%4wE $A% 611(` V
$ 1N%$ 4w $ q"*1"V 10|ZN1 (1%4wE $;; (8
$ 84w 1;|610L 0 Vq $ $ ;c`VwV$ 6 !L V
$ $EA!;,%L `( $ }% A0$ E6 $ $8$V" * ;1%
wEV $$ Ef$c Z$nVVEw 0 $ 0w V q0 (1(4q
w 1[! lq ,"wq ,w w0 !0 l0$ Efo1 0qw V"AZ
$4$ 1;%E V $fE q4$ Nf ` `w }oV*V4w f!
V ; $Z$ $o`wl$4wo[ 8E; V$, V
o`V$ r0w ! ! [E $8 0w w $ |180V` !VN0$
,w$V rw0 |611
$ w ( $6 $ AV6
0$




fiff ff























!#"%$'& fi()+*(!fi,( &
-/.10324065870fi9;:=<?>A@CBfiD;E<?>GFC.?FHB'9;><?>GIJ<LKM<?>A2;EGFNDO0fi>2;FOPQ2R0'58DO0fi> DOFTSG2%.?FHB'9 ><1>GIUV-WSG9;0fi2 0fi2YXZS<LDTB[.
FOP B'@\S.?F]065^2;E<LK<LK_.?FHB'9;><?>GI2;0N9;FHDO0fiI3><1`TFRE B'> abc79Y<?2;2;FT>dD E B'9BfiDO2;FT9OKe59;03@fD .gB3K;Kh<ji FHakFOPlB6@\S.?FHKTU
m0fi> DOFTSG2n.?FHB'9;><?>GI
<LK2 EGFoKYpGqGrYFHDO20'5sB6>%<?>Z2;FT> KtF2;EGFT0fi9;FT2t<gDTBu.vKY2;p aX%pG> aFT92;EGF> B'@RF0'5nwyx9;0fiq B6q.1X
-SGSG9;0[PZ<?@]B'2 FO.1Xzm0fi9;9;FHDO2R{|x-
m}o~FHB69;><?>GIZJKt0DTB[.j.?FHakq FHDTB'p KtF]2;EGFR.?FHB'9;>GFT9M<LKM9;FHfip<?9;FHaJ0fi>.?X
2;0.1FHB69;>B6>B'SGSG9 0HP<?@]B'2Y<?0fi>J2;02;EGFC2B'9 IfiFT24DO0fi> DOFTSG2%7<?2;EdBNE<?IfiEkSG9;0fiq B6q<.j<?2YXA{hB[.j<gB6>Z2HeHfi'G}OU
E<LK+9Y<LD;EB6> a\Ifi9 0u7<1>GIMq 0GaX\0'5#:>G0u7.1FHaI3FKY2;p a3<?FHK+2;EGFoS0ZK;Kh<?q<j.j<?2|XN065v.?FHB'9 ><1>GICB'SGSG9;0[PZ<?@]B62Y<?0fi> K
2;0DO0fi> DOFTSG2OK<?>Va3<j#FT9;FT>Z2
9 FTSG9;FHKYFT>2B'2Y<?0fi> K_pG> aFT9uB69Y<?0fip K.?FHB'9;><?>GI=SG9;0fi2;0GDO0'.LKTUC{;fiFTF\B'2B'9B6rtB6>#
Hfilfi-o>2;EG0fi>Xn<?IfiIZKy#HfifilZ039FHB69;> KoB6`O<19OB'><H3'M50fi9B%qG9;0ZBfiaR<?>2;9;0Gap DO2Y<?0fi>#Uj}
>N2;E<LKS B6S FT9HG7FB'9 FDO0fi> DOFT9;>GFHaN7<12 EB\a3<jsFT9 FT>Z2:3<?> aN0'5.?FHB'9;><?>GINDTB[.j.1FHadwTKtS FTFHapGSN.?FHB'9 >Gb
<?>GI k7E<LD EaFHB[.LK\7<?2;Ez<?@\SG9;0'fi<?>GId2;EGFDO03@\SGpG2B'2Y<?0fi> Bu.oFOD <?FT> DOX0'5_BSG9;0fiq.?FT@KY0'.?fiFT9C7<?2;E
FOPGS FT9Y<?FT> DOFfiU>GF0'5
2;EGF=@]B[<?>a3<j#FT9;FT> DOFHKCq FT2Y7FTFT>A2 EGFDO0fi> DOFTSG2R.?FHB'9;><?>GIAB'> ad2 EGFKYS FTFHapGS
.?FHB'9;><?>GI<LKM2;E B'2Hs<?>d2;EGF%.LB'2;2;FT9[#<?2M<LKM2;EGFT0fi9;FT2Y<LDTB[.j.?XkS 0ZK;K|<1q.?FN2;0KY0'.?fiFC2;EGFCSG9;0fiq.?FT@]KM0fiSG2Y<?@]B[.j.?X
p Kh<?>GINB%qG9;pG2;FTb5c0fi9DOFMSG9;0fiq.?FT@Kt0'.?fiFT9HUe0'7^FT3FT9HSG9;0fiq.?FT@Kt0'.?fi<?>GIC7<?2;EG0fipG2.?FHB'9;><?>GIC<LKoxnbcE B'9a
<?>A@\0KY2\0'5
2;EGFHKYFa0fi@CB[<?> KTB'> akEGFT> DOFN<LK<?@\SG9BfiDO2Y<LDTB[.<?>A@\0KY2]DTBfiKYFHKyU EGFN9;0'.?F=0'5o.?FHB'9 ><1>GI
DTB'>q FCKYFTFT>BfiKe<?@\SG9;0'fi<?>GIN2;EGF%FOD <?FT> DOX065^BCqG9;pG2;FTb5c0fi9DOF%SG9;0fiq.?FT@fKY0'.?fiFT9
qXB3DTfip<?9Y<?>GIKt0fi@\F
wTDO0fi>2;9;06.v:>G0u7.1FHaI3FH%2;E B'2n<gKp KYFO5p.2;0%Ifip<LaF
SG9;0fiq.?FT@KY0'.?3<?>GI\<?>C5c9;p<?2Y5p.a3<19 FHDO2Y<?0fi> KTU >B%DO0fi>Gb
DOFTSG2e.1FHB69;><?>GIN2BfiKY:Gq FO5c0fi9;F_.?FHB'9;><?>GI 2;EGFT9;F_<LK>G0fi2FT>G0fipGIfiE<?>50fi9 @]B'2Y<?0fi>=2;0D .LBfiK Kh<j5XB'>=FOPlB6@\S.?F
+--8.rfi|fi"$##G <r <+[8ff:(19
#& # fi

%&# #c<'

fiG 3TGHH^''GGG;G

TfiTCZRG;G;TcfiOfifiTC;GfiG3\;G
THGC?H';?G\G fifi' 3^3TOH;+;4_G;G;TcfiO
G;fi?TY'?fiT[[1nL^tYj# [j?TGfio
;O3;%g6;+1OGu1H3?]_[
; 6]'3HG;fiG
?Tt'?fi?G\O ?THGT;
[fi TTC'ZNY TOH;|t THG=?H';?GtQY T]H;OY? H
?d;GCOQTY?\TZO[]fi 1GC?H' 1G?;TO';G;fi%4y4|?;fi#HfifiQ8' #M%
|#u1O ZYT?Z3veTOjHfi3 O?GNY^%';G\YG;fi%?GTfiGHT
fi hLT;GfiC[?N'ntZ% 6L81;TfiO'Y?fi#e13TN;GO ?Y?fi';GfiC[?' N
Y6 G'NO'?'?;Tfi[LT'fifiG
fiOfi\?T;M?cfi;]'t13fi=Gu;t'?fi4'NY'?u'?
G;fi?T=THj?N\ZGC' ' T' 61C'oY'?3?GG fi?T]%'oY% 'jL%?;Tfi6Y?fi#
\';GNRH' 1T
?3Go\T GQGT_'GH'; '%GTH;COl'1G\;6\?
? Y' OHy Y; ]t'?GY?fi ;%;GHYe? Y' OHy ' R fiYHCfiC;GHYY61Gt13 GjgG%YT'
GTGYLYYLT_; '
jjT '?] 3L;3?VY'?fiMcG;G;%G;fi?T]TMJ;L
YT Yfi# G?H' 1G
G;GOH; fieGO? H?\G;'fi
fiGOfi\GG't13 [#O ?T Ofi
^Gt1O \H;Ot1o G?;?Y?fiC T? \3G6\Tfi;GT;fiZOcT;Y?G Gcfi;C[lTO[jg
;ML';TYHOY?fi yNH;tT OfiG
^3gj?fifiG+?H';?GCG;fifiO';%T H3? Gc'j?u1G
]'GGT 4Ofi |gT\= g3;
6fi]u1 yt ;k; '
Hfi VfiC[??k;G] Lfi;8gMG';
;YH;+'\O ?Tu13fiY?;G o6;+1;T;Ht;H%?]8?H';?G\[?fifiY?;G fi Go Lfi;
'
[?fifit1 G; ''3H^3?GGG;GYH vT6Y?fi'\fiC[?[\c;fi ;G Lfi fiOjfi
;'R1+? Y6 OH' ;GG fi?T] ?4 '3][?\'
;GO?Y61Gt13 TZ' _G;G OHfi#3G;GG
'
?Tu13fiY?;Gfi;Gfi][?#

j YTfi[;G^ '\?? Y6 OH' GO1Y'?GY?fi
L[4O; L[ ;'?+?4 G^G QOH;yfifi?\ GO16 YT OfifiOfi Y; OY?G'O ?TG;31Tt'?fiT
cfio G
?GGG4fiC[?T6 \OfiRGG'Y?fi [j?=?Z;OfiO'?fi;Le ' TH%';
?;T;HY H
?]O '%??G\;GOfi 3?Y?fi G TL;NY %?H';?G%L+]fioO3\GG'Y?fi u?\ON 1TZ
h?G\;'\?? Y6 OH^6 ]Y61Gt13 G;HYT
Gj HR3;][6\Tfi; '^T'G G;H
3;YG fiLYH' CG YGT;3gtHNcfi;C6nYTHGN?H';?G GT;
'\?He'Y TOH;hc
G;fi?TY613?G' G u3LHZ;H3;GT' AdYH' AG fifi' ;HYHOY?fiO?fi
G
c'\Tfi;g fiYHN3Yfi\'fiGG;T3?fi fi \;T 3;;HCH'Yj?TM|o6'
t'CfiT uj
Hfi3fi ff#3T [j[fiGHO
GM\T;GGG
'' [?Q|g%' |1L'%;N '
'
?H';?G ?
; 'e^fi?N;H3?;;G_?H';GT;%fiG;GGo'6GG;[Z?]' O1O3;;HOG;31TWY'?fiT?;
?fiG;3 'j?Yfi
' Y%fie?
?H';?G %;H31 ];GM?H';GT%;C NY TOH;|fi'Z
Y6Y?fi ';\G fi?T3LY;t1GGt13GGG'N;\ G1H6;GTH
GT;o [fi TTtfi\fi;GT';;TRG;_3;][j TY THG?H';?GNfi ? 3GT#v[fi G
O1GT?Z tfi#H3fiGfi fiGGO']'L'N
eG;TH [fi
'TfiTH3\ZY+'#;GHYe3;][j H'
Y?fi n'sYTHG\?H' 1GM Yo\H3YG;'G;fi?T\Y613?G% TYcfi;]6 Ot ;]3;GG4T
'GGHOG ' H=?Y'?3?G%G;fi?T 3GT#H3 fi;GMZG% T'GjvT'Y?fi MfiG
?6 YTY?G3GT;k;;O?GT4? Y3eHfifiO \ Oj?TfiN; 6;GHYR\HfiYG;HM';\ Z
GfiO[?GH%;
YOcvfiecfiG G't13%3;fiG Y;GT3;'YTHG\?H' 1G\OfiR ''?
;%;G
' [?GhL'Ofi OTG1H6;?G\?N;G
?H';?GR6\Tfi; G6?'?G\;G
Y6 G'
GfiOtgO?NOfi\?O1YC;GTfi;fi Yo Gofit;TfiYo3YZRG;fiYLOfi\?OZ?YNfifiG\H3YG;
' TYcfi;]6 Ofi %;H31 ]NY TOH;hc^YTHG?H';?G=G;fifi6;C;HY?_1k\G;31T
Y'?fiT+GZYfit;TfiYfiY\G;fiYLO3\?OZ?YMgn T; T; '%; 6'v;GeGfi?H';?G%G;G;T
cfiOG;fi?TY'?fiTH=fi;T'fiT[H;G1H6;?G
?YOj% YOfi tG\fi?\ '?Gfi%L['\3GZ
'Y?\%' R 61Gfi%L[G4T
'Ol'R1Hyoefi; 'H fiTO33?G];CfiGoO ?Y?fi# ;G
Y6 G'Rcfi;]6^Ofi\j?T
fiGY?% H'Y?fi
3+?Zfi=GG;'jj?G ' ?\G;'fiT\T+?;G
'' \fiL V G\G;fifiO'WL GVGfifi ujc3?H';?GG;GOH;YHM HT' t];GT
\Gfi 'Gfi
;G3YZRG;fiYLOfi\?OZ?YN'G fi?TY613?G
e'^T3TH\fi;
YfiGLYYLT' H
!

fi"$#&%('fi)*fi+,#-'fi*fi).0/1%('fi243!%('456..0798fi6;:9.*fi'fi<fi=><9?

@BACEDGFHAI@JfiCEAKCMLIDONECPLIQ9FR@BACED,LINTSAQGFTU9VXWYL0FJ9L
CENTSZL[\^]IL[U9LINTSAQYL
CE\_VMAQ9FRSZ`a\^CE\`bLFH[\LICEQaSQfiK,JfiCEAc
]0S>`a\`dNEWfi\^efSD JfiCEAI]\gNXWfi\LFTeaD JfiNEANhS>ViVMAD Ja[\MjkSNTe AI@-JfiCEA0KCPLIDl\Mjfi\VMUfiNTSAQ-mnoWaU9F^p0qr\is9\M[tS\^]\NEW9LIN
SD JfiCEAI]0SuQfiKNXWfi\_qoACPFTNXcvV^LFT\LFTeaD JfiNEANTSZVfVMAD Ja[\MjkSNTewA
@xJfiCEA0sa[u\^DyFhAI[]SQfiKwV^LIJfiNEUfiCX\FL,VMAKQaSNTS]\M[e
SQkNX\^CE\FTNTSQfiKb@zA0CED{AI@|FhJ9\^\`aUfiJ}[u\L
CEQaSQfiK9m,~g[NEWfiAUfiK0WLIQaeY`a\VMCX\LFT\dSuQLFTeaD JfiNEANTSZV VMA0D Ja[\MjkSNTebS>F
U9FT\M@BUa[xLIQ9`dSQaNE\^CE\FTNTSQfiK9pSuQbNEWaSZFoJ9LIJ\^Cqo\qoSt[[s9\_VMA0Q9VM\^CEQfi\`qoSNEW,[\LICXQaSuQfiK,AI@J9AI[eaQfiADSZL[cBNTSD \
JfiCEAsa[\^DFhAI[]\^CPF@BACg`aADGL!SuQ9FHqgWaSZVXWV^L
QGAQa[e,s9\FTAI[]\`dSQ,\MjJAQfi\^QkNhS>L![NTSD \iSuQ,NEWfi\qoACPFTNoV^LFT\
qoSNEWfiAUfiN|[u\L
CEQaSQfiK9m
Q\VMNTSAQ(pqr\fSQaNECEAfi`aU9VM\ NEWfi\ JfiCE\M[tSDSQ9LICTS\FA
@rJfiCEAsa[\^DFTAI[]SQfiK;LIQ9`4~[\LICEQaSQfiK9m Q
\VMNTSAQ fipqo\SQkNXCEA`aU9VM\oAUfiC@BACEDGL[I@BCPLID \^qoACXg@BACFTJ9\^\`aUfiJf[\LICEQaSQfiK9mCMLq|SuQfiKiAQ_JfiCTSACCX\FTUa[NPF
SQY~c[\LICXQaSuQfiKpfiqo\JfiCEAI]\1LK\^Qfi\^CPL[-NEWfi\^ACX\^DOSZ`a\^QkNTSt@BeSQfiKwVMAQ9`0SNTSAQ9FFTUaVXS\^QaNgNEAGL![[AIqFTU9VXW
[\LICEQaSQfiK9m Q\VMNTSAQpqr\,LIJfiJa[eYA0UfiC@BCPLID \^qoACENXA,[u\L
CEQaSQfiKVMA0QkNECXAI[xCXUa[u\FLIQ9`}`a\FEVMCTSs9\LIQ
SD Ja[\^D \^QaNPLINTSAQLIQ9`}\MjfiJ9\^CTSD \^QaNPL[oCE\FTUa[NPFSQNEWfi\GFTeaDs9AI[tSZVdSQkNE\^K0CPLINTSAQ`aADGLSQ-m Q\VMNhSuA0Q
pqo\_LIJfiJa[ewAUfiCi@zCMLID \^qoACE NXA [\LICEQaSQfiKDGL0VMCEAcBAJ\^CPLINEACMFHSQwNEWfi\ `aADGL!SuQbAI@rSKWaNUfi^M[\m Q
\VMNTSAQ(pqr\,`0S>FXVMU9FEFAUfiCqoACXSQ4CE\M[ZLINTSAQ}NEAGJfiCX\^]SAU9F@BACEDGL[tSLINTSAQ9FA
@|FTJ\^\`aUfiJ4[\LICEQaSQfiK9m
Q\VMNTSAQ}kpqr\_`0SZFEVMU9FXFFTAD \@zUfiNEUfiCX\_\MjfiNE\^Q9FRSAQ9FNEA A0UfiCg@BCPLID \^qoACXp0SQ9VX[U9`0SuQfiKb[\LICEQaSQfiK,@BCEAD
UfiQ9FTAI[]\`JfiCEAsa[\^DGFLIQ9`,\Mjfi\^CPVXSZFT\F^mi\VMNTSAQY VMA0Q9VX[uU9`a\FNXWfi\J9LIJ9\^Cm
09vRvHvfi
NEWfiAUfiN[AkFXFAI@K\^Qfi\^CPL[tSNTepkqo\gLFEFhUfiD \!fi
rNEAs\gNEWfi\L![uJfiW9L
s9\^NoAI@NXWfi\o[ZLIQfiKU9LIK\iAI@FTNPLINE\
`a\FEVMCTSJfiNTSAQ9FpLIQ9`U9FT\d@zA0CrNXWfi\_FT\^NgAI@saSQ9LICEewFTNEChSuQfiKaFoAI@[u\^QfiK0NEWYm
~Tk^tl_SZFLiNEUfiJa[\_TP MPpqgWfi\^CE\pa4 SZFLFh\^NAI@B0B^^pSZFJfiCEAVM\`aUfiCX\
NEACE\VMAKQaS^\LFhUfis9FT\^NAI@FTNPL
NE\FSQwLFNXWfi\ra^BB^^pLIQ9`SZFLFT\^NAI@xMfiTBXI!EMIIkp
qgWfi\^CE\\LVXW4SZF_L,JfiCEAfiVM\`aUfiCE\qgWaSZVXW4NPLI\FL,FTNPL
NE\SQL0FoSQfiJfiUfiNL
Q9`wA0UfiNEJfiUfiNPFLIQfiANXWfi\^C_FTNPLINE\
L[ZFTASQrmnoWfi\4VMADsaSuQ9L
NTSAQAI@KAaL[ZFGLIQ9`AJ\^CPLINEA0CPFfSZFV^L[t[\`NEWfi\}vfiP oP0BZ}AI@bm~
BIT,
S>FL FT\^NoAI@`aADGL!SuQ9F`a\M9Qfi\`AI]\^CiNEWfi\_FELID \_Fh\^NgAI@FTNPLINE\Fm
\`a\^QfiANE\gNXWfi\CE\FhUa[uNoAI@-LIJfiJa[eSQfiKGL
Q AJ9\^CMLINEACoNEALfFTNPLINE\gsae TIPm~Tk^tS>FoLfFTNPLINE\
f;rm~JfiCEAsa[\^D{S>F^tkHSt@NEWfi\^CX\S>FL FT\0Ufi\^Q9VM\_A
@xAJ\^CPLINEA0CPFHlp-p9PpLIQ9`L
FT\0Ufi\^Q9VM\AI@FTNPLINX\Fop9pfiMpFhU9VEW,NEW9LINoLxf^pvs-@BACoL[t[&@BCEADiNEApkh^9Pp
LIQ9`Vx FXLINTSZFR9\FHNEWfi\gKAaL[&dm QGNXWaS>FoV^LFh\pkSZFrLd^tkB,^Pk9MAI@pfiLIQ9`,SZFNXWfi\ta
AI@NEWfi\_FTAI[UfiNTSAQ;FT\0Ufi\^Q9VM\_m
n|Wfi\a^t^^SZFLFTeaQkNMLVMNTSZVHD \LFTUfiCE\iAI@9NEWfi\VMADdJa[u\MjaSNTeA
@&LJfiCEAsa[\^DFTU9VXWGLFSNPF[\^QfiKNXW
qgWfi\^Qb\^Q9VMA`a\`SuQbsaSQ9LICEem @SZFLIQwLICEsaSNECMLICEe JfiCEA0sa[u\^DSQ44paNEWfi\^QSNPFgFRS^\k0SZFm
ANTSZVM\NEW9L
NA0UfiC`aA0DGLSQFTJ\VXStV^LINTSAQSZFQfiA0NGLFf\MjJa[tSZVXSNwLFfNEWfi\w`aADGL!SuQNXWfi\^ACEeU9FT\`}SQ
NTekJaSZV^L[FhJ9\^\`aUfiJ[\LICEQaSQfiKfJfiCEAKCPL
DGF[tS\_ $ SQkNEA0Q-pk aPmnoWfi\oAJ9\^CPL
NEACPFQfi\^\`fQfiAN
s9\g`a\FEVMChSus\`fSQNEWfi\ _M @BACEDGL[tSZFTDwpL
Q9`KAkL![>FQfi\^\`fQfiANs9\H[AKISZV^L[fi@BACEDUa[ZLF^m Qf@ LVMNpNEWfi\^e
Qfi\^\`dQfiANs\1`a\VX[ZLICPL
NTS]\M[e CE\^JfiCE\FT\^QaNE\`,LINL[t[vpfisfiUfiND,Lefs9\`a\FEVMCTSs9\`,sae JfiCEAfiVM\`aUfiCE\FHqgWfiAkFh\gCEUfiQ
NTSD \iSZFCX\LFTAQ9LIsa[e sAUfiQ9`a\`mnoWaU9F^paAUfiC [u\L
CEQaSQfiK@zCMLID \^qoACEfCE\UaSCE\FHNEWfi\i[\LICEQaSQfiK NE\VXWfiQaSZUfi\F
NEA s\_D A0CE\SQ9`a\^J9\^Q9`a\^QaNAI@xNXWfi\_AJ\^CPLINEACoCE\^JfiCX\FT\^QkNMLINTSAQNEW9LIQbNEWfi\NECPL0`0SuNhSuA0Q9L[D \^NEWfiAfi`fiF^mn|WaS>F
L[t[AIqFgVXWfiAkAkFSuQfiKdNEWfi\A0J9\^CPLINXACoCE\^JfiCE\FT\^QaNPLINTSAQ,qgWaSZVXWSZFos9\FhN|FhUaSuNX\`NEANEWfi\`aADGLSQCPL
NEWfi\^CoNEW9LIQ
s9\MSQfiKVMAQ9FTNECMLSQfi\`saeGNEWfi\LFEFTUfiDdJfiNTSAQ9FoAI@NEWfi\[\LICEQaSQfiK,NE\VEWfiQaSZ0Ufi\m
QNEWfi\FTJ9\^\`aUfiJb[\LICEQaSQfiKGFTefiFTNE\^D,FrFhNEU9`0S\`saeGNEWfi\\MjfiJ9\^CTSD \^QaNPL[xVMA0D DUfiQaSuNTepfiNXWfi\KAaL[ZFgLIQ9`
AJ\^CPLINEACMFL
CE\U9FTU9L[t[eJ9LICPLIDd\^NE\^CTS^\`mnoWfi\FT\wFheFTNX\^DGFL[ZFTA;[\LICEQVMAQkNXCEAI[gCEUa[\F LIQ9`DGLVMCEAc
AJ\^CPLINEACMF_q|SuNXWJ9LICPL
\^NE\^CPF^m \L
CEQaSQfiKJ9LICMLID \^NE\^CTS^\`CEUa[\F,LIQ9`DGLVMCXAcBAJ9\^CMLINEACPFfDGL
\FfSN
J9AaFEFRSsa[\gNEALIJfiJa[efNEWfi\^DNXAgJfiCEAsa[\^DGFAI@9LICEsaSNECMLICEe_FRS^\m~QfiA0NEWfi\^CxL0`a]!LIQaNPLIK\A
@9J9LICPLID \^NX\^CTSLINTSAQ







ff

fi





fi
!!"#$%
&('*),+-.!/0&212&3)45)$67.!880134)$+-'$.!9:-<;$=013->;$-?@=;@'A&3BC-@134D9E.!F04G)H&39:-'I!JK+-;$-<-.C?,+.!88012&(?.!)H&36CFL/0&3FM'N)$+8.!;O.P9:-)$-;O'N)$6DM&2Q*-;$-F0).!;,RC=9:-F0)O'SUTVF0WX6C;$)$=F.!),-@1Y4CI+6!J-B-;IC8.!;@.!9:-)$-;H&3Z.!)[&Y6F\.1]'[6&3F?@;$-.C'H-'
)$+-^?@6C9:8=)@.!)H&36CF._1?@6`'[)a6!W&3F'H)O.!F0)H&(.!)H&3FRb),+-^6C8-;@.!)$6C;O'dce6C;f;$=013-'@gOSihj+-Fj)$+-^F0=9L/-;a6!W
8.!;O.P9:-)$-;O'5?.!Fk/l-m.P;$/0&3)$;O.!;H&2134n+0&YR+*Io)$+-p&YF'[)O.!F0)H&(.!)H&36CFn8;$6C/013-9q&]'pr7sute?@6C9:8013-)$-p&YF R-F-;O._1eS
v F-JK._4),6),+-6C;$-)H&(?._12134f12&Y9\&Y)V)$+0&('K?@6`'H)<&(')$65=88-;$tX/l6C=FMw)$+-7F0=9L/-;K6PWU8.!;O.!9p-)$-;O'o6PWU)$+6C8l-;O.!)$6C;@'I9f.C?@;$6CtX6C8l-;O.!)$6;O'I.!FM5?@6CF0)$;$6!13tX;$=013-'N)$67.G?@6CF'H)@.!F`)_SUx+0&('y-F'H=;$-'N)$+.!)y/6C)$+5)$+-)H&39:WX6C;G&3F'H)O.!F0)H&(.!)H&36CF .!FMz)$+-pF`=9L/-;56!W{M&2Q*-;$-F0)&3F'H)O.PF`)H&(.!)[&Y6F'.P;$-L86!1340F6C9L&(._1('5&3F^)$+-L13-FRC),+
6!WU),+-D'H)O.!)$-M0-'$?@;H&38)H&36CF*S|A)&('860'$'A&3/013-D)$6:-@})$-FMf6C=;V;$-'H=013)O')$6:'H=?,+a8.!;O.P9:-)$-;H&3Z-M~M069E._&3F'
J&3)$+'H6C9p-'H=0&3)O.!/013-z;$-'H),;H&(?@)H&36CF'f6CFb),+-F0=9L/-;f6!WD8.!;O.P9:-)$-;O'p6C;f)$+-@&3;f&YF0)$-;OM0-8l-FM0-F?,&3-'
cxN.!9L/-CINrK-J-@121eI>6`'[-F`/013606C9~IKC0gOSL|AFzW.?@)I*6C=;L.!88012&(?.!)H&36CF 6!WK)$+-LWX6C;$9f._1UWX;O.!9p-J6;$
)$6z)$+-~'H409L/6!12&(?f&3F0)$-RC;O.P)H&36CFbM06C9E._&3FjM06`-'5&3F0BC6!13BC-~.!Fn&39:8012&(?,&3)a8.!;O.!9p-)$-;:)$+.!):M0-F6C),-'L)$+'H=/l-@}8;$-','A&36CF6PWD)$+-k?@=;$;$-F0)a-@}8;$-'$'A&36CF),6kJK+0&(?,+.!Fj6C8l-;O.!)$6C;f&('a.!88012&3-M
SV6!J-BC-;IWX6C;
'A&39:8012&(?,&3)4L6!W-@}86`'A&3)H&36CF*I!J->?@=;$;,-F`)H134;$-'[)$;H&(?@)*6C=;*),+-6C;$-)H&(?._1CWX;O.!9p-J6;$K)$6KF6F8.!;O.!9:-),-;H&3Z-M
6C8l-;O.!)$6C;@'S
H`2_D W6;.zM06C9E._&3Fbq&('L.zM0-)$-;$9L&3F0&('H)H&(?d8;$6CRC;O.P9)$+.!)5)O.!C-'L.C'ff&3F8=)E.
8;$6C/013-9~IlCI.!FMf6=)$8=)O'<&3)O'K'[6!13=)H&36CF~'H-=-F?@-&2W'H=?,+a-@}`&('H)O'I6C;)$+-D'H8l-?,&].1u'H409L/6!1!K&2Wo&3)
M060-'KF6C)-@}0&('H)S
`,C`]feCOL &(':.z'H-):6PW78;$6C/013-9'H6P1YB-;O'S^|W &(':.z'H8.C?@-a6PW7+`4086)$+-'H-'I<)$+;$-'H),;H&(?@)H&36CFE6PW ),6D8;$6C/013-9E'<6!WU'A&3Z-Lbf&('?._1213-Ma. eC@ 6!W*+`4086)$+-'H-'.!FM5&('M0-F6C)$-Mp/`4
K SK6C;$9E.1134CIW6;7-BC-;,4 ^w I)$+-;$-&('7.f?@6C;$;$-'[86CFM&3FRa8;$6C/013-9'H6P1YB-; _~~V 'H=?,+z)$+.!)
cH
go cg*&2W{NCbw.!FMp=FM0-@F-Mf6C)$+-;$J&('H-CS<x+- 2@CC@(0CLC 6; CLC
6!W>.L+04`8l6C)$+-'A&('G'H8.C?@- &('7M0-@F-Mw)$6L/-136CRff V .!FMp&('GM0-F6C)$-Ma/04a`Awc K gOS
:G
C<e>$C* !*XN>C*<el
|eFf)$+0&(''H-?@)H&36CF*I0J-7M0-'$?@;H&3/l-6C=;o13-.!;$F0&3FRLWX;O.!9p-J6;$SNy&3;O'H)I)$+-7M06C9E.&YFa'H8l-?,&?.P)H&36CFa&('oRP&YB-F
)$6a)$+-513-.!;$F-;_Sx+-p)$-.C?,+-;)$+-F^'H-@13-?@)O'D.!Fn.!;$/0&3)$;O.!;,4a8;$6C/013-9M&('H)$;[&Y/=)[&Y6F.!FMz.f8;$6/01Y-9
'H6!13BC-;_Sbh^-z.C'$'H=9p-a)$+.!):)$+-;,-a&('E.P)L13-.C'H)f6CF-~8;$6/01Y-9'H6!13BC-;p&3Fb)$+-w+`4086)$+-'A&('E'[8.C?@-~6!W
)$+-L13-.!;,F-;D)$+.!)G&('WX=F?@)H&36CF._12134^-C=0&3B!._13-F`):),6a)$+-:)$-.C?,+-;'G8;,6C/013-9'H6!13BC-;_I
&eS-CS3Iy6CF-:JK+0&(?,+
6C=)$8=)@'>),+-D'$.!9:-D'H6P1Y=)[&Y6F~.C')$+-G)$-.C?,+-;'<8;$6C/013-9'H6!13BC-;K6FE-.C?,+a8;$6C/013-9~S<h^-?._121'H=?,+~.
8;$6C/013-9'H6!13BC-;&3Fw)$+-G13-.!;$F-;'V+04`8l6C)$+-'A&('G'H8.C?@-CI. XH`2C2_
x{+-:13-.!;$F0&3FR ._13RC6C;H&3)$+9+.C'L.C??@-'$'L)$6^.!F 6;O.C?,13-~?._1213-Mj v7!> tes< v7*< )L-.C?,+
?._121eIG v7!> tes> v7*< ;O.!FM06C9\1Y4b?,+606`'H-'L.z8;$6C/013-9&3Fb)$+-~?@=;$;,-F`):M06C9E.&YF*IK'[6!13BC-'5&3)
='A&3FR:)$+-G)$-.C?,+-;'u8;$6C/013-9'H6!13BC-;I.!FMp;$-)$=;,F'>),+-3l!X__!XA!*o8._&3;I0JK+0&(?,+f&('?.113-M
.!F !CV2 X(l(0fK
&]'5.a'H-)6!WK'H=?$+)$;O._&3F0&3FR~-@}.!9:8013-'SLh^-:.C','H=9:-L)$+.!)G&2WK)$+8;$6C/013-9&('KF6C)K'H6P1YB!.!/013-L/`4f)$+-D),-.C?$+-;_'<8;$6C/013-9'[6!13BC-;I0&3)K6C=),8=)O')$+-8._&3;L3!!Xz_GOS
|HM0-._12134CIU),+-R6`._1U6!WK'H8--M0=81Y-.P;$F0&3FR~&('G)$6pFM^.f)O.P;$RC-)K8;$6/01Y-9'H6!13BC-;G&3F^)$+-513-.!;$F-;_'
+04`8l6C)$+-'A&('G'H8.C?@-CS>V6!J-B-;I0)$+0&('<&('KF6C)K._13JK._4'<8l6`'$'A&3/013-L/-?.!='H-56C=;K9:6M0-@1U6!Wy1Y-.P;$F0&3FRa;$-@12&Y-'
6CFz;O.!FM069L134?,+6`'H-F~),;O._&3F0&3FR~-@}.!9:8013-'SDV-F?@-CI
J-\.1136!J)$+-513-.!;$F0&3FR~._13RC6C;[&Y),+9)$6E6=)$8=)
.!F^.!88;$6_}0&39E.!)$-@134~?@6C;$;,-?@)8;$6C/013-9'H6!13BC-;J&3)$+^.p+0&3RC+^8;$6C/.P/0&12&3)H4^._W),-;ff'H--@&3FR~.f;$-.'H6CF.!/013F0=9/l-;:6!WK-@}.!9:8013-'Skx{+-a8;$6C/013-9'H6!13BC-;LF--M'L6F01Y4n)$6z/-a.!88;$6_}0&39E.!)$-@134 ?@6C;$;$-?@)5&3F )$+'H-F'H-5)$+.!)V&3)79E._4pW.&1o)$6:8;,6M0=?@-:.:?@6C;,;$-?@)ff'[6!13=)H&36CFwW6C;G.:8;$6/01Y-9J&3)$+z.E'H9f._1218;$6/.!/0&21&3)H4
-BC-F^),+6C=RC+^)$+-p)$-.C?,+-;'H=??@--M'&3F'[6!13BC&3FRw&3)Sfh^-E.!;$-pF6J;,-.CM04z)$6aWX6C;$9E.1134^M0-@F-f6C=;
9:6M0-@1U6!WN13-.!;$F0&3FRS


fi





ff
fi ff

ff"!$#&%'#($!*),+.-/021ff34658789;:=<>5&?@/?BAC6CEDFGAH0ICE/46-J5K-L1/02134E5K789L:*MN34O/P:@C'7Q/SRTD3:@/58-U 58-/
9;VWAX3ff789;CY?Y5&?Z?[AX/\EC^]P_`5 MaMN3ff4b/-JV@D3:@/58-dc>ePUf_`/-V@\Y9L35K\ECg3hMfi/iAj4T3;kY0ICl:mDff5&?Y7Q465QkYFL7Q583-onO_/-JD
/-JVp7Q/4Q1Cl7qAj4T3;kG0rCl:=?Y30Is'Cl4"teu]P_
vw <x7Q/SyCG?g/S?z58-SAjFL7^789;CO?BAC6\l5 {^\6/ff7Q5K3-|3TMz/D3:@/58-}c~e|Uf_:z/58:zFL:Aj4T3;kY0IC':>?Y5&YCb_"/ffC'4E43ff4AX/4T/:@C'7QCl4bE_^/-D/\63-G{"DCl-\EC^A/4T/:@C'7QC'4ff
;w <:@/V\6/080`a^ZRB.^"a|_"a9;58\Y94Cl7QF;4E-?CY/ff:Aq0rCG?oB6tqW`MN34@cu_fia9;Cl4TCp
5&?p\Y9L3S?NCl-`58789PAj4T3;kW/;kG5K0I587QVpnOq^MY43ff:789;COAj4T3;kG0IC':?YC'7.789;CP-JF;:pkWC'4@3TMo34T/\l0ICP\6/080?
3TMo<f:@F?Y7zkWC@A30IV-J3:@5K/ff05K-789;C:@/SS5K:@FL:Aj4T3;kG0IC':f?Y5&YCo_P _dff_z/ff-D789;C0IC'-L17893TM
5K78?g58-SAjFL785K78?@46FL--J5K-L17Q5K:@Cp:@Fff?N7kWCZAX30IVS-3ff:z58/0$58-/080a789;CZAj4TC'sl5K3ffF?AX/4T/:@C'7QCl4?@/-DP/ffFYAAC'4@k3FL-DO.3-789;Cp4EF;-J-58-;17Q5K:@CY?g3hMAj4T361ff4T/:O?g5K-c3-58-SAjFL78?g3TM.?Y5&YCb
/-D/080qAq43Lk/;kG5K0I587QV}D5?Y7Q465QkYFL7Q5K3ff-?znf3slC'4@ _.`5K789Aj4T3;kW/;kG5K0I587QV}/7.0ICE/S?N7
;w MN34/0K0aceU
6`PW_j<3ffF;7AjFL78?g/ZAj4T3E14T/:tJ789;/7/EAAj4T3SS5K:@/7QCG?"]58-}789;Ci?NCl-?YCb789L/7aSnO"E_
a9;C'4TC.a2t Tq,
tb/-Dpt
Z/-JD
w 789;C'4TC}5?P/@{SC6DA30IVS-3:@58/0?NFL\Y9789L/78_MY34/:@/SS5K:@FL:~Aj4T3;kY0ICl:?Y5NCp_z:@/SS5K:@FL:
?N30IFL7Q5K3ff-}0IC'-L1789g^_b _b _/-D789;C@FYAAC'4gkW3FL-D..3-|789;CAj4T3614T/:O?.5K-cP_`5 M<x3FL7AjFL78?bt _
789;C@46FL-7Q58:zC@3TMbtJ5?zkW3F;-JDCEDdkGVzOaEE w
YSWHYL&;SPTLKJ'LKESpY^q*S&@I}
Q6SpY^SfiaLYlIIz6l'LELr&pSfiZS6SESSSLYrI@6'ffL6a
&'SL&l&T&KgQET&S"Wp;Lff'[K@TEo@gpW'YZKiQYp
YY6TS&T&`@SLgpYqaBpTKELKS'L^'WY`olgJbZ;S@.T'S6W
ff6S}$^L&bW'L&YpYLS$^ESYqL&YTS&Y$WSqKMYFL-\'7Q583-J/0K0IV'L&YL
@.'ffY'WL&YTS&YELKT6&|Z&'SY';LW'[KTELTphff
Y'L'}hgS^&'SYbYdSW'L&oSZ.6YWL&YhS&YbSL&ESTI&dW&;TE&;
6Sff&@&PpgpZ6l&L&ESL'G`B^LKJ.&pL[&L&zWLLKI.ESY
L&YTS&Y^&6hEr"LL'^E;KT&}Z&'SYlLL'ZTE
Tgpi8&'SL&PI&Y6SWJ&'SL&}@TZohYE'[QL`LYJYLY;@S^
WSKESg6L&ffKThhffn@WL&YTS&Y}t ffoLW}&'SWYPWlK
l;&oSWP}6YpffLYTS&YlrgWYLEWS}TS&T&^&
LrI&Tu&'OS.Y`Y'T'WpL&Yo@SL'[&n@S&EP
6L&@L&Yo^SW"ESL@&oW;TY$ffYpTpYT&p' ol@JY'TYL6ShffX@
&'SY.ol r`@&'SWHSlL&oSE&}EW'EbffLYhS&Y'
YEq"OL&P'ffLW
STWL&Y hS&Y^K^&'SW'}S&'TZ^&}@ffSLIr&T^;
&lI&qYZKZ.W'L&YpYL.S.&'S'uL&YTS&Yzh@ dW&YL'
YSWgLKKL'Lp&[KTT&P^&fiTffLuP&|T&pgJS&;&}SST&S6SYY6Y
&W&ffL&Y=[ Y;hS&T&p&YW$S&;ffY6T'SjWYgWErKSLIIKTS6SpYWY6`
SL;SZY^`ffoT&pbY'L'QEX'ET&pWfiLl&oJY6Sff6YL
KT6SpYYBEff6bW.T&p.QE'ET&p&ff&ffL"JY6SEW=.ffLY
TS&ff&T&p^[&E}WLT&pKpTffpYL&W'L& l&ffT& YSPE'E'
&pSL'YTg.ffY6SW6^S.T'@gff BPY"ff6Y^.'ffL&.L&
SZ@LzJYgSffY6SWbE'ET&fiiS&L@Kl`&SST&.6SpYYEYYffYW
.T&piff"E'ET&}p[&S&OY6^oloJzSWLW6SpZffL'q
bTY'L}ffL&Y';^&'SY'WL&YhS&Y`K^&'T'E^@gLEBSL&^Y
Q6E}WL&YTS&Y'LKK}ffLllIKSL&W}&'SWYPEQ}W}&'SWL
YGS-3ff7WW'T'E.@L&YfTS&Y6&O&'SY'ZLL'OTE.I


ff
fi

fiffff!"#
$"%&'(*),+.- &/102)- 3,&(4065 78$"%&9%;:.'),$*%&ff0<5=002'>,?&)A@B$*%&- &ff>("7&(>(*&C02D'').02&ffEF$*)G+&H&IJ?*5 &7.$ffK0<5 7?&
L &>("&),7.- :F/F&ff>,0MD(25 7NO&IJ?*5 &7?:8+.:P?);>A(402&0"?>Q- &ff0R0MD?"%8>0S("D77.5 7NG5 7F'T)- :;7)/U5=>Q-V$25 /F&,WYXZ0 L &
0">Q5=E[&ff>(2-\5 &(ffK L &C>(*&7),$?),7?&("7&ffE L 5 $"%]/F),("&(*&^7&ffE]7),$25 ),70_)@`&IJ?*5 &7?:,K02D?*%J>0S5 /F'(")35a7N
$"%&O$25 /F&U?),/F'.- &b.5a$2:c)@d'("),+.- &/e02)- 35a7N[@f("),/hgUikjTlmd$")8gnikjoQm4K5 7p$"%.5=09'>'&(ffW
X9- $"%),DN,% L &c$"("&ff>$"&ffEq>r'("),+.- &/s02)- 3,&(8>0F0<5 /F'.- :t>uE.&$*&("/U5 7.5=02$25=?v'("),N(4>/w$"%>$F/]>'0
'("),+.- &/80x$")F02)- D$25 ),70K$2:.'.5=?>Q-\-a:]5 $C?)70<5=02$40)@S$ L )U?),/F')7&7;$40zy6>[E.&ff?*-{>A(4>$25 3,&O("&'(*&ff02&7;$>$25 ),7
)@C02),/[&8|5 7Eu)@C?),7.$"(")A-|.7) L -a&ffE.N&pik>J@}D7?$M5a)7mZ$*%>$P02'T&ff?*5\^&ff0 L %.5=?"%q)'&(4>$*),(G),(~),'&(>$"),(
02&ffD&7?&8$*)J>''.- :5 7>]NA5a3&702$4>$*&,KV>7E>75a7.$"&("'(*&$"&(G$"%>A$OD02&ff0H$"%&8?),7.$"(")A-6|.7) L - &ffE.N,&F$")
02)- 3,&U>7.:8'(")+.-a&/5 7p$25 /F&~')- :.7),/U5=>Q-S5 7c5 $40H0<5 &,W5 7?&U$"%&O5 7.$"&("'("&$*&(5=0D02D> -- :^b&ffEKT$"%&
%.:;'T),$"%&ff0<5=0O02'>,?&U)@'("),+.- &/02)- 3,&(0CE5 ("&ff?$2- :u?),("(*&ff02'),7E0H$*)J>]%.:;'T),$"%&ff0<5=0O02'>?&P)A@R');0"0k5a+.- &
?),7.$"(")-|;7) L - &ffE.N,&,WSXO0"02D/U5 7NF$*%>$R$"%&(*&5=0>7]&IJ?*5 &7;$$4>(*N,&$R'(")+.-a&/h02)A-a3&(R5 7c$"%&H%.:;'T),$"%
&ff0<5=0U02'>,?&])@'("),+.- &/02)- 3,&(40Z5 /F'.-\5a&ff0[$"%>$~$"%&("&U5=0U>$4>("N&$H@}D7?$25 ),75 7$*%&J?),(*("&ff02')7E5a7N
%.:;'T),$"%&ff0<5=0F02'>?&8)@C?),7.$"(")A-R|;7) L - &ffE.N,&,W,'&&ffE.D'-a&ff>A("7.5 7Nu)@C>c%.:;'T),$"%&ff0<5=0F0M'>,?&8)A@x$>("N,&$
'("),+.- &/0M)- 3,&(40F?>7q+T&p>,?*%.5a&3&ffEq+.:TXOR- &ff>(*7.5a7N)@Z$*%&J?),(*("&ff02')7E5a7N%.:;'T),$"%&ff0<5=0]02'>,?&
)@?),7.$"(")A-6|.7) L - &ffE.N,&,WG9) L &3,&(QK L &PE.)c%>Q3,&F>7v>,EE5 $25 ),7>Q-'("),+.- &/e)@?),7.3,&("$25 7Nc'(")+.-a&/[
02)- D$25 ),7'>Q5 (40U)@H$"%&c$4>("N&$P'(*),+.- &/02)- 3,&(F$*)u&b>A/F'.- &ff0U)@O$"%&c$4>("N&$P?),7.$"(")-|.7) L - &ffE.N,&,W
&~$4>|,&O>,E.3 >A7;$4>AN,&O)@d$"%&PE.),/]>Q5 702'T&ff?*5^V?>A$25 ),7ikE.&^7.5 $25 ),7v)@6N);>Q-=0>7E),'&(4>A$"),(40mS5 7E.)A5a7N
$"%.5=0H?),7;3&(40<5 ),7`W&7?&F02'T&&ffE.D'p- &ff>(*7.5a7N[5 7v),D(@f(4>/F& L ),("|]?),70<5=02$409)@d$ L )F02$"&'0zy5 (402$QK$"%&
'("),+.- &/F0M)- D$25 ),7p'>Q5 (40x)@$"%&O$4>A("N,&$R'(*),+.- &/102)- 3,&(C0M%),D.-=E]+&G?),7.3,&("$"&ffE]$*)P&b>/F'.- &ff0)A@$"%&
$4>(*N,&$G?),7.$"(")-|;7) L - &ffE.N,&]D0<5 7Nv$"%&JE.)/8>Q5 70M'&ff?*5\^V?>$25 ),7`W,&ff?),7EKd$"%&F&b>/F'.- &ff0U)A@x$>("N,&$
?),7.$"(")-|;7) L - &ffE.N,&H/UD02$+&HN,&7&(4>Q-\5 &ffEcD0k5a7N]02),/F&9@}D7?$M5a)7]-a&ff>A("7.5 7N80"?*%&/F&,K>7EF$*%&O("&ff02D.- $
/UD02$+T&O'.-aDNN,&ffE5a7.$")F$*%&H5a7.$"&("'(*&$"&(C$*)n?(*&ff>$"&G>7p>''(*)ffb.5 /8>$"&O'(*),+.- &/02)A-a3&(ffW
),(0k5a/['.-5=?*5 $2:p)@S&b'T);0<5 $25 ),7`KT$"%.5=0_@}(>/F& L ),("|F>,0"0MD/F&ff0_$"%>$$"%&O/8>Qb.5 /UD/'(*),+.- &/10<5 &Pj
5=0N5 3,&7`W),(>UNA5a3&7c'("),+.- &/E5=02$*(25 +D$25 ),7`K$*%.5{09?>7p>Q-=02)F+T&G&ff>,0<5\- :c&ff02$25 /8>A$"&ffE[@}(*),/&b>/['.-a&ff0
+.:c$"%&P02$>7E>(4E'(")?&ffE.D(*&P)A@Y02$>("$25 7N L 5 $"%0<5 &cU>7E]5 $"&(4>A$25 3,&- :E.),D+.-\5 7Nc5 $O>7Ec3,&(M5@f:5a7N
5 $ L 5 $"%v>F02D.IJ?*5 &7.$2- :]-{>A("N,&P0M&$)@(4>A7E.),/U- :cN,&7&(4>$*&ffE]'("),+.- &/80ikC>A$4>(4>AM>7`KQ,,.m4W
]` ff{`V2;zQQffdv{ ?)70<5=02$"&7.$C nUf2,;H,9V\H ,ffffQ4~ fff
fk j ,Gp;[fk=T=.U,9V\Z fQ fk< j
5 /U5\-=>($*)P/]>7;:]XR- &ff>("7.5 7N8>Q- N,),(M5a$*%/80K.$"%&G02'T&&ffE.D']-a&ff>A("7.5 7NJ>Q- N,)(25 $"%/80 L &Z?),70k5{E.&(
)L ,("|v+.:&Ic?*5a&7.$2- :^-a$*&(25 7N$*%&8%.:;'T),$"%&ff0<5=0U02'>,?&[@f),(P>'("),+.- &/02)- 3,&( L %.5{?*%5=0U?),70<5=02$"&7.$
L 5 $"%F$*%&$"(4>Q5 7.5 7NF0">/F'.- &,WR_&@f),("& L &'(")3,&$*%&),("&/80d>+T),D$R'>A("$25=?D.-=>(%;:.'),$*%&ff02&ff0R02'>?&ff0K L &
^(402$G0M$4>$"&U>]N,&7&(4>Q-R$"%&)("&/ L %.5=?"%r5{0~>cE5 ("&ff?$P?),70M&ff,D&7?&])@$*%&P(*&ff02D.- $40H5 7TXOR- &ff>(*7.5a7N
)@S^7.5 $"&G%.:;'T),$"%&ff0<5=0H02'>,?&ff0Hi2d- D/F&(QKVY%("&7.@}&D?*%.$ffKH>D0"0<- &(ffK >("/UD$"%`Kff,.m4W
U .VQ`Q "UfffAk,,U,F,Tp"u;*,;{8<,pMU,\,U,kfU
V2;zQQff*,U,P=8=]`ffR. *H,\P,\p",.4= j q;ffu
,,4; G[4,zr\,;F ,=;, ]c6 G
~f zG;~4Q 4,f,2Chcw j dR,FG=AV.4
xf\U,=O;O,Q,G,]Q2,f,*P8
~4,\4QP . m,- 7C9u- 7 f2,;Fz ,9V\z4
~fffPfO=f=U,\,U,= j C C R,TF=;~{C2O;U,,U,4Q 4f=,
,];Uf2,=T=.] z4R
~,;;UOV2;z\ff,\QffH=] ;p{G, fffT p;Ff2,;n


fi ff
fi
fiff
ff
!
" #$"%

&('*),+-/.1032546472)98;:<)<=?>@=BA' =C.@+EDF8.@+ =;2>@=;GH.I2J.K)<2!L-5>@.KDFMBNE4F-5>O+EPQLR2.@+ =8(DF8O8KL-MB=8TS8K=;.U8V250
BM 2'E.@>@254W>@NE47=8X-5'YCGZ-MB>I2[32L=;>B-5.@2>U8T\,]=^D$YE=;'E.KD603P_8`NEabMID7=;'Q.VMB2'YD7.KD72'89.@2cN-5>U-5'E.@=;=V8KL=;=YEN L
47=-5>@'ED7' cdDe'C=-MI+?250f.I+ =8K=X.K)<2V+QPEL2.I+ =8(DF8g8KL-MB=8;\
hji1k9lmonprqsptvu wpyx/nwzgn{zslR|
} ' =~),-P.I2 NED64FYO=BaOMIDe=;'E.,L >@2E4e=;G8K2547=;>U8DF8<EPV47=-5>@'ED7' cZMB2'E.@>@254o>@NE47=8,-8D7'!,D7.UMI+ =B464:
.@c25: <-5' =;>3(Ds:Ej2><D7'9XV;VD7'E.@2':oU\<2'E.@>@2/4>@NE47=8>@=YENMB=18K=-5>UMI+OQP
8K=B47=MB.KD7' c: >I=K=MB.KD7' cV2>r2>UYE=;>KD7' cV2L=;>B-5.@2>U8r-5L L >@2L >`D$-/.@=B47P\j&s'.@+EDF8,8K=MB.KD72')<=gMB2'8(DFYE=;><47=-5>I'EDe' c
250MB2'E.@>@254>@NE47=8.@+-5.,8K=B47=MB.-/L L >@2L >KDF-5.@=~2L=;>U-5.I2>U8<.@21-5L LE47PD7'-c5D7=;'8K.B-5.@=\
f35oQoRg5y$`77E


MB2'E.@>@254W>@NE47=~DF8~-VL-D7>1@^UUU:),+ =;>@=O~YE=8IMB>KD7=89.@+ =V8K=;.g2/0L >@2E47=;G8`.U-5.@=8<2'?),+EDFMI+
.@+EDF8>@NE47=8K=B47=MB.U8.@+ =~2LR=;>U-5.@2>,E\,^jDF8,M;-4647=Y?.I+ =VB(2/0E\
]=?-8@8KN G1=O.I+-5.V.@+ =?8K=B47=MB.@[s8K=;.U8d250~2L=;>B-5.@2>U8250XYE2GZ-De'8De'v
-5>@=OYE=8@MB>KD7=YD7'8`2G1=
4F-5' cN-5c=bd\Z]=ZMB2'8(DFYE=;>Z-+QPEL2.I+ =8(DF88`L-MB=Z2/0L >I2E47=;G8K2547=;>U8T:),+ =;>@==;=;>@P!L >@2E4e=;G
8K2547=;>~MB2'8(DF8K.B8,250-8K=;.,2508K=B47=MB.@[s8`=;.U8<D7'1:ff2' =~032>9=-M@+C2L=;>B-5.@2><D7'C.@+ =YE2G-D7'\,o=;.X<1R=
.@+ =X8K=B47=MB.@[s8`=;.U8,>I=8K.@>KDFMB.@=Y.@2VL >@2E47=;GZ82508De;=*W\
+ =1+QPEL2.@+ =8(DF8V8KL-MB=1N8K=8-A =Y.I2.U-4j2>UYE=;>KD7' c!25=;>~.@+ =12L=;>U-5.I2>U89250,.@+ =ZYE2GZ-D7'\
<+EDF8<2>BYE=;>KD7' cXDF8<N8`=Y.@2X>@=8`2547=XMB2'E DFMB.U8<R=;.)<=;=;'O-5L LE46DFM;-5E47=2L=;>U-5.I2>U8j)+ =;'OG12>@=9.@+-5'2' =
8K=B47=MB.@[s8K=;.<MB2'E.U-D7'8j.@+ =c5D7=;'1L >@2E47=;G?\W&('1),+-5.W032546472)98;:)<D7.@+ 2N .j472Q8@8W250oc=;' =;>U-46D7.KP:E)=,-8I8KN G1=
.@+-5.<.@+ =~2L=;>U-5.I2>U8<-5>@=~'EN GV=;>@=YN8(D7' cZ.@+EDF8<2>UYE=;>KD7' c\<De=;'-L >@2E47=;G-5'YO-18K=;.,2/0MB2'E.@>@254
>@NE47=8;:-L >@2E47=;GH8K2547=;>D7'JLEDFM@ 8.@+ =147=-8`.X'EN GR=;>@=Y2LR=;>U-5.@2>~)+ 2Q8K=O8K=B47=MB.@[s8K=;.1MB2'E.U-D7'8
.@+ =1L >I2E47=;G?:j-5'YJ-/L LE4D7=8^D7.\O<+EDF8^D$8>@=;LR=-5.@=Y!N 'E.KD64.I+ =ZL >@2E4e=;GDF88`2547=Y2>~' 28K=B47=MB.@[s8K=;.
MB2'E.U-D7'8.@+ =MBN >@>I=;'Q.,L >I2E47=;G?: D7'!),+EDFMI+M;-8K=:R.@+ =~L >@2E4e=;G8`2547=;><0s-D64$8K8K=;=XyD7cN >@=Z/U\&0.@+ =
G1=;GV=;>B8K+ED7L?D7'C.@+ =X8K=B47=MB.@[s8K=;.U89M;-5'OR=MI+ =M@=YbD7'?L2/4ePE' 2GVDF-4j.KD7G1=:R.@+ =;'O.@+EDF8L >@2E47=;G8`2547=;>
>@N '8rD7'C.KD7G1=XLR2547PQ' 2GD$-4fD7'?5-5>KD72N8L-5>U-5Gd=;.@=;>U8;\
25)X:E)<=X-5>@=9>@=-YEP.@2Z8K.U-/.@=-/'YOL >@25=9.@+ =~GZ-D7'C.@+ =;2>@=;G2/0.@+EDF8,8`=MB.KD72'\+ =X8K.U-5.@=;Gd=;'Q.
-5'YCL >@2E2502/0.@+EDF89.@+ =;2>@=;GM;-5'!R=YE=;>`De=YC0>I2GL >I=;D72N8^>@=8KNE47.U892'C47=-5>@'ED7' c8`=;.U8,)De.I+2' =;[
8(DFYE=YO=;>I>@2>~9-5.U-/>U-5@-5':EU\y]=~L >@25=9D7.<03>@2G.I+ =9A>U8K.,L >KD7'MID7LE47=89032>,MB2G1LE47=;.@=;' =8@8T\
=;.?YE=;' 2.@=J-8K=;.O25018K=;'E.@=;'MB=8;:9=-MI+2/0),+EDFMI+>@=;L >@=8K=;'E.U8O-8K=;.O2/0L >@2E47=;GZ8bD7'.@+ =
YE2GZ-De'\<+ =;>@=VDF8-'-5.@N >U-4L-/>@.KDF-42>UYE=;>`De' cC25=;>~.@+ =V=B47=;G1=;'Q.B8X2/0YE=BA' =YQP!.@+ =!BG12>@=
8KLR=MIDAMd.@+-5'>I=B4$-/.KD72'\ 8K=;'E.@=;'MB=VDF8dd(B 1Q?-5' 2.@+ =;>9D60,.@+ =18K=;.~>@=;L >I=8K=;'Q.I=Y!QP
.@+ =^A>U8K.g8K=;'E.@=;'MB=^D$89-18KN 8`=;.,250j.@+-5.,>@=;L >I=8K=;'Q.I=YOEPZ.@+ =V8K=MB2'YC8K=;'Q.I=;'MB=\]=XYE=BA' =1-OV5;
(B ZRK67;3K X250X-J8`=;.?250~L >@2E47=;GZ8dD7'.@2JR=-8K=;'E.@=;'MB=CDe'),+EDFMI+
>@=;L >@=8`=;'Q.U8.@+ =~G12Q8K.98KL=MID6AMV8KN L=;>B8K=;.g2/0\

fiff


!
ff ("
+
$
. 0/ /





fiff )
#
(










ff ff

' "
*$ &%
*,

- "

V3/ V
5BTU;;VCs U 6UURFE? UQFQ IQ;;7

E
(B, <K T; @ I5 1C;6Bs; ;
V
!
Q51VK T6 XO Q<; TX ; ;6K ;6
5Vo3FV
9 X;W rK ;6X~ jQKX$g E EVV5W(B <$;3FZZ<
FrU IXB E3 Od 5V3V
@@BQ _!QXF<B ITQB FC3FV9 RVFO
4e2c o7X 5VZ

1324

fi57698:<;*69==>@?BAC6EDE69F969GH69I
Jhji KJRLNQ)MEO9kPRlNQSm KTOVU)W9XTYHZ[W'\ ]fiZ_^<\ `*]fiaEZ[W9b<\ `Ecd]fefiW9\ gT`EZ
n Oo hji
xwprn qsuOh[zo Othjivw{}|m ~Ef'l7}PNL
#[H9wfi9<}fi#[fi*9}l3@dH~Tm _T9Hm
lVpqqss~lSTm79p7~Tfm
P#

LNO iQ7PkJRU)Q)W'kXp}YHZ&m W9\ ]fiZ^\ `*]fiaEZ[W'b\ `Ecd]fefiW9\ gT`EZ
9H$q}9H*fiE*Efi<
HENH
KTL7L9<*w[(H99HEfEE qd*9H*TVN9E9
HT9d<$fi9fiC@fi"@RH)9H<9*H*9H*H*TdE9EE9fiHdHTH
Efi[EfiH97dH@H[E$$*E9dfi9dH*EHH_*fi*H<E9}}H9w[HV"EH*99E*H<E[HTdEER9
7@f}<H99
T99*9fi[TU7W9$X_YZ[W9fiH\ ]fi*Z_^H\ `*]fiEaE$Z[W9b9\ `Ecd]fejW'\ g`E@Z79EH9 *&9H$0fi*THN@@HE$wHE9990HTfi
H_H*9RH*[EH9*
fH99HfiTE09 9HET[@HH9*T[*)*T[Cw9fi*9
EHS*9wT(fiH9[E9<<CE<7*9fiH*$*T$EdH9 H9*<H*rH9<<<EH9
H9*E$*
Efi9
H[TH@@fiE$T@99uEHE9}39<fiH9H9$
U7fiW9@XfiYHZ&W9\ ]fiZ)^\ `*]fi\*`9E*ZXT`EZfi w<9EH$fi*#fiE
HE[Tfi<fiHH99*9*$T9N`EX`E9Zf\ *`*9fi<<*fiH H@E99HTEfif@H9dfiH_fi*C$
E99<"*9EHfi['fiE[E$9fid9*fiEU7<W99X*YfZ[W9\ ]fiZ_^\ `*]fi\ `9 HZ_<X`EZ*@VHV@9*TEHf$ETE~T-9
*H$fi<3[EENH'0fi9V9H99HH*H<9fi
*
EN0`EX`E9Zf99\ H*`d*VH9ffifi*[H|#~Tf[99[HVfi9H99*EEH99HTE
9H0H9_fi*fid9~T<*(9**T
9w[ *wE9




9

E



fi

9

E



H



E





9



9

H

9







9

H



)
U
9
W

X
H

[
Z
'
W
\
fi
]
_
Z
<
^
\
*
`
fi
]
\

`


_
Z

X
E
`

Z



TU7W9XYHZ&W99\ ]fiZ^\ f`*]fi}\ `EZX`EZ9T99**H9H<*TfiE *<#[H9H*[fifiHE**}$wVH 9HTE@9fi9HEEr$HE


9



9

H








[


















fi











H

9





*

9







E

}

H

9



H







9

*







fi

E



9

fi









E















9



9

H

<





*



*


f_C[EH$fiH[TT9HH99H<f*EEfiE9Hfi9##H*H*$9H<[E'dfi099fi99H$*HE@dH9$E"EEHfiEfiEH*$*H[HHHE9
3

fi
fiff

fi

" ! #%$&(')$+*-,.)/102,3 45076fi3 89453 8":;07."8%0
<>= ('-?A@9BDC)BDEGFIHKJLBDMANOBDP BDQ
R $TS <>=
UV9W MXFZY[)\K]_^ Fa`fiB"b"b)b1BDcedTf
<>= < ? <gh2<i $kjH5[lmNDB)b"b"bnB9jH5[omN W1p Ydqf
rfisut $#%? < ! =avwyx V9zV9{D|%W1V}VO~|%TV"pfi{VOV"OWn75V9WD sr
)$ $ g ? l KH c pfi ]] zyz l N? < $
R $TS >< =
# gh2h U% y7 U W1ppfiWD|)z KH BO NDf
> kFa

?%$ =
UV9W Fm["")BD[)9%B"b)b"b5BD[)"O
jHK[)"ONFXjHK[)"mN Yd
jHK[)99NFjHK[)9)N Y[)"9H5N1d
b"b"b
jHK[ " NFjHK[ " N Y[ HDb)b"b5HK[ " HKNnN-b"b)b N1dqf
$ = &
rfisut $#%? < ! =I-wy pz5W1{1OW|%{np"~T|%W5pzp%(VOV"OWn75V9WD sr
!-^Fa`?%)! 'S c&!
HK[ \ NF 8%."8%0:fi3 98%H1jHK[ \ N1NDf
!'-?"('? W1V}{1pfiTV95p%V9{p%({nV `} W1WnVyV"|;{1zV" HK[fiND
$ = &*-,."/10,3 45063 89453 8":%07.)8%0
{1VA z|)p{5W1Gp{yOpzqWn{1p%-{nTVV"|%{1zTz
p eV9{D|%W1pfi{ |%T29|%W5pzq+W1VWnV"|1V9{) pz2TV9{| H 5 N {1pTV9 Tn+2}5p%V"+q+W1V
5V"fiV9zOV F9[)""BO[)9"B"bbbBD[)"5 TW1VWnV"|1V9{)VLV"|;{1zTz|)p{5W1znTV" zW1V
5V9W jHK[ " N V9zW1VV"|;{1zTz|fip{5W1V9zV9{D|9V"W1T25V9W"WnVAV"|%{1zV"5VOV"OW17V9Wp% [ "
znTV" zOV|{1pTV95p;fiV9{DzW1VqTpWnV"2}5|OV}znfizW1VWD|;{1V9W{1pfiTV9
5p%V9{|) |)A5VW1VV"|5WzTAV9{1V"pV9{D|;W1p{ pT5V5VOV"OW175V9WOpzTWD|)zAW1V+{1pTV9W
p%p W1|;WW1VWD|%{1fiV9W5VOV"OW175V9WDAp;pfiV9{D|%Wnp{D [%l)B"b)b"b1BO[""nl TpzpWAOpzqWO|)z +p{1V9p%V9{)
zOVW1VV"|;{1zV9{yz}W1VApT5W5V"n-LV9zV9{D|)"|%W5pzp%W1V5V9W}p;VO~|%TV"9 WnVV"|%{nzV"
5VOV"OW175V9WOp%}peV9{D|%W1p{O [%l"B)b"b"bnBD[)"nl A5WLV5V9WDLp% WnVOp{1{1V"5epzfizWD|%{1fiV9WVOV"OWn
5V9WDmq|%zAV9zOV}TpzpfiWOpzTWD|)z V9zOV [)" W1VV"|fi5WzTAV9{1V"LpV9{O|%W1p{ pT5V5VOV"OW175V9W
OpzTWD|)z |;z eVVOV"OWnV"kqW1VAV"|%{nzV"{1pfiTV95p;fiV9{uWnp5p%V zOV [ " HKN
5p%V" Wn|uV"V9zOVp% V9zW1V"nW1|%z qWnV}W1V"|nV9{"TqLzTOW5VTqepW1V"29W
V5p%V" W1W1V1|%V5V"fiV9zOVqW1VAV"|%{nzV"{1pfiTV95p;fiV9{"V9zOV V5p%V"
z qW1VyV"|%{1zV"{1pTV95p%V9{"
"

fifi9"" 1


fiff ff
!!" !#$ %
& !'(!!" !#)' %
*+ !,-%#!".%#! !#$ 0/ 1$!! !23
4 657895;:!<>=#?#@$'A
B
fiCDFEG+HIKJMLC!
N JMLC+CODPEfi
Q DPRST>U&VXWZYI[A\]FUfiLA^_DPEY>U`RTI[;YaDFLbE7Lc+U`TI[AY>LTIC
UeEL;fZC#gL;fhY>g+[;YY>gUCi[;jkc]PUCODPl`UGDPEKLSTm[n]PRLTaDFYigjoDpCmC#SqrJiDPU`EYm^sLTt]PUu[;T>EDPERr[AEr[;ccTiLuvw
DPjx[;Y>U$cT>Lb\]FU`jyC#L;]PzU`Tu Q LTGcT>L\]PU`jxC{L;^|CODPl`Ux@}LT~]PUuC>C`Uu[bJ>g(C#U`Yk{bJ`[;E \UxJ>gLC#U`EDFEh 5
fm[nCDFEUuJMY#DPLE L;^GY>gUr[3]FRbLT#DPY>gjK;DFE+JMUY>gU`T>UK[;TiUrff}LcU`TI[;Y>LbTIC`0Y>gU7ESj\U`TkL;^bDpC#YaDFE+JMY
C#UM]PUuJMY>wC#U`Y~Y>Sc]PUuC`+[;E+"gU`E+JMUfiY>gUGESj$\+U`T~L;^_bDCaY#DPE+JMYecT>Lb\]FU`jC#LA]FzbU`TICtY>g+[;YmJ`[;E\+UJMLE+C#YiT>S+JMY>Uu
DPEUuJMYaDFLbErDC7 5 7mU`E+JMU7 5 ff|]FLbR 5 x~U`E+JMUx\(gU`LT>U`jkY>gUC>[;jkc]PU7CODPl`U
R;DPzU`EDFEY>gU[n]PRLTaDFYigjoDpCeC#Sq7JiDFU`EY~^LTt]PUu[;T>EDPER+
;DPE+JMUG]PLR 5 ADpCc+LA]FELj$Dp[n]!DPE@0bD^_jkU`j$\+U`TMC#gDPc7DPE7Y>gUfiC#U`YICXDPEK 5 [;E+"Y>gUfijkLC#YC#c+UuJiD%J
RU`EU`TI[3]9DPlu[;YaDFLbE+CL;^YigUKC#U`YIC"L;^UMv+[;jkc]PUuCJ`[;E\LY>g.\UJMLjkcSYiUu}DPEhc+L;]PELj$Dp[n]fiY#DPjkUY>gU`E
!u> `Inu;n; T>SE+CtDPE7c+L;]PELj$Dp[n]_YaDFj"U[C-fUM]]|~U`E+JMU\xgU`LT>U`jbDPYDpC[$C#c+U`UuSc
]PUu[;T>EDPER7[n]PRLT#DPY>gj^LbT| DPEK
LY>U~Y>g+[;Y
YigUe[;\+L;zU~Y>gU`LT>U`jJ`[;Ex[3]CaL\UCaYI[;Y>Uu$S+CDFER$Y>gUGLEw]DPEUfij$DpC#YI[;bU`ws\+LSE+"jkLUM]
DPE.fmgDpJ>g.YigUk]PUu[;T>EU`T)DPE+JMT>U`jkU`EYI[n]]P.Sc%[AY>UuCk[7gc+LY>gUuCDCfmgU`EU`zU`T$DPYkJ`[;EELYkC#L;]PzU7[EU`f
Y>TI[3DFEDPERcT>L\]PU`jDPE(Y>gUkC>[AjkU$f[nK[CGYigUkY>Uu[JigU`TLUuCDUPfmgU`EU`zbU`TfiY>gU$]PUu[;T>EU`T)jx[;UuCG[
j$DpC#YM[;UuDPY>Y#]PUuC#Y>LbEU
ubIgDpCGbDPUM]C$[7CO]DPRgY#]P jkLT>U$RU`EU`TI[3]
T>UuC#S]PYY>g+[AEgU`LT>U`j
\+UuJ`[AS+C#U+SE+U`TGY>gU$C>[;jkUJMLE+bDPY#DPLE+CGLA^Y>gDpC~Y>gU`LT>U`jYigUESj\U`TeLA^j$DpC#YM[;UuCL;^-Y>gUG]PUu[;T>EU`T
DPE.Y>gUf|LbTIC#Y>wJ`[CaU$DpCcL;]PELbj$Dp[n]]F\+LSE+Uu^sLT$[;E}[;T>\DPY>TI[;TiJigL;DpJMU7L;^eYiTI[nDPEDPER(UMv6[Ajkc]PUuC`
DUP-ELbYEUuJMUuCiC>[;T#D]PRbU`EU`TI[;Y>Uu}S+CODPER[vUu(cTiL\+[;\D]DFY#bDpC#Y>TaDF\SYaDFLbEgUxj$DpC#YM[;U`ws\+LbSE+
[n]PRLTaDFYigjxC$J`[;E.\+UKJMLEzU`T>YiUu}Y>L \+[;YIJigWfi
w]PUu[;TiEDFER.[n]PRLTaDFYigjxC{DFE[C#Y>TI[nDPRgY#^sLT>fm[;TIfm[n
DPY>Ya]FUuCaY>LEUubI
_70_;uP`.`X0%p7+`%nu
UkEL;fJMLE+CODpU`Tk[;E[;cc]DpJ`[;Y#DPLE}L;^egU`LT>U`jyY>L7Y>gUxLbjx[nDPELA^C#j$\+L;]DpJ$DPEY>U`RTI[AY#DPLE
[ C{f[bCLEU"DPEY>gU
mcT>LRbTI[;jyDPYIJigUM]9]mU`Y$[n]Pu&M UfD]]~CagL3fgL;fY>gDpC$J`[;E(\U
UMqrJiDPU`EY#]P}DFj"c]FU`j"U`EY>UuS+CODPER[ C#Y>TI[3DFRbgY#^sLT>fm[;TM[Acc]9DpJ`[;YaDFLbEL;^gU`LT>U`j^LTk[ C#S\+C#U`Y$L;^

mCLjx[nDPE
tLE+CODpU`TeYigUJi][bC>CmLA^XC#j$\+L;]DpJ{DPEY>U`RbTI[n]pCY>g+[;YGJ`[;EK\UC#L;]PzUu\Y>gU$C#YI[;E+[;TM"DFEY>U`RTM[;Y#DPLE
LcU`TI[;Y>LTMC`
!U`YG \+U)Y>gUC#U`Y~L;^XLbjx[nDPE+Cmf~gLC#ULbc+U`TI[;YiLTIC[;T>U$UuC>JMT#DP\+Uu\7T>S]PUuCeC#S+Jig[bCtDPE
Q DPRST>U$&%[AE+7fmgLCaUcT>L\]PU`jxCfiJ`[AEK\+U$UuC>JMT#DP\+Uu\K[;ESE+[;j$\DPRSLS+CfiJMLEY>UMvY^TiU`URTI[Ajkjx[;T
C#S+J>gK[bCmC#gL;fmEDPE Q DPRST>Ufi*
U`Y.\+Uk[;ECaU`EY>U`EY#Dp[n]-^sLT>jK6DUP[C#Y>T#DPER7L;^Y>U`T>j$DPE+[n]pC)[;E+z3[;T#Dp[;\]PUuC`L;^
Y>gU$RTI[Ajkjx[;T
L;^ Q DFRbST>Ufi*xU`T#DPz3[A\]FU)^sT>LjY>gUC#YI[;TiYCaj$\+LA]fi;;;0WCaU`EY>U`EY#Dp[n]!^sLT>jU`ELbY>UuCY>gUC#U`YL;^
cT>L\]PU`jxC)U`T#DPz3[;\]PU^sT>LjS+CODPERKYigUkcT>LS+JMY#DPLE+CfiL;^ 7tLE+CODpU`T[gcLY>gUuCODpC$C#c+[JMU"L;^
3`

fi6

b6(36(b+ ++

ff
fi ff
fi


fi !" #$%!'&( ff
fi) # !+*" ,
-fi
ff
#$%!.0/214365$798:;/<14365$798>=$# !"?/21@365 7A8>B #$%!
/21@365 7A8>DCE%FHGJIKLffNM9*O# !QPK #RTS) U ; VMW ff
fiP
U#$EM4EEX#$!QP
#RTSYZM4[]\_^#ff-PKNM4`Aa[,-P
CFHGbI<c;FIKLHLd
ffeD

c;FfI<DgfihLjfLk lmLn;Lofpqf r
st\_u v;wyxElfz|{}u wA~Y~wOyau xb^;xbw9~yx;x\i^yxbu wA~@\_a ^;wya _xb[
;wya _xb[@a_ xbwA[;a[x#[@xA_x `AyW[@xb9[O~wxwyxb;wyx [@xb^yx ffy;x[@xb^xb^@\~$a-wy[|ay;xu-w9~Y~w2
"xff~^"ya[@;ayf~y;xm;$y? @W- b@ W @f\i(~^~;;wya ;w@\~yx 6- bffwya-v;@\_^;xYyf~
`Aa Y;v;x [Oy;x,;a2~Y[@xba;wa _xb[b\[~E_x ~wy^\_^;u~%iu-a w@\_y;>6a w \_^Q
xfw9[@Y^;xbx ~Qxb:xAf^\i\ia-^f[b{; 7K5367@595\[~^a-w9xbwyx ywyxbxQ;xbwyx~$#^;a;x [~wyx
~fxA_x ;x,~w@\~_x [a-w#yxbwff\_^f~$[a2y;xu-w9~Y~w f~^fQy;xffwyaa O\[~fxA_x Qy;x,[9~wy
[@fffaW(Qa wxba%-xbw H\~^;a;xf~-[,`\wyxb^H $ @9H;xb^ H $ $y9Hffvf[@x~
;wya;vf`A@\_a ^ay;xu w9~Y~w YO;x[yw@\_^;ua[@,a[Ea ;9~$\_^;x wyx ~ -\_^;uy;xix ~$ x [eay;x
f~w9[x#ywxbxOwa _xA6aw@\_u <\[O`b~$_x y;xff5$Nay;xywyxbxeM4a-m`Awya6N_~^h rp%rP9)y;x
u w9~Y~w\[v;^f~ff\iu-v;a vf[b;y;xb^6a wOxb xbwy[@xb^yxb^f`Ax\``b~^fxEu xb^;xbw9~yx y;xu w9~Y~w
y;xbwyx\[~Ev;^\ v;xEf~w9[@xywyxbx\`yQ \_xA;[yf~[xb^yxb^f`Ax <O\[Oywyxbx\[`b~$_x y;xf~wA[@x#a)yf~
[@xb^yxb^f`Ax
{>AAa~ffwyxbx\[~^a wAxbwyx [@v;;wyxbx[@vf`yf~eM4~P~$);x^;a;x [#~^fx u x [Oa
~wyx\i^E;MWHP|y;xwyaa 2a\[<\_^ ;~^fM4`$Pt\N~^;a;x\[\_^ y;xb^\_9[<f~wyxb^~^fE\_9[O[]\_\_^;u[
\_^}~wyx~$[@aff\_^
]^v\i\i-xAi ~#`b~\[ta ;9~$\_^;x E;wyv;^\_^;uy;xO[@v;;ywyxbx [|wyaa yx ev;^fxbw2[@a YxO[@xA_x `Ayx e\_^yxbw^f~$
^;a;x [\_^y;xf~w9[@xYwyxbx~^f~ \_^;uy;a[@x^;ax [e\_9[e_x ~ -x [bQ\i^f`Ax;xu wA~Y~w\[Ev;^;
~ff\_u v;a vf[b~$;xu xb^;xbw9~$\_ ~@\_a ^f[eM6\_^CffPa2~^xA?~ixff`Aa wywyx [fa ^fyaffy;x-\_xA;[a|%~w@\_a vf[
`b~f[<a);x#f~wA[@x#af~OxA?~ix 6)y;xbwx~wyx@Oa,`b~f[O ~^fb<a-w~ef~w9[@xywyxbx[@vf`yf~
b\[#~%[a~`b~a f;xb^ [O-\_xA\[Ya-wyx[@fx `\m`fff~^b [b\_^Qyf~y;x[@xba[xb^yxb^f`Ax [
xbw@\_%~ix6wya :y;xY-\_xA"aOy;xEa-wyYxbw\[~[@v;f[xbaOy;xY`Aa wywyx [@a ^f-\_^;u"[@xbxbw@\_%~_x6wya
y;xY-\_xA"aOy;xff~yxbw ff"xY[y~$yf~ \[eYa wyx[fx `\m`Yf~^(be\_^"y\[`b~ [x \[E[@yw@\`A@_
Ya wx[@fx `\m`Eyf~^b\ \[Ya wyx[@x `\m`f~^b~^f E b
\_ xb^"@Oaa wEYa wyxfff~wA[@xYywyxbx [6a wEy;x[y~x,u-w9~Y~w Hy;x8ffb3]5A 9AM4;P<\[
xAf^;x ~ [~[@v;;ywyxbxff\`"\[E~`b~a~$2y;xf~w9[@xffywyxbx [[vf`y"f~^;aa y;xbw`Aa Ya ^"`b~
6a wYy;x [@xQywyxbx [E\[[@yw@\`A@_Ya wyx[@x `\m`\_^f`Axy;x`b~f[ay;xf~w9[xywyxbxa,~^xA?~Y_x
`Aa wywx [@fa ^fya~$2a[y[]\__xu xb^;xbw9~$\_ ~@\_a ^f[Eayf~xA?~ix\_^"a v;wEfa ;x []\[,[f~ `AxCYty;x
-\ixAa|y;x;ay;x#f~w9[@xywyxbx [a~[@xbOa;wya-ixb[`Aa-wywyx [@a ^f;[OyaEy;x#;ayf~O[@xb
axA?~Y_x [b
x^;ax [y`Aw@\_fxy;x 6- b~$_u w@\_y;\``Aa-Y;v;yx [;x#;a~[xb2aHxA?~ix [
`Aa Y;v;\i^;uQy;xY;a2y;xA\_wf~wA[@xffywyxbx [bffO;xY~$_u w@\_y;:\[eya~w9`a^y;x [x,f~w9[@x
%

fiH;f-b; O+;;;y;
]_#











]_


ff
fi f (]_












fi ;
]_#

!#"

&%


fi ;

$

"

4i


fi
' )(+*-,.0/-132 ,4..,.5-,. 6.b87:9;7@ fi f =fi < 7>-. fi=? *-7@ fi f fi=< 7>-.ff7A@ fi . 9=BC5 ? .

7,.. <])B0* ? 7 9-. fi *f ?)D0< , fi BE7>-.F, fiGfi 7IH$_
4? * -_-(J9e fi G.ff9 E)79<])K ? _-(iL7>-.ffMON-P < 9 fi ?)D
< 7>-. 9=,.9 ?Q? 5-,. 6.b87O_R9 ?Q? 7>-.ff5 9;,9A.ff7,.. iS. T9
7 ?)D 7>-.9=BC.ff5 fi ])7@ fi f 9 U7>-.A),F5 9=,.bG7
9 ? ,4.I9+ _
4? * G.I:V
P fi f]WG.,IH <Xfi ,Y. T9=BZ5 ? .+H7> 9=7Y7>-.5-, fi (+,:9=B+F(\[.b]74>-. <Xfi=?Q?)fi @O_-(^7A@ fi . 9=BC5 ? . V ' )(+*-,.0/
A> fi #@ _74>-
. fi=? *-@7 fi f fi=< 74>-.ff7`@ fi 5-, fi K ? .
B ia7>-. <bfi ,B fi=< 7,.. cV
V ]_#0 Cegi f:h ]_# 0 Cegi f:j :
fi ; Cegi flk
fi f#m 4n+
V
fi ;0]_# Cegi f:h
fi ; 0(4i Cegi flp ]_(]_# Zebi f j ]_ :
fi ;#
B A.7
' , fi Bq7>-. A.07A@ fi . T9;BC5 ? . H 7>-.!5-, fi-
.IG*-,.^rs-tIuvws-x yAv{z+x |yAx |I}=v{tI|=v3(+b. -.,:9=7. 74>-.~5-, fi K ? .
% ]_#
fi ;4i <bfi , fi 5.,:9=7 fi ,-HI9 7>-. 4i-( ? .7 fi ,A.A7 % GH % ]_ 8H

fi ; #8H <bfi , fi 5.,:9=7 fi 9, F/-H TH 9 a!,. A5 .
7 \[. ?\D V2_>-.~MON- fi=< 7>-ff. ]_-( ? .7 fi 6.97 9;,.ff7>-.
. 9=BC5 ? . &7>-.
B A. ? [+. cV2>-.5 9=9, 6.7,.. fi=< 7>-.7A@ fi 5-, fi K ? .
B ]_#& 9 ff
fi ;e4i
. A> fi @ _ ' )(+*-,.^-V2>-.JMON-P fi;< 74>-.7A@ fi 5 9=9, A.L7,.. !9=,.CBL9=,+.IOO@ )7>
<bfi , fi 5.,:9=7 fi ,~9;,
7@, W9 -( ? . cV2_>-. ). ? fi=< 7>-.MON-PH =X AbI H
fi ,,. 65 fi ; 7 fi 7>-.C;* W*-.JMON- fi=<
7>-.ff7A@ fi . T9=BZ5 ? . cV
|=t|=vX}x |
fi BC5-*-74. C7>-.aMON- fi;< B fi ,.7> 9 . 9=BC5 ? . _
,.BCb. G7:9 ?Q?)D K ,.5.I9=7.I ?)D
- _-(74>-.~MON-P fi=< 74>-.ff5 9=9, A.ff7,4.. fi=< 7>-.
*-,,b. G7FMRN- fi ,_7>-. 9, 67F5-, fi K ? .B^F9 L7>-. -. -7
5-, fi K ? .BVR.9=,4. fi @,.I9 7 fi A7:9=7.ff9 U5-, fi [+.74>-. <bfi=?Q?\fi @O_-(L7>-. fi ,.BSV
0Gn Ow8Sl~6I b bw+l:!]I W8Q!lA+w8ab+ !l=CFA]
+CQ ] + bclA +A+00+L I_4aw8S84+w8cl^{+ aA0nA8QlI4
+g lw8Lw8!IQ IwIwlA+LY)ff:+c++0+waWaw800Ib=A++0+w^ G=ff`GQI
+QII_wqw8+&::A IbQl_+WTl8Q&`GQI!wwG++0+WTw8^w8IA)-:
Q::WGC++:ww8l+_ WL]
. A> fi @7> 9=70rs-tIu4vWs-x yAvz+x |yAx |I}=v{tI|=v 9 ? .I9=, i-(R9 ? ( fi @, )7>-B <Xfi ,ff _K A> fi O@ _-(

7> 9=77>-.
fi - )@7 fi f fi;< 2>-. fi ,.B > fi=? V3R.ff9 ? ,.I9+ 9 A*-BC.IL7>-. 9, A7
fi - )@7 fi fi=< 2_>-. fi ,.B
;H 9;BC. ?)D 7>-.!. A74b.
. fi=<
fi BC5 ? .7.!5-, fi K ? .B fi=? [+.9, <_SV


fi#T--L --T-RTR+++ -a +---
















_a

_



3= _

:=

3=

#





ff










`=








fi




=



+


fi


















_



_
3=
=



! "



3=

$

3 ff





A3=








fi



3 ff


















fi

`=




%&'


(*),+-".0/21"34(*),5'67),5"+980:"/<;>="?A@CBD80:"/E8GF@</H'ICJLKM,/ONP3 N#),5EQSRTQ'UPVQWIX5'6

EY @N'QZRTN#),5[Q\VQ

]^:"/^;>="?_@XB`IENG/P8@CBaK".0@7bMc/PJ9Nd) ND-"5) e7-"/b\/ IC-'NG/fg)hNi-"5'ICJSb),+-"@-'NPjDkEND6/ON .G),b\/O6L/OIC.&Ml),/P.Om
80:"/n;>="o IC5pb\/ @JqK"-"80/O6r)c5s8G),JL/SMt),5"/OIC.u)c5s80:"/S5-"J<b\/P.2@CB/HvIXJLKM,/ON2IC5'6p8:"/ZNw)cxP/ONy@CB80:"/
K'IC.N&/z80.0/P/ONPj{=C),5 /pK'IC.N#),5"+pB|@.L-"5'ICJSb),+-"@-'N @58/H}8SB|.0/P/z+.ICJLJnIC.N IC5{b'/W6@5"/r)c5{8G),JL/
~SG U w IX.GM,/Pm'OC mC80:"/E;>="?@CBIyNG/P8@CBK".0@bM,/PJnN IC59b'/B@-"5'6Z),5nK\@CM,5"@7JS) IM8G),JL/j4]:-'N
80:"/2NG/ @75'6 @5'67),8G),@5@CBi]^:"/P@.0/PJy) NN0IC8G) N#'/O6INF/MlM$j
]^:"/8:)c.6 @5'67),8G),@5L@CB]:"/P@.0/PJEIMhN&@E:"@CM 6"N4N#),5 /JL/PJSb\/P.NG:),KL),5nN&/Mc/ 8$NG/P8N @7.0.0/ONGK\@5'6"N
80@K'IX.N#),5"+F:) :) NnIX5 ~Sw U K".0@bM,/PJWj(*),5'IMtMcmNw)c5 /p80:"/z5-"JSb'/P.9@CB[NG/P580/P58G) IMB|@.0JnNy@CB
M,/P5"+80: ) NEIC8EJL@N&8[9B|@.ENG@JL/ @5'NG8IC58ECmvMc@7+u
v)hN@XB80:"/S@.6/P.E@XB ~Sw mN0IX8G) N#B7),5"+z80:"/

ING8 @75'67)c8&)c@75@XBi]:"/P@7.0/PJ"j4/P5 /E),8B@XMlM,@CFN8:'IC8
:'INIqNGK'/P/O6-"KM,/OIC.05),5"+WIM,+@.&)c8:"J),5
Wj


fi"'7P"OOCC"""0"

,d"p''P'q}X`hO
"z"O " G$ PG OX$OCC'p0"ra"0 v G$7 PGC "7 CE|G" CC0y,L,PLP0O>,9"07C
t,O\^9C'{0O&0Oc0"GS'Ct ,0PXG,An,"p""0O0#,'0
'GXc7O_C0P"PCOs'#,"0"WCLnCZc*,"0p'"9,'7n'PX0



, ,'7c"p0"G,*,"<"'X'WGq<7tP0P&hXG,C'W#,Ltt CG,>'PX0P C,7c"
>"0,P
'# G9 20PLC,"s0",0P72#,AC'_#,Lt |,"0"p0OG,WqS










'0#,, $p0 }\PG,LPO L0G"qOW'C"Z&C,"G,'090"S"0,Pn2X02"0C7
ZXn"0P0'0O "PODSt,
00Xl,O9"'PG,LPC&c7 ,0z7tP0P0,c"LGPP
,L,PLP0Os0"L0O "Py'#,"GP2 ^G, 00C40,OP $ 0"LO "P 00C



ff

fi

0,O90"Pn&c7OP0z,OC"Offp"nC|P"PX0O{vCqcOr'#,"0"COC7 P
0"&c"0'C 7L""0OS0"LGLG\ P"PtO
CG,2
GPq[
",PnP!4 G
'CL, '# GO9"0,P
C'0"L\OGE'&E'PX0E0WC",p'CE"0,PW#
0 9n,O {'PC0SS,L'pC"t C,0nX'X0S2
C""0O0#,!" P
0"p0O "P#|P"PC0OGC,"G,L
"07cP 0't, 7'# G9L
pt GW2
'CCLPPGP
\PC0Pff"P0"'CXLP0PP"70O0", C&c7%w$ G"'""0O0wc7'i
& X( 9,hC"t,O
"0"OG2'cP"0"E"07C,OC0'0"2G, #$ GPO G,'Or,W0"E"P,'G G,
^""0,PGC,P ,n0"'"O# &' C00G"LOZ02PL,CS'&#| P0P0

"z}"0O#,00Pz00C') &"'""0O0#,'qsC",,"s0"z'PC0 Cc 0"
,OC0"P* EG, #$ GPyC0L","OpLGEG\ l 7P"PtO
CG, 0"L0,c"vXL,OP
C'#, <\00"E,OC0"O"07cP GC,PEC'r0"2X0P"07cP GC,PPL,Cz0"<CL
,&c"0q0PGE"2""0O0#,p00P\z0"<C"LP,"2"0 4 4"P0P "\0"
,OC0"O9"07cP GC,P O" '# G0P ,0z0"0,,"L'CL,OP P E"0"P70P



+




+"

+

X"l C
,S0Lh'CXLP0PG POzncp t$
>L0,"O>0"nG"G0P p,0PC&c7"0,Pny0'C '# G0O> G"nu "0" E
' P C'WG7LE0G,"7LP0G |" G,' * #W'C0& C'O z"
0,P D0"
|Ctc ," 70W "PEO
cP ^7G, 0Oz" |0S,C'S, tdc "X
C'q,'P'P'PG, |0t"P "X

,




.1



Z


[ff

32


4

.- /







1

0





%
65
97 8 +:z<;->=@?BACEDGF 8 wcH-I <-IE+:z<;KJL-GMGF 8 #,N-9I <-I3+:z<;KJL-4F 8 #,O-I -I3+:gP; QRS | 0PO W0,c"9'CL,'"<G"G0PT^0OG0OrL&PU*ffZ0OG"07cP9P"
0OG"0,Pn4
P0<h&gG, 0O'#,"z0"S0CLE,,"W7h&0G,""G,LPG,"Or'3| 0
0OG"0,PV
"0OW7 00 &cW&C,Ozz0"E,OC"P4 ,GC,"G,v G,znC "O
0'C4
0"2O "P ESL70 '&P0C&c7W nD LO&"G,"W P " z0'X "G,"
0"E"07cP9+
pC00O Oz<*7X ,,P""0O0wc7'O ,0"7""c0P#,K$ G
| OE"L,OC0"PS0p#,Lt 0"q0OG,y ,0PC&c7p,0"gCLY p0'CE0"90O "P
OP



%

*," gG" u0"L\P PC9 0"90OGE",Pn 00 G,T&C,O |0 "L0OGSGP
PC7OCP

,,"_0G 9,0OX,'G90"p"S'Pz <0c,"'CL,OP"
P007'XP"0"2GC'"XnP7 CG,9,0P0h^z'g# 0"ELOCD",OCc"

\X ^]

<_



1

ff



P0O ,90O
,"Z7
P
\ C'GEO

" ,0,W 0,,"SvXL,OP4
0c,"L'CL,, $ 4C,O^ G
02nXn&nt0,,"L"'PG,P OP\O 0OG'7'7c"
0L7"E'PX0C"t C
G,

`

a3b>cWd\eBf6gih3d@jWkmlnf6oKp q^rmrms#tuomvw*omd@kKs#x>kmld/ermsEwny6d)z+o{f6pomkmd|q^jWs^x}kmldKd@pBk~f6rmd(jsEzOq|f6pWwd@g#q#vomdUq3'zOq^pBUermsEwny6d)z+o
f6psEvrKj*s3zq|f6pj*s4p*s3kKl q@3d+gy6sEomd@jxs3rmzoms^y6v*k~f6s3p*o@}q^p*j`w 9o~f6p*g@dOkmld+kmd|q^gild@r{f6o/kmsOwd\ rmomkKkmriq|f6p*d@jv*o~f6p
l q^p*jnomdy6d)g@kmd@jd@*q#z+eny6d)o@'f6k.f6oOkmd@jBf6s3v*o+kmsWjsWkmlnf6o(s3p
qyq^rmEdHjsEzOq|f6pffbYld+y6d#q#rmpnf6p*Yed@r~xsErmzOq^p*g@d4s#xKkml*d
omBomkmd@zf6opsEk{omd@p*o~f6k~f63d(kms\kmlBf6o{gils#f6g@d/s^x>kml*dKermsEwny6d@zjnf6omkmr~f6w*vk~f6sEpb
*

fi/`/N ffffff>%>ff>

100
Average accuracy

80

60

Accuracy



40

20

0
0

5

10

15
20
Number training examples

25

30

9ff#R4'*#<%3#ffmffY>34/O`'#YB#^ffH>E#ff
@3>E<B@ff
ff3#BH<WL^WY*
LY4>'ff\i/'ff'BOGffG9ff
`
ff3#ffmffGBE#/i4
3#GKR@*ffB>3}ffGBE#E^>#<Bff*\H@ffRiK%ff3#ff
ffGBE#ff3/`ffH#Hff3BW#W|*E^u@>ff33<E@B+#>^@>ff3H@@ffffGB#
#EBG#B#B<`>3^*ffu#3^B>3%O@*E^4i%#<W@*3|'(3ff><BW^W*^<
K
ff3#mff>BE#ffE+>1mff#
B>
ff4@>B*<*#<1EYB+ff#'
} m'R'*ff@
{ffB} fi>

B#W+W`ffH#ff#@Y|u#>4@E#*#H#B#*|BRE<Yffff*3#ffE#3#B# *
m*##*B<OB^U#H`ff R RGBOL*Um*##HB`Eff+>ff><*fiR`WGR
K
{i
+< 6
/>ff9#*##*H#Y3<i*ff^(# !@'3>*34>Y*B{>
#3O*O#H3<W><@ff>B\i
"{ff<$ #KB3ffG#Om*#^*(#U|*BG>`#3O *#
@Y><@ff>B&
%>@#WE@ff(
'*),+.-ff/0/0/0-1)324+#`#B#*@B<H
@3#5
)'OB#6
)37K4#
{E$ 8:9<;Hm*#^ff
<ff
>
= @
?:AB?:CDEDGF HBIKJ1ALNMOA.P3CRQSDTI.L#41 U*3(9RY>BE^ffK
= ff`Hm*#^U
O>3|uL^H HKff<
^>\*^#N>1(><BGB><BRHff#B+m*##4 *
W V4ff||Y
X,Z[\PE$ +<
6
/GWH%1 >Y<(1#ffE<*3Y>R^< <
'>*B>|Y#
1 U*3YW<##ff#><@ffW3<`3ffY<B#3#*3E<m#ff #@>
><@ff%/#>U3<ff^
]9ffE <*3ffYGR#<)ffU>Bff*<u<ff
>ff "{ff<_
#.B3ff
i(
"{ff<5
#KB3ff{#Y1 U*3HO<uffGBE#ff4ffa
`bRY c.d.e$fR{BEff`@<B>B>ffHff<
ff^Y>R)| O#>W@ff{@%ff^Y>R)| O#1<}
V4@UK XffZ[\<U<1 g>*
Yff^HffBBEff@ff.<*3ff1>R#<@B*
hn@B@<*3ff1>R#<@O i1O<^U<B<
#
|>#u<ff`>B
jlk,m

finoqprBs3oqt,t,uwvyxo.z.oq{qoq|}oq~

ff0_ffK<<T611NO.3R3GB.03}1ff
130q5B}q^3
:,.}q,G}q
1 ,1.31.}05qBff06 w:,.}q}lff0qwEw:q106.
qllq,.06}.
:,.}q}^.affGq&,.q},$.$q},1,W&ffK:}},.w}qE.qN ,Eff
:,.}q}w0 BRwq310q}q3>Y0q31&Ew3EffG0>,1 }.0&B.,.q}@BK30q
}.K3K}qE.qKffN}&}.w3Eff,1 R}.GG0N^q3} q^ff0>
w1GBY0K}3q},3K.B:0R$q31&E}3q},3R},R00$,}q},w.G,
wq1&llq,S}q30wENq@3lT0,1 }.0:@.K30q .}q&:,.}q},S
5}q 6}.&}q6RffYE}.E, >0q.0 ffK.},}10,ff,ff,}OK
E$q q0>ffG0l&q$0,.}q3N}^},1B03$q3>}qqqRffEw.}03,ff0
}.W, 1q,: ffG.,G$}q 3^.0..01.},0R,q363K.}13q*33}
K}@,} ..051.}.>B3K.&.0..0
E3ff0 $E}3EffG0,1 R}.0NB :,.}q}wK3q6 qq
0R}6@3q3KffG003y>}q>K30qB,,ffKY1}K.0E aK.0>.@1}R}
035 WqRffK.}$q3}q51.0q N}3q},3R$q&,.q},0 }q6..BK30q
.}q>}.@}3q},36}q10 R0alff0q,31} $0}q>:Ea1.0q .@q R<
}.}E,w}q5B,.W:3G&
1q,1:&G$q3q33w0NN, .0l.0 K._$q3}}q
:,.}q},,ff,}16,B^}q10Rl.ff0q,3 ,ff,} < .>}q:,.}q} N}q6lff0q^
}q6:,.}q, 6}q}Bq(}q ,00qK.} .}.K.}3,a}a,B5q Rff.ff0q,ff
,ff 1} W YENqq},qq.NT0@.E,N}q^B1}:K.0q}B3}>.>q}E1
q}1q . }.E,ff
&Bq.w,6}YTff06Ew3EffG0,1 R}.0. }E,}q$3K.}0Rq}
}q30a (B1}K.0W&}^,BK 3$N03R>K.: .}0}K}
W}*3.}.,.q}$llq,Sff0$E}Ew3EffG0,1B R.0w0},,1w}
.>31.},Bq31.@1.,&1.} ff,ff} < *3,333w}< ,ff,} SR,ff33
}.W,}q&1}B:K.0$q}3}R01&}q$.ff0q,w.:,.}q,$$q}q0qwE.}}3wK.}
330(}q10 .ff0q,60q0Eff^K}.(q6q>lff0q,._B}q3,.q},
*}3q,3R,>w0}(RK}q ff0}.E,N31.}0Rq}}q3}&qa}5
31.}6,q31BR.0l.0K.}a}3.3,GE6},qqR, _3
}3E1,w0}0Kwq},q}1q>}.w}.E,}q&1:K.0&q}3WN31. 3EffG0
,1 R}.0 ff0}.6}.E,}q B3K.50Rqa1}>K}.61Kff0
0>qq},qqR&1}3 ^3ffG61}6K.0^qBq},qq.,
G1}K.0 w0}qq}qE.}10K3,a,}q},NE.03Y}q3^3 ,
} 1}}1.0q0&:} R0Eff$K.^w0}qq6. RK0q>,.Kq.$a0
0q}@W&}q,BK qw,}q},
}Bw}6>.}$31,}010K.3}}q10fflff0q,3
.q00q1R W w$q3}(E }qalff0q.:,.}q} 0qK}>1:}.q0Bq
}q61 G}q:,.}q},66}qqY6ff>qff6}q10^BRfflff0q,^qqa}q
.qGE3.0}q1}}q3ffGwGG3Rff0}q10Rff.ff0q,^ff0q>q:,.}q} R
}q63a.}q .qGE3.0}q61} 3..0..0 q}03 E.0,01}
.qGE3.0&}>q}B3 .03ff

ff
fi
RSTff Bff ,3 B6 "! .3Ka#a6,.:R3G
%R$ ffW6^>::BWB,1'&5y#ff,^%$(31:RS)$>%$R%*
$ 3O1,:60/ #
+#, 03,1EBEG61KNq.3G.- E%
1# :1Rff<@<<$BT:<#q _S6K'3 '&54.687:9%;#<>=?9;A@B C?D<FE>;AB G:C?9#a%$R^6ff :R3G& - <%$
12,
%R$ H 1:W6ffK%26
& ,
I8JfiK

fiLNMPORQAS(TAUM QATAS(VWXORQAYZ8ORQ[#\#V#V]3^A\`_3V#TAQAaAba3c

dfefig h?iAjlkPe:im.n8o:p%q#r>s?pqAtu v?wrFx>qAu y:v?p
z|{ dlk~}.
#A#'ff)"
fi?FR?`A
ge.}? e:g k jg
i' z|{
)>#A?:ffA?AF%Ffi?A?l#
fi?FR?(#fiA>fi?FA>?f F
H#3FR`>
{ jH
g k}fidlk~}fi?FA>?
{ jm.n?o:pq#r>s?pqRt#u v?wrFx>qAu y:v?p
AA(0#8`#5')#?ff#)
0 >80A ?A.A'' ffA ff3#'>fi?`F#A 2:>?
0#'fi## #A 5'ff)5 ? ###F#3)?#?fffi? fi )5 :.' )? #
))3 'ffA >. >fi? # '#A#'ff)AfiA) >305>A
#?ff? f HAA)fiA(A# 5A>3)fi #A (~A0#F?'ff#2>? ffA
3>.?ffA #>? ffA Afi?`##F#3)?# ff ?ffA Afi?Afi.
5?>A5fi`3:#.A 2:8:ffAfi8fi#' . A.88A?A5 fiA
?A5A#'ff)?3 AfiA)A ?">))A.#F#3)#0F#A ?
`A ?ffA ?A#?'ff A) 8fi#'A) #3ff`fffi? fi ff
ff) Afi#A) 5'ff?A' fi AA#'ff) fi)?3>.A5'# 0>) :ff
'fi#52?'ff` fi>3fiA#') HAFfi?AfiA# fi#A 2:
88Afi:F) ?A' )? H A) >3A) #3fi fi5##F#)?(`>
fi#A)3>fi0.55>fi? #)?#0>fi#A .3A ffA AFfi?AfiA#
.A 2: 88Afi ( ?A' fi #3)?#>fiA .AA A#'ff) A)fiA
AA F) ffA A# :AXA (PA#F?'ff ff H )fi
:A.Ffi?Afi#'3:#fi#AfiA ':(8:ffAfi #)?>fiA 3?
5fiA ff' :(>?05A0 :> )A fi#A?ffA
53 #3A?3?# 5'ff)5 ? ?~Afi#A) ff. ?~


3' fi Afi#A)##ff. ff '3> #A)0# ?>fi?`# ff '35)'fi
'5'A A#'ff)">) ff fi )5 (#AA)# 0 ff3A).
? )A')"?ff#) ff A'2#Afi >3?Afffi? ) >
fi#A)fiA#'ff)">?ff#) 0fi )5 'Afffi'A.?3?# ') ?(A
fi#A)3fi)?3> )3>fiA##F#3)?# ?:fffi'fi fi'Afffi? )
)#)32 'ff . >Afi3?.A) #F?'ff3?5) )?:
>?ffA AA'3fi' AfiA) 'fi fffi? 'ffA 5)A):fffi#) :? ffA
5'ff)5 ? ?AfiA)H##ff#lff #HA?8fi )5
? 3fi



fiff fififififi!fi"

#%$&('*)+-,/.0 132.4+653)+7983&&(:<;>=
?@A&(B*C?83&8ED!&F
GA$&@!HI=;>J
&KL&MNF4OQP*R>&8SD!TQ;>KG@!&OU&KVDWFRXR>YLZ?*;[R6\LD!$&
O]F
G@AT^DWFZ*R>&
_`#aTb8c;>OQP*R[;XdIYQD!$&EP@!&83&K*DWFD3;>T
Kaef&gF
8A83?OQ&gD!$F4D`D!$&gP@!T
=@WFOh;i8j=;>J
&KUD!$&gK*?O(Zk&@
TddI&FD!?@!&8mlnejFK\oDA$&QKV?ObZk&@(T4dE\;683Dp;qKGDrds&FDA?@!&QJFRq?&8Uteufg$*;6GA$vD!T=
&D!$&@(\*&DA&@!Ob;>K&wD!$&
P@!T
Z*R>&Ox8c;>y&bTd`T
?@Eds@WF4OQ&fT
@!z _{jT
D!$wT4d|D!$&83&(GF4K}Z&b&83Dp;qOFD!&\dI@!T
O~&MNFOUP*Rq&8^F4DgD!$&(GT*83D
Td|FmR[;>D!D3R>&(F
\\;>D3;>T
KFRGT
OUP*Rq&M*;>D3YwTdDA$&SRq&F4@!K*;>K=FR>=
T@3;>D!$O}_

ua '*)+-, .0 132.+59)+3
bWW 79 W C l tk
*
&D !/u!Wc
u*( 7 W l tNC
*
46j ^|SH-j {<|D!TQT
ZDWF;qKL73 W CW
f;>D!$D!$&^K&MD%;qDA&@WFD3;>T
K
&D W>>>3
PP*R>Y}D!$&^TP&@WFDAT
@g83&
?&KG&bD!TbD!$&^P@AT
Z*R>&O

mo[^aa aoba

k [ 79kCW
%

&GT
=
K*;>y&bD!$&^D!&@!Ob;>KFDp;qK=PT4;qK*DW8jdIT
@gOF
G@!T
HIT
Pk&@WFD!T@W8

Pk&@WFD!T@;>K\*&M] [
mg[^aa l <
V

JFRq?&bTdudI&FD!?@!& Td|
[



&D w wZk&D!$&8pO]FR[R>&83Dg;>K*D!&=
&@8_/D_- e e e e
igL3 ;i8E&OQPD3Y
3 q p ! >>iW !
P q z



O]F
G@!TQDFZ*R>&

'*)+-,/.
0 132.+53)+

:<;>=
?@!&(B | &@p;iFR`F@8c;>K= R>=
T@3;>D!$O}_
#%$&b;6\*&F}Zk&$*;>K\D!$&]'*)+c, .0 132.+653)+j;68(89;qOUP*Rq&
_-DbGTR[R>&GDW8QF}8p?*GA;>&KVDbK*?O(Zk&@TdgD!@F;>K*;qK=
P@!T
Z*R>&O]8mFK\D!$&;>@83TR>?D3;>T
K8?8c;>K= jSH-| {a|_u#aTw&F
GA$LD!@WF;>K*;>K=wP@!T
Z*R>&O
ek;>D
FPP*R[;>&8S;>DW8m83TR>?D3;>T
Ko83&?&KG&QT
ZDWF;>K*;>K=}DA$&r8p&
?&KG&QT4d|;>KVDA&@!OQ&\;6FD!&Q83DWF4D!&8kmD!$@!T
?=
$L_
;>KG&;>D;68bz*KTfgKoD!$F4DD!$&w83TR>?D3;>T
K8bDAT}P@!T
Z*R>&O]8bF@A&]=
&K&@FD!&\?8c;>K=DA$&]O]FG@!T}P@!TZ*Rq&O
83TR>J
&@uf;>D!$QFSz*KTfEK^dI&FD!?@!&jT
@W\*&@3;>K=eD!$&g83T4Rq?Dp;qTKr8p&
?&KG&gOb?83DuZ&gFSGT
OQPkTV8c;>D3;>T
KQTd8p&J
&@WFR
O]F
G@AT
HIT
P&@FD!T
@W8_`-DSZ@!&Fz8D!$*;68S83TR>?D3;>T
K;>KVD!TU;>DW8gGT
K8pD3;>D!?&KVDSO]F
G@ATV8 9 dIT
@g&F
GA$wdI&FD!?@A&
Z*YQ@!&GT
=
K*;>y;>K=QD!$&S&F@3R[;>&83D;>K*D!&@!OQ&\;6FDA&83DWFD!&8;>Kwfg$*;6G!$wD!$&E@W83D ds&F4D!?@!&8`TZDWF;>KwD!$&;>@=
TVFR
JFRq?&8_]#$&bO]F
G@ATV8^F@!&Q83D!T@!&\;qKD!$&QFPP@!T
P@3;6FDA&]G&R[Ri8mTdD!$&bO]F
G@!THIDWFZ*R>&b?K*Rq&8A8DA$&rG&R[R68
$FJ
&FR>@!&F
\*YZ&&KR[R>&\ZVYP@!&J;qT?8cR>YRq&F4@!K&\O]F
G@!T
HITP&@WFDAT
@W8_
#%$&^@!&83?*R>DT4dnD!$*;688p&GD3;>T
KGFKwKTfZk&(83DFD!&\FK\wP@!TJ
&\ _


fiN|a|NoNo





ff
fifi "!$#"&%('*)*+-,/.10 24365 798;:&<>=?8@:A0BCD)FE"GH'*I'*IKJALNMOGHPRQSTLNUV'UWG-STLXGHEYIZ'$IJAGHS[JH\4]
EX'$)*+KP_^`\HEbadce'*Igfhc9ij' ^
kHl>GHS$SnmH\HPffGH'*IUo'*IhadcpGHE"LoU`LqEX'*GHS$STrWmLYsX\HPRQt\U`GKuNSvL>%('$)*+-E"LNUwQLYs)()F\Wxzy
{KlRfhc|';U)*+KLU`Lq)ff\"^}GHS*S(Q E6\u`STLqP~U\SvLqEUuGU`LYm\HIIZ\HIE"LXmHKIZmHGHIZ)DPffGHsE"\]6)FGKuNSTL`U%('*)*+)*+L
^LXGH)FE6Lff\HE"mHLEY'*IKJffxp^\EomH\HPffGH'*IUo'*Iga c yjGHIm
Kl>)*+KLWIPWuLEo\"^WmH'U`)F'$IZsq)^`LXGH)FKE"LgqGHSTL`U7';UWu\HImLYm-uNrAG>Qt\HSTrI\HPff'*GHS^Is)F'$\HI?\"^DPffGM';]
PDPQE"\Ku`STLqPU'Lz5(l
X};H8;K`:`Y8T;6HzH"AAx|HY`Yqq>;:A;:XqH6@:
HY`
;YqbFH ;HRb"`Hgeff$H"!*H F#" q:;:";WV365Z6C
Hff;:o";WRD8;Wo6@:48@zHZ`YH$(:qgKff(:ffgK;ff8;`:H
o}gHX;:e-hHX}Y8;A$H:qK"R`:Xfhc$A}"`o8;:Hff$8;F";W
H8;`1"8;H`YN
"!$#"Hq9z"Hq(H`Y4H(""q`:X(`XY(Z`6b``:h4:Kff"Y
* 0 qq[ $ 0 K`qX`HXt4: * 0 ;q t* 0 NKqq"`>hHXg" R;:XWffHh;:
$-"`O8v8;qXHWZK8;FHAFqHY`";:Hqq"Y59XqXAW:H`Y4HgH
hHXHFH`XH:-FqqoffHH$:`Z`:`:K>H:o8;qqqq`
:z`:XH@6qzffhHXHFX8;oHZ`"H:-wHXq 6 D&Zo$`:"$`8jg
XHq"H:;:hHX;:egY`DH8;`"8;H`qogHXY8;H D"q`"
YH`j8@`"8;H`$bH"qH:Ao::q::hHXffY48@H:g:h""q`:Xo4
"8;";H:AR- " Hbff"XRXbH8$gZq::qb;:XD8T8&hXKq"`:
;:}h8;";H:bWX;:@:H8;`h>WXHqX"8;XYXqKhw[ "!O4$#"6
Wff$H"!$H F#" v8T8Zo8;ff`X"8;";H:b(8T8q"zH8;`hb6@:
g8;q:qphHX?Y8;H`:XAH8;`"8;H`WHWKg8;q:;:8;HH";
XH:$"`:K@-X;:@:A8@H
;:X>hHXjXYXqWgow[H "!O*#"8;hYhoXHq"ZH:;:hgHXK
;:eY`DgHXY8;HbAH:8;};:$g8;q:qhHXY48@ff;HffOv8
X"8;";H:;H`:KqH`W$g`:"W-:qXqegHXHFHZ`Yff;:
YH`(hHX>Y8;H:`H`b``:g8;q:q b;:X`&5A&:g7DX48@:``z
W"D5 7hHX`::K""`ffbq"ghHXKX8?gff$;:;:8;q:q?hHX
Y8;HoR`:Xffff:KffZ`T``:KhXhX8;qzH&H8;`d"8;H`Yz4`:Zo8;q:q
FH1W;H`:Y`hHXgY8;>$z`FZH:q<H $R$zffXqX";HoHq6
"HX>j8@q4:`q`:XH `H`H ,.0 2 3"578;:<z=?8;:B0 CbX8@qR$"A@`:
`:"8;q:T8T;6H
ff$gqH""`H ;H}4WA::;:";WDw[H "!O*#"$WZH:qK
ff3",AO5ZCY`$>ff8;`:Hb8;H:Hq"W"48@@:};:WYO@:;:}8@H(:?W`
Y4W``Yff-WX:qeZHH@:X},.ff3 q2 8;:h0B CY(A:";WAow[H "!O*#"b
48@:Hff$8R@:eO8v8zhqH;qpYW``Y``:X@Vff}`q8;q:@:8;HH";/FH
adc;:-fhc(R
jH(``"&Z bw[H "!O*#"HXt8;;Y9"`"$8tqXWKT8v;"HKH8;`W
"HX"X$O8v8;&;bDXHq(&ZH`:"$8T8@XtZH:`:KO8:ff`b"8;";H:
;:KZ8;K:ff$8D;`hHX}X8;HH`"$8D(Y;:}qH;qg4W-qH`W"8;";H:
3w$q-A&zwjR&bj?C`:-ZV`@q;:-h6@:48@ff8@`d8;H`q H;
;:8;}hHXY8;H}"$q$gq;`W`:AK8;`";:-q`A"qYFHA


fi
fiff






!#"%$!&'$!(fi)"#*$!"*$,+-$.0/!)12+-3
4
5!$67.8$!9:.0;=<?>
)9@+-3fi%>=!#"$!&'$(fi)"A*%$!">fi!+9
$!*4fi))9@(
")/B0$!3fi+C.0;
.0)"9
)DEGFH>:BI+H)9fi+-3
")+J*>fi*J*>
)")BK+?7.0<?7;L+MN#"$1*AO4:.P)QB09*%>
)?.0)"9
)"7RS++-)"A%>+-(fi!#)T<?>:BK%>
BK+?#$!9fi+CBK+-*%)96*T<B0*>U7.V.W*>
)1+X$.03
*-B0$!9fi+5!)9
)"AO*)DY*>:3fi+JZ[O"EMF>:BK+\(
(
"%$6!%>]%.0$6+-)#.0;YB09:*)5!"A*%)+*>
)
^ .0)"9
)"#_9fiD`*>
) ^ *)!%>
)"A_,9fiD`4
"-B09
56+?$3
"a+-;
+-*)b%.P$:+-)"\*%$,*>
)1(
"%)/!B0$!3fi+QB0,(:.0),)9:*A*-B0$!9fi+
$ZM3
9fi+-3
(fi)"%/!BK+-)D@+-(c))D:3
(U.0)"%9:BP9
5=+-3fi%>d!+fe8gNhjikml87B0"AD`)*\7.nE0opq!rs:tAE
Fu$=+-))1*>
)NB0,(c$!"*A9fi#)v$ZG*%>
)j4c$/)1")w3:B0"),)9:*o8#$!9fi+CBKD:)"1<Q>fi*?>fi(
(c)9fi+TBVZ*>
)N*)!%>
)"
3fi+-)+N+-$!,)NZ'$!"b$ZH!D:xBI+%+CB04:.P)U+X)"A%>y.P5$!"-B0*>
z*$=5B0/!)Y9@$!(
*-B07.+-$O.P3
*XBP$9{*$=)/!)";@|}B05!>:*
~ 3
#.0)Y(
"$!4:.0)B0*TBI+v!+X!)D*$U+-$.0/!)!E2'Z?*>
),(
"$4:.P)Y+1"),%>
$6+-)9{3
9:BVZ'$!"v.0;{"#9fiD:$!v.0;!oB0*fBI+
>:B05!>:.0;=3
9:.VBP)#.P;=*%>fi*7.V.8*>
)+-)$(
*-B07.+X$.03
*-B0$!9fi+?9Y4fi)1D:)"-B0/!)D2Z'"$!9:;+CB09
5.0)!#"$&'*A4:.0)!E
F>
)UDBV]#3:.0*-;$Z?fi9fiDB09
5$!(
*-B07.a+X$.03
*-B0$!9fi+NZ'$!"v*>
)U5)9
)"A7.VB0*-B0$!9$Za|BP5>6* ~ 3
#.0)
"53
)+?)/!)9,$!")1+X*"$!9
5.0;U5:7B09fi+-*\*%>fi*?(fi$:++CB04:BV.B0*-;Z$"?4:B05!5!)"(
3
#.0)+Nkm?*%9
)"a"v3
*>8o
pq!rs:tAE8F>:BK+M+-3
5!5)+-*A+*>fiO*8*>
)G*%)!>
)"uBK+89
$*8Z'"))G*%$?3fi+-)O96;(
"$4:.P)+X$.0/!B09
5$!"W+X)"A%>1,)*>
$

*$U+-$O.P/)(
"$4:.P)Y+oBVZ\*>
)x.P)O"9:B09
5d>fi+*$=4c)]+-3fi#)+%+CZ3:.nEUQ$<)/!)"ocBVZ?*>
)v.0)"9
)"NBK+v7.V.P$<)D
*$@!+-w3
)"-B0)+o}BnE)!E0oJ!+-@*>
)Y*)!%>
)"j*%$d+-$.0/!)=")#Z'3:.V.0;D:)+CB05!9
)D(
"$!4:.0)+o*>
)]+mBP*%3fi*-B0$!9BI+
DBV8)")96*7E\F>
)9@*>
)N.0)"9
)"19@!+-=*>
)N*)!%>
)"\*%$]+-$O.P/)j2(
"$!4:.0)D:)+CB05!9
)D{+-(c)%BV7.V.P;+-$
*>fi*QB0*A+1+X$.03
*-B0$!9{<$!3:.KD`fi*1Y(fi"*-BK#3:.K"j#)#.V.JBP9*>
)v!#"$!&'*#4:.0)!E1C9UZn!#*ocB09{$3
")#L(c)"-B0,)96*
D:)+#"-B04c)DBP9*>
),9
)#
*j+X)#*-B0$!98o8*>
)2*)!%>
)"13fi+-)+N+-)"A%>{$!9:.0;@*$d+X$.0/!),*>
)+X3
4
(
"$!4:.0)+N*>fi*
#$!""%)+-(fi$!9fiD2*$TB09fiDB0/!BKD:3fi7.M#)#..K+JB09Y*>
)?#"$!&'*A4:.0)!E}n9fi+X*)!D2$ZBP9:*)"(
"%)*-B09
5j*%>:BI+J!+M*%>
)?*)!%>
)"
)9fi+-3
"-B09
5,*%>fi**>
)")TBK+,+CB09
5.0)!#"$&'*A4:.0)<?>:BK>YBK+?#$9fi+CBK+-*)9:*<HBP*%>U7.V.B0*A+Q+-$.03
*-B0$!9fi+o
<)9
*>:B09
U$OZ*>
)N*)!%>
)"\*%$,4fi)T-3fi+-*\*%>
)j+-)"#>(
"$!5!"AO<?>:BK>@+-$.0/!)+Q*>
),+-3
4
(
"$!4:.0)+E?B0/!)9@
(
"$!4:.0)Uou*>
)N.0)"9
)"jD:)#$,(fi$6+X)+TB0*TB096*$U+X3
4
(
"$!4:.0)+1O9fiD@*"-B0)+T*$=3fi+-)v*>
),7.0")D:;U.0)"%9
)D
!#"%$!&'$!(fi)"#*$!"A+B09=B0*A+H!#"$!&'*#4:.0)1*$,+-$.0/!)N*>
)UEM>
)9
)/!)"Tv(fi"*-BK#3:.K"+-3
4
(
"$!4:.0)D:$:)+
9
$!*?>fi7/!),#$!"%")+-(fi$9fiDBP9
5Y!#"%$!&'$!(fi)"#*$!"BP9`B0*A+*A4:.0)!o:B0*\+mBP2(:.P;U..K+Q*>
) ^ *)>
)"A_x*$,+-$.0/!)
B0*14:;{+-)"A%>{9fiD{+-*$")+T*>
)+-$O.P3
*XBP$9@B09@B0*A+*AO4:.P)!EYF>:BK+TBK+1O9fi7.0$!5!$!3fi+f*$U!+-B09
5U,)v4fi)"#+->:B0(
w3
)"-B0)+MB09Y$!9
)T$Z8\9
5.03:B098RS+J,$
D:)#.K+G$OZ ~ .0)"9:B09
5=knT9
5.03:B098o8pq!r!rLtAEBP*%>*>:BK+J,)v4fi)"#+->:B0(
w3
)";,$
D:)#.[o7B0*uBK+89
$Q.0$!9
5!)"}")w!3:B0")DN*>fiO*8*>
)")BI+}Q(
"$!4:.0)+X$.0/!)"8B09N*>
)M.0)"9
)"7RS+8>:;:(fi$!*>
)+mBI+
+-(fi!#)N<?>:BK>BK+T#$!9fi+CBK+-*)9:*<B0*>U*%>
)1*)!%>
)"RS+Q+-$.03
*-B0$!9fi+ETF>
)")BK+7.K+-$Y9
$,5!3fiO"A9:*))1*>fiO*?*>
)
.0)"9
)",9fiD@*%>
),*)!%>
)"1(
"$
D:3fi#)Y*>
)+2)+-$.03
*-B0$!9fi+N$!9{"#9fiD:$!(
"%$!4:.0)+EYn9Z[#*o8*>:BK+aBI+
,$6+X*.VBP)#.P;@9
$!*Q*>
)j!+X)!ofi4c)3fi+-)v*>
).0)"%9
)"3fi+-)+?*%>
)1!#"%$!&'*A4:.0)1*%$,(
"$
D:3fi#)1B0*A+T+-$.03
*-B0$!9fi+
9fiD2*>
)?*)>
)"J7;j9
$*EM
$!"J)#2(:.P)!oBVZW*>
)Q*)!%>
)"7.0<?7;L+fi9fiD
+G*%>
)a+->
$"*)+-*G+-$O.P3
*XBP$9*$v
(
"$!4:.0)b46;=+-)"#>8o
*%>
)9YBP*#+\+-$O.P3
*XBP$9fi+?*$v"AO9fiD:$!v.0;U>
$:+-)9=(
"$!4:.0)+T")T.VB0!)#.0;U*$24fi)j+X>
$!"*)"
*>fi9=*%>
$6+-)(
"$
D:3fi#)D=4:;*%>
).0)"9
)"E
QW}
[@fififi2LOI
n9*>:BK+?+-)#*-B0$!98oc<)TBV..03fi+-*%"A*)v9U(
(:.VBK*-B0$!9@$ZM*>
)*>
)$!";Y*$v*>
)v|}B05!>:* ~ 3
#.0)jD:$7B098E
C 9|}B05!>:* ~ 3
#.0)!o!.0)*Qmo
Vo
6o:9fiD,f")(
")+-)9:*G*>
)Q(
"-B0vB0*-B0/!)\$(fi)"A*%$!"A+}$Z,$/!B09
5jf*-BV.0)\"-B05!>:*o
.0)#Z*7o63
(8oc9fiD=D:$<?9"%)+-(fi)#*-B0/!)#.0;!E@!#"$6+")T")(
")+X)96*)DY!++-*"XBP9
5:+!D:)T3
(=$ZW*>
)+X)?.0)**)"#+E

$"W)#fi,(:.0)!o!*%>
)+-*"XBP9
5 ^ :_Q")(
")+-)9:*A+MD:$<?9fZ$.V.0$<)D14:;"-B05!>:*EM
$!"}9
$!*AO*-B0$!9fi7.6)!+X)!o7Z)*%3
")+
kn*-BV.0)+#tO")?.K4c)#.0)D=Z'"$!N*$,r

v+-*A9fiDB09
5vZ'$!"*>
)4:.K9
=9fiD=uZ'$!"*-BV.P)vEM
"%$!*>
)1"5!3
2)96*
$ZM*>
)1(
"%)/!B0$!3fi++-)#*XBP$98o
B0*BK++-)"-BK7.V.0;dD:)#$,(fi$6+%4:.0)1Z'$!"?*%>
)Z')*3
")$!"#D:)"-B09
5]v*%>
"$!3
5!>@r
EG
!#"%$@- }")(
")+-)9:*A+Q*>
)j+-)w3
)9fi#)1$OZ,$/!)+H9
))D:)DU*%$,5!)*?*>
)1[Kv*-BV.0)1*%$,*>
)15$67.8(fi$6+mBP*XBP$9
Z'"$!B0*A+#3
"")9:*?(fi$6+mBP*XBP$9,o:<?>:BV.0)1(
"%)+-)"/BP9
52*>
)\(c$6+CB0*-B0$!9fi+Q$Z7.V.8(
")/B0$!3fi+*-BV.0)+B09fi%.03fiDB09
5=*>
)
4:.K9
E


fi
G
Y8

Ga
{{!!!fi
@fi!


Ifi

6

1

3

8

4

7

2

5

dr

6

1

8
2

(a)

4
5

2

8
7

6

(f)

rdlu

7

(b)
urdl

1

3

1

8

6
2

3
4

5

(c)

7

druuldrdlu

uldrurdllurd

3

1

4

7

5

6

2
8

3

1

4

6

5

5

(e)

2

3
4

8

7

(d)

u0!
,\{}0!:M
#0v
!:0fi=%
06%,K,-

!:7K\
N!#'!fiAO!A
-0!1%
1-fi#C0!v-

!67K

H
%!
uKu0,:0,6%!}:#-0cv0,

0!fiM-#-0!8'%
MKGOxO
:K:0
!#%!'!fi#!?:KYKJ
PfiC,0
Y%xX0!\%
\

?-

!670fi-J0?
K-


0\fiXfiff HP%U

fiA
nK-#fi#

-K--K0fi#,%
1!%
?
!AKT-A%

fi!-vfiffi-}
?#
:}!#!'#:0Gfi
J0
!
}0,:0,6#-0!,!#fi7V v:0
A}

K--0fi#-0!Yfi ,
!%
GfiN
0%
6fiN,?%P:-# xO

60%}
?-fi:

0:0
{
-T-fi%!
Ofi
!ff#"!$%?:K%@0@0{
fiX
fiK-{2L:

[ !
0
!
Qfi#0
=0!=
v-0
-0!fi'&
H
1fi:A=c6C0-0!)
(m+
*P{u0!
Y2
-:AQ
0:0-K7-AO1fi
1c6A`fi6mPXP
(' *?%
-:AT
,!:7-A%!,
,-V0c6C0-0!fiv26
vfi%@, @
2-VPY:
jcAT0

!6-#

-?:K{,!%-
,Yfi
./L
0Qfi#j%
j%!P#'-V0,fi:C0-0!@KT6
vc


!vK
0jX0vfi:C0-0!IT:
vfi2
1/cfiU-Y!8?
v#6%\-V0vfi6mPXPUKT:
vfi@

3 0!
,
%!:00
4
.fi!


}!
1
%!!Ab0:

5
'!v=!#%!'!fi#!fiO1-0!N

.fiA-v-

!6[Y[\
,0!'
,A:-0
Iv`
!
!)
6/0
.fiA-N-0A7
5!2
,:K/
5
'!c6C0-0!9
8=`0AN!67Gc6C0-0!
G
Y-
v0
@fiN
2K170!+
=!#%!'!fi#!
:);< =?>A@CB+DFE]0
G Pvfi-NfiO1!#%H 8:0
{
2fi6C0-0!I
(- *\0
J
.fi!
!nv

06K
5L!#!'!cA!M%fiMA7
5!}X0,?fi2
?:K/
521
#0J!67fic6C0-0!fiM-
xP

fiv:KN!#%!'!fi#!Nfi!N
!,70!+
fi0
G
0,fi-
ff @fi2
.fifi
xfiv


L#MHNPOQPRTSVUWCUYX[Z\RT]_^X`a-b'ZdcCe'^Zd^OCUR Ufb'XOSge'^ R O^VaRTOh\ZdXaY^^gi\Sb'^OCUWUYj^UY^ R Sgj^aWaRTUYj^aHUYj7R Olk b'mb'OknUYj^^OCUfb'aY^
X e'`Ufb'X_OJUYXpRlcaYXqCe'^ZrUYX\UYj^de'^ RTaYO^atsujCb'Sjvb'U jX_`#e'hUYj^O c7R ^b'OCUYXZ\RTSVaYXTwPXc7^VaRTUYXa WHhCb'aY^SUfe'x kTb'm_^ b'U
O^sZ\RTSVaYXTwPXc7^VaRTUYXa QX_`OhqCx ^yRTaYSjKMnzuj^x R aY^ b'ZdcCe'x UYX_aY^hqCxUYj^e'^ R aYO^Va{b'Ob'U Z|RTSaYX wfUR q#e'^_Mdzj#b
RmX b'h UYj^ ^afb}Re/c~RTa b'Ok UY^cfisujCb'SjsGX`Ce'hq7^tO^^Vh^hbQHUYj^[UY^ R Sgj^acaYX_kaR Zb Xc~RTC`^M
~

fiG/HKC/dn77/// /



HC7
F+~P\//
/P7 |?TH7[d//,d-
/fiy}7H /+/ T/

,~-,g
HC[gt7/+tT+


-C_7 KT
+/ /dH,7 -,g
G_ut K +P

TJ__7+fi
+gHfi /lH,g
?!|-,7_gGPt/gt/ /C/dg
/ K/
y} /+//7/ /C+ /\
+-,g
H\7HJ7T+//CK ,[l/C
/C+Gd\
Tfi H|VH\7HH
?H

T/ /\
+-,g
H
/d ?
J-,7_V-_?++nK ?d|- K /K -
HK/P7\ /
KY_7+

7H TH

2d+HCuF KA +P+
/
//
K_7AC7)H
TC,P
7
+[HTHK/ /++
,C
|/t/
K_7r|K / |T!+\ +/H7+{Hg/ d//

Y_7+nH
Cd
_~+/H7+l
?!T7+7nK 7H+
dH7+Ctd/T_+/7H
l7J+ldC Hg/ / /+Y
KP y//yK7
C\{77+ / K+CC\d/
+_|u
/
7T!
C_7Kt7
C
KC C, _~+/?C_Cdd/C
\H_\+C/

/
_H/7_9+CK
9+ C ~~J
9-
gP+J7pT/?79d/?7T+/9
+
C
J
+PT


T+/
/

C_7
\
C/
+?, _+/+C[nCH- G/ +P
H yC\ H7 /d
_TC
\ /CK CPCH
/H7
K H7+d|T7H?
+

C/_K2V/7_d/~|/?TH7\ /C 7
+
/K, yKP[
,n2 /
+ [/C
+
~?T+Hd / /C
TCPCH
/H

/K\+P lH ?+
/C\TH7? /7J+
/C+/_


100

80

60

Accuracy



40

20

0
0

5

10

15
20
25
Number Training Examples

30

35

40

u
/
K\G7 +/?/

/
KY
HC_
[7T+//
K_7K /C
dH7n
/
_7H/7?+C
P7
2|K?H
V+l7[ /7
|/ng
HHtH /[7 +/ / K[Pd
/
y/-g /HC7/+/H+ /{ +/
g 7-C7Hd7
/ d/C) /
+Cr K+T[



-C_7 K_F

P# /n//K,P
_/yK )
+J 9/HCCH
Pv}
) H7
/J
K 7\ /7
P g/Hdl+TJ7H H7 /?/T
+CC7//KH27
G/d 9T_+/Hd /?
7t++7 +/-CCH H7
TH
H ?T/!KC C, y//
,Pl
+/
H7+ C/H7Hl /7 +/-CG/
K/ CCH
v / -CY
7H

~

fi
fffi
!
"$#&%')(+*,
(+(+'.- /102/
35467)08693;:<->=?
@BADCEAGFIHKJMLN2HPOAGQPAROSDTAVUAGAGFSXWYAGZ[W]\QP^_S`J2W]QaS+^&AGZB\QcbedBfQc\f\gdhADijHP\jCGS+fHPkQPAldKfAGADiIkfmJMADS+QPFInMFop
q\Q
AErS+^&fIJMAN9s
\OAGFmtauDvvwexS+FSJyLIzGADdBSj{`|\+JMkHKnM\Fj}$S+HPOjs~SCPOInMFog^&ADCPOSFIn dh^SFi_dKO\+ZdHPOS+H
\QPoIS+FInMzEnyFofiHPOAVdK\JykHhny\FdB\+W$HPOAHPQaS`nMFInMFo&fQP\UIJyAG^fidBnMFSRHPQcAGAVS+FifiQPADdKHPQhn CEHhnyFofiHPOAVdKADSQaCPOfi\+W$HPOA
fQP\UIJMAG^dK\+JMTAGQHP\&HPOIndHPQPAGAny^fQP\+TADdHcOAfAGQKW]\QP^_SFCEAV\+W
HPOARfQP\UIJMAG^dK\+JMTAGQnyFmHPOAdhAGFdKAV\+W
QPADiIkCcnMFoVHcOA~FIk^lUAGQ$\+W2F\eiIADd$dhADS+QaCcOADiVZnyHcO_SOInMoOfQP\US+UInJnMHLp\+ZBAGTAGQDNs
\OAGF dQcADdKkIJMHad$iI\
F\HokS+QaSFgHPAGABS+FlAGTAGFIHPkS`JCE\FgTAGQPoAGFCEABHP\S+Fj]DD9fQP\UIJyAG^dh\+JMTAGQDN+UkH$\FIJMLlHP\S+FV\fHhny^fiS`J
fQP\UIJMAG^dK\+JMTAGQVSCcOInMAGTS+UIJMAfiUgLmQPADdKHPQKnCEHKnMFojHPOAfidKADS+QaCcOHP\_HcOAHcQPAGAl\+WdK\JykHhny\Fd\+W~HPOAlHPQES`nMFInyFo
fQP\UIJMAG^_dp~LjiIAEFInyFofiJMADS+QcFInyFoSdBfQP\iIkCcnyFojSf\+JMLIF\^lnS`JM]HKnM^&AlfQP\UIJMAG^dK\+JMTAGQSd\ff\gdhADi
HP\d)nM^&fIJMLlQPkFFInMFoVW.SdKHcAGQHPOSFVHPOAB\QKnMo+nMFS`JfQc\UIJMAG^dK\+JMTAGQ`N+Z~ABOS`TA^&\QPAdKHPQhnyFoAGFgH
CE\FinMHKnM\Fd
\FdhkCGCEADdPd)W]kIJ~JMADS+QcFInyFonyF\kQWYQaS^&AGZB\QPb2p_q\QVAErS+^&fIJMANnMFiI\^_S`nMFdJnMbAfiHPOAjnMoOIH}kzGzEJyAN
|\+JMkHKnM\F}$SHPOs~SCcOInMFoZBnJJVfQc\eiIkCEASFAErf\FAGFIHKnS`JJMLJS+QPoAmHPQPAGA\+WdK\+JMkHKnM\FdNd)nMFCEAADSCcO
dK\+JMkHKnM\FoAGFAGQES+HPADijUILHcOAV^_SCEQc\]HaS+UIJMAnddKHc\QPADiSdSfS+HPOnyFHPOARHPQPAGAp2ADS+QPFInMFodKkCcOmJ SQPoA
HPQPAGADd>ZBnJJFAGADiRAErf\FAGFIHKnS`JJMLlJS+QPoAFgk^lUAGQ$\WAErS+^&fIJMADd$S+FiRAEref9\FAGFIHKnS`JJyLRJM\FoQPkFFInMFoRHKnM^&Ap
.FQPAGHPQP\gdhfADCEHDN+HPOIndn dF\H
dKkQPfQhn dnyFoRUADCGS+kdhAdK\+JMkHKnM\FfifS+HPO&CGSCcOInMFond
SZBADS+bJMADS+QPFInMFol^&AGHPO\i
HPOS+HiI\IADdF\HSdPdKk^&AR\QAErfIJM\+nMHS+FILdKHPQckCEHPkQPAnMFHPOAlfQP\UIJMAG^dKfSCEApnMHPOAGQVS_iI\^_S`nMFOSd
dK\^AdKHPQPkCEHckQPAS+FiOAGFCEAdnyoFInCGSFgHdKf9AGADiIkffin dBSCcOInMAGTS+UIJMARUgLlAErfIJy\nyHhnyFoXnyHnMFjdK\^&ABJMADS+QcFInyFo
S`JMo\QhnyHcO^N\QnMHViI\IADdF\HOS`TASFgLdKHcQPkCEHPkQPANnMFZOIn CcOCGSdKAJyADSQPFInMFoCGS+Fm\FIJMLOS`TAJnM^lnMHPADi
UAGFAEH`pARUAEJnMAGTA&HPOS+HHPOAVQP\JyAR\+W~SlHPOAG\QPLj\+W
dKfAGADiIkfJMADS+QcFInyFofindHP\jin dhHKnMFokIndKOUAGHKZBAGAGF
HPOADdKAHKZB\CGSdKADdNeSFi&fQP\+TniIAJMADS+QPFInMFo_SJyo\QKnMHPO^_dW]\Q~CGSdhADd$nMFjZOInCcOd)nMoFInCGS+FIHdKfAGADiIkfdS+QPA
SCcOInMAGTS+UIJMApBOAT+S`Jn inMHKLj\+W$HPOIndBoAGFAGQaSJ^&AGHPO\iI\+JM\oLndS`JMQPADSiILfiU\QcFA\kHUIL_HPOAQhn CcOjU\iIL
\+WQcADdKkIJMHadRnMFCE\^&fkHaSHKnM\FS`JBJMADS+QPFInMFoHPOAG\QcLJnyHcAGQaS+HPkQPAfinMFHPOACE\FgHPAErHV\WCE\FCEAGfHlJMADS+QPFInMFop
kQS`nM^ndBHc\&HPQaS+FdWYAGQBHPOIndB^&AGHcO\eiI\+JM\oL_HP\&dKf9AGADiIkfjJMADS+QPFInMFoNniIAGFgHhnW]LjfQP\UIJMAG^<iI\^_S`nMFdBW]\Q
ZOInCcOAE ADCEHKnMTAdhfAGADiIkfjndf\gdcd)nMUIJyAN SFifiUkInJidKf9AGADiIkfJMADS+QcFInyFojS`JMo\QhnyHcO^_dW]\QHPOAG^p
kQZ~\QPbZSdV\QKnMo+nMFS`JJyLSny^ADiS+HRWY\Qc^_S`JnMzEnyFoSWY\QP^\+WrfIJS+FS+HKnM\F.BSdhADi2ADS+QcFInyFo
t~x$tSiIAGfS`JJn.N2uDvvuDSIxap ~jCE\FdKHPQPkCEHadSfQP\I\+W\+W2O\ZSfQP\UIJMAG^1nd$dK\+JMTADinyFlHPOABHPQES`nMFInyFo
AErS+^&fIJMAkd)nMFoSF;AErefIJnCcnMHmWY\Qc^\+WXiI\^fiS`nMFHcOAG\QPLNS+FiHPOAGF;oAGFAGQES`JnyzGADdS+FiHPQES+Fd)W]\QP^_d
HPOS+HfQP\I\+W
HP\_SjCE\FgHPQc\+JQPkIJMAl\QS&^_SCEQP\]\f9AGQaS+HP\Q`NZOInCPOmndKkdKHKnADimUgLmHPOAV\QhnyonyFSJ~iI\^_S`nMF
HPOAG\QcLtKjnMHaCcOAEJJ.N
AEJJyAGQ`N
ADiS+QPKsBS+UAEJJn.NuDvAD+\Fom\g\FAGLNBuDvnMFIHP\F N
u`vv
|OS`TJnMb2NuDvvIxapnybA|\JykHhny\F_}$S+HPOs~SCcOInyFo9Ng
Bnd$SZBADS+bJMADS+QPFInMFoV^AGHPO\ei2NSFinMF&oAGFAGQaS`J.N
CGS+FF\H$U9AAErfADCEHPADiHP\ny^fQP\+TAHPOABfAGQhWY\Qc^_S+FCEAp.FiIAGADi2NIHPOAQcADdKkIJMHad>nMF&HPOAdKf9AGADiIkf&JMADS+QcFInyFo
JnMHPAGQaS+HPkQcAdKkooADdKHBHPOS+H~CE\kIJifiJMADSiHP\&fQP\UIJyAG^dK\+JMTAGQEdBZOInCPOS+QPA^lkCcOj^&\QPA&DD
HPOS+FHPOA_\QKnMo+nMFS`J~fQP\UIJyAG^dK\JyTAGQadtnyFIHP\F NuDvvHPzEnM\FIn.NBuDvvIxEp\+ZBAGTAGQDN iIAGfAGFinMFo\F
HPOAdKHcQPkCEHPkQPAfi\+WHPOAjfQc\UIJMAG^dKfSCEAjkdKADi2NBS+FiHPOAjZS`LmnMFZOInCcO
B$d&iI\^_S`nMFHPOAG\QPLn
CE\iIADi_S+FiRkdKADi2NnMH$ndf\gdPdnyUIJMAW]\Q

BHP\JMADS+QPF_dKkCGCEADdcd)WYkIJJMLnyFjdK\^AdnyHckS+HKnM\FdGp$q\Q$AErS+^fIJyAN
$HczEny\FIndKO\+ZBADifiHPOS+HnyFHPOAVXGldhLedKHcAG^NB$ddKkCGCEADdcdOInyFoADd\FnMHadSUInJnMHKLHP\XFi
CE\FdKHES+FgHc.d)nMzGAVF\FQPADCEkQEd)nMTAVfQP\I\+W.dBHPOS+HdKO\+Z;HPOS+HCcO\g\gdnyFojdK\^A\fAGQaSHP\QadnMFCEAGQPHaS`nMFdhHaS+HPADd
nd
S`JMZS`Led>USifit)\QSJyZS`Ldo\g\i9x$tHPzEnM\FIn.NuDvvIxap|kCcO_CE\FdKHaSFgHP.dnyzGAfQP\g\WdQPADdhkIJyHnMF_CE\FdKHaSFgHP
d)nMzGAjCE\FgHcQP\+J
QPkIJMADdGNZOInCcOS+QPARnMFAEref9AGFd)nMTAHP\m^_S+HaCcO p&WHPOAGQPAXn dlSFInyHcAdKAGHR\+WdKkCcOCE\FIHPQP\+J
QPkIJMADd$HcOS+HCGSFQcADiIkCEAHPOABFgk^lU9AGQ$\+WdKHES+HPADd>AErefS+FiIADinyFfQP\UIJMAG^dK\JyTnMFoWYQc\^S+FVAErf\FAGFgHKnS`J
W]kFCEHKnM\F\+W$HPOAldKHaS+HPAd)nMzGAlHP\_Slf9\+JMLgF\^lnS`JW]kFCEHKnM\F N9HPOAfQP\UIJMAG^dK\+JMTnyFoCGSFjUAVokS+QaS+FIHPAGADi
HP\fiHaS+bAR\FIJMLf9\+JMLgF\^Xn SJ
HKnM^&Afit$HPzEnM\FIn.N$uDvvN}QP\f\Id)nMHKnM\FwpMupMNfopluDwIxap$HPzEnM\FIn.d\QKnMo+nMFS`J
dKLdKHPAG^|g
PsAErefIJM\+nMHPADiHcOIn dfidKHPQckCEHPkQaS`JW]ADS+HPkQcA\+WHPOAfQc\UIJMAG^dKfSCEAHc\JMADS+QPFAECcnMAGFgH
fQP\UIJMAG^dK\+JMTAGQadZBnMHPO\kHkd)nMFoSFgLAErS+^&fIJMADdltHPzEnM\FIn.N
uDvvIxapRdKkUdhADkAGFIHdKLdKHPAG^CGSJJMADi


fi2GDDB~++P
mcKDlE+&IMDPIGgh l P IyGfiP EI`MD P EMI
E&IMD
l ~ c+MD$Mfi
G EK c IK K KMfiM c_+KMfi +
IGP PXyD _ c ] E+P aR EI P cIyDfi P B c MD cIy +K E B
EM MD P _ EG) KD ] lK+MKM P $ EMI KG E+P
DK B P+MD jKE+&IMDI jPlMD Pj lK K KMy ] Pfi+KM
IM E cM)MB EMI _ PDPgRDPyX hK ] PIMDI h
P P E a) mEeI+KM PDKIMa BM _ P ]a IMDl + I++KM BhD
2D cIy G+ KDficVMD cj cMGg+ Il` ]KM&R P IMG h+ IM P
MGK h hy I_`M&E IyE~h&K P Ec PK &$K K` E&IP Iy
+I G+KM R G PMD PIM _ c ] E+P G+ fiKDPEI`M _K EDP

M;I_`MR $ EM EG_`
E)M hD
5 G`MD g G` + K` $ a)M fi + G & aI
K I` P_ IM j. KPG_ +VK EDP ]I yD Pfi Pg)M
g` aI hy RM&I cMK + G XI IM l K g K _ $ & PK EDc) E
cM) ` aXG~`
$E+Iy g $ EM g $ P GcKM I+
E P E9g)MKM GcKM & I+ jKM E c Egyhy GPKM & I+
+KMD fi+ E c E9g)MKM +K PG PDhGgaKM` K E IMD
BM j Vh K`I E&9gP IM VI_`Mfifi`MI cDK IM ]
K EDcM
MD PIM fi_ P ] a+c Ey
cG ED G BGG h $ ay l+
g _` hy _
c B P E)
KM fi l E+P aKDmM 2 &9 a+P K` $ E)M PV +
PfiRMK EE IM fiMD cIy & +IK KjEePGI
X aP aR P
`Kj RP&MaMD cIy m&G RMD PIM j& +IK g BMD l
Ea P EP & a+c IIDV _ EDPP aP
G_hE ` aG`
IK DhaB + lM EG`
MBBB EDPP _c EDPP&I c a+K E PG PDhGgPDfi a+c aP& Iy VKGDI
)M _ c ] E+P BM YDP P aI KM [_ I_`M;K K`
E&9gP IMfilK cMGIcjM l j+ P K+P EMKMXP+I m_ P ]9 a+P
&fi`M g`$ g P EDP l K YD+c Pl aI KM j
gEM &l E BM lKfi`I _ P ] aP
BM K& Mj $Ky + ) yIP
IG`
ID K 9DjG e GMD B+ $ a)M yD Pl E P E
]D+P P aI KM `M BM & _ ca IM ) ID c$cMD P& _ PRa IM
E+M& E+M& >)M jIMKMIMfiE+IyDcP+ +c& ]D+P c + E PDK9
P + G E+M& Iym I&G e B g ] g EM BM
lIP) IMc EK P E&I_y ] GEll)MDMIP P ]D+P c + E +
GDP j. G P& c + E P EKGDIMD cIy
` hy lMD Pj E P E ]D+P c aI hy + _ P ]a IM P E&IMD&
K+MKM EK c EPD P +_ P ]E IM
+IGP EKM h K`
E&9gP IM BM E+&IMDB G a` EI hPG El fi P ]a IyD~
a+IPGD 2 P$I g`KaP
+R B E+P cMgK P$P+K
DKR P9 PKMDB G a` 9 +I
B M_ 9lD)M


!"

#



-$








/.%

3

$

:



C .

,*

F

3

fi



3=







K



.%



,* :ffSF

*

(hgie 4





4

Zo

4):s



3



"

3

/.%



K fe

B*

OffSF

,

.

K

4

. 0

.

$
Q

+$

$






AE"F

2.%


Q





3



4

,

,4

%

.Q

.

,W4

3



7. % . 1

n.





4

,*

,

4





Q

4

%

W

Q

:


L(MON_P:m

,4

%

2

3

.%

BQ

)..

3 $

R.

3



4

u

*

BvQ



49$

/.



n$

).



4

.%



4)J$

/.



*



. 0



:

3





-

3

,rTU

,



3$

6.%

R.







3Q

0





..

aYE"F9ff





.

3

<E

>!"

6.%

n

$

Q

/.

/.

W$







3

W

,4

%



K



,



#





4

. 0

1

.%Q






i4

3 iffEA



4j

j. % .

.



3

>.

r6A

3

.

B4

R



I.%



/.

,j. .

3

4



3$

"!"



%

4i R

. 0

1ff6



%



% .%Q

+* 3

4

K

3





_EY

3



7$





J0

3

14



4

"]z

)..

(X

. 0

/t





4aYE"F>

3

K _e 4

K]

#



* 4

n J0

i4



/.

/.%Ql.






9.





4

B

4

,fi

GAE"F

fi$





,4

:$



Z



,*







+$

.



hgie 4

Q

:

@Q

3

j$

3





.

).

).

3

K

ff



)..

^$

"L(MON:P_m



v





.%Q|



O.







X



B



:



4

(<;<

a.

1



4



(

.

Q



44

4

K

f. .





. ,

4O$

7j.

a4 ,*

4



0

"!"

3WAE"F





j8!





Q



fi*

,

).%

GAE"Frq+.%



.%

.

3$

3n.%

:ff6}

.

4

4

..

3

3



3



74

!



#*

"

K

.

%

Bfi



HI.



(

p

gyz

}~

3

/.%

4

_

B4

0E



`

(i!"

,XTU |{f!"





)..



B*

1

,* J

4

R






4 ,*



3

(



4

%

n

3

).



lYE"F>



3

xw .

3

,4



f



$



j]








ff6

8

K

%

4

Q

.



,@T U6VffS!"

2

kg

f

$



.

W

3$

).%

@.

'



45



B

7L(MON_P

G







:]





3



).

,Q

BQ

4

.

8ffSF

3

9



3

,0

%

9. .

. 0

!2



9



.

,fi4 ,*

,5TU dc

j

R



b.

f. % .





4

,

,4

4)Z\[



/.@

).

. 0








. 0

,

4





,

#:



,

3

Q





% ,

3 fi$

3

HI.

fiff

+*

8

D$







2

>ff6 ?



:.





1

,Q 7

LXMWN:P

3



3






%

36.%7



31L(MON:P

$

1.

,*

).

3

4





&(')

,


.

/.

4



%



4G.

).



#*

B





. 0





;<

$

3



).

.

3



+$

. 0

`'

* #

4





#R



K

.%Q

4



}(=Yih@ d#A%/%h""S#_%}|h%h@n%n )#h
#6#S>#S%h63h3
, h33%"Sh`h
#>|hd%r+%%3|h#
9h@3S
#A3
#)W>3#x%W+%}B ||h#h3
h

+

fipxAG(A x7+x7 )i)b)

l%%lj5,-)I 1j ) ,,)x1 )31,@A)#/KnR#6K
,5_3fi#,,)K2% 6"/#d3)-6#( 15z3ZZ,SGY):63B R%)
A)+b 3A>% B Yv"K6fi@R )63%6WKR )^,W

l 63,(
,:R#a1,1O)fi5z`,,j%f%lI%,zR ) BB )O D/#2 -)zKZ,9z
)%) ,I-6Kfi3)))%,WK1GWR#,v3K)j% ,fi)3",@)
1 %,3,:Z/3,j:I%,z@ ) ,,G )63%%,z%@bY)K 1563Y)"3
,z 6b%GR#W))zSB,
W%%WZ("") n J
Z/%,n ),,)Y3 )63%6



@/O,3r,z%,G/#("DBvW3D3=5 Bfiy,3/
63R}

% BAK)_B) (6 ,z%) -1 )35:,3,W")3>R

,jK7<8)B _R#, 31 ,i

,j

5,)1#





% B#K67 3R3/393)9zG6# )9%bO"il ,,^)

6+B zfi1 /fifi



3,B 6`,9 ,v3)<)`,%



,



ZZBvR W%)3n ,5_%_ ) ,,))
/i)%j)3@R

,fi,3%B8/-) %/#

K/`/V6%



,fi3)

ff
fi

,)7i1+,

)3/W) 87ZZ%Z,_,83_v:3,:,3,9 3)`R3B )
7%R B
3,#1A%i63)`G3,fi/O,iROZ"B:`,Z3,i,13
%_%,
)%R3) 19fi 3):,7\W3"6#,,i,R



R 63

/
)363

1 @13j3%,R3,+,v,R#(,3,",jO,j%)3ZZB

@3fi



"G3

63 j3)



91n

r,i



3, h





3Z,3nn-`,:ZbKW63%

33, ,aG,3A63)S%R3,)"b%l3%W3v) A71%6#,,j31,1Z

5 R3) O:,",13}

fi ()

R5^_3%z%l3)8#Z-1
/)3W _ z
- Yb



:)fi-W z3
)3v
6}>v)3/ /35"
,fi ,O)3%
B
3,
:)@v,i)3,:/#A,1Z,1: ,)@")#O1)6#,7

fi@33z

1 /

/ ,)

3fi3
G)6+BV1 /:/:%)319 /9RKfi1fi%%b

3""i#/9G8z3^,G1%fi, R3,<)%

1 B

/,,)-7G3Gfi / ,)V z r1 z#"/:v/zZ) %G,
%,

!)#Z,63B

) )3>)3 3z%

BG`,Z3z5R2B%R@ )3,DR#Z

31 z:3n%6#,,)89:"6}d"j3%I ,l%):,6:b%9R#7:5j
)G6#,rW ,3,W,Bi B-) fi) %d3)7KR31)KBGj zK6
%/#=3)#,`/


"fi zK6
b%j3)fi ,fi3)#,Sb3d%WR#,fK)-bZfi)zSB,

G/1v,13
)GR _,,8fi"j B)-3
1 b}A3"

1 dR#15W)G-R3


,:,3B )n3)y,,j5)K)j16+,v1 =GGv)W)fi:)%
G,:,3,)}jO xK1,>13-,z%,iG
3i863RKbK



3):zY6 /r)3r1 ,W@`,3,)3>R ,b%13:)"3
/3D,IO )63>,1
3:,) n@36#Z zA3(%

$#

3 BO 5/#ZB





1%i
//
)KWR %,@,) R-z:)O3n p%zO2#
fi#AR#

)O33)-,)
63)%,1 KJK6(Y>j1R#ZB) %
,v63)%B


%-




/,,)}



/fi )/#Z,fi,)
%

3,R "1
B

&)363ZZ,S':/(>%Gz

WK6J/zfi_ ) BB f#Z,

1G ,:,3%("ZZ)#:jz3B W,1) _3)iDB zO) _ ) z/#") )
3,1,j63%)363ZZ,Sfi /6Yv x B-WKb O3d,@/" ,jK 1

!(=/`,3,

,1: 16K,(3)O) n %))r%
)6 %3 (
,

3,6X/%_3Y)W 3z%Br3)R )63%6#/@ ,
W)%6%z

)+*,



fi-$.0/21435.066798;:<.>=>.0?0.0@A.0B

CEDAF5FHGIFJ>DLKMNCEOD!FQPRKSTF5U2KQVW!XYUZC[J4KQ\]^_0`0DQJ>a%J>UbScJ>U%JdU2egfh_0Ub\AF5DjiQkllmbn o0DAFGIF5Ub\jJ>UZFQpqJ>arobstFhuHM0F5DAF
Je0e4STU0v%JGwStU0vdstFxDAFeb_0U2e0J>Ub\Ha%JKQDOyEDA_bsTFKQDAFJ>\AFG9GIF5o2JdDLJ>`bSsSt\IVO>Cz\AM0FGIODA\9o0DAFG{F5U|\9STUO_0Dha%JKQDAOy
\LJ>`bsTFG5]\AM0F5DAF5`bVNFQp0o2OU0F5Ub\IScJssTVgGIo F5Fe4StU0vN_0oxo0DAO`bsTF5a}GIO>sT~STU0v2WzXY\u9O_bsce`2FSTU|\AF5DFGI\ISTU0v\OHFQpobsTODF
uHJVGO>C!\ADLJ>U2GwCODaZSTU0vebO4a%JSTU\AM0F5O4DISTFG9STUJxuHJV%\M2J>\HGIF5o2J>DLJ>`bSsST\IVSGho0DAFGIF5D~FeODHKQDAFJd\AFe$W
H_0DCDLJdaF5u9ODAK5J>o0\A_0DAFG`2O\AMF5aobSTDIScK5JsJ>U2eFQpobscJ>U2Jd\ISTOU0yE`2JGIFeGIo2F5Feb_0osTFJ>DAUbSTU0vaF5\AM0y
O0e0GrStUJ"_0UbSCODaa%J>U0U0F5DW;H_0D2QjHGIV0GI\AF5aSGebFGwStv4U0FeJCE\AF5D5b5]9uHMbScKMSG
ebFGAKQDIST` Fe JGZJ>UQF5aobSTDIScK5Js9sTFJ>DAUbSTU0vGIVG{\AF5a%iIST\LKM0FQssHF5\J+s[W]HklnL]J>U2e"O_0DZa%JKQDOyE\LJ>`bsTF
sTFJ>DAU0F5DxScGZGYSTaZSsJdD\AO&ZZ]uhMbSKM"SGZebFGAKQD{St` Fe JGxJ>UQFQpobscJ>U2Jd\ISTOU0yE`2JGIFe"sTFJ>DAU0F5DQiIJSTDLe
F5\<J+s[W]0klbnQWFH~4STF5u \AM0FGIo2F5Feb_0orsTFJ>DAUbSTU0vZo0DAO`bsTF5aJGzO4U0FHO>C22U2e4STU0vJKsTO|G{FJ>o0o0DAOpbStaJ>\ISTOU
O>C!\AM0F\QJ>DAvF5\9o0DAO`bsTF5aGIO>sT~F5DhCDOaFQpqJ>arobstFGOdC\AM2Jd\<o0DO`bsTF5aGIO>sT~F5DJ>U2e\AM0FebOa%J+StUGIo2FKS2y
K5J>\ISTOU`bVFQPRKSTF5U|\IsTVGIFJdDLKAMbSTU0v\AM0FNMbV|o O\AM0FGYScGHGIo2J4KQFNO>Co0DO`bsTF5aGIOdst~4F5DLG5WF5U0F5DLJ+ssTV\AM0F5DFJ>DAF
\Iu9OSTU2e0GO>C<KQO4U2GI\ADLJSTUb\LG9O` F5VFe`|V\M0Fo0DAO`bsTF5aGIOdst~4F5DLG9STU\AM0FxM|Vbo2O\M0FGYScGNGIo2JKQFWNHU0FZ4STU2e
J>DAFN\M0FGIF5a%J>Ub\IScKNKQOU2GI\ADQJSTU|\LGuHMbScKMJ>DFO` F5VFe`bVJssebOa%JSTU2GSTU\AM0FNaF5\LJ>yebO4a%JSTUW!0ODHFQp0y
J>aobsTF]0GIF5DIScJsebFKQOaro2O|GAJd`bSsST\IVScG9GI_2KMRJZKQOU2G{\ADLJSTUb\W!9M0FHO\AM0F5D94STU2e%J>DFGIVbUb\LJKQ\IScKHKQOU2G{\ADLJSTUb\LG
OU\AM0FG{\ADA_2KQ\A_0DAFNOdC\AM0FN\QJ>DAvF5\9o0DAO`bsTF5aG{O>sT~F5DWjqOD9FQpqJ>arobstF] \AM0FKQOU2G{\ADLJSTUb\LG9\AM2J>\9\AM0FN\QJ>DAvF5\
o0DAO`bsTF5aGIO>sT~F5DScGODAv|JdUbSt5FeJGJaJKQDAOyE\LJd`bstFhOD<JG9JxGIF5\jO>C!KQOU|\DAO>sTyEDA_bsTFGu9ST\AMsTFQCE\9M2J>U2e%GwSebFG
uHMbScKMJ>DFG{F5U|\AF5Ub\IScJs!CEODAa%GOdChJv4DLJ>aa%JdDJ>DFFQpqJdaobsTFGNO>CGIVbU|\QJKQ\IScKKQOU2GI\DLJSTU|\QG5W9M0F%GIVbU0y
\LJKQ\{SKxJ>U2eG{F5a%J>Ub\IScKgKQO4U2GI\ADLJSTUb\LG9OU\AM0F\QJ>DAvF5\Ho0DO`bsTF5aGIO>sT~F5DM0FQsto"`bSJ4GH\AM0FsTFJ>DAU0F5D] STU\AM2J>\
\AM0F5VrSTao0DAO>~FST\LGHJ>`bSsST\IV\AOZvF5U0F5DLJsST5FxCDAO4aJZGIaJssU|_0aZ` F5DHO>C!\ADLJSTUbSTU0vFQp2J>aobsTFG5WhF5U0F5DLJssTV
GIo FJ>STU0v2]$\M0FgGIF5a%JdU|\IScKxKQOU2GI\ADLJ+StUb\hSGNG{\ADAOU0vF5DStU<GIV0GI\AF5a%GJ>U2e\AM0FgGIVbUb\LJKQ\IScKKQO4U2GI\ADLJSTUb\
ScGHGI\ADOU0vF5DSTUF5aobSTDIScK5JssTFJ>DAUbSTU0vGIVG{\AF5a%G5W!F5o2F5U2e4STU0vOU\AM0FGI\DA_2KQ\A_0DAFO>C\AM0FG{FN\wu9OxSTU2e0G9O>C
KQOU2GI\DLJSTU|\QG5]>\AM0F9sTFJ>DAU0F5Da%JVgJebO4o0\jJN~+JdDISTF5\wVxO>CGIFJ>DLKMGI\ADLJd\AF5v>STFGz\AO2U2eJ>UJ>o0o0DAOp|STa%Jd\ISTOU\AO
\AM0FN\LJdDAvF5\jo0DO`bsTF5aGIOdst~4F5DjSTU\AM0FNM|Vbo2O4\AM0FGYScGHGIo2JKQFWXUv4F5U0F5DLJs]2JssGIo2F5Feb_0osTFJ>DAUbSTU0v%G{VGI\F5a%G
JGAG{_0aF\AM2Jd\H\AM0FQSTDDF5o0DAFGIF5Ub\LJ>\ISTOU2Js!GI\ADA_2KQ\A_0DFGa%J4KQDAO|G5]2KQO4U|\ADO>sDA_bsTFG5]$ODHuHM2Jd\AF5~F5DHFQscGIFx
J>DAFJebF4_2J>\AFr\AOGI_2K5KSTU2KQ\IsTV&DF5o0DAFGIF5Ub\\AM0F%KQO4U|\ADO>sbU0O>u9sTFebvFU0F5FebFe\AOFQPKStF5Ub\IsTV&GIOdst~4F\AM0F
o0DAO`bsTF5a%GhSTU\M0FQStDebO4a%JSTUWXU&Je0e4ST\ISTOU\AOGIVbU|\QJKQ\IScKZJ>U2eGIF5a%J>Ub\IScKx`bSJ4GIFGuHMbScKAMDAFG{\ADIScKQ\N\AM0F
MbV|o O\AM0FGYScGGIo2J4KQFO>Co0DAO4`bstF5aGIOdst~4F5DLG5]!JstFJdDAUbSTU0v&GIV0GI\AF5aaStv4M|\ZJscGIOStU2KQO4DAo2ODQJ>\AFo0DAFQCF5DF5U2KQF
`bScJGIFG5]CO4DNFQpqJ>arobstF]zo0DAFQCEF5DgG{M0ODA\AF5DNDA_bsTFG5]ODD_bstFGxebF5DIST~FeCDAO4aGIM0O4DA\AF5DNFQp0obsJdU2J>\ISTOU2G5WScJG
GIo FKS2FG\AM0FKQOU2e4ST\ISTOU2G_0U2ebF5DjuhMbSKMrstFJdDAUbSTU0vG{_2K5KQF5Fe0G<JdU2eJscGIOo0DO+~4ScebFG!\AM0Fw_2GI\{SK5Jd\ISTOUCEOD
\AM0FsTFJ>DAUbSTU0vJsTvO4DIST\AM0aW
^o F5Feb_0osTFJ>DAUbSTU0vGIVG{\AF5a%GjG{OaF5\ISTaFG9GI_bF5D!CEDAOa}uhM2J>\jM2JG` F5F5URK5JssTFe\AM0FQ_0\ISsST\wVro0DAO`0y
sTF5a]"uHMbScKAMScG\AM0FSTU0FQPRKSTF5U2KQVO>C\M0FstFJdDAU0Feo0DAO4`bstF5aGIO>sT~F5DK5J>_2G{Fe`bV&\AM0Fo0DO>sSCEF5DLJ>\{StO4U
O>C!sTFJ>DAU0FeKQOUb\ADAOds|U0O>u9sTFebvFZuHMbScKMSG\O|OFQpo F5U2GYST~F\AOr_2GIFiwStUb\AOU]zkll4bnLWH_0DNJ>o0o0DAO|JKM
GI_0vv4FGI\LGN\M2J>\\AM0F_0\ISsST\wVo0DAO`bsTF5aK5JdU&` FRGIOdst~4Fe"StUGIOarFRK5JG{FG`bV&KQOU2G{\ADLJSTUbSTU0v\AM0F%\QJ>DAvF5\
o0DAO`bsTF5aG{O>sT~F5DGIOZ\AM2J>\ST\HOUbsTVrsTFJ>DAU2G9FQPRKSTF5Ub\HCEODAaG!O>CjKQOUb\ADAO>s$|U0O>u9sTFebvFZio0DOo2F5DIsTVrSTU2ebFQpFe
a%JKQDOyEOo2F5DQJ>\AODLGhODKQOUb\ADAOdsDA_bsTFGQnNJ>U2e_2GIFGN\M0F5aStUJKQOUb\ADAO>ssTFeCJGIMbSTOUWr^>STU2KQF%\AM0Fr_0\ISsST\IV
o0DAO`bsTF5aSG_0U2GIO>sT~+Jd`bstFSTUv4F5U0F5DLJsHiwSTU|\OU]!kllnL]2O4_0DJ>o0o0DAO|JKM&GI_0v4vFGI\LGJuHJV\AOScebF5U|\{SCEV
\AM0FK5JG{FGSTUuHMbScKMST\QZ` FgGIO>sT~FeJ>U2eo0DAFKScGIFQsTVKM2J>DLJ4KQ\AF5DIST5F\AM0F5aW
M2JdDLebOUiLkllbnFQp0\AF5U2e0GO_0Dju9ODg\O\AM0FHDFQStUbCEODLKQF5arF5U|\sTFJ>DAUbSTU0vo0DAO4`bstF5auHM0F5DF\AM0FhvO|Js
ScGx\AOsTFJ>DAUJ>UFQPRKSTF5U|\JKQ\{StO4UGI\DLJ>\AF5vV]SWFWT]!Ja%J>o0obSTU0vCDAO4aGIF5U2GIO4DAVStU0o0_0\QGZ\AOJ4KQ\ISTOU2G5]STU
JGI\OKM2JGI\IScKebOa%JSTUWffHUbsSTF\AM0F\IVbobScK5JsDFQStUbCEODLKQF5arF5U|\rsTFJ>DAUbSTU0vJ+stv4ODIST\AM0a%GruHM0F5DAFJKQ\ISTOU
GI\ADQJ>\AF5v>STFGjJdDAF9sTFJ>DAU0FerSTU2e4STDAFKQ\IsTV`bVZsTFJ>DAUbSTU0vZ~+JsT_0FCE_0U2KQ\ISTOU2G9O>~F5D<G{\LJ>\AFGzOD9GI\LJ>\F5yJKQ\ISTOUo2JSTDLG
J>FQsT`bsSTU0v2]! ST\A\Aa%JdU]O|ODF]!kllbnQ]0M0F5DAF\AM0FJ>o0o0DO|JKMScG\AOsTFJ>DU\M0F5ae4StDFKQ\IsTV`|VF5ay
obSTDIScK5JsvF5U0F5DQJsStJd\ISTOUO>C!JKQ\ISTOUGIF4_0F5U2KQFG9O`2GIF5D~FerCDOaJZbU0O>u9sTFebvFJ>`bsTFN\AFJKM0F5DW M2JdDLebOU
GIM0O>uG\AM2J>\9JKQ\ISTOUGI\DLJ>\AF5v>STFGDAF5o0DAFGIF5Ub\AFeJG9GIV0GI\AF5a%GO>Czo2J>DLJ>aF5\F5DIST5Feo0DAO0eb_2KQ\ISTOUDA_bsTFG9u9ST\AM
+

fiq0j000j4q0+q42020002

I%+20Q24TIT29>A9QRT5|{ttdA2>bTHTbQ> 9 2YT
Z
A5b ITA0 wtff

c>AA2>Nd fi IhcI0b t"AQTbQQ55|T>AbT0 |zcAT|IQT
>00A" !bT%>AZA0ZA05 #N4QIT&IAQ>A LdA05NA2>AT>A"A0Z0IT%<IL>A%$H0
T|5AIIT
Qgd2 0ZAbc>00|cZA2> &0b TA0 0AA5bgQtbELQ5r5|ZT>bt

5A000'
(fi)2AIQ+
*-,!>A ."/01 TAA>.
.29AA>2bQ
*43NdQtbT
5/bT2h02>AIc

bc>N4 btI6
9TA0bT5%7
h05AA0{L>AhcHbT2>Ict42I5 +dbt
8:9 ;=<?>@<5ABDCFE.AHG

>0bT0J9AKLNbc>0bT&b4%T2ZM2AA0%bT00N9IcO&<r05A!52
0
AMbTLhAIc05bb2A0wN{2QH2>9T2 2bP2LILb5HQdIT2!0A4c5>A 05A
>AZ%d|2T5 tM2
AbT2
2 2Q50T>AbT
A42
! >bTT&Q2
AIAR2
Q0
L

b%+t2hc'|
0SHA2xT|LQL>bT6M
S2
YT5=UV
bc>2A2>9Z24

! 52
0
0bQAtS}054
T20>TbEA%dIT.q
W 2
! >bT$
A0T>A05NZ
b tS9
|ITL!SHZ0AbT5%ANA09A05b
2>0
L00
! A52YTAN0<2dL4
d5Z25QIbT
XH0
5ITY
>
b
t.&HV:
fi 0b%5+Z5V"[z
A52ZI 50
T>AbT
r5A02>!T>A2
A0
Lw\4
bQ2bYTITb
TE9bd0btr
EA 2
! >bTHd25Z25LIbT]XH0
5IT(9
fi 0b
b52Z&*^9
fi 25S>
T>T=H[V)
bQ2bYTITb
Tg0
LY
QTbQ2bIN 4
|TbA
0
g 5>HM0

|+x>20ITZTI
QIT2N
TA0K_`acbdfe7
g 0AH
L>]
A0ZT>bt
{t0 05A9050A72
P 2Zh
505L
>ITr>{5>2 |YTI
H2
! >bT=h
S<4
5t00

A0Hr5g 5LIbT6X0
5ITH2
P 24T
NbIjI2j
P
505Q\
dIT%d{5j>$q
! >rbtc=b
0SH
A2lk,=E
2>Lm(
S2
Awt5"JHV 0nXH0
5ITS
|AYTbT&l2
P 2ffD
505L+
>{t4
T2QA55bLT
|c
5{E4tch05A05TA5LT 0Q424t{t4 cAQT>
>|A0Rb

xbd
! 52Abc9
AA+\E
{trb4%T27h05AgQ{t42>02b5A5ZTb{Ic>2
bc>0bT
>20
! 0
IT>AhtbA5IT"

I0S<"A2>r0<4c%d0bc5>bTA0M0254cIffI5AITtpHbcAff0%>

A0590AScbH{> 0IT29AZ0AbT5%q(rs> *ut|405v"[0xw2" O.yUV|59Q
A0
02M 025 4cII5IT
H05AxA0I> 0IT2>A4505L>A|I>QA0AHL>zM1TL
5Y H[0{tTbA.
y0 b52+Y O|UVvk>LdLS }A>N
VUQS 904THQ 0L{j25M 955

A0IM <9 !0AA5~ 0A05$cM 0NM 00bTAhA0zT>A05!I5>Q>2bZT0|I5
!05LcI ff]2IQ b<M 000A4bt5NA2>0QT&I> ZA0Z0A4bt5NA2>2> 0Qt05 0TA0
b%+t.
bc!c5
5Ar
2YTZdHA0 !05LcI0Z |5
P22>j052>xA !0j |
bT
69TA.
$A 2Ibg >cT| L>IT4j 5A5bIc XH2>IT2~ 0NT>A05hcH XbTAA
Q
5 xT2>Tb0Zc<{tr%>2
9TA"2>Tb0Zc<
0Z25x>< !05LcI k>Q>LS }A>
VU
0AS 92>HA0ZQ24TIT2Q bRT5|E9T>AbT
rI> 0AbT5%N<tAbcH2d25N>A
bRT5bHET>AbT
EA !5Q{N5
<QY
$H09>$A0HA2+T5
>v
02Q 025 4cI%{25
0rtdAbT
NczA0
bTITbthT%S 90A4bt5K

LM 025R cII 5
0T>bt
x92M 2TAQ 0NA2>H0NA05 #HId\ 0{t42>jQ



YcIA5b)
9TAZYT
>T0A4bt5I> 59TA0|b24A0YcHI2Q 0NAI4%A2d<b<hQ 2c
cNA2dN 5A0bT5T0%b%T2" r
bTITbTI> 0IT2
[tS V Wq !q>rbt
A05A>A7
2M 2+T>|ZAH
0A!A
4AN0 #'!>RQA4 0 b>2ZA05H%Z2H%d|
H
>)
P!bt
bcQT LA0Rd2I52Q>A4A05
H052M 0AZA2dg<0RId\ 0{t42Z>HA0
!2>bT>%Q2YcIA5b
9TA&wt
dt0AbT5I> 5" $A0ZT>A05x2NAbcb05Ax
L>R 50bT5I> 5gQ42YcIA5b
9TA
> 5&I5N><{> 0IT2NN04 L"A05
<4L0
2AIQTQI> 0IT2x>H0A4bt5TQ 2A6
HA2dg>TIZ0rL> 5N0bT5 {> 5
{

fiOH)-+SS

" 7hV" VR.M xMH%QS K l
MffS \H= ] lY@J" j 6& J= H6MR"
MR=Sff\cMM"ff@j M)7{ SNR]"VS l
"j S+KN SRj+SjfSfJV@\) lS6RS7RSqQS
HMxH M{Z&7j 6'\ R xMRSJ{\ RVS
SM KHxVS7S)OH S+RMK"6
SD pV"7 FH" . SM "qHMV" Sff"H\
MS h7R S7MhJ6R\S5hSMSM ]Hj
5YS))"7KJM@\Q\HH"j\M ]HS" =YKSH"

j J )MOvRj :@]HV"j SM @?S \hHj
KM MS KH\K7j NH 6M "QS h=S
S.:"xq@^SHM.&{7@qS.+hRKM@\
UMS DMHS & lMMzM@ RVS:j
5)R\SS 7@fj ff\6RJMHM]Q ffV@ )ffH
qH7lSH MS @S6QS ]S:& (S
jSN \ @R DMRMpMD HSff\
K SS" "M VM "M@\Q\H]" \D ~7ScMRV
DMR MS ffHK)"cH6RMjvR @J]Q\Q\
FQS 5S j@M SR7S7" 7"U?V"j )-j 6M@\Q\H)"
"HMjvMS H"j MY"j l QUMR
S)@7RVSK\S6M {.ZHM .
=.5Y5{.f
l5M7j "VS )MM SR\xf7=)RJV"OH
NS=) H]=@\" SR\5(x7& )h '
7MV@SVQ&Svxf JRq? @M S)5 S==H
6" YMxRv@M c D'S7"SSV~O7JQlM" l
x7MHSR FMS V~ Sff\h"M)J@\+\6H7OVM
SHHQM SQ\H S= ?7 xVSHV.HfRS 5S
)ff OS]QO R\@q"@(\HMSM@\Q\H
7)RR K RVSff@+Sh7HJ\R@+
j. %SSM HM Oxc xS M"&\@ ]SRM@
H
llMS)p{zRnRS KS )SxlMcM xl " Q\RS
SlHj.'HhS+S=S) .Ql@ SKHxVSf
RUR lVS )RD6Mx S6SR6M H jS
MM~f)6x]l"]Q{hc6M j@M (c@'"jM6 RUpYp
)7SNUff5)JRJ:S " f \RMR)MVSM 5S
" zMS H\xqS RFVRMS ]%\ffSJ{
SfMS SJ"Mff+HVM


=f]
x ff

)KVM]@ff6QRpl@M "R\cHSM ]Sx
( YY fiS6R)6@+"{{)MSV@xq fiSfi fi5l



fi"!#"$% "#"!&'(")+*+, - & &.0/"-210& #""3"45306

7980:;"<>=@? ACBDAE? 798 F GIHJ: K9: LNM80:O5:K9:;QPF = ?ASRTPVUW7X79UWHYP[Z98 F]\^HXUW;`_a79bJPc? ;QPF deP[ZX<gf@O[:;"; F]h HYP[LQ80:H
:80: LQUWj:; Fh 79UWj U Pc;Q79? ; F^=@? Pc7kZX8"UJOlOFnme:H9;"UWopG]UJOqOF^BH9Ar:;0LsGIHYPcULPc7YP[KWF:;0LtM80:u;0LQHk:
v UL"LQoxwy? H%Ar:;QozPc;7XUWH9UKY7YPc;"{pLP[K9ZJ|0K9K}Pc? ;0K%?;~7X8QP5KE79? QP[Z=e80:;"<"KE79?+sPqOlOqP[:A M? 8"UW; F v ? ;QP
80:HkLQ?; Fh 79UWj U Pc;Q79? ; F^:;0Lx798"U>H9UWjPcUWeUWHkK?uw(798QP[K%0:UWHw? H7X8"UJPVH7X8"? H9? |"{ 8p:u;0L8"UJOcQwy|QO
ZJ? AAEUW;7kKF:u;0L79?GI: LQAr:u:B<<:Hk:u|Ewy?H^8"UWHDZW:H9UJw|QO"H9?Q?wH9U: LPc;"{0
2"900"
B;"{Oc|QPV; F Rk Qka^|"UWHYPcUKn:;0LZJ? ;0ZJUW"7TOcU:H9;QPc;"{0Tg [J J0F" kF0""
B;798"?;o F cF0maPc{ {KFdk Qk TQ 0 @ kk[Q2 J JM:A"HYP[LQ{ UT;QPcj UWHkK}Pc7Yo
GIH9UK9KFdTUW
I? HX<F0dT
maOc|"AUWHF]B5F _I8"H9UW;QwUW|0Z98Q7F@BcF :|0K9KOVUWHFR5F z:H9A|"798 F ]k QkU:uH9;0:QPqOqPV7Yo:u;0L2798"U
:";QPc<QYM8"UWH9j ?;"UW;"< P[KnLPcAUW;0K}Pc? ; TJ0 ]YnagF"kF0 "
mo O[:;0LQUWHF=n@k QkM? AEQOVUJQPc7Yo>H9UKY|QOc7kKTw? HnKYUWHYP[:OLQUZJ? A?K9:QPqOqPc7o ;zekJJ [QnY(u
[ IWWYkn nJ klq 0JF" @ 0h:;>?KYU FMBBDBBDeGIH9UKXKW
M80:O[: KX:;QPFG]cF_I7XbJPV?;QPFT\cF ? |";Q7FI"k "uk>RUW79UZJ7YPc;"{:u;0L+UJ"QOc?Pc7YPc;"{LQUZJ? AE0?K9:uQPlOc
PV7Yo2Pc;z|"L":7XU{ Hk:"80KW;
2k0k 0 e WWYk% zek0qrY elJ
Y0 N 0 JW 0F " "M:A"HYP[LQ{ U F B
M? 8"UW; FQk QkDK}Pc;"{rLP[KY7XHYPc"|"7YPc? ;"wyHXUWUnOcU:H9;QPc;"{798"UW?H9o79?:;0:OcobWUK?Oc|"7YPc? ;0:798ZW: ZX8QPV;"{
AUZ980:u;QP5KArKWn^ J0qlkF0kF" "
RU? ;"{F cF0 ?Q? ;"UWo F v J QJ_]"QO[:;0:7YPc? ;20: KYULOcU:H9;QPc;"{0eB;+:Oc79UWH9;0:7PVjUjPVUWngu
W0J J0FF "
_:uHYOcUWo F"]kQQkB;+UJZXPcUW;Q7ZJ? ;Q79UJ"79wyHXUWU0:uHkK}Pc;"{>:Oc{ ? HYPc798"ANc> Q0J ["IgF
YQkF" "
_I7XbJPV?;QPF\ J QJB~KY79HX|0ZJ79|"Hk:O7X8"UW? H9on?w0UJ"QO[:;0:7YPc? ;"0: KULOcU:H9;QPc;"{00nJ k0[qq 0JF
JkF" "
(:uH9UWo F 5Fu? 8";0KY? ; F0RkQQkT TQX 0Ek[quC^Q[ N Qk JY
Dk Tq0kW "f"H9UWUWA%:;
DHJ:7kZX8 F"cFpRU?;"{0FQJ QJIu"]B"H9? 0:QPqOqP[KY7YP[Z^KY?uOV|"7PV?;7X?^798"U|"7PlOqPc7Yo"H9?QOVUWA
PV;>K0UWULQ|""OcU:H9;QPc;"{0e};+YkJk[Q^Ynn [0 WWYk nJ e k0[qq 0JF
" 0 ""h:;>?KYU FMBBDBBD^GaH9UK9KW
DHXUJPV;"UWHF v Qk "uk0f@Pc;0LPc;"{7X8"U^? "7YPcAr:OLQUWHPVj:7YPc? ;>KY7XHk:79UW{ oPc;:nH9ULQ|";0L":u;7<;"?eOcULQ{ U^0:KYU
nk [0J0qlkF0 JkF0 "
DHXUJPV;"UWHF v cF t|"HP5KP5ZW:FJ"k QkB~KY7J:7YP[KY7YP[ZW:O:""HX?: ZX87X?(KY?uOVjPc;"{798"U_me%|"7YPqOqPV7Yo"HX? QOcUWA>
;eYJkJ ]Yn 0 Y0J(k [ uk0[qq kFJ" 0J""h:;?QKYU F
MBQBBDB^GIH9UKXKW


fi"0W ff
fi" 9

!#"$%'&)(*%+,-!.0/1+2+143065,)78+9)8+0:<;=>?@4&)@:ff';BA:ffC@DC&4A(E6FG9AHIJ):ffK
LM8+IN:AO:ff):AK(PQ(8ffLRTS"!U5PWVYX=Z[\0\]+^`_'a<bcZ*de0_$fg\X_)h+fg^`Z+_$h+ijZ+^`_)f![Z+_d\X=\_$[0\kZ+_mlnX0fg^ oY[^Qhi
e0_)fg\iQip^pa+\_$[0\ff99Urqs+s<t)qs'14uk;CC8ff;wvx5vx8C>':ffzyn:ff&4LgIN:ffU
&9;{):Y|B!}|k:ff&UYuJT.0/1+1+~430;C{x78+I94A4;K8<L4A87C%(J8+=AQ@94AQ:ff4>)lnX0fg^ oY[^Qhi
e0_)fg\iQip^pa+\_$[0\)+.~ff,430~+~t~+ffs$
:ff&)(C(PA)uJ$.0/1+2+1430r"r:ff4>z78+ff=&)7;=+78+)79;0(Yx(*;CC&)7;C&0:A@48IN:)(T?h+['^`_)\nU\h+X_
^Q_4a+rqt4s'
8+9O7C8<LM;n-ApAIN:ffUB-n.0/14q1430e0_)fgX=Z]'[fg^QZ+_fgZl'fgZ+Jh+fgh U4\0Z+XUh+_'a'ha+\bNh+_$]
Z+O4fgh+fg^QZ_)!Fn@@Q(=8+g(PAK+
yn:ffA4Ap>)"!+"O;;CIN:ffU+vff"!+v8'8++FJ.0/1+1430+4Lg8+07I4;!A:ffC4>)EF(=&+K+
jZ+4X_)h+iUZ*dlnX0fg^ oY[^Qhire0_)fg\iQip^pa+\_)[\\b\0h+X=[')~+4qt~+2+
yk:ffC)(v-:ff0:ff4,'BJff.0/11ffs30l_e_)fgX=Z]+4[fg^`Z+_JfgZJZ+O4fgh+fg^`Z+_)hi4\h+X_)^`_'aU'\Z+X0ff
{v50 C(C(Y:ffIJ=Q@4>++vF
yk{):ff0@48Un.0/1+1+30"r:ff4>W;C8z;:ff%+:+7;=8+)(J5PVYX=Z[0\\0]^Q_4affbZ=dnh+fg^`Z+_)hiZ+_d\X=\_)[\Z_
lnX0fg^ oY[^Qhi)e_)fg\i`ip^Ha\_$[0\+8+C;=AQ:ff)@rr
yk8+=LCrn.0/1+2430v?:7C8+g8+9$0:ff;C8+(EY:6:ff%zI;C{8@mLg8+A:ffC4>)lnXfg^ oY[^`h+ire_)fg\i`ip^Ha\_$[0\O4ff
+t)q+q4
"U:0@r4-+84(='4A848+Ix++|YApA,+FJ.0/1+2+30{4&%>kN(*8':ffE {:ff):<;C8+IJK8<LO:>+:A
A:<C4>NI7{):ff4Q(=Ixc?h+['^Q_$\B\h+X_)^`_'aOffU//t's'
"$;C;=A(=;8++|B.0/1+22430":ffC4>?&4Q7C%AK{CCA:ff4;:<;C;C=&;C(n:ff$8+&)@rEkFAH:ff
;C{C(={8ffAQ@:A>+8+=;C{IxTWh+['^`_)\BU\h+X0_$^Q_4a+$+)~+2+t/2
"$;C;CIm:ffU)v"!$Y:+(C(:ff)@40:4FJyn:ffA4AH>)U"))./1+1+43"r:ff4>69)8ffApQ7(9):ffC;=Q:ApAK
8+)(=Cff:ff4Ax4+C8+I6';0(Ex'7:AH>&9U5,VX=Z[\0\]+^`_'affbmZ=df`4\xe_)fg\X_)hfg^QZ+_$h+i!Wh+[4^Q_$\
\h+X_)^`_'amZ_d\X=\_$[0\+99U)+~t4q'v8+C>':<yn:ff&4LgIN:ffU
v';C8U4)4.0/1+1+30&):ff4;=;0:ff;*Y(=&4A;0(78+)74>B;C{&;=pAp;=KJ8ffLr94AQ:ff):ff;*8g):+(=@BA:ffC4>)
lnX0fg^ oY[^Qhi)e_)fg\i`ip^Ha\_$[0\+U.=~ff,430)+t+1/+
v;07C{ApA, UykApAnUyk@:ff=Y:ff)ApAp,).0/1+2+30R94AQ:ff):ff;*8?):(=@W>+0:AH:ff;*8UEF
&4HLgK>mBWh+['^`_)\BU\h+X0_$^Q_4a+rffsqt2+
v;07C{ApA, ';C>+8ff'4S:ff,,n$.0/1+2430r"r:ffC4>4KJ9$=I';:ff;=8+UE!Fn7+&4=>z:ff)@
C)4>9C8+4AI(=8<A>W{&=Q(=;=Q7(5Pv7{):AQ(=%,Yn:ffC)8AHA,-v;07C{ApA,
.R@(30$?h+['^Q_$\nU\h+X0_$^Q_4a+99UU/+tr/1+ 8+>':)!:A86FA;C8)F
|k:ff;0:ff0:<*:<USk./1+24q+3A:ffC4>ST848ffA:ffLg&)7;=8+)(5,VYX=Z[\\0]+^`_'a<bZ=dxf`'\/1<QWlJ!
OffZffb^Q4#Z+_U4\0Z+XZ=dTZ+O'fg^`_'a99Ur~+1+tffs)ffFnYvC(C(
|k:ff;0:ff0:<*:<U'Sk$.0/1+2+130!mA:ffC4>mLgC8+I07Q(=(!5PVYX=Z[0\\0]^Q_4affbZ=df`'\~<l_)_)4h+izZ+X
b0'ZZ+_Zr'fgh+fg^`Z+_$h+iUU\h+X_)^`_'aU'\Z+Xff99Urq~t24q'$':ff4;0:6C&+rFJ


fimU+++)W)+)

ff
fi


!#""$' $%'+& )'( ff+n* -,. N/ ffU'0 ff
% 213& 5476
kff0ff0<*<U+98;:<13=)?>@>@ACB$0DD0:#E#&513EF.g0-/+13E#&+2G5H. &+7>I1ffCA5()J,LK#

-NOP

RQff
? ff!$3 ff
Rfi
TSU3P3

?ff
=5U= WVXY?Z$V [ k6 \k6 2]+& ff%'0J
B1213+^ % CJ6 C8`!_ )^Aa& 7b 0+4Y 0Rc 'd )-/eAgf-hiG
6 513
E )&->I1j.k& nC0?A 5(\= 2&]>a13/TH lT_ #!m
,J nK#o?

M-NpPq
rQ
ff ff!UL3 ff
rfi
LSUPs
?o
?ff
5=5U
= tuv3Z9tv-Y ff6 ]5w
1 <131 UC0f+& Q> ffffr
< C%'+& 2'( <xn* -,. N/ ffU
yff51 5Jc 8{z ff2/j, 2U|
% 0D4u 0C}~A ff<A 5m( H|5+& 21=H H&->I, AI+& T.k& 2|51p`+1 21 ffHoAa& q&-.
2|5z
1 [-w =5,5^3^>I1+AHA 4C0f 0]>a+1 jPJ nK?

M-NpPq
r !#SUPs
?
ff
jLp ?!
Qff
?!!@@

-=5U= UuDOZ -vY B|A@Q> <1>I=5|QA BJ6 ff%'+& 2'( ffn* -,. N/ ffU
y#1<5<+d Ck4 : <13)= ?>@>@,A $B 8y#& fff -(->@AI&->I)& C)0 $0+4u 0ff:#|513& 2dw (,Ag<1<T13/r=A Agf >H=ff131<,5=T>I1 ff5w
5(R&-.('
& ?>ff<1f&/+=&HA AI+& m2,>I1H C,J 'K?

M-NP
Q
ff ff!4
fi
ff
SU3P3

?ff
+ff,A ,J 0?>I+d
y7AI1=H ffny 0Dv 0mC1 <CA 5(x<1f)AHAI+& '>@A=H H L3
fi
ff + U 4t 05Y+Y ?Z5Y-Vu
y#,ffH2H1>,> R)0 #8!B ffy 0+4[ 0F6 =5= 2&?AIN/ ffA 5(&= AIN/ >=&->@Agf)AI1Hx.k+& T)= ffCA >>IdF&]ffH1 2ff -]>I1
=H 2&5f2)
| *H AfO<&N/ ?A ffH CPJ \K#

-N#P
iQff
? ff!5 ff7SUPs
?o
?ff
+p ?!
Qff
?!!@@

-=5U= UXDD?OZ +X )V %'+& '21 ?,> C4UJ6
0)| ?>@AIrG 9 04X 06pf3,A 5J( 21f, HAIn1 ffff<RA 21 ffAI1f+& fff13= HE#A 2|+15=Q> ff)ffAI+& 5w )] H1<j>I1 ff5w
5$
( ff
Rfi
ff $ff +t ?ZffvX
0,5] 0-m/ ffQA ffU$Jc ff8, ')1 C)0 0+Y 0%\1 H, 5(q, A>@A ?d ffff< 2|51<1HoAa( &.= 2&-ff -]>Id(&&5<
_ #

?>I(& 2|5/qH jPJ K#o?

M-NP+
TQff
ff !3 ff
rfi
ff 'SU3P3



=5U= 5VYu?ZVt[ 6]1 <131 U0f& Q> ffffr< $%'+& )'( ffn* -,.km/ ffU
:<13)= ?>@>@A #B T0+40T6. +& 2N/ ?>@AI^ ffAI+& &-.O15=> <)ffAI+& 5w )] H1<N/ f 2&w &=ff1 0<2+& >I1 ffCA 5)( LPJ
K

NPjQ
ff !C5O3P3


j'p ?!Q
! !@
ff
=5U= u ?u?Z9uYY
0d94< 513+d 6,ff*H C0?>@QA ff%'+& 24( ffxn* ,.kN/ <U
:<13)= ?>@>@A B 4+3r] 9m1 ffCA 5(jE#A 2|jA ffH2f 2, 0]>ak1 2|513+& Aa1H ffPJ K

NP
Ofi
5
)NO" =5U= ff[-VV-Z9[-VD 54U|Af -($& 5Jo!m C%'+& 2'( <xn* -,. N/ ffU
:<13)= ?>@>@A B 4+4Y
6 2|513& 2d&-.C, ffH,5=ff1 )AH1<H=131<,5=+>I1 ffA 5$( C,J 'K

NUPOp ff!
SU3P3

?ff
pp ?!ffQff
! !@

-=5U= $YY ?Z9Yt-$V 5'0 ff\-&H+1 C4UJ6 56 66 JO!B 21H2H
:-/j]ff+1 5
% '13E#1>,> 5J6 58;y#&H1 ]>I&&/ ffB ff)0 $0+4X 0C:#|51 = 2&]>I13/`&-.19=1 ffHAI1f2|, 5G5H ffff<
HOH&->I
, AI+& ']md 21=H CAgf 5(T15= 21H2HAI1 51H2H L3 ff
Rfi
+ ff o4t 0+Y +?Z9tVD
?>@QA ff4ff!m rD-V C
6 2|513& 2d+&-!. 2|51 >I1 ffC)-]>I+1 pS7jff 5NpP
pj!S Wffff 0+ffOt-V-Z
+3V

kff0ff0<*<Uk+ffr
?< )



fiJournal Artificial Intelligence Research 4 (1996) 77-90

Submitted 10/95; published 3/96

Improved Use Continuous Attributes C4.5
J. R. Quinlan

Basser Department Computer Science
University Sydney, Sydney Australia 2006

quinlan@cs.su.oz.au

Abstract

reported weakness C4.5 domains continuous attributes addressed
modifying formation evaluation tests continuous attributes. MDL-inspired
penalty applied tests, eliminating consideration altering
relative desirability tests. Empirical trials show modifications lead
smaller decision trees higher predictive accuracies. Results also confirm new
version C4.5 incorporating changes superior recent approaches use global
discretization construct small trees multi-interval splits.

1. Introduction
empirical learning systems given set pre-classified cases, described
vector attribute values, construct mapping attribute values
classes. attributes used describe cases grouped continuous attributes,
whose values numeric, discrete attributes unordered nominal values.
example, description person might include weight kilograms, value
73.5, color eyes whose value one `brown', `blue', etc.
C4.5 (Quinlan, 1993) one system learns decision-tree classifiers. Several
authors recently noted C4.5's performance weaker domains preponderance continuous attributes learning tasks mainly discrete attributes.
example, Auer, Holte, Maass (1995) describe T2, system searches good
two-level decision trees, comment:
\The accuracy T2's trees rivalled surpassed C4.5's 8 [15] datasets,
including one datasets continuous attributes."
Discussing effect replacing continuous attributes discrete attributes, whose
values corresponds interval continuous attribute, Dougherty, Kohavi, Sahami (1995) write:
\C4.5's performance significantly improved two datasets : : : using
entropy discretization method significantly degrade dataset.
: : : conjecture C4.5 induction algorithm taking full advantage
possible local discretization."
paper explores new version C4.5 changes relative desirability using
continuous attributes. Section 2 sketches current system, following section
describes modifications. Results comprehensive set trials, reported Section 4,
show modifications lead trees smaller accurate. Section 5

c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiQuinlan

compares performance new version results obtained two alternative
methods exploiting continuous attributes quoted above.

2. Constructing Decision Trees
C4.5 uses divide-and-conquer approach growing decision trees pioneered
Hunt co-workers (Hunt, Marin, & Stone, 1966). brief description
method given here; see Quinlan (1993) complete treatment.
following algorithm generates decision tree set cases:

satisfies stopping criterion, tree leaf associated

frequent class D. One reason stopping contains cases
class, criteria also formulated (see below).

test mutually exclusive outcomes T1; T2 ; : : : ; Tk used partition

subsets D1 ; D2 ; : : : ; Dk , Di contains cases outcome Ti .
tree test root one subtree outcome Ti
constructed applying procedure recursively cases Di .

Provided cases identical attribute values belong different
classes, test produces non-trivial partition eventually lead singleclass subsets above. However, expectation smaller trees preferable (being
easier understand often accurate predictors), family possible tests examined one chosen maximize value splitting criterion. default
tests considered C4.5 are:

A=? discrete attribute A, one outcome value A.
continuous attribute A, two outcomes, true false. find

threshold maximizes splitting criterion, cases sorted
values attribute give ordered distinct values v1 ,v2 ,: : : ,vN . Every pair adjacent values suggests potential threshold = (vi + vi+1 )=2 corresponding
partition D.1 threshold yields best value splitting criterion
selected.

default splitting criterion used C4.5 gain ratio, information-based measure
takes account different numbers (and different probabilities) test outcomes. Let
C denote number classes p(D; j ) proportion cases belong
j th class. residual uncertainty class case belongs
expressed
C
X
Info(D) = , p(D; j ) log2 (p(D; j ))
j=1

1. Fayyad Irani (1992) prove that, convex splitting criteria information gain,
necessary examine thresholds. cases value vi adjacent value vi+1 belong
class, threshold cannot lead partition maximum value
criterion.

78

fiImproved Use Continuous Attributes C4.5

corresponding information gained test k outcomes
Gain(D; ) = Info (D) ,

k jD j
X
Info (D ) :

i=1

jDj

information gained test strongly affected number outcomes
maximal one case subset Di . hand, potential
information obtained partitioning set cases based knowing subset Di
case falls; split information

k jD j
X
log jDi j
Split(D; ) = ,
2 jD j
jDj
i=1

tends increase number outcomes test. gain ratio criterion assesses
desirability test ratio information gain split information.
gain ratio every possible test determined and, among least average gain,
split maximum gain ratio selected.
situations, every possible test splits subsets class
distribution. tests zero gain, C4.5 uses additional stopping
criterion.
recursive partitioning strategy results trees consistent
training data, possible. practical applications data often noisy { attribute
values incorrectly recorded cases misclassified. Noise leads overly complex
trees attempt account anomalies. systems prune initial tree,
identifying subtrees contribute little predictive accuracy replacing
leaf.

3. Modified Assessment Continuous Attributes

return selection threshold continuous attribute A.
N distinct values set cases D, N , 1 thresholds could
used test A. threshold gives unique subsets D1 D2 value
splitting criterion function threshold. ability choose threshold
maximize value gives continuous attribute advantage discrete
attribute (which similar parameter adjusts partition D), also
continuous attributes fewer distinct values D. is, choice

test biased towards continuous attributes numerous distinct values.
paper proposes correction bias consists two modifications C4.5.
first these, inspired Minimum Description Length principle (Rissanen, 1983),
adjusts apparent information gain test continuous attribute. Discussion
change prefaced brief introduction MDL.
Following Quinlan Rivest (1989), let sender receiver possess ordered
list cases training data showing case's attribute values. sender also
knows class case belongs must transmit information
receiver. first encodes sends theory classify cases. Since
theory might imperfect, sender must also identify exceptions theory
79

fiQuinlan

occur training cases state classes predicted theory
corrected. total length transmission thus number bits required
encode theory (the theory cost) plus bits needed identify correct
exceptions (the exceptions cost). sender may choice among several alternative
theories, simple leaving many errors corrected others
elaborate accurate. MDL principle may stated as: Choose theory
minimizes sum theory exceptions costs.
MDL thus provides framework trading complexity theory
accuracy training data D. exceptions cost associated set cases
asymptotically equivalent jDj Info (D), jDj Gain (D; ) measures reduction
exceptions cost partitioned test . Partitioning way, however,
requires transmission complex theory includes definition . Whereas
test A=? discrete attribute specified nominating attribute involved,
test must also include threshold t; N , 1 possible thresholds A,
take additional log2 (N , 1) bits.2 first modification \charge" increased
cost associated test continuous attribute apparent gain achieved
test, reducing (per-case) information Gain (D; ) log2 (N , 1)/jDj.
test continuous attribute numerous distinct values less likely
maximum value splitting criterion among family possible tests,
less likely selected. Further, thresholds continuous attribute
adjusted gain less zero, attribute effectively ruled out.
consequences first change thus re-ranking potential tests possible
exclusion them.
second change much straightforward. Recall gain ratio criterion
divides apparent gain information available split. latter varies
function threshold maximal many cases
below. gain ratio criterion used select t, effect penalty described
also vary t, least impact divides cases equally. seems
unnecessary complication, threshold chosen instead maximize gain.
threshold chosen, however, final selection attribute used
test still made basis gain ratio criterion using adjusted gain.

4. Empirical Evaluation
effects changes assessed empirically series \before after"
experiments substantial number learning tasks. Release 7 C4.5 (abbreviated
Rel 7) modified produce new version (Rel 8). systems
applied twenty databases UCI Repository involve continuous attributes,
either alone combination discrete attributes. summary characteristics
data sets appears Appendix A. following experiments, versions
2. Even convex splitting criterion satisfies requirements Fayyad Irani (1992),
cannot use number N potentially gain-maximizing thresholds instead greater number N
possible thresholds. Since receiver knows cases' attribute values classes,
cannot determine whether cases two adjacent values belong class.
message must consequently identify chosen threshold among possible thresholds.
0

80

fiImproved Use Continuous Attributes C4.5

Table 1: Results using modified (Rel 8 ) previous (Rel 7 ) C4.5.
anneal
auto
breast-w
colic
credit-a
credit-g
diabetes
glass
heart-c
heart-h
hepatitis
hypo
iris
labor
letter
segment
sick
sonar
vehicle
waveform

average

Rel 8
7.67 .12
17.7 .5
5.26 .19
15.0 .2
14.7 .2
28.4 .3
25.4 .3
32.5 .8
23.0 .5
21.5 .2
20.4 .6
.48 .01
4.80 .17
19.1 1.0
12.0 .0
3.21 .08
1.34 .03
25.6 .7
27.1 .4
27.3 .3

Error Rate
Tree Size
Rel 7
w/d/l ratio Rel 8
Rel 7
7.49 .16 3/2/5 1.02 75.2 .7 70.1 1.1
23.8 .6 10/0/0 .74 63.7 .4 62.9 .5
5.29 .09 5/1/4 .99 25.0 .5 20.3 .5
15.1 .4
5/2/3 .99 9.7 .2 20.0 .5
15.8 .3
7/1/2 .93 33.2 1.1 57.3 1.2
28.9 .3
5/1/4 .98 124 2
155 2
28.3 .3 10/0/0 .90 44.0 1.6 128.2 1.8
32.1 .5
4/1/5 1.01 45.7 .4 51.3 .4
24.9 .4
8/0/2 .92 39.9 .4 45.3 .3
21.6 .5
4/0/6 1.00 19.1 .6 29.7 1.2
21.7 .8
6/1/3 .94 17.8 .3 15.5 .4
.49 .02 6/3/1 .97 27.5 .1 25.3 .1
4.87 .20 3/3/4 .99 8.5 .0
9.3 .1
16.7 .9
1/2/7 1.15 7.0 .3
7.3 .1
12.2 .0 10/0/0 .98 2328 4 2370 4
3.77 .07 9/1/0 .85 82.9 .5 83.5 .6
1.29 .03 2/1/7 1.04 50.8 .5 51.5 .5
28.4 .6
8/0/2 .90 28.4 .2 33.1 .5
29.1 .3 10/0/0 .93 135 2
181 1
28.1 .6
6/2/2 .97 44.6 .4 49.2 .4

.96

ratio
1.07
1.01
1.23
.49
.58
.80
.34
.89
.88
.64
1.15
1.09
.91
.96
.98
.99
.99
.86
.75
.91

.88

C4.5 run default settings parameters; attempt made
tune either system tasks.

4.1 Initial experiments

Table 1 displays results first trials, consisting ten complete ten-fold crossvalidations3 task. figure shown system mean ten
cross-validation results error rates tree sizes refer C4.5's pruned trees;
standard error mean appears small font. column headed `w/d/l' shows
number complete cross-validations Rel 8 gives lower error rate, error
rate, higher error rate Rel 7. figures `ratio' present results Rel 8
divided corresponding figure Rel 7.
overall averages foot table indicate, trees produced Rel 8
trials 4% accurate 12% smaller generated Rel 7. Rel 8
3. ten-fold cross-validation performed dividing data ten blocks cases similar
size class distribution. block turn, decision tree constructed remaining nine
blocks tested unseen cases hold-out block.

81

fiQuinlan

less accurate Rel 7 four twenty tasks; smallest data set (labor,
57 cases), however, trees produced Rel 8 substantially less accurate.
pruned trees generated Rel 8 tasks great deal smaller Rel 7
counterparts { diabetes particularly notable example.
recommend use unpruned trees constructed initially C4.5 but,
sake completeness, corresponding figures unpruned trees also
determined. average ratio error rate Rel 8 Rel 7 0.95,
ratio tree size 0.94. unpruned trees, then, modifications incorporated
Rel 8 lead 5% reduction error 6% reduction size.

4.2 Adding irrelevant attributes
practical applications, unlikely analyst would knowingly add irrelevant
attributes data! However, even attribute relevant parts
tree might quite irrelevant others. bias towards continuous attributes inherent
Rel 7 implies system occasionally select test irrelevant continuous
attribute preference tests relevant discrete attributes.
explore potential deficiency, twenty data sets modified adding irrelevant attributes. Ten continuous attributes, uniformly distributed
random values x, 0 x < 1. (Since order values continuous attribute
important, distribution values matter { use another distribution
Gaussian N (0; 1) produce comparable results.) Kohavi (personal
communication, 1995) points out, unfair compare Rel 8 Rel 7 data sets
irrelevant continuous-valued attributes added, since modifications
incorporated Rel 8 make less likely choose tests involving continuous attributes.
circumvent problem, ten discrete attributes ten equiprobable values
added, giving twenty irrelevant attributes all. experiments repeated
enlarged data sets, results shown Table 2.
results highlight effects modifications implemented Rel 8. Addition
irrelevant attributes increases error Rel 7 trees average 12%,
much smaller impact produced Rel 8. head-to-head comparison
altered data sets, presented table, shows pruned trees found Rel 8
10% lower error average, also great deal smaller. split random
continuous attribute unlikely generate sucient gain \pay for" threshold,
tests tend filtered Rel 8 Rel 7. Consequently, Rel 7
prone split data (uselessly) random attribute, leading larger trees
higher error rates.

4.3 Ablation experiments
effects modifications implemented Rel 8 factored choosing (slightly)
different thresholds using gain rather gain ratio, excluding attributes
threshold gives sucient gain offset penalty, re-ranking potential tests penalizing involve continuous attributes. ascertain contributions each, two
intermediate versions C4.5 constructed:
82

fiImproved Use Continuous Attributes C4.5

Table 2: Results addition irrelevant attributes.
Rel 8
anneal+
7.72 .23
auto+
18.7 .5
breast-w+ 5.69 .11
colic+
15.1 .2
credit-a+ 13.6 .3
credit-g+ 28.5 .3
diabetes+ 26.9 .3
glass+
37.0 .5
heart-c+
22.6 .7
heart-h+ 20.3 .4
hepatitis+ 19.1 .6
hypo+
.47 .02
iris+
5.67 .15
labor+
19.1 .8
letter+
12.7 .1
segment+ 3.91 .09
sick+
1.61 .05
sonar+
25.5 .8
vehicle+
28.7 .3
waveform+ 30.1 .7

average

Error Rate
Rel 7
w/d/l
8.13 .18 9/0/1
26.0 .7 10/0/0
6.17 .13 8/0/2
20.1 .3 10/0/0
16.4 .3 10/0/0
32.4 .4 10/0/0
30.3 .5 10/0/0
35.9 .8
3/0/7
30.3 .4 10/0/0
24.9 .5 10/0/0
23.9 .7 10/0/0
.49 .02 7/2/1
5.73 .41 4/0/6
24.6 .7
9/1/0
13.3 .1 10/0/0
3.85 .05 4/0/6
1.57 .05 4/1/5
29.3 .6
9/1/0
28.8 .2
5/3/2
28.0 .6
4/0/6

ratio
.95
.72
.92
.75
.83
.88
.89
1.03
.75
.82
.80
.96
.99
.78
.95
1.01
1.02
.87
.99
1.08

.90

Tree Size
Rel 8
Rel 7 ratio
74.3 1.1 84.0 1.6 .88
63.4 .7 62.3 .6 1.02
16.8 .4 25.0 .4
.67
8.9 .2 39.9 1.1 .22
34.7 .7 58.4 .9
.60
111 3
174 2
.64
43.6 2.1 115.5 1.8 .38
31.0 .6 46.2 .9
.67
24.9 .8 52.0 .5
.48
19.9 .5 32.0 .9
.62
5.6 .4 20.4 .6
.28
27.8 .2 25.9 .2 1.08
7.6 .1
9.7 .1
.79
6.8 .2 10.6 .1
.64
2300 3 2372 6
.97
69.2 .6 88.2 .6
.78
37.1 .8 54.8 .7
.68
20.1 .4 34.0 .5
.59
109 1
162 1
.67
27.9 1.0 48.9 .5
.57

.66

7G differs Rel 7 threshold chosen maximize information
gain rather gain ratio;

7GS also chooses thresholds gain; gain best threshold less
penalty log2 (N , 1)/jDj, however, test excluded.
difference 7GS Rel 8 latter's application penalty
determining relative desirability possible tests.
trials repeated using cross-validation blocks intermediate versions. Average error rates, tree sizes, ratios (again computed respect
Rel 7) presented Table 3 summarized graphically Figure 1.
Selection thresholds gain rather gain ratio (7G) little impact {
average error rate tree size ratios respect Rel 7 close one.
non-trivial changes tasks, however; instance, error rate
segment data considerably lower trees found breast-w task noticeably
larger.
83

fiQuinlan

Table 3: Results intermediate systems 7G 7GS.
7G
anneal
7.73 .16
auto
23.0 .9
breast-w 5.21 .23
colic
15.0 .4
credit-a 14.7 .3
credit-g 29.7 .3
diabetes 27.1 .4
glass
30.9 .5
heart-c
25.0 .4
heart-h 22.2 .4
hepatitis 22.0 .8
hypo
.49 .02
iris
4.93 .23
labor
18.8 1.1
letter
12.3 .0
segment 3.39 .08
sick
1.34 .02
sonar
28.8 1.2
vehicle
28.1 .2
waveform 28.1 .8

average

Error Rate
ratio 7GS
1.03 7.62 .16
.97 22.7 .8
.98 5.32 .17
.99 15.0 .3
.93 14.1 .2
1.03 29.1 .2
.96 25.2 .3
.96 31.3 .7
1.01 23.8 .5
1.03 20.9 .3
1.01 21.4 .6
1.00 .50 .01
1.01 4.80 .17
1.13 19.5 1.0
1.00 12.2 .0
.90 3.36 .07
1.04 1.34 .02
1.02 26.6 1.1
.97 27.6 .3
1.00 27.3 .6

1.00

ratio
1.02
.95
1.01
.99
.89
1.01
.89
.97
.96
.97
.98
1.02
.99
1.17
1.00
.89
1.04
.94
.95
.97

.98

7G
73.9 .7
59.5 .9
24.4 .3
20.2 .5
50.1 1.0
148 2
127 2
50.3 .3
44.5 .8
30.2 1.1
17.3 .4
25.7 .2
8.5 .1
7.8 .1
2327 3
83.7 .3
50.4 .3
32.6 .4
178 1
46.8 .4

Tree Size
ratio
7GS
1.05 73.4 .7
.95 59.0 .9
1.21 24.1 .5
1.01 17.9 .6
.88 38.3 1.0
.96 138 2
.99 45.4 2.0
.98 46.2 .6
.98 42.2 .9
1.02 19.3 .7
1.12 15.2 .4
1.02 27.0 .1
.92 8.5 .0
1.06 7.6 .1
.98 2330 3
1.00 82.8 .3
.98 51.4 .3
.98 28.5 .3
.99 145 2
.95 44.4 .6

1.00

Error Rate

ratio
1.05
.94
1.19
.90
.67
.89
.35
.90
.93
.65
.98
1.07
.92
1.04
.98
.99
1.00
.86
.80
.90

.90

Tree Size

Rel 7
7G
7GS
Rel 8

.9

.95

1.0

.8

Figure 1: Summary ablation results.

84

.9

1.0

fiImproved Use Continuous Attributes C4.5

Use penalty filter tests continuous attributes (7GS) produces noticeable differences. Ruling tests continuous attributes accounts
reduction tree size observed Rel 8. cases, trees markedly smaller
{ diabetes data, 7GS trees average one-third size
produced Rel 7. change also accounts half Rel 8's improvement
error rate, diabetes data providing greatest change 7G.
Finally, use Rel 8 penalty re-rank attributes yields improvement error rate small decrease average tree size. re-ranking may
beneficial even attributes continuous: average error rate Rel 8
1% lower 7GS nine tasks kind, two 7GS
give lower error rate Rel 8.

5. Related Research
section examines two alternative methods utilizing continuous attributes
mentioned introduction, compares empirically C4.5 Rel 8.

5.1 Global discretization
Dougherty et al. (1995) consider various ways converting continuous attribute
discrete one dividing values intervals, becomes separate value
replacement discrete attribute. method found give best results, entropy
discretization, first investigated Catlett (1991) means reducing time
required construct tree. Fayyad Irani (1993) subsequently introduced clever
refinement led final form used Dougherty et al. experiments
reported here.
find set intervals, training cases first sorted value continuous attribute question. procedure outlined Section 2 used find threshold
maximizes information gain. process repeated corresponding
subsets cases attribute values t. (Since cases reordered,
need re-sorted, source reduced learning times.) w
thresholds found, continuous attribute mapped discrete attribute w+1
values, one interval.
stopping criterion required prevent process resulting large
number intervals (which could become numerous training cases values
attribute distinct). Catlett uses four-pronged heuristic criterion, Fayyad
Irani developed elegant test based MDL principle (Section 3). view
discretization rule classifying theory uses single attribute associates
class interval. Introduction additional threshold, increasing complexity
discretization rule, allowed greater theory coding cost
offset consequent reduction exceptions cost. scheme generally leads
thresholds regions cases' classes vary much finer divisions
required.
Similar experiments described Dougherty et al. carried
learning tasks Section 4. trial, training data used find discretization
85

fiQuinlan

Table 4: Comparison C4.5 using global discretization (Discr ).
anneal
auto
breast-w
colic
credit-a
credit-g
diabetes
glass
heart-c
heart-h
hepatitis
hypo
iris
labor
letter
segment
sick
sonar
vehicle
waveform

average

Rel 8
7.67 .12
17.7 .5
5.26 .19
15.0 .2
14.7 .2
28.4 .3
25.4 .3
32.5 .8
23.0 .5
21.5 .2
20.4 .6
.48 .01
4.80 .17
19.1 1.0
12.0 .0
3.21 .08
1.34 .03
25.6 .7
27.1 .4
27.3 .3

Error Rate
Discr
w/d/l
9.48 .14 10/0/0
23.8 .6
9/1/0
5.38 .15 6/0/4
15.1 .1
6/2/2
14.0 .1
0/1/9
28.1 .4
5/1/4
25.5 .3
5/0/5
28.4 .3
1/0/9
21.7 .6
2/1/7
20.8 .4
3/0/7
19.6 .8
3/1/6
.72 .03 10/0/0
5.47 .29 6/3/1
20.0 .9
6/0/4
21.1 .0 10/0/0
5.65 .10 10/0/0
2.14 .03 10/0/0
24.6 .7
3/1/6
31.5 .5 10/0/0
26.5 .6
4/0/6

ratio
.81
.74
.98
.99
1.05
1.01
.99
1.14
1.06
1.04
1.04
.67
.88
.96
.57
.57
.63
1.04
.86
1.03

.90

Tree Size
Rel 8
Discr
75.2 .7 68.1 .5
63.7 .4 94.8 1.8
25.0 .5 19.9 .5
9.7 .2
7.8 .2
33.2 1.1 22.3 .6
124 2
82 1
44.0 1.6 19.6 .7
45.7 .4 35.8 .3
39.9 .4 25.9 .4
19.1 .6
9.7 .6
17.8 .3 11.5 .5
27.5 .1 45.1 .3
8.5 .0
6.2 .1
7.0 .3
5.2 .1
2328 4 9600 12
82.9 .5 296.4 2.6
50.8 .5 32.8 .4
28.4 .2 28.6 .5
135 2
175 2
44.6 .4 42.2 .8

ratio
1.11
.67
1.25
1.23
1.49
1.50
2.25
1.28
1.54
1.97
1.55
.61
1.36
1.34
.24
.28
1.55
.99
.78
1.06

1.20

rules convert every continuous attribute discrete attribute. C4.54 invoked find
tree evaluated test data, using discretization intervals found
training data. before, data set subjected ten cross-validations using
blocks cases previously.
Results trials, summarized Table 4, show comments Dougherty
et al. quoted introduction apply Rel 8. Discretization leads improved
accuracy eight tasks degradation 12 them. improvements
modest, however, several tasks exhibit marked increase error; average value
error ratio indicates strong advantage local threshold selection employed
Rel 8 global thresholding used discretization.
Kohavi (personal communication, 1996) suggests might \middle ground"
thresholds determined locally subsets cases relatively small,
point subsequent possible thresholds would found using discretization strategy
above. Evidence support idea provided Figure 2 where, task,
error ratio appears Table 4 plotted size data set (on logarithmic
4. Since continuous attributes, Rel 7 Rel 8 give identical results discretized tasks.

86

fiImproved Use Continuous Attributes C4.5

size



20000



1000
100
0.5










0.75

1.0



1.25

ratio

Figure 2: Effect discretization vs data set size.
scale). clear trend shows global discretization degrades performance data
sets become larger, beneficial tasks fewer cases.

5.2 Multi-threshold splits

contrast, T2 (Auer et al., 1995) determines thresholds locally allows values
continuous attribute partitioned multiple intervals. intervals
found heuristically recursive application binary splitting, above. Instead,
thorough exploration carried find set intervals minimizes
error training set. (The default value C +1 C classes
data.) Search intervals expensive, T2 restricts decision trees two levels
tests (in spirit one-level decision \stumps" described Holte, 1993)
second level employs non-binary splits continuous attributes. Within restricted
theory language, however, T2 guaranteed find tree misclassifies
training cases possible.
Even so, computational cost T2 using default value proportional
C 4 (C +1)2 a2 , number attributes (Auer, personal communication, 1996).
example, time required process small auto data set six classes 25
attributes four orders magnitude greater needed C4.5. effectively
rules trials T2 learning tasks used above, specifically
four classes. remaining 14 tasks, experiments following pattern
using cross-validation blocks carried reported
Table 5. T2 produces trees error rates much lower generated Rel 8
two tasks, slightly lower two more, higher remaining ten. ected
average error ratio, trials still favor C4.5 Rel 8 overall. (Had possible run
tasks larger numbers classes, T2's restricted theory language would perhaps
caused even noticeable increase error.)
87

fiQuinlan

Table 5: Comparison T2.
breast-w
colic
credit-a
credit-g
diabetes
heart-c
heart-h
hepatitis
iris
labor
sick
sonar
vehicle
waveform

average

Rel 8
5.26 .19
15.0 .2
14.7 .2
28.4 .3
25.4 .3
23.0 .5
21.5 .2
20.4 .6
4.80 .17
19.1 1.0
1.34 .03
25.6 .7
27.1 .4
27.3 .3

Error Rate
Tree Size
T2
w/d/l ratio Rel 8
T2
ratio
4.06 .09 0/0/10 1.30 25.0 .5 10.0 .0 2.50
16.2 .2 10/0/0
.92 9.7 .2 15.5 .2 .63
16.6 .2 10/0/0
.89 33.2 1.1 46.1 .4 .72
32.2 .2 10/0/0
.88 124 2
49 1 2.51
24.9 .2
3/0/7 1.02 44.0 1.6 11.5 .0 3.81
26.8 .6 10/0/0
.86 39.9 .4 20.5 .0 1.94
26.1 .3 10/0/0
.82 19.1 .6 16.3 .3 1.18
24.8 .3 10/0/0
.82 17.8 .3 13.7 .2 1.30
4.60 .35 3/1/6 1.04 8.5 .0 12.0 .0 .71
15.3 1.6 3/0/7 1.25 7.0 .3 14.9 .1 .47
2.21 .01 10/0/0
.61 50.8 .5 12.0 .0 4.23
28.4 .7
8/0/2
.90 28.4 .2 11.1 .0 2.56
38.1 .3 10/0/0
.71 135 2
16 0 8.46
35.2 .6 10/0/0
.78 44.6 .4 13.9 .0 3.21

.91

2.44

worth noting T2's trees much smaller found C4.5 { less
half size, average. despite fact tests T2 one outcome
(for unknown values) corresponding tests C4.5.

6. Conclusion
results Section 4 show straightforward changes C4.5's use continuous
attributes lead overall improvement performance twenty learning tasks
investigated here.5 pruned trees substantially smaller somewhat accurate,
especially presence irrelevant attributes. tasks representative selection
UCI Repository involve continuous attributes, similar learning tasks
also benefit. course, C4.5's performance domains continuous attributes
also improved complementary ways, selecting attributes (John,
Kohavi, & P eger, 1994), exploring space parameter settings (Kohavi & John, 1995),
generating multiple classifiers (Breiman, 1996; Freund & Schapire, 1996).
Comparisons well-known global discretization scheme, system
carries thorough search space two-level decision trees, also favor
modified C4.5. However, suggest ways system might improved.
Non-binary splits continuous attributes make trees easier understand also seem
lead accurate trees domains. would also interesting investigate
5. files necessary update C4.5 Release 5 (available Quinlan, 1993) new Release 8
obtained anonymous ftp ftp.cs.su.oz.au, file pub/ml/patch.tar.Z.

88

fiImproved Use Continuous Attributes C4.5

Kohavi's suggestion use discretization within tree local number training
cases small.
another tack, C4.5 option affects tests discrete attributes. Instead
default, value attribute associated separate subtree,
values grouped subsets one tree formed subset. Many possible subsets
explored, many possible thresholds continuous attribute considered.
argument application penalty tests continuous attributes would seem
apply also subset tests.

Appendix A. Description learning tasks
Abbrev

Domain

Cases Classes

anneal
auto
breast-w
colic
credit-a
credit-g
diabetes
glass
heart-c
heart-h
hepatitis
hypo
iris
labor
letter
segment
sick
sonar
vehicle
waveform

annealing processes
898
auto insurance
205
breast cancer (Wisc)
699
horse colic
368
credit screening (Aust)
690
credit screening (Ger)
1000
Pima diabetes
768
glass identification
214
heart disease (Clev)
303
heart disease (Hun)
294
hepatitis prognosis
155
hypothyroid diagnosis
3772
iris classification
150
labor negotiations
57
letter identification
20000
image segmentation
2310
sick euthyroid
3772
sonar classification
208
silhouette recognition
846
waveform differentiation 300

6
6
2
2
2
2
2
6
2
2
2
5
3
2
26
7
2
2
4
3

Attributes
Cont Discr
9
29
15
10
9
{
10
12
6
9
7
13
8
{
9
{
8
5
8
5
6
13
7
22
4
{
8
8
16
{
19
{
7
22
60
{
18
{
21
{

Acknowledgements
research made possible grant Australian Research Council.
T2 system programmed Peter Auer made available comparisons
Rob Holte. Thanks Thierry Van de Merckt comments results led
ablation experiments. also grateful suggestions regarding paper's content
presentation made Ron Kohavi, Usama Fayyad, Pat Langley. UCI Data
Repository owes existence David Aha Patrick Murphy. breast cancer data
(breast-w) provided Repository Dr William H. Wolberg.
89

fiQuinlan

References

Auer, P., Holte, R. C., & Maass, W. (1995). Theory application agnostic paclearning small decision trees. Proceedings Twelfth International Conference
Machine Learning, pp. 21{29. San Francisco: Morgam Kaufmann.
Breiman, L. (1996). Bagging predictors. Machine Learning, (to appear).
Catlett, J. (1991). changing continuous attributes ordered discrete attributes.
Kodratoff, Y. (Ed.), Proceedings European Working Session Learning { EWSL-91,
pp. 164{178. Berlin: Springer Verlag.
Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised unsupervised discretization
continuous features. Proceedings Twelfth International Conference Machine
Learning, pp. 194{202. San Francisco: Morgan Kaufmann.
Fayyad, U. M., & Irani, K. B. (1992). handling continuous-valued attributes
decision tree generation. Machine Learning, 8, 87{102.
Fayyad, U. M., & Irani, K. B. (1993). Multi-interval discretization continuous-valued
attributes classification learning. Proceedings Thirteenth International Joint
Conference Artificial Intelligence, pp. 1022{1027. San Francisco: Morgan Kaufmann.
Freund, Y., & Schapire, R. E. (1996). decision-theoretic generalization on-line learning
application boosting. Unpublished manuscript, available authors'
home pages (\http://www.research.att.com/orgs/ssr/people/fyoav,schapireg").
Holte, R. C. (1993). simple classification rules perform well commonly used
datasets. Machine Learning, 11, 63{91.
Hunt, E. B., Marin, J., & Stone, P. J. (1966). Experiments Induction. New York:
Academic Press.
John, G. H., Kohavi, R., & P eger, K. (1994). Irrelevant features subset selection
problem. Proceedings Eleventh International Conference Machine Learning, pp.
121{129. San Francisco: Morgan Kaufmann.
Kohavi, R., & John, G. H. (1995). Automatic parameter selection minimizing estimated
error. Proceedings Twelfth International Conference Machine Learning, pp.
304{312. San Francisco: Morgan Kaufmann.
Quinlan, J. R. (1993). C4.5: Programs Machine Learning. San Mateo: Morgan Kaufmann.
Quinlan, J. R., & Rivest, R. L. (1989). Inferring decision trees using minimum description length principle. Information Computation, 80, 227{248.
Rissanen, J. (1983). universal prior integers estimation minimum description
length. Annals Statistics, 11, 416{431.
90

fiJournal Artificial Intelligence Research 4 (1996) 237-285

Submitted 9/95; published 5/96

Reinforcement Learning: Survey
Leslie Pack Kaelbling
Michael L. Littman

Computer Science Department, Box 1910, Brown University
Providence, RI 02912-1910 USA

lpk@cs.brown.edu
mlittman@cs.brown.edu

Andrew W. Moore

Smith Hall 221, Carnegie Mellon University, 5000 Forbes Avenue
Pittsburgh, PA 15213 USA

awm@cs.cmu.edu

Abstract

paper surveys field reinforcement learning computer-science perspective. written accessible researchers familiar machine learning.
historical basis field broad selection current work summarized.
Reinforcement learning problem faced agent learns behavior
trial-and-error interactions dynamic environment. work described
resemblance work psychology, differs considerably details use
word \reinforcement." paper discusses central issues reinforcement learning,
including trading exploration exploitation, establishing foundations field
via Markov decision theory, learning delayed reinforcement, constructing empirical
models accelerate learning, making use generalization hierarchy, coping
hidden state. concludes survey implemented systems assessment
practical utility current methods reinforcement learning.

1. Introduction
Reinforcement learning dates back early days cybernetics work statistics,
psychology, neuroscience, computer science. last five ten years, attracted
rapidly increasing interest machine learning artificial intelligence communities.
promise beguiling|a way programming agents reward punishment without
needing specify task achieved. formidable computational
obstacles fulfilling promise.
paper surveys historical basis reinforcement learning current
work computer science perspective. give high-level overview field
taste specific approaches. is, course, impossible mention important
work field; taken exhaustive account.
Reinforcement learning problem faced agent must learn behavior
trial-and-error interactions dynamic environment. work described
strong family resemblance eponymous work psychology, differs considerably
details use word \reinforcement." appropriately thought
class problems, rather set techniques.
two main strategies solving reinforcement-learning problems. first
search space behaviors order find one performs well environment.
approach taken work genetic algorithms genetic programming,
c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiKaelbling, Littman, & Moore






R


r

B

Figure 1: standard reinforcement-learning model.
well novel search techniques (Schmidhuber, 1996). second use
statistical techniques dynamic programming methods estimate utility taking
actions states world. paper devoted almost entirely second set
techniques take advantage special structure reinforcement-learning
problems available optimization problems general. yet clear
set approaches best circumstances.
rest section devoted establishing notation describing basic
reinforcement-learning model. Section 2 explains trade-off exploration
exploitation presents solutions basic case reinforcement-learning
problems, want maximize immediate reward. Section 3 considers
general problem rewards delayed time actions crucial
gaining them. Section 4 considers classic model-free algorithms reinforcement
learning delayed reward: adaptive heuristic critic, TD() Q-learning. Section 5
demonstrates continuum algorithms sensitive amount computation
agent perform actual steps action environment. Generalization|the
cornerstone mainstream machine learning research|has potential considerably
aiding reinforcement learning, described Section 6. Section 7 considers problems
arise agent complete perceptual access state
environment. Section 8 catalogs reinforcement learning's successful applications.
Finally, Section 9 concludes speculations important open problems
future reinforcement learning.

1.1 Reinforcement-Learning Model
standard reinforcement-learning model, agent connected environment
via perception action, depicted Figure 1. step interaction agent
receives input, i, indication current state, s, environment; agent
chooses action, a, generate output. action changes state
environment, value state transition communicated agent
scalar reinforcement signal, r. agent's behavior, B , choose actions tend
increase long-run sum values reinforcement signal. learn
time systematic trial error, guided wide variety algorithms
subject later sections paper.
238

fiReinforcement Learning: Survey

Formally, model consists

discrete set environment states, ;
discrete set agent actions, A;
set scalar reinforcement signals; typically f0; 1g, real numbers.
figure also includes input function , determines agent views
environment state; assume identity function (that is, agent perceives
exact state environment) consider partial observability Section 7.
intuitive way understand relation agent environment
following example dialogue.
Environment: state 65. 4 possible actions.
Agent:
I'll take action 2.
Environment: received reinforcement 7 units. state
15. 2 possible actions.
Agent:
I'll take action 1.
Environment: received reinforcement -4 units. state
65. 4 possible actions.
Agent:
I'll take action 2.
Environment: received reinforcement 5 units. state
44. 5 possible actions.

..
.

..
.

agent's job find policy , mapping states actions, maximizes
long-run measure reinforcement. expect, general, environment
non-deterministic; is, taking action state two different
occasions may result different next states and/or different reinforcement values.
happens example above: state 65, applying action 2 produces differing reinforcements differing states two occasions. However, assume environment
stationary; is, probabilities making state transitions receiving specific
reinforcement signals change time.1
Reinforcement learning differs widely studied problem supervised learning several ways. important difference presentation input/output pairs. Instead, choosing action agent told immediate reward
subsequent state, told action would best long-term
interests. necessary agent gather useful experience possible system
states, actions, transitions rewards actively act optimally. Another difference
supervised learning on-line performance important: evaluation system
often concurrent learning.
1. assumption may disappointing; all, operation non-stationary environments one
motivations building learning systems. fact, many algorithms described later sections
effective slowly-varying non-stationary environments, little theoretical analysis
area.

239

fiKaelbling, Littman, & Moore

aspects reinforcement learning closely related search planning issues
artificial intelligence. AI search algorithms generate satisfactory trajectory
graph states. Planning operates similar manner, typically within construct
complexity graph, states represented compositions
logical expressions instead atomic symbols. AI algorithms less general
reinforcement-learning methods, require predefined model state transitions,
exceptions assume determinism. hand, reinforcement learning,
least kind discrete cases theory developed, assumes
entire state space enumerated stored memory|an assumption
conventional search algorithms tied.

1.2 Models Optimal Behavior

start thinking algorithms learning behave optimally,
decide model optimality be. particular, specify
agent take future account decisions makes behave
now. three models subject majority work
area.
finite-horizon model easiest think about; given moment time,
agent optimize expected reward next h steps:
h
X

E ( rt) ;
t=0

need worry happen that. subsequent expressions,

rt represents scalar reward received steps future. model used

two ways. first, agent non-stationary policy; is, one changes
time. first step take termed h-step optimal action.
defined best action available given h steps remaining act
gain reinforcement. next step take (h , 1)-step optimal action,
on, finally takes 1-step optimal action terminates. second, agent
receding-horizon control, always takes h-step optimal action. agent
always acts according policy, value h limits far ahead looks
choosing actions. finite-horizon model always appropriate. many cases
may know precise length agent's life advance.
infinite-horizon discounted model takes long-run reward agent account, rewards received future geometrically discounted according
discount factor , (where 0 < 1):
1
X

E ( trt) :
t=0

interpret several ways. seen interest rate, probability living
another step, mathematical trick bound infinite sum. model conceptually similar receding-horizon control, discounted model mathematically
tractable finite-horizon model. dominant reason wide attention
model received.
240

fiReinforcement Learning: Survey

Another optimality criterion average-reward model, agent supposed
take actions optimize long-run average reward:
h
1X
rt) :
lim
E
(
h!1 h
t=0

policy referred gain optimal policy; seen limiting case
infinite-horizon discounted model discount factor approaches 1 (Bertsekas, 1995).
One problem criterion way distinguish two policies,
one gains large amount reward initial phases
not. Reward gained initial prefix agent's life overshadowed
long-run average performance. possible generalize model takes
account long run average amount initial reward gained.
generalized, bias optimal model, policy preferred maximizes long-run
average ties broken initial extra reward.
Figure 2 contrasts models optimality providing environment
changing model optimality changes optimal policy. example, circles
represent states environment arrows state transitions.
single action choice every state except start state, upper left
marked incoming arrow. rewards zero except marked.
finite-horizon model h = 5, three actions yield rewards +6:0, +0:0, +0:0,
first action chosen; infinite-horizon discounted model = 0:9,
three choices yield +16:2, +59:0, +58:5 second action chosen;
average reward model, third action chosen since leads
average reward +11. change h 1000 0.2, second action
optimal finite-horizon model first infinite-horizon discounted model;
however, average reward model always prefer best long-term average. Since
choice optimality model parameters matters much, important choose
carefully application.
finite-horizon model appropriate agent's lifetime known; one important aspect model length remaining lifetime decreases,
agent's policy may change. system hard deadline would appropriately modeled
way. relative usefulness infinite-horizon discounted bias-optimal models
still debate. Bias-optimality advantage requiring discount parameter;
however, algorithms finding bias-optimal policies yet well-understood
finding optimal infinite-horizon discounted policies.

1.3 Measuring Learning Performance

criteria given previous section used assess policies learned
given algorithm. would also like able evaluate quality learning itself.
several incompatible measures use.

Eventual convergence optimal. Many algorithms come provable guar-

antee asymptotic convergence optimal behavior (Watkins & Dayan, 1992).
reassuring, useless practical terms. agent quickly reaches plateau
241

fiKaelbling, Littman, & Moore

+2
Finite horizon, h=4
+10
Infinite horizon, =0.9

+11

Average reward

Figure 2: Comparing models optimality. unlabeled arrows produce reward zero.
99% optimality may, many applications, preferable agent
guarantee eventual optimality sluggish early learning rate.

Speed convergence optimality. Optimality usually asymptotic result,

convergence speed ill-defined measure. practical speed
convergence near-optimality. measure begs definition near
optimality sucient. related measure level performance given time,
similarly requires someone define given time.
noted another difference reinforcement learning
conventional supervised learning. latter, expected future predictive accuracy statistical eciency prime concerns. example, well-known
PAC framework (Valiant, 1984), learning period mistakes
count, performance period do. framework provides
bounds necessary length learning period order probabilistic
guarantee subsequent performance. usually inappropriate view
agent long existence complex environment.
spite mismatch embedded reinforcement learning train/test
perspective, Fiechter (1994) provides PAC analysis Q-learning (described
Section 4.2) sheds light connection two views.
Measures related speed learning additional weakness. algorithm
merely tries achieve optimality fast possible may incur unnecessarily
large penalties learning period. less aggressive strategy taking longer
achieve optimality, gaining greater total reinforcement learning might
preferable.

Regret. appropriate measure, then, expected decrease reward gained

due executing learning algorithm instead behaving optimally
beginning. measure known regret (Berry & Fristedt, 1985). penalizes
mistakes wherever occur run. Unfortunately, results concerning
regret algorithms quite hard obtain.
242

fiReinforcement Learning: Survey

1.4 Reinforcement Learning Adaptive Control
Adaptive control (Burghes & Graham, 1980; Stengel, 1986) also concerned algorithms improving sequence decisions experience. Adaptive control much
mature discipline concerns dynamic systems states actions vectors system dynamics smooth: linear locally linearizable around
desired trajectory. common formulation cost functions adaptive control
quadratic penalties deviation desired state action vectors. importantly,
although dynamic model system known advance, must estimated data, structure dynamic model fixed, leaving model estimation
parameter estimation problem. assumptions permit deep, elegant powerful
mathematical analysis, turn lead robust, practical, widely deployed adaptive
control algorithms.

2. Exploitation versus Exploration: Single-State Case
One major difference reinforcement learning supervised learning
reinforcement-learner must explicitly explore environment. order highlight
problems exploration, treat simple case section. fundamental issues
approaches described will, many cases, transfer complex instances
reinforcement learning discussed later paper.
simplest possible reinforcement-learning problem known k-armed bandit
problem, subject great deal study statistics applied
mathematics literature (Berry & Fristedt, 1985). agent room collection
k gambling machines (each called \one-armed bandit" colloquial English). agent
permitted fixed number pulls, h. arm may pulled turn. machines
require deposit play; cost wasting pull playing suboptimal
machine. arm pulled, machine pays 1 0, according underlying
probability parameter pi , payoffs independent events pi unknown.
agent's strategy be?
problem illustrates fundamental tradeoff exploitation exploration.
agent might believe particular arm fairly high payoff probability;
choose arm time, choose another one less information
about, seems worse? Answers questions depend long agent
expected play game; longer game lasts, worse consequences
prematurely converging sub-optimal arm, agent explore.
wide variety solutions problem. consider representative
selection them, deeper discussion number important theoretical results,
see book Berry Fristedt (1985). use term \action" indicate
agent's choice arm pull. eases transition delayed reinforcement models
Section 3. important note bandit problems fit definition
reinforcement-learning environment single state self transitions.
Section 2.1 discusses three solutions basic one-state bandit problem
formal correctness results. Although extended problems real-valued
rewards, apply directly general multi-state delayed-reinforcement case.
243

fiKaelbling, Littman, & Moore

Section 2.2 presents three techniques formally justified, wide
use practice, applied (with similar lack guarantee) general case.

2.1 Formally Justified Techniques

fairly well-developed formal theory exploration simple problems.
Although instructive, methods provides scale well complex
problems.
2.1.1 Dynamic-Programming Approach

agent going acting total h steps, use basic Bayesian reasoning
solve optimal strategy (Berry & Fristedt, 1985). requires assumed prior
joint distribution parameters fpig, natural pi
independently uniformly distributed 0 1. compute mapping belief
states (summaries agent's experiences run) actions. Here, belief state
represented tabulation action choices payoffs: fn1; w1; n2; w2; : : :; nk ; wk g
denotes state play arm pulled ni times wi payoffs.
write V (n1; w1; : : :; nk ; wk ) expected payoff remaining, given total h pulls
available,
use remaining pulls optimally.
P
ni = h, remaining pulls, V (n1 ; w1; : : :; nk ; wk ) = 0.
basis recursive definition. know V value belief states pulls
remaining, compute V value belief state + 1 pulls remaining:

V (n1; w1; : : :; nk ; wk) = maxi E
= maxi

"

#

Future payoff agent takes action i,
acts optimally remaining pulls !
iV (n1; wi; : : :; ni + 1; wi + 1; : : :; nk ; wk)+
(1 , i)V (n1 ; wi; : : :; ni + 1; wi; : : :; nk ; wk )

posterior subjective probability action paying given ni , wi
prior probability. uniform priors, result beta distribution, =
(wi + 1)=(ni + 2).
expense filling table V values way attainable belief states
linear number belief states times actions, thus exponential horizon.
2.1.2 Gittins Allocation Indices

Gittins gives \allocation index" method finding optimal choice action
step k-armed bandit problems (Gittins, 1989). technique applies
discounted expected reward criterion. action, consider number times
chosen, n, versus number times paid off, w. certain discount factors,
published tables \index values," (n; w) pair n w. Look
index value action i, (ni ; wi). represents comparative measure
combined value expected payoff action (given history payoffs) value
information would get choosing it. Gittins shown choosing
action largest index value guarantees optimal balance exploration
exploitation.
244

fiReinforcement Learning: Survey

a=1

a=0
1

2

3

N-1

N

2N
r=1

2

3

N-1

N+3

N+2

N+1

N+2

N+1

a=1

a=0
1

2N-1

N

2N

2N-1

N+3

r=0

Figure 3: Tsetlin automaton 2N states. top row shows state transitions
made previous action resulted reward 1; bottom
row shows transitions reward 0. states left half figure,
action 0 taken; right, action 1 taken.
guarantee optimal exploration simplicity technique
(given table index values), approach holds great deal promise use
complex applications. method proved useful application robotic manipulation
immediate reward (Salganicoff & Ungar, 1995). Unfortunately, one yet
able find analog index values delayed reinforcement problems.
2.1.3 Learning Automata

branch theory adaptive control devoted learning automata, surveyed
Narendra Thathachar (1989), originally described explicitly finite state
automata. Tsetlin automaton shown Figure 3 provides example solves
2-armed bandit arbitrarily near optimally N approaches infinity.
inconvenient describe algorithms finite-state automata, move made
describe internal state agent probability distribution according
actions would chosen. probabilities taking different actions would adjusted
according previous successes failures.
example, stands among set algorithms independently developed
mathematical psychology literature (Hilgard & Bower, 1975), linear reward-inaction
algorithm. Let pi agent's probability taking action i.
action ai succeeds,
pi := pi + ff(1 , pi)
pj := pj , ffpj j 6=

action ai fails, pj remains unchanged (for j ).

algorithm converges probability 1 vector containing single 1
rest 0's (choosing particular action probability 1). Unfortunately, always
converge correct action; probability converges wrong one
made arbitrarily small making ff small (Narendra & Thathachar, 1974).
literature regret algorithm.
245

fiKaelbling, Littman, & Moore

2.2 Ad-Hoc Techniques

reinforcement-learning practice, simple, ad hoc strategies popular.
rarely, ever, best choice models optimality used, may
viewed reasonable, computationally tractable, heuristics. Thrun (1992) surveyed
variety techniques.
2.2.1 Greedy Strategies

first strategy comes mind always choose action highest estimated payoff. aw early unlucky sampling might indicate best action's
reward less reward obtained suboptimal action. suboptimal action
always picked, leaving true optimal action starved data superiority
never discovered. agent must explore ameliorate outcome.
useful heuristic optimism face uncertainty actions selected
greedily, strongly optimistic prior beliefs put payoffs strong negative
evidence needed eliminate action consideration. still measurable
danger starving optimal unlucky action, risk made arbitrarily small. Techniques like used several reinforcement learning algorithms
including interval exploration method (Kaelbling, 1993b) (described shortly), exploration bonus Dyna (Sutton, 1990), curiosity-driven exploration (Schmidhuber, 1991a),
exploration mechanism prioritized sweeping (Moore & Atkeson, 1993).
2.2.2 Randomized Strategies

Another simple exploration strategy take action best estimated expected
reward default, probability p, choose action random. versions
strategy start large value p encourage initial exploration, slowly
decreased.
objection simple strategy experiments non-greedy action
likely try promising alternative clearly hopeless alternative.
slightly sophisticated strategy Boltzmann exploration. case, expected
reward taking action a, ER(a) used choose action probabilistically according
distribution
ER(a)=T
P (a) = P e ER(a )=T :
2A e

0

0

temperature parameter decreased time decrease exploration.
method works well best action well separated others, suffers somewhat
values actions close. may also converge unnecessarily slowly unless
temperature schedule manually tuned great care.
2.2.3 Interval-based Techniques

Exploration often ecient based second-order information
certainty variance estimated values actions. Kaelbling's interval estimation
algorithm (1993b) stores statistics action ai : wi number successes ni
number trials. action chosen computing upper bound 100 (1 , ff)%
246

fiReinforcement Learning: Survey

confidence interval success probability action choosing action
highest upper bound. Smaller values ff parameter encourage greater exploration.
payoffs boolean, normal approximation binomial distribution
used construct confidence interval (though binomial used small
n). payoff distributions handled using associated statistics
nonparametric methods. method works well empirical trials. also related
certain class statistical techniques known experiment design methods (Box &
Draper, 1987), used comparing multiple treatments (for example, fertilizers
drugs) determine treatment (if any) best small set experiments
possible.

2.3 General Problems

multiple states, reinforcement still immediate,
solutions replicated, state. However, generalization required,
solutions must integrated generalization methods (see section 6);
straightforward simple ad-hoc methods, understood maintain
theoretical guarantees.
Many techniques focus converging regime exploratory
actions taken rarely never; appropriate environment stationary.
However, environment non-stationary, exploration must continue take place,
order notice changes world. Again, ad-hoc techniques modified
deal plausible manner (keep temperature parameters going 0; decay
statistics interval estimation), none theoretically guaranteed methods
applied.

3. Delayed Reward

general case reinforcement learning problem, agent's actions determine
immediate reward, also (at least probabilistically) next state
environment. environments thought networks bandit problems,
agent must take account next state well immediate reward
decides action take. model long-run optimality agent using determines
exactly take value future account. agent
able learn delayed reinforcement: may take long sequence actions, receiving
insignificant reinforcement, finally arrive state high reinforcement. agent
must able learn actions desirable based reward take place
arbitrarily far future.

3.1 Markov Decision Processes

Problems delayed reinforcement well modeled Markov decision processes (MDPs).
MDP consists

set states ,
set actions A,
247

fiKaelbling, Littman, & Moore

reward function R : ! <,
state transition function : SA ! (S ), member (S ) probability
distribution set (i.e. maps states probabilities). write (s; a; s0)

probability making transition state state s0 using action a.
state transition function probabilistically specifies next state environment
function current state agent's action. reward function specifies expected
instantaneous reward function current state action. model Markov
state transitions independent previous environment states agent actions.
many good references MDP models (Bellman, 1957; Bertsekas, 1987; Howard,
1960; Puterman, 1994).
Although general MDPs may infinite (even uncountable) state action spaces,
discuss methods solving finite-state finite-action problems. section 6,
discuss methods solving problems continuous input output spaces.

3.2 Finding Policy Given Model

consider algorithms learning behave MDP environments, explore techniques determining optimal policy given correct model. dynamic
programming techniques serve foundation inspiration learning algorithms follow. restrict attention mainly finding optimal policies
infinite-horizon discounted model, algorithms analogs finitehorizon average-case models well. rely result that, infinite-horizon
discounted model, exists optimal deterministic stationary policy (Bellman, 1957).
speak optimal value state|it expected infinite discounted sum
reward agent gain starts state executes optimal policy.
Using complete decision policy, written

!
1
X


V (s) = max
E t=0 rt

:

optimal value function unique defined solution simultaneous
equations
0
1

@
V (s) = max
R(s; a) +

X

2S

(s; a; s0)V (s0)A ; 8s 2 ;

(1)

0

assert value state expected instantaneous reward plus
expected discounted value next state, using best available action. Given
optimal value function, specify optimal policy

0
1
X
@
(s) = arg max
(s; a; s0)V (s0)A :
R(s; a) +
2S
0

3.2.1 Value Iteration

One way, then, find optimal policy find optimal value function.
determined simple iterative algorithm called value iteration shown
converge correct V values (Bellman, 1957; Bertsekas, 1987).
248

fiReinforcement Learning: Survey

V

initialize ( ) arbitrarily
loop policy good enough
loop
loop

s2S
a2A
Q(s; a) := R(s; a) + Ps 2S (s; a; s0)V (s0)
V (s) := maxa Q(s; a)
0

end loop
end loop

obvious stop value iteration algorithm. One important result
bounds performance current greedy policy function Bellman residual
current value function (Williams & Baird, 1993b). says maximum difference
two successive value functions less , value greedy policy,
(the policy obtained choosing, every state, action maximizes estimated
discounted reward, using current estimate value function) differs value
function optimal policy 2 =(1 , ) state. provides
effective stopping criterion algorithm. Puterman (1994) discusses another stopping
criterion, based span semi-norm, may result earlier termination. Another
important result greedy policy guaranteed optimal finite number
steps even though value function may converged (Bertsekas, 1987).
practice, greedy policy often optimal long value function converged.
Value iteration exible. assignments V need done strict order
shown above, instead occur asynchronously parallel provided value
every state gets updated infinitely often infinite run. issues treated
extensively Bertsekas (1989), also proves convergence results.
Updates based Equation 1 known full backups since make use information possible successor states. shown updates form

Q(s; a) := Q(s; a) + ff(r + max
Q(s0; a0) , Q(s; a))

0

also used long pairing updated infinitely often, s0 sampled
distribution (s; a; s0), r sampled mean R(s; a) bounded variance,
learning rate ff decreased slowly. type sample backup (Singh, 1993) critical
operation model-free methods discussed next section.
computational complexity value-iteration algorithm full backups, per
iteration, quadratic number states linear number actions. Commonly, transition probabilities (s; a; s0) sparse. average constant
number next states non-zero probability cost per iteration linear
number states linear number actions. number iterations required
reach optimal value function polynomial number states magnitude
largest reward discount factor held constant. However, worst case
number iterations grows polynomially 1=(1 , ), convergence rate slows
considerably discount factor approaches 1 (Littman, Dean, & Kaelbling, 1995b).
249

fiKaelbling, Littman, & Moore

3.2.2 Policy Iteration

policy iteration algorithm manipulates policy directly, rather finding indirectly via optimal value function. operates follows:
choose arbitrary policy
loop

0

:= 0

compute value function policy
solve linear equations

:

V (s) = R(s; (s)) + Ps 2S (s; (s); s0)V (s0)
0

0(s) := arg maxa (R(s; a) + Ps 2S (s; a; s0)V(s0))
= 0

improve policy state:
0



value function policy expected infinite discounted reward
gained, state, executing policy. determined solving set
linear equations. know value state current policy,
consider whether value could improved changing first action taken. can,
change policy take new action whenever situation. step
guaranteed strictly improve performance policy. improvements
possible, policy guaranteed optimal.
Since jAjjSj distinct policies, sequence policies improves
step, algorithm terminates exponential number iterations (Puterman, 1994). However, important open question many iterations policy iteration
takes worst case. known running time pseudopolynomial
fixed discount factor, polynomial bound total size MDP (Littman
et al., 1995b).
3.2.3 Enhancement Value Iteration Policy Iteration

practice, value iteration much faster per iteration, policy iteration takes fewer
iterations. Arguments put forth effect approach better
large problems. Puterman's modified policy iteration algorithm (Puterman & Shin, 1978)
provides method trading iteration time iteration improvement smoother way.
basic idea expensive part policy iteration solving exact value
V . Instead finding exact value V , perform steps modified
value-iteration step policy held fixed successive iterations.
shown produce approximation V converges linearly . practice,
result substantial speedups.
Several standard numerical-analysis techniques speed convergence dynamic
programming used accelerate value policy iteration. Multigrid methods
used quickly seed good initial approximation high resolution value function
initially performing value iteration coarser resolution (Rude, 1993). State aggregation works collapsing groups states single meta-state solving abstracted
problem (Bertsekas & Casta~non, 1989).
250

fiReinforcement Learning: Survey

3.2.4 Computational Complexity

Value iteration works producing successive approximations optimal value function.
iteration performed O(jAjjS j2) steps, faster sparsity
transition function. However, number iterations required grow exponentially
discount factor (Condon, 1992); discount factor approaches 1, decisions must
based results happen farther farther future. practice, policy
iteration converges fewer iterations value iteration, although per-iteration costs
O(jAjjS j2 + jS j3) prohibitive. known tight worst-case bound available
policy iteration (Littman et al., 1995b). Modified policy iteration (Puterman & Shin,
1978) seeks trade-off cheap effective iterations preferred
practictioners (Rust, 1996).
Linear programming (Schrijver, 1986) extremely general problem, MDPs
solved general-purpose linear-programming packages (Derman, 1970; D'Epenoux,
1963; Hoffman & Karp, 1966). advantage approach commercial-quality
linear-programming packages available, although time space requirements
still quite high. theoretic perspective, linear programming known
algorithm solve MDPs polynomial time, although theoretically ecient
algorithms shown ecient practice.

4. Learning Optimal Policy: Model-free Methods

previous section reviewed methods obtaining optimal policy MDP
assuming already model. model consists knowledge state transition probability function (s; a; s0) reinforcement function R(s; a). Reinforcement
learning primarily concerned obtain optimal policy model
known advance. agent must interact environment directly obtain
information which, means appropriate algorithm, processed produce
optimal policy.
point, two ways proceed.

Model-free: Learn controller without learning model.
Model-based: Learn model, use derive controller.
approach better? matter debate reinforcement-learning
community. number algorithms proposed sides. question also
appears fields, adaptive control, dichotomy direct
indirect adaptive control.
section examines model-free learning, Section 5 examines model-based methods.
biggest problem facing reinforcement-learning agent temporal credit assignment.
know whether action taken good one, might farreaching effects? One strategy wait \end" reward actions taken
result good punish result bad. ongoing tasks, dicult
know \end" is, might require great deal memory. Instead,
use insights value iteration adjust estimated value state based
251

fiKaelbling, Littman, & Moore

r


v

AHC


RL

Figure 4: Architecture adaptive heuristic critic.
immediate reward estimated value next state. class algorithms
known temporal difference methods (Sutton, 1988). consider two different
temporal-difference learning strategies discounted infinite-horizon model.

4.1 Adaptive Heuristic Critic TD()

adaptive heuristic critic algorithm adaptive version policy iteration (Barto,
Sutton, & Anderson, 1983) value-function computation longer implemented solving set linear equations, instead computed algorithm called
TD(0). block diagram approach given Figure 4. consists two components: critic (labeled AHC), reinforcement-learning component (labeled RL).
reinforcement-learning component instance k-armed bandit algorithms, modified deal multiple states non-stationary rewards. instead
acting maximize instantaneous reward, acting maximize heuristic value,
v, computed critic. critic uses real external reinforcement signal
learn map states expected discounted values given policy executed
one currently instantiated RL component.
see analogy modified policy iteration imagine components
working alternation. policy implemented RL fixed critic learns
value function V policy. fix critic let RL component learn
new policy 0 maximizes new value function, on. implementations,
however, components operate simultaneously. alternating implementation
guaranteed converge optimal policy, appropriate conditions. Williams
Baird explored convergence properties class AHC-related algorithms
call \incremental variants policy iteration" (Williams & Baird, 1993a).
remains explain critic learn value policy. define hs; a; r; s0i
experience tuple summarizing single transition environment.
agent's state transition, choice action, r instantaneous reward
receives, s0 resulting state. value policy learned using Sutton's TD(0)
algorithm (Sutton, 1988) uses update rule
V (s) := V (s) + ff(r + V (s0) , V (s)) :
Whenever state visited, estimated value updated closer r + V (s0 ),
since r instantaneous reward received V (s0) estimated value actually
occurring next state. analogous sample-backup rule value iteration|the
difference sample drawn real world rather simulating
known model. key idea r + V (s0 ) sample value V (s),
252

fiReinforcement Learning: Survey

likely correct incorporates real r. learning rate ff adjusted
properly (it must slowly decreased) policy held fixed, TD(0) guaranteed
converge optimal value function.
TD(0) rule presented really instance general class
algorithms called TD(), = 0. TD(0) looks one step ahead adjusting
value estimates; although eventually arrive correct answer, take quite
so. general TD() rule similar TD(0) rule given above,

V (u) := V (u) + ff(r + V (s0) , V (s))e(u) ;
applied every state according eligibility e(u), rather
immediately previous state, s. One version eligibility trace defined
e(s) =


X

k=1

( )t,k s;s

k

, s;sk =

(

1 = sk .
0 otherwise

eligibility state degree visited recent past;
reinforcement received, used update states recently
visited, according eligibility. = 0 equivalent TD(0). = 1,
roughly equivalent updating states according number times
visited end run. Note update eligibility online follows:

(

(s) + 1 = current state .
e(s) := e
e(s)
otherwise
computationally expensive execute general TD(), though often
converges considerably faster large (Dayan, 1992; Dayan & Sejnowski, 1994).

recent work making updates ecient (Cichosz & Mulawka, 1995)
changing definition make TD() consistent certainty-equivalent
method (Singh & Sutton, 1996), discussed Section 5.1.

4.2 Q-learning

work two components AHC accomplished unified manner
Watkins' Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992). Q-learning
typically easier implement. order understand Q-learning, develop
additional notation. Let Q(s; a) expected discounted reinforcement taking action
state s, continuing choosing actions optimally. Note V (s) value
assuming best action taken initially, V (s) = maxa Q (s; a). Q (s; a)
hence written recursively

Q(s; a) = R(s; a) +

X

2S
0

(s; a; s0) max
Q(s0; a0) :

0

Note also that, since V (s) = maxa Q (s; a), (s) = arg maxa Q (s; a)
optimal policy.
Q function makes action explicit, estimate Q values online using method essentially TD(0), also use define policy,
253

fiKaelbling, Littman, & Moore

action chosen taking one maximum Q value
current state.
Q-learning rule

Q(s; a) := Q(s; a) + ff(r + max
Q(s0; a0) , Q(s; a)) ;

0

hs; a; r; s0i experience tuple described earlier. action executed
state infinite number times infinite run ff decayed appropriately,
Q values converge probability 1 Q (Watkins, 1989; Tsitsiklis, 1994; Jaakkola,
Jordan, & Singh, 1994). Q-learning also extended update states occurred
one step previously, TD() (Peng & Williams, 1994).
Q values nearly converged optimal values, appropriate
agent act greedily, taking, situation, action highest Q value.
learning, however, dicult exploitation versus exploration trade-off
made. good, formally justified approaches problem general case;
standard practice adopt one ad hoc methods discussed section 2.2.
AHC architectures seem dicult work Q-learning practical
level. hard get relative learning rates right AHC two
components converge together. addition, Q-learning exploration insensitive:
is, Q values converge optimal values, independent agent
behaves data collected (as long state-action pairs tried often
enough). means that, although exploration-exploitation issue must addressed
Q-learning, details exploration strategy affect convergence
learning algorithm. reasons, Q-learning popular seems
effective model-free algorithm learning delayed reinforcement. not,
however, address issues involved generalizing large state and/or action
spaces. addition, may converge quite slowly good policy.

4.3 Model-free Learning Average Reward

described, Q-learning applied discounted infinite-horizon MDPs. also
applied undiscounted problems long optimal policy guaranteed reach
reward-free absorbing state state periodically reset.
Schwartz (1993) examined problem adapting Q-learning average-reward
framework. Although R-learning algorithm seems exhibit convergence problems
MDPs, several researchers found average-reward criterion closer true
problem wish solve discounted criterion therefore prefer R-learning
Q-learning (Mahadevan, 1994).
mind, researchers studied problem learning optimal averagereward policies. Mahadevan (1996) surveyed model-based average-reward algorithms
reinforcement-learning perspective found several diculties existing algorithms.
particular, showed existing reinforcement-learning algorithms average reward
(and dynamic programming algorithms) always produce bias-optimal policies. Jaakkola, Jordan Singh (1995) described average-reward learning algorithm
guaranteed convergence properties. uses Monte-Carlo component estimate
expected future reward state agent moves environment.
254

fiReinforcement Learning: Survey

addition, Bertsekas presents Q-learning-like algorithm average-case reward new
textbook (1995). Although recent work provides much needed theoretical foundation
area reinforcement learning, many important problems remain unsolved.

5. Computing Optimal Policies Learning Models

previous section showed possible learn optimal policy without knowing
models (s; a; s0) R(s; a) without even learning models en route. Although
many methods guaranteed find optimal policies eventually use
little computation time per experience, make extremely inecient use data
gather therefore often require great deal experience achieve good performance.
section still begin assuming don't know models advance,
examine algorithms operate learning models. algorithms
especially important applications computation considered cheap
real-world experience costly.

5.1 Certainty Equivalent Methods

begin conceptually straightforward method: first, learn R
functions exploring environment keeping statistics results
action; next, compute optimal policy using one methods Section 3.
method known certainty equivlance (Kumar & Varaiya, 1986).
serious objections method:
makes arbitrary division learning phase acting phase.
gather data environment initially? Random exploration
might dangerous, environments immensely inecient method
gathering data, requiring exponentially data (Whitehead, 1991) system
interleaves experience gathering policy-building tightly (Koenig &
Simmons, 1993). See Figure 5 example.
possibility changes environment also problematic. Breaking
agent's life pure learning pure acting phase considerable risk
optimal controller based early life becomes, without detection, suboptimal
controller environment changes.
variation idea certainty equivalence, model learned continually
agent's lifetime and, step, current model used compute
optimal policy value function. method makes effective use available data,
still ignores question exploration extremely computationally demanding,
even fairly small state spaces. Fortunately, number model-based
algorithms practical.

5.2 Dyna

Sutton's Dyna architecture (1990, 1991) exploits middle ground, yielding strategies
effective model-free learning computationally ecient
255

fiKaelbling, Littman, & Moore

1

2

3

.......

n

Goal

Figure 5: environment, due Whitehead (1991), random exploration would take
take O(2n ) steps reach goal even once, whereas intelligent exploration strategy (e.g. \assume untried action leads directly goal") would
require O(n2 ) steps.
certainty-equivalence approach. simultaneously uses experience build model (T^
R^ ), uses experience adjust policy, uses model adjust policy.
Dyna operates loop interaction environment. Given experience tuple
hs; a; s0; ri, behaves follows:

Update model, incrementing statistics transition s0 action
receiving reward r taking action state s. updated models T^
R^ .

Update policy state based newly updated model using rule
X^
0
0 0
^
Q(s; a) := R(s; a) +



0

(s; a; ) max
Q(s ; ) ;

0

version value-iteration update Q values.

Perform k additional updates: choose k state-action pairs random update
according rule before:

Q(sk ; ak ):=R^(sk ; ak ) +

X^


0

(sk ; ak; s0) max
Q(s0; a0) :

0

Choose action a0 perform state s0, based Q values perhaps modified
exploration strategy.

Dyna algorithm requires k times computation Q-learning per instance,
typically vastly less naive model-based method. reasonable value
k determined based relative speeds computation taking action.
Figure 6 shows grid world cell agent four actions (N, S, E,
W) transitions made deterministically adjacent cell, unless block,
case movement occurs. see Table 1, Dyna requires order
magnitude fewer steps experience Q-learning arrive optimal policy.
Dyna requires six times computational effort, however.
256

fiReinforcement Learning: Survey

Figure 6: 3277-state grid world. formulated shortest-path reinforcementlearning problem, yields result reward 1 given
goal, reward zero elsewhere discount factor used.

Q-learning
Dyna
prioritized sweeping

Steps Backups
convergence
convergence
531,000
62,000
28,000

531,000
3,055,000
1,010,000

Table 1: performance three algorithms described text. methods used
exploration heuristic \optimism face uncertainty": state
previously visited assumed default goal state. Q-learning used
optimal learning rate parameter deterministic maze: ff = 1. Dyna
prioritized sweeping permitted take k = 200 backups per transition.
prioritized sweeping, priority queue often emptied backups
used.

257

fiKaelbling, Littman, & Moore

5.3 Prioritized Sweeping / Queue-Dyna
Although Dyna great improvement previous methods, suffers relatively
undirected. particularly unhelpful goal reached
agent stuck dead end; continues update random state-action pairs, rather
concentrating \interesting" parts state space. problems addressed
prioritized sweeping (Moore & Atkeson, 1993) Queue-Dyna (Peng & Williams,
1993), two independently-developed similar techniques. describe
prioritized sweeping detail.
algorithm similar Dyna, except updates longer chosen random
values associated states (as value iteration) instead state-action pairs
(as Q-learning). make appropriate choices, must store additional information
model. state remembers predecessors: states non-zero transition
probability action. addition, state priority, initially set
zero.
Instead updating k random state-action pairs, prioritized sweeping updates k states
highest priority. high-priority state s, works follows:

Remember current value state: Vold = V (s).
Update state's value
^
V (s) := max
R(s; a) +

X^


(s; a; s0)V (s0)

!

:

0

Set state's priority back 0.
Compute value change = jVold , V (s)j.
Use modify priorities predecessors s.
updated V value state s0 changed amount ,
immediate predecessors s0 informed event. state exists
action T^(s; a; s0) 6= 0 priority promoted T^(s; a; s0), unless
priority already exceeded value.
global behavior algorithm real-world transition \surprising"
(the agent happens upon goal state, instance), lots computation directed
propagate new information back relevant predecessor states. realworld transition \boring" (the actual result similar predicted result),
computation continues deserving part space.
Running prioritized sweeping problem Figure 6, see large improvement
Dyna. optimal policy reached half number steps experience
one-third computation Dyna required (and therefore 20 times fewer steps
twice computational effort Q-learning).
258

fiReinforcement Learning: Survey

5.4 Model-Based Methods
Methods proposed solving MDPs given model used context modelbased methods well.
RTDP (real-time dynamic programming) (Barto, Bradtke, & Singh, 1995) another
model-based method uses Q-learning concentrate computational effort areas
state-space agent likely occupy. specific problems
agent trying achieve particular goal state reward everywhere else 0.
taking account start state, find short path start goal,
without necessarily visiting rest state space.
Plexus planning system (Dean, Kaelbling, Kirman, & Nicholson, 1993; Kirman,
1994) exploits similar intuition. starts making approximate version MDP
much smaller original one. approximate MDP contains set states,
called envelope, includes agent's current state goal state, one.
States envelope summarized single \out" state. planning
process alternation finding optimal policy approximate MDP
adding useful states envelope. Action may take place parallel planning,
case irrelevant states also pruned envelope.

6. Generalization
previous discussion tacitly assumed possible enumerate state
action spaces store tables values them. Except small environments,
means impractical memory requirements. also makes inecient use experience.
large, smooth state space generally expect similar states similar values similar optimal actions. Surely, therefore, compact representation
table. problems continuous large discrete state spaces;
large continuous action spaces. problem learning large spaces addressed
generalization techniques, allow compact storage learned information
transfer knowledge \similar" states actions.
large literature generalization techniques inductive concept learning
applied reinforcement learning. However, techniques often need tailored specific
details problem. following sections, explore application standard
function-approximation techniques, adaptive resolution models, hierarchical methods
problem reinforcement learning.
reinforcement-learning architectures algorithms discussed included
storage variety mappings, including ! (policies), ! < (value functions),
! < (Q functions rewards), ! (deterministic transitions),
! [0; 1] (transition probabilities). mappings, transitions
immediate rewards, learned using straightforward supervised learning,
handled using wide variety function-approximation techniques supervised
learning support noisy training examples. Popular techniques include various neuralnetwork methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991).
CMAC (Albus, 1981), local memory-based methods (Moore, Atkeson, & Schaal, 1995),
generalizations nearest neighbor methods. mappings, especially policy
259

fiKaelbling, Littman, & Moore

mapping, typically need specialized algorithms training sets input-output pairs
available.

6.1 Generalization Input

reinforcement-learning agent's current state plays central role selection rewardmaximizing actions. Viewing agent state-free black box, description
current state input. Depending agent architecture, output either
action selection, evaluation current state used select action.
problem deciding different aspects input affect value output
sometimes called \structural credit-assignment" problem. section examines
approaches generating actions evaluations function description agent's
current state.
first group techniques covered specialized case reward
delayed; second group generally applicable.
6.1.1 Immediate Reward

agent's actions uence state transitions, resulting problem becomes
one choosing actions maximize immediate reward function agent's current
state. problems bear resemblance bandit problems discussed Section 2
except agent condition action selection current state.
reason, class problems described associative reinforcement learning.
algorithms section address problem learning immediate boolean
reinforcement state vector valued action boolean vector.
algorithms used context delayed reinforcement, instance,
RL component AHC architecture described Section 4.1. also
generalized real-valued reward reward comparison methods (Sutton, 1984).
CRBP complementary reinforcement backpropagation algorithm (Ackley & Littman,
1990) (crbp) consists feed-forward network mapping encoding state
encoding action. action determined probabilistically activation
output units: output unit activation yi , bit action vector value
1 probability yi , 0 otherwise. neural-network supervised training procedure
used adapt network follows. result generating action r = 1,
network trained input-output pair hs; ai. result r = 0,
network trained input-output pair hs; ai, = (1 , a1; : : :; 1 , ).
idea behind training rule whenever action fails generate reward,
crbp try generate action different current choice. Although
seems like algorithm might oscillate action complement,
happen. One step training network change action slightly since
output probabilities tend move toward 0.5, makes action selection
random increases search. hope random distribution generate
action works better, action reinforced.
ARC associative reinforcement comparison (arc) algorithm (Sutton, 1984)
instance ahc architecture case boolean actions, consisting two feed260

fiReinforcement Learning: Survey

forward networks. One learns value situations, learns policy.
simple linear networks hidden units.
simplest case, entire system learns optimize immediate reward. First,
let us consider behavior network learns policy, mapping vector
describing 0 1. output unit activation yi , a, action generated,
1 + > 0, normal noise, 0 otherwise.
adjustment output unit is, simplest case,

e = r(a , 1=2) ;
first factor reward received taking recent action second
encodes action taken. actions encoded 0 1, , 1=2 always
magnitude; reward action sign, action 1
made likely, otherwise action 0 be.
described, network tend seek actions given positive reward. extend
approach maximize reward, compare reward baseline, b.
changes adjustment
e = (r , b)(a , 1=2) ;
b output second network. second network trained standard
supervised mode estimate r function input state s.
Variations approach used variety applications (Anderson, 1986;
Barto et al., 1983; Lin, 1993b; Sutton, 1984).
REINFORCE Algorithms Williams (1987, 1992) studied problem choosing actions maximize immedate reward. identified broad class update rules perform gradient descent expected reward showed integrate rules
backpropagation. class, called reinforce algorithms, includes linear reward-inaction
(Section 2.1.3) special case.
generic reinforce update parameter wij written
wij = ffij (r , bij ) @w@ ln(gj )
ij
ffij non-negative factor, r current reinforcement, bij reinforcement baseline,
gi probability density function used randomly generate actions based unit
activations. ffij bij take different values wij , however, ffij
constant throughout system, expected update exactly direction
expected reward gradient. Otherwise, update half space gradient
necessarily direction steepest increase.
Williams points choice baseline, bij , profound effect
convergence speed algorithm.
Logic-Based Methods Another strategy generalization reinforcement learning
reduce learning problem associative problem learning boolean functions.
boolean function vector boolean inputs single boolean output. Taking
inspiration mainstream machine learning work, Kaelbling developed two algorithms
learning boolean functions reinforcement: one uses bias k-DNF drive
261

fiKaelbling, Littman, & Moore

generalization process (Kaelbling, 1994b); searches space syntactic
descriptions functions using simple generate-and-test method (Kaelbling, 1994a).
restriction single boolean output makes techniques dicult apply.
benign learning situations, possible extend approach use collection
learners independently learn individual bits make complex output.
general, however, approach suffers problem unreliable reinforcement:
single learner generates inappropriate output bit, learners receive low
reinforcement value. cascade method (Kaelbling, 1993b) allows collection learners
trained collectively generate appropriate joint outputs; considerably
reliable, require additional computational effort.
6.1.2 Delayed Reward

Another method allow reinforcement-learning techniques applied large state
spaces modeled value iteration Q-learning. Here, function approximator used
represent value function mapping state description value.
Many reseachers experimented approach: Boyan Moore (1995) used
local memory-based methods conjunction value iteration; Lin (1991) used backpropagation networks Q-learning; Watkins (1989) used CMAC Q-learning; Tesauro (1992,
1995) used backpropagation learning value function backgammon (described
Section 8.1); Zhang Dietterich (1995) used backpropagation TD() learn good
strategies job-shop scheduling.
Although positive examples, general unfortunate interactions function approximation learning rules. discrete environments
guarantee operation updates value function (according
Bellman equations) reduce error current value function
optimal value function. guarantee longer holds generalization used.
issues discussed Boyan Moore (1995), give simple examples value
function errors growing arbitrarily large generalization used value iteration.
solution this, applicable certain classes problems, discourages divergence permitting updates whose estimated values shown near-optimal
via battery Monte-Carlo experiments.
Thrun Schwartz (1993) theorize function approximation value functions
also dangerous errors value functions due generalization become
compounded \max" operator definition value function.
Several recent results (Gordon, 1995; Tsitsiklis & Van Roy, 1996) show appropriate choice function approximator guarantee convergence, though necessarily
optimal values. Baird's residual gradient technique (Baird, 1995) provides guaranteed
convergence locally optimal solutions.
Perhaps gloominess counter-examples misplaced. Boyan Moore (1995)
report counter-examples made work problem-specific hand-tuning
despite unreliability untuned algorithms provably converge discrete domains.
Sutton (1996) shows modified versions Boyan Moore's examples converge
successfully. open question whether general principles, ideally supported theory,
help us understand value function approximation succeed. Sutton's com262

fiReinforcement Learning: Survey

parative experiments Boyan Moore's counter-examples, changes four aspects
experiments:
1. Small changes task specifications.
2. different kind function approximator (CMAC (Albus, 1975)) weak
generalization.
3. different learning algorithm: SARSA (Rummery & Niranjan, 1994) instead value
iteration.
4. different training regime. Boyan Moore sampled states uniformly state space,
whereas Sutton's method sampled along empirical trajectories.
intuitive reasons believe fourth factor particularly important,
careful research needed.
Adaptive Resolution Models many cases, would like partition
environment regions states considered purposes
learning generating actions. Without detailed prior knowledge environment,
dicult know granularity placement partitions appropriate.
problem overcome methods use adaptive resolution; course learning,
partition constructed appropriate environment.
Decision Trees environments characterized set boolean discretevalued variables, possible learn compact decision trees representing Q values.
G-learning algorithm (Chapman & Kaelbling, 1991), works follows. starts assuming
partitioning necessary tries learn Q values entire environment
one state. parallel process, gathers statistics based individual
input bits; asks question whether bit b state description
Q values states b = 1 significantly different Q values
states b = 0. bit found, used split decision tree. Then,
process repeated leaves. method able learn small
representations Q function presence overwhelming number irrelevant,
noisy state attributes. outperformed Q-learning backpropagation simple videogame environment used McCallum (1995) (in conjunction techniques
dealing partial observability) learn behaviors complex driving-simulator.
cannot, however, acquire partitions attributes significant combination
(such needed solve parity problems).
Variable Resolution Dynamic Programming VRDP algorithm (Moore, 1991)
enables conventional dynamic programming performed real-valued multivariate
state-spaces straightforward discretization would fall prey curse dimensionality. kd-tree (similar decision tree) used partition state space coarse
regions. coarse regions refined detailed regions, parts state
space predicted important. notion importance obtained running \mental trajectories" state space. algorithm proved effective number
problems full high-resolution arrays would impractical.
disadvantage requiring guess initially valid trajectory state-space.
263

fiKaelbling, Littman, & Moore

(a)

(b)

(c)

G

G

G

Goal

Start

Figure 7: (a) two-dimensional maze problem. point robot must find path
start goal without crossing barrier lines. (b) path taken
PartiGame entire first trial. begins intense exploration find
route almost entirely enclosed start region. eventually reached
suciently high resolution, discovers gap proceeds greedily towards
goal, temporarily blocked goal's barrier region. (c)
second trial.

PartiGame Algorithm Moore's PartiGame algorithm (Moore, 1994) another solution

problem learning achieve goal configurations deterministic high-dimensional
continuous spaces learning adaptive-resolution model. also divides environment
cells; cell, actions available consist aiming neighboring cells
(this aiming accomplished local controller, must provided part
problem statement). graph cell transitions solved shortest paths online
incremental manner, minimax criterion used detect group cells
coarse prevent movement obstacles avoid limit cycles. offending
cells split higher resolution. Eventually, environment divided enough
choose appropriate actions achieving goal, unnecessary distinctions made.
important feature that, well reducing memory computational requirements,
also structures exploration state space multi-resolution manner. Given failure,
agent initially try something different rectify failure, resort
small local changes qualitatively different strategies exhausted.
Figure 7a shows two-dimensional continuous maze. Figure 7b shows performance
robot using PartiGame algorithm first trial. Figure 7c shows
second trial, started slightly different position.
fast algorithm, learning policies spaces nine dimensions less
minute. restriction current implementation deterministic environments
limits applicability, however. McCallum (1995) suggests related tree-structured
methods.
264

fiReinforcement Learning: Survey

6.2 Generalization Actions

networks described Section 6.1.1 generalize state descriptions presented
inputs. also produce outputs discrete, factored representation thus could
seen generalizing actions well.
cases actions described combinatorially, important
generalize actions avoid keeping separate statistics huge number actions
chosen. continuous action spaces, need generalization even
pronounced.
estimating Q values using neural network, possible use either distinct
network action, network distinct output action.
action space continuous, neither approach possible. alternative strategy use
single network state action input Q value output. Training
network conceptually dicult, using network find optimal action
challenge. One method local gradient-ascent search action
order find one high value (Baird & Klopf, 1993).
Gullapalli (1990, 1992) developed \neural" reinforcement-learning unit use
continuous action spaces. unit generates actions normal distribution; adjusts
mean variance based previous experience. chosen actions
performing well, variance high, resulting exploration range choices.
action performs well, mean moved direction variance decreased,
resulting tendency generate action values near successful one. method
successfully employed learn control robot arm many continuous degrees
freedom.

6.3 Hierarchical Methods

Another strategy dealing large state spaces treat hierarchy
learning problems. many cases, hierarchical solutions introduce slight sub-optimality
performance, potentially gain good deal eciency execution time, learning time,
space.
Hierarchical learners commonly structured gated behaviors, shown Figure 8.
collection behaviors map environment states low-level actions
gating function decides, based state environment, behavior's
actions switched actually executed. Maes Brooks (1990) used
version architecture individual behaviors fixed priori
gating function learned reinforcement. Mahadevan Connell (1991b) used
dual approach: fixed gating function, supplied reinforcement functions
individual behaviors, learned. Lin (1993a) Dorigo Colombetti (1995,
1994) used approach, first training behaviors training gating
function. Many hierarchical learning methods cast framework.
6.3.1 Feudal Q-learning

Feudal Q-learning (Dayan & Hinton, 1993; Watkins, 1989) involves hierarchy learning
modules. simplest case, high-level master low-level slave. master
receives reinforcement external environment. actions consist commands
265

fiKaelbling, Littman, & Moore



b1
b2

g



b3
Figure 8: structure gated behaviors.
give low-level learner. master generates particular command
slave, must reward slave taking actions satisfy command, even
result external reinforcement. master, then, learns mapping states
commands. slave learns mapping commands states external actions.
set \commands" associated reinforcement functions established advance
learning.
really instance general \gated behaviors" approach, slave
execute behaviors depending command. reinforcement functions
individual behaviors (commands) given, learning takes place simultaneously
high low levels.
6.3.2 Compositional Q-learning

Singh's compositional Q-learning (1992b, 1992a) (C-QL) consists hierarchy based
temporal sequencing subgoals. elemental tasks behaviors achieve
recognizable condition. high-level goal system achieve set conditions sequential order. achievement conditions provides reinforcement
elemental tasks, trained first achieve individual subgoals. Then, gating
function learns switch elemental tasks order achieve appropriate high-level
sequential goal. method used Tham Prager (1994) learn control
simulated multi-link robot arm.
6.3.3 Hierarchical Distance Goal

Especially consider reinforcement learning modules part larger agent architectures, important consider problems goals dynamically input
learner. Kaelbling's HDG algorithm (1993a) uses hierarchical approach solving problems goals achievement (the agent get particular state quickly
possible) given agent dynamically.
HDG algorithm works analogy navigation harbor. environment
partitioned (a priori, recent work (Ashar, 1994) addresses case learning
partition) set regions whose centers known \landmarks." agent
266

fiReinforcement Learning: Survey

office
1/5

2/5
2/5

hall

hall
+100

printer

Figure 9: example partially observable environment.
currently region goal, uses low-level actions move goal.
not, high-level information used determine next landmark shortest
path agent's closest landmark goal's closest landmark. Then, agent uses
low-level information aim toward next landmark. errors action cause deviations
path, problem; best aiming point recomputed every step.

7. Partially Observable Environments

many real-world environments, possible agent perfect
complete perception state environment. Unfortunately, complete observability
necessary learning methods based MDPs. section, consider case
agent makes observations state environment, observations
may noisy provide incomplete information. case robot, instance,
might observe whether corridor, open room, T-junction, etc.,
observations might error-prone. problem also referred problem
\incomplete perception," \perceptual aliasing," \hidden state."
section, consider extensions basic MDP framework solving
partially observable problems. resulting formal model called partially observable
Markov decision process POMDP.

7.1 State-Free Deterministic Policies

naive strategy dealing partial observability ignore it. is,
treat observations states environment try learn
behave. Figure 9 shows simple environment agent attempting get
printer oce. moves oce, good chance agent
end one two places look like \hall", require different actions
getting printer. consider states same, agent cannot
possibly behave optimally. well do?
resulting problem Markovian, Q-learning cannot guaranteed converge. Small breaches Markov requirement well handled Q-learning,
possible construct simple environments cause Q-learning oscillate (Chrisman &
267

fiKaelbling, Littman, & Moore

Littman, 1993). possible use model-based approach, however; act according
policy gather statistics transitions observations, solve
optimal policy based observations. Unfortunately, environment
Markovian, transition probabilities depend policy executed, new
policy induce new set transition probabilities. approach may yield plausible
results cases, again, guarantees.
reasonable, though, ask optimal policy (mapping observations
actions, case) is. NP-hard (Littman, 1994b) find mapping, even
best mapping poor performance. case agent trying get
printer, instance, deterministic state-free policy takes infinite number steps
reach goal average.

7.2 State-Free Stochastic Policies

improvement gained considering stochastic policies; mappings
observations probability distributions actions. randomness
agent's actions, get stuck hall forever. Jaakkola, Singh, Jordan (1995)
developed algorithm finding locally-optimal stochastic policies, finding
globally optimal policy still NP hard.
example, turns optimal stochastic policy
p agent,
2 0:6 west
state
looks
like

hall,

go
east

probability
2
,
p
probability 2 , 1 0:4. policy found solving simple (in case)
quadratic program. fact simple example produce irrational numbers
gives indication dicult problem solve exactly.

7.3 Policies Internal State

way behave truly effectively wide-range environments use memory
previous actions observations disambiguate current state. variety
approaches learning policies internal state.
Recurrent Q-learning One intuitively simple approach use recurrent neural network learn Q values. network trained using backpropagation time (or
suitable technique) learns retain \history features" predict value.
approach used number researchers (Meeden, McGraw, & Blank, 1993; Lin
& Mitchell, 1992; Schmidhuber, 1991b). seems work effectively simple problems,
suffer convergence local optima complex problems.
Classifier Systems Classifier systems (Holland, 1975; Goldberg, 1989) explicitly
developed solve problems delayed reward, including requiring short-term
memory. internal mechanism typically used pass reward back chains
decisions, called bucket brigade algorithm, bears close resemblance Q-learning.
spite early successes, original design appear handle partially observed environments robustly.
Recently, approach reexamined using insights reinforcementlearning literature, success. Dorigo comparative study Q-learning
classifier systems (Dorigo & Bersini, 1994). Cliff Ross (1994) start Wilson's zeroth268

fiReinforcement Learning: Survey


SE

b





Figure 10: Structure POMDP agent.
level classifier system (Wilson, 1995) add one two-bit memory registers. find
that, although system learn use short-term memory registers effectively,
approach unlikely scale complex environments.
Dorigo Colombetti applied classifier systems moderately complex problem
learning robot behavior immediate reinforcement (Dorigo, 1995; Dorigo & Colombetti,
1994).

Finite-history-window Approach One way restore Markov property allow

decisions based history recent observations perhaps actions. Lin
Mitchell (1992) used fixed-width finite history window learn pole balancing task.
McCallum (1995) describes \utile sux memory" learns variable-width window
serves simultaneously model environment finite-memory policy.
system excellent results complex driving-simulation domain (McCallum,
1995). Ring (1994) neural-network approach uses variable history window,
adding history necessary disambiguate situations.
POMDP Approach Another strategy consists using hidden Markov model (HMM)
techniques learn model environment, including hidden state, use
model construct perfect memory controller (Cassandra, Kaelbling, & Littman, 1994;
Lovejoy, 1991; Monahan, 1982).
Chrisman (1992) showed forward-backward algorithm learning HMMs could
adapted learning POMDPs. He, later McCallum (1993), also gave heuristic statesplitting rules attempt learn smallest possible model given environment.
resulting model used integrate information agent's observations
order make decisions.
Figure 10 illustrates basic structure perfect-memory controller. component
left state estimator, computes agent's belief state, b function
old belief state, last action a, current observation i. context, belief
state probability distribution states environment, indicating likelihood,
given agent's past experience, environment actually states.
state estimator constructed straightforwardly using estimated world model
Bayes' rule.
left problem finding policy mapping belief states action.
problem formulated MDP, dicult solve using techniques
described earlier, input space continuous. Chrisman's approach (1992)
take account future uncertainty, yields policy small amount computation. standard approach operations-research literature solve
269

fiKaelbling, Littman, & Moore

optimal policy (or close approximation thereof) based representation piecewiselinear convex function belief space. method computationally intractable,
may serve inspiration methods make approximations (Cassandra
et al., 1994; Littman, Cassandra, & Kaelbling, 1995a).

8. Reinforcement Learning Applications

One reason reinforcement learning popular serves theoretical tool
studying principles agents learning act. unsurprising also
used number researchers practical computational tool constructing
autonomous systems improve experience. applications
ranged robotics, industrial manufacturing, combinatorial search problems
computer game playing.
Practical applications provide test ecacy usefulness learning algorithms.
also inspiration deciding components reinforcement learning
framework practical importance. example, researcher real robotic task
provide data point questions as:
important optimal exploration? break learning period exploration phases exploitation phases?
useful model long-term reward: Finite horizon? Discounted?
Infinite horizon?
much computation available agent decisions
used?
prior knowledge build system, algorithms capable
using knowledge?
Let us examine set practical applications reinforcement learning, bearing
questions mind.

8.1 Game Playing

Game playing dominated Artificial Intelligence world problem domain ever since
field born. Two-player games fit established reinforcement-learning
framework since optimality criterion games one maximizing reward
face fixed environment, one maximizing reward optimal adversary
(minimax). Nonetheless, reinforcement-learning algorithms adapted work
general class games (Littman, 1994a) many researchers used reinforcement
learning environments. One application, spectacularly far ahead time,
Samuel's checkers playing system (Samuel, 1959). learned value function represented
linear function approximator, employed training scheme similar updates
used value iteration, temporal differences Q-learning.
recently, Tesauro (1992, 1994, 1995) applied temporal difference algorithm
backgammon. Backgammon approximately 1020 states, making table-based reinforcement learning impossible. Instead, Tesauro used backpropagation-based three-layer
270

fiReinforcement Learning: Survey

Training
Games

Hidden
Units

300,000

80

TD 2.0

800,000

40

TD 2.1

1,500,000

80

Basic
TD 1.0

Results
Poor
Lost 13 points 51
games
Lost 7 points 38
games
Lost 1 point 40
games

Table 2: TD-Gammon's performance games top human professional players.
backgammon tournament involves playing series games points one
player reaches set target. TD-Gammon none tournaments came
suciently close considered one best players world.
neural network function approximator value function
Board Position ! Probability victory current player:

Two versions learning algorithm used. first, call Basic TDGammon, used little predefined knowledge game, representation
board position virtually raw encoding, suciently powerful permit neural
network distinguish conceptually different positions. second, TD-Gammon,
provided raw state information supplemented number handcrafted features backgammon board positions. Providing hand-crafted features
manner good example inductive biases human knowledge task
supplied learning algorithm.
training learning algorithms required several months computer time,
achieved constant self-play. exploration strategy used|the system always
greedily chose move largest expected probability victory. naive exploration strategy proved entirely adequate environment, perhaps surprising
given considerable work reinforcement-learning literature produced
numerous counter-examples show greedy exploration lead poor learning performance. Backgammon, however, two important properties. Firstly, whatever policy
followed, every game guaranteed end finite time, meaning useful reward
information obtained fairly frequently. Secondly, state transitions suciently
stochastic independent policy, states occasionally visited|a wrong
initial value function little danger starving us visiting critical part state
space important information could obtained.
results (Table 2) TD-Gammon impressive. competed top
level international human play. Basic TD-Gammon played respectably,
professional standard.
271

fiFigure 11: Schaal Atkeson's devil-sticking robot. tapered stick hit alternately
two hand sticks. task keep devil stick falling
many hits possible. robot three motors indicated torque
vectors 1; 2; 3.
Although experiments games cases produced interesting learning
behavior, success close TD-Gammon repeated. games
studied include Go (Schraudolph, Dayan, & Sejnowski, 1994) Chess (Thrun,
1995). still open question success TD-Gammon
repeated domains.

8.2 Robotics Control
recent years many robotics control applications used
reinforcement learning. concentrate following four examples, although
many interesting ongoing robotics investigations underway.
1. Schaal Atkeson (1994) constructed two-armed robot, shown Figure 11,
learns juggle device known devil-stick. complex non-linear control
task involving six-dimensional state space less 200 msecs per control decision. 40 initial attempts robot learns keep juggling hundreds
hits. typical human learning task requires order magnitude practice
achieve proficiency mere tens hits.
juggling robot learned world model experience, generalized
unvisited states function approximation scheme known locally weighted
regression (Cleveland & Delvin, 1988; Moore & Atkeson, 1992). trial,
form dynamic programming specific linear control policies locally linear
transitions used improve policy. form dynamic programming
known linear-quadratic-regulator design (Sage & White, 1977).
272

fiReinforcement Learning: Survey

2. Mahadevan Connell (1991a) discuss task mobile robot pushes large
boxes extended periods time. Box-pushing well-known dicult robotics
problem, characterized immense uncertainty results actions. Q-learning
used conjunction novel clustering techniques designed enable
higher-dimensional input tabular approach would permitted. robot
learned perform competitively performance human-programmed solution. Another aspect work, mentioned Section 6.3, pre-programmed
breakdown monolithic task description set lower level tasks
learned.
3. Mataric (1994) describes robotics experiment with, viewpoint theoretical reinforcement learning, unthinkably high dimensional state space, containing
many dozens degrees freedom. Four mobile robots traveled within enclosure collecting small disks transporting destination region.
three enhancements basic Q-learning algorithm. Firstly, pre-programmed signals called progress estimators used break monolithic task subtasks.
achieved robust manner robots forced use
estimators, freedom profit inductive bias provided.
Secondly, control decentralized. robot learned policy independently
without explicit communication others. Thirdly, state space brutally
quantized small number discrete states according values small number pre-programmed boolean features underlying sensors. performance
Q-learned policies almost good simple hand-crafted controller
job.
4. Q-learning used elevator dispatching task (Crites & Barto, 1996).
problem, implemented simulation stage, involved four
elevators servicing ten oors. objective minimize average squared
wait time passengers, discounted future time. problem posed
discrete Markov system, 1022 states even simplified version
problem. Crites Barto used neural networks function approximation
provided excellent comparison study Q-learning approach
popular sophisticated elevator dispatching algorithms. squared wait
time controller approximately 7% less best alternative algorithm
(\Empty System" heuristic receding horizon controller) less half
squared wait time controller frequently used real elevator systems.
5. final example concerns application reinforcement learning one
authors survey packaging task food processing industry.
problem involves filling containers variable numbers non-identical products.
product characteristics also vary time, sensed. Depending
task, various constraints placed container-filling procedure.
three examples:

mean weight containers produced shift must
manufacturer's declared weight W .

273

fiKaelbling, Littman, & Moore

number containers declared weight must less P %.
containers may produced weight W 0.
tasks controlled machinery operates according various setpoints.
Conventional practice setpoints chosen human operators, choice
easy dependent current product characteristics current
task constraints. dependency often dicult model highly non-linear.
task posed finite-horizon Markov decision task state
system function product characteristics, amount time remaining
production shift mean wastage percent declared shift
far. system discretized 200,000 discrete states local weighted
regression used learn generalize transition model. Prioritized sweeping used maintain optimal value function new piece transition
information obtained. simulated experiments savings considerable,
typically wastage reduced factor ten. Since system
deployed successfully several factories within United States.
interesting aspects practical reinforcement learning come light
examples. striking cases, make real system work proved
necessary supplement fundamental algorithm extra pre-programmed knowledge.
Supplying extra knowledge comes price: human effort insight required
system subsequently less autonomous. also clear tasks
these, knowledge-free approach would achieved worthwhile performance within
finite lifetime robots.
forms pre-programmed knowledge take? included assumption
linearity juggling robot's policy, manual breaking task subtasks
two mobile-robot examples, box-pusher also used clustering technique
Q values assumed locally consistent Q values. four disk-collecting robots
additionally used manually discretized state space. packaging example far fewer
dimensions required correspondingly weaker assumptions, there, too, assumption local piecewise continuity transition model enabled massive reductions
amount learning data required.
exploration strategies interesting too. juggler used careful statistical analysis judge profitably experiment. However, mobile robot applications
able learn well greedy exploration|always exploiting without deliberate exploration. packaging task used optimism face uncertainty. None
strategies mirrors theoretically optimal (but computationally intractable) exploration,
yet proved adequate.
Finally, also worth considering computational regimes experiments.
different, indicates differing computational demands
various reinforcement learning algorithms indeed array differing applications.
juggler needed make fast decisions low latency hit,
long periods (30 seconds more) trial consolidate experiences
collected previous trial perform aggressive computation necessary
produce new reactive controller next trial. box-pushing robot meant
274

fiReinforcement Learning: Survey

operate autonomously hours make decisions uniform length control
cycle. cycle suciently long quite substantial computations beyond simple Qlearning backups. four disk-collecting robots particularly interesting. robot
short life less 20 minutes (due battery constraints) meaning substantial
number crunching impractical, significant combinatorial search would
used significant fraction robot's learning lifetime. packaging task easy
constraints. One decision needed every minutes. provided opportunities
fully computing optimal value function 200,000-state system every
control cycle, addition performing massive cross-validation-based optimization
transition model learned.
great deal work currently progress practical implementations
reinforcement learning. insights task constraints produce
important effect shaping kind algorithms developed future.

9. Conclusions

variety reinforcement-learning techniques work effectively variety
small problems. techniques scale well larger problems.
researchers done bad job inventing learning techniques,
dicult solve arbitrary problems general case. order solve highly
complex problems, must give tabula rasa learning techniques begin incorporate
bias give leverage learning process.
necessary bias come variety forms, including following:
shaping: technique shaping used training animals (Hilgard & Bower, 1975);
teacher presents simple problems solve first, gradually exposes learner
complex problems. Shaping used supervised-learning systems,
used train hierarchical reinforcement-learning systems bottom
(Lin, 1991), alleviate problems delayed reinforcement decreasing
delay problem well understood (Dorigo & Colombetti, 1994; Dorigo, 1995).
local reinforcement signals: Whenever possible, agents given reinforcement
signals local. applications possible compute gradient,
rewarding agent taking steps gradient, rather achieving
final goal, speed learning significantly (Mataric, 1994).
imitation: agent learn \watching" another agent perform task (Lin, 1991).
real robots, requires perceptual abilities yet available.
another strategy human supply appropriate motor commands robot
joystick steering wheel (Pomerleau, 1993).
problem decomposition: Decomposing huge learning problem collection smaller
ones, providing useful reinforcement signals subproblems powerful technique biasing learning. interesting examples robotic reinforcement
learning employ technique extent (Connell & Mahadevan, 1993).
exes: One thing keeps agents know nothing learning anything
hard time even finding interesting parts space; wander
275

fiKaelbling, Littman, & Moore

around random never getting near goal, always \killed" immediately.
problems ameliorated programming set \re exes" cause
agent act initially way reasonable (Mataric, 1994; Singh, Barto,
Grupen, & Connolly, 1994). exes eventually overridden
detailed accurate learned knowledge, least keep agent alive
pointed right direction trying learn. Recent work Millan (1996)
explores use exes make robot learning safer ecient.
appropriate biases, supplied human programmers teachers, complex reinforcementlearning problems eventually solvable. still much work done many
interesting questions remaining learning techniques especially regarding methods
approximating, decomposing, incorporating bias problems.

Acknowledgements
Thanks Marco Dorigo three anonymous reviewers comments helped
improve paper. Also thanks many colleagues reinforcement-learning
community done work explained us.
Leslie Pack Kaelbling supported part NSF grants IRI-9453383 IRI9312395. Michael Littman supported part Bellcore. Andrew Moore supported
part NSF Research Initiation Award 3M Corporation.

References

Ackley, D. H., & Littman, M. L. (1990). Generalization scaling reinforcement learning. Touretzky, D. S. (Ed.), Advances Neural Information Processing Systems
2, pp. 550{557 San Mateo, CA. Morgan Kaufmann.
Albus, J. S. (1975). new approach manipulator control: Cerebellar model articulation
controller (cmac). Journal Dynamic Systems, Measurement Control, 97, 220{
227.
Albus, J. S. (1981). Brains, Behavior, Robotics. BYTE Books, Subsidiary McGrawHill, Peterborough, New Hampshire.
Anderson, C. W. (1986). Learning Problem Solving Multilayer Connectionist
Systems. Ph.D. thesis, University Massachusetts, Amherst, MA.
Ashar, R. R. (1994). Hierarchical learning stochastic domains. Master's thesis, Brown
University, Providence, Rhode Island.
Baird, L. (1995). Residual algorithms: Reinforcement learning function approximation. Prieditis, A., & Russell, S. (Eds.), Proceedings Twelfth International
Conference Machine Learning, pp. 30{37 San Francisco, CA. Morgan Kaufmann.
Baird, L. C., & Klopf, A. H. (1993). Reinforcement learning high-dimensional, continuous actions. Tech. rep. WL-TR-93-1147, Wright-Patterson Air Force Base Ohio:
Wright Laboratory.
276

fiReinforcement Learning: Survey

Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic
programming. Artificial Intelligence, 72 (1), 81{138.
Barto, A. G., Sutton, R. S., & Anderson, C. W. (1983). Neuronlike adaptive elements
solve dicult learning control problems. IEEE Transactions Systems, Man,
Cybernetics, SMC-13 (5), 834{846.
Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ.
Berenji, H. R. (1991). Artificial neural networks approximate reasoning intelligent
control space. American Control Conference, pp. 1075{1080.
Berry, D. A., & Fristedt, B. (1985). Bandit Problems: Sequential Allocation Experiments.
Chapman Hall, London, UK.
Bertsekas, D. P. (1987). Dynamic Programming: Deterministic Stochastic Models.
Prentice-Hall, Englewood Cliffs, NJ.
Bertsekas, D. P. (1995). Dynamic Programming Optimal Control. Athena Scientific,
Belmont, Massachusetts. Volumes 1 2.
Bertsekas, D. P., & Casta~non, D. A. (1989). Adaptive aggregation infinite horizon
dynamic programming. IEEE Transactions Automatic Control, 34 (6), 589{598.
Bertsekas, D. P., & Tsitsiklis, J. N. (1989). Parallel Distributed Computation: Numerical Methods. Prentice-Hall, Englewood Cliffs, NJ.
Box, G. E. P., & Draper, N. R. (1987). Empirical Model-Building Response Surfaces.
Wiley.
Boyan, J. A., & Moore, A. W. (1995). Generalization reinforcement learning: Safely
approximating value function. Tesauro, G., Touretzky, D. S., & Leen, T. K.
(Eds.), Advances Neural Information Processing Systems 7 Cambridge, MA.
MIT Press.
Burghes, D., & Graham, A. (1980). Introduction Control Theory including Optimal
Control. Ellis Horwood.
Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally partially
observable stochastic domains. Proceedings Twelfth National Conference
Artificial Intelligence Seattle, WA.
Chapman, D., & Kaelbling, L. P. (1991). Input generalization delayed reinforcement
learning: algorithm performance comparisons. Proceedings International Joint Conference Artificial Intelligence Sydney, Australia.
Chrisman, L. (1992). Reinforcement learning perceptual aliasing: perceptual
distinctions approach. Proceedings Tenth National Conference Artificial
Intelligence, pp. 183{188 San Jose, CA. AAAI Press.
277

fiKaelbling, Littman, & Moore

Chrisman, L., & Littman, M. (1993). Hidden state short-term memory.. Presentation
Reinforcement Learning Workshop, Machine Learning Conference.
Cichosz, P., & Mulawka, J. J. (1995). Fast ecient reinforcement learning truncated temporal differences. Prieditis, A., & Russell, S. (Eds.), Proceedings
Twelfth International Conference Machine Learning, pp. 99{107 San Francisco,
CA. Morgan Kaufmann.
Cleveland, W. S., & Delvin, S. J. (1988). Locally weighted regression: approach
regression analysis local fitting. Journal American Statistical Association,
83 (403), 596{610.
Cliff, D., & Ross, S. (1994). Adding temporary memory ZCS. Adaptive Behavior, 3 (2),
101{150.
Condon, A. (1992). complexity stochastic games. Information Computation,
96 (2), 203{224.
Connell, J., & Mahadevan, S. (1993). Rapid task learning real robots. Robot Learning.
Kluwer Academic Publishers.
Crites, R. H., & Barto, A. G. (1996). Improving elevator performance using reinforcement
learning. Touretzky, D., Mozer, M., & Hasselmo, M. (Eds.), Neural Information
Processing Systems 8.
Dayan, P. (1992). convergence TD() general . Machine Learning, 8 (3), 341{
362.
Dayan, P., & Hinton, G. E. (1993). Feudal reinforcement learning. Hanson, S. J., Cowan,
J. D., & Giles, C. L. (Eds.), Advances Neural Information Processing Systems 5
San Mateo, CA. Morgan Kaufmann.
Dayan, P., & Sejnowski, T. J. (1994). TD() converges probability 1. Machine Learning, 14 (3).
Dean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. (1993). Planning deadlines
stochastic domains. Proceedings Eleventh National Conference Artificial
Intelligence Washington, DC.
D'Epenoux, F. (1963). probabilistic production inventory problem. Management
Science, 10, 98{108.
Derman, C. (1970). Finite State Markovian Decision Processes. Academic Press, New York.
Dorigo, M., & Bersini, H. (1994). comparison q-learning classifier systems.
Animals Animats: Proceedings Third International Conference
Simulation Adaptive Behavior Brighton, UK.
Dorigo, M., & Colombetti, M. (1994). Robot shaping: Developing autonomous agents
learning. Artificial Intelligence, 71 (2), 321{370.
278

fiReinforcement Learning: Survey

Dorigo, M. (1995). Alecsys AutonoMouse: Learning control real robot
distributed classifier systems. Machine Learning, 19.
Fiechter, C.-N. (1994). Ecient reinforcement learning. Proceedings Seventh
Annual ACM Conference Computational Learning Theory, pp. 88{97. Association
Computing Machinery.
Gittins, J. C. (1989). Multi-armed Bandit Allocation Indices. Wiley-Interscience series
systems optimization. Wiley, Chichester, NY.
Goldberg, D. (1989). Genetic algorithms search, optimization, machine learning.
Addison-Wesley, MA.
Gordon, G. J. (1995). Stable function approximation dynamic programming. Prieditis, A., & Russell, S. (Eds.), Proceedings Twelfth International Conference
Machine Learning, pp. 261{268 San Francisco, CA. Morgan Kaufmann.
Gullapalli, V. (1990). stochastic reinforcement learning algorithm learning real-valued
functions. Neural Networks, 3, 671{692.
Gullapalli, V. (1992). Reinforcement learning application control. Ph.D. thesis,
University Massachusetts, Amherst, MA.
Hilgard, E. R., & Bower, G. H. (1975). Theories Learning (fourth edition). Prentice-Hall,
Englewood Cliffs, NJ.
Hoffman, A. J., & Karp, R. M. (1966). nonterminating stochastic games. Management
Science, 12, 359{370.
Holland, J. H. (1975). Adaptation Natural Artificial Systems. University Michigan
Press, Ann Arbor, MI.
Howard, R. A. (1960). Dynamic Programming Markov Processes. MIT Press,
Cambridge, MA.
Jaakkola, T., Jordan, M. I., & Singh, S. P. (1994). convergence stochastic iterative
dynamic programming algorithms. Neural Computation, 6 (6).
Jaakkola, T., Singh, S. P., & Jordan, M. I. (1995). Monte-carlo reinforcement learning
non-Markovian decision problems. Tesauro, G., Touretzky, D. S., & Leen, T. K.
(Eds.), Advances Neural Information Processing Systems 7 Cambridge, MA.
MIT Press.
Kaelbling, L. P. (1993a). Hierarchical learning stochastic domains: Preliminary results.
Proceedings Tenth International Conference Machine Learning Amherst,
MA. Morgan Kaufmann.
Kaelbling, L. P. (1993b). Learning Embedded Systems. MIT Press, Cambridge, MA.
Kaelbling, L. P. (1994a). Associative reinforcement learning: generate test algorithm.
Machine Learning, 15 (3).
279

fiKaelbling, Littman, & Moore

Kaelbling, L. P. (1994b). Associative reinforcement learning: Functions k-DNF. Machine
Learning, 15 (3).
Kirman, J. (1994). Predicting Real-Time Planner Performance Domain Characterization.
Ph.D. thesis, Department Computer Science, Brown University.
Koenig, S., & Simmons, R. G. (1993). Complexity analysis real-time reinforcement
learning. Proceedings Eleventh National Conference Artificial Intelligence,
pp. 99{105 Menlo Park, California. AAAI Press/MIT Press.
Kumar, P. R., & Varaiya, P. P. (1986). Stochastic Systems: Estimation, Identification,
Adaptive Control. Prentice Hall, Englewood Cliffs, New Jersey.
Lee, C. C. (1991). self learning rule-based controller employing approximate reasoning
neural net concepts. International Journal Intelligent Systems, 6 (1), 71{93.
Lin, L.-J. (1991). Programming robots using reinforcement learning teaching.
Proceedings Ninth National Conference Artificial Intelligence.
Lin, L.-J. (1993a). Hierachical learning robot skills reinforcement. Proceedings
International Conference Neural Networks.
Lin, L.-J. (1993b). Reinforcement Learning Robots Using Neural Networks. Ph.D. thesis,
Carnegie Mellon University, Pittsburgh, PA.
Lin, L.-J., & Mitchell, T. M. (1992). Memory approaches reinforcement learning nonMarkovian domains. Tech. rep. CMU-CS-92-138, Carnegie Mellon University, School
Computer Science.
Littman, M. L. (1994a). Markov games framework multi-agent reinforcement learning. Proceedings Eleventh International Conference Machine Learning,
pp. 157{163 San Francisco, CA. Morgan Kaufmann.
Littman, M. L. (1994b). Memoryless policies: Theoretical limitations practical results.
Cliff, D., Husbands, P., Meyer, J.-A., & Wilson, S. W. (Eds.), Animals
Animats 3: Proceedings Third International Conference Simulation
Adaptive Behavior Cambridge, MA. MIT Press.
Littman, M. L., Cassandra, A., & Kaelbling, L. P. (1995a). Learning policies partially
observable environments: Scaling up. Prieditis, A., & Russell, S. (Eds.), Proceedings Twelfth International Conference Machine Learning, pp. 362{370 San
Francisco, CA. Morgan Kaufmann.
Littman, M. L., Dean, T. L., & Kaelbling, L. P. (1995b). complexity solving
Markov decision problems. Proceedings Eleventh Annual Conference
Uncertainty Artificial Intelligence (UAI{95) Montreal, Quebec, Canada.
Lovejoy, W. S. (1991). survey algorithmic methods partially observable Markov
decision processes. Annals Operations Research, 28, 47{66.
280

fiReinforcement Learning: Survey

Maes, P., & Brooks, R. A. (1990). Learning coordinate behaviors. Proceedings Eighth
National Conference Artificial Intelligence, pp. 796{802. Morgan Kaufmann.
Mahadevan, S. (1994). discount discount reinforcement learning: case
study comparing R learning Q learning. Proceedings Eleventh International Conference Machine Learning, pp. 164{172 San Francisco, CA. Morgan
Kaufmann.
Mahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms,
empirical results. Machine Learning, 22 (1).
Mahadevan, S., & Connell, J. (1991a). Automatic programming behavior-based robots
using reinforcement learning. Proceedings Ninth National Conference
Artificial Intelligence Anaheim, CA.
Mahadevan, S., & Connell, J. (1991b). Scaling reinforcement learning robotics exploiting subsumption architecture. Proceedings Eighth International
Workshop Machine Learning, pp. 328{332.
Mataric, M. J. (1994). Reward functions accelerated learning. Cohen, W. W., &
Hirsh, H. (Eds.), Proceedings Eleventh International Conference Machine
Learning. Morgan Kaufmann.
McCallum, A. K. (1995). Reinforcement Learning Selective Perception Hidden
State. Ph.D. thesis, Department Computer Science, University Rochester.
McCallum, R. A. (1993). Overcoming incomplete perception utile distinction memory.
Proceedings Tenth International Conference Machine Learning, pp. 190{
196 Amherst, Massachusetts. Morgan Kaufmann.
McCallum, R. A. (1995). Instance-based utile distinctions reinforcement learning
hidden state. Proceedings Twelfth International Conference Machine Learning, pp. 387{395 San Francisco, CA. Morgan Kaufmann.
Meeden, L., McGraw, G., & Blank, D. (1993). Emergent control planning autonomous vehicle. Touretsky, D. (Ed.), Proceedings Fifteenth Annual Meeting
Cognitive Science Society, pp. 735{740. Lawerence Erlbaum Associates, Hillsdale, NJ.
Millan, J. d. R. (1996). Rapid, safe, incremental learning navigation strategies. IEEE
Transactions Systems, Man, Cybernetics, 26 (3).
Monahan, G. E. (1982). survey partially observable Markov decision processes: Theory,
models, algorithms. Management Science, 28, 1{16.
Moore, A. W. (1991). Variable resolution dynamic programming: Eciently learning action maps multivariate real-valued spaces. Proc. Eighth International Machine
Learning Workshop.
281

fiKaelbling, Littman, & Moore

Moore, A. W. (1994). parti-game algorithm variable resolution reinforcement learning multidimensional state-spaces. Cowan, J. D., Tesauro, G., & Alspector, J.
(Eds.), Advances Neural Information Processing Systems 6, pp. 711{718 San Mateo,
CA. Morgan Kaufmann.
Moore, A. W., & Atkeson, C. G. (1992). investigation memory-based function approximators learning control. Tech. rep., MIT Artifical Intelligence Laboratory,
Cambridge, MA.
Moore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning
less data less real time. Machine Learning, 13.
Moore, A. W., Atkeson, C. G., & Schaal, S. (1995). Memory-based learning control.
Tech. rep. CMU-RI-TR-95-18, CMU Robotics Institute.
Narendra, K., & Thathachar, M. A. L. (1989). Learning Automata: Introduction.
Prentice-Hall, Englewood Cliffs, NJ.
Narendra, K. S., & Thathachar, M. A. L. (1974). Learning automata|a survey. IEEE
Transactions Systems, Man, Cybernetics, 4 (4), 323{334.
Peng, J., & Williams, R. J. (1993). Ecient learning planning within Dyna framework. Adaptive Behavior, 1 (4), 437{454.
Peng, J., & Williams, R. J. (1994). Incremental multi-step Q-learning. Proceedings
Eleventh International Conference Machine Learning, pp. 226{232 San Francisco,
CA. Morgan Kaufmann.
Pomerleau, D. A. (1993). Neural network perception mobile robot guidance. Kluwer
Academic Publishing.
Puterman, M. L. (1994). Markov Decision Processes|Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY.
Puterman, M. L., & Shin, M. C. (1978). Modified policy iteration algorithms discounted
Markov decision processes. Management Science, 24, 1127{1137.
Ring, M. B. (1994). Continual Learning Reinforcement Environments. Ph.D. thesis,
University Texas Austin, Austin, Texas.
Rude, U. (1993). Mathematical computational techniques multilevel adaptive methods. Society Industrial Applied Mathematics, Philadelphia, Pennsylvania.
Rumelhart, D. E., & McClelland, J. L. (Eds.). (1986). Parallel Distributed Processing:
Explorations microstructures cognition. Volume 1: Foundations. MIT
Press, Cambridge, MA.
Rummery, G. A., & Niranjan, M. (1994). On-line Q-learning using connectionist systems.
Tech. rep. CUED/F-INFENG/TR166, Cambridge University.
282

fiReinforcement Learning: Survey

Rust, J. (1996). Numerical dynamic programming economics. Handbook Computational Economics. Elsevier, North Holland.
Sage, A. P., & White, C. C. (1977). Optimum Systems Control. Prentice Hall.
Salganicoff, M., & Ungar, L. H. (1995). Active exploration learning real-valued
spaces using multi-armed bandit allocation indices. Prieditis, A., & Russell, S.
(Eds.), Proceedings Twelfth International Conference Machine Learning,
pp. 480{487 San Francisco, CA. Morgan Kaufmann.
Samuel, A. L. (1959). studies machine learning using game checkers. IBM
Journal Research Development, 3, 211{229. Reprinted E. A. Feigenbaum
J. Feldman, editors, Computers Thought, McGraw-Hill, New York 1963.
Schaal, S., & Atkeson, C. (1994). Robot juggling: implementation memory-based
learning. Control Systems Magazine, 14.
Schmidhuber, J. (1996). general method multi-agent learning incremental selfimprovement unrestricted environments. Yao, X. (Ed.), Evolutionary Computation: Theory Applications. Scientific Publ. Co., Singapore.
Schmidhuber, J. H. (1991a). Curious model-building control systems. Proc. International
Joint Conference Neural Networks, Singapore, Vol. 2, pp. 1458{1463. IEEE.
Schmidhuber, J. H. (1991b). Reinforcement learning Markovian non-Markovian
environments. Lippman, D. S., Moody, J. E., & Touretzky, D. S. (Eds.), Advances
Neural Information Processing Systems 3, pp. 500{506 San Mateo, CA. Morgan
Kaufmann.
Schraudolph, N. N., Dayan, P., & Sejnowski, T. J. (1994). Temporal difference learning
position evaluation game Go. Cowan, J. D., Tesauro, G., & Alspector,
J. (Eds.), Advances Neural Information Processing Systems 6, pp. 817{824 San
Mateo, CA. Morgan Kaufmann.
Schrijver, A. (1986). Theory Linear Integer Programming. Wiley-Interscience, New
York, NY.
Schwartz, A. (1993). reinforcement learning method maximizing undiscounted rewards. Proceedings Tenth International Conference Machine Learning,
pp. 298{305 Amherst, Massachusetts. Morgan Kaufmann.
Singh, S. P., Barto, A. G., Grupen, R., & Connolly, C. (1994). Robust reinforcement
learning motion planning. Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.),
Advances Neural Information Processing Systems 6, pp. 655{662 San Mateo, CA.
Morgan Kaufmann.
Singh, S. P., & Sutton, R. S. (1996). Reinforcement learning replacing eligibility traces.
Machine Learning, 22 (1).
283

fiKaelbling, Littman, & Moore

Singh, S. P. (1992a). Reinforcement learning hierarchy abstract models.
Proceedings Tenth National Conference Artificial Intelligence, pp. 202{207
San Jose, CA. AAAI Press.
Singh, S. P. (1992b). Transfer learning composing solutions elemental sequential
tasks. Machine Learning, 8 (3), 323{340.
Singh, S. P. (1993). Learning Solve Markovian Decision Processes. Ph.D. thesis, Department Computer Science, University Massachusetts. Also, CMPSCI Technical
Report 93-77.
Stengel, R. F. (1986). Stochastic Optimal Control. John Wiley Sons.
Sutton, R. S. (1996). Generalization Reinforcement Learning: Successful Examples Using
Sparse Coarse Coding. Touretzky, D., Mozer, M., & Hasselmo, M. (Eds.), Neural
Information Processing Systems 8.
Sutton, R. S. (1984). Temporal Credit Assignment Reinforcement Learning. Ph.D. thesis,
University Massachusetts, Amherst, MA.
Sutton, R. S. (1988). Learning predict method temporal differences. Machine
Learning, 3 (1), 9{44.
Sutton, R. S. (1990). Integrated architectures learning, planning, reacting based
approximating dynamic programming. Proceedings Seventh International
Conference Machine Learning Austin, TX. Morgan Kaufmann.
Sutton, R. S. (1991). Planning incremental dynamic programming. Proceedings
Eighth International Workshop Machine Learning, pp. 353{357. Morgan
Kaufmann.
Tesauro, G. (1992). Practical issues temporal difference learning. Machine Learning, 8,
257{277.
Tesauro, G. (1994). TD-Gammon, self-teaching backgammon program, achieves masterlevel play. Neural Computation, 6 (2), 215{219.
Tesauro, G. (1995). Temporal difference learning TD-Gammon. Communications
ACM, 38 (3), 58{67.
Tham, C.-K., & Prager, R. W. (1994). modular q-learning architecture manipulator task decomposition. Proceedings Eleventh International Conference
Machine Learning San Francisco, CA. Morgan Kaufmann.
Thrun, S. (1995). Learning play game chess. Tesauro, G., Touretzky, D. S., &
Leen, T. K. (Eds.), Advances Neural Information Processing Systems 7 Cambridge,
MA. MIT Press.
284

fiReinforcement Learning: Survey

Thrun, S., & Schwartz, A. (1993). Issues using function approximation reinforcement
learning. Mozer, M., Smolensky, P., Touretzky, D., Elman, J., & Weigend, A.
(Eds.), Proceedings 1993 Connectionist Models Summer School Hillsdale, NJ.
Lawrence Erlbaum.
Thrun, S. B. (1992). role exploration learning control. White, D. A., &
Sofge, D. A. (Eds.), Handbook Intelligent Control: Neural, Fuzzy, Adaptive
Approaches. Van Nostrand Reinhold, New York, NY.
Tsitsiklis, J. N. (1994). Asynchronous stochastic approximation Q-learning. Machine
Learning, 16 (3).
Tsitsiklis, J. N., & Van Roy, B. (1996). Feature-based methods large scale dynamic
programming. Machine Learning, 22 (1).
Valiant, L. G. (1984). theory learnable. Communications ACM, 27 (11),
1134{1142.
Watkins, C. J. C. H. (1989). Learning Delayed Rewards. Ph.D. thesis, King's College,
Cambridge, UK.
Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8 (3), 279{292.
Whitehead, S. D. (1991). Complexity cooperation Q-learning. Proceedings
Eighth International Workshop Machine Learning Evanston, IL. Morgan Kaufmann.
Williams, R. J. (1987). class gradient-estimating algorithms reinforcement learning
neural networks. Proceedings IEEE First International Conference
Neural Networks San Diego, CA.
Williams, R. J. (1992). Simple statistical gradient-following algorithms connectionist
reinforcement learning. Machine Learning, 8 (3), 229{256.
Williams, R. J., & Baird, III, L. C. (1993a). Analysis incremental variants policy
iteration: First steps toward understanding actor-critic learning systems. Tech. rep.
NU-CCS-93-11, Northeastern University, College Computer Science, Boston, MA.
Williams, R. J., & Baird, III, L. C. (1993b). Tight performance bounds greedy policies
based imperfect value functions. Tech. rep. NU-CCS-93-14, Northeastern University, College Computer Science, Boston, MA.
Wilson, S. (1995). Classifier fitness based accuracy. Evolutionary Computation, 3 (2),
147{173.
Zhang, W., & Dietterich, T. G. (1995). reinforcement learning approach job-shop
scheduling. Proceedings International Joint Conference Artificial Intellience.
285

fiJournal Artificial Intelligence Research 4 (1996) 397-417

Submitted 12/95; published 6/96

Experimental Evidence Utility
Occam's Razor
Geoffrey I. Webb

webb@deakin.edu.au

School Computing Mathematics
Deakin University
Geelong, Vic, 3217, Australia.

Abstract

paper presents new experimental evidence utility Occam's razor.
systematic procedure presented post-processing decision trees produced C4.5.
procedure derived rejecting Occam's razor instead attending assumption similar objects likely belong class. increases decision
tree's complexity without altering performance tree training data
inferred. resulting complex decision trees demonstrated have,
average, variety common learning tasks, higher predictive accuracy less
complex original decision trees. result raises considerable doubt utility
Occam's razor commonly applied modern machine learning.

1. Introduction

fourteenth century William Occam stated \plurality assumed without necessity". principle since become known Occam's razor. Occam's razor
originally intended basis determining one's ontology. However, modern times
widely reinterpreted adopted epistemological principle|a means
selecting alternative theories well ontologies. Modern reinterpretations
Occam's razor widely employed classification learning. However, utility
principle subject widespread theoretical experimental attack. paper
adds debate providing experimental evidence utility
modern interpretation Occam's razor. evidence takes form systematic procedure adding non-redundant complexity classifiers manner demonstrated
frequently improve predictive accuracy.
modern interpretation Occam's razor characterized \of two hypotheses H H0 , explain E, simpler preferred" (Good, 1977).
However, specify aspect theory measured simplicity.
Syntactic, semantic, epistemological pragmatic simplicity alternative criteria
employed Bunge (1963). practice, common use Occam's razor
machine learning seeks minimize surface syntactic complexity. interpretation
paper addresses.
assumed Occam's razor usually applied expectation
application will, general, lead particular form advantage. widely
accepted articulation precisely Occam's razor applied advantages
expected application classification learning. However, literature
contain two statements seem capture least one widely adopted approach

c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiWebb

principle. Blumer, Ehrenfeucht, Haussler, Warmuth (1987) suggest wield
Occam's razor adopt goal discovering \the simplest hypothesis consistent
sample data" expectation simplest hypothesis \perform well
observations taken source". Quinlan (1986) states
\Given choice two decision trees, correct
training set, seems sensible prefer simpler one grounds
likely capture structure inherent problem. simpler tree would
therefore expected classify correctly objects outside training set."
statements would necessarily accepted proponents Occam's
razor, capture form Occam's razor paper seeks address|a learning
bias toward classifiers minimize surface syntactic complexity expectation
maximizing predictive accuracy.
statements Occam's razor restrict classifiers
correctly classify objects training set. Many modern machine learning systems
incorporate learning biases tolerate small levels misclassification training data
(Clark & Niblett, 1989; Michalski, 1984; Quinlan, 1986, 1990, example). context,
extending scope definition beyond decision trees classifiers general,
seems reasonable modify Quinlan's (1986) statement (above)
Given choice two plausible classifiers perform identically
training set, simpler classifier expected classify correctly objects
outside training set.
referred Occam thesis.
concept identical performance training set could defined many different
ways. might tempting opt definition requires identical error rates
two classifiers applied training set. less strict interpretation might allow two
classifiers differing error rates long difference within statistical
confidence limit. However, maximize applicability results, paper adopt
strict interpretation identical performance|that every object training
set, classifiers provide classification o.
noted Occam thesis claiming two classifiers
equal empirical support least complex always greater predictive accuracy
previously unseen objects. Rather, claiming frequently less
complex higher predictive accuracy.
paper first examines arguments Occam thesis.
presents new empirical evidence thesis. evidence acquired using
learning algorithm post-processes decision trees learnt C4.5. post-processor
developed rejecting Occam thesis instead attending assumption
similarity predictive class. post-processor systematically adds complexity
decision trees without altering performance training data. demonstrated
lead increase predictive accuracy previously unseen objects range
`real-world' learning tasks. evidence taken incompatible Occam thesis.
398

fiFurther Experimental Evidence Utility Occam's Razor

2. Previous Theoretical Experimental Work
provide context new evidence Occam thesis, worth brie
examining previous relevant theoretical experimental work. relevant, outline
provided reasons contribution may failed persuade
side debate.

2.1 Law Conservation Generalization Performance

conservation law generalization performance (Schaffer, 1994) proves learning
bias outperform bias space possible learning tasks1. follows
Occam's razor valuable learning bias, subset
possible learning tasks. might argued set `real-world' learning tasks
subset.
paper predicated accepting proposition set `real-world' learning
tasks distinguished set possible learning tasks respects render
conservation law inapplicable. Rao, Gordon, Spears (1995) argue case
learning tasks universe uniformly distributed across space
possible learning tasks.
so? One argument support proposition follows.
`Real-world' learning tasks defined people use machine learning systems.
end, task constructors sought ensure independent variables
(class attributes) related dependent variables (other attributes) ways
captured within space classifiers made available learning system.
Actual machine learning tasks drawn randomly space possible learning
tasks. human involvement formulation problems ensures this.
simple thought experiment support proposition, consider learning task
class attribute generated random number generator way
relates attributes. majority machine learning researchers would
slightest disconcerted systems failed perform well trained data.
example, consider learning task class attribute simple count
number missing attribute values object. Assume learning task
submitted system, C4.5 (Quinlan, 1993), develops classifiers
mechanism testing classification whether attribute value missing. Again,
majority machine learning researchers would unconcerned systems failed
perform well circumstances. Machine learning simply unsuited tasks.
knowledgeable user would apply machine learning data, least
expectation obtaining useful classifier therefrom.
paper explores applicability Occam thesis `real-world' learning tasks.

2.2 Theoretical Objections Occam Thesis

machine learning systems explicitly implicitly employ Occam's razor. addition
almost universal use machine learning, principle Occam's razor widely
1. law proved discrete valued learning tasks, reason believe
also apply continuous valued tasks

399

fiWebb

accepted general scientific practice. persisted, despite Occam's razor
subjected extensive philosophical, theoretical empirical attack, suggests
attacks found persuasive.
philosophical front, summarize Bunge (1963), complexity theory
(classifier) depends entirely upon language encoded. claim
acceptability theory depends upon language happens expressed
appears indefensible. Further, obvious theoretical relationship syntactic complexity quality theory, possibility world
intrinsically simple use Occam's razor enables discovery intrinsic
simplicity. However, even world intrinsically simple, reason
simplicity correspond syntactic simplicity arbitrary language.
merely state less complex explanation preferable specify
criterion preferable. implicit assumption underlying much machine learning research appears that, things equal, less complex classifiers be,
general, accurate (Blumer et al., 1987; Quinlan, 1986). Occam thesis
paper seeks discredit.
straight-forward interpretation, syntactic measure used predict
expected accuracy appears absurd. two classifiers identical meaning (such
20AGE40 POS 20AGE30 30AGE40 POS)
possible accuracies differ, matter greatly complexities differ.
simple example highlights apparent dominance semantics syntax
determination predictive accuracy.

2.3 Previous Experimental Evidence Occam Thesis
empirical front, number recent experimental results appeared con ict
Occam thesis. Murphy Pazzani (1994) demonstrated number artificial classification learning tasks, simplest consistent decision trees lower predictive
accuracy slightly complex consistent trees. experimentation, however,
showed results dependent upon complexity target concept.
bias toward simplicity performed well target concept best described
simple classifier bias toward complexity performed well target concept
best described complex classifier (Murphy, 1995). addition, simplest classifiers
obtained better average (over consistent classifiers) predictive accuracy
data augmented irrelevant attributes attributes strongly correlated target
concept, required classification.
Webb (1994) presented results suggest wide range learning tasks
UCI repository learning tasks (Murphy & Aha, 1993), relative generality
classifiers better predictor classification performance relative surface
syntactic complexity. However, could argued results demonstrate
strategy selecting simplest pair theories lead maximization
predictive accuracy, demonstrate selecting simplest available
theories would fail maximize predictive accuracy.
Schaffer (1992, 1993) shown pruning techniques reduce complexity
decreasing resubstitution accuracy sometimes increase predictive accuracy sometimes
400

fiFurther Experimental Evidence Utility Occam's Razor

decrease predictive accuracy inferred decision trees. However, proponent Occam
thesis could explain results terms positive effect application Occam's
razor (the reduction complexity) counter-balanced negative effect
reduction empirical support (resubstitution accuracy).
Holte, Acker, Porter (1989) shown specializing small disjuncts (rules
low empirical support) exclude areas instance space occupied training
objects frequently decreases error rate unseen objects covered disjuncts.
specialization involves increasing complexity, might viewed contrary
Occam thesis. However, research shows total error rates classifiers
disjuncts embedded increases disjuncts specialized.
proponent Occam thesis could thus dismiss relevance former results
arguing thesis applies complete classifiers elements
classifiers.

2.4 Theoretical Experimental Support Occam Thesis
theoretical experimental objections Occam thesis exists
body apparent theoretical empirical support.
Several attempts made provide theoretical support Occam thesis
machine learning context (Blumer et al., 1987; Pearl, 1978; Fayyad & Irani, 1990).
However, proofs apply equally systematic learning bias favors small
subset hypothesis space. Indeed, argued equally support
preference classifiers high complexity (Schaffer, 1993; Berkman & Sandholm, 1995).
Holte (1993) compared learning simple classification rules use sophisticated learner complex decision trees. found that, number tasks UCI
repository machine learning datasets (Murphy & Aha, 1993), simple rules achieved
accuracies within percentage points complex trees. could considered
supportive Occam thesis. However, case simple rules outperform
complex decision trees. demonstrated exist yet another
learning bias consistently outperformed studied.
final argument might considered support Occam thesis
majority machine learning systems employ form Occam's razor appear perform well practice. However, demonstrated even better
performance would obtained Occam's razor abandoned.

3. New Experimental Evidence Occam Thesis
theoretical experimental objections Occam thesis appear
greatly diminished machine learning community's use Occam's razor. paper
seeks support objections Occam thesis robust general experimental
counter-evidence. end presents systematic procedure increasing complexity inferred decision trees without modifying performance training data.
procedure takes form post-processor decision trees produced C4.5 (Quinlan, 1993). application procedure range learning tasks UCI
repository learning tasks (Murphy & Aha, 1993) demonstrated result, average,
401

fiWebb

increased predictive accuracy inferred decision trees applied previously
unseen data.

3.1 Theoretical Basis Decision Tree Post-processor
similarity assumption common assumption machine learning|that objects
similar high probability belonging class (Rendell & Seshu, 1990).
techniques described rely upon assumption theoretical justification
rather upon Occam thesis.
Starting similarity assumption, machine learning viewed inference
suitable similarity metric learning task. decision tree viewed
partitioning instance space. partition, represented leaf, contains
objects similar relevant respects thus expected belong
class.
raises issue similarity measured. Instance-based learning methods (Aha, Kibler, & Albert, 1991) tend map instance space onto ndimensional geometric space employ geometric distance measures within
space measure similarity. approach problematic number grounds.
First, assumes underlying metrics different attributes commensurable.
possible determine priori whether difference five years age signifies
greater lesser difference similarity difference one inch height? Second,
assumes possible provide priori definitions similarity respect
single attribute. one really make universal prescription value 16 always
similar value 2 value 64? never case
relevant similarity metric based log2 surface value, case 16 would
similar 64 2?
wish employ induction learn classifiers expressed particular language
would appear forced assume language question manner
captures relevant aspect similarity. potential leaf decision tree presents
plausible similarity metric (all objects fall within leaf similar respect).
Empirical evaluation (the performance leaf training set) used
infer relevance similarity metric induction task hand. leaf l covers
large number objects class c classes, provides evidence
similarity respect tests define l predictive c.
Figure 1 illustrates simple instance space partition C4.5 (Quinlan, 1993)
imposes thereon. Note C4.5 forms nodes continuous attributes, B ,
consist test cut value x. test takes form x. respect
Figure 1 one cut, value 5 attribute A.
C4.5 infers relevant similarity metric relates attribute only. partition
(shown dashed line) placed value 5 attribute A. However, one
accept Occam thesis, accept similarity assumption, reason
believe area instance space B > 5 5 (lightly shaded
Figure 1) belong class + (as determined C4.5) rather class {.
C4.5 uses Occam thesis justify termination partitioning instance
space soon decision tree accounts adequately training set. consequence,
402

fiFurther Experimental Evidence Utility Occam's Razor

..........
.......... { {
..........
.......... { {
.......... {
+
{
+
{
+
1 2 3 4 5 6 7 8 9 10

Figure 1: simple instance space
10
9
8
7
6
B5
4
3
2
1

large areas instance space occupied objects training set may
left within partitions similarity assumption provides little support.
example, respect Figure 1, could argued relevant similarity metric
respect region 5 B > 5 similarity respect B . Within
entire instance space, objects values B > 5 belong class {. five
objects. contrast, three objects values 5 provide
evidence objects area instance space belong class +.
tests represents plausible similarity metric basis available evidence. Thus,
object within region similar plausible respect three positive five
negative objects. objects similar relevant respects high probability
belonging class, information available plausible
object similar three positive five negative objects, would appear
probable object negative positive.
disagreement C4.5 similarity assumption case contrasts
with, example, area instance space 5 B < 1. region,
similarity assumption suggests C4.5's partition appropriate plausible
similarity metrics indicate object region similar positive objects
only2 .
post-processor developed research analyses decision trees produced C4.5
order identify regions|those occupied objects training set
evidence (in terms similarity assumption) favoring relabeling
2. provide example implausible similarity metric, consider similarity metric defined
root node, everything similar. plausible great level
dissimilarity classes respect metric. relevant similarity metric,
distribution training examples representative distribution objects domain
whole, similarity assumption would violated, similar objects would probability
0.58 belonging class. probability calculated follows. probabilities
object + { 0.3 0.7 respectively. object + probability belonging
class another object similar 0.3. object { probability
belonging class another object similar 0.7. Thus, probability
object belonging class another similar object 0:3 0:3 + 0:7 0:7 = 0:58. numbers
involved simple example are, course, small reach conclusion high level
confidence|the example intended illustrative only.

403

fiWebb

different class assigned C4.5. regions identified, new branches
added decision tree, creating new partitionings instance space. trees
must provide identical performance respect training set regions
instance space occupied objects training set affected.
dicult see plausible metric complexity could interpret addition
branches increasing complexity tree.
end result post-processor adds complexity decision tree without
altering tree applies training data. Occam thesis predicts will,
general, lower predictive accuracy similarity assumption predicts will,
general, increase predictive accuracy. seen, latter prediction consistent
experimental evidence former not.

3.2 Post-processor

process could applied continuous discrete attributes,
current implementation addresses continuous attributes.
post-processor operates examining leaf l tree turn. l,
attribute considered turn. a, possible thresholds
region instance space occupied objects l explored. First, minimum
(min) maximum (max) determined values possible objects
reach l. l lies branch split threshold
split provides upper limit (max) values l. lies > branch,
threshold provides lower limit (min). node lie branch,
max = 1. node lie > branch, min = ,1. objects
training set values within range min::max considered
following operations.
value observed training set attribute within allowable range
outside actual range values objects l, evidence evaluated
support reclassifying region threshold. level support
given threshold evaluated using Laplacian accuracy estimate (Niblett & Bratko, 1986).
leaf relates binary classification (an object belongs class question
not), binary form Laplace used. threshold attribute leaf l,
evidence support labeling partition class n maximum value
ancestor node x l formula
P +1
+2
number objects x min < t; P number
objects belong class n.
evidence support labeling partition threshold calculated identically
exception objects < max instead considered.
maximum evidence new labeling exceeds evidence current labeling
region, new branch added appropriate threshold creating new leaf node
labeled appropriate class.
addition evidence favor current labeling gathered above, evidence support current labeling region calculated using Laplace accuracy
404

fiFurther Experimental Evidence Utility Occam's Razor

estimate considering objects leaf, number objects leaf
P number objects belong class node labeled.
approach ensures new partitions define true regions. is,
attribute value v possible partition v unless possible
objects domain values greater v objects values less
equal v reach node partitioned (even though objects
training set fall within new partition). particular, ensures new cuts
simple duplications existing cuts ancestors current node. Thus, every
modification adds non-redundant complexity tree.
algorithm presented Figure 2. implemented modification
C4.5 release 6, called C4.5X. source code modifications available
on-line appendix paper.
C4.5X, multiple sets values equally satisfy specified constraints
maximize Laplace function, values na nb deeper tree selected
closer root and, single node, preference values aa ab depends
upon order attributes definition data preference values va
vb dependent upon data order. selection strategies side effect
implementation system. reason believe experimental results
would differ general strategies used select competing constraints.
default, C4.5 develops two decision trees time run, unpruned
pruned (simplified) decision tree. C4.5X produces post-processed versions
trees.

3.3 Evaluation
evaluate post-processor applied datasets containing continuous attributes
UCI machine learning repository (Murphy & Aha, 1993) held (due
previous machine learning experimentation) local repository Deakin University.
datasets believed broadly representative repository
whole. experimentation eleven data sets, two additional data sets, sick
euthyroid discordant results, retrieved UCI repository added
study order investigate specific issues, discussed below.
resulting thirteen datasets described Table 1. second column contains
number attributes object described. Next proportion
continuous. fourth column indicates proportion attribute values
data missing (unknown). fifth column indicates number objects
data set contains. sixth column indicates proportion belong
class represented objects within data set. final column indicates
number classes data set describes. Note glass type dataset uses
Float/Not Float/Other three class classification rather commonly used six
class classification.
data set divided training evaluation sets 100 times. training
set consisted 80% data, randomly selected. evaluation set consisted
remaining 20% data. C4.5 C4.5X applied resulting 1300
(13 data sets 100 trials) training evaluation set pairs.
405

fiWebb

Let cases(n) denote set training examples reach node n.
Let value(a; x) denote value attribute training example x.
Let pos(X; c) denote number objects class c set training examples X.
Let Laplace(X; c) = ( +2)+1 X set training examples, jX j number training
examples c class.
Let upperlim(n; a) denote minimum value cut attribute ancestor node n
n lies branch. cut, upperlim(n; a) = 1. determines
upper bound values may reach n.
Let lowerlim(n; a) denote maximum value cut attribute ancestor node n
n lies > branch. cut, lowerlim(n; a) = ,1. determines
lower bound values may reach n.
post-process leaf l dominated class c
1. Find values
n : n ancestor l
: continuous attribute
v : 9x : x 2 cases(n ) & v = value(a ; x) & v min(v : 9y : 2 cases(l) & v =
value(a ; y)) & v > lowerlim(l; )
c : c class
maximize L = Laplace(fx : x 2 cases(n ) & value(a ; x) v & value(a ; x) >
lowerlim(l; )g; c ).
2. Find values
n : n ancestor l
: continuous attribute
v : 9x : x 2 cases(n ) & v = value(a ; x) & v > max(v : 9y : 2 cases(l) & v =
value(a ; y)) & v upperlim(l; )
c : c class
maximize L = Laplace(fx : x 2 cases(n ) & value(a ; x) > v & value(a ; x)
upperlim(l; )g; c ).
3. L > Laplace(cases(l); c) & L L
(a) c 6= c
i. replace l node n test v .
ii. set branch n lead new leaf class c .
iii. set > branch n lead l.
else L > Laplace(cases(l); c)
(b) c 6= c
i. replace l node n test v .
ii. set > branch n lead new leaf class c .
iii. set branch n lead l.
pos X;c
jX j

































b

b

b

b



b

b

b

b







b

b

b



b

b

b

b

b

b

b

b



b

b



b









b

b

b

b

b

Figure 2: C4.5X post-processing algorithm
406

fiFurther Experimental Evidence Utility Occam's Razor

Table 1: UCI data sets used experimentation
%
%
No. contin%
No. common No.
Name
Attrs. uous missing objects class classes
breast cancer Wisconsin
9
100
<1
699
66
2
Cleveland heart disease
13
46
<1
303
54
2
credit rating
15
40
1
690
56
2
discordant results
29
24
6
3772
98
2
echocardiogram
6
83
3
74
68
2
glass type
9
100
0
214
40
3
hepatitis
19
32
6
155
79
2
Hungarian heart disease
13
46
20
295
64
2
hypothyroid
29
24
6
3772
92
4
iris
4
100
0
150
33
3
new thyroid
5
100
0
215
70
3
Pima indians diabetes
8
100
0
768
65
2
sick euthyroid
29
24
6
3772
94
2
Table 2 summarizes percentage predictive accuracy obtained unpruned decision trees generated C4.5 C4.5X. presents mean (x) standard
deviation (s) set 100 trials respect data set C4.5
C4.5X along results two-tailed matched pairs t-test comparing means.
twelve thirteen data sets C4.5X obtained higher mean accuracy C4.5.
remaining data set, hypothyroid, C4.5 obtained higher mean predictive accuracy
C4.5CS (albeit small margin|measured two decimal places respective mean accuracies 99.51 99.46, respectively). nine data sets advantage toward
C4.5X statistically significant 0.05 level (p 0:05), although advantage
respect discordant results data small apparent measured one
decimal place (measured two decimal places values 98.58 98.62 respectively).
advantage toward C4.5 hypothyroid data also statistically significant
0.05 level. differences mean predictive accuracy Hungarian heart disease,
new thyroid sick euthyroid data sets significant 0.05 level.
Table 3 uses format Table 2 summarize predictive accuracy obtained
pruned decision trees generated C4.5 C4.5X. twelve data
sets C4.5X obtained higher mean predictive accuracy C4.5. remaining data
set, hypothyroid, C4.5 obtained higher mean predictive accuracy, although
magnitude difference small apparent level precision
displayed (measured two decimal places mean accuracies 99.51 99.46).
six data sets advantage toward C4.5X statistically significant 0.05
level, although difference apparent precision two decimal places
discordant results data (99.81 99.82, respectively). advantage toward C4.5
hypothyroid data also statistically significant 0.05 level. differences
407

fiWebb

Table 2: Percentage predictive accuracy unpruned decision trees.
Name
breast cancer Wisconsin
Cleveland heart disease
credit rating
discordant results
echocardiogram
glass type
hepatitis
Hungarian heart disease
hypothyroid
iris
new thyroid
Pima indians diabetes
sick euthyroid

C4.5

x

94.1
72.8
82.2
98.6
72.0
74.0
79.6
77.0
99.5
95.4
89.9
70.2
98.7



1.8
5.0
3.4
0.5
9.8
7.0
7.1
5.3
0.2
3.4
4.2
3.5
0.5

C4.5X

x





p

94.4 1.7 {3.2 0.002
74.4 4.8 {6.1 0.000
83.0 3.3 {7.6 0.000
98.6 0.5 {5.4 0.000
73.5 10.2 {2.8 0.007
75.3 7.2 {4.2 0.000
80.8 6.9 {3.3 0.001
77.4 5.2 {1.8 0.082
99.5 0.2 4.4 0.000
95.7 3.5 {2.2 0.028
90.1 4.3 {1.0 0.302
71.3 3.6 {8.1 0.000
98.7 0.5 {0.0 0.963

Table 3: Percentage accuracy pruned decision trees.
Name
breast cancer Wisconsin
Cleveland heart disease
credit rating
discordant results
echocardiogram
glass type
hepatitis
Hungarian heart disease
hypothyroid
iris
new thyroid
Pima indians diabetes
sick euthyroid

C4.5

x

95.1
74.1
84.1
98.8
74.2
74.4
79.9
79.2
99.5
95.4
89.6
72.2
98.7



1.7
5.3
3.2
0.4
9.3
6.9
6.2
4.9
0.2
3.6
4.2
3.5
0.4

C4.5X

x

95.2
74.8
84.6
98.8
75.1
75.4
80.7
79.4
99.5
95.7
89.8
72.8
98.7



1.7
5.3
3.2
0.4
9.8
6.9
6.2
4.8
0.2
3.7
4.2
3.5
0.4



p

{2.0 0.051
{3.7 0.000
{5.3 0.000
{2.6 0.010
{1.6 0.1180
{3.3 0.001
{3.0 0.003
{1.0 0.310
5.4 0.000
{1.6 0.109
{0.8 0.451
{5.9 0.000
{0.7 0.480

breast cancer Wisconsin, echocardiogram, Hungarian heart disease, iris, new thyroid
sick euthyroid statistically significant 0.05 level.
completing experimentation initial eleven data sets, results
hypothyroid data stood stark contrast ten. raised
possibility might distinguishing features hypothyroid data
408

fiFurther Experimental Evidence Utility Occam's Razor

accounted difference performance. Table 1 indicates data set clearly
distinguishable ten initial data sets following six respects|

attributes;
containing greater proportion discrete attributes (which directly addressed
C4.5X);






containing objects;
greater proportion objects belong common class;
classes;
producing decision trees extremely high predictive accuracy without post-processing.

explore issues discordant results sick euthyroid data sets retrieved
UCI repository added study. data sets identical
hypothyroid data set exception different class attribute. three
data sets contain objects, described attributes. addition
discordant results sick euthyroid data little illuminate issue however.
three data sets changes accuracy small magnitude. hypothyroid
significant advantage C4.5. sick euthyroid significant advantage
either system. discordant results data significant advantage C4.5X.
question whether distinguishing feature hypothyroid data
explains observed results remains unanswered. investigation issue lies
beyond scope current paper remains interesting direction future research.
results suggest C4.5X's post-processing frequently increases predictive
accuracy type data found UCI repository. (Of twenty-six
comparisons, significant increase fifteen significant decrease
two. sign test reveals rate success significant 0.05 level,
p = 0:001.)
Tables 4 5 summarize number nodes decision trees developed. Table 4
addresses unpruned decision trees Table 5 addresses pruned decision trees. postprocessing modification replaces single leaf split two leaves. one
modification performed per leaf original tree. data sets postprocessed decision trees significantly complex original decision trees.
cases post-processing increased mean number nodes decision trees
approximately 50%. demonstrates post-processing causing substantial
change.

4. Discussion

primary objective research discredit Occam thesis.
end uses post-processor disregards Occam thesis instead theoretically
founded upon similarity assumption. Experimentation post-processor
409

fiWebb

Table 4: Number nodes unpruned decision trees.
C4.5
C4.5X
Name
x

x


p
breast cancer Wisconsin 38.1 6.0 64.0 10.3 {51.5 0.000
Cleveland heart disease 66.7 7.1 100.2 11.3 {61.9 0.000
credit rating
117.6 18.1 177.9 28.4 {44.2 0.000
discordant results
64.0 10.6 85.2 16.2 {33.3 0.000
echocardiogram
15.4 4.1 22.1 6.3 {26.1 0.000
glass type
43.0 5.2 69.7 8.4 {57.2 0.000
hepatitis
24.5 4.2 34.8 6.0 {49.1 0.000
Hungarian heart disease 62.1 7.5 94.8 13.0 {50.1 0.000
hypothyroid
29.4 4.4 47.5 7.1 {57.8 0.000
iris
9.0 1.9 16.0 4.0 {31.5 0.000
new thyroid
14.7 2.4 23.4 3.8 {41.5 0.000
Pima indians diabetes 164.8 10.8 238.8 16.3 {108.9 0.000
sick euthyroid
71.7 6.6 111.4 12.1 {65.8 0.000
Table 5: Number nodes pruned decision trees.
C4.5

C4.5X

Name
x

x
breast cancer Wisconsin 19.2 5.0 33.1
Cleveland heart disease 44.6 8.3 68.3
credit rating
51.2 14.8 78.4
discordant results
24.9 5.6 32.5
echocardiogram
10.4 3.0 14.8
glass type
36.6 5.5 61.0
hepatitis
13.7 4.8 19.8
Hungarian heart disease 26.8 11.4 41.2
hypothyroid
23.6 2.9 37.1
iris
8.2 1.9 14.8
new thyroid
14.1 2.7 22.5
Pima indians diabetes 112.0 16.4 163.9
sick euthyroid
46.5 5.8 72.6



8.6
12.8
24.2
8.8
4.8
9.5
6.6
17.3
5.6
3.9
4.3
24.0
8.7



{34.9
{43.6
{25.8
{21.1
{21.0
{48.5
{30.7
{22.1
{46.7
{30.3
{36.9
{62.5
{76.7

p

0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000

demonstrated possible develop systematic procedures that, range `realworld' learning tasks increase predictive accuracy inferred decision trees result
changes substantially increase complexity without altering performance
upon training data.
is, general, dicult attack Occam thesis due absence widely
agreed formulation thereof. However, far apparent Occam thesis might
410

fiFurther Experimental Evidence Utility Occam's Razor

..........
. . . .{. . . . . . { {
..........
.......... { {
.......... {
+
{
+
{
+
1 2 3 4 5 6 7 8 9 10

Figure 3: Modified simple instance space
10
9
8
7
6
B5
4
3
2
1

recast accommodate experimental results provide practical learning
bias.

4.1 Directions Future Research
implications research reach beyond relevance Occam's razor. postprocessor appears practical utility increasing quality inferred decision trees.
However, objective research improve predictive accuracy rather
discredit Occam thesis, post-processor would modified number ways.
first modification would enable addition multiple partitions single
leaf original tree. C4.5X selects single modification
maximum support. design decision originated desire minimize likelihood
performing modifications decrease accuracy. principle, however, would
appear desirable select modifications strong support,
could inserted tree order level supporting evidence.
Even greater increases accuracy might expected one removed constraint
post-processing alter performance decision tree respect
training set. case, new partitions may well found employ objects
regions instance space provide evidence support adding partitions
correct misclassifications small numbers objects leaf node original tree.
similarity assumption would provide strong evidence repartitioning.
situation would occur, example, respect learning problem illustrated
Figure 1, additional object class { attribute values A=2 B=9.
illustrated Figure 3. case C4.5 would still create indicated partitions.
However, C4.5X would unable relabel area containing additional object due
constraint alter performance original decision tree respect
training set. Thus addition object prevents C4.5X relabeling shaded
region even though, basis similarity assumption, improves evidence
support relabeling.
extended post-processor would encourage following model inductive inference decision trees. role C4.5 (or similar system) would identify clusters
411

fiWebb

objects within instance space grouped single leaf node.
second stage would analyze regions instance space lie outside clusters
order allocate classes regions. Current decision tree learners, motivated
Occam thesis, ignore second stage, leaving regions outside identified clusters
associated whatever classes assigned by-product cluster
identification process.

4.2 Related Research
number researchers developed learning systems viewed considering
evidence neighboring regions instance space order derive classifications
within regions instance space occupied examples training
set. Ting (1994) explicitly, examining training set directly explore
neighborhood object classified. system uses instance based learning
classification within nodes decision tree low empirical support (small disjuncts).
number systems also viewed considering evidence neighboring
regions classification. systems learn apply multiple classifiers (Ali,
Brunk, & Pazzani, 1994; Nock & Gascuel, 1995; Oliver & Hand, 1995). context,
point within region instance space occupied training objects
likely covered multiple leaves rules. these, leaf rule greatest
empirical support used classification.
C4.5X uses two distinct criteria evaluating potential splits. standard C4.5 stage
tree induction employs information measure select splits. post-processor uses
Laplace accuracy estimate. Similar uses dual criteria investigated elsewhere.
Quinlan (1991) employs Laplace accuracy estimate considering neighboring regions
instance space estimate accuracy small disjuncts. Lubinsky (1995) Brodley
(1995) employ resubstitution accuracy select splits near leaves induction
decision trees.
adding split leaf, C4.5X specializing respect class leaf
(and generalizing respect class new leaf). Holte et al. (1989) explored
number techniques specializing small disjuncts. C4.5X differs leaves
candidates specialization, low empirical support. differs
manner selects specialization perform considering evidence
support alternative splits rather strength evidence support
individual potential conditions current disjunct.

4.3 Bias Versus Variance
Breiman, Friedman, Olshen, Stone (1984) provide analysis complexity induction terms trade-off bias variance. classifier partitions instance
space regions. regions large, degree fit accurate partitioning instance space poor, increasing error rates. effect called bias.
regions small, probability individual regions labeled
wrong class increased. effect, called variance, also increases error rates. According
analysis, due variance, fine partitioning instance space tends increase
412

fiFurther Experimental Evidence Utility Occam's Razor

error rate while, due bias, coarse partitioning also tends increase error
rate.
Increasing complexity decision tree creates finer partitionings instance
space. analysis used argue addition undue complexity
decision trees ground increase variance hence error rate.
However, success C4.5X decreasing error rate demonstrates
successfully managing bias/variance trade-off introduces complexity
decision tree. using evidence neighboring regions instance space, C4.5X
successful increasing error rate resulting variance lower rate
decreases error rate resulting bias. success C4.5X demonstrates
adding undue complexity C4.5's decision trees.

4.4 Minimum Encoding Length Induction

Minimum encoding length approaches perform induction seeking theory enables
compact encoding theory available data. Two key approaches
developed, Minimum Message Length (MML) (Wallace & Boulton, 1968)
Minimum Description Length (MDL) (Rissanen, 1983). approaches admit probabilistic interpretations. Given prior probabilities theories data, minimization
MML encoding closely approximates maximization posterior probability (Wallace & Freeman, 1987). MDL code length defines upper bound \unconditional
likelihood" (Rissanen, 1987).
two approaches differ MDL employs universal prior, Rissanen (1983)
explicitly justifies terms Occam's razor, MML allows specification distinct
appropriate priors induction task. However, practice, default prior usually
employed MML, one appears also derive justification Occam's razor.
Neither MDL MML default prior would add complexity decision tree
justified solely basis evidence neighboring regions
instance space. evidence study presented herein appears support
potential desirability so. casts doubt upon utility universal
prior employed MDL default prior usually employed MML, least
respect use maximizing predictive accuracy.
noted, however, probabilistic interpretation minimum
encoding length techniques indicates encoding length minimization represents maximization posterior probability unconditional likelihood. Maximization
factors necessarily directly linked maximizing predictive accuracy.

4.5 Appropriate Application Grafting Pruning

important note although paper calls question value learning
biases penalize complexity, way provide support learning biases
encourage complexity sake. C4.5X grafts new nodes onto decision tree
empirical support so.
results way argue appropriate use decision tree pruning.
generate pruned trees, C4.5 removes branches statistical estimates upper
bounds error rates indicate increase branch removed.
413

fiWebb

could argued C4.5 reduces complexity empirical support
so. interesting note eight thirteen data sets examined, C4.5X's
post-processing pruned trees resulted higher average predictive accuracy
post-processing unpruned trees. results suggest pruning grafting
play valuable role applied appropriately.

5. Conclusion
paper presents systematic procedure adding complexity inferred decision trees
without altering performance training data. procedure demonstrated lead increases predictive accuracy range learning tasks applied
pruned unpruned trees inferred C4.5. one thirteen learning
tasks examined procedure lead statistically significant loss accuracy
case magnitude difference mean accuracy extremely small.
face it, provides strong experimental evidence Occam thesis.
post-processing technique developed rejecting Occam thesis instead attending similarity assumption|that similar objects high probability
belonging class.
procedure developed constrained need ensure revised decision
tree performed identically original decision tree respect training data.
constraint arose desire obtain experimental evidence Occam
thesis. possible constraint removed, basic techniques outlined
paper could result even greater improvements predictive accuracy reported
herein.
research considered one version Occam's razor favors minimization
syntactic complexity expectation tend increase predictive accuracy.
interpretations Occam's razor also possible, one minimize
semantic complexity. others (Bunge, 1963) provided philosophical objections
formulations Occam's razor, paper sought investigate them.
version Occam's razor examined research used widely machine
learning apparent success. objections principle substantiated research raise question, apparent success
awed? Webb (1994) suggests apparent success principle due
manner syntactic complexity usually associated relevant qualities
inferred classifiers generality prior probability. thesis accepted one
key challenges facing machine learning understand deeper qualities
employ understanding place machine learning sounder theoretical footing.
paper offers small contribution direction demonstrating minimization
surface syntactic complexity not, itself, general maximize predictive accuracy
inferred classifiers.
nonetheless important realize that, thrust paper notwithstanding,
Occam's razor often useful learning bias employ. frequently good pragmatic reasons preferring simple hypothesis. simple hypothesis
general easier understand, communicate employ. preference simple
414

fiFurther Experimental Evidence Utility Occam's Razor

hypotheses cannot justified terms expected predictive accuracy may justified
pragmatic grounds.

Acknowledgements
research supported Australian Research Council. grateful
Charlie Clelland, David Dowe, Doug Newlands, Ross Quinlan anonymous reviewers
extremely valuable comments paper benefited greatly.

References

Aha, D. W., Kibler, D., & Albert, M. K. (1991). Instance-based learning algorithms.
Machine Learning, 6, 37{66.
Ali, K., Brunk, C., & Pazzani, M. (1994). learning multiple descriptions concept.
Proceedings Tools Artificial Intelligence New Orleans, LA.
Berkman, N. C., & Sandholm, T. W. (1995). minimized decision tree:
re-examination. Technical report 95-20, University Massachusetts Amherst,
Computer Science Department, Amherst, Mass.
Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. (1987). Occam's Razor.
Information Processing Letters, 24, 377{380.
Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification
Regression Trees. Wadsworth International, Belmont, Ca.
Brodley, C. E. (1995). Automatic selection split criterion tree growing based
node selection. Proceedings Twelth International Conference Machine
Learning, pp. 73{80 Taho City, Ca. Morgan Kaufmann.
Bunge, M. (1963). Myth Simplicity. Prentice-Hall, Englewood Cliffs, NJ.
Clark, P., & Niblett, T. (1989). CN2 induction algorithm. Machine Learning, 3,
261{284.
Fayyad, U. M., & Irani, K. B. (1990). minimized decision tree?
AAAI-90: Proceedings Eighth National Conference Artificial Intelligence, pp.
749{754 Boston, Ma.
Good, I. J. (1977). Explicativity: mathematical theory explanation statistical
applications. Proceedings Royal Society London Series A, 354, 303{330.
Holte, R. C. (1993). simple classification rules perform well commonly used
datasets. Machine Learning, 11 (1), 63{90.
Holte, R. C., Acker, L. E., & Porter, B. W. (1989). Concept learning problem
small disjuncts. Proceedings Eleventh International Joint Conference
Artificial Intelligence, pp. 813{818 Detroit. Morgan Kaufmann.
415

fiWebb

Lubinsky, D. J. (1995). Increasing performance consistency classification trees
using accuracy criterion leaves. Proceedings Twelth International
Conference Machine Learning, pp. 371{377 Taho City, Ca. Morgan Kaufmann.
Michalski, R. S. (1984). theory methodology inductive learning. Michalski,
R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning: Artificial
Intelligence Approach, pp. 83{129. Springer-Verlag, Berlin.
Murphy, P. M. (1995). empirical analysis benefit decision tree size biases
function concept distribution. Tech. rep. 95-29, Department Information
Computer Science, University California, Irvine.
Murphy, P. M., & Aha, D. W. (1993). UCI repository machine learning databases.
[Machine-readable data repository]. University California, Department Information Computer Science, Irvine, CA.
Murphy, P. M., & Pazzani, M. J. (1994). Exploring decision forest: empirical investigation Occam's razor decision tree induction. Journal Artificial Intelligence
Research, 1, 257{275.
Niblett, T., & Bratko, I. (1986). Learning decision rules noisy domains. Bramer,
M. A. (Ed.), Research Development Expert Systems III, pp. 25{34. Cambridge
University Press, Cambridge.
Nock, R., & Gascuel, O. (1995). learning decision committees. Proceedings
Twelth International Conference Machine Learning, pp. 413{420 Taho City, Ca.
Morgan Kaufmann.
Oliver, J. J., & Hand, D. J. (1995). pruning averaging decision trees. Proceedings
Twelth International Conference Machine Learning, pp. 430{437 Taho City,
Ca. Morgan Kaufmann.
Pearl, J. (1978). connection complexity credibility inferred
models. International Journal General Systems, 4, 255{264.
Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81{106.
Quinlan, J. R. (1990). Learning logical definitions relations. Machine Learning, 5,
239{266.
Quinlan, J. R. (1991). Improved estimates accuracy small disjuncts. Machine
Learning, 6, 93{98.
Quinlan, J. R. (1993). C4.5: Programs Machine Learning. Morgan Kaufmann, Los
Altos.
Rao, R. B., Gordon, D., & Spears, W. (1995). every generalization action really
equal opposite reaction? Analysis conservation law generalization performance. Proceedings Twelth International Conference Machine
Learning, pp. 471{479 Taho City, Ca. Morgan Kaufmann.
416

fiFurther Experimental Evidence Utility Occam's Razor

Rendell, L., & Seshu, R. (1990). Learning hard concepts constructive induction:
Framework rationale. Computational Intelligence, 6, 247{270.
Rissanen, J. (1983). universal prior integers estimation minimum description
length. Annals Statistics, 11, 416{431.
Rissanen, J. (1987). Stochastic complexity. Journal Royal Statistical Society Series
B, 49 (3), 223{239.
Schaffer, C. (1992). Sparse data effect overfitting avoidance decision tree
induction. AAAI-92: Proceedings Tenth National Conference Artificial
Intelligence, pp. 147{152 San Jose, CA. AAAI Press.
Schaffer, C. (1993). Overfitting avoidance bias. Machine Learning, 10, 153{178.
Schaffer, C. (1994). conservation law generalization performance. Proceedings
1994 International Conference Machine Learning San Mateo, Ca. Morgan
Kaufmann.
Ting, K. M. (1994). problem small disjuncts: remedy decision trees.
Proceedings Tenth Canadian Conference Artificial Intelligence, pp. 63{70.
Morgan Kaufmann,.
Wallace, C. S., & Boulton, D. M. (1968). information measure classification. Computer Journal, 11, 185{194.
Wallace, C. S., & Freeman, P. R. (1987). Estimation inference compact coding.
Journal Royal Statistical Society Series B, 49 (3), 240{265.
Webb, G. I. (1994). Generality significant complexity: Toward alternatives
Occam's razor. Zhang, C., Debenham, J., & Lukose, D. (Eds.), AI'94 { Proceedings Seventh Australian Joint Conference Artificial Intelligence, pp. 60{67
Armidale. World Scientific.

417

fiA Divergence Critic
Bundy, A., van Harmelen, F., Horn, C., & Smaill, A. (1990). Oyster-Clam system.
Stickel, M. (Ed.), 10th International Conference Automated Deduction, pp. 647{
648. Springer-Verlag. Lecture Notes Artificial Intelligence No. 449.
Dershowitz, N., & Pinchover, E. (1990). Inductive Synthesis Equational Programs.
Proceedings 8th National Conference AI, pp. 234{239. American Association
Artificial Intelligence.
Hermann, M. (1989). Crossed term rewriting systems. CRIN report 89-R-003, Centre de
Recherche en Informatique de Nancy.
Ireland, A. (1992). Use Planning Critics Mechanizing Inductive Proof. Proceedings LPAR'92, Lecture Notes Artificial Intelligence 624. Springer-Verlag. Also
available Research Report 592, Dept AI, Edinburgh University.
Ireland, A., & Bundy, A. (1992). Using failure guide inductive proof. Tech. rep., Dept.
Artificial Intelligence, University Edinburgh. Available Edinburgh DAI
Research Paper 613.
Kirchner, H. (1987). Schematization infinite sets rewrite rules. Application
divergence completion processes. Proceedings RTA'87, pp. 180{191.
Protzen, M. (1992). Disproving conjectures. Kapur, D. (Ed.), 11th Conference
Automated Deduction, pp. 340{354. Springer Verlag. Lecture Notes Computer
Science No. 607.
Thomas, M., & Jantke, K. (1989). Inductive Inference Solving Divergence KnuthBendix Completion. Proceedings International Workshop AII'89, pp. 288{303.
Thomas, M., & Watson, P. (1993). Solving divergence Knuth-Bendix completion
enriching signatures. Theoretical Computer Science, 112, 145{185.
Walsh, T. (1994). divergence critic. Bundy, A. (Ed.), 12th Conference Automated
Deduction, pp. 14{25. Springer Verlag. Lecture Notes Artificial Intelligence No.
814.
Walsh, T., Nunes, A., & Bundy, A. (1992). use proof plans sum series. Kapur,
D. (Ed.), 11th Conference Automated Deduction, pp. 325{339. Springer Verlag.
Lecture Notes Computer Science No. 607. Also available Edinburgh DAI
Research Paper 563.
Yoshida, T., Bundy, A., Green, I., Walsh, T., & Basin, D. (1994). Coloured rippling:
extension theorem proving heuristic. Cohn, A. (Ed.), Proceedings ECAI-94,
pp. 85{89. John Wiley. Also available Edinburgh DAI Research Paper 779.

235

fiWalsh
way. types divergence could perhaps recognized
divergence critic. research needed identify divergence patterns, isolate
causes propose ways fixing them. research may take advantage close
links divergence patterns particular types generalization. instance,
may possible identify specific divergence patterns need generalize common
subterms theorem proved.

Acknowledgements

research supported Human Capital Mobility Postdoctoral Fellowship.
wish thank: Adel Bouhoula Michael Rusinowitch invaluable assistance
Spike; Pierre Lescanne inviting visit Nancy research
performed; David Basin, Alan Bundy, Miki Hermann, Andrew Ireland, Michael
Rusinowitch comments questions; reviewers comments suggestions; members Eureca Protheo groups INRIA; members
DReaM group Edinburgh MRG groups Trento Genova.

References

Aubin, R. (1976). Mechanizing Structural Induction. Ph.D. thesis, University Edinburgh.
Basin, D., & Walsh, T. (1992). Difference matching. Kapur, D. (Ed.), 11th Conference
Automated Deduction, pp. 295{309. Springer Verlag. Lecture Notes Computer
Science No. 607. Also available Edinburgh DAI Research Paper 556.
Basin, D., & Walsh, T. (1993). Difference unification. Proceedings 13th IJCAI.
International Joint Conference Artificial Intelligence. Also available Technical
Report MPI-I-92-247, Max-Planck-Institute fur Informatik.
Basin, D., & Walsh, T. (1994). Termination orderings rippling. Bundy, A. (Ed.), 12th
Conference Automated Deduction, pp. 466{483. Springer Verlag. Lecture Notes
Artificial Intelligence No. 814.
Bouhoula, A., Kounalis, E., & Rusinowitch, M. (1992). Spike: theorem-prover.
Proceedings LPAR'92, Lecture Notes Artificial Intelligence 624. Springer-Verlag.
Bouhoula, A., & Rusinowitch, M. (1995a). Implicit induction conditional theories. Journal Automated Reasoning, 14 (2), 189{235.
Bouhoula, A., & Rusinowitch, M. (1995b). Spike user manual.. INRIA Lorraine
CRIN, 615 rue du Jardin Botanique, Villers-les-Nancy, France. Available
ftp://ftp.loria.fr/pub/loria/protheo/softwares/Spike/.
Boyer, R., & Moore, J. (1979). Computational Logic. Academic Press. ACM monograph
series.
Bundy, A., Stevens, A., van Harmelen, F., Ireland, A., & Smaill, A. (1993). Rippling:
heuristic guiding inductive proofs. Artificial Intelligence, 62, 185{253. Also
available Edinburgh DAI Research Paper No. 567.
234

fiA Divergence Critic
rev(qrev(x; nil)) = x
rev(qrev(x; cons(y; nil) )) = cons(y; x)
rev(qrev(x; cons(z; cons(y; nil)) )) = cons(z; cons(y; x))
..
.

annotated sequence unique maximal difference match. annotations suggest
need wave rule,
rev(qrev(X; cons(Y; nil) )) = cons(Y; rev(qrev(X; nil))) :

rule allows proof go without divergence. comparison, specific
generalization seems unable identify rule. specific generalization
left hand side sequence gives term rev(qrev(X; Z)) (or, ignoring first term
sequence, rev(qrev(X; cons(Y; Z)))). specific generalization cannot, however,
identify useful pattern, rev(qrev(X; cons(Y; nil))).
Nqthm contains simple test divergence based subsumption. instance,
example 13 last section, Nqthm unable simplify following subgoal
step case proof,
(EQUAL (ROT (LENGTH X) (APPEND X (LIST Z)))
(CONS Z (ROT (LENGTH X) X))))

Note lemma speculated divergence critic. Nqthm generalizes (LENGTH
X) subgoal giving false conjecture,
(EQUAL (ROT (APPEND X (LIST Z)))
(CONS Z (ROT X))))

several attempts induction generalization, Nqthm realizes proof
diverging since subgoal subsumed parent. proof therefore loop,
Nqthm gives up. attempt made analyse failed proof attempt identify
started go wrong. addition, subsumption weak test divergence, much
weaker tests based difference matching generalization. subsumption test
recognizes divergence small number failed examples last section.

10. Conclusions

described divergence critic, computer program attempts identify diverging proof attempts propose lemmas generalizations overcome
divergence. divergence critic proved successful; enables system Spike
prove many theorems definitions alone. divergence critic's success
largely attributed power rippling heuristic. heuristic originally
developed proofs using explicit induction since found several applications.
Difference matching used identify accumulating term structure causing divergence. Lemmas generalizations proposed ripple term structure
233

fiWalsh
divergence critic described works implicit (and explicit) induction setting.
Second, divergence critic automatically invoked must identify proof
failing. Third, divergence critic less specialized. last two differences ect
fact critics Clam usually associated failure particular precondition
heuristic. divergence pattern can, comparison, arise many different
reasons: need generalize variables apart, generalize common subterms, add
lemma, etc. Fourth, divergence critic must use difference matching annotate terms;
Clam, terms usually already appropriately annotated. Finally, divergence critic
less tightly coupled theorem prover's inference rules heuristics. critic
therefore exploit strengths prover without needing reason complex
rules heuristics used. instance, divergence critic diculty identifying
divergence complex situations like nested mutual inductions. critic also benefits
powerful simplification rules used Spike.
Divergence studied quite extensively completion procedures. Two
main novelties critic described use difference matching identify
divergence, use rippling speculation lemmas overcome divergence.
Dershowitz Pinchover, comparison, use specific generalization identify divergence patterns critical pairs produced completion (Dershowitz & Pinchover, 1990).
Kirchner uses generalization modulo equivalence relation recognise divergence
patterns (Kirchner, 1987); meta-rules synthesized describe infinite families
rules common structure. Thomas Jantke use generalization inductive
inference recognize divergence patterns replace infinite sequences critical pairs
finite number generalizations (Thomas & Jantke, 1989). Thomas Watson use
generalization replace infinite set rules finite complete set enriched
signature (Thomas & Watson, 1993).
Generalization modulo equivalence enables complex divergence patters identified. However, general undecidable. specific generalization, comparison,
limited. cannot recognize divergence patterns give nested wave-fronts like,

s( s(x) + x) :
addition, specific generalization cannot identify term structure wave-holes.
example, consider divergence sequence equations produced Spike attempts
prove example 25 Section 8,
rev(qrev(x; nil)) = x
rev(qrev(x; cons(y; nil))) = cons(y; x)
rev(qrev(x; cons(z; cons(y; nil)))) = cons(z; cons(y; x))
..
.

Divergence analysis identifies term structure accumulating within accumulator argument
qrev ,
232

fiA Divergence Critic
Unfortunately heuristics instantiating right hand side speculated lemmas
strong enough suggest rule,

X + (Y + Z ) = + (X + Z )
rule, Spike finds proof commutativity multiplication without diculty.
diculties speculating rule arise wave-front stuck similar
position sides equality. clues therefore suggest ripple
top term tree.
example 33, divergence critic proposes lemma one needed. Spike
able find proof theorem definitions alone using 16 inductions. Three
inductions equations,
0+x = x
s(0) + x = s(x)
s(s(0)) + x = s(s(x))
sequence equations satisfies divergence critic's preconditions. critic therefore
proposes wave rules moving accumulating successor functions first argument
position +. Although proposed lemmas necessary, either give much shorter
simpler proof needing 7 inductions.
Example 34 lemma speculated example 24. Divergence analysis Spike's
attempt prove theorem identifies term structure accumulating second (alias
accumulator) argument qrev . first two lemmas proposed removing term
structure use subsumed recursive definition qrev . third
lemma also fails prevent divergence. lemma simplifies two element lists second
argument position qrev . However, divergence still occur prover cannot simplify
lists occur second argument position qrev contain 3 elements.
Divergence overcome introduce derived function appending onto end
list. used simplify terms list arbitrary size occurs
second argument position qrev . example, simplify rule,

qrev(X; ) = app(qrev(X; nil); )
Unfortunately, append occur specification theorem dicult
find heuristic would speculate rule.

9. Related Work

Critics monitoring construction proofs first proposed Ireland Clam
prover (Ireland, 1992). framework, failure one proof methods automatically
invokes critic. Various critics explicit induction developed speculate
missing lemmas, perform generalizations, look suitable case splits, etc. rippling plays
central role Clam's proof methods, many heuristics similar described
(Ireland & Bundy, 1992). are, however, several significant differences. First,
231

fiWalsh

Theorem
31
s(x) y=y+(x y)
32
x y=y x
33 x+(y+(z+(v+w))) = w+(x+(y+(z+v)))

Lemmas speculated
Time/s
{
n/a
s(X)+Y=X+s(Y)
8.0
s(X)+Y=s(X+Y)
17.7
s(X)+Y=X+s(Y)
34 qrev(qrev(x,[y]),z)=y ::qrev(qrev(x,[]),z)
qrev(Y,X ::Z)) = qrev(X ::Y,Z)
9.6
qrev(qrev(X,Y ::Z),W)=qrev(qrev(Y ::X,Z),W)
qrev(qrev(X,Y ::[Z]),W)=Z::qrev(qrev(X,[Y]),W)

Table 2: divergence critic's failures.
diverges, generating following sequence equations,
s(y) + (x + (x y)) = s(y) + (y + (x y))
s(s(y)) + (x + (x + (x y))) = s(x) + (s(y) + (x + (x y))
s(s(s(y))) + (x + (x + (x + (x y)))) = s(x) + (s(s(y)) + (x + (x + (x y))))
..
.
Divergence analysis left hand sides equations suggests need rule
form,

s(Y ) + (X + Z ) = F (Y + Z )
Unfortunately heuristics lemma speculation suciently strong suggest
suitable instantiation F (for example, z : s(X + z )). lemma rather complex
result two overlapping divergence patterns. annotations considered
separately, suggest rules,

s(X ) + = s(X + )
+ (X + Z ) = X + (Y + Z )
two rules, Spike finds proof without diculty.
Example 32 commutativity multiplication. divergence critic identifies
divergence pattern proposes transverse wave rule,

s(X ) + = X + s(Y )
However, Spike unable prove commutativity multiplication addition
rule. proof attempt somewhat simpler contains diverging sequence
equations,
x + (y + (x + (x y))) = + (x + (x + (x y)))
x + (y + (x + (x + (x y))) ) = + (x + (x + (x + (x y))))

x + (y + (x + (x + (x + (x y)))) ) = + (x + (x + (x + (x + (x y)))))
..
.

230

fiA Divergence Critic
speculate non-theorems. research optimal strength generalization
heuristics would valuable.
Example 24 disappointment; lemma proposed fixes divergence
dicult proved automatically, even assistance divergence critic.
See example 34 end section details. Example 25 discussed
detail related work Section 9 demonstrates superiority difference
matching generalization techniques divergence analysis. Examples 26 28 require
little discussion. Finally, examples 29 30 demonstrate critic cope
divergence moderately complex theories containing conditional equations.
results pleasing. Using divergence critic, 30 theorems listed (with
exception 24) proved definitions alone. provide indication
diculty theorems, Nqthm system (Boyer & Moore, 1979),
perhaps best known explicit induction theorem prover, unable prove
half theorems definitions alone. precise, Nqthm failed 5, 6, 7, 8, 9,
11, 12, 13, 14, 15, 18, 19, 21, 22, 24, 25, 26, 27 28. course, addition
simple lemmas, Nqthm able prove theorems. Indeed, many cases, Nqthm
needs lemmas proposed divergence critic required Spike.
suggests divergence critic especially tied particular prover used
even implicit induction setting.
test hypothesis, presented output diverging proof attempt Nqthm
critic. chose commutativity multiplication perhaps simplest
theorem causes Nqthm diverge. critic proposed lemma,
(EQUAL (TIMES (ADD1 X)) (PLUS (TIMES X))))

TIMES PLUS primitives Nqthm's logic recursively defined first
arguments. exactly lemma needed Nqthm prove commutativity
multiplication. Nqthm fails many examples similar reasons Spike,
divergence analysis identifies appropriate lemma. supports suggestion
divergence critic likely useful wide variety provers.
divergence critic several limitations. Recognizing divergence is, general,
undecidable since reduces halting problem. divergence critic therefore
sometimes fail identify diverging proof attempt. addition, critic sometimes
identify \divergence" pattern proof attempt diverging. Even divergence correctly identified, critic sometimes fail speculate appropriate
lemma. Finally, critic speculates wave-rules. Whilst many theories contain large
number wave-rules, often useful fixing divergence, types
lemma needed.
Table 2 lists four theorems divergence critic fails. problems
representative different ways critic fail. two main cause
failure overlapping divergence patterns, inability heuristics speculate
appropriate right hand side lemma. times speculate lemmas
find proof theorem.
Example 31 commuted version recursive definition multiplication (
defined recursively second argument position). Spike's attempt prove theorem
229

fiWalsh
examples 6 7, optimal rules fixing divergence. Nevertheless, either proposed rules fix divergence proved without diculty
Spike. Example 9 similar example 8.
Examples 10 12 require little comment. example 13, proposed lemma
dicult proved automatically. However, divergence critic able identify
cause diculty propose lemma allows proof go (example
15). example 14, speculated lemma optimal. simpler lemma speculated
example 13 would adequate prove theorem without divergence. speculated
lemma optimal divergence critic attempts ripple accumulating
term structure two functors, len rot top term tree. However,
sucient problem ripple one functor, rot.
Examples 16 19 straightforward require discussion. example 20,
critic identifies two separate divergence patterns. overcome divergence, first lemma
plus one second third therefore needed. first divergence pattern
occurs sequence subgoals,

len(rev(x)) = 0 + len(x)
len( app(rev(x); cons(y; nil)) ) = s(0 + len(x))
len( app(app(rev(x); cons(y; nil)); cons(z; nil)) ) = s(s(0 + len(x)))
..
.

Term structure accumulating second argument append. term structure
removed first rule,

len( app(X; cons(Y; nil)) ) = s(len(X ))
second divergence pattern occurs sequence subgoals,

s(x) + len(y) = s(x + len(y))
s(s(x)) + len(y) = s(s(x + len(y)))
s(s(s(x))) + len(y) = s(s(s(x + len(y))))
..
.

Term structure accumulating first argument +. removed one
second third rules,

s(X ) + = s(X + )
s(X ) + = X + s(Y )
Examples 21 23 reasonably straightforward. lemma speculated example
22 special case associativity append. powerful generalization heuristics
could speculated associativity append. However, heuristics would also
228

fiA Divergence Critic
causes divergence current release. speculated lemmas do, however, simplify
proof. Example 4 used text illustrate generalization heuristics. second
lemma example 5 perhaps little surprising,

len(app(X; (cons(W; cons(Z; ) )))) = s(len(app(X; cons(W; )))) :
Although complex first lemma, nearly good fixing divergence.
example 6, lemma proposed,

even( s(s(X )) + ) = even(X + )
optimal. is, simplest possible lemma fixes divergence. fix
divergence, merely need one rules, s(X ) + = s(X + ) s(X ) + = X +
s(Y ). Either ripple successor functions accumulating first argument
position +. divergence critic attempts construct lemma ripple two successor
functions across first second argument positions +. Unfortunately,
critic fails find appropriate instantiation right hand side lemma.
critic instead proposes rule move two successor functions top term
wave-front peter out. Example 7 similar example 6.
Examples 8 10 demonstrate critic cope divergence theories involving mutual recursion. example 8, Spike attempts prove induction equations,

evenm(x + x)
oddm(s(x) + x)
evenm (s(s(x)) + x)
oddm(s(s(s(x))) + x)
evenm (s(s(s(s(x)))) + x)

=
=
=
=
=
..
.

true
true
true
true
true

critic identifies two inter-linking divergence patterns,

evenm (x + x) = true
evenm ( s(s(x)) + x) = true
evenm ( s(s(s(s(x)))) + x) = true

oddm(s(x) + x) = true
oddm( s(s(s(x))) + x) = true
oddm( s(s(s(s(s(x))))) + x) = true

..
.

..
.

critic therefore proposes rules ripple accumulating term structure
top term peters out,

evenm( s(s(X )) + ) = evenm (X + )
oddm( s(s(X )) + ) = oddm(X + )
227

fiWalsh

1

Theorem
s(x)+x=s(x+x)

Lemmas speculated
Time/s
s(X)+Y=s(X+Y)
7.8
s(X)+Y=X+s(Y)
2
dbl(x)=x+x $
s(X)+Y=s(X+Y)
8.2
dbl(0)=0, dbl(s(x))=s(s(dbl(x)))
s(X)+Y=X+s(Y)
3
len(x @ y)=len(y @ x)
len(X @ (Z ::Y))=s(len(X @ Y))
3.6
len(X @ (Z ::Y))=len((W ::X) @ Y)
4
len(x @ y)=len(x)+len(y)
s(X)+Y=s(X+Y)
7.2
s(X)+Y=X+s(Y)
5
len(x @ x)=dbl(len(x))
len(X @ (Z ::Y))=s(len(X @ Y))
11.6
len(X @ (W ::Z ::Y))=s(len(X @ (W ::Y)))
6
even(x+x)
even(s(s(X))+Y)=even(X+Y)
5.4
7
odd(s(x)+x)
odd(s(s(X))+Y)=odd(X+Y)
16.0
8
evenm (x+x)
evenm (s(s(X))+Y)=evenm (X+Y)
28.4
oddm (s(s(X))+Y)=oddm (X+Y)
9
oddm (s(x)+x)
evenm (s(s(X))+Y)=evenm (X+Y)
65.5
oddm (s(s(X))+Y)=oddm (X+Y)
10
evenm (x) ! half(x)+half(x)=x
s(X)+Y=s(X+Y)
6.0
s(X)+Y=X+s(Y)
11
half(x+x)=x
s(s(X))+Y=X+s(s(Y))
11.1
half(s(s(X))+Y)=half(X+Y)
12
half(s(x)+x)=x
s(s(X))+Y=X+s(s(Y))
31.0
half(s(s(X))+Y)=half(X+Y)
13
rot(len(x),x)=x
rot(len(X),X @ [Y])=Y::rot(len(X),X)
2.4
14
len(rot(len(x),x))=len(x)
len(rot(X,Z @ [Y]))=s(len(rot(X,Z)))
4.8
15
rot(len(x),x @ [y])=y ::rot(len(x),x)
(X @ [Y])@ Z=X @ (Y ::Z)
86.3
rot(len(X),(X @ [Y])@ Z)=Y ::rot(len(X),X @ Z)
16
len(rev(x))=len(x)
len(X @ [Y])=s(len(X))
2.0
17
rev(rev(x))=x
rev(X @ [Y])=Y::rev(X)
1.2
18
rev(rev(x) @ [y])=y ::x
rev(X @ [Y])=Y::rev(X)
16.0
19
rev(rev(x) @ [y])=y ::rev(rev(x))
rev(X @ [Y])=Y::rev(X)
18.6
20
len(rev(x @ y))=len(x)+len(y)
len(X @ [Y])=s(len(X))
10.0
s(X)+Y=s(X+Y)
s(X)+Y=X+s(Y)
21
len(qrev(x,[]))=len(x)
len(qrev(X,Z ::Y))=s(len(qrev(X,Y)))
2.2
22
qrev(x,y)=rev(x) @
(X @ [Y])@ Z=X @ (Y ::Z)
3.4
23
len(qrev(x,y))=len(x)+len(y)
s(X)+Y=s(X+Y)
12.0
s(X)+Y=X+s(Y)
24
qrev(qrev(x,[]),[])=x
qrev(qrev(X,[Y]),Z)=Y ::qrev(qrev(X,[]),Z)
5.0
25
rev(qrev(x,[]))=x
rev(qrev(X,[Y]))=Y ::rev(qrev(X,[]))
5.8
26
qrev(rev(x),[])=x
qrev(X @ [Y],Z)=Y::qrev(X,Z)
5.2
27
nth(i,nth(j,x))=nth(j,nth(i,x))
nth(s(I),nth(J,Y ::X))=nth(I,nth(J,X))
7.4
28 nth(i,nth(j,nth(k,x)))=nth(k,nth(j,nth(i,x)))
nth(s(I),nth(J,Y ::X))=nth(I,nth(J,X))
7.6
29
len(isort(x))=len(x)
len(insert(Y,X))=s(len(X))
2.0
30
sorted(isort(x))
sorted(insert(Y,X))=sorted(X)
114
sorted(insert(Y,insert(Z,X)))=sorted(X)

Table 1: lemmas speculated divergence critic.
Notes: :: written infix cons, @ infix append, [] nil, [x] cons(x,nil).
addition, even defined s(s(x)) recursion, evenm mutual recursion oddm ,
rot(n; l) rotates list l n elements.
226

fiA Divergence Critic
critic successful identifying divergence proposing appropriate lemmas
generalizations significant number theorems. Divergence analysis quick
examples. divergence pattern recognized usually less second.
time spent looking generalizations refuting over-generalizations
conjecture disprover. usually takes 1 100 seconds. Additional heuristics
preventing over-generalization ecient implementation conjecture
disprover would speed critic considerably.

8. Results

Table 1 lists 30 theorems cause Spike diverge lemmas speculated
divergence critic analysing diverging proof attempts. problems provide
representative sample type theorems cause divergence
identified appropriate lemma generalization speculated. Many problems
come Clam library corpus. Part table appeared (Walsh, 1994).
Times divergence critic speculate lemmas average 10
runs Sun 4 running Quintus 3.1.1.
Spike's proof attempt diverges example given definitions alone.
30 cases, critic quickly able suggest lemma overcomes divergence.
multiple lemmas proposed (with exception 20) one
sucient fix divergence. every case (except 13 24) lemmas proposed
suciently simple proved automatically without introducing fresh divergence.
majority cases, lemmas proposed optimal; is, simplest possible
lemmas fix divergence. cases lemma optimal, usually
slightly complex simplest lemma fixes divergence. many
examples, lemmas conjectured divergence analysis quickly
rejected conjecture disprover. example, example 16, divergence analysis
petering heuristic suggest rule,
## len( app(X; cons(Y; nil)) ) = len(X ) ##
However, refuted exhaustive normalization using ground terms X .
case, cancellation heuristic identifies required lemma,
len( app(X; cons(Y; nil)) ) = s(len(X )) :
examples deserve additional comment. example 1, divergence critic
identifies successor functions accumulating first argument position +.
critic speculates lemma moving successor functions either top
term (so immediate cancellation occur) onto second argument position
(so simplification recursive definition + occur). first lemma
speculated fact generalization theorem proved. Example 2 simple
program verification problem taken Dershowitz Pinchover (1990). forward
direction theorem discussed introduction. Similar divergence occurs
example 1 and, generalization, lemmas speculated.
Example 3 caused divergence beta-version Spike available summer
1994. proof rules Spike since strengthened example longer
225

fiWalsh

% compiling file /home/dream5/tw/work/Spike/diverge/data.double.x+x
% data.double.x+x compiled module user, 0.233 sec 1,612 bytes
| ?- speculate.
Equations input:
double(x1)=x1+x1
s(x1+x1)=s(x1)+x1
s(s(x1+x1))=s(s(x1))+x1
s(s(s(x1+x1)))=s(s(s(x1)))+x1
Lemmas speculated:
s(x1)+x1=s(x1+x1)
s(x1)+x99=s(x1+x99)
s(x99)+x1=s(x99+x1)
s(x99)+x100=s(x99+x100)
s(x1)+x1=x1+s(x1)
s(x1)+x99=x1+s(x99)
s(x99)+x1=x99+s(x1)
s(x99)+x100=x99+s(x100)
Deleting lemmas subsumed:
s(x1)+x99=s(x1+x99)
s(x1)+x99=x1+s(x99)
Merging remaining lemmas:
s(x1)+x99=s(x1+x99)
s(x1)+x99=x1+s(x99)
yes
| ?-

Figure 4: Example output divergence critic.
Figure 1 gives divergence critic's output problem discussed introduction.
Either proposed lemmas used rewrite rule adequate fix divergence.
addition, proposed lemmas suciently simple proved automatically without
introducing fresh divergence. first lemma rewrite rule moving accumulating
successor functions first argument position + top term tree.
second lemma transverse wave rule discussed Section 6 moving accumulating
successor functions first argument position + second argument position.
224

fiA Divergence Critic

Preconditions:
1. sequence equations si = ti
prover attempts prove induction (i = 0, 1 ...);
2. exists (non trivial) G; H j ,
maximal difference match sj = G(Uj ; Acc)
sj+1 = G( H (Uj ) ; Acc).
Postconditions:
1. critic proposes rule form,

G( H (U0) ; Acc) = G(U0; F (Acc) )
2. F instantiated fertilization simplification
heuristics;
3. lemma generalized using (augmented) primary terms equality heuristics;
4. Generalized lemmas filtered type checker
conjecture disprover;
5. several lemmas suggested, critic deletes
subsumed.
Figure 3: Speculation transverse wave rules.
annotations. could also speculate hybrid wave rules ripple part wave-front
across part term tree. However, rules appear rare. addition,
hybrid wave rules often decomposed pair wave rules, one
moves wave-fronts term tree, another moves wave-fronts
across.

7. Implementation

divergence critic described previous sections implemented Prolog.
system consists 787 lines code defining approximate 100 different Prolog predicates. recently cut version incorporated directly within Spike
system written Caml Light (Bouhoula & Rusinowitch, 1995b). output
Spike parsed generate input critic. input consists of: equations
prover attempts prove induction; sort information (for type checker
difference matcher); recursive argument positions (for constructing primary terms);
rewrite rules defining theory (used conjecture disprover).
223

fiWalsh
rule allows proof go without divergence.
Speculated transverse wave rules generalized using extended primary terms
heuristic described Section 5. divergence critic also generalizes transverse wave
rules means equality heuristic. heuristic attempts cancel equal outermost
functors possible. example, consider theorem,

8x; : (x + y) , x =
addition defined recursively second argument position subtraction
defined rewrite rules,

X ,0 = X
0,X = 0
s(X) , s(Y) = X , Y:
Spike's attempt prove theorem diverges generating (amongst others) goals,

(x + ) , x =
(s(x) + ) , x = s(y )
(s(s(x)) + ) , x = s(s(y ))
..
.
Divergence analysis identifies accumulating term structure within equations,
(x + ) , x =
( s(x) + ) , x = s(y)
( s(s(x)) + ) , x = s(s(y ))
..
.
unique maximal difference match. annotations suggest need
transverse rule,
( s(X ) + ) , X = (X + s(Y ) ) , X:
equality heuristic deletes equal outermost function, z : z , X . gives
general lemma,
s(X ) + = X + s(Y ) :
speculated lemmas filtered type checker ensure erasure
well typed. Speculated lemmas also filtered conjecture disprover guard
over-generalization.
actions critic summarized Figure 3. specification preconditions
postconditions uses second order variables limited manner. implementation merely requires second order matching first order difference matching.
preconditions postconditions easily generalised include multiple nested
222

fiA Divergence Critic
qrev(a; b) = app(rev(a); b)
qrev(a; cons(c; b) ) = app( app(rev(a); cons(c; nil)) ; b)
qrev(a; cons(c; cons(d; b)) ) = app( app(app(rev(a); cons(c; nil)); cons(d; nil)) ; b)
..
.

unique maximal difference match. Rather move accumulating term
structure right hand side equations top term, much simpler
move accumulating term structure first onto second argument
outermost append. critic therefore proposes transverse wave rule, preserves
skeleton moves difference onto different argument position. example,
rule form,

app( app(rev(A); cons(C; nil)) ; B) = app(rev(A); F (B) ):
moving difference onto another argument position, difference may change syntactically. right hand side lemma therefore partially determined.
instantiate F , critic uses two heuristics: fertilization simplification.
fertilization heuristic uses matching find instantiation F enables
immediate fertilization. case, matching universally quantified variable b
induction hypothesis suggests,

app( app(rev(A); cons(C; nil)) ; B) = app(rev(A); cons(C; B) ):
Finally critic generalizes lemma using extended primary term heuristic
(i.e., augmenting recursive positions wave-hole positions). gives rule,

app( app(A; cons(C; nil)) ; B) = app(A; cons(C; B) ):
exactly rule needed Spike complete proof. addition, simple
enough proved without divergence; true ungeneralized rule.
heuristic used instantiate right hand side speculated lemma
simplification heuristic. heuristic uses regular matching find instantiation
F enable wave-front simplified using one recursive definitions.
Consider dbl theorem introduction. Divergence analysis identifies successor functions accumulating first argument position +. accumulating term
structure either moved top term tree alternatively onto second
argument position + using transverse wave rule form,

s(X ) + = X + F (Y ) :
right hand side transverse wave rule instantiated simplification heuristic.
wave-front right hand side simplified rewrite rule recursively
defining + F instantiated z : s(z ). is, rule,

s(X ) + = X + s(Y ) :
221

fiWalsh
f0; s(Y)g cover set natural numbers, two rules merged give,
sorted( insert(Y; X) ) = sorted(X):

6. Transverse Wave Rules
lemmas speculated far moved accumulating term structure directly top
term removed cancellation petering out. alternative way
removing accumulating term structure move onto another argument position where:
either removed matching \sink", universally quantified variable
induction hypothesis; moved upwards rewriting recursive
definitions. Annotated rewrite rules preserve skeleton move wave-fronts
across argument positions called transverse wave rules (Bundy et al., 1993).
Theorems involving functions accumulators provide rich source examples
rewrite rules prevent divergence.
Consider, example, theorem correctness tail recursive list reversal,

8a; b : qrev(a; b) = app(rev(a); b)
b universally quantified, rev naive list reversal using append,
qrev tail recursive list reversal building reversed list second argument position.
functions defined rewrite rules,
rev(nil)
rev(cons(H; T))
qrev(nil; R)
qrev(cons(H; T); R)

=
=
=
=

nil
app(rev(T); cons(H; nil))
R
qrev(T; cons(H; R)):

Spike's attempt prove theorem diverges generating following sequence equa-

tions prover attempts show induction,

qrev(a; b) = app(rev(a); b)
qrev(a; cons(c; b)) = app(app(rev(a); cons(c; nil)); b)
qrev(a; cons(c; cons(d; b))) = app(app(app(rev(a); cons(c; nil)); cons(d; nil)); b)
..
.

Difference matching identifies term structure accumulating within equations
causing divergence,
220

fiA Divergence Critic
s(0) + len(b) = s(len(b))
s(s(0)) + len(b) = s(s(len(b)))

..
.
Difference matching identifies term structure causing divergence,
0 + len(b) = len(b)
s(0) + len(b) = s(len(b))

s(s(0)) + len(b) = s(s(len(b)))

..
.
unique maximal difference match. annotations suggest need
wave rule,
s(0) + len(B) = s(0 + len(B)) :
set candidate terms generalization constructed computing intersection
primary terms two sides rule. case, primary terms left
hand side set fs(0) + len(B); len(B); Bg, primary terms right hand
side set fs(0 + len(B)); 0 + len(B); len(B); Bg. intersection primary terms
thus set flen(B); Bg. critic picks members intersection generalize new
variables. Picking B justs gives equivalent lemma renaming variables. Picking
len(B) gives generalization,
s(0) + = s(0 + Y) :
reason considering primary terms recursive definitions typically
provide wave rules removing term structure accumulates positions.
addition primary terms, divergence critic therefore also considers positions
wave-holes (but wave-fronts) skeleton lemma speculated.
motivation extension speculated lemma allow accumulating term
structure moved wave-hole positions; positions therefore also candidates generalization. Positions wave-fronts included since want
speculate lemma move term structure positions.
instance, wave-hole first argument + last example,
0 also included intersection set candidate terms generalization. Picking 0
generalize gives,
s(X) + = s(X + Y) :
speculated lemma general possible. rule allows proof go
without divergence.
critic also heuristic merging speculated lemmas. instance,
theorem sorted(isort(x)), critic speculates several rules including,
sorted( insert(0; X) ) = sorted(X)
sorted( insert(s(Y); X) ) = sorted(X)
219

fiWalsh

1. critic proposes rule form,

G( H (U0) ) = F (G(U0))
2. F instantiated cancellation petering
heuristics;
3. Lemmas filtered type checker conjecture disprover;
4. several lemmas suggested, critic deletes
subsumed.
Figure 2: Postconditions divergence critic

5. Generalization

major cause divergence need generalize. lemmas proposed
critic fix divergence, attempting prove lemmas cause
fresh divergence. addition, several speculated lemmas sometimes replaced
single generalization. Generalized lemmas also lead shorter, elegant
natural proofs. critic therefore attempts generalize lemma speculated, using
conjecture disprover guard over-generalization.
main heuristic used generalization extension primary term heuristic
(Aubin, 1976). primary terms terms encountered term explored
root leaves ignoring non-recursive argument positions functions.
notion recursive argument position used critic defined Bouhoula
Rusinowitch (1995a) used Spike performing inductions.
Consider, example, theorem,

8a; b : len(a) + len(b) = len(app(a; b))
+ defined recursively second argument, len app defined
means rewrite rules,
len(nil)
len(cons(H; T))
app(nil; T)
app(cons(H; T); R)

=
=
=
=

0
s(len(T))

cons(H; app(T; R)):

problem taken Clam library corpus (Bundy et al., 1990). Spike's attempt
prove theorem diverges. One sequences equations generated is,
0 + len(b) = len(b)
218

fiA Divergence Critic
Spike's diverging attempt prove theorem generates equations,

nth(s(i); nth(j; x)) = nth(s(j ); nth(i; x))
nth(s(s(i)); nth(j; cons(y; x))) = nth(s(j ); nth(i; x))
nth(s(s(s(i))); nth(j; cons(z; cons(y; x)))) = nth(s(j ); nth(i; x))
..
.

Divergence analysis identifies term structure accumulating two different places,

nth(s(i); nth(j; x)) = nth(s(j ); nth(i; x))
nth( s(s(i)) ; nth(j; cons(y; x) )) = nth(s(j ); nth(i; x))
nth( s(s(s(i))) ; nth(j; cons(z; cons(y; x)) )) = nth(s(j ); nth(i; x))
..
.

unique maximal difference match. divergence pattern suggests need
rewrite rule form,

nth( s(I ) ; nth(J; cons(Y; X ) )) = F (nth(I; nth(J; X ))) :
petering heuristic instantiates F identity function z : z giving rule,

nth( s(I ) ; nth(J; cons(Y; X) )) = nth(I; nth(J; X )):
rule allows proof go without divergence.
Since erasure wave rule must properly typed, sort information used
prune inappropriate instantiations F . speculated lemmas therefore filtered
type checker. Speculated lemmas also filtered conjecture disprover.
con uent set rewrite rules exists ground terms, exhaustive normalization
representative set ground instances equations used filter nontheorems. Alternatively, prover could used filter non-theorems. Unlike
many induction theorem provers, Spike refute conjectures since inference
rules refutationally complete conditional theories axioms ground
convergent defined functions completely defined free constructors (Bouhoula &
Rusinowitch, 1995a). techniques disproving conjectures described Protzen
(1992).
critic's lemma speculation summarized Figure 2 (using variable
names preconditions). specification uses second order variables
limited manner. First order difference matching merely required construct lemmas.
preconditions, specification postconditions easily extended
deal multiple nested wave-fronts (as nth(i; nth(j; l)) = nth(j; nth(i; l))
example). Since rules proposed critic move wave-fronts top term,
usually introduce fresh divergence rare cases cancellation fertilization
fails. unlikely since cancellation petering heuristics attempt ensure
precisely cancellation fertilization take place.
217

fiWalsh
divergence pattern suggests F instantiated z : s(z) enable immediate cancellation. Thus, required, cancellation heuristic suggests rule,
s(X ) + = s(X + ) :
heuristic used instantiate right hand side speculated lemmas
petering out. moving differences top term, may disappear
altogether. Consider, example, theorem,
8l : sorted(isort(l)) = true
isort insertion sort sorted true iff list sorted order. defined
conditional rewrite rules,
sorted(nil) = true
sorted(cons(X; nil)) = true
X < ! sorted(cons(X; cons(Y; Z ))) = sorted(cons(Y; Z ))
isort(nil) = nil
isort(cons(X; )) = insert(X; isort(Y ))
insert(X; Z ), inserts element X list Z order, X <
defined rewrite rules,
0 < X = true
s(X ) < 0 = false
s(X ) < s(Y ) = X <
insert(X; nil) = cons(X; nil)
X < ! insert(X; cons(Y; Z )) = cons(X; cons(Y; Z ))
:(X < ) ! insert(X; cons(Y; Z )) = cons(Y; insert(X; Z ))
Divergence analysis Spike's attempt prove theorem suggests need
rule form,
sorted( insert(Y; X) ) = F (sorted(X) :
petering heuristic instantiates F identity function z : z. gives rule,
sorted( insert(Y; X) ) = sorted(X):
rule allows proof go without divergence.
complex example, consider theorem,
8i; j; l : nth(i; nth(j; l)) = nth(j; nth(i; l))
nth defined rewrite rules,
nth(0; L) = L
nth(N; nil) = nil
nth(s(N ); cons(H; )) = nth(N; ):
216

fiA Divergence Critic
nested annotations. allows critic recognise multiple sources divergence
equation. Techniques identify accumulating term structure specific
generalization (Dershowitz & Pinchover, 1990) cannot cope divergence patterns
give rise nested annotations (see Section 9 details).
specification preconditions left length sequence undefined.
sequence length 2, critic preemptive. is, propose lemma
another induction attempted divergence begins. short sequence risks
identifying divergence none exists. hand using long sequence expensive test allows prover waste time diverging proof attempts. Empirically,
good compromise appears look sequences length 3. cheap
test reliable. identify accumulating term structure, appears sucient
use ground difference matching alpha conversion variable names. exists
fast polynomial algorithm perform difference matching based upon ground
difference matching algorithm using dynamic programming (Basin & Walsh, 1993). Since
skeleton must well typed (along erasure), algorithm extended use
sort information prune potential difference matches.

4. Lemma Speculation
One way removing accumulating nested term structure propose wave rule
moves difference top term leaving skeleton unchanged. hope
either cancel wave-fronts side equality
disappear process moved. dbl theorem, generalization
(which discussed next section) divergence pattern suggests rule form,

s(X ) + = F (X + )
F second order variable need instantiate. Instantiating F ultimately dicult synthesis problem hope heuristics work
time. Two heuristics used divergence critic instantiate F
cancellation petering out.
cancellation heuristic uses difference matching identify term structure accumulating opposite side sequence would allow cancellation occur. Failing
that, cancellation heuristic looks suitable term structure cancel new
sequence (the original sequence usually divergence pattern step case, whilst
new sequence usually divergence pattern base case). dbl example, successor
functions accumulate top left hand side diverging equations,

s(x + x) = s(x) + x
s(s(x + x)) = s(s(x)) + x
s(s(s(x + x))) = s(s(s(x))) + x
..
.

215

fiWalsh
critic attempts find accumulating nested term structure
sequence causing divergence. case, successor functions accumulating
first argument +. identify accumulating term structure, critic uses
difference matching. Difference matching successive equations gives annotated sequence,

s(x + x) = s(x) + x
s(s(x + x)) = s(s(x)) + x
s(s(s(x + x))) = s(s(s(x))) + x
..
.

unique maximal difference match.
critic tries speculate lemma used rewrite rule move
accumulating nested term structure way. case, critic speculates
rule moving successor function first argument +. is, rule,

s(X ) + = s(X + ) :
rule, Spike able prove dbl theorem without divergence. addition,
rule suciently simple proved without assistance. heuristics used
critic perform lemma speculation described detail next two
sections.
divergence analysis performed critic summarised Figure 1. analysing
1. sequence equations si = ti
prover attempts prove induction (i = 0, 1 ...);
2. exists (non trivial) G; H j ,
maximal difference match sj = G(Uj ), sj +1 =
G( H (Uj ) ).
Figure 1: Preconditions divergence critic
divergence, consider equations prover attempts prove induction. includes equation induction proof succeeds often
suggest useful patterns. \non-trivial" wish exclude z : z, identity substitution. H thus accumulating nested term structure appears causing
divergence. dbl example, H z : s(z), G z : z + x, U0 s(x). Although
G H second order variables, second order nature divergence analysis
limited. Indeed, implementation critic merely requires first order difference
matching polynomial. simplicity, preconditions ignore orientation
equations. addition, preconditions easily generalised include multiple
214

fiA Divergence Critic
Rippling several desirable properties. highly goal directed, manipulating
differences induction hypothesis induction conclusion.
annotations restrict application rewrite rules, rippling also involves little
search. Difference matching rippling proved useful domains outside explicit
induction. example, used sum series (Walsh, Nunes, & Bundy, 1992)
prove limit theorems (Yoshida, Bundy, Green, Walsh, & Basin, 1994). rest
paper, show difference matching rippling also useful identifying
correcting divergence prover neither uses explicit rules induction uses
annotations control rewriting.

3. Divergence Analysis

initial problem recognizing proof diverging. Various properties rewrite
rules identified cause divergence like, example, forwards backwards
crossed systems (Hermann, 1989). However, properties fail capture diverging
rewrite systems since problem is, general, undecidable. divergence critic instead
studies proof attempt looking patterns divergence; attempt made analyse
rewrite rules structures give rise divergence. advantage
approach critic need know details rewrite rules applied,
type induction performed, control structure used prover. critic
thus recognise divergence patterns arising complex mutual multiple inductions
little diculty divergence patterns arising simple straightforward
inductions. disadvantage approach critic identify \divergence"
pattern none exists. Fortunately, cases appear rare, even
occur, critic usually suggests lemma generalization gives shorter
elegant proof (see Section 8 example).
illustrate ideas behind critic's divergence analysis, consider theorem
introduction,
8n : dbl(n) = n + n:
divergence critic first partitions sequence equations prover attempts
prove induction. necessary since several diverging sequences may interleaved
prover's output. Several heuristics used reduce number partitions
considered. useful heuristic parentage sequence partitioned
equation derived previous one. is, equations lie
single branch proof tree. particular, base case step case induction
partitioned different sequences. heuristics used include:
function constant symbols occur one equation occur next equation
partition, weights equations partition form simple arithmetic
progression. case, single open branch proof tree,
s(x + x) = s(x) + x
s(s(x + x)) = s(s(x)) + x
s(s(s(x + x))) = s(s(s(x))) + x
..
.
213

fiWalsh
annotated term r. Difference matching unitary. is, two terms
one difference match. example, s(s(x)) s( s(x) ) difference matches
s(s(x)) s(x). number difference matches reduced compute
maximal difference match wave-fronts high possible term tree.
formal definition well founded ordering annotated terms given
Basin Walsh (1994).
aim rippling rewrite annotated induction conclusion skeleton,
induction hypothesis, preserved differences, wave-fronts moved
harmless places (for example, top term). rewriting succeeds,
able appeal induction hypothesis. rewrite annotated induction
conclusion, use following annotated rewrite rules, wave rules:

dbl( s(X ) ) = s(s(dbl(X )))
X + s(Y ) = s(X + )
s(X ) + = s(X + )

(1)
(2)
(3)

first two annotated rewrite rules derived recursive definitions dbl
+ whilst second derived lemma proposed end introduction.
annotated rewrite rules preserves skeleton term rewritten,
moves wave-fronts higher term tree. Wave rules guarantee this: wave rule
annotated rewrite rule identical skeleton left right hand sides
moves wave-fronts well founded direction like, instance, top term tree
(Basin & Walsh, 1994).
Rippling left hand side annotated induction conclusion using (1) yields,

s(s(dbl(x))) = s(x) + s(x) :
rippling right hand side (2) gives,

s(s(dbl(x))) = s( s(x) + x) :
Finally rippling (3) right hand side yields,

s(s(dbl(x))) = s(s(x + x)) :
wave-fronts top term, successfully rippled sides
equality. appeal induction hypothesis left hand side giving,

s(s(x + x)) = s(s(x + x)) :
simple identity proof complete. Note complete proof,
needed rewrite lemma, (3). aim divergence critic described
paper propose lemmas.
212

fiA Divergence Critic
Section 2, describe difference matching rippling, two key ideas heart
divergence critic. outline difference matching identifies accumulating
term structure causing divergence (Section 3). Section 4 6, show
lemmas speculated \ripple" term structure way. Section 5,
describe heuristics used generalizing lemmas. Finally, implementation
results described Sections 7 8.

2. Difference matching rippling

Rippling powerful heuristic developed Edinburgh proving theorems involving
explicit induction (Bundy, Stevens, van Harmelen, Ireland, & Smaill, 1993) implemented Clam theorem prover (Bundy, van Harmelen, Horn, & Smaill, 1990).
step case inductive proof, induction conclusion typically differs induction
hypothesis addition constructors destructors. Rippling uses annotations
mark differences applies annotated rewrite rules remove them.
simple example, consider theorem discussed introduction.
step case, induction hypothesis is,

dbl(x) = x + x
induction conclusion is,

dbl(s(x)) = s(x) + s(x):
\difference match" induction conclusion induction hypothesis (Basin
& Walsh, 1992), obtain following annotated induction conclusion,

dbl( s(x) ) = s(x) + s(x) :
annotation consists wave-front, box wave-hole, underlined term. Wavefronts always one functor thick (Basin & Walsh, 1994). is, every wave-front
one immediate subterm annotated wave-hole. make presentation simpler,
display adjacent wave-fronts merged. Thus, s(s(x)) syntactic sugar
annotated term, s( s(x) ) . Wave-fronts also include arrows indicate
whether moving towards top term tree towards leaves.
extension can, however, safely ignored here.
skeleton annotated term formed deleting everything appears
wave-front wave-hole. erasure annotated terms formed
deleting annotations terms contain. case, skeleton
annotated induction conclusion identical induction hypothesis, erasure
annotated induction conclusion unannotated induction conclusion. Difference
matching guarantees this; is, difference matching induction conclusion
induction hypothesis annotates induction conclusion skeleton matches
induction hypothesis.
Formally, r difference match substitution iff (skeleton(r)) =
erase(r) = skeleton(r) erase(r) build skeleton erasure
211

fiWalsh
alpha convert variable names necessary. Rewriting induction conclusion
recursive definitions dbl + gives,

s(s(dbl(x))) = s(s(x) + x):
outermost successor functions either side equality cancelled,

s(dbl(x)) = s(x) + x:
prover fertilizes induction hypothesis left hand side,

s(x + x) = s(x) + x:
equation cannot simplified another induction performed. Unfortunately,
generates diverging sequence subgoals,

s(x + x)
s(s(x + x))
s(s(s(x + x)))
s(s(s(s(x + x))))
s(s(s(s(s(x + x)))))

=
=
=
=
=
..
.

s(x) + x
s(s(x)) + x
s(s(s(x))) + x
s(s(s(s(x)))) + x
s(s(s(s(s(x))))) + x

problem prover repeatedly tries induction x unable simplify
successor functions introduces first argument position +. proof
go without divergence rewrite rule,

s(X ) + = s(X + ):
rule \ripples" accumulating successor functions first argument position +.
rewrite rule derived lemma,

8x; : s(x) + = s(x + y):
commuted version recursive definition addition is, coincidently,
generalization first subgoal. lemma proved without divergence
induction variable, occurs second argument position +.
paper describe simple \divergence critic", computer program attempts
automate process. divergence critic identifies proof attempt diverging
means \difference matching" procedure. critic proposes lemmas
generalizations hopefully allow proof go without divergence. Although
critic designed work prover Spike, also work induction
provers (Walsh, 1994). Spike rewrite based theorem prover first order conditional
theories. contains powerful rules case analysis, simplification implicit induction
using notion test set. Unfortunately, case inductive theorem
provers, attempts prove many theorems diverge without appropriate generalization
addition suitable lemma.
210

fiJournal Artificial Intelligence Research 4 (1996) 209-235

Submitted 1/96; published 4/96

Divergence Critic Inductive Proof
toby@itc.it

Toby Walsh

IRST, Location Pante di Povo
I38100 Trento, ITALY

Abstract

Inductive theorem provers often diverge. paper describes simple critic, computer program monitors construction inductive proofs attempting identify
diverging proof attempts. Divergence recognized means \difference matching"
procedure. critic proposes lemmas generalizations \ripple" differences away proof go without divergence. critic enables
theorem prover Spike prove many theorems completely automatically definitions alone.

1. Introduction

Two key problems inductive theorem proving proposing lemmas generalizations.
prover's divergence often suggests user appropriate lemma generalization
enable proof go without divergence. simple example, consider
theorem,

8n : dbl(n) = n + n:
part simple program verification problem (Dershowitz & Pinchover, 1990).
Addition doubling defined recursively means rewrite rules,

X+0 = X
X + s(Y ) = s(X + )
dbl(0) = 0
dbl(s(X )) = s(s(dbl(X )))
s(X ) represents successor X (that is, X + 1). adopted Prolog
convention writing meta-variables like X upper case.

theorem prover Spike (Bouhoula, Kounalis, & Rusinowitch, 1992) fails prove
theorem. proof attempt begins simple one step induction x. base
case trivial. step case, induction hypothesis is,
dbl(x) = x + x
induction conclusion is,

dbl(s(x)) = s(x) + s(x):
ease presentation, variables paper are, here, sometimes renamed
introduced Spike. effect results prover divergence critic

c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiJournal Artificial Intelligence Research 4 (1996) 129-145

Submitted 11/95; published 3/96

Active Learning Statistical Models
David A. Cohn
Zoubin Ghahramani
Michael I. Jordan

Center Biological Computational Learning
Dept. Brain Cognitive Sciences
Massachusetts Institute Technology
Cambridge, 02139 USA

cohn@harlequin.com
zoubin@cs.toronto.edu
jordan@psyche.mit.edu

Abstract

many types machine learning algorithms, one compute statistically \optimal" way select training data. paper, review optimal data selection
techniques used feedforward neural networks. show
principles may used select data two alternative, statistically-based learning architectures: mixtures Gaussians locally weighted regression. techniques
neural networks computationally expensive approximate, techniques
mixtures Gaussians locally weighted regression ecient accurate. Empirically, observe optimality criterion sharply decreases number training
examples learner needs order achieve good performance.

1. Introduction
goal machine learning create systems improve performance
task acquire experience data. many natural learning tasks, experience
data gained interactively, taking actions, making queries, experiments.
machine learning research, however, treats learner passive recipient data
processed. \passive" approach ignores fact that, many situations,
learner's powerful tool ability act, gather data, uence world
trying understand. Active learning study use ability effectively.
Formally, active learning studies closed-loop phenomenon learner selecting actions making queries uence data added training set. Examples
include selecting joint angles torques learn kinematics dynamics robot
arm, selecting locations sensor measurements identify locate buried hazardous
wastes, querying human expert classify unknown word natural language
understanding problem.
actions/queries selected properly, data requirements problems
decrease drastically, NP-complete learning problems become polynomial computation time (Angluin, 1988; Baum & Lang, 1991). practice, active learning offers
greatest rewards situations data expensive dicult obtain,
environment complex dangerous. industrial settings training point may take
days gather cost thousands dollars; method optimally selecting points
could offer enormous savings time money.
c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiCohn, Ghahramani & Jordan

number different goals one may wish achieve using active learning. One optimization, learner performs experiments find set inputs
maximize response variable. example optimization problem would
finding operating parameters maximize output steel mill candy factory.
extensive literature optimization, examining cases learner
prior knowledge parameterized functional form cases learner
knowledge; latter case generally greater interest machine learning
practitioners. favored technique kind optimization usually form response surface methodology (Box & Draper, 1987), performs experiments guide
hill-climbing input space.
related problem exists field adaptive control, one must learn control
policy taking actions. control problems, one faces complication value
specific action may known many time steps taken. Also, control
(as optimization), one usually concerned performing well learning
task must trade exploitation current policy exploration may improve
it. subfield dual control (Fe'ldbaum, 1965) specifically concerned finding
optimal balance exploration control learning.
paper, restrict examining problem supervised learning:
based set potentially noisy training examples = f(xi; yi )gmi=1, xi 2 X
yi 2 , wish learn general mapping X ! . robot control, mapping may
state action ! new state; hazard location may sensor reading ! target position.
contrast goals optimization control, goal supervised learning
able eciently accurately predict given x.
active learning situations, learner responsible acquiring training
set. Here, assume iteratively select new input x~ (possibly constrained
set), observe resulting output y~, incorporate new example (~x; y~) training
set. contrasts related work Plutowski White (1993), concerned
filtering existing data set. case, x~ may thought query, experiment,
action, depending research field problem domain. question
concerned choose x~ try next.
many heuristics choosing x~, including choosing places don't
data (Whitehead, 1991), perform poorly (Linden & Weber, 1993),
low confidence (Thrun & Moller, 1992), expect change model (Cohn,
Atlas, & Ladner, 1990, 1994), previously found data resulted learning
(Schmidhuber & Storck, 1993). paper consider one may select x~
statistically \optimal" manner classes machine learning algorithms. first
brie review statistical approach applied neural networks, described
earlier work (MacKay, 1992; Cohn, 1994). Then, Sections 3 4 consider two
alternative, statistically-based learning architectures: mixtures Gaussians locally
weighted regression. Section 5 presents empirical results applying statistically-based
active learning architectures. optimal data selection neural network
computationally expensive approximate, find optimal data selection
two statistical models ecient accurate.
130

fiActive Learning Statistical Models

2. Active Learning { Statistical Approach

begin defining P (x; ) unknown joint distribution x , P (x)
known marginal distribution x (commonly called input distribution).
denote learner's output input x, given training set y^(x; D).1 write
expected error learner follows:
Z

h

x



ET (^y (x; D) , (x))2 jx P (x)dx;

(1)

ET [] denotes expectation P (y jx) training sets D. expectation
inside integral may decomposed follows (Geman, Bienenstock, & Doursat, 1992):
h



h



ET (^y (x; D) , y(x))2 jx = E (y (x) , E [y jx])2
+ (EDh [^y (x; D)] , E [y jx])2
+ED (^y (x; D) , ED [^y(x; D)])2

(2)

ED [] denotes expectation training sets remaining expectations
right-hand side expectations respect conditional density P (y jx).
important remember case active learning, distribution may
differ substantially joint distribution P (x; ).
first term Equation 2 variance given x | noise
distribution, depend learner training data. second term
learner's squared bias, third variance; last two terms comprise
mean squared error learner respect regression function E [y jx].
second term Equation 2 zero, say learner unbiased. shall assume
learners considered paper approximately unbiased; is,
squared bias negligible compared overall mean squared error. Thus
focus algorithms minimize learner's error minimizing variance:
h



y2^ y2^(x) = ED (^y (x; D) , ED [^y (x; D)])2 :

(3)

(For readability, drop explicit dependence x | unless denoted otherwise, y^ y2^ functions x D.) active learning setting, chosen
x-component training set D; indicate rewriting Equation 3


E

y2^ = (^y , hy^i)2 ;
hi denotes ED [] given fixed x-component D. new input x~ selected
queried, resulting (~x; y~) added training set, y2^ change.
denote expectation (over values y~) learner's new variance


E

h



~y2^ = ED[(~x;y~) y2^jx~ :

(4)

1. present equations univariate setting. results paper apply equally multivariate case.

131

fiCohn, Ghahramani & Jordan

2.1 Selecting Data Minimize Learner Variance

paper consider algorithms active learning select data attempt
minimize value Equation 4, integrated X . Intuitively, minimization proceeds
follows: assume estimate y2^, variance learner x. If,
new input x~, knew conditional distribution P (~y jx~), could compute
estimate learner's new variance x given additional example x~.
true distribution P (~y jx~) unknown, many learning architectures let us approximate
giving us Destimates
mean variance. Using estimated distribution y~,
E
2
estimate ~y^ , expected variance learner querying x~.
E
Given estimate ~y2^ , applies given x given query x~, must
integrate x input distribution compute integrated average variance
learner.D Epractice, compute Monte Carlo approximation integral, evaluating ~y2^ number reference points drawn according P (x). querying
x~ minimizes average expected variance reference points, solid
statistical basis choosing new examples.

2.2 Example: Active Learning Neural Network

section review use techniques Optimal Experiment Design (OED)
minimize estimated variance neural network (Fedorov, 1972; MacKay, 1992; Cohn,
1994). assume given learner y^ = fw^ (), training set = f(xi ; yi)gmi=1
parameter vector estimate w^ maximizes likelihood measure given D. If,
example, one assumes data produced process whose structure matches
network, noise process outputs normal independently
identically distributed, negative log likelihood w^ given proportional

2 = m1


X
i=1

(yi , y^(xi ))2 :

maximum likelihood estimate w^ minimizes 2.
estimated output variance network

y2^ 2



!
@ y^(x) @ 2 2 ,1 @ y^(x) ; (MacKay, 1992)
@w
@w2
@w

true variance approximated second-order Taylor series expansion around

2. estimate makes assumption @ y^=@w locally linear. Combined
assumption P (yDjx)Eis Gaussian constant variance x, one derive closed
form expression ~y2^ . See Cohn (1994) details.
practice, @ y^=@w may highly nonlinear, P (y jx) may far Gaussian;

spite this, empirical results show works well problems (Cohn, 1994).
advantage grounded statistics, optimal given assumptions.
Furthermore, expectation differentiable respect x~. such, applicable
continuous domains continuous action spaces, allows hillclimbing find x~
minimizes expected model variance.
132

fiActive Learning Statistical Models

neural networks, however, approach many disadvantages. addition
relying simplifications assumptions hold approximately, process
computationally expensive. Computing variance estimate requires inversion jwjjwj
matrix new example, incorporating new examples network requires
expensive retraining. Paass Kindermann (1995) discuss Markov-chain based sampling
approach addresses problems. rest paper, consider
two \non-neural" machine learning architectures much amenable optimal
data selection.

3. Mixtures Gaussians
mixture Gaussians model powerful estimation prediction technique
roots statistics literature (Titterington, Smith, & Makov, 1985); has, last
years, adopted researchers machine learning (Cheeseman et al., 1988; Nowlan,
1991; Specht, 1991; Ghahramani & Jordan, 1994). model assumes data
produced mixture N multivariate Gaussians gi, = 1; :::; N (see Figure 1).
context learning random examples, one begins producing joint density
estimate input/output space X based training set D. EM algorithm
(Dempster, Laird, & Rubin, 1977) used eciently find locally optimal fit
Gaussians data. straightforward compute y^ given x conditioning
joint distribution x taking expected value.

y1





2









g

g

2









g3



1

x

Figure 1: Using mixture Gaussians compute y^. Gaussians model data
density. Predictions made mixing conditional expectations
Gaussian given input x.
One benefit learning mixture Gaussians fixed distinction
inputs outputs | one may specify subset input-output dimensions,
compute expectations remaining dimensions. one learned forward model
dynamics robot arm, example, conditioning outputs automatically gives
model arm's inverse dynamics. mixture model, also straightforward
compute mode output, rather mean, obviates many
problems learning direct inverse models (Ghahramani & Jordan, 1994).
133

fiCohn, Ghahramani & Jordan

Gaussian gi denote input/output means x;i y;i vari2 , 2 xy;i respectively. express probability
ances covariances x;i
y;i
point (x; ), given gi


1
1

,
1
(5)
P (x; ji) = p exp , 2 (x , ) (x , )
2 ji j
defined
"

x = xy

#

"

= x;i
y;i

#

"

#

2
xy;i :
= x;i
2
xy;i y;i

practice, true means variances unknown, estimated data
via EM algorithm. (estimated) conditional variance given x
2
2 , xy;i :
y2jx;i = y;i
2
x;i
conditional expectation y^i variance y2^;i given x are:
2!
y2jx;i

(
x
,

)
xy;i
x;i
2
y^i = y;i + 2 (x , x;i ); y^;i = n 1 + 2
:
(6)

x;i
x;i
Here, ni amount \support" Gaussian gi training data.

computed

ni =


X
j =1

P (xj ; yj ji) :
k=1 P (xj ; yj jk)

PN

expectations variances Equation 6 mixed according probability
gi responsible x, prior observing :
hi hi (x) = PNP (xji) ;
j =1 P (xjj )

"
2#
(
x
,

)
1
x;i
:
(7)
P (xji) = q 2 exp , 22
2x;i
x;i
input x then, conditional expectation y^ resulting mixture variance
may written:
N
N h2i 2
2!
X
X
(
x
,

)
yjx;i
x;i
2
y^ = h y^ ; =
1+
;
i=1



y^

i=1

ni

2
x;i

assumed y^i independent calculating y2^. terms
computed eciently closed form. also worth noting y2^ one many
variance measures might interested in. If, example, mapping stochastically
multivalued (that is, Gaussians overlapped significantly x dimension), may
wish prediction y^ ect likely value. case, y^ would mode,
preferable measure uncertainty would (unmixed) variance individual
Gaussians.
134

fiActive Learning Statistical Models

3.1 Active Learning Mixture Gaussians

context active learning, assuming input distribution P (x) known.
mixture Gaussians, one interpretation assumption know x;i
2 Gaussian. case, application EM estimate y;i , 2 ,
x;i
y;i
xy;i .
Generally however, knowing input distribution correspond knowing
2 Gaussian. may simply know, example, P (x)
actual x;i x;i
uniform, approximated set sampled inputs. cases, must use
2 addition parameters involving . simply estimate
EM estimate x;i x;i
values training data, though, estimating joint distribution
P (~x; ji) instead P (x; yji). obtain proper estimate, must correct Equation 5
follows:
(8)
P (x; yji) = P (~x; ji) PP ((~xxjjii)) :
Here, P (~xji) computed applying Equation 7 given mean x variance
training data, P (xji) computed applying equation using mean
x variance set reference data drawn according P (x).
goal inD active
learning minimize variance, select
examples
E
training
E
2
2
x~ minimize ~y^ . mixture Gaussians, compute ~y^ eciently.
model's estimated distribution y~ given x~ explicit:

P (~y jx~) =

N
X

h~i P (~yjx~; i) =

N
X

~hi N (^yi (~x); y2jx;i(~x));

i=1
i=1
h~ hi (~x), N (; 2) denotes normal distribution mean variance
2. Given this, model change gi separately, calculating expected

variance given new point sampled P (~y jx~; i) weight change ~hi . new
expectations combine form learner's new expected variance

E
N h2i ~y2jx;i
E
2!
X
(
x
,

)
x;i
2
~y^ =
1+
(9)
2
~
x;i
i=1 ni + hi

expectation computed exactly closed form:

E


2
2 + (^
2
~
2

E

E

E
n

~
h


(~
x
)
,

)
n




y;i
yjx~;i
2 = y;i +
2 , xy;i ;
~y;i
;
~y2jx;i = ~y;i
2
2
~
~
x;i
ni + hi
(ni + hi )

E

E
~
n2~h22 (~x , x;i )2
2
~xy;i = ni xy;i~ + ni hi (~x , x;i)(^~yi (~2x) , y;i ) ;
~xy;i
= h~xy;i i2 + yjx~;i ~ 4
:
ni + hi
(ni + hi )
(ni + hi )
2 , must take account
If, discussed earlier, also estimating x;i x;i
2
effect new example estimates, must replace x;i x;i
equations
2
~ (~x , x;i )2
~
2 = ni x;i + ni h
:
~x;i = ni x;i +~hi x~ ; ~x;i
ni + hi
ni + ~hi
(ni + ~hi )2
135

fiCohn, Ghahramani & Jordan

use Equation 9 guide active learning. evaluating expected new variance
reference set given candidate x~, select x~ giving lowest expected model
variance. Note high-dimensional spaces, may necessary evaluate excessive
number candidate points get good coverage potential query
Espace. cases,
ecient differentiate Equation 9 hillclimb @ ~y2^ =@ x~ find locally
maximal x~. See, example, (Cohn, 1994).

4. Locally Weighted Regression
Model-based methods, neural networks mixture Gaussians, use data
build parameterized model. training, model used predictions
data generally discarded. contrast, \memory-based" methods non-parametric
approaches explicitly retain training data, use time prediction needs
made. Locally weighted regression (LWR) memory-based method performs
regression around point interest using training data \local" point.
One recent study demonstrated LWR suitable real-time control constructing
LWR-based system learned dicult juggling task (Schaal & Atkeson, 1994).



















x

Figure 2: locally weighted regression, points weighted proximity current
x question using kernel. regression computed using weighted
points.
consider form locally weighted regression variant LOESS
model (Cleveland, Devlin, & Grosse, 1988). LOESS model performs linear regression
points data set, weighted kernel centered x (see Figure 2). kernel
shape design parameter many possible choices: original LOESS
model uses \tricubic" kernel; experiments used Gaussian

hi(x) h(x , xi ) = exp(,k(x , xi )2);
k smoothing parameter. Section 4.1 describe several methods
automatically setting k.
136

fiActive Learning Statistical Models



















kernel wide includes nonlinear region
kernel right
kernel narrow excludes linear region

x

Figure 3: estimator variance minimized kernel includes many training
points accommodated model. linear LOESS model
shown. large kernel includes points degrade fit; small kernel
neglects points increase confidence fit.
P

brevity, drop argument x hi (x), define n = hi .
write estimated means covariances as:
P
P
P
2
= hi xi ; 2 = hi (xi , x ) ; = hi(xi , x)(yi , )
x

xy
x
n
n
n
P
2
2

= inhiyi ; y2 = hi (yni , ) ; y2jx = y2 , xy2 :
x
P

use data covariances express conditional expectations estimated
variances:
y2jx X 2 (x , x )2 X 2 (xi , x )2 !

xy
2
y^ = + (x , ); =
h +
h
(10)


x2

x

y^

n2



4.1 Setting Smoothing Parameter k



x2





x2

number ways one set k, smoothing parameter. method used
Cleveland et al. (1988) set k reference point predicted
predetermined amount support, is, k set n close target value.
disadvantage requiring assumptions noise smoothness
function learned. Another technique, used Schaal Atkeson (1994), sets k
minimize crossvalidated error training set. disadvantage technique
assumes distribution training set representative P (x),
may active learning situation. third method, also described Schaal
Atkeson (1994), set k minimize estimate y2^ reference points.
k decreases, regression becomes global. total weight n increase (which
decreases y2^ ), conditional variance y2jx (which increases y2^).
value k, two quantities balance produce minimum estimated variance (see
Figure 3). estimate computed arbitrary reference points domain,
137

fiCohn, Ghahramani & Jordan

user option using either different k reference point single
global k minimizes average y2^ reference points. Empirically, found
variance-based method gave best performance.

4.2 Active Learning Locally Weighted Regression



E

mixture Gaussians, want select x~ minimize ~y2^ . this,
must estimate mean variance P (~y jx~). locally weightedDregression,

E
2
2
explicit: mean y^(~x) variance yjx~ . estimate ~y^ also explicit.
Defining h~ weight assigned x~ kernel compute expectations
exactly closed form. LOESS model, learner's expected new variance


E

~y2jx "X 2 ~2 (x , ~x)2 X 2 (xi , ~x )2 ~ 2 (~x , ~x )2 !#
2
h + h + ~ 2
hi ~ 2 + h ~ 2
: (11)
~y^ =
(n + ~h)2
x
x
x

P
P
P
P
Note that, since h2i (xi , x )2 = h2i x2i + 2x h2i , 2x h2i xi ,Pthe new expectation
P
Equation 11 may eciently computed caching values h2i x2i h2i xi .


E

obviates need recompute entire sum new candidate point.
component expectations Equation 11 computed follows:

E


2
2
2
~
2

E

E
E

~
n
h

+
(^

(~
x
)
,

)

n
yjx~
~y2 = y~ +
~y2jx = ~y2 , ~xy2 ;
;
2
~
n+h
(n + h)
x
~
~
~x = nx +~hx~ ; h~xy = nxy~ + nh(~x , x)(^y~(~x2 ) , ) ;
n+h
n+h
(n + h)
2 ~ 2 2 x , x )2
2

E
2
~
2 = h~ i2 + n h yjx~ (~
~x2 = nx~ + nh(~x ,~x2) ;
~xy
:
xy
n+h
(n + h)
(n + h~ )4
mixture Gaussians, use expectation Equation 11 guide
active learning.

5. Experimental Results

experimental testbed, used \Arm2D" problem described Cohn (1994).
task learn kinematics toy 2-degree-of-freedom robot arm (see Figure 4).
inputs joint angles (1 ; 2), outputs Cartesian coordinates
tip (X1; X2). One implicit assumptions models described
noise Gaussian output dimensions. test robustness algorithm
assumption, ran experiments using noise, using additive Gaussian noise outputs,
using additive Gaussian noise inputs. results comparable;
report results using additive Gaussian noise inputs. Gaussian input noise
corresponds case arm effectors joint angle sensors noisy, results
non-Gaussian errors learner's outputs. input distribution P (x) assumed
uniform.
compared performance variance-minimizing criterion comparing
learning curves learner using criterion one learning random
138

fiActive Learning Statistical Models

(x1 ,x2 )
2

1

Figure 4: arm kinematics problem. learner attempts predict tip position given
set joint angles (1 ; 2).
samples. learning curves plot mean squared error variance learner
training set size increases. curves created starting initial sample,
measuring learner's mean squared error estimated variance set \reference"
points (independent training set), selecting adding new example training
set, retraining learner augmented set, repeating.
step, variance-minimizing learner chose set 64 unlabeled reference
points drawn input distribution
P (x). selected query x~ = (1 ; 2)

E
2
estimated would minimize ~yjx reference set. experiments reported here,
best x~ selected another set 64 \candidate" points drawn random
iteration.2

5.1 Experiments Mixtures Gaussians

mixtures Gaussians model, three design parameters must
considered | number Gaussians, initial placement, number iterations EM algorithm. set parameters optimizing learner
using random examples, used settings learner using varianceminimization criterion. Parameters set follows: Models fewer Gaussians
obvious advantage requiring less storage space computation. Intuitively, small
model also advantage avoiding overfitting, thought occur
systems extraneous parameters. Empirically, increased number Gaussians,
generalization improved monotonically diminishing returns (for fixed training set size
number EM iterations). test error larger models generally matched
smaller models small training sets (where overfitting would concern),
continued decrease large training sets smaller networks \bottomed out."
therefore preferred larger mixtures, report results mixtures 60
Gaussians. selected initial placement Gaussians randomly, chosen uniformly
smallest hypercube containing current training examples. arbitrarily chose



ff

2. described earlier, could also selected queries hillclimbing @ ~y2 x =@ x~ ; low
dimensional problem computationally ecient consider random candidate set.
j

139

fiCohn, Ghahramani & Jordan

identity matrix initial covariance matrix. learner surprisingly sensitive
number EM iterations. examined range 5 40 iterations EM algorithm
per step. Small numbers iterations (5-10) appear insucent allow convergence
large training sets, large numbers iterations (30-40) degraded performance small
training sets. ideal training regime would employ form regularization, would
examine degree change iterations detect convergence; experiments,
however, settled fixed regime 20 iterations per step.

1
random
variance

1

random
variance

0.3
0.3
0.1

0.1
MSE

Var

0.03

0.03

0.01
0.01
0.003
0.003
0.001
50 100 150 200 250 300 350 400 450 500

50 100 150 200 250 300 350 400 450 500

Figure 5: Variance MSE learning curves mixture 60 Gaussians trained
Arm2D domain. Dotted lines denote standard error average 10 runs,
started one initial random example.
Figure 5 plots variance MSE learning curves mixture 60 Gaussians
trained Arm2D domain 1% input noise added. estimated model variance
using variance-minimizing criterion significantly better learner selecting data random. mean squared error, however, exhibits even greater improvement,
error consistently 1=3 randomly sampling learner.

5.2 Experiments LOESS Regression
LOESS, design parameters size shape kernel. described
earlier, arbitrarily chose work Gaussian kernel; used variance-based
method automatically selecting kernel size.
case LOESS, variance MSE learner using varianceminimizing criterion significantly lower learner selecting data randomly.
worth noting Arm2D domain, form locally weighted regression also
significantly outperforms mixture Gaussians neural networks discussed
Cohn (1994).
140

fiActive Learning Statistical Models

10

0.001
random
variance

random
variance

1
0.0004
0.1
MSE

Var
0.0002

0.01
0.0001
0.001
5e-05
0.0001

50 100 150 200 250 300 350 400 450 500
training set size

50 100 150 200 250 300 350 400 450 500
training set size

Figure 6: Variance MSE learning curves LOESS model trained Arm2D domain. Dotted lines denote standard error average 60 runs, started
single initial random example.

5.3 Computation Time

One obvious concern criterion described computational cost. situations obtaining new examples may take days cost thousands dollars,
clearly wise expend computation ensure examples useful possible.
situations, however, new data may relatively inexpensive, computational
cost finding optimal examples must considered.
Table 1 summarizes computation times two learning algorithms discussed
paper.3 Note that, mixture Gaussians, training time depends linearly
number examples, prediction time independent. Conversely, locally
weighted regression, \training time" per se, cost additional examples
accrues predictions made using training set.
training time incurred mixture Gaussians may make infeasible
selecting optimal action learning actions realtime control, certainly fast enough
used many applications. Optimized, parallel implementations also enhance
utility.4 Locally weighted regression certainly fast enough many control applications,
may made faster still optimized, parallel implementations. worth noting
3. times reported \per reference point" \per candidate per reference point"; overall time must
computed number candidates reference points examined. case LOESS
model, example, 100 training points, 64 reference points 64 candidate points, time
required select action would (58 + 0:16 100) 4096seconds, 0.3 seconds.
4. worth mentioning approximately half training time mixture Gaussians spent
computing correction factor Equation 8. Without correction, learner still computes P (yjx),
modeling training set distribution rather reference distribution.
found however, problems examined, performance \uncorrected" learners
differ appreciably \corrected" learners.

141

fiCohn, Ghahramani & Jordan

Training
Evaluating Reference Evaluating Candidates
Mixture 3:9 + 0:05m sec 15000 sec
1300 sec
LOESS 92 + 9:7m sec
58 + 0:16m sec
Table 1: Computation times Sparc 10 function training set size m. Mixture
model 60 Gaussians trained 20 iterations. Reference times per reference
point; candidate times per candidate point per reference point.
that, since prediction speed learners depends training set size, optimal
data selection doubly important, creates parsimonious training set allows
faster predictions future points.

6. Discussion
Mixtures Gaussians locally weighted regression two statistical models offer
elegant representations ecient learning algorithms. paper shown
also offer opportunity perform active learning ecient statistically
correct manner. criteria derived computed cheaply and, problems
tested, demonstrate good predictive power. industrial settings, gathering single
data point may take days cost thousands dollars, techniques described
potential enormous savings.
paper, considered function approximation problems. Problems
requiring classification could handled analogously appropriate models.
learning classification mixture model, one would select examples maximize
discriminability Gaussians; locally weighted regression, one would use logistic
regression instead linear one considered (Weisberg, 1985).
future work proceed several directions. important active bias
minimization. noted Section 2, learner's error composed bias
variance. variance-minimizing strategy examined ignores bias component,
lead significant errors learner's bias non-negligible. Work
progress examines effective ways measuring optimally eliminating bias (Cohn, 1995);
future work examine jointly minimize bias variance produce
criterion truly minimizes learner's expected error.
Another direction future research derivation variance- (and bias-) minimizing techniques statistical learning models. particular interest class
models known \belief networks" \Bayesian networks" (Pearl, 1988; Heckerman,
Geiger, & Chickering, 1994). models advantage allowing inclusion
domain knowledge prior constraints still adhering statistically sound framework. Current research belief networks focuses algorithms ecient inference
learning; would important step derive proper criteria learning actively
models.
142

fiActive Learning Statistical Models

Appendix A. Notation
X

x

y^
xi
yi

x~
y~
y2^
D~y2^ E
~y2^
P (x)

General
input space
output space
arbitrary point input space
true output value corresponding input x
predicted output value corresponding input x
\input" part example
\output" part example
number examples training set
specified input query
(possibly yet known) output query x~
estimated variance y^
new variance y^, example (~x; y~) added
expected value ~y2^
(known) natural distribution x

w
w^
fw^ ()
S2

Neural Network
weight vector neural network
estimated \best" w given training set
function computed neural network given w^
average estimated noise data, used estimate y2

N
gi
ni
x;i
y;i
2
x;i
2
y;i
xy;i
y2jx;i
P (x; ji)
P (xji)
hi
~hi

Mixture Gaussians
total number Gaussians
Gaussian number
total point weighting attributed Gaussian
estimated x mean Gaussian
estimated mean Gaussian
estimated x variance Gaussian
estimated variance Gaussian
estimated xy covariance Gaussian
estimated variance Gaussian i, given x
joint distribution input-output pair given Gaussian
distribution x given Gaussian
weight given point attributed Gaussian
weight new point (~x; y~) attributed Gaussian

k
hi
n
x

~h

Locally Weighted Regression
kernel smoothing parameter
weight given example kernel centered x
sum weights given points kernel
mean inputs, weighted kernel centered x
mean outputs, weighted kernel centered x
weight new point (~x; y~) given kernel centered x
143

fiCohn, Ghahramani & Jordan

Acknowledgements
David Cohn's current address is: Harlequin, Inc., One Cambridge Center, Cambridge,
02142 USA. Zoubin Ghahramani's current address is: Department Computer Science,
University Toronto, Toronto, Ontario M5S 1A4 CANADA. work funded
NSF grant CDA-9309300, McDonnell-Pew Foundation, ATR Human Information Processing Laboratories Siemens Corporate Research. deeply indebted Michael
Titterington Jim Kay, whose careful attention continued kind help allowed us
make several corrections earlier version paper.

References

Angluin, D. (1988). Queries concept learning. Machine Learning, 2, 319{342.
Baum, E., & Lang, K. (1991). Neural network algorithms learn polynomial time
examples queries. IEEE Trans. Neural Networks, 2.
Box, G., & Draper, N. (1987). Empirical model-building response surfaces. Wiley.
Cheeseman, P., Self, M., Kelly, J., Taylor, W., Freeman, D., & Stutz, J. (1988). Bayesian
classification. AAAI 88, 7th National Conference Artificial Intelligence,
pp. 607{611. AAAI Press.
Cleveland, W., Devlin, S., & Grosse, E. (1988). Regression local fitting. Journal
Econometrics, 37, 87{114.
Cohn, D. (1994). Neural network exploration using optimal experiment design. Cowan,
J., Tesauro, G., & Alspector, J. (Eds.), Advances Neural Information Processing
Systems 6. Morgan Kaufmann. Expanded version available MIT AI Lab memo
1491 anonymous ftp publications.ai.mit.edu.
Cohn, D. (1995). Minimizing statistical bias queries. AI Lab memo AIM1552, Massachusetts Institute Technology. Available anonymous ftp
publications.ai.mit.edu.
Cohn, D., Atlas, L., & Ladner, R. (1990). Training connectionist networks queries
selective sampling. Touretzky, D. (Ed.), Advances Neural Information Processing
Systems 2. Morgan Kaufmann.
Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization active learning.
Machine Learning, 5 (2), 201{221.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete data
via EM algorithm. J. Royal Statistical Society Series B, 39, 1{38.
Fedorov, V. (1972). Theory Optimal Experiments. Academic Press.
Fe'ldbaum, A. A. (1965). Optimal control systems. Academic Press, New York, NY.
144

fiActive Learning Statistical Models

Geman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks bias/variance
dilemma. Neural Computation, 4, 1{58.
Ghahramani, Z., & Jordan, M. (1994). Supervised learning incomplete data via
EM approach. Cowan, J., Tesauro, G., & Alspector, J. (Eds.), Advances Neural
Information Processing Systems 6. Morgan Kaufmann.
Heckerman, D., Geiger, D., & Chickering, D. (1994). Learning Bayesian networks:
combination knowledge statistical data. Tech report MSR-TR-94-09, Microsoft.
Linden, A., & Weber, F. (1993). Implementing inner drive competence ection.
Roitblat, H. (Ed.), Proceedings 2nd International Conference Simulation
Adaptive Behavior. MIT Press, Cambridge, MA.
MacKay, D. J. (1992). Information-based objective functions active data selection.
Neural Computation, 4 (4), 590{604.
Nowlan, S. (1991). Soft competitive adaptation: Neural network learning algorithms based
fitting statistical mixtures. Tech report CS-91-126, Carnegie Mellon University.
Paass, G., & Kindermann, J. (1995). Bayesian query construction neural network models.
Tesauro, G., Touretzky, D., & Leen, T. (Eds.), Advances Neural Information
Processing Systems 7. MIT Press.
Pearl, J. (1988). Probablistic Reasoning Intelligent Systems. Morgan Kaufmann.
Plutowski, M., & White, H. (1993). Selecting concise training sets clean data. IEEE
Transactions Neural Networks, 4, 305{318.
Schaal, S., & Atkeson, C. (1994). Robot juggling: implementation memory-based
learning. Control Systems, 14, 57{71.
Schmidhuber, J., & Storck, J. (1993). Reinforcement driven information acquisition nondeterministic environments. Tech report, Fakultat fur Informatik, Technische Universitat Munchen.
Specht, D. (1991). general regression neural network. IEEE Trans. Neural Networks,
2 (6), 568{576.
Thrun, S., & Moller, K. (1992). Active exploration dynamic environments. Moody,
J., Hanson, S., & Lippmann, R. (Eds.), Advances Neural Information Processing
Systems 4. Morgan Kaufmann.
Titterington, D., Smith, A., & Makov, U. (1985). Statistical Analysis Finite Mixture
Distributions. Wiley.
Weisberg, S. (1985). Applied Linear Regression. Wiley.
Whitehead, S. (1991). study cooperative mechanisms faster reinforcement learning.
Technical report CS-365, University Rochester, Rochester, NY.
145

fi
ff fi


"!$#&%'(()*,+#'.-/+0)+

1234$56'0'87(92:;3
!5$97()

<>=6?A@BDCD=FEG=$HI?AJKL$?FBMKNOEG@D?PEGQRCSHI=6?TBU=6@BDVXWY=&Z$K?AJK0L$?TBMKNOEG@
NP[\VX=$BU@]NP[\^_J?P`G@U=6@

acbMdUeUfg>hjilkTmnkoiepbrqMsptuf2vwbMieUx

yz|{,}~Iy| { {,|p }|

IeUdriSUo

{ X|y| { {,|p }|

$jM,o2o6$
A. 2 cXI$|2& 0&8u
$"$|&w IX6\$2Ac, 2

6 U
$ j> X" w|U8G >oIXw0 8u2,
0/u2$ G,Xj$ jA "G 2U"U Gp"
I0 8u",$6, 2 XG,uTopA ,
$ $$ "pXo,u& G$YuAo,u&Io jo,u&
G6w0/u 0$$ $PuIG |0T /u 0I" 6c
X0uG0&p P,XT 0 0XpuT,8uu,,
u$o c &$M0/u 0$X2j,TX w2I>$,0
G ,uP TGT p06uI c$
,jw GU j juj XA0 8u2
, jG >A0n jAou Xu w,"8u
, co AuG,$,,U 08uA co,I0 cp
$ YAu$ j w,c6 , $0/uTG,u,$wM
T$$cPA w,G, ,$0 T&, r$
cAFP0G,0T ,wI Gu j " PIT0/u2 $
uuI T,/u 0

6
ff
fiff "!$#%&#(')*)+, .-/fi01!$23 4$'"457689*;:< "!$#%&#(')*)+, =' ?>@';BA, "C'#
, EFA7';5#G 4H(@, @9,'4F';9IFA&#%4J:ff#G&)K C L4((4J;:NMO4( ( @' P &';QR7')?D
M9 4CS?T= U, ( 9 V@, :WC##G@FA&#%VQ4FA&9 @,)M9 V@';9 9;:$FAMO4( ( X' @ & +;:YFA ;D
';( URZ')M9 4CS\[&#*, 4(F' ]^4F7M7M74`_Ya'#GQC cb-ed2]Yb-/fCg-/d2(2]Yb-/f # -/d2(2]Yb-/f ) -/d2(2
';4hMO4( ( R7')M9 4i' jb-/f&-ed2(2k5b-/f + -ed2(2k5b-/f 9 -ed2(2I';4h &';( *RZ')*M94S 'ml AC nFA+4(
oqpsrtb-/d2k;-/b-/f g -/uv2(2?w b-/u2(2Bx4'y4(9,( & z ?{)*M9 4H';9 9|MO4( ( Q' E ` &'}( XRD
')M9 4CS?~"hFA7';IFA 44(IC' a674C `'}4='*74(C#G,M( & a}:|FA?C U{ (C#%4C]9 C'#( a:ff#G&)
FA4(IR7')M9 4CS l A4$, ( & J}:9,'45';9FA&#G 4Y 4|'N:W#()\;:19 C'#( , +:ff#G&)R7')M9 4CS[&#
'J)+&#GR(C 4( +, 5#G& Qfi01!]_|h#%5:WC#^(@-/1'Z#' X C#G45/]1C;7>@9 ( Q
h"';7]1C;2S
C'#( , +:ff#G&)RZ')M9 4=)+C' 4)+7:/V{ H'?FA&#GVJ(67#G, )+#G, Q';&#%' _" FA
FA RZ')*M94S l A_Y")';, N&M7C#('}( & 4, ?fiff1!U:/&#)+78C';( & ?;:'YFA&#GV'#GCvCG ,/
' *ff7F B/vS1C C#('}9 '; h4F#GC FAC 4'"FA#GV5A7';1 4&"_YC'v]_A 9Y4FMO,';9 '}( &
_YC';C 4'.FA&#GVJFA7';" 4I(4(F#G l A4(M7C#('; 4& 9 VQ)';4(C 4(?_" FA, `'.&CvCG /
GFS l A4I 4='*#G9,';( & J4(F'}(, _AC H4&)+9,'4(?4)+&#%C C#';91FA7' J4&)+FAC#9{'4(S
50ff$G5,CCffG55/G$55,/0tG5,B(CffG5F/G$5B5,B//0/5


'((0)pfi|fi75 5 T0 U 2ff4$ 3
!fffi

!25;

fiY,77<($7vXE7
^F7GX.(,7GFCC(; G@&G7C%h(j,ff1\G*F7F7( &/; (HC}
F7F77 C}1, C}( &aQ, C;( &aG,;( .7}B&G&7Q;" 7H0
F?F7F7( &n&G7CvYhFHF7;I,(+ h+&G?CC(;F7U+&7 t; C(
"+&GIF7F7N{*C;(IF7F7.Uff*FI{* C;H&G7C \ .&G"CC;
F7H C; U, +Q {7} +&%CC(;157Q G,;( ?(X7;G&7
t^7 q L +(" ,(F7

fiff C; Q, iU
FF7GG7CGC3F7F7( & hFJ+(F;F X0j7G( C,F7F7
X7 ICGC; C;, C;( & HJ7 =C

"(E,(";
(F FE
@G,7;;"F@; F 0aF7(1%{}( +{* C;@ i7G7C
F7y, C}( & OFQGi77 7=7%& 7G7G &Y, C;( &Q(JF;
& H7CG t}( & %&
h,(X;7COCGC;*7G& 7G7G & !#"$%'&!", C;( &
F& UBH; 17CG t}( & G&

fiff
( F,Q.CC(; GH&G7C75CGhGGY77G&;"(*CC(; # )C;( &U&Y57,; # )C;( &
"+<%(h77G&}BUCC(}# )+&IFO,; * )+
%',.-%'&!%'-/0",C1 (@7Q?7 (C(h5?,
X75; ,UF =77C`.CG Q+CH &^& (C(! 2F;|^77G;ByC
7JF(}m7;a(4
3Y7 7 2 56^&C 8
7:9&!5`ei";, C+C( &N< ;Z
>
=77,G@ ?@C,yFhF757( &HG7CY,HF &( AB,CD1!,C$E9:F@"$9HGBI ";YC
J,( B , FCI,(X m57F7+XUJ v&I,(F3F;+L
K
N Ov(Q
PRM OIjS
KTM N OQ PUM Ov+"CU 7&+IF7F7+XU17hS
VKqU
"F7F7a hYC;C^F7U, C;( &1 W 7GFCIa 5YC( F ;
F7; F( |QY7IF7F7+ 7 t; CCCF&+FXG" C; HZ ;; CC
^H(&c77G&;CC(; # )H&+57,; # )8
G<$XG+ ,Cc^J X5Q77G&;
YQ" 7a&C(" F{LF Q7OCY
^CGaFQ&CJ Z
#:"[G<$\0!,C!"*%^]<"$%'9,_J

,7GFC"(" 5B
#:"[G<$7CC(; # )C;( &I; ;"YY(hCC; * )C( &( 7,
;C CC; * )C}( &C;(ICC(; # )C;( &? ((I ,(YCG<%(7 (C(jQ F,
567 5@7` 5aC7B
"7G;UF7;J< (+(c b ,(h7}=* C;(CC(}# )C;
77C57F7( &/` de=7<^? iJ,(J BnF7F7++; {(h,f
bPa Bn
F7F7.mj; $FC,(+F7}i; (UF7F7+H; Y{(+{f
b"HX< ;7 +C
7*CC(; # )mQ5,UF,` dJ=h g &7G|EIF;+F` de=a "'7
<;7 C"IEY& L& aFGE&( ((, & E&U{(Pff(C;1"YQ+ &
7 & 7HFJ7( ( *< ;Z*+,(QF7(C@F;J(C7(;(` de=@ C;aF7(CJ"7;
"Y&F;,Q?FGXF}{,H+&G57J&,(
v&5Y(77%&;&57F7( &* ;&;,J XF}( }(&Gi < ;7 b
&( ( F,(k
jfiKlM N m&I PnM m"` KoM N p(I PnM p7FCJF+` dJ=J
ba fi
N q(I PnM Ov"Y,(r N Ov(I PnM Ov; +(C+Y+&GY77G&7G,;}$IC}(
CC(}# )C;@ b"C7h7 &7@@ F, 2 i77%&;17C(J 7&+F7F7.
j E WI|5Y< ;7 h; (+F;"CFIF7F7( &JG7C I7G( C,G H7F;( ;(&%iIC
Y&( 7Ce :7/sHGt%X&Y,(! $,(I HCU7hG(Q" FyFC+( C
uwvCxy{z}|~zii|~z~:X:zX!{X[fi[Xk@:X<h0.<<}}[y!yJhh@zi|z}[rH{y{zQh<zX1v
!vC0z:`:z~[z}|~h:X:{fiH|~zEHz}cHhzQzE<[X::z}{z}|~h:XH|`h[[z@:X<h<<}}
{:zH8[ @:z
:<}!z:I':!1::E<:}``h!z1:<}Xy[[y+{:k@:X<h
<Ht}![zzz}@XyI :z~[z}|~!`|~zw{!v
!vCxy{z}|~z`H|~z}XHczXzz~z:`:z}{z}|~h:Xe[z}|X[X{I{XHJH{1*[HiX:!#<E {:Hz~
s0:tu:}v
<{

fiCCHf@@@X<[@'@C@@Z@[@0H8ss@'w[@'@C
<sHs4Bss< [J@s@s#0c# #@<s#8H
[+s4~0s

@s@s#0 0@B1s0c@[B}ss*#[H*ss@<EQ<<0@s#.**Q
@s
sf0scrs<seE#ha## <s#104s
<s#s<k[Jh1s#@J0Q
s##[#0
[1B<#s~<s s<H#0frs<s
s# <
[Q<s<>@ *
s* [H*s
@s@e
s#*#0 e@[Jss*f@<Z@H*8sH<` .0cs:s< @<H ~Q1s#
@
:s@*<E#[HEs[# [#0sfi@s@
r[fiJ@@[s
[#0
8s#<0s>E
<H*s[
<s<E <sQ#B< ~@[<k #
#B@s@1<
<srQs*<k
s##<r#iY<s T#r< ~@[#0[
*#[#*k<@s*[[#Qs T#fi< ~[@<`ir 8s#<01s E[
[H `B
<s< rs c@J< ~[@#0s rs .s4+`J [ rs Q#J[#
` fi

s#c@H*H*<*8sH0@s<H*s <0s<ss se<0#Hs<` Hh

:s[ <sH<firs#<@!r< ~:[@[H*s [B<< 8s#<0sE[Q[#Js[
@[BsJ<s*H<s<Qrs#t+@
H< ~[:@[#0 s<J:sJ<0s<s < ~[@H[#0
@s#c0@B@s<#0



)
* ,+-
9
?:?@ A:CB
ff
fi
: @ I: B KJ



fiff
fi











"

$
!

#
&
%



'
(
fi.0/1 2$.3#%&465#%&/87
"ff
9
"ff
;: <:
ff
>)
1,+-
D9=@ E9FB
G9H@ E9FB
"ff
ML
,)
ff

L
ff




=9


N1[fis1s s[##0@w`<[#1
s#*#0i*e<4101@<s*@+s
sk[FO s[64
P s*#[H*sEQB<[#
s##[#0*#[
sJH@[<00@s ss{*<@
H[<<0@rs#t 8a>s<fk0
*
s:~s@0a#<sZ<[#0se[s
@0
[@s##[#0F. 0s#[H0#[cs* [H*ss@s@
s#*#0 <#

@[<s0@sss{*<@[#
@<HJ[H#0


&R HS-5/$#%&/3.3%K.TU/$!# WV0#%&'(
X
Tff


@0Ssk#[1s* [H*sfis#
[#4#1C@[

c
.
J[<ca<<[# #0s>@!
a<Z[<<0@< 1s<4#<
[#04 ~`o@ *1s
[# [#0s`@sc:sJ<0s<sc[0<a<<[# #0 s<s<#<J@
s:~s X<es
@<@ss [fis<<#0 <









[Z

ff

}Zs#@a<B+#
[H*+[:1[Js4<0#Hs< ss ~<0#s<4[
#[csH[# #0ss40[<:@<<[# [#0sE@s##< H[<+[fis<1@<Js*#
0@fic@~0 @#s0s#ss4@<B< s<e@<e0@a1[#k@*Hs0s#8@<B<
s0@[<c[fis[i<s<Js81s<H#<<4s0@[<Q[ 0+<s< sss041
<ts ~`f<:#< [HH*sc 0e<s<![s[s<s<fi[#Q0<1< sH<
*
@
! @@sC
c [<@ LBsss0s
@<: ~Qs#
@
w 40<[<E
Z<ts1rs @ ss s[<s< [<@*#[##[
s<0[H*
#*H[#1@@c8:s@s@4[rk<s @[c#is<8s
<s<e[s10
EQs
*
f[s sf[<
# ##[ s<H
[##<
<s< #[s`
<1fis <s<
+<c*#[#*f<@s*[[#1 s[fi<sH<
Csfis:s<@sc@0H <s
i#<s#[[#QJ:sJs0
<sH
< Csis<B [0sIB<0sH*@BsEss*s0@[<E[ 01<sH<
@sr[#k@ [H#0kH
s0@[<[ s[<s<



\

> ]^ _ ]` ba
> E)
gih j @k1lllmk j_n
g

c]

rF&sx{?y v &sx{?zF&sx _



YL
> c` ad Q

Fe

GL

fJ

pj
rFtsu hwv &sx3k8y_zF&sx

`a

EJ

?J

fiff

ffL
<L

qb5'mS-5#

JsE<01s@#0c[@@<Is[##rs@seB@}|` as#Bs0@[<`<s<
#<re[C# @}| B0@< s0@[<pC 0I[<k[s<H@Br 1rs<s*
s* [H*ss= `h8Q
s80[<Q@<<[# [#0sI eh8Q [#sc<s*H?~8
:@<<s#
[[@+0s[s< sJs8[# <0@s1s1
sBc[fi0@Q[rC0Qs1[J[
<#EB
##@
:s><:s*Hs@c@< #0[esss<I} 8:s
##{s
s#1s#0s <J
@H*H*sB<@s"cC1sr
s<0[H*JsB<
3

fi>``6m
`ux q`1
K6H363
6,b363
` }-`
;
;

``6$`F6 -K




TF6 , -q?

-Gt`63 -&3
3 F6 , -q?;



<6H__6 63\,M;Y6b
`p,Yfi3-$`6 -6
?$686 YK8`3$& ;_G6fi&-\,>K`,>8`,_}M
K?6 ?m3
,<36m3K3-8,66
,>-6?6-86m -6`63 -&t3?36*t`
-i86 F6-86m -6C`63 -&3 36mMF1A3686 8I6 f6`*
,<86_3631`636`6f363>Y6 3H3-8,\`63 -68`8686C3- C?3-F`6$6
;fi,<K_K6 K386 >>-86K66D`6f`,3H-\$6D``68`6 -fK63-6
36m6
&b3`uC,K, 8&_`u `-66K66`\E}<36Y6 3A ,K?6? 3m8 3
,m -6mF3C,
-6`6A6$633?,
3636I6I$6*86Y`6A6AH\6`}`
, -,_-\\`-K366366IF6 -Y6 3II`fi&-`6IX&&`3$fiCU6
`u3DHF\3`,m3- ,86 Y386 K K86C36m63*6f36F`686 fM
;fib,C6 Km3_,`t`63m6&t3363_Ct<,_>>-6,8fi-`6,u;fi3$6
6f86 ?` 63- f63386 1
6 3D1`b`13
8`,C _
F6 Km?86,8`<86Y_}$6G66`,
K38 33*u63 m3
,;3-6$_mU\`>63?, `6Hm`38,m66G6m
_3f3&-H86 f833,,mE,-`H6m,386 C A`f8`I6`-`,I3
``6 863;K6 , u>>&-`6f?`33K`6,;,;86
,mU`686 >
6G f*m6
8`86?`6,<,;86?\-C6386 K$`,
>C YH86 K`3
6 31K `\8`,86
`6,< >F6 *F1F`?u6?m*m-\K3-6 -f-6>86?3, -f`3>3f`6f\6`3
6H ,bF6 -C63>`$63fH
,
-\6`3 `6;6K`6f\6`3$3
EF `3`f86 \,?86F6X,``6m33 - `6\-6F\ -86`$`,?86
`33 u6 DF6 , -&-t`63m6&t3363fF3`3 F6 3I86[3- 63,
X;m_G6\,3`33 u6 q 6Ff$ 3 F366`m6-86H3- 63[,Y[
6 ,m6bu-3U6C Hf`33 u6 XY6386H-6t`63m6&t336F6 3
6$6C`63 -&t3f36= 3, Hf`63 -&3=`0--`6q663`f_3\
6 ,m663 FH`63 -&3=`0--`6E66,K 3`F`A6, 16?3- 1<,?>
K <$6 633 -I
`;386 K`63G6 C86K6m,u`63 -,;86C3- 63C,<;fi``6>`63
`,6`I[`mmA6>D-\f3,\,Y3\86I`3m63f,K-6I6-86m -6
`63 -&t3*36m*6m,,`f-`33Y$636m63F6I3-`686 [,G
;fi;,;K6FY`,_8636m3G3-8,6 36,86K`6\663Y,xt`63m6
3F363fi ;$6fi,FF1AF,>-,`3 ,_ -6}63 &b<` 6 E
F6 \8, -6\,K M-m\86F6-`,F *3`63mI`\`63 -&3-t`63m6

fiff ff!" #$ %&' #&$ $( &)+* , - ff.' /$0,fi12#
3 45/$+$6 /$2#*7098:,28; #$6<=8;,fi# 0ff ##>5$6&"6 -6&1@?&$6
,fi) 0,AA$6%ff%ff'/$A0BCED6) 0,GFIH"J
KL=MONP NQR9STKLUP NQ&NVMWR )XFYEJ KL=MONVP NQR9STKL=Q&NVMONPZR[L $6\#]Z 71?ff^ /) ?$#)_8;,fi71a`=) .$ /1<
ff12b/ $ R cF H 2 d,fi#ff*/0$A8eFYA &)fFYA )fFY22 d,fi#ff*/0$A8gF H )9F H h(0 #"F H )fFY
,fi(ff/B . ff%ffi!\#b/* ff0$#5j 512# &$6 /$'FAHk( lnm'`k8n$6.$(o7FIH N F Yp h(.*/0,>$6&'lqm'rd8n$6#
$U2 0ff &# KL=MONVP NfiQRkSsKLUtuNv Nfiw(R > + 0ff# ,ff!+ +*70,<UB/00,fi ff%x. /$6iC
yZzz

fi{|W}~A|Wn|Wn}n6Z}&nUn~G}nnAn|W}&n|C~XWW|OO=}nU}&nUn~

n[nW;EW#E[&WZfi#CZ=WOZEWOnOf=7OXn&ZWCCnOOO&A;ZnWXWZZnO;qOZ
7OZ&dAWkUu2COWZ=#uVdWuWZ_XZ;.OO&"0(fiC[7OZ&5(U+O=O=W
Wc#CufiWCOZ WWWZ\56 nk
9Z #W7uWCfuO55VO#=d#OnZ
;ZW@a@[nW2uWW+&;OfCO;
finOZ;Cufi#Z9Z:uOZ
C#^7nW
COn&"&A7O9fie
O[27nWZWZW+n#WuW';A#Z7iZZ+finOZ;Cufi#ZZ=WOZ 'nOZ;Cufi#Z+Z=WOZ
W#i7ufZ;uACA[uAWnO;i;CO+ZCOZO=O9&7Wn&Z
nZZCO9ZCu7#=nO;C^7O&A@7n@+ZAXOZZOWn Wd+;&2WOi;&iOf#Z=&iu
C[+n&uC#OnOOOIiZnu+OWAZu9"OEWO
sW#W7^finOZ;Cufi#ZW
O:niWk2+ZCu7#=nOcZCO;Z[n;O;C_&'ZC;OZWO
OCufiZC;OZeC&Z
7ZZ:i;&iOO=_&&@7O\;C#n#Z_:uOCn&WZA0
nW#;O=WZk27O&s7n"Wu9Oi
Z9&+Z:uOZEn&XC&Z/nZZ=&;;&;CnOn=O;;&;C C[O:O=O7O;9A;77O
ZC#ui=Wf&'CnAZ7O;"Cgn;2W;i&AI7n&"\finOZ;Cufi#ZZ=WO/&=WOCn&u;";W67
5ufi0sfiWfin
fi7O;+ZZ;C+Ii;'n/O[Cc+&e/OZCOZOd2OZZ2kC27O\n/OiiOO&2i[OnZ=
7uOW;WW='7nO;7O;C 5ZW='2f#/\E/WOW#2Cu7OZ nWOaZWA
;&CI

6fi ff;U #W;W&C5XkfiOWfi ff;U #W;[7OOZC&;C &"WXC
; ZA; OiZA&'i;&;dOiZ_;"7&ZE&A/OnO;W#7i;_qnWuZEn; nOZ;C
&@7Ou[i;&;
W 26
# ; d;^Z=WOIi/EOOcW;;WfuOZC"cC#
OZC&iu;;iWO
W 26+ W;[9Z:uOfIi/OCOnWiiu;i&;
7; Gi
Z;7O9n/O;n#WOWaZ=WOO+9n/OifWO&6!

;9Z=WOW(2O"
$#nOW

7Onu;;W[i;&;=%
fWO
"&fnOW/OOZC&iu;;i+=
9 O[O#Z=WOW
"O;ZE#n#ZWd[ZCu7&n;Z;Cn;dnOWZ
W
'"
(*)+-,/.102.43/,65 Z87 nW
&=nnWZ&A7O\#fiC#nd;WW;W
7O; u'iuWOZO:9
;=< 7 i+/OZ"&i'Z=WOZ"O;ZEu[ZCO7OZZEfi#C 7O[n[nW;+=7f O*: /
;WWC C> ;?< 7 id7O\ZA&@&;AI
@ C9Z=WOZ""O;Z_Wn[ZCO/OZZ9fi#C 7O\nW;
=B9
7
'

fi7O;EnWZA2*
nZ7nnnWC[XWO;7u#&=nnWnZB79"WOZCOin9/OZ=WO/&
=WOCn&uC9WOD@IC_=WOCnW$> n&Z C_7O;:7f[X[A;;O&n/Of7nZ=O#=OW;
7CO[WO&;;#C#n#AO9Z:uOZ2E7nO7nO;C:^Oi;&iOXWOE#Z=&;W\=O;i;C
(*)+-,/.102.43/,GF
ZH WOI Z=WOZ+WOEs[ZI&eZ=WOZ'X\7 7n&! = ; =[=
I_nOWZ&CKJLI_A7O#ZO;[97nO;7O;CNM97OZ
7n&:$MOPIERQS WODI W#
= ; T[
=.U WV2O fiff W;kAJXa
WOYIZJ[9
fi\ ] OU/W6 <_^ `H ;6= afnnOWZ&Acb defO@ZW#[CnZg& i+&;f^[CnZg&A9f
0\ g C67uU <h^ fi! i6= !_
nnOWZE&aZb di_
Caj2"klb d6E
uOYI WYfi\ ] OU/W6 <_^ nVC fiff W; n
ob dia
WOIpb dG9
`H ;6= -IK# iu`ff + GWnOWZ&mZb d:qBE
uesrtj2"kCb di_
f WO*I u#WV2O `ff W;
# ;Wfiff [[
uob d:qYa
WOYIKb d:q*f
'
v?wyx{zR|h}2~u]Wi~fi}Hn:~uh2zRW~fizRWhfifi
u
~fizR_BzB1~fi}
a}Wmsna!h2~fi_W$h_
~
~fi}m=hH1_fifi
u
~fizR_/RW~fi=zsTvh=$Rm2{}zRux/WhR2a=v_=$mRwCf
aRm
}WfizRzRh-~ax/WhR2mR=_
u{f1zR~/Wf~fi}fz2~fizR~fizR_"~aWzRW/Wa}
_G1zR|n|Wh
`~fih|W!~fi}hl=z}
_Rw
=

fia1h_uycs[

fi p="CnW$%fa"]_* %1$]1]aas_*nBn
hg
Z:
%
n2$ Ym!a=2h!=au]/
mna="]=atn!1Wy=hg
n
a=/1$]1]/ %
n2$ Yt
18na=2_!==su]/mahg2
i"/%m"= i" "=W n2* : %y!
Y===S_
==y%C]g21"]]2
_]G=a=2[=1h=2
_1B=_1_]%Bnn"hg{
ff
fi
ffc-as=1_=$"B1]=$1="t{{n2%n=$n
] _B
!" #$"
* -(/
. +*,*=
* -(/0 21 43 & +*,*=
* -3-5 +**,2
* 6387 "as=1_=
% =:'& )( & +*,*=


]= 8_ny :9==N2:-n$n_hgW_]<; gCBt_22_
m=y2 $

_= > (/.@?n/3A52 n2 n$=_ _ '&/B (/.ffCD 21DB 3A5?_ ;$]:2
]g=NFE G"H ! IJLK ! M{
u'&
BN
1
m!8]gh2_
] (O. 3A5 :ng%_"8nC]]_2_
]P ! IJLK !RQ #SIy

fim'&!BN1:a_nyL9== 2-2*nU
8
1!W1 V
N1n=*n
a'&!
Xa_1"]]=]A*1Y=h]2
u'& 1 V g:
]_B2
]]=
1 V C_nyL 9== 2D
1Y=h]2H
AY
& B2
1gh_=


ff
fi
ff[Z\%{=$KY =_]; ^(
=1_8e;$]!2
]]= 2fgIas
hH_!n-si2""=1hP]!

$*$hg8]gh2_
f

& ?**,*_?"(O7F`

abdc Y!2n





=hf
]__=__1"n2"lW__]n_]kjl


ff
fi
fflnmo ! IJLK ! Mfp =1h=l &
B
=hm
fN21 2 n:]]_2_g=_]=Y

=_Y& %N
1 :n2#S ! MpgJL Q ! u
ms



1 g"%D=_g28
*
=_
! &
sW]]_2_
]:=y=Y1*n:=n==hg


ff
fi
ffqr%{=!S::_=
/=_="P8=1_/m ! K I$
/6 Lg!sgh
_t2=$
A=1h=Wuv& ?,***w? u'x sWT n
!2
=6u . ]:=]n2!1:!"=_g2

=1_=1 u & ?*,**_? u .zy & mfiun=B$2]

_]%=]_h2a:]_C|{~}a*

ff
fi
ff%{=i$$_=a
m=_=Bo$=1_O Cn"W2:=]_h !Q g I"

K: ]__2
C{ff:
*/!Z]"*n_]{ W2"=gh_l =_s n=n

|{ }


!Y= =n]2- =X|h!]2i!2X[6 @c, f22h
]:9==6H2__ff
!= 2
,Mr ! oE ! - ! 2
I@fgJL Q ! E !
@I Q ,
g JL Q !>8 fl
P
!^ '2 ! ! f
pIfN@I Q SMg ! NIfegJL Q ! B2/ Q g6
fp





ks

! ! !



!]g
!1$n"_]"H=2 ]=u==]_!=_
=h2L 92
_]8
1"]]2
_]*=a=2
=1_=*1N_2_$$
C=_1_][[nn"_]{S"!
$
=X|%{= @c,
gU
@ c 2
=Y
h!]2@ iH2YBW

R @ c
>

fiMMw+SSS^SzSMYSS|S^Sw< Sz^SzSM

P~,Mff<ffff|^~~,M+-,M_2v,,LG2d_2PL^W
n r|ffD
WLRRP^p@wS>6Y__L-@
S:Lff fiLfiLNfi RPfi ^_@L @
/"'

Sk fiL"@


G+|MO|ffG
GW/.0+1.



w'M"! L#s,:%$ 4 &(' R*)+,-)

2 ,~4365 fi 7 ) +| fffi8 & ffPDNS, ) & 5 fi 7 ) SDS'@_LfiL"
fiL__8ff4fiL%LL> % )"'
SRfiL@)"S *)+, :9; " <) * _D


"WLfi Lfi W=fiL__:>fi
ffM7 <)+,) 5 fi ?fi: : . +, .
@
'fi kS@A_B 7eC "7fiLLRkD7w" FE G _HffMS<^:L:^H
KJ ff ffMLON PWS__QRPSfiLR>Ffi LT_"ff~<" FE %P>fiLQR
P &
U RPVWHXXXYWP[Z7ff\^]nKff"NSGP_fiLW'` "'S"k">PbafffiL" Fc_DS"d
e
? K
_ :fi 2"'S"
"v__ Lfi " Fc _S fi
@
E
fMD$RLffS"Ww_gP &U ih4Wj4DS^S^k &l U ij4Y:m l U ij4WhpY S^S^qff
fi 7
U ij W
h7~
S^ SDqGQ _LL> r @'_w:ds LRk'"S^rfi8 & ff^"P
Lfi '"R^LL2"S BtS
^_"PS"C
ff D@fiL_ ) S2

SP
)
S'



P
f fi _"7 G
ff "u
n iwpY_m l xn ih7WyffN7fi=-S^
.
.
vl ijW U ijWF
S"zq/
6*
w "Cfi 7 fi 7
c P__ U ijWFn4{ iw7W
S"|7% fi RL

KJ ff ffM}ON ~,~_DS7fiL_6w_R wff;jVWXHXXWjZ<^LfiL_fi 7>Ffi LK
SF fi 7c fi v~As
ff h V WXHXXWh Z fi:wfi 7_"_FFSFfi 7cMfi v~ z~
jVeh"VWXHXXWjZhKZ4DLfi ^L:z LH Sz >~ ~ DQA~ fiLPR@ffffDS _
","S^Lfi N 5 ^
u :

_Y:fi
Lfi ~A
@
G
+< ,/ _Pp@RL
'ff
ffWv,n nr

_R,L'DM

wR

L

$P

2 ,~43
'7fiLp
5 SS_ fiLNv"K _LcL jdV
h"VWHXXHXW
j"ZphKZp_QR r _fffiL%LL^
@"



kfiLP" "S^"@%fiL
7v"S^ | +| _7 /"@
Lfi e7~
ff
7 -"S^
^ _ _P"S'fi 7 r
PD"*_"_h"VWHXXHXW
hKZ
S@% fi
ff '
"w w"_ S@fi
_^LHffS'"fi @ fi 7cDfi
^Lff7S
R
_@7 'Ch ^j ff7 DF,M\r
ffM7 +[ p@@
n
@
|ffD 67



fi%`7Y4O -6H

_DS4dH0 d[[ [:g0K4 4H [d[b Cb
[7/=pd_%=A7^
7p77D%76p77`7C*7===p77
pFC%
`===p7H
K;`otd<?7^v"<D7`F^eC=pp

R*_" ip:Fz_


ifi ffK
iR*_8_ip: F
^477*D7==4
*! "DK 8_p: e#
$%&z_
4/ D7==
p


ff
i) (* +*,
- "D_8_ip: F
^
47
7
D7==
/.04
'




R D"
6F14273^7
7=37 _;
H =-54!67ff )(Kfi8Kp R =-"

F 14273%
7YY9K^7,K;:[<=*
F=^76_%
H =-><ff iff@?A8*7 _1R<
F 14 273 47=3 7z
&*
F=>p4= 4!67R!ffB?=C"
F!?3ffB iR*p^


EGFH

; :[pe> 7`7 H Dz FH 9=7:F`=pJIgp_Si==!K%*
H
FD4!67R!ffB?=C 4`7 H KMLR7NOI07P%8;K
477RQ`7D7S FH 9=7
F`=pd FH ; :[pF%Mpz`7 H F` pF%TSp/
7Y FH 9=7*C%`
_7 ==! K_8V
UW<X7S
7> F<H 3 =7D`SY
5LRi `
U;7XZ.\[]N^7OI_`]P
7
UW<XT
SUWaX; bc 4
dFY3 T7d7==pC=Y7===p7S7-7F:
`===p7Sip
A` H YpeF^e
? F<H YW :[7F7=%>
7(7Y
K`>`; bFKHpH
K;`o4&d**C` H Hgf&"< F<H YW:[7F%pz_hji, "<7=

` H K7kG^_gklfnmgip:FOmoh7
7/%
H&kv=
p74HAffB?=qffB iRC$h H 7
f>s r H ^ kg===0t
? ff uppH4A ffB?=) qff@ ix5
vxwzy|{s
h H 7

fMz_ _%,
k fJkvip
F7C=Y7
k zs
h H 7
f>
} H C=7 kv=&
~>t "- ffB?=) qffB iR>C$
h H 7
f>78
mMfkp FO
moh
r H -

k=;C=
ppF ffB | *>% "H ff@?) qffB ixK


vyz@{
h H 7

fM`_8%>, V
klfJk ip: FD
"`=C=p
kSu^ h H 7
f>

iAAD*>%Ye*Sh67
r H 7fM`^_"
7
7= pzr/=
H 7 F<H H (D
7 F<H 3 =7-`7 H
f`= 8k K7m F"
z& Ap
ru G7 *>V
hA
7%,
kI0mz
A7p77Y;7 >FA7Y
7r
7
/
7! DYC4p77A
LR=% P H
H 77
LRpe% P=%% H 7
H %%>
"B *_`= :Fp"F=S F<H C; :p
FYC=peF^AG` H ! K
K;`o<d<1-;` H Y<7nfg- F<H YW:[7F>p_R%ipSF(7Y
H 7
sh0_7F%2p="
DA=_7C=Y7-7DSper
`===p/h H 7
f&`z_
7
77FF^KfMS== O?3ffB x i-"


7 H =O" 7, H H =M`D
7!/=C=7 p/ e`7F
`76C F<H YW:[7FA%r`7`6[g% H H =6,%=
`797= H
` H ^C
7D6=
F<H 3=7 `Y` H YS=p@7=/7 H 7
`YB C7_ FH ; :[pF:7F E 78 F<H C;:[pFTf6=SC=7D_7D
7SF`
pF7 H K
f=*`=p
7YG FH 3 =7D`Y`^

]

fiB5BB!-JBeBB77!
~ *|B7!ABAsAz&
7%B@3e7xe$%7zB7z|3%7!|zBTB7
B7e@ se=%B&e@7eB/7 3@A3Bs%7!OBA&A&B!A7N7@9
xe%7T;*eA!T|z!B/7V3%7!T-%7%=73e =793!97eT A3B
&733!3eRxee V@eAe7O773BV~&%7
R*eA!7z33ue zBA7;

%7! W3!VB!7!39!a3eNj7!T%7%&73eu@7We!33!3eN
57!z%7%&797u&3=g7B73=)BA%9@9B7 3BA3BC/%7 *7A!
z33e %!@B
-3!B!7!a93!977!&739!a3eB7/Z~
%7O *7A!z337l-3!@!7!33!3e7!
A!3Bx-!=793!97B7
7 fiz;!A!@@7eB B7N%A&;!A!@;*eA!A@9@O/B%agz3Ne

73e7!7!7+797ugz/!BNV7%7!
B$Me
NB!ANZ73/&
79!V~%7+77V3euuz39se3%7 B7z!Ae%7%&73eB3B!N@B%7!Ae
@eA3B!z7!zBB
977V=9!aV@!7!33!3e7B7eA!33!,
3e77!&733!3eu|A%3B3B33Bu3!79!aB!7!a93!977B7e!
%a93!9777!/A3BN&733!3eu7A%73/%73/%ABn,BNA!n!
%&=BA3|B73Ot%7BAe797u
/ssR0B*$
ATzBBea!@3e//%7%&73eu
u!TB!7!33!3e7 7!T%7%&797
,B!/3!7e!73B3#@O3BBA!@ B|7=A%73zY3B%@u s*B7
3V7TB337
*-,!/.10!3243B35467276e98;:=<7>5@?AB!,!CBEDF5-x

ff fi !"$#&%(') +
H5
G +s
e
< ,A+K]J L3
3 45M,&N&O!AP?05=F8
-0e735e@%7&eB7,%7!c9 !B57]TN%
%7#$5V_9=9;3
e/B7B77%7
<73e/xez%773!7 sB
B!A/Z73
VJaB7
3@7z!AB99@9t7B%/B7%agB7eaBVB7/xee/B7eaB!
Y3B%@B7397%773!7&aBNuu77!#,B
3e7eAN%7&t
A!@ !@!B73!B7&39=B!A%a97BAxeAB#A79,xT3*A3!
&7]_%z%7+
eTZ79V@73V&73%7 73ea9$@7VN
,x!R%7BA%B7BA3NBBA!R Q1S77 %7+e7!AB
*-,!/.10,9423Bu3562e67X8Y:=<eff5@?ZABT!,!CBEDF5-x

ff fiUTV !"W%M'=') +
H5
G +s
e
< ,A+K]J L3
3 (N&O O-A[?V\5=E8
@&^]9_a`Zbffc>cffcbd_efGJs+S3%7e7T3BT@!7!33fiBCz%%7B7
0B+%B7BA3@BA! ug_ h_ `+i c>c>c _ e $%7!j_kl_ 7B!amon@mlp|
q]_9W
rhst
%7t%W
_ k
r/xez@!A;
mtn&mVp|7!txezB!;
mtn&mVp|C%7!A
3v
u k %7w
_ k u k GtrMB7!
u k e73aV7]@AB73j
_ k Vz3;
u l
u9` cffc>c uKe
%7!(
_vux
_y`Pu ` c>cffc _euezGHrO!7g
_ H
r/ev
_M3&/aY5ts
{
\ ff "

|ff} ~C)+XC=>XC9dffX@)FffvffX LK}79X))X[C
MCE9Eff)[)9;9+>
1;ffY &E97WEff)[}=[d4
>YX))>d)1>ff(d&)X&)[=
d)C&[79X&C)[)[[y
C)d&;9E[)L+X>Y)))&d)C@}
X++>C)d&XWX&d)CX>ffX[dvC9ff[Z} 9X))X[CX++>
1>@W>=[d) E9)[}
X

fi&
/ovHff

@dd@d9$C9794@[>[M4-tW9
E[971C;>9a
9+>[

;$9
7z$ [
Pw
7

P



&
fi
7











g



>

[



9





z





1

[









;

























7

@





ff !#"$%&!'
(')*+!
ff
;1
(9
,-.0/1"2%&/3
5467!-"8)*+!:9Yd96
>4! ;7C9
4! <
;=>?/@BACd ff d1>9W9D
, =
4 @*
,FE 4 C G
(9D
,
<
4 yE$>
fi g(zd@)H fi *
[9
>P
fi&;9d ff #
7C9$
[I 9 ff ff >
)J LKd1d
MhN
K)7>[
MPOCQQ
KST
R M^+>[
MU7
R K;L 9@d9# KxWdd ff W V
X >4
fiCad>&d4>

XY[Z!\^]_Z`$acb'd1efhg

ZkjmlnZ3\ko#pQqrqcfj *sutwvJxy{zv*|<}~|!|
!v
#&xh=KO3r!vJ
~v_JvJWLLhxv6TTC:xhv_v5v
H
xh|L=~ 7h<
]_\[\o c >H,(_JJ(WA2(
tC7j4M91>>9[fi&H9d ff -d
( [jd4 [
PJ- , (J_J'(Wj
[jdjW
[7 ff (-
< , (_JJ( 4[Wd;W

79< L :. ) :/
7M74d9@ 79d >
&&>9g;d?
fi C9:, E JJ E g
F 5M
F*@ vC>H , (_JJ(
w4$

d><^
K W1Gz9H
*(
:@ d>fi #; ff 7a9L>H , (J_J'(W
- , ; E J_ E ;) 4d9;9d ;u+
)7#7*r ad
1T^
[ad 4[ 4[Q< g9 7a>9& 1>9w 1
W

[ ff & [d> ff 7[&fi Wff6 c 7>[Q#&cr ;<c(

ad?o
fi ga(9: <Q C MO= W ff d(d9#& NM
)7@>[u*.#=
7@4>[6r
1CZjC9
67M 4& 7
7 X[W< -
G.n, E JJ E
L 74>[3NL[[ G

aI >Y9->? , (J_J'(W A&;-; ff 7 9FCWJ zd>[;-CZ(d9#S;[+
9 Hfi5 , ;m E J_ E ;[ , E _J E # , ,mE J_ E v

> TM
wGz9 *M
L# 7P@ &C97 9CW>9#$fi >[*













0


7P W1Ga9, <4
+

$7a7 # [>[!4jtW9v
>vC&1a[W 1>v4[_>yvJ|xhJv
z!1^x~ E
7>j9dd ff 7
fi$ZjC 9$9
47[7=
H fi4g4z974>W;9 nV
7[Q
7? fiYW9C>[dPw
ff $G

6
t9&
~!*hxvJ|F
E>
9;[d[>J [fi&1ffg4$[9dd ff 7jjQ
4
9>[CJ
fi& ff >47&>[ >$>9E>>9 >97z>&[94$dC ff 7 7 ff +dW9
9+d#
fi&I V? fi$Q
ff h-47*
7 >
JK

Z:jwfiJf\wj (W vJ:>H , (J_J'(W&#AzMC
z97[ >9>
>CM
zW
v
-8v_|x_vx~#>4MMh-*Q 7 E >?&L,J(J_J'(W& AH
JJ

fi-

ff fi
fi


fi
ff fi
fi

"!$#&%(' )%+*,!+-.%+''/!10
)2!4356#&%879';:'<#&='>)?@A!17+)?%#B!#B=C'D%+-.#B!(#E>FG-%+'%/)IH
J
)%+#LKM-7N@
' 7$-?O).%+'!-P
QR )J%+'%
J
F
FG-%+'OUWVYXZ\[^]`_ _`_+]1ZIabO)?@cdVYXfeg[^]`_ _`_+]1eih(bSOj!4#&%>' )2%+*!+-k%+''l!;0
)!4)?mon -P
Xqp"Z [sr Xftue [ ] _ _`_+]1tue h bfv] _`_ _9]fpjZ awr Xftue [ ] _`_ _+];tue h bv;b4#&%/) R ' )%+!$x' ?' 7+) R #&y )!+#&-?A-2PzU{J
?@
' 7
3 5 D|%+-A'=' 79*~}?#B!'l%+'!I-P QR )CJ%+'%>0
)%>) R ' )%!4xC' ?' 7+) R #&y )!+#&-?J
?@
' 7\3 5 #?6S,A-7N'-f=C' 7Do#LP
' ) Q 0lZo#&%w)/-7+? QR )J%+'/)?@,' ) Q 0\e|#&%w)$
^" x79-J
?@ R #&!+' 7+) R p#"S'SDq)x7N-J
?@)2!+-E>vDC!10' ?
!10#&% R ' )%+!/xC' ?' 7+) R #&y )!+#&-?6# RBR #&!+%+' R P8) R %-
',)O-7+? QR )CJ%+'S, QQ -79@
#?x R *Du#LP8c#&%I)\}?#&!+'\%+'!
-PF
-%+#&!+#&='\x79-J
?@ R #&!+' 7+) R %`Ds!;0' ?A'=' 79*k}?#&!+'>%+'!(-2Pg-7+? QR )J%+'%I0
)%4) R ' )%!/x' ?' 7+) R #&y )!#B-?
J
?@
' 73 5 #?,OS
s>$`A~MM
M\k MM

-f'l!1J
7?kPj7N-EY%1J
%;J
E>F!+#&-?!+-k!10'l#E>F R # Q )!+#&-?-79@
' 7SAM?6!10#&%I%+' Q !+#&-?'># R&R @
#&% Q J%+%
mon+% DC#?>!10'$?'<!%+' Q !+#&-?I'w0
)?@ R '(n
%`Ss-7u-7+? QR )J%+'%`D!;0'wmon/KMHqJ'%!+#&-?l0
)2%z) R 79' )@
*

'' ?k)?%+' 79'@O?'x)2!+#&=' R *O*\Jxx R '!+-?,)?@k('/)'@
!4pN vS
m'!Z\[,V>p"p"v+v >p"vDZ
V>p"p"v+v, >p"vD([,V>pjpjv+v >p"v\)?@

VW>p" p"v+v>p"vS0' ?,'I0
) =C'/
-!10([8 VXfZ\[]1Z b>)?@O
VXfZ[^];Z bSj!w#&%(?-!




=' 79*I@
#L Q J R !!+-%+''$!10
)!!;0' 79'w)79'(?-4E-79'%1FG' Q #L} Q -7+? QR )J%+'%!10
)C?>([)?@\ !;0
)!#E>F R *


-C!10Z\[()?@Z SI-7$[^(?-O79'%+- R =C' ?!-Pz([$#&!10A#&!+%+' R P#E>F R #&'%lZ
)C?@?- QR )J%+'\!10
)!$#B%


F
79-FG' 7 R *l%;J
%1J
EI'@C*>([%+!+# R&R #E>F R #B'%/Z[z)?@OZ Su-7 |'=' 79*,79'%+- R =C' ?!-Ps #&!10\#&!+%+' R P



#&%8)=f)79#)?C!-Pu D)C?@O?- QR )J%+'!10
)!#&%(F
79-FG' 7 R *,%1J
%1J
EI'@A*, %+!+# R&R #E.F R #&'%IZ [ )C?@Z



0J%w[w)?@A
)79'I
-!101EI#?#E>) R& x' ?' 7+) R #&y )!#B-?%4J
?@
' 7#E.F R # Q )!#B-?-PXfZ\[ ];Z bS #? Q '


([z)C?@l )7N'8?-! R -x# Q ) R&R *l'H
J#&=f) R ' ?C!8J
?@
' 7#E>F R # Q )!+#&-?D!10' 79'#&%?->mon/|-PuXfZ[^];Z b$#?\OS


-'=' 7D!10'Pj) Q !!10
)!!10' 79'#&%?->mon/|-2PXZ\[^];Z bw#?>@
-'%?-!EI' )?I!10
)2!zZ\[z)C?@Z


0
)`='w?->mon|#?D%+#? Q '8)I-7+? R )?xJ
)2x'w#&%)/E-79'(79'%+!179# Q !+'@\%1F
) Q '$!10
)?.) QR )J%1) RR )?xJ
)2x'S
M?.PM) Q ! D#&!#&%%10-w?\*\Jxx R '!+-?\)?@Ou)x'Ip9 vu!10
)!V>p"up"v+vl>pj pjv+vi>p"v#B%
)?kmon/-2PiZ\[()?@Z
#?S-7!10#&%/79' )2%+-?D
#&!/E>)`*,
'I-79!10Cw0# R '(P"-7!;0'4mon!+- Q -?%#B@
' 7

) QR )J%1) RoR )?xJ
)x'#?%+!+' )@,-Pu-? R *O-7+? QR )J%+'%
?!;0'l?'<!I%1J
%' Q !+#&-?Du'>%10-!;0
)!/)?C*O}?#&!+',%'!/-P QR )J%+'%Iw0# Q 0 Q -?!;)#?%4)! R ' )2%+!
-?'I?-?CKM!1)J!+- R -Cx-J%PMJ
? Q !+#&-?CK"PM79'' QR )CJ%+'D0
)2%8)?kmon/#?\S8?#E>EI'@
#)!+' Q -79- RBR )7N*,-P!10#&%
79'%1J R !#B%!10''<#&%+!+' ? Q '$-Ps)?Imon/-Ps)?C*(}?#&!+'$%+'!u-PPjJ
? Q !#B-?CK"Pj79'' QR )CJ%+'% S?>-J
7uJ%;)x'-P!10'
-79@oD
)\PMJ
? Q !+#&-?CK"PM79'' QR )CJ%+'4E>)`* Q -?!1)2#? Q -?%+!1)?C!+% D'=' ?!10-Jx0 Q -?%!1)?C!+%)79'(%+-E'!+#EI'%
%+'' ?k)%PMJ
? Q !+#&-?%w-2P)79#&!9*GS
\Czo oW
-7E-79'S

QR )J%+'>#&% "
L^9;$#P#&!(@
-'%/?-! Q -?!1)2#?PjJ
? Q !+#&-?k%+*
EI
- R %(-P)79#&!9*


-!+'>!10
)2!4) Q R )J%',#B%PMJ
? Q !+#&-?CK"PM79'',#L#B!>0
)%I@
' F!10SO? Q )2%+'l-2Pg%'!+%/-P QR )J%+'%.w0# Q 0) R&R
Q -?C!1)#?.PMJ
? Q !+&# -?,%+*
EI
- R % DG!10'4monKMH
J'%+!+#&-?k79' E>)#?%$-F
' ?S
|/oos o.AsoA ff

M?!10#&%l%;J
%+' Q !+#&-?D',# R&R %10- !;0
)!l)?C*}?#&!+'k%+'!lU-P QR )J%'% Q -?C!1)#?#?x)! R ' )%+!\-?'
?-?CKM!1)CJ!+- R -x-J%gPjJ
? Q !+#&-?CK"PM79'' QR )CJ%+'D0
)2%w)?,monu#?\S
\Czo m'!>
'O) QR )CJ%+'Dz[^]`_ _`_+]1Ga~) RBR @
#B%!+#? Q !l=)79#) R '%>#?\D)?@ fi )k%+'!I-P
!+' 7+E% S0' ?!;0'
;\ -P8YS7S! Sfi #&%wp4]fivV X k6V Xf [[ ] _`_ _9]1 aMa b]



fi"!$#%'&'(*),+.-0/1&'#%324657#98;:'<>=
?A@*BCDBFEHGJI6KMLONQPCRBTSBCVUXWZY\[JY\]_^*`baQNdc\egfhjiTkl>ll0k.hAm>^onqp9rsp.BTtJPNAuTv$rw*p0BTp>L"t@*BxMt@*B
yz'{T|}z3~j{T| PN1c?`CT`t`'Knqp1cjkKse;Ah kKsl>llTAh kKsT`

P*CBTr*vBL*nN_heJ3F*1rx*J ef3k^L>t@*BxRAhk0AefJ*F.TkJ
ff00TkJ.'ff*0TkJ07ff00^`
_$_ BTtBr,x*nt.BAp0BTt"PN_uTv$rw*p0BTp>Lrx*ff'Bjr'PvqBpw'*p0t.ntw*t0nqPxNPC`@*Bx
t@*B |>{>| PNUFsnqpAt@*Bjp0BTtPN1rvqv_t0BC0pdQnx*uTv$w*'n$x*opw'*t0BC0p PuTuw'C0CVnx*n$xo`

t0BC0p0BTtPNAUZp0P*BJ;nqpJr,x*nqt0Bp0BTtPNCVPw'x*Zt0BC0p` PCjn$x*p0trx*uTBLt@*B9t0BC.\p0BTtPN
eJ7Tk7kJ7k'k,>30"Uffef73kk^nqpA ef3kk*Tkk>^`
w'Cd'B,x*nqt0nqPxZPN rFt.BC0\p0BTtjuTPC0CVBTp'Px*'pRt0PF?A@'rta'BTp0tr v$'w*nqp0toDWLWurvqvp
rn$x*n$Jrv"t0BC0gp0BTt>$`aHx;@*nqp'B,x*nqt0nqPxL"nN;nqp9rs'PvqBgpw'*p0t.ntw*t0nqPxNQP*Crp0BTtRPNAuTv$rw*p0BTp
Mef kl>ll0k ^?`CT`t`*p0PBjPt@*BC"p0BTtPNuTv$rw*p0BTpOL't@*Bxr |{>| PNOnqprR,x*nqt0BRp0BTtPN
t0BC0p?A@*nqu@uTPxtrn$x*pt@*Bn$x*n$Jrvt0BC0p0BTtPNUFZrprpw'*p0BTt`
p0n$x*J@*nqpAx*Pt0nqPxPNt0BC.p0BTtL'@*B'B,x*BTp9 y,qy~}|yz rpNQPvvqP?pnNhrx* rCVBAuTv$rw*p0BTp
rx*FnqpArRt0BC.p0BTt"PN"f ^UJp0PB'PvqBpw'*p0t.ntw*t0nqPxo?`C`t`'f>hJ^L*t@*Bxh y,qyT{
?`C`t`'nNAhk0A e `An$J*vqnqurt0nqPxnqpj'BTuTnq3r*vqBL?"BrBCt@'rxFvqPnqurvn$J*vqnqurt0nqPxMrx*
p0tCVPx*BCt@'rxpw'*pw'J*t0nqPx`"a'BTp0trR v$'w*nqp0tJDWL3WnqSBTpt@*BCDBTpw*vtt@'rtrxU,x*nt.B
p0BTtPNuTv$rw*p0BTp@'rp1rvBrp0t1Bx*BC.rvqnrt0nqPx9w'x*'BCn$J*vqnqurt0nqPx?`CT`t>`rxUt.BC0p.BTt1`P?"BTSBCTL
rpj@*BJrvqp0Pox*Pt0BTpLn$J*vqnqurt0nqPxnqpx*PtAtC0rx*p0nqt0nqSBJrx*Z@*Bx*uTBJx*Ptrw'rp.nP*CD'BCT`j@*BCDBNPCDBnqt
'P*BTpx*Pt3tRnxt0PFPw'CdBx*BC0rvNC0rBT?P*C0F@*BCDB` PCt@*nqpCVBrp0PxL?"B?nqvqv x*Ptj'np.uw*p0pntdNw*vqvqU
@*BCDBL3rx*FNPCt@*BjprBCVBrp0Px?"B@'r>SBx*Ptn$x*uTv$w*'BTrCDP?NPC"n$J*vqnqurt0nqPxsn$xor*vqB9W`
BTtw*px*P?bBTn$x?nqt@jt@*B 'CVPPN'PN3Pw'CCDBTpw*vqtuTPx*uTBC.x*nx*jt@*BBT*nqp0t0Bx*uTB"PN _ a0p>`"Px*p.n'BC
h6eJ_k7kJ'kk*rx* L9rx*J rpr'PSB`@*BxRh e rx*Jrvqp0PAhk0 e L
p0n$x*uTB nqpArJCDBTp0PvSBxt"PN1J *TkkT_Jk kTrx*oJk *TkTJkk *0TL
?A@*nquT@rCDBn$xAhk.AT` p?"B1?nqvqv*p@*P?Zn$xt@*Bx*BT*tvqBJJrLt@*nqp@*Pvq'pn$xBx*BC0rv_nN,h e rx*
hnqp"Nw'x*uTt0nqPxNCDBTBL7t@*Bx?"BjurxFCDBTp0tCVnuTtrt0t0Bxt0nqPxt0PJt@*BjCVPw'x*Fn$x*p0trx*uTBTpPNhnx*p.trxt0n$rt0BT
t0PJt.BC0p"n$xFt@*BRt0BC0p0BTtPN Up0PBO`
@*BA'CDP*PNPN BJrRw*p0BTp"t@*BNQPvqvqP?n$x*n'Br`1"Px*p0nq'BC"rR'BCDnqSrt.nP*xPNrjuTv$rw*p0BNCDP
rjp.BTt cPNCVPw'x*JuTv$rw*p0BTp`'w'''Pp0Bjp.PBPN_t@*BAuTv$rw*p.BTpn$xFcuTPxtrn$xJt0BC.px*Ptr'BrCDn$x*n$x
`1@*BxrxUFvqnqt0BC0rvpuTPxtrn$x*n$x*Ft@*BTp0BRt0BC0pn$xcw*p0tBCDBTp.PvqSBTr>?r>UJn$xFt@*BR'BCDnqSrt0nqPx`
@*nqpBrx*pt@'rtjnN?BJCDB*v$ruTBFrvqv1t@*BJt0BC0pjn$xt@*BJ'BCDnqSrt0nqPxt@'rtrCDBx*Ptjn$xL_Up.PB
Pt@*BCt0BC0ELt@*BxRt@*BCDBTpw*vqt?nqvqv3'Brx*Pt@*BC'BCDnqSrt0nqPxPN,` P*C_BT'rJ*vqBLt@*B"vqBNQtPN'3w'CDB
p@*P?pAr'BCDnqSrt0nqPxPNvqBx*t@WjPN19`@*Bt0BC.T nxt@*B'rCDBxtuTvrw*p0BTpA'P*BTpjx*PtAr'BrC
n$xo`aQN?"BCDB*v$ruTBjt@*npt0BC0UJt@*BjuTPx*p.trxt 7L*t@*BCDBTpw*vqtnqpArx*Pt@*BC'BCDnqSrt.nP*xFPN1CDnq@t
PNt@*B3w'CDB`
$Z $ * $'Z * $

ff




ff










ff



ff

ff fi








ff fi

$'

nqw'CDBC0rx*pVNQPC.n$x*Jt@*BjvqBNQt'BCDnqSrt0nqPxFU*nqBTv'pjt@*BCDnq@t'BCDnqSrt0nqPx









>| h 9
} *z3~>|yz TD~q} {
| fh^ } z
| |{>|1 j









/.10 $0 20



3

4

"!$#

65

!

%#

7!

&
8

:<;<=


0






J}s~>} {T 9} >\{ {T|y| |yz
z h e
hk0A e
'#

5

&

(
9

*) +

$#

,#

0

-!

fi> ?%@ ACBDE?%FG?%HG@GI<J6K@LBGJ
MGF A"@GFGNODEHG?%@LBG?PACBQ%R%?TSTJ@GI<J
K@LBGJ
MGF

UWVYXZX [
\^]T_T`aTbcedgf hOikj6dmlonqpsr%aTtWikj6dmlonqpuf h(vwsx'yucqzGr<{'cEdgf h(vws|s}E~Lydgf h(v'Zc<rmG|
^]T6v`Erer'To~%~%%%x'zTc<avw`Erer%TC~%~%%%x%C~WzT`b<rLocE`~P'{P`~PTY|1_GGGG~%Ccv`EaT~%
rr%TC~%~%%%xzTc<avw`maT~%rr'To~%~%%%|_T`aTbcd

f hvwsx`e,~%~LyE"~PEzTc~P7c</-zGrL

zTc<7ccP`ooraT`ocEoc~LP~PGaTt`aTor%aTbc~Ldx%TbzzGrL1f h(vw|mzTck_GGTGTC`~Ta
EzTc~Pc<xzTc<ccT`oor*tGc<7`{rLC`~Ta27~P~LErbr%TocxTbzzGrLWvws|_T`aTbc`
P7~TGaTtxTCWrLo~cP~PGaTtxo~yczGrY{%cvws|zT``T`c-zGrL~PaTb~Pa'rL`aT
oc<o~PnW|
cEucr%ar%CT`or%7oc<o`anr'aTtckGc~PTrL`aTct*7~Tg%7c<TrLb`aTc{'c<7Cc<o
`abr%TCcu`aykzT`b$z-`uaT~%`anWx%'m|1}~%oczGr1o`aTbcqc<rLb$zbr'Tock`a`rqT7~PGaTt`aTor'aTbc
~L1zTc2GaTbo`~Pa',27ccbr%TocdxZc{%c<7br'Tocm`a`rLo~r-P7~TGaTt`aTor%aTbc~L1d|W}E~Ly`e`
c<rLoo~occzGrLzTcr%mc7c<TrLbc<mc<a'm~Lkoc<oW`azTctGc<7`{rLo`~Pa~27~P7cTom`a
rtGc<`{LrLo`~Pa~Lk~P

]j7<pqc<rLbz7co~%To`~Paooc<`azTctGc<7`{rLo`~Pa27~Pb<r%arLo~c

b<r%o`ct~PT`azTcmtGc<7`{LrLo`~Pa*~P4
oc<ou`a

xo`aTbczTcWr%cWoc<ok`ar'7c7c<TrLbct'zTcWr%mc

xr'aTtj,%pzTceCc<om`a^zGrr'7cWaT~%`angj,r'aTtzTc<aTbcr%7ce7c<TrLbct%ptG~aT~%

r%Gc<r%`azTcb~PaTbTo`~Pa~LszTcetGc<7`{LrLo`~PaZ|


_T`aTbczTc<7cm`Wr-tGc<7`{ro`~Pa~u~P4yczGr<{'c
oce~LEP~PGaTt`aTCr%aTbcm~Ludr%aTtrL1Cc<ome`a


f hxZr%aTtzTc<aTbc

r%7cmoc<oW`anWxo~




f hvws|e



`Wr

^ikj6dmlonqp|Wc<aTbc

ikj6dmlonqp1f h(vws|



Zc<-rb<r%aGaT~%c%c<aTc<or`ct^o~zTcb<rLCcykzTc<7cdb~Ta%r`aTGaTbo`~PaoG~%~Lr'7`


'x r'LcWdhj,sj, plPpj,GlpEr%aTtvhj,sj,TplTpj,lsj,Tpopj
7~TzTcmcr'Tc

%`{%c<a~TaZ|'~Lu,tGcor%2qT`o<x<'%%p|WzTc<anhlsj,TpW`WzTcmoc<Coce~Lvr%aTt
ycWzGrY{%cWdf hvxG%c`qb<r%aGcWCcc<azGrL1ikj6dmlonqpEf hv|EzTcmr%7PGmc<a'kToct`azTcmG7c{P`~PT
c<rtG~TcaT~'1y~PozTc<7c%xGGcb<r%TocetG`c<7c<a'oc<om`a-o~PcEP7~PGaTt`aTor%aTbceaTcctaT~%7crLoc
o~tG`c<7c<a'e{r%`r'TcY|k~PcGr%Tc%xZ`azTcWP~PGaTt`aTor'aTbcj%jPplPpj ljPpop~L1dx
ycmb<r%aGaT~%oToW7c<TrLbc

j,Tpk'o~Pmc~%zTc<qoc<Cx~TqzTc<azTc7cTo`aTbr'Tocy~PTtaT~%

Gcmr%a`aTor%aTbc~Lsd|


azTcm~%zTc<kzGr%aTtxc<r-b<r%aGcm%c<aTc<orL`cto~r*<~1br'TocW`aTooc<rt~uro`aT%c

br%Toc%|k`roc~LGaTbo`~Pa',27ccbr%Toc<xsd`r%ar%oT`Cr%7br%TCc%x1r%aTtw`r_GL~%c<
GToC`To`~Pam,~Pd(y||<|Lex'zTc<amyckzGrY{%czGrL1f h^d(`ikj,elCnkpf h^dws|sEzTcqG7~P~`urLm~'o
`oc<orLzTcer%mcWrLkr'G~L{%c%|
zT`m7cTW`T`czGrWf hd`7ct Tb`TcC~r%a`T`b<rLo`~Paikj,"lonkpWf hdwOGc7ycc<a
P7~TGaTtbr%Toc<|1_T`aTbc%x 'zTcqaTcPc<rGx'`T`b<ro`~Pacycc<a-P7~PGaTt-br'Toc`tGcb`t r%Tc%x
`,~%~LyEqzGrLEf h^d`EtGcb`t r%Tc`ab<rocW^`E2GaTbo`~Pa',27cc%|

ZT<

<Ef h^ds

P7 ff
fi<'1
%fiY
'P

6,q$YsP7 ff
fiY%qff


<7qe



UWVYXZX [

r%aTt!GcWzTceoc~LrP7~PGaTtrLC~Pm~Pbb<GC7`aT`a^r%aTtd|

ZcEdh

}~yb~PaTo`tGc<EzTcq~%~LyE`aTCrLoc<mc<a'o<xTykzT`bzb<r%acWzT~LykacGT`{rLc<a'<|
j7<pf h^d|

#" $lol%$G`eGaTrLC`r%Tc%|
#" $ lol%$GmzGrLkaT~c<oGor%aTtm~TtGc|
j'&Pp}~GTocq~L(`er%aEc<CGor%aTtm~TtGc~L1#
" $ l ol$
j,%p
j,%p

)*)

P|

fi+,.-/1012 3547698:01-/ff;=<?>@-BADC1EF
GH IJDKLMON?KQPMSRTKQPMN?KQUVMXWVY!GH ISZ\[I]_^@R`P ZbacKQdH1eJ fgihjISIk:LlVmUMSRon pqJ rSIseutwv9ZxKQUMN?Ky^\MSk1zI
H1e{IBKLMON?K'^\MSRK'^\M|pwvX}1ISrSpw}ffeW twIBW~ISre v9Ipwv5J pw9Ik@v9ZKLM:pwvo}1ISrSpw}ffeW twIebvzISttQR

jjff\o5\ '\c :\i.To@QS:S ffQ'~`S%TwV
@wV%'\Swu
GH ITaQZtwtwZuzp.J f!v9IS1 IJ rSIBZbatwI] ] euvT]TZ\[IZ\[otwISv9vaQZtwtwZzvH IB1eb99I[9JZuaQ}1ISv9eV]Xt.]T1 pwv9v
KLlVlMjhjI] ] eTL9ZhjI] ] eLPK'v7pq]pt.e[7ZchjI]] eXU~R.L9ZohjI] ] eoU1R.LPZua5Q}1ISv9%e]otq] pwv9k
LllVUMSR(GH I[IH Ifpw{ISveT1[Z\Zba(Zua(H IIS pwv99IJ rSIZuaetwIeuv7fIJ I[9eutwpweu9pwZ\J1J }1I[G|p.] twpwreu9pwZ\J
Zua:eJVYs5J p7Iv9ISoZuaXKQJ ZXJ ISrSISv9v%e[pwtwYa1J rS9pwZ\JVQa[OISIMorStqeV v9ISvR#IreJ!eu}u9 v9%H I1[Z ZuapqJv rSH
ezeY!%H1euzI reJ v9IBpw9Z!ISv9%eW twpv%H#H IsIS pv79IJ rSIBZbaeVJ#hoZbaeVJY5J pw9Isv9ISZuarSt.e v9ISv
rSZ\JVeup.J p.J febtwIeuv9Z\J IJ Z JVe 9ZtwZfVZ\ va1J rS9pwZ\JVQa[OISIrSt.e v9IR
@#:i9sff'QB@1Q\QV7\VBVSS%:_71ox7BS
V'\Swffsw 7XS7#qu%SyT'Sff'QS|X ffw.SQ'1XOc\@
'j'%QV5\'\S1DXK'SMTXK' %Mj.off'QS|wV
jffhjISXsW1I%H I] eu pq]eutj}1I HZuaH I9I[7]TvpqJrSt.e v9ISvopqJRQayZtwtwZuzva[Z\]ihjI] ]eL
H1euoKQeVJ }H IJ rSIBeutwv9Z MoreJ1J ZVrSZ\J%eup.J!7I[9]TvoZua:}1I Hf\[OIeu9I[H1eJ!@kJ Z\[o1[IS}1pwreu9ISvk
a1J rS9pwZ\J vZ\[rSZ\J v9eJV9vZVH I[H1eJH Zv9Ip.JRGH ITv9ISoZuatwpw9I[9ebtvzXH pwrHreJW1ITrSZ\J v9%[9 rS9IS}
a[Z\]1[IS}1pwreu9ISvp.JTeVJ }a[Z\]9I[9]TvZbaff}1I HTeu]Zv9(orSZ\J v7pv79p.J fZua1a1J rS7pZ J v:eJ }TrSZ\J v9eJV9v
p.JeVJ }{ue[p.eW twISvp.Jskjpwvo5J p7IRIJ rSITH ITv9ISoZua:rSt.e v9ISvzXH pwrHreJW~IrSZ J v9[9 rS9IS}a[Z\]
H Zv7Itpw9I[7eutwvXpwv5J p7IeuvzIStwtQR: pwveTv1W v9ISZbaH pwvov9ISk v9Z pvoe5J pw9Iv9ISZuarSt.e v7ISvR
@#17wV7S\@Q'1%wVS ST'\1 17991
7 '\sQ%_c iiujS9~7 %\wSVff

.
V
ff
fffiXK'TM%:'\
?
K'pw}1Z ISvcJ Zo] eu99I[|9ZTzXH pwrSH9I[9]Tv
jffhjIS
9% 9
H {ue[p.eW twISv~9~ e[I ] e1~IS}xWYkjeuvtwZ\J f!euvH ISYeV[I ] e11IS}9Z9I[7]TvXp.JXMSR
n1111ZVv9IXXKy%M:
ff%99 R:GH IJ oK'7XM@9\RhjIS W1ITeJ!hn
ZuaoK'%XMKQJ ZV9IB%H1eu!
]T v9TW1Isa1J rS9pwZ\JVQa[OISIMSR#GH IJaQZ\[IS{I[OY
L "$#%"'&jkH I[Ie[
(*)
v rSHH1eu%
%(*),+
-)RBGH pwv ]TIeJ vH1eu
%(*)./+
).DeJ }xH IJ rS
!(0)1
-)2kjaQZ\[oIS{I[Y
L "/#3"/&jR:GH I[OI%ayZ\[OI DXK'T9oMSR
n p.J rSI k1z5
4 J Zuz a[OZ\]ihjI] ] eLoH1ebrSZ\J v9eVJ9ve1~Ie[p.J f p.Ji]T v9eutwv9ZBe1~Ie[
p.J!RGH pwv]TIeJ v%H1euXpwveB6n 4bZtwI]_v1W v99pw 9pwZ\JaQZ\[o_zR`[SRRj RXGH IJa[Z\]_hjI] ] esU
z7
4\J ZuzXK'T9oMX skH IJ rS

R 851[OH I[9]TZ\[OIkjv9p.J rS
ipwveJhnZua(XKy%MSkeutwt
rSZ\J v9%eJ7v:p.
J ebtv7ZeV11Ie[|p.JsBk H IJ rSIeutwtffrSZ\J v9%eJ7v:p.
J ]T v9e1~Ie[|p.JBR:GH vpwveutwv9Z
e 6n 4uZVtI] v1W v99pw 9pwZ\JayZ [zR`[SRR~ \RGH I[I%aQZ\[
_WYhjI] ] eP1R

dZ\J v7p}1I[ '9 KQ:ff;M=<>9 K?:ff;1@MXeJ } <A@KCBXMR!D|ZH eJ }x p.] twY#H rSt.e v7I

9 KQ:ff;ME<F9 KC;1%: MSG@KCH%MSRJIZuz J Z9IH1eb(X9 KQ:ff;ME<F9 KC:@;1%ffMSG@KCBoMjeutwv9Z
p.] twpwIS
v BRGH pwvcH ZVt}1vayZ\[|rSt.e v9ISvpqJfIJ I[9eutQk IS{IJsp.J%H Ic1[ISv7IJ rSIZuaW1ebKr 4Vf\[Z\1J
} 4 J ZztIS}1fVI

LNM*O

fiPRQTSRUGVXWQTY6QTZ6S6[N\.]*SV6\1^6YRU=S6Y6_`WZ6QTSV6QUGVbaTcTQ-d-\2S6[N\1]0SV6\1^6YRU
e fhgi-jk-j*l-mnojNp7p7q%rostujNvxw7yTjNk-jNvzq{n}|~6-mr2kmi-rosszj**mzrokffj k-nowk-j*j* mi-j s6j**r2qnNqsGj ,i-jNvxj
Tq k- qTvxj?6k-*mzrokuCvxj*jqTk-erosjNp7-mxwTfbj5k-j*j* mi-j,yTjNk-jNvGqnJNqszj mz!6vxtTj mi-j j*lroszmGjNk-*j
q76rk-j**mzrokf
hXX 7 uRGNoTu***0.-T TRTx *CTTTxTK76T3TRe
G!*N!NoTu**Nff2 TR% .N /
JR6666Tszj qTk- |qTk-noj*m! ~6j7q p%-6j*nhe N f%-rk-*j
qTk-bqTvxjszmqTk-Rquvx6ro*j*qu6qTvxmN|6mi-j!*n2qT-sGj rs j*6-rotqnojNkummzmi-j=}vGp!-n2q ! C
.,i-jNvjff ! 6jNk-Tmzj*sffmi-j,6k-rotTjNvsqnonow66qTkumzrRj* *nqu-szj % fhgi-rosp%jNqTk-smi6qmrosqp%-6j*nR
-vqp6j*n{EfJ6vxmi-jNvzpvxjT|R ros qnoszq7p%-6j*nEe=|6sz7rom}Tnonos?vx-pe
mi6qm ros q7p%-6j*nfhgi--s5e |i-jNk-*j fb
ffjNqTk6vxtTj mi-j%j*l-rsGmzjNk-*j%qTkffqukTwk-romzj%szj*m `*n2qT-sGj*s ,i-roKi*kumqr2k-sqm
nojNqszmhk-jk-kumqT-mzTnoTyT--sqTk-=?6k-*mGr-kuC?vxj*j*nqu-szjTfk=q*mjffNqTk6vxtTjsGp%j*mi-r2k-y szmvk-yTjNv*|
k6qTp%j*now7mi6qmffmi-rosros,q%N.TTzf6gi-rosffros,qTk7hmi6q{mrosk-Tmffk-now7r2p7-norj*|J~6-mq*m6qnonow
*KK-%~uw qTkTwTmi-jNvyTjNk-jNvzqnoroNqmzrokE
u2NRJj*m`~6j!q*n2qT-sqnn2qTk-y6qyuj!qTk-~j7qk-romzj%s6~-szj*m hfff5k
r2kros Nqnonoj*qN.T3Er2k|6r / }vj*tTjNvxw yTjNk-jNvzq{nroNqmGr-k b/6k-6jNv
r2p7-noroNqmzrokJf

Tmzj mi6qmr!ros qTk`{=qszj*m7*kumqr2k-r2k-y`qm7nojNqszmk-jk--kumqT-mzTnoTyu-s%6k-*mzrokuC?vj*j
*n2qT-szjT|3mi-jNkX~TwjNp7p7qX ros7romzszj*n,6k-*mzrokuCvxj*jT|ff~6j*NqT-szj rom%si--no`r2p7-now`mi-j?6k-*mGr-ku
vxj*j *n2qT-szj.s rkfRvr2k-szmqTk-*jT| CR C rosqTk`{5


C ff F C 6 fi h z qTk- CR 6 fumzj mi6qmmi-rs
ros,6vxjNvxnows6~-s6p%j*~uw7mi-j
K *-|T,i-ro*irs CR C fi f k
pqNwszp%j*mzr2p%j*s ~6jmi-j jNp7-mxw*n2qT-szj,|}vj*lqTp-njrE fi f
**" !{# %$& !T( ',* )+%? ,b Gb/uT*T u. --/ -- 22
-R
0 1C*Nx,NoTu**JKxT /TRCu1u oKTR T&
1xCTCufi -Tu*NC.T& 1Kx,oTT*
..-Nx%/ 2*.7N.T4 3ff x,.7
JR`Jj*mff

*% 565%5z 76,~j5q5k-rmGj,szj*m3*n2qT-sGj*svxp|us-Kimi6qm*kTmqr2k-sqm3njNq{szm
k-jk-kumqT-mzTnoTyT--sff?6k-*mGr-kuC?vxj*j*n2qT-szjTfffbj NqTk q{szs6p%jromi--mnoTszsyujNk-jNvzqnoromw mi6qm,
*kumqr2k-sk-%mqu-mzTnoTyTroj*sNfhJj*9m 8~6j q& :{TnojNp s6~-szmzrom-mzrok}-vh.| ; % < 6 5%5%5G <>= 5~6j mi-jmGjNvzp
szj*m ~u?
w 8hA| @ B *% 565%5z = ~j7qszj*m tqTvxr2qT~-noj*s7qTk-D
C N6 5%5%5 ~j7mi-j%szj*m
qnonyTjNk-jNvGqnorNq{mzrok-s `6k-6jNvr2p7-noroNqmzrokr2kEfhTmzj mi6q{m C|6sz C ros5k-TmjNp7-mxwTf-r2k-*j
jNq*i*n2qT-szj r2E
k C p%-szmffr2p7-now mi-j 6k-*mzrokuC?vj*j*n2qT-szj.s r2k |-romff}unnosffvxp'JjNpp7q mi6qm
H * @ H @ 565%5urs
qnon p%jNp%~jNvxs C qTvxj 6k-*mzrokuCvxj*jT4f F3w JjNp7p7q G6|6mi-j szj*m C 6I
q=k-romzj,sGj*m{*n2qT-sGj*sNf-r2k-*Jj C6rs3k-romzjT|6mi-j,szj*mff H L K @ sffrosq{nsGk-romzjTfERvEszr2p7-nor*romxwT|nj*m
H K @ 6
5%565 H # @ ~jmi-j szj*mEq{nn6rsGmzr2k-*m H L K @ sf
j*m K ~jquk! H K @ |0}-vj*tTjNvwJ NOPNQJ|s-*imi6qmhJ *% 565%5z qTvxj3szmqTk-Rquvx6ro*j*
qT6qTvmNfv j*tTjNvxw`

NSRTNVU |mi-j%mzjNvGpszj*mX W~u?
w 8Xros%szpj!sGj*m % < WZYN% 565%5z < WZ[z]
\^;|s-*i
mi6qJm _a`IRcb*f vxpJjNp7pe
q d6|Jffj%i6qNtujmi6qm K W |6Cvj*tTjNvxw NfONfQquk- NgR]NfU|
h%i%i

fij9kml.n&o&prqtsfiuvwo&l.n(xzy|{}le~I&6

r%r/L9 |LLc^/JVV"%%/#(r%9&6. ^zB%eD.&rrr
rJ%cfiJcff"%
r%ZS%rZ
&Zc.&cL^#//Affc#%cmrLrcS&c99#P/.%
f.
r/.%ZET.r%Z.TZEVJ&/Ar/D&L# % JV# /rEr
.
AXEct 9 /9L#fi/wceX.%r%fic%ct# * L&r&%*&r&rB
r%r/* r%t&cE9mr/

#/r/&r%r/.}4fim/&mc/?&6.Jr.cLmr(
B%B(6I //}"?}*t"%mtZ6rT.mBA/&cA%9J/r ff

fi % 6 }%!"#Z/ %$w r&Z '()/+*,-. +

B/f&etrefi/Jcw>&r/
0>Z/e/m.rfi/m rT/cmrJ.r./%
.r..A&AffcL1r%Zfi./L2(#rrcmr/].&//mrecB.rfi.../
.
>Z 3wr#&Z/rrLr/%E.r%#LX&//mcAAXc 2 49%.Z.&r9*fi &//mc
AXwI/ wm


/6B(

rE/Zm.I]rermcLmr/D/.%grrrIrD&&%
65%&.rSm%&Z/r
m.r&.ELrr>&r/&&.tr (rEDr
/&cr]r&&%c
c
&mre&r?cfitrrJ%c%/AS6]r/.%Zr//fi&mr/.]]mtr
r&%
cr
0>/&r c%?/m.rfi/% r]6I/%
8 7c &.c&ff
9B9mr/Tr%ZT.]r
tr/].%c/Z/9c/m.r/&cP%.;
:&%&6.&" <A%Z%.fi>
=m] r&&%9ctrfi
5%&.rEm%&Z/r% rrL49rE/%&4rr&&%9cr
0>/rcc%.&r/fir
0
>Z/#/m.rfi/4rr]c&trfi. rLL&r&%Zfi.r&mr(&mr/J..%cBmr%c.r
r%r/c..%c/&rcc%r/%JJ&tr/DmE% #c %&/cr%Br.
?r
5L%fi&.r
m.fi%&Z/cfirr% rfi.r/.}&trA
@ C
B DrL.rE
B F F//H
GJ
IffB K KfiX&H
z
L
eN
MPO r%E9&6.J G Q
I(@
@SRTO& G G(U #.r G(U V
Lf G r9/.%ZW
Mf.}/
c.
&%#.cS
X %&rr 0Y4Lr%r(&
ZZ\[/
Jr.r%m%Z/firD/rfi/r%r/
c r/Z%^
]
/r/%r/ 0>c& /rm.Zff
0
(r/
r#&&%ffrwr#&trc/ 0>c&&/ V#X&//mccfi/
L}r%JJ/%&cLJ/ 0>c&fic/AL.E/m.r#r/r/c
r/S.r
r%r/Erfi #&_ ` emr/e E&//mc4cLL /r&%
Iff@bO#r%Zc
@
rr
0>.r...rr&r/
0>Z//m.rfi. r%D
&//mc9A c*Sw..%c
/&rcc%.JfiA
@
dDZ/c.%fir/Xr#V&X/ e 0>&c/AJ/ 0>c&fic
c
@]
B%B(6gfC,) @
E @


8JSh!Zij#>(6ekl ZX& T$ & &ZX '()/V/& _m/.>\

npo rqAstT9Eptvufft/wm
>r&Z/rX&r//4&Zc./Dr
'ff8/&
} cL.DA9m c4/.%Etr/gc
/m.r/J/cmrmr]c%crr0>.r...rrL>&r/0>Z//r.J>rJ&r//9
LL/rzr _ `yxzr
9rc.Z&EP
9rc//>Z r
c#r/I.Afi r]r
&Zrccr&Z/r//B/&r/LrJAXwff

{&|&}

fi~j;H!!!& ff!e!6!!H!ff!jv>>8!&eTff!e!

_b)()ff
zz> bff! 8> & ff8>8>ffPi&\p>>m_>>>!> ji>
8>j
c&Wp\%8
H&VffHff> mJ 8>j-+_>"Wi >\>ij&ff
>W !&\iP>&ff8>> -e
H W!v>;!V!&\!8> E&Vv8> 8>!8>!>&A 8 -
Cff &&kHy _!ff>>V>!V!&ffH>>!V!&
ff&AH8b>&jr H>j W8b>!.ffffH> &!
H Wkv>gez>ij ffAff > W-> &!v >>> >
! !&ff >b >8Wp>b(.>i 8S
H &&& k!> Hffffp!>j Hffc
H A!SWPb\H!y & Vg H> SV&!V S!>
b8 ff.i_!b
kH&Vff. ff&&&_V>cff > -> v!W>>i;+j> &ff#
Vpm!> . 8 >>i&ffji>WH! > >b_
! >.&Sff8>8>c 8> ( &Y_ff
_!&
H !> 8> fiv&fib

H>S > > ffH>ff>!(ym>c!.ffHff-H> >& !ff8j> !ff

r&SS !>&V>+>W!\cffHm_ V&ji>> g8 >E!\ff
H> .& !zV i&A&\>V V &&T >y>iVV&>!ffy > & 8>V
6#&V&> ;8v!\!\.>&iS >8ff#&
c> v_!ffH
> AV!>_!V_m >ffff&&H>&v6 ffH_!ff>Sp"!r#6$fi;&%fi&VffH!ff
!>ffH!>!V V> 8>b fi&%fivff
!ff!\i! . !>&'>S>&
!>p(!QV_! 8ffpj!)!J>E V>-k!>_!V_m >ffff&&
!!!k>+>!V!&ff!j> >_jp8 +*>>+>!V!&Pff!!ij8> ff8> 8
& ff>-ff>!C+E,z!_>&Vff_!!!y>&H+-S!j> ! !&ffHj V!i8
HV _! .k &&.0/%y>&V>->!V!&ff!ij8> \V!ff&Vk >V
/
_> >_j&pff8> c>! !&ff &ff1H32 54p 6*7N 98: >v>b>!k&ff
!8> i&jffb!ffV&kbV ; 1 y>W \!8> b 8> -> &k
V > &\iH>k(&j +\%>Hj Hff&ff&h <ff => ? H>&_>
#&&j kffffp!> yff% ff8> A@ B,@ =DCFE G V&V!&
y>>+\i! >8> c!ff>&Hc &-H>&ff PgV>Hc &
ff.i_!JS I&>!>8> H8S>&ffi!!&!!>!>S>V> S!>_ff
ffbpiff8&ff!<!> I!!&>ji>>E>-!>H!y>! >c>+y &_> j&
> Ij&> ff_>p Yff!r!> > >-_> &ffp!H!\+_>pff
z>H ffz 8>j ff8>8>ff&ffj>H>m>>m!> >H_p
S!>_>
K>zKNMPOQpv R(S8TN(U- VXWZY[!]\^Y_`%a _bdce_dafa _gdhibj_ hiYlknmpo rq5c"_tsng(q[uY"c Y%[+vFo
J L
`%a _bdc cwo xFvy +z_g|{c}`vg|[u_q~gc_[aY_lc [wvg|Y)g|vgAF[u_bj[uva v9hivdbcno big|`%[uqvgAoexFYYff`%a _bdceYz[iYg[jY
WnnmvFoqgV]qpc`vyfbi[u_j\ Yk
%%

fin5AAjf^9wA(|)A

+(|(&+t(AfA| ffww|wA|
nA^Fi%09n%j%9l %l9 ijAjA%j %l^j<9iF%l9 ^w 0 %l^jj%n+5jj
5j %l9"d%j%9l %l9 ij09 9"0 5j9 " jffi&A9l ^ , "&jj%"Aj
jtjF%l9 9A 5l %l9 iL|j ff njt(Al|l j%09w%j%^l %09 iLA tj9 )AFA ff d%
%Fj 9%jj"A,0ZFn" 5j9 tdjff inAj9juwAAAAd9"Al+n"A%dtij
Ad9 9 " dffj %Xj)Fnj jl9 j l j0 j<AA9Altw5ffj "%
j Lj )"5ffj jX tj% i"d%Fn ffAljffAl"j
ff^9 5ffj <%+AjAj ffj5ffj j% ij ij ijl5< i^F
0 %l^jd5jffjAw0$ j&
6t jj ijtFijAF jjj9 ^jjA0tj" j^9%j l$d< lw j 9AF
9 9j L+|i, 5j9 nj el L idl5Aj 9 ij%jjt j 99%j lff ,n9^ Li%
Aj 99 iL(j %jwj%l^j%D idl5jtln %l9nij"jidj^ ijAj 9 idu6D
5j9LjAFjltlFi#Dj5lu"tln % Aj j^9%j lffA j %l9 %
9)ADdwl %A)Al$fj^^ jdltjAl$(A <jAj i<l
j" 5j9 5^%wj 9D"9j(DA &AF
t9 jwF%l^iuiLj ,A 99Aff F j,0wn99%AleFij 5jll9 j9

j& ij9 A%j ,fF9FiDA%d ui<i%d%"jfDFn)9j 9dFfF9FjFA%
ui9j5l%,j%& %XA%l9 jlAlj %l9%j%9l %l9 iX 9j9
j
9Aj 9 ij <j %,jj^A5ffj ]dd&j% ui9j5<j 5ffj
Adffwjfffl Al jjniF%l^ 9+A 5l %l9 Dj9w "L j+ j dffA
jL(j<j^5ffj dffj%j9jA0w t5ffj d<Aj<flj (iAF












fi

+ iAF















ff








fi





L%l9td%j%9l %l9 idjiF%09 9nA 5l %l9 ifF9FjFA%d

lAAAd9djLtDt 5j9 %wj%ffdAnAAFij %5fjA5jffj"
l nj)L tjF%l9ii,j Alw ftjw 5j9wj99%lff0j^j Lln
A9fj)9j %l^t 5j9"j j "A dj9 djLlwAj < dj9AA%dF
Aj^AF<^fj9j 95 %







"Xjj%Aj w5%l9<l"A 5l %l9 j%F< jXAFij %<t%F<n<%l
ffLlA9j95j ffLt Aj l %d^ 5j9An"AjA "jAj %Dl AAd9(%F

56Dinl+ 5j^AFi%F j<f"dF^j(FA AdF%jj%jj9j )L
Ajll %9j" 5j9j e tjAjjl$tjL%tj n60 tltj9 <5jADil
lwL%ffff
AAA9

ij

DdF5^<lj 9& 5j9 <j F
9dj(FA PdAF%fftj% %dF 3

ff<Alt95j ) "u lt"Fi
L%ff
AlnA 5l %l9 ilAjA%w5ffj %l9 5ffj d)+tijnn"A%dtj
u lt5j<F j









2

"!$# %'&&'&(% *),+




.- %'&&'&(% .)/! -10 &'&& 0



3'4'5

)-

fi67ff89;:=<>7ff?@7ffA@8@B'CED8:@CGF@?9H8@?@IJ<>A@7ff8:@79;:LKffMff7,N,CO8@B'CGDP8:@CGF@?9
Q*RTS,UVSWYX[Z(\^]`_EabSdcfeSgUdhjilkm._nc=o1prqs'tfovu(sxwzy'{|wff}~wff{T{|wff},ws*s'Hjs''sEts
J otEss2sP~tE~ wj E.o
>,l@,|,.,,',ff|@*, Off,;.OJff','((r@ff(|',Off.;lH@,(|n
Off,('@,';*ff@ff|x@ff(ff(,>@Ozj@,(|jOff, ,@ffjff
x1ff'.,|( ,;('>,|j*'ff,>@, (^`@,(| Off,;@@(@'
|ffff|'O.,||'(||O>*O(;ff
TU V'UTGV[[Z(@bb'_GeS(ab'VebV'S=Uhh(ceb'_OU chV'S,SLeffTfaPSa`c SVr xp },y'tEffss
y'{|wff}~wff{{|wffff,}wPs*ffffs'suP.E>T{EywfftGg~ w{wttGys
'(x.^(zOff,,@ff j'ff@,ffjff@,|l,jff**@*,rO, ';
Off,Off,,@ffffd(O,.,2@,|>;zOff,( ,, ^(zOff,;.|((j
'ff'ff*,@,ff'*,|((`>;.Off,(>@>x @Ol^''ff,(
,*ff Off,(j@2,;g;,@((^(Off,( ,L,ff @ff2lxO
ld,|>(O*Hn,,2@'@(|ffx@O||'(|rO,(;'lff','(||'(|T
OT@|j .nffn(.n .dd(.d @j .(.d ff,
. ,(Y. *>,'rH ff,r ff, ,'*|2,l^(
Off,(z , @* '` j ff,L lz^',,'|.,gx@>
j ( Ozl

fiff ffff !"# fi%$& ff(' *),+.-#/ff

, |';,L| ;,ff'@ff @.,(|T , s'{|wfftE's ,|';,L| 'L*.',
',( 0ff,@,10,,>|@ffL'ff ,; (J*@j|j((2,(n, @@';rff,
O(|,=, ((|T>|^@;',(|';2','(||'(|,z@,@'*O.,|'(|[O;
(L;*zff|ff' 0,@,2
0,>|@ff
3 5
476 J,g,,.((| '*'((
|';,,2@,@'O(|ffjO.,||'(|T
O;',>||`@ff,j@,|', '@@,;,l1O(|ffjO.,||'(|rff,z
@,|(|ff|ff' 9
8^ ; ; :,<=>= @T :??ff ^jff|ff>,ff||^, @,;,l` @.,;,
O(|ff(l 0@,@
0,,>|@ffA
3 (z@|((O,,| Hn"@ ,(|r @.,(|T
j>||`'|,5
BC8 @.,;, 6 DFE
G >
K ~}u~},*s~ Off,(*>|z@>( 0@,A0,>|@ffM3
Hc_Ob'_UJ
c IX Off,(jL
,'x| . ;(|,(|N.,@
3.O PNRQ$ * ,'!BCQS6d|j, O.,||'(|@,;ff
ff,
BCOT6@|ffzff |;ff.,|(2@@,@
U
V

V



VU WUa'_b'_U.
c q st w u(s*y'{|wff}~s~jwff
EtElsP~@sytt 3 Z 7[
3

u;s2w ~s't y'{|wff}~sP~Pxs XK ~}@u~},2sP~

V UTUh ,;@',jff||^,(|((''ff(,z' ,,|''
;:' L8 @* >|@^(\3j
>,' |* ;(|,(|]N,@^32O PN7Q
N,@^ 3 P
N
Q
(?ff >,' |* ;(|,(|]
_`fiacbFdegf;h(hijh(hlk(bFd,mnh(fpoqhlr,bgos,fut,vbghlbgwxvRh(wzy{egw|hl}bFv~hlrwmry{egw|hl}bFvRxbgjf;oci#ki|hlrnfuksbFfukfuv,hsftv,bghlbgwvPwkfuegijhlbg|f
olm,dnolm#hlbgwvRbFvyegwjhl}bFv~;_du~i|ofri|f#o(fpfuv
bFvPf;;hlbgwxvR,`
,,

fixfi\

l^,MR|x|x]||n^%,R> .%
(,M .%
(# 7A]
;,(!jCC((!S|x,]n;l!>1j&7fiC
^,A^lx.(>c>&(.ln^;xRM{^|^,;5|xx|x
,l;(>cn((>jxz#|x
xx|n,R,;^n,>,
]
|Mjn,nx5n 7*,;PJ,p>x,;\|5x,
ncMF^ x^];MC>x,;|7Cx,RnTM7F^ F^%]
|,,|R 7 ,>|;^AjMx]Jn q\7,;\n,#x|7|;&5


,,x>x,cj\cM xC@nxnMAnC>\x, R^|n,;

|n,
]S;7>|Rx7xn

fiff >x,Ac52
;nx7xR
qx,
!#
" % $&
*
'|#
"P !
(*),+9 -x
'% $
(
),+9% -fixj 7^Cq|n.|zjC.
nAc5.;nx5x\0
/
x]x 73
2xAjn >|n!|]x,x4
!
{x% ]c5n1
|,|4
5,xx!,x;|nC\
6n7
% !T|x!\8
6nzn|,;
x,xnnx9{MPn A,M^#n " xM|,!^]^:
97] "
$#|,;
97] <
5,M ,^|0
% !*
$
R; >|n4
2n >
=2 |>]9>,; \L]
-@
?
(A+9 =|,J
7
^C
B#@j]jxnM2c57#zAj ]
7!1,2>@|%||C
|,A|,;RRxR,Cnnx; %MnzRx#
D!%|jR!^^
xn
x!,{],;lF
ER];xx x,|R; 1AC

55 % !8
(. -fixG
' $8
(. -fixH
Bz^nc|^xj|R\{,x

|5||>I
=fic|x,xn*
!,
$,
5^,J
9 !njxnx
|Mn|x|n7AcMJn
;nx5xS
/
^,,xncjPc5.n<
;nx5x\.x,*
5,7,,x (]
|{c,nx
!|7cML
K8Mx,^^,xC7n nx,nfi>x,;x
L &
N 'OPOOQ'NHR
A\!x%xMnS
ffP U VWXZYT[8
fiK\x,xC !,nxjR9
#jnC,,|>;RCx,nPl(FT
\.0
)H\|,^x9{xx,,

'OPOOQ'jZ
^xnc>x,n,nxnq |
] Cfi|1

x ^,S
7LZ
_M `L,M Z
_5W `
Z
_H(a)^ N @b OOPO b NR

`
Z
_,(c)HN (COPOO(a)HNHR5 C],\jn9>2c5RnM|%x\M>x (
)HN (>OPOO(0)HNHR7 'POOO'n( ^ (d)HN (>OOO(d)HNRjMnxAAc5Sn<
2;nC>Mx7 R
e(*)HN (fOOOg(*)HNHR |>xq5x ;n|c5Cx >P|
#


5,MA|
|nq,nx{|McMJn ;nx5xSxA
|>C

hHijlkmnfiompPo>qHr@mstunwvutyxnfiotyz|{Sp#}<{S~<mfik>mfivynfiotImlP r<vutysnfiotyz|{
A|>x,;AxAj\xx,An FM* Cnx,; >\x%|
Cx,\ F7]q|R;,xx*|] 7 ;,C5|xx,n

F
/ Mx,#xR|^ x!CC>zun6n5| A|7 |
JnR
xM4

/H
xx^|RAn|\l|fi;g ;|#x


ZTwPFQIfifi.d| /Hl| V/3Q#ffg8M8
TYT1 fiKMCA/71

P*7 fiU .a/H
P[&M M>*fi4g Z R /7


fiA@Ua@
lTTUGU
PL8Q#cPPQXZTZa<HCgXT
TfgQf8TQgg[fi*1QX.XT1TQ
1
>381QTglP@1;L P fiXTZ .>
,

& fi
ff
XT X.Z< .
ff


Ty
;#
V*1
ff
:g
<aQ Q
!C
"
#%$'&(*)+,.-/102(*)
3y
3u* 4,XTZX d8TQgg[
.XTX#TT|#TggQg
I8T Q Z T65
Qg
'@T[*0# 8T
ZZTXS 8T
ZT
QT
7@Tg1 Z
QI<Q 86 9|8.
QQ:3u>XTg 8*#Z lQ
Q#QgZXXZ84
XT@ 8TgQQTf
HT
; 5y 8TgQQT.Q<TgQg
T@T TQQ,XV gTT4ZZ
XTQ*Q 8
ZQQP< @T*
TQTZ<TH; QQg
Qg
H##gZ=
,> Tg?
8@ T<
Tg
# TTlTB
TQTlg#XT 8
gQQT0T1T
; 5y 8TgQQTd gQZTQggQg

#TggQg
> T*QQ@ITQf4TggI
8XyTQg
0
8P
,>
Tg
d#; CZ 8 Tggg0XTdXD 5yE 9F*a6 9|8#TQgA
H
G. XT0

T| TgTA
QfXTgQc1Tg>
XX TXTQg
QgT 8gl
Q4Q TQI
Q #TggQg
g1QQgg
Q% TQg gg@#T1QX> C
CT
TI
CTT&8. 8Q
P
G.XT.XTT|*48Q.y
XT.X; T.@
yIHXT< 8TgQQTfgQHTQgTT<TX TXTQg
Zg<Z
*QXQ <8g
#TggQg
T8g#TggQg
g<XQZZ
4QXQ <#Q TgT
J

X TXTQg
g4TgQg
T@PT@ 8
gQP
Jfi
*gg,#TgQg
*.0
TgdE
XT 8
gQQTZ<g4TgQg
T.0XTZQTZ7yTQg
; 5yZ 8TQ
K.T
Lwg

Q @#TggQg
gQH8TQggQg
T7TTH 8
gQH#yTQg
; 5y@T

3yZQQg7T<THX|**TH
<g<g% 37STTQ*Q.TQ@M
.QX
T@TQg

SXT0@Q .#TggQg
figQ*TQgTT4
Tg 8
gQ '
8#gZgQd% G.
XTXTT|@1*d@8#QA
Q8Q0 g TgQg
Tdg@T 8
gQ**>

#Q0Q#XT.*> CQ*TQgg0
NH TX#TQg



+>P%)(RQI,.SUT%V<SUWSX)ZY>/

lf*
Tg g[CfQ1X8C=4,Q3%QX5.K4ATgQ4TXT4S
HXTS
#QP4Tg
T>Q## 18.XT1G
\

S^]_SX`;SX)+SU/

K.^a#Lcbde>e>f>gh*QT0Qg
figQTT8
gXZ*3u'ilTgQ
TjblkH|mgRn_oXp;qsrut
X>Z
n { w{ Uf>>f;E
>f
K Zg<
4 QP
vxwzy{:|*}~sw rfi }~ d> w ~ ^H


Tfi 5[* @ 5bde>>;g=hE
K44<'a.g

}> w r |*}~^w r'>oUp7

{ r^>o w r_ {D} { }>yw ~

a.I@P7H@*QTTT
Tibde>e>>gK XT
l4TX@gQEH3y } r D{ { p w ~>
}dv {7E>v n_o vx{ _oU vxw} oX j}>w v } { { {} oI vxw r w n_o vx{w6~s{ oXr {= n <nt
>>h>>
E>


'.
8%
>



fi[>^c>XUH

fi; shX@d>>>^_=^^;^[=^ E sh_>D;xz>dDD>*xx*x>
>c;>>



ff
fi^>[ ^> >
_>.[^ c *d>>*fi^E s7 ;^?^ ^ E^h^_U>*dfiDx
!>U_Ux6sXD#"uD>dff$s&%>('*)>*^' >
^[fi
3 E 4 ^5 ^:[ >^s= =s^6E sh
+ ,E
>x.-fiZd>>;0/1^E2 fifi
< E=^ >>?^ x>>xzA?
@ >xd>xz;X.B%>x fi*_Csd
dD_67>>;d78$s:9&;E
>xu:zAE>8$^>xXh<;c;>Fd [fi>3 2 G%B [U3
H IFK>J fiL: NMOPJ > ;
xh d>^' %_K7^ x8Q *sfidsd>>SRUhT Dffs$ U=VIs u?>K70W<c< _X
xz>R.
[MZ *4, s^*
- Ffi
\
/ cd>) >&] >1c< xXu_[T^^$ _>d ;N=
7 : >1<_s xdsd>>1` zK>7 z;aT^s$ b
H >Ffi
>dfi
@ D=Q *c %d>8Q d?Ee z>*fh =
; ^ N ^> 4=
.\ E g * ^ RSh
ff>
H *fiXj fik Xd;>>l*` K>7 >x>d*^zsd>z;x^s^' Fd [fi>3 2 G%B [X3
h [h

%En^ ;6>
x H >>>u> ^ X>^ [4 fi
^ a^ * ;. ;^I[=^ E
[FE
, ;
xfUj [.
Fd s^ 7 dD_6>
7 >; d8s$ >%;% 7>
XN^ >pofofo]9&1; 1< E8^ >qj` K>7 >x>Id
>1<_^ x?h9 U_shXXE' >) % DF4X3

tMuY >&k @ >>>u#:
v Xw u7 s[ 6V^ [zx>>K7:
E X
*d rU
+ [&ksd sx*h[ + i> &Uj :
ffs
$ XjD>DX>FR?T^s$ D>6;x.
E 8s$ IE7 :>K7dy<h< D>xz;<E fd H s^sh
fi3>3 sh>hX >>>u ^X^; >3 Fd 3fi >=[fiX3 fi3;3 sh>hXC%Y *6>DN7^ x8Q M*s
d^;>^*%
B ; U>ffi
sc9 _hc>>E =f
?- E H *d>^' ^X^; fi>3 :Fd 3* ;=[fi3N{|/1^ *4=
>^7 ^ ^
fi3>3 sh^h[*Mzfi
^ DX>* fijsd^;>^~}E N>) >1S>) >

\ %d>;>fY
>fi[^X^ >3 =Fd 3fi > fi3>3 shhh
fi3>3 shhh[NMX fiX3 &fi
C%
*muXDN7^ x8Q *^zd^;>^*%B ; h>X
c9 Dhh>>>;
fd
\ .=
d>s' ^gz ._ _F E X?
6c ^ ;^ skd ^xh
fi3>3 shh[.M %X>3 >ffi
C%
*mu>_D6>7 z;>%d 8s$ 8
$ _UxDX>x>X;t7
>p 6s$ 6I
< >_N7^ x8Q M*^zdsd>>
fi I> %B >xX;>?
yE@c9 Cfi>7 Xh*I) DcX'0h EaMZ s^6h ^hc n Ix F

^:
E> _fiX3
^ =Er=
L ^ fi

/ >>>u_^FH
X>3 ^ E DE sH[ >3 F 3* > ^ =>hy=
CY *m
dD_6>
7 >;d:8s$ U>% 472o^ d6< D>7
>iE z;dh9 D>> *_>DX> o0PX9 ^6 ^hhD
_>.[^ ><d;>>UUd> [>x>: >1 h = ^>^s

>

LMF^fi^d\^fi3Xhh Z[*M ?k;F-fi*d>)>t/^fi_^D=^ s' ^* ['[^X^>3>
Fd 3* ;=[fi3N{th
E>^lE [E 2fi-ME H cCY*m#7Q;XDz'DN7^x8QM*s
d^;>^hU)
;fi
) fi X>7 G f


fiNNpsFFF8ffF=FNFFFF*p>fifirF=bF=FN
.6r&*dSbfiiy*2rfiNfiffi#fiii*&lb*8N1 NCI=g*K I*FI_(
.6r&FddN4IFbS*C2C8#I8*X4 KfiC8# bnIIN6Ff&d fiffpISFr*FF4fi
fi4p4
.6r&tdKd.4FIFffGF4 fi?fip[*rfiNfiffidfipXi*&dfffi=K2 NCI=g*IN6
tFn_N
firfir&.N^?r|24fiG*fiff^?tf4ffUNCUFpi4(*4d
yiXffFC.^
C.tb 46 68W8* 8
y*4 F
ffff4K #Ab*=Kff
6N8
fi.


.CNU
IC*4 ?Cb[=:?6C y8K NCI=g*IN6*F&KISF FF4rfi .4rN
yffFfiFNf?4ff!
^ifi *pdi*Fifiii2dfis fiaFiX:p ififf F4PXi*2
*p0firIffffi=K[6NC8fiK INt _F
y*fiff Cf?bd# "fiifii*fi2r4i*sXW4ffi fipfiFfiff p: fiff*4ff*dfiffi*&
G fiffi*&$ &Ctff.6NfiC8%

~p*8
y4i*4228*& .Cf( ':)
f* ,+_I 8ffff.F&
IS(#F204ffi
FFfir4NF1. -2t4/ '(ff&GfiNfiffiU C4fiff?^ fiff*4ff*ffptyffinP4Ffi4?F( -f
fi4p4
>F
0lN.^.?^Nr|2
11F*fifi4fifiK3 & 5 4U4X*ffa6 "*iififfafi fi*fi7 "fiiififf:
ff*dfiffi?4 8_fi2*Fpi*4I_
f*NfiNr*z??ffF 0.tCFff yI 66=
48*
9ff8:
fi; S6
fbnIIN6b*8Nff
K=<
fi=ff.!

&>@?*X . fi/ '?&
IC*4
Cb[=P?6C y8& KC8g*IN6F&NISF. FF4rfi .4rN

ACBCA

fiJournal Artificial Intelligence Research 4 (1996) 1-18

Submitted 9/95; published 1/96

Design Experimental Analysis Algorithms
Temporal Reasoning
Peter van Beek
Dennis W. Manchak

Department Computing Science, University Alberta
Edmonton, Alberta, Canada T6G 2H1

vanbeek@cs.ualberta.ca
dmanchak@vnet.ibm.com

Abstract

Many applications|from planning scheduling problems molecular biology|
rely heavily temporal reasoning component. paper, discuss design
empirical analysis algorithms temporal reasoning system based Allen's uential
interval-based framework representing temporal information. core system
algorithms determining whether temporal information consistent, and, so,
finding one scenarios consistent temporal information. Two
important algorithms tasks path consistency algorithm backtracking
algorithm. path consistency algorithm, develop techniques result
ten-fold speedup already highly optimized implementation.
backtracking algorithm, develop variable value ordering heuristics shown
empirically dramatically improve performance algorithm. well, show
previously suggested reformulation backtracking search problem reduce
time space requirements backtracking search. Taken together, techniques
develop allow temporal reasoning component solve problems practical
size.

1. Introduction
Temporal reasoning essential part many artificial intelligence tasks. desirable,
therefore, develop temporal reasoning component useful across applications.
applications, planning scheduling, rely heavily temporal reasoning component success application depend eciency
underlying temporal reasoning component. paper, discuss design empirical analysis two algorithms temporal reasoning system based Allen's (1983)
uential interval-based framework representing temporal information. two algorithms, path consistency algorithm backtracking algorithm, important two
fundamental tasks: determining whether temporal information consistent, and, so,
finding one scenarios consistent temporal information.
stress designing algorithms robust ecient practice.
path consistency algorithm, develop techniques result ten-fold
speedup already highly optimized implementation. backtracking algorithm,
develop variable value ordering heuristics shown empirically dramatically
improve performance algorithm. well, show previously suggested
reformulation backtracking search problem (van Beek, 1992) reduce time
space requirements backtracking search. Taken together, techniques develop
c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fivan Beek & Manchak

Relation
Symbol Inverse
x
b
bi
x meets

mi
x overlaps

oi
x starts



si

x



di

x finishes

f

fi

x equal

eq

eq

Meaning
x

x

x

x

x

x

x


Figure 1: Basic relations intervals
allow temporal reasoning component solve problems realistic size. part
evidence support claim, evaluate techniques improving algorithms
large problem arises molecular biology.

2. Representing Temporal Information
section, review Allen's (1983) framework representing relations intervals. discuss set problems chosen test algorithms.

2.1 Allen's framework

thirteen basic relations hold two intervals (see Figure 1; Allen,
1983; Bruce, 1972). order represent indefinite information, relation two
intervals allowed disjunction basic relations. Sets used list
disjunctions. example, relation fm,o,sg events B represents
disjunction, (A meets B) _ (A overlaps B) _ (A starts B): Let set basic
relations, fb,bi,m,mi,o,oi,s,si,d,di,f,fi,eqg. Allen allows relation two events
subset .
use graphical notation vertices represent events directed edges
labeled sets basic relations. graphical convention, never show edges
(i; i), show edge (i; j ), show edge (j; i). edge
explicit knowledge relation labeled ; convention edges
also shown. call networks labels arbitrary subsets , interval
algebra IA networks.
Example 1. Allen Koomen (1983) show IA networks used non-linear
planning concurrent actions. example representing temporal information using
IA networks, consider following blocks-world planning problem. three blocks,
A, B, C. initial state, three blocks table. goal state
2

fiAlgorithms Temporal Reasoning

simply tower blocks B B C. associate states, actions,
properties intervals hold over, immediately write
following temporal information.
Initial Conditions
Goal Conditions
Initial fdg Clear(A)
Goal fdg On(A,B)
Initial fdg Clear(B)
Goal fdg On(B,C)
Initial fdg Clear(C)
action called \Stack". effect stack action On(x; ): block x
top block . action successfully executed, conditions Clear(x)
Clear(y ) must hold: neither block x block block them. Planning introduces
two stacking actions following temporal constraints.
Stacking Action
Stacking Action
Stack(A,B) fbi,mig Initial
Stack(B,C) fbi,mig Initial
Stack(A,B) fdg Clear(A)
Stack(B,C) fdg Clear(B)
Stack(A,B) ffg Clear(B)
Stack(B,C) ffg Clear(C)
Stack(A,B) fmg On(A,B)
Stack(B,C) fmg On(B,C)
graphical representation IA network planning problem shown
Figure 2a. Two fundamental tasks determining whether temporal information
consistent, and, so, finding one scenarios consistent temporal
information. IA network consistent exists mapping
real interval (u) event vertex u network relations events satisfied (i.e., one disjuncts satisfied). example, consider
small subnetwork Figure 2a consisting events On(A,B), On(B,C), Goal.
subnetwork consistent demonstrated assignment, (On(A,B)) = [1; 5],
(On(B,C)) = [2; 5], (Goal) = [3; 4]. change subnetwork insist
On(A,B) must On(B,C), mapping would exist subnetwork
would inconsistent. consistent scenario IA network non-disjunctive subnetwork (i.e., every edge labeled single basic relation) consistent.
planning example, finding consistent scenario network corresponds finding
ordering actions accomplish goal stacking three blocks. One
consistent scenario reconstructed qualitative mapping shown Figure 2b.
Example 2. Golumbic Shamir (1993) discuss IA networks used
problem molecular biology: examining structure DNA organism (Benzer, 1959). intervals IA network represent segments DNA. Experiments
performed determine whether pair segments either disjoint intersects.
Thus, IA networks result contain edges labeled disjoint (fb,big), intersects
(fm,mi,o,oi,s,si,d,di,f,fi,eqg), , set basic relations|which indicates experiment performed. IA network consistent, evidence hypothesis
DNA linear structure; inconsistent, DNA nonlinear (it forms loops,
example). Golumbic Shamir (1993) show determining consistency restricted version IA networks NP-complete. show problems arise
application often solved quickly practice.
3

fivan Beek & Manchak










@






@@

=





@R

1







PP -

HYHHPHPPPPPq J]
JJ ,
HH
HHH
JJ ,,
Z}
ZZJ ,
HHH Z
HH


(a) IA network block-stacking example:

fbi,mig
fdg

fdg

1
Initial

fdg

fbi,mig

fdg

2
Clear(A)
3
Clear(B)
4
Clear(C)

5
Stack(A,B)
ffg

fdg

ffg

fmg

7
On(A,B)
8
On(B,C)

fmg

PiP
)PP

fdg
fdg

9
Goal

6
Stack(B,C)

(b) Consistent scenario:
Initial

Stack(B,C)
Goal
Stack(A,B)
Clear(C)
On(B,C)
Clear(B)
On(A,B)
Clear(A)

Figure 2: Representing qualitative relations intervals

2.2 Test problems
tested well heuristics developed improving path consistency backtracking algorithms perform test suite problems.
purpose empirically testing algorithms determine performance
algorithms proposed improvements \typical" problems. two
approaches: (i) collect set \benchmark" problems representative problems
arise practice, (ii) randomly generate problems \investigate algorithmic
performance depends problem characteristics ... learn predict algorithm
perform given problem class" (Hooker, 1994).
IA networks, existing collection large benchmark problems actually
arise practice|as opposed to, example, planning toy domain blocks
world. start collection, propose IA network 145 intervals arose
problem molecular biology (Benzer, 1959, pp. 1614-15; see Example 2, above).
proposed benchmark problem strictly speaking temporal reasoning problem
4

fiAlgorithms Temporal Reasoning

intervals represent segments DNA, intervals time. Nevertheless,
formulated temporal reasoning problem. value benchmark problem
arose real application. refer problem Benzer's matrix.
addition benchmark problem, paper use two models random IA
network, denoted B(n) S(n; p), evaluate performance algorithms, n
number intervals, p probability (non-trivial) constraint two
intervals. Model B(n) intended model problems arise molecular biology (as
estimated problem discussed Benzer, 1959). Model S(n; p) allows us study
algorithm performance depends important problem characteristic sparseness
underlying constraint graph. models, course, allow us study algorithm
performance depends size problem.
B(n), random instances generated follows.
Step 1. Generate \solution" size n follows. Generate n real intervals randomly
generating values end points intervals. Determine IA network determining, pair intervals, whether two intervals either intersect disjoint.
Step 2. Change constraints edges trivial constraint setting
label , set 13 basic relations. represents case experiment
performed determine whether pair DNA segments intersect disjoint.
Constraints changed percentage non-trivial constraints (approximately
6% intersects 17% disjoint) distribution graph similar
Benzer's matrix.
S(n; p), random instances generated follows.
Step 1. Generate underlying constraint graph indicating possible (n2)
edges present. Let edge present probability p, independently presence
absence edges.
Step 2. edge occurs underlying constraint graph, randomly chose label
edge set possible labels (excluding empty label) label
chosen equal probability. edge occur, label edge , set
13 basic relations.
Step 3. Generate \solution" size n follows. Generate n real intervals randomly
generating values end points intervals. Determine consistent scenario
determining basic relations satisfied intervals. Finally, add solution
IA network generated Steps 1{2.
Hence, consistent IA networks generated S(n; p). omit Step 3,
shown analytically empirically almost different possible
IA networks generated distribution inconsistent inconsistency
easily detected path consistency algorithm. avoid potential pitfall, test
algorithms consistent instances problem. method appears generate
reasonable test set temporal reasoning algorithms problems range easy
hard. found, example, instances drawn S(n; 1=4) hard problems
backtracking algorithms solve, whereas values p either side (S(n; 1=2)
S(n; 1=8)) problems easier.
5

fivan Beek & Manchak

3. Path Consistency Algorithm
Path consistency transitive closure algorithms (Aho, Hopcroft, & Ullman, 1974; Mackworth, 1977; Montanari, 1974) important temporal reasoning. Allen (1983) shows
path consistency algorithm used heuristic test whether IA network
consistent (sometimes algorithm report information consistent
really not). path consistency algorithm useful also backtracking search
consistent scenario used preprocessing algorithm (Mackworth, 1977;
Ladkin & Reinefeld, 1992) algorithm interleaved backtracking search (see next section; Nadel, 1989; Ladkin & Reinefeld, 1992). section,
examine methods speeding path consistency algorithm.
idea behind path consistency algorithm following. Choose three
vertices i, j , k network. labels edges (i; j ) (j; k) potentially
constrain label edge (i; k) completes triangle. example, consider
three vertices Stack(A,B), On(A,B), Goal Figure 2a. Stack(A,B) fmg
On(A,B) On(A,B) fdig Goal deduce Stack(A,B) fbg Goal therefore
change label edge , set basic relations, singleton
set fbg. perform deduction, algorithm uses operations set intersection
(\) composition () labels checks whether C = C \ C C , C
label edge (i; k). C updated, may constrain labels, (i; k)
added list processed turn, provided edge already list.
algorithm iterates changes possible. unary operation, inverse,
also used algorithm. inverse label inverse elements
(see Figure 1 inverses basic relations).
designed experimentally evaluated techniques improving eciency
path consistency algorithm. starting point variation Allen's (1983) algorithm
shown Figure 3. implementation algorithm ecient, intersection
composition operations labels must ecient (Steps 5 & 10). Intersection
made ecient implementing labels bit vectors. intersection two labels
simply logical two integers. Composition harder make ecient.
Unfortunately, impractical implement composition two labels using table
lookup table would need size 213 213, 213 possible labels.
experimentally compared two practical methods composition
proposed literature. Allen (1983) gives method composition uses table
size 13 13. table gives composition basic relations (see Allen, 1983,
table). composition two labels computed nested loop forms
union pairwise composition basic relations labels. Hogge (1987) gives
method composition uses four tables size 27 27, 27 26, 26 27, 26 26.
composition two labels computed taking union results four array
references (H. Kautz independently devised similar scheme). experiments,
implementations two methods differed composition computed.
both, list, L, edges processed implemented using first-in, first-out policy
(i.e., stack).
also experimentally evaluated methods reducing number composition operations need performed. One idea examined improving eciency
ik

ik

6

ik

ij

jk

ik

fiAlgorithms Temporal Reasoning

(C; n)

Path-Consistency

1. L f(i; j ) j 1 < j ng
2. (L empty)
3. select delete (i; j ) L
4.
k 1 n, k 6= k 6= j
5.
C \ C C
6.
(t 6= C )
7.
C
8.
C
Inverse(t)
9.
L
L [ f(i; k )g
10.

C
\C
C
11.
(t 6= C )
12.
C
13.
C
Inverse(t)
14.
L
L [ f(k; j )g
ik

ij

jk

ki

ij

ik

ik

ki

kj

kj

kj

jk

Figure 3: Path consistency algorithm IA networks
avoid computation predicted result constrain
label edge completes triangle. Three cases identified shown
Figure 4. Another idea examined, first suggested Mackworth (1977, p. 113),
order edges processed affect eciency algorithm.
reason following. edge appear list, L, edges processed
many times progressively gets constrained. number times particular edge
appears list reduced good ordering. example, consider edges
(3; 1) (3; 5) Figure 2a. process edge (3; 1) first, edge (3; 2) updated
fo,oi,s,si,d,di,f,fi,eqg added L (k = 2 Steps 5{9). process edge
(3; 5), edge (3; 2) updated fo,s,dg added L second time. However,
process edge (3; 5) first, (3; 2) immediately updated fo,s,dg
added L once.
Three heuristics devised ordering edges shown Figure 9. edges
assigned heuristic value processed ascending order. new edge
added list (Steps 9 & 14), edge inserted appropriate spot according
new heuristic value. little work ordering heuristics path consistency
algorithms. Wallace Freuder (1992) discuss ordering heuristics arc consistency
algorithms, closely related path consistency algorithms. Two heuristics
cannot applied context heuristics assume constraint satisfaction problem
finite domains, whereas IA networks examples constraint satisfaction problems
infinite domains. third heuristic (due B. Nudel, 1983) closely corresponds
cardinality heuristic.
experiments performed Sun 4/25 12 megabytes memory.
report timings rather measure number iterations believe
gives accurate picture whether results practical interest. Care
7

fivan Beek & Manchak

computation, C \ C C , skipped known result
composition constrain label edge (i; k):
a. either C C equal , result composition therefore
constrain label edge (i; k). Thus, Step 1 Figure 3, edges
labeled added list edges process.
b. condition,
ik

ij

ij

jk

jk

(b 2 C

ij ^

bi 2 C ) _ (bi 2 C

ij ^

jk

b 2 C ) _ (d 2 C
jk

ij ^

di 2 C );
jk

true, result composing C C . condition quickly tested
using bit operations. Thus, condition true Step 5, Steps 5{9
skipped. similar condition formulated tested Step 10.
c. point computation C C determined result
accumulated far would constrain label C , rest computation
skipped.
ij

jk

ij

jk

ik

Figure 4: Skipping techniques
taken always start base implementation algorithm add
enough code implement composition method, new technique, heuristic
evaluating. well, every attempt made implement method heuristic
eciently could.
Given implementations, Hogge's method composition found
ecient Allen's method benchmark problem random instances
(see Figures 5{8). much surprising. However, addition skipping
techniques, two methods became close eciency. skipping techniques sometimes
dramatically improved eciency methods. ordering heuristics improve
eciency, although results less dramatic. cardinality heuristic
constraintedness heuristic also tried ordering edges. found
cardinality heuristic costly compute weight heuristic
perform it. constraintedness heuristic reduced number iterations proved
costly compute. illustrates balance must struck effectiveness
heuristic additional overhead heuristic introduces.
S(n; p), skipping techniques weight ordering heuristic together result
ten-fold speedup already highly optimized implementation using Hogge's
method composition. largest improvements eciency occur IA networks
sparse (p smaller). encouraging appears problems arise
planning molecular biology also sparse. B(n) Benzer's matrix, speedup
approximately four-fold. Perhaps importantly, execution times reported indicate
path consistency algorithm, even though O(n3 ) algorithm, used
practical-sized problems. Figure 8, show well algorithms scale up.
8

fiAlgorithms Temporal Reasoning

Allen

137.7

Hogge

10.3

Allen+skip

5.7

Hogge+skip

4.0

Hogge+skip+weight

2.7

Figure 5: Effect heuristics time (sec.) path consistency algorithms applied
Benzer's matrix

time (sec.)

100

10

1

Allen
Hogge
Allen+skip
Hogge+skip
Hogge+skip+weight

0.1
50

75

100
n

125

150

Figure 6: Effect heuristics average time (sec.) path consistency algorithms.
data point average 100 tests random instances IA networks drawn
B(n); coecient variation (standard deviation / average) set
100 tests bounded 0.20
seen algorithm includes weight ordering heuristic performs others.
However, algorithm requires much space largest problem able solve
500 intervals. algorithms included skipping techniques
able solve much larger problems running space (up 1500 intervals)
constraint time took solve problems.

9

fivan Beek & Manchak

100

time (sec.)

10

1

Allen
Hogge
Allen+skip
Hogge+skip
Hogge+skip+weight

0.1
1/8

1/4

1/2
p

3/4

1

Figure 7: Effect heuristics average time (sec.) path consistency algorithms.
data point average 100 tests random instances IA networks drawn
S(100; p); coecient variation (standard deviation / average)
set 100 tests bounded 0.25
9000
8000
S(n,1/4):

Allen+skip
Hogge+skip
Hogge+skip+weight

7000

time (sec.)

6000

B(n):

Allen+skip
Hogge+skip
Hogge+skip+weight

5000
4000
3000
2000
1000
0
100

200

300

400

500

600
n

700

800

900

1000

Figure 8: Effect heuristics average time (sec.) path consistency algorithms.
data point average 10 tests random instances IA networks drawn
S(n; 1=4) B(n); coecient variation (standard deviation / average) set 10 tests bounded 0.35
10

fiAlgorithms Temporal Reasoning

4. Backtracking Algorithm

Allen (1983) first propose backtracking algorithm (Golomb & Baumert,
1965) could used find consistent scenario IA network. worst case,
backtracking algorithm take exponential amount time complete. worst
case also applies Vilain Kautz (1986, 1989) show finding consistent
scenario NP-complete IA networks. spite worst case estimate, backtracking
algorithms work well practice. section, examine methods speeding
backtracking algorithm finding consistent scenario present results well
algorithm performs different classes problems. particular, compare eciency
algorithm two alternative formulations problem: one previously
proposed others one proposed (van Beek, 1992). also improve
eciency algorithm designing heuristics ordering instantiation
variables ordering values domains variables.
starting point, modeled backtracking algorithm Ladkin
Reinefeld (1992) results experimentation suggests successful
finding consistent scenarios quickly. Following Ladkin Reinefeld algorithm
following characteristics: preprocessing using path consistency algorithm, static order
instantiation variables, chronological backtracking, forward checking pruning
using path consistency algorithm. chronological backtracking, search reaches
dead end, search simply backs next recently instantiated variable
tries different instantiation. Forward checking (Haralick & Elliott, 1980) technique
determined recorded instantiation current variable restricts
possible instantiations future variables. technique viewed hybrid
tree search consistency algorithms (see Nadel, 1989; Nudel, 1983). (See Dechter, 1992,
general survey backtracking.)

4.1 Alternative formulations

Let C matrix representation IA network, C label edge (i; j ).
traditional method finding consistent scenario IA network search
subnetwork network C that,
(a) C ,
(b) jS j = 1, i; j ,
(c) consistent.
find consistent scenario simply search different possible 's satisfy
conditions (a) (b)|it simple matter enumerate them|until find one
also satisfies condition (c). Allen (1983) first propose using backtracking search
search potential 's.
alternative formulation based results two restricted classes IA networks,
denoted SA networks NB networks. IA networks, relation two
intervals subset , set thirteen basic relations. SA networks
(Vilain & Kautz, 1986), allowed relations two intervals subsets
translated, using relations f<, , =, >, , 6=g, conjunctions
ij

ij

ij

ij

11

fivan Beek & Manchak

relations endpoints intervals. example, IA network Figure 2a
also SA network. specific example, interval relation \A fbi,mig B"
expressed conjunction point relations, (B, < B+ ) ^ (A, < A+ ) ^ (A, B+ );
A, A+ represent start end points interval A, respectively. (See Ladkin
& Maddux, 1988; van Beek & Cohen, 1990, enumeration allowed relations
SA networks.) NB networks (Nebel & Burckert, 1995), allowed relations
two intervals subsets translated, using relations f<,
, =, >, , 6=g, conjunctions Horn clauses express relations
endpoints intervals. set NB relations strict superset SA relations.
alternative formulation follows. describe method terms SA
networks, method applies NB networks. idea that, rather
search directly consistent scenario IA network previous work, first
search something general: consistent SA subnetwork IA network.
is, use backtrack search find subnetwork network C that,
(a)

Sij Cij

(b)

Sij

(c)



,

allowed relation SA networks, i; j ,

consistent.

previous work, search alternative singleton labelings edge, i.e.,
jS j = 1. key idea proposal decompose labels largest
possible sets basic relations allowed SA networks search
decompositions. considerably reduce size search space. example,
suppose label edge fb,bi,m,o,oi,sig. six possible ways label
edge singleton label: fbg, fbig, fmg, fog, foig, fsig, two possible ways
label edge decompose labels largest possible sets basic relations
allowed SA networks: fb,m,og fbi,oi,sig. another example, consider
network shown Figure 2a. searching alternative singleton labelings,
worst case size search space C12 C13 C89 = 314 (the edges labeled
must included calculation). decomposing labels largest
possible sets basic relations allowed SA networks searching
decompositions, size search space 1, backtracking necessary (in general,
search is, course, always backtrack free).
test whether instantiation variable consistent instantiations past
variables possible instantiations future variables, use incremental path
consistency algorithm (in Step 1 Figure 3 instead initializing L edges,
initialized single edge changed). result backtracking algorithm
consistent SA subnetwork IA network, report IA network inconsistent.
backtracking completes, solution SA network found using fast
algorithm given van Beek (1992).
ij

4.2 Ordering heuristics

Backtracking proceeds progressively instantiating variables. consistent instantiation
exists current variable, search backs up. order variables
12

fiAlgorithms Temporal Reasoning

Weight. weight heuristic estimate much label edge restrict
labels edges. Restrictiveness measured basic relation successively composing basic relation every possible label summing cardinalities
results. results suitably scaled give table shown below.
relation b bi mi oi si di f fi eq
weight 3 3 2 2 4 4 2 2 4 3 2 2 1
weight label sum weights elements. example, weight
relation fm,o,sg 2 + 4 + 2 = 8.
Cardinality. cardinality heuristic variation weight heuristic. Here,
weight every basic relation set one.
Constraint. constraintedness heuristic estimate much change label
edge restrict labels edges. determined follows. Suppose
edge interested (i; j ). constraintedness label edge (i; j )
sum weights labels edges (k; i) (j; k), k = 1; :::; n; k 6= i; k 6= j .
intuition comes examining path consistency algorithm (Figure 3) would
propagate change label C . see C composed C (Step 5)
C (Step 10), k = 1; :::; n; k 6= i; k 6= j .
ij

ij

ki

jk

Figure 9: Ordering heuristics
instantiated order values domains tried possible
instantiations greatly affect performance backtracking algorithm various
methods ordering variables (e.g. Bitner & Reingold, 1975; Freuder, 1982; Nudel,
1983) ordering values (e.g. Dechter & Pearl, 1988; Ginsberg et al., 1990; Haralick
& Elliott, 1980) proposed.
idea behind variable ordering heuristics instantiate variables first
constrain instantiation variables most. is, backtracking search
attempts solve highly constrained part network first. Three heuristics
devised ordering variables (edges IA network) shown Figure 9.
alternative formulation, cardinality redefined count decompositions rather
elements label. variables put ascending order. experiments
ordering static|it determined backtracking search starts
change search progresses. context, cardinality heuristic similar
heuristic proposed Bitner Reingold (1975) studied Purdom (1983).
idea behind value ordering heuristics order values domains
variables values likely lead solution tried first. Generally,
done putting values first constrain choices variables least.
propose novel technique value ordering based knowledge structure
solutions. idea first choose small set problems class problems,
find consistent scenario instance without using value ordering.
set solutions, examine solutions determine values domains
13

fivan Beek & Manchak

120

100

SI

SA

time (sec.)

80

60

40

20

0
50

100

150
n

200

250

Figure 10: Effect decomposition method average time (sec.) backtracking algorithm. data point average 100 tests random instances IA
networks drawn B(n); coecient variation (standard deviation /
average) set 100 tests bounded 0.15
likely appear solution values least likely. information
used order values subsequent searches solutions problems
class problems. example, five problems generated using model S(100; 1=4)
consistent scenarios found using backtracking search variable ordering
heuristic constraintedness/weight/cardinality. rounding two significant digits,
relations occurred solutions following frequency,
relation
b, bi d, di o, oi
value (10) 1900 240 220

eq
53

m, mi f, fi s, si
20
15 14

example using information order values domain, suppose
label edge fb,bi,m,o,oi,sig. decomposing labels singleton labels,
would order values domain follows (most preferred first): fbg, fbig, fog,
foig, fmg, fsig. decomposing labels largest possible sets basic
relations allowed SA networks, would order values domain
follows: fb,m,og, fbi,oi,sig, since 1900 + 20 + 220 > 1900 + 220 + 14. technique
used whenever something known structure solutions.

4.3 Experiments

experiments performed Sun 4/20 8 megabytes memory.
first set experiments, summarized Figure 10, examined effect problem
formulation execution time backtracking algorithm. implemented three
14

fiAlgorithms Temporal Reasoning

10000
Random value ordering, Random
Heuristic value ordering, random
Random value ordering, best heuristic
Heuristic value ordering, best heuristic

variable
variable
variable
variable

ordering
ordering
ordering
ordering

time

1000

100

10
0

10

20

30

40

50
test

60

70

80

90

100

Figure 11: Effect variable value ordering heuristics time (sec.) backtracking
algorithm. curve represents 100 tests random instances IA networks
drawn S(100; 1=4) tests ordered time taken solve
instance. backtracking algorithm used SA decomposition method.
versions algorithm identical except one searched singleton
labelings (denoted hereafter Figure 10 SI method) two searched
decompositions labels largest possible allowed relations SA networks NB networks, respectively. methods solved set random
problems drawn B(n) also applied Benzer's matrix (denoted +
Figure 10). problem, amount time required solve given IA network recorded. mentioned earlier, IA network preprocessed path
consistency algorithm backtracking search. timings include preprocessing
time. experiments indicate speedup using SA decomposition method
three-fold SI method. well, SA decomposition method
able solve larger problems running space (n = 250 versus n = 175).
NB decomposition method gives exactly result SA method
problems structure constraints. also tested three methods
set random problems drawn S(100; p), p = 1; 3=4; 1=2, 1=8.
experiments, SA NB methods consistently twice fast SI method.
well, NB method showed advantage SA method problems.
surprising branching factor, hence size search space, smaller
NB method SA method.
second set experiments, summarized Figure 11, examined effect
execution time backtracking algorithm heuristically ordering variables
values domains variables backtracking search begins. variable
ordering, six permutations cardinality, constraint, weight heuristics tried
15

fivan Beek & Manchak

primary, secondary, tertiary sorting keys, respectively. basis comparison,
experiments included case heuristics. Figure 11 shows approximate cumulative
frequency curves experimental results. Thus, example, read
curve representing heuristic value ordering best heuristic variable ordering
approximately 75% tests completed within 20 seconds, whereas random value
variable ordering approximately 5% tests completed within 20 seconds.
also read curves 0, 10, : : : , 100 percentiles data sets (where
value median 50th percentile value 50th test). curves
truncated time = 1800 (1/2 hour), backtracking search aborted
time limit exceeded.
experiments found S(100; 1=4) represents particularly dicult class
problems different heuristics resulted dramatically different performance, heuristic case also different heuristics.
value ordering, best heuristic variable ordering combination constraintedness/weight/cardinality constraintedness primary sorting key
remaining keys used break subsequent ties. Somewhat surprisingly, best heuristic
variable ordering changes heuristic value ordering incorporated. combination weight/constraintedness/cardinality works much better. heuristic together
value ordering particularly effective \ attening out" distribution allowing much greater number problems solved reasonable amount time.
S(100; p), p = 1; 3=4; 1=2, 1=8, problems much easier three
hundreds tests completed within 20 seconds. problems, heuristic used
result significantly different performance.
summary, experiments indicate changing decomposition method
able solve larger problems running space (n = 250 vs n = 175
machine 8 megabytes; see Figure 10). experiments also indicate good heuristic
orderings essential able find consistent scenario IA network
reasonable time. good heuristic ordering able solve much larger problems
running time (see Figure 11). experiments also provide additional evidence
ecacy Ladkin Reinefeld's (1992, 1993) algorithm. Nevertheless, even
improvements, problems still took considerable amount time solve.
consideration, surprising. all, problem known NP-complete.

5. Conclusions
Temporal reasoning essential part tasks planning scheduling. paper, discussed design empirical analysis two key algorithms temporal
reasoning system. algorithms path consistency algorithm backtracking algorithm. temporal reasoning system based Allen's (1983) interval-based framework
representing temporal information. emphasis make algorithms
robust ecient practice problems vary easy hard. path consistency algorithm, bottleneck performing composition operation. developed
methods reducing number composition operations need performed.
methods result almost order magnitude speedup already highly
optimized implementation algorithm. backtracking algorithm, developed
16

fiAlgorithms Temporal Reasoning

variable value ordering heuristics showed alternative formulation
problem considerably reduce time taken find solution. techniques allow
interval-based temporal reasoning system applied larger problems perform
eciently existing applications.

References

Aho, A. V., Hopcroft, J. E., & Ullman, J. D. (1974). Design Analysis Computer
Algorithms. Addison-Wesley.
Allen, J. F. (1983). Maintaining knowledge temporal intervals. Comm. ACM, 26,
832{843.
Allen, J. F., & Koomen, J. A. (1983). Planning using temporal world model. Proceedings
Eighth International Joint Conference Artificial Intelligence, pp. 741{747
Karlsruhe, West Germany.
Benzer, S. (1959). topology genetic fine structure. Proc. Nat. Acad. Sci. USA,
45, 1607{1620.
Bitner, J. R., & Reingold, E. M. (1975). Backtrack programming techniques. Comm. ACM,
18, 651{655.
Bruce, B. C. (1972). model temporal references application question
answering program. Artificial Intelligence, 3, 1{25.
Dechter, R. (1992). local global consistency. Artificial Intelligence, 55, 87{107.
Dechter, R., & Pearl, J. (1988). Network-based heuristics constraint satisfaction problems. Artificial Intelligence, 34, 1{38.
Freuder, E. C. (1982). sucient condition backtrack-free search. J. ACM, 29, 24{32.
Ginsberg, M. L., Frank, M., Halpin, M. P., & Torrance, M. C. (1990). Search lessons learned
crossword puzzles. Proceedings Eighth National Conference Artificial
Intelligence, pp. 210{215 Boston, Mass.
Golomb, S., & Baumert, L. (1965). Backtrack programming. J. ACM, 12, 516{524.
Golumbic, M. C., & Shamir, R. (1993). Complexity algorithms reasoning
time: graph-theoretic approach. J. ACM, 40, 1108{1133.
Haralick, R. M., & Elliott, G. L. (1980). Increasing tree search eciency constraint
satisfaction problems. Artificial Intelligence, 14, 263{313.
Hogge, J. C. (1987). TPLAN: temporal interval-based planner novel extensions. Department computer science technical report UIUCDCS-R-87, University Illinois.
Hooker, J. N. (1994). Needed: empirical science algorithms. Operations Research,
42, 201{212.
17

fivan Beek & Manchak

Ladkin, P., & Reinefeld, A. (1992). Effective solution qualitative interval constraint
problems. Artificial Intelligence, 57, 105{124.
Ladkin, P., & Reinefeld, A. (1993). symbolic approach interval constraint problems.
Calmet, J., & Campbell, J. (Eds.), Artificial Intelligence Symbolic Mathematical
Computing, Springer Lecture Notes Computer Science 737. Springer-Verlag.
Ladkin, P. B., & Maddux, R. D. (1988). binary constraint networks. Technical report,
Kestrel Institute, Palo Alto, Calif.
Mackworth, A. K. (1977). Consistency networks relations. Artificial Intelligence, 8,
99{118.
Montanari, U. (1974). Networks constraints: Fundamental properties applications
picture processing. Inform. Sci., 7, 95{132.
Nadel, B. A. (1989). Constraint satisfaction algorithms. Computational Intelligence, 5,
188{224.
Nebel, B., & Burckert, H.-J. (1995). Reasoning temporal relations: maximal
tractable subclass Allen's interval algebra. J. ACM, 42, 43{66.
Nudel, B. (1983). Consistent-labeling problems algorithms: Expected-complexities
theory-based heuristics. Artificial Intelligence, 21, 135{178.
Purdom, Jr., P. W. (1983). Search rearrangement backtracking polynomial average
time. Artificial Intelligence, 21, 117{133.
van Beek, P. (1992). Reasoning qualitative temporal information. Artificial Intelligence, 58, 297{326.
van Beek, P., & Cohen, R. (1990). Exact approximate reasoning temporal
relations. Computational Intelligence, 6, 132{144.
Vilain, M., & Kautz, H. (1986). Constraint propagation algorithms temporal reasoning.
Proceedings Fifth National Conference Artificial Intelligence, pp. 377{382
Philadelphia, Pa.
Vilain, M., Kautz, H., & van Beek, P. (1989). Constraint propagation algorithms
temporal reasoning: revised report. Weld, D. S., & de Kleer, J. (Eds.), Readings
Qualitative Reasoning Physical Systems, pp. 373{381. Morgan Kaufmann.
Wallace, R. J., & Freuder, E. C. (1992). Ordering heuristics arc consistency algorithms.
Proceedings Ninth Canadian Conference Artificial Intelligence, pp. 163{
169 Vancouver, B.C.

18

fiJournal Artificial Intelligence Research 4 (1996) 419-443

Submitted 2/96; published 6 /96

Principled Approach Towards Symbolic
Geometric Constraint Satisfaction
Sanjay Bhansali

BHANSALI@EECS.WSU.EDU

School EECS, Washington State University
Pullman, WA 99164-2752

Glenn A. Kramer

GAK@EIT.COM

Enterprise Integration Technologies, 800 El Camino Real
Menlo Park, CA 94025

Tim J. Hoar

TIMHOAR@MICROSOFT.COM

Microsoft Corporation
One Microsoft Way, 2/2069
Redmond, WA 98052

Abstract
important problem geometric reasoning find configuration collection
geometric bodies satisfy set given constraints. Recently, suggested
problem solved efficiently symbolically reasoning geometry. approach, called
degrees freedom analysis, employs set specialized routines called plan fragments
specify change configuration set bodies satisfy new constraint
preserving existing constraints. potential drawback, limits scalability approach,
concerned difficulty writing plan fragments. paper address limitation
showing plan fragments automatically synthesized using first principles
geometric bodies, actions, topology.

1. Introduction
important problem geometric reasoning following: given collection geometric
bodies, called geoms, set constraints them, find configuration i.e., position,
orientation, dimension geoms satisfies constraints. Solving problem
integral task many applications like constraint-based sketching design, geometric
modeling computer-aided design, kinematics analysis robots mechanisms
(Hartenberg & Denavit, 1964), describing mechanical assemblies.
General purpose constraint satisfaction techniques well suited solving constraint
problems involving complicated geometry. techniques represent geoms constraints
algebraic equations, whose real solutions yield numerical values describing desired
configuration geoms. equation sets highly non-linear highly coupled
general case require iterative numerical solutions techniques. Iterative numerical techniques
particularly efficient problems stability robustness (Press,
Flannery, Teukolsky & Vetterling, 1986). many tasks (e.g., simulation optimization
mechanical devices) equations solved repeatedly makes compiled solution
desirable. theory, symbolic manipulation equations often yield non-iterative, closed
form solution. found, closed-form solution executed efficiently.
1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiBHANSALI, KRAMER & HOAR

However, computational intractability symbolic algebraic solution equations renders
approach impractical (Kramer, 1992; Liu & Popplestone, 1990).
earlier work Kramer describes system called GCE uses alternative approach
called degrees freedom analysis (1992, 1993). approach based symbolic reasoning
geometry, rather equations, shown efficient systems based
algebraic equation solvers. approach uses two models. symbolic geometric model used
reason symbolically assemble geoms satisfy constraints
incrementally. "assembly plan" thus developed used guide solution complex
nonlinear equations - derived second, numerical model - highly decoupled, stylized
manner.
GCE system used analyze problems domain kinematics shown
perform kinematics simulation complex mechanisms (including Stirling engine,
elevator door mechanism, sofa-bed mechanism) much efficiently pure numerical
solvers (Kramer, 1992). GCE subsequently integrated commercial system
called BravoTM Applicon used drive 2D sketcher (Brown-Associates, 1993).
Several academic systems currently using degrees freedom analysis
applications like assembly modeling (Anantha, Kramer & Crawford, 1992), editing
animating planar linkages (Brunkhart, 1994), feature-based design (Salomons, 1994; Shah &
Rogers, 1993).
GCE employs set specialized routines called plan fragments create assembly plan.
plan fragment specifies change configuration geom using fixed set
operators available degrees freedom, new constraint satisfied
preserving prior constraints geom. assembly plan completed
constraints satisfied degrees freedom reduced zero. approach
canonical: constraints may satisfied order; final status geom terms
remaining degrees freedom (p. 80-81, Kramer, 1992). algorithm finding
assembly procedure time complexity O(cg) c number constraints
g number geoms (p. 139, Kramer, 1992).
Since crux problem-solving taken care plan fragments, success
approach depends ones ability construct complete set plan fragments meeting
canonical specification. number plan fragments needed grows geometrically
number geoms constraints increase. Worse, complexity plan
fragments increases exponentially since various constraints interact subtle ways creating
large number special cases need individually handled. potentially serious
limitation extending degrees freedom approach. paper address problem
showing plan fragments automatically generated using first principles
geoms, actions, topology.
approach based planning. Plan fragment generation reduced planning
problem considering various geoms invariants describing state.
Operators actions, rotate, change configuration geoms, thereby
violating achieving constraint. initial state specified set existing
invariants geom final state additional constraints satisfied. plan
sequence actions applied initial state achieves final state.
formulation, one could presumably use classical planner, STRIPS (Fikes
& Nilsson, 1971), automatically generate plan-fragment. However, operators
domain parametric operators real-valued domain. Thus, search space consists
infinite number states. Even real-valued domain discretized considering realvalued intervals still large search space finding plan satisfies
420

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

specified constraints would intractable problem. approach uses loci information
(representing set points satisfy constraints) reason effects various
operators thus reduces search problem problem topology, involving reasoning
intersection various loci.
issue faced using conventional planner frame problem: determine
properties relationships change result action. typical solution use
assumption: action modify property relationship unless explicitly stated
effect action. approach works well one knows priori possible
constraints invariants might interest relatively constraints get affected
action - true case. use novel scheme representing effects
actions. based reifying (i.e., treating first class objects) actions addition geometric
entities invariant types. associate, pair geom invariants, set actions
used achieve preserve invariant geom. Whenever new geom
invariant type introduced corresponding rules actions achieve/preserve
invariants added. Since many invariant types actions
domain, scheme results simpler rules. Borgida, Mylopoulos & Reiter (1993) propose
similar approach reasoning program specifications. unique feature work
use geometric-specific matching rules determine two general actions
achieve/preserve different constraints reformulated less general action.
Another shortcoming using conventional planner difficulty representing
conditional effects operators. GCE operations effect depends type geom
well particular geometry. example, action translating body intersection
two lines plane would normally reduce bodys translational degrees freedom
zero; however, two lines happen coincide body still retains one degree
translational freedom two lines parallel coincide action fails.
situations called degeneracies. One approach handling degeneracies use
reactive planner dynamically revises plan run-time. However, could result
unacceptable performance many real-time applications. approach makes possible precompile potential degeneracies plan. achieve dividing planning
algorithm two phases. first phase skeletal plan generated works normal
case second phase, skeletal plan refined take care singularities
degeneracies. approach similar idea refining skeletal plans MOLGEN
(Friedland, 1979) idea critics HACKER (Sussman, 1975) fix known bugs
plan. However, skeletal plan refinement MOLGEN essentially consisted instantiating
partial plan work specific conditions, whereas method complete plan works
normal case extended handle special conditions like degeneracies singularities.
1.1 Plan Fragment Example.
use simple example plan fragment specification illustrate approach.
Domains mechanical CAD computer-based sketching rely heavily complex
combinations relatively simple geometric elements, points, lines, circles
small collection constraints coincidence, tangency, parallelism. Figure 1
illustrates fairly complex mechanisms (all implemented GCE) using simple geoms
constraints.

421

fiBHANSALI, KRAMER & HOAR

Automobile suspension

Elevator Doors
Stirling Engine
Figure 1. Modeling complex mechanisms using simple geoms constraints. constraints
needed model joints mechanisms solvable using degrees freedom approach.

example problem illustrated Figure 2 specified follows:
Geom-type: circle
Name: $c
Invariants: (fixed-distance-line $c $L1 $dist1 BIAS_COUNTERCLOCKWISE)
To-be-achieved: (fixed-distance-line $c $L2 $dist2 BIAS_CLOCKWISE)
example, variable-radius circle $c1 prior constraint specifying circle
fixed distance $dist1 left fixed line $L1 (or alternatively, line drawn parallel
$L1 distance $dist1 center $c tangent counterclockwise direction
circle). new constraint satisfied circle fixed distance $dist2
right another fixed line $L2.

$L2
$L2
$c
$c

$dist2

$L1

$dist1
$L1

Figure 2. Example problem (initial state)

1We use following conventions: symbols preceded $ represent constants, symbols preceded ?
represent variables, expressions form (>> parent subpart) denote subpart compound term, parent.

422

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

solve problem, three different plans used: (a) translate circle
current position position touches two lines $L2 $L1 shown figure
(b) scale circle keeping point contact $L1 fixed, touches $L2 (c)
scale translate circle touches $L2 $L1.
action sequences constitute one plan fragment used
situation would available GCE plan-fragment library. Note
plan fragments would applicable certain situations. example, $L1 $L2
parallel, single translation never achieve constraints, plan-fragment (a)
would applicable. paper show plan-fragments
automatically synthesized reasoning fundamental principles.
rest paper organized follows: Section 2 gives architectural overview
system built synthesize plan fragments automatically detailed description various
components. Section 3 illustrates plan fragment synthesis process using example
Figure 2. Section 4 describes results current implementation system. Section
5 relates approach work geometric constraint satisfaction. Section 6 summarizes
main results suggests future extensions work.

2. Overview System Architecture
Figure 3 gives overview architecture system showing various knowledge
components plan generation process. knowledge represented system broadly
categorized Geom knowledge-base contains knowledge specific particular
geometric entities Geometry knowledge-base independent particular geoms
reused generating plan fragments geom.
Knowledge Components
Geometry knowledge-base
Geom knowledge-base
Geoms

Actions

Invariants

Action Matching Rules
Action Rules

Loci

Reformulation Rules

Signatures

Measurements

Plan fragment
specification

Planner
Phase

Prioritization Strategy

Skeletal
Plan

Planner
Phase II

Plan fragment

Figure 3. Architectural overview plan fragment generator

2.1 Geom Knowledge-base
geom specific knowledge-base decomposed seven knowledge
components.
423

fiBHANSALI, KRAMER & HOAR

2.1.1 ACTIONS
describe operations performed geoms. GCE domain, three actions
suffice change configuration body arbitrary configuration: (translate g v)
denotes translation geom g vector v; (rotate g pt ax amt) denotes rotation
geom g, around point pt, axis ax, angle amt; (scale g pt amt) g
geom, pt point geom, amt scalar. semantics scale operation depends
type geom; example, circle, scale indicates change radius
circle line-segment denotes change line-segments length. Pt point
geom fixed (e.g., center circle).
2.1.2 INVARIANTS
describe constraints solved geoms. initial version system
designed generate plan fragments variable-radius circle variable length linesegment fixed workplane, constraints distances geoms points,
lines, geoms workplane. seven invariant types represent
constraints. Examples two invariants are:



(Invariant-point g pt glb-coords) specifies point pt geom g
coincident global coordinates glb-coords,
(Fixed-distance-point g pt dist bias) specifies geom g lies fixed
distance dist point pt; bias either BIAS_INSIDE BIAS_OUTSIDE
depending whether g lies inside outside circle radius dist around point pt.

2.1.3 LOCI
represent sets possible values geom parameter, position point
geom. various kinds loci grouped either 1d-locus (representable set
parametric equations one parameter) 2d-locus (representable set parametric
equations two variables). For, example line 1d locus specified (make-line-locus
through-pt direc) represents infinite line passing through-pt
direction direc. loci represented system include rays, circles, parabolas, hyperbolas,
ellipses.
2.1.4 MEASUREMENTS
used represent computation function, object, relationship
objects. terms mapped set service routines get called plan
fragments. example measurement term is: (0d-intersection 1d-locus1 1d-locus2).
represents intersection two 1d-loci. normal case, intersection two 1dimensional loci point. However, may singular cases, example, two
loci happen coincide; case intersection returns one locus instead
point. may also degenerate cases, example, two loci intersect;
case, intersection undefined. exceptional conditions also represented
measurement type used second phase plan generation process
elaborate skeletal plan (see Section 3.3).
424

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

2.1.5 GEOMS
objects interest solving geometric constraint satisfaction problems. Examples
geoms lines, line-segments, circles, rigid bodies. Geoms degrees freedoms
allow vary location size. example, 3D-space circle variable
radius, three translational, two rotational, one dimensional degree freedom.
configuration variables geom defined minimal number real-valued
parameters required specify geometric entity space unambiguously. Thus, circle
six configuration variables (three center, one radius, two plane
normal). addition, representation geom includes following:





name: unique symbol identify geom;
action-rules: set rules describe invariants geom
preserved achieved actions (see below);
invariants: set current invariants geom;
invariants-to-be-achieved: set invariants need achieved
geom.

2.1.6 ACTION RULES
action rule describes effect action invariant. two facts interest
planner constructing plan: (1) achieve invariant using action (2)
choose actions preserve many existing invariants possible. general,
several ways achieve invariant several actions preserve invariant.
intersection two sets actions set feasible solutions. system, effect
actions represented part geom-specific knowledge form Action rules, whereas
knowledge compute intersections two sets actions represented
geometry-specific knowledge (since depend particular geom acted on).
action rule consists three-tuple (pattern, to-preserve, to-[re]achieve). Pattern
invariant term interest; to-preserve list actions taken without violating
pattern invariant; to-[re]achieve list actions taken achieve invariant
re-achieve existing invariant clobbered earlier action. actions stated
general form possible. matching rules Geometry Knowledge base
used obtain general unifier two actions. example action rule,
associated variable-radius circle geoms is:
pattern: (1d-constrained-point ?circle (>> ?circle CENTER) ?1dlocus)
to-preserve: (scale ?circle (>> ?circle CENTER) ?any)
(translate ?circle (v- (>> ?1dlocus ARBITRARY-POINT)
(>> ?circle CENTER))
to-[re]achieve: (translate ?circle (v- (>> ?1dlocus ARBITRARY-POINT)
(>> ?circle CENTER))

(AR-1)

action rule used preserve achieve constraint center circle geom
lie 1d locus. two actions may performed without violating constraint:
(1) scale circle center. would change radius circle position
center remains hence 1d-constrained-point invariant preserved. (2)
425

fiBHANSALI, KRAMER & HOAR

translate circle vector goes current center arbitrary point 1dimensional locus ((v- b) denotes vector point b point a). achieve invariant
one action may performed: translate circle center moves current
position arbitrary position 1-dimensional locus.
2.1.7 SIGNATURES
completeness, necessary exist plan fragment possible combination
constraints geom. However, many cases, two constraints describe
situation geom (in terms degrees freedom). example, constraints
ground two end-points line-segment constraints ground direction, length,
one end-point line-segment reduce degrees freedom line-segment
zero hence describe situation. order minimize number plan fragments
need written, desirable group sets constraints describe situation
equivalence classes represent equivalence class using canonical form.
state geom, terms prior constraints it, summarized signature.
signature scheme geom set canonical signatures plan fragments need
written. Kramers earlier work (1993) signature scheme determined manually
examining signature obtained combining constraint types designating one
set equivalent signatures canonical. approach allows us construct signature
scheme geom automatically using reformulation rules (described shortly).
reformulation rule rewrites one constraints simpler form. signature scheme
obtained first generating possible combinations constraint types yield set
possible signatures. signatures reduced using reformulation rules
signature reduced simplest form. set (unique) signatures left constitute
signature scheme geom.
example, consider set constraint types variable radius circle. signature
geom represented tuple <Center, Normal, Radius, FixedPts, FixedLines> where:







Center denotes invariants center point either Free (i.e.,
constraint center point), L2 (i.e., center point constrained 2dimensional locus), L1 (i.e., center point constrained 1-dimensional
locus), Fixed.
Normal denotes invariant normal plane circle
either Free, L1, Fixed (in 2D always fixed).
Radius denotes invariant radius either Free Fixed.
FixedPts denotes number Fixed-distance-point invariants either 0,1,
2.
FixedLines denotes number Fixed-distance-line invariants either
0,1, 2.

L2 L1 denote 2D 1D locus respectively. assume 2D geometry, L2 invariant
Center redundant, Normal always Fixed. 3 x 1 x 2 x 3 x 3 = 54
possible signatures geom. However, several describe situation.
example, signature:
<Center-Free,Radius-Free, FixedPts-0,FixedLines-2>
describes circle constrained specific distances two fixed lines,
rewritten to:
426

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

<Center-L1, Radius-Free,FixedPts-0,FixedLines-0>
describes circle constrained 1-dimensional locus (in case angular
bisector two lines). Using reformulation rules, derive signature scheme variable
radius circles consisting 10 canonical signatures given below:
<Center-Free,Radius-Free, FixedPts-0,FixedLines-0>
<Center-Free,Radius-Free, FixedPts-0,FixedLines-1>
<Center-Free,Radius-Free, FixedPts-1,FixedLines-0>
<Center-Free,Radius-Fixed, FixedPts-0,FixedLines-0>
<Center-L1,Radius-Free, FixedPts-0,FixedLines-0>
<Center-L1,Radius-Free, FixedPts-0,FixedLines-1>
<Center-L1,Radius-Free, FixedPts-1,FixedLines-0>
<Center-L1,Radius-Fixed, FixedPts-0,FixedLines-0>
<Center-Fixed,Radius-Free, FixedPts-0,FixedLines-0>
<Center-Fixed,Radius-Fixed, FixedPts-0,FixedLines-0>
Similarly, number signatures line-segments reduced 108 19 using
reformulation rules.
2.2 Geometry Specific Knowledge
geometry specific knowledge organized three different kinds rules.
2.2.1 MATCHING RULES
used match terms using geometric properties. planner employs unification
algorithm match actions determine whether two actions common unifier. However,
standard unification algorithm sufficient purposes, since purely syntactic
use knowledge geometry. illustrate this, consider following two
actions:
(rotate $g $pt1 ?vec1 ?amt1),
(rotate $g $pt2 ?vec2 ?amt2).
first term denotes rotation fixed geom $g, around fixed point $pt1
arbitrary axis arbitrary amount. second term denotes rotation geom
around different fixed point $pt2 rotation axis amount unspecified before.
Standard unification fails applied terms binding variables
makes two terms syntactically equal2. However, resorting knowledge geometry,
match two terms yield following term:
(rotate $g $pt1 (v- $pt2 $pt1) ?amt1)
denotes rotation geom around axis passing points $pt1 $pt2.
point around body rotated point axis (here arbitrarily chosen
one fixed points, $pt1) amount rotation anything.
planner applies matching rules match outermost expression term first;
rule applies, tries subterms term, on. none matching rules apply,
2 Specifically, unification fails tries unify $pt1 $pt2.

427

fiBHANSALI, KRAMER & HOAR

algorithm degenerates standard unification. matching rules also conditions
attached them. condition boolean function; however, part
tend simple type checks.
2.2.2 REFORMULATION RULES
mentioned earlier, several ways specify constraints restrict degrees
freedom geom. GCE, plan fragments indexed signatures summarize
available degrees freedom geom. reduce number plan fragments need
written indexed, desirable reduce number allowable signatures.
accomplished set invariant reformulation rules used rewrite pairs
invariants geom equivalent pair simpler invariants (using well-founded
ordering). equivalence means two sets invariants produce range
motions geom. reduces number different combinations invariants
plan fragments need written. example invariant reformulation following:
(fixed-distance-line ?c ?l1 ?d1 BIAS_COUNTERCLOCKWISE)
(fixed-distance-line ?c ?l2 ?d2 BIAS_CLOCKWISE)



(RR-1)

(1d-constrained-point ?c (>> ?c center) (angular-bisector
(make-displaced-line ?l1 BIAS_LEFT ?d1)
(make-displaced-line ?l2 BIAS_RIGHT ?d2)
BIAS_COUNTERCLOCKWISE
BIAS_CLOCKWISE))

rule takes two invariants: (1) geom fixed distance left given line,
(2) geom fixed distance right given line. reformulation produces
invariant geom lies angular bisector two lines parallel two given
lines specified distance them. Either two original invariants conjunction
new one equivalent original set invariants.
Besides reducing number plan fragments, reformulation rules also help simplify
action rules. Currently action rules (for variable radius circles line-segments) use
single action preserve achieve invariant. restrict allowable signatures
geom, possible create examples need sequence (more one) actions
rule achieve invariant, need complex conditions need checked
determine rule applicability. Allowing sequences conditionals rules increases
complexity rules pattern matcher. makes difficult verify
correctness rules reduces efficiency pattern matcher.
Using invariant reformulation rules allows us limit action rules contain
single action. Unfortunately, seems still need conditions achieve certain invariants.
example, consider following invariant variable radius circle:
(fixed-distance-point ?circle ?pt ?dist BIAS_OUTSIDE)
states circle, ?circle distance ?dist point ?pt lie outside
circle around ?pt radius ?dist. One action may taken achieve constraint is:
(scale ?circle
(>> ?circle center)
428

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

(minus (>> (v- (>> ?circle center) ?pt)
magnitude)
?dist)
is, scale circle setting radius distance center point ?pt
minus scalar amount ?dist (see Figure 4). However, action achieves constraint
circle happens lie outside circular region radius ?dist center ?pt.

$C
$c

Figure 4. geom $c scaled touch $C center $c lies shaded region.

Therefore, need pre-condition rule checks indeed case. Note
action necessary completeness (otherwise planner would able solve
certain cases solution). Instead allowing conditional rules, use rules without
condition second phase plan generation check see
exceptions. Thus, example, exception would detected since third argument
scale operation returns negative number considered exception condition scale
operation.
2.2.3 PRIORITIZING STRATEGY
Given set invariants achieved geom, planner generally creates multiple
solutions. valid solutions absence exception conditions yield
configuration geom. However, plan fragments contain redundant action
sequences (e.g., two consecutive translations). Moreover, geom constrained
exception conditions, plan fragments able provide solution
whereas others not. prioritization strategy used prioritize skeletal plan
fragments plan fragments least redundancy flexibility chosen.
Eliminating plan fragments redundant actions turns straightforward.
assume one degree dimensional freedom geometric body.
assumption proved 1 translation, 1 rotation, 1 scale sufficient change
configuration object arbitrary configuration 3D space. Therefore, plan
fragment contains one instance action type contains redundancies
rewritten equivalent plan fragment eliminating redundant actions, combining two
action single composite action. example, consider following pair
translations geom:


(translate $g ?vec)
429

fiBHANSALI, KRAMER & HOAR



(translate $g (v- ?to 2 (>> $g center)))

?vec represents arbitrary vector ?to2 represents arbitrary position. ?to2
independent positional parameter geom, first translate action redundant
removed. Hence plan fragments contain redundant actions
eliminated.
prioritize remaining plan fragments following principle used:
Prefer solutions subsume alternative solution.
rationale principle permits greater flexibility solving constraints
exception conditions. example, suppose two solutions circle geom:
Solution 1: Translate circle center lies fixed position 1dimensional locus.
Solution 2: Translate circle center lies arbitrary point 1dimensional locus; scale fixed amount (which function
position arbitrary point).
first solution subsumed second solution since always choose
arbitrary point Solution 2 fixed position specified Solution 1 (the scale operation
case leaves dimension circle unchanged). Therefore Solution 2 preferred
Solution 1.
subsumption relation imposes partial order set skeletal plan fragments.
prioritization strategy selects maximal elements partial order. runtime
tried turn one yields solution.

3.0 Plan Fragment Generation
plan fragment generation process divided two phases (Figure 1). first phase
specification plan fragment taken input, planner used generate set
skeletal plans. form input second phase chooses one
skeletal plans elaborates take care singularities degeneracies. output
phase complete plan fragments.
3.1 Phase
skeletal plan generated using breadth-first search process. Figure 5 gives general form
search tree produced planner. first action typically reformulation
planner uses reformulation rules rewrite geom invariants canonical form. Next,
planner searches actions produce state least 1 invariant Preserved
list preserved least 1 action To-be-achieved (TBA) list achieved. preserved
achieved invariants pushed Preserved list, clobbered unachieved
invariants pushed TBA list child state.
strategy produce intermediate nodes search tree might clobber
one preserved invariant without achieving new invariant might produce state
identical parent state terms invariants Preserved

430

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

Preserved: P
TBA:
Reformulate
Preserved: P
TBA:
Action 1
Preserved: P1
TBA: A1

Action 3
Action 2
Preserved: P2
TBA: A2

Preserved: P3
TBA: A3

Actions
Preserved: P +
TBA: nil
Figure 5. Overview search tree produced planner

TBA list. initial state geom may arbitrary configuration
(among set allowable configurations) may necessary first move geom
alternative allowable configuration find optimal solution.
illustrate need, consider example Figure 6. example, one prior
constraint variable radius circle geom: center lies 1-dimensional locus. new
constraint achieved is: geom lie fixed distance line. order
achieve constraint one following two actions may taken: 1) scale

(a)

(b) Scale

(c) Translate

(d) Translate & scale

Figure 6. Example illustrate need actions produce state equivalent parent state.

431

fiBHANSALI, KRAMER & HOAR

circle fixed distance line (Figure 6b), 2) translate circle
new position 1-dimensional locus touches line (Figure 6c). However,
infinite number additional solutions consisting combinations scale translation
(Figure 6d). solutions derived planner first changes configuration
geom preserves existing invariant without achieving new invariant (i.e.,
scale arbitrary amount translate arbitrary point 1-dimensional locus)
followed action achieves new invariant. Therefore planner also creates child
states identical parent state terms invariants Preserved TBA lists.
planner iteratively expands leaf node search tree one following
true:
1. node represents solution; is, TBA list nil.
2. node represents cycle; is, invariants Preserved TBA lists
identical one ancestor nodes.
node marked terminal search tree pruned point. leaf nodes
marked terminal, search terminates. planner collects terminal nodes
solutions. plan-steps solution nodes represents skeletal plan
fragment. multiple skeletal plan fragments obtained planner, one
chosen using prioritizing rule described earlier passed second phase plan
fragment generation.
3.2 Phase I: Example
use example Section 1 illustrate Phase planner. planner begins
attempting reformulate given constraints. uses reformulation rule RR-1 described earlier
repeated convenience:
(fixed-distance-line ?c ?l1 ?d1 BIAS_COUNTERCLOCKWISE)
(fixed-distance-line ?c ?l2 ?d2 BIAS_CLOCKWISE)



(RR-1)

(1d-constrained-point ?c (>> ?c center) (angular-bisector
(make-displaced-line ?l1 BIAS_LEFT ?d1)
(make-displaced-line ?l2 BIAS_RIGHT ?d2)
BIAS_COUNTERCLOCKWISE
BIAS_CLOCKWISE))

ii

L2


iii

iv

L1

Figure 7. Four possible angular bisectors two lines L1 L2. bias symbols L1
L2 corresponding ray (i) BIAS_COUNTERCLOCKWISE & BIAS_CLOCKWISE,respectively.

432

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

rule two measurement terms: make-displaced-line angular-bisector.
Make-displaced-line takes three arguments: line, l, bias symbol indicating whether
displaced line left right l, distance, d. returns line parallel
given line l distance left right line depending bias. Angular-bisector
takes two lines, l1 l2, two bias symbols returns one four rays bisects
lines l1 l2 depending bias symbols (see Figure 7). reformulation, state
search tree shown Figure 8. reformulation rules applicable point.

Preserved: (fixed-distance-line $c $L1 $dist1 BIAS_COUNTERCLOCKWISE)
TBA:
(fixed-distance-line $c $L2 $dist2 BIAS_CLOCKWISE)

Reformulation

Preserved: (fixed-distance-line $c $L1 $dist1 BIAS_COUNTERCLOCKWISE)
TBA:
(1d-constrained-point $c
(>> $c CENTER)
(angular-bisector
(make-displaced-line $L1 $BIAS_LEFT $dist1)
(make-displaced-line $L2 $BIAS_RIGHT $dist2)
BIAS_COUNTERCLOCKWISE
BIAS_CLOCKWISE))
Figure 8. Search tree reformulating invariants

Next, planner searches actions achieve new invariant preserve
existing invariant both. describe steps involved finding actions satisfy
maximal number constraints (in case, two). planner first finds actions
achieve 1d-constrained-point invariant examining action rules associated
variable-circle geom. action rule AR-1 contains pattern matches 1d-constrainedpoint invariant:
pattern: (1d-constrained-point ?circle (>> ?circle center) ?1dlocus)
to-preserve: (scale ?circle (>> ?circle center) ?any)
(translate ?circle (v- (>> ?1dlocus arbitrary-point)
(>> ?circle center))
to-[re]achieve: (translate ?circle (v- (>> ?1dlocus arbitrary-point)
(>> ?circle center))
following bindings:

(AR-1)

{?circle = $c, ?1d-locus = (angular-bisector (make-displaced -line ...) ...)}
Substituting bindings obtain following action:

433

fiBHANSALI, KRAMER & HOAR

(translate $c (v- (>> (angular-bisector (make-displaced-line $L1 BIAS_LEFT $dist1)
(make-displaced-line $L2 BIAS_RIGHT $dist2)
arbitrary-point)
(>> $c center)))
(a1)
taken achieve constraint. Similarly, planner finds actions
preserve fixed-distance-line invariant. relevant action rule following:
pattern: (fixed-distance-line ?circle ?line ?distance)
(AR-2)
to-preserve: (translate ?circle (v- (>> (make-line-locus (>> ?circle center)
(>> ?line direction))
arbitrary-point)
(>> ?circle center))
to-[re]achieve: (translate ?circle (v- (>> (make-displaced-line
?line
BIAS_LEFT

(plus ?distance (>> ?circle radius)))
arbitrary-point)
(>> ?circle center)))
relevant action appropriate substitutions is:
(translate $c (v- (>> (make-line-locus
(>> $c center)
(>> L1 direction))
arbitrary-point)
(>> $c center))

(a2)

Now, find action preserves preserved invariant achieves TBA
invariant, planner attempts match preserving action (a2) achieving action (a1).
two actions match using standard unification, match employing following
geometry-specific matching rule:
# move arbitrary point two
# different loci, move point
# intersection two loci

(v- (>> $1d-locus1 arbitrary-point) $to)
(v- (>> $1d-locus2 arbitrary-point) $to)



(v- (0d-intersection $1d-locus1 $1d-locus2) $to)
yield following action:
(translate $c (v- (0d-intersection (angular-bisector
(make-displaced-line ...) ...)
(make-line-locus (>> $c center) (>> $L1 direction))
(>> $c CENTER)))
action moves circle point shown Figure 9 achieves constraints.
simple one-step plan constitutes skeletal plan fragment.
434

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

$L2

$c

angular-bisector
make-line-locus

$dist2
$dist1
$L1

Figure 9.

denotes point circle moved.

two actions generated planner first iteration. One
achieves new constraint clobbers prior invariant. moves circle
another configuration without achieving new constraint preserving prior constraint.
first action produces terminal state since constraints achieved.
Hence search tree pruned point. However, planner continues search
alternative solutions expanding two nodes. two iterations following
solutions obtained:
1. Translate intersection angular-bisector make-line-locus.
2. Translate arbitrary point angular-bisector, followed translation
intersection point.
3. Translate arbitrary point make-line-locus, followed translation
intersection point.
4. Translate arbitrary point angular-bisector scale.
stage first phase plan fragment generation terminated skeletal
plan fragments passed second phase planner.
3.3 Phase II: Elaboration Skeletal Plan Fragment
purpose Phase 2 planning i) select one skeletal plan fragments, ii)
elaborate generate desirable configuration geom
constrained well handle exception conditions.
3.3.1 SELECTION SKELETAL PLAN FRAGMENTS
two primary considerations selecting skeletal plan fragment reduce redundant
actions plan increase generality plan. considerations used formulate
prioritization strategy described Section 2. strategy implemented lookup table
assigns weights various plan fragments. plan fragments maximal weights
selected elaboration Phase 2. Readers interested implementation details
referred (Hoar, 1995).
3.3.2 PLAN FRAGMENT ELABORATION
Plan fragment elaboration refines skeletal plan fragment two ways. First, refines actions
435

fiBHANSALI, KRAMER & HOAR

constrained (e.g., translate arbitrary point locus) appropriate
instantiation unconstrained parameters (e.g., selecting specific point locus). Second,
handles exception conditions result constrained over-constrained systems.
action refinement exception handling treated using common technique.
Plan elaboration based "principle least motion": multiple
solutions problem choose solution minimizes total amount perturbation
(motion) system. Implementing principle requires definition motion function,
CA,G action, A, geom type, G. example, translation geom, motion
function, CT,circle could square displacement center geom
initial final position. also need motion summation function, G sums motion
produced individual actions geom G. example summation function
normal addition operator: plus. total motion produced geom computed using
summation function motion functions action- geom pairs.
plan fragment constrained, expression representing total motion
would contain one variables representing ungrounded parameters geom.
Formal optimization techniques, based finite difference methods, used obtain values
parameters would minimize motion function. However, use efficient,
algorithm based hill-climbing guarantee optimality yields good results
practice. use heuristic algorithm justified many interactive applications like
sketching, fast, sub-optimal solution preferable computationally expensive,
optimal one.
algorithm begins segmenting continuous loci discrete intervals.
systematically searches resultant, discrete n-dimensional space. algorithm first finds
local minima along one dimension holding variables constant values.
holds first variable minimum value found searches lower local minima along
second dimension on. Although algorithm guarantee finding global
even local minima, efficient yields good results practice. implemented
algorithm somewhat complex simple description above; details
found elsewhere (Hoar, 1995).
Exception conditions handled using technique above. Exception
conditions identified service routine returns set solutions solution (e.g.,
routine compute intersection two 1-dimensional loci returns 1-dimensional locus
nil). Multiple solutions represent constrained system requires search among
set solutions returned. conditions handled exactly described previous
paragraph. no-solution exception occurs, system aborts plan fragment prints
diagnostic message explaining constraint could solved.
3.4 Phase II: Example
Four skeletal plan fragments generated first phase planner (Section 3.2). Using
rule eliminating redundant translations given earlier, second third plan fragments
reduced single translation plan fragments equivalent first plan fragment.
leaves two distinct plan fragment solutions consider.
Using prioritizing rule, system concludes first plan fragment consisting
single translation subsumed second plan fragment consisting translation
scale. Thus, second plan fragment chosen preferred solution.
plan fragment deterministic since contains action translates circle
436

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

geom arbitrary point angular-bisector. Therefore, system inserts iterative loop
computes amount motion circle various points angular bisector,
breaking loop finds minima. Similarly, service routine may
return exception, system inserts case statement contains loop handle
situations one solution returned. Online Appendix 1 contains complete
example plan fragment generated system.

4.0 Results
plan fragment generator described implemented using CLOS (Common Lisp
Object System). implemented parts geometric constraint engine (GCE) described
Kramer C++ XMotif based graphical user interface. also written
translator translates synthesized plan fragments C++. complete plan fragment
library representative geom (line segment) synthesized integrated
constraint engine. Using able successfully demonstrate solution
several geometric constraints. present evaluation system.
primary contribution research novel geometric constraint satisfaction
approach. perspective constraint satisfaction techniques, novel feature
approach - degrees freedom analysis - already described earlier works
second author (Kramer, 1992, 1993). goal research develop automated
techniques enable degrees freedom approach scale reducing amount
effort needed creating plan fragment libraries. Hence, evaluation based
successful automating plan fragment synthesis process.
used plan fragment generator described automatically synthesize plan
fragments two representative geoms -- line-segments circles -- 2D. seven
types constraints thirty four rules system (12 action rules line-segments, 8 action
rules circles, 7 Reformulation rules, 7 Matching rules). Using rules
successfully generated skeletal plan fragments various combinations constraints line
segments (249) circles (50). largest search tree produced planner order
hundred nodes takes minutes Macintosh Quadra. evaluation
purposes, present data one representative geom - line segment.
4.1 Programming Effort
Figure 10 shows number lines code comprising current system. areas solid
represent code written manually. includes 5000 lines CLOS code
plan fragment synthesizer, 5400 lines C/C++ user interface, 3300 lines C/C++
support routines. hatched area represent code synthesized plan
fragment generator. represents 27000 lines C++ code (for plan fragments linesegment geom). size synthesized plan fragment (about 121 lines average) much less
plan fragments written manually (in C) original version GCE. Thus, using
automated plan fragment generator considerably reduced amount programming.
reduction ratio 5:1 good indicator reduction programming effort,
subject criticism since compares code two different programming languages
comprising different degrees difficulty.
accurate evaluation obtained comparing total effort required writing plan
fragments manually total effort required synthesizing using
437

fiBHANSALI, KRAMER & HOAR

12% (CLOS)

13% (C/C++)

67%
(C++)

User Interface

8% (C/C++)

Support routines

Generator

Plan Fragments

Figure 10. Lines code different parts system

technique described paper. extremely difficult, impossible,
controlled experimental setting number factors cost involved. best
done compare empirical data based experience developing system.
following table shows effort person days developing plan fragment library
line-segment geom using technique.

Plan Fragment Generator
Manually

Research
90
0

Development
150
498

Total
210
498

Table 1. Effort (in person-days) creating plan fragments
effort involved writing plan fragments manually, use conservative estimate
2 person days plan fragment3. table shows using plan fragment generator
obtained 58% reduction effort creating plan fragment library. testing
debugging time ignored assumed cases (although
believe time much manually generated plan fragments).
4.2 Scalability
much stronger evidence support technique obtained look effort
3 estimate based effort required developing plan fragment library GCE well
experimental data obtained two graduate students write plan fragments manually.

438

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

required extending plan fragment library adding features (e.g., new kinds
geoms constraints). evaluate scalability approach, decided extend plan
fragments 3D geoms added degrees rotational translational freedom.
extension done manually would significant exercise software maintenance since
requires changes plan fragment library. Using plan fragment generator
needed revise rules used planner make changes support routines. Since
support routines written manually, cost modify
approaches, effort needed rewrite rules relevant. took 1 week
effort rewrite debug action rules synthesize complete plan fragment library
3D, link successfully constraint engine. significant result
demonstrating technique used scale degrees freedom analysis
complex geoms geometries.
4.3 Correctness
important issue ignored far is: one verify correctness
completeness plan fragment generator? done extensive testing evaluation
plan fragments synthesized plan fragment generator. Table 2 summarizes results.
Number plan fragment specifications
Specs. solutions:
Completeness
solution exists
Missing rules
symbolic solution
Total
Plan fragments errors:
Correctness
Faults due errors logic
Support routine errors
Total

249
65
13
2
80
0
56
56

Table 2. Completeness Correctness synthesized plan fragments
eighty plan fragment specifications planner failed produce
solution. sixty five specifications, solutions general case --
specifications represent overconstrained problems, constraining one end point linesegment one-dimensional locus previous constraints already reduced
end-points translational degrees freedom zero. action planner take
cases check new constraint already satisfied. Thirteen cases
solutions two missing rules: one action rule, one reformulation rule. two
rules added thirteen specifications solved. Finally, two plan
fragments planner failed produce analytical solution. cases shown
Figure 11. solve problems need reformulation rule reformulates existing
invariant constraint endpoint $lseg curve $L3. Instead representing
complex 1-dimensional (and higher dimensional) loci like $L3, assume constraint
engine would call numerical solver computes solution iteratively. alternative would
extend set support routines handle complex loci intersections.

439

fiBHANSALI, KRAMER & HOAR

.
$L2

$P
$L3

$lseg

$L1

Figure 11. Example problem generated symbolic solution. $lseg line-segment
constrained one end-point $L1, fixed length, tangent
circle centered $P. new constraint end-point $lseg $L2.

check correctness plan fragments, exhaustive evaluation
plan fragments. seen Table 2, code synthesized perfect.
20% plan fragments function correctly. analyzed reasons
failure manually inspecting plan fragments. significant finding none
failures due logical errors plan fragments. words skeletal plan
fragments generated Phase correct complete. failures
bugs mathematical support routines called plan fragments.
instances failures traced bugs implementing Phase 2 plan fragment: either
selecting wrong skeletal plan fragment computing least motion correctly.
expected first version automatically generated plan fragments completely
bug-free. Indeed, high percentage plan fragments function correctly (almost 80%)
positive result reflects significant increase quality corresponding
decrease maintenance effort building geometric constraint satisfaction systems using
approach.

5.0 Related Work
Geometric constraint satisfaction old problem. Probably first application problem
constraint-based sketching Sketchpad program developed Sutherland (1963).
Sketchpad program based constraint relaxation limited problems
modeled point variables.
field mechanical design, graph based approach constraint satisfaction
described Serrano (1987). Serranos approach constraints modeled using
constraint network; constraint satisfaction engine finds values constrained variables
satisfy constraints network using constraint propagation techniques. approach
identifies loops cycles network, collapses supernodes, applies
conventional sequential local propagation. approach uses numerical iterative techniques
problems stability. computational advantage approach reduces
equations tightly coupled.
commercial systems kinematics analysis based numerical iterative
techniques algebraic techniques combination two. Although approaches
principle robust, several shortcomings make inappropriate real-time
applications.
Among non-commercial systems, notable new approach constraint based sketching
440

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

Juno-2 developed DEC-SRC (Heydon & Nelson, 1994). Constraints Juno-2
specified using expressive, declarative constraint language seems powerful enough
express constraints arise practice. Juno-2 uses combination symbolic
numerical techniques solve geometric constraints efficiently. key difference Juno2 degrees freedom approach Juno-2 symbolic reasoning done
domain equations. example, Juno-2 uses symbolic techniques like local propagation,
unpacking, unification closure reduce number unknowns system equations.
equations solved Newtons method. degrees freedom analysis, symbolic
reasoning done domain geometry rather equations.
Geometric constraints also arise robotics, primary issues concerned
finding physically realizable path space robot manipulator part
assembly. fundamental analytical tool solving motion planning problems robotics
configuration space framework (Lozano-Perez, 1983). configuration space approach,
problem planning motion part space obstacles transformed
equivalent simpler problem planning motion point space enlarged
configuration-space obstacles. Degrees freedom analysis finesses problem since uses
notion incremental assembly metaphor solving geometric constraint systems.
physical meaning ascribed objects move need
- factor quite important real-world assembly problem arising robotics.
use plan guide solution complicated non-linear equations arising
formulating solving problems algebraically.

6.0 Conclusions
described plan fragment generation methodology synthesize plan fragments
geometric constraint satisfaction systems reasoning first principles geometric
entities, actions, topology. technique used successfully synthesize plan
fragments realistic set constraints geoms. may seem substituted one
hard task - writing complete set correct plan fragments various combinations geoms
constraints - even harder task: creating knowledge base rules automate
process. rules difficult write found necessary spend
effort debugging rules. However, estimate total effort write debug rules
still order magnitude less writing debugging manually written plan fragment
code. future work investigate approach scales complex constraints
geometries.
Another useful extension work would concerned pushing automation one
level automatically acquire types knowledge simpler building
blocks. example, technique automatically synthesizing least motion function
description geometry would useful.
method plan fragment generation divided two disjoint phases.
alternative method would explore two phases interleaved. One possibility
degeneracy redundant constraint, planner could reformulate
problem removing redundant constraint re-synthesize skeletal plan fragment
new set constraints. resultant plan would form part original plan
fragment deal degenerate cases. words, plan fragments would generated
on-the-fly needed constraint solver.

441

fiBHANSALI, KRAMER & HOAR

Acknowledgments
thank Qiqing Xia helped implementing parts system described paper.
also acknowledge support resources provided School Electrical
Engineering Computer Science, Washington State University. work originated
first author Knowledge Systems Laboratory, Stanford University, second
author Schlumberger Laboratory Computer Science, Austin.

References
Anantha, R., Kramer, G., & Crawford, R. (1992). architecture represent over, under,
fully constrained assemblies. Proceedings ASME Winter Annual Meeting, 233-244.
Borgida, A., Mylopoulos, J., & Reiter, R. (1993). ... nothing else changes: frame problem
procedure specifications. Proceedings 15th International Conference
Software Engineering, Baltimore, MD.
Brown-Associates. (1993). Applicons GCE: Strong Technical Framework. Brown Associates
Inc.
Brunkhart, M. W. (1994). Interactive geometric constraint systems. Masters thesis, TR No.
CSD-94-808, Department EE&CS, University California, Berkeley.
Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: new approach applicatiion theorem
proving problem solving. Artificial Intelligence, 2, 198-208.
Friedland, P. E. (1979). Knowledge-based experiment design molecular genetics. Tech. report
CSD-79-771, Department Computer Science, Stanford University.
Hartenberg, R. S., & Denavit, J. (1964). Kinematic Synthesis Linkages. New York: McGraw
Hill.
Heydon, A., & Nelson, G. (1994). Juno-2 constraint-based drawing editor. SRC Research
report 131a, Digital Systems Research Center, Palo Alto, CA.
Hoar, T. (1995). Automatic program synthesis geometric constraint satisfaction. Masters
Thesis, School EECS, Washington State University.
Kramer, G. A. (1992). Solving Geometric Constraint Systems: Case Study Kinematics.
Cambridge, MA: MIT Press.
Kramer, G. A. (1993). geometric constraint engine. Artificial Intelligence, 58(1-3), 327-360.
Liu, Y., & Popplestone, R. J. (1990, ). Symmetry constraint inference assembly planning:
automatic assembly configuration specification. Proceedings AAAI-90, Boston, MA,
1038-1044.
Lozano-Perez, T. (1983). Spatial planning: configuration space approach. IEEE Transactions
Computers, C-32, 108-120.
442

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

Press, W. H., Flannery, B. P., Teukolsky, S. A., & Vetterling, W. T. (1986). Numerical Recipes:
Art Scientific Computing. Cambridge, England: Cambridge University Press.
Salomons, O. (1994). Computer support design mechanical products. Ph.D. Thesis,
Universiteit Twente, Netherlands.
Serrano, D. (1987). Constraints conceptual design. Ph.D. thesis, Massachusetts Institute
Technology.
Shah, J. J., & Rogers, M. T. (1993). Assembly modeling extension feature-based design.
Research Engineering Design, 5, 218-237.
Sussman, G. J. (1975). Computer Model Skill Acquisition. New York: American Elsevier.
Sutherland, I. E. (1963). Sketchpad, man-machine graphical communication system. Ph.D.
Thesis, Massachusetts Institute Technology.

443

fiJournal Artificial Intelligence Research 4 (1996) 477-507

Submitted 9/95; published 6/96

Partially Controlled Multi-Agent Systems
Ronen I. Brafman

brafman@cs.ubc.ca

Computer Science Department
University British Columbia
Vancouver, B.C., Canada V6L 1Z4

Moshe Tennenholtz

moshet@ie.technion.ac.il

Industrial Engineering Management
Technion - Israel Institute Technology
Haifa 32000, Israel

Abstract

Motivated control theoretic distinction controllable uncontrollable
events, distinguish two types agents within multi-agent system: controllable
agents , directly controlled system's designer, uncontrollable agents ,
designer's direct control. refer systems partially
controlled multi-agent systems, investigate one might uence behavior
uncontrolled agents appropriate design controlled agents. particular,
wish understand problems naturally described terms, methods
applied uence uncontrollable agents, effectiveness methods,
whether similar methods work across different domains. Using game-theoretic framework,
paper studies design partially controlled multi-agent systems two contexts:
one context, uncontrollable agents expected utility maximizers,
reinforcement learners. suggest different techniques controlling agents'
behavior domain, assess success, examine relationship.

1. Introduction
control agents central research topic two engineering fields: Artificial Intelligence (AI) Discrete Events Systems (DES) (Ramadge & Wonham, 1989). One
particular area fields concerned multi-agent environments;
examples include work distributed AI (Bond & Gasser, 1988), work decentralized
supervisory control (Lin & Wonham, 1988). fields developed
techniques incorporated particular assumptions models. Hence,
natural techniques assumptions used one field may adopted
may lead new insights field.
difference AI work multi-agent systems, work decentralized discrete
event systems distinguishes controllable uncontrollable events. Controllable
events events directly controlled system's designer, uncontrollable events directly controlled system's designer. Translating terminology context multi-agent systems, introduce distinction two
types agents: controllable agents , directly controlled system's designer,
uncontrollable agents , designer's direct control. leads
c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiBrafman & Tennenholtz

naturally concept partially controlled multi-agent system (PCMAS)
following design challenge: ensuring agents system behave appropriately
adequate design controllable agents. believe many problems
naturally formulated instances PCMAS design. goal characterize important
instances design problem, examine tools used solve it,
assess effectiveness generality tools.
distinguishes partially controlled multi-agent systems AI context similar models DES structural assumptions make uncontrolled agents
involved. Unlike typical DES models concerned physical processes devices, AI particularly interested self-motivated agents, two concrete examples
rational agents, i.e., expected utility maximizers, learning agents, e.g., reinforcement learners. Indeed, examples constitute two central models self-motivated
agents game theory decision theory, referred educative evolutive models
(e.g., see Gilboa & Matsui, 1991). special nature uncontrollable agents
special structure uncontrollable events induce differentiates PCMAS
corresponding models DES literature. difference raises new questions
suggests new perspective design multi-agent systems. particular, calls
techniques designing controllable agents that, exploiting structural assumptions,
uence behavior uncontrollable agents lead system desired
behavior.
order understand issues, study two problems stated solved
adopting perspective PCMAS design; problems
interest large community. problems goal uence
behavior agents control. exert uence indirectly
choosing suitable behaviors agents direct control. one case,
attempt uence behavior rational agents, case, try
uence learning agents.
first study concerned enforcement social laws. number
agents designed different designers work within shared environment, beneficial
impose certain constraints behavior, that, overall, system function
better. example, Shoham Tennenholtz (1995) show imposing certain \trac
laws," considerably simplify task motion planning robot, still
enabling ecient motions. Indeed, see later, conventions heart
many coordination techniques multi-agent systems. Yet, without suitable mechanisms,
rational agents may incentive follow conventions. show how,
certain cases, use perspective partially controlled multi-agent systems
structural assumption rationality enforce conventions.
second study involves two-agent system consisting teacher student.
teacher knowledgeable agent, student agent learning
behave domain. goal utilize teacher (which control)
improve behavior student (which controlled us). Hence,
instance partially controlled multi-agent systems structural assumption
uncontrolled agent employs particular learning algorithm.
studies presented paper suggest techniques achieving satisfactory system
behavior design controllable agents, relevant, techniques
478

fiOn Partially Controlled Multi-Agent Systems

experimentally assessed. Beyond formulation solution two interesting problems multi-agent system design, paper suggests general perspective certain
design problems. Although feel still premature draw general conclusion
potential general theory PCMAS design, certain concepts, punishment
reward, suggest central area.
paper organized follows: Section 2, describe problem enforcing
social behavior multi-agent systems. Section 3 describe standard game-theoretic
model problem suggest mechanism threats punishments general
tool class problems. Issues pertain design threats punishments
discussed Section 4. Section 5 introduces second case study PCMAS design:
embedded teaching reinforcement learners. context, teacher learner
embedded shared environment teacher serving controller whose
aim direct learner desired behavior. formal model problem
introduced Section 6. Section 7, show derive optimal teaching policies (under
certain assumptions) viewing teaching Markov decision process. effectiveness
different teaching policies studied experimentally Section 8. Finally, Section 9,
examine relationship methods used two domains
possibility general methodology designing partially controlled multi-agent systems.
conclude Section 10, summary discussion related work.

2. Enforcement Social Behavior
section introduce problem enforcement social laws multi-agent
context. proposed solution falls naturally PCMAS design perspective
take. Here, explain motivate particular problem social law enforcement
approach solution. Sections 3 4 formalize investigate approach
framework general game-theoretic model.
use following scenario illustrate problem:
hired design new working environment artificial
agents. Part job involves designing number agents use
maintain warehouse. agents, designed different designers,
using warehouse obtain equipment. make sure different agents
designed different designers operate eciently environment,
choose introduce number social laws, is, constraints behavior
agents, help agents coordinate activities domain.
rules include number `trac laws', regulating motion domain,
well law specifies every tool used agent must
returned designated storage area. robots programmed follow
laws, expect others so. laws quite successful, allow ecient activity warehouse, new designer arrives.
Pressed corporate bosses deliver better performance, decides
exploit rules. designs agent locally maximize performance,
regardless social laws. do?
479

fiBrafman & Tennenholtz

multi-participant environments, one above, agent might
dynamic goals, interested finding ways agents coexist
achieving goals. Several approaches coordination agent activity discussed
distributed systems DAI literature. examples are: protocols reaching
consensus (Dwork & Moses, 1990), rational deals negotiations (Zlotkin & Rosenschein,
1993; Kraus & Wilkenfeld, 1991; Rosenschein & Genesereth, 1985), organizational structures (Durfee, Lesser, & Corkill, 1987; Fox, 1981; Malone, 1987), social laws (Moses
& Tennenholtz, 1995; Shoham & Tennenholtz, 1995; Minsky, 1991; Briggs & Cook, 1995).
methods, behavior agent predetermined prescribed
certain stage, example, content deal reached, outcome
negotiation process completed, social law instituted. work
relies assumption agents follow prescribed behaviors, e.g., obey
law stick agreement. assumption central success
methods. However, makes agents follow rules vulnerable rational agent
performs local maximization payoff, exploiting knowledge others follow
rules. example, new designer may program robot return tools,
saving time required so, thus causing agents fail tasks.
Despite somewhat futuristic avor (although instances shared environments
beginning appear cyberspace), scenario useful illustrating vulnerability
popular coordination mechanism appearing multi-agent literature
within AI (e.g., see Bond & Gasser, 1988) assume agents involved
fully rational. aside, note that, case, actually need attribute much
intelligence agents themselves, sucient assume designers
design way maximizes utility, disregarding utility
agents.
order handle problem need modify existing design paradigms.
adopting perspective partially controlled multi-agent systems, obtain one possible
handle problem, requires making following basic assumption:
original designer, scenario, controls number reliable agents.1
basic idea reliable agents designed punish agents
deviate desirable social standard. punishment mechanism `hardwired' (unchangeable) common-knowledge. agents controlled
original designer aware punishment possibility. punishment
mechanism well designed, deviations social standard become irrational.
result, deviation actually occur punishment actually executed! Hence,
making agents bit sophisticated, prevent temptation breaking
social laws.
suggested solution adopt perspective partially controlled multi-agent systems. agents controllable, others uncontrollable assumed
adopt basic model expected utility maximization. punishment mechanism
(part of) control strategy used uence behavior uncontrolled
agents.
1. ease exposition, assume reliable agents follow designer's instructions; assume
non-malicious failures, crash failures, possible.

480

fiOn Partially Controlled Multi-Agent Systems

3. Dynamic Game Theoretic Model

section introduce basic game-theoretic model, use study
problem enforcement social behavior solution. Later on, Sections 5{8,
model used study embedded teaching. wish emphasize model
use common model representing emergent behavior population2
(e.g., Huberman & Hogg, 1988; Kandori, Mailath, & Rob, 1991; Altenberg & Feldman,
1987; Gilboa & Matsui, 1991; Weidlich & Haag, 1983; Kinderman & Snell, 1980).

Definition 1 k-person game g defined k-dimensional matrix size n1

nk , nm number possible actions (or strategies) m'th agent. entries
vectors length k real numbers, called payoff vectors. joint strategy
tuple (i1; i2; : : :; ik ), 1 j k, case 1 ij nj .
Intuitively, dimension matrix represents possible actions one k
players game. Following convention used game theory, often use term
strategy place action . Since dimensions matrix n1 nk , i'th
agent ni possible strategies choose from. j 'th component vector residing
(i1; i2; : : :; ik ) cell (i.e., Mi1 ;i2 ;:::;ik ) represents feedback player j receives
players' joint strategy (i1; i2; : : :; ik ), is, agent m's strategy im
1 k. Here, use term joint strategy refer combined choice
strategies agents.

Definition 2 n-k-g iterative game consists set n agents given k person
game g . game g played repetitively unbounded number times. iteration,
random k-tuple agents play instance game, members k-tuple
selected uniform distribution set agents.

Every iteration n-k-g game represents local interaction k agents. agents
play particular iteration game must choose strategy use
interaction; agent use different strategies different interactions. outcome
iteration represented payoff vector corresponding agents' joint strategy.
Intuitively, payoff tells us good outcome joint behavior point
view agent. Many situations represented n-k-g game, example,
\trac" aspect multi-agent system represented n-k-g game,
time number agents meet intersection. encounter instance
game agents choose number strategies, e.g., move ahead, yield.
payoff function gives utility set strategies. example, time
two agents meet agents choose move ahead, collision occurs payoffs
low.
Definition 3 joint strategy game g called ecient sum players' payoffs
maximal.
2. paper use term emergent behavior classical mathematical-economics interpretation:
evolution behavior based repetitive local interactions (usually pairs of) agents,
agent may change strategy following interactions based feedback received previous
interactions.

481

fiBrafman & Tennenholtz

Hence, eciency one global criterion judging \goodness" outcomes
system's perspective, unlike single payoffs describe single agent's perspective.3

Definition 4 Let fixed joint strategy given game g, payoff pi(s) player
i; instance g joint strategy s0 played, pi (s) pi (s0 ) say
i's punishment w.r.t. pi (s) , pi (s0 ), otherwise say benefit w.r.t.
pi(s0) , pi (s).
Hence, punishment benefit w.r.t. joint strategy measure gain (benefit)
loss (punishment) agent somehow change joint behavior agents
s0 .
current discussion punishment benefit always respect chosen
ecient solution.
designers multi-agent system, would prefer ecient possible.
cases entails behavior sense unstable, is, individual agents
may locally prefer behave differently. Thus, agents may need constrained behave
way locally sub-optimal. refer constraints exclude
possible behaviors social laws .
Due symmetry system assumption agents
rational utility additive (i.e., utility two outcomes sum
utilities), clear agent's expected payoff higher one obtained
using strategies giving ecient solution. Thus, clear case ecient
solution fair, sense agents get least could law
existed, solution provide better expected payoff.
However, good intentions designer creating environment beneficial
participating agents, may backfire. social law provides information behavior
agents conforming it, information agents (or respective designers)
use increase expected payoff.
Example 1 Assume playing n-2-g game g prisoner's dilemma,
represented strategic form following matrix.
agent 2
agent 1
1
2
1
(2,2) (-10,10)
2
(10,-10) (-5,-5)
ecient solution game obtained players play strategy 1. Assume
solution chosen original designer, followed agents
control.
designer new agent function environment social law
obeyed may tempted program agent conform chosen law. Instead,
program agent play strategy maximizes expected outcome, strategy
3. Addition payoffs utilities across agents dangerous practice. However, particular model,
shown system joint-strategies always ecient maximizes agent's expected
cumulative rewards.

482

fiOn Partially Controlled Multi-Agent Systems

#2. new agent obtain payoff 10 playing one `good' agents.
Thus, even though social law accepted order guarantee payoff 2
agent, `good' agents obtain payoff -10 playing non-conforming
agents. Note new designer exploits information strategies `good' players,
dictated social law. agents controlled new designer uncontcolable
agents; behavior dictated original designer.
Agents conforming social law referred malicious agents . order
prevent temptation exploit social law, introduce number punishing
agents , designed initial designer, play `irrationally' detect behavior
conforming social law, attempting minimize payoff malicious agents.
knowledge future participants punishment policy would deter deviations eliminate need carrying out. Hence, punishing behavior used
threat aimed deterring agents violating social law. threat (part of)
control strategy adopted controllble agents order uence behavior
unconrollable agents. Notice control strategy relies structural assumption
unconrollable agents expected utility maximizers.
define minimized malicious payoff minimal expected payoff malicious players guaranteed punishing agents. punishment exists ,
minimized malicious payoff lower expected payoff obtained playing according
social law. strategy guarantees malicious agents expected payoff lower
one obtained playing according social law called punishing strategy .
Throughout section following section make natural assumption
expected payoff malicious agents playing greater
one obtained ecient solution4 .
Example 1 (continued) Example 1, punishment would simply play strategy
2 on. may cause payoff punishing agent decrease, would
guarantee malicious agent obtains payoff better -5 playing punishing
agent. many non-malicious agents punishing, malicious agents' expected payoff
would decrease become smaller payoff guaranteed social law. Strategy
2 would punishing strategy.

4. Design Punishments

previous section described general model multi-agent interaction showed
perspecive partially controlled multi-agent systems leads one possible solution
problem enforcing social behavior setting, via idea threats
punishments. proceed examine issue punishment design.
assume p agents designer controls either ability
observe instances game occur, informed outcome
games. c additional agents conform law (that is, play strategies
entailed chosen ecient solution), malicious agents, bound
law.
4. assumptions may treated similarly.

483

fiBrafman & Tennenholtz

would like answer questions as: game offer ability punish?
minimized malicious payoff? optimal ratio p; c; m?
difference different social laws?
Example 1 (continued) Consider Example 1 again. observed
cause expected maximal loss malicious agents 7 (= 2 , (,5)). occurs
punishing agents play strategy 2. gain malicious agent makes
playing agent following social law 8 (= 10 , 2). order punishing
strategy effective, must case expected payoff malicious agent
greater expected payoff obtained following social law. order
achieve this, must ensure ratio punishing/conforming agents
malicious agent sucient encounters punishing agents. case, assuming
2 deviators meet expected benefit 0 recalling agent equally
likely meet agent, need pc > 87 make incentive deviate negative.
Implementing punishment approach requires complex behavior. agents
must able detect deviations well switch new punishing strategy.
whole behavior viewed new, complex, social law. calls
complex agents carry out, makes programming task harder.
Clearly, would like minimize number complex agents, keeping
benefit malicious behavior negative. Here, major question ratio
benefit deviation prospective punishment.
seen example, larger punishment, smaller number
sophisticated punishing agents needed. Therefore, would like find
strategies minimize malicious agent's payoff. order require
additional definitions.

Definition 5 two person game g zero-sum game every joint strategy
players, sum players' payoffs 0.

Hence, zero-sum game, win/win situations, larger payoff one
agent, smaller payoff agent. convention, payoff matrix two
person zero-sum game mention payoffs player 1.

Definition 6 Let g two person game. Let Pig (s,t) payoff player g (where
2 f1; 2g) strategies played player 1 2 respectively. projected

game, gp , following two person zero-sum game: strategies players
g , payoff matrix P gp (s; t) = ,P2g (s,t). Define transposed game g , g ,
game g roles players change.

projected game, first agent's payoff equals negated value second agent's
payoff original game. Thus, game ects desire lower payoffs
second player original game.
give general result two-person game, g (with number strategies).
make use following standard game-theoretic definition:
484

fiOn Partially Controlled Multi-Agent Systems

Definition 7 Given game g, joint strategy players Nash equilibrium

g whenever player takes action different action , payoff given
players play higher payoff given everybody plays .
is, strategy Nash equilibrium game agent obtain better payoff
unilaterally changing behavior agents play according .
Nash-equilibrium central notion theory non-cooperative games (Luce &
Raiffa, 1957; Owen, 1982; Fudenberg & Tirole, 1991). result, notion well studied
understood, reducing new concepts basic concept may quite useful
design perspective. particular, Nash-equilibrium always exists finite games,
payoffs prescribed Nash-equilibria given zero-sum game uniquely defined.
show:

Theorem 1 Given n-2-g iterative game, minimized malicious payoff achieved

playing strategy player 1 prescribed Nash equilibrium projected game gp,
playing player 1 (in g ), strategy player 1 prescribed Nash equilibrium
projected game (g )p, playing player 2 (in g ).5

Proof: Assume punishing agent plays role player 1. player 1 adopts

strategy prescribed Nash-equilibrium player 2 get better payoff
one guaranteed since deviation player 2 improve situation (by
definition Nash-equilibrium). hand, player 1 cause harm
harm obtained playing strategy . see this, assume player 1 uses
arbitrary strategy s, player 2 adopts strategy prescribed . outcome
player 1 higher one guaranteed playing Nash-equilibrium
(by definition Nash-equilibrium). addition, due fact
zero-sum game implies outcome player 2 lower one
guaranteed player 1 would play according . case punishing agent
player 2 treated similarly.

Example 1 (continued) Continuing prisoner's dilemma example, gp would
agent 2
agent 1 1 2
1
-2 -10
2
10 5
Nash equilibrium attained playing strategies yielding 5. example,
(g )p = gp. Therefore, punishing strategies strategy # 2 case.

Corollary 1 Let n-2-g iterative game, p punishing agents. Let v v'

payoffs Nash equilibria gp gpT respectively (which, case, uniquely
defined). Let b,b' maximal payoffs player 1 obtain g g respectively,
5. Notice that, cases, strategies prescribed original game determined strategies
player 1 Nash-Equilibria projected games.

485

fiBrafman & Tennenholtz

assuming player 2 obeying social law. Let e e' payoffs player 1 2,
respectively, g , players play according ecient solution prescribed
social law. Finally, assume expected benefit two malicious agents meet
0. necessary sucient condition existence punishing strategy
(n,1,p) (b + b0) , p (v + v 0 ) < (e + e0 ).
n,1
n,1

Proof: expected payoff obtained malicious agent encountering law-

abiding agent b+2b , expected payoff encountering punishing agent ,(v2+v ) .
order test conditions existence punishing strategy would need
consider best case scenario point view malicious agent; case
non-punishing agents law-abiding agents. order obtain expected utility
malicious agent make average quantities taking account
proportion law-abiding punishing agents population. gives us
expected utility malicious agent (2(n,n1,,1)p) (b + b0) , 2(np,1) (v + v 0). definition,
punishing strategy exists expected utility lower expected
utility guaranteed social law. Since expected utility guaranteed
social law e+2e , get desired result.
value punishment, (v+2v ) above, independent ecient solution
chosen, e + e0 identical ecient solutions, definition. However, b + b0 depends
choice ecient solution. number solutions exist, minimizing
b + b0 important consideration design social law, affects incentive
`cheat'.
0

0

0

0

Example 2 Let's look slightly different version prisoner's dilemma. game
matrix

agent 2
agent 1
1
2
1
(0,0) (-10,10)
2
(10,-10) (-5,-5)
3 ecient solutions, given joint strategies (1,1), (1,2), (2,1).
case (1,1) b+b'=20 (gained playing strategy 2 instead 1). case
(2,1) (1,2) b+b'=5.
Clearly, incentive deviate social law prescribing strategies (1,1)
social law prescribing (2,1) (1,2).
summarize, preceding discussion suggests designing number punishing agents,
whose behavior punishment mode prescribed Theorem 1 case n-2-g games.
ensuring sucient number agents take away incentive deviate
social laws. Hence, given malicious agents rational, follow social
norm, consequently, need utilize punishment mechanism.
observed different social laws leading solutions equally ecient different
properties comes punishment design. Consequently, assumption
would like minimize number punishing agents guaranteeing ecient
486

fiOn Partially Controlled Multi-Agent Systems

solution participants, choose ecient solution minimizes value
b + b0.

5. Embedded Teaching
section move second study PCMAS design problem; now,
uncontrollable agent reinforcement learner. choice arbitrary; rational
agents reinforcement learners two major types agents studied mathematical
economics, decision theory, game theory. also types agents discussed
work DAI concerned self-motivated agents (e.g., Zlotkin & Rosenschein,
1993; Kraus & Wilkenfeld, 1991; Yanco & Stein, 1993; Sen, Sekaran, & Hale, 1994).
agent's ability function environment greatly affected knowledge
environment. special cases, design agents sucient knowledge
performing task (Gold, 1978), but, general, agents must acquire information on-line
order optimize performance, i.e., must learn. One possible approach
improving performance learning algorithms employing teacher. example,
Lin (1992) uses teaching example improve performance agents, supplying
examples show task achieved. Tan's work (1993) also
viewed form teaching agents share experiences. methods nontrivial form communication perception required. strive model broad notion
teaching encompasses behavior improve learning agent's performance.
is, wish conduct general study partially controlled multi-agent systems
uncontrollable agent runs learning algorithm. time, want
model clearly delineate limits teacher's (i.e., controlling agent's) ability
uence student.
Here, propose teaching approach maintains situated \spirit" much like
reinforcement learning (Sutton, 1988; Watkins, 1989; Kaelbling, 1990), call
embedded teaching . embedded teacher simply \knowledgeable" controlled agent
situated student shared environment. Her6 goal lead student
adopt specific behavior. However, teacher's ability teach restricted
nature environment share: repertoire actions limited,
may also lack full control outcome actions. example, consider
two mobile robots without means direct communication. Robot 1 familiar
surroundings, Robot 2 not. situation, Robot 1 help Robot 2 reach
goal certain actions, blocking Robot 2 headed wrong
direction. However, Robot 1 may limited control outcome
interaction uncertainty behavior Robot 2 control uncertainty.
Nevertheless, Robot 2 specific structure, learner obeying learning scheme,
attempt control indirectly choice actions Robot 1.7
6. differentiate teacher student, use female pronouns former male pronouns
latter.
7. general, fact agent controllable imply perfectly control outcome
actions, choice. Hence, robot may controllable sense, running program
supplied us, yet move-forward command may always desired outcome.

487

fiBrafman & Tennenholtz

follows, goal understand embedded teacher help student
adopt particular behavior. address number theoretical questions relating
problem, experimentally explore techniques teaching two types
reinforcement learners.

6. Basic Teaching Setting
consider teacher student repeatedly engage joint activity.
student prior knowledge pertaining activity, teacher understands
dynamics. model, teacher's goal lead student adopt particular
behavior interactions. example, teacher student meet occasionally
road teacher wants teach student drive right side. perhaps,
teacher student share resource, CPU time, goal teach
judicious use resource. model encounters 2-2-g iterative games.
capture idea teacher knowledgeable student, assume
knows structure game, i.e., knows payoff function,
recognizes actions taken play. hand, student know
payoff function, although perceive payoff receives. paper, make
simplifying assumptions teacher student two actions
choose outcome depends choice actions. Furthermore,
excluding study Section 8.4, ignore cost teaching, hence, omit
teacher payoff description.8 provides basic setting take
first step towards understanding teaching problem.9
teaching model concisely modeled 2 2 matrix. teacher's actions
designated II , student's actions designated numbers 1
2. entry corresponds joint action represents student's payoff
joint action played. suppose matrix Figure 1,
wish teach student use action 1. stage, assume student
always receives better payoff following action 1 learn play it.
see situations teaching trivial. Assume first row dominates second row, i.e., > c b > d. case, student naturally prefer
take action 1, teaching challenging, although might useful speeding
learning process. example, , c > b , d, matrix B Figure 1, teacher
make advantage action 1 noticeable student always playing action
I.
suppose one > c b > holds. case, teaching still easy.
use basic teaching strategy, call preemption . preemption teacher
chooses action makes action 1 look better action 2. example,
situation described matrix C Figure 1, teacher always choose action .
8. case could made inherent value teaching, may appropriate forum
airing views.
9. fact, idea consider basic embedded teaching setting already challenging. later see, basic setting closely related fundamental issue non-cooperative
games.

488

fiOn Partially Controlled Multi-Agent Systems

II

II

1 b

1 6 5

2 c
(A)

II

1 5 1
2 2 6
(C)

2 1 2
(B)

II

1 3 -2
2 5 6
(D)



II

1 5 -10
2 10 -5
(E)

Figure 1: Game matrices A, B, C, D, E. teacher's possible actions II ,
student's possible actions 1 2.
Next, assume c greater b, matrix Figure 1.
Regardless action teacher chooses, student receives higher payoff
playing action 2 (since minf5; 6g > maxf3; ,2g). Therefore, matter teacher
does, student learn prefer action 2. Teaching hopeless situation.
types interactions isomorphic case c > > > b,
matrix E Figure 1. still challenging situation teacher action 2
dominates action 1 (because 10 > 5 ,5 > ,10). Therefore, preemption cannot work.
teaching strategy exists, complex always choosing action.
Since seems challenging teaching situation, devote attention
teaching reinforcement learner choose action 1 class games.
turns situation quite important game-theory multi-agent
interaction. projection famous game, prisoner's dilemma, discussed
previous sections. general, represent prisoner's dilemma using
following game matrix:
teacher
student Coop Defect
student Coop
Coop (a,a) (b,c) commonly Coop (a,a)
Defect (c,b) (d,d)
Defect (c,-c)

teacher
Defect
(-c,c)
(d,d)

c > > > b. actions prisoner's dilemma called Cooperate (Coop)
Defect; identify Coop actions 1 , Defect actions 2 II .
prisoner's dilemma captures essence many important social economic situations;
particular, encapsulates notion cooperation. thus motivated enormous discussion among game-theorists mathematical economists (for overview, see Eatwell,
Milgate, & Newman, 1989). prisoner's dilemma, whatever choice one player,
second player maximize payoff playing Defect. thus seems \rational"
player defect. However, players defect, payoffs much worse
cooperate.
489

fiBrafman & Tennenholtz

example, suppose two agents given $10 moving object.
agent perform task alone, take amount time energy
value $20. However, together, effort make valued $5. get
following instance prisoner's dilemma:
Agent 1
Agent 2 Move
Rest
Move
(5,5) (-10,10)
Rest (10,-10) (0,0)
experimental part study, teacher's task teach student
cooperate prisoner's dilemma. measure success teaching strategy
looking cooperation rate induces students period time, is,
percentage student's actions Coop. experimental results presented
paper involving prisoner's dilemma respect following matrix:
Teacher
Student Coop Defect
Coop (10,10) (-13,13)
Defect (13,-13) (-6,-6)
observed qualitatively similar results instantiations prisoner's
dilemma, although precise cooperation rate varies.

7. Optimal Teaching Policies

previous section concentrated modeling teaching context instance
partially controlled multi-agent system, determining particular problems
interesting. section start exploring question teacher
teach. First, define optimal policy is. Then, define Markov decision
processes (MDP) (Bellman, 1962), show certain assumptions teaching
viewed MDP. allow us tap vast knowledge accumulated
solving problems. particular, use well known methods, value
iteration (Bellman, 1962), find optimal teaching policy.
start defining optimal teaching policy. teaching policy function
returns action iteration; possibly, may depend complete history
past joint actions. \right" definition optimal policy, teacher's
motivation may vary. However, paper, teacher's objective maximize
number iterations student's action \good", Coop prisoner's
dilemma. teacher know precise number iterations playing,
slightly prefers earlier success later success.
formalized follows: Let u(a) value teacher places student's
action, a, let teacher's policy, assume induces probability distribution
Pr;k set possible student actions time k. define value strategy

1
X
val( ) = kEk (u)
k=0

490

fiOn Partially Controlled Multi-Agent Systems

Ek (u) expected value u:

Ek (u) =

X Pr

a2As

;k (a) u(a)

Here, student's set actions. teacher's goal find strategy
maximizes val(), discounted expected value student's actions. example,
case prisoner's dilemma, could
= fCoop,Defectg u(Coop) = 1 u(Defect) = 0.
Next, define MDPs. MDP, decision maker continually moving
different states. point time observes current state, receives payoff
(which depends state), chooses action. action current state
determine (perhaps stochastically) next state. goal maximize function
payoffs. Formally, MDP four-tuple hS; A; P; ri, state-space,
decision-maker's set possible actions, P : ! [0; 1] probability
transition states given decision-maker's action, r : ! < reward
function. Notice given initial state 2 , policy decision maker , P
induces probability distribution Ps;;k , Ps;;k (s0 ) probability
kth state obtained s0 current state s.
0-optimal policy MDP policy maximizes state
discounted sum expected values payoffs received future states, starting
s, i.e.,
1
X
X
0k( Ps;;k (s0 ) r(s0))
k=0

2S
0

Although may immediately obvious, single policy maximizing discounted sums
starting state exists, well-known ways finding policy.
experiments use method based value-iteration (Bellman, 1962).
suppose student set possible states, set actions
, teacher's set actions . Moreover, suppose following
properties satisfied:
(1) student's new state function old state current joint-action,
denoted : ! ;
(2) student's action stochastic function current state, probability
choosing state (s; a);
(3) teacher knows student's state. (The natural way happen
teacher knows student's initial state, function , outcome game,
uses simulate agent.)
Notice assumptions teaching policy function :
know student's next action function next state. know
student's next state function current state, current action, teacher's
current action. Hence, next action function current state action,
well teacher's current action. However, know student's current action
function current state. Hence, student's next action function
current state teacher's current action. implies knowledge
teacher needs optimally choose current action student's current state,
491

fiBrafman & Tennenholtz

additional information redundant cannot improve success. generally,
repeat line reasoning indefinitely future, see teacher's
policy function student's state: function .
possible see makings following MDP.
Given observation three assumptions, see that, indeed, teacher's
policy induces probability distribution set possible student actions time k.
implies definition val makes sense here.
Define teacher's MDP TMDP= h; At; P; U i,

X

P (s; s0; at) def
=

2As

(s; as) ; (s;as;at)
0

(i;j defined 1 = j , 0 otherwise). is, probability transition
s0 sum probabilities student's actions induce
transition. reward function expected value u:

X

U (s) def
=

as2As

(s; as) u(as)

Theorem 2 optimal teaching policy given 0 optimal policy TMDP.
Proof: definition, 0 optimal policy TMDP policy 2
maximizes
is,

1
X
X
k( P
0

k=0

2

s;;k (s0 ) U (s0 ))

0

1
X
X
k( P

k=0

However, equal

()

0

0
s;;k (s ) (

2
0

X
as2As

(s0 ; as) u(as)))

1
X
X X (s0; ) P
k

k=0

0



as2As 2
0

0
s;;k (s ) u(as )

know Ps;;k (s0 ) probability s0 state student time
k, given teacher uses current state s. Hence,

X (s0; ) P

2
0



0
s;;k (s )

probability action taken student time k given initial
(current) state s. Upon examination, see (*) identical val( ).
optimal policy used teaching, teacher possess sucient information determine current state student. even case,
allows us calculate upper bound success val( ) teaching policy .
number property learning algorithm, measures degree uence
agent given student.
492

fiOn Partially Controlled Multi-Agent Systems

8. Experimental Study

section describe experimental study embedded teaching. First, define
learning schemes considered, then, describe set results obtained using computer
simulations.

8.1 Learning Schemes

experiment two types students: One uses reinforcement learning algorithm
viewed Q-learning one state, uses Q-learning. choosing
parameters students tried emulate choices made reinforcement learning
literature.
first student, call Blind Q-learner (BQL), perceive rewards,
cannot see teacher acted remember past actions. keeps
one value action, example, q (Coop) q (Defect) case prisoner's
dilemma. update rule following: performed action received reward
R
qnew (a) = (1 , ff) qold(a) + ff R
parameter ff, learning-rate, fixed (unless stated otherwise) 0:1 experiments. wish emphasize although BQL bit less sophisticated \real"
reinforcement learners discussed AI literature (which defined below), popular powerful type learning rule, much discussed used literature
(Narendra & Thathachar, 1989).
second student Q-learner (QL). observe teacher's actions
number possible states. QL maintains Q-value state-action pair.
states encode recent experiences, i.e., past joint actions. update rule is:
qnew (s; a) = (1 , ff) qold (s; a) + ff (R + V (s0))
R reward received upon performing state s; s0 state student
following performance s; called discount factor, 0:9, unless
otherwise noted; V (s0) current estimate value best policy s0 ,
defined maxa2As q (s0 ; a). Q-values initially set zero.
student's update rule tells us Q-values change result new experiences. must also specify Q-values determine behavior. QL
BQL students choose actions based Boltzmann distribution. distribution
associates probability Ps (a) performance action state (P (a)
BQL).
exp(q (a)=T )
q(s; a)=T )
def
P
(
QL
)
P
(

)
=
(BQL)
Ps(a) def
= P exp(exp(
0
q (s; )=T )
2A
2A exp(q (a0)=T )
called temperature . Usually, one starts high value ,
makes action choice random, inducing exploration part student.
slowly reduced, making Q-values play greater role student's choice action.
use following schedule: (0) = 75 (n +1) = (n) 0:9+0:05. schedule
characteristic properties fast initial decay slow later decay. also experiment
fixed temperature.
0

0

493

fiBrafman & Tennenholtz

Approximately Optimal Policy
1

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.8

Fraction Coops

Fraction Coops

1

Two Q-learners

0.6
0.4
0.2

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.8
0.6
0.4
0.2

0

0
0

1

2

3

4 5 6 7
Temperature

8

9 10

0

1

2

3

4 5 6 7
Temperature

8

9 10

Figure 2: Fraction Coops function temperature approximately optimal
policy (left) \teaching" using identical Q-learner (right). curve
corresponds Coop rate fixed number iterations. approx.
optimal policy curves 1000, 5000 10000 iterations nearly identical.

8.2 Blind Q-Learners

Motivated discussion Section 6 concentrate section
following section teaching context prisoner's dilemma. Section 8.4
discuss another type teaching setting. section describes experimental results
BQL. examined policy approximates optimal policy, two teaching
methods rely student model.
8.2.1 Optimal Policy

First show BQLs fit student model Section 7. state space, use
set possible assignment Q-values. continuous subspace <2,
discretize (in order able compute policy), obtaining state space
approximately 40,000 states. Next, notice transitions stochastic function
current state (current Q-values) teacher's action. see notice Q-value
updates function current Q-value payoff; payoff function
teacher's student's actions; student's actions stochastic function
current Q-value. left side Figure 2 see success teaching using policy
generated using dynamic programming solve optimization problem. curve
represents fraction Coops function temperature fixed number
iterations. values means 100 experiments.
8.2.2 Two Q-Learners

also ran experiments two identical BQLs. viewed \teaching" using
another Q-learner. results shown right side Figure 2. temperatures
optimal strategy performs better Q-learning \teaching" strategy. fact
temperatures 1.0 less success rate approaches 1 beneficial later
add temperature decay. However, also see inherent limit ability
494

fiOn Partially Controlled Multi-Agent Systems

Tit-For-Tat
1

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.8

Fraction Coops

Fraction Coops

1

2-Tit-For-Tat

0.6
0.4
0.2

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.8
0.6
0.4
0.2

0

0
0

1

2

3

4 5 6 7
Temperature

8

9 10

0

1

2

3

4 5 6 7
Temperature

8

9 10

Figure 3: Fraction Coops function temperature teaching strategy based
TFT (left) 2TFT (right).
affect behavior higher temperatures. interesting phenomenon phase
transition observed around = 2:5. qualitative explanation phenomenon
high temperature adds randomness student's choice action, makes
probabilities P (a) less extreme. Consequently, ability predict student's behavior
lessens, probability choosing good action. However, randomness
serves lower success rate initially, also guarantees level effective cooperation,
approach 0.5 temperature increases. Finally, notice although
(Coop,Coop) seems like best joint-action pair agents, two interacting Q-learners
never learn play joint strategy consistently, although approach 80% Coops
low temperatures.
8.2.3 Teaching Without Model

teacher precise model student, cannot use techniques
Section 7 derive optimal policy; models, assume teacher
\observe" student's current state (i.e. knows student's Q-values).
therefore explore two teaching methods exploit knowledge game
fact student BQL.
methods motivated basic strategy countering student's move.
basic idea try counter good actions student action lead
high payoff, counter bad actions action give low payoff.
Ideally, would like play Coop student plays Coop, Defect
student plays Defect. course, don't know action student choose,
try predict past actions.
assume Q-values change little one iteration other,
student's likely action next game action took
recent game. Therefore, saw student play Coop previous turn, play
Coop . Similarly, teacher follow Defect student Defect
495

fiBrafman & Tennenholtz

Fraction Coops time

Fraction Coops

1
0.9
0.8
0.7
approximately optimal
Q-learning
Tit-For-Tat
2-Tit-For-Tat

0.6
0.5
2000

4000 6000
Iterations

8000

10000

Figure 4: Fraction Coops function time BQL using temperature decay
scheme Section 8.1. Teaching strategies shown: approximately optimal strategy, Q-learning, TFT, 2TFT.
part. strategy, called Tit-For-Tat (TFT short), well known (Eatwell et al., 1989).
experiments show successful teaching BQL (see Figure 3).
also experimented variant TFT, call 2TFT. strategy
teacher plays Defect observing two consecutive Defects part student.
motivated observation certain situations better let student
enjoy free lunch (that is, match Defect Coop) make Coop look bad
him, may cause Q-value Coop low unlikely
try again. Two consecutive Defects indicate probability student playing
Defect next quite high. results, shown Figure 3, indicate strategy worked
better TFT, ranges temperature, better Q-learning. However,
general, TFT 2TFT gave disappointing results.10
Finally, Figure 4 shows performance four teaching strategies discussed
far incorporate temperature decay. see optimal policy
successful. explained before, teaching easier student predictable,
case temperature lower. temperature decay student spends
time relatively low temperature behaves similarly case fixed,
low temperature. initial high-temperature phase could altered behavior,
observe effects.

8.3 Teaching Q-Learners

Unlike BQL, Q-learners (QL) number possible states encode joint actions
previous games played. QL memory one four possible states, corresponding
four possible joint actions prisoner's dilemma; QL memory
states, encoding sequence joint actions.
complex learning architectures structure, brings certain
problems. One possible problem may structure \teaching-resistant."

10. sense use identical Q-learner implies model student, TFT
2TFT make use model.

496

fiOn Partially Controlled Multi-Agent Systems

Tit-For-Tat

Two Q-learners
1
Fraction Coops

Fraction Coops

1
0.8
0.6
0.4

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.2

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.8
0.6
0.4
0.2

0

0
0

1

2

3

4 5 6 7
Temperature

8

9 10

0

1

2

3

4 5 6 7
Temperature

8

9 10

Figure 5: curve shows fraction Coops QL function temperature
fixed number iterations TFT used teach (left) identical Q-learner used teach (right). Values means 100 experiments.
real threat added computational complexity. mentioned, approximate
optimal teaching policy BQL compute space approximately 40,000
discretized states. representing state BQL requires two numbers, one
Q-value, representing state QL states requires 2m + 1 numbers:
one Q-value state/action pair, one encoding current state. size
corresponding discretized state-space teacher's Markov decision process grows
exponentially m. simplest case memory one (a student four states)
would 1018 states. Since solving problem 40,000 states took 12 hours
sun sparcstation-10, able approximate optimal teaching policies
even simplest QL.
lost. structure may mean complexity, also means
properties exploit. reach surprisingly good results exploiting structure
Q-learners. Moreover, using teaching method introduced previous
section. However, QL method takes new meaning suggests familiar
notions reward punishment. Interestingly, one may recall punishment
major tool approach enforcement social behavior.
choosing actions, QLs \care" immediate rewards, also
current action's effect future rewards. makes suitable reward
punishment scheme. idea following: suppose QL something \bad" (Defect
case). Although cannot reliably counter move move lower
reward, punish later choosing action always gives negative
payoff, matter student plays. achieve following student's Defect
Defect teacher. immediate reward obtained QL playing Defect
may high, also learn associate subsequent punishment Defect action.
Thus, may locally beneficial perform Defect, may able make
long-term rewards Defect less desirable. Similarly, follow student's Coop
reward form Coop teacher, since guarantees positive payoff
student.
497

fiBrafman & Tennenholtz

Fraction Coops time

Fraction Coops

1

Tit-For-Tat
Q-learning

0.9
0.8
0.7
0.6
0.5
2000

4000 6000
Iterations

8000

10000

Figure 6: Fraction Coops QL function time temperature decay TFT
Q-learning teaching strategies.
suggests using Tit-For-Tat again. Notice BQLs, TFT cannot understood
reward/punishment strategy BQLs care immediate outcome
action; value associate action weighted average immediate
payoffs generated playing action.
Figure 5 see success rates TFT function temperature, well
rates Q-learning teaching strategy. latter case, teacher identical
student. apparent TFT extremely successful, especially higher temperatures.
Interestingly, behavior quite different two QLs. Indeed, examine
behavior two QLs, see that, lesser extent, phase change noticed
BQLs still exists. obtain completely different behavior TFT used: Coop levels
increase temperature, reaching almost 100% 3.0. Hence, see TFT works
better student Q-learner exhibits certain level experimentation. Indeed,
examine success teaching strategies low temperature, see
Q-learning performs better TFT. explains behavior TFT QL
temperature decay introduced, described Figure 6. figure, QL seems
effective TFT. probably result fact experiment
student's temperature quite low time.
experiments QL remembers last joint action. experimented
QL memory performance worse. explained follows.
QL memory one more, problem fully observable Markov decision process
teacher plays TFT, TFT deterministic function previous joint action.
know Q-learning converges optimal policy conditions (Watkins &
Dayan, 1992). Adding memory effectively adds irrelevant attributes, which, turn,
causes slower learning rate. also examined whether 2TFT would successful
agents memory two. results shown here, success rate
considerably lower TFT, although better two QLs.
TFT performed well teaching strategy, explained motivation using it.
want produce quantitative explanation, one used predict
success vary various parameters, payoff matrix.
498

fiOn Partially Controlled Multi-Agent Systems

Coop rates function DIF

Fraction Coops

1
0.8
0.6
0.4
0.2
0
-20 -10

0

10

20 30
DIF

40

50

60

Figure 7: Coop rates function DIF = + b + (a + c) , (c + + (b + d)). means
100 experiments, 10000 iterations each. Student's memory 1.
Let student's payoff matrix matrix Figure 1; let p probability
student plays Coop, let q = 1 , p probability student plays
Defect. probabilities function student's Q-values (see description
Section 8.1). Let us assume probabilities p q change considerably
one iteration next. seems especially justified learning rate, ff, small.
Given information, student's expected reward playing Coop?
TFT, teacher's current action student's previous action, also assume
teacher play Coop probability p. Thus, student's expected payoff
playing Coop (p + q b). Since Q-learners care discounted future reward
(not current reward), happens next also important. Since assumed
student cooperated, teacher cooperate next iteration, still
assume p probability student cooperate next, student's expected
payoff next step (p + q c). ignore higher order terms expected reward
playing Coop becomes: p + q b + (p + q c): expected reward Defect thus:
p c + q + (p b + q d): Therefore, TFT succeed teaching strategy when:

p + q b + (p + q c) > p c + q + (p b + q d):
Since initially p = q = 0:5, behavior stage p q approximately equal determine whether TFT succeeds, attempt predict
success TFT based whether:
DIF = + b + (a + c) , [(c + + (b + d))] 0
test hypothesis ran TFT number matrices using Q-learners different
discount factors. results Figure 7 show fraction Coops 10000 iterations
function DIF teacher using TFT, temperature decay. see
DIF reasonable predictor success. 0, almost rates
20%, 8 rates 65%. However, 0 8 successful.
499

fiBrafman & Tennenholtz

8.4 Teaching Design Tool

Section 6 identified class games challenging teach, previous
sections mostly devoted exploring teaching strategies games
student Q-learner. One assumptions made teacher trying
optimize function student's behavior care
order achieve optimal behavior. However, often teacher would like maximize
function depends behavior student's behavior.
case, even simple games discussed Section 6 pose challenge.
section, examine basic coordination problem, block pushing,
objective teaching, teaching essential obtaining good results.
aim section demonstrate point, hence value understanding
embedded teaching. results show teaching strategy achieves much
better performance naive teaching strategy leads behavior much better
two reinforcement learners.
Consider two agents must push block far possible along given path
course 10,000 time units. time unit agent push block along
path, either gently (saving energy) hard (spending much energy). block
move iteration c x h + (2 , x) h units desired direction, h; c > 0
constants x number agents push hard. iteration, agents
paid according distance block pushed. Naturally, agents wish work
little possible paid much possible, payoff iteration
function cost pushing payment received. assume agent prefers
block pushed hard least one agents (guaranteeing reasonable
payment), agent also prefers agent one pushing hard.
denote two actions gentle hard, get related game described
follows:
hard gentle
hard (3,3) (2,6)
gentle (6,2) (1,1)

Notice game falls category games teaching easy.
teacher cares student learn push hard, simply push
gently. However, teacher actually trying maximize distance
block moved, teaching strategy may optimal. Notice
20000 instances hard push; naive teaching strategy mentioned yield
10000 instances hard push. order increase number, need
complex teaching strategy.
results use BQL ff = 0:001. Consider following strategy
teacher: push gently K iterations, start push hard. see,
right selection K , obtain desired behavior. student push hard
time, total number hard push instances improve dramatically.
Figure 8, x coordinate corresponds parameter K , coordinate
corresponds number hard push instances occur 10000 iterations.
results obtained average results 50 trials.
500

fiOn Partially Controlled Multi-Agent Systems

17000
16000
15000
14000
13000
12000
11000
2000

4000

6000

8000

Figure 8: Teaching push hard: number hard push instances student 10000
iterations function number iterations teacher
push hard (avg. 50 trials).
see Figure 8, eciency system non-monotonic
threshold K . behavior obtain appropriate selection K much better
would obtained naive teaching strategy. interesting
note existence sharp phase transition performance neighborhood
optimal K . Finally, mention agents reinforcement learners,
get 7618 instances \push hard", much worse obtained
knowledgeable agent utilizes knowledge uence behavior
agent.

9. Towards General Theory
two case studies presented paper raise natural question whether general,
domain independent techniques PCMAS design exist, whether learned
tools case studies. believe still premature say whether
general theory PCMAS design emerge; requires much additional work. Indeed,
given considerable differences exist two domains explored
paper, given large range multi-agent systems agents envisioned,
doubt existence common low-level techniques PCMAS design. Even within
class rational agents investigated, agents differ considerably
physical, computational, memory capabilities, approach decision making
(e.g., expected utility maximization, maximization worst-case outcomes, minimization
regret). Similarly, problem social-law enforcement take different forms,
example, malicious agents could cooperate among other. However,
abstract view taken, certain important unifying concepts appear, namely, punishment
reward.
Punishment reward abstract descriptions two types high-level feedbacks
controllable agents provide uncontrollable agents. Although punishment
reward take different form meaning two domains, cases, uncon501

fiBrafman & Tennenholtz

trollable agents seem \care" controllable agent's reaction action.
see cases, controllable agents uence uncontrollable agents'
perception worthiness actions. precise manner controllable
agents affect perception differs, cases utilizes inherent aspect
uncertainty uncontrollable agent's world model. case rational agents, despite
perfect knowledge dynamics world, uncertainty remains regarding
outcome non-malicious agents' actions. fixing certain course action
controllable agents, uence malicious agents' perception outcome
actions. case learning agent, one affect perception student's
action affecting basic world model. Hence, seems high-level approach PCMAS design two stages: First, analyze factors uence uncontrollable
agent's perception actions. Next, analyze ability control factors.
retrospect, implicit approach. study social-law enforcement,
used projected game find agent's perception action
changed used indirect mechanism threats enforce perception desired.
study embedded teaching, started analysis different games
possibility affecting agent's perception action games. Next, tried
provide perception. case BQL students, controllable teacher
complete control elements determine student's perception
random nature student's action. Yet, try somehow affect them.
case Q-learners, direct control available factors determining
student's perception. Yet, teacher could control aspects perception,
found sucient.
One might ask representative studies general PCMAS domains,
therefore, relevant insight may provide. chosen two domains
belief represent key aspects types agents studied AI.
AI, study dynamic agents act improve state. agents likely
use information revise assessment state world, much like learning
agents, need make decisions based current information, much like
expected utility maximizers studied. Hence, typical multi-agent systems studied
AI include agents exhibit one properties.
punishment rewards provide conceptual basis designing controllable agents, MDPs supply natural model many domains. particular, MDPs
suitable uncertainty exists, stemming either agents' choices
nature. showed Section 7, least principle, use established techniques
obtain strategies controllable agents problem phrased Markov
decision process. Using MDP perspective cases would require sophisticates tools number important challenges must met first: (1) assumptions
agent's state fully observable environment's state fully observable
unrealistic many domains. assumptions invalid, obtain partially
observable Markov decision process (POMDP) (Sondik, 1978). Unfortunately, although
POMDPs used principle obtain ideal policy agents, current techniques solving POMDPs limited small problems. Hence, practice one
resort heuristic punishment reward strategies. (2) Section 7
502

fiOn Partially Controlled Multi-Agent Systems

one controlling agent. poses natural challenge generalizing tools techniques
MDPs distributed decision making processes.

10. Summary Related Work
paper introduces distinction controllable uncontrollable agents
concept partially controlled multi-agent systems. provides two problems multi-agent
system design naturally fall framework PCMAS design suggests concrete
techniques uencing behavior uncontrollable agents domains.
work contributes AI research introducing exploring promising perspective
system design contributes DES research considering two types structural
assumptions agents, corresponding rational learning agents.
application approach enforcement social behavior introduces new
tool design multi-agent systems, punishment threats. used notion
investigated part explicit design paradigm. Punishment, deterrence, threats
studied political science (Dixit & Nalebuff, 1991; Schelling, 1980); yet,
difference line work (and related game-theoretic models), consider case
dynamic multi-agent system concentrate punishment design issues,
question minimizing number reliable agents needed control system. Unlike
much work multi-agent systems, assume agents rational agents
law-abiding. Rather, assumed designer control agents
deviations social laws uncontrolled agents need rational.
Notice behavior controllable agents may considered irrational cases;
however, eventually lead desired behavior agents. approaches
negotiations viewed incorporating threats. particular, Rosenschein
Genesereth (1985) consider mechanism making deals among rational agents, agents
asked offer joint strategy followed agents declare move
would take agreement joint strategy. latter move viewed
threat describing implications refusing agent's suggested joint strategy.
example, prisoner's dilemma setting agent may propose joint cooperation
threaten defecting otherwise. work first part paper could viewed
examining threat could credible effective particular context
iterative multi-agent interactions.
part study, proposed embedded teaching situated teaching paradigm
suitable modeling wide range teaching instances. modeled teacher
student players iterated two-player game. concentrated particular
iterative game, showed challenging game type. model,
dynamics teacher-student interaction made explicit, clearly delineated
limits placed teacher's ability uence student. showed
detailed model student, optimal teaching policies theoretically generated
viewing teaching problem Markov decision process. performance
optimal teaching policy serves bound agent's ability uence student.
examined ability teach two types reinforcement learners. particular,
showed optimal policy cannot used, use TFT teaching method.
case Q-learners policy successful. Consequently, proposed model
503

fiBrafman & Tennenholtz

explains success. Finally, showed even games teaching
challenging, nevertheless quite useful. Moreover, objective
simply teaching student, even simpler domains present non-trivial
choices. future hope examine learning architectures see whether
lessons learned domain generalized, whether use methods
accelerate learning domains.
number authors discussed reinforcement learning multi-agent systems.
Yanco Stein (1993) examine evolution communication among cooperative reinforcement learners. Sen et al. (1994) use Q-learning induce cooperation two
block pushing robots. Matraic (1995) Parker (1993) consider use reinforcement
learning physical robots. consider features real robots, discussed
paper. Shoham Tennenholtz (1992) examine evolution conventions
society reinforcement learners. Kittock (1994) investigates effects societal structure multi-agent learning. Littman (1994) develops reinforcement learning techniques
agents whose goals opposed, Tan (1993) examines benefit sharing information
among reinforcement learners. Finally, Whitehead (1991) shown n reinforcement
learners observe everything decrease learning time factor
n. However, work concerned teaching, question
much uence one agent another. Lin (1992) explicitly concerned
teaching way accelerating learning enhanced Q-learners. uses experience replay supplies students examples task achieved. remarked
earlier, teaching approach different ours, since teachers embedded
student's domain. Within game theory extensive body work tries
understand evolution cooperation iterated prisoner's dilemma find
good playing strategies (Eatwell et al., 1989). work players
knowledge, teaching issue.
Last least, work important links work conditioning especially
operant conditioning psychology (Mackintosh, 1983). conditioning experiments
experimenter tries induce changes subjects arranging certain relationships
hold environment, explicitly (in operant conditioning) reinforcing
subjects' actions. framework controlled agent plays similar role
experimenter. work uses control-theoretic approach related problem,
applying two basic AI contexts.
main drawback case studies simple domains conducted. typical initial exploration new problems, future work try
remove limiting assumptions models incorporate. example,
embedded teaching context, assumed uncertainty outcome joint action. Similarly, model multi-agent interaction Section 3
symmetric, assuming agents play k roles game, equally
likely play role, etc. Another assumption made malicious agents
\loners" acting own, opposed team agents. Perhaps importantly,
future work identify additional domains naturally described terms
PCMAS formalize general methodology solving PCMAS design problems.
504

fiOn Partially Controlled Multi-Agent Systems

Acknowledgements
grateful Yoav Shoham members Nobotics group Stanford
input, anonymous referees productive comments suggestions.
especially grateful James Kittock comments help improving
presentation paper. research supported fund promotion
research Technion, NSF grant IRI-9220645, AFOSR grant AF F49620-941-0090.

References

Altenberg, L., & Feldman, M. W. (1987). Selection, generalized transmission,
evolution modifier genes. i. reduction principle. Genetics, 559{572.
Bellman, R. (1962). Dynamic Programming. Princeton University Press.
Bond, A. H., & Gasser, L. (1988). Readings Distributed Artificial Intelligence. Ablex
Publishing Corporation.
Briggs, W., & Cook, D. (1995). Flexible Social Laws. Proc. 14th International Joint
Conference Artificial Intelligence, pp. 688{693.
Dixit, A. K., & Nalebuff, B. J. (1991). Thinking strategically : competitive edge
business, politics, everyday life. Norton, New York.
Durfee, E. H., Lesser, V. R., & Corkill, D. D. (1987). Coherent Cooperation Among Communicating Problem Solvers. IEEE Transactions Computers, 36, 1275{1291.
Dwork, C., & Moses, Y. (1990). Knowledge Common Knowledge Byzantine Environment: Crash Failures. Information Computation, 88 (2), 156{186.
Eatwell, J., Milgate, M., & Newman, P. (Eds.). (1989). New Palgrave: Game Theory.
W.W.Norton & Company, Inc.
Fox, M. S. (1981). organizational view distributed systems. IEEE Trans. Sys., Man.,
Cyber., 11, 70{80.
Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.
Gilboa, I., & Matsui, A. (1991). Social stability equilibrium. Econometrica, 59 (3),
859{867.
Gold, M. (1978). Complexity Automaton Identificaion Given Data. Information
Control, 37, 302{320.
Huberman, B. A., & Hogg, T. (1988). Behavior Computational Ecologies.
Huberman, B. A. (Ed.), Ecology Computation. Elsevier Science.
Kaelbling, L. (1990). Learning embedded systems. Ph.D. thesis, Stanford University.
505

fiBrafman & Tennenholtz

Kandori, M., Mailath, G., & Rob, R. (1991). Learning, Mutation Long Equilibria
Games. Mimeo. University Pennsylvania, 1991.
Kinderman, R., & Snell, S. L. (1980). Markov Random Fields Applications.
American Mathematical Society.
Kittock, J. E. (1994). impact locality authority emergent conventions.
Proceedings Twelfth National Conference Artificial Intelligence (AAAI '94),
pp. 420{425.
Kraus, S., & Wilkenfeld, J. (1991). Function Time Cooperative Negotiations.
Proc. AAAI-91, pp. 179{184.
Lin, F., & Wonham, W. (1988). Decentralized control coordination discrete-event
systems. Proceedings 27th IEEE Conf. Decision Control, pp. 1125{1130.
Lin, L. (1992). Self-improving reactive agents based reinforcement learning, planning,
teaching. Machine Learning, 8 (3{4).
Littman, M. (1994). Markov games framework multi-agent reinforcement learning.
Proc. 11th Int. Conf. Mach. Learn.
Luce, R. D., & Raiffa, H. (1957). Games Decisions- Introduction Critical Survey.
John Wiley Sons.
Mackintosh, N. (1983). Conditioning Associative Learning. Oxford University Press.
Malone, T. W. (1987). Modeling Coordination Organizations Markets. Management
Science, 33 (10), 1317{1332.
Mataric, M. J. (1995). Reward Functions Accelerating Learning. Proceedings
11th international conference Machine Learning, pp. 181{189.
Minsky, N. (1991). imposition protocols open distributed systems. IEEE
Transactions Software Engineering, 17 (2), 183{195.
Moses, Y., & Tennenholtz, M. (1995). Artificial social systems. Computers Artificial
Intelligence, 14 (6), 533{562.
Narendra, K., & Thathachar, M. A. L. (1989). Learning Automata: Introduction.
Prentice Hall.
Owen, G. (1982). Game Theory (2nd Ed.). Academic Press.
Parker, L. E. (1993). Learning Cooperative Robot Teams. Proceedings IJCAI-93
Workshop Dynamically Interacting Robots.
Ramadge, P., & Wonham, W. (1989). Control Discrete Event Systems. Proceedings
IEEE, 77 (1), 81{98.
Rosenschein, J. S., & Genesereth, M. R. (1985). Deals Among Rational Agents. Proc.
9th International Joint Conference Artificial Intelligence, pp. 91{99.
506

fiOn Partially Controlled Multi-Agent Systems

Schelling, T. (1980). Strategy Con ict. Harvard University Press.
Sen, S., Sekaran, M., & Hale, J. (1994). Learning coordinate without sharing information.
Proc. AAAI-94, pp. 426{431.
Shoham, Y., & Tennenholtz, M. (1992). Emergent Conventions Multi-Agent Systems:
initial experimental results observations. Proc. 3rd International Conference Principles Knowledge Representation Reasoning, pp. 225{231.
Shoham, Y., & Tennenholtz, M. (1995). Social Laws Artificial Agent Societies: Off-line
Design. Artificial Inteligence, 73.
Sondik, E. J. (1978). optimal control partially observable markov processes
infinite horizon: Discounted costs. Operations Research, 26 (2).
Sutton, R. (1988). Learning predict method temporal differences. Mach. Lear.,
3 (1), 9{44.
Tan, M. (1993). Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents.
Proceedings 10th International Conference Machine Learning.
Watkins, C. (1989). Learning Delayed Rewards. Ph.D. thesis, Cambridge University.
Watkins, C., & Dayan, P. (1992). Q-learning. Machine Learning, 8 (3/4), 279{292.
Weidlich, W., & Haag, G. (1983). Concepts Models Quantitative Sociology;
Dynamics Interacting Populations. Springer-Verlag.
Whitehead, S. (1991). complexity analysis cooperative mechanisms reinforcement
learning. Proceedings AAAI-91, pp. 607{613.
Yanco, H., & Stein, L. (1993). Adaptive Communication Protocol Cooperating
Mobile Robots. Animal Animats: Proceedings Second International
Conference Simulation Adaptive Behavior, pp. 478{485.
Zlotkin, G., & Rosenschein, J. S. (1993). Domain Theory Task Oriented Negotiation.
Proc. 13th International Joint Conference Artificial Intelligence, pp. 416{422.

507

fi
fffi


! #"$ % '&)(+*', --.0/1,32-54678

FHGJILKNMPO:K=ILQSR

9:;<= >?,,3@-0A:BDC;
%&>E(@-.

T+MPUWVYX[Z]\^V_G`FHGJVbaWO:cWdfegTbGihjO:cWI?M1O:V_ckVL\
l

TbcWTbGJImQSnSVodoO!KpFHGJVod_GJILhqZ

rostvu#wyxzw|{^}ff~{

1L$

J0!v^L!0_5bo
$ Y0vN^[ ?05^mE$ffE?0v

gE^Pv
:5LD D?^3^5^o_=3:D3:y^ o! ^:y^::5DE^
3^3o^So33o3DE[ !^3_5Do^S0o3D3DL!^5L
3: 3ff:5?:^ :D3oPD[#3v ?:D3^?D Y^S L033503
:^ $ o^3: :5LD 0 :m=b!)330!L:3^Y
3ff !3LD3D:!3^^^# 5^o$ : ::5N#3^^3:035=:)
3 5m 5D5= 5^!!b3:1151 !3LD3?!!1
5)3: _55#:5)^ JDN5#!^mffff^ff':5^3^5^
o5 0:$ _5:3^33[L:3^W3 !3LD3
^^!:5D^E3^5^o:3y3:3:3 _3:off'L^3)3:
:^b3oD:^o!L: !Wm!^N^3:5L^ o^!:5D^^b3^5^o:


ff
fi

fi "!$#&%')(*
+-,.
-/01.
24356 (*
27fi84fi9/- :fi 6;.<=/2fi9/-.>(?

@AfiB(*
2<C(D FE /-.G
24fi- .5H&fiGI.@/@
fi9/-6KJMLONP./Gfi 6RQTSVUUW=0Gfi 6RQXfi9
fi
fi 6Y(? 6;TSVUUW=0Z\[
: ] fi^_H.`D
5%'1 fi9/-YfiGa. b/-c(?fiIF
2c&ff
fi9/-
4fidJAeI -6
/-+.=/f/-, G/-&
fi Aff
2--/- P.
2J.g<NP
-/-a.fi_hi$/-.Gfijfi)P

fi
lk:VmXfi-nM1oqp5ff
2/HcSVUrsMtcuv3fi 4H\SVUUwMtcN> /OoxQ8yH:SVUUMSz0Z|{j, d.</2fi9/-}(
Aff
fi9/-
2ebn~Cfi9
-n%': fi9/-;fic(fiI
2K ~Xfi9
-nHSVUr0cfi.6~X,.fi%': @/@
2I.ff/-b3.fi9/-
~X,.fiHSVUrr0Hv\J_fi26/-ff
2.fi9/-Y 6 /- ZK{j, ff
(*
2^/-, ^-/-I 6FL(X/-ff
2<.fi9/-Y(
B!=#&%':lZ'5ZbHcp52,
2ffLco
-/-HBSVUUW=0jjfiO&
2/2fi=//-.Z
{CmXOfi2-\(jB!=#&%'\/-,.fi9/\JA,.fi43+mm:Z
4Z/Z/-ff
_fi9/-Yfi9
2/-, ^-9E 4fi6fiffL5<fi 6
fiff /2fi9J.j
2
fiXlN> /oQ8H SVUUMStFNP /o`f6F
2-, lH SVUUMSz0Zhi+(fiff/HMN> /fi.6^Q8
TSVUUMSz0e
43</-,.fi9/+(j fi9/-;fi)a_ b/-<(?fiIF
2).
-A
fi9/-6=/-/-, <
2M(C/-,
-LHv/-,
(?
1fi$LdfiffL5e

fiyHAfi-65E 6.ff
2b3fi9/- jm>b/-,dfi9
2J.b/@
fi9
-L-ff/-d
2I c(D
6R$I.ff
2
/-ff
2<.fi9/-ZX{j, :$3ff
2-e(f/-, P
2-I /H.lZ'ZbH_(ffi
2
fi/-ff
2<.fi9/-j(?
GfiB
2I 6$I.ff
2H
/-, Yb/11fiffL5H, 6 bLI. 6 ff
G/-, fi--I /-O/-,.fi9/1/-, )
2
fic 5E.I .6 ff
2 5%bZ
N> /ffi.6e`f6F
-2, &TSVUUMSz0f-/2fi9J.-,)fi.fiI.B
-I b/-e/-ff
2.fi9/-\(?
-9E 4fi6+fiff /2fi9J_

2
fiH&/-, >/-\m:Z
4Z/ZA/-, +`v
2K-ff/-R
2I HAmP,.2,R-ff/-:/-, +(*/-@/b/-ff
fi(Xfi
MI ff
-LZ
[fI .6 ff
2 ^1fiRfi9J_
27fi"(?
2(D/-ff
2<.fi9/-OmP, ,dfi9
2-1fi1-$Yfi:fi< 5E
6
fi9/-6Ofi9/-X-ff/-6AH fi ]5.fi 6Z'5ZbH lN> /CoQlH&SVUUW=0Z{Be/@
24fi9/jfi-e 5E
6
fi9/-6+fi9/-<H~C,.fiTSVUrr0B$/@
2M6 6efij
$6
2n5 4mP)fi~X,.fiA%' @/@
2I.ff/-b3v fi9/-AZ
- +~X,.fiA%' @/@
ff/-b3 fi9/-AH dfi9
22,.
2fTSVUUs0X-, Vm6</-,.fi9/X/-, /- (fiffLMb/ L
fi 6}fiff /2fi9J.b/ L
2V3M6.fid^_ff/-^,.fi9
fiff/-ff
284fi9/-(j

fi\/-,.fi9/)/-ff
2<.fi9/-^(?
fi

2I .6K$I ff
Z
JW = +v





X







z

, --. v %% !> 5 > b0 P !fi< ;!
%&%

^&0%|% 0 >

fiO.5M .F
j.G.-Kff.29.b<)_ - 1 ff. -fff5b^Pb-O)2<$-G
-A5 +- ff*2j 245 :ff.29.b+<4cAX9- ff )Aff2-<Aj P_-._9Aff
1- ff;O<ff- $.?\ 45 7-ff2<.9-dP-Y2@Aff\-O- ^v2O-ff-
2 D$ -.db@-<-<$-< ?279-+A-2b. 2)@.b)$-R X
_9--457DP 7f9F- K G_9-C 2V7-)Aff55- 1-.ffj G-)&1ff 29_
y- -:- b-P9:). 7- .1-.9P-.:2_A 2C-ff2<.9- e:4
- cD2<-ff-y2.jj c ^A--K> .\- 2R^X4O-.9 2*9-
ff..<<vj$ P791=@2M G- - Cv9.@i=ff P ffGvjX 2V
-KAff 29. -KAff5.@i=fffP R@249-\-.+=ff22
4-Rlv+ff5K <ff 29.Vi2 ff)-dA^>2 ff-4X --. -
FFff.29.b) ff }-;29Yz_TT_=ff?_}O$-9-7- -K - +
A@- <ff- $.O* 45 -ff2.9-yv ffc 2 G9 .OF
2-.b-e-R 2<c?27.7 2.<:; F -.242 5i_9-- 94X
- V-.9C- P_ ?$*d?5)4ffzGvff7 2.47AG*27 7^_$-e$
<4 XfyFFff 29.1 2j X.245 j+*-P$.ff2:lF &. 7$.ff2
-.9P4yAG.ff- @Xff2A
7- KFC7- Glj 2$$.M'.K$C 4Pb-OX.A'X -@2
-be 9-Y &<-..@Aff-y-.c.2$?G- 4>>-_9>- ffd $.bKB?
- :42:f 9-yP. b-F2
9 2< 2VM.c-.ff- +?.245 1-ff_9-+BB=&'MM
.. +- e22 b-P 1 YfF222 fff5+ Yff 29.).2<1j
2ff <C- cff-.$ R\?A?y9^-)X 4be4ff2<e-
F4j_2&- j<ff- $+A> .^fF2- "TVMz _bc- P.-j&-M\+ -7=-
?279-A. db4>P-< $-*<-.-\9-->D- : P -\-ff2.9-KG
$Be- -F- D2>-ff-\ 2Vff4=- ^.-.992bA- V
-.9@5@-<_-)e- X 2 < v_9 45 G-.b29.D?2749-e
.<=29-*P 2.<K 5 < - >24-. 5
j.7_9Aff^+2 ?VP j.K M2ff- $2 -<-ff2<
2.92&T
5ff-
) 7e- - CffMb 7 Kff 29.b 792G 2-$-A
5ff-.G$M& $=2FG-ff2.9-bc ff. b- Xfff.29.bXi
Fff-
)-.-
ff..b- 92+$-9-Y7<ff- y?1 2VM.-ff_9-Aff

f.&fi
Fff-
-<c. -.9+bA+j G_9Aff1: 5- .R2ffM-Rff2-YG d922 2l
V
!" #

j <?4> 29-;PDA.-AK?V|v2R@5=2$} --.+-_9^-@2
@29-- +Pb-O^49.b2Bff@-ffP2ff.2-$->)929.FP - ff@@2 X2ff 22=G @2$-
-ff2<\ 29- 4&
%>9-@M)A:9^*- .-M(
'*),+-)/.5Ob-ffD\b- ff)
9-0
'B 12 )4343432)51$6.>+ 9-R9-8
7!'" 14 )4343432)51$6.GOM.
1:9<; .> M.b
= 1?9@
> ;-fP ff2 = M.$-b.e4ff+-<OAff2.9.:. V:X-.929.c$ -2 y?
1 ;
M.- $_b-92-4 2F @?.?>. - $B
Az> M.b
= 1&9C
> ;-Cj. E D)E FGfbP>29-@_9_1.FG PG

. @2 HJ9L
> K7P.2<b-bvG
l 5- _X.ff1.T2@M DO& -OM7+ N) j)_ b--ffPf 2
OQPSR 4 )434343T) R U 3
V5WTX

fiY Z\[^]`_ba`cedgfZihj_balknml_o[`ap[`qrsfafZ\ktvu[Ic_owY Z\[`c`Z\k`h x

y{zb|~}@^y{}i5zovM|~njMij:zoozb|~5M Mvn/(Q445zovizb|~
~4^i4i4ffnozb|~Mo4iMizog4iM|~4^(
&|/,2|i4|~4in\M,ni|~n4G}`MTnni,Mn~4s|~Mn4-|" ,4,nol|~zon
y{zb|~}ffgi,^44il,MTziEM5MooG4~,zb4M*noo2y:4iMG~l~|~zb|~l|~zonL4&$524442/i$`n
yG:i4M|~^{g|~}{4^Mozb|EM,&G<5i44,vl5jiMM"4|~4&|~n
zoiMo-|~}is"i,zbn|~zonini5 zb|~MMin4l442,My{zo|~}v&^",|~}i4nl|~4Mi/yG
~l`/|~zb|~l|~zon4n|~}i4|~}i:Mi~y,EM,sMi|,Mzoi45n0|~}i:i4|~zonn!{/44~:5
y{}i5g-M-|~zb4T |~}iGn,z`o4iM|j^44l~,zo:zoiM zo/|,Mi4M"4ni,zoi|~}iGi5nM5M


}ifiMi~y&|~|~}ifi^i~<!ezo&
?
Tfi2Mvn"Ti ,4,nol|~zon"
,4~nl|~zonvMin4-|~4y{zb|~}G}M i,^44il,M ~}2y|~}i4M~,4|~i4~n*n"Ti ,4,nol|~zon
yGff,}i^n~Mi,nMM ~4&M"|~z4G|~}iG g4no|~zoneGE TM5 }izog~4&M"|~z4gzo{
|~lMzo"|~,i,|,|~zonfinG
M&~|ffnizb|~zoni4ff

-|~izb|~zo4bn|~}iG 4no|~zon
ng&i,nMM
fii4iM|~4^
fi5z|~}ii5/|M,iff|~}4M~fiMi|,Mzoi4fi^e,M4zi&|~}i
zoozo2|~znn2M5}p4EMi~ffn
fi y{zb|~}Mp4^izbnMo4i4M

|:zo{4ni~|/,i|~4eM:nooTy{4
4o2yff
^M"|~zo42:&2444T/"
M:~(,4E|~zon(/\v!n-^44l~,zivzofi}2^zi,4,ni4"|~


z -4M|-44lszo&|~}i}i2MnM-(4Mi~|~}4Mi(|~}iM,iE
444T/M!"$#&%4M('
M|~}~y{zo~Miz|~}i4M&|~}iEM5ig)
+*5, 'zo)
-Mi/
. 444T(.10 ly{zb|~}
2 ,ffMoI|~}i4Mi,4n3
fi y:zb|~}(}i2M(/\vn"*ly:zb|~
} .*?5 4 4442(4 j
|~}4Mip|~}ivEM,6
&444$/7
9 8;: = < 0?> @{!p /5*y{}i5ff
@zos|~}i
,|:nn,zo4{n
.`{jzoff5 4 &p44M/
4 5!Mi&24442/5E,4~}
n,z`o44





zo`Mobn*|~}iEno2y{zoB5~,(,4"$#DC9*E"GF$CHJI6%&,Mii4~(|~}|ff|~}4-`Mozb|e|~}i4M,en
$ fiG!44n4G|~}i,MM|~}inffn|~}iL
K:,i5Mi(izb,~M
&2444T/M O*4444TPO!MQ &GROp44RO!5
M{~ii|~zn(/^vn
S` 444$POVU5
NM 444T/MsT
M{~zo/|~zoi|Gii|~zon(/\vno Mi)S
4
W
M{~(|~5X
4ff2 |4i ^44l,{zo/
4n
NM

}~niii4~gn "Tl" ,4~nl|~zonyff 2 |4Ig g~4M-|~zo4gnooTy{g5n
$fiY ZR

!{ p44&: /5

y{}i57e^M"|~zo42Mo|~}ff54M5zo4gn|~}iffM,iE^"Tl" ,4~nl|~zon(zog4no|~
nibEM:-i5zo4}2^zovM|~,zo|~zoii,zbn|~zon[

(M|4G}M G,-44l,zoiM|i4
zoQ}`MMzol`izb|~(i,zoM|~zon* s?4ni~4^i4i4M:|~}iM|~zonnzolizo|~Tvi,zbn|~zon<zo
\(]\

fi^_V`Aa;bcedV`fc

ghJikjJlemjGn;okpqrVgqptsvuwVxeoxeojzy{(hJ|Vleq}+~hJ{kiPwqo=iPpnTh$~ffiPq{}xegVjiPxeh$gTh$~ o|tqGjJoPq
iPwqLghJiPxh$gh$~pq{xeJjiPxeh$g)xeoh$~{xjJlx}ythJ{PijJgqJs[uwq{(q~hJ{qJAmq{q~9q{wq{qiPhjJg/jJliPq{(gVjiPxq
pqrVgVxiPxeh$g6h$~wVjJgo3yV{h qpVf{q1$xqgB| nBkj{(wxehJ{(x$$(;mwq{(qiPwVq1of|i={qqooqpffiPh{(qoPh$lq
gqjiPxq6lexiPq{jJleo7j{q|VxeleixegkjiPhJyf5phmg/mjn$th$goPi={iPxegiPwqx{7|{(jJg(wqo7xgyj{(jJlelqls7oLj
h$goPq;qgqJViPwq})jJxegpVq{x$jiPxeh$gxeoxegrVgxiPqxe~3ji7leqGjJo=ih$gqLh$~iPwqoqoPf|i={(qqoxeoxegfrgxiPqJs
uq{}xegVjiPxeh$gh$~ opqytqgpoh$giPwVqoPqleqiPxeh$g{leqJshJ{xego=ijJgVqJfiPwqLy{h$J{jJ}
N



iPq{}xegVjiPqoxe~iPwqk{h$leh$oPqleqiPxeh$gZ{lqJmwxe(wwVh h$oPqo)iPwVq/leq~iP}h$oPilexiPq{(jJl7h$~j q{n$xeo
oPqps[1fi iPwqy{h$J{jJ}ph;qoghJiiPq{}xegVjiPq1xe~iPwqoPqleqiPxh$g{lqm7wxew(wh;h$oPqoiPwq{xe$w iP}h$o=i
lexiPq{jJlh$~6jz Vq{Pnxeo)oPqpsqoPwVjJllh$gVoPxepq{)iPwqE$qgq{(jJlxeGjiPxeh$gh$~ffiPwqE{h$leh$zoPqlqiPxeh$g
{lqiPhyV{h$J{(jJ}oh$g ijJxegxegV6h$gVo=i={(jJxeg iPofmwxw)pqljGn;oiPwVqoqleqiPxeh$gh$~3y{xe}xiPxqh$gVo=i={(jJxeg iPo
jJo~h$lelhGmo/iPwq/leq~iP}h$o=ilexiPq{(jJlh$~Lj q{nm7wxewxeoghJijy{xe}xiPxqxegq;VjJlexinxowh$oqgts
hJ{6oPxe}yVlexexin$mqh$g iPxeg VqiPh{q~9q{iPhiPwxoLoPqleqiPxeh$g{(leqjJoiPwqPEP$f,sg
G i={qqiPwVjixeohJ|VijJxegqp|;nVoPxeg6iPwq{h$leh$oPqlqiPxeh$g{lqxoGjJleleqp ,V i={qqJs
uhy{hGqiPq{(}xegjiPxeh$gh$~leh$$xey{h$J{(jJ}oJoPxij|Vlq~9giPxeh$go[~{h$}J{h$gpjiPh$}oiPhLgjiPf{(jJl
g;}B|tq{o GjJlleqpleqql})jyyVxg$o,m7xelel|qLVoPqptsqi7pVqghJiPqLiPwqLq{P|{(jJgVp)|jJoPqh$~s
[9RvAf V; $1V9 ~9hJ{ffBxeoj~gViPxeh$g[~{h$}
)
gVjiPf{jJltg }B|tq{oGs

iPh


Rleqql})jyyVxgxeoqAiPqgpqpiPhgqjiPqpJ{(h$gpjiPh$}o| n?[$ DsqphghJigVqqpiPh
qAiPqgpiPwxeoghJiPxeh$gjJleoPhiPh)h$go=i={(jJxeg iPot|tqGjJoPqffiPwqn{(qy{qoPqg iiPq{}xgVjiPxegjiPh$}xejJiPxeh$gVos
hGmqq{GghJiPqiPwji7iPwqy{qoPqgVqh$~h$go=i={jJxeg,iPoxegj;q{Pnxe
g qgqoiPq{}xegVjiPxh$gtf|tqGjJoPqJ
~9hJ{xego=ijJgVqJVjpq{x$jiPxeh$g)rVgVxiPqln~jJxeleox~3jJggVojiPxeo=rj|leq7h$gVo=i={(jJxeg ixeooPqlqiPqpts


fffi
f{B}qiPwh pmxelel|tq|jJoPqph$giPwqgVhJiPxeh$goBh$~jJnAlexexinjJgpzjJqyij|Vxelexei5n$mwVxewj{q)oPqp

iPhwj{(jJiPq{xeqj6ljJoPoh$~iPq{}xegVjiPxegy{h$J{(jJ}oms?{s?isfjJgj{|Vxi={(j{PnjJgpiPwq{h$leh$BoPqlqiPxeh$g
{lqJ1{(qo=ytqiPxqln$s giPwxeooPqiPxeh$gZm1qk{qGjJleliPwq/pVqrVgxiPxeh$gh$~jJn;lexxi5n$7jJgVpoPh$}qoPq~9l
{qoPVliPo~{(h$} kj{(wxehJ{(xt$$(m7wxeleqjJqyij|Vxelxi5n)mxellV|tqLpxeooPoPqpxe"
g !fqiPxeh$$
g #As
7yijJgpZqq} $;o=iPpfniPq{}xegVjiPxeh$gzh$~ oms?{s?isjJgZj{P|xi={(j{PnoPqleqiPxeh$g{leqJs
uwqn)xeg i={h;pqiPwq~9h$lelehmxegBqleqjJg io=nAg,ijJiPxghJiPxeh$gts
[9&%tv(' *)$fA,+.-./,
-A*03 y{(h$J{(jJ} !xeo21GD4365e2575$ tx~
)
~9hJ{jJlelJ{h$gVp/xegoPijJgqo98 ;:=<2>2?2?2?@>A:6B h$~ljJoPqoh$~ m1q6wVjGqffiPwji 8 DC :ffE wh$lpo~9hJ{
jJleG
l FIHJ
HLKos?is :ffE xo7ghJijh$gVo=i={(jJxeg xeo621&xe~[iPwq{q6qfxeo=iPojleqql})jyyVxegVos?is
xeo7jJn;lexms?{Gs?isfDs


~jy{h$J{(jJ} xeojJn;lxeJiPwqgjJlel3J{(h$gp q{(xeqo7wjqh$glnrgxiPqpVq{x$jiPxeh$gojJgpEwqgq
iPq{}xegVjiPqJsu3hqAiPqgpiPwxeo{qoliiPhLgh$gA5J{h$gVp6;q{xqo$iPwq~h$llehGmxg7gVhJiPxeh$gh$~|th$gpVqpgqoPo
xeooqpts
[9&%tXAM /ON;f PON -Q) 3qit|tqBjleqql[}jyyxegAs7
)
|th$gpqpEms?{s?is?xe~3~hJ{7qq{PnEUHVFWHXKViPwqLoPqi

Vq{PnSR

YR E [Z :ff\E :6\E xeojJ{h$gVpxego=ijJgqLh$~ :6E]

^`_*a

:=<T>2?2?2?T>A:6B xeo

fib6c.d
egfihgjk=lcnmfffihoqpfrdghSdgsutvlhwlc.owxzyGdDjwfr{$b6c.dgjgc.ogm6|

}r~ww}iAO



OA}r2AwO`qnn
n(}2~("qnnn2 9nOn222n(@"AgO4OLO.2r}r
v
n(qOO"(Jqwnn2
nAuwO~qnizwn}iAn(}iqA}rqn~6YTY2Qw2A}rqO~ww}iA=O}r(O
n62q(~(q.Aw}r~(2~(nin
2~nOGnq*nwffAIAw6q~A~A}rw}rr}iq.wqnwn(}rn.n~AATOw~A}rn
wO~I2qn~(wA}in2A}rqwOw(O}r"J2quwrA(g`OA(}rTA}rq7`(n}rO`}@qq`
}i(~A2O(4Or}r29An2qn2nvqA`u}rgA}rqUYTY2gO"Ag}i`A~A2r2A}rq"(nrO
4Q*g&rAO*gqg@n wQnL.
QQ*z
nA$}~2`zgqQY@Y2}
Or}iA~Iqw@nn`}iOA}qn~}rJ(Iwn}iAOw(qO`O}r~IzDq
}rOrnO.gQw(}r2~
(A`u}rgA}rnUYTY2wJ


g*
i29`SOL2 wAO qg Tv;(SVAO.g(2w
fiff.2 O2`
gTnA(qv 4 (Og7OO 4(O.gA2wUOgu6n

g*
[2 `J4zgQwAqO fiff.2 ff.A!q"J# O$wQ%&%'(")
* , +I 92 .-/r(0%1%32 * (+42O O2`52w76 v(O.gA-/r(8%1%w 9 "92`zgqQ:
`q n2O(2~<;
>=4On?;
A@}iqrrT9~IAwA(}rwA}rww(qO`O~2q}rn2}n}iAO.2r}r
n(qOOu~On AwOO.2r}rJw(qO`O~"QwASgO~4ww}iA4qgTn7`2}rOn qni}r}i
}r~9qnnn299OA}r2Awvn2Sn2A}rq O~vww}iAO}rr(}r~9O~(~An2 w2O(2B;
A@un
2~nO
nqrwOv}rw~(On2OAnn`qO`OC
HJILK8M
!EgGF
T!EDQNF

I!ED8M
Pw

GFQM

}r~A(}rwA}nugqnnn(}ngI}iI}r~=nOvO
2r}O
}rnw}rnUr2ww}rnOWn`T.}rnzO
2r}2}i}~W(TA}ivn(
22~A~20R v(2An`TOn
TSV
U.(n(?
WXSv22O(AOJ@qY =4O$,AnO`qnqLn(2~A2Q(A}rq qUO(}qn~A2(nn}Qn2~4O
2qn~`nA}rnJr2ng}rnq~2
w9qrr@}rn~(2A}rq4}rrrn~A`A2~6n@O}A(2~AA}rnw(Owr2 }ruwqnuqwOAqn}r=(TO~(qn}rn
TO$O(O}r22On}ruwr22A2O~vO"O.2r}rn`qO`O"
[Z]\4_^a`'bgdc8efhg5ejilknmpogle"q
wr
Lr.~O`r }r~O`JnA}rqqI4n`Owr2 }r n`(OO(O(~Qwz
n
qn(}u}iA}rOA}rqw~} "~(}rgrUO(r~A2Oz}rn~(Ow2v}rr~A~Aq6@tsuq`?v9`u2qn~(}rn
~A}ruwr=(~(}rqq6An}r~9n(Ow2
wUO2(wOA}@xtxq` n(z(A(2wQ!r
~zyn8{@'|*On
A(2zn~} }D(2Qvq~(}iA}rqn
~ '
Qj
uq u(wrOU wr
L rTO22}rAn@OnOAnwQ! r"O
q$qnUqGAw2~Aq~A}iA}rqw~2.Ow$}iITO$T2(q qnq~(}iA}rqAOnOAn@ nw(Owr2
2qn~A}~A~9q~A22}.}rnzq~A~A}rwr2qwqA}rqn~2D}OigAwq~AOw(O}rn2"(q An}n}iA}O~A}rAwA}rq

`O(}rn~A2
n2n2qq~A~A}iwu@2~2=9$ OuwrqOS}rn}iA}O~(}iAwA}rq}r~q}i2}r
}rq(zq
@O~ r
}D@
xOq}i2~92On~(OQ(n(2~(2(A}rqzq.An}~n`Owr2
TOn~qw(Onq~
2qnn}rA}rqn~2h
v9(O(JnAAnn(Owr2 n~A}rwu AAQOn
vvT2~2~A}iAgA}rq"TOr2wrn~
P

fi~1

b

p

c
q

r

0~t1P5t/~L(Y!~

( lLLhttP&~L!pt[YLL.YLLlL~t!7l!.L!.,j
tP[l [L !Y"Y>~L ~5~~lL~t :!YY [L Y#>~L ~5t
~L p YLY /"Yh>LPa~~!~lL~t a!at~5tajatY[
YY [L 5!Y5"Y#~L ~L~lLt 0~Y~tP[[t[TY1l!#L!j
L~~L#YL7L~~LY4LLlLtjY LpLLlLtY!Y~Pt
j!"YPp~5L5: ? YL?twLlj5!!L!lL~tjjL~LhY!wYj
Y!Y~YaL.t~~<~fi&08j:





%

?



Lt
N


:'P4 .ff
fi
ff
ff



!"ff

$#
:'Pff
j' &) ('
+*& ,/.00 &/)(213&4
:ff
' &fi
5 0 (fi
:'6

.87-94) 15
' &8') &8
'
:'6

.87-940 (') &8
fi
(;< : /' &>=
:'Pff
j' &) ('
&/.@1A&
B
:ff
' &fi
5 0 (fi
C 78 95*78$ '0 &/) (fiD &/.D &8 '
:'$:ff
j' &/D (') &8
E=
:'F 75G
,H.' &/ IJ') &8
K
:'$:ff
j' &/ 15
0 I8') &8
>=
:'F 75G
,H.' &/ IJ') &8
K
:'$:ff
j' &/ 15
F Mff8') &8
'
:'$:ff
jF 15
0 I8') &8
>=
:'O
.87-90' (') &/
K


8

6

8
P
J



.
0


'
/
(



&
8 E=
C
78- 95*7P '' &) (fiA *- ,/.0' &) (2QO') &/ 4R=

SFT6U

fiVXW/Y8Z[]\^`_baGWcd[]\He fH[gY\hYikjlaG\ffaGW/effmon>Y^ff[gp VXW/Y^W/ecXq

rtsGuwvffx8x6y8z{G|}~'d)8
v8-}ff~-vffx~'XD)8E
r ruh|/8H~~)J-G~D@DH-~Ox>/ff2)B
v8-}ff~-vffx~2DJ-)ff/
v8-}ff~-vffx~D@DH)ff/
v8-}ff~-vffx~6x>D/ff)ff/>
'b v5z~' O50O20@05 ffg80 5tffFt't 8)'-)))'>A8uF))FuF)DFAOuF
l
$tAff3uF'5t' uF'5A)-uF 5'O6G ggO r ff '/ 0l00b 5 0O
g r 0JOOgffOE0bg]0 0g ff 0g 'O 0 v/G}" O'']OE"O gEJ '0]ffg
g ]GO0] 0g 0ff'O 0g |-H8H "O 0g gJ '0]ffg w ]GO
0] 0g
ff0 0 ggt"g gGOJ g "F"5 ' 0' G O50O@0lgO 0
E0gg0 50'"g0 ] $gb5 gg) u g s8
5uO-s8
6AdJuOGs8
/'t58 '-uO-s8

ff8G> uO

" r
"
"


g g >05'6 FuF
g g >05't'5 FuF
g g >05'-'55 FuF
500"0

5F-t> uO8"
3/3- uO-"
"gb )$0FO00 ff
fi`g /Ogg E
ff'O+5F 0O 0 O)0g ff8ff'O00 8 k ff @J 'gO KO
ffg0Og 0' 5g) ffO 0 O)0g "Off' 'O gg g0g]0 0g0g
X0lO 0'OO 0 "5' g"!O 0# ' v/G}~Fvx~z%$8
0 ffOOG0 >]0 -}'x &)( '] 0g X]0 "]0 )@+* , .-0 /

2 1/GA , u)uF
3l500 "0g4 0 6
5 bO G 0g bff]0 g ' 0

798:;<;>=)?A@ B C+DE=GFHJILKMHJBNPO
D0lF8g X0O0g @ G0OO$t0'k 0g QR 0]) 00OO0g
`ff'6GO`8 d0o50g ` /OggO] 50g ` OOff ffgg] .S
TVU 'O0F JXr W W8ru g 0 +5bff'6/g 0'k 0g ffQMYRZ 0 U ' g 0OO0g g
DK0ffgo0O0g @$'O g0ff50g 0 0"]00 0O 'O g0+' ''5'
Xr W W [GuF\
/OggO] OO ffg] ggO offgff g40+ g6"g 0O0g 0ff'6/
5'l 0g 08 g5F6/g 0' g 0g $ ]
QMYRZ b 0 U ' 0OgO0g g
D5 0) 0' g 0g g wff' "g0 'O0JO$00 U ' g
0OgO0 ff U FO0' DXr W W8ru g-)F 050 OO ffg' E g

^`_2a

fibdcfe>gh)i"jfe<i
k)lnmpo"l korqtsunqpvxwyl kzmp{)v|q`un}~vxl k)w)o"mpo"l kz)qpvxwzmpldw)v k)v|un x"or)l n`un}qxZv>xv )mmp{fumxlnu
nl )kfw~o"kfqmunk)xv#'xxx'p9~l Muxrun)qpvnmp{)v#mpvxqm V 9JorqAv ln}vxwl k))kYmpo"mp{)v
)qpm"ompv `unM {)o"`{uno""qx{)o"qo"qqp)xo"vxkYmqpo"k)xvnw))vmpl~mp{)vAl "l qvx"vx mpo"l k\)"vnfrompv `un"q
unmpv o"" k)lnmsZvtqpvxrvx mpvxwAlxl }~f<mpv ])uxrunqpq4l }~lw)vx"ql |){fv vt'un""vxw>p' "Xp
''o"qfqpvxw{)vl ""l o"k)6k)lnmpo"l korq)qpvxw{fv~p `.'.nzl +unko"kmpv )v mumpo"l k\mplu
qpv m#l `vxrumpo"l k)qx<w)vxkflnmpvxwds~J forqmp{)vqpv ml ffumpl }ql M6{fu'o"kfmp{)vxo4vxrumporl k)q4o"k+
6Yff2Ln)f> Y<Y>f v mz svmp{)v"v'unqpmqpv mPl Avxumpo"l k)qqxmxmp{)v
vxrumporl k)ql lxx<p`o"k)6o"kyk)vxJumpvxwumpl }quvork+Aunk)wyo"unkvxrvx}~vxkYml lxx<qorkmp{)v
{)v'unwl +u~xun)qpvnmp{)vxkun"mp{)vvxrumporl k)qtlYxx<o"k)o"k\mp{)vsZlw<dl Amp{fumxrun)qpv|uvo"k+ v
svmp{)vqv m#l 9xun)qpvxq#o"k {)l qv{)v'unwxl kmunork)quvxumpo"l k\l }z #l u}lYwfvxZ
l ffo"q%>x. 2po"M Ap o"qu|}lYwfvxl ] 2|M.`

6Yff2LrY n<)
< M| v mfsZvu6"v Jvxff}6u)fo"k)ln unk)w"v mtdsvunk
o"kYmpv p)v mumporl kl ||o"q` E ' ff

fir fi fi9n~or9\orqtuqpZvxxoun"o xvxwz}~lw)vxl unk)w
lntun"nl )kfwo"k)qpmunk)xvxq xxxXp l 9xrun)qpvxq#l 9 v{fu'Jvmp{um| ){)l rw)qln

%9 x !#"%$& ( '2
) 2`A o"q
v Jv p
zq'mx) orq+kflnmuxl kfqm`uno"kYmx {)v v
J`` E xo"Momorqunxxv )musf"v 'mxfqpl }v#"v JvxM}6u)fo"kfunk)wdo"kYmpv p)`v mumpo"l k

* u )`l n`un} o"qdunxxv )mus"vnmp{)vxkv Jv pnl )kf,
w +Y)v {funqdl kffkfompv J. -0/ 1 w)v umpo"l k)qx
{)vxk)xvommpv }o"kfumpvxqx~ldv>mpvxk)wmp{)o"qvxq)mmplkfl k 1 nl )k)2
w +Y)v `o"vxqxunqlnmp{)vun >x"o"~'unqpvn
mp{)vl ""l o"k)k)lnmpo"l kl sl )kfw)vxw)k)vxqpqo"q4fqpvxw
6Yff2L 35 4<7 68-<>Y,
976f : v mdAsv~udrv Jvx}6u)fo"kf6unkfw"v msZv~u qvxxorun"o xvxw
}lYw)vx l ff|< ;=+Yfv p
>? xxx2p orq4sl )k)w)vxwz 'mxff<<unk)w <+o"lnv Jv p@

B > C 4 xxx2p< nl )k)wo"k)qmunk)xvl xxxXp unk)w
xxxXp
o"qfkfompvn

;)m+unkfwffvxw<vxqp`{)o)l'Jvmp{fumlnunkunxxv )mus"vfl n`un}\v Jv p|sl )k)w)vxEw +Y)v {unql k)"
fk)o"mpv4w)v umpo"l k)q XmxJmp{)v49`l "l qpvx"vx mporl k|)"vunkfwk)vxJumpo"l kunq]fk)ompv4uno"")vn{fv4xl kJv qv
l mp{)o"qvxq)m4{)l "w)q {)vxk F{fun(k Gqxl k)qm) mpo"Jvk)vxJumpo"l kdorq)qpvxw Hu{fo"lno.I KJLJLMJ`O Nffoqmx v
ln}unro xvmp{fvxl k)xv )ml mpv }o"kfumpo"l k 'mxfmp{)v9`l "l |qpvx"vx mpo"l kd)"vn
6Yff2LQ P> /YS RUT< 7 - K -0V976) : W -)X
8> <K ;Y+Y)v zorq~[ Z [ \ %x` ]





.
0
U
/
1
'mx~to"un"]ompq
wfv umpo"l k)quv|fkfompvn^
;fl n`un} o"q| _ Z[ \ % 8 ]~orAv Jv p
nl )kf
w +Y)v o"q"vxrm 1 mpv `}~o"kumpo"k) 'mx)|

`ba c28 e Lg fih xffj
`|nGJ`` E xf Q ] n nd ' >j| 7 k>m
l!kf nUfimprq> G! snxt n
fK -S/ \ `< Zxnu
> `nfEn<nv nmp7 k>Yp
l!kfx`En w9)%. fi
`ba c28 e L x=h xffj
`| [ Z [ \ %x` ]f5 ] n^
fiprq< q>x~5 zJr 6 ! sn'LJf ]zB {


~
n~ >p' "Xp''> Z fi fiy|u} rJ E x

fir fi fi])Zn O} ~ Z ! snxt n^l!kfxt n
>{> p7 k>Yp
fir fi fiA)n ,> _ Z[ \ % J ]fi
* kmp{)vl "rl o"kfqpvx mpo"l k6unk~unxxv )musfrv)`l n`un}mp{fum+ln`}un"go xvxqfffrunk)k)ork)to"k|mp{)vsf"l5 >q ln"w
o"q4 oJvxkL

tK

fi 8dd@U0<SLSddffuUUm ddd
[Idyduy[%[y(WffdW
L0Q0Edg7005 gQ8ffO7t(7L0 5g0550@(!!.W5LVL2Q!0!!L
(L55dm.U!t705y7Q0@50@5g7!L0LStW5LX52d7!LSLStW5gL(77
gLSQ00Q7g!j.7ff5LU!%05 L005Q055b7gL7Q5085Q
WdQKL7 yyoty55Q7 U[[7 WdU
!fi ffQt UQ <d ^ L U.[_7
8
LbyS t5d#QL L[_7 7dQ ^y LO[<fi WdU L& U ff. 75 !O0yLK0
0QL7o7#
"%$&(')')*+'(,=yLK50W00tU7Q(%!L055Lb75!705!L500tL7t7
7U709 8Lt70d050yLK0!g70Q!!
- $./02134.5$ 6<880

c

b

b


p

c
q



r

p

LStfi: g700b50<;O85=>7Qg
?

@BABCDFEGBH AIKJ LE(M N@MOQPCD)RTS
E@QC@BU2JVN@QW%R(M
PU XQCPYE)JZN@BW[ML%EBR)M
@BABCDFE)J L%EM\N@MQ]N@QW^[MOQPCDR_
@BABCDFE)J L%EM\N@Ma`bEMY]c^RdS
PU XQCPYE)JZN@fiMLER_
@BABCDFE)J L%EM\N@Ma`bEMY]gfhi@kjgf%h@E^RTS
E@QC@BU2JVN@l R(M
IUIBn%UA2JVN@l)Ma`bEBR(M
PU XQCPYE)JZN@Fl)MY]gfhi@kjgL%E^R(M
@BABCDFE)J]gfhi@kjgL%E^[M N@fiMY]aN@ljo`%bBEi^fiMfh@pEBR_
E@QC@BU2J]JVC2MqFl R(MJnrMqQsYR(M JhMqt%R^pRuS
OBvK]owKMxfiMAfiMa@QHwfiJVCYR(M@BHwfiJn)R)Ma@BHw[JhR^

IpUiIQnYUA
J\qFlMOFR)M
IpUiIQnYUA
J\qQs2MOFR)M
IpUiIQnYUA
J\qBt
MOFR_
zIpUiIQnYUA
J\LfiMQ]{L|jo}B^R~S_

Z?
?
e
?


?

Z

q

r

fi%BY%Q

VpzpiQY
\fiQgkg~
piQY
\fi

fi+YYY\Y% r T<u\Y4 \T QF) TY\9 p YZ
FQ%YZ\+K\Y\%\\%\4K+%Y44Y[\%YZY\ B \Y
\YY Y\ Q Y\\% YcYZ u%T+ \YQQZ\\
\Y%Y[akY\k9Yc%\kk\YZZY\%Z\\%YkY
\{\Q %\Y Q%YZ\|Y\ p cVpYYY%
\<+cY%QZ\TYZ|%ZQuY\YZ2%\Y%Y\\Y
%\%\BY%<\%Q%QZ\Kr%\< p %V \\ %\\Y
YZY\%\Y%%Y%QZ\9r %
\YY %gfiY)+
Fr%\%Y%)fiY ()(\%
\%[Vi (iZFZ VpZi p24 ZZZ Zi Y4YY\4Y\
\Y\\Y+Z\)Y\+ \YB{ p24

c\%
()([+\ %\Y\YkYY VFF|Y %[Yr\Y
\Y\%\\%|\F\YYVB\\
F

Q
r %\ )T
2V (
BF) %\ Q [
p
BF) %\ Ycd
F
Q)\( BQ
ff fi ff %T
(
2V
B(




B

%



2V

BF) ff g%[ Ya
Bifi ff B
p
2V2 (r Y( ff %pu
K K
fifi
fiVY(
fi))
[ ff


piQY
F)
piQY
2 F)
piQY

F
ZpzpiQY
\fiQ{|oB~
VpzpiQY
\fiQgkg~
piQY
\fi

FFY YuZ Y((+(~Y Y\Y\Y\QYYZZ! Y%Y%") B )Y
YVB\Y\Z\ \u B Q\ZY49\YQ[\Y YYY\$#fi
Y\Y YFFKY4\[\ \KY+Z\)YBFFr\fi\<
Yi (iZpZ Zi \%FF|Y YrY\Yr\YY
Y%Y
% '&'()&* , +(-Y %/.0% - %
%2134/3 & , +F %/.





% 3 * 46571 , +(-p98B: %;.<3

3>=

4 *? &@ fi8 7ACB[D
IKJLJ

FE % + %

GDdHD

%8 %



fiMONPQRTSUWVYX6N[Z\RTS]^R_PS`PacbdX6SX6N]egf7PhUR_iCMONPUN]ZOj

k lm6n6o7prqsrmutwv,x7yzy9{;|k}<lsrlG~Hv9k xk~<r|G~
L6d[['; vfin[yr|Kyvy|KyvfiLyh6|
k

Gyh;yh$

y9/ymuyls97vfin|KylsKv|KylsKvfi|K

{ W vfi{/| [[
[ lsrl g[ u


L6
n/mv vfi{/| |d [ u `O
/G[ 9 [
[[
'
vfi{/|r

x k x>k [[ [G6[ 9 >[[\ 6
6 _
v,lsLl\n6m6[v L vfi{/|> ||d [ kk[ _ [
vpyyKp| [
[ F



v

p








L

K


p




|








[g _ [
[ [
[ [ [$ [ /
}





\ _9_
}


}



7

lmn/o7prqsLmtwv,'yOyK|


l
m6n/o7p6v,'yOyKHyK|




}

__ }

pln/l v,x|Ck>x'



t' mv,xyz[|Ckz

g [x`p lv,z[|)



` L6c

; F7 w
6 ) c}
t' [
r h|Ky9)h|K 6
| [
[
sLt7v hu K



9
7 [ [ kk [9[ _ [g H kk 9 u
6


7u

k l
m6n/o7pLqsrmtwv,xyzhy9{/|kHy

vr|

k lm6n6o7p/v,xyzhy9{y|kdy

v|

k lmn/o7p/v,xyzhy9{y|k
77[
[ [ F


[
7d ;[

[[

k {k

vfi6|

H



[

lm6n6o7prqsrmtwv,xp6yxl)y n6o7|pln/l vpl |Kylmn/o7p/v,xp6yKpl)y pl n/o7|K
h|K

vr|Y H

H

k l
m6n/o7pLqsrmtwv,xp6yxl)y n/o7|k
[d

k}

pln/l vpl |KO [

pl





k l
m6n/o7pLqsrmtwv,xp6yxl)y n6o7|k
[ [ F

[[

lm6n6o7p/v,xp6yKplKy



)n/m6[v L v



v pl ||}

k lm6n6o7p/v,xp6yKpl)y pl n/o7|k

[

py nKlk nKlp|\

pln/l vplKr|Kyt
h|K\

[

k2pln6l vpl |k

t' mvpl)y

v|Y H _



pr|Kylm6n6o7p/v ;
n Klk xpyKpl)y plK;k py9nKlpu|K



p6y n;Klk n;Klp|k



k l
m6n/o7p6v,xpyKplKy


fiff

k2pln6l vpl)r|ky

[

fi

!"$#&%('*)+
, -/.*0132 %54 2$627-698:;2*6< 0=-7, 0=->2@? ) ,AB,DCFEHGIEHJIGI. % 27-K$698:;2 ) ,ML
h 2VGI- % 8:;2 )j
N OPP "$Q>RTS>UVSXW , YZ2@-90-9G % 2@-[K ) 6CFEHGIEHJIG@. % 72 -[K$698:;2 ) \^]_UR@ 72 -[Ka`cbedgf 2@-[KH`i

- wy=I0*.*l % b^mnG7o % <D27-K, 8:;2@? )9)_z
Qk" =I0.$l % b^mnGfio % <D2@-[K, 8:;27? )9) Yp=[0.*l % bqmnG7o % 8:r2 )9)s K
j UR@u@R -rvx
-9v-Fw{=[0.*l % bHmG7o % 8:r2 )9)\e]_URIkR@|"kR
, -/.*0132 %54 2$627-698:;2*6< 0=-7, 0=->2@? ) ,AB, -/.*0132 % < 0=-@, 4 2@?}627-6<D27-[K, 8:;27?}6k0=->2 ) ,ML
~ ]UR P k""$3|"_S>UR"S>URIu@! QkR@Q_"$xFQQ>#a!Vfi\
3I$e/}7

S>U!QQ>R@uIS>"$ RaS9k" u@R^ykQ9S!S>R@VS>!"$ u7R@ OP u@u@R P Sk f S/ "$S>URq"S>!"$QT"$
uIu@u@S/yu@u@R P Sk f S/$\_RQ>U" S>UVS OP u@u@R P Sk f S/ P k"7R@Qn#a"kR P uIS>!u7S>""$
S>Uyu@u@R P Sk f S/a|" P k"7R@S S>RIk#aVS>"$a"$xQ@\
N R@uIS>"$\ K Ru@!#S>UVS3X"kRIS>" P k"fi*RR@S S>RIk#q!VS>"$"$*gg Sg!QQ u@R@S
S>" P "7*Ru@u@R P Sk f S/"$tS>UR P V>S@"$x*yu7R@qxF^uIu@u@S/"$S>UR_kR@Q9S"$S>UR
P k"$[#y\3RIS QR[ P !!HU" RV>k*RTS>"aS>UQu@"$u@ Qk"$\Q9S *ggQ P V>S>S>"$R@
S>"S " P V>S>Q7+ OPP RI P V>S Q@u@"$QkQ9S>"$Fu@! Q>R@Q K ) 6@L@L@Lfi6k ) " RI P VkS Qk@
u@"$Q>Q9S>!"$S>UR^kR@Q9ST"$_*gg\]_UQ P V>S>S>"${QQ ukUS>UVS"kR@!VS>"$RIR@
"u@u QX \n]_U!Q"$ P V>S>S>"$a"$ P k"$#QRIR@ f P VukU"
!#aR@Q>Q>F% Kfi$ )Q_"$!" Q7\
N 7S>UVS>@$5}Q5ka!3S_"u@u kQS>URXUR7y"$VSR7Q9S"$RX"$S>Q_u@! Q>R@Q
S>UtVS5I9$QX5ka!S>QR@!VS>"$Q_RIR@\
F|tx5>$x*g P k"$#>It9$9
!3"R@!VS>"$RItR@"u@u kQ_! \

R@"S>R@ f


N
" R[S>R@Q ! ZRIR@QR kR@!VS>!"$Q P "$Q>Qk f Q>!nS>URkR@VS>"$QRIR@kR7
\"Q>Sku@R S>
U R P k"$[# fi+


fiff



R[S>R@QS>UR P k"$#







+





[R Ru@"$QkRIS>UR P k"$[#xFe" f SkR@k"$# f TR@RIS>S>URS>RI[Q3RIR@
\RTu7S>UQ" P RIVS>!"$ I>I[ RIR@Q|"$" @Q \


F|tx fi

] UR$ @9@"!$#9*r
R@"S>R@ f &% QS>UR P k"$[# " f Sk R@k"$# f q
R@RIS>xS>URu@ Q>R@Q"$

S>URX!S>RIQRIR@
\

')(+*

fi,fi-/.0132465879-;:<132>=@?>1A.2B.CEDF792G79-/=GHJIK.L4G1AMN,fi-/.4-/=:fiO

P;QRFSAT;UVXWT;Y[Z\;S]K^fi_`WT;aN^bcWdRXZea;ZgfT;Z[aNWUhWdiQkj9Z\Vl;Z[TN^b<mn^<_hSU8Vl;Zpo;RXQ@qRrWsutGv

wx

PSATGWyy3z@\|{}ZJo;R)Qkj9ZJVlGWdV~||KJSUpWY[Y[ZgoGVXWdiGyAZWT;a6VlWdVeuSUpWYgz/Y[yASAY\WT;aSAT6a;Q@SAT;qEVlGWdVc{Z
lGWkj9ZpVQJVXWd9ZcYkWdRXZpVlGWdV8Vl;ZFV{QyAZgj9Z[ysWdo;oGSATGq@U<;UZ[aWdRXZFRXZ[yWdVZ[aizWY[Q@T;aGS3VSAQ@T\/TGWsZ[y3zJVlGWdV
]$QRZgj9ZgRzqRXQ@GT;aSATGUVXWT;Y[Z\UXW[zv

_k)bd\Q@]8WY[yW;UZQ@]8^fi_k\]QRZgj9ZgRzyAS3VZgR)Wyfi

Y[Q@TVXWSAT;Z[aSATWT;aa;ZgfT;Z[aSATJ\dVlGZfiyAZgj9Z[ysWdo;oSAT;q}Q@]>SAUKT;QVKqR)ZkWdVZgRVlWTeVl;ZyAZgj9Z[ysWdo;oGST;q
Q@]8xl;SUeY[Q@T;a;S3VSQ@TSAUSAsEo|QRXVXWT VpVQBZ[T;U>RXZEyAZ[]V"VZgRXsESTGWdVSAQ@TxP;QRSAT;UVXWT;Y[Z\Y[Q@TGUSAa;ZgRVl;Z
o;RXQ@qRrWs^
XL

v

|NLK

XL

v

XLK

@

WT;aEVXWd9Zc^<_} ) WTGa @) x}l;Z[T^<_8Zr/VZ[T;a;U}J\/^fi_Lm6SAU}WY[Y[ZgoGVXWdiGyAZF{exRkxV[xVl;ZFyAZgj9Z[y
sWdo;oGSAT;q 9 \SUWYgz/Y[yASAY{xRkxV[xVl;ZEyAZgj9Z[y}sWdo;oSAT;q \iG>V^SAUT;QV
yAZ[]V"VZgRXsSATGWdVSAT;q/x
Q/\Vl;ZUVZgoGU{ZWdoGoGyASAZ[anVQG9WdRXZNUX;sEsWdRXSA[Z[aSATnVl;Z]Q@yAyQk{hSATGqBaGZgfGT;S3VSAQ@TQ@]
>o>WY[Y[ZgoGVXWdiGSAyAS3Vz@\>VlGWdVhY)lGWdR)WYgVZgRXSA[Z[UFyAZ[]V"VZgRXsSATGWdVSAT;qo;RXQ@qR)WsU[x
P;QRFWyZgj9Z[ysWdo;oSAT;q;WT;aWo;RXQ@qRrWsuJ\>VlGZrg)k";|J\>a;Z[T;QVZ[a; \;SAU}Vl;Z
yAZgj9Z[ysWdo;oGST;q]$QRh a;ZgfGTGZ[aiz J
$|A

x

" tG G@t>>|9>d ZgVfii|Z6WyAZgj9Z[ycsWdo;oGSAT;q]$QRN^x

ZgVNiZBU[xV[x

^^fi_ ]$QRhUQ@sZ`^fi_k\WT;ayAZgV8EiZeWTNSATVZgRo;RXZgVXWdVSAQ@TQ@]K^mJx<^SAUpdG9Xr
;ff


fiES]Vl;Zc]$Q@yAyAQ+{hSAT;qJY[Q@T;aGS3VSAQ@T;U8l;Q@ya

xh^ _ Zr/VZ[T;a;Uh
xh^

mn SAUhWY[Y[Zgo;VXWdiyAZc{exRkxV[x>

WT;aN

xh SUhWYgzY[ySAYp{exRkxV[x>
xh]$QRhZgj9ZgRzNqR)Q@;T;aSAT;UVXWT;Y[Zc
v

8_kkQ@]WY[yW;UXZcQ@]^fi_k\;]$QRhZgj9ZgRz

\

S]KSAU8aGZgfGT;Z[aNSATWTGaSAUT;QVhWY[Q@TGUVR)WSATV[\WT;a




S]

_g ! \;{hl;ZgRXZc _kk !

WdRXZcVl;Q@UXZyAS3VZgRrWyAUWsEQ@TGq8_[+ {`l;Q@UZ

R)Z[yWdVSAQ@T;UQY[Y[>RhST^mnJ\

%

VlGZ[T n#" $x
oGRXQ@qR)Ws

SAUpdG9Xr S]Vl;ZgRXZpZr>SAUV;\; WT;aEU[xV[x>^SU};o>WY[Y[Zgo;VXWdiGyAZF{exR+xV[xK>\;J\>Gx

&
' iUZgRj9ZFVlGWdV8izVXWd/SAT;qJ]QRVl;ZpZ[sEo;VzEUZgVQ@]Y[yWGUZ[U[\/{}ZcQiGVXWSATVl;ZpQRXSAq@SATWya;ZgfT;S3VSAQ@T
Q@]WY[Y[Zgo;VXWdiGSAyS3Vz@x(FZr/V[\;{}ZSAT VR)Qa;GY[Z`VlGZeT;QVSAQ@TNQ@]dG / fi )fi+*[Gg-,x
$|/.


" tG103254#6/7698:2;w ZgVe^iZ>o>WY[Y[Zgo;VXWdiyAZJ{exRkxV[xfi\

_ + x}SAU8>o>iQ@;T;aGZ[aS]]$QRhZgj9ZgRz
<)
=3> ? $@ $@ _ +$@

;

WT;aB\KWT;a6yAZgV

Vl;ZcUZgV

SAUWqRXQ@;TGaSAT;UVXWT;Y[ZcQ@]<

WT;a

@ BA

@)C

SAUfGTGS3VZ\>{hl;ZgRXZc @ + @ C WdRXZcVl;ZeyS3VZgR)WyAU8Q@] @ _ + @ED _ {hl;Q@UZpRXZ[yWdVSAQ@T;UQY[Y[>RhSAT^

FHGIF

mnJx

&

fiJLKMON5P1Q!RM#Q

SUT+V:W)X1YZW\[]V_^]`1Vab[]`cd[ec:f!fhg7ijlk#m7noX1YZW)p!q:cd[]p!V3T^V3rBc:TLs#t1n uvV3sT1X1YXxwys1YZW)zxcdW)Y|{ T1p/[]Y:}BaY^]`c:f!f
t1W)VlqIY[]`cd[c+gIi jlk#m7noX1YZW)p/q3cd[]p!V3T~V3rc:Ts#t1n uvV3sT1X1YXw5s1YZW]z~V3T7[)c:pT1^|V3T1f/zs1t#n uV3s1T1X1YX~wysYZW)p!Y^
c:T1XaYx^)`c:f!fc:^)^]Vypcd[]Yaep/[]`Yc:)`X1YZW)p!q:cd[]p!V3TV3r\[]`Yw5s1YZW]zc~X1Y^)YT1X1p!T1)`c:pTpT[]`1YxaYf!fn
rEV3s1T1X1YX~^]YZ[;V3r$tc:p/W)^V3rs1f/[]p^]YZ[]^eV3rTcd[]s#WHc:fTysuvYZWH^aep![]`[]`1Yf!Y-#p!V3:WHcdt `1p!V:W)X1YZW;Yc:f!f
[]`cd[c#^]YYY:O/Y^]`Vaep/[]:l3I3p!^c+s1TV:W)X1YZW)YXV3f!fYZ[]p!V3Tp!Ta`1p!)`[]`YTysuvYZW
V3reV5s#W]WHYT1Y^V3reYc:H`Yf!YxYT7[p!^V3s1Ty[]YX1V:W)c:f!f/z3$cs1f/[]p!^)YZ[V3reTcd[]s1WHc:fTysuvYZWH^;p!^c
rEs1T1Z[]p!V3TrW)V3[]`Y^]YZ[_|V3rT cd[]s#WHc:f$T5s1uYZW)^[]Vp/[]^]Yfroh3p!q5p!T[]`Ys1f/[]p/t f!p!p/[zV3rYc:H`
Tcd[]s#W-c:fT5s1uYZWe\`YT []`YV:W)XYZW)p!T1lBV3Ts1f/[]p^]YZ[]^\p!^X1YZ{T1YXc:^[]`1Y[WHc:T1^]p![]p/qIYfV3^]s#W)Y
V3r[]`1YW)YZtfc:YxYT7[V3rcTcd[]s#WHc:fBT5s1uYZWeaep/[]`c:Tyz{ T1p/[]YTysuvYZWtV3^]^]p/uf!zLYZW)V7V3rT cd[]s#WHc:f
T5s1uYZW)^[]`cd[|cdW)Y^]c:f!f!YZWes1TX1YZW|OpT1Y_p!^aYffnorV3sT1X1YX []`1Yp!T1X1s1YXV:W)X1YZW)pT1lBp!^
c:f!^]VaYf!fnorEV3s1T1XYX1V:W^]p_tfp!p/[ozaY;^]`c:ffV3xp/[p!T[]`1Y;^)Ywys1Yf[]`1Y|^]s#u^)ZW)p/t1[OOrW)V3l
p/[]`+c:Ts#t#n uV3s1T1X1YX_wys1YZW)zOaYc:^]^]V5pcd[]Y|ct c:p/Wo ): )/ff ): /ff| ): V3r
s1f/[]p^]YZ[]^5ae`1YZW)Y|rEV:Wct1W)V3:W-c:c:T1Xc:TLp!Ty[]YZW]t1W)YZ[)cd[]p!V3TL
/ff )3 ~bHI

):
7 ff l


7 ff

):



):
ae`1YZWHY| l cdW)Y|[]`1V3^)Y;f!p/[]YZWHc:f^V3rBae`1V3^]Y|WHYfcd[]p!V3T1^V5s#Wep!Tc:T1X+ 7 ff

):
):
p!^[]`1Yxc#p!s1V3r\ff
ae`1p!H`Lp!^u5zxV3TyqIYTy[]p!V3T~p!rff
p!^[]`1YY_t1[z_^]YZ[HH


Yc:f!f[]`cd[[]`Y+f!Y-OpV3:WHcdt`1p!V:W)XYZWV3Ttc:p/W)^V3rs1f/[]p!^]YZ[]^-;p!^XYZ{T1YXu5z ] L
H p/~Yp![]`1YZW\; V:W\ bc:T1X
\`YT+aY|c:Tt1W)VlqIY|[]`1Y;rEV3f!f!Vlaep!T1W)Y^]sf/[

ff
fi
fi fi7
fi
fi fi fi # $fi % fi~

I5dEO3dIHH /
B1 ; hh
; :
h5 5v

h# 3 g7ijlk1m 7 - d3
Z
H : #+ ~d 3 7
-Z
1o

fi! "
fi
& fi '
()*+hYZ[), .- 0/ uvY|cg7ijlk1mInoX1YZWHp/q:cd[]pV3TrEV:Wp!T Y|tW)VqIYu5zp!TX1s1Z[]p!V3TxV3T
/ p^s1t#n uV3s1T1X1YXOc:T1Xx[]`cd[\p!rp![p^[]`YWHY^]V3f/qIYTy[\V3rcwysYZW]z2/435-uyz[]`1Y^]Yf!YZ[]p!V3T+V3r
1 []`cd[e0_
cf!p![]YZWHc:fvae`1p!H`+p!^T1V:[cV3T1^[WHc:p!Ty[#[]`1YTLo2#
/ H: o0/635-- ):
1V:W[]`1Yu c:^]Y;c:^]Y 1 3aY`cqIY[]`cd[| - p!^es#t#n uV3s1T1X1YXu5zLc:^]^]s1t1[]p!V3T8
7 Vla V3T1^)p!X1YZW
3



:
c
1


X
)
^
#

1



3
V
]
^


]
[

`

c
|
[
]
[
1
`


)
W


]
^


/
f
|
[
1
`
3
V
!
f
1
X

^
E
r
:
V
W
3



\


`



^


0

6
/
5
3
;
!
p

^
#

#


n

u
3
V
1

1

1
X
YXOs#t1tV3^]Y
19
1;:
[]`cd[[]`1Y\W)Y^)V3f/qIYT7[V3r0/635
- p!^$X1YZ{ T1YX_c:T1X_[]`cd[[]`1Ye^]Yf!YZ[]YXxf!p/[]YZWHc:fy^)cze5p!^T1V:[cV3T1^[WHc:p!Ty[
[$rV3f!fVae^$rW)V3[]`YrEc:Z[[]`cd[0/635$
- p!^s#t#n uV3s1TX1YXc:T1XrW)V3[]`1YX1YZ{ T1p/[]p!V3TV3rvs#t#nc:YZt1[)cdu p!f!p/[z
`1YZW)Yc:f!^]VV3T1X1p/[]pV3T=~
< p!^s^]YX ;[]`cd[2
/ p!^s1t#n uV3s1T1X1YX>7 Y-5[aYx^]`1Vla []` cd[o2O/ ): p!^
^]c:f!f!YZW[]`c:To2/435H- ): pT[]`1YLf!Y-#p!V3:WHcdt`p!V:W)X1YZWSUre[]`Y+W)Yfcd[]p!V3T^]z5uV3fV3r Vys1W)^
p!Tb[]`1YT[]`1Y{1W)^[V3tV3T1YTy[;V3r\o0#
/ ): uYV3xY^^]c:f!f!YZWuYc:s1^]YV3r\V3T1X1p/[]pV3T=y?
@ []`1YZW)aep!^]Y:hpr$[]`1YWHYfcd[]p!V3T^zOuV3fV3rbV5s#W)^p!T []`1YT[]`Y{1W)^[;V3tV3T1YT7[|V3ro / ):
X1V5Y^eT1V:[p!T1ZW)Yc:^]Y;uvYc:s^]Y;V3rBV3T1X1p/[]pV3T3ae`p!f!Y[]`1Y;^]YV3TXV3T1YuvYV3xY^^]c:f!f!YZWuYc:s1^]Y;V3r
V3T1X1p![]p!V3T 5\`YV3T1f!s1^]pV3TrV3f!fVae^|rW)V3 []`1Y_rEc:Z[[]`cd[;[]`1Yf!Y-#p!V3:WHcdt `1p!V:W)X1YZWHp!T1p!^|aYf!fn
rEV3s1T1X1YXc:TXrW)V3[]`1YrEc:Z[[]`cd[ pTcX1YZWHp/q:cd[]pV3TcV3T1^][WHc:p!Ty[ec:TuY;V3T^]Ys#[]p/qIYf/z+^]YfYZ[]YX
V3T1f/zc{T1p![]Y|Tys1uYZWV3rh[]pY^

BA

C

DFEHG JILK

MONPRQSUTHT5VWTUXZY[]\$^`_4a jyj"b ^dcRegf g4bhji c:f!fnlk Q6mLnoqprsmUt5Q4u []`1Yt1W)V3:WHc: V:u#n
vk Q6mLnoqprsmUt5Q4u u5z+X1Yf!YZ[]p!T1[]`1Y;fc:s1^)Y^FwIc:T1X3H Y;tW)VqIY[]`cd[ PRQ4SxTHT5VTxX p!^
lk Q6mLnoqprsmUt5Q4u 5c:T1X_XYZ{T1YXLc:^\p!Tx[]`1Y|Y-1c:tf!Y^V3rBOYZ[]pV3T1^yA5/



[)c:p!T1YXrWHV3
s#t#nc:YZt[)cduf!Ya;ffWff[# n

z{}|

fi~$R8d"d;``.U`~
R;"RUH5WUU0yl$UqsU54R 2RR4 d)%R`" U yRR$"
"qddl RglsR$R FW"''
"F`xHHWxj$%`Ryl6LLsx56
y)`'%"%`gFU0y Rg`
y)`'%" %`gyl$Uqsx56 $
"R%R "RR>` R
U}5} q '*4% } $ } U } } }g
"} `%dR5"%%`g! '*4% } yR
U}5} " } F " } J " $ #
"R%R "RR>` R
U } } 8 $
"}R
U } } 4 *}U 4 ldx " } 5 "

y`W"'`> "g%g`%R0%Rs6R `"%RR %"`" Rg` B R
ddl `g`l"lFW"'R%W" R RyR

ff fi!"$#&%('
"$)+*,#.-0/1'
2)3%4%5)'76839:!;# <=+7>?
A@B# <=#6C#D-$%5%5)'76839E:F

GIHKJMLONPRQTSVUWYXZX[LOS]\N^`_bac_8\:d
e ' R%%" 2' %0ddl `g`l>R6FR F`%%`W.%RR6"q"q% `g%"H
J%`%%")"R Df%RsR %" `66`R>)'1gddl Rg` "ihB%g%
% !d `"xR" %`g)' RR 56%l.R"d sRl.R" 8g%R
ddl `g`l"]jFR)6$RRF'1g>ddl `g`l Rg`! """RR%
)Rdl Rg` "]kq` "4)2gR`)'1gdRl Rg` %`W$Ry%RR6"H"
W* % `g%" "q"dR s`R"
l &monqpirtsu 7vxwyz7{}|~ 7vx.py
px( Z
%"R g% " g`= J $g% "6 RR; 8%`R$R"%Rs g`5
yRW"'`F`" H'5xHx>``R8` %"` g%Rq8"R%%%"%Ry"R
R
}q74])]
3C")ZV])
$ `7
3 "C]
3 "Cy
$ "C7]
5

fi~
[O
Z

3[4C)u71[
I4])y
3
I"C7x
ZOV
b])[]7

I"C7x
ZOV
cC]7
+O3"q(E"q"O0O5+5+u
y+O

OO+[71"

u 7$$ uOIC
73 "uZcO$C)C)
7$$ "O8IC]O8I7
7 7$$ "OcIC])
ZOV
cu]u
71[
I"uOc8u
73 "uZcI8]Cy
3 1[
3V4]Oc8
1[
3V4]O8b3
ZOV"C7
+51
4O443"B:+
0~0OO++517OO"D"OqOEO
OO ]~+O+O"Oq $3"D "
1"Ox144O5"3" "O?
ff
fifi`(O4


D+O [K)i"OYD+O" O44E?+D71"O?K +51
0+O
:+C"O O" fi! # "tO"O14O3] CO"
"O4
KDO434+ + +[B71" OO"O$
tE"%
O"DOD+
" ]"O
71"".+71"&
')( u fiy5+O"+4BD3"
*+')( u fiDu"O40D+?O,
*+' fiuq"O0+ 1
B"O5
"
1.
*KO+ +E4:+Z O4"/
'0( u fi1
"OqO4
""4BD
1"D2 * K3
+"4

:+O"(D+O"t 5"O]"Ot4D1" `O
OBOO
"$E;"O(D+O"` +O


"O;"O0&+BDD14D1"7
6$836$9:83;<
E
"
0D+O" 3 +O? > =(+""
13

Ett.4 "
1(D? +O@
153 7+
"O1
fi}+""O" =++[71"
.4A
".

+(+

"
T4D1"DBC6DE E
RDR"4~ +OBFHGJI7RE4O +
+[~1" "
1qK $4?+(E" OOL
"tO?4D1"DM
FHG O7O +"O
$1"
N OG FHG
4@
OG FHG I! 4O("O4D TOO~ tOD4 OO+ ZD

+P "O(4D1"D,
OGQR8O
OqB"4C 6$836$9H8H;DB"
(: S["q:`O+
4+O
7
bZCO&:T+CZ ]OTycO3C bHT+7 ODO3+"4O
"~"O;71"TZ5:t
?q"O0+ 1
~IIZ4
"

4+5+
UWVYX: Z?[VD? [
+"5B71"O< :+O"u
y+OqO+]
\(H $J
UWVYX^ Z?[V.O [T(. _8"4BD
1"O[> +4O(" O5K $"O5"OEt
"O +O417EbD
`}7Oa _


" =
~"B
BOVOUWVYX:Z?[VD?[Y"
1}(+4
I"O=B
"
FHG 6 b 3 3 ff c C"O=
4+5+ O"""OV"OKD+O" 3 3
"O
(O+e $"O f O3 OO:H $Et(O+(
+t="7+"~"
=:= =(+"K+" "
1
"O
+"TZg _b+O417Eb OO+OET"?
4K $BD _8"5B71"
"O?4"q+3"
O+4q1"h
=(H $""
"3 :+
DEuO UeVEX: ZO[VDO [ Tc~"ZO
4+5+ i)jKO"D"O
Cu
y+O
O}D+O" y+
kimlO"""O]"O;4q+O
D+O" n o=}+"K"
1
i)jqp +3 r7: S["OO
imlK:+O4s iml4+O"E" +s &"O4(E"~4.1"&
b36$836$9H8H; c
tffuKv

fiwmx<yRz!{|!}4~2xC5{|gPg{y!|ky!, | x< q?y+} {wmx<y!}!x<!m



b

c

)PgffRmCN,PC!P Cff Jff C ff

H )<C3,C@g CPffff+ff:+ H C C3@Hffqq:<3 R%
R@5 ff^ff:+ffPYPKe W3YH- P7P# ff+ff:+ff R3eC H3 P
CsH, P-3 3<Pg P)C3eP5OWYK??)e<<!EeCffK<Cd3YH- P@P
OWYJ^?YO>>g335hCK ^km##33HC < 5>H<3g
C733^P C$H3-C!CPsffYDC73P P%1NH CP1R1RC307C3
PC,d d, 3+P)51m eq- 3^P)HJ.? ff+Pff ff

R @ff-ROJ EY:
7P#W, YogC33HC qd3P HC-P3CH: 23P^+PCCNC
C
CP:C33! C qCsEPJeC.CPP)K:R3 P2e3K YdHP
3PC3C# 3C3eC3C3eP H NCN3P ffP#
)Y E !A<R
fiff P
CffPff !#"$&%(')+*-, ./021435(14
687q
3 C
@:9<;=7q OEeP-NH 1P3YPC 3s 3

> @?5BA@ + 5e:<3 W C
> 7:<3 W C
> 1 :R3 D7 1


E 5 ?PCe 03P Pg7eCff H Y3P0N 5^J H!CP
NH CP7GF+3CKNPCNK H PRCC ,3PC HffC
IHR K
J@WC@C3 3#CsK $
?LJff
MON QPY NCPff
R &S2UT V0S-XW0&S2BY
)&)
Z &S-[T )&S-\Y
KR@:<3 2 NCPff

]_^`

fiacbQdegfhjiQdh

kml&n2oUp qXln-oXrtsul&n2oBv
ql&n2oUpwv
xzy{}|~/D|&II/& g{jQ@Ig
xzyIz{j| l&n-oUp u
/{j| sul&ml&n2ogo[p
k ln-ortsml&n2oBv_








Q

c

@







c



g





Q

c








_





(

+







j
{

|

suln-o\v_

l n-oUp kml&n2oBv QcU {j|
kml&n2oUp qXln-o\v
ql&n2oUpwv
{jQjj j{ z{j|<|/~&yQI~yQ Dfi gIQ|zU


x yg|yK{j

~I~I+Q& Q{jj{G{}|&{jI&
+Q{j{j
$#/g=&+2}~I{j
z
{j~I
Q{G{j
#I|IQ= GI|IG
/mB2j lg( 2Qg - g +KXj+(Im QQ{j +
|+
~I}|I||IIXut|
j+c{}+Q&+& {j

Uw{j|
Q(&2IGjI
2 {jy
jjD{j~I
{j{j
|<y
j

Dz gGI|z

D{j|D~I~I+& 2jIm c
gD{}|D~+g~I}{j~IB


DD+(+c_
{j|&~Ififf p
m~I}|&
B D+(+
}{
{j|<Q+QIc{j:Q{j|zD~I
Q|_{j
{}t Dy+& &y
|&j{G+j|z
Q Oy
|
_I} {j
|zg~I~ID{}

yQI ff"!


# =_+D&(yQj
xDyI&I QIIt|&{GQjI|D
uQ{GG{j|+|
{j|
z2{G&| D{jy$y/j{j~I
_ 2y{j~K_+&{j%$fi'&)( *(,+-*(/.0,$1&32 425+-426.0{87[I{jy+
&)(/ *( + 09$:&32 42 + 0;& g 2|
Q& {}
<|&<$ j|&OQI j{j~I
_ Qy{}~&Q+&{j

2{j&|<
mG{j|+=| 0 >
(/?
2XOcQ@
( +
2 + B
( .fiC 2 . ED~I
|{}+<y&{jQjF
G &IH0KJMLFN OOP&&8QRHTS JMLFN VU W 8QRHTS JMLXN YVU W 8QRHTS JMLXN VU W 0Z
[]\ gY^ V_ 8`ba (Vc-dfeg
hg Q(&2IGjI
2 >
H _
Q
4
c g-kjQ+
BlmeI onI_ qp r + g n
g csoc Hhg ZcQ g "d/cQ Q 4c
g-kjQ+ g od
2 g dE
g )
u *ccvsI + KyQ|+
z&I} {}
||I&(/yQ {j| G~I~I+& Q}/I


[2xDy&g
{j|K|{}{j yQt
c
xDyI&I

~I+yQ K
~I
|&{j+

K
0
yx JMLFN OD{jKy
Q{j
z y+&D{j|
<
0 JMLXN gI|/

G &IH0KJZLFN OD{j|K
wE&IH0KJZLFN Q D|y y2 G &IH 0KJMLFN OD{}||/jj+myQ G &IH
j{j~I
_ QyQ{j~&+|
jjD|I # y&I {j
|g

~I~I{j
yI=yQ&|D~I

IO
G &IH 0 JZLFN I~I
I|z|/jj+<I~|
u~I
{G{j

{jy&I {j
|g
z
~I~I{j8yIytQ&|/~I
K

G &IH
{Z| }

fi~VY8Y8"X"YY< 6 k~YYY

oZF4FM<XX84X'X" 4T4X @X<5X4FoF4TXX 84X
V' F8X4,M-4X%*5XEXV"4%4/ M*F4XX5X q4
XITKZF BFoMFMF5F 4X;X 84XX 4 48MXX4BFo
F4fiXX4X'

khZm"XVYYyXX-Yy ]M]4 - fmm
-8"KFoM- % X4oF"o6-44 ,ZXFZFX444'X F5
FXq4B84@F4Z5F'F>9oY-44 fiMXFZF
X E ff
6 fi
E ff
6
qY
fi
qY ff














6
5

V ff

fi
E! "#%$ff
Y&

fi
' V ff

E! (# ff)$ff
Y&


*

-,+-/.-/01.2%3F4o XmZ-4X

<9-V8@144>


TMXFZF5 XM*4X
*

76
859fi
6 ff:<;8:>=?ff
85



6 :<;<:A=Bff
6C


X


fi
@
6 ;6 :<;<>D:>=$ff
5
@ X YX 6 ;5 :E;A(:<;&FDE;G$
A(:<;&FHE;G$YIfi
fi
@ YX 6 ;5 :E;A"E;&FDE;G$
85J
EK D#<;<ffE;L$
6M







E


ff
<
#
<
;
>

"
N
?
;
H
F
<;G$YM


6 ;6 :<;<>D#<;E;&FD<;L$
6








E







"

H
F
%
#

$

C

fi

@

E A(#PF(Q$YC
fi
@ '
E Q &


FoM- 4fifX om-Y%
R -/.-/0S.S2"( TUffV4 RWXR V RZY
R @N[ V\5^ ]E
U @ U_KU @ R%WXR @ Ra`bR _ Ra` V R _ RcdR @ feg_ R `
R @N[ V\^ ]E
Uff] U_!U @ RW R _ RL`
R \ [ - h _KU @ RW R _ Ra`
F @4y4o4ZoM-4Xj
W iLklmnoi lpqkrnji lpqk nsimft(mfuHt(vF"4<
wyx{z

fi|~}!%)>!A

La>I("H
LqffN<^Bff!HE/P <a &Z
Lq{fffNE^BHKHNO oE% ?{d oEZ
f(fH(r//SSA(ff~&aNG1^y
>SPh f
ffs>KGP > qff >> > Psr q

hPr >S1 ff sa& ff)MP K


>L {Psr &{
9 > , >7!ff> > q GO>~ A>< {
~ CKE^BG GH!HNy
{o G") G!~EMo &b ,~Eg o~E EZo >
?% Ea > > 7 CKff^Bff!HNy
9 > , >7!ff> > q K O>~ A>< {
7 /S/S% N HAyy&/S/SSA^NG yHNE^B K DNHNy
N "^j K 8EE &Hs ?EZ r>Sy N,
M!>~ K
K >Sh K rE> rEZ N "" N rE)
"NX MIEZ > "N IE ?>X <Z >
~ CKE^BG MH!HNy


>>)h > !S q Pr!


1P/S/S)>~
K
qr ?1 MH

Dyah%y"rq > 7 {P f(fH(

r rS% > > ! >%
M% &ab EL



%)X &b <a9
(ffffX
//SSA(ffX aZ

> , >7!ff>
>Ih!HN

Kff^Bff!HNy Ay

G> A>< { N<^Bff!HEy
<L &b Ea X %Z

ff fi

Ebg &Z >

>Ih!HNK




fi !#"%$#&'(*)+,",-/.01,&23"4

50687:9,;=<?>A@#@B C!D%EGF HJIKHLDNM
OQP*RTS,UWV X UZY[&\%],^ RJ_`\/^TU?aZRT[b\%P ^?c#_dU+Se?YUf[&PgRTU?hX eiRTU?jkRTS UfP \RT[&\%P ^d\%lmeaZna?o&[ba?[RKn*eP,jkea?a?UZV,Rpeiq,[&o&[RJn%c
qn*rsUNeP ^ \%lteuVeiXTRT[RT[b\%Ps\%l0RTS U+V Xp\%hXverw[&PxRT\8eP=]#V,VGUZXyeP j=e/o&\_`UZXV1eiXTR?z!{|U}[&PgR~Xp\j ] a?U?jsRTS U
P \RT[&\%P=\%lt]#V !eP,jk_`UNei=]#V#Jea?a?UZV,Rpeiq,[&o&[RJn%c#_WS UZXpUfRTS,U}]#V VUZXdVeiXTRy\%l0RTS UfV Xp\%hX er[&^dV Xp\YU?P=RT\
qUea?a?UZV Rpeiq,obU/eP,j3RTS Uuo&\N_dUZXfVeiXTR+eaZna?o&[&azOQP'\Xpj UZXWRT\*R~XpUNeiR}eo&^T\RTS Uua?\%PgYUZXp^TUaNe^TUc[zUzc1RTS U
]#V VUZXVeiXpRqGU?[bP h}eaZna?o&[baWeP j8RTS UWo&\N_dUZXVeiXpRea?a?UZV Rpeiq,o&Uc_`UW[&PxR~X \gj ],a?UP \N_RTS,UyP \RT[&\%P\%lGo&\_
ea?a?UZV Rpeiq,[bo&[RKn%zt{|Udl\%o&o&\N_RTS U`^~R~Xp] aZRT] XpU\%l#RTS,UV X UZY[&\%],^^TU?aZRT[&\%P,^? X ^~R?ceWrs\RT[Y%eiRT[&P hUv erV,obU![&^
V XpU?^pU?PxRTU?jz :UvR?c _dUj UZ,P UfRTS,U}P \RT[&\%P3\%lto&\_Jea?a?UZV Rpeiq,[&ob[RKn=eP j=V X \NYU^T\%rsU+XpU?^T] o&RT^?zm[bP,eo&on%c
_dUeiV V,on*RTS [&^P \RT[b\%P=RT\8RTS U}V,Xp\%hX er\%lm\%]#XWUv erVo&Uz
kW0k,mx,#
X eiVS^~R~X ] aZRT]#XpU?^eiXpU*],^TU?j[bP:Ol\X/rsePgnAeiV V,o&[&aNeiRT[b\%P ^?ct^T] apSe^uRTS UX UZV XpU?^TU?PgRpeiRT[&\%P\%lXpUv
bo eiRT[&\%P,^?c^T[RT],eiRT[&\%P,^:\X}V X \q,o&U?rs^}^TU?U8Uzhzc X eiR~\c!%% zuy_`\sRKnV,[&aNeot\VUZX eiRT[&\%P,^+\%PAhXveiV,S ^
eiXpU !*+%pN pZ'%Z=Ngv%c,eP ju!/ ,LQv,s?fJTN `p}Qv#ZZZz
yS UfV,Xp\%hX er ,b0sqGU?ob\N_] ^pU?^q\RTS=RTS U?^TU\VUZX eiRT[b\%P ^yRT\^p\%oYU}RTS Ul\%o&o&\_W[&P hV X \q,o&U?rz
[YU?P*RK_d\P \gj,U?^!t?~[&PeuhX eiV,S,c,P jeuP \j U3RTS,eiR`j \U?^ P \R qU?o&\%P h}RT\ePxneaZna?o&[&aWVeiRTS
[&PsklbXp\%r RT\ z yS UfV Xp\%hXverN ,%ka?\%P ^T[b^~RT^y\%l0RTS Ua?obe] ^pU?^?
|, ~ tQ Q1
g N,,TGQ# ~m
N, TG Q1
%xTQ#~!Q1
# g~ Gm

e] h%rsU?PxRTU?j_W[RTSRTS UV Xp\%hX er,0 |\%l!RTS,UV X UZY[&\%],^W^TU?aZRT[&\%PzfWS UX U?obeiRT[&\%P [&^+^~VU?av
[ ,U?j e^kRTS U'P U?heiRT[&\%P\%lff
fi cW_WS,UZXpU
fi i%~ ~Qs[&^*R~Xp] U'[&l}RTS UZX U[&^=ePeaZna?o&[&a
VeiRTS\%luRTS UhX eiV,S a?\%P,P U?aZRT[&P hRTS UAP \j U?^k|eP j eP ja?\%PgRpe[&P [&P,hz \X[&P,^~RpeP a?Uc
,v v0Qt Z Qtp WS \%obj ^}t[&h%]#XpUffx z

q^TUZXTYUtRTS,eiRm ,b0f[&^P \RRTUZXprs[&P,eiRT[bP hl\X[&P ^TRpeP a?UcRTS Ug] UZXpny%x1vt Z !T,
S,e^ePk[&P#,P,[RTUyj UZXp[&YeiRT[&\%Ps\q Rpe[&P,U?j*qna \g\%^p[&P h/e^`[bP#V,]#R`a?obe] ^TULeuYeiXp[ePxRd\%lpRTS U:a?oe] ^TU
eP jqgnA^TU?o&U?aZRT[bP heo_e?n^u[&RT^}Xp[&h%SgRTr*\%^TRfob[RTUZX eoz:\_`UZYUZX* ,bx%[b^o&U?lRQLRTUZXpr*[bP,eiRT[&P hzOQP
\Xpj UZXRT\=V Xp\YURTS [&^fX U?^T] oR}] ^T[bP h3ea?a?UZV Rpeiq,[&ob[RKn fUZ,P [RT[&\%!
P z c0_dUP U?U?jART\,P j|ekrs\gj,U?om\%l
N,,bx%RTS,eiR}[&^}eo&^T\kesr*\j U?o\%"
l $#&%'K ,b0, cG_WS [&a S[&^+X eiRTS,UZXfj,)[ (a?] oR?zf+\RTUeo&^T\
RTS,eiR:RTS \RT[&\%P,^W\%lm_`UNei]#V# eP j]#V#Jea?a?UZV Rpeiq,[bo&[RKnkj \P,\R:S U?oVRT\^T[&r*V,o&[blbn/RTS U}V,Xp\g\%lJ*
z :\_
UZYUZXNcm_`UaNeP^~V,ob[RN ,%[&PARJ_`\^T]#q V X \%hX er*^N,
+ a?\%P ^T[&^TRT[&P hk\%l`RTS Ua?obe] ^pU=}eP j +
a?\%P ^T[b^~RT[&P h\%lRTS UyXpU?^TR`\%lGRTS U:V Xp\%hXverzm:\RTU+RTS,ei*
R +mWUvRTU?P,j
^ +NzyS UZXpU?l\XpUc[&P\X j UZX RT\^TS \_
RTS,eiR: ,bx%k[b^`obU?lbRQLRTUZXprs[&P,eiRT[bP hc[&Rd[b^^T.
] (a?[&U?PgRdRT\8V Xp\YUfRTS,ei/
R + 1
R +
0 + [&^yeaZna?o&[&ac#RTS,ei2
[&^yea?a?UZV Rpeiqo&Uc1eP j3RTS,eiRyRTS U}a?\XTX U?^~V\%P j [&P h/o&UZYU?orseiV V[&P h%^ eiX U^T] [RpeiqonXpU?oeiRTU?jz
4365 87'9 s:0<;
\Xprkeo&on%c_dU}[&PgR~Xp\gj,] a?UfRTS U}l\%o&o&\N_:[&P h/P \RT[&\%P3\%lmo&\_Jea?a?UZV Rpeiq,[&ob[RKn%z

=<> >

fi?A@CBD8EFHGCB.F



b

c

IKJHLNM.OQPffRS*TUV WYX[Z\<] \QW&\&^)^ Z\<]Q_ \&^`] \QW<_ \&^ Z\QZ_[\&^ WY\<]Q_)_baffcCdNeHfg

hjikKl1mbn&mpolrqsutwvQxyNzC{}|~8~N8PPffeHP}PejYCJ4LpdO/'P2PffgP/dN
e4MgQPgg`rpdO*gdNP 8CfeHPP/JHPOOQPQYJ4dNdNJ4g/Q<[
4b[`1J4cPffpdNeHeHd /JHL dNfCJ)JHdNgcdNe4fS
//P$Pfg/
/JHg/ eHJ `O `1.
/J4g/ PQYCeHP`O `1. f
R/pdO2P}PO LOQdNMfjJHgQ P, \ \dNfi e4MgPdN .bdO2P}PO .JH
J4g2fPCPfJ4fAJHgd dNgO<JHcP
OQdNLO$JHg}Q$[ J4cPO<PffP$JHgCfAg`JHg2e4d PQYeHP`O `1.
f

/J)c

cCP,dJ4dNdNQ*JHgfCPCPfjg"J4cP,OQPJHdNMggP JHdN OQPCeb JHL` Q
` /K \ \ JHg,LOQdNMfAJ4gQ ff
P dNCfj <1 \





ff
fi


fi

/cPO<Pff < \ \ YOQPffcPeHJ)PO<eHgdN1 \ &\ p /cdNgPO<Pe4YJHdNg2d M.O/J4
dOQd }PAcPAeHdNLNMPAdN cPdO<P pdOeHd [dNMfPf 8MPOQJHPg *Pggd J4YP,J)c
\&)^`_
adN'MCe)JHgPg/J)c pdO
eHd [dNMfPf 8MPO J)O X
QX )^`_



OQdNLO$fJHPOQOQPQYJHdN





r]$Z X Z K` \&\ Z ` a<\

YOQPffcPffe4J)PO<eHgdN,cdNgPOQPe4YJ4dNgd MO,JH'
)^`ff_


C i8#"}o%&i$& qs(')Y"}8 Y*C ~,ff+.-8N)/'bN,ff 10}Q$2[p Q4b[[13.b`6C"N fiQ< 'N#[$ Q4b
!
5 ! 6u
/cPO<Pff < \ \

798,8

fi:<;>=?@BACDFE ;6GH@BA.IKJ.@L=AM=NPOQE ;>IRSUTV=WCR@LXY:<;>=C;>IG<Z
[ \}]9]_^(` ac bRy dfe6gihhKjUkLl4lmknPkLo/pqgPrmhsrmbRpqrthKjuavb6d(h3gid(nxwzywz{}|vb6d~g#d%hK6d%g#d~eRop3(d(lM}i3 |vkBrm b
acbRdj/hKoLoh9|vkLRg#d(lm6oBrFkLlvpURkBgid(~rc(hK6lmd(6d(6(d}hKjavb6d(h3gid(nlFzy1p36Y>yy
W,WR,Lz(W \K&] ff \] \_3 / ]K_ ##_ ( U ^} ] ^ < ##_ ( `
kLp3llv oLh9jRhK|coLoLh,p3|v(/(l(Md~y e6WriBpqRoLd3)9y<6 hKR6/&likL 6ffd~Kg&rmbR>dHe6gi>hKK3Rg) p3n 3,K6>R63 Rz,R6R d& lmb6Kh,|zKrmbR #pqKrcyVrmacbRdb6d(e6gihKrmb63g)dp3e6n gih9hK6je6Rgih/&( d(d(ffK6 l
yvavb6de6gihK3g#p3n K #d)>rmd(66l ,6R
zyvavb6de6gihK3g#p3n K # 96V kLlcp3~(okL|yg9yr(yRrmb6doLd~ d(onpqe6eRkLR
mW, ff K V _ , wzy
yvkav b6dF6e6p3gin hK3eRg)op3dn yw 96jh3gVVpq rmhKkl<np3l (|v(d~kBe6rmbripqgiRd(oLo/dvpqrm|kLhKyg9 yr(y . p36 1{ rm b6dvkL rm{d~gie6gid~ri{qpqp3rmRkLhK u { |vmWkL9rmb ff.K6 Vd R R d(_ p3 l
, p3R |vkBrmb {6l9yr(y
/ mW9 K {
p3R {R {p3R pqg#d}p3lc d(jh3gidu 6p3n eoLdQywK#y
>yQFhK6lmkL6d~gvp13gihK66 kLRlrip36(d
mW, ff K V _ mW9 ff K V _
hKj #y<avb6d(
mW, ff K V _ , w 9 mW9 ff K V _
6d(hK 6pq 6rm6hKkLhKd(RMlmkL{ 6p3kBlcd~rPgMjffp3bRrmkLp3b6oL.ldgipdzR Rd~6gmRkB rmhKd 6 6 6d~ g#l(y 9 Rrg#Rd( d3){F |vkBrmb p36lm_| d~g ff ) ,y Qh3rmkLd((9dYp36rmlmbRd pqrkLlRoLlmh,kL|F6








)! +*


-,

,



!





fiff

" "$# &%



('

/. &0

,

" "$# &%

1


fiff

-

$243



65

F
PO

fiG HAI

87 97;:<7>=?= 87 >@<7A= 97 !@<7>= 87 B@C@ ED
KJ
L J
L NM

Q8RTSVUXW>YBZ\[^]\[`_a[bdcfe[hgjikgB[
l^m&n\bpoqWAeY1rstWAgfiuvmwn\xyYBma[`n

kLlPd lmR66nkBrmnkhK6pqgilQk (wzd(y {kL Qp3d 66kBzrmkLy hK e6gih,y k6 dd(oLp h,|nPy d~rmb6 hYrmb6jkLh3lPglmeRd(gi~h9rm>kLhKkL6 1{F|od(djr_ 6grmd~#lgirPn6kLRklmpq(rm6khKlmYl hKp3jH63p p3zri p3 K{Rd(|vlYb6p3k6ib
.hKjg#p9|c p3l rmlQbRpqhKr j kLrm6b6(kLh3lcgmne h3d~g#rmpqb6rmhd( ly rmb6Qdd)>6r(h3{rmkL| hKd}6lkLz| rdQgihbR6p96 (dQd}kp rnPgihd~6rm6bR(hzd(6PhKokLhK13rmbRdvjh3e6gvg#e6d~g#h9kLhK>RkL6lH1lmd(oLd(~j/rmr_kLhKrm6d~l9giynkLRkRpqp3rmkLoLhKoBK {
| dKkB d}p3Yd)6p3nPeRoLdQkL h3gi6d~gcrmh kLoLoL6lmrg#pqrmdQrmb6dnPd~rmbRhz6hKohK3Ky
6 6 R > 6 . R
klckLRl(yPr(yRpd(kBnrmbRpd~.g kLnp3d)olmrmd~d(r R6l hKj (vhop336gclmd(kl (dhKj d~gilil(pyr(y j/h3ginlHp3p3~(oLkce6gihK3g)p3np36
zy _j d)rmd(6Rl crmb6d(
z

K{

|

pz

~}
$M





`{







C8
~

\

~

q

fi CGI C fi HA


q



q<



CI

C1 )H

C

<

d'

B

!H

k

<

fi?A>

aB<B1qP!A1yB1AC> dd\
!!C
1d
yKt ddABAK1A6A !$C<!y> <?!A
!B!/AA!A\^T!A6
!tB9^
`<A f \ ddA
; AAAd
aB<B1qP!BhK1A
AC/A <
yKt vAAB1A ddN!TN\!A1

< dd~!!CA1\!A!6BF!AAA!
!Ak 8!6B++^9\AA!A
(/!9!KA qt1A?>6A ) (
1/A \ ( AA ddf \ AA

~!A!!C!tAKA?1K
1)BK>/
!p A!/!
KAC A9!A8>/A
Bt;ak1dqAt!\AKd!
KCA8A
fiff !/!TA!A
1(AkCA!!(1A
1!A
1AdAtd!AdC
C6p6?A1AATA(/! XC>AK
<
(!A /!!C!q;ak1Kd>q8<
!t!A!
AA!/!



f






q
B !AdKAA6!A A!A!<d(B$A ?AT6
><8+
y1(A&!A1A!!qA9A!
!d)BKC
AAA!K^!fi^A!/K11d>h61qA!AqA<)qA!(/1(B
A(8>>&!A1(w
L
"!$#&%('*)+',-%/.103254687:9<; fi=?>-@$@ 7 8
ACBDFEHG8EIA$JFKML dBh1AA!
NN<&O ! K\yq
d!3k?Ad^6!A HP
d!
KCA
h6\PQ RTSVU+W-XZYH[\[\ZW-ff]Y?^
_`aSb3Tb*ff]b\Adc`ch)eYI -f)HPhA qAAA

q AAqhg
Cq!!h!AA!A

)

`!8 A!qB$

8Aq

/ia\ZjlkmOjnporq 1s)pO)HP tvuIk/
w

`<A~A yxf \
w

`<A ? \

qPq!? >


z|{+z

fi}~y-`-H~C`8:8--HfiH~fihvRfi}~--~-

Qfi|I"C
fiC
CmC\|H
"*Im88I

Tfifi`C:pIm
R*ICQFH
CTfifi`:1/V-
fi3|CCI3:V:MCT88I

TC-`3
yC`:I
?C:C:


TCfi3`:lImfilCTm-
HmfiCh:p
"I$fi*I"::h1IC8C1T|
ICC3|mT|-R:&CaC:IIfifi|mTHT|1CIp3::C|mT|R-C
:C
:Ch8C1T|fi-:&CC:I\ICT
IIFI3C|II1

"I?3MCIp3:
MC

fi
fiI`CI:3fiIlCTfiC::I

TC-`yCIfi&
8
\CvTC
3C\CC-|::(CT8CI

TCfi""C:Ca|CI-I
fi(* :fiCTIHICQC:I\I
+"

"

:

&

C|Hfi`fiT88I

TC-I


CQC:I\I +\


CC :\

:CCTfiMTH
"CfiC
&H

8:

dyHe

IfiC$TC\T:C
+
C


+Z* e

\1

3I

pyC
(: +\ IC:( :\ $\
1
T`H
`:
Z*

e


- ++Z
C &H 8

C:


* Ifi

8H



Ndy `*:3|fi :\

-TH-ff
QTfiC: fiy

fi|H



3ITy
3Md|d
C

3CI"Cfi|-V:|I
|ICIC


C

C:II
+"



:

&



1
|ICQCC:I\I

+\

:\

-IC

1

3I3

"yC


CICTp8\
-I::8pTC$`3I:QfiT

$I`:THTfi


mm|I
CTfifipIfiT

$I(1I:8pTC$(CTC"C

:1::TC-
r`T
`H
`

C8fiC:I\IpC$C"-:

:Q8fiC:I\I

1

:
`3ICh

1:`-h1
|IC:mfi
yC`:I
C
:88+m|883|I

TC-`

IC



Tfifi`:



C
IfiTI3:C\fi
I1I\
3C:|CCT-C`:1
|I3--fi
CI
mC
TC\IC"RI:

!#"$!&%'/1)(+*-,/.102.43658769'7:.8;=<?>A@=7/;CBEDCFG
HJI

fi`K

$LNM

R
C|O QPSRSRSRQP

UTp|d
-ITHTOVXWZY P L\[ ]_^

` Uacb ed |`
CCfUa
`
CTUa&Ighacb

iQj2k

filnmEo8p?q=rcsEotr

uAvxwzyE{Ew}|~{=z-wSffwS{C-cc=wcwz/wSO6=E|c{=n /t 6O /NSS/Z4 6\{==w}|c{wz
E-wz-6|c:{ /t } / SS6O &:cc2S
ew2hX|cfz?S~|cx=wS{OyE{=OEwS-wS4:{==|~{=ecwz/wShff6='|c{== Q =wz-|cw
E-Q/wE6X|cSSwz=-6EcwQS?cwz/wSEO6=E|c{E= /K{E{|~{CwzE-wz-6|c:{2/2
e_|~{===z|c:{8tE:-wE6 :|cx=wzyE{EwS&wz/wz {=t=4:weE6
2 |c=wzy'{=wS#wz/wz eff |ch|cfz8Sc|c {En#gwz/wz eO |chU|c
SSwzE-6Ecw g|) eXJ f=wS{
/ffh cU |~Kz8Sc|cg=wS{=-w= 6 =wzyE{Ewcwz/wSO6=E|c{E= 6 &K cU4
QSUcU hX|~z8Sc|cQS4 6 {=SS#c-:={=|c{=-{ESwS
QSSSQ :S~=wS:UcU &wz/wz eN |~ |c=wzy'{=wS|c{U=wS{
6 g U 6
U|~z8Sc|c}=wS{=w 6K=wzyE{Ew}ffcwz/wSO6=E|c{=\h 6 #UcU h
QS)
|_ egwS|)=wzUcU h|cSSwzE-6EcwneQS wSS|#c|cSwSff=wS26 {=
6 |c{OE|cfQw-wzf264wx6
gyE{=wSS|#c|cSwSO?=wSE26:U h&4 {=n4wSS|~c|cSwSffff=wS
26 :UcU hSS#cf:={=J|c{E-{=SwS SSSS2 X :
S~=wSg:hcU{=#xwz/wz \ |c |cg=wzyE{=wS|c{nU)Ug=wS{
6 g 6 Q
|c|KEc8-:={=|c{E-{=SwS+ QSSSQ X :ES~EwS:EUcU{E}&wz/wz
|c |cf=wzyE{EwS|c{h=wS{ 6 g 6
g4Q/w ffZ /t C6

wE-Q/w'6=|cffwz=}|cSwSz |_w '6X|ccwS#wz-ff|c{E6|c{=g|c=Ewf62/wwzE
|c6=Ec|cQ6'cwfU=wQU|)\{E:{8-:={=Ewz-|cwS w}=we=we-|~:|c{E{=|~:{:4:={=EwS={=wS
?ffg=g{=wStwS-=| =|cK|cffwgQS==w|~{CwzE-wz-6|c:{-wS-=)|c{=#-:=wffwz=?4

ff
fiff fi

! #" %$
$ '& ( )*,+- +/.1032452672 98 *,+
;:
@? ?
<$ =1 >
> CB f CB SS S2B CB |cf B -:= {E|c{=-{=Sw:C${=
2/t
ED SS #4

O#$&'
tt=4:w'6Ew6|)|c:{ SSS2 :X
/t
{=2/t 6w=-|c{=wS\=|c{=EwwzEO:vxwzyE{=|)|c:{ ? 8 Uwz SSS2 X f=wS{
|~
XE }2 6 x|ch&wz/wz =w-wz

|cKyE{E|)w



FHG ?,I$JKMLON )QP;P(QRS-T6VU8;6W6VU-XP8 4Y6[Z[6[Z[( *OXQSSSQ ]\ E /8 C 8 *+x2/8 C 8 4 -T(&5678 ZV*- +
)w R^ZV*_`6VU-bac-S6VU,(S{S+H]|(@df} eg-ihC*]Z[6[Z[( *kj2ml,2Xn-S6$o&p-q8T& ( )*,+-p+srS)]-54^t%0324Y2u672t C6v8 *,+f2 8 C6,2
U-5*x-5y -54^tHz +-54^ZVyQ8~6[Zi( *x(@d$ZuRWhC*]Z[6-g8 *,+Z[6Xp(~*678 ZV*ffRH( *t& ( )*,+-p+r5)-Y4Z7-RY2
4(S(@d52 wSQ~=E62/t C62/tSS26==4:={E=wSEwz
$
}
$
QSSS2^$ wg=w
yE{=wEw tEcwf
e$ => @?~A ? > - $ =1> ] SSSQ$ $ => / $ $ =1> / :8=)|cwz
|)Z#nE-: {EZ{|c{Cwz-=-wz-6|c:{ $ ~'
<$ = SSSQ <$ =
=
SS

fi3,,#;]Cff~ffM,,k%;;gM3,,,3
]Y^^S555 X Qp% ]M MX~f]~ p5Q ~]b55ff#MC]p~MXpMkQW
]T~`~E]5 p5

]`V~MMM]g 5Y M~sMMM] ^Q sQ]MSQ M~~ M#kY
[O9,V,TEkE~V,
vW] kMq!]T]^~ ^ OMX]5 YpM5c]M^vV bs ^Qv ]vb']5XW ]k]]
g]] ,Qq]5b] #5M~s c Y5MM%,Q vvup~W gC% M]H ]M]^~ ^
b`]Y]]Yp% ]p~ ^ ]M^V g]~
5k]1v ^Q bc]]5bWs ]
5~v] 5W ] Qcfi ff ]p M]5k^~op5k ]T M] ]]5C ,Q]'] 35M~`
Y5MMT,Q Mup~ WT v, ] ]'Q^55~p M]M]kQMS %~]T~f ]^]]5
5 fM~]ps ^



b



c

fM~ffpsfi ! #"i $&%"i "i %'&"!"i(! '&"i "!")`]~M]


q]p~ ^ * 5~] q~ ]T5 ]p5
,+
-fi.0/#132546187946:32546:87;=<
>@?fiA3BDC /&:2548EGF;54
HI-*JK/#1K2K461879461L4M:2fi;4
+
- NO/M1P4M:2546:;4
+
-.9/M1K254Q1704M:L46:87;SR
,+
-fi.0/#132546187946:L4M:T;U<
> HVI*-J5/#13254Q1794Q1P46:;SR
W ,+
-(NX/#1L4EYE[ZL4Q\
F^]:32FL4M:
7*;_<
N- N8`-fi+9/M1P48EGZL4Q\
FT;4
+
- NO/M1P4M:2546:87*;R
,+
-(NX/#1L4EYE[ZL4Q\
F^]:32FL4EfiEGZ46\
FX][:
7F*;_<
> NT-(N
`*-fi+0/#1L4EGZ46\YF;4
N- N8`-fi+9/M1P46:32fi;54
+
- NO/M1P4M:2546:87*;R
,+
-(NX/#1L4EGF4E[F;=<aR
b ?YA3BDC /#:L4M:T;c<@R







dVe f

b

figih*j
kYlmon*j8m

p*qor*sut#v*wxpy&z{Dy|D}~*T)K=v*wywq|(t#ozUV*9Mfi6*6)os*w*wUY_t[Oz}r8t#rT|Dqoq!
w'8qors#!wV|Ds&ws*zDy#y&ws#p3z*o{t#zt#vwq|Drs#ws |DXOq|Drs#w Ows&y&!KwsXt#vwV|Ds#wvwy&w
t#vwy&ws|zfi*wt#vT|(tzYwszDtKwqoz{t#z_|DfiU|D
qooxpT|(t#vo=y&z}9t#zfiy&sMtPt#vw
y&wq|(t#z=#Vosrs&w_t#z*s&r&vU|,*zfiwD*w'Yt9t#vwzYw|D_t#vwzDy#y&wsMpKz{i|(y&s|(y&w
wqowt#wy&z}t#vw{Dy|(p*vK^rs&o{t#vw,y&wq|(t#ozT*|Dqoq!OV=osV|Dqqowy&wry&s#!wq!czt#vw
y&ws#r*q!t#o{{Dy|(p*vKOq|Drs#wws#y!Kwst#v*wuT*|Dq0s#!t#r*|(t#oz5Kvwy&wzfit&|Doszqo,zYwst#v*|(t
Kwqoz{t#zs#z}wz!t#s|D
qoop|(t#vs^yz}@9t#zLvwy&wq|(t#zx96*6
vzqosOot#vw
{Dy|(p*v
osLzDt&|Dwy&z}t#vw{Dy|(pTv*Ywqowt#o{u|Dqoqt#vw|(y&sXzt&|Do*o{t#v*wzYw^z*
tos^y&wr8ys#!wq!w*wxYt#v*wq|Drs#ws*^|D3|DszwOzrqow'
pKwt
Ts#wy#wt#v*|(tYrwy&wszt#v*wzDy}aV9Mfi 6
O|Doq)zDywwy#x9Mfi6*
wpy&zwt#v*|(tK*suqowt6t#wy&}o*|(t#o*{Y_rs&o{izr8yKzDtMt#z}[rp_}wt#v*zfiKK
V|DiKwpT|(y#t#ot#ozwoxt#v8ywwpT|(y#t#s
ULos^t#v*wpy&z{Dy|D}~*SzS9|D}p*qwfi

U0zs#sMt#szPt#vwq|Drs&wsS9z^Kp*qor*st#v*wq|Drs#ws p*pz~V

T3
USzs#sMt#szSt#vwq|Drs&ws 3T|D*,Oz9KTK

oswV|DsMit#z&vw&,t#v*|(tSos|D
qooD,zDy&wzwyV5Sw'
t#wsS(3|D0OwV|(
q!iw'Yt#w*s
$yV$t9
8zOwV|D_|(ppTq!it#vwKzDtMt#z}[rp|(pp*y&z|D&vt#zz*sMtMy&rt|xqowwq9}x|(pp*{
Q6fiD |D*,|Diofit#wy#py&wt&|(t#z QMD Lv*wpy&zYz5py&zYwws|DszqqozVsV
U os|Dwp*t&|(*qow$y$t 8 |D {!wio8|D}p*qowfi

U0 osO|DYqo$yV$t 8 wTw|DsX8|D}p*qowfi)zDyM3V|D
|Dxs$t
96*6
* fi
zDy&wzwyV5q|Drs#w zX~V*Tis&|(t#sM*wst#v*wuz*!t#oziywq|(t#o{t#v*wt[OzxqowwqS}|(p8
pTo{s

zDy&*wyt#zw**w 8 8 D| Ow|(ppTq!pKzofit^Ozs#wyOt#vwqowwq5}x|(pp*{
9Mfi6*6
fi
KG0)*6

|D*iqowt
96*6
*
8qoosMt#s|D*iw!t#vwy^T
(5zDy
*
$#V(,#P G
968 xqoosMt|D, )

fi G KG0
8 fi ff
toswV|DsM,t#zx&v*w#t#v*|(t |D |(y&wsMpKw|Dqo w}zfiwqsz0P |DO0(
ywsMpKwt#!wq! t^y&w}x|DosPt#z&vw&t#vwt#wsMt#soxpKzofit#s^|Do


^zs#owy|{Dy&zro*sMt&|Dw
V*9Mfi6*6


KG0)* (#V(9MfiM96*
96*68M9Mfi6*68ff




fi!#"!$&%'(*)+#"-,fi.-!"/!0213'("4'(,457689$4:;!$!,!)<

=fi>@?BAC+DFEHG4IKJ(EMLG4INLKO
P QSRT*UV

?fiW V8X WZY4?fiWZY X P []\M^`_*P Y!? Pacb7dfeM^gPihkj4VmlonpU Y4?fiWBq]rsA P []\Nt

P QSRT*UV

?fiW V8X WZY4?fiWZY X P []\

^`_*P Y!? PacbM^uPiv
w9RxNUV

P QSRT*UV

?fiW V8X WZY4?fiWZY X P []\

^`_*P Y!? Pacb7dzP Y|{ PKacX}^gP QSR~FUV

?fiW V8X W V

WZY4?BA P [fiy
WZY4?fiWZYA P [fiy C

[ y+ ]
[ \ P ^hkj!VmlnpU Y4?fiWBq|rsAW QSR~FUV WZY!?fiWZYACHG4EKYI]*Y4?INE
=}-*m=fiELG!INL fi
P

P


P

LKY4?^
qBr!I]*

Y4? P @
C G*EKm


P QSRT*UV

?fiW V8X WZY4?fiWZY X P []\

=fi* *EI]=fi**

g

^`_*P Y!? Pacb7df_*P PacbM^uP Q(RT*UV

4
LI]*KE
QSRT*UV

=fi>

?fiW V8X WZY4WZY X P []\ C

?fiW V8X WZY!WZYA

h+vw9RxNUV

?fiW V8X W V

WZYA

X AC+DFEHG4IKJ(EMLG4INLKO

P QSRT*UV

?fiW V8X WZY4WZYA P []\

^`_*P PBacb^gPivw9RxNUV

3 EJ(ELG4INLLG*E*EKEK4KE=fi>LG*E
LG*=fi-L
LE 4INL =fiCD
QSRT4UV ?fiW V8X WBq]rWZYA
AC

LK EK>sLZLE

LEI

hkj!VmlnpU

4INL =fi=fi 2*=

?fiW V8X W V
?fiWBq8rsA

WZYA P [fiy C
7>**4I]EK|LI L=fi4INI]|LEKE EK>sLZ
U LIN(E>s=] *

LI]4KELG*E3|*E

=fi4fiE@G*=

p28-]
LG !INmE@E*=]m=fiEK& E2ELG*=*3>s=]4=J *2LE
]I]|

LGEK
mEKLL=EK= -L =fi2

G*EKE2ELG*=*3K=fi

LGK=fi*
L
4L J(E*EK(INL =fi;I]*= =fi7E EKL =fi E]C

4 *E}LG*E*=]L =fi4=fi>@I]KKE*LI ! L&I]*I] LfiCG*E/*=J *EI2=]E

*I]L 4=|=fi>LEKG* |*E3>=]@LE
EKmCpDFEG!IKJ(E

4INL =fi=fi>IfiEK*EI =fi *=N

!INL =fim(G4EELG4EEKI]|L

4>s=]INL =fi*EK





*
L
INLEKLG*EE EJfiI]*KE=fi>9LG*E2ELG4=|* EI]*p=fi>9=fi2EE*I] EKKfiG*=

!INL K IN+LG4INLEK= -L =fi2I]4fi2EK|LEK

LG

G4I]K=fi*
L
*L (
J E3*EK(INL =fi

*

+m=E>


EK*=fi*fiG;L=>s=]I E3I]* EK2EK|L |LEEK
L 4H4= 4 EK *=fi=fi*=]L=fi EI]=fi *C
(
DFE=fi
E L=K=fi* *4E LGI]c= EJ]INL =fi=fiE INLEK=]9Cc4LI]*pEK-EKG
U ?fi]SA SL
=|*4KEKI2=* INkIN**=(I]G>=]+*=J *MI]KKE*LI 4 L=fi> =fi 4=fi]I]K CE]C#SLG*E
*=7*=]L*EI
*=fi]I]2
EKI]*E

LG*=fi]I]K=fi|LI *7*EK(INLEKINL=fiKC++=J *LE

LkI]=fi*|LL=

=fi>+LG4EK=fi
E L =fi*=fi>

*I3=|*E =fi>4LG*E@K=fi EL =fiM=fi>4I*=fi]I]
L *=fi]I]KC4LI]4kEK*EKG

E EKL =fi; E]-G*EEM=fi4EL
EKL=2 2 s> 7LG*E*=|=fi> * *I]
I]m=fi 4 ]
E =fi #4

;I]

}K=fi

K L@LI]
9

4 *2=*E

*=*=]LLI]
E LG 3*= 4 EKC

LG
!INmEEG4IJ(E*=J *EKFI]I LE4INL J(EIKF=fi>@*=J *LE



4INL =fi=fi>k|K9
fi =fi

;I7=|* IN+IKfi!4 *HLG*E*=]L =fi=fi>pI]KKE*LI 4 Lfi*EKEK2IINLG*E #



4INL =fi/MCiCiLKCLG*E= =fi
L
L EEKI]SL

*EK2EK|LI IK* *LG*EELG*=|4= =fi]

*
L
INLEK

*>=]INL =fi
EKL =fiC

(44+4`48(

G p
EKEINGI]+!INL }**m=]LEK 7LG*E
*
L I]
EKEINGL fi
= 2fi? eU =fi24 =fi
X AC =fi (E@L=LG4I]-SI]-L
LEK>=]+*==fi>*EI] *3LG p!INmE(M
L =fi>!*L+>=]+*=]m=fiZ
*2LG*E7
L**=fi>I]
I]*I]KKE*LI 4 E7*=fi]I]K-I]-/8EK* ->=] EI]I]|L K* =fi*K
I]@E I]LG*EI]*=fiS=fi*EK>EEKEK>=]*EK> *fifiEK
L =fi*I]*2K=fi2EK|L=fi2I]EIN
E +J(E =fi
=fi>kLG !INmEC

N

fi;4*4-

&*Z*(44

#


fffi @
"!$#%&!'&ff)(
+*-,
.0/1,
23,465798;:&2=<>:&?@AB798C2EDfi>FNHGGJILKGMG

*K

>#NO#+PQ&RSUTV#% 2"!$#%&!'&ff)ff) W"%X'&WYZW"
%J'N[#W]\^'_($`B![aJcbVdB,)ef:A4g2H5hi:j
k :[D8;lnmi4:[DJ45?o?p8C2EfiD >qSFsrutNv wKfxE

*K

y#i{zN
YB!$
([|"~}oz!$#Laf W"%[!$ff) W'N[ #W=#n%
W"!'&3"!# #% "!$#%&!'&ff)(
W
mi4:l,$,$&8C2ED&:6j7CdB,g2H7><>:&2Lj
3:2UbVdf,$:&4[,
798;l$5hf@f,[l7C1:6jn<>:&?@AB7,41H:6jg79.5&4[,E+#;TV1
I&M !*H&MJILKf& "!$W"%!i+!$'&%f

*K

's!'&#
C#W"Ymfi+&RSoT#% "!$#%&!'&ff)ff) W"%'&W"YW#L~
Y%-!"!$
([
W|$'N[ #W]bVdB,
eB:&Af4g2H5h:6j k :[DJ89lQmi4:$D465&?p?o8C2E(D NqSF&rt]v wx&GKLRJ
!'N6J#gVMJ1mo k /m46:[D465&?p?p82Dj
:&44798 l
8;5hg27,Lhh8D,2Hlg,+Y"Y" ($#Wf
([
|'&WH}o>JQi#W"(
6!$`"[ aJnW"
%J'N[ #WH'&($
Y#W/[|"o
#ffH [
YY'N$'sH'&($&yWmi4:l,$,$&8C2ED&
:6jn7Cdf,p7CdXg27~<>:&2Lj
>5&2H)"&?@3:2 k :[D8;lQm46:[DJ45&?p?p8C2EfiD 9*]gKIE
's!$!NJx&J k :[DJ89l_5&2H_o575S$5s
,+|'NNy
%J'N[ #W'&(o'& `"!$]84&GKGJEz
W`"ff
z!$
([K( "~p
}B$|B!$J]+}o#>}

#&![]w&RS!$ff) W'N[#WU#i #% !$#%&!'&ff)(
\-1|^W"aJ!6
WY" W"%
( [#&![pbVdf,neB:&Af4235h:j k :[DJ89lQmi4:[DJ45?o?p8C2EfiD >qSFsrutNv Kf&M

}!$([|"#~#[ ]1-~Jx!$ff) W'N[ #W#!$1!$[ W"%feB:&Af4235h~:js?^$:h8;l<>:&?@AB75798;:&2*-fi
MKM
a'&W"(KBJy
%J'N[ #W'&(C'& `B!$i'&('&W)'N*"!$#J'&|[#H[|"1|'&W"('&W"Yff)
Y"!$ff)#]
8"!#&
ff
Wmi4:Llg,$,$&82D&p:6jQ7Cdf,-tJ2Hg27,
42357989:2H5h"&?y@f:s8CAf?:?y -*H&GKfxE
Q#'& ([1Pn#{f!$%#]Kfi MJ #% 3'&([
YcL'&
`" `"(^#QaJ
WS[(L*o,./1,23,
45J7989:&2
<>:?y@HAf798C2E(D H- HMJxLKJIE
fi's!$$|"#&!$ JIFff)[|"#EY# #%&U#&!7"!$#aW"%;[!$ff) W'N[#W #1%
W"!'& #% 2"!$#%&!g'&ffX(

Wmi4:Llg,$,$&82D&:j7Cdf,q
7Cd g2H7,4g2H5798;:&2H5hieB:&8C27X<>:2Lj
,
46,
2Hl,:24798 l
8;5h>g27,
hCh8D,2Hlg,
gew<yB F 4*GJI&MKGMJxE
fi's!$$|"#&!$ "MJW[!$ff) W'N[ #Wo#%
W!'&" #% @"!#%&!'&ffX(Q!Li

#W"(6!$`"[aJiW"
%J'N[#W]
bVdB,-ef:&Af4g2H5h:j k :$D8;lQm46:[D465&?p?p82ED(tf qB3MK
fii's![|Efi"#>'
J
(z1MJ=f#ff)| #([#]| L'&"!$#&H
ffX(-!#ff
's! [
'&] |W [
%
W"
& 5Jl
dS8C23,g27,
hCh8DE,
2HlK, - RJMGKfI&JE
([([#Wf-VJ1m4g823l
8@Hh,-:j4798 l
8;5h"g27,Lhh8D,2Hlg,"!$ W"%![+!$'&%f
[!$ W"%BT+#Hf|'N!# V&RSbVdB,4g7:jmi4:h:[DJfi_uz!$
([(


L

[|"(
$'&W"Y-]# W|o#


fiJournal Artificial Intelligence Research 4 (1996) 37{59

Submitted 9/95; published 2/96

Logarithmic-Time Updates Queries
Probabilistic Networks
Arthur L. Delcher

Computer Science Department, Loyola College Maryland
Baltimore, MD 21210

Adam J. Grove

delcher@cs.loyola.edu

grove@research.nj.nec.com

NEC Research Institute
Princeton, NJ 08540

Simon Kasif

kasif@cs.jhu.edu

Judea Pearl

pearl@lanai.cs.ucla.edu

Department Computer Science, Johns Hopkins University
Baltimore, MD 21218
Department Computer Science, University California
Los Angeles, CA 90095

Abstract

Traditional databases commonly support ecient query update procedures
operate time sublinear size database. goal paper
take first step toward dynamic reasoning probabilistic databases comparable
eciency. propose dynamic data structure supports ecient algorithms
updating querying singly connected Bayesian networks. conventional algorithm,
new evidence absorbed time O(1) queries processed time O(N ), N
size network. propose algorithm which, preprocessing phase,
allows us answer queries time O(log N ) expense O(log N ) time per evidence
absorption. usefulness sub-linear processing time manifests applications
requiring (near) real-time response large probabilistic databases. brie discuss
potential application dynamic probabilistic reasoning computational biology.

1. Introduction
Probabilistic (Bayesian) networks increasingly popular modeling technique
used successfully numerous applications intelligent systems real-time planning navigation, model-based diagnosis, information retrieval, classification, Bayesian
forecasting, natural language processing, computer vision, medical informatics computational biology. Probabilistic networks allow user describe environment using
\probabilistic database" consists large number random variables, corresponding important parameter environment. random variables could
fact hidden may correspond unknown parameters (causes) uence
observable variables. Probabilistic networks quite general store information
probability failure particular component computer system, prob c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiDelcher, Grove, Kasif & Pearl

ability page computer cache requested near future, probability
document relevant particular query, probability amino-acid
subsequence protein chain folding alpha-helix conformation.
applications mind include networks dynamically maintained
keep track probabilistic model changing system. instance, consider task
automated detection power-plant failures. might repeat cycle consists
following sequence operations: First perform sensing operations. operations
cause updates performed specific variables probabilistic database. Based
evidence estimate (query) probability failure certain sites. precisely,
query probability distribution random variables measure probability
failure sites based evidence. Since plant requires constant monitoring,
must repeat cycle sense/evaluate frequent basis.
conventional (non-probabilistic) database tracking plant's state would
appropriate here, possible directly observe whether failure
occur. hand, probabilistic \database" based Bayesian network
useful operations|update query|can performed quickly.
real-time near real-time often necessary, question extremely
fast reasoning probabilistic networks important.
Traditional (non-probabilistic) databases support ecient query update procedures
often operate time sublinear size database (e.g., using binary search). goal paper take step toward systems perform
dynamic probabilistic reasoning (such probability event given set
observations) time sublinear size probabilistic network. Typically,
sublinear performance complex networks attained using parallelism. paper
relies preprocessing.
Specifically, describe new algorithms performing queries updates belief
networks form trees (causal trees, polytrees join trees). define two natural
database operations probabilistic networks.
1.

Update-Node

: Perform sensory input, modify evidence leaf node (single
variable) network absorb evidence network.

2.

Query-Node

: Obtain marginal probability distribution values
arbitrary node (single variable) network.

standard algorithms introduced Pearl (1988) perform Query-Node operation O(1) time although evidence absorption, i.e., Update-Node operation, takes
O(N ) time N size network. Alternatively, one assume
Update-Node operation takes O(1) time (by simply recording change) QueryNode operation takes O(N ) time (evaluating entire network).
paper describe approach perform queries updates O(log N )
time. significant systems since improve ability system
respond change encountered O(N ) time O(log N ). approach
based preprocessing network using form node absorption carefully structured
way create hierarchy abstractions network. Previous uses node absorption
techniques reported Peot Shachter (1991).
38

fiQueries & Updates Probabilistic Networks

note measuring complexity terms size network, N ,
overlook important factors. Suppose variable network domain
size k less. many purposes, k considered constant. Nevertheless,
algorithms consider slowdown power k, become
significant practice unless N large. Thus careful state slowdown
exists.
Section 2 considers case causal trees, i.e., singly connected networks
node one parent. standard algorithm (see Pearl, 1988) must use O(k2 N )
time either updates retrieval, although one operations done
O(1) time. discuss brie Section 2.1, also straightforward variant
algorithm takes O(k2D) time queries updates, height
tree.
present algorithm takes O(k3 log N ) time updates O(k2 log N )
time queries causal tree. course represent tremendous speedup,
especially large networks. algorithm begins polynomial-time preprocessing
step (linear size network), constructing another data structure (which
probabilistic tree) supports fast queries updates. techniques use
motivated earlier algorithms dynamic arithmetic trees, involve \caching" sucient intermediate computations update phase querying also relatively
easy. note, however, substantial interesting differences
algorithm probabilistic networks arithmetic trees. particular,
apparent later, computation probabilistic trees requires bottom-up top-down
processing, whereas arithmetic trees need former. Perhaps even interesting relevant probabilistic operations different algebraic structure
arithmetic operations (for instance, lack distributivity).
Bayesian trees many applications literature including classification.
instance, one popular methods classification Bayes classifier
makes independence assumption features used perform classification
(Duda & Hart, 1973; Rachlin, Kasif, Salzberg, & Aha, 1994). Probabilistic trees
used computer vision (Hel-Or & Werman, 1992; Chelberg, 1990), signal processing
(Wilsky, 1993), game playing (Delcher & Kasif, 1992), statistical mechanics (Berger
& Ye, 1990). Nevertheless, causal trees fairly limited modeling purposes. However
similar structures, called join trees, arise course one standard algorithms
computing arbitrary Bayesian networks (see Lauritzen Spiegelhalter, 1988). Thus
algorithm join trees potential relevance many networks trees.
join trees special structure, allow optimization basic
causal-tree algorithm. elaborate Section 5.
Section 6 consider case arbitrary polytrees. give O(log N ) algorithm updates queries, involves transforming polytree join tree,
using results Sections 2 5. join tree polytree particularly
simple form, giving algorithm updates take O(kp+3 log N ) time queries
O(kp+2 log N ), p maximum number parents node. Although
constant appears large, must noted original polytree takes O(kp+1 N ) space
merely represent, conditional probability tables given explicit matrices.
39

fiDelcher, Grove, Kasif & Pearl


U

,
@

MV jU ,,
,

,


,
V


@



@ X jU
@
@
R
@


X

,
@

jX ,




,

,



@ Z jX
@
R
@


Z


Figure 1: segment causal tree.
Finally, discuss specific modelling application computational biology probabilistic models used describe, analyze predict functional behavior biological sequences protein chains DNA sequences (see Delcher, Kasif, Goldberg,
Hsu, 1993 references). Much information computational biology databases
noisy. However, number successful attempts build probabilistic models
made. case, use probabilistic tree depth 300 consists 600 nodes
matrices conditional probabilities 2 2. tree used model dependence
protein's secondary structure chemical structure. detailed description
problem experimental results given Delcher et al. (1993). problem
obtain effective speed-up factor 10 perform update compared
standard algorithm. Clearly, getting order magnitude improvement response
time probabilistic real-time system could tremendous importance future use
systems.

2. Causal Trees

probabilistic causal tree directed tree node represents discrete random
variable X , directed edge annotated matrix conditional probabilities
jX (associated edge X ! ). is, x possible value X; Y;
(x; )th component jX Pr(Y = jX = x). tree represents joint
probability distribution product space variables; detailed definitions
discussion see Pearl (1988). Brie y, idea consider product, nodes,
conditional probability node given parents. example, Figure 1
implied distribution is:
Pr(U = u; V = v; X = x; = y; Z = z ) =
Pr(U = u) Pr(V = v jU = u) Pr(X = xjU = u) Pr(Y = jX = x) Pr(Z = z jX = x):
Given particular values u; v; x; y; z; conditional probabilities read
appropriate matrices . One advantage product representation
40

fiQueries & Updates Probabilistic Networks

concise. example, need four matrices unconditional probability U ,
size square largest variable's domain size. contrast,
general distribution N variables requires exponential (in N ) representation.
course, every distribution represented causal tree. turns
product decomposition implied tree corresponds particular pattern
conditional independencies often hold (if perhaps approximately) real
applications. Intuitively speaking, Figure 1 implied independencies
conditional probability U given V , X , Z depends values V
X ; probability given U , V , X , Z depends X . Independencies
sort arise many reasons, instance causal modeling interactions
variables. refer reader Pearl (1988) details related modeling
independence assumptions using graphs.
following, make several assumptions significantly simplify presentation, sacrifice generality. First, assume variable ranges
same, constant, number values k.1 follows marginal probability distribution
variable viewed k-dimensional vector, conditional probability
matrix jX square k k matrix. common case binary random
variables (k = 2); distribution values (TRUE, FALSE) (p; 1 , p)
probability p.
next assumption tree binary, complete, node 0
2 children. tree converted form, doubling number
nodes. instance, suppose node p children c1 ; c2; c3 original tree.
create another \copy" p, p0, rearrange tree two children p
c1 p0, two children p0 c2 c3. constrain p0 always
value p simply choosing identity matrix conditional probability table
p p0 . distribution represented new tree effectively
original. Similarly, always add \dummy" leaf nodes necessary ensure
node two children. explained introduction, interested processes
certain variables' values observed, upon wish condition. final
assumption observed evidence nodes leaves tree. Again,
possible \copy" nodes add dummy nodes, restrictive.
product distribution alluded corresponds distribution variables
prior observations. practice, interested conditional distribution,
simply result conditioning observed evidence (which, earlier
assumption, corresponds seeing values leaf nodes). Thus, non-leaf node
X interested conditional marginal probability X , i.e., k-dimensional
vector:
Bel (X ) = Pr(X j evidence values):
main algorithmic problem compute Bel (X ) (non-evidence) node X
tree given current evidence. well known probability vector Bel (X )
computed linear time (in size tree) popular algorithm based
1. assumption nonrestrictive add \dummy" values variable's range,
given conditional probability 0. Nevertheless, may computational advantage
allowing different variable domain sizes. changes required permit dicult, since
complicate presentation somewhat omit them.

41

fiDelcher, Grove, Kasif & Pearl

following equation:
Bel (X ) = Pr(X j evidence) = ff (X ) (X )
ff normalizing constant, (X ) probability evidence subtree
node X given X , (X ) probability X given evidence rest
tree. interpret equation, note X = (x1; x2; : : :; xk ) (Y = y1 ; y2 ; : : :; yk )
two vectors define operation component-wise product (pairwise
dyadic product vectors):
X = (x1y1; x2y2; : : :; xkyk ):
usefulness (X ) (X ) derives fact computed recursively, follows:
1. X root node, (X ) prior probability X .
2. X leaf node, (X ) vector 1 ith position (where ith value
observed) 0 elsewhere. value X observed, (X )
vector consisting 1's.2
3. Otherwise, if, shown Figure 1, children node X Z , sibling
V parent U , have:
(X ) = (MY jX (Y )) (MZjX (Z ))


(X ) = MX jU (U ) (MV jU (V ))


presentation technique follows Pearl (1988). However, use
somewhat different notation don't describe messages sent parents successors, rather discuss direct relations among vectors terms simple
algebraic equations. take advantage algebraic properties equations
development.
easy see equations evaluated time proportional
size network. formal proof given Pearl (1988).
Theorem 1: belief distribution every variable (that is, marginal probability
distribution variable, given evidence) causal tree evaluated
O(k2N ) time N size tree. (The factor k2 due multiplication
matrix vector must performed node.)
theorem shows possible perform evidence absorption O(N ) time,
queries constant time (i.e., retrieving previously computed values lookup
table). next sections show perform queries updates
worst-case O(log N ) time. Intuitively, recompute marginal distributions
update, rather make small number changes, sucient, however,
compute value variable logarithmic delay.
2. set 1 components corresponding possible values|this especially useful
observed variable part joint-tree clique (Section 5). general, (X ) thought
likelihood vector X given observations X .

42

fiQueries & Updates Probabilistic Networks

2.1 Simple Preprocessing Approach

obtain intuition new approach begin simple observation.
Consider causal tree depth D. node X tree initially compute
(X ) vector. vectors left uncomputed. Given update node , calculate
revised (X ) vectors nodes X ancestors tree. clearly
done time proportional depth tree, i.e., O(D). rest information
tree remains unchanged. consider Query-Node operation node V
tree. obviously already accurate (V ) vector every node tree
including V . However, order compute (V ) vector need compute
(Y ) vectors nodes V tree multiply appropriate
vectors kept current. means compute accurate (V ) vector
need perform O(D) work well. Thus, approach don't perform complete
update every (X ) (X ) vector tree.
Lemma 2: Update-Node Query-Node operations causal tree performed O(k2 D) time depth tree.
implies tree balanced, operations done O(log N )
time. However, important applications trees balanced (e.g., models
temporal sequences, Delcher et al., 1993). obvious question therefore is: Given causal
tree produce equivalent balanced tree 0? answer question
appears dicult, possible use sophisticated approach produce data
structure (which causal tree) process queries updates O(log N ) time.
approach described subsequent sections.

2.2 Dynamic Data Structure Causal Trees

data structure allow ecient incremental processing probabilistic tree =
T0 sequence trees, T0; T1; T2; : : :; Ti; : : :; Tlog N . Ti+1 contracted
version Ti, whose nodes subset Ti . particular, Ti+1 contain
half many leaves predecessor.
defer details contraction process next section. However, one key
idea maintain consistency, sense Bel (X ); (X ); (X ) given
values trees X appears. choose conditional probability
matrices contracted trees (i.e., trees T0 ) ensure this.
Recall equations form

(X ) = (MY jX (Y )) (MZjX (Z ))


(X ) = MX jU (U ) (MV jU (V ))


Z children X , X right child U , V X 's sibling (Figure 1).
However, equations convenient form following notational
conventions helpful. First, let Ai (x) (resp., Bi (x)) denote conditional
probability matrix X X 's left (resp., right) child tree Ti. Note
identity children differ tree tree, X 's original children
might removed contraction process. One advantage new notation
43

fiDelcher, Grove, Kasif & Pearl

uj

Ti

, @
,
@
,
@

vj

e

xj

Rake

, @
,
@

p p zjp p

(e; x)

)

uj

Ti+1

, @
,
@
,
@

vj

p p zjp p

Figure 2: effect operation Rake (e; x). e must leaf, z may may
leaf.
explicit dependence identity children suppressed. Next, suppose X 's
parent Ti u. let Ci (x) denote either Ai (u) Bi (u), Di(x) denote either
Bi (u) Ai(u) , depending whether X right left child, respectively, U .
necessary keep careful track correspondences, simply note
equations become:3
(x) = Ai(x) (y) Bi (x) (z)
(x) = Di(x) ((u) Ci(x) (v))
next section describe preprocessing step creates dynamic data
structure.


2.3

Rake



Operation

basic operation used contract tree Rake removes leaf
parent tree. effect operation tree shown Figure 2.
define algebraic effect operation equations associated tree.
Recall want define conditional probability matrices raked tree
distribution remaining variables unchanged. achieve substituting
equations (x) (x) equations (u), (z ), (v ). following,
important note (u), (z ) (v ) unaffected rake operation.
following, let Diagff denote diagonal matrix whose diagonal entries
components vector ff. derive algebraic effect rake operation follows:
(u) = Ai (u) (v) Bi (u) (x)
= Ai (u) (v ) Bi (u) (Ai (x) (e) Bi (x) (z ))
= Ai (u) (v ) Bi (u) DiagA (x)(e) Bi (x) (z )


= Ai (u) (v ) Bi (u) DiagA (x)(e) Bi (x) (z )
= Ai+1 (u) (v ) Bi+1 (u) (z )
Ai+1 (u) = Ai (u) Bi+1 (u) = Bi (u) DiagA (x)(e) Bi (x). (Of course, case
leaf raked right child generates analogous equations.) Thus, defining






3. Throughout, assume lower precedence matrix multiplication (indicated ).

44

fiQueries & Updates Probabilistic Networks

Ai+1(u) Bi+1 (u) way, ensure values raked tree identical

corresponding values original tree. yet enough, must
check values similarly preserved. two values could possibly change
(z ) (v ), check both. former, must

(z) = Di(z) ((x) Ci(z) (e))
= Di+1 (z ) ( (u) Ci+1(z ) (v )) :
substituting (x) algebraic manipulation, see assured
Ci+1(z) = Ci(x) Di+1(z) = Di(z) DiagC (z)(e) Di(x). However recall that, definition, Ci+1 (z ) = Ai+1 (u) Ci (x) = Ai (u), Ci+1 (z ) = Ci(x) follows. Furthermore,
Di+1(z) = Bi+1(u)
= (Bi (u) DiagA (x)(e) Bi (x))
= Bi (x) DiagA (x)(e) Bi (u)
= Di(z ) DiagC (z)(e) Di (x)
















required.
(v ) necessary verify

(v) = Di(v) ((u) Ci(v) (x))
= Di+1 (v ) ( (u) Ci+1 (v ) (z )) :
substituting (x), shown true Di+1(v ) = Di (v ) = Ai (u) =
Ai+1(u) Ci+1(v) = Ci(v) DiagA (x)(e) Bi(x) = Bi+1(u). identities follow






definition, done.
Beginning given tree = T0, successive tree constructed performing
sequence rakes, rake away half remaining evidence nodes.
specifically, let Contract operation apply Rake operation every
leaf causal tree, left-to-right order, excluding leftmost rightmost
leaf. Let fTig set causal trees constructed Ti+1 causal tree generated
Ti single application Contract. following result proved using easy
inductive argument:

Theorem 3: Let T0 causal tree size N . number leaves Ti+1 equal

half leaves Ti (not counting two extreme leaves) starting T0,
O(log N ) applications Contract, produce three-node tree: root,
leftmost leaf rightmost leaf.
observations process:
1. complexity Contract linear size tree. Additionally, log N applications Contract reduce set tree equations single equation involving
root O(N ) total time.
2. total space store sets equations associated fTi g0ilog N
twice space required store equations T0.
45

fiDelcher, Grove, Kasif & Pearl

3. equation Ti+1 also store equations describe relationship
conditional probability matrices Ti+1 matrices Ti . Notice
that, even though Ti+1 produced Ti series rake operations, matrix
Ti+1 depends directly matrices present Ti. would case
attempted simultaneously rake adjacent children.
regard equations part Ti+1. So, formally speaking fTig causal trees
augmented auxiliary equations. contracted trees describes
probability distribution subset first set variables consistent
original distribution.
note ideas behind Rake operation originally developed Miller
Reif (1985) context parallel computation bottom-up arithmetic expression
trees (Kosaraju & Delcher, 1988; Karp & Ramachandran, 1990). contrast, using
context incremental update query operations sequential computing.
similar data structure independently proposed Frederickson (1993)
context dynamic arithmetic expression trees, different approach incremental
computing arithmetic trees developed Cohen Tamassia (1991).
important interesting differences arithmetic expression-tree case
own. arithmetic expressions computation done bottom-up. However, probabilistic networks -messages must passed top-down. Furthermore, arithmetic expressions
two algebraic operations allowed, typically require distributivity one
operation other, analogous property hold us. respects approach substantial generalization previous work, remaining
conceptually simple practical.

3. Example: Chain

obtain intuition algorithms, sketch generate utilize
Ti; 0 log N equations perform -value queries updates O(log N )
time N = 2L + 1 node chain length L. Consider chain length 4 Figure 3,
trees generated repeated application Contract chain.
equations correspond contracted trees figure follows (ignoring trivial equations). Recall Ai (xj ) matrix associated left edge
random variable xj Ti.

(x1)
(x2)
(x3)
(x4)

=
=
=
=

A0(x1) (e1) B0(x1) (x2)
A0(x2) (e2) B0(x2) (x3)
A0(x3) (e3) B0(x3) (x4)
A0(x4) (e4) B0(x4) (e5)

9
>
>
>
>
>
>
=
>
>
>
B0 (x1) DiagA0 (x2)(e2) B0(x2) >
>
>
B0 (x3) DiagA0 (x4)(e4) B0(x4) ;

(x1) = A1(x1) (e1) B1(x1) (x3)
(x3) = A1(x3) (e3) B1(x3) (e5)


B1(x1) =
B1(x3) =

9
>
>
>
=
>
>
>
;

46

T0

T1

fiQueries & Updates Probabilistic Networks

T0 :

xm
1
em
1
?

T1 :

xm
1
em
1
?

T2 :

xm
1

xm - xm
3 - xm
4 - e5m

- 2

em
2

em
3

?

em
4

?

?

xm - em
5

- 3

em
3
?

em

- 5

em
1
?

Figure 3: simple chain example.

(x1) = A2(x1) (e1) B2(x1) (e5)

9
>
>
=
>
>
;


T2
B2(x1) = B1 (x1) DiagA (x )(e ) B1(x3)
listed matrices because, example, constant.
consider query operation x2 . Rather performing standard computation
find level x2 \raked". Since occurred level 0, obtain
equation
(x2) = A0(x2) (e2) B0(x2) (x3)
Thus must compute (x3), find x3 \raked". happened
level 1. However, level equation associated x3 is:
(x3) = A1(x3) (e3) B1(x3) (e5)
means need follow chain. general chain N nodes
answer query node chain evaluating log N equations instead N
equations.
consider update e4 . Since e4 raked immediately, first modify
equation
B1(x3) = B0(x3) DiagA (x )(e ) B0(x4)
first level e4 occurs right-hand side. Since B1 (x3) affected
change e4 , subsequently modify equation
B2(x1) = B1(x1) DiagA (x )(e ) B1(x3)
1

47

3

3

0

4

4

1

3

3

fiDelcher, Grove, Kasif & Pearl

second level. general, clearly need update log N equations; i.e., one
per level. generalize example describe general algorithms queries
updates causal trees.

3.1 Performing Queries Updates Eciently

section shall show utilize contracted trees Ti; 0 log N
perform queries updates O(log N ) time general causal trees. shall show
logarithmic amount work necessary sucient compute enough information
data structure update query value.

3.2 Queries

compute (x) node x following. first locate ind (x),
defined highest level x appears Ti . equation (x)
form:
(x) = Ai(x) (y) Bi(x) (z)
z left right children, respectively, x Ti.
Since x appear Ti+1 , raked level equations, implies
one child (we assume z ) leaf. therefore need compute (y ),
done recursively. instead raked leaf, would compute (z ) recursively.
either case O(1) operations done addition one recursive call,
value higher level equations. Since O(log N ) levels, operations
matrix vector multiplications, procedure takes O(k2 log N ) time. function
-Query (x) given Figure 4.

3.3 Updates

describe update operations modify enough information data
structure allow us query vectors vectors eciently. importantly
reader note update operation try maintain correct
values. sucient ensure that, x, matrices Ai(x) Bi (x) (and
thus also Ci (x) Di(x)) always date.
update value evidence node, simply changing value
leaf e. level equations, value (e) appear twice:
-equation e's parent -equation e's sibling Ti. e
disappears, say level i, value incorporated one constant matrices Ai+1 (u)
Bi+1 (u) u grandparent e Ti . constant matrix turn affects
exactly one constant matrix next higher level, on. Since effect
level computed O(k3 ) time (due matrix multiplication) O(log N )
levels equations, update accomplished O(k3 log N ) time. constant k3
actually pessimistic, faster matrix multiplication algorithms exist.
update procedure given Figure 5. Update initially called Update((E ) =
e; i) E leaf, level raked, e new evidence.
operation start sequence O(log N ) calls function -Update (X = Term; i)
change propagate log N equations.
48

fiQueries & Updates Probabilistic Networks

FUNCTION -Query (x)
look equation associated (x) Tind (x).
Case 1: x leaf. equation form: (x) = e e known.
case return e.
Case 2: equation associated (x) form

(x) = Ai(x) (y) Bi (x) (z)
z leaf therefore (z ) known. case return
Ai(X ) -Query (y) Bi (X ) (z)
case leaf analogous.
Figure 4: Function compute value node.

3.4 Queries

relatively easy use similar recursive procedure perform (x) queries. Unfortunately, approach yields O(log2N )-time algorithm simply use recursion
calculate terms calculate terms using earlier procedure.
O(log N ) recursive calls calculate values, defined equation
also involves term taking O(log N ) time compute.
achieve O(log N ) time, shall instead implement (x) queries defining procedure Calc (x; i) returns triple vectors hP; L; Ri P = (x), L = (y )
R = (z ) z left right children, respectively, x Ti.
compute (x) node x following. Let = ind (x). equation
(x) Ti form:

(x) = Di(x) ((u) Ci(x) (v))
u parent x Ti v sibling. call procedure Calc (u; + 1)
return triple h (u); (v); (x)i, immediately compute (x)
using equation.
Procedure Calc (x; i) implemented following fashion.
Case 1: Ti 3-node tree x root, children x leaves, hence
values known, (x) given sequence prior probabilities x.
Case 2: x appear Ti+1 , one x's children leaf, say e raked
level i. Let z child. call Calc (u; + 1), u parent
x Ti, receive back h(u); (z); (v)i h(u); (v); (z)i according whether x
49

fiDelcher, Grove, Kasif & Pearl

FUNCTION -Update (Term = Value; i)
1. Find (at one) equation Ti , defining Ai Bi , Term
appears right-hand side; let Term0 matrix defined equation
(i.e., left-hand side).
2. Update Term0; let Value new value.
3. Call -Update (Term0 = Value; + 1) recursively.
Figure 5: update procedure.
left right child u Ti (and v u's child). compute (x)
(u) (v ), (e) (z ), return necessary triple.
Specifically,

(x) =

(

Di(x) ((u) Ai+1 (u) (v))
Di(x) ((u) Bi+1 (u) (v))

choice depends whether x right left child, respectively, u Ti.
Case 3: x appear Ti+1, call Calc (x; + 1). returns correct
value (x). child z x Ti remains child x Ti+1 , also returns
correct value (z ). z child x occur Ti+1 , must
case z raked level one z 's children, say e, leaf let
child q . situation Calc (x; + 1) returned value (q )
compute

(z) = Ai(z) (e) Bi (z) (q)
return value.
three cases, constant amount work done addition single recursive
call uses equations higher level. Since O(log N ) levels equations,
requiring matrix vector multiplication, total work done O(k2 log N ).

4. Extended Example
section illustrate application algorithms specific example. Consider
sequence contracted trees shown Figure 6. Corresponding trees
50

fiQueries & Updates Probabilistic Networks

xl
1

T0 :







xl

#
#
#
#

2

xl
4

xl
6






e1

xl
8






e2

e4



e8











xl
2




xl
5

xl
7

xl
3






Z
Z

c
c
c
c

, @
@
@

,
,

xl
1

T1 :

Z
Z
Z

e6






xl
4



e9






xl
6

e7






e5

e1

e3

T2:

xl
1






e1

e3

T3: xl
1






xl
4

e5

e9

e5

Figure 6: Example tree contraction.

51






e1

e9

e7

e9

fiDelcher, Grove, Kasif & Pearl

equations following:
T0 :
(x1) = A0(x1 ) (x2 ) B0 (x1 ) (x3 )
..
.





(x2) = D0 (x2) ((x1) C0(x2) (x3 ))
..
.





T1 :
(x1) = A1(x1 ) (x2 ) B1 (x1 ) (e9 )
..
.











(x2) = D1 (x2) ((x1) C1(x2) (e9 ))
..
.





T2 :
(x1) = A2(x1 ) (x4 ) B2 (x1 ) (e9 )
..
.







(x4) = D2 (x4) ((x1) C2(x4) (e9 ))
..
.









T3 :
(x1) = A3(x1 ) (e1 ) B3 (x1) (e9 )






consider, instance, effect update e2 . Since raked immediately,
new value (e2) incorporated in:
B1 (x6 ) = B0 (x6 ) DiagA0 (x8 ) (e2) B0 (x8 )
subsequent Rake operations know A2(x4 ) depends B1 (x6), A3 (x1)
depends A2 (x4), must also update values follows:
A2 (x4 ) = A1 (x4) DiagB1 (x6 ) (e3 ) A1 (x6)
A3 (x1 ) = A2 (x1) DiagB2 (x4 ) (e5 ) A2 (x4)


















Finally, consider query x7 . Since x7 raked together e5 T0 , follow
steps outlined generate following calls: Calc (x7; 0), Calc (x4; 1),
Calc (x4; 2), Calc (x1; 3). provides us (x7). case, (x7)
particularly easy compute since x7 's children leaf nodes. simply
compute (x7) (x7) normalize, giving us conditional marginal distribution
Bel (x7) required.

5. Join Trees

Perhaps best-known technique computing arbitrary (i.e., singly-connected)
Bayesian networks uses idea join trees (junction trees) (Lauritzen & Spiegelhalter,
1988). many ways join tree thought causal tree, albeit one somewhat
special structure. Thus algorithm previous section applied. However,
structure join tree permits optimization, describe section.
becomes especially relevant next section, use join-tree technique
show O(log N ) updates queries done arbitrary polytrees. review
join-trees utility extremely brief quite incomplete; clear expositions
see, instance, Spiegelhalter et al. (1993) Pearl (1988).
Given Bayesian network, first step towards constructing join-tree moralize
network: insert edges every pair parents common node, treat
52

fiQueries & Updates Probabilistic Networks

edges graph undirected (Spiegelhalter et al., 1993). resulting undirected
graph called moral graph. interested undirected graphs chordal :
every cycle length 4 contain chord (i.e., edge two nodes
non-adjacent cycle). moral graph chordal, necessary add
edges make so; various techniques triangulation stage known (for instance,
see Spiegelhalter et al., 1993).
p probability distribution represented Bayesian network G = (V; E ),
= (V; F ) result moralizing triangulating G, then:
1. jV j cliques,4 say C1; : : :; CjV j.
2. cliques ordered > 1 j (i) <

Ci \ Cj(i) = Ci \ (C1 [ C2 [ : : : [ Ci,1:)
tree formed treating cliques nodes, connecting node Ci
\parent" Cj (i), called join tree.
3. p =





p(CijCj(i))

4. p(CijCj (i)) = p(CijCj (i) \ Ci )
2 3, see direct edges away \parent" cliques,
resulting directed tree fact Bayesian causal tree represent original
distribution p. true matter form original graph. course,
price cliques may large, domain size (the number possible values
clique node) exponential size. technique guaranteed
ecient.
use Rake technique Section 2 directed join tree without
modification. However, property 4 shows conditional probability matrices
join tree special structure. use gain eciency.
following, let k domain size variables G usual. Let n maximum
size cliques join tree; without loss generality assume cliques
size (because add \dummy" variables). Thus domain size
clique K = kn . Finally, let c maximum intersection size clique parent
(i.e., jCj (i) \ Cij) L = kc .
standard algorithm, would represent p(CijCj (i)) K K matrix, MC jC .
However, p(Ci jCj (i) \ Ci) represented smaller L K matrix, MC jC \C .
property 4 above, MC jC identical MC jC \C , except many rows repeated.
Thus K L matrix J








j (i)



j (i)





j (i)

MC jC = J MC jC


j (i)

j (i)

j (i)

\Ci :

(J actually simple matrix whose entries 0 1, exactly one 1 per row; however
use fact.)
4. clique maximal completely-connected subgraph.

53

fiDelcher, Grove, Kasif & Pearl

claim that, case join trees, following true. First, matrices

Ai Bi used Rake algorithm stored factored form, product
two matrices dimension K L L K respectively. So, instance, factor Ai
Ali Ari . never need explicitly compute, store, full matrices.
seen, claim true = 0 matrices factor way. proof
> 1 uses inductive argument, illustrate below. second claim that,

matrices stored factored form, matrix multiplications used
Rake algorithm one following types: 1) L K matrix times K L matrix,
2) L K matrix times K K diagonal matrix, 3) L L matrix times L K
matrix, 4) L K matrix times vector.
prove claims consider, instance, equation defining Bi+1 terms lowerlevel matrices. Section 2, Bi+1 (u) = Bi (u) DiagA (x)(e) Bi (x): But, assumption,
is:
(Bil (u) Bir (u)) Diag(A (x)A (x))(e) (Bil (x) Bil (x));
which, using associativity, clearly equivalent
h

Bil (u) ((Bir (u) DiagA (x)(A (x)(e))) Bil(x)) Bil (x) :
However, every multiplication expression one forms stated earlier. Identifying
Bil+1 (u) Bil(u) Bir+1 (u) bracketed part expression proves case,
course case rake left child (so Ai+1 (u) updated) analogous.
Thus, even using straightforward technique matrix multiplication, cost
updating Bi+1 O(KL2) = O(kn+2c ). contrasts O(K 3) factor
matrices, may represent worthwhile speedup c small. Note overall time
update using scheme O(kn+2c log N ). Queries, involve matrix
vector multiplication, require O(kn+c log N ) time.
many join trees difference N log N unimportant,
clique domain size K often enormous dominates complexity. Indeed, K L
may large cannot represent required matrices explicitly. course,
cases technique little offer. cases benefits
worthwhile. important general class so, immediate
reason presenting technique join trees, case polytrees.


l


r


l


r


6. Polytrees

polytree singly connected Bayesian network; drop assumption Section 2
node one parent. Polytrees offer much exibility causal
trees, yet well-known process update query O(N ) time,
causal trees. reason polytrees extremely popular class networks.
suspect possible present O(log N ) algorithm updates queries
polytrees, direct extension ideas Section 2. Instead propose different
technique, involves converting polytree join tree using ideas
preceding section. basis simple observation join tree
polytree already chordal. Thus (as show detail below) little lost considering
join tree instead original polytree. specific property polytrees
require following. omit proof well-known proposition.
54

fiQueries & Updates Probabilistic Networks

Proposition 4: moral graph polytree P = (V; E ) chordal,
set maximal cliques ffv g [ parents (v ) : v 2 V g.
Let p maximum number parents node. proposition, every
maximal clique join tree p +1 variables, domain size node
join tree K = kp+1 . may large, recall conditional probability
matrix original polytree, variable p parents, K entries anyway since
must give conditional distribution every combination node's parents. Thus K
really measure size polytree itself.
follows proposition perform query update
polytrees time O(K 3 log N ), simply using algorithm Section 2 directed
join tree. But, noted Section 5, better. Recall savings depend
c, maximum size intersection node parent join tree.
However, join tree formed polytree, two cliques share
single node. follows immediately Proposition 4, two cliques
one node common must either two nodes share one parent,
else node one parents share yet another parent. Neither
consistent network polytree. Thus complexity bounds Section 5,
put c = 1. follows process updates O(Kk2c log N ) = O(kp+3 log N )
time queries O(kp+2 log N ).

7. Application: Towards Automated Site-Specific Muta-Genesis

experiment commonly performed biology laboratories procedure
particular site protein changed (i.e., single amino-acid mutated)
tested see whether protein settles different conformation. many cases,
overwhelming probability protein change secondary structure outside
mutated region. process often called muta-genesis. Delcher et al. (1993) developed
probabilistic model protein structure basically long chain. length
chain varies 300{500 nodes. nodes network either protein-structure
nodes (PS-nodes) evidence nodes (E-nodes). PS-node network discrete
random variable Xi assumes values corresponding descriptors secondary sequence
structure: helix, sheet coil. PS-node model associates evidence node
corresponds occurrence particular subsequence amino acids particular
location protein.
model, protein-structure nodes finite strings alphabet fh; e ; c g.
example string hhhhhh string six residues ff-helical conformation,
eecc string two residues fi -sheet conformation followed two residues folded
coil. Evidence nodes nodes contain information particular region
protein. Thus, main idea represent physical statistical rules form
probabilistic network.
first set experiments converged following model that, clearly
biologically naive, seems match prediction accuracy many existing approaches
neural networks. network looks like set PS-nodes connected chain.
node connect single evidence node. experiments PS-nodes strings
length two three alphabet fh; e ; c g evidence nodes strings
55

fiDelcher, Grove, Kasif & Pearl


cc

?
GS


ch
?
SA


hh
?







Figure 7: Example causal tree model using pairs, showing protein segment GSAT
corresponding secondary structure cchh.
length set amino acids. following example clarifies representation.
Assume string amino acids GSAT. model string network comprised
three evidence nodes GS, SA, three PS-nodes. network shown Figure 7.
correct prediction assign values cc, ch, hh PS-nodes shown
figure.
probabilistic model, test robustness protein
whether small changes protein affect structure certain critical sites
protein. experiments, probabilistic network performs \simulated evolution"
protein, namely simulator repeatedly mutates region chain tests
whether designated sites protein coiled helix predicted
remain conformation. main goal experiment test stable bonds far
away mutated location affected. previous results (Delcher et al., 1993)
support current thesis biology community, namely local distant changes
rarely affect structure.
algorithms presented previous sections paper perfectly suited
type application predicted generate factor 10 improvement
eciency current brute-force implementation presented Delcher et al. (1993)
change propagated throughout network.

8. Summary
paper proposed several new algorithms yield substantial improvement
performance probabilistic networks form causal trees. updating procedures
absorb sucient information tree query procedure compute
correct probability distribution node given current evidence. addition,
procedures execute time O(log N ), N size network. algorithms
expected generate orders-of-magnitude speed-ups causal trees contain long
paths (not necessarily chains) matrices conditional probabilities
relatively small. currently experimenting approach singly connected
networks (polytrees). likely dicult generalize techniques general
networks. Since known general problem inference probabilistic networks
NP -hard (Cooper, 1990), obviously possible obtain polynomial-time incremental
56

fiQueries & Updates Probabilistic Networks

solutions type discussed paper general probabilistic networks.
natural open question extending approach developed paper dynamic
operations probabilistic networks addition deletion nodes modifying
matrices conditional probabilities (as result learning).
would also interesting investigate practical logarithmic-time parallel algorithms probabilistic networks realistic parallel models computation. One
main goals massively parallel AI research produce networks perform real-time
inference large knowledge-bases eciently (i.e., time proportional depth
network rather size network) exploiting massive parallelism. Jerry
Feldman pioneered philosophy context neural architectures (see Stanfill
Waltz, 1986, Shastri, 1993, Feldman Ballard, 1982). achieve type performance neural network framework, typically postulate parallel hardware
associates processor node network typically ignores communication requirements. careful mapping parallel architectures one indeed achieve ecient
parallel execution specific classes inference operations (see Mani Shastri, 1994,
Kasif, 1990, Kasif Delcher, 1992). techniques outlined paper presented
alternative architecture supports fast (sub-linear time) response capability
sequential machines based preprocessing. However, approach obviously limited
applications number updates queries time constant. One would
naturally hope develop parallel computers support real-time probabilistic reasoning
general networks.

Acknowledgements
Simon Kasif's research Johns Hopkins University sponsored part National
Science foundation Grants No. IRI-9116843, IRI-9223591 IRI-9220960.

References

Berger, T., & Ye, Z. (1990). Entropic aspects random fields trees. IEEE Trans.
Information Theory, 36 (5), 1006{1018.
Chelberg, D. M. (1990). Uncertainty interpretation range imagery. Proc. Intern.
Conference Computer Vision, pp. 654{657.
Cohen, R. F., & Tamassia, R. (1991). Dynamic trees applications. Proceedings
2nd ACM-SIAM Symposium Discrete Algorithms, pp. 52{61.
Cooper, G. (1990). computational complexity probabilistic inference using bayes
belief networks. Artificial Intelligence, 42, 393{405.
Delcher, A., & Kasif, S. (1992). Improved decision making game trees: Recovering
pathology. Proceedings 1992 National Conference Artificial Intelligence.
Delcher, A. L., Kasif, S., Goldberg, H. R., & Hsu, B. (1993). Probabilistic prediction protein secondary structure using causal networks. Proceedings 1993 International
Conference Intelligent Systems Computational Biology, pp. 316{321.
57

fiDelcher, Grove, Kasif & Pearl

Duda, R., & Hart, P. (1973). Pattern Classification Scene Analysis. Wiley, New York.
Feldman, J. A., & Ballard, D. (1982). Connectionist models properties. Cognitive
Science, 6, 205{254.
Frederickson, G. N. (1993). data structure dynamically maintaining rooted trees.
Proc. 4th Annual Symposium Discrete Algorithms, pp. 175{184.
Hel-Or, Y., & Werman, M. (1992). Absolute orientation uncertain data: unified
approach. Proc. Intern. Conference Computer Vision Pattern Recognition,
pp. 77{82.
Karp, R. M., & Ramachandran, V. (1990). Parallel algorithms shared-memory machines.
Van Leeuwen, J. (Ed.), Handbook Theoretical Computer Science, pp. 869{941.
North-Holland.
Kasif, S. (1990). parallel complexity discrete relaxation constraint networks.
Artificial Intelligence, 45, 275{286.
Kasif, S., & Delcher, A. (1994). Analysis local consistency parallel constraint networks.
Artificial Intelligence, 69.
Kosaraju, S. R., & Delcher, A. L. (1988). Optimal parallel evaluation tree-structured
computations raking. Reif, J. H. (Ed.), VLSI Algorithms Architectures:
Proceedings 1988 Aegean Workshop Computing, pp. 101{110. Springer Verlag.
LNCS 319.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computations probabilities graphical
structures applications expert systems. J. Royal Statistical Soc. Ser. B,
50, 157{224.
Mani, D., & Shastri, L. (1994). Massively parallel reasoning large knowledge
bases. Tech. rep., Intern. Computer Science Institute.
Miller, G. L., & Reif, J. (1985). Parallel tree contraction application. Proceedings
26th IEEE Symposium Foundations Computer Science, pp. 478{489.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.
Peot, M. A., & Shachter, R. D. (1991). Fusion propagation multiple observations
belief networks. Artificial Intelligence, 48, 299{318.
Rachlin, J., Kasif, S., Salzberg, S., & Aha, D. (1994). Towards better understanding
memory-based bayesian classifiers. Proceedings Eleventh International
Conference Machine Learning, pp. 242{250 New Brunswick, NJ.
Shastri, L. (1993). computational model tractable reasoning: Taking inspiration
cognition. Proceeding 1993 Intern. Joint Conference Artificial Intelligence.
AAAI.
58

fiQueries & Updates Probabilistic Networks

Spiegelhalter, D., Dawid, A., Lauritzen, S., & Cowell, R. (1993). Bayesian analysis expert
systems. Statistical Science, 8 (3), 219{283.
Stanfill, C., & Waltz, D. (1986). Toward memory-based reasoning. Communications
ACM, 29 (12), 1213{1228.
Wilsky, A. (1993). Multiscale representation markov random fields. IEEE Trans. Signal
Processing, 41, 3377{3395.

59

fiJournal Artificial Intelligence Research 4 (1996) 91-128

Submitted 6/95; published 3/96

Quantum Computing Phase Transitions
Combinatorial Search
Tad Hogg

Xerox Palo Alto Research Center
3333 Coyote Hill Road Palo Alto, CA 94304 USA

hogg@parc.xerox.com

Abstract

introduce algorithm combinatorial search quantum computers capable significantly concentrating amplitude solutions NP search problems,
average. done exploiting aspects problem structure used
classical backtrack methods avoid unproductive search choices. quantum algorithm
much likely find solutions simple direct use quantum parallelism. Furthermore, empirical evaluation small problems shows quantum algorithm displays
phase transition behavior, location, seen many previously
studied classical search methods. Specifically, dicult problem instances concentrated
near abrupt change underconstrained overconstrained problems.

1. Introduction

Computation ultimately physical process (Landauer, 1991). is, practice
range physically realizable devices determines computable resources,
computer time, required solve given problem. Computing machines exploit
variety physical processes structures provide distinct trade-offs resource
requirements. example development parallel computers trade-off
overall computation time number processors employed. Effective use
trade-off require algorithms would inecient implemented serially.
Another example given hypothetical quantum computers (DiVincenzo, 1995).
offer potential exploiting quantum parallelism trade computation time
use coherent interference among many different computational paths. However,
restrictions physically realizable operations make trade-off dicult exploit
search problems, resulting algorithms essentially equivalent inecient method
generate-and-test. Fortunately, recent work factoring (Shor, 1994) shows better
algorithms possible. continue line work introducing new quantum algorithm particularly dicult combinatorial search problems.
algorithm represents substantial improvement quantum computers, particularly
inecient classical search method, memory time requirements.
evaluating algorithms, computational complexity theory usually focuses
scaling behavior worst case. particular theoretical concern whether search
cost grows exponentially polynomially. However, many practical situations, typical
average behavior interest. especially true many instances
search problems much easier solve suggested worst case analyses.
fact, recent studies revealed important regularity class search problems.
Specifically, wide variety search methods, hard instances rare
c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiHogg

also concentrated near abrupt transitions problem behavior analogous physical phase
transitions (Hogg, Huberman, & Williams, 1996). exhibit concentration hard
instances search algorithm must exploit problem constraints prune unproductive
search choices. Unfortunately, easy within range allowable quantum
computational operations. thus interest see results generalize quantum
search methods well.
paper, new algorithm evaluated empirically determine average behavior. algorithm also shown exhibit phase transition, indicating indeed
managing to, effect, prune unproductive search. leaves future work analysis
worst case performance.
paper organized follows. First discuss combinatorial search problems
phase transitions hard problem instances concentrated. Second,
brief summary quantum computing, new quantum search algorithm motivated
described. fact, number natural variants general algorithm.
Two evaluated empirically exhibit generality phase transition
performance. Finally, important caveats implementation quantum
computers open issues presented.

2. Combinatorial Search

Combinatorial search among hardest common computational problems: solution
time grow exponentially size problem (Garey & Johnson, 1979). Examples arise scheduling, planning, circuit layout machine vision, name areas.
Many examples viewed constraint satisfaction problems (CSPs) (Mackworth, 1992). given set n variables assigned b possible
values. problem find assignment variable together satisfy
specified constraints. instance, consider small scheduling problem selecting one
two periods teach two classes taught person.
regard class variable time slot value, i.e., n = b = 2.
constraints two classes assigned time.
Fundamentally, combinatorial search problem consists finding combinations
discrete set items satisfy specified requirements. number possible combinations consider grows rapidly (e.g., exponentially factorially) number
items, leading potentially lengthy solution times severely limiting feasible size
problems. example, number possible assignments constraint problem
bn , grows exponentially problem size (given number variables
n).
exponentially large number possibilities appears time required
solve problems must grow exponentially, worst case. However many
problems easy verify solution fact correct. problems form wellstudied class NP problems: informally say hard solve easy check.
One well-studied instance graph coloring, variables represent nodes graph,
values colors nodes constraints pair nodes linked
edge graph must different colors. Another example propositional satisfiability
(SAT), variables take logical values true false, assignment must
92

fiQuantum Computing Phase Transitions Combinatorial Search

satisfy specified propositional formula involving variables. examples
instances particularly dicult NP problems known class NP-complete search
problems (Garey & Johnson, 1979).

2.1 Phase Transitions

Much theoretical work NP search problems examines worst case behavior.
Although search problems hard, worst case, great deal
individual variation problems among different search methods. number
recent studies NP search problems focused regularities typical behavior (Cheeseman, Kanefsky, & Taylor, 1991; Mitchell, Selman, & Levesque, 1992; Williams &
Hogg, 1994; Hogg et al., 1996; Hogg, 1994). work identified number common
behaviors. Specifically, large problems, parameters characterizing structure
determine relative diculty wide variety common search methods, average.
Moreover, changes parameters give rise transitions, becoming abrupt
larger problems, analogous phase transitions physical systems. case,
transition underconstrained overconstrained problems, hardest cases
concentrated transition region. One powerful result work concentration hard cases occurs parameter values wide range search
methods. is, behavior property problems rather details
search algorithm.
understood viewing search making series choices solution
found. overall search usually relatively easy (i.e., require steps) either
many choices leading solutions else choices lead solutions
recognized quickly such, unproductive search avoided. Whether
condition holds turn determined tightly constrained problem is.
constraints almost choices good ones, leading quickly solution.
many constraints, hand, good choices bad ones
recognized quickly violating constraints much time wasted
considering them. two cases hard problems: enough constraints
good choices rare enough bad choices usually recognized
lot additional search.
detailed analysis suggests series transitions (Hogg & Williams, 1994).
constraints, average search cost scales polynomially. constraints
added, transition exponential scaling. rate growth exponential
increases transition region described reached. Beyond point,
concentration hard problems, growth rate decreases. Eventually, highly
constrained problems, search cost grows polynomially size.

2.2 Combinatorial Search Space

general view combinatorial search problem consists N items1
requirement find solution, i.e., set L<N items satisfies specified conditions
constraints. conditions turn described collection nogoods, i.e., sets
1. CSPs, items possible variable-value pairs.

93

fiHogg

{1,2,3,4}

{1,2}

{1,2,3}

{1,3,4}

{2,3,4}

{1,2,4}

{2,3}

{1,3}

{3,4}

{2,4}

{1}

{2}

{3}

{4}

{1,4}

{}

Figure 1: Structure set lattice problem four items. subsets f1; 2; 3; 4g
grouped levels size lines drawn set immediate
supersets subsets. bottom lattice, level 0, represents single set
size zero, four points level 1 represent four singleton subsets, etc.
items whose combination inconsistent given conditions. context
define good set items consistent constraints problem.
also say set complete L items, smaller sets partial incomplete.
Thus solution complete good set. addition, partial solution incomplete good
set.
key property makes set representation conceptually useful set
nogood, supersets. sets, grouped size set linked
immediate supersets subsets, form lattice structure. structure N = 4
shown Fig. 1. say
N
Ni =
(1)

sets size level lattice. described below, various paths
lattice levels near bottom solutions, level L, used create
quantum interference basis search algorithm.
example, consider problem N = 4 L = 2, suppose constraints
eliminate items 1 3. sets fg, f2g, f4g partial goods, f1g
f3g partial nogoods. Among 6 complete sets, f2,4g good others
supersets f1g f3g hence nogood.
94

fiQuantum Computing Phase Transitions Combinatorial Search

search problems studied here, nogoods directly specified problem
constraints small sets items, e.g., size two three. hand,
number items size solutions grow problem size. gives
number small nogoods, i.e., near bottom lattice. Examples problems include binary constraint satisfaction, graph coloring propositional satisfiability
mentioned above.
CSPs, items possible variable-value pairs problem. Thus
CSP n variables b values N = nb items2 . solution consists
assignment variable satisfies whatever constraints given problem.
Thus solution consists set L = n items. terms general framework
combinatorial search constraint satisfaction problems also contain number
problem-independent necessary nogoods, namely
b corresponding giving
variable two different values. n 2 necessary nogoods. nontrivial
search must b2, restrict attention case LN=2.
requirement important allowing construction quantum search method described below.
Another example given simple CSP consisting n = 2 variables (v1 v2)
take one b = 2 values (1 2) single constraint
two variables take distinct values, i.e., v1 6= v2 . Hence N = nb = 4 variablevalue pairs v1 = 1; v1 = 2; v2 = 1; v2 = 2 denote items 1; 2; 3; 4 respectively.
corresponding lattice given Fig. 1. nogoods problem?
First due explicit constraint two variables distinct
values: fv1 = 1; v2 = 1g fv1 = 2; v2 = 2g f1; 3g f2; 4g. addition,
necessary nogoods implied requirement variable takes unique value
set giving multiple assignments variable necessarily nogood, namely
fv1 = 1; v1 = 2g fv2 = 1; v2 = 2g f1; 2g f3; 4g. Referring Fig. 1, see
four nogoods force sets size 3 4 nogood too. However, sets size zero
one goods remaining two sets size two: f2; 3g f1; 4g corresponding
fv1 = 2; v2 = 1g fv1 = 1; v2 = 2g solutions problem.
Search methods use various strategies examining sets lattice. instance,
methods simulated annealing (Kirkpatrick, Gelatt, & Vecchi, 1983), heuristic repair (Minton, Johnston, Philips, & Laird, 1992) GSAT (Selman, Levesque, & Mitchell,
1992) move among complete sets, attempting find solution series small changes
sets. Generally search techniques continue indefinitely problem
solution thus never show problem insoluble. methods called
incomplete. methods, search repeated, different initial conditions
making different random choices, either solution found specified limit
number trials reached. latter case, one cannot distinguish problem
solution series unlucky choices soluble problem. search
techniques attempt build solutions starting smaller sets, often process extending consistent set either solution found consistent extensions
possible. latter case search backtracks previous decision point tries
2. lattice sets also represent problems variable different number assigned
values.

95

fiHogg

another possible extension choices remain. recording pending choices
decision point, backtrack methods determine problem insoluble, i.e.,
complete systematic search methods.
description highlights two distinct aspects search procedure: general
method moving among sets, independent particular problem, testing procedure checks sets consistency particular problem's requirements. Often,
heuristics used make search decisions depend problem structure hoping
identify changes likely lead solution avoid unproductive regions
search space. However, conceptually aspects separated, case
quantum search algorithm presented below.

3. Quantum Search Methods

section brie describes capabilities quantum computers, straightforward attempts exploit capabilities search particularly effective,
motivates describes new search algorithm.

3.1 Overview Quantum Computers

basic distinguishing feature quantum computer (Benioff, 1982; Bernstein & Vazirani, 1993; Deutsch, 1985, 1989; Ekert & Jozsa, 1995; Feynman, 1986; Jozsa, 1992; Kimber,
1992; Lloyd, 1993; Shor, 1994; Svozil, 1995) ability operate simultaneously
collection classical states, thus potentially performing many operations time classical computer would one. Alternatively, quantum parallelism viewed
large parallel computer requiring hardware needed single processor.
hand, range allowable operations rather limited.
describe concretely, adopt conventional ket notation quantum
mechanics (Dirac, 1958, section 6) denote various states3. is, use jffi denote
state computer described ff. low level description, state classical
computer described values bits. instance n bits,
N = 2n possible states machine, associated numbers
s1 = 0; : : :; sN = 2n , 1. say computer state jsi values
bits correspond number , 1. commonly, computer described terms
higher level constructs formed groups bits, integers, character strings, sets
addresses variables program. example, state could arise
search jfv1 = 1; v2 = 1g; soln = Falsei corresponding set assignments variables
CSP value false program variable soln, e.g., used represent whether
solution found. higher level descriptions, often aspects
computer's state, e.g., stack pointers values various iteration counters,
explicitly mentioned.
states presented far, bit higher-level construct definite value,
apply classical quantum computers. However, quantum computers
far richer set possible states. Specifically, js1 i; : : :; jsN possible states
3. ket notation conceptually similar use boldface denote vectors distinguish
scalars.

96

fiQuantum Computing Phase Transitions Combinatorial Search

classical computer, possible states corresponding quantum
linear
P js computer
superpositions states, i.e., states form jsi =



complex
number called amplitude associated state jsi i. physical interpretation
amplitudes comes measurement process. measurement made
quantum computer state jsi, e.g., determine result computation
represented particular configuration bits register, one possible classical
states obtained. Specifically, classical state jsi obtained probability j j2.
Furthermore, measurement process changes state computer exactly match
result. is, measurement said collapse original superposition
new superposition consisting single classical state (i.e., amplitude returned
state 1 amplitudes zero). means repeated measurements always
return result.
important consequence interpretation results fact probabilities
must sum one. Thus amplitudes superposition states must satisfy
normalization condition
X 2
j ij = 1
(2)


Another consequence full state quantum computer, i.e., superposition,
observable quantity. Nevertheless, changing amplitude associated
different classical states, operations superposition affect probability
various states observed. possibility crucial exploiting quantum
computation, makes potentially powerful probabilistic classical machines,
choices program made randomly.
superpositions also viewed vectors space whose basis individual classical states jsi component vector along ith basis element
space. state vector also specified components ( 1; : : :; N )

basis understood context. inner product two vectors
= PNi=1 denotes complex conjugate . matrix notation,
also written treated column vector row vector
given transpose entries changed complex conjugate values.
vectors, normalization condition amounts requiring = 1.
complete overview quantum computers, remains describe superpositions used within program. addition measurement process described
above, two types operations performed superposition states.
first type run classical programs machine, second allows creating manipulating amplitudes superposition. cases, key
property superposition linearity: operation superposition states gives
superposition operation acting states individually. described
below, property, combined normalization condition, greatly limits range
physically realizable operations.
first case, quantum computer perform classical program provided
reversible, i.e., final state contains enough information recover initial state. One
way achieve retain initial input part output. illustrate
linearity operations, consider reversible classical computation states, e.g.,
f (si ) produces new state given input one. applied superposition
97

fiHogg

P

states, result f (jsi) =
jf (si )i. reversibility required? Suppose
procedure f reversible, i.e., maps least two distinct states result.
example, suppose f (s1 ) = f (s2 ) = s3 . superposition jsi = p12 (js1 + js2 i)
p
linearity requires f (jsi) = p12 (jf (s1 )i + jf (s2 )i) giving 2js3 i, superposition
violates normalization condition. Thus irreversible classical operation physically realizable superposition, i.e., cannot used quantum parallelism.
contrast use computations individual states, second type operation
modifies amplitude various states within superposition. is, starting
jsi =
P
jk sk operation, denoted U, creates new superposition js0i = U jsi = P j0 jsj i.
operations linear respect superpositions,
new amplitudes
P
expressed terms original ones j0 = k Ujk k , matrix notation
0 = U . is, linearity means operation changing amplitudes
represented matrix. satisfy normalization condition, Eq. 2, matrix must
( 0)y 0 = 1. terms matrix U condition becomes4


1 = (U )y(U ) = U yU

(3)

must hold initial state vector = 1. see implies
matrix U yU , suppose = e^j = (: : :; 0; 1; 0; : : :) jth unit vector,
corresponding superposition jsj amplitudes zero except j = 1.
case yA = Ajj must equal one Eq. 3. is, diagonal elements
U yU must equal one. = p12 (^ej + e^k ) j 6= k,
yA = 1 (^ej + ^ek )A(^ej + ^ek )
2

(4)

= 12 [Ajj + Akk + Ajk + Akj ]

must equal one Eq. 3, already know diagonal terms equal one. Thus
conclude Ajk = ,Akj . similar argument using = p12 (^ej + ie^k ), superposition
imaginary value second amplitude, gives Ajk = Akj . Together conditions
mean identity matrix, U yU = , i.e., matrix U must unitary
operate superpositions. Moreover, condition sucient make initial state
satisfy Eq. 3. shows restriction linear unitary operations arises directly
linearity quantum mechanics Eq. 2, normalization condition probabilities.
class unitary matrices includes permutations, rotations arbitrary phase changes
(i.e., diagonal matrices element diagonal complex number
magnitude equal one).
Reversible classical programs, unitary operations superpositions measurement process basic ingredients used construct program quantum
computer. used search algorithm described below, program consists
first preparing initial superposition states, operating states series
unitary matrices conjunction classical program evaluate consistency
4.

Uy

transpose U elements changed complex conjugates.

98

,U


jk = (Ukj ) .

fiQuantum Computing Phase Transitions Combinatorial Search

various states respect search requirements, making measurement
obtain definite final answer. amplitudes superposition measurement made determine probability obtaining solution. overall structure
probabilistic Monte Carlo computation (Motwani & Raghavan, 1995)
trial probability get solution, guarantee. means search
method incomplete: find solution one exists never guarantee solution
doesn't exist.
alternate conceptual view quantum programs provided path integral approach quantum mechanics (Feynman, 1985). view, final amplitude
given state obtained weighted sum possible paths produce state.
way, various possibilities involved computation interfere
other, either constructively destructively. differs classical combination
probabilities different ways reach outcome (e.g., used probabilistic
algorithms): probabilities simply added, giving possibility interference. Interference also seen classical waves, sound ripples surface water.
systems lack capability quantum parallelism. various formulations
quantum mechanics, involving operators, matrices sums paths equivalent
suggest different intuitions constructing possible quantum algorithms.

3.2 Example: One-Bit Computer

simple example ideas given single bit. case two possible
classical states j0i j1i corresponding values 0 1, respectively, bit.
defines two dimensional vector space superpositions quantum bit.
number proposals implementing quantum bits, i.e., devices whose quantum mechanical
properties controlled produce desired superpositions two classical values. One
example (DiVincenzo, 1995; Lloyd, 1995) atom whose ground state corresponds
value 0 excited state value 1. use lasers appropriate frequencies
switch atom two states create superpositions two classical
states. ability manipulate quantum superpositions demonstrated small
cases (Zhu, Kleiman, Li, Lu, Trentelman, & Gordon, 1995). Another possibility
use atomically precise manipulations (DiVincenzo, 1995) using scanning tunneling
atomic force microscope. possibility precise manipulation chemical reactions
also demonstrated (Muller, Klein, Lee, Clarke, McEuen, & Schultz, 1995).
also number proposals investigation (Barenco, Deutsch, & Ekert, 1995;
Sleator & Weinfurter, 1995; Cirac & Zoller, 1995), including possibility multiple
simultaneous quantum operations (Margolus, 1990).
simple computation quantum bit logical operation, i.e., NOT(j0i) =
j1i NOT(j1i) = j0i. operator simply exchanges state vector's components:


0
1

NOT( 0j0i + 1j1i) = 0j1i + 1j0i

99

1
0

(5)

fiHogg

0 1
operation also represented multiplication permutation matrix 1 0 .

Another operator given rotation matrix
cos , sin
U () = sin cos
(6)
used create superpositions single classical states, e.g.,
1
1
1
1
U 4 0 U 4 j0i = p (j0i + j1i) p 1
(7)
2
2
rotation matrix also used illustrate interference, important way
quantum computers differ probabilistic classical algorithms. First, consider
classical algorithm two methods generating random bits, R0 (producing \0"
probability 3=4) R1 (producing \0" probability 1=4). Suppose \0" represents
failure (e.g., probabilistic search find solution) \1" represents
success. Finally, let classical algorithm consist selecting one methods use,
probability p pick R0 . overall probability obtain \0" final
result 34 p + 14 (1 , p)
(8)
Pclassical = 41 + p2
best done choose p = 0, giving probability 1=4 failure.
quantum analog simple calculation obtained rotation = 3 .
Starting individual classical states gives superpositions

1 1 p3 !
U 3 0 =2 1
0 1 ,1
U
= p

(9)

3

1
2 3
correspond generators R0 R1 respectively, respective
probabilities 3=4 1=4 produce \0" measured. Starting instead

superposition two classical states, cos
sin , corresponds step classical
algorithm generator R0, isselected
probability p = cos2 . resulting state

cos

applying rotation, U 3 sin , probability

p

2
Pquantum = 41 + cos2 , 43 sin (2)
p
= Pclassical , 43 sin (2)

(10)

produce \0" value. case minimum value probability obtain
\0" 1=4 fact made equal 0 choice = 3 . case
amplitudes two original states exactly cancel other, example destructive
interference.
final example, illustrating limits operations superpositions, consider
simple classical program sets bit value one. is, SET(j0i) = j1i
100

fiQuantum Computing Phase Transitions Combinatorial Search

SET(j1i) = j1i. operation isnot reversible: knowing result determine
original input. linearity, SET p12 (j0i + j1i) = p12 (SET(j0i) + SET(j1i)), turn
p
p12 2j1i = 2j1i. state violates normalization condition. Thus see
classical operation physically realizable quantum computer. Similarly, another
common classical operation, making copy bit, also ruled (Svozil, 1995), forming
basis quantum cryptography (Bennett, 1992).

3.3 Approaches Search

device consisting n quantum bits allows operations superpositions 2n classical
states. ability operate simultaneously exponentially large number states
linear number bits basis quantum parallelism. particular, repeating operation Eq. 7 n times, different bit, gives superposition equal
amplitudes 2n states.
first sight quantum computers would seem ideal combinatorial search problems class NP. problems, ecient procedure f (s) takes
potential solution set determines whether fact solution, exponentially many potential solutions, fact solutions. s1 ; : : :; sN
potential sets consider, quickly form superposition p1N (js1i + : : : + jsN i)
simultaneously evaluate f (sP
) states, resulting superposition
sets evaluation, i.e., p1N jsi ; soln = f (si )i. jsi ; soln = f (si )i represents
classical search state considering set si along variable soln whose value true
false according result evaluating consistency set respect
problem requirements. point quantum computer has, sense, evaluated
possible sets determined solutions. Unfortunately, make measurement system, get set equal probability 1=N unlikely
observe solution. thus better slow classical search method random
generate-and-test sets randomly constructed tested solution found.
Alternatively, obtain solution high probability repeating operation
O(N ) times, either serially (taking long time) multiple copies device (requiring large amount hardware energy if, say, computation done using multiple
photons). shows trade-off time energy (or physical resources),
conjectured apply generally solving search problems (Cerny, 1993),
also seen trade-off time number processors parallel computers.
useful combinatorial search, can't evaluate various sets instead
must arrange amplitude concentrated solution sets greatly increase
probability solution observed. Ideally would done mapping
gives constructive interference amplitude solutions destructive interference nonsolutions. Designing maps complicated fact must linear unitary
operators described above. Beyond physical restriction, algorithmic
computational requirement: mapping eciently computable (DiVincenzo &
Smolin, 1994). example, map cannot require priori knowledge solutions
(otherwise constructing map would require first search). computational
requirement analogous restriction search heuristics: useful, heuristic
must take long time compute. requirements mapping trade
101

fiHogg

other. Ideally one would like find way satisfy map
computed polynomial time give, worst, polynomially small probability get
solution problem soluble. One approach arrange constructive interference
solutions nonsolutions receive random contributions amplitude.
random contributions effective complete destructive interference,
easier construct form basis recent factoring algorithm (Shor, 1994) well
method presented here.
Classical search algorithms suggest ways combine use superpositions
interference. include local repair styles search complete assignments
modified, backtracking search, solutions built incrementally. Using superpositions, many possibilities could simultaneously considered. However search
methods priori specification number steps required reach solution
unclear determine enough amplitude might concentrated solution
states make measurement worthwhile. Since measurement process destroys
superposition, possible resume computation point measurement made produce solution. subtle problem arises
different search choices lead solutions differing numbers steps. Thus one would also
need maintain amplitude already solution states search continues.
dicult due requirement reversible computations.
may fruitful investigate approaches further, quantum method
proposed based instead breadth-first search incrementally builds
solutions. Classically, methods maintain list goods given size. step,
list updated include goods one additional variable. Thus step i, list
consists sets size used create new list sets size + 1. CSP
n variables, ranges 0 n , 1, completing n steps list
contain solutions problem. Classically, useful method finding
single solution list partial assignments grows exponentially number
steps taken. quantum computer, hand, handle lists readily
superpositions. method described below, superposition step consists
sets size i, consistent ones, i.e., sets level lattice.
question make final measurement computation requires exactly
n steps. Moreover, opportunity use interference concentrate amplitude
toward goods. done changing phase amplitudes corresponding nogoods
encountered moving lattice.
division search methods general strategy (e.g., backtrack)
problem specific choices, quantum mapping described general matrix
corresponds exploring possible changes partial sets, separate, particularly
simple, matrix incorporates information problem specific constraints.
complex maps certainly possible, simple decomposition easier design
describe. decomposition, dicult part quantum mapping independent
details constraints particular problem. suggests possibility
implementing special purpose quantum device perform general mapping.
constraints specific problem used adjust phases described below,
comparatively simple operation.
102

fiQuantum Computing Phase Transitions Combinatorial Search

constraint satisfaction problems, simple alternative representation full lattice
structure use partial assignments only, i.e., sets variable-value pairs
variable once. first sight might seem better removes
consideration necessary nogoods hence increases proportion complete sets
solutions. However, case number sets function level lattice
decreases reaching solution level, precluding simple form unitary mapping
described quantum search algorithm. Another representation avoids
problem consider assignments single order variables (selected randomly
use heuristics). version set lattice previously used
theoretical analyses phase transitions search (Williams & Hogg, 1994). may
useful explore quantum search, unlikely effective.
fixed ordering sets become nogood last steps,
resulting less opportunity interference based nogoods focus solutions.

3.4 Motivation

motivate mapping described below, consider idealized version. shows
paths lattice tend interfere destructively nonsolution states, provided
constraints small.
idealized map simply maps set lattice equally supersets next
level, introducing random phases sets found nogood. discussion
concerned relative amplitude solutions nogoods ignore overall
normalization. Thus instance, N = 6, state jf1; 2gi map unnormalized
superposition four supersets size 3, namely state jf1; 2; 3gi + : : : + jf1; 2; 6gi.
mapping, good level j receive equal contribution j
subsets prior level. Starting amplitude 1 level 0 gives amplitude
j ! goods level j. particular, L! solutions.
compare contribution nogoods, average? depend
many subsets nogoods also. simple case comparison sets
lattice nogood (starting level k given size constraints,
e.g., k = 2 problems binary constraints). Let rj expected value
magnitude amplitude sets level j. set level k rk = k! (and
zero phase) smaller subsets goods. set level j>k sum
j contributions (nogood) subsets, giving total contribution
(s) =

j
X
m=1

(sm )eim

(11)

sm subsets size j , 1 phases randomly selected.
(sm ) expected magnitude rj ,1 phase combined
give new random phase . Ignoring variation magnitude amplitudes
level gives
+
*X
j
p

(12)
rj = rj,1
e = rj,1 j
m=1

103

fiHogg

sum j random phases equivalent unbiased prandom walk (Karlin
p &
j
.
Thus
r
=
r
Taylor, 1975)

j
unit
steps


expected
net
distance

j
k j !=k!
p
rj = j !k! j>k.
crude argument gives rough estimate relative probabilities solutions
compared complete nogoods. Suppose one solution. relative
probability L!2 . nogoods relative probability (NL , 1)rL2 NLL!k! NL
given Eq. 1. interesting scaling regime L = N=b fixed b, corresponding
variety well-studied constraint satisfaction problems. gives
!
N
L
!
P
soln
= ln N k! b ln N + O(N )
(13)
ln P
nogood
L
goes infinity problems get large enhancement solutions
enough compensate rareness among sets solution level.
main limitation argument assuming subsets nogood also
nogood. many nogoods, case, resulting less opportunity
cancellation phases. worst situation respect subsets goods.
could constraints large, i.e., don't rule states many
items included. Even small constraints, could happen occasionally due
poor ordering choice adding items sets, hence suggesting lattice restricted
assignments single order much less effective canceling amplitude nogoods.
problems considered here, small constraints, large nogood cannot
many good subsets nogood means small subset violates (small) constraint
hence subsets obtained removing one element still contain bad subset
giving nogood. fact, numerical experiments (with class unstructured
problems described below) show mapping effective canceling amplitude
nogoods. Thus assumptions made simplified argument seem provide
correct intuitive description behavior.
Still assumption many nogood subsets underlying argument suggest extreme cancellation derived least apply problem many
large partial solutions. gives simple explanation diculty encountered
full map described phase transition point: transition associated
problems relatively many large partial solutions complete solutions. Hence
expect relatively less cancellation least nogoods solution level
lower overall probability find solution.
discussion suggests mapping sets supersets along random phases
introduced inconsistent set greatly decrease contribution nogoods
solution level. However, mapping physically realizable
unitary. example, mapping level 1 2 N = 3 takes states
jf1gi; jf2gi; jf3gi jf1; 2gi; jf1; 3gi; jf2; 3gi matrix
01 1 01
= @1 0 1A
(14)
0 1 1
Here, first column means state jf1gi contributes equally jf1; 2gi jf1; 3gi,
supersets, gives contribution jf2; 3gi. see immediately columns
104

fiQuantum Computing Phase Transitions Combinatorial Search

thispmatrix orthogonal, though easily normalized dividing entries
2. generally, mapping takes set level N , sets one
element. corresponding matrix one column i{set one row
(i +1)-set. column exactly N , 1's (corresponding supersets
given i{set) remaining entries 0. Two columns single
nonzero value common (and two corresponding i{sets one
values common: way share superset common).
means N gets large, columns matrix almost orthogonal (provided
i<N=2, case interest here). fact used obtain unitary matrix
fairly close M.

3.5 Search Algorithm

general idea mapping introduced move much amplitude possible
supersets (just classical breadth-first search, increments partial sets give supersets).
combined problem specific adjustment phases based testing partial
states consistency (this corresponds diagonal matrix thus particularly simple
require mixing amplitudes different states). specific
methods used described section.
3.5.1 Problem-Independent Mapping

take advantage potential cancellation amplitude nogoods described
need unitary mapping whose behavior similar ideal mapping supersets.
two general ways adjust ideal mapping sets supersets (mixtures
two approaches possible well). First, keep amplitude
level lattice instead moving amplitude next level. allows
using ideal map described (with suitable normalization) gives excellent
discrimination solutions nonsolutions, unfortunately much amplitude
reaches solution level. surprising: use random phases cancel amplitude
nogoods doesn't add anything solutions (because solutions superset
nogood hence cannot receive amplitude them). Hence best, even
nogoods cancel completely, amplitude solutions
relative number among complete sets, i.e., small. Thus random phases prevent
much amplitude moving nogoods high lattice, instead contributing
solutions amplitude simply remains lower levels lattice. Hence
better chance random selection finding solution (but, solution found,
instead getting nogood solution level, likely get smaller set
lattice). Thus must arrange amplitude taken nogoods contribute instead
goods. requires map take amplitude sets supersets,
least extent.
second way fix nonunitary ideal map move amplitude also nonsupersets. move amplitude solution level. allows canceled
amplitude nogoods go goods, also vice versa, resulting less effective
concentration solutions. done unitary matrix close possible
nonunitary ideal map supersets, also relatively simple form.
105

fiHogg

general question given k linearly independent vectors dimensional space,
km, find k orthonormal vectors space close possible k original ones.
Restricting attention subspace defined original vectors, obtained5
using singular value decomposition (Golub & Loan, 1983) (SVD) matrix
whose columns k given vectors. Specifically, decomposition = AyB ,
diagonal matrix containing singular values Ay B
orthonormal columns. real matrix M, matrices decomposition
also real-valued. matrix U = AyB orthonormal columns closest set
orthogonal vectors according
Frobenius matrix norm. is, choice U
P
minimizes jU , j2 rs jUrs , Mrs j2 among unitary matrices. construction fails
k>m since m{dimensional space cannot orthogonal vectors. Hence
restrict consideration mappings lattice levels level + 1
least many sets level i, i.e., Ni Ni+1 . Obtaining solution requires mapping
level L so, Eq. 1, restricts consideration problems LdN=2e.
example, mapping level 1 2 N = 3 given Eq. 14 singular
value decomposition = AyB decomposition given explicitly

0 p1 , p1
BB p13 p1 2

B = @ 3
2
p13

10
10 p13 p13 p13 1
2
0
0
C
B
p1
p1 C
q6 2 CA@ 00 10 01 AB@ q02 , 12 21 CA
, 3
3 , p6 , p6
p16
p1

0
closest unitary matrix

(15)

0 2 2 ,1 1
1
U = AyB = 3 @ 2 ,1 2
(16)
,1 2 2
gives set orthonormal vectors close original map, one might
concerned requirement compute SVD exponentially large matrices.
Fortunately, however, resulting matrices particularly simple structure
entries depend overlap sets. Thus write matrix elements
form Urfi = ajr\fi j (r (i+1)-subset, fi i-subset). overlap jr \ fi j ranges
fi r 0 overlap. Thus instead exponentially many
distinct values, + 1, linear number. exploited give
simpler method evaluating entries matrix follows.
get expressions values given N since resulting column
vectors orthonormal. Restricting attention real values, gives




Xi

1 = U yU ffff = nk a2k
k=0




nk = ki +N 1,,i k

(17)
(18)

5. thank J. Gilbert pointing technique, variant orthogonal Procrustes problem (Golub & Loan, 1983).

106

fiQuantum Computing Phase Transitions Combinatorial Search

number ways pick r specified overlap. off-diagonal terms,
suppose jff \ fi j = p<i then, real values matrix elements,





Xi

0 = U yU fffi =
n(jkp)aj ak
j;k=0


(p) =
njk

X , p p , p N , 2i + p
x k,x x j ,x i+1,j,k+x

(19)
(20)

number sets r required overlaps ff fi , i.e., jr \ ffj = ki
jr \ fij = j i. sum, x number items set r common
ff fi . Together give + 1 equations values a0; : : :; ai, readily
solved numerically6 . multiple solutions system quadratic equations,
representing possible unitary mapping. unique one closest
ideal mapping supersets, given SVD. solution use quantum
search algorithm7 , although possible solution, conjunction various
choices phases, performs better. Note number values equations grows
linearly level lattice, even though number sets level grows
exponentially. necessary distinguish values different levels lattice,
use a(ki) mean value ak mapping level + 1.
example Eq. 14, N = 3 = 1, 1 = a20 + 2a21 Eq. 17
0 = 2a0a1 + a21 Eq. 19. solution unitarity conditions closest Eq. 14
a0 = , 13 ; a1 = 23 corresponding Eq. 16.
normalized version ideal map a(ii) = p1ni = pN1 ,i values equal
zero. actual values a(ki) fairly close (confirming ideal map
close orthogonal already), alternate sign. illustrate behavior, useful
consider scaled values b(ki) (,1)k a(i,i)k pni,k , ni,k evaluated using Eq. 18.
behavior values N = 10 shown Fig. 2. Note b(0i) close one,
decreases slightly higher levels lattice (i.e., larger values) considered:
ideal mapping closer orthogonal low levels lattice.
Despite simple values example Eq. 16, ak values general
appear simple closed form expression. suggested obtaining exact solutions Eqs. 17 19 using Mathematica symbolic algebra program (Wolfram, 1991).
cases gives complicated expressions involving nested roots. Since expressions could simplify, ak values also checked close rational numbers
whether roots single variable polynomials low degree8 . Neither simplification
found apply.
Finally note mapping describes sets level
mapped next level. full quantum system also perform mapping
6. High precision values obtained FindRoot function Mathematica.
7. values given Online Appendix 1.
8. Using Mathematica function Rationalize package NumberTheory`Recognize`.

107

fiHogg

1
0.5
0.2
0.1
0.05
0

1

2

3

4

k

Figure 2: Behavior b(ki) vs. k log scale N = 10. three curves show values
= 4 (black), 3 (dashed) 2 (gray).
remaining sets lattice. changing map step, sets
simply left unchanged, need map sets level + 1
identity mapping orthogonal map level i. orthogonal set
mapping partly back level partly remaining sets level + 1 suitable
this: application amplitude level + 1 map used hence
doesn't matter mapping used. However, choice part overall
mapping remains degree freedom could perhaps exploited minimize errors
introduced external noise.
3.5.2 Phases Nogoods

addition general mapping one level next, problem-specific
aspect algorithm, namely choice phases nogood sets level.
ideal case described above, random phases given nogood, resulting
great deal cancellation nogoods solution level. reasonable
choice unitary mapping, policies possible well. example, one could
simply invert phase nogood9 (i.e., multiply amplitude -1). choice
doesn't work well idealized map supersets but, shown below, helpful
unitary map. motivated considering coecients shown Fig. 2.
Specifically, nogood encountered first time path lattice,
would like cancel phase supersets time enhance amplitude
sets likely lead solutions. a(i,i)1 negative, inverting phase tend
add sets differ one element nogood. least avoid
violating small constraint produced nogood set, hence may contribute
eventually sets lead solutions.
Moreover, one could use information sets next level decide
phase: currently described, computation makes use testing
9. thank J. Lamping suggesting this.

108

fiQuantum Computing Phase Transitions Combinatorial Search

consistency sets solution level itself, hence completely ineffective problems
test requires complete set. Perhaps better would mark state nogood
consistent extensions one item (this simple check since number
extensions grows linearly problem size). Another possibility phase
adjusted based many constraints violated, could particularly
appropriate partial constraint satisfaction problems (Freuder & Wallace, 1992)
optimization searches.
3.5.3 Summary

search algorithm starts evenly dividing amplitude among goods low level
K lattice. convenient choice binary CSPs start level K = 2,
number sets proportional N 2. level K L , 1, adjust
phases states depending whether good nogood map next
level. Thus ff(j ) represents amplitude set ff level j,
(j +1) = X U (j )
rff ff ff
r
ff

=

X
k

a(kj )

X

jr\ffj=k

ff

(j )
ff

(21)

ff phase assigned set ff testing whether nogood, final
inner sum sets ff k items common r. is, ff = 1 ff
good set. nogoods, ff = ,1 using phase inversion method, ff = ei
uniformly selected [0; 2 ) using random phase method. Finally
measure state, obtaining complete set. set solution probability

psoln =

X fifi
fi


fi

(L) fi2
fi

(22)

sum solution sets, depending particular problem method
selecting phases.
computational resources required algorithm? storage requirements
quite modest: N bits produce superposition 2N states, enough represent
possible sets lattice structure. Since trial algorithm gives
solution probability psoln , average need repeated 1=psoln times
find solution. cost trial consists time required construct
initial superposition evaluate mapping step level K
solution level L, total L , K<N=2 mappings. initial
state consists
sets size K, polynomial number (i.e., N K ) hence
cost construct initial superposition relatively modest. mapping
one level next need produced series elementary operations
directly implemented physical devices. Determining required number
operations remains open question, though particularly simple structure
matrices require involved computations also able exploit
special purpose hardware. rate, mapping independent structure
problem cost affect relative costs different problem structures.
Finally, determining phases use nogood sets involves testing sets
constraints, relatively rapid operation NP search problems. Thus examine
109

fiHogg

cost search algorithm depends problem structure, key quantity
behavior psoln .

3.6 Example Quantum Search

illustrate algorithm's operation behavior, consider small case N = 3
map starting level K = 0 going level L = 2. Suppose f3g
supersets nogoods. begin amplitude empty set, i.e.,
state j;i. map level 0 1 gives equal amplitude singleton sets,
producing p13 (jf1gi + jf2gi + jf3gi). introduce phase nogood set, giving


p1 jf1gi + jf2gi + ei jf3gi . Finally use Eq. 16 map sets level 2, giving
3
final state







p1 4 , ei jf1; 2gi + 1 + 2ei jf1; 3gi + 1 + 2ei jf2; 3gi
(23)
3 3
level, set f1,2g good, i.e., solution. Note algorithm make
use testing states solution level consistency.
probability obtain solution final measurement made determined
amplitude solution set, case Eq. 22 becomes

fifi 1
fififi2 1

fi
psoln = fi p 4 , e fi = 27 (17 , 8 cos )
3 3

(24)

see effect different methods selecting phase nogoods.
phase selected randomly, psoln = 17
27 = 0:63 average value cos
zero. Inverting phase nogood, i.e., using = , gives psoln = 25
27 = 0:93.
probabilities compare 1/3 chance selecting solution random choice.
case, optimal choice phase obtained simple inversion.
However true general: picking phases optimally require knowledge
solutions hence feasible mapping. Note also even optimal choice
phase doesn't guarantee solution found.

4. Average Behavior Algorithm

section, behavior quantum algorithm evaluated two classes combinatorial search problems. first class, unstructured problems, used examine
phase transition particularly simple context using random inverted phases
nogoods. second class, random propositional satisfiability (SAT), evaluates
robustness algorithm problems particular structure.
classical simulation algorithm explicitly compute amplitude sets
lattice solution level mapping levels. Unfortunately,
results exponential slowdown compared quantum implementation severely
limits feasible size classical simulations. Moreover, determining expected
behavior random phase method requires repeating search number times
problem (10 tries experiments reported here). limits feasible
problem size.
110

fiQuantum Computing Phase Transitions Combinatorial Search

simple check numerical errors calculation, recorded total
normalization sets solution level. double precision calculations Sun
Sparc10, experiments reported typically norm 1 within times
10,11. indication execution time unoptimized C++ code, single trial
problem N = 14 16, L = N=2, required 70 1000 seconds,
respectively. uses direct evaluation map one level next given
Eq. 21. substantial reduction compute time possible exploiting simple
structure mapping give recursive evaluation10 . additional improvement
possible exploiting fact amplitudes real using method
inverts phases nogoods. reduced execution time 1 6 seconds per
trial N 14 16, respectively.

4.1 Unstructured Problems

examine typical behavior quantum search algorithm respect problem
structure, need suitable class problems. particularly important average
case analyses since one could inadvertently select class search problems dominated
easy cases. Fortunately observed concentration hard cases near phase transitions
provides method generate hard test cases.
phase transition behavior seen variety search problem classes.
select particularly simple class problems supposing constraints specify
nogoods randomly level 2 lattice. corresponds binary constraint satisfaction
problems (Prosser, 1996; Smith & Dyer, 1996), ignores detailed structure
nogoods imposed requirement variables unique assignment. ignoring
additional structure, able test wider range number specified nogoods
problems would case considering constraint satisfaction problems.
lack additional structure also likely make asymptotic behavior readily
apparent small problem sizes feasible classical simulation.
Furthermore, since quantum search algorithm appropriate soluble problems, restrict attention random problems solution. could obtained
randomly generating problems rejecting solution (as determined
using complete classical search method). However, overconstrained problems soluble ones become quite rare dicult find method. Instead, generate
problems prespecified solution. is, randomly selecting nogoods add
problem, pick nogoods subsets prespecified solution set.
always produces problems least one solution. Although problems tend
bit easier randomly selected soluble problems, nevertheless exhibit
concentration hard problems location general random
problems (Cheeseman et al., 1991; Williams & Hogg, 1994). quantum search started
level 2 lattice.
10. thank S. Vavasis suggesting improvement classical simulation algorithm.

111

fiHogg
cost
25
20
15
10
5
1

2

3

4

5



Figure 3: solid curves show classical backtrack search cost randomly generated
problems prespecified solution function fi = m=N N = 10
(gray) 20 (black) L = N=2. number nogoods selected
level 2 search lattice. cost average number backtrack steps,
starting empty set, required find first solution problem,
averaged 1000 problems. error bars indicate standard deviation
estimate average value, cases smaller size
plotted points. comparison, dashed curves show probability
solution randomly generated problems specified fi value,
ranging 1 left 0 right.
4.1.1 Theory

class problems, phase transition behavior illustrated Fig. 3. Specifically,
shows cost solve problem simple chronological backtrack search.
cost given terms number search nodes considered solution found.
minimum cost, search proceeds directly solution backtrack
L + 1. parameter distinguishing underconstrained overconstrained problems
ratio fi number nogoods level 2 given constraints number
items N.
Even relatively small problems, peak average search cost evident.
Moreover, peak near transition region random problems11 change
mostly soluble mostly insoluble. simple, approximate, theoretical value
location transition given point expected number solutions
equal one (Smith & Dyer, 1996; Williams & Hogg, 1994). Applying class
problems considered straightforward. Specifically, NL complete sets
problem, given Eq. 1. particular set size L good, i.e., solution,
none nogoods selected problem subset s. Hence probability
11. is, problems generated random selection nogoods without regard whether
solution.

112

fiQuantum Computing Phase Transitions Combinatorial Search

solution given

( N ),( L )
2
2
L = (mN )

(25)

2



N

2 sets size 2 choose nogoods specified directly
constraints. average number solutions Nsoln = NL L. set
= fiN L = N=b, large N becomes
1
1
ln N N h
+ fi ln 1 ,
(26)
soln

b

b2

h(x) ,x ln x , (1 , x) ln (1 , x). predicted transition point12 given
(27)
ficrit = , lnh(1(1,=b1)=b2)
ficrit = 2:41 case considered (i.e., b = 2). closely matches
location peak search cost problems prespecified solution,
shown Fig. 3, 20% larger location step solubility13 .
Furthermore, theory predicts regime polynomial average cost suciently
constraints (Hogg & Williams, 1994). determined condition
expected number goods level lattice monotonically increasing. Repeating
argument smaller levels lattice, find condition holds
2
(28)
fipoly = b 2,b 1 ln(b , 1)
fipoly = 0 b = 2.
estimates approximate, indicate class random
soluble problems defined behaves qualitatively quantitatively respect
transition behavior variety other, perhaps realistic, problem classes.
close correspondence theory (derived limit large problems), suggests
observing correct transition behavior even relatively small problems.
Moreover approximate theoretical argument suggests average cost
general classical search methods scales exponentially size problem
full range fi>0. Thus provides good test case average behavior
quantum algorithm. final observation, important obtain sucient number
samples, especially near transition region. considerable variation
problems near transition, specifically highly skewed distribution number
solutions. region, problems solutions extremely many:
enough fact give substantial contribution average number solutions even
though problems quite rare.
12. differs slightly results problems specified structure nogoods,
explicitly removing necessary nogoods consideration (Smith & Dyer, 1996; Williams & Hogg,
1994).
13. particularly large error theory: better problems larger constraints
allowed values per variable.

113

fiHogg
<T>
35
30
25
20
15
10
5
1

2

4

3

5

6

7



Figure 4: Expected number trials hT find solution vs. fi random problems
prespecified solution binary constraints, using random phases nogoods.
solid curve N = 10, 100 samples per point. gray curve
N = 20 10 samples per point (but additional samples used around
peak). error bars indicate standard error estimate hT i.
4.1.2 Phase Transition

see problem structure affects search algorithm, evaluate psoln , probability
find solution problems different structures, ranging underconstrained
overconstrained. Low values probability indicate relatively harder problems.
expected number repetitions search required find solution given
= 1=psoln. results shown Figs. 4 5 different ways introducing
phases nogood sets. see general easy-hard-easy pattern cases. Another
common feature phase transitions increased variance around transition region.
quantum search property well, shown Fig. 6.
4.1.3 Scaling

important question behavior search method average performance
scales problem size. examine question, consider scaling fixed fi .
shown Figs. 7 8 algorithms using random inverted phases nogoods,
respectively. help identify likely scaling, show results log plot
(where straight lines correspond exponential scaling) log-log plot (where straight
lines correspond power-law polynomial scaling).
dicult make definite conclusions results two reasons. First,
variation behavior different problems gives statistical uncertainty estimates
average values, particularly larger sizes fewer samples available.
standard errors estimates averages indicated error bars figures
(though cases, errors smaller size plotted points). Second,
scaling behavior could change larger cases considered. caveats mind,
figures suggest psoln remains nearly constant underconstrained problems, even
though fraction complete sets solutions decreasing exponentially.
114

fiQuantum Computing Phase Transitions Combinatorial Search

<T>
15

10

5

1

2

4

3

5

6

7



Figure 5: Expected number trials hT find solution vs. fi random problems
prespecified solution binary constraints, using inverted phases nogoods.
solid curve N = 10, 1000 samples per point. gray curve
N = 20 100 samples per point (but additional samples used around
peak). error bars indicate standard error estimate hT i.

dev(T)

40
30
20
10

1

2

4

3

5

6

7



Figure 6: Standard deviation number trials find solution N = 20
function fi . black curve random phases assigned nogoods,
gray one inverting phases.

115

fi0.5

0.5

0.3

0.3
p(soln)

p(soln)

Hogg

0.2
0.1
0.05

0.2
0.1
0.05

8

10

12

14
N

16

18

20

8

10

12

14

16 18 20

N

0.5

0.5

0.3

0.3

p(soln)

p(soln)

Figure 7: Scaling probability find solution using random phase method,
fi 1 (solid), 2 (dashed), 3 (gray) 4 (dashed gray). shown log
log-log scales (left right plots, respectively).

0.2
0.1
0.05

8

0.2
0.1
0.05

10 12 14 16 18 20 22 24
N

8

10

12

14 16 18 20 2224
N

Figure 8: Scaling probability find solution using phase inversion method,
fi 1 (solid), 2 (dashed), 3 (gray) 4 (dashed gray). shown log
log-log scales (left right plots, respectively).
behavior also seen overlap curves small fi Figs. 4 5. problems
constraints, psoln appears decrease polynomially size problem,
i.e., curves closer linear log-log plots log plots. confirmed
quantitatively making least squares fit values seeing residuals
fit power-law smaller exponential fit. interesting observation
comparing two phase choices scaling qualitatively similar, even though
phase inversion method performs better. suggests detailed values phase
choices critical scaling behavior, particular high precision evaluation
phases required. Finally note illustration average
scaling leaves open behavior worst case instances.
underconstrained cases Figs. 7 8 small additional difference
cases even odd number variables. due oscillations
amplitude goods level lattice, discussed fully context
SAT problems below.
116

fiQuantum Computing Phase Transitions Combinatorial Search
ratio
5
10
4
10
3
10
2
10
1
10
8

10

12

14

16

18

20

22

24

N

Figure 9: Scaling ratio probability find solution using quantum algorithm probability find solution random selection solution
level, using phase inversion method, fi 1 (solid), 2 (dashed), 3 (gray)
4 (dashed gray). curves close linear log scale indicating exponential improvement direct selection among complete sets,
higher enhancement problems constraints.
-1
p(soln)

10
-2
10
-3
10
-4
10
8

10

12

14

16
N

18

20

22

24

Figure 10: Comparison scaling probability find solution quantum algorithm using phase inversion method (dashed curve) random selection
solution level (solid curve) fi = 2.
Another scaling comparison see much algorithm enhances probability
find solution beyond simple quantum algorithm evaluating complete sets
making measurement. shown Fig. 9, quantum algorithm appears
give exponential improvement concentration amplitude solutions.
explicit view difference behavior shown Fig. 10 fi = 2. figure,
dashed curve shows behavior psoln phase inversion method, identical
fi = 2 curve Fig. 8.
117

fiHogg

4.2 Random 3SAT

experiments leave open question additional problem structure might affect
scaling behaviors. universality phase transition behavior search
methods suggests average behavior algorithm also
wide range problems, useful check empirically. end algorithm
applied satisfiability (SAT) problem. constraint satisfaction problem consists
propositional formula n variables requirement find assignment (true
false) variable makes formula true. Thus b = 2 assignments
variable N = 2n possible variable-value pairs. consider well-studied
NP-complete 3SAT problem formula conjunction c clauses,
disjunction 3 (possibly negated) variables.
SAT problem readily represented nogoods lattice sets (Williams &
Hogg, 1994). described Sec. 2.2, n necessary nogoods, size 2.
addition, distinct clause proposition gives single nogood size 3. case
thus additional interest specified nogoods two sizes. evaluating
quantum algorithm, start level 3 lattice. Thus smallest case
phase choices uence result n = 5.
generate random problems given number clauses selecting number
different nogoods size 3 among sets already excluded necessary nogoods14. random 3SAT, hard problems concentrated near transition (Mitchell et al., 1992) c = 4:2n. Finally, among randomly generated
problems, use fact solution15 . Using randomly selected
soluble problems results somewhat harder problems using prespecified solution.
Like studies need examine many goods nogoods lattice (Schrag &
Crawford, 1996), results restricted much smaller problems studies
random SAT. Consequently, transition region rather spread out. Furthermore,
additional structure necessary nogoods larger size constraints,
compared previous class problems, makes likely larger problems
required see asymptotic scaling behavior. However, least asymptotic
behaviors observed (Crawford & Auton, 1993) persist quite accurately even
problems small n = 3, indication scaling behavior
question small problems considered here.
4.2.1 Phase Transition

behavior algorithm function ratio clauses variables shown
Fig. 11 using phase inversion method. shows phase transition behavior.
Comparing Fig. 5, also shows class random 3SAT problems harder,
average, quantum algorithm class unstructured problems.
14. differs slightly studies random 3SAT allowing duplicate clauses propositional formula.
15. values c=n small problems examined here, enough soluble instances randomly
generated need rely prespecified solution eciently find soluble test problems.

118

fiQuantum Computing Phase Transitions Combinatorial Search
<T>
70
60
50
40
30
20
10
2

4

6

c/n

8

Figure 11: Average number tries find solution quantum search algorithm
random 3SAT function c=n, using phase inversion method.
curves correspond n = 5 (black) n = 10 (gray).
0.2
p(soln)

p(soln)

0.2
0.1
0.05
0.02
0.01

0.1
0.05
0.02

5

6

7

0.01
8
n

9

10

11

5

6

7

8

9

10 11

n

Figure 12: Scaling probability find solution, using phase inversion method,
function number variables random 3SAT problems. curves
correspond different clause variable ratios: 2 (dashed), 4 (solid), 6 (gray)
8 (gray, dashed). shown log log-log scales (left right
plots, respectively).
4.2.2 Scaling

scaling probability find solution shown Fig. 12 using phase inversion method. limited experiments random phase method showed
behavior seen unstructured class problems: somewhat worse performance
similar scaling behavior. results less clear-cut Fig. 8. c=n = 2
results consistent either polynomial exponential scaling. problems
constraints, exponential scaling somewhat better fit.
addition general scaling trend, also noticeable difference behavior
cases even odd number variables. due behavior
amplitude step lattice. Instead monotonic decrease concentration
amplitude goods, oscillatory behavior amplitude alternates
119

fiHogg

probability

1

0.95

0.9

0.85
3

4

6

8

10

12

level

Figure 13: Probability goods (i.e., consistent sets) function level lattice
3SAT problems constraints. shows behavior n equal 9
(gray dashed), 10 (black dashed), 11 (gray) 12 (black). problem,
final probability level n probability solution obtained
quantum algorithm.
dispersing focused goods different levels. extreme example
behavior shown Fig.fi 13fifor 3SAT problems constraints, i.e., c = 0. Specifically,
2
P
level shows fifi s(i)fifi sum sets level lattice
consistent, which, problems constraints, assignments variables.
probability good would found algorithm terminated
level gives indication well algorithm concentrates amplitude among
consistent states. case, expanded search space quantum algorithm results
slightly worse performance random selection among complete assignments
(all solutions case). search starts amplitude goods
level 3. total probability goods alternately decreases increases
map proceeds solution level. Cases even number variables (the black
curves figure) end step decreases probability goods, resulting
relatively lower performance compared odd variable cases (gray curves). Although
might suggest improvement even n cases starting level 2 rather
level 3, fact turns case: starting level 2 gives essentially
behavior upper levels starting search level 3 lattice due
one oscillation intermediate levels takes 2 steps complete. Increasing value
c=n, i.e., examining SAT problems constraints, reduces extent oscillations,
particularly higher levels lattice, eventually results monotonic decrease
probability search moves lattice. Nevertheless, problems
constraints existence oscillations gives rise observed difference behavior
cases even odd number variables. oscillations also seen
underconstrained cases unstructured problems Figs. 7 8.
Fig. 13 shows oscillatory behavior decreases larger problems, also
suggests may appropriate choices phases. Specifically, may
possible obtain greater concentration amplitude solutions allowing
120

fiQuantum Computing Phase Transitions Combinatorial Search

3

ratio

10
2
10
1
10

5

6

7

8
n

9

10

11

Figure 14: Scaling ratio probability find solution using quantum
algorithm probability find solution random selection solution
level function number variables random 3SAT problems
clause variable ratio equal 4. solid dashed curves correspond
using phase inversion random phase methods, respectively.
black curves compare random selection among complete sets, gray
compare selection among complete assignments. curves
close linear log scale indicating exponential improvement
direct selection among complete sets.
dispersion nogoods intermediate levels lattice using initial condition
amplitude nogoods. so, would represent new policy selecting
phases takes account problem-independent structure necessary nogoods.
would somewhat analogous focusing light lens: paths many directions
modified lens cause convergence single point.
definite results obtained improvement random selection. Specifically, Fig. 14 shows exponential improvement phase inversion random
phase methods, corresponding behavior unstructured problems Fig. 9. Similar
improvement seen values c=n well: Fig. 9 highly constrained
problems give larger improvements. stringent comparison random selection
among complete assignments (i.e., variable given single value) rather
among complete sets variable-value pairs. also shown Fig. 14, appearing
grow exponentially well. particularly significant quantum algorithm
uses larger search space containing necessary nogoods. Another view comparison given Fig. 15, showing probabilities find solution quantum
search random selection among complete assignments. conclude
results additional structure necessary nogoods constraints different sizes
qualitatively similar unstructured random problems detailed comparison
scaling behaviors requires examining larger problem sizes.

121

fiHogg

0.2

p(soln)

0.1
0.05
0.02
0.01
0.005
4

5

7

6

8

9

10

11

n

Figure 15: Comparison scaling probability find solution quantum algorithm using phase inversion method (solid curve) random selection
among complete assignments (gray curve) c=n = 4.

5. Discussion

summary, introduced quantum search algorithm evaluated average
behavior range small search problems. appears increase amplitude
solution states exponentially compared evaluating measuring quantum superposition potential solutions directly. Moreover, method exhibits transition
behavior, associated concentration hard problems, seen many classical
search methods. thus extends range methods phenomenon applies.
importantly, indicates algorithm effectively exploiting structure
search problems as, say, classical backtrack methods, prune unproductive search directions. thus major improvement simple applications quantum computing
search problems behave essentially classical generate-and-test, method
completely ignores possibility pruning hence doesn't exhibit phase transition.
transition behavior readily understood problems near transition point
many large partial goods lead solutions (Williams & Hogg, 1994). Thus
relatively high proportion paths lattice appear good
quite eventually give deadends. choice phases based detecting nogoods
able work paths near solution level hence give less
chance cancel move amplitude paths fact lead solutions.
Hence problems many large partial goods likely prove relatively dicult
quantum algorithms operate distinguishing goods nogoods various sizes.
remain many open questions. algorithm, division problem{
independent mapping lattice simple problem-specific adjustment
phases allows range policies selecting phases. would useful understand effect different policies hope improving concentration amplitude
solutions. example, use phases two distinct jobs: first, keep amplitude moving along good sets rather diffusing nogoods, second,
122

fiQuantum Computing Phase Transitions Combinatorial Search

deadend reached (i.e., good set good supersets) send amplitude
deadend promising region search space, possibly far
deadend occurred. goals, keeping amplitude concentrated one hand
sending away other, extent contradictory. Thus may prove
worthwhile consider different phase choice policies two situations. Furthermore,
mapping lattice motivated classical methods incrementally build
solutions moving sets supersets lattice. Instead using unitary maps
step close possible classical behavior, approaches could
allow significant spreading amplitude intermediate levels lattice
concentrate solutions last steps. may prove fruitful consider another type mapping based local repair methods moving among neighbors complete
sets. case, sets evaluated based number constraints violate
appropriate phase selection policy could depend number, rather whether
set inconsistent not. possibilities may also suggest new probabilistic classical
algorithms might competitive existing heuristic search methods.
new example search method exhibiting transition behavior, work
raises issues prior studies phenomenon. instance, extent
behavior apply realistic classes problems, clustering
inherent situations involving localized interactions (Hogg, 1996). dicult
check empirically due limitation small problems feasible classical
simulation algorithm. However observation behavior persists many
classes problems search methods suggests widely applicable.
also interest see phase transition phenomena appear quantum search
algorithms, observed optimization searches (Cheeseman et al., 1991; Pemberton
& Zhang, 1996; Zhang & Korf, 1996; Gent & Walsh, 1995). may also transitions
unique quantum algorithms, example required coherence time sensitivity
environmental noise.
specific instances algorithm presented here, also remaining
issues. important one cost mapping one level next terms
basic operations might realized hardware, although simple structure
matrices involved suggest costly. scaling behavior
algorithm larger cases also interest, perhaps approached examining
asymptotic nature matrix coecients Eqs. 17 19.
important practical question physical implementation quantum computers general (Barenco et al., 1995; Sleator & Weinfurter, 1995; Cirac & Zoller, 1995),
requirements imposed algorithm described here. implementation
quantum computer need deal two important diculties (Landauer, 1994).
First, defects construction device. Thus even ideal design
exactly produces desired mapping, occasional manufacturing defects environmental
noise introduce errors. thus need understand sensitivity algorithm's
behavior errors mappings. main diculty likely problemindependent mapping one level lattice next, since choice phases
problem-specific part doesn't require high precision. context note
standard error correction methods cannot used quantum computers light
requirement operations reversible. also need address extent
123

fiHogg

errors minimized first place, thus placing less severe requirements
algorithm. Particularly relevant respect possibility drastically reducing defects manufactured devices atomically precise control hardware (Drexler, 1992;
Eigler & Schweizer, 1990; Muller et al., 1995; Shen, Wang, Abeln, Tucker, Lyding, Avouris,
& Walkup, 1995). also uniquely quantum mechanical approaches controlling
errors (Berthiaume, Deutsch, & Jozsa, 1994) based partial measurements state.
work could substantially extend range ideal quantum algorithms
possible implement.
second major diculty constructing quantum computers maintaining coherence superposition states long enough complete computation. Environmental
noise gradually couples state device, reducing coherence eventually
limiting time superposition perform useful computations (Unruh, 1995;
Chuang, La amme, Shor, & Zurek, 1995). effect, coupling environment
viewed performing measurement quantum system, destroying superposition
states. problem particularly severe proposed universal quantum computers
need maintain superpositions arbitrarily long times. method presented
here, number steps known advance could implemented special purpose search device (for problems given size) rather program running
universal computer. Thus given achievable coherence time would translate limit
feasible problem size. extent limit made larger feasible
alternative classical search methods, quantum search could useful.
open question greatest theoretical interest whether algorithm simple
variants concentrate amplitude solutions suciently give polynomial,
rather exponential, decrease probability find solution NP search
problem small constraints. especially interesting since class problems
includes many well-studied NP-complete problems graph coloring propositional
satisfiability. Even worst case, may average
classes otherwise dicult real-world problems. means clear
extent quantum coherence provides powerful computational behavior classical
machines, recent proposal rapid factoring (Shor, 1994) encouraging indication
capabilities.
subtle question along lines average scaling behaves away
transition region hard problems. particular, quantum algorithms expand
range polynomially scaling problems seen highly underconstrained overconstrained instances? so, would provide class problems intermediate diculty
quantum search exponentially faster classical methods, average.
highlights importance broadening theoretical discussions quantum algorithms
include typical average behaviors addition worst case analyses. generally,
differences phase transition behaviors location compared
usual classical methods? questions, involving precise location transition
points, currently well understood even classical search algorithms. Thus comparison behavior quantum algorithm may help shed light nature
various phase transitions seem associated intrinsic structure
search problems rather specific search algorithms.
124

fiQuantum Computing Phase Transitions Combinatorial Search

Acknowledgements
thank John Gilbert, John Lamping Steve Vavasis suggestions comments work. also benefited discussions Peter Cheeseman, Scott
Clearwater, Bernardo Huberman, Kimber, Colin Williams, Andrew Yao Michael
Youssefmir.

References

Barenco, A., Deutsch, D., & Ekert, A. (1995). Conditional quantum dynamics logic
gates. Physical Review Letters, 74, 4083{4086.
Benioff, P. (1982). Quantum mechanical hamiltonian models Turing machines. J. Stat.
Phys., 29, 515{546.
Bennett, C. H. (1992). Quantum cryptography: Uncertainty service privacy.
Science, 257, 752{753.
Bernstein, E., & Vazirani, U. (1993). Quantum complexity theory. Proc. 25th ACM
Symp. Theory Computation, pp. 11{20.
Berthiaume, A., Deutsch, D., & Jozsa, R. (1994). stabilization quantum computations. Proc. Workshop Physics Computation (PhysComp94), pp.
60{62 Los Alamitos, CA. IEEE Press.
Cerny, V. (1993). Quantum computers intractable (NP-complete) computing problems.
Physical Review A, 48, 116{119.
Cheeseman, P., Kanefsky, B., & Taylor, W. M. (1991). really hard problems
are. Mylopoulos, J., & Reiter, R. (Eds.), Proceedings IJCAI91, pp. 331{337 San
Mateo, CA. Morgan Kaufmann.
Chuang, I. L., La amme, R., Shor, P. W., & Zurek, W. H. (1995). Quantum computers,
factoring decoherence. Science, 270, 1633{1635.
Cirac, J. I., & Zoller, P. (1995). Quantum computations cold trapped ions. Physical
Review Letters, 74, 4091{4094.
Crawford, J. M., & Auton, L. D. (1993). Experimental results cross-over point
satisfiability problems. Proc. 11th Natl. Conf. Artificial Intelligence
(AAAI93), pp. 21{27 Menlo Park, CA. AAAI Press.
Deutsch, D. (1985). Quantum theory, Church-Turing principle universal quantum computer. Proc. R. Soc. London A, 400, 97{117.
Deutsch, D. (1989). Quantum computational networks. Proc. R. Soc. Lond., A425, 73{90.
Dirac, P. A. M. (1958). Principles Quantum Mechanics (4th edition). Oxford.
DiVincenzo, D. P. (1995). Quantum computation. Science, 270, 255{261.
125

fiHogg

DiVincenzo, D. P., & Smolin, J. (1994). Results two-bit gate design quantum computers. Proc. Workshop Physics Computation (PhysComp94), pp.
14{23 Los Alamitos, CA. IEEE Press.
Drexler, K. E. (1992). Nanosystems: Molecular Machinery, Manufacturing, Computation. John Wiley, NY.
Eigler, D. M., & Schweizer, E. K. (1990). Positioning single atoms scanning tunnelling
microscope. Nature, 344, 524{526.
Ekert, A., & Jozsa, R. (1995). Shor's quantum algorithm factorising numbers. Rev.
Mod. Phys. appear.
Feynman, R. P. (1986). Quantum mechanical computers. Foundations Physics, 16,
507{531.
Feynman, R. P. (1985). QED: Strange Theory Light Matter. Princeton Univ.
Press, NJ.
Freuder, E. C., & Wallace, R. J. (1992). Partial constraint satisfaction. Artificial Intelligence, 58, 21{70.
Garey, M. R., & Johnson, D. S. (1979). Guide Theory NP-Completeness. W.
H. Freeman, San Francisco.
Gent, I. P., & Walsh, T. (1995). TSP phase transition. Tech. rep. 95-178, Dept.
Computer Science, Univ. Strathclyde.
Golub, G. H., & Loan, C. F. V. (1983). Matrix Computations. John Hopkins University
Press, Baltimore, MD.
Hogg, T. (1994). Phase transitions constraint satisfaction search. World Wide Web
page URL ftp://parcftp.xerox.com/pub/dynamics/constraints.html.
Hogg, T. (1996). Refining phase transitions combinatorial search. Artificial Intelligence, 81, 127{154.
Hogg, T., Huberman, B. A., & Williams, C. (1996). Phase transitions search
problem. Artificial Intelligence, 81, 1{15.
Hogg, T., & Williams, C. P. (1994). hardest constraint problems: double phase
transition. Artificial Intelligence, 69, 359{377.
Jozsa, R. (1992). Computation quantum superposition. Proc. Physics
Computation Workshop. IEEE Computer Society.
Karlin, S., & Taylor, H. M. (1975). First Course Stochastic Processes (2nd edition).
Academic Press, NY.
Kimber, D. (1992). introduction quantum computation. Tech. rep., Xerox PARC.
126

fiQuantum Computing Phase Transitions Combinatorial Search

Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization simulated annealing.
Science, 220, 671{680.
Landauer, R. (1991). Information physical. Physics Today, 44 (5), 23{29.
Landauer, R. (1994). quantum mechanically coherent computation useful? Feng, D. H.,
& Hu, B.-L. (Eds.), Proc. Drexel-4 Symposium Quantum Nonintegrability.
International Press.
Lloyd, S. (1993). potentially realizable quantum computer. Science, 261, 1569{1571.
Lloyd, S. (1995). Quantum-mechanical computers. Scientific American, 273 (4), 140{145.
Mackworth, A. (1992). Constraint satisfaction. Shapiro, S. (Ed.), Encyclopedia Artificial Intelligence, pp. 285{293. Wiley.
Margolus, N. (1990). Parallel quantum computation. Zurek, W. H. (Ed.), Complexity,
Entropy Physics Information, pp. 273{287. Addison-Wesley, New York.
Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing con icts:
heuristic repair method constraint satisfaction scheduling problems. Artificial
Intelligence, 58, 161{205.
Mitchell, D., Selman, B., & Levesque, H. (1992). Hard easy distributions SAT
problems. Proc. 10th Natl. Conf. Artificial Intelligence (AAAI92), pp.
459{465 Menlo Park. AAAI Press.
Motwani, R., & Raghavan, P. (1995). Randomized Algorithms. Cambridge University Press.
Muller, W. T., Klein, D. L., Lee, T., Clarke, J., McEuen, P. L., & Schultz, P. G. (1995).
strategy chemical synthesis nanostructures. Science, 268, 272{273.
Pemberton, J. C., & Zhang, W. (1996). Epsilon-transformation: Exploiting phase transitions solve combinatorial optimization problems. Artificial Intelligence, 81, 297{
325.
Prosser, P. (1996). empirical study phase transitions binary constraint satisfaction
problems. Artificial Intelligence, 81, 81{109.
Schrag, R., & Crawford, J. (1996). Implicates prime implicates random 3-SAT.
Artificial Intelligence, 81, 199{222.
Selman, B., Levesque, H., & Mitchell, D. (1992). new method solving hard satisfiability
problems. Proc. 10th Natl. Conf. Artificial Intelligence (AAAI92), pp.
440{446 Menlo Park, CA. AAAI Press.
Shen, T. C., Wang, C., Abeln, G. C., Tucker, J. R., Lyding, J. W., Avouris, P., & Walkup,
R. E. (1995). Atomic-scale desorption electronic vibrational excitation
mechanisms. Science, 268, 1590{1592.
127

fiHogg

Shor, P. W. (1994). Algorithms quantum computation: Discrete logarithms factoring. Goldwasser, S. (Ed.), Proc. 35th Symposium Foundations
Computer Science, pp. 124{134. IEEE Press.
Sleator, T., & Weinfurter, H. (1995). Realizable universal quantum logic gates. Physical
Review Letters, 74, 4087{4090.
Smith, B. M., & Dyer, M. E. (1996). Locating phase transition binary constraint
satisfaction problems. Artificial Intelligence, 81, 155{181.
Svozil, K. (1995). Quantum computation complexity theory I. Bulletin European
Association Theoretical Computer Sciences, 55, 170{207.
Unruh, W. G. (1995). Maintaining coherence quantum computers. Physical Review A,
41, 992.
Williams, C. P., & Hogg, T. (1994). Exploiting deep structure constraint problems.
Artificial Intelligence, 70, 73{117.
Wolfram, S. (1991). Mathematica: System Mathematics Computer (2nd
edition). Addison-Wesley, NY.
Zhang, W., & Korf, R. E. (1996). unified view complexity transitions travelling
salesman problem. Artificial Intelligence, 81, 223{239.
Zhu, L., Kleiman, V., Li, X., Lu, S. P., Trentelman, K., & Gordon, R. J. (1995). Coherent
laser control product distribution obtained photoexcitation HI. Science,
270, 77{80.

128

fiJournal Artificial Intelligence Research 4 (1996) 287{339

Submitted 1/96; published 5/96

Planning Contingencies: Decision-based Approach
Louise Pryor

louisep@aisb.ed.ac.uk

Gregg Collins

collins@ils.nwu.edu

Department Artificial Intelligence, University Edinburgh
80 South Bridge
Edinburgh EH1 1HN, Scotland
Institute Learning Sciences, Northwestern University
1890 Maple Avenue
Evanston, IL 60201, USA

Abstract

fundamental assumption made classical AI planners uncertainty
world: planner full knowledge conditions plan
executed outcome every action fully predictable. planners cannot
therefore construct contingency plans, i.e., plans different actions performed
different circumstances. paper discuss issues arise representation
construction contingency plans describe Cassandra, partial-order contingency
planner. Cassandra uses explicit decision-steps enable agent executing plan
decide plan branch follow. decision-steps plan result subgoals acquire
knowledge, planned way subgoals. Cassandra thus
distinguishes process gathering information process making decisions.
explicit representation decisions Cassandra allows coherent approach
problems contingent planning, provides solid base extensions use
different decision-making procedures.

1. Introduction
Many plans use everyday lives specify ways coping various problems
might arise execution. words, incorporate contingency plans .
contingencies involved plan often made explicit plan communicated
another agent, e.g., \try taking Western Avenue, it's blocked use Ashland,"
\crank lawnmower twice, still doesn't start jiggle spark plug." Socalled classical planners 1 cannot construct plans sort, due primarily reliance
three perfect knowledge assumptions:
1. planner full knowledge initial conditions plan
executed, e.g., whether Western Avenue blocked;
2. actions fully predictable outcomes, e.g., cranking lawnmower definitely either work work;
1. category includes systems strips (Fikes & Nilsson, 1971), hacker (Sussman, 1975), noah
(Sacerdoti, 1977) molgen (Stefik, 1981a, 1981b). Recent classical planners include tweak (Chapman, 1987), snlp (McAllester & Rosenblitt, 1991) ucpop (Penberthy & Weld, 1992). term
due Wilkins (1988).

c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiPryor & Collins
3. change world occurs actions performed planner, e.g., nobody
else use car empty gas tank.
assumptions world totally predictable ; need contingency
plans.
perfect knowledge assumptions idealization planning context intended simplify planning process. allow development planning algorithms
provable properties completeness correctness. Unfortunately,
domains realistic: mostly, world extent unpredictable. Relying perfect knowledge assumptions unpredictable world may prove
cost-effective planner's uncertainty domain small, cost recovering failure low. general, however, may lead planner forgo options
would available potential problems anticipated advance.
example, assumption weather sunny, forecast, may neglect
take along umbrella; forecast later turns erroneous, impossible
use umbrella stay dry. cost recovering failure high, failing
prepare possible problems advance expensive mistake. order avoid
mistakes sort, autonomous agent complex domain must able make
execute contingency plans.
Recently, number researchers begun investigating possibility
relaxing perfect knowledge assumptions staying close framework
classical planning (Etzioni, Hanks, Weld, Draper, Lesh, & Williamson, 1992; Peot & Smith,
1992; Pryor & Collins, 1993; Draper, Hanks, & Weld, 1994a; Goldman & Boddy, 1994a).
work embodied Cassandra,2 contingency planner whose plans following
features:
include specific decision steps determine possible courses
action pursue;
Information gathering steps distinct decision-steps;
circumstances possible perform action distinguished
necessary perform it.

1.1 Issues Contingency Planner

contingency planner must able construct plans expected succeed
despite unknown initial conditions uncertain outcomes nondeterministic actions.
effective contingency planner must possess following capabilities:
must able anticipate outcomes nondeterministic actions;
must able recognize uncertain outcome threatens achievement
goal;
must able make contingency plans possible outcomes various
sources uncertainty affect given plan;
2. Cassandra Trojan prophet fated believed accurately predicted future
disasters. earlier version Cassandra described (Pryor & Collins, 1993).

288

fiPlanning Contingencies: Decision-based Approach


must able schedule sensing actions detect occurrence particular
contingency;



must produce plans executed correctly regardless contingency
arises.

design Cassandra addresses issues. However, several issues
addressed:


considered problem determining whether worth planning
particular outcome;



Cassandra probabilistic planner: cannot make use information
likelihood otherwise events;



ignored possibility interleaving planning execution (but see Section 7.4);



Cassandra handle exogenous events;



version Cassandra described cannot solve Moore's bomb toilet
problem (McDermott, 1987): find plans involve deciding
courses action succeed different contingencies (but see Section 6.5.5).

Cassandra assumes sources uncertainty possible outcomes known,
plans affect achievement goals. firmly classical
planning mold: job construct plans guaranteed achieve goals.
decide plan, plan for. Moreover, although believe Cassandra
sound complete, systematic. addition, current implementation
slow practical use.

1.2 Note Terminology

word conditional used variety senses literature. avoid use
altogether, except describing work authors use specialized
senses: example, conditional actions conditioning Peot Smith (1992).
use term contingency plan refer plan contains actions may may
actually executed, depending circumstances hold time. use
term context-dependent refer action effects depend context
action performed.

1.3 Outline

paper present Cassandra, describe algorithm detail, discuss
approach takes important issues contingency planning, show handles
variety example problems.
start describing structure Cassandra's plans. Section 2 describes
Cassandra represents actions, including uncertain outcomes; explains system
289

fiPryor & Collins
labels allows determination alternative courses action
contingency plan pursued; introduces notion explicit decision steps.
Section 3 brie describes basic planning algorithm absence uncertainty.
Section 4 explains algorithm extended handle uncertain outcomes actions.
particular, structure Cassandra's decisions considered, problems
ensuring soundness plan constructed. resulting algorithm described
detail properties discussed Section 5.
Section 6 consider issues arise contingency planning. Section 7
describes related work planning uncertainty. Finally, Section 8 summarizes
contributions work discusses limitations.

2. Cassandra's Plan Representation

Cassandra's representation contingency plans three major components:
action representation supports uncertain outcomes;
plan schema;
system labels keeping track elements plan relevant
contingencies.
components described remainder section.

2.1 Action Representation

Cassandra's action representation modified form strips operator (Fikes & Nilsson, 1971). consists preconditions executing action effects may
become true result executing it, standard strips operator. syntax
used ucpop (Penberthy & Weld, 1992). ucpop, action effects
complex standard strips effects: may associated set secondary preconditions , govern occurrence effect (Pednault, 1988, 1991).
Secondary preconditions allow representation context-dependent effects actions,
i.e., effects depend upon context action executed. use secondary preconditions critical Cassandra's ability represent uncertain effects,
hence nondeterministic actions, discuss Section 2.1.1.
Figure 1 shows simplified operator schema action making selection
soft-drink machine (the effects describing \make another selection" indicator light
turned omitted). operator describes two possible effects carrying
action: effect acquiring soda, depends secondary precondition
soda selected type available; effect \make another selection"
indicator light come on, depends secondary precondition soda
selected type available. effects depend upon preconditions money
entered machine machine plugged in.
2.1.1 Representing Uncertain Effects

uncertain effect Cassandra context-dependent effect unknown precondition , i.e., precondition planner neither knowingly perceive deliberately affect.
290

fiPlanning Contingencies: Decision-based Approach

Action:

(make-selection ?machine ?selection)

Preconditions:
Effects:

(:and (money-entered ?machine)
(plugged-in ?machine))
(:when
(available ?machine ?selection)
:effect (:and (dispensed ?selection)
(:not (money-entered ?machine))))
(:when
(:not (available ?machine ?selection))
:effect (another-selection-indicator-on ?machine))

secondary precondition
secondary precondition

Figure 1: Simplified representation operating vending machine
example, malfunctioning soft-drink machine may operate intermittently; planner aware intermittent functioning, unaware conditions govern
behavior, correct functioning device depends upon unknown precondition. point view planner, uncertain effect nondeterministic;
planner cannot tell advance whether occur. Clearly, definition fundamentally subjective: another planner better information might able specify
precisely conditions device functions properly, example knew
internal mechanism machine worked. another example, consider
happens coin tossed: principle, given perfect knowledge forces
distances involved, would possible predict outcome. practice, knowledge
unavailable effect action uncertain. principle, would possible
specify conditions would lead coin landing tails up; practice,
conditions unknown.
interesting note circumstances might possible planner
learn predict outcomes hitherto regarded uncertain: example,
learned soda machine worked. \Unknown" refers current situation.
representation would facilitate learning, would simply involve learning new
secondary preconditions rather whole new action representation.
Unknown preconditions play syntactic role normal preconditions within
operator schema; represented expressions formed using pseudo-predicate
:unknown. effect secondary precondition type occur
certain contexts cannot distinguished planner contexts
occur.
Figure 2 depicts simplified example operator uncertain effect|it represents action operating soft-drink machine intermittently fails dispense
soda despite operated correctly. operator two uncertain effects, one
soda dispensed, soda dispensed.
Clearly, uncertainty respect effects stems single underlying source, namely uncertainty whether machine malfunction.
effect, two unknown preconditions operator represent alternative results
underlying source uncertainty. relationship ected two arguments
291

fiPryor & Collins

Action:
Preconditions:
Effects:

(enter-selection ?machine)
(:and (money-entered ?machine)
(plugged-in ?machine))
(:when (:and (available ?machine ?selection)
(:unknown ?ok T))
:effect (dispensed ?selection))
(:when (:and (available ?machine ?selection)
(:unknown ?ok F))
:effect (:not (dispensed ?selection)))
(:when (available ?machine ?selection)
:effect (:not (money-entered ?machine)))
(:when (:not (available ?machine ?selection))
:effect (another-selection-indicator-on ?machine))

uncertain effect
uncertain effect

Figure 2: Operating faulty soft-drink machine
:unknown pseudo-predicate, first designates source uncertainty
associated, second designates particular outcome
uncertainty represents. possible contexts effectively partitioned set
equivalence classes, context class producing outcome
uncertainty. outcome used label equivalence class. condition
form (:unknown ?class outcome) true actual context class
designated outcome.
Notice instantiation operator introduce new source uncertainty,
means first argument unknown precondition must represented
variable operator schema. Cassandra binds variable unique identifier (i.e.,
skolem constant) operator instantiated.
Cassandra's representation assumed different sources uncertainty
independent other. source uncertainty linked uncertain outcomes
one operator, single operator may introduce number sources
uncertainty, may number outcomes. source uncertainty
exhaustive set mutually exclusive outcomes, unique name.
2.1.2 Representing Sources Uncertainty

key element Cassandra's design use single format represent sources
uncertainty affect planning. particular, uncertainty assumed manifest
uncertain effects planning operators, outlined above. Uncertainty initial
conditions handled within format treating initial conditions though
effects phantom \start step" action. treatment initial conditions,
initially developed reasons unrelated problem representing uncertain
outcomes, common snlp family planners Cassandra belongs.
Cassandra's formulation ignores uncertainty might stem outside interference
execution agent's plans, except inasmuch represented
292

fiPlanning Contingencies: Decision-based Approach
incomplete knowledge initial conditions. is, course, limitation classical
planners general; change world assumed caused directly actions
agent.

2.2 Basic Plan Representation

Cassandra's plan representation extension used ucpop (Penberthy & Weld,
1992) snlp (McAllester & Rosenblitt, 1991; Barrett, Soderland, & Weld, 1991),
turn derived representation used nonlin (Tate, 1977). plan represented
schema following components:
set steps ;
set anticipated effects steps;
set links relating effects steps produce consume (a step
consumes effect requires effect achieve one preconditions).
Note links effect denote protection intervals , i.e., intervals particular
conditions must remain true order plan work properly.
set variable bindings instantiating operator schema;
partial ordering steps;
set open conditions , i.e., unestablished goals;
set unsafe links , i.e., links conditions could falsified
effects plan.
plan complete contains open conditions unsafe links.

2.3 Representing Contingencies

contingency plan intended achieve goal regardless foreseeable
contingencies associated actually arise execution. construct valid contingency plan, planner must able enumerate contingencies. set
foreseeable contingencies computed sources uncertainty associated plan. effect, contingency one possible set outcomes relevant
sources uncertainty.
2.3.1 Contingency Labels

Keeping track whether plan achieves goal every contingency somewhat
complex process. Cassandra, like cnlp, uses system labels accomplish necessary
bookkeeping (Peot & Smith, 1992). goal, step, effect Cassandra's plan labeled
indicate contingencies element participates:
Goals labeled indicate contingencies must achieved;
Effects labeled indicate contingencies expected occur,
i.e., contingencies goals satisfy arise;
293

fiPryor & Collins


Steps labeled indicate contingencies must performed, i.e.,
union contingencies effects expected occur.

preconditions effect become new goals, labels correspond
labels effect give rise them.
general, assumed particular step could executed contingency,
albeit possibly purpose. However, sometimes necessary rule particular step
particular contingency means preventing interference plan
contingency. example, consider plan achieve goal coin heads
up, first action toss coin (see Section 4.2.3 detailed discussion
plan). one contingency coin lands heads up, actions required.
another contingency, coin lands tails must turned order goal
achieved. clear, however, turning action must performed
first contingency: would mean goal coin heads
achieved. Cassandra, ruling steps accomplished associating negative labels
plan steps indicate contingencies steps executed.
Peot Smith (1992) call process conditioning .
addition, every step depends, directly indirectly, particular outcome
given source uncertainty ruled every contingency involves alternative
outcome source uncertainty. discuss reason restriction
detail below.
Cassandra's labeling system thus provides clear guidance agent executing
plan, simply performs steps whose positive labels ect actual circumstances hold execution. Steps neither positive negative labels involving
current contingency affect goals, guaranteed executable.
contrast, agent executing plan produced cnlp guided reason labels attached steps. cnlp's plans, action need executed least one goals
represented reason labels feasible. agent must therefore method
deciding top-level goals feasible. assume done comparing
context labels top-level goal (which labeled represented
dummy actions) circumstances actually hold. Cassandra's method thus
simpler: agent simply uses positive labels plan steps instead using
labels attached step indicate goals whose context labels must analyzed.
general principles label propagation Cassandra are:


Positive labels, denote plan element concerned contributes goal
achievement contingency, propagate along causal links subgoals
plan elements establish them;



Negative labels, denote plan element concerned would prevent goal
achievement contingency, propagate along causal links effects plan
elements establish.

details given Section 5.1.4.
294

fiPlanning Contingencies: Decision-based Approach

KEY
Link

Western
Drive
Western
Belmont

Western Check
traffic
Western

Take Western
Evanston
know
traffic
status

condition

Alternative
control flow

Decide
Take Belmont
Ashland

Belmont

Ashland

Take Ashland
Evanston

Figure 3: plan includes decision-step
2.3.2 Representing Decisions

Planning seen process deciding advance done
(Collins, 1987). need contingency plans arises necessary decisions cannot
made advance missing information (see Section 6.4). decisions cannot
made advance, must made plan executed. agent executing
contingency plan must point decide possible courses action
pursue, words branch take.
Previous work effect assumed agent execute steps
consistent contingency actually holds (Warren, 1976; Peot & Smith, 1992).
However, determination steps consistent cannot (by definition) made
advance; order know contingency holds execution, agent executing
plan must general gather information decision based. ensure
viable plan, planner must able guarantee steps required gather
information con ict required carry rest plan. Therefore,
planner must general able include information gathering steps, well
steps support decision making, plan constructing. Cassandra achieves
representing decisions explicitly plan steps. preconditions decisionsteps include goals possession information relevant making decision;
scheduling actions obtain information thus handled normal planning process.
instance, consider contingency plan alluded above: \try taking Western Avenue, it's blocked use Ashland ." execution plan, agent must
point decide branch plan execute. decision-step case
would precondition knowing whether Western Avenue blocked not,
would cause planner schedule information-gathering action check trac
status Western. operation might turn precondition Western, achieved traveling junction Western Belmont.
decision taken, agent either take Western Evanston continue along
Belmont Ashland.
Assuming goal plan Evanston, final plan might depicted
Figure 3. Note control ow decision represented heavy lines. Solid lines
diagram represent links, action tail link achieving precondition
295

fiPryor & Collins
action head link. plan, agent take Western Evanston
one contingency, take Belmont Ashland Ashland Evanston
other.3
Notice order determine appropriate precondition given decision-step,
planner must way determining exactly need know order
make decision execution time. somewhat complex determination depends
part decision-making process carried out. Cassandra, decisions
modeled evaluation set condition-action rules form:
condition 1 contingency 1
condition 2 contingency 2
...
condition n contingency n
possible outcome given uncertainty gives rise decision rule; condition
decision-rule specifies set effects agent test order determine
whether execute contingency plan outcome. example, decision-rules
driving plan example would look like this:
Western Avenue blocked
execute contingency using Ashland
Western Avenue blocked execute contingency using Western
Cassandra's derivation inference rules decisions explained detail Section 4.
preconditions decision-step goals know truth values conditions
decision-rules: thus knowledge goals (McCarthy & Hayes, 1969; Pryor, 1995)
(see Section 6.4). goals treated way preconditions
step. Cassandra thus requires special provisions allow construction
information-gathering plans.
explicit representation decision-steps provides basis supporting alternative
decision procedures. Cassandra's basic model decision procedure quite
simple, complex decision procedures supported within framework (one
procedure described Section 6.5.5). example, model could changed
differential-diagnosis procedure. representation decision procedures templates
way actions represented templates would allow planner choose
alternative methods making decision way choose
alternative methods achieving subgoal. even better approach might formulate
explicit goal make correct decision, allow system construct plan
achieve goal using inferential operators. However, would effect require
goals operators stated meta-language describing preconditions
results operators. yet addressed possibility detail.
Cassandra's separation gathering information making decisions
allows one information-gathering step serve several decisions. allows exible use
information-gathering actions; effective difference actions
action may appear plan.
3. Appendix shows plans Cassandra constructs examples described paper.
plan Section A.1.

296

fiPlanning Contingencies: Decision-based Approach
New step Add plan new step effect establish open condition. Add
step preconditions secondary preconditions effect open conditions. open condition
becomes completed link.
Reuse step Make open condition complete link effect existing plan step. Add
secondary preconditions effect open conditions.

Figure 4: Resolving open conditions

3. Planning Without Contingencies
section brie review basic planning algorithm Cassandra based.
follows closely used ucpop (Penberthy & Weld, 1992), turn based
snlp (McAllester & Rosenblitt, 1991). principal difference ucpop snlp
use secondary preconditions (see Collins & Pryor, 1992).
Cassandra attempt construct contingency plan encounters
uncertainty. point, constructs plan much manner
planners snlp family. fact, uncertainty ever introduced plan,
Cassandra effectively function ucpop would circumstances.
Planning proceeds alternation two processes: resolving open conditions
protecting unsafe links . processes involves choice methods, may
therefore give rise several alternative ways extend current plan. possible
extensions constructed, best-first search algorithm guides planner's exploration
space partial plans.
initial plan consists two steps: start step, preconditions
initial conditions effects, goal step, goal conditions preconditions
effects. planner attempts modify initial plan complete :
i.e., open conditions unsafe links.

3.1 Resolving Open Conditions
planning process driven need satisfy open conditions, initially
simply input goals. course planning satisfy open condition, new subgoals
may generated; added set open conditions. planner
establish open condition one two ways: introducing new step plan,
reusing existing step making use one effects (see Figure 4). secondary
preconditions effect establishes condition become open conditions. new
step added, preconditions step become open conditions well. Finally,
time open condition established, link added plan protect newly
established condition.
One way establishing condition simply notice condition true
initial state. initial conditions treated results start operator,
always part plan, method treated establishment reusing
existing step; indeed, simplification motivation representing initial
conditions way.
297

fiPryor & Collins
link establishing condition Cond unsafe effect Eff plan (other effect
SourceEff establishes Cond (possible) effect GoalEff either established disabled
link) following properties:
Unification One postconditions Eff possibly unify either Cond negation;
Ordering step produces Eff can, according partial order, occur step
produces GoalEff step produces SourceEff.
unsafe link may resolved one three ways:
Ordering Modify ordering steps plan ensure step producing Eff occurs either
step produces SourceEff step produces GoalEff;
Separation Modify variable bindings plan ensure threatening effect Eff cannot fact
unify threatened condition Cond;
Preservation Introduce new open condition plan disable Eff. new open condition
negation one Eff's secondary preconditions.

Figure 5: Unsafe links

3.2 Protecting Unsafe Links
Whenever open condition established, links plan may jeopardized either
new step threatens existing link, new link threatened
existing step. situations link unsafe shown Figure 5. general,
link considered unsafe effect plan could possibly interfere
condition established link.
three general methods protecting threatened link (see Figure 5). First,
ordering used constrain threatening action occur either beginning
end threatened link. Second, threatening effect threatened
link separated imposing constraints variables involved effect
cannot unified established condition. Third, link preserved
generating new subgoal disable effect threatens link.

4. Contingency Planning
Cassandra proceeds described previous section either plan completed
uncertainty introduced. section describes uncertainties introduced
handled.
example plan involving uncertainty, let us consider version Moore's
classic \bomb toilet" problem (McDermott, 1987), goal bomb
disarmed , initial conditions bomb package1 bomb package2 .
uncertainty case lies initial conditions: depending outcome
uncertainty, start operator either effect bomb package1
effect bomb package2 .

4.1 Contingencies
Uncertainty introduced plan open condition plan achieved
uncertain effect, i.e., effect unknown precondition. bomb-in-the-toilet
298

fiPlanning Contingencies: Decision-based Approach

KEY
Move
package1
Pa
cka
toi ge
let

Start

Link

condition

Link
uncertain
effect

condition

Dunk
package1

Bomb
package1

Bomb
disarmed

End

Figure 6: introduction uncertainty plan
example, instance, Cassandra may achieve condition bomb disarmed selecting
dunk operator, preconditions package toilet , bomb
package . condition bomb package established identifying
bomb package1 , effect start operator. However,
condition uncertain, determined noting unknown precondition.
Cassandra attempt deal uncertainty introducing new contingency (or
new contingencies) plan. state plan introduction
uncertainty illustrated Figure 6.
4.1.1 Introducing Contingencies

Cassandra notices uncertainty current plan becomes dependent upon particular outcome uncertainty use uncertain effect, i.e., effect
unknown precondition specifies outcome uncertainty. plan
Cassandra built point effect plan branch outcome.
Since branches must also constructed possible outcomes uncertainty,
Cassandra makes copy overall goal possible outcome uncertainty,
copy carrying label indicating outcome uncertainty must
achieved. thus effectively splits plan set branches, one possible
outcome uncertainty.4
planning otherwise identical goals, Cassandra must make certain
element branch goal one outcome relies different outcome
uncertainty. words, goal, subgoals, may achieved effect
depends, directly indirectly, outcome uncertainty one
goal's label. described above, Cassandra achieves using system negative
labels indicating contingencies particular plan elements must excluded.
4. alternative method would split plan two branches, regardless number outcomes.
case, one branch would associated given outcome uncertainty,
would associated possible outcomes uncertainty. effectively sensp
operates (Etzioni et al., 1992).

299

fiPryor & Collins

Move
package1


Bomb 1
package

Start

KEY

Pa
ckag
toil e1
et

Bomb
packagin
e2
2
age
ack ilet
P
Move

package2

II
Dunk
package1

Bomb
disarm
e

Link

condition

Link
uncertain
effect

condition



End
Dunk
package2

Bomb ed
disarm

III
IV

Element label classes


package1 contingency

II

package1 contingency
package2 contingency

III

package2 contingency
package1 contingency

IV

package2 contingency

Figure 7: contingency plan disarm bomb
bomb-in-the-toilet example, plan made dependent upon uncertain
outcome bomb package1 , new copy top level goal bomb disarmed added
set open conditions. new copy given label indicating belongs
contingency bomb package2 .5 existing top level goal
subgoals labeled indicate belong contingency bomb
package1 . effect bomb package1 , action dunk package1 , effects
action dunk package1 labeled indicate cannot play role
contingency bomb package2 .
Notice action move package1 , although plays role plan contingency bomb package1 , fact depend upon bomb
package1 . could principle made part plan disarming bomb
contingency bomb package2 , prove useful anything.
indicated fact negative label package2 contingency.
Cassandra attempts achieve new open condition bomb disarmed , may
choose dunk operator (notice prohibited using effects
existing dunk operator). new instance dunk operator turn gives rise
subgoal bomb package dunked. achieved
identification effect bomb package2 . plan thus constructed depicted
Figure 7 (the decision-step omitted clarity) listed Section A.2.
4.1.2 Uncertainties Multiple Outcomes

Although algorithm described deal uncertainties number
possible outcomes, far discussed examples two possible outcomes.
fact, two-outcome uncertainties suce describe majority problems
5. Note describing contingency way clarity exposition. actual label
constructed described Section 2.3.1.

300

fiPlanning Contingencies: Decision-based Approach

Package location1
Pickup
package

Drive ?car
location1
Start


ot
Rob tion1
loca

KEY



Link
Link
uncertain
effect

Decide

condition

condition

Alternative
control flow

B

Incomplete
portion
plan

Figure 8: partial plan pick package
considered. Indeed, technically, situation could described terms number
two-outcome uncertainties. However, hard think situations might
naturally represented terms source uncertainty two outcomes.
example, suppose planner interested getting hold particular object
situation object known one three places. case,
start pseudo-operator would naturally represented three uncertain effects
(one possible location object) associated alternative outcomes
single source uncertainty. Cassandra's plan acquiring object would involve
three contingencies, one possible location.
4.1.3 Multiple Sources Uncertainty

plan may involve two sources uncertainty, case plan
one set branches. example, suppose Cassandra given goal picking
package one two locations, one two cars available
use. uncertainty regarding location package encountered first
construction plan, Cassandra respond building plan involving
two contingencies, one location. Call contingencies B (see Figure 8
Section A.3).
point construction plan contingency A, Cassandra
encounter uncertainty concerning car available make current
plan dependent upon one particular outcome uncertainty. Since new source
uncertainty arises context planning contingency A, contingency effect
bifurcated two contingencies: A1 , package location 1 car 1
301

fiPryor & Collins

Package location1
car1 available

Drive car1
location1

Robot
location1

Package location1

Start

car2 available

Drive car2
location1

1
Decide


Robot n1
io

loca

2

Link
uncertain
effect

Decide

Pickup
package

KEY
Link



Pickup
package

condition

condition

Alternative
control flow

B

Incomplete
portion
plan

Figure 9: plan two sources uncertainty
available; A2 , package location 1 car 2 available). Cassandra
must replace existing contingency labels contingency A1 labels. must
introduce new copy top-level goal labeled contingency A2 .
Note Cassandra must plan scratch achieve top-level goal contingency
A2 , spite fact already viable plan goal contingency A1 .
necessary situations may encountered successful plans
involve using different methods achieve goal two contingencies. example,
extreme differences two cars might necessitate different plans driving
(e.g., detailed representation situation presented here,
differences might affect routes cars could driven places
could parked). Cassandra must therefore consider possible ways achieve
goal contingency A2 search completion plan. particular car used
fact affect driving plan, one path search space result
isomorphic contingency plans A1 A2 (see Figure 9 Section A.4).
reasoning applies extension plan deal contingency B .
cannot assumed priori plan contingency B way resemble
plan constructed contingency A. interesting consequence
302

fiPlanning Contingencies: Decision-based Approach
uncertainty concerning availability cars necessarily arise given plan
contingency B . example, location package contingency B close
enough agent could get without using car, final plan might
three contingencies: A1 (location 1 car 1), A2 (location 1 car 2), B (location
2, foot).
Cassandra may, course, produce extension plan car used
contingency B well, case encounter uncertainty associated
location car, proceed bifurcate contingency B done
previously contingency A. limit, plan involve one contingency every
member cross product possible outcomes relevant uncertainties. However,
important note every member cross-product set must appear
contingency, since, shown, uncertainties may arise given particular
outcomes uncertainties.

4.2 Decision-steps

Cassandra encounters new source uncertainty adds decision-step plan
represent act determining path plan followed
execution. following ordering constraints added plan time:


decision-step must occur step uncertainty associated;



decision-step must occur step precondition whose achievement
depends particular outcome uncertainty.

4.2.1 Formulating Decision-rules

decision-step operational, must effective procedure
agent executing plan determine decision make. Cassandra, action
deciding contingency execute modeled evaluation set conditionaction rules form:
condition 1 contingency 1
condition 2 contingency 2
condition 3 contingency 3
...
Cassandra annotates decision-step plan set rules used
make decision. executing agent make decision evaluating
rules comes decision-step course executing plan. order
evaluate decision-rule, executing agent must able determine whether rule's
antecedent holds. preconditions decision-step must thus include goals know
current status condition appears antecedent rule condition.
preconditions decision-step become open conditions plan way
preconditions step.
intended effect evaluating decision-rules choose appropriate contingency given outcome particular uncertainty, conditions diagnostic
particular outcomes uncertainty. executing agent cannot, course, directly
303

fiPryor & Collins
determine outcome uncertainty, must infer presence absence
effects depend upon outcome.
straightforward approach constructing antecedent conditions
decision-rule would analyze plan operators identify effects could
expected result given outcome uncertainty, make condition
conjunction effects. However, turns overkill. fact,
necessary check effects given outcome uncertainty actually
used establish preconditions contingency associated outcome .
words, necessary verify contingency plan can, fact, succeed.
interesting consequence executing agent might, principle, end selecting
contingency plan even though outcome uncertainty one
plan associated. Notice would cause problem execution
plan, since would occur conditions plan's success met.
fact, shall see, Cassandra depends effect certain circumstances.
antecedent condition decision-rule thus conjunction direct effects
particular outcome used establish preconditions contingency plan
outcome. Decision-rules constructed incrementally plan elaborated.
discuss Cassandra's construction rules detail Section 4.2.3 below.
approach used formulating Cassandra's decision-rules consistent
Morgenstern's observation agent execute plan \make sure"
events plan executable (Morgenstern, 1987).
4.2.2 Adding Decision-rule Example

bomb-in-the-toilet example, Cassandra introduce decision-step determine
whether bomb package1 . uncertainty initial conditions,
decision constrained occur start step. must also occur
either dunk actions, since depend upon particular outcomes uncertainty.
decide step precondition know whether bomb package1 .
actions available would allow determine this|X-raying box,
example|Cassandra achieve precondition one actions, decide
basis branch plan execute.
4.2.3 Cassandra Constructs Decision-rules

point planning process Cassandra constructs decision-rule,
one precondition plan known depend upon particular outcome uncertainty gave rise decision: namely, one led Cassandra discovering
uncertainty first place. decision-rule set Cassandra initially builds thus
looks like this:
effect 1 contingency 1

contingency 2

contingency 3
...
construction plan, Cassandra must modify initial rule set time
effect depending directly source uncertainty used establish open condition
304

fiPlanning Contingencies: Decision-based Approach

Action:

(toss-coin ?coin)

Preconditions:

(holding ?agent ?coin)

Effects:

(:when (:unknown ?U H)
:effect (:and (flat ?coin)
(heads ?coin)))
(:when (:unknown ?U T)
:effect (:and (flat ?coin)
(tails ?coin)))
(:when (:unknown ?U E)
:effect (on-edge ?coin)))

uncertain effect
uncertain effect
uncertain effect

Figure 10: Representing action tossing coin
plan. particular, Cassandra must determine contingency open
condition resides, conjoin effect existing antecedent decision-rule
contingency.
Consider, example, happens coin tossed. might say theory
three possible outcomes action: coin land heads up;
tails up; edge (Figure 10). Suppose Cassandra given goal
coin at. established using at-heads effect tossing it. Since
uncertain effect, Cassandra introduces two new contingencies plan, one
outcome coin lands tails up, another outcome lands
edge.
introduction contingencies mandates introduction decision-step
whose initial rule set looks like this:6
(flat coin) [U1: H] rule heads

[U1: T] rule tails

[U1: E] rule edge
time, new open condition (know-if (flat coin)) introduced precondition decision-step, new goal conditions introduced must achieved
contingencies [U1: T] [U1: E]. Cassandra next establishes goal condition contingency [U1: T] using at-tails effect toss step. decision-rules associated
tails contingency thus modified follows:
(flat coin) [U1: H] rule heads
(flat coin) [U1: T] rule tails

[U1: E] rule edge
Finally, goal condition established contingency [U1:E] introducing new
step, tip, plan. precondition tip step coin edge,
established on-edge effect toss action. Since effect depends directly
6. Assuming ?U, variable representing source uncertainty, instantiated U1.

305

fiPryor & Collins
upon uncertainty U1, decision-rule edge contingency modified include
condition:
(flat coin)
[U1: H] rule heads
(flat coin)
[U1: T] rule tails
(on-edge coin) [U1: E] rule edge
Since plan complete, final set decision-rules (see Section A.5). Notice
rules discriminate heads-up outcome tails-up outcome.
fact, either outcome do, reason make discrimination. plan
executed either conditions depends solely upon order agent
executing plan chooses evaluate decision-rules.7
somewhat complex problem arises give Cassandra goal
coin flat heads-up. case effects established using toss
action. lead introduction two new contingencies plan, one
coin lands tails up, one lands edge. Although Cassandra
could establish (flat coin) tails-up case, would fail complete plan,
coin would heads-up. However, turn-over action used,
leaving coin flat heads-up given flat tails-up begin with.
point decision-rules follows:
(and (flat coin) (heads-up coin)) [U1: H] rule heads
(and (flat coin) (tails-up coin)) [U1: T] rule tails

[U1: E] rule edge
Cassandra must plan goal outcome coin lands
edge. effects established result tip action. However,
result heads-up uncertain effect tip action, since coin might easily
land tails up. Cassandra must therefore add another new contingency coin
lands tails tipped. instance, goal established using
turn-over action, tails-up precondition action established
uncertain result tip action. final decision-rule set first decision
follows:
(and (flat coin) (heads-up coin)) [U1: H] rule heads
(and (flat coin) (tails-up coin)) [U1: T] rule tails
(on-edge coin)
[U1: E] rule edge
on-edge contingency pursued, another decision, stemming uncertain
result tip, must added plan. name second source uncertainty U2,
rules decision are:
(heads-up coin) [U2: H]
(tails-up coin) [U2: T]
plan depicted Figure 11 shown Section A.6.
7. obvious extension Cassandra would construction post-processor spots decision-rules
discriminate particular sets outcomes, prunes plan remove super uous
contingencies. Note cannot determined plan complete whether condition
pertains.

306

fiPlanning Contingencies: Decision-based Approach

flat
heads
flat
tails

heads

Decide

flat



Tip
coin

Decide

KEY

tails

condition

Link uncertain effect

ad

edge

Link

End

heads



Toss
coin

Turn coin


Turn coin


condition

flat

Alternative control flow
Incomplete portion plan

Figure 11: plan two decisions
Kick door

lock broken

Open
door





op

Link

Decide

condition

End

Start

lock

KEY

en

inta

ct



Pick
lock

n
pe

Alternative
control flow





door
unlocked

Open
door

Figure 12: Opening door
4.2.4 Decision-rules Unsafe Links

fact Cassandra allows decision-rules fully differentiate outcomes uncertainty raises somewhat subtle issue. Consider partial plan
opening locked door shown Figure 12. action kicking door has, let us say,
two possible outcomes, one lock broken one agent's foot
broken. plan contingency lock broken simply open door.
plan alternative contingency pick lock open door.
Since second plan depend causally outcome uncertainty (the
agent's foot broken order pick lock open door),
decision-rules based discussion would be:
307

fiPryor & Collins



(lock-broken)





[O: L]
[O: F]

rule lock broken
rule foot broken

Notice case pick action depends lock intact,
action may effect lock longer intact. words, kick
action potentially clobbers precondition pick. However, planner arguably
ignore clobbering, two actions belong different contingencies.
valid, though, structure decision-rules guarantees agent
choose execute contingency involving pick outcome kick
lock broken. decision-rules clearly enforce this. solution
case augment decision-rule contingency lock broken
test whether lock fact intact. results following decision-rules (the plan
shown Section A.7):
kick




(lock-broken)
(not (lock-broken))




[O: L]
[O: F]

rule lock broken
rule foot broken

Cassandra augments decision-rules way whenever direct effect uncertainty
could clobber link different contingency.

5. Contingency Planning Algorithm
section give details Cassandra's algorithm. properties considered
Section 6.

5.1 Plan Elements

plan consists steps, effects, links (some may unsafe), open conditions,
variable bindings, partial ordering, contingency labels. plan complete
open conditions unsafe links.
5.1.1 Steps Effects

plan step Step represents action. may enabling preconditions. least
one effect Eff. instantiation operator.
plan step may decision-step Decide. decision-step enabling preconditions
form (know-if Cond) condition Cond. Decide also set decision-rules.
effect Eff represents results action. attached step Step, representing action. may secondary preconditions. least one postcondition
Cond, condition becomes true result executing Step secondary
preconditions hold.
5.1.2 Links Open Conditions

link represents causal dependency plan, specifying condition Cond established effect Eff, Cond postcondition. Eff secondary preconditions
SecPre result step Step. link supports step SupStep effect SupEff
condition Cond one of:
308

fiPlanning Contingencies: Decision-based Approach
enabling precondition SupStep;
secondary precondition effect SupEff result SupStep;
negation secondary precondition effect result SupStep, thus
preserving link.
link unsafe contingency Conting required effect
ClobberEff postcondition ClobberCond (the clobbering condition) resulting step
ClobberStep that:
Either ClobberCond unify Cond;
Cond form (know-if KnowCond) ClobberCond unify KnowCond;
Step ClobberStep occur steps Step SupStep;
Effect ClobberEff occur contingency Conting.
open condition (an unachieved subgoal) represented Cassandra incomplete
link, i.e., link missing information effect establishes it.


5.1.3 Bindings Orderings

Plan bindings (codesignation constraints) specify relationships variables
constants. following relationships possible:
Two variables may codesignate;
variable may designate constant;
variable may constrained designate constant;
Two variables may constrained codesignate.
ordering constrains order two steps respect other, step
S1 must precede step S2 (S1 < S2).
5.1.4 Contingency Labels

Every step, effect open condition partial plan two sets contingency labels
attached it. interests brevity, also refer labels link; case,
mean labels step effect link establishes.
contingency label two parts: symbol representing source uncertainty,
symbol representing possible outcome source uncertainty. Positive contingency labels denote circumstances plan element must necessarily
occur; negative contingency labels denote circumstances plan element cannot
must occur.
Contingency labels must propagated plan. general, positive contingency labels propagated goals effects establish them, negative
contingency labels propagated steps effects result them.
details follows:
309

fiPryor & Collins
Plan (PartList)

1. Choose partial plan Plan PartList;
2. Plan complete, finish;
3. unsafe link Unsafe:
resolve (Plan, Unsafe) add resulting plans PartList;
Return step 1;
4. open condition Open:
establish (Plan, Open) add resulting plans PartList;
Return step 1.

Figure 13: Top level planning algorithm


step inherits positive labels effects result it;



step inherits negative labels effects establish enabling preconditions;



effect inherits positive labels steps whose enabling preconditions
establishes;



effect inherits positive labels effects whose secondary preconditions
establishes;



effect inherits negative labels step results;



effect inherits negative labels effects establish secondary preconditions;



open condition inherits positive labels step effect required
establish.

Cassandra's system label propagation based cnlp complex.
Indeed, rather complex would like. complexity mandated
need deal operators involve multiple context-dependent effects,
result step effects necessarily share labels.

5.2 Algorithm

planning process starts constructing partial plan consisting two steps:


initial step preconditions initial conditions effects;



goal step effects goal conditions enabling preconditions.

plan added (initially empty) list partial plans PartList. Planning
proceeds shown Figure 13.
remains describe threats unsafe links resolved open
conditions established.
310

fiPlanning Contingencies: Decision-based Approach
Resolve (Plan, Unsafe)

1. Initialize list NewPlans;
2. unification clobbering condition ClobberCond condition Cond established link
Unsafe involves adding codesignation constraints bindings Plan:
Make possible modification bindings Plan ensures ClobberCond cannot
unify Cond;
Add resulting partial plan NewPlans;
3. clobbering step ClobberStep precede step Step establishes Unsafe:
Add ordering ensure ClobberStep precedes Step;
Add resulting partial plan NewPlans;
4. step SupStep supported Unsafe precede ClobberStep:
Add ordering ensure SupStep precedes ClobberStep;
Add resulting partial plan NewPlans;
5. Prevent clobbering effect ClobberEff occurring contingency Conting link
Unsafe unsafe:
one of:
(a) Add negation secondary preconditions ClobberEff open condition
positive contingency label Conting;
(b) Add Conting negative contingency labels ClobberStep;
(c) Add Conting negative contingency labels effect SupEff step SupStep
Unsafe supports;
appropriate modify relevant decision-rule discussed Section 4.2.4;
Add orderings ensure step ClobberStep occurs steps Step SupStep;
Propagate labels appropriate;
Add resulting partial plan NewPlans;
6. Return NewPlans.

Figure 14: Resolving threats
5.2.1 Resolving Threats Unsafe Links

Figure 14 shows threats resolved. methods shown steps 2, 3, 4
standard methods found snlp ucpop; often termed separation, demotion,
promotion respectively. say methods step 5 disable threat.
methods steps 5a 5b ensure threatening effect occur given
contingency. method step 5a modification standard method found ucpop
planners use secondary preconditions. Essentially, idea prevent
effect occurring ensuring context occurs cannot hold.
method 5b prevents effect occurring contingency forbidding execution
step produces it. method step 5c notes established step effect
cannot occur given contingency. techniques result inconsistent labeling
plan element (so that, example, cannot occur every contingency
required) resulting partial plan abandoned, represents dead end
search space.
311

fiPryor & Collins
5.2.2 Establishing Open Conditions

Figure 15 shows procedure used. Procedure EstablishPre shows methods adding
new step reusing existing step; essentially methods used ucpop
extended ect need check propagate contingency labels.
Procedure EstablishUnk shows methods adding new decision reusing existing
decision specific Cassandra. issues involved discussed Section 4.2.

6. Issues Contingency Planning

Cassandra partial order planner directly descended ucpop, sound, complete, systematic|all plans produced ucpop guaranteed achieve goals,
plan ucpop find it, ucpop never revisits partial plan.
section discuss properties related issues context contingency planning.

6.1 Soundness

Ucpop's soundness depends perfect knowledge assumptions discussed Section 1.
particular, ucpop's plans sound initial conditions fully specified,

possible effects actions specified operators represent them.
uncertainties involved plan, Cassandra equivalent ucpop therefore
constructs sound plans.
uncertainties involved plan, longer assumed initial
conditions effects actions fully specified. Indeed, uncertainties arise
assumptions violated. However, assumptions adapted account
presence uncertainty: would possible, example, insist possible
initial conditions action effects specified. Cassandra's representation, means
every source uncertainty must specified use unknown secondary
preconditions, every possible outcome source uncertainty must specified.
conjecture Cassandra sound conditions. proof would follow
procedure adding new goals whenever new source uncertainty
encountered ensures every goal achieved every possible outcome uncertainty.

6.2 Completeness

conjecture Cassandra complete limited sense that, sound plan
form construct, Cassandra find it. believe simple
extension ucpop's completeness. uncertainties involved, Cassandra
always find plan way ucpop. introduction source uncertainty
plan leads addition new contingent goals. Cassandra find plan
new goals appropriate contingency. Thus, goal indeed
achieved every contingency, Cassandra find plan achieves it, long
way determining contingency holds.
example, plan disarm bomb described Section 4.1 relies
method determining package bomb in. McDermott's presentation
example, two packages indistinguishable, point example
illustrate nonetheless plan succeed disarming bomb, namely,
312

fiPlanning Contingencies: Decision-based Approach
Establish (Plan, Open)

1. open condition type :unknown EstablishPre (Plan, Open) return resulting list
plans;
2. open condition type :unknown source uncertainty Uncertainty outcome Outcome
EstablishUnk (Plan, Open, Uncertainty, Outcome) return resulting list plans.

EstablishPre (Plan, Open)

1. Initialize list NewPlans;
2. effect Eff resulting step Step Plan
Eff occur every contingency Open must established
Eff precede step SupStep Open required support
postcondition EffCond Eff unify condition Cond Open required
establish:
Complete link Open using Eff establishing effect;
Add resulting partial plan NewPlans;
3. operator effect Eff postcondition EffCond unify Cond:
Instantiate new step Step;
Complete link Open using Eff establishing effect;
Add enabling preconditions Step open conditions;
Add resulting partial plan NewPlans;
4. plan NewPlans:
Add ordering ensure Step precedes SupStep;
Add bindings necessary ensure EffCond unifies Cond;
Add secondary preconditions SecPre Eff open conditions;
Propagate labels appropriate;
5. Return NewPlans.

EstablishUnk (Plan, Open, Uncertainty,

Outcome)
1. Initialize list NewCPlans;
2. Uncertainty new source uncertainty plan:
Add new decision-step DecStep uncertainty Uncertainty;
Add new top-level goals open conditions appropriate labels;
Add resulting partial plan NewCPlans;
3. Uncertainty existing source uncertainty plan:
Find existing decision-step DecStep uncertainty Uncertainty;
Add resulting partial plan NewCPlans;
4. plan NewCPlans:
Modify decision-rule DecStep Outcome include Cond antecedent;
Add (know-if Cond) open condition required establish DecStep;
Add orderings ensure DecStep precedes SupStep;
Propagate labels appropriate;
5. Return NewCPlans.

Figure 15: Establishing open conditions
313

fiPryor & Collins
dunking packages (McDermott, 1987). algorithm described previous section
cannot find plan situation impossible achieve preconditions
decision-step determines package dunk. Section 6.5.5 discuss
example detail describe simple extension Cassandra allows correct
plan (to dunk packages) found.
Ucpop's completeness, like soundness, depends perfect knowledge assumptions discussed Section 1. Cassandra's completeness depends three extensions
assumptions:


sources uncertainty specified;



specified outcomes exhaustive;



actions available allow determination outcome uncertainty, even indirectly.

Unfortunately, conditions necessary sucient. Cassandra
find plans actions uses determine contingency interfere
achievement goal. instance, might dropping action available
would detonate bomb inside package dropped. certainly action
allows determination outcome uncertainty, sound plan
makes use it.
order useful notion Cassandra's completeness, must therefore specify form plans construct. problem common proving
completeness planner: example, claim snlp, say, incomplete
cannot find plan bomb-in-the-toilet problem. say instead
valid plan form construct. fairly simple specify form
plans snlp construct: consist partially ordered sequences steps,
executed. introduction contingencies makes description
Cassandra's plans rather complex; yet formalize description,
actively working direction. Informally, Cassandra construct plans
every source uncertainty include step decide one relevant plan branches.
extension Cassandra solves bomb-in-the-toilet problem
construct plans meet criterion.

6.3 Systematicity

Ucpop systematic: never visit partial plan twice searching. Cas-

sandra, described paper, systematic; may visit partial plans
search space once. Consider plan disarm bomb discussed
Section 4.1. plan, two different ways establishing goal disarm
bomb: dunking package1 , dunking package2 . Cassandra initially choose
either way establishing goal, leading case introduction contingency
necessity replanning achieve goal contingency. search
paths arrive final plan, search systematic.
Cassandra could made systematic insisting handling contingencies
certain order, search path uses order treated dead end.
314

fiPlanning Contingencies: Decision-based Approach
However, extension added currently debate
desirability systematicity. example, Langley (1992) argues non-systematic
search method, iterative sampling, often better systematic method, depth-first
search, problems multiple solutions deep solution paths. Peot
Smith (1992) observe performance non-systematic version snlp better
original systematic version. ascribed behavior fact
exploring duplicate plans consumed less overhead ensuring systematicity.

6.4 Knowledge Goals
agent executing contingency plans must able acquire information actual
state world determine possible courses action pursue.
system constructs contingency plans must able plan information
acquisition: general, acquisition process may arbitrarily complex (Pryor & Collins,
1991).
early uential discussion goals possess knowledge world
McCarthy Hayes (1969). Since then, various theories developed
account (e.g., Moore, 1985; Haas, 1986; Morgenstern, 1987; Steel, 1995).
common thread work knowledge goals arise need specify
actions performed; words, need make actions operational . Work area whole concentrated able describe
represent knowledge goals, largely ignored issues involved building planners
construct plans containing them.
structure Cassandra based notion knowledge goals arise
need make decisions actions performed (Pryor, 1995). view,
planning process deciding advance done (Collins,
1987). world conforming perfect knowledge assumptions classical planning
always possible world totally predictable, plans therefore need
contain knowledge goals. However, assumptions relaxed may
possible make decisions advance information necessary make
available planner. information may unavailable either planner's
limited knowledge world events nondeterministically cause
conditions affect decisions yet occurred. cases may
possible planner determine decision must made even though cannot
time actually make it. case planner defer decision: plan make
future, necessary information available. Part plan
acquire information; plan thus contains knowledge goals.
Cassandra's use \unknown" preconditions indicate nondeterminism thus crucial
part mechanism. Cassandra, knowledge goals arise result deferring decisions. deferred decisions represented explicitly plans, arise
directly incompleteness Cassandra's knowledge world, whether
effects nondeterministic actions incompletely specified initial conditions.
forms uncertainty handled way: Cassandra recognized need defer decision, reason deferral important except
inasmuch results incomplete knowledge world.
315

fiPryor & Collins
view knowledge goals arising deferred decisions basically consistent
view needed order make actions operational, differs
traditional view knowledge goals directly preconditions physical actions,
instead preconditions actions make decisions. example, McCarthy
Hayes consider problem combination safe: commonly held action
opening safe precondition know combination. Cassandra, however,
goal knowing combination would arise subgoal deciding plan branch
follow, would branch possible combination.8 branches would
arise Cassandra's incomplete knowledge world: initial conditions
plan executed fully specified.
Cassandra uses variant syntactic approach proposed Haas (1986) represent
knowledge goals, limiting knowledge goals form know-if(fact). turns
adequate if, assume, possible outcomes given uncertainty known.
general, representation used Cassandra, based strips representation add
delete lists, less powerful logics proposed either Morgenstern Haas.

6.5 Miscellaneous Issues Contingency Planning
Cassandra's approach raises number questions concerning desired behavior
contingency planner, many obvious answers. section brie
consider issues raised.
6.5.1 Dependence Outcomes Superfluous Contingencies

fact contingency plan assumes particular outcome uncertainty means
cannot depend upon different outcome uncertainty. Cassandra
enforce constraint plan must causally depend upon outcome assumes.
instance, example described Section 2.3.2, plan take Ashland
actually depend Western blocked; could executed successfully regardless
level trac Western.
observation raises interesting question: plan contingency turns
depend outcome uncertainty gave rise it, would
obviate need plans alternative contingencies? instance, example,
might seem sensible execute plan use Ashland regardless whether Western
blocked. might thus seem planner edit plan way
eliminate apparently super uous contingencies. However, easily shown
version plan involve dependence outcome uncertainty
generated elsewhere search space. example, would mean
planner would fact consider plan simply involved taking Ashland. search
heuristics penalize plans involving contingencies appropriately plan
preferred contingency plan, things equal.
8. raises obvious question whether planning advance every possibility sensible
thing do. See Section 7.4 discussion issue.

316

fiPlanning Contingencies: Decision-based Approach
6.5.2 One-sided Contingencies

preceding discussion notwithstanding, plan involving contingencies always
superior plan involving contingency. planner might fact construct
plan like Western/Ashland one. take clear-cut example, suppose Pat needs
$50 bet horse. might try borrow $50 Chris, outcome
action uncertain|Chris might refuse. Alternatively, could rob convenience store.
robbery plan would (we shall stipulate) involve uncertainties, bad plan
reasons. would better first try borrow $50 Chris, then,
fails, rob convenience store. Cassandra could generate plan. order make
prefer plan contingency-free alternative, however, search metric would
take account estimated costs various actions, perform something akin
expected value computation. (See, example, Feldman & Sproull, 1977; Haddawy &
Hanks, 1992, discussions decision-theoretic measures applied planning.) order
execute plan properly, would also necessary way knowing
borrowing plan preferred robbery plan possible execute
either them.
6.5.3 Identical Branches

possible single plan could work well several different outcomes
uncertainty. instance, suppose action asking Chris $50 three possible
outcomes: either Pat gets money Chris happy (at opportunity
favor); Pat gets money Chris unhappy (at obliged
favor); Pat get money all. Pat constructs plan tries
borrow $50 Chris bet horse, then, assuming plan depend
upon Chris's happiness (which might, example, Pat needed get ride track
Chris), plan work either \get money + Chris happy" outcome
\get money + Chris unhappy" outcome.
Cassandra could find plan, would effect find twice|once
outcome uncertainty|and would still require decision-step discriminate
outcomes. inecient two ways: extra search time required
find essentially plan twice wasted, effort put making
unnecessary decision. looking ways avoid former problem. latter
could solved post-processor would \merge" identical contingency plans,
implemented technique.
6.5.4 Branch Merging

possible construct plan branches split reunite. instance,
consider Western/Ashland plan again. context goal get
Evanston arises might obligation deliver toast dinner held
Evanston restaurant. contingency due uncertainty trac Western Avenue
would case seem affect portion plan concerned getting
Evanston; probably little bearing wording toast, choice wine,
on. natural way frame plan might thus assume regardless
317

fiPryor & Collins
contingency carried out, planner eventually arrive certain location
Evanston, point single plan developed achieve final goal.
Constructing plan way would result compact plan description,
might thus reduce effort needed construct plan avoiding, example,
construction multiple copies subplan. considering methods
branch re-merging might achieved, methods considered far seem
complicate planning process considerably.
6.5.5 Fail-safe Planning

discussed Section 6.2, Cassandra's operation relies able determine,
even indirectly, outcome uncertainty. However, may always
possible, necessary precondition existence viable plan.
bomb-in-the-toilet problem, example, valid plan Cassandra cannot find:
dunk packages.
suggests method constructing plans face uncertainty
outcome uncertainty cannot determined|what one might call fail-safe plans.
Whenever uncertainty arises principle possible might non-contingent
plan would achieve goal whatever outcome uncertainty. find
plan, planner must construct version contingency plan actions
contingency branches arising uncertainty executed unconditionally.
Cassandra extended way, adding new type decision, one
execute branches parallel (Collins & Pryor, 1995). plan containing
decision sound none actions must performed achieve goal
one contingency interfere actions must performed
contingency, ability perform actions independent outcome
uncertainty. conditions clearly hold bomb-in-the-toilet problem.
Cassandra reason possibility labeling scheme distinguishes
actions must performed given contingency need
performed. possible execute branches actions branch may
performed (but need not) branches.
parallel decision added plan extended version Cassandra, new
goals added usual way labeling handled differently. branches
separated, Cassandra longer reason causal links one branch
affected actions another branch.
6.5.6 Contingent Failure

Cassandra produce plan possible achieve goal plan
possible contingencies. Often, however, goal cannot fact achieved outcome
underlying uncertainty. Consider, instance, Peot Smith's example trying
get ski resort car, road leading resort either clear blocked
snowdrifts (Peot & Smith, 1992). road clear, goal achieved,
blocked, plans doomed failure.
planner expected recognize impossibility achieving goal
general case (Chapman, 1987). However, possible approach suggested Peot
318

fiPlanning Contingencies: Decision-based Approach
Smith. could introduce alternative method resolving open goal conditions: simply
assume goal question fails.
undesirable method resolving open goal conditions subgoal fact
achievable, theory plans involving contingent failure considered
planner failed find plan goals achieved. sometimes
possible, general problem determining whether successful plan
undecidable. may always partial plans involve goal failure
cannot completed. example, partial plan modified may become
complex, resolution open condition involving introduction
unachieved subgoals. case, plans involving contingent failure never considered
unless ranked plans involve contingent failure. order
generally useful, approach must weakened: instead considering goal failure
avenues attack failed, apply high fixed penalty plans involving
failed goals. aim would fix penalty high enough contingent failure would
apply genuine cases goals unachievable. However, would necessity
heuristic approach completeness would lost.

7. Related Work
Cassandra constructed using ucpop (Penberthy & Weld, 1992) platform. Ucpop
partial order planner handles actions context-dependent effects universally quantified preconditions effects. Ucpop extension snlp (Barrett et al.,
1991; McAllester & Rosenblitt, 1991) uses subset Pednault's adl representation
(Pednault, 1989).
early contingency planner Warren's warplan-c (1976). Contingency planning
less abandoned mid seventies early nineties,9 sensp
(Etzioni et al., 1992) cnlp (Peot & Smith, 1992). sensp cnlp members
snlp family: sensp is, like Cassandra, based ucpop, cnlp based directly
snlp. C-buridan (Draper et al., 1994a; Draper, Hanks, & Weld, 1994b), probabilistic
contingency planner, based probabilistic planner buridan (Kushmerick, Hanks,
& Weld, 1995) (which based snlp) cnlp. Plinth (Goldman & Boddy,
1994a, 1994b) total-order planner based McDermott's Pedestal (1991),
strongly uenced cnlp treatment contingency plans.
Warplan-c, unlike planners considered here, use strips-based
action representation, based predicate calculus. could handle actions
two possible outcomes, merge resulting plan branches.
Sensp also differs planners considered here. represents uncertainty
use run-time variables, distinguished ordinary variables treated
constants whose values yet known. sensp plan branches arise
introduction information-gathering steps bind run-time variables. Sensp handles
plan branching constructing separate plans achieve goal particular
contingency. combines separate plans later stage, keeping branches
totally separate. Sensp thus considers contingency branches separately, rather
9. Neither noah (Sacerdoti, 1977) Interplan (Tate, 1975) explicitly addressed issues uncertainty,
although tackled problems involving (Collins & Pryor, 1995).

319

fiPryor & Collins
parallel. Actions achieve knowledge goals may preconditions sensp:
restriction required order maintain completeness.
surprisingly, Cassandra, cnlp, c-buridan, lesser extent Plinth,
many respects similar. except Plinth use basic snlp algorithm, use
extended strips representations. Cassandra differs cnlp Plinth principally
way uncertainty represented (Section 7.1); difference important implications handling knowledge goals (Section 7.2). principal difference
Cassandra c-buridan lies latter's use probabilities (Section 7.3).
Contingency planning one approach problem planning uncertainty. aim contingency planning construct single plan succeed
circumstances: essentially extension classical planning.
approaches planning uncertainty share aim: probabilistic planners
aim construct plans high probability succeeding (Section 7.3); systems
interleave planning execution attempt plan fully advance (Section 7.4).
approaches possible address problem determining contingencies planned for, currently possible Cassandra. third
approach reactive planning, behavior controlled set reaction
rules (Section 7.5).

7.1 Representation Uncertainty
cnlp Plinth, uncertainty represented combination uncertain outcomes nondeterministic actions effects observing outcomes. threevalued logic used: postcondition action may true , false , unknown .
example, action tossing coin might postcondition unk(side-up ?x). Special conditional actions , unknown precondition several mutually
exclusive sets postconditions, used observe results nondeterministic actions. example, operator observe results tossing coin might
precondition unk(side-up ?x) three possible outcomes: (side-up heads),
(side-up tails), (side-up edge).
Cnlp thus spreads representation uncertainty action whose execution produces uncertainty action observes result. consequence
cnlp cannot use observation action observe results different
actions. example, would require different actions observe results tossing
coin (which three possible outcomes) tipping coin landed edge
(which two possible outcomes).
Plinth, notion conditional action extended cover action (not
observation actions) nondeterministic effects planner's world model .
example, image-processing domain operator remove noise image may
may succeed. However, outcome evident soon applied,
special observation action required.
cnlp Plinth, information-gathering actions included plan whenever
action uncertain effects occurs. necessary uncertainty actually
represented information-gathering action rather action actually
320

fiPlanning Contingencies: Decision-based Approach
produces uncertainty. Knowledge goals thus represented explicitly two
systems.
representation used cnlp Plinth arises desire use \single
model world, representing planner's state knowledge, rather
complex formalization including epistemic ground formulas" (Goldman & Boddy,
1994b). operator therefore represents effects execution underlying action planner's knowledge world, effects
actual state world. is, course, important represent actions affect
planner's world model, believe also important represent affect
world. all, purpose reasoning actions achieve goals world,
planner's world model. particular, execution nondeterministic action actual effects, although may indeed unknown planner,
occurred cannot altered. Cassandra's representation ects this: indeed, Cassandra reason possible effects without scheduling observation actions.
means extension Cassandra can, example, solve original bomb-in-thetoilet problem, possible actions resolve uncertainty
package contains bomb: bomb's state represented planner's
world model stage beginning, known armed,
end, packages dunked known safe.
implication method representing uncertainty diculty
representing actions whose uncertain effects cannot determined execution
single action. Consider, example, malfunctioning soda machine one
indicator lights cannot make change, another lights run
product requested. Suppoe that, functioning correctly, two
indicators light simultaneously. malfunctions, must kicked make
work. Observing either light enough determine uncertain effect
(working properly malfunctioning) occurred.

7.2 Knowledge Goals
method representing uncertainty cnlp Plinth important implications
knowledge goals handled plans.
acquisition information planning task like (Pryor & Collins, 1991,
1992; Pryor, 1994). general, sequence actions required achieve given knowledge
goal may arbitrarily complex. example, action observe tossed coin might
require observer appropriate location; cases, might
several different possible methods information gathering, involving perception,
involving reasoning, combination. contingency planner, whose plans
necessarily involve achievement knowledge goals, must therefore able plan
fully generally information gathering.
confusion source uncertainty observation uncertain results
limits ways knowledge goals achieved cnlp Plinth: must
achieved special observation actions specify uncertain outcomes.
result representation terms planner's world model, means
represent effects actions (except ag unknown)
321

fiPryor & Collins
planner observed (or otherwise incorporated world model).
discussion issue Goldman Boddy (1994b) explicitly exclude knowledge
goals consideration. point out, planning uncertainty requires
distinction made actual state world planner's knowledge
it. order plan effectively knowledge goals, must represented.
done Cassandra separating representation uncertainty representation
information-gathering. effect results deterministically action, Cassandra
reasons need observe it, forms part world model.
uncertain effect, hand, incorporated unconditionally Cassandra's
world model; noted possibly true, (if necessary) Cassandra sets
subgoal determine whether indeed true.
Sensp, uses uwl representation goals actions, three different
kinds precondition used represent information goals either alone
combination (Etzioni et al., 1992). well satisfy preconditions, may achieved
actions observation, uwl hands-off preconditions indicating
value propositions must changed order achieve subgoal, find-out
preconditions. latter ways similar preconditions know-if propositions
Cassandra. precondition (find-out (P . v)) tells planner ascertain
whether P truth value v. certain circumstances type precondition
may achieved action changes value P. Knowledge goals may thus
represented find-out preconditions satisfy preconditions (often used conjunction
hands-off preconditions). Etzioni et al. argue knowledge goals
achieved actions change value proposition question
change required another purpose plan. believe unnecessary
limitation, circumstances enforcement actions may best way
achieving knowledge goals.

7.3 Probabilistic Decision-theoretic Planning
constructing plans, Cassandra recognizes presence uncertainty
extent. planners specifically address issues probability: example, buridan
constructs plans whose probability achieving goal given threshold (Kushmerick et al., 1995); Drips uses utility different possible outcome various
plans choose one highest expected utility (Haddawy & Suwandi, 1994).
Neither buridan drips constructs contingency plans, i.e., plans involve alternative courses action performed different circumstances. C-buridan, based
buridan, constructs contingency plans likely succeed (Draper et al., 1994b,
1994a). represents extension cnlp direction decision-theoretic planning.
Probabilistic planners use information probabilities possible uncertain
outcomes construct plans likely succeed. Cassandra, hand, cannot
use information constructs plans guaranteed succeed. Probabilistic
planning, relies explicit probabilities, less powerful
deterministic contingency planning performed Cassandra. Cassandra cannot use
information probabilities construct plans circumstances
information available. example, order solve bomb-in-the-toilet problem,
322

fiPlanning Contingencies: Decision-based Approach
c-buridan would information, least make assumption,

probabilities bomb package. Whatever assumptions made might
turn wrong, thus invalidating basis plan.
believe would possible build probabilistic planner using ideas
c-buridan Cassandra. explicit representation decisions Cassandra,
planner would provide excellent opportunity investigating use different
decision procedures. C-buridan relies full knowledge probabilities
time constructs plans. knowledge, like other, may available
plan executed. would relatively simple add decision procedures
Cassandra's decision representation depend information probabilities, e.g.,
follow particular course action probability given outcome exceeds
certain value. introduction decision procedures might, course, result
introduction knowledge goals determine probabilities, possibly leading eventually
system would construct plans perform empirical studies determine probabilities.
problem associated contingency planning branch merging, i.e.,
determination whether two steps separate branches treated step.
C-buridan performs full merging: effect probabilistic algorithm
based. Adding capability Cassandra area future work. major
problem encountered considering branch merging identify variables
different branches other: c-buridan's representations include variables,
problem arise. may cause diculties adaptation c-buridan's
merging mechanism Cassandra's use.
advantage combining probabilistic planning contingency planning resulting ability judge whether worth planning given contingency. One
limitations Cassandra present form requirement every possible contingency planned for. complex situations makes resulting plans cumbersome.
Moreover, Cassandra's performance deteriorates number distinct branches
plan. cost determining presence particular branch would significantly change probability plan's success might well much less cost
constructing branch. interesting issue considered future.

7.4 Interleaving Planning Execution

Although Cassandra's plans may include sensing actions, course action
actually executed depending results actions, Cassandra
interleave planning execution. Plans fully specified executed.
circumstances clearly inecient. Consider, example, Cassandra
constructs plan open combination safe (see Section 6.4). requires prior knowledge
possible combinations, constructs plan branch combination.
obvious alternative would construct plan fully specified
information-gathering step, execute plan stage and, information
gathered, construct rest plan.10 could done Cassandra
introducing another type decision procedure, planning achieve goal,
assuming would always possible find plan achieve goal. strong
10. See Section 8.2 discussion issue alternative approach.

323

fiPryor & Collins
assumption, would certainly valid cases problem opening safe.
area future work. Interleaving planning execution way would
advantage would necessary plan contingencies actually
arise. would however lose advantages planning advance. example,
possible interference actions performed information gathering
might missed, leading planner find suboptimal plans. Indeed, sensing actions
may general change world, executing full construction viable plan
might unfortunate result making achievement goal impossible.
Planners interleave planning execution include ipem (Ambros-Ingerson & Steel,
1988), xii (Golden, Etzioni, & Weld, 1994) Sage (Knoblock, 1995). three use
basic interleaving technique: planning possible steps
executed. thus set decide advance exactly planning
necessary, plans include explicit provision planning.
effects different interleaving strategies investigated design bump (Olawsky
& Gini, 1990). Continue Elsewhere strategy much preplanning possible
performed; Stop Execute strategy, goals defined terms sensor readings
executed soon encountered. found neither strategy
clear advantage other, strategies sometimes produced plans
suboptimal might fail.

7.5 Reactive Planning

different approach problem planning uncertainty taken reactive
planning paradigm. approach, specific sequence actions planned advance.
contingency planning, planner given set initial conditions goal.
However, instead producing plan branches, produces set condition-action
rules: example, universal plans (Schoppers, 1987) Situated Control Rules (SCRs)
(Drummond, 1989).
theory, reactive planning system handle exogenous events well uncertain
effects unknown initial conditions: possible provide reaction rule every
possible situation may encountered, whether circumstances would
lead envisaged. contrast, contingency planner Cassandra cannot
handle exogenous events cannot predict them. Cassandra contingency planners focus planning effort circumstances predicted possible (or likely,
case probabilistic contingency planner c-buridan).
would possible represent Cassandra's contingency plans sets conditionaction rules, using causal links preconditions specify conditions
action performed. However, reasoning required execution time
use reaction rules required execute contingency plan. Instead simply
executing next step plan, reasoning branch points, use reaction
rules requires evaluation conditions every cycle order select relevant rule.

8. Discussion

described Cassandra, partial-order contingency planner represent uncertain outcomes construct contingency plans outcomes. design Cassandra
324

fiPlanning Contingencies: Decision-based Approach
based coherent view issues arising planning uncertainty. recognizes
that, uncertain world, distinction must drawn actual state
world planner's model it; instantiates intuitively natural account
knowledge goals exist arise; bases treatment plan branching
requirements agent execute plan. result, Cassandra explicitly
plans gather information allows information-gathering actions fully general.
coherence design provides solid base advanced capabilities
use varying decision-making procedures.

8.1 Contributions
principal contribution work lies explicit representation decision steps
implications handling knowledge goals. Cassandra is, believe,
first planner decisions represented explicit actions plans
constructs. Cassandra's knowledge goals arise specifically need decide
alternative courses action, preconditions decision actions. Cassandra thus
consistent view planning process making decisions advance.
view, contingency plans plans defer decisions information
based available (Pryor, 1995). Different plan branches correspond
different decision outcomes.
use explicit decision steps, Cassandra distinguishes sensing
information-gathering actions one hand, decision making other. One
important reason making distinction decision may depend
one piece information, available performing different actions. addition,
separating information-gathering decision-making provides basis introducing alternative methods making decisions. example, extension Cassandra described
Section 6.5.5 introduces type decision directs executing agent perform
branches resulting given source uncertainty, allows construction
plans succeed situations way telling actual outcome (e.g., bomb-in-the-toilet problem). believe explicit representation
different methods making decisions important direction future research.
knowledge goals arise preconditions decisions Cassandra, need
know whether particular plan branch work distinguished need know
actual outcome uncertainty. Cassandra plan determine outcomes unless
relevant achievement otherwise goals. Moreover, Cassandra
treat knowledge goals special cases: plans achieve may complex plans
achieve goals. well planning achieve knowledge goals arise
preconditions decisions, Cassandra also produce plans top-level knowledge goals.
Two features Cassandra worth noting: exibility afforded labeling
scheme; potential learning adaptation afforded representation
uncertainty.
Cassandra's labeling scheme, although complex, allows agent executing plan
distinguish three classes action: must executed given
contingency; must not; whose execution affect achievement
325

fiPryor & Collins
goal contingency.11 feature paves way extension described
allows Cassandra build plans requiring execution branches resulting
source uncertainty.
Cassandra's representation makes assumptions intrinsic nature uncertainty. unknown precondition simply denotes information context
produce particular effect action available planner. may
information principle unknowable (in domains involving quantum effects,
example); much likely uncertainty results limitations
planner information available it. general, agent operating real-world
domain much effective learn improve performance adapt
changing conditions. use unknown preconditions represent uncertainty means
circumstances would relatively simple incorporate results learning adaptation planner's domain knowledge. example, planner might
discover predict certain outcomes: could change unknown preconditions
ones ecting new knowledge. If, hand, discovered predicted
effects consistently failing occur, could change relevant preconditions
unknown ones.

8.2 Limitations

Cassandra one increasing number planners aim extend techniques
classical planning realistic domains. Cassandra designed operate domains
two three principal constraints observed classical planners relaxed:
namely, allow non-deterministic actions incomplete knowledge initial conditions. Cassandra is, however, subject third constraint, changes take place
except result actions specified plan. clearly limits effectiveness
many real-world domains. Moreover, limits extent nondeterminism
incompleteness knowledge handled. Cassandra's plans necessarily
achieve goals sources uncertainty ignored, possible outcomes
specified.
Cassandra cannot make use information likely particular outcomes are,
unlike probabilistic decision-theoretic planners; cannot plan interleave planning
execution; provide reaction rules possible circumstances.
solve problems valid plans involving ways discriminating
possible outcomes; algorithm given cannot solve original version bombin-the-toilet problem, although extension described Section 6.5.5 (Collins
& Pryor, 1995).
algorithm described paper two major practical limitations: first,
plans produces often complex necessary; second, time taken
produce plans precludes use except simple problems.
complexity Cassandra's plans results necessity planning every
contingency lack branch merging. example, suppose open
combination safe could obtain money pay evening out. Cassandra's
11. agents make use information, guarantee third type step
actually executable.

326

fiPlanning Contingencies: Decision-based Approach
plan goal enjoying evening would one branch possible safe
combination. branch would start actions open safe,
different combination, would continue actions going restaurant
movies, say, would identical branch. simpler plan would
merge separate branches safe opened. consideration methods
branch merging area future work (see Sections 6.5.4 7.3).
circumstances, example, plan complexity could reduced
use run-time variables, introduced ipem (Ambros-Ingerson
& Steel, 1988) used sensp (Etzioni et al., 1992) (see Section 7).
uncertainty value action parameter takes (which case opening
combination safe) would possible use run-time variable represent parameter, obviating need separate plan branches. Implementing strategy would
require effective methods determining effects uncertainty limited
parameter values. general, notion indicates possible approach problem
branch merging: taking least commitment approach variable binding,
way least commitment approach taken step ordering partial order
planner. would allow concept \conditional" variable binding: variable
binding could labeled required forbidden given contingency.
analyzed complexity Cassandra's algorithm, believe
exponential. effect multiple plan branches, whose presence
increases number steps plan also increases number potential
interactions number ways resolving them. Certainly, subjective impression
Cassandra runs even slowly planners snlp family. Effective
domain-independent search control heuristics dicult find, many (toy)
domains used Cassandra even problem-specific heuristics hard come
by.

8.3 Conclusion
Cassandra planning system based firmly classical planning paradigm. Many
strengths weaknesses classical planning systems. example,
believe certain circumstances plans valid guaranteed
find valid plan one exists. However, techniques uses valid limited
circumstances, computational complexity make direct scaling unlikely
feasible.
view, principal strengths Cassandra arise explicit representation
decisions plans. shown use decisions provides natural account
knowledge goals arise planning process. also sketched
decisions used basis extensions provide added functionality. new
type decision allows fail-safe plans, provide method solving problems
bomb-in-the-toilet problem (Section 6.5.5); another type decision may provide
effective method interleaving planning execution (Section 7.4).
believe use explicit decision procedures enable extension
range applicability techniques classical planning. general, idea constructing
single plan succeed circumstances is, feel, unlikely productive:
327

fiPryor & Collins
real world complex uncertain enough trying predict behavior detail
simply impossible. However, use decision procedures that, example, involve
probabilistic techniques interleave planning execution, appears likely provide
exible framework that, although inevitably sacrificing completeness correctness,
provide basis effective, practical planning real world.

Appendix A. Cassandra's Plans
appendix shows plans constructed Cassandra examples body
paper. plan consists initial conditions, plan steps goals. initial conditions
shown top plan. unknown shown depending
particular contingency. plan steps shown next. shown number
denoting order plan. numbers parentheses show order
steps added plan. right step contingency labels.
brevity, individual effects step always omitted links establish
step's enabling secondary preconditions often omitted.
Finally, bottom plan come goal conditions. goal stated first,
contingency goal shown links establish it. usual, contingency
labels right.

A.1 Plan Get Evanston
plan shown Figure 3 discussed Section 2.3.2. Note decision-step
single active decision-rule. situation discussed comments onesided contingencies Section 6.5: route using Western quicker clear,
Ashland route slower always possible.
Initial:

[TRAFFIC0S: GOOD] (NOT (TRAFFIC-BAD))
[TRAFFIC0S: BAD] (TRAFFIC-BAD)
(AND (AT START) (ROAD WESTERN) (ROAD BELMONT) (ROAD ASHLAND))

Step

1 (4): (GO-TO-WESTERN-AT-BELMONT)
YES: [TRAFFIC0S: GOOD BAD]
(AND (NOT (AT START)) (ON WESTERN) (ON BELMONT))
0 -> (AT START)

Step

2 (3): (CHECK-TRAFFIC-ON-WESTERN)
(KNOW-IF (TRAFFIC-BAD))
1 -> (ON WESTERN)

Step

3 (2): (DECIDE TRAFFIC0S)
(and (NOT (TRAFFIC-BAD))

) => [TRAFFIC0S: GOOD]
(and
) => [TRAFFIC0S: BAD]
2 -> (KNOW-IF (TRAFFIC-BAD))

Step

4 (6): (TAKE-BELMONT)

YES: [TRAFFIC0S: BAD]
: [TRAFFIC0S: GOOD]
(AND (NOT (ON WESTERN)) (ON ASHLAND))
1 -> (ON BELMONT)

328

fiPlanning Contingencies: Decision-based Approach
Step

5 (5): (TAKE-ASHLAND)

YES: [TRAFFIC0S: BAD]
: [TRAFFIC0S: GOOD]

(AT EVANSTON)
4 -> (ON ASHLAND)
Step

: [TRAFFIC0S: GOOD]

6 (1): (TAKE-WESTERN)

YES: [TRAFFIC0S: GOOD]
: [TRAFFIC0S: BAD]

(AT EVANSTON)
1 -> (ON WESTERN)
0 -> (NOT (TRAFFIC-BAD))
Goal:

: [TRAFFIC0S: BAD]
: [TRAFFIC0S: BAD]

(AT EVANSTON)
GOAL
5 -> (AT EVANSTON)

YES: [TRAFFIC0S: BAD]
: [TRAFFIC0S: GOOD]

6 -> (AT EVANSTON)

YES: [TRAFFIC0S: GOOD]
: [TRAFFIC0S: BAD]

GOAL

Complete!

A.2 Disarming Bomb

plan shown Figures 6 7 discussed Section 4.1.1. Note
moving steps dunking steps always possible, necessary one
outcome uncertainty. fail-safe plan (see Section 6.2) therefore possible.
Initial:

[UNK0S: O2] (CONTAINS PACKAGE-2 BOMB)
[UNK0S: O1] (CONTAINS PACKAGE-1 BOMB)
(AND (AT PACKAGE-1 RUG) (AT PACKAGE-2 RUG))

Step

1 (5): (X-RAY PACKAGE-2)
(KNOW-IF (CONTAINS PACKAGE-2 BOMB))

Step

2 (3): (X-RAY PACKAGE-1)
(KNOW-IF (CONTAINS PACKAGE-1 BOMB))

Step

3 (2): (DECIDE UNK0S)
(and (CONTAINS PACKAGE-2 BOMB)

) => [UNK0S: O2]
(and (CONTAINS PACKAGE-1 BOMB)

) => [UNK0S: O1]
1 -> (KNOW-IF (CONTAINS PACKAGE-2 BOMB))
2 -> (KNOW-IF (CONTAINS PACKAGE-1 BOMB))

Step

4 (7): (MOVE RUG TOILET PACKAGE-1)
YES: [UNK0S: O1]
(AND (NOT (AT PACKAGE-1 RUG)) (AT PACKAGE-1 TOILET))
0 -> (AT PACKAGE-1 RUG)

Step

5 (6): (MOVE RUG TOILET PACKAGE-2)
YES: [UNK0S: O2]
(AND (NOT (AT PACKAGE-2 RUG)) (AT PACKAGE-2 TOILET))
0 -> (AT PACKAGE-2 RUG)

Step

6 (4): (DUNK PACKAGE-2)
(WET PACKAGE-2)

YES: [UNK0S: O2]

329

fiPryor & Collins
5 -> (AT PACKAGE-2 TOILET)
(DISARMED BOMB)
0 -> (CONTAINS PACKAGE-2 BOMB)
Step

7 (1): (DUNK PACKAGE-1)
(WET PACKAGE-1)
4 -> (AT PACKAGE-1 TOILET)
(DISARMED BOMB)
0 -> (CONTAINS PACKAGE-1 BOMB)

Goal:

: [UNK0S: O1]
YES: [UNK0S: O1]

: [UNK0S: O2]

(DISARMED BOMB)
GOAL
6 -> (DISARMED BOMB)

YES: [UNK0S: O2]
: [UNK0S: O1]

7 -> (DISARMED BOMB)

YES: [UNK0S: O1]
: [UNK0S: O2]

GOAL

Complete!

A.3 Fetching Package

plan Figure 8, discussed Section 4.1.3, involves one source uncertainty
hence contains one decision-step. two possible ways achieving goal,
one outcome uncertainty.
Initial:

(AVAILABLE CAR-1)
[LOC0S: B] (PACKAGE-AT LOCATION-2)
[LOC0S: A] (PACKAGE-AT LOCATION-1)
(AND (IS-CAR CAR-1) (IS-CAR CAR-2) (LOCATION LOCATION-1)
(LOCATION LOCATION-2))

Step

1 (2): (ASK-ABOUT-PACKAGE)
(KNOW-IF (PACKAGE-AT LOCATION-2))
0 -> (LOCATION LOCATION-2)
(KNOW-IF (PACKAGE-AT LOCATION-1))
0 -> (LOCATION LOCATION-1)

Step

2 (1): (DECIDE LOC0S)
(and (PACKAGE-AT

(and (PACKAGE-AT

1 -> (KNOW-IF
1 -> (KNOW-IF

LOCATION-2)
) => [LOC0S: B]
LOCATION-1)
) => [LOC0S: A]
(PACKAGE-AT LOCATION-2))
(PACKAGE-AT LOCATION-1))

Step

3 (4): (DRIVE CAR-1 LOCATION-1)
(AT LOCATION-1)
0 -> (AVAILABLE CAR-1)

YES: [LOC0S: A]

Step

4 (3): (DRIVE CAR-1 LOCATION-2)
(AT LOCATION-2)
0 -> (AVAILABLE CAR-1)

YES: [LOC0S: B]

Goal:

(AND (AT ?LOC) (PACKAGE-AT ?LOC))

330

fiPlanning Contingencies: Decision-based Approach

GOAL

YES: [LOC0S: B]
4 -> (AT LOCATION-2)
0 -> (PACKAGE-AT LOCATION-2)

GOAL

: [LOC0S: A]
YES: [LOC0S: A]

3 -> (AT LOCATION-1)
0 -> (PACKAGE-AT LOCATION-1)

: [LOC0S: B]

Complete!

A.4 Fetching Another Package

plan Figure 9, discussed Section 4.1.3, two sources uncertainty two
decision-steps. four possible ways achieving goal, one combination
outcomes two sources uncertainty.
Initial:

Step





(AND

[CAR0S: C2] (AVAILABLE
[CAR0S: C1] (AVAILABLE
[LOC0S: B] (PACKAGE-AT
[LOC0S: A] (PACKAGE-AT
(IS-CAR CAR-1) (IS-CAR
(LOCATION LOCATION-2))

CAR-2)
CAR-1)
LOCATION-2)
LOCATION-1)
CAR-2) (LOCATION LOCATION-1)

1 (5): (ASK-ABOUT-CAR)

YES: [LOC0S: B]

(KNOW-IF (AVAILABLE CAR-2))
0 -> (IS-CAR CAR-2)
(KNOW-IF (AVAILABLE CAR-1))
0 -> (IS-CAR CAR-1)
Step

2 (4): (DECIDE CAR0S)
YES: [LOC0S: B]
(and (AVAILABLE CAR-2)

) => [CAR0S: C2]
(and (AVAILABLE CAR-1)

) => [CAR0S: C1]
1 -> (KNOW-IF (AVAILABLE CAR-2))
1 -> (KNOW-IF (AVAILABLE CAR-1))

Step

3 (2): (ASK-ABOUT-PACKAGE)

YES: [CAR0S: C2 C1]

(KNOW-IF (PACKAGE-AT LOCATION-2))
0 -> (LOCATION LOCATION-2)
(KNOW-IF (PACKAGE-AT LOCATION-1))
0 -> (LOCATION LOCATION-1)
Step

4 (1): (DECIDE LOC0S)
(and (PACKAGE-AT

(and (PACKAGE-AT

3 -> (KNOW-IF
3 -> (KNOW-IF

YES: [CAR0S: C2 C1]
LOCATION-2)
) => [LOC0S: B]
LOCATION-1)
) => [LOC0S: A]
(PACKAGE-AT LOCATION-2))
(PACKAGE-AT LOCATION-1))

Step

5 (8): (DRIVE CAR-2 LOCATION-1)

YES: [LOC0S: A][CAR0S: C2]

331

fiPryor & Collins
: [CAR0S: C1]
(AT LOCATION-1)
0 -> (AVAILABLE CAR-2)
Step

: [CAR0S: C1]

6 (6): (DRIVE CAR-2 LOCATION-2)

YES: [LOC0S: B][CAR0S: C2]
: [CAR0S: C1]

(AT LOCATION-2)
0 -> (AVAILABLE CAR-2)
Step

: [CAR0S: C1]

7 (7): (DRIVE CAR-1 LOCATION-1)

YES: [LOC0S: A][CAR0S: C1]
: [CAR0S: C2]

(AT LOCATION-1)
0 -> (AVAILABLE CAR-1)
Step

: [CAR0S: C2]

8 (3): (DRIVE CAR-1 LOCATION-2)

YES: [LOC0S: B][CAR0S: C1]
: [CAR0S: C2]

(AT LOCATION-2)
0 -> (AVAILABLE CAR-1)
Goal:

: [CAR0S: C2]

(AND (AT ?LOC) (PACKAGE-AT ?LOC))
GOAL
5 -> (AT LOCATION-1)
0 -> (PACKAGE-AT LOCATION-1)

YES: [LOC0S: A][CAR0S: C2]
: [CAR0S: C1]
: [LOC0S: B]

6 -> (AT LOCATION-2)
0 -> (PACKAGE-AT LOCATION-2)

YES: [LOC0S: B][CAR0S: C2]
: [CAR0S: C1]
: [LOC0S: A]

8 -> (AT LOCATION-2)
0 -> (PACKAGE-AT LOCATION-2)

YES: [LOC0S: B][CAR0S: C1]
: [CAR0S: C2]
: [LOC0S: A]

7 -> (AT LOCATION-1)
0 -> (PACKAGE-AT LOCATION-1)

YES: [LOC0S: A][CAR0S: C1]
: [CAR0S: C2]
: [LOC0S: B]

GOAL

GOAL

GOAL

Complete!

A.5 Tossing Coin

Section 4.2.3 described plan ending coin. decision plan
distinguish coin landing heads-up tails-up|the decision rules
ambiguous.
Initial:

(HOLDING-COIN)

Step

1 (2): (TOSS-COIN)
(AND (NOT (HOLDING-COIN)) (ON-TABLE))
0 -> (HOLDING-COIN)

Step

2 (4): (INSPECT-COIN)
(AND (KNOW-IF (FLAT-COIN)) (KNOW-IF (HEADS-UP))
(KNOW-IF (TAILS-UP)) (KNOW-IF (ON-EDGE)))

332

fiPlanning Contingencies: Decision-based Approach
Step

3 (3): (DECIDE UNK2S)
(and (FLAT-COIN)

) => [UNK2S: H]
(and (FLAT-COIN)

) => [UNK2S: T]
(and (ON-EDGE)

) => [UNK2S: E]
2 -> (KNOW-IF (FLAT-COIN))
2 -> (KNOW-IF (ON-EDGE))

Step

4 (1): (TIP-COIN)

YES: [UNK2S: E]
: [UNK2S: H T]

(FLAT-COIN)
1 -> (ON-EDGE)
Goal:

: [UNK2S: H T]

(FLAT-COIN)
GOAL
1 -> (FLAT-COIN)

YES: [UNK2S: T]
: [UNK2S: H E]

1 -> (FLAT-COIN)

YES: [UNK2S: H]
: [UNK2S: E]

4 -> (FLAT-COIN)

YES: [UNK2S: E]
: [UNK2S: H T]

GOAL

GOAL

Complete!

A.6 Tossing Another Coin
plan Figure 11 two decisions unambiguous decision-rules. four
ways achieving goal plan, two sources uncertainty.
Initial:

(HOLDING-COIN)

Step

1 (1): (TOSS-COIN)
(AND (NOT (HOLDING-COIN)) (ON-TABLE) (KNOW-IF (FLAT-COIN))
(KNOW-IF (HEADS-UP)) (KNOW-IF (TAILS-UP)) (KNOW-IF (ON-EDGE)))
0 -> (HOLDING-COIN)

Step

2 (2): (DECIDE TOSS1S)
(and (FLAT-COIN)
(HEADS-UP)

(and (ON-EDGE)

(and (FLAT-COIN)
(TAILS-UP)

1 -> (KNOW-IF
1 -> (KNOW-IF
1 -> (KNOW-IF
1 -> (KNOW-IF

Step

) => [TOSS1S: H]
) => [TOSS1S: E]

) => [TOSS1S: T]
(ON-EDGE))
(FLAT-COIN))
(TAILS-UP))
(HEADS-UP))

3 (4): (TIP-COIN)

YES: [TOSS1S: E]

333

fiPryor & Collins
: [TOSS1S: H]
(AND (FLAT-COIN) (KNOW-IF (HEADS-UP)) (KNOW-IF (TAILS-UP)))
1 -> (ON-EDGE)
: [TOSS1S: H T]
Step

4 (5): (DECIDE TIP4S)

YES: [TOSS1S: E]
: [TOSS1S: H]

(and (TAILS-UP)

) => [TIP4S:
(and (HEADS-UP)

) => [TIP4S:
3 -> (KNOW-IF (TAILS-UP))
3 -> (KNOW-IF (HEADS-UP))
Step

5 (3): (TURN-OVER)

(HEADS-UP)
1 -> (TAILS-UP)

: [TOSS1S: H E]

6 (6): (TURN-OVER)

YES: [TOSS1S: E][TIP4S: T]
: [TOSS1S: H][TIP4S: H]
: [TOSS1S: H]

3 -> (FLAT-COIN)
(HEADS-UP)
3 -> (TAILS-UP)
Goal:

H]
: [TOSS1S: H]
: [TOSS1S: H]
YES: [TOSS1S: T]
: [TOSS1S: E H]
: [TOSS1S: H E]

1 -> (FLAT-COIN)

Step

T]

: [TOSS1S: H][TIP4S: H]

(AND (FLAT-COIN) (HEADS-UP))
GOAL
3 -> (FLAT-COIN)
6 -> (HEADS-UP)

YES: [TOSS1S: E][TIP4S: T]
: [TOSS1S: H]
: [TOSS1S: H][TIP4S: H]

3 -> (FLAT-COIN)
3 -> (HEADS-UP)

YES: [TOSS1S: E][TIP4S: H]
: [TOSS1S: H]
: [TOSS1S: H T][TIP4S: T]

1 -> (FLAT-COIN)
5 -> (HEADS-UP)

YES: [TOSS1S: T]
: [TOSS1S: H E]
: [TOSS1S: E H]

1 -> (FLAT-COIN)
1 -> (HEADS-UP)

YES: [TOSS1S: H]
: [TOSS1S: E]
: [TOSS1S: E]

GOAL

GOAL

GOAL

Complete!

A.7 Opening Door
Section 4.2.4 described plan opening locked door without key; depicted
Figure 12. plan Cassandra produces situation shown here. Even
though preconditions pick step depend effect kick step, former
cannot performed lock broken result kicking door. decision-rules
ect dependence.
334

fiPlanning Contingencies: Decision-based Approach
Initial:

(LOCK-INTACT)

Step

1 (2): (KICK)

Step

2 (4): (LOOK)
(AND (KNOW-IF (LOCKED)) (KNOW-IF (LOCK-INTACT))
(KNOW-IF (FOOT-BROKEN)))

Step

3 (3): (DECIDE KICK2S)
(and ((LOCK-INTACT))

) => [KICK2S: F]
(and (NOT (LOCKED))

) => [KICK2S: L]
2 -> (KNOW-IF (LOCKED))

Step

4 (6): (PICK)

YES: [KICK2S: F]
: [KICK2S: L]

(NOT (LOCKED))
0 -> (LOCK-INTACT)
Step

: [KICK2S: L]

5 (5): (OPEN-DOOR)

YES: [KICK2S: F]
: [KICK2S: L]

(OPEN)
4 -> (NOT (LOCKED))
Step

: [KICK2S: L]

6 (1): (OPEN-DOOR)

YES: [KICK2S: L]
: [KICK2S: F]

(OPEN)
1 -> (NOT (LOCKED))
Goal:

: [KICK2S: F]

(OPEN)
GOAL
5 -> (OPEN)

YES: [KICK2S: F]
: [KICK2S: L]

6 -> (OPEN)

YES: [KICK2S: L]
: [KICK2S: F]

GOAL

Complete!

Acknowledgements
Thanks Dan Weld Tony Barrett supplying ucpop code, Mark Peot Robert
Goldman comments earlier drafts, Fitzgerald many useful discussions,
anonymous reviewers constructive helpful criticism. Much
work performed first author student Institute Learning
Sciences, Northwestern University. work supported part AFOSR
grant number AFOSR-91-0341-DEF. Institute Learning Sciences established
1989 support Andersen Consulting, part Arthur Andersen Worldwide
Organization. Institute receives additional support Ameritech North West
Water, Institute Partners, IBM.
335

fiPryor & Collins

References

Allen, J., Hendler, J., & Tate, A. (Eds.). (1990). Readings Planning. Morgan Kaufmann,
San Mateo, CA.
Ambros-Ingerson, J., & Steel, S. (1988). Integrating planning, execution, monitoring.
Proceedings Seventh National Conference Artificial Intelligence, pp. 83{88
St Paul, MN. AAAI. Also (Allen, Hendler, & Tate, 1990).
Barrett, A., Soderland, S., & Weld, D. S. (1991). Effect step-order representations
planning. Technical report 91-05-06, Department Computer Science Engineering, University Washington, Seattle.
Chapman, D. (1987). Planning conjunctive goals. Artificial Intelligence, 32, 333{377.
Also (Allen et al., 1990).
Collins, G. C. (1987). Plan creation: Using strategies blueprints. Technical report
YALEU/CSD/RR 599, Department Computer Science, Yale University.
Collins, G., & Pryor, L. (1992). Achieving functionality filter conditions partial
order planner. Proceedings Tenth National Conference Artificial Intelligence, pp. 375{380 San Jose, CA. AAAI.
Collins, G., & Pryor, L. (1995). Planning uncertainty: key issues. Proceedings
Fourteenth International Joint Conference Artificial Intelligence, pp. 1567{
1573 Montreal, Canada. IJCAI.
Draper, D., Hanks, S., & Weld, D. (1994a). probabilistic model action leastcommitment planning information gathering. Proceedings Tenth Conference Uncertainty Artificial Intelligence, pp. 178{186 Seattle, WA. Morgan
Kaufmann.
Draper, D., Hanks, S., & Weld, D. (1994b). Probabilistic planning information gathering contingent execution. Proceedings Second International Conference
Artificial Intelligence Planning Systems, pp. 31{36 Chicago, IL. AAAI Press.
Drummond, M. (1989). Situated control rules. Proceedings First International
Conference Principles Knowledge Representation Reasoning, pp. 103{113
Toronto. Morgan Kaufmann.
Etzioni, O., Hanks, S., Weld, D., Draper, D., Lesh, N., & Williamson, M. (1992). approach planning incomplete information. Proceedings Third International Conference Knowledge Representation Reasoning, pp. 115{125 Boston,
MA. Morgan Kaufmann.
Feldman, J. A., & Sproull, R. F. (1977). Decision theory artificial intelligence II:
hungry monkey. Cognitive Science, 1, 158{192. Also (Allen et al., 1990).
Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: new approach application
theorem proving problem solving. Artificial Intelligence, 2, 189{208. Also (Allen
et al., 1990).
336

fiPlanning Contingencies: Decision-based Approach
Golden, K., Etzioni, O., & Weld, D. (1994). Omnipotence without omniscience: Ecient
sensor management planning. Proceedings Twelfth National Conference
Artificial Intelligence, pp. 1048{1054. AAAI Press.
Goldman, R. P., & Boddy, M. S. (1994a). Conditional linear planning. Proceedings
Second International Conference Artificial Intelligence Planning Systems, pp.
80{85 Chicago, IL. AAAI Press.
Goldman, R. P., & Boddy, M. S. (1994b). Representing uncertainty simple planners.
Proceedings Fourth International Conference Principles Knowledge
Representation Reasoning, pp. 238{245 Bonn. Morgan Kaufmann.
Haas, A. R. (1986). syntactic theory belief action. Artificial Intelligence, 28,
245{292.
Haddawy, P., & Hanks, S. (1992). Representations decision-theoretic planning: Utility
functions deadline goals. Proceedings Third International Conference
Principles Knowledge Representation Reasoning, pp. 71{82 Boston, MA.
Morgan Kaufmann.
Haddawy, P., & Suwandi, M. (1994). Decision-theoretic refinement planning using inheritance abstraction. Proceedings Second Internatinal Conference Artificial
Planning Systems, pp. 266{271 Chicago. AAAI Press.
Knoblock, C. (1995). Planning, executing, sensing, replanning information gathering. Proceedings Fourteenth International Joint Conference Artificial
Intelligence, pp. 1686{1693 Montreal. IJCAI.
Kushmerick, N., Hanks, S., & Weld, D. (1995). algorithm probabilistic planning.
Artificial Intelligence, 76, 239{286.
Langley, P. (1992). Systematic nonsystematic search strategies. Proceedings
First International Conference Artificial Intelligence Planning Systems, pp. 145{
152 College Park, Maryland. Morgan Kaufmann.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. Proceedings
Ninth National Conference Artificial Intelligence, pp. 634{639 Anaheim, CA.
AAAI.
McCarthy, J., & Hayes, P. J. (1969). philosophical problems standpoint
artificial intelligence. Meltzer, B., & Michie, D. (Eds.), Machine Intelligence 4, pp.
463{502. Edinburgh University Press. Also (Allen et al., 1990).
McDermott, D. (1987). critique pure reason. Computational Intelligence, 3, 151{160.
McDermott, D. (1991). Regression planning. International Journal Intelligent Systems,
6 (4), 357{416. Also available Yale TR YALEU/CSD/RR 752.
Moore, R. C. (1985). formal theory knowledge action. Hobbs, J. R., & Moore,
R. C. (Eds.), Formal Theories Commonsense World. Ablex, Norwood, NJ. Also
(Allen et al., 1990).
337

fiPryor & Collins
Morgenstern, L. (1987). Knowledge preconditions actions plans. Proceedings
Tenth International Joint Conference Artificial Intelligence, pp. 867{874
Milan. IJCAI.
Olawsky, D., & Gini, M. (1990). Deferred planning sensor use. Proceedings
Workshop Innovative Approaches Planning, Scheduling Control, pp. 166{
174 San Diego, CA. DARPA.
Pednault, E. P. D. (1988). Extending conventional planning techniques handle actions
context-dependent effects. Proceedings Seventh National Conference
Artificial Intelligence, pp. 55{59 St Paul, MN. AAAI.
Pednault, E. P. D. (1989). ADL: Exploring middle ground STRIPS
situation calculus. Proceedings First International Conference Principles
Knowledge Representation Reasoning, pp. 324{332. Morgan Kaufmann.
Pednault, E. P. D. (1991). Generalizing nonlinear planning handle complex goals
actions context-dependent effects. Proceedings Twelfth International
Joint Conference Artificial Intelligence, pp. 240{245 Sydney, Australia. IJCAI.
Penberthy, J. S., & Weld, D. S. (1992). UCPOP: sound, complete, partial order planner ADL. Proceedings Third International Conference Knowledge
Representation Reasoning, pp. 103{114 Boston, MA. Morgan Kaufmann.
Peot, M. A., & Smith, D. E. (1992). Conditional nonlinear planning. Proceedings
First International Conference Artificial Intelligence Planning Systems, pp.
189{197 College Park, Maryland. Morgan Kaufmann.
Pryor, L. (1994). Opportunities planning unpredictable world. Technical report 53,
Institute Learning Sciences, Northwestern University.
Pryor, L. (1995). Decisions, decisions: Knowledge goals planning. Hallam, J. (Ed.),
Hybrid Problems, Hybrid Solutions (Proceedings AISB-95), Frontiers Artificial
Intelligence Applications, pp. 181{192. IOS Press, Amsterdam.
Pryor, L., & Collins, G. (1991). Information-gathering planning task: position paper.
Notes AAAI workshop Knowledge-Based Construction Probabilistic
Decision Models, pp. 101{105 Anaheim, CA. AAAI.
Pryor, L., & Collins, G. (1992). Planning perceive: utilitarian approach. Working
notes AAAI Spring Symposium: Control Selective Perception, pp. 113{122
Stanford, CA. AAAI.
Pryor, L., & Collins, G. (1993). Cassandra: Planning contingencies. Technical report 41, Institute Learning Sciences, Northwestern University.
Sacerdoti, E. (1977). structure plans behavior. American Elsevier, New York.
Schoppers, M. J. (1987). Universal plans reactive robots unpredictable environments.
Proceedings Tenth International Joint Conference Artificial Intelligence,
pp. 1039{1046 Milan. IJCAI.
338

fiPlanning Contingencies: Decision-based Approach
Steel, S. (1995). Knowing how: semantic approach. Hallam, J. (Ed.), Hybrid Problems,
Hybrid Solutions (Proceedings AISB-95), Frontiers Artificial Intelligence
Applications, pp. 193{202. IOS Press, Amsterdam.
Stefik, M. (1981a). Planning constraints (MOLGEN: Part 1). Artificial Intelligence,
16, 111{140. Also (Allen et al., 1990).
Stefik, M. (1981b). Planning constraints (MOLGEN: Part 2). Artificial Intelligence,
16, 141{170.
Sussman, G. J. (1975). computer model skill acquisition. American Elsevier, New
York.
Tate, A. (1975). Using goal structure direct search problem solver. Ph.D. thesis,
University Edinburgh.
Tate, A. (1977). Generating project networks. Proceedings Fifth International
Joint Conference Artificial Intelligence, pp. 888{893 Cambridge, MA. IJCAI. Also
(Allen et al., 1990).
Warren, D. (1976). Generating conditional plans programs. Proceedings
Summer Conference Artificial Intelligence Simulation Behaviour, pp.
344{354 Edinburgh. AISB.
Wilkins, D. E. (1988). Practical Planning: Extending Classical AI Planning Paradigm.
Morgan Kaufmann, San Mateo, CA.

339

fiJournal Artificial Intelligence Research 4 (1996) 147-179

Submitted 3/95; published 4/96

Iterative Optimization Simplification
Hierarchical Clusterings
Doug Fisher

Department Computer Science, Box 1679, Station B
Vanderbilt University, Nashville, TN 37235 USA

dfisher@vuse.vanderbilt.edu

Abstract

Clustering often used discovering structure data. Clustering systems differ
objective function used evaluate clustering quality control strategy used
search space clusterings. Ideally, search strategy consistently construct
clusterings high quality, computationally inexpensive well. general,
cannot ways, partition search system inexpensively
constructs `tentative' clustering initial examination, followed iterative optimization,
continues search background improved clusterings. Given motivation,
evaluate inexpensive strategy creating initial clusterings, coupled several
control strategies iterative optimization, repeatedly modifies initial
clustering search better one. One methods appears novel iterative
optimization strategy clustering contexts. clustering constructed
judged analysts { often according task-specific criteria. Several authors abstracted criteria posited generic performance task akin pattern completion,
error rate completed patterns used `externally' judge clustering utility.
Given performance task, adapt resampling-based pruning strategies used supervised learning systems task simplifying hierarchical clusterings, thus promising
ease post-clustering analysis. Finally, propose number objective functions, based
attribute-selection measures decision-tree induction, might perform well
error rate simplicity dimensions.

1. Introduction
Clustering often used discovering structure data. Clustering systems differ
objective function used evaluate clustering quality control strategy used search
space clusterings. Ideally, search strategy consistently construct clusterings high quality, computationally inexpensive well. Given combinatorial
complexity general clustering problem, search strategy cannot computationally inexpensive give guarantee quality discovered clusterings across
diverse set domains objective functions. However, partition search
initial clustering inexpensively constructed, followed iterative optimization
procedures continue search background improved clusterings. allows
analyst get early indication possible presence form structure data,
search continue long seems worthwhile. seems primary motivation
behind design systems Autoclass (Cheeseman, Kelly, Self, Stutz, Taylor,
& Freeman, 1988) Snob (Wallace & Dowe, 1994).
paper describes evaluates three strategies iterative optimization, one inspired iterative `seed' selection strategy Cluster/2 (Michalski & Stepp, 1983a,
c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiFisher

1983b), one common form optimization iteratively reclassifies single observations, third method appears novel clustering literature. latter strategy
inspired, part, macro-learning strategies (Iba, 1989) { collections observations
reclassified en masse, appears mitigate problems associated local maxima
measured objective function. evaluation purposes, couple strategies simple, inexpensive procedure used Cobweb (Fisher, 1987a, 1987b)
system Anderson Matessa (1991), constructs initial hierarchical clustering. iterative optimization strategies, however, paired methods
constructing initial clusterings.
clustering constructed judged analysts { often according
task-specific criteria. Several authors (Fisher, 1987a, 1987b; Cheeseman et al., 1988; Anderson & Matessa, 1991) abstracted criteria generic performance task
akin pattern completion, error rate completed patterns used
`externally' judge utility clustering. systems, objective function
selected performance task mind. Given performance task
adapt resampling-based pruning strategies used supervised learning systems task
simplifying hierarchical clusterings, thus easying post-clustering analysis. Experimental
evidence suggests hierarchical clusterings greatly simplified increase
pattern-completion error rate.
experiments clustering simplification suggest `external' criteria simplicity
classification cost, addition pattern-completion error rate, judging relative
merits differing objective functions clustering. suggest several objective functions
adaptations selection measures used supervised, decision-tree induction,
may well dimensions simplicity error rate.

2. Generating Hierarchical Clusterings

Clustering form unsupervised learning partitions observations classes
clusters (collectively, called clustering). objective function quality measure guides
search, ideally clustering optimal measured objective function.
hierarchical-clustering system creates tree-structured clustering, sibling clusters
partition observations covered common parent. section brie summarizes
simple strategy, called hierarchical sorting, creating hierarchical clusterings.

2.1 Objective Function

assume observation vector nominal values, Vij along distinct variables,
Ai . measure category utility (Gluck & Corter, 1985; Corter & Gluck, 1992),
X X[P (A = V jC )2 , P (A = V )2];
CU (Ck ) = P (Ck )

ij k

ij


j

and/or variants used extensively system known Cobweb (Fisher, 1987a)
many related systems (Gennari, Langley, & Fisher, 1989; McKusick & Thompson, 1990;
Iba & Gennari, 1991; McKusick & Langley, 1991; Reich & Fenves, 1991; Biswas, Weinberg,
& Li, 1994; De Alte Da Veiga, 1994; Kilander, 1994; Ketterlin, Gancarski, & Korczak,
1995). measure rewards clusters, Ck , increase predictability variable values
148

fiOptimization Hierarchical Clusterings

within Ck (i.e., P (Ai = Vij jCk )) relative predictability population whole
(i.e., P (Ai = Vij )). favoring clusters increase predictability (i.e., P (Ai = Vij jCk ) >
P (Ai = Vij )), also necessarily favor clusters increase variable value predictiveness
(i.e., P (Ck jAi = Vij ) > P (Ck )).
Clusters many variable values predictable cohesive. Increases predictability stem shared variable values observations within cluster. cluster
well-separated decoupled clusters many variable values predictive
cluster. High predictiveness stems differences variable values shared
members one cluster shared observations another cluster. general principle clustering increase similarity observations within clusters (i.e.,
cohesion) decrease similarity observations across clusters (i.e., coupling).
Category utility similar form Gini Index, used supervised
systems construct decision trees (Mingers, 1989b; Weiss & Kulikowski, 1991).
Gini Index typically intended address issue well values variable, Ai ,
predict priori known class labels supervised context. summation Gini Indices
ected CU addresses extent cluster predicts values variables.
CU rewards clusters, Ck , reduce collective impurity variables.
Fisher's (1987a) CobwebPsystem, CU used measure quality partition
data, PU (fC1; C2; : : :CN g) = k CU (Ck )=N average category utility clusters
partition. Sections 3.5 5.2 note nonoptimalities measure partition
quality, suggest alternatives. Nonetheless, measure commonly used,
take opportunity note problems, none techniques describe
tied measure.

2.2 Structure Clusters

Cobweb, Autoclass (Cheeseman et al., 1988), systems (Anderson &
Matessa, 1991), assume clusters, Ck , described probabilistically:
variable value associated conditional probability, P (Ai = Vij jCk ), ects
proportion observations Ck exhibit value, Vij , along variable Ai . fact,
variable value actually associated number observations cluster
value; probabilities computed `on demand' purposes evaluation.
Probabilistically-described clusters arranged tree form hierarchical clustering
known probabilistic categorization tree. set sibling clusters partitions
observations covered common parent. single root cluster, identical
structure clusters, covering observations containing frequency information necessary compute P (Ai = Vij )'s required category utility. Figure 1 gives
example probablistic categorization tree (i.e., hierarchical clustering)
node cluster observations summarized probabilistically. Observations leaves
described three variables: Size, Color, Shape.

2.3 Hierarchical Sorting

strategy initial clustering sorting, term adapted psychological
task requires subjects perform roughly procedure describe
(Ahn & Medin, 1989). Given observation current partition, sorting evaluates
149

fiFisher

Size
Shape
Color

sma 0.50
squ 0.50
blu 0.25

med 0.25 lar 0.25
sph 0.50
gre 0.25 red 0.50

P(C1jroot)=0.50
sma 1.00
squ 1.00
blu 0.50 gre 0.50

sma 1.00
squ 1.00
blu 1.00
P(C3jC1)=0.50

P(root)=1.0

P(C2jroot)=0.50
med 0.50 lar 0.50
sph 1.00
red 1.00

sma 1.00
squ 1.00
gre 1.00
P(C4jC1)=0.50

med 1.00
sph 1.00
red 1.00
P(C5jC2)=0.50

lar 1.00
sph 1.00
red 1.00
P(C6jC2)=0.50

Figure 1: probabilistic categorization tree.
quality new clusterings result placing observation existing
clusters, quality clustering results creating new cluster
covers new observation; option yields highest quality score (e.g., using PU )
selected. clustering grows incrementally new observations added.
procedure easily incorporated recursive loop builds tree-structured
clusterings: given existing hierarchical clustering, observation sorted relative
top-level partition (i.e., children root); existing child root chosen
include observation, observation sorted relative children node,
serves root recursive call. leaf reached, tree extended
downward. maximum height tree bounded, thus limiting downward
growth fixed depth. Figure 2 shows tree Figure 1 two new observations
added it: one observation extends left subtree downward, second
made new leaf deepest, existing level right subtree.
sorting strategy identical used Anderson Matessa (1991). children cluster partition observations covered parent, though
measure, PU , used guide sorting differs Anderson Matessa. observations stored singleton clusters leaves tree. hierarchical-sort
based strategies augment basic procedure manner described Section 3.3 (Fisher,
1987a; Hadzikadic & Yun, 1989; Decaestecker, 1991).

150

fiOptimization Hierarchical Clusterings

Size
Shape
Color
P(C1jroot)=0.50
sma 1.00
squ 0.67
blu 0.33 gre 0.67

sma 1.00
sma 1.00
sqr 0.50
squ 1.00
gre 1.00
blu 1.00
P(C3jC1)=0.33

sma 0.50
squ 0.33
blu 0.17

med 0.33
sph 0.33
gre 0.33

lar 0.17
pyr 0.33
red 0.50

P(root)=1.0

P(C2jroot)=0.50
med 0.67 lar 0.33
sph 0.67 pyr 0.33
red 1.00

pyr 0.33

P(C4jC1)=0.67
med 1.00
lar 1.00
med 1.00
pyr 0.50
pyr 1.00
sph 1.00
sph 1.00
red 1.00
red 1.00
red 1.00
P(C5jC2)=0.33 P(C7jC2)=0.33 P(C6jC2)=0.33

sma 1.00
squ 1.00
gre 1.00
P(C7|C4)=0.50

New Object

sma 1.00
pyr 1.00
gre 1.00

New Object

P(C8|C4)=0.50

Figure 2: updated probabilistic categorization tree.

3. Iterative Optimization
Hierarchical sorting quickly constructs tree-structured clustering, one typically nonoptimal. particular, control strategy suffers ordering effects: different
orderings observations may yield different clusterings (Fisher, Xu, & Zard, 1992).
Thus, initial clustering phase, (possibly oine) process iterative optimization
seeks uncover better clusterings.

3.1 Seed Selection, Reordering, Reclustering
Michalski Stepp's (1983a) Cluster/2 seeks optimal K-partitioning data.
first step selects K random `seed' observations data. seeds `attractors'
around K clusters grown remaining data. Since seed selection
greatly impact clustering quality, Cluster/2 selects K new seeds `centroids'
K initial clusters. Clustering repeated new seeds. process iterates
improvement quality generated clusterings.
151

fiFisher

Function ORDER(Root)
Root leaf Return(observations covered Root)
Else Order children Root covering
observations covering least.
child, Ck , Root (in order) Lk ORDER(Ck )
L MERGE(fLk jlist objects constructed ORDER(Ck )g)
Return(L)
Table 1: procedure creating `dissimilarity' ordering data.
Ordering effects sorting related effects arise due differing fixed-K seed
selections: initial observations ordering establish initial clusters `attract'
remaining observations. general, sorting performs better initial observations
diverse areas observation-description space, since facilitates establishment initial clusters ect different areas. Fisher, Xu, Zard (1992) showed
ordering data consecutive observations dissimilar based Euclidean distance led good clusterings. Biswas et al. (1994) adapted technique Iterate
system similar results. cases, sorting used PU score described previously.
procedure presumes observations appear dissimilar Euclidean distance
tend placed different clusters using objective function. Taking lead
Cluster/2, measure-independent idea first sorts using random data ordering,
extracts biased `dissimilarity' ordering hierarchical clustering, sorts again.
function Table 1 outlines reordering procedure. recursively extracts list
observations probable (i.e., largest) cluster least probable,
merges (i.e., interleaves) lists, exiting recursive call { step,
element probable cluster placed first, followed element second
probable, forth. Whatever measure guides clustering, observations differing clusters judged dissimilar measure. Thus, measure-independent
procedure returns measure-dependent dissimilarity ordering placing observations
different clusters back-to-back.
Following initial sorting, extract dissimilarity ordering, recluster, iterate,
improvement clustering quality.

3.2 Iterative Redistribution Single Observations
common long-known form iterative optimization moves single observations
cluster cluster search better clustering (Duda & Hart, 1973). basic strategy
used one form another numerous sort-based algorithms well (Fisher
et al., 1992). idea behind iterative redistribution (Biswas, Weinberg, Yang, & Koller,
1991) simple: observations single-level clustering `removed' original
cluster resorted relative clustering. cluster contains one observation,
cluster `removed' single observation resorted. process continues
two consecutive iterations yield clustering.
152

fiOptimization Hierarchical Clusterings



HHH
HH



HH


H






#
c
c
#
,
,
@@
cc
cc

##
,

,
##








,@

J
@
@@


@

,

@

JJ
J









J

J


B0

D0

J

K


TJ JJ



....

F

....

C0

B0

C

E
L



D0

G H
....

E

K L
....

F

....

G H
....

J

J

Figure 3: Hierarchical redistribution: left subfigure indicates cluster J
removed descendent B , thus producing D0 B 0 ,
resorted relative children root (A). rightmost figure
shows J placed new child C . Fisher (1995). Figure reproduced permission Proceedings First International Conference
c 1995 American Association
Knowledge Discovery Data Mining, Copyright
Artificial Intelligence.
Isodata algorithm (Duda & Hart, 1973) determines target cluster observation, actually change clustering targets observations
determined; point, observations moved targets, thus altering
clustering. limit sequential version, also described Duda Hart
(1973), moves observation target identified sorting.
strategy conceptually simple, limited ability overcome local
maxima { reclassification particular observation may true direction
better clustering, may perceived objective function applied
clustering results resorting single observation.

3.3 Iterative Hierarchical Redistribution

iterative optimization strategy appears novel clustering literature iterative
hierarchical redistribution. strategy rationalized relative single-observation iterative redistribution: even though moving set observations one cluster another
may lead better clustering, movement single observation may initially reduce
clustering quality, thus preventing eventual discovery better clustering. response, hierarchical redistribution considers movement observation sets, represented
existing clusters hierarchical clustering.
Given existing hierarchical clustering, recursive loop examines sibling clusters
hierarchy depth-first fashion. set siblings, inner, iterative loop examines
sibling, removes current place hierarchy (along subtree),
resorts cluster relative entire hierarchy. Removal requires various
153

fiFisher

counts ancestor clusters decremented. Sorting removed cluster done based
cluster's probabilistic description, requires minor generalization procedure
sorting individual observations: rather incrementing certain variable value counts
1 cluster ect addition new observation, `host' cluster's variable
value counts incremented corresponding counts cluster classified.
cluster may return original place hierarchy, Figure 3 illustrates, may
sorted entirely different location.
inner loop reclassifies sibling set, repeats two consecutive iterations lead set siblings. recursive loop turns attention
children remaining siblings. Eventually, individual observations represented leaves resorted (relative entire hierarchy) changes
one iteration next. Finally, recursive loop may applied hierarchy
several times, thus defining outermost (iterative) loop terminates changes
occur one pass next.
one modification basic strategy implemented reasons cost:
change subtree pass outermost loop hierarchy,
subsequent passes attempt redistribute clusters subtree unless
cluster (from location hierarchy) placed subtree, thus
changing subtree's structure. addition, cases PU scores obtained
placing cluster, C (typically singleton cluster), either two hosts
same. cases, algorithm prefers placement C original host one
candidates high PU score. policy avoids infinite loops stemming
ties PU score.
sum, hierarchical redistribution takes large steps search better clustering. Similar macro-operator learners (Iba, 1989) problem-solving contexts, moving
observation set cluster bridges distant points clustering space, desirable
change made would otherwise viewed desirable redistribution limited movement individual observations. redistribution increasingly
smaller, granular clusters (terminating individual observations) serves increasingly refine clustering.
large extent hierarchical redistribution inspired Fisher's (1987a) Cobweb
system, fundamentally hierarchical-sort-based strategy. However, Cobweb
augmented operators merging, splitting, promotion. Merging combines two sibling
clusters hierarchical clustering increases quality partition
clusters members; splitting remove cluster promote children
next higher partition; distinct promotion operator promote individual cluster
next higher level. fact, could regarded `iterative optimization' operators,
keeping Cobweb's cognitive modeling motivations, cost applying
`amortized' time: many observations sorted, cluster may migrate one part
hierarchical clustering another collective repeated application
merging, splitting, promotion. similar view expressed McKusick Langley
(1991), whose Arachne system differs Cobweb, part, way exploits
promotion operator. Unfortunately, Cobweb, lesser extent Arachne,
merging, splitting, promotion applied locally migration hierarchy
limited practice. contrast, hierarchical redistribution resorts cluster, regardless
154

fiOptimization Hierarchical Clusterings

initial location tree, root entire tree, thus vigorously
pursuing migration globally evaluating merits moves.1
idea hierarchical redistribution also closely related strategies found
Bridger (Reich & Fenves, 1991) Hierarch (Nevins, 1995) systems. particular,
Bridger identifies `misplaced' clusters hierarchical clustering using criterion specified,
part, domain expert, whereas hierarchical redistribution simply uses objective
function. Bridger misplaced cluster removed (together subtree),
cluster/subtree resorted single unit; rather, observations covered
cluster resorted individually. approach captures, part, idea hierarchical
redistribution, though resorting individual observations may escape local optima
extent hierarchical redistribution.
Given existing hierarchical clustering new observation, Hierarch conducts
branch-and-bound search clustering, looking cluster `best matches'
observation. best host found, clusters `vicinity' best host
reclassified using branch-and-bound respect entire hierarchy. clusters
need singletons, reclassification spawn reclassifications
termination condition reached.
unclear Hierarch's procedure scales large data sets; number experimental trials size test data sets considerably less describe shortly.
Nonetheless, importance bridging distant regions clustering space reclassifying observation sets en masse made explicit. Like Cobweb, Hierarch incremental,
changes hierarchy triggered along path classifies new observation,
changes may move many observations simultaneously, thus `amortizing' cost
optimization time. contrast, hierarchical redistribution motivated philosophy sorting (or method) produce tentative clustering
data quickly, followed iterative optimization procedures background revise
clustering intermittently. hierarchical redistribution ects many ideas
implemented Hierarch, Cobweb, related systems, appears novel iterative
optimization strategy decoupled particular initial clustering strategy.

3.4 Comparisons Iterative Optimization Strategies
section compares iterative optimization strategies two experimental conditions.
first condition, random ordering observations generated hierarchically
sorted. optimization strategies applied independently resultant
hierarchical clustering. experiments assume primary goal clustering
discover single-level partitioning data optimal quality. Thus, objective
function score first-level partition taken important dependent variable.
independent variable height initially-constructed clustering; effect
granularity clusters used hierarchical redistribution. hierarchical clus1. Considering global changes also motivated redistribution individual observations Iterate.
Nevins (1995) notes commentary experimental comparisons Iterate Cobweb
(Fisher et al., 1992), even global movement single observations typically perform well
local movement sets observations simultaneously, implemented Cobweb's merging
splitting operators.

155

fiFisher

sort
Soybean (small)
reorder/resort
(47 obs, 36 vars) iter. redist.
hier. redist.
sort
Soybean (large)
reorder/resort
(307 obs, 36 vars) iter. redist.
hier. redist.
sort
House
reorder/resort
(435 obs, 17 vars) iter. redist.
hier. redist.
sort
Mushroom
reorder/resort
(1000 obs, 23 vars) iter. redist.
hier. redist.

Random
1.53 (0.11)
1.61 (0.02)
1.54 (0.10)
1.60 (0.05)
0.89 (0.08)
0.97 (0.04)
0.92 (0.07)
1.06 (0.02)
1.22 (0.30)
1.66 (0.09)
1.24 (0.28)
1.68 (0.00)
1.10 (0.13)
1.10 (0.08)
1.10 (0.12)
1.27 (0.00)

Similarity
1.08 (0.18)
1.56 (0.08)
1.34 (0.20)
1.50 (0.08)
0.66 (0.14)
0.96 (0.05)
0.84 (0.10)
1.06 (0.01)
0.83 (0.16)
1.57 (0.18)
1.06 (0.19)
1.68 (0.00)
0.73 (0.22)
1.16 (0.08)
0.95 (0.19)
1.24 (0.10)

Table 2: Iterative optimization strategies initial clusterings generated sorting random similarity ordered observations. Tree height 2. Averages standard
deviations PU scores 20 trials.
tering height 2 corresponds single level partition data depth 1 (the root
depth 0), leaves corresponding individual observations depth 2.
addition experiments clusterings derived sorting random initial orderings,
redistribution strategy tested exceptionally poor initial clusterings generated
nonrandom orderings. `dissimilarity' orderings lead good clusterings, `similarity' orderings lead poor clusterings (Fisher et al., 1992). Intuitively, similarity ordering
samples observations within region data description space sampling
observations differing regions. reordering procedure Section 3.1 easily modified produce similarity orderings ranking set siblings hierarchical clustering
least probable, appending rather interleaving observation lists
differing clusters algorithm pops recursive levels. similarity ordering
produced applying procedure initial clustering produced earlier sort
random ordering. Another clustering produced sorting similarity-ordered
data, three iterative optimization strategies applied independently.
advocate one build clusterings similarity orderings practice, experiments
orderings better test robustness various optimization strategies.
Table 2 shows results experiments random similarity orderings data
four databases UCI repository.2 results assume initial clustering
height 2 (i.e., top-level partition + observations leaves). cell represents average
2. reduced
data set.

mushroom

data set obtained randomly selecting 1000 observations original

156

fiOptimization Hierarchical Clusterings

standard deviation 20 trials. first cell (labeled `sort') domain
mean PU scores initially obtained sorting. Subsequent rows domain ect
mean scores obtained reordering/resorting procedure Section 3.1, iterative
redistribution single observations described Section 3.2, hierarchical redistribution
described Section 3.3.
main findings ected Table 2 are:
1. Initial hierarchical sorting random input reasonably well; PU scores
case closer scores optimized trees, poorest scores obtained
sorting similarity orderings. weakly suggests initial sorting
random input takes substantial step space clusterings towards discovery
final structure.
2. Hierarchical redistribution achieves highest mean PU score random
similarity case 3 4 domains. small soybean domain exception.
3. House domain (random similarity case) Mushroom domain (random
case only), standard deviation PU scores clusterings optimized hierarchical
redistribution 0.00, indicating always constructed level-1 partitions
PU score 20 trials.
4. Reordering reclustering comes closest hierarchical redistribution's performance
cases, bettering Small Soybean domain.
5. Single-observation redistribution modestly improves initial sort, substantially worse two optimization methods.
Note initial hierarchical clusterings height 2, difference
iterative hierarchical redistribution redistribution single observations hierarchical redistribution considers `merging' clusters partition (by reclassifying one
respect others) prior redistributing single observations pass
hierarchy.
Section 3.3 suggested expected benefits hierarchical redistribution might
greater deeper initial trees granular clusters. Table 3 shows results
domains initial orderings tree height 4 hierarchical redistribution; reader's convenience also repeat results Table 2 hierarchical
redistribution tree height 2. moving height 2 4, modest improvement small Soybean domain (particularly Similarity orderings),
slight improvement large Soybean domain Mushroom domain Similarity orderings.3 improvements modest, moving height 4 trees leads
near identical performance random similarity ordering conditions. suggests
hierarchical redistribution able effectively overcome disadvantage initially
poor clusterings.
Experiments reorder/resort iterative distribution single observations also
varied respect tree height (e.g., height 3). methods,
3. standard deviation 0:00 indicates standard deviation non-0, observable
2nd decimal place rounding.

157

fiFisher

Random
height 2
height 4
Soybean (small) 1.60 (0.05) 1.62 (0.00)
Soybean (large) 1.06 (0.02) 1.07 (0.02)
House
1.68 (0.00) 1.68 (0:00)
Mushroom
1.27 (0.00) 1.27 (0.00)

Similarity
height 2
height 4
1.50 (0.08) 1.62 (0.00)
1.06 (0.01) 1.07 (0.01)
1.68 (0.00) 1.68 (0:00)
1.24 (0.10) 1.27 (0.00)

Table 3: Hierarchical redistribution initial clusterings generated sorting random
similarity ordered observations. Results shown tree heights 2 (copied
Table 2) 4. Averages standard deviations PU scores 20 trials.
deepest set clusters initial hierarchy leaves, taken initial
partition. Reordering/resorting scores remained roughly height 2 condition, clusterings produced single-observation redistribution PU scores
considerably worse given Table 2.
also recorded execution time method. Table 4 shows time required
method seconds.4 particular, domain, Table 4 lists mean time
initial sorting, mean additional time optimization method. Ironically,
experiments demonstrate even though hierarchical redistribution `bottoms-out'
single-observation form redistribution, former consistently faster latter
trees height 2 { reclassifying cluster simultaneously moves set observations,
would otherwise repeatedly evaluated redistribution individually
increased time stabilization.5
Table 4 assumes tree constructed initial sorting bounded height 2. Table 5
gives time requirements hierarchical sorting hierarchical redistribution
initial tree bounded height 4. tree gets deeper cost hierarchical
redistribution grows substantially, comparison performance height 2
4 trees Table 3 suggests, drastically diminishing returns terms partition
quality. Importantly, limited experiments trees height 2, 3, 4 indicate
cost hierarchical redistribution comparable cost reorder/resort greater tree
heights significantly less expensive single-observation redistribution. dicult
give cost analysis hierarchical redistribution (and methods matter),
since bounds loop iterations probably depend nature objective function.
Suce say number nodes subject hierarchical redistribution
tree covering n observations bounded 2n , 1; may n leaves
n , 1 internal nodes given internal node less 2 children.
iterative optimization occur background, real-time response important,
cluster quality paramount, probably worth applying hierarchical redis4. Routines implemented SUN Common Lisp, compiled, run SUN 3/60.
5. Similar timing results occur computational contexts well. Consider relation
insertion sort Shell sort. Shell sort's final `pass' table insertion sort limited
moving table elements consecutive table locations time. large eciency advantage
Shell Sort stems fact previous passes table moved elements large distances,
thus final pass, table nearly sorted.

158

fiOptimization Hierarchical Clusterings

sort
Soybean (small)
reorder/resort
(47 obs, 36 vars)
iter. redist.
hier. redist.
sort
Soybean (large)
reorder/resort
(307 obs, 36 vars) iter. redist.
hier. redist.
sort
House
reorder/resort
(435 obs, 17 vars) iter. redist.
hier. redist.
sort
Mushroom
reorder/resort
(1000 obs, 23 vars) iter. redist.
hier. redist.

Random
6.98 (1.43)
14.82 (2.60)
9.00 (5.94)
6.99 (1.28)
50.62 (6.11)
141.36 (46.99)
166.53 (55.53)
79.00 (19.23)
34.99 (7.55)
87.78 (23.94)
177.75 (94.53)
55.90 (11.92)
111.47 (19.19)
301.34 (100.56)
162.58 (85.20)
91.87 (29.50)

Similarity
7.21 (1.31)
18.27 (6.00)
15.51 (7.72)
8.87 (3.58)
54.09 (13.25)
153.22 (43.59)
307.59 (160.66)
87.27 (19.64)
39.15 (7.60)
97.63 (29.54)
320.43 (124.78)
73.54 (10.05)
119.33 (25.86)
391.80 (211.54)
390.11 (191.62)
151.45 (48.89)

Table 4: Time requirements (in seconds) hierarchical sorting iterative optimization
initial clusterings generated sorting random similarity ordered observations. Tree height 2. Averages standard deviations 20 trials.
tribution deeper trees; consistent philosophy behind systems
Autoclass Snob. domains examined here, however, seem cost
effective optimize trees height greater 4. Thus, adopt tree construction strategy builds hierarchical clustering three levels time (with hierarchical
redistribution) experiments Section 4.

3.5 Discussion Iterative Optimization Methods
experiments demonstrate relative abilities three iterative optimization strategies,
coupled PU objective function hierarchical sorting generate
initial clusterings. reorder/resort optimization strategy Section 3.1 makes
sense sorting primary clustering strategy, optimization techniques
strongly tied particular initial clustering strategy. example, hierarchical
redistribution also applied hierarchical clusterings generated agglomerative
strategy (Duda & Hart, 1973; Everitt, 1981; Fisher et al., 1992), uses bottom-up
procedure construct hierarchical clusterings repeatedly `merging' observations
resulting clusters all-inclusive root cluster generated. Agglomerative methods
suffer ordering effects, greedy algorithms, susceptible
limitations local decision making generally, would thus likely benefit iterative
optimization.
159

fiFisher

Soy (small) sort
hier.
Soy (large) sort
hier.
House
sort
hier.
Mushroom sort
hier.

redist.
redist.
redist.
redist.

Random
height 2
height 4
6.98 (1.4)
18 (2)
6.99 (1.3)
94 (28)
50.62 (6.1) 142 (10)
79.00 (19.2) 436 (139)
34.99 (7.6) 104 (9)
55.90 (11.9) 355 (71)
111.47 (19.2) 407 (64)
91.87 (29.5) 1288 (458)

Similarity
height 2
height 4
7.21 (1.3)
21 (2)
8.87 (3.6)
133 (28)
54.09 (13.3) 152 (11)
87.27 (19.6) 576 (260)
39.15 (7.6) 120 (12)
73.54 (10.1) 425 (105)
119.33 (25.9) 443 (65)
151.45 (48.9) 1368 (335)

Table 5: Time requirements (in seconds) hierarchical sorting hierarchical redistribution initial clusterings generated sorting random similarity ordered
observations. Results shown tree heights 2 (copied Table 4) 4.
Averages standard deviations 20 trials.
addition, three optimization strategies applied regardless objective function. Nonetheless, relative benefits methods undoubtedly varies objective function. example, PU function undesirable characteristic
may, particular circumstances, view two partitions close form
separated `cliff' (Fisher, 1987b; Fisher et al., 1992). Consider partition
observations involving
two, roughly equal-sized clusters; PU score form
P
2
PU (fC1; C2g) = [ k=1 CU (Ck )]=2. create partition three clusters removing single observation
from, say C2, creating new singleton cluster, C3
P
3
0
PU (fC1; C2; C3g) = [ k=1 CU (Ck )]=3. relatively large, CU (C3)
small score due term, P (C3) = 1=M (see Section 2.1). taking
average CU score clusters, difference PU (fC1; C2g) PU (fC1; C20 ; C3g)
may quite large, even though differ placement one observation. Thus,
limiting experiments PU function may exaggerate general advantage hierarchical redistribution relative two optimization methods. statement
simultaneously positive statement robustness hierarchical redistribution
face objective function cliffs, negative statement PU defining
discontinuities. Nonetheless, PU variants adopted systems fall
within Cobweb family (Gennari et al., 1989; McKusick & Thompson, 1990; Reich &
Fenves, 1991; Iba & Gennari, 1991; McKusick & Langley, 1991; Kilander, 1994; Ketterlin
et al., 1995; Biswas et al., 1994). Section 5.2 suggests alternative objective functions.
Beyond nonoptimality PU , findings taken best
strategies engineered particular clustering system.
could introduce forms randomization systematic variation three strategies. example, Michalski Stepp's seed-selection methodology inspires reordering/resorting, Michalski Stepp's approach selects `border' observations
selection `centroids' fails improve clustering quality one iteration next;
160

fiOptimization Hierarchical Clusterings

example kind systematic variations one might introduce pursuit
better clusterings. contrast, Autoclass may take large heuristically-guided `jumps'
away current clustering. approach might be, fact, somewhat less systematic
(but equally successful) variation macro-operator theme inspired hierarchical redistribution, similar Hierarch's approach well. Snob (Wallace & Dowe, 1994)
employs variety search operators, including operators similar Cobweb's merge
split (though without restrictions local application), random restart clustering process new seed observations, `redistribution' observations.6 fact,
user program Snob's search strategy using differing primitive search operators.
case, systems Cluster/2, Autoclass, Snob simply `give up'
fail improve clustering quality one iteration next.
Snob illustrates, one strategies might combined advantage.
additional example, Biswas et al. (1994) adapt Fisher, Xu, Zard's (1992) dissimilarity
ordering strategy preorder observations prior clustering. sorting using PU ,
Iterate system applies iterative redistribution single observations using category
match measure Fisher Langley (1990).
combination preordering iterative redistribution appears yield good results
Iterate. results reorder/resort suggest preordering primarily responsible quality benefits simple sort, relative contribution Iterate's
redistribution operator certain since differs respects redistribution technique described paper.7 However, use three different measures {
distance, PU , category match { clustering may unnecessary adds undesirable coupling design clustering algorithm. If, example, one wants
experiment merits differing objective functions, undesirable worry
`compatibility' function two measures. contrast, reordering/resorting generalizes Fisher et al.'s (1992) ordering strategy; generalization
iterative redistribution strategy describe assume auxiliary measures beyond objective function. fact, Fisher (1987a, 1987b), evaluation Iterate's clusterings
made using measures variable value predictability P (Ai = Vij jCk ), predictiveness
P (Ck jAi = Vij ), product. clear system need exploit several related,
albeit different measures generation evaluation clusterings; undoubtedly
single, carefully selected objective function used exclusively clustering.
Reordering/resorting iterative redistribution single observations could combined manner similar Iterate's exploitation certain specializations
procedures. results suggest reordering/resorting would put clustering good
`ballpark', iterative redistribution would subsequently make modest refinements.
combined strategies, sense conducted inverse `ablation' study,
evaluating individual strategies isolation. limited number domains explored
Section 3.4, however, appears dicult better hierarchical redistribution.
Finally, experiments applied various optimization techniques data
sorted. may desirable apply optimization procedures intermittent points
sorting. may improve quality final clusterings using reordering/resorting
6. Importantly, Snob (and Autoclass) assumes probabilistic assignment observations clusters.
7. Iterate uses measure redistribution (Fisher & Langley, 1990) probably smoothes `cliffs',
uses Isodata, non-sequential version redistribution.

161

fiFisher

redistribution single observations, well reduce overall cost constructing
final optimized clusterings using methods, including hierarchical redistribution,
already appears quite well quality dimension. fact, Hierarch
viewed performing something akin restricted form hierarchical redistribution
observation. probably extreme { iterative optimization performed
often, resultant cost outweigh savings gleaned maintaining relatively well
optimized trees throughout sorting process. Utgoff (1994) makes similar suggestion
intermittent restructuring decision trees incremental, supervised induction.

4. Simplifying Hierarchical Clusterings

hierarchical clustering grown arbitrary height. structure data,
ideally top layers clustering ect structure (and substructure one
descends hierarchy). However, lower levels clustering may ect meaningful
structure. result overfitting, one finds supervised induction well.
Inspired certain forms retrospective (or post-tree-construction) pruning decisiontree induction, use resampling identify `frontiers' hierarchical clustering
good candidates pruning. Following initial hierarchy construction iterative
optimization, simplification process final phase search space
hierarchical clusterings intended ease burden data analyst.

4.1 Identifying Variable Frontiers Resampling

Several authors (Fisher, 1987a; Cheeseman et al., 1988; Anderson & Matessa, 1991) motivate clustering means improving performance task akin pattern completion,
error rate completed patterns used `externally' judge utility
clustering. Given probablistic categorization tree type assumed, new
observation unknown value variable classified hierarchy using
small variation hierarchical sorting procedure described earlier.8 Classification
terminated selected node (cluster) along classification path, variable value
highest probability cluster predicted unknown variable value new
observation. Naively, classification might always terminate leaf (i.e., observation),
leaf's value along specified variable would predicted variable value
new observation. use simple resampling strategy known holdout (Weiss
& Kulikowski, 1991) motivated fact variable might better predicted
internal node classification path. identification ideal-prediction frontiers
variable suggests pruning strategy hierarchical clusterings.
Given hierarchical clustering validation set observations, validation set
used identify appropriate frontier clusters prediction variable. Figure 4
illustrates preferred frontiers two variables may differ, clusters within
frontier may different depths. variable, Ai , objects validation
set classified hierarchical clustering value variable Ai
`masked' purposes classification; cluster encountered classification
8. Classification identical sorting except observation added clustering
statistics node encountered sorting permanently updated ect new
observation.

162

fiOptimization Hierarchical Clusterings

frontier A1
A2
A3
....

....

Figure 4: Frontiers three variables hypothetical clustering. Fisher (1995).
Figure reproduced permission Proceedings First International
c 1995 American
Conference Knowledge Discovery Data Mining, Copyright
Association Artificial Intelligence.
observation's value Ai compared probable value Ai cluster;
same, observation's value would correctly predicted
cluster. count correct predictions variable cluster
maintained. Following classification variables observations validation
set, preferred frontier variable identified maximizes number correct
counts variable. simple, bottom-up procedure insures number
correct counts node variable's frontier greater equal sum
correct counts variable set mutually-exclusive, collectively-exhaustive
descendents node.
Variable-specific frontiers enable number pruning strategies. example, node
lies frontier every variable offers apparent advantage terms patterncompletion error rate; node probably ects meaningful structure (and
descendents) may pruned. However, analyst focusing attention subset
variables, frontiers might exibly exploited pruning.

4.2 Experiments Validation
test validation procedure's promise simplifying hierarchical clusterings,
data sets used optimization experiments Section 3.4 randomly divided
three subsets: 40% training, 40% validation, 20% test. hierarchical
clustering first constructed sorting training set randomized order. hierarchy optimized using iterative hierarchical redistribution. Actually, cost
considerations, hierarchy constructed several levels time. hierarchy initially
constructed height 4, deepest level set training observations.
hierarchy optimized using hierarchical redistribution. Clusters bottommost level
(i.e., 4) removed children level 3 clusters, subset training observations
163

fiFisher

Soybean (small)
Leaves
Accuracy
Ave. Frontier Size
Soybean (large)
Leaves
Accuracy
Ave. Frontier Size
House
Leaves
Accuracy
Ave. Frontier Size
Mushroom
Leaves
Accuracy
Ave. Frontier Size

Unvalidated

Validated

18.00 (0.00)
0.85 (0.01)
18.00 (0.00)

13.10 (1.59)
0.85 (0.01)
2.75 (1.17)

122.00 (0.00) 79.10 (5.80)
0.83 (0.02) 0.83 (0.02)
122.00 (0.00) 17.01 (4.75)
174.00 (0.00) 49.10 (7.18)
0.76 (0.02) 0.81 (0.01)
174.00 (0.00) 9.90 (5.16)
400.00 (0.00) 96.30 (11.79)
0.80 (0.01) 0.82 (0.01)
400.00 (0.00) 11.07 (4.28)

Table 6: Characteristics optimized clusterings validation. Average
standard deviations 20 trials.
covered cluster level 3 hierarchically sorted height 4 tree optimized.
roots subordinate clusterings substituted cluster depth
3 original tree. process repeated clusters level 3 subordinate
trees subsequent trees thereafter decomposition possible. final
hierarchy, constant-bounded height, decomposes entire training set
singleton clusters, containing single training observation. validation set
used identify variable frontiers within entire hierarchy.
testing validated clustering, variable test observation masked
turn; classification reaches cluster frontier masked variable,
probable value predicted value observation; proportion correct
predictions variable test set recorded. comparative purposes,
also use test set evaluate predictions stemming unvalidated tree,
variable predictions made leaves (singleton clusters) tree.
Table 6 shows results 20 experimental trials using optimized, unvalidated
validated clusterings generated described random orderings. first row
domain lists average number leaves (over 20 experimental trials)
unvalidated validated trees. unvalidated clusterings decompose training data
single-observation leaves { number leaves equals number training observations.
validated clustering, assume clusters pruned lie frontiers
variables. Thus, leaf validated clustering cluster (in original clustering)
frontier least one variable, none descendent clusters (in
original clustering) frontier variable. example, assume
164

fiOptimization Hierarchical Clusterings

tree Figure 4 covers data described terms variables A1 , A2, A3,
number leaves validated clustering would 7.
Prediction accuracies second row domain entry mean proportion
correct predictions variables 20 trials. Predictions generated leaves
(singleton clusters) unvalidated hierarchical clusterings appropriate variable
frontiers validated clusterings. cases, validation/pruning substantially reduces
clustering size diminish accuracy.
number leaves validated case, described it, assumes
coarse pruning strategy; necessarily discriminate clustering uniformly
deep frontiers one single deep frontiers. suggested
exible pruning `attention' strategies might possible analyst focusing
one variables. specify strategies, statistic given row
3 domain entry suggests clusterings rendered considerably simpler
forms analyst's attention selective. Row 3 average number frontier
clusters per variable. average variables experimental trials.9
validated tree Figure 4 average frontier size (1 + 4 + 6)=3 = 3:67.
Intuitively, frontier cluster variable `leaf' far prediction variable
concerned. `frontier size' unvalidated clusterings simply given number
leaves, since variable predictions made unvalidated case.
results suggest attention selective, partial clustering captures
structure involving selected variables presented analyst simplified form.

4.3 Discussion Validation

resampling-based validation method inspired earlier work Fisher (1989),
identified variable frontiers within strict incremental (i.e., sorting) context { separate
validation set reserved, rather training set used identifying variable
frontiers well. particular, training observation hierarchically sorted using
Cobweb, observation's variable values predicted `correct' counts
node updated correctly anticipated variables. Fisher (1989) variable
values masked sorting { knowledge variable value used
sorting, thus helping guide classification, validation. addition, hierarchy
changed sorting/validation. incremental strategy led desirable results
terms pattern-completion error rate, likely variable frontiers identified
incremental method less desirable frontiers identified holdout,
strictly segregate training validation sets observations. addition Fisher
(1989), work variable frontiers traced back ideas Lebowitz (1982)
Kolodner (1983), directly Fisher (1987b), Fisher Schlimmer (1988),
Reich Fenves (1991), use different method identify something
similar spirit frontiers defined here.
method validation pruning inspired retrospective pruning strategies
decision tree induction reduced error pruning (Quinlan, 1987, 1993; Mingers,
1989a). Bayesian clustering system Autoclass (Cheeseman et al., 1988),
9. `standard deviations' given Row 3 actually mean standard deviations
frontier sizes individual variables.

165

fiFisher

minimum message length (MML) approach adopted Snob (Wallace & Dowe, 1994),
expansion hierarchical clustering mediated tradeoff prior belief
existence structure evidence data structure.
detail fundamental tradeoff, suce say expansion hierarchical
clustering cease along path evidence structure data
insucient face prior bias. Undoubtedly, Bayesian MML approaches
adapted identify variable-specific frontiers, thus used kind exible
pruning focusing strategies implied. fact, something similar
intent implemented Autoclass (Hanson, Stutz, & Cheeseman, 1991) way
reducing cost clustering system: variables covary may `blocked',
sense treated one. version Autoclass searches space hierarchical
clusterings, blocks variables assigned particular clusters hierarchy.
interpretation assignments cluster `inherits' variable value distributions
variable blocks assigned cluster's ancestors. Inversely, basic idea one
need proceed cluster determine value distributions variables assigned
cluster.
experimental results suggest utility resampling validation, identification variable frontiers, pruning. However, procedure described method
per se clustering available data, since requires validation set held
initial hierarchy construction.10 several options seem worthy
experimental evaluation adapting validation strategy tool simplification
hierarchical clusterings. One strategy would hold validation set, cluster
training set, identify variable frontiers validation set, sort validation
set relative clustering. single holdout methodology problems, however,
reasons similar identified single holdout supervised settings (Weiss &
Kulikowski, 1991).
better strategy might one akin n-fold-cross-validation: hierarchical clustering
constructed available data, observation removed,11 used
validation respect variable, observation reinstated original
location (together original variable value statistics clusters along path
location).

5. General Discussion
evaluation various strategies discussed paper ect two paradigms
validating clusterings. Internal validation concerned evaluating merits
control strategy searches space clusterings: evaluating extent search
strategy uncovers clusterings high quality measured objective function. Internal
validation focus Section 3.4. External validation concerned determining
utility discovered clustering relative performance task. noted
10. purposes evaluating merits validation strategy terms error rate, also held
separate test set. demonstrated point, however, would require separate test
set held using resampling validation strategy.
11. observation physically removed, variable value statistics clusters lie along path
root observation decremented.

166

fiOptimization Hierarchical Clusterings

Soybean (small)
Leaves
Accuracy
Ave. Frontier Size
Soybean (large)
Leaves
Accuracy
Ave. Frontier Size
House
Leaves
Accuracy
Ave. Frontier Size
Mushroom
Leaves
Accuracy
Ave. Frontier Size

Unoptimized
Unvalidated Validated

Optimized
Unvalidated Validated

18.00 (0.00)
0.84 (0.18)
18.00 (0.00)

18.00 (0.00)
0.85 (0.01)
18.00 (0.00)

15.35 (1.81)
0.85 (0.01)
3.97 (1.62)

13.10 (1.59)
0.85 (0.01)
2.75 (1.17)

122.00 (0.00) 88.55 (4.46)
0.82 (0.02) 0.82 (0.02)
122.00 (0.00) 24.74 (7.52)

122.00 (0.00) 79.10 (5.80)
0.83 (0.02) 0.83 (0.02)
122.00 (0.00) 17.01 (4.75)

174.00 (0.00) 68.95 (8.15)
0.76 (0.02) 0.81 (0.02)
174.00 (0.00) 17.72 (7.81)

174.00 (0.00) 49.10 (7.18)
0.76 (0.02) 0.81 (0.01)
174.00 (0.00) 9.90 (5.16)

400.00 (0.00) 145.50 (20.64) 400.00 (0.00) 96.30 (11.79)
0.80 (0.01) 0.82 (0.01)
0.80 (0.01) 0.82 (0.01)
400.00 (0.00) 22.85 (8.75)
400.00 (0.00) 11.07 (4.28)

Table 7: Characteristics unoptimized optimized clusterings validation. Average standard deviations 20 trials.
several authors point minimization error rate pattern completion generic
performance task motivates choice objective function. External validation
focus Section 4.2.
section explores validation issues closely, identifies error rate simplicity (or `cost') necessary external criteria discriminating clustering utility, suggests
number alternative objective functions might usefully compared using
criteria, speculates external validation criteria (taken collectively) ect
reasonable criteria data analysts may use judge utility clusterings.

5.1 Closer Look External Validation Criteria
Ideally, clustering quality measured objective function well correlated
clustering utility determined performance task: higher quality
clustering judged objective function, greater performance improvement
(e.g., reduction error rate), lower quality, less performance improves.
However, several authors (Fisher et al., 1992; Nevins, 1995; Devaney & Ram, 1993)
pointed PU scores seem well-correlated error rates. precisely,
hierarchical clusterings (constructed hierarchical sorting) top-level partition
low PU score lead roughly error rates hierarchies top-level
partition high PU score, variable-value predictions made leaves (singleton
clusters). Apparently, even poor partitions level measured PU , test
167

fiFisher

observations classified similar observations leaves hierarchical
clustering. Pattern-completion error rate circumstances seems insucient
discriminate might otherwise consider good poor clusterings.
work simplification Section 4 suggests addition error rate, might
choose judge competing hierarchical clusterings based simplicity similarlyintended criterion. error rate simplicity used judge classifiers supervised
contexts. seen holdout used substantially `simplify' hierarchical
clustering. question ask whether hierarchical clusterings
optimized relative PU simplified substantially unoptimized clusterings
degradation pattern-completion error rate?
answer question repeated validation experiments Section 4.2
second experimental condition: hierarchical clusterings constructed similarity
orderings observations using hierarchical sorting. saw Section 3.4 similarity
orderings tend result clusterings judged poor PU function. optimize
hierarchies using hierarchical optimization. Table 7 shows accuracies, number
leaves, average frontier sizes, unoptimized hierarchies constructed similarity
orderings case subjected holdout-based validation
case not. results given heading `Unoptimized'.
convenience, copy results Table 6 heading `Optimized'.
optimized case, identifying exploiting variable frontiers unoptimized
clusterings appears simplify clustering substantially degradation error rate.
interest here, however, optimized clusterings simplified substantially
greater extent unoptimized clusterings degradation error rate.
Thus far, focused criteria error rate simplicity, many
applications, real interest simplicity stems broader interest minimizing
expected cost exploiting clustering classification: expect simpler
clusterings lower expected classification costs. view various distinctions
unvalidated/validated unoptimized/optimized clusterings terms expected
classification cost. Table 8 shows additional data obtained experiments
validation. particular, table shows:

Leaves (L) mean number leaves (over 20 trials) validation (assuming coarse pruning strategy described Section 4.2) optimized
unoptimized cases (copied Table 7).

EPL mean total path length (over 20 trials). total path length unvalidated

tree, leaf corresponds single observation, sum depths
leaves tree. case validated tree, leaf may cover multiple
observations, contribution leaf total path length depth
leaf times number observations leaf.

Depth (D) average depth leaf tree, computed EPL
L .

p
Breadth (B)log
average branching factor tree. Given B = L, B = L
L
B=m




m.

168

fiOptimization Hierarchical Clusterings

Soybean (small)
Leaves
EPL
Depth
Breadth
Cost
Soybean (large)
Leaves
EPL
Depth
Breadth
Cost
House
Leaves
EPL
Depth
Breadth
Cost
Mushroom
Leaves
EPL
Depth
Breadth
Cost

Unoptimized
Unvalidated
Validated

Optimized
Unvalidated
Validated

18.00 (0.00)
40.90 (3.64)
2.27
3.57
8.10

15.35 (1.81)
31.90 (6.94)
2.08
3.72
7.74

18.00 (0.00)
54.20 (4.74)
3.01
2.61
7.86

13.10 (1.59)
34.50 (6.49)
2.63
2.66
7.00

122.00 (0.00)
437.20 (34.74)
3.58
3.82
13.68

88.55 (4.46)
280.40 (28.07)
3.17
4.11
13.03

122.00 (0.00)
657.65 (28.38)
5.39
2.44
13.15

79.10 (5.80)
380.65 (43.63)
4.81
2.48
11.93

174.00 (0.00)
664.65 (41.16)
3.82
3.86
14.75

68.95 (8.15)
196.20 (35.32)
2.85
4.42
12.60

174.00 (0.00)
1005.10 (27.42)
5.78
2.44
14.10

49.10 (7.18)
217.25 (39.75)
4.42
2.41
10.65

400.00 (0.00)
2238.20 (123.63)
5.60
2.92
16.35

145.50 (20.64)
660.90 (117.86)
4.54
3.00
13.62

400.00 (0.00)
2608.85 (56.01)
6.52
2.51
16.37

96.30 (11.79)
503.40 (72.22)
5.23
2.39
12.50

Table 8: Cost characteristics unoptimized optimized clustering validation. Average standard deviations 20 trials. Characteristics
ed computed mean values `Leaves' EPL.

Cost (C) expected cost classifying observation root leaf terms

number nodes (clusters) examined classification. level
examine cluster select best. Thus, cost product number
levels number clusters per level. C = B D.
Table 8 illustrates expected cost classification less optimized clusterings
unoptimized clusterings unvalidated validated cases. However,
results taken grain salt, simply estimated
values. particular, expressed cost terms expected number nodes
would need examined classification. implicit assumption cost
examination constant across nodes. fact, cost per examination roughly constant
(per domain) across nodes implementation many others: node,
variables examined. Consider measure cost, least cost (unvalidated)
169

fiFisher

clustering one splits observations thirds node, thus forming balanced
ternary tree, regardless form structure data.
course, tree reasonably capture structure data, might
expect ected error rate and/or post-validation simplicity. Nonetheless,
probably better measures cost available. particular, Gennari (1989) observed
classifying observation, evaluating objective function proper subset
variables often sucient categorize observation relative cluster
would selected evaluation occurred variables. ideal
circumstances, clusters partition well separated (decoupled), testing
`critical' variables may sucient advance classification.
Gennari implemented focusing algorithm sequentially evaluated objective
function variables, one additional variable time least `critical',
categorization respect one clusters could made unambiguously.
Using Gennari's procedure, examination cost constant across nodes.12 Carnes
Fisher (Fisher, Xu, Carnes, Reich, Fenves, Chen, Shiavi, Biswas, & Weinberg, 1993) adapted
Gennari's procedure good effect diagnosis task, intent minimize
number probes necessary diagnose fault. Gennari offers principled
focusing strategy used conjunction objective function, general
idea focusing selected features classification traced back Unimem
(Lebowitz, 1982, 1987) Cyrus (Kolodner, 1983).
results Table 8 illustrate form expected classification-cost analysis,
might also measured cost time directly using test set. fact, comparisons
time requirements sorting random similarity ordering conditions
Tables 4 5 suggest cost differences good poor clusterings terms
time well. Regardless form analysis, however, seems desirable one
express branching factor cost terms number variables need tested
assuming focusing strategy Gennari's. likely tend make
better distinctions clusterings.

5.2 Evaluating Objective Functions: Getting Bang Buck

results Section 5.1 suggest PU function useful identifying structure
data: clusterings optimized relative function simpler accurate
clusterings optimized relative function. Thus, PU leads something
reasonable along error rate simplicity dimensions, objective functions
better job along dimensions? Based earlier discussion limitations
PU , notably averaging CU clusters partition introduced `cliffs'
space partitions, likely better objective functions found. example,
might consider Bayesian variants like found Autoclass (Cheeseman et al., 1988)
Anderson Matessa's (1991) system, closely related MML approach Snob
(Wallace & Dowe, 1994). evaluate alternative measures here,
suggest number candidates.
12. fact, cost constant across observations, even classified along exactly
path { number variables one need test depends observation's values along previously
examined variables.

170

fiOptimization Hierarchical Clusterings

Section 2.1 noted CU function could viewed summation Gini
Indices, measured collective impurity variables conditioned cluster membership. Intuition may helped information-theoretic analog CU (Corter
& Gluck, 1992):

P (Ck )

X X[P (A


j

= Vij jCk ) log2 P (Ai = Vij jCk ) , P (Ai = Vij ) log2 P (Ai = Vij )]:

information-theoretic analog understood summation information gain
values, information gain often used selection criterion decision tree induction
(Quinlan, 1986): clustering analog rewards clusters, Ck , maximize sum
information gains individual variables, Ai .
Gini Information Gain measures often-used bases selection measures decision tree induction. used measure expected decrease impurity
uncertainty class label, conditioned knowledge given variable's value.
clustering context, interested decrease impurity variable's value
conditioned knowledge cluster membership { thus, use summation suitable
Gini Indices alternatively, information gain scores. However, well known
context decision tree induction, measures biased select variables
legal values. Thus, various normalizations measures different measures
altogether, devised.
clustering adaptation measures normalizaP
N
tion also necessary, since k=1 CU alone information-theoretic analog favor
clustering greatest cardinality, data partitioned singleton clusters,
one observation. Thus, PU normalizes sum Gini indices averaging.
general observation many selection measures used decision tree induction
adapted objective functions clustering. number selection
measures suggest candidates clustering, normalization
principled averaging. Two candidates Quinlan's (1986) Gain Ratio
Lopez de Mantaras' (1991) normalized information gain.13

Pj P (Ai=Vij ) Pk [P (Ck jAi =Vij ) log P (Ck jAi =Vij ),P (Ck )log P (Ck )]
P
(Quinlan, 1986)
, j P (Ai =Vij ) log P (Ai =Vij )
Pj P (Ai=Vij ) Pk [P (Ck jAi =Vij ) log P (Ck jAi =Vij ),P (Ck )log P (Ck )]
PP
(Lopez de Mantaras, 1991)
, j k P (Ck ^Ai =Vij )log P (Ck ^Ai =Vij )
2

2

2

2

2

2

derive two objective functions clustering:

Pi Pk P (Ck ) Pj [P (Ai=Vij jCk )log
P P (Ai =Vij jCk ),P (Ai =Vij ) log P (Ai=Vij )]
2

, k P (Ck )log2 P (Ck )

2

Pi Pk P (Ck ) Pj [P (PAi=VPij jCk )log P (Ai =Vij jCk ),P (Ai =Vij ) log P (Ai=Vij )]
2

, k j P (Ai =Vij ^Ck )log2 P (Ai =Vij ^Ck )

2

13. Jan Hajek independently pointed relationship C U measure Gini Index,
made suggestions one might select one another normalizations above.

171

fiFisher

latter clustering variations defined Fisher Hapanyengwi (1993).
nonsystematic experimentation Lopez de Mantaras' normalized information gain variant suggests mitigates problems associated PU , though conclusions
merits must await experimentation. general, wealth promising
objective functions based decision tree selection measures might consider.
described two, others Fayyad's (1991) ORT function.
relationship supervised unsupervised measures also pointed
context Bayesian systems (Duda & Hart, 1973). Consider Autoclass (Cheeseman et al., 1988), searches probable clustering, H , given available
data set, { i.e., clustering highest P (H jD) / P (DjH )P (H ). independence assumptions made Autoclass, computation P (DjH ) includes easily
seen mechanisms simple Bayes classifier used supervised contexts.
compared proposed derivations decision tree selection measures
Bayesian/MML measures Autoclass Snob yet, proposed patterncompletion error rate, simplicity, classification cost external, objective criteria
could used comparisons. advantage Bayesian MML approaches
that, proper selection prior biases, require separate strategy (e.g.,
resampling) pruning, strategies adapted variable frontier identification. Rather, objective function used cluster formation serves cease hierarchical
decomposition well. Though know experimental studies Bayesian
MML techniques along accuracy cost dimensions outlined here, expect
would perform quite well.

5.3 Final Comments External Validation Criteria
proposal external validation criteria clustering error rate classification
cost stem larger, often implicit, long-standing bias AI learning
systems serve ends artificial autonomous agent. Certainly, Cobweb
family systems trace ancestry systems Unimem (Lebowitz, 1982)
Cyrus (Kolodner, 1983) autonomous agency primary theme,
Fisher (1987a); Anderson Matessa's (1991) work expresses similar concerns.
short, view clustering means organizing memory observations
autonomous agent begs question agent's tasks memory organization
intended support? Pattern completion error rate simplicity/cost seem obvious
candidate criteria.
However, underlying assumption article criteria also appropriate externally validating clusterings used data analysis contexts, clustering
external human analyst, nonetheless exploited analyst purposes
hypothesis generation. Traditional criteria cluster evaluation contexts include
measures intra-cluster cohesion (i.e., observations within clusters similar) inter-cluster coupling (i.e., observations differing clusters dissimilar).
criteria proposed article traditional criteria certainly related. Consider
following derivation portion category utility measure, begins
expected number variable values correctly predicted given prediction
guided clustering fC1; C2; :::; CN g:
172

fiOptimization Hierarchical Clusterings

E (# P
correct variable predictionsjfC1 ; C2; :::; CN g)
= Pk P (Ck )EP(# correct variable predictionsjCk )
= Pk P (Ck ) Pi E
P(# correct predictions variable Ai jCk)
= Pk P (Ck ) Pi Pj P (Ai = Vij jCk )E (# times Vij correct prediction Ai jCk )
= Pk P (Ck ) Pi Pj P (Ai = Vij jCk ) P (Ai = Vij jCk )
= k P (Ck ) j P (Ai = Vij jCk )2
final steps derivation assume variable value predicted probability
P (Ai = Vij jCk) probability prediction correct { i.e.,
derivation category utility assumes probability matching prediction strategy (Gluck &
Corter, 1985; Corter & Gluck, 1992).14 favoring partitions improve prediction along
many variables, hierarchical clustering using category utility tends result hierarchies
variable frontiers, described Section 4.1, near top clustering;
tends reduce post-validation classification cost.
Thus, category utility motivated measure rewards cohesion within
clusters decoupling across clusters noted Section 2.1, measure motivated
desire reduce error rate (and indirectly, classification cost). general, measures
motivated desire reduce error rate also favor cohesion decoupling; stems
two aspects pattern-completion task (Lebowitz, 1982; Medin, 1983). First,
assign observation cluster based known variable values observation,
best facilitated variable value predictiveness high across many variables (i.e.,
clusters decoupled).15 assigned observation cluster, use cluster's
definition predict values variables known observation's
description. process successful many variables predictable clusters
(i.e., clusters cohesive). fact, designing measures cohesion decoupling
mind undoubtedly results useful clusterings purposes pattern completion, whether
explicit goal designer.
external validation criteria error rate cost well correlated traditional
criteria cohesion coupling, use former criteria all? part,
stems AI machine learning bias systems designed evaluated
specific performance task mind. addition, however, plethora measures
assessing cohesion coupling found, system assessed relative
variant. variation make dicult assess similarities differences across
systems. article suggests pattern-completion error rate cost relatively unbiased
alternatives comparative studies. Inversely, use direct measures error
rate classification cost (e.g., using holdout) `objective function' guide search
space clusterings? expensive. Thus, use cheaply computed
objective function designed external error rate cost evaluation mind;
undoubtedly, objective function ects cohesion coupling.
14. Importantly, prediction Cobweb actually performed using probability maximizing strategy {
frequent value variable cluster always predicted. Fisher (1987b) discusses advantage
constructing clusters implicit probability matching strategy, even cases clusters
exploited probability maximizing strategy.
15. MML Bayesian approaches Snob Autoclass support probabilistic assignment observations clusters, importance decoupling cohesion remain.

173

fiFisher

course, computed error rate identified variable frontiers given simplified
performance task: variable independently masked predicted test observations. unreasonable generic method computing error rate, different
domains may suggest different computations, since often many variables simultaneously
unknown and/or analyst may interested subset variables. addition,
proposed simplicity (i.e., number leaves) expected classification cost
external validation criteria. Section 5.1 suggests one latter criteria probably
necessary, addition error rate, discriminate `good' `poor' clusterings judged
objective function. general, desirable realizations error rate, simplicity,
cost likely vary domain interpretation tasks analyst.
short, analyst's task largely one making inferences clustering,
error-rate cost components (i.e., information analyst glean
clustering much work required part analyst extract
information). probably case expressed components
precisely way cognitively-implemented analyst. Nonetheless,
article others (Fisher, 1987a; Cheeseman et al., 1988; Anderson & Matessa, 1991)
viewed attempts formally, tentatively describe analyst's criteria cluster
evaluation, based criteria might prescribe autonomous, artificial agent
confronted much task.

5.4 Issues
many important issues clustering address depth. One
possible advantage overlapping clusters (Lebowitz, 1987; Martin & Billman, 1994).
assumed tree-structured clusterings, store observation
one cluster, clusters related proper subset-of relation one descends
path tree. many cases, lattices (Levinson, 1984; Wilcox & Levinson, 1986;
Carpineto & Romano, 1993), generally, directed acyclic graphs (DAG) may
better representation scheme. structures allow observation included
multiple clusters, one cluster need subset another. such, may
better provide analyst multiple perspectives data. example, animals
partitioned clusters corresponding mammals, birds, reptiles, etc., may
partitioned clusters corresponding carnivores, herbivores, omnivores. tree would
require one partitions (e.g., carnivore, etc.) `subordinate' (e.g.,
mammals, birds, etc.); Classes subordinate partition would necessarily `distributed'
across descendents (e.g., carnivorous-mammal, omnivorous-mammal, carnivorous-reptile,
etc.) top level clusters, ideally would represent clusters partition.
DAG allows perspectives coexist relative equality, thus making perspectives
explicit analyst.
also assumed variables nominally valued. numerous
adaptations basic PU function, functions, discretization strategies accommodate numeric variables (Michalski & Stepp, 1983a, 1983b; Gennari et al., 1989; Reich
& Fenves, 1991; Cheeseman et al., 1988; Biswas et al., 1994). basic sorting procedure
iterative optimization techniques used data described whole part
numerically-valued variables regardless approach one takes. identification
174

fiOptimization Hierarchical Clusterings

numeric variable frontiers using holdout done using mean value variable node generating predictions, identifying variable's frontier set
clusters collectively minimize measure error mean-squared error.

6. Concluding Remarks

partitioned search space hierarchical clusterings three
phases. phases, together opinion desirable characteristics
data analysis standpoint, (1) inexpensive generation initial clustering suggests form structure data (or absence), (2) iterative optimization (perhaps
background) clusterings better quality, (3) retrospective simplification generated clusterings. evaluated three iterative optimization strategies operate
independent objective function. these, varying degrees, inspired previous
research, hierarchical redistribution appears novel iterative optimization technique
clustering; also appears quite well.
Another novel aspect work use resampling means validating clusters simplifying hierarchical clusterings. experiments Section 5 indicate
optimized clusterings provide greater data compression unoptimized clusterings.
surprising, given PU compresses data reasonable manner; whether
`optimally' though another issue.
made several recommendations research.
1. suggested experiments alternative objective functions, including Bayesian
MML measures, inspired variable-selection measures decision tree induction.
2. may cost quality benefits applying optimization strategies intermittent points hierarchical sorting.
3. holdout method identifying variable frontiers pruning suggests strategy
akin n-fold-cross validation clusters data, still identifying
variable frontiers facilitating pruning.
4. Analyses classification cost purposes external validation probably best
expressed terms expected number variables using focusing method
Gennari's.
sum, paper proposed criteria internal external validation,
made experimental comparisons various approaches along dimensions. Ideally, researchers explore objective functions, search control strategies, pruning
techniques, kind experimental comparisons (particularly along external criteria
error rate, simplicity, classification cost) de rigueur comparisons
supervised systems, become prominent unsupervised contexts.

175

fiFisher

Acknowledgements
thank Sashank Varma, Arthur Nevins, Diana Gordon comments paper.
reviewers editor supplied extensive helpful comments. work supported
grant NAG 2-834 NASA Ames Research Center. abbreviated discussion
article's results appear Fisher (1995), published AAAI Press.

References

Ahn, W., & Medin, D. L. (1989). two-stage categorization model family resemblance
sorting.. Proceedings Eleventh Annual Conference Cognitive Science
Society, pp. 315{322. Ann Arbor, MI: Lawrence Erlbaum.
Anderson, J. R., & Matessa, M. (1991). iterative Bayesian algorithm categorization.
Fisher, D., Pazzani, M., & Langley, P. (Eds.), Concept formation: Knowledge
Experience Unsupervised Learning. San Mateo, CA: Morgan Kaufmann.
Biswas, G., Weinberg, J., & Li, C. (1994). Iterate: conceptual clustering method
knowledge discovery databases. Braunschweig, B., & Day, R. (Eds.), Innovative
Applications Artificial Intelligence Oil Gas Industry. Editions Technip.
Biswas, G., Weinberg, J. B., Yang, Q., & Koller, G. R. (1991). Conceptual clustering
exploratory data analysis. Proceedings Eighth International Machine
Learning Workshop, pp. 591{595. San Mateo, CA: Morgan Kaufmann.
Carpineto, C., & Romano, G. (1993). Galois: order-theoretic approach conceptual
clustering. Proceedings Tenth International Conference Machine Learning,
pp. 33{40. Amherst, MA: Morgan Kaufmann.
Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. (1988). AutoClass:
Bayesian classification system. Proceedings Fifth International Machine
Learning Conference, pp. 54{64. Ann Arbor, MI: Morgan Kaufmann.
Corter, J., & Gluck, M. (1992). Explaining basic categories: feature predictability
information. Psychological Bulletin, 111, 291{303.
De Alte Da Veiga, F. (1994). Data Analysis Biomedical Research: Novel Methodological Approach Implementation Conceptual Clustering Algorithm (in
Portuguese). Ph.D. thesis, Universidade de Coimbra, Unidade de Biomatematica e
Informatica Medica da Faculdade de Medicina.
Decaestecker, C. (1991). Description contrasting incremental concept formation. Kodratoff, Y. (Ed.), Machine Learning { EWSL-91, No. 482, Lecture Notes Artificial
Intelligence, pp. 220{233. Springer-Verlag.
Devaney, M., & Ram, A. (1993). Personal communication, oct. 1993..
Duda, R. O., & Hart, P. E. (1973). Pattern Classification Scene Analysis. New York,
NY: Wiley Sons.
176

fiOptimization Hierarchical Clusterings

Everitt, B. (1981). Cluster Analysis. London: Heinemann.
Fayyad, U. (1991). Induction Decision Trees Multiple Concept Learning. Ph.D.
thesis, University Michigan, Ann Arbor, MI: Department Computer Science
Engineering.
Fisher, D. (1995). Optimization simplification hierarchical clusterings. Proceedings
First International Conference Knowledge Discovery Data Mining, pp.
118{123. Menlo Park, CA: AAAI Press.
Fisher, D., & Hapanyengwi, G. (1993). Database management analysis tools machine
induction. Journal Intelligent Information Systems, 2, 5{38.
Fisher, D., Xu, L., Carnes, J., Reich, Y., Fenves, S., Chen, J., Shiavi, R., Biswas, G., &
Weinberg, J. (1993). Applying AI clustering engineering tasks. IEEE Expert, 8,
51{60.
Fisher, D., Xu, L., & Zard, N. (1992). Ordering effects clustering. Proceedings
Ninth International Conference Machine Learning, pp. 163{168. San Mateo, CA:
Morgan Kaufmann.
Fisher, D. H. (1987a). Knowledge acquisition via incremental conceptual clustering. Machine Learning, 2, 139{172.
Fisher, D. H. (1987b). Knowledge Acquisition via Incremental Conceptual Clustering. Ph.D.
thesis, University California, Irvine, CA: Department Information Computer
Science.
Fisher, D. H. (1989). Noise-tolerant conceptual clustering. Proceedings International Joint Conference Artificial Intelligence, pp. 825{830. Detroit, MI: Morgan
Kaufmann.
Fisher, D. H., & Langley, P. (1990). structure formation natural categories.
Bower, G. H. (Ed.), Psychology Learning Motivation, Vol. 25. San Diego,
CA: Academic Press.
Fisher, D. H., & Schlimmer, J. (1988). Concept simplification prediction accuracy.
Proceedings Fifth International Conference Machine Learning, pp. 22{28.
Ann Arbor, MI: Morgan Kaufmann.
Gennari, J. (1989). Focused concept formation. Proceedings Sixth International
Workshop Machine Learning, pp. 379{382. San Mateo, CA: Morgan Kaufmann.
Gennari, J., Langley, P., & Fisher, D. (1989). Models incremental concept formation.
Artificial Intelligence, 40, 11{62.
Gluck, M. A., & Corter, J. E. (1985). Information, uncertainty, utility categories.
Proceedings Seventh Annual Conference Cognitive Science Society, pp.
283{287. Hillsdale, NJ: Lawrence Erlbaum.
177

fiFisher

Hadzikadic, M., & Yun, D. (1989). Concept formation incremental conceptual clustering.
Proceedings International Joint Conference Artificial Intelligence, pp. 831{
836. San Mateo, CA: Morgan Kaufmann.
Hanson, R., Stutz, J., & Cheeseman, P. (1991). Bayesian classification correlation
inheritance. Proceedings 12th International Joint Conference Artificial
Intelligence, pp. 692{698. San Mateo, CA: Morgan Kaufmann.
Iba, G. (1989). heuristic approach discovery macro operators. Machine Learning,
3, 285{317.
Iba, W., & Gennari, J. (1991). Learning recognize movements. Fisher, D., Pazzani, M.,
& Langley, P. (Eds.), Concept Formation: Knowledge Experience Unsupervised
Learning. San Mateo, CA: Morgan Kaufmann.
Ketterlin, A., Gancarski, P., & Korczak, J. (1995). Hierarchical clustering composite
objects variable number components. Preliminary papers Fifth
International Workshop Artificial Intelligence Statistics, pp. 303{309.
Kilander, F. (1994). Incremental Conceptual Clustering On-Line Application. Ph.D.
thesis, Stockholm University, Stockholm, Sweden: Department Computer Systems Sciences.
Kolodner, J. L. (1983). Reconstructive memory: computer model. Cognitive Science, 7,
281{328.
Lebowitz, M. (1982). Correcting erroneous generalizations. Cognition Brain Theory,
5, 367{381.
Lebowitz, M. (1987). Experiments incremental concept formation: Unimem. Machine
Learning, 2, 103{138.
Levinson, R. (1984). self-organizing retrieval system graphs. Proceedings
National Conference Artificial Intelligence, pp. 203{206. San Mateo, CA: Morgan
Kaufmann.
Lopez de Mantaras, R. (1991). distance-based attribute selection measure decision
tree induction. Machine Learning, 6, 81{92.
Martin, J., & Billman, D. (1994). Acquiring combining overlapping concepts. Machine
Learning, 16, 121{155.
McKusick, K., & Langley, P. (1991). Constraints tree structure concept formation.
Proceedings International Joint Conference Artificial Intelligence, pp.
810{816. San Mateo, CA: Morgan Kaufmann.
McKusick, K., & Thompson, K. (1990). Cobweb/3: portable implementation (Tech. Rep.
No. FIA-90-6-18-2). Moffett Field, CA: AI Research Branch, NASA Ames Research
Center.
178

fiOptimization Hierarchical Clusterings

Medin, D. (1983). Structural principles categorization. Tighe, T., & Shepp, B.
(Eds.), Perception, Cognition, Development, pp. 203{230. Hillsdale, NJ: Lawrence
Erlbaum.
Michalski, R. S., & Stepp, R. (1983a). Automated construction classifications: conceptual
clustering versus numerical taxonomy. IEEE Transactions Pattern Analysis
Machine Intelligence, 5, 219{243.
Michalski, R. S., & Stepp, R. (1983b). Learning observation: conceptual clustering.
Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning:
Artificial Intelligence Approach. San Mateo, CA: Morgan Kaufmann.
Mingers, J. (1989a). empirical comparison pruning methods decision-tree induction. Machine Learning, 4, 227{243.
Mingers, J. (1989b). empirical comparison selection measures decision-tree induction. Machine Learning, 3, 319{342.
Nevins, A. J. (1995). branch bound incremental conceptual clusterer. Machine
Learning, 18, 5{22.
Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81{106.
Quinlan, J. R. (1987). Simplifying decision trees. International Journal Man-machine
Studies, 27, 221{234.
Quinlan, J. R. (1993). C4.5: Programs Machine Learning. San Mateo, CA: Morgan
Kaufmann.
Reich, Y., & Fenves, S. (1991). formation use abstract concepts design.
Fisher, D., Pazzani, M., & Langley, P. (Eds.), Concept Formation: Knowledge
Experience Unsupervised Learning. San Mateo, CA: Morgan Kaufmann.
Utgoff, P. (1994). improved algorithm incremental induction decision trees.
Proceedings Eleventh International Conference Machine Learning, pp. 318{
325. San Mateo, CA: Morgan Kaufmann.
Wallace, C. S., & Dowe, D. L. (1994). Intrinsic classification MML - Snob program.
Proceedings 7th Australian Joint Conference Artificial Intelligence, pp.
37{44. UNE, Armidale, NSW, Australia: World Scientific.
Weiss, S., & Kulikowski, C. (1991). Computer Systems Learn. San Mateo, CA: Morgan
Kaufmann.
Wilcox, C. S., & Levinson, R. A. (1986). self-organized knowledge base recall, design,
discovery organic chemistry. Pierce, T. H., & Hohne, B. A. (Eds.), Artificial
Intelligence Applications Chemistry. Washington, DC: American Chemical Society.

179

fiJournal Artificial Intelligence Research 4 (1996) 61|76

Submitted 11/95; published 3/96

Mean Field Theory Sigmoid Belief Networks
Lawrence K. Saul
Tommi Jaakkola
Michael I. Jordan

Center Biological Computational Learning
Massachusetts Institute Technology
79 Amherst Street, E10-243
Cambridge, 02139

lksaul@psyche.mit.edu
tommi@psyche.mit.edu
jordan@psyche.mit.edu

Abstract

develop mean field theory sigmoid belief networks based ideas statistical
mechanics. mean field theory provides tractable approximation true probability distribution networks; also yields lower bound likelihood evidence. demonstrate utility framework benchmark problem statistical pattern recognition|the
classification handwritten digits.

1. Introduction

Bayesian belief networks (Pearl, 1988; Lauritzen & Spiegelhalter, 1988) provide rich graphical
representation probabilistic models. nodes networks represent random variables,
links represent causal uences. associations endow directed acyclic graphs (DAGs)
precise probabilistic semantics. ease interpretation afforded semantics explains
growing appeal belief networks, widely used models planning, reasoning,
uncertainty.
Inference learning belief networks possible insofar one eciently compute (or
approximate) likelihood observed patterns evidence (Buntine, 1994; Russell, Binder, Koller,
& Kanazawa, 1995). exist provably ecient algorithms computing likelihoods belief
networks tree chain-like architectures. practice, algorithms also tend perform
well general sparse networks. However, networks nodes many parents,
exact algorithms slow (Jensen, Kong, & Kjaefulff, 1995). Indeed, large networks
dense layered connectivity, exact methods intractable require summing
exponentially large number hidden states.
One approach dealing networks use Gibbs sampling (Pearl, 1988),
stochastic simulation methodology roots statistical mechanics (Geman & Geman, 1984).
approach paper relies different tool statistical mechanics|namely, mean field
theory (Parisi, 1988). mean field approximation well known probabilistic models
represented undirected graphs|so-called Markov networks. example, Boltzmann
machines (Ackley, Hinton, & Sejnowski, 1985), mean field learning rules shown yield
tremendous savings time computation sampling-based methods (Peterson & Anderson,
1987).
main motivation work extend mean field approximation undirected
graphical models directed counterparts. Since belief networks transformed Markov
networks, mean field theories Markov networks well known, natural ask
new framework required all. reason probabilistic models compact
representations DAGs may unwieldy representations undirected graphs. shall see,
avoiding complexity working directly DAGs requires extension existing methods.
paper focus sigmoid belief networks (Neal, 1992), resulting mean
field theory straightforward. networks binary random variables whose local
c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiSaul, Jaakkola, & Jordan

conditional distributions based log-linear models. develop mean field approximation
networks use compute lower bound likelihood evidence. method applies
arbitrary partial instantiations variables networks makes restrictions
network topology. Note lower bound available, learning procedure maximize
lower bound; useful true likelihood cannot computed eciently. similar
approximation models continous random variables discussed Jaakkola et al (1995).
idea bounding likelihood sigmoid belief networks introduced related
architecture known Helmholtz machine (Hinton, Dayan, Frey, & Neal 1995). fundamental
advance work establish framework approximation especially conducive
learning parameters layered belief networks. close connection idea
mean field approximation statistical mechanics, however, developed.
paper hope elucidate connection, also convey sense
approximations likely generate useful lower bounds while, time, remaining
analytically tractable. develop perhaps simplest approximation belief
networks, noting sophisticated methods (Jaakkola & Jordan, 1996a; Saul & Jordan, 1995)
also available. emphasized approximations form required handle
multilayer neural networks used statistical pattern recognition. networks, exact
algorithms hopelessly intractable; moreover, Gibbs sampling methods impractically slow.
organization paper follows. Section 2 introduces problems inference
learning sigmoid belief networks. Section 3 contains main contribution paper:
tractable mean field theory. present mean field approximation sigmoid belief
networks derive lower bound likelihood instantiated patterns evidence. Section 4
looks mean field algorithm learning parameters sigmoid belief networks.
algorithm, give results benchmark problem pattern recognition|the classification
handwritten digits. Finally, section 5 presents conclusions, well future issues research.

2. Sigmoid Belief Networks

great virtue belief networks clearly exhibit conditional dependencies
underlying probability model. Consider belief network defined binary random variables
= (S1 ; S2 ; : : : ; SN ). denote parents Si pa(Si ) fS1 ; S2; : : : Si,1 g; smallest
set nodes
P (Si jS1; S2 ; : : : ; Si,1) = P (Sijpa(Si )):
(1)
sigmoid belief networks (Neal, 1992), conditional distributions attached node
based log-linear models. particular, probability ith node activated given

1
0
X
Jij Sj + hi ;
P (Si = 1jpa(Si )) = @
j

(2)

Jij hi weights biases network,
1
(z ) =
(3)
1 + e,z
sigmoid function shown Figure 1. sigmoidbelief networks, Jij = 0 Sj 62 pa(Si );
moreover, Jij = 0 j since network's structure directed acyclic graph.
sigmoid function eq. (2) provides compact parametrization conditional probability
distributions1 eq. (2) used propagate beliefs. particular, P (Sijpa(Si )) depends pa(Si )
sum weighted inputs, weights may viewed parameters
1. relation noisy-OR models discussed appendix A.

62

fiMean Field Theory Sigmoid Belief Networks

1

0.8

0.6
(z)
0.4

0.2

0
6

4

2

0
z

2

4

6

Figure 1: Sigmoid function (z ) = [1 + e,z ],1. z sum weighted inputs node ,
P (S = 1jz ) = (z ) conditional probability node activated.
logistic regression (McCullagh & Nelder, 1983). conditional probability distribution Si may
summarized as:
hP

exp
J

+
h
Si
ij
j

hjP
i:
(4)
P (Sijpa(Si )) =
1 + exp j Jij Sj + hi
Note substituting Si = 1 eq. (4) recovers result eq. (2). Combining eqs. (1) (4),
may write joint probability distribution variables network as:
P (S )

=



P (Si jpa(Si ))

i9
8 h
< exp Pj Jij Sj + hi Si =
=
: 1 + exp hPj Jij Sj + hii ; :



(5)
(6)

denominator eq. (6) ensures probability distribution normalized unity.
turn problem inference sigmoid belief networks. Absorbing evidence divides
units belief network two types, visible hidden. visible units (or \evidence
nodes") instantiated values; hidden units
not. possible ambiguity, use H V denote subsets hidden
visible units. Using Bayes' rule, inference done conditional distribution
P (H; V )
P (H jV ) =
;
(7)
P (V )

X
P (V ) =
P (H; V )
(8)
H

likelihood evidence V . principle, likelihood may computed summing
2jH j configurations hidden units. Unfortunately, calculation intractable large,
densely connected networks. intractability presents major obstacle learning parameters
networks, nearly procedures statistical estimation require frequent estimates
likelihood. calculations exact probabilistic inference beset diculties.
63

fiSaul, Jaakkola, & Jordan

Unable compute P (V ) work directly P (H jV ), resort approximation
statistical physics known mean field theory.

3. Mean Field Theory

mean field approximation appears multitude guises physics literature; indeed,
\almost old statistical mechanics" (Itzykson & Drouffe, 1991). Let us brie explain
acquired name ubiquitous. physical models described Markov networks,
variables
P Si represent localized magnetic moments (e.g., sites crystal lattice),
sums j Jij Sj + hi represent local magnetic fields. Roughly speaking, certain cases central
limit theorem may applied sums, useful approximation ignore uctuations
fields replace mean value|hence name, \mean field" theory.
models, excellent approximation; others, poor one. simplicity, however,
widely used first step understanding many types physical phenomena.
Though explains philological origins mean field theory, fact many ways
derive amounts approximation (Parisi, 1988). paper present
formulation appropriate inference learning graphical models. particular, view
mean field theory principled method approximating intractable graphical model
tractable one. done via variational principle chooses parameters tractable
model minimize entropic measure error.
basic framework mean field theory remains directed graphs, though
found necessary introduce extra mean field parameters addition usual ones.
Markov networks, one finds set nonlinear equations mean field parameters
solved iteration. practice, found iteration converge fairly quickly scale
well large networks.
Let us return problem posed end last section. found
many belief networks, intractable decompose joint distribution P (S ) = P (H jV )P (V ),
P (V ) likelihood evidence V . purposes probabilistic modeling, mean
field theory two main virtues. First, provides tractable approximation, Q(H jV ) P (H jV ),
conditional distributions required inference. Second, provides lower bound
likelihoods required learning.
Let us first consider origin lower bound. Clearly, approximating distribution
Q(H jV ), equality:
ln P (V ) = ln

X

P (H; V )

H

= ln

X
H

Q(H jV )

P (H; V )
Q(H jV )

(9)
:

(10)

obtain lower bound, apply Jensen's inequality (Cover & Thomas, 1991), pushing
logarithm sum hidden states expectation:


X
V)
ln P (V ) Q(H jV ) ln PQ((H;
:
(11)
H jV )
H
straightforward verify difference left right hand side eq. (11)
Kullback-Leibler divergence (Cover & Thomas, 1991):
Q(H jV )
X
KL(QjjP ) = Q(H jV ) ln P (H jV ) :
(12)
H
Thus, better approximation P (H jV ), tighter bound ln P (V ).
64

fiMean Field Theory Sigmoid Belief Networks

Anticipating connection statistical mechanics, refer Q(H jV ) mean field
distribution. natural divide calculation bound two components,
particular averages approximating distribution. components mean field
entropy energy; overall bound given difference:
ln P (V ) ,

X
H

Q(H jV ) ln Q(H jV )

!

, ,

X
H

!

Q(H jV ) ln P (H; V ) :

(13)

terms physical interpretations. first measures amount uncertainty meanfield distribution follows standard definition entropy. second measures average
value2 , ln P (H; V ); name \energy" arises interpreting probability distributions
belief networks Boltzmann distributions3 unit temperature. case, energy
network configuration given (up constant) minus logarithm probability
Boltzmann distribution. sigmoid belief networks, energy form

, ln P (H; V ) = ,

X
ij

Jij Si Sj

,

X


2
0
13
X 4
X
hi Si +
ln 1 + exp @ Jij Sj + hi A5 ;


j

(14)

follows eq. (6). first two terms equation familiar Markov networks
pairwise interactions (Hertz, Krogh, & Palmer, 1991); last term peculiar sigmoid belief
networks. Note overall energy neither linear function weights polynomial
function units. price pay sigmoid belief networks identifying P (H jV )
Boltzmann distribution log-likelihood P (V ) partition function. Note
identification made implicitly form eqs. (7) (8).
bound eq. (11) valid probability distribution Q(H jV ). make use it,
however, must choose distribution enables us evaluate right hand side eq. (11).
Consider factorized distribution
Q(H jV ) =


2

H

Si (1 , )1,Si ;

(15)

binary hidden units fSi gi2H appear independent Bernoulli variables adjustable
means . mean field approximation obtained substituting factorized distribution,
eq. (15), true Boltzmann distribution, eq. (7). may seem approximation replaces
rich probabilistic dependencies P (H jV ) impoverished assumption complete factorizability. Though true degree, reader keep mind values
choose fi gi2H (and hence statistics hidden units) depend evidence V .
best approximation form, eq. (15), found choosing mean values, figi2H ,
minimize Kullback-Leibler divergence, KL(QjjP ). equivalent minimizing gap
true log-likelihood, ln P (V ), lower bound obtained mean field theory.
2. similar average performed E-step EM algorithm (Dempster, Laird, & Rubin, 1977); difference
average performed mean field distribution, Q(H jV ), rather true posterior,
P (H jV ). related discussion, see Neal & Hinton (1993).
3. terminology follows. Let denote degrees freedom statistical mechanical system. energy
system, E (S ), real-valued function degrees freedom, Boltzmann distribution
e,fiE (S)
P (S ) = P ,fiE (S)
Se

defines probabilitydistributionover possible configurationsof . parameter fi inverse temperature;
serves calibrate energy scale fixed unity discussion belief networks. Finally,
sum denominator|known partition function|ensures Boltzmann distribution normalized
unity.

65

fiSaul, Jaakkola, & Jordan

mean field bound log-likelihood may calculated substituting eq. (15) right
hand side eq. (11). result calculation

X

ln P (V )

Jij j

X

ij

,



+

X


hi ,

X


P

ln 1 + e

j

Jij Sj +hi



(16)

[i ln + (1 , ) ln(1 , )] ;

hi indicates expectation value mean field distribution, eq. (15). terms
first line eq. (16) represent mean field energy, derived eq. (14); second
represent mean field entropy. slight abuse notation, defined mean values
visible units; course set instantiated values 2 f0; 1g.
Note compute average energy
P mean field approximation, must find
expected value hln [1 + ez ]i, zi = j Jij Sj + hi sum weighted inputs ith
unit belief network. Unfortunately, even mean field assumption hidden
units uncorrelated, average simple closed form. term arise
mean field theory Markov networks pairwise interactions; again, peculiar sigmoid
belief networks.
principal, average may performed enumerating possible states pa(Si ).
result calculation, however, would extremely unwieldy function parameters
belief network. ects fact general, sigmoid belief network defined
weights Jij equivalent Markov network N th order interactions pairwise ones.
avoid complexity, must develop mean field theory works directly DAGs.
handle expected value hln [1 + ez ]i distinguishes mean field theory
previous ones. Unable compute term exactly, resort another bound. Note
random variable z real number , equality:







ff

hln[1 + ez ]i = ln ez e,z (1 + ez )

E
= hz + ln[e,z + e(1,)z ] :

(17)
(18)

upper bound right hand side applying Jensen's inequality opposite direction
before, pulling logarithm outside expectation:


E
hln[1 + ez ]i hz + ln e,z + e(1,)z :

(19)

Setting = 0 eq. (19) gives standard bound: hln(1 + ez )i lnh1 + ez i. tighter bound
(Seung, 1995) obtained, however, allowing non-zero values . illustrated
Figure 2 special case z Gaussian distributed random variable zero mean
unit variance. bound eq. (19) two useful properties state without proof:
(i) right hand side convex function ; (ii) value minimizes function
occurs interval 2 [0; 1]. Thus, provided possible evaluate eq. (19) different values
, tightest bound form found simple one-dimensional minimization.
bound put immediate use attaching extra mean field parameter
unit belief network. upper bound intractable terms mean field
energy



ln 1 + e

PJ
j

ij


+h
j



1
0

E
X
@ Jij j + hi + ln e, z + e(1, )z ;


j

66





(20)

fiMean Field Theory Sigmoid Belief Networks

1

0.95

0.9
bound
0.85
exact
0.8

0.75
0

0.2

0.4



0.6

0.8

1

Figure 2: Bound eq. (19) case z normally distributed zero mean
unit variance.
case,
exact result hln(1 + ez )i = 0:806; bound gives

n 1 2 In1 (1this
2
min ln[e 2 + e 2 ,) ] = 0:818. standard bound Jensen's inequality occurs
= 0 gives 0:974.

P

zi = j Jij Sj + hi. expectations inside logarithm evaluated exactly
factorial distribution, eq. (15); example,
Y,

he, z = e, h
1 , j + j e, J :
(21)






ij

j

holds he(1,i )zi i.

similar result
Though averages tractable, tend write
follows. reader, however, keep mind averages
present diculty; simply averages products independent random variables,
opposed sums.
Assembling terms eqs. (16) (20) gives lower bound ln P (V ) LV ,

LV =

X
ij

,

Jij j +

X

X , z


ln

e





0
1
X @X
hi ,

Jij j + hi

E Xj
(1, )z

+e





+



(22)

[i ln + (1 , ) ln(1 , )] ;

log-likelihood evidence V . far specified parameters fi gi2H
fi g; particular, bound eq. (22) valid choice parameters. naturally seek
values maximize right hand side eq. (22). Suppose fix mean values fi gi2H
ask parameters fig yield tightest possible bound. Note right hand

side eq. (22) couple terms belong different units network.
minimization fi g therefore reduces N independent minimizations interval [0; 1].
done number standard methods (Press, Flannery, Teukolsky, & Vetterling,
1986).
choose means, set gradients bound respect fi gi2H equal zero.
end, let us define intermediate matrix:
Kij


E
= , @@ ln e, z + e(1, )z ;


j

67





(23)

fiSaul, Jaakkola, & Jordan

Si

Figure 3: Markov blanket unit
parents children.

Si

includes parents children, well

zi weighted sum inputs ith unit. Note Kij zero unless Sj parent
Si ; words, connectivity weight matrix Jij . Within mean field
approximation, Kij measures parental uence Sj Si given instantiated evidence V .
degree correlation (positive negative) measured relative parents Si .
matrix elements K may evaluated expanding expectations eq. (21); full
derivation given appendix B. Setting gradient @ LV =@i equal zero gives final mean
field equation:
0
1


= @ hi +

X
j

[Jij j + Jji(j , j ) + Kji ]A ;

(24)

() sigmoid function. argument sigmoid function may viewed
effective input ith unit belief network. effective input composed terms
unit's Markov blanket (Pearl, 1988), shown Figure 3; particular, terms take
account unit's internal bias, values parents children, and, matrix
Kji, values children's parents. solving equations iteration, values
instantiated units propagated throughout entire network. analogous propagation
information occurs exact algorithms (Lauritzen & Spiegelhalter, 1988) compute likelihoods
belief networks.
factorized approximation true posterior exact, mean field equations
set parameters figi2H values make approximation accurate possible.
turn translates tightest mean field bound log-likelihood. overall procedure
bounding log-likelihood thus consists two alternating steps: (i) update fi g fixed fi g;
(ii) update figi2H fixed fi g. first step involves N independent minimizations
interval [0; 1]; second done iterating mean field equations. practice, steps
repeated mean field bound log-likelihood converges4 desired degree accuracy.
quality bound depends two approximations: complete factorizability
mean field distribution, eq. (15), logarithm bound, eq. (19). reliable approximations belief networks? study question, performed numerical experiments
three layer belief network shown Figure 4. advantage working small
network (2x4x6) true likelihoods computed exact enumeration. considered
particular event units bottom layer instantiated zero. event,
compared mean field bound likelihood true value, obtained enumerating
4. shown asychronous updates mean field parameters lead monotonic increases lower
bound (just case Markov networks).

68

fiMean Field Theory Sigmoid Belief Networks

Figure 4: Three layer belief network (2x4x6) top-down propagation beliefs. model
images handwritten digits section 4, used 8x24x64 networks units
bottom layer encoded pixel values 8x8 bitmaps.
4500

4000

4000

3500

3500

3000
mean field approximation

3000

uniform approximation

2500

2500

2000
2000

1500
1500

1000

1000

500

500
0
0

0.01

0.02
0.03
0.04
0.05
relative error loglikelihood

0.06

0
1

0.07

0.5

0
0.5
1
relative error loglikelihood

1.5

Figure 5: Histograms relative error log-likelihood 10000 randomly generated three layer
networks. left: relative error mean field approximation; right:
relative error states bottom layer assumed occur equal probability.
log-likelihood computed event nodes bottom layer
instantiated zero.
states top two layers. done 10000 random networks whose weights biases
uniformly distributed -1 1. Figure 5 (left) shows histogram relative error
log likelihood, computed LV = ln P (V ) , 1; networks, mean relative error 1.6%.
Figure 5 (right) shows histogram results assuming states bottom layer
occur equal probability; case relative error computed (ln2,6 )= ln P (V ) , 1.
\uniform" approximation, root mean square relative error 22.6%. large discrepancy results suggests mean field theory provide useful lower bound
likelihood certain belief networks. course, ultimately matters behavior mean
field theory networks solve meaningful problems. subject next section.

4. Learning

One attractive use sigmoid belief networks perform density estimation high dimensional
input spaces. problem parameter estimation: given set patterns particular
units belief network, find set weights Jij biases hi assign high probability
patterns. Clearly, ability compute likelihoods lies crux algorithm
learning parameters belief networks.
69

fiSaul, Jaakkola, & Jordan

true loglikelihood

lower bound
true loglikelihood

lower bound

training time

training time

Figure 6: Relationship true log-likelihood lower bound learning. One
possibility (at left) increase together. true log-likelihood
decreases, closing gap bound. latter viewed
form regularization.
Mean field algorithms provide strategy discovering appropriate values Jij hi without
resort Gibbs sampling. Consider, instance, following procedure. pattern
training set, solve mean field equations fi; g compute associated bound
log-likelihood, LV . Next, adapt weights belief network gradient ascent5 mean
field bound,
Jij =
hi =

@ LV
@Jij
@ LV
;

@hi


(25)
(26)

suitably chosen learning rate. Finally, cycle patterns training set,
maximizing likelihoods6 fixed number iterations one detects onset
overfitting (e.g., cross-validation).
procedure uses lower bound log-likelihood cost function training belief
networks (Hinton, Dayan, Frey, & Neal, 1995). fact lower bound loglikelihood, rather upper bound, course crucial success learning algorithm.
Adjusting weights maximize lower bound affect true log-likelihood two ways
(see Figure 6). Either true log-likelihood increases, moving direction bound,
true log-likelihood decreases, closing gap two quantities. purposes
maximum likelihood estimation, first outcome clearly desirable; second, though less
desirable, also viewed positive light. case, mean field approximation acting
regularizer, steering network toward simple, factorial solutions even expense lower
likelihood estimates.
tested algorithm building maximum-likelihood classifier images handwritten
digits. data consisted 11000 examples handwritten digits [0-9] compiled U.S. Postal
Service Oce Advanced Technology. examples preprocessed produce 8x8 binary
images, shown Figure 7. digit, divided available data training set
700 examples test set 400 examples. trained three layer network7 (see
5. Expressions gradients LV given appendix B.
6. course, one also incorporate prior distributions weights biases maximize approximation
log posterior probability training set.
7. many possible architectures could chosen purpose density estimation; used layered
networks permit comparison previous benchmarks data set.

70

fiMean Field Theory Sigmoid Belief Networks

Figure 7: Binary images handwritten digits: two five.
0
1
2
3
4
5
6
7
8
9

0
1
2
3
4
5
6
7
8
9
388 2
2
0
1
3
0
0
4
0
0 393 0
0
0
1
0
0
6
0
1
2 376 1
3
0
4
0 13 0
0
2
4 373 0 12 0
0
6
3
0
0
2
0 383 0
1
2
2 10
0
2
1 13 0 377 2
0
4
1
1
4
2
0
1
6 386 0
0
0
0
1
0
0
0
0
0 388 3
8
1
9
1
7
0
7
1
1 369 4
0
4
0
0
0
0
0
8
5 383

Table 1: Confusion matrix digit classification. entry ith row j th column counts
number times digit classified digit j .
Figure 4) digit, sweeping training set five times learning rate = 0:05.
networks 8 units top layers, 24 units middle layer, 64 units bottom
layer, making far large treated exact methods.
training, classified digits test set network assigned
highest likelihood. Table 1 shows confusion matrix ij th entry counts number
times digit classified digit j . 184 errors classification (out possible 4000),
yielding overall error rate 4.6%. Table 2 gives performance various algorithms
partition data set. Table 3 shows average log-likelihood score network
digits test set. (Note scores actually lower bounds.) scores
normalized network zero weights biases (i.e., one 8x8 patterns
equally likely) would receive score -1. expected, digits relatively simple constructions
(e.g., zeros, ones, sevens) easily modeled rest.
measures performance|error rate log-likelihood score|are competitive previously published results (Hinton, Dayan, Frey, & Neal, 1995) data set. success
algorithm arms strategy maximizing lower bound utility mean field
approximation. Though similar results obtained via Gibbs sampling, seems require
considerably computation methods based maximizing lower bound (Frey, Dayan, &
Hinton, 1995).
71

fiSaul, Jaakkola, & Jordan

algorithm
classification error
nearest neighbor
6.7%
back-propagation
5.6%
wake-sleep
4.8%
mean field
4.6%
Table 2: Classification error rates data set handwritten digits. first three reported
Hinton et al (1995).
digit log-likelihood score
0
-0.447
1
-0.296
2
-0.636
3
-0.583
4
-0.574
5
-0.565
6
-0.515
7
-0.434
8
-0.569
9
-0.495

-0.511
Table 3: Normalized log-likelihood score network digits test set. obtain
raw score, multiply 400 64 ln2. last row shows score averaged across
digits.

5. Discussion

Endowing networks probabilistic semantics provides unified framework incorporating
prior knowledge, handling missing data, performing inference uncertainty. Probabilistic
calculations, however, quickly become intractable, important develop techniques
approximate probability distributions exible manner. especially true networks
multilayer architectures large numbers hidden units. Exact algorithms Gibbs sampling
methods generally practical networks; approximations required.
paper developed mean field approximation sigmoid belief networks.
computational tool, mean field theory two main virtues: first, provides tractable
approximation conditional distributions required inference; second, provides lower
bound likelihoods required learning.
problem computing exact likelihoods belief networks NP-hard (Cooper, 1990);
true approximating likelihoods within guaranteed degree accuracy (Dagum &
Luby, 1993). follows one cannot establish universal guarantees accuracy mean
field approximation. certain networks, clearly, mean field approximation bound fail|it
cannot capture logical constraints strong correlations uctuating units. preliminary
results, however, suggest worst-case results apply belief networks.
worth noting, moreover, qualifications apply Markov networks,
domain, mean field methods already well-established.
72

fiMean Field Theory Sigmoid Belief Networks

idea bounding likelihood sigmoid belief networks introduced related
architecture known Helmholtz machine (Hinton, Dayan, Neal, & Zemel, 1995). formalism
paper differs number respects Helmholtz machine. importantly,
enables one compute rigorous lower bound likelihood. cannot said
wake-sleep algorithm (Frey, Hinton, & Dayan, 1995), relies sampling-based methods,
heuristic approximation Dayan et al (1995), guarantee rigorous lower bound.
Also, mean field theory|which takes place \recognition model" Helmholtz
machine|applies generally sigmoid belief networks without layered structure. Moreover,
places restrictions locations visible units; may occur anywhere within
network|an important feature handling problems missing data. course, advantages
accrued without extra computational demands complicated learning rules.
recent work builds theory presented here, begun relax assumption
complete factorizability eq. (15). general, one would expect sophisticated approximations
Boltzmann distribution yield tighter bounds log-likelihood. challenge
find distributions allow correlations hidden units remaining computationally
tractable. tractable, mean choice Q(H jV ) must enable one evaluate (or
least upper bound) right hand side eq. (13). Extensions kind include mixture models
(Jaakkola & Jordan, 1996) and/or partially factorized distributions (Saul & Jordan, 1995)
exploit presence tractable substructures original network. approach paper
work simplest mean field theory computationally tractable, clearly
better results obtained tailoring approximation problem hand.

Appendix A. Sigmoid versus Noisy-OR

semantics sigmoid function similar, identical, noisy-OR gates (Pearl,
1988) commonly found belief network literature. Noisy-OR gates use weights
network represent independent causal events. case, probability unit Si
activated given

P (Si = 1jpa(Si )) = 1 , (1 , pij )S
(27)
j

j

pij probability Sj = 1 causes Si = 1 absence causal events.
define weights noisy-OR belief network ij = , ln(1 , pij ), follows

0
1
X
p(Si jpa(Si )) = @
ij Sj ;
j



(28)

(z ) = 1 , e,z

(29)
noisy-OR gating function. Comparing sigmoid function, eq. (3), see
model P (Si jpa(Si )) monotonically increasing function sum weighted inputs.
main difference noisy-OR networks, weights ij constrained positive
underlying set probabilities, pij . Recently, Jaakkola Jordan (1996b) developed mean
field approximation noisy-OR belief networks.

Appendix B. Gradients

P
provide expressions gradients appear eqs. (23), (25) (26). usual, let
zi = j Jij Sj + hi denote sum inputs unit Si . factorial distribution, eq. (15),
73

fiSaul, Jaakkola, & Jordan

compute averages:



he, z = e, h
1 , j + j e, J ;




he(1, )z =




(30)

ij

j

e(1,i )hi

Yh
j

1 , j + j e(1, )J


ij



(31)

:

unit network, let us define quantity
(1,i )zi

(32)
= he,hze + e(1,i )z :
Note lies zero one. definition, write matrix elements
eq. (23) as:
(1 , )(1 , e, J ) + (1 , e(1, )J ) :
(33)
Kij =
1 , j + j e, J
1 , j + j e(1, )J
gradients eqs. (25) (26) found similar means. weights,








ij

ij

ij


,i Jij

@ LV
@Jij



ij

(1,i )Jij

, )i j e
i(1 , )j e
= ,(i , )j + (1
,
,

J
1 , j + j e
1 , j + j e(1, )J
Likewise, biases,
@ LV
= , :
@h
ij





ij

:

(34)
(35)

Finally, note one may obtain simpler gradients expense introducing weaker bound
eq. (19). advantageous speed computation important
quality bound. experiments paper used bound eq. (19).

Acknowledgements

especially grateful P. Dayan, G. Hinton, B. Frey, R. Neal, H. Seung sharing early
versions manuscripts providing many stimulating discussions work.
paper also improved greatly comments several anonymous reviewers. facilitate comparisons similar methods, results reported paper used images preprocessed
University Toronto. authors acknowledge support NSF grant CDA-9404932, ONR
grant N00014-94-1-0777, ATR Research Laboratories, Siemens Corporation.

References
Ackley, D., Hinton, G., & Sejnowski, T. (1985) learning algorithm Boltzmann machines.

Cognitive Science, 9, 147{169.

Buntine, W. (1994) Operations learning graphical models. Journal Artificial Intelligence
Research, 2, 159-225.
Cooper, G. (1990) Computational complexity probabilistic inference using Bayesian belief networks. Artificial Intelligence, 42, 393{405.
Cover, T., & Thomas, J. (1991) Elements Information Theory. New York: John Wiley & Sons.
Dagum, P., & Luby, M. (1993) Approximately probabilistic reasoning Bayesian belief networks
NP-hard. Artificial Intelligence, 60, 141{153.
74

fiMean Field Theory Sigmoid Belief Networks

Dayan, P., Hinton, G., Neal, R., & Zemel, R. (1995) Helmholtz machine. Neural Computation,
7, 889{904.
Dempster, A., Laird, N., Rubin, D. (1977) Maximum likelihood incomplete data via
EM algorithm. Journal Royal Statistical Society B39, 1{38.
Frey, B., Hinton, G., & Dayan, P. (1995) wake-sleep algorithm learn good density estimators? D. Touretzky, M. Mozer, M. Hasselmo (eds). Advances Neural Information
Processing Systems: Proceedings 1995 Conference.

Geman, S., & Geman, D. (1984) Stochastic relaxation, Gibbs distributions, Bayesian restoration images. IEEE Transactions Pattern Analysis Machine Intelligence, 6, 721{741.
Hertz, J., Krogh, A., Palmer, R. G. (1991) Introduction Theory Neural Computation.
Redwood City, CA: Addison-Wesley.
Hinton, G., Dayan, P., Frey, B., & Neal, R. (1995) wake-sleep algorithm unsupervised neural
networks. Science, 268, 1158{1161.
Itzykson, C., & Drouffe, J.M. (1991). Statistical Field Theory. Cambridge: Cambridge University
Press.
Jaakkola, T., Saul, L., & Jordan, M. (1995) Fast learning bounding likelihoods sigmoid-type
belief networks. D. Touretzky, M. Mozer, M. Hasselmo (eds). Advances Neural Information
Processing Systems: Proceedings 1995 Conference.

Jaakkola, T., & Jordan, M. (1996a) Mixture model approximations belief networks. Manuscript
preparation.
Jaakkola, T., & Jordan, M. (1996b) Computing upper lower bounds likelihoods intractable
networks. Submitted.
Jensen, C. S., Kong, A., & Kjaerulff, U. (1995) Blocking Gibbs sampling large probabilistic
expert systems. International Journal Human Computer Studies. Special Issue Real-World
Applications Uncertain Reasoning.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computations probabilities graphical structures application expert systems. Journal Royal Statistical Society B, 50, 157{224.
McCullagh, P., & Nelder, J. A. (1983) Generalized Linear Models. London: Chapman Hall.
Neal, R. (1992) Connectionist learning belief networks. Artificial Intelligence, 56, 71{113.
Neal, R., & Hinton, G. (1993) new view EM algorithm justifies incremental
variants. Submitted publication.
Parisi, G. (1988) Statistical Field Theory. Redwood City, CA: Addison-Wesley.
Pearl, J. (1988) Probabilistic Reasoning Intelligent Systems. San Mateo, CA: Morgan Kaufmann.
Peterson, C., & Anderson, J.R. (1987) mean field theory learning algorithm neural networks.
Complex Systems, 1, 995{1019.
Press, W. H., Flannery, B. P., Teukolsky, S.A., & Vetterling, W. T. (1986) Numerical Recipes.
Cambrige: Cambridge University Press.
Russell, S., Binder, J., Koller, D., & Kanazawa, K. (1995). Local learning probabilistic networks
hidden variables. Proceedings IJCAI{95.
75

fiSaul, Jaakkola, & Jordan

Saul, L., & Jordan, M. (1995) Exploiting tractable substructures intractable networks. D.
Touretzky, M. Mozer, M. Hasselmo (eds). Advances Neural Information Processing Systems:
Proceedings 1995 Conference.

Seung, H. (1995). Annealed theories learning. J.-H. Oh, C. Kwon, S. Cho, eds. Neural

Networks: Statistical Mechanics Perspective, Proceedings CTP-PRSRI Joint Workshop
Theoretical Physics. Singapore, World Scientific.

76

fi
ff
fi
! #"$ %
'&)(+*, --.0/1, -3254.

C

6789: ;=<>
-?!@A8 %&;=B0>
-.

D)EE3FHGJILKNMJONDPORQSDUTWVXMZY1[0\:]_^`IbadcfegY1DUMJONDPOihjIlkb[\nmoapIbklapVXTW]
q

[Y1rtsvuwMJVXTx[\nmoapDP^yDPapDUMJ\:DP]

z|{~}ff$}}{H

~ffgpbp$

P0=0) Jw
030lH'
010Hj0)ff


+yp~
SyH7L!~7L`3y`P5! 7`l5ybyg3y3yZX:3
Sff~7+`#y~=0ff35y33`~y+305770!U~0737+y
~Sff!7Syyy``g`7y!ff`Zyybb
7y!ff|
J373y!37`bH`H|JH`Zw37
#yH03XyLJ!X`3Z! 7
:7~3PZ)y`$3y7Z53:5`y$P5yH7y
`3!Sy)3yy
`yw~X0L!SJHffL`3Z7`37
yZ=
37 573:`Xy3
$
pN :)U~ lP#: $ 1ff
fi

!"$#%&'(*),+-(./012'3.(4(.53.%!%)63.763."%8:9;%5'%5

+<)=>)?=A@Bfi6)'#3*#C3.D)63*E(*/03.6+B3GF-3*)H/JI&:K(*LNMOPQKRTSU'%)6VWK/LNMOPXKR
SU'%)6VWK/L SY=%76ZLT[]\^)_/LYMOO Ma`284bff6c^d e-3.)?@B)_fgJ#%<'(*)Afgc)6#h)6
+<A)=5!i6+B3GF-j%53.7)=3kl#%&'(*)l+aE 3.#%l!7(.3.>@-(*73.%<7>)3*Z8ZmY8n"%8*LU3.
fg^W %afo)=>)N6)'#%p)g>^#'(*)Lp#'(*)q>r%27(.(*/?!+(*s/#L 6)'#%p)t>^%5(.(*/!%)
!+(.a/#L fg,fgp)u)67(.'#iv6:)6^3kg)g7+(*a/#wjx)=%l3k%27>)3*y)=>)Y:)6z3.
76)'#%D)L%)=K'u+3.%"j)=%l6)'#%D)^#%<'(*)NaEr)=%l e-3.)3k%"7#'(*)u#%&'(*)8
{ +B3GF|3*)_/}3.;~3.!+B)p);6'%?q+LB@'%)^)r)=?(*/%L#~>),(*6),3.
6!j>++(k3.>)3*^%)r>3.(./i)=?!)r3.7+|)D)^%8-b$)=%(*"(T#%53.13*)r7/LB<
3.)L@B5)=y6i)=>)j$!5"%(t'(.53.l+#C3ky3*)+D)#(q(.af
Y++|#)6y6)>)6?(.fI& >WKWLMOO`28Bb)=%66z+z7a/c@|,@-6#}$6!
@3k;+3.3.+(*t"'(.>)3."j=af%e-3.)Y!"7'(*N>A)67@B,6(*E#8
(.63.h)=%y>++(.3.>)3.J#%73.L:(.3.Wc7K#%(N@-6##3.>"3.?5 F"'>)3*LT+
c+-(./h&'#7D)(r(*8N$K#%(r@#o#3.>"%3.i'6(."3.(;#%3*+)3*7)=%
%5(@B=E 3*'%t!+B%p)gZ#%E 3.^)6")=%ufz3*)=y(*"3k(-#%3*+)3*5)=%;)'
(.(*/!@-6E#c@B=E 3*'%a8 9;%r)63*g)6!'!r7(-@|=E 3*'%guu7p/5!+B%p)u
+B3*@-(*8 #3.>"%3.q6+B# )6j?6)tZ7+|D)tgfz=3k=7)=6r7(./7'!+%
)3*z(*#)6y3k3.)6/8%4/c)6$7(.>"p'@BuT+|3*@(*;#3.>"6z3.N@)3.%#8b
(-(.3*<Y6!^7+|D)t>^(*4(.3.>@-(*u)=7)=8pSUl(.3k73.>)6Y(* +(.'3*@(*N#3.>"6
%A}"3*El)=%A7(./i'!+)3*N<Y(.3.>@-(*,!+B%D)z=3*"=%u+23*3*)H/8
bc F"'%>)3.y)WKu3*)N3.g<)63.!+B3*@-(*u)6!=3*El(k(U)=%,#%3*"y"(.89r<)6c%
1#3.6)3.%"'3.=57l3.!+B)p)u"(.u<(*Y3.!+B)p)N%8SU76)6')Y)=%l@B6)N+B
3*@-(*^ F-"'%>)3*N"(ku)=%c=EA)6!@|;+6p)6#u#%<'(*)gfz3*)=c#3GBp)g+
#3k%"!)67)=%3*^#%3*>@-3.(.3*)H/8


, --. ~ %%-!;3X
;N=0
gp
!fi9P
q~8! %&%_ &%%0;

fig%,-

Y?*?t<,.rg.G*0.d%!%6.?6.%B$*G
*i.gu!6gr%!%6.r*.Na;|i|Dn%*%2***5*6
7**ff&.<2*642.>.6pk667.j*.&,.*42Z.**
%&*q..A&gY> |D2%76^>%t<g>r*5.7hK6.
7Nky,..az.?66%6!,%2.%j!%7%&*N.N6c67p6B%,
*$ %!%7%6...-;k6B.$%??q*.*}%&*r*.
.?.%<7>.fi.?6h60p6N%i%>*ff^K6*Tzsta4%c<
.%<7>.J*6*r.!%7 J.ff%1*kY.%>tY.j!7>7.j .ff
.%<7>.yg6jB^&..?_6BGp%,.N%?gay6.?>B%,&uB6y6
6.%cz*-Y<ZY.r.,.~6>~p66,6}%jN~B*?60>%
z*Cff%afi*g>7D>*ZU<?k6:<?>yp6 _%B%p
C%56!pAN%7A!%d.p*C |..%}.azAkl}kz&*u%
%A76Y.<t>uT%,6.%%
>Yglg.}..,65-%<.z1>>r.*szYN6y6pu
c.%<7>.<$%c.%>ih%.y2fik%27>*% 7k.*Zfi$p
>BA&gY> |u%A%%^Y%2*Bij>>.pu4%7<.N..,.czk
6.%ff>|yikyB**qr*%%~*oA%&*c*.$6p6
.C.,>k*A>B l6>*5*.C766UkA>h
%aYK
>zz%Y>25.q!%>*.*?K67*!%7k%, *q.p*4z66*
?%0Y.6K6***r!>*-..*H%o<K6*
>D6k%7>Yl<l.5>*cgz>1.p65pz%K>6*
KY^5}>B>^k66Dr%&*u%.-n*-%*zz%666.Y->^.z>
k->*-B6;%c 6*;>,.BYk^.;66..y.?.$>;>B;tj.
*Yz*7%&*z%l% %.66,T 6*Y.u%,6y%**;
rk%27>*N.t**jp6*>.Yz%%N2yk%27>*5%.5|^>*
6y%66sckK6*
Kzz%Y>.. >B .7 >^%%_%7<. .p6ApK* k4pq%
%6ysr7*.;7u^*%>*$u<k.%
%!>~6D60k$.r>B,z..|j~~K60*.?5;z*d_g

_KBA^%>*:Y.!>?kC!>2.6^*%?>..AB tc>c!
662.*}.J}6B77}%^.J%%a t$>1!166..}.g
%i%rk*a>*62>i6Y2%^<?.r^k$%27%&*^*.>|tj>7
.utr7* %>.5 &..u5%Y*T^.56B!6% _%5%&*
.Y*6y*.> d.fi6z.<%1 6.6o67p.yl%&*5*k6
g.G<%~67p.i<fi;.%Zz 2.*HT >:q .6 U >T.<2*6
2Nn* 6fi.%D*h6.z>2z%1%!%6..*7>
%-*.Z%l%%5*0u 6*HAklg.G %az~>Ag.G<%
67p.!67.!!*K6!.p**.0KB6k*UYk.j.d}6}.fi%
B ^ag|.Yku%_g*%1pc76!%Nk1.cy.-%lg.G<%
.*uBl!-%6}.cB* %7.|.7
%..4%% 6U %q>BkZ<.*szakK*?ug K.fiz-**;
g.G<%77Dkt<N*.Y7tz*5_g?HpBtZ%>*izk5kq5y%


fit%D$:--.it -- gYlG%du-%-q.it%DD%KD

%%-*?* dff
gfi ff0ff! %ff6 %5ff| 26ff %!%* $6"!g 2 ff
# %
ff $7ff $kff &%'(
<ff )?. * yff
+*g. ,
<$ % 67ff .
<ff /%ff )7B*ff 0 1$ ff 0ff ff 2"34!N
5
# %
ff $7ff $kff &6879ff9:7<;lff =
*Nff A$~ >=6? q@ %ff )A34!N B
5DCr
< &
6(79ff9FE@GIH+
JK$MLff6
79ff9ffN ;K$O0P
< *Q /$g% * A:R| KS
J ff $ $ >2*g T:% uff $ UuV


<ff
). * W q.$ X6AB M6 ( # * AY6 %Z
7ff [6* 5ff
\%/
B 6 $
@ u@ >:ff 2]M ! Nff
+$
< $ S6ff 0 % *Z^? \ 7._ !ff ?> @0? ff 2* O0
%` *a
T@ 8 .$ * U ff 6ffb0 % , 6c
@B )ff
$%B
*g. ,
<$ %b
$.$ . $ # *
EW...$
6 (%r :$ ?]
B<*g Nff
Z@ N $$ ff K$ @0b
!ff Z
Mu %ff [.Z

J x* 0
ff
6 @0@ # * feb)%< *]S .%1
*tff KMBff 1d!Z
[-* :>2
<ff .0 % @0V*g. ,
<$ %

$. * $I
<ff ($ ).ff )Lb
$ ff 0ff ff A(IB >:% 2.g # * ^N"? M0 6 (
. * $V6
Cr
<
?ff W
H+
4K$MLff%':ff$M*g II7ff Ud.3,C;
W
5hHi
J))$MLff679ff9jU;K # * 2kz$ 6
. 6l
*tff Tff
$.%
nIoWprqssgtuXv+wSxzyzqy|{8q}h~&xiff@fvbIffqxSy(qyv++A v+ ~&}h
3J$ ffB * ;q6}*ff0 c$ ff0ff ffD $)MNff
*Nff
8%P
<ff )


]$_ff<<ffK
*]% z% U) ff z$ ff| )* d6 6`gnff6s% +$ ff| )* 6 AUff I$K6
$ }>^%b.ff+%0* m)0 &$%bM>B ff %ff6]%0. }>-
&ff..$ 23J*tT
% 0 * _;K6io
%%
ff6 P.ff) 34MM $0;1%

0 * &%
ff V U?*$[*g[*]..86 !7,$67
.ZK%
7
W6 W $ Uu
W6N
ff
$ ffB *
$2 *6:
ff!
>A%
1u
ff

._0ff $
-$ff$
ff
I% PK% 7
:
}*ff0 1$ ff0ff2 ff2Y "
? c
> $6&
\<
ff /T
<z
* *$
0 " $ 6 * fi-@ B 66@6"<
ff
6
$ff$ W34N
! i5;
C

&6_79ff9FE;.
ffu
[K
$A
Bg
ff
i. k.$MM
? z%
ff[*:/g
* [7
` ff 7
ff:
b
ff
6
%fi.ff0 [$ ff0ff2 ff2b
? i|fi
%%
z37<l
; ff$*t W6b7
ffUd-3,r
C

5
H+
JK$MLff6+79ff9jU;KZ
6 ff$
:6 $* $
ff
(M>*V!
%+65
ffU6-ff
a3g
;
? )* $
ff
Xg
* .,<
$
%
67
ffc34( L>:?
$$Tg6$79ff9:7<;K@
]6
i$$ ffK^ ffy
B?:*
* yfftffy
^*
$
7
. c
ff
8
c )M
H

$S )M.M K$ rffM*t /6@0
Pc
> V *ZV
ff
+%
c<
ff )
>|`
? PN%<
6 >
5d6 &X
X+ ) <
ff Z6 7
BU7 FM 0
b>fi2Y
6 $
> ^6z
ff4

6 2 /

ff$Y7d6 \
%<
:$
$ %
)!
ff ff6$t
* P k&%
" .r ff%
cc
> *@0Wg
* K
>
% 0 6 $ $*
J rA% W!
%
ff6 $db $6 )r
ff
zy
ff
d%%
ff6 [1*]Fz3JU;K
O0d67$>-F=6 AYff
8 *.*]f%
l?* !ff$$0@
l 4WMff V
fiI8`JQi
" : _ ff J ) KM

" 4 [+: ff <P cU ff

4J :VU ) b B


h 2 - : J 4 P ffff,$gAK `z3JU;
fiI8`JQi @

/Z B J 44ff F : z3 ; U
ff :B2
BM 4 B
]
-

ff

fiB@ff"

4KfiF)FSgJ<BJJ@bK@K/V$+,-J:.bJJ

fiff Kc 4 MffM!V#"cB4M%$W'&(\f)@Mff,(*,+

F-J.I0/

*1+32"547698;:72'=<>4
"

b@).S"B?@r A"B6C* + 2"54

DFEHGJI?KLMNE,OGQPM;R0SUT%P?KWV,XKWT%R0K3SNYZM\[#L]SN^NLMN_ X%T%`KLMNT%P?abKL P?KI(P?KW_cMNTfiI]GQRPdN`KWT%SNI?KW`eOgfih :kj,2'l4
GnmoOGQP3R0SUTpI]MNGHTKW`qGQT
MNEHErMNT%P?abKL@P?KI]PsSNY et
usv K>P?KWR0SUT#`
_cMwSNLsP]KW_cMNTpI]GHRPxYSNLxK0y1I?KWT%`KW`
EJSN^UGQR>[%LSN^NLMN_cPd,azKWEHEn{|YSUX#T%`KW`}P?KW_cMNTfiI]GQRPdGQPxMNT
GQT v KL]KWTpI]EQ~qP?NK[%I]GQRMNEzP?KW_cMNTfiI]GQRP=I v MIL]KYL)MNGQT%P;YL]SU_`LMa@GQT^
R0SUT%REHX%P]GJSUT%P;a v KWTKNKLI v KLKcGQPM
[-SNI?KWTpI]GHMNErR0SUT1GQR0I t%usv K=SNLGJ^UGQT%MNEZYSNL_X%EQMI]GJSUT}SNYazKWEQEJ{|YSUX%T#`KW`P?KW_cMNTfiI]GQRP3YSNLs^NKWTKLMNEEQSN^UGQR>[%L]S{
^NLMN_cPz,~c;KWEQ`KLWdp@SUP]PbMNT%`1R v EQGJ[%Y 2WN14 GQP #MNP]KW`cSUT}MR0KL]I]MNGQT[#ML]I]GQMNE#_eS,`%KWE tp L~,_X%P]GQT#P?,G
L]KWR0SUT%P]I?LX%R0I?KW`
v GHP3`K0T%GJI]GQSUTGQT{|MNEQXKW`
EQSN^UGQR 2' L]~1_X%P]GQT%P],Gd WNp4)tZu@v K;YSNL_X%EQMI]GQSUT}X#P]GQT^
MNToMNTfiI]Gn{_eSUTSNI?SUTKeSN[-KLMI?SNL=axMNP;#L)P?I\^UGQNKWT5fi~bML)MNE MNT%`1X%LM v _cMNT%GQMNT 2WN14 YSNL=^NKWT%KLMNE
EJSN^UGQR[%LSN^NLMN_cP;I?SN^NKI v KLla@GJI v
R0SNL]L]KWP?[-SUT%`#GQT^`%K0T%GJI]GJSUTiYSNL`KYMNX#EJI>EQSN^UGQR tusv KeP?I?LMNGQ^ v I?YSNL?{
axML`K0y,I?KWT%PGJSUTSNY7I v GQP\YSNL_lX#EQMI]GJSUT 2 L]KWP?[-KWR0I]GJNKWEJ~Nd-I v KL]KWP?I?LGHR0I]GJSUTSNY(I v Ke`KY'MNX%EJI\EQSN^UGQR`K0T%Gn{
I]GJSUT 4 I?SK0y,I?KWT%`%KW`qEJSN^UGQR;[#L]SN^NLMN_cP3I v MIsa@GQEQEZ-K=GQTfiI?L]S,`#X%R0KW`
TSaabMNP@X%P?KW`
,~}P]KNKLMNEkMNXI v SNLPd
K ^ t2 xMLMNEr>KWEQYSUT%`d WN% GQYP]R v GJI?Nd WNNU4)tn3 SNI?K;I v MI3GQTcI v GQP7[M[ZKLzazK>a@GQEHESUT%EJ~cR0SUT%PGQ`KL
v KEHGJI?KLMNEQPbI v MI@MLKI?LXKGQTI v KR0SNL]L]KWP?[-SUT%`#GQT^e{|MNEQX%KW`P?KW_cMNTfiI]GQRP
GJNKMNT%P?abKLP?KIP]KW_cMNTpI]GHRPeI v K
abKWEQEn{|YSUX%T%`KW`P?KW_cMNTfiI]GQRPYSNLeK0y1I?KWT%`KW`EQSN^UGQR[%L]SN^NLMN_PcGQP
#MNP?KW`}SUTI v K>SN[-KLMI?SNL *,+ SabKNKLWdI v K;SN[-KLMI?SNLsGQPxX%P?KW`GQT}MI?SNI]MNEQEJ~`%GJmZKL]KWTfiIbabM~ 1GQT%R0K *,+
GQP\MNTpI]Gn{_cSUTSNI?SUTKI v KY'X%T%R0I]GJSUTi +62*,+(4 GQP\_eSUTSNI?SUTK D>RR0SNL`%GQT^I?SI v KYMN_eSUX#P\T#MNP?I?KL?{
u MLP?1G3I v KSNL]KW_ 2'u ML)P?,G|d WNNU4 KNKL~_eSUTSNI?SUTKSN[-KLMI?SNL v MNPMEJKWMNP]Ie%y1[ZSUGHTpI t(usv KqP?KISNY
abKWEQEn{|YSUX%T%`KW`R0SUT#REQX%P]GJSUT#PcSNYM[%L]SN^NLMN_ dz`%KWTSNI?KW`l 2'l4 dbGHP`K0T%KW`I?SZKiI v GQPEJKWMNP?I
%y1[-SUGQTpI\SNYz +zt-usv K%y1[-SUGQTfiI\RMNT5-KM[#[%L]SUMNR v KW`5YL]SU_-KWEJSa,~iGQI?KLMI]GQT^ + SUT5I v KKW_e[%I~
P?KI t% TRMNP?K GQPbT#GJI?K;I v GQPsGJI?KL)MI]GJSUT
GQP3^UX%ML)MNTpI?KKW`qI?SMNR0I]X%MNEQEJ~}L]KWMNR v v K;#y,[-SUGQTfiI
usv KGQTfiI]X%GJI]GJSUT-K v GQT#`I v GQPsX%P?K=SNY v KSN[-KLMI?SNL\GQP@MNP@YSUEQEJSa@Pa v KWT%KNKL *,+ GHPsM[%[#EQGQKW`I?SM
P?KI3SNYEQGJI?KLMNEQP " 1TSa\TI?SeZK>I?LXK=GJIb[%L]S1`%X%R0KWPbI v K=P?KI3SNYMNEQE-EQGJI?KLMNEHP7I v MIsMLK;P]I]GQEQE[ZSNI?KWTfiI]GQMNEQEQ~
`KLGQM#EQK D@[%[EJ~,GHT^GQIcI?SP]X%R v MP]KISNY;[-SNI?KWTpI]GQMNEHEJ~`%KLGJM#EJK
EQGJI?KLMNEHPcGJIe[%L]S1`%X%R0KWPMP?KISNY
EQGJI?KL)MNEQP1TSa@TI?So-KI?LX%KNd SNYI?KWTEQML^NKLeI v MNTI v KSNLGJ^UGQT#MNEzP?KI "t ,I]ML]I]GQT%^ia@GQI v v K
KW_e[%I~
P?KI@MNT#`GJI?KLMI]GQT%^cX%TpI]GHE-I v K=%y1[-SUGQTpI3GQPsL]KWMNR v KW`qI v X#Ps[%L]S1`%X%R0KWP3MP?KIsSNYI?LXKEHGJI?KLMNEQP t1 I@RMNT-K
P v Sa@TI v MI=KNKL]~azKWEHEn{|YSUX#T%`KW`R0SUT%REQX%P]GQSUTGHP>MR0SUT%REHX%P]GJSUT5X%T%`%KL>I v KMNT#P?azKLP]KI;P]KW_cMNTpI]GHRP
KWEQEn{|YSUX%T%`%KW`
P?KW_cMNTfiI]GQRP@RMNT
v X%P3-K=1GJKabKW`MNP@MNTM[#[%L]SWyGQ_cMI]GQSUTSNYMNT#P?azKL\P?KI@P?KW_cMNTfiI]GQRP
TYSNL]I]X%T%MI?KWEJ~GJII]XLT%PSUX%I v MI YSNLz_cMNTp~[%L]SN^NL)MN_cPI v KsP?KI SNYZazKWEHEn{|YSUX#T%`KW`R0SUT%REQX%PGJSUT%PGQP
K0y1I?L]KW_eKWEJ~}P]_cMNEQEZMNT%`}[%L]S,GQ`%KWPzMNKL]~[ZS,SNL3M[%[%L]Sy1GH_cMI]GJSUTSNYgMNT%P?abKLsP?KI3P?KW_cMNTfiI]GQRP t# SUT#P]GQ`KL
v KYSUEQEJSa@GQT^[%L]SN^NL)MN_ v GQR v
v MNPsMNEQP?Se-KKWT`#GQP]RX%P]P]KW`,~
xMLMNEMNT%`i;KWEJYSUT#` 2WNfi4
4ssff]
U43eff]
4@ff3
)N1|s);g|b'k|W?;30|)=)ZQ?=s||||x|()?
)|s|@|?|(|]b'|)0|)3(-)0]|)fi13-?|b
?b?| )| | ?>N |W?\;)k)|)fi


fi7Q}zpU#,p

z%U%#piQ}z11#bsn


fiff !!"
"$#%!#&!ff(')!ff'ff *,+!.-/ff('!#% 1032457689 :;!<ff=?>@BA!(
fiff( C<D' EF<ffA
<!"G(
$=H>I@IE( "J!#%fiLKNMO#%P(<'!ff,
EF!) O<QR<7ST!'ffU'ffVff(
E(+JEF'ff('!WXffF'!#% AC'P(!')(')Y )-AN(

#%3Z['&#%\] .^ _89<!"a`8!<ff
(!'&
W, b"
,'(acCdL<!"adeS
J'ff9+!E(\J *f<QEF'ff ff9!
E(
g)e'
W,#%!"J')(')!ff
&"ih
jQS<X#%*,+[) *, P(<QE(-k+[<')El') EF<&ffl'ffL+!EFmYQ<Q\J) UnE(*o(! V*R
!'#V#%!P E(+J<QE((ff1^(

Ep!) ff9C<V+!E(WEF<*qK,A!<!"
_;S(! E( fi'ff<Ql) <ff
r+!EFPTnEl <#F(
r#%*,+[) *, P(<QE(-/') EF<&ff9
ff gEp!) ff<QE( U

"! n <Q "s\P-ut1v?5wKyx8pAi
E( ,Kyx#%!ff('&ff (ff1^(! {z ff( EF'#%(|$EFJ) ff1'{K,AD'7S} S)Ai(
ff ,l')(


! W<Q(')a<ff<'&
E(
~ r(!'&ffN#<ff !!"! "rff *R<P('#ff#%!#J"
ffN['Ttyv?5wKyx8pS ~ Tff(!!"g\ \PY3')!ffT(!<Qff(!#p
<Off(')(!<Q('){'ffL
? Jff L<EF<QE( V'*b')('
WR#<ff SDNO(
,#% EF<QEF-A'L#<\ fi %3+ #% "(!<Qy*R<#%*R*,Jff !ff g3
e) "
W y\J<ff( ff'&[W'Y gEF'ff ff(!#Fa!!"
ff('E( "O\] J<Y3')
ES!
EL'!ff (<J#% A!<ff
ff(!*R <g3
ml) "
W \[<ff l#%P(<'!ff^'
EF*R<Q('),(J<Q?\J')Ep"!ffH
EF*b<)-fiZJ-,<!"R+
W!'&!ffC
EF*R<&)"
i}AJ %;+!EF ff(ff "$<ff(! rff TWE(!!"$'&!ff (<!#% ff(
y)e'
WVEFJ) yff(#F
*b<Q(<3h
jm8ni58C[(cCi58pFdp>[5i8
_8cC5i8C[i58pw]vJ>v?5i8
ff(ff(!*,
EF(
E?(!<Q(
l;
e) "
W l\J<ff L#%(<'Jff?(
L'
EF*R<Q(')R(!<Q^ .-b'ff?<r+
W!'
\J')Ep"iS
e')
')(
ELni5n1m@!89
ELcCi5nLe@
89nmffE(*ff( EF'#%EFJ) ff'X(
g3
e
"
W ,\J<ff( U R<QE( R'(
,ff(<*R Rff(')(!<Q('){<ffyl')({KNMQh !!"
"ff *R<P('#ffg"
; ffy
y"
EF<m
<P-z "
<ff(')\J |fi#%!#&!ff(')iA;'7S} S3<U#%J#!ff(')O"
EF'Y "bE(*<UEF!) l')(^ <QO
W<Q(')$'(

\;"!-A3<QL<7S
19<9 ,ff(
e(J<Q9<V*R'&
E?E( EF*U!<Q(')(! !3+'P?+ EF<Q E#<XmY EF#%*R y(!'ff
'P ) EF<Q\J L <Q3
ff(ff<J"/) <"!ff *fi!#p$\ EE( ffF!)(ffSJ!ffF'"
E(
y)e'
WV+ EF<Q E
0!2 5u8Hty 5wKyy8

EF fity 5wr8^"

ff(! g*R'!'*b<]ff( T' EF<ff#)ff "a!!"
E9(
5w#&<ff(ff('#<n8^Ep!) ff9VS[ty 5wr8
'ff9(;!ff' fit1v?5wr8^')(!
9(
gE( :;!')E( *R 9C)W'#<D#ff "!
ff(ffS!e"
%[!
2 5{8H0325n0!2 5{8 8
W<'^ V') EF<Q V(
U *,+!I-kff 1 O\J(<'a(! fi !!"
"k#%!#!ffF')!ffl?<b+JE(WEF<*oK
!'&#FX g'"
! Ufi 5wKfi8pS
Jff('"
EH(! l % #%(ffi(!'ff?*R;"!')[#<Q(')R
E %3<*R+J) KTMQS0 2 4 5768?mJpcCJFdmS
! rjm8
'ffy#%P(<'
"u'(
Xm[pc[FdmE( "!!#%g9KTMU<J"{(P!ff 2 4 57681fedmSN3'!#% Rdfi'ffy<ff X(
,!') Ep<i#%(<'! "a'/<&D<!ff Eff (fflCKNM1
EL<Q+!+!E(m3'&*R<Q(')$<#%(!<-X#%'&!#'"
ffl')($<!ff E
ff ff( *R<('&#ff'X(!'&ff#<ff
~ (
^ .- %
<*,+J) \(i5nLe@
8<!"cCi5nLe@
8<QE( +!E(eY<Q\J kE(*(

6eE( "!J#%C(
g3
e) "
W fi\J<ff S[lm EA](!'ffl!<ff
'3Z[
J#% ya! (
El<,Ep!) r
l#%3
(<'!'&
WV(
y^ <QX
W<Q(')aN! yN(
ff( yI') Ep<ff9'O(
y\;"
-O'ff!ff "X ,+JE(;"J!#% L0 2 5768
Eg
Si
U % #%y?(
V#%3Z['#%('&
WO'&
nEp*R<Q(')<Q\]!L I-]ff1ZJ-3'
WO<Q\J''I-/'ffL(;!ffL +!
)3#<?<!"G"
; ffr
V!<mY (
O"!'ff(<ff( E(!ffr#%Jff :P! !#% fffi'fi!<ffU'(! bEF')W'&!<CEF*fiJ<Q(')
!!"
"Xff *R<P('#ffS


fi
y[
fifi
V!!y (!QU(

,
!FQ Vg;!)Q)Pr k(
p)!

Rl

rq
Pr
r%P(s
Q(sgw!(u(Jg (
,/(J!%g,D
Q(J FQ(,s
;!)Q)P^ ,!O(J?F $)9
P9
RQgPJ]F!%ll
(

u! O;U! fi(!OFQ , {XQ!J)sJF(U(
$
%[J)()L^C
X(,$
;;)J() F!O,!(pRU)(

,F(9
Q()iC,(!p!(FRby

!(3!!%L%,J)R(Q(J)F^ F!On9(!^( X(!1[ !
(9%!J)()
9J()
$(

p g(!r
FQ J(;J!%,(r%J!()J9(!$(
yF!

gJH[mne[CF G P
fiff ff ff w
!" ff#$ %I& '!y)
(^
* +
- ,/
.10 ,/
.2
gi4 36587u/!O! ,9
.:0;- ,/
.pC(;!V ;=<?>%@BA0f ;D<C >%@BA(F E
((!fi(
X((J)fi)e
RR!Q )HG
lFR! fiF
mk(!Q1(
U
FQ 1!F;!J%l
O!PP ((!)(J7})](!Q

! RP(k (]y;^/l/Q!!FJ
RQ()XC! l RP(
gJH[mne[L
KT FM 6 I& N P?ff QCNS RU TV,w$ .UFU * $ !B ff#W %I& '!Q6ffYX
Z [] W\N \ ]]!=(^ 1![ !Bff# _^a`$b c!-6ff=( (FBZ d]!=(^ 1!? !& QJ &!6edwJ fe]^a`$b ,wg .10
RlU Th,w$ .
gi4 365
1!(()() F)3),(Q(&JX
!9J
,! Q7;

ikjmlD(!(&
)^J ^ TJ, n;1((!,]F ( Tl!3,!.r TJ%!(( !
(J(
g!)(RJ
V(()J&)I,%FJ
R RF(!)(o
,Zp1)nJ
qsr(p!) uJvvwx.p
N(
ef(!Q,) FQ(
u G(
$,JI(R!
,!(3!!%Ou) F- Tz{ RU TV,w$ .fi)
(3O% b(
e(!Q|
0}RlU Th,wg .,Ji ,/
.10}RlU TV,w$ .p
r:
R /Q(J FQ(J ^O J ((!R/ 0~RlU TV,w$ .pB n3&!%X 0~R k!m
W 0 @U] n3&!%;bd ((!R!() R^%J( P^1! R1 ,w .10W ,wW @.p3
((
,9
.k1 ,w$ > .0y ,w .FLRrU G
E
y(!fi((lH(!fiJQ1b
FnFU!Q(a(
p!!l Xfi%P!)P!( k
!(&
b(
,,! !U%! FJQ((1^J!
Q kp!)l^V&D(a(a(
VF!Dp!)
!r I; !fi(

%[!)(!
(
?.^LFQ FC1bJU
%p!
)m(PJ)F
9(!QTQ;)

Q a!(%!!()!^]y
% iJ7})
$Q(J FQ((NFJ)9)(OQX!Q()
k
%[!k1 ,w$ .N ,a ,w$ ..R!W ,wg .: , ,w$ ..p 7u
e ;!)Q)(
FJQF% FO tr;!XJ ;O(
g;!Q()!
,/
.1 ,w @ .
,9
.W ,w @ .

Fg @

(
g F!

Q /;O
(,^V(
pk X(
V (Q(Rg^!((!%LUR F!Q()Rp!QF% FO tQ(
C J $a(
y)ml
,
()i
hTne[
N1 F/
N?ff $ey !M ff# wI !|=V N c!b
Xm!&# (FBZ
{ bFRP`W @ ,wg .m #-\!WffB #6 /& g! ,9
.ff e96 &_\+ J %e #- { ; <C >@-A
&

fiY]Y4xSfN__41"Wfi]-]fN]xx]4Vx
OYf-Y&]O1&V_OYON6Y &6O&$H"[fODP&
B[9kD? %B k %B kZP g
fFYckfF[6gYfOQ4kPff_Q_YO[$c&JxY6J&fJO]P]OQ 6\J]J6
fcd1JJ]J
:YYYOY6&\&J4J"]JVYJO
9m:YY_4
6]&PYY4OY&[&JYJ1Y&Jdk4O_YJH4&\&6JYf][6fVff&OO
f4Q:1h]
Yc]:ff&kf41:YD:VJY]:cYU9N]
Yf-c m1JdJ&
"1:4
&-6YfYQJff
&Y&gPOo
4fYYO fi
d4f
6&\&"4f&Yc-6YOJ&
"1YMdJ&f fY V$dY"&NW64OPYJ--YQJh1Q&YJJh
4&\&6JYJ"Q6YOJ&
xYfOJO P]& 4 16YOk4QJJY]6YOOWY fUY&[&J
D& ]64OOYQ
Y6f6OO&JOfMY66 fW fi ]&6 f&W16YfJY fi
Yc[4YOH!$ Y"Y]P_$? Y1OcxJYYJQJYf"]x Ox6&YJJh
W YOO YNx V&6f-JcOHx1P6YQPY xcYc"Y4fYYJ
]66]J

#
%$'&(
#

)
%$

*
#
= &(
#

]&6+
%,P646Q&&6\YJ&.-
Yf]Q]Q4OJM1 0/ JxOYJ
6YfJBV4cf&OO
!"Y[4YO fi f4ff21JOYc[4YO/Yg3Y
YQ 54

QMYg YOJ f 6-PQY]fU YOJ ]]JJ6cOYJ Y6!]$6J
f YWYJffYO
4fJ OQf/7c 6Jf]O/Q"]&\J6O4Og6YOJJdZ O? YOJ
Og1JVfg]8JYJY4OOY&29]4fSJYJF\k6f6YOJJZ O6YOJfgYc6
]Q_d:<;=]VJ]YdJJd10/ ?Q]&JYJH J64Yf&F]x1OJDVcQ
6YfJWYYYJofY$]:4B6YO66]JOo46Q&&6$[6YfYQJg]hJ$]
]JJ66fOMY66f4OYkM1YfJVYOgYQJ \f6YfJ1MYcYD
/Y46Q&& O&J4Q[$J 6]JYh4JY]POf4OQ&>

YQJJ
]_4OJk1P6YQ"4?
f.fJfQff&OO9 f$Y@]-]P[Y4O fi
fQff&OOh9"
,A'& fiBC8D8D8DEfi FB G;='H C8D8D8DC <;='HJI
_YJ"Y fi x AK& fiB88D8D8DEfi FB <;=LH 88D8D8DM <;='HJI k ,
N8OPRQTSVURSVWXS YZQJ[WM\^]SVQT[V\`_aXJb[VUaXJ]cQJd2] e6UYZQJb`b^QaefSVW2U_gQJdCh@Ugd8SV\`QidLSVW2XTS SVW2URjk\^dM\`j'XJb[Vl2b`Ug]XT[VUSVWMUmM[VUnYZUo[c[VUapLQid2Uo]gO
qEr

fis:tuvxw<y
z{|~}@{ C"3C"}@%?L6<3a6C{"|{|TL"aaC{RB|T3{|T38C3{}~"|{|J
KC"88|'"'"{ C'"{'3Ca{""8|Ja"""8
:8{| a|{""}88@<B33{"%?:{"a{6{ {{M?8
@B}83"|T88|CB}@{|T3{+a{6"C0B~|K"B|{}T{3{MKkn?{B|T"}83<3C8
':x'a{@aC":CZV{"""CC}88"c:x: k"'88|%8}@{"}8B3{
|TC"k}@{B}8"3{+"|T{|T338C+""|T{}T0
'::: a{""|88|C"}@CRa{~a{3}@{<}@KC"88|"6{3BK3{{
B|T"}83<3
8?"RB|Ta@B%L"?LC>}@{G}@.8|L"3CEGk}@{G}@xa88|J"C8
g"~TB3C:}8axn?{|J"3CC"C}@{B3CC |+3a8|T:C3|kCB8}8G"
6@V}@{<}@8L:{<}@+{>"+6C|CB8|a8{:CZV{""C}@{"}8B3{"
"}@{BaaC 8B"{{?C}@CT|T3{a{ {|"a"}@ffB|C}@{""3{{ {L{ K|J"3C
"{8|T3B3+{||J"38CaC"}@{<}@B}833|C{3C0g"}8af
"|88|C"}@{|J3{+KGTB3Gx8C}@aC06:{B8|{3{MK"|T{|TC
kGk
.<'

8|Tff'n 6@Vk}@{<}@'68:8C K" BL{ff@B}83k"|88|C"}@%{|T3{
3C"|C}@CC"}@a{ ?: a{K"<3 8|T.a{K}@{BLK~B|T0{ {|:{K"|T"}83B3CC
C}T""}83BffCB%B%:}8"<3"|88||C|T"{"33k:+|T|">`
"B}83{}@B33C"?a{%"3{K8|3a8|T?8Cff%}8"{>3{"8|?6L"8|T3C0
"x{{EK">n?{|T"3C:@"3B"Z68|Ckn 6x{}@{<}@8
Gk
'<
:C"{"Ca:|T"C|{}@{B3CC|L{E:88|C"B}8{{"{:|T"8C
L{8|:B}@'8|TL}8""?ff"|C}@.6@V}@{<}@80}@{|TaKC8|TG}8a
8C{}@{<}@"|T"ffCf6>B"3|C}@8BV<BCf{>@aaC"}@{"B33{"
|T"C8%EK"+ |n 6@V}@{<}@'|8"8'{|T"3CkZ
aff:C38|K"8CK {|k
a#J8C' BB
a# 8C'
L8|ff#""|T8aCK#fCB"3|C}@kn 6@V}@{<}@'k B'6@V}@{<}@k'|T8"
C"na8K{|T"C8 x|TB3a8K"K"E%a{6C a{}8}@{" {|':CZV{""C
aC }8BaC"B|{}T|K<a8{G'|T"CK"}J|K"""8CaC >Ka8{63a8|T
{EK%a{~6a|TL{a:BR"?n?{~6CR{B}@{<}@|{"a{ 8Mc2:{~|T"3CRCffG:
}@{<}@.{<6{n 6@V."G@VTJ :{B8|K{{EK"B|{|T T3 .{BZ<}83{
{.8
C

fi?""BfR<<?<<B:'Z<<?2B

fiff
ff ! "#fi
$ff&%'

(*),+-+/.103254.626798 :;.=<?>>@=)BADCEF@=7&G;.79+H..5A
2A3I KJML )N.O:3NP.5@ERQS.5TU8VW2XI3EUNP.5@=7&798 :;.=<?>>&2A3IY2
7Z8[:;.=<?>&@=)BACER@=7]\*70327^ERQ_A3)7^QS)BTU42G`TU.aG 870.1E L :TRER@EU7 L .5@P032A3EFQ L Q)b/+H.5TRTU<?bc)Bd3A`I.5IeQS. L 2A 7ER@Q
2TU)BA.f`>97gEFQ703ERQ"hDERA3IK)b@=)BADCEF@=7fi70327fi+/.i7SN8j7S)6Q)BTU4.^G[8j703.^.=k[:TRER@EU7":3N.bl.N.5A3@=.^EFAbc)N L 27EU)BAWf
>mAn)BdN/.=k2 L :`TU. +gERTRT3G.&d3QS.5In7S)aI.N]EU4. f[(g)7S.70327"A),+o70.&2:3:`TREF@27EU)BA1)b I.bc.527Q

2A3In70.N.gERQpA)iI32Aq.NH70327H2rTREU7S.NP2T`I.bc.527EFAq L EUqB0 7!G;.5@=) L .&I.NPEU42G`TU.TR27S.N5fs&.5A.NP2TFTU8V2
7Z8[:;.=<?>>t@=)BADCER@=7uG;.79+H..5Awv 2A3Iav J +fiN7fQS) L ."d3A3I3.bc.527S.5IwNPd`TU.5Qt)b`70.":3N)qNP2 L \+gERTFTBG;./Q)BTU4.5I
ERAnbl2,4)BdN)b70.&:3N.bl.NN.5InNPd3TU.V[Q2581v V )BA3TR81EUbW2:3:`TU8DERAq^v .=k@TRd3I.5Q/2A8nbldNP70.NH:;)BQQPEUG`ERTRER7Z8i)b
I.NPER4[ERA3qa2Axv <ZI.bc.527EFAqnTREU7S.N]2Tyf
(*)7S.n70327i.4.N8Y7Z8[:;.=<?>@=)BACER@=7^@2AzG;.O7dN]A.5IzERA 7S){2{I3EUNP.5@=7_7Z8[:.=<?>P>@=)BACER@=7_G[8e2 J A)BAD<
.5|[d3EU42TU.5A7}~\fiN.N.:`N.5QS.5A 727EU)BAe)b!70.aNPd3TU.5Q ff EUb!.52@]0Y@=)BADCER@=7EFAqnNPd`TU.ivjERQgNP.:`TR2@=.5IG 8EU7Q*Q. L E<
A)N L 2Ttbl)N L $ 70.5Ae2TRTu@=)BACER@=7Q&G;.5@=) L .r798 :;.=<?>>*@=)BADCER@=7Q2A3I2N.w70 d`Q*2 L .5A`2G`TU.a7S)K@=)BACER@=7
N.5QS)BTFd7EU)BAK70N)Bd3qB0X:3N.bl.N.5A3@=.^ERA3bc)N L 27EU)BAWf
bc7S.Nn703ERQ L )7EU427ERAqYI3ERQP@d3QQEU)BATU.7Od3Qw:3N.5Q.5A7w70.XA.+I.=A3EU7EU)BA3Q5f
dNw7SN.527 L .5A71)b
:3NPER)NPEU7EU.5Q&ERQG`2QS.5Ie)BA2K+H.52h.5A3EFAqX)bH703.1A)7EU)BAe)b/{<ZQ2bl.5A.5QQfW>mA[.5@=7fuj+/.1@=)BA3QEFI.N.5IY2
NPd3TR.nvx2Qwx<ZQ2bc.K+g03.5A.4.Nw70.N.KERQwA):`N) )bbl)NO2TFEU7S.NP2T/I.bc.527EFAqvbcNP) L 703. L )BA3)7S)BA3ER@
@=)Bd3A 7S.N:`2N7Q^)bHx<Zd3A3I.bl.527S.5IzNPd`TU.5Qf;(g)+ERA70.1@=)BA 7S.=k[7r)b/2K:`NPEU)NPEU7ER.5ITU)qBEF@w:3N)qNP2 L +/.
+gERTFTW@=)BA3QERI.N*21NPd3TR.^v12Qgx<ZQ2bl.rEUb
70.N.aERQfiA)jQd3@]0{:3N)[)b
bcNP) )BA)7S)BA`ER@i@=)Bd3A 7S.N:`2NP7Q mw
]?ce3=5 )b703.^{<Zd3A`I.bc.527S.5IxNPd3TU.5Q5f3fi0.^QdGQS.77S)1G;.^d3QS.5I{I3.:.5A`I3Q")BAX70.^N]d3TU.v12A3I
@=)BA3QEFQS7Q&)bp70)BQS.wNPd`TU.5Q&70327_2N.OA)7XSI) L ERA327S.5I`6G[8{vDf;>A 7d3EU7ER4.5TU8VvtEFQ&I3) L ERA327S.5IG[8xv6EU
v ERQ Jm \ph[A)+gAj7S)wG.TU.5QQH:3N.bl.NN.5I67032Ajvr2A`I J B\/I.bl.527S.5Ij+g0.5AjviERQ"2:3:TREU.5I17S)q.703.N+gEU70
NPd3TR.5Q70327i2TUN.52I38032,4.nG;..5Ae.5QS72G`TREFQ0.5Ie7S){G;.O{<ZQP2bc.f J B\^ERQ_A.5@=.5QQP2N8e7S) L 2h.nQd3N.w70327
.=kD:`TRER@EU7":3NP.bc.N.5A`@=.^ERAbl)N L 27EU)BAXERQd`QS.5IX70._NPERqB07+"258V2@@=)NPI3ERAqn7S)1)BdNgI3EFQ@d3QQER)BAn)b f
>7ERQ)G 4DEU)Bd3Q70327
+g0.5A3.4.N70.N.ERQ
A):3N)[)b3bl)N
2I.bl.527ERAq_TRER7S.NP2TblN) L 2TRT {<Zd`A3I.bl.527S.5I
NPd3TR.5Q6703.N.Y@2AG;.eA3)QPd3@P0o:3NP) )b^blN) L 2Qd3G`QS.7K)bi70.5Q.YNPd3TU.5Qf"gd`TU.5Qj70327X+H.NP.Y{<ZQ2bl.
2@@=)NPI3EFAq_7S)_)BdN!.52NPTFEU.NI.=A3EU7ER)BAr70[d3Q
N. L 2ERAw7S)^G;."{<ZQ2bl.f*.N.g2N.70.:`N.5@ERQS.I.=A`EU7EU)BA3Q ff
X
clo JMwS x \ ]_*M]MMU1BM"SSa a5m&~MZ=S~PWa5
m[R] vw 1"tDHm]DR^BwcZX= vK =? rnZPijXH fiJ v\ c
lDi5

v 3x J vB\ %x J v \H{ 1_SJMo vB\ M5Bl v R
(*)7S.i70327 6X/ fiJ v\ERQ L )BA)7S)BA3ER@^EFAXG;)70X2A`I f3Y.r@2AA),+I.=A._703.^{<ZQ2bl.iN]d3TU.5Q
ERA3I`d3@=7EU4.5TU8 ff
X
cl JMwS x \ P^g]ylMR,POUyHSBma wm*~MZ=S~="tD*5
m6DU=nm *Z{*^* J r\ fiFny!Pfi=lU =p*^* J r\&R ;
DS
"*=g![
vw v / M5ZP{=w^SJM*"jX/ U[J v\S\
5;ym]?fiP]?P
fiff P


ff ]



9? ?m! m"W]P;#??ymP?fi]Z$=mg?&%tm'?(*)+,.-./9

01

fi24365798;:

<>=?@A?CBD?EGFIHCDJK@ML@MHCHNOHP=QSRTNU=WVHCXOY[Z\=WL6=?=WLN^]_NOL`EbaWcdDH@Mef=WL\?CBNOHL=?CNU=WL\g4@ANOLS?hC=iejV]k@!D9L6@"g
Z\=WL6=?=WLjNO]l=m@"hD?=h>n;o"qfp r
sutSv#wxKyx{z;w}|}~"d*`SGu;.{*OfiUC;CW _E
6" W9njo"q p K[*PCuP"{#"

M"9 f*Ik"6

njoMq p {EP}l>>o"p *

HQ@"J{=hC@Ag4@le6@k;L6@>?CB6@`{mjhNU=hNU?CNO"@Me;g@MX^XFJK=WVjLe6@Mef]k=WLj]"XOVHCNU=WLjHP=J\ie6@ML6=?@Me o"p *6DH
?CB6@\XU@MDH?ljim=WNOLS?>=J4n o"q p aJ4DmhC=hDZe6=i@MH9L6=?]k=WLS?CDNOLmhC@"J{@"hC@ML]k@N^L6JK=hZD?CNU=WLD?DXOXNa'@aU
NUJd?CB6@HYiZQ=WX!e6=S@MHL6=?Dmjm@MDhNOLb`?CB6@L@"gH@MZDLS?CNO]"H]k=WNOL]"NOe@MHgANU?CBHCNOLj]k@fNOL
?CBD?d]"DH@lL6=[hVjXU@A]"DLe6=WZNOLjD?@>DL6=?CB6@"hdhVjXU@aW L?CB6@@ML6@"hDX]"DH@6HCNOL]k@A?CB6@L@"ge6@kLNU?CNU=WL\=J
EFIHDJK@ML6@MHHPNOH#g4@MD@"h?CBDL\?CB@4=WL@_VH@Me`@MDhXONO@"hPNOLi@M]k?"aSg4@_ZDMY`BDMR@>Z\=hC@!EFIHDJK@!hVjXU@MHDLe
J{=h_?CBNOHdhC@MDHC=WL=Q?CDNOLGZ\=hC@]k=WL]"XOVjHCNU=WLHd?CBDLRiNOD\nq a_B6@9J{=WXOXU=fig>NOL6`hC@MHCVXO?dN^H!?CBSVjH!=QSRTNU=WVH r
zjPz;fixKyx{z;w~"#*`SG``[.{*OfiuUC>;Ckk.M"! {I"
E[in {Ednjo"q p {Ek

6hC=WZ?CBN^HDLe?CB6@lZ\=WL6=?=WLN^]"NU?IY=JQ=?CB=m@"hD?=hHdNU?JK=WXOXO=figAH4NOZZ\@MejNOD?@MXUY`?CBD?A *[
o"p *a'
@MXOXUFJK=WVLje6@MeH@MZDLS?CNO]"HBDH9H=WZ@"?CNOZ\@MH9Q@"@ML]khNU?CN^]"NU"@MeJK=h9Q@MNOL6?=i=g@MDDLeZN^HCHCNOL6
NOLS?@MLe6@Me]k=WLj]"XOVHCNU=WLjH"a_B6@mjhC=m=WHCNU?CNU=WLHCB=figAH[?CBD?[g4@f]"DLH?hC@ML6?CB6@MLb?CB6@f=Qj?CDNOL6@MehC@MHVXU?CH
QiYDeejNOL6\De6@MiVD?@[mh@"JK@"hC@MLj]k@NOL6J{=hZD?CNU=WLa
H_D`jhH?_HCNOZ\m;XU@>@k6DZ\mjXO@XU@"?AVH!]k=WLHNOe6@"h_?CB6@9J{=WXOXU=gANOL6`mh=hDZ r

# r ! !
r ;
r

@;hH?DmmjXOYn o"qp ?=?CB6@@MZ\mj?IY&H@"?"ac4@MHCN^e6@MH?CB6@NOLH?CDL]k@MH=J9?CB6@?hDLHNU?CNURTNU?IYDLeDLS?CNF
HYTZZ\@"?hCYHC]B6@MZD\?CBD?Ag@[N^Z\mjXONO]"NO?CXUYDHCHCVZ@9=WLXUYflNOHANOLG# o" p *fia@[?CBiVH!=Q?CDN^L
4M>#k_{#_M
@AL6@kT?Dmm;XUYn;o"q p ?=kaTTNOL]k@d#!Pg4@_BDMR@>#!GGW {fial>>o" p *
HCNOLj]k@
*
M#kde6=i@MH_L6=?Ae6@"J{@MD?_lDLeug4@=Q?CDN^L
& _{ Cfi
V6h?CB6@"hNU?@"hD?CNU=WL=Jn o"qp YiNU@MX^eHPL6=[L@"gXONU?@"hDXOH"WNa'@a6_NOH?CB@>XU@MDH?Tm=WN^L?"aW<>=?@>?CBjD?_NOHL6=?
D]k=WL]"X^VHCNU=WLuVLje6@"hd?CB6@=hNUWN^LDXg@MX^XFJK=WVjLe6@MeH@MZDLS?CNO]"H"a
@L6@kT?lHCB6=fig?CBD?l?CB6@mhC=hDZHl DLe eNOHC]"VjHCH@Me@MDhXONU@"hlDhC@`BDLeXU@MeDH9NOLS?@MLe6@Mea
>@"hC@[NOH! r

ff
fifi fifi!#"%$'&&)(*+,.-/fi10203fi'42fi560fi7!8:9.9<;=fi:>?@9-7 fi'A!8fiBCD0:4!E090F;=9G0IH Jfffi!=9K /fi,)0L9
!8279-7LM!K0:!% HN;+20#027fiHM4'K:O4927fifi74fiPJ :74:J7:fi.Q<C;=fi'KR HN7fi,)K0fiSJ/fi49.7/0:9.T!82/9.-/UNVfffiW497!8L/fiXfi'
!K0:!%Y7fi'Z;+27fi/fi[)fi027fi498fi!8Jff9 L/,A!%0897,:HM/fi,)0fi'ML0fiK\:!#/fi:[)fi]_^`9Ba96/fi`027:!1J :74:J7:fiLM9-/
J/J/9.42A97fi1;E9.-7L2'[)fi09;=fi'KR)fiC027fi790:9b9 cb>d!K@fi7fi!8!+fi[)fi -/8027fiX'6eIC027fi1:7/-74X0L[.fif/fiXY70:9.]g/
-7:fifh;=9-7Lb2'[)fi09PVfffif497!8L/fifiZWBafiBVfffi39\i1j\;+27fi/fi[)fi+@9Kkfi'42b;=fiRJ/fi49 :0:9TlffmnEof9 h
p oPrq sft "?ikuwv3xPyzAu+{ |_}~7."h)*8*Xg 9K
p r cTg7;+2/fifi#o\o1: of:!KK09B7A: o+\ff
6

fi\_]1OO? OO WCM@\aO\O?\]7]\_`]
fC]aEffa
+Z<GPOffC
+Zff+bf
?)C` 68_\ 6W ffW DI6 f ?K??WFbI _ A=D_??\\_8ff fafbM6 %

6 8_P 6
+bfD fC+ K
?Cffd M6ff E ` d?D P DID6) \ 6 ff ? DKFD N ??D
\ff ffAM? 6 ff\ff6b?

K K?Zd ?C ?ff)
fC]aEffaDOff#
b\ K f ?W ff ? #AM % ?)b ? DF6 Z '6`\6) + ff +
?a ffA\_ ? f D_ 8_W 6

W +Af) fC+
+Z#AMAD % ?) + \_ ? 6 f ff ' 6D`
D? # ffC? 6 M6ff P = ?

3a +ZfD fa+ K6
% C 6ff7ff ' ff ' ??? K = ?S ? 6)
6 ZC ? D?C6 K?DA ff \?AP? E N?<DID ff ` K Kff D%ff _ ?a 6ff ffK ?d\
w
K6' 6 ? P W ffM ? ' d\
fCff+bfWOff +bf
+Zfffa+Off fa+
?M Kff? ] ' ffDK ' N 6?@ Dd Z dZ ?
X
GC
fiff ff
?f'6 ff G' # C ff< 6?ff6\6 ` \G ff
ff ff? 6 CK 6'6 ?\K6ff K 6\ d?3'Tff K ?D
6ff ` 1
'6D+ ff
ff? ' ? % C k6ff7ff_ a' ??d K N ff 6'6= Kff
Z DK ff
DC'6DK ? DK6 ?)D ff? ? DI6
PD\K6 ff = 6 '
P ?\ 'Z !A? #"W_ DD?ff "P` "$"
%/ '&)( '6D\ D6 dw `` = DKF6 w ?\ E K'6 ?d DKff8
_ff ffD ?\
* DffD6 D?NF Dff? Dff?? ,
'6D\K X ? D6 ? E DI 6 +
d?\ + ffD d\ 6' W
6 ffM ff ED6 w`\6 ? "$" + 6
? 6) 6)S? ?D ?? Dff1 ff0fiE24 36 5 DN298; : 8 _ ff 6 K6)D_ff6 ) K?D \ 6WD ?b 6D ?` ?\b D/ff.O ?
?

' 7.




'

"$"
<>=

fi?1@BACEDGF

HJI$KLNMLNO$PRQTSKUPRQBLWV/XZYW[]\^K_PRQBLa`PRQBLNObQSKTcd/PRQBLfeO4HgKhNHieTjiL`klfimRn6oGprqm9s;tvu's!w)HJxL>I$eORL>h9L>cL>Kh9L
Py`-jJSrMzI!IR{BeTe]`O4PyL>c}|~PRQBLfQHiw)QLNOS{PRQB`O4HiP~[K}`){OzhNSIyLfPRQBLWVBXYQSI$QHJw)QBLNO$S{BPRQB`O4HJP~}IRHJKh9L
HiP$HJIkL>cBLNO4SjfijJS>Mf[
QLSrx
SHJjgS'|TjiLHJKBk`O4S'PRHi`)KZhNSKKHJh9L>ji~_|LORLNeORL>IRL>KPyL>cHJK`){BOES'eeOR`)ShQ[ `_S'LPRQLWL9
SeTjJLfIy`)LNMzQS'P$I4QB`ORPyLNOzM1Lf{IyLEPRQLaKB`PRS'PRHi`)K
z>NNN>R]N u$ NNNNrN u$
SIzSKS'||ORLNx/HJS'PRHi`)K}k`O$PRQBLfO{jiL
!>NNN>R]N u$ NNNNrN u$ N u ;
MzQBLNO4L HJIEPRQBLh9`)eTjiL>L>KPf`k dH[L[ Hik HJIWSKS'Py`)SKc Hik - [V/{Th4QO4{TjiL>IEPRQ{I
h9`ORORL>IRe]`)KTcPy`IyL>HK`O4Sj`O>dHik dK`O4SjcBLNkvS{jiPRIEHgK$L>HiPyLNO>IacBLNkS{TjiPEji`w)HJhUv$L>HJPyLNO>d
> [
L{IyLPRQBLwO4`){KcaHgKIyPRSKh9L>Ifi`kBPRQBLk`)jJji`MzHJKBw$KTSL>caO{jiL>IPy`bORLNeORL>IRL>KPPRQBLO4L>jiLNx
SKPS'O4PRHJhNjiL
`kPRQBLf^zdBPRQBLWV/XZYadGL9_`)IRPyLNO4Hi`OWv dTSKc_fiL9UVB{BeLNO4Hi`OfvV [ QBLEIy~W|`)jJI! SKTc_
S'ORLfeGS'O4SLNPyLNO4I!k`OzO{jiLEKSL>IN
fa>]N) ; >
G]>) ;

-v $
v >


r



G
r
r


BJ ] NN
U
)9) > v
r v ; 9 v

BQ L-k`)jJjJ`rMzHJKw_kSh9PRIS'ORL-/KB`MzKS'|`){BPWPRQBL-hNSIyL}SKcS'O4L-ORLNeO4L>IyL>KPyL>cSIaO{jiL>IEMzHJPRQB`){BPf|`/cB~
vSKc_MbHiPRQB`){BP$KSL
]'

r
>


BJ
] N>
U
)9) > fa
r
9 EW
LNPNIhNSjgjPRQBLS'|`rxL!IyLNP`kTjJHJPyLNO4SjJI[
PyLNOS'PyL>cS'eeTjJHJhNS'PRHJ`)Kf`kTGN ~/HiL>jJcIfiPRQBLk`)jJji`MzHJKBwIyL>{BL>Kh9L

`kjJHiPyLNO4SjIyLNPRIvHJK_L>ShQhNSIyL ] v N







QBLHiPyLNO4S'PRHi`)KeOR`/c{h9L>IbKB`6KBLNMORL>IR{TjiPRI!|L>IRHJcBL>IzPRQBLakvSh9PRI^SjiORL>ScB~Uh9`)KPRSHJKBL>cZHJKPRQBLaeO4`wO4S_[
QBL6ORL>SIy`)KHJIaPRQS'PE$SKcV/XY|Tji`/hRL>Sh4Q`PRQLNO>dSKTcPRQS'PKB`eORLNkLNORL>Kh9L}HJKBk`O4S'PRHi`)K
HJIzeOR`/c{h9L>c_IRHJKTh9LaSjJIy`6PRQBLfORL>jiLNx'SKPbHJKIRPRSKh9L>I$`kfiL9`)IRPyLNO4Hi`ObSKTcL9ZVB{BeLNO4Hi`O!|Tji`/hR}L>ShQ
`PRQBLNO>[ QBLUIRHiPR{TS'PRHi`)Kh4QTSKBwL>I-HikM1LSccHJKk`O4-S'PRHi`)KPyL>jJjJHJKw{TIQB`rMh9`)K/GHJh9PRI-|]LNPMLNL>KPRQBL
jJS'PyPyLNObPM1`-S'ORLfPy`|LEORL>IR`)jixL>c[TYIRIR{LM1LaSccUPRQBLfk`)jJji`rMbHJKBwHJKBk`O4S'PRHi`)K
fa -fa
N'$y4v
;

^49E1
$
;br9$


]


N
v4
ff/

$ fiN


fi "!"#%$'&()"!"()*+-,/.0#12435"*"+6#%$'&(78($9,

:<;=>=-?0;A@BDCAEGFHBDI?KJL;M%MN;=<E%FOQPR?TS9U?TFVW?AX
Z\[

]_^a`b-YcdYfehgjiWkKlmljn-opbqrckKljlsitYfehgunti

Y{z|[

b qwckKlmluitYfexgKn-opbYcdYfehgmiWk0lmljny
v 6
Z ^`8Yfehgho}kKljlsi v k0lmlho~Yexgmy

Y{|[
Y[

z ^` "
v {TRT


IEGPm?WCAQMN?HFEGVW?TMNE%M%MGUPRBRCBR?TPI;=_EGF;UCD;CAVIVW;F"E%VWBD?TPR;M%UBDEN;FPDBRCBR?OEN?TPVCAF@?
PR?TVE1?TQ?TVM%CCBDENA?TMNAA@9QPDE%Q"MN0CAPDPR?DBDEGFO0D?TMN?CAFBD?JL?D?TFVW?TPCAQ;FO0BDI?E%FA;MNA?TVW;F"E%VWB
E%FOQtUMN?TP

f'w-ddT
I?jBDE%Q?VW;QMN?WENBa;AJ=?TMGM1dJ;UF?TPR?TrCAFBDE%VPsJL;AsCwOA?TF?CAMfMN;AOEGV0D;AOAtCA
@?0SUCACBDE%V0EGFBDI?mPDEN?m;AJ q
D;9;AJ=CAPOENA?TF@'~E%BRBR?A??TF

CD?TPUMNB/CBRBREN@"UBR?THBR;rJL;MN)MN;AD?0E%F

q

E%P<9F;=FBR;

c CCAMfs?TM%J;FTA8 n

c TA) n <E%PCAFCAMN9PE%PE%P@CAPR?T;Fs;=M%E%FOQCAF4uCAM%M%EN? P?TPDUMNB

=I??@mPCBDE%P"C@EGM%ENBK;AJ;AtFQVM%CAUPR?TPVCAF@?-BR?TPRBR?TE%FM%E%F?TCBDE%?

c s;=M%E%FOKuCAM%MGEN?T)TA8 n

Fas;=MGE%FOrCAFuCAM%M%E%?T PCD;CAVI4ENBE%P<CAVWBDUCAM%MNHCwE%FE%rCAM{Q;9?TM;AJCr<;AFBDI?;ADHBDICBsE%P
VW;QUBR?TQE%FM%E%F?TC BDE%Q?A)E%FVW?-EGFE%CAM9Q;)?TM%P ;AJ"<;AFBDI?;AE%?TP C??TSUE%8CAMN?TFBfBR;KVMN;PDUD?TPf;AJ
UM%?TPf=ENBDI;UBF?OCBDE%;FQBDI?/D?TPUMNBE%PEN?TVWBDMNjC"M%E%VC@MN?-BR;0=-?TM%M1dJL;UF?TPR?TrCAFBDE%VPJ;AOA?TF?CAM
MN;AOE%VuD;AOACAP)B/CAM%PR;C"M%EN?TP-BR;=-?TM%M1dJL;UF?TPR?TCAFBDE%VPJL;A/?W)BR?TF?THMN;AOEGVuD;AOACAPPDE%FVW?
JL;ABDI?VW;QUBDCBDEN;F;AJBDI?M%?TCAPRB9?Tj;E%FB;AJD?TPR?TVWBDENA?TMN0{

BDI?VW;QMN?TQ?TFBDCDmMGENBR?CAM%P

CAF

V CAFH@?K)EN?=-?TCAP/B=;'E%PRBDE%FVWB-CBR;PT
v
;ArBDI?HVW;QMN?WENBCAFCAMN)PDE%Pj;AJ;UtEN;AENBDEN?TC;CAVIMN?B~@?BDI?HF9Uj@?;AJtUMN?TP

q[ci n PRBRCAENOIBRJL;AD=-CtE%Q"MN?TQ?TFBDCBDEN;F=;UM%Q;)?TM
E%FCAF;UBR?uM%;;A4CAF4BDI?jVW;Q"UBDCBDEN;F4;AJ Yfgs0 E%FaCAFE%FF?<MN;9;A

;ADBDUFCBR?TMNAf=?VCAFVW;m@"E%F?rBDI?wB=;MN;9;APE%FBR;4CPE%FOMN?'MN;;A=I;PR?'@;9EGPm?W)?TVUBR?TCB
E%FCEN;AE%BDEN?T4D;AOACA

BDI?CM%EGVCBDEN;F;AJ6"


Yfgs0 OAD;=<PwQ;F;ABR;FE%VCAM%MN=E%BDICAFp"
OAD;=<P

Yfg<m
?D?0EGP-CQF;F?BR?EGFE%PRBDE%VCAMNOA;AENBDIJL;A/VW;QUBDEGFOjBDI?0MN?TCAPRB


Q;PRBw>BDE%?TP I?D?TCAPR;F~E%PBDICB
Q;F;ABR;FE%VCAM%MN=<ENBDI
)?TH;E%FB-;AJ"
X

m{9f8~
Tf{hE%;AENBDEN?TMN;AOE%VuD;AOACA


qx[ci

fBDI?0M%?TCAPRB)?TH;E%FB-;AJ"


Y{

X [}9

u

X [}9

JL;A/ [

n =ENBDI

KBR;;



N9 {Z PDUVIBDICB
E%JBDI??0E%P/CUM%?
K
l c N9
N9 % c nRn
;9?TP/F;AB/?J?TCB
BDI?TF
X [} {Z tY
X [l c n



Z
?TMGPR?KD?BDUF Y{


ff
fi





?TFJ;A
)





[


fi

ff!#"$&%('*)+-,/.0"214301#25)+6)*%87#9:9<;>=5?221#1!$@52$-9:?A'*BC52'-DE*#'+,F#$@)*BCG#9:HI'*"J=6E&?29:#' #'*)*"JK;
9:B:'&%#1L'+5M=N"JE#OQPR%SKF5T1HM5=U)*%V=5E+;W9:5X5,4B:' @YT#$-?2)+#1Z"J)[85'+)]\^)*B_[8#']"21`)*%-E*V"JE*8"J)a[85'+)
\bE&?29:#')*%2"J)c%A"#Gd)+5LKe$&%#$&f#1^=5Eg'*"J)*B_'+="$@)*B:5Z5=)*%eB:=_;W$@521ABC)*BC5QOQPR%6B:=_;W$@521ABC)*BC54BC)*'+#9:=
$-"QDh"$-$@5E&12B_iI)+5!)*%ME*#'&?29C)*'c5=kj5l7k9:B_i`"A1bm"9:9:BC-E#DKFe$&%2#$*f#1nB:/9:B:2#"JEc)*B:[6o7p-#1
)+5Z#'+)*"JK9:B:'*%rq sutLvuwCxTy+z {wCxTy-|}~S7k%AB:$&%bB:XG59CG#'8)*%2$@5[8,A?)*"J)*BC5n5=]"4[dB:2B:[6"9[851#95=])*%
[855)+5AB:$V$@5?2)+-E&,A"JE*)*'5=3 0Q }OF46)*%#!%2"lG6)+5p#9:B:[6B_2"J)+g)*%8E?29C#'15[6B:2"J)+#1`KTHL}
=5E&[3vlw:xXyR"21L$@5[8,A?)+g"25)*%-Ek[6B:AB:[6"9F[851#9F)+5d'+-a7 %-)*%-ER}VB:'R1-=#"J)+#1QO
I5E*g,AE*#$-B:'+#9CHD2j5u7k9:B:i6"A1`m]"9:9_BC-ER'*%5u7)*%2"J)R)*%2g-#1#1L)*B:[8aB:'R9_B:#"JEkB:M)*%2gX?A[gKF-E
5=,2E&5,5'&BC)*BC52"92$@5A'+)*")*'#OPk%2B:'T?2[cKF-EU[6"lHdKFkiE*#"J)+-E)*%A"d\!B:6,AE&B:2$-BC,9COk5u7U-G-ElD2'*B:2$@
9:BC)+-E"9:')*%A"J)125S5)"J,2,F#"JEB:e)*%%#"1 5="cE&?29C][S?2'+)K=N"9:'+]B:d)*%2][dB:2B:[6"9A[85T12#97$-"
#9:B:[dB:2"J)+S)*%#["$-$@5E&12B_i9CHL"21!75E*f`7kBC)*%!"p'+-)5=E&?29C#')*%2"J)a%A"'"J)g[85'+)\b9:BC)+-E"9:'-OFPR%2B:'
9C#"12'R)+5d"p5lG-E"9:9)*B:[6a$@5[8,A9C@YBC)WH 5=6|\#~O
)'*%5?29:1IKFS[6#)*BC52#1QDQ%5l7-G-E#D)*%2"J)12?2c)+5p)*%8?2'*S5=UE&?29CS'*$&%2#[6"J)*"p=5E)+E"2'*BC)*BCGBC)H
"21`")*BC;W'+HT[d[8-)+E*Hp5=R,2E&BC5EBC)*BC-#1p,2E&5iE&"[6'$-"LKFc$@5A'*B:1-E&"JK9CHp9:"JE*i-E)*%2"I$@5E&E*#'+,F521;
B:i?A,2E&BC5EBC)*BC-#1`,2E*5iE&"[d'-OPR%d)+E&"2'*BC)*B:GTBC)HI'*$%#[6"D=5EcB:2'+)*"2$@D%2"'e b B:2'*)*"2$@#'-O
B:[8,9C#[8#X)*"J)*BC5I'&%5?29:1QD)*%-E*-=5E*DFKFcKA"'*#1!5`"4"J,A,2E*5"$&%`7k%2-E*8B:2'+)*"2$@#']"JE*V529:Hpi#;
-E&"J)+#147 %#4"$@)*?2"9:9CH!-#1#1D5Eg545)*%-EaKA?AB:9C)]B:`)+#$&%22B_X?#'])*%A"J)g%2"2129:c)+E&"A'*BC)*BCGBC)WH`"21
"X)*B<;W'+H[6[8-)+E*HO?2$&%p)+#$%22B:T?#'R"JE&K-H5A1p)*%g'*$@5,F5=)*%2B:',"J,-ElO
8!AWW4u A(
ff6)*%2B:'U'+#$@)*BC5e7U 7kB:9:9B_G#'+)*B:i"J)+ )*% E*#9:"J)*BC5d5=Q5?EU[65T12BC$-"J)*BC585=Q7#9:9<;>=5?221#1d'+#[6"X)*B:$-'
)+5]"2'+7-E'+-)'+#[6"X)*B:$-'|>m#9C=521cBC=N'*$%2BC)+D#J~OB:A$@5?E"J,2,2E*5"$%c%2"A129C#'"c@YT)+#A1#1
9:"i?A"Ji6B:!7k%2B:$%!$@-E*)*"B:Z'+H[cK59_'"JE&8iBCG#Z"M,A"JE*)*B_$-?29:"JE],2E*@;W1@#1![8#"2B:ip"M)*%5E&5?i%
B:XG#'+)*BCi"J)*BC5L5=)*%2B:'E*#9_"J)*BC52'*%2B:,MB_'529CH ,F5'*'*BCKA9:"J=)+-E "e$@5E*E*#'+,F5212B_iV@YT)+#2'&BC5p5=h"2'+7-E
'+-)R'+#[d")*B:$-'R)+56,2E&BC5EBC)*BC-#1 9C5iB:$],2E*5iE"[6'%2"'KF-#p1@2#1QO4g"JE*a5),A9_"22B:ic)+5dB:X)+E*5J;
12?2$@"21V1-=#21S'*?2$&%V"S@Y)+#2'*BC5VB:c)*%2B:',A"J,F-E#Ok-G-E*)*%2#9C#'*'-DJ7$-"ViB:G'*5[8U,AE*#9:B:[6B_2"JE*H
E*#'*?A9C)*'%-E&OXI5E*k,2E&#$-B:'+#9CHD77kB:9:9'*%25l7)*%A"J)U)*%$@52$-9:?2'&BC52'h,2E&5T12?A$@#16B:65?2E,2E*5,F5'*"9"JE*
$@5E*E*#$@)V7RE&)-O"`,A"JE*)*B_$-?29:"JEc'&?KQ$-9:"'*'a5="2'+7-E8'+-)*'-D)*%p'+5J;W$-"9:9C#1/,2E&B:5E&BC)WHX;>,2E*#'*-E*GTB_i"2'+7-E
'+-)*'-O
pXu-3&VeC*k+*+JShJZJU@a#-ffk3SUJd:-
}a2#---#*X- l#---#-
& cT: S36IR-Jk}6R*l@W&g8:}V}l{|N~Wa l2#---u*Ag }6_k-&W*
MV
pXu-|N3V+\XtLu~88cJNN:l*<&+*+JS^JU@c--R+ 36
c&JC&aJNc*#@lX ^-JV-J-a}V}u{|N~gg--
|N3]Mqestz {Ah|}~+~
-&k}2
PR%pB:X)*?2BC)*B:5ZKF#%2B:21^)*%p1@ABC)*BC5/B:'S)*%M=59:9:5l7kB:2io7k%2#-G-E6"`E&?29C }`B_'cE&-KA?)+)+#1rB:r"
"2'+7-E8'+-)VKA?)SBC)*'cE*-KA?)+)*"9B:'S'+59C#9CH4K"'+#1(5/E&?A9C#'c125[6B:2"J)+#1^KXH(}7kBC)*%^E*#'+,F#$@)S)+5!
u

fi
ff fi
!#"%$&ff'(ff
)ff
*,+- .0/1325467198:+;,.<-1>=?19*@.A19-#B)CEDGF13HI;+85J-1>2K.0/
J8:*,8L*NM)J;6*@.0J7;+O;,=&.0/13*PM(*,JQ6*@B
671SR201>=T1>2019+HI1
J+=T;,25UV*@.0J;+W*,+-YXZ201\[A19HI.0]^.0/1*,+
8AF1>2:8A1>.>_
` 13H>*,+O+;PFa80/;bFcHI;,202019HI.0+19858;,=d;42K*@RR
20;*,H5/WFK20.>_R25J7;,2eJ7.fCNR20198A1>20MJ+gh*,+80F1>2K801>.08>_
ij(k
lmknPo?pboTkqsrGtvu>wyx{z}|~AffE(V5u^3,ew7bu0E 5,<A0,A@VmS>,|xSST:u
%?S5@w@?u0T,?e@Tw0u>u>eb?)O@uI9u>wTSZ~V
ij(kk
5 /1SR20;);,=yJ8:85JUVJ6*@2K.A;N.0/1HI;,202019HI.0+19808<R20;);,=y;,= Sv F25.>_*,+8AF1>28A1>.L8A19UV*,+.0JH>8
| 25;,R;85J7.0J7;+Y _mKg*,JQ+.0/1R20;,R;80J7.0J7;+J8.A25JM)J*,6Q67C80*@.0J8Z
19-FK/19+1>M,1>2V.0/1>201WJ8+;R
25J7;,25J7.C
R2019801>20M)JQ+g3*,+
8AF1>2801>.>;,2mJ8.0/180J+g61:R2eJ7;,25J7.CVR20198A1>25M)J+g3*,+80F1>28A1>.>_ ` 1SUV*9CN.0/1>201>=T;,201
*,80804
Uh1.0/*@.1>M,1>20CR2eJ7;,25J7.C^R2019801>20M)JQ+gh*,+8AF1>2K8A1>.;,= ~ J8%HI;+85J8A.A19+.>_
+E.0/1JQ+-4HI.0J7M,1S8A.A1>RF1h80/;bF.0/*@.:=T;,2*,+*@20B
J7.A25*@25CWR25J7;,25J.fCR20198A1>25M)J+gN*,+8AF1>28A1>.LD*
25461<VJQ8%+;,.K-1>=T19*@.A19- JQ+WDF:/19+1>M,1>2K > |xS
gJM,19+W.0/*@.KJ8*^8A1>.;,=y6J7.A1>25*,6Q8.A2541
J+DS_25;U.0/J8J.=?;6Q67;PFK8.0/*@.3 &| > |x3A HI;+.0*,J+8;+
67C#6QJ7.A1>25*,68L.A2541VJ+*,66yR
25J7;,25J7.C
R2019801>20M)JQ+gh*,+8AF1>2K8A1>.08>_
1>. ~< B1N-1I+19-*,83JQ+<1>=_y | .0/1WJ+-4HI.0JM,1^-1I+J.0J7;+;,=Ef80*@=?19+19808 *,+-*,80804Uh1J7.
J8N*,672019*,-CY+;PF:+.0/*@..0/1E2e467198^JQ+ ~<? *@201 +;,.N-1>=T19*@.A19-J+DS_C-1I+J7.0J7;+ z

I>>>b0ff>,wP>>>>P>,we{~: J cJ8+;,.-1>=T19*@.A19-BC A|~ aSN
| _ ` 1
-J80.0J+g4J85/^.F;^H>*,80198>
*,801E@ >>>P0 DdJ+HI1D}*,+- N J8UV;+;,.A;+JHhJ+B;,.0/J+-JHI198LF1^/*PM,1



|
L
~
N
| A|~ E
| _ /1>201>=?;,251%H>*,++;,.mB1-1>=T19*@.A19-NJQ+VD80J+HI1

DaJ8%R25J;,25J7.fCNR20198A1>25M)J+g_
*,801#)
9>>>90 DSJ+
HI1O.0/1OR251>20194
J80J7.A198h;,=LH>*,++;,.NB1E-1>2eJ7M,19-Y=T20;U ~L .0/1
8A1>. | HI;+ff.0*,JQ+8;+67C2e4671983-1>=?19*@.A19-B)C Z|~Ld *,6;+1,_&J+HI1DJ8*,+*,+8AF1>2h8A1>.
.0/198A125467198OH>*,+.EB1HI;+ff.0*,JQ+19-cJ+ ~< _ /1>251>=?;,201 A|~Lz A|~L | *,+.0/)48L A|~< A|~ 7) | _J+
HI1BCO*,85804UhR.0J;+ODGJ8KHI;+
80J8A.A19+.KF13*,6Q8A;N/*PM,1
A|~ yz &|~ *,+-O.0/1>201>=T;,201hH>*,++;,.B1-1>=T19*@.A19-EJQ+WD_
` 1/*PM,1L801>19+N.0/
*@.;42%*@RR
20;*,H5/J8&g4*@25*,+.A1>19-.A;3R
20;)-
4HI1;+67CVHI;+
H>6480J7;+
8&HI;+.0*,J+19-J+
*,66R2eJ7;,25J7.CR2019801>20M)JQ+g*,+8AF1>2&8A1>.08>_ ` 1<H>*,+V*,68A;S*,8AS.0/1;,RR;80J7.A1)4198A.0J7;+(gJM,19+^*R
*@25.0JH>46*@2
*,+8AF1>2m8A1>.yDS@JQ8J.d*,6F*PC)8dR;8080JB
671&.A;L;,B.0*,JQ+hD | ;,29,Uh;,201R2519H>J8A1967C,@*804R1>258A1>.;,=DHI;+ff.0*,J+
J+g
*,--J.0J7;+*,6%R251>=?1>2019+
HI1EJ+=T;,25UV*@.0J7;+ .0/20;4g/R25J7;,25J.0J7>19-F196Q6 =?;4
+-19-8A19U^*,+ff.0JH>8NB)C*,-
-J+g
*,-19)4*@.A1R251>=?1>2019+
HI1J+=T;,25UV*@.0J7;+
/1*,+8AF1>2.A;^.0/
J8)4198A.0J7;+OJQ8+;_ /12019*,8A;+ J8%.0/*@.=T;,2:85*@,1;,=d.A25*,HI.0*@B
J6QJ7.fCF1*,67F*9C8
HI;+80JQ-1>280J+g61<2e467198FK/19+E-1>.A1>25UVJ+
J+g3Of80*@=T19+19808:J+;42K*@RR
20;*,H5/_ K1>201SJ8%*,+O1 *,UhR
671,
% ,w%
K ,w%

w

h w


/JQ8mR20;,g,2e*,U /*,8.F;*,+8AF1>2%8A1>.08 fi
z ffbb *,+- fi
z ffP(0 _ ;+
80J-1>2 _ &M,19+J=F1*,-.0/13R201>=T1>2019+HI1J+=T;,25UV*@.0J7;+E.0/*@.LB;,.0/ *,+- *@2013R201>=T1>202019- .A;N19*,He/;,= *,+-
F1h*@201
4+*@B671%.A;S-1>2eJ7M,1 *,+
- _);,2JQ+8A.0*,+HI1, J8m+;,.Of80*@=T1:B19H>*,4801KJ7.08&/19*,-^-;)198+;,.&-1>=T19*@. _
+O;,25-1>2K.A;N-1>25J7M,1 J.F;4
6-OB1+19HI198085*@20CW.A;^.0*@,13.0/1R;8085J7B
J6J.fCV;,=y8A1>.08:;,=y25467198 | /1>201
*,+- 9 -1>=?19*@.0J+g 6719808R201>=T1>202019-8A1>.08S;,=25467198 | /1>251 *,+-
P JQ+ff.A;E*,H>HI;4
+ff.>_d:6.0/;4g/
.0/J8%JQ8R;8580J7B
671<J+R25JQ+H>J7R
671:J7.%F;4
6-H>6719*@2567C6719*,-W.A;VJ+.A25*,HI.0*@B
J6QJ7.fC80J+HI1J+.0/1F;,258A.H>*,801*,+


fi

"!#$%#'&)(+*,#-$.0/1324!576)-/683&)64!592:-$,;%6=<!-$,+>@?$*%AB&8!C/1EDF?%D)G%>
HJI(;A(K#LM-@&82:*D&)*N/O(K,+(;&QP
683%.R6S&8!!T?$(;L?U*E$2:(+D45V!2S<7?O*N&W(+6WL*(+#$%>U*#O>M<X68&)(+D:GY&8!T!-2.X!2)ZD3*-&)(;!-$6S*N$O2)!*D:?C5[!2
&)?$(+6\2:%*68!#
H
]_^X`CaObQcdaOegfihkjlmc
noeqpEhknorbtsouNvwhknJu
x 3A32:*,*NO$2)!*D:?$%6T&82)%*N&)(+#Ly$2:35V32)%#OD%6T(+#z&)?{D!#'&8 &Y!5=,+!L(+DU$2)!L2:*.R.|(+#L}?O*%A~/13%#
>%6)D2F(;/1%>E(K#B&)?0,+(;&832F*N&)-2)H{<7(K,+,1#!<g>O(+6)D3-$6)6?$!<m&)?3PB2)%,K*N&8&8!R!-29O2)!1!6)*,H
!<*,K68G(_*#$> x *>2:(o% $2:!"!6)%>T&8!D!#$6:(+>3272F-$,;%6<7(+&)?#3L*N&)(;!#U(+#T&)?$=?%*>M*67
D3$&)(;!#O6&8!=.R!2)7L%#32F*,$2:-$,+%6*#$>R&8!=L(;A7&)?$%.?$(+L?32$2:(+!2:(;&QPH_%D:?O#$(+D3*,+,;P'&)?$(+6o(K6*D:?$(+3A%>
/P=*W2)%>#O(;&)(;!#=!5*#$68<32683&)63H&&)-2:#$6!-&&)?$*N&&)?$!2F(;L(+#$*,*#$68<32683&)62)%.|*(+#Z*#$68<32683&)6
*D3D!2:>$(K#L&8!&)?#3<>"#$(;&)(;!#4<7?%#$3A32o&)?3PX*N2)7D!#O6)(+68&8%#&3H9?$9.R*(+#R*DF?$(;3A%.X%#&J(+6&)?$*N&
$2)!L2F*.R6W<7?!68X6)(+#$L,;=*#$6)<J32683&(+6S(+#OD!#$6)(+68&8%#&W/"%D!.RZD!#O6)(+68&8%#&(+#&)?$Z#$3<6)%.R*#'&)(KD363H
9?R*NO$2)!*D:?@D3*#?O*N2:>$,;PM/"|A(;3<%>~*6=*6)*N&)(+6)5[*D&8!2)PC&82)%*N&).R%#'&4!5\$2)35[32)%#$D%65[!2=6)3A32:*,
2)%*68!#O63
NH9O2)35V32:%#$D%6=*N2:|(+.XO,K(+D3(;&*#$>?O(;L?$,;PM2)%68&82:(+D&8%>&)?*6)P.R.R3&82:(+DX&82)%*N&).R%#'&4!51!6)(;&)(;A
*#O>Y#3L*N&)(;A4(+#5[!2:.R*N&)(;!#B683%.R6-$#)-$68&)(%>

H7(+&9(+69>$(ED3-$,;&&8!|6830?!<$5[!27(+#$6)&)*#$D D3O&)(;!#$69!5D3$&)(;!#$69D3*#/12)3$2)%68%#&8%>

H95[3<32D!#$D3,+-O6)(;!#$6*N2)=!/$&)*(+#%>Y&)?$*#Y(+#T&)?$!2:(;L(+#O*,"*#O68<J32W683&768%.|*#'&)(+D36%D!#&82:*N2)PB&8!
<?$*N&9!#<!-$,+>B 1%D&9<7?%#TO2)35V32:%#$D%69*N2)&)*NG%#M(+#&8!R*D3D!-$#'&3H
&(+631&)?32)35[!2).X!2)X2:%*68!#$*N/O,;Z&8!YA (;3< !<*,+6)G(*#O> x *>2:(6S*N$O2)!*D:?{*6*TD!#&82:(;/-&)(;!#
&8!|(+#OD!#$6)(+68&8%#ODP?$*#$>O,+(+#L42:*N&)?329&)?O*#T$2)35[32)%#$D0?O*#$>$,+(+#$LH
#{*N$$2)!*DF?{&)?$*N&(+6D3,+!6832(+#C68O(;2F(;&7&8!Y!-$2:6(+6W!2:>32)%>{,+!L(+D4$2)!L2:*.R.|(+#L{-OD3D3*N5[-2:2:(
3!#$O7-$,+,+!%FH #E!2:>32)%>T,+!L(+DW$2)!L2:*.(+6\*Z683&!5D!.X1!#%#'&)6\5[!2:.R(K#L4*#T(K#$?32:(
&)*#$D7?O(;32:*N2:DF?'PH'J*D:?RD!.R"!#$%#'&oD!#O6)(+68&)6!51*683&!5"2:-$,+%63H9?9(+#$?$32:(;&)*#$D?$(;32F*N2:D:?P4(K6-$68%>
&8!683&8&),;9D!#(+D&)6*.X!#$L2:-$,;%632:-$,+%6,+!<32o(+#4&)?9?$(;32F*N2:D:?P=?$*A9$2)35[32)%#$D\!A32o&)?!689?O(;L?32
-R(+#4&)?9?O(;32:*N2:DF?'P46)(+#$D\&)?5[!2:.X32*N2)D!#$6)(+>32:%>4.R!2)9681%D3(DNH #!&)(;!#Z!51*68&)*N/O,;.X! >%,
5[!29!2:>32)%>,;!L(+DS$2)!L2F*.R69D3*#Y/1>#$%>6830\-$D3D3*N5-2)2:(13&*,H;k%O5[!29&)?=>3&)*(+,+6FFH
9?$32)E*N2:E&t<J!~.|*(+#}>O("32:%#$D%64/"3&t<J3%#y!2F>32)%>y,;!L(+D$2)!L2:*.|64*#O>}!-$24 &8%#$6)(+!#}!5
<%,+,w5[!-$#$>%>B68%.R*#&)(+D363
NH9!2F>32)%>Z,;!L(+Do$2)!L2:*.R6-$6)o!#$,;P!#$G (+#$>0!5O#3L*N&)(;!#&)?>$(K68&)(+#$D&)(;!#/"3&t<J3%#X#$3L*N&)(;!#
*675[*(K,+-2)S*#$>YD3,+*6:6)(+D3*,#3L*N&)(+!#Y(+69#!& $2)%6:6)(;/O,;(+#B&)?$,+*#L-$*NL
H9&)?$4O2)35V32:%#$D%6S!5!2:>32:%>U,+!L(+D4$2)!L2:*.R6S*N2)Z$2:%>#%>M&)?2)!-L?U&)?X(+#O?32:(;&)*#$D4?$(
32F*N2:D:?PO&)?32:0(+6#!X<\*%PT!5>32:(;A (+#LXD!#&8&Q>$3"%#O>%#'&7$2)35[32)%#$D%69>P #$*.R(KD3*,+,;PH
(+#$*,K,;P0<7<!-$,+>R,K(;G&8!=.X%#&)(;!#*#R*N$$2)!*DF?|2:%D%#'&),;PZ$2:%68%#'&8%>|/PXo2:*NGG%#E*#$> x *N2)&8!2
%FHJ7?3P &8%#$>S-$#L6R*N2:L-$.X%#&Z6)P68&8%.68&QP ,;T2)%D!#$6)&82:-$D&)(;!#y!5W,;!L(+DE$2)!L2:*.|.R(+#L
S-$#L7% R<7(;&)?*{$2)35[32)%#$DY?$*#O>$,+(+#L{.X3&)?$!>&)?O*N&|(+6XA32)PD3,;!6)Y&8!!-2F63H9?$(+6X(K6R#!&
*68&8!#$(K6)?$(+#LR6:(+#$D*6&)?Z*-&)?!2:671!(+#'&7!-&3&)?$%(;2*N$O2)!*D:?U(+69/*68%>!#z8-$#-/O,+(+6:?%>B(+>%*6
!5IW32:?O*N2:>}2)3<7G* H#5[*D&3(;&=<*64*O2)%,+(+.R(K#$*N2)PYA32:6:(;!#!5&)?O(+6O*N132=&)?$*N&4,+%>~&8!U&)?%(;2
5[!2:.=-O,+*N&)(;!#
H
%

fiJ$$O'M+EJ O9Uo+EJ''O'

{X$:%8%'8%{KM)O+WON13SC 8%$)+CJ;+4$::RW7;)UQTQ1%S$3N);
73:O)3V3:%$9+[:RN);ZX$:$;%3X1\ $)%)8%XKZ)9;+3+$$N9$+
8%$)+X+3)Z$83$VO:)+3N$OK+3N);$3\%R$88:N8%RO)+SXXO;\[)
;3)%8O+19=|+M$)NXoSN$$::{+)$N+8E)$+Q1=+[:RN);
+8Q$3"%O%'W$Y3T1)%)%1T$T$3:;%Y $RK3+;
:%+w[$$%y8%R)+3X+O3:;Z8XYF%9O:$N')N%%\X3)KX%
)%8ONO;0$3K$);$7N)0$)+%
1ST)=)3$O
)=O$;);B$:3V3)%O0+
[:RN);3|RN)783J1O3+$);O$)+$3:NO;4+N)3%'oJW$%W)7
'M)33
9$U)$+S3))+O;+W)Z)%N8%8=)NRJJ%+;wV$O%8%|')+30$US$)18%
8%$);BJ)%8$+$Z31+B1; R+")+R
9$4)KXO;4$C$N):)3$:%8%')N)+C)X;3XO;Z$+)3O)8%U+C %3
B)3%R
8R+$O+3N8S)$N9%3F+;%N);BJ%KwVO$%B8%R)+39RO) +X3mN88:)+
X$:R+8"3tJ3%@ $)%):;%%)4$~E3;%$~7+)U$=1307K'83)%))+Y18%)+
N$OK+3N);$3
{oOMQOkOzO_
$+K;08E)$YF\3 )% W;""kff
W:$
fi%)o:N%3%W ;
833%
$ZtJ$' X$)3[3)3%[+'83:%8)+SRX%)%;OK78+XO)\) $+;t

)$+\ON13%
OJ

\N:W;
W%+V$} "!F#k+oO):RRK7$=$7;%$J)3O)%8%)N);
%$&(')+* ,(&/.102&4357698)&:3)/,(;<;5=*"3?>A@CB=DEGF(HJI AK
\N:W;

O:$R$K
AL42 J!FMS$+;t|13Q3%Y;83:ON);8%R)+3\_++W$)N
FRJOOX8$+7VFR++)|3 N8)&J6JOPQ*R7-SOUTV&):WYX:Z[&Q\]&(*N0^&435_6`8)&43),;;<5=*"3
,* acbd&*;c&(* &Re&(*576dfUg4,CXh&(*5=*"3$:K3%

)39N G
4i A!FUjW$$+XO:;:;)+%$81%3lk"3;QE83$;9++Nm 8)&J6JOM$no0^P/p9q@hrsB
&():W"$F+3%
)39N
4 %!F^u7%8$+$=N1$F;:;);%K|3$;o++NGv8m)/& 6JOGp`p1p1PQq@hrsB^wGg:,R_R7-Sg3
\$33N):;M#3$yx0;Mzu$+;iU {!F)NO;TX %+Z$@)%;RRO)N);y[
++W$):R|+47;)B+$3F;)$S$E8:3N);
8RN$1%N9+| :$}k
# +
o):R|+
W7K+


; }~
SK+;3%^$M K"!Fv#_+%N)KX|;:+)$RS[08%8)+$T)|)N)+/kNO+++Q
O)1););$1FBV:4$+NU$&(')+* ,(-&/.102&4357698)&:3)/,(;<;5=*"3i {F I[K

S$[_ H!F9733$)N++;tRN)$X%)$+)o$$$R%')O);KR$X$8$+
:%8$+B$C;+=$)FRR+ ]8m)/& 6JOm>R=ZVPQ*R7-SOo$&(5=*Rd?&* .hgh)/gh* 6Qg&(*p1)+R75 o6 57,(PQ* Rg-=-5S3"gh* 6Qg3

W%;[$
1};#_;):$+8 L4y (A!F#k+0O):RS7+)M3+:)+33N);
d8m)/& 6JOJR=Z
PQ* R7-lO?&* .hgh)/gh* 6Qgc&(*V02&4357698)&43),;;<5=*"3


fis

1:} Q^S:(="(N( |17 oh_(%QhS"%Q]JAi1m/ +4(/(
[+7_QhMMs4s :h(:c/4sh
sS Q`h:(::s/[h// |4sh: =ss/4 /s
JJ11`Q
Y4[_ 9S%sQ_} JQ`^94:c`US4|sh4ShUhUh%/7_(?
7="(
^=4QS/}U Q s4shS:h4:S4 m/ Y1=Qe ( ?( 4 +V
+7 oh_(%Q =S"h Qh
^=4QS/%M Qs(4S1s h(:(4Sc4:css4hUCy7oQ
Q= + (4 [` 4+hhe7_( s2is4sh: [4S y2(^(
|:(4S






ff
fi

}" } Q4s(:4|?4sh:S y4h=h4:s4sc/U/G h 9[(4S
Y^ 2`Qff^
C11"[++
yQ(" 2M Q ^
4_:( Uh( A=" eA17ch7M4s :hG11c
/hQ|
yQ(" ?SM (4/ Q!
4sv4 (4S Ghh Sh(| Sh("
A#$`:44hi(h( S&% '
% c:S:S4 h JJUQhQ%77 (
? hh/h Q<`d( (^
(*V) (:A="e
y:#[
% 4/[eC (AQ2Uso es </ c"4hyhs ?U4<4s4s4h+e(s c44(S
4 cA4hh$G
, A(eQJ(+c7_:Ah-/.103C2 AJAACs
y:#[
% 4/[e(9% JQi4(SU/ A4hMm5/4 4S4:h^h Uh%/7_( ?
7="(sQAffC
/h 9 (AQ6S=Ush7Sm4 /sso17 h_(%Qhl[ +78- Q [

}(:/_9" Q' (/4+e4sh4h4hff
GA4h4 9S4^(h(4ShA (+ (
/1]=sc7_Q( :
J[([
s4h/h;
% `1%d Q ^]=hc77h QshQ7e( +~oS4c U / (::=U< /
`44%
hG"/ 2
s4h/h;
% CC%SYsc/2J9CdSYU:>% J" JQ' /h43? (h:( A%@ /:h
4h /: 4 /4(/hQh hS | ss/sh:S4 m/ J3A
B =
21CED/
F (%+(F
(| sh ;dSs:h S:SGs
JQ?s`o Hes / A4h=o h:GS
4:ch (+ (/d=2I-ffJ.>-+C2 G( Y[(
S//hh 1` JQ i(44U4 cA4hc=/:s4c"/ (o 44 4%Ks U7oQ
4_`Qs2CC[(^[:hm7<L`"([ ` SGh4s
^
}h4 S:2MS2 `S=h4 h Q / c"4h=9S<4QcUS4hS
h(4Sy m/ (= [
/ :( ?(Jhh/h Q+7 oh_( QhS"%Q 2e%(

M9N

fi
