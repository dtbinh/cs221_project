Journal Arti ial Intelligen e Resear h 31 (2008) 473-495Submitted 03/07; published 03/08Axiomati Foundations Ranking Systemsepsalonstanford.eduAlon AltmanDepartment Computer ien eStanford UniversityStanford, CA 94305-9020 USAmoshetie.te hnion.a .ilMoshe TennenholtzFa ulty Industrial Engineering ManagementTe hnion Israel Institute Te hnologyHaifa 32000, IsraelAbstraReasoning agent preferen es set alternatives, aggregation su hpreferen es ial ranking fundamental issue reasoning multi-agentsystems. set agents set alternatives oin ide, get rankingsystems setting. famous type ranking systems page ranking systems ontextsear h engines. paper present extensive axiomati study ranking systems.parti ular, onsider two fundamental axioms: Transitivity, Ranked Independen eIrrelevant Alternatives. Surprisingly, nd general ial ranking rulesatises requirements. Furthermore, show impossibility result holdsvarious restri tions lass ranking problems onsidered. However,transitivity weakened, interesting possibility result obtained. addition, showomplete axiomatization approval voting using ranked IIA.1. Introdu tionranking agents based agents' input fundamental multi-agent systems(see e.g. Resni k, Ze khauser, Friedman, & Kuwabara, 2000). Moreover, omeentral ingredient variety Internet sites, perhaps famous examplesGoogle's PageRank algorithm (Page, Brin, Motwani, & Winograd, 1998) eBay'sreputation system (Resni k & Ze khauser, 2001).basi problem introdu es new ial hoi e model.lassi al theoryial hoi e, manifested Arrow (1963), set agents/voters alled rank setalternatives. Given agents' input, i.e. agents' individual rankings, ial rankingalternatives generated.theory studies desired properties aggregationagents' rankings ial ranking.parti ular, Arrow's elebrated impossibilitytheorem (Arrow, 1963) shows aggregation rule satises minimalrequirements, relaxing requirements appropriate ial aggregationrules dened.novel feature ranking systems setting setagents set alternativesoin ide.Therefore, su h setting one may needonsider transitive ee ts voting. example, agent(i.e.votes for) agentimportan e agentc;breports importan emay inuen e redibility reportbindire ee ts onsidered wish aggregateinformation provided agents ial ranking.2008 AI ess Foundation. rights reserved.fiAltman & TennenholtzNoti e natural interpretation/appli ation setting ranking Internetpages. ase, set agents represents set Internet pages, linkspagepQ viewed two-level rankingp agents (pages) whi h Q.set pagespreferred agent (page)agentsQproblem ndingappropriate ial ranking ase fa problem (global) page ranking.Parti ular approa hes obtaining useful page ranking implemented sear hengines su h Google (Page et al., 1998).theory ial hoi e onsists two omplementary axiomati perspe tives:des riptive perspe tive: given parti ular ruleraggregation individualrankings ial ranking, nd set axioms sound ompleteis, nd set requirementsrsatises; moreover, every ial aggregationrule satises requirements oin ideaxiomatization termedr.representation theoremr.result showing su haptures exa essen e(and assumptions behind) use parti ular rule.normative perspe tive: devise set requirements ial aggregation rulesatisfy, try nd whether ial aggregation rule satisesrequirements.Mu h eort invested des riptive approa h framework lassi altheory ial hoi e.setting, representation theorems presentedmajor voting rules su h majority rule (May, 1952; see Moulin, 1991 overview).ranking systems setting, su essfully applied des riptive perspe tiveproviding representation theorem (Altman & Tennenholtz, 2005b) well-knownPageRank algorithm (Page et al., 1998), whi h basis Google's sear h te hnology(Brin & Page, 1998).ex ellent example normative perspe tive Arrow's impossibility theoremmentioned above. Tennenholtz (2004) presented preliminary results rankingsystems set voters set alternatives oin ide. However, axiomspresented work onsist several strong requirements whi h naturally leadimpossibility result. Still normative approa h ranking systems, ta kledissue entives (Altman & Tennenholtz, 2006b, 2006 ), positive negativeresults. ently, onsidered variation ranking systems, personalizedranking generated every parti ipant system (Altman & Tennenholtz, 2006a),surprisingly dierent results.paper provide extensive study ranking systems.introdu e twofundamental axioms. One axioms aptures transitive ee ts voting ranking systems, adapts Arrow's well-known independen e irrelevant alternatives (IIA) axiom ontext ranking systems. Surprisingly, nd generalranking system simultaneously satisfy two axioms! result meanswould like fully apture transitive ee ts, ranking de isions must made globally,based numeri al ulations. show impossibility result holdsvarious restri tions lass ranking problems onsidered.hand, show positive result ase transitivity axiomrelaxed. new ranking system pra ti al useful algorithm provided474fiAxiomati Foundations Ranking Systemsomputation. Finally, use IIA axiom present positive result formrepresentation theorem well-known approval voting ranking system, whi h ranksagents based number votes eived.axiomatization showsignoring transitive ee ts, one ranking system satises IIA axiom.paper stru tured follows: Se tion 2 formally denes setting notionranking systems. Se tions 3 4 introdu e axioms Transitivity Ranked Independen e Irrelevant Alternatives respe tively. main impossibility result presentedSe tion 5, strengthened Se tion 6. main positive result, formranking system satisfying weaker version transitivity given Se tion 7,axiomatization Approval Voting ranking system presented Se tion 8. Finally,luding remarks given Se tion 9.2. Ranking Systemsdes ribing results regarding ranking systems, must rst formally denemean words ranking system terms graphs linear orderings:Denition 2.1.Letset. relationreexive, transitive, omplete. LetNotation.2.2LetabR AAalledorderingdenote set orderingsordering,. Formally, bb a.stri order indu edL(A)equality predi ateabb a;,A.abGiven dene ranking system is:Denition 2.3.system ForderingLetGVFG L(V ).otherwise alledFV . rankingG GVset dire ted graphs vertex setfun tional every nite vertex setVmaps graphspartial fun tion alledgeneral ranking system.partial ranking system,One view setting variation/extension lassi al theory ial hoi emodeled Arrow (1963). ranking systems setting diers two main properties.First, setting assume set voters set alternatives oin ide,se ond, allow agents two levels preferen e alternatives, opposedArrow's setting agents ould rank alternatives arbitrarily.two-level limitation important order avoid Arrow-style impossibility results.Indeed, di hotomous (i.e. two level) setting su h results apply (Bogomolnaia,Moulin, & Stong, 2005). allowed general rankings input system,would rea hed impossibility results dire result Arrow-style impossibility.adding di hotomous limitation, ensure results onsequen eo-in iden e voters alternatives related transitive ee ts.2.1 Examples Ranking Systemsorder make abstra denition ranking systems rete, shallgive examples several well-known ranking systems.order denesystems, throughout paper, shall use following notation:475fiAltman & TennenholtzecfbFigure 1: Example graph ranking systems.Notation.G = (V, E) graph v V vertex. Let PG (v) ,{u|(u, v) E} SG (v) , {u|(v, u) E} denote prede essor su essor sets vG respe tively. G understood ontext, sloppily use P (v) S(v).2.4LetApproval Voting simple ranking system ranks agents ordingnumber votes (i.e. oming edges) have. Formally,Denition 2.5.approval voting ranking system AVranking system dened by:v1 AVG v2 |PG (v1 )| |PG (v2 )|Consider graph Figure 1.f cdeAVranking system would rank graphbased fa verti es{a, b}, {f },{c, d, e}ab0, 1,2 prede essors respe tively. full axiomatization approval voting ranking systemgiven se tion 8.One major appli ation Ranking Systems ontext Internet pages.ontext, represent Internet dire ted graph, verti es websites,edges links websites.prominent ranking system settingPageRank (Page et al., 1998), whi h based random walk Internet graph.Namely, pro ess start random page, iteratively move one pageslinked urrent page, assigning equal probabilities ea h su h page.dene PageRank matrix whi h aptures random walk reated PageRankpro edure:Denition 2.6.ank Matrix AGLetG = (V, E) graph, assume V = {v1 , v2 , . . . , vn }.n n) dened as:1/|SG (vj )| (vj , vi ) E[AG ]i,j =0Otherwise.PageR-(of dimensionPageRank pro edure rank pages ording stationary probability distribution obtained limit random walk; formally dened follows:Denition 2.7.Let G = (V, E) graph, assume V = {v1 , v2 , . . . , vn }. Let0 < 1 damping fa tor. Let rPbe unique solution system (1 d) AGri = n. unique solution,r + ( 1 1 1 )T = rranking dened. Otherwise, PageRank P RG (vi ) vertex vi V denedP RG (vi ) = ri . PageRank ranking system ranking system vertex setV maps G PGR , PGR dened as: vi , vj V : vi PGR vjP RG (vi ) P RG (vj ).476fiAxiomati Foundations Ranking SystemsbcFigure 2: Example Transitivityshown> 0,indeed unique solution thus rankingsystem general one. However,= 0 ranking system omes partial rankingsystem, always well dened.= 0.2graph Figure 1,(0.2, 0.2, 0.52, 1.7, 1.77, 1.61)PageRank values assignedgiving rankingb c f e.a...fNoteranking diers one assigned approval voting, neither rankingsrenement other.example shows PageRank Approval Votingdistin ranking systems, two may disagree ranking two verti es.soon see systems satisfy two mutually ex lusive properties ranking systems.3. Transitivitybasi property one would assume ranking systems agentranked higher agentb,agenta'svotersranked higher agentb.notion formally aptured below:Denition 3.1.F ranking system. say F satises strong transitivitygraphs G = (V, E) verti es v1 , v2 V : Assume 1-1 mapping (butne essarily onto) f : P (v1 ) 7 P (v2 ) s.t. v P (v1 ): v f (v). Then, v1 v2 .assume either f onto v P (v1 ): v f (v). Then, v1 v2 .Letexplain formal denition aptures intuition, onsider simple graphabcintuition tells uscthus ranked higherbend vote hain trusted,b,ause favote ompareddenition above:mustbtrusteda,duenone. intuition orre tly apturedranked stri tlyb ausefun tion mappingP (a) =P (b) = {a} onto, b must ranked stri tly c ause trivial mappingP (b) = {a} P (c) = {b} satises b, thus get b c, expe ted.involved example, onsider graph G Figure 2 ranking systemF satises strong transitivity. F must rank vertex verti es,Fprede essors, unlike verti es. assume G b, strong transitivityFFmust lude b G c well. must lude b G (as b'sprede essor ranked lower a's prede essor c, additional prede essor d),Fwhi h leads ontradi tion. Given b G a, transitivity, must ludec FG b, ranking graph G satises strong transitivity FG c FGb FG a.477fiAltman & TennenholtzTennenholtz (2004) suggested algorithm denes ranking system satises strong transitivity iteratively rening ordering verti es startingranking suggested approval voting.Note PageRank ranking system satisfy strong transitivity. duefa PageRank redu es weight links (or votes) nodes whi hhigher out-degree. Thus, assuming Yahoo! Mi rosoft equally ranked, linkYahoo! means less link Mi rosoft, ause Yahoo! links external pagesMi rosoft. Noting fa t, weaken denition transitivity requireprede essors ompared agents equal out-degree:Denition 3.2.Let F ranking system. say F satises weak transitivityG = (V, E) verti es v1 , v2 V : Assume 1-1 mappingf : P (v1 ) 7 P (v2 ) s.t. v P (v1 ): v f (v) |S(v)| = |S(f (v))|. Then, v1 v2 .assume either f onto v P (v1 ): v f (v). Then, v1 v2 .graphsexample weak transitivity, one onsider strong transitivity exampleabove, still applies weak transitivity.PageRank ranking system satises weakened version transitivity. duefa that:P R(v1 ) =X P R(v)X P R(f (v))|S(v)||S(f (v))|vP (v1 )asev f (v)vP (v1 )v P (v1 )X P R(v)= P R(v2 ).|S(v)|vP (v2 )rst inequality stri t,fonto se ond inequality stri t.4. Ranked Independen e Irrelevant Alternativesstandard assumption ial hoi e settings agent's relative rankdepend (some property ) agents voted them. Su h axioms usuallyalled independen e irrelevant alternatives (IIA) axioms. setting, su h IIA axiomsmean agent's rank must depend property immediate prede essors.setting, require relative ranking two agents must dependpairwise omparisons ranks prede essors, identity ardinalvalue. IIA axiom, alledrankedIIA, diers one suggested Arrow (1963)fa onsider identity voters, rather relative rank.Fb c e f . lookomparison c d. c's prede essors, b, ranked equally,ranked lower d's prede essor f . also true onsidering e f e'sprede essors c ranked equally, ranked lower f 's prede essore. Therefore, agree ranked IIA, relation c d, relatione f must same, whi h indeed c e f . However,situation also urs omparing c f (c's prede essors b equallyranked ranked lower f 's prede essor e), ase c f . three asesexample, onsider graph Figure 3. Furthermore, assume ranking systemranked verti es graph following:involve omparing two verti es, one two weaker prede essors one one stronger478fiAxiomati Foundations Ranking SystemscebfFigure 3: example RIIA.bFigure 4: Graph proleh(1, 1), (2)i.prede essor, ome omparisons onsistent.lude ranking systemFTherefore,whi h produ ed rankings satisfy rankedIIA.formally dene ondition, one must onsider possibilities omparing twonodes graph based ordinal omparisons prede essors.possibilities omparison proles:Denition 4.1. omparison prole pair ha, bi = (a1 , . . . , ), b = (b1 , . . . , bm ),a1 , . . . , , b1 , . . . , bm N, a1 a2 , b1 b2 bm . Let P setsu h proles.ranking systemsatisfyF,G = (V, E), pair verti es v1 , v2 V saidha, bi exist 1-1 mappings f1 : P (v1 ) 7 {1 . . . n}given f : ({1} P (v1 )) ({2} P (v2 )) 7 N denedgraphsu h omparison prolef2 : P (v2 ) 7 {1 . . . m}su has:f (1, v) = af1 (v)f (2, u) = bf2 (u) ,f (i, x) f (j, y) x FGConsider prole(i, x), (j, y) ({1} P (v1 )) ({2} P (v2 )).h(1, 1), (2)i.omparison prole illustrates basi questionomparing agent got two low-rank votes one got one high-rank vote.question unde ided transitivity alone, assume transitivity omparisonprole satised pairmaps prede essors(a, b) graph Figureb 1 2 respe tively.4.ffun tion simplyrequire every su h prole ranking system ranks nodes onsistently:479fiAltman & TennenholtzDenition 4.2.F satises ranked independen ef : P 7 {0, 1} su h everygraph G = (V, E) every pair verti es v1 , v2 V every omparison prolep P v1 v2 satisfy, v1 FG v2 f (p) = 1.LetFranking system. sayirrelevant alternatives (RIIA)Notation.4.3fun tionabb a.use notationmeanexists mappingabfdenition understood ontext,meanf ha, bi = 1, bmeanf hb, ai = 0,ab(c, d), (c, f ), (e, f )h(1, 1), (2)i. seen above, pairs (c, d) (e, f )(1, 1) (2), (c, f ) entails (1, 1) (2). results ontradi ea hexample, example onsidered above, pairssatisfy omparison proleentailother, therefore lude ranking system produ ed rankingsatisfy RIIA.denition RIIA formalizes requirement onsisten omparisonssu h one seen above. means ranking system satisfying RIIA mustde ide relative rankingbFigure 4, (assuming transitivity) rankurren es two weak vs. one strong prede essor.RIIA independen e property, ranking systemF= ,ranks agentsequally, satises RIIA.AV also satises RIIA. dueh(a1 , . . . , ), (b1 , . . . bm )i, f fun tion AVapproval voting ranking systemomparison prolen m.faranksabuse fa axiomatization approval voting present Se tion8.5. Impossibilitymain result illustrates impossibility satisfying (weak) transitivity RIIA simultaneously.Theorem 5.1. general ranking system satises weak transitivity RIIA.Proof.Assume ontradi tion exists ranking systemtransitivity RIIA. Consider rst graphG1Figure 5(a).Fsatises weakNote verti esgraph out-degree 2 0, thus out-degree requirement weaknote a1 a2 must satisfy omparisonpa = ((x, y), (x, y)) ause identi al prede essors. Thus, RIIA, a1 FG1a2 a2 FG1 a1 , therefore a1 FG1 a2 . weak transitivity, easy seec FG1 a1 c FG1 b. assume b FG1 a1 , weak transitivity, a1 FG1 b whi hFFontradi ts assumption. lude c G a1 G b.11onsider graph G2 Figure 5(b). Again, out-degree requirement weakFtransitivity trivially satised, RIIA, a1 G a2 . weak transitivity,2FFFeasy see a1 G c b G c. assume a1 G b, weak transitivity,222Fb G2 a1 whi h ontradi ts assumption. lude b FG2 a1 FG2 c.Consider omparison prole p = ((1, 3), (2, 2)). Given F , a1 b satisfy p G1FFFFFF(be ause c G a1 G a2 G b) G2 (be ause b G a1 G a2 G c). Thus,111222transitivity trivially fullled.prole480fiAxiomati Foundations Ranking Systemsba1a2(a) Graph G1a1ba2(b) Graph G2Figure 5: Graphs proof Theorem 5.1RIIA,bFG2a1 FG1 b a1 FG2 b,whi h ontradi tion faa1 FG1 ba1 .result quite surprise.Intuitively, would like ranking pro eduresensitive relative ranking ea h agent's voters (transitivity) inuen edseemingly irrelevant information (RIIA). Although requirements mayseem omplementary, impossibility theorem shows requirements faontradi tory.onsider transitivity basi requirement, learn axiomatizationtransitive ranking system annot restri ted lo al ordinal properties. is,designing ranking system transitivity required, one must hoose whether basesystem numeri omputation, ordinal axioms operate globalale.example, standard formalism PageRank ranking system Denition 2.7axiomatization similar system suggested Pala ios-Huerta Volij (2004)based numeri al omputation, suggested axiomatization (Altman & Tennenholtz, 2005b) uses ordinal axioms global ale. axioms refer invariantsrelations ranking dierent graphs, rather pairs verti esgraph.PageRank example demonstrates ranking systems may dened usingeither approa hes. feel numeri approa h suitable dening exe uting ranking systems, global ordinal approa h suitableaxiomati lassi ation.6. Relaxing Generalityhidden assumption impossibility result fa onsidered generalranking systems. se tion analyze several spe ial lasses graphs relateommon ranking enarios.481fiAltman & Tennenholtz6.1 Small Graphsnatural limitation preferen e graph ap number verti es (agents)parti ipate ranking. Indeed, three less agents involved ranking, strong transitivity RIIA simultaneously satised. appropriate rankingalgorithm ase one suggested Tennenholtz (2004).algorithmsimply starts ranking in-degree renes ranking required strong transitivity satised. easy see de isions omparison proles possible3-vertex graph di tated either in-degree transitivity. Spe ally, proleh(1, 3), (2, 2)iused proof impossible su h graphs.four agents, strong transitivity RIIA annot simultaneously satised (the proof similar Theorem 5.1, vertexremovedgraphs). agents involved, even weak transitivity RIIAannot simultaneously satised, implied proof Theorem 5.1.6.2 Single Vote SettingAnother natural limitation domain graphs might interestedrestri tion ea h agent (vertex) exa tly one vote (su essor). example, votingparadigm ould viewed setting every agent votes exa tly one agent.following proposition shows even simple setting weak transitivity RIIAannot simultaneously satised.Proposition 6.1. Let G1 set graphs G = (V, E) su h |S(v)| = 1v V . partial ranking system G1 satises weak transitivity RIIA.Proof.Assume ontradi tion partial ranking systemsatises weak transitivity RIIA. LetF.G1 G1f : P 7 {0, 1}FG1mapping denitionRIIAx1 FG1 x2 FG1 b FG1 a.(a, b) satises omparison prole h(1, 1, 2), (3)i, must (3) (1, 1, 2). letG2 G1 graph Figure 6b. weak transitivity x1 FG2 x2 FG2 FG2 FG2 b.(b, a) satises omparison prole h(2, 3), (1, 4)i, must (1, 4) (2, 3).Let G3 G1 graph Figure 6 . weak transitivity easy seex1 FG3 FG3 x7 FG3 y1 FG3 y2 FG3 c FG3 d. Furthermore, weak transitivityFFFFFlude G b G b c G d; y1 G b x3 G d.33333FFFonsider vertex pair (c, b ). shown x1 G x2 G y1 G b. So, (c, b )333Fsatises omparison prole h(1, 1, 2), (3)i, thus RIIA b G c. onsider3FFFvertex pair (b, a). already shown G b G c G d. So, (a, b) satises333Fomparison prole h(2, 3), (1, 4)i, thus RIIA b G a. However, already shown3FG b ontradi tion. Thus, ranking system F annot exist.3Letgraph Figure 6a. weak transitivity,6.3 Bipartite Settingworld reputation systems (Resni k et al., 2000), frequently observe distin tiontwo types agents su h ea h type agent ranks agents482fiAxiomati Foundations Ranking Systemsx1bx2x2x1(a) Graph G1x4b(b) Graph G2y2x5bb'x6a'x7x1x2x3y1( ) Graph G3Figure 6: Graphs proof proposition 6.1type. example buyers intera sellers vi e versa. type limitationaptured requiring preferen e graphs bipartite, dened below.Denition 6.2.V = V 1 V2 ,G = (V, E) alled bipartite exist V1 , V2 su hV1 V2 = , E (V1 V2 ) (V2 V1 ). Let GB set bipartitegraphgraphs.impossibility result extends limited domain bipartite graphs.Proposition 6.3. partial ranking systemtransitivity RIIA.Proof.proof exa tlyG1 ,GB G1 satises weakonsidering graphs Figure 6bipartite.6.4 Strongly Conne ted Graphswell-known PageRank ranking system (ideally) dened set strongly onne ted graphs. is, set graphs exists dire ted pathtwo verti es.Let us denote set strongly onne ted graphsGSC .following propositionextends impossibility result strongly onne ted graphs.Proposition 6.4. partial ranking systemtivity RIIA.483GSC satises weak transi-fiAltman & TennenholtzProof.proof similar proof Theorem 5.1, additional vertexegraphs edges verti es.7. Relaxing Transitivityimpossibility result omes possibility result relax transitivity requirement. Instead omparing verti es similar out-degree weak transitivityaxiom above, weaken requirement stri preferen e hold asemat hing prede essors one agent preferredprede essors other.Denition 7.1.Let F ranking system. say F satises strong quasi-transitivityG = (V, E) verti es v1 , v2 V : Assume 1-1 (butne essarily onto) mapping f : P (v1 ) 7 P (v2 ) s.t. v P (v1 ): v f (v). Then,v1 v2 . And, P (v1 ) 6= v P (v1 ): v f (v), v1 v2 .graphsStrong quasi transitivity signi antly weaker property strong transitivity,allows mu h indieren e resulting ranking. Spe ally, ranking systemF=always ranks verti es equally satises strong quasi transitivity. generally,ranking system value vertex proportional sum valuessubset prede essors satises strong quasi transitivity. shall see examplesquasi-transitive ranking systems below.require strongquasi-transitivity RIIA, nd interesting familyranking systems rank agents ording in-degree, breaking ties omparingranks strongest prede essors. ursive in-degree systems work assigningrational value every vertex, based following idea: rank rst basedin-degree. tie, rank based strongest prede essor's value, on.Loops ranked periodi al rational numbers base(n + 1)period lengthloop, ase ontinuing loop maximally ranked option.ursive in-degree systems dier way dierent in-degrees ompared.monotone reasing mapping in-degrees ould used initial ranking.show systems well-dened values al ulated denesystems algorithmi ally follows:Denition 7.2.r : N 7 N monotone nonde reasing fun tion su h r(i)N. ursive in-degree ranking system rank fun tion r dened follows:Given graph G = (V, E), let n = |V |. relative ranking two verti es basedLetnumeri al ulation:rv2 valuer (v1 ) valuer (v2 ),v1 RIDGvaluer (v) dened maximizing valuation fun tion vpr () paths leadv:valuer (v)=maxaPath(v)vpr (a)(1)ensure denition sound, eliminate loops, dene path reverse order:Path(v)= { (v = a1 , a2 , . . . , )|m N,(am , . . . , a1 )path484G (am1 , . . . , a1 )simple}fiAxiomati Foundations Ranking Systemse0.12320.2123fbgh0.1121230.31121230.121230.23210.10.32120Figure 7: Values assigned ursive in-degree algorithmpath valuation fun tion vp: V 7 Qdenes value onform lexi ographiorder in-degrees along path:r(|P(a)|)+11m=10vpr (a1 , a2 , . . . , ) =vpr (a2 , . . . , , a2 ) a1 = > 1n+1vpr (a2 , . . . , )Otherwise.(2)Note vpr (a1 , a2 , . . . , ) innitely ursive ase path ontainsloop ( .f.a1 = > 1).omputation sake redene ase nitely as:vpr (a1 , . . . , , a1 )=Xi=0=Example 7.3.Xr(|P (aj )|)1=mi(n + 1)(n + 1)jj=1(n + 1)mvp (a1 , . . . , ).(n + 1)m 1 r(3)example values assigned parti ular graphidentity fun tion given Figure 7.ursive divisionn = 9,rdenition (2) basedn + 1, values simply de imals whi h onsist atenationin-degrees along maximal path.value zero assignedonsistsitself. valuebvia rst ase (2), path leadingarises pathursive gives value pathaddedr(|P (b)|) = 1(a)(b, a)third ase (2),whi h seen equal 0.divided 10, giving result0.1.valuesc, d, e,arise loop onsisting verti es. Applying se ond ase (2),equations1[3 + vpr (e, d, c, i, e)]101[2 + vpr (d, c, i, e, d)]valuer (e) = vpr (e, d, c, i, e) =101[1 + vpr (c, i, e, d, c)]valuer (d) = vpr (d, c, i, e, d) =101valuer (c) = vpr (c, i, e, d, c) =[2 + vpr (i, e, d, c, i)]10valuer (i)=vpr (i, e, d, c, i)485=fiAltman & Tennenholtzusing (3), get periodi de imals seen Figure 7. values verti eshf , g,assigned using third ase (2). Note omplete maximal paths(e, d, c, i, e)verti es ontain loopthus verti es' values ludeperiodi de imal part, seen Figure 7.ursive in-degree system satises interesting xed point propertyused fa ilitate e ient omputation:Proposition 7.4. Let r : N 7 N monotone nonde reasing fun tion su h r(i)N dene r(0) = 0. value fun tion ursive in-degree rankingsystem satises:valuer (v) =Proof.Denote Pathv(p, v)1n+10r(|P (v)|) + maxpP (v) valuer (p) P (v) 6=set almost-simple dire ted pathsunless immediately looping ba kPath(4)Otherwisepwhi h passp:(p, v) = { (p = a1 , a2 , . . . , )|(am , . . . , a1 )pathG (am1 , . . . , a1 )simple{1, . . . , 2, m} : ai 6= vam1 = v = p}.LetvVvertex. Then,valuer (v)=====maxaPath(v)vpr (a)=r(|P (v)|) + max(v=a1 ,...,am )Path(v)1vpr (a2 , . . . , , a2 ) a1 = > 1 =n+1vpr (a2 , . . . , )Otherwise."#1vpr (a) =r(|P (v)|) + maxmaxn+1pP (v) aPath (p,v)"#1r(|P (v)|) + maxmax vpr (a) =n+1pP (v) aPath(p)1r(|P (v)|) + max valuer (p) .n+1pP (v)Note (5) equal zero0P (v) = ,required.holds, assume ontradi tion existsvpr (a)wlogpath> maxp P (v)maxp P (v) Path (p ,v)(5)(6)show equality (6)vpr (aPath(p)).su h(7)\ Path (p, v), know ai = v {1, . . . , m}. Assumeminimal. Let b denote path (p = a1 , a2 , . . . , ai , p) let c denote(p = ai+1 , . . . , , aj+1 , . . . , ai+1 ) = aj j < (p = ai+1 , . . . , )Path(p)486fiAxiomati Foundations Ranking Systemspv= (p, x, v, p , x)xb = (p, x, v, p)pc = (p , x, v, p )Figure 8: Example paths proof Proposition 7.5.otherwise. example su h paths given Figure 8. NotecPath (p , v),p, pvpr (a)P (v).=bPath(p, v)Now, note(n + 1)j 11vpr (b) +vp (c),(n + 1)j(n + 1)j rthus vpr (a) must vpr (b) vpr (c), ontradi tion assumption (7).Note although might look ompelling use xed point property definition ursive-indegree, well dened, loops indu e innite seriesmaximizations must prove onverges. essen e proof above.xed point property basis e ient algorithm ursive-indegree providedbelow.shall show ranking system fa satisfy RIIA weakenedversion transitivity.Proposition 7.5. Let r : N 7 N monotone nonde reasing fun tion su h r(i)N dene r(0) = 0. ursive in-degree ranking system rank fun tionr satises strong quasi-transitivity RIIA.Proof.0 valuer (v) < 1, thusverti es ordered rst r(|P (v)|) maxpP (v) valuer (p). Therefore, everyomparison prole ha, bi = (a1 , . . . , ak ), b = (b1 , . . . , bl ) ranked follows:xed point result Proposition 7.4 impliesf ha, bi = 1 (k = 0) (r(k) < r(l)) [(r(k) = r(l)) (ak bl )] .ranking proles trivially yields strong quasi-transitivity required.previously presented preliminary version personalized variant ursive in-degree (Altman & Tennenholtz, 2006a). algorithm presented basedequivalent ursive denition value:valuer (v)=vpr (pvr ((), v))(8)(v)P (v) =/av, maxpP (v) pvr (a, v, p) vpvr (a, v) =(ak , . . . , , v)= (a1 , . . . , ak = v, . . . , ),maximum paths taken vpr (pvr (a, v, p)).487(9)fiAltman & TennenholtzAlgorithm 1 E ient algorithm ursive in-degree1. Initialize valuer (v)2. LetV1n+1 r(|P (v)|)r(0)assumed0.set verti es oming edges.|V |3. Iteratetimes:(a) every vertexv V :i. Update valuer (v)4. Sortv V,V1n+1valuer ().5. Output verti esV \ Vr(|P (v)|) + maxpP (v) valuer (p) .weakest, followed verti esVsortedvaluer () ending order.xed point property (4) satises lassi al Bellman prin iple optimality(Stokey & Lu as, 1989),v(xt ) = max [F (xt , xt+1 ) + v(xt+1 )] .Thus, apply dynami programming algorithm e iently ompute values,seen Algorithm 1. Note due limits size graph limitnumber iterations still ensure exa resultO(|V | |E|)time. simple heuristiimproving e ien algorithm pra ti al purposes redu e numberiterations, like xed point algorithms su h PageRank (Page et al., 1998).shall prove orre tness omplexity algorithm.Proposition 7.6. Algorithm 1 outputs verti esDenition 7.2 works O(|V | |E|) time.Proof.V order RID denedLet us rst denote1[r(|P (a1 )| + vpr (a2 , . . . , , . . .)]n+1vpr () = 0.vpr (a1 , a2 , . . . , , . . .)=a1 , . . . , Path(v): a1 , . . . , simple, vpr (a1 , . . . , ) =vpr (a1 , . . . , ). Otherwise = ai , vpr (a1 . . . , ) = vpr (a1 , . . . , ai+1 , . . . , , . . .).Let P(v) set reverse paths v G, simple otherwise.v V:NotevVvaluer (v)ause rst loop=p P(v)maxpPath(v)vpr (p)= maxpP(v)vpr (p),repla ed one maximizing vpr (), thusreasing value.488fiAxiomati Foundations Ranking Systemsiteration step 3 algorithm al ulates""v:"# ##111r0 + maxr+max,rn+1n + 1 |V |1 p|V | P (p|V |1 ) n + 1 |V |p1 P (v)ri = r(|P (pi )|)p0 = v .value equalmaxmaxp1 P (v) p2 P (p1 )maxp|V | P (p|V | 1)|V |+1==Pm (v)max(p1 ,...,p|V |+1 )P|V | (v)maxpP|V |+1 (v)vpr (v),Xi=1|V |i=0ri=(n + 1)i+1ri=(n + 1)iset reverse paths length|V |X(10)v,simple otherwise.Asverti es, two verti es dier value assigned valuefun tion (1) must also dier value (10) al ulated algorithmdire tion.shall prove time omplexity algorithm, tra ing ea h step. Steps 1O(|V |) time. iteration step 3 repeated |V | times, every vertexO(|P (v)|) al ulations, ea h iteration takes O(|E|) time thus totaltime O(|V | |E|). Step 4 takes O(|V | log |V |) O(|V | log |E|) O(|V | |E|). Finally,output step 5 takes O(|V |) time. every step takes O(|V | |E|) time,2 takeVperformsentire algorithm.8. Axiomatization Approval VotingSe tions 5 6 seen mostly negative results whi h arise trying ommodate (weak) transitivity RIIA. shown although ea h axiomssatised separately, exists general ranking system satises axioms.Tennenholtz (2004) previously shown non-trivial ranking system satises(weak) transitivity, previous se tion seen su h system RIIA. However, provided representation theorem new system.se tion provide representation theorem ranking system satisesRIIA weak transitivity approval voting ranking system (see Denition 2.5).axiomatization provide se tion shows power RIIA, showsexists one (interesting) ranking system satises without introdu ing transitiveee ts.Fishburn (1978) axiomatized Approval Voting ranking system ontextial hoi e, output algorithm ranking, rather set winners.two distin settings similar, thus Fishburn's axiomatization approvalvoting great relevan e work. shall ompare two axiomatizations laterse tion.order spe ify axiomatization, following lassi al denitionstheory ial hoi e:489fiAltman & Tennenholtzpositive response axiom (sometimes referredpositive responsiveness ) essentiallymeans agent eives additional votes, rank must improve:Denition 8.1.F satises positive responseG = (V, E) (v1 , v2 ) (V V ) \ E , v1 6= v2 , v3 V :(V, E (v1 , v2 )). v3 FG v2 , v3 FG v2 .LetFranking system.graphsLetG =anonymity neutrality axioms mean names voters alternativesrespe tively matter ranking:Denition 8.2.ranking system: V 7 V ,v2 v1 F(V,E ) v2 .permutationsv1 F(V,E)F satises anonymity G = (V, E),v1 , v2 V : Let E = {((v1 ), v2 )|(v1 , v2 ) E}. Then,Denition 8.3.ranking system F satises neutrality G = (V, E), per : V 7 V , v1 , v2 V : Let E = {(v1 , (v2 ))|(v1 , v2 ) E}. Then,v2 (v1 ) F(V,E ) (v2 ).mutationsv1 F(V,E)Arrow's lassi al Independen e Irrelevant Alternatives axiom requires relativerank two agents dependant set agents preferred one other.Denition 8.4.FG = (V, E),PG (v1 ) \ PG (v2 ) = PG (v1 ) \ PG (v2 )v1 FG v2 v1 FG v2 .natives (AIIA)ranking systemsatisesArrow's Independen e Irrelevant Alter-G = (V, E ), v1 , v2 V : LetPG (v2 ) \ PG (v1 ) = PG (v2 ) \ PG (v1 ). Then,representation theorem states together positive response RIIA,one three independen e onditions (anonymity, neutrality, AIIA) essential su ient ranking systemAV .addition, showlassi al ial hoi e setting onsidering two-level preferen es, positive response,anonymity, neutrality, AIIA essential su ient representation approvalvoting. result extends well known axiomatization majority rule due May(1952):Proposition 8.5. (May's Theorem) ial welfare fun tional two alternativesmajority ial welfare fun tional satises anonymity, neutrality, positiveresponse.formally state theorem:Theorem 8.6. Letequivalent:F general ranking system. Then, following statements1. F approval voting ranking system (F = AV )2. F satises positive response, anonymity, neutrality, AIIA3. F satises positive response, RIIA, either one anonymity, neutrality, AIIA490fiAxiomati Foundations Ranking SystemsvxuFigure 9: Example graphProof.easy seeAVGproleh(1, 3, 3), (2, 4)isatises positive response, RIIA, anonymity, neutrality,AIIA. remains show (2) (3) entail (1) above.F satises positive response, anonymity, neutrality,G = (V, E) graph let v1 , v2 V agents. AIIA,ranking v1 v2 depends sets PG (v1 ) \ PG (v2 ) PG (v2 ) \prove (2) entails (1), assumeAIIA. LetrelativePG (v1 ).narrowed onsideration set agents preferen estwo alternatives, apply Proposition 8.5 omplete proof.prove (3) entails (1), assumeanonymity neutrality AIIA.parison proles. Letf : P 7 {0, 1}FFsatises positive response, RIIA eithersatises RIIA limit dis ussion om-fun tion denition RIIA.a. positive response(1, 1, . . . , 1) (1, 1, . . . , 1) n m. Let P = h(a1 , . . . , ), (b1 , . . . , bm )i| {z }| {z }denition RIIA, easy seealso easy seenG = (V, E)h(1, 3, 3), (2, 4)i Figure 9):omparison prole. LetproleVfollowing graph (an example su h graph= {x1 , . . . , xmax{an ,bm } }{v1 , . . . , vn , v 1 , . . . , vn , v}{u1 , . . . , um , u1 , . . . , um , u}E = {(xi , vj )|i aj } {(xi , uj )|i bj }{(vi , v)|i = 1, . . . , n} {(ui , u)|i = 1, . . . , m}.491fiAltman & Tennenholtzeasy see graphpermutation:G, vuvivi(x) =uiuixFvfollowingOtherwise .Fsatises:E = {((x), y)|(x, y) E}. Note graph (V, E )h(1, 1, . . . , 1), (1, 1, . . . , 1)i, thus v F(V,E ) u n m.| {z } | {z }usatisfy prolenarbitraryFvsatises neutrality, letuu F(V,E) v u F(V,E ) v , thus provingomparison prole P , thus F = AV .satisfy prolef (P ) = 1 nE = {(x, (y))|(x, y) E}. Note graph (V, E )h(1, 1, . . . , 1), (1, 1, . . . , 1)i, thus v F(V,E ) u n m.| {z } | {z }narbitraryu F(V,E) v u F(V,E ) v , showingomparison prole P , thus F = AV .neutrality,satises anonymity, letanonymity,Letx = vix = vix = uix = uiremainder proof depends whi h additional axiomP.satisfy prolef (P ) = 1 nF satises AIIA, let E = {(x, (y))|(x, y) E} before. So, also v F(V,E )u n m. Note PG (v) = P(V,E ) (v) PG (u) = P(V,E ) (u), AIIA,u F(V,E) v u F(V,E ) v , thus before, F = AV .axiomatization approval voting, spe ally one (2) relatedprevious axiomatization Fishburn (1978). axiomatizations share requirements Anonymity1Neutrality, dier additional assumptions: Fishburn'srequirements refer relations results dierent voter sets, whi h annoteasily used ranking systems setting, voters also alternatives,requirements relate hanges preferen es single agent ability (positiveresponse) inability (AIIA) inuen e nal result. requirements may mappedFishburn's setting would probably lead distin axiomatization approval votingsetting.9. Con luding RemarksReasoning preferen es preferen e aggregation fundamental task reasoningmulti-agent systems (see e.g. Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004;Conitzer & Sandholm, 2002; LaMura & Shoham, 1998).typi al instan e preferen eaggregation setting ranking systems. Ranking systems fundamental ingredientsfamous tools/te hniques Internet (e.g. Google's PageRankeBay's reputation systems, among many others).1. Fishburn onsider Anonymity axiom, rather denes ial hoi e model allowanonymous fun tions.492fiAxiomati Foundations Ranking SystemsMoreover, task building su essful ee tive on-line trading environmentsome entral hallenge AI ommunity (Boutilier, Shoham, & Wellman, 1997;Monderer, Tennenholtz, & Varian, 2000; Sandholm, 2003). Ranking systems believedfundamental establishment su h environments.Although reputationalways major issue e onomi (see e.g. Kreps & Wilson, 1982; Milgrom & Roberts,1982), reputation systems ome entral ently due fainuential powerful Internet sites ompanies put reputation systemsore business.aim paper treat ranking systems axiomati perspe tive.lassi al theory ial hoi e lay foundations large part rigorouswork multi-agent systems. Indeed, lassi al results theory hanismdesign, su h Gibbard-Satterthwaite Theorem (Gibbard, 1973; Satterthwaite, 1975)appli ations theory ial hoi e. Moreover, previous work AI employedtheory ial hoi e obtaining foundations reasoning tasks (Doyle & Wellman, 1989)multi-agent oordination (Kr-Dahav & Tennenholtz, 1996). however interestingnote ranking systems suggest novel new type theory ial hoi e.see point espe ially attra tive, main reason entrating studyaxiomati foundations ranking systems.paper identied two fundamental axioms ranking systems, ondu tedbasi axiomati study su h systems. parti ular, presented surprising impossibilityresults, omplemented new ranking algorithm, representation theoremwell-known approval voting heme.knowledgementswork partially supported grant Israeli ien e Foundations(ISF).Referen esAltman, A., & Tennenholtz, M. (2005). Ranking systems: PageRank axioms.Pro eedings 6th ACM onferen e Ele troni ommer e,EC '05:pp. 18, New York,NY, USA. ACM Press.Altman, A., & Tennenholtz, M. (2006).systems..Pro . AAAI-06.Quantifying entive ompatibility rankingAltman, A., & Tennenholtz, M. (2007a). axiomati approa h personalized rankingsystems.Pro . 20th International Joint Conferen e Arti ial Intelligen e.Altman, A., & Tennenholtz, M. (2007b). entive ompatible ranking systems.AAMAS-07.Arrow, K. (1963).ial Choi e Individual Values (2nd Ed.).Bogomolnaia, A., Moulin, H., & Stong, R. (2005).mous preferen es.Yale University Press.Colle tive hoi e di hoto-Journal E onomi Theory, 122 (2),165184.http://ideas.repe .org/a/eee/jetheo/v122y2005i2p165-184.html.493Pro .availablefiAltman & TennenholtzBoutilier, C., Shoham, Y., & Wellman, M. (1997). Spe ial issue e onomi prin iplesmulti-agent systems.Arti ial Intelligen e, 94.Boutilier, C., Brafman, R. I., Domshlak, C., Hoos, H. H., & Poole, D. (2004). Cp-nets: toolrepresenting reasoning onditional eteris paribus preferen e statements..J. Artif. Intell. Res. (JAIR), 21,135191.Brin, S., & Page, L. (1998). anatomy large-s ale hypertextual Web sear h engine.Computer Networks ISDN Systems, 30 (17),107117.Conitzer, V., & Sandholm, T. (2002). Complexity hanism design.18th onferen e un ertainity Arti ial Intelligen e (UAI-02),Pro eedingspp. 103110.Doyle, J., & Wellman, M. (1989). Impediments Universal Preferen e-Based Default The-Pro eedings 1st onferen e prin iples knowledge representationreasoning.ories.Journal E onomi Theory, 19 (1), 180185. available http://ideas.repe .org/a/eee/jetheo/v19y1978i1p180-Fishburn, P. C. (1978). Axioms approval voting: Dire proof.185.html.Gibbard, A. (1973). Manipulation voting hemes.E onometri a, 41, 587601.Pro eedings6th onferen e theoreti al aspe ts rationality knowledge (TARK).Kr-Dahav, N. E., & Tennenholtz, M. (1996). Multi-Agent Belief Revision.Journal E onomiKreps, D., & Wilson, R. (1982). Reputation imperfe information.Theory, 27, 253279.LaMura, P., & Shoham, Y. (1998). Conditional, Hierar hi al Multi-Agent Preferen es.Pro eedings Theoreti al Aspe ts Rationality Knowledge,May, K. O. (1952).pp. 215224.set independent, ne essary su ient onditions simplemajority de ision.E onometri a, 20 (4),68084.Milgrom, P., & Roberts, J. (1982). Predation, reputation entry deterren e.E onomi Theory, 27, 280312.JournalMonderer, D., Tennenholtz, M., & Varian, H. (2000). Game theory arti ial intelligen e.Spe ial issue Games E onomi behavior.Moulin, H. (1991).Axioms Cooperative De ision Making.Cambridge University Press.Page, L., Brin, S., Motwani, R., & Winograd, T. (1998). PageRank itation ranking:Bringing order web. Te hni al Report, Stanford University.Pala ios-Huerta, I., & Volij, O. (2004). measurement intelle tual inuen e.metri a, 73 (3).E ono-Resni k, P., & Ze khauser, R. (2001). Trust among strangers internet transa tions: Empiri al analysis ebay's reputation system. Working Paper NBER workshopempiri al studies ele troni ommer e.Resni k, P., Ze khauser, R., Friedman, R., & Kuwabara, E. (2000).Communi ations ACM, 43 (12),4548.494Reputation systems.fiAxiomati Foundations Ranking SystemsSandholm, T. (2003). Making markets demo ra work: story entives om-Pro eedings International Joint Conferen e Arti ial Intelligen e(IJCAI-03), pp. 16491671.puting.Satterthwaite, M. (1975). Strategy proofness arrow's onditions: Existen e orresponden e theorems voting pro edures ial welfare fun tions..E onomi Theory, 10, 187217.Stokey, N. L., & Lu as, R. E. (1989).Journalursive Methods E onomi Dynami s.HarvardUniversity Press.Tennenholtz, M. (2004). Reputation systems: axiomati approa h.20th onferen e un ertainity Arti ial Intelligen e (UAI-04).495Pro eedingsfiJournal Artificial Intelligence Research 31 (2008) 591-656Submitted 11/07; published 3/08Multiagent ApproachAutonomous Intersection ManagementKurt DresnerPeter Stonekdresner@cs.utexas.edupstone@cs.utexas.eduDepartment Computer Sciences, University Texas Austin1 University Station [C0500], Austin, TX 78712 USAAbstractArtificial intelligence research ushering new era sophisticated, mass-markettransportation technology. computers already fly passenger jet bettertrained human pilot, people still faced dangerous yet tedious task driving automobiles. Intelligent Transportation Systems (ITS) field focuses integratinginformation technology vehicles transportation infrastructure make transportation safer, cheaper, efficient. Recent advances point futurevehicles handle vast majority driving task. autonomous vehiclesbecome popular, autonomous interactions amongst multiple vehicles possible. Current methods vehicle coordination, designed work human drivers,outdated. bottleneck roadway efficiency longer drivers,rather mechanism drivers actions coordinated. open-roaddriving well-studied more-or-less-solved problem, urban traffic scenarios, especiallyintersections, much challenging.believe current methods controlling traffic, specifically intersections,able take advantage increased sensitivity precision autonomous vehiclescompared human drivers. article, suggest alternative mechanismcoordinating movement autonomous vehicles intersections. Driversintersections mechanism treated autonomous agents multiagent system.multiagent system, intersections use new reservation-based approach built arounddetailed communication protocol, also present. demonstrate simulationnew mechanism potential significantly outperform current intersectioncontrol technologytraffic lights stop signs. mechanism emulatetraffic light stop sign, subsumes popular current methods intersectioncontrol. article also presents two extensions mechanism. first extensionallows system control human-driven vehicles addition autonomous vehicles.second gives priority emergency vehicles without significant cost civilian vehicles.mechanism, including extensions, implemented tested simulation,present experimental results strongly attest efficacy approach.1. Introductionconcepts, any, embody goals aspirations artificial intelligence wellfully autonomous robots. Countless films stories made focusfuture filled humanoid agents which, violently overthrowing humanmasters, run errands, complete menial tasks, perform duties would difficultdangerous humans. However, machines sense, think about, take actionsreal world around us longer stuff science fiction fantasy. Researchc2008AI Access Foundation. rights reserved.fiDresner & Stoneinitiatives like Robocup (Noda, Jacoff, Bredenfeld, & Takahashi, 2006) DARPAGrand Challenge (DARPA, 2007) shown current AI produce autonomous,embodied, competent agents complex tasks like playing soccer navigating MojaveDesert, respectively. certainly small feat, traversing barren desert devoidpedestrians, narrow lanes, multitudes fast-moving vehicles typicaldaily task humans. Gary Bradski, researcher Intel Corp. said followingsuccessful completion 2005 Grand Challenge Stanley, modified VolkswagenTouareg, need teach drive traffic (Johnson, 2005). Since then,competitors 2007 DARPA Urban Challenge took significant strides towards nextmilestone, though competition cars need sense traffic signs signalstraffic relatively sparsemore characteristic suburban dense urban settings.modern urban settings, automobile traffic collisions lead endless frustrationwell significant loss life, property, productivity. 2004 study 85 U.S.cities researchers Texas A&M University estimated annual time spent waitingtraffic 46 hours per capita, 16 hours 1982 (Texas Transportation Institute,2004). Americans burn approximately 5.6 billion gallons fuel year simply idlingengines. told, annual financial cost traffic congestion swollen $14 billion$63 billion (in 2002 US dollars) period. cost wastedtime fuel due congestion pales comparison costs associated automobilecollisions. 2002 report, National Highway Traffic Safety Administration (NHTSA)put annual societal cost automobile collisions U.S. $230 billion (NationalHighway Traffic Safety Administration, 2002).Fully autonomous vehicles may able spare us much, nearly costs.autonomous driver agent much accurately judge distances velocities,attentively monitor surroundings, react instantly situations would leave(relatively) sluggish human driver helpless. Furthermore, autonomous driver agentget sleepy, impatient, angry, drunk. Alcohol, speeding, running red lightstop three causes automobile collision fatalities. Autonomous driver agentsproperlyprogrammedwould eliminate three.fully autonomous vehicle drive traffic everythingobeying speed limit staying lane detecting tracking pedestrianschoosing best route mall. certainly complex task, advances artificial intelligence, specifically, Intelligent Transportation Systems (ITS), suggestmay soon reality (Bishop, 2005). Cars already equipped featuresautonomy adaptive cruise control, GPS-based route planning (Rogers, Flechter,& Langley, 1999; Schonberg, Ojala, Suomela, Torpo, & Halme, 1995), autonomoussteering (Pomerleau, 1993; Reynolds, 1999). current production vehicles even sportfeatures. DaimlerBenzs Mercedes-Benz S-Class adaptive cruise control systemmaintain safe following distance car front it, apply extrabraking power determines driver braking hard enough. ToyotaBMW currently selling vehicles parallel park completely autonomously, evenfinding space park without driver input. 2008, General Motors (GM)plans release nearly autonomous vehicle European Opel brand. 2008Opel Vectra able drive speeds 60 miles per hour, even heavytraffic. Using video camera, lasers, lot processing power, car able592fiA Multiagent Approach Autonomous Intersection Managementidentify traffic signs, curves street, lane markings, well vehicles.end decade, GM hopes incorporate system many models.Autonomous vehicles coming. article, present well-defined multiagentframework manage large numbers autonomous vehicles intersections.still exist many technical hurdles rigorous safety tests, show simulationframework may someday dramatically improve safety efficiency roadways.1.1 Multiagent Systemsautonomous vehicles become prevalent, possibility autonomousinteractions among multiple vehicles becomes interesting. Multiagent Systems (MAS)subfield AI aims provide principles construction complex systems involving multiple agents mechanisms coordination independent agentsbehaviors (Stone & Veloso, 2000). Automobile traffic vast multiagent system involving millions heterogeneous agents: commuters, truck drivers, pedestrians, cyclists,even traffic-directing police officers. mechanism coordinates behavioragents complex conglomeration laws, signs, signaling systems vary slightlystate state widely country country. mechanism designedwork closely agentsthe humansthat populate multiagent system. Trafficlights leave time green lights allow slower perhaps impatient driversclear intersections. Street signs colored brightly make easier see usesimple designs make easy understand. Drivers must maintain sufficient following distance make slow reaction times. Speed limits ensure humansenough time process necessary information position velocitiesvehicles. Safety buffers myriad sorts built almost every part systemcompensate limitations humans.first generation autonomous vehicles undoubtedly need work withinsystem. Processing-intensive vision algorithms identify extract semantic information signs signals, special subroutines ensure vehicles exceedspeed limit, middle night, another moving vehicle blocks,autonomous vehicle come stop red light. However, vehiclesautonomous limitations eliminated, make sense use mechanismdesigned control fundamentally different agentsit inefficient, termsprocessing power getting vehicles destinations quickly.Replacing soon-to-be-outdated mechanism inherently multiagent challengeseveral reasons. First, viable single-agent solutions; one computer cannothandle vehicles world. Second, vehicles constantly entering leavingcountries, states, cities, towns, solution flexible distributed.Third, different agents separate, sometimes conflicting objectives.human-driven vehicles, autonomous vehicles act self-interest, attemptingminimize travel time, distance, fuel use. types agents may aim maximizesocial welfare, minimizing quantities average vehicle. Finally, even singlecomputer could control citys worth traffic, would sensitive point failure.593fiDresner & Stone1.2 Intersectionsopen road, automobiles less completely autonomous. Furthermore,little need simple reactive behavior keeps vehiclelane, maintains reasonable distance vehicles, avoids obstacles. Even lanechanging safely efficiently accomplished autonomous vehicle (Hatipo,Redmill, & Ozguner, 1997). algorithmic AI aspects open-road drivingessentially solved. problem difficult: pedestrians cyclistsvehicles travel direction similar velocities; relative movement smoothrare.Intersections completely different story: vehicles constantly cross paths, manydifferent directions. vehicle approaching intersection quickly find situation collision unavoidable, even acted optimally. Traffic statisticssupport sensitive nature intersections. Vehicle collisions intersections accountanywhere 25% 45% collisions. intersections make smallportion roadway, wildly disproportionate amount. Collisions intersections tend involve cars traveling different directions, thus frequently resultgreater injury damage. modern-day intersections controlled traffic lightsstop signs, former usually reserved larger, busier intersections. busiestintersectionsfreeway interchangeslarge, extremely expensive cloverleaf junctionsbuilt.vastly improved precision control sensing autonomous vehiclesoffer, must efficient safe way manage intersections. Imaginescenario autonomous vehicle stops red light middle nightvehicles nearby. least, vehicle able communicatepresence intersection, verify vehicles nearby, turnlight green stopped vehicle. ambitious implementation, intersectioncould turn light green preemptively, obviating stop altogether. article, gostep further, allowing vehicles call ahead reserve space-time intersection.remainder article organized follows. Section 2, describe problem autonomous intersection management framework attemptsolve problem. Section 3, describe implementation solution framework. Section 4 presents experiments empirical results. Section 5, conductfailure mode analysis proposed mechanism. Related work discussed Section 6.Section 7 briefly explores avenues future research concludes.2. Problem Statement Solution FrameworkAutomobile traffic already huge multiagent system millions human driver agents,various signaling control mechanisms, complicated protocol governing actionsdriver agents, form traffic laws. However, human driversreplaced autonomous driving agents, elements multiagent systemrethought. Traffic lights, stop signs, current traffic laws designedhuman drivers mind fail take advantage increased sensitivity precisioncomputerized driver agents. want autonomous vehicles operate high efficiencysafety, must design new way coordinate them. section, formulate594fiA Multiagent Approach Autonomous Intersection Managementproblem trying solve present framework within believeproblem best solved.2.1 Desideratadesigning mechanism traffic controlled intersections, aim satisfyfollowing list properties.Autonomy vehicle autonomous agent. entire mechanismcentrally controlled, would susceptible single-point failure, require massiveamounts computational power, exert unnecessary control vehicles situationsperfectly capable controlling themselves.Low Communication Complexity keeping number messages amountinformation transmitted minimum, system afford put communicationreliability measures place. Furthermore, vehicle, autonomous agent, mayprivacy concerns respected. Keeping communication complexity lowalso make system scalable.Sensor Model Realism agent access sensors availablecurrent-day technology. mechanism rely fictional sensor technologymay never materialize.Protocol Standardization mechanism employ simple, standardized protocol communication agents. Without standardized protocol, agentwould need understand internal workings every agent interacts.requirement would forbid introduction new agents system. open, standardized protocol would make adoption system easier simpler private vehiclemanufacturers.Deadlock/Starvation Avoidance Deadlocks starvation occursystem. Every vehicle approaching intersection eventually cross, evenbetter rest agents leave vehicle stranded.Incremental Deployability system incrementally deployable, twosenses. First, possible set selected intersections use system,slowly expand intersections needed. Second, system functioneven autonomous vehicles. stage deployment, whetherincrease proportion autonomous vehicles number equipped intersections,overall performance system improve. point net disincentivecontinue deploying system exist.Safety Excepting gross vehicle malfunction extraordinary circumstances (e.g. natural disasters), long follow protocol, vehicles never collideintersection. Note stronger guarantee possibleas modern mechanisms,suicidal human driver always steer vehicle oncoming traffic. Furthermore,system safe event total communication failure. messages droppedcorrupted, safety system compromised. impossible prevent negative effects due communication failures, negative effects595fiDresner & Stoneisolated efficiency. message gets dropped, make someone arrive 10 secondslater destination, cause collision. rare unpreventablecase gross vehicle malfunction, system react attempt minimize damagecasualties.Efficiency Vehicles get across intersection way little timepossible. quantify efficiency, introduce delay, defined amount additionaltravel time incurred vehicle result passing intersection.2.2 Reservation Ideadesiderata, modern-day traffic lights stop signs completely satisfylast one. many accidents take place intersections governed traffic lights,accidents rarely, ever, fault traffic light system itself, ratherhuman drivers. However, show, traffic lights stop signs terribly inefficient.vehicles traversing intersections equipped mechanisms experiencelarge delays, intersections manage somewhat limited amounttraffic. stretch open road accommodate certain level traffic givenvelocity. capacity intersection involving road trivially boundedcapacity road. also show, capacity traffic lights stop signsmuch less roads feed them. aim research createintersection control mechanism exceeds efficiency traffic lights stop signs,maintaining desiderata.desiderata mind, developed multiagent approach direct vehiclesintersections efficiently. approach, computer programs called driveragents control vehicles, arbiter agent called intersection manager placedintersection. driver agents call ahead attempt reserve blockspace-time intersection. intersection manager decides whether grant reject requested reservations according intersection control policy. Figure 1 shows oneinteraction driver agent intersection manager. system functionsanalogously human attempting make reservation hotelthe potential guestspecifies arriving, much space required, long staybe; human reservation agent determines whether grant reservation,according hotels reservation policy. guest need understandhotels decision process, driver agents require knowledgeintersection control policy used intersection manager.vehicle approaches intersection, vehicles driver agent transmits reservation request, includes parameters time arrival, velocity arrival, wellvehicle characteristics like size acceleration/deceleration capabilities, intersection manager. intersection manager passes information policy,determines whether safe vehicle cross intersection. policydeems safe, intersection manager responds driver agent messageindicating reservation accepted including supplemental restrictionsdriver must observe order guarantee safety traversal. Otherwise,intersection manager sends message indicating reservation request rejected, possibly including grounds rejection. addition confirming rejecting596fiA Multiagent Approach Autonomous Intersection ManagementREQUESTDriverAgentREJECTPostprocessPreprocessNo, ReasonYes,RestrictionsIntersectionControl PolicyCONFIRMIntersection ManagerFigure 1: One driver agents attempts make reservation. intersection manager responds based decision intersection control policy.request, intersection manager may respond counter-offer. driver agentmay pilot vehicle intersection without reservation. Even reservation, driver agent may proceed intersection according parametersrestrictions associated reservation. sake brevity, may refervehicle obtaining reservation, rather specifically stating driveragent vehicle obtains reservation.3. Building Systemsection describes realization reservation idea implemented algorithm.process involved developing simulator run algorithm, wellcreating behaviors agents protocol communicate.3.1 Custom Simulatororder empirically evaluate reservation idea, built custom time-based simulator.simulator models area 250 250 m. intersection locatedcenter area, size determined number lanes travelingdirection, variable. assume throughout vehicles drive right sideroad, however assumption required system work properly.Figure 2 shows screenshot simulators graphical display. time step,simulator:1.2.3.4.5.Probabilistically spawns new vehiclesProvides sensor input vehiclesAllows driver agents actUpdates position vehicles according physical modelRemoves vehicles outside simulated area completed journey3.1.1 VehiclesVehicles simulator following properties:597fiDresner & StoneFigure 2: screenshot simulator action.Vehicle Identification Number (VIN)LengthWidthDistance front vehicle front axleDistance front vehicle rear axleMaximum velocityMaximum accelerationMinimum accelerationMaximum steering angleSensor rangefollowing state variables:PositionVelocityHeadingAccelerationSteering angledriver agent assigned pilot vehicle may access quantities,without noise, depending configuration simulator. driver agent mayalso access several simulated external sensors: list vehicles within sensor range,simplified laser range finder. detailed description simplified laser range finderfound Appendix A.steering angle angle front wheels respect vehicle.angle changed driver agent, simulator limits ratechanged. limitation simulates fact even computerized driver cannotmove steering wheel infinitely fast. introducing limitation, accuratelyapproximate vehicle turning, including dangerous aspects. driver598fiA Multiagent Approach Autonomous Intersection Managementcannot turn wheels instantaneously, must ensure drive around cornershigh velocityit may able straighten quickly enough windveering road instead.constants representing distance front vehicle frontrear axles allow accurate simulation vehicle turning. Specifically, allowsimulator treat different styles vehicle differently. distance frontrear axles known wheelbase. Vehicles shorter wheelbases turnsharply longer wheelbasesif simulator accurately model turning,needs access important parameters. Furthermore, vehicle long hoodturn differently vehicle whose front wheels located nearer frontvehicle.3.1.2 LanesLanes system consist directed line segment, width, left right bordersvehicles may may permitted cross, references lanes, any, borderright left side. real-life implementation, would software constructvehicles driver agents would use perform lane following changing. vehiclewants change lanes left right, must first establish vehicle allowedcross border lanes, feed lane-following algorithmreference desired lane.3.1.3 Physical Modeltime step, simulator must update position every vehicle.model planar vehicle kinematics dynamics, must make assumptions.First, assume vehicles skid road. Second, assume vehiclesmove according following differential equations non-holonomic motion:x= v cos()= v sin()tan=vLequations, x, y, describe vehicles position orientation, v represents vehicles velocity, describes vehicles steering angle, L vehicleswheelbase. solve equations holding v constant time step.3.1.4 Measuring DelaySection 2.1, introduced delaythe increase travel time vehicle duepresence intersection. simulation, measured first assumingopen road, vehicle maintain velocity speed limit. vehicletimestamped enters simulation keeps track far traveled.vehicle removed simulation, total delay calculated differencelong actually took travel far long would take599fiDresner & Stonevehicle travel speed limit entire journey. measure, zero delaypossible vehicle turning, needs slow order safely maketurn. practice, compare delays vehicles delays using policyallows vehicles intersection unhindered, also non-zerovehicles turn road congested. way, quantify effectintersection vehicle, directly (not able go intersectionrequests rejected) indirectly (having decelerate another vehiclecannot get through).3.2 Communication Protocolsection presents detailed communication protocol vehicles intersectionscoordinate behavior. protocol presented offers three major benefits:information agents goes one monitorable channel,makes reasoning communication straightforward.limiting interactions agents message types, ensureagent unrealistic amount control another.agents way communicate identical intersection management policy driver agent policy. Thus, vehicle cross intersection withoutidea policy intersection manager usingit simply sendsreceives messages obeys rules.protocol consists several message types kind agent, wellrules governing messages sent sorts guarantees accompanythem. Driver agents send Request, Change-Request, Cancel, Done messages. Request Change-Request used driver agent wants makereservation change existing reservation, respectively. types request messageinclude relevant properties vehicle. Driver agents send Cancel messagewant cancel existing reservation. vehicle successfully crossedintersection, driver agent sends Done message intersection manager.Cancel Done messages include VIN vehicle, well identifierreservation cancelled reported complete.Intersection managers send Confirm, Reject, Acknowledge messages,well special Emergency-Stop message, used intersectionmanager detects major problem intersection (see Section 5). Confirm sentintersection manager approves Request Change-Request message.includes information describing reservationa unique identifier reservation,start time, start lane, departure lane (which identical start lane unlessvehicle turning), list constraints vehicles accelerationintersection. Reject message used reject either Request Change-Requestmessage. intersection sends Acknowledge message response CancelDone messages sent vehicles. detailed specification protocol includingfull syntax semantics found Appendix B.600fiA Multiagent Approach Autonomous Intersection Management3.2.1 Message Corruption Lossassume messages digitally signed, possibility undetectedmessage corruption acceptably small. protocol designed specifically robustmessage loss. message sent receivedor deemed corruptedthe worstthing happen additional delay. collisions occur due lost messages.vehicle makes reservation request, assume space reservedreceives confirmation intersection manager. Request message dropped,Confirm message follow. Confirm Reject message dropped, vehiclesimply try againit wont assume valid reservation.3.2.2 Enabling Policy Switchingprotocol hides implementation policy driver agentsidea intersection manager making decisions, guaranteedfollow them, safe. Thus, stipulations policy mustremain fixed. intersection manager could use one policy one moment switchappropriate policy later, provided still guarantee vehicles followingprotocol make safely across intersection.3.2.3 Intersection Managerintersection manager acts stable communication interface driver agentsintersection control policy therefore contain lot functionality.However, regardless policy makes decision, intersection manager mustpresent interface driver agents. general intersection manager algorithmshown Algorithm 1. it, Cancel messages Done messages treated almostidenticallywhen Done message received, intersection manager knowspolicy erase information related reservation. However, Done messagealso may contain information useful intersection manager policy.example, vehicle sends Done message, could include delay experiencedcrossing intersection, providing intersection manager sort reward signal,judge performance.3.3 Driver Agentvast majority research focuses make better intersection managercontrol policy. parts designed work driver agent followsprotocol. However, testing purposes, driver agent implementation required. Despitefact lot work went driver agent (it probably intricatepart system), focus article. refer interested readerAppendix C, explains driver agent detail. brief, driver agent estimatestime velocity reach intersection, requests appropriatereservation. granted reservation, attempts arrive schedule. determinesunable keep reservation, cancels reservation. believessubstantially early, attempts change earlier reservation. unable getreservation, decelerates (down minimum velocity) requests again.601fiDresner & StoneAlgorithm 1 intersection manager algorithm. Vehicle V sends messageintersection manager, responds according policy P .1: loop2:receive message V3:message type Request4:process request new reservation P5:P accepts request6:send Confirm message V containing reservation returned P7:else8:send Reject message V9:else message type Change-Request10:process request change reservation P11:P accepts request12:send Confirm message V containing reservation returned P13:else14:send Reject message V15:else message type Cancel16:process cancel P17:send Acknowledge message V18:else message type Done19:record statistics supplied message20:process cancel P21:send Acknowledge message Venter intersection without reservation. open road, driver agent employssimple lane-following algorithm, maintains following distance one secondvehicle vehicle front it.3.4 FCFS Policypoint, weve described substrate infrastructure enables research.remainder Section 3 introduces core contribution article main payoffcreating infrastructure, namely intersection control policy enables fine-grainedcoordination vehicles intersections, subsequent dramatic decrease delays.intersection manager communicates directly driver agents, intersection control policy brains behind operation. describe intersectioncontrol policy created reservation idea discussed Section 2.2.First Come, First Served nature policy, name policy FCFS. main partpolicythe request processingis shown Algorithm 2.Recall FCFS enables car reserve advance space-time needs crossintersection. Planning ahead allows vehicles coming directions traverseintersection simultaneously minimal delay. policy works follows:intersection divided n n grid reservation tiles, ngranularity policy.602fiA Multiagent Approach Autonomous Intersection ManagementUpon receiving reservation parameters approaching driver agent, policy runs internal simulation trajectory vehicle across intersectionusing parameters.time step internal simulation, policy determines reservationtiles occupied vehicletime simulation requesting vehicle occupies reservation tilealready reserved another vehicle, policy rejects request. Otherwise,policy accepts reservation reserves appropriate tiles timesrequired.Figure 3 shows graphical depiction concept behind FCFS policy.(a) Successful(b) RejectedFigure 3: internal simulation granularity-8 FCFS policy. black rectangles represent vehicles, shaded tiles tiles currently reserved. 3(a),vehicles request accepted, intersection reserves set tiles timet. 3(b), second vehicles request rejected simulationtrajectory, policy determines requires tile (darkly shaded) alreadyreserved first vehicle time t.concept behind FCFS sound, requires modificationswork reliably, safely, efficientlyeven simulation. remainder section,present modifications, created response early experimentalresults documented Section 4.3.4.1 Determining Outbound Lanefirst implementation reservation system, vehicles capable travelingstraight lines. allowed vehicles turn, became apparentdriver agents determine lane use exit intersection. Instead,intersection manager, information intersection, makesdecision. Driver agents indicate request message way intend turn,complicated intersections, direction intend go. intersectioncontrol policy decides outbound lane place vehicle. experimentsdocumented article, FCFS policy chooses natural lane: left603fiDresner & Stoneright turns, chooses nearest lane, whereas vehicles going turn,chooses lane planning arrive intersection. However, policycould behave differently configured so. example, policy create prioritylist outbound lanes based inbound lane, run internal simulations usinglanes found acceptable configuration. turning vehicles, listwould set outbound lanes correct direction, sorted nearest farthest.vehicles turning, would spiral lane arrivefirstarrival lane, lane left, lane right, two lanes left,forth. manner, vehicle might otherwise request rejectedobtain reservation different path intersection.3.4.2 Acceleration IntersectionGiven set reservation parameters, infinite number possible trajectoriesvehicle take, allowed accelerate intersection.time step, driver agent could set vehicles acceleration value withinlimits vehicles capabilities. Depending trajectory, intersection managermay may able grant reservationone set accelerations may causecollide another vehicle, second set might let vehicle safely.reason, acceleration intersection must constrained intersection controlpolicy. Allowing driver agents decide acceleration within intersection wouldrequire policy much conservative estimating vehicle trajectories, therebyreducing efficiency substantially. Instead, responsibility intersection controlpolicy choose safe efficient acceleration schedule include Confirmmessage, driver agents request accepted.Choosing best acceleration schedule requesting vehicle, evenbasic level, finding schedule intersection manager grant reservation, difficult challenge intersection control policy. initial solutionallow acceleration within intersection; driver agents required maintainvelocity throughout entire trajectory. approach several major flaws,severe causing deadlock scenario vehicles traversed intersectionslowly, unable recover slightest decelerations. scenariodescribed much detail Section 4.2.FCFS policy, implemented still takes fairly straightforward approachproblem determining acceleration schedules reservation requests. first attempts trajectory requesting vehicle accelerates quickly possiblemaximum velocity soon enters intersection. cannot grant reservationbased trajectory, tries one requesting vehicle maintains constantvelocity throughout intersection. neither work, rejects request. Furthermore,request indicates vehicle arrive sufficiently slow velocityin case10 m/sit grant fixed-velocity reservation. grant arbitrarily slowreservations, vehicle could use excessively large amount space-time intersection, causing vehicles undue delay. enforcing minimum velocity fixed-velocityreservations, policy ensures vehicle spend long intersection.complex solutions exist, solution good several reasons. First, compu604fiA Multiagent Approach Autonomous Intersection ManagementFigure 4: Several vehicles waiting intersection. reservation distanced, front (white) vehicle incapable obtaining reservationvehicles behind (shaded) hold conflicting reservations. white vehiclesrequest rejected, reservation distance decreased d0 . shadedvehicles cancel reservations, white vehicle obtain reservation uncontested.tationally tractable: policy runs two internal simulations per request. Second,allows vehicles stopped moving slowly intersection clear intersection timely manner get reservation. Third, eliminates deadlockscenario presented Section 4.2 allowing vehicles recover deceleratingcannot obtain reservation; even vehicle comes full stop intersectionaccelerate back reasonable velocity crosses intersection.3.4.3 Reservation DistanceAllowing accelerations intersection helps eliminate deadlocks, problemsarose prototype implementation significantly impaired performancesystem. Frequently, lane traffic would become congested many vehiclesspawned lane. Even simulator stopped spawning vehicles lane,lane would remain congested. problem FCFS, first described, nothingcontrol vehicles lane alloted reservations. best, frontmostvehicle get reservation make intersection unhindered. However,often case. Sometimes vehicle front cannot obtain reservation (duecongestion), must decelerate. shown Figure 4, driver agents vehiclesback may expect accelerate soon successfully reserve space-time intersectionfrontmost vehicle needs. vehicles eventually make (a vehiclemight get reservation immediately vehicles behind cancel), process repeatmany times frontmost vehicle gets reservation. worst scenarios, singlevehicle continue quite time obtain reservations prevent front carcrossing intersection.could maintain invariant vehicles get reservations unless carsfront (in lane) reservations, scenario could avoided entirely.605fiDresner & Stonesimple way enforce would insist vehicle get reservation unlessvehicle front already one. Unfortunately, way strictly enforce this:vehicles communicate positions (and even did, could untruthful).However, vehicles communicate time plan arriveintersection, well velocity get (quantitiesvehicles incentive misrepresent), possible approximate vehiclesdistance intersection, given reservation request vehicle. approximatedistance, call reservation distance, va (ta t), va proposedarrival velocity vehicle (at intersection), ta proposed arrival timevehicle, current time. approximation assumes vehicle maintainingconstant velocity.policy uses approximation follows. lane i, policy variabledi , initialized . reservation request r lane i, policy computesreservation distance, d(r). d(r) > di , r rejected. If, hand, d(r) di , rprocessed normal. r rejected processed normal, min(di , d(r)).Otherwise, di .guarantee vehicles get reservations vehicles frontalready reservations, makes much likely. Two properties makeapproximation particularly well-suited problem. First, vehicle stoppedintersection, reservation distance approximated zero. meansvehicle behind granted reservation isno smaller reservation distancepossible. Furthermore, reservation distance product arrivalvelocity time vehicle arrives, vehicles approach intersectionslow down, reservation distance gets smaller accurate. Thus, vehiclessusceptible problem described Figure 4 likely protectedit. second property estimate uses arrival velocity vehicle,overestimates distance vehicles expecting accelerate significantly reachingintersection. expectation causes driver agents reserve space-timeneeded vehicles front them. Note also heuristic works withinsingle laneeach lane keeps track reservation distance.example Figure 4, white vehicles rejected reservation request would shortenmaximum allowed reservation distance lane. This, turn, would cause futurerequests shaded vehicles immediately rejected, giving white vehicle exclusiveaccess (within lane) reservation mechanism. white vehicle securedreservation, maximum allowed reservation distance would reset maximum,vehicles would equal priority.3.4.4 Timeoutsdriver agents reservation request rejected, driver agent may immediatelymake new request. Unless new request significantly different, likelyrejected well. exception request made immediately firstrejected request, driver agents estimate arrival intersection likelychange much instant consecutive requests. Eventually, vehicledecelerated enough driver agents conflicting reservations canceled, ve606fiA Multiagent Approach Autonomous Intersection Managementhicle obtain reservation make intersection. standpointintersection manager, requests successful one wasted effort.policy runs two internal simulations per request, simulations maycomputationally expensive, especially FCFS policy high granularity. Furthermore, rejected vehicle makes request every possible instant, workadd quickly.order keep required amount computation discourage driver agentsoverloading intersection manager requests, policy employs systemtimeouts. driver agents request rejected, subsequent requests considered period time (determined reservation parameters) elapsed.rejecting request, policy includes rejection message timeconsider requests driver agent. implementation, timeequal + min( 12 , (ta2t) ), current time ta time arrivalrequest message. process serves two purposes. First, dramatically reducesamount computation policy needs do, intersection manager receivesfewer requests. Vehicles may obtain reservations earliest moment possible,computational savings worth it. Second, gives preference vehiclesenter intersection sooner. vehicle stopped intersection, sendrequests quickly wishes, giving best chance getting reservation approved.vehicle farther away, however, may wait full half-second attemptingmake another reservation. vehicle approaches intersection, unableprocure reservation, frequency opportunities send reservation requests increases.practice, timeouts significantly improve performance system, allowinghandle much higher traffic loads avoiding backups.3.4.5 Buffers: Static vs. Timesystem involving physical robots, noise sensor readings errors actuatorsinevitably manifest themselves. Even simulation, artifacts resulting discretization time enough weaken reservation tiles guarantees exclusivity.intersection, vehicles move high speeds different directions, potentialsources calamity cannot ignored. example, happens driver agentrealizes make reservation exactly time, close enough intersectionpossible stop entering intersection? sort safety bufferrequired. Two types buffers natural: static buffers time buffers.Static buffersbuffers whose size constantcertainly suffice safety purposes.intersection manager assumes vehicle ten times large dimension, certainlyvehicle even get close another vehicle. However, defeats pointintersection manager, leverage increased precision autonomous vehicles.Furthermore, static buffer take account direction motion vehicle.Two vehicles whose paths would never intersect may begin interfere one anothersreservation process large static buffer used, Figure 5(a).Time buffers, hand, take account motion vehicles.intersection manager instead assumes vehicle might early late, actualarea restricted buffer shrink grow vehicles velocity,607fiDresner & Stone(a) Static buffer(b) Time buffer, low velocity(c) Time buffer, high velocity(d) Hybrid bufferFigure 5: Various styles buffers designed cope sensor noise actuator errors.hatched areas show buffers would cause reservation conflicts: onepair conflicting vehicles would granted reservation.direction movement. Figures 5(b) 5(c) show buffer scales speedvehicle. Thus, two vehicles traveling along parallel lines, time buffersvehicles interfere unless vehicles could potentially collide (theylane lanes close together vehicles width). Alone, time bufferssufficient guarantee safety small error lateral positioning (orthogonaldirection motion) may still cause collision. Figure 5(d) shows best solution:hybrid buffer. hybrid buffer time buffer scales velocity, wellsmall static buffer protects lateral positioning errors serves minimumbuffer slow-moving vehicles.3.4.6 Edge Tilesdriving open road, vehicles must maintain reasonable following interval(usually measured amount time) one another. vehicle decelerates suddenly, puts vehicle behind dangerous situationif rear vehicle doesnt reactquickly enough, may collide front vehicle. intersection, following intervalspractical, vehicles traveling many different directions. Vehiclesintersection cannot react normally sensor readings, intersectionmanager may orchestrate close calls would look like potential collisionvehicle operating open road mode. Instead, vehicles trust constraints givenintersection manager. pose problem intersection,vehicle exits intersection, may enounter vehicle also leftintersection, much slower velocity. shown Figures 6(a) 6(b), maylead unavoidable collision, later vehicle unable stop quickly enough.Even autonomous vehicles, react almost instantaneously, amountfollowing interval required vehicles leaving intersection.608fiA Multiagent Approach Autonomous Intersection ManagementBBB(a) turns right frontB.(b) B cannot stop time.(c) B must slow preemptively.Figure 6: Edge tiles prevent collisions vehicles leave intersection. 6(a), vehicleturns front vehicle B, traveling slowly making right turn.6(b), vehicle B gets intersection without incident, findsleaves intersection, cannot stop colliding vehicle A.extra buffers edge tiles, shown 6(c), prevent vehicle B obtainingreservation would cause exit intersection close vehicle A.shaded tiles edge tiles, darkly shaded tiles specific tileswould prevent collision 6(a) 6(b).first-cut solution problem simply increase time buffers reservation tiles desired following interval. Thus, vehicles require following intervalone second exiting intersection, vehicle able reserve tilewithin one second another vehicle. ensures vehicles leaving intersectionlane exit within one second other, gapleast one second vehicles. Unfortunately, wreaks havoc FCFSsability conduct vehicles efficiently intersection. close callssystem gets efficiency advantages longer possible.Instead, divide reservation tiles two groups. Internal tiles tilessurrounded sides reservation tiles. Edge tiles, shown shadedFigure 6(c), tiles abut intersection. sufficiently high granularities, edgetiles relatively small fraction total number tiles. tilesincrease time buffer desired following interval. (at sufficientlyhigh granularities) vehicles leaving lane require edge tiles,modification enforces desired following intervals without otherwise preventingintersection exploiting ability interleave vehicles closely.3.5 Policieslayer abstraction provided protocol, intersection managerwork emulation mode, imitating modern-day control mechanisms, stopsign traffic light. briefly explain implementation two intersection controlpolicies designed mimic mechanisms.609fiDresner & StoneAlgorithm 2 FCFSs request processing algorithm. FCFS persistent state variables:tiles, map tiles times vehicles, reservations, map vehicles setstiles, timeouts, map vehicles times.1: tc current time2: timeouts[vehicle id] < tc3:reject request4: ta proposed arrival time5: timeouts[vehicle id] tc + min(0.5, (ta tc )/2)6: acceleration {true, false}7:tile times {}8:ta9:V temporary vehicle initialized according reservation parameters10:V intersection11:tiles occupied V V static buffer time12:tile times tile times {(t, S)}13:14:edge tile15:buf edge tile buffer16:else17:buf internal tile buffer18:= buf buf19:tiles[s, + i] reserved another vehicle20:acceleration21:goto line 2922:else23:reject request24:+ time step25:move V according physical model26:acceleration27:increase V velocity V maximum acceleration28:break29:30:31:32:33:34:35:36:37:38:39:request changeold tile times reservations[vehicle id](ti , Si ) old tile timesSiclear reserved status tiles[s, ti ](ti , Si ) tile timesSitiles[s, ti ] vehicle idreservations[vehicle id] tile timesaccept request, return reservation constraints (incl. accelerations)610fiA Multiagent Approach Autonomous Intersection ManagementStop-Sign Stop signs traditionally used intersections light traffic.much cost-effective reliable, cannot provide throughputefficiency traffic light. Thus, would never reason systememulate stop sign, however include description completeness.Stop-Sign exactly like FCFS, except accepts reservations vehiclesstopped intersection. reservation requests rejectedmessage indicating vehicle must stop intersection. intersectiondetermines whether vehicle stopped intersection examining differencecurrent time arrival time request message.Traffic-Light Traffic-Light policy receives reservation request message,calculates next time proposed arrival time light sendingvehicles lane green. responds confirmation message reflectsinformation. confirmation messages maximum tolerable errors associated them, intersection manager uses errors encode beginningend green light period.3.6 Compatibility Human Driversintersection control mechanism autonomous vehicles somedayuseful, always people enjoy driving. Additionally, fairlylong transitional period current situation (all human drivers) onehuman drivers rarity. Even switching system comprised solely autonomousvehicles possible, pedestrians cyclists must also able traverse intersectionscontrolled safe manner. reason, necessary create intersection controlpolicies aware able accommodate humans, whether bicycle,walking corner store, driving classic car entertainment purposes.section explain extended FCFS policy reservation frameworkincorporate human drivers. order accommodate human drivers, control policymust able direct human autonomous vehicles, coordinating them,despite much less control information regarding humandrivers be. main concept behind extension assumptionhuman-driven vehicle anywhere one could be. may less efficientapproach attempts precisely model human behavior, guaranteedsafe, one desiderata unwilling compromise. Adding pedestrianscyclists follows naturally, give brief descriptions would differextensions human drivers.Compatibility human drivers offers ability handle occasionalhuman driver levels human drivers everyday traffic reaches steady state.also help facilitate transition current standardall human-driven vehiclessteady state, human drivers scarce. Section 2.1, emphasizedneed incremental deployability. show experimentally, human compatibilityadds significantly incremental deployability reservation system. alsoshow specifics implementation offer benefits: incentivescommunities private individuals adopt autonomous vehicle technology.611fiDresner & Stone3.6.1 Using Existing Infrastructurereliable method communicating human drivers prerequisite includingsystem. simplest best solution use something human driversalready know understand traffic lights. Traffic light infrastructure already presentmany intersections engineering manufacturing traffic light systems welldeveloped. pedestrians cyclists, standard push-button crossing signalsused give enough time person traverse intersection. also servealert intersection presence.3.6.2 Light Modelsreal traffic lights used communicate human drivers, must controlledunderstood intersection manager. Thus, add new componentintersection control policy, called light model. light model controls physical lightswell providing information policy make decisions.complicated scenarios, light model modified control policy, example,order adapt changing traffic conditions. lights semanticsmodern-day lights: red (do enter), yellow (if possible, enter; light soonred), green (enter). control policy requires light model human usersknow do. instance, light model FCFS keeps lights redtimes, indicating humans never safe enter. Traffic-Light policys lightmodel, hand, corresponds exactly light system policy emulating.Here, describe light models used experiments.All-Lanes model, similar current traffic light systems,direction succession gets green lights lanes. Thus, northbound traffic (turninggoing straight) green lights eastbound, westbound, southbound trafficred lights. green lights cycle directions. similarcurrent traffic lights, light model particularly well-suited controllingdistributions vehicles significant contingents human drivers. demonstratefact experimentally Section 4.5. Figure 7 shows graphical depiction light model.Figure 7: All-Lanes light model. direction gets green lights cycle: north,east, south, west. phase, available paths autonomousvehicles red lights right turns.Single-Lane Single-Lane light model, green light rotates lanesone time instead direction. example, left turn lane northbound traffic612fiA Multiagent Approach Autonomous Intersection Managementwould green light, lanes would red light. Next, straightlane northbound traffic would green light, right turn. Next, greenlight would go lane eastbound traffic, forth. graphical descriptionmodels cycle seen Figure 8. light model work wellvehicles human-driven, show, useful intersectionscontrol mostly autonomous vehicles need also handle occasional humandriver.Figure 8: Single-Lane light model. individual lane gets green light (left turn,straight, right turn), process repeated direction. Notesmaller part intersection used human vehicles giventime. rest intersection available autonomous vehicles.3.6.3 FCFS-Light Policyorder obtain benefits FCFS policy still accommodating humandrivers, policy needs two things:1. light green, ensure safe vehicle (autonomous human-driven)drive intersection lane light regulates.2. Grant reservations driver agents whenever possible. Autonomous vehicles thusmove red lights (whereas humans cannot), provided reservationsimilar right red, extended much safe situations.policy FCFS-Light, these, described follows:FCFS, intersection divided grid n n tiles.Upon receiving request message, policy uses parameters messageestablish vehicle arrive intersection.light controlling lane vehicle arrive intersectiongreen time, reservation confirmed.light controlling lane yellow, reservation rejected.light controlling lane red, journey vehicle simulatedFCFS (Section 3.4).throughout simulation, required tile reserved another vehicle uselane green yellow light, policy reserves tiles confirmsreservation. Otherwise, request rejected.613fiDresner & StoneREQUESTPreprocessYes,RestrictionsRedLight ModelNo, ReasonFCFSREJECTPostprocessYellowDriverAgentGreenCONFIRMIntersection ManagerFigure 9: FCFS-Light combination FCFS light model. requestreceived, FCFS-Light first checks see color light be.green, grants request. yellow, rejects. red, defersFCFS.Off-Limits Tiles Unfortunately, simply deferring FCFS guarantee safetyvehicle. vehicle granted reservation conflicts vehicle followingphysical lights, collision could easily ensue. determine tiles uselight system given time, associate set off-limits tiles light.example, light northbound left turn lane green (or yellow), tilescould used vehicle turning left lane considered reservedpurposes FCFS. length yellow light adjusted vehicles enteringintersection enough time clear intersection tiles longerlimits.FCFS-Light Subsumes FCFS Using traffic lightlike light model (for example AllLanes), FCFS-Light behave exactly like Traffic-Light drivers human.light model keeps lights constantly red, FCFS-Light behaves exactly likeFCFS. case, human drivers present fail spectacularly, leavinghumans stuck intersection indefinitely. However, absence human drivers,perform exceptionally well. FCFS special case FCFS-Light. thusalter FCFS-Lights behavior vary strictly superior Traffic-Light exactlyFCFS.3.7 Emergency Vehiclescurrent traffic laws special procedures involving emergency vehiclesambulances, fire trucks, police cars. Vehicles required pull sideroad come complete stop emergency vehicle passed.emergency vehicle may traveling quickly, posing dangervehicles, emergency vehicle must arrive destination quicklypossiblelives may stake. Hopefully, system implemented,automobile accidentsa major reason emergency vehicles dispatchedwilleradicated. Nonetheless, emergency vehicles still required time time fires,heart attacks, emergencies still exist. previously proposedmethods giving priority emergency vehicles (Dresner & Stone, 2006),present new, simpler method, fully implemented tested.614fiA Multiagent Approach Autonomous Intersection Management3.7.1 Augmenting Protocolorder accommodate emergency vehicles, intersection manager must first abledetect presence. easiest way accomplish add new fieldrequest messages. implementation, field simply flag indicatesintersection manager requesting vehicle emergency vehicle emergencysituation (lights flashing siren blaring). practice, however, safeguards would needincorporated prevent normal vehicles abusing feature order obtainpreferential treatment. could accomplished using sort secret key insteadsimply boolean value, even sort public/private key challenge/response mechanism. details implementation, however, beyond scope projectalready well-studied area cryptography computer security.3.7.2 FCFS-Emerg Policyintersection control policy detect emergency vehicles, process reservation requests giving priority emergency vehicles. first-cut solution simplydeny reservations vehicles emergency vehicles. However, solutionsatisfactory, traffic comes stop due rejected reservationrequests, emergency vehicles may get stuck resulting congestion. Instead,FCFS-Emerg policy keeps track lanes currently contain approaching emergencyvehicles. long least one emergency vehicle approaching intersection,policy grants reservations vehicles lanes. ensures vehicles frontemergency vehicles also receive priority. Due increase priority, lanesemergency vehicles tend empty rapidly, allowing emergency vehicles proceedrelatively unhindered.3.8 Summarysection, explained created reservation-based intersection controlmechanism simulation. described construction simulator itself, wellcommunication protocol, intersection manager, driver agent, severalintersection control policies. first policy, FCFS fully autonomous vehicles.FCFS-Light extends FCFS allow human interoperability using existing traffic lightinfrastructure. last policy, FCFS-Emerg, extends FCFS give priority emergencyvehicles without significant increasing delays vehicles.4. Experimental Resultssection, fully test features introduced Section 3 demonstratereservation system reduce delay two orders magnitude. experiments evaluate performance reservation system using different intersection control policies,amounts traffic, granularities, levels human drivers, presence emergencyvehicles. first compare system using FCFS traffic lights varying cycle periodsusing prototype simulator. show results full version, including stopsign control policy implemented protocol, comparing resultstraffic light experiments. Next, experiment allowing vehicles turn615fiDresner & Stonelanesomething would extremely dangerous without reservation-based mechanism. Finally, evaluate two extensions FCFS: FCFS-Light FCFS-Emerg.Videos simulator action, including many scenarios section, wellsupplementary materials found http://www.cs.utexas.edu/~kdresner/aim/.4.1 Low-Granularity FCFS vs. Traffic Lightsimplest implementation FCFS granularity 1the entire intersection single reservation tile. one vehicle may intersection time,vehicle traveling sufficiently fast, total amount time occupyintersection small. increase granularity 2, intersection longer entirely exclusive. example, non-turning vehicles traveling north longer competereservation tiles non-turning vehicles traveling south (similarly, eastboundwestbound non-turning vehicles longer compete). present initial resultscomparing two instances reservation mechanism several incarnationstraffic light.4.1.1 Experimental Setupexperiments carried using prototype version simulator, fullydescribed earlier publication (Dresner & Stone, 2004). version simulator,vehicles allowed turn accelerate intersection. restrictionsdetract core challenge problem, results relevant evenrestrictions relaxed. simulation contains one lane traveling direction,speed limits 25 meters per second. Traffic spawning probability varies0.0001 0.02 increments 0.0001, configuration runs 500,000 stepssimulator, corresponds approximately 2.5 hours simulated time.4.1.2 ResultsFigure 10(a) shows delay times traffic light systems varying periods, rangingextremely short (10 seconds) fairly long (50 seconds). expected real-life experience, short-period traffic lights control light traffic well, traffic lights longerperiods work better heavy-traffic scenarios. traffic sparse, short period allowsvehicles wait shorter time getting green light. many cities, traffic lightperiods shortened early hours morning take advantage fact.scenarios densely packed vehicles, per-vehicle costs slowing stopaccelerating back full speed, well intervals needed clear intersection(the time yellow light, lights red), tend dominate.makes longer-period lights better situations. Figure 10(a),certain traffic level, traffic light systems reaches appears maximumdelay level. artifact simulatorwhen traffic level gets high enough,vehicles back far simulator cannot keep track (it cannot spawnnew vehicles, lack place put them). point, vehicles arrivingintersection faster traffic lights safely coordinate passage. Thus,point delay spikes upwards indicates maximum throughput trafficconfiguration.616fi10090807060504030201000.7Period 100.6Period 30Average Delay (s)Average Delay (s)Multiagent Approach Autonomous Intersection ManagementPeriod 50Granularity 10.50.40.3Granularity 10.2Granularity 20.100.20.40.60.810.2Traffic Level (vehicles/s)0.40.60.81Traffic Level (vehicles/s)(a) Reservation vs. Traffic Light(b) Increasing GranularityFigure 10: 10(a) shows average delays traffic light systems period 10, 30, 50 seconds plotted varying traffic levels along 1-tiled reservation-basedsystem. 10(b) shows average delays granularity 1 2 FCFS policiesvarying traffic levels. Spawning probability varied increments 0.0001,configuration run 1,000,000 steps simulation (approximately5.5 hours simulated time). direction 1 lane.Also Figure 10(a) delays granularity-1 2 FCFS policies.car spawning probability 0.013, granularity-1 policys delay visuallyindistinguishable x-axis, true granularity-2 reservation systemwhole graph. Figure 10(b) shows bottom 0.7% graph, enlarged showresults detail. vehicle spawning rate 0.02, traffic lightsystems already beyond maximum capacity, granularity-2 system allowingvehicles without even adding tenth second average vehicles traveltime.4.2 Choosing Granularitynote Figure 10(a) spike delay granularity-1 FCFS policy. system looks though behaving chaoticallyin Figure 10(b), delay slowly steadilyincreases traffic level, spiking graph probability spawningvehicle time step reaches 0.013.granularity-1 system, vehicles traveling parallel one another competetiles. also happens vehicles lanes closest middleroad whenever granularity small, odd number, Figure 11(b). Recallprototype simulator, acceleration intersection forbidden. Thus, vehicleslows cannot obtain reservation, finally get reservationmoving slowly entirety reservation occupy reservationtiles longer period time. next car approach intersection thereforelikely slow well. process feeds vehicles slowmore. small average amounts traffic, delays increase, systemrecovers probabilistically generated periods light traffic. However, heavy617fiDresner & Stonetraffic, intersection eventually reach deadlocked state. traffic generatedstochastically, could happen early late experiment. happens early,large effect average delay, whereas happens late, effect smaller.Deadlocking difficult measure quantitatively, progresses, driver agentsmake reservations long periods timeso long, fact, overflowmemory computer running simulator. effect seen rough lineFigure 10(a). explore effects granularity, ran several experiments,varying granularity well number lanes.(a) Granularity 8(b) Granularity 9Figure 11: Increasing granularity always improve performance. 11(a),granularity 8 suffices. 11(b), increasing granularity 9 actually hurtsperformancevehicles traveling parallel (but opposite directions) competing middle row tiles.4.2.1 Experimental Setupexperiments also used prototype simulator described Section 4.1.1.data point represents 500,000 steps simulation (approximately 2.5 hours simulatedtime). traffic level fixed 0.2 vehicles per second.4.2.2 Resultsshown Figure 12, 2 lanes direction, 2 2 grid performs better3 3 grid. Increasing 4 4 grid better 2 2, increasing 5 5worse. increase granularity correspond decrease delay. However,small granularities, incrementing granularity small even number smallodd number actually increases delay. case maximum delay, even granularity-2618fiA Multiagent Approach Autonomous Intersection Management0.1430.122.50.1Maximum Delay (s)Average Delay (s)system performs better granularity-5 system; ill effects odd granularitiesshown Figure 11 tend slow unfortunate vehicles.0.080.060.0421.510.50.020023452Granularity345Granularity(a) Average delay(b) Maximum delayFigure 12: Simulation statistics FCFS policies varying granularity. 2lanes direction traffic level 0.2 vehicles per second.experiment run 500,000 simulation steps. Note increasing granularity always improve performance.experiment suggests FCFS always run granularity high enoughvehicles never cross paths never compete reservation tiles.Figure 13 shows, lanes require higher granularity (though even low granularity,system out-performs traffic light). However, computational complexitysystem increases proportional square granularity, granularityincreased indiscriminately.4.3 Full Power FCFSearlier experiments used prototype simulator, experiments use full powerFCFSturning, acceleration, modifications Section 3.4. vehiclesturn, thus always travel within line reservation tiles, increasing granularitybeyond twice number lanes improve performance even more. additionFCFS, evaluate stop sign policy presented Section 3.2.Technically, optimal delay individual vehicle delay all. However,although vehicle could experience delay low 0 seconds, turning vehicles may needslow avoid losing control. order create worthwhile benchmarkcompare reservation system, empirically measure optimal average delayintersection manager. this, use special control policy accepts requests.also deactivate vehicles ability detect vehicles, eliminating interactionsthem. results presented optimal control policy,optimal terms delay, provides safety guarantees.Small intersections slow-moving traffic tend amenable control trafficlights. light traffic usually regulate fairly effectively. example, considerintersection stop signall vehicles must come stop, afterwards may proceed619fiDresner & Stone6 Lanes3 Lanes2 Lanes1 Lane0.14Average Delay (s)0.120.10.080.060.040.020126Granularity1248Figure 13: Average delays FCFS policy independently varying numbers lanesgranularity. Increasing granularity beyond twice number lanesresults marginal improvements. simulations run least500,000 steps. 6 lanes 1 tile deadlocks overflows system memory500,000 steps complete.intersection clear. situations, stop sign often much efficienttraffic light, vehicles never stuck waiting light changecross-traffic. protocol enables us define control policy, compareexperimentally policies. Note policy much efficientactual stop sign, vehicle stopped intersection, driver agentintersection determine car may safely proceed much preciselymuch less conservatively human driver.4.3.1 Experimental Setupsimulator simulates 3 lanes 4 cardinal directions. speed limitlanes 25 meters per second. Every configuration shown run least 100,000 stepssimulator, corresponds approximately half hour simulated time. Vehiclesspawned turn probability 0.1, turning vehicles turn left rightequal probability. Vehicles turning right spawned right lane, whereas vehiclesturning left spawned left lane. Vehicles turning distributedprobabilistically amongst lanes traffic lane equal possible.FCFS stop sign (implemented extension FCFSsee Section 3.5)granularity 24.4.3.2 Resultsresults experiments shown Figure 14. expected, average delayoptimal system positive nonzero, small.620fiA Multiagent Approach Autonomous Intersection ManagementFCFS performs well, nearly matching performance optimal policy.higher levels traffic, average delay vehicle gets high 0.35 seconds,never 1 second optimal. none tested conditions FCFSeven approach delay traffic light system previous experiment, shownFigure 10(a).stop sign perform well FCFS, low amounts traffic,still performs fairly well, average delay 3 seconds greater optimal.However, traffic level increases, performance degrades. difficult imaginescenario implementation stop sign would actually usedit requirestechnology reservation system, advantagesFCFS.54.5Stop Sign43.5Delay (s)Traffic Light Minimum32.521.51Optimal0.5FCFS000.511.522.5Traffic Level (vehicles/s)Figure 14: Delays varying amounts traffic FCFS, stop sign, optimalsystem.4.4 Allowing Turns Lanetraditional traffic systems, especially traffic lights, vehicles wishing turnonto cross street must specially designated turning lanes. helps preventcars want turn holding non-turning traffic. However, system likereservation system, restriction longer necessary. nothing inherentreservation system demands vehicles turn specific lane. Investigatingeffects allowing turning lane produced surprising results. seenFigure 15, relaxing restriction actually hurts FCFSs performance slightly. one621fiDresner & Stonemight think allows vehicles flexibility, average increases resourcesused one turning vehicle. making left turns left lane right turnsright lane, vehicles travel shorter distance reserve reservation tilesless heavily used. However, experiments may misleading. Vehicles changinglanes get designated turn lane could potentially delay vehicles behindprocess. currently model lane changing intersection,able experimentally verify conjecture.1Fixed LaneLaneDelay (s)0.80.60.40.2000.511.5Traffic Level (vehicles/s)22.5Figure 15: Comparison FCFS policy traditional turns one allowing turninglane. Allowing turns lane decreases performance slightly,producing longer delays.4.5 Effects Human InteroperabilitySection 4.3, showed vehicles autonomous, intersection-associateddelays reduced dramatically. following experiments suggest stronger result:using two light models presented Section 3.6.2, delays reduced stageadoption. Furthermore, additional incentives exist stage drivers switchautonomous vehicles.4.5.1 Experimental Setupexperiments, simulator models 3 lanes 4 cardinal directions.speed limit lanes 25 meters per second. intersection control policyreservation tiles, granularity 24. simulator spawns vehicles turning leftleft lane, vehicles turning right right lane, vehicles traveling straight622fiA Multiagent Approach Autonomous Intersection Managementcenter lane1 . Unless otherwise specified, data point represents 180000 time steps,one hour simulated time. simulated human-driven vehicles use two-secondfollowing distance, use lane-following algorithm autonomous drivers.also employ point-of-no-return mechanism reacting lightsif vehiclestop yellow red light, does, otherwise proceeds.4.5.2 Resultspresent experimental results human-compatible policies two parts.first focuses policies facilitate smooth transition all-autonomousmostly-autonomous vehicle system. second focuses incentives throughoutprocess, global individual, continue deployment system. Combined,results suggest incremental deployment (one desiderata) technicallypossible desirable.Transition Full Deployment purpose hybrid intersection control policyconfer benefits autonomy passengers driver-agent controlled vehiclesstill allowing human users participate system. Figure 16 shows smoothmonotonically improving transition modern-day traffic lights (representedTraffic-Light policy) completely mostly autonomous vehicle mechanism (FCFSLight Single-Lane light model). early stages (100%-10% human), AllLanes light model used. Later (less 10% human), Single-Lane light modelintroduced. change (both driver populations light models), delaysdecreased. Notice rather drastic drop delay FCFS-Light All-Laneslight model FCFS-Light Single-Lane light model. Although noneresults quite close minimum pure FCFS, Single-Lane light modelallows greater use intersection FCFS portion FCFS-Light policy,translates higher efficiency lower delay.systems significant proportion human drivers, All-Lanes light modelworks wellhuman drivers experience would Traffic-Lightpolicy, autonomous driver agents extra opportunities makeintersection. small amount benefit passed human drivers, mayfind closer front lane waiting red light turn green.explore much average vehicle would benefit, ran simulator FCFSLight policy, All-Lanes light model, 100%, 50%, 10% rate human drivers.means vehicle spawned, receives human driver (instead driveragent) probability 1, .5, .1 respectively. seen Figure 17, proportionhuman drivers decreases, delay experienced average driver also decreases.decreases large brought Single-Lane light model,least possible significant numbers human drivers.Incentives Individuals Even without sort autonomous intersection controlmechanism, incentives humans switch autonomous vehicles.1. constraint likely relax future. included work give Single-Lanelight model flexibility fair comparison FCFS policy, performs even betterabsence.623fiDresner & Stone60505% HumanDelay (s)4030100% Human201% Human10% Human10Fully Autonomous000.511.5Traffic Level (vehicles/s)22.5Figure 16: Average delays vehicles function traffic level FCFS-Lighttwo different light models: All-Lanes light model, well-suitedhigh percentages human-driven vehicles, Single-Lane light model,works well relatively human-driven vehicles. adoptionautonomous vehicles increases, average delays decrease.20Delay (s)15105TRAFFIC-LIGHTFCFS-LIGHT 50% HumanFCFS-LIGHT 10% Human000.250.5Traffic Level (vehicles/s)0.751Figure 17: Average delays vehicles function traffic level FCFS-LightAll-Lanes light model. Shown results 100%, 50%,10% human-driven vehicles. 100% case equivalent Traffic-Lightpolicy. Note average delay decreases percentage human-drivenvehicles decreases.driving, well myriad safety benefits strong incentives promoteautonomous vehicles marketplace. experimental results suggest additional incentives. Using reservation system, autonomous vehicles experience lower average delayshuman-driven vehicles difference increases autonomous vehicles becomeprevalent.Figure 18 shows average delays human drivers compared autonomous driveragents FCFS-Light policy using All-Lanes light model. experiment,half drivers human. Humans experience slightly longer delays autonomousvehicles, worse Traffic-Light policy. Thus, putting624fiA Multiagent Approach Autonomous Intersection Managementautonomous vehicles road, drivers experience equal smaller delays comparedcurrent situation. expected autonomous driver everythinghuman driver more.35HumansAutonomous30Delay (s)252015105000.250.5Traffic Level (vehicles/s)0.751Figure 18: Average delays human-driven vehicles vehicles function trafficlevel FCFS-Light All-Lanes light model. experiment,50% vehicles human driven. Autonomous vehicles experience slightlylower delays across board, human drivers experience delays worseTraffic-Light policy.reservation system widespread use autonomous vehicles makevast majority road, door opened even efficient lightmodel FCFS-Light policy. low concentration human drivers,Single-Lane light model drastically reduce delays, even levels overall trafficTraffic-Light policy handle. Using light model, autonomous driverspass red lights even frequently fewer tiles off-limits giventime. Figure 19 compare delays experienced autonomous drivershuman drivers 5% drivers human thus Single-Lane light modelused. improvements using All-Lanes light model benefit driversextent, Single-Lane light models sharp decrease average delays (Figure 16)comes high price human drivers.shown Figure 19, human drivers experience much higher delays average.lower traffic levels, delays even higher associated TrafficLight policy. Figure 16 shows despite this, high levels traffic, human driversbenefit relative Traffic-Light. Additionally, intersections using FCFS-Light stillable handle far traffic using Traffic-Light.SingleLane light model effectively gives humans high, fairly constantdelay. green light one lane comes around lanegreen light, human-driven vehicle may find sitting red light timelight changes. However, since light model would put operationhuman drivers fairly scarce, huge benefit 95% 99% vehiclesfar outweighs cost. light model detects reacts presence human625fiDresner & Stone60HumansAutonomous50Delay (s)40302010000.250.5Traffic Level (vehicles/s)0.751Figure 19: Average delays human-driven vehicles vehicles function trafficlevel FCFS-Light Single-Lane light model. Humans experienceworse delay Traffic-Light, average delay vehicles muchlower. experiment, 5% vehicles human-driven.drivers might able achieve even better overall performance, without causing humandrivers wait long.data suggest incentive early adopters (personspurchasing vehicles capable interacting reservation system) citiestowns. properly equipped vehicles get going faster (notmention safely). Cities towns equip intersections utilizereservation paradigm experience fewer traffic jams efficient use roadways(along fewer collisions less wasted gasoline). penaltyhuman drivers (which would presumably majority point), wouldreason party involved oppose introduction system. Later,drivers made transition autonomous vehicles, Single-Lane lightmodel introduced, incentive move new technology increasedbothcities individuals. time, autonomous vehicle owners far outnumber humandrivers, still benefit traffic worst.4.6 Emergency Vehicle Experimentsalready shown FCFS significantly reduce average delaysvehicles, FCFS-Emerg helps reduce delays emergency vehicles even further.4.6.1 Experimental Setupdemonstrate improvement, ran simulator varying amounts traffic,keeping proportion emergency vehicles fixed 0.1% (that is, spawned vehiclemade emergency vehicle probability 0.001). small numberemergency vehicles created realistically low proportions, ran configuration(data point) 100 hours simulated timemuch longer experiments.626fiA Multiagent Approach Autonomous Intersection Management4.6.2 Resultsshown Figure 20, emergency vehicles average experience lower delaysnormal vehicles. amount emergency vehicles outperform normalvehicles increases traffic increases, suggesting designed, FCFS-Emerg helpstraffic contending space-time intersection.10Delay (s)8642VehiclesEmergency Vehicles00123Traffic Level (vehicles/s)45Figure 20: Average delays vehicles emergency vehicles function trafficlevel FCFS-Emerg policy. One thousand vehicles (on average)emergency vehicle. Delays emergency vehicles lower datapoints.5. Performance Failure ModesFully autonomous vehicles promise enormous gains safety, efficiency, economytransportation. However, gains realized, plethora safety reliability concerns must addressed. previous sections, assumedvehicles perform without gross malfunctions. section, relax assumptiondemonstrate reservation-based mechanism reacts scenarios malfunctions occur. Additionally, intentionally disable elements system orderinvestigate necessity efficacy.5.1 Causes Accidentscollision purely autonomous traffic number causes, including softwareerrors driver agent, physical malfunction vehicle, even meteorologicalphenomena. modern-day traffic, factors largely ignored two reasons. First,exclusively human-populated system, generous margins error,sensitive small moderate aberrations. Second, none significantrespect driver error causes accidents (Wierwille, Hanowski, Hankey, Kieliszewski,Lee, Medina, Keisler, & Dingus, 2002). However, future infallible autonomousdriver agents, exactly issues prevalent causes automobilecollisions. safety allowances explained Sections 3.4.5 3.4.6 adjustablegivenmaximum allowable error vehicle positioning, buffers extended handle627fiDresner & Stoneerrorbut reasonable adjustment account gross mechanical malfunction likeblowout failed brakes. types issues infrequent, believe safetyintersection control mechanism acceptable even individual occurrencesslightly worse accidents today.5.2 Adding Safety NetOne easily imagine badly accident efficient system could withoutreactive safety measures place. Here, explain system dealsrare, dangerous events. show Section 5.3, disabling safety measuresleaves system prone spectacular failure modes, sometimes involving dozens vehicles.Intact, measures make events much manageable.5.2.1 AssumptionsSection 5.3, show reactive safety measures reduce average numbervehicles involved crash dozens one two. However, order employsafety measures fully, must make additional assumptions.Detecting Problem First, assume intersection manager able detectsomething gone wrong. certainly non-trivial assumption, withoutit, substantial mitigation possible. Simply put, intersection manager cannot reactsomething cannot detect. two basic ways intersection managercould detect vehicle encountered sort problem: vehicle informintersection manager, intersection manager detect vehicle directly.instance, event collision, device similar triggers airbagsend signal intersection manager. Devices like already exist aircraft emitdistress signals locator beacons event crash. intersection managermight notice less severe problem, vehicle supposed be,using cameras sensors intersection. However, method detection likelymuch slower react problem. advantages disadvantages,combination two would likely safest. specifics implementationbeyond scope analysis. important whenever vehicle violatesreservation way, intersection manager become aware soon possible.simulations deal collisions, assume colliding vehicle sendssignal intersection manager becomes aware situation immediately.described Appendix B, protocol includes Done message vehicles transmit complete reservations. One way reliably sense vehicledistress would notice missing Done message. approach two drawbacks.First, Done message optional, mainly incentive driveragent transmit it. Second, intersection manager may able notice missing message time incident occurred. intend investigatealternative future work.Informing Vehicles also assume exists way intersectionmanager broadcast fact something wrong vehicles. Since intersection manager already communicate vehicles, big assumption.628fiA Multiagent Approach Autonomous Intersection ManagementHowever, mode communication bit different employed restcommunication protocol (see Appendix B). normal operating conditions, individualmessages containing multiple pieces information transmitted agents.cannot verify receipt messages without response, semanticsprotocol ensure whenever message sent, sending agent makes conservative assumptionin case Request message, received;case Confirm message, was. event collision, however, intersectionmanager needs communicate one bit information many vehicles possible:something wrong. important vehicles receive message,transmitted repeatedly, vehicles, exclusion messages.would like assume vehicles receive message, show Section 5.3even significant number vehicles not, safety measures place still protectmany vehicles would otherwise wind crashing.5.2.2 Incident Mitigationvehicle deviates significantly planned course intersection resulting physical harm vehicle presumed occupants, refer situationincident. incident occurred, first priority ensure safetypersons vehicles nearby. expect incidents infrequentoccurrences, re-establishing normal operation intersection lower priorityoptimization process left future work.Intersection Manager Response soon intersection manager detectsnotified incident, immediately stops granting reservations. subsequent receivedrequests rejected without consideration. Due nature protocol, intersection manager cannot revoke reservations, driver agents would incentiveacknowledge receipt. However, intersection manager send messagevehicles incident occurred. message special Emergency-Stop message, intersection manager may send emergency situation,(as rest protocol) must assume received.Emergency-Stop message lets vehicles know event taken placeintersection that:reservations acceptedvehicles able come stop entering intersectionvehicles intersection longer assume near misses resultcollisionshuman-compatible policies, FCFS-Light, intersection manager alsoturns lights red. real-world implementation, conspicuous visual cue couldprovided, semantically important intersection informs humandrivers may enter.Vehicle Response Emergency-Stop message useful way, driveragents must react it. explain specific actions implementation driveragent takes receives message. Normally, approaching intersection,driver agent ignores vehicles sensed intersection. mightotherwise appear imminent collision open road almost certainly precisely629fiDresner & Stonecoordinated near-miss intersection. However, driver agent receivesEmergency-Stop message intersection manager, disables behavior.vehicle intersection, driver agent blindly drive another vehiclehelp it. vehicle intersection stop time, enter,even reservation.first inclination make driver agent immediately deceleratestop, quickly realized safest behavior. vehicles receivemessage come stop, vehicles would otherwise cleared intersection withoutcolliding may find stuck intersectionanother object vehiclesrun into. especially true vehicle caused incident edgeintersection unlikely hit. Trying stop vehiclesintersection makes situation worse.driver agent detect impending collision, take evasive actionsapply brakes. Since true multiagent system self-interested agents,cannot prevent driver agents so, even detrimental vehicles overall.Thus, driver agent brakes believes collision imminent.5.3 Experimentsorder evaluate effects reactive safety measures, performed several experiments various components intentionally disabled. various configurationsseparated three classes. oblivious intersection manager takes actionupon detecting incident. intersection manager utilizing passive safety measures stopsaccepting reservations, send Emergency-Stop messages nearby driveragents. Finally, active configuration intersection managerwhich correspondsfull version protocol specified Appendix Bhas safety featuresplace. addition considering three incarnations intersection manager,also study effects unreliable communication active case. Notevehicles receive Emergency-Stop message, active passive configurationsidentical.5.3.1 Experimental Setupgreat efficiency reservation-based system comes extreme sensitivityerror. buffering might protect minute discrepancies, cannot hope covergross mechanical malfunctions. determine much effect malfunctionwould have, created simulation individual vehicles could crashed, causingimmediately stop remain stopped. Whenever vehicle crashedcomes contact one is, becomes crashed well. modelspecifics individual impacts, allow us estimate malfunction mightlead collisions.order ensure included malfunctions different parts intersection,triggered incident choosing random (x, y) coordinate pair inside intersection, crashing first vehicle cross either x coordinate. akincreating two infinitesimally thin walls, one horizontal vertical, intersect(x, y). Figure 21 provides visual depiction process.630fiA Multiagent Approach Autonomous Intersection ManagementFigure 21: Triggering incident intersection simulator. dark vehicle turningleft crashed crossed randomly chosen x coordinate. different vehicle crossed x coordinate randomly chosen coordinateearlier, would crashed instead.initiating incident, ran simulator additional 60 seconds, observingsubsequent collisions recording occurred. Using information,constructed crash log, essentially histogram crashed vehicles. stepremaining simulation, crash log indicates many vehicles crashedstep. averaging many crash logs configuration, ableconstruct average crash log, gives picture typical incident wouldproduce.system compatible humans, included experiments humancompatible intersection control policy. demonstrated Section 4.5, significantnumber human drivers present, FCFS-Light cannot offer much performancebenefit traditional traffic light systems. such, limited experimentationscenarios 5% vehicles controlled simulated human drivers, usedSingle-Lane light model (see Section 3.6.2). 5% human drivers, FCFSLight policy still create lot precarious situations focusinvestigation.experiments, ran simulator scenarios 3, 4, 5, 6 lanesfour cardinal directions, although discuss results 3-6-lane cases (other results similar) sake brevity. earlier experiments,vehicles spawned equally likely directions, generated via Poisson processcontrolled probability vehicle generated step. Vehiclesgenerated set destination15% vehicles turn left, 15% turn right,remaining 70% go straight. before, leftmost lane always left turn lane,right lane always right turn lane. Turning vehicles always spawned correctlane, non-turning vehicles spawned turn lanes. scenarios involvingautonomous vehicles, set traffic level average 1.667 vehicles per second perlane direction. equates 5 total vehicles per second 3 lanes, 10 total631fiDresner & Stonevehicles per second 6 lanes. Scenarios human-driven vehicles one thirdtraffic fully autonomous scenariosthe intersection cannot nearly efficienthuman drivers present. chose amounts traffic toward high endspectrum manageable traffic respective variants intersection manager.wanted traffic flowing smoothly, also wanted intersection fullvehicles test situations likely lead destructive possible collisions.5.3.2 Bad It?suspected, average crash log oblivious intersection manager quite grisly.explained Section 5.2.2, driver agents must ignore sensors intersection, many close calls would appear impending collisions. Withoutway react situation going awry, vehicles careen intersection, pilingentire intersection filled crashed vehicles protrude incoming lanes.Figure 22 shows 6-lane casesfully autonomous 5% human driverstherate collisions abate 70 vehicles crashed. Even full 60 secondsincident begins, vehicles still colliding. 3-lane case, intersectionmuch smaller thus fills much rapidly; 50 seconds, number collidedvehicles levels off.100606 Lanes3 Lanes906 Lanes3 Lanes5080Cars CrashedCars Crashed706050404030203020101001020304050600Time (s)102030405060Time (s)(a) autonomous(b) 5% humansFigure 22: Average crash logs (with 95% confidence interval) 3- 6-lane obliviousintersections. 22(a), intersection manages autonomous vehicles,22(b) includes 5% human drivers.scenarios human drivers, shown Figure 22(b), number vehicles involved average incident noticeably smaller. outcome likely resulttwo factors. First foremost, FCFS-Light policy must make broad allowancesaccommodate human drivers, thus overall inherently less dangerous.characteristic close calls standard FCFS policy less common. Second,simulated human driver agents drive blindly intersectiontrustingintersection managerthe way autonomous vehicles do. Also note Figure 22(b)visible periodicity light model portion policy. paths open632fiA Multiagent Approach Autonomous Intersection Managementautonomous vehicles due changes lights, drive unwittingly growingmass crashed cars.5.3.3 Reducing Number Collisionstwo main components safety mechanism introduced Section 5.2. First,intersection manager stops accepting reservations. Second, intersection managersends messages informing driver agents incident taken place.possibility second part might always work perfectly; vehicles mightreceive message. investigate effects potential communication failures,intentionally disabled vehicles ability receive Emergency-Stopmessage. parameter simulator controls fraction vehicles createdproperty, varying parameter, could observe subsequent effectaverage number vehicles involved incidents.compared oblivious intersection manager, number vehicles involvedaverage incident active intersection manager decreases dramatically. Table 1shows numerical results 3- 6-lane intersections, along 95%confidence interval. average crash logs runs shown Figure 22,would indistinguishable one another scale. Instead, presentFigure 23.ObliviousPassiveActive20% receiving40% receiving60% receiving80% receiving100% receivingFully Autonomous3 Lanes6 Lanes27.9 1.3 90.9 4.92.63 .13 3.23 .165% Human3 Lanes6 Lanes19.3 1.1 49.3 2.72.23 .10 2.35 .132.44 .132.28 .121.89 .101.71 .081.36 .062.07 .101.91 .101.72 .091.46 .071.22 .053.15 .172.90 .162.69 .152.30 .131.77 .102.29 .132.07 .121.98 .111.65 .091.50 .09Table 1: Average number simulated vehicles involved incidents 3- 6-lane intersections. Even passive safety measures, number crashedvehicles dramatically decreased oblivious intersection manager.active configuration, vehicles receive emergency signal, numbercrashed vehicles decreases further.Figure 23 shows effects reactive safety measures intersections 6 lanes,proportion receiving vehicles varying 0% (passive) 100% increments20%. Even passive configuration, overall number vehicles involvedaverage incident decreases factor almost 30 fully autonomous scenario,factor 20 scenario 5% human drivers, compared obliviousintersection manager. expected active configuration, vehicles receiveemergency signal, fewer wind crashing. graphs Figure 23 show first633fiDresner & Stone15 seconds incident, case collision occur 15 secondsincident started.3.52.4Passive20% receiving40% receiving60% receiving80% receiving100% receiving2Cars CrashedCars Crashed3Passive20% receiving40% receiving60% receiving80% receiving100% receiving2.22.521.81.61.41.51.211024681012140Time (s)2468101214Time (s)(a) autonomous(b) 5% humansFigure 23: first 15 seconds average crash logs 6-lane passive active intersections. vehicles react signal, safety improves.5.3.4 Reducing Severity Collisionsreassuring know number vehicles involved average incidentkept fairly low, data give entire picture. example, compareincident 30 vehicles lose hubcap one two vehicles completelydestroyed occupants killed. currently plans modelintricate physics individual collision high fidelity, simulations allow usobserve velocity collisions occur. previous example, mightnotice 30 vehicles bumped one another low velocities, twovehicles traveling full speed. quantify information, recordcollision happens, velocity happens. collision, amountdamage done approximately proportional amount kinetic energy lost.kinetic energy proportional square velocity, use running totalsquares crash velocities create rough estimate amount damagecaused incident. Figure 24 shows average damage log 6-lane intersectionautonomous vehicles. Qualitatively similar results found intersectiontypes.Figure 24(a) shows, effect safety measures metric quitedramatic well. passive case total accumulated squared velocity decreasesfactor 25. active case, vehicles receiving signal, decreasesanother factor 2. particular note zoomed-in graph Figure 24(b).passive configuration, total squared velocity accumulates intersection manageroblivious, first vehicles stop short intersection around 3 seconds;without reservation, may enter. active scenario, vehiclesreceive message, improvement almost immediate.634fiA Multiagent Approach Autonomous Intersection Management1800ObliviousPassiveActive2 225000Accumulated Squared Velocity (m /s )2 2Accumulated Squared Velocity (m /s )3000020000150001000050000ObliviousPassiveActive160014001200100080060040020001020304050600123Time (s)4567Time (s)(a) average incident(b) zoomedFigure 24: Average total squared velocity crashed vehicles 6-lane intersectionautonomous vehicles. Sending emergency message vehiclescauses fewer collisions, also makes collisions happen less dangerous.5.3.5 Delayed Incident DetectionImplicit results assumption intersection managers become awareincidents instantaneously. could case many collisionsvehiclescommunicate collidedif vehicles communications faulty,vehicle realize collided, intersection may discover problemseconds, another vehicle sensor detect problem. assess effectsdelayed incident detection, artificially delayed intersection managers responsesimulations. Figure 25 shows results experiments.45Number crashed vehiclesNumber crashed vehicles5s delay3.533s delay2.51s delay2delay1.514.5gvinei4%3.5,0ay35sldecg2s delay, 50% receivin2.521.5102468100Time (s)246810Time (s)(a) delaying detection(b) delays faulty communicationFigure 25: Crash logs showing effects delayed incident detection.Figure 25(a), intersection managers reaction delayed 0, 1, 3, 5 seconds.Note total number crashed vehicles delay 5 seconds parnumber experiment intersection manager reacts immediately, none635fiDresner & Stonevehicles receive message, shown Figure 23(a). Figure 25(b) shows happensdelayed detection faulty communication. graph, along earlierresults, suggests small values, second delay approximately equivalent20% vehicles receiving Emergency-Stop message, combined,delayed detection faulty communication additive effect. larger delays,number vehicles involved approximated using data shown Figure 22(a),cases, number vehicles crash intersection muchsmaller number crash reacts.5.4 Safety Discussionresults section suggest may possible improve efficiency alsoimproving safety. course deployment real world, extensive testingreal vehicles would needed order verify suggested efficiency benefits,well safety properties system. People often hesitant put wellbeing (physical otherwise) hands computer unless convincedreceive significant safety benefit exchange surrendering precious control.Humans often suffer overconfidence effect, erroneously believingskillful others. 1981 survey Swedish drivers, respondents asked ratedriving ability relation others. full 80% asked placedtop 30% drivers (Svenson, 1981). effect creates high standardcomputerized systems held. insufficient systems marginallysafer, safer average user; must paragon safety.experiments, showed number vehicles involved individual incidents drastically reduced utilizing fairly straightforward reactive safety mechanism. fact, active configuration 3 lanes, 75% incidents involvedone vehicle: one intentionally crashed (60% 6 lanes). Even passive case6 lanes traffic, average 3.23 vehicles involved.compare current systems? conservatively assume accidents traffic todayinvolve one vehicle, represents 223% increase per occurrence. Thus,things equal, frequency accidents reduced 70%, experimentssuggest autonomous intersection management system safer overall. 2002report U.S. Federal Highway Administration blamed 95% accidentsdriver error (Wierwille et al., 2002). remaining accidents divided equallyvehicle failures problems roads. important note numbersdriving, intersection driving. Accidents intersections even likelycaused driver error, sometimes even drivers willfully disobeying law: runningred lights stop signs making illegal U-turns.Even make overly conservative assumptionsthat driving dangerous intersection driving, driver error accountable intersection crashestypes drivingour data suggest automobile traffic autonomousdriver agents intersection control mechanism like reduce collisions intersections 80%. believe reality, improvement much greater.safety measures presented section constitute one approach mitigatingsystems failure modes. sophisticated methods involving explicit cooperation636fiA Multiagent Approach Autonomous Intersection Managementamongst vehicles may create even safer system. shown (or attemptedshow) particular solution best possible. Rather demonstratedeven simple straightforward response accidents, overall safetysystem maintained, without sacrificing benefits vastly improved efficiency.6. Related WorkTraffic control vast area research computer scientists engineers alike. fieldIntelligent Transportation Systems (ITS) concerned applying information, computing, sensor technologies solve problems traffic road management (Bishop,2005). includes intelligent vehicles (IV) well infrastructure, intersections. Unfortunately, aspects heavily studied, relatively little currentresearch considers intelligent autonomous vehicles infrastructure work together improve efficiency safety overall traffic system. BerkeleyPATH project produced lot interesting work, including work fully-automatedhighway (Alvarez & Horowitz, 1997).section, describe work related own, directly tangentially. work specifically concerned intersection control, takesmultiagent approach aspects traffic management, represents worktechnologies necessary bring fully autonomous vehicles mainstream.6.1 Requisite Technologyautonomous vehicles take roads, need able interactaspects roadways, including pedestrians, vehicles, lanes. early1991, driver agent system named Ulysses developed simulation (Reece& Shafer, 1991). systems currently development implementationreal vehicles geared toward assisting human drivers, many technologies createdefforts applicable creation completely autonomous driveragent. successful driver agent needs three main things: detect entitiesroad, keep vehicle lane, maintain safe distances vehicles.Fortunately, three subtasks currently attracts extensive amount research.6.1.1 Object Detection Trackingfully autonomous vehicle must able reliably detect, classify, track various objectsmay roadway. pedestrians bicycles cars trucks, autonomousvehicles require robust sensors monitor world around mannerlighting conditions weather. Without abilities, amount higher reasoningdriver agent irrelevant. Fortunately, researchers attacking problemmany techniques.2004, Honda introduced intelligent night vision system Japanese marketcapable detecting pedestrians (Liu & Fujimura, 2003). system uses two far-IR(FIR) cameras front vehicle detect heat-emitting objects beyond rangeilluminated vehicles headlights. two cameras allow system obtain distanceinformation detected pedestrians warn driver. DaimlerChrysler637fiDresner & Stonedeveloping similar system also extrapolates trajectories classified objectsorder predict possible outcomes sooner (Gavrila, Giebel, & Munder, 2004). Mahlischet al. (2005) developed sensor fusion technique glean informationpedestrians reliably even low-resolution images.Ford Motor Company investigating track vehicles using colorshape information (She, Bebis, Gu, & Miller, 2004). Gepperth et al. (2005)demonstrated gray-valued videos (no color), two-stage (initial detectionconfirmation) mechanism using simple neural network confirmation reliablyquickly classify vehicles.Vehicle pedestrian classification tracking well-studied area IV researchprogressing quickly. glance IV-related conference symposium revealplethora articles aimed using lidar, FIR, normal video, combinationsensors algorithms like Kalman filters, particle filters, neural networks trackclassify objects road.6.1.2 Lane Followingpedestrian vehicle detection tracking, lane following heavily studied area IV research. Varying passive lane- road-departure warning systems(LDWS/RDWS) active lane keeping assistance (LKA), many systems already showing production vehicles.far RDWS go, Kohl et al. (2006) used neuroevolution create warningsystem warn drivers road departure impending crashesvehicles. system tested simulation robotic vehicle. worksponsored Toyota, also currently LDWS market Japan.system unique uses rear-facing camera predict warn impending lanedepartures. LDWS RDWS promise extensive benefits drivers, warnimminent road lane departures, provide information specificaction taken. Autonomous vehicles need ensure reach pointlane road departure imminent.Lane keeping, hand, provides executes actions. example,Hands Across America project 1995 drove vehicle 2,849 miles Pittsburgh LosAngeles. 98.2% journey, vehicle steered (Pomerleau, 1993). recentprojects concentrated making systems robust varying speed, inclementweather poor lighting conditions beneath overpasses tunnels. Wu etal. (2005) proposed tested vision-based lane-keeping system operatevarying speed providing smooth human-like steering. Watanabe Nishida (2005),working Toyota, developed lane detection algorithm specifically designedsteering assistance systems extremely robust varying road conditions lighting.several LKA systems market Japan, systems intendedallow autonomous driving. Rather, attempt reduce driver fatigue maketurning stable (Bishop, 2005). Production systems allow autonomous steeringalmost invariably based specially painted lines limited special vehiclesclosed courses.638fiA Multiagent Approach Autonomous Intersection ManagementEven without benefit explicitly designated lanes, autonomous vehicles keeproadway. 2005 DARPA Grand Challenge (DARPA, 2007),winning vehicle, Stanley, used technique fusing short-range laser range finderslong-range video cameras follow rough dirt path. First, vehicle found smooth areasfront using laser range finders. mapped information onto videoimages forward-facing cameras. determining color area imagecorresponding smooth areas found laser range finder, Stanley ableextrapolate using flood-fill-type algorithm find areas video imagedirt path (NOVA, 2006). Ramstrom Christensen (2005) achieved similar goalusing strategy based probabilistic generative model.6.1.3 Adaptive Cruise Controllane-keeping systems represent main lateral component autonomous vehiclesdriver agent, adaptive cruise control (ACC) main longitudinal component. ACCallows vehicle maintain safe following distance react quicker humandriver case sudden deceleration vehicle front. ACC systems alreadyavailable marketDaimlerChryslers Mercedes-Benz S-class, example, comessystem automatically apply brake detects driver slowingsufficiently fast. Jaguar, Honda, BMW offer similar systems. Nissan Toyotarecently begun offering low-speed following systems, follow vehiclesslower, denser, urban traffic scenarios (Bishop, 2005). ACC relies robust sensinguses radar, lidar, traditional machine vision algorithms. combining various flavorsACC low speed, high speed, etc.an agent could control longitudinal motionvehicle situations. Recently, notion cooperative adaptive cruise control (CACC)emerged (Laumonier, Desjardins, & Chaib-draa, 2006). concept goes muchtoward realizing goal fully autonomous vehicles. allowing vehicles collaboratetake advantage precision autonomous driver agents, vehicles useexisting road space much efficiently.6.2 Intersection Collision Avoidancedate, much work relating intersections focused Intersection CollisionAvoidance (ICA). work seeks warn driver vehicle may enteringintersection unsafely. aid high-precision digital maps GPS equipment,vehicle detects classifies state traditional signaling systems placedintersection (Lindner, Kressel, & Kaelberer, 2004). ICA systems typically takeaction behalf driver, simply provide visual auditory warning.Rasche Naumann (1997, 1998, 1997) worked extensively decentralized solutions intersection collision avoidance problems, including involving autonomousvehicles. work similar uses potential points collisionrestrict access intersection. one vehicle may occupy potential pointcollision time. Vehicles attempt obtain token (similar token-ring computer networking) point needed cross intersection. vehiclenecessary tokens, may cross. Rasche Naumanns system also includes prioritymodel allows emergency vehicles cross quickly prevents deadlocks amongst639fiDresner & Stonenormal vehicles. However, system fails satisfy several desiderata.make guarantees, authors provide results regarding efficiencysystem compared traditional system. Furthermore, distributed algorithmshown resilient unreliable communication. authors also provideinsight system could adapted work mixed human/autonomousvehicle population. striking difference, however, mechanismseem notion planning ahead. Tokens potential points collisioneither taken takena vehicle seek obtain token pointfuture, thus allowing proceed toward intersection without slowingvehicles tokens.context video games animation, Reynolds (1999) developed autonomous steering algorithms attempt avoid collisions intersectionssignaling mechanisms. system would enormous advantagerequiring special infrastructure agent intersectionvehicles equippedalgorithms could operate intersection. Unfortunately, two main drawbackssystem make unsuitable use real-life traffic. First, algorithmlet agent choose path take intersection; vehicle may even findexiting intersection way came in, due efforts avoid collidingvehicles. Second, algorithm attempts avoid collisionsit makeguarantees safety.Cooperative intersection collision avoidance form cooperative vehicle-highway system (CVHS) intersection allowed participate ICA problem. ICAsystems contained entirely individual vehicles cannot account gaps sensor viewssources incomplete information. Thus, CVHS approach required.many technologies, production systems still assume human driver attemptwarn violation occur, cases, punishfact, cameras detect vehicle run red light automaticallyissues driver citation. U.S. Department Transportation sponsoring several ICA projects including infrastructure-only cooperative approaches (USDOT,2003). intention first deploy infrastructure-only systems,market penetration ICA-equipped vehicles increases, roll cooperative systems.Significant work ICA also underway Japan (Bishop, 2005).systems large step toward enabling autonomous vehicles takeroads, none designed work specifically autonomous vehicles. exceptionalgorithm designed games, assumes human driver traditionalsignaling systemsa clumsy, inefficient interface find obsolete dueautonomous vehicle technology.6.3 Optimizing Traffic Signal Timingvast majority deployed technology intersection control involves calibratingtiming traditional traffic lights order create wave green vehiclesreach one green light, continue subsequent intersections withoutstop. Unfortunately, practice, waves tend sporadic short-lived due640fiA Multiagent Approach Autonomous Intersection Managementrapidly changing traffic patterns. However, offer substantial benefits comparedsystems without coordination.TRANSYT, Traffic Network Study Tool, off-line system that, given averagetraffic flows, determine optimum fixed-time coordinated traffic signal timings (Robertson, 1969). TRANSYT requires extensive data gathering analysis, usedheavily world. Unfortunately, system brittleability react unusual changes traffic flow. example, endmajor sporting event, thousands vehicles may attempting cross intersectiondirection normal circumstances rarely used. light timingsset reflect normal circumstances, length time departingvehicles get green light may significantly less cross traffic, maylittle.SCOOT, Split, Cycle, Offset Optimisation Technique, represents advancement TRANSYT (Hunt, Robertson, Bretherton, & Winton, 1981). SCOOTon-line adaptive traffic control system react changes traffic levels, give priority vehicles buses, even estimate vehicle emissions. SCOOTshown reduce traffic delays average 20% systems like TRANSYT, stillrelies traditional signaling systems vehicles. Furthermore, SCOOT requires reliabletraffic data order adapt, thus may slow react changes traffic flow.6.4 MAS TrafficAutomobile traffic great example multiagent system, surprisinglot research modelling studying traffic using multiagent techniques.Many approaches consider systems consisting traffic-light-controlling agentsdriver agents, opposed heterogeneous multiagent system many kindsagents. Nevertheless, many ideas involved could potentially adapted workwithin framework reservation system.6.4.1 Cooperative Traffic SignalsMuch MAS traffic research focuses improving current technology (systems trafficlights). example, Roozemond (1999) allows intersections act autonomouslysharing data gather. intersections use information makeshort- long-term predictions traffic adjust accordingly. strategyattempts overcome one weaknesses SCOOT: need large amountsreliable traffic data. multiple intersections share data, intersection getaccurate picture current traffic situation.Bazzan (2005) used decentralized approach combining MAS evolutionarygame theory. approach models intersection individually-motivated agentmust focus local goals (getting vehicles intersection),also global goals (reducing travel times vehicles). Bazzan Roozemondstechniques still assume traditional signaling mechanisms human drivers.641fiDresner & Stone6.4.2 Platoonsaddition multi-intersection systems, multi-vehicle systems focus lotresearch. Much research centers creating platoons vehicles order minimizeeffects stop-and-go driving. Consider line cars stopped red light.light turns green, first car begins move. Eventually, car behind noticesenough space accelerate well. time later, vehicle backline begin move, may late actually get intersectioncurrent green phase light. If, hand, vehiclessimultaneously uniformly accelerate, vehicles could make greenphase, vehicles would efficiently use space-time availablecross intersection.Clement (2002) proposed model called Simple Platoon Advancement (SPA),addresses exact problem. SPA boasts ability get nearly twice manyvehicles green light (increasing lights throughput) compared normalhuman drivers, addition safety delay benefits associated automated control. One vehicles intersection dispersed safe following distances,control returned human driver.Halle Chaib-draa (2005) used platoon approach facilitate collaborativedriving general. allow vehicles, controlled separate agents, formplatoons, varying degrees autonomy. Vehicles merge split platoonsusing carefully crafted maneuvers, vehicle platoon specificresponsibility. present centralized version, master vehicle gives ordersrest platoon, decentralized version, social laws dictateagents role, platoons leader acts representative platoons.platooning systems assume automated control vehicles, use ordinary trafficlights intersection control. using platoons, methods attempt solve probleminherent traffic lights themselvesthey designed humans use,well suited automated vehicle control. work presented article attemptsfree autonomous vehicles control traffic lights instead design new systemspecifically utilizes capabilities fully autonomous vehicles.6.4.3 History-Based Traffic ControlTaking different approach intersection control, Balan Luke (2006) use historybased method maximize fairness (all vehicles experience similar delays) opposedefficiency (the average vehicle experiences short delays). paradigm, vehicleshistorically (previously journey) experienced long delayslikely experience shorter delays subsequent intersections. additionmulti-intersection approach, method uses marketplace model involving systemcredits given taken exchange shorter longer delays, respectively.Coordination individual intersections still done traditional traffic lights, timings part mechanism. Interestingly, fairness approach actually yieldsresults also reasonably efficient.642fiA Multiagent Approach Autonomous Intersection Management6.5 Machine Learning TrafficAbdulhai et al. (2003) used Q-learning, simple, yet powerful form reinforcementlearning, on-line adaptive signal control. work, authors exploreisolated intersection well linear chain intersections. demonstrate Qlearning significantly reduce delays vehicles quickly adapt changing trafficpatterns. Bull et al. (2004) shown Learning Classifier Systems (LCS) also maketraditional traffic signals efficient. Wiering (2000) demonstrated multiagent,model-based reinforcement learning also used optimize signal timingscomplex networks intersections.focusing intersections, Moriarty Langley (1998) shownreinforcement learningspecifically neuro-evolutioncan train efficient driver agentslane, speed, route selection freeway driving, critical componentsfully autonomous vehicle. Additionally, many object tracking detectionexamples mentioned previously use neural networks classify objects.6.6 Physical Robotsreal autonomous vehicles, Kolodko Vlacic (2003) created small-scale systemintersection control similar granularity-1 FCFS policy. authorsdeveloped mechanism small Cooperative Autonomous Mobile Robots (CAMRs),30 cm diameter top speed 10 cm/s. CAMRsprogrammed follow Australian traffic laws, communicate several different typesmessages. demonstrated CAMRs, mechanism scaled use Imaravehicles, much larger (capable carrying two human passengers) faster (topspeed 30 km/h). system completely distributed require extensiveinfrastructure intersection. However, assume vehicles cooperateone another.6.7 Safety AnalysisSection 5 includes failure-mode analysis proposed intersection control mechanism. best knowledge, first study impactautonomous intersection protocol driver safety. However, enormousbody work regarding safety properties traditional intersections. includesgeneralcorrelating traffic level accident frequency (Sayed & Zein, 1999) analysesparticular types intersections (Bonneson & McCoy, 1993; Harwood, Bauer, Potts, Torbic, Richard, Rabbani, Hauer, Elefteriadou, & Griffith, 2003; Persaud, Retting, Gardner,& Lord, 2001)as well plenty esoteric work, characterizing roleAlzheimers Disease intersection collisions (Rizzo, McGehee, Dawson, & Anderson,2001). However, concerns human-operated vehicles, none workparticularly applicable setting concerned with.7. Conclusion Future Workreservation system presented large step toward easing traffic woes, termswasted time injury loss life. However, substantial work must still done643fiDresner & Stonesystem ready deploy. work represents possible future directionsline research. example, detailed studies safety propertiessystemhow reacts various failures whether effects failuresmitigatedare required. Another area ripe improvement intersection manager.manager switch among several different policies, learning reservation histories policy best suited particular traffic conditions, could significantly improveperformance. Furthermore, light model could react traffic conditions,also presence individual vehicles, might better able exploit abilitiesautonomous vehicles, without adversely affecting human drivers. Framing intersectionmarketplace space-time commodity could allow system handle vehicle priorities intelligently allow driver agents exchange long wait one dayquick passage later, important day. Finally, driver agent mayable benefit machine learning techniques, perhaps learning makeaccurate reservations thus needing cancel less frequently.article makes three main contributions. First, defines problem autonomousintersection management, including set desiderata potential solutionsevaluated. Second, presents framework meet desiderata,algorithm (FCFS) shows advantages framework current intersectioncontrol methods. Third, demonstrates framework extended allowhuman-driven (not autonomous) vehicles use system, still exploiting abilitiesautonomous vehicles increase throughput subsequently decrease delays.Getting today future humans longer burdenedmundane yet dangerous task piloting automobiles involve vast amountwork many different disciplines. extensively address engineeringsocietal challenges involved building deploying system, article suggestsalgorithmically feasible worthwhile (in terms decreasing delay)so.Acknowledgmentsresearch supported NSF CAREER award IIS-0237699, experimentscarried machines provided NSF grant EIA-0303609.644fiA Multiagent Approach Autonomous Intersection ManagementAppendix A. Simplified Laser Range Finderappendix describes implementation detail driver agents sensor model. RecallSection 3.1.1 driver access set simulated external sensors.set simplified laser range finder, intended give agent typeinformation actual laser range finder, without expensive computation requiredfully simulate sensor. Instead, simplified laser range finder sensor examinesvehicle within sensor range determines closest front sensingvehicle. Then, records point vehicle closest sensing vehicleprovides distance angle point.Modern laser range finders distance sensors provide large amount distanceangle data mobile agent. real life setting, information would definitelyprove useful fine-tuning driver agent. However, simple simulation, must processsensor information vehicles simultaneously, accurately simulating full laserrange finder feasible. Thus, use simple, yet pertinent sensor readingdriver agent use control actions respect vehicles. purelystraight-ahead sensor suffices vehicles traveling straight lines. However,vehicle turns, must also take account going directionturning. complicate matters, vehicle turning must still takeaccount going directly front point might straightenwheels continue current heading. sensor points directionwheels sufficient vehicles coming turns may runvehicles ahead them. Instead, sensors scope widens direction turn,narrowing slightly side. Figure 26 shows scenario demonstratesconcept. testament sensors usefulness, vehicles equipped sensor(i.e. intersection manager present) able avoid many collisions intersection,even moderate amounts traffic.Appendix B. Communication ProtocolSection 3.2 gave brief introduction communication protocol used agentsreservation system. appendix, specify protocol much greater detail.protocol consists several message types kind agent, well rulesgoverning messages sent sorts guarantees accompany them.section present aspects essential understanding remainderarticle.B.1 Message Typesvehicles intersection manager restricted types messagesmust coordinate.B.1.1 Vehicle Intersectionfour types messages sent vehicles intersection.645fiDresner & StoneFigure 26: depiction sensor model driver agents. sensor focusedgray lines provide information outside them.black line represents reading provided driver agent.1. Request message vehicle sends reservationwishes make one. contains properties vehicle (ID number, performance, size, etc.) well properties proposed reservation (arrival time,arrival velocity, type turn, arrival lane, etc.). message also communicatesvehicles status emergency vehicle (in emergency situation). practice,would implemented using secure method normal vehicles couldimpersonate emergency vehicles. methods well understood detailsimplementation beyond scope research.message 15 fields:vehicle id unique identifier vehicle.arrival time absolute time vehicle agrees arrive intersection.arrival lane unique identifier lane vehiclearrives intersection.turn way vehicle turn reaches intersection.arrival velocity velocity vehicle agrees travelingarrives intersection.maximum velocity maximum velocity vehicle travel.maximum acceleration maximum rate vehicle accelerate.646fiA Multiagent Approach Autonomous Intersection Managementminimum acceleration minimum rate vehicle accelerate (i.e.negative number representing maximum deceleration).vehicle length length vehicle.vehicle width width vehicle.front wheel displacement distance front vehiclefront axle.rear wheel displacement distance front vehiclerear axle.max steering angle maximum angle front wheels turnedpurposes steering.max turn per second rate vehicle turn wheels.emergency whether emergency vehicle emergency situation.2. Change-Request message vehicle sends reservation,would like switch different set parameters. new parametersacceptable intersection, vehicle may keep old reservation.identical request message, except includes unique reservation IDreservation vehicle currently has.message identical Request message, except one added field:reservation id identifier reservation changed.3. Cancel message vehicle sends longer desires currentreservation.2 fields:vehicle id unique identifier vehicle.reservation id identifier reservation cancelled.4. Done message sent vehicle completed traversalintersection. communicates information Cancel message,may behavior tied Cancel message occur vehicle successfully completes trip across intersection. Additionally, messagecould extended order communicate statistics vehicle, couldrecorded order analyze performance intersection manager.message used collect statistics vehicle, recordedorder analyze improve performance intersection manager.2 fields:vehicle id unique identifier vehicle.reservation id identifier reservation completed.647fiDresner & StoneB.1.2 Intersection Vehiclefour types messages sent intersection individualvehicles.1. Confirm message response vehicles Request (or Change-Request)message. always mean parameters transmitted vehicleacceptable. could, example, contain counter-offer intersection.reservation parameters message implicitly accepted vehicle,must explicitly cancelled driver agent vehicle approve. Notesafe even faulty communicationthe worst happenintersection reserves space get used. Included messageacceleration constraints determined intersection. list ratesdurations. list created depends intersection manager. However,vehicles safety must guaranteed adheres list.message 7 fields:reservation id unique identifier reservation created.arrival time absolute time vehicle expected arrive.early error tolerable error (early) arrival time vehicle.late error tolerable error (late) arrival time vehicle. Noteintersection manager must assume car could arrive traverseintersection time within resulting boundsarrival lane unique identifier lane vehiclearrives intersection.arrival velocity velocity vehicle expected travelingarrives intersection. negative number signifies velocityacceptable.accelerations run-length encoded description expected accelerationvehicle travels intersection. Here, run-length encoded description sequence order pairs acceleration durationstartinginstant vehicle enters intersection, maintainacceleration duration paired. sequence empty,accelerations acceptable.2. Reject sending message, intersection inform vehicleparameters sent latest Request (or Change-Request) acceptable,intersection either could want make counter-offer.message also indicates whether rejection reservationmanager requires vehicle stop intersection entering. letsdriver agent know attempt reservations reachesintersection.message 1 field:648fiA Multiagent Approach Autonomous Intersection Managementstop required boolean value indicating whether vehicle must first comefull stop entering intersection.3. Acknowledge message acknowledges receipt Cancel Donemessage.1 field:reservation id unique identifier reservation cancelled completed.4. Emergency-Stop message sent intersection managerdetermined collision similar problem occurred intersection.message informs receiving driver agent reservation requestsgranted, possible, vehicle attempt stop instead enteringintersection, even reservation. specifics message useddiscussed Section 5.2.2. message fields, communicatessingle bit information.B.1.3 Vehicle Vehiclecurrently protocol communication vehicles.B.2 Protocol Actionsaddition message types, agents involved (the vehicles intersection) mustobey set rules. entirely unlike rules human drivers followdriving.B.2.1 Vehicle Actionsrules vehicles expected follow order allow intersectionfunction efficiently.1. vehicle may enter intersection without reservation.2. vehicle going cross intersection, must everything reasonable withinpower cross accordance parameters included recentConfirm message received intersection.3. vehicle sends another message intersection manager sent response,intersection manager may choose ignore it. Thus, vehicle sendmessage received response previous message.4. vehicle yet entered intersection reservation,may send Request message. yet entered intersectionreservation, may send either Change-Request Cancel message.sends messages allowed to, intersection may chooseignore them.5. vehicle reservation successfully crossed intersection, may sendDone message.649fiDresner & Stone6. vehicle receives Confirm message, considered reservation.B.2.2 Intersection Actionsrules representing obligations intersection manager expectedfulfill.1. intersection receives Request message, must respond eitherConfirm Reject message. responds Confirm message, guaranteeing cross-traffic interfere vehicle crosses intersectionaccordance parameters message.2. intersection receives Change-Request message, must respondeither Confirm Reject message. responds Confirm message,guaranteeing cross-traffic interfere vehicle crosses intersection accordance parameters message. previous guaranteesnullified.3. intersection receives Cancel message, must respond Acknowledge message. guarantee made sending vehicle nullified.Appendix C. Driver Agentstated Section 3.3, main focus work improving frameworkalgorithms intersection control. However, order require sortdriver agent. Furthermore, efficiency reservation framework depends driveragents reasonably intelligent, non-trivial. appendix describes driveragent implementation used experiments.Containing behaviors control turning vehicles well optimizations increasepeformance system overall, driver agent represents single intricate component reservation mechanism. Algorithm 3 gives high-level pseudocode descriptiondriver agent.C.1 Lane FollowingGiven model lanes, driver must able drive vehicles lanes.accomplish means lane following behavior acts modifyingsteering angle vehicle. behavior entirely independent rest agentsbehavior, controls vehicles acceleration communicates intersectionmanager. behavior active timesthe vehicle always attempting staycurrent lane. lane-following behavior designed robust sudden lanereassignment, turning lane changing implemented: driveragent simply changes lane current lane, lane-following behavior steersvehicle correct lane. process entirely smooth, provided vehicletraveling reasonable velocitya condition enforced parts driver agentsbehavior.lanes modeled directed line segments, lane-following behavior attempts keep vehicle evenly straddling lane. line segment represents650fiA Multiagent Approach Autonomous Intersection Managementmiddle lane, thus condition equivalent keeping vehicle centeredlane. driver agent accomplishes turning front wheels toward pointsegment. point, call aim point farther along segmentvehicle. aim point computed first projecting point front centervehicle onto line segment, displacing point direction linesegment amount call lead distance. part, lead distanceproportional velocity vehicle. proportion smaller inside intersectionvehicles pull strongly new lane turningthey mustentirely correct lane leave intersection collidevehicles outside intersection. proportional lead distance necessaryotherwise high velocities, required steering angle may change fasterdriver agent steer, resulting either wildly erratic steering vehicle drivingcircles. lead distance also minimum value 1 meter. lead distance getssmall, effect velocity largeby ensuring aim pointleast meter farther lane, ensure vehicle endstable configuration traveling proper direction. Figure 27 depicts driveragent determines lead distance (and subsequent aim point) different velocities.bcFigure 27: vehicle attempting follow lane. so, first calculates pointrepresents projection position onto directed line segmentrunning center lane (a). Then, depending velocity,displaces resulting point direction travel small large amountobtain point aim front wheels. low velocities,point displaced muchonly enough ensure vehicle movescorrect direction (b). higher velocities, aim point must fartheralong lane, vehicles steering gradual thusstable (c).method lane following one possible method, selectedsufficient purposes. Furthermore, reservation systems functionalitydepend driver agent using particular method, work method,provided driver agent turns within mutually understood constraints.651fiDresner & StoneC.2 Optimistic Pessimistic Driver Agentsnave driver agent perform poorly when, example, makes reservationstuck behind slower-moving vehicle. vehicle front eventually accelerates, wouldideally accelerate well (possibly switching earlier reservation).account situations like this, introduce notion optimistic pessimisticdriver agent. optimistic agent makes reservation assuming arrive intersection minimum possible time. agent finds longer stuck behindslower vehicle become optimistic attempt make new, earlier reservation.pessimistic agent assumes stuck current velocity reaches intersection. agent cancel reservation way arrivetime, becomes pessimistic. Due relatively infrequent smooth transitionsmoods, driver agent take advantage improving circumstanceswithout causing send excessive numbers messages things change.shown Figure 2, addition optimism pessimism driver agentreduced average number reservations made well average numbermessages transmitted. expected, effect less pronounced lower amountstraffic.WithoutMessages560.855.97Reservations165.891.02Table 2: moderate amount traffic, average number messages sent reservations made driver agents without optimism/pessimism heuristic.C.3 Estimating Time Intersectiondriver agents estimate long take get intersection mustprecise vehicles arrive time reservations. If, point,vehicle certain whether arrive schedule, cannot safely continue.pessimistic driver agent simply divides distance intersection currentvelocityit assumes able accelerate. optimistic driver first determinesvelocity arrives. turning, example, velocity maylower speed limit. Otherwise, may limited amount vehicleaccelerate reaching intersection. computes minimum possible timereach intersection velocity, is, assumes accelerate muchpossible decelerating arrival velocity.652fiA Multiagent Approach Autonomous Intersection ManagementAlgorithm 3 driver agent behavior. driver agents initialized optimistic.1: determine aim point attempt point wheels2: current time3: Velocity speed limit4:Accelerate5: intersection6:Optimistic7:ti optimisitic estimate time intersection8:else9:ti pessimistic estimate time intersection10:reservation11:+ ti scheduled arrival12:Cancel reservation13:become pessimistic14:else + ti significantly scheduled arrival15:Become optimistic16:Attempt change reservation earlier time17:else18:Try make reservation according ti19:Reservation request rejected20:Decelerate21: else intersection22:Set acceleration according parameters reservation23: intersection less 1 second behind car front24:DecelerateReferencesAbdulhai, B., Pringle, R., & Karakoulas, G. J. (2003). Reinforcement learning trueadaptive traffic signal control. Journal Transportation Engineering, 129 (3), 278285.Alvarez, L., & Horowitz, R. (1997). Traffic flow control automated highway systems.Tech. rep. UCB-ITS-PRR-97-47, University California, Berkeley, Berkeley, California, USA.Balan, G., & Luke, S. (2006). History-based traffic control. Proceedings FifthInternational Joint Conference Autonomous Agents Multiagent Systems, pp.616621, Hakodate, Japan.Bazzan, A. L. C. (2005). distributed approach coordination traffic signal agents.Autonomous Agents Multi-Agent Systems, 10(2), 131164.Bishop, R. (2005). Intelligent Vehicle Technology Trends. Artech House.Bonneson, J. A., & McCoy, P. T. (1993). Estimation safety two-way stopcontrolledintersections rural highways. Transportation Research Record, 1401, 8389.653fiDresner & StoneBull, L., ShaAban, J., Tomlinson, A., Addison, J. D., & Heydecker, B. G. (2004). Towardsdistributed adaptive control road traffic junction signals using learning classifiersystems. Bull, L. (Ed.), Applications Learning Classifier Systems, pp. 276299.Springer.Clement, S. (2002). SPA model smooth acceleration. 24th ConferenceAustralian Institutes Transport Research (CAITR-2002), Sydney, Australia.DARPA (2007). DARPA urban challenge.. http://www.darpa.mil/grandchallenge.Dresner, K., & Stone, P. (2004). Multiagent traffic management: reservation-based intersection control mechanism. Third International Joint Conference Autonomous Agents Multiagent Systems, pp. 530537, New York, NY, USA.Dresner, K., & Stone, P. (2006). Multiagent traffic management: Opportunities multiagent learning. K. Tuyls et al. (Ed.), LAMAS 2005, Vol. 3898 Lecture NotesArtificial Intelligence, pp. 129138. Springer Verlag, Berlin.Gavrila, D. M., Giebel, J., & Munder, S. (2004). Vision-based pedestrian detection:PROTECTOR+ system. Proceedings IEEE Intelligent Vehicles Symposium(IV2004), Parma, Italy.Gepperth, A., Edelbrunner, J., & Bucher, T. (2005). Real-time detection classificationcars video sequences. Proceedings IEEE Intelligent Vehicle Symposium(IV2005), pp. 625631, Las Vegas, NV, USA.Halle, S., & Chaib-draa, B. (2005). collaborative driving system based multiagentmodelling simulations. Journal Transportation Research Part C (TRC-C):Emergent Technologies, 13, 320345.Harwood, D. W., Bauer, K. M., Potts, I. B., Torbic, D. J., Richard, K. R., Rabbani, E.R. K., Hauer, E., Elefteriadou, L., & Griffith, M. S. (2003). Safety effectivenessintersection left- right-turn lanes. Transportation Research Record, 1840, 131139.Hatipo, C., Redmill, K., & Ozguner, U. (1997). Steering lane change: working system.IEEE Conference Intelligent Transportation Systems, pp. 272277.Hunt, P. B., Robertson, D. I., Bretherton, R. D., & Winton, R. I. (1981). SCOOT - trafficresponsive method co-ordinating signals. Tech. rep. TRRL-LR-1014, TransportRoad Research Laboratory.Johnson, R. C. (2005). Steady pace takes DARPA race. EE Times. Accessed http://www.eetimes.com.Kohl, N., Stanley, K., Miikkulainen, R., Samples, M., & Sherony, R. (2006). Evolvingreal-world vehicle warning system. Proceedings Genetic EvolutionaryComputation Conference 2006, Seattle, WA, USA.Kolodko, J., & Vlacic, L. (2003). Cooperative autonomous driving intelligent controlsystems laboratory. IEEE Intelligent Systems, 18 (4), 811.Laumonier, J., Desjardins, C., & Chaib-draa, B. (2006). Cooperative adaptive cruise control:reinforcement learning approach. Fourth Workshop Agents TrafficTransportation, Hakodate, Hokkaido, Japan.654fiA Multiagent Approach Autonomous Intersection ManagementLindner, F., Kressel, U., & Kaelberer, S. (2004). Robust recognition traffic signals.Proceedings IEEE Intelligent Vehicles Symposium (IV2004), Parma, Italy.Liu, X., & Fujimura, K. (2003). Pedestrian detection using stereo night vision. IEEEInternational Conference Intelligent Transportation Systems, Shanghai, China.Mahlisch, M., Oberlander, M., Lohlein, O., Gavrila, D., & Ritter, W. (2005). multipledetector approach low-resolution FIR pedestrian recognition. ProceedingsIEEE Intelligent Vehicles Symposium (IV2005), Las Vegas, NV, USA.Moriarty, D., & Langley, P. (1998). Learning cooperative lane selection strategies highways. Proceedings Fifteenth National Conference Artificial Intelligence,pp. 684691, Madison, WI. AAAI Press.National Highway Traffic Safety Administration (2002). Economic impact U.S. motorvehicle crashes reaches $230.6 billion, new NHTSA study shows. NHTSA Press Release38-02. http://www.nhtsa.dot.gov.Naumann, R., & Rasche, R. (1997). Intersection collision avoidance means decentralized security communication management autonomous vehicles. Proceedings30th ISATA - ATT/IST Conference.Naumann, R., Rasche, R., & Tacken, J. (1998). Managing autonomous vehicles intersections. IEEE Intelligent Systems, 13 (3), 8286.Noda, I., Jacoff, A., Bredenfeld, A., & Takahashi, Y. (Eds.). (2006). RoboCup-2005: RobotSoccer World Cup IX. Springer Verlag, Berlin.NOVA (2006). great robot race.. Originally aired 28 March 2006 PBS, availableonline http://www.pbs.org/wgbh/nova/darpa.Persaud, B. N., Retting, R. A., Gardner, P. E., & Lord, D. (2001). Safety effect roundabout conversions united states: Empirical bayes observational before-afterstudy. Transportation Research Record, 1751, 18.Pomerleau, D. A. (1993). Neural Network Perception Mobile Robot Guidance. KluwerAcademic Publishers.Ramstrom, O., & Christensen, H. (2005). method following umarked roads.Proceedings IEEE Intelligent Vehicle Symposium (IV2005), pp. 650655, LasVegas, NV, USA.Rasche, R., Naumann, R., Tacken, J., & Tahedl, C. (1997). Validation simulationdecentralized intersection collision avoidance algorithm. Proceedings IEEEConference Intelligent Transportation Systems (ITSC 97).Reece, D. A., & Shafer, S. (1991). computational model driving autonomousvehicles. Tech. rep. CMU-CS-91-122, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA.Reynolds, C. W. (1999). Steering behaviors autonomous characters. ProceedingsGame Developers Conference, pp. 763782.Rizzo, M., McGehee, D. V., Dawson, J. D., & Anderson, S. N. (2001). Simulated car crashesintersections drivers Alzheimer disease. Alzheimer Disease AssociatedDisorders, 15 (1), 1020.655fiDresner & StoneRobertson, D. I. (1969). TRANSYT traffic network study tool. Tech. rep. TRRL-LR253, Transport Road Research Laboratory.Rogers, S., Flechter, C.-N., & Langley, P. (1999). adaptive interactive agent routeadvice. Etzioni, O., Muller, J. P., & Bradshaw, J. M. (Eds.), Proceedings ThirdInternational Conference Autonomous Agents (Agents99), pp. 198205, Seattle,WA, USA. ACM Press.Roozemond, D. A. (1999). Using intelligent agents urban traffic control systems. Proceedings International Conference Artificial Intelligence TransportationSystems Science, pp. 6979.Sayed, T., & Zein, S. (1999). Traffic conflict standards intersections. TransportationPlanning Technology, 22 (4), 309323.Schonberg, T., Ojala, M., Suomela, J., Torpo, A., & Halme, A. (1995). Positioningautonomous off-road vehicle using fused DGPS inertial navigation. 2ndIFAC Conference Intelligent Autonomous Vehicles, pp. 226231.She, K., Bebis, G., Gu, H., & Miller, R. (2004). Vehicle tracking using on-line fusioncolor shape features. Proceedings IEEE International ConferenceIntelligent Transportation Systems, Washington, DC, USA.Stone, P., & Veloso, M. (2000). Multiagent systems: survey machine learningperspective. Autonomous Robots, 8 (3), 345383.Svenson, O. (1981). less risky skillful fellow drivers?. ActaPsychologica, 47 (2), 143148.Texas Transportation Institute (2004). 2004 urban mobility report.. Accessed http://mobility.tamu.edu/ums December 2004.USDOT (2003). Inside USDOTs intelligent intersection test facility. NewsletterCooperative Deployment Network. Accessed online 17 May 2006 http://www.ntoctalks.com/icdn/intell_intersection.php.Watanabe, A., & Nishida, M. (2005). Lane detection steering assistance system.Proceedings IEEE Intelligent Vehicle Symposium (IV2005), pp. 159164, LasVegas, NV, USA.Wiering, M. A. (2000). Multi-agent reinforcement learning traffic light control.Langley, P. (Ed.), Proceedings Seventeenth International Conference MachineLearning (ICML2000), pp. 11511158.Wierwille, W. W., Hanowski, R. J., Hankey, J. M., Kieliszewski, C. A., Lee, S. E., Medina,A., Keisler, A. S., & Dingus, T. A. (2002). Identification evaluation drivererrors: Overview recommendations. Tech. rep. FHWA-RD-02-003, Virginia TechTransportation Institute, Blacksburg, Virginia, USA. Sponsored Federal Highway Administration.Wu, S.-J., Chiang, H.-H., Perng, J.-W., Lee, T.-T., & Chen, C.-J. (2005). automatedlane-keeping design intelligent vehicle. Proceedings IEEE IntelligentVehicle Symposium (IV2005), pp. 508513, Las Vegas, NV, USA.656fiJournal Artificial Intelligence Research 31 (2008) 157-204Submitted 06/07; published 01/08Conjunctive Query Answering Description LogicSHIQBirte GlimmIan Horrocksbirte.glimm@comlab.ox.ac.ukian.horrocks@comlab.ox.ac.ukOxford University Computing Laboratory, UKCarsten Lutzclu@tcs.inf.tu-dresden.deDresden University Technology, GermanyUlrike Sattlersattler@cs.man.ac.ukUniversity Manchester, UKAbstractConjunctive queries play important role expressive query language Description Logics (DLs). Although modern DLs usually provide transitive roles, conjunctivequery answering DL knowledge bases poorly understood transitive rolesadmitted query. paper, consider unions conjunctive queries knowledge bases formulated prominent DL SHIQ allow transitive rolesquery knowledge base. show decidability query answering settingestablish two tight complexity bounds: regarding combined complexity, provedeterministic algorithm query answering needs time single exponentialsize KB double exponential size query, optimal.Regarding data complexity, prove containment co-NP.1. IntroductionDescription Logics (DLs) family logic based knowledge representation formalisms(Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003). DLs fragmentsFirst-Order Logic restricted unary binary predicates, called conceptsroles DLs. constructors building complex expressions usually chosenkey inference problems, concept satisfiability, decidable preferably lowcomputational complexity. DL knowledge base (KB) consists TBox, containsintensional knowledge concept definitions general background knowledge,ABox, contains extensional knowledge used describe individuals. Usingdatabase metaphor, TBox corresponds schema, ABox correspondsdata. contrast databases, however, DL knowledge bases adopt open worldsemantics, i.e., represent information domain incomplete way.Standard DL reasoning services include testing concepts satisfiability retrievingcertain instances given concept. latter retrieves, knowledge base consistingABox TBox , (ABox) individuals instances given (possiblycomplex) concept expression C, i.e., individuals entailinstance C. underlying reasoning problems well-understood, knowncombined complexity reasoning problems, i.e., complexity measuredsize TBox, ABox, query, ExpTime-complete SHIQ (Tobies,c2008AI Access Foundation. rights reserved.fiGlimm, Horrocks, Lutz, & Sattler2001). data complexity reasoning problem measured size ABoxonly. Whenever TBox query small compared ABox, oftencase practice, data complexity gives useful performance estimate. SHIQ,instance retrieval known data complete co-NP (Hustadt, Motik, & Sattler,2005).Despite high worst case complexity standard reasoning problemsexpressive DLs SHIQ, highly optimized implementations available, e.g.,FaCT++ (Tsarkov & Horrocks, 2006), KAON21 , Pellet (Sirin, Parsia, Cuenca Grau, Kalyanpur, & Katz, 2006), RacerPro2 . systems used wide range applications,e.g., configuration (McGuinness & Wright, 1998), bio informatics (Wolstencroft, Brass,Horrocks, Lord, Sattler, Turi, & Stevens, 2005), information integration (Calvanese,De Giacomo, Lenzerini, Nardi, & Rosati, 1998b). prominently, DLs knownuse logical underpinning ontology languages, e.g., OIL, DAML+OIL,OWL (Horrocks, Patel-Schneider, & van Harmelen, 2003), W3C recommendation (Bechhofer, van Harmelen, Hendler, Horrocks, McGuinness, Patel-Schneider, & Stein,2004).data-intensive applications, querying KBs plays central role. Instance retrievalis, aspects, rather weak form querying: although possibly complex conceptexpressions used queries, query tree-like relational structures, i.e.,DL concept cannot express arbitrary cyclic structures. property knowntree model property considered important reason decidabilityModal Description Logics (Gradel, 2001; Vardi, 1997). Conjunctive queries (CQs)well known database community constitute expressive query languagecapabilities go well beyond standard instance retrieval. example, considerknowledge base contains ABox assertion (hasSon.(hasDaughter.>))(Mary),informally states individual (or constant FOL terms) Mary sondaughter; hence, Mary grandmother. Additionally, assumeroles hasSon hasDaughter transitive super-role hasDescendant. implies Mary related via role hasDescendant (anonymous) grandchild.knowledge base, Mary clearly answer conjunctive query hasSon(x, y)hasDaughter(y, z) hasDescendant(x, z), assume x distinguished variable(also called answer free variable) y, z non-distinguished (existentially quantified)variables.variables query non-distinguished, query answer true falsequery called Boolean query. Given knowledge base K Boolean CQ q,query entailment problem deciding whether q true false w.r.t. K. CQ containsdistinguished variables, answers query tuples individual namesknowledge base entails query obtained replacing free variablesindividual names answer tuple. problem finding answer tuplesknown query answering. Since query entailment decision problem thus bettersuited complexity analysis query answering, concentrate query entailment.restriction since query answering easily reduced query entailmentillustrate detail Section 2.2.1. http://kaon2.semanticweb.org2. http://www.racer-systems.com158fiConjunctive Query Answering DL SHIQDevising decision procedure conjunctive query entailment expressive DLsSHIQ challenging problem, particular transitive roles admitted query(Glimm, Horrocks, & Sattler, 2006). conference version paper, presentedfirst decision procedure conjunctive query entailment SHIQ. paper,generalize result unions conjunctive queries (UCQs) SHIQ knowledge bases.achieve rewriting conjunctive query set conjunctive queriesresulting query either tree-shaped (i.e., expressed concept) grounded(i.e., contains constants/individual names variables). entailmenttypes queries reduced standard reasoning problems (Horrocks & Tessaris, 2000;Calvanese, De Giacomo, & Lenzerini, 1998a).paper organized follows: Section 2, give necessary definitions, followeddiscussion related work Section 3. Section 4, motivate query rewritingsteps means example. Section 5, give formal definitions rewritingprocedure show Boolean query indeed entailed knowledge base K iffdisjunction rewritten queries entailed K. Section 6, present deterministicalgorithm UCQ entailment SHIQ runs time single exponential sizeknowledge base double exponential size query. Since combinedcomplexity conjunctive query entailment already 2ExpTime-hard DL ALCI(Lutz, 2007), follows problem 2ExpTime-complete SHIQ. showsconjunctive query entailment SHIQ strictly harder instance checking,also case simpler DLs EL (Rosati, 2007b). show(the decision problem corresponding to) conjunctive query answering SHIQ co-NPcomplete regarding data complexity, thus harder instance retrieval.presented decision procedure gives insight query answering; alsoimmediate consequence field extending DL knowledge bases rules.work Rosati (2006a, Thm. 11), consistency SHIQ knowledge base extended(weakly-safe) Datalog rules decidable iff entailment unions conjunctivequeries SHIQ decidable. Hence, close open problem well.paper extended version conference paper: Conjunctive Query Answering Description Logic SHIQ. Proceedings Twentieth International JointConference Artificial Intelligence (IJCAI07), Jan 06 - 12, 2007.2. Preliminariesintroduce basic terms notations used throughout paper. particular,introduce DL SHIQ (Horrocks, Sattler, & Tobies, 2000) (unions of) conjunctivequeries.2.1 Syntax Semantics SHIQLet NC , NR , NI countably infinite sets concept names, role names, individualnames. assume set role names contains subset NtR NR transitive rolenames. role element NR {r | r NR }, roles form r calledinverse roles. role inclusion form r v r, roles. role hierarchy Rfinite set role inclusions.159fiGlimm, Horrocks, Lutz, & Sattlerinterpretation = (I ,I ) consists non-empty set , domain I,function , maps every concept name subset AI , every role name r NRbinary relation rI , every role name r NtR transitive binary relationrI , every individual name element aI . interpretationsatisfies role inclusion r v rI sI role hierarchy R satisfies roleinclusions R.use following standard notation:1. define function Inv roles Inv(r) := r r NR Inv(r) :=r = role name s.2. role hierarchy R, define v* R reflexive transitive closure vR {Inv(r) v Inv(s) | r v R}. use r R abbreviation r v* Rsv* R r.3. role hierarchy R role s, define set TransR transitive roles{s | role r r R r NtR Inv(r) NtR }.4. role r called simple w.r.t. role hierarchy R if, role v* R r,/ TransR .subscript R v* R TransR dropped clear context. set SHIQconcepts (or concepts short) smallest set built inductively NC usingfollowing grammar, NC , n IN, r role simple role:C ::= > | | | C | C1 u C2 | C1 C2 | r.C | r.C |6 n s.C |> n s.C.Given interpretation I, semantics SHIQ-concepts defined follows:>I(r.C)I(r.C)I(6 n s.C)I(> n s.C)I======(C u D)I = C DI(C)I = \ C(C D)I = C DI{d | (d, d0 ) rI , d0 C }{d | (d, d0 ) rI d0 C }{d | ](sI (d, C)) n}{d | ](sI (d, C)) n}](M ) denotes cardinality set sI (d, C) defined{d0 | (d, d0 ) sI d0 C }.general concept inclusion (GCI) expression C v D, Cconcepts. finite set GCIs called TBox. interpretation satisfies GCI C vC DI , TBox satisfies GCI ..(ABox) assertion expression form C(a), r(a, b), r(a, b), =6 b,C concept, r role, a, b NI . ABox finite set assertions. use Inds(A)denote set individual names occurring A. interpretation satisfies assertion.C(a) aI C , r(a, b) (aI , bI ) rI , r(a, b) (aI , bI )/ rI , =6 b aI 6= bI .160fiConjunctive Query Answering DL SHIQinterpretation satisfies ABox satisfies assertion A, denote|= A.knowledge base (KB) triple (T , R, A) TBox, R role hierarchy,ABox. Let K = (T , R, A) KB = (I ,I ) interpretation. saysatisfies K satisfies , R, A. case, say model K write|= K. say K consistent K model.2.1.1 Extending SHIQ SHIQufollowing section, show reduce conjunctive query set groundtree-shaped conjunctive queries. reduction, may introduce conceptscontain intersection roles existential quantification. define, therefore,extension SHIQ role conjunction/intersection, denoted SHIQu and,appendix, show decide consistency SHIQu knowledge bases.addition constructors introduced SHIQ, SHIQu allows conceptsformC ::= R.C | R.C |6 n S.C |> n S.C,R := r1 u . . . u rn , := s1 u . . . u sn , r1 , . . . , rn roles, s1 , . . . , sn simpleroles. interpretation function extended (r1 u . . . u rn )I = r1 . . . rn .2.2 Conjunctive Queries Unions Conjunctive Queriesintroduce Boolean conjunctive queries since basic form queriesconcerned with. later also define non-Boolean queries showreduced Boolean queries. Finally, unions conjunctive queries disjunctionconjunctive queries.simplicity, write conjunctive query set instead conjunction atoms.example, write introductory example Section 1{hasSon(x, y), hasDaughter(y, z), hasDescendant(x, z)}.non-Boolean queries, i.e., consider problem query answering,answer variables often given head query, e.g.,(x1 , x2 , x3 ) {hasSon(x1 , x2 ), hasDaughter(x2 , x3 ), hasDescendant(x1 , x3 )}indicates query answers tuples (a1 , a2 , a3 ) individual names that,substituted x1 , x2 , x3 respectively, result Boolean query entailedknowledge base. simplicity since mainly focus query entailment,use query head even case non-Boolean query. Instead, explicitly sayvariables answer variables ones existentially quantified. givedefinition Boolean conjunctive queries.Definition 1. Let NV countably infinite set variables disjoint NC , NR , NI .term element NV NI . Let C concept, r role, t, t0 terms. atomexpression C(t), r(t, t0 ), t0 refer three different types atomsconcept atoms, role atoms, equality atoms respectively. Boolean conjunctive query161fiGlimm, Horrocks, Lutz, & Sattlerq non-empty set atoms. use Vars(q) denote set (existentially quantified)variables occurring q, Inds(q) denote set individual names occurring q,Terms(q) set terms q, Terms(q) = Vars(q) Inds(q). terms qindividual names, say q ground. sub-query q simply subset q(including q itself). usual, use ](q) denote cardinality q, simplynumber atoms q, use |q| size q, i.e., number symbols necessarywrite q. SHIQ conjunctive query conjunctive query concepts Coccur concept atom C(t) SHIQ-concepts.Since equality reflexive, symmetric transitive, define * transitive,reflexive, symmetric closure terms q. Hence, relation *equivalence relation terms q and, Terms(q), use [t] denoteequivalence class * .Let = (I ,I ) interpretation. total function : Terms(q) evaluation (i) (a) = aI individual name Inds(q) (ii) (t) = (t0 ) t* t0 .write|= C(t) (t) C ;|= r(t, t0 ) ((t), (t0 )) rI ;|= t0 (t) = (t0 ).If, evaluation , |= atoms q, write |= q. saysatisfies q write |= q exists evaluation |= q. callmatch q I.Let K SHIQ knowledge base q conjunctive query. |= K implies |= q,say K entails q write K |= q.4query entailment problem defined follows: given knowledge base Kquery q, decide whether K |= q.atoms q follows:brevity simplicity notation, define relation0* 0qC(t) q term Terms(q) C(t0 ) q, r(t1 , t2 )000000* 0* 0terms t1 , t2 Terms(q) t1 t1 , t2 t2 , r(t1 , t2 ) q Inv(r)(t2 , t1 ) q.clearly justified definition semantics, particular, |= r(t, t0 )implies |= Inv(r)(t0 , t).devising decision procedure CQ entailment, complications arisecyclic queries (Calvanese et al., 1998a; Chekuri & Rajaraman, 1997). context,say cyclic, mean graph structure induced query cyclic, i.e., graphobtained q term considered node role atom inducesedge. Since, presence inverse roles, query containing role atom r(t, t0 )equivalent query obtained replacing atom Inv(r)(t0 , t), directionedges important say query cyclic underlying undirectedgraph structure cyclic. Please note also multiple role atoms two termsconsidered cycle, e.g., query {r(t, t0 ), s(t, t0 )} cyclic query. followingformal definition property.Definition 2. query q cyclic exists sequence terms t1 , . . . , tn n > 3162fiConjunctive Query Answering DL SHIQq,1. 1 < n, exists role atom ri (ti , ti+1 )2. t1 = tn ,3. ti 6= tj 1 < j < n.4definition, Item 3 makes sure consider queries cycliccontain two terms t, t0 two role atoms usinghere, implicitly uses relationtwo terms. Please note use relation*abstracts directedness role atoms.q s(t1 , t2 ), . . . , s(tn1 , tn )following, write replace r(t, t0 )0= t1 = tn , mean first remove occurrences r(t, t0 ) Inv(r)(t0 , t)* 0t* t0q, add atoms s(t1 , t2 ), . . . , s(tn1 , tn ) q.W.l.o.g., assume queries connected. precisely, let q conjunctivequery. say q connected if, t, t0 Terms(q), exists sequence t1 , . . . , tnq.t1 = t, tn = t0 and, 1 < n, exists role r r(ti , ti+1 )collection q1 , . . . , qn queries partitioning q q = q1 . . . qn , qi qj =1 < j n, qi connected.Lemma 3. Let K knowledge base, q conjunctive query, q1 , . . . , qn partitioningq. K |= q iff K |= qi 1 n.proof given Tessaris (2001, 7.3.2) and, lemma, clearrestriction connected queries indeed w.l.o.g. since entailment q decidedchecking entailment qi time. follows, therefore assume queriesconnected without notice.Definition 4. union Boolean conjunctive queries formula q1 . . . qn ,disjunct qi Boolean conjunctive query.knowledge base K entails union Boolean conjunctive queries q1 . . . qn , writtenK |= q1 . . . qn , if, interpretation |= K,|= qi 1 n.4W.l.o.g. assume variable names disjunct differentvariable names disjuncts. always achieved naming variablesapart. assume disjunct connected conjunctive query.w.l.o.g. since UCQ contains unconnected disjuncts always transformedconjunctive normal form; decide entailment resulting conjunctseparately conjunct union connected conjunctive queries. describetransformation detail and, convenient notation, writeconjunctive query {at1 , . . . , atk } at1 . . . atk following proof, instead usualset notation.Lemma 5. Let K knowledge base, q = q1 . . . qn union conjunctive queriesthat, 1 n, qi1 , . . . , qiki partitioning conjunctive query qi . K |= q iff^(q1i1 . . . qnin ).K |=(i1 ,...,in ){1,...,k1 }...{1,...,kn }163fiGlimm, Horrocks, Lutz, & SattlerAgain, detailed proof given Tessaris (2001, 7.3.3). Please note that, duetransformation conjunctive normal form, resulting number unions connectedconjunctive queries test entailment exponential sizeoriginal query. analysing complexity decision procedures presentedSection 6, show assumption CQ UCQ connectedincrease complexity.make connection query entailment query answering clearer.query answering, let variables conjunctive query typed: variable eitherexistentially quantified (also called non-distinguished ) free (also called distinguishedanswer variables). Let q query n variables (i.e., ](Vars(q)) = n), v1 , . . . , vm(m n) answer variables. answers K = (T , R, A) q m-tuples(a1 , . . . , ) Inds(A)m that, models K, |= q satisfies(vi ) = ai 1 m. hard see answers K qcomputed testing, (a1 , . . . , ) Inds(A)m , whether query q[v1 ,...,vm /a1 ,...,am ]obtained q replacing occurrence vi ai 1 entailed K.answer q set m-tuples (a1 , . . . , ) K |= q[v1 ,...,vm /a1 ,...,am ] .Let k = ](Inds(A)) number individual names used ABox A. Since finite,clearly k finite. Hence, deciding tuples belong set answers checkedk entailment tests. clearly efficient, optimizationsused, e.g., identify (hopefully small) set candidate tuples.algorithm present Section 6 decides query entailment. reasonsdevising decision procedure query entailment instead query answering twofold: first, query answering reduced query entailment shown above; second,contrast query answering, query entailment decision problem studiedterms complexity theory.remainder paper, stated otherwise, use q (possibly subscripts)connected Boolean conjunctive query, K SHIQ knowledge base (T , R, A),interpretation (I ,I ), evaluation.3. Related Workrecently, automata-based decision procedure positive existential path queriesALCQIbreg knowledge bases presented (Calvanese, Eiter, & Ortiz, 2007).Positive existential path queries generalize unions conjunctive queries since SHIQknowledge base polynomially reduced ALCQIbreg knowledge base, presented algorithm decision procedure (union of) conjunctive query entailmentSHIQ well. automata-based technique considered elegantrewriting algorithm, give NP upper bound data complexitytechnique.existing algorithms conjunctive query answering expressive DLs assume,however, role atoms conjunctive queries use roles transitive.consequence, example query introductory section cannot answered.restriction, decision procedures various DLs around SHIQ known (Horrocks &Tessaris, 2000; Ortiz, Calvanese, & Eiter, 2006b), known answering conjunctivequeries setting data complete co-NP (Ortiz et al., 2006b). Another common164fiConjunctive Query Answering DL SHIQrestriction individuals named ABox considered assignmentsvariables. setting, semantics queries longer standard First-Orderone. restriction, answer example query introduction wouldfalse since Mary named individual. hard see conjunctive queryanswering restriction reduced standard instance retrieval replacingvariables individual names ABox testing entailmentconjunct separately. implemented DL reasoners, e.g., KAON2, Pellet,RacerPro, provide interface conjunctive query answering settingemploy several optimizations improve performance (Sirin & Parsia, 2006; Motik,Sattler, & Studer, 2004; Wessel & Moller, 2005). Pellet appears reasoneralso supports standard First-Order semantics SHIQ conjunctive queriesrestriction queries acyclic.best knowledge, still open problem whether conjunctive queryentailment decidable SHOIQ. Regarding undecidability results, knownconjunctive query entailment two variable fragment First-Order Logic L2 undecidable (Rosati, 2007a) Rosati identifies relatively small set constructorscauses undecidability.Query entailment answering also studied context databasesincomplete information (Rosati, 2006b; van der Meyden, 1998; Grahne, 1991).setting, DLs used schema languages, expressivity considered DLsmuch lower expressivity SHIQ. example, constructors providedlogics DL-Lite family (Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2007)chosen standard reasoning tasks PTime query entailmentLogSpace respect data complexity. Furthermore, TBox reasoningdone independently ABox ABox stored accessed using standarddatabase SQL engine. Since considered DLs considerable less expressive SHIQ,techniques used databases incomplete information cannot appliedsetting.Regarding query language, well known extension conjunctive queriesinequalities undecidable (Calvanese et al., 1998a). Recently,shown even DLs low expressivity, extension conjunctive queriesinequalities safe role negation leads undecidability (Rosati, 2007a).related reasoning problem query containment. Given schema (or TBox)two queries q q 0 , q contained q 0 w.r.t. iff every interpretationsatisfies q also satisfies q 0 . well known query containment w.r.t.TBox reduced deciding query entailment (unions of) conjunctive queries w.r.t.knowledge base (Calvanese et al., 1998a). Hence decision procedure (unions of)conjunctive queries SHIQ also used deciding query containment w.r.t.SHIQ TBox.Entailment unions conjunctive queries also closely related problemadding rules DL knowledge base, e.g., form Datalog rules. AugmentingDL KB arbitrary Datalog program easily leads undecidability (Levy & Rousset,1998). order ensure decidability, interaction Datalog rulesDL knowledge base usually restricted imposing safeness condition. DL+logframework (Rosati, 2006a) provides least restrictive integration proposed far. Rosati165fiGlimm, Horrocks, Lutz, & Sattlerpresents algorithm decides consistency DL+log knowledge base reducingproblem entailment unions conjunctive queries, proves decidabilityUCQs SHIQ implies decidability consistency SHIQ+log knowledge bases.4. Query Rewriting Examplesection, motivate ideas behind query rewriting technique meansexamples. following section, give precise definitions rewriting steps.4.1 Forest Bases Canonical Interpretationsmain idea focus models knowledge base kindtree forest shape. well known one reason Description Modal Logicsrobustly decidable enjoy form tree model property, i.e., everysatisfiable concept model tree-shaped (Vardi, 1997; Gradel, 2001). goingconcept satisfiability knowledge base consistency, need replace tree modelproperty form forest model property, i.e., every consistent KB modelconsists set trees, root corresponds named individual ABox.roots connected via arbitrary relational structures, induced role assertionsgiven ABox. forest model is, therefore, forest graph theoretic sense.Furthermore, transitive roles introduce short-cut edges elements withintree even elements different trees. Hence talk form forest modelproperty.define forest models show that, deciding query entailment,restrict attention forest models. rewriting steps used transform cyclicsubparts query tree-shaped ones forest-shaped matchrewritten query forest models.order make forest model property even clearer, also introduce forest bases,interpretations interpret transitive roles unrestricted way, i.e.,necessarily transitive way. forest base, require particular relationships elements domain inferred transitively closing roleomitted. following, assume ABox contains least one individual name,i.e., Inds(A) non-empty. w.l.o.g. since always add assertion >(a)ABox fresh individual name NI . readers familiar tableau algorithms,worth noting forest bases also thought tableaux generatedcomplete clash-free completion tree (Horrocks et al., 2000).Definition 6. Let denote non-negative integers set (finite) wordsalphabet IN. tree non-empty, prefix-closed subset . w, w0 ,call w0 successor w w0 = w c c IN, denotes concatenation.call w0 neighbor w w0 successor w vice versa. empty wordcalled root.forest base K interpretation J = (J ,J ) interprets transitive rolesunrestricted (i.e., necessarily transitive) way and, additionally, satisfies followingconditions:T1 J Inds(A) that, Inds(A), set {w | (a, w) J } tree;166fiConjunctive Query Answering DL SHIQT2 ((a, w), (a0 , w0 )) rJ , either w = w0 = = a0 w0 neighbor w;T3 Inds(A), aJ = (a, );interpretation canonical K exists forest base J Kidentical J except that, non-simple roles r,[(sJ )+rI = rJv* R r, sTransRcase, say J forest base |= K say canonicalmodel K.4convenience, extend notion successors neighbors elements canonical models. Let canonical model (a, w), (a0 , w0 ) . call (a0 , w0 )successor (a, w) either = a0 w0 = w c c w = w0 = . call(a0 , w0 ) neighbor (a, w) (a0 , w0 ) successor (a, w) vice versa.Please note definition implicitly relies unique name assumption(UNA) (cf. T3). w.l.o.g. guess appropriate partition among individual names replace individual names partition one representativeindividual name partition. Section 6, show partitioning individual names used simulate UNA, hence, decision procedure relyUNA. also show affect complexity.Lemma 7. Let K SHIQ knowledge base q = q1 . . . qn union conjunctivequeries. K 6|= q iff exists canonical model K 6|= q.detailed proof given appendix. Informally, direction,take arbitrary counter-model query, exists assumption, unravelnon-tree structures. Since, unraveling process, replace cyclesmodel infinite paths leave interpretation concepts unchanged, querystill satisfied unravelled canonical model. direction proof trivial.4.2 Running Exampleuse following Boolean query knowledge base running example:Example 8. Let K = (T , R, A) SHIQ knowledge base r, NtR , k= { Ck v > k p.>,C3 v > 3 p.>,D2 v .> u t.>}R = { v ,v r}= { r(a, b),(p.Ck u p.C u r .C3 )(a),(p.D1 u r.D2 )(b)}q = {r(u, x), r(x, y), t(y, y), s(z, y), r(u, z)} Inds(q) = Vars(q) = {u, x, y, z}.167fiGlimm, Horrocks, Lutz, & Sattlersimplicity, choose use CQ instead UCQ. case UCQ, rewritingsteps applied disjunct separately.r(a, )p(a, 1) Ckp(a, 11)(a, 12)p...prr(a, 2) Cp(a, 1k)rr(a, 31)p(a, 32)pD1 (b, 1)r(a, 3) C3p(b, )rp(a, 33)rt,(b, 2)r D2r,t,(b, 21)t,(b, 22)Figure 1: representation canonical interpretation K.Figure 1 shows representation canonical model knowledge base KExample 8. labeled node represents element domain, e.g., individualname represented node labeled (a, ). edges represent relationshipsindividuals. example, read r-labeled edge (a, ) (b, )directions, i.e., (aI , bI ) = ((a, ), (b, )) rI (bI , aI ) = ((b, ), (a, )) r .short-cuts due transitive roles shown dashed lines, relationshipnodes represent ABox individuals shown grey. Please noteindicate interpretations concepts figure.Since canonical model K, elements domain pairs (a, w),indicates individual name corresponds root tree, i.e., aI = (a, )elements second place form tree according definition trees.individual name ABox, can, therefore, easily define tree rooted{w | (a, w) }.(a, )p(a, 1)p(a, 11)(a, 12)p...p(a, 2)(a, 1k)rp(a, 3)pp(a, 31)(b, )rp(a, 32)(b, 1)p(a, 33)r(b, 2)r,t,(b, 21)(b, 22)Figure 2: forest base interpretation represented Figure 1.Figure 2 shows representation forest base interpretation Figure 1above. simplicity, interpretation concepts longer shown. two trees,rooted (a, ) (b, ) respectively, clear.graphical representation query q Example 8 shown Figure 3,meaning nodes edges analogous ones given interpretations.call query cyclic query since underlying undirected graph cyclic (cf. Definition 2).Figure 4 shows match q and, although consider one canonicalmodel here, hard see query true model knowledge base,i.e., K |= q.168fiConjunctive Query Answering DL SHIQxrrurzFigure 3: graph representation query Example 8.(a, )r(a, 1)(a, 11)(a, 12)...(a, 2)(a, 1k)(a, 31)x(b, )rr(a, 3)(a, 32)u(a, 33)rrrt,r(b, 1)rr(b, 2)r,t,z(b, 21)t,(b, 22)Figure 4: match query q Example 8 onto model Figure 1.forest model property also exploited query rewriting process. wantrewrite q set queries q1 , . . . , qn ground tree-shaped queries K |= qiff K |= q1 . . . qn . Since resulting queries ground tree-shaped queries,explore known techniques deciding entailment queries. first step,transform q set forest-shaped queries. Intuitively, forest-shaped queries consistset tree-shaped sub-queries, roots trees might arbitrarilyinterconnected (by atoms form r(t, t0 )). tree-shaped query special caseforest-shaped query. call arbitrarily interconnected terms forest-shapedquery root choice (or, short, roots). end rewriting process,replace roots individual names Inds(A) transform tree partsconcept applying called rolling-up tuple graph technique (Tessaris, 2001;Calvanese et al., 1998a).proof correctness procedure, use structure forest basesorder explicate transitive short-cuts used query match. explicatingmean replace role atom mapped short-cut sequencerole atoms extended match modified query uses pathsforest base.4.3 Rewriting Stepsrewriting process query q six stage process. end process,rewritten query may may forest shape. show later, dont knownon-determinism compromise correctness algorithm. first stage,derive collapsing qco q adding (possibly several) equality atoms q. Consider,169fiGlimm, Horrocks, Lutz, & Sattlerexample, cyclic query q = {r(x, y), r(x, 0 ), s(y, z), s(y 0 , z)} (see Figure 5),transformed tree-shaped one adding equality atom 0 .xxrrry, 0y0zzFigure 5: representation cyclic query tree-shaped query obtained addingatom 0 query depicted left hand side.common property next three rewriting steps allow substitutingimplicit short-cut edges explicit paths induce short-cut. three stepsaim different cases short-cuts occur describe goalsapplication detail:second stage called split rewriting. split rewriting take care roleatoms matched transitive short-cuts connecting elements two different treesby-passing one roots. substitute short-cuts either onetwo role atoms roots included. running example, maps u (a, 3)x (b, ). Hence |= r(u, x), used r-edge transitive short-cut connectingtree rooted tree rooted b, by-passing (a, ). Similar arguments holdatom r(u, z), path implies short-cut relationship goes viatwo roots (a, ) (b, ). clear r must non-simple role since, forestbase J I, direct connection different treesroots trees. Hence, ((u), (x)) rI holds role TransRv* R r. case example, r transitive. split rewriting eliminatestransitive short-cuts different trees canonical model adds missingvariables role atoms matching sequence edges induce short-cut.uxrrxrruzFigure 6: split rewriting qsr query shown Figure 3.Figure 6 depicts split rewritingqsr = { r(u, ux), r(ux, x), r(x, y), t(y, y), s(z, y),r(u, ux), r(ux, x), r(x, z)}170fiConjunctive Query Answering DL SHIQq obtained q replacing (i) r(u, x) r(u, ux) r(ux, x) (ii) r(u, z)r(u, ux), r(ux, x), r(x, z). Please note introduced new variable (ux)re-used existing variable (x). Figure 7 shows match qsr canonical modelK two trees connected via roots. rewritten query,also guess set roots, contains variables mapped rootscanonical model. running example, guess set roots {ux, x}.(a, )uxx(b, )rr(a, 1)r(a, 3)(a, 2)(b, 1)u,(b, 2)rs, r(a, 11)(a, 12)...(a, 1k)(a, 31)(a, 32)(a, 33)z(b, 21)(b, 22)Figure 7: split match sr query qsr Figure 6 onto canonical interpretationFigure 1.third step, called loop rewriting, eliminate loops variables vcorrespond roots replacing atoms r(v, v) two atom r(v, v 0 ) r(v 0 , v), v 0either new existing variable q. running example, eliminateloop t(y, y) follows:q`r = { r(u, ux), r(ux, x), r(x, y), t(y, 0 ), t(y 0 , y), s(z, y),r(u, ux), r(ux, x), r(x, z)}query obtained qsr (see Figure 6) replacing t(y, y) t(y, 0 ) t(y 0 , y)new variable 0 . Please note that, since defined transitive symmetric, t(y, y)still implied, i.e., loop also transitive short-cut. Figure 8 shows canonicalinterpretation Figure 1 match `r q`r . introduction new variable0 needed case since variable could re-used individual(b, 22) range match sr .(a, )uxx(b, )rr(a, 1)(a, 11)(a, 12)...(a, 2)(a, 1k)(a, 31)r(a, 3)(a, 32)u(a, 33)(b, 1)(b, 2)rs, rt,z(b, 21)(b, 22)y0Figure 8: loop rewriting q`r match canonical interpretation Figure 1.forth rewriting step, called forest rewriting, allows replacement roleatoms sets role atoms. allows elimination cycles within single171fiGlimm, Horrocks, Lutz, & Sattlertree. forest rewriting qf r example obtained q`r replacing roleatom r(x, z) r(x, y) r(y, z), resulting queryqf r = { r(u, ux), r(ux, x), r(x, y), t(y, 0 ), t(y 0 , y), s(z, y),r(u, ux), r(ux, x), r(x, y), r(y, z)}.Clearly, results tree-shaped sub-queries, one rooted ux one rooted x.Hence qf r forest-shaped w.r.t. root terms ux x. Figure 9 shows canonicalinterpretation Figure 1 match f r qf r .(a, )uxx(b, )rr(a, 1)(a, 2)r(a, 3)u(b, 1)(b, 2)r,t,(a, 11)(a, 12)...(a, 1k)(a, 31)(a, 32)(a, 33)z(b, 21)(b, 22)y0Figure 9: forest rewriting qf r forest match f r canonical interpretationFigure 1.fifth step, use standard rolling-up technique (Horrocks & Tessaris, 2000;Calvanese et al., 1998a) express tree-shaped sub-queries concepts. orderthis, traverse tree bottom-up fashion replace leaf (labeledconcept C, say) incoming edge (labeled role r, say) concept r.Cadded predecessor. example, tree rooted ux (i.e., role atom r(u, ux))replaced atom (r .>)(ux). Similarly, tree rooted x (i.e., roleatoms r(x, y), r(y, z), s(z, y), t(y, 0 ), t(y 0 , y)) replaced atom(r.(((r u Inv(s)).>) u ((t u Inv(t)).>))(x).Please note use role conjunctions resulting query order capturesemantics multiple role atoms relating pair variables.Recall that, split rewriting, guessed x ux correspond roots and,therefore, correspond individual names Inds(A). sixth last rewriting step,guess variable corresponds individual name replace variablesguessed names. possible guess running example would ux correspondsx b. results (ground) query{(r .>)(a), r(a, b), (r.(((r u Inv(s)).>) u ((t u Inv(t)).>)))(b)},entailed K.Please note focused running example reasonable rewriting.several possible rewritings, e.g., obtain another rewriting qf rreplacing ux b x last step. UCQ, apply rewriting stepsdisjuncts separately.172fiConjunctive Query Answering DL SHIQend rewriting process, have, disjunct, set ground queriesand/or queries rolled-up single concept atom. latter queries resultforest rewritings tree-shaped empty set roots. tree-shapedrewritings match anywhere tree can, thus, grounded. Finally, checkknowledge base entails disjunction rewritten queries. showbound number (forest-shaped) rewritings hence numberqueries produced rewriting process.Summing up, rewriting process connected conjunctive query q involvesfollowing steps:1. Build collapsings q.2. Build split rewritings collapsing w.r.t. subset R roots.3. Build loop rewritings split rewritings.4. Build (forest-shaped) forest rewritings loop rewritings.5. Roll tree-shaped sub-query forest-rewriting concept atom6. replace roots R individual names ABox possible ways.Let q1 , . . . , qn queries resulting rewriting process. next section,define rewriting step prove K |= q iff K |= q1 qn . Checking entailmentrewritten queries easily reduced KB consistency decision procedureSHIQu KB consistency could used order decide K |= q. present onedecision procedure Section 6.5. Query Rewritingprevious section, used several terms, e.g., tree- forest-shaped query,rather informally. following, give definitions terms used queryrewriting process. done, formalize query rewriting steps provecorrectness procedure, i.e., show forest-shaped queries obtainedrewriting process indeed used deciding whether knowledge base entailsoriginal query. give detailed proofs here, rather intuitions behindproofs. Proofs full detail given appendix.5.1 Tree- Forest-Shaped Queriesorder define tree- forest-shaped queries precisely, use mappingsqueries trees forests. Instead mapping equivalence classes terms * nodestree, extend well-known properties functions follows:Definition 9. mapping f : B, use dom(f ) ran(f ) denote f domain*range B, respectively. Given equivalence relationdom(f ), say f00*injective moduloif, a, dom(f ), f (a) = f (a ) implies a* a0 say f*bijective modulof injective modulo * surjective. Let q query. treemapping q total function f terms q tree173fiGlimm, Horrocks, Lutz, & Sattler*1. f bijective modulo,q, f (t) neighbor f (t0 ), and,2. r(t, t0 )3. Inds(q), f (a) = .query q tree-shaped ](Inds(q)) 1 tree mapping q.root choice R q subset Terms(q) Inds(q) R and, R* 0, t0 R. R, use Reach(t) denote set terms t0 Terms(q)exists sequence terms t1 , . . . , tn Terms(q)1. t1 = tn = t0 ,q, and,2. 1 < n, role r r(ti , ti+1 )3. 1 < n, ti R, ti* t.call R root splitting w.r.t. q either R = if, ti , tj R, ti 6 * tj impliesReach(ti ) Reach(tj ) = . term R induces sub-queryq | terms occur Reach(t)}\subq(q, t) := {atq}.{r(t, t) | r(t, t)query q forest-shaped w.r.t. root splitting R either R = q tree-shapedsub-query subq(q, t) R tree-shaped.4term R, collect terms reachable set Reach(t).Condition 3, make sure R * t0 Reach(t) eitherR t* t0 . Since queries connected assumption, would otherwise collect termsReach(t) t0/ R. root splitting, require resulting setsmutually disjoint terms t, t0 R equivalent. guaranteespaths sub-queries go via root nodes respective trees. Intuitively,forest-shaped query one potentially mapped onto canonical interpretation= (I ,I ) terms root splitting R correspond roots (a, ) .q, partsdefinition subq(q, t), exclude loops form r(t, t)query grounded later query rewriting process ground terms,allow arbitrary relationships.Consider, example, query qsr running example previous section(cf. Figure 6). Let us make root choice R := {ux, x} q. sets Reach(ux)Reach(x) w.r.t. qsr R {ux, u} {x, y, z} respectively. Since setsdisjoint, R root splitting w.r.t. qsr . choose, however, R := {x, y}, set Rroot splitting w.r.t. qsr since Reach(x) = {ux, u, z} Reach(y) = {z} disjoint.5.2 Graphs Forestsready define query rewriting steps. Given arbitrary query, exhaustively apply rewriting steps show use resulting queriesforest-shaped deciding entailment original query. Please note followingdefinitions conjunctive queries unions conjunctive queries sinceapply rewriting steps disjunct separately.174fiConjunctive Query Answering DL SHIQDefinition 10. Let q Boolean conjunctive query. collapsing qco q obtainedadding zero equality atoms form t0 t, t0 Terms(q) q. use co(q)denote set queries collapsing q.Let K SHIQ knowledge base. query qsr called split rewriting q w.r.t. Kq, either:obtained q choosing, atom r(t, t0 )1. nothing,2. choose role TransR v* R r replace r(t, t0 ) s(t, u), s(u, t0 ),3. choose role TransR v* R r replace r(t, t0 ) s(t, u), s(u, u0 ),s(u0 , t0 ),u, u0 NV possibly fresh variables. use srK (q) denote set pairs(qsr , R) query qco co(q) qsr split rewriting qco Rroot splitting w.r.t. qsr .query q`r called loop rewriting q w.r.t. root splitting R K obtainedqq choosing, atoms form r(t, t)/ R, role TransRv* R r replacing r(t, t) two atoms s(t, t0 ) s(t0 , t) t0 NV possiblyfresh variable. use lrK (q) denote set pairs (q`r , R) tuple(qsr , R) srK (q) q`r loop rewriting qsr w.r.t. R K.forest rewriting, fix set V NV variables occurring q](V ) ](Vars(q)). forest rewriting qf r w.r.t. root splitting R q K obtainedqq choosing, role atom r(t, t0 ) either R = r(t, t0 )subq(q, tr ) eithertr R r(t, t0 )1. nothing,2. choose role TransR v* R r replace r(t, t0 ) ` ](Vars(q)) roleatoms s(t1 , t2 ), . . . , s(t` , t`+1 ), t1 = t, t`+1 = t0 , t2 , . . . , t` Vars(q) V .use frK (q) denote set pairs (qf r , R) tuple (q`r , R) lrK (q)qf r forest-shaped forest rewriting q`r w.r.t. R K.4K clear context, say q 0 split, loop, forest rewritingq instead saying q 0 split, loop, forest rewriting q w.r.t. K. assumesrK (q), lrK (q), frK (q) contain isomorphic queries, i.e., differences (newlyintroduced) variable names neglected.next section, show build disjunction conjunctive queriesq1 q` queries frK (q) qi 1 ` either formC(v) single variable v Vars(qi ) qi ground, i.e., qi contains constantsvariables. remains show K |= q iff K |= q1 q` .5.3 Trees Conceptsorder transform tree-shaped query single concept atom forest-shapedquery ground query, define mapping f terms tree-shaped subquery tree. incrementally build concept corresponds tree-shapedquery traversing tree bottom-up fashion, i.e., leaves upwardsroot.175fiGlimm, Horrocks, Lutz, & SattlerDefinition 11. Let q tree-shaped query one individual name.Inds(q), let tr = otherwise let tr = v variable v Vars(q). Let f treemapping f (tr ) = . inductively assign, term Terms(q),concept con(q, t) follows:f (t) leaf ran(f ), con(q, t) := C(t) q C,f (t) successors f (t1 ), . . . , f (tk ),con(q, t) :=Cuq r .con(q, ti ).r(t,ti )1ikqC(t)Finally, query concept q w.r.t. tr con(q, tr ).4Please note definition takes equality atoms account.*function f bijective moduloand, case concept atoms C(t) C(t0 ).t* t0 , concepts conjoined query concept due use relationSimilar arguments applied role atoms.following lemma shows query concepts indeed capture semantics q.Lemma 12. Let q tree-shaped query tr Terms(q) defined above, Cq =con(q, tr ), interpretation. |= q iff match elementCq (tr ) = d.proof given Horrocks, Sattler, Tessaris, Tobies (1999) easily transfersDLR SHIQ. applying result lemma, transformforest-shaped query ground query follows:Definition 13. Let (qf r , R) frK (q) R 6= , : R Inds(A) total functionthat, Inds(q), (a) = and, t, t0 R, (t) = (t0 ) iff t* t0 . callmapping ground mapping R w.r.t. A. obtain ground query ground(qf r , R, )qf r w.r.t. root splitting R ground mapping follows:replace R (t), and,ran( ), replace sub-query qa = subq(qf r , a) con(qa , a).define set groundK (q) ground queries q w.r.t. K follows:groundK (q) := {q 0 | exists (qf r , R) frK (q) R 6=ground mapping w.r.t. Rq 0 = ground(qf r , R, )}define set treesK (q) tree queries q follows:treesK (q) := {q 0 | exists (qf r , ) frK (q)v Vars(qf r ) q 0 = (con(qf r , v))(v)}1764fiConjunctive Query Answering DL SHIQGoing back running example, already seen (qf r , {ux, x}) belongsset frK (q)qf r = {r(u, ux), r(ux, x), r(x, y), t(y, 0 ), t(y 0 , y), s(z, y), r(y, z)}.also several queries set frK (q), e.g., (q, {u, x, y, z}), qoriginal query root splitting R R = Terms(q), i.e., termsroot choice q. order build set groundK (q), build possible groundmappings set Inds(A) individual names ABox root splittingsqueries frK (q). tuple (qf r , {ux, x}) frK (q) contributes two ground queriesset groundK (q):ground(qf r , {ux, x}, {ux 7 a, x 7 b}) ={r(a, b), (Inv(r).>)(a), (r.(((r u Inv(s)).>) u ((t u Inv(t)).>)))(b)},Inv(r).> query concept (tree-shaped) sub-query subq(qf r , ux)r.(((r u Inv(s)).>) u ((t u Inv(t)).>) query concept subq(qf r , x)ground(qf r , {ux, x}, {ux 7 b, x 7 a}) ={r(b, a), (Inv(r).>)(b), (r.(((r u Inv(s)).>) u ((t u Inv(t)).>)))(a)}.tuple (q, {u, x, y, z}) frK (q), however, contribute ground query since,ground mapping, require (t) = (t0 ) iff t* t0 two individualnames Inds(A) compared four terms q need distinct value. Intuitively,restriction, since first rewriting step (collapsing) produce queriesterms q identified possible ways.example, K |= q K |= q1 q` , q1 q` queries treesK (q)groundK (q) since model K satisfies qi = ground(qf r , {ux, x}, {ux 7 a, x 7 b}).5.4 Query MatchesEven query true canonical model, necessarily mean querytree- forest-shaped. However, match canonical interpretation guideprocess rewriting query. Similarly definition tree- forest-shaped queries,define shape matches query. particular, introduce three different kindsmatches: split matches, forest matches, tree matches every tree matchforest match, every forest match split match. correspondence queryshapes follows: given split match , set root nodes (a, ) rangematch define root splitting query, additionally forest match,query forest-shaped w.r.t. root splitting induced , additionally treematch, whole query mapped single tree (i.e., query tree-shapedforest-shaped w.r.t. empty root splitting). Given arbitrary query matchcanonical model, first obtain split match tree forest match, usingstructure canonical model guiding application rewriting steps.Definition 14. Let K SHIQ knowledge base, q query, = (I ,I ) canonicalmodel K, : Terms(q) evaluation |= q. call split matchq, one following holds:if, r(t, t0 )177fiGlimm, Horrocks, Lutz, & Sattler1. (t) = (a, ) (t0 ) = (b, ) a, b Inds(A);2. (t) = (a, w) (t0 ) = (a, w0 ) Inds(A) w, w0 .call forest match if, additionally, term tr Terms(q) (tr ) = (a, )Inds(A), total bijective mapping f {(a, w) | (a, w) ran()}subq(q, tr ) implies f ((t)) neighbor f ((t0 )).tree r(t, t0 )call tree match if, additionally, Inds(A) element ran()form (a, w).split match canonical interpretation induces (possibly empty) root splittingR R iff (t) = (a, ) Inds(A). call R root splitting induced.4two elements (a, w) (a, w0 ) canonical model, path (a, w) (a, w0 )sequence (a, w1 ), . . . , (a, wn ) w = w1 , w0 = wn , and, 1 < n, wi+1successor wi . length path n. Please note that, forest match,require w neighbor w0 vice versa. still allows map role atomspaths canonical model length greater two, paths mustancestors elements different branches tree. mapping ftree also makes sure R induced root splitting, sub-query subq(q, t)* 0R tree-shaped. tree match, root splitting either empty0*t, R, i.e., single root modulo , whole query tree-shaped.5.5 Correctness Query Rewritingfollowing lemmas state correctness rewriting step steprewriting stages. Full proofs given appendix. motivated previoussection, use given canonical model guide rewriting processobtain forest-shaped query also match model.Lemma 15. Let model K.1. |= q, collapsing qco q |=co qco co injection*modulo.2. |=co qco collapsing qco q, |= q.Given model satisfies q, simply add equality atoms pairs termsmapped element I. hard see resultsmapping injective modulo * . second part, easy see modelsatisfies collapsing also satisfies original query.Lemma 16. Let model K.1. canonical |= q, pair (qsr , R) srK (q) split matchsr |=sr qsr , R induced root splitting sr , sr injection*modulo.2. (qsr , R) srK (q) |=sr qsr match sr , |= q.178fiConjunctive Query Answering DL SHIQfirst part lemma, proceed exactly illustrated example sectionuse canonical model match guide rewriting steps. first buildcollapsing qco co(q) described proof Lemma 15 |=co qco co*injection modulo. Since canonical, paths different trees occurdue non-simple roles, thus replace role atom uses short-cuttwo three role atoms roots explicitly included query (cf.query match Figure 4 obtained split rewriting split matchFigure 7). second part lemma follows immediately fact usetransitive sub-roles replacement.Lemma 17. Let model K.1. canonical |= q, pair (q`r , R) lrK (q) mapping `r*|=`r q`r , `r injection modulo, R root splitting inducedq`r , R.`r and, r(t, t)2. (q`r , R) lrK (q) |=`r q`r match `r , |= q.second part straightforward, given use transitive sub-rolesloop rewriting. first part, proceed described examplessection use canonical model match guide rewriting process.first build split rewriting qsr root splitting R described proof Lemma 16(qsr , R) srK (q) |=sr qsr split match sr . Since canonicalmodel, forest base J . forest base, non-root nodes cannot successorsthemselves, loop short-cut due transitive role. elementis, say, r-related has, therefore, neighbor r- Inv(r)-successor.Depending whether neighbor already range match, eitherre-use existing variable introduce new one, making path explicit (cf.loop rewriting depicted Figure 8 obtained split rewriting shown Figure 7).Lemma 18. Let model K.1. canonical |= q, pair (qf r , R) frK (q) |=f r qf rforest match f r , R induced root splitting f r , f r injection*modulo.2. (qf r , R) frK (q) |=f r qf r match f r , |= q.main challenge proof (1) give short idea here.point, know Lemma 17 use query q`rroot splitting R split match `r . Since `r split match, matchsub-query restricted tree thus transform sub-query q`r inducedterm root choice separately. following example meant illustrategiven bound ](Vars(q)) number new variables role atomsintroduced forest rewriting suffices. Figure 10 depicts representation treecanonical model, use second part names elements, e.g.,use instead (a, ). simplicity, also indicate conceptsroles label nodes edges, respectively. use black color indicate nodes179fiGlimm, Horrocks, Lutz, & Sattleredges used match query dashed lines short-cuts duetransitive roles. example, grey edges also belong forest basequery match uses short-cuts.11112111Figure 10: part representation canonical model, black nodesedges used match query dashed edges indicate short-cuts duetransitive roles.forest rewriting aims making short-cuts explicit replacingedges necessary obtain tree match. order this, need includecommon ancestors forest base two nodes used match.w, w0 , therefore define longest common prefix (LCP) w w0 longestw w prefix w w0 . forest rewriting, determineLCPs two nodes range match add variable LCPsyet range match set V new variables used forestrewriting. example Figure 10 set V contains single variable v1node 1.explicate short-cuts follows: edge used match, e.g.,edge 111 example, define path sequence elementspath forest base, e.g., path edge 111 , 1, 11, 111. relevantpath obtained dropping elements path rangemapping correspond variable set V , resulting relevant path , 1, 111example. replace role atom matched edge 111two role atoms match uses edge 1 1 111.appropriate transitive sub-role exists since otherwise could short-cut. Similararguments used replace role atom mapped edge 111 12one mapped edge 12, resulting match representedFigure 11. given restriction cardinality set V limitation sincenumber LCPs set V maximal pair nodes oneancestor other. see nodes n leaf nodes tree leastbinarily branching. Since tree n inner nodes, need nnew variables query n variables.180fiConjunctive Query Answering DL SHIQ11112111Figure 11: match forest rewriting obtained example given Figure 10.bound number role atoms used replacementsingle role atom, consider, example, cyclic queryq = {r(x1 , x2 ), r(x2 , x3 ), r(x3 , x4 ), t(x1 , x4 )},knowledge base K = (T , R, A) = , R = {r v t} TransR= {(r.(r.(r.>)))(a)}. hard check K |= q. Similarly runningexample previous section, also single rewriting truecanonical model KB, obtained building forest rewritingnothing rewriting steps, except choosing empty set root splittingsplit rewriting step. forest rewriting, explicate short-cut usedmapping t(x1 , x4 ) replacing t(x1 , x4 ) t(x1 , x2 ), t(x2 , x3 ), t(x3 , x4 ).using Lemmas 15 18, get following theorem, shows useground queries groundK (q) queries treesK (q) order check whether Kentails q, well understood problem.Theorem 19. Let K SHIQ knowledge base, q Boolean conjunctive query,{q1 , . . . , q` } = treesK (q) groundK (q). K |= q iff K |= q1 . . . q` .give upper bounds size number queries treesK (q) groundK (q).before, use ](S) denote cardinality set S. size |K| (|q|) knowledgebase K (a query q) simply number symbols needed write alphabetconstructors, concept names, role names occur K (q), numbersencoded binary. Obviously, number atoms query bounded size, hence](q) |q| and, simplicity, use n size cardinality q follows.Lemma 20. Let q Boolean conjunctive query, K = (T , R, A) SHIQ knowledge base,|q| := n |K| := m. polynomial p1. ](co(q)) 2p(n) and, q 0 co(q), |q 0 | p(n),2. ](srK (q)) 2p(n)log p(m) , and, q 0 srK (q), |q 0 | p(n),3. ](lrK (q)) 2p(n)log p(m) , and, q 0 lrK (q), |q 0 | p(n),181fiGlimm, Horrocks, Lutz, & Sattler4. ](frK (q)) 2p(n)log p(m) , and, q 0 frK (q), |q 0 | p(n),5. ](treesK (q)) 2p(n)log p(m) , and, q 0 treesK (q), |q 0 | p(n),6. ](groundK (q)) 2p(n)log p(m) , and, q 0 groundK (q), |q 0 | p(n).consequence lemma, bound number queriesgroundK (q) treesK (q) hard see two sets computedtime polynomial exponential n.next section, present algorithm decides entailment unions conjunctive queries, queries either ground query consists singleconcept atom C(x) existentially quantified variable x. Theorem 19 Lemma 20,algorithm decision procedure arbitrary unions conjunctive queries.5.6 Summary Discussionsection, presented main technical foundations answering (unionsof) conjunctive queries. known queries contain non-simple roles cyclesamong existentially quantified variables difficult handle. applying rewritingsteps Definition 10, rewrite cyclic conjunctive queries set acyclicand/or ground queries. types queries easier handle algorithmstypes exist. point, reasoning algorithm SHIQu knowledge base consistencyused deciding query entailment. order obtain tight complexity results,present following section decision procedure based extensiontranslation looping tree automata given Tobies (2001).worth mentioning that, queries simple roles, algorithm behavesexactly existing rewriting algorithms (i.e., rolling-up tuple graph technique)since, case, collapsing step applicable. need identifying variablesfirst pointed work Horrocks et al. (1999) also required (althoughmentioned) algorithm proposed Calvanese et al. (1998a).new rewriting steps (split, loop, forest rewriting) requiredapplicable non-simple roles and, replacing role atom, transitive sub-rolesreplaced role used. Hence number resulting queries fact determinedsize whole knowledge base, number transitive sub-rolesnon-simple roles query. Therefore, number resulting queries really dependsnumber transitive roles depth role hierarchy non-simple rolesquery, can, usually, expected small.6. Decision Proceduredevise decision procedure entailment unions Boolean conjunctive queriesuses, disjunct, queries obtained rewriting process definedprevious section. Detailed proofs lemmas theorems sectionfound appendix. knowledge base K union Boolean conjunctive queriesq1 . . . q` , show use queries treesK (qi ) groundK (qi ) 1 `order build set knowledge bases K1 , . . . , Kn K |= q1 . . . q` iffKi inconsistent. gives rise two decision procedures: deterministic one182fiConjunctive Query Answering DL SHIQenumerate Ki , use derive tight upper bound combinedcomplexity; non-deterministic one guess Ki , yields tightupper bound data complexity. Recall that, combined complexity, knowledgebase K queries qi count input, whereas data complexityABox counts input, parts assumed fixed.6.1 Deterministic Decision Procedure Query Entailment SHIQfirst define deterministic version decision procedure give upper boundcombined complexity. given algorithm takes input union connectedconjunctive queries works unique name assumption (UNA). show afterwards extended algorithm make UNA takesarbitrary UCQs input, complexity results carry over.construct set knowledge bases extend original knowledge base Kw.r.t. TBox ABox. extended knowledge bases given KB Kentails query q iff extended KBs inconsistent. handle concepts obtainedtree-shaped queries differently ground queries: axioms addTBox prevent matches tree-shaped queries, whereas extended ABoxes containassertions prevent matches ground queries.Definition 21. Let K = (T , R, A) SHIQ knowledge base q = q1 . . . q` unionBoolean conjunctive queries. set1. := treesK (q1 ) . . . treesK (q` ),2. G := groundK (q1 ) . . . groundK (q` ),3. Tq := {> v C | C(v) }.extended knowledge base Kq w.r.t. K q tuple (T Tq , R, Aq ) Aqcontains, q 0 G, least one assertion q 0 .4Informally, extended TBox Tq ensures tree matches.extended ABox Aq contains, ground query q 0 obtained rewriting process,least one assertion q 0 spoils match q 0 . modelextended ABox can, therefore, satisfy ground queries. modelextended knowledge bases, know counter-model originalquery.use extended knowledge bases order define deterministicversion algorithm deciding entailment unions Boolean conjunctive queriesSHIQ.Definition 22. Given SHIQ knowledge base K = (T , R, A) union connectedBoolean conjunctive queries q input, algorithm answers K entails q extendedknowledge base w.r.t. K q inconsistent answers K entail q otherwise.4following lemma shows described algorithm indeed correct.183fiGlimm, Horrocks, Lutz, & SattlerLemma 23. Let K SHIQ knowledge base q union connected Boolean conjunctive queries. Given K q input, algorithm Definition 22 answers Kentails q iff K |= q unique name assumption.proof direction lemma, use canonical modelK order guide rewriting process. direction, assumecontrary shown consistent extended knowledge base,K 6|= q. use model K 6|= q, exists assumption,show also model extended knowledge base.6.1.1 Combined Complexity Query Entailment SHIQAccording lemma, algorithm given Definition 22 correct.analyse combined complexity thereby prove also terminating.complexity analysis, assume, usual (Hustadt et al., 2005; Calvanese,De Giacomo, Lembo, Lenzerini, & Rosati, 2006; Ortiz et al., 2006b), conceptsconcept atoms ABox assertions literals, i.e., concept names negated conceptnames. input query ABox contains non-literal atoms assertions, easilytransform literal ones truth preserving way: concept atom C(t)query C non-literal concept, introduce new atomic concept AC NC ,add axiom C v AC TBox, replace C(t) AC (t); non-literalconcept assertion C(a) ABox, introduce new atomic concept AC NC , addaxiom AC v C TBox, replace C(a) AC (a). transformationobviously polynomial, without loss generality, safe assume ABoxquery contain literal concepts. advantage size atomABox assertion constant.Since algorithm involves checking consistency SHIQu knowledge base,analyse complexity reasoning service. Tobies (2001) shows ExpTimeupper bound deciding consistency SHIQ knowledge bases (even binarycoding numbers) translating SHIQ KB equisatisfiable ALCQIb knowledgebase. b stands safe Boolean role expressions built ALCQIb roles usingoperator u (role intersection), (role union), (role negation/complement) that,transformed disjunctive normal form, every disjunct contains least one nonnegated conjunct. Given query q SHIQ knowledge base K = (T , R, A), reducequery entailment deciding knowledge base consistency extended SHIQu knowledgebase Kq = (T Tq , R, Aq ). Recall Tq Aq parts containrole conjunctions use role negation ABox assertions. extendtranslation given SHIQ used deciding consistency SHIQuKBs. Although translation works SHIQu KBs, assume input KBexactly form extended knowledge bases described above.translation unrestricted SHIQu longer polynomial, case SHIQ,exponential size longest role conjunction universal quantifier. Sincerole conjunctions occur extended ABox TBox, since size roleconjunction is, Lemma 20, polynomial size q, translation exponentialsize query case extended knowledge bases.184fiConjunctive Query Answering DL SHIQassume here, usual, concepts negation normal form (NNF);concept transformed linear time equivalent one NNF pushing negationinwards, making use de Morgans laws duality existential universalrestrictions, atmost atleast number restrictions (6 n r.C > n r.Crespectively) (Horrocks et al., 2000). concept C, use Cdenote NNFC.define closure cl(C, R) concept C w.r.t. role hierarchy R smallestset satisfying following conditions:sub-concept C, cl(C, R),cl(C, R),cl(C, R),r.D cl(C, R), v* R r, TransR , s.D cl(C, R).show extend translation SHIQ ALCQIb givenTobies. first consider SHIQu -concepts extend translation KBs.Definition 24. role hierarchy R roles r, r1 , . . . , rn , letl(r, R) =(r1 u . . . u rn , R) =(r1 , R) u . . . u (rn , R).r v* R4Please note that, since r v* R r, r occurs (r, R).Lemma 25. Let R role hierarchy, r1 , . . . , rn roles. every interpretation|= R, holds ((r1 u . . . u rn , R))I = (r1 u . . . u rn )I .extended definition role conjunctions, adapt definition(Def. 6.22) Tobies provides translating SHIQ-concepts ALCQIb-concepts.Definition 26. Let C SHIQu -concept NNF R role hierarchy. everyconcept (r1 u . . . u rn ).D cl(C, R), let Xr1 u...urn ,D NC unique concept nameoccur cl(C, R). Given role hierarchy R, define function tr inductivelystructure concepts settingtr(A, R)tr(A, R)tr(C1 u C2 , R)tr(C1 C2 , R)tr(./ n(r1 u . . . u rn ).D, R)tr((r1 u . . . u rn ).D, R)tr((r1 u . . . u rn ).D, R)=======NCNCtr(C1 , R) u tr(C2 , R)tr(C1 , R) tr(C2 , R)(./ n (r1 u . . . u rn , R).tr(D, R))Xr1 u...urn ,D(Xr1 u...urn ,D)./ stands 6 >. Set tc((r1 u . . . u rn ), R) := {(t1 u . . . u tn ) | ti v* R ri tiTransR 1 n} define extended TBox TC,RTC,R ={Xr1 u...urn ,D (r1 u . . . u rn , R).tr(D, R)| (r1 u . . . u rn ).D cl(C, R)}4{Xr1 u...urn ,D v (T, R).XT,D| tc(r1 u . . . u rn , R)}Lemma 27. Let C SHIQu -concept NNF, R role hierarchy, tr TC,Rdefined Definition 26. concept C satisfiable w.r.t. R iff ALCQIb-concepttr(C, R) satisfiable w.r.t. TC,R .185fiGlimm, Horrocks, Lutz, & SattlerGiven Lemma 25, proof Lemma 27 long, straightforward extensionproof given Tobies (2001, Lemma 6.23).analyse complexity described problem. Let := |R|r1 u . . . u rn longest role conjunction occurring C, i.e., maximal number rolesoccur role conjunction C n. TBox TC,R contain exponentiallymany axioms n since cardinality set tc((r1 u . . . u rn ), R) longest roleconjunction bounded mn ri one transitivesub-role. hard check size axiom polynomial |C|. Sincedeciding whether ALCQIb concept C satisfiable w.r.t. ALCQIb TBoxExpTime-complete problem (even binary coding numbers) (Tobies, 2001, Thm.p(n)4.42), satisfiability SHIQu -concept C checked time 2p(m)2 .extend translation concepts knowledge bases. Tobies assumesrole assertions ABox form r(a, b) r role name inverserole name. Extended ABoxes contain, however, also negated roles role assertions,require different translation. positive role assertion r(a, b) translatedstandard way closing role upwards. difference using directlyadditionally split conjunction ((r, R))(a, b) = (r1 u . . . u rn )(a, b) n different roleassertions r1 (a, b), . . . , rn (a, b), clearly justified semantics. negated rolesrole assertion r(a, b), close role downwards instead upwards addrole atom s(a, b) sub-role r. justified semantics. LetK = (T Tq , R, Aq ) extended knowledge base. precisely, settr(T Tq , R) := {tr(C, R) v tr(D, R) | C v Tq },tr(A Aq , R) := {(tr(C, R))(a) | C(a) Aq }{s(a, b) | r(a, b) Aq r v* R s}{s(a, b) | r(a, b) Aq v* R r},use tr(K, R) denote ALCQIb knowledge base (tr(T Tq , R), tr(A Aq , R)).complexity deciding consistency translated SHIQu knowledge base,apply arguments concept satisfiability, gives following result:Lemma 28. Given SHIQu knowledge base K = (T , R, A) := |K| sizelongest role conjunction n, decide consistency K deterministic timep(n)2p(m)2p polynomial.ready show algorithm given Definition 22 runs deterministictime single exponential size input KB double exponential sizeinput query.Lemma 29. Let K = (T , R, A) SHIQ knowledge base = |K| q unionconnected Boolean conjunctive queries n = |q|. Given K q input, algorithm given Definition 22 decides whether K |= q unique name assumptionp(n)deterministic time 2p(m)2 .186fiConjunctive Query Answering DL SHIQproof lemma, show polynomial pp(n)extended knowledge bases consistencycheck 2p(m)2consistency check done time bound well.precisely, let q = q1 . . .q` , = treesK (q1 ). . .treesK (q` ), G = groundK (q1 ). . . groundK (q` ). Together Lemma 20, get ](T ) ](G) bounded2p(n)log p(m) polynomial p size query G polynomialn. 2p(n)log p(m) ground queries G contributes p(n) negated assertionp(n)extended ABox Aq . Hence, 2p(m)2extended ABoxes Aq and,p(n)p(m)2extended knowledge bases tested consistency.therefore, 2Given bounds cardinalities G fact sizequery G polynomial n, hard check size extendedknowledge base Kq = (T Tq , R, Aq ) bounded 2p(n)log p(m) Kqcomputed time bound well. Since extended parts contain roleconjunctions number roles role conjunction polynomial n,polynomial p1. |tr(T , R)| p(m),2. |tr(Tq , R)| 2p(n)log p(m) ,3. |tr(A, R)| p(m),4. |tr(Aq , R)| 2p(n)log p(m) , and, hence,5. |tr(Kq , R)| 2p(n)log p(m) .p(n)polynomialLemma 28, consistency check done time 2p(m)2p(n)p(m)2p. Since check 2extended knowledge bases consistency,p(n)check done time 2p(m)2 , obtain desired upper bound.show result carries even restrict interpretationsunique name assumption.Definition 30. Let K = (T , R, A) SHIQ knowledge base q SHIQ unionBoolean conjunctive queries. partition P Inds(A), knowledge base KP =(T , R, AP ) query q P called A-partition w.r.t. K q AP q Pobtained q follows:P P1. Choose one individual name P .2. b P , replace occurrence b q a.4Please note w.l.o.g. assume constants occur query occurABox well thus partition individual names ABox alsopartitions query.Lemma 31. Let K = (T , R, A) SHIQ knowledge base q union Booleanconjunctive queries. K 6|= q without making unique name assumption iffA-partition KP = (T , R, AP ) q P w.r.t. K q KP 6|= q P uniquename assumption.187fiGlimm, Horrocks, Lutz, & SattlerLet K = (T , R, A) knowledge base Description Logic DL, C complexityclass deciding whether K |= q unique name assumption C, letn = 2|A| . Since number partitions ABox exponential numberindividual names occur ABox, following straightforward consequencelemma: Boolean conjunctive DL query q, deciding whether K |= q withoutmaking unique name assumption reduced deciding n times problem C.order extend algorithm unions possibly unconnected Boolean conjunctivequeries, first transform input query q conjunctive normal form (CNF).check entailment conjunct qi , union connected Booleanconjunctive queries. algorithm returns K entails q entailment check succeedsanswers K entail q otherwise. Lemma 5 Lemma 23, algorithmcorrect.Let K knowledge base Description Logic DL, q union connected Booleanconjunctive DL queries, C complexity class deciding whether K |= qC. Let q 0 union possibly unconnected Boolean conjunctive queries cnf(q 0 )CNF q 0 . Since number conjuncts cnf(q 0 ) exponential |q 0 |, deciding0whether K |= q 0 reduced deciding n times problem C, n = 2p(|q |) ppolynomial.observation together results Lemma 29 gives followinggeneral result:Theorem 32. Let K = (T , R, A) SHIQ knowledge base = |K| q unionBoolean conjunctive queries n = |q|. Deciding whether K |= q donep(n)deterministic time 2p(m)2 .corresponding lower bound follows work Lutz (2007). Henceresult tight. result improves known co-3NExpTime upper bound settingroles query restricted simple ones (Ortiz, Calvanese, & Eiter, 2006a).Corollary 33. Let K SHIQ knowledge base = |K| q union Booleanconjunctive queries n = |q|. Deciding whether K |= q 2 ExpTime-complete problem.Regarding query answering, refer back end Section 2.2, explaindeciding tuples belong set answers checked mkAentailment tests, k number answer variables querynumber individual names Inds(A). Hence, least theoretically, absorbedcombined complexity query entailment SHIQ.6.2 Non-Deterministic Decision Procedure Query Entailment SHIQorder study data complexity query entailment, devise non-deterministicdecision procedure provides tight bound complexity problem. Actually,devised algorithm decides non-entailment queries: guess extended knowledgebase Kq , check whether consistent, return K entail q check succeedsK entails q otherwise.Definition 34. Let SHIQ TBox, R SHIQ role hierarchy, q unionBoolean conjunctive queries. Given SHIQ ABox input, algorithm guesses188fiConjunctive Query Answering DL SHIQA-partition KP = (T , R, AP ) q P w.r.t. K = (T , R, A) q. query q Ptransformed CNF one resulting conjuncts, say qiP , chosen. algorithmPPguesses extended knowledge base KqPi = (T Tqi , R, AP APqi ) w.r.t. K qiPreturns K entail q Kqi consistent returns K entails q otherwise.4Compared deterministic version algorithm given Definition 22,make UNA guess partition individual names. also non-deterministicallychoose one conjuncts result transformation CNF. conjunct,guess extended ABox check whether extended knowledge base guessedABox consistent and, therefore, counter-model query entailment.(equivalent) negated form, Lemma 23 says K 6|= q iff extendedknowledge base Kq w.r.t. K q Kq consistent. Together Lemma 31follows, therefore, algorithm Definition 34 correct.6.2.1 Data Complexity Query Entailment SHIQanalyze data complexity algorithm given Definition 34 showdeciding UCQ entailment SHIQ indeed co-NP data complexity.Theorem 35. Let SHIQ TBox, R SHIQ role hierarchy, q unionBoolean conjunctive queries. Given SHIQ ABox = |A|, algorithmDefinition 34 decides non-deterministic polynomial time whether K 6|= q K =(T , R, A).Clearly, size ABox AP A-partition bounded . Since querylonger input, size constant transformation CNF doneconstant time. non-deterministically choose one resulting conjuncts. Letconjunct qi = q(i,1) . . . q(i,`) . established Lemma 32, maximal sizePPextended ABox APqi polynomial . Hence, |A Aqi | p(ma ) polynomialp. Due Lemma 20 since size q, , R fixed assumption, setstreesKP (q(i,j) ) groundKP (q(i,j) ) j 1 j ` computed timepolynomial . Lemma 29, know translation extended knowledgebase ALCQIb knowledge base polynomial close inspectionalgorithm Tobies (2001) deciding consistency ALCQIb knowledge base showsruntime also polynomial .bound given Theorem 35 tight since data complexity conjunctive queryentailment already co-NP-hard ALE fragment SHIQ (Schaerf, 1993).Corollary 36. Conjunctive query entailment SHIQ data complete co-NP.Due correspondence query containment query answering (Calvaneseet al., 1998a), algorithm also used decide containment two unionsconjunctive queries SHIQ knowledge base, gives following result:Corollary 37. Given SHIQ knowledge base K two unions conjunctive queries qq 0 , problem whether K |= q q 0 decidable.189fiGlimm, Horrocks, Lutz, & Sattlerusing result Rosati (2006a, Thm. 11), show consistencySHIQ knowledge base extended (weakly-safe) Datalog rules decidable.Corollary 38. consistency SHIQ+log-KBs (both FOL semanticsNM semantics) decidable.7. Conclusionsdecision procedure presented entailment unions conjunctive queriesSHIQ, close long standing open problem. solution immediate consequencesrelated areas, shows several open problems query answering,query containment extension knowledge base weakly safe Datalog rulesSHIQ decidable well. Regarding combined complexity, present deterministicalgorithm needs time single exponential size KB double exponentialsize query, gives tight upper bound problem. resultshows deciding conjunctive query entailment strictly harder instance checkingSHIQ. prove co-NP-completeness data complexity. Interestingly,shows regarding data complexity deciding UCQ entailment (at least theoretically)harder instance checking SHIQ, also previously open question.part future work extend procedure SHOIQ,DL underlying OWL DL. also attempt find implementable algorithmsquery answering SHIQ. Carrying query rewriting steps goal directedway crucial achieving this.Acknowledgmentswork supported EU funded IST-2005-7603 FET Project Thinking Ontologies (TONES). Birte Glimm supported EPSRC studentship.190fiConjunctive Query Answering DL SHIQAppendix A. Complete ProofsLemma (7). Let K SHIQ knowledge base q = q1 . . . qn union conjunctivequeries, K 6|= q iff exists canonical model K 6|= q.Proof Lemma 7. direction trivial.direction, since inconsistent knowledge base entails every query,00assume K consistent. Hence, interpretation 0 = (I , )0 |= K 0 6|= q. 0 , construct canonical model K forest base J0follows: define set P (I ) paths smallest set0Inds(A), aI path;d1 dn path,d1 dn path,0(dn , d) rI role r,0Inds(A) = aI , n > 2.path p = d1 dn , length len(p) p n. fix set Inds(A)bijection f : P(i) Inds(A) {} S,(ii) Inds(A), {w | (a, w) S} tree,0(iii) f ((a, )) = aI ,(iv) (a, w), (a, w0 ) w0 successor w, f ((a, w0 )) = f ((a, w))0.(a, w) S, set Tail((a, w)) := dn f ((a, w)) = d1 dn . Now, define forest baseJ = (J ,J ) K follows:(a) J := S;(b) Inds(A), aJ := (a, ) S;(c) b NI \ Inds(A), bJ = aJ fixed Inds(A);0(d) C NC , (a, w) C J (a, w) Tail((a, w)) C ;(e) roles r, ((a, w), (b, w0 )) rJ either000(I) w = w0 = (aI , bI ) rI0(II) = b, w0 neighbor w (Tail((a, w)), Tail((b, w0 ))) rI .191fiGlimm, Horrocks, Lutz, & Sattlerclear J forest base K due definition constructionJ S.Let = (I ,I ) interpretation identical J except that, non-simpleroles r, set[(sJ )+rI = rJv* R r, sTransRtedious hard verify |= K J forest base I. Hencecanonical model K.Therefore, show 6|= q. Assume contrary |= q.1 n |= qi . define mapping00 : Terms(qi ) setting 0 (t) := Tail((t)) Terms(qi ). difficult00check 0 |= qi hence 0 |= q, contradiction.Lemma (15). Let model K.1. |= q, collapsing qco q |=co qco co injection*modulo.2. |=co qco collapsing qco q, |= q.Proof Lemma 15. (1), let |= q, let qco collapsing qobtained adding atom t0 terms t, t0 Terms(q) (t) = (t0 ).*definition semantics, |= qco injection modulo.Condition (2) trivially holds since q qco hence |= co q.Lemma (16). Let model K.1. canonical |= q, pair (qsr , R) srK (q) split matchsr |=sr qsr , R induced root splitting sr , sr injection*modulo.2. (qsr , R) srK (q) |=sr qsr match sr , |= q.Proof Lemma 16. proof second claim relatively straightforward: since(qsr , R) srK (q), collapsing qco q qsr split rewriting qco .Since roles replaced split rewriting non-simple |= qsr assumption,|= qco . Lemma 15 (2), |= q required.go proof first claim detail: let qco co(q)|=co qco match co injective modulo * . collapsing qcomatch co exist due Lemma 15. co split match w.r.t. q already,done, since split match induces root splitting R (qco , R) trivially srK (q).qcoco split match, least two terms t, t0 r(t, t0 )co (t) = (a, w), co (t0 ) = (a0 , w0 ), 6= a0 , w 6= w0 6= . distinguish two cases:192fiConjunctive Query Answering DL SHIQ1. t0 mapped roots, i.e., w 6= w0 6= . Since |=co r(t, t0 ),(co (t), co (t0 )) rI . Since canonical model K, mustrole v* R r TransR{(co (t), (a, )), ((a, ), (a0 , )), ((a0 , ), co (t0 ))} sI .Terms(qco ) co (t) = (a, ), let u = t, otherwise let ufresh variable. Similarly, t0 Terms(qco ) co (t0 ) = (a0 , ),let u0 = t0 , otherwise let u0 fresh variable. Hence, define splitrewriting qsr qco replacing r(t, t0 ) s(t, u), s(u, u0 ), s(u0 , t0 ).define new mapping sr agrees co terms occur qcomaps u (a, ) u0 (a0 , ).2. Either t0 mapped root. W.l.o.g., let t, i.e., (t) = (a, ). usearguments above: since |=co r(t, t0 ), ((t), (t0 )) rI and,since canonical model K, must role v* R r TransR{((t), (a0 , )), ((a0 , ), (t0 ))} sI . Terms(qco )co (t) = (a0 , ), let u = t, otherwise let u fresh variable. definesplit rewriting qsr qco replacing r(t, t0 ) s(t, u), s(u, t0 )and mapping sragrees co terms occur qco maps u (a0 , ).immediately follows |=sr qsr . proceed described roleatom r(t, t0 ) (t) = (a, w) (t0 ) = (a0 , w0 ) 6= a0 w 6= w0 6=. result split rewriting qsr split match sr |=sr qsr .Furthermore, sr injective modulo * since introduce new variables,variable mapped element yet range match. Since srsplit match, induces root splitting R and, hence, (qsr , R) srK (q) required.Lemma (17). Let model K.1. canonical |= q, pair (q`r , R) lrK (q) mapping `r*|=`r q`r , `r injection modulo, R root splitting induced`r and, r(t, t) q`r , R.2. (q`r , R) lrK (q) |=`r q`r match `r , |= q.Proof Lemma 17. proof (2) analogous one given Lemma 16 since,definition loop rewritings, roles replaced loop rewriting non-simple.(1), let (qsr , R) srK (q) |=sr qsr , sr split match, Rroot splitting induced sr . split rewriting qsr match sr exist dueLemma 16 canonicity I.qsrLet r(t, t)/ R. Since R root splitting induced sr since/ R, sr (t) = (a, w) Inds(A) w 6= . Now, let J forest baseI. show exists neighbor sr (t) role TransR v* Rr(sr (t), d) sI Inv(s)I . Since |=sr qsr , (sr (t), sr (t)) rI . Since Jforest base since w 6= , (sr (t), sr (t))/ rJ . followssequence d1 , . . . , dn role TransR v* R r, d1 = sr (t) = dn ,193fiGlimm, Horrocks, Lutz, & Sattler(di , di+1 ) sJ 1 < n di 6= d1 1 < < n. hardsee that, {w0 | (a, w0 ) } tree w 6= , d2 = dn1 . Since(d1 , d2 ) sJ (dn1 , dn ) sJ dn1 = d2 dn = d1 , role elementqsr= d2 required. r(t, t)/ R, select element dr,t rolesr,t described above. let q`r obtained qsr followingqsrr(t, t)/ R:dr,t = sr (t0 ) t0 Terms(qsr ), replace r(t, t) sr,t (t, t0 ) sr,t (t0 , t);otherwise, introduce new variable vr,t NV replace r(t, t) sr,t (t, vr,t )sr,t (vr,t , t).Let `r obtained sr extending `r (vr,t ) = dr,t newly introducedvariable vr,t . definition q`r `r , q`r connected, `r injective modulo * ,|=`r q`r .Lemma (18). Let model K.1. canonical |= q, pair (qf r , R) frK (q) |=f r qf rforest match f r , R induced root splitting f r , f r injection*modulo.2. (qf r , R) frK (q) |=f r qf r match f r , |= q.Proof Lemma 18. proof (2) analogous one given Lemma 16.(1), let (q`r , R) lrK (q) |=`r q`r , R root splitting induced `r , `rq`r , R. loop rewriting match `rinjective modulo * and, r(t, t)exist due Lemma 17 canonicity I. definition, R root splitting w.r.t.q`r K.w, w0 , longest common prefix (LCP) w, w0 longest ww prefix w w0 . match `r define set follows::= ran(`r ) {(a, w) | w LCP w, w0(a, w0 ), (a, w00 ) ran(`r )}.Let V NV \ Vars(q`r ) that, \ ran(`r ), unique vd V .define mapping f r `r {vd V 7 d}. definition V vd , f rsplit match well. set V Vars(q`r ) set variables new query qf r .Note ran(f r ) = D.Fact (a) (a, w), (a, w0 ) ran(f r ), (a, w00 ) ran(f r ), w00 LCP ww0 ;Fact (b) ](V ) ](Vars(q`r )) (Because, worst case, (a, w) ran(`r ) incomparable thus seen leaves binarily branching tree. Now, treen leaves least binarily branching every non-leaf n innernodes, thus ](V ) ](Vars(q`r )).194fiConjunctive Query Answering DL SHIQpair individuals d, d0 , path d0 (unique) shortest sequenceelements d1 , . . . , dn d1 = d, dn = d0 , di+1 neighbor di1 < n. length path number elements it, i.e., path d1 , . . . , dnlength n. relevant path d01 , . . . , d0` d0 sub-sequence d1 , . . . , dnobtained dropping elements di/ D.subq(q`r , tr ) tr R let d01 , . . . , d0` relevant pathClaim 1. Let r(t, t0 )= d01 = `r (t) d0 = d0` = `r (t0 ). ` > 2, role TransRv* R r (d0i , d0i+1 ) sI 1 < `.Proof. Let d1 , . . . , dn path d01 , . . . , d0` relevant path `r (t) `r (t0 ).` > 2 implies n > 2. show role claim. Let Jforest base I. Since |=`r q`r , n > 2 implies (`r (t), `r (t0 )) rI \ rJ . Sincebased J , follows TransR v* R r, (di , di+1 ) sJ1 < n. construction J , follows (d0i , d0i+1 ) sI 1 < `,finishes proof claim.subq(q`r , tr )let qf r obtained q`r follows: role atom r(t, t)000tr R, length relevant path d1 , . . . , d` = d1 = `r (t) d0 = d0` = `r (t0 )greater 2, select role variables tj f r (tj ) = d0jClaim 1 replace atom r(t, t0 ) s(t1 , t2 ), . . . , s(t`1 , t` ), = t1 , t0 = t` .Please note tj chosen dont care non-deterministic way since f rinjective modulo * , i.e., f r (tj ) = dj = f r (t0j ), tj * t0j pick these.show(i) |=f r qf r ,(ii) f r forest match.q`r \ qf r let s(t1 , t2 ), . . . , s(t`1 , t` ) atoms replaced(i), let r(t, t0 )0`rr(t, ). Since |= q`r , |=`r r(t, t0 ) (`r (t), `r (t0 )) rI . Since r(t, t0 ) replacedqf r , length relevant path `r (t) `r (t0 ) greater 2. Hence, mustcase (`r (t), `r (t0 )) rI \ rJ . Let d1 , . . . , dn d1 = `r (t) dn = `r (t0 )path `r (t) `r (t0 ) d01 , . . . , d0` relevant path `r (t) `r (t0 ).construction J , means role TransR v* R r(di , di+1 ) sJ 1 < n. construction I, means (d0i , d0i+1 ) sI1 < ` required. Hence |=f r s(ti , ti+1 ) < ` definition f r .(ii): mapping f r differs `r newly introduced variables.Furthermore, introduced new role atoms within sub-query subq(q`r , tr ) `rsplit match assumption. Hence, f r trivially split matchshow f r forest match. Since f r split match, tree tree.Inds(A), let Ta := {w | (a, w) ran(f r )}. need construct mappingf specified Definition 14, start root tr . Ta 6= , let tr Terms(q)unique term f r (tr ) = (a, wr ) Terms(q)f r (t) = (a, w) w proper prefix wr . term exists since f r split matchunique due Fact (a) above. Define trace sequence w = w1 wn Ta+w1 = wr ;195fiGlimm, Horrocks, Lutz, & Sattler1 < n, wi longest proper prefix wi+1 .Since canonical, wi Ta IN. hard see = {w | w trace}{} tree. trace w = w1 wn , let Tail(w) = wn . Define mapping f mapsterm f r (t) = (a, w) Ta unique trace wt w = Tail(wt ). Letr(t, t0 ) qf r f r (t), f r (t0 ) Ta . construction qf r , implieslength relevant path f r (t) f r (t0 ) exactly 2. Thus, f (t) f (t0 )neighbors and, hence, f r forest match required.Theorem (19). Let K SHIQ knowledge base, q Boolean conjunctive query,{q1 , . . . , q` } = treesK (q) groundK (q). K |= q iff K |= q1 . . . q` .Proof Theorem 19. direction: let us assume K |= q1 . . . q` . Hence,model K, query qi 1 ` |= qi . distinguishtwo cases: (i) qi treesK (q) (ii) qi groundK (q).(i): qi form C(v) C query concept query qf r w.r.t.v Vars(qf r ) (qf r , ) frK (q). Hence |= qi match , thus |= C(v).Let = (v) C . Lemma 12, |= qf r and,Lemma 18, |= q required.(ii): since qi groundK (q), pair (qf r , R) frK (q) qi =ground(qf r , R, ). show |=f r qf r match f r . Since |= q1 ,match |=i qi . construct match f r . R,qi contains concept atom C( (t)) C = con(subq(qf r , t), t) query conceptsubq(qf r , t) w.r.t. t. Since |=i C( (t)) Lemma 12, match|=t subq(qf r , t). define f r union , R. Please notef r (t) = ( (t)). Since Inds(qf r ) R that, Inds(qf r ), (a) =qf r(t) = (t0 ) iff t* t0 , follows |=f r atomfrcontains terms root choice R hence |=qf r required.direction show that, K |= q, K |= q1 . . . q` , letus assume K |= q. Lemma 7 negated form K |= q iff canonicalmodels K |= q. Hence, restrict attention canonicalmodels K. Lemma 18, |= K |= q implies pair (qf r , R) frK (q)|=f r qf r forest match f r , R induced root splitting f r , f r*injection modulo. distinguish two cases:(i) R = , i.e., root splitting empty f r tree match,(ii) R 6= , i.e., root splitting non-empty f r forest match treematch.(i): since (qf r , ) frK (q), v Terms(qf r ) C = con(qf r , v)qi = C(v). Lemma 12 and, since |= qf r , element C .Hence |= C(v) : v 7 required.(ii): since R root splitting induced f r , RInds(A) f r (t) = (at , ). define mapping : R Inds(A)follows: R, (t) = iff f r (t) = (at , ). definition ground(qf r , R, ),qi = ground(qf r , R, ) groundK (q). Since |=f r qf r , |= subq(qf r , t) R.196fiConjunctive Query Answering DL SHIQSince qf r forest-shaped, subq(qf r , t) tree-shaped. Then, Lemma 12, |= qi0 ,qi0 query obtained qf r replacing sub-query subq(qf r , t) C(t)C query concept subq(qf r , t) w.r.t. t. definition forest matchf r , clear |= ground(qf r , R, ) required.Lemma (20). Let q Boolean conjunctive query, K = (T , R, A) SHIQ knowledgebase, |q| := n |K| := m. polynomial p1. ](co(q)) 2p(n) and, q 0 co(q), |q 0 | p(n),2. ](srK (q)) 2p(n)log p(m) , and, q 0 srK (q), |q 0 | p(n),3. ](lrK (q)) 2p(n)log p(m) , and, q 0 lrK (q), |q 0 | p(n),4. ](frK (q)) 2p(n)log p(m) , and, q 0 frK (q), |q 0 | p(n),5. ](treesK (q)) 2p(n)log p(m) , and, q 0 treesK (q), |q 0 | p(n),6. ](groundK (q)) 2p(n)log p(m) , and, q 0 groundK (q), |q 0 | p(n).Proof Lemma 20.1. set co(q) contains queries obtained q adding n equalityatoms q. number collapsings corresponds, therefore, building equivalence classes terms q * . Hence, cardinality set co(q)exponential n. Since add one equality atom pair terms,size query q 0 co(q) n + n2 , |q 0 | is, therefore, polynomial n.2. n role atoms, choose nothing, replaceatom two atoms, three atoms. every replacement, chooseintroduce new variable re-use one existing variables. introduce newvariable every time, new query contains 3n terms. Since K containnon-simple roles sub-role role used role atoms q,roles choose replacing role atom. Overall, gives us1 + m(3n) + m(3n)(3n) choices n role atoms queryand, therefore, number split rewritings query q 0 co(q) polynomialexponential n. combination results (1), also showsoverall number split rewritings polynomial exponential n.Since add two new role atoms existing role atoms, sizequery q 0 srK (q) linear n.3. n role atoms form r(t, t) query q 0 srK (q) couldgive rise loop rewriting, non-simple sub-roles r Kused loop rewriting, introduce one new variablerole atom r(t, t). Therefore, query srK (q), number loop rewritingspolynomial exponential n. Combined results (2),bound also holds cardinality lrK (q).loop rewriting, one role atom replaced two role atoms, hence, sizequery q 0 lrK (q) doubles.197fiGlimm, Horrocks, Lutz, & Sattler4. use similar arguments order derive bound exponentialn polynomial number forest rewritings frK (q).Since number role atoms introduce forest rewriting polynomial n, size query q 0 frK (q) quadratic n.5. cardinality set treesK (q) clearly also polynomial exponentialn since query frK (q) contribute one query set treesK (q).hard see size query q 0 treesK (q) polynomial n.6. (1)-(4) above, number terms root splitting polynomial nindividual names occurring used mappingterms individual names. Hence number different ground mappingspolynomial exponential n. number ground queriessingle tuple (qf r , R) frK (q) contribute is, therefore, also polynomialexponential n. Together bound number forest rewritings(4), shows cardinality groundK (q) polynomial exponentialn. hard see size query q 0 groundK (q)polynomial n.Lemma (23). Let K SHIQ knowledge base q union connected Booleanconjunctive queries. algorithm Definition 22 answers K entails q iff K |= qunique name assumption.Proof Lemma 23. if-direction: let q = q1 . . . q` . show contrapositive assume K 6|= q. assume K consistent since inconsistentknowledge base trivially entails every query. Let model K 6|= q.show also model extended knowledge base Kq = (T Tq , R, Aq ).first show model Tq . end, let > v C Tq . C(v)C = con(qf r , v) pair (qf r , ) frK (q1 ) . . . frK (q` ) v Vars(qf r ). Let(qf r , ) frK (qi ). C 6= implies, Lemma 12, |= qf r and,Lemma 18, |= qi and, hence, |= q, contradicting assumption. Thus |= > v Cand, thus, |= Tq .Next, define extended ABox Aq that, q 0 G,C(a) q 0 aI C , C(a) Aq ;r(a, b) q 0 (aI , bI )/ rI , r(a, b) Aq .assume query q 0 = ground(qf r , R, ) groundK (q1 ). . .groundK (q` )atom q 0 Aq . trivially |= q 0 . Let(qf r , R) frK (qi ). Theorem 19, |= qi thus |= q, contradiction. HenceKq extended knowledge base |= Kq required.if-direction, assume K |= q, algorithm answers Kentail q. Hence extended knowledge base Kq = (T Tq , R, Aq )consistent, i.e., model |= Kq . Since Kq extension K,198fiConjunctive Query Answering DL SHIQ|= K. Moreover, |= Tq hence, , CC(v) treesK (q1 ) . . . treesK (q` ). Lemma 12, 6|= q 0q 0 treesK (q1 ) . . . treesK (q` ) and, Lemma 18, 6|= qi 1 `.definition extended knowledge bases, Aq contains assertion least oneatom query q 0 = ground(qf r , R, ) groundK (q1 ) . . . groundK (q` ). Hence6|= q 0 q 0 groundK (q1 ) . . . groundK (q` ). Then, Theorem 19, 6|= q,contradicts assumption.Lemma (25). Let R role hierarchy, r1 , . . . , rn roles. every interpretation|= R, holds ((r1 u . . . u rn , R))I = (r1 u . . . u rn )I .Proof Lemma 25. proof straightforward extension Lemma 6.19 Tobies(2001). definition, (r1 u . . . u rn , R) = (r1 , R) u . . . u (rn , R) and, definition semantics role conjunctions, ((r1 , R) u . . . u (rn , R))I =(r1 , R)I . . . (rn , R)I . v* R r, {s0 | r v* R s0 } {s0 | v* R s0 } hence(s, R)I (r, R)I . |= R, rI sI every r v* R s. Hence, (r, R)I = rI((r1 u . . . u rn , R))I = ((r1 , R) u . . . u (rn , R))I = (r1 , R)I . . . (rn , R)I =r1 . . . rn = (r1 u . . . u rn )I required.Lemma (28). Given SHIQu knowledge base K = (T , R, A) := |K| sizelongest role conjunction n, decide consistency K deterministic timep(n)2p(m)2p polynomial.Proof Lemma 28. first translate K ALCQIb knowledge base tr(K, R) =(tr(T , R), tr(A, R)). Since longest role conjunction size n, cardinalityset tc(R, R) role conjunction R bounded mn . Hence, TBox tr(T , R)contain exponentially many axioms n. hard check size axiompolynomial m. Since deciding whether ALCQIb KB consistent ExpTimecomplete problem (even binary coding numbers) (Tobies, 2001, Theorem 4.42),p(n)consistency tr(K, R) checked time 2p(m)2 .Lemma (29). Let K = (T , R, A) SHIQ knowledge base := |K| qunion connected Boolean conjunctive queries n := |q|. algorithm givenDefinition 22 decides whether K |= q unique name assumption deterministicp(n)time 2p(m)2 .Proof Lemma 29. first show polynomial pp(n)check 2p(m)2extended knowledge bases consistencyp(n)p(n)consistency check done time 2p(m)2 , gives upper bound 2p(m)2time needed deciding whether K |= q.Let q := q1 . . . q` . Clearly, use n bound `, i.e., ` n. Moreover,size query qi 1 ` bounded n. Together Lemma 20, get](T ) ](G) bounded 2p(n)log p(m) polynomial p clearsets computed time bound well. size query q 0 G w.r.t.ABox polynomial n and, constructing Aq , add subset (negated)199fiGlimm, Horrocks, Lutz, & Sattlerp(n)atoms q 0 G Aq . Hence, 2p(m)2extended ABoxes Aqp(n)p(m)2extended knowledge bases tested consistency.and, therefore, 2Due Lemma 20 (5), size query q 0 polynomial n. Computingquery concept Cq0 q 0 w.r.t. variable v Vars(q 0 ) done time polynomialn. Thus TBox Tq computed time 2p(n)log p(m) . size extendedABox Aq maximal add, 2p(n)log p(m) ground queries G, atomsnegated form. Since, Lemma 20 (6), size queries polynomial n,size extended ABox Aq bounded 2p(n)log p(m) clearcompute extended ABox time bound well. Hence, size extendedKB Kq = (T Tq , R, Aq ) bounded 2p(n)log p(m) . Since role conjunctions occurTq Aq , size concept Tq Aq polynomial n, lengthlongest role conjunction also polynomial n.translating extended knowledge base ALCQIb knowledge base,number axioms resulting concept C occurs Tq Aq exponentialn. Thus, size extended knowledge base bounded 2p(n)log p(m) .Since deciding whether ALCQIb knowledge base consistent ExpTimecomplete problem (even binary coding numbers) (Tobies, 2001, Theorem 4.42),p(n)checked time 2p(m)2K consistent not.p(n)Since check 2p(m)2knowledge bases consistency,p(n)p(n)p(m)2check done time 2, obtain desired upper bound 2p(m)2deciding whether K |= q.Lemma (31). Let K = (T , R, A) SHIQ knowledge base q union Booleanconjunctive queries. K 6|= q without making unique name assumption iffA-partition KP = (T , R, AP ) q P w.r.t. K q KP 6|= q P uniquename assumption.Proof Lemma 31. if-direction: Since K 6|= q, model K6|= q. Let f : Inds(A) Inds(A) total function that, setindividual names {a1 , . . . , } a1 = ai 1 n, f (ai ) = a1 . Let APq P obtained q replacing individual name q f (a).PClearly, KP = (T , R, AP ) q P A-partition w.r.t. K q. Let P = (I , )interpretation obtained restricting individual names Inds(AP ).easy see P |= KP unique name assumption holds P .0show P 6|= q P . Assume, contrary shown, P |= q Pmatch 0 . define mapping : Terms(q) 0 (a) = 0 (f (a))individual name Inds(q) (v) = 0 (v) variable v Vars(q). easysee |= q, contradiction.Pif-direction: Let P = (I , ) P |= KP UNAP 6|= q P let f : Inds(A) Inds(AP ) total function f (a) individualreplaced AP q P . Let = (I ,I ) interpretation extends PPaI = f (a)I . show |= K 6|= q. clear |= . LetC(a) assertion replaced aP AP . Since P |= C(aP )PaI = f (a)I = aPIPPC , |= C(a). use similar argument (possibly200fiConjunctive Query Answering DL SHIQ.negated) role assertions. Let =6 b assertion replaced aPP.b bP AP , i.e., f (a) = aP f (b) = bP . Since P |= aP =6 bP , aI = f (a)I =PIPIP.aP 6= bP = f (b)I = bI |= =6 b required. Therefore, |= Krequired.Assume |= q match . Let P : Terms(q P ) mappingP(v) = (v) v Vars(q P ) P (aP ) = (a) aP Inds(q P )aP = f (a). Let C(aP ) q P C(a) q replaced aP , i.e., f (a) =PIP= P (aP ) CaP . assumption, (a) C , (a) = aI = f (a)I = aPPP|= C(a ). Similar arguments used show entailment role equalityatoms, yields desired contradiction.PTheorem (35). Let K = (T , R, A) SHIQ knowledge base := |K| q :=q1 . . . q` union Boolean conjunctive queries n := |q|. algorithm givenDefinition 34 decides non-deterministic time p(ma ) whether K 6|= q := |A| ppolynomial.Proof Theorem 35. Clearly, size ABox AP A-partition bounded .established Lemma 32, maximal size extended ABox APq polynomial. Hence, |AP AP|p(m)polynomialp.DueLemma20 sinceqsize q, , R fixed assumption, sets treesKP (qi ) groundKP (qi )1 ` computed time polynomial . Lemma 29,know translation extended knowledge base ALCQIb knowledge basepolynomial close inspection algorithm Tobies (2001) decidingconsistency ALCQIb knowledge base shows runtime also polynomial.ReferencesBaader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (Eds.).(2003). Description Logic Handbook. Cambridge University Press.Bechhofer, S., van Harmelen, F., Hendler, J., Horrocks, I., McGuinness, D. L., PatelSchneider, P. F., & Stein, L. A. (2004). OWL web ontology language reference. Tech.rep., World Wide Web Consortium. http://www.w3.org/TR/2004/REC-owl-ref-20040210/.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2006). Datacomplexity query answering description logics. Doherty, P., Mylopoulos, J., &Welty, C. A. (Eds.), Proceedings 10th International Conference PrinciplesKnowledge Representation Reasoning (KR 2006), pp. 260270. AAAI Press/TheMIT Press.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractablereasoning efficient query answering description logics: dl-lite family. Journal Automated Reasoning, 39 (3), 385429.Calvanese, D., De Giacomo, G., & Lenzerini, M. (1998a). decidability querycontainment constraints. Proceedings 17th ACM SIGACT-SIGMOD201fiGlimm, Horrocks, Lutz, & SattlerSIGART Symposium Principles Database Systems (PODS 1998), pp. 149158.ACM Press Addison Wesley.Calvanese, D., De Giacomo, G., Lenzerini, M., Nardi, D., & Rosati, R. (1998b). Descriptionlogic framework information integration. Proceedings 6th InternationalConference Principles Knowledge Representation Reasoning (KR 1998).Calvanese, D., Eiter, T., & Ortiz, M. (2007). Answering regular path queries expressive description logics: automata-theoretic approach. Proceedings 22thNational Conference Artificial Intelligence (AAAI 2007).Chekuri, C., & Rajaraman, A. (1997). Conjunctive query containment revisited. Proceedings 6th International Conference Database Theory (ICDT 1997), pp.5670, London, UK. Springer-Verlag.Glimm, B., Horrocks, I., & Sattler, U. (2006). Conjunctive query answering descriptionlogics transitive roles. Proceedings 19th International WorkshopDescription Logics (DL 2006). http://www.cs.man.ac.uk/~glimmbx/download/GlHS06a.pdf.Gradel, E. (2001). modal logics robustly decidable?. Paun, G., Rozenberg,G., & Salomaa, A. (Eds.), Current Trends Theoretical Computer Science, Entering21th Century, Vol. 2, pp. 393408. World Scientific.Grahne, G. (1991). Problem Incomplete Information Relational Databases. SpringerVerlag.Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). SHIQ RDFOWL: making web ontology language. Journal Web Semantics, 1 (1),726.Horrocks, I., Sattler, U., Tessaris, S., & Tobies, S. (1999). Query containment using DLR ABox. Ltcs-report LTCS-99-15, LuFG Theoretical Computer Science,RWTH Aachen, Germany. Available online http://www-lti.informatik.rwth-aachen.de/Forschung/Reports.html.Horrocks, I., Sattler, U., & Tobies, S. (2000). Reasoning Individuals DescriptionLogic SHIQ. McAllester, D. (Ed.), Proceedings 17th International Conference Automated Deduction (CADE 2000), No. 1831 Lecture Notes ArtificialIntelligence, pp. 482496. Springer-Verlag.Horrocks, I., & Tessaris, S. (2000). conjunctive query language description logic aboxes.Proceedings 17th National Conference Artificial Intelligence (AAAI 2000),pp. 399404.Hustadt, U., Motik, B., & Sattler, U. (2005). Data complexity reasoning expressivedescription logics. Proceedings International Joint Conference ArtificialIntelligence (IJCAI 2005), pp. 466471.Levy, A. Y., & Rousset, M.-C. (1998). Combining horn rules description logicsCARIN. Artificial Intelligence, 104 (12), 165209.Lutz, C. (2007). Inverse roles make conjunctive queries hard. Proceedings 20thInternational Workshop Description Logics (DL 2007).202fiConjunctive Query Answering DL SHIQMcGuinness, D. L., & Wright, J. R. (1998). industrial strength description logic-basedconfiguration platform. IEEE Intelligent Systems, 13 (4).Motik, B., Sattler, U., & Studer, R. (2004). Query answering OWL-DL rules.Proceedings 3rd International Semantic Web Conference (ISWC 2004), Hiroshima, Japan.Ortiz, M., Calvanese, D., & Eiter, T. (2006a). Data complexity answering unionsconjunctive queries SHIQ. Proceedings 19th International WorkshopDescription Logics (DL 2006).Ortiz, M. M., Calvanese, D., & Eiter, T. (2006b). Characterizing data complexityconjunctive query answering expressive description logics. Proceedings21th National Conference Artificial Intelligence (AAAI 2006).Rosati, R. (2006a). DL+log: Tight integration description logics disjunctive datalog. Proceedings Tenth International Conference Principles KnowledgeRepresentation Reasoning (KR 2006), pp. 6878.Rosati, R. (2006b). ddecidability finite controllability query processingdatabases incomplete information. Proceedings 25th ACM SIGACTSIGMOD Symposium Principles Database Systems (PODS-06), pp. 356365.ACM Press Addison Wesley.Rosati, R. (2007a). limits querying ontologies. Proceedings EleventhInternational Conference Database Theory (ICDT 2007), Vol. 4353 Lecture NotesComputer Science, pp. 164178. Springer-Verlag.Rosati, R. (2007b). conjunctive query answering EL. Proceedings 2007Description Logic Workshop (DL 2007). CEUR Workshop Proceedings.Schaerf, A. (1993). complexity instance checking problem concept languagesexistential quantification. Journal Intelligent Information Systems, 2 (3), 265278.Sirin, E., & Parsia, B. (2006). Optimizations answering conjunctive abox queries.Proceedings 19th International Workshop Description Logics (DL 2006).Sirin, E., Parsia, B., Cuenca Grau, B., Kalyanpur, A., & Katz, Y. (2006). Pellet: practicalOWL-DL reasoner. Accepted Journal Web Semantics, Available onlinehttp://www.mindswap.org/papers/PelletJWS.pdf.Tessaris, S. (2001). Questions answers: reasoning querying Description Logic.PhD thesis, University Manchester.Tobies, S. (2001). Complexity Results Practical Algorithms Logics KnowledgeRepresentation. PhD thesis, RWTH Aachen.Tsarkov, D., & Horrocks, I. (2006). FaCT++ description logic reasoner: System description.Furbach, U., & Shankar, N. (Eds.), Proceedings Third International JointConference Automated Reasoning (IJCAR 2006), Vol. 4130 Lecture NotesComputer Science, pp. 292 297. Springer-Verlag.203fiGlimm, Horrocks, Lutz, & Sattlervan der Meyden, R. (1998). Logical approaches incomplete information: survey.Logics Databases Information Systems, pp. 307356. Kluwer Academic Publishers.Vardi, M. Y. (1997). modal logic robustly decidable?. Descriptive ComplexityFinite Models: Proceedings DIMACS Workshop, Vol. 31 DIMACS: SeriesDiscrete Mathematics Theoretical Computer Science, pp. 149184. AmericanMathematical Society.Wessel, M., & Moller, R. (2005). high performance semantic web query answering engine.Proceedings 18th International Workshop Description Logics.Wolstencroft, K., Brass, A., Horrocks, I., Lord, P., Sattler, U., Turi, D., & Stevens, R.(2005). Little Semantic Web Goes Long Way Biology. Proceedings2005 International Semantic Web Conference (ISWC 2005).204fiJournal Artificial Intelligence Research 31 (2008) 353-398Submitted 09/07; published 02/08Gesture Salience Hidden Variable CoreferenceResolution Keyframe ExtractionJacob EisensteinRegina BarzilayRandall Davisjacobe@csail.mit.eduregina@csail.mit.edudavis@csail.mit.eduComputer Science Artificial Intelligence LaboratoryMassachusetts Institute Technology77 Massachusetts AvenueCambridge, 02139 USAAbstractGesture non-verbal modality contribute crucial information understanding natural language. gestures informative, non-communicativehand motions may confuse natural language processing (NLP) impede learning. People little difficulty ignoring irrelevant hand movements focusing meaningfulgestures, suggesting automatic system could also trained perform task.However, informativeness gesture context-dependent labeling enough datacover cases would expensive. present conditional modality fusion, conditionalhidden-variable model learns predict gestures salient coreference resolution, task determining whether two noun phrases refer semanticentity. Moreover, approach uses coreference annotations, annotationsgesture salience itself. show gesture features improve performance coreferenceresolution, attending gestures salient, method achievessignificant gains. addition, show model gesture salience learnedcontext coreference accords human intuition, demonstrating gesturesjudged salient model used successfully create multimedia keyframesummaries video. summaries similar created human raters,significantly outperform summaries produced baselines literature.11. IntroductionGesture nearly ubiquitous feature face-to-face natural language communicationmay used supplement speech additional information reinforce meaningalready conveyed (McNeill, 1992). either case, gesture increase robustnessnatural language processing (NLP) systems inevitable disfluency spontaneous1. article extension unification two conference publications (Eisenstein, Barzilay, & Davis,2007; Eisenstein & Davis, 2007). extends prior published work several new, unpublished results:stability analysis respect initialization weights (Section 4.3); analysis verbal features termscentering theory (Section 5.1); interrater agreement analysis coreference annotations (Section 6);evaluation coreference using global metric (Section 6.2); expanded empirical evaluationcoreference task additional fusion models (Section 6.2); analysis different types gesture featuresmultimodal coreference resolution (Section 6.4.1); study interaction gesturalverbal features (Section 6.4.2); interrater agreement keyframe extraction (Section 7.3). Sourcecode data available http : //mug.csail.mit.edu/publications/2008/Eisenstein JAIR/c2008AI Access Foundation. rights reserved.fiEisenstein, Barzilay, & Davistop one clears area here, goesway top...moves up. everything moves up.12Figure 1: excerpt explanatory narrative gesture helps disambiguatemeaning.speech. example, consider following excerpt presentationspeaker describes mechanical device:moves up, everything moves up. top one clears areahere, goes way top.references passage difficult disambiguate, meaning becomes clearerset context accompanying hand gestures (Figure 1).Despite apparent benefits offered gestural cues, obtaining concrete gains natural language understanding difficult. key problem combine gesturelinguistic features. Existing systems typically address issue directly concatenatinglow-level visual information (e.g., hand position speed) traditional textual features (Eisenstein & Davis, 2006), combining posteriors separately-trainedmodels (Chen, Harper, & Huang, 2006). appealing alternative consider inherent linguistic quality gesture, distinguished hand movements maymeaningful desired language understanding task (Goodwin & Goodwin, 1986).2show better results obtained focusing hand movements likelycorrespond relevant gestures.move beyond low-level representation gesture, one could attempt developgeneral-purpose taxonomy gestures based relation language. taxonomiesproved useful psychological linguistic research gesture, applicationcorpus-based statistical language processing immediately practical. Gesturemultifaceted phenomenon, key features understanding gestures meaning mayhighly context-dependent (Lascarides & Stone, 2006). example, flexion2. hand motions even absence hand motion may meaningful sense. However,specific language processing problem, gestures directly relevant. remainderarticle, terms meaningful meaningless always assumed framed withincontext specific language processing task. Hand motions meaningless coreferenceresolution may indeed quite useful another problem, sentiment classification.354fiGesture Salience Hidden Variablesingle finger might crucial component one gesture irrelevant detail anothercontext. possible create formal annotation scheme expressive enough capturedetails, yet compact enough tractable? topic ongoing research.even possible, annotation would time-consuming, particularlyscale necessary corpus-based NLP.paper propose middle path: model learns attend salient gestureswithout explicit gesture annotation. Instead top-down approach attemptsanalyze gestures according universal taxonomy, work bottom-up specificlanguage understanding problem: coreference resolution. speaker produces similar,meaningful deictic3 gestures two noun phrases, good indication nounphrases coreferent (Eisenstein & Davis, 2006). automatically identify gesturesrelevant coreference resolution, among hand motions co-occurnoun phrases. approach shown enhance contribution low-level gesturefeatures towards coreference.concretely, employ conditional model hidden variable governswhether gesture features included determination coreference pairnoun phrases. model, possible learn gesture salience jointly coreference. baseline, demonstrate even low-level concatenative approachgesture-speech fusion4 yields small statistically significant improvement coreference resolution, compared textual features alone. importantly, showcontribution gesture features increases substantially gesture speechcombined using structured model.model gesture salience learn relevant coreference resolution, would useful engineering perspective. interesting questionwhether estimates gesture salience related humans perceive multimodal communication. answer this, examine whether model gesture saliencerelevant language processing tasks. demonstrate model learnedcoreference resolution applied selection keyframes generating visualsummaries instructional presentations. Without explicit training keyframeextraction task, approach selects keyframes cohere meaningfully chosenhuman annotators.main contributions paper summarized follows.New applications gesture: demonstrate benefits incorporating gesturetwo tasks: coreference resolution video keyframe extraction. coreferencetask, substantially improve previous work showed gesture similarity help predict coreference resolution (Eisenstein & Davis, 2006); applicationlinguistic analysis gesture video keyframe extraction novel. previousresearch, gesture information shown boost performance sentence segmentation, local syntactic phenomenon. work demonstrates gestures usefulnessnon-local, discourse-level tasks. end, introduce novel set featurestightly combine linguistic gestural information.3. Deictic gestures communicate meaning spatial location (McNeill, 1992).4. use term speech indicate dealing spoken language, note handtranscriptions rather automatic speech recognition (ASR) used throughout experiments.applicability techniques context noisy ASR transcripts topic future work.355fiEisenstein, Barzilay, & DavisGesture salience language: develop idea gesture informationconsidered language processing gesture salient. prior research (e.g., Chen, Liu, Harper, & Shriberg, 2004), model uses low-level featuresextracted directly vision-based hand tracker, avoiding need manual annotation gesture features. However, relevance low-level features dependslinguistic context. modeling relationship gesture salience,obtain significant performance gains. addition, present set features designedcapture salience gesture associated speech.Hidden-variable modeling gesture salience: develop framework gesture salience modeled jointly coreference resolution. show gesture salience expressed hidden variable learned without explicit labels,leveraging coreference annotations. novel framework realized within conditional model, enabling use arbitrary possibly non-independent features.experiments demonstrate estimates gesture salience obtainedmodel applied extract keyframes containing salient deictic gestures.Section 2, consider prior work relating existing models taxonomiesgesture psychology literature, well previous efforts incorporate gesturenatural language understanding. Section 3, describe datasetconduct experiments. Section 4, present model, conditional modality fusion.show gesture salience treated hidden variable learned withoutexplicit annotations. Section 5 includes description textual gestural featuresuse experiments. Section 6 present experimental results showingmodel improves performance coreference resolution. Section 7, showestimates gesture salience general, applied select useful keyframesvideo. Finally, Section 8 discuss implications research, conclude.2. Related Worksection, describe four general areas related work provide backgroundcontext efforts. Section 2.1 discusses models gesture languagepsychology linguistics communities. Section 2.2 describes projects employedgesture natural language processing. Section 2.3 describes general modality fusiontechniques, particularly ones used incorporate prosodic features NLP.Finally, Section 2.4 considers models machine learning literature relatedconditional modality fusion.2.1 Models Gesture LanguagePsychology research explored problem modeling gesture relation language. discuss two frequently-cited models: Kendons taxonomy (1980), focuseskinematic structure individual gestures; McNeills (1992), identifiesways gesture communicates meaning within discourse.According Kendon, gestures constructed set movement phases: prepare, stroke, hold, retract. prepare retract phases initiate terminategesture, respectively. stroke content-carrying part gesture, hold356fiGesture Salience Hidden Variablepause may occur immediately stroke. phases provideessentially kinematic description gesture. McNeill focuses way gestures communicate meaning, identifying four major types conversational gestures: deictic, iconic,metaphoric, beat. Deictics communicate reference spatial locations, iconicsmetaphorics create imagery using form gesture, beats communicate usingtiming emphasis.5taxonomies proved useful psychological linguistic research,substantial effort would required create corpus annotations statisticalnatural language processing. circumvent problem learning model gesturedirectly automatically-extracted visual features. relationship gesturelanguage semantics learned context specific language phenomenon, using annotations verbal language semantics. approach may capture meaningfulhand gestures, capture a) relevant coreference resolutionb) identified using feature set.2.2 Multimodal Natural Language ProcessingEarly computational work relationship language gesture focused identifying examples connections discourse elements automatically-recognizedproperties hand gesture (Quek et al., 2000, 2002a). Quek et al. (2002a) show examplessimilar gestures used connection repetitions associated discourseelements. exploit idea using features quantify gesture similarity predictnoun phrase coreference. Follow-up papers attempt capture contribution individual gesture features, spatial location (Quek, McNeill, Bryll, & Harper, 2002b)symmetric motion (Xiong & Quek, 2006), using similar methodology. linework provides helpful framework understanding relationship gesturenatural language.engineering side, several papers report work exploiting relationship gesture language. one line research, linguistic features used improve gesture processing (Poddar, Sethi, Ozyildiz, & Sharma, 1998; Kettebekov, Yeasin,& Sharma, 2005). papers evaluate performance human-human languagedomain weather broadcasts, stated goal developing techniques gesturebased human-computer interaction. authors note domain weather broadcasts, many hand motions well-described relatively small taxonomy gestures,identify points, contours, regions. Lexical features ASR transcriptsshown improve gesture recognition (Poddar et al., 1998), prosodic features usedidentify key gestural segments (Kettebekov et al., 2005).Similarly, linguistic analysis shown important consequences gesture generation animated agents. Nakano, Reinstein, Stocky, Cassell (2003) presentempirical study human-human interaction, showing statistical relationshiphand-coded descriptions head gestures discourse labels associated utterances (e.g., acknowledgment, answer, assertion). demonstratedfindings encoded model generate realistic conversational grounding5. McNeill notes types thought mutually exclusive bins, featuresmay present varying degrees.357fiEisenstein, Barzilay, & Davisbehavior animated agent. addition discourse-moderating function, gesturesalso shown useful supplementing semantic content verbal explanations.Kopp, Tepper, Ferriman, Cassell (2007) describe system animated agents givenavigation directions, using hand gestures describe physical properties landmarksalong route. research describes interesting relationships gesturelanguage exploited generation, focus recognition multimodalcommunication.research cited uses linguistic context supplement gesture generationrecognition, work used gesture features supplement natural language processing. Much research taken place context spoken language dialoguesystems incorporating pen gestures automatically recognized speech. early example system Quickset (Cohen et al., 1997), pen gestures speechinput used plan military missions. Working domain, Johnston Bangalore (2000) describe multimodal integration algorithm parses entire utterancesresolves ambiguity speech gesture modalities. Chai Qu (2005) presentalternative take similar problem, showing speech recognition improvedincreasing salience entities targeted gestures. research projects differassume ontology possible referents known advance.addition, gestures performed pen rather free hand, gesturesegmentation inferred contact pen sensing surface. Finally,dialogue cases human-computer, rather human-human, languageusage probably differs.research similar involves using gesture features improve languageprocessing spontaneous human-to-human discourse. Gesture shown improve tasksentence segmentation using automatically recognized features (Chen et al., 2004),successfully manual gesture annotations (Chen et al., 2006). hidden Markovmodel (HMM) used capture relation lexical tokens sentence boundaries. train maximum entropy model using feature vector posteriorprobability estimates HMM set gesture features based KendonMcNeill taxonomies described above. earlier work, show gesture featuresalso improve coreference resolution; describe system classifier trainedcoreference, using joint feature vector gesture textual features (Eisenstein & Davis,2006). approaches, gesture speech combined unstructured way,even irrelevant hand movements may influence classification decisions. approachpresent paper includes gesture features likely relevant, substantially improving performance previous reported results (Eisenstein& Davis, 2006).2.3 Model Combination Techniques NLPlarge literature integrating non-verbal features NLP, much relatingprosody. example, Shriberg, Stolcke, Hakkani-Tur, Tur (2000) explore useprosodic features sentence topic segmentation. first modality combinationtechnique consider trains single classifier modalities combinedsingle feature vector; sometimes called early fusion. also consider training358fiGesture Salience Hidden Variableseparate classifiers combining posteriors, either weighted additionmultiplication; sometimes called late fusion (see also Liu, 2004). Experimentsmultimodal fusion prosodic features find conclusive winner among early fusion,additive late fusion, multiplicative late fusion (Shriberg et al., 2000; Kim, Schwarm, &Osterdorf, 2004). techniques also employed gesture-speech fusion.prior work, employed early fusion gesture-speech combination (Eisenstein & Davis,2006); late fusion also applied gesture-speech combination (Chen et al., 2004,2006).Toyama Horvitz (2000) introduce Bayesian network approach modality combination speaker identification. late fusion, modality-specific classifiers trainedindependently. However, Bayesian approach also learns predict reliabilitymodality given instance, incorporates information Bayes net.flexible early late fusion, training modality-specific classifiers separatelystill suboptimal compared training jointly, independent trainingmodality-specific classifiers forces account data cannot possibly explain. example, speakers gestures relevant language processingtask, counterproductive train gesture-modality classifier featuresinstant; lead overfitting poor generalization.approach combines aspects early late fusion. early fusion, classifiers modalities trained jointly. Toyama Horvitzs Bayesian latefusion model, modalities weighted based predictive power specific instances. addition, model trained maximize conditional likelihood, ratherjoint likelihood.2.4 Related Machine Learning Approachesmachine learning perspective, research relates three general areas: domainadaptation, co-training, hidden-variable conditional models.domain adaptation, one small amount in-domain data relevanttarget classification task, large amount out-of-domain data related,different task (Blitzer, McDonald, & Pereira, 2006; Chelba & Acero, 2006). goaluse out-of-domain data improve performance target domain. one recentapproach, feature replicated separate weights learned generaldomain-specific applications feature (Daume III, 2007). sense, model learnsfeatures relevant generally, relevant specific domains.task somewhat similar, interested learning apply gesturefeatures, simultaneously learning predict coreference. However, one keydifference domain adaptation, data partitioned separate domainsadvance, model must learn identify cases gesture salient.Co-training another technique combining multiple datasets (Blum & Mitchell,1998). co-training, small amount labeled data supplemented large amountunlabeled data. Given sets features sufficient predict desiredlabel called views separate classifiers trained predictions oneclassifier provide labeled data classifier. approach shownyield better performance using labeled data applications,359fiEisenstein, Barzilay, & Davisparsing (Sarkar, 2001). large amounts unlabeled data available, co-training couldapplied here, using gesture verbal features independent views.research, acquiring data greater bottleneck creating coreference annotations.addition, previous attempts apply co-training textual coreference resolution provedlargely unsuccessful (Muller, Rapp, & Strube, 2002), possibly viewsindependently sufficient predict label. investigation topicmerited, approach make use unlabeled data; instead, treat gesturessalience hidden variable within existing dataset.methodological standpoint, work closely related literaturehidden variables conditionally trained models. Quattoni, Collins, Darrell (2004)improve object recognition use hidden variable indicating partobject contains localized visual feature. Part-based object recognitionpreviously performed generative framework, conditional approach permitsuse broader feature set, without concern whether features mutuallyindependent. Subsequent work shown conditional hidden-variable modelsused gesture recognition (Wang, Quattoni, Morency, Demirdjian, & Darrell, 2006)language processing (Koo & Collins, 2005; Sutton, McCallum, & Rohanimanesh, 2007).Wang et al. (2006) employ model similar HMM-based gesture recognition,hidden variable encoding different phases gesture recognized; again,conditional approach shown improve performance. Hidden variables appliedstatistical parsing Koo Collins (2005), assigning lexical items word clustersword senses. Finally, Sutton et al. (2007) use hidden variables encode intermediatelevels linguistic structure relevant overall language-processing task.example, one application, hidden variables encode part-of-speech tags,used noun phrase chunking. continue line work, extending hidden-variableconditional models novel, linguistically-motivated hidden-variable architecturegesture-speech combination.3. Datasetresearch described paper based corpus multimodal presentations.existing corpora include visual data, none appropriateresearch. Ami corpus (Carletta et al., 2005) includes video audio meetings,participants usually seated hands often visible video.Vace corpus (Chen et al., 2005) also contains recordings meetings, tracking beaconsattached speakers providing accurate tracking. corpus publiclyreleased time writing.corpora address seated meeting scenarios; observed gesturefrequent speakers give standing presentations, classroom lectures businesspresentations. many video recordings available, typicallyfilmed circumstances frustrate current techniques automatic extractionvisual features, including camera movement, non-static background, poor lighting,occlusion speaker. Rather focusing substantial challenges computervision, chose gather new multimodal corpus.360fiGesture Salience Hidden VariableFigure 2: example pre-printed diagram used gathering corpus. diagramschematic depiction candy dispenser.gathering corpus, aimed capture conversations gesture frequent direct, also natural unsolicited. sought middle groundtask-oriented dialogues Trains (Allen et al., 1995) completely open-endeddiscussions Switchboard (Godfrey, Holliman, & McDaniel, 1992). work,participants given specific topics discussion (usually function mechanicaldevices), permitted converse without outside interference. speakersgiven pre-printed diagrams aid explanations. interpretation gesturescondition usually relatively straightforward; many, gestures involvepointing locations diagram. Visual aids printed projected diagramscommon important application areas, including business presentations, classroomlectures, weather reports. Thus, restriction seem overly limitingapplicability work. leave presumably challenging problem understanding gestures produced without visual aids future work.Figure 1 shows two still frames corpus, accompanying text.visual aid shown detail Figure 2. corpus includes sixteen short videosnine different speakers. total 1137 noun phrases transcribed; roughlyhalf number found MUC6 training set, text-only dataset also usedcoreference resolution (Hirschman & Chinchor, 1998). Building multimodal corpustime-consuming task requiring substantial manpower, hope initial worklead larger future corpora well-suited study gesture natural languageprocessing. Corpus statistics found Appendix C, data available on-lineat: http : //mug.csail.mit.edu/publications/2008/Eisenstein JAIR/Finally, draw readers attention differences corpuscommonly-used textual corpora coreference resolution, MUC (Hirschman & Chinchor, 1998). Topically, corpus focuses description mechanical devices, rathernews articles. Consequently, emphasis less disambiguating entities peopleorganizations, resolving references physical objects. corpora alsodiffer genre, corpus comprised spontaneous speech, MUC corpus361fiEisenstein, Barzilay, & Davisincludes edited text. genre distinctions known play important role patterns reference (Strube & Muller, 2003) language use generally (Biber, 1988). Fourdifferent mechanical devices used topics discussion: piston, candy dispenser(Figure 2), latch box (shown Appendix B), pinball machine.3.1 Data Gathering ProtocolFifteen pairs participants joined study responding posters universitycampus; ages ranged 18-32, participants university students staff.subset nine pairs participants selected basis recording quality,6speech transcribed annotated. corpus composed two videosnine pairs; audio recording problems forced us exclude two videos, yielding16 annotated documents, two three minutes duration.One participant randomly selected pair speaker,listener. speakers role explain behavior mechanical devicelistener. listeners role understand speakers explanations well enoughtake quiz later. Prior discussion, speaker privately viewed simulationoperation relevant device.speaker limited two minutes view video object three minutesexplain it; majority speakers used time allotted. suggests couldobtained natural data limiting explanation time. However, foundpilot studies led problematic ordering effects, participants devotedlong time early conditions, rushed later conditions. timeconstraints, total running time experiment usually around 45 minutes.data used study part larger dataset initially described Adler, Eisenstein,Oltmans, Guttentag, Davis (2004).3.2 Speech ProcessingSpeech recorded using headset microphones. integrated system controlled synchronization microphones video cameras. Speech transcribed manually,audio hand-segmented well-separated chunks duration longertwenty seconds. chunks force-aligned Sphinx-II speech recognitionsystem (Huang, Alleva, Hwang, & Rosenfeld, 1993).wide range possibilities exist regarding fidelity richness transcribedspeech. Choices include transcription quality, existence punctuation capitalization,presence sentence boundaries syntactic annotations. assume perfect transcription words sentence boundaries,7 additional punctuation. similarmuch NLP research Switchboard corpus, (e.g., Kahn, Lease, Charniak,Johnson, & Ostendorf, 2005; Li & Roth, 2001), although automatic speech recognition(ASR) transcripts also used (e.g., Shriberg et al., 2000). Using ASR may accurately replicate situation application developer. However, approachwould also introduce certain arbitrariness, results would depend heavily amount6. Difficulties microphones prevented us getting suitable audio recordings several cases;cases difficulties synchronizing two microphones two video cameras.7. Sentence boundaries annotated according NIST Rich Transcription Evaluation (NIST, 2003).362fiGesture Salience Hidden Variableeffort spent tuning recognizer. particular, recognizer well-tuned,approach risks overstating relative contribution gesture features, verbalfeatures would little value.natural language task coreference resolution requires noun phrase boundariespreprocessing step, provide gold-standard noun phrase annotation. goalisolate contribution model gesture-speech combination coreference task,thus wish deliberately introduce noise noun phrase boundaries.Gold standard noun phrase annotations used previous research coreferenceresolution, (e.g., McCallum & Wellner, 2004; Haghighi & Klein, 2007).8 addition,automatic noun phrase chunking possible high accuracy. F-measures exceeding.94 reported textual corpora (Kudo & Matsumoto, 2001; Sha & Pereira, 2003);transcripts Switchboard corpus, state-of-the-art performance exceeds .91 (Li &Roth, 2001).9annotation noun phrases followed MUC task definition markable NPs(Hirschman & Chinchor, 1998). Personal pronouns annotated, discoursefocused descriptions mechanical devices. pronouns could easily filteredautomatically. Annotation attempted transcribe noun phrases. total 1137markable NPs transcribed. roughly half size MUC6 training set,includes 2072 markable NPs 30 documents. gold standard coreferencemarkable annotation performed first author, using audio videoinformation.additional rater performed coreference annotations help assess validity. raternative speaker English author paper. annotated two documents, comprising total 270 noun phrases. Using interrater agreement methodologydescribed Passonneau (1997), score .65 obtained Krippendorfs alpha.comparable results MUC textual corpus (Passonneau, 1997),higher agreement reported corpus multi-party spoken dialogues (Muller,2007).Finally, assume gold standard sentence boundaries, additional punctuation.3.3 Vision ProcessingVideo recording performed using standard digital camcorders. Participants giventwo different colored gloves facilitate hand tracking. Despite use colored gloves,post-study questionnaire indicated one thirty participants guessedstudy related gesture. study deliberately designed participantslittle free time think; actually conducting dialogue, speakerbusy viewing next mechanical system, participant busy testedprevious conversation. also presented consent forms immediately gloves,may diverted attention gloves purpose.8. cited references include noun phrase unless participate coreference relations;include noun phrases regardless.9. high accuracy switchboard imply good performance data, sinceannotated data noun phrase boundaries. Thus overall impact noisy preprocessingcoreference performance unknown. addition, possible noisy noun phrase boundaries maypose particular problems approach, assesses gesture features duration NP.363fiEisenstein, Barzilay, & Davisarticulated upper-body tracker used model position speakers torso,arms, hands. building complete upper-body tracker, rather simply trackingindividual hands, able directly model occlusion hands arms.frame, annealed particle filter used search space body configurations.Essentially, system performs randomized beam search simultaneously achieve threeobjectives: a) maximize overlap model pixels judgedforeground, b) match known glove color color observed hypothesized handpositions, c) respect physiological constraints temporal continuity. systemimplemented using OpenCV library.10tracker inspired largely annealed particle filter Deutscher, Blake,Reid (2000); main differences Deutscher et al. use color cuesgloves, use multiple cameras facilitate 3D tracking. usedsingle monocular camera 2.5D model (with one degree freedom depthplane, permitting body rotation). Parameters model, body dimensions,customized speaker. speaker provided two different explanations,segmentation videos performed manually. additional post-processing,calibration, cleaning tracker output performed.inspection, lack depth information appears cause many systemserrors; bending arm joints depth dimension caused arm length appearchange ways confusing model. Nonetheless, estimate manualexamination tracking output hands tracked accurately smoothly90% time occluded. difficult assess tracker performanceprecisely, would require ground truth data actual hand positionsannotated manually time step.4. Conditional Modality Fusion Coreference Resolutionsection describe conditional modality fusion. Section 4.1 describehidden variables incorporated conditional models. Section 4.2, describevarious theories model combination expressed framework. Section 4.3,give details implementation.4.1 Hidden Variables Conditional Modelsgoal learn use non-verbal features make predictions helpful,ignore not. call approach conditional modality fusion.formally, trying predict label {1, 1}, representing single binarycoreference decision whether two noun phrases refer entity.hidden variable h describes salience gesture features. observablefeatures written x, model learn set weights w. hidden variableapproach learns predict h jointly, given labeled training data y. useconditional model, writing:10. http://www.intel.com/technology/computing/opencv/364fiGesture Salience Hidden Variablep(y|x; w) =Xp(y, h|x; w)h=Pexp((y, h, x; w))P h.00 ,h exp((y , h, x; w))Here, potential function representing compatibility label y,hidden variable h, observations x; potential parameterized vectorweights, w. numerator expresses compatibility label observations x,summed possible values hidden variable h. denominator sumsh possible labels 0 , yielding conditional probability p(y|x; w).model trained gradient-based optimization maximize conditionallog-likelihood observations. unregularized log-likelihood gradient givenby:l(w)=Xlog(p(yi |xi ; w))(1)Pexp((yi , h, xi ; w))log P h00 ,h exp((y , h, xi ; w))(2)=Xliwj=Xhp(h|yi , xi ; w)X(yi , h, xi ; w)p(h, 0 |xi ; w)(y 0 , h, xi ; w)wjwj0,huse hidden variables conditionally-trained model follows Quattoni et al.(2004). However, reference gives general outline hidden-variable conditional models, form potential function depends role hidden variable.problem-specific, novel contribution research exploration severaldifferent potential functions, permitting different forms modality fusion.4.2 Models Modality Fusionform potential function intuitions role hiddenvariable formalized. consider three alternative forms , capturing differenttheories gesture-speech integration. models range simple concatenationgesture-speech features structured fusion model dynamically assesses relevancegesture features every noun phrase.models consider influenced goal, determine whether twonoun phrases (NPs) coreferent. Gesture salience assessed NP, determinewhether gestural features influence decision whether noun phrasescorefer. set h = hh1 , h2 i, h1 {1, 1} representing gesture saliencefirst noun phrase (antecedent), h2 {1, 1} representing gesture saliencesecond noun phrase (anaphor).365fiEisenstein, Barzilay, & Davis4.2.1 Same-Same modeltrivial case, ignore hidden variable always include featuresgesture speech. Since weight vectors modalities unaffectedhidden variable, model referred same-same model. Noteidentical standard log-linear conditional model, concatenating features singlevector. model thus type early fusion, meaning verbal non-verbalfeatures combined prior training.ss (y, h, x; w) y(wvT xv + wnvxnv )(3)xv wv refer features weights verbal modality; xnv wnv refernon-verbal modality.4.2.2 Same-Zero ModelNext, consider model treats hidden variable gate governing whethergesture features included. model called same-zero model, since verbalfeatures weighted identically regardless hidden variable, gesture featureweights go zero unless h1 = h2 = 1.(x ) + h wT x + h wT x , h = h = 1y(wvT xv + wnv122 h h2nv1 h h1sz (y, h, x; w)otherwise.ywvT xv + h1 whT xh1 + h2 whT xh2 ,(4)features xh weights wh contribute estimation hidden variable h.may include features xv xnv , different features.features assessed independently noun phrase, yielding xh1 antecedentxh2 anaphor.model reflects intuition gesture features (measured xnv ) relevantgestures noun phrases salient. Thus, features contributetowards overall potential h1 = h2 = 1.4.2.3 Different-Zero Modelmay add flexibility model permitting weights verbal featureschange hidden variable. model called different-zero model, sincedifferent set verbal weights (wv,1 wv,2 ) used depending value hiddenvariable. model motivated empirical research showing language usage different used combination meaningful non-verbal communicationused unimodally (Kehler, 2000; Melinger & Levelt, 2004).formal definition potential function is:(x + wT x ) + h wT x + h wT x , h = h = 1y(wv,1v1 h h12 h h212nv nvdz (y, h, x; w)ywv,2 xv + h1 wh xh1 + h2 wh xh2 ,otherwise.366(5)fiGesture Salience Hidden Variable4.2.4 Modelspresented three models increasing complexity; different-different modelone step complex, including two pairs weight vectors verbal gesturalfeatures (see Equation 6). model, distinction verbal non-verbalfeatures (xv xnv ) evaporates, reason hidden variable hactually indicate relevance non-verbal features. addition, high degreefreedom model may lead overfitting.(x + wT x ) + h wT x + h wT x , h = h = 1y(wv,1v1 h h12 h h212nv,1 nvdd (y, h, x; w)x + wT x ) + h wT x + h wT x , otherwise.y(wv,2v1 h h12 h h2nv,2 nv(6)models considered assume verbal features always relevant, gesture features may sometimes ignored. words,considered whether might necessary assess salience verbal features. Onemight consider alternative potential functions zero-same model,verbal features sometimes ignored. consider models, gesture unaccompanied speech extremely rare dataset.4.3 Implementationobjective function (Equation 1) optimized using Java implementation L-BFGS,quasi-Newton numerical optimization technique (Liu & Nocedal, 1989). Standard L2norm regularization employed prevent overfitting, cross-validation selectregularization constant. Java source code available online:http://rationale.csail.mit.edu/gestureAlthough standard logistic regression optimizes convex objective, inclusionhidden variable renders objective non-convex. Thus, convergence global optimumguaranteed, results may differ depending initialization. Nonetheless, nonconvexity encountered many models natural language processing machinelearning generally, Baum-Welch training hidden Markov models (HMMs) (Rabiner, 1989) hidden-state conditional random fields (Quattoni et al., 2004; Sutton &McCallum, 2006). Often, results shown reasonably robust initialization;otherwise, multiple restarts used obtain greater stability. present empiricalevaluation Section 6.2 showing results overly sensitive initialization.experiments, weights initialized zero, enabling results reproduceddeterministically.5. FeaturesCoreference resolution studied thirty years AI community (Sidner, 1979; Kameyama, 1986; Brennan, Friedman, & Pollard, 1987; Lappin & Leass, 1994;Walker, 1998; Strube & Hahn, 1999; Soon, Ng, & Lim, 2001; Ng & Cardie, 2002). Basedlarge body work, broad consensus core set useful verbal features. paper contributes literature study gesture features,multimodal coreference resolution identifying salient gestures. describe367fiEisenstein, Barzilay, & Davisfeatureedit-distanceexact-matchstr-matchnonpro-strtypesimilaritysimilaritysimilaritysimilaritypro-strj-substring-ii-substring-joverlapnp-distsent-distboth-subjsame-verbnumber-matchsimilaritysimilaritysimilaritysimilaritycentering-basedcentering-basedcentering-basedcentering-basedcompatibilitypronouncounthas-modifiersindef-npdef-npdem-nplexical featurescentering-basedcentering-basedcentering-basedcentering-basedcentering-basedcentering-basedcentering-basedPairwise verbal featuresdescriptionnumerical measure string similarity two NPstrue two NPs identicaltrue NPs identical removing articlestrue antecedent anaphor j pronouns,str-match truetrue j pronouns, str-match truetrue j substringtrue substring jtrue shared words jnumber noun phrases j documentnumber sentences j documenttrue j precede first verb sentencestrue first verb sentences j identicaltrue j numberSingle-phrase verbal featurestrue NP pronounnumber times NP appears documenttrue NP adjective modifierstrue NP indefinite NP (e.g., fish)true NP definite NP (e.g., scooter )true NP begins this, that, these,lexical features defined common pronouns: it, that,this,Table 1: set verbal features multimodal coreference resolution. table,refers antecedent noun phrase j refers anaphor.features Sections 5.2 5.3, begin review verbal featuresselected literature.5.1 Verbal Featuresselection verbal features motivated extensive empirical literature textbased coreference resolution (Soon et al., 2001; Ng & Cardie, 2002; Strube & Muller, 2003;Daume III & Marcu, 2005). proliferation variety features exploredconsequence fact coreference complex discourse phenomenon. Moreover,realization coreference highly dependent type discourse appears;relevant factors include modality (e.g., speech vs. language), genre (e.g., meeting vs.lecture) topic (e.g., politics vs. scientific subject). Although certain feature typesapplication-specific, three classes features centering-based, similarity, compatibilityfeatures useful across coreference applications. classes form basisverbal features used model. Table 1 provides brief description verbal featureset. draw examples transcript Appendix provide detailedexplanation features motivate use application.368fiGesture Salience Hidden Variablefocus-distanceDTW-agreementsame-cluster*JS-div*dist-to-restjitterspeedrest-cluster*movement-cluster*Pairwise gesture featuresEuclidean distance pixels average hand position twoNPsmeasure agreement hand-trajectories two NPs, computedusing dynamic time warpingtrue hand positions two NPs fall clusterJensen-Shannon divergence cluster assignment likelihoodsSingle-phrase gesture featuresdistance hand rest positionsum instantaneous motion across NPtotal displacement NP, divided durationtrue hand usually cluster associated rest positiontrue hand usually cluster associated movementTable 2: set gesture features multimodal coreference resolution. Featuresused prior work gesture analysis annotated asterisk (*).Centering-related features: set features captures relative prominencediscourse entity local discourse, likelihood act coreferentgiven phrase. features inspired linguistic analysis formalized Centering Theory, links coreferential status entity discourse prominence (Grosz, Joshi, & Weinstein, 1995; Walker, Joshi, & Prince, 1998; Strube &Hahn, 1999; Poesio, Stevenson, Eugenio, & Hitzeman, 2004; Kibble & Power, 2004).theory hypothesizes point coherent discourse, one entityfocus characterizes local discourse terms focus transitionsadjacent sentences.existing machine-learning based coreference systems attemptfully implement Centering-style analysis.11 Instead, number centering-related features included. instance, identify focus-preserving transitions (i.e., CONTINUE transitions) feature both-subj introduced. According theory,transitions common locally-coherent discourse, therefore coreferenceassignments consistent principle may preferable. also characterizetransitions terms span (np-dist sent-dist). Transitions involveshort gaps preferred transitions long gaps.Another important set Centering-related features defined level singlephrase. syntactic role phrase sentence captured featurespronoun, has-modifiers, indef-np indicates discourse prominencetherefore likelihood coreference antecedent. example, considerutterance lines 12 13: spring active meaning goingdown. Here, anaphor clearly refers antecedent spring.11. implementation challenging several respects: one specify free parameterssystem (Poesio et al., 2004) determine ways combining effects various constraints.Additionally, implementation centering depends obtaining detailed syntactic information,available case.369fiEisenstein, Barzilay, & Davisfact antecedent demonstrative noun phrase (beginning this)12anaphor pronoun also centering-related features suggestcoreference likely. addition syntactic status, also take accountfrequency noun phrase monologue (see count). Frequency informationcommonly used approximate topical salience entity text (Barzilay &Lapata, 2005).Similarity features: simple yet informative set coreference cues basedstring-level similarity noun phrases. instance, referencespring line 12 identical noun phrase line 5 resolved exactmatch surface forms. general, researchers text-based coreference resolutionfound string match feature single predictive featurediscourse entity commonly described using identical similar noun phrases (Soonet al., 2001).system, similarity information captured seven features quantifydegree string overlap. instance, feature (exact-match) indicates fulloverlap noun phrases, feature (overlap) captures whether twophrases share common words. context coreference resolution, nounphrase match informative pronoun match, use distinct featuresmatching strings syntactic categories (e.g., nonpro-str vs. pro-str),following (Ng & Cardie, 2002). Surface similarity may also quantified termsedit-distance (Strube, Rapp, & Muller, 2002).Compatibility features: important source coreference information compatibility two noun phrases. instance, utterance ball line 11refer preceding noun phrase things, since incompatible number. Feature number-match captures information. Since topicdiscourse corpus relates mechanical devices, almost noun phrasesneuter-gendered. eliminates utility features measure gender compatibility. Finally, note complex semantic compatibility featurespreviously explored (Harabagiu, Bunescu, & Maiorano, 2001; Strube et al., 2002;Yang, Zhou, Su, & Tan, 2003; Ji, Westbrook, & Grishman, 2005; Daume III & Marcu,2005; Yang, Su, & Tan, 2005).features traditionally used coreference avoided here. Featuresdepend punctuation seem unlikely applicable automatic recognitionsetting, least near future. addition, many systems MUC ACEcoreference corpora use gazetteers list names nations business entities,features relevant corpus. Another possibility use features identifyingspeaker, capture individual variation patterns reference (Chai, Hong, Zhou, &Prasov, 2004; Jordan & Walker, 2005). However, wished develop approachspeaker-independent.12. Simple string matching techniques used assess phrase types: definite noun phrasesbeginning article the; indefinite noun phrases begin an; demonstrative nounphrases begin this. Bare plurals marked indefinites, proper names appeardataset.370fiGesture Salience Hidden Variable5.2 Non-Verbal Featuresnon-verbal features attempt capture similarity speakers hand gestures,similar gestures suggest semantic similarity (McNeill, 1992; Quek et al., 2002a).example, two noun phrases may likely corefer accompaniedidentical pointing gestures. section, describe features quantify variousaspects gestural similarity.general, features computed duration noun phrase, yielding single feature vector per NP. universally true beginningend points relevant gestures line exactly beginning end associated words, several experiments demonstrated close synchrony gesturespeech (McNeill, 1992). future work, hope explore whether sophisticatedgesture segmentation techniques improve performance.straightforward measure similarity Euclidean distanceaverage hand position noun phrase call focus-distance.13 Euclideandistance captures cases speaker performing gestural hold roughlylocation (McNeill, 1992). However, Euclidean distance may correlate directlysemantic similarity. example, gesturing detailed part diagram,small changes hand position may semantically meaningful, regionspositional similarity may defined loosely. Ideally, would compute semanticfeature capturing object speakers reference (e.g., red block),possible general, since complete taxonomy possible objects reference usuallyunknown.Instead, perform spatio-temporal clustering hand position velocity, usinghidden Markov model (HMM). Hand position speed used observations,assumed generated Gaussians, indexed model states. statescorrespond clusters, cluster membership used discretized representationpositional similarity. Inference state membership learning model parametersperformed using traditional forward-backward Baum-Welch algorithms (Rabiner,1989).standard hidden Markov model may suitable, increase robustnessmake better use available training data reducing models degrees-of-freedom.Reducing number degrees-of-freedom means learning simpler models,often general. done parameter tying: requiring subsetsmodel parameters take values (Bishop, 2006). employ three formsparameter tying:1. one state permitted expected speed greater zero. statecalled move state; states hold states, speed observationsassumed generated zero-mean Gaussians. single move stateused concerned location hold gestures.2. Transitions distinct hold states permitted. reflects commonsense idea possible transition two distinct positions withoutmoving.13. general, features computed duration individual noun phrases.371fiEisenstein, Barzilay, & Davis3. outgoing transition probabilities hold states assumed identical.Intuitively, means likelihood remaining within hold statedepend hold located. possible imagine scenarioshold, reasonable simplification dramatically reducesnumber parameters estimated.Two similarity features derived spatio-temporal clustering. samecluster feature reports whether two gestures occupy state majoritydurations two noun phrases. boolean feature indicates whethertwo gestures roughly area, without need explicit discretization step.However, two nearby gestures may classified similar method,near boundary two states, gestures move multiple states.reason, quantify similarity state assignment probabilities usingJensen-Shannon divergence, metric probability distributions (Lin, 1991). JS-divreal-valued feature provides nuanced view gesture similarity basedHMM clustering. same-cluster JS-div computed independently modelscomprising five, ten, fifteen states.gesture features described thus far largely capture similarity staticgestures; is, gestures hand position nearly constant. However,features capture similarity gesture trajectories, may also usedcommunicate meaning. example, description two identical motions mightexpressed similar gesture trajectories. measure similaritydynamic gestures, use dynamic time warping (Huang, Acero, & Hon, 2001);reported DTW-distance feature. Dynamic time warping used frequentlyrecognition predefined gestures (Darrell & Pentland, 1993).features computed hand body pixel coordinates, obtainedautomatically via computer vision, without manual post-processing kind (see Section 3.3). feature set currently supports single-hand gestures, using handfarthest body center. verbal feature set, Wekas default supervisedbinning class applied continuous-valued features (Fayyad & Irani, 1993).14method identifies cut points minimize class-wise impurity side cut,measured using average class entropy. greedy top-down approach used recursivelypartition domain attribute value. Partitioning terminated criterion basedminimum description length.5.3 Meta FeaturesMeta features observable properties speech gesture give clueswhether speaker gesturing way meaningful language languageprocessing task hand. hypothesize difference relevant irrelevant hand motions apparent range verbal visual features. equations 4-6,features represented xh1 xh2 . Unlike similarity-based features described above, meta features must computable single instant time, encodeproperties individual gestures cotemporal NPs.14. specific class weka.filters.supervised.attribute.Discretize.372fiGesture Salience Hidden VariablePrevious research investigated types verbal utterances likelyaccompanied gestural communication (Melinger & Levelt, 2004; Kehler, 2000). However,first attempt formalize relationship context machine-learningapproach predicts gesture salience.5.3.1 Verbal Meta FeaturesMeaningful gesture shown frequent associated speechambiguous (Melinger & Levelt, 2004). Kehler (2000) finds fully-specified noun phrasesless likely receive multimodal support. findings lead us expect gestureslikely co-occur pronouns, unlikely co-occur noun phrasesbegin determiner the, particularly include adjectival modifiers. captureintuitions, single-phrase verbal features (Table 1) included meta-features.5.3.2 Non-verbal Meta FeaturesResearch gesture shown semantically meaningful hand motions usually takeplace away rest position, located speakers lap sides (McNeill,1992). Effortful movements away default positions thus expectedpredict gesture used communicate. identify rest position centerbody x-axis, fixed, predefined location y-axis. dist-torest feature computes average Euclidean distance hand rest position,duration NP.Hand speed may also related gesture salience. speed feature capturesoverall displacement (in pixels) divided length noun phrase. Writing xhand position {1, 2, . . . , } time index, speed = T1 ||xT x1 ||2 .Pjitter feature captures average instantaneous speed: jitter = T1 Tt=2 (xt xt1 )T (xtxt1 ). feature captures periodic jittery motion, quantifiedspeed feature end position far original position. Also, high jitteroften indicates tracker lost hand position, would excellentreason ignore gesture features.noted previous section, HMM used perform spatio-temporalclustering hand positions velocities. rest-cluster feature takes valuetrue iff frequently occupied state NP closest rest position.addition, parameter tying used HMM ensure states onestatic holds, remaining state represents transition movementsholds. last state permitted expected non-zero speed; handfrequently state NP, movement-cluster feature takesvalue true.6. Evaluation Coreference Resolutionevaluation, assess whether gesture features improve coreference resolution,compare conditional modality fusion approaches gesture-speech combination.373fiEisenstein, Barzilay, & Davis6.1 Evaluation Setupdescribe procedure evaluating performance approach. includesevaluation metric (Section 6.1.1), baselines comparison (Section 6.1.2), parametertuning (Section 6.1.3). coreference annotations described Section 3.2.6.1.1 Evaluation MetricCoreference resolution often performed two phases: binary classification phase,likelihood coreference pair noun phrases assessed; globalpartitioning phase, clusters mutually-coreferring NPs formed. modeladdress global partitioning phase, question whether pairnoun phrases document corefer. Moving pairwise noun phrase coreferenceglobal partitioning requires clustering step may obscure performance differenceslevel model operates. Moreover, results depend choiceclustering algorithm mechanism selecting number clusters (or,alternatively, cut-off value merging clusters). parameterization particularlychallenging corpus large dedicated development set. Consequently, bulk evaluation performed binary classification phase. However, purpose comparing prior work coreference, also perform globalevaluation, measures overall results clustering.binary evaluation, use area ROC curve (auc) error metric (Bradley, 1997). auc evaluates classifier performance without requiring specificationcutoff. metric penalizes misorderings cases classifier ranks negativeexamples highly positive examples. ROC analysis increasingly popular,used variety NLP tasks, including detection action itemsemails (Bennett & Carbonell, 2007) topic segmentation (Malioutov & Barzilay, 2006).Although binary evaluation typically used coreference resolution, believeappropriate choice here, reasons noted above.global evaluation uses constrained entity-alignment f-measure (ceaf) evaluation (Luo, 2005). metric avoids well-known problems earlier MUC evaluationmetric (Vilain, Burger, Aberdeen, Connolly, & Hirschman, 1995). clustering stepperformed using two standard techniques literature, describe Section 6.3. future work plan explore techniques perform coreference singlejoint step (e.g., Daume III & Marcu, 2005). global metric would appropriatemeasure contributions model directly.6.1.2 BaselinesConditional modality fusion (cmf) compared traditional approaches modalitycombination NLP tasks:Early fusion. early fusion baseline includes features single vector,ignoring modality. equivalent standard maximum-entropy classification.Early fusion implemented conditionally-trained log-linear classifier; usescode cmf model, always includes features.374fiGesture Salience Hidden VariableLate fusion. two late fusion baselines train separate classifiers gesturespeech, combine posteriors. modality-specific classifiers conditionallytrained log-linear classifiers, use code cmf model.simplicity, parameter sweep identifies interpolation weights maximize performance test set. Thus, likely results somewhat overestimateperformance baseline models. additive multiplicative combination considered.fusion. baselines include features single modality,build conditionally-trained log-linear classifier. Implementation usescode cmf model, weights features outside target modality forcedzero.important question results compare existing state-of-the-art coreference systems. fusion, verbal features baseline provides reasonable representation prior work coreference, applying maximum-entropy classifier settypical textual features. direct comparison existing implemented systems wouldideal, available systems use textual features inapplicablespoken-language dataset, punctuation, capitalization, gazetteers.6.1.3 Parameter Tuningsmall size corpus permit dedicated test training sets, resultscomputed using leave-one-out cross-validation, one fold sixteendocuments corpus. Parameter tuning performed using cross validation withintraining fold. includes selection regularization constant, controlstrade-off fitting training data learning model simpler (andthus, potentially general). addition, binning continuous features performedwithin cross-validation fold, using method described Section 5.2. Finally,noted above, model weights initialized zero, enabling deterministic reproducibilityexperiments.6.2 ResultsConditional modality fusion outperforms approaches statistically significantmargin (Table 4). Compared early fusion, different-zero model conditionalmodality fusion offers absolute improvement 1.17% area ROC curve (auc)compare lines 1 4 table. paired t-test shows result statisticallysignificant (p < .01, t(15) = 3.73). cmf obtains higher performance fourteensixteen cross-validation folds. additive multiplicative late fusion perform parearly fusion. p-values significance tests pairwise comparisonsshown Table 5.Early fusion gesture features superior unimodal verbal classificationabsolute improvement 1.64% auc (p < .01, t(15) = 4.45) compare lines 4 7Table 4. additional 1.17% auc provided conditional modality fusion amountsrelative 73% increase power gesture features. results relatively robustvariations regularization constant, shown Figure 3. means375fiEisenstein, Barzilay, & Daviscmf different-different (DD)cmf different-zero (DZ)cmf same-zero (SZ)Early fusion (E)Late fusion, multiplicative (LM)Late fusion, additive (LA)fusion (VO, GO)Uses two different sets weights verbal gestural features, dependinghidden variable (equation 6).Uses different weights verbal featuresdepending hidden variable; hidden variable indicates non-salience, gestureweights set zero (equation 5).Uses weights verbal features regardless gesture salience; hidden variable indicates non-salience, gesture weightsset zero (equation 4).Standard log-linear classifier. Usesweights verbal gestural features, regardless hidden variable (equation 3).Trains separate log-linear classifiers gesture verbal features. Combines posteriorsmultiplication.Trains separate log-linear classifiers gesture verbal features. Combines posteriorsinterpolation.Uses one modality classification.Table 3: Summary systems compared evaluationmodel1. cmf different-zero2. cmf different-different3. cmf same-zero4. Early fusion (same-same)5. Late fusion, multiplicative6. Late fusion, additive7. fusion (verbal features only)8. fusion (gesture features only)auc.8226.8174.8084.8109.8103.8068.7945.6732Table 4: Coreference performance, area ROC curve (auc). systemsdescribed Table 3376fiGesture Salience Hidden VariableDD.01cmf different-zero (DZ)cmf different-different (DD)cmf same-zero (SZ)Early fusion (E)Late fusion, multiplicative (LM)Late fusion, additive (LA)Verbal features (VO)Gesture features (GO)SZ.01.05E.01nsnsLM.01nsnsnsLA.01.05nsnsnsVO.01.01.05.01.01.01GO.01.01.01.01.01.01.01Table 5: P-values pairwise comparison models. ns indicatesdifference model performance significant p < .05. parenthesesleft column explain abbreviations top line.Comparison modelsComparison among CMF models0.82area ROC curvearea ROC curve0.820.810.80.79CMF (diffzero)early fusionverbalonly0.780.77123456log regularization constant70.810.80.79differentzerodifferentdifferentsamezero0.780.778123456log regularization constant78Figure 3: Results regularization constantperformance gains obtained conditional modality fusion highly dependentfinding optimal regularization constant.noted Section 4.3, conditional modality fusion optimizes non-convex objective.perform additional evaluation determine whether performance sensitive initialization. Randomizing weights five different iterations best-performingsystem, observed standard deviation 1.09 103 area ROC curve(auc). experiments weights initialized zero, enabling resultsreproduced deterministically.6.3 Global MetricCoreference traditionally evaluated global error metric. research directedspecifically binary classification coreference pairs noun phrases,377fiEisenstein, Barzilay, & Davismodelcmf (different-zero)cmf (different-different)cmf (same-zero)Early fusion (same-same)Late fusion, multiplicativeLate fusion, additivefusion (verbal features only)fusion (gesture features only)first-antecedent55.6754.7153.9154.1853.7453.5653.4744.68best-antecedent56.0256.2055.3255.5054.4455.9455.1544.85Table 6: ceaf global evaluation scores, using best clustering thresholdFirstantecedent clusteringBestantecedent clustering0.50.5CEAF0.55CEAF0.550.450.450.40.4CMF (diffzero)late fusion, additiveverbal0.350.10.20.30.40.5clustering thresholdCMF (diffzero)late fusion, additiveverbal0.350.10.60.20.30.40.5clustering threshold0.6Figure 4: Global coreference performance, measured using ceaf scores, plottedthreshold clusteringfocused evaluating specific portion larger coreference problem. However,purpose comparing prior research coreference, present results usingtraditional global metric.perform global evaluation, must cluster noun phrases document, usingpairwise coreference likelihoods similarity metric. experiment two clustering methods commonly used literature. first-antecedent techniqueresolves NPs first antecedent whose similarity predefined threshold (Soonet al., 2001). best-antecedent technique resolves noun phrase compatible prior noun phrase, unless none threshold (Ng & Cardie, 2002).Figure 4 shows global scores, plotted value clustering threshold.clarity, best performing system class shown: conditional378fiGesture Salience Hidden Variablemodality fusion, plot different-zero model; multimodal baselines, plotadditive late fusion model (the combination additive late fusion best-antecedentclustering method best performing multimodal baseline); unimodal baseline,plot verbal-features baseline. Table 6 lists performance methodoptimum clustering threshold. comparison, Ng reports ceaf score 62.3 (Ng,2007) ACE dataset, although results directly comparable duedifferences corpora.shown results, performance sensitive clustering methodclustering threshold. Conditional modality fusion generally achieves best results,best-antecedent clustering generally outperforms first-antecedent technique. Nonetheless, advantage conditional modality fusion smaller ROC analysis.believe ROC analysis demonstrates advantage conditional modality fusiondirectly, global metric interposes clustering step obscures differencesclassification techniques. Nonetheless, global metric may better overall measure quality coreference downstream applications searchsummarization. future work, hope investigate whether conditional modalityfusion approach applied global models coreference require separateclassification clustering phases (e.g., Daume III & Marcu, 2005).6.4 Feature Analysismachine learning approach adopted permits novel analysiscompare linguistic contribution gesture features presence verbalfeatures. Thus investigate gesture features supply unique informationverbal features. addition, analyze types verbal features correlateclosely gesture features, independent. statistical significance resultsbased two-tailed, paired t-tests.6.4.1 Gestural SimilarityFigure 5 shows contribution three classes gestural similarity features: focusdistance, DTW-agreement, two HMM-based features (same-cluster JSdiv). top dotted line graph shows performance different-zero modelcomplete feature set, bottom line shows performance model withoutgestural similarity features.15feature group conveys useful information, performance one featuregroup always better performance without gestural similarity features (p < .01, t(15) =3.86 DTW-agreement, weakest three feature groups). performance using focus-distance significantly better DTW-agreementfeature used (p < .05, t(15) = 2.44); comparisons significant. also find15. Note baseline gesture features higher fusion (verbal features only)baseline Table 4. Although feature groups identical, classifiers different.fusion (verbal features only) baseline uses standard log-linear classifier, gesturefeatures uses conditional modality fusion, permitting two sets weights verbal features,shown equation 5.379fiEisenstein, Barzilay, & Davis0.83feature absentothers absent0.825gesture featuresfeature groupgesture similarity featuresfocus-distanceDTW-agreementHMM-basedAUC0.820.8150.81+.8226.8200.8149.8183.8054.8200.8234.8198gesture features0.8050.8distanceDTWHMMbasedFigure 5: analysis contributions set gestural similarity features.plus column left table shows results feature setpresent; minus column shows results removed. before,metric area ROC curve (auc).evidence redundancy feature groups, removing individual featuregroup significantly impair performance two feature groups remain.6.4.2 Verbal Gestural OverlapNext, assess degree overlap gesture verbal information. hypothesize gesture complementary certain verbal features, redundantothers. example, string match features edit-distance exact-matchseem unlikely convey information gesture. see why, consider casesstring match likely helpful: fully-specified noun phrases redball, rather pronouns. Empirical research suggests majority informativegestures occur pronouns underspecified utterances, string matchunlikely helpful (Kehler, 2000). Thus, expect low level overlapgesture string match features.Distributional features another source verbal information. include number intervening sentences noun phrases two candidate NPs,number times NP appears document. features establish contextmay permit resolution references ambiguous surface form alone.example, noun phrase occurred recently, often, pronominal referencemay sufficiently clear. Since gesture may also used cases, expectredundancy gestural similarity distributional features.intuitions lead us specific predictions system performance. presencegesture similarity features mitigate cost removing distributionalfeatures, gesture features reinforce information. However,presence gesture features effect cost removing stringmatch features.380fiGesture Salience Hidden Variable0.010Cost AUC0.01feature group0.02string matchdistance, count0.03gesture similarity features.7721.8258without.7522.80180.040.05gesturewithout gesture0.06string matchdistance countFigure 6: contribution verbal features, without gesture similarity features.graph shows loss incurred removing verbal feature class, conditioned presence gesture similarity features. table shows overallperformance combination feature groups.predictions supported data (Figure 6). Removing distributionalfeatures impair performance long gesture features present,impair performance gesture features also removed difference statisticallysignificant (p < .01, t(15) = 3.76). observation consistent hypothesisfeature groups convey similar information. contrast, cost removingstring match features vary significant margin, regardless whethergesture features present. accords intuition feature groupsconvey independent information.7. Evaluation Keyframe Extractionprevious sections describe application conditional modality fusion naturallanguage processing: using gesture features meaningful, contribution coreference classification enhanced. section, show conditionalmodality fusion also predicts gestures useful human viewers. Specifically,use conditional modality fusion estimate gesture salience select keyframesvideo. demonstrate keyframes selected method match selectedhuman raters better keyframes selected traditional text image-basedalgorithms.Section 7.1, explain keyframe-based summarization task. describebasic modeling approach Section 7.2. evaluation setup presented Section 7.3.Section 7.4 gives experimental results.7.1 Keyframe-Based Summarizationgoal produce comic book summary video, transcript augmented salient keyframes still images clarify accompanying text. Keyframe381fiEisenstein, Barzilay, & Davisbased summaries allow viewers quickly review key points video presentation, withoutrequiring time hardware necessary view actual video (Boreczky, Girgensohn,Golovchinsky, & Uchihashi, 2000). argued above, textual transcriptions alonecapture relevant information, keyframe-based summary may provide minimal visual information required understand presentation. Appendix B containsexcerpt summary produced system.noted, gesture supplements speech unique semantic information. Thus, keyframesshowing salient gestures would valuable addition transcript text. Ideally,would select keyframes avoid redundancy visual verbal modalities,conveying relevant information.Existing techniques keyframe extraction usually focused edited videosnews broadcasts (e.g., Uchihashi, Foote, Girgensohn, & Boreczky, 1999; Boreczky et al.,2000; Zhu, Fan, Elmagarmid, & Wu, 2003). systems seek detect large-scale changesimage features identify different scenes, choose representative examplescene. approach poorly suited unedited videos, recordingclassroom lecture business presentation. videos, key visual informationvariation scenes camera angles, visual communication providedgestures speaker. goal capture relevant keyframes identifying salientgestures, using model developed previous sections paper.7.2 Modeling ApproachOne possible approach formulate gesture extraction standard supervised learningtask, using corpus salient gestures annotated. However, annotationexpensive, prefer avoid it. Instead learn salience bootstrappingmultimodal coreference resolution, using conditional modality fusion. learning predictspecific instances gesture helps, obtain model gesture salience.example, expect pointing gesture presence anaphoric expressionwould found highly salient (as Figure 1); ambiguous hand posepresence fully-specified noun phrase would salient. approachidentify salient gestures, identify occur context selectedlanguage understanding task. coreference resolution, gestures co-occurnoun phrases selected. noun phrases ubiquitous language, stillcover usefully broad collection gestures.Using model coreference resolution introduced Section 4, obtain probability distribution hidden variable, controls whether gesture featuresincluded coreference resolution. basic hypothesis instances gesturefeatures included high likelihood likely correspond salient gestures.gestures rated salient method used select keyframes summary.Models coreference resolution gesture salience learned jointly, basedsame-zero model defined Equation 4. training, set weights wh obtained,allowing estimation gesturesum possibleP salience noun phrase.values h2 , obtaining y,h2 (y, h, x; w) = h1 wh xh1 . find potential382fiGesture Salience Hidden Variablecase gesture salient setting h1 = 1, yielding whT xh1 .16 workingassumption potential reasonable proxy informativeness keyframedisplays noun phrases accompanying gesture.potential provides ordering noun phrases document. selectkeyframes midpoints top n noun phrases, n specified advanceannotator. Providing system ground truth number keyframes followscommon practice textual summarization literature summaries different lengthsdifficult compare, summary duration governed partially annotatorspreference brevity completeness (Mani & Maybury, 1999). keyframe givencaption includes relevant noun phrase accompanying text, nounphrase next keyframe. Portions output system shown Figure 1Appendix B.7.3 Evaluation Setupevaluation methodology similar intrinsic evaluation developed Document Understanding Conference.17 assess quality automatically extractedkeyframes comparing human-annotated ground truth.7.3.1 Datasetdataset consists dialogues collected using procedure described Section 3.coreference annotations described Section 3.2 used. Additionally, ninesixteen videos annotated keyframes. these, three used developingsystem baselines, remaining six used final evaluation (theseindicated asterisks table Appendix C). explicit trainingkeyframe annotations, development set used evaluation systemconstruction.specification ground truth annotation required keyframes capturestatic visual information annotator deems crucial understanding contentvideo. number selected frames left discretion; average, 17.8keyframes selected per document, average total 4296 frames per document.Annotation performed two raters; subset two videos annotated raters,raw interrater agreement 86%, yielding kappa .52.One important difference dataset standard sentence extraction datasetsmany frames may nearly identical, due high frame rate video.reason, rather annotating individual frames, annotators marked regionsidentical visual information. regions define equivalence classes, framegiven region would equally acceptable. single keyframe selectedevery ground truth region, result would minimal set keyframes necessaryreader fully understand discourse. average, 17.8 regions selected perdocument spanned 568 frames.16. Note consider noun phrase anaphor (xh2 ) sum possible valuesh1 , resulting potential identical.17. http://duc.nist.gov383fiEisenstein, Barzilay, & DavisGround truth12System responseScoreFalseNegativeFalsePositiveTruePositivescoredFigure 7: example scoring setup.7.3.2 Training Coreference Resolutiondescribed Section 7.2, approach keyframe extraction based modelgesture salience learned labeled data coreference resolution. trainingphase performed leave-one-out cross-validation: separate set weights learnedpresentation, using fifteen presentations training set. learnedweights used obtain values hidden variable indicating gesture salience,described previous subsection.7.3.3 Evaluation MetricFigure 7 illustrates scoring setup. top row figure represents groundtruth; middle row represents system response, vertical lines indicating selectedkeyframes; bottom row shows response scored.systems number keyframes fixed equal number regionsground truth annotation. system response includes keyframewithin ground truth region, false positive recorded. system response failsinclude keyframe region ground truth, false negative recorded; truepositive recorded first frame selected given ground truth region,additional frames region scored. system thus still penalizedredundant keyframe, wasted one finite number keyframesallowed select. time, error seems less grave true substitutionerror, keyframe containing relevant visual information selected. reportF-measure, harmonic mean recall precision.7.3.4 Baselinescompare performance system three baselines, presentorder increasing competitiveness.Random-keyframe: simplest baseline selects n keyframes randomthroughout document. baseline similar random sentence baselinescommon textual summarization literature (Mani & Maybury, 1999).number keyframes selected baseline equal number regionsground truth. baseline expresses lower bound performancereasonable system achieve task. results report average500 independent runs.384fiGesture Salience Hidden VariableNP-salience: NP-salience system based frequency-based approachesidentifying salient NPs purpose text summarization (Mani & Maybury,1999). salience heuristic prefers common representative tokenslargest homogeneous coreference clusters.18 largest cluster onecontaining noun phrases; homogeneity measured inversenumber unique surface forms. provides total ordering NPs document; select keyframes midpoint top n noun phrases, nnumber keyframe regions ground truth. future work hope explorefinding best point within noun phrase keyframe selection.Pose-clustering: final baseline based purely visual features. employsclustering find representative subset frames minimum mutual redundancy.Uchihashi et al. (1999), seminal paper keyframe selection, perform clusteringframes video, using similarity color histograms distancemetric. Representative images cluster used keyframes.recent video summarization techniques advanced clustering algorithms (Liu& Kender, 2007) similarity metric (Zhu et al., 2003), basic approachforming clusters based visual similarity choosing exemplar keyframesclusters still used much state-of-the-art research topic (seeLew, Sebe, Djeraba, & Jain, 2006, survey).dataset, single fixed camera change video exceptmovements speaker; thus, color histograms nearly constantthroughout. Instead, use tracked coordinates speakers hands upperbody, normalize values, use Euclidean distance metric. setting,clusters correspond typical body poses, segments correspond holdsposes. Following Uchihashi et al. (1999), video divided segmentscluster membership constant, keyframes taken midpoints segments.use importance metric paper ranking segments, choose topn, n number keyframes ground truth.7.4 ResultsTable 7 compares performance method (Gesture-salience) threebaselines. Using paired t-tests, find Gesture-salience significantly outperformsalternatives (p < .05 cases). Pose-clustering NP-salience systemsstatistically equivalent; significantly better Random-keyframe baseline(p < .05).set baselines system compared necessarily incomplete,many ways keyframes extraction could performed. example, prosodic features could used identify moments particular interest dialogue (Sundaram & Chang, 2003). addition, combination baselines including visuallinguistic features may also perform better individual baseline. However, developing complicated baselines somewhat beside point. evaluation demonstrates simple yet effective technique selecting meaningful keyframes obtained18. Here, coreference clusters based manual annotations.385fiEisenstein, Barzilay, & DavisModelGESTURE-SALIENCEPose-clusteringNP-salienceRandom-keyframeF-Measure.404.290.239.120Recall.383.290.234.119Precision.427.290.245.121Table 7: Comparison performance keyframe selection taskbyproduct conditional modality fusion. suggests estimates gesturesalience given model cohere human perception.Error analysis manual inspection system output revealed many casessystem selected noun phrase accompanied relevant gesture, specifickeyframe slightly off. method always chooses keyframe midpointaccompanying noun phrase; often, relevant gesture brief, necessarilyoverlap middle noun phrase. Thus, one promising approach improvingresults would look inside noun phrase, using local gesture features attemptidentify specific frame gesture salient.errors arise key gestures related noun phrases.example, suppose speaker says shoots ball up, accompanies wordgesture indicating balls trajectory. gesture might importantunderstanding speakers meaning, since overlap noun phrase,gesture identified system. believe results showfocusing noun phrases good start linguistically-motivated keyframe extraction,unsupervised approach successful identifying noun phrases requirekeyframes. gesture applied language tasks, hope model saliencegesture phrase types, thus increasing coverage approach keyframeextraction.8. Conclusions Future Worksummary, work motivated idea gestures best interpretedindividual units self-contained meaning, context gesturallinguistic information. Previous NLP research gesture largely focused buildingrecognizers gestures characterize specific language phenomena: example, detecting hand gestures cue sentence boundaries (Chen et al., 2006), body languagesuggests topic shifts (Nakano et al., 2003). approaches treat gesture sortvisual punctuation. contrast, interested semantics gesture carries.take recognition-based approach believe unlikely spacepossible meaningful gestures could delineated training set. Instead, starthypothesis patterns gesture correspond patterns meaning,degree similarity two gestures predicts semantic similarity associatedspeech. approach validated experimental results, show substantialimprovement noun phrase coreference resolution using gesture features. one386fiGesture Salience Hidden Variablefirst results showing automatically-extracted visual features significantly improvediscourse analysis.second key finding structured approach multimodal integration crucialachieving full benefits offered gesture features language understanding. Ratherbuilding separate verbal gesture interpretation units simply concatenatingfeatures build model whose structure encodes role modality.particular, gesture modality supplements speech intermittently, thereforerepresent gesture salience explicitly hidden variable. approach, callconditional modality fusion, yields 73% relative improvement contributiongesture features towards coreference resolution. improvement attained modelinggesture salience hidden variable ignoring gestures salient.Conditional modality fusion induces estimate gesture salience within contextspecific linguistic task. test generality salience model, transfer derivedestimates completely different task: keyframe extraction. Without labeled datakeyframe task, simple algorithm outperforms competitive unimodal alternatives.suggests model gesture salience learned coreference cohereshuman perception gesture salience.theme generality gesture salience suggestive future research. principle, general model gesture salience could applied range discourse-relatedlanguage processing tasks. example, consider topic segmentation: text, changesdistribution lexical items strong indicator topic boundaries (Hearst, 1994).Assuming salient gestures carry unique semantic content, changes distributionfeatures salient gestures could used similar way, supplementing purely textualanalysis.Moreover, combination multiple language processing tasks single joint framework raises possibility gesture salience could modeled robustly knowledge transferred tasks. argued using explicit universaltaxonomy gesture, favoring approach focused relevance gesture specificlanguage processing problems. However, joint framework would generalize notionsalience bottom-up, data-driven way. research may relevant purelylinguistic standpoint: example, investigating types language phenomena sharecoherent notions gesture salience, gesture salience expressed visuallinguistic features. paper argued structured models conditionalmodality fusion used incorporate linguistic ideas gesture principledway. hope future work show models also provide new toolstudy linguistics gesture.Another possibility future work investigate richer models gesture salience.structure explored minimal binary variable indicate saliencegesture coreference resolution. see first step towards complexstructural representations gesture salience may yield greater gains performance.example, likely gesture salience observes temporal regularity, suggestingMarkov state model. tasks may involve structured dependencies amonggestures, requiring models probabilistic context-free grammars.Finally, note hand gesture one several modalities accompany spokenlanguage. Prosody attracted greatest amount attention natural language387fiEisenstein, Barzilay, & Davisprocessing, coverbal modalities include facial expressions, body posture,settings lectures meetings writing diagrams. relationshipmodalities poorly understood; future research might explore mapping linguisticfunctions modalities, whether sets modalities redundantcomplementary.Acknowledgmentsauthors acknowledge editor anonymous reviewers helpful comments.also thank colleagues Aaron Adler, S. R. K. Branavan, Emma Brunskill, SonyaCates, Erdong Chen, C. Mario Christoudias, Michael Collins, Pawan Deshpande, Lisa Guttentag, Igor Malioutov, Max Van Kleek, Michael Oltmans, Tom Ouyang, Christina Sauper,Tom Yeh, Luke Zettlemoyer. authors acknowledge support NationalScience Foundation (Barzilay; CAREER grant IIS-0448168 grant IIS-0415865)Microsoft Faculty Fellowship. opinions, findings, conclusions recommendations expressed authors necessarily reflect viewsNational Science Foundation Microsoft.388fiGesture Salience Hidden VariableAppendix A. Example Transcript12345678910111213141516171819202122232425262728293031323334353637383940414243ok [(0) object right here].im going attempt explain [(0) this] you.[(1) ball right here][(1) ball][(2) spring][(3) long arm][(4) this] [(5) objects actually move].take [(6) that] back.[(7) this] rotates well.[(8) things] stay fixed.happens [(1) ball] comes [(9) here].[(2) spring] active.meaning [(2) its] going down.[(4) this] come up.jostle [(3) that].go around.[(3) itll] [(4) this] raises [(3) it] up.[(10) hand] goes down.[(10) itll] spring back up.[(1) ball] typically goes [(11) here].bounces [(12) here].gets caught like [(13) groove].[(7) this] continually moving around [(14) circle][(15) this] happened three timeswatched [(16) video] [(15) it] happened [(17) three times][(1) ball] never went [(18) there] [(19) here][(1) it] always would get back [(20) here][(9) here]sometimes [(21) thing] would hit [(1) it] harder[(1) it] would go highersometimes [(1) it] would kind loop[(1) it] came [(9) here]idea theres [(22) anchors] [(23) here][(24) that] wasnt really made clearyeah [(25) thats] pretty much [(26) it][(1) its] essentially [(1) bouncy ball][(1) it] pretty much drops like [(27) dead weight][(1) it] hits [(28) something][(26) it][(16) it] probably like [(16) forty five second video][(29) it] happened [(17) three times] [(16) video][(16) it] moves relatively quicklymuch lodged like [(1) it] would like come [(13) here]389fiEisenstein, Barzilay, & Davis444546474849505152535455565758[(7) this] moving [(7) it] wouldkind like dump [(1) it] [(20) here][(7) its] something thats [(30) way]actually [(31) transfer][(7) this] wasnt [(32) here][(1) it] would still fall [(20) here] get [(9) here]thatsim actually sure [(0) this][(0) it] looks like [(0) it] looks like[(0) this] [(33) computer screen]um [(0) it] basically looks like[(34) kind game pinball][(35) that] [(36) understanding it]im sure else [(37) its] supposedok done guys [(37) one]390fiGesture Salience Hidden VariableAppendix B. Example Keyframe Summaryexcerpt keyframe summary generated automatically, describedSection 7.spring brings thing back in.latches here. spring right here. thing dontknow is. goes like this.um goes level here.12goes down.goes out.34soon passes thing.comes back.56391fiEisenstein, Barzilay, & DavisAppendix C. Dataset Parametersnumber123*45*6*789*10*11121314*1516totalspeaker1122344556778899topicpinballcandy dispenserlatchpinballpinballcandy dispenserpinballpinballpistonpinballlatchboxpinballpinballpistonpinballcandy dispensergenderFFFFFFFduration3:002:271:192:313:003:003:003:003:003:002:203:002:230:472:302:4341:00# words455428104283325404421362313315347221192483783584954# NPs95101276569100109896971725147887771137Corpus statistics dataset used experiments. Asterisks indicate videosused keyframe evaluation.ReferencesAdler, A., Eisenstein, J., Oltmans, M., Guttentag, L., & Davis, R. (2004). Buildingdesign studio future. Proceedings AAAI Workshop Making Pen-BasedInteraction Intelligent Natural, pp. 17.Allen, J., Schubert, L., Ferguson, G., Heeman, P., Hwang, C., Kato, T., Light, M., Martin,N., Miller, B., Poesio, M., et al. (1995). TRAINS project: case study buildingconversational planning agent. Journal Experimental & Theoretical ArtificialIntelligence, 7 (1), 748.Barzilay, R., & Lapata, M. (2005). Modeling local coherence: entity-based approach.Proceedings ACL, pp. 141148.Bennett, P., & Carbonell, J. (2007). Combining Probability-Based Rankers Action-ItemDetection. Proceedings HLT-NAACL, pp. 324331.Biber, D. (1988). Variation Across Speech Language. Cambridge University Press.Bishop, C. M. (2006). Pattern Recognition Machine Learning. Springer.Blitzer, J., McDonald, R., & Pereira, F. (2006). Domain adaptation structural correspondence learning. Proceedings EMNLP, pp. 120128.Blum, A., & Mitchell, T. (1998). Combining labeled unlabeled data co-training.Proceedings COLT, pp. 92100.392fiGesture Salience Hidden VariableBoreczky, J., Girgensohn, A., Golovchinsky, G., & Uchihashi, S. (2000). interactivecomic book presentation exploring video. Proceedings CHI, pp. 185192.Bradley, A. (1997). use area ROC curve evaluation machinelearning algorithms. Pattern Recognition, 30 (7), 11451159.Brennan, S. E., Friedman, M. W., & Pollard, C. J. (1987). centering approach pronouns.Proceedings ACL, pp. 155162.Carletta, J., Ashby, S., Bourban, S., Flynn, M., Guillemot, M., Hain, T., Kadlec, J.,Karaiskos, V., Kraaij, W., Kronenthal, M., Lathoud, G., Lincoln, M., Lisowska, A.,McCowan, I., Post, W., Reidsma, D., & Wellner, P. (2005). ami meeting corpus: pre-announcement. Proceedings Workshop Machine LearningMultimodal Interaction, pp. 2839.Chai, J. Y., Hong, P., Zhou, M. X., & Prasov, Z. (2004). Optimization multimodalinterpretation. Proceedings ACL, pp. 18.Chai, J. Y., & Qu, S. (2005). salience driven approach robust input interpretationmultimodal conversational systems. Proceedings HLT-EMNLP, pp. 217224.Chelba, C., & Acero, A. (2006). Adaptation maximum entropy capitalizer: Little datahelp lot. Computer Speech & Language, 20 (4), 382399.Chen, L., Harper, M., & Huang, Z. (2006). Using maximum entropy (ME) model incorporate gesture cues sentence segmentation. Proceedings ICMI, pp. 185192.Chen, L., Liu, Y., Harper, M. P., & Shriberg, E. (2004). Multimodal model integrationsentence unit detection. Proceedings ICMI, pp. 121128.Chen, L., Rose, R. T., Parrill, F., Han, X., Tu, J., Huang, Z., Harper, M., Quek, F., McNeill,D., Tuttle, R., & Huang, T. (2005). VACE multimodal meeting corpus. ProceedingsWorkshop Machine Learning Multimodal Interaction, pp. 4051.Cohen, P. R., Johnston, M., McGee, D., Oviatt, S., Pittman, J., Smith, I., Chen, L., &Clow, J. (1997). Quickset: Multimodal interaction distributed applications.Proceedings ACM Multimedia, pp. 3140.Darrell, T., & Pentland, A. (1993). Space-time gestures. Proceedings CVPR, pp.335340.Daume III, H. (2007). Frustratingly easy domain adaptation. Proceedings ACL,pp. 256263.Daume III, H., & Marcu, D. (2005). large-scale exploration effective global featuresjoint entity detection tracking model. Proceedings HLT-EMNLP, pp.97104.Deutscher, J., Blake, A., & Reid, I. (2000). Articulated body motion capture annealedparticle filtering. Proceedings CVPR, Vol. 2, pp. 126133.Eisenstein, J., Barzilay, R., & Davis, R. (2007). Turning lectures comic bookslinguistically salient gestures. Proceedings AAAI, pp. 877882.Eisenstein, J., & Davis, R. (2006). Gesture improves coreference resolution. ProceedingsHLT-NAACL, Companion Volume: Short Papers, pp. 3740.393fiEisenstein, Barzilay, & DavisEisenstein, J., & Davis, R. (2007). Conditional modality fusion coreference resolution.Proceedings ACL, pp. 352359.Fayyad, U. M., & Irani, K. B. (1993). Multi-interval discretization continuous-valuedattributes classification learning. Proceedings IJCAI, Vol. 2, pp. 10221027.Godfrey, J., Holliman, E., & McDaniel, J. (1992). Switchboard: Telephone speech corpusresearch development. Proceedings IEEE Conference Acoustics, Speech,Signal Processing (Vol. 1), pp. 517520.Goodwin, M., & Goodwin, C. (1986). Gesture co-participation activity searching word. Semiotica, 62, 5175.Grosz, B., Joshi, A. K., & Weinstein, S. (1995). Centering: framework modelinglocal coherence discourse. Computational Linguistics, 21 (2), 203225.Haghighi, A., & Klein, D. (2007). Unsupervised coreference resolution nonparametricbayesian model. Proceedings ACL, pp. 848855.Harabagiu, S. M., Bunescu, R. C., & Maiorano, S. J. (2001). Text knowledge miningcoreference resolution. Proceedings NAACL, pp. 18.Hearst, M. A. (1994). Multi-paragraph segmentation expository text. ProceedingsACL.Hirschman, L., & Chinchor, N. (1998). MUC-7 coreference task definition. ProceedingsMessage Understanding Conference.Huang, X., Acero, A., & Hon, H.-W. (2001). Spoken Language Processing. Prentice Hall.Huang, X., Alleva, F., Hwang, M.-Y., & Rosenfeld, R. (1993). overview SphinxII speech recognition system. Proceedings ARPA Human Language TechnologyWorkshop, pp. 8186.Ji, H., Westbrook, D., & Grishman, R. (2005). Using semantic relations refine coreferencedecisions. Proceedings HLT-EMNLP, pp. 1724.Johnston, M., & Bangalore, S. (2000). Finite-state multimodal parsing understanding,.Proceedings COLING-2000, pp. 369375.Jordan, P., & Walker, M. (2005). Learning Content Selection Rules Generating ObjectDescriptions Dialogue. Journal Artificial Intelligence Research, 24, 157194.Kahn, J. G., Lease, M., Charniak, E., Johnson, M., & Ostendorf, M. (2005). Effectiveuse prosody parsing conversational speech. Proceedings HLT-EMNLP, pp.233240.Kameyama, M. (1986). property-sharing constraint Centering. ProceedingsACL, pp. 200206.Kehler, A. (2000). Cognitive status form reference multimodal human-computerinteraction. Proceedings AAAI, pp. 685690.Kendon, A. (1980). Gesticulation speech: Two aspects process utterance.Key, M. R. (Ed.), relation verbal non-verbal communication, pp.207227. Mouton.394fiGesture Salience Hidden VariableKettebekov, S., Yeasin, M., & Sharma, R. (2005). Prosody based audiovisual coanalysiscoverbal gesture recognition. IEEE Transactions Multimedia, 7 (2), 234242.Kibble, R., & Power, R. (2004). Optimising referential coherence text generation. Computational Linguistics, 30 (4), 401416.Kim, J., Schwarm, S. E., & Osterdorf, M. (2004). Detecting structural metadatadecision trees transformation-based learning. Proceedings HLT-NAACL04.Koo, T., & Collins, M. (2005). Hidden-variable models discriminative reranking.Proceedings HLT-EMNLP, pp. 507514.Kopp, S., Tepper, P., Ferriman, K., & Cassell, J. (2007). Trading spaces: humanshumanoids use speech gesture give directions. Nishida, T. (Ed.), Conversational Informatics: Engineering Approach. Wiley.Kudo, T., & Matsumoto, Y. (2001). Chunking support vector machines. ProceedingsNAACL, pp. 18.Lappin, S., & Leass, H. J. (1994). algorithm pronominal anaphora resolution. Computational Linguistics, 20 (4), 535561.Lascarides, A., & Stone, M. (2006). Formal semantics iconic gesture. Proceedings10th Workshop Semantics Pragmatics Dialogue (BRANDIAL), pp.6471.Lew, M. S., Sebe, N., Djeraba, C., & Jain, R. (2006). Content-based multimedia information retrieval: State art challenges. ACM Transactions MultimediaComputing, Communications Applications, 2 (1), 119.Li, X., & Roth, D. (2001). Exploring evidence shallow parsing. Proceedings CoNLL,pp. 17.Lin, J. (1991). Divergence measures based shannon entropy. IEEE TransactionsInformation Theory, 37, 145151.Liu, D. C., & Nocedal, J. (1989). limited memory BFGS method large scaleoptimization. Mathematical Programming, 45, 503528.Liu, T., & Kender, J. R. (2007). Computational approaches temporal sampling videosequences. ACM Transactions Multimedia Computing, Communications Applications, 3 (2), 7.Liu, Y. (2004). Structural Event Detection Rich Transcription Speech. Ph.D. thesis,Purdue University.Luo, X. (2005). coreference resolution performance metrics. Proceedings HLTEMNLP, pp. 2532.Malioutov, I., & Barzilay, R. (2006). Minimum cut model spoken lecture segmentation.Proceedings ACL, pp. 2532.Mani, I., & Maybury, M. T. (Eds.). (1999). Advances Automatic Text Summarization.MIT Press, Cambridge, MA, USA.McCallum, A., & Wellner, B. (2004). Conditional models identity uncertaintyapplication noun coreference. Proceedings NIPS, pp. 905912.395fiEisenstein, Barzilay, & DavisMcNeill, D. (1992). Hand Mind. University Chicago Press.Melinger, A., & Levelt, W. J. M. (2004). Gesture communicative intentionspeaker. Gesture, 4 (2), 119141.Muller, C., Rapp, S., & Strube, M. (2002). Applying Co-Training reference resolution.Proceedings ACL, pp. 352359.Muller, C. (2007). Resolving it, this, unrestricted multi-party dialog. Proceedings ACL, pp. 816823.Nakano, Y., Reinstein, G., Stocky, T., & Cassell, J. (2003). Towards model face-to-facegrounding. Proceedings ACL, pp. 553561.Ng, V. (2007). Shallow semantics coreference resolution. Proceedings IJCAI, pp.16891694.Ng, V., & Cardie, C. (2002). Improving machine learning approaches coreference resolution. Proceedings ACL, pp. 104111.NIST (2003). Rich Transcription Fall 2003 (RT-03F) Evaluation plan..Passonneau, R. J. (1997). Applying reliability metrics co-reference annotation. Tech.rep. CUCS-017-97, Columbia University.Poddar, I., Sethi, Y., Ozyildiz, E., & Sharma, R. (1998). Toward natural gesture/speechHCI: case study weather narration. Proceedings Perceptual User Interfaces,pp. 16.Poesio, M., Stevenson, R., Eugenio, B. D., & Hitzeman, J. (2004). Centering: parametrictheory instantiations. Computational Linguistics, 30 (3), 309363.Quattoni, A., Collins, M., & Darrell, T. (2004). Conditional random fields object recognition. Proceedings NIPS, pp. 10971104.Quek, F., McNeill, D., Bryll, R., Duncan, S., Ma, X., Kirbas, C., McCullough, K. E., &Ansari, R. (2002a). Multimodal human discourse: gesture speech. ACM Transactions Computer-Human Interaction, 9:3, 171193.Quek, F., McNeill, D., Bryll, R., & Harper, M. (2002b). Gestural spatialization natural discourse segmentation. Proceedings International Conference SpokenLanguage Processing, pp. 189192.Quek, F., McNeill, D., Bryll, R., Kirbas, C., Arslan, H., McCullough, K. E., Furuyama, N.,& Ansari, R. (2000). Gesture, speech, gaze cues discourse segmentation.Proceedings CVPR, Vol. 2, pp. 247254.Rabiner, L. R. (1989). tutorial hidden markov models selected applicationsspeech recognition. Proceedings IEEE, 77 (2), 257286.Sarkar, A. (2001). Applying co-training methods statistical parsing. ProceedingsNAACL, pp. 18.Sha, F., & Pereira, F. (2003). Shallow parsing conditional random fields. ProceedingsNAACL, pp. 134141.Shriberg, E., Stolcke, A., Hakkani-Tur, D., & Tur, G. (2000). Prosody-based automaticsegmentation speech sentences topics. Speech Communication, 32.396fiGesture Salience Hidden VariableSidner, C. L. (1979). Towards computational theory definite anaphora comprehensionenglish discourse. Tech. rep. AITR-537, Massachusetts Institute Technology.Soon, W. M., Ng, H. T., & Lim, D. C. Y. (2001). machine learning approach coreferenceresolution noun phrases. Computational Linguistics, 27 (4), 521544.Strube, M., & Hahn, U. (1999). Functional centering: grounding referential coherenceinformation structure. Computational Linguistics, 25 (3), 309344.Strube, M., Rapp, S., & Muller, C. (2002). influence minimum edit distancereference resolution. Proceedings EMNLP, pp. 312319.Strube, M., & Muller, C. (2003). machine learning approach pronoun resolutionspoken dialogue. Proceedings ACL, pp. 168175.Sundaram, H., & Chang, S.-F. (2003). Video analysis summarization structuralsemantic levels. D. Feng, W. C. S., & Zhang, H. (Eds.), Multimedia InformationRetrieval Management: Technological Fundamentals Applications, pp. 7594.Springer Verlag.Sutton, C., & McCallum, A. (2006). introduction conditional random fieldsrelational learning. Getoor, L., & Taskar, B. (Eds.), Introduction StatisticalRelational Learning, pp. 95130. MIT Press.Sutton, C., McCallum, A., & Rohanimanesh, K. (2007). Dynamic conditional random fields:Factorized probabilistic models labeling segmenting sequence data. JournalMachine Learning Research, 8, 693723.Toyama, K., & Horvitz, E. (2000). Bayesian modality fusion: Probabilistic integrationmultiple vision algorithms head tracking. Proceedings Asian ConferenceComputer Vision (ACCV).Uchihashi, S., Foote, J., Girgensohn, A., & Boreczky, J. (1999). Video manga: generatingsemantically meaningful video summaries. Proceedings ACM MULTIMEDIA,pp. 383392.Vilain, M., Burger, J., Aberdeen, J., Connolly, D., & Hirschman, L. (1995). modeltheoretic coreference scoring scheme. Proceedings Message Understanding Conference, pp. 4552.Walker, M., Joshi, A., & Prince, E. (Eds.). (1998). Centering Theory Discourse. Clarendon Press, Oxford.Walker, M. A. (1998). Centering, anaphora resolution, discourse structure. MarilynA. Walker, A. K. J., & Prince, E. F. (Eds.), Centering Discourse, pp. 401435.Oxford University Press.Wang, S., Quattoni, A., Morency, L.-P., Demirdjian, D., & Darrell, T. (2006). Hiddenconditional random fields gesture recognition. Proceedings CVPR, Vol. 02,pp. 15211527.Xiong, Y., & Quek, F. (2006). Hand Motion Gesture Frequency Properties MultimodalDiscourse Analysis. International Journal Computer Vision, 69 (3), 353371.Yang, X., Su, J., & Tan, C. L. (2005). Improving pronoun resolution using statistics-basedsemantic compatibility information. Proceedings ACL, pp. 165172.397fiEisenstein, Barzilay, & DavisYang, X., Zhou, G., Su, J., & Tan, C. L. (2003). Coreference resolution using competitionlearning approach. Proceedings ACL, pp. 176183.Zhu, X., Fan, J., Elmagarmid, A., & Wu, X. (2003). Hierarchical video content descriptionsummarization using unified semantic visual similarity. Multimedia Systems,9 (1), 3153.398fiJournal Artificial Intelligence Research 31 (2008) 497-542Submitted 08/07; published 03/08Exploiting Subgraph StructureMulti-Robot Path PlanningMalcolm R. K. Ryanmalcolmr@cse.unsw.edu.auARC Centre Excellence Autonomous SystemsUniversity New South Wales, AustraliaAbstractMulti-robot path planning difficult due combinatorial explosion searchspace every new robot added. Complete search combined state-space soonbecomes intractable. paper present novel form abstraction allowsus plan much efficiently. key abstraction partitioningmap subgraphs known structure entry exit restrictionsrepresent compactly. Planning becomes search much smaller space subgraphconfigurations. abstract plan found, quickly resolved correct(but possibly sub-optimal) concrete plan without need search. provetechnique sound complete demonstrate practical effectivenessreal map.contending solution, prioritised planning, also evaluated shown similarperformance albeit cost completeness. two approaches necessarilyconflicting; demonstrate combined single algorithm outperforms either approach alone.1. Introductionmany scenarios require large groups robots navigate around sharedenvironment. Examples include: delivery robots office (Hada & Takase, 2001),warehouse (Everett, Gage, Gilbreth, Laird, & Smurlo, 1994), shipping yard (Alami, Fleury,Herrb, Ingrand, & Robert, 1998), mine (Alarie & Gamache, 2002); even virtualarmies computer wargame (Buro & Furtak, 2004). case many robotsindependent goals must traverse shared environment without collidingone another. planning path single robot usually considerrest world static, world represented graph calledroad-map. path-planning problem amounts finding path road-map,reasonably efficient algorithms exist. However, multi-robot scenario worldstatic. must avoid collisions obstacles, also robots.Centralised methods (Barraquand & Latombe, 1991), treat robots single composite entity, scale poorly number robots increases. Decoupled methods(LaValle & Hutchinson, 1998; Erdmann & Lozano-Perez, 1986), first planrobot independently resolve conflicts afterwards, prove much faster incomplete many problems require robots deliberately detour optimalpath order let another robot pass. Even priority ordering used (van den Berg& Overmars, 2005), requiring low priority robots plan avoid high-priority robots,problems found cannot solved priority ordering.c2008AI Access Foundation. rights reserved.fiRyanrealistic maps common structures roads, corridors open spacesproduce particular topological features map constrain possible interactions robots. long narrow corridor, instance, may impossible one robotovertake another robots must enter exit first-in/first-out order.hand, large open space may permit many robots pass simultaneouslywithout collision.characterise features particular kinds subgraphs occurringroad-map. decompose map collection simple subgraphs,build plans hierarchically, first planning movements one subgraph another,using special-purpose planners build paths within subgraph.paper propose abstraction. limit consideringhomogeneous group robots navigating using shared road-map. identify particularkinds subgraphs road-map place known constraints ordering robotspass them. use constraints make efficient planning algorithmstraversing kind subgraph, combine local planners hierarchicalplanner solving arbitrary problems.abstraction used implement centralised prioritised planners,demonstrate paper. Unlike heuristic abstractions, methodsound complete. is, used centralised search guaranteed findcorrect plan one exists. guarantee cannot made prioritised searchused, however two-stage planning process means prioritised plannerabstraction often find plans would available otherwise. Experimentalinvestigation shows approach effective maps sparsely connectedgraph representations.2. Problem Formulationassume work provided road-map form graphG = (V, E) representing connectivity free space single robot moving aroundworld (e.g. vertical cell decomposition visibility graph, LaValle, 2006). alsoset robots R = {r1 , . . . , rk } shall consider homogeneous, single mapsuffices all. shall assume starting locations goals lie road-map.Further, shall assume map constructed collisions occurone robot entering vertex v time another robot occupying, enteringleaving vertex. Robots occupying vertices map affect movement.appropriate levels underlying control assumptions satisfiedreal-world problems.simple centralised approach computing plan proceeds follows: First, initialiseevery robot starting position, select robot move neighbouring vertex,checking first robot currently occupying vertex. Continue fashion, selecting moving one robots step goal. Pseudocodeprocess shown Algorithm 1. code presented non-deterministic algorithm, choice points indicated choose operator, backtracking requiredfail command encountered. practice, search algorithm depth-first,breadth-first A* search necessary evaluate alternative paths presents.498fiExploiting Subgraph Structure Multi-Robot Path PlanningAlgorithm 1 simple centralised planning algorithm.1: function Plan(G, a, b)2:= b3:return hi4:end5:choose r R6:select vf : a[vf ] = r7:choose vt {v | (vf , v) G}8:a[vt ] 6= 29:fail10:else11:a[vf ] 212:a[vt ] r13:return (r, vf , vt ).Plan(G, a, b)14:end15: end function. Build plan b graph G.. Nothing do.. Choose robot.. Find location.. Choose edge.. destination occupied; backtrack.. Move robot vf vt .. Recurse.algorithm complete search composite space Gk = G G G,k = |R| robots. eliminating vertices represent collisions robots,size composite graph given by:fififififiV (Gk )fi = n Pk=n!(n k)!fififik fifiE(G )fi = k |E(G)| (n2) P(k1)= k |E(G)|(n 2)!(n k 1)!n = |V (G)| k = |R|. running time algorithm dependsearch algorithm used, expected long moderately large valuesn k.3. Subgraph AbstractionConsider problem shown Figure 1. road-map contains 18 vertices 17 edges,3 robots plan for. So, according formulae, compositegraph 18!/15! = 4896 vertices 3 17 16!/14! = 12240 edges. small mapexpanded large search problem. human mind obvious lotarrangements equivalent. important exact positions robots,ordering.Consider subgraph labeled X. recognise subgraph stack. is, robotsmove subgraph last-in-first-out (LIFO) order. Robots insidestack cannot change order without exiting re-entering stack.goal reverse order robots X, know immediately cannot donewithout moving robots stack re-enter oppositeorder. robots right order, rearranging right positions499fiRyany1x1x2x3x4x5y2y3y4y5y6z2z3z4z5z6Zx6cbXz1Figure 1: planning problem illustrating use subgraphs.trivial. Thus make distinction arrangement robots (inspecify exactly vertex robot occupies) configuration stack (ininterested order).X 6 vertices, robots stack, 6 Pm =6!/(6 m)! possible arrangements. total number arrangements is:6P3 + 3 6 P2 + 3 6 P1 + 6 P0 = 120 + 3 30 + 3 6 + 1= 229terms deciding whether robot leave stack, however, need knoworder. need represent 3! + 3 2! + 3 1! + 1 = 16 different configurationsstack.Subgraphs Z also stacks. Applying analysis three, findrepresent abstract state space 60 different states, 144 possibletransitions states (moving top-most robot one stack onto another).dramatically smaller composite map space above.stack simple kind subgraph need larger collection canonicalsubgraphs represent realistic problems. key features looking follows:1. Computing transitions subgraph require knowledge exact arrangement robots within subgraph, abstract configuration(in case, order).2. two arrangements robots share configuration, transforming onedone easily without search,3. Therefore planning need done configuration space, significantlysmaller.Later introduce three subgraph types cliques, halls rings alsoshare properties readily found realistic planning problems. firstneed formalise ideas subgraph planning.500fiExploiting Subgraph Structure Multi-Robot Path Planning4. Definitionssection outline concepts use later paper. complete formaldefinition terms provided Appendix, along proof soundnesscompleteness subgraph planning process.Given map represented graph G partition set disjoint subgraphsS1 , . . . , Sm . subgraphs induced, i.e. edge exists two verticessubgraph also exists G.arrangement robots G 1-to-1 partial function : V (G) R,represents locations robots within G. robot r vertex v, write a(v) = r.also speak arrangement robots within subgraph S. denotearrangements lowercase roman letters a, bconfiguration subgraph set equivalent arrangements robots within S.Two arrangements equivalent exists plan move robots onewithout robots leaving subgraph. denote configuration subgraph Sxcx . configuration whole map represented tuple subgraphconfigurations = (c1 , . . . , cm ).two operators operate configurations, representing robotentering leaving subgraph respectively. robot r moves two subgraphs Sx Sy configurations change depending identity edge (u, v)robot traveled. write:c0x cx (r, u),c0y cy (r, v)complex subgraphs possible transition result several possible configurations, operators return sets. also possible transitionimpossible particular configuration, case operation returns emptyset.abstract plan defined sequence transitions intermediateconfigurations . every abstract plan two arrangements exists least onecorresponding concrete plan, vice versa. subgraph transitions concreteplan must also exist abstract plan. equivalence arrangements configurationguarantees existence intermediate steps. See Appendix completeproof.5. Subgraph Planningconstruct planning algorithm searches space abstract plans (Algorithm 2). procedure much before. First compute configurationtuple initial arrangement. extend plan one step time. stepconsists selecting robot r moving subgraph currently occupies Sxneighbouring subgraph Sy reduced graph X, along connecting edge (u, v).transition possible plan-step (s, (u, v)) applicable. is, mayresult number different configurations subgraph entered. need chooseone create configuration tuple next step. applicability testselection subsequent configurations performed lines 10-11 AbstractPlan.501fiRyanabstract plan extended step step fashion reaches configurationtuple matches goal arrangement. resulting abstract plan resolvedconcrete plan. transition abstract plan build two short concrete plansone move robot outgoing vertex transition, one make sureincoming vertex clear subgraph appropriately arranged create subsequentconfiguration. Since two plans separate subgraphs, combinedparallel. final step rearrange robots goal arrangement. Again,done parallel subgraphs.AbstractPlan written non-deterministic program, including choicepoints. search algorithm breadth-first depth-first search needed examinepossible set choices ordered fashion. search completeabstract plan guaranteed found, one exists theoremplanning algorithm sound complete. Note resolution phaseplanner entirely deterministic, search needed abstract planfound.5.1 Subgraph Methodsefficiency algorithm relies able compute several functions withoutlot search:Exit compute c (r, u), testing possible robot exit subgraphdetermining resulting configuration(s).Enter compute c (r, v), testing possible robot enter subgraphdetermining resulting configuration(s).Terminate compute b/S c, testing possible robots subgraphmove terminating positions.ResolveExit build plan rearranging robots subgraph allow one exit.ResolveEnter build plan rearranging robots subgraph allow oneenter.ResolveTerminate build plan rearranging robots subgraphterminating positions.key efficient subgraph planning carefully constrain allowed structuresubgraphs partition, functions simple implementrequire expensive search. advantage approach functionsalways computed based arrangement robots within particularsubgraph, relying positions robots elsewhere.6. Subgraph Structureskey process therefore selection subgraph types. abstractionsneed chosen that:502fiExploiting Subgraph Structure Multi-Robot Path Planningv1v2v3v1vkv2(a) stackv3vk(b) hallv1v2v1v2v4v3v4v3(c) clique(d) ringFigure 2: Examples four different subgraph structures.1. commonly occurring real road-maps,2. easy detect extract road-map,3. abstract large portion search space,4. Computing legality transitions fast, sound complete,5. Resolving abstract plan concrete sequence movements efficient.paper present four subgraph types: stacks, halls, cliques rings, satisfyrequirements. following analysis, let n number verticessubgraph k number robots occupying subgraph action takesplace.6.1 Stacksstack (Figure 2(a)) represents narrow dead-end corridor road-map.one exit narrow robots pass one another, robots must enter leavelast-in-first-out order. one simplest subgraphs occur oftenreal maps, serves easy illustration subgraph methods. Formallyconsists chain vertices, linked predecessor successor.vertex one end chain, called head, connected subgraphsentrances exits happen there.configuration stack corresponds ordering robots reside it,head down. Robots stack cannot pass other, ordering cannotchanged without robots exiting re-entering stack.503fiRyan6.1.1 Enterrobot always enter stack long stack full. one new configuration created, adding robot front ordering. computationdone O(1) time.6.1.2 Exitrobot exit stack top robot ordering. one newconfiguration created, removing robot ordering. computation alsodone O(1) time.6.1.3 Terminatedetermine whether termination possible, need check order robotscurrent configuration terminating arrangement. operationtakes O(k) time.6.1.4 ResolveEnterRearranging robots inside stack simple since know ordering constant.vacate top stack (the possible entrance point) move robots deeperstack (as necessary). guaranteed room, since entering full stackpermitted. worst takes O(k) time.6.1.5 ResolveExitrobot exits stack, abstract planner already determinedfirst robot stack others head vertex. simply movestack head, out. robots need moved. worsttakes O(n) time.6.1.6 ResolveTerminateFinally, moving robots terminating positions done top-to-bottom order.robot terminating position move upwards without interference.robot terminating position, robots may need moved lowerorder clear path. approach sound, since terminating positionsrobots must stack (or else ordering would different). processO(nk) total worst-case running time.6.2 Hallshall generalisation stack (Figure 2(b)). Like stack, narrow corridorpermit passing, hall may multiple entrances exits alonglength. Formally consists single chain vertices, one joined predecessorsuccessor. must edges vertices hall, mayedges connecting subgraphs node hall. Halls muchcommonly occurring structures, still maintain property stacks: robots504fiExploiting Subgraph Structure Multi-Robot Path Planningj=0v1v2v3v4v5v6BCj=1v1v2v3v4v5Bv6Cj=2v1v2v3v4Bv5v6CFigure 3: Example entering hall subgraph, k = 3, n = 6 = 3. Robotenter three possible sequence positions j = 0, 1 2 j = 3.cannot reordered without exiting re-entering. Thus, stacks, configurationhall corresponds order robots occupying it, one end hallother.6.2.1 Enterrobot enter hall long full. configurations generatedentrance depend three factors: 1) size hall n, 2) number robotsalready hall k, 3) index vertex enters (ranging 1n).Figure 3 shows entering hall result several different configurations.matter robots already hall arranged, left rightentrance, entering robot moves in. enough space hall eitherside entrance vertex, new robot inserted point ordering.space limited (as example) may possible move robotsone side another, limiting possible insertion points.Given three variables k, n, above, compute maximum minimuminsertion points as:j min(i 1, k)j max(0, k (n i))505fiRyanCreating new configuration matter inserting new robotordering appropriate point. Since list robots needs copied orderthis, takes O(k) time new configuration.6.2.2 ExitWhether robot exit hall via given edge depends several factors: 1)size hall n, 2) number robots hall k, 3) index vertexexits (from 1 k), 3) index j robot ordering (from 1 k). Exitpossible if:j n (k j)exit possible one resulting configuration: previous ordering robotremoved. takes O(k) time compute.6.2.3 TerminateChecking termination halls stacks, test orderrobots final arrangement matches current configuration. doneO(k) time k robots hall.6.2.4 ResolveEnterresolve entrance hall need know subsequent configurationsaiming generate, know proper insertion point entering robot.robots insertion point shuffled one direction one sideentry vertex, rest side. worst take O(nk) time.6.2.5 ResolveExitResolving exit involves moving robot hall exit vertex, shufflingrobots way. worst case, robots shuffleone end hall other, takes O(nk) time.6.2.6 ResolveTerminateResolveTerminate hall identical stack, described above.6.3 Cliquesclique (Figure 2(c)) represents large open area map many exit points(vertices) around perimeter. Robots cross directly vertex another,long clique full, robots inside shuffled way allowhappen.Formally clique totally connected subgraph. Cliques quite different propertieshalls stacks. long least one empty vertex clique, possiblerearrange arbitrarily. configuration clique, circumstance,set robots contains.506fiExploiting Subgraph Structure Multi-Robot Path PlanningHowever special set configurations clique locked.occurs number robots clique equals number vertices.impossible clique rearranged. configuration locked cliqueexplicitly record position robot.6.3.1 Enterclique always entered long full. clique onevacant vertex, single new configuration entering robot addedset occupants. clique one space remaining, entering robot locksclique. theory, point necessary make new configuration everypossible arrangement occupying robots (with entering robot always vertexenters).practice, efficient create single locked configurationrecords locking robot vertex, leaves positions unspecified.permutation robots possible, exact details configuration needpinned next action (either Exit Terminate) requires be.form least commitment, significantly reduce branching factorsearch.Performing test creating new configuration takes O(k) time k robotsclique.6.3.2 Exitclique unlocked robot exit vertex new configurationcreated simply removing robot set occupants.clique locked robot exit specific vertex occupies. resulting configuration unlocked exact locations robotsdiscarded.least-commitment version, locking robot constrained exit vertexevery robot exit vertex except one occupied lockingrobot.Performing test creating new configuration takes O(k) time k robotsclique.6.3.3 Terminateunlocked configuration, checking termination simply consists making sure(and only) required occupants clique. locked configurationrobots must terminating positions (as possibility rearrangingthem). least-commitment version locking robot must terminating vertex. assume robots also places (thuscommitting choice configuration delayed earlier).Performing test takes O(k) time k robots clique.507fiRyan6.3.4 ResolveEnterentrance vertex occupied robot wishes enter simply moveoccupant directly another vacant vertex clique, since every vertex connectedevery other.using least commitment entering robot locks clique needlook ahead plan see next action involving clique. exit transitionneed move exiting robot exit vertex (before clique locked).subsequent exit, meaning robots terminating clique,need rearrange terminating positions point.amortise cost rearrangements subsequent call ResolveExitResolveTerminate treat operation taking O(1) time.6.3.5 ResolveExitclique full time exit assume exiting robot alreadyexit vertex nothing needs done. hand, clique fullmay robot exit vertex. must moved there. exit vertexalready occupied another robot, moved another unoccupied vertex.movements done directly, clique totally connected. operationtakes O(1) time.6.3.6 ResolveTerminateclique locked assume robots already appropriatelyarranged terminal positions work needs done. Otherwiserobots may need rearranged. simple way proceed follows:robot place, first vacate terminating position moving occupantanother unoccupied vertex, move terminating robot vertex. robotmoved way move again, process correctmay produce longer plans necessary. upside takes O(n) time.6.4 Ringsring (Figure 2(d)) resembles hall ends connected. Formally, subgraphvertices V (S) = {v1 , . . . , vn } induced edges E(S) satisfying:(vi , vj ) E(S) iff |i j| 1 (mod n)hall, ordering important ring. Robots ring cannot pass one anothercannot re-order themselves. can, however, rotate ordering (providedring full). Thus ring size 4 more, sequence hr1 , r2 , r3 equivalenthr3 , r1 , r2 hr2 , r1 , r3 i. Equivalent sequences represent configuration.Like cliques, rings locked full. locked ring cannot rotated,ring size three sequences hr1 , r2 , r3 hr3 , r1 , r2 equivalent.represent two locked configurations different properties.508fiExploiting Subgraph Structure Multi-Robot Path Planning6.4.1 Enterrobot may always enter ring provided full. k robots alreadyoccupying ring, k possible configurations result (or one kzero), one possible insertion point.entering robot locks ring must also record specific positionsrobot ring. still produce k different configurationsrobots cannot arbitrarily rearranged, unlike cliques.also possible produce least-commitment versions Enter ringscliques. Again, significantly reduce branching factor search,details involved wish enter paper.operation takes O(k) time new configuration generated.6.4.2 Exitring locked robot exit recorded position, otherwiseexit vertex. robot removed sequence produce resultantconfiguration. new configuration unlocked position informationdiscarded. done O(k) time k robots ring.6.4.3 Terminatecheck termination possible need see order robots around ringterminal arrangement matches current configuration. configurationlocked rotations allowed, otherwise match must exact. testdone O(k) time k robots ring.6.4.4 ResolveEnterrobot enter ring, need first rearrange entryvertex empty nearest robots either side vertex provide correctinsertion point subsequent configuration, selected Enter above. mayrequire shuffling robots one way another, much fashion stackhall. worst case take O(nk) operations k robots ring n vertices.6.4.5 ResolveExitring locked robot exiting must already exit position nothing needsdone. Otherwise, unlocked ring, robots may need shuffled aroundring order move robot exit. worst case take O(nk) operationsk robots ring n vertices.6.4.6 ResolveTerminatering locked robots must already terminating positions;guaranteed abstract planner. Otherwise need rotated correctpositions. one robot moved correct vertex, rest ringtreated stack ResolveTerminate method described used,O(nk) worst case running time k robots ring n vertices.509fiRyan6.5 Summaryfour subgraphs halls rings powerful. subgraphscommon structured maps man-made environments, also found oftenpurely random graphs (consider: shortest path unweighted graph hall).Halls, rings cliques significant size found many realistic planning problems.Importantly, structures well constrained enough six proceduresplanning outlined implemented efficiently deterministically, withoutneed search. cases clique ring, resolution methodsdescribe sometimes sacrifice path optimality speed, could improvedusing smarter resolution planners. Since resolution stage done once, probablywould major effect overall running time planner.7. Prioritised Planningcommon solution rapid growth search spaces multi-robot planning prioritisedplanning (Erdmann & Lozano-Perez, 1986; van den Berg & Overmars, 2005).approach give robots fixed priority ordering begin. Planning performedpriority ordering: first plan built robot highest priority; plansecond highest, interfere first; third,on. new plan must constructed interfere plansit. example implementation shown Algorithm 3. Usually backtrackingplan made. signified algorithm cut operator line 8Plan.cut, search longer complete. problems solutionsprioritised planner cannot find. Figure 4 example. Robots b wishchange positions. plan either robot easy; plan contains onestep. plan robots together requires move way,right hand side map pass. prioritised plannercommitted one-step plan either b cannot construct planrobot interfere.incompleteness mistake, however. core makes prioritised planning efficient. search space pruned significantly eliminatingx1x2x3x4bFigure 4: simple planning problem cannot solved naive prioritised planning.goal swap positions robots b.510fiExploiting Subgraph Structure Multi-Robot Path Planningcertain plans consideration. still viable solution within pruned space(and often is) found much quickly. (hopefully few) casesfails, always resort complete planner backup.7.1 Prioritised Subgraph PlanningPrioritised planning strictly competitor subgraph planning. fact, prioritisedsearch subgraph representation orthogonal ideas, quite possible usetogether. Algorithm 3, plan constructed robot consecutively,rather building entire concrete plan, abstract version produced,fashion Algorithm 2 earlier. compatible abstract plans producedevery robot, resolved concrete plan.well adding advantage abstraction prioritised planning, subgraphrepresentation also allows planner cover space possible plans.delaying resolution end, avoid commitment concrete choices highpriority robot hamper planning later robots.illustrate this, lets return example Figure 4 above. partitionsubgraph vertices {x1 , x2 , x3 , x4 } hall X, prioritised subgraph plannersolve problem. abstract plan highest priority robot empty;nothing already goal subgraph. Given plan, second highestpriority robot plan move X back again. plan producegoal configuration required. Resolving plan move highest priority robotx4 back needed, plan built Resolve methods halls,search.course thing free lunch example works chooseright partition. instead treat {x1 , x2 } stack {x4 , x3 , y} separate hallprioritised subgraph planner help us. Furthermore exist problems,one Figure 5 solved standard prioritised plannersfail introduce wrong subgraph abstraction. difficult generate realisticx1x2x3x4bFigure 5: simple planning problem solved naive prioritised planningsubgraph abstraction. goal swap positions robotsb. priority ordering a, b subgraph planner choose robotremain inside hall. Robot b trapped, blocksexit (note edges (x1 , y) (y, x4 ) directed).511fiRyancases problem small numbers robots, see Section 9.3 below,occur number robots large.8. Search ComplexityLet us consider carefully advantages (if any) subgraph decomposition lie. Subgraph transitions act macro-operators one abstract state (setconfigurations) another. long history planners using macros one kindanother, advantages disadvantages well known (see Section10.1).widely recognised macros advantageous reduce depth search,become disadvantage many macros created branching factorsearch becomes large. guidelines also apply use subgraphs.typical search algorithm proceeds follows: select plan frontier incomplete plans create expansions. Add expansions frontier recursecomplete plan found. time taken complete search determinednumber nodes search tree, turn determined three factors:1. d, depth goal state,2. b, average branching factor tree, i.e. number nodes generated pernode expanded3. efficiency search.perfect search algorithm, heads directly goal, nevertheless contain O(bd)nodes alternative nodes must still generated, even never followed.uninformed breadth-first search, hand, generate O(bd ) nodes.regarded sensible upper bound efficiency search (although possibleworse).Macro-operators tend decrease expense increasing b, welluninformed search dominates, show less advantage good heuristic exists,b equally important. becomes important consider keepincreases branching factor minimum. case subgraph planning,two main reasons b increases:1. reduced graph may larger average degree original. Sincesubgraph contains many vertices, tends out-going edges singlevertex. edges connect different subgraphs, branching factorsignificantly larger. Sparse subgraphs (such halls) worse regarddense subgraphs (such cliques). subgraph decomposition needs chosencarefully avoid problem.2. single subgraph transition may create large number possible configurations,robot enters large hall already occupied several robots.cases may strictly matter configuration generatedpossible use least commitment avoid creating unnecessary alternatives,possibility different configurations result different outcomes512fiExploiting Subgraph Structure Multi-Robot Path Planningtrack, need considered. Halls particularproblem.see experiments follow, careful choice subgraph decomposition important avoid pitfalls, appropriate partition abstractionsignificantly improve informed uninformed search.9. Experimentsempirically test advantages subgraph approach, ran several experimentsreal randomly generated problems. first experiment demonstratesalgorithms scale changes size problem, terms number vertices,edges robots, standard breadth-first search. second experiment showsresults affected using heuristic guide search. experimentsuse randomly generated graphs. final experiment demonstrates algorithmrealistic problem.first two experiments, maps generated randomly automatically partitioned subgraphs. Random generation done follows: first spanning treegenerated adding vertices one one, connecting randomly selected vertexgraph. edges required generated randomly selecting twonon-adjacent vertices creating edge them. edges undirected.1Automated partitioning worked follows:1. Initially mark vertices unused.2. Select pair adjacent unused vertices.3. Use pair basis growing hall, ring clique:Hall: Randomly add unused vertices adjacent either end hall, providedviolate hall property. Continue growth possible.Ring: Randomly add unused vertices adjacent either end ring loopcreated. Discard vertices involved loop.Clique: Randomly add unused vertices adjacent every vertex clique. Continue growth possible.4. Keep biggest three generated subgraphs. Mark vertices used.5. Go back step 2, adjacent unused pairs found.6. remaining unused vertices singletons.intended ideal algorithm. results far optimal fasteffective. Experience suggests partition generated approach containtwice many subgraphs one crafted hand, makes effort minimisedegree reduced graph, even randomly generated partitionsadvantages subgraph abstraction apparent.1. noted algorithm generate uniform distribution connected graphsgiven size, difficult generate sparse connected graphs uniform distribution.bias deemed significant.513fiRyan4.0OriginalReduced303.5degree# subgraphs3.0202.52.0101.501.010203040506070809010010# vertices2030405060708090100# verticesFigure 6: results automatic partitioning program Experiment 1a. leftgraph shows average number subgraphs generated right graphshows average degree reduced graph.9.1 Experiment 1: Scaling Problem Size9.1.1 Scaling |V |first experiment investigate effect scaling number verticesgraph search time. Random graphs generated number verticesranging 10 100. Edges added average degree = |E|/|V | alwaysequal 3. (This value seems typical realistic maps.) One hundred graphsgenerated size, one partitioned using method described above.Figure 6 shows performance auto-partitioning. see, numbersubgraphs increased roughly linearly size graph, average subgraphsize 4. small graphs (with fewer 40 vertices) reduced graph partitioningsparser original, size increases average degree reduced graphgets larger. results presented informative purposes only. make claimsquality partitioning algorithm, indeed reducingsize graph, small factor.graph, three robots given randomly selected initial final locations,plan generated. Figure 7(a) shows average run times four approaches.2 shows clear performance hierarchy. complete planners significantlyslower priority planners, cases subgraph abstraction shows significant improvement naive alternative. Nevertheless, every case combinatorialgrowth runtime apparent (note graph plotted log scale). linearrelationship number vertices number subgraphs prevents subgraph2. noted times overall rather slow. acknowledge attributeimplementation, Java heavily optimised avoid garbage collection.currently working implementation optimised search engine, believeresults still provide valuable comparison methods.514fiExploiting Subgraph Structure Multi-Robot Path Planning1000000100000Time (ms)100001000100Naive completeNaive prioritySubgraph completeSubgraph priority101102030405060708090100# vertices(a) run times8Naive completeNaive prioritySubgraph completeSubgraph priority7Naive completeNaive prioritySubgraph completeSubgraph priority305path lengthbranching factor6420310210010203040506070809010010# vertices2030405060708090100# vertices(b) branching factor(c) goal depthFigure 7: results Experiment 1a. graph (a) boxes show first thirdquartile whiskers show complete range. experiment failedcomplete due time memory limits incompleteness search, runtime treated infinite. value plotted cases 50%experiments failed. graph (c) goal depth naive completesubgraph priority approaches identical graphs 30 60 vertices,lines overlap. naive complete planner could solve problems60 vertices.515fiRyanTable 1: number planning failures recorded two prioritised planning approaches Experiment 1a.# FailuresVertices Naive Subgraph102020 - 7000801090 - 10000approaches better this. better partitioning algorithm ameliorateproblem.analyse causes variation run times, need consider searchprocess carefully. measure search depth average branching factor bexperiment. results plotted Figure 7(b) (c). expected,subgraph abstraction used, goal depth decreased grows slowly,branching factor increased. Since uninformed search, dominatesoverall result improvement planning time.incompleteness prioritised planning shows Table 1. three occasionsnaive prioritised search failed find available solutions. However problemprioritised subgraph search.9.1.2 Scaling |E|Next examine effect graph density. Fixing number vertices 30,generated random graphs average degree ranging 2.0 4.0. value,100 graphs randomly generated automatically partitioned. planningproblem move three robots selected initial goal locations.results experiment shown Figure 8. appear muchoverall change run times approaches, small improvementnaive prioritised planner graph gets denser. Figures 8(b) (c) showexpected result: increasing density graph increases branching factordecreases depth. appears affect four approaches similarly.interesting difference, however, shown Table 2. records percentageexperiments prioritised planners unable find solution.sparse graphs, naive planner failed many 10% problems, improvedquickly density increased. subgraph abstraction added, planner ablesolve two problems. case find problems solvednaive planner subgraph planner.9.1.3 Scaling |R|last scaling experiments, investigate approach performsvarying numbers robots. before, 100 random graphs generated partitioned,30 vertices average degree 3, one partitioned using516fiExploiting Subgraph Structure Multi-Robot Path Planning10000Time (ms)100010010Naive completeNaive prioritySubgraph completeSubgraph priority12.02.22.42.62.83.03.23.43.63.84.0degree(a) run times8406Naive completeNaive prioritySubgraph completeSubgraph priority30path lengthbranching factor7Naive completeNaive prioritySubgraph completeSubgraph priority5420310212.02.22.42.62.83.03.23.43.63.84.002.02.22.42.62.8degree3.03.2degree(b) branching factor(c) goal depthFigure 8: results Experiment 1b.5173.43.63.84.0fiRyan100000Time (ms)100001000100Naive completeNaive prioritySubgraph completeSubgraph priority10112345678910# robots(a) run timesNaive completeNaive prioritySubgraph completeSubgraph priority7Naive completeNaive prioritySubgraph completeSubgraph priority10005100path lengthbranching factor64310211123456789101234# robots56# robots(b) branching factor(c) goal depthFigure 9: results Experiment 1c.51878910fiExploiting Subgraph Structure Multi-Robot Path PlanningTable 2: number planning failures recorded two prioritised planning approaches Experiment 1b.# FailuresDegree Naive Subgraph2.01002.2802.4502.6112.8003.0203.2113.4 - 4.000Table 3: number planning failures recorded two prioritised planning approaches Experiment 1c.# Failures# Robots Naive Subgraph1-3004305406100771871926010461automatic partitioning algorithm. Ten planning problems set graphnumber robots varying 1 10. case initial goal locations selectedrandomly.running times four approaches plotted Figure 9(a). majorperformance difference prioritised non-prioritised planners, prioritised planners able handle twice many robots. two complete-searchapproaches, subgraph abstraction unnecessary overhead small problems,shows significant advantage number robots increases.less obvious advantage subgraph abstraction case prioritisedplanning, look failure rates shown Table 3. number robotsincreases incompleteness naive prioritised algorithm begins become apparent,10 robots see 46% problems could solved planner.advantage subgraph abstraction apparent: total 3 problemscould solved 1000 tried.519fiRyanFigures 9(b) (c) plot average branching factor goal depth problems.previous experiments, subgraph abstraction seen increase branchingfactor decrease depth. complete search approaches branching factorgrows rapidly number robots, node search path contains choicerobot move. prioritised approach reverses trend, planningever done one robot time, later robots much heavily constrainedoptions available them, providing fewer alternatives search tree.9.1.4 Discussionsummarise experiments, advantages subgraph abstraction twofold. Firstly, decreases necessary search depth planning problem compressingmany robot movements single abstract step. Like macro-based abstractions,expense increasing branching factor gains seem outweighlosses practice. course, dependent degree use uninformedsearch, shall address below.advantage specific prioritised planner. tightly constrained problems sparse maps and/or many robots incompleteness naive prioritisedsearch becomes significant issue. addition subgraph abstractionnumber failures dramatically reduced, without additional search.9.2 Experiment 2: Heuristic Searchexperiments far involved uninformed breadth-first search without useheuristic. such, runtime algorithms strongly affected changessearch depth branching factor. explained above, uninformed searchO(bd ) expected running time. However perfect heuristic reduce O(bd),making branching factor much significant aspect. perfect heuristic is, course,unavailable, possible efficiently compute reasonably good search heuristictask relaxing problem. Disregarding collisions simply compute sumshortest path lengths robots location goal. underestimateactual path length, accurate loosely constrained problems (with robotsdense graphs).experiment used best-first search algorithm guided heuristic.3every node search tree, selected plan minimised value. casesubgraph planner, actual locations robots time-point specified,subgraph occupy, heuristic calculated using maximum distancesvertex robots subgraph goal. pre-computed shortest pathdistances every pair nodes running planner, timecomputation counted runtime algorithm.utility heuristic depends largely constrained problem is.graph dense relatively robots, heuristic direct plannerquickly goal. However graph sparser, interactions robotsbecome important, heuristic less useful. reason,3. A* algorithm used, desire minimise length solution, findsolution quickly possible.520fiExploiting Subgraph Structure Multi-Robot Path Planningconcentrate attention experiment varying density graph affectsperformance different approaches.Random maps 200 vertices generated, average degree ranging 23. One hundred graphs generated size partitioned using algorithmdescribed earlier. Figure 10 shows results. original graph gets denser,number subgraphs decreases, mostly possible create longer halls.good, fewer subgraphs mean shorter paths, consequential increase degreeadversely affect branching factor.Ten robots placed randomly graph assigned random goal locations.four planning approaches applied problems. resulting run-timesplotted Figure 11(a). first thing apparent graphdistinction different approaches greatly reduced. size graphnumber robots much larger previous experiments,corresponding effect goal depth branching factor (Figure 11(b) (c)),run-times much smaller, clearly heuristic effective guiding search.average ratio search nodes expanded goal depth close 1.0experiments, slight increase constrained cases, concludeheuristic close perfect.compare four approaches see three distinct stages. constrained case, 200 edges, see subgraph approaches outperforming either naiveapproach, small benefit prioritised search complete search. 220 edgespattern changed. two prioritised methods significantly better twocomplete approaches. number edges increases, naive methods continueimprove, prioritised subgraph search holds steady complete subgraph searchgets significantly worse (due rapid increase branching factor). 300 edgesnaive approaches significantly better subgraph approaches.SingletonsHallsCliquesRings604.5OriginalReduced4.050degree# subgraphs3.540303.02.5202.0101.501.0200210220230240250260270280290300200# edges210220230240250260270280290# edges(a) subgraphs(b) degreeFigure 10: results auto-partitioner graphs Experiment 2.521300fiRyan10000Time (ms)1000100Naive completeNaive prioritySubgraph completeSubgraph priority10200210220230240250260270280290300edges(a) run times120Naive completeNaive prioritySubgraph completeSubgraph priority400Naive completeNaive prioritySubgraph completeSubgraph priority30080path lengthbranching factor10060200401002002002202402602803000200220240edges260edges(b) branching factor(c) goal depthFigure 11: results Experiment 2.522280300fiExploiting Subgraph Structure Multi-Robot Path PlanningTable 4: number planning failures recorded two prioritised planning approaches Experiment 2.# Failures# Edges Naive Subgraph20014021020220002300024010250 - 30000cause clearly seen Figures 11(b) (c). branching factors subgraphapproaches increase significantly faster naive approaches, correspondingimprovement goal depth sufficient outweigh cost.benefits subgraph abstraction highly constrained cases also shownfailure cases (Table 4). 200 edges naive prioritised search unable solve 10%problems, prioritised search subgraphs could solve all. numberfailures fell quickly density graph increased.9.2.1 Discussiongraph becomes moderately dense interactions robots become few,total-single-robot-paths measure becomes near perfect heuristic. makesbranching factor much critical factor using uninformed search.auto-partitioning algorithm use poor job limiting factorsubgraph approaches perform poorly.Better results could achieved better decomposition, clear whethercould found random graph without excessive computation. Certainly partitioning graphs hand easy task. Realistic graphs, hand, generallyshaped natural constraints (e.g. rooms, doors corridors) make decompositionmuch simpler, see following experiment.9.3 Experiment 3: Indoor MapFigure 12 shows map final two experiments, based floor-plan Level 4K17 building University New South Wales. road-map 113 vertices 308edges drawn (by hand) connecting offices open-plan desk locations.imagined might used map delivery task involving teammedium-sized robots.road-map partitioned 47 subgraphs 11 cliques, 7 halls 1 ring,plus 28 remaining singleton nodes (subgraphs containing one vertex). average523fiRyanFigure 12: map Experiment 3. Vertices coloured subgraph.524fiExploiting Subgraph Structure Multi-Robot Path Planning10000Naive completeNaive prioritySubgraph completeSubgraph priorityTime (ms)1000100101234567891011121314151617181920robotsFigure 13: Comparing run times Experiment 3.degree reduced graph 2.1, compared 2.7 original.4 Partitioning donehand aid interactive GUI performed simple graph analysisoffered recommendations (by indicating nodes could added hall cliqueuser creating). road-map clearly laid partitioning minddeciding partitioning whole difficult. Large open spaces generallybecame cliques. Corridors became halls rings. foyer area (around vertex 94)caused particular trouble finding ideal partitioning, due slightly unusualtopology.5series experiments run world, varying number robots 120. experiment 100 runs performed robot placedrandom office desk required make delivery another random officedesk (chosen without replacement, two robots goal). Plans builtusing complete prioritised planners without subgraph abstraction.four approaches utilised total single-robot shortest path heuristic previousexperiment. running times algorithm shown Figure 13.see small numbers robots (1 2) naive approaches significantly better subgraph approaches. overhead subgraph searchoutweighs disadvantages simple problems. number robots increasessubgraph methods take over, around 9 16 robots subgraph methodssignificant better either naive approach. 17 robots combination completesearch subgraphs begins perform less well two prioritised approachesbest performers, considerable advantage subgraph approach.4. comparison, auto-partitioner yielded partition fewer subgraphs (avg. 41.8) higherdegree (avg. 2.25).5. curious, empty rooms centre map, near vertex 91, bathrooms.consider robots would need make deliveries there.525fiRyanNaive completeNaive prioritySubgraph completeSubgraph priority1.81.7expanded / path1.61.51.41.31.21.11.0123456789 10 11 12 13 14 15 16 17 18 19 20robotsFigure 14: Assessing quality heuristic Experiment 3. value plottedratio number expanded nodes search tree goal depth.perfect heuristic yields value 1.0.Considering search complexity, let us first examine performance heuristic.Figure 14 plots ratio average number expanded nodes search treegoal depth. perfect heuristic, value 1.0, experiment11 robots. 11 robots heuristic begins become inaccurate.inaccuracy seems affect complete planners badly prioritised ones,cases subgraph approach seriously affected naive approach.explain difference, note heuristic using contains significantly lessinformation subgraph search naive search. know exactlyrobot within subgraph, assume worst possible position.means value configuration tuple based solely allocation robotssubgraphs, particular configurations subgraphs. Hall subgraphsparticular may several different configurations set robots,assigned heuristic value despite significantly different real distancesgoal.This creates plateau heuristic function broadens search. largenumbers robots permutations become significant factor search. improveheuristic need find way distinguish value different configurationssubgraph. probably require extra method specific subgraph structure.graphs branching factor goal depth (Figure 15) show comeexpect branching factor larger complete search prioritised searchsubgraph abstraction makes worse. Significantly, branching factor prioritised526fiExploiting Subgraph Structure Multi-Robot Path PlanningNaive completeNaive prioritySubgraph completeSubgraph priority50Naive completeNaive prioritySubgraph completeSubgraph priority2000path lengthbranching factor40302010001000123456789 10 11 12 13 14 15 16 17 18 19 201robots23456789 10 11 12 13 14 15 16 17 18 19 20robots(a) branching factor(b) goal depthFigure 15: branching factor goal depth Experiment 3.Table 5: number planning failures recorded two prioritised planning approaches Experiment 3.# FailuresEdges Naive Subgraph1-90010 - 19012002search increase robots added, step planone robot moved. goal depth shows opposite pattern, complete searchesshorter prioritised searches subgraph abstraction approximately halvessearch depth cases.Failure rates recorded Table 5. story different previousexperiments. naive prioritised planner able solve problems every depth,adding subgraph abstraction caused small number failures complexproblems. clear caused reversal. cases involved complexelude analysis. problem warrants investigation.9.3.1 Discussionexperiment shown realistic problem appropriately chosen setsubgraphs subgraph abstraction effective way reduce search evengood heuristic available. subgraph abstraction work wellexample, compared random graphs Experiment 2? answer seems founddegree reduced graph. Automatically partitioning random graph significantlyincreases degree, saw Figure 10(b). This, turn, increases branching factorthus search time.527fiRyancontrast, partition realistic map decreased degree graph2.7 2.1 (by hand) 2.25 (automatically). branching factor subgraphmethods still larger (as one transition still create multiple configurations)effect reduced enough overcome decrease goal depth. indicationrealistic map structure exploited abstraction.investigation warranted characterise features many possible.10. Conclusiondemonstrated new kind abstract representation multi-robot path planningallows much faster planning without sacrificing completeness. Decomposingroad-map subgraphs simple intuitive way providing background knowledgeplanner efficiently exploited. key find subgraph structuresallow us treat many arrangements robots equivalent configurations computetransitions configurations quickly deterministically. describedfour structures paper: stacks, halls, cliques rings. structuressimple enough compute configurations easily also common enough foundmany realistic maps.shown abstract plans subgraphs resolved deterministicallyconcrete plans without need search. planner sound complete,although plans produced necessarily optimal. Future work could proveworth spending time resolution phase trim unnecessarily wasteful plans,using, example, simulated annealing (Sanchez, Ramos, & Frausto, 1999). maytime saved abstract planning leaves us space clever resolution.conventional solution search-space explosion multi-robot planning prioritisation. shown subgraph-based planning competitiveprioritised planning also combination two methods powerful stillcases, partly alleviates incompleteness prioritised approach.10.1 Related WorkAbstraction hierarchical decomposition standard techniques planningrelated search problems. use macro-operators dates back far Sacerdotis earlywork Abstrips planning system (Sacerdoti, 1974) introduced abstractionhierarchies, whereby problem could first solved high level abstractionignoring lower-level details. idea re-expressed many different wayshistory planning far many review detail. present work particularlyinspired generic types Long Fox (2002) similarly detectedsubstructures task-planning problem solved using structure-specific planners.Hierarchical planning applied path-planning abstractionsapproximate cell decomposition (Barbehenn & Hutchinson, 1995; Conte & Zulli, 1995),generalised Voronoi graphs (Choset & Burdick, 1995; Choset, 1996) general ad-hochierarchical maps (Bakker, Zivkovic, & Krose, 2005; Zivkovic, Bakker, & Krose, 2005, 2006),structures identified examples carry well multi-robotscenario.528fiExploiting Subgraph Structure Multi-Robot Path Planningfaster solutions multi-robot problem available assumeexistence garage locations robot (LaValle & Hutchinson, 1998) kindstemporary free space (Sharma & Aloimonos, 1992; Fitch, Butler, & Rus, 2003).method present makes assumption thus general application.appear previous work provides complete abstraction-basedplanner general multi-robot problem.work bears similarity explicitly robot path planning,solving Sokoban puzzle (Botea, Muller, & Schaeffer, 2003; Junghanns & Schaeffer, 2001). domain significantly constrained (the map necessarilyorthogonal grid stones move pushed man)method employ similar. Dividing map rooms tunnels usestrongly-connected-component algorithm identify equivalent arrangements boulders subpart. Equivalent arrangements treated single abstract statecorresponding configuration formulation used state globalsearch. particular structures represent different, general ideas partitioning independent local subproblems identifying abstract states stronglyconnected components, employed work.10.2 Future Plansnext stage project plan examine symmetries provided subgraphrepresentation. Recent work symbolic task-planning (Porteous, Long, & Fox, 2004)shown recognising exploiting symmetries almost-symmetries planningproblems eliminate large amounts search. Subgraph configurations provide naturalground similar work problem domain expect similar improvementspossible.also plan investigate problem automatic subgraph partitioningmaps. identified importance trading path depth branchingfactor, plan make partitioning algorithm chooses subgraphs optimiserelationship. Automatically finding optimal partition could hard, creatingpowerful interactive partitioning tool human operator would seem viablecompromise. One approach would adapt auto-partitioner describepaper seed vertices selected user, allowed choosenumber possible subgraphs based selection.subgraph structures also identified, currently workingformalising properties tree-structured subgraphs. Another possibility wouldgeneralise cliques rings new ring-with-chords structure, although characterisingstructure may prove difficult.many advances search technology may applicablemulti-robot planning problem. currently process re-expressingentire problem constraint satisfaction problem (CSP) Gecode constraint engine(Gecode Team, 2006). believe CSP formulation powerful way takeadvantage structural knowledge subgraph decomposition represents.Acknowledgments529fiRyanId like thank Jonathan Paxman, Brad Tonkes Maurice Pagnucco helpdeveloping ideas paper proofreading drafts.Appendix A. Proof Soundness Completenessappendix set necessary formal definitions prove soundnesscompleteness abstract planning process. main result theorem showingabstract plan exists given problem concrete plan also exists.A.1 Graphs Subgraphsinduced subgraph G graph = (V (S), E(S))V (S) V (G)E(S) = {(u, v) | u, v V (S), (u, v) E(G)}Intuitively describes subgraph consisting subset vertices connecting edges parent graph. Thus induced subgraph specified solelyterms vertices. shall henceforth assume subgraphs refer induced.partition P G set {S1 , . . . , Sm } subgraphs G satisfying[V (G) =V (Si )V (Si ) V (Sj ) = , i, j : 6= ji=1...mGiven graph G partition P construct reduced graph X Gcontracting subgraph single vertexV (X) = PE(X) = {(Si , Sj ) | x Si , Sj : (x, y) G}A.2 Robots ArrangementsLet us assume set robots R. arrangement robots graph G1-to-1 partial function : V (G) R. arrangement represents locations robotswithin G. a(v) = r, robot r vertex v. shall use notation a(v) = 2indicate undefined v, i.e. vertex v unoccupied. arrangement maynecessarily include every robot R. Two arrangements b said disjointrange(a) range(b) = . Let AG represent set arrangements R G.subgraph G, arrangement R G define a/S,induced arrangement R S,a/S(v) = a(v), v V (S)S1 S2 disjoint subgraphs G disjoint arrangements a1 S1 a2S2 , define combined arrangement = a1 a2 arrangement S1 S2satisfying(a1 (v) v S1a(v) =a2 (v) v S2530fiExploiting Subgraph Structure Multi-Robot Path PlanningLemma 1 arrangement G partition P = {S1 , . . . , Sm } {a1 , . . . , }set induced arrangements ai = a/Si , combined arrangement a1 =a.Given identity, uniquely identify arrangement G combinationinduced arrangements partition P.A.3 Concrete Plansneed define means move robots around graph. First definetwo operators respectively add remove robots given arrangement.Formally : AG R V (G) AG mapping satisfiesG(r, v) = bG(rb(u) =a(u)u = votherwiseSimilarly : AG R AG mapping satisfiesGr =bG(2b(u) =a(u)a(u) = rotherwiseomit subscript G clear context.define plan-step R E(G) G robot/edge pair (r, u, v),representing movement r along edge u v, u 6= v. plan-stepapplicable arrangement AG iff a(u) = r a(v) = 2. case applyproduce new arrangement b = s(a)s(a) = (a r) (r, v)concrete plan (or plan) G AG b AG sequence plan-stepshs1 , . . . , sl exist arrangements a0 , . . . , al AG si applicable ai1a0 =al = bai = si (ai1 ), : 0 < lLemma 2 subgraph G P plan P also plan G.Lemma 3 P plan G b Q plan G b c,concatenation P Q, written P.Q plan G c.531fiRyanLemma 4 Let P kQ denote set interleavings sequences P Q. Let S1S2 disjoint subgraphs G, P1 plan S1 a1 b1 P2 plan S2a2 b2 , a1 a2 disjoint. arbitrary interleaving P P1 kP2 planG a1 a2 b1 b2 .A.4 Configurationsdefined machinery concrete plans, introduce abstraction. keyidea configuration abstraction arrangements. robotssubgraph rearranged one arrangement another, without robotsleave subgraph rearrangement, two arrangementstreated equivalent. Configurations represent sets equivalent arrangementssubgraph. So, example, stack subgraph configuration set arrangementsordering robots. arrangement entire partitioned graphabstracted list configurations produces subgraphs.Formally, define configuration relation graph G equivalence relationGAG b iff exists plans Pab Pba G b bGrespectively.configuration c G equivalence class . write c = (a) representGGequivalence class containing arrangement a. Let CG set configurationsG.Lemma 5 b range(a) = range(b)GGiven identity, unambiguously define range configuration crange(c) = range(a), cextend definitions configurations. c CGconfiguration G, r R v V (G)c (r, v) = (a (r, v)) | c, a(v) = 2GGGc (r, v) = (a r) | c, a(v) = rGGGNote map configurations sets configurations.6Given partition P = {S1 , . . . , Sm } G corresponding set configurationrelations { , . . . , } define configuration tuple R G tuple (c1 , . . . , cm )S1Sm: ci CSi ,[range(ci ) = Ri=1...mrange(ci ) range(cj ) = , i, j : 6= j6. Astute readers notice c (r, v) never contains one element, although mayGempty.532fiExploiting Subgraph Structure Multi-Robot Path Planningconfiguration tuple represents abstract state robots entire graph,terms configurations individual subgraphs partition. Given arrangement G construct corresponding configuration tuple (a) = (c1 , . . . , cm )ci = (a/Si ). Conversely, a/Si ci ci , write .SiLemma 6 b arrangements graph G partition {S1 , . . . , Sm }configuration tuple G a, b , exists plan b G.Proof= 1 . . . m, let ai = a/Si bi = b/Si . ai ci bi ciai bi . Therefore definition exists plan Pi ai bi Si .SiLet P P1 k . . . kPm . Since Pi plans disjoint subgraphs, P plana1 = b1 bm = b required.A.5 Abstract Plansconfiguration tuples abstract state representation, define abstractplans, sequences subgraph transitions plan steps move robot onesubgraph another. prove main result section, abstractplan problem exists corresponding concrete plan exists. allowus later prove soundness completeness subgraph planning algorithm.rest section shall assume graph G partition P ={S1 , . . . , Sm } corresponding configuration relations { , . . . , }.S1Smsubgraph transition (or transition) plan-step = (r, u, v) u Sx ,v Sy Sx 6= Sy . transition = (r, u, v) applicable configuration tuple= (c1 , . . . , cm ) Gcx (r, u) 6= , u Sx ,Sxcy (r, v) 6= , v Sy .Syis, robots Sx rearranged robot r leave via u robotsSy rearranged v empty r enter.transition = (r, u, v) applicable = (c1 , . . . , cm ) u Sx v Syapply compute set s() configuration-tuples(c01 , . . . c0m ) s()c0x cx (r, u),Sxc0ycy (r, v),c0z= cz , otherwise.SyLemma 7 arrangement G partition {S1 , . . . , Sm } transition =(r, u, v) applicable also applicable (a),(s(a)) s((a))533fiRyanProof Let Sx , Sy disjoint subgraphs partition u Sx , v Sy . Letax = a/Sx ay = a/Sy . Let (a) = (c1 , . . . , cm ).ax cxax (u) = rcx (r, u) 6= .similarlyay cyay (v) = 2cy (r, v) 6= .Therefore applicable (a).Further, let b = s(a) (b) = (c01 , . . . , c0m ).c0x = (b/Sx )Sx= (ax r)Sxcx (r, u)c0y = (b/Sy )Sy= (ay (r, v))Sycy (r, v)c0z = cz .Therefore (b) s() required.Lemma 8 = (r, u, v) u, v Sx (i.e. transition) arrangement G applicable a, (a) = (s(a)).ProofLet b = s(a). Let ai = a/Si bi = b/Si = 1 . . . m. Let (a) =(c1 , . . . , cm ) (b) = (c01 , . . . , c0m ).plan Px = hsi plan ax bx Sx , ax bx implying cx = c0x .z 6= x, az = bz cz = c0z . Therefore (a) = (b) required.534fiExploiting Subgraph Structure Multi-Robot Path Planningdefine abstract plan arrangement G tuple (, )sequence configuration tuples h0 , . . . , l sequence plan stepshs1 , . . . , sl i,0 = (),l = (),si applicable i1 ,s(i1 ).Theorem 1 abstract plan G exists exists corresponding concrete plan P G.Proof Case ( P ):Let = (, ) abstract plan G , = h0 , . . . , l= hs1 , . . . , sl i. Let = (ci0 , . . . , cim ).shall construct concrete planP = P0 . hs1 .P1 . .Pl1 . hsl .PlPi concrete plan ai bi , satisfyinga0 = ,bl = ,ai , bi ,si+1 applicable bi ,ai+1 = si+1 (bi ), = 0 . . . l 1.Proposition 1 ai bi exist satisfying conditions = 1 . . . l.Proof induction:a0 = therefore a0 exists.Assume ai exists:Let si+1 = (r, u, v) u Sx v Sy . definition abstract plan,si+1 applicable , i+1 = si+1 (i ). Thereforeci+1cix (r, u) 6=xnci+1(a(r,u))|a(u)=r,cxx 6=cix : a(u) = rSet bix equal a.ci+1= (bix (r, u))xbix (r, u) ci+1x535fiRyanAlsoci+1ciy (r, v) 6=nci+1(a(r,v))|a(v)=2,c6=ciy : a(v) = 2Set biy equal a.ci+1= (biy (r, v))biy (r, v) ci+1Set biz = ai /Sz z/ {x, y}bij defined every subgraph Sj partition G. Therefore bi = bi1 bimexists arrangement G.ai exists bi also exists = 0 . . . l 1.si+1 applicable bi sincebi (u) = bix (u) = rbi (v) = biy (v) = 2ai+1 = si+1 (bi ) exists,ai+1 /Sx = bix r ci+1xai+1 /Sy = biy (r, v) ci+1ai+1 /Sz = biz , z/ {x, y} cizci+1zai+1 i+1induction, ai exists = 0 . . . l bi exists = 0 . . . l 1.Furthermore bl = () = l , bi exists = 0 . . . l, required.Proposition 2 concrete plan Pi ai bi exists, = 0, . . . , lProofSince ai , bi plan Pi must exist ai bi , Lemma 6 above.Proposition 3 P concrete plan G.ProofPi plan ai bi = 0, . . . , l. Furthermore ai+1 = si+1 (bi ),hsi+1 plan bi ai+1 . Therefore concatenation plansP = P0 . hs1 . . hsl .Plplan G a0 = bl = , required.536fiExploiting Subgraph Structure Multi-Robot Path PlanningCase (P ):Let P = hs1 , . . . , sL concrete plan G. wish constructabstract plan = (, ) G.Let = ht0 , . . . , tl increasing sequence integers t0 = 0 ti = iff stsubgraph transition. (Note: using capital L designate length concreteplan P lowercase l designate number transitions plan,length corresponding abstract plan .)construct sequence arrangements = h0 , . . . , L0 =i+1 = si+1 (i ), = 0 . . . L 1split subsequences A0 , . . . , AlffAi = ti , . . . , ti+1 1Define = (ti ), = 0, . . . , l, = h0 , . . . , l = hst1 , . . . , stl i.Proposition 4 : AiProof induction:definition,ti (ti ) =assume = ti + j, j < |Ai | 1. need prove t+1 .Let st+1 = (r, u, v). Since + 1/ must u, v Sx . using Lemma 8(t+1 ) = (st+1 (t ))= (t )= .Therefore, induction, Airequired.Proposition 5 = (, ) valid abstract plan .ProofFirst check initial final configuration-tuples containrespectively:0 = (0 ) = ().All ,l = ().537fiRyanNow, = 0 . . . l 1 let bi = ti+1 1 (i.e. final element Ai ), let= bi /Sz z = 1 . . . m.Let = sti+1 = (r, u, v) u Sx v Sy . applicable bidefinition P . Therefore, Lemma 7 above, applicablebizi+1 = (ai+1 )= (s(bi ))s((bi )) = s(i ), required.Therefore valid abstract plan.theorem significant planning problem. tells us needperform search concrete plans. Instead, need search abstract planconvert concrete form. search succeed concreteplan exists.ReferencesAlami, R., Fleury, S., Herrb, M., Ingrand, F., & Robert, F. (1998). Multi-robot cooperationMARTHA project. Robotics & Automation Magazine, IEEE, 5 (1), 3647.Alarie, S., & Gamache, M. (2002). Overview Solution Strategies Used Truck Dispatching Systems Open Pit Mines. International Journal Surface Mining, Reclamation Environment, 16 (1), 5976.Bakker, B., Zivkovic, Z., & Krose, B. (2005). Hierarchical dynamic programming robotpath planning. Proceedings IEEE/RSJ International Conference IntelligentRobots Systems, 27562761.Barbehenn, M., & Hutchinson, S. (1995). Efficient search hierarchical motion planningdynamically maintaining single-source shortest paths trees. IEEE transactionsrobotics automation, 11 (2), 198214.Barraquand, J., & Latombe, J.-C. (1991). Robot motion planning: distributed representation approach. International Journal Robotics Research, 10 (6), 628649.Botea, A., Muller, M., & Schaeffer, J. (2003). Using abstraction planning sokoban.Computers Games: Lecture Notes Computer Science, Vol. 2883, pp. 360375.Springer.Buro, M., & Furtak, T. (2004). RTS games real-time AI research. ProceedingsBehavior Representation Modeling Simulation Conference (BRIMS), ArlingtonVA 2004, 5158.Choset, H. (1996). Sensor based motion planning: hierarchical generalized voronoigraph. Ph.D. thesis, California Institute Technology, Pasadena, California.Choset, H., & Burdick, J. (1995). Sensor based planning. I. generalized Voronoi graph.Proceedings International Conference Robotics utomation, 2.538fiExploiting Subgraph Structure Multi-Robot Path PlanningConte, G., & Zulli, R. (1995). Hierarchical path planning multi-robot environmentsimple navigation function. IEEE Transactions Systems, Man Cybernetics,25 (4), 651654.Erdmann, M., & Lozano-Perez, T. (1986). Multiple Moving Objects. Tech. rep. 883,M.I.T. AI Laboratory.Everett, H., Gage, D., Gilbreth, G., Laird, R., & Smurlo, R. (1994). Real-world issueswarehouse navigation. Proceedings SPIE Conference Mobile Robots IX,2352.Fitch, R., Butler, Z., & Rus, D. (2003). Reconfiguration planning heterogeneous selfreconfiguring robots. Proceedings IEEE/RSJ International Conference Intelligent Robots Systems, 3, 24602467.Gecode Team (2006). Gecode: Generic constraint development environment,. Availablehttp://www.gecode.org.Hada, Y., & Takase, K. (2001). Multiple mobile robot navigation using indoor globalpositioning system (iGPS). Proceedings IEEE/RSJ International ConferenceIntelligent Robots Systems, 2.Junghanns, A., & Schaeffer, J. (2001). Sokoban: Enhancing general single-agent searchmethods using domain knowledge. Artificial Intelligence, 129 (1-2), 219251.LaValle, S. M. (2006). Planning Algorithms. Cambridge University Press.LaValle, S. M., & Hutchinson, S. A. (1998). Optimal Motion Planning Multiple RobotsIndependent Goals. IEEE Transactions Robotics Automation,Vol. 14.Long, D., & Fox, M. (2002). Planning Generic Types, chap. 4, pp. 103138. MorganKaufmann.Porteous, J., Long, D., & Fox, M. (2004). Identification Exploitation AlmostSymmetry Planning Problems. Brown, K. (Ed.), Proceedings 23rd UKPlanning Scheduling SIG.Sacerdoti, E. (1974). Planning hierarchy abstraction spaces. Artificial Intelligence,5 (2), 115135.Sanchez, G., Ramos, F., & Frausto, J. (1999). Locally-Optimal Path Planning UsingProbabilistic Road Maps Simulatead Annealing. Proceedings IASTED International Conference Robotics Applications.Sharma, R., & Aloimonos, Y. (1992). Coordinated motion planning: warehousemansproblem constraints free space. IEEE Transactions Systems, ManCybernetics, 22 (1), 130141.van den Berg, J., & Overmars, M. (2005). Prioritized Motion Planning Multiple Robots.Proceedings IEEE/RSJ International Conference Intelligent Robots Systems, pp. 430435.Zivkovic, Z., Bakker, B., & Krose, B. (2005). Hierarchical map building using visual landmarks geometric constraints. Proceedings IEEE/RSJ International ConferenceIntelligent Robots Systems, 24802485.539fiRyanZivkovic, Z., Bakker, B., & Krose, B. (2006). Hierarchical Map Building Planning basedGraph Partitioning. IEEE International Conference Robotics Automation.540fiExploiting Subgraph Structure Multi-Robot Path PlanningAlgorithm 2 Planning subgraph abstraction.1: function Plan(G, P, R, a, b)2:(a)3:(b)4:AbstractPlan(G, P, R, , )5:P Resolve(G, P, , a, b)6:return P7: end function. Build plan b G using partition P.. Get initial configuration.. Get final configuration.. Build abstract plan.. Resolve concrete plan.1: function AbstractPlan(G, P, R, , ) . Build abstract plan G using P.2:=3:return (hi , hi). Done.4:end5:(c1 , . . . , cm ) =6:choose r R. Choose robot.7:select x : r range(cx ). Find subgraph occupies.8:choose Sy P : (Sx , Sy ) X. Choose neighbouring subgraph.9:choose (u, v) E(G) : u Sx , v Sy. Choose connecting edge.. Choose resulting configurations Sx Sy .10:choose c0x cx (r, u)11:choose c0y cy (r, v)12:(c1 , . . . , c0x , . . . , c0y , . . . , cm ). Construct new configuration tuple.13:(, ) AbstractPlan(G, P, R, , ). Recurse.14:0 .15:0 (r, u, v).16:return (0 , 0 )17: end function1: function Resolve(G, P, , a, b). Resolve abstract plan concrete plan.2:= (, )3:= h0 , . . . , l4:= hs1 , . . . , sl5:P hi6:a07:= 0 . . . (l 1)8:(r, u, v) = si+1. next transition.9:(c01 , . . . , c0m ) = i+1. target configurations.10:find Sx : u Sx11:find Sy : v Sy12:aiz ai /Sz , z = 1 . . .13:(Pxi , bix ) Sx .ResolveExit(aix , r, u, c0x ). Rearrange Sx let robot r exit.. Rearrange Sy let robot r enter.14:(Pyi , biy ) Sy .ResolveEnter(aiy , r, v, c0y )15:P P.(Pxi ||Pyi )16:bi = ai1 . . . bix . . . biy . . . aim17:ai+1 si+1 (bi )18:P P.. hsi+1. Add transition.19:end20:z = 1 . . .21:Tz Sz .ResolveTerminate(al /Sz , b/Sz ). Rearrange Sz final arrangement.22:end23:P P.(T1 || . . . ||Tm )24:return P25: end function541fiRyanAlgorithm 3 simple prioritised planning algorithm.1: function Plan(G, a, b)2:a0 [v] 2, v G. a0 initial arrangement robots03:b [v] 2, v G. b0 final arrangement robots4:= 1 . . . k5:a0 [v] = ri , v : a[v] = ri6:b0 [v] = ri , v : b[v] = ri7:(P, Pi ) PlanOne(G, ri , hP1 , . . . , Pi1 , h0, . . . , 0i , a0 , b0 ). Build plan8:cut. backtrack plan9:end10:return P11: end functionr1 . . . ri .r1 . . . ri .r1 . . . ri .found1: function PlanOne(G, ri , hP1 , . . . , Pi1 , ht1 , . . . , ti1 , a, b)2:= b3:return (hi, hi). Done.4:end5:choose rj R : j. Choose robot move.6:j =7:select vf : a[vf ] = ri8:choose vt {v | (vf , v) G}. Choose new action ri9:else10:(r, vf , vt ) Pj [tj ]. Select old action rj Pj11:tj tj + 112:end13:a[vt ] 6= 214:fail. Backtrack destination occupied.15:end16:a[vf ] 2. Move robot.17:a[vt ] r18:(P, Pi ) PlanOne(G, ri , hP1 , . . . , Pi1 , ht1 , . . . , ti1 , a, b). Recurse.19:P (rj , vf , vt ).P. Add step global plan.20:j =21:Pi (ri , vf , vt ).Pri. Add step ri plan.22:end23:return (P, Pi )24: end function542fiJournal Artificial Intelligence Research 31 (2008) 319-351Submitted 09/07; published 02/08Complexity Planning ProblemsSimple Causal GraphsOmer Gimenezomer.gimenez@upc.eduDept. de Llenguatges Sistemes InformaticsUniversitat Politecnica de CatalunyaJordi Girona, 1-308034 Barcelona, SpainAnders Jonssonanders.jonsson@upf.eduDept. Information Communication TechnologiesPasseig de Circumvallacio, 808003 Barcelona, SpainAbstractpresent three new complexity results classes planning problems simplecausal graphs. First, describe polynomial-time algorithm uses macros generate plans class 3S planning problems binary state variables acycliccausal graphs. implies plan generation may tractable even planningproblem exponentially long minimal solution. also prove problemplan existence planning problems multi-valued variables chain causal graphsNP-hard. Finally, show plan existence planning problems binary statevariables polytree causal graphs NP-complete.1. IntroductionPlanning area research artificial intelligence aims achieve autonomouscontrol complex systems. Formally, planning problem obtain sequencetransformations moving system initial state goal state, given descriptionpossible transformations. Planning algorithms successfully used varietyapplications, including robotics, process planning, information gathering, autonomousagents spacecraft mission control. Research planning seen significant progresslast ten years, part due establishment International PlanningCompetition.important aspect research planning classify complexity solvingplanning problems. able classify planning problem according complexitymakes possible select right tool solving it. Researchers usually distinguishtwo problems: plan generation, problem generating sequence transformations achieving goal, plan existence, problem determining whethersequence exists. original STRIPS formalism used, plan existence undecidable first-order case (Chapman, 1987) PSPACE-complete propositionalcase (Bylander, 1994). Using PDDL, representation language used InternationalPlanning Competition, plan existence EXPSPACE-complete (Erol, Nau, & Subrahmanian, 1995). However, planning problems usually exhibit structure makes muchc2008AI Access Foundation. rights reserved.fiGimenez & Jonssoneasier solve. Helmert (2003) showed many benchmark problems usedInternational Planning Competition fact P NP.common type structure researchers used characterize planning problems called causal graph (Knoblock, 1994). causal graph planningproblem graph captures degree independence among state variablesproblem, easily constructed given description problem transformations. independence state variables exploited devise algorithmsefficiently solving planning problem. causal graph used tooldescribing tractable subclasses planning problems (Brafman & Domshlak, 2003; Jonsson& Backstrom, 1998; Williams & Nayak, 1997), decomposing planning problemssmaller problems (Brafman & Domshlak, 2006; Jonsson, 2007; Knoblock, 1994),basis domain-independent heuristics guide search valid plan (Helmert,2006).present work explore computational complexity solving planning problems simple causal graphs. present new results three classes planning problems studied literature: class 3S (Jonsson & Backstrom, 1998), class Cn(Domshlak & Dinitz, 2001), class planning problems polytree causal graphs(Brafman & Domshlak, 2003). brief, show plan generation instancesfirst class solved polynomial time using macros, plan existencesolvable polynomial time remaining two classes, unless P = NP. work firstappeared conference paper (Gimenez & Jonsson, 2007); current paper providesdetail additional insights well new sections plan length CP-nets.planning problem belongs class 3S causal graph acyclic statevariables either static, symmetrically reversible splitting (see Section 3 precise definition terms). class 3S introduced studied JonssonBackstrom (1998) example class plan existence easy (there existspolynomial-time algorithm determines whether particular planning problemclass solvable) plan generation hard (there exists polynomial-time algorithm generates valid plan every planning problem class). precisely,Jonsson Backstrom showed planning problems class 3Severy valid plan exponentially long. clearly prevents existence efficientplan generation algorithm.first contribution show plan generation 3S fact easyallowed express valid plan using macros. macro simply sequence operatorsmacros. present polynomial-time algorithm produces valid plansform planning problems class 3S. Namely, algorithm outputs polynomialtime system macros that, executed, produce actual valid plan planningproblem instance. algorithm sound complete, is, generates valid planone exists. contrast algorithm incremental algorithm proposedJonsson Backstrom (1998), polynomial size output.also investigate complexity class Cn planning problems multivalued state variables chain causal graphs. words, causal graphdirected path. Domshlak Dinitz (2001) showed solvable instancesclass require exponentially long plans. However, case class 3S,could exist efficient procedure generating valid plans Cn instances using320fiComplexity Planning Problemsmacros novel idea. show plan existence Cn NP-hard, henceruling efficient procedure exists, unless P = NP.also prove plan existence planning problems whose causal graph polytree (i.e., underlying undirected graph acyclic) NP-complete, even restrictproblems binary variables. result closes complexity gap appears Brafman Domshlak (2003) regarding planning problems binary variables. authorsshow plan existence NP-complete planning problems singly connected causalgraphs, plan generation polynomial planning problems polytree causalgraphs bounded indegree. use reduction prove similar problempolytree CP-nets (Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004) NP-complete.1.1 Related WorkSeveral researchers used causal graph devise algorithms solving planningproblems study complexity planning problems. Knoblock (1994) usedcausal graph decompose planning problem hierarchy increasingly abstractproblems. certain conditions, solving hierarchy abstract problems easiersolving original problem. Williams Nayak (1997) introduced several restrictionsplanning problems ensure tractability, one causal graphacyclic. Jonsson Backstrom (1998) defined class 3S planning problems,also requires causal graphs acyclic, showed plan existence polynomialclass.Domshlak Dinitz (2001) analyzed complexity several classes planningproblems acyclic causal graphs. Brafman Domshlak (2003) designed polynomialtime algorithm solving planning problems binary state variables acyclic causalgraph bounded indegree. Brafman Domshlak (2006) identified conditionspossible factorize planning problem several subproblems solvesubproblems independently. claimed planning problem suitablefactorization causal graph bounded tree-width.idea using macros planning almost old planning (Fikes & Nilsson,1971). Minton (1985) developed algorithm measures utility plan fragmentsstores macros deemed useful. Korf (1987) showed macrosexponentially reduce search space size planning problem chosen carefully. Vidal(2004) used relaxed plans generated computing heuristics produce macroscontribute solution planning problems. Macro-FF (Botea, Enzenberger, Muller,& Schaeffer, 2005), algorithm identifies caches macros, competed fourthInternational Planning Competition. authors showed macros help reducesearch effort necessary generate valid plan.Jonsson (2007) described algorithm uses macros generate plans planningproblems tree-reducible causal graphs. exist planning problemsalgorithm generate exponentially long solutions polynomial time, like algorithm 3S. Unlike ours, algorithm handle multi-valued variables, enablessolve problems Towers Hanoi. However, planning problems 3Stree-reducible causal graphs, algorithm cannot used show plan generation3S polynomial.321fiGimenez & Jonsson1.2 Hardness Plan Lengthcontribution paper show plan generation may polynomial evenplanning problems exponential length minimal solutions, provided solutions mayexpressed using concise notation macros. motivate resultdiscuss consequences. Previously, thought plan generation planningproblems exponential length minimal solutions harder NP, sinceknown whether problems NP intractable, certain cannot generateexponential length output polynomial time.However, planning problem exponential length minimal solution,clear plan generation inherently hard, difficulty lies factplan long. Consider two functional problemsf1 (F ) = w(1, 2|F | ),f2 (F ) = w(t(F ), 2|F | ),F 3-CNF formula, |F | number clauses F , w(, k) word containingk copies symbol , t(F ) 1 F satisfiable (i.e., F 3Sat), 0not. cases, problem consists generating correct word. Observef1 f2 provably intractable, since output exponential size input.Nevertheless, intuitive regard problem f1 easier problem f2 . One wayformalize intuition allow programs produce output succinctnotation. instance, allow programs write w(,k) instead string containingk copies symbol , problem f1 becomes polynomial, problem f2(unless P = NP).wanted investigate following question: regarding class 3S, plan generation intractable solution plans long, like f1 , problem intrinsically hard, like f2 ? answer plan generation 3S solved polynomialtime, provided one allowed give solution terms macros, macrosimple substitution scheme: sequence operators and/or macros. backclaim, present algorithm solves plan generation 3S polynomial time.researchers argued intractability using fact plans may exponential length. Domshlak Dinitz (2001) proved complexity results several classesplanning problems multi-valued state variables simple causal graphs. arguedclass Cn planning problems chain causal graphs intractable since plansmay exponential length. Brafman Domshlak (2003) stated plan generationSTRIPS planning problems unary operators acyclic causal graphs intractableusing reasoning. new result puts question argument used provehardness problems. reason, analyze complexity problemsprove hard showing plan existence problem NP-hard.2. NotationLet V set state variables, let D(v) finite domain state variable v V .define state function V maps state variable v V values(v) D(v) domain. partial state p function subset Vp V state322fiComplexity Planning Problemsvariables maps state variable v Vp p(v) D(v). subset C Vstate variables, p | C partial state obtained restricting domain p Vp C.Sometimes use notation (v1 = x1 , . . . , vk = xk ) denote partial state p definedVp = {v1 , . . . , vk } p(vi ) = xi vi Vp . write p(v) = denote v/ Vp .Two partial states p q match, denote pq, p | Vq = q | Vp ,i.e., v Vp Vq , p(v) = q(v). define replacement operator qr two partial states, p = q r partial state defined Vp = Vq Vr , p(v) = r(v)v Vr , p(v) = q(v) v Vq Vr . Note that, general, p q 6= q p.partial state p subsumes partial state q, denote p q, pqVp Vq . remark p q r s, follows p r q s. differencetwo partial states q r, denote q r, partial state p definedVp = {v Vq | q(v) 6= r(v)} p(v) = q(v) v Vp .planning problem tuple P = hV, init, goal, Ai, V set variables,init initial state, goal partial goal state, set operators. operator= hpre(a); post(a)i consists partial state pre(a) called pre-conditionpartial state post(a) called post-condition. Operator applicable statespre(a), applying operator state results new state post(a).valid plan P sequence operators sequentially applicable state initresulting state satisfies goal.causal graph planning problem P directed graph (V, E) state variablesnodes. edge (u, v) E u 6= v exists operatoru Vpre(a) Vpost(a) v Vpost(a) .3. Class 3SJonsson Backstrom (1998) introduced class 3S planning problems studyrelative complexity plan existence plan generation. section, introduceadditional notation needed describe class 3S illustrate properties3S planning problems. begin defining class 3S:Definition 3.1 planning problem P belongs class 3S causal graph acyclicstate variable v V binary either static, symmetrically reversible,splitting.Below, provide formal definitions static, symmetrically reversible splitting.Note fact causal graph acyclic implies operators unary, i.e.,operator A, |Vpost(a) | = 1. Without loss generality, assume 3S planningproblems normal form, mean following:state variable v, D(v) = {0, 1} init(v) = 0.post(a) = (v = x), x {0, 1}, implies pre(a)(v) = 1 x.satisfy first condition, relabel values D(v) initial goalstates well pre- post-conditions operators. satisfy second condition,operator post(a) = (v = x) pre(a)(v) 6= 1 x, either remove323fiGimenez & JonssonvV0uv=0vv=0w=0ww=1wV*uv=0v(a)v=0w=0ww=1wV0V1w(b)Figure 1: Causal graph splitting variable partitions (a) v, (b) w.pre(a)(v) = x, let pre(a)(v) = 1 x previously undefined. resulting planningproblem normal form equivalent original one. process donetime O(|A||V |).following definitions describe three categories state variables 3S:Definition 3.2 state variable v V static one following holds:1. exist post(a)(v) = 1,2. goal(v) = 0 exist post(a)(v) = 0.Definition 3.3 state variable v V reversiblepost(a) = (v = x), exists post(a ) = (v = 1 x). addition, vsymmetrically reversible pre(a ) | (V {v}) = pre(a) | (V {v}).definitions follows value static state variable cannotmust change, whereas value symmetrically reversible state variable changefreely, long possible satisfy pre-conditions operators changevalue. third category state variables splitting. Informally, splitting state variablev splits causal graph three disjoint subgraphs, one depends valuev = 1, one depends v = 0, one independent v. However,precise definition involved, need additional notation.v V , let Qv0 subset state variables, different v, whose valuechanged operator v = 0 pre-condition. Formally, Qv0 = {u V {v} |s.t. pre(a)(v) = 0 u Vpost(a) }. Define Qv1 way v = 1. LetGv0 = (V, E0v ) subgraph (V, E) whose edges exclude v Qv0 Qv1 .Formally, E0v = E {(v, w) | w Qv0 w/ Qv1 }. Finally, let V0v V subset statevariables weakly connected state variable Qv0 graph Gv0 . DefineV1v way v = 1.Definition 3.4 state variable v V splitting V0v V1v disjoint.Figure 1 illustrates causal graph planning problem two splitting statevariables, v w. edge label v = 0 indicates operators changingvalue u v = 0 pre-condition. words, Qv0 = {u, w}, graphGv0 = (V, E0v ) excludes two edges labeled v = 0, V0v includes state state variables,324fiComplexity Planning Problemssince v weakly connected u w connects remaining state variables. setQv1 empty since operators changing value state variablev v = 1 pre-condition. Consequently, V1v empty well. Figure 1(a) showsresulting partition v.wwcase w, Qw0 = {s}, G0 = (V, E0 ) excludes edge labeled w = 0,wV0 = {s}, since state variable connected edge w = 0 removed.Likewise, V1w = {t}. use Vw = V V0w V1w denote set remaining statevariables belong neither V0w V1w . Figure 1(b) shows resulting partitionw.Lemma 3.5 splitting state variable v, two sets V0v V1v non-empty,v belongs neither V0v V1v .Proof contradiction. Assume v belongs V0v . v weakly connectedstate variable Qv0 graph Gv0 = (V, E0v ). since E0v exclude edgesv Qv1 , state variable Qv1 weakly connected state variableQv0 Gv0 . Consequently, state variables Qv1 belong V0v V1v , contradictsv splitting. reasoning holds show v belong V1v .Lemma 3.6 value splitting state variable never needs change twicevalid plan.Proof Assume valid plan changes value splitting state variable vleast three times. show reorder operators wayvalue v need change twice. need address three cases: vbelongs V0v (cf. Figure 1(a)), v belongs V1v , v belongs Vv (cf. Figure 1(b)).v belongs V0v , follows Lemma 3.5 V1v empty. Consequently,operator plan requires v = 1 pre-condition. Thus, safely removeoperators change value v, except possibly last, needed casegoal(v) = 1. v belongs V1v , follows Lemma 3.5 V0v empty. Thus,operator plan requires v = 0 pre-condition. first operator changesvalue v necessary set v 1. that, safely remove operatorschange value v, except last case goal(v) = 0. cases resultingplan contains two operators changing value v.v belongs Vv , edges V0v , V1v , Vv v VvQv0 V0v Qv1 V1v . Let 0 , 1 , subsequences operatorsaffect state variables V0v , V1v , Vv , respectively. Write = h , av1 , i, av1last operator changes value v 0 1. claim reorderingh0 , , av1 , 1 , plan still valid. Indeed, operators 0 require v = 0,holds initial state, operators 1 require v = 1, holdsdue operator av1 . Note operators changing value v safelyremoved since value v = 1 never needed pre-condition change valuestate variable Vv . result valid plan changes value v twice(its value may reset 0 ).325fiGimenez & JonssonVariablev1v2v3v4v5v6v7v8Operatorsav11 = h(v1 = 0); (v1 = 1)iav01 = h(v1 = 1); (v1 = 0)iav12 = h(v1 = 1, v2 = 0); (v2 = 1)iav13 = h(v1 = 0, v2 = 1, v3 = 0); (v3 = 1)iav15av16av06av17av18= h(v3= h(v3= h(v3= h(v6= h(v6= 0, v4= 1, v6= 1, v6= 1, v7= 0, v7= 0, v5 = 0); (v5 = 1)i= 0); (v6 = 1)i= 1); (v6 = 0)i= 0); (v7 = 1)i= 1, v8 = 0); (v8 = 1)iV0viVV1viV{v4 , v5 }V {v4 }VV{v6 , v7 , v8 }VVTable 1: Operators sets V0vi V1vi example planning problem.v4v1v5v7v3v6v2v8Figure 2: Causal graph example planning problem.previous lemma, holds splitting state variables general, providesadditional insight solve planning problem splitting state variable v.First, try achieve goal state state variables V0v value v 0,initial state. Then, set value v 1 try achieve goal state statevariables V1v . Finally, goal(v) = 0, reset value v 0.3.1 Exampleillustrate class 3S using example planning problem. set state variablesV = {v1 , . . . , v8 }. Since planning problem normal form, initial stateinit(vi ) = 0 vi V . goal state defined goal = (v5 = 1, v8 = 1),operators listed Table 1. Figure 2 shows causal graph (V, E)planning problem. operators easy verify v4 static v1v6 symmetrically reversible. planning problem 3S, remainingstate variables splitting. Table 1 lists two sets V0vi V1vi statevariable vi V show indeed, V0vi V1vi = state variables set{v2 , v3 , v5 , v7 , v8 }.326fiComplexity Planning Problems4. Plan Generation 3Ssection, present polynomial-time algorithm plan generation 3S.algorithm produces solution instance 3S form system macros.idea construct unary macros change value single state variable.macros may change values state variables execution, always resetterminating. macros generated, goal achievedone state variable time. show algorithm generates valid planone exists.begin defining macros use paper. Next, describealgorithm pseudo-code (Figures 3, 4, 5) prove correctness. facilitatereading moved straightforward involving proof appendix. Followingdescription algorithm analyze complexity steps involved.follows, assume 3S planning problems normal form defined previoussection.4.1 Macrosmacro-operator, macro short, ordered sequence operators viewed unit.operator sequence respect pre-conditions operators followit, pre-condition operator sequence violated. Applying macroequivalent applying operators sequence given order. Semantically,macro equivalent standard operator pre-condition postcondition, unambiguously induced pre- post-conditions operatorssequence.Since macros functionally operators, operator sequence associated macroinclude macros, long create circular definition. Consequently,possible create hierarchies macros operator sequences macrosone level include macros level below. solution planning problemviewed macro sits top hierarchy.define macros first introduce concept induced pre- post-conditionsoperator sequences. = ha1 , . . . , ak operator sequence, write , 1 k,denote subsequence ha1 , . . . , ai i.Definition 4.1 operator sequence = ha1 , . . . , ak induces pre-condition pre() =pre(ak ) pre(a1 ) post-condition post() = post(a1 ) post(ak ). addition,operator sequence well-defined (pre(i1 )post(i1 ))pre(ai )1 < k.follows, assume P = (V, init, goal, A) planning problemVpost(a) Vpre(a) operator A, = ha1 , . . . , ak operator sequence.Lemma 4.2 planning problem P type , Vpost() Vpre() .Proof direct consequence definitions Vpre() = Vpre(a1 ) Vpre(ak ) Vpost() =Vpost(a1 ) Vpost(ak ) .327fiGimenez & JonssonLemma 4.3 operator sequence applicable state well-definedspre(). state sk resulting application sk = post().Proof induction k. result clearly holds k = 1. k > 1, notepre() = pre(ak ) pre(k1 ), post() = post(k1 ) post(ak ), well-definedk1 well-defined (pre(k1 ) post(k1 ))pre(ak ).hypothesis induction state sk1 resulting application k1sk1 = post(k1 ). follows sk = sk1 post(ak ) = post().Assume applicable state s. means k1 applicable akapplicable sk1 = post(k1 ). hypothesis induction, former impliesspre(k1 ) k1 well-defined, latter (s post(k1 ))pre(ak ).last condition implies (pre(k1 ) post(k1 ))pre(ak ) use pre(k1 ) s,consequence spre(k1 ) total state. Finally, deduces(pre(ak ) pre(k1 )) spre(k1 ) (s post(k1 ))pre(ak ), usingVpost(k1 ) Vpre(k1 ) . follows well-defined spre().Conversely, assume well-defined spre(). implies k1well-defined spre(k1 ), hypothesis induction, k1 applicable state s.remains show ak applicable state sk1 , is, (s post(k1 ))pre(ak ).(pre(k1 ) post(k1 ))pre(ak ) follows post(k1 )pre(ak ). facts(pre(ak ) pre(k1 )) Vpost(k1 ) Vpre(k1 ) completes proof.Since macros induced pre- post-conditions, Lemmas 4.2 4.3 trivially extendcase operator sequence includes macros. readyintroduce definition macros:Definition 4.4 macro sequence = ha1 , . . . , ak operators macrosinduces pre-condition pre(m) = pre() post-condition post(m) = post()pre(). macro well-defined circular definitions occurwell-defined.make macros consistent standard operators, induced post-conditioninclude state variables whose values indeed changed macro, achievedcomputing difference post() pre(). particular, holds3S planning problem normal form, derived macros satisfy second condition normalform, namely post(m) = (v = x), x {0, 1}, implies pre(m)(v) = 1 x.Definition 4.5 Let Ancv set ancestors state variable v 3S planningproblem. define partial state prev Vprev = Ancv1. prev (u) = 1 u Ancv splitting v V1u ,2. prev (u) = 0 otherwise.Definition 4.6 macro 3S-macro well-defined and, x {0, 1}, post(m) =(v = x) pre(m) prev (v = 1 x).328fiComplexity Planning ProblemsMacromv11mv01mv12mv13mv15mv16mv06mv17mv18SequencePre-conditionhav11hav01hmv11 , av12 , mv01hav13hav15hav16hav06hmv16 , av17 , mv06hav18(v1(v1(v1(v1(v3(v3(v3(v3(v3= 0)= 1)= 0, v2= 0, v2= 0, v4= 1, v6= 1, v6= 1, v6= 1, v6= 0)= 1, v3= 0, v5= 0)= 1)= 0, v7= 0, v7Post-condition= 0)= 0)= 0)= 1, v8 = 0)(v1(v1(v2(v3(v5(v6(v6(v7(v8= 1)= 0)= 1)= 1)= 1)= 1)= 0)= 1)= 1)Table 2: Macros generated algorithm example planning problem.algorithm present generates 3S-macros. fact, generates onemacro = mvx post(m) = (v = x) state variable v value x {0, 1}.illustrate idea 3S-macros give flavor algorithm, Table 2 lists macrosgenerated algorithm example 3S planning problem previous section.claim macro 3S-macro. example, operator sequence hav16induces pre-condition (v3 = 1, v6 = 0) post-condition (v3 = 1, v6 = 0) (v6 = 1) =(v3 = 1, v6 = 1). Thus, macro mv16 induces pre-condition pre(mv16 ) = (v3 = 1, v6 = 0)post-condition post(mv16 ) = (v3 = 1, v6 = 1) (v3 = 1, v6 = 0) = (v6 = 1). Since v2v3 splitting since v6 V1v2 v6 V1v3 , follows prev6 (v6 = 0) =(v1 = 0, v2 = 1, v3 = 1, v6 = 0), pre(mv16 ) = (v3 = 1, v6 = 0) prev6 (v6 = 0).macros combined produce solution planning problem. ideaidentify state variable v goal(v) = 1 append macro mv1solution plan. example, results operator sequence hmv15 , mv18 i. However,pre-condition mv18 specifies v3 = 1 v7 = 1, makes necessary insert mv13mv17 mv18 . addition, pre-condition mv13 specifies v2 = 1, makesnecessary insert mv12 mv13 , resulting final plan hmv15 , mv12 , mv13 , mv17 , mv18 i.Note order macros matter; mv15 requires v3 0 mv18 requiresv3 1. splitting state variable v, goal state achieved statevariables V0v value v set 1. expand solution planconsists solely operators A. example, results operator sequencehav15 , av11 , av12 , av01 , av13 , av16 , av17 , av06 , av18 i. case, algorithm generates optimalplan, although true general.4.2 Description Algorithmproceed providing detailed description algorithm plan generation 3S.first describe subroutine generating unary macro sets value statevariable v x. algorithm, call GenerateMacro, described Figure 3.algorithm takes input planning problem P , state variable v, value x (either 0329fiGimenez & Jonsson1234567891011121314function GenerateMacro(P , v, x, )post(a)(v) = xS0 S1 hisatisf trueU {u Vpre(a) {v} | pre(a)(u) = 1}u U increasing topological orderu static mu1/satisf falseelse u splitting mu0 mu1S0 hS0 , mu0S1 hmu1 , S1satisfreturn hS1 , a, S0return f ailFigure 3: Algorithm generating macro sets value v x.1), set macros vs ancestors causal graph. Prior executingalgorithm, perform topological sort state variables. assume that,v V x {0, 1}, contains one macro mvx post(mvx ) = (v = x).algorithm, use notation mvx test whether contains mvx .operator sets value v x, algorithm determines whetherpossible satisfy pre-condition pre(a) starting initial state. this,algorithm finds set U state variables pre(a) assigns 1 (the valuesstate variables already satisfy pre(a) initial state). algorithm constructs twosequences operators, S0 S1 , going state variables U increasingtopological order. operator sequence, use hS, oi shorthand denoteoperator sequence length |S| + 1 consisting operators followed o,either operator macro. possible satisfy pre-condition pre(a)operator A, algorithm returns macro hS1 , a, S0 i. Otherwise, returns f ail.Lemma 4.7 v symmetrically reversible GenerateMacro(P , v, 1, ) successfully generates macro, GenerateMacro(P , v, 0, ).Proof Assume GenerateMacro(P , v, 1, ) successfully returns macro hS1 , a, S0operator post(a) = 1. definition symmetricallyreversible follows exists operator post(a ) = 0pre(a ) | V {v} = pre(a) | V {v}. Thus, set U identical .consequence, values S0 , S1 , satisf loop,means GenerateMacro(P , v, 0, ) returns macro hS1 , , S0 . NoteGenerateMacro(P , v, 0, ) may return another macro goes operators different order; however, guaranteed successfully return macro.Theorem 4.8 macros 3S-macros GenerateMacro(P , v, x, )generates macro mvx 6= f ail, mvx 3S-macro.330fiComplexity Planning Problems12345678910function Macro-3S(P )v V increasing topological ordermv1 GenerateMacro(P , v, 1, )mv0 GenerateMacro(P , v, 0, )mv1 6= f ail mv0 6= f ail{mv1 , mv0 }else mv1 6= f ail goal(v) 6= 0{mv1 }return GeneratePlan(P , V , )Figure 4: algorithm Macro-3S.proof Theorem 4.8 appears Appendix A.Next, describe algorithm plan generation 3S, call Macro-3S.Figure 4 shows pseudocode Macro-3S. algorithm goes state variablesincreasing topological order attempts generate two macros state variablev, mv1 mv0 . macros successfully generated, added currentset macros . mv1 generated goal state assign 0 v,algorithm adds mv1 . Finally, algorithm generates plan using subroutineGeneratePlan, describe later.Lemma 4.9 Let P 3S planning problem let v V state variable.exists valid plan solving P sets v 1, Macro-3S(P ) adds macro mv1 .If, addition, plan resets v 0, Macro-3S(P ) adds mv0 .Proof First note mv1 mv0 generated, Macro-3S(P ) adds .mv1 generated mv0 , Macro-3S(P ) adds mv1 unless goal(v) = 0. However,goal(v) = 0 contradicts fact valid plan solving P sets v 1without resetting 0. remains show GenerateMacro(P , v, 1, ) alwaysgenerates mv1 6= f ail GenerateMacro(P , v, 0, ) always generates mv0 6= f ailplan resets v 0.plan solving P sets v 1 contain operatorpost(a)(v) = 1. plan also resets v 0, contain operatorpost(a )(v) = 0. show GenerateMacro(P , v, 1, ) successfully generatesmv1 6= f ail operator selected line 2. Note algorithm may return anothermacro selects another operator a; however, always generates macro a,guaranteed successfully return macro mv1 6= f ail. true mv0 .prove lemma induction state variables v. v ancestorscausal graph, set U empty default. Thus, satisf never set falseGenerateMacro(P , v, 1, ) successfully returns macro mv1 = hai a. exists,GenerateMacro(P , v, 0, ) successfully returns mv0 = ha .v ancestors causal graph, let U = {u Vpre(a) {v} | pre(a)(u) = 1}.Since plan contains set u U 1. hypothesis induction,Macro-3S(P ) adds mu1 u U . consequence, satisf never set331fiGimenez & Jonsson1234567891011121314151617function GeneratePlan(P , W , )|W | = 0return hiv first variable topological order present Wv splittingv0 Generate-Plan(P , W (V0v {v}), )v1 Generate-Plan(P , W (V1v {v}), )v Generate-Plan(P , W (V V0v V1v {v}), )v0 = f ail v1 = f ail v = f ail (goal(v) = 1 mv1/ )return f ailelse mv1/ return hv , v0 , v1else goal(v) = 0 return hv , v0 , mv1 , v1 , mv0else return hv , v0 , mv1 , v1Generate-Plan(P , W {v}, )= f ail (goal(v) = 1 mv1/ ) return f ailelse goal(v) = 1 return h, mv1else returnFigure 5: Algorithm generating final planfalse thus, GenerateMacro(P , v, 1, ) successfully returns mv1 a. exists,let W = {w Vpre(a ) {v} | pre(a )(w) = 1}. plan contains , setw W 1. hypothesis induction, Macro-3S(P ) adds mw1 w Wconsequently, GenerateMacro(P , v, 0, ) successfully returns mv0 .Finally, describe subroutine GeneratePlan(P , W , ) generating finalplan given planning problem P , set state variables W set macros .set state variables empty, GeneratePlan(P , W , ) returns empty operatorsequence. Otherwise, finds state variable v W comes first topological order.v splitting, algorithm separates W three sets described V0v , V1v ,Vv = V V0v V1v . algorithm recursively generates plans three setsnecessary, inserts mv1 V0v V1v final plan. case,algorithm recursively generates plan W {v}. goal(v) = 1 mv1 , algorithmappends mv1 end resulting plan.Lemma 4.10 Let W plan generated GeneratePlan(P , W , ), let vfirst state variable topological order present W , let V = ha , W , b finalplan generated Macro-3S(P ). mv1 follows (pre(a )post(a ))pre(mv1 ).Proof determine content operator sequence precedes W finalplan inspection. Note call GeneratePlan(P , W , ) nested withinsequence recursive calls GeneratePlan starting GeneratePlan(P , V , ).Let Z set state variables u Z first state variabletopological order call GeneratePlan prior GeneratePlan(P , W , ).u Z correspond call GeneratePlan set state variablesU W U . u splitting, u contribute since332fiComplexity Planning Problemspossible addition macro plan line 16 places macro mu1 endplan generated recursively.Assume u Z splitting state variable. three cases: W V0u , W V1u ,W Vu = V V0u V1u . W Vu , u contribute since never placesmacros u . W V0u , plan u part since precedes u0 lines 11,12, 13. W V1u , plans u u0 part since precede u1cases. mu1 , macro mu1 also part since precedes u1 lines 1213. macros part .Since macros unary, plan generated GeneratePlan(P , U , )changes values state variables U . splitting state variable u,edges Vu {u} V0u , Vu {u} V1u , V0u V1u . followsplan u change value state variable appears pre-conditionmacro u0 . holds u respect u1 u0 respect u1 .Thus, macro changes value splitting state variable u Ancvmu1 case W V1u .Recall prev defined Ancv assigns 1 u u splittingv V1u . ancestors v, value 0 holds initial statealtered . u splitting v V1u , follows definition 3S-macrospre(mv1 )(u) = 1 pre(mv1 )(u) =. pre(mv1 )(u) = 1, correct append mu1mv1 satisfy pre(mv1 )(u). mu1/ follows u/ Vpre(mv1 ) , since pre(mv1 )(u) = 1would caused GenerateMacro(P , v, 1, ) set satisf false line 8. Thus,pre-condition pre(mv1 ) mv1 agrees pre(a ) post(a ) value statevariable, means two partial states match.Lemma 4.11 GeneratePlan(P , V , ) generates well-defined plan.Proof Note state variable v V , GeneratePlan(P , W , ) calledprecisely v first state variable topological order. Lemma 4.10follows (pre(a ) post(a ))pre(mv1 ), plan precedes Wfinal plan. Since v first state variable topological order W , plans v0 ,v1 , v , , recursively generated GeneratePlan, change valuestate variable pre(mv1 ). follows mv1 applicable following ha , v , v0 ha , i.Since mv1 changes value v, mv0 applicable following ha , v , v0 , mv1 , v1 i.Theorem 4.12 Macro-3S(P ) generates valid plan solving planning problem 3Sone exists.Proof GeneratePlan(P , V , ) returns f ail exists state variablev V goal(v) = 1 mv1/ . Lemma 4.9 followsexist valid plan solving P sets v 1. Consequently, existplan solving P . Otherwise, GeneratePlan(P , V , ) returns well-defined plan dueLemma 4.11. Since plan sets 1 state variable v goal(v) = 1resets 0 state variable v goal(v) = 0, plan valid plan solvingplanning problem.333fiGimenez & Jonssonv1v2v3v4v5Figure 6: Causal graph planning problem P5 .4.3 Examplesillustrate algorithm example introduced Jonsson Backstrom (1998)show instances 3S exponentially sized minimal solutions. Let Pn =hV, init, goal, Ai planning problem defined natural number n, V = {v1 , . . . , vn },goal state defined Vgoal = V , goal(vi ) = 0 vi {v1 , . . . , vn1 },goal(vn ) = 1. state variable vi V , two operators A:av1i = h(v1 = 0, . . . , vi2 = 0, vi1 = 1, vi = 0); (vi = 1)i,av0i = h(v1 = 0, . . . , vi2 = 0, vi1 = 1, vi = 1); (vi = 0)i.words, state variable symmetrically reversible. causal graph planning problem P5 shown Figure 6. Note state variable vi {v1 , . . . , vn2 },vvpre(a1i+1 )(vi ) = 1 pre(a1i+2 )(vi ) = 0, vi+1 Qv1i vi+2 Q0vi . Sinceedge causal graph vi+1 vi+2 , state variable {v1 , . . . , vn2 }vsplitting. hand, vn1 vn splitting since V0 n1 = V0vn = V1vn = .Backstrom Nebel (1995) showed length shortest plan solving Pn 2n 1,i.e., exponential number state variables.state variable vi {v1 , . . . , vn1 }, algorithm generates two macros mv1ivim0 . single operator, av1i , changes value vi 0 1. pre(av1i )assigns 1 vi1 , U = {vi1 }. Since vi1 splitting, mv1i defined mv1i =vvvvhm1i1 , av1i , m0i1 i. Similarly, mv0i defined mv0i = hm1i1 , av0i , m0i1 i. state variablevn , U = {vn1 }, splitting, mv1n defined mv1n = hav1n i.generate final plan, algorithm goes state variables topological order. state variables v1 vn2 , algorithm nothing, sincestate variables splitting goal state 1. state variable vn1 ,algorithm recursively generates plan vn , hmv1n since goal(vn ) = 1.vSince goal(vn1 ) = 0, algorithm inserts m1n1 mv1n satisfy pre-conditionvvn1 = 1 m0n1 mv1n achieve goal state goal(vn1 ) = 0. Thus, final planvn1vvnhm1 , m1 , m0n1 i. expand plan, end sequence 2n 1 operators. However, individual macro operator sequence length greater 3. Together,macros recursively specify complete solution planning problem.also demonstrate planning problems 3S polynomial lengthsolutions algorithm may generate exponential length solutions. this,modify planning problem Pn letting goal(vi ) = 1 vi V . addition,state variable vi V , add two operators A:bv1i = h(v1 = 1, . . . , vi1 = 1, vi = 0); (vi = 1)i,bv0i = h(v1 = 1, . . . , vi1 = 1, vi = 1); (vi = 0)i.334fiComplexity Planning Problemsalso add operator cv1n = h(vn1 = 0, vn = 0); (vn = 1)i A. consequence, state variables {v1 , . . . , vn2 } still symmetrically reversible splitting.vn1 also symmetrically reversible longer splitting, since pre(av1n )(vn1 ) = 1vvpre(cv1n )(vn1 ) = 0 implies vn V0 n1 V1 n1 . vn still splitting since V0vn = V1vn = .Assume GenerateMacro(P , vi , x, ) always selects bvxi first. consequence,state variable vi V x {0, 1}, GenerateMacro(P , vi , x, ) generatesvvmacro mvxi = hm1i1 , . . . , mv11 , bvxi , mv01 , . . . , m0i1 i.Let Li length plan represented mvxi , x {0, 1}. definitionvmx Li = 2(L1 + . . . + Li1 ) + 1. show induction Li = 3i1 .length macro v1 L1 = 1 = 30 . > 1,Li = 2(30 + . . . + 3i2 ) + 1 = 23i1 13i1 1+1=2+ 1 = 3i1 1 + 1 = 3i1 .312generate final plan algorithm change value state variable0 1, total length plan L = L1 + . . . + Ln = 30 + . . . + 3n1 =(3n 1)/2. However, exists plan length n solves planning problem,namely hbv11 , . . . , bv1n i.4.4 Complexitysection prove complexity algorithm polynomial.analyze step algorithm separately. summary complexity resultstep algorithm given below. Note number edges |E| causalgraph O(|A||V |), since operator may introduce O(|V |) edges. complexity resultO(|V | + |E|) = O(|A||V |) topological sort follows Cormen, Leiserson, Rivest,Stein (1990).Constructing causal graph G = (V, E)Calculating V1v V0v v VPerforming topological sort GGenerateMacro(P , v, x, )GeneratePlan(P , V , )Macro-3S(P )O(|A||V |)O(|A||V |2 )O(|A||V |)O(|A||V |)O(|V |2 )O(|A||V |2 )Lemma 4.13Lemma 4.14Lemma 4.15Lemma 4.16Theorem 4.17Lemma 4.13 complexity constructing causal graph G = (V, E) O(|A||V |).Proof causal graph consists |V | nodes. operator statevariable u Vpre(a) , add edge u unique state variable v Vpost(a) .worst case, |Vpre(a) | = O(|V |), case complexity O(|A||V |).Lemma 4.14 complexity calculating sets V0v V1v state variablev V O(|A||V |2 ).Proof state variable v V , establish sets Qv0 Qv1 , requiresgoing operator worst case. Note interestedpre-condition v unique state variable Vpost(a) , means335fiGimenez & Jonssonneed go state variable Vpre(a) . Next, construct graph Gv0 .copying causal graph G, takes time O(|A||V |), removingedges v Qv0 Qv1 , takes time O(|V |).Finally, construct set V0v find state variable weakly connected state variable u Qv0 graph Gv0 . state variable u Qv0 ,performing undirected search starting u takes time O(|A||V |). performed search starting u, need search state variables Qv0reached search. way, total complexity search exceedO(|A||V |). case constructing V1v identical. Since performprocedure state variable v V , total complexity step O(|A||V |2 ).Lemma 4.15 complexity GenerateMacro(P , v, x, ) O(|A||V |).Proof operator A, GenerateMacro(P , v, x, ) needs check whetherpost(a)(v) = x. worst case, |U | = O(|V |), case complexityalgorithm O(|A||V |).Lemma 4.16 complexity GeneratePlan(P , V , ) O(|V |2 ).Proof Note state variable v V , GeneratePlan(P , V , ) called recursively exactly v first variable topological order. words,GeneratePlan(P , V , ) called exactly |V | times. GeneratePlan(P , V , ) containsconstant operations except intersection difference sets lines 6-8.Since intersection set difference done time O(|V |), total complexityGeneratePlan(P , V , ) O(|V |2 ).Theorem 4.17 complexity Macro-3S(P ) O(|A||V |2 ).Proof Prior executing Macro-3S(P ), necessary construct causal graph G,find sets V0v V1v state variable v V , perform topological sortG. shown steps take time O(|A||V |2 ). state variablev V , Macro-3S(P ) calls GenerateMacro(P , v, x, ) twice. Lemma 4.15follows step takes time O(2|V ||A||V |) = O(|A||V |2 ). Finally, Macro-3S(P ) callsGeneratePlan(P , V , ), takes time O(|V |2 ) due Lemma 4.16. followscomplexity Macro-3S(P ) O(|A||V |2 ).conjecture possible improve complexity result Macro3S(P ) O(|A||V |). However, proof seems somewhat complex, main objectivedevise algorithm efficient possible. Rather, interestedestablishing algorithm polynomial, follows Theorem 4.17.4.5 Plan Lengthsection study length plans generated given algorithm. beginwith, derive general bound length plans. Then, show computeactual length particular plan without expanding macros. also presentalgorithm uses computation efficiently obtain i-th action plan336fiComplexity Planning Problemsmacro form. start introducing concept depth state variablescausal graph.Definition 4.18 depth d(v) state variable v longest path vstate variable causal graph.Since causal graph acyclic planning problems 3S, depth state variableunique computed polynomial time. Also, follows least one statevariable depth 0, i.e., outgoing edges.Definition 4.19 depth planning problem P 3S equals largest depthstate variable v P , i.e., = maxvV d(v).characterize planning problem based depth state variables. Letn = |V | number state variables, let ci denote number state variablesdepth i. planning problem depth d, follows c0 + . . . + cd = n.example, consider planning problem whose causal graph appears Figure 2.planning problem, n = 8, = 5, c0 = 2, c1 = 2, c2 = 1, c3 = 1, c4 = 1, c5 = 1.Lemma 4.20 Consider values Li {0, . . . , d} defined Ld = 1, Li =2(ci+1 Li+1 + ci+2 Li+2 + . . . + cd Ld ) + 1 < d. values Li upper boundlength macros generated algorithm state variable v depth i.Proof prove decreasing induction value i. Assume v depth = d.follows Definition 4.18 v incoming edges. Thus, operator changingvalue v pre-condition state variable v, Ld = 1 upperbound, stated.Now, assume v depth < d, Li+k k > 0 upper boundslength corresponding macros. Let operator changes value v.definition depth follows cannot pre-condition state variableu depth j i; otherwise would edge u v causal graph, causingdepth u greater i. Thus, worst case, macro v changevalues state variables depths larger i, change value v, resetvalues state variables lower levels. follows Li = 2(ci+1 Li+1 + . . . + cd Ld ) + 1upper bound.Theorem 4.21 upper bounds Li Lemma 4.20 satisfy Li = dj=i+1 (1 + 2cj ).Proof NoteLi = 2(ci+1 Li+1 + ci+2 Li+2 + . . . + cd Ld ) + 1 == 2ci+1 Li+1 + 2(ci+2 Li+2 + . . . + cd Ld ) + 1 == 2ci+1 Li+1 + Li+1 = (2ci+1 + 1)Li+1 .result easily follows induction.337fiGimenez & Jonssonobtain upper bound L total length plan. worstcase, goal state assigns different value state variable initial state,i.e., goal(v) 6= init(v) v V . achieve goal state algorithm applies onemacro per state variable. HenceL = c0 L0 + c1 L1 + . . . + cd Ld = c0 L0 +L0 1(1 + 2c0 )L0 11Y1==(1 + 2cj ) .2222j=0previous bound depends distribution variables depths accordingcausal graph. obtain general bound depend depthsvariables first find distribution maximizes upper bound L.QLemma 4.22 upper bound L = 21 dj=0 (1+2cj ) 12 planning problems n variablesdepth maximized ci equal, is, ci = n/(d + 1).Proof Note ci > 0 i, c0 + + cd = n. result follows directapplication well known AM-GM (arithmetic mean-geometric mean) inequality,states arithmetic mean positive values xi greater equal geometricmean, equality xi same.implies product positivePfactors xi = (1 + 2ci ) fixed sum = dj=0 xj = 2n + maximizedequal, is, ci = n/(d + 1).Theorem 4.23 length plan generated algorithm planning problem3S n state variables depth ((1 + 2n/(d + 1))d+1 1)/2.Proof direct consequence Lemma 4.22. Since c0 , . . . , cd discrete, maypossible set c0 = . . . = cd = n/(d + 1). Nevertheless, ((1 + 2n/(d + 1))d+1 1)/2upper bound L case.Observe bound established Theorem 4.23 increasing function d.implies given d, bound also applies planning problems 3S depthsmaller d. consequence, depth planning problem 3S boundedd, algorithm generates solution plan planning problempolynomial length O(nd+1 ). Since complexity executing plan proportionalplan length, use depth define tractable complexity classes planningproblems 3S respect plan execution.Theorem 4.24 length plan generated algorithm planning problem3S n state variables (3n 1)/2.Proof worst case, depth planning problem n1. follows Theorem4.23 length plan ((1 + 2n/n)n 1)/2 = (3n 1)/2.Note bound established Theorem 4.24 tight; second example Section4.3, showed algorithm generates plan whose length (3n 1)/2.338fiComplexity Planning Problems123456789function Operator(S, i)first operatorlength(o) <length(o)next operatorprimitive(o)returnelsereturn Operator(o, i)Figure 7: algorithm determining i-th operator sequenceLemma 4.25 complexity computing total length plan generatedalgorithm O(|V |2 ).Proof algorithm generates 2|V | = O(|V |) macros, 2 state variable.operator sequence macro consists one operator 2(|V | 1) = O(|V |)macros. use dynamic programming avoid computing length macroonce. worst case, compute length O(|V |) macros,sum O(|V |) terms, resulting total complexity O(|V |2 ).Lemma 4.26 Given solution plan length l integer 1 l, complexitydetermining i-th operator plan O(|V |2 ).Proof prove lemma providing algorithm determining i-th operator,appears Figure 7. Since operator sequences consist operators macros,variable represents either operator macro generated Macro-3S.function primitive(o) returns true operator f alse macro. functionlength(o) returns length macro, 1 otherwise. assume lengthmacros pre-computed, know Lemma 4.25 takes time O(|V |2 ).algorithm simply finds operator macro i-th position sequence,taking account length macros sequence. i-th position partmacro, algorithm recursively finds operator appropriate positionoperator sequence represented macro. worst case, algorithm goO(|V |) operators sequence call Operator recursively O(|V |) times,resulting total complexity O(|V |2 ).4.6 Discussiongeneral view plan generation output consist valid sequencegrounded operators solves planning problem. contrast, algorithm generatessolution plan form system macros. One might argue truly solveplan generation problem, algorithm expand system macros arrivesequence underlying operators. case, algorithm would longer polynomial,since solution plan planning problem 3S may exponential length. fact,objective execute solution plan once, algorithm offers marginalbenefit incremental algorithm proposed Jonsson Backstrom (1998).339fiGimenez & Jonssonhand, several reasons view system macros generatedalgorithm complete solution planning problem 3S. macros collectivelyspecify steps necessary reach goal. solution plan generatedverified polynomial time, plan stored reused using polynomial memory.even possible compute length resulting plan determine i-thoperator plan polynomial time shown Lemmas 4.25 4.26. Thus,practical purposes system macros represents complete solution. Evenobjective execute solution plan once, algorithm fasterJonsson Backstrom (1998). necessary execute plan generatedalgorithm maintain stack currently executing macros select next operatorexecute, whereas algorithm Jonsson Backstrom perform several stepsoperator output.Jonsson Backstrom (1998) proved bounded plan existence problem 3SNP-hard. bounded plan existence problem problem determining whetherexists valid solution plan length k. consequence, optimalplan generation problem 3S NP-hard well; otherwise, would possiblesolve bounded plan existence problem generating optimal plan comparinglength resulting plan k. examples seen algorithmgenerate optimal plan general. fact, algorithm badincremental algorithm Jonsson Backstrom, sense algorithms maygenerate exponential length plans even though exists solution polynomial length.Since algorithm makes possible compute total length valid solutionpolynomial time, used generate heuristics planners. Specifically,Katz Domshlak (2007) proposed projecting planning problems onto provably tractablefragments use solution fragments heuristics original problem.shown 3S tractable fragment. Unfortunately, optimal planning3S NP-hard, hope generating admissible heuristic. However,heuristic may still informative guiding search towards solution originalproblem. addition, planning problems exponential length optimal solutions,standard planner hope generating heuristic polynomial time, makingmacro-based approach (and Jonsson, 2007) (current) viable option.5. Class CnDomshlak Dinitz (2001) defined class Cn planning problems multi-valuedstate variables chain causal graphs. Since chain causal graphs acyclic, followsoperators unary. Moreover, let vi i-th state variable chain. > 1,operator Vpost(a) {vi } holds Vpre(a) = {vi1 , vi }. words,operator changes value state variable vi may pre-conditionsvi1 vi .authors showed instances Cn exponentially sized minimalsolutions, therefore argued class intractable. light previous section,argument length solutions discard possibility instancesclass solved polynomial time using macros. showcase, unless P = NP.340fiComplexity Planning Problemsv1vkwFigure 8: Causal graph P (F ).C1C1, C10,1Cn,CnCnC10,100,11C1, C10,1Cn,CnCnFigure 9: Domain transition graph vi .define decision problem Plan-Existence-Cn follows. valid input PlanExistence-Cn planning instance P Cn . input P belongs Plan-ExistenceCn P solvable. show section problem Plan-ExistenceCn NP-hard. implies that, unless P = NP, solving instances Cn trulyintractable problem, namely, polynomial-time algorithm distinguish solvableunsolvable instances Cn . particular, polynomial-time algorithm solve Cninstances using macros kind output format.1prove Plan-Existence-Cn NP-hard reduction Cnf-Sat, is,problem determining whether CNF formula F satisfiable. Let C1 , . . . , Cnclauses CNF formula F , let v1 , . . . , vk variables appear F .briefly describe intuition behind reduction. planning problem createformula F state variable variable appearing F , plans forcedcommit value (either 0 1) state variables actually using them. Then,satisfy goal problem, variables used pass messages. However,operators defined way plan succeedstate variable values committed satisfying assignment F .proceed describe reduction. First ,we define planning problem P (F ) =hV, init, goal, Ai follows. set state variables V = {v1 , . . . , vk , w}, D(vi ) ={S, 0, 1, C1 , C1 , . . . , Cn , Cn } vi D(w) = {S, 1, . . . , n}. initial state definesinit(v) = v V goal state defines goal(w) = n. Figure 8 showscausal graph P (F ).domain transition graph state variable vi shown Figure 9. noderepresents value D(vi ), edge x means exists operatorpre(a)(vi ) = x post(a)(vi ) = y. Edge labels represent pre-conditionoperators state variable vi1 , multiple labels indicate several operatorsassociated edge. enumerate operators acting vi using notation= hpre(a); post(a)i (when = 1 mention vi1 understood void):1. valid output format one enables efficient distinction output representing validplan output representing fact solution found.341fiGimenez & JonssonC1, C1n11Cn,CnnFigure 10: Domain transition graph w.(1) Two operators hvi1 = S, vi = S; vi = 0i hvi1 = S, vi = S; vi = 1i allow vimove either 0 1.(2) > 1. clause Cj X {Cj , Cj }, two operatorshvi1 = X, vi = 0; vi = Cj hvi1 = X, vi = 1; vi = Cj i. operators allow vi move Cj Cj vi1 done so.(3) clause Cj X {0, 1}, operator hvi1 = X, vi = 0; vi = Cj voccurs clause Cj , operator hvi1 = X, vi = 1; vi = Cj vi occurs clauseCj . operators allow vi move Cj Cj even vi1 done so.(4) clause Cj X = {0, 1}, two operators hvi1 = X, vi = Cj ; vi = 0ihvi1 = X, vi = Cj ; vi = 1i. operators allow vi move back 0 1.domain transition graph state variable w shown Figure 10. every clauseCj two operators acting w hvk = X, w = j 1; w = ji, X {Cj , Cj }(if j = 1, pre-condition w = j 1 replaced w = S).Proposition 5.1 CNF formula F satisfiable planning instance P (F )solvable.Proof proof follows relatively straightforward interpretation variablesvalues planning instance P (F ). every state variable vi , must useoperator (1) commit either 0 1. Note that, choice made, variable vicannot set value. reason need two values Cj Cj clauseenforce commitment (Cj corresponds vi = 0, Cj corresponds vi = 1).reach goal state variable w advance step step along values 1, . . . , n.Clearly, every clause Cj must exist variable vi first set values CjCj using operator (3). Then, message propagated along variablesvi+1 , . . . , vk using operators (2). Note existence operator (3) actingvi implies initial choice 0 1 state variable vi , applied formulavariable vi , makes clause Cj true. Hence, plan solving P (F ), useinitial choices state variables vi define (partial) assignment satisfiesclauses F .Conversely, assignment satisfies F , show obtain plansolves P (F ). First, set every state variable vi value (vi ). every oneclauses Cj , choose variable vi among make Cj true using assignment .Then, increasing order j, set state variable vi corresponding clause Cjvalue Cj Cj (depending (vi )), pass message along vi+1 , . . . , vk w.Theorem 5.2 Plan-Existence-Cn NP-hard.342fiComplexity Planning ProblemsvxvCvCvCvCvCvC123vxvyvyvzvz12v1v2v3v4v53Figure 11: Causal graph PF F = C1 C2 C3 three variables x, y, z.Proof Producing planning instance P (F ) CNF formula F easily donepolynomial time, polynomial-time reduction Cnf-Sat p Plan-ExistenceCn .6. Polytree Causal Graphssection, study class planning problems binary state variablespolytree causal graphs. Brafman Domshlak (2003) presented algorithm findsplans problems class time O(n2 ), n number variablesmaximum indegree polytree causal graph. Brafman Domshlak (2006)also showed solve time roughly O(n ) planning domains local depthcausal graphs tree-width . interesting observe algorithms fail solvepolytree planning domains polynomial time different reasons: first one failstree broad (unbounded indegree), second one fails tree deep(unbounded local depth, since tree-width polytree 1).section prove problem plan existence polytree causal graphsbinary variables NP-hard. proof reduction 3Sat classplanning problems. example reduction, Figure 11 shows causal graphplanning problem PF corresponds formula F three variables threeclauses (the precise definition PF given Proposition 6.2). Finally, endsection remark reduction solves problem expressed terms CP-nets(Boutilier et al., 2004), namely, dominance testing polytree CP-nets binaryvariables partially specified CPTs NP-complete.Let us describe briefly idea behind reduction. planning problem PF two, . . . , v ) dependsdifferent parts. first part (state variables vx , vx , . . . , vC1 , vC11formula F property plan may change value v1 0 1many times number clauses F truth assignment satisfy. However,condition v1 cannot stated planning problem goal. overcome difficultyintroducing second part (state variables v1 , v2 , . . . , vt ) translates regularplanning problem goal.first describe second part. Let P planning problem hV, init, goal, AiV set state variables {v1 , . . . , v2k1 } set 4k 2 operators{1 , . . . , 2k1 , 1 , . . . , 2k1 }. = 1, operators defined 1 = hv1 = 1; v1 = 0i343fiGimenez & Jonsson1 = hv1 = 0; v1 = 1i. > 1, operators = hvi1 = 0, vi = 1; vi = 0i= hvi1 = 1, vi = 0; vi = 1i. initial state init(vi ) = 0 i, goal stategoal(vi ) = 0 even goal(vi ) = 1 odd.Lemma 6.1 valid plan planning problem P changes state variable v1 0 1least k times. valid plan achieves minimum.Proof Let Ai Bi be, respectively, sequences operators h1 , . . . , h1 , . . . , i.easy verify plan hB2k1 , A2k2 , B2k3 , . . . , B3 , A2 , B1 solves planningproblem P . Indeed, applying operators Ai (respectively, operators Bi ),variables v1 , . . . , vi become 0 (respectively, 1). particular, variable vi attains goalstate (0 even, 1 odd). Subsequent operators plan modify vi ,variable remains goal state end. operator 1 appears k timesplan (one sequence type Bi ), thus value v1 changes k times 0 1.proceed show k minimum. Consider plan solvesplanning problem P , let number operators appearing (inwords, number times value vi changes, either 0 11 0). Note number times operator appears equal precisely onenumber occurrences . show i1 > . Since 2k1 1,implies 1 2k 1, plan has, least, k occurrences 1 , completingproof.show i1 > . Let Si subsequence operators plan .Clearly, Si starts (since initial state vi = 0), operator cannotappear twice consecutively Si , Si = , , , , etc. Also note that, > 1,vi1 = 1 pre-condition, vi1 = 0, hence must least one operatori1 plan betweeen two operators . reason mustleast one operator i1 two operators , one operator i1first operator . shows i1 . hand, variables vivi1 different values goal state, subsequences Si Si1 must differentlengths, is, i1 6= . Together, implies i1 > , desired.Proposition 6.2 3Sat reduces plan existence planning problems binary variables polytree causal graphs.Proof Let F CNF formula k clauses n variables. produce planningproblem PF 2n + 4k 1 state variables 2n + 14k 3 operators. planningproblem two state variables vx vx every variable x F , two state variables vCevery clause C F , 2k 1 additional variables v , . . . , vvC12k1 . variables0 initial state. (partial) goal state defined Vgoal = {v1 , . . . , v2k1 },goal(vi ) = 0 even, goal(vi ) = 1 odd, like problem P Lemma6.1. operators are:(1) Operators hvx = 0; vx = 1i hvx = 0; vx = 1i every variable x F .= 0; v = 1i, hv = 0, v = 0; v = 1i hv = 1, v = 1; v = 0i(2) Operators hvCCCCCCCCevery clause C F .344fiComplexity Planning Problems(3) Seven operators every clause C, one partial assignment satisfies C.Without loss generality, let x, y, z three variables appear clause C.operator among seven, Vpre(a) = {vx , vx , vy , vy , vz , vz , vC , v1 },Vpost(a) = {v1 }, pre(a)(vC ) = 1, pre(a)(v1 ) = 0, post(a)(v1 ) = 1. precondition state variables vx , vx , vy , vy , vz , vz depends corresponding satisfying partial assignment. example, operator corresponding partialassignment {x = 0, = 0, z = 1} clause C = x z pre-condition(vx = 0, vx = 1, vy = 0, vy = 1, vz = 1, vz = 0).(4) operator h(C, vC = 0), v1 = 1; v1 = 0i.(5) Operators = hvi1 = 0, vi = 1; vi = 0i = hvi1 = 1, vi = 0; vi = 1i2 2k 1 (the operators problem P except 1 1 ).note simple facts problem PF . variable x, state variables vxvx PF start 0, applying operators (1) change 1back 0. particular, plan cannot reach partial states hvx = 1, vx = 0ihvx = 0, vx = 1i course execution.Similarly, C clause F , state variable vC change 0 1 and, first1, v change back 0. changes possible, sincechanging vCC0.operator brings back vCinterpret operators (3) (4), operators affect v1 .change v1 0 1 need apply one operators (3), thus requirevC = 1 clause C. way bring back v1 0 applying operator(4) pre-condition vC = 0. deduce every time v1 changesvalue 0 1 back 0 plan , least one k state variables vCused up, sense vC brought 0 1 back 0, cannotused purpose.show F 3Sat valid plan problem PF . AssumeF 3Sat, let truth assignment satisfies F . Consider following plan. First, set vx = (x) vx = 1 (x) variables x using operators (1).Then, clause C F , set vC = 1, apply operator (3) correspondsrestricted variables clause C (at point, v1 changes 0 1), set= 1 v = 0, apply operator (4) (at point, v change 1vC1C0). repeating process every clause C F switching state variable v1exactly k times 0 1. Now, following proof Lemma 6.1, easily extendplan plan sets variables vi goal values.show converse, namely, existence valid plan PF implies Fsatisfiable. Define assignment setting (x) = 1 partial state {vx = 1, vx = 0}appears execution , (x) = 0 otherwise. (Recall onepartial states {vx = 1, vx = 0} {vx = 0, vx = 1} appear executionplan). Lemma 6.1, must state variable v1 changes 0 1 least ktimes. implies k operators (3), corresponding different clauses,used move v1 0 1. apply operator, values statevariables {vx , vx } must satisfy corresponding clause. Thus assignment satisfiesk clauses F .345fiGimenez & JonssonTheorem 6.3 Plan existence planning problems binary variables polytreecausal graph NP-complete.Proof Due Proposition 6.2 need show problem NP.Brafman Domshlak (2003) showed holds general setting planningproblems causal graphs component directed-path singly connected (thatis, one directed path pair nodes). proof exploitsnon-trivial auxiliary result: solvable planning problems binary variables directedpath singly connected causal graph plans polynomial length (the truenon-binary variables, unrestricted causal graphs).6.1 CP-netsBoutilier et al. (2004) introduced notion CP-net graphical representationuser preferences. brief, CP-net network dependences set variables:preferences user variable depend values others,ceteris paribus (all else equal) assumption, is, user preferences variablecompletely independent values variables mentioned. preferencesvariable given parent variables network stored conditional preferencetables, CPTs.Boutilier et al. (2004) showed dominance query problem acyclic CP-nets,is, problem deciding one variable outcome preferable another,expressed terms planning problem. network dependences CP-netbecomes causal graph planning problem.However, certain conditions, perform opposite process: transformplanning problem CP-net dominance query problem, answeringquery amounts solving planning problem. possible followingconditions planning problems acyclic causal graph binary variables:1. Two operators modify variable opposing directions must nonmatching prevail conditions (the prevail condition operator partial statepre(a) | V Vpost(a) ).2. must allow partially specified CPTs CP-net description.first condition guarantees obtain consistent CPTs planning instanceoperators. second condition ensures reduction polynomial-size preserving,since fully specified CPTs exponential maximum node indegree CP-net.particular, planning instance PF reduced F satisfies first condition.(Note true planning problem P Lemma 6.1, dropreversing operators 1 1 constructing PF Proposition 6.2.) consequence,claim following:Theorem 6.4 Dominance testing polytree CP-nets binary variables partiallyspecified CPTs NP-complete.346fiComplexity Planning Problems7. Conclusionpresented three new complexity results planning problems simple causalgraphs. First, provided polynomial-time algorithm uses macros generate solution plans class 3S. Although solutions generally suboptimal, algorithmgenerate representations exponentially long plans polynomial time. several implications theoretical work planning, since generally acceptedexponentially sized minimal solutions imply plan generation intractable. workshows always case, provided one allowed express solutionsuccinct notation macros. also showed plan existence class CnNP-hard, plan existence class planning problems binary variablespolytree causal graph NP-complete.Jonsson Backstrom (1998) investigated whether plan generation significantlyharder plan existence. Using class 3S, demonstrated plan existencesolved polynomial time, plan generation intractable sensesolution plans may exponential length. work casts new light result: eventhough solution plans exponential length, possible generate representationsolution polynomial time. Thus, appears class 3S, plan generationinherently harder plan existence. aware workdetermines relative complexity plan existence plan generation, questionwhether plan generation harder plan existence remains open.potential criticism algorithm solution form macrosstandard, intractable expand system macros arrive possiblyexponentially long sequence underlying operators. Although true, shownsystem macros share several characteristics proper solution. possiblegenerate validate solution polynomial time, solution storedusing polynomial memory. also showed possible compute total lengthsolution polynomial time, well determine i-th operatorunderlying sequence.Since relatively simple, class Cn class planning problemsbinary state variables polytree causal graphs could seen promising candidatesproving relative complexity plan existence plan generation. However,shown plan existence Cn NP-hard, plan existence planning problemspolytree causal graphs NP-complete. Consequently, classes cannot usedshow plan generation harder plan existence, since plan existence alreadydifficult. work also closes complexity gaps appear literature regardingtwo classes.however possible exist subsets planning problems classesplan existence solved polynomial time. fact, polytree causalgraphs binary variables know case, due algorithms BrafmanDomshlak (2003, 2006) mentioned Section 6. Hence plan generation problempolynomial restrict polytree causal graphs either bounded indegreebounded local depth . Consequently, reduction 3Sat exhibits unboundedindegree unbounded local depth.347fiGimenez & JonssonSimilarly, one may ask class Cn planning problems parameter that,bounded, would yield tractable subclass. state variables reductiondomains whose size depends number clauses corresponding CNF formula,domain size appears interesting candidate. Planning problems Cnbinary variables tractable due work Brafman Domshlak (2003),ideas use extend domain sizes 2. Hence would interestinginvestigate whether problem plan existence class Cn easier sizestate variable domains bounded constant.Appendix A. Proof Theorem 4.8Assume GenerateMacro(P , v, x, ) successfully returns macro mvx = hS1 , a, S0 i.Let U = {u Vpre(a) {v} | pre(a)(u) = 1} let W = {w1 , . . . , wk } U setwistate variables U wi splitting, {mw0 , m1 } , wi comes wj topological order < j. follows u U static,wkw1w1kS1 = hmw1 , . . . , m1 S0 = hm0 , . . . , m0 i. Since state variable wi Wsplitting, symmetrically reversible.Lemma A.1 wi W , prewi prev .Proof Since wi Vpre(a) v Vpost(a) , edge wi v causal graph.Thus, ancestor wi also ancestor v, Ancwi Ancv . state variableu Ancwi , prewi (u) = 1 u splitting wi V1u . graph Gu1 = (V, E1u )includes edge wi v, means v V1u wi V1u . followsprewi (u) = 1 prev (u) = 1, consequence, prewi prev .Let = hS0 , a, S1 i. wi W {0, 1}, let wsequence precedingwi1wwwwi+1w1ki.macro , is, 1 = hm1 , . . . , m1 0 = hS0 , a, mw0 , . . . , m0Further, let sequence appearing a, is, = hS0 i.wiLemma A.2 1 k, post-conditions sequences w1 , , 0post(w1 ) = (wi+1 = 1, . . . , wk = 1),post(a ) = (w1 = 1, . . . , wk = 1),post(w0 ) = (w1 = 0, . . . , wi1 = 0, wi = 1, . . . , wk = 1, v = x).Proof direct consequence post(ha1 , . . . , ak i) = post(a1 ) post(ak ) post(mw)=(wi = y), post(a) = (v = x).wiLemma A.3 1 k, pre-conditions sequences w1 , , 0 ,wivsatisfy pre(w1 ) pre( ) pre(0 ) pre() pre (v = 1 x).Proof Since pre(ha1 , . . . , ak i) = pre(ak ) pre(a1 ), follows pre(w1 ) pre( )wivpre(0 ) pre(). prove pre() pre (v = 1 x). state variable upre()(u) 6=, let mu first operator hS0 , a, S1 u Vpre(mu ) ,pre()(u) = pre(mu )(u).348fiComplexity Planning Problemsuwi (w = 0) prev ,mu = mw1 , follows pre(m ) prewiused m1 3S-macro, wi symmetrically reversible, prewi prev dueLemma A.1. particular, pre(mu )(u) = prev (u).Since assume planning problems normal form, u = wi implieswiuu Vpre(mwi ) . follows mu 6= mw1 i, u 6= wi i. = m01pre(mu ) prewi (wi = 1), due u 6= wi , deduce pre(mu )(u) =prewi (u) = prev (u).Finally, consider case mu = a. u = v pre(mu )(u) = 1 x, desired.u 6= v splitting, either v belongs V0u pre(mu )(u) = 0, v belongs V1upre(mu )(u) = 1. is, pre(mu )(u) = prev (u). u 6= v symmetrically reversiblefollows pre(mu )(u) = 0, since case pre(mu )(u) = 1 would forced algorithmeither fail include u W . u 6= v static, pre(mu )(u) = 0, else algorithm wouldfailed.Lemma A.4 Let p, p , q r partial states. p p (p q)r, (p q)r.Proof direct consequence p q p q.Lemma A.5 macro mvx generated algorithm well-defined.Proof Since includes macros ancestors v causal graph, sincecausal graph acyclic, cyclic definitions occur. remains show that, macrosequence preceding , holds (pre(m ) post(m ))pre(m).Note due Lemmas A.3 A.4 enough showwi(a) (prev (v = 1 x) post(w1 ))pre(m1 ),(b) (prev (v = 1 x) post(a ))pre(a),wi(c) (prev (v = 1 x) post(w0 ))pre(m0 ).wiCase (a) follows easily since Vpost(wi ) Vpre(mwi ) = pre(mw1 ) = pre (wi = 0)11wprev . Case (c) similar, although time must use post(0 )(wi ) = 1wiwi (w = 1). Finally, case (b)post(w0 )(wj ) = 0 j < i, required pre(m0 ) = preholds variable u Vpre(a) either u = v, covered (v = 1 x),splitting static, covered prev , symmetrically reversible, coveredprev (u) = 0 pre(a)(u) = 0, post(a )(u) = 1 pre(a)(u) = 1.remains show mvx 3S-macro. follows Lemmas A.3 A.5well-defined satisfies pre(mvx ) = pre() prev (v = 1 x). Finally, post(mvx ) =post()pre() = (v = x) direct consequence post() = (w1 = 0, . . . , wk = 0, v = x)Lemma A.2, pre()(wi ) = 0, pre()(v) = 1 x proof Lemma A.3.Acknowledgmentswork partially funded MEC grants TIN2006-15387-C03-03 TIN2004-07925C03-01 (GRAMMARS).349fiGimenez & JonssonReferencesBackstrom, C., & Nebel, B. (1995). Complexity Results SAS+ Planning. ComputationalIntelligence, 11 (4), 625655.Botea, A., Enzenberger, M., Muller, M., & Schaeffer, J. (2005). Macro-FF: Improving AIPlanning Automatically Learned Macro-Operators. Journal Artificial Intelligence Research, 24, 581621.Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004). CP-nets: ToolRepresenting Reasoning Conditional Ceteris Paribus Preference Statements.Journal Artificial Intelligence Research, 21, 135191.Brafman, R., & Domshlak, C. (2003). Structure Complexity Planning UnaryOperators. Journal Artificial Intelligence Research, 18, 315349.Brafman, R., & Domshlak, C. (2006). Factored Planning: How, When, Not.Proceedings 21st National Conference Artificial Intelligence.Bylander, T. (1994). computational complexity propositional STRIPS planning.Artificial Intelligence, 69, 165204.Chapman, D. (1987). Planning conjunctive goals. Artificial Intelligence, 32(3), 333377.Cormen, T., Leiserson, C., Rivest, R., & Stein, C. (1990). Introduction Algorithms. MITPress McGraw Hill.Domshlak, C., & Dinitz, Y. (2001). Multi-Agent Off-line Coordination: Structure Complexity. Proceedings 6th European Conference Planning, pp. 277288.Erol, K., Nau, D., & Subrahmanian, V. (1995). Complexity, decidability undecidabilityresults domain-independent planning. Artificial Intelligence, 76(1-2), 7588.Fikes, R., & Nilsson, N. (1971). STRIPS: new approach application theoremproving problem solving. Artificial Intelligence, 5 (2), 189208.Gimenez, O., & Jonsson, A. (2007). Hardness Planning Problems SimpleCausal Graphs. Proceedings 17th International Conference AutomatedPlanning Scheduling, pp. 152159.Helmert, M. (2003). Complexity results standard benchmark domains planning.Artificial Intelligence, 143(2), 219262.Helmert, M. (2006). Fast Downward Planning System. Journal Artificial IntelligenceResearch, 26, 191246.Jonsson, A. (2007). Role Macros Tractable Planning Causal Graphs.Proceedings 20th International Joint Conference Artificial Intelligence, pp.19361941.Jonsson, P., & Backstrom, C. (1998). Tractable plan existence imply tractableplan generation. Annals Mathematics Artificial Intelligence, 22(3-4), 281296.Katz, M., & Domshlak, C. (2007). Structural Patterns Heuristics: Basic Idea ConcreteInstance. Workshop Heuristics Domain-independent Planning: Progress,Ideas, Limitations, Challenges (ICAPS-07).350fiComplexity Planning ProblemsKnoblock, C. (1994). Automatically generating abstractions planning. Artificial Intelligence, 68(2), 243302.Korf, R. (1987). Planning search: quantitative approach. Artificial Intelligence, 33(1),6588.Minton, S. (1985). Selectively generalizing plans problem-solving. Proceedings9th International Joint Conference Artificial Intelligence, pp. 596599.Vidal, V. (2004). Lookahead Strategy Heuristic Search Planning. Proceedings14th International Conference Automated Planning Scheduling, pp. 150159.Williams, B., & Nayak, P. (1997). reactive planner model-based executive.Proceedings 15th International Joint Conference Artificial Intelligence, pp.11781185.351fiJournal Artificial Intelligence Research 31 (2008) 217-257Submitted 09/07; published 02/08Loosely Coupled Formulations Automated Planning:Integer Programming PerspectiveMenkes H.L. van den Brielmenkes@asu.eduDepartment Industrial EngineeringArizona State University, Tempe, AZ 85281 USAThomas Vossenvossen@colorado.eduLeeds School BusinessUniversity Colorado Boulder, Boulder CO, 80309 USASubbarao Kambhampatirao@asu.eduDepartment Computer Science EngineeringArizona State University, Tempe, AZ 85281 USAAbstractrepresent planning set loosely coupled network flow problems,network corresponds one state variables planning domain. networknodes correspond state variable values network arcs correspond valuetransitions. planning problem find path (a sequence actions) networkthat, merged, constitute feasible plan. paper present number integer programming formulations model loosely coupled networksvarying degrees flexibility. Since merging may introduce exponentially many orderingconstraints implement so-called branch-and-cut algorithm, constraintsdynamically generated added formulation needed. resultspromising, improve upon previous planning integer programming approacheslay foundation integer programming approaches cost optimal planning.1. Introductioninteger programming1 approaches automated planning ablescale well compilation approaches (i.e. satisfiability constraint satisfaction), extremely successful solution many real-world large scaleoptimization problems. Given integer programming framework potentialincorporate several important aspects real-world automated planning problems (forexample, numeric quantities objective functions involving costs utilities),significant motivation investigate effective integer programming formulationsclassical planning could lay groundwork large scale optimization (in termscost resources) automated planning. paper, study novel decomposition based approach automated planning yields effective integer programmingformulations.1. use term integer programming refer integer linear programming unless stated otherwise.c2008AI Access Foundation. rights reserved.fiVan den Briel, Vossen & KambhampatiDecomposition general approach solving problems efficiently. involvesbreaking problem several smaller subproblems solving subproblems separately. paper use decomposition break planning problemseveral interacting (i.e. loosely coupled) components. decomposition, planningproblem involves finding solutions individual components trying mergefeasible plan. general approach, however, prompts following questions:(1) components, (2) component solutions, (3) hardmerge individual component solutions feasible plan?1.1 Componentslet components represent state variables planning problem. Figure 1illustrates idea using small logistics example, one truck packageneeds moved location 1 location 2. total five componentsexample, one state variable. represent components appropriatelydefined network, network nodes correspond values state variable(for atoms = true F = false), network arcs correspond valuetransitions. source node network, represented small in-arc, correspondsinitial value state variable. sink node(s), represented double circles,correspond goal value(s) state variable. Note effects actiontrigger value transitions state variables. example, loading packagelocation 1 makes atom pack-in-truck true pack-at-loc1 false. addition, loadingpackage location 1 requires atom truck-at-loc1 true.idea components representing state variables planning problemused state variable representation, particularly synergistic multivalued state variables. Multi-valued state variables provide compact representationplanning problem binary-valued counterparts. Therefore, makingconversion multi-valued state variables reduce number componentscreate better partitioning constraints. Figure 2 illustrates use multi-valuedstate variables small logistics example. two multi-valued state variablesproblem, one characterize location truck one characterizelocation package. network representation, nodes correspond statevariable values (1 = at-loc1, 2 = at-loc2, = in-truck), arcs correspondvalue transitions.1.2 Component Solutionslet component solutions represent path value transitions state variables.networks, nodes arcs appear layers. layer represents plan periodwhich, depending structure network, one value transitionsoccur. networks Figures 1 2 three layers (i.e. plan periods)structure allows values persist change exactly per period. layers usedsolve planning problem incrementally. is, start one layer networktry solve planning problem. plan found, networks extendedone extra layer new attempt made solve planning problem. processrepeated plan found time limit reached. Figures 1 2, path (i.e.218fiLoosely Coupled Formulations Automated Planningloc1truck-at-loc1loc2Load loc1Ftruck-at-loc2-Drive loc1loc2-F-Unload loc2-FF-FUnload loc2FFFFLoad loc1-F-FFpack-in-truckFFLoad loc1Fpack-at-loc2Drive loc1loc2FFpack-at-loc1FUnload loc2FFFigure 1: Logistics example broken five components (binary-valued state variables)represented network flow problems.truck-location1Load loc12pack-location11Drive loc1loc22Load loc111Unload loc22-112Unload loc212222Figure 2: Logistics example broken two components (multi-valued state variables)represented network flow problems.solution) source node one sink nodes highlighted network.Since execution action triggers value transitions state variables, pathnetwork corresponds sequence actions. Consequently, planning problemthought collection network flow problems problem findpath (i.e. sequence actions) networks. However, interactions219fiVan den Briel, Vossen & Kambhampatinetworks impose side constraints network flow problems, complicatesolution process.1.3 Merging Processsolve loosely coupled networks using integer programming formulations. Onedesign choice make expand networks (i.e. components) together,cost finding solutions individual networks well merging dependsdifficulty solving integer programming formulation. This, turn, typically dependssize integer programming formulation, partly determinednumber layers networks. simplest idea numberlayers networks equal length plan, sequential planningplan length equals number actions plan. case, manytransitions networks actions plan, differencesequence actions corresponding path network could contain no-op actions.idea reduce required number layers allowing multiple actionsexecuted plan period. exactly done Graphplan (Blum & Furst,1995) planners adopted Graphplan-style definition parallelism.is, two actions executed parallel (i.e. plan period) longnon-interfering. formulations adopt general notions parallelism.particular, relax strict relation number layers networkslength plan changing network representation state variables.example, allowing multiple transitions network per plan period permitinterfering actions executed plan period. This, however, raises issuessolutions individual networks searched combined.network representations state variables allow multiple transitionsnetwork per plan period, thus become flexible, becomes harder mergesolutions feasible plan. Therefore, evaluate tradeoffs allowing flexiblerepresentations, look variety integer programming formulations.refer integer programming formulation uses network representationshown Figures 1 2 one state change model, allows onetransition (i.e. state change) per plan period state variable. Note networkrepresentation plan period mimics Graphplan-style parallelism. is, two actionsexecuted plan period one action delete preconditionadd-effect action. flexible representation values changepersist change refer generalized one statechange model. Clearly, increase number changes allow planperiod. representations values change twice k times, refergeneralized two state change generalized k state change model respectively.One disadvantage generalized k state change model creates one variableway k value changes, thus introduces exponentially many variables perplan period. Therefore, another network representation consider allows pathvalue transitions value visited per plan period.way, limit number variables, may introduce cycles networks.220fiLoosely Coupled Formulations Automated Planninginteger programming formulation uses representation referred statechange path model.general, allowing multiple transitions network per plan period (i.e. layer),complex merging process becomes. particular, merging process checkswhether actions solutions individual networks linearizedfeasible plan. integer programming formulations, ordering constraints ensure feasiblelinearizations. may, however, exponentially many ordering constraintsgeneralize Graphplan-style parallelism. Rather inserting constraintsinteger programming formulation front, add needed using branch-and-cutalgorithm. branch-and-cut algorithm branch-and-bound algorithm certainconstraints generated dynamically throughout branch-and-bound tree.show performance integer programming (IP) formulations show newpotential competitive SATPLAN04 (Kautz, 2004). significant resultforms basis sophisticated IP-based planning systems capablehandling numeric constraints non-uniform action costs. particular, new potentialIP formulations led successful use solving partial satisfaction planningproblems (Do, Benton, van den Briel, & Kambhampati, 2007). Moreover, initiatednew line work integer linear programming used heuristic state-spacesearch automated planning (Benton, van den Briel, & Kambhampati, 2007; van denBriel, Benton, Kambhampati, & Vossen, 2007).remainder paper organized follows. Section 2 provide briefbackground integer programming discuss approaches used integerprogramming solve planning problems. Section 3 present series integer programming formulations adopt different network representation. describeset loosely coupled networks, provide corresponding integer programmingformulation, discuss different variables constraints. Section 4 describebranch-and-cut algorithm used solving formulations. providegeneral background branch-and-cut concept show apply formulations means example. Section 5 provides experimental results determinecharacteristics approach greatest impact performance. Relatedwork discussed Section 6 conclusions given Section 7.2. BackgroundSince formulations based integer programming, briefly review techniquediscuss use planning. mixed integer program represented linear objectivefunction set linear inequalities:min{cx : Ax b, x1 , ..., xp 0 integer, xp+1 , ..., xn 0},(m n) matrix, c n-dimensional row vector, b m-dimensionalcolumn vector, x n-dimensional column vector variables. variablescontinuous (p = 0) linear program, variables integer (p = n)integer program, x1 , ..., xp {0, 1} mixed 0-1 program. set ={x1 , ..., xp 0 integer, xp+1 , ..., xn 0 : Ax b} called feasible region,n-dimensional column vector x called feasible solution x S. Moreover, function221fiVan den Briel, Vossen & Kambhampaticx called objective function, feasible solution x called optimal solutionobjective function small possible, is, cx = min{cx : x S}Mixed integer programming provides rich modeling formalism generalpropositional logic. propositional clause represented one linear inequality0-1 variables, single linear inequality 0-1 variables may require exponentially manyclauses (Hooker, 1988).widely used method solving (mixed) integer programs applyingbranch-and-bound algorithm linear programming relaxation, much easiersolve2 . linear programming (LP) relaxation linear program obtainedoriginal (mixed) integer program relaxing integrality constraints:min{cx : Ax b, x1 , ..., xn 0}Generally, LP relaxation solved every node branch-and-bound tree,(1) LP relaxation gives integer solution, (2) LP relaxation value inferiorcurrent best feasible solution, (3) LP relaxation infeasible, impliescorresponding (mixed) integer program infeasible.ideal formulation integer program one solution linear programming relaxation integral. Even though every integer program idealformulation (Wolsey, 1998), practice hard characterize ideal formulation may require exponential number inequalities. problems idealformulation cannot determined, often desirable find strong formulationinteger program. Suppose feasible regions P1 = {x Rn : A1 x b1 }P2 = {x Rn : A2 x b2 } describe linear programming relaxations two IP formulations problem. say formulation P1 stronger formulation P2P1 P2 . is, feasible region P1 subsumed feasible region P2 .words P1 improves quality linear relaxation P2 removing fractional extremepoints.exist numerous powerful software packages solve mixed integer programs.experiments make use commercial solver CPLEX 10.0 (Inc., 2002),currently one best LP/IP solvers.use integer programming techniques solve artificial intelligence planning problems intuitive appeal, especially given success IP solving similar typesproblems. example, IP used extensively solving problems transportation, logistics, manufacturing. Examples include crew scheduling, vehicle routing,production planning problems (Johnson, Nemhauser, & Savelsbergh, 2000). One potentialadvantage IP techniques provide natural way incorporate several importantaspects real-world planning problems, numeric constraints objective functionsinvolving costs utilities.Planning integer programming has, nevertheless, received limited attention. Onefirst approaches described Bylander (1997), proposes LP heuristicpartial order planning algorithms. LP heuristic helps reduce numberexpanded nodes, evaluation rather time-consuming. general, performance2. integer programming problem N P -complete (Garey & Johnson, 1979) linear programming problem polynomially solvable (Karmarkar, 1984).222fiLoosely Coupled Formulations Automated PlanningIP often depends structure problem problem formulated.importance developing strong IP formulations discussed Vossen et al. (1999),compare two formulations classical planning: (1) straightforward formulationbased conversion propositional representation SATPLAN yieldsmediocre results, (2) less intuitive formulation based representationstate transitions leads considerable performance improvements. Several ideasimprove formulation based representation state transitions describedDimopoulos (2001). ideas implemented IP-based planner Optiplan(van den Briel & Kambhampati, 2005). Approaches rely domain-specific knowledgeproposed Bockmayr Dimopoulos (1998, 1999). exploiting structureplanning problem IP formulations often provide encouraging results. useLP IP also explored non-classical planning. Dimopoulos Gerevini(2002) describe IP formulation temporal planning Wolfman Weld (1999)use LP formulations combination satisfiability-based planner solve resourceplanning problems. Kautz Walser (1999) also solve resource planning problems,use domain-specific IP formulations.3. Formulationssection describes four IP formulations model planning problem collectionloosely coupled network flow problems. network represents state variable,nodes correspond state variable values, arcs correspond valuetransitions. state variables based SAS+ planning formalism (Backstrom &Nebel, 1995), planning formalism uses multi-valued state variables insteadbinary-valued atoms. action SAS+ modeled pre-, post- prevailconditions. pre- post-conditions express state variables changedvalues must execution action, prevailconditions specify unchanged variables must specific valueexecution action. SAS+ planning problem described tuple= hC, A, s0 , where:C = {c1 , ..., cn } finite set state variables, state variable c Cassociated domain Vc implicitly defined extended domain Vc+ = Vc {u},u denotes undefined value. state variable c C, s[c] denotes valuec state s. value c said defined state s[c] 6= u.total state space = Vc1 ... Vcn partial state space + = Vc+1 ... Vc+nimplicitly defined.finite set actions form hpre, post, previ, pre denotes preconditions, post denotes post-conditions, prev denotes prevail-conditions.action A, pre[c], post[c] prev[c] denotes respective conditionsstate variable c. following two restrictions imposed actions: (1)value state variable defined, never become undefined. Hence,c C, pre[c] 6= u pre[c] 6= post[c] 6= u; (2) prevail- post-conditionaction never define value state variable. Hence, c C,either post[c] = u prev[c] = u both.223fiVan den Briel, Vossen & Kambhampatis0 denotes initial state + denotes goal state. SAS+planning allows initial state goal state partial states, assumes0 total state partial state. say state satisfiedstate c C s[c] = u s[c] = t[c]. implies[c] = u state variable c, defined value f Vc satisfies goal c.obtain SAS+ description planning problem use translator componentFast Downward planner (Helmert, 2006). translator stand-alone componentcontains general purpose algorithm transforms propositional descriptionplanning problem SAS+ description. algorithm provides efficientgrounding minimizes state description length based preprocessingalgorithm MIPS planner (Edelkamp & Helmert, 1999).remainder section introduce notation describe IP formulations. formulations presented way progressively generalizeGraphplan-style parallelism incorporation flexible network representations. formulation describe underlying network, define variablesconstraints. concentrate objective function muchconstraints tolerate feasible plans.3.1 Notationformulations described paper assume following information given:C: set state variables;Vc : set possible values (i.e. domain) state variable c C;Ec : set possible value transitions state variable c C;Gc = (Vc , Ec ) : directed domain transition graph every c C;State variables represented domain transition graph, nodes correspondpossible values, arcs correspond possible value transitions. exampledomain transition graph variable given Figure 3. example depictscomplete graph, domain transition graph need complete graph.Furthermore, assume given:Eca Ec represents effect action c;Vca Vc represents prevail condition action c;EAEc := {a : |Ec | > 0} represents actions effect c, Ac (e)represents actions effect e c;AVc := {a : |Vca | > 0} represents actions prevail condition c,AVc (f ) represents actions prevail condition f c;VC := {c C : AEc Ac } represents state variables actioneffect prevail condition.224fiLoosely Coupled Formulations Automated PlanningfghFigure 3: example domain transition graph, Vc = {f, g, h} possiblevalues (states) c Ec = {(f, g), (f, h), (g, f ), (g, h), (h, f ), (h, g)}possible value transitions c.Hence, action defined effects (i.e. pre- post-conditions) prevailconditions. SAS+ planning, actions one effect prevail conditionstate variable. words, c C, Eca Vcaempty |Eca | + |Vca | 1. example effects prevail conditions affect onedomain transition graphs given Figure 4.fffggghhhFigure 4: example action effects prevail conditions representeddomain transition graph. Action implications three state variables C ={c1 , c2 , c3 }. effects represented Eca1 = {(f, g)} Eca2 = {(h, f )},prevail condition represented Vca3 = {h}.addition, use following notation:Vc+ (f ): denote in-arcs node f domain transition graph Gc ;Vc (f ): denote out-arcs node f domain transition graph Gc ;+Pc,k(f ): denote paths length k domain transition graph Gc end+node f . Note Pc,1(f ) = Vc+ (f ).Pc,k(f ): denote paths length k domain transition graph Gc startnode f . Note Pc,1(f ) = Vc (f ).225fiVan den Briel, Vossen & Kambhampati(f ): denote paths length k domain transition graph G visit nodePc,kcf , start end f .3.2 One State Change (1SC) Formulationfirst IP formulation incorporates network representation seenFigures 1 2. name one state change relates number transitionsallow state variable per plan period. restriction allowing one valuetransition network also restricts actions execute planperiod. happens case network representation 1SC formulationincorporates standard notion action parallelism used Graphplan (Blum& Furst, 1995). idea actions executed plan period longdelete precondition add-effect another action. terms valuetransitions state variables, saying actions executed planperiod long change state variable (i.e. one valuechange value persistence state variable).3.2.1 State Change NetworkFigure 5 shows single layer (i.e. period) network underlies 1SC formulation. set IP formulation plan periods, + 1 layersnodes layers arcs network (the zeroth layer nodes initialstate remaining layers nodes arcs successive plan periods).possible state transition arc state change network. horizontalarcs correspond persistence value, diagonal arcs correspond valuechanges. solution path individual network follows arcs whose transitionssupported action effect prevail conditions appear solution plan.1SC networkffgghhPeriodFigure 5: One state change (1SC) network.3.2.2 Variablestwo types variables formulation: action variables represent execution action, arc flow variables represent state transitions network.226fiLoosely Coupled Formulations Automated Planninguse separate variables changes state variable (the diagonal arcs 1SCnetwork) persistence value state variable (the horizontal arcs1SC network). variables defined follows:xat {0, 1}, A, 1 ; xat equal 1 action executed plan periodt, 0 otherwise.yc,f,t {0, 1}, c C, f Vc , 1 ; yc,f,t equal 1 value f statevariable c persists period t, 0 otherwise.yc,e,t {0, 1}, c C, e Ec , 1 ; yc,e,t equal 1 transition e Ecstate variable c executed period t, 0 otherwise.3.2.3 Constraintstwo classes constraints. constraints network flowsstate variable network constraints action effects determine interactionsnetworks. 1SC integer programming formulation is:State change flows c C, f VcX1 f = s0 [c]yc,e,1 + yc,f,1 =0 otherwise.(1)eVc (f )XXyc,e,t+1 + yc,f,t+1 =Xyc,e,t + yc,f,t1 1(2)eVc+ (f )eVc (f )yc,e,T + yc,f,T= 1f = [c](3)eVc+ (f )Action implications c C, 1Xxat = yc,e,t e Ec(4)aA:eEcaxat yc,f,tA, f Vca(5)Constraints (1), (2), (3) network flow constraints state variable c C.Constraint (1) ensures path state transitions begins initial statestate variable constraint (3) ensures that, goal exists, path ends goalstate state variable. Note that, goal value state variable c undefined(i.e. [c] = u) path state transitions may end values f Vc .Hence, need goal constraint state variables whose goal states [c]undefined. Constraint (2) flow conservation equation enforces continuityconstructed path.Actions may introduce interactions state variables. instance, effectsload action logistics example affect two different state variables. Actions linkstate variables interactions represented action implication227fiVan den Briel, Vossen & Kambhampaticonstraints. transition e Ec , constraints (4) link action execution variablese effect (i.e. e Eca ) arc flow variables. example, actionxat effect e Eca executed, path state variable c must follow arcrepresented yc,e,t. Likewise, choose follow arc represented yc,e,t,exactly one action xat e Eca must executed. summation left hand sideprevents two actions interfering other, hence one action maycause state change e state variable c period t.Prevail conditions action link state variables similar way action effectsdo. Specifically, constraint (5) states action executed period (xat = 1),prevail condition f Vca required state variable c period (yc,f,t = 1).3.3 Generalized One State Change (G1SC) Formulationsecond formulation incorporate network representation 1SCformulation, adopt general interpretation value transitions, leadsunconventional notion action parallelism. G1SC formulation relaxcondition parallel actions arranged order requiring weaker condition.allow actions executed plan period long exists orderingfeasible. specifically, within plan period set actions feasible (1)exists ordering actions preconditions satisfied, (2)one state change state variables. generalization conditionssimilar Rintanen, Heljanko Niemela (2006) refer -step semanticssemantics.illustrate basic concept, let us examine small logistics example introduced Figure 1. solution problem load package location 1, drivetruck location 1 location 2, unload package location 2. Clearly, planwould require three plan periods Graphplan-style parallelism three actionsinterfere other. If, however, allow load loc1 drive loc1 loc2action executed plan period, exists orderingtwo actions feasible, namely load package location 1 drivingtruck location 2. key idea behind example clear: maypossible find set actions linearized order, may nevertheless exist ordering actions viable. question is, course,incorporate idea IP formulation.truck-location1Load loc1Drive loc1loc22pack-location11Unload loc22Load loc1112Unload loc21222Figure 6: Logistics example represented network flow problems generalized arcs.228fiLoosely Coupled Formulations Automated Planningexample illustrates looking set constraints allow setsactions which: (1) action preconditions met, (2) exists orderingactions plan period feasible, (3) within state variable, valuechanged once. incorporation ideas requires minor modifications1SC formulation. Specifically, need change action implication constraintsprevail conditions add new set constraints call orderingimplication constraints.3.3.1 State Change Networkminor modifications revealed G1SC network. networkidentical 1SC network, interpretation transition arcs somewhat different.incorporate new set conditions, implicitly allow values persist (the dashedhorizontal arcs G1SC network) tail head transition arc.interpretation implicit arcs plan period value may requiredprevail condition, value may change, new value may also requiredprevail condition shown Figure 7.G1SC networkGeneralized state change arcffffgggghhhhPeriodPeriodFigure 7: Generalized one state change (G1SC) network.3.3.2 VariablesSince G1SC network similar 1SC network variables used, thus,action variables represent execution action, arc flow variables representflow network. difference interpretation state change arcsdealt constraints G1SC formulation, therefore introducenew variables. variable definitions, refer Section 3.2.2.3.3.3 Constraintsthree classes constraints, is, constraints network flowsstate variable network, constraints linking flows action effects prevailconditions, ordering constraints ensure actions plan linearizedfeasible ordering.229fiVan den Briel, Vossen & Kambhampatinetwork flow constraints G1SC formulation identical 1SCformulation given (1)-(3). Moreover, constraints link flows actioneffects equal action effect constraints 1SC formulation given (4).G1SC formulation differs 1SC formulation relaxes conditionparallel actions arranged order requiring weaker condition. weakercondition affects constraints link flows action prevail conditions,introduces new set ordering constraints. constraints G1SC formulationgiven follows:Action implications c C, 1XXxat yc,f,t +yc,e,t +eVc+ (f )Ordering implicationsXyc,e,tA, f Vca(6)eVc (f )xat |V ()| 1 cycles Gprec(7)aV ()Constraint (6) incorporates new set conditions actions executedplan period. particular, need ensure state variable c,value f Vc holds required prevail condition action plan period t.three possibilities: (1) value f holds c throughout period. (2) value fholds initially c, value changed value f another action. (3)value f hold initially c, value changed f another action.either three cases value f holds point period prevailcondition action satisfied. words, value f may prevail implicitly longstate change includes f . before, prevail implication constraintslink action prevail conditions corresponding network arcs.action implication constraints ensure preconditions actionsplan satisfied. This, however, guarantee actions linearizedfeasible order. Figure 7 indicates implied orderings actions.Actions require value f prevail condition must executed actionchanges f g. Likewise, action changes f g must executedactions require value g prevail condition. state change flow actionimplication constraints outlined indicate ordering actions,ordering could cyclic therefore infeasible. make sure orderingacyclic start creating directed implied precedence graph Gprec = (V prec , E prec ).graph nodes V prec correspond actions, is, V prec = A,create directed arc (i.e. ordering) two nodes (a, b) E prec actionexecuted action b time period t, b executed a. particular,[[E prec =(a, b)(a, b)(a,b)AA,cC,f Vca ,eEcb :eVc,f230(a,b)AA,cC,gVcb ,eEca :+eVc,gfiLoosely Coupled Formulations Automated Planningimplied orderings become immediately clear Figure 8. figure leftdepicts first set orderings expression E prec . says orderingtwo actions b executed plan period implied action requiresvalue prevail action b deletes. Similarly, figure right depicts second setorderings expression E prec . is, ordering implied action addsprevail condition b.fffbfggghhhbghFigure 8: Implied orderings G1SC formulation.ordering implication constraints ensure actions final solutionlinearized. basically involve putting n-ary mutex relation actionsinvolved cycle. Unfortunately, number ordering implication constraintsgrows exponentially number actions. result, impossible solveresulting formulation using standard approaches. address complicationimplementing branch-and-cut approach ordering implication constraintsadded dynamically formulation. approach discussed Section 4.3.4 Generalized k State Change (GkSC) FormulationG1SC formulation actions executed plan period (1) existsordering actions preconditions satisfied, (2) occursone value change state variables. One obvious generalizationwould relax second condition allow kc value changes statevariable c, kc |Vc | 1. allowing multiple value changes state variable perplan period we, fact, permit series value changes. Specifically, GkSC modelallows series value changes.Obviously, tradeoff loosening networks versus amountwork takes merge individual plans. implemented GkSCformulation, provide insight tradeoff describing evaluating GkSCformulation kc = 2 c C refer special case generalizedtwo state change (G2SC) formulation. One reason restrict special casegeneral case k state changes would introduce exponentially many variablesformulation. IP techniques, however, deal exponentially manyvariables (Desaulniers, Desrosiers, & Solomon, 2005), discuss here.3.4.1 State Change Networknetwork underlies G2SC formulation equivalent G1SC, spans extralayer nodes arcs. extra layer allows us series two transitions per plan231fiVan den Briel, Vossen & Kambhampatiperiod. transitions generalized implicitly allow values persistG1SC network. Figure 9 displays network corresponding G2SC formulation.G2SC network generalized one two state change arcs. example,generalized one state change arc transition (f, g), generalized twostate changes arc transitions {(f, g), (g, h)}. Since arcs generalized, valuevisited also persisted. also allow cyclic transitions, as, {(f, g), (g, f )}f prevail condition action. allow cyclic transitionsf prevail condition action, action ordering plan periodimplied anymore (i.e. prevail condition f would either occurvalue transitions g, transitions back f ). Thus prevail conditionf safely allow cyclic transition {(f, g), (g, f )}.1 state change arcs2 state changes arcsffffffgggggghhhhhhPeriodPeriodFigure 9: Generalized two state change (G2SC) network. left subnetworkconsists generalized one state change arcs no-op arcs, right subnetwork consists generalized two state change arcs. subnetworktwo state change arcs may include cyclic transitions, as, {(f, g), (g, f )}long f prevail condition action.3.4.2 Variablesvariables representing execution action, variables representing flows one state change (diagonal arcs) persistence (horizontal arcs).addition, variables representing paths two consecutive state changes. Hence,variables pair state changes (f, g, h) (f, g) Ec (g, h) Ec .restrict paths visit unique values only, is, f 6= g, g 6= h, h 6= f ,f prevail condition action also allow paths f = h.variables G1SC formulation also used G2SC formulation. is, however,additional variable represent arcs allow two state changes:yc,e1,e2 ,t {0, 1}, c C, (e1 , e2 ) Pc,2 , 1 ; yc,e1,e2 ,t equal 1exists value f Vc transitions e1 , e2 Ec , e1 Vc+ (f )e2 Vc (f ), state variable c executed period t, 0 otherwise.232fiLoosely Coupled Formulations Automated Planning3.4.3 Constraintsthree classes constraints, given follows:State change flows c C, f VcXyc,e1,e2 ,1 +(e1 ,e2 )Pc,2(f )XXyc,e,1 + yc,f,1 =eVc (f )yc,e1,e2 ,t+1 +(e1 ,e2 )Pc,2(f )X10f = s0 [c]otherwise.(8)1 1(9)yc,e,t+1 + yc,f,t+1 =eVc (f )X+(e1 ,e2 )Pc,2(f )XXyc,e1,e2 ,t +yc,e,t + yc,f,teVc+ (f )yc,e1,e2 ,T ++(e1 ,e2 )Pc,2(f )Xyc,e,T + yc,f,T= 1 {f [c]}(10)eVc+ (f )Action implications c C, 1XXxat = yc,e,t +aA:eEcayc,e1,e2 ,te EcXyc,e1,e2 ,t +(11)(e1 ,e2 )Pc,2 :e1 =ee2 =exat yc,f,t +XeVc+ (f )XXyc,e,t +eVc (f )Xyc,e1,e2 ,t ++(e1 ,e2 )Pc,2(f )Ordering implicationsXyc,e,t +(f )(e1 ,e2 )Pc,2yc,e1,e2 ,tA, f Vca(12)(e1 ,e2 )Pc,2(f )xat |V ()| 1 cycles Gprec(13)aV ()Constraints (8), (9), (10) represent flow constraints G2SC network. Constraints (11) (12) link action effects prevail conditions correspondingflows, constraint 13 ensures actions linearized feasible ordering.3.5 State Change Path (PathSC) Formulationseveral ways generalize network representation G1SC formulationloosen interaction networks. GkSC formulation presented onegeneralization allows k transitions state variable per plan period. Sinceuses exponentially many variables another way generalize network representationG1SC formulation requiring value true per planperiod. illustrate idea consider logistics example again, use233fiVan den Briel, Vossen & Kambhampatitruck-location1Load loc1Drive loc1loc2Unload loc22pack-location112Load loc1Unload loc2-122Figure 10: Logistics example represented network flow problems allow pathvalue transitions per plan period value true once.network representation allows path transitions per plan period depictedFigure 10.Recall solution logistics example consists three actions: first loadpackage location 1, drive truck location 1 location 2, last unloadpackage location 2. Clearly, solution would allowed within single planperiod Graphplan-style parallelism. Moreover, would also allowed withinsingle period G1SC formulation. reason number valuechanges package-location state variable two. First, changes pack-at-loc1pack-in-truck, changes pack-in-truck pack-at-loc2. before, however,exists ordering three actions feasible. key idea behindexample show allow multiple value changes single period. limitvalue changes state variable simple paths, is, one period valuevisited once, still use implied precedences determine orderingrestrictions.3.5.1 State Change Networkformulation value true plan period, hencenumber value transitions plan period limited kc kc = |Vc | 1c C. PathSC network, nodes appear layers correspond valuesstate variable. However, layer consists twice many nodes. setIP encoding maximum number plan periods layers. Arcswithin layer correspond transitions value persistence, arcs layersensure plan periods connected other.Figure 11 displays network corresponding state variable c domain Vc ={f, g, h} allows multiple transitions per plan period. arcs pointing rightwardscorrespond persistence value, arcs pointing leftwards correspondvalue changes. one plan period needed curved arcs pointing rightwards234fiLoosely Coupled Formulations Automated Planninglink layers two consecutive plan periods. Note unit capacityarcs, path network visit node once.PathSC networkffgghhPeriodFigure 11: Path state change (PathSC) network.3.5.2 Variablesaction execution variables arc flow variables (as defined previousformulations), linking variables connect networks two consecutivetime periods. variables defined follows:zc,f,t {0, 1}, c C, f Vc , 0 ; zc,f,t equal 1 value f statevariable c end value period t, 0 otherwise.3.5.3 Constraintsprevious formulations, state change flow constraints, action implicationconstraints, ordering implication constraints. main difference underlyingnetwork. PathSC integer programming formulation given follows:State change flows c C, f Vc1 f = s0 [c]zc,f,0 =0 otherwise.Xyc,e,t + zc,f,t1 = yc,f,t(14)(15)eVc+ (f )yc,f,t =Xyc,e,t + zc,f,t1 1(16)eVc (f )zc,f,T= 1 f [c]Action implications c C, 1Xxat = yc,e,t(17)e Ec(18)f Vca(19)aA:eEcaxat yc,f,t235fiVan den Briel, Vossen & KambhampatiOrdering implicationsXxat |V ()| 1 cycles Gprec(20)aV ()Constraints (14)-(17) network flow constraints. node, except initialgoal state nodes, ensure balance flow (i.e. flow-in must equal flow-out).initial state node supply one unit flow goal state node demandone unit flow, given constraints (14) (17) respectively. interactionsactions impose upon different state variables represented action implicationconstraints (18) (19), discussed earlier.implied precedence graph formulation given Gprec = (V prec , E prec ).extra set arcs incorporate implied precedences introducedtwo actions imply state change class c C. nodes V preccorrespond actions, arc (a, b) E prec action executedaction b time period, b executed a. specifically,E prec[= E prec(a, b)(a,b)AA,cC,f Vc ,eEca ,e Ecb :eVc+ (f )e Vc (f )before, ordering implication constraints (20) ensure actions solution plan linearized feasible ordering.4. Branch-and-Cut AlgorithmIP problems usually solved LP-based branch-and-bound algorithm. basicstructure technique involves binary enumeration tree branches prunedaccording bounds provided LP relaxation. root node enumeration treerepresents LP relaxation original IP problem node representssubproblem objective function constraints root node exceptadditional bound constraints. IP solvers use LP-based branch-and-boundalgorithm combination various preprocessing probing techniques. lastyears significant improvement performance solvers (Bixby,2002).LP-based branch-and-bound algorithm, LP relaxation original IP problem (the solution root node) rarely integer. integer variable xfractional solution v branch create two new subproblems, boundconstraint x v added left-child node, x v added right-childnode. branching process carried recursively expand subproblems whosesolution remains fractional. Eventually, enough bounds placed variables,integer solution found. value best integer solution found far, Z , referredincumbent used pruning.236fiLoosely Coupled Formulations Automated Planningminimization problem, branches emanating nodes whose solution value ZLPgreater current incumbent, Z , never give rise better integer solutionchild node smaller feasible region parent. Hence, safely eliminatenodes consideration prune them. Nodes whose feasible regionreduced empty set, many bounds placed variables,pruned well.solving IP problem LP-based branch-and-bound algorithm mustconsider following two decisions. several integer variables fractional solution,variable branch next, branch currently workingpruned, subproblem solve next? Basic rules include usefractional variable rule branching variable selection best objective value rulenode selection.formulations standard LP-based branch-and-bound algorithm approachineffective due large number (potentially exponentially many) ordering implicationconstraints G1SC, G2SC, PathSC formulations. possible reducenumber constraints introducing additional variables (Martin, 1991), resultingformulations would still intractable smallest problem instances. Therefore,solve IP formulations so-called branch-and-cut algorithm, considersordering implication constraints implicitly. branch-and-cut algorithm branch-andbound algorithm certain constraints generated dynamically throughoutbranch-and-bound tree. flowchart branch-and-cut algorithm given Figure 12.If, solving LP relaxation, unable prune node basisLP solution, branch-and-cut algorithm tries find violated cut, is, constraintvalid satisfied current solution. also known separationproblem. one violated cuts found, constraints added formulationLP solved again. none found, algorithm creates branchenumeration tree (if solution current subproblem fractional) generatesfeasible solution (if solution current subproblem integral).basic idea branch-and-cut leave constraints LP relaxationmany handle efficiently, add formulationbecome binding solution current LP. Branch-and-cut algorithmssuccessfully applied solving hard large-scale optimization problems widevariety applications including scheduling, routing, graph partitioning, network design,facility location problems (Caprara & Fischetti, 1997).branch-and-cut algorithm stop soon find first feasible solution,implicitly enumerate nodes (through pruning) find optimal solutiongiven objective function. Note formulations used find boundedlength optimal plans. is, find optimal plan given plan period (i.e. boundedlength). experimental results, however, focus finding feasible solutions.4.1 Constraint Generationpoint runtime cut generator called solutioncurrent LP problem, consists LP relaxation original IP problem plusadded bound constraints added cuts. implementation branch-and-cut237fiVan den Briel, Vossen & KambhampatiSTARTSTOPInitialize LPyesLP solverFeasible?PruneNodes found?Node selectionyesZLP > Z*?yesCut generatoryesCuts found?Integer?BranchingyesOptimize?yesFigure 12: Flowchart branch-and-cut algorithm. finding feasible solution (i.e.optimize = no) algorithm stops soon first feasible integer solutionfound. searching optimal solution (i.e. optimize = yes)given formulation continue open nodes left.238fiLoosely Coupled Formulations Automated Planningalgorithm, start LP relaxation ordering implication constraintsomitted. given solution current LP relaxation, could fractional,separation problem determine whether solution violates one omitted orderingimplication constraints. so, identify violated ordering implication constraints, addformulation, resolve new problem.4.1.1 Cycle IdentificationG1SC, G2SC, PathSC formulations ordering implication constraint violated cycle implied precedence graph. Separation problems involvingcycles occur numerous applications. Probably best known kind traveling salesman problem subtours (i.e. cycles) identified subtour eliminationconstraints added current LP. algorithm separating cycles basedone described Padberg Rinaldi (1991). interested finding shortestcycle implied precedence graph, shortest cycle cuts fractional extremepoints. general idea behind approach follows:1. Given solution LP relaxation, determine subgraph Gt plan periodconsisting nodes xat > 0.2. arcs (a, b) Gt , define weights wa,b := xat + xbt 1.3. Determine shortest path distance da,b pairs ((a, b) Gt ) based arcweights wa,b := 1 wa,b (for example, using Floyd-Warshall all-pairs shortest pathalgorithm).4. da,b wb,a < 0 arc (a, b) Gt , exists violated cycle constraint.general principles behind branch-and-cut algorithms rather straightforward, number algorithmic implementation issues may significant impact overall performance. heart issues trade-offcomputation time spent node enumeration tree number nodesexplored. One issue, example, decide generate violated cuts.Another issue generated cuts (if any) added LP relaxation,whether delete constraints added LP before.implementation, addressed issues straightforward manner: cutsgenerated every node enumeration tree, first cut found algorithmadded, constraints never deleted LP relaxation. However, given potential advanced strategies observed applications, believestill may considerable room improvement.4.1.2 Examplesection show workings branch-and-cut algorithm G1SCformulation using small hypothetical example involving two state variables c1 c2 , fiveactions A1, A2, A3, A4, A5, one plan period. particular showcycle detection procedure works ordering implication constraint generated.239fiVan den Briel, Vossen & KambhampatiFigure 13 depicts solution current LP planning problem. state variablec1 actions A1 A2 prevail condition g, A4 prevail conditionh, action A3 effect changes g h. Likewise, state variable c2action A4 effect changes g f , action A5 changes g h,action A1 prevail condition f . Note given solution fractional. Thereforeaction variables fractional values. particular, xA1 = xA4 = 0.8,xA5 = 0.2, xA2 = xA3 = 1. words, actions A2 A3 fully executedactions A1, A4 A5 fractionally executed. Clearly, automated planningfractional execution action meaning whatsoever, commonLP relaxation IP formulation gives fractional solution. simply try showfind violated cut even fractional solution. Also, noteactions A4 A5 interfering effects c2 . would generally infeasible,actions executed fractionally, actually feasible solution LPrelaxation IP formulation.State variable 1State variable 2A1ffffA4A1,A2ggghhgA3A4hA5A1 = A4 = 0.8, A2 = A3 = 1hA1 = A4 = 0.8, A5 = 0.2Figure 13: Solution small hypothetical planning example. solution currentLP flows indicated paths executes actions A1, A2, A3, A4,A5.order determine whether actions linearized feasible ordering first create implied precedence graph Gprec = (V prec , E prec ),V prec = {A1, A2, A3, A4, A5} E prec = {(A1, A3),(A2, A3),(A3, A4),(A4, A1)}. ordering (A1, A3), example, established effects actions state variablec1 . A1 prevail condition g c1 A3 changes g h c1 , impliesA1 must executed A3. orderings established similar way.complete implied precedence graph example given Figure 14.cycle detection algorithm gets implied precedence graph solutioncurrent LP input. Weights arc (a, b) E prec determined valuesaction variables current solution. LP solution given Figure13, example wA1,A3 = wA3,A4 = 0.8, wA2,A3 = 1, wA4,A1 = 0.6.length shortest path A1 A4 using weights wa,b equal 0.4 (0.2+0.2). Hence,dA1,A4 = 0.4 wA4,A1 = 0.6. Since dA1,A4 wA4,A1 < 0, violated cycle(i.e. violated ordering implication) includes actions shortest pathA1 A4 (i.e. A1, A3, A4, retrieved shortest path algorithm).240fiLoosely Coupled Formulations Automated PlanningA3A4generates following ordering implication constraint xA11 + x1 + x1 2,added current LP. Note ordering constraint violated currentA3A4LP solution, xA11 + x1 + x1 = 0.8 + 1 + 0.8 = 2.6. constraint addedLP, next solution select set actions violate newly added cut.procedure continues cuts violated solution integer.A1Implied precedence graph(0.6,0.4)(0.8,0.2)A4(0.8,0.2)A3(1,0)A2A5Figure 14: Implied precedence graph example, labels show (wa,b , wa,b ).5. Experimental Resultsdescribed formulations based two key ideas. first idea decomposeplanning problem several loosely coupled components represent componentsappropriately defined network. second idea reduce number planperiods adopting different notions parallelism use branch-and-cut algorithmdynamically add constraints formulation order deal exponentiallymany action ordering constraints efficient manner.evaluate tradeoffs allowing flexible network representations compareperformance one state change (1SC) formulation, generalized one state changeformulation (G1SC), generalized two state change (G2SC) formulation, statechange path (PathSC) formulation. easy reference, overview formulationsgiven Figure 15.experiments focus finding feasible solutions. Note, however,formulations used bounded length optimal planning. is, given planperiod (i.e. bounded length), find optimal solution.5.1 Experimental Setupcompare analyze formulations use STRIPS domains secondthird international planning competitions (IPC2 IPC3 respectively). is,Blocksworld, Logistics, Miconic, Freecell IPC2 Depots, Driverlog, Zenotravel,Rovers, Satellite, Freecell IPC3. compare formulationsSTRIPS domains IPC4 IPC5 mainly peripheral limitationcurrent implementation G2SC PathSC formulations. particular, G2SCformulation cannot handle operators change state variable undefined valuedefined value, PathSC formulation cannot handle operators domain241fiVan den Briel, Vossen & Kambhampati1SCstate variable changeprevail value perplan period.G1SCstate variable change (andprevail valuechange) per plan period.ffffgggghhhhG2SCstate variable change (andprevail valuechange) twice per planperiod. Cyclic changes (f, g, f )allowed fprevail condition actionPathSCstate variable changenumber times, valuetrue per planperiod.ffffffgggggghhhhhhFigure 15: Overview 1SC, G1SC, G2SC, PathSC formulations.size state variable larger two. limitations could testG2SC formulation Miconic, Satellite Rovers domains, could testPathSC formulation Satellite domain.order setup formulations translate STRIPS planning problemmulti-valued state description using translator Fast Downward planner (Helmert,2006). formulation uses network representation starts settingnumber plan periods equal one. try solve initial formulationplan found, increased one, try solve new formulation. Hence,IP formulation solved repeatedly first feasible plan found 30 minutetime limit (the time limit used international planning competitions)reached. use CPLEX 10.0 (ILOG Inc., 2002), commercial LP/IP solver, solvingIP formulations 2.67GHz Linux machine 1GB memory.set experiments follows. First, Section 5.2 provide brief overviewmain results looking aggregated results IPC2 IPC3. Second, Section5.3, give detailed analysis loosely coupled encodings planning242fiLoosely Coupled Formulations Automated Planningfocus tradeoffs reducing number plan periods solve planning problemversus increased difficulty merging solutions different components. Third,Section 5.4 briefly touch upon different state variable representationsplanning problem influence performance.5.2 Results Overviewgeneral overview compare formulations following planning systems:Optiplan (van den Briel & Kambhampati, 2005), SATPLAN04 (Kautz, 2004), SATPLAN06(Kautz & Selman, 2006), Satplanner (Rintanen et al., 2006)3 .Optiplan integer programming based planner participated optimal trackfourth international planning competition4 . Like formulations, Optiplan modelsstate transitions use factored representation planning domain.particular, Optiplan represents state transitions atoms planning domain,whereas formulations use multi-valued state variables. Apart this, Optiplansimilar 1SC formulation adopt Graphplan-style parallelism.SATPLAN04, SATPLAN06, Satplanner satisfiability based planners. SATPLAN04 SATPLAN06 versions well known system SATPLAN (Kautz &Selman, 1992), long track record international planning competitions.Satplanner received much attention, among state-of-the-art planning satisfiability. Like formulations Satplanner generalizes Graphplan-styleparallelism improve planning efficiency.main results summarized Figure 16. displays aggregate results IPC2IPC3, number instances solved (y-axis) drawn function log time(x-axis). must note graph IPC2 results favors PathSC formulationplanners. However, see Section 5.3, mainly reflectionexceptional performance Miconic domain rather overall performanceIPC2. Morever, graph IPC3 results include Satellite domain.decided remove domain, could run public versionsSATPLAN04 SATPLAN06 G2SC PathSC formulations. resultsFigure 16 provide rather coarse overview, sum following main findings.Factored planning using loosely coupled formulations helps improve performance. Noteinteger programming formulations use factored representations,1SC, G1SC, G2SC, PathSC (except G2SC formulation could3. note SATPLAN04, SATPLAN06, Optiplan, 1SC formulation step-optimalG1SC, G2SC, PathSC formulations not. is, however, considerable controversyplanning community whether step-optimality guaranteed Graphplan-style plannersconnection plan quality metrics users would interested in. refer readerKambhampati (2006) longer discussion issue us several prominent researchersplanning community. Given background, believe quite reasonable compare formulations step-optimal approaches, especially since main aim show IP formulationscome long way made competitive respect SAT-based encodings.turn makes worthwhile consider exploiting features IP formulations,amenability variety optimization objectives done recent work (van den Brielet al., 2007).4. list participating planners results available http://ipc04.icaps-conference.org/243fiVan den Briel, Vossen & Kambhampatitested domains), able solve problem instances given amounttime Optiplan, use factored representation. Especially,difference 1SC Optiplan remarkable adopt Graphplanstyle parallelism. Section 5.3, however, see Optiplan perform welldomains either serial design significant serial component.Decreasing encoding size relaxing Graphplan-style parallelism helps improveperformance. surprising, Dimopoulos et al. (1997) already notereduction number plan periods helps improve planning performance. However, always hold tradeoff reducing numberplan periods versus increased difficulty merging solutions differentcomponents. Section 5.3 see different relaxations Graphplan-styleparallelism lead different results. example, PathSC formulation shows superior performance Miconic Driverlog, poorly Blocksworld, Freecell,Zenotravel. Likewise, G2SC formulation well Freecell,seem excel domain.2009018080160Solved intstances (IPC3)Solved intstances (IPC2)Planning integer programming shows new potential. conventional wisdomplanning community planning integer programming cannot compete planning satisfiability constraint satisfaction. Figure 16, however,see 1SC, G1SC PathSC formulation compete quite wellSATPLAN04. SATPLAN04 state-of-the-art planning satisfiability anymore, show planning integer programming come longway. fact IP competitive allows us exploit virtuesoptimization (Do et al., 2007; Benton et al., 2007; van den Briel et al., 2007).140120100806040SatplannerSAT04G1SCPathSC20SAT061SCG2SCOptiplan101006050403020100017010001101001000Solution time (sec)Solution time (sec)Figure 16: Aggregate results second third international planning competitions.5.3 Comparing Loosely Coupled Formulations Planningsection compare IP formulations try evaluate benefits allowingflexible network representations. Specifically, interested effects reducing number plan periods required solve planning problem versus dealing244fiLoosely Coupled Formulations Automated Planningmerging solutions different components. Reducing number plan periodslead smaller encodings, lead improved performance. However, also makesmerging loosely coupled components harder, could worsen performance.order compare formulations analyze following two things. First,examine performance formulations comparing solution times probleminstances IPC2 IPC3. comparison include results Optiplangives us idea differences formulation based Graphplanformulations based loosely coupled components. Moreover, also show usimprovements IP based approaches planning. Second, examine numberplan periods formulation needs solve problem instance. Also, looktradeoffs reducing number plan periods increased difficultymerging solutions loosely coupled components. comparisoninclude results Satplanner because, like formulations, adopts generalizednotion Graphplan-style parallelism.use following figures table. Figure 17 shows total solution time (y-axis)needed solve problem instances (x-axis), Figure 18 shows number plan periods(y-axis) solve problem instances (x-axis), Table 1 shows number orderingconstraints added solution process, seen indicatormerging effort. selected problem instances Table 1 represent five largestinstances could solved formulations (in domains, however,formulations could solve least five problem instances).label GP steps Figure 18 represents number plan steps SATPLAN06,state-of-the-art Graphplan-based planner, would use. Satellite domain, however,use results 1SC formulation unable run public versionSATPLAN06 domain. like point Figure 18 intendedfavor one formulation other, simply shows possible generate encodings automated planning use drastically fewer plan periods Graphplan-basedencodings.5.3.1 Results: Planning PerformanceBlocksworld domain Optiplan solves problems formulations. Zenotravel Satellite, Optiplan generally outperformed respectsolution time, Rovers Freecell, Optiplan generally outperformed respectnumber problems solved. IP formulations, G1SC providesoverall best performance performance PathSC formulation somewhatirregular. example, Miconic, Driverlog Rovers PathSC formulationwell, Depots Freecell rather poorly.Logistics domain formulations generalize Graphplan-style parallelism(i.e. G1SC, G2SC, PathSC) scale better 1SC formulation Optiplan,adopt Graphplan-style parallelism. Among G1SC, G2SC, PathSC formulationsclear best performer, larger Logistics problems G1SC formulationseems slightly better. Logistics domain provides great example tradeoffflexibility merging. allowing actions executed planperiod, generally shorter plans (in terms number plan periods) needed solve245fiVan den Briel, Vossen & Kambhampatiplanning problem (see Figure 18), time merging solutionsindividual components harder one respect ordering constraints (seeTable 1).Optiplan versus 1SC. compare 1SC formulation Optiplan, noteOptiplan fares well domains either serial design (Blocksworld)domains significant serial aspect (Depots). think Optiplans advantage1SC formulation domains due following two possibilities. First,intuition serial domains reachability relevance analysis Graphplanstronger detecting infeasible action choices (due mutex propagation) networkflow restrictions 1SC formulation. Second, appears state variablesdomains tightly coupled (i.e. actions effects, thus transitions onestate variable coupled several transitions state variables)domains, may negatively affect performance 1SC formulation.1SC versus G1SC. comparing 1SC formulation G1SC formulationsee domains, except Blocksworld Miconic, G1SC formulationsolves least many problems 1SC formulation. results Blocksworldsurprising attributed semantics domain. operatorBlocksworld requires one state change state variable arm (stack putdownchange status arm arm empty, unstack pickup change statusarm holding x x block lifted). Since, 1SCG1SC formulations allow one state change state variable,possibility G1SC formulation allow one action executedplan period. Given this, one may think 1SC G1SC formulationssolve least number problems, case prevail constraints (5)1SC formulation stronger prevail constraints (6) G1SC formulation.is, right-hand side (6) subsumes (i.e. allows larger feasible region LPrelaxation) right-hand side (5). Figure 17 see slight advantage1SC G1SC Blocksworld domain.results Miconic domain are, hand, intuitive.would expected G1SC formulation solve least many problems 1SCformulation, turn case. One thing noticeddomain G1SC formulation required lot time determine plangiven number plan periods.G1SC versus G2SC PathSC. Table 1 shows five largest problemsdomain solved formulations, yet representative whole setproblems. table indicates Graphplan-style parallelism generalized,ordering constraints needed ensure feasible plan. average, G2SC formulationincludes ordering constraints G1SC formulation, PathSC formulationturn includes ordering constraints G2SC formulation. performanceformulations shown Figure 17 varies per planning domain. PathSCformulation well Miconic Driverlog, G2SC formulation well Freecell,G1SC well Zenotravel. performance differences, believeideal amount flexibility generalization Graphplan-style parallelismdifferent planning domain.246fiLoosely Coupled Formulations Automated Planning10000Optiplan1SCG1SCG2SCPathSC1000100Solution time (sec.)Solution time (sec.)100001010.1Optiplan1SCG1SCG2SCPathSC10001001010.10.010.011234567819 10 11 12 13 14 15 16 1735791113100001000100010010Optiplan1SCG1SCG2SCPathSC10.10.013579111315171921Solution time (sec.)Solution time (sec.)1000010.123Optiplan1SCG1SCG2SCPathSC10.10.018Solution time (sec.)Solution time (sec.)107Optiplan1SCG1SCG2SCPathSC1001010.10.019 10 11 12 13 14 15 16 17123456Depots10000Optiplan1SCG1SCG2SCPathSC10001001010.10.018910 11 12 13 14 15Optiplan1SCG1SCPathSC10001001010.10.0112345678910 11 12 13 14 15 16123456Zenotravel100001000010001000100101Optiplan1SCG1SC0.10.01123456789 10 11 12 13 14 15 16 17 18Rovers7891011Solution time (sec.)Solution time (sec.)7DriverlogSolution time (sec.)Solution time (sec.)1000027Miconic1006251 10 19 28 37 46 55 64 73 82 91 100 109 118 127 136 1451000523Optiplan1SCG1SCG2SCPathSC0.0110004211100003191010000217100Freecell (IPC2)115LogisticsBlocksworld10010Optiplan1SCG1SCG2SCPathSC10.10.011Satellite2345Freecell (IPC3)Figure 17: Solution times planning domains second third internationalplanning competition.247fiVan den Briel, Vossen & Kambhampati60#Plan periods40#Plan periodsGPstepsSatplannerG1SCG2SCPathSC50302010013571816141210864209 11 13 15 17 19 21 23 25 27 29 31 33 35GPstepsSatplannerG1SCG2SCPathSC135791113Blocksworld14864232527GPstepsSatplannerG1SCPathSC201510001357911131517192112310 19 28 37 46 55 64 73 82 91 100 109 118 127 136 145MiconicFreecell (IPC2)GPstepsSatplannerG1SCG2SCPathSC25201510#Plan periods30#Plan periods2152501816141210864201 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21GPstepsSatplannerG1SCG2SCPathSC12345678Depots1016GPstepsSatplannerG1SCPathSC14#Plan periods69 10 11 12 13 14 15 16 17 18DriverlogGPstepsSatplannerG1SCG2SCPathSC8#Plan periods1925#Plan periods#Plan periods101730GPstepsSatplannerG1SCG2SCPathSC1215Logistics421210864200123456789110 11 12 13 14 15 1623456789 10 11 12 13 14 15 16 17 18 19 20Rovers14GPstepsSatplannerG1SC#Plan periods121086420123456789 10 11 12 13 14 15 16 17 18 19#Plan periodsZenotravel181614121086420GPstepsSatplannerG1SCG2SCPathSC123456Freecell (IPC3)SatelliteFigure 18: Number plan periods required formulation solve planningproblems second third international planning competition.248fiLoosely Coupled Formulations Automated PlanningProblemBlocksworld5-15-26-06-18-2Logistics10-111-011-112-014-0Freecell2-12-22-32-45-5Miconic6-47-07-27-39-4G1SCG2SCPathSC000000000016625562000007001671110499110000000002300840*00000-00245ProblemDepots12101317Driverlog7891011Zenotravel56101112Rovers1415161718Satellite567911Freecell12345G1SCG2SCPathSC000000020072*30*132585616628053021083269110000045006048562145866259012111-1311924192268140--000**00110324801989**Table 1: Number ordering constraints, cuts, added dynamicallysolution process problems IPC2 (left) IPC3 (right). dash - indicatesIP formulation could tested domain star * indicatesformulation could solve problem instance within 30 minutes.249fiVan den Briel, Vossen & Kambhampati5.3.2 Results: Number Plan PeriodsFigure 18, see domains flexible network representation G1SCformulation slightly general 1-linearization semantics used Satplanner. is, number plan periods required G1SC formulation alwaysless equal number plan periods used Satplanner. Moreover, flexiblenetwork representation G2SC PathSC formulations generalone used G1SC formulation. One may think network representationPathSC formulation provide general interpretation action parallelism, since G2SC network representations allows values change backoriginal value plan period always case.domains Logistics, Freecell, Miconic, Driverlog, PathSC never requiredtwo plan periods solve problem instances. Miconic domaineasy understand. Miconic elevator needs bring travelersone floor another. state variables representation domain one statevariable elevator two traveler (one represent whether travelerboarded elevator one represent whether traveler serviced). Clearly,one devise plan value state variable visited twice.elevator simply could visit floors pickup travelers, visit floorsbring destination floor.5.4 Comparing Different State Variable Representationsinteresting question find whether different state variable representations leaddifferent performance benefits. loosely coupled formulations componentsrepresent multi-valued state variables. However, idea modeling value transitionsflows appropriately defined network applied binary multi-valued statevariable representation. section concentrate efficiency tradeoffsbinary multi-valued state descriptions. generally fewer multi-valued statevariables binary atoms needed describe planning problem, expectformulations compact use multi-valued state description.comparison concentrate G1SC formulation showed overall bestperformance among formulations. recent work (van den Briel, Kambhampati, &Vossen, 2007) analyze different state variable representations detail.Table 2 compares encoding size G1SC formulation set problemsusing either binary multi-valued state description. table clearly showsencoding size becomes significantly smaller (both CPLEX presolve)multi-valued state description used. encoding size presolve gives ideaimpact using compact multi-valued state description, whereas encodingsize presolve shows much preprocessing done removing redundanciessubstituting variables.Figure 19 shows total solution time (y-axis) needed solve problem instances(x-axis). Since make changes G1SC formulation, performancedifferences result using different state descriptions. several domains multivalued state description shows clear advantage binary state descriptionusing G1SC formulation, also domains multi-valued state250fiLoosely Coupled Formulations Automated Planningdescription provide much advantage. general, however, G1SCformulation using multi-valued state description leads better performanceusing binary state description. tests, encountered one probleminstance (Rovers pfile10) binary state description notably outperformedmulti-valued state description.6. Related Workinteger programming-based planning systems. Bylander (1997) considersIP formulation based converting propositional representation given Satplan(Kautz & Selman, 1992) IP formulation variables take value 1certain proposition true, 0 otherwise. LP relaxation formulationused heuristic partial order planning, tends rather time-consuming.different IP formulation given Vossen et al. (1999). consider IP formulationoriginal propositional variables replaced state change variables. Statechange variables take value 1 certain proposition added, deleted, persisted,0 otherwise. Vossen et al. show formulation based state change variablesoutperforms formulation based converting propositional representation. Van denBriel Kambhampati (2005) extend work Vossen et al. incorporatingimprovements described Dimopoulos (2001). integer programming approachesplanning rely domain-specific knowledge (Bockmayr & Dimopoulos, 1998, 1999)explore non-classical planning problems (Dimopoulos & Gerevini, 2002; Kautz & Walser,1999).formulations model transitions state variable separate flowproblem, individual problems connected action constraints.Graphplan planner introduced idea viewing planning network flow problem,decompose domain several loosely coupled components. encodingsdescribed related loosely-coupled modular planning architecture Srivastava, Kambhampati, (2001), well factored planning approaches AmirEngelhardt (2003), Brafman Domshlak (2006). work BrafmanDomshlak, example, proposes setting separate CSP problem handling factor. individual factor plans combined global CSP. way,similarities work (with individual state variable flows correspondingencodings factor plans). Although Brafman Domshlak provide empiricalevaluation factored planning framework, provide analysisfactored planning expected best. would interesting adapt minimaltree-width based analysis scenario.branch-and-cut concept introduced Grotschel, Runger, Reinelt (1984)Padberg Rinaldi (1991), successfully applied solutionmany hard large-scale optimization problems (Caprara & Fischetti, 1997). planning,approaches use dynamic constraint generation search include RealPlan (Srivastava et al., 2001) LPSAT (Wolfman & Weld, 1999).Relaxed definitions Graphplan-style parallelism investigated severalresearchers. Dimopoulos et al. (1997) first point necessaryrequire two actions independent order execute plan period.251fiVan den Briel, Vossen & KambhampatiProblemBlocksworld6-27-08-0Logistics14-115-015-1Miconic6-47-07-2Freecell(IPC2)3-33-43-5Depots71013Driverlog81011Zenotravel121314Rovers161718Satellite6711Freecell(IPC3)123Binarypresolvepresolve#va#co#va#coMultipresolve presolve#va#co#va#co7645101661174312561168811965757847384994795641231816773512568067855728197411130537164761643854096967947916464164651411516801168011440170527044462573867385493510843108449297111801118095832693269617713007300921332220284225273403447439771776229519992843376432871905247321993088410536494285034311495197217191286361293921286363998694014863998692792828234279477936979577794442526723734233426258861601610837123634662371558815101149312184530436360063618150785594251157213727147292323327570297121725024120279001538121713252974122464343725592673168063431432884573673464591012245215959072506233364042595355169972513329264711146140935581102117130739656137384033215589216497002142947779178157046124493295928584466928258218417241211051188233672398417410619863125794208958093239062024154241954912056529718384121447367228891835166372070017377439416652100814155152579988447154331675849455833215373584429413643377442671671340875013155784561541320357228829747108247829251011873322821439638191857634310599529651621819603733943427508191624487370293265113831600362426043394133964168055Table 2: Formulation size binary multi-valued state description problem instancesIPC2 IPC3 number variables (#va), number constraints(#co), number ordering constraints, cuts, (#cu) added dynamically solution process.252fiLoosely Coupled Formulations Automated Planning10000binary1000Solution time (sec.)Solution time (sec.)10000multi1001010.10.01binary1000multi1001010.10.0112345678910 11 12 13 14 15135791113Blocksworld10000binarySolution time (sec.)Solution time (sec.)100001000multi1001010.1171921232527binary1000multi1001010.10.010.011 2 3 4 516 7 8 9 10 11 12 13 14 15 16 17 18 19 203579 11 13 15 17 19 21 23 25 27 29 31 33MiconicFreecell (IPC2)100001000010001000Solution time (sec.)Solution time (sec.)15Logistics100101binary0.1binarymulti1001010.1multi0.010.01123456789 10 11 12 13 14 15 16 1712345Depots10000binarySolution time (sec.)Solution time (sec.)100001000multi1001010.10.017891011binary1000multi1001010.10.0112345678910 11 12 13 14 15 161234Zenotravel100006789 10 11 12 13 14 15 16 17 1810000binary10005RoversSolution time (sec.)Solution time (sec.)6Driverlogmulti1001010.11000binarymulti1001010.10.010.011234567891011123Freecell (IPC3)SatelliteFigure 19: Comparing binary state descriptions multi-valued state descriptions usingG1SC formulation.253fiVan den Briel, Vossen & Kambhampatiwork introduce property post-serializability set actions. setactions said post-serializable (1) union preconditionsconsistent, (2) union effects consistent, (3) preconditions-effectsgraph acyclic. preconditions-effects graph directed graph containsnode action , arc (a, b) a, b preconditionsinconsistent effects a. certain planning problems Dimopoulos et al. (1997)show modifications reduce number plan periods improve performance.Rintanen (1998) provides constraint-based implementation idea showsimprovements hold broad range planning domains.Cayrol et al. (2001) introduce notion authorized linearizations, impliesorder execution two actions. particular, action authorizes actionb implies executed b, preconditions b deletedeffects deleted b. notion authorized linearizationssimilar property post-serializability. adopt ideasnetwork-based representations would compare G1SC networkgeneralized state change arcs (see Figure 7) allows values prevail after,before, transition arcs.recent discussion definitions parallel plans given Rintanen, Heljanko Niemela (2006). work introduces idea -step semantics, saysnecessary parallel actions non-interfering longexecuted least one order. -step semantics provide general interpretationparallel plans notion authorized linearizations used LCGP (Cayrol et al.,2001). current implementation -step semantics Satplanner is, however, somewhat restricted. semantics allow actions conflicting effects, currentimplementation Satplanner not.7. Conclusionswork makes two contributions: (1) improves state art modeling planninginteger programming, (2) develops novel decomposition methods solving boundedlength (in terms number plan periods) planning problems.presented series IP formulations represent planning problem setloosely coupled network flow problems, network flow problem correspondsone state variables planning domain. incorporated different notionsaction parallelism order reduce number plan periods needed find planimprove planning efficiency. IP formulations described paper ledsuccessful use solving partial satisfaction planning problems (Do et al., 2007).Moreover, initiated new line work integer linear programmingused heuristic state-space search automated planning (Benton et al., 2007; van denBriel et al., 2007). would interesting see approach context IPformulations applies formulations based satisfiability constraint satisfaction.254fiLoosely Coupled Formulations Automated PlanningAcknowledgmentsresearch supported part NSF grant IIS308139, ONR grant N000140610058,Lockheed Martin subcontract TT0687680 Arizona State University partDARPA integrated learning program. thank Derek Long valuable input,especially thank anonymous reviewers whose attentive comments helpfulsuggestions greatly improved paper.ReferencesAmir, E., & Engelhardt, B. (2003). Factored planning. Proceedings 18th International Joint Conference Artificial Intelligence (IJCAI-2003), pp. 929935.Backstrom, C., & Nebel, B. (1995). Complexity results SAS+ planning. ComputationalIntelligence, 11 (4), 625655.Benton, J., van den Briel, M., & Kambhampati, S. (2007). hybrid linear programmingrelaxed plan heuristic partial satisfaction planning problems. ProceedingsInternational Conference Automated Planning Scheduling (ICAPS-2007),pp. 3441.Bixby, R. E. (2002). Solving real-world linear programs: decade progress.Operations Research, 50 (1), 315.Blum, A., & Furst, M. (1995). Fast planning planning graph analysis. Proceedings14th International Joint Conference Artificial Intelligence (IJCAI-1995), pp.16361642.Bockmayr, A., & Dimopoulos, Y. (1998). Mixed integer programming models planning problems. Working notes CP-98 Constraint Problem ReformulationWorkshop.Bockmayr, A., & Dimopoulos, Y. (1999). Integer programs valid inequalities planning problems. Proceedings European Conference Planning (ECP-1999),pp. 239251.Brafman, R., & Domshlak, C. (2006). Factored planning: How, when, not.Proceedings 21st National Conference Artificial Intelligence (AAAI-2006),pp. 809814.Bylander, T. (1997). linear programming heuristic optimal planning. Proceedings14th National Conference Artificial Intelligence (AAAI-1997), pp. 694699.Caprara, A., & Fischetti, M. (1997). Annotated Bibliographies Combinatorial Optimization, chap. Branch Cut Algorithms, pp. 4563. John Wiley Sons.Cayrol, M., Regnier, P., & Vidal, V. (2001). Least commitment graphplan. ArtificialIntelligence, 130 (1), 85118.255fiVan den Briel, Vossen & KambhampatiDesaulniers, G., Desrosiers, J., & Solomon, M. (Eds.). (2005). Column Generation. Springer.Dimopoulos, Y. (2001). Improved integer programming models heuristic search AIplanning. Proceedings European Conference Planning (ECP-2001), pp.301313.Dimopoulos, Y., & Gerevini, A. (2002). Temporal planning mixed integer programming. Proceedings Workshop Planning Temporal Domains (AIPS2002), pp. 28.Dimopoulos, Y., Nebel, B., & Koehler, J. (1997). Encoding planning problems nonmonotic logic programs. Proceedings 4th European Conference Planning(ECP-1997), pp. 167181.Do, M., Benton, J., van den Briel, M., & Kambhampati, S. (2007). Planning goalutility dependencies. Proceedings 20th International Joint ConferenceArtificial Intelligence (IJCAI-2007), pp. 18721878.Edelkamp, S., & Helmert, M. (1999). Exhibiting knowledge planning problems minimize state encoding length. Proceedings European Conference Planning(ECP-1999), pp. 135147.Garey, M., & Johnson, D. (1979). Computers Intractability: Guide TheoryNP-Completeness. Freeman Company, N.Y.Grotschel, M., Junger, M., & Reinelt, G. (1984). cutting plane algorithm linearordering problem. Operations Research, 32, 11951220.Helmert, M. (2006). Fast Downward planning system. Journal Artifical IntelligenceResearch, 26, 191246.Hooker, J. (1988). quantitative approach logical inference. Decision Support Systems,4, 4569.ILOG Inc., Mountain View, CA (2002). ILOG CPLEX 8.0 users manual.Johnson, E., Nemhauser, G., & Savelsbergh, M. (2000). Progress linear programmingbased branch-and-bound algorithms: exposition. INFORMS Journal Computing, 12, 223.Kambhampati, S., & commentary planning researchers (2006).suboptimality optimal planning track ipc 2006.http://raosruminations.blogspot.com/2006/07/on-suboptimality-of-optimal-planning.html.Karmarkar, N. (1984). new polynomial time algorithm linear programming. Combinatorica, 4 (4), 373395.Kautz, H. (2004). SATPLAN04: Planning satisfiability. Working Notes FourthInternational Planning Competition (IPC-2004), pp. 4445.256fiLoosely Coupled Formulations Automated PlanningKautz, H., & Selman, B. (1992). Planning satisfiability. Proceedings EuropeanConference Artificial Intelligence (ECAI-1992).Kautz, H., & Selman, B. (2006). SATPLAN04: Planning satisfiability. Working NotesFifth International Planning Competition (IPC-2006), pp. 4546.Kautz, H., & Walser, J. (1999). State-space planning integer optimization. Proceedings16th National Conference Artificial Intelligence (AAAI-1999), pp. 526533.Martin, K. (1991). Using separation algorithms generate mixed integer model reformulations. Operations Research Letters, 10, 119128.Padberg, M., & Rinaldi, G. (1991). branch-and-cut algorithm resolution largescale symmetric traveling salesman problems. SIAM Review, 33, 60100.Rintanen, J. (1998). planning algorithm based directional search. ProceedingsSixth International Conference Principles Knowledge RepresentationReasoning (KR-1998), pp. 617624.Rintanen, J., Heljanko, K., & Niemela, I. (2006). Planning satisfiability: parallel plansalgorithms plan search. Artificial Intelligence, 170 (12), 10311080.Srivastava, B., Kambhampati, S., & Do, M. (2001). Planning project management way:Efficient planning effective integration causal resource reasoning realplan.Artificial Intelligence, 131 (1-2), 73134.van den Briel, M., Benton, J., Kambhampati, S., & Vossen, T. (2007). LP-based heuristicoptimal planning. Proceedings International Conference PrinciplesPractice Constraint Programming (CP-2007), pp. 651665.van den Briel, M., & Kambhampati, S. (2005). Optiplan: Unifying IP-based graphbased planning. Journal Artificial Intelligence Research, 24, 623635.van den Briel, M., Kambhampati, S., & Vossen, T. (2007). Fluent merging: general technique improve reachability heuristics factored planning. ProceedingsWorkshop Heuristics Domain-Independent Planning: Progress, Ideas, Limitations,Challenges (ICAPS-2007).Vossen, T., Ball, M., Lotem, A., & Nau, D. (1999). use integer programmingmodels AI planning. Proceedings 18th International Joint ConferenceArtificial Intelligence (IJCAI-1999), pp. 304309.Wolfman, S., & Weld, D. (1999). LPSAT engine application resource planning. Proceedings 18th International Joint Conference Artificial Intelligence (IJCAI-1999), pp. 310317.Wolsey, L. (1998). Integer Programming. John Wiley Sons.257fiJournal Artificial Intelligence Research 31 (2008) 431-472Submitted 11/2007; published 3/2008First Order Decision Diagrams Relational MDPsChenggang WangSaket JoshiRoni Khardoncwan@cs.tufts.edusjoshi01@cs.tufts.eduroni@cs.tufts.eduDepartment Computer Science, Tufts University161 College Avenue, Medford, 02155, USAAbstractMarkov decision processes capture sequential decision making uncertainty,agent must choose actions optimize long term reward. paper studies efficient reasoning mechanisms Relational Markov Decision Processes (RMDP)world states internal relational structure naturally described termsobjects relations among them. Two contributions presented. First, paperdevelops First Order Decision Diagrams (FODD), new compact representation functions relational structures, together set operators combine FODDs,novel reduction techniques keep representation small. Second, paper showsFODDs used develop solutions RMDPs, reasoning performedabstract level resulting optimal policy independent domain size (numberobjects) instantiation. particular, variant value iteration algorithm developed using special operations FODDs, algorithm shown convergeoptimal policy.1. IntroductionMany real-world problems cast sequential decision making uncertainty.Consider simple example logistics domain agent delivers boxes. agenttake three types actions: load box truck, unload box truck,drive truck city. However effects actions may perfectly predictable.example gripper may slippery load actions may succeed, navigationmodule may reliable may end wrong location. uncertaintycompounds already complex problem planning course action achievegoals maximize rewards.Markov Decision Processes (MDP) become standard model sequential decision making uncertainty (Boutilier, Dean, & Hanks, 1999). models also providegeneral framework artificial intelligence (AI) planning, agent achievemaintain well-defined goal. MDPs model agent interacting world.agent fully observe state world takes actions change state.that, agent tries optimize measure long term reward obtainusing actions.classical representation algorithms MDPs (Puterman, 1994) require enumeration state space. complex situations specify state spaceterms set propositional variables called state attributes. state attributestogether determine world state. Consider simple logistics problemc2008AI Access Foundation. rights reserved.fiWang, Joshi, & Khardonone box one truck. state attributes truck Paris (TP), boxParis (BP), box Boston (BB), etc. let state space represented n binarystate attributes total number states would 2n . problems, however,domain dynamics resulting solutions simple structure describedcompactly using state attributes, previous work known propositionally factored approach developed suite algorithms take advantage structureavoid state enumeration. example, one use dynamic Bayesian networks, decision trees, algebraic decision diagrams concisely represent MDP model.line work showed substantial speedup propositionally factored domains (Boutilier,Dearden, & Goldszmidt, 1995; Boutilier, Dean, & Goldszmidt, 2000; Hoey, St-Aubin, Hu,& Boutilier, 1999).logistics example presented small. realistic problemlarge number objects corresponding relations among them. Consider problemfour trucks, three boxes, goal box Paris,matter box Paris. propositionally factored approach, needone propositional variable every possible instantiation relations domain,e.g., box 1 Paris, box 2 Paris, box 1 truck 1, box 2 truck 1, on,action space expands way. goal becomes ground disjunctiondifferent instances stating box 1 Paris, box 2 Paris, box 3 Paris, box 4Paris. Thus get large MDP time lose structure implicitrelations potential benefits structure terms computation.main motivation behind relational first order MDPs (RMDP). 1 firstorder representation MDPs describe domain objects relations among them,use quantification specifying objectives. logistics example, introduce three predicates capture relations among domain objects, i.e., Bin(Box, City),in(T ruck, City), On(Box, ruck) obvious meaning. three parameterized actions, i.e., load(Box, ruck), unload(Box, ruck), drive(T ruck, City).domain dynamics, reward, solutions described compactly abstractly usingrelational notation. example, define goal using existential quantification,i.e., b, Bin(b, P aris). Using goal one identify abstract policy, optimalevery possible instance domain. Intuitively 0 steps go,agent rewarded box Paris. one step gobox Paris yet, agent take one action help achieve goal.box (say b1 ) truck (say t1 ) truck Paris, agent executeaction unload(b1 , t1 ), may make Bin(b1 , P aris) true, thus goal achieved.two steps go, box truck Paris, agenttake unload action twice (to increase probability successful unloadingbox), box truck Paris, agent first take actiondrive followed unload. preferred plan depend success probabilitydifferent actions. goal paper develop efficient solutions problemsusing relational approach, performs general reasoning solving problemspropositionalize domain. result complexity algorithms1. Sanner Boutilier (2005) make distinction first order MDPs utilize full powerfirst order logic describe problem relational MDPs less expressive. followcalling language RMDP.432fiFirst Order Decision Diagrams Relational MDPschange number domain objects changes. Also solutions obtained gooddomain size (even infinite ones) simultaneously. abstractionpossible within propositional approach.Several approaches solving RMDPs developed last years. Muchwork devoted developing techniques approximate RMDP solutions usingdifferent representation languages algorithms (Guestrin, Koller, Gearhart, & Kanodia,2003a; Fern, Yoon, & Givan, 2003; Gretton & Thiebaux, 2004; Sanner & Boutilier, 2005,2006). example, Dzeroski, De Raedt, Driessens (2001) Driessens, Ramon,Gartner (2006) use reinforcement learning techniques relational representations. Fern,Yoon, Givan (2006) Gretton Thiebaux (2004) use inductive learning methodslearn value map policy solutions simulations small instances. SannerBoutilier (2005, 2006) develop approach approximate value iteration needpropositionalize domain. represent value functions linear combinationfirst order basis functions obtain weights lifting propositional approximatelinear programming techniques (Schuurmans & Patrascu, 2001; Guestrin, Koller, Par, &Venktaraman, 2003b) handle first order case.also work exact solutions symbolic dynamic programming(SDP) (Boutilier, Reiter, & Price, 2001), relational Bellman algorithm (ReBel) (Kersting, Otterlo, & De Raedt, 2004), first order value iteration (FOVIA) (Gromann,Holldobler, & Skvortsova, 2002; Hoolldobler, Karabaev, & Skvortsova, 2006).working implementation SDP hard keep state formulas consistentmanageable size context situation calculus. Compared SDP, ReBelFOVIA provide practical solutions. use restricted languages representRMDPs, reasoning formulas easier perform. paper developrepresentation combines strong points approaches.work inspired successful application Algebraic Decision Diagrams (ADD)(Bryant, 1986; McMillan, 1993; Bahar, Frohm, Gaona, Hachtel, Macii, Pardo, & Somenzi,1993) solving propositionally factored MDPs POMDPs (Hoey et al., 1999; St-Aubin,Hoey, & Boutilier, 2000; Hansen & Feng, 2000; Feng & Hansen, 2002). intuitionbehind idea ADD representation allows information sharing, e.g., sharingvalue states belong abstract state, algorithms considermany states together need resort state enumeration. sufficientregularity model, ADDs compact, allowing problems representedsolved efficiently. provide generalization approach lifting ADDshandle relational structure adapting MDP algorithms. main difficulty liftingpropositional solution, relational domains transition function specifiesset schemas conditional probabilities. propositional solution uses concreteconditional probability calculate regression function. possibleschemas. One way around problem first ground domain problem handperform reasoning (see example Sanghai, Domingos, & Weld, 2005).However allow solutions abstracting domains problems. LikeSDP, ReBel, FOVIA, constructions perform general reasoning.First order decision trees even decision diagrams already consideredliterature (Blockeel & De Raedt, 1998; Groote & Tveretina, 2003) several semanticsdiagrams possible. Blockeel De Raedt (1998) lift propositional decision433fiWang, Joshi, & Khardontrees handle relational structure context learning relational datasets.Groote Tveretina (2003) provide notation first order Binary Decision Diagrams(BDD) capture formulas Skolemized conjunctive normal form providetheorem proving algorithm based representation. paper investigatesapproaches identifies approach Groote Tveretina (2003) better suitedoperations value iteration algorithm. Therefore adapt extendapproach handle RMDPs. particular, First Order Decision Diagrams (FODD)defined modifying first order BDDs capture existential quantification well realvalued functions use aggregation different valuations diagram.allows us capture MDP value functions using algebraic diagrams natural way.also provide additional reduction transformations algebraic diagrams help keepsize small, allow use background knowledge reductions. developappropriate representations algorithms showing value iteration performedusing FODDs. core algorithm introduce novel diagram-based algorithmgoal regression where, given diagram representing current value function,node diagram replaced small diagram capturing truth valueaction. offers modular efficient form regression accounts potentialeffects action simultaneously. show version abstract value iterationcorrect hence converges optimal value function policy.summarize, contributions paper follows. paper identifiesmultiple path semantics (extending Groote & Tveretina, 2003) useful representationRMDPs contrasts single path semantics Blockeel De Raedt (1998).paper develops FODDs algorithms manipulate generalcontext RMDPs. paper also develops novel weak reduction operations first orderdecision diagrams shows relevance solving relational MDPs. Finally paperpresents version relational value iteration algorithm using FODDs showscorrect thus converges optimal value function policy. relationalvalue iteration developed specified previous work (Boutilier et al., 2001),knowledge first detailed proof correctness convergence algorithm.section briefly summarized research background, motivation, approach. rest paper organized follows. Section 2 provides backgroundMDPs RMDPs. Section 3 introduces syntax semantics First Order Decision Diagrams (FODD), Section 4 develops reduction operators FODDs. Sections5 6 present representation RMDPs using FODDs, relational value iterationalgorithm, proof correctness convergence. last two sections concludepaper discussion results future work.2. Relational Markov Decision Processesassume familiarity standard notions MDPs value iteration (see exampleBellman, 1957; Puterman, 1994). following introduce notions.also introduce relational MDPs discuss previous work solving them.Markov Decision Processes (MDPs) provide mathematical model sequential optimization problems stochastic actions. MDP characterized state spaceS, action space A, state transition function P r(sj |si , a) denoting probability434fiFirst Order Decision Diagrams Relational MDPstransition state sj given state si action a, immediate reward function r(s),specifying immediate utility state s. solution MDP optimalpolicy maximizes expected discounted total reward defined Bellman equation:V (s) = maxaA [r(s) +XP r(s0 |s, a)V (s0 )]s0V represents optimal state-value function. value iteration algorithm (VI)uses Bellman equation iteratively refine estimate value function:Vn+1 (s) = maxaA [r(s) +XP r(s0 |s, a)Vn (s0 )](1)s0Vn (s) represents current estimate value function Vn+1 (s) nextestimate. initialize process V0 reward function, Vn captures optimalvalue function n steps go. discussed algorithmknown converge optimal value function.Boutilier et al. (2001) used situation calculus formalize first order MDPsstructured form value iteration algorithm. One useful restrictions introducedwork stochastic actions specified randomized choice among deterministic alternatives. example, action unload logistics example succeedfail. Therefore two alternatives action: unloadS (unload success)unloadF (unload failure). formulation algorithms support number actionalternatives. randomness domain captured random choice specifyingaction alternative (unloadS unloadF ) gets executed agent attemptsaction (unload). choice determined state-dependent probability distributioncharacterizing dynamics world. way one separate regressioneffects action alternatives, deterministic, probabilistic choiceaction. considerably simplifies reasoning required since need performprobabilistic goal regression directly. work RMDPs used assumption, use assumption well. Sanner Boutilier (2007) investigate modelgoing beyond assumption.Thus relational MDPs specified set predicates domain, setprobabilistic actions domain, reward function. probabilistic action,specify deterministic action alternatives effects, probabilistic choiceamong alternatives. relational MDP captures family MDPs generatedchoosing instantiation state space. Thus logistics example correspondspossible instantiations 2 boxes 3 boxes on. get concreteMDP choosing instantiation.2 Yet algorithms attempt solve entireMDP family simultaneously.Boutilier et al. (2001) introduce case notation represent probabilities rewardscompactly. expression = case[1 , t1 ; ; n , tn ], logical formula,equivalent (1 (t = t1 )) (n (t = tn )). words, equals ti2. One could define single MDP including possible instances time, e.g. includestates 2 boxes, states 3 boxes infinite number boxes. obviouslysubsets states form separate MDPs disjoint. thus prefer view RMDPfamily MDPs.435fiWang, Joshi, & Khardontrue. general, constrained steps VI algorithm requiredisjoint partition state space. case, exactly onetrue state. denotes abstract state whose member statesvalue probability reward. example, reward function logisticsdomain, discussed illustrated right side Figure 1, capturedcase[b, Bin(b, P aris), 10; b, Bin(b, P aris), 0]. also following notationoperations function defined case expressions. operators definedtaking cross product partitions adding multiplying case values.case[i , ti : n] case[j , vj : j m] = case[i j , ti + vj : n, j m]case[i , ti : n] case[j , vj : j m] = case[i j , ti vj : n, j m].iteration VI algorithm, value stochastic action A(~x) parameterizedfree variables ~x determined following manner:QA(~x) (s) = rCase(s) [ j (pCase(nj (~x), s) Regr(nj (~x), vCase(do(nj (~x), s))))] (2)rCase(s) vCase(s) denote reward value functions case notation, n j (~x)denotes possible outcomes action A(~x), pCase(nj (~x), s) choice probabilities nj (~x). Note replace sum possible next states s0 standardvalue iteration (Equation 1) finite sum action alternatives j (reflected jEquation 2), since different next states arise different action alternatives.Regr, capturing goal regression, determines states one must actionorder reach particular state action. Figure 1 illustrates regressionb, Bin(b, P aris) reward function R action alternative unloadS(b , ).b, Bin(b, P aris) true action unloadS(b , ) true boxb truck truck Paris. Notice reward function R partitionsstate space two regions abstract states, may include infinitenumber complete world states (e.g., infinite number domain objects).Also notice get another set abstract states regression step.way first order regression ensures work abstract states never needpropositionalize domain.regression, get parameterized Q-function accounts possibleinstances action. need maximize action parameters Q-functionget maximum value could achieved using instance action.illustrate step, consider logistics example two boxes b 1 b2 ,b1 truck t1 , Paris (that is, On(b1 , t1 ) in(t1 , P aris)), b2Boston (Bin(b2 , Boston)). action schema unload(b , ), instantiate bb1 t1 respectively, help us achieve goal; instantiate bb2 t1 respectively, effect. Therefore need performmaximization action parameters get best instance action. Yet, mustperform maximization generically, without knowledge actual state. SDP,done several steps. First, add existential quantifiers action parameters (whichleads non disjoint partitions). sort abstract states Q A(~x) valuedecreasing order include negated conditions first n abstract statesformula (n + 1)th , ensuring mutual exclusion. Notice step leads complex436fiFirst Order Decision Diagrams Relational MDPsRb , Bin ( b , Paris )b , Bin ( b , Paris )10b , Bin ( b , Paris )0( b *, *)Tin ( *, Paris )Figure 1: example illustrating regression action alternative unloadS(b , ).description resulting state partitions SDP. process performed everyaction separately. call step object maximization denote obj-max(Q A(~x) ).Finally, get next value function maximize Q-functions differentactions. three steps provide one iteration VI algorithm repeatsupdate convergence.solutions ReBel (Kersting et al., 2004) FOVIA (Gromann et al., 2002;Hoolldobler et al., 2006) follow outline use simpler logical language representing RMDPs. abstract state ReBel captured using existentially quantifiedconjunction. FOVIA (Gromann et al., 2002; Hoolldobler et al., 2006) complexrepresentation allowing conjunction must hold state set conjunctionsmust violated. important feature ReBel use decision list (Rivest,1987) style representations value functions policies. decision list gives usimplicit maximization operator since rules higher list evaluated first. resultobject maximization step simple ReBel. state partition representedimplicitly negation rules it, explicitly conjunction rule.hand, regression ReBel requires one enumerate possible matchessubset conjunctive goal (or state partition) action effects, reasonseparately. step potentially improved.following section introduce new representation First Order Decision Diagrams (FODD). FODDs allow sharing parts partitions, leading space timesaving. importantly value iteration algorithm based FODDs simpleregression simple object maximization.437fiWang, Joshi, & Khardon3. First Order Decision Diagramsdecision diagram graphical representation functions propositional (Boolean)variables. function represented labeled rooted directed acyclic graphnon-leaf node labeled propositional variable exactly two children.outgoing edges marked values true false. Leaves labeled numericalvalues. Given assignment truth values propositional variables, traversegraph node follow outgoing edge corresponding truth value.gives mapping assignment leaf diagram turnvalue. leaves marked values {0, 1} interpret graphrepresenting Boolean function propositional variables. Equivalently, graphseen representing logical expression satisfied 1 leafreached. case {0, 1} leaves known Binary Decision Diagrams (BDDs)case numerical leaves (or general algebraic expressions) known AlgebraicDecision Diagrams (ADDs). Decision Diagrams particularly interesting imposeorder propositional variables require node labels respect orderevery path diagram; case known Ordered Decision Diagrams (ODD).case every function unique canonical representation serves normal formfunction. property means propositional theorem proving easy ODDrepresentations. example, formula contradictory fact evidentrepresent BDD, since normal form contradiction single leaf valued0. property together efficient manipulation algorithms ODD representationsled successful applications, e.g., VLSI design verification (Bryant, 1992;McMillan, 1993; Bahar et al., 1993) well MDPs (Hoey et al., 1999; St-Aubin et al.,2000). following generalize representation relational problems.3.1 Syntax First Order Decision Diagramsvarious ways generalize ADDs capture relational structure. One coulduse closed open formulas nodes, latter case must interpretquantification variables. process developing ideas paperconsidered several possibilities including explicit quantifiers leaduseful solutions. therefore focus following syntactic definitionexplicit quantifiers.representation, assume fixed set predicates constant symbols,enumerable set variables. also allow using equality pair terms(constants variables).Definition 1 First Order Decision Diagram1. First Order Decision Diagram (FODD) labeled rooted directed acyclic graph,non-leaf node exactly two children. outgoing edges markedvalues true false.2. non-leaf node labeled with: atom P (t1 , . . . , tn ) equality t1 = t2ti variable constant.3. Leaves labeled numerical values.438fiFirst Order Decision Diagrams Relational MDPsp (x)q (x)1h (y)0 10Figure 2: simple FODD.Figure 2 shows FODD binary leaves. Left going edges represent true branches.simplify diagrams paper draw multiple copies leaves 0 1 (andoccasionally values small sub-diagrams) represent nodeFODD.use following notation: node n, nt denotes true branch n, nffalse branch n; na outgoing edge n, true false.edge e, source(e) node edge e issues from, target(e) node edge epoints to. Let e1 e2 two edges, e1 = sibling(e2 ) iff source(e1 ) = source(e2 ).following slightly abuse notation let na mean either edgesub-FODD edge points to. also use na target(e1 ) interchangeablyn = source(e1 ) true false depending whether e1 liestrue false branch n.3.2 Semantics First Order Decision Diagramsuse FODD represent function assigns values states relational MDP.example, logistics domain, might want assign values different statesway box Paris, state assigned value 19;box Paris box truck Paris raining, stateassigned value 6.3, on.3 question define semantics FODDsorder intended meaning.semantics first order formulas given relative interpretations. interpretation domain elements, mapping constants domain elements and,predicate, relation domain elements specifies predicatetrue. MDP context, state captured interpretation. examplelogistics domain, state includes objects boxes, trucks, cities, relationsamong them, box 1 truck 1 (On(b1 , t1 )), box 2 Paris (Bin(b2 , P aris))on. one way define meaning FODD B interpretation I.following discuss two possibilities.3.2.1 Semantics Based Single Pathsemantics relational decision trees given Blockeel De Raedt (1998)adapted FODDs. semantics define unique path followed traversing3. result regression logistics domain cf. Figure 19(l).439fiWang, Joshi, & KhardonB relative I. variables existential node evaluated relative pathleading it.particular, reach node variables seenpath new. Consider node n label l(n) path leadingroot, let C conjunction labels nodes exited truebranch path. node n evaluate ~x, C l(n), ~x includesvariables C l(n). formula satisfied follow true branch.Otherwise follow false branch. process defines unique path rootleaf value.example, evaluate diagram Figure 2 interpretation 1domain {1, 2, 3} true atoms {p(1), q(2), h(3)} followtrue branch root since x, p(x) satisfied, follow false branch q(x)since x, p(x) q(x) satisfied. Since leaf labeled 0 say Bsatisfy I. attractive approach, partitions set interpretationsmutually exclusive sets used create abstract state partitions MDPcontext. However, reasons discuss later, semantics leads various complicationsvalue iteration algorithm, therefore used paper.3.2.2 Semantics Based Multiple Pathssecond alternative builds work Groote Tveretina (2003) defined semantics based multiple paths. Following work, define semantics first relativevariable valuation . Given FODD B variables ~x interpretation I, valuationmaps variable ~x domain element I. done, node predicateevaluates either true false traverse single path leaf. valueleaf denoted MAPB (I, ).Different valuations may give different values; recall use FODDs representfunction states, state must assigned single value. Therefore, nextdefineMAPB (I) = aggregate {MAPB (I, )}aggregation function. is, consider possible valuations ,valuation calculate MAPB (I, ). aggregate values. specialcase Groote Tveretina (2003) leaf labels {0, 1} variables universallyquantified; easily captured formulation using minimum aggregationfunction. paper use maximum aggregation function. correspondsexistential quantification binary case (if valuation leading value 1,value assigned 1) gives useful maximization value functionsgeneral case. therefore define:MAPB (I) = max{MAPB (I, )}.Using definition B assigns every unique value v = MAPB (I) B defines functioninterpretations real values. later refer function map B.Consider evaluating diagram Figure 2 interpretation I1 giventrue atoms {p(1), q(2), h(3)}. valuation x mapped 2440fiFirst Order Decision Diagrams Relational MDPsmapped 3 denoted {x/2, y/3} leads leaf value 1 maximum 1. leaflabels {0,1}, interpret diagram logical formula. MAP B (I) = 1,example, say satisfies B MAPB (I) = 0 say falsifiesB.define node formulas (NF) edge formulas (EF) recursively follows. noden labeled l(n) incoming edges e1 , . . . , ek , node formula NF(n) = (i EF(ei )).edge formula true outgoing edge n EF(nt ) = NF(n) l(n). edge formulafalse outgoing edge n EF(nf ) = NF(n) l(n). formulas,variables existentially quantified, capture conditions node edgereached.3.3 Basic Reduction FODDsGroote Tveretina (2003) define several operators reduce diagram normalform. total order node labels assumed. describe operators brieflygive main properties.(R1) Neglect operator: children node p FODD lead node qremove p link parents p q directly.(R2) Join operator: two nodes p, q label point twochildren join p q (remove q link qs parents p).(R3) Merge operator: node child label parent pointdirectly grandchild.(R4) Sort operator: node p parent q label ordering violated (l(p) >l(q)) reorder nodes locally using two copies p q labelsnodes violate ordering.Define FODD reduced none four operators applied.following:Theorem 1 (Groote & Tveretina, 2003)(1) Let {Neglect, Join, Merge, Sort} operator O(B) result applyingFODD B, B, I, , MAPB (I, ) = MAPO(B) (I, ).(2) B1 , B2 reduced satisfy , MAPB1 (I, ) = MAPB2 (I, ) identical.Property (1) gives soundness, property (2) shows reducing FODD gives normalform. However, holds maps identical every conditionstronger normal equivalence. normal form suffices Groote Tveretina(2003) use provide theorem prover first order logic, strongenough purposes. Figure 3 shows two pairs reduced FODDs (with respect R1R4) MAPB1 (I) = MAPB2 (I) , MAPB1 (I, ) 6= MAPB2 (I, ). casealthough maps FODDs reduced form. Considerfirst pair part (a) figure. interpretation p(a) false p(b)true substitution {x/a, y/b} leads value 0 B1 B2 always evaluates1. diagrams equivalent. interpretation, p(c) true object441fiWang, Joshi, & KhardonB1B2p (x)(a)11p (y)01p (x, y)(b)p (y, z)1p (x, y)0p (z, x)0100Figure 3: Examples illustrating weakness normal form.c MAPB1 (I) = 1 substitution {x/c}; p(c) false object cMAPB1 (I) = 1 substitution {x/c, y/c}. Thus map always 1B1 well. Section 4.2 show additional reduction operatorsdeveloped, B1 first pair reduced 1. Thus diagrams (a) formreduction. However, reductions resolve second pair given part (b)figure. Notice functions capture path two edges labeled p graph(we change order two nodes rename variables) diagrams evaluate1 interpretation path. Even though B1 B2 logicallyequivalent, cannot reduced form using R1-R4 new operators.identify unique minimal syntactic form one may consider possible renamingsvariables sorted diagrams produce, expensive operation.discussion normal form conjunctions uses operation given Garriga,Khardon, De Raedt (2007).3.4 Combining FODDsGiven two algebraic diagrams may need add corresponding functions, takemaximum use binary operation, op, values represented functions. adopt solution propositional case (Bryant, 1986) formprocedure Apply(B1 ,B2 ,op) B1 B2 algebraic diagrams. Let p qroots B1 B2 respectively. procedure chooses new root label (the loweramong labels p, q) recursively combines corresponding sub-diagrams, accordingrelation two labels (, =, ). order make sure resultreduced propositional sense one use dynamic programming avoid generatingnodes either neglect join operators ((R1) (R2) above) would applicable.Figure 4 illustrates process. example, assume predicate orderingp1 p2 , parameter ordering x1 x2 . Non-leaf nodes annotated numbersnumerical leaves underlined identification execution trace. example,442fiFirst Order Decision Diagrams Relational MDPs1p1 (x1)2p2 (x1)103p2 (x2)9001+3p1 (x1)=2+3p2 (x1)0+310+3p2 (x2)1910p2 (x2)90Figure 4: simple example adding two FODDs.top level call adds functions corresponding nodes 1 3. Since p1 (x1 )smaller label picked label root result. must addleft right child node 1 node 3. calls performed recursively. easysee size result may product sizes input diagrams. However,much pruning occur shared variables pruning made possible weakreductions presented later.Since interpretation fixed valuation FODD propositional,following lemma. later refer property correctness Apply.Lemma 1 Let C = Apply(A, B, op), , MAPA (I, ) op MAPB (I, ) =MAPC (I, ).Proof: First introduce terminology. Let #nodes(X) refer set nodesFODD X. Let root nodes B Aroot Broot respectively. LetFODDs rooted Aroott , Arootf , Broott , Brootf , Croott , Crootf Al , Ar , B l , B r ,C l C r respectively.proof induction n = |#nodes(A)| + |#nodes(B)|. lemma truen = 2, case Aroot Broot single leaves operationoperation two real numbers. inductive step needconsider two cases.Case 1: Aroot = Broot . Since root nodes equal, valuation reaches Al ,also reach B l reaches Ar , also reach B r . Also,definition Apply, case C l = Apply(Al , B l , op) C r = Apply(Ar , B r , op). Therefore statement lemma true MAPAl (I, ) op MAPB l (I, ) = MAPC l (I, )MAPAr (I, ) op MAPB r (I, ) = MAPC r (I, ) I. Now, since |#nodes(Al ) +#nodes(B l )| < n |#nodes(Ar ) + #nodes(B r )| < n, guaranteed inductionhypothesis.Case 2: Aroot 6= Broot . Without loss generality let us assume Aroot Broot .definition Apply, C l = Apply(Al , B, op) C r = Apply(Ar , B, op). Thereforestatement lemma true MAPAl (I, ) op MAPB (I, ) = MAPC l (I, )MAPAr (I, ) op MAPB (I, ) = MAPC r (I, ) I. guaranteedinduction hypothesis.2443fiWang, Joshi, & Khardon3.5 Order Labelssyntax FODDs allows two types objects: constants variables.argument predicate constant variable. assume complete orderingpredicates, constants, variables. ordering two labels givenfollowing rules.1. P (x1 , ..., xn ) P 0 (x01 , ..., x0m ) P P 02. P (x1 , ..., xn ) P (x01 , ..., x0n ) exists xj = x0j j < i,type(xi ) type(x0i ) (where type constant variable) type(xi ) = type(x0i )xi x0i .predicate order set arbitrarily appears useful assign equalitypredicate first predicate ordering equalities topdiagrams. reductions often encounter situations one side equalitycompletely removed leading substantial space savings. may also usefulorder argument types constant variables. ordering may helpfulreductions. Intuitively, variable appearing lower diagram boundvalue constant appears it. heuristic guidelines bestordering may well problem dependent. later introduce forms arguments:predicate parameters action parameters. ordering discussed Section 6.4. Additional Reduction Operatorscontext, especially algebraic FODDs, may want reduce diagrams further.distinguish strong reductions preserve MAPB (I, ) weak reductionspreserve MAPB (I). Theorem 1 shows R1-R4 given strong reductions. details relational VI algorithm directly depend reductionsused. Readers interested RMDP details skip Section 5 readindependently (except reductions illustrated examples).reduction operators incorporate existing knowledge relationshipspredicates domain. denote background knowledge B. exampleBlocks World may know block block clear:x, y, [on(x, y) clear(y)].following define conditions reduction operators, two typesconditions: reachability condition value condition. name reachabilityconditions starting P (for Path Condition) reduction operator number.name conditions values starting V reduction operator number.4.1 (R5) Strong Reduction Implied BranchesConsider node n whenever n reached true branch followed.case remove n connect parents directly true branch. firstpresent condition, followed lemma regarding operator.(P5) : B |= ~x, [NF(n) l(n)] ~x variables EF(nt ).444fiFirst Order Decision Diagrams Relational MDPsLet R5(n) denote operator removes node n connects parents directlytrue branch. Notice generalization R3. easy seefollowing lemma true:Lemma 2 Let B FODD, n node condition P5 holds, B 0 resultR5(n). interpretation valuation MAP B (I, ) =MAPB 0 (I, ).similar reduction formulated false branch, i.e., B |= ~x, [NF(n)l(n)] whenever node n reached false branch followed. caseremove n connect parents directly false branch.Implied branches may simply result equalities along path. example (x =y) p(x) p(y) may prune p(y) (x = y) p(x) known true. Impliedbranches may also result background knowledge. example Blocks Worldon(x, y) guaranteed true reach node labeled clear(y)remove clear(y) connect parent clear(y)f .4.2 (R7) Weak Reduction Removing Dominated EdgesConsider two edges e1 e2 FODD whose formulas satisfy followe2 using valuation also follow e1 using possibly different valuation.e1 gives better value e2 intuitively e2 never determines value diagramtherefore redundant. formalize reduction operator R7. 4Let p = source(e1 ), q = source(e2 ), e1 = pa , e2 = qb , b truefalse. first present conditions operator followdefinition operator.(P7.1) : B |= [~x, EF(e2 )] [~y , EF(e1 )] ~x variables EF(e2 ) ~yvariables EF(e1 ).(P7.2) : B |= ~u, [[w,~ EF(e2 )] [~v , EF(e1 )]] ~u variables appeartarget(e1 ) target(e2 ), ~v variables appear EF(e1 ) ~u,w~ variables appear EF(e2 ) ~u. condition requiresevery valuation 1 reaches e2 valuation 2 reaches e1 12 agree variables appear target(e1 ) target(e2 ).(P7.3) : B |= ~r, [[~s, EF(e2 )] [~t, EF(e1 )]] ~r variables appeartarget(e1 ) target(sibling(e2 )), ~t variables appear EF(e1 ) ~r,~s variables appear EF(e2 ) ~r. condition requiresevery valuation 1 reaches e2 valuation 2 reaches e1 12 agree variables appear target(e1 ) target(sibling(e2 )).(V7.1) : min(target(e1 )) max(target(e2 )) min(target(e1 )) minimum leafvalue target(e1 ), max(target(e2 )) maximum leaf value target(e2 ). caseregardless valuation know better follow e1 e2 .(V7.2) : min(target(e1 )) max(target(sibling(e2 ))).(V7.3) : leaves = target(e1 ) target(e2 ) non-negative values, denoted0. case fixed valuation better follow e1 instead e2 .4. use R7 skip notation R6 consistency earlier versions paper. Seediscussion Section 4.2.1.445fiWang, Joshi, & Khardon(V7.4) : leaves G = target(e1 ) target(sibling(e2 )) non-negative values.define operators R7-replace(b, e1 , e2 ) replacing target(e2 ) constant b0 min(target(e1 )) (we may write R7-replace(e1 , e2 ) b = 0),R7-drop(e1 , e2 ) dropping node q = source(e2 ) connecting parentstarget(sibling(e2 )).need one safety condition guarantee reduction correct:(S1) : NF(source(e1 )) sub-FODD target(e1 ) remainR7-replace R7-drop. condition says must harm value promisedtarget(e1 ). words, must guarantee p = source(e1 ) reachablesub-FODD target(e1 ) modified replacing branch 0.condition violated q sub-FODD pa , p sub-FODD qb .holds cases, p q unrelated (one descendantother), q sub-FODD pa , p sub-FODD qb , a, bnegations a, b.Lemma 3 Let B FODD, e1 e2 edges conditions P7.1, V7.1, S1hold, B 0 result R7-replace(b, e1 , e2 ), 0 b min(target(e1 )),interpretation MAPB (I) = MAPB 0 (I).Proof: Consider valuation 1 reaches target(e2 ). according P7.1,another valuation reaching target(e1 ) V7.1 gives higher value. Therefore, MAPB (I) never determined target(e2 ) replace target(e2 )constant 0 min(target(e1 )) without changing map.2Lemma 4 Let B FODD, e1 e2 edges conditions P7.2, V7.3, S1hold, B 0 result R7-replace(b, e1 , e2 ), 0 b min(target(e1 )),interpretation MAPB (I) = MAPB 0 (I).Proof: Consider valuation 1 reaches target(e2 ). P7.2 anothervaluation 2 reaching target(e1 ) 1 2 agree variables appeartarget(e1 ) target(e2 ). Therefore, V7.3 achieves higher value (otherwise,must branch = target(e1 ) target(e2 ) negative value). Therefore accordingmaximum aggregation value MAPB (I) never determined target(e2 ),replace constant described above.2Note conditions previous two lemmas comparable since P7.2P7.1 V7.1 V7.3. Intuitively relax conditions values, needstrengthen conditions reachability. subtraction operation = target(e 1 )target(e2 ) propositional, test V7.3 implicitly assumes common variables operands P7.1 check this. Figure 5 illustratesreachability condition P7.1 together V7.3, i.e., combining weaker portions conditions Lemma 3 Lemma 4, cannot guarantee replacebranch constant. Consider interpretation domain {1, 2, 3, 4} relations {h(1, 2), q(3, 4), p(2)}. addition assume domain knowledge B = [x, y, h(x, y)z, w, q(z, w)]. P7.1 V7.3 hold e1 = [q(x, y)]t e2 = [h(z, y)t ].MAPB1 (I) = 3 MAPB2 (I) = 0. therefore possible replace h(z, y)t 0.446fiFirst Order Decision Diagrams Relational MDPsq(x,y)p(y)q(x,y)h(z,y)0 p(y) 0533B10p(y)500B2Figure 5: example illustrating subtraction condition R7.10B1B2p(x)p(x)q(y)710p(y)20h(y)9 20h(y)00Figure 6: example illustrating condition removing node R7.Sometimes drop node q completely R7-drop. Intuitively,remove node, must guarantee gain extra value. conditionsR7-replace guarantee lose value. remove nodeq, valuation supposed reach e2 may reach better value e2 sibling.would change map, illustrated Figure 6. Notice conditions P7.1V7.1 hold e1 = [p(x)]t e2 = [p(y)]t replace [p(y)]t constant.Consider interpretation domain {1, 2} relations {q(1), p(2), h(2)}.MAPB1 (I) = 10 via valuation {x/2} MAPB2 (I) = 20 via valuation {x/1, y/2}. Thusremoving p(y) correct.Therefore need additional condition guarantee gain extra valuenode dropping. condition stated as: valuation 1 reaches e2thus redirected reach value v1 sibling(e2 ) q removed,valuation 2 reaches leaf value v2 v1 . However, condition complextest practice. following identify two stronger conditions.Lemma 5 Let B FODD, e1 e2 edges condition V7.2 hold additionconditions replacing target(e2 ) constant, B 0 result R7-drop(e1 , e2 ),interpretation MAPB (I) = MAPB 0 (I).Proof: Consider valuation reaching target(e2 ). true value dominatedanother valuation reaching target(e1 ). remove q = source(e2 ) valuationreach target(sibling(e2 )) V7.2 value produced smaller valuetarget(e1 ). map preserved.2447fiWang, Joshi, & KhardonLemma 6 Let B FODD, e1 e2 edges P7.3 V7.4 hold additionconditions replacing target(e2 ) constant, B 0 result R7-drop(e1 , e2 ),interpretation MAPB (I) = MAPB 0 (I).Proof: Consider valuation 1 reaching target(e2 ). value dominatedanother valuation reaching target(e1 ). remove q = source(e2 ) valuationreach target(sibling(e2 )) conditions P7.3 V7.4, valuation 2reach leaf greater value target(e1 )(otherwise branch G leadingnegative value). maximum aggregation map changed.2summarize P7.1 V7.1 S1 hold P7.2 V7.3 S1 holdreplace target(e2 ) constant. replace V7.2 P7.3 V7.4 holddrop q = source(e2 ) completely.following provide detailed analysis applicability variants R7.4.2.1 R6: Special Case R7special case R7 p = q, i.e., e1 e2 siblings. context R7considered focus single node n instead two edges. Assuming e 1 = nte2 = nf , rewrite conditions R7 follows.(P7.1) : B |= [~x, NF(n)] [~x, ~y , EF(nt )]. condition requires n reachablent reachable.(P7.2) : B |= ~r, [~v , NF(n)] [~v , w,~ EF(nt )] ~r variables appearnt nf , ~v variables appear NF(n) ~r, w~ variablesl(n) ~r ~v .(P7.3) : B |= ~u, [~v , NF(n)] [~v , w,~ EF(nt )] ~u variables appearnt (since sibling(e2 ) = e1 ), ~v variables appear NF(n) ~u, w~variables l(n) ~u ~v .(V7.1) : min(nt ) max(nf ).(V7.2) : nt constant.(V7.3) : leaves diagram = nt nf non-negative values.Conditions S1 V7.4 always true. previously analyzed special caseseparate reduction operator named R6 (Wang, Joshi, & Khardon, 2007).special case, may still useful check separately applying generalizedcase R7, provides large reductions seems occur frequently example domains.important special case R6 occurs l(n) equality t1 =variable occur FODD node n. case, condition P7.1holds since choose value y. also enforce equality subdiagram nt . Therefore V7.1 holds remove node n connecting parentsnt substituting t1 diagram nt . (Note may need make copiesnodes this.) Section 4.4 introduce elaborate reduction handleequalities taking maximum left right children.4.2.2 Application Ordercases several instances R7 applicable. turns orderapply important. following, first example shows order affects448fiFirst Order Decision Diagrams Relational MDPsp(x1,y1)q(x3)p(x1,y1)q(x3)10p(x2,y2)q(x2)60 5(a)p(x1,y1)100q(x2)6q(x2) 0q(x3)100(b)0q(x3)p(x2,y2)0 q(x2)5(d)(c)p(x1,y1)p(x1,y1)10000q(x3)10p(x2,y2)000(e)Figure 7: example illustrating effect application order R7.number steps needed reduce diagram. second example showsorder affects final result.Consider FODD Figure 7(a). R7 applicable edges e1 = [p(x1 , y1 )]te2 = [p(x2 , y2 )]t , e01 = [q(x3 )]t e02 = [q(x2 )]t . reduce topmanner, i.e., first apply R7 pair [p(x1 , y1 )]t [p(x2 , y2 )]t , get FODDFigure 7(b), apply R7 [q(x3 )]t [q(x2 )]t , getFODD Figure 7(c). However, apply R7 first [q(x3 )]t [q(x2 )]t thus gettingFigure 7(d), R7 cannot applied [p(x1 , y1 )]t [p(x2 , y2 )]t [p(x1 , y1 )]t[p(x2 , y2 )]t negative leaves. case, diagram still reduced.reduce comparing [q(x3 )]t [q(x2 )]t right part FODD. firstremove q(x2 ) get FODD shown Figure 7(e), use neglect operatorremove p(x2 , y2 ). see example applying one instance R7 may renderinstances applicable may introduce possibilities reductions generalmust apply reductions sequentially. Wang (2007) develops conditionsseveral instances R7 applied simultaneously.One might hope repeated application R7 lead unique reduced resulttrue. fact, final result depends choice operators orderapplication. Consider Figure 8(a). R7 applicable edges e1 = [p(x)]t e2 = [p(y)]t ,e01 = [q(x)]t e02 = [q(y)]t . reduce top manner, i.e., first applyR7 pair [p(x)]t [p(y)]t , get FODD Figure 8(b), cannotreduced using existing reduction operators (including operator R8 introduced below).However, apply R7 first [q(x)]t [q(y)]t get Figure 8(c).apply R7 e1 = [p(x)]t e2 = [p(y)]t get final result Figure 8(d),clearly compact Figure 8(b). interesting first example seems449fiWang, Joshi, & Khardonp(x)10p(x)10p(y)10 q(x)1010 q(y)1q(y)100(a)(b)p(x)10q(x)p(x)10 q(x)p(y)10 010 q(x)10 0(d)(c)Figure 8: example illustrating final result R7 reductions order dependent.suggest applying R7 top manner (since takes fewer steps), secondseems suggest opposite (since final result compact). researchneeded develop useful heuristics guide choice reductions applicationorder general develop complete set reductions.Note could also consider generalizing R7. Figure 8(b), reach [q(y)]clearly reach [p(x)]t [q(x)]t . Since [p(x)]t [q(x)]t give better values, safely replace [q(y)]t 0, thus obtaining final result Figure 8(d). theory generalize P7.1 B |= [~x, EF(e2 )] [y~1 , EF(e11 )] [y~n , EF(e1n )]~x variables EF(e2 ) y~i variables EF(e1i ) 1 n, generalizecorresponding value condition V7.1 [1, n], min(target(e1i )) max(target(e2 )).generalize reachability value conditions similarly. However resultingconditions expensive test practice.4.2.3 Relaxation Reachability Conditionsconditions P7.2 P7.3 sufficient, necessary guarantee correct reductions. Sometimes valuations need agree smaller set variablesintersection variables. see this, consider example shown Figure 9,B > 0 intersection {x, y, z}. However, guarantee B > 0 needagree either {x, y} {x, z}. Intuitively agree variable x avoidsituation two paths p(x, y) q(x) p(x, y) q(x) h(z) co-exist. orderprevent co-existence two paths p(x, y) h(z) p(x, y) q(x) h(z), eitherz well. change example little bit replace450fiFirst Order Decision Diagrams Relational MDPsh(z) h(z, v), two minimal sets variables different size, one {x, y},{x, z, v}. result cannot identify minimum set variablessubtraction must either choose intersection heuristically identify minimal set,example, using greedy procedure.Bp(x, y)p(x, y)q(x)3h(z)2 3q(x)21 2h(z)3h(z)11Figure 9: example illustrating minimal set variables subtractionunique.4.3 (R8) Weak Reduction UnificationConsider FODD B. Let ~v denote variables, let ~x ~y disjoint subsets ~v ,cardinality. define operator R8(B, ~x, ~y ) replacing variables~x corresponding variables ~y . denote resulting FODD B{~x/~y }result variables ~v \~x. following condition correctness R8:(V8) : leaves B{~x/~y } B non negative.Lemma 7 Let B FODD, B 0 result R8(B, ~x, ~y ) V8 holds,interpretation MAPB (I) = MAPB 0 (I).Proof: Consider valuation 1 ~v B. V8, B{~x/~y } gives better valuevaluation. Therefore lose value operator. also gainextra value. Consider valuation 2 variables B 0 reaching leaf node valuev, construct valuation 3 ~v B variables ~x taking correspondingvalue ~y , reach leaf node B value. Therefore mapchanged unification.2Figure 10 illustrates cases R8 applicable R7 not. applyR8 {x1 /x2 } get FODD shown Figure 10(b). Since (b) (a) 0, (b) becomesresult reduction. Note unify way, i.e.,{x2 /x1 }, getFigure 10(c), isomorphic Figure 10(b), cannot reduce original FODDresult, (c) (a) 6 0. phenomenon happens since subtraction operation(implemented Apply) used reductions propositional therefore sensitivevariable names.4.4 (R9) Equality ReductionConsider FODD B equality node n labeled = x. Sometimes drop nconnect parents sub-FODD result taking maximum left451fiWang, Joshi, & Khardonp(x2)p(x1)p(x2)0x1 / x2q(x2)10q(x2) 010(a)000(b)x2/ x1p(x1)q(x1)1000(c)Figure 10: example illustrating R8.right children n. reduction applicable B satisfy followingcondition.(E9.1) : equality node n labeled = x least one x variableappears neither nf node formula n. simplify descriptionreduction procedure below, assume x variable.Additionally make following assumption domain.(D9.1) : domain contains one object.assumption guarantees valuations reaching right child equalitynodes exist. fact needed proving correctness Equality reduction operator.First describe reduction procedure R9(n). Let Bn denote FODD rootednode n FODD B. extract copy Bnt (and name Bnt -copy), copyBnf (Bnf -copy) B. Bnt -copy, rename variable x produce diagramBn0 -copy. Let Bn0 = Apply(Bn0 -copy, Bnf -copy, max). Finally drop node n Bconnect parents root Bn0 obtain final result B 0 . example shownFigure 11.Informally, extracting parts FODD rooted node n, one x =(and renaming x part) one x 6= t. condition E9.1assumption D9.1 guarantee regardless value t, valuations reachingparts. Since definition MAP, maximize valuations, casemaximize diagram structure itself. calculating functionmaximum two functions corresponding two children n (usingApply) replacing old sub-diagram rooted node n new combined diagram.Theorem 9 proves affect map B.One concern implementation simply replace old sub-diagramnew sub-diagram, may result diagram strong reductions applicable.problem semantically, avoid need strong reductionsusing Apply implicitly performs strong reductions R1(neglect) R2(join) follows.452fiFirst Order Decision Diagrams Relational MDPsLet Ba denote FODD resulting replacing node n B 0, BbFODD resulting replacing node n 1 leaves node n 0,final result B 0 = Ba Bb0 Bb0 = Bb Bn0 . correctness Apply twoforms calculating B 0 give map.b=x0p(y)q(x)x=yp(y)5q(x)10q(x)q(x)10p(x)010(b)0(a)(c)b=xq(x)500(d)0p(x)q(x)10q(x)50(e)Figure 11: example equality reduction. (a) FODD reduction.node x = satisfies condition E9.1 variable y. (b) Bnt -copy (nt extracted).(c) Bnt -copy renamed produce Bn0 -copy. (d) Bnf -copy. (e) Final resultnode n replaced apply(Bn0 -copy, Bnf -copy, max)following prove node n equality condition E9.1 holds Bperform equality reduction R9 without changing map interpretationsatisfying D9.1. start properties FODDs defined above, e.g., B , Bb , Bb0 . Letn denote set valuations reaching node n let denote set valuationsreaching node n B. basic definition MAP following:Claim(a)(b)(c)(d)1 interpretation I,, MAPBa (I, ) = MAPB (I, ).n , MAPBa (I, ) = 0., MAPBb (I, ) = 0.n , MAPBb (I, ) = 1.Claim 1 definition MAP, have,Claim 2 interpretation I,(a) , MAPBb0 (I, ) = 0.(b) n , MAPBb0 (I, ) = MAPBn0 (I, ).Claim 1, Claim 2, definition MAP have,453fiWang, Joshi, & KhardonClaim 3 interpretation I,(a) , MAPB 0 (I, ) = MAPB (I, ).(b) n , MAPB 0 (I, ) = MAPBn0 (I, ).Next prove main property reduction stating valuations reachingnode n B, old sub-FODD rooted n new (combined) sub-FODD producemap.Lemma 8 Let n set valuations reaching node n FODD B. interpretation satisfying D9.1, maxn MAPBn (I, ) = maxn MAPBn0 (I, ).Proof: condition E9.1, variable x appear N F (n) hence valuen constrained. therefore partition valuations n disjointsets, n = { | valuation variables x}, variablesx fixed value x take value domain I. AssumptionD9.1 guarantees every contains least one valuation reaching Bnt least onevaluation reaching Bnf B. Note valuation reaches Bnt = x satisfiedthus MAPBnt (I, ) = MAPBn0 -copy (I, ). Since x appear Bnf alsoMAPBn0 -copy (I, ) constant . Therefore correctnessfApply max MAPBn (I, ) = max MAPBn0 (I, ).Finally, definition MAP, maxn MAPBn (I, ) = max max MAPBn (I, )= max max MAPBn0 (I, ) = maxn MAPBn (I, ).2Lemma 9 Let B FODD, n node condition E9.1 holds, B 0 resultR9(n), interpretation satisfying D9.1, MAP B (I) = MAPB 0 (I).Proof: Let X = maxm MAPB 0 (I, ) = maxn MAPB 0 (I, ). definition MAP, MAPB 0 (I) = max(X, ). However, Claim 3, X = maxm MAPB (I, )Claim 3 Lemma 8, = maxn MAPBn0 (I, ) = maxn MAPBn (I, ). Thus2max(X, ) = MAPB (I) = MAPB 0 (I).Lemma 9 guarantees correctness, applying practice may importantavoid violations sorting order (which would require expensive re-sortingdiagram). x variables sometimes replace new variablename resulting diagram sorted. However always possible.violation unavoidable, tradeoff performing reduction sortingdiagram ignoring potential reduction.summarize, section introduced several new reductions compress diagrams significantly. first (R5) generic strong reduction removes impliedbranches diagram. three (R7, R8, R9) weak reductions alteroverall map diagram alter map specific valuations. threereductions complementary since capture different opportunities space saving.5. Decision Diagrams MDPssection show FODDs used capture RMDP. therefore useFODDs represent domain dynamics deterministic action alternatives, probabilistic choice action alternatives, reward function, value functions.454fiFirst Order Decision Diagrams Relational MDPs5.1 Example Domainfirst give concrete formulation logistics problem discussed introduction. example follows exactly details given Boutilier et al. (2001), usedillustrate constructions MDPs. domain includes boxes, trucks cities,predicates Bin(Box, City), in(T ruck, City), On(Box, ruck). FollowingBoutilier et al. (2001), assume On(b, t) Bin(b, c) mutually exclusive,box truck city vice versa. is, background knowledge includesstatements b, c, t, On(b, t) Bin(b, c) b, c, t, Bin(b, c) On(b, t). rewardfunction, capturing planning goal, awards reward 10 formula b, Bin(b, P aris)true, box Paris. Thus reward allowed include constantsneed completely ground.domain includes 3 actions load, unload, drive. Actions effectpreconditions met. Actions also fail probability. attemptingload, successful version loadS executed probability 0.99, unsuccessful version loadF (effectively no-operation) probability 0.01. drive action executeddeterministically. attempting unload, probabilities depend whether raining not. raining successful version unloadS executed probability0.9, unloadF probability 0.1. raining unloadS executed probability0.7, unloadF probability 0.3.5.2 Domain Dynamicsfollow Boutilier et al. (2001) specify stochastic actions randomized choiceamong deterministic alternatives. domain dynamics defined truth value diagrams (TVDs). every action schema A(~a) predicate schema p(~x) TVD(A(~a), p(~x)) FODD {0, 1} leaves. TVD gives truth value p(~x)next state A(~a) performed current state. call ~a action parameters, ~x predicate parameters. variables allowed TVD;reasoning behind restriction explained Section 6.2. restriction sometimes sidestepped introducing action parameters instead variables.truth value TVD valid fix valuation parameters.TVD simultaneously captures truth values instances p(~x) next state.Notice TVDs different predicates separate. safely done evenaction coordinated effects (not conditionally independent) since action alternativesdeterministic.Since allow action parameters predicate parameters, effects actionrestricted predicates action arguments TVD expressivesimple STRIPS based schemas. example, TVDs easily express universal effectsaction. see note p(~x) true ~x action A(~a) TVD(A(~a), p(~x)) captured leaf valued 1. universal conditional effectscaptured similarly. hand, since explicit universal quantifiers,TVDs cannot capture universal preconditions.domain, TVD predicate p(~x) defined generically Figure 12.idea predicate true true undone actionfalse brought action. TVDs logistics domain455fiWang, Joshi, & Khardonp( x )bringundo001Figure 12: template TVDBin (B, C)1(B, T)(B, t*)B= b*T= t*0Tin (t*, C)10B= b*B= b*Bin (B, C)01C= c*10(b)Bin (B, c*)1Tin (T, c*)(c)C c*0(d)T= t*C= c*1(e)101Tin (T, C)T= t*B= b*T= t*Tin( t*, C)0(a)(B, T)0rain0.7Bin (b, Paris)0.910(f)0(g)Figure 13: FODDs logistics domain: TVDs, action choice, reward function. (a)(b) TVDs Bin(B, C) On(B, ) action choiceunloadS(b , ). (c)(d) TVDs Bin(B, C) On(B, ) actionchoice loadS(b , , c ). Note c must action parameter (d)valid TVD. (e) TVD in(T, C) action choice driveS(t , c ).(f) probability FODD action choice unloadS(b , ). (g) rewardfunction.456fiFirst Order Decision Diagrams Relational MDPsrunning example given Figure 13. TVDs omitted figuretrivial sense predicate affected action. order simplifypresentation give TVDs generic form sort diagrams usingorder proposed Section 3.5; TVDs consistent ordering Bin =rain. Notice TVDs capture implicit assumption usually takenplanning-based domains preconditions action satisfiedaction effect.Notice utilize multiple path semantics maximum aggregation. predicate true true according one paths specified get disjunctionconditions free. use single path semantics Blockeel De Raedt(1998) corresponding notion TVD significantly complicated since singlepath must capture possibilities predicate become true. capture that, musttest sequentially different conditions take union substitutionsdifferent tests turn requires additional annotation FODDs appropriatesemantics. Similarly operation would require union substitutions, thus complicating representation. explain issues detail Section 6.3introduce first order value iteration algorithm.5.3 Probabilistic Action ChoiceOne consider modeling arbitrary conditions described formulas statecontrol natures probabilistic choice action. multiple path semantics makeshard specify mutually exclusive conditions using existentially quantified variablesway specify distribution. therefore restrict conditions either propositionaldepend directly action parameters. condition interpretation followsexactly one path (since variables thus empty valuation) thusaggregation function interact probabilities assigned. diagram showingaction choice unloadS logistics example given Figure 13. example,condition propositional. condition also depend action parameters,example, assume result also affected whether box big not,diagram Figure 14 specifying action choice probability.Big(b*)rain0.90.7 0.9Figure 14: example showing choice probability depend action parameters.Note probability usually depends current state. depend arbitrary properties state (with restriction stated above), e.g., rain big(b ),shown Figure 14. allow arbitrary conditions depend predicates arguments restricted action parameters dependence complex. However,allow free variables probability choice diagram. example, cannotmodel probabilistic choice unloadS(b , ) depends boxes truck ,457fiWang, Joshi, & Khardone.g., b, On(b, ) b 6= b : 0.2; otherwise, 0.7. write FODD capturecondition, semantics FODD means path 0.7 selected max aggregation distribution cannot modeled way. clearly restriction,conditions based action arguments still give substantial modeling power.5.4 Reward Value FunctionsReward value functions represented directly using algebraic FODDs. rewardfunction logistics domain example given Figure 13.6. Value Iteration FODDsFollowing Boutilier et al. (2001) define first order value iteration algorithm follows:given reward function R action model input, set V0 = R, n = 0 repeatprocedure Rel-greedy termination:Procedure 1 Rel-greedy1. action type A(~x), compute:A(~x)QV n= R [ j (prob(Aj (~x)) Regr(Vn , Aj (~x)))](3)A(~x)2. QAVn = obj-max(QVn ).3. Vn+1 = maxA QAVn .notation steps procedure discussed Section 2 exceptwork FODDs instead case statements. Note since reward functiondepend actions, move object maximization step forward addingreward function. I.e., firstA(~x)TV n= j (prob(Aj (~x)) Regr(Vn , Aj (~x))),followedA(~x)QAVn = R obj-max(TVn ).Later see object maximization step makes reductions possible; therefore moving step forward get savings computation. computeupdated value function way comprehensive example value iteration givenlater Section 6.8.(Puterman, 1994). caseValue iteration terminates kVi+1 Vi k (1)2need test values achieved two diagrams within (1)2 .formulations goal based planning problems use absorbing state zeroadditional reward goal reached. handle formulationone non-zero leaf R. case, replace Equation 3A(~x)QV n= max(R, j (prob(Aj (~x)) Regr(Vn , Aj (~x))).see correct, note due discounting max value always R. Rsatisfied state care action (max would R) R 0state get value discounted future reward.458fiFirst Order Decision Diagrams Relational MDPsNote goal based domains, i.e., one non-zeroleaf. mean cannot disjunctive goals, means mustvalue goal condition equally.6.1 Regressing Deterministic Action Alternativesfirst describe calculation Regr(Vn , Aj (~x)) using simple idea call block replacement. proceed discuss obtain result efficiently.Consider Vn nodes FODD. node take copy corresponding TVD, predicate parameters renamed correspondnodes arguments action parameters unmodified. BR-regress(V n , A(~x)) FODDresulting replacing node Vn corresponding TVD, outgoing edgesconnected 0, 1 leaves TVD.Recall RMDP represents family concrete MDPs generated choosingconcrete instantiation state space (typically represented number objectstypes). formal properties algorithms hold concrete instantiation.Fix concrete instantiation state space. Let denote state resultingexecuting action A(~x) state s. Notice Vn BR-regress(Vn , A(~x)) exactlyvariables. following lemma:Lemma 10 Let valuation variables Vn (and thus also variablesBR-regress(Vn , A(~x))). MAPVn (s, ) = MAPBRregress(Vn ,A(~x)) (s, ).Proof: Consider paths P, P followed valuation two diagrams.definition TVDs, sub-paths P applied guarantee corresponding nodesP take truth values s. P, P reach leaf valueobtained.2naive implementation block replacement may efficient. use blockreplacement regression resulting FODD necessarily reduced moreover,since different blocks sorted start result even sorted. Reducingsorting results may expensive operation. Instead calculate resultfollows. FODD Vn traverse BR-regress(Vn , A(~x)) using postorder traversalterms blocks combine blocks. step combine 3 FODDsparent block yet processed (so TVD binary leaves)two children processed (so general FODDs). call parentBn , true branch child Bt false branch child Bf representcombination [Bn Bt ] [(1 Bn ) Bf ].Lemma 11 Let B FODD Bt Bf FODDs, Bn FODD {0, 1}leaves. Let B result using Apply calculate diagram [Bn Bt ][(1 Bn )Bf ].interpretation valuation MAPB (I, ) = MAPB (I, ).Proof: true since fixing valuation effectively ground FODDpaths mutually exclusive. words FODD becomes propositional clearlycombination using propositional Apply correct.2high-level description algorithm calculate BR-regress(V n , A(~x)) blockcombination follows:459fiWang, Joshi, & KhardonProcedure 2 Block Combination BR-regress(Vn , A(~x))1. Perform topological sort Vn nodes (see example Cormen, Leiserson, Rivest,& Stein, 2001).2. reverse order, non-leaf node n (its children Bt Bf alreadyprocessed), let Bn copy corresponding TVD, calculate [Bn Bt ] [(1Bn ) Bf ].3. Return FODD corresponding root.Notice different blocks share variables cannot perform weak reductionsprocess. However, perform strong reductions intermediate steps sincechange map valuation. process completed performcombination weak strong reductions since change mapregressed value function.Blue (b)(B, T)1Big(t)On(b,t)00B= b*Big(t)(b, t)T= t*1(a)Blue (b)0Bin (B, c)Tin (T, c)0b= b*t= t*01(b)Bin (b, c)Tin (t, c)10(c)Figure 15: example illustrating variables allowed TVDs.explain cannot variables TVDs example illustrated Figure 15. Suppose value function defined Figure 15(a), sayingblue block big truck block truckvalue 1 assigned. Figure 15(b) gives TVD On(B, ) action loadS,c variable instead action parameter. Figure 15(c) gives resultblock replacement. Consider interpretation domain {b1 , t1 , c1 , c2 } relations{Blue(b1 ), Big(t1 ), Bin(b1 , c1 ), in(t1 , c1 )}. action loadS(b1 , t1 ) reachstate = {Blue(b1 ), Big(t1 ), On(b1 , t1 ), in(t1 , c1 )}, gives us value 0. Figure 15(c) b = b1 , = t1 evaluated gives value 1 valuation {b/b1 , c/c2 , t/t1 }.choice c/c2 makes sure precondition violated. making c action parameter, applying action must explicitly choose valuation leads correctvalue function. Object maximization turns action parameters variables allows uschoose argument maximize value.460fiFirst Order Decision Diagrams Relational MDPs6.2 Regressing Probabilistic Actionsregress probabilistic action must regress deterministic alternatives combine choice probability Equation 3. discussed Section 2, duerestriction RMDP model explicitly specifies finite number deterministicaction alternatives, replace potentially infinite sum Equation 1 finitesum Equation 3. done correctly every state result Equation 3correct. following specify done FODDs.Recall prob(Aj (~x)) restricted include action parameters cannot include variables. therefore calculate prob(Aj (~x))Regr(Vn , Aj (~x)) step (1) directlyusing Apply. However, different regression results independent functionssum j (prob(Aj (~x)) Regr(Vn , Aj (~x))) must standardize apart different regression results adding functions (note action parameters still consideredconstants stage). holds addition reward function. needstandardize apart complicates diagrams often introduces structurereduced. performing operations first use propositional Apply procedurefollow weak strong reductions.V0ASucc(x*)q (x)p (x)10p (A)51A=x*0q (A)1(a)0(b)q (x2)q (x1)p (x1) 2.5x1= x*q (x2)+p (x2) 2.55q (x1)50q (x1)p (x1)0x1= x*q (x1)0(c)7.5Figure 16: example illustrating need standardize apart.Figure 16 illustrates need standardize apart different action outcomes. Actionsucceed (denoted ASucc) fail (denoted AF ail, effectively no-operation),chosen probability 0.5. Part (a) gives value function V 0 . Part (b) givesTVD P (A) action choice ASucc(x ). TVDs trivial. Part(c) shows part result adding two outcomes standardizing apart(to simplify presentation diagrams sorted). Consider interpretationdomain {1, 2} relations {q(1), p(2)}. seen (c), choosing x = 1, i.e.461fiWang, Joshi, & Khardonaction A(1), valuation x1 = 1, x2 = 2 gives value 7.5 action (withoutconsidering discount factor). Obviously standardize apart (i.e x 1 = x2 ),leaf value 7.5 get wrong value. Intuitively contributionASucc value comes bring portion diagram AF ailscontribution uses bindings undo portion two portions referdifferent objects. Standardizing apart allows us capture simultaneously.Lemma 10 11 discussion far have:Lemma 12 Consider concrete instantiation RMDP. Let Vn value functioncorresponding MDP, let A(~x) probabilistic action domain.A(~x)QVn calculated Equation 3 correct. is, state s, MAPQA(~x) (s)Vnexpected value executing A(~x) receiving terminal value V n .6.3 Observations Single Path SemanticsSection 5.2 suggested single path semantics Blockeel De Raedt (1998)support value iteration well multiple path semantics. explanationregression, use example illustrate this. Suppose value functiondefined Figure 17(a), saying red block big city value 1assigned. Figure 17(b) gives result block replacement action unloadS(b , ).However correct. Consider interpretation domain {b 1 , b2 , t1 , c1 }relations {Red(b2 ), Blue(b1 ), Big(c1 ), Bin(b1 , c1 ), in(t1 , c1 ), On(b2 , t1 )}. Note usesingle path semantics. follow true branch root since b, c, Bin(b, c) true{b/b1 , c/c1 }. follow false branch Red(b) since b, c, Bin(b, c) Red(b)satisfied. Therefore get value 0. Clearly, get value 1 instead{b/b2 , c/c1 }, impossible achieve value Figure 17(b) singlepath semantics. reason block replacement fails top node decides truebranch based one instance predicate really need true instancespredicate filter true leaf TVD.correct problem, want capture instances trueundone instances made true one path. Figure 17(c) gives onepossible way it. means variable renaming, stands union operator,takes union substitutions. treated edge operations. Notecoordinated operation, i.e., instead taking union substitutionsb0 b00 , c0 c00 separately need take union substitutions (b0 , c0 )(b00 , c00 ). approach may possible clearly leads complicated diagrams.Similar complications arise context object maximization. Finally userepresentation procedures need handle edge marking unionssubstitutions approach look promising.6.4 Object MaximizationNotice since handling different probabilistic alternatives actionseparately must keep action parameters fixed regression processadded step 1 algorithm. step 2 maximize choice actionparameters. mentioned get maximization free. simply rename462fiFirst Order Decision Diagrams Relational MDPsBin(b ,c )Bin(b , c )Bin(b, c)b =b*Red(b)Bin(b ,c )On(b , t*)b =b*Tin(t*,c )Big(c)01(a)Red(b )Big(c )1Red(b )On(b ,t*)010On(b ,t*)Tin(t*,c )Tin(t*,c ) (b,c)(b ,c )(b,c)(b ,c )(b ,c )Big(c )0b =b*(b,c)(b ,c )0Red(b)Big(c)(b)10(c)Figure 17: example illustrating union or.action parameters using new variable names (to avoid repetition iterations)consider variables. aggregation semantics provides maximizationdefinition selects best instance action. Since constants turnedvariables additional reduction typically possible stage. combination weakstrong reductions used. discussion following lemma:Lemma 13 Consider concrete instantiation RMDP. Let Vn value functioncorresponding MDP, let A(~x) probabilistic action domain.QAVn calculated object maximization step 2 algorithm correct. is,state s, MAPQA (s) maximum expected values achievable executingVninstance A(~x) receiving terminal value Vn .potential criticism object maximization essentially addingvariables diagram thus future evaluation diagram state becomesexpensive (since substitutions need considered). However, truediagram remains unchanged object maximization. fact, illustratedexample given below, variables may pruned diagram processreduction. Thus long final value function compact evaluation efficienthidden cost.6.5 Maximizing Actionsmaximization Vn+1 = maxA QAn+1 step (3) combines independent functions. Therefore must first standardize apart different diagrams, followpropositional Apply procedure finally follow weak strong reductions.clearly maintains correctness concrete instantiation state space.463fiWang, Joshi, & Khardon6.6 Order Argument Typesresume discussion ordering argument types extend predicateaction parameters. above, structure suggested operationsalgorithm. Section 3.5 already suggested order constants variables.Action parameters special constants object maximization becomevariables object maximization. Thus position allow behavevariables. therefore also order constants action parameters.Note predicate parameters exist inside TVDs, replaced domainconstants variables regression. Thus need decide relativeorder predicate parameters action parameters. put action parameterspredicate parameters latter replaced constant get orderviolation, order useful. hand, put predicate parametersaction parameters instantiations predicate parameters possible.Notice substituting predicate parameter variable, action parametersstill need larger variable (as TVD). Therefore, also orderaction parameters variables.summarize, ordering: constants variables (predicate parameters caseTVDs) action parameters, suggested heuristic considerations orders maximize potential reductions, avoid need re-sorting diagrams.Finally, note want maintain diagram sorted times, needmaintain variant versions TVD capturing possible ordering replacementspredicate parameters. Consider TVD Figure 18(a). rename predicate parametersX x2 x1 respectively, x1 x2 , resulting sub-FODDshown Figure 18(b) violates order. solve problem define anotherTVD corresponding case substitution X substitution ,shown Figure 18(c). case replacing X x2 x1 , use TVDFigure 18(c) instead one Figure 18(a).On(X, Y)On(x2, x1)On(X, Y)p(X)p(x2)p(Y)p(x1)p(Y)10(a)1p(X)0(b)10(c)Figure 18: example illustrating necessity maintain multiple TVDs.6.7 Convergence ComplexitySince step Procedure 1 correct following theorem:464fiFirst Order Decision Diagrams Relational MDPsTheorem 2 Consider concrete instantiation RMDP. Let Vn value functioncorresponding MDP n steps go. value Vn+1 calculatedProcedure 1 correctly captures value function n + 1 steps go.is, state s, MAPVn+1 (s) maximum expected value achievable n + 1steps.Note RMDPs problems require infinite number state partitions.Thus cannot converge V finite number steps. However, since algorithmimplements VI exactly, standard results approximating optimal value functionspolicies still hold. particular following standard result (Puterman, 1994) holdsalgorithm, stopping criterion guarantees approximating optimal value functionspolicies.Theorem 3 Let V optimal value function let Vk value function calculatedrelational VI algorithm.(1) r(s) kVn V k n(2) kVn+1 Vn k(1)22M)log( (1)log 1.kVn+1 V k .algorithm maintains compact diagrams, reduction diagrams guaranteed domains. Therefore provide trivial upper bounds termsworst case time complexity. Notice first every time use Apply proceduresize output diagram may large product size inputs.must also consider size FODD giving regressed value function. Blockreplacement O(N ) N size current value function, sortedsorting may require exponential time space worst case. example,Bryant (1986) illustrates ordering may affect size diagram. function2n arguments, function x1 x2 + x3 x4 + + x2n1 x2n requires diagram2n + 2 nodes, function x1 xn+1 + x2 xn+2 + + xn x2n requires 2n+1 nodes.Notice two functions differ permutation arguments.x1 x2 + x3 x4 + + x2n1 x2n result block replacement clearly sortingrequires exponential time space. true block combination proceduremethod calculating result, simply output exponentialsize. case heuristics change variable ordering, propositional ADDs(Bryant, 1992), would probably useful.Assuming TVDs, reward function, probabilities size C, actionaction alternatives, current value function Vn N nodes, worst casespace expansion regression Apply operations, overall size result2time complexity one iteration O(C (N +1) ). However noteworst case analysis take reductions account. methodguaranteed always work efficiently, alternative grounding MDPunmanageable number states deal with, despite high worst case complexitymethod provides potential improvement. next example illustrates, reductionssubstantially decrease diagram size therefore save considerable time computation.465fiWang, Joshi, & Khardon6.8 Comprehensive Example Value IterationFigure 19 traces steps application value iteration logistics domain.TVDs, action choice probabilities, reward function domain given Figure 13. simplify presentation, continue using predicate ordering Bin =rain introduced earlier.5Given V0 = R shown Figure 19(a), Figure 19(b) gives result regressionV0 unloadS(b , ) block replacement, denoted Regr(V0 , unloadS(b , )).Figure 19(c) gives result multiplying Regr(V0 , unloadS(b , )) choiceprobability unloadS P r(unloadS(b , )).Figure 19(d) gives result P r(unloadF (b , )) Regr(V0 , unloadF (b , )). Notice diagram simpler since unloadF change state TVDstrivial.Figure 19(e) gives unreduced result adding two outcomes unload(b , ), i.e.,result adding [P r(unloadS(b , ))Regr(V0 , unloadS(b , ))] [P r(unloadF (b , ))Regr(V0 , unloadF (b , ))]. Note first standardize apart diagrams unloadS(b , )unloadF (b , ) respectively renaming b b1 b2 . Action parameters bstage considered constants change them. Also noterecursive part Apply (addition ) performed reductions, i.e., removing noderain children lead value 10.Figure 19(e), apply R6 node Bin(b2 , P aris) left branch.conditionsP7.1: [b1 , Bin(b1 , P aris)] [b1 , b2 , Bin(b1 , P aris) Bin(b2 , P aris)],V7.1: min(Bin(b2 , P aris)t ) = 10 max(Bin(b2 , P aris)f ) = 9,V7.2: Bin(b2 , P aris)t constanthold. According Lemma 3 Lemma 5 drop node Bin(b2 , P aris) connectparent Bin(b1 , P aris) true branch. Figure 19(f ) gives result reduction.Next, consider true child Bin(b2 , P aris) true child root.conditionsP7.1: [b1 , b2 , Bin(b1 , P aris) Bin(b2 , P aris)] [b1 , Bin(b1 , P aris)],V7.1: min(Bin(b1 , P aris)t ) = 10 max(Bin(b2 , P aris)t ) = 10,V7.2: min(Bin(b1 , P aris)t ) = 10 max(Bin(b2 , P aris)f ) = 9hold. According Lemma 3 Lemma 5, drop node Bin(b2 , P aris)connect parent Bin(b1 , P aris) Bin(b2 , P aris)f . Figure 19(g) gives resultunload(b ,t )reduction get fully reduced diagram. TV0.next step perform object maximization maximize action parametersb get best instance action unload. Note bbecome variables, perform one reduction: drop equalityright branch R9. Figure 19(h) gives result object maximization, i.e.,unload(b ,t )obj-max(TV0). Note renamed action parameters avoidrepetition iterations.unload(b ,t )Figure 19(i) gives reduced result multiplying Figure 19(h), obj-max(TV0),= 0.9, adding reward function. result Qunload.15. details change substantially use order suggested Section 3.5 (where equalityfirst).466fiFirst Order Decision Diagrams Relational MDPsBin (b, Paris)V010Bin (b, Paris)10b= b*b= b*(b, t*)Tin (t*, Paris)Tin (t*, Paris)1007913(d)(c)Bin (b1, Paris)100rain0rain(b)Bin (b2, Paris)Bin (b, Paris)(b, t*)0(a)Bin (b, Paris)Bin (b1, Paris)10Bin (b2, Paris)Bin (b2, Paris)rainb1= b*9(b1, t*)7Tin (t*, Paris)10rainb1= b*b1= b*(b1, t*)(b1, t*)3 1 7(b1, t*)Tin (t*, Paris)Tin (t*, Paris)0rainb1= b*10Tin (t*, Paris)rain93 1 79(e)(f)Bin (b1, Paris)10Bin (b1, Paris)b1= b*(b1, t*)7Q1unload(b1, t1)1006.3 8.1(h)V16.3 8.1(l)0Bin (b, Paris)019Tin (t, Paris)(b, t*)1Tin (t*, Paris)rainQ(i)b= b*(b, t)Tin (t, Paris)(j)0(k)Bin (b, Paris)Bin (b, Paris)190drive1rain9(g)19Tin (t, Paris)97Bin (b, Paris)(b, t)19rain0Q1loadBin (b, Paris)Tin (t1, Paris)Tin (t*, Paris)rain0rainTin (t, Paris)t= t*00On(b, t)Tin (t, Paris)0rain6.3 8.10rain6.3 8.1b=b*19001=Tin (t, Paris)0rain6.3 8.1(n)(m)Figure 19: example value iteration Logistics Domain.467fiWang, Joshi, & Khardoncalculate QloadQ1drive way results shown Figure 19(j)1Figure 19(k) respectively. drive TVDs trivial calculationrelatively simple. load, potential loading box already Paris droppeddiagram reduction operators process object maximization.Figure 19(l) gives V1 , result maximizing Qunload, QloadQdrive.111standardized apart diagrams, maximized them, reducedresult. case diagram unload dominates actions. Therefore Q unload1becomes V1 , value function first iteration.start second iteration, i.e., computing V2 V1 . Figure 19(m) givesresult block replacement regression V 1 action alternative unloadS(b , ).Note sorted TVD on(B, ) obeys ordering chosen.However, diagram resulting block replacement sorted.address use block combination algorithm combine blocks bottomup. Figure 19(n) illustrates combine blocks in(t, P aris), TVD,two children, processed general FODDs. combinein(t, P aris) two children, On(b, t)t processed. Since On(b, t)f = 0,combine On(b, t) two children next step block combination.Continuing process get sorted representation Regr(V1 , unloadS(b , )).6.9 Extracting Optimal Policiesone way represent policies FODDs. simply notepolicy represented implicitly set regressed value functions. valueiteration terminates, perform one iteration compute set Q-functionsusing Equation 3.Then, given state s, compute maximizing action follows:1. Q-function QA(~x) , compute MAPQA(~x) (s), ~x considered variables.2. maximum map obtained, record action name action parameters (fromvaluation) obtain maximizing action.clearly implements policy represented value function. alternativeapproach represents policy explicitly developed context policyiteration algorithm (Wang & Khardon, 2007).7. DiscussionADDs used successfully solve propositional factored MDPs. work gives oneproposal lifting ideas RMDPs. general steps similar, technicaldetails significantly involved propositional case. decision diagramrepresentation combines strong points SDP ReBel approaches RMDP.one hand get simple regression algorithms directly manipulating diagrams.hand get object maximization free ReBel. also get space savingsince different state partitions share structure diagrams. possible disadvantagecompared ReBel reasoning required reduction operators might complex.468fiFirst Order Decision Diagrams Relational MDPsterms expressiveness, approach easily capture probabilistic STRIPS styleformulations ReBel, allowing flexibility since use FODDs capturerewards transitions. example, representation capture universal effectsactions. hand, limited SDP since cannot use arbitraryformulas rewards, transitions, probabilistic choice. example cannot expressuniversal quantification using maximum aggregation, cannot used rewardfunctions action preconditions. approach also capture grid-world RL domainsstate based reward (which propositional) factored form since rewarddescribed function location.contrasting single path semantics multiple path semantics seeinteresting tension choice representation task. multiple path methoddirectly support state partitions, makes awkward specify distributionspolicies (since values actions must specified leaves). However,semantics simplifies many steps easily supporting disjunction maximizationvaluations crucial value iteration likely lead significant savingspace time.implementation empirical evaluation progress. precise choicereduction operators application crucial obtain effective system, sincegeneral tradeoff run time needed reductions size resultingFODDs. apply complex reduction operators get maximally reduced FODDs,takes longer perform reasoning required. optimization still open issuetheoretically empirically. Additionally, implementation easily incorporateidea approximation combining leaves similar values control sizeFODDs (St-Aubin et al., 2000). gives simple way trading efficiencyaccuracy value functions.many open issues concerning current representation. resultsFODDs give first step toward complete generalization ADDs. Cruciallyyet semantically appropriate normal form important simplifying reasoning.one define normal form (cf., Garriga et al., 2007, treatment conjunctions)clear calculated incrementally using local operations ADDs.would interesting investigate conditions guarantee normal form useful setreduction operators FODDs.Another possible improvement representation modified allowcompression. example allow edges rename variables traversedcompress isomorphic sub-FODDs illustrated Figure 17(c). Anotherinteresting possibility copy operator evaluates several copies predicate (withdifferent variables) node illustrated Figure 20. constructsusable one must modify FODD MDP algorithmic steps handle diagramsnew syntactic notation.8. Conclusionpaper makes two main contributions. First, introduce FODDs, generalizationADDs, relational domains may useful various applications. developedcalculus FODDs reduction operators minimize size many open469fiWang, Joshi, & Khardonp (x) p (y)p (x)q (x)0q (x)p (y) 0f (y) 0f (y) 020211Figure 20: Example illustrating copy operator.issues regarding best choice operators reductions. second contributiondeveloping FODD-based value iteration algorithm RMDPs potentialsignificant improvement previous approaches. algorithm performs generalrelational probabilistic reasoning without ever grounding domains provedconverge abstract optimal value function solution exists.ReferencesBahar, R. I., Frohm, E. A., Gaona, C. M., Hachtel, G. D., Macii, E., Pardo, A., & Somenzi,F. (1993). Algebraic decision diagrams applications. ProceedingsInternational Conference Computer-Aided Design, pp. 188191.Bellman, R. E. (1957). Dynamic programming. Princeton University Press.Blockeel, H., & De Raedt, L. (1998). Top induction first order logical decision trees.Artificial Intelligence, 101, 285297.Boutilier, C., Dean, T., & Goldszmidt, M. (2000). Stochastic dynamic programmingfactored representations. Artificial Intelligence, 121(1), 49107.Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,11, 194.Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure policy construction. Proceedings International Joint Conference Artificial Intelligence,pp. 11041111.Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming first-orderMDPs. Proceedings International Joint Conference Artificial Intelligence,pp. 690700.Bryant, R. E. (1986). Graph-based algorithms boolean function manipulation. IEEETransactions Computers, C-35 (8), 677691.Bryant, R. E. (1992). Symbolic boolean manipulation ordered binary decision diagrams. ACM Computing Surveys, 24 (3), 293318.Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001). Introduction Algorithms. MIT Press.470fiFirst Order Decision Diagrams Relational MDPsDriessens, K., Ramon, J., & Gartner, T. (2006). Graph kernels gaussian processesrelational reinforcement learning. Machine Learning, 64 (1-3), 91119.Dzeroski, S., De Raedt, L., & Driessens, K. (2001). Relational reinforcement learning.Machine Learning, 43, 752.Feng, Z., & Hansen, E. A. (2002). Symbolic heuristic search factored Markov DecisionProcesses. Proceedings National Conference Artificial Intelligence, pp.455460.Fern, A., Yoon, S., & Givan, R. (2003). Approximate policy iteration policy languagebias. International Conference Neural Information Processing Systems.Fern, A., Yoon, S., & Givan, R. (2006). Approximate policy iteration policy languagebias: Solving relational Markov Decision Processes. Journal Artificial IntelligenceResearch, 25, 75118.Garriga, G., Khardon, R., & De Raedt, L. (2007). mining closed sets multi-relationaldata. Proceedings International Joint Conference Artificial Intelligence,pp. 804809.Gretton, C., & Thiebaux, S. (2004). Exploiting first-order regression inductive policyselection. Proceedings Conference Uncertainty Artificial Intelligence,pp. 217225.Groote, J. F., & Tveretina, O. (2003). Binary decision diagrams first-order predicatelogic. Journal Logic Algebraic Programming, 57, 122.Gromann, A., Holldobler, S., & Skvortsova, O. (2002). Symbolic dynamic programmingwithin fluent calculus. Proceedings IASTED International ConferenceArtificial Computational Intelligence.Guestrin, C., Koller, D., Gearhart, C., & Kanodia, N. (2003a). Generalizing plans newenvironments relational MDPs. Proceedings International Joint ConferenceArtificial Intelligence, pp. 10031010.Guestrin, C., Koller, D., Par, R., & Venktaraman, S. (2003b). Efficient solution algorithmsfactored MDPs. Journal Artificial Intelligence Research, 19, 399468.Hansen, E. A., & Feng, Z. (2000). Dynamic programming POMDPs using factoredstate representation. Proceedings International Conference ArtificialIntelligence Planning Systems, pp. 130139.Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planning using decision diagrams. Proceedings Conference Uncertainty ArtificialIntelligence, pp. 279288.Hoolldobler, S., Karabaev, E., & Skvortsova, O. (2006). FluCaP: heuristic search plannerfirst-order MDPs. Journal Artificial Intelligence Research, 27, 419439.Kersting, K., Otterlo, M. V., & De Raedt, L. (2004). Bellman goes relational. ProceedingsInternational Conference Machine Learning.McMillan, K. L. (1993). Symbolic model checking. Kluwer Academic Publishers.471fiWang, Joshi, & KhardonPuterman, M. L. (1994). Markov decision processes: Discrete stochastic dynamic programming. Wiley.Rivest, R. L. (1987). Learning decision lists. Machine Learning, 2 (3), 229246.Sanghai, S., Domingos, P., & Weld, D. (2005). Relational dynamic bayesian networks.Journal Artificial Intelligence Research, 24, 759797.Sanner, S., & Boutilier, C. (2005). Approximate linear programming first-order MDPs.Proceedings Conference Uncertainty Artificial Intelligence.Sanner, S., & Boutilier, C. (2006). Practical linear value-approximation techniques firstorder MDPs. Proceedings Conference Uncertainty Artificial Intelligence.Sanner, S., & Boutilier, C. (2007). Approximate solution techniques factored first-orderMDPs. Proceedings International Conference Automated PlanningScheduling.Schuurmans, D., & Patrascu, R. (2001). Direct value approximation factored MDPs.International Conference Neural Information Processing Systems, pp. 15791586.St-Aubin, R., Hoey, J., & Boutilier, C. (2000). APRICODD: Approximate policy construction using decision diagrams. International Conference Neural InformationProcessing Systems, pp. 10891095.Wang, C. (2007). First order Markov Decision Processes. Tech. rep. TR-2007-4, ComputerScience Department, Tufts University.Wang, C., Joshi, S., & Khardon, R. (2007). First order decision diagrams relationalMDPs. Proceedings International Joint Conference Artificial Intelligence,pp. 10951100.Wang, C., & Khardon, R. (2007). Policy iteration relational MDPs. ProceedingsConference Uncertainty Artificial Intelligence.472fiJournal Artificial Intelligence Research 31 (2008) 1-32Submitted 5/07; published 01/08INI AX AT: Efficient Weighted Max-SAT SolverFederico HerasJavier LarrosaAlbert OliverasFHERAS @ LSI . UPC . EDULARROSA @ LSI . UPC . EDUOLIVERAS @ LSI . UPC . EDUTechnical University Catalonia, LSI DepartmentJordi Girona 1-3, 08034, Barcelona, Spain.Abstractpaper introduce INI AX AT, new Max-SAT solver built top incorporates best current SAT Max-SAT techniques. handle hard clauses(clauses mandatory satisfaction SAT), soft clauses (clauses whose falsification penalized cost Max-SAT) well pseudo-boolean objective functions constraints.main features are: learning backjumping hard clauses; resolution-based substractionbased lower bounding; lazy propagation two-watched literal scheme. empiricalevaluation comparing wide set solving alternatives broad set optimization benchmarksindicates performance INI AX usually close best specialized alternativeand, cases, even better.+.1. IntroductionMax-SAT optimization version SAT goal satisfy maximum numberclauses. considered one fundamental combinatorial optimization problems many important problems naturally expressed Max-SAT. include academic problemsmax cut max clique, well real problems domains like routing, bioinformatics, schedulingelectronic markets.long tradition theoretical work structural complexity (Papadimitriou,1994) approximability (Karloff & Zwick, 1997) Max-SAT. work restrictedsimplest case clauses equally important (i.e., unweighted Max-SAT)fixed size (mainly binary ternary). practical point view, significant progressmade last 3 years (Shen & Zhang, 2004; Larrosa & Heras, 2005; Larrosa, Heras, & de Givry,2007; Xing & Zhang, 2005; Li, Manya, & Planes, 2005, 2006). result, handfulnew solvers deal, first time, instances involving hundreds variables.main motivation work comes study Max-SAT instances modelling realworld problems. usually encounter three features:satisfaction clauses importance, clause needsassociated weight represents cost violation. extreme case,often happens practice observed Cha, Iwama, Kambayashi, Miyazaki (1997),clauses whose satisfaction mandatory. usually modelled associatinghigh weight them.Literals appear randomly along clauses. contrary, easy identifypatterns, symmetries kinds structures.c2008AI Access Foundation. rights reserved.fiH ERAS , L ARROSA , & LIVERASproblems mandatory clauses reduce dramatically number feasibleassignments, optimization part problem plays secondary role. However,problems mandatory clauses trivially satisfiable real difficulty laysoptimization part.look current Max-SAT solvers, find none robust threefeatures. instance, Li et al.s (2005, 2006) solvers restricted formulas clausesequally important (i.e. unweighted Max-SAT), Shen Zhangs (2004) one restricted binary clauses, one described Larrosa et al. (2007) seems efficient overconstrainedproblems (i.e., small fraction clauses simultaneously satisfied), oneAlsinet, Manya, Planes (2005) seems efficient slightly overconstrained problems (i.e.almost clauses satisfied). solver described Argelich Manya (2007), developed parallel research described paper, handle mandatory clausesone incorporates learning, seems perform well structured problems. However,non-mandatory clauses must weight. Finally, approaches based translatingMax-SAT instance SAT instance solve SAT solver seem effectivehighly structured problems almost clauses mandatory (Fu & Malik, 2006; Le Berre,2006).paper introduce INI AX AT, new weighted Max-SAT solver incorporatescurrent best SAT Max-SAT techniques. build top INI + (Een & Sorensson,2006), borrows capability deal pseudo-boolean problems INI (Een& Sorensson, 2003) features processing mandatory clauses learning backjumping.extended allowing deal weighted clauses, preserving two-watched literallazy propagation method. main original contribution INI AX implementsnovel efficient lower bounding technique. Specifically, applies unit propagationorder detect disjoint subsets mutually inconsistent clauses done Li et al. (2006).simplifies problem following Larrosa Heras (2005), Heras Larrosa (2006), Larrosaet al. (2007) order increment lower bound. However, works clausesaccomplish specific patterns transformed, INI AX need definepatterns.structure paper follows: Section 2 provides preliminary definitions SATSection 3 presents state-of-the-art solving techniques incorporated modern SAT solverINI AT. Then, Section 4 presents preliminary definitions Max-SAT Section 5 overviewsINI AX AT. that, Sections 6 7 focus lower bounding additional features,respectively. Section 8 present benchmarks used empirical evaluationreport experimental results. Finally, Section 9 presents related work Section 10 concludespoints possible future work.2. Preliminaries SATsequel X = {x1 , x2 , . . . , xn } set boolean variables. literal either variable xinegation xi . variable literal l refers noted var(l). Given literal l, negation l xil xi xi l xi . clause C disjunction literals. size clause, noted |C|,number literals has. set variables appear C noted var(C). Sometimesassociate subscript Greek letter clause (e.g. (xi x j ) ) order facilitate future referencesclause.2fiM INI AX AT:E FFICIENT W EIGHTED AX -SAT OLVERAlgorithm 1: DPLL basic structure.Function Search() : boolean1InitQueue( ) ;2Loop3UP( ) ;4Conflict5AnalyzeConflict( ) ;6Top Conflict return f alse ;else7LearnClause( ) ;8Backjump( ) ;9101112else variables assigned return true ;elsel := SelectLiteral( ) ;Enqueue(Q, l) ;assignment set literals containing variable negation. Assignmentsmaximal size n called complete, otherwise called partial. Given assignment ,variable x unassigned neither x x belong . Similarly, literal l unassigned var(l)unassigned.assignment satisfies literal iff belongs assignment, satisfies clause iff satisfiesone literals falsifies clause iff contains negation literals.latter case say clause conflicting always happens empty clause, noted2. boolean formula F conjunctive normal form (CNF) set clauses representingconjunction. model F complete assignment satisfies clauses F .F model, call satisfiable, otherwise say unsatisfiable. Moreover,complete assignments satisfy F , say F tautology.Clauses size one called unit clauses simply units. formula contains unit l,simplified removing clauses containing l removing l clausesappears. application rule quiescence called unit propagation (UP) wellrecognized fundamental propagation technique current SAT solvers.Another well-known rule resolution, which, given formula containing two clausesform (x A), (x B) (called clashing clauses), allows one add new clause (A B) (calledresolvent).3. Overview State-of-the-art DPLL-based SAT Solverssection overview architecture SAT solvers based DPLL (Davis, Logemann,& Loveland, 1962) procedure. procedure, currently regarded efficient completesearch procedure SAT, performs systematic depth-first search space assignments.internal node associated partial assignment two successors obtained selectingunassigned variable x extending current assignment x x, respectively.visited node, new units derived due application unit propagation (UP). leads3fiH ERAS , L ARROSA , & LIVERASAlgorithm 2: Unit Propagation.Function UP(Q) : Conflict(Q contains non-propagated literals)13l := GetFirstNonPropagatedLit(Q); MarkAsPropagated(l) ;14foreach clause C l becomes unit falsified15C l becomes unit q Enqueue(Q, q) ;16else C l becomes falsified return Conflict ;return None ;conflicting clause, procedure backtracks, performing non-chronological backtrackingclause learning, originally proposed Silva Sakallah (1996).algorithmic description DPLL procedure appears Algorithm 1. algorithm usespropagation queue Q contains units pending propagation also contains representationcurrent assignment.First, propagation queue Q filled units contained original formula (line 1).main loop starts line 2 iteration procedure charge propagating pendingunits (line 3). conflicting clause found (line 4), conflict analyzed (line 5) resultnew clause learned (i.e, inferred recorded, line 7).Then, procedure backtracks, using propagation queue Q undo assignmentexactly one literals learned clause becomes unassigned (line 8). onebacktrack still maintaining condition, advantageous (this commonly referred backjumping non-chronological backtracking, see Silva & Sakallah, 1996).leads conflict, new unassigned literal selected extend current partial assignment.new literal added Q (line 10) new iteration takes place.procedure stops complete assignment found (line 9) top level conflictfound (line 6). first case, procedure returns true indicates modelfound, second case returns f alse means model exists inputformula.performance DPLL-based SAT solvers greatly improved 2001, SATsolver C HAFF (Moskewicz, Madigan, Zhao, Zhang, & Malik, 2001) incorporated two-watchedliteral scheme efficient unit propagation, First UIP scheme (Zhang, Madigan, Moskewicz,& Malik, 2001) clause learning cheap VSIDS branching heuristic. Currently, stateof-the-art SAT solvers, like INI (Een & Sorensson, 2003), implement small variationsthree features. following describe depth.3.1 Unit Propagationaim unit propagation twofold: one hand, finds clauses become unitsdue current assignment, hand, detects whether clause becomeconflicting. concrete procedure given Algorithm 2. non-propagated literals existQ, picks oldest one l marks propagated (line 13). clauses containing lmay become falsified units traversed (we later describe clausesdetected). one clauses becomes unit q, enqueued Q propagated later (line4fiM INI AX AT:E FFICIENT W EIGHTED AX -SAT OLVER15). procedure iterates units propagate conflicting clausefound (line 16).two types literals Q: decision literals algorithm heuristicallyselected assigned branching point (lines 11 12 Algorithm 1); consequence literalsadded logical consequences previous decision literals (line 15).INI uses non-standard queue handle units pending propagation. Unlike classical queues,fetching element, removed, marked such. Consequently, Q formedtwo sets elements: already propagated literals literals pending propagation.advantage strategy execution point, Q also contains current assignment.Besides, propagated literals Q divided decision levels. decision level containsdecision literal set related consequences. Furthermore, literal l associatedoriginal clause caused propagation noted l(); clause usually referredreason l. Note decision literal l reason representedld .Example 1 Consider formula {(x1 x2 ) , (x1 x3 ) , (x4 x5 ) }. starting execution,propagation queue empty Q = [k]. use symbol k separate propagated literals (onleft) literals pending propagation (on right). literal x1 selected, addedQ. propagation queue contains Q = [kxd1 ]. propagate x1 add two newconsequences x2 x3 . propagation queue Q = [xd1 kx2 (), x3 ()] currentassignment {x1 , x2 , x3 }. propagation x2 x3 add new literals Q, becomesQ = [xd1 , x2 (), x3 ()k]x4 decided, add new consequence x5 . propagation, Q =[x1 , x2 (), x3 (), xd4 , x5 ()k]. current assignment {x1 , x2 , x3 , x4 , x5 }. Note literalspropagated complete assignment found. Note well Q contains twodecision levels: first one formed literals x1 , x2 x3 second one formedliterals x4 x5 .3.1.1 L AZY DATA TRUCTURES .mentioned, aim detect units conflicting clauses. Taking accountprocess typically takes 80% total runtime SAT solver, importantdesign efficient data structures.first attempt use adjacency lists. literal, one keeps list clausesliteral appears. Then, upon addition literal l assignment, clausescontaining l traversed. main drawback refinements detect efficientlyclause become unit, keeping counters indicating number unassignedliterals clause, involved considerable amount work upon backtracking.method used INI two-watched literal scheme introduced Moskewicz et al.(2001). basic idea clause cannot unit conflicting (i) one satisfied literal(ii) two unassigned literals.algorithm keeps two special literals clause, called watched literals, initiallytwo unassigned literals, tries maintain invariant always one satisfied literal twounassigned literals watched.invariant may broken one two watched literals becomes falsified.case, clause traversed looking another non-false literal watch order restore5fiH ERAS , L ARROSA , & LIVERASinvariant. one literal cannot found, clause declared true, unit conflicting depending value watched literal. Hence, literal l added assignment,clauses may become falsified unit (line 14 Algorithm 2) clausesl watched.main advantage approach work clauses done uponbacktracking. However, main drawback way know many literalsunassigned given clause traversing literals. Note information usedtechniques Two-sided Jeroslow branching heuristic (See Section 3.3).3.1.2 R ESOLUTION R EFUTATION REES .detects conflict, unsatisfiable subset clauses F 0 determined using information provided Q. Since F 0 unsatisfiable, empty clause 2 derived F 0 viaresolution. resolution process called refutation. refutation unsatisfiable clause setF 0 resolution refutation tree (or simply refutation tree) every clause used exactlyresolution process.refutation tree built propagation queue Q follows: let C0 conflictingclause. Traverse Q LIFO (Last First Out) fashion clashing clause D0 found.resolution applied C0 D0 , obtaining resolvent C1 . Next, traversal Q continuesclause D1 clashes C1 found, giving resolvent C2 iterate processresolvent obtain empty clause 2. importance refutation trees becomerelevant Section 6.Example 2 Consider F = {(x1 ) , (x1 x4 ) , (x1 x2 ) , (x1 x3 x4 ) , (x1 x2 x3 ) , (x1 x5 ) }.apply unit propagation unit clause enqueued producing Q = [kx1 ()]. x1propagated Q becomes [x1 ()kx4 (), x2 (), x5 ()]. that, literal x4 propagated causingclause become unit Q becomes [x1 (), x4 ()kx2 (), x5 (), x3 ()]. that, literal x2propagated clause found conflicting. Figure 1.a shows state Qpropagation.build refutation tree. Starting tail Q first clause clashingconflicting clause . Resolution generates resolvent x1 x2 x4 . firstclause clashing x2 , producing resolvent x1 x4 . next clause clashing x4resolution generates x1 . Finally, resolve clause obtain 2.Figure 1.b showsresulting refutation tree.3.2 Learning BackjumpingLearning backjumping best illustrated example (see Silva & Sakallah, 1996; Zhanget al., 2001, precise description):Example 3 Consider formula {(x1 x2 ) , (x3 x4 ) , (x5 x6 ) , (x2 x5 x6 ) } partialassignment {x1 , x2 , x3 , x4 , x5 , x6 } leads conflict clause . Suppose currentpropagation queue Q = [xd1 , x2 (), xd3 , x4 (), xd5 , x6 ()k].example easy see decision xd1 incompatible decision xd5 . incompatibility represented clause (x1 x5 ). Similarly, consequence x2 incompatibledecision xd5 represented clause (x2 x5 ).6fiM INI AX AT:E FFICIENT W EIGHTED AX -SAT OLVERF = {(x1 ) , (x1 x4 ) , (x1 x2 ) , (x1 x3 x4 ) , (x1 x2 x3 ) , (x1 x5 ) }(x1 x2 x3 ) (x1 x3 x4 )x3 ()x1 x2 x4(x1 x2 )x5 ()x2 ()x1 x4(x1 x4 )x1(x1 )x4 ()x1 ()2a)b)Figure 1: Graphical representation propagation queue Q refutation tree example2. top, original formula F . left, propagation Q step 1. Arrowsindicate order resolving clauses selected. right, resolution treecomputed step 2.Clause learning implements different techniques used discover implicit incompatibilities adds formula. Learned clauses accelerate subsequent search,since increase potential future executions. However, observedunrestricted clause learning impractical cases (recorded clauses consume memoryrepeated recording may lead exhaustion). reason, current SAT solvers incorporatedifferent clause deletion policies order remove learned clauses.Learned clauses also used backjump presence would allowed unit propagation earlier decision level. case, say clause asserting backjumpingproceed going back level adding unit propagated literal. Among severalautomated ways generating asserting clauses, INI uses so-called First Unique Implication Point (1UIP) (Zhang et al., 2001).3.3 Branching HeuristicBranching occurs function SelectLiteral (Algorithm 1). literalspropagate, function chooses one variable unassigned ones assigns value.7fiH ERAS , L ARROSA , & LIVERASimportance branching heuristic well known, since different branching heuristic mayproduce different-sized search trees.Early branching heuristics include Bohms Heuristic (Buro & Buning, 1993), Maximum Ocurrences Minimum sized clauses (MOM) (Freeman, 1995) Two sided-JeroslowWang Heuristic (Jeroslow & Wang, 1990). heuristics try choose literalassignment generate largest number implications satisfy clauses.heuristics state dependent, is, use information state clauses givencurrent assignment. them, information number unassigned literalsclause. Hence, implemented jointly data structures based adjacency lists sincekeep information. instance, Two sided-Jeroslow Wang Heuristic computesliteral l F following function:J(l) =2|C|CFs.t. lCselects literal l maximizes function J(l).solvers become efficient, updating metrics state-dependent heuristics dominatesexecution time. Hence INI uses slight modification state-independent heuristic firstproposed Moskewicz et al. (2001). heuristic, called Variable State Independent DecayingSum (VSIDS), selects literal appears frequently clauses, giving priorityrecently learned clauses. advantage heuristic metrics updatedclauses learned. Since occurs occasionally, computation low overhead.VSIDS heuristic suits perfectly lazy data structures two-watched literal scheme.4. (Weighted) Max-SATweighted clause pair (C, w), C clause w integer representing costfalsification, also called weight. problem contains clauses must satisfied,call clauses mandatory hard associate special weight >. Non-mandatoryclauses also called soft. weighted formula conjunctive normal form (WCNF) setweighted clauses. model complete assignment satisfies mandatory clauses. costassignment sum weights clauses falsifies. Given WCNF formula F ,Weighted Max-SAT problem finding model F minimum cost. costcalled optimal cost F . Note formula contains mandatory clauses, weightedMax-SAT equivalent classical SAT. clauses weight 1, so-called(unweighted) Max-SAT problem. following, assume weighted Max-SAT.say weighted formula F 0 relaxation weighted formula F (noted F 0 v F )optimal cost F 0 less equal optimal cost F (non-models consideredcost infinity). say two weighted formulas F 0 F equivalent (noted F 0 F )F 0 v F F v F 0 .Max-SAT simplification rules transforms formula F equivalent, presumably simpler formula F 0 . SAT simplification rules (e.g. unit propagation, tautology removal,...)directly applied Max-SAT restricted mandatory clauses. However, several specific Max-SATsimplification rules exist (Larrosa et al., 2007). instance, formula contains clauses (C, u)(C, v), replaced (C, u + v). contains clause (C, 0), may removed.contains unit (l, >), simplified removing (including soft) clauses containing l8fiM INI AX AT:E FFICIENT W EIGHTED AX -SAT OLVERremoving l clauses (including soft clauses) appears. applicationrule quiescence natural extension unit propagation Max-SAT.empty clause may appear weighted formula. weight >, clearformula model. weight w < >, cost assignment includeweight, w obvious lower bound formula optimal cost. Weighted empty clausesinterpretation terms lower bounds become relevant Section 6.shown Larrosa et al. (2007), notion resolution extended weighted formulasfollows 1 ,(A B, m),(x A, u m),(x B, w m),{(x A, u), (x B, w)}(x B, m),(x B, m)B arbitrary disjunctions literals = min{u, w}.(x A, u) (x B, w) called prior clashing clauses, (A B, m) called resolvent,(x A, u m) (x B, w m) called posterior clashing clauses, (x B, m)(x B, m) called compensation clauses. effect Max-SAT resolution, classicalresolution, infer (namely, make explicit) connection B. However,important difference classical resolution Max-SAT resolution. former yieldsaddition new clause, Max-RES transformation rule. Namely, requires replacementleft-hand clauses right-hand clauses. reason cost prior clashingclauses must substracted order compensate new inferred information. Consequently,Max-RES better understood movement knowledge formula.resolution rule Max-SAT preserves equivalence (). last two compensation clausesmay lose clausal form, following rule (Larrosa et al., 2007) may needed recover it:l : |B| = 0CNF(A l B, u) ={(A l B, u)} CNF(A B, u) : |B| > 0Example 4 apply weighted resolution following clauses {(x1 x2 , 3), (x1 x2 x3 , 4)}obtain {(x2 x2 x3 , 3), (x1 x2 , 3 3), (x1 x2 x3 , 4 3), (x1 x2 (x2 x3 ), 3), (x1 x2 x2x3 , 3)}. first clause simplified. second clause omitted weightzero. fifth clause omitted tautology. fourth element clausesimple disjunction. Hence, apply CNF rule obtain two newclauses CNF(x1 x2 (x2 x3 ), 3) = {(x1 x2 x2 x3 , 3), (x1 x2 x3 , 3)}. Note first newclause tautology. Therefore, obtain equivalent formula {(x2 x3 , 3), (x1 x2 x3 , 1), (x1x2 x3 , 3)}.5. Overview INI AXINI AX weighted Max-SAT solver built top INI + (Een & Sorensson, 2006).DPLL-based SAT solver could used, INI + particularly wellsuited short open-source code. Besides, deal pseudo-boolean constraints.1. empty clause represents tautology. special weight >, relations > = >> > = > (Larrosa et al., 2007)9fiH ERAS , L ARROSA , & LIVERASAlgorithm 3: INI AX basic structure.Function Search() : integer17ub := LocalSearch(); lb := 0 ;18InitQueue(Q) ;19Loop20Propagate() ;21Hard ConflictAnalyzeConflict() ;Top Level Hard Conflict return ub ;elseLearnClause() ;Backjump() ;2223242526else Soft ConflictChronologicalBactrack() ;End Search return ub ;else variables assignedub := lb ;ub = 0 return ub ;ChronologicalBactrack() ;End Search return ub ;elsel := SelectLiteral() ;Enqueue(Q, l) ;Given WCNF formula (possibly containing hard soft clauses), INI AX returnscost optimal model (or > model). achieved means branch-andbound search, usually done solve optimization problems.Like INI AT, tree assignments traversed depth-first manner. search point,algorithm tries simplify current formula and, ideally, detect conflict, would meancurrent partial assignment cannot successfully extended. INI AX distinguishestwo types conflicts: hard soft. Hard conflicts indicate model extendingcurrent partial assignment (namely, mandatory clauses cannot simultaneously satisfied).Hard conflicts detected taking account hard clauses using methods INI AT.hard conflict occurs, INI AX learns hard clause backjumps INI woulddo. Soft conflicts indicate current partial assignment cannot extended optimalassignment. order identify soft conflicts, algorithm maintains two values search:cost best model found far, upper bound ub optimal solution.underestimation best cost achieved extending current partial assignment model, lower bound lb current subproblem.soft conflict detected lb ub, means current assignment cannot leadoptimal model. soft conflict detected, algorithm backtracks chronologically. Note10fiM INI AX AT:E FFICIENT W EIGHTED AX -SAT OLVERAlgorithm 4: MiniMaxSat propagation.Function MS-UP() : conflict(Q contains non-propagated literals)27l := GetFirstNonPropagatedLit(Q); MarkAsPropagated(l) ;;28lb := lb +V (l))29lb ub return Soft Conflict ;>) becomes unit falsified30foreach Hard clause (C l,31(C l, >) becomes unit (q, >) Enqueue(Q, q) ;>) becomes falsified return Hard Conflict ;32else (C l,u) becomes unit33foreach Soft clause (C l,34(C l, u) becomes unit (q, u) V (q) := V (q) + u ;3536373839return None ;Function Propagate() : conflictc := MS-UP( ) ;c = Hard Soft Conflict return c ;improveLB( ) ;lb ub return Soft Conflict ;return None ;one could also backjump computing clause expressing reasons led lb ub.However, presence lots soft clauses, approach ends creating many longclauses affect negatively efficience solver hence decided performsimple chronological backtracking.also want remark soft clause (C, w) w ub must satisfied optimalassignment. Therefore, following assume soft clauses automatically transformed hard clauses previous search. ones, soft clause promotedhard one search.algorithmic description INI AX presented Algorithm 3. startingsearch, good initial upper bound obtained local search method (line 17) may yieldidentification new hard clauses. current implementation use U BCSAT (Tompkins & Hoos, 2004) default parameters. selected local search algorithm IROTS (IteratedRobust Tabu Search) (Smyth, Hoos, & Stutzle, 2003). Besides, lower bound initializedzero. Next, queue Q initialized unit hard clauses resulting formula (line 18).main loop starts line 19 iteration charge propagating pending literals(line 20) and, conflict detected, attempting extension current partial assignment(line 26). Pending literals Q propagated function Propagate (line 20), may return hard soft conflict. hard conflict encountered (line 21) conflict analyzed,new hard clause learned backjumping performed. done introduced Section 3.soft conflict encountered (line 22) chronological backtracking performed. conflictfound (line 26), literal heuristically selected added Q propagation next iteration.However, current assignment complete (line 23), upper bound updated. Search stopszero-cost solution found, since cannot improved (line 24). Else, chronologicalbacktracking performed (line 25). Note backjumping leads termination top level hard11fiH ERAS , L ARROSA , & LIVERASconflict found, chronological backtracking leads termination two values firstassigned variable tried.Algorithm 4 describes propagation process (function Propagate). uses array V (l)accumulates weight soft clauses become unit l; namely, originalclauses (A l, w) current assignment falsifies A. clauses exists, assumeV (l) = 0. First all, performs Max-SAT-adapted form unit propagation (MS-UP, line 35).MS-UP iterates non-propagated literals l Q (line 27). Firstly, adding l assignmentmay make set soft clauses falsified. Since cost clauses kept V (l),add lower bound (line 28). lower bound increment identifies soft conflict,returned (line 29). Then, hard clause becomes unit, corresponding literal added Qfuture propagation (line 31). Finally, soft clause becomes unit clause (q, u) (line 33),weight u added V (q) (line 34). process hard conflict detected, functionreturns (lines 32,36). Else, algorithm attempts detect soft conflict call procedureimproveLB (line 37), returns soft conflict found (line 38). next sectiondetailed description improveLB found. Finally, conflict detected, functionreturns None (line 39).6. Lower Bounding INI AXfollowing, consider arbitrary search state INI AX callprocedure improveLB. purpose section, search state characterizedcurrent assignment. current assignment determines current subformulaoriginal formula conditioned current assignment: clause contains literal partcurrent assignment, removed. Besides, literals whose negation appear currentassignment removed clauses appear.value lb maintained INI AX precisely aggregation costsclauses become empty due current assignment. Similarly, recall valueV (l) aggregation costs clauses become unit l due currentassignment. Thus, current subformula contains (2, lb) (l,V (l)) every l.INI AX computes lower bound deriving new soft empty clauses (2, w)resolution process. clauses added already existing clause (2, lb) producingincrement lower bound.w) (l, u m), (l,wfirst step, improveLB replaces occurrence (l, u) (l,m), (2, m) (with = min{u, w}), amounts applying restricted version Max-SAT resolution known Unit Neighborhood Resolution (UNR) (Larrosa et al., 2007).produces immediate increment lower bound (i.e., weight empty clauseline 43) illustrated following example,Example 5 Consider current state {(2, 3), (x1 , 1), (x2 , 1), (x1 , 2), (x2 , 2), (x1 x2 , 3)}. UNRwould resolve clauses (x1 , 1) (x1 , 2) replacing (x1 , 1) (2, 1) (all compensation clauses removed weight zero tautologies). two emptyclauses grouped (2, 3 + 1 = 4). UNR would also resolve clauses (x2 , 1) (x2 , 2)replacing (x2 , 1) (2, 1). two empty clauses grouped (2, 4 + 1 = 5). So,new equivalent formula {(2, 5), (x1 , 1), (x2 , 1), (x1 x2 , 3)} higher lower bound 5.12fiM INI AX AT:E FFICIENT W EIGHTED AX -SAT OLVERAlgorithm 5: Lower Bounding INI AXFunction SUP() : conflict40InitQueue(Q) ;(Q contains non-propagated literals)l := GetFirstNonPropagatedLit(Q); MarkAsPropagated(l) ;41foreach (Hard Soft) Clause C l becomes unit falsifiedC l becomes unit q Enqueue(Q, q) ;else C l becomes falsified return conflict ;42434445464748return None ;Procedure improveLB() : lbw) Fforeach (l, v), (l,w m), (2, m) := min (v, w) ;replace (l, v m), (l,SU P() = con f lict:= BuildTree() ;:= minimum weight among clauses ;Condition ApplyResolution( , ) ;else lb := lb + m; remove weight clauses ;second step improveLB executes simulation unit propagation (SUP, line 44)soft clauses treated hard. First, SUP adds Q unit soft clauses (line40). Then, new literals Q propagated. new (hard soft) clauses become unit,inserted Q (line 41). SUP yields conflict, means subset (softhard) clauses cannot simultaneously satisfied. showed Section 3 Q usedidentify subset build refutation tree . ImproveLB computes tree (line 45).take account weights clauses apply Max-SAT resolution (Section 4)dictated , one see produce new clause (2, m), minimumweight among clauses tree (line 46). means extension current partialassignment unassigned variables cost least m.important remark step Max-SAT resolution process considerminimum weight two clauses, rather minimum clausesresolution tree. passed parameter line 47.result resolution process replacement clauses leaves(2, m) corresponding compensation clauses (function ApplyResolution line 47),thus obtaining equivalent formula lower bound increment m. call procedureresolution-based lower bounding.Example 6 Consider formula F = {(x1 , 2) , (x1 x4 , 1) , (x1 x2 , >) , (x1 x3 x4 , 2) , (x1x2 x3 , 3) , (x1 x5 , 1) }Step 1. Apply SUP. Initially, unit clause enqueued producing Q = [kx1 ()].x1 propagated Q becomes [x1 ()kx4 (), x2 (), x5 ()]. Literal x4 propagated clausebecomes unit, producing Q = [x1 (), x4 ()kx2 (), x5 (), x3 ()]. that, literal x2 propagatedclause found conflicting. Figure 2.a shows state Q propagation.13fiH ERAS , L ARROSA , & LIVERASF = {(x1 , 2) , (x1 x4 , 2) , (x1 x2 , >) , (x1 x3 x4 , 2) , (x1 x2 x3 , 3) , (x1 x5 , 1) }(x1 x2 x3 , 3) (x1 x3 x4 , 2)x3 ()x5 ()x2 ()(x1 x2 x3 , 1)(x1 x2 x3 x4 , 2) (x1 x2 x4 , 2) (x1 x2 , >)(x1 x2 x3 x4 , 2)(x1 x2 , >)(x1 x4 , 2)(x1 x4 , 2)x4 ()x1 ()(x1 , 2)2(x1 , 2)(2, 2)a)b)c)F 0 = {(x1 x2 , >), (x1 x5 , 1), (2, 2), (x1 x2 x3 , 1), (x1 x2 x3 x4 , 2), (x1 x2 x3 x4 , 2)}F00= {(x1 x2 , >), (x1 x2 x3 , 1), (x1 x5 , 1), (2, 2)}Figure 2: Graphical representation INI AX lower bounding. top, originalcurrent formula F . left, propagation Q step 1. middle, structurerefutation tree computed simulation step 2. right,effect actually executing Max-SAT resolution (step 3). resulting formula F 0appears bellow. substraction-based lower bounding performed, step 3 replacedsubstraction weights, producing formula F 00 .Step 2. Build simulated refutation tree. Starting tail Q first clause clashingconflicting clause . Resolution generates resolvent x1 x2 x4 .first clause clashing x2 , producing resolvent x1 x4 . next clause clashingx4 resolution generates x1 . Finally, resolve clause obtain 2.Figure 2.bshows resulting resolution tree.Step 3. Apply Max-SAT resolution. apply Max-SAT resolution indicated refutationtree computed Step 2. Figure 2.c graphically shows result process. Leaf clausesoriginal (weighted) clauses involved resolution. internal node indicates resolutionstep. resolvents appear junction edges. Beside resolvent, inside box,compensation clauses must added formula preserve equivalence. Sinceclauses used resolution must removed, resulting formula F 0 consists root14fiM INI AX AT:E FFICIENT W EIGHTED AX -SAT OLVERtree ((2, 2)),all compensation clauses clauses used refutation tree. is,resulting formula F 0 = {(x1 x2 , >), (x1 x5 , 1), (2, 2), (x1 x2 x3 , 1), (x1 x2 x3 x4 , 2), (x1x2 x3 x4 , 2)}. soundness Max-SAT resolution guarantees F F 0 .Remark 1 transformations applied resolution-based lower bounding passeddescendent nodes changes preserve equivalence. Nevertheless, transformationsrestored backtracking takes place.alternative problem transformation resolution identify lower bound increment substract clauses would participated resolutiontree. procedure similar lower bound computed Li et al. (2005) callsubstraction-based (line 48) lower bounding.Example 7 Consider formula F previous example. Steps 1 2 identical. However,substraction-based lower bounding would replace Step 3 Step 3 substracts weight 2clauses appear refutation tree adds (2, 2) formula. resultF 00 = {(x1 x2 , >), (x1 x2 x3 , 1), (x1 x5 , 1), (2, 2)}. Note F 00 v F .Remark 2 substractions applied substraction-based lower bounding restored moving descendent node preserve equivalence.increment lower bound either technique, procedure SUP executedagain, may yield new lower bound increments. process repeated SUPdetect conflict.comparing two previous approaches, observe resolution-based lower boundinglarger overhead, resolution steps need actually computed consequencesmust added current formula removed upon backtracking. However, effort investedtransformation may well amortized increment obtained lower boundbecomes part current formula, discovereddescendent nodes search. hand, substraction-based lower boundingsmaller overhead resolution needs actually computed. also facilitatescontext restoration upon backtracking.INI AX incorporates two alternatives chooses apply one heuristically (lines 47,48) depending specific condition (line 47). observed resolution-basedlower bounding seems effective resolution applied low arity clauses.consequence, identification resolution tree, INI AX applies resolution-basedlower bounding largest resolvent resolution tree arity strictly less 4. Otherwise, applies substraction-based lower bounding. See Section 8 details.7. Additional Features INI AXsection overview important features INI AX AT, namely use twowatched literal scheme, branching heuristic, use soft probing INI AXdeals pseudo-boolean functions.15fiH ERAS , L ARROSA , & LIVERAS7.1 Two-Watched LiteralsINI AX uses two-watched literal scheme also soft clauses. Recall one mainadvantages technique, applied pure SAT problems, backtracking takesplace, work done clauses. Unfortunately, case soft clauses restoration needs done. soft clause becomes unit literal l function MS-UP, weightadded V (l) clause eliminated (or marked eliminated) avoid reusinglower bounding procedure. changes, well addition lb, restoredbacktracking performed. However, note executions SUP (simulation unitpropagation) clauses considered hard. case two-watched literal scheme worksexactly SAT solver hard soft clauses. inconsistency detectedSUP stops literals propagate, initial state recovered.situation restoring initial state completely overhead free.7.2 Branching HeuristicINI AX incorporates two alternative branching heuristics. first one VSIDS heuristic (Moskewicz et al., 2001) disregarding soft clauses (that is, INI default). heuristiclikely good structured problems learning backjumping play significant role,well problems difficult find models (namely, satisfaction componentproblem difficult optimization component). Since heuristic disregards softclauses, likely ineffective problems easy find models difficultyfind optimal one prove optimality. extreme case, problems contain soft clauses (every complete assignment model) VSIDS heuristic blind thereforecompletely useless.overcome limitation VSIDS, INI AX also incorporates Weighted Jeroslowheuristic (Heras & Larrosa, 2006). extension SAT Jeroslow heuristic describedSection 3. Given weighted formula F, literal l F following function defined:J(l) =2|C| w(C,w)Fs.t. lCmandatory clauses assumed weight equal upper bound ub. heuristicselects literal highest value J(l). main disadvantage metrics needupdated visited node. combination two-watched literal updating becomesexpensive seem pay general. Thus, current implementationheuristic, J(l) values computed root node used throughout solvingprocess. found experiments heuristic good alternative problemsdifficulty lies optimization part (e.g. problems many models). INI AXautomatically changes VSIDS weighted Jeroslow problem contain literall hard clauses l hard clauses l.heuristics, literal l V (l) + lb ub node searchtree, l selected literal l never assigned.16fiM INI AX AT:E FFICIENT W EIGHTED AX -SAT OLVER7.3 Soft ProbingProbing well-known SAT technique allows formulation hypothetical scenarios (Lynce& Silva, 2003). idea temporarily assume l hard unit clause execute unitpropagation. yields conflict, know model extending current assignment mustcontain l. process iterated literals quiescence. Exhaustive experimentsSAT context indicate expensive probe search (Le Berre, 2001; Lynce& Silva, 2003), normally done pre-process order reduce initial numberbranching points.easily extend idea Max-SAT. context, besides discovery unit hardclauses, may used make explicit weighted unit clauses. call soft probing. SAT,idea temporarily assume l unit clause simulate unit propagation (i.e., executeSUP()). Then, build resolution tree propagation queue Q. clauseshard, know l must added assignment. Else, reproduce applying Max m) minimumSAT resolution weighted clauses derive unit clause (l,weight among clauses . unit soft clauses upfront makes future executionsimproveLB much effective subsequent search. Besides, derive (l, u)w), generate via unit neighborhood resolution (see Example 5) initial non-trivial lower(l,bound min{u, w}. tested soft probing search preprocessing severalbenchmarks. observed empirically soft probing preprocessing best optionSAT.Example 8 Consider formula F = {(x1 x2 , 1) , (x1 x3 , 1) , (x2 x3 , 1) }. assume x1adding Q execute SUP conflict reached. obtain Q = [xd1 , x2 (), x3 ()]detect conflicting clause. clauses involved refutation tree , , .Resolving clauses results {(x1 x2 , 1) , (x1 x2 , 1), (x1 x2 x3 , 1), (x1 x2 x3 , 1)}.resolution previous resolvent produces (equivalent) formula F 0 = {(x1 , 1), (x1x2 x3 , 1), (x1 x2 x3 , 1)}.7.4 Pseudo-boolean Functionspseudo-boolean optimization problem (PBO) (Barth, 1995; Sheini & Sakallah, 2006; Een &Sorensson, 2006) form:minimize nj=1 c j x jsubject nj=1 ai j l j bi , = 1 . . .x j {0, 1}, l j either x j 1 x j , c j , ai j bi non-negative integers.INI AX provided PBO instance, translates Max-SAT formula follows: pseudo boolean constraint translated set hard clauses using INI + (Een& Sorensson, 2006) (the algorithm heuristically decides appropriate translation choosingamong adders, sorters BDDs). objective function translated set soft unit clauses.summand c j x j becomes new soft unit clause (x j , c j ). translation INI AXexecuted usual.17fiH ERAS , L ARROSA , & LIVERAS8. Empirical Resultssection present benchmarks solvers used empirical evaluation. Then,report experiments performed order adjust parameters INI AX AT. Finally,comparison solvers presented.8.1 Benchmarks Encodingsgood set problems fundamental show effectiveness new solvers.following, present several problems explain encode Weighted Max-SAT.8.1.1 AX - K -SATk-SAT CNF formula CNF formula clauses size k. generated randomunsatisfiable 2-SAT 3-SAT formulas Cnfgen generator2 solved correspondingMAX-SAT problem. benchmarks, fixed number variables varied numberclauses, repeated.8.1.2 AX - CUTGiven graph G = (V, E), cut defined subset vertices U V . size cutnumber edges (vi , v j ) vi U v j V U . Max-cut problem consistsfinding cut maximum size. encoded Max-SAT associating one variable xigraph vertex. Value true (respectively, false) indicates vertex vi belongs U (respectively,V U ). edge (vi , v j ), two soft clauses (xi x j , 1), (xi x j , 1). Given completeassignment, number violated clauses |E| size cut associatedassignment. experiments considered Max-Cut instances extracted random graphs60 nodes varying number edges.8.1.3 AX - ONEGiven satisfiable CNF formula, max-one problem finding model maximumnumber variables set true. problem encoded Max-SAT consideringclauses original formula mandatory adding weighted unary clause (xi , 1)variable formula. Note solving problem much harder solving usual SATproblem, search cannot stop soon model found. optimal model mustfound optimality must proved. considered max-one problem two typesCNF formula: random 3-SAT instances 120 variables (generated Cnfgen), structuredsatisfiable instances coming 2002 SAT Competition3 .8.1.4 INIMUM V ERTEX C OVERINGAX -C LIQUEGiven graph G = (V, E), vertex covering set U V every edge (vi , v j ) eithervi U v j U . size vertex covering |U |. minimum vertex covering problemconsists finding covering minimal size. naturally formulated (weighted) MaxSAT. associate one variable xi graph vertex vi . Value true (respectively, false) indicates2. A. van Gelder ftp://dimacs.rutgers.edu/pub/challenge/satisfiability/contributed/UCSC/instances3. http://www.satcompetition.org/2002/18fiM INI AX AT:E FFICIENT W EIGHTED AX -SAT OLVERvertex vi belongs U (respectively, V U ). binary hard (xi x j , >) edge(vi , v j ). specifies one two vertices coveringedge connecting them. unary clause (xi , 1) variable xi , order specifypreferred add vertices U . simple way transform minimum vertexcoverings max-cliques vice-versa (Fahle, 2002).experiments, considered maximum clique instances extracted random graphs150 nodes varying number edges. also considered 66 Max-Clique instancesDIMACS challenge4 .8.1.5 C OMBINATORIAL AUCTIONScombinatorial auction defined set goods G set bidders bid indivisiblesubsets goods. bid defined subset requested goods Gi G amountmoney offered. bid-taker, wants maximize revenue, must decide bidsaccepted. Note two bids request good, cannot jointly accepted (Sandholm,1999). Max-SAT encoding, one variable xi associated bid. unitclauses (xi , ui ) indicating bid accepted loss profit ui . Besides,pair i, j conflicting bids, mandatory clause (xi x j , >).experiments, used CATS generator (K. Leyton-Brown & Shoham, 2000)allows generate random instances inspired real-world scenarios. particular, generatedinstances Regions, Paths Scheduling distributions. number goods fixed60 increased number bids. increasing number bids, instances becomeconstrained (namely, conflicting pairs bids) harder solve.8.1.6 ISCELLANEOUSalso considered following sets instances widely used literature:unsatisfiable instances 2nd DIMACS Implementation Challenge 5 consideredde Givry, Larrosa, Meseguer, Schiex (2003) Li et al. (2005): random 3-SAT instances(aim dubois), pigeon hole problem (hole) coloring problems (pret). Observeinstances modelled unweighted Max-SAT (i.e. clauses weight 1).Max-CSP random instances generated using protocol specified Larrosa Schiex(2003) de Givry, Heras, Larrosa, Zytnicki (2005). distinguish 4 different setsproblems: Dense Loose (DL), Dense Tight (DT), Sparse Loose (SL) Sparse Tight (ST).Tight instances 20 variables loose instances 40 variables.set contains 10 instances 3 values 10 instances 4 values per variable.Planning (Cooper, Cussat-Blanc, de Roquemaurel, & Regnier, 2006) graph coloring 6structured instances taken Weighted Constraint Satisfaction Problem (WCSP) repository 7 .4.5.6.7.ftp://dimacs.rutgers.edu/pub/challenge/graph/benchmarks/cliquehttp://mat.gsia.cmu.edu/challenge.htmlhttp://mat.gsia.cmu.edu/COLORING02/benchmarkshttp://mulcyber.toulouse.inra.fr/plugins/scmcvs/cvsweb.php/benchs/?cvsroot=toolbar19fiH ERAS , L ARROSA , & LIVERASProblems taken 2006 pseudo-boolean evaluation 8 : logic synthesis, misc (garden),routing, MPI (Minimum Prime Implicant), MPS (miplib). instances encodedMax-SAT specified previous section.Note Max-CSP, Planning graph coloring instances encoded Max-SAT usingdirect encoding (Walsh, 2000).8.2 Alternative Solverscompare INI AX several optimizers different communities. restrictedcomparison freely available solvers. considered following ones:AXSATZ (Li et al., 2006; Li, Manya, & Planes, 2007). Unweighted Max-SAT solver.best unweighted Max-SAT solver 2006 Max-SAT Evaluation.AX -DPLL (Heras & Larrosa, 2006; Larrosa et al., 2007). Weighted Max-SAT solver.part OOLBAR package. best solver weighted Max-SAT secondbest solver unweighted Max-SAT 2006 Max-SAT Evaluation.OOLBAR (Larrosa, 2002; Larrosa & Schiex, 2003; de Givry et al., 2003, 2005).state-of-the-art Weighted CSP solver.P UEBLO 1.5 (Sheini & Sakallah, 2006). pseudo-boolean solver. ranked firstseveral categories 2005 Pseudo Boolean Evaluation.INISAT + (Een & Sorensson, 2006). pseudo-boolean solver translates problems SAT solves MiniSat. ranked first several categories 2005Pseudo Boolean Evaluation.instances taken pseudo-boolean evaluation given original formatP UEBLO INISAT +. instances translated Max-SAT PBO partitioningset clauses three sets: H contains mandatory clauses (C, >), W contains nonunary weighted clauses (C, u < >) U contains unary weighted clauses (l, u).hard clause (C j , >) H pseudo boolean constraint C0j 1, C0j obtainedC j replacing + negated variables x 1 x. non-unary weighted clause(C j , u j ) W pseudo boolean constraint C0j + r j 1, C0j computed before,r j new variable that, set 1, trivially satisfies constraint. Finally, objectivefunction minimize is,u jr j +u jl j(l j ,u j )U(C j ,u j )W8.3 Experimental Resultsdivide experiments two parts. purpose first part evaluate impactdifferent techniques INI AX set different parameters. Sincetechniques effective benchmarks useless even counterproductive others (Brglez, Li, & Stallman, 2002), aimed finding configuration INI AX8. http://www.cril.univ-artois.fr/PB06/20fiM INI AX AT:E FFICIENT W EIGHTED AX -SAT OLVERperforms reasonably well instances. purpose second part compare AX alternative solvers. Since solvers specifically designedtype problems, expect INI AX outperform them. rather wantshow robustness INI AX showing usually close performancebest alternative type problems.Results presented plots tables. Regarding tables, first column contains nameset problems. second column shows number instances. remaining columnsreport performance different solvers. cell contains average cpu timesolver required solve instances. solver could solve instances set,number inside brackets indicates number solved instances average cpu timetakes account solved instances. cell contains dash, means instance couldsolved within time limit. Regarding plots, note legend goes accordanceperformance solvers. time limit set 900 seconds instance.solver, written C++, implemented top INISAT + (Een & Sorensson, 2006).Executions made 3.2 Ghz Xeon computer Linux. experiments randominstances, samples 30 instances plots report mean cpu time seconds.8.4 Setting Parameters INI AXfollowing evaluate order importance following techniques inside INI AX AT: lower bounding, soft probing, branching heuristics, learning backjumping.Starting basic version guides search Jeroslow branching heuristicrest techniques deactivated, analyze one one. analysis studies one techniqueincorporates previously analyzed ones corresponding tuned parameters.three first experiments consider little challenging instances generated randomlylower bounding plays fundamental role solve them. Finally, consider structured instanceslearning backjumping required solve them.8.4.1 L OWERBOUNDINGexperiment analyze impact resolution-based lower bounding versus substractionbased lower bounding, well combined strategies. considered following combinationtwo techniques: SUP detects inconsistency refutation tree computed,look resolvent maximum size. size less equal parameter K,resolution-based lower bounding applied, otherwise substraction-based lower boundingapplied. tested K = {0, 1, 2, 3, 4, 5, }. Note K = 0 corresponds pure substraction-basedlower bounding (and therefore similar approach Li et al., 2005), K = correspondspure resolution-based lower bounding.results presented Figure 3. seen, pure substraction-based lower bounding K = 0 always worst option. Better results obtained K increases. However,improvement stops (or nearly stops) K = 3. K > 3 significant improvement noticed. plot omits K = 4 K = 5 case clarity reasons. Since higher values K mayproduce new clauses higher size may cause overhead instances, set K = 3rest experiments.21fiH ERAS , L ARROSA , & LIVERAS(a) Max-2-SAT, 100 variables1510(b) Max-3-SAT, 60 variablesK=0K=1K=2K=3K=infcpu timecpu time2050200 300 400 500 600 700 8006050403020100K=0K=1K=2K=infK=3300number clauses400500600700800number clausescpu time(c) Max-CUT, 60 nodes14121086420200K=0K=1K=2K=3K=inf250300350400450number nodesFigure 3: Performance INI AX different mixed lower boundings (K = 0, 1, 2, 3, inf).8.4.2 OFTPROBINGsecond experiment, evaluate impact soft probing. preliminary experiments,observed soft probing time consuming, decided limit soft probingfollows. Initially, assign propagation level 0 variable probe. Then, new literalpropagate assigned propagation level L + 1 literal produces propagationlevel L. limited probing propagate literals maximum propagation level M.finally restricted 2 since gives best results. Note propagation leveldecision level.compare three alternatives: probing node search (S), probing pre-processsearch (P) probing (N). results, Figure 4, indicates probingsearch worst option Max-2-SAT Max-3-SAT produces improvementMax-CUT. Finally, probing preprocessing gives slightly improvement Max-2-SATbest results Max-CUT. Note soft probing preprocessing Max-3-SAT effectomitted plot (its results similar N). Given results, decided includesoft probing preprocessing.8.4.3 J EROSLOWBRANCHING HEURISTICfollowing experiment, evaluate importance weighted Jeroslow heuristic. Figure5 shows time difference INI AX Jeroslow heuristic previoustwo experiments (Jeroslow) without heuristic (None). results indicates guiding searchJeroslow heuristic gives important speed ups. Hence, maintain Jeroslow heuristicINI AX AT.22fiM INI AX AT:E FFICIENT W EIGHTED AX -SAT OLVER(a) Max-2-SAT, 100 variablescpu time2015(b) Max-3-SAT, 60 variablesNPcpu time251050200 300 400 500 600 700 800706050403020100N300number clauses400500600700800number clauses(c) Max-CUT, 60 nodes5cpu time4NP3210300350400450number nodes500Figure 4: Performance INI AX without soft probing, probing preprocessing (P)probing search (S).8.4.4 L EARNING ,BACKJUMPINGVSIDSfinal experiment, evaluate importance learning backjumping. experiments use structured instances, since well known learning backjumpinguseful type problems. Besides, also evaluate importance VSIDS heuristiccombination learning backjumping. Recall heuristic specially designedwork cooperation learning, meaningless analyze effect itself.Table 6 reports results experiment. third column reports results without learningbackjumping lower bounding, probing Jeroslow heuristic (None).fourth column reports results adding learning backjumping previous version (Learning).fifth column reports results adding learning, backjumping changing Jeroslow heuristicVSIDS heuristic (VSIDS). results show INI AX without learning backjumping (None) clearly worst option. Significant improvements obtained learningbackjumping (Learning) added. Finally, adding VSIDS heuristic (VSIDS) improve results specially routing instances. Based results, incorporated learningbackjumping INI AX AT.Regarding branching heuristic, problems literals appear hard clausespolarities applies VSIDS heuristic, otherwise Jeroslow heuristic computedroot search tree stated Section 7. choice done startingsearch.23fiH ERAS , L ARROSA , & LIVERAS(a) Max-2-SAT, 100 variables15(b) Max-3-SAT, 60 variables100NoneJeroslowNoneJeroslow80cpu timecpu time201056040200200 300 400 500 600 700 8000300number clauses400500600700800number clauses(c) Max-CUT, 60 nodes5cpu time4NoneJeroslow3210200 250 300 350 400 450 500number nodesFigure 5: Performance INI AX without Heuristic (None) Jeroslow heuristiccomputed root node search tree (Jeroslow).ProblemMax-One 3colMax-One cntMax-One dpMax-One ezfact32Routing S3Routing S4n. inst.403610510None13.57(1)16.11(4)654.94(2)22.26(4)Learning29.06119.5340.030.701.02410.61(2)VSIDS15.416.5828.630.770.1091.09(9)Figure 6: Structured instances.8.5 Comparison Boolean Optimizersreporting results, omit solver cannot deal corresponding instancestechnical reasons (e.g. cannot deal weighted clauses) performs extremely badcomparison others.Figure 7 contains plots results different benchmarks. Plots b reports resultsrandom unweighted Max-SAT instances. P UEBLO INISAT + orders magnitude slower,included graphics. Max-2-SAT (plot a), INI AX laysAX -DPLL AXSATZ, best option. Max-3-SAT (plot b) INI AXclearly outperforms AX -DPLL close AXSATZ, best.Max-2-SAT Max-3-SAT AXSATZ 3 times faster INI AX AT.24fiM INI AX AT:E FFICIENT W EIGHTED AX -SAT OLVERPlot c reports results random Max-CUT instances. INI AX performs slightly betterAXSATZ, second alternative.random Max-One (plot d) INI AX best solver far. Almost instancessolved instantly P UEBLO AX -DPLL require 10 seconds difficult instances. INISAT + performs poorly. results structured Max-One instances reportedFigure 9. INISAT + seems fastest general. INI AX close performanceP UEBLO. Note, however, p instances, INI AX system solvinginstances.Plot e reports results Random Max-Clique instances. INI AX best solver,order magnitude faster AX -DPLL, second best option. P UEBLO INISAT +perform poorly again. Regarding structured Dimacs instances, INI AX bestoption. solves 36 instances within time limit, AX -DPLL,M INISAT + P UEBLOsolve 34, 22 18 respectively.Plots f , g h present results Combinatorial Auctions following different distributions.paths distribution, INI AX best solver, twice faster AX -DPLL,ranks second. regions distribution, INI AX best solver AX -DPLLsecond best solver requiring double time. paths regions distributions, P UEBLOINISAT + perform poorly. scheduling distribution, INISAT + best solverINI AX AX -DPLL one order magnitude slower.Results regarding unsatisfiable DIMACS instances presented Figure 8. Noteinstances optimum cost 1. Hence, soon INI AX find solution cost 1,clauses declared hard learning backjumping applied hard conflictsarise. results indicate AXSATZ AX -DPLL solve instance sets(Pret150 Aim200), INI AX solves sets instances best timesthem, except hole instances AXSATZ slightly faster. encodeproblems advantageous way P UEBLO INISAT +, is, decision problemsrather optimization problems solve instances similar times INI AX AT.planning instances (Fig. 10) P UEBLO best solver. INI AX second bestsolver, OOLBAR third last one INISAT +. surprising since OOLBARperform learning hard constraints. Results regarding graph coloring instancespresented Fig. 10. observed, INI AX able solve one instanceOOLBAR, P UEBLO INISAT + solve many less instances. Max-CSP problems(Fig. 10) OOLBAR solves instances instantly P UEBLO worst option unablesolve lot instances. INI AX clearly second best solver INI + thirdbest performing solver. Note solve instances.Results regarding instances taken pseudo-boolean evaluation found Figure11. Note first time Max-SAT solver tested pseudo-boolean instances.Results indicate solver consistently outperforms INI AX fairlycompetitive P UEBLO INISAT +.results conclude INI AX robust Weighted MaxSAT solver. competitive pure optimization problems problems lotshard clauses and, sometimes, best option.final remark, note INI AX almost previous benchmarks submitted Second Max-SAT Evaluation 2007, co-located event Tenth International Conference Theory Applications Satisfiability Testing. Hence, interested reader find25fiH ERAS , L ARROSA , & LIVERAS(a) Max-2-SAT, 100 variables5030cpu timeMax-DPLLMiniMaxSatMaxsatz40cpu time(b) Max-3-SAT, 60 variables20100200 300 400 500 600 700 800 900300250200150100500Max-DPLLMiniMaxSatMaxsatz300 400 500 600 700 800 900number clausesnumber clauses(c) Max-CUT, 60 nodes8cpu timeMax-DPLLMaxsatzMiniMaxSat10cpu time(d) Max-ONE, random 3-SAT, 120 variables642030035040045050030Minisat+25PuebloMax-DPLL20MiniMaxSat151050150 200 250 300 350 400 450 500 550number edgesnumber hard clauses(e) Max-Clique, 150 nodes50Minisat+PuebloMax-DPLLMiniMaxSat30PuebloMinisat+Max-DPLLMiniMaxSat80cpu time40cpu time(f) C. Auctions PATHS, 60 Goods100201060402000025507510070 80 90 100 110 120 130 140 150connectivity (%)number bids(g) C. Auctions SCHEDULING, 60 Goodscpu time4030(h) C. Auctions REGIONS, 60 Goods20PuebloMax-DPLLMiniMaxSatMinisat+cpu time50201510Minisat+PuebloMax-DPLLMiniMaxSat5100010070 80 90 100 110 120 130 140 150number bids120140160180200number bidsFigure 7: Plots different benchmarks. Note order legend goes accordanceperformance solvers.exhaustive comparison, including instances solvers, Second Max-SAT Evaluation 2007 web page9 . results evaluation showed INI AX bestperforming solver two four existing categories.9. http://www.maxsat07.udl.es/26fiM INI AX AT:n. inst.13445888ProblemDuboisPret60Pret150HoleAim50Aim100Aim200E FFICIENT W EIGHTED AX -SAT OLVERINI AX0.020.070.018.680.000.000.00AXSATZ148.18(7)10.068.340.019.55AX -DPLL174.33(6)22.0028.000.00172.00Figure 8: Unsatisfiable DIMACS instances.Problem3col803col1003col1203col140cntdpezfact32n. inst.101010103610INI AX0.152.2520.4938.336.5928.810.77P UEBLO0.101.7314.5283.170.131.19(3)0.34INISAT +0.020.120.741.610.121.21(4)0.33Figure 9: Structured Max-one instances.ProblemPlanningGraph ColoringMax-CSP DLMax-CSP DTMax-CSP SLMax-CSP STn. inst.712220202020Toolbar4.0249.29(16)0.080.000.010.00INI AX3.814.16(17)0.200.010.030.01P UEBLO0.1668.50(11)349.08(13)123.67INISAT +7.400.57(11)8.602.400.481.29Figure 10: Results WCSP Max-CSP instances.9. Related Workprevious work done incorporating SAT-techniques inside Max-SAT solver.Alsinet et al. (2005) presented lazy data structure detect clauses become unit, requires static branching heuristic. Argelich Manya (2006a) test different versions branchbound procedure. One versions uses two-watched literals, uses basiclower bounding. conclude none previous approaches general usetwo-watched literals. far know, rest Max-SAT solvers based adjacencylists. Therefore, presumably inefficient unit propagation (Lynce & Silva, 2005), par27fiH ERAS , L ARROSA , & LIVERASProblemmiscLogic synthesisMPIMPSRoutingn. inst.7171481615INI AX3.08(5)82.55(2)37.35(107)22.65(5)58.74(14)P UEBLO8.51(5)36.21(5)32.04(101)36.90(8)5.96INISAT +0.14(5)253.93(5)3.06(105)8.50(8)13.09Figure 11: Results pseudo-boolean instances.ticularly presence long clauses. Argelich Manya (2006b) enhance Max-SAT branchbound procedure learning hard constraints, used combination simple lower bounding techniques. improved version presented Argelich Manya (2007)powerful lower bound, incorporate two-watched literal scheme,backjumping, etc. best knowledge, Max-SAT solver incorporates backjumping.Note INI AX restricts backjumping occurrence hard conflicts. Related worksintegration backjumping techniques branch bound include work ZivanMeisels (2007) Weighted CSP, Manquinho Silva (2004) pseudo-boolean optimization,Nieuwenhuis Oliveras (2006) SAT Modulo Theories.Max-SAT solvers use variations call substraction-based lower bounding.cases, search special patterns mutually inconsistent subsets clauses (Shen &Zhang, 2004; Xing & Zhang, 2005; Alsinet et al., 2005). efficiency reasons, patternsalways restricted small sets small arity clauses (2 3 clauses arity less 3). INI AX uses natural weighted extension approach proposed Li et al. (2005).first one able detect inconsistencies arbitrarily large sets arbitrarily large clauses.idea call resolution-based lower bounding inspired WCSP domain(Larrosa, 2002; Larrosa & Schiex, 2003; de Givry et al., 2003, 2005) first proposedMax-SAT context Larrosa Heras (2005) developed Li et al. (2007), HerasLarrosa (2006), Larrosa et al. (2007). works, special patterns fixed-sizeresolution trees executed. use simulated unit propagation allows INI AXidentify arbitrarily large resolution trees. following example, present two inconsistentsubsets clauses detected INI AX transformed equivalent formulaprevious solvers cannot transform since limited specific patterns:{(x1 , w1 ), (x2 , w2 ), (x3 , w3 ), (x1 x2 x3 , w4 )}{(x1 , w1 ), (x1 x2 , w2 ), (x1 x2 x3 , w3 ), (x1 x2 x3 x4 , w4 ), (x1 x2 x3 x4 , w5 )}first case, INI AX replaces clauses (2, m) = min{w1 , w2 , w3 , w4 }set compensation clauses. second case, INI AX replaces (2, m)= min{w1 , w2 , w3 , w4 , w5 } set compensation clauses. cases, equivalencepreserved. However, solvers literature detect inconsistent subset clausescannot transform problem equivalent one (Li et al., 2007) simply cannot detect(Heras & Larrosa, 2006).probing method derive weighted unit clauses related 2 RES cycle ruleHeras Larrosa (2006) Larrosa et al. (2007), failed literals Li et al. (2006),28fiM INI AX AT:E FFICIENT W EIGHTED AX -SAT OLVERsingleton consistency CSP (Debruyne & Bessiere, 1999). Again, use simulated unitpropagation allows INI AX identify arbitrarily large resolution trees.10. Conclusions Future WorkINI AX efficient robust Max-SAT solver deal hard softclauses well pseudo-boolean functions. incorporates best available techniquestype problems, performance similar best specialized solver. Besides development INI AX combining, first time, known techniques different fields,main original contribution paper novel lower bounding technique based resolution.INI AX lower bounding combines clean elegant way approaches proposed last years, mainly based unit-propagation-based lowerbounding resolution-based problem transformation. paper use information provided propagation queue (i) determine subset inconsistent clauses (ii) determinesimple ordering resolution applied increase lower bound generateequivalent formula. However, necessarily best ordering so. easy seedifferent orderings may generate resolvents compensation clauses different arities. oneselects ordering generates smallest resolvents compensation clauses resultingformula may presumably simpler. Future work concerns study orderings, development VSIDS-like heuristics soft clauses backjumping techniques soft conflicts.Acknowledgmentswould like thank Niklas Een Niklas Sorensson making INISAT + code publiclyavailable. also grateful anonymous referees helpful suggestions improvingpaper.work partially supported Spanish Ministry Education Scienceprojects TIN2006-15387-C03-02 (Heras Larrosa) TIN2004-03382 (Oliveras).ReferencesAlsinet, T., Manya, F., & Planes, J. (2005). Improved Exact Solvers Weighted Max-SAT.Proceedings SAT05, Vol. 3569 LNCS, pp. 371377. Springer.Argelich, J., & Manya, F. (2006a). Exact Max-SAT solvers over-constrained problems. J.Heuristics, 12(4-5), 375392.Argelich, J., & Manya, F. (2006b). Learning Hard Constraints Max-SAT. ProceedingsCSCLP06, Vol. 4651 LNCS, pp. 112. Springer.Argelich, J., & Manya, F. (2007). Partial Max-SAT Solvers Clause Learning. ProceedingsSAT07, Vol. 4501 LNCS, pp. 2840. Springer.Barth, P. (1995). Davis-Putnam Based Enumeration Algorithm Linear pseudo-Boolean Optimization. Research report MPI-I-95-2-003, Max-Planck-Institut fur Informatik, Im Stadtwald, D-66123 Saarbrucken, Germany.Brglez, F., Li, X., & Stallman, M. (2002). role skeptic agent testing benchmarkingSAT algorithms. Proceedings SAT02, pp. 354361.29fiH ERAS , L ARROSA , & LIVERASBuro, M., & Buning, H. K. (1993). Report SAT Competition. Bulletin EuropeanAssociation Theoretical Computer Science, 49, 143151.Cha, B., Iwama, K., Kambayashi, Y., & Miyazaki, S. (1997). Local search algorithms partialMAXSAT. Proceedings AAAI97, pp. 263268. MIT Press.Cooper, M., Cussat-Blanc, S., de Roquemaurel, M., & Regnier, P. (2006). Soft Arc ConsistencyApplied Optimal Planning. Proceedings CP06, Vol. 4204 LNCS, pp. 680684.Springer.Davis, M., Logemann, G., & Loveland, G. (1962). machine program theorem proving. Communications ACM, 5, 394397.de Givry, S., Heras, F., Larrosa, J., & Zytnicki, M. (2005). Existential arc consistency: gettingcloser full arc consistency weighted CSPs. Proceedings 19th IJCAI, pp. 8489.Professional Book Center.de Givry, S., Larrosa, J., Meseguer, P., & Schiex, T. (2003). Solving Max-SAT weighted CSP.Proceedings CP03, Vol. 2833 LNCS, pp. 363376. Springer.Debruyne, R., & Bessiere, C. (1999). practicable filtering techniques constraint satisfaction problem. Proceedings ICJAI97, pp. 412417. Morgan Kaufmann.Een, N., & Sorensson, N. (2003). Extensible SAT-solver. Proceedings SAT03, Vol. 2919LNCS, pp. 502518. Springer.Een, N., & Sorensson, N. (2006). Translating Pseudo-Boolean Constraints SAT. JournalSatisfiability, Boolean Modeling Computation, 2, 126.Fahle, T. (2002). Simple fast: Improving branch-and-bound algorithm maximum clique.Proceedings ESA02, Vol. 2461 LNCS, pp. 485498. Springer.Freeman, J. W. (1995). Improvements Propositional Satisfiability Search Algorithms. Ph.D.thesis, University Pennsylvania.Fu, Z., & Malik, S. (2006). Solving Partial MAX-SAT Problem. Proceedings SAT06,Vol. 4121 LNCS, pp. 252265. Springer.Heras, F., & Larrosa, J. (2006). New Inference Rules Efficient Max-SAT Solving. Proceedings21th AAAI. AAAI Press.Jeroslow, R. G., & Wang, J. (1990). Solving propositional satisfiability problems. Annals Mathematics Artificial Intelligence, 1, 167187.K. Leyton-Brown, M. P., & Shoham, Y. (2000). Towards universal test suite combinatorialauction algorithms. Proceedings ACM Conference Electronic Commerce00, pp.6676.Karloff, H. J., & Zwick, U. (1997). 7/8-Approximation Algorithm MAX 3SAT?. FOCS,pp. 406415.Larrosa, J., & Heras, F. (2005). Resolution Max-SAT relation local consistencyweighted CSPs. Proceedings IJCAI05, pp. 193198. Professional Book Center.Larrosa, J., Heras, F., & de Givry, S. (2007). logical approach efficient max-sat solving.Artificial Intelligence. appear.30fiM INI AX AT:E FFICIENT W EIGHTED AX -SAT OLVERLarrosa, J., & Schiex, T. (2003). quest best form local consistency weightedCSP. Proceedings 18th IJCAI, pp. 239244.Larrosa, J. (2002). Node Arc Consistency Weighted CSP. Proceedings AAAI02, pp.4853. AAAI Press.Le Berre, D. (2001). Exploiting real power Unit Propagation Lookahead. ProceedingsLICS Workshop Theory Applications Satisfiability Testing.Le Berre, D. (2006). SAT4j project Max-SAT.. http://www.sat4j.org/.Li, C., Manya, F., & Planes, J. (2005). Exploiting Unit Propagation Compute Lower BoundsBranch Bound Max-SAT Solvers. Proceedings CP05, Vol. 3709 LNCS, pp.403414.Li, C., Manya, F., & Planes, J. (2007). New Inference Rules Max-SAT. Journal ArtificialIntelligence Research. appear.Li, C.-M., Manya, F., & Planes, J. (2006). Detecting Disjoint Inconsistent Subformulas Computing Lower Bounds Max-SAT. Proceedings 21th AAAI. AAAI Press.Lynce, I., & Silva, J. P. M. (2003). Probing-Based Preprocessing Techniques PropositionalSatisfiability. Proceedings ICTAI03, pp. 105111. IEEE Computer Society.Lynce, I., & Silva, J. P. M. (2005). Efficient data structures backtrack search SAT solvers. Ann.Math. Artif. Intell., 43(1), 137152.Manquinho, V. M., & Silva, J. P. M. (2004). Satisfiability-Based Algorithms Boolean Optimization. Ann. Math. Artif. Intell., 40(3-4), 353372.Moskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: EngineeringEfficient SAT Solver. Proceedings DAC01, pp. 530535. ACM.Nieuwenhuis, R., & Oliveras, A. (2006). SAT Modulo Theories Optimization Problems.Proceedings SAT06, Vol. 4121 LNCS, pp. 156169. Springer.Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley, USA.Sandholm, T. (1999). Algorithm Optimal Winner Determination Combinatorial Auctions.Proceedings IJCAI99, pp. 542547. Morgan Kaufmann.Sheini, H. M., & Sakallah, K. A. (2006). Pueblo: Hybrid Pseudo-Boolean SAT Solver. JournalSatisfiability, Boolean Modeling Computation, 2, 165189.Shen, H., & Zhang, H. (2004). Study lower bounds Max-2-SAT. Proceedings AAAI04,pp. 185190. AAAI Press / MIT Press.Silva, J. P. M., & Sakallah, K. A. (1996). GRASP - new search algorithm satisfiability.ICCAD, pp. 220227.Smyth, K., Hoos, H. H., & Stutzle, T. (2003). Iterated Robust Tabu Search MAX-SAT.Proceedings AI03, Vol. 2671 LNCS, pp. 129144. Springer.Tompkins, D. A. D., & Hoos, H. H. (2004). UBCSAT: Implementation ExperimentationEnvironment SLS Algorithms SAT & MAX-SAT. Proceedings SAT04, Vol.3542 LNCS, pp. 306320. Springer.Walsh, T. (2000). SAT v CSP. Proceedings CP00, Vol. 1894 LNCS, pp. 441456. Springer.31fiH ERAS , L ARROSA , & LIVERASXing, Z., & Zhang, W. (2005). MaxSolver: efficient exact algorithm (weighted) maximumsatisfiability. Artificial Intelligence, 164(1-2), 4780.Zhang, L., Madigan, C. F., Moskewicz, M. W., & Malik, S. (2001). Efficient Conflict Driven Learning Boolean Satisfiability Solver. Proceedings ICCAD01, pp. 279285.Zivan, R., & Meisels, A. (2007). Conflict directed Backjumping MaxCSPs. ProceedingsIJCAI07, pp. 198204.32fiJournal Artificial Intelligence Research 31 (2008) 543-590Submitted 08/07; published 03/08Creating Relational Data UnstructuredUngrammatical Data SourcesMatthew MichelsonCraig A. Knoblockmichelso@isi.eduknoblock@isi.eduUniversity Southern CaliforniaInformation Sciences Instistute4676 Admiralty WayMarina del Rey, CA 90292 USAAbstractorder agents act behalf users, retrieve integratevast amounts textual data World Wide Web. However, much useful dataWeb neither grammatical formally structured, making querying difficult.Examples types data sources online classifieds like Craigslist1 auctionitem listings like eBay.2 call unstructured, ungrammatical data posts.unstructured nature posts makes query integration difficult attributesembedded within text. Also, attributes conform standardized values,prevents queries based common attribute value. schema unknownvalues may vary dramatically making accurate search difficult. Creating relationaldata easy querying requires define schema embedded attributesextract values posts standardizing values. Traditional informationextraction (IE) inadequate perform task relies clues data,structure natural language, neither found posts. Furthermore,traditional information extraction incorporate data cleaning, necessaryaccurately query integrate source. two-step approach described papercreates relational data sets unstructured ungrammatical text addressingissues. this, require set known entities called reference set. first stepaligns post member reference set. allows algorithm defineschema post include standard values attributes defined schema.second step performs information extraction attributes, including attributeseasily represented reference sets, price. manner create relationalstructure previously unstructured data, supporting deep accurate queriesdata well standard values integration. experimental results showtechnique matches posts reference set accurately efficiently outperformsstate-of-the-art extraction systems extraction task posts.1. Introductionfuture vision Web includes computer agents searching information, makingdecisions taking actions behalf human users. instance, agent could querynumber data sources find lowest price given car email usercar listing, along directions seller available appointments see car.1. www.craigslist.org2. www.ebay.comc2008AI Access Foundation. rights reserved.fiMichelson & Knoblockrequires agent contain two data gathering mechanisms: ability querysources ability integrate relevant sources information.However, data gathering mechanisms assume sources designed support relational queries, well defined schema standard valuesattributes. Yet always case. many data sourcesWorld Wide Web would useful query, textual data within unstructured designed support querying. call text data sourcesposts. Examples posts include text eBay auction listings, Internet classifiedslike Craigslist, bulletin boards Bidding Travel3 , even summary texthyperlinks returned querying Google. running example, consider threeposts used car classifieds shown Table 1.Table 1: Three posts Honda Civics CraigslistCraigslist Post93 civic 5speed runs great obo (ri) $180093- 4dr Honda Civc LX Stick Shift $180094 DEL SOL Si Vtec (Glendale) $3000current method query posts, whether agent person, keyword search.However, keyword search inaccurate cannot support relational queries. example,difference spelling keyword attribute within post wouldlimit post returned search. would case user searchedexample listings Civic since second post would returned. Another factorlimits keyword accuracy exclusion redundant attributes. example,classified posts cars include car model, make, since makeimplied model. shown first third post Table 1. cases,user keyword search using make Honda, posts returned.Moreover, keyword search rich query framework. instance, considerquery, average price Hondas 1999 later?keyword search requires user search Honda retrieve 1999later. user must traverse returned set, keeping track pricesremoving incorrectly returned posts.However, schema standardized attribute values defined entitiesposts, user could run example query using simple SQL statementaccurately, addressing problems created keyword search. standardizedattribute values ensure invariance issues spelling differences. Also, postassociated full schema values, even though post might contain carmake, instance, schema correct value it, returnedquery car makes. Furthermore, standardized values allow integrationsource outside sources. Integrating sources usually entails joining two sourcesdirectly attributes translations attributes. Without standardized values3. www.biddingfortravel.com544fiRelational Data Unstructured Data Sourcesschema, would possible link ungrammatical unstructured datasources outside sources. paper addresses problem adding schemastandardized attributes set posts, creating relational data set supportdeep accurate queries.One way create relational data set posts define schemafill values schema elements using techniques information extraction. sometimes called semantic annotation. example, taking secondpost Table 1 semantically annotating might yield 93- 4dr Honda Civc LX StickShift $1800 <make>Honda< \make> <model>Civc< \model> <trim>4dr LX< \trim><year>1993< \year> <price>1800< \price>. However, traditional information extraction, relies grammatical structural characteristics text identify attributesextract. Yet posts definition structured grammatical. Therefore, wrapperextraction technologies Stalker (Muslea, Minton, & Knoblock, 2001) RoadRunner(Crescenzi, Mecca, & Merialdo, 2001) cannot exploit structure posts.posts grammatical enough exploit Natural Language Processing (NLP) based extractiontechniques used Whisk (Soderland, 1999) Rapier (Califf & Mooney,1999).Beyond difficulties extracting attributes within post using traditional extraction methods, also require values attributes standardized,process known data cleaning. Otherwise, querying newly relational data wouldinaccurate boil keyword search. instance, using annotation above,would still need query model Civc return record. Traditionalextraction address this.However, data cleaning algorithms assume tuple-to-tuple transformations (Lee, Ling, Lu, & Ko, 1999; Chaudhuri, Ganjam, Ganti, & Motwani, 2003).is, function maps attributes one tuple attributes another. approach would work ungrammatical unstructured data,attributes embedded within post, maps set attributesreference set. Therefore need take different approach problems figuringattributes within post cleaning them.approach creating relational data sets unstructured ungrammaticalposts exploits reference sets. reference set consists collections known entitiesassociated, common attributes. reference set online (or offline) setreference documents, CIA World Fact Book.4 also online (oroffline) database, Comics Price Guide.5 Semantic Web one envisionbuilding reference sets numerous ontologies already exist. Using standardizedontologies build reference sets allows consensus agreement upon reference set values,implies higher reliability reference sets others might exist oneexperts opinion. Using car example, reference set might Edmunds car buyingguide6 , defines schema cars well standard values attributesmodel trim. order construct reference sets Web sources,4. http://www.cia.gov/cia/publications/factbook/5. www.comicspriceguide.com6. www.edmunds.com545fiMichelson & KnoblockEdmunds car buying guide, use wrapper technologies (Agent Builder7 case)scrape data Web source, using schema source defines car.use reference set build relational data set exploit attributesreference set determine attributes post extracted. first stepalgorithm finds best matching member reference set post.called record linkage step. matching post member reference setdefine schema elements post using schema reference set,provide standard attributes attributes using attributes referenceset user queries posts.Next, perform information extraction extract actual values postmatch schema elements defined reference set. step informationextraction step. information extraction step, parts post extractedbest match attribute values reference set member chosenrecord linkage step. step also extract attributes easily representedreference sets, prices dates. Although already schemastandardized attributes required create relational data set posts, stillextract actual attributes embedded within post accuratelylearn extract attributes represented reference set, prices dates.attributes extracted using regular expressions, extract actualattributes within post might able accurately. example, considerFord 500 car. Without actually extracting attributes within post, mightextract 500 price, actually car name. overall approach outlinedFigure 1.Although previously describe similar approach semantically annotating posts(Michelson & Knoblock, 2005), paper extends research combining annotation work scalable record matching (Michelson & Knoblock, 2006).make matching step annotation scalable, also demonstrateswork efficient record matching extends unique problem matching posts,embedded attributes, structured, relational data. paper also presentsdetailed description past work, including thorough evaluation procedure previously, using larger experimental data sets including reference setincludes tens thousands records.article organized follows. first describe algorithm aligningposts best matching members reference set Section 2. particular,show matching takes place, efficiently generate candidate matchesmake matching procedure scalable. Section 3, demonstrateexploit matches extract attributes embedded within post. presentexperiments Section 4, validating approaches blocking, matching informationextraction unstructured ungrammatical text. follow discussionresults Section 5 present related work Section 6. finish finalthoughts conclusions Section 7.7. product Fetch Technologies http://www.fetch.com/products.asp546fiRelational Data Unstructured Data SourcesFigure 1: Creating relational data unstructured sources2. Aligning Posts Reference Setexploit reference set attributes create relational data posts, algorithm needs first decide member reference set best matches post.matching, known record linkage (Fellegi & Sunter, 1969), provides schema attribute values necessary query integrate unstructured ungrammatical datasource. Record linkage broken two steps: generating candidate matches, calledblocking; separating true matches candidates matchingstep.approach, blocking generates candidate matches based similarity methodscertain attributes reference set compare posts. carsexample, algorithm may determine generate candidates finding commontokens posts make attribute reference set. step detailedSection 2.1 crucial limiting number candidates matches later examinematching step. generating candidates, algorithm generates large setfeatures post candidate matches reference set. Usingfeatures, algorithm employs machine learning methods separate true matchesfalse positives generated blocking. matching detailed Section 2.2.547fiMichelson & Knoblock2.1 Generating Candidates Learning Blocking Schemes Record Linkageinfeasible compare post members reference set. Thereforepreprocessing step generates candidate matches comparing recordssets using fast, approximate methods. called blocking thoughtpartitioning full cross product record comparisons mutually exclusive blocks(Newcombe, 1967). is, block attribute, first sort cluster data setsattribute. apply comparison method single member block.blocking, candidate matches examined detail discover true matches.two main goals blocking. First, blocking limit number candidate matches, limits number expensive, detailed comparisons neededrecord linkage. Second, blocking exclude true matches set candidate matches. means trade-off finding matching recordslimiting size candidate matches. So, overall goal blocking makematching step scalable, limiting number comparisons must make,hindering accuracy passing many true matches possible.blocking done using multi-pass approach (Hernandez & Stolfo, 1998),combines candidates generated independent runs. example, carsdata, might make one pass data blocking tokens car model,another run might block using tokens make along common tokens trimvalues. One view multi-pass approach rule disjunctive normal form,conjunction rule defines run, union rules combinescandidates generated run. Using example, rule might become ({tokenmatch, model} ({token-match, year}) ({token-match, make})). effectivenessmulti-pass approach hinges upon methods attributes chosen conjunctions.Note conjunction set {method, attribute} pairs, makerestrictions methods used. set methods could include full stringmetrics cosine similarity, simple common token matching outlined above, evenstate-of-the-art n-gram methods shown experiments. key methodsnecessarily choosing fastest (though show account method speedbelow), rather choosing methods generate smallest set candidatematches still cover true positives, since matching step consumetime.Therefore, blocking scheme include enough conjunctions cover many truematches can. example, first conjunct might cover true matchesdatasets compared overlap years, second conjunctcover rest true matches. adding independent runsmulti-pass approach.However, since blocking scheme includes many conjunctions needs,conjunctions limit number candidates generate. example, secondconjunct going generate lot unnecessary candidates since return recordsshare make. adding {method, attribute} pairs conjunction,limit number candidates generates. example, change ({token-match,548fiRelational Data Unstructured Data Sourcesmake}) ({token-match, make} {token-match, trim}) still cover new true matches,generate fewer additional candidates.Therefore effective blocking schemes learn conjunctions minimize falsepositives, learn enough conjunctions cover many true matches possible. two goals blocking clearly defined Reduction Ratio PairsCompleteness (Elfeky, Verykios, & Elmagarmid, 2002).Reduction Ratio (RR) quantifies well current blocking scheme minimizesnumber candidates. Let C number candidate matches N sizecross product data sets.RR = 1 C/Nclear adding {method,attribute} pairs conjunction increasesRR, changed ({token-match, zip}) ({token-match, zip} {token-match,first name}).Pairs Completeness (PC) measures coverage true positives, i.e., manytrue matches candidate set versus entire set. Sm numbertrue matches candidate set, Nm number matches entire dataset,then:P C = Sm /NmAdding disjuncts increase PC. example, added second conjunction example blocking scheme first cover matches.blocking approach paper, Blocking Scheme Learner (BSL), learns effectiveblocking schemes disjunctive normal form maximizing reduction ratio pairscompleteness. way, BSL tries maximize two goals blocking. Previouslyshowed BSL aided scalability record linkage (Michelson & Knoblock, 2006),paper extends idea showing also work case matching postsreference set records.BSL algorithm uses modified version Sequential Covering Algorithm (SCA),used discover disjunctive sets rules labeled training data (Mitchell, 1997).case, SCA learn disjunctive sets conjunctions consisting {method, attribute}pairs. Basically, call LEARN-ONE-RULE generates conjunction, BSL keepsiterating call, covering true matches left iteration. waySCA learns full blocking scheme. BSL algorithm shown Table 2.two modifications classic SCA algorithm, shown bold.First, BSL runs examples left cover, rather stoppingthreshold. ensures maximize number true matches generatedcandidates final blocking rule (Pairs Completeness). Note might, turn,yield large number candidates, hurting Reduction Ratio. However, omitting truematches directly affects accuracy record linkage, blocking preprocessing steprecord linkage, important cover many true matches possible.way BSL fulfills one blocking goals: eliminating true matches possible. Second,learn new conjunction (in LEARN-ONE-RULE step) current blockingscheme rule already contains newly learned rule, removerule containing newly learned rule. optimization allows us check rulecontainment go, rather end.549fiMichelson & KnoblockTable 2: Modified Sequential Covering AlgorithmSEQUENTIAL-COVERING(class, attributes, examples)LearnedRules {}Rule LEARN-ONE-RULE (class, attributes, examples)examples left cover,LearnedRules LearnedRules RuleExamples Examples - {Examples covered Rule}Rule LEARN-ONE-RULE (class, attributes, examples)Rule contains previously learned rules, removecontained rules.Return LearnedRulesrule containment possible guarantee learn less restrictiverules go. prove guarantee follows. proof done contradiction.Assume two attributes B, method X. Also, assume previouslylearned rules contain following conjunction, ({X, A}) currently learned rule({X, A} {X, B}). is, assume learned rules contains rule lessspecific currently learned rule. case, must leastone training example covered ({X, A} {X, B}) covered ({X, A}), sinceSCA dictates remove examples covered ({X, A}) learn it. Clearly,cannot happen, since examples covered specific ({X, A} {X, B})would covered ({X, A}) already removed, means couldlearned rule ({X, A} {X, B}). Thus, contradiction.stated before, two main goals blocking minimize size candidate set, removing true matches set. already mentionedBSL maximizes number true positives candidate set describeBSL minimizes overall size candidate set, yields scalable recordlinkage. minimize candidate sets size, learn restrictive conjunctioncall LEARN-ONE-RULE SCA. define restrictive minimizing number candidates generated, long certain number true matchesstill covered. (Without restriction, could learn conjunctions perfectly minimizenumber candidates: simply return none.)this, LEARN-ONE-RULE step performs general-to-specific beam search.starts empty conjunction step adds {method, attribute} pairyields smallest set candidates still cover least set number true matches.is, learn conjunction maximizes Reduction Ratio,time covering minimum value Pairs Completeness. use beam search allowbacktracking, since search greedy. However, since beam search goesgeneral-to-specific, ensure final rule restrictive possible. fullLEARN-ONE-RULE given Table 3.constraint conjunction minimum PC ensures learned conjunction over-fit data. Without restriction, would possibleLEARN-ONE-RULE learn conjunction returns candidates, uselessly producingoptimal RR.550fiRelational Data Unstructured Data Sourcesalgorithms behavior well defined minimum PC threshold. Consider,case algorithm learning restrictive rule minimumcoverage. case, parameter ends partitioning space cross productexample records threshold amount. is, set threshold amount 50%examples covered, restrictive first rule covers 50% examples.next rule covers 50% remaining, 25% examples. nextcover 12.5% examples, etc. sense, parameter well defined. setthreshold high, learn fewer, less restrictive conjunctions, possibly limiting RR,although may increase PC slightly. set lower, cover examples,need learn conjuncts. newer conjuncts, turn, may subsumed laterconjuncts, waste time learn. So, long parameter smallenough, affect coverage final blocking scheme, smallerslows learning. set parameter 50% experiments8 .analyze running time BSL show BSL take accountrunning time different blocking methods, need be. Assume x (method,attribute) pairs (token, f irst name). Now, assume beam size b, sinceuse general-to-specific beam-search Learn-One-Rule procedure. Also, timebeing, assume (method, attribute) pair generate blocking candidates O(1)time. (We relax assumption later.) time hit Learn-One-Rule within BSL,try rules beam (attribute, method) pairs currentbeam rules. So, worst case, takes O(bx) time, since (method,attribute) pair beam, try (method, attribute) pairs. Now,worst case, learned disjunct would cover 1 training example, ruledisjunction pairs x. Therefore, run Learn-One-Rule x times, resultinglearning time O(bx2 ). e training examples, full training time O(ebx2 ),BSL learn blocking scheme.Now, assumed (method, attribute) runs O(1) time,clearly case, since substantial amount literature blocking methods8. Setting parameter lower 50% insignificant effect results, setting muchhigher, 90%, increased PC small amount (if all), decreasing RR.Table 3: Learning conjunction {method, attribute} pairsLEARN-ONE-RULE (attributes, examples, min thresh, k)Best-Conjunction {}Candidate-conjunctions {method, attribute} pairsCandidate-conjunctions empty,ch Candidate-conjunctionsfirst iterationch ch {method,attribute}Remove ch duplicates, inconsistent max. specificREDUCTION-RATIO(ch) > REDUCTION-RATIO(Best-Conjunction)PAIRS-COMPLETENESS(ch) min threshBest-Conjunction chCandidate-conjunctions best k members Candidate-conjunctionsreturn Best-conjunction551fiMichelson & Knoblockblocking times vary significantly (Bilenko, Kamath, & Mooney, 2006). Letus define function tx (e) represents long takes single (method, attribute)pair x generate e candidates training example. Using notation,Learn-One-Rule time becomes O(b(xtx (e))) (we run tx (e) time pair x)full training time becomes O(eb(xtx (e))2 ). Clearly running time dominatedexpensive blocking methodology. rule learned, boundedtime takes run rule (method, attribute) pairs involved, takes O(xtx (n)),n number records classifying.practical standpoint, easily modify BSL account time takescertain blocking methods generate candidates. Learn-One-Rule step,change performance metric reflect Reduction Ratio blocking timeweighted average. is, given Wrr weight Reduction Ratio Wbweight blocking time, modify Learn-One-Rule maximize performancedisjunct based weighted average. Table 4 shows modified version LearnOne-Rule, changes shown bold.Table 4: Learning conjunction {method, attribute} pairs using weightsLEARN-ONE-RULE (attributes, examples, min thresh, k)Best-Conj {}Candidate-conjunctions {method, attribute} pairsCandidate-conjunctions empty,ch Candidate-conjunctionsfirst iterationch ch {method,attribute}Remove ch duplicates, inconsistent max. specificSCORE(ch) = Wrr REDUCTION-RATIO(ch)+Wb BLOCK-TIME(ch)SCORE(Best-Conj) = Wrr REDUCTION-RATIO(Best-conj)+Wb BLOCK-TIME(Best-conj)SCORE(ch) > SCORE(Best-conj)PAIRS-COMPLETENESS(ch) min threshBest-conj chCandidate-conjunctions best k members Candidate-conjunctionsreturn Best-conjNote set Wb 0, using version Learn-One-Ruleused throughout paper, consider Reduction Ratio. Sincemethods (token n-gram match) simple compute, requiring time buildinitial index candidate generation, safely set Wb 0. Also,making trade-off time versus reduction might always appropriate decision.Although method may fast, sufficiently reduce reduction ratio,time takes record linkage step might increase time wouldtaken run blocking using method provides larger increase reduction ratio.Since classification often takes much longer candidate generation, goalminimize candidates (maximize reduction ratio), turn minimizes classificationtime. Further, key insight BSL choose blocking method,importantly choose appropriate attributes block on. sense,BSL like feature selection algorithm blocking method. show552fiRelational Data Unstructured Data Sourcesexperiments, blocking important pick right attribute combinations,BSL does, even using simple methods, blocking using sophisticatedmethods.easily extend BSL algorithm handle case matching posts membersreference set. special case posts attributes embeddedwithin reference set data relational structured schema elements.handle special case, rather matching attribute method pairs acrossdata sources LEARN-ONE-RULE, instead compare attribute methodpairs relational data entire post. small change, showingalgorithm works well even special case.learn good blocking scheme, efficiently generate candidatespost set align reference set. blocking step essential mapping largeamounts unstructured ungrammatical data sources larger larger referencesets.2.2 Matching Stepset candidates generated blocking one find memberreference set best matches current post. is, one data sources record (thepost) must align record data source (the reference set candidates).whole alignment procedure referred record linkage (Fellegi & Sunter,1969), refer finding particular matches blocking matching step.Figure 2: traditional record linkage problemHowever, record linkage problem presented article differs traditionalrecord linkage problem well studied. Traditional record linkage matches recordone data source record another data source relating respective,decomposed attributes. instance, using second post Table 1, assumingdecomposed attributes, make post compared make reference553fiMichelson & KnoblockFigure 3: problem matching post reference setset. also done models, trims, etc. record reference setbest matches post based similarities attributes would consideredmatch. represented Figure 2. Yet, attributes posts embeddedwithin single piece text yet identified. text compared referenceset, already decomposed attributes extraneoustokens present post. Figure 3 depicts problem. type matchingtraditional record linkage approaches apply.Instead, matching step compares post attributes reference setconcatenated together. Since post compared whole record reference set(in sense attributes), comparison record levelapproximately reflects similar embedded attributes postattributes candidate match. mimics idea traditional record linkage,comparing fields determines similarity record level.However, using record level similarity possible two candidatesgenerate record level similarity differing individual attributes. oneattributes discriminative other, needs way reflectthat. example, consider Figure 4. figure, two candidates share makemodel. However, first candidate shares year second candidate sharestrim. Since candidates share make model, anotherattribute common, possible generate record level comparison. Yet,trim car, especially rare thing like Hatchback discriminativesharing year, since lots cars make, model year,differ trim. difference individual attributes needs reflected.discriminate attributes, matching step borrows idea traditionalrecord linkage incorporating individual comparisons attribute554fiRelational Data Unstructured Data SourcesFigure 4: Two records equal record level different field level similaritiesdata source best way determine match. is, record levelinformation enough discriminate matches, field level comparisons must exploitedwell. field level comparisons matching step compares postindividual attribute reference set.record field level comparisons represented vector different similarity functions called RL scores. incorporating different similarity functions, RL scoresreflects different types similarity exist text. Hence, record levelcomparison, matching step generates RL scores vector postattributes concatenated. generate field level comparisons, matching step calculates RL scores post individual attributes referenceset. RL scores vectors stored vector called VRL . populated,VRL represents record field level similarities post memberreference set.example reference set Figure 3, schema 4 attributes <make, model,trim, year >. Assuming current candidate <Honda, Civic, 4D LX, 1993>,VRL looks like:VRL =<RLRLRLRLRLscores(post,scores(post,scores(post,scores(post,scores(post,Honda),Civic),4D LX),1993),Honda Civic 4D LX 1993)>generally:555fiMichelson & KnoblockVRL =<RL scores(post,RL scores(post,...,RL scores(post,RL scores(post,attribute1 ),attribute2 ),attributen ),attribute1 attribute2 . . . attributen )>RL scores vector meant include notions many ways exist definesimilarity textual values data sources. might caseone attribute differs another misplaced, missing changed letters. sortsimilarity identifies two attributes similar, misspelled, called editdistance. Another type textual similarity looks tokens attributesdefines similarity based upon number tokens shared attributes.token level similarity robust spelling mistakes, puts emphasisorder tokens, whereas edit distance requires order tokens matchorder attributes similar. Lastly, cases one attribute may soundlike another, even spelled differently, one attribute may share commonroot word another attribute, implies stemmed similarity. last twoexamples neither token edit distance based similarities.capture different similarity types, RL scores vector built three vectors reflect different similarity types discussed above. Hence, RL scoresis:RL scores(post, attribute)=<token scores(post, attribute),edit scores(post, attribute),scores(post, attribute)>vector token scores comprises three token level similarity scores. Two similarityscores included vector based Jensen-Shannon distance, definessimilarities probability distributions tokens. One uses Dirichlet prior (Cohen,Ravikumar, & Feinberg, 2003) smooths token probabilities using JelenikMercer mixture model (Zhai & Lafferty, 2001). last metric token scores vectorJaccard similarity.scores included, token scores vector takes form:token scores(post, attribute)=<Jensen-Shannon-Dirichlet(post, attribute),Jensen-Shannon-JM-Mixture(post, attribute),Jaccard(post, attribute)>vector edit scores consists edit distance scores comparisonsstrings character level defined operations turn one string another.instance, edit scores vector includes Levenshtein distance (Levenshtein, 1966),returns minimum number operations turn string string T, SmithWaterman distance (Smith & Waterman, 1981) extension Levenshteindistance. last score vector edit scores Jaro-Winkler similarity (Winkler& Thibaudeau, 1991), extension Jaro metric (Jaro, 1989) used findsimilar proper nouns. strict edit-distance, regard operationstransformations, Jaro-Winkler metric useful determinant string similarity.character level metrics, edit scores vector defined as:556fiRelational Data Unstructured Data Sourcesedit scores(post, attribute)=<Levenshtein(post, attribute),Smith-Waterman(post, attribute),Jaro-Winkler(post, attribute)>similarities edit scores token scores vector defined SecondString package (Cohen et al., 2003) used experimental implementationdescribed Section 4.Lastly, vector scores captures two types similarity fiteither token level edit distance similarity vector. vector includes two typesstring similarities. first Soundex score post attribute.Soundex uses phonetics token basis determining similarity.is, misspelled words sound receive high Soundex score similarity.similarity based upon Porter stemming algorithm (Porter, 1980),removes suffixes strings root words compared similarity.helps alleviate possible errors introduced prefix assumption introducedJaro-Winkler metric, since stems scored rather prefixes. Includingscores, scores vector becomes:scores(post, attribute)=<Porter-Stemmer(post, attribute),Soundex(post, attribute)>Figure 5: full vector similarity scores used record linkageFigure 5 shows full composition VRL , constituent similarity scores.VRL constructed candidates, matching step performsbinary rescoring VRL help determine best match amongst candidates. rescoring helps determine best possible match post separating557fiMichelson & Knoblockbest candidate much possible. might candidatessimilarly close values, one best match, rescoring emphasizesbest match downgrading close matches element values obvious non-matches, boosting difference score bestcandidates elements.rescore vectors candidate set C, rescoring method iterateselements xi VRL C, VRL (s) contain maximum value xi mapxi 1, VRL (s) map xi 0. Mathematically, rescoring method is:VRLj C, j = 0... |C|fifififixi VRLj , = 0... fiVRLj fi(f (xi , VRLj ) =1, xi = max(xt VRLs , VRLs C, = i, = 0... |C|)0, otherwiseexample, suppose C contains 2 candidates, VRL1 VRL2 :VRL1 = <{.999,...,1.2},...,{0.45,...,0.22}>VRL2 = <{.888,...,0.0},...,{0.65,...,0.22}>rescoring become:VRL1 = <{1,...,1},...,{0,...,1}>VRL2 = <{0,...,0},...,{1,...,1}>rescoring, matching step passes VRL Support Vector Machine (SVM)(Joachims, 1999) trained label matches non-matches. best matchcandidate SVM classifies match, maximally positive scoredecision function. one candidate share maximum scoredecision function, thrown matches. enforces strict 1-1 mappingposts members reference set. However, 1-n relationship capturedrelaxing restriction. algorithm keeps either first candidatemaximal decision score, chooses one randomly set candidatesmaximum decision score.Although use SVMs paper differentiate matches non-matches,algorithm strictly tied method. main characteristics learningproblem feature vectors sparse (because binary rescoring)concepts dense (since many useful features may needed thus nonepruned feature selection). also tried use Nave Bayes classifier matchingtask, monumentally overwhelmed number features numbertraining examples. Yet say methods deal sparsefeature vectors dense concepts, online logistic regression boosting, couldused place SVM.match post found, attributes matching reference set memberadded annotation post including values reference set attributestags reflect schema reference set. overall matching algorithmshown Figure 6.558fiRelational Data Unstructured Data SourcesFigure 6: approach matching posts records reference setaddition providing standardized set values query posts, standardized values allow integration outside sources values standardizedcanonical values. instance, want integrate car classifieds safetyratings website, easily join sources across attribute values.manner, approaching annotation record linkage problem, create relationaldata unstructured ungrammatical data sources. However, aid extractionattributes easily represented reference sets, perform information extractionposts well.3. Extracting Data PostsAlthough record linkage step creates relational data posts,still attributes would like extract post easily representedreference sets, means record linkage step used attributes.Examples attributes dates prices. Although many attributesextracted using simple techniques, regular expressions, makeextraction annotation ever accurate using sophisticated information extraction.motivate idea, consider Ford car model called 500. used regularexpressions, might extract 500 price car, would case.However, try extract attributes, including model, wouldextract 500 model correctly. Furthermore, might want extract actualattributes post, are, extraction algorithm allows this.perform extraction, algorithm infuses information extraction extra knowledge, rather relying possibly inconsistent characteristics. garner extra559fiMichelson & Knoblockknowledge, approach exploits idea reference sets using attributesmatching reference set member basis identifying similar attributes post.Then, algorithm label extracted values post schemareference set, thus adding annotation based extracted values.broad sense, algorithm two parts. First label token possibleattribute label junk ignored. tokens post labeled,clean extracted labels. Figure 7 shows whole procedure graphically,detail, using second post Table 1. steps shown figuredescribed detail below.Figure 7: Extraction process attributesbegin extraction process, post broken tokens. Using first postTable 1 example, set tokens becomes, {93, civic, 5speed,...}.tokens scored attribute record reference setdeemed match.score tokens, extraction process builds vector scores, VIE . Like VRLvector matching step, VIE composed vectors represent similaritiestoken attributes reference set. However, compositionVIE slightly different VRL . contains comparison concatenationattributes, vectors compose VIE different composeVRL . Specifically, vectors form VIE called IE scores, similar560fiRelational Data Unstructured Data SourcesRL scores compose VRL , except contain token scores component, sinceIE scores uses one token post time.RL scores vector:RL scores(post, attribute)=<token scores(post, attribute),edit scores(post, attribute),scores(post, attribute)>becomes:IE scores(token, attribute)=<edit scores(token, attribute),scores(token, attribute)>main difference VIE VRL VIE contains unique vectorcontains user defined functions, regular expressions, capture attributeseasily represented reference sets, prices dates. attribute typesgenerally exhibit consistent characteristics allow extracted,usually infeasible represent reference sets. makes traditional extraction methodsgood choice attributes. vector called common scores typescharacteristics used extract attributes common enough usedextraction.Using first post Table 1, assume reference set match make Honda,model Civic year 1993. means matching tuple would {Honda,Civic, 1993}. match generates following VIE token civic post:VIE =<common scores(civic),IE scores(civic,Honda),IE scores(civic,Civic),IE scores(civic,1993)>generally, given token, VIE looks like:VIE =<common scores(token),IE scores(token, attribute1 ),IE scores(token, attribute2 )...,IE scores(token, attributen )>VIE passed structured SVM (Tsochantaridis, Joachims, Hofmann,& Altun, 2005; Tsochantaridis, Hofmann, Joachims, & Altun, 2004) trained giveattribute type label, make, model, price. Intuitively, similar attribute typessimilar VIE vectors. makes generally high scoresmake attribute reference set, small scores attributes. Further,structured SVMs able infer extraction labels collectively, helps decidingpossible token labels. makes use structured SVMs ideal machinelearning method task. Note since VIE member clusterwinner takes all, binary rescoring.Since many irrelevant tokens post annotated, SVMlearns VIE associate learned attribute type labeled561fiMichelson & Knoblockjunk, ignored. Without benefits reference set, recognizing junkdifficult characteristics text posts unreliable. example,extraction relies solely capitalization token location, junk phrase Great Dealmight annotated attribute. Many traditional extraction systems workdomain ungrammatical unstructured text, addresses bibliographies,assume token text must classified something, assumptioncannot made posts.Nonetheless, possible junk token receive incorrect class label.example, junk token enough matching letters, might labeled trim (sincetrims may single letter two). leads noisy tokens within wholeextracted trim attribute. Therefore, labeling tokens individually gives approximationdata extracted.extraction approach overcome problems generating noisy, labeled tokenscomparing whole extracted field analogue reference set attribute.tokens post processed, whole attributes built compared corresponding attributes reference set. allows removal tokens introducenoise extracted attribute.removal noisy tokens extracted attribute starts generating twobaseline scores extracted attribute reference set attribute. OneJaccard similarity, reflect token level similarity two attributes. However,since many misspellings such, edit-distance based similarity metric,Jaro-Winkler metric, also used. baselines demonstrate accurately systemextracted/classified tokens isolation.Using first post Table 1 ongoing example, assume phrase civic (ri)extracted model. might occur car model Civic Rx,instance. isolation, token (ri) could Rx model. Comparingextracted car model reference attribute Civic generates Jaccard similarity 0.5Jaro-Winkler score 0.83. shown top Figure 8.Next, cleaning method goes extracted attribute, removing one tokentime calculating new Jaccard Jaro-Winkler similarities. new scoreshigher baselines, token becomes removal candidate. tokensprocessed way, removal candidate highest scores removed,whole process repeated. scores derived using removed token becomenew baseline compare against. process ends tokensyield improved scores baselines.Shown Iteration 1 Figure 8, cleaning method finds (ri) removalcandidate since removing token extracted car model yields Jaccard score1.0 Jaro-Winkler score 1.0, higher baseline scores. Sincehighest scores trying token iteration, removedbaseline scores update. Then, since none remaining tokens provide improved scores(since none), process terminates, yielding accurate attribute value.shown Iteration 2 Figure 8. Note process would keep iterating,tokens removed improve scores baseline. pseudocodealgorithm shown Figure 9.562fiRelational Data Unstructured Data SourcesFigure 8: Improving extraction accuracy reference set attributesNote, however, limit machine learning component extractionalgorithm SVMs. Instead, claim cases, reference sets aid extractiongeneral, test this, architecture replace SVM componentmethods. example, extraction experiments replace SVM extractorConditional Random Field (CRF) (Lafferty, McCallum, & Pereira, 2001) extractoruses VIE features.Therefore, whole extraction process takes token text, creates VIEpasses machine-learning extractor generates label token.field cleaned extracted attribute saved.4. ResultsPhoebus system built experimentally validate approach building relationaldata unstructured ungrammatical data sources. Specifically, Phoebus teststechniques accuracy record linkage extraction, incorporatesBSL algorithm learning using blocking schemes. experimental data, comesthree domains posts: hotels, comic books, cars.data hotel domain contains attributes hotel name, hotel area, starrating, price dates, extracted test extraction algorithm. datacomes Bidding Travel website9 forum users share successfulbids Priceline items airline tickets hotel rates. experimental datalimited postings hotel rates Sacramento, San Diego Pittsburgh,compose data set 1125 posts, 1028 posts match referenceset. reference set comes Bidding Travel hotel guides, special9. www.biddingfortravel.com563fiMichelson & KnoblockAlgorithm 3.1: CleanAttribute(E, R)comment: Clean extracted attribute E using reference set attribute RRemovalCandidates C nullJaroW inklerBaseline JaroWinkler(E, R)JaccardBaseline Jaccard(E, R)token EX RemoveToken(t, E)JaroW inklerXt JaroWinkler(X , R)Xt Jaccard(X , R)JaccardJaroWinklerXt >JaroW inklerBaselineJaccard >JaccardXtBaselinenC C(C = nullreturn (E)(E RemoveMaxCandidate(C,E)elseCleanAttribute(E, R)Figure 9: Algorithm clean extracted attributeposts listing hotels ever posted given area. special posts providehotel names, hotel areas star ratings, reference set attributes. Therefore,3 attributes standardized values used, allowing us treatposts relational data set. reference set contains 132 records.experimental data comic domain comes posts items saleeBay. generate data set, eBay searched keywords Incredible HulkFantastic Four comic books section website. (This returned itemscomics, tshirts sets comics limited searchedfor, makes problem difficult.) returned records contain attributescomic title, issue number, price, publisher, publication year description,extracted. (Note: description word description commonly associatedcomic book, 1st appearance Rhino.) total number posts dataset 776, 697 matches. comic domain reference set uses dataComics Price Guide10 , lists Incredible Hulk Fantastic Four comics.reference set attributes title, issue number, description, publisher contains918 records.cars data consists posts made Craigslist regarding cars sale. datasetconsists classifieds cars Los Angeles, San Francisco, Boston, New York, New10. http://www.comicspriceguide.com/564fiRelational Data Unstructured Data SourcesJersey Chicago. total 2,568 posts data set, postcontains make, model, year, trim price. reference set Cars domain comesEdmunds11 car buying guide. data set extracted make, model,year trim cars 1990 2005, resulting 20,076 records. 15,338matches posts Craigslist cars Edmunds.Unlike hotels comics domains, strict 1-1 relationship postreference set enforced cars domain. described previously, Phoebus relaxed 1-1 relationship form 1-n relationship posts referenceset. Sometimes records contain enough attributes discriminate single bestreference member. instance, posts contain model year might matchcouple reference set records would differ trim attribute,make, model, year. Yet, still use make, model year accuratelyextraction. So, case, mentioned previously, pick one matches. way,exploit attributes reference set, since confidencethose.experiments, posts domain split two folds, one trainingone testing. usually called two-fold cross validation. However, many cases twofold cross validation results using 50% data training 50% testing.believe much data label, especially data sets become large,experiments instead focus using less training data. One set experiments uses 30%posts training tests remaining 70%, second set experimentsuses 10% posts train, testing remaining 90%. believe trainingsmall amounts data, 10%, important empirical procedure since realworld data sets large labeling 50% large data sets time consumingunrealistic. fact, size Cars domain prevented us using 30% datatraining, since machine learning algorithms could scale number trainingtuples would generate. Cars domain run experiments training10% data. experiments performed 10 times, average results10 trials reported.4.1 Record Linkage Resultssubsection report record linkage results, broken separate discussionsblocking results matching results.4.1.1 Blocking Resultsorder BSL algorithm learn blocking scheme, must provided methodsuse compare attributes. domains experiments use two commonmethods. first, call token, compares matching tokenattributes. second method, ngram3, considers matching 3-gramsattributes.important note comparison BSL blocking methods,Canopies method (McCallum, Nigam, & Ungar, 2000) Bigram indexing (Baxter,Christen, & Churches, 2003), slightly misaligned algorithms solve different11. www.edmunds.com565fiMichelson & Knoblockproblems. Methods Bigram indexing techniques make processblocking pass attribute efficient. goal BSL, however, selectattribute combinations used blocking whole, trying different attributemethod pairs. Nonetheless, contend important select right attributecombinations, even using simple methods, use sophisticated methods,without insight attributes might useful. test hypothesis, compareBSL using token 3-gram methods Bigram indexing attributes.equivalent forming disjunction attributes using Bigram indexingmethod. chose Bigram indexing particular designed perform fuzzyblocking seems necessary case noisy post data. stated previously (Baxteret al., 2003), use threshold 0.3 Bigram indexing, since works best.also compare BSL running disjunction attributes using simple token methodonly. results, call blocking rule Disjunction. disjunction mirrorsidea picking simplest possible blocking method: namely using attributessimple method.stated previously, two goals blocking quantified Reduction Ratio(RR) Pairs Completeness (PC). Table 5 shows values alsomany candidates generated average entire test set, comparing threedifferent approaches. Table 5 also shows long took method learn rulerun rule. Lastly, column Time match shows long classifier needsrun given number candidates generated blocking scheme.Table 6 shows example blocking schemes algorithm generated.comparison attributes BSL selected attributes picked manually differentdomains data structured reader pointed previous worktopic (Michelson & Knoblock, 2006).results Table 5 validate idea important pick correctattributes block (using simple methods) use sophisticated methods withoutattention attributes. Comparing BSL rule Bigram results, combinationPC RR always better using BSL. Note although Cars domain Bigramtook significantly less time classifier due large RR,PC 4%. case, Bigrams even covering 5% true matches.Further, BSL results better using simplest method possible (the Disjuction), especially cases many records test upon. numberrecords scales up, becomes increasingly important gain good RR, maintaininggood PC value well. savings dramatically demonstrated Cars domain,BSL outperformed Disjunction PC RR.One surprising aspect results prevalent token method withindomains. expect ngram method would used almost exclusively sincemany spelling mistakes within posts. However, case. hypothesizelearning algorithm uses token methods occur regularityacross posts common ngrams would since spelling mistakes might vary quitedifferently across posts. suggests might regularity, termslearn data, across posts initially surmised.Another interesting result poor reduction ratio Comic domain. happensrules contain disjunct finds common token within comic566fiRelational Data Unstructured Data SourcesHotels (30%)BSLDisjunctionBigramsHotels (10%)BSLDisjunctionBigramsComics (30%)BSLDisjunctionBigramsComics (10%)BSLDisjunctionBigramsCars (10%)BSLDisjunctionBigramsRRPC# CandsTime Learn (s)Time Run (s)Time match (s)81.5667.0261.3599.7999.8272.7719,15334,26240,15169.250024.0512.491.260.93109.00127.7484.4766.9160.7199.0799.8290.3920,74244,20252,49237.670031.8715.6761.5765.99140.62167.0042.9737.3936.7299.75100.0069.20284,283312,078315,45385.590036.6645.77102.23834.94916.57926.4842.9737.3336.7599.74100.0088.41365,454401,541405,28334.260035.6552.183131.341,073.341,179.321,190.3188.4887.9297.1192.2389.904.315,343,4245,603,1461,805,275465.8500805.36343.22996.4525,114.0926.334.798,484.79Table 5: Blocking results using BSL algorithm (amount data used training shownparentheses).Hotels Domain (30%)({hotel area,token} {hotel name,token} {star rating, token}) ({hotel name, ngram3})Hotels Domain (10%)({hotel area,token} {hotel name,token}) ({hotel name,ngram3})Comic Domain (30%)({title, token})Comic Domain (10%)({title, token}) ({issue number,token} {publisher,token} {title,ngram3})Cars Domain (10%)({make,token}) ({model,ngram3}) ({year,token} {make,ngram3})Table 6: example blocking schemes learned domains.567fiMichelson & Knoblocktitle. rule produces poor reduction ratio value attributeacross almost reference set records. say,unique values BSL algorithm use blocking, reduction ratio small.domain, two values comic title attribute, Fantastic FourIncredible Hulk. makes sense blocking done using title attribute only,reduction half, since blocking value Fantastic Four gets ridIncredible Hulk comics. points interesting limitation BSL algorithm.many distinct values different attribute method pairs BSLuse learn from, lack values cripples performance reductionratio. Intuitively though, makes sense, since hard distinguish good candidatematches bad candidate matches share attribute values.Another result worth mentioning Hotels domain get lower RRPC use less training data. happens BSL algorithmruns examples cover, last examples introduce newdisjunct produces lot candidates, covering true positives,would cause RR decrease, keeping PC high rate.fact happens case. One way curb behavior would setsort stopping threshold BSL, said, maximizing PCimportant thing, choose this. want BSL cover many true positivescan, even means losing bit reduction.fact, next test notion explicitly. set threshold SCA95% training examples covered, algorithm stops returns learnedblocking scheme. helps avoid situation BSL learns general conjunction, solely cover last remaining training examples. happens, BSLmight end lowering RR, expense covering last training examples,rule learned cover last examples overly general returns manycandidate matches.DomainHotels DomainThresh (30%)95% Thresh (30%)Comic DomainThresh (30%)95% Thresh (30%)Cars DomainThresh (10%)95% Thresh (10%)Record LinkageF-MeasureRRPC90.6390.6381.5687.6399.7997.6691.3091.4742.9742.9799.7599.6977.0467.1488.4892.6792.2383.95Table 7: comparison BSL covering training examples, covering 95%training examples568fiRelational Data Unstructured Data SourcesTable 7 shows use threshold Hotels Cars domain seestatistically significant drop Pairs Completeness statistically significant increaseReduction Ratio.12 expected behavior since threshold causes BSL kickSCA cover last training examples, turn allows BSLretain rule high RR, lower PC. However, look record linkageresults, see threshold fact large effect.13 Althoughstatistically significant difference F-measure record linkage Hotels domain,difference Cars domain dramatic. use threshold, candidatesdiscovered rule generated using threshold effect 10% finalF-measure match results.14 Therefore, since F-measure results differ much,conclude worthwhile maximize PC learning rules BSL, evenRR may decrease. say, even presence noise, turn may leadoverly generic blocking schemes, BSL try maximize true matches covers,avoiding even difficult cases cover may affect matching results.see Table 7, especially true Cars domain matching muchdifficult Hotels domain.Interestingly, Comic domain see statistically significant differenceRR PC. across trials almost always learn rulewhether use threshold not, rule covers enough training examplesthreshold hit. Further, statistically significant change F-measurerecord linkage results domain. expected since BSL would generatecandidate matches, whether uses threshold not, since cases almost alwayslearns blocking rules.results using BSL encouraging show algorithm also worksblocking matching unstructured ungrammatical text relational datasource. means algorithm works special case too, casetraditional record linkage matching one structured source another.means overall algorithm semantic annotation much scalableusing fewer candidate matches previous work (Michelson & Knoblock, 2005).4.1.2 Matching ResultsSince alignment approach hinges leveraging reference sets, becomes necessaryshow matching step performs well. measure accuracy, experiments employusual record linkage statistics:P recision =Recall =#CorrectM atches#T otalM atchesM ade#CorrectM atches#P ossibleM atches12. Bold means statistically significant using two-tailed t-test set 0.0513. Please see subsection 4.1.2 description record linkage experiments results.14. Much difference attributed non-threshold version algorithm learning finalpredicate includes make attribute itself, version threshold learn.Since make attribute value covers many records, generates many candidates resultsincreasing PC reducing RR.569fiMichelson & KnoblockF easure =2 P recision RecallP recison + Recallrecord linkage approach article compared WHIRL (Cohen, 2000).WHIRL performs record linkage performing soft-joins using vector-based cosine similarities attributes. record linkage systems require decomposed attributesmatching, case posts. WHIRL serves benchmarkrequirement. mirror alignment task Phoebus, experimentsupplies WHIRL two tables: test set posts (either 70% 90% posts)reference set attributes concatenated approximate record level match.concatenation also used matching individual attribute,obvious combine matching attributes construct whole matching referenceset member.perform record linkage, WHIRL soft-joins across tables, produceslist matches, ordered descending similarity score. post matchesjoin, reference set member(s) highest similarity score(s) called match.Cars domain matches 1-N, means 1 match referenceset exploited later information extraction step. mirror idea, numberpossible matches 1-N domain counted number posts matchreference set, rather reference set members match. Also,means add single match total number correct matches givenpost, rather correct matches, since one matters. doneWHIRL Phoebus, accurately reflects well algorithm would performprocessing step information extraction step.record linkage results Phoebus WHIRL shown Table 8. Noteamount training data domain shown parentheses. resultsstatistically significant using two-tailed paired t-test =0.05, exceptprecision WHIRL Phoebus Cars domain, precisionPhoebus trained 10% 30% training data Comic domain.Phoebus outperforms WHIRL uses many similarity types distinguishmatches. Also, since Phoebus uses record level attribute level similarities,able distinguish records differ discriminative attributes.especially apparent Cars domain. First, results indicate difficultymatching car posts large reference set. largest experimental domain yetused problem, encouraging well approach outperforms baseline. also interesting results suggest techniques equally accurateterms precision (in fact, statistically significant differencesense) Phoebus able retrieve many relevant matches. meansPhoebus capture rich features predict matches WHIRLs cosine similarity alone. expect behavior Phoebus notion field tokenlevel similarity, using many different similarity measures. justifies use manysimilarity types field record level information, since goal find manymatches can.also encouraging using 10% data labeling, Phoebus ableperform almost well using 30% data training. Since amount dataWeb vast, label 10% data get comparative results preferable570fiRelational Data Unstructured Data SourcesHotelPhoebus (30%)Phoebus (10%)WHIRLComicPhoebus (30%)Phoebus (10%)WHIRLCarsPhoebus (10%)WHIRLPrecisionRecallF-measure87.7087.8583.5393.7892.4683.6190.6390.0983.1387.4985.3573.8995.4693.1881.6391.3089.0977.5769.9870.4385.6863.3677.0466.71Table 8: Record linkage resultscost labeling data great. Especially since clean annotation, hencerelational data, comes correctly matching posts reference set,label much data important want technique widely applicable.fact, faced practical issue Cars domain unableuse 30% training since machine learning method would scale numbercandidates generated much training data. So, fact report goodresults 10% training data allows us extend work much larger Carsdomain.method performs well outperforms WHIRL, results above,clear whether use many string metrics, inclusion attributesconcatenation SVM provides advantage. test advantagespiece, ran several experiments isolating ideas.First, ran Phoebus matching concatenation attributesreference set, rather concatenation attributes individually. Earlier,stated use concatenation mirror idea record level similarity alsouse attribute mirror field level similarity. hypothesis cases,post match different reference set records record level score (usingconcatenation), matching different attributes. removingindividual attributes leaving concatenation matching, testconcatenation influences matching isolation. Table 9 shows resultsdifferent domains.Cars Comic domains see improvement F-measure, indicatingusing attributes concatenation much better matching usingconcatenation alone. supports notion also need method capturesignificance matching individual attributes since attributes better indicatorsmatching others. also interesting note domains, WHIRLbetter job machine learning using concatenation, even though WHIRL571fiMichelson & KnoblockHotelsPhoebus (30%)ConcatenationWHIRLComicPhoebus (30%)ConcatenationWHIRLCarsPhoebus (10%)ConcatenationWHIRLPrecisionRecallF-Measure87.7088.4983.6193.7893.1983.5390.6390.7883.1387.4961.8173.8995.4646.5581.6391.3051.3177.5769.9847.9470.4385.6858.7363.3677.0452.7966.71Table 9: Matching using concatenationalso uses concatenation attributes. WHIRL uses informationretrieval-style matching find best match, machine learning technique trieslearn characteristics best match. Clearly, difficult learncharacteristics are.Hotels domain, find statistically significant difference F-measureusing concatenation alone. means concatenation sufficient determinematches, need individual fields play role. specifically,hotel name area seem important attributes matchingincluding part concatenation, concatenation still distinguishable enoughrecords determine matches. Since two three domains seehuge improvement, never lose F-measure, using concatenationindividual attributes valid matching. Also, since two domains concatenationalone worse WHIRL, conclude part reason Phoebus outperformWHIRL use individual attributes matching.next experiment tests important include string metricsfeature vector matching. test idea, compare using metrics usingone, Jensen-Shannon distance. choose Jensen-Shannon distanceoutperformed TF/IDF even soft TF/IDF (one accounts fuzzy tokenmatches) task selecting right reference sets given set posts (Michelson& Knoblock, 2007). results shown Table 10.Table 10 shows, using metrics yielded statistically significant, large improvement F-measure Comic Cars domains. meansstring metrics, edit distances, capturing similarities JensenShannon distance alone not. Interestingly, domains, using PhoebusJensen-Shannon distance dominate WHIRLs performance. Therefore,results Table 10 Table 9 demonstrate Phoebus benefits combination572fiRelational Data Unstructured Data SourcesHotelsPhoebus (30%)Jensen-ShannonWHIRLComicPhoebus (30%)Jensen-ShannonWHIRLCarsPhoebus (10%)Jensen-ShannonWHIRLPrecisionRecallF-Measure87.7089.6583.6193.7892.2883.5390.6390.9483.1387.4965.3673.8995.4669.9681.6391.3067.5877.5769.9872.8770.4385.6859.4363.3677.0467.9466.71Table 10: Using string metrics versus using Jensen-Shannon distancemany, varied similarity metrics along use individual attributes field levelsimilarities, aspects contribute Phoebus outperforming WHIRL.case Hotels data, statistically significant differencematching results, case metrics provide relevant informationmatching. Therefore, matches missed Jensen-Shannon method alsomissed include metrics. Hence, either missed matchesdifficult discover, string metric method yet capturesimilarity. example, post token DT reference set recordmatch hotel area Downtown, abbreviation metric could capturerelationship. However, Phoebus include abbreviation similarity measure.Since none techniques isolation consistently outperforms WHIRL, concludePhoebus outperforms WHIRL combines multiple string metrics, usesindividual attributes concatenation, and, stated Section 2.2, SVM classifierwell suited record linkage task. results also justify inclusion manymetrics individual attributes, along use SVM classifier.last matching experiment justifies binary rescoring mechanism. Table 11 showsresults performing binary rescoring record linkage versus performingbinary recoring. hypothesize earlier paper binary rescoring allowclassifier accurately make match decisions rescoring separatesbest candidate much possible. Table 11 shows case, acrossdomains perform binary rescoring gain statistically significant amountF-measure. shows record linkage easily able identifytrue matches possible candidates difference record linkagealgorithm use binary rescoring.573fiMichelson & KnoblockHotelsPhoebus (30%)Binary RescoringPhoebus (10%)Binary RescoringComicPhoebus (30%)Binary RescoringPhoebus (10%)Binary RescoringCarsPhoebus (10%)Binary RescoringPrecisionRecallF-Measure87.7075.4487.8573.4993.7881.8292.4678.4090.6378.5090.0975.8687.4984.8785.3581.5295.4689.9193.1888.2691.3087.3189.0984.7569.9839.7885.6848.7777.0443.82Table 11: Record linkage results without binary rescoring4.2 Extraction Resultssection presents results experimentally validate approach extractingactual attributes embedded within post. also compare approach twoinformation extraction methods rely structure grammar posts.First, experiments compare Phoebus baseline Conditional Random Field(CRF) (Lafferty et al., 2001) extractor. Conditional Random Field probabilisticmodel label segment data. labeling tasks, Part-of-Speech tagging, CRFs outperform Hidden Markov Models Maximum-Entropy Markov Models.Therefore, representing state-of-the-art probabilistic graphical model, presentstrong comparison approach extraction. CRFs also used effectivelyinformation extraction. instance, CRFs used combine information extraction coreference resolution good results (Wellner, McCallum, Peng, & Hay, 2004).experiments use Simple Tagger implementation CRFs MALLET(McCallum, 2002) suite text processing tools.Further, stated Section 3 Extraction, also created version Phoebususes CRFs, call PhoebusCRF. PhoebusCRF uses extraction features(VIE ) Phoebus using SVM, common score regular expressionsstring similarity metrics. include PhoebusCRF show extraction generalbenefit reference set matching.Second, experiments compare Phoebus Natural Language Processing (NLP) basedextraction techniques. Since posts ungrammatical unreliable lexical characteristics, NLP based systems expected well type data.Amilcare system (Ciravegna, 2001), uses shallow NLP extraction,shown outperform symbolic systems extraction tasks, use Amilcaresystem compare against. Since Amilcare exploit gazetteers extra574fiRelational Data Unstructured Data Sourcesinformation, experiments Amilcare receives reference data gazetteer aidextraction. Simple Tagger Amilcare used default settings.Lastly, compare Phoebus trained using 30% data training Phoebustrained using 10% data. (We PhoebusCRF well.) experimentalresults, amount training data put parentheses.One component extraction vector VIE vector common scores, includesuser defined functions, regular expressions. Since domain specificfunctions used algorithm, common scores domain must specified.Hotels domain, common scores includes functions matchPriceRegexmatchDateRegex. functions gives positive score token matches pricedate regular expression, 0 otherwise. Comic domain, common scores containsfunctions matchPriceRegex matchYearRegex, also give positive scorestoken matches regular expression. Cars domain, common scores uses functionmatchPriceRegex (since year attribute reference set, use commonscore capture form).cars data set, posts labeled training testingextraction. domain, labeled 702 posts extraction, usetraining testing extraction algorithm. Note, however, Phoebus performextraction posts, able report results those. fact,running demo Phoebus, Cars domain live.15extraction results presented using Precision, Recall F-Measure. Noteextraction results field level results. means extraction countedcorrect tokens compromise field post correctly labeled.Although much stricter rubric correctness, accurately models usefulextraction system would be. Tables 12, 13 14 show results correctly labelingtokens within posts correct attribute label Hotel, Comic Carsdomains, respectively. Attributes italics attributes exist reference set.column Freq shows average number fields test set associatedlabel. Also, observe * means results highest Phoebus score (PhoebusPhoebusCRF) highest baseline (Amilcare Simple Tagger CRF) F-Measurestatistically significant using two-tailed paired t-test =0.05.Phoebus PhoebusCRF outperform systems almost attributes (1316), shown Table 15. fact, one attribute baseline systembest: using Amilcare extract Date attribute Hotels domain.attribute, Phoebus PhoebusCRF use common-score regular-expressionmain identifying feature. Since regular expression user supplied, proposebetter regular expression could make Phoebus/PhoebusCRF extract dates evenaccurately, overcoming baseline. Since systems perform well using referenceset data aid extraction, results show using reference sets greatly aidextraction. especially evident compare PhoebusCRF Simple TaggerCRF, since difference two extraction methods reference set attributesimilarity scores common scores.15. http://www.isi.edu/integration/Phoebus/demos.html demo uses extraction model trained702 labeled extraction examples, running live months writingarticle.575fiMichelson & KnoblockAreaDateNamePriceStarPhoebus (30%)Phoebus (10%)PhoebusCRF (30%)PhoebusCRF (10%)Simple Tagger CRF (30%)Amilcare (30%)Phoebus (30%)Phoebus (10%)PhoebusCRF (30%)PhoebusCRF (10%)Simple Tagger CRF (30%)Amilcare (30%)Phoebus (30%)Phoebus (10%)PhoebusCRF (30%)PhoebusCRF (10%)Simple Tagger CRF (30%)Amilcare (30%)Phoebus (30%)Phoebus (10%)PhoebusCRF(30%)PhoebusCRF (10%)Simple Tagger CRF (30%)Amilcare (30%)Phoebus (30%)Phoebus (10%)PhoebusCRF (30%)PhoebusCRF (10%)Simple Tagger CRF (30%)Amilcare (30%)HotelRecall83.7377.8085.1380.7178.6264.7885.4182.1387.2084.3963.6086.1877.2775.5985.7081.4674.4358.9693.0693.1292.5690.3471.6888.0497.3996.9496.8396.1797.1695.58Precision84.7683.5886.9383.3879.3871.5987.0283.0687.1184.4863.2594.1075.1874.2585.0781.6984.8667.4498.3898.4694.9092.6073.4591.1097.0196.9098.0696.7496.5597.35F-Measure84.2380.5286.0282.0179.0068.0186.2182.5987.1584.4363.4289.9776.2174.9285.3881.5779.2962.9195.6595.7293.7191.4672.5589.5497.2096.9297.4496.4596.8596.46Frequency~580~700~750~720~730Table 12: Field level extraction results: Hotels domain576fiRelational Data Unstructured Data SourcesDescript.IssuePricePublisherTitleYearPhoebus (30%)Phoebus (10%)PhoebusCRF (30%)PhoebusCRF (10%)Simple Tagger CRF (30%)Amilcare (30%)Phoebus (30%)Phoebus (10%)PhoebusCRF (30%)PhoebusCRF (10%)Simple Tagger CRF (30%)Amilcare (30%)Phoebus (30%)Phoebus (10%)PhoebusCRF (30%)PhoebusCRF (10%)Simple Tagger CRF (30%)Amilcare (30%)Phoebus (30%)Phoebus (10%)PhoebusCRF (30%)PhoebusCRF (10%)Simple Tagger CRF (30%)Amilcare (30%)Phoebus (30%)Phoebus (10%)PhoebusCRF (30%)PhoebusCRF (10%)Simple Tagger CRF (30%)Amilcare (30%)Phoebus (30%)Phoebus (10%)PhoebusCRF (30%)PhoebusCRF (10%)Simple Tagger CRF (30%)Amilcare (30%)ComicRecall32.4330.1626.0215.4532.308.0083.3980.9087.7783.0178.3177.6668.0939.8451.0629.0944.2441.21100.0099.8577.9153.2278.1363.7589.3489.3792.9390.6493.5789.8878.4477.5076.2454.6339.9377.05Precision30.7127.1533.0326.8334.7552.5583.6582.1788.7084.6877.8189.1190.0060.0085.3455.4084.4466.6785.3883.8988.3087.2988.5290.4889.3489.3793.7092.1392.7995.6597.6997.3593.4685.0772.8985.67F-Measure31.5128.5228.9518.5433.43*13.7883.5281.5288.2383.8478.0582.9877.39*46.9161.1635.7155.7750.9392.0991.1882.5064.2682.7274.7589.3489.3793.31*91.3793.1892.6586.9986.2883.8066.1451.5481.04Frequency~90~510~15~60~540~100Table 13: Field level extraction results: Comic domain.577fiMichelson & KnoblockMakeModelPriceTrimYearPhoebus (10%)PhoebusCRF (10%)Simple Tagger CRF (10%)Amilcare (10%)Phoebus (10%)PhoebusCRF (10%)Simple Tagger CRF (10%)Amilcare (10%)Phoebus (10%)PhoebusCRF (10%)Simple Tagger CRF (10%)Amilcare (10%)Phoebus (10%)PhoebusCRF (10%)Simple Tagger CRF (10%)Amilcare (10%)Phoebus (10%)PhoebusCRF (10%)Simple Tagger CRF (10%)Amilcare (10%)CarsRecall98.2190.7385.6897.5892.6184.5878.7678.4497.1793.5983.6690.0663.1155.6155.9427.2188.4885.5491.1286.32Precision99.9396.7195.6991.7696.6794.1091.2184.3195.9192.5998.1691.2770.1564.9566.4953.9998.2396.4476.7891.92F-Measure99.0693.3690.3994.5794.5988.7984.5281.2496.5393.0990.3390.2866.4359.2860.5735.9493.0890.5983.3188.97Frequency~580~620~580~375~600Table 14: Field level extraction results: Cars domain.DomainHotelComicCarsPhoebus1258Num. Max. F-MeasuresPhoebusCRF Amilcare Simple Tagger310100000410Total Attributes56516Table 15: Summary results extraction showing number times systemstatistically significant highest F-Measure attribute.578fiRelational Data Unstructured Data SourcesPhoebus performs especially well Cars domain, best systemattributes. One interesting thing note result recordlinkage results spectacular Cars domain, good enough yieldhigh extraction results. times system picking bestmatch reference set, still picking one close enoughreference set attributes useful extraction. trim extractionresults lowest, often attribute determines matchnon-match. record linkage step likely selects car close, differs trim,match incorrect trim likely extracted correctly,rest attributes extracted using reference set member.couple interesting notes come results. One intriguingaspects results allow us estimate level structure differentattributes within domain. Since CRFs rely structure tokens withinpost structured SVM method, hypothesize domainsstructure, PhoebusCRF perform best domains least structure,Phoebus perform best. Table 15 shows case. PhoebusCRF dominatesHotels domain, where, example, many posts structure star ratingcomes hotel name. using structure allow extractor gethotel name accurately using information. Therefore seeoverall structure within Hotels domain PhoebusCRF methodperforms best, Phoebus. Contrast Cars domain, highlyunstructured, Phoebus performs best across attributes. domainmany missing tokens order attributes varied. Comic domainvaried attributes exhibit structure not, Table15 shows, cases Phoebus PhoebusCRF dominates. However, althoughHotels data exhibits structure, important aspect research usingPhoebus allows one perform extraction without assuming structure data.Also, result worth noting price attribute Comic domain bitmisleading. fact, none systems statistically significant respectprices extract F-Measuressystems.Another aspect came light statistical significance generalizationalgorithm. Hotels Comic domains, able use 30% 10%data training, many cases statistically significant differenceF-Measures extracted attributes using Phoebus. Hotels domainname, area date statistically significant F-Measures training 30%10% data, Comic domain difference F-Measureissue description attributes significant (though description borderline).means 11 attributes domains, roughly half insignificant.Therefore little difference extraction whether use 10% data training30%, extraction algorithm generalizes well. important since labelingdata extraction time consuming expensive.One interesting result note except comic price (which insignificantsystems) hotel date (which close), Phoebus, using either 10% 30%training data, outperformed systems attributes included579fiMichelson & Knoblockreference set. lends credibility claim earlier section trainingsystem extract attributes, even reference set,accurately extract attributes reference set training systemidentify something not.overall performance Phoebus validates approach semantic annotation.infusing information extraction outside knowledge reference sets, Phoebusable perform well across three different domains, representative different typesource posts: auction sites, Internet classifieds forum/bulletin boards.5. Discussiongoal research produce relational data unstructured ungrammaticaldata sources accurately queried integrated sources.representing attributes embedded within post standardized valuesreference set, support structural queries integration. instance,perform aggregate queries treat data source relational databasenow. Furthermore, standardized values performing joins across data sources,key integration multiple sources. standardized values also aid casespost actually contain attribute. instance, Table 1, twolistings include make Honda. However, matched reference set,contain standardized value attribute used queryingintegrating posts. especially powerful since posts never explicitly statedattribute values. reference set attributes also provide solution casesextraction extremely difficult. example, none systems extracteddescription attribute Comic domain well. However, one instead considersdescription attribute reference set, quantified record linkageresults Comic domain, yields improvement 50% F-Measureidentifying description post.may seem using reference set attributes annotation enough sincevalues already cleaned, extraction unnecessary. However, case.one thing, one may want see actual values entered different attributes.instance, user might want discover common spelling mistake abbreviationattribute. Also, cases extraction results outperform recordlinkage results. happens even post matched incorrect memberreference set, incorrect member likely close correct match,used correctly extract much information. strong example this,consider Cars domain. F-measure record linkage results goodextraction results domain. means matches chosenprobably incorrect differ correct match something small.example, true match could trim 2 Door incorrectly chosenmatch might trim 4 Door, would still enough information,rest trim tokens, year, make model correctly extractdifferent attributes post itself. performing extraction valuespost itself, overcome mistakes record linkage step stillexploit information incorrectly chosen reference set member.580fiRelational Data Unstructured Data SourcesExtraction attributes also helps system classify (and ignore) junktokens. Labeling something junk much descriptive labeled junkmany possible class labels could share lexical characteristics. helps improveextraction results items reference set, prices dates.topic reference sets, important note algorithm tiedsingle reference set. algorithm extends include multiple reference sets iteratingprocess reference set used.Consider following two cases. First, suppose user wants extract conferencenames cities individual lists each. approach confined usingone reference set, would require constructing reference set contains power setcities crossed conference names. approach would scale many attributesdistinct sources. However, lists used two reference sets, oneattribute, algorithm run conference name data,reference set cities. iterative exploitation reference sets allows n referenceset attributes added without combinatorial explosion.next interesting case post contains one attribute.example, user needs extract two cities post. one reference set used,includes cross product cities. However, using single reference set citynames done slightly modifying algorithm. new algorithm makes firstpass city reference set. pass, record linkage match eitherone cities matches best, tie them. case tie, choosefirst match. Using reference city, system extract city post,remove post. system simply runs process again,catch second city, using same, single reference set. could repeated manytimes needed.One issue arises reference sets discrepancy users knowledgedomain experts generally create reference sets. Cars domain,instance, users interchangeably use attribute values hatchback, liftback,wagon. reference set never includes term liftback suggests synonymhatchback used common speech, Edmunds automobile jargon. termwagon used Edmunds, used cars users describehatchbacks. implies slight difference meaning two, accordingreference set authors.Two issues arise discrepancies. first users interchanging wordscause problems extraction record linkage,overcome incorporating sort thesaurus algorithm. record linkage,thesaurus could expand certain attribute values used matching, example includinghatchback liftback reference set attribute includes term wagon.However, subtle issues here. mostly case hatchbackcalled wagon happen wagon called hatchback. frequencyreplacement must taken consideration errant matches created.automate line future research. issue arises trustingcorrectness Edmunds source. assume Edmunds right define one carwagon different meaning classifying hatchback. fact,581fiMichelson & KnoblockEdmunds classifies Mazda Protege5 wagon, Kelly Blue Book16 classifieshatchback. seems invalidate idea wagon different meaninghatchback. appear simple synonyms, would remain unknownwithout outside knowledge Kelly Blue Book. generally, one assumesreference set correct set standardized values, absolute truth.meaningful reference sets constructed agreed-uponontologies Semantic Web. instance, reference set derived ontologycars created biggest automotive businesses alleviate manyissues meaning, thesaurus scheme could work discrepancies introducedusers, rather reference sets.6. Related Workresearch driven principal cost annotating documentsSemantic Web free, is, automatic invisible users (Hendler, 2001).Many researchers followed path, attempting automatically mark documentsSemantic Web, proposed (Cimiano, Handschuh, & Staab, 2004; Dingli,Ciravegna, & Wilks, 2003; Handschuh, Staab, & Ciravegna, 2002; Vargas-Vera, Motta,Domingue, Lanzoni, Stutt, & Ciravegna, 2002). However, systems rely lexicalinformation, part-of-speech tagging shallow Natural Language Processingextraction/annotation (e.g., Amilcare, Ciravegna, 2001). optiondata ungrammatical, like post data. similar vein, systemsADEL (Lerman, Gazen, Minton, & Knoblock, 2004) rely structure identifyannotate records Web pages. Again, failure posts exhibit structuremakes approach inappropriate. So, fair amount work automaticlabeling, little emphasis techniques could label text unstructuredungrammatical.Although idea record linkage new (Fellegi & Sunter, 1969) well studiedeven (Bilenko & Mooney, 2003) current research focuses matching one setrecords another set records based decomposed attributes. little workmatching data sets one record single string composed data setsattributes match on, case posts reference sets. WHIRL system(Cohen, 2000) allows record linkage without decomposed attributes, shownSection 4.1 Phoebus outperforms WHIRL, since WHIRL relies solely vector-basedcosine similarity attributes, Phoebus exploits larger set featuresrepresent field record level similarity. note interest EROCS system(Chakaravarthy, Gupta, Roy, & Mohania, 2006) authors tackle problemlinking full text documents relational databases. technique involves filteringnon-nouns text, finding matches database.intriguing approach; interesting future work would involve performing similar filteringlarger documents applying Phoebus algorithm match remaining nounsreference sets.Using reference sets attributes normalized values similar idea datacleaning. However, data cleaning algorithms assume tuple-to-tuple transformations16. www.kbb.com582fiRelational Data Unstructured Data Sources(Lee et al., 1999; Chaudhuri et al., 2003). is, function maps attributesone tuple attributes another. approach would work ungrammaticalunstructured data, attributes embedded within post, mapsset attributes reference set.Although work describes technique information extraction, many methods,Conditional Random Fields (CRF), assume least structure extractedattributes extraction. extraction experiments show, Phoebus outperforms methods, Simple Tagger implementation Conditional RandomFields (McCallum, 2002). IE approaches, Datamold (Borkar, Deshmukh, &Sarawagi, 2001) CRAM (Agichtein & Ganti, 2004), segment whole records (like bibliographies) attributes, little structural assumption. fact, CRAM even usesreference sets aid extraction. However, systems require every tokenrecord receive label, possible posts filled irrelevant, junktokens. Along lines CRAM Datamold, work Bellare McCallum (2007)uses reference set train CRF extract data, similar PhoebusCRFimplementation. However, two differences PhoebusCRF work(Bellare & McCallum, 2007). First, work Bellare McCallum (2007) mentionsreference set records matched using simple heuristics, uncleardone. work, matching done explicitly accurately record linkage. Second, work uses records reference set label tokens trainingextraction module, PhoebusCRF uses actual values matching referenceset record produce useful features extraction annotation.Another IE approach similar performs named entity recognition using SemiCRFs dictionary component (Cohen & Sarawagi, 2004), functions likereference set. However, work dictionaries defined lists single attributeentities, finding entity dictionary look-up task. reference setsrelational data, finding match becomes record linkage task. Further, workSemi-CRFs (Cohen & Sarawagi, 2004) focuses task labeling segments tokensuniform label, especially useful named entity recognition. caseposts, however, Phoebus needs relax restriction casessegments interrupted, case hotel name area middlehotel name segment. So, unlike work, Phoebus makes assumptionsstructure posts. Recently, Semi-CRFs extended use database recordstask integrating unstructured data relational databases (Mansuri & Sarawagi, 2006).work similar links unstructured data, paper citations,relational databases, reference sets authors venues. differenceview record linkage task, namely finding right reference set tuple match.paper, even though use matches database aid extraction, viewlinkage task extraction procedure followed matching task. Lastly,first consider structured SVMs information extraction. Previous work usedstructured SVMs perform Named Entity Recognition (Tsochantaridis et al., 2005)extraction task use reference sets.method aiding information extraction outside information (in formreference sets) similar work ontology-based information extraction (Embley,Campbell, Jiang, Liddle, Ng, Quass, & Smith, 1999). Later versions work even talk583fiMichelson & Knoblockusing ontology-based information extraction means semantically annotate unstructured data car classifieds (Ding, Embley, & Liddle, 2006). However, contrastwork, information extraction performed keyword-lookup ontologyalong structural contextual rules aid labeling. ontology containskeyword misspellings abbreviations, look-up performed presence noisy data. believe ontology-based extraction approach less scalablerecord linkage type matching task creating maintaining ontology requiresextensive data engineering order encompass possible common spelling mistakesabbreviations. Further, new data added ontology, additional data engineeringmust performed. work, simply add new tuples reference set. Lastly,contrast work, ontology based work assumes contextual structural rulesapply, making assumption data extract from. work, makeassumptions structure text extracting from.Yet another interesting approach information extraction using ontologies Textpresso system extracts data biological text (Muller & Sternberg, 2004).system uses regular expression based keyword look-up label tokens text basedontology. tokens labeled, Textpresso perform fact extractionextracting sequences labeled tokens fit particular pattern, gene-allelereference associations. Although system uses reference set extraction,differs keyword look-up lexicon.recent work learning efficient blocking schemes Bilenko et al., (2006) developedsystem learning disjunctive normal form blocking schemes. However, learnschemes using graphical set covering algorithm, use version SequentialCovering Algorithm (SCA). also similarities BSL algorithm workmining association rules transaction data (Agrawal, Imielinski, & Swami, 1993).algorithms discover propositional rules. Further, algorithms use multiple passesdata set discover rules. However despite similarities, techniquesreally solve different problems. BSL generates set candidate matches minimalnumber false positives. this, BSL learns conjunctions maximally specific(eliminating many false positives) unions together single disjunctive rule (tocover different true positives). Since conjunctions maximally specific, BSL usesSCA underneath, learns rules depth-first, general specific manner (Mitchell,1997). hand, work mining association rules (Agrawal et al., 1993) looksactual patterns data represent internal relationships. maymany relationships data could discovered, approach coversdata breadth-first fashion, selecting set rules iteration extendingappending new possible item.7. Conclusionarticle presents algorithm semantically annotating text ungrammaticalunstructured. Unstructured, ungrammatical sources contain much information,cannot support structured queries. technique allows informative usesources. Using approach, eBay agents could monitor auctions looking bestdeals, user could find average price four-star hotel San Diego. semantic584fiRelational Data Unstructured Data Sourcesannotation necessary society transitions Semantic Web, informationrequires annotation useful agents, users unwilling extra workprovide required annotation.future, technique could link mediator framework (Thakkar, Ambite, &Knoblock, 2004) automatically acquiring reference sets. similar automaticallyincorporating secondary sources record linkage (Michalowski, Thakkar, & Knoblock,2005). automatic formulation queries retrieve correct domain reference setdirection future research. mediator framework place, Phoebus couldincorporate many reference sets needed full coverage possible attribute valuesattribute types.Unsupervised approaches record linkage extraction also topics future research. including unsupervised record linkage extraction mediator component, approach would entirely self-contained, making semantic annotation postsautomatic process. Also, current implementation gives one class label pertoken. Ideally Phoebus would give token possible labels, remove extraneous tokens systems cleans attributes, described Section 3.disambiguation lead much higher accuracy extraction.Future work could investigate inclusion thesauri terms attributes,frequency replacement terms taken consideration. Also, exploring technologies automatically construct reference sets (and eventually thesauri)numerous ontologies Semantic Web intriguing research path.long term goal annotation extraction unstructured, ungrammaticalsources involves automating entire process. record linkage extraction methodscould become unsupervised, approach could automatically generate incorporate reference sets, apply automatically annotate data source.would ideal approach making Semantic Web useful userinvolvement.Acknowledgmentsresearch based upon work supported part National Science Foundation award number IIS-0324955, part Air Force Office Scientific Researchgrant number FA9550-07-1-0416, part Defense Advanced Research ProjectsAgency (DARPA), Department Interior, NBC, Acquisition Services Division, Contract No. NBCHD030010.U.S.Government authorized reproduce distribute reports Governmental purposes notwithstanding copyright annotation thereon. views conclusionscontained herein authors interpreted necessarily representing official policies endorsements, either expressed implied,organizations person connected them.585fiMichelson & KnoblockReferencesAgichtein, E., & Ganti, V. (2004). Mining reference tables automatic text segmentation.Proceedings 10th ACM Conference Knowledge Discovery DataMining, pp. 20 29. ACM Press.Agrawal, R., Imielinski, T., & Swami, A. (1993). Mining association rules setsitems large databases. Proceedings ACM SIGMOD International Conference Management Data, pp. 207216. ACM Press.Baxter, R., Christen, P., & Churches, T. (2003). comparison fast blocking methodsrecord linkage. Proceedings 9th ACM SIGKDD Workshop Data Cleaning,Record Linkage, Object Identification, pp. 2527.Bellare, K., & McCallum, A. (2007). Learning extractors unlabeled text using relevantdatabases. Proceedings AAAI Workshop Information IntegrationWeb, pp. 1016.Bilenko, M., Kamath, B., & Mooney, R. J. (2006). Adaptive blocking: Learning scalerecord linkage clustering. Proceedings 6th IEEE International ConferenceData Mining, pp. 8796.Bilenko, M., & Mooney, R. J. (2003). Adaptive duplicate detection using learnable stringsimilarity measures. Proceedings 9th ACM International ConferenceKnowledge Discovery Data Mining, pp. 3948. ACM Press.Borkar, V., Deshmukh, K., & Sarawagi, S. (2001). Automatic segmentation textstructured records. Proceedings ACM SIGMOD International ConferenceManagement Data, pp. 175186. ACM Press.Califf, M. E., & Mooney, R. J. (1999). Relational learning pattern-match rulesinformation extraction. Proceedings 16th National Conference ArtificialIntelligence, pp. 328334.Chakaravarthy, V. T., Gupta, H., Roy, P., & Mohania, M. (2006). Efficiently linking textdocuments relevant structured information. Proceedings InternationalConference Large Data Bases, pp. 667678. VLDB Endowment.Chaudhuri, S., Ganjam, K., Ganti, V., & Motwani, R. (2003). Robust efficient fuzzymatch online data cleaning. Proceedings ACM SIGMOD International Conference Management Data, pp. 313324. ACM Press.Cimiano, P., Handschuh, S., & Staab, S. (2004). Towards self-annotating web.Proceedings 13th International Conference World Wide Web, pp. 462471.ACM Press.Ciravegna, F. (2001). Adaptive information extraction text rule inductiongeneralisation.. Proceedings 17th International Joint Conference ArtificialIntelligence, pp. 12511256.586fiRelational Data Unstructured Data SourcesCohen, W., & Sarawagi, S. (2004). Exploiting dictionaries named entity extraction: combining semi-markov extraction processes data integration methods. Proceedings10th ACM International Conference Knowledge Discovery Data Mining,pp. 8998, Seattle, Washington. ACM Press.Cohen, W. W. (2000). Data integration using similarity joins word-based informationrepresentation language. ACM Transactions Information Systems, 18 (3), 288321.Cohen, W. W., Ravikumar, P., & Feinberg, S. E. (2003). comparison string metricsmatching names records. Proceedings ACM SIGKDD WorkshopData Cleaning, Record Linkage, Object Consoliation, pp. 1318.Crescenzi, V., Mecca, G., & Merialdo, P. (2001). Roadrunner: Towards automatic dataextraction large web sites. Proceedings 27th International ConferenceLarge Data Bases, pp. 109118. VLDB Endowment.Ding, Y., Embley, D. W., & Liddle, S. W. (2006). Automatic creation simplified querying semantic web content: approach based information-extraction ontologies.Proceedings Asian Semantic Web Conference, pp. 400414.Dingli, A., Ciravegna, F., & Wilks, Y. (2003). Automatic semantic annotation using unsupervised information extraction integration. Proceedings K-CAP Workshop Knowledge Markup Semantic Annotation.Elfeky, M. G., Verykios, V. S., & Elmagarmid, A. K. (2002). TAILOR: record linkagetoolbox. Proceedings 18th International Conference Data Engineering, pp.1728.Embley, D. W., Campbell, D. M., Jiang, Y. S., Liddle, S. W., Ng, Y.-K., Quass, D., &Smith, R. D. (1999). Conceptual-model-based data extraction multiple-recordweb pages. Data Knowledge Engineering, 31 (3), 227251.Fellegi, I. P., & Sunter, A. B. (1969). theory record linkage. Journal AmericanStatistical Association, 64, 11831210.Handschuh, S., Staab, S., & Ciravegna, F. (2002). S-cream - semi-automatic creationmetadata. Proceedings 13th International Conference Knowledge Engineering Knowledge Management, pp. 165184. Springer Verlag.Hendler, J. (2001). Agents semantic web. IEEE Intelligent Systems, 16 (2), 3037.Hernandez, M. A., & Stolfo, S. J. (1998). Real-world data dirty: Data cleansingmerge/purge problem. Data Mining Knowledge Discovery, 2 (1), 937.Jaro, M. A. (1989). Advances record-linkage methodology applied matching1985 census tampa, florida. Journal American Statistical Association, 89,414420.Joachims, T. (1999). Advances Kernel Methods - Support Vector Learning, chap. 11:Making large-Scale SVM Learning Practical. MIT-Press.587fiMichelson & KnoblockLafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random fields: Probabilistic models segmenting labeling sequence data. Proceedings 18thInternational Conference Machine Learning, pp. 282289. Morgan Kaufmann.Lee, M.-L., Ling, T. W., Lu, H., & Ko, Y. T. (1999). Cleansing data miningwarehousing. Proceedings 10th International Conference DatabaseExpert Systems Applications, pp. 751760. Springer-Verlag.Lerman, K., Gazen, C., Minton, S., & Knoblock, C. A. (2004). Populating semantic web.Proceedings AAAI Workshop Advances Text Extraction Mining.Levenshtein, V. I. (1966). Binary codes capable correcting deletions, insertions,reversals. English translation Soviet Physics Doklady, 10 (8), 707710.Mansuri, I. R., & Sarawagi, S. (2006). Integrating unstructured data relationaldatabases. Proceedings International Conference Data Engineering, p. 29.IEEE Computer Society.McCallum, A. (2002).Mallet:http://mallet.cs.umass.edu.machinelearninglanguagetoolkit.McCallum, A., Nigam, K., & Ungar, L. H. (2000). Efficient clustering high-dimensionaldata sets application reference matching. Proceedings 6th ACMSIGKDD, pp. 169178.Michalowski, M., Thakkar, S., & Knoblock, C. A. (2005). Automatically utilizing secondarysources align information across sources. AI Magazine, Special Issue SemanticIntegration, Vol. 26, pp. 3345.Michelson, M., & Knoblock, C. A. (2005). Semantic annotation unstructured ungrammatical text. Proceedings 19th International Joint Conference ArtificialIntelligence, pp. 10911098.Michelson, M., & Knoblock, C. A. (2006). Learning blocking schemes record linkage.Proceedings 21st National Conference Artificial Intelligence.Michelson, M., & Knoblock, C. A. (2007). Unsupervised information extraction unstructured, ungrammatical data sources world wide web. International JournalDocument Analysis Recognition (IJDAR), Special Issue Noisy Text Analytics.Mitchell, T. M. (1997). Machine Learning. McGraw-Hill, New York.Muller, H.-M., & Sternberg, E. E. K. P. W. (2004). Textpresso: ontology-based information retrieval extraction system biological literature. PLoS Biology, 2 (11).Muslea, I., Minton, S., & Knoblock, C. A. (2001). Hierarchical wrapper inductionsemistructured information sources. Autonomous Agents Multi-Agent Systems,4 (1/2), 93114.588fiRelational Data Unstructured Data SourcesNewcombe, H. B. (1967). Record linkage: design efficient systems linking recordsindividual family histories. American Journal Human Genetics, 19 (3),335359.Porter, M. F. (1980). algorithm suffix stripping. Program, 14 (3), 130137.Smith, T. F., & Waterman, M. S. (1981). Identification common molecular subsequences.Journal Molecular Biology, 147, 195197.Soderland, S. (1999). Learning information extraction rules semi-structured freetext. Machine Learning, 34 (1-3), 233272.Thakkar, S., Ambite, J. L., & Knoblock, C. A. (2004). data integration approachautomatically composing optimizing web services. Proceedings ICAPSWorkshop Planning Scheduling Web Grid Services.Tsochantaridis, I., Hofmann, T., Joachims, T., & Altun, Y. (2004). Support vector machinelearning interdependent structured output spaces. Proceedings 21stInternational Conference Machine Learning, p. 104. ACM Press.Tsochantaridis, I., Joachims, T., Hofmann, T., & Altun, Y. (2005). Large margin methodsstructured interdependent output variables. Journal Machine LearningResearch, 6, 14531484.Vargas-Vera, M., Motta, E., Domingue, J., Lanzoni, M., Stutt, A., & Ciravegna, F. (2002).MnM: Ontology driven semi-automatic automatic support semantic markup.Proceedings 13th International Conference Knowledge EngineeringManagement, pp. 213221.Wellner, B., McCallum, A., Peng, F., & Hay, M. (2004). integrated, conditional modelinformation extraction coreference application citation matching.Proceedings 20th Conference Uncertainty Artificial Intelligence, pp. 593601.Winkler, W. E., & Thibaudeau, Y. (1991). application fellegi-sunter modelrecord linkage 1990 U.S. Decennial Census. Tech. rep., Statistical ResearchReport Series RR91/09 U.S. Bureau Census.Zhai, C., & Lafferty, J. (2001). study smoothing methods language models appliedad hoc information retrieval. Proceedings 24th ACM SIGIR ConferenceResearch Development Information Retrieval, pp. 334342. ACM Press.589fiJournal Artificial Intelligence Research 31 (2008) 273-318Submitted 07/07; published 02/08Modular Reuse Ontologies: Theory PracticeBernardo Cuenca GrauIan HorrocksYevgeny Kazakovberg@comlab.ox.ac.ukian.horrocks@comlab.ox.ac.ukyevgeny.kazakov@comlab.ox.ac.ukOxford University Computing LaboratoryOxford, OX1 3QD, UKUlrike Sattlersattler@cs.man.ac.ukSchool Computer ScienceUniversity ManchesterManchester, M13 9PL, UKAbstractpaper, propose set tasks relevant modular reuse ontologies. order formalize tasks reasoning problems, introduce notionsconservative extension, safety module general class logic-based ontologylanguages. investigate general properties relationships notionsstudy relationships relevant reasoning problems previouslyidentified. study computability problems, consider, particular, Description Logics (DLs), provide formal underpinning W3C Web OntologyLanguage (OWL), show problems consider undecidable algorithmically unsolvable description logic underlying OWL DL. order achievepractical solution, identify conditions sufficient ontology reuse set symbols safelythat is, without changing meaning. provide notion safetyclass, characterizes sufficient condition safety, identify family safetyclassescalled localitywhich enjoys collection desirable properties. use notionsafety class extract modules ontologies, provide various modularization algorithms appropriate properties particular safety class use.Finally, show practical benefits safety checking module extraction algorithms.1. MotivationOntologiesconceptualizations domain shared community usersplay major role Semantic Web, increasingly used knowledge managementsystems, e-Science, bio-informatics, Grid applications (Staab & Studer, 2004).design, maintenance, reuse, integration ontologies complex tasks. Likesoftware engineers, ontology engineers need supported tools methodologieshelp minimize introduction errors, i.e., ensure ontologiesconsistent unexpected consequences. order develop support, important notions software engineering, module, black-box behavior, controlledinteraction, need adapted.Recently, growing interest topic modularity ontology engineering (Seidenberg & Rector, 2006; Noy, 2004a; Lutz, Walther, & Wolter, 2007; CuencaGrau, Parsia, Sirin, & Kalyanpur, 2006b; Cuenca Grau, Horrocks, Kazakov, & Sattler,c2008AI Access Foundation. rights reserved.fiCuenca Grau, Horrocks, Kazakov, & Sattler2007), motivated above-mentioned application needs. paper,focus use modularity support partial reuse ontologies. particular,consider scenario developing ontology P want reuse setsymbols foreign ontology Q without changing meaning.example, suppose ontology engineer building ontology researchprojects, specifies different types projects according research topicfocus on. ontology engineer charge projects ontology P may use termsCystic Fibrosis Genetic Disorder descriptions medical research projects.ontology engineer expert research projects; may unfamiliar, however,topics projects cover and, particular, terms Cystic FibrosisGenetic Disorder. order complete projects ontology suitable definitionsmedical terms, decides reuse knowledge subjects wellestablished medical ontology Q.straightforward way reuse concepts construct logical unionP Q axioms P Q. reasonable assume additional knowledgemedical terms used P Q implications meaningprojects defined P; indeed, additional knowledge reused terms provides newinformation medical research projects defined using medical terms.Less intuitive fact importing Q may also result new entailments concerningreused symbols, namely Cystic Fibrosis Genetic Disorder. Since ontology engineerprojects ontology expert medicine relies designers Q,expected meaning reused symbols completely specified Q;is, fact symbols used projects ontology P implyoriginal meaning Q changes. P change meaning symbolsQ, say P Q conservative extension Qrealistic application scenarios, often unreasonable assume foreign ontologyQ fixed; is, Q may evolve beyond control modelers P. ontologyengineers charge P may authorized access information Q or,importantly, may decide later time reuse symbols Cystic FibrosisGenetic Disorder medical ontology Q. application scenariosexternal ontology Q may change, reasonable abstract particular Qconsideration. particular, given set external symbols, factaxioms P change meaning symbol independentparticular meaning ascribed symbols Q. case, say P safeS.Moreover, even P safely reuses set symbols ontology Q, may stillcase Q large ontology. particular, example, foreign medicalontology may huge, importing whole ontology would make consequencesadditional information costly compute difficult ontology engineerscharge projects ontology (who medical experts) understand. practice,therefore, one may need extract module Q1 Q includes relevant information. Ideally, module small possible still guarantee capturemeaning terms used; is, answering queries research projectsontology, importing module Q1 would give exactly answers wholemedical ontology Q imported. case, importing module274fiModular Reuse Ontologies: Theory Practiceobservable effect projects ontology importing entire ontology. Furthermore, fact Q1 module Q independent particular Pconsideration.contributions paper follows:1. propose set tasks relevant ontology reuse formalizereasoning problems. end, introduce notions conservative extension,safety module general class logic-based ontology languages.2. investigate general properties relationships notionsconservative extension, safety, module use properties study relationships relevant reasoning problems previously identified.3. consider Description Logics (DLs), provide formal underpinningW3C Web Ontology Language (OWL), study computability tasks.show tasks consider undecidable algorithmically unsolvabledescription logic underlying OWL DLthe expressive dialect OWLdirect correspondence description logics.4. consider problem deciding safety ontology signature. Givenproblem undecidable OWL DL, identify sufficient conditions safety,decidable OWL DLthat is, ontology satisfies conditionssafe; converse, however, necessarily hold. proposenotion safety class, characterizes sufficiency condition safety,identify family safety classescalled localitywhich enjoys collection desirableproperties.5. next apply notion safety class task extracting modulesontologies; provide various modularization algorithms appropriateproperties particular safety class use.6. present empirical evidence practical benefits techniques safetychecking module extraction.paper extends results previous work (Cuenca Grau, Horrocks, Kutz, &Sattler, 2006; Cuenca Grau et al., 2007; Cuenca Grau, Horrocks, Kazakov, & Sattler, 2007).2. Preliminariessection introduce description logics (DLs) (Baader, Calvanese, McGuinness,Nardi, & Patel-Schneider, 2003), family knowledge representation formalismsunderlie modern ontology languages, OWL DL (Patel-Schneider, Hayes, & Horrocks, 2004). hierarchy commonly-used description logics summarized Table 1.syntax description logic L given signature set constructors.signature (or vocabulary) Sg DL (disjoint) union countably infinite sets ACatomic concepts (A, B, . . . ) representing sets elements, AR atomic roles (r, s, . . . )representing binary relations elements, Ind individuals (a, b, c, . . . ) representing constants. assume signature fixed every DL.275fiCuenca Grau, Horrocks, Kazakov, & SattlerDLsConstructorsCon>, A, C1 u C2 , R.Cpp, CppAxioms [ Ax ]TBoxABoxC, C1 v C2 : C, r(a, b)ppppppppRolRBoxELrALC ppppTrans(r)+r+ HR 1 v R2+ FFunct(R)+ N(> n S)+ Q(> n S.C)+{i}r AR, AC, a, b Ind, R(i) Rol, C(i) Con, n 1 Rol simplerole (see (Horrocks & Sattler, 2005)).Table 1: hierarchy standard description logicsEvery DL provides constructors defining set Rol (general) roles (R, S, . . . ),set Con (general) concepts (C, D, . . . ), set Ax axioms (, , . . . )union role axioms (RBox), terminological axioms (TBox) assertions (ABox).EL (Baader, Brandt, & Lutz, 2005) simple description logic allows oneconstruct complex concepts using conjunction C1 u C2 existential restriction R.Cstarting atomic concepts A, roles R top concept >. EL provides roleconstructors role axioms; thus, every role R EL atomic. TBox axiomsEL either concept definitions C general concept inclusion axioms (GCIs)C1 v C2 . EL assertions either concept assertions : C role assertions r(a, b).paper assume concept definition C abbreviation two GCIs v CC v A.basic description logic ALC (Schmidt-Schau & Smolka, 1991) obtained ELadding concept negation constructor C. introduce additional constructorsabbreviations: bottom concept shortcut >, concept disjunction C1 C2stands (C1 u C2 ), value restriction R.C stands (R.C). contrastEL, ALC express contradiction axioms like > v . logic extensionALC where, additionally, atomic roles declared transitive using roleaxiom Trans(r).extensions description logics add features inverse roles r (indicatedappending letter name logic), role inclusion axioms (RIs) R1 v R2 (+H),functional roles Funct(R) (+F), number restrictions (> n S), n 1, (+N ), qualifiednumber restrictions (> n S.C), n 1, (+Q)1 , nominals {a} (+O). Nominals makepossible construct concept representing singleton set {a} (a nominal concept)individual a. extensions used different combinations; example ALCOextension ALC nominals; SHIQ extension role hierarchies,1. dual constructors (6 n S) (6 n S.C) abbreviations (> n + 1 S) (> n + 1 S.C),respectively276fiModular Reuse Ontologies: Theory Practiceinverse roles qualified number restrictions; SHOIQ DL usesconstructors axiom types presented.Modern ontology languages, OWL, based description logics and, certain extent, syntactic variants thereof. particular, OWL DL corresponds SHOIN(Horrocks, Patel-Schneider, & van Harmelen, 2003). paper, assume ontologybased description logic L finite set axioms L. signatureontology (of axiom ) set Sig(O) (Sig()) atomic concepts, atomic rolesindividuals occur (respectively ).main reasoning task ontologies entailment: given ontology axiom, check implies . logical entailment |= defined using usual Tarski-styleset-theoretic semantics description logics follows. interpretation pair =(I , ), non-empty set, called domain interpretation,interpretation function assigns: every AC subset AI , every r ARbinary relation rI , every Ind element aI . Notesets AC, AR Ind defined interpretation assumed fixedontology language (DL).interpretation function extended complex roles concepts via DLconstructors follows:(>)I(C u D)I(R.C)I(C)I(r )I=====C DI{x | y.hx, yi RI C }\ C{hx, yi | hy, xi rI }(> n R)I = { x | ]{y | hx, yi RI } n }(> n R.C)I = { x | ]{y | hx, yi RI C } n }{a}I = {aI }satisfaction relation |= interpretation DL axiom (readsatisfies , model ) defined follows:|= C1 v C2 iff C1I C2I ;|= R1 v R2 iff R1I R2I ;|= : C iff aI C ;|= r(a, b) iff haI , bI rI ;|= Trans(r) iff x, y, z [ hx, yi rI hy, zi rI hx, zi rI ];|= Funct(R) iff x, y, z [ hx, yi RI hx, zi RI = z ];interpretation model ontology satisfies axioms O.ontology implies axiom (written |= ) |= every model O. Givenset interpretations, say axiom (an ontology O) valid everyinterpretation model (respectively O). axiom tautology validset interpretations (or, equivalently, implied empty ontology).say two interpretations = (I , ) J = (J , J ) coincide subsetsignature (notation: I|S = J |S ) = J X = X J every X S.say two sets interpretations J equal modulo (notation: I|S = J|S )every exits J J J |S = I|S every J J existsI|S = J |S .277fiCuenca Grau, Horrocks, Kazakov, & SattlerOntology medical research projects P:P1Genetic Disorder Project Project u Focus.Genetic DisorderP2Cystic Fibrosis EUProject EUProject u Focus.Cystic FibrosisP3EUProject v ProjectP4Focus.> v ProjectE1Project u (Genetic Disorder ::u Cystic Fibrosis) vE2Focus.Cystic Fibrosis v Focus.Genetic Disorder::Ontology medical terms Q:M1 Cystic Fibrosis Fibrosis u located In.Pancreas u Origin.Genetic OriginM2 Genetic Fibrosis Fibrosis u Origin.Genetic OriginM3 Fibrosis u located In.Pancreas v Genetic FibrosisM4 Genetic Fibrosis v Genetic DisorderM5 DEFBI Gene v Immuno Protein Gene u associated With.Cystic FibrosisFigure 1: Reusing medical terminology ontology research projects3. Ontology Integration Knowledge Reusesection, elaborate ontology reuse scenario sketched Section 1. Basedapplication scenario, motivate define reasoning tasks investigatedremainder paper. particular, tasks based notions conservativeextension (Section 3.2), safety (Sections 3.2 3.3) module (Section 3.4). notionsdefined relative language L. Within section, assume L ontologylanguage based description logic; Section 3.6, define formally classontology languages given definitions conservative extensions, safetymodules apply. convenience reader, tasks consider papersummarized Table 2.3.1 Motivating ExampleSuppose ontology engineer charge SHOIQ ontology research projects,specifies different types projects according research topic concernedwith. Assume ontology engineer defines two conceptsGenetic Disorder ProjectCystic Fibrosis EUProjectin ontology P. first one describes projects geneticdisorders; second one describes European projects cystic fibrosis, givenaxioms P1 P2 Figure 1. ontology engineer expert research projects:knows, example, every instance EUProject must instance Project (theconcept-inclusion axiom P3) role Focus applied instancesProject (the domain axiom P4). may unfamiliar, however, topicsprojects cover and, particular, terms Cystic Fibrosis Genetic Disordermentioned P1 P2. order complete projects ontology suitable definitions278fiModular Reuse Ontologies: Theory Practicemedical terms, decides reuse knowledge subjects wellestablished widely-used medical ontology.Suppose Cystic Fibrosis Genetic Disorder described ontology Q containing axioms M1-M5 Figure 1. straightforward way reuse conceptsimport P ontology Qthat is, add axioms Q axioms Pwork extended ontology P Q. Importing additional axioms ontologymay result new logical consequences. example, easy see axioms M1M4Q imply every instance Cystic Fibrosis instance Genetic Disorder:Q |= := (Cystic Fibrosis v Genetic Disorder)(1)Indeed, concept inclusion 1 := (Cystic Fibrosis v Genetic Fibrosis) follows axiomsM1 M2 well axioms M1 M3; follows axioms 1 M4.Using inclusion (1) axioms P1P3 ontology P proveevery instance Cystic Fibrosis EUProject also instance Genetic Disorder Project:P Q |= := (Cystic Fibrosis EUProject v Genetic Disorder Project)(2)inclusion , however, follow P alonethat is, P 6|= . ontologyengineer might aware Entailment (2), even though concerns terms primaryscope projects ontology P.natural expect entailments like (1) imported ontology Q resultnew logical consequences, like , (2), terms defined main ontology P.One would expect, however, meaning terms defined Q changesconsequence import since terms supposed completely specified withinQ. side effect would highly undesirable modeling ontology P sinceontology engineer P might expert subject Q supposedalter meaning terms defined Q even implicitly.meaning reused terms, however, might change import, perhaps duemodeling errors. order illustrate situation, suppose ontology engineerlearned concepts Genetic Disorder Cystic Fibrosis ontology Q(including inclusion (1)) decided introduce additional axioms formalizingfollowing statements:Every instance Project different every instance Genetic Disorderevery instance Cystic Fibrosis.:::(3)::::::Every::::::::project Focus Cystic Fibrosis, also Focus Genetic Disorder(4)Note statements (3) (4) thought adding new informationprojects and, intuitively, change constrain meaning medicalterms.Suppose ontology engineer formalized statements (3) (4) ontologyP using axioms E1 E2 respectively. point, ontology engineer introduced modeling errors and, consequence, axioms E1 E2 correspond (3)(4): E1 actually formalizes following statement: Every instance Project different every common instance Genetic Disorder Cystic Fibrosis, E2 expresses279fiCuenca Grau, Horrocks, Kazakov, & SattlerEvery object either Focus nothing, Focus Cystic Fibrosis,also Focus Genetic Disorder. kinds modeling errors difficult detect,especially cause inconsistencies ontology.Note that, although axiom E1 correspond fact (3), still consequence(3) means constrain meaning medical terms.hand, E2 consequence (4) and, fact, constrains meaning medicalterms. Indeed, axioms E1 E2 together axioms P1-P4 P imply new axiomsconcepts Cystic Fibrosis Genetic Disorder, namely disjointness:P |= := (Genetic Disorder u Cystic Fibrosis v )(5)entailment (5) proved using axiom E2 equivalent to:> v Focus.(Genetic Disorder Cystic Fibrosis)(6)inclusion (6) P4 imply every element domain must projectthatis, P |= (> v Project). Now, together axiom E1, implies (5).axioms E1 E2 imply new statements medical terms, alsocause inconsistencies used together imported axioms Q. Indeed,(1) (5) obtain P Q |= := (Cystic Fibrosis v ), expresses inconsistencyconcept Cystic Fibrosis.summarize, seen importing external ontology lead undesirableside effects knowledge reuse scenario, like entailment new axioms even inconsistencies involving reused vocabulary. next section discuss formalizeeffects consider undesirable.3.2 Conservative Extensions Safety Ontologyargued previous section, important requirement reuse ontology Qwithin ontology P P Q produces exactly logical consequencesvocabulary Q Q alone does. requirement naturally formulatedusing well-known notion conservative extension, recently investigated context ontologies (Ghilardi, Lutz, & Wolter, 2006; Lutz et al., 2007).Definition 1 (Deductive Conservative Extension). Let O1 two Lontologies, signature L. say deductive S-conservative extensionO1 w.r.t. L, every axiom L Sig() S, |= iff O1 |= .say deductive conservative extension O1 w.r.t. L deductiveS-conservative extension O1 w.r.t. L = Sig(O1 ).words, ontology deductive S-conservative extension O1signature language L every logical consequence constructedusing language L symbols S, already logical consequence O1 ;is, additional axioms \ O1 result new logical consequencesvocabulary S. Note deductive S-conservative extension O1 w.r.t. L,deductive S1 -conservative extension O1 w.r.t. L every S1 S.notion deductive conservative extension directly applied ontologyreuse scenario.280fiModular Reuse Ontologies: Theory PracticeDefinition 2 (Safety Ontology). Given L-ontologies O0 , saysafe O0 (or imports O0 safe way) w.r.t. L O0 deductive conservativeextension O0 w.r.t. L.Hence, first reasoning task relevant ontology reuse scenario formulatedfollows:T1.given L-ontologies O0 , determine safe O0 w.r.t. L.shown Section 3.1 that, given P consisting axioms P1P4, E1, E2, Qconsisting axioms M1M5 Figure 1, exists axiom = (Cystic Fibrosis v )uses symbols Sig(Q) Q 6|= P Q |= . According Definition 1,means P Q deductive conservative extension Q w.r.t. languageL expressed (e.g. L = ALC). possible, however, showaxiom E2 removed P resulting ontology P1 = P \ {E2}, P1 Qdeductive conservative extension Q. following notion useful proving deductiveconservative extensions:Definition 3 (Model Conservative Extension, Lutz et al., 2007).Let O1 two L-ontologies signature L. say modelS-conservative extension O1 , every model O1 , exists model JI|S = J |S . say model conservative extension O1 modelS-conservative extension O1 = Sig(O1 ).notion model conservative extension Definition 3 seen semanticcounterpart notion deductive conservative extension Definition 1: latterdefined terms logical entailment, whereas former defined terms models.Intuitively, ontology model S-conservative extension O1 every modelO1 one find model domain interprets symbolsway. notion model conservative extension, however, providecomplete characterization deductive conservative extensions, given Definition 1;is, notion used proving ontology deductive conservativeextension another, vice versa:Proposition 4 (Model vs. Deductive Conservative Extensions, Lutz et al., 2007)1. every two L-ontologies O, O1 O, signature L, modelS-conservative extension O1 deductive S-conservative extension O1w.r.t. L;2. exist two ALC ontologies O1 deductive conservativeextension O1 w.r.t. ALC, model conservative extension O1 .Example 5 Consider ontology P1 consisting axioms P1P4, E1 ontologyQ consisting axioms M1M5 Figure 1. demonstrate P1 Q deductiveconservative extension Q. According proposition 4, sufficient show P1 Qmodel conservative extension Q; is, every model Q exists modelJ P1 Q I|S = J |S = Sig(Q).281fiCuenca Grau, Horrocks, Kazakov, & Sattlerrequired model J constructed follows: take J identical except interpretations atomic concepts Genetic Disorder Project,Cystic Fibrosis EUProject, Project, EUProject atomic role Focus,interpret J empty set. easy check axioms P1P4, E1satisfied J hence J |= P1 . Moreover, since interpretation symbols Qremains unchanged, I|Sig(Q) = J |Sig(Q) J |= Q. Hence, P1 Q modelconservative extension Q.example Statement 2 Proposition, refer interested readerliterature (Lutz et al., 2007, p. 455).3.3 Safety Ontology Signaturefar, ontology reuse scenario assumed reused ontology Q fixedaxioms Q copied P import. practice, however,often convenient keep Q separate P make axioms available demandvia reference. makes possible continue developing P Q independently.example, ontology engineers project ontology P may willing dependparticular version Q, may even decide later time reuse medical terms(Cystic Fibrosis Genetic Disorder) another medical ontology instead. Therefore,many application scenarios important develop stronger safety condition Pdepends little possible particular ontology Q reused. orderformulate condition, abstract particular ontology Q importedfocus instead symbols Q reused:Definition 6 (Safety Signature). Let L-ontology signature L.say safe w.r.t. L, every L-ontology O0 Sig(O) Sig(O0 ) S,safe O0 w.r.t. L; is, O0 deductive conservative extensionO0 w.r.t. L.Intuitively, knowledge reuse scenario, ontology safe signature w.r.t.language L imports safe way ontology O0 written L sharessymbols O. associated reasoning problem formulatedfollowing way:T2.given L-ontology signature L,determine safe w.r.t. L.seen Section 3.2, ontology P consisting axioms P1P4, E1, E2 Figure 1,import Q consisting axioms M1M5 Figure 1 safe way L = ALC.According Definition 6, since Sig(P) Sig(Q) = {Cystic Fibrosis, Genetic Disorder},ontology P safe w.r.t. L.fact, possible show stronger result, namely, ontology containingaxiom E2 safe = {Cystic Fibrosis, Genetic Disorder} w.r.t. L = ALC. Considerontology O0 = {1 , 2 }, 1 = (> v Cystic Fibrosis) 2 = (Genetic Disorder v ).Since E2 equivalent axiom (6), easy see O0 inconsistent. Indeed E2,1 2 imply contradiction = (> v ), entailed O0 . Hence, O0deductive conservative extension O0 . Definition 6, since Sig(O) Sig(O0 ) S,means safe S.282fiModular Reuse Ontologies: Theory Practiceclear one could prove ontology safe signature S: simply findontology O0 Sig(O) Sig(O0 ) S, O0 deductive conservativeextension O0 . clear, however, one could prove safe S.turns notion model conservative extensions also used purpose.following lemma introduces property relates notion model conservativeextension notion safety signature. Intuitively, says notionmodel conservative extension stable expansion new axioms providedshare symbols original ontologies.Lemma 7 Let O, O1 O, O0 L-ontologies signature Lmodel S-conservative extension O1 Sig(O) Sig(O0 ) S. O0model S0 -conservative extension O1 O0 S0 = Sig(O0 ).Proof. order show OO0 model S0 -conservative extension O1 O0 accordingDefinition 3, let model O1 O0 . construct model J O0I|S0 = J |S0 .(])Since model S-conservative extension O1 , model O1 , Definition 3exists model J1 I|S = J1 |S . Let J interpretationJ |Sig(O) = J1 |Sig(O) J |S0 = I|S0 . Since Sig(O) S0 = Sig(O) (S Sig(O0 ))(Sig(O) Sig(O0 )) I|S = J1 |S , interpretation J always exists. SinceJ |Sig(O) = J1 |Sig(O) J1 |= O, J |= O; since J |S0 = I|S0 , Sig(O0 ) S0 ,|= O0 , J |= O0 . Hence J |= O0 I|S0 = J |S0 , proves (]).Lemma 7 allows us identify condition sufficient ensure safety ontologysignature:Proposition 8 (Safety Signature vs. Model Conservative Extensions)Let L-ontology signature L model S-conservativeextension empty ontology O1 = ; is, every interpretation existsmodel J J |S = I|S . safe w.r.t. L.Proof. order prove safe w.r.t. L according Definition 6, takeSHOIQ ontology O0 Sig(O) Sig(O0 ) S. need demonstrate O0deductive conservative extension O0 w.r.t. L.(])Indeed, Lemma 7, since model S-conservative extension O1 = ,Sig(O)Sig(O0 ) S, OO0 model S0 -conservative extension O1 O0 = O0S0 = Sig(O0 ). particular, since Sig(O0 ) S0 , O0 deductiveconservative extension O0 , required prove (]).Example 9 Let P1 ontology consisting axioms P1P4, E1 Figure 1.show P1 safe = {Cystic Fibrosis, Genetic Disorder} w.r.t. L = SHOIQ.Proposition 8, order prove safety P1 sufficient demonstrate P1model S-conservative extension empty ontology, is, every S-interpretationexists model J P1 I|S = J |S .Consider model J obtained Example 5. shown Example 5, Jmodel P1 I|Sig(Q) = J |Sig(Q) Q consists axioms M1M5 Figure 1.particular, since Sig(Q), I|S = J |S .283fiCuenca Grau, Horrocks, Kazakov, & Sattler3.4 Extraction Modules Ontologiesexample Figure 1 medical ontology Q small. Well established medicalontologies, however, large may describe subject matters designer P interested. example, medical ontology Q could contain informationgenes, anatomy, surgical techniques, etc.Even P imports Q without changing meaning reused symbols, processingis, browsing, reasoning over, etcthe resulting ontology P Q may considerablyharder processing P alone. Ideally, one would like extract (hopefully small) fragment Q1 external medical ontologya modulethat describes conceptsreused P.Intuitively, answering arbitrary query signature P, importingmodule Q1 give exactly answers whole ontology Qimported.Definition 10 (Module Ontology). Let O, O0 O10 O0 L-ontologies.say O10 module O0 w.r.t. L, O0 deductive S-conservativeextension O10 = Sig(O) w.r.t. L.task extracting modules imported ontologies ontology reuse scenariothus formulated follows:T3.given L-ontologies O, O0 ,compute module O10 O0 w.r.t. L.Example 11 Consider ontology P1 consisting axioms P1P4, E1 ontology Qconsisting axioms M1M5 Figure 1. Recall axiom (1) consequence axioms M1, M2 M4 well axioms M1, M3 M4 ontology Q.fact, sets axioms actually minimal subsets Q imply . particular,subset Q0 Q consisting axioms M2, M3, M4 M5, Q0 6|= .demonstrated P1 Q0 deductive conservative extension Q0 . particularP1 Q0 6|= . then, according Definition 10, Q0 module P1 Q w.r.t.L = ALC extensions, since P1 Q deductive S-conservative extensionP1 Q0 w.r.t. L = ALC = Sig(P1 ). Indeed, P1 Q |= , Sig() P1 Q0 6|= .Similarly, one show subset Q imply module Qw.r.t. P1 .hand, possible show subsets Q1 = {M1, M2, M4}Q2 = {M1, M3, M4} Q modules P1 Q w.r.t. L = SHOIQ. so, Definition 10, need demonstrate P1 Q deductive S-conservative extensionP1 Q1 P1 Q2 = Sig(P1 ) w.r.t. L. usual, demonstrate strongerfact, P1 Q model S-conservative extension P1 Q1 P1 Q2sufficient Claim 1 Proposition 4.order show P1 Q model S-conservative extension P1 Q1 =Sig(P1 ), consider model P1 Q1 . need construct model J P1 QI|S = J |S . Let J defined exactly except interpretationsatomic concepts Fibrosis, Pancreas, Genetic Fibrosis, Genetic Origin definedinterpretation Cystic Fibrosis I, interpretations atomic roles located284fiModular Reuse Ontologies: Theory PracticeOrigin defined identity relation. easy see axioms M1M3 M5satisfied J . Since modify interpretation symbols P1 , J alsosatisfies axioms P1 . Moreover, J model M4, Genetic FibrosisGenetic Disorder interpreted J like Cystic Fibrosis Genetic Disorder I,model Q1 , implies concept inclusion = (Cystic Fibrosis v Genetic Disorder).Hence constructed model J P1 Q I|S = J |S , thus P1 Qmodel S-conservative extension P1 Q1 .fact, construction works replace Q1 subset Q implies. particular, P1 Q also model S-conservative extension P1 Q2 . way,demonstrated modules P Q exactly subsets Q imply.algorithm implementing task T3 used extracting moduleontology Q imported P prior performing reasoning terms P. However,ontology P modified, module extracted since module Q1P Q might necessarily module modified ontology. Since extractionmodules potentially expensive operation, would convenient extractmodule reuse version ontology P reuses specifiedsymbols Q. idea motivates following definition:Definition 12 (Module Signature). Let O0 O10 O0 L-ontologiessignature L. say O10 module O0 (or S-module O0 ) w.r.t.L, every L-ontology Sig(O) Sig(O0 ) S, O10 moduleO0 w.r.t. L.Intuitively, notion module signature uniform analog notionmodule ontology, similar way notion safety signature uniformanalog safety ontology. reasoning task corresponding Definition 12formulated follows:T4.given L-ontology O0 signature L,compute module O10 O0 .Continuing Example 11, possible demonstrate subset Q impliesaxiom , fact module = {Cystic Fibrosis, Genetic Disorder} Q, is,imported instead Q every ontology shares Q symbols S.order prove this, use following sufficient condition based notion modelconservative extension:Proposition 13 (Modules Signature vs. Model Conservative Extensions)Let O0 O10 O0 L-ontologies signature L O0 modelS-conservative extension O10 . O10 module O0 w.r.t. L.Proof. order prove O10 module O0 w.r.t. L according Definition 12,take SHOIQ ontology Sig(O) Sig(O0 ) S. need demonstrateO10 module O0 w.r.t. L, is, according Definition 10, O0deductive S0 -conservative extension O10 w.r.t. L S0 = Sig(O).(])285fiCuenca Grau, Horrocks, Kazakov, & SattlerIndeed, Lemma 7, since O0 model S-conservative extension O10 , Sig(O0 )Sig(O) S, O0 model S00 -conservative extension O10S00 = Sig(O). particular, since S0 = Sig(O) S00 , O0 deductiveS0 -conservative extension O10 w.r.t. L, required prove (]).Example 14 Let Q Example 11. demonstrate subset Q1 Qimplies module = {Cystic Fibrosis, Genetic Disorder} Q. AccordingProposition 13, sufficient demonstrate Q model S-conservative extensionQ1 , is, every model Q1 exists model J Q I|S = J |S .easy see model J constructed Example 11, requiredproperty holds.Note module signature Q necessarily contain axiomscontain symbols S. example, module Q1 consisting axiom M1, M2M4 Q contain axiom M5 mentions atomic concept Cystic FibrosisS. Also note even minimal module like Q1 might still axiomslike M2 mention symbols all.3.5 Minimal Modules Essential AxiomsOne usually interested extracting arbitrary modules reused ontology,extracting modules easy process afterwards. Ideally, extracted modulessmall possible. Hence reasonable consider problem extractingminimal modules; is, modules contain module subset.Examples 11 14 demonstrate minimal module ontology signaturenecessarily unique: ontology Q consisting axioms M1M5 two minimal modules Q1 = {M1, M2, M4}, Q2 = {M1, M3, M4}, ontology P1 = {P1, P2, P3, E1}well signature = {Cystic Fibrosis, Genetic Disorder}, since minimal sets axioms imply axiom = (Cystic Fibrosis v Genetic Disorder). Dependingapplication scenario, one consider several variations tasks T3 T4 computing minimal modules. applications might necessary extract minimalmodules, whereas others minimal module suffices.Axioms occur minimal module Q essential Palways removed every module Q, thus never need importedP. true axioms occur minimal modules Q. mightnecessary import axioms P order lose essential information Q.arguments motivate following notion:Definition 15 (Essential Axiom). Let O0 L-ontologies, signatureaxiom L. say essential O0 w.r.t. L containedminimal module O0 w.r.t. L. say essential axiom O0 w.r.t.L (or S-essential O0 ) contained minimal module O0 w.r.t. L.example axioms M1M4 Q essential ontology P1signature = {Cystic Fibrosis, Genetic Disorder}, axiom M5 essential.certain situations one might interested computing set essential axiomsontology, done computing union minimal modules. Note286fiModular Reuse Ontologies: Theory PracticeNotationInputTaskChecking Safety:T1O, O0 , LCheck safe O0 w.r.t. LT2O, S, LCheck safe w.r.t. LExtracting [all / / union of] [minimal] module(s):T3[a,s,u][m]O, O0 , LExtract modules O0 w.r.t. LT4[a,s,u][m]O0 , S, LExtract modules O0 w.r.t. LO, O0 ontologies signature LTable 2: Summary reasoning tasks relevant ontology integration reusecomputing union minimal modules might easier computing minimalmodules since one need identify axiom belongs minimal module.Table 2 summarized reasoning tasks found potentially relevantontology reuse scenarios included variants T3am, T3sm, T3umtask T3 T4am, T4sm, T4um task T4 computation minimal modulesdiscussed section.variants tasks T3 T4 could considered relevant ontology reuse.example, instead computing minimal modules, one might interested computingmodules smallest number axioms, modules smallest size measurednumber symbols, complexity measure ontology. theoreticalresults present paper easily extended many reasoningtasks.3.6 Safety Modules General Ontology Languagesnotions introduced Section 3 defined respect ontology language.far, however, implicitly assumed ontology languages descriptionlogics defined Section 2that is, fragments DL SHOIQ. notions considered Section 3 applied, however, much broader class ontology languages.definitions apply ontology language notion entailment axiomsontologies, mechanism identifying signatures.Definition 16. ontology language tuple L = (Sg, Ax, Sig, |=), Sg setsignature elements (or vocabulary) L, Ax set axioms L, Sig functionassigns every axiom Ax finite set Sig() Sg called signature ,|= entailment relation sets axioms Ax axioms Ax, written|= . ontology L finiteSset axioms Ax. extend function Sigontologies follows: Sig(O) := Sig().Definition 16 provides general notion ontology language. language Lgiven set symbols (a signature), set formulae (axioms) constructedsymbols, function assigns formula signature, entailmentrelation sets axioms. ontology language OWL DL well description287fiCuenca Grau, Horrocks, Kazakov, & Sattlerlogics defined Section 2 examples ontology languages accordance Definition 16.examples ontology languages First Order Logic, Second Order Logic, LogicPrograms.easy see notions deductive conservative extension (Definition 1), safety(Definitions 2 6) modules (Definitions 10 12), well reasoning tasksTable 2, well-defined every ontology language L given Definition 16.definition model conservative extension (Definition 3) propositions involvingmodel conservative extensions (Propositions 4, 8, 13) also extendedlanguages standard Tarski model-theoretic semantics, Higher Order Logic.simplify presentation, however, formulate general requirementssemantics ontology languages, assume deal sublanguages SHOIQwhenever semantics taken account.remainder section, establish relationships differentnotions safety modules arbitrary ontology languages.Proposition 17 (Safety vs. Modules Ontology) Let L ontology language,let O, O0 , O10 O0 ontologies L. Then:1. O0 safe w.r.t. L iff empty ontology module O0 w.r.t. L.2. O0 \ O10 safe O10 O10 module O0 w.r.t. L.Proof. 1. Definition 2, O0 safe w.r.t. L iff (a) O0 deductive conservativeextension w.r.t. L. Definition 10, empty ontology O00 = moduleO0 w.r.t. L iff (b) O0 deductive S-conservative extension O00 = w.r.t. L= Sig(O). easy see (a) (b).2. Definition 2, O0 \ O10 safe O10 w.r.t. L iff (c) O10 (O0 \ O10 ) = O0deductive conservative extension O10 w.r.t. L. particular, O0 deductiveS-conservative extension O10 w.r.t. L = Sig(O), implies, Definition 10,O10 module O0 w.r.t. L.also provide analog Proposition 17 notions safety modulessignature:Proposition 18 (Safety vs. Modules Signature) Let L ontology language,O0 O10 O0 , ontologies L, subset signature L. Then:1. O0 safe w.r.t. L iff empty ontology O00 = S-module O0 w.r.t. L.2. O0 \ O10 safe Sig(O10 ) w.r.t. L, O10 S-module O0 w.r.t. L.Proof. 1. Definition 6, O0 safe w.r.t. L iff (a) every Sig(O0 ) Sig(O)S, case O0 safe w.r.t. L. Definition 12, O00 = S-moduleO0 w.r.t. L iff (b) every Sig(O0 ) Sig(O) S, case O00 =module O0 w.r.t. L. Claim 1 Proposition 17, easy see (a)equivalent (b).2. Definition 6, O0 \ O10 safe Sig(O10 ) w.r.t. L iff (c) every O,Sig(O0 \ O10 ) Sig(O) Sig(O10 ), O0 \ O10 safe w.r.t. L.288fiModular Reuse Ontologies: Theory PracticeDefinition 12, O10 S-module O0 w.r.t. L iff (d) every Sig(O0 )Sig(O) S,O10 module O0 w.r.t. L.order prove (c) implies (d), let Sig(O0 ) Sig(O) S. needdemonstrate O10 module O0 w.r.t. L.(?)000000Let := O1 . Note Sig(O \ O1 ) Sig(O) = Sig(O \ O1 ) (Sig(O) Sig(O1 ))(Sig(O0 \ O10 ) Sig(O)) Sig(O10 ) Sig(O10 ). Hence, (c) O0 \ O10 safe= O10 w.r.t. L implies Claim 2 Proposition 17 O10 moduleO0 w.r.t. L (?).4. Undecidability Complexity Resultssection study computational properties tasks Table 2 ontologylanguages correspond fragments description logic SHOIQ. demonstratereasoning tasks algorithmically unsolvable even relatively inexpressive DLs, computationally hard simple DLs.Since notions modules safety defined Section 3 using notiondeductive conservative extension, reasonable identify (un)decidabilitycomplexity results conservative extensions applicable reasoning tasks Table 2. computational properties conservative extensions recently studiedcontext description logics. Given O1 language L, problemdeciding whether deductive conservative extension O1 w.r.t. L 2-EXPTIMEcomplete L = ALC (Ghilardi et al., 2006). result extended Lutz et al.(2007), showed problem 2-EXPTIME-complete L = ALCIQ undecidable L = ALCIQO. Recently, problem also studied simple DLs;shown deciding deductive conservative extensions EXPTIME-completeL = EL(Lutz & Wolter, 2007). results immediately applied notionssafety modules ontology:Proposition 19 Given ontologies O0 L, problem determining whethersafe O0 w.r.t. L EXPTIME-complete L = EL, 2-EXPTIME-completeL = ALC L = ALCIQ, undecidable L = ALCIQO. Given ontologies O, O0 ,O10 O0 L, problem determining whether O10 module O0EXPTIME-complete L = EL, 2-EXPTIME complete L = ALC L = ALCIQ,undecidable L = ALCIQO.Proof. Definition 2, ontology safe O0 w.r.t. L iff O0 deductiveconservative extension O0 w.r.t. L. Definition 2, ontology O10 moduleO0 w.r.t. L O0 deductive S-conservative extension O10 = Sig(O) w.r.t.L. Hence, algorithm checking deductive conservative extensions reusedchecking safety modules.Conversely, demonstrate algorithm checking safety modulesused checking deductive conservative extensions. Indeed, deductive conservativeextension O1 w.r.t. L iff O\O1 safe O1 w.r.t. L iff, Claim 1 Proposition 17,O0 = module O1 \ O1 w.r.t. L .289fiCuenca Grau, Horrocks, Kazakov, & SattlerCorollary 20 exist algorithms performing tasks T1, T3[a,s,u]m Table 2L = EL L = ALCIQ run EXPTIME 2-EXPTIME respectively.algorithm performing tasks T1, T3[a,s,u]m Table 2 L = ALCIQO.Proof. task T1 corresponds directly problem checking safety ontology,given Definition 2.Suppose 2-EXPTIME algorithm that, given ontologies O, O0 O100, determines whether O10 module O0 w.r.t. L = ALCIQ. demonstratealgorithm used solve reasoning tasks T3am, T3sm T3umL = ALCIQ 2-EXPTIME. Indeed, given ontologies O0 , one enumeratesubsets O0 check 2-EXPTIME subsets modules O0w.r.t. L. determine modules minimal return them,one them, union depending reasoning task.Finally, prove solving reasoning tasks T3am, T3sm T3umeasier checking safety ontology. Indeed, Claim 1 Proposition 17,ontology safe O0 w.r.t. L iff O0 = module O0 w.r.t. L. Noteempty ontology O0 = module O0 w.r.t. L iff O0 = minimalmodule O0 w.r.t. L.demonstrated reasoning tasks T1 T3[a,s,u]m computationallyunsolvable DLs expressive ALCQIO, 2-EXPTIME-hard ALC.remainder section, focus computational properties reasoningtasks T2 T4[a,s,u]m related notions safety modules signature.demonstrate reasoning tasks undecidable DLs expressiveALCO.Theorem 21 (Undecidability Safety Signature) problem checkingwhether ontology consisting single ALC-axiom safe signature undecidable w.r.t. L = ALCO.Proof. proof variation construction undecidability deductive conservative extensions ALCQIO (Lutz et al., 2007), based reduction domino tilingproblem.domino system triple = (T, H, V ) = {1, . . . , k} finite set tilesH, V horizontal vertical matching relations. solution dominosystem mapping ti,j assigns every pair integers i, j 1 element ,hti,j , ti,j+1 H hti,j , ti+1,j V . periodic solution domino systemsolution ti,j exist integers 1 , n 1 called periodsti+m,j = ti,j ti,j+n = ti,j every i, j 1.Let set domino systems, Ds subset admit solutionDps subset Ds admit periodic solution. well-known (Borger, Gradel,& Gurevich, 1997, Theorem 3.1.7) sets \ Ds Dps recursively inseparable,is, recursive (i.e. decidable) subset D0 domino systemsDps D0 Ds .use property reduction. domino system D, constructsignature = S(D), ontology = O(D) consists single ALC-axiomthat:290fiModular Reuse Ontologies: Theory Practice(q1 )> v A1 Ak= {1, . . . , k}(q2 )u At0 vFv rH .( ht,t0 iH At0 )Fv rV .( ht,t0 iV At0 )1 < t0 k(q3 )(q4 )1tk1tkFigure 2: ontology Otile = Otile (D) expressing tiling conditions domino system(a) solution = O(D) safe = S(D) w.r.t. L = ALCO,(b) periodic solution = O(D) safe = S(D) w.r.t. L = ALCO.words, set D0 consisting domino systems = O(D)safe = S(D) w.r.t. L = ALCO, Dps D0 Ds . Since \ Ds Dpsrecursively inseparable, implies undecidability D0 hence problemchecking S-safe w.r.t. L = ALCO, otherwise one use problemdeciding membership D0 .signature = S(D), ontology = O(D) constructed follows. Givendomino system = (T, H, V ), let consist fresh atomic concepts everytwo atomic roles rH rV . Consider ontology Otile Figure 2 constructed D. NoteSig(Otile ) = S. axioms Otile express tiling conditions domino systemD, namely (q1 ) (q2 ) express every domain element assigned unique tile; (q3 ) (q4 ) express every domain element horizontal vertical matchingsuccessors.let atomic role B atomic concept s, B/ S. Let := {}where:hF:= > v s.(Ci vDi )Otile (Ci u Di ) (rH .rV .B u rV .rH .B)say rH rV commute interpretation = (I , ) domainelements a, b, c, d1 d2 ha, bi rH , hb, d1 rV , ha, ci rV ,hc, d2 rH , d1 = d2 . following claims easily proved:Claim 1.Otile (D) model rH rV commute, solution.Indeed model = (, ) Otile (D) used guide construction solutionti,j follows. every i, j 1, construct ti,j inductively together elementsai,j hai,j , ai,j+1 rV hai,j , ai+1,j rH . set a1,1 element.suppose ai,j i, j 1 constructed. Since model axioms (q1 )(q2 ) Figure 2, unique 1 k ai,j . setti,j := t. Since model axioms (q3 ) (q4 ) ai,j exist b, ct0 , t00 hai,j , bi rH , hai,j , ci rV , ht, t0 H, ht, t00 V , b At0 ,c At00 . case assign ai,j+1 := b, ai+1,j := c, ti,j+1 := t0 , ti+1,j := t00 .Note values ai,j ti,j assigned two times: ai+1,j+1 ti+1,j+1constructed ai,j+1 ai,j+1 . However, since rV rH commute I, value291fiCuenca Grau, Horrocks, Kazakov, & Sattlerai+1,j+1 unique, (q2 ), value ti+1,j+1 unique. easy seeti,j solution D.Claim 2.model Otile O, rH rV commute I.Indeed, easy see Otile |= (> v s.[rH .rV .B u rV .rH .B]). Hence,= (I , ) model Otile O, exist a, b, c, d1 d2 hx, ai sIevery x , ha, bi rH , hb, d1 rV , d1 B , ha, ci rV , hc, d2 rH ,d2 (B)I . implies d1 6= d2 , so, rh rV commute I.Finally, demonstrate = O(D) satisfies properties (a) (b).order prove property (a) use sufficient condition safety given Proposition 8 demonstrate solution every interpretationexists model J J |S = I|S . Proposition 8, imply safew.r.t. L.Let arbitrary interpretation. Since solution, contra-positiveClaim 1 either (1) model Otile , (2) rH rV commute I.demonstrate cases construct required model JJ |S = I|S .Case (1). = (I , ) model Otile exists axiom (Ci v Di )Otile 6|= (Ci v Di ). is, exists domain elementCiI 6 DiI . Let us define J identical except interpretationatomic role define J sJ = {hx, ai | x }. Since interpretationssymbols remained unchanged, CiJ , DiJ , J |= (> vs.[Ci u Dj ]). implies J |= , so, constructed model JJ |S = I|S .Case (2). Suppose rH rV commute = (I , ). meansexist domain elements a, b, c, d1 d2 ha, bi rH , hb, d1 rV ,ha, ci rV , hc, d2 rH , d1 6= d2 . Let us define J identical exceptinterpretation atomic role atomic concept B. interpret JsJ = {hx, ai | x }. interpret B J B J = {d1 }. Note (rH .rV .B)J(rV .rH .B)J since d1 6= d2 . So, J |= (> v s.[rH .rV .B u rV .rH .B])implies J |= , thus, constructed model JJ |S = I|S .order prove property (b), assume periodic solution ti,jperiods m, n 1. demonstrate S-safe w.r.t. L. purposeconstruct ALCO-ontology O0 Sig(O) Sig(O0 ) O0 |= (> v ),O0 6|= (> v ). imply safe O0 w.r.t. L = ALCO, and, hence,safe w.r.t. L = ALCO.define O0 every model O0 finite encoding periodic solution ti,jperiods n. every pair (i, j) 1 1 j n introducefresh individual ai,j define O0 extension Otile following axioms:(p1 ) {ai1 ,j } v rV .{ai2 ,j }(p2 ) {ai1 ,j } v rV .{ai2 ,j },i2 = i1 + 1mod(p3 ) {ai,j1 } v rH .{ai,j2 }(p4 ) {ai,j1 } v rH .{ai,j2 },j2 = j1 + 1F(p5 ) > v 1im, 1jn {ai,j }mod n292fiModular Reuse Ontologies: Theory Practicepurpose axioms (p1 )(p5 ) ensure rH rV commute every modelO0 . easy see O0 model corresponding every periodic solutionperiods n. Hence O0 6|= (> v ). hand, Claim 2, since O0 containsOtile , every model O0 O, rH rV commute. possibleO0 models, O0 |= (> v ).direct consequence Theorem 21 Proposition 18 undecidabilityproblem checking whether subset ontology module signature:Corollary 22 Given signature ALC-ontologies O0 O10 O0 , problemdetermining whether O10 S-module O0 w.r.t. L = ALCO undecidable.Proof. Claim 1 Proposition 18, S-safe w.r.t. L O0 = S-modulew.r.t. L. Hence algorithm recognizing modules signature L usedchecking ontology safe signature L.Corollary 23 algorithm perform tasks T2, T4[a,s,u]m L =ALCO.Proof. Theorem 21 directly implies algorithm task T2, since taskcorresponds problem checking safety signature.Solving reasoning tasks T4am, T4sm, T4um L least hardchecking safety ontology, since, Claim 1 Proposition 18, ontologyS-safe w.r.t. L iff O0 = (the minimal) S-module w.r.t. L.5. Sufficient Conditions SafetyTheorem 21 establishes undecidability checking whether ontology expressedOWL DL safe w.r.t. signature. undecidability result discouraging leavesus two alternatives: First, could focus simple DLs problemdecidable. Alternatively, could look sufficient conditions notion safetyis, ontology satisfies conditions, guarantee safe;converse, however, necessarily hold.remainder paper focuses latter approach. go further,however, worth noting Theorem 21 still leaves room investigating formerapproach. Indeed, safety may still decidable weaker description logics, EL,even expressive logics SHIQ. case SHIQ, however, existingresults (Lutz et al., 2007) strongly indicate checking safety likely exponentiallyharder reasoning practical algorithms may hard design. said,follows focus defining sufficient conditions safety use practicerestrict OWL DLthat is, SHOIQontologies.5.1 Safety Classesgeneral, sufficient condition safety defined giving, signatureS, set ontologies language satisfy condition signature.ontologies guaranteed safe signature consideration.intuitions lead notion safety class.293fiCuenca Grau, Horrocks, Kazakov, & SattlerDefinition 24 (Class Ontologies, Safety Class). class ontologies languageL function O() assigns every subset signature L, subset O(S)ontologies L. class O() anti-monotonic S1 S2 implies O(S2 ) O(S1 );compact O(S Sig(O)) O(S); subset-closed O1 O2O2 O(S) implies O1 O(S); union-closed O1 O(S) O2 O(S) implies(O1 O2 ) O(S).safety class (also called sufficient condition safety) ontology language Lclass ontologies O() L case (i) O(S),(ii) ontology O(S) S-safe L.Intuitively, class ontologies collection sets ontologies parameterizedsignature. safety class represents sufficient condition safety: ontology O(S)safe S. Also, w.l.o.g., assume empty ontology belongsevery safety class every signature. follows, whenever ontology belongssafety class given signature safety class clear context,sometimes say passes safety test S.Safety classes may admit many natural properties, given Definition 24. Antimonotonicity intuitively means ontology proved safe w.r.t.using sufficient condition, proved safe w.r.t. every subset S.Compactness means sufficient consider common elements Sig(O)checking safety. Subset-closure (union closure) means (O1 O2 ) satisfysufficient condition safety, every subset (the union O1 O2 ) also satisfiescondition.5.2 Localitysection introduce family safety classes L = SHOIQ basedsemantic properties underlying notion model conservative extensions. Section 3,seen that, according Proposition 8, one way prove S-safe showmodel S-conservative extension empty ontology.following definition formalizes classes ontologies, called local ontologies,safety proved using Proposition 8.Definition 25 (Class Interpretations, Locality). Given SHOIQ signature S,say set interpretations local w.r.t. every SHOIQ-interpretationexists interpretation J I|S = J |S .class interpretations function I() given SHOIQ signature returns setinterpretations I(S); local I(S) local w.r.t. every S; monotonic S1 S2implies I(S1 ) I(S2 ); compact every S1 , S2 (S1 S2 ) =I(S1 )|S = I(S2 )|S , S1 S2 symmetric difference sets S1S2 defined S1 S2 := S1 \ S2 S2 \ S1 .Given class interpretations I(), say O() class ontologies O() basedI() every S, O(S) set ontologies valid I(S); I() localsay O() class local ontologies, every O(S) everyO, say local (based I()).294fiModular Reuse Ontologies: Theory PracticerExample 26 Let IA() class SHOIQ interpretations defined follows. Givenrsignature S, set IA (S) consists interpretations J rJ = every atomicrrole r/ AJ = every atomic concept/ S. easy show IA(S)local every S, since every interpretation = ( , ) interpretationJ = (J , J ) defined J := , rJ = r/ S, AJ =/ S, X J := Xrrrremaining symbols X, J IA (S) I|S = J |S . Since IA(S1 ) IA(S2 )rrevery S1 S2 , case IA () monotonic; IA () also compact, sincerr(S2 ) defined differently(S1 ) IAevery S1 S2 sets interpretations IAelements S1 S2 .rrGiven signature S, set AxA(S) axioms local w.r.t. based IA(S)rconsists axioms every J IA (S), case J |= .rrr(S).(S) iff AxA() defined OAclass local ontologies based IAProposition 27 (Locality Implies Safety) Let O() class ontologies SHOIQbased local class interpretations I(). O() subset-closed union-closedsafety class L = SHOIQ. Additionally, I() monotonic, O() anti-monotonic,I() compact O() also compact.Proof. Assume O() class ontologies based I(). Definition 25every SHOIQ signature S, O(S) iff valid I(S) iff J |= everyinterpretation J I(S). Since I() local class interpretations,every SHOIQ-interpretation exists J I(S) J |S = I|S . Henceevery I(S) every SHOIQ interpretation model J I(S)J |S = I|S , implies Proposition 8 safe w.r.t. L = SHOIQ. ThusO() safety class.fact O() subset-closed union-closed follows directly Definition 25since (O1 O2 ) O(S) iff (O1 O2 ) valid I(S) iff O1 O2 valid I(S) iffO1 O(S) O2 O(S). I() monotonic I(S1 ) I(S2 ) every S1 S2 ,O(S2 ) implies valid I(S2 ) implies valid I(S1 )implies O(S1 ). Hence O() anti-monotonic.I() compact every S1 , S2 (S1 S2 ) =I(S1 )|S = I(S2 )|S , hence every Sig(O) validI(S1 ) iff valid I(S2 ), so, O(S1 ) iff O(S2 ). particular, O(S) iffO(S Sig(O)) since (S (S Sig(O))) Sig(O) = (S \ Sig(O)) Sig(O) = . Hence,O() compact.rCorollary 28 class ontologies OA(S) defined Example 26 anti-monotoniccompact subset-closed union-closed safety class.Example 29 Recall Example 5 Section 3, demonstrated ontologyP1 Q given Figure 1 P1 = {P1, . . . , P4, E1} deductive conservative extensionQ = {Cystic Fibrosis, Genetic Disorder}. done showing everyS-interpretation expanded model J axioms P1P4, E1 interpretingsymbols Sig(P1 ) \ empty set. terms Example 26 means295fiCuenca Grau, Horrocks, Kazakov, & SattlerrrP1 OA(S). Since OA() class local ontologies, Proposition 27, ontologyP1 safe w.r.t. L = SHOIQ.Proposition 27 Example 29 suggest particular way proving safety ontologies. Given SHOIQ ontology signature sufficient check whetherr(S); is, whether every axiom satisfied every interpretationOArIA (S). property holds, must safe according Proposition 27.turns notion provides powerful sufficiency test safety workssurprisingly well many real-world ontologies, shown Section 8. nextsection discuss perform test practice.5.3 Testing Localityrsection, focus detail safety class OA(), introduced Example 26. ambiguity arise, refer safety class simply locality.2rdefinition AxA(S) given Example 26 easy see axiomrlocal w.r.t. (based IA (S)) satisfied every interpretation fixes interpretation atomic roles concepts outside empty set. Note defininglocality fix interpretation individuals outside S, principle,could done. reason elegant way describe interpretations.Namely, every individual needs interpreted element domain,canonical element every domain choose.order test locality w.r.t. S, sufficient interpret every atomic conceptatomic role empty set check satisfied interpretationsremaining symbols. observation suggests following test locality:Proposition 30 (Testing Locality) Given SHOIQ signature S, concept C, axiomontology let (C, S), (, S) (O, S) defined recursively follows:(C, S) ::=(>, S)| (A, S)| ({a}, S)| (C1 u C2 , S)| (C1 , S)| (R.C1 , S)| (> n R.C 1 , S)= >;=/ otherwise = A;= {a};= (C1 , S) u (C2 , S);= (C1 , S);= Sig(R) * otherwise = R. (C1 , S);= Sig(R) * otherwise = (> n R. (C1 , S)).(C1 v C2 , S) = ( (C1 , S) v (C2 , S));| (R1 v R2 , S) = ( v ) Sig(R1 ) * S, otherwise= R1 .> v Sig(R2 ) * S, otherwise = (R1 v R2 );| (a : C, S)= : (C, S);| (r(a, b), S)= > v r/ otherwise = r(a, b);| (Trans(r), S) = v r/ otherwise = Trans(r);| (Funct(R), S) = v Sig(R) * otherwise = Funct(R).(O, S) ::=(, S)(, S) ::=2. notion locality exactly one used previous work (Cuenca Grau et al., 2007).296(a)(b)(c)(d)(e)(f )(g)(h)(i)(j)(k)(l)(m)(n)fiModular Reuse Ontologies: Theory PracticerThen, OA(S) iff every axiom (O, S) tautology.Proof. easy check every atomic concept atomic role r (C, S),r S, words, atomic concepts roleseliminated transformation.3 also easy show induction everyrinterpretation IA(S), C = ( (C, S))I |= iff |= (, S). Henceraxiom local w.r.t. iff |= every interpretation IA(S) iff |= (, S)revery Sig()-interpretation IA (S) iff (, S) tautology.Example 31 Let = {} ontology consisting axiom = M2 Figure 1.demonstrate using Proposition 30 local w.r.t. S1 = {Fibrosis, Genetic Origin},local w.r.t. S2 = {Genetic Fibrosis, Origin}.Indeed, according Proposition 30, order check whether local w.r.t. S1sufficient perform following replacements (the symbols S1 underlined):M2[by (f)][by (b)]}|{z}|{zGenetic Fibrosis Fibrosis u Origin.Genetic Origin(7)Similarly, order check whether local w.r.t. S2 , sufficient performfollowing replacements (the symbols S2 underlined):[by (b)][by (b)]}|{z }| {zM2 Genetic Fibrosis Fibrosis u Origin.Genetic Origin(8)first case obtain (M2, S1 ) = ( Fibrosis u ) SHOIQ-tautology.Hence local w.r.t. S1 hence Proposition 8 S1 -safe w.r.t. SHOIQ.second case (M2, S2 ) = (Genetic Fibrosis u Origin.) SHOIQtautology, hence local w.r.t. S2 .5.4 Tractable Approximation LocalityOne important conclusions Proposition 30 one use standard capabilities available DL-reasoners FaCT++ (Tsarkov & Horrocks, 2006), RACER(Moller & Haarslev, 2003), Pellet (Sirin & Parsia, 2004) KAON2 (Motik, 2006) testinglocality since reasoners, among things, allow testing DL-tautologies. Checking tautologies description logics is, theoretically, difficult problem (e.g.DL SHOIQ known NEXPTIME-complete, Tobies, 2000). are, however,several reasons believe locality test would perform well practice. primaryreason sizes axioms need tested tautologies usuallyrelatively small compared sizes ontologies. Secondly, modern DL reasonershighly optimized standard reasoning tasks behave well realistic ontologies.case reasoning costly, possible formulate tractable approximationlocality conditions SHOIQ:3. Recall constructors , C1 C2 , R.C, (6 n R.C) assumed expressed using >,C1 u C2 , R.C (> n R.C), hence, particular, every role R Sig(R ) * occurs eitherR .C, (> n R .C), R v R, R v R , Trans(R ), Funct(R ), hence eliminated. atomicconcepts/ eliminated likewise. Note necessarily case Sig( (, S)) S,since (, S) may still contain individuals occur S.297fiCuenca Grau, Horrocks, Kazakov, & SattlerDefinition 32 (Syntactic Locality SHOIQ). Let signature. followinggrammar recursively defines two sets concepts Con(S) Con(S) signature S:Con(S) ::= | C | C u C | C u C | R .C | R.C | (> n R .C) | (> n R.C ) .Con(S) ::= > | C | C1 u C2 ./ atomic concept, R (possibly inverse of) atomic role r/ S, CCon(S), {1, 2}.concept, R role, C Con(S), C(i)axiom syntactically local w.r.t. one following forms: (1) R v R,(2) Trans(r ), (3) Funct(R ), (4) C v C, (5) C v C , (6) : C . denoterAxA(S) set SHOIQ-axioms syntactically local w.r.t. S.rSHOIQ-ontology syntactically local w.r.t. AxA(S). denoterOA (S) set SHOIQ ontologies syntactically local w.r.t. S.Intuitively, syntactic locality provides simple syntactic test ensure axiomrsatisfied every interpretation IA(S). easy see inductive definitionsCon (S) Con (S) Definition 32 every interpretation = (I , )rIA(S) case (C )I = (C )I = every C Con(S)C Con(S). Hence, every syntactically local axiom satisfied every interpretationrIA(S), obtain following conclusion:rrProposition 33 AxA(S) AxA(S).Further, shown safety class SHOIQ based syntactic localityenjoys properties Definition 24:rProposition 34 class syntactically local ontologies OA() given Definition 32anti-monotonic, compact, subset-closed, union-closed safety class.rr() safety class Proposition 33. Anti-monotonicity OA()Proof. OAshown induction, proving Con (S2 ) Con (S1 ), Con (S2 ) Con(S1 )rrAxA(S2 ) AxA(S1 ) S1 S2 . Also one show inductionrrrrAxA (S) iff AxA (S Sig()), OA() compact. Since OA(S)rriff AxA (S), OA () subset-closed union-closed.Example 35 (Example 31 continued) easy see axiom M2 Figure 1syntactically local w.r.t. S1 = {Fibrosis, Genetic Origin}. indicate sub-conceptsCon(S1 ):M2Con(S1 ) [matches ]Con(S1 ) [matches R .C]z}|{z}|{Genetic Fibrosis Fibrosis u Origin.Genetic Origin(9)|{z}Con(S1 ) [matches C u C ]easy show similar way axioms P1 P4, E1 Figure 1 syntactically local w.r.t. = {Cystic Fibrosis, Genetic Disorder}. Hence ontology P1 ={P1, . . . , P4, E1} considered Example 29 syntactically local w.r.t. S.298fiModular Reuse Ontologies: Theory Practicer (S)IAr, 6 :rIA(S)rIA(S)rid (S)IArJAJr (S)IArIA(S)rIA(S)rid (S)IAJ J{hx, xi | x J }r, 6 :rJAJJJ J{hx, xi | x J }JJTable 3: Examples Different Local Classes Interpretationsconverse Proposition 33 hold general since semanticallylocal axioms syntactically local. example, axiom = (A v B)local w.r.t. every since tautology (and hence true every interpretation).hand, easy see syntactically local w.r.t. = {A, B} accordingDefinition 32 since involves symbols only. Another example, tautology,GCI = (r.A v r.B). axiom semantically local w.r.t. = {r}, since(, S) = (r. v r.) tautology, syntactically local. examples showlimitation syntactic notion locality inability compare differentoccurrences concepts given signature S. result, syntactic localitydetect tautological axioms. reasonable assume, however, tautological axiomsoccur often realistic ontologies. Furthermore, syntactic locality checkingperformed polynomial time matching axiom according Definition 32.Proposition 36 exists algorithm given SHOIQ ontology sigrnature S, determines whether OA(S), whose running time polynomial|O| + |S|, |O| |S| number symbols occurring respectively.45.5 Locality Classesrlocality condition given Example 26 based class local interpretations IA()particular example locality used testing safety. classeslocal interpretations constructed similar way fixing interpretationselements outside different values. Table 3 listed several classeslocal interpretations fix interpretation atomic roles outside eitherempty set , universal relation , identity relation id ,interpretation atomic concepts outside either empty set setelements.local class interpretations Table 3 defines corresponding class localontologies analogously Example 26. Table 4 listed classes togetherexamples typical types axioms used ontologies. axioms assumedextension project ontology Figure 1. indicate axiomslocal w.r.t. locality conditions assuming, usual, symbolsunderlined.seen Table 4 different types locality conditions appropriaterdifferent types axioms. locality condition based IA(S) captures domainaxiom P4, definition P5, disjointness axiom P6, functionality axiom P74. assume numbers number restrictions written using binary coding.299fiCuenca Grau, Horrocks, Kazakov, & Sattlerrrridrrrid377333333777P6 Project u Bio Medicine v333777P7 Funct(has Focus)373373P8 Human Genome : Project777333P9 Focus(Human Genome, Gene)737737777777? AxAxiomP4 Focus.> v ProjectP5E2BioMedical Project Project uu Focus.Bio MedicineFocus.Cystic Fibrosis vv Focus.Cystic FibrosisTable 4: Comparison Different Types Locality Conditionsneither assertions P8 P9, since individuals Human Genome Gene preventus interpreting atomic role Focus atomic concept Project emptyr(S), atomic roles concepts outsideset. locality condition based IAinterpreted largest possible sets, capture assertions generallypoor types axioms. example, functionality axiom P7 capturedlocality condition since atomic role Focus interpreted universalrelation , necessarily functional. order capture functionalityrid (S) rid (S), every atomic role outsideaxioms, one use locality based IAinterpreted identity relation id interpretation domain. Notemodeling error E2 local given locality conditions. Note alsopossible come locality condition captures axioms P4P9, sinceP6 P8 together imply axiom = ({Human Genome} u Bio Medicine v )uses symbols only. Hence, every subset P containing P6 P8 safew.r.t. S, cannot local w.r.t. S.might possible come algorithms testing locality conditionsclasses interpretation Table 3 similar ones presented Proposition 30.rexample, locality based class IA(S) tested Proposition 30,case (a) definition (C, S) replaced following:(A, S)= >/ otherwise =(a0 )rrid (S), checkingremaining classes interpretations, IA(S) IAlocality, however, straightforward, since clear eliminateuniversal roles identity roles axioms preserve validity respectiveclasses interpretations.Still, easy come tractable syntactic approximations localityconditions considered section similar manner done Section 5.4. idea used Definition 32, namely define two setsCon(S) Con(S) concepts signature interpreted empty300fiModular Reuse Ontologies: Theory PracticeCon(S) ::= C | C u C | C u CCon(S) ::= > | C | C1 u C2| R.C | > n R.Cr ():IA|r ():IA|rIA():| R .C | (> n R .C )rIA():| R .C | > n R .Crid ():IA| Rid .C | (> 1 Rid .C ) .rid ():IA| (> Rid .C), 2 .r (S) ::= C v C | C v C | : CAxArIA():| R v R | Trans(r ) | Funct(R )rIA():| R v R | Trans(r ) | r (a, b)rid ():IA| Trans(rid ) | Funct(Rid )Where:, , r , r , rid 6 S;Sig(R ), Sig(R ), Sig(Rid ) * S;Con(S);C Con(S), C(i)C concept, R roleFigure 3: Syntactic Locality Conditions Classes Interpretations Table 3set and, respectively, every interpretation class see situations DL-constructors produce elements sets. Figure 3 gave recursiver (S) correspond classes r (S)definitions syntactically local axioms AxAinterpretations Table 3, cases recursive definitions presentindicated classes interpretations.5.6 Combining Extending Safety Classesprevious section gave examples several safety classes based different localclasses interpretations demonstrated different classes suitable differenttypes axioms. order check safety ontologies practice, one may try applydifferent sufficient tests check succeeds. Obviously, givespowerful sufficient condition safety, seen union safety classesused tests.Formally, given two classes ontologies O1 () O2 (), union (O1 O2 )()class ontologies defined (O1 O2 )(S) = O1 (S)O2 (S). easy see Definition 24O1 () O2 () safety classes union (O1 O2 )() safetyclass. Moreover, safety classes also anti-monotonic subset-closed,union anti-monotonic, respectively, subset-closed well. Unfortunately unionclosure property safety classes preserved unions, demonstratedfollowing example:rrExample 37 Consider union (OAOA)() two classes local ontologiesrrOA () OA () defined Section 5.5. safety class union-closed since,example, ontology O1 consisting axioms P4P7 Table 4 satisfies firstlocality condition, ontology O2 consisting axioms P8P9 satisfies second localitycondition, union O1 O2 satisfies neither first second locality conditionand, fact, even safe shown Section 5.5.shown Proposition 33, every locality condition gives union-closed safety class;however, seen Example 37, union safety classes might longer unionclosed. One may wonder locality classes already provide powerful sufficient301fiCuenca Grau, Horrocks, Kazakov, & Sattlerconditions safety satisfy desirable properties Definition 24. Surprisinglycase certain extent locality classes considered Section 5.5.Definition 38 (Maximal Union-Closed Safety Class). safety class O2 () extendssafety class O1 () O1 (S) O2 (S) every S. safety class O1 () maximal unionclosed language L O1 () union-closed every union-closed safety class O2 ()extends O1 () every L O2 (S) implies O1 (S).rrProposition 39 classes local ontologies OA() OA() defined Section 5.5maximal union-closed safety classes L = SHIQ.Proof. According Definition 38, safety class O() maximal union-closedlanguage L, exists signature ontology L, (i)/O(S), (ii) safe w.r.t. L, (iii) every P O(S), case Psafe w.r.t. L; is, every ontology Q L Sig(Q) Sig(O P)case P Q deductive conservative extension Q. demonstraterrpossible O() = OA() O() = OA()rfirst consider case O() = OA () show modify proofrcase O() = OA().Let ontology L satisfies conditions (i)(iii) above. define ontologiesP Q follows. Take P consist axioms v r.> v every atomicrconcept atomic role r Sig(O) \ S. easy see P OA(S). Take Qconsist tautologies form v v r.> every A, r S. NoteSig(O P) Sig(Q) S. claim P Q deductive Sig(Q)-conservativeextension Q.(])rIntuitively, ontology P chosen way P OA(S) P Qrmodels IA(S). Q ontology implies nothing tautologiesuses atomic concepts roles S.rrSince/ OA(S), exists axiom/ AxA(S). Let:= (, S) (, ) defined Proposition 30. shown proofrproposition, |= iff |= every IA(S). Now, since |= P Qrmodels IA (S), case P Q |= . Proposition 30, sincercontain individuals, Sig() = Sig(Q) and, since/ AxA(S),tautology, thus Q 6|= . Hence, Definition 1, P Q deductiveSig(Q)-conservative extension Q (]).rO() = OA() proof repeated taking P consist axioms > vr.> v A, r Sig(O) \ S, modifying (, ) discussedSection 5.5.difficulties extending proof Proposition 39 localityclasses considered Section 5.5. First, clear force interpretations rolesuniversal identity relation using SHOIQ axioms. Second, cleardefine function (, ) cases (see related discussion Section 5.5). Notealso proof Proposition 39 work presence nominals, sinceguarantee = (, S) contains symbols (see Footnote 3rrp. 297). Hence probably room extend locality classes OA() OA()L = SHOIQ preserving union-closure.302fiModular Reuse Ontologies: Theory Practice6. Extracting Modules Using Safety Classessection revisit problem extracting modules ontologies. shownCorollary 23 Section 4, exists general procedure recognize extract(minimal) modules signature ontology finite time.techniques described Section 5, however, reused extracting particularfamilies modules satisfy certain sufficient conditions. Proposition 18 establishesrelationship notions safety module; precisely, subset O1S-module provided \ O1 safe Sig(O1 ). Therefore, safety classO() provide sufficient condition testing modulesthat is, order proveO1 S-module O, sufficient show \ O1 O(S Sig(O1 )). notionmodules based property defined follows.Definition 40 (Modules Based Safety Class).Let L ontology language O() safety class L. Given ontologysignature L, say Om O()-based S-module \ OmO(S Sig(Om )).Remark 41 Note every safety class O(), ontology signature S, existsleast one O()-based S-module O, namely itself; indeed, Definition 24, emptyontology = \ also belongs O(S) every O() S.Note also follows Definition 40 Om O()-based S-module iffOm O()-based S0 -module every S0 S0 (S Sig(Om )).clear that, according Definition 40, procedure checking membershipsafety class O() used directly checking whether Om module based O().order extract O()-module, sufficient enumerate possible subsetsontology check subsets module based O().practice, however, possible avoid checking possible subsetsinput ontology. Figure 4 presents optimized version module-extraction algorithm.procedure manipulates configurations form Om | Ou | Os , representpartitioning ontology three disjoint subsets Om , Ou Os . set Omaccumulates axioms extracted module; set Os intended safe w.r.t.Sig(Om ). set Ou , initialized O, contains unprocessed axioms.axioms distributed among Om Os according rules R1 R2. Given axiomOu , rule R1 moves Os provided Os remains safe w.r.t. Sig(Om ) accordingsafety class O(). Otherwise, rule R2 moves Om moves axiomsOs back Ou , since Sig(Om ) might expand axioms Os might become longersafe w.r.t. Sig(Om ). end process, axioms left Ou ,set Om O()-based module O.rewrite rules R1 R2 preserve invariants I1I3 given Figure 4. Invariant I1states three sets Om , Ou Os form partitioning O; I2 states set Ossatisfies safety test Sig(Om ) w.r.t. O(); finally, I3 establishes rewriterules either add elements Om , add elements Os without changing Om ;words, pair (|Om |, |Os |) consisting sizes sets increases lexicographicalorder.303fiCuenca Grau, Horrocks, Kazakov, & SattlerInput:ontology O, signature S, safety class O()Output:module Om based O()unprocessedConfiguration: Om | Ou | Os ;modulesafeInitial Configuration =|O|Termination Condition: Ou =Rewrite rules:R1. Om | Ou {} | Os = Om | Ou | Os {}(Os {}) O(S Sig(Om ))R2. Om | Ou {} | Os = Om {} | Ou Os |(Os {}) 6 O(S Sig(Om ))Invariants Om | Ou | Os :I1. = Om ] Ou ] Os0 | O0 | O0 :Invariant Om | Ou | Os = Omu0 |, |O 0 |)I3. (|Om |, |Os |) <lex (|OmI2. Os O(S Sig(Om ))Figure 4: Procedure Computing Modules Based Safety Class O()Proposition 42 (Correctness Procedure Figure 4) Let O() safetyclass ontology language L, ontology L, signature L. Then:(1) procedure Figure 4 input O, S, O() terminates returns O()based S-module Om O;(2) If, additionally, O() anti-monotonic, subset-closed union-closed,unique minimal O()-based S-module O, procedure returns preciselyminimal module.Proof. (1) procedure based rewrite rules Figure 4 always terminatesfollowing reasons: (i) every configuration derived rewrite rules, sets Om ,Ou Os form partitioning (see invariant I1 Figure 4), therefore sizeevery set bounded; (ii) rewrite step, (|Om |, |Os |) increases lexicographicalorder (see invariant I3 Figure 4). Additionally, Ou 6= always possibleapply one rewrite rules R1 R2, hence procedure always terminatesOu = . Upon termination, invariant I1 Figure 4, partitioned Om Osinvariant I2, Os O(S Sig(Om )), implies, Definition 40, OmO()-based S-module O.(2) Now, suppose that, addition, O() anti-monotonic, subset-closed, union0 O()-based S-module O. demonstrateclosed safety class, suppose Ominduction every configuration Om | Ou | Os derivable | | rewrite0 . prove module computedrules R1 R2, case Om Omprocedure subset every O()-based S-module O, hence, smallestO()-based S-module O.0 . rewrite rule R1 changeIndeed, base case Om = Omset Om . rewrite rule R2 have: Om | Ou {} | Os = Om {} | Ou Os |(Os {}) 6 O(S Sig(Om )).(])304fiModular Reuse Ontologies: Theory PracticeInput: ontology O, signature S, safety class O()Output: module Om based O()Initial Configuration =|O|Termination Condition: Ou =Rewrite rules:R1.Om | Ou {} | Os = Om | Ou | Os {}{} O(S Sig(Om ))R2. Om | Ou {} | Os Os = Om {} | Ou Os | Os {} 6 O(S Sig(Om )),Sig(Os ) Sig() Sig(Om )Figure 5: Optimized Procedure Computing Modules Based Compact SubsetClosed Union-Closed Safety Class O()0 {} * 0 . 0 := \ 0 .Suppose, contrary, Om Om0 O()-based S-module O, 0 O(S Sig(O 0 )). Since O()Since Om0 )). Since0subset-closed, {} O(S Sig(OmOm O() anti-monotonic,{} O(S Sig(Om )). Since invariant I2 Figure 4, Os O(S Sig(Om ))O() union-closed, Os {} O(S Sig(Om )), contradicts (]). contradiction0 .implies rule R2 also preserves property Om OmClaim (1) Proposition 42 establishes procedure Figure 4 terminatesevery input produces module based given safety class. Moreover, possibleshow procedure runs polynomial time assuming safety testalso performed polynomial time.safety class O() satisfies additional desirable properties, like based classeslocal interpretations described Section 5.2, procedure, fact, produces smallestpossible module based safety class, stated claim (2) Proposition 42.case, possible optimize procedure shown Figure 4. O() union closed,then, instead checking whether (Os {}) O(S Sig(Om )) conditions rulesR1 R2, sufficient check {} O(S Sig(Om )) since already knownOs O(S Sig(Om )). O() compact subset closed, instead movingaxioms Os Ou rule R2, sufficient move axioms Os containleast one symbol occur Om before, since set remainingaxioms stay O(S Sig(Om )). Figure 5 present optimized versionalgorithm Figure 4 locality classes.Example 43 Table 5 present trace algorithm Figure 5 ontologyconsisting axioms M1M5 Figure 1, signature = {Cystic Fibrosis, Genetic Disorder}rsafety class O() = OA() defined Example 26. first column table listsconfigurations obtained initial configuration | | applying rewriterules R1 R2 Figure 5; row, underlined axiom onetested safety. second column table shows elements Sig(Om )appeared current configuration present precedingconfigurations. last column indicate whether first conditions rules R1rR2 fulfilled selected axiom Ou is, whether local IA().rewrite rule corresponding result test applied configuration.305fiCuenca Grau, Horrocks, Kazakov, & SattlerOm | Ou , | OsNew elements Sig(Om )1 | M1, M2, M3, M4, M5 | Cystic Fibrosis, Genetic Disorder{} O(S Sig(Om ))?YesR12| M1, M3, M4, M5 | M2YesR13| M1, M4, M5 | M2, M3R24M1 | M2, M3, M4, M5 |Fibrosis, located In, Pancreas,Origin, Genetic OriginR25M1, M3 | M2, M4, M5 |Genetic FibrosisR26M1, M3, M4 | M2, M5 |YesR17M1, M3, M4 | M2 | M5R28M1, M2, M3, M4 | | M5Table 5: trace Procedure Figure 5 input Q = {M1, . . . , M5} Figure 1= {Cystic Fibrosis, Genetic Disorder}Note axioms tested safety several times different configurations,set Ou may increase applications rule R2; example, axiom= M2 tested safety configurations 1 7, = M3 configurations2 4. Note also different results locality tests obtained cases:M2 M3 local w.r.t. Sig(Om ) Om = , became non-localnew axioms added Om . also easy see that, case, syntactic localityproduces results tests.example, rewrite procedure produces module Om consisting axioms M1M4. Note possible apply rewrite rules different choices axiomOu , results different computation. words, procedureFigure 5 implicit non-determinism. According Claim (2) Proposition 42rcomputations produce module Om , smallest OA()-basedS-module O; is, implicit non-determinism procedure Figure 5impact result procedure. However, alternative choicesmay result shorter computations: example could selected axiom M1first configuration instead M2 would led shorter trace consistingconfigurations 1, 48 only.worth examining connection S-modules ontology basedparticular safety class O() actual minimal S-modules O. turnsO()-based module Om guaranteed cover set minimal modules, providedO() anti-monotonic subset-closed. words, given S, Om containsS-essential axioms O. following Lemma provides main technical argumentunderlying result.Lemma 44 Let O() anti-monotonic subset-closed safety class ontology language L, ontology, signature L. Let O1 S-module w.r.t. LOm O()-based S-module O. O2 := O1 Om S-module w.r.t. L.306fiModular Reuse Ontologies: Theory PracticeProof. Definition 40, since Om O()-based S-module O, \ OmO(S Sig(Om )). Since O1 \ O2 = O1 \ Om \ Om O() subset-closed, caseO1 \ O2 O(S Sig(Om )). Since O() anti-monotonic, O2 Om ,O1 \ O2 O(S Sig(O2 )), hence, O2 O()-based S-module O1 . particular O2S-module O1 w.r.t. L. Since O1 S-module w.r.t. L, O2 S-modulew.r.t. L.Corollary 45 Let O() anti-monotonic, subset-closed safety class L OmO()-based S-module O. Om contains S-essential axioms w.r.t. L.Proof. Let O1 minimal S-module w.r.t. L. demonstrate O1 Om . Indeed,otherwise, Lemma 44, O1 Om S-module w.r.t. L strictly containedO1 . Hence Om superset every minimal S-module hence, containsS-essential axioms w.r.t. L.shown Section 3.4, axioms M1M4 essential ontologysignature considered Example 43. seen example locality-basedS-module extracted contains axioms, accordance Corollary 45.case, extracted module contains essential axioms; general, however, localitybased modules might contain non-essential axioms.interesting application modules pruning irrelevant axioms checkingaxiom implied ontology O. Indeed, order check whether |=suffices retrieve module Sig() verify implication holds w.r.t.module. cases, sufficient extract module subset signaturewhich, general, leads smaller modules. particular, order test subsumptionpair atomic concepts, safety class used enjoys nice propertiessuffices extract module one them, given following proposition:Proposition 46 Let O() compact union-closed safety class ontology languageL, ontology A, B atomic concepts. Let OA O()-based module= {A} O. Then:1 |= := (A v B) {B v } O() OA |= ;2 |= := (B v A) {> v B} O() OA |= ;Proof. 1. Consider two cases: (a) B Sig(OA ) (b) B/ Sig(OA ).(a) Remark 41 case OA O()-based module = {A, B}.Since Sig() S, Definition 12 Definition 1, case |= impliesOA |= .(b) Consider O0 = {B v }. Since Sig(B v ) = {B} B/ Sig(OA ), {B v }O() O() compact, Definition 24 case {B v } O(S Sig(OA )).Since OA O()-based S-module O, Definition 40, \ OA O(S Sig(OA )).Since O() union-closed, case (O \ OA ) {B v } O(S Sig(OA )). NoteB v 6 OA , since B 6 Sig(OA ), hence (O \ OA ) {B v } = O0 \ OA , and,Definition 40, OA O()-based S-module O0 . Now, since |= (A v B),case O0 |= (A v ), hence, since OA module O0 = {A},OA |= (A v ) implies OA |= (A v B).307fiCuenca Grau, Horrocks, Kazakov, & Sattler2. proof case analogous Case 1: Case (a) applicable withoutchanges; Case (b) show OA O()-based module = {A} O0 ={> v B}, and, hence, since O0 |= (> v A), case OB |= (> v A),implies O0 |= (B v A).r ()Corollary 47 Let SHOIQ ontology A, B atomic concepts. Let OAr () locality classes based local classes interpretations form r ()OArr ()IA (), respectively, Table 3. Let OA module = {A} based OAr (). |= (A v B) iff |= (A v B) iffOB module = {B} based OAOB |= (A v B).r () {> v A} r ().Proof. easy see {B v } OAProposition 46 implies module based safety class single atomic conceptused capturing either super-concepts (Case 1), sub-concepts (Case2) B A, provided safety class captures, applied empty signature,axioms form B v (Case 1) (> v B) (Case 2). is, B super-conceptsub-concept ontology module. propertyused, example, optimize classification ontologies. order checksubsumption v B holds ontology O, sufficient extract module= {A} using modularization algorithm based safety classontology {B v } local w.r.t. empty signature, check whether subsumptionholds w.r.t. module. purpose, convenient use syntactically tractableapproximation safety class use; example, one could use syntactic localityconditions given Figure 3 instead semantic counterparts.possible combine modularization procedures obtain modules smallerones obtained using procedures individually. example, order checkr ()-based modulesubsumption |= (A v B) one could first extract OA1= {A} O; Corollary 47 module complete super-conceptsO, including Bthat is, atomic concept super-concept O, alsor ()-based module = {B}super-concept M1 . One could extract OA2M1 which, Corollary 47, complete sub-concepts B M1 , including A.Indeed, M2 S-module M1 = {A, B} M1 S-module originalontology O. Proposition 46, therefore, case M2 also S-module O.7. Related Workseen Section 3 notion conservative extension valuable formalization ontology reuse tasks. problem deciding conservative extensionsrecently investigated context ontologies (Ghilardi et al., 2006; Lutz et al., 2007;Lutz & Wolter, 2007). problem deciding whether P Q deductive S-conservativeextension Q EXPTIME-complete EL (Lutz & Wolter, 2007), 2-EXPTIME-completew.r.t. ALCIQ (Lutz et al., 2007) (roughly OWL-Lite), undecidable w.r.t. ALCIQO(roughly OWL DL). Furthermore, checking model conservative extensions already undecidable EL (Lutz & Wolter, 2007), ALC even semi-decidable (Lutzet al., 2007).308fiModular Reuse Ontologies: Theory Practicelast years, rapidly growing body work developedheadings Ontology Mapping Alignment, Ontology Merging, Ontology Integration,Ontology Segmentation (Kalfoglou & Schorlemmer, 2003; Noy, 2004a, 2004b). fieldrather diverse roots several communities.particular, numerous techniques extracting fragments ontologies purposes knowledge reuse proposed. techniques rely syntacticallytraversing axioms ontology employing various heuristics determineaxioms relevant not.example procedure algorithm implemented Prompt-Factortool (Noy & Musen, 2003). Given signature ontology Q, algorithm retrieves fragment Q1 Q follows: first, axioms Q mentionsymbols added Q1 ; second, expanded symbols Sig(Q1 ).steps repeated fixpoint reached. example Section 3, ={Cystic Fibrosis, Genetic Disorder}, Q consists axioms M1M5 Figure 1, algorithm first retrieves axioms M1, M4, M5 containing terms, expandssymbols mentioned axioms, contains symbols Q.step, remaining axioms Q retrieved. Hence, fragment extractedPrompt-Factor algorithm consists axioms M1-M5. case, PromptFactor algorithm extracts module (though minimal one). general, however,extracted fragment guaranteed module. example, consider ontologyQ = {A A, B v C} = (C v B). ontology Q inconsistent dueaxiom A: axiom (and particular) thus logical consequence Q. Given= {B, C}, Prompt-Factor algorithm extracts Q2 = {B v C}; however, Q2 6|= ,Q2 module Q. general, Prompt-Factor algorithm may fail even Qconsistent. example, consider ontology Q = {> v {a}, v B}, = (A v r.A),= {A}. easy see Q consistent, admits single element models,satisfied every model; is, Q |= . case, Prompt-Factoralgorithm extracts Q1 = {A v B}, imply .Another example Seidenbergs segmentation algorithm (Seidenberg & Rector, 2006),used segmentation medical ontology GALEN (Rector & Rogers, 1999).Currently, full version GALEN cannot processed reasoners, authorsinvestigate possibility splitting GALEN small segments processedreasoners separately. authors describe segmentation procedure which, given setatomic concepts S, computes segment ontology. descriptionprocedure high-level. authors discuss concepts rolesincluded segment not. particular, segment containsuper- sub- concepts input concepts, concepts linkedinput concepts (via existential restrictions) super-concepts, subconcepts; included concepts, also restrictions, intersection, union,equivalent concepts considered including roles concepts contain,together super-concepts super-roles sub-conceptssub-roles. description procedure entirely clear whether worksclassified ontology (which unlikely case GALEN since full versionGALEN classified existing reasoner), or, otherwise, super-sub- concepts computed. also clear axioms included309fiCuenca Grau, Horrocks, Kazakov, & Sattlersegment end, since procedure talks inclusion conceptsroles.different approach module extraction proposed literature (Stuckenschmidt& Klein, 2004) consists partitioning concepts ontology facilitate visualizationnavigation ontology. algorithm uses set heuristics measuringdegree dependency concepts ontology outputs graphicalrepresentation dependencies. algorithm intended visualization technique,establish correspondence nodes graph sets axiomsontology.common modularization procedures mentioned lackformal treatment notion module. papers describing modularizationprocedures attempt formally specify intended outputs procedures,rather argue modules based intuitive notions.particular, take semantics ontology languages account. mightpossible formalize algorithms identify ontologies intuitionbased modularization procedures work correctly. studies beyond scopepaper.Module extraction ontologies also investigated formal point view(Cuenca Grau et al., 2006b). Cuenca Grau et al. (2006) define notion module QAontology Q atomic concept A. One requirements module Qconservative extension QA (in paper QA called logical module Q).paper imposes additional requirement modules, namely module QAentail subsumptions original ontology atomic concepts involvingatomic concepts QA . authors present algorithm partitioning ontologydisjoint modules proved algorithm correct provided certain safetyrequirements input ontology hold: ontology consistent,contain unsatisfiable atomic concepts, safe axioms (whichterms means local empty signature). contrast, algorithmpresent works ontology, including containing non-safe axioms.growing interest notion modularity ontologies recentlyreflected workshop modular ontologies5 held conjunction InternationalSemantic Web Conference (ISWC-2006). Concerning problem ontology reuse,various proposals safely combining modules; proposals,E-connections (Cuenca Grau, Parsia, & Sirin, 2006a), Distributed Description Logics(Borgida & Serafini, 2003) Package-based Description Logics (Bao, Caragea, & Honavar,2006) propose specialized semantics controlling interaction importingimported modules avoid side-effects. contrast works, assumereuse performed simply building logical union axioms modulesstandard semantics, establish collection reasoning services,safety testing, check side-effects. interested reader find literaturedetailed comparison different approaches combining ontologies (CuencaGrau & Kutz, 2007).5. information see homepage workshop http://www.cild.iastate.edu/events/womo.html310fiModular Reuse Ontologies: Theory Practice8. Implementation Proof Conceptsection, provide empirical evidence appropriateness locality safetytesting module extraction. purpose, implemented syntactic locarrlity checker locality classes OA() OA() well algorithmextracting modules given Figure 5 Section 6.rFirst, show locality class OA() provides powerful sufficiency testrsafety works many real-world ontologies. Second, show OA()-basedmodules typically small compared size ontology modulesextracted using techniques. Third, report implementation ontologyeditor Swoop (Kalyanpur, Parsia, Sirin, Cuenca Grau, & Hendler, 2006) illustraterrcombination modularization procedures based classes OA() OA().8.1 Locality Testing Safetyrrun syntactic locality checker class OA() ontologieslibrary 300 ontologies various sizes complexity import(Gardiner, Tsarkov, & Horrocks, 2006).6 ontologies P import ontology Q,check P belongs locality class = Sig(P) Sig(Q).turned 96 ontologies library import ontologies,11 syntactically local (and hence also semantically local S). 11non-local ontologies, 7 written OWL-Full species OWL (Patel-Schneider et al.,2004) framework yet apply. remaining 4 non-localities duepresence so-called mapping axioms form B 0 ,/ B 0 S.Note axioms simply indicate atomic concepts A, B 0 two ontologiesconsideration synonyms. Indeed, able easily repair non-localitiesfollows: replace every occurrence P B 0 remove axiomontology. transformation, 4 non-local ontologies turned local.8.2 Extraction Modulessection, compare three modularization7 algorithms implementedusing Manchesters OWL API:8A1: Prompt-Factor algorithm (Noy & Musen, 2003);A2: segmentation algorithm proposed Cuenca Grau et al. (2006);rA3: modularisation algorithm (Algorithm 5), based locality class OA().aim experiments described section provide throughout comparison quality existing modularization algorithms since algorithm extractsmodules according requirements, rather give idea typical sizemodules extracted real ontologies algorithms.6. library available http://www.cs.man.ac.uk/~horrocks/testing/7. section module understand result considered modularization proceduresmay necessarily module according Definition 10 128. http://sourceforge.net/projects/owlapi311fiCuenca Grau, Horrocks, Kazakov, & Sattler(a) ModularizationNCI(a)ModularizationNCI(b) ModularizationModularizationGALEN-Small(b)GALEN-Small(c) ModularizationModularization SNOMED(c)SNOMED(d) ModularizationModularization GALEN-Full(d)GALEN-Full(e) SmallSmall modulesGALEN-Full(e)modulesGALEN-Full(f) LargeLarge modulesGALEN-Full(f)modulesGALEN-FullFigure 6: Distribution sizes syntactic locality-based modules: X-Axis givesnumber concepts modules Y-Axis number modules extractedsize range.312fiModular Reuse Ontologies: Theory PracticeA2: SegmentationA3: Loc.-based mod.] AtomicA1: Prompt-FactorConceptsMax.(%)Avg.(%)Max.(%)Avg.(%)Max.(%)Avg.(%)NCI2777287.675.845530.80.80.08SNOMED2553181001001001000.50.05GOOntology2235710.110.10.40.05SUMO86910010010010020.09GALEN-Small2749100100100100101.7GALEN-Full2408910010010010029.83.5SWEET181696.488.783.351.51.90.1DOLCE-Lite49910010010010037.324.6Table 6: Comparison Different Modularization Algorithmstest suite, collected set well-known ontologies available Web,divided two groups:Simple. group, included National Cancer Institute (NCI) Ontology,9SUMO Upper Ontology,10 Gene Ontology (GO),11 SNOMED Ontology12 .ontologies expressed simple ontology language simple structure;particular, contain GCIs, definitions.Complex. group contains well-known GALEN ontology (GALEN-Full),13DOLCE upper ontology (DOLCE-Lite),14 NASAs Semantic Web Earth Environmental Terminology (SWEET)15 . ontologies complex since use manyconstructors OWL DL and/or include significant number GCIs. caseGALEN, also considered version GALEN-Small commonly usedbenchmark OWL reasoners. ontology almost 10 times smaller originalGALEN-Full ontology, yet similar structure.Since benchmark ontology modularization use casesavailable, systematic way evaluating modularization procedures. Thereforedesigned simple experiment setup which, even may necessarily reflectactual ontology reuse scenario, give idea typical module sizes.ontology, took set atomic concepts extracted modules every atomicconcept. compare maximal average sizes extracted modules.worth emphasizing algorithm A3 extract moduleinput atomic concept: extracted fragment also module whole signature,typically includes fair amount concepts roles.9.10.11.12.13.14.15.http://www.mindswap.org/2003/CancerOntology/nciOncology.owlhttp://ontology.teknowledge.com/http://www.geneontology.orghttp://www.snomed.orghttp://www.openclinical.org/prj_galen.htmlhttp://www.loa-cnr.it/DOLCE.htmlhttp://sweet.jpl.nasa.gov/ontology/313fiCuenca Grau, Horrocks, Kazakov, & Sattlerrr(a) Concepts DNA Sequence (b) OA(S)-based module (c) OA(S)-based moduleMicroanatomy NCIDNA Sequence NCIMicro Anatomy fragment 7bFigure 7: Module Extraction Functionality Swoopresults obtained summarized Table 6. table provides sizelargest module average size modules obtained usingalgorithms. table, clearly see locality-based modules significantlysmaller ones obtained using methods; particular, case SUMO,DOLCE, GALEN SNOMED, algorithms A1 A2 retrieve whole ontologymodule input signature. contrast, modules obtain using algorithmsignificantly smaller size input ontology.NCI, SNOMED, GO SUMO, obtained small locality-based modules. explained fact ontologies, even large, simplestructure logical expressivity. example, SNOMED, largest locality-basedmodule obtained approximately 0.5% size ontology, average sizemodules 1/10 size largest module. fact, modulesobtained ontologies contain less 40 atomic concepts.GALEN, SWEET DOLCE, locality-based modules larger. Indeed,largest module GALEN-Small 1/10 size ontology, opposed 1/200case SNOMED. DOLCE, modules even bigger1/3 sizeontologywhich indicates dependencies different conceptsontology strong complicated. SWEET ontology exception: eventhough ontology uses constructors available OWL, ontology heavilyunderspecified, yields small modules.Figure 6, detailed analysis modules NCI, SNOMED,GALEN-Small GALEN-Full. Here, X-axis represents size ranges ob314fiModular Reuse Ontologies: Theory Practicetained modules Y-axis number modules whose size within given range.plots thus give idea distribution sizes different modules.SNOMED, NCI GALEN-Small, observe size modulesfollows smooth distribution. contrast, GALEN-Full, obtained large numbersmall modules significant number big ones, medium-sized modulesin-between. abrupt distribution indicates presence big cycle dependenciesontology. presence cycle spotted clearly Figure 6(f); figureshows large number modules size 6515 6535 concepts.cycle occur simplified version GALEN thus obtain smoothdistribution case. contrast, Figure 6(e) see distributionsmall modules GALEN-Full smooth much similar onesimplified version GALEN.considerable differences size modules extracted algorithms A1A3 due fact algorithms extract modules according different requirements. Algorithm A1 produces fragment ontology contains input atomicconcept syntactically separated rest axiomsthat is, fragmentrest ontology disjoint signatures. Algorithm A2 extracts fragmentontology module input atomic concept additionally semanticallyseparated rest ontology: entailment atomic conceptmodule atomic concept module hold original ontology. Sincealgorithm based weaker requirements, expected extracts smallermodules. surprising difference size modules significant.order explore use results ontology design analysis,integrated algorithm extracting modules ontology editor Swoop (Kalyanpuret al., 2006). user interface Swoop allows selection input signatureretrieval corresponding module.16Figure 7a shows classification concepts DNA Sequence MicroanatomyrNCI ontology. Figure 7b shows minimal OA()-based module DNA Sequence,robtained Swoop. Recall that, according Corollary 47, OA()-based moduleatomic concept contains necessary axioms for, least, (entailed) super-conceptsO. Thus module seen upper ontology O. fact, Figure 7shows module contains concepts path DNA Sequencetop level concept Anatomy Kind. suggests knowledge NCIparticular concept DNA Sequence shallow sense NCI knowsDNA Sequence macromolecular structure, which, end, anatomy kind. onewants refine module including information ontology necessaryrentail path DNA Sequence Micro Anatomy, one could extract OA()based module Micro Anatomy fragment 7b. Corollary 47, module containssub-concepts Micro Anatomy previously extracted module. resultingmodule shown Figure 7b.16. tool downloaded http://code.google.com/p/swoop/315fiCuenca Grau, Horrocks, Kazakov, & Sattler9. Conclusionpaper, proposed set reasoning problems relevant ontologyreuse. established relationships problems studied computability. Using existing results (Lutz et al., 2007) results obtained Section 4,shown problems undecidable algorithmically unsolvable logicunderlying OWL DL. dealt problems defining sufficient conditionssolution exist, computed practice. introduced studiednotion safety class, characterizes sufficiency condition safetyontology w.r.t. signature. addition, used safety classes extract modulesontologies.future work, would like study approximations produce smallmodules complex ontologies like GALEN, exploit modules optimize ontologyreasoning.ReferencesBaader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. IJCAI-05, Proceedings Nineteenth International Joint Conference Artificial Intelligence,Edinburgh, Scotland, UK, July 30-August 5, 2005, pp. 364370. Professional BookCenter.Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (Eds.).(2003). Description Logic Handbook: Theory, Implementation, Applications.Cambridge University Press.Bao, J., Caragea, D., & Honavar, V. (2006). semantics linking importingmodular ontologies. Proceedings 5th International Semantic Web Conference(ISWC-2006), Athens, GA, USA, November 5-9, 2006, Vol. 4273 Lecture NotesComputer Science, pp. 7286.Borger, E., Gradel, E., & Gurevich, Y. (1997). Classical Decision Problem. PerspectivesMathematical Logic. Springer-Verlag. Second printing (Universitext) 2001.Borgida, A., & Serafini, L. (2003). Distributed description logics: Assimilating informationpeer sources. J. Data Semantics, 1, 153184.Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2007). logical frameworkmodularity ontologies. IJCAI-07, Proceedings Twentieth InternationalJoint Conference Artificial Intelligence, Hyderabad, India, January 2007, pp. 298304. AAAI.Cuenca Grau, B., & Kutz, O. (2007). Modular ontology languages revisited. ProceedingsWorkshop Semantic Web Collaborative Knowledge Acquisition, Hyderabad, India, January 5, 2007.Cuenca Grau, B., Parsia, B., & Sirin, E. (2006a). Combining OWL ontologies using Econnections. J. Web Sem., 4 (1), 4059.Cuenca Grau, B., Parsia, B., Sirin, E., & Kalyanpur, A. (2006b). Modularity web ontologies. Proceedings Tenth International Conference Principles Knowledge316fiModular Reuse Ontologies: Theory PracticeRepresentation Reasoning (KR-2006), Lake District United Kingdom, June2-5, 2006, pp. 198209. AAAI Press.Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2007). right amount:extracting modules ontologies. Proceedings 16th International Conference World Wide Web (WWW-2007), Banff, Alberta, Canada, May 8-12, 2007,pp. 717726. ACM.Cuenca Grau, B., Horrocks, I., Kutz, O., & Sattler, U. (2006). ontologies fittogether?. Proceedings 2006 International Workshop Description Logics(DL-2006), Windermere, Lake District, UK, May 30 - June 1, 2006, Vol. 189 CEURWorkshop Proceedings. CEUR-WS.org.Gardiner, T., Tsarkov, D., & Horrocks, I. (2006). Framework automated comparison description logic reasoners. Proceedings 5th International SemanticWeb Conference (ISWC-2006), Athens, GA, USA, November 5-9, 2006, Vol. 4273Lecture Notes Computer Science, pp. 654667. Springer.Ghilardi, S., Lutz, C., & Wolter, F. (2006). damage ontology? case conservative extensions description logics. Proceedings Tenth InternationalConference Principles Knowledge Representation Reasoning (KR-2006),Lake District United Kingdom, June 2-5, 2006, pp. 187197. AAAI Press.Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). SHIQ RDFOWL: making web ontology language. J. Web Sem., 1 (1), 726.Horrocks, I., & Sattler, U. (2005). tableaux decision procedure SHOIQ. ProceedingsNineteenth International Joint Conference Artificial Intelligence (IJCAI-05),Edinburgh, Scotland, UK, July 30-August 5, 2005, pp. 448453. Professional BookCenter.Kalfoglou, Y., & Schorlemmer, M. (2003). Ontology mapping: state art.Knowledge Engineering Review, 18, 131.Kalyanpur, A., Parsia, B., Sirin, E., Cuenca Grau, B., & Hendler, J. A. (2006). Swoop:web ontology editing browser. J. Web Sem., 4 (2), 144153.Lutz, C., Walther, D., & Wolter, F. (2007). Conservative extensions expressive descriptionlogics. Proceedings Twentieth International Joint Conference ArtificialIntelligence (IJCAI-07), Hyderabad, India, January 2007, pp. 453459. AAAI.Lutz, C., & Wolter, F. (2007). Conservative extensions lightweight description logicEL. Proceedings 21st International Conference Automated Deduction(CADE-21), Bremen, Germany, July 17-20, 2007, Vol. 4603 Lecture Notes Computer Science, pp. 8499. Springer.Moller, R., & Haarslev, V. (2003). Description logic systems. Description LogicHandbook, chap. 8, pp. 282305. Cambridge University Press.Motik, B. (2006). Reasoning Description Logics using Resolution DeductiveDatabases. Ph.D. thesis, Univesitat Karlsruhe (TH), Karlsruhe, Germany.Noy, N. F. (2004a). Semantic integration: survey ontology-based approaches. SIGMODRecord, 33 (4), 6570.317fiCuenca Grau, Horrocks, Kazakov, & SattlerNoy, N. F. (2004b). Tools mapping merging ontologies. Staab, & Studer (Staab& Studer, 2004), pp. 365384.Noy, N., & Musen, M. (2003). PROMPT suite: Interactive tools ontology mappingmerging. Int. Journal Human-Computer Studies, Elsevier, 6 (59).Patel-Schneider, P., Hayes, P., & Horrocks, I. (2004). Web ontology language OWL AbstractSyntax Semantics. W3C Recommendation.Rector, A., & Rogers, J. (1999). Ontological issues using description logic representmedical concepts: Experience GALEN. IMIA WG6 Workshop, Proceedings.Schmidt-Schau, M., & Smolka, G. (1991). Attributive concept descriptions complements. Artificial Intelligence, Elsevier, 48 (1), 126.Seidenberg, J., & Rector, A. L. (2006). Web ontology segmentation: analysis, classificationuse. Proceedings 15th international conference World Wide Web(WWW-2006), Edinburgh, Scotland, UK, May 23-26, 2006, pp. 1322. ACM.Sirin, E., & Parsia, B. (2004). Pellet system description. Proceedings 2004 International Workshop Description Logics (DL2004), Whistler, British Columbia,Canada, June 6-8, 2004, Vol. 104 CEUR Workshop Proceedings. CEUR-WS.org.Staab, S., & Studer, R. (Eds.). (2004). Handbook Ontologies. International HandbooksInformation Systems. Springer.Stuckenschmidt, H., & Klein, M. (2004). Structure-based partitioning large class hierarchies. Proceedings Third International Semantic Web Conference (ISWC2004), Hiroshima, Japan, November 7-11, 2004, Vol. 3298 Lecture Notes Computer Science, pp. 289303. Springer.Tobies, S. (2000). complexity reasoning cardinality restrictions nominalsexpressive description logics. J. Artif. Intell. Res. (JAIR), 12, 199217.Tsarkov, D., & Horrocks, I. (2006). FaCT++ description logic reasoner: System description.Proceedings Third International Joint Conference Automated Reasoning(IJCAR 2006), Seattle, WA, USA, August 17-20, 2006, Vol. 4130 Lecture NotesComputer Science, pp. 292297. Springer.318fiJournal Artificial Intelligence Research 31 (2008) 83-112Submitted 06/07; published 01/08CUI Networks: Graphical Representation ConditionalUtility IndependenceYagil EngelMichael P. Wellmanyagil@umich.eduwellman@umich.eduUniversity Michigan, Computer Science & Engineering2260 Hayward St, Ann Arbor, MI 48109-2121, USAAbstractintroduce CUI networks, compact graphical representation utility functionsmultiple attributes. CUI networks model multiattribute utility functions usingwell-studied widely applicable utility independence concept. show conditionalutility independence leads effective functional decomposition exhibitedgraphically, local, compact data graph nodes used calculatejoint utility. discuss aspects elicitation, network construction, optimization,contrast new representation previous graphical preference modeling.1. IntroductionModern AI decision making based notion expected utility, probability distributions used weigh utility values possible outcomes.representation probability distribution functions Markov Bayesian networks(Pearl, 1988)exploiting conditional independence achieve compactness computational efficiencyhas led plethora new techniques applications. Despiteequal importance decision making, preferences utilities generally receivedlevel attention AI researchers devoted beliefs probabilities.(increasing) efforts develop representations inference methods utility achieveddegree success comparable impact graphical models probabilistic reasoning.Recognizing utility functions multidimensional domains may also amenablefactoring based independence (Keeney & Raiffa, 1976), several aimed developmodels analogous benefits (Bacchus & Grove, 1995; Boutilier, Bacchus, & Brafman,2001; La Mura & Shoham, 1999; Wellman & Doyle, 1992). goal well,compare approach methods Related Work section (2.2).development compact representations multiattribute utility beginsnotion preferential independence (PI), separability subdomains outcome space.subdomain outcomes separable PI sense preference order subdomain depend rest domain. subsets attributes induceseparable subdomains, ordinal utility (value) function decomposes additivelyvariables (Debreu, 1959; Fishburn, 1965; Gorman, 1968). cardinal utility function represents preferences outcomes also notion strength preferences,notably represent preferences actions uncertain outcomes, lotteries. Directadaptation PI concept cardinal utility requires generalization notion:set attributes utility independent (UI) preference order lotteriesc2008AI Access Foundation. rights reserved.fiEngel & Wellmaninduced subdomain depend values rest attributes. strongerjudgement assert preference order joint domain dependsmargins attribute subsets. latter leads powerful additive decompositions,either fully additive (when subsets attributes disjoint), generalized,additive decomposition overlapping subsets (Fishburn, 1967; Bacchus & Grove, 1995).Utility independence leads less convenient decompositions, multilinear (Keeney& Raiffa, 1976) hierarchical (Von Stengel, 1988; Wellman & Doyle, 1992). previousefforts AI community adapt modern graphical modeling utility functions employ generalized additive decomposition (Bacchus & Grove, 1995; Boutilier et al., 2001;Gonzales & Perny, 2004). contrast, work continues thread, basedweaker utility independence assumption. elaborate difference typesindependence following presentation formal definitions.2. Backgroundutility-theoretic terminology follows definitive text Keeney Raiffa (1976).multiattribute utility framework, outcome represented vector valuesn variables, called attributes. decision makers preferences representedtotal pre-order, , set outcomes. common applications decision makersability choose certain outcome, rather action resultsprobability distribution outcomes, also called lottery. decision maker henceset possible lotteries. Given standard setassumed preference orderaxioms, represented real-valued utility function outcomes, U (),numeric ranking probabilistic outcomes expected utility respects orderingutility function unique positive affine transformations. positive linear.transform U () represents preferences, thus strategically equivalent.ability represent utility probability distributions function outcomesprovides structure, multiattribute settings outcome space n-dimensional.Unless n quite small, therefore, explicit (e.g., tabular) representation U ()generally practical. Much research multiattribute utility theory aimsidentify structural patterns enable compact representations. particular,subsets attributes respect various independence relationships, utility function maydecomposed combinations modular subutility functions smaller dimension.Let = {x1 , . . . , xn } set attributes. following definitions (and restwork) capital letters denote subsets attributes, small letters (with without numericsubscripts) denote specific attributes, X denotes complement X respectS. denote (joint) domain X D(X), indicate specific attribute assignmentsprime signs superscripts. represent instantiation subsets Xtime use sequence instantiation symbols, X 0 0 .order meaningfully discuss preferences subsets attributes, need notionpreferences subset given fixed values rest attributes.00Definition 1. Outcome 0 conditionally preferred outcome 00 given , 00000 . denote conditional preference order given 0 .84fiCUI networks0Similarly define conditional preference order lotteries. preference order0lotteries represented conditional utility function, U (Y, ).Definition 2. Preferential Independent (PI) 0 depend value0chosen .Preferential independence useful qualitative preference assessment. Firstorder preferential independence (i.e., independence single attribute rest)natural assumption many domains. example, typical purchase decisions greaterquantity higher quality desirable regardless values attributes. Preferential independence higher order, however, requires invariance tradeoffs amongattributes respect variation others, stringentthough still oftensatisfiableindependence condition. standard PI condition applies subset respect full complement remaining attributes. conditional version PI specifiesindependence respect subset complement, holding remaining attributesfixed.Definition 3. Conditionally Preferential Independent (CPI) X given Z (Z = XY ),Z 0 , X 0 Z 0 depend value chosen X 0 . denote relationshipCPI(Y, X | Z).counterpart preferential independence considers probability distributionsoutcomes called utility independence.Definition 4. Utility Independent (UI) , conditional preference order0 , depend value chosen 0 .lotteries ,notations, apply UI conditions defined sets attributesspecific attributes.Given UI(Y, X), taking X = , conditional utility function given X 0invariant positive affine transformations, fixed value X 0 . factexpressed decompositionU (X, ) = f (X) + g(X)U (X 0 , ),g() > 0.Note functions f () g() may different particular choice X 0 . SinceU (X 0 , ) function , sometimes use notation UX 0 (Y ).Utility independence conditional version well.Definition 5. Conditionally Utility Independent (CUI) X given Z (Z = XY )X 0 Z 0 depend value chosen X 0 . denote relationshipZ 0 ,CUI(Y, X | Z).CUI also supports functional decomposition. Z 0 , conditional utility functiongiven X 0 Z 0 strategically equivalent function given different instantiationX. However, transformation depends X, also Z 0 . Hencewrite:U (X, Y, Z) = f (X, Z) + g(X, Z)U (X 0 , Y, Z), g() > 0.(1)85fiEngel & Wellmanis, fix X arbitrary level X 0 use two transformation functions fg get value U () levels X. stronger, symmetric form independenceleads additive decomposition utility function called additive independence.provide definition conditional version.Definition 6. X Conditionally Additive Independent given Z, CAI(X, | Z),Z 0 depends marginal conditional probability disif instantiation Z 0 ,0tributions XZ Z 0 .means value Z 0 , two probability distributions p, qp(X, , Z 0 ) q(X, , Z 0 ), p(, Y, Z 0 ) q(, Y, Z 0 ), decision maker indifferentp q. necessary (but always sufficient) condition holdutility differences U (X 0 , Y, Z 0 ) U (X 00 , Y, Z 0 ) (for X 0 , X 00 ) dependvalue .CAI leads following decomposition (Keeney & Raiffa, 1976):U (X, Y, Z) = f (X, Z) + g(Y, Z).variations utility independence considered theoretical literature,leading various decomposition results (Fishburn, 1975; Krantz, Luce, Suppes, & Tversky,1971; Fuhrken & Richter, 1991).2.1 Motivationobvious benefit model based (conditional) utility independencegenerality admitted weaker independence condition, comparison additive independence. Whereas additivity practically excludes interaction utility oneattribute subset (X Definition 6) value another (Y ), utility independenceallows substitutivity complementarity relationships, long risk attitude towards one variable affected value another. One could also argue UIparticularly intuitive, based invariance condition preference order.contrast, (conditional) additive independence requires judgment effects jointversus marginal probability distributions. Moreover, additive independence symmetric,whereas condition U I(X, ) allow preference order depend X.Bacchus Grove exemplify difference additive utility independencesimple state space two boolean attributes: Health Wealth. example,shown Table 1, attributes additive independent (it immediately seenusing preference differences), H W complements: worthsum one without other. would consideredtwo attributes substitutes if, example, U (W, H) = 4 U (W, H) = 3. casesH W nonetheless preferential independent, since always prefer richer (allelse equal) healthier (all else equal). boolean variables, preferentialutility independence equivalent (we always prefer lotteries give higher probabilitypreferred level) therefore Health Wealth also UI other.(Conditional) additive independence resulting additive decompositiongeneralized multiple subsets necessarily disjoint. condition called86fiCUI networksWWH52H10Table 1: Utility values Health Wealth example (Bacchus & Grove, 1995).generalized additive independence (GAI). GAI holds, U () decomposes sum independent functions fi () GAI subsets Xi . shown Bacchus Grove, CAIconditions accumulated global GAI decomposition (see Section 2.2). lattermay also exist without CAI conditions leading it, GAI condition hardidentify: whereas CAI condition corresponds independence two attributestwo subsets, global GAI condition intuitive interpretation.next example, cardinal independence condition exists, except non symmetric CUI. example also shows difference PI UI, hence requiresdomains H W include least three values each. also add third attribute outcome space, location (L), indicating whether live citycountryside (Table 2). order show U I(H, {W, L}) hold enoughfind violated one pair lotteries. Given partial outcome Wr , Lciprefer equal chance lottery < Hf , Hs >, whose expected utility 12+52 , sureoutcome Hg (value 8), whereas given Wp , Lci indifferent (expected utility 2lotteries). Intuitively, may case additional value get fitness(over good health) higher also rich, making significant valueHg adds Hs . Similarly, U I(W, {H, L}) hold, comparing even-chancegamble < Wr , Wp > sure outcome Wm , first given Hf , Lci givenHs , Lci .W H therefore utility independent, preferential independent.L, however, not: rich would rather live city, wayround poor, except case poor sick prefercity.WrWmWpHf1263LciHg842Hs531Hf1064LcoHg631.5Hs420Table 2: Utility values Health, Wealth Location example. Wr means rich, Wmmedium income, Wp means poor. Hf healthy top fitness, Hg means good health,Hs means sick. Lci stands city location, Lco means countryside location.Therefore, symmetric independence condition exists here, rules additive multiplicative independence, conditional not, subsets attributes.Also, since single variable unconditionally UI, subset unconditionallyUI. Further, fact preferences L depend combination H W rulesGAI decomposition form {W, L}, {W, H}, {H, L}.87fiEngel & Wellmancan, however, achieve decomposition using CUI. case CUI(W, L|H),since column left matrix (Lci ) affine transformation counterpartright side (Lco ). example, transform first column (Hf ), multiply 23add 2.example illustrates subtlety utility independence. particular, whereaspreferences L depend W , W may still (conditionally) UI L. CAI assumption attributes must inevitably ignore reversal preferences Ldifferent values W , hence decision maker queried preferencesassumption may able provide meaningful answers.interaction system requires preference representation normally requiresidentification structure, population utility values requiredcompact representation. therefore important two aspectssimplified possible, whereas functional form handled system maysophisticated. exactly tradeoff made CUI nets, compared GAI-basedrepresentation: GAI condition based CAI, CUI nets achieve lower dimensionality(Section 7), therefore easier elicitation. GAI condition based collectionCAI conditions, hard identify. CUI nets simplify bottleneck aspects,driving complexity algorithms functional form handledbehind scenes.2.2 Related WorkPerhaps earliest effort exploit separable preferences graphical model extension influence diagrams Tatman Shachter (1990) decompose value functionssums products multiple value nodes. structure provided computationaladvantages, enabling use dynamic programming techniques exploiting value separability.Bacchus Grove (1995) first develop graphical model based conditionalindependence structure. particular, establish CAI condition perfectmap (Pearl & Paz, 1989); is, graph attribute nodes node separationreflects exactly set CAI conditions S. specifically, two sets nodesX, S, CAI(X, |XY ) holds direct edge node Xnode . use term CAI map referring graph reflectsperfect map CAI conditions, context preference order D(S). BacchusGrove go show utility function GAI decomposition setmaximal cliques CAI map. show Section 7, CUI network representationdeveloped achieves weakly better dimensionality CAI maps due greatergenerality independence assumption.Initiating another important line work, Boutilier et al. (1999) introduced CP networks, efficient representation ordinal preferences multiple attributes.CP network, variable conditionally PI rest given parents. Ordinal multiattribute preference representation schemes (for decision making certainty),especially CP networks, dramatically simplify preference elicitation process, basedintuitive relative preference statements avoid magnitude considerations.However, limited expressive power CP networks may suffice complex decision88fiCUI networksproblems, tradeoff resolution may hinge complicated way attribute settingsrich domains. problem particularly acute continuous almost continuousattributes involved, money time.Boutilier et al. (2001) subsequently extended approach numeric, cardinal utilityUCP networks, graphical model utilizes GAI decomposition combinedCP-net topology. requires dominance relations parents children,somewhat limiting applicability representation. GAI structure alsoapplied graphical models Gonzales Perny (2004), employ clique graphCAI map (the GAI network ) elicitation purposes.earlier work, La Mura Shoham (1999) redefine utility independence symmetric multiplicative condition, taking closer probability analog, supportingBayes-net like representation. Although multiplicative independence different additive independence, necessarily weaker. Recent work Abbas (2005) definessubclass utility functions multiplicative notion UI obeys analogBayess rule.graphical decomposition suggested past utility functions basedoriginal, non-symmetric notion utility independence utility tree (Von Stengel,1988, see also Wellman Doyle, 1992, discussion AI context). utility treedecomposes utility function using multilinear multiplicative decomposition (Keeney& Raiffa, 1976), tries decompose subset similarly. Usinghierarchical steps utility function becomes nested expression functionssmallest separable subsets complements.2.3 Graphical Models CUIconcluding remarks, Bacchus Grove (1995) suggest investigating graphicalmodels independence concepts, particular utility independence. Foundinggraphical model UI difficult, however, utility independence decomposeeffectively additive independence. particular, condition U I(Y, X) ensuressubutility function, since X one harder carrydecomposition X. Hence case X large dimensionalityrepresentation may remain high. approach therefore employs CUI conditionslarge subsets , case decomposition driven decomposingconditional utility function using CUI conditions.sequel show serial application CUI leads functional decomposition.corresponding graphical model, CUI network, provides lower-dimension representation utility function function vertex depends nodeparents. demonstrate use CUI networks constructing examplerelatively complex domain. Next elaborate technical semantic properties model knowledge required construct it. Subsequent technical sectionspresent optimization algorithms techniques reducing complexityrepresentation.89fiEngel & Wellman3. CUI Networksbegin constructing DAG representing set CUI conditions, followed derivation functional decomposition nodes DAG.3.1 CUI DAGSuppose obtain set CUI conditions variable set = {x1 , . . . , xn },x S, contains condition formCUI(S \ ({x} P (x)) , x | P (x)).words, exists set P (x) separates rest variables x.P (x) always exists, P (x) = \ {x} condition trivially holds. setrepresented graphically following procedure, name procedure C.1. Define order set (for convenience assume ordering x1 , . . . , xn ).2. Define set parents x1 P a(x1 ) = P (x1 ).3. = 2, . . . , n), set nodesDefine set intermediate descendants xi , Dn(xx1 , . . . , xi1 turned descendants xi , xi) smallestparent another descendant xi parent. Formally, Dn(xset satisfies following condition:) P a(xj )j {1, . . . , 1}, [xi P a(xj ), k {1, . . . , 1}.xk Dn(x).] (2)xj Dn(xDefine parents xi nodes P (xi ) already descendants xi ,).P a(xi ) = P (xi ) \ Dn(xprocedure defines DAG. denote Dn(x) final set descendants x.set defined Equation (2), replacing {1, . . . , 1} {1, . . . , n}).definitions, Dn(x) Dn(x),henceP a(x) Dn(x) P a(x) Dn(x)= P (x).(3)Proposition 1. Consider DAG defined procedure C set attributes S.x S,CUI(S \ ({x} P a(x) Dn(x)) , x | P a(x) Dn(x)).(4)Proof. definitions P a(x) P (x), (4) holds replacing Dn(x) Dn(x).definition CUI, straightforwardCUI(S \ (Y W ) , | W ) CUI(S \ (Y W Z) , | W Z),invariance preference order \ (Y W ) implies invariance preferenceorder subset \ (Y W Z), difference set Z fixed. Given (3),taking W = P a(x) Dn(x)Z = Dn(x) \ Dn(x),get (4).90fiCUI networksexample, show construction structure small set variables= {x1 , x2 , x3 , x4 , x5 , x6 }, given following set CUI conditions:= {CUI({x4 , x5 , x6 }, x1 | {x2 , x3 }), CUI({x4 , x3 , x6 }, x2 | {x1 , x5 }),CUI({x2 , x4 , x6 }, x3 | {x1 , x5 }), CUI({x1 , x3 , x5 }, x4 | {x2 , x6 }),CUI(x6 , x5 | {x1 , x2 , x3 , x4 }), CUI({x1 , x2 , x3 , x5 }, x6 | x4 )}.Construction network using order implied indices results CUIDAG illustrated Figure 1. minimal separating set x1 {x2 , x3 }. x2 , get2 ) = {x1 }, non-descendant variable required separateDn(xrest x5 , therefore parent. rest graph constructedsimilar way. x4 placed, find P (x4 ) = {x2 , x6 }. Therefore, x4 becomes2 ) {x4 } = {x1 , x4 }.descendant x2 x2 placed, words Dn(x2 ) = Dn(xix6ix5ZZ=x~ ixZ2?=x43ZZ~iZ=x1Figure 1: CUI DAG given order x1 ,. . . ,x6 .Definition 7. Let U (S) utility function representing cardinal preferences D(S).CUI DAG U () DAG, x S, (4) holds.Procedure C yields CUI DAG Proposition 1. direction, given CUIDAG G (in parents descendants denoted P aG (), DnG (), respectively)constructed using C, follows. Define P (x) = P aG (x) DnG (x) variableordering according reverse topological order G, complete execution C.straightforward show set parents selected xi exactly P aG (xi ),hence result DAG identical G.3.2 CUI Decompositionshow CUI conditions, guaranteed Proposition 1, applied iteratively decompose U () lower dimensional functions. first pick variable orderingagrees reverse topological order CUI DAG. simplify presentation,rename variables ordering x1 , . . . , xn . CUI condition (4) x1implies following decomposition, according (1):U (S) = f1 (x1 , P a(x1 ), Dn(x1 )) + g1 (x1 , P a(x1 ), Dn(x1 ))Ux01 (S \ {x1 }).(5)Note Dn(x1 ) = .assume specified reference point 0 , arbitrary value chosenattribute x S, denoted x0 . Ux01 () right hand side conditional91fiEngel & Wellmanutility function given x1 fixed reference point x01 . convenience omitattributes whose values fixed list arguments.applying decomposition based CUI condition x2 conditionalutility function Ux01 (), getUx01 (S \ {x1 }) = f2 (x, P a(x2 ), Dn(x2 )) + g2 (x2 , P a(x2 ), Dn(x2 ))Ux01 ,x02 (S \ {x1 , x2 }). (6)Note Dn(x2 ) {x1 }, x1 fixed x01 , hence f2 g2 effectively dependx2 P a(x2 ). point exploited below.Substituting Ux01 () (5) according (6) yields:U (S) = f1 + g1 (f2 + g2 Ux01 ,x02 (S \ {x1 , x2 })) = f1 + g1 f2 + g1 g2 Ux01 ,x02 (S \ {x1 , x2 }).list arguments functions fj , gj always (xj , P a(xj ), Dn(xj )), omitreadability.continue fashion getU (S) =i1Xk1(fkj=1k=1gj ) +gj Ux01 ,...,x0 (xi , . . . , xn ),i1j=1apply CUI condition xi ,Ux01 ,...,x0 (xi , xi+1 , . . . xn ) =i1fi (xi , P a(xi ), Dn(xi )) + gi (xi , P a(xi ), Dn(xi ))Ux01 ,...,x0 (xi+1 , . . . , xn ). (7)convenience, define constant function fn+1 Ux01 ,...,x0n (). Ultimately obtainU (S) =n+1Xi1i=1j=1(fi (xi , P a(xi ), Dn(xi ))gj (xj , P a(xj )), Dn(xj )).(8)variable ordering restricted agree reverse topological order graph,hence (7), Dn(xi ) {x1 , . . . , xi1 }. Therefore, variables Dn(xi ) righthand side (7) fixed reference points, fi gi depend xiP a(xi ). Formally, let y1 , . . . , yk variables Dn(xi ). abuse notation,define:fi (xi , P a(xi )) = fi (xi , P a(xi ), y10 , . . . , yk0 ),gi (xi , P a(xi )) = gi (xi , P a(xi ), y10 , . . . , yk0 ).(9)(8) becomesU (S) =n+1Xi1i=1j=1(fi (xi , P a(xi ))gj (xj , P a(xj ))).(10)term decomposition multiattribute utility function lower dimensionalfunctions, whose dimensions depend number variables P a(x). result,92fiCUI networksdimensionality representation reduced (as Bayesian networks) maximalnumber parents node plus one.illustrate utility function decomposed example Figure 1.pick ordering x4 , x1 , x6 , x3 , x2 , x5 agrees reverse topological ordergraph (note renaming variables here). simplify notationdenote conditional utility function xi fixed reference point addingsubscript U ().U (S) = f4 (x4 x2 x6 ) + g4 (x4 x2 x6 )U4 (S \ {x4 })U4 (S \ {x4 }) = f1 (x1 x2 x3 ) + g1 (x1 x2 x3 )U1,4 (S \ {x4 x1 })U1,4 (S \ {x4 x1 }) = f6 (x6 ) + g6 (x6 )U1,4,6 (x2 x3 x5 )U1,4,6 (x2 x3 x5 ) = f3 (x3 x5 ) + g3 (x3 x5 )U1,3,4,6 (x2 x5 )U1,3,4,6 (x2 x5 ) = f2 (x2 x5 ) + g2 (x2 x5 )U1,2,3,4,6 (x5 )U1,2,3,4,6 (x5 ) = f5 (x5 ) + g5 (x5 )U1,2,3,4,5,6 ()Note fi gi depends xi parents. Merging equations,using definition f7 U1,2,3,4,5,6 () producesU (S) = f4 + g4 f1 + g4 g1 f6 + g4 g1 g6 f3 + g4 g1 g6 g3 f2 + g4 g1 g6 g3 g2 f5 + g4 g1 g6 g3 g2 g5 f7 . (11)established U (S) represented using set functions F, includes,x S, functions (fx , gx ) resulting decomposition (1) based CUIcondition (4). means fully specify U (S) sufficient obtain datafunctions F (this aspect discussed Section 5).Definition 8. Let U (S) utility function representing cardinal preferences D(S).CUI network U () triplet (G, F, 0 ). G = (S, E) CUI DAG U (S), 0reference point, F set functions {fi (xi , P a(xi )), gi (xi , P a(xi )) | = 1 . . . , n}defined above.utility value assignment calculated CUI networkaccording (10), using variable ordering agrees reverse topological orderDAG. example, choose different variable ordering one usedabove, x1 , x3 , x4 , x2 , x5 , x6 , leading following expression.U (S) = f1 + g1 f3 + g1 g3 f4 + g1 g3 g4 f2 + g1 g3 g4 g2 f5 + g1 g3 g4 g2 g5 f6 + g1 g3 g4 g2 g5 g6 f7 .sum product different one (11). However, based CUIdecompositions therefore functions (fi , gi ).3.3 Properties CUI NetworksBased Procedure C decomposition following it, conclude following.Proposition 2. Let set attributes, set CUI conditions S.includes condition form CUI(S \ (x Zx ), x | Zx ) x S,represented CUI network whose dimensionality exceed maxx (|Zx | + 1).93fiEngel & WellmanNote Zx denotes minimal set attributes (variables) renders restCUI x. bound dimensionality obtained regardless variableordering. expect maximal dimension lower network constructedusing good variable ordering. good heuristic determining ordering woulduse attributes smaller dependent sets first, attributes dependentswould descendants. Based ordering would expectless important attributes lower topology, crucial attributeswould either present higher larger number parents.point usually omit third argument referring CUI condition,CUI(X, ), taken equivalent CUI(X, | \ (X )).order achieve low dimensional CUI networks, required detect CUIconditions large sets. may difficult task, addressexample Section 4. task made somewhat easier fact setCUI single variable; note condition CU I(Y, x) weaker conditionCU I(Y, X) x X. Furthermore, Section 7 shows dimensionalityreduced initial CUI decomposition sufficiently effective.Based properties CUI, read additional independence conditionsgraph. First, observe CUI composition property second argument.Lemma 3. Let CUI(X, ) [X, S], CUI(A, B) [A, B S].CUI(A X , B).property leads following claim, allows us derive additional CUIconditions graph constructed.Proposition4. ConsiderCUI network set attributes S. Define P a(X) =Pa(x)Dn(X)=xX Dn(x). X S,xXCUI(S \ (X P a(X) Dn(X)) , X).Proof. recursion X, using Lemma 3 Proposition 1.also consider direction, defining set nodes renders set CUIrest. dual perspective becomes particularly useful optimization (Section 6),optimization based preference order attribute meaningfulholding enough attributes fixed make CPI CUI rest. Let Ch(X)denote union children nodes X, let An(X) denote ancestorsnodes X, cases excluding nodes X.Proposition 5. Consider CUI network set attributes S. CUI(X, \(X An(X)Ch(X))) X S.Proof. Let/ X Ch(X) An(X). clearly x X, x/ P a(y) Dn(y). HenceProposition 1, CUI(X, y). apply Lemma 3 iteratively/ X Ch(X) An(X)(note first argument X CUI condition, X result well),get desired result.conclude section relating CUI networks CAI maps.94fiCUI networksProposition 6. Let G = (X, E) CAI map, x1 , . . . , xn ordering nodesX. Let G0 = (X, E 0 ) DAG directed arc (xi , xj ) E 0 iff < j(xi , xj ) E. G0 CUI network.note, however, CAI maps decompose utility function maximalcliques, whereas CUI networks decompose nodes parents. Section 7 bridgesgap. addition, result used Section 6.3.4. CUI Modeling Exampledemonstrate potential representational advantage CUI networks require domain difficult simplify otherwise. example use choice softwarepackage enterprise wishes automate sourcing (strategic procurement) process. focus softwares facilities running auction RFQ (request quotes)events, tools select winning suppliers either manually automatically.identified nine key features kinds software packages. choice scenario,buyer evaluates package nine features, graded discrete scale (e.g.,one five). features are, brief:Interactive Negotiations (IN ) allows separate bargaining procedure supplier.Multi-Stage (MS ) allows procurement event comprised separate stages different types.Cost Formula (CF ) buyers formulate total cost businesssupplier.Supplier Tracking (ST ) allows long-term tracking supplier performance.MultiAttribute (MA) bidding multiattribute items, potentially using scoring function.1Event Monitoring (EM ) provides interface running events real-time graphicalviews.Bundle Bidding (BB ) bidding bundles goods.Grid Bidding (GB ) adds bidding dimension corresponding aspect timeregion.Decision Support (DS ) tools optimization aiding choice bestsupplier(s).observe first additive independence widely apply domain.example, Multi-Stage makes several features useful important: InteractiveNegotiations (often useful last stage), Decision Support (to choose suppliers1. hope fact software may include facilities multiattribute decision makingcause undue confusion. Naturally, consider important feature.95fiEngel & Wellmanproceed next stage), Event Monitoring (helps keep track usefulstage reducing costs). Conversely, circumstances Multi-Stage substitutefunctionality features: MultiAttribute (by bidding different attributes different stages), Bundle Bidding (bidding separate items different stages), Grid Bidding(bidding different time/regions different stages) Supplier Tracking (by extractingsupplier information Request Information stage). potential dependenciesattribute shown Table 3.AttrEMCFSTMSDSGBBBComplementsCF ST MSST MSEM MS DS GB BBEM MS DSDS CFDS EM STCF GB ST BB MSCF DSCF DSSubstitutesDSMS BB ST GBGB BB CFMS BBMS GBCUI setIN,DS,MA,GB,BBEM,CF,ST,DS,GB,BBMA,GB,BBIN,CF,GB,BBGB,BBMA,GB,BBIN,EMMA,BBMA,GBTable 3: Dependent independent sets attribute.presence complement substitute relation precludes additive independence.fact identify set six attributes must mutually (additive) dependent: {BB , GB , DS , MA, MS , CF }. consequence, best-case dimensionality achievedCAI map (and CAI-based representations, see Section 2.2), domain wouldsix, size largest maximal clique.order construct CUI network first identify, attribute x, setCUI it. first guess set according complement/substitute informationTable 3; typically, set attributes neither complements substitutes wouldCUI. approach taken attributes EM DS . However, attributescomplements substitutes may still CUI other, therefore attemptdetect verify potentially larger CUI sets. Keeney Raiffa (1976) provide severaluseful results help detection UI, results generalized CUI.particular show first detect conditional preferential independence(CPI) condition one element also CUI. Based result, order verifyexampleCUI({BB , GB , MA}, CF | \ {BB , GB , MA, CF }),(12)following two conditions sufficient:CPI({BB , GB , MA}, CF | \ {BB , GB , MA, CF }),(13)CUI(BB , {GB , MA, CF } | \ {BB , GB , MA, CF }).(14)Detection verification conditions also discussed Keeney Raiffa (1976).example, observe features BB , GB , add qualitative96fiCUI networkselement bidding. bidding element best exploited cost formulationavailable, complements CF . complementarity similar feature, thusimplying (13). Moreover, BB crucial feature therefore risk attitude towardsexpected vary level CF , MA, GB , implies (14), togetherleading (12).similar fashion, observe nature substitutivity threemechanisms BB , GB , MS similar: simulated using multiple stages.means tradeoffs among three depend MS , meaningCPI({BB , GB , MA}, MS ) holds. Next, dependency among triplet {BB , GB , MA}also result option substitute one another. result, pair CPIthird. Finally, find complementarity ST marginalaffect tradeoffs attributes. therefore verify following conditions:CUI({BB , GB , MA}, MS ), CUI({BB , GB }, MA), CUI({ST , EM , CF , DS , GB , BB }, ),CUI({GB , BB , CF , }, ST ). resulting maximal CUI sets attributeshown Table 3.construct network start variable largest CUI set, ,needs MS parents, EM gets CF , MS , STparents. Next, consider ST needs four attributes conditional set, EMdescendant, therefore DS , MS , needed parents. next variablechoose MS , needs CF DS parents since dependant variablesdescendants. chosen CF MS would needed four parents: , MS ,ST , DS (note although CUI CF set {BB , GB , MA},case union {BB , GB , MA, }). choose CF MSMS , ST , descendants therefore DS parent. complete variableordering , EM , ST , MS , CF , DS , MA, GB , BB , resulting CUI networkdepicted Figure 2. maximal dimension four.structure obtained utility function example based largelyobjective domain knowledge, may common various sourcing departments.demonstrates important aspect graphical modeling captured CUI networks:encoding qualitative information domain, thus making process extractingnumeric information easier. structure cases differs among decision makers,cases (as above) makes sense extract data domain expertsreuse structure across decision makers.5. Representation Elicitationsection, derive expression local node data terms conditional utility functions, discuss elicit utility information judgments relativepreference differences.5.1 Node Data RepresentationRepresenting U CUI network requires determine f g functionsCUI condition. node functions f, g represent affine transformationconditional utility function U (x0 , Y, Z) (here Z = P a(x)) strategically equivalent utilityfunctions values x. Like transformation functions UI (Keeney & Raiffa,97fiEngel & WellmanFigure 2: CUI network example. maximal number parents 3, leadingdimension 4.1976), transformation functions CUI represented terms conditionalutility functions U (x, 1 , Z) U (x, 2 , Z) suitable values 1 2 (see below).determine f g solving system two equations below, basedapplying (1) specific values :U (x, 1 , Z) = f (x, Z) + g(x, Z)U (x0 , 1 , Z),U (x, 2 , Z) = f (x, Z) + g(x, Z)U (x0 , 2 , Z),yieldingU (x, 2 , Z) U (x, 1 , Z),U (x0 , 2 , Z) U (x0 , 1 , Z)f (x, Z) = U (x, 1 , Z) g(x, Z)U (x0 , 1 , Z).g(x, Z) =(15)(16)restriction choice 1 , 2 decision maker mustindifferent given x0 current assignment Z. example, 1 , 2may differ single attribute strictly essential.5.2 Elicitation Measurable Value Functionsutility function used choosing action leads known probabilitydistribution outcomes, obtained elicitation preferenceslotteries, example using even-chance gambles certainty equivalents (Keeney &Raiffa, 1976). Based preceding discussion, fully specify U () via CUI network,need obtain numeric values conditional utility functions U (x, 1 , P a(x))U (x, 2 , P a(x)) node x. significantly easier obtaining fulln-dimensional function, general done using methods described preference98fiCUI networkselicitation literature (Keeney & Raiffa, 1976). section show elicitationconducted cases choice assumed done certain outcomes,cardinal representation nevertheless useful.particular applications point specific attributes usedmeasurement others. common example preferences quasi-linearspecial attribute money time. kind preferences representedmeasurable value function, MVF (Krantz et al., 1971; Dyer & Sarin, 1979). MVFcardinal utility function defined certainty represents preference differences.shown (Dyer & Sarin, 1979) UI analogous interpretation MVFsimilar resulting decomposition. extension CUI straightforward.case monetary scaling, preference difference pair outcomesrepresents difference willingness pay (wtp) each. potential way elicitMVF asking decision maker provide wtp improve one outcomeanother, particularly outcomes differ single attribute.interpretation, first observe (15) g(x, Z) elicited termspreference differences, outcomes possibly differ single attribute.result convey qualitative preference information. Assume 2 1 x.x0 x.g(x, Z) ratio preference difference 1 2 given xdifference given x0 (Z fixed outcomes). Hence, x complementsg(x, Z) > 1 increasing x. x substitutes, g(x, Z) < 1 decreasingx. holds regardless choice 1 , 2 , since CUI(Y, x | Z) attributesmaintain complementarity substitutivity relationship x. Note alsog(x, Z) = 1 iff CAI(Y, x | Z). Another important observation though xmay depend Z, practice expect level dependency xdepend particular value Z. case g becomes single-dimensional function,independent Z.f (x, Z), intuitively speaking, measurement wtp improve x0 x.value U (x0 , 1 , Z) multiplied g(x, Z) compensate interactionx, allowing f () independent . perform elicitation obeyingtopological order graph, function U (x0 , 1 , Z) readily calculatednew node data stored predecessors. Choose 1 = 0 , let Z = {z1 , . . . , zk },ordered children precede parents. Since Y, x fixed reference point,ki1XU (x , , Z) =(fzigzj )fn+1 ().00i=1j=1obtain f (x, Z) follows: first elicit preference difference functione(x, Z) = U (x, 1 , Z) U (x0 , 1 , Z). Then, assuming g(x, Z) already obtained, calculate:f (x, Z) = e(x, Z) (g(x, Z) 1)U (x0 , 1 , Z).6. OptimizationOne primary uses utility functions support optimal choices, selectingoutcome action. complexity choice depends specific properties99fiEngel & Wellmanenvironment. choice among limited set definite outcomes, recoverutility outcome using compact representation choose onehighest value. instance, software example Section 4 would normally chooseamong enumerated set vendors packages. procurement scenario assumeutility MVF, usually choose outcome yields highest utilitynet price. case decision uncertainty, choice among actionslead probability distributions outcomes, optimal choice selected computingexpected utility action. action involves reasonably bounded numberoutcomes non-zero probability, done exhaustive computation.Nevertheless, often useful directly identify maximal utility outcome givenquantitative representation utility. case direct choice constrained outcomespace, optimization algorithm serves subroutine systematic optimization procedures, adapted probabilistic reasoning literature (Nilsson, 1998).algorithm may also useful heuristic aid optimization expected utilitynet utility mentioned above, set possible outcomes large explicit,exhaustive choice.section, develop optimization algorithms discrete domains, showmany cases CUI networks provide leverage optimization CAI maps.typical graphical models, optimization algorithm particularly efficientgraph restricted tree.6.1 Optimization CUI TreesDefinition 9. CUI tree CUI network node one child.Note type graph corresponds upside-down version standard directed tree (or forest).Let CUI tree. assume WLOG connected (a forest turnedtree adding arcs). upside-down sort tree, number roots,single leaf. denote root nodes ai {a1 , . . . , ak }, child ai bi ,on. root node ai , define functionhai (bi ) = arg 0 max U (bi , a0i ),ai D(ai )denoting selection optimal value ai corresponding given value child.Proposition 5, hai depend reference values chosen \ {ai , bi }.function hai (), call optimal value function (OVF) ai , stored node aisince used descendants described below.Next, bi children single child ci , number parents. simplicity exposition present case bi two parents, ai aj . maximizationfunction bi definedhbi (ci ) = arg 0 max U (ci , b0i , hai (b0i ), haj (b0i )).bi D(bi )words, pick optimal value bi assignment child parents.since already know optimum parents value bi , needconsider optimum evaluation domain bi .100fiCUI networks(a)(b)Figure 3: CUI networks optimization examples: (a) Tree (b) Non-treeexternal child set {ai , aj , bi } ci , external ancestors,hence {ai , aj , bi } CUI rest given ci , therefore maximizationdepend reference values rest attributes. Similarly, computinghci (di ) child ci bi , value ci fixes bi (and parents ci ),fixes ai aj (and ancestors ci ). last computation, leaf x, evaluatesvalue x. value x0 causes cascade fixed values ancestors,meaning finally get optimal choice comparing |D(x)| complete assignments.illustrate execution algorithm CUI tree Figure 3a. computeha (c) optimal value value c, similarly hb (c). Next,compute hc (e), value e0 e compare outcomes (e0 , c0 , ha (c0 ), hb (c0 )), c0 D(c).node compute hd (f ), independent nodes. node e compute(f ) = arg maxe0 U (f, e0 , hc (e0 ), hb (hc (e0 )), ha (hc (e0 )) (node ignored here)node fhf () = arg max U (f 0 , (f 0 ), hd (f 0 ), hc (he (f 0 )), hb (hc (he (f 0 ))), ha (hc (he (f 0 ))).f 0 D(f )Note candidate value f causes cascade optimal valuesancestors. solution hf () resulting values ancestors.optimization algorithm iterates nodes topological order, xicalculates OVF hxi (xj ), xj child xi . calculation uses valuesOVF stored parents, therefore involves comparison |D(xi )||D(xj )|outcomes. case numeric data nodes available, factoring time takesrecover utility value outcome (which O(n)), algorithm runs timeO(n2 maxi |D(xi )|2 ).6.2 Optimization General DAGscommon way graphical models apply tree algorithms non-trees usingjunction graph. However, common notion junction graph DAG polytree,101fiEngel & Wellmanwhereas algorithm specialized (unit) tree. Instead, optimize CUInetwork directly generalizing tree algorithm.tree case, fixing value child node x sufficient order separatex rest graph, excluding ancestors. consider value childtime, also determines values ancestors. general DAG longersufficient OVF depend children, provide sufficientinformation determine values An(x). Hence generalize notionscope x (Sc(x), defined below), set nodes OVF x mustdepend, order iterative computation OVF sound.generalization, DAG algorithm similar tree algorithm. Let GCUI network, x1 , . . . , xn variable ordering agrees topological orderG (parents precede children). xi (according ordering), compute hxi (Sc(xi ))instantiation Sc(xi ). optimal instantiation selected backwardshxn (), since node xi reached values Sc(xi ) already selected.Sc(xi ) computed follows: scan variables xi+1 , . . . , xn order. scanningxj , add xj Sc(xi ) following conditions hold:1. undirected path xj xi .2. path blocked node already Sc(xi ).conditions, Sc(xi ) includes children xi , non xi ancestor sinceprecede xi ordering. addition, Sc(xi ) includes nodes neededblock paths reach xi ancestors. example, xk , xj childrenancestor xa xi , k < < j, xj must Sc(xi ), pathxa . children xj blocked xj , unless another path xiSc(xi ). children xk , ordered later xi , Sc(xi ) (butchildren not), on.Figure 3b example CUI network tree. consider scopesvariable ordering a, b, . . . , j. scope roots always equals set children(because path reaching them), meaning Sc(a) = {d, e}, Sc(b) = {d, e, f },Sc(c) = {e, f, h}, Sc(i) = {j}. scope must include child g siblings ef . paths h, j, blocked g, e, f therefore Sc(d) = {g, e, f }. e,must include child g, younger sibling f . h blocked path ef Sc(e), also non-blocked one c/ Sc(e), therefore Sc(e) = {g, f, h}.Similarly, g h scope f due paths b c respectively, henceSc(f ) = {g, h, j}. g, addition child h add j whose path g f, b, eblocked (Sc(g) = {h, j}) finally Sc(h) = Sc(i) = {j} Sc(j) = {}.next step, computing OVF, requires compare set outcomesdiffer xi Co(xi ), Co(xi ) set nodes whose OVF determinedxi Sc(xi ) (hence covered xi ). maximization valid, conditionCUI(xi Co(xi ), \ (xi Co(xi ) Sc(xi ))) must hold. formally define Co(xi ),establish result proved appendix.Definition 10. Co(xi ) smallest set nodes satisfied following conditionj < i, Sc(xj ) ({xi } Sc(xi ) Co(xi )) xj Co(xi ).102(17)fiCUI networksIntuitively, xj covered xi node xk 6= xi scope, either scope xidetermined (according scope) covered xi . Figure 3b, f Co(g)Sc(f ) = {g} Sc(g). e Co(g) Sc(e) {g} Sc(g) {f }. Moreover,Sc(d) = {g, e, f } hence Co(g) well, similarly find a, b Co(g).example nodes preceding g ordering covered, necessarilyalways case.Lemma 7. assignment xi Sc(xi ) sufficient determine hxj () xjCo(xi ).Lemma 8. node xi , CUI({xi } Co(xi ), \ ({xi } Co(xi ) Sc(xi ))). Meaningxi nodes covers CUI rest given Sc(xi ).algorithm reaches node xi , every choice assignment Sc(xi ) {xi } determines optimal values Co(xi ) (Lemma 7). compare |D(xi )| assignmentsdiffer values xi Co(xi ), select optimal one value hxi (Sc(xi )).optimum depend nodes \({xi }Co(xi )Sc(xi ))) due Lemma 8.illustrate, examine happens algorithm reaches node g Figure 3b.point hx (Sc(x)) known x precedes g. showed, nodesCo(g). Indeed, assignment Sc(g) = {g, h, j} directly determines valuehf (), together hf () determines value (), cascadesrest nodes. CUI network shows CUI({a, b, c, d, e, f, g}, {i}) (given{h, j}) therefore maximization operation (over choice value g) validregardless value i.performance optimization algorithm exponential size largestscope (plus one). Note would seriously affected choice variable ordering. Also note, case tree algorithm specializes tree optimizationabove, since node path ancestor xi , except ancestors xi must precede xi ordering. Therefore always caseSc(xi ) = Ch(xi ), meaning hxi () function single child. Based that,expect algorithm perform better similar CUI network tree.6.3 CUI Tree Optimization CAI Mapsoptimization procedure CUI trees particularly attractive due relativelylow amount preference information requires. cases comparisondone directly, without even data comprises utility function. Asidedirect benefit CUI networks, interested applying structureoptimization CAI maps. domains CAI map simple effective waydecompose utility function. However, optimization CAI maps exponentialsize tree width, requires full data terms utility functionsmaximal cliques. CAI map happens simple structure, tree,CP condition, faster optimization algorithms used. However, could caseCAI map tree, subtle CUI conditions might exist cannotcaptured CAI conditions. enough conditions could detected turn CAImap CUI tree (or close enough tree), could take advantage simpleoptimization procedure.103fiEngel & Wellman(a)(b)(c)Figure 4: (a) CAI map containing cycle. (b) Enhanced CAI map, expressing CUI{a, d, f } b. (c) equivalent CUI tree.Definition 11. Let G = (V, E) CAI map. enhanced CAI map directed graphG0 = (V, A), pair arcs (u, v), (v, u) implies dependencyedge (u, v) E, addition node x, CUI(S \ ({x} In(x)), x) (In(x) denotingset nodes (y, x) A). call pair arcs (u, v), (v, u) hardlink arc (u, v) s.t. (v, u)/ weak link.CAI map, enhanced CAI map generated replacing edge (u, v)arcs (u, v) (v, u). require additional CUI conditionsentailed CAI map. However, additional CUI conditionsdetected, might able remove one (or both) directions. Figure 4a showsCAI map contains cycle. could detect CUI({a, d, f }, b), could removedirection (a, b) get enhanced CAI map Figure 4b. set CUI conditionsimplied enhanced CAI map expressed CUI tree, Figure 4c.Proposition 9. Consider enhanced CAI map G. Let ordering nodesG, G0 DAG result removing arcs (u, v) whose directionagree . removed arc, v ancestor u G0 , G0 CUInetwork.hard links, removal (u, v) leaves v parent u, condition triviallyholds. obtain CUI tree, key therefore find variable orderingenough weak links removed turn graph tree, maintaining conditionProposition 9. large number variables, exhaustive search variable orderingsmay feasible. However many cases effectively constrained, restrictingnumber orderings need consider. example, order break cycleFigure 4b clear weak link (b, a) must implied ordering,could ancestor b. way happen (given existing hard links),c parent b, parent c, parent d.Proposition 10. Let c = (y1 , . . . , yk ) cycle enhanced CAI map G. Assumec contains exactly one weak link: (yi , yi+1 ) < k, (yk , y1 ). Let variable104fiCUI networksordering agree order path p = (yi+1 , yi+2 , . . . , yk , y1 , . . . , yi ).CUI network constructed G (by Proposition 9), tree.Therefore cycle contains one weak link leads constraint variableordering. Cycles one weak link also lead constraints. canother weak link (yj , yj+1 ), one two links must removed, ordering mustagree either path p path p0 = (yj+1 , yj+2 , . . . , yk , y1 , . . . , yj ). AssumingWLOG j > i, paths (yi+1 , . . . , yj ) (yj+1 , . . . , yi ) required pp0 , therefore used constraints. Similarly find intersectionpaths implied number weak links cycle.Sometimes constraint set lead immediate contradiction, casesearch redundant. not, significantly reduce search space. However,major bottleneck preference handling usually elicitation, rather computation.Therefore, given good variable ordering may lead reduction optimizationproblem simpler, qualitative task, eliminating need full utility elicitation,would worthwhile invest required computation time.7. Nested RepresentationSection 5.1 conclude node data represented conditional utility functions depending node parents. may best dimensionalityachieved network. Perhaps set Z = P a(x) internal structure,sense subgraph induced Z maximal dimension lower |Z|.case could recursively apply CUI decomposition conditional utility functionssubgraph. approach somewhat resembles hierarchical decomposition doneutility trees (Keeney & Raiffa, 1976; Von Stengel, 1988). example, represent f1network Figure 1, require conditional utility function U (x1 , x14 , x15 , x16 , x2 , x3 ).However network see CUI(x3 , x2 | x1 , x4 , x5 , x6 ). Hence decompose conditional utility:U (x1 , x14 , x15 , x16 , x2 , x3 ) = f 0 (x1 , x2 ) + g 0 (x1 , x2 )U (x1 , x3 , x02 , x14 , x15 , x16 ).use notation f 0 g 0 since f g functionstop level decomposition.nested representation generated systematically (Algorithm 1), decomposinglocal function node x (xs utility factors) whose argument set Z P a(x)form clique. performing complete CUI decomposition subgraphinduced Z (keeping mind resulting factors depend also x).Proposition 11. Let G CUI network utility function U (S). U (S)represented set conditional utility functions, depending set attributescorresponding (undirected) cliques G.7.1 Discussionresult reduces maximal dimensionality representation sizelargest maximal clique CUI network. instance, applying example105fiEngel & WellmanData: CUI Utility factors U (x, P a(x), ), U (x, P a(x), ) node x/* note: , D(Y ) */Determine order x1 , . . . , xn ;j = 1, . . . , n /* initialization */Kj1 = {xj } P a(xj ) /* scope utility factors */ ;Yj1 = \ Kj1 /* rest variables */ ;Q1j = P a(xj );A1j = , dj = 1;endj = 1, . . . , n= 1, . . . , dj /* loop factors node j*/Qij 6= Kji cliqueLet Gij subgraph induced Qij ;Decompose Uji (Kji ) according CUI network Gij ;foreach xr QijLet dr = dr + 1 (current num. factors xr ) denote = dr ;Adr = Aij {xj }, Qdr = P a(xr ) Qij ;Krd = Adr {xr } Qdr , Yrd = \ Krd ;Store new CUI factors xr : U (Krd , Yrd ), U (Krd , Yrd );/*Yrd , Yrd fixed assignments Yrd */endRemove factors U (Kji , Yji ), (Kji , Yji );endendendAlgorithm 1: Recursive CUI decomposition. Process node reverse topologicalorder (outermost loop). Decompose factor stored current node, whose parentsform clique. parent xr (innermost loop) store resulting newfactors. defined xr , xr parents also P a(xi ) (thisQdr ), clique Adr original factor depends. time factordecomposed set Q shrinks. empty, K clique.106fiCUI networksSection 4 reduces dimensionality four three. important implicationsomewhat relax requirement find large CUI sets. variables endmany parents, reduce dimensionality using technique. exampleillustrates, technique aggregates lower order CUI conditions effectivedecomposition.procedure may generate complex functional form, decomposing function multipletimes factors become restricted clique. ultimate number factorsrequired represent U (S) exponential number nesting levels. However,decomposition based CUI network subgraph, therefore typicallyreduces number entries maintained.expect typical application technique composition ratherdecomposition. execute Algorithm 1 without actual data, resulting list factorsper node (that conditional utility functions cliques graph). meanselicitation purposes restrict attention conditional utility functionsmaximal cliques. obtained, sufficient data factors.recover original, convenient CUI-network representation functionstore (more example below). Therefore, effective dimensionalityelicitation maximal cliques. storage efficient usage requirespotentially higher dimension original CUI network, typically lessconcern.result Proposition 6, CUI networks shown always achieve weaklybetter dimensionality CAI maps, since representations reduce dimensionalitysize maximal clique.7.2 Exampleillustrate result using simple example. Consider domain four attributes(a, b, c, d), following CUI conditions:CUI(b, c), CUI(c, b), CUI(d, a)CUI network corresponding variable ordering a, b, c, depicted Figure 5.Since CUI sets small (a single variable each), variable ordering mustnode two parents, meaning dimensionality three. nesting operationcombines lower order conditions reduce dimensionality two.Initially, utility function represented using conditional utility functions listedaccording corresponding nodes column Level 0 Table 4. removethree-dimensional factors, need decompose functions node accordingCUI network {b, c}, contains arcs. proceeds follows:U (a, b, c, d1 ) =fb1 (a, c) + gb1 (a, c)U (a, b, c0 , d1 ) = fb1 (a, c) + gb1 (a, c)(fc1 (a, b) + gc1 (a, b)U (a, b0 , c1 , d1 ))U (a, b, c, d2 ) =fb2 (a, c) + gb2 (a, c)U (abc0 d2 ) = fb2 (a, c) + gb2 (a, c)(fc2 (a, b) + gc2 (a, b)U (a, b0 , c0 , d2 ))107fiEngel & WellmanFigure 5: Nesting exampleresulting functions fbi (a, c), gbi (a, c), fci (a, b), gci (a, b), = 1, 2. functionsgbi (a, c) represented using conditional utility functions U (a, b1 , c, di )U (a, b2 , c, di ), similarly two functions. delete factors a,U (a, b, c, di ), add new lower dimensional factors second column parentsb c. Though multiply number factors store four, newfactors conditional utility functions subdomains (deleted) higher dimensionalfactors. algorithm continues node b, loops six factors. factorsdefined set parents b clique decomposes storenew factors next table column. case factor column level 1 coulddecomposed, would add level 2 column store result. simple exampledecomposition possible.fbi (a, c)AttrbcLevel 0 (CUI net)U (a, b, c, d1 ), U (a, b, c, d1 )U (a0 , b, c1 , d),U (a0 , b, c2 , d)U (a0 , b1 , c, d),U (a0 , b2 , c, d)U (a0 , b0 , c0 , d)Level 1U (a, b, c1 , d1 ),U (a, b, c1 , d2 ),U (a, b1 , c, d1 ),U (a, b1 , c, d2 ),U (a, b, c2 , d1 )U (a, b, c2 , d2 )U (a, b2 , c, d1 )U (a, b2 , c, d2 )Table 4: Nested CUI decompositionreverse direction mentioned done follows: run Algorithm 1 withoutdata, resulting table Table 4 (without actual utility values).elicit data non deleted factors (all limited maximal cliques). Next,recover convenient level 0 CUI representation using table, computingdeleted factor (going rightmost columns left) function factorsstored parents.8. Conclusionspresent graphical representation multiattribute utility functions, based conditional utility independence. CUI networks provide potentially compact representationmultiattribute utility function, via functional decomposition lower-dimensional functions depend node parents. CUI weaker independence condition108fiCUI networkspreviously employed basis graphical utility representations, allowing commonpatterns complementarity substitutivity relations disallowed additive models.proposed techniques obtain verify structural information, use construct network elicit numeric data. addition, developed optimizationalgorithm performs particularly well special case CUI trees. casesalso leveraged efficient optimization CAI maps. Finally, show functionsdecomposed set maximal cliques CUI network.technique, CUI networks achieve dimensionality graphical models basedCAI GAI decompositions, yet broadly applicable independence conditions.Acknowledgmentspreliminary version paper published proceedings AAAI-06.work supported part NSF grant IIS-0205435, STIET program NSFIGERT grant 0114368. grateful thorough work anonymous reviewers,whose suggestions provided valued help finalizing paper.Appendix A. ProofsA.1 Lemma 3Proof. Let Z = \ (X ) C = \ (A B). simply apply two independenceconditions consequentially, define f, g that:U (S) = U (XY Z) = f (Y Z) + g(Y Z)UY (S \ ) = f (Y Z) + g(Y Z)(f 0 ((BC) \ )+ g 0 ((BC) \ )UY B (S \ (Y B))) = f(ZBY C) + g(ZBY C)U (S \ (Y B)).Since Z B C = \(AX), last decomposition equivalent decomposition(1) condition CUI(A X , B).A.2 Proposition 6Proof. CAI condition stronger CUI condition, CAI(x, y) CUI(x, y)CUI(y, x). CUI network, node xi must case nodesCUI given parents descendants. obvious since xi CAInodes given parents children.A.3 Lemma 7Proof. determine hxj (), Sc(xj ) needs determined. {xi } Sc(xi )done, scope covered therefore recursively determinedassignment {xi } Sc(xi ).A.4 Lemma 8first introduce two additional lemmas.Lemma 12. Ch({xi } Co(xi )) ({xi } Sc(xi ) Co(xi ))109fiEngel & WellmanProof. Let xj {xi } Co(xi ), Ch(xj ). xj = xi proof immediateCh(xi ) Sc(xi ). Assume xj Co(xi ). know Definition 10 Ch(xj )Sc(xj ) ({xi } Sc(xi ) Co(xi )), proves lemma.Lemma 13. An({xi } Co(xi )) Co(xi )Proof. Let xj An(xi ) (clearly j < i, therefore xj/ Sc(xi )). Let xj1 Sc(xj ).j1 > j undirected path xj1 xj , blocked Sc(xj ). j1 i,xj1 Sc(xi ){xi } unblocked path xj (and xi ). Otherwise,let xj2 Sc(xj1 ), apply argument xj2 . continue xjkxy Sc(xjk ), > point xy Sc(xi ) {xi } path xy , xjk , . . . , xj1 , xj , xirecursion halts (note includes empty scopes), proving xj Co(xi ).left prove An(Co(xi )) Co(xi ). Let xj Co(xi ), An(xj ). Applyingfirst part proof xj , get Co(xj ). Definition Co(xj ), get< j w Sc(y), either w = xj , w Sc(xj ) w Co(xj ). show Co(xi ),need prove cases w {xi } Sc(xi ) Co(xi ).1. w = xj immediately w Co(xi ).2. w Sc(xj ), xj Co(xi ) get either w = xi , w Sc(xi ) w Co(xi ).3. w Co(xj ), repeat argument recursively z Sc(w). Note z precedesw therefore recursion halt point.Lemma 8. Let X = {xi } Co(xi ). Lemma 13, X external ancestors.Lemma 12, external children X Sc(xi ). Therefore \ (X An(X) Ch(X)) =\ (X Sc(xi )) result immediate Proposition 5.A.5 Proposition 9Proof. Let x node G0 . Let = \ (x In(x)) G = \ (x P a(x) Dn(x))G0 . definition G, know CAI(Y, x), also CUI(Y, x). Let/ Y, 6= x (so0In(x) G). ,/ P a(x) = In(x) G . arc (y, x) removed,meaning Dn(x). therefore must case/ . Therefore henceCUI(Y , x).A.6 Proposition 10Proof. G become CUI tree, cycle least one weak link must removed.Since (yi , yi+1 ) weak link c, must removed. Proposition 9,variable ordering must ensure yi+1 ancestor yi . donepath according order p, might another path yi+1 yi . Let p1path. combination p1 p another cycle c1 , therefore mustbroken. Since p comprises strong links, must least one weak link (u, v)p1 . (u, v) removed, v must ancestor u. donepath cycle c1 , path includes p, another path exists,repeat argument. stage get larger cycle ci , larger path110fiCUI networkspi pi1 . Therefore point one path pi must guaranteedvariable ordering, path includes p.A.7 Proposition 11Proof. show Algorithm 1 leads functional decomposition cliques.outer loop algorithm maintains following iteration properties:1. Aij , Qij P a(a)2. Uij defined Kji3. Aij xj cliqueproperties hold trivially initialization. Assume valid factorsstored network outer iteration j inner iteration i, next showremain valid factor Urd created iteration j, i:1. definition Adr = Aij xj . previous iteration definition Qdr ,Aij , Qdr Qij P a(a). definitions Qij Qdr get P a(xj ) Qij Qdr ,together yields result.2. Urd factor CUI decomposition Uji (Kji ) Gij . scope contains: (i)nodes affected last CUI decomposition, i.e. Kji \ Qij =Aij xj = Adr , (ii) node xr , (iii) parents P a(xr ) fixedUji (i.e. P a(xr Kji )). know Kji = Aij xj Qij , xj/ P a(xr ) (becausexr P a(xj )), also P a(xr ) Aj = (using similar argument property 1).Therefore (P a(xr ) Kji ) Qij , (i),(ii),(iii) get Krd = Adr xr Qdr .3. Adr clique definition property previous iteration. xr Qij ,therefore property 1 previous iteration xr P a(a) Aij . Also xrQij P a(xj ) (the last containment immediate definition Qij ). Therefore xrparent members Adr , result Adr xr clique.iteration properties, either Krd clique, Qdr non empty decompositionapplied reach node r outer loop. end process factorsdefined cliques removed. factors remained definedcliques. U (S) still represented new set factors since appliedvalid decompositions factors.ReferencesAbbas, A. (2005). Attribute dominance utility. Decision Analysis, 2, 185206.Bacchus, F., & Grove, A. (1995). Graphical models preference utility. EleventhConference Uncertainty Artificial Intelligence, pp. 310, Montreal.Boutilier, C., Bacchus, F., & Brafman, R. I. (2001). UCP-networks: directed graphicalrepresentation conditional utilities. Seventeenth Conference UncertaintyArtificial Intelligence, pp. 5664, Seattle.111fiEngel & WellmanBoutilier, C., Brafman, R. I., Hoos, H. H., & Poole, D. (1999). Reasoning conditional ceteris paribus preference statements. Fifteenth Conference UncertaintyArtificial Intelligence, pp. 7180, Stockholm.Debreu, G. (1959). Topological methods cardinal utility theory. Arrow, K., Karlin, S.,& Suppes, P. (Eds.), Mathematical Methods Social Sciences. Stanford UniversityPress.Dyer, J. S., & Sarin, R. K. (1979). Measurable multiattribute value functions. OperationsResearch, 27, 810822.Fishburn, P. C. (1965). Independence utility theory whole product sets. OperationsResearch, 13, 2845.Fishburn, P. C. (1967). Interdependence additivity multivariate, unidimensionalexpected utility theory. International Economic Review, 8, 335342.Fishburn, P. C. (1975). Nondecomposable conjoint measurement bisymmetric structures.Journal Mathematical Psychology, 12, 7589.Fuhrken, G., & Richter, M. K. (1991). Polynomial utility. Economic Theory, 1 (3), 231249.Gonzales, C., & Perny, P. (2004). GAI networks utility elicitation. Ninth InternationalConference Principles Knowledge Representation Reasoning, pp. 224234,Whistler, BC, Canada.Gorman, W. M. (1968). structure utility functions. Review Economic Studies,35, 367390.Keeney, R. L., & Raiffa, H. (1976). Decisions Multiple Objectives: PreferencesValue Tradeoffs. Wiley.Krantz, D. H., Luce, R. D., Suppes, P., & Tversky, A. (1971). Foundations Measurement,Vol. 1. Academic Press, New York.La Mura, P., & Shoham, Y. (1999). Expected utility networks. Fifteenth ConferenceUncertainty Artificial Intelligence, pp. 366373, Stockholm.Nilsson, D. (1998). efficient algorithm finding probable configurationsprobabilistic expert systems. Statistics Computing, 8 (2), 159173.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.Pearl, J., & Paz, A. (1989). Graphoids: graph based logic reasoning relevancerelations. Du Boulay, B. (Ed.), Advances Artificial Intelligence II. North-Holland,New York.Tatman, J. A., & Shachter, R. D. (1990). Dynamic programming influence diagrams..20, 365379.Von Stengel, B. (1988). Decomposition multiattribute expected utility functions. AnnalsOperations Research, 16, 161184.Wellman, M. P., & Doyle, J. (1992). Modular utility representation decision-theoreticplanning. First International Conference Artificial Intelligence Planning Systems, pp. 236242, College Park, MD.112fiJournal Artificial Intelligence Research 31 (2008) 205216Submitted 10/07; published 01/08Sound Complete Inference Rules SE-ConsequenceKa-Shu WongKSWONG @ CSE . UNSW. EDU . AUUniversity New South Wales National ICT AustraliaSydney, NSW 2052, AustraliaAbstractnotion strong equivalence logic programs answer set semantics gives rise consequence relation logic program rules, called SE-consequence. present sound completeset inference rules SE-consequence disjunctive logic programs.1. Introductionrecent years much research various notions equivalence two logicprograms. particular, notion strong equivalence logic programs answer set semantics (Lifschitz, Pearce, & Valverde, 2001; Turner, 2001, 2003; Cabalar, 2002; Lin, 2002)received much attention. say two logic programs P Q strongly equivalent iffset rules R, P R Q R answer sets.Recent work area (Eiter, Fink, Tompits, & Woltran, 2004; Turner, 2003; Osorio, Navarro,& Arrazola, 2001) focused simplification logic programs strong equivalence.resulted number logic program transformation rules preserve strong equivalence. transformations, logic program rules identified removedmaintaining strong equivalence original program.paper look different related aspect strong equivalence. notionstrong equivalence logic programs gives rise consequence relation |=s logic programrules, called SE-consequence (Eiter et al., 2004), defined saying rule rconsequence logic program P iff P P r strongly equivalent1 . consequencerelation useful testing strong equivalence, well identifying redundant rules logicprogram simplification.paper, present set inference rules logic program rules showsound complete SE-consequence. set inference rules consists adaptationsseveral well-known logic program simplification rules, together new rule call SHYP. main contribution paper new inference rule S-HYP completeness result.completeness proof makes use construction used reduction strong equivalencetesting classical logic Lin (2002), applies restricted form resolution called lockresolution Boyer (1971).1. Eiter et al. (2004) uses different definition SE-consequence based Turners SE-models (Turner, 2001, 2003).equivalence two definitions proved Section 3.c2008AI Access Foundation. rights reserved.fiW ONG2. Definitionsdeal propositional disjunctive logic programs negation-as-failure, ruleforma1 ; a2 ; ; ak b1 , b2 , , bm , c1 , c2 , , cn .a1 , , ak , b1 , , bm c1 , , cn set atoms, assume setatoms fixed. Given rule r form, denote H(r) = {a1 , , ak } (head r),B + (r) = {b1 , , bm } (positive part r), B (r) = {c1 , , cn } (negative part r).ignore order atoms within rule; therefore, rule considered triple atomsets. abbreviation, include atom sets rule, means atoms setcorresponding part rule. particular, X = {x1 , , xk }, X bodyrule abbreviation x1 , , xk . Applied rule r above, = H(r),B = B + (r) C = B (r), rule abbreviatedB, C.set X atoms logic program P , use notation X |= P mean X modelP classical sense: r P , B + (r) X B (r) X = , H(r) Xnon-empty. say X minimal model P X minimal set inclusion amongmodels P , i.e. X |= P X 0 X 0 X X 0 |= P .Gelfond-Lifschitz reduct (1988) P X program P respect set atoms Xdefined P X = {H(r) B + (r) | r P X B (r) = }. say X answer setP X minimal model P X .3. Strong Equivalencenotion strong equivalence (Lifschitz et al., 2001) describes property two programsremain equivalent regardless additional rules added, defined follows:Definition 1. Logic programs P Q strongly equivalent, iff sets R rules,programs P R Q R answer sets.Lifschitz et al. (2001) showed strong equivalence reduced equivalence logichere-and-there. Based result, Turner (2003) gave following definition SE-models,characterises strong equivalence sense two programs strongly equivalent iffSE-models:Definition 2. Let P logic program, let X, sets atoms. say pair (X, )SE-model P , written (X, ) |= P , X , |= P X |= P . setSE-models, write |= P mean (X, ) |= P (X, ) . Let Ms (P ) denote setSE-models P .SE-models property pair (X, ) SE-model P iff SE-model every ruler P . implies Ms (P Q) = Ms (P ) Ms (Q).notion strong equivalence gives rise consequence relation logic program rules,called SE-consequence denoted |=s . SE-consequence defined by:206fiS OUND C OMPLETE NFERENCE RULES SE-C ONSEQUENCEDefinition 3. Let P, Q logic programs, r logic program rule. say P |=s r iffMs (P ) |= r, i.e. every (X, ) Ms (P ) SE-model r. Furthermore, write P |=s Q iffP |=s r every r Q.equivalent definition SE-consequence make use SE-models:Proposition 1. Let P logic program r logic program rule. P |=s r iff PP {r} strongly equivalent.Proof. P P {r} strongly equivalent iff Ms (P ) = Ms (P {r}) (= Ms (P ) Ms (r)).holds iff Ms (P ) Ms (r), i.e. every (X, ) Ms (P ) SE-model r.relation |=s properties consequence relation:Proposition 2. Let P, Q logic programs, r logic program rule.r P , P |=s rP Q P |=s r Q |=s rP |=s r Q |=s P Q |=s rproofs follow easily fact P |=s Q iff Ms (P ) Ms (Q).exist binary resolution-style calculi logic here-and-there (also known Godels3-valued logic). However, Example 1 suggests cannot applied SE-consequence, seemingly takes one outside logic fragment corresponding disjunctive logic programs.supported fact construction Section 6.1 may produce clausescorrespond logic program rules. currently investigated.4. Inference Rules Strong Equivalenceconsequence relation `s defined following rules inference:(TAUT)x x.(NONMIN)(WGPPE)(CONTRA)x, x.B, C.A; X B, Y, C, Z.A1 B1 , x, C1 .A2 ; x B2 , C2 .A1 ; A2 B1 , B2 , C1 , C2 .A1 B1 , x1 , C1 ....Bn , xn , Cn .x1 , , xn , C.(S-HYP)A1 ; ; B1 , , Bn , C1 , , Cn , A, C.207fiW ONGMany rules well-known: tautological rules (TAUT), contradiction (CONTRA), nonminimal rules (NONMIN), weak partial evaluation (WGPPE) (also called partial deduction,Sakama & Seki, 1997). shown strong equivalence preserving (Brass & Dix,1999; Osorio et al., 2001; Eiter et al., 2004). new rule S-HYP thought formhyper-resolution. knowledge considered literature.Instead S-HYP, one might expect general rule S-HYP+ allows additional positive atoms B final rule:A1 B1 , x1 , C1 ....Bn , xn , Cn .x1 , , xn , B, C.(S-HYP+)A1 ; ; B, B1 , , Bn , C1 , , Cn , A, C.However, shown replacing S-HYP S-HYP+ change consequencerelation:Proposition 3. Let ` consequence relation satisfying CONTRA. S-HYP S-HYP+interchangeable.Proof. Since S-HYP+ general form S-HYP, suffices show S-HYP+simulated using S-HYP CONTRA. SupposeA1 B1 , x1 , C1 ....Bn , xn , Cn .x1 , , xn , b1 , , bk , C.bi , CONTRA gives us bi , bi . using S-HYP rules, plus above,getA1 ; ; B1 , , Bn , b1 , , bk , C1 , , Cn , A, C.result applying S-HYP+ initial set rules.Example 1. consider possibility using binary inference rules logic program P :r1 : x.r2 : y.r3 :x, y.following rule SE-consequence P , derived using S-HYP r1 , r2 r3 :s:a.suppose restrict binary inference rules replacing S-HYP binaryvariant S-HYP+. Applying S-HYP+ r1 r3 gives:r4 :y.208fiS OUND C OMPLETE NFERENCE RULES SE-C ONSEQUENCEusing S-HYP+ r2 r4 gives:t:a.observe weaker s. suggests may possible derive usingbinary inference rules. However, still open question whether n-ary rules indeedrequired.Observe rule CONTRA replaced s-implication (S-IMP) (Wang & Zhou, 2005):Proposition 4. Let ` consequence relation satisfying TAUT WGPPE. CONTRAreplaced following inference rule:(S-IMP)A; X B, C.B, C, X.Proof. (S-IMP CONTRA) rule x, x. formed applying TAUT followedS-IMP.(CONTRA S-IMP) Suppose ruleA; x B, C.applying CONTRA get x, x. followed WGPPE two rules, getB, C, x.repeating steps derive new rules moving set atoms headnegative part rule.note S-IMP special case solution 1-1-0 problem Lin Chen (2007).addition, WGPPE well binary variant S-HYP+ contained solution LinChens 2-1-0 problem.shown inference rules presented sound complete SEconsequence:Theorem 1. P |=s r iff P `s r.Proof (Soundness). prove soundness inference rule S-HYP, soundnessrules already known.proof proceeds contradiction. Assume rule sound. program PA1 B1 , x1 , C1 ....Bn , xn , Cn .x1 , , xn , C.rule rA1 ; ; B1 , , Bn , C1 , , Cn , A, C.209fiW ONGP `s r P 6|=s r. means Ms (P ) 6 Ms (r), P SE-model (X, )SE-model r.(X, ) SE-model r means either 6|= r X 6|= rY . exclude firstcase since clear r classical consequence P . Therefore assume X 6|= rY .1 n Ai X = , Bi X, Ci = . Furthermore AY = CY = .(X, ) SE-model P , hence X |= P . 1 n, followingrule P :Ai Bi , xi , Ci .Since Ai X = , body must hold, rule must eliminated reduct.know Bi X Ci = . Therefore must xi rule eliminatedreduct.= C = x1 , , xn , classical modelx1 , , xn , C.hence 6|= P , contradicts (X, ) SE-model P .5. Background Completeness Proofsection introduce two results used completeness proof.5.1 Lins ConstructionLin (2002) presented method reducing strong equivalence equivalence classical logic.Given logic program, construction produces set clauses two logic programsstrongly equivalent iff two sets clauses equivalent classical logic.Let {x1 , , xn } set atoms. construction, atom xi represented twopropositional letters xi x0i . logic program P consists set rules following form:a1 ; a2 ; ; ak b1 , b2 , , bm , c1 , c2 , , cn .rule r, construct two clauses (r) 0 (r):(r) := a1 ak b1 bm c01 c0n0 (r) := a01 a0k b01 b0m c01 c0nLet (P ) := {(r) | r P } 0 (P ) := { 0 (r) | r P }. Furthermore atom xi addclause xi x0i . Let denote set clauses. Lins result showed P Qstrongly equivalent iff^^((P ) 0 (P ) ) ((Q) 0 (Q) )immediate corollary result is:Proposition 5. P |=s r iff^((P ) 0 (P ) ) |= (r) 0 (r)Observe given clause , find corresponding rule r (r) =literals form x0 .210fiS OUND C OMPLETE NFERENCE RULES SE-C ONSEQUENCE5.2 Boyers Lock ResolutionResolution following inference rule: given two clauses x C1 x C2 , derive clauseC1 C2 . say x x literals resolved on, clause C1 C2 resolvent.well-known resolution refutation-complete, is, set clauses unsatisfiable,empty clause derived using resolution.Definition 4. deduction clause C set clauses sequence clauses C1 , , CnCn = C Ci either clause resolvent clauses preceding Ci .deduction exists, say C derived S.Lock resolution restricted form resolution introduced Boyer (1971). numeric labelgiven literal clause. Resolution permitted literals lowest valuedlabel clause. Note clause one literal label:many literals lowest valued label, resolution allowed.Literals resolvent inherit labels parent clauses. literal resolventtwo possible labels, lower value used. deduction follows restrictions calledlock deduction.Example 2. Consider following clauses:C1 : a(1) b(2)C2 : a(2) b(3)C3 : b(1) c(2)resolve C1 C2 a(1) a(2) form following clause:b(2)Note b labelled 2 C1 3 C2 , lower value used. However cannot resolveC2 C3 b(3) b(1) since 3 minimum label C2 .Boyer showed lock resolution refutation-complete.6. Completeness ProofProof Theorem 1 (Completeness). prove completeness, need show P |=s r impliesP `s r. showing existence lock deduction (r0 ) r0 subsetr (in sense H(r0 ) H(r), B + (r0 ) B + (r), B (r0 ) B (r))construct deduction r P using inference rules.6.1 Lock Deduction (r0 ) (P ) 0 (P )VProposition 5 know (r) logical consequence ((P ) 0 (P ) ). Ideally,want lock deduction (r) (P ) 0 (P ) . However, may possibleresolution refutation-complete. fact show lock deduction (r0 )exist r0 , provided r contain atom head positive body.first obtaining lock deduction D0 empty clause (P ) 0 (P )negation (r). modify form deduction (r0 ) (P ) 0 (P ) ,way restrictions lock deduction preserved.211fiW ONGlabel literals (P ) 0 (P ) either 1 0: literal form x0 ,give label 0, otherwise give label 1. example, (a b, c) becomesa(1) b(1) c0(1)0 (a b, c) becomesa0(1) b0(0) c0(1)Let r rulea1 ; a2 ; ; ak b1 , b2 , , bm , c1 , c2 , , cn .(r)a1 ak b1 bm c01 c0nNegating (r) gives us following set clauses, call N (r):a1 , , ak , b1 , , bm , c01 , , c0nlabel clauses way above. Note (r) contain literals label1, N (r) set ofVsingle-literal clauses, may also label 0. Since(r) consequence ((P ) 0 (P ) ), adding negation (r) makes unsatisfiable.Therefore lock deduction D0 empty clause (P ) 0 (P ) plus N (r).next step, construct new lock deduction contain clausesN (r). Let C10 , , Cn0 clauses D0 . construct inductively clauses C1 , , Cn :Ci0 resolvent. case set Ci := Ci0 . Note Ci0 N (r),removed next step construction.Ci0 resolvent Cj0 Ck0 literals x x, neither Cj0 Ck0 N (r).set Ci resolvent Cj Ck x x.Ci0 resolvent Cj0 Ck0 , one N (r). Without loss generality, assumeCj0 N (r). case set Ci := Ck .complete construction, remove every Ci N (r) resolvent.remaining clauses form deduction D. Note possible Ci0 resolventtwo clauses N (r). r contain atom headpositive body, hence N (r) cannot contain pair complementary literals. Note alsofinal clause Cn never removed, since Cn0 empty clause, N (r).Example 3. Suppose P program consisting single ruleb.r ruleb, a.(P ) 0 (P ) consists clausesa(1) b(1)a0(1) b0(0)a(1) a0(1)212b(1) b0(1)fiS OUND C OMPLETE NFERENCE RULES SE-C ONSEQUENCE(r) b(1) a0(1) (after labelling). negation (r), denoted N (r), consists clausesb(1)a0(0)example lock deduction D0 empty clause (P ) 0 (P ) N (r):(1)(2)(3)(4)(5)(6)(7)b(1)b(1) b0(1)b0(1)a0(1) b0(0)a0(1)a0(0)N (r)resolvent (1), (2)0 (P )resolvent (3), (4)N(r)resolvent (5), (6)construction produces following sequence clauses:(1)(2)(3)(4)(5)(6)(7)b(1)b(1) b0(1)b(1) b0(1)a0(1) b0(0)a0(1) b(1)a0(0)a0(1) b(1)copy (2)0 (P )resolvent (3), (4)resolvent (3), (4)copy (5)clauses marked removed, resulting deduction formed remainingclauses.construction, deduction. need show satisfies restrictions lock deduction. addition, show clause Ci consists Ci0 zero literals(r) added, i.e. Ci = Ci0 disjunction zero literals (r).proof induction.Ci0 resolvent. case Ci Ci0 .Ci0 resolvent clauses Cj0 Ck0 literals x x, neither N (r).Ci resolvent Cj Ck literals x x. induction, Cj = Cj0 jCk = Ck0 k . x occur j k , Ci = Ci0 j k . OtherwiseCi = Ci0 contains literals j k except possibly x x.cases, Ci consists Ci0 plus zero literals (r).resolution step satisfies restriction lock deduction added literalslabelled 1, highest label value use. Thus x lowest label valueCj0 x Ck0 , must hold x Cj x Ck .Ci0 resolvent clauses Cj0 Ck0 , Cj0 N (r). Since clause N (r)negation literal found (r) (note might carry different labels), mustliteral (r) Cj0 = Ck0 = Ci0 . construction Ci gives usCi = Ck , induction Ck = Ck0 k . Therefore Ci = Ci0 k .213fiW ONGshown lock deduction, clause Ci consists Ci0 zeroliterals (r) added. Recall final clause D0 empty clause. Thereforefinal clause clause contains subset literals (r), hencelock deduction (r0 ) r0 subset r.6.2 Existence Deduction r PSuppose lock deduction (r) (P ) 0 (P ) labelling describedabove. prove induction P `s r.BASE C ASE(r) either (P ) 0 (P ) .(r) (P ). r P , therefore P `s r.(r) 0 (P ). means (r) = 0 (s) P . B + (s) must empty sinceliterals form x0 (r). Hence writea1 ; ; ak c1 , cn .ra1 , , ak , c1 , cn .Since `s ai , ai , move atoms head negative part using WGPPE.Therefore P `s r.(r) . case rx, x.atom x, `s r applying CONTRA.NDUCTION TEP(r) resolvent clauses . literal resolved either a0 a0atom a. Assume without loss generality positive literal negativeliteral .literal resolved on. Since literal form x0resolvent, cannot literal form . Therefore find logicprogram rules = (s) = (t). inference rule WGPPE gives uss, `s rP `s s, induction. Therefore P `s r.a0 a0 literal resolved on. a0 , logic programrule = (t). However, find logic program rule = (s),lock resolution property: a0 labelled 1 literal formx0 , labelled 0, .214fiS OUND C OMPLETE NFERENCE RULES SE-C ONSEQUENCEresolvent, literal resolved must form x0 x0 , a0parent clause contains x0 . Again, lock resolutionproperty: presence a0 clause prevents resolution literallabelled 0. Let 2 parent clause containing x0 , let 2 parent clause.2 contains literals labelled 1, find logic program rule s22 = (s2 ).repeating this, form chain resolvents 1 (= ), 2 , , n 1 (= ), 2 , , n .resolvent i+1 i+1 literal a0i i+1 a0i i+1 .logic program rule si corresponding (si ) = .extend chain far possible, n resolvent. n contains literalsform x0 , come 0 (P ). Therefore P 0 (t) = n .Observe ruleX a1 , , , Y.sets atoms X, . cannot additional positive atoms body,correspond literals form x0 n , literals remainingresult chain resolution steps, (r).inference rule S-HYP gives uss1 , , sn , `s rP `s s1 , , sn induction, P `s P . Therefore P `s r.6.3 Final Stepshown P |=s r, find r0 subset r P `s r0 , apartspecial cases. final step proof given applying NONMIN, allowsus deduction r0 `s r r0 subset r.special case r contains atom head positive body handledobserving r produced using rules TAUT followed NONMIN, shows `s r.Therefore P |=s r implies P `s r.7. Conclusionpaper presented sound complete set inference rules SE-consequencedisjunctive logic programs, consisting number well-known logic program transformationrules, TAUT, CONTRA, NONMIN, WGPPE, plus new rule call S-HYP. provedset rules complete SE-consequence using reduction logic programspropositional clauses apply restricted form resolution. result leadssyntactic definition closure operator logic programs strong equivalence. Future workinvolves applying construct logic program update operators respect strong equivalence,well finding similar results notions equivalence logic programs.Acknowledgmentsthank anonymous reviewers many helpful suggestions used revisingpaper. work partially supported scholarship National ICT Australia. NICTA215fiW ONGfunded Australian Governments Backing Australias Ability initiative, partAustralian Research Council.ReferencesBoyer, R. (1971). Locking: Restriction Resolution. Ph.D. thesis, University Texas, Austin.Brass, S., & Dix, J. (1999). Semantics (disjunctive) logic programs based partial evaluation.Journal Logic Programming, 38(3), 167213.Cabalar, P. (2002). three-valued characterization strong equivalence logic programs.Proceedings 18th National Conference Artificial Intelligence (AAAI-2002), pp. 106111.Eiter, T., Fink, M., Tompits, H., & Woltran, S. (2004). Simplifying logic programs uniformstrong equivalence. Proceedings 7th International Conference Logic Programming Nonmonotonic Reasoning, pp. 8799.Gelfond, M., & Lifschitz, V. (1988). stable model semantics logic programming. Proceedings 5th International Conference Logic Programming, pp. 10701080.Lifschitz, V., Pearce, D., & Valverde, A. (2001). Strongly equivalent logic programs. ComputationalLogic, 2(4), 526541.Lin, F. (2002). Reducing strong equivalence logic programs entailment classical propositional logic. Proceedings 8th International Conference Principles KnowledgeRepresentation Reasoning, pp. 170176.Lin, F., & Chen, Y. (2007). Discovering classes strongly equivalent logic programs. JournalArtificial Intelligence Research, 28, 431451.Osorio, M., Navarro, J. A., & Arrazola, J. (2001). Equivalence answer set programming. Proceedings 11th International Workshop Logic Based Program Synthesis Transformation, pp. 5775.Sakama, C., & Seki, H. (1997). Partial deduction disjunctive logic programming. JournalLogic Programming, 32(3), 229245.Turner, H. (2001). Strong equivalence logic programs default theories (made easy).Proceedings 6th International Conference Logic Programming NonmonotonicReasoning, pp. 8192.Turner, H. (2003). Strong equivalence made easy: nested expressions weight constraints. TheoryPractice Logic Programming, 3(4-5), 609622.Wang, K., & Zhou, L. (2005). Comparisons computation well-founded semantics disjunctive logic programs. ACM Transactions Computational Logic, 6(2), 295327.216fiJournal Artificial Intelligence Research 31 (2008) 399-429Submitted 09/07; published 03/08Global Inference Sentence CompressionInteger Linear Programming ApproachJames Clarkejclarke@ed.ac.ukMirella Lapatamlap@inf.ed.ac.ukSchool InformaticsUniversity Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, UKAbstractSentence compression holds promise many applications ranging summarizationsubtitle generation. work views sentence compression optimization problemuses integer linear programming (ILP) infer globally optimal compressionspresence linguistically motivated constraints. show previous formulationssentence compression recast ILPs extend models novel globalconstraints. Experimental results written spoken texts demonstrate improvementsstate-of-the-art models.1. Introductioncomputational treatment sentence compression recently attracted much attentionliterature. task viewed producing summary single sentenceretains important information remains grammatical (Jing, 2000). sentencecompression mechanism would greatly benefit wide range applications. example,summarization, could improve conciseness generated summaries (Jing, 2000;Lin, 2003; Zajic, Door, Lin, & Schwartz, 2007). examples include compressing textdisplayed small screens mobile phones PDAs (Corston-Oliver, 2001),subtitle generation spoken transcripts (Vandeghinste & Pan, 2004), producingaudio scanning devices blind (Grefenstette, 1998).Sentence compression commonly expressed word deletion problem: given input source sentence words x = x1 , x2 , . . . , xn , aim produce target compressionremoving subset words (Knight & Marcu, 2002). compression problem extensively studied across different modeling paradigms, supervisedunsupervised. Supervised models typically trained parallel corpus source sentences target compressions come many flavors. Generative models aim modelprobability target compression given source sentence either directly (Galley& McKeown, 2007) indirectly using noisy-channel model (Knight & Marcu, 2002;Turner & Charniak, 2005), whereas discriminative formulations attempt minimize errorrate training set. include decision-tree learning (Knight & Marcu, 2002), maximum entropy (Riezler, King, Crouch, & Zaenen, 2003), support vector machines (Nguyen,Shimazu, Horiguchi, Ho, & Fukushi, 2004), large-margin learning (McDonald, 2006).c2008AI Access Foundation. rights reserved.fiClarke & LapataUnsupervised methods dispense parallel corpus generate compressions eitherusing rules (Turner & Charniak, 2005) language model (Hori & Furui, 2004).Despite differences formulation, approaches model compression processusing local information. instance, order decide words drop, exploitinformation adjacent words constituents. Local models good jobproducing grammatical compressions, however somewhat limited scope sincecannot incorporate global constraints compression output. constraintsconsider sentence whole instead isolated linguistic units (words constituents).give concrete example may want ensure target compression verb,provided source one first place. verbal arguments presentcompression. pronouns retained. constraints fairly intuitiveused instill linguistic also task specific information model.instance, application compresses text displayed small screens wouldpresumably higher compression rate system generating subtitles spokentext. global constraint could force former system generate compressionsfixed rate fixed number words.Existing approaches model global properties compression problemgood reason. Finding best compression source sentence given spacepossible compressions1 (this search process often referred decoding inference)become intractable many constraints overly long sentences. Typically,decoding problem solved efficiently using dynamic programming often conjunctionheuristics reduce search space (e.g., Turner & Charniak, 2005). Dynamicprogramming guarantees find global optimum provided principle optimality holds. principle states given current state, optimal decisionremaining stages depend previously reached stages previously madedecisions (Winston & Venkataramanan, 2003). However, know falsecase sentence compression. example, included modifiers leftnoun compression probably include noun include verbalso include arguments. dynamic programming approach cannoteasily guarantee constraints hold.paper propose novel framework sentence compression incorporatesconstraints compression output allows us find optimal solution.formulation uses integer linear programming (ILP), general-purpose exact frameworkNP-hard problems. Specifically, show previously proposed models recastinteger linear programs. extend models constraints expresslinear inequalities. Decoding framework amounts finding best solution givenlinear (scoring) function set linear constraints either global local.Although ILP previously used sequence labeling tasks (Roth & Yih, 2004;Punyakanok, Roth, Yih, & Zimak, 2004), application natural language generationless widespread. present three compression models within ILP framework,representative unsupervised (Knight & Marcu, 2002), semi-supervised (Hori & Furui,2004), fully supervised modeling approach (McDonald, 2006). propose smallnumber constraints ensuring compressions structurally semantically1. 2n possible compressions n number words sentence.400fiGlobal Inference Sentence Compressionvalid experimentally evaluate impact compression task. cases,show added constraints yield performance improvements.remainder paper organized follows. Section 2 provides overviewrelated work. Section 3 present ILP framework compression modelsemploy experiments. constraints introduced Section 3.5. Section 4.3discusses experimental set-up Section 5 presents results. Discussion futurework concludes paper.2. Related Workpaper develop several ILP-based compression models. presentingmodels, briefly summarize previous work addressing sentence compression emphasis data-driven approaches. Next, describe ILP techniques usedpast solve inference problems natural language processing (NLP).2.1 Sentence CompressionJing (2000) perhaps first tackle sentence compression problem. approachuses multiple knowledge sources determine phrases sentence remove. Centralsystem grammar checking module specifies sentential constituentsgrammatically obligatory therefore present compression.achieved using simple rules large-scale lexicon. knowledge sources includeWordNet corpus evidence gathered parallel corpus source-target sentencepairs. phrase removed grammatically obligatory, focuslocal context reasonable deletion probability (estimated parallel corpus).contrast Jing (2000), bulk research sentence compression relies exclusively corpus data modeling compression process without recourse extensive knowledge sources (e.g., WordNet). large number approaches basednoisy-channel model (Knight & Marcu, 2002). approaches consist languagemodel P (y) (whose role guarantee compression output grammatical), channelmodel P (x|y) (capturing probability source sentence x expansiontarget compression y), decoder (which searches compression maximizesP (y)P (x|y)). channel model acquired parsed version parallel corpus;essentially stochastic synchronous context-free grammar (Aho & Ullman, 1969) whoserule probabilities estimated using maximum likelihood. Modifications modelpresented Turner Charniak (2005) Galley McKeown (2007) improvedresults.discriminative models (Knight & Marcu, 2002; Riezler et al., 2003; McDonald, 2006;Nguyen et al., 2004) sentences represented rich feature space (also inducedparse trees) goal learn words word spans deleted givencontext. instance, Knight Marcus (2002) decision-tree model, compressionperformed deterministically tree rewriting process inspired shift-reduceparsing paradigm. Nguyen et al. (2004) render model probabilistic usesupport vector machines. McDonald (2006) formalizes sentence compression largemargin learning framework without making reference shift-reduce parsing. modelcompression classification task: pairs words source sentence classified401fiClarke & Lapataadjacent target compression. large number features definedwords, parts-of-speech, phrase structure trees dependencies. featuresgathered adjacent words compression words in-betweendropped (see Section 3.4.3 detailed account).compression models developed written text mind, HoriFurui (2004) propose model automatically transcribed spoken text. modelgenerates compressions word deletion without using parallel data syntactic information way. Assuming fixed compression rate, searches compressionhighest score using dynamic programming algorithm. scoring function consists language model responsible producing grammatical output, significance scoreindicating whether word topical not, score representing speech recognizersconfidence transcribing given word correctly.2.2 Integer Linear Programming NLPILPs constrained optimization problems objective functionconstraints linear equations integer variables (see Section 3.1 details). ILPtechniques recently applied several NLP tasks, including relation extraction(Roth & Yih, 2004), semantic role labeling (Punyakanok et al., 2004), generationroute directions (Marciniak & Strube, 2005), temporal link analysis (Bramsen, Deshpande,Lee, & Barzilay, 2006), set partitioning (Barzilay & Lapata, 2006), syntactic parsing (Riedel& Clarke, 2006), coreference resolution (Denis & Baldridge, 2007).approaches combine local classifier inference procedure basedILP. classifier proposes possible answers assessed presence globalconstraints. ILP used make final decision consistent constraintslikely according classifier. example, semantic role labeling task involvesidentifying verb-argument structure given sentence. Punyakanok et al. (2004) firstuse SNOW, multi-class classifier2 (Roth, 1998), identify label candidate arguments.observe labels assigned arguments sentence often contradict other.resolve conflicts propose global constraints (e.g., argumentinstantiated given verb, every verb least one argument) useILP reclassify output SNOW.Dras (1999) develops document paraphrasing model using ILP. key premisework cases one may want rewrite document conformglobal constraints length, readability, style. proposed model threeingredients: set sentence-level paraphrases rewriting text, set global constraints, objective function quantifies effect incurred paraphrases.formulation, ILP used select paraphrases applyglobal constraints satisfied. Paraphrase generation falls outside scope ILPmodel sentence rewrite operations mainly syntactic provided module basedsynchronous tree adjoining grammar (S-TAG, Shieber & Schabes, 1990). Unfortunately,proof-of-concept presented; implementation evaluation module leftfuture work.2. SNOWs learning algorithm variation Winnow update rule.402fiGlobal Inference Sentence Compressionwork models sentence compression optimization problem. show previously proposed models reformulated context integer linear programmingallows us easily incorporate constraints decoding process. constraints linguistically semantically motivated designed bring less localsyntactic knowledge model help preserve meaning source sentence.Previous work identified several important features compression task (Knight& Marcu, 2002; McDonald, 2006); however, use global constraints novelknowledge. Although sentence compression explicitly formulated termsoptimization, previous approaches rely optimization procedure generatingbest compression. decoding process noisy-channel model searches bestcompression given source channel models. However, compression found usually sub-optimal heuristics used reduce search space locally optimaldue search method employed. example, work Turner Charniak(2005) decoder first searches best combination rules apply. traverseslist compression rules, removes sentences outside 100 best compressions (according channel model). list eventually truncated 25 compressions.models (Hori & Furui, 2004; McDonald, 2006) compression score maximizedusing dynamic programming however yield suboptimal results (see discussionSection 1).Contrary NLP work using ILP (a notable exception Roth & Yih, 2005),view compression generation two stage process learning inferencecarried sequentially (i.e., first local classifier hypothesizes list possible answers best answer selected using global constraints). models integratelearning inference unified framework decoding takes place presenceavailable constraints, local global. Moreover, investigate influenceconstraint set across models learning paradigms. Previous work typically formulates constraints single model (e.g., SNOW classifier) learning paradigm(e.g., supervised). therefore assess constraint-based framework advocatedarticle influences performance expressive models (which require large amountsparallel data) non-expressive ones (which use little parallel data none all).words, able pose answer following question: kinds modelsbenefit constraint-based inference?work close spirit rather different content Dras (1999). concentratecompression, specific paraphrase type, apply models sentence-level.constraints thus affect document whole individual sentences. Furthermore, compression generation integral part ILP models, whereas Dras assumesparaphrases generated separate process.3. Frameworksection present details proposed framework sentence compression.mentioned earlier, work models sentence compression directly optimizationproblem. 2n possible compressions source sentence manyunreasonable, unlikely one compression satisfactory (Knight & Marcu, 2002). Ideally, require function captures operations403fiClarke & Lapata(or rules) performed sentence create compressiontime factoring desirable operation makes resulting compression.perform search possible compressions select best one, determineddesirable is. wide range models expressed framework.prerequisites implementing fairly low, require decoding process expressed linear function set linear constraints. practice, manymodels rely Markov assumption factorization usually solved dynamic programming-based decoding process. algorithms formulated integerlinear programs little effort.first give brief introduction integer linear programming, extension linearprogramming readers unfamiliar mathematical programming. compressionmodels next described Section 3.4 constraints Section 3.5.3.1 Linear ProgrammingLinear programming (LP) problems optimization problems constraints.consist three parts:Decision variables. variables control wish assignoptimal values to.linear function (the objective function). function wish minimizemaximize. function influences values assigned decision variables.Constraints. problems allow decision variables take certainvalues. restrictions constraints.terms best demonstrated simple example taken WinstonVenkataramanan (2003). Imagine manufacturer tables chairs shall callTelfa Corporation. produce table, 1 hour labor 9 square board feet woodrequired. Chairs require 1 hour labor 5 square board feet wood. Telfa6 hours labor 45 square board feet wood available. profit madetable 8 GBP 5 GBP chairs. wish determine number tableschairs manufactured maximize Telfas profit.First, must determine decision variables. case define:x1 = number tables manufacturedx2 = number chairs manufacturedobjective function value wish maximize, namely profit.Profit = 8x1 + 5x2two constraints problem: must exceed 6 hours labor45 square board feet wood must used. Also, cannot create negativeamount chairs tables:404fiGlobal Inference Sentence CompressionLabor constraintx 1 + x2Wood constraint9x1 + 5x2Variable constraintsx1x264500decision variables, objective function constraints determinedexpress LP model:max z = 8x1 + 5x2 (Objective function)subject (s.t.)x1 + x29x1 + 5x2x1x26 (Labor constraint)45 (Wood constraint)00Two basic concepts involved solving LP problems feasibility regionoptimal solution. optimal solution one constraints satisfiedobjective function minimized maximized. specification valuedecision variable referred point. feasibility region LP regionconsisting set points satisfy LPs constraints. optimal solutionlies within feasibility region, point minimum maximum objectivefunction value.set points satisfying single linear inequality half-space. feasibility regiondefined intersection half-spaces (for linear inequalities) formspolyhedron. Telfa example forms polyhedral set (a polyhedral convex set)intersection four constraints. Figure 1a shows feasible region Telfaexample. find optimal solution graph line (or hyperplane) pointsobjective function value. maximization problems called isoprofitline minimization problems isocost line. One isoprofit line representeddashed black line Figure 1a. one isoprofit line find isoprofitlines moving parallel original isoprofit line.extreme points polyhedral set defined intersections linesform boundaries polyhedral set (points B C Figure 1a).shown LP optimal solution, extreme point globallyoptimal. reduces search space optimization problem finding extremepoint highest lowest value. simplex algorithm (Dantzig, 1963) solves LPsexploring extreme points polyhedral set. Specifically, moves one extremepoint adjacent extreme point (extreme points lie line segment)optimal extreme point found. Although simplex algorithm exponentialworst-case complexity, practice algorithm efficient.159optimal solution Telfa example z = 1654 , x1 = 4 , x2 = 4 . Thus,achieve maximum profit 41.25 GBP must build 3.75 tables 2.25 chairs.obviously impossible would expect people buy fractions tables chairs.Here, want able constrain problem decision variablestake integer values. done Integer Linear Programming.405fiClarke & Lapataa.b.101099= LPs feasible region9x1 + 5x2 = 459x1+ 5x2 = 4588776 B6x2 5x2 544= IP feasible point= IP relaxations feasible region33Optimal LP solutionOptimal LP solution2C2x 1 + x2 = 6100123x1456x 1 + x2 = 611700123x14567Figure 1: Feasible region Telfa example using linear (graph (a)) integer linear(graph (b)) programming3.2 Integer Linear ProgrammingInteger linear programming (ILP) problems LP problemsvariables required non-negative integers. formulated similar mannerLP problems added constraint decision variables must take non-negativeinteger values.formulate Telfa problem ILP model merely add constraints x1x2 must integer. gives:max z = 8x1 + 5x2 (Objective function)subject (s.t.)x1 + x29x1 + 5x2x1x26 (Labor constraint)45 (Wood constraint)0; x1 integer0; x2 integerLP models, proved optimal solution lies extreme pointfeasible region. case integer linear programs, wish consider pointsinteger values. illustrated Figure 1b Telfa problem. contrastlinear programming, solved efficiently worst case, integer programmingproblems many practical situations NP-hard (Cormen, Leiserson, & Rivest, 1992).406fiGlobal Inference Sentence CompressionFortunately, ILPs well studied optimization problem number techniquesdeveloped find optimal solution. Two techniques cutting planesmethod (Gomory, 1960) branch-and-bound method (Land & Doig, 1960).briefly discuss methods here. detailed treatment refer interestedreader Winston Venkataramanan (2003) Nemhauser Wolsey (1988).cutting planes method adds extra constraints slice parts feasible regioncontains integer extreme points. However, process difficultimpossible (Nemhauser & Wolsey, 1988). branch-and-bound method enumeratespoints ILPs feasible region prunes sections region knownsub-optimal. relaxing integer constraints solving resultingLP problem (known LP relaxation). solution LP relaxation integral,optimal solution. Otherwise, resulting solution provides upper boundsolution ILP. algorithm proceeds creating two new sub-problems basednon-integer solution one variable time. solved processrepeats optimal integer solution found.Using branch-and-bound method, find optimal solution Telfaproblem z = 40, x1 = 5, x2 = 0; thus, achieve maximum profit 40 GBP, Telfamust manufacture 5 tables 0 chairs. relatively simple problem, couldsolved merely inspection. ILP problems involve many variables constraintsresulting feasible region large number integer points. branch-and-boundprocedure efficiently solve ILPs matter seconds forms part manycommercial ILP solvers. experiments use lp solve 3 , free optimization packagerelies simplex algorithm brand-and-bound methods solving ILPs.Note special circumstances solving methods may applicable.example, implicit enumeration used solve ILPs variables binary(also known pure 01 problems). Implicit enumeration similar branch-andbound method, systematically evaluates possible solutions, without however explicitlysolving (potentially) large number LPs derived relaxation. removesmuch computational complexity involved determining sub-problem infeasible. Furthermore, class ILP problems known minimum cost network flowproblems (MCNFP), LP relaxation always yields integral solution. problemstherefore treated LP problems.general, model yield optimal solution variables integersconstraint matrix property known total unimodularity. matrix totallyunimodular every square sub-matrix determinant equal 0, +1 1.case constraint matrix looks totally unimodular, easierproblem solve branch-and-bound methods. practice goodformulate ILPs many variables possible coefficients 0, +1 1constraints (Winston & Venkataramanan, 2003).3.3 Constraints Logical ConditionsAlthough integer variables ILP problems may take arbitrary values, frequentlyrestricted 0 1. Binary variables (01 variables) particularly useful rep3. software available http://lpsolve.sourceforge.net/.407fiClarke & LapataConditionImplicationIffXorStatementbbb cxor b xor cbConstraintba0ab=0a+b+c1a+b+c=1= 1; b = 11a=1Table 1: represent logical conditions using binary variables constraints ILP.resenting variety logical conditions within ILP framework use constraints. Table 1 lists several logical conditions equivalent constraints.also express transitivity, i.e., c b. Although often thought transitivity expressed polynomial expression binaryvariables (i.e., ab = c), possible replace latter following linear inequalities (Williams, 1999):(1 c) + 1(1 c) + b 1c + (1 a) + (1 b) 1easily extended model indicator variables representing whether set binaryvariables take certain values.3.4 Compression Modelssection describe three compression models reformulate integer linearprograms. first model simple language model used baselineprevious research (Knight & Marcu, 2002). second model based work HoriFurui (2004); combines language model corpus-based significance scoringfunction (we omit confidence score derived speech recognizer sincemodels applied text only). model requires small amount parallel datalearn weights language model significance score.third model fully supervised, uses discriminative large-margin framework(McDonald, 2006), trained trained larger parallel corpus. chose modelinstead popular noisy-channel decision-tree models, two reasons, practical one theoretical one. First, McDonalds (2006) model delivers performance superiordecision-tree model (which turn performs comparably noisy-channel). Second, noisy channel entirely appropriate model sentence compression.uses language model trained uncompressed sentences even though representsprobability compressed sentences. result, model consider compressed sentences less likely uncompressed ones (a discussion provided Turner &Charniak, 2005).408fiGlobal Inference Sentence Compression3.4.1 Language Modellanguage model perhaps simplest model springs mind. requireparallel corpus (although relatively large monolingual corpus necessary training),naturally prefer short sentences longer ones. Furthermore, language modelused drop words either infrequent unseen training corpus. KnightMarcu (2002) use bigram language model baseline noisy-channeldecision-tree models.Let x = x1 , x2 , . . . , xn denote source sentence wish generate targetcompression. introduce decision variable word source constrainbinary; value 0 represents word dropped, whereas value 1 includesword target compression. Let:=(1 xi compression[1 . . . n]0 otherwiseusing unigram language model, objective function would maximizeoverall sum decision variables (i.e., words) multiplied unigram probabilities(all probabilities throughout paper log-transformed):maxnXP (xi )(1)i=1Thus, word selected, corresponding given value 1, probabilityP (xi ) according language model counted total score.unigram language model probably generate many ungrammatical compressions.therefore use context-aware model objective function, namely trigrammodel. Dynamic programming would typically used decode language modeltraversing sentence left-to-right manner. algorithm efficient providescontext required conventional language model. However, difficultimpossible incorporate global constraints model decisions wordinclusion cannot extend beyond three word window. formulating decoding processtrigram language model integer linear program able take accountconstraints affect compressed sentence globally. process muchinvolved task unigram case context, instead mustmake decisions based word sequences rather isolated words. first createadditional decision variables:=(ij =1ijk =1 xi starts compression[1 . . . n]0 otherwisesequence xi , xj endscompression[0 . . . n 1]0 otherwisej [i + 1 . . . n]1sequence xi , xj , xk [0 . . . n 2]compression j [i + 1 . . . n 1]0 otherwisek [j + 1 . . . n]409fiClarke & Lapataobjective function given Equation (2). sum possible trigramsoccur compressions source sentence x0 represents starttoken xi ith word sentence x. Equation (3) constrains decision variablesbinary.max z =nXP (xi |start)i=1n2nX n1X X+ijk P (xk |xi , xj )i=1 j=i+1 k=j+1+n1XnXij P (end|xi , xj )(2)i=0 j=i+1subject to:, , ij , ijk = 0 1(3)objective function (2) allows combination trigrams selected.means invalid trigram sequences (e.g., two trigrams containing end token)could appear target compression. avoid situation introducing sequentialconstraints (on decision variables , ijk , , ij ) restrict set allowabletrigram combinations.Constraint 1Exactly one word begin sentence.nX= 1(4)i=1Constraint 2 word included sentence must either start sentencepreceded two words one word start token x0 .k kk2X k1Xijk = 0(5)i=0 j=1k : k [1 . . . n]Constraint 3 word included sentence must either preceded oneword followed another must preceded one word end sentence.jj1XnXijki=0 k=j+1j1Xij = 0(6)i=0j : j [1 . . . n]Constraint 4 word sentence must followed two words followedone word end sentence must preceded one word endsentence.n1XnXj=i+1 k=j+1ijknXj=i+1410iji1Xhi = 0h=0: [1 . . . n](7)fiGlobal Inference Sentence CompressionConstraint 5Exactly one word pair end sentence.n1XnXij = 1(8)i=0 j=i+1sequential constraints described ensure second order factorization (fortrigrams) holds different compression-specific constraints presented Section 3.5.Unless normalized sentence length, language model naturally prefer one-wordoutput. normalization however non-linear cannot incorporated ILPformulation. Instead, impose constraint length compressed sentence.Equation (9) forces compression contain least b tokens.nXb(9)i=1Alternatively, could force compression exactly b tokens (by substitutinginequality equality (9)) less b tokens (by replacing ).4constraint (9) language model-specific used elsewhere.3.4.2 Significance Modellanguage model described notion content words includecompression thus prefers words seen before. words constituentsdifferent relative importance different documents even sentences.Inspired Hori Furui (2004), add objective function (see Equation (2))significance score designed highlight important content words. Hori Furuisoriginal formulation word weighted score similar un-normalized tf idf .significance score applied indiscriminately words sentence solelytopic-related words, namely nouns verbs. score differs one respect. combinesdocument-level sentence-level significance. addition tf idf , wordweighted level embedding syntactic tree.Intuitively, sentence multiply nested clauses, deeply embedded clausestend carry semantic content. illustrated Figure 2 depictsclause embedding sentence Mr Field said resign reselected,move could divide party nationally. Here, important informationconveyed clauses S3 (he resign) S4 (if reselected) embedded.Accordingly, give weight words found clauses mainclause (S1 Figure 2). simple way enforce give clauses weight proportionallevel embedding. modified significance score becomes:I(xi ) =Falfi logNFi(10)xi topic word, fi Fi frequency xi document corpusrespectively, Fa sum topic words corpus, l number clause4. Compression rate also limited range including two inequality constraints.411fiClarke & LapataS1S2Mr Field saidS3resignS4reselected, moveSBARcould divide party nationallyFigure 2: clause embedding sentence Mr Field said resignreselected, move could divide party nationally; nested boxescorrespond nested clauses.constituents xi , N deepest level clause embedding. Fa Fiestimated large document collection, fi document-specific, whereas Nl sentencespecific. So, Figure 2 term Nl 1.0 (4/4) clause S4 , 0.75 (3/4) clause S3 ,on. Individual words inherit weight clauses.modified objective function significance score given below:max z =nXI(xi ) +i=1n2X n1X+nXP (xi |start)i=1nXijk P (xk |xi , xj )i=1 j=i+1 k=j+1+n1XnXij P (end|xi , xj )(11)i=0 j=i+1also add weighting factor () objective, order counterbalance importance language model significance score. weight tuned smallparallel corpus. sequential constraints Equations (4)(8) used ensuretrigrams combined valid way.3.4.3 Discriminative Modelfully supervised model, used discriminative model presented McDonald(2006). model uses large-margin learning framework coupled feature setdefined compression bigrams syntactic structure.Let x = x1 , . . . , xn denote source sentence target compression = y1 , . . . , ymyj occurs x. function L(yi ) {1 . . . n} maps word yi target com412fiGlobal Inference Sentence Compressionpression index word source sentence, x. also include constraintL(yi ) < L(yi+1 ) forces word x occur compressiony. Let score compression sentence x be:(12)s(x, y)score factored using first-order Markov assumption words targetcompression give:s(x, y) =|y|Xs(x, L(yj1 ), L(yj ))(13)j=2score function defined dot product high dimensional featurerepresentation corresponding weight vector:s(x, y) =|y|Xw f (x, L(yj1 ), L(yj ))(14)j=2Decoding model amounts finding combination bigrams maximizesscoring function (14). McDonald (2006) uses dynamic programming approachmaximum score found left-to-right manner. algorithm extensionViterbi case scores factor dynamic sub-strings (Sarawagi & Cohen,2004; McDonald, Crammer, & Pereira, 2005a). allows back-pointers usedreconstruct highest scoring compression well k-best compressions.similar trigram language model decoding process (see Section 3.4.1),except bigram model used. Consequently, ILP formulation slightlysimpler trigram language model. Let:=(1 xi compression(1 n)0 otherwiseintroduce decision variables:==ij =(((1 xi starts compression[1 . . . n]0 otherwise1 word xi ends compression0 otherwise[1 . . . n]1 sequence xi , xj compression [1 . . . n 1]0 otherwisej [i + 1 . . . n]discriminative model expressed as:max z =nXi=1n1Xs(x, 0, i)+s(x, i, n + 1)+nXi=1 j=i+1nXi=1413ij s(x, i, j)(15)fiClarke & LapataConstraint 1Exactly one word begin sentence.nX= 1(16)i=1Constraint 2 word included sentence must either start compressionfollow another word.j jjXij = 0(17)i=1j : j [1 . . . n]Constraint 3 word included sentence must either followed anotherword end sentence.nXij = 0(18)j=i+1: [1 . . . n]Constraint 4Exactly one word end sentence.nX= 1(19)i=1Again, sequential constraints Equations (16)(19) necessary ensureresulting combination bigrams valid.current formulation provides single optimal compression given model. However, McDonalds (2006) dynamic programming algorithm capable returning k-bestcompressions; useful learning algorithm described later. order producek-best compressions, must rerun ILP extra constraints forbid previoussolutions. words, first formulate ILP above, solve it, add solutionk-best list, create set constraints forbid configuration decisionvariables form current solution. procedure repeated k compressionsfound.computation compression score crucially relies dot producthigh dimensional feature representation corresponding weight vector (see Equation (14)). McDonald (2006) employs rich feature set defined adjacent wordsindividual parts-of-speech, dropped words phrases source sentence, dependency structures (also source sentence). features designed mimicinformation presented previous noisy-channel decision-tree models KnightMarcu (2002). Features adjacent words used proxy language modelnoisy channel. Unlike models, treat parses gold standard, McDonalduses dependency information another form evidence. Faced parsesnoisy learning algorithm reduce weighting given features prove414fiGlobal Inference Sentence Compressionpoor discriminators training data. Thus, model much robustportable across different domains training corpora.weight vector, w learned using Margin Infused Relaxed Algorithm (MIRA,Crammer & Singer, 2003) discriminative large-margin online learning technique (McDonald, Crammer, & Pereira, 2005b). algorithm learns compressing sentencecomparing result gold standard. weights updated scorecorrect compression (the gold standard) greater score compressions margin proportional loss. loss function number words falselyretained dropped incorrect compression relative gold standard. sourcesentence exponentially many compressions thus exponentially many marginconstraints. render learning computationally tractable, McDonald et al. (2005b) createconstraints k compressions currently highest score, bestk (x; w).3.5 Constraintsready describe compression-specific constraints. models presentedprevious sections contain sequential constraints thus equivalentoriginal formulation. constraints linguistically semantically motivatedsimilar fashion grammar checking component Jing (2000). However,rely additional knowledge sources (such grammar lexicon WordNet)beyond parse grammatical relations source sentence. obtainRASP (Briscoe & Carroll, 2002), domain-independent, robust parsing system English.However, parser broadly similar output (e.g., Lin, 2001) could also servepurposes. constraints revolve around modification, argument structure, discourserelated factors.Modifier Constraints Modifier constraints ensure relationships head wordsmodifiers remain grammatical compression:j 0(20)i, j : xj xi ncmodsj 0(21)i, j : xj xi detmodsEquation (20) guarantees include non-clausal modifier5 (ncmod) compression (such adjective noun) head modifier must also included;repeated determiners (detmod) (21). Table 2 illustrate constraints disallow deletion certain words (starred sentences denote compressionswould possible given constraints). example, modifier word Pasoksentence (1a) compression, head Party also included (see (1b)).also want ensure meaning source sentence preservedcompression, particularly face negation. Equation (22) implements forcingcompression head included (see sentence (2b) Table 2). similarconstraint added possessive modifiers (e.g., his, our), including genitives (e.g., Johns5. Clausal modifiers (cmod) adjuncts modifying entire clauses. example ate cakehungry, because-clause modifier sentence ate cake.415fiClarke & Lapata1a.1b.2a.2b.2c.3a.3b.3c.3d.3e.3f.became power player Greek Politics 1974, foundedsocialist Pasok Party.*He became power player Greek Politics 1974, foundedPasok.took troubled youth dont fathers, broughtroom Dads dont children.*We took troubled youth fathers, broughtroom Dads children.*We took troubled youth dont fathers, broughtroom Dads dont children.chain stretched Uganda Grenada Nicaragua, since 1970s.*Stretched Uganda Grenada Nicaragua, since 1970s.*The chain Uganda Grenada Nicaragua, since 1970s.*The chain stretched Uganda Grenada Nicaragua, since 1970s.*The chain stretched Grenada Nicaragua, since 1970s.*The chain stretched Uganda Grenada Nicaragua, since 1970s.Table 2: Examples compressions disallowed set constraints.gift), shown Equation (23). example possessive constraint givensentence (2c) Table 2.j = 0(22)i, j : xj xi ncmods xj =j = 0(23)i, j : xj xi possessive modsArgument Structure Constraints also define intuitive constraints takeoverall sentence structure account. first constraint (Equation (24)) ensuresverb present compression arguments,arguments included compression verb must also included. thusforce program make decision verb, subject, object (seesentence (3b) Table 2).j = 0(24)i, j : xj subject/object verb xisecond constraint forces compression contain least one verb providedsource sentence contains one well:X1(25)i:xi verbsconstraint entails possible drop main verb stretched sentence (3a) (see also sentence (3c) Table 2).416fiGlobal Inference Sentence Compressionsentential constraints include Equations (26) (27) apply prepositional phrases subordinate clauses. constraints force introducing term(i.e., preposition, subordinator) included compression wordwithin syntactic constituent also included. subordinator mean wh-words(e.g., who, which, how, where), word that, subordinating conjunctions (e.g., after,although, because). reverse also true, i.e., introducing term included,least one word syntactic constituent also included.j 0(26)i, j : xj PP/SUBxi starts PP/SUBXj 0(27)i:xi PP/SUBj : xj starts PP/SUBexample consider sentence (3d) Table 2. Here, cannot drop prepositionUganda compression. Conversely, must include Ugandacompression (see sentence (3e)).also wish handle coordination. two head words conjoined sourcesentence, included compression coordinating conjunction mustalso included:(1 ) + j 1(28)(1 ) + k 1(29)+ (1 j ) + (1 k ) 1(30)i, j, k : xj xk conjoined xiConsider sentence (3f) Table 2. Uganda Nicaragua presentcompression, must include conjunction and.Finally, Equation (31) disallows anything within brackets source sentenceincluded compression. somewhat superficial attempt excludingparenthetical potentially unimportant material compression.= 0(31): xi bracketed words (inc parentheses)Discourse Constraints discourse constraint concerns personal pronouns. Specifically, Equation (32) forces personal pronouns included compression.constraint admittedly important generating coherent documents (as opposedindividual sentences). nevertheless impact sentence-level compressions,particular verbal arguments missed parser. pronominal,constraint (32) result grammatical output since argument structuresource sentence preserved compression.= 1: xi personal pronouns417(32)fiClarke & Lapatanote constraints described would capturedmodels learn synchronous deletion rules corpus. example, noisy-channelmodel Knight Marcu (2002) learns drop head latter modifiedadjective noun, since transformations DT NN DT AJD NN ADJalmost never seen data. Similarly, coordination constraint (Equations (28)(30))would enforced using Turner Charniaks (2005) special rules enhanceparallel grammar rules modeling structurally complicated deletionsattested corpus. designing constraints aimed capturing appropriatedeletions many possible models, including rely training corpusexplicit notion parallel grammar (e.g., McDonald, 2006).modification constraints would presumably redundant noisy-channel model,could otherwise benefit specialized constraints, e.g., targeting sparse rulesnoisy parse trees, however leave future work.Another feature modeling framework presented deletions (or nondeletions) treated unconditional decisions. example, require dropnoun adjective-noun sequences adjective deleted well. also requirealways include verb compression source sentence one. hardwired decisions could cases prevent valid compressions considered. instance,possible compress sentence appropriate behaviorappropriate orBob loves Mary John loves Susan Bob loves Mary JohnSusan. Admittedly lose expressive power, yet ensure compressionsbroadly grammatically, even unsupervised semi-supervised models. Furthermore, practice find models consistently outperform non-constraint-basedalternatives, without extensive constraint engineering.3.6 Solving ILPmentioned earlier (Section 3.1), solving ILPs NP-hard. cases coefficient matrix unimodular, shown optimal solution linearprogram integral. Although coefficient matrix problems unimodular,obtained integral solutions sentences experimented (approximately 3,000,see Section 4.1 details). conjecture due fact variables 0, +1 1 coefficients constraints therefore constraint matrixshares many properties unimodular matrix. generate solve ILP everysentence wish compress. Solve times less second per sentence (includinginput-output overheads) models presented here.4. Experimental Set-upevaluation experiments motivated three questions: (1) constraintbased compression models deliver performance gains non-constraint-based ones?expect better compressions model variants incorporate compression-specificconstraints. (2) differences among constraint-based models? Here, would likeinvestigate much modeling power gained addition constraints.example, may case state-of-the-art model like McDonalds (2006)benefit much addition constraints. effect much bigger less418fiGlobal Inference Sentence Compressionsophisticated models. (3) models reported paper port across domains?particular, interested assessing whether models proposed constraintsgeneral robust enough produce good compressions written spokentexts.next describe data sets models trained tested (Section 4.1),explain model parameters estimated (Section 4.2) present evaluation setup(Section 4.3). discuss results Section 5.4.1 Corporaintent assess performance models described written spokentext. appeal written text understandable since summarization work todayfocuses domain. Speech data provides natural test-bed compressionapplications (e.g., subtitle generation) also poses additional challenges. Spoken utterances ungrammatical, incomplete, often contain artefacts false starts,interjections, hesitations, disfluencies. Rather focusing spontaneous speechabundant artefacts, conduct study less ambitious domainbroadcast news transcripts. lies in-between extremes written text spontaneous speech scripted beforehand usually read autocue.Previous work sentence compression almost exclusively used Ziff-Davis corpustraining testing purposes. corpus originates collection news articlescomputer products. created automatically matching sentences occurarticle sentences occur abstract (Knight & Marcu, 2002). abstractsentences contain subset source sentences words word orderremain same. earlier work (Clarke & Lapata, 2006) arguedZiff-Davis corpus ideal studying compression several reasons. First, showedhuman-authored compressions differ substantially Ziff-Davis tendsaggressively compressed. Second, humans likely drop individual wordslengthy constituents. Third, test portion Ziff-Davis contains solely 32 sentences. extremely small data set reveal statistically significant differencesamong systems. fact, previous studies relied almost exclusively human judgmentsassessing well-formedness compressed output, significance tests reportedby-subjects analyses only.thus focused present study manually created corpora. Specifically,asked annotators perform sentence compression removing tokens sentence-bysentence basis. Annotators free remove words deemed superfluous provideddeletions: (a) preserved important information source sentence,(b) ensured compressed sentence remained grammatical. wished, could leavesentence uncompressed marking inappropriate compression.allowed delete whole sentences even believed contained information contentrespect story would blur task abstracting. Followingguidelines, annotators produced compressions 82 newspaper articles (1,433 sentences)British National Corpus (BNC) American News Text corpus (henceforthwritten corpus) 50 stories (1,370 sentences) HUB-4 1996 English BroadcastNews corpus (henceforth spoken corpus). written corpus contains articles LA419fiClarke & LapataTimes, Washington Post, Independent, Guardian Daily Telegraph. spokencorpus contains broadcast news variety networks (CNN, ABC, CSPAN NPR)manually transcribed segmented story sentence level.corpora split training, development testing sets6 randomly articleboundaries (with set containing full stories) publicly available http://homepages.inf.ed.ac.uk/s0460084/data/.4.2 Parameter Estimationwork present three compression models ranging unsupervised semisupervised, fully supervised. unsupervised model simply relies trigram language model driving compression (see Section 3.4.1). estimated 25 million tokens North American corpus using CMU-Cambridge Language ModelingToolkit (Clarkson & Rosenfeld, 1997) vocabulary size 50,000 tokens GoodTuring discounting. discourage one-word output force ILP generate compressions whose length less 40% source sentence (see constraint (9)).semi-supervised model weighted combination word-based significance scorelanguage model (see Section 3.4.2). significance score calculated using25 million tokens American News Text corpus. optimized weight (seeEquation (11)) small subset training data (three documents case) using Powells method (Press, Teukolsky, Vetterling, & Flannery, 1992) loss functionbased F-score grammatical relations found gold standard compressionsystems best compression (see Section 4.3 details). optimal weightapproximately 1.8 written corpus 2.2 spoken corpus.McDonalds (2006) supervised model trained written spoken trainingsets. implementation used feature sets McDonald, differencephrase structure dependency features extracted outputRoarks (2001) parser. McDonald uses Charniaks (2000) parser performs comparably.model learnt using k-best compressions. development data, foundk = 10 provided best performance.4.3 EvaluationPrevious studies relied almost exclusively human judgments assessing wellformedness automatically derived compressions. typically rated naive subjects two dimensions, grammaticality importance (Knight & Marcu, 2002). Althoughautomatic evaluation measures proposed (Riezler et al., 2003; Bangalore, Rambow, & Whittaker, 2000) use less widespread, suspect due small sizetest portion Ziff-Davis corpus commonly used compression work.evaluate output models two ways. First, present results usingautomatic evaluation measure put forward Riezler et al. (2003). comparegrammatical relations found system compressions found goldstandard. allows us measure semantic aspects summarization quality termsgrammatical-functional information quantified using F-score. Furthermore,6. splits 908/63/462 sentences written corpus 882/78/410 sentences spokencorpus.420fiGlobal Inference Sentence CompressionClarke Lapata (2006) show relations-based F-score correlates reliablyhuman judgments compression output. Since test corpora larger ZiffDavis (by factor ten), differences among systems highlighted usingsignificance testing.implementation F-score measure used grammatical relations annotationsprovided RASP (Briscoe & Carroll, 2002). parser particularly appropriatecompression task since provides parses full sentences sentence fragmentsgenerally robust enough analyze semi-grammatical sentences. calculated F-scorerelations provided RASP (e.g., subject, direct/indirect object, modifier; 15total).line previous work also evaluate models eliciting human judgments.Following work Knight Marcu (2002), conducted two separate experiments.first experiment participants presented source sentence targetcompression asked rate well compression preserved importantinformation source sentence. second experiment, asked rategrammaticality compressed outputs. cases used five point ratingscale high number indicates better performance. randomly selected 21 sentencestest portion corpus. sentences compressed automaticallythree models presented paper without constraints. also includedgold standard compressions. materials thus consisted 294 (21 2 7) sourcetarget sentences. Latin square design ensured subjects see two differentcompressions sentence. collected ratings 42 unpaid volunteers, selfreported native English speakers. studies conducted Internet usingcustom build web interface. Examples experimental items given Table 3.5. ResultsLet us first discuss results compression output evaluated terms F-score.Tables 4 5 illustrate performance models written spoken corpora,respectively. also present compression rate7 system. casesconstraint-based models (+Constr) yield better F-scores non-constrained ones.difference starker semi-supervised model (Sig). constraints bringimprovement 17.2% written corpus 18.3% spoken corpus.examined whether performance differences among models statistically significant, usingWilcoxon test. written corpus constraint models significantly outperformmodels without constraints. tendency observed spoken corpus exceptmodel McDonald (2006) performs comparably without constraints.also wanted establish best constraint model. corporafind language model performs worst, whereas significance model McDonaldperform comparably (i.e., F-score differences statistically significant). getfeeling difficulty task, calculated much annotators agreedcompression output. inter-annotator agreement (F-score) written corpus65.8% spoken corpus 73.4%. agreement higher spoken texts sinceconsists many short utterances (e.g., Okay, Thats now, Good night)7. term refers percentage words retained source sentence compression.421fiClarke & LapataSourceaim give councils control future growth secondhomes.Goldaim give councils control growth homes.LMaim future.LM+Constr aim give councils control.Sigaim give councils control future growth homes.Sig+Constr aim give councils control future growth homes.McDaim give councils.McD+Constr aim give councils control growth homes.SourceClinton administration recently unveiled new means encouragebrownfields redevelopment form tax incentive proposal.GoldClinton administration unveiled new means encourage brownfields redevelopment tax incentive proposal.LMClinton administration form tax.LM+Constr Clinton administration unveiled means encourage redevelopment form.SigClinton administration unveiled encourage brownfields redevelopment form tax proposal.Sig+Constr Clinton administration unveiled means encourage brownfieldsredevelopment form tax proposal.McDClinton unveiled means encourage brownfields redevelopmenttax incentive proposal.McD+Constr Clinton administration unveiled means encourage brownfieldsredevelopment form incentive proposal.Table 3: Example compressions produced systems (Source: source sentence, Gold:gold-standard compression, LM: language model compression, LM+Constr: language model compression constraints, Sig: significance model, Sig+Constr:significance model constraints, McD: McDonalds (2006) compression model,McD+Constr: McDonalds (2006) compression model constraints).compressed little all. Note marked differenceautomatic human compressions. best performing systems inferior humanoutput 20 F-score percentage points.Differences automatic systems human output also observedrespect compression rate. seen language model compressesaggressively, whereas significance model McDonald tend conservativecloser gold standard. Interestingly, constraints necessarily increasecompression rate. latter increases significance model decreaseslanguage model remains relatively constant McDonald. straightforwardimpose compression rate constraint-based models (e.g., forcing modelPretain b tokens ni=1 = b). However, refrained since wanted422fiGlobal Inference Sentence CompressionModelsLMSigMcDLM+ConstrSig+ConstrMcD+ConstrGoldCompR46.260.660.141.272.063.770.3F-score18.423.336.028.240.540.8Table 4: Results written corpus; compression rate (CompR) grammatical relation F-score (F-score); : +Constr model significantly different modelwithout constraints; : significantly different LM+Constr.ModelsLMSigMcDLM+ConstrSig+ConstrMcD+ConstrGoldCompR52.060.968.649.578.468.576.1F-score25.430.447.634.848.750.1Table 5: Results spoken corpus; compression rate (CompR) grammatical relation F-score (F-score); : +Constr model significantly different withoutconstraints; : significantly different LM+Constr.models regulate compression rate sentence individually accordingspecific information content structure.next consider results human study assesses detail qualitygenerated compressions two dimensions, namely grammaticality informationcontent. F-score conflates two dimensions therefore theory could unduly rewardsystem produces perfectly grammatical output without information loss. Tables 67 show mean ratings8 system (and gold standard) writtenspoken corpora, respectively. first performed Analysis Variance (Anova)examine effect different system compressions. Anova revealed reliable effectgrammaticality importance corpus (the effect significantsubjects items (p < 0.01)).next examine impact constraints (+Constr tables). casesobserve increase ratings grammaticality importance modelsupplemented constraints. Post-hoc Tukey tests reveal grammaticalityimportance ratings language model significance model significantly improve8. statistical tests reported subsequently done using mean ratings.423fiClarke & LapataModelsGrammar2.25$ImportanceLMSigMcD3.051.82$2.99$2.84LM+ConstrSig+ConstrMcD+ConstrGold3.473.763.504.252.37$3.533.173.982.26$Table 6: Results written text corpus; average grammaticality score (Grammar)average importance score (Importance) human judgments; : +Constr modelsignificantly different model without constraints; : significantly differentgold standard; $ : significantly different McD+Constr.ModelsGrammar2.20$ImportanceLMSigMcD2.29$3.331.562.643.32LM+ConstrSig+ConstrMcD+ConstrGold3.183.803.604.452.49$3.693.314.25Table 7: Results spoken text corpus; average grammaticality score (Grammar)average importance score (Importance) human judgments; : +Constr modelsignificantly different model without constraints; : significantly differentgold standard; $ : significantly different McD+Constr.constraints ( < 0.01). contrast, McDonalds system sees numerical improvementadditional constraints, difference statistically significant.tendencies observed spoken written corpus.Upon closer inspection, see constraints influence considerablygrammaticality unsupervised semi-supervised systems. Tukey tests revealLM+Constr Sig+Constr grammatical McD+Constr. terms importance,Sig+Constr McD+Constr significantly better LM+Constr ( < 0.01).surprising given LM+Constr simple model without mechanismhighlighting important words sentence. Interestingly, Sig+Constr performs wellMcD+Constr retaining important words, despite fact requiresminimal supervision. Although constraint-based models overall perform better models without constraints, receive lower ratings (for grammaticality importance)comparison gold standard. differences significant cases.424fiGlobal Inference Sentence Compressionsummary, observe constraints boost performance. pronounced compression models either unsupervised use small amountsparallel data. example, simple model like Sig yields performance comparableMcDonald (2006) constraints taken account. encouraging resultsuggesting ILP used create good compression models relatively littleeffort (i.e., without extensive feature engineering elaborate knowledge sources). Performance gains also obtained competitive models like McDonalds fullysupervised. gains smaller, presumably initial model containsrich feature representation consisting syntactic information generally good jobproducing grammatical output. Finally, improvements consistent across corporaevaluation paradigms.6. Conclusionspaper presented novel method automatic sentence compression. keyaspect approach use integer linear programming inferring globally optimalcompressions presence linguistically motivated constraints. shownprevious formulations sentence compression recast ILPs extendedmodels local global constraints ensuring compressed output structurallysemantic well-formed. Contrary previous work employed ILP solelydecoding, models integrate learning inference unified framework.experiments demonstrated advantages approach. Constraint-basedmodels consistently bring performance gains models without constraints. improvements impressive models require little supervision. casepoint significance model discussed above. no-constraints incarnationmodel performs poorly considerably worse McDonalds (2006) state-of-the-artmodel. addition constraints improves output model performance indistinguishable McDonald. Note significance model requiressmall amount training data (50 parallel sentences), whereas McDonald trained hundreds sentences. also presupposes little feature engineering, whereas McDonald utilizesthousands features. effort associated framing constraints, howevercreated applied across models corpora. also observed smallperformance gains McDonalds system latter supplemented constraints.Larger improvements possible sophisticated constraints, however intentdevise set general constraints tuned mistakes specificsystem particular.Future improvements many varied. obvious extension concerns constraint set. Currently constraints mostly syntactic consider sentenceisolation. incorporating discourse constraints could highlight words important document-level. Presumably words topical document retainedcompression. constraints could manipulate compression rate. example,could encourage higher compression rate longer sentences. Another interestingdirection includes development better objective functions compression task.objective functions presented far rely first second-order Markov assumptions.Alternative objectives could take account structural similarity source425fiClarke & Lapatasentence target compression; whether share content couldoperationalized terms entropy.Beyond task systems presented paper, believe approach holdspromise generation applications using decoding algorithms searching spacepossible outcomes. Examples include sentence-level paraphrasing, headline generation,summarization.Acknowledgmentsgrateful annotators Vasilis Karaiskos, Beata Kouchnir, Sarah Luger.Thanks Jean Carletta, Frank Keller, Steve Renals, Sebastian Riedel helpfulcomments suggestions anonymous referees whose feedback helped substantially improve present paper. Lapata acknowledges support EPSRC (grantGR/T04540/01). preliminary version work published proceedingsACL 2006.ReferencesAho, A. V., & Ullman, J. D. (1969). Syntax directed translations pushdown assembler. Journal Computer System Sciences, 3, 3756.Bangalore, S., Rambow, O., & Whittaker, S. (2000). Evaluation metrics generation.Proceedings first International Conference Natural Language Generation,pp. 18, Mitzpe Ramon, Israel.Barzilay, R., & Lapata, M. (2006). Aggregation via set partitioning natural languagegeneration. Proceedings Human Language Technology ConferenceNorth American Chapter Association Computational Linguistics, pp. 359366, New York, NY, USA.Bramsen, P., Deshpande, P., Lee, Y. K., & Barzilay, R. (2006). Inducing temporal graphs.Proceedings 2006 Conference Empirical Methods Natural LanguageProcessing, pp. 189198, Sydney, Australia.Briscoe, E. J., & Carroll, J. (2002). Robust accurate statistical annotation general text.Proceedings Third International Conference Language Resources Evaluation, pp. 14991504, Las Palmas, Gran Canaria.Charniak, E. (2000). maximum-entropy-inspired parser. Proceedings 1st NorthAmerican Annual Meeting Association Computational Linguistics, pp. 132139, Seattle, WA, USA.Clarke, J., & Lapata, M. (2006). Models sentence compression: comparison acrossdomains, training requirements evaluation measures. Proceedings 21stInternational Conference Computational Linguistics 44th Annual MeetingAssociation Computational Linguistics, pp. 377384, Sydney, Australia.Clarkson, P., & Rosenfeld, R. (1997). Statistical language modeling using CMUCambridge toolkit. Proceedings Eurospeech97, pp. 27072710, Rhodes, Greece.426fiGlobal Inference Sentence CompressionCormen, T. H., Leiserson, C. E., & Rivest, R. L. (1992). Intoduction Algorithms.MIT Press.Corston-Oliver, S. (2001). Text Compaction Display Small Screens. Proceedings Workshop Automatic Summarization 2nd Meeting NorthAmerican Chapter Association Computational Linguistics, pp. 8998, Pittsburgh, PA, USA.Crammer, K., & Singer, Y. (2003). Ultraconservative online algorithms multiclass problems. Journal Machine Learning Research, 3, 951991.Dantzig, G. B. (1963). Linear Programming Extensions. Princeton University Press,Princeton, NJ, USA.Denis, P., & Baldridge, J. (2007). Joint determination anaphoricity coreferenceresolution using integer programming. Human Language Technologies 2007:Conference North American Chapter Association Computational Linguistics; Proceedings Main Conference, pp. 236243, Rochester, NY.Dras, M. (1999). Tree Adjoining Grammar Reluctant Paraphrasing Text. Ph.D.thesis, Macquarie University.Galley, M., & McKeown, K. (2007). Lexicalized markov grammars sentence compression.Proceedings North American Chapter Association ComputationalLinguistics, pp. 180187, Rochester, NY, USA.Gomory, R. E. (1960). Solving linear programming problems integers. Bellman,R., & Hall, M. (Eds.), Combinatorial analysis, Proceedings Symposia AppliedMathematics, Vol. 10, Providence, RI, USA.Grefenstette, G. (1998). Producing Intelligent Telegraphic Text Reduction ProvideAudio Scanning Service Blind. Hovy, E., & Radev, D. R. (Eds.), ProceedingsAAAI Symposium Intelligent Text Summarization, pp. 111117, Stanford,CA, USA.Hori, C., & Furui, S. (2004). Speech summarization: approach word extractionmethod evaluation. IEICE Transactions Information Systems, E87D (1), 1525.Jing, H. (2000). Sentence reduction automatic text summarization. Proceedings6th Applied Natural Language Processing Conference, pp. 310315, Seattle,WA,USA.Knight, K., & Marcu, D. (2002). Summarization beyond sentence extraction: probabilisticapproach sentence compression. Artificial Intelligence, 139 (1), 91107.Land, A. H., & Doig, A. G. (1960). automatic method solving discrete programmingproblems. Econometrica, 28, 497520.Lin, C.-Y. (2003). Improving summarization performance sentence compression pilotstudy. Proceedings 6th International Workshop Information RetrievalAsian Languages, pp. 18, Sapporo, Japan.Lin, D. (2001). LaTaT: Language text analysis tools. Proceedings first HumanLanguage Technology Conference, pp. 222227, San Francisco, CA, USA.427fiClarke & LapataMarciniak, T., & Strube, M. (2005). Beyond pipeline: Discrete optimization NLP.Proceedings Ninth Conference Computational Natural Language Learning,pp. 136143, Ann Arbor, MI, USA.McDonald, R. (2006). Discriminative sentence compression soft syntactic constraints.Proceedings 11th Conference European Chapter AssociationComputational Linguistics, Trento, Italy.McDonald, R., Crammer, K., & Pereira, F. (2005a). Flexible text segmentation structured multilabel classification. Proceedings Human Language Technology Conference Conference Empirical Methods Natural Language Processing, pp.987994, Vancouver, BC, Canada.McDonald, R., Crammer, K., & Pereira, F. (2005b). Online large-margin training dependency parsers. 43rd Annual Meeting Association ComputationalLinguistics, pp. 9198, Ann Arbor, MI, USA.Nemhauser, G. L., & Wolsey, L. A. (1988). Integer Combinatorial Optimization. WileyInterscience series discrete mathematicals opitmization. Wiley, New York, NY,USA.Nguyen, M. L., Shimazu, A., Horiguchi, S., Ho, T. B., & Fukushi, M. (2004). Probabilisticsentence reduction using support vector machines. Proceedings 20th international conference Computational Linguistics, pp. 743749, Geneva, Switzerland.Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1992). NumericalRecipes C: Art Scientific Computing. Cambridge University Press, NewYork, NY, USA.Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). Semantic role labeling via integer linear programming inference. Proceedings International ConferenceComputational Linguistics, pp. 13461352, Geneva, Switzerland.Riedel, S., & Clarke, J. (2006). Incremental integer linear programming non-projectivedependency parsing. Proceedings 2006 Conference Empirical MethodsNatural Language Processing, pp. 129137, Sydney, Australia.Riezler, S., King, T. H., Crouch, R., & Zaenen, A. (2003). Statistical sentence condensationusing ambiguity packing stochastic disambiguation methods lexical-functionalgrammar. Human Language Technology Conference 3rd MeetingNorth American Chapter Association Computational Linguistics, pp. 118125, Edmonton, Canada.Roark, B. (2001). Probabilistic top-down parsing language modeling. ComputationalLinguistics, 27 (2), 249276.Roth, D. (1998). Learning resolve natural language ambiguities: unified approach.Proceedings 15th American Association Artificial Intelligence, pp.806813, Madison, WI, USA.Roth, D., & Yih, W. (2004). linear programming formulation global inferencenatural language tasks. Proceedings Annual Conference ComputationalNatural Language Learning, pp. 18, Boston, MA, USA.428fiGlobal Inference Sentence CompressionRoth, D., & Yih, W. (2005). Integer linear programming inference conditional randomfields. Proceedings International Conference Machine Learning, pp. 737744, Bonn.Sarawagi, S., & Cohen, W. W. (2004). Semi-markov conditional random fields information extraction. Advances Neural Information Processing Systems, Vancouver,BC, Canada.Shieber, S., & Schabes, Y. (1990). Synchronous tree-adjoining grammars. Proceedings 13th International Conference Computational Linguistics, pp. 253258,Helsinki, Finland.Turner, J., & Charniak, E. (2005). Supervised unsupervised learning sentencecompression. Proceedings 43rd Annual Meeting Association Computational Linguistics, pp. 290297, Ann Arbor, MI, USA.Vandeghinste, V., & Pan, Y. (2004). Sentence compression automated subtitling:hybrid approach. Marie-Francine Moens, S. S. (Ed.), Text Summarization BranchesOut: Proceedings ACL-04 Workshop, pp. 8995, Barcelona, Spain.Williams, H. P. (1999). Model Building Mathematical Programming (4th edition). Wiley.Winston, W. L., & Venkataramanan, M. (2003). Introduction Mathematical Programming: Applications Algorithms (4th edition). Duxbury.Zajic, D., Door, B. J., Lin, J., & Schwartz, R. (2007). Multi-candidate reduction: Sentencecompression tool document summarization tasks. Information ProcessingManagement Special Issue Summarization, 43 (6), 15491570.429fiJournal Artificial Intelligence Research 31 (2008) 259-272Submitted 08/07; published 02/08Expressiveness Levesques Normal FormYongmei Liuymliu@mail.sysu.edu.cnDepartment Computer ScienceSun Yat-sen UniversityGuangzhou 510275, ChinaGerhard Lakemeyergerhard@cs.rwth-aachen.deDepartment Computer ScienceRWTH Aachen52056 Aachen, GermanyAbstractLevesque proposed generalization database called proper knowledge base (KB),equivalent possibly infinite consistent set ground literals. contrastdatabases, proper KBs make closed-world assumption hence entailmentproblem becomes undecidable. Levesque proposed limited efficient inferencemethod V proper KBs, sound and, query certain normalform, also logically complete. conjectured every first-order queryequivalent one normal form. note, show conjecture false. fact,show class formulas V complete must strictly less expressivefull first-order logic. Moreover, propositional case unlikelyformula always polynomial-size normal form.1. Introductionargued Levesque (1998), one deductive technique efficient enoughfeasible knowledge bases (KBs) size seemingly required common-sense reasoning: deduction underlying classical database query evaluation. yet, databasesrestricted serve representational scheme common-sense reasoning, since require, among things, complete knowledge domain. Levesqueproposed generalization database called proper knowledge base, equivalentpossibly infinite consistent set ground literals. illustrate meantproper KB consider following example:Ann likes Bob, Dan likes Fred.Likes(ann, bob)Likes(dan, fred)Ann like Dan.Likes(ann, dan)Carol likes everyone.x. Likes(carol, x)Eve like anyone Ann herself.x. x 6= ann x 6= eve Likes(eve, x)c2008AI Access Foundation. rights reserved.fiLiu & Lakemeyercontrast databases, proper KBs make closed-world assumption.example, Likes(eve, fred) follows KB (if assume unique names),neither Likes(ann, eve) Likes(ann, eve) does. Sadly, even restricted formincompleteness renders entailment problem undecidable, entailment emptyKB reduces validity classical logic.Nevertheless, given KBs like many queries seem easy answer. example,consider formulaLikes(eve, carol) Likes(carol, eve),follows KB simply Likes(carol, eve) does. work Levesquedevised limited efficient inference mechanism V gets examples like rightexpense incomplete others, is, V sometimes answers dont knoweven though query logically entailed.give flavor V works, consider sentence conjunctive normal form(CNF), = c1 c2 . . . cn , ci disjunction ground literals.order see whether follows KB, V simply checks whether ci containsliteral instance one sentences KB. evaluation-basedscheme clearly sound also easily seen incomplete. example, V would returndont know given query Likes(ann, eve) Likes(ann, eve), neither literalcontained KB.paper, Levesque introduced certain normal form (NF) sentences provedV logically complete queries NF. propositional case, examplessentences NF CNF contain tautological clausesotherwise closed resolution. Since every propositional sentence equivalentsentence form, follows immediately every propositional sentenceconverted equivalent one NF.Levesque conjectured every sentence first-order logic alsoequivalent one NF . note, show conjecture Levesque false.fact, show class formulas V complete must strictly lessexpressive full first-order logic. Moreover, propositional case unlikelyformula always polynomial-size normal form.Note Levesques conjecture weaker statement exists algorithm converts every first-order sentence equivalent one NF. latterstatement easily refuted since whether first-order sentence entailed properKB undecidable, whether NF sentence entailed proper KB decidable (Vdecision procedure).next section, briefly review Levesques evaluation-based inference methodproper KBs. Section 3 contains main result, is, show every sentenceequivalent normal form. Section 4 considers size NF formulas propositionalcase.2. Levesques Evaluation-Based Reasoning Procedure Vunderlying language L standard first-order dialect equality. countably infinitely many first-order variables predicate symbols every arity (includingbinary equality predicate). addition countably infinite set C = {d1 , d2 , . . .}260fiOn Expressiveness Levesques Normal Formconstants (but function symbols). logical connectives , , .atomic formulas L predicate symbols variables constants arguments.set formulas L least set contains atomic formulas,set x variable, , x. set.sometimes refer propositional subset language, consistsground atoms L equality closed negation conjunction.Notation: usual, equality written = using infix notation.freely use connectives , , , , understood usual abbreviations.Formulas without free variables called sentences. Variables written x, y, zsub- superscripts. ewffs mean quantifier-free formulas whose predicateequality. example, (x = z 6= d1 ) ewff. use denote universalclosure . example, (x = x 6= z P (x, y, z)) stands xyz.x = x 6= zP (x, y, z). write xd denote free occurrences x replaced constant d.clause disjunction literals, identify set literals contained it.let meta-variable c range clauses write c denote set {l | l c},Vl complement literal l. finite set formulas, writedenote conjunction elements (and true, empty). use H()denote set constants appearing set formulas H + () denoteset constants appearing plus extra one occurring . set returnedH + () made unique assuming constants ordered letting newconstant least constant appearing . singleton {} simplywrite H() H + ().Levesque considered special class Tarskian interpretations called standard interpretations, equality interpreted identity set constants isomorphicdomain discourse. shown following definition theorem, restrictionstandard interpretations captured precisely set axioms, provided limitlogically implied finite sets sentences.Definition 1 Let set E consist following axioms:1. x.x = x;2. (xi = (P (x1 , . . . , xi , . . . , xn ) P (x1 , . . . , y, . . . , xn )));3. {(di 6= dj ) | 6= j}.(1) (2) version axioms equality. P ranges predicate symbolsincluding equality. (3) asserts unique names assumption constants.Theorem 2 (Levesque) Let finite set sentences. E |= iff everystandard interpretation also model .write |=E E |= .database viewed maximally consistent set ground literals. Levesqueproposed generalization database called proper KB, equivalent (possiblyinfinite) consistent set ground literals.following use e range ewffs range atoms (excludingequality) whose arguments distinct variables.261fiLiu & LakemeyerDefinition 3 (Levesque) set sentences proper E consistent, finiteevery sentence form (e ) (e ).propositional case, definition simplifies finite consistent setground literals excluding equality. (As equality plays role case, E ignored.)general, proper KB represents set ground literals lits() = {l | (el) E |= e}, consistent, since E consistent. special case,database represented proper KB: relation R = {d~1 , . . . , d~m } represented(e R(~x)) (e R(~x)), e ~x = d~1 . . .~x = d~m . importantly,proper KB represent incomplete set literals, specifying positive instancesnegative instances leaving status rest open.example introduction formulated proper KB. rephrasingneeded predicates right hand sides implications may mentiondistinct variables.Ann likes Bob, Dan likes Fred.xy. x = ann = bob x = dan = fred Likes(x, y)Ann like Dan.xy. x = ann = dan Likes(x, y)Carol likes everyone.xy. x = carol Likes(x, y)Eve like anyone Ann herself.xy. x = eve 6= ann 6= eve Likes(x, y)Again, note information cannot expressed traditional database where,example, cannot leave open whether Ann likes Eve.Levesques evaluation-based inference procedure V proper KBs defined follows.use range substitutions variables constants, write meanresult applying formula . restrict attention Boolean queries,is, sentences L. Given proper KB sentence L, V returns one threevalues 0 (known false), 1 (known true), 12 (unknown). precisely,1. V [, ] =1012(e ) V [, e] = 1(e ) V [, e] = 1otherwise2. V [, = ] = 1 identical , 0 otherwise;3. V [, ] = 1 V [, ];4. V [, ] = min{V [, ], V [, ]};5. V [, x] = mindH + ({}) V [, xd ].262fiOn Expressiveness Levesques Normal Formhard show procedure logically sound, is, whenever returns1 0, either query negation follows knowledge base. V also obviouslydecidable quantification handled finite number variable substitutions. shownLiu Levesque (2003), also efficient sense database retrieval.see V incomplete, let set sentences example KB= (Likes(ann, eve) Likes(ann, eve)). obviously follows , yet V [, ] = 12V returns 12 Likes(ann, eve) Likes(ann, eve). problem is, roughly,V requires one disjuncts derivable order whole disjunctionderivable.slightly complex example, let = (p q)(q r), p = Likes(ann, bob),q = Likes(ann, eve), r = Likes(ann, dan). again, V [, ] = 12 , correctanswer 0 since |= |= p |= r. However, noticeclauses (p q) (q r) entail clause (p r). conjoin (p r) ,logical equivalence would preserved V would return correct answer 0 sinceV [, p] = 1 V [, r] = 0.Despite limitation, Levesque showed queries certain normal form calledNF, V actually complete. first state result, followed definition NF .Theorem 4 (Levesque) Let proper.every NF, V [, ] = 1 iff |=E ; V [, ] = 0 iff |=E .definition NF based logical separability:Definition 5 (Levesque) set sentences logically separable iff every consistentset ground literals L, L standard interpretation, L {} inconsistent.intuition behind logical separability consistent set literals entails disjunction, one disjuncts must entailed. see this, consider propositionalcase = {p, q} L consistent set propositional literals. Suppose Lstandard interpretation, propositional case L inconsistentor, equivalently, L |= (p q). L must contain either p q. Hence either L {p}L {q} inconsistent, is, either L |= p L |= q. cases, proveslogically separable.set {p, p}, hand, logically separable. {p, p}already inconsistent let L empty set. case, L {p}L {p} consistent.Definition 6 (Levesque) NF least set1. ground atom ewff, NF;2. NF, NF ;3. NF, logically separable, finite,VNF ;4. NF, logically separable, , = {xd | C}, x NF.263fiLiu & Lakemeyer(1) (2) say, roughly, NF contains ground atoms closed negation.(3) (4) say closure conjunction universal generalization restrictedformulas logically separable.idea behind definition formula NF , sense, containlogical puzzles. case, example, non-tautologous clause. seewhy, consider consistent set literals. Similar earlier example {p, q},shown set logically separable. Hence conjunction literalsnegation, clause, NF. hand, tautology like (p p)NF. shown above, set {p, p} logically separable. Hence neither p pnegation, is, (p p) NF.Levesque (1998) showed propositional case, CNF formula NFclauses non-tautologous closed resolution, is, resolvent twoclauses also belongs clauses. Consider = (p q) (q r)earlier example. NF since clauses closed resolution. However,(p q) (q r) (p r) NF.first-order case, two examples formulas NF x(P (x) Q(x))x(P (x) Q(x)). see let constant. {p, q} logically separable,{P (d), Q(d)}. Thus (P (d) Q(d)) NF . consider = {(P (d) Q(d)) | C}.Let L consistent set literals. L standard model, L must containP (d) Q(d) d, hence L {(P (d) Q(d))} inconsistent d. Thuslogically separable, x(P (x) Q(x)) NF. Similarly, x(P (x) Q(x))NF. Therefore, x(P (x) Q(x)), is, x(P (x) Q(x)), NF.Note use expression normal form somewhat non-standard. Unliketraditional normal forms like CNF, even clear whether membership NFdecidable. Levesque pointed number classes first-order formulasmembership NF decided syntactically, unlikely general. example,showed x.R(a, x) R(x, b) NF.turns Levesques original definition logical separability (Def. 5) littlestrong rules certain sentences NF definitely would likein. problem definition mixes use standard regularTarskian interpretations. peculiar effect formulas like x(x = P (x)),make proper KBs, NF.1 see why, definitionNF, x(x = P (x)), is, x(x = P (x)), NF, must(d = P (d)) NF every constant d, requires {d = a, P (d)} logicallyseparable. However, let b constant distinct a, {b = a, P (b)} logicallyseparable. reason {b = a, P (b)} standard model built-inunique names assumption, {b = a} {P (b)} consistent classical logic.turns anomaly easy fix using following, slightly weakerdefinition logical separability, talk standard interpretations.Definition 7 set sentences logically separable iff every consistent set groundliterals L, L standard interpretation, L{} standard interpretation.1. anomaly first observed Thomas Eiter (personal communication).264fiOn Expressiveness Levesques Normal Formnew definition, {b = a, P (b)} logically separable, since {b = a}standard model. result, show x(x = P (x)) NF. NoteNF using new definition logical separability strictly bigger original NF,use new version on. main result, sayssentences equivalent sentence NF, trivially extends Levesques originaldefinition.turn that, let us briefly recall NF good point relatedwork. user poses query NF proper KB, need V obtaincorrect (sound complete) answer respect logical entailment. Moreover,mentioned earlier, V proven efficient database retrieval (Liu & Levesque,2003), standard database technology brought bear implementation.regard, also interesting connection recent work evaluating certain queries description logics (Baader, Calvanese, McGuiness, Nardi, & Patel-Schneider,2003). description-logic KB consists two parts, TBOX terminological definitionslike mother female person least one child ABOX, setatomic formulas. ABOX special case proper KB. importantly,ABOX make closed world assumption, proper KBs. Calvanese, deGiacomo, Lembo, Lenzerini, Rosati (2006) showed conjunctive queries,consist conjunctions atoms existentially quantified variables, query answeringreduced database retrieval well. interesting note queries consider NF. consider small fragment NF, go beyond properKBs also perform terminological reasoning (using TBOX). remarkextensions proper KBs explicitly allowing disjunctions (Lakemeyer& Levesque, 2002; Liu, Lakemeyer, & Levesque, 2004), reasoning goes beyonddatabase retrieval.3. First-Order NF ExpressiveLevesque showed propositional case, every formula transformedequivalent one NF. transformation this. Convert formula CNF, runresolution repeatedly set clauses, deleting tautologous subsumed onesnew clauses generated. resulting set clauses set prime implicates. conjunction clauses NF equivalent . However,transformation cannot extended first-order case. see why, consider =xyz[R(x, y) R(y, z) R(x, z)], says R transitive. run first-orderversion Levesques transformation , would end infinite set clauses,format R(x1 , x2 ) . . . R(xn , xn+1 ) R(x1 , xn+1 ), n 2.section, prove first-order case, every formula equivalentone NF . purpose, need first-order version prime implicates.propositional case, prime implicates defined follows:Definition 8 Let theory. implicate non-tautologous clause c|= c. prime implicate implicate c proper subset c c|= c .265fiLiu & LakemeyerSince consider standard interpretations, easily generalize prime implicates first-order case:Definition 9 Let L. implicate non-tautologous ground clause c|=E c. prime implicate implicate c proper subset c c|=E c . use PI() denote set prime implicates .|=E , is, E |= , E {} E {} entailsentences and, particular, PI() = PI().following basic property prime implicates:Proposition 10 Let c non-tautologous ground clause. |=E c iff existsc PI() c c.note, length formula, mean number predicate symbols,variables, constants logical connectives contained formula. lengthclause, mean length corresponding disjunctive formula. key propertynote defined follows:Definition 11 say PI() bounded exists number n lengthevery member PI() n. PI() bounded, use B() denotemaximum length member PI().show every NF, PI() bounded.equivalent NF, PI() bounded too. However, exist formulas PI()bounded. Thus every formula transformed equivalent one NF.proof prime implicates formulas NF bounded proceedinduction. following lemma useful establish induction conjunctionsnegations.Lemma 12 Let = {1 , . . . , n }.1.V2. PI(NF, PI( )VV) {ciPI(i ).| ci PI(i ), = 1, . . . , n}.Proof:1. Let c PI( ). |=E c. Theorem 2, c standard interpretation.VSince NF, logically separable. Since c non-tautologous, c consistent.Thus {i } c standard interpretation i, |=E c. Let c cVV|=E c . |=E c . Since c PI( ), c = c. Thus c PI(i ).VV2. Let c PI( ). |=E c. Thus |=E c i. Proposition 10,i, exists ci PI(i ) (and hence |=E ci ) ci c.VV|=E ci ci c. Since c PI( ), c = ci .2VV266fiOn Expressiveness Levesques Normal FormNote that, prime implicates bounded, follows easily lemmaVVprime implicates bounded well. obtain similar resultquantified formulas complicated. obvious generalization Lemma 12,Vreplacing {xd1 , xd2 , . . .} x work, would leadinfinite union sets first part infinite union clauses second part.get around observing similarity PI(xb ) PI(xd ), bconstants appearing , shown Proposition 14 below.begin property useful proof. Let mapping CC. use denote every constant replaced . use denote{ | }.Proposition 13 (Levesque) Let bijection C C. |=E , |=E .Proposition 14 Let formula single free variable x. Let b, constants Cappearing . Let bijection swaps b leaves constantsunchanged. PI(xd ) = {c | c PI(xb )}.Proof: Let c PI(xb ). xb |=E c. Proposition 13, (xb ) |=E c , is, xd |=E c .Let c c xd |=E c . xb |=E c . Since c PI(xb ) c c, c = c,hence c = c . Thus c PI(xd ). Similarly, c PI(xd ), c PI(xb ). Therefore,PI(xd ) = {c | c PI(xb )}.2Basically, proposition says prime implicates xb xdmodulo constant renaming.Lemma 15 Let formula single free variable x.1. x NF , constants C, PI(xd ) bounded, PI(x) alsobounded.2. constants C, PI(xd ) bounded, PI(x) also bounded.Proof:1. Since C, PI(xd ) bounded, let n = max{B(xd ) | H + ()}.Proposition 14, C, PI(xd ) relabeling PI(xb ) b H + ().Thus C, PI(xd ) bounded n. show PI(x) also boundedn, showing every element PI(x) also element PI(xd ),C.suppose c PI(x). x |=E c. Thus {x} c standardinterpretation. {xd | C} c standard interpretation. Since x NF,{xd | C} logically separable. Thus exists C {xd } cstandard interpretation. xd |=E c. Let c c xd |=E c . x |=E c .Since c PI(x), c = c. Thus c PI(xd ).267fiLiu & Lakemeyer2. Part 1, let n = max{B(xd ) | H + ()}. C, PI(xd )bounded n. show PI(x) bounded (m + n + 1) n,length . done showing c PI(x), existsset (m + n + 1) constants D, existscd PI(xd ) c = dD cd .suppose c PI(x). x |=E c, i.e., x |=E c. Letarbitrary constant C. xd |=E c xd |=E x. Proposition 10,exists cd PI(xd ) cd c. let b constant appearsneither c cb PI(xb ) cb c. b appear cb ,length cb n. Let = H() H(cb ) {b}.(m + n + 1) elements. show c = dD cd . so, letarbitrary constant D. Let bijection swaps bleaves constants unchanged. Since cb PI(xb ) xb |=E cb ,and, Proposition 13, (xb ) |=E (cb ) . Since neither b appears cb ,(xb ) = xa (cb ) = cb . Hence xa |=E cb . D,xd |=E cd ; 6 D, xa |=E cb . Thus x |=E dD cd , subset2c. Since c PI(x), c = dD cd .pieces hand prove main theorem:Theorem 16 Let NF . PI() bounded.Proof: technical reasons, easier prove slightly general statement, namelyPI() PI() bounded provided NF. proof induction.1. ground atom ewff. ground atom, PI() = {} PI() ={}, hence bounded. ground ewff true,entail ground clause, hence PI() empty set; ground ewfffalse, entails empty clause, hence PI() set consistingempty clause. Therefore, ground ewff, PI() PI() bounded.2. . induction, PI() PI() bounded. Since PI() = PI(),PI() PI() bounded.3. . induction, , PI() PI() bounded. Lemma 12,VVPI( ) bounded max{B() | }, PI( ) sum B().V4. x. induction, constant d, PI(xd ) PI(xd ) bounded.Lemma 15, PI(x) PI(x) bounded.2easy corollary, have:Corollary 17 every sentence equivalent one NF.268fiOn Expressiveness Levesques Normal FormProof: Let = xyz[R(x, y) R(y, z) R(x, z)], says R transitive.n, following PI():R(d1 , d2 ) . . . R(dn , dn+1 ) R(d1 , dn+1 ).Thus PI() bounded. Suppose exists NFequivalent. PI() = PI( ). Theorem 16, PI( ) bounded, contradiction.2Moreover, easy generalize inexpressiveness result:Theorem 18 exist class F sentences properties:1. every formula equivalent one F;2. V logically complete F (i.e., every proper KB every F, |=EV [, ] = 1, |=E V [, ] = 0);3. F F;constants d.VF, F; x F, xd FNote theorem require logical separability, V completeF. call set formulas satisfies third requirement downward saturated,which, besides desirable property normal form, needed technical reasons.Proof: Suppose, contrary, exists class F sentences satisfiesthree properties stated above. case NF , show PI() boundedevery F use sentence proof Corollary 17 obtaincontradiction.boundedness proof almost identical argument NF ,repeat here. Instead note necessary changes. fact,changes needed proofs Item 1 Lemma 12 15, appeal logicalseparability show |=E c respectively xd |=E c d. showconclusions drawn using assumption V complete F.First, note following: Let F, let c non-tautologous ground clause.c essentially proper KB. Thus |=E c, c |=E , hence V [c, ] = 0,completeness V F.Change Lemma 12, Item 1:VVVVLet c PI( ). |=E c. Since F, V [c, ] = 0. definitionV , V [c, ] = 0 . soundness V , c |=E . Thus |=E c.Change Lemma 15, Item 1:Let c PI(x). x |=E c. Since x F, V [c, x] = 0. definitionV , V [c, xd ] = 0 constant d. soundness V , c |=E xd . Thus xd |=E c.small changes proofs two lemmas go F instead NF.Finally, proof Theorem 16 carries without change, since induction worksdownward-saturated set.2269fiLiu & Lakemeyer4. Propositional NF Succinctpropositional case, Levesques transformation NF, is, taking conjunctionprime implicates formula, may cause exponential blowup sizeformula. number prime implicates formula n propositionsexponential n worst case (Chandra & Markowsky, 1978). section, showpropositional case, certain complexity assumption, every formulapolynomial-size equivalent one NF. done relating NF existingresult knowledge compilation.Knowledge compilation (Selman & Kautz, 1996; Darwiche & Marquis, 2002)proposed one main techniques deal computational intractabilitygeneral propositional reasoning. approach, tractable language, usuallymeans language whether clause entailed formula languagedecided polynomial time, identified target compilation language.propositional theory first compiled off-line target language, resultused on-line answer multiple queries. main motivation shiftcomputational cost off-line phase, amortized on-line queries.shown following, NF serve knowledge compilation language. reasonpropositional case, answering arbitrary query proper KBequivalent answering clausal query arbitrary KB. mentioned Section 2,propositional case, proper KB simply consistent set literals.Proposition 19 Clausal entailment NF decided polynomial time.Proof: Let NF , let c non-tautologous clause. c proper KB. Thus|= c iff c |= iff V [c, ] = 0, soundness completeness V NF. Clearly,propositional case, V runs polynomial time.2following well-known result knowledge compilation:Theorem 20 (Selman & Kautz, 1996) Unless NP P/poly, exist classF formulas every propositional formula polynomial-size equivalent oneF, clausal entailment F decided polynomial time.complexity class P/poly, also known non-uniform P , originated circuit complexity (Boppana & Sipser, 1990). Roughly, problem P/poly every integer nexists circuit size polynomial n solves instances size n. Without goingdetails, NP P/poly implies collapse polynomial hierarchysecond level, considered unlikely.easy corollary proposition theorem, have:Corollary 21 Unless NP P/poly, every propositional formula polynomial-sizeequivalent one NF.words, unlikely obtain compact NF representationsarbitrary propositional formulas.270fiOn Expressiveness Levesques Normal Form5. ConclusionLevesque remarked paper envision use NF sense queryoptimization taking arbitrary query converting NF handingV . main argument high computational cost, usually cannot affordedon-line. Besides, except special cases even clear convert formulaNF, one exists. Instead suggested NF could guideline users formulategood queries evaluated efficiently.contribution technical note point limits evenuse NF . propositional case result says likely queriescannot NF compactly representable. sense,bad news, since practice queries tend small compared knowledge base.first-order case result serious showed queriesnormal form all. words, matter ingenious user might be,always queries easy-to-answer form, least insistform independent knowledge base NF. Indeed, may still possiblefind another notion normal form depends way knowledge base,example, constants contains. future work.Acknowledgmentsthank Hector Levesque many helpful discussions topic paperreading earlier version paper. also thank anonymous reviewersdetailed comments improving presentation paper.ReferencesBaader, F., Calvanese, D., McGuiness, D., Nardi, D., & Patel-Schneider, P. (2003).Description Logic Handbook: Theory, Implementation Applications. CambridgeUniversity Press.Boppana, R. B., & Sipser, M. (1990). complexity finite functions. van Leeuwen,J. (Ed.), Handbook Theoretical Computer Science, Vol. A, pp. 757804. Elsevier.Calvanese, D., de Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2006). Datacomplexity query answering description logics. Proc. Tenth InternationalConference Principles Knowledge Representation Reasoning (KR-06), pp.260270.Chandra, A. K., & Markowsky, G. (1978). number prime implicants. DiscreteMathematics, 24, 711.Darwiche, A., & Marquis, P. (2002). knowledge compilation map. Journal ArtificialIntelligence Research, 17, 229264.Lakemeyer, G., & Levesque, H. J. (2002). Evaluation-based reasoning disjunctive information first-order knowledge bases. Proc. Eighth International ConferencePrinciples Knowledge Representation Reasoning (KR-02), pp. 7381.271fiLiu & LakemeyerLevesque, H. J. (1998). completeness result reasoning incomplete first-orderknowledge bases. Proc. Sixth International Conference PrinciplesKnowledge Representation Reasoning (KR-98), pp. 1423.Liu, Y., Lakemeyer, G., & Levesque, H. J. (2004). logic limited belief reasoningdisjunctive information. Proc. Ninth International Conference PrinciplesKnowledge Representation Reasoning (KR-04), pp. 587597.Liu, Y., & Levesque, H. J. (2003). tractability result reasoning incomplete firstorder knowledge bases. Proc. Eighteenth International Joint ConferenceArtificial Intelligence (IJCAI-03), pp. 8388.Selman, B., & Kautz, H. (1996). Knowledge compilation theory approximation. JournalACM, 43 (2), 193224.272fiJournal Artificial Intelligence Research 31 (2008) 113-155Submitted 08/07; published 01/08CTL Model Update System ModificationsYan Zhangyan@scm.uws.edu.auIntelligent Systems LaboratorySchool Computing MathematicsUniversity Western Sydney, AustraliaYulin Dingyulin@cs.adelaide.edu.auDepartment Computer ScienceUniversity Adelaide, AustraliaAbstractModel checking promising technology, applied verificationmany hardware software systems. paper, introduce concept model update towards development automatic system modification tool extends modelchecking functions. define primitive update operations models ComputationTree Logic (CTL) formalize principle minimal change CTL model update.primitive update operations, together underlying minimal change principle, serve foundation CTL model update. Essential semantic computationalcharacterizations provided CTL model update approach. describeformal algorithm implements approach. also illustrate two case studies CTLmodel updates well-known microwave oven example Andrew File System 1,propose method optimize update results complex systemmodifications.1. IntroductionModel checking one effective technologies automatic system verifications.model checking approach, system behaviours modeled Kripke structure,specification properties require system meet expressed formulaspropositional temporal logic, e.g., CTL. model checker, e.g., SMV, takesKripke model formula input, verifies whether formula satisfiedKripke model. formula satisfied Kripke model, system reporterrors, possibly provides useful information (e.g., counterexamples).past decade, model checking technology considerably developed,many effective model checking tools demonstrated provision thorough automatic error diagnosis complex designs e.g., (Amla, Du, Kuehlmann, Kurshan,& McMillan, 2005; Berard, Bidoit, Finkel, Laroussinie, Petit, Petrucci, & Schnoebelen,2001; Boyer & Sighireanu, 2003; Chauhan, Clarke, Kukula, Sapra, Veith, & Wang, 2002;Wing & Vaziri-Farahani, 1995). current state-of-the-art model checkers,SMV (Clarke, Grumberg, & Peled, 1999), NuSMV (Cimatti, Clarke, Giunchiglia, & Roveri,1999) Cadence SMV (McMillan & Amla, 2002), employ SMV specification languageComputational Tree Logic (CTL) Linear Temporal Logic (LTL) variants (Clarkeet al., 1999; Huth & Ryan, 2004). model checkers, SPIN (Holzmann, 2003),use Promela specification language on-the-fly LTL model checking. Additionally,c2008AI Access Foundation. rights reserved.fiZhang & DingMCK (Gammie & van der Meyden, 2004) model checker developed integratingknowledge operator CTL model checking verify knowledge-related propertiessecurity protocols.Although model checking approaches used verification problems largecomplex systems, one major limitation approaches verifycorrectness system specification. words, errors identified systemspecification model checking, task correcting system completely leftsystem designers. is, model checking generally used verify correctnesssystem, modify it. Although idea repair indeed proposed modelbased diagnosis, repairing system possible specific cases (Dennis, Monroy, &Nogueira, 2006; Stumptner & Wotawa, 1996).1.1 MotivationSince model checking handle complex system verification problems mayimplemented via fast algorithms, quite natural consider whether developassociated algorithms handle system modification well. ideaintegrating model checking automatic modification investigated recentyears. Buccafurri, Eiter, Gottlob, Leone (1999) proposed approach wherebyAI techniques combined model checking enhanced algorithmidentify errors concurrent system, also provide possible modificationssystem.approach, system described Kripke structure , modificationset state transitions may added removed . CTLformula satisfied i.e., system contains errors respect property ,repaired adding new state transitions removing existing ones specified. result, new Kripke structure 0 satisfy formula . approachBuccafurri et al. (1999) integrates model checking abductive theory revisionperform system repairs. also demonstrate approach appliedrepair concurrent programs.observed type system repair quite restricted, relationelements (i.e., state transitions) Kripke model changed 1 . implies errorsfixed changing system behaviors. fact, show paper,allowing change states relation elements Kripke structure significantlyenhances system repair process situations. Also, since providing admissiblemodifications (i.e., set ) pre-condition repair, approach Buccafurriet al. lacks flexibility. Indeed, stated authors themselves, approach maygeneral enough system modifications.hand, knowledge-base update subject extensive studyAI community since late 1980s. Winsletts Possible Model Approach (PMA)viewed pioneering work towards model-based minimal change approach knowledgebase update (Winslett, 1988). Many researchers since proposed different approachesknowledge system update (e.g., see references Eiter & Gottlob, 1992; Herzig &1. NB: state changes occur specified system repairs (see Definitions 3.2 3.3 Buccafurriet al., 1999).114fiCTL Model Update System ModificationsRifi, 1999). works, Harris Ryan (2002, 2003) considered using updateapproach system modification, designed update operations tackle featureintegration, performing theory change belief revision. However, study focusedmainly theoretical properties system update, practical implementationapproach system modification remains unclear.Baral Zhang (2005) recently developed formal approach knowledge updatebased single-agent S5 Kripke structures observing system modification closelyrelated knowledge update. knowledge dynamics perspective, viewfinite transition system, represents real time complex system, modelknowledge set (i.e., Kripke model). Thus problem system modification reducedproblem updating model new updated model satisfies knowledgeformula.observation motivated initial development general approach updatingKripke models, integrated model checking technology, towardsgeneral automatic system modification. Ding Zhangs work (2005) may viewedfirst attempt apply idea LTL model update. LTL model update modifiesexisting LTL model abstracted system automatically correct errors occurringwithin model.Based investigation described above, intend integrate knowledge updateCTL model checking develop practical model updater, represents generalmethod automatic system repairs.1.2 Contributions Paperoverall aim work design model updater improves model checkingfunction adding error repair (see schematic Figure 1). outcome updatercorrected Kripke model. model updaters function automatically correcterrors reported (possibly counterexamples) model checking compiler. Eventually,model updater intended universal compiler used certain commonsituations model error detection correction.CTLKripke ModelSystemDesignModel checking& UpdatingCorrectedKripke ModelFigure 1: CTL model update.main contributions paper described follows:115fiZhang & Ding1. propose formal framework CTL model update. Firstly, define primitive CTL model update operations and, based operations, specify minimalchange principle CTL model update. study relationshipproposed CTL model update traditional propositional belief update. Interestingly, prove CTL model update obeys Katsuno Mendelzon updatepostulates (U1) - (U8). provide important characterizations specialCTL model update formulas EX, AG EG. characterizationsplay important role optimization update procedure. Finally, studycomputational properties CTL model update show that, general, modelchecking problem CTL model update co-NP-complete. also classify usefulsubclass CTL model update problems performed polynomial time.2. develop formal algorithm CTL model update. principle, algorithmperform update given CTL Kripke model arbitrary satisfiableCTL formula generate model satisfies input formula minimalchange respect original model. model viewed possiblecorrection original system specification. Based algorithm, implementsystem prototype CTL model updater C code Linux.3. demonstrate important applications CTL model update approach twocase studies well-known microwave oven example (Clarke et al., 1999)Andrew File System 1 (Wing & Vaziri-Farahani, 1995). casestudies, propose new update principle minimal change maximalreachable states, significantly improve update results complex systemmodification scenarios.summary, work presented paper initial step towards formal studyautomatic system modification. approach may integrated existing modelcheckers may develop unified methodology system model checkingmodel correction. sense, work enhance current model checkingtechnology. results presented paper published ECAI 2006 (Ding &Zhang, 2006).rest paper organized follows. overview CTL syntax semantics provided Section 2.1. Primitive update operations CTL models definedSection 3, minimal change principle CTL model update developed.Section 4 consists study relationship CTL model update KatsunoMendelzons update postulates (U1) - (U8), various characterizations special CTL model updates. Section 5, general computational complexity result CTLmodel update proved, useful tractable subclass CTL model update problemsidentified. formal algorithm proposed CTL model update approach describedSection 6. Section 7, two update case studies illustrated demonstrate applications CTL model update approach. Section 8 proposes improved CTL modelupdate approach significantly optimize update results complex systemmodification scenarios. Finally, paper concludes future work discussionsSection 9.116fiCTL Model Update System Modifications2. Preliminariessection, briefly review syntax semantics Computation Tree Logicbasic concepts belief update, foundation CTL model update.2.1 CTL Syntax Semanticsbegin with, briefly review CTL syntax semantics (refer Clarke et al., 1999Huth & Ryan, 2004 details).Definition 1 Let AP set atomic propositions. Kripke model APtriple = (S, R, L) where:1. finite set states;2. R binary relation representing state transitions;3. L : 2AP labeling function assigns state set atomicpropositions.example finite Kripke model represented graph Figure 2,node represents state S, attached set propositional atomsassigned labeling function, edge represents state transition - relationelement R describing system transition one state another.S0p, qS2q, rrS1Figure 2: Transition state graph.Computation Tree Logic (CTL) temporal logic allowing us refer future.also branching-time logic, meaning model time tree-like structurefuture determined consists different paths, one mightactual path eventually realized (Huth & Ryan, 2004).Definition 2 CTL following syntax given Backus-Naur form:::= > || p | () | (1 2 ) | (1 2 ) | | AX | EX| AG | EG | AF | EF | A[1 U2 ] | E[1 U2 ]p propositional atom.117fiZhang & DingCTL formula evaluated Kripke model. path Kripke model statea(n) (infinite) sequence states. Note given path, state may occurinfinite number times path (i.e., path contains loop). simplifyfollowing discussions, may identify states path different position subscripts,although states occurring different positions path may same. way,say one state precedes another path without much confusion.present useful notions formal way. Let = (S, R, L) Kripke model S.path starting denoted = [s 0 , s1 , , si1 , si , si+1 , ], s0 =(si , si+1 ) R holds 0. write si si state occurring path .path = [s0 , s1 , , si , , sj , ] < j, also denote si < sj . Furthermoregiven path , use notion si denote state state < si .simplicity, may use succ(s) denote state 0 relation element (s, s0 ) R.Definition 3 Let = (S, R, L) Kripke model CTL. Given S, definewhether CTL formula holds state s. denote (M, s) |= .satisfaction relation |= defined structural induction CTL formulas:1. (M, s) |= > (M, s) 6|= S.2. (M, s) |= p iff p L(s).3. (M, s) |= iff (M, s) 6|= .4. (M, s) |= 1 2 iff (M, s) |= 1 (M, s) |= 2 .5. (M, s) |= 1 2 iff (M, s) |= 1 (M, s) |= 2 .6. (M, s) |= 1 2 iff (M, s) |= 1 , (M, s) |= 2 .7. (M, s) |= AX iff s1 (s, s1 ) R, (M, s1 ) |= .8. (M, s) |= EX iff s1 (s, s1 ) R, (M, s1 ) |= .9. (M, s) |= AG iff paths = [s0 , s1 , s2 , ] s0 = si , si ,(M, si ) |= .10. (M, s) |= EG iff path = [s 0 , s1 , s2 , ] s0 = si , si ,(M, si ) |= .11. (M, s) |= AF iff paths = [s 0 , s1 , s2 , ] s0 = si , si ,(M, si ) |= .12. (M, s) |= EF iff path = [s 0 , s1 , s2 , ] s0 = si , si ,(M, si ) |= .13. (M, s) |= A[1 U2 ] iff paths = [s0 , s1 , s2 , ] s0 = s, si , (M, si ) |=2 j < i, (M, sj ) |= 1 .14. (M, s) |= E[1 U2 ] iff path = [s0 , s1 , s2 , ] s0 = s, si ,(M, si ) |= 2 j < i, (M, sj ) |= 1 .118fiCTL Model Update System Modificationsdefinition, see intuitive meaning A, E, X, Gquite clear: means paths, E means exists path, X refers nextstate G means states globally. semantics CTL formula easycapture follows.first six clauses, truth value formula state depends truthvalue 1 2 state. example, truth value statedepends truth value state. contrasts clauses 7 8AX EX. instance, truth value AX state determinedtruth value s, truth values states 0 (s, s0 ) R; (s, s) R,value also depends truth value s.next four clauses (9 - 12) also exhibit phenomenon. example, truth valueAG involves looking truth value immediately related states,indirectly related states well. case AG, must examine truth valueevery state related number forward links (paths) current state s.clauses 13 14, symbol U may explained until: path = [s 0 , s1 , s2 , ] satisfies1 U2 state si < si , (M, s) |= 1 (M, si ) |= 2 .Clauses 9 - 14 refer computation paths models. is, therefore, usefulvisualize possible computation paths given state unwinding transitionsystem obtain infinite computation tree. greatly facilitates deciding whetherstate satisfies CTL formula. unwound tree graph Figure 2 depictedFigure 3 (note assume s0 initial state Kripke model).S0p, qS2S1q,rrS2p,qS0rS2rq,rS1rS2rS2Figure 3: Unwinding transition state graph infinite tree.Figure 3, = r, AXr true; = q, EXq true. figure,= r, AFr true states paths satisfy r timefuture. = q, EFq true states paths satisfy q timefuture. clauses AG EG explained Figure 4. tree,states satisfy r. Thus, AGr true Kripke model. one path statessatisfy = q. Thus, EGq true Kripke model.119fiZhang & Dingp,q,rAG = r;EG= q.p,q, rq,rq,rS0S2S1rS2S0rS2rS1rS2rS2Figure 4: AG EG unwound tree.following De Morgan rules equivalences (Huth & Ryan, 2004) usefulCTL model update algorithm implementation:AF EG;EF AG;AX EX;AF A[>U];EF E[>U];A[1 U2 ] (E[2 U(1 2 )] EG2 ).rest paper, without explicit declaration, assume CTLformulas occurring context satisfiable. instance, consider updatingKripke model satisfy CTL formula , already assume satisfiable.Definition 3, see given CTL Kripke model = (S, R, L),(M, s) |= propositional formula, truth value solely dependslabeling function Ls assignment state s. case may simply write L(s) |=confusion context.2.2 Belief UpdateBelief change primary research topic AI community almost two decadese.g., (Gardenfors, 1988; Winslett, 1990). Basically, studies problem agentchange beliefs wants bring new beliefs belief set. twotypes belief changes, namely belief revision belief update. Intuitively, belief revisionused modify belief set order accept new information static world,120fiCTL Model Update System Modificationsbelief update bring belief set date world describedchanges.Katsuno Mendelzon (1991) discovered original AGM revision postulates cannot precisely characterize feature belief update. proposed followingalternative update postulates, argued propositional belief update operatorssatisfy postulates. following (U1) - (U8) postulates, occurrences, , , etc. propositional formulas.|= .|= .satisfiable also satisfiable.T1 T2 1 2 1 T2 2 .(T ) |= ( ).1 |= 2 2 |= 1 1 2 .complete (i.e., unique model)(T 1 ) (T 2 ) |= (1 2 ).(U8) (T1 T2 ) (T1 ) (T2 ).(U1)(U2)(U3)(U4)(U5)(U6)(U7)shown Katsuno Mendelzon (1991), postulates (U1) - (U8) precisely captureminimal change criterion update defined based certain partial orderingmodels. typical model based belief update approach, briefly introduceWinsletts Possible Models Approach (PMA) (Winslett, 1990). consider propositional language L. Let I1 I2 two Herband interpretations L. symmetricdifference I1 I2 defined dif f (I1 , I2 ) = (I1 I2 ) (I2 I1 ).given interpretation I, define partial ordering follows: I1 I2dif f (I, I1 ) dif f (I, I2 ). Let collection interpretations, denote in(I, )set minimal models respect ordering , modelfixed. let two propositional formulas, update usingPMA, denoted pma , defined follows:od( pma ) =od()in(M od(), ),od() denotes set models formula . proved PMAupdate operator pma satisfies postulates (U1) - (U8).work CTL model update close connection idea belief update.shown paper, approach, view CTL Kripke model descriptionworld interested in, i.e., description system dynamic behaviours,update Kripke model occurs setting system dynamicbehaviours change accommodate desired properties. Althoughsignificant difference classical propositional belief update CTL modelupdate, show Katsuno Mendelzons update postulates (U1) - (U8) alsosuitable characterize minimal change principle CTL model update.3. Minimal Change CTL Model Updatewould like extend idea minimal change belief update CTL modelupdate. principle, need update CTL Kripke model satisfy CTL formula,121fiZhang & Dingexpect updated model retain much information possible representedoriginal model. words, prefer change model minimal way achievegoal. section, propose formal metrics minimal change CTL modelupdate.3.1 Primitive Update OperationsGiven CTL Kripke model (satisfiable) CTL formula, consider modelupdated order satisfy given formula. discussion previoussection, try incorporate minimal change principle update approach.first step towards aim, way measure difference twoCTL Kripke models relation given model. first illustrate initial considerationaspect example.Example 1 Consider simple CTL model = ({s 0 , s1 , s2 }, {(s0 , s0 ), (s0 , s1 ), (s0 , s2 ),(s1 , s1 ), (s2 , s2 ), (s2 , s1 )}, L), L(s0 ) = {p, q}, L(s1 ) = {q, r} L(s2 ) = {r}.described Figure 5.p,qs1s0rq,rs2Figure 5: Model .consider formula AGp. Clearly, (M, 0 ) 6|= AGp. One way update satisfyAGp update states s1 s2 updated states satisfy p 2 . Therefore,obtain new CTL model 0 = ({s0 , s1 , s2 }, {(s0 , s0 ), (s0 , s1 ), (s0 , s2 ), (s1 , s1 ), (s2 , s2 ),(s2 , s1 )}, L0 ), L0 (s0 ) = L(s0 ) = {p, q}, L0 (s1 ) = {p, q, r} L0 (s2 ) = {p, r}.update, see labeling function changed associate different truthassignments states s1 s2 . Another way update satisfy formula AGpsimply remove relation elements (s 0 , s1 ) (s0 , s2 ) , gives (M 00 , s0 ) |= AGp,00 = ({s0 , s1 , s2 }, {(s0 , s0 ), (s1 , s1 ), (s2 , s2 ), (s2 , s1 )}, L). closely resemblesapproach Buccafurri et al. (Buccafurri et al., 1999), state changes occur.interesting note first updated models retains structureoriginal, significantly changed second. two possible resultsdescribed Figure 6. 22. Precisely, update labeling function L changes truth assignments s1 s2 .122fiCTL Model Update System Modificationsp,qp,q,rs1p,qs0p,rq,rs2s1s0rs2Figure 6: Two possible results updating AGp.example shows order update CTL model satisfy formula,may apply different kinds operations change model. possible operationsapplicable CTL model, consider five basic ones changes CTL modelachieved.PU1: Adding one relation elementGiven = (S, R, L), updated model 0 = (S 0 , R0 , L0 ) obtained addingone new relation element. is, 0 = S, L0 = L, R0 = R {(si , sj )},(si , sj ) 6 R two states si , sj S.PU2: Removing one relation elementGiven = (S, R, L), updated model 0 = (S 0 , R0 , L0 ) obtained removingone existing relation element. is, 0 = S, L0 = L, R0 = R {(si , sj )},(si , sj ) R two states si , sj S.PU3: Changing labeling function one stateGiven = (S, R, L), updated model 0 = (S 0 , R0 , L0 ) obtained changinglabeling function particular state. is, 0 = S, R0 = R, (S {s }), S,L0 (s) = L(s), L0 (s ) set true variable assigned state L0 (s ) 6= L(s ).PU4: Adding one stateGiven = (S, R, L), updated model 0 = (S 0 , R0 , L0 ) obtained addingone new state. is, 0 = {s }, 6 S, R0 = R, S, L0 (s) = L(s)L0 (s ) set true variables assigned .PU5: Removing one isolated stateGiven = (S, R, L), updated model 0 = (S 0 , R0 , L0 ) obtained removingone isolated state: 0 = {s }, 6= , neither(s, ) (s , s) R, R0 = R, 0 , L0 (s) = L(s).call five operations primitive since express kinds changesCTL model. Figure 7 illustrates examples applying operations model.five operations, PU1, PU2, PU4 PU5 represent basic operations graph. Generally, using four operations, perform changesCTL model. instance, want substitute state CTL model,123fiZhang & Dingfollowing: (1) remove relation elements associated state, (2) remove isolatedstates, (3) add state want replace original one, (4) add relevantrelation elements associated new state.Although four operations sufficient enough represent changes CTLmodel, sometimes complicate measure changes CTL models. Considercase state substitution. Given CTL model , one CTL model 0 exactlygraphical structure except 0 one particular state different, tend think 0 obtained single change statereplacement, instead sequence operations PU1, PU2, PU4 PU5.motivates us operation PU3. PU3 effect state substitution,fundamentally different combination PU1, PU2, PU4 PU5, PU3change state name relation elements original model, assignsdifferent set propositional atoms state original model. sense,combination PU1, PU2, PU4 PU5 cannot replace operation PU3. Using PU3represent state substitution significantly simplifies measure model differenceillustrated Definition 4. rest paper, assume statesubstitutions CTL model achieved PU3 unique waymeasure differences CTL model changes relation states substitutions.also note operation PU3 way substitute state CTLmodel, PU5 becomes unnecessary, actually need remove isolatedstate model. need remove relevant relation element(s) model,state becomes unreachable initial state. Nevertheless, remaindiscussions coherent primitive operations described above, followingdefinition CTL minimal change, still consider measure changes causedapplying PU5 CTL model update.S0S3S0S3S1S1S2M1S2PU2 applied M.S0S3M2S1S2PU2, PU2, PU5, PU4,PU1 PU1 applied M.Figure 7: Illustration primitive updates.3.2 Defining Minimal ChangeFollowing traditional belief update principle, order make CTL model satisfyproperty, would expect given CTL model changed little possible.using primitive update operations, CTL Kripke model may updated different ways:124fiCTL Model Update System Modificationsadding removing state transitions, adding new states, changing labeling functionstate(s) model. Therefore, first need method measurechanges CTL models, develop minimal change criterion CTLmodel update.Given two CTL models = (S, R, L) 0 = (S 0 , R0 , L0 ), operation P U(i = 1, , 5), Diff P U (M, 0 ) denotes differences two models 0updated model , makes clear several operations type P Uoccurred. Since PU1 PU2 change relation elements, define Diff P U 1 (M, 0 ) =R0 R (adding relation elements only) Diff P U 2 (M, 0 ) = R R0 (removing relation elements only). operation PU3, since labeling function changed, difference measure 0 PU3 defined Diff P U 3 (M, 0 ) = {s |0 L(s) 6= L0 (s)}. operations PU4 PU5, hand, defineDiff P U 4 (M, 0 ) = 0 (adding states) Diff P U 5 (M, 0 ) = 0 (removing states).Let = (M, s) M0 = (M 0 , s0 ), convenience, also denote Diff (M, 0 ) =(Diff P U 1 (M, 0 ), Diff P U 2 (M, 0 ), Diff P U 3 (M, 0 ), Diff P U 4 (M, 0 ), Diff P U 5 (M, 0 )).worth mentioning given two CTL Kripke models 0 ,ambiguity compute Diff P U (M, 0 ) (i = 1, , 5), primitive operationcause one type changes (states, relation elements, labeling function) modelsmatter many times applied. precisely define orderingCTL models.Definition 4 (Closeness ordering) Let , 1 M2 three CTL Kripke models.say M1 least close M2 , denoted M1 M2 ,set PU1-PU5 operations transform 2 , exists set PU1-PU5operations transform M1 following conditions hold:(1) (i = 1, , 5), Diff P U (M, M1 ) Diff P U (M, M2 ),(2) Diff P U 3 (M, M1 ) = Diff P U 3 (M, M2 ), Diff P U 3 (M, M1 ),dif f (L(s), L1 (s)) dif f (L(s), L2 (s)).denote M1 <M M2 M1 M2 M2 6M M1 .Definition 4 presents measure difference two models respectgiven model. Intuitively, say model 1 closer relative model M2 ,(1) M1 obtained applying primitive update operations cause fewerchanges applied obtain model 2 ; (2) set states M1 affectedapplying PU3 2 , take closer look differenceset propositional atoms associated relevant states. orderingspecified Definition 4, define CTL model update formally.Definition 5 (Admissible update) Given CTL Kripke model = (S, R, L), =(M, s0 ) s0 S, CTL formula , CTL Kripke model U pdate(M, ) calledadmissible model (or admissible updated model) following conditions hold: (1)U pdate(M, ) = (M 0 , s00 ), (M 0 , s00 ) |= , 0 = (S 0 , R0 , L0 ) s00 0 ; and, (2)exist another updated model 00 = (S 00 , R00 , L00 ) s000 00 (M 00 , s000 ) |=00 <M 0 . use Poss(U pdate(M, )) denote set possible admissiblemodels updating satisfy .125fiZhang & DingExample 2 Figure 8, model updated two different ways. Model 1 resultupdating applying PU1. Model 2 another update resulting applyingPU1, PU2 PU5. Diff P U 1 (M, M1 ) = {(s0 , s2 )}, Diff P U 1 (M, M2 ) ={(s1 , s0 ), (s0 , s2 )}, results Diff P U 1 (M, M1 ) Diff P U 1 (M, M2 ). Also, easysee Diff P U 2 (M, M1 ) = Diff P U 2 (M, M2 ) = {(s3 , s0 ), (s2 , s3 )}, Diff P U 2 (M, M1 )Diff P U 2 (M, M2 ). Similarly, see Diff P U 3 (M, M1 ) = Diff P U 3 (M, M2 ) = ,Diff P U 4 (M, M1 ) = Diff P U 4 (M, M2 ) = . Finally, Diff P U 5 (M, M1 ) =Diff P U 5 (M, M2 ) = {s3 }. According Definition 4, 1 <M M2 . 2s0s3s1s2s0s3s0s1M1s2s1s2M2Figure 8: Illustration minimal change rules.note CTL model update, simply replace initial stateanother existing state model satisfy formula, model actuallychanged, unique admissible model according Definition 5.case, updates ruled Definition 5. example, consider CTLmodel described Figure 9: want update (M, 0 ) AXp, seeS0S1pS2Figure 9: special model update scenario.(M, s1 ) becomes admissible updated model according definition: simplyreplace initial state s0 s1 . Nevertheless, would expect update126fiCTL Model Update System Modificationsmay also equally reasonable. instance, may change labeling functionmake L0 (s1 ) = {p}. updates, changed something , changecaused first update represented minimal change definition.overcome difficulty creating dummy state ] CTL Kripke model ,initial state , add relation element (], s) . way, changeinitial state s0 imply removal relation element (], s) additionnew relation element (], s0 ). changes measured minimal changedefinition. treatment, updated models described admissible.rest paper, without explicit declaration, assume CTL Kripkemodel contains dummy state ] special state transitions ] initial states.4. Semantic Propertiessection, first explore relationship CTL model update traditional belief update, provide useful semantic characterizations typicalCTL model update cases.4.1 Relationship Propositional Belief UpdateFirst show following result ordering defined Definition 4.Proposition 1 partial ordering.Proof: Definition 4, easy see reflexive antisymmetric.show also transitive. Suppose M1 M2 M2 M3 . According Definition 4, Dif fP U (M, M1 ) Dif fP U i(M, M2 ), Dif fP U i(M, M2 )Dif fP U i(M, M3 ) (i = 1, , 5). Consequently, Dif f P U (M, M1 ) Dif fP U i(M, M3 )(i = 1, , 5). Condition 1 Definition 4 holds. consider Condition 2 definition. case need consider Dif f P U 3 (M, M1 ) = Dif fP U 3(M, M2 )Dif fP U 3 (M, M2 ) = Dif fP U 3 (M, M3 ) (note cases directly implyDif fP U 3 (M, M1 ) Dif fP U 3 (M, M3 ) Dif fP U 3 (M, M1 ) 6= Dif fP U 3 (M, M3 )).case, obvious Dif f P U 3 (M, M1 ) = Dif fP U 3(M, M3 ), dif f (L(s), L1 (s))dif f (L(s), L3 (s)). M1 M3 . 2also interesting consider special case CTL model update updateformula classical propositional formula. following proposition indicatespropositional formula considered CTL model update, admissible modelobtained traditional model based belief update approach (Winslett, 1988).Proposition 2 Let = (S, R, L) CTL model 0 S. Supposesatisfiable propositional formula (M, 0 ) 6|= , admissible model updating(M, s0 ) satisfy (M 0 , s0 ), 0 = (S, R, L0 ), (S {s0 }), L0 (s) = L(s),L0 (s0 ) |= , exist another 00 = (S, R, L00 ) L00 (s0 ) |=dif f (L(s0 ), L00 (s0 )) dif f (L(s0 ), L0 (s0 )).Proof: Since propositional formula, update (M, 0 ) satisfy affectrelation elements states except 0 . Since L(s0 ) 6|= , obvious127fiZhang & Dingapplying PU3, change labeling function L L 0 assigns s0 new setpropositional atoms satisfy . Definition 5, see model specifiedproposition indeed minimally changed CTL model respect ordering . 2see problem addressed CTL model update essentially differentproblem concerned traditional propositional belief update. Nevertheless,idea model based minimal change CTL model update closely related belief update.Therefore, worth investigating relationship CTL model updatetraditional propositional belief update postulates (U1) - (U8). order makecomparison possible, lift update operator occurring postulates (U1) - (U8)beyond propositional logic case.purpose, first introduce notions. Given CTL formula Kripkemodel = (S, R, L), let Init(S) set initial states . (M, s) calledmodel iff (M, s) |= , Init(S). use od() denote setmodels . specify update operator c impose CTL formulas follows:given two CTL formulas , define c CTL formula whose modelsdefined as:od( c ) =(M,s)M od()Poss(U pdate((M, s), )).Theorem 1 Operator c satisfies Katsuno Mendelzon update postulates (U1) (U8).Proof: Definitions 4 5, easy verify c satisfies (U1)-(U4). provec satisfies (U5). prove (c ) |= c (), sufficient provemodel (M, s) od(), Poss(U pdate((M, s), ))M od() Poss(U pdate((M, s), )).particular, need show (M 0 , s0 ) Poss(U pdate((M, s), )) od(),(M 0 , s0 ) Poss(U pdate((M, s), )). Suppose (M 0 , s0 ) 6 Poss(U pdate((M, s), )).(1) (M 0 , s0 ) 6|= ; (2) exists different admissible model (M 00 , s00 )od() 00 <M 0 . case (1), (M 0 , s0 ) 6 Poss(U pdate((M, s), ))od(). result holds. case (2), also implies (M 00 , s00 ) |=00 <M 0 . means, (M 0 , s0 ) 6 Poss(U pdate((M, s), )). result still holds.prove c satisfies (U6). prove result, sufficient prove(M, s) od(), Poss(U pdate((M, s), 1 )) od(2 ) Poss(U pdate((M, s), 2 ))od(1 ), Poss(U pdate((M, s), 1 )) = Poss(U pdate((M, s), 2 )). first provePoss(U pdate((M, s), 1 )) Poss(U pdate((M, s), 2 )). Let (M 0 , s0 ) Poss(U pdate((M, s),1 )). (M 0 , s0 ) |= 2 . Suppose (M 0 , s0 ) 6 Poss(U pdate((M, s), 2 )). existsdifferent admissible model (M 00 , s00 ) Poss(U pdate((M, s), 2 )) 00 <M 0 . Alsonote (M 00 , s00 ) |= 1 . contradicts fact (M 0 , s0 ) Poss(U pdate((M, s), 1 )).Poss(U pdate((M, s), 1 )) Poss(U pdate((M, s), 2 )). Similarly, provePoss(U pdate((M, s), 2 )) Poss(U pdate((M, s), 1 )).prove c satisfies (U7), sufficient prove Poss(U pdate((M, s), 1 ))Poss(U pdate((M, s), 1 )) Poss(U pdate((M, s), 1 2 )), (M, s) unique model(note complete). Let (M 0 , s0 ) Poss(U pdate((M, s), 1 ))Poss(U pdate((M, s),1 )). Suppose (M 0 , s0 ) 6 Poss(U pdate((M, s), 1 2 )). exists admissible model (M 00 , s00 ) Poss(U pdate((M, s), 1 2 )) 00 <M 0 . Note128fiCTL Model Update System Modifications(M 00 , s00 ) |= 1 2 . (M 00 , s00 ) |= 1 , implies (M 0 , s0 ) 6 Poss(U pdate((M, s), 1 )).(M 00 , s00 ) |= 2 , implies (M 0 , s0 ) 6 Poss(U pdate((M, s), 2 )). cases,(M 0 , s0 ) 6 Poss(U pdate((M, s), 1 )) Poss(U pdate((M, s), 1 )). proves result.Finally, show c satisfies (U8). Definition 5, od(( 1 2 )c) = (M,s)M od(1 2 ) Poss(U pdate((M, s), )) = (M,s)M od(1 ) Poss(U pdate((M, s), ))(M,s)M od(2 ) Poss(U pdate((M, s), )) = od(1 c ) od(2 c ). completesproof. 2Theorem 1, evident Katsuno Mendelzons update postulates (U1) (U8) characterize wide range update formulations beyond propositional logic case,model based minimal change principle employed. sense, viewKatsuno Mendelzons update postulates (U1) - (U8) essential requirementsmodel based update approaches.4.2 Characterizing Special CTL Model Updatesprevious description, observe that, given CTL Kripke model formula, may many admissible models satisfying , simpler others.section, provide various results present possible solutions achieve admissible updates certain conditions. general, order achieve admissible updateresults, may combine various primitive operations update process.Nevertheless, shown below, single type primitive operation enoughachieve admissible updated model many situations. characterizations also playessential role simplifying CTL model update implementation.Firstly, following proposition simply shows CTL update reachablestates taken account sense unreachable state never removednewly introduced.Proposition 3 Let = (S, R, L) CTL Kripke model, 0 initial state ,satisfiable CTL formula (M, s0 ) 6|= . Suppose (M 0 , s00 ) admissible modelupdating (M, s0 ) , 0 = (S 0 , R0 , L0 ). following properties hold:1. state (i.e. S) reachable 0 (i.e. existpath = [s0 , ] ), must also state 0 (i.e.0 );2. s0 state 0 reachable s00 , s0 must also state .Proof: give proof result 1 since proof result 2 similar. Suppose0 . is, removed generation (M 0 , s00 ).Definitions 4 5, know way remove apply operationPU5 (and possibly associated operations PU2 - removing transition relations,connected states).construct new CTL Kripke model 00 way 00 exactly0 except also 00 . is, 00 = (S 00 , R00 , L00 ), 00 = 0 {s},R00 = R0 , 0 , L00 (s ) = L0 (s ), L00 (s) = L(s). Note 00 , state129fiZhang & Dingisolated state, connecting states. Since , Definition 4see 00 <M 0 . show (M 00 , s00 ) |= . prove showingbit general result:Result: satisfiable CTL formula state 0 , (M 00 , ) |=iff (M 0 , ) |= .showed induction structure . (a) Suppose propositionalformula. case, (M 00 , ) |= iff L00 (s ) |= . Since L00 (s ) = L0 (s ), (M 0 , ) |=iff L0 (s ) |= , (M 00 , ) |= iff (M 00 , ) |= . (b) Assume result holdsformula . (c) consider variours cases formulas constructed . (c.1) Supposeform AG. (M 0 , ) |= AG iff every path 0 = [s , , ],every state s0 0 , (M 0 , s0 ) |= . construction 00 , obvious everypath 0 must also path 00 , vice versa. Also inductionassumption, (M 0 , s0 ) |= iff (M 00 , s0 ) |= . follows (M 0 , ) |= AG iff(M 00 , ) |= AG. Proofs cases AF, EG, etc. similar.Thus, find another model 00 (M 00 , s00 ) |= 00 <M 0 .contradicts fact (M 0 , s00 ) admissible model update (M, 0 ). 2Theorem 2 Let = (S, R, L) Kripke model = (M, 0 ) 6|= EX, s0propositional formula. Let 0 = Update(M, EX) model obtainedupdate EX following 1 2, 0 admissible model.1. PU3 applied one succ(s0 ) make L0 (succ(s0 )) |=diff (L(succ(s0 )), L0 (succ(s0 ))) minimal, or, PU4 PU1 applied successively add new state L0 (s ) |= new relation element (s0 , );2. exists si L(si ) |= si 6= succ(s0 ), PU1 appliedadd new relation element (s 0 , si ).Proof: Consider case 1 first. PU3 applied change assignment succ(s 0 ),PU4 PU1 applied add new state relation element (s0 , ), newmodel 0 contains succ(s0 ) L0 (succ(s0 )) |= . Thus, M0 = (M 0 , s0 ) |= EX.PU3 applied once, Diff (M, M0 ) = (, , {succ(s0 )}, , ); PU4 PU1 applied successively, Diff (M, M0 ) = ({(s0 , )}, , , {, }, ). Thus, updates singleapplication PU3 applications PU4 PU1 successively compatibleother. PU3, update applied combination, Diff (M, 00 )either compatible Diff (M, 0 ) contain Diff (M, M0 ) (e.g., another PU3together predecessor). similar situation occurs applications PU4PU1. Thus, applying either PU3 PU4 PU1 successively representsminimal change. case 2, PU1 applied connect 0 L(si ) |= , newmodel 0 successor satisfies . Thus, 0 = (M 0 , s0 ) |= EX. PU1 applied,Diff (M, M0 ) = ({(s0 , si )}, , , , ). Note case remains minimal changerelation element original model compatible case 1. Hence, case 2130fiCTL Model Update System Modificationsalso represents minimal change. 2Theorem 2 provides two cases admissible CTL model update resultsachieved formula EX. important note restrict propositional formula. first case says either select one successor statess0 change assignment minimally satisfy (i.e., apply PU3 once), simply addnew state new relation element satisfies successor 0 (i.e., apply PU4PU1 successively). second case indicates state alreadysatisfies , enough simply add new relation element (s 0 , si ) makesuccessor s0 . Clearly, cases yield new CTL models satisfy EX.Theorem 3 Let = (S, R, L) Kripke model = (M, 0 ) 6|= AG, s0 S,propositional formula s0 |= . Let M0 = Update(M, AG) model obtainedupdate AG following way, 0 admissible model.path starting s0 : = [s0 , , si , ]:1. < si , L(s) |= L(si ) 6|= , PU2 applied remove relationelement (si1 , si );2. PU3 applied states satisfying change assignmentsL0 (s) |= diff (L(s), L0 (s)) minimal.Proof: Case 1 simply cut path first state satisfy . Clearly,one minimal way cut : remove relation element (s i1 , s) (i.e., apply PU2once). Case 2 minimally change assignments states belongingsatisfy . Since changes imposed case 1 case 2 compatibleother, generate admissible update results. 2Theorem 3, case 1 considers special form path first statesstarting s0 already satisfy formula . condition, simply cutpath disconnect states satisfying . Case 2 straightforward: minimallymodify assignments states belonging satisfy formula .Theorem 4 Let = (S, R, L) Kripke model, = (M, 0 ) 6|= EG, s0propositional formula. Let 0 = Update(M, EG) model obtainedupdate EG following way, 0 admissible model: Selectpath = [s0 , s1 , , si , , sj , ] contains minimal number differentstates satisfying 3 ,1. s0 L(s0 ) 6|= , exist si , sj satisfying si < s0 < sjsi sj , L(s) |= , PU1 applied add relation element (s , sj ),PU4 PU1 applied add state L0 (s ) |= new relationelements (si , ) (s , sj );2. si si , L(s) |= , sk 00 , 00 = [s0 , , sk , ]sk L(s) |= , PU1 applied connect sk ;3. Note although path may infinite, contain finite number different states.131fiZhang & Ding3. si (i > 1) s0 < si , L(s0 ) |= , L(si ) 6|= , then,a. PU1 applied connect si1 s0 form new transition (si1 , s0 );b. si successor si1 , PU2 applied remove relation element(si1 , si );4. s0 , L(s0 ) 6|= , PU3 applied change assignmentsstates s0 L0 (s0 ) |= diff (L(s), L0 (s0 )) minimal.Proof: case 1, without loss generality, assume selected path ,exist states s0 satisfy , states satisfy . also assumes0 middle path . Therefore, two states , sjsi < s0 < sj . is, = [s0 , , si1 , si , , s0 , , sj , sj+1 , ]. firstconsider applying PU1. clear applying PU1 add new relation element(si , sj ), new path formed: 0 = [s0 , , si1 , si , sj , sj+1 , ]. Note state0 also path s0 6 0 . Accordingly, know EG holds newmodel 0 = (S, R {(si , sj )}, L) state s0 . Consider = (M, s0 ) M0 = (M 0 , s00 ).Clearly, Diff (M, M0 ) = ({(si , sj )}, , , , ), implies (M 0 , s0 ) must minimally changed model respect satisfies EG.consider applying PU4 PU1. case, new model0 = (S {s }, R {(si , ), (s , sj )}, L0 ) L0 extension L new statesatisfies . see 0 = [s0 , , si , , sj , ] path 0 sharesstates path except state 0 states si+1 sj1including s0 . also (M 0 , s0 ) |= EG. Furthermore, Diff (M, 0 ) =({(si , ), (s , sj )}, , , {s }, ). Obviously, (M 0 , s0 ) minimally changed modelrespect satisfies EG.worth mentioning case 1, model obtained applying PU1comparable model obtained applying PU4 PU1, set inclusionrelation holds changes relation elements caused two different ways.case 2, consider two different paths = [s 0 , , si , ] 0 = [s0 , , sk , ]states state si path satisfy , states state k path 0satisfy , PU1 applied form new transition (s , sk ). transition thereforeconnects states s0 si path states sk path 0 . Hence statesnew path [s0 , , si , sk ] satisfy . Thus, M0 |= EG. change also minimal,PU1 applied, Diff (M, 0 ) = ({(si , sk )}, , , , ) minimum (M 0 , s0 )minimally changed model respect satisfies EG.case 3, two situations. (a) PU1 applied form new transition (si1 , s0 ), new path containing [s0 , , s0 , , si1 , s0 , , si1 , s0 , ] consistsStrongly Connected Components states satisfy ,Diff (M, M0 ) = ({(si1 , s0 )}, , , , ) minimum. Thus, (M 0 , s0 ) minimally changedmodel respect satisfies EG.(b) PU2 applied, then, new path 0 containing [s0 , , s0 , , si1 ] derivedstates satisfy Dif f (M, 0 ) = (, {(si1 , si )}, , , ) minimal. Obviously,(M 0 , s0 ) minimally changed model respect satisfies EG.case 4, suppose n states selected path satisfy .PU3 applied states, Diff (M, 0 ) = (, , {s01 , s02 , , s0n }, , ),s0 {s01 , , s0n }, dif f (L(s0 ), L0 (s0 )) minimal. Diff (M, M0 ) case132fiCTL Model Update System Modificationscompatible cases 1, 2 3. Thus, (M 0 , s0 ) minimally changed modelrespect satisfies EG. 2Theorem 4 characterizes four typical situations update formula EGpropositional formula. Basically, theorem says order make formulaEG true, first select path, either make new path based pathstates new path satisfy (i.e., case 1, case 2 case 3(a)), trim pathstate previous states satisfy (i.e., case 3(b)), previous statestate successor; simply change assignments states satisfyingpath (i.e., case 4). proof shows models obtained operationsadmissible.possible provide semantic characterizations updates specialCTL formulas EF, AX, E[U]. fact, prototype implementation,characterizations used simplify update process whenever certainconditions hold.also indicate characterization theorems presented sectionprovide sufficient conditions compute admissible models. admissiblemodels captured theorems.5. Computational Propertiessection, study computational properties CTL model update approachdetail. first present general complexity result, identify usefulsubclass CTL model updates always achieved polynomial time.5.1 General Complexity ResultTheorem 5 Given two CTL Kripke models = (S, R, L) 0 = (S 0 , R0 , L0 ),s0 s00 0 , CTL formula , co-NP-complete decide whether (M 0 , s00 )admissible model update (M, 0 ) satisfy . hardness holds evenform EX propositional formula.Proof: Membership proof: Firstly, know Clarke et al. (1999) checkingwhether (M 0 , s00 ) satisfies performed time O(|| (|S| + |R|)). ordercheck whether (M 0 , s00 ) admissible update result, need check whether 0minimally updated model respect ordering . purpose, considercomplement problem checking whether 0 minimally updated model.Therefore, two things: (1) guess another updated model : 00 = (S 00 , R00 , L00 )satisfying s00 00 ; and, (2) test whether 00 <M 0 . Step (1) donepolynomial time. check 00 <M 0 , first compute dif f (S, 0 ), dif f (S, 00 ),dif f (R, R0 ) dif f (R, R00 ). computed polynomial time. Then, according sets, identify Dif f P U (M, 0 ) Dif fP U i(M, 00 ) (i = 1, , 5)terms PU1 PU5. Again, steps also completed polynomial time. Finally,checking Dif fP U i(M, 00 ) Dif fP U i(M, 0 ) (i = 1, , 5), dif f (L(s), L0 (s))dif f (L(s), L00 (s)) Dif fP U 3 (M, 00 ) (if Dif fP U 3 (M, 00 ) = Dif fP U 3 (M, 0 )),133fiZhang & Dingdecide whether 00 <M 0 . Thus, steps (1) (2) achievedpolynomial time non-deterministic Turing machine.Hardness proof: well known validity problem propositional formulaco-NP-complete. Given propositional formula , construct transformationproblem deciding validity CTL model update polynomial time. Let Xset variables occurring , a, b two new variables occur X.Vdenote X = xi X xi . Then, specify CTL Kripke model based variable setX {a, b}: = ({s0 , s1 }, {(s0 , s1 ), (s1 , s1 )}, L), L(s0 ) = (i.e., variables assigned false), L(s1 ) = X (i.e., variables X assigned true, a, b assigned false).define new formula = EX((( a)(X b))(a)). Clearly, formula ((a)(X b))(a) satisfiable 1 6|= (( a)(X b))(a). (M, 0 ) 6|= .Consider update U pdate((M, s0 ), ). define 0 = ({s0 , s1 }, {(s0 , s1 ), (s1 , s1 )}, L0 ),L0 (s0 ) = L(s0 ) L0 (s1 ) = {a, b}. Next, show valid iff (M 0 , s0 )admissible update result U pdate((M, 0 ), ).Case 1: show valid, (M 0 , s0 ) admissible update resultU pdate((M, s0 ), ). Since valid, X |= . Thus, L 0 (s1 ) |= ( a) (X b)).leads (M 0 , s0 ) |= . Also note 0 obtained applying PU3 change L(s 1 )L0 (s1 ). dif f (L(s1 ), L0 (s1 )) = X {a, b}, presents minimal change L(s 1 )order satisfy ( a) (X b).Case 2: Suppose valid. Then, X 1 X exists X1 |= . construct 00 = ({s0 , s1 }, {(s0 , s1 ), (s1 , s1 )}, L00 ), L00 (s0 ) = L(s0 ) L00 (s1 ) = X1 {a}.seen L00 (s1 ) |= ( a), hence (M 00 , s0 ) |= . show (M 0 , s0 ) |=implies 00 <M 0 . Suppose (M 0 , s0 ) |= . Clearly, 0 00 obtained applying PU3 change assignment 1 . However,dif f (L(s1 ), L00 (s1 )) = (X X1 ) {a} X {a, b} = dif f (L(s), L0 (s1 )). Thus, conclude(M 0 , s0 ) admissible updated model. 2Theorem 5 implies probably feasible develop polynomial time algorithmimplement CTL model update. Indeed, algorithm described next section,generally runs exponential time.5.2 Tractable Subclass CTL Model Updateslight complexity result Theorem 5, expect identify useful casesCTL model updates performed efficiently. First, followingobservation.Observation: Let = (S, R, L) CTL Kripke model, CTL formula (M, 0 ) 6|=s0 S. admissible model U pdate((M, 0 ), ) obtained applyingoperations PU1 PU2 , result computed polynomial time.Intuitively, admissible updated model obtained using PU1 PU2,implies need visit states relation elements ,operation involving PU1 PU2 completed adding removing relationelements, obviously done linear time.134fiCTL Model Update System Modificationsobservation tells us certain conditions, operations PU1 PU2 mayefficiently applied compute admissible model. quite obvious PU3PU4 involved finding models propositional formulas, applyingPU3 usually needs find minimal change assignment state,operations may cost exponential time size input updating formula. However, observation tell us kinds CTL model updatesreally achieved polynomial time. following, provide sufficient conditionclass CTL model updates always solved polynomial time.first specify subclass CTL formulas AEClass: (1) formulas AX, AG, AF,A[1 U2 ], EX, EG, EF E[1 U2 ] AEClass, , 1 2 propositional formulas; (2) 1 2 AEClass, 1 2 1 2AEClass; (3) formulas specified (1) (2) AEClass.also call formulas forms specified (1) atomic AEClass formulas.Note AEClass class CTL formulas without nested temporal operators.Although somewhat restricted, show next, updates kind CTLformulas may much simpler cases. define valid states pathsAEClass formulas respect given model.Definition 6 (Valid state path AEClass) Let = (S, R, L) CTL Kripkemodel, AEClass, (M, s0 ) 6|= , s0 S. define valid state validpath (M, s0 ) follows.1. form AX, state valid state (M, 0 ) (s0 , s) RL(s) |= ;2. form (a) AG, (b) AF (c) A[ 1 U2 ], path = [s0 , ]valid path (M, s0 ) , L(s) |= (case (a)); > 0 ,L(s) |= (case (b)); , |= 2 s0 < L(s0 ) |= 1 (case (c)) respectively;3. form EX, state valid state (M, 0 ) L(s) |= ;4. form (a) EG, (b) EF (c) E[ 1 U2 ], path = [s00 , ](s00 6= s0 ) valid path (M, s0 ) , L(s) |= L(s0 ) |= (case(a)); > s00 , L(s) |= (case (b)); L(s0 ) |= 1 , L(s) |= 2s0 < L(s0 ) |= 1 (case (c)) respectively.arbitrary AEClass, say valid witness (M, 0 ) every atomicAEClass formula occurring valid state path (M, 0 ).Intuitively, formulas AX, AG, AF A[ 1 U2 ], valid state pathCTL model represents local structure partially satisfies underlying formula.formulas EX, EG, EF E[1 U2 ], hand, valid state path alsorepresents local structure satisfy underlying formula relation elementadded connect local structure initial state.Example 3 Consider CTL Kripke model Figure 10 formula EX(p q).Clearly, (M, s0 ) 6|= EX(p q). Since p, q L(s3 ), s3 valid state EX(p q).135fiZhang & DingS0S1pS2qrS3S4p,qr,pFigure 10: simple CTL model update.simply add one relation element (s 0 , s3 ) form new model 0(M 0 , s0 ) |= EX(p q). Obviously, (M 0 , s0 ) admissible updated model.2example, observe update CTL model AEClassformula formula valid witness model, possible computeadmissible model adding removing relation elements (i.e. operations PU1PU2). following results confirm CTL model update AEClass formulamay achieved polynomial time formula valid witness model.Theorem 6 Let = (S, R, L) CTL Kripke model, AEClass, (M, 0 ) 6|=. Deciding whether valid witness (M, 0 ) solved polynomial time.Furthermore, valid witness (M, 0 ), valid states paths atomicAEClass formulas occurring computed (M, 0 ) time O(||(|S|+|R|)2 ).Proof: prove theorem, show using CTL model checking algorithm SAT(Huth & Ryan, 2004), takes CTL Kripke model AEClass formula inputs,generate valid states paths atomic AEClass formulas occurring (ifany). know complexity algorithm SAT O(|| (|S| + |R|)). considercase atomic AEClass formulas.AX. use SAT check whether (M, 0 ) |= EX. (M, s0 ) 6|= EX,AE valid state (M, 0 ). Otherwise, SAT return stateL(s) |= (s0 , s) R. remove relation element (s 0 , s) , continuechecking formula EX model. end process, obtain valid states(M, s0 ) formula AX. Altogether, |S| SAT calls.AG. use SAT check whether (M, 0 ) |= EG. (M, s0 ) |= EG,obtain path SAT = [s0 , s1 , ] , L(s) |= . Clearly,valid path AG. exist state 6(s, s) R , i.e. state connects state leading different path,136fiCTL Model Update System Modificationsprocess stops, valid path AG. Otherwise, removeone relation element (s, s0 ) (i.e. s, s0 ) states s00s0 < s00 , relation element (s00 , ) leading different path (i.e. 6 ).way, actually disable path satisfy formula EG without affecting paths.continue checking formula EG newly obtained model. endprocess, obtain paths make EG true, paths valid pathsAG. Since generated valid path, need remove one relation elementpath generate next valid path, |R| valid pathsgenerated. together, |R| SAT calls find valid pathsAG.cases AF A[U2 ], valid paths formulas generatedsimilar way described formula AG. different pointcase A[U2 ], valid path generated, need find last state2 becomes true, connects state 6 leading differentpath, disable removing relation element (s, succ(s)) . continueprocedure generate next valid path A[U 2 ]. exists ,process stops.EX. case, valid state found checking whether L(s) |= .need visit |S| states checking.EG. Similarly, find valid path selecting state (s 6= 0 ),(M, s) |= EG. most, need visit |S| states, |S| SAT calls check(M, s) |= EG.Finally, valid paths EF E[ 1 2 ] found similar way. 2Theorem 7 Let = (S, R, L) CTL Kripke model, AEClass, (M, 0 ) 6|= .admissible model U pdate((M, s0 ), ) computed polynomial time validwitness (M, s0 ).Proof: proof Theorem 6, obtain valid states paths atomicAEClass formulas time O(|| (|S| + |R|) 2 ). consider case atomicAEClass formulas , cases conjunctive disjunctive AEClass formulaseasy justify.AX. Let = {s1 , , sk } valid states AX. remove relationelements (s0 , s) 6 . way, obtain new model 0 = (S, R0 , L),R0 = R {(s0 , s) | 6 }. Obviously, (M 0 , s0 ) |= AX. also easy seechange 0 minimal order satisfy AX. (M 0 , s0 ) alsoadmissible model.AG. Let set states valid paths AG.state s0 L(s0 ) 6|= , check whether s0 reachable s0 . reachable,remove relation element (s 1 , s2 ) s0 becomes unreachables0 (s1 , s2 ) relation element valid path AG. Clearly, model (M 0 , s0 )satisfy AG. Also, checking whether state reachable 0 donepolynomial time computing spanning tree rooted 0 (Pettie & Ramachandran,2002).137fiZhang & DingAF. case, need cut paths starting 0valid paths AF (M, s0 ). this, sufficient disconnect statesreachable s0 occur AFs valid paths (M, 0 ). Letset states, R set relation elements directly connectedstates, i.e. (s1 , s2 ) R iff s1 s2 . remove minimal subsetR removing disconnect states s0 . setidentified polynomial time computing spanning tree rooted 0 ,minimal subset R disconnects states s0 found timeO(|R |2 ). entire process completed polynomial time.case A[1 U2 ] handled similar way described AF.consider EX. case, need select one valid stateEX, add relation element (s0 , s) . model (M 0 , s0 ) satisfies EX.case EG, also select valid path = [s, ] EG, add relationelement (s0 , s), (M 0 , s0 ) |= EG. two cases EF E[ 1 U2 ]handled similar way. 2emphasize although results characterize useful subclassCTL model update scenarios admissible updated models computedsimple operations adding removing relation elements, meanadmissible models represent intuitive modifications practical viewpoint.Sometimes, update problem, using operations PU3 PU4probably preferred order generate sensible system modification.illustrated Section 7.6. CTL Model Update Algorithmimplemented prototype CTL model update. implementation,CTL model update algorithm designed line CTL model checking algorithmused SAT (Huth & Ryan, 2004), updated formula parsed accordingstructure recursive calls appropriate functions used. recursive call usageallows checked property range nested modalities atomic propositionalformulas. section, focus discussions key ideas handling CTLmodel update provide high level pseudo code major functions algorithm.6.1 Main FunctionsHandling propositional formulasSince satisfaction propositional formula involve relation elementsCTL Kripke model, implement update propositional formula directlyoperation PU3 minimal change labeling function truth assignment relevant state. procedure outlined follows.function Updateprop ((M, s0 ), )input: (M, s0 ) , = (S, R, L) s0 S;output: (M 0 , s00 ), 0 = (S 0 , R0 , L0 ), s00 0 L0 (s00 ) |= ;138fiCTL Model Update System Modifications01 begin02apply PU3 change labeling function L state 0 form new model 0 =00(S , R , L0 ):030 = S; R0 = R; 6= s0 , L0 (s) = L(s);04L0 (s0 ) defined L0 (s0 ) |= , dif f (L0 (s0 ), L(s0 )) minimal;05return (M 0 , s0 );06 endeasy observe procedure implemented PMA belief update(Winslett, 1988). used lowest level CTL model update prototype.Handling modal formulas AF, EX E[ 1 2 ]De Morgan rules equivalences displayed Section 2.1, knowCTL formulas modal operators expressed terms three typical CTLmodal formulas. Hence sufficient give update functions three typesformulas without considering types CTL modal formulas.function UpdateAF ((M, s0 ), AF)input: (M, s0 ) AF, = (S, R, L), s0 S, (M, s0 ) 6|= AF;output: (M 0 , s00 ), 0 = (S 0 , R0 , L0 ), s00 0 (M 0 , s00 ) |= AF;01 begin02S, (M, s) 6|= ,03select state reachable 0 , (M 0 , ) = CTLUpdate((M, s), )4 ;04else select path starting 0 , (M, s) 6|= , (a) (b):05(a) select state , (M 0 , s0 ) = CTLUpdate((M, s), );06(b) apply PU2 disable path form new model:07remove relation element affect paths;08form new model 0 = (S 0 , R0 , L0 ):090 = S, R0 = R {(si , si+1 )} (note (si , si+1 ) ),100 , L0 (s) = L(s);011(M , s00 ) |= AF, return (M 0 , s00 )5 ;12else UpdateAF ((M 0 , s00 ), AF);13 endFunction UpdateAF handles update formula AF follows: statemodel satisfies formula , UpdateAF first update model one state satisfy ;otherwise, path model fails satisfy AF, Update AF either disablespath minimal way, updates path make valid AF.function UpdateEX ((M, s0 ), EX)input: (M, s0 ) EX, = (S, R, L), s0 S, (M, s0 ) 6|= EX;output: (M 0 , s00 ), 0 = (S 0 , R0 , L0 ), s00 0 (M 0 , s00 ) |= EX;01 begin02one (a), (b) (c):4. CTLUpdate((M, s), ) main update function describe later.5. s00 corresponding state s0 updated model 0 , functionsdescribed next.139fiZhang & Ding0304050607080910111213(a) apply PU1 form new model:select state (M, s) |= ;add relation element (s0 , s) form new model 0 = (S 0 , R0 , L0 ):0 = S; R0 = R {(s0 , s)}; S, L0 (s) = L(s);(b) select state = succ(s0 ), (M 0 , ) = CTLUpdate((M, s), );(c) apply PU4 PU1 form new model 0 = (S 0 , R0 , L0 ):0 = {s }; R0 = R {(s0 , }; S, L0 (s) = L(s),L0 (s ) defined (M 0 , ) |= ;0(M , s00 ) |= EX, return (M 0 , s00 );else UpdateEX ((M 0 , s00 ), EX);endFunction UpdateEX may viewed implementation algorithm characterization EX Theorem 2 Section 4. However, worth mentioningalgorithm illustrates difference update functions updatecharacterizations demonstrates wider application algorithm comparedcorresponding characterizations. usage recursive calls algorithm allowsarbitrary CTL formula rather propositional formula demonstratedcharacterizations. major difference characterizationsalgorithmic implementation.function UpdateEU ((M, s0 ), E[1 U2 ])input: (M, s0 ) E[1 U2 ], = (S, R, L), s0 S, (M, s0 ) 6|= E[1 U2 ];output: (M 0 , s00 ), 0 = (S 0 , R0 , L0 ), s00 0 (M 0 , s00 ) |= E[1 U2 ];01 begin02(M, s0 ) 6|= 1 , (M 0 , s00 ) = CTLUpdate((M, s0 ), 1 );03else (a) (b):04(a) (M, s0 ) |= 1 , path = [s , ] (s0 6= )05(M, ) |= E[1 U2 ],06apply PU1 form new model 0 = (S 0 , R0 , L0 ):070 = S; R0 = R {(s0 , }; L0 (s) = L(s);08(b) select path = [s0 , , si , , sj , ];09s0 < < si , (M, s) |= 1 , (M, sj ) |= 2 ,10s0 si+1 < s0 < sj1 , (M, s0 ) 6|= 1 211apply PU1 form new model 0 = (S 0 , R0 , L0 ):120 = S; R0 = R {(si , sj )}; S, L0 (s) = L(s);13< si , (M, s) |= 1 , s0 s0 > si+1 , (M, s0 ) 6|= 1 2 ,14apply PU4 form new model 0 = (S 0 , R0 , L0 ):150 = {s }; R0 = R {(si1 , ), (s , si )};16S, L0 (s) = L(s), L(s ) defined (M 0 , ) |= 2 ;0017(M , s0 ) |= E[1 U2 ], return (M 0 , s00 );18else UpdateEU ((M 0 , s00 ), E[1 U2 ]);19 endupdate (M, s0 ) satisfy formula E[1 U2 ], function UpdateEU first checks whethersatisfies 1 initial state s0 . not, UpdateEU update140fiCTL Model Update System Modificationsinitial state model satisfies 1 initial state. make laterupdate possible. condition (M, 0 ) satisfies 1 , UpdateEU considerstwo cases: valid path formula E[ 1 U2 ], simply linksinitial state s0 path forms new path satisfies E[ 1 U2 ] (i.e. case (a));UpdateEU directly selects path make satisfy formula E[ 1 U2 ] (i.e. case (b)).Handling logical connectives ,De Morgan rules equivalences CTL modal formulas, updateformula handled quite easily. fact need consider primary formsnegative formulas algorithm implementation. Update disjunctive formula1 2 , hand, simply implemented calling CTLUpdate((M, 0 ), 1 )CTLUpdate((M, s0 ), 2 ) nondeterministic way. Hence describefunction updating conjunctive formula 1 2 .function Update ((M, s0 ), 1 2 )input: (M, s0 ) 1 2 , = (S, R, L), s0 S, (M, s0 ) 6|= 1 2 ;output: (M 0 , s00 ), 0 = (S 0 , R0 , L0 ), s00 0 (M 0 , s00 ) |= 1 2 ;01 begin021 2 propositional formula, (M 0 , s00 ) = Updateprop ((M, s0 ), 1 2 );03else (M , s0 ) = CTLUpdate((M, s0 ), 1 );04(M 0 , s00 ) = CTLUpdate((M , s0 ), 2 ) constraint 1 ;05return (M 0 , s00 );06 endFunction Update handles update conjunctive formula obvious way. Line04 indicates conduct update 2 , view 1 constraintupdate obey. Without condition, result updating 2 may violatesatisfaction 1 achieved previous update. address pointdetails next subsection.Finally, describe CTL model update algorithm follows.algorithm CTLUpdate((M, s0 ), )input: (M, s0 ) , = (S, R, L), s0 S, (M, s0 ) 6|= ;output: (M 0 , s00 ), 0 = (S 0 , R0 , L0 ), s00 0 (M 0 , s00 ) |= ;01 begin02case03propositional formula: return Update prop ((M, s0 ), );041 2 : return Update ((M, s0 ), 1 2 );051 2 : return Update ((M, s0 ), 1 2 );061 : return Update ((M, s0 ), 1 );07AX1 : return CTLUpdate((M, s0 ), EX1 );08EX1 : return UpdateEX ((M, s0 ), EX1 );09A[1 U2 ]: return CTLUpdate((M, s0 ), (E[2 U(1 2 )] EG2 ));10E[1 U2 ]: return UpdateEU ((M, s0 ), E[1 U2 ]);11EF1 ; return CTLUpdate((M, s0 ), E[>U1 ]);12EG1 : return CTLUpdate((M, s0 ), AF1 );141fiZhang & Ding13141516AF1 : return UpdateAF ((M, s0 ), AF1 );AG1 : return CTLUpdate((M, s0 ), E[>U1 ]);end case;endTheorem 8 Given CTL Kripke model = (S, R, L) satisfiable CTL formula, (M, s0 ) 6|= s0 S. Algorithm CTLUpdate((M, s0 ), ) terminatesgenerates admissible model satisfy . worst case, CTLUpdate runs timeO(2|| ||2 (|S| + |R|)2 ).Proof: Since assumed satisfiable, descriptions,difficult see CTLUpdate call functions finite times, callfunctions (recursively) generate result satisfies underlying updatedformula, return main algorithm CTLUpdate. CTLUpdate((M, 0 ), )terminate, output model (M 0 , s00 ) satisfies .show output model (M 0 , s00 ) admissible induction structure. proof quite tedious - involves detailed examinations runningupdate function. sufficient observe update function, timeinput model updated minimal way, e.g., adds one state relation element, removesminimal set relation elements disconnect state, updates state minimally.iterated updates sub-formulas , minimal changes original input modelretained.consider complexity CTLUpdate. first analyze functions complexity without considering embedded recursions. Function Update prop updatestate propositional formula, worst time complexity O(2 || ). Function UpdateAF contains following major computations: (1) finding reachable state(M, s0 ); (2) selecting path state satisfy ; (3) checking(M 0 , s00 ) |= AF. Task (1) achieved computing spanning tree rooteds0 , done time O(|R| log|S|) (Pettie & Ramachandran, 2002). Task(2) reduced find valid path formula AG. Theorem 6,done time O(|| (|S| + |R|)2 ). Task (3) complexity task (2). So,overall, function UpdateAF complexity O(|| (|S| + |R|) 2 ). Similarly,show functions UpdateEX UpdateEU complexity O(|| (|S| + |R|)2 )O(|| (|S| + |R|)2 + 2|| ) respectively. functions complexity obvious eitherimplementations based De Morgan rules equivalences, callsfunctions (i.e. Update ) main algorithm (i.e. Update Update ).algorithm CTLUpdate || calls functions. Therefore, worst time,CTLUpdate runs time O(2|| ||2 (|S| + |R|)2 ). 26.2 Discussionsworth mentioning except functions Update prop , Update Update ,functions used algorithm CTLUpdate involved nondeterministic choices.implies algorithm CTLUpdate syntax independent. words, given142fiCTL Model Update System ModificationsCTL model two logical equivalent formulas, updating model one formulamay generate different admissible models.description function Update , briefly mentioned issue constraintsCTL model update. general, perform CTL model update, usuallyprotect properties violated update procedure.properties usually called domain constraints. difficult modify algorithmCTLUpdate cope requirement. particular, suppose C set domainconstraints system specification = (S, R, L), need update (M, 0 )formula , s0 S, C {} satisfiable. function CTLUpdate,simply add model checking condition candidate model 0 = (S 0 , R0 , L0 ): (M 0 , s00 ) |=C (s00 0 ). result (M 0 , s00 ) returned function satisfies C. Otherwise,function look another candidate model. Since model checking (M 0 , s00 ) |= Cdone time O(|C| (|S 0 | + |R0 |)), modified algorithm significantly increaseoverall complexity. implemented system prototype, integrated genericconstraint checking component option added update functionsdomain constraints may taken account necessary.addition implementation algorithm CTLUpdate, implementedseparate update functions typical CTL formulas EX, AG, EG, AF, EF,etc., propositional formula, based characterizations provided Section4.2. functions simplify update procedure input formula containnested CTL temporal operators converted simplified formula.7. Two Case Studiessection, show two case studies applications CTL model update approachsystem modifications. two cases implemented CTL model updaterprototype, simplified compiler. prototype, input complete CTLKripke model CTL formula, output updated CTL Kripke modelsatisfies input formula.indicate prototype contains three major components: parsing, modelchecking model update functions. prototype first parses input formulabreaks atomic subformulas. model checking function checkswhether input formula satisfied underlying model. formula satisfiedmodel, model checking function generate relevant states violateinput formula. Consequently, information directly used model updatefunction update model.7.1 Microwave Oven Exampleconsider well-known microwave oven scenario presented Clarke et al. (1999),used illustrate CTL model checking algorithm model describingbehaviour microwave oven. Kripke model shown Figure 11 viewedhardware design microwave oven. Kripke model, state labeledpropositional atoms true state negations propositionalatoms false state. labels arcs present actions causestate transitions Kripke model. Note actions part Kripke model.143fiZhang & Dinginitial state state 1. given Kripke model describes behaviourmicrowave oven.s1start ovens2Start~Close~HeatErroropen doors5close doorStartClose~HeatError~Start~Close~Heat~Erroropen doorclose doors3s4~StartClose~Heat~Errorresets6done~StartCloseHeat~Errorstart cookingstart ovenStartClose~Heat~Errorcookopen doors7warmStartCloseHeat~ErrorFigure 11: CTL Kripke model microwave oven.observed model satisfy desired property = EF(StartEGHeat): microwave oven started, stuff inside eventually heated(Clarke et al., 1999)6 . is, (M, s1 ) 6|= . would like applyCTL model update prototype modify Kripke model satisfy property .mentioned earlier, since prototype combines formula parsing, model checking modelupdate together, update procedure case study exactly follow genericCTL model update algorithm illustrated Section 6.s1~Start~Close~Heat~Erroropen doors2Start~Close~HeatErroropen doors5close doorStartClose~HeatErrorclose doors3s4~StartClose~Heat~Errorresets6dones7warm~StartCloseHeat~Errorstart cookingstart ovenStartClose~Heat~Errorcookopen doorStartCloseHeat~ErrorFigure 12: Updated microwave oven model using PU2.6. formula equivalent AG(Start AFheat).144fiCTL Model Update System ModificationsFirst, parse AG((StartEGHeat)) remove front . translationperformed function Update , called CTLUpdate((M, 1 ), ). checkwhether state satisfies (StartEGHeat). First, select EGHeat checkedusing model checking function EG. model checking, path everystate Heat identified. find paths [s 1 , s2 , s5 , s3 , s1 , ] [s1 , s3 , s1 , ]strongly connected component loops (Clarke et al., 1999) containing statesHeat. Thus model satisfies EGHeat. Consequently, identify statesStart: {s2 , s5 , s6 , s7 }. select states Start Heat:{s2 , s5 }. Since formula AG((StartEGHeat)) requires modelstates Start Heat, perform model update relatedstates s2 s5 . Now, using Theorem 3 Section 4.2, proper update performed.Eventually, obtain two possible minimal updates: (1) applying PU2 remove relationelement (s1 , s2 ); (2) applying PU3 change truth assignments 2 s5 .update, model satisfies formula minimal change originalmodel . instance, choosing update (1) above, obtain new Kripke model(as shown Figure 12), simply states state transition 1 s2 allowed,whereas choosing update (2), obtain new Kripke model (as shown Figure 13),says allowing transition state 1 state s2 cause error microwaveoven could start s2 , error message carry next state 5 .s1start ovens2~Start~Close~HeatErroropen doors5 ~Startclose doorClose~HeatError~Start~Close~Heat~Erroropen doorclose doors3s4~StartClose~Heat~Errorresets6open doordone~StartCloseHeat~Errorstart cookingstart ovenStartClose~Heat~Errorcooks7warmupStartCloseHeat~ErrorFigure 13: Updated microwave oven model using PU3.7.2 Updating Andrew File System 1 ProtocolAndrew File System 1 (AFS1) (Wing & Vaziri-Farahani, 1995) cache coherenceprotocol distributed file system. AFS1 applies validation-based techniqueclient-server protocol, described Wing Vaziri-Farahani (1995). protocol,client two initial states: either files one files beliefsvalidity. protocol starts client suspect files, clientmay request file validation server. file invalid, client requestsnew copy run terminates. file valid, protocol simply terminates.145fiZhang & DingAFS1 abstracted model one client, one server one file. state transitiondiagrams single client server modules presented Figure 14. nodesarcs labelled value state variable, belief , and, name receivedmessage causes state transition, respectively. protocol run begins initialstate (one leftmost nodes) ends final state (one rightmost nodes).ClientvalnofilevalidvalsuspectinvalvalfetchServernoneinvalidvalidvalidate & valid-filevalidate& !valid-fileinvalidfetchFigure 14: State transition diagrams AFS1.clients belief file 4 possible values {nof ile, valid, invalid, suspect},nofile means client cache empty; valid, client believes cachedfile valid; invalid believes caches file valid; suspect, beliefvalidity file (it could valid invalid). servers belief filecached client ranges {valid, invalid, none}, valid, server believesfile cached client valid; invalid, server believes valid; none,server belief existence file clients cache validity.set messages client may send server {f etch, validate}.message f etch stands request file, validate message used clientdetermine validity file cache. set messages server may sendclient {val, inval}. server sends val (inval) message indicateclient cached file valid (invalid). valid-f ile used client suspectfile cache requests validation server. update clientoccurred server reflects fact nondeterministically setting valuevalid-f ile 0; otherwise, 1 (the file cached client still valid). specificationproperty AFS1 is:AG((Server.belief = valid) (Client.belief = valid)).(1)file system design, client belief leads server belief. specificationproperty deliberately chosen fail AFS1 (Wing & Vaziri-Farahani, 1995).Thus, model updating, need pay much attention rationalityupdated models. model updater update AFS1 model derive admissible146fiCTL Model Update System Modificationsmodels satisfy specification property (1). case study, focusupdate procedure according functionality prototype.Extracting Kripke model AFS1 NuSMVnoted that, CTL model update algorithm described Section 6,complete Kripke model describing system behaviours one two input parameters (i.e.,(M, s0 ) ), original AFS1 model checking process demonstrated (Wing &Vaziri-Farahani, 1995) contain Kripke model. fact, provides SMVmodel definitions (e.g., AFS1.smv) input SMV model checker. requires initialextraction complete AFS1 Kripke model performing update it.purpose, NuSMV (Cimatti et al., 1999) used derive Kripke modelloaded model (AFS1). output Kripke model shown Figure 15. methodalso used extracting Kripke model.#1: Client.out={0,fetch,validate} ;#2: Client.belief={valid,invalid,suspect,nofile} ;#3: Server.out={0,val,inval} ;#4: Server.belief={none,valid,invalid} ;#5: Server.validfile={true,false} ;1117190,n,0,n,0,n,0,n,ff,n,0,n,ff,n,0,n,f,n,v,v,f,n,v,v,f2513v,s,12122 f,i,23f,i,v,v,26v,s,i, i,f4v,i,0,i,20v,s,0,n,fv,s,i, i,0,i,ff,v,0,v,0,s,0,n,f146 0,n,3180,s,0,n,2v,i,0,i,f21f,i,0,i,2478 v,s,v,v,v,s,v,v,9fv,v,0,v,v,v,10 0,v,ff,i,v,v,ff,v,0,v,f#1,#2,#3,#4, shows order variables state;15#5Initials values variables shown states.50,v,0,v,0,v,0,v,f16Initial states: {11, 12, 13, 14}False states: {19, 20, 23, 24, 7, 8}Figure 15: CTL Kripke model AFS1.AFS1 Kripke model (see Figure 15), 26 reachable states (out total 216states) 52 transitions them. model contains 4 initial states {11, 12, 13, 14}5 variables individual variable 2, 3 4 possible values. variables are: Client.out, (range {0, f etch, validate}); Client.belief (range {valid, invalid,147fiZhang & Dingsuspect, nof ile}); Server.out (range {0, val, inval}); Server.belief (range {none, valid,invalid}); Server.valid-file (range {true, f alse}).Update procedureModel checking: CTL model update prototype, first check whether formula (1)satisfied AFS1 model. is, need check whether reachable state containseither Server.belief = valid Client.belief = valid. model updater identifiesset reachable states satisfy conditions {19, 20, 23, 24, 7, 8}.call states false states.Model update: Figure 15 reveals false state AFS1 different path.Theorem 3 Section 4 UpdateAG Section 6, know update modelsatisfy property, operations PU2 PU3 may applied false statescertain combinations. result, one admissible model depicted Figure 16.model results update false state false path updated usingPU2. observe update, states 25, 26, 15 16 longer reachableinitial states 11 12, states 9 10 become unreachable initial states 1314.#1: Client.out={0,fetch,validate} ;#2: Client.belief={valid,invalid,suspect,nofile} ;#3: Server.out={0,val,inval} ;#4: Server.belief={none,valid,invalid} ;#5: Server.validfile={true,false} ;0,n,0,n,f0,n,0,n,11f,n,0,n,ff,n,0,n,1719f,n,v,v,f,n,v,v,f25f,v,0,v,13312118v,s,i, i,4 v,s,i, i,f22 f,i,0,i,f23f,i,v,v,260,s,0,n,fv,s,6 0,n,fv,i,0,i,200,s,0,n,2v,i,0,i,f21f,i,0,i,24v,s,0,n,790,v,0,v,58 v,s,v,v,v,s,v,v,fv,v,v,v,0,v,10 0,v,ff,i,v,v,ff,v,0,v,f#1,#2,#3,#4, shows order variables state;15#5Initials values variables shown states.140,v,0,v,f16Figure 16: One admissible models AFS1 model update.148fiCTL Model Update System Modificationsknow Figure 16 presents one possible updated model update AFS1 model. fact many possible admissible models. instance,instead using PU2 operation, could also use PU2 PU3 different combinations produce many admissible models. total number admissiblemodels 64.8. Optimizing Update ResultsSection 7.2, observe often, CTL model update approach may derivemany possible admissible models really need. practice, would expectsolution CTL model update provides concrete information correctunderlying system specification. motivates us improve CTL model updateapproach eliminate unnecessary admissible models narrowupdate results.Consider AFS1 update case again. model described Figure 16 satisfiesrequired property admissible, it, however, retain similar structureoriginal AFS1 model. implies update, significant changesystem behaviour. admissible model may represent desirable correctionoriginal system. One way reduce possibility impose notion maximalreachable states minimal change principle, possible updated modelalso retain many reachable states possible original model.Given Kripke model = (S, R, L) 0 S, and, let = (M, s0 ), says0 reachable state M, path = (S, R, L) form = [s 0 , s1 , ]s0 . RS(M) = RS(M, s0 ) used denote set reachable statesM. Now, propose refined CTL model update principle significantly reducenumber updated models. Let = (S, R, L) CTL Kripke model 0 S.Suppose 0 = (S 0 , R0 , L0 ) (M 0 , s00 ) updated model obtained update(M, s0 ) satisfy CTL formula. specifyRS(M) RS(M0 ) = {s | RS(M) RS(M0 ) L(s) = L0 (s)}.States RS(M) RS(M0 ) common reachable states 0 , called unchanged reachable states. Note state name may reachable twodifferent models different truth assignments defined L L 0 respectively.case, state common reachable state 0 .Definition 7 (Minimal change maximal reachable states) Given CTL Kripkemodel = (S, R, L), = (M, s0 ), s0 S, CTL formula , modelU pdate(M, ) called committed respect update satisfy ,following conditions hold: (1) U pdate(M, ) = 0 = (M 0 , s00 ) admissible; and, (2)model M00 = (M 00 , s000 ) M00 |= RS(M) RS(M0 )RS(M) RS(M00 ).Condition (2) Definition 7 ensures maximal set unchanged reachable statesretained updated model. prove next, amended CTL model updateapproach based Definition 7 significantly increase overall computationalcost.149fiZhang & DingLemma 1 Given CTL Kripke model = (S, R, L), = (M, 0 ), s0 S, CTLformula , two models M0 = (M 0 , s00 ) M00 = (M 00 , s000 ) update (M, s0 )satisfy , checking whether RS(M) RS(M0 ) RS(M) RS(M00 ) achievedpolynomial time.Proof: given = (S, R, L), view directed graph G(M ) = (S, R),set vertices R represents edges graph. Obviously, problem finding reachable states 0 finding reachablevertices vertex s0 graph G(M ), obtained computing spanning treeroot s0 G(M ). well known spanning tree computed polynomialtime (Pettie & Ramachandran, 2002). Therefore, sets RS(M), RS(M 0 ), RS(M00 )obtained polynomial time. Also, RS(M) RS(M0 ) RS(M) RS(M00 )checked polynomial time. 2Theorem 9 Given two CTL Kripke models = (S, R, L) 0 = (S 0 , R0 , L0 ),s0 s00 0 , CTL formula , co-NP-complete decide whether (M 0 , s00 )committed result update (M, 0 ) satisfy .Proof: Since every committed result also admissible one, Theorem 5, hardness holds. membership, need check (1) whether (M 0 , s00 ) admissible; and,(2) updated model 00 exist (M 00 , s000 ) |= RS(M) RS(M0 )RS(M) RS(M00 ). Theorem 5, checking whether (M 0 , s00 ) co-NP. (2),consider complement: updated model 00 exits (M 00 , s000 ) |=RS(M) RS(M0 ) RS(M) RS(M00 ). Lemma 1, concludeproblem NP. Consequently, original problem checking (2) co-NP. 2Section 4, many commonly used CTL formulas, also provide usefulsemantic characterizations simplify process computing committed modelupdate. Here, present one result formula AF, propositionalformula. Given CTL model = (S, R, L) (M, 0 ) 6|= AF (s0 S). recall= [s0 , ] (M, s0 ) valid path AF exists state > 0L(s) |= ; otherwise, called false path AF.Theorem 10 Let = (S, R, L) Kripke model, = (M, 0 ) 6|= AF,s0 propositional formula. Let 0 = U pdate(M, AF) model obtainedfollowing 1 2, M0 committed model. false path = [s 0 , s1 , ]:1. false path 0 sharing common state , PU3 appliedstate (s > s0 ) change ss truth assignment L 0 (s) |=Dif f (L(s), L0 (s)) minimal; otherwise, operation applied sharedstate sj (j > 0) maximum number false paths;2. PU2 applied remove relation element (s 0 , s1 ), s1 also occurs another validpath 0 , 0 = [s0 , s01 , , s0k , s1 , s0k+1 , . . .] exists s0i (1 k)L(s0i ) |= .150fiCTL Model Update System ModificationsProof: first prove Result 1. Consider false path = [s 0 , , si , si+1 , ]. Sincestate satisfy , need (minimally) change one state ss truth assignmentalong path L0 (s) satisfies (i.e., apply PU3 once). falsepath shares states , apply PU3 state path .case, one reachable state original model respect path changedsatisfy . Thus, updated model retains maximal set unchanged states.Suppose false paths sharing common state . Without lossgenerality, let 0 = [s0 , , s0i1 , si , s0i+1 , ] false path sharing common state. applying PU3 state rather necessarily retainmaximal set unchanged reachable states, change statesi could made path 0 order make 0 valid. Since si sharing statetwo paths 0 , implies updating two states PU3 retain maximalset unchanged reachable states comparing change one state makes0 valid.consider general case. order retain maximal set unchangedreachable states original model, consider states alsofalse paths. case, need apply PU3 operation one state jshared maximal number false paths. way, changing j satisfyalso minimally change false paths valid time. Consequently,retain maximal set unchanged reachable states original model.prove Result 2. Let = [s0 , s1 , s2 , ] false path. Accordingcondition, valid path 0 form 0 = [s0 , s01 , , s0k , s1 , s0k+1 , ],s0i 0 (1 k), s0i |= . Note third path, formed 0 ,00 = [s0 , s01 , , s0k , s1 , s2 , ] also valid. Applying PU2 relation element (s 0 , s1 )simply eliminate false path model. condition, easy seeoperation actually affect state reachability original modelvalid path 00 connect s1 states path still reachable 0path 00 . described Figure 17 follows. 2optimization function UpdateAF described Section 6.1, Theorem 10 proposesefficient way update CTL model satisfy formula AF guarantee updatemodel retains maximal set reachable states original model. Compared(a) function UpdateAF , updates state path, case 1 Theorem 10updates state shared maximum number false paths minimize changesupdate protect unchanged reachable states. Compared (b) function UpdateAF,could disconnect false path make disconnected part unreachable, case 2Theorem 10 disconnects false path accompanied alternate path ensuredisconnected path still reachable via alternate path. theorem illustratesprinciple optimization characterizations CTL formulas.general, committed models computed revising previous CTL modelupdate algorithms particular emphasis identifying maximal reachable states.example, using improved approach, obtain committed model AFS1 modelupdate (as illustrated Figure 18), rules model presented Figure 16.shown using improved approach AFS1 model update, numbertotal possible updated models reduced 64 36.151fiZhang & Dings0sks1sk+1Figure 17: s1 occurs another valid path {s0 , s01 , , s0k , s1 , s0k+1 , ].#1: Client.out={0,fetch,validate} ;#2: Client.belief={valid,invalid,suspect,nofile} ;#3: Server.out={0,val,inval} ;#4: Server.belief={none,valid,invalid} ;#5: Server.validfile={true,false} ;1117190,n,0,n,0,n,0,n,ff,n,0,n,f,n,0,n,ff,n,v,v,f,v,v,v,f25f,v,0,v,130,s,0,n,fv,s,6 0,n,3121180,s,0,n,4 v,s,i, i,fv,i,0,i,22 f,i,0,i,f2023f,i,v,v,26v,s,0,n,fv,s,i, i,2v,i,0,i,f21f,i,0,i,24790,v,0,v,fv,v,0,v,v,v,10 0,v,ff,i,v,v,f0,v,0,v,fFigure 18: One committed models AFS1.15258 v,s,v,n,v,s,v,v,f,v,0,v,f#1,#2,#3,#4, shows order variables state;15#5Initials values variables shown states.1416fiCTL Model Update System Modifications9. Concluding Remarkspaper, present formal approach update CTL models. specifyingprimitive operations CTL Kripke models, defined minimal change criteriaCTL model update. semantic computational properties approach alsoinvestigated detail. Based formalization, developed CTL modelupdate algorithm implemented system prototype perform CTL model update.Two case studies used demonstrate important applications work.number issues merit investigations. current researchfocuses following two tasks:- Partial CTL model update: current approach, model update performedcomplete Kripke model. practice, may feasible system complexlarge number states transition relations. One possible method handleproblem employ model checker extract partial useful informationuse model update input. could counterexample partial Kripkemodel containing components repaired (Buccafurri, Eiter, Gottlob, &Leone, 2001; Clarke, Jha, Lu, & Veith, 2002; Groce & Visser, 2003; Rustan, Leino,Millstein, & Saxe, 2005). way, update directly performedcounterexample partial model generate possible corrections. possibledevelop unified prototype integrating model checking (e.g., SMV) model update.- Combining maximal structure similarity minimal change: demonstratedSection 8, principle minimal change maximal reachable states may significantly reduce number updated models. However, evidentmaximal reachable states principle applied minimal change (see Definition7). may improve principle defining unified analogue integratesminimal change maximal structural similarity level. mayrestrict number final updated models. unified principle may definedbased notion bisimulation Kripke models (Clarke, Grumberg, Jha, Liu,& Veith, 2003). instance, two states preserved updatepath two states original model, new definitionpreserve path updated model well, updated model retainsmaximal structural similarity respect original. Consider committedmodel described Figure 18: since path state 21 state 26original model (i.e., Figure 15), would require retention path 2126 updated model. Accordingly, model displayed Figure 18ruled final updated model.Acknowledgmentsresearch supported part Australian Research Council Discovery Grant(DP0559592). authors thank three anonymous reviewers many valuable comments earlier version paper.153fiZhang & DingReferencesAmla, N., Du, X., Kuehlmann, A., Kurshan, R., & McMillan, K. (2005). analysissat-based model checking techniques industrial environment. ProceedingsCorrect Hardware Design Verification Methods - 13th IFIP WG 10.5 AdvancedResearch Working Conference (CHARME 2005), pp. 254268.Baral, C., & Zhang, Y. (2005). Knowledge updates: semantics complexity issues. Artificial Intelligence, 164, 209243.Berard, B., Bidoit, M., Finkel, A., Laroussinie, F., Petit, A., Petrucci, L., & Schnoebelen,P. (2001). System Software Verification: Model-Checking Techniques Tools.Springer-Verlag Berlin Heidelberg.Boyer, M., & Sighireanu, M. (2003). Synthesis verification constraints pgmprotocol. Proceedings 12th International Symposium Formal MethodsEurope (FME03), pp. 264281. Springer Verlag.Buccafurri, F., Eiter, T., Gottlob, G., & Leone, N. (1999). Enhancing model checkingverification ai techniques. Artificial Intelligence, 112, 57104.Buccafurri, F., Eiter, T., Gottlob, G., & Leone, N. (2001). actl formulas linearcounterexamples. Journal Computer System Sciences, 62, 463515.Chauhan, P., Clarke, E., Kukula, J., Sapra, S., Veith, H., & Wang, D. (2002). Automatedabstraction refinement model checking large state spaces using sat based conflictanalysis. Proceedings Formal Methods Computer Aided Design (FMCAD02),pp. 3351.Cimatti, A., Clarke, E., Giunchiglia, F., & Roveri, M. (1999). Nusmv: new symbolicmodel verifier. Proceedings 11th International Conference ComputerAided Verification, pp. 495499.Clarke, E., Grumberg, O., Jha, S., Liu, Y., & Veith, H. (2003). Counterexample-guidedabstraction refinement symbolic model checking. Journal ACM, 50, 752794.Clarke, E., Grumberg, O., & Peled, D. (1999). Model Checking. MIT Press.Clarke, E., Jha, S., Lu, Y., & Veith, H. (2002). Tree-like counterexamples model checking.Proceedings 17th Annual IEEE Symposium Logic Computer Science(LICS02), pp. 1929.Dennis, L., Monroy, R., & Nogueira, P. (2006). Proof-directed debugging repair.Proceedings 7th Symposium Trends Functional Programming, pp. 131140.Ding, Y., & Zhang, Y. (2005). logic approach ltl system modification. Proceedings15th International Symposium Methodologies Intelligent Systems (ISMIS2005), pp. 436444. Springer.Ding, Y., & Zhang, Y. (2006). Ctl model update: Semantics, computations implementation. Proceedings 17th European Conference Artificial Intelligence (ECAI2006), pp. 362366. IOS Press.Eiter, T., & Gottlob, G. (1992). complexity propositional knowledge base revision,updates, counterfactuals. Artificial Intelligence, 57, 227270.154fiCTL Model Update System ModificationsGammie, P., & van der Meyden, R. (2004). Mck-model checking logic knowledge.Proceedings 16th International Conference Computer Aided Verification(CAV-2004), pp. 479483.Gardenfors, P. (1988). Knowledge Flux: Modeling Dynamics Epistemic States.MIT Press.Groce, A., & Visser, W. (2003). went wrong: Explaining counterexamples. Proceedings SPIN Workshop Model Checking Software, pp. 121135.Harris, H., & Ryan, M. (2002). Feature integration operation theory change.Proceedings 15th European Conference Artificial Intelligence (ECAI-2002),pp. 546550.Harris, H., & Ryan, M. (2003). Theoretical foundations updating systems. Proceedings18th IEEE International Conference Automated Software Engineering, pp.291298.Herzig, A., & Rifi, O. (1999). Propositional belief base update minimal change. ArtificialIntelligence, 115, 107138.Holzmann, C. (2003). SPIN Model Checker: Primer Reference Manual. AddisonWesley Professional.Huth, M., & Ryan, M. (2004). Logic Computer Science: Modelling ReasoningSystems. 2nd edition, Cambridge University Press.Katsuno, H., & Mendelzon, A. (1991). difference updating knowledgebase revising it. Proceedings International Conference PrinciplesKnowledge Representation Reasoning (KR91), pp. 387394.McMillan, K., & Amla, N. (2002). logic implicit explicit belief. Automatic Abstraction without Counterexamples. Cadence Berkeley Labs, Cadence Design Systems.Pettie, S., & Ramachandran, V. (2002). optimal minimum spanning tree algorithm.Journal ACM, 49, 1634.Rustan, K., Leino, M., Millstein, T., & Saxe, J. (2005). Generating error tracesverification-condition counterexamples. Science Computer Programming, 55, 209226.Stumptner, M., & Wotawa, F. (1996). model-based approach software debugging.Proceedings 7th International Workshop Principles Diagnosis.Wing, J., & Vaziri-Farahani, M. (1995). case study model checking software.Proceedings 3rd ACM SIGSOFT Symposium Foundations SoftwareEngineering.Winslett, M. (1988). Reasoning action using possible models approach. Proceedings AAAI-88, pp. 8993.Winslett, M. (1990). Updating Logical Databases. Cambridge University Press.155fiJournal Artificial Intelligence Research 31 (2008) 33-82Submitted 02/07; published 01/08Planning Durative Actions Stochastic DomainsMausamDaniel S. WeldMAUSAM @ CS . WASHINGTON . EDUWELD @ CS . WASHINGTON . EDUDept Computer Science EngineeringBox 352350, University WashingtonSeattle, WA 98195 USAAbstractProbabilistic planning problems typically modeled Markov Decision Process (MDP).MDPs, otherwise expressive model, allow sequential, non-durative actions.poses severe restrictions modeling solving real world planning problem. extendMDP model incorporate 1) simultaneous action execution, 2) durative actions, 3) stochastic durations. develop several algorithms combat computational explosion introducedfeatures. key theoretical ideas used building algorithms modeling complex problem MDP extended state/action space, pruning irrelevant actions, samplingrelevant actions, using informed heuristics guide search, hybridizing different plannersachieve benefits both, approximating problem replanning. empirical evaluationilluminates different merits using various algorithms, viz., optimality, empirical closenessoptimality, theoretical error bounds, speed.1. IntroductionRecent progress achieved planning researchers yielded new algorithms relax, individually, many classical assumptions. example, successful temporal planners like SGPlan,SAPA, etc. (Chen, Wah, & Hsu, 2006; & Kambhampati, 2003) able model actions taketime, probabilistic planners like GPT, LAO*, SPUDD, etc. (Bonet & Geffner, 2005; Hansen &Zilberstein, 2001; Hoey, St-Aubin, Hu, & Boutilier, 1999) deal actions probabilisticoutcomes, etc. However, order apply automated planning many real-world domains musteliminate larger groups assumptions concert. example, NASA researchers noteoptimal control NASA Mars rover requires reasoning uncertain, concurrent, durativeactions mixture discrete metric fluents (Bresina, Dearden, Meuleau, Smith, & Washington, 2002). todays planners handle large problems deterministic concurrentdurative actions, MDPs provide clear framework non-concurrent durative actionsface uncertainty, researchers considered concurrent, uncertain, durative actionsfocus paper.example consider NASA Mars rovers, Spirit Oppurtunity. goalgathering data different locations various instruments (color infrared cameras, microscopic imager, Mossbauer spectrometers etc.) transmitting data back Earth. Concurrentactions essential since instruments turned on, warmed calibrated, rovermoving, using instruments transmitting data. Similarly, uncertainty must explicitlyconfronted rovers movement, arm control actions cannot accurately predicted.Furthermore, actions, e.g., moving locations setting experiments, taketime. fact, temporal durations uncertain rover might lose wayc2008AI Access Foundation. rights reserved.fiM AUSAM & W ELDtake long time reach another location, etc. able solve planning problems encountered rover, planning framework needs explicitly model domain constructsconcurrency, actions uncertain outcomes uncertain durations.paper present unified formalism models domain features together.Concurrent Markov Decision Processes (CoMDPs) extend MDPs allowing multiple actions perdecision epoch. use CoMDPs base model planning problems involving concurrency.Problems durative actions, concurrent probabilistic temporal planning (CPTP), formulatedCoMDPs extended state space. formulation also able incorporate uncertaintydurations form probabilistic distributions.Solving planning problems poses several computational challenges: concurrency, extended durations, uncertainty durations lead explosive growth state space,action space branching factor. develop two techniques, Pruned RTDP Sampled RTDPaddress blowup concurrency. also develop DUR family algorithms handlestochastic durations. algorithms explore different points running time vs. solutionquality tradeoff. different algorithms propose several speedup mechanisms 1) pruning provably sub-optimal actions Bellman backup, 2) intelligent sampling actionspace, 3) admissible inadmissible heuristics computed solving non-concurrent problems, 4)hybridizing two planners obtain hybridized planner finds good quality solution intermediate running times, 5) approximating stochastic durations mean values replanning, 6)exploiting structure multi-modal duration distributions achieve higher quality approximations.rest paper organized follows: section 2 discuss fundamentalsMDPs real-time dynamic programming (RTDP) solution method. Section 3 describemodel Concurrent MDPs. Section 4 investigates theoretical properties temporalproblems. Section 5 explains formulation CPTP problem deterministic durations.algorithms extended case stochastic durations Section 6. section supportedempirical evaluation techniques presented section. Section 7 surveyrelated work area. conclude future directions research Sections 8 9.2. BackgroundPlanning problems probabilistic uncertainty often modeled using Markov Decision Processes (MDPs). Different research communities looked slightly different formulationsMDPs. versions typically differ objective functions (maximizing reward vs. minimizingcost), horizons (finite, infinite, indefinite) action representations (DBN vs. parametrized actionschemata). formulations similar nature, algorithms solvethem. Though, methods proposed paper applicable variants models,clarity explanation assume particular formulation, known stochastic shortest pathproblem (Bertsekas, 1995).define Markov decision process (M) tuple hS, A, Ap, Pr, C, G, s0finite set discrete states. use factored MDPs, i.e., compactly representedterms set state variables.finite set actions.34fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSState variables : x1 , x2 , x3 , x4 , p12ActionPreconditionEffecttoggle-x1p12x1 x1toggle-x2p12x2 x2toggle-x3truex3 x3changetoggle-x4truex4 x4changetoggle-p12truep12 p12Goal : x1 = 1, x2 = 1, x3 = 1, x4 = 1Probability110.90.10.90.11Figure 1: Probabilistic STRIPS definition simple MDP potential parallelismAp defines applicability function. Ap : P(A), denotes set actionsapplied given state (P represents power set).Pr : [0, 1] transition function. write Pr(s0 |s, a) denoteprobability arriving state s0 executing action state s.C : <+ cost model. write C(s, a, s0 ) denote cost incurredstate s0 reached executing action state s.G set absorbing goal states, i.e., process ends one statesreached.s0 start state.assume full observability, i.e., execution system complete access new stateaction performed. seek find optimal, stationary policy i.e., function: minimizes expected cost (over indefinite horizon) incurred reach goalstate. Note cost function, J: <, mapping states expected cost reaching goalstate defines policy follows:J (s) = argminXPr(s0 |s, a) C(s, a, s0 ) + J(s0 )(1)aAp(s) s0optimal policy derives optimal cost function, J , satisfies following pairBellman equations.J (s) = 0, G elseJ (s) = minaAp(s)XPr(s0 |s, a) C(s, a, s0 ) + J (s0 )(2)s0example, Figure 1 defines simple MDP four state variables (x1 , . . . , x4 ) needset using toggle actions. actions, e.g., toggle-x3 probabilistic.Various algorithms developed solve MDPs. Value iteration dynamic programming approach optimal cost function (the solution equations 2) calculatedlimit series approximations, considering increasingly long action sequences. Jn (s)35fiM AUSAM & W ELDcost state iteration n, cost state next iteration calculatedprocess called Bellman backup follows:Jn+1 (s) = minaAp(s)XPr(s0 |s, a) C(s, a, s0 ) + Jn (s0 )(3)s0Value iteration terminates S, |Jn (s) Jn1 (s)| , termination guaranteed > 0. Furthermore, limit, sequence {Ji } guaranteed convergeoptimal cost function, J , regardless initial values long goal reached every reachable state non-zero probability. Unfortunately, value iteration tends quite slow,since explicitly updates every state, |S| exponential number domain features. Oneoptimization restricts search part state space reachable initial state s0 . Two algorithms exploiting reachability analysis LAO* (Hansen & Zilberstein, 2001) focus:RTDP (Barto, Bradtke, & Singh, 1995).RTDP, conceptually, lazy version value iteration states get updated proportion frequency visited repeated executions greedy policy.RTDP trial path starting s0 , following greedy policy updating costsstates visited using Bellman backups; trial ends goal reached numberupdates exceeds threshold. RTDP repeats trials convergence. Note common statesupdated frequently, RTDP wastes time states unreachable, given currentpolicy. RTDPs strength ability quickly produce relatively good policy; however, completeconvergence (at every relevant state) slow less likely (but potentially important) states getupdated infrequently. Furthermore, RTDP guaranteed terminate. Labeled RTDP (LRTDP)fixes problems clever labeling scheme focuses attention states valuefunction yet converged (Bonet & Geffner, 2003). Labeled RTDP guaranteed terminate,guaranteed converge -approximation optimal cost function (for states reachable using optimal policy) initial cost function admissible, costs (C) positivegoal reachable reachable states non-zero probability.MDPs powerful framework model stochastic planning domains. However, MDPs maketwo unrealistic assumptions 1) actions need executed sequentially, 2) actionsinstantaneous. Unfortunately, many real-world domains assumptionsunrealistic. example, concurrent actions essential Mars rover, since instrumentsturned on, warmed calibrated rover moving, using instrumentstransmitting data. Moreover, action durations non-zero stochastic rover mightlose way navigating may take long time reach destination; may make multipleattempts finding accurate arm placement. paper successively relax twoassumptions build models algorithms scale spite additional complexitiesimposed general models.3. Concurrent Markov Decision Processesdefine new model, Concurrent MDP (CoMDP), allows multiple actions executedparallel. model different semi-MDPs generalized state semi-MDPs (Younes& Simmons, 2004b) incorporate action durations explicitly. CoMDPs focusadding concurrency MDP framework. input CoMDP slightly differentMDP hS, A, Apk , Prk , Ck , G, s0 i. new applicability function, probability model cost36fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS(Apk , Prk Ck respectively) encode distinction allowing sequential executionssingle actions versus simultaneous executions sets actions.3.1 Modelset states (S), set actions (A), goals (G) start state (s0 ) follow input MDP.difference lies fact instead executing one action time, may executemultiple them. Let us define action combination, A, set one actionsexecuted parallel. action combination new unit operator available agent,CoMDP takes following new inputsApk defines new applicability function. Apk : P(P(A)), denotes set actioncombinations applied given state.Prk : P(A) [0, 1] transition function. write Prk (s0 |s, A) denoteprobability arriving state s0 executing action combination state s.Ck : P(A) <+ cost model. write Ck (s, A, s0 ) denote cost incurredstate s0 reached executing action combination state s.essence, CoMDP takes action combination unit operator instead single action.approach convert CoMDP equivalent MDP (Mk ) specifiedtuple hS, P(A), Apk , Prk , Ck , G, s0 solve using known MDP algorithms.3.2 Case Study: CoMDP Probabilistic STRIPSgeneral CoMDP could require exponentially larger input MDP, since transition model, cost model applicability function defined terms action combinationsopposed actions. compact input representation general CoMDP interesting, openresearch question future. work, consider special class compact CoMDPone defined naturally via domain description similar probabilistic STRIPSrepresentation MDPs (Boutilier, Dean, & Hanks, 1999).Given domain encoded probabilistic STRIPS compute safe set co-executableactions. safe semantics, probabilistic dynamics gets defined consistent waydescribe below.3.2.1 PPLICABILITY F UNCTIONfirst discuss compute sets actions executed parallel sinceactions may conflict other. adopt classical planning notion mutual exclusion (Blum & Furst, 1997) apply factored action representation probabilistic STRIPS.Two distinct actions mutex (may executed concurrently) state one following occurs:1. inconsistent preconditions2. outcome one action conflicts outcome3. precondition one action conflicts (possibly probabilistic) effect other.37fiM AUSAM & W ELD4. effect one action possibly modifies feature upon another actions transitionfunction conditioned upon.Additionally, action never mutex itself. essence, non-mutex actions interact effects executing sequence a1 ; a2 equals a2 ; a1 semanticsparallel executions clear.Example: Continuing Figure 1, toggle-x1 , toggle-x3 toggle-x4 execute paralleltoggle-x1 toggle-x2 mutex conflicting preconditions. Similarly, toggle-x1toggle-p12 mutex effect toggle-p12 interferes precondition toggle-x1 .toggle-x4 outcomes depended toggle-x1 would mutex too, due point 4 above.example, toggle-x4 toggle-x1 mutex effect toggle-x4 follows: togglex1 probability x4 x4 0.9 else 0.1. 2applicability function defined set action-combinations, A, actionindependently applicable actions pairwise non-mutex other.Note pairwise concurrency sufficient ensure problem-free concurrency multipleactions A. Formally Apk defined terms original definition Ap follows:Apk (s) = {A A|a, a0 A, a, a0 Ap(s) mutex(a, a0 )}(4)3.2.2 RANSITION F UNCTIONLet = {a1 , a2 , . . . , ak } action combination applicable s. Since none actionsmutex, transition function may calculated choosing arbitrary order applyfollows:Prk (s0 |s, A) =X...XPr(s1 |s, a1 )Pr(s2 |s1 , a2 ) . . . Pr(s0 |sk1 , ak )(5)s1 ,s2 ,...skdefine applicability function transition function allowing consistent set actions executable concurrently, alternative definitions possible.instance, one might willing allow executing two actions together probabilityconflict small. conflict may defined two actions asserting contradictory effectsone negating precondition other. case, new state called failure could created system transitions state case conflict. transition maycomputed reflect low probability transition failure state.Although impose model conflict-free, techniques dont actually depend assumption explicitly extend general CoMDPs.3.2.3 C OST MODELmake small change probabilistic STRIPS representation. Instead defining singlecost (C) action, define additively sum resource time components follows:Let durative cost, i.e., cost due time taken complete action.Let r resource cost, i.e., cost resources used action.38fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSAssuming additivity think cost action C(s, a, s0 ) = t(s, a, s0 ) + r(s, a, s0 ),sum time resource usage. Hence, cost model combination actions termscomponents may defined as:Ck (s, {a1 , a2 , ..., ak }, s0 ) =kXr(s, ai , s0 ) + max {t(s, ai , s0 )}i=1..ki=1(6)example, Mars rover might incur lower cost preheats instrument changinglocations executes actions sequentially, total time reducedenergy consumed change.3.3 Solving CoMDP MDP Algorithmstaken concurrent MDP allowed concurrency actions formulated equivalent MDP, Mk , extended action space. rest paper use term CoMDPalso refer equivalent MDP Mk .3.3.1 B ELLMAN EQUATIONSextend Equations 2 set equations representing solution CoMDP:Jk (s) = 0, G elseJk (s) = minAApk (s)XnPrk (s0 |s, A) Ck (s, A, s0 ) + Jk (s0 )(7)s0equations traditional MDP, except instead considering singleactions backup state, need consider applicable action combinations. Thus,small change must made traditional algorithms (e.g., value iteration, LAO*, Labeled RTDP).However, since number action combinations worst-case exponential |A|, efficientlysolving CoMDP requires new techniques. Unfortunately, structure exploit easily,since optimal action state classical MDP solution may even appear optimalaction combination associated concurrent MDP.Theorem 1 actions optimal combination CoMDP (Mk ) may individually suboptimal MDP M.Proof: domain Figure 1 let us additional action toggle-x34 toggles x3x4 probability 0.5 toggles exactly one x3 x4 probability 0.25 each. Letactions take one time unit each, therefore cost action combination one well.Let start state x1 = 1, x2 = 1, x3 = 0, x4 = 0 p12 = 1. MDP optimalaction start state toggle-x34 . However, CoMDP Mk optimal combination{toggle-x3 , toggle-x4 }. 23.4 Pruned Bellman BackupsRecall trial, Labeled RTDP performs Bellman backups order calculate costsapplicable actions (or case, action combinations) chooses best action (combination); describe two pruning techniques reduce number backups computed.39fiM AUSAM & W ELDLet Qk (s, A) expected cost incurred executing action combination statefollowing greedy policy, i.e.Qkn (s, A) =XnPrk (s0 |s, A) Ck (s, A, s0 ) + Jkn1 (s0 )(8)s0Bellman update thus rewritten as:Jkn (s) =minAApk (s)Qkn (s, A)(9)3.4.1 C OMBO -S KIPPINGSince number applicable action combinations exponential, would like prunesuboptimal combinations. following theorem imposes lower bound Qk (s, A) termscosts Qk -values single actions. theorem costs actions may dependaction starting ending state, i.e., states s, s0 C(s, a, s0 ) = C(a).Theorem 2 Let = {a1 , a2 , . . . , ak } action combination applicable state s.CoMDP probabilistic STRIPS, costs dependent actions Qkn valuesmonotonically non-decreasingQk (s, A) max Qk (s, {ai }) + Ck (A)i=1..kkX!Ck ({ai })i=1Proof:Qkn (s, A) = Ck (A) +XPrk (s0 |s, A)Jkn1 (s0 )(using Eqn. 8)s0XPrk (s0 |s, A)Jkn1 (s0 ) = Qkn (s, A) Ck (A)(10)s0Qkn (s, {a1 }) = Ck ({a1 }) +XPr(s00 |s, a1 )Jkn1 (s00 )s00"Ck ({a1 }) +X#00Pr(s |s, a1 ) Ck ({a2 }) +s00X00000000Pr(s |s , a2 )Jkn2 (s )s000(using Eqns. 8 9)= Ck ({a1 }) + Ck ({a2 }) +X000000Prk (s |s, {a1 , a2 })Jkn2 (s )s000=kXi=1kXCk ({ai }) +XPrk (s0 |s, A)Jknk (s0 )(repeating actions A)s0Ck ({ai }) + [Qknk+1 (s, A) Ck (A)]i=1Replacing n n + k 140(using Eqn. 10)fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSQkn (s, A) Qkn+k1 (s, {a1 }) + Ck (A)kX!Ck ({ai })i=1Qkn (s, {a1 }) + Ck (A)kX!Ck ({ai })(monotonicity Qkn )i=1max Qkn (s, {ai }) + Ck (A)i=1..kkX!Ck ({ai })i=12proof assumes equation 5 probabilistic STRIPS. following corollaryused prune suboptimal action combinations:Corollary 3 Let dJkn (s)e upper bound Jkn (s).dJkn (s)e < max Qkn (s, {ai }) + Ck (A)i=1..kkX!Ck ({ai })i=1cannot optimal state iteration.Proof: Let = {a1 , a2 , . . . , ak } optimal combination state iteration n. Then,dJkn (s)e Jkn (s)Jkn (s) = Qkn (s, )Combining Theorem 2dJkn (s)e maxi=1..k Qkn (s, {ai }) +Ck (An )kX!Ck ({ai }) 2i=1Corollary 3 justifies pruning rule, combo-skipping, preserves optimality iterationalgorithm maintains cost function monotonicity. powerful Bellman-backupbased algorithms preserve monotonicity started admissible cost function. applycombo-skipping, one must compute Qk (s, {a}) values single actions applicables. calculate dJkn (s)e one may use optimal combination state previous iteration(Aopt ) compute Qkn (s, Aopt ). value gives upper bound value Jkn (s).Example: Consider Figure 1. Let single action incur unit cost, let cost action combination be: Ck (A) = 0.5 + 0.5|A|. Let state = (1,1,0,0,1) represent ordered values x1 = 1, x2 =1, x3 = 0, x4 = 0, p12 = 1. Suppose, nth iteration, cost function assigns values:Jkn (s) = 1, Jkn (s1 =(1,0,0,0,1)) = 2, Jkn (s2 =(1,1,1,0,1)) = 1, Jkn (s3 =(1,1,0,1,1)) = 1. Let Aoptstate {toggle-x3 , toggle-x4 }. Now, Qkn+1 (s, {toggle-x2 }) = Ck ({toggle-x2 }) + Jkn (s1 ) = 3Qkn+1 (s, Aopt ) = Ck (Aopt ) + 0.810 + 0.09Jkn (s2 ) + 0.09Jkn (s3 ) + 0.01Jkn (s) = 1.69.apply Corollary 3 skip combination {toggle-x2 , toggle-x3 } iteration, sinceusing toggle-x2 a1 , dJkn+1 (s)e = Qkn+1 (s, Aopt ) = 1.69 3 + 1.5 - 2 = 2.5. 2Experiments show combo-skipping yields considerable savings. Unfortunately, comboskipping weakness prunes combination single iteration. contrast,second rule, combo-elimination, prunes irrelevant combinations altogether.41fiM AUSAM & W ELD3.4.2 C OMBO -E LIMINATIONadapt action elimination theorem traditional MDPs (Bertsekas, 1995) prove similartheorem CoMDPs.Theorem 4 Let action combination applicable state s. Let bQk (s, A)c denotelower bound Qk (s, A). bQk (s, A)c > dJk (s)e never optimal combinationstate s.Proof: CoMDP MDP new action space, original proof MDPs (Bertsekas,1995) holds replacing action action combination. 2order apply theorem pruning, one must able evaluate upper lowerbounds. using admissible cost function starting RTDP search (or value iteration,LAO* etc.), current cost Jkn (s) guaranteed lower bound optimal cost; thus,Qkn (s, A) also lower bound Qk (s, A). Thus, easy compute left hand sideinequality. calculate upper bound optimal Jk (s), one may solve MDP M,i.e., traditional MDP forbids concurrency. much faster solving CoMDP,yields upper bound cost, forbidding concurrency restricts policy usestrict subset legal action combinations. Notice combo-elimination used generalMDPs restricted CoMDPs probabilistic STRIPS.Example: Continuing previous example, let A={toggle-x2 } Qkn+1 (s, A) = Ck (A) +Jkn (s1 ) = 3 dJk (s)e = 2.222 (from solving MDP M). 3 > 2.222, eliminatedstate remaining iterations. 2Used fashion, combo-elimination requires additional overhead optimally solvingsingle-action MDP M. Since algorithms like RTDP exploit state-space reachability limitcomputation relevant states, computation incrementally, new states visitedalgorithm.Combo-elimination also requires computation current value Qk (s, A) (for lowerbound Qk (s, A)); differs combo-skipping avoids computation. However,combo-elimination prunes combination, never needs reconsidered. Thus,tradeoff: one perform expensive computation, hoping long-term pruning, trycheaper pruning rule fewer benefits? Since Q-value computation costly step, adoptfollowing heuristic: First, try combo-skipping; fails prune combination, attemptcombo-elimination; succeeds, never consider again. also tried implementingheuristics, as: 1) combination skipped repeatedly, try prune altogether combo-elimination. 2) every state, try combo-elimination probability p. Neitheralternative performed significantly better, kept original (lower overhead) heuristic.Since combo-skipping change step labeled RTDP combo-elimination removes provably sub-optimal combinations, pruned labeled RTDP maintains convergence, termination, optimality efficiency, used admissible heuristic.3.5 Sampled Bellman BackupsSince fundamental challenge posed CoMDPs explosion action combinations, sampling promising method reduce number Bellman backups required per state.describe variant RTDP, called sampled RTDP, performs backups random set42fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSaction combinations1 , choosing distribution favors combinations likelyoptimal. generate distribution by:1. using combinations previously discovered low Qk -values (recorded memoizing best combinations per state, iteration)2. calculating Qk -values applicable single actions (using current cost function)biasing sampling combinations choose ones contain actions lowQk -values.Algorithm 1 Sampled Bellman Backup(state, m)1:2:3:4:5:6:7:8:9:10:11:Function 2 SampleComb(state, i, l)1:2:3:4:5:6:7:8:9:10:11:12:13:14:15://returns best combination foundlist l = //a list applicable actions valuesactioncompute Qk (state, {action})insert ha, 1/Qk (state, {action})i l[1..m]newcomb = SampleComb(state, i, l);compute Qk (state, newcomb)clear memoizedlist[state]compute Qmin minimum Qk values computed line 7store combinations Qk (state, A) = Qmin memoizedlist[state]return first entry memoizedlist[state]//returns ith combination sampled backupsize(memoizedlist[state])return ith entry memoizedlist[state] //return combination memoized previous iterationnewcomb =repeatrandomly sample action l proportional valueinsert newcombremove actions mutex ll emptydone = trueelse |newcomb| == 1done = false //sample least 2 actions per combinationelse|newcomb|done = true prob. |newcomb|+1donereturn newcombapproach exposes exploration / exploitation trade-off. Exploration, here, refers testing wide range action combinations improve understanding relative merit. Exploitation, hand, advocates performing backups combinations previouslyshown best. manage tradeoff carefully maintaining distributioncombinations. First, memoize best combinations per state; always backed-up1. similar action sampling approach also used context space shuttle scheduling reduce numberactions considered value function computation (Zhang & Dietterich, 1995).43fiM AUSAM & W ELDBellman update. combinations constructed incremental probabilistic process,builds combination first randomly choosing initial action (weighted individual Qk -value), deciding whether add non-mutex action stop growing combination.many implementations possible high level idea. tried severalfound results similar them. Algorithm 1 describes implementation usedexperiments. algorithm takes state total number combinations inputreturns best combination obtained far. also memoizes best combinationsstate memoizedlist. Function 2 helper function returns ith combination eitherone best combinations memoized previous iteration new sampled combination.Also notice line 10 Function 2. forces sampled combinations least size 2, sinceindividual actions already backed (line 3 Algo 1).3.5.1 ERMINATION PTIMALITYSince system consider every possible action combination, sampled RTDP guaranteed choose best combination execute state. result, even startedadmissible heuristic, algorithm may assign Jkn (s) cost greater optimal Jk (s)i.e., Jkn (s) values longer admissible. better combination chosen subsequentiteration, Jkn+1 (s) might set lower value Jkn (s), thus sampled RTDP monotonic.unfortunate, since admissibility monotonicity important properties required termination2 optimality labeled RTDP; indeed, sampled RTDP loses important theoreticalproperties. good news extremely useful practice. experiments, sampledRTDP usually terminates quickly, returns costs extremely close optimal.3.5.2 MPROVING OLUTION Q UALITYinvestigated several heuristics order improve quality solutions foundsampled RTDP. heuristics compensate errors due partial search lack admissibility.Heuristic 1: Whenever sampled RTDP asserts convergence state, immediatelylabel converged (which would preclude exploration (Bonet & Geffner, 2003));instead first run complete backup phase, using admissible combinations, ruleeasy-to-detect inconsistencies.Heuristic 2: Run sampled RTDP completion, use cost function produces, J (),initial heuristic estimate, J0 (), subsequent run pruned RTDP. Usually,heuristic, though inadmissible, highly informative. Hence, pruned RTDP terminates quitequickly.Heuristic 3: Run sampled RTDP pruned RTDP, Heuristic 2, except insteadusing J () cost function directly initial estimate, scale linearly downward i.e.,use J0 () := cJ () constant c (0, 1). guarantees hopelies admissible side optimal. experience often casec = 0.9, run pruned RTDP yields optimal policy quickly.2. ensure termination implemented policy: number trials exceeds threshold, force monotonicitycost function. achieve termination reduce quality solution.44fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSExperiments showed Heuristic 1 returns cost function close optimal. AddingHeuristic 2 improves value moderately, combination Heuristics 1 3 returnsoptimal solution experiments.3.6 Experiments: Concurrent MDPConcurrent MDP fundamental formulation, modeling concurrent actions general planningdomain. first compare various techniques solve CoMDPs, viz., pruned sampled RTDP.following sections use techniques model problems durative actions.tested algorithms problems three domains. first domain probabilisticvariant NASA Rover domain 2002 AIPS Planning Competition (Long & Fox, 2003),multiple objects photographed various rocks tested resultingdata communicated back base station. Cameras need focused, arms needpositioned usage. Since rover multiple arms multiple cameras, domainhighly parallel. cost function includes resource time components, executing multiple actions parallel cheaper executing sequentially. generated problems20-30 state variables 81,000 reachable states average number applicablecombinations per state, Avg(Ap(s)), measures amount concurrency problem,2735.also tested probabilistic version machineshop domain multiple subtasks (e.g.,roll, shape, paint, polish etc.), need performed different objects using differentmachines. Machines perform parallel, capable every task. testedproblems 26-28 state variables around 32,000 reachable states. Avg(Ap(s)) ranged170 2640 various problems.Finally, tested artificial domain similar one shown Figure 1 muchcomplex. domain, Boolean variables need toggled; however, toggling probabilistic nature. Moreover, certain pairs actions conflicting preconditions thus,varying number mutex actions may control domains degree parallelism.problems domain 19 state variables 32,000 reachable states, Avg(Ap(s))1024 12287.used Labeled RTDP, implemented GPT (Bonet & Geffner, 2005), base MDPsolver. implemented C++. implemented3 various algorithms, unpruned RTDP (U RTDP), pruned RTDP using combo skipping (Ps -RTDP), pruned RTDP using comboskipping combo elimination (Pse -RTDP), sampled RTDP using Heuristic 1 (S-RTDP) sampled RTDP using Heuristics 1 3, value functions scaled 0.9 (S3 -RTDP). testedalgorithms number problem instantiations three domains, generatedvarying number objects, degrees parallelism, distances goal. experimentsperformed 2.8 GHz Pentium processor 2 GB RAM.observe (Figure 2(a,b)) pruning significantly speeds algorithm. comparison Pse -RTDP S-RTDP S3 -RTDP (Figure 3(a,b)) shows sampling dramaticspeedup respect pruned versions. fact, pure sampling, S-RTDP, converges extremelyquickly, S3 -RTDP slightly slower. However, S3 -RTDP still much faster Pse -RTDP.comparison qualities solutions produced S-RTDP S3 -RTDP w.r.t. optimal shownTable 1. observe solutions produced S-RTDP always nearly optimal. Since3. code may downloaded http://www.cs.washington.edu/ai/comdp/comdp.tgz45fiM AUSAM & W ELDComparison Pruned Unpruned RTDP Rover domainComparison Pruned Unpruned RTDP Factory domain12000y=xPs-RTDPPse-RTDP25000Times Pruned RTDP (in sec)Times Pruned RTDP (in sec)300002000015000100005000y=xPs-RTDPPse-RTDP100008000600040002000000500010000 15000 20000 25000Times Unpruned RTDP (in sec)300000200040006000800010000Times Unpruned RTDP (in sec)12000Figure 2: (a,b): Pruned vs. Unpruned RTDP Rover MachineShop domains respectively. Pruningnon-optimal combinations achieves significant speedups larger problems.Comparison Pruned Sampled RTDP Rover domainComparison Pruned Sampled RTDP Factory domain8000y=xS-RTDPS3-RTDP8000Times Sampled RTDP (in sec)Times Sampled RTDP (in sec)100006000400020000y=xS-RTDPS3-RTDP7000600050004000300020001000002000 4000 6000 8000 1000012000140001600018000Times Pruned RTDP (Pse-RTDP) - (in sec)01000 2000 3000 4000 5000 6000 7000 8000Times Pruned RTDP (Pse-RTDP) - (in sec)Figure 3: (a,b): Sampled vs Pruned RTDP Rover MachineShop domains respectively. Randomsampling action combinations yields dramatic improvements running times.46fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSComparison algorithms size problem Rover domain30000S-RTDPS3-RTDPPse-RTDPU-RTDP25000S-RTDPS3-RTDPPse-RTDPU-RTDP1500020000Times (in sec)Times (in sec)Comparison different algorithms Artificial domain20000150001000010000500050000005e+071e+081.5e+08Reach(|S|)*Avg(Ap(s))2e+082.5e+080200040006000 8000Avg(Ap(s))10000 12000 14000Figure 4: (a,b): Comparison different algorithms size problems Rover Artificial domains. problem size increases, gap sampled pruned approaches widensconsiderably.Results varying Number samples Rover Problem#4300250200150100500Running timesValues start state300100200300 400 500 600 700Concurrency : Avg(Ap(s))/|A|80090012.825012.7920012.7815012.7710012.7650J*(s0)0012.81Value start state350S-RTDP/Pse-RTDPTimes Sampled RTDP (in sec)Speedup : Sampled RTDP/Pruned RTDPSpeedup vs. Concurrency Artificial domain35010203040 50 60 70 80Number samples12.7490 100Figure 5: (a): Relative Speed vs. Concurrency Artificial domain. (b) : Variation quality solutionefficiency algorithm (with 95% confidence intervals) number samples Sampled RTDP one particular problem Rover domain. number samples increase,quality solution approaches optimal time still remains better Pse -RTDP (whichtakes 259 sec. problem).47fiM AUSAM & W ELDProblemRover1Rover2Rover3Rover4Rover5Rover6Rover7Artificial1Artificial2Artificial3MachineShop1MachineShop2MachineShop3MachineShop4MachineShop5J(s0 ) (S-RTDP)10.753810.753511.001612.74907.316310.506312.93434.51376.38476.558315.085914.141416.377115.85889.0314J (s0 ) (Optimal)10.753510.753511.001612.74617.316310.506312.92464.51376.38476.558315.033814.032916.341215.85888.9844Error<0.01%000.02%000.08%0000.35%0.77%0.22%00.56%Table 1: Quality solutions produced Sampled RTDPerror S-RTDP small, scaling 0.9 makes admissible initial cost function prunedRTDP; indeed, experiments, S3 -RTDP produced optimal solution.Figure 4(a,b) demonstrates running times vary problem size. use productnumber reachable states average number applicable action combinations per stateestimate size problem (the number reachable states artificial domainssame, hence x-axis Figure 4(b) Avg(Ap(s))). figures, verifynumber applicable combinations plays major role running times concurrent MDPalgorithms. Figure 5(a), fix factors vary degree parallelism. observespeedups obtained S-RTDP increase concurrency increases. encouraging result,expect S-RTDP perform well large problems inolving high concurrency, evenapproaches fail.Figure 5(b), present another experiment vary number action combinations sampled backup. solution quality inferior samplingcombinations, quickly approaches optimal increasing number samples.experiments sample 40 combinations per state.4. Challenges Temporal PlanningCoMDP model powerful enough model concurrency actions, still assumesaction instantaneous. incorporate actual action durations modeling problem.essential increase scope current models real world domains.present model algorithms discuss several new theoretical challengesimposed explicit action durations. Note results section apply wide rangeplanning problems:regardless whether durations uncertain fixedregardless whether effects stochastic deterministic.48fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSActions uncertain duration modeled associating distribution (possibly conditionedoutcome stochastic effects) execution times. focus problems whose objectiveachieve goal state minimizing total expected time (make-span), results extendcost functions combine make-span resource usage. raises questiongoal counts achieved. require that:Assumption 1 executing actions terminate goal considered achieved.Assumption 2 action, started, cannot terminated prematurely.start asking question restricted set time points optimalitypreserved even actions started points?Definition 1 time point new action allowed start execution called decisionepoch. time point pivot either 0 time new effect might occur (e.g.,end actions execution) new precondition may needed existing precondition maylonger needed. happening either 0 time effect actually occurs newprecondition definitely needed existing precondition longer needed.Intuitively, happening point change world state action constraints actuallyhappens (e.g., new effect new precondition). execution crosses pivot (a possiblehappening), information gained agents execution system (e.g., didnt effectoccur) may change direction future action choices. Clearly, action durationsdeterministic, set pivots set happenings.Example: Consider action whose durations follow uniform integer duration 110. started time 0 timepoints 0, 1, 2,. . ., 10 pivots. certain executionfinishes time 4 4 (and 0) happening (for execution). 2Definition 2 action PDDL2.1 action (Fox & Long, 2003) following hold:effects realized instantaneously either (at start) (at end), i.e., beginningcompletion action (respectively).preconditions may need hold instaneously start (at start), end (atend) complete execution action (over all).(:durative-action:duration (= ?duration 4):condition (and (over P ) (at end Q)):effect (at end Goal))(:durative-action b:duration (= ?duration 2):effect (and (at start Q) (at end (not P ))))Figure 6: domain illustrate expressive action model may require arbitrary decision epochssolution. example, b needs start 3 units execution reach Goal.49fiM AUSAM & W ELDTheorem 5 PDDL2.1 domain restricting decision epochs pivots causes incompleteness(i.e., problem may incorrectly deemed unsolvable).Proof: Consider deterministic temporal planning domain Figure 6 uses PDDL2.1 notation(Fox & Long, 2003). initial state P =true Q=false, way reach Goalstart time (e.g., 0), b timepoint open interval (t + 2, + 4). Clearly,new information gained time points interval none pivot. Still,required solving problem. 2Intuitively, instantaneous start end effects two PDDL2.1 actions may require certainrelative alignment within achieve goal. alignment may force one action startsomewhere (possibly non-pivot point) midst others execution, thus requiringintermediate decision epochs considered.Temporal planners may classified one two architectures: constraint-postingapproaches times action execution gradually constrained planning (e.g.,Zeno LPG, see Penberthy Weld, 1994; Gerevini Serina, 2002) extended statespace methods (e.g., TP4 SAPA, see Haslum Geffner, 2001; Kambhampati, 2001).Theorem 5 holds architectures strong computational implications state-spaceplanners limiting attention subset decision epochs speed planners.theorem also shows planners like SAPA Prottle (Little, Aberdeen, & Thiebaux, 2005)incomplete. Fortunately, assumption restricts set decision epochs considerably.Definition 3 action TGP-style action4 following hold:effects realized unknown point action execution, thus usedaction completed.preconditions must hold beginning action.preconditions (and features transition function conditioned) mustchanged actions execution, except effect action itself.Thus, two TGP-style actions may execute concurrently clobber others preconditions effects. case TGP-style actions set happenings nothing settime points action terminates. TGP pivots set points action mightterminate. (Of course sets additionally include zero).Theorem 6 actions TGP-style, set decision epochs may restricted pivotswithout sacrificing completeness optimality.Proof Sketch: contradiction. Suppose optimal policy satisfies theorem;must exist path optimal policy one must start action, a, time eventhough action could terminated t. Since planner hasnt gainedinformation t, case analysis (which requires actions TGP-style) shows one couldstarted earlier execution path without increasing make-span. detailed proofdiscussed Appendix. 2case deterministic durations, set happenings set pivots; hencefollowing corollary holds:4. original TGP (Smith & Weld, 1999) considered deterministic actions fixed duration, usephrase TGP-style general way, without restrictions.50fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSProbabillity: 0.5a0s0a2Ga1Makespan: 3Probability 0.5a0s0a1Gb0Makespan: 902468TimeFigure 7: Pivot decision epochs necessary optimal planning face nonmonotonic continuation. domain, Goal achieved h{a0 , a1 }; a2 hb0 i; a0 duration 29; b0 mutex a1 . optimal policy starts a0 then, a0 finishtime 2, starts b0 (otherwise starts a1 ).Corollary 7 actions TGP-style deterministic durations, set decisionepochs may restricted happenings without sacrificing completeness optimality.planning uncertain durations may huge number pivots; usefulconstrain range decision epochs.Definition 4 action independent duration correlation probabilisticeffects duration.Definition 5 action monotonic continuation expected time action terminationnonincreasing execution.Actions without probabilistic effects, nature, independent duration. Actions monotonic continuations common, e.g. uniform, exponential, Gaussian, many duration distributions. However, actions bimodal multi-modal distributions dont monotonic continuations. example consider action uniform distribution [1,3].action doesnt terminate 2, expected time completion calculated 2, 1.5,1 times 0, 1, 2 respectively, monotonically decreasing. examplenon-monotonic continuation see Figure 18.Conjecture 8 actions TGP-style, independent duration monotonic continuation,set decision epochs may restricted happenings without sacrificing completenessoptimality.actions continuation nonmonotonic failure terminate increase expectedtime remaining cause another sub-plan preferred (see Figure 7). Similarly, actionsduration isnt independent failure terminate changes probability eventual effectsmay prompt new actions started.exploiting theorems conjecture may significantly speed planning sinceable limit number decision epochs needed decision-making. use theoreticalunderstanding models. First, simplicity, consider case TGP-style actionsdeterministic durations. Section 6, relax restriction allowing stochastic durations,unimodal well multimodal.51fiM AUSAM & W ELDtogglep12p12 (effect)conflictp12 (Precondition)togglex10246810Figure 8: sample execution demonstrating conflict due interfering preconditions effects. (Theactions shaded disambiguate preconditions effects)5. Temporal Planning Deterministic Durationsuse abbreviation CPTP (short Concurrent Probabilistic Temporal Planning) referprobabilistic planning problem durative actions. CPTP problem input modelsimilar CoMDPs except action costs, C(s, a, s0 ), replaced deterministicdurations, (a), i.e., input form hS, A, Pr, , G, s0 i. study objective minimizing expected time (make-span) reaching goal. rest paper makefollowing assumptions:Assumption 3 action durations integer-valued.assumption negligible effect expressiveness one convert problemrational durations one satisfies Assumption 3 scaling durations g.c.d.denominators. case irrational durations, one always find arbitrarily close approximation original problem approximating irrational durations rational numbers.reasons discussed previous section adopt TGP temporal action model SmithWeld (1999), rather complex PDDL2.1 (Fox & Long, 2003). Specifically:Assumption 4 actions follow TGP model.restrictions consistent previous definition concurrency. Specifically,mutex definitions (of CoMDPs probabilistic STRIPS) hold required assumptions. illustration, consider Figure 8. describes situation two actionsinterfering preconditions effects executed concurrently. see not, supposeinitially p12 false two actions toggle-x1 toggle-p12 started time 2 4, respectively. p12 precondition toggle-x1 , whose duration 5, needs remain falsetime 7. toggle-p12 may produce effects anytime 4 9, may conflictpreconditions executing action. Hence, forbid concurrent executiontoggle-x1 toggle-p12 ensure completely predictable outcome distribution.definition concurrency, dynamics model remains consistentEquation 5. Thus techniques developed CoMDPs derived probabilistic STRIPS actionsmay used.52fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSAligned Epoch policy execution(takes 9 units)togglex1t3fffft3 t3 t3 t30510timetogglex1fffft3 t3 t3 t3 t3Interwoven Epoch policy execution(takes 5 units)Figure 9: Comparison times taken sample execution interwoven-epoch policy alignedepoch policy. trajectories toggle-x3 (t3) action fails four times succeeding.aligned policy must wait actions complete starting more, takestime interwoven policy, start actions middle.5.1 Formulation CoMDPmodel CPTP problem CoMDP, thus MDP, one way. listtwo prominent formulations below. first formulation, aligned epoch CoMDP modelsproblem approximately solves quickly. second formulation, interleaved epochs modelsproblem exactly results larger state space hence takes longer solve using existingtechniques. subsequent subsections explore ways speed policy constructioninterleaved epoch formulation.5.1.1 LIGNED E POCH EARCH PACEsimple way formulate CPTP model standard CoMDP probabilistic STRIPS,action costs set durations cost combination maximumduration constituent actions (as Equation 6). formulation introduces substantialapproximation CPTP problem. true deterministic domains too, illustrateusing example involving stochastic effects. Figure 9 compares trajectoriestoggle-x3 (t3) actions fails four consecutive times succeeding. figure, fdenote failure success uncertain actions, respectively. vertical dashed lines representtime-points action started.Consider actual executions resulting policies. aligned-epoch case (Figure 9top), combination actions started state, next decision takeneffects actions observed (hence name aligned-epochs). contrast, Figure 9bottom shows decision epoch optimal execution CPTP problem, many actionsmay midway execution. explicitly take account actionsremaining execution times making subsequent decision. Thus, actual state spaceCPTP decision making substantially different simple aligned-epoch model.Note due Corollary 7 sufficient consider new decision epoch happening,i.e., time-point one actions complete. Thus, using Assumption 3 inferdecision epochs discrete (integer). course, optimal policies property.53fiM AUSAM & W ELDState variables : x1 , x2 , x3 , x4 , p12Action(a) Preconditiontoggle-x15p12toggle-x25p12toggle-x31trueEffectx1 x1x2 x2x3 x3changetoggle-x41truex4 x4changetoggle-p125truep12 p12Goal : x1 = 1, x2 = 1, x3 = 1, x4 = 1Probability110.90.10.90.11Figure 10: domain Example 1 extended action durations.easy see exists least one optimal policy action beginshappening. Hence search space reduces considerably.5.1.2 NTERWOVEN E POCH EARCH PACEadapt search space representation Haslum Geffner (2001), similarresearch (Bacchus & Ady, 2001; & Kambhampati, 2001). original state spaceSection 2 augmented including set actions currently executing times passedsince started. Formally, let new interwoven state5 - ordered pair hX,where:XS= {(a, )|a A, 0 < (a)}X represents values state variables (i.e. X state original state space)denotes set ongoing actions timespassed since start . ThusNoverall interwoven-epoch search space - = aA {a} Z(a) , Z(a) representsNset {0, 1, . . . , (a) 1}denotes Cartesian product multiple sets.Also define set actions already execution. words, projectionignoring execution times progress:= {a|(a, ) = hX, i}Example: Continuing example domain Figure 10, suppose state s1 statevariables false, suppose action toggle-x1 started 3 units ago current time.state would represented hX1 , Y1 X1 =(F, F, F, F, F ) Y1 ={(toggle-x1 ,3)} (the fivestate variables listed order: x1 , x2 , x3 , x4 p12 ). set As1 would {toggle-x1 }.allow possibility simply waiting action complete execution, is, deciding decision epoch start additional action, augment set no-op action,applicable states = hX, 6= (i.e. states action stillexecuted). state s, no-op action mutex non-executing actions, i.e.,\ . words, decision epoch either no-op started combination5. use subscript - denote interwoven state space (S - ), value function (J - ), etc..54fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSinvolving no-op. define no-op variable duration6 equal time anotheralready executing action completes (next (s, A) defined below).interwoven applicability set defined as:(Ap - (s) =Apk (X) = else{noop}{A|AAs Apk (X) AAs = }Transition Function: also need define probability transition function, Pr - ,interwoven state space. decision epoch let agent state = (X, ). Supposeagent decides execute action combination A. Define Ynew set similarconsisting actions starting; formally Ynew = {(a, (a))|a A}. system,next decision epoch next time executing action terminates. Let us call timenext (s, A). Notice next (s, A) depends executing newly started actions. Formally,next (s, A) =min(a,)Y Ynew(a)Moreover, multiple actions may complete simultaneously. Define Anext (s, A)set actions complete exactly next (s, A) timesteps. -component statedecision epoch next (s, A) timeYnext (s, A) = {(a, + next (s, A))|(a, ) Ynew , (a) > next (s, A)}Let s=hX, let s0 =hX 0 , 0 i. transition function CPTP defined as:Prk (X 0 |X, Anext (s, A)) 0= Ynext (s, A)0otherwise(0Pr - (s |s, A)=words, executing action combination state = hX, takes agentdecision epoch next (s, A) ahead time, specifically first time combinationAnext (s, A) completes. lets us calculate Ynext (s, A): new set actions still executingtimes elapsed. Also, TGP-style actions, probability distribution differentstate variables modified independently. Thus probability transition function due CoMDPprobabilistic STRIPS used decide new distribution state variables,combination Anext (s, A) taken state X.Example: Continuing previous example, let agent state s1 execute action combination = {toggle-x4 }. next (s1 , A) = 1, since toggle-x4 finish first. Thus,Anext (s1 , A)= {toggle-x4 }. Ynext (s1 , A) = {(toggle-x1 ,4)}. Hence, probability distributionstates executing combination state s1((F, F, F, T, F ), Ynext (s1 , A)) probability = 0.9((F, F, F, F, F ), Ynext (s1 , A)) probability = 0.16. precise definition model create multiple no-opt actions different constant durations no-optapplicable interwoven state one = next (s, A).55fiM AUSAM & W ELDStart Goal States: interwoven space, start state hs0 , new set goalstates G - = {hX, i|X G}.redefining start goal states, applicability function, probability transitionfunction, finished modeling CPTP problem CoMDP interwoven state space.use techniques CoMDPs (and MDPs well) solve problem. particular,use Bellman equations described below.Bellman Equations: set equations solution CPTP problem written as:J - (s) = 0, G - else(11)next (s, A) + Pr - (s0 |s, A)J - (s0 )J - (s) = minAAp - (s)s0Xuse DURsamp refer sampled RTDP algorithm search space. mainbottleneck naively inheriting algorithms like DURsamp huge size interwoven statespace. worst case (when actions executed concurrently) size state spaceQ|S| ( aA (a)). get bound observing action a, (a) numberpossibilities: either executing remaining times 1, 2, . . . , (a) 1.Thus need reduce abstract/aggregate state space order make problemtractable. present several heuristics used speed search.5.2 Heuristicspresent admissible inadmissible heuristics used initial costfunction DURsamp algorithm. first heuristic (maximum concurrency) solves underlying MDP thus quite efficient compute. second heuristic (average concurrency)inadmissible, tends informed maximum concurrency heuristic.5.2.1 AXIMUM C ONCURRENCY H EURISTICprove optimal expected cost traditional (serial) MDP divided maximumnumber actions executed parallel lower bound expected make-spanreaching goal CPTP problem. Let J(X) denote value state X traditionalMDP costs action equal duration. Let Q(X, A) denote expected cost reachgoal initially actions combination executed greedy serial policy followedPthereafter. Formally, Q(X, A) = X 0 Prk (X 0 |X, A)J(X 0 ). Let J - (s) value equivalentCPTP problem interwoven-epoch state space. Let concurrency statemaximum number actions could executed state concurrently. define maximumconcurrency domain (c) maximum number actions concurrently executedworld state domain. following theorem used provide admissibleheuristic CPTP problems.Theorem 9 Let = hX, i,J - (s)J - (s)J (X)=cQ (X, )6=c56(12)fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSProof Sketch: Consider trajectory make-span L (from state = hX, goal state)CPTP problem using optimal policy. make concurrent actions sequential executingchronological order started. concurrent actions non-interacting,outcomes stage similar probabilities. maximum make-span sequentialtrajectory cL (assuming c actions executing points semi-MDP trajectory). HenceJ(X) using (possibly non-stationary) policy would cJ - (s). Thus J (X) cJ - (s).second inequality proven similar way. 2cases bounds tight. example, consider deterministic planningproblem optimal plan concurrently executing c actions unit duration (makespan = 1). sequential version, actions would taken sequentially (make-span =c).Following theorem, maximum concurrency (MC) heuristic state = hX,defined follows:Q (X, )J (X)else HM C (s) == HM C (s) =ccmaximum concurrency c calculated static analysis domain onetime expense. complete heuristic function evaluated solving MDP states.However, many states may never visited. implementation, calculationdemand, states visited, starting MDP current state. RTDP runseeded previous value function, thus computation thrown awayrelevant part state space explored. refer DURsamp initiated MC heuristicDURMCsamp .5.2.2 AVERAGE C ONCURRENCY H EURISTICInstead using maximum concurrency c heuristic use average concurrencydomain (ca ) get average concurrency (AC) heuristic. call resulting algorithmDURACsamp . AC heuristic admissible, experiments typically informedheuristic. Moreover, case actions duration, AC heuristic equalsMC heuristic.5.3 Hybridized Algorithmpresent approximate method solve CPTP problems. many kindspossible approximation methods, technique exploits intuition best focus computation probable branches current policys reachable space. dangerapproach chance that, execution, agent might end unlikely branch,poorly explored; indeed might blunder dead-end case. undesirable, apparently attractive policy might true expected make-span infinity.Since, wish avoid dead-ends, explore desirable notion propriety.Definition 6 Propriety: policy proper state guaranteed lead, eventually, goalstate (i.e., avoids dead-ends cycles) (Barto et al., 1995). define planning algorithmproper always produces proper policy (when one exists) initial state.describe anytime approximation algorithm, quickly generates proper policyuses additional available computation time improve policy, focusinglikely trajectories.57fiM AUSAM & W ELD5.3.1 H YBRIDIZED P LANNERalgorithm, DURhyb , created hybridizing two policy creation algorithms. Indeed,novel notion hybridization general powerful, applying many MDP-like problems; however, paper focus use hybridization CPTP. Hybridization usesanytime algorithm like RTDP create policy frequently visited states, uses faster (andpresumably suboptimal) algorithm infrequent states.case CPTP, algorithm hybridizes RTDP algorithms interwoven-epochaligned-epoch models. aligned-epochs, RTDP converges relatively quickly, statespace smaller, resulting policy suboptimal CPTP problem, policywaits currently executing actions terminate starting new actions. contrast,RTDP interwoven-epochs generates optimal policy, takes much longer converge.insight run RTDP interwoven space long enough generate policygood common states, stop well converges every state. Then, ensurerarely explored states proper policy, substitute aligned policy, returning hybridizedpolicy.Algorithm 3 Hybridized Algorithm DURhyb (r, k, m)1: -2:initialize J - (s) admissible heuristic3: repeat4:perform RTDP trials5:compute hybridized policy (hyb ) using interwoven-epoch policy k-familiar states aligned-epoch policy otherwiseclean hyb removing dead-ends cyclesJ - hs0 , evaluation hyb start stateJ - (hs0 ,i)J - (hs0 ,i)8:<rJ - (hs0 ,i)9: return hybridized policy hyb6:7:Thus key question decide states well explored not.define familiarity state number times visited previous RTDPtrials. reachable state whose familiarity less constant, k, aligned policy createdit. Furthermore, dead-end state reached using greedy interwoven policy, createaligned policy immediate precursors state. cycle detected7 , computealigned policy states part cycle.yet said hybridized algorithm terminates. Use RTDP helps us definingsimple termination condition parameter varied achieve desiredcloseness optimality well. intuition simple. Consider first, optimal labeled RTDP.starts admissible heuristic guarantees value start state, J - (hs0 , i),remains admissible (thus less equal optimal). contrast, hybridized policys makespan always longer equal optimal. Thus time progresses, values approachoptimal make-span opposite sides. Whenever two values within optimality ratio (r),know algorithm found solution, close optimal.7. implementation cycles detected using simulation.58fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSFinally, evaluation hybridized policy done using simulation, performfixed number RTDP trials. Algorithm 3 summarizes details algorithm. One seecombined policy proper two reasons: 1) policy state alignedpolicy, proper RTDP aligned-epoch model run convergence,2) rest states explicitly ensured cycles dead-ends.5.4 Experiments: Planning Deterministic DurationsContinuing Section 3.6, set experiments evaluate various techniquessolving problems involving explicit deterministic durations. compare computation timesolution quality five methods: interwoven Sampled RTDP heuristic (DURsamp ),ACmaximum concurrency (DURMCsamp ), average concurrency (DURsamp ) heuristics, hybridizedalgorithm (DURhyb ) Sampled RTDP aligned-epoch model (DURAE ). testRover, MachineShop Aritificial domains. also use Artificial domain see relativeperformance techniques varies amount concurrency domain.5.4.1 E XPERIMENTAL ETUPmodify domains used Section 3.6 additionally including action durations. NASARover MachineShop domains, generate problems 17-26 state variables 12-18 actions, whose duration range 1 20. problems 15,000-700,000 reachable states interwoven-epoch state space, - .use Artificial domain control experiments study effect degree parallelism.problems domain 14 state variables 17,000-40,000 reachable statesdurations actions 1 3.use implementation Sampled RTDP8 implement heuristics: maximum concurrency (HM C ), average concurrency (HAC ), initialization value function. calculateheuristics demand states visited, instead computing complete heuristicwhole state space once. also implement hybridized algorithm initial valuefunction set HM C heuristic. parameters r, k, kept 0.05, 100 500,respectively. test algorithms number problem instances threedomains, generate varying number objects, degrees parallelism, durationsactions distances goal.5.4.2 C OMPARISON RUNNING IMESFigures 11(a, b) 12(a) show variations running times algorithms differentproblems Rover, Machineshop Artificial domains, respectively. first three bars representbase Sampled RTDP without heuristic, HM C , HAC , respectively. fourthbar represents hybridized algorithm (using HM C heuristic) fifth bar computationaligned-epoch Sampled RTDP costs set maximum action duration. whiteregion fourth bar represents time taken aligned-epoch RTDP computationshybridized algorithm. error bars represent 95% confidence intervals running times. Noteplots log scale.8. Note policies returned DURsamp guaranteed optimal. Thus implemented algorithmsapproximate. replace DURsamp pruned RTDP (DURprun ) optimality desired.59fiM AUSAM & W ELDRover16Mach1110^310^210^1Mach12Mach13Mach14Mach15Mach160ACHAERover150ACHAERover14Time sec (on log scale)10^310^210^10ACHAE0ACHAE0ACHAE0ACHAE0ACHAE0ACHAE0ACHAE10^010^00ACHAETime sec (on log scale)Rover130ACHAERover12Rover110ACHAE10^410^4Figure 11: (a,b): Running times (on log scale) Rover Machineshop domain, respectively.problem five bars represent times taken algorithms: DURsamp (0), DURMCsamp(AE), DURAC(AC),DUR(H),DUR(AE),respectively.whitebarDURhybAEhybsampdenotes portion time taken aligned-epoch RTDP.AlgosDURMCsampDURACsampDURhybDURAESpeedup compared DURsampRoverMachineshop Artificial Average3.0167641.5454181.071645 1.8779423.5859932.1738091.950643 2.57014810.534182.15486316.53159 9.74021135.284116.42708241.8623 131.1911Table 2: ratio time taken - S-RTDP heuristics algorithm.heuristics produce 2-3 times speedups. hybridized algo produces 10x speedup. Alignedepoch search produces 100x speedup, sacrifices solution quality.notice DURAE solves problems extremely quickly; natural since alignedepoch space much smaller. Use HM C HAC always speeds search - model.Comparing heuristics amongst themselves, find average concurrency heuristic mostlyperforms faster maximum concurrency presumably HAC informed heuristic practice, although cost inadmissible. find couple cases HACdoesnt perform better; could focusing search incorrect region, giveninadmissible nature.Rover domain, hybridized algorithm performs fastest. fact, speedupsdramatic compared methods. domains, results comparable smallproblems. However, large problems two domains, hybridized outperforms othershuge margin. fact largest problem Artificial domain, none heuristics ableconverge (within day) DURhyb DURAE converge solution.60fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS10^4Art12Art13Art14Art150ACHAEArt110ACHAE1.6Art11(68) Art12(77) Art13(81) Art14(107) Art15(224) Art16(383) Art17(1023)Art16Art17Ratio make-span optimalTime sec (on log scale)1.510^310^210^11.41.31.21.110.90ACHAE0ACHAE0ACHAE0ACHAE0ACHAE0ACHAE0ACHAE0ACHAE0ACHAE0ACHAE0ACHAE0ACHAE0.810^0Figure 12: (a,b): Comparison different algorithms (running times solution quality respectively)Artificial domain. degree parallelism increases problems become harder;largest problem solved DURhyb DURAE .Table 2 shows speedups obtained various algorithms compared basic DURsamp .Rover Artificial domains speedups obtained DURhyb DURAE muchprominent Machineshop domain. Averaging domains, H produces 10x speedupAE produces 100x speedup.5.4.3 C OMPARISON OLUTION Q UALITYFigures 13(a, b) 12(b) show quality policies obtained five methodsdomains. measure quality simulating generated policy across multiple trials,reporting average time taken reach goal. plot ratio so-measured expectedmake-span optimal expected make-span9 . Table 3 presents solution qualities method,averaged problems domain. note aligned-epoch policies usually yieldsignificantly longer make-spans (e.g., 25% longer); thus one must make quality sacrificespeedy policy construction. contrast, hybridized algorithm extorts small sacrificequality exchange speed.5.4.4 VARIATION C ONCURRENCYFigure 12(a) represents attempt see relative performance algorithms changedincreasing concurrency. Along top figure, problem names, numbers brackets;list average number applicable combinations MDP state, AvgsS - |Ap(s)|,range 68 1023 concurrent actions. Note difficult problems lot parallelism, DURsamp slows dramatically, regardless heuristic. contrast, DURhyb still ablequickly produce policy, almost loss quality (Figure 12(b)).9. large problems optimal algorithm converge. those, take optimal, best policy foundruns.61fiM AUSAM & W ELDRover161.7 Mach11Ratio make-span optimal1.41.31.21.11Mach161.21.110ACHAE0ACHAEMach151.30.80ACHAEMach141.40.80ACHAEMach131.50.90ACHAEMach121.60.90ACHAERatio make-span optimal1.50ACHAERover150ACHAERover140ACHAERover130ACHAERover120ACHAERover110ACHAE1.81.6Figure 13: (a,b): Comparison make-spans solution found optimal(plotted 1 yaxes) Rover Machineshop domains, respectively. algorithms except DURAE producesolutions quite close optimal.AlgosDURsampDURMCsampDURACsampDURhybDURAERover1.0596251.0184051.0171411.0593491.257205Average QualityMachineshop Artificial1.0650781.0425611.0625641.0134651.0463911.0205231.0755341.0592011.2448621.254407Average1.0557041.0314781.0280191.0646911.252158Table 3: Overall solution quality produced algorithms. Note algorithms except DURAE produce policies whose quality quite close optimal. average DURAE produces make-spans125% optimal.6. Optimal Planning Uncertain Durationsextend techniques previous section case action durations deterministic. before, consider TGP-style actions discrete temporal model. assumeindependent durations, monotonic continuations, Section 6.3 relaxes latter, extendingalgorithms handle multimodal duration distributions. aim minimizeexpected time required reach goal.6.1 Formulating CoMDPformulate planning problem CoMDP similar Section 5.1.parameters CoMDP used directly work deterministic durations, needrecompute transition function.62fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSState Space: aligned epoch state space well interwoven epoch space, definedSection 5.1 adequate model planning problem. determine size interwovenspace, replace duration action max duration. Let (a) denote maximumtime within action complete. overall interwoven-epoch search space - ={a} ZM (a) , ZM (a) represents set {0, 1, . . . , (a) 1}denotesCartesian product multiple sets.Action Space: state may apply combination actions applicability functionreflecting fact combination actions safe w.r.t (and w.r.t. already executingactions case interwoven space) previous sections. previous state spaceaction space work well problem, transition function definition needs change, sinceneed take account uncertainty durations.Transition Function: Uncertain durations require significant changes probability transitionfunction (Pr - ) interwoven space definitions Section 5.1.2. Since assumptions justify Conjecture 8, need consider happenings choosing decision epochs.NNaAAlgorithm 4 ComputeTransitionFunc(s=hX, i,A)1:2:3:4:5:6:7:8:9:10:11:12:{(a, 0)}mintime min(a,)Y minimum remaining timemaxtime min(a,)Y maximum remaining timeinteger [mintime, maxtime]set actions could possibly terminatenon-empty subsets Asubtpc (prob. exactly Asubt terminates (see Equation 13).W {(Xt , pw ) | Xt world state; pw probability Asubt terminates yielding Xt }.(Xt , pw ) WYt {(a, + t) | (a, ) Y,/ Asubt }insert (hXt , Yt i, pw pc ) outputreturn outputcomputation transition function described Algorithm 4. Although next decisionepoch determined happening, still need consider pivots next state calculationspotential happenings. mintime minimum time executing action couldterminate, maxtime minimum time guaranteed least one actionterminate. times mintime maxtime compute possible combinationscould terminate resulting next interwoven state. probability, pc , (line 7) maycomputed using following formula:pc =(prob. terminates + t|a hasn0 terminated till )(a,a )Y,aAsubt(prob. b doesn0 terminate b + t|b hasn0 terminated till b )(13)(b,b )Y,bAsub/Considering pivots makes algorithm computationally intensive maymany pivots many action combinations could end one, many outcomes each.implementation, cache transition function recomputeinformation state.63fiM AUSAM & W ELDStart Goal States: start state goal set developed deterministic durationswork unchanged durations stochastic. So, start state hs0 , goal setG - = {hX, i|X G}.Thus modeled problem CoMDP interwoven state space. redefined start goal states, probability transition function. use techniquesCoMDPs solve problem. particular, use Bellman equations below.Bellman Equations Interwoven-Epoch Space: Define el (s, A, s0 ) time elapsed two interwoven states s0 combination executed s. set equationssolution problem written as:J - (s) = 0, G - elsenXPr - (s0 |s, A) el (s, A, s0 ) + J - (s0 )J - (s) = minAAp - (s) 0-(14)Compare equations Equation 11. one difference besides new transitionfunction time elapsed within summation sign. time elapsed dependsalso next interwoven state.modeled problem CoMDP use algorithms Section 5. useDUR denote family algorithms CPTP problems involving stochastic durations.main bottleneck solving problem, besides size interwoven state space,high branching factor.6.1.1 P OLICY C ONSTRUCTION : RTDP & H YBRIDIZED P LANNINGSince modeled problem CoMDP new interwoven space, may use prunedRTDP (DURprun ) sampled RTDP (DURsamp ) policy construction. Since cost function problem (el ) depends also current next state, combo-skippingapply problem. Thus DURprun refers RTDP combo-elimination.Furthermore, small adaptations necessary incrementally compute (admissible)maximum concurrency (M C) (more informed, inadmissible) average concurrency (AC)heuristics. example, serial MDP (in RHS Equation 12) need computeaverage duration action use actions cost.Likewise, speed planning hybridizing (DURhyb ) RTDP algorithms interwoven aligned-epoch CoMDPs produce near-optimal policy significantly less time.dynamics aligned epoch space Section 5 one exception. costcombination, case deterministic durations, simply max duration constituentactions. novel twist stems fact uncertain durations require computation costaction combination expected time last action combination terminate.example, suppose two actions, uniform duration distributions [1,3], startedconcurrently. probabilities actions finished times 1, 2 3 (and earlier) 1/9, 3/9, 5/9 respectively. Thus expected duration completion combination(let us call AE ) 11/9 + 23/9 + 35/9 = 2.44.64fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS6.2 Expected-Duration Plannermodeled CoMDP full-blown interwoven space, stochastic durations causeexlposive growth branching factor. general, n actions started possibledurations r probabilistic effects, (m 1)[(r + 1)n rn 1] + rnpotential successors. number may computed follows: duration 11 subset actions could complete action could result r outcomes. Hence, totalPnumber successors per duration i[1..n] n Ci ri = (r + 1)n rn 1. Moreover, noneactions finish time 1 last step actions terminate leading rn outcomes.So, total number successors (m 1)[(r + 1)n rn 1] + rn . Thus, branching factormultiplicative duration uncertainty exponential concurrency.manage extravagant computation must curb branching factor. One methodignore duration distributions. assign action constant duration equal meandistribution, apply deterministic-duration planner DURsamp . However,executing deterministic-duration policy setting durations actually stochastic,action likely terminate time different mean, expected duration. DURexpplanner addresses problem augmenting deterministic-duration policy created accountunexpected outcomes.6.2.1 NLINE V ERSIONprocedure easiest understand online version (Algorithm 5): wait unexpectedhappens, pause execution, re-plan. original estimate actions duration implausible,compute revised deterministic estimate terms Ea (min) expected valueduration given terminated time min. Thus, Ea (0) compute expectedduration a.Algorithm 5 Online DURexp1: build deterministic-duration policy start state s02: repeat3:execute action combination specified policy4:wait interrupt5:case: action terminated expected {//do nothing}6:case: action terminates early7:extend policy current state8:case: action didnt terminate expected9:extend policy current state revisingduration follows:10:time elapsed since started executing11:nextexp dEa (0)e12:nextexp <13:nextexp dEa (nextexp)e14:endwhile15:revised duration nextexp16:endwait17: goal reached65fiM AUSAM & W ELDExample: Let duration action follow uniform distribution 1 15.expected value gets assigned first run algorithm (dEa (0)e) 8. runningalgorithm, suppose action didnt terminate 8 reach state runningfor, say, 9 time units. case, revised expected duration would (dEa (8)e) = 12.Similarly, doesnt terminate 12 either next expected duration would 14,finally 15. words states executing times 0 8, expectedterminate 8. times 8 12 expected completion 12, 12 14 14doesnt terminate 14 15. 26.2.2 FFLINE V ERSIONalgorithm also offline version re-planning contingencies done aheadtime fairness used version experiments. Although offline algorithm planspossible action durations, still much faster algorithms. reasonplanning problems solved significantly smaller (less branching factor, smallerreachable state space), previous computation succinctly stored formhinterwoven state, valuei pairs thus reused. Algorithm 6 describes offline plannersubsequent example illustrates savings.Algorithm 6 Offline DURexp1: build deterministic-duration policy start state s0 ; get current J - - values2: insert s0 queue open3: repeat4:state = open.pop()5:currstate s.t. Pr - (currstate|state, - (state)) > 0currstate goal currstate set visitedvisited.insert(currstate)J - (currstate) convergedrequired, change expected durations actions currently executingcurrstate.10:solve deterministic-duration planning problem start state currstate11:insert currstate queue open12: open empty6:7:8:9:Line 9 Algorithm 6 assigns new expected duration actions currently runningcurrent state completd time previous termination point.reassignment follows similar case online version (line 13).Example: Consider domain two state-variables, x1 x2 , two actions set-x1set-x2 . task set variables (initially false). Assume set-x2 alwayssucceeds whereas set-x1 succeeds 0.5 probability. Moreover, let actionsuniform duration distribution 1, 2, 3. case complete interwoven epoch searchcould touch 36 interwoven states (each state variable could true false, action couldrunning, running 1 unit, running 2 units). Instead, build deterministicduration policy actions deterministic duration 2, total number statestouched 16 interwoven states (each action could runningrunning 1 unit).66fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSProblemA2B2GC2GOptimal Solution (Trajectory 1, pr =0.5, makespan 9)A2B2GC2Optimal Solution (Trajectory 2, pr =0.5, makespan 5)A2C2GDUR exp Solution (makespan 8)A2Time0B24G812Figure 14: example domain DURexp algorithm compute optimal solution.Now, suppose deterministic planner decides execute actions start state.committed combination, easy see certain states never reached.example, state h(x1 , x2 ), {(setx1 , 2)}i never visited, since set-x2 completesguaranteed x2 set. fact, example, 3 new states initiate offlinereplanning (line 10 Algo 6), viz., h(x1 , x2 ), {(setx2 , 2)}i, h(x1 , x2 ), {(setx2 , 2)}i,h(x1 , x2 ), {(setx1 , 2)}i 26.2.3 P ROPERTIESUnfortunately, DURexp algorithm guaranteed produce optimal policy. badpolicies generated expected-duration planner? experiments show DURexptypically generates policies extremely close optimal. Even worst-case pathologicaldomain able construct leads expected make-span 50% longeroptimal (in limit). example illustrated below.Example: consider domain actions A2:n , B2:n , C2:n D. Ai Bitakes time 2i . Ci probabilistic duration: probability 0.5, Ci takes 1 unit time,remaining probability, takes 2i+1 + 1 time. Thus, expected duration Ci2i + 1. takes 4 units. sub-problem SPi , goal may reached executing Ai followedBi . Alternatively, goal may reached first executing Ci recursively solvingsub-problem SPi1 . domain, DURexp algorithm always compute hAi ; Bibest solution. However, optimal policy starts {Ai , Ci }. Ci terminates 1,policy executes solution SPi1 ; otherwise, waits Ai terminates executes Bi .Figure 14 illustrates sub-problem SP2 optimal policy expected make-span7 (vs. DURexp make-span 8). general, expected make-span optimal policy3SPn 13 [2n+2 + 24n ] + 22n + 2. Thus, limn expopt = 2 .26.3 Multi-Modal Duration Distributionsplanners previous two sections benefited considering small set happeningsinstead pivots, approach licensed Conjecture 8. Unfortunately, simplification67fiM AUSAM & W ELDwarranted case actions multi-modal duration distributions, commoncomplex domains factors cant modeled explicitly. example, amounttime Mars rover transmit data might bimodal distribution normally wouldtake little time, dust storm progress (unmodeled) could take much longer.handle cases model durations mixture Gaussians parameterized triplehamplitude, mean, variancei.6.3.1 C MDP F ORMULATIONAlthough cannot restrict decision epochs happenings, need consider pivots;required actions multi-modal distributions. fact, suffices consider pivotsregions distribution expected-time-to-completion increases. casesneed consider happenings.Two changes required transition function Algorithm 4. line 3, maxtimecomputation involves time next pivot increasing remaining time regionactions multi-modal distributions (thus forcing us take decision points, evenaction terminates). Another change (in line 6) allows non-empty subset Asub =maxtime. is, next state computed even without action termination. makingchanges transition function reformulate problem CoMDP interwoven spacethus solve, using previous methods pruned/sampled RTDP, hybrid algorithm expectedduration algorithm.6.3.2 RCHETYPAL -D URATION P LANNERalso develop multi-modal variation expected-duration planner, called DURarch . Instead assigning action single deterministic duration equal expected value, plannerassigns probabilistic duration various outcomes means different modesdistribution probabilities probability mass mode. enhancementreflects intuitive understanding multi-modal distributions experiments confirmDURarch produces solutions shorter make-spans DURexp .6.4 Experiments: Planning Stochastic Durationsevaluate techniques solving planning problems involving stochastic durations.compare computation time solution quality (make-span) five planners domainswithout multi-modal duration distributions. also re-evaluate effectivenessmaximum- (MC) average-concurrency (AC) heuristics domains.6.4.1 E XPERIMENTAL ETUPmodify Rover, MachineShop, Artificial domains additionally including uncertaintyaction durations. set experiments, largest problem 4 million world states65536 reachable. algorithms explored 1,000,000 distinct statesinterwoven state space planning. domains contained many 18 actions,actions many 13 possible durations. details domains please referlonger version (Mausam, 2007).68fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSPlanning Time (in sec)600050004000RoverMachine-Shop3000PrunedDURprunDURsampSampledDURhybHybrid20001000DURexpExp-Dur021222324252627282930 ProblemsFigure 15: Planning time comparisons Rover MachineShop domains: Variation along algorithmsinitialized average concurrency (AC) heuristic; DURexp performs best.AlgosDURsampDURhybDURexpAverage Quality Make-SpanRover MachineShop Artificial1.0011.0001.0011.0221.0111.0191.0081.0151.046Table 4: three planners produce near-optimal policies shown table ratiosoptimal make-span.116.4.2 C OMPARING RUNNING IMEScompare algorithms without heuristics reaffirm heuristics significantlyspeed computation problems; indeed, problems large solved withoutheuristics. Comparing amongst find AC beats C regardlessplanning algorithm; isnt surprising since AC sacrifices admissibility.Figure 15 reports running times various algorithms (initialized AC heuristic)Rover Machine-Shop domains durations unimodal. DURexp out-performsplanners substantial margins. algorithm solving comparatively simplerproblem, fewer states expanded thus approximation scales better others solving,example, two Machine-Shop problems, large planners.cases hybridization speeds planning significant amounts, performs better DURexpartificial domain.6.4.3 C OMPARING OLUTION Q UALITYmeasure quality simulating generated policy across multiple trials. report ratioaverage expected make-span optimal expected make-span domains unimodaldistributions Table 4. find make-spans inadmissible heuristic AC par11. optimal algorithm doesnt converge, use best solution found across runs optimal.69fiM AUSAM & W ELD2826241000DURprunPrunedDURsampSampled100J*(s0)Planning time (log scale)10000DURhybHybridDURarchArch-DurDURexpExp-Dur1022DURprunDUR-prunDURsampDUR-samp2018DURhybDUR-hybDURarchDUR-arch16DURexpDUR-exp14313233343536 Problems313233343536 ProblemsFigure 16: Comparisons Machine-Shop domain multi-modal distributions. (a) ComputationTime comparisons: DURexp DURarch perform much better algos. (b) Makespans returned different algos: Solutions returned DURsamp almost optimal. OverallDURarch finds good balance running time solution quality.admissible heuristic C. hybridized planner approximate userdefined bound. experiments, set bound 5% find make-spans returnedalgorithm quite close optimal always differ 5%. DURexpquality guarantees, still solutions returned problems tested upon nearly goodalgorithms. Thus, believe approximation quite useful scaling largerproblems without losing solution quality.6.4.4 ULTIMODAL OMAINSdevelop multi-modal variants domains; e.g., Machine-Shop domain, time fetching paint bimodal (if stock, paint fetched fast, else needs ordered).alternative costly paint action doesnt require fetching paint. Solutions producedDURsamp made use pivots decision epochs starting costly paint action casefetch action didnt terminate within first mode bimodal distribution (i.e. paintstock).running time comparisons shown Figure 16(a) log-scale. find DURexpterminates extremely quickly DURarch far behind. However, make-span comparisons Figure 16(b) clearly illustrate approximations made methods order achieveplanning time. DURarch exhibits good balance planning time solution quality.7. Related Workpaper extends prior work, originally reported several conference publications (Mausam& Weld, 2004, 2005, 2006a, 2006b).Temporal planners may classified using constraint-posting extended state-space methods (discussed earlier Section 4). constraint approach promising, (if any) probabilistic planners implemented using architecture; one exception Buridan (Kush70fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSstochasticdeterministicconcurrentdurativenon-durativeDUR, Tempastic,Concurrent MDP,GSMDP, Prottle,Factorial MDP,FPG, Aberdeen et al.ParagraphTemporal PlanningStep-optimal planning(TP4, Sapa, MIPS(GraphPlan, SATPlan)TLPlan, etc.)non-concurrentdurativenon-durativeTime Dependent MDP,MDPIxTeT, CIRCA,(RTDP, LAO*, etc.)Foss & OnderPlanningClassical PlanningNumerical Resources(HSP, FF, etc.)(Sapa, Metric-FF, CPT)Figure 17: table listing various planners implement different subsets concurrent, stochastic, durative actions.merick, Hanks, & Weld, 1995), performed poorly. contrast, MDP communityproven state-space approach successful. Since powerful deterministic temporal planners,various planning competitions, also use state-space approach, adoptalgorithms combine temporal planning MDPs. may interesting incorporateconstraint-based approaches probabilistic paradigm compare techniquespaper.7.1 Comparison Semi-MDPsSemi-Markov Decision Process extension MDPs allows durative actions take variable time. discrete time semi-MDP solved solving set equations directextension Equations 2. techniques solving discrete time semi-MDPs natural generalizations MDPs. main distinction semi-MDP formulationconcurrent probabilistic temporal planning stochastic durations concerns presence concurrently executing actions model. semi-MDP allow concurrent actionsassumes one executing action time. allowing concurrency actions intermediate decision epochs, algorithms need deal large state action spaces, encounteredsemi-MDPs.Furthermore, Younes Simmons shown general case, semi-MDPs incapable modeling concurrency. problem concurrent actions stochastic continuousdurations needs another model known Generalized Semi-Markov Decision Process (GSMDP)precise mathematical formulation (Younes & Simmons, 2004b).7.2 Concurrency Stochastic, Durative ActionsTempastic (Younes & Simmons, 2004a) uses rich formalism (e.g. continuous time, exogenousevents, expressive goal language) generate concurrent plans stochastic durative actions. Tempastic uses completely non-probabilistic planner generate plan treatedcandidate policy repaired failure points identified. method guaranteecompleteness proximity optimal. Moreover, attention paid towards heuristicssearch control making implementation impractical.GSMDPs (Younes & Simmons, 2004b) extend continuous-time MDPs semi-Markov MDPs,modeling asynchronous events processes. Younes Simmonss approaches handle71fiM AUSAM & W ELDstrictly expressive model due modeling continuous time. solveGSMDPs approximation standard MDP using phase-type distributions. approachelegant, scalability realistic problems yet demonstrated. particular, approximate, discrete MDP model require many states yet still behave differentlycontinuous original.Prottle (Little et al., 2005) also solves problems action language expressiveours: effects occur middle action execution dependent durations supported.Prottle uses RTDP-type search guided heuristics computed probabilistic planninggraph; however, plans finite horizon thus acyclic state space. difficultcompare Prottle approach Prottle optimizes different objective function (probability reaching goal), outputs finite-length conditional plan opposed cyclic planpolicy, guaranteed reach goal.FPG (Aberdeen & Buffet, 2007) learns separate neural network action individuallybased current state. execution phase decision, i.e., whether action needsexecuted not, taken independently decisions regarding actions. way FPG ableeffectively sidestep blowup caused exponential combinations actions. practiceable quickly compute high quality solutions.Rohanimanesh Mahadevan (2001) investigate concurrency hierarchical reinforcementlearning framework, abstract actions represented Markov options. proposealgorithm based value-iteration, focus calculating joint termination conditions rewards received, rather speeding policy construction. Hence, consider possible Markovoption combinations backup.Aberdeen et al. (2004) plan concurrent, durative actions deterministic durationsspecific military operations domain. apply various domain-dependent heuristics speedsearch extended state space.7.3 Concurrency Stochastic, Non-durative ActionsMeuleau et al. Singh & Cohn deal special type MDP (called factorial MDP)represented set smaller weakly coupled MDPs separate MDPs completelyindependent except common resource constraints, reward cost modelspurely additive (Meuleau, Hauskrecht, Kim, Peshkin, Kaelbling, Dean, & Boutilier, 1998; Singh& Cohn, 1998). describe solutions sub-MDPs independently solvedsub-policies merged create global policy. Thus, concurrency actions differentsub-MDPs by-product work. Singh & Cohn present optimal algorithm (similarcombo-elimination used DURprun ), whereas domain specific heuristics Meuleau et al.guarantees. work Factorial MDPs assumes weak coupling existsidentified, factoring MDP hard problem itself.Paragraph (Little & Thiebaux, 2006) formulates planning concurrency regressionsearch probabilistic planning graph. uses techniques like nogood learning mutexreasoning speed policy construction.Guestrin et al. solve multi-agent MDP problem using linear programming (LP) formulation expressing value function linear combination basis functions. assumingbasis functions depend agents, able reduce size LP(Guestrin, Koller, & Parr, 2001).72fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS7.4 Stochastic, Non-concurrent, Durative ActionsMany researchers studied planning stochastic, durative actions absence concurrency.example, Foss Onder (2005) use simple temporal networks generate plansobjective function time component. Simple Temporal Networks allow effective temporalconstraint reasoning methods generate temporally contingent plans.Boyan Littman (2000) propose Time-dependent MDPs model problems actionsconcurrent time-dependent, stochastic durations; solution generates piecewise linear value functions.NASA researchers developed techniques generating non-concurrent plans uncertain continuous durations using greedy algorithm incrementally adds branches straightline plan (Bresina et al., 2002; Dearden, Meuleau, Ramakrishnan, Smith, & Washington, 2003).handle continuous variables uncertain continuous effects, solution heuristicquality policies unknown. Also, since consider limited contingencies,solutions guaranteed reach goal.IxTeT temporal planner uses constraint based reasoning within partial order planning(Laborie & Ghallab, 1995). embeds temporal properties actions constraintsoptimize make-span. CIRCA example system plans uncertain durationsaction associated unweighted set durations (Musliner, Murphy, & Shin, 1991).7.5 Deterministic, Concurrent, Durative ActionsPlanning deterministic actions comparitively simpler problem much workplanning uncertainty based previous, deterministic planning research. instance,interwoven state representation transition function extensions extended state representations TP4, SAPA, TLPlan (Haslum & Geffner, 2001; & Kambhampati, 2003;Bacchus & Ady, 2001).planners, like MIPS AltAltp , also investigated fast generation parallel plansdeterministic settings (Edelkamp, 2003; Nigenda & Kambhampati, 2003) Jensen Veloso(2000) extend problems disjunctive uncertainty.8. Future Workpresented comprehensive set techniques handle probabilistic outcomes, concurrentdurative actions single formalism, direct attention towards different relaxationsextensions proposed model. particular, explore objective functions, infinitehorizon problems, continuous-valued duration distributions, temporally expressive action models,degrees goal satisfaction interruptibility actions.8.1 Extension Cost Functionsplanning problems durative actions (sections 4 beyond) focused make-spanminimization problems. However, techniques quite general applicable (directlyminor variations) variety cost metrics. illustration, consider mixed costoptimization problem addition duration action, also givenamount resource consumed per action, wish minimize sum make-spantotal resource usage. Assuming resource consumption unaffected concurrent73fiM AUSAM & W ELDexecution, easily compute new max-concurrency heuristic. mixed-cost counterpartEquations 12 is:Jt (X)+ Jr (X)=cQt (X, )+ Qr (X, ) 6=cJ - (s)J - (s)(15)Here, Jt single-action MDP assignng costs durations Jr singleaction MDP assigning costs resource consumptions. informed average concurrencyheuristic similarly computed replacing maximum concurrency average concurrency.hybridized algorithm follows fashion, fast algorithm CoMDPsolved using techniques Section 3.lines, objective function minimize make-span given certain maximumresource usage, total amount resource remaining included state-spaceCoMDPs underlying single-action MDPs etc. techniques may used.8.2 Infinite Horizon Problemspaper defined techniques case indefinite horizon problems,absorbing state defined reachable. problems alternative formulationpreferred allows infinite execution discounts future costs multiplyingdiscount factor step. Again, techniques suitably extended scenario.example, Theorem 2 gets modified following:Qk (s, A)1kQk (s, {a1 }) + Ck (A)kX!ikCk ({ai })i=1Recall theorem provides us pruning rule, combo-skipping. Thus, usePruned RTDP new pruning rule.8.3 Extensions Continuous Duration Distributionsconfined actions discrete durations (refer Assumption 3).investigate effects dealing directly continuous uncertainty duration distributions. Let fiT (t)dt probability action ai completing times + + + dt,conditioned action ai finishing time . Similarly, define FiT (t) probabilityaction finishing time + .Let us consider extended state hX, {(a1 , )}i, denotes action a1 started unitsago world state X. Let a2 applicable action started extended state. Define= min(M (a1 )T, (a2 )), denotes maximum possible duration executionaction. Intuitively, time least one action complete.Q - n+1 (hX, {(a1 , )}i, a2 ) =Z0Z0hf1T (t)F20 (t) + J - n (hX1 , {a2 , t}i) dt +hF1T (t)f20 (t) + J - n (hX2 , {a1 , + }i) dt74(16)fi024Time68101010Expected time reach goalExpected Remaining Time action a0Duration Distribution a0P LANNING URATIVE ACTIONS TOCHASTIC OMAINS8642024Time681086420246810TimeFigure 18: durations continuous (real-valued) rather discrete, may infinite numberpotentially important decision epochs. domain, crucial decision epoch could requiredtime (0, 1] depending length possible alternate plans.X1 X2 world states obtained applying deterministic actions a1 a2respectively X. Recall J - n+1 (s) = mina Q - n+1 (s, a). fixed point computationform, desire Jn+1 Jn functional form12 . Going equationseems difficult achieve, except perhaps specific action distributionsspecial planning problems. example, distributions constantconcurrency domain, equations easily solvable. interesting cases,solving equations challenging open question.Furthermore, dealing continuous multi-modal distributions worsens decision epochsexplosion. illustrate help example.Example: Consider domain Figure 7 except let action a0 bimodal distribution,two modes uniform 0-1 9-10 respectively shown Figure 18(a). Alsolet a1 small duration. Figure 18(b) shows expected remaining termination timesa0 terminates time 10. Notice due bimodality, expected remaining execution timeincreases 0 1. expected time reach goal using plan h{a0 , a1 }; a2 shownthird graph. suppose, started {a0 , a1 }, need choose next decisionepoch. easy see optimal decision epoch could point 0 1would depend alternative routes goal. example, duration b0 7.75,optimal time-point start alternative route 0.5 (right expected time reach goalusing first plan exceeds 7.75).Thus, choice decision epochs depends expected durations alternative routes.values known advance, fact ones calculated planningphase. Therefore, choosing decision epochs ahead time seem possible. makesoptimal continuous multi-modal distribution planning problem mostly intractable reasonablesized problem.8.4 Generalizing TGP Action Modelassumption TGP style actions enables us compute optimal policies, since prunenumber decision epochs. case complex action models like PDDL2.1 (Fox & Long, 2003),old, deterministic state-space planners incomplete. reasons, algorithms12. idea exploited order plan continuous resources (Feng, Dearden, Meuleau, & Washington,2004).75fiM AUSAM & W ELDincomplete problems PPDDL2.1 . Recently, Cushing et al. introduced Tempo, statespace planner, uses lifting time achieve completeness (Cushing, Kambhampati,Mausam, & Weld, 2007). pursuit finding complete, state-space, probabilistic plannercomplex action models, natural step consider Tempo-like representation probabilisticsetting. working details seems relatively straightforward, important researchchallenge find right heuristics streamline search algorithm scale.8.5 Extensionsseveral extensions basic framework suggested. differentconstruct introduces additional structure need exploit knowledge order designfast algorithms. Many times, basic algorithms proposed paper may easily adaptedsituations, sometimes may not. list two important extensions below.Notion Goal Satisfaction: Different problems may require slightly different notionsgoal reached. example, assumed thus far goal officiallyachieved executed actions terminated. Alternatively, one might consider goalachieved satisfactory world state reached, even though actions maymidst execution. intermediate possibilities goal requires specificactions necessarily end. changing definition goal set, problemsmodeled CoMDP. hybridized algorithm heuristics easily adaptedcase.Interruptible Actions: assumed that, started, action cannot terminated.However, richer model may allow preemptions, well continuation interruptedaction. problems, actions could interrupted will, significantlydifferent flavor. Interrupting action new kind decision requires full studymight action termination useful. large extent, planning similarfinding different concurrent paths goal starting together, since onealways interrupt executing paths soon goal reached. instance, exampleFigure 7 longer holds since b0 started time 1, later terminated neededshorten make-span.8.6 Effect Large Durationsweakness extended-state space approaches, deterministic well probabilisticsettings, dependence absolute durations (or accurate, greatest commondivisor action durations). instance, domain action large duration,say 100 another concurrently executable action duration 1, world statesexplored tuples (a, 1), (a, 2), . . ., (a, 98), (a, 99). general, many statesbehave similarly certain decision boundaries important. Start bexecuting 50 units c otherwise one example decision boundary.Instead representing flat discrete states individually, planning aggregate spacestate represents several extended states help alleviate inefficiency.However, obvious achieve aggregation automatically, since adaptingwell-known methods aggregation hold case. instance, SPUDD (Hoey76fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSet al., 1999) uses algebraic decision diagrams represent abstract states Jvalue. Aggregating valued states may enough us, since expected timecompletion depends linearly amount time left longest executing action. So,states differ amount time action executing ableaggregate together. similar way, Feng et al. (2004) use piecewise constant piecewiselinear representations adaptively discretize continuous variables. case, |A|variables. executing active given time, modelingsparse high-dimensional value function easy either. able exploit structure dueaction durations essential future direction order scale algorithms complex realworld domains.9. ConclusionsAlthough concurrent durative actions stochastic effects characterize many real-world domains, planners handle challenges concert. paper proposes unified statespace based framework model solve problems. State space formulations populardeterministic temporal planning well probabilistic planning. However,features bring additional complexities formulation afford new solution techniques.develop DUR family algorithms alleviates complexities. evaluate techniques running times qualities solutions produced. Moreover, study theoreticalproperties domains also identify key conditions fast, optimal algorithmspossible. make following contributions:1. define Concurrent MDPs (CoMDP) extension MDP model formulatestochastic planning problem concurrent actions. CoMDP cast back newMDP extended action space. action space possibly exponentialnumber actions, solving new MDP naively may take huge performance hit.develop general notions pruning sampling speed algorithms. Pruningrefers pruning provably sub-optimal action-combinations state, thus performing less computation still guaranteeing optimal solutions. Sampling-based solutionsrely intelligent sampling action-combinations avoid dealing exponentialnumber. method converges orders magnitude faster methods producesnear-optimal solutions.2. formulate planning concurrent, durative actions CoMDP two modifiedstate spaces aligned epoch, interwoven epoch. aligned epoch based solutionsrun fast, interwoven epoch algorithms yield much higher quality solutions. also define two heuristic functions maximum concurrency (MC), average concurrency (AC)guide search. MC admissible heuristic, whereas AC, inadmissible, typically more-informed leading better computational gains. call algorithms DURfamily algorithms. subscripts samp prun refer sampling pruning respectively,optional superscripts AC MC refer heuristic employed, optional ""DUR notifies problem stochastic durations. example, Labeled RTDPdeterministic duration problem employing sampling started AC heuristicabbreviated DURACsamp .77fiM AUSAM & W ELD3. also develop general technique hybridizing two planners. Hybridizing interwovenepoch aligned-epoch CoMDPs yields much efficient algorithm, DURhyb .algorithm parameter, varied trade-off speed optimality.experiments, DURhyb quickly produces near-optimal solutions. larger problems,speedups algorithms quite significant. hybridized algorithm alsoused anytime fashion thus producing good-quality proper policies (policiesguaranteed reach goal) within desired time. Moreover, idea hybridizing twoplanners general notion; recently applied solving general stochastic planningproblems (Mausam, Bertoli, & Weld, 2007).4. Uncertainty durations leads complexities addition state actionspaces, also blowup branching factor number decision epochs.bound space decision epochs terms pivots (times actions may potentially terminate) conjecture restrictions, thus making problem tractable.also propose two algorithms, expected duration planner (DURexp ) archetypalduration planner (DURarch ), successively solve small planning problemslimited duration uncertainty, respectively. DURarch also able make useadditional structure offered multi-modal duration distributions. algorithms performmuch faster techniques. Moreover, DURarch offers good balanceplanning time vs. solution quality tradeoff.5. Besides focus stochastic actions, expose important theoretical issues relateddurative actions repercussions deterministic temporal planners well.particular, prove common state-space temporal planners incomplete faceexpressive action models, e.g., PDDL2.1 , result may strong impactfuture temporal planning research (Cushing et al., 2007).Overall, paper proposes large set techniques useful modeling solvingplanning problems employing stochastic effects, concurrent executions durative actionsduration uncertainties. algorithms range fast suboptimal solutions, relatively slowoptimal. Various algorithms explore different intermediate points spectrum alsopresented. hope techniques useful scaling planning techniques realworld problems future.Acknowledgmentsthank Blai Bonet providing source code GPT well comments coursework. thankful Sumit Sanghai theorem proving skills advice variousstages research. grateful Derek Long anonymous reviewers papergave several thoughtful suggestions generalizing theory improving claritytext. also thank Subbarao Kambhampati, Daniel Lowd, Parag, David Smith othersprovided useful comments drafts parts research. work performed University Washington 2003 2007 supported generous grants NationalAeronautics Space Administration (Award NAG 2-1538), National Science Foundation (AwardIIS-0307906), Office Naval Research (Awards N00014-02-1-0932, N00014-06-1-0147)WRF / TJ Cable Professorship.78fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSReferencesAberdeen, D., Thiebaux, S., & Zhang, L. (2004). Decision-theoretic military operations planning.ICAPS04.Aberdeen, D., & Buffet, O. (2007).gradients. ICAPS07.Concurrent probabilistic temporal planning policy-Bacchus, F., & Ady, M. (2001). Planning resources concurrency: forward chainingapproach. IJCAI01, pp. 417424.Barto, A., Bradtke, S., & Singh, S. (1995). Learning act using real-time dynamic programming.Artificial Intelligence, 72, 81138.Bertsekas, D. (1995). Dynamic Programming Optimal Control. Athena Scientific.Blum, A., & Furst, M. (1997). Fast planning planning graph analysis. Artificial Intelligence,90(12), 281300.Bonet, B., & Geffner, H. (2003). Labeled RTDP: Improving convergence real-time dynamicprogramming. ICAPS03, pp. 1221.Bonet, B., & Geffner, H. (2005). mGPT: probabilistic planner based heuristic search. JAIR,24, 933.Boutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptionscomputational leverage. J. Artificial Intelligence Research, 11, 194.Boyan, J. A., & Littman, M. L. (2000). Exact solutions time-dependent MDPs. NIPS00, p.1026.Bresina, J., Dearden, R., Meuleau, N., Smith, D., & Washington, R. (2002). Planning continuous time resource uncertainty : challenge AI. UAI02.Chen, Y., Wah, B. W., & Hsu, C. (2006). Temporal planning using subgoal partitioning resolution sgplan. JAIR, 26, 323.Cushing, W., Kambhampati, S., Mausam, & Weld, D. S. (2007). temporal planning reallytemporal?. IJCAI07.Dearden, R., Meuleau, N., Ramakrishnan, S., Smith, D. E., & Washington, R. (2003). IncrementalContingency Planning. ICAPS03 Workshop Planning Uncertainty Incomplete Information.Do, M. B., & Kambhampati, S. (2001). Sapa: domain-independent heuristic metric temporalplanner. ECP01.Do, M. B., & Kambhampati, S. (2003). Sapa: scalable multi-objective metric temporal planner.JAIR, 20, 155194.Edelkamp, S. (2003). Taming numbers duration model checking integrated planningsystem. Journal Artificial Intelligence Research, 20, 195238.79fiM AUSAM & W ELDFeng, Z., Dearden, R., Meuleau, N., & Washington, R. (2004). Dynamic programming structured continuous Markov decision processes. UAI04, p. 154.Foss, J., & Onder, N. (2005). Generating temporally contingent plans. IJCAI05 WorkshopPlanning Learning Apriori Unknown Dynamic Domains.Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal planningdomains.. JAIR Special Issue 3rd International Planning Competition, 20, 61124.Gerevini, A., & Serina, I. (2002). LPG: planner based local search planning graphsaction graphs. AIPS02, p. 281.Guestrin, C., Koller, D., & Parr, R. (2001). Max-norm projections factored MDPs. IJCAI01,pp. 673682.Hansen, E., & Zilberstein, S. (2001). LAO*: heuristic search algorithm finds solutionsloops. Artificial Intelligence, 129, 3562.Haslum, P., & Geffner, H. (2001). Heuristic planning time resources. ECP01.Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planning using decisiondiagrams. UAI99, pp. 279288.Jensen, R. M., & Veloso, M. (2000). OBDD=based universal planning synchronized agentsnon-deterministic domains. Journal Artificial Intelligence Research, 13, 189.Kushmerick, N., Hanks, S., & Weld, D. (1995). algorithm probabilistic planning. ArtificialIntelligence, 76(1-2), 239286.Laborie, P., & Ghallab, M. (1995). Planning sharable resource constraints. IJCAI95, p.1643.Little, I., Aberdeen, D., & Thiebaux, S. (2005). Prottle: probabilistic temporal planner.AAAI05.Little, I., & Thiebaux, S. (2006). Concurrent probabilistic planning graphplan framework.ICAPS06.Long, D., & Fox, M. (2003). 3rd international planning competition: Results analysis.JAIR, 20, 159.Mausam (2007). Stochastic planning concurrent, durative actions. Ph.d. dissertation, University Washington.Mausam, Bertoli, P., & Weld, D. (2007). hybridized planner stochastic domains. IJCAI07.Mausam, & Weld, D. (2004). Solving concurrent Markov decision processes. AAAI04.Mausam, & Weld, D. (2005). Concurrent probabilistic temporal planning. ICAPS05, pp. 120129.80fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINSMausam, & Weld, D. (2006a). Challenges temporal planning uncertain durations.ICAPS06.Mausam, & Weld, D. (2006b). Probabilistic temporal planning uncertain durations.AAAI06.Meuleau, N., Hauskrecht, M., Kim, K.-E., Peshkin, L., Kaelbling, L., Dean, T., & Boutilier, C.(1998). Solving large weakly coupled Markov Decision Processes. AAAI98, pp.165172.Musliner, D., Murphy, D., & Shin, K. (1991). World modeling dynamic constructionreal-time control plans. Artificial Intelligence, 74, 83127.Nigenda, R. S., & Kambhampati, S. (2003). Altalt-p: Online parallelization plans heuristicstate search. Journal Artificial Intelligence Research, 19, 631657.Penberthy, J., & Weld, D. (1994). Temporal planning continuous change. AAAI94, p. 1010.Rohanimanesh, K., & Mahadevan, S. (2001). Decision-Theoretic planning concurrent temporally extended actions. UAI01, pp. 472479.Singh, S., & Cohn, D. (1998). dynamically merge markov decision processes. NIPS98.MIT Press.Smith, D., & Weld, D. (1999). Temporal graphplan mutual exclusion reasoning. IJCAI99,pp. 326333 Stockholm, Sweden. San Francisco, CA: Morgan Kaufmann.Vidal, V., & Geffner, H. (2006). Branching pruning: optimal temporal pocl planner basedconstraint programming. AIJ, 170(3), 298335.Younes, H. L. S., & Simmons, R. G. (2004a). Policy generation continuous-time stochasticdomains concurrency. ICAPS04, p. 325.Younes, H. L. S., & Simmons, R. G. (2004b). Solving generalized semi-markov decision processesusing continuous phase-type distributions. AAAI04, p. 742.Zhang, W., & Dietterich, T. G. (1995). reinforcement learning approach job-shop scheduling.IJCAI95, pp. 11141120.AppendixProof Theorem 6prove statement Theorem 6, i.e., actions TGP-style set pivotssuffices optimal planning. proof make use fact actions TGP-styleconsistent execution concurrent plan requires two executing actions non-mutex(refer Section 5 explanation that). particular, none effects conflictprecondition one conflict effects another.prove theorem contradition. Let us assume problem optimal solutionrequires least one action start non-pivot. Let us consider one optimal plans,81fiM AUSAM & W ELDfirst non-pivot point action needs start non-pivot minimized. Letus name time point let action starts point a. prove caseanalysis may, well, start time 1 without changing nature plan. 1also non-pivot contradict hypothesis minimum first non-pivot point.1 pivot hypothesis contradicted "need to" start non-pivot.prove left-shifted 1 unit, take one trajectory time (recallactions could several durations) consider actions playing role 1, t, + (a) 1,+ (a), (a) refers duration trajectory. Considering pointssuffice, since system state change points trajectory. proveexecution none actions affected left shift. following twelvecases:1. actions b start 1: b cant end (t non-pivot). Thus b executeconcurrently t, implies b non-mutex. Thus b may well start together.2. actions b continue execution 1: Use argument similar case 1 above.3. actions b end 1: b TGP-style, effects realized open intervalending 1. Therefore, start conflict end b.4. actions b start t: b start together hence dependentpreconditions. Also, non-mutex, starting times shifteddirection.5. actions b continue execution t: b started 1 refer case 1 above. not,1 similar points b.6. actions b end t: Case possible due assumption non-pivot.7. actions b start + (a) 1: Since continued execution point, bnon-mutex. Thus effects clobber bs preconditions. Hence, b still executedrealizing effects.8. actions b continue execution + (a) 1: b non-mutex, may endearlier without effect b.9. actions b end + (a) 1: b executing concurrently. Thusnon-mutex. may end together.10. actions b start + (a): b may still start + (a), since state + (a)doesnt change.11. actions b continue execution + (a): b started + (a) 1 refer case7 above, else state change + (a) cause effect b.12. actions b end + (a): b non-mutex executing concurrently. Thus, effects dont clobber bs preconditions. Hence, may end earlier.Since left shifted trajectories, therefore left-shift legal. Also,multiple actions start may shifted one one using argument.Hence Proved. 282fi