Journal Arti ial Intelligen e Resear h 31 (2008) 473-495

Submitted 03/07; published 03/08

Axiomati Foundations Ranking Systems
epsalonstanford.edu

Alon Altman

Department Computer ien e
Stanford University
Stanford, CA 94305-9020 USA

moshetie.te hnion.a .il

Moshe Tennenholtz

Fa ulty Industrial Engineering Management
Te hnion Israel Institute Te hnology
Haifa 32000, Israel

Abstra

Reasoning agent preferen es set alternatives, aggregation su h
preferen es ial ranking fundamental issue reasoning multi-agent
systems. set agents set alternatives oin ide, get ranking
systems setting. famous type ranking systems page ranking systems ontext
sear h engines. paper present extensive axiomati study ranking systems.
parti ular, onsider two fundamental axioms: Transitivity, Ranked Independen e
Irrelevant Alternatives. Surprisingly, nd general ial ranking rule
satises requirements. Furthermore, show impossibility result holds
various restri tions lass ranking problems onsidered. However,
transitivity weakened, interesting possibility result obtained. addition, show
omplete axiomatization approval voting using ranked IIA.
1. Introdu tion
ranking agents based agents' input fundamental multi-agent systems
(see e.g. Resni k, Ze khauser, Friedman, & Kuwabara, 2000). Moreover, ome
entral ingredient variety Internet sites, perhaps famous examples
Google's PageRank algorithm (Page, Brin, Motwani, & Winograd, 1998) eBay's
reputation system (Resni k & Ze khauser, 2001).
basi problem introdu es new ial hoi e model.

lassi al theory

ial hoi e, manifested Arrow (1963), set agents/voters alled rank set
alternatives. Given agents' input, i.e. agents' individual rankings, ial ranking
alternatives generated.

theory studies desired properties aggregation

agents' rankings ial ranking.

parti ular, Arrow's elebrated impossibility

theorem (Arrow, 1963) shows aggregation rule satises minimal
requirements, relaxing requirements appropriate ial aggregation
rules dened.

novel feature ranking systems setting set

agents set alternatives

oin ide.

Therefore, su h setting one may need

onsider transitive ee ts voting. example, agent
(i.e.

votes for) agent

importan e agent

c;

b



reports importan e

may inuen e redibility report

b



indire ee ts onsidered wish aggregate

information provided agents ial ranking.

2008 AI ess Foundation. rights reserved.


fiAltman & Tennenholtz
Noti e natural interpretation/appli ation setting ranking Internet
pages. ase, set agents represents set Internet pages, links
page

p

Q viewed two-level ranking
p agents (pages) whi h Q.

set pages

preferred agent (page)

agents

Q



problem nding

appropriate ial ranking ase fa problem (global) page ranking.
Parti ular approa hes obtaining useful page ranking implemented sear h
engines su h Google (Page et al., 1998).
theory ial hoi e onsists two omplementary axiomati perspe tives:



des riptive perspe tive: given parti ular rule

r

aggregation individual

rankings ial ranking, nd set axioms sound omplete
is, nd set requirements

r

satises; moreover, every ial aggregation

rule satises requirements oin ide
axiomatization termed

r.

representation theorem

r.

result showing su h

aptures exa essen e

(and assumptions behind) use parti ular rule.



normative perspe tive: devise set requirements ial aggregation rule
satisfy, try nd whether ial aggregation rule satises
requirements.

Mu h eort invested des riptive approa h framework lassi al
theory ial hoi e.

setting, representation theorems presented

major voting rules su h majority rule (May, 1952; see Moulin, 1991 overview).
ranking systems setting, su essfully applied des riptive perspe tive
providing representation theorem (Altman & Tennenholtz, 2005b) well-known
PageRank algorithm (Page et al., 1998), whi h basis Google's sear h te hnology
(Brin & Page, 1998).
ex ellent example normative perspe tive Arrow's impossibility theorem
mentioned above. Tennenholtz (2004) presented preliminary results ranking
systems set voters set alternatives oin ide. However, axioms
presented work onsist several strong requirements whi h naturally lead
impossibility result. Still normative approa h ranking systems, ta kled
issue entives (Altman & Tennenholtz, 2006b, 2006 ), positive negative
results. ently, onsidered variation ranking systems, personalized
ranking generated every parti ipant system (Altman & Tennenholtz, 2006a),
surprisingly dierent results.
paper provide extensive study ranking systems.

introdu e two

fundamental axioms. One axioms aptures transitive ee ts voting ranking systems, adapts Arrow's well-known independen e irrelevant alternatives (IIA) axiom ontext ranking systems. Surprisingly, nd general
ranking system simultaneously satisfy two axioms! result means
would like fully apture transitive ee ts, ranking de isions must made globally,
based numeri al ulations. show impossibility result holds
various restri tions lass ranking problems onsidered.
hand, show positive result ase transitivity axiom
relaxed. new ranking system pra ti al useful algorithm provided

474

fiAxiomati Foundations Ranking Systems
omputation. Finally, use IIA axiom present positive result form
representation theorem well-known approval voting ranking system, whi h ranks
agents based number votes eived.

axiomatization shows

ignoring transitive ee ts, one ranking system satises IIA axiom.
paper stru tured follows: Se tion 2 formally denes setting notion
ranking systems. Se tions 3 4 introdu e axioms Transitivity Ranked Independen e Irrelevant Alternatives respe tively. main impossibility result presented
Se tion 5, strengthened Se tion 6. main positive result, form
ranking system satisfying weaker version transitivity given Se tion 7,
axiomatization Approval Voting ranking system presented Se tion 8. Finally,
luding remarks given Se tion 9.

2. Ranking Systems
des ribing results regarding ranking systems, must rst formally dene
mean words ranking system terms graphs linear orderings:

Denition 2.1.

Let



set. relation

reexive, transitive, omplete. Let

Notation

.

2.2

Let



ab



R AA

alled

ordering

denote set orderings

ordering,



. Formally, b
b a.



stri order indu ed


L(A)

equality predi ate

ab



b a;

,







A.

ab








Given dene ranking system is:

Denition 2.3.

system F
ordering

Let

GV

FG L(V ).

otherwise alled



F

V . ranking
G GV

set dire ted graphs vertex set

fun tional every nite vertex set

V

maps graphs

partial fun tion alled

general ranking system.

partial ranking system,

One view setting variation/extension lassi al theory ial hoi e
modeled Arrow (1963). ranking systems setting diers two main properties.
First, setting assume set voters set alternatives oin ide,
se ond, allow agents two levels preferen e alternatives, opposed
Arrow's setting agents ould rank alternatives arbitrarily.
two-level limitation important order avoid Arrow-style impossibility results.
Indeed, di hotomous (i.e. two level) setting su h results apply (Bogomolnaia,
Moulin, & Stong, 2005). allowed general rankings input system,
would rea hed impossibility results dire result Arrow-style impossibility.
adding di hotomous limitation, ensure results onsequen e
o-in iden e voters alternatives related transitive ee ts.

2.1 Examples Ranking Systems
order make abstra denition ranking systems rete, shall
give examples several well-known ranking systems.

order dene

systems, throughout paper, shall use following notation:

475

fiAltman & Tennenholtz



e
c

f


b

Figure 1: Example graph ranking systems.

Notation

.

G = (V, E) graph v V vertex. Let PG (v) ,
{u|(u, v) E} SG (v) , {u|(v, u) E} denote prede essor su essor sets v
G respe tively. G understood ontext, sloppily use P (v) S(v).
2.4

Let

Approval Voting simple ranking system ranks agents ording
number votes (i.e. oming edges) have. Formally,

Denition 2.5.



approval voting ranking system AV

ranking system dened by:

v1 AV
G v2 |PG (v1 )| |PG (v2 )|
Consider graph Figure 1.

f cde

AV

ranking system would rank graph

based fa verti es

{a, b}, {f },



{c, d, e}

ab

0, 1,

2 prede essors respe tively. full axiomatization approval voting ranking system
given se tion 8.
One major appli ation Ranking Systems ontext Internet pages.
ontext, represent Internet dire ted graph, verti es websites,
edges links websites.

prominent ranking system setting

PageRank (Page et al., 1998), whi h based random walk Internet graph.
Namely, pro ess start random page, iteratively move one pages
linked urrent page, assigning equal probabilities ea h su h page.
dene PageRank matrix whi h aptures random walk reated PageRank
pro edure:

Denition 2.6.
ank Matrix AG

Let

G = (V, E) graph, assume V = {v1 , v2 , . . . , vn }.
n n) dened as:

1/|SG (vj )| (vj , vi ) E
[AG ]i,j =
0
Otherwise.



PageR-

(of dimension

PageRank pro edure rank pages ording stationary probability distribution obtained limit random walk; formally dened follows:

Denition 2.7.

Let G = (V, E) graph, assume V = {v1 , v2 , . . . , vn }. Let
0 < 1 damping fa tor. Let rPbe unique solution system (1 d) AG
ri = n. unique solution,
r + ( 1 1 1 )T = r
ranking dened. Otherwise, PageRank P RG (vi ) vertex vi V dened
P RG (vi ) = ri . PageRank ranking system ranking system vertex set
V maps G PGR , PGR dened as: vi , vj V : vi PGR vj
P RG (vi ) P RG (vj ).

476

fiAxiomati Foundations Ranking Systems

b




c

Figure 2: Example Transitivity

shown

> 0,

indeed unique solution thus ranking

system general one. However,

= 0 ranking system omes partial ranking

system, always well dened.

= 0.2

graph Figure 1,

(0.2, 0.2, 0.52, 1.7, 1.77, 1.61)

PageRank values assigned

giving ranking

b c f e.

a...f



Note

ranking diers one assigned approval voting, neither rankings
renement other.

example shows PageRank Approval Voting

distin ranking systems, two may disagree ranking two verti es.
soon see systems satisfy two mutually ex lusive properties ranking systems.

3. Transitivity
basi property one would assume ranking systems agent
ranked higher agent

b,

agent



a's

voters

ranked higher agent

b.

notion formally aptured below:

Denition 3.1.

F ranking system. say F satises strong transitivity
graphs G = (V, E) verti es v1 , v2 V : Assume 1-1 mapping (but
ne essarily onto) f : P (v1 ) 7 P (v2 ) s.t. v P (v1 ): v f (v). Then, v1 v2 .
assume either f onto v P (v1 ): v f (v). Then, v1 v2 .
Let

explain formal denition aptures intuition, onsider simple graph

abc
intuition tells us

c

thus ranked higher


b

end vote hain trusted,

b,

ause fa

vote ompared

denition above:

must



b

trusted

a,

due

none. intuition orre tly aptured

ranked stri tly

b ause

fun tion mapping

P (a) =

P (b) = {a} onto, b must ranked stri tly c ause trivial mapping
P (b) = {a} P (c) = {b} satises b, thus get b c, expe ted.
involved example, onsider graph G Figure 2 ranking system
F satises strong transitivity. F must rank vertex verti es,
F
prede essors, unlike verti es. assume G b, strong transitivity
F
F
must lude b G c well. must lude b G (as b's
prede essor ranked lower a's prede essor c, additional prede essor d),
F
whi h leads ontradi tion. Given b G a, transitivity, must lude
c FG b, ranking graph G satises strong transitivity FG c FG
b FG a.




477

fiAltman & Tennenholtz
Tennenholtz (2004) suggested algorithm denes ranking system satises strong transitivity iteratively rening ordering verti es starting
ranking suggested approval voting.
Note PageRank ranking system satisfy strong transitivity. due
fa PageRank redu es weight links (or votes) nodes whi h
higher out-degree. Thus, assuming Yahoo! Mi rosoft equally ranked, link
Yahoo! means less link Mi rosoft, ause Yahoo! links external pages
Mi rosoft. Noting fa t, weaken denition transitivity require
prede essors ompared agents equal out-degree:

Denition 3.2.

Let F ranking system. say F satises weak transitivity
G = (V, E) verti es v1 , v2 V : Assume 1-1 mapping
f : P (v1 ) 7 P (v2 ) s.t. v P (v1 ): v f (v) |S(v)| = |S(f (v))|. Then, v1 v2 .
assume either f onto v P (v1 ): v f (v). Then, v1 v2 .

graphs

example weak transitivity, one onsider strong transitivity example
above, still applies weak transitivity.
PageRank ranking system satises weakened version transitivity. due
fa that:

P R(v1 ) =

X P R(v)
X P R(f (v))


|S(v)|
|S(f (v))|

vP (v1 )
ase

v f (v)

vP (v1 )



v P (v1 )

X P R(v)
= P R(v2 ).
|S(v)|

vP (v2 )

rst inequality stri t,

f



onto se ond inequality stri t.

4. Ranked Independen e Irrelevant Alternatives
standard assumption ial hoi e settings agent's relative rank
depend (some property ) agents voted them. Su h axioms usually
alled independen e irrelevant alternatives (IIA) axioms. setting, su h IIA axioms
mean agent's rank must depend property immediate prede essors.
setting, require relative ranking two agents must depend
pairwise omparisons ranks prede essors, identity ardinal
value. IIA axiom, alled

ranked

IIA, diers one suggested Arrow (1963)

fa onsider identity voters, rather relative rank.

F
b c e f . look
omparison c d. c's prede essors, b, ranked equally,
ranked lower d's prede essor f . also true onsidering e f e's
prede essors c ranked equally, ranked lower f 's prede essor
e. Therefore, agree ranked IIA, relation c d, relation
e f must same, whi h indeed c e f . However,
situation also urs omparing c f (c's prede essors b equally
ranked ranked lower f 's prede essor e), ase c f . three ases
example, onsider graph Figure 3. Furthermore, assume ranking system

ranked verti es graph following:

involve omparing two verti es, one two weaker prede essors one one stronger

478

fiAxiomati Foundations Ranking Systems


c

e

b

f


Figure 3: example RIIA.

b



Figure 4: Graph prole

h(1, 1), (2)i.

prede essor, ome omparisons onsistent.
lude ranking system

F

Therefore,

whi h produ ed rankings satisfy ranked

IIA.
formally dene ondition, one must onsider possibilities omparing two
nodes graph based ordinal omparisons prede essors.



possibilities omparison proles:

Denition 4.1. omparison prole pair ha, bi = (a1 , . . . , ), b = (b1 , . . . , bm ),
a1 , . . . , , b1 , . . . , bm N, a1 a2 , b1 b2 bm . Let P set
su h proles.
ranking system

satisfy


F,

G = (V, E), pair verti es v1 , v2 V said
ha, bi exist 1-1 mappings f1 : P (v1 ) 7 {1 . . . n}
given f : ({1} P (v1 )) ({2} P (v2 )) 7 N dened

graph

su h omparison prole

f2 : P (v2 ) 7 {1 . . . m}

su h

as:

f (1, v) = af1 (v)
f (2, u) = bf2 (u) ,
f (i, x) f (j, y) x FG
Consider prole



(i, x), (j, y) ({1} P (v1 )) ({2} P (v2 )).

h(1, 1), (2)i.

omparison prole illustrates basi question

omparing agent got two low-rank votes one got one high-rank vote.
question unde ided transitivity alone, assume transitivity omparison
prole satised pair
maps prede essors



(a, b) graph Figure
b 1 2 respe tively.

4.

f

fun tion simply



require every su h prole ranking system ranks nodes onsistently:

479

fiAltman & Tennenholtz
Denition 4.2.

F satises ranked independen e
f : P 7 {0, 1} su h every
graph G = (V, E) every pair verti es v1 , v2 V every omparison prole
p P v1 v2 satisfy, v1 FG v2 f (p) = 1.
Let

F

ranking system. say

irrelevant alternatives (RIIA)

Notation

.

4.3

fun tion

ab
b a.

use notation
mean

exists mapping

ab



f

denition understood ontext,

mean

f ha, bi = 1, b

mean

f hb, ai = 0,



ab

(c, d), (c, f ), (e, f )
h(1, 1), (2)i. seen above, pairs (c, d) (e, f )
(1, 1) (2), (c, f ) entails (1, 1) (2). results ontradi ea h

example, example onsidered above, pairs
satisfy omparison prole
entail

other, therefore lude ranking system produ ed ranking
satisfy RIIA.
denition RIIA formalizes requirement onsisten omparisons
su h one seen above. means ranking system satisfying RIIA must
de ide relative ranking





b

Figure 4, (assuming transitivity) rank

urren es two weak vs. one strong prede essor.
RIIA independen e property, ranking system

F= ,

ranks agents

equally, satises RIIA.

AV also satises RIIA. due
h(a1 , . . . , ), (b1 , . . . bm )i, f fun tion AV

approval voting ranking system
omparison prole

n m.

fa
ranks

ab



use fa axiomatization approval voting present Se tion

8.

5. Impossibility
main result illustrates impossibility satisfying (weak) transitivity RIIA simultaneously.

Theorem 5.1. general ranking system satises weak transitivity RIIA.
Proof.

Assume ontradi tion exists ranking system

transitivity RIIA. Consider rst graph

G1

Figure 5(a).

F

satises weak

Note verti es

graph out-degree 2 0, thus out-degree requirement weak
note a1 a2 must satisfy omparison
pa = ((x, y), (x, y)) ause identi al prede essors. Thus, RIIA, a1 FG1
a2 a2 FG1 a1 , therefore a1 FG1 a2 . weak transitivity, easy see
c FG1 a1 c FG1 b. assume b FG1 a1 , weak transitivity, a1 FG1 b whi h
F
F
ontradi ts assumption. lude c G a1 G b.
1
1
onsider graph G2 Figure 5(b). Again, out-degree requirement weak
F
transitivity trivially satised, RIIA, a1 G a2 . weak transitivity,
2
F
F
F
easy see a1 G c b G c. assume a1 G b, weak transitivity,
2
2
2
F
b G2 a1 whi h ontradi ts assumption. lude b FG2 a1 FG2 c.
Consider omparison prole p = ((1, 3), (2, 2)). Given F , a1 b satisfy p G1
F
F
F
F
F
F
(be ause c G a1 G a2 G b) G2 (be ause b G a1 G a2 G c). Thus,
1
1
1
2
2
2
transitivity trivially fullled.

prole

480

fiAxiomati Foundations Ranking Systems


b

a1

a2


(a) Graph G1
a1


b



a2

(b) Graph G2
Figure 5: Graphs proof Theorem 5.1

RIIA,

b

FG2

a1 FG1 b a1 FG2 b,

whi h ontradi tion fa

a1 FG1 b



a1 .

result quite surprise.

Intuitively, would like ranking pro edure

sensitive relative ranking ea h agent's voters (transitivity) inuen ed
seemingly irrelevant information (RIIA). Although requirements may
seem omplementary, impossibility theorem shows requirements fa
ontradi tory.
onsider transitivity basi requirement, learn axiomatization
transitive ranking system annot restri ted lo al ordinal properties. is,
designing ranking system transitivity required, one must hoose whether base
system numeri omputation, ordinal axioms operate global
ale.
example, standard formalism PageRank ranking system Denition 2.7
axiomatization similar system suggested Pala ios-Huerta Volij (2004)
based numeri al omputation, suggested axiomatization (Altman & Tennenholtz, 2005b) uses ordinal axioms global ale. axioms refer invariants
relations ranking dierent graphs, rather pairs verti es
graph.
PageRank example demonstrates ranking systems may dened using
either approa hes. feel numeri approa h suitable dening exe uting ranking systems, global ordinal approa h suitable
axiomati lassi ation.

6. Relaxing Generality
hidden assumption impossibility result fa onsidered general
ranking systems. se tion analyze several spe ial lasses graphs relate
ommon ranking enarios.

481

fiAltman & Tennenholtz
6.1 Small Graphs
natural limitation preferen e graph ap number verti es (agents)
parti ipate ranking. Indeed, three less agents involved ranking, strong transitivity RIIA simultaneously satised. appropriate ranking
algorithm ase one suggested Tennenholtz (2004).

algorithm

simply starts ranking in-degree renes ranking required strong transitivity satised. easy see de isions omparison proles possible
3-vertex graph di tated either in-degree transitivity. Spe ally, prole

h(1, 3), (2, 2)i

used proof impossible su h graphs.

four agents, strong transitivity RIIA annot simultaneously satised (the proof similar Theorem 5.1, vertex



removed

graphs). agents involved, even weak transitivity RIIA
annot simultaneously satised, implied proof Theorem 5.1.

6.2 Single Vote Setting
Another natural limitation domain graphs might interested
restri tion ea h agent (vertex) exa tly one vote (su essor). example, voting
paradigm ould viewed setting every agent votes exa tly one agent.
following proposition shows even simple setting weak transitivity RIIA
annot simultaneously satised.

Proposition 6.1. Let G1 set graphs G = (V, E) su h |S(v)| = 1
v V . partial ranking system G1 satises weak transitivity RIIA.
Proof.

Assume ontradi tion partial ranking system

satises weak transitivity RIIA. Let

F.
G1 G1

f : P 7 {0, 1}

F



G1



mapping denition

RIIA

x1 FG1 x2 FG1 b FG1 a.
(a, b) satises omparison prole h(1, 1, 2), (3)i, must (3) (1, 1, 2). let
G2 G1 graph Figure 6b. weak transitivity x1 FG2 x2 FG2 FG2 FG2 b.
(b, a) satises omparison prole h(2, 3), (1, 4)i, must (1, 4) (2, 3).
Let G3 G1 graph Figure 6 . weak transitivity easy see
x1 FG3 FG3 x7 FG3 y1 FG3 y2 FG3 c FG3 d. Furthermore, weak transitivity
F

F

F
F
F
lude G b G b c G d; y1 G b x3 G d.
3
3
3
3
3

F
F
F

onsider vertex pair (c, b ). shown x1 G x2 G y1 G b. So, (c, b )
3
3
3

F
satises omparison prole h(1, 1, 2), (3)i, thus RIIA b G c. onsider
3

F

F
F
vertex pair (b, a). already shown G b G c G d. So, (a, b) satises
3
3
3
F
omparison prole h(2, 3), (1, 4)i, thus RIIA b G a. However, already shown
3
F
G b ontradi tion. Thus, ranking system F annot exist.
3
Let

graph Figure 6a. weak transitivity,

6.3 Bipartite Setting
world reputation systems (Resni k et al., 2000), frequently observe distin tion
two types agents su h ea h type agent ranks agents

482

fiAxiomati Foundations Ranking Systems

x1



b

x2

x2



x1



(a) Graph G1
x4

b

(b) Graph G2

y2

x5



b

b'

x6

a'

x7


x1


x2

x3

y1

( ) Graph G3
Figure 6: Graphs proof proposition 6.1

type. example buyers intera sellers vi e versa. type limitation
aptured requiring preferen e graphs bipartite, dened below.

Denition 6.2.
V = V 1 V2 ,

G = (V, E) alled bipartite exist V1 , V2 su h
V1 V2 = , E (V1 V2 ) (V2 V1 ). Let GB set bipartite
graph

graphs.
impossibility result extends limited domain bipartite graphs.

Proposition 6.3. partial ranking system
transitivity RIIA.
Proof.

proof exa tly

G1 ,

GB G1 satises weak

onsidering graphs Figure 6

bipartite.

6.4 Strongly Conne ted Graphs
well-known PageRank ranking system (ideally) dened set strongly onne ted graphs. is, set graphs exists dire ted path
two verti es.
Let us denote set strongly onne ted graphs

GSC .

following proposition

extends impossibility result strongly onne ted graphs.

Proposition 6.4. partial ranking system

tivity RIIA.

483

GSC satises weak transi-

fiAltman & Tennenholtz
Proof.

proof similar proof Theorem 5.1, additional vertex

e



graphs edges verti es.

7. Relaxing Transitivity
impossibility result omes possibility result relax transitivity requirement. Instead omparing verti es similar out-degree weak transitivity
axiom above, weaken requirement stri preferen e hold ase
mat hing prede essors one agent preferred



prede essors other.

Denition 7.1.

Let F ranking system. say F satises strong quasi-transitivity
G = (V, E) verti es v1 , v2 V : Assume 1-1 (but
ne essarily onto) mapping f : P (v1 ) 7 P (v2 ) s.t. v P (v1 ): v f (v). Then,
v1 v2 . And, P (v1 ) 6= v P (v1 ): v f (v), v1 v2 .
graphs

Strong quasi transitivity signi antly weaker property strong transitivity,
allows mu h indieren e resulting ranking. Spe ally, ranking system

F=

always ranks verti es equally satises strong quasi transitivity. generally,

ranking system value vertex proportional sum values
subset prede essors satises strong quasi transitivity. shall see examples
quasi-transitive ranking systems below.
require strong

quasi-transitivity RIIA, nd interesting family

ranking systems rank agents ording in-degree, breaking ties omparing
ranks strongest prede essors. ursive in-degree systems work assigning
rational value every vertex, based following idea: rank rst based
in-degree. tie, rank based strongest prede essor's value, on.
Loops ranked periodi al rational numbers base

(n + 1)

period length

loop, ase ontinuing loop maximally ranked option.
ursive in-degree systems dier way dierent in-degrees ompared.
monotone reasing mapping in-degrees ould used initial ranking.



show systems well-dened values al ulated dene
systems algorithmi ally follows:

Denition 7.2.

r : N 7 N monotone nonde reasing fun tion su h r(i)
N. ursive in-degree ranking system rank fun tion r dened follows:
Given graph G = (V, E), let n = |V |. relative ranking two verti es based
Let

numeri al ulation:
r
v2 valuer (v1 ) valuer (v2 ),
v1 RID
G

valuer (v) dened maximizing valuation fun tion vpr () paths lead


v:
valuer (v)

=

max

aPath(v)

vpr (a)

(1)

ensure denition sound, eliminate loops, dene path reverse order:
Path(v)

= { (v = a1 , a2 , . . . , )|m N,
(am , . . . , a1 )

path

484

G (am1 , . . . , a1 )

simple}

fiAxiomati Foundations Ranking Systems



e

0.1232

0.2123

f


b

g

h

0.112123

0.3112123

0.12123

0.2321

0.1


0.3212

0

Figure 7: Values assigned ursive in-degree algorithm

path valuation fun tion vp

: V 7 Q

denes value onform lexi ographi

order in-degrees along path:




r(|P
(a
)|)+
1


1
m=1
0


vpr (a1 , a2 , . . . , ) =
vpr (a2 , . . . , , a2 ) a1 = > 1
n+1

vpr (a2 , . . . , )
Otherwise.

(2)

Note vpr (a1 , a2 , . . . , ) innitely ursive ase path ontains
loop ( .f.

a1 = > 1).

omputation sake redene ase nitely as:

vpr (a1 , . . . , , a1 )

=


X
i=0

=

Example 7.3.


X
r(|P (aj )|)
1
=
mi
(n + 1)
(n + 1)j
j=1

(n + 1)m
vp (a1 , . . . , ).
(n + 1)m 1 r

(3)

example values assigned parti ular graph

identity fun tion given Figure 7.
ursive division

n = 9,



r



denition (2) based

n + 1, values simply de imals whi h onsist atenation

in-degrees along maximal path.
value zero assigned
onsists



itself. value

b



via rst ase (2), path leading

arises path

ursive gives value path
added

r(|P (b)|) = 1

(a)

(b, a)



third ase (2),

whi h seen equal 0.

divided 10, giving result

0.1.

values

c, d, e,





arise loop onsisting verti es. Applying se ond ase (2),
equations

1
[3 + vpr (e, d, c, i, e)]
10
1
[2 + vpr (d, c, i, e, d)]
valuer (e) = vpr (e, d, c, i, e) =
10
1
[1 + vpr (c, i, e, d, c)]
valuer (d) = vpr (d, c, i, e, d) =
10
1
valuer (c) = vpr (c, i, e, d, c) =
[2 + vpr (i, e, d, c, i)]
10
valuer (i)

=

vpr (i, e, d, c, i)

485

=

fiAltman & Tennenholtz
using (3), get periodi de imals seen Figure 7. values verti es


h

f , g,

assigned using third ase (2). Note omplete maximal paths

(e, d, c, i, e)

verti es ontain loop

thus verti es' values lude

periodi de imal part, seen Figure 7.
ursive in-degree system satises interesting xed point property
used fa ilitate e ient omputation:

Proposition 7.4. Let r : N 7 N monotone nonde reasing fun tion su h r(i)
N dene r(0) = 0. value fun tion ursive in-degree ranking
system satises:


valuer (v) =
Proof.

Denote Path



v



(p, v)

1
n+1

0



r(|P (v)|) + maxpP (v) valuer (p) P (v) 6=

set almost-simple dire ted paths

unless immediately looping ba k

Path



(4)

Otherwise
p

whi h pass

p:

(p, v) = { (p = a1 , a2 , . . . , )|
(am , . . . , a1 )

path

G (am1 , . . . , a1 )

simple



{1, . . . , 2, m} : ai 6= v
am1 = v = p}.
Let

vV

vertex. Then,

valuer (v)

=

=

=
=
=

max

aPath(v)

vpr (a)

=


r(|P (v)|) + max(v=a1 ,...,am )Path(v)
1
vpr (a2 , . . . , , a2 ) a1 = > 1 =
n+1
vpr (a2 , . . . , )
Otherwise.
"
#
1
vpr (a) =
r(|P (v)|) + max
max
n+1
pP (v) aPath (p,v)
"
#
1
r(|P (v)|) + max
max vpr (a) =
n+1
pP (v) aPath(p)


1
r(|P (v)|) + max valuer (p) .
n+1
pP (v)


Note (5) equal zero

0



P (v) = ,

required.

holds, assume ontradi tion exists
vpr (a)


wlog
path

> max

p P (v)

max

p P (v) Path (p ,v)

(5)

(6)

show equality (6)



vpr (a



Path(p)
).

su h
(7)

\ Path (p, v), know ai = v {1, . . . , m}. Assume
minimal. Let b denote path (p = a1 , a2 , . . . , ai , p) let c denote
(p = ai+1 , . . . , , aj+1 , . . . , ai+1 ) = aj j < (p = ai+1 , . . . , )


Path(p)

486

fiAxiomati Foundations Ranking Systems

p
v

= (p, x, v, p , x)

x

b = (p, x, v, p)

p

c = (p , x, v, p )
Figure 8: Example paths proof Proposition 7.5.

otherwise. example su h paths given Figure 8. Note

c




Path (p , v),

p, p

vpr (a)

P (v).
=

b

Path



(p, v)



Now, note

(n + 1)j 1
1
vpr (b) +
vp (c),
(n + 1)j
(n + 1)j r

thus vpr (a) must vpr (b) vpr (c), ontradi tion assumption (7).
Note although might look ompelling use xed point property definition ursive-indegree, well dened, loops indu e innite series
maximizations must prove onverges. essen e proof above.
xed point property basis e ient algorithm ursive-indegree provided
below.
shall show ranking system fa satisfy RIIA weakened
version transitivity.

Proposition 7.5. Let r : N 7 N monotone nonde reasing fun tion su h r(i)
N dene r(0) = 0. ursive in-degree ranking system rank fun tion
r satises strong quasi-transitivity RIIA.
Proof.

0 valuer (v) < 1, thus
verti es ordered rst r(|P (v)|) maxpP (v) valuer (p). Therefore, every
omparison prole ha, bi = (a1 , . . . , ak ), b = (b1 , . . . , bl ) ranked follows:
xed point result Proposition 7.4 implies

f ha, bi = 1 (k = 0) (r(k) < r(l)) [(r(k) = r(l)) (ak bl )] .
ranking proles trivially yields strong quasi-transitivity required.
previously presented preliminary version personalized variant ursive in-degree (Altman & Tennenholtz, 2006a). algorithm presented based
equivalent ursive denition value:
valuer (v)

=

vpr (pvr ((), v))

(8)


(v)
P (v) =
/a
v, maxpP (v) pvr (a, v, p) v
pvr (a, v) =

(ak , . . . , , v)
= (a1 , . . . , ak = v, . . . , ),

maximum paths taken vpr (pvr (a, v, p)).

487

(9)

fiAltman & Tennenholtz
Algorithm 1 E ient algorithm ursive in-degree
1. Initialize valuer (v)
2. Let

V

1
n+1 r(|P (v)|)





r(0)

assumed

0.

set verti es oming edges.

|V |

3. Iterate

times:

(a) every vertex

v V :

i. Update valuer (v)
4. Sort

v V,

V

1
n+1



valuer ().

5. Output verti es

V \ V



r(|P (v)|) + maxpP (v) valuer (p) .

weakest, followed verti es

V

sorted

valuer () ending order.

xed point property (4) satises lassi al Bellman prin iple optimality
(Stokey & Lu as, 1989),

v(xt ) = max [F (xt , xt+1 ) + v(xt+1 )] .
Thus, apply dynami programming algorithm e iently ompute values,
seen Algorithm 1. Note due limits size graph limit
number iterations still ensure exa result

O(|V | |E|)

time. simple heuristi

improving e ien algorithm pra ti al purposes redu e number
iterations, like xed point algorithms su h PageRank (Page et al., 1998).
shall prove orre tness omplexity algorithm.

Proposition 7.6. Algorithm 1 outputs verti es

Denition 7.2 works O(|V | |E|) time.
Proof.

V order RID dened

Let us rst denote

1
[r(|P (a1 )| + vpr (a2 , . . . , , . . .)]
n+1

vpr () = 0.



vpr (a1 , a2 , . . . , , . . .)

=

a1 , . . . , Path(v): a1 , . . . , simple, vpr (a1 , . . . , ) =

vpr (a1 , . . . , ). Otherwise = ai , vpr (a1 . . . , ) = vpr (a1 , . . . , ai+1 , . . . , , . . .).
Let P(v) set reverse paths v G, simple otherwise.
v V:
Note

vV



valuer (v)

ause rst loop

=

p P(v)

max

pPath(v)

vpr (p)

= max

pP(v)



vpr (p),

repla ed one maximizing vpr (), thus

reasing value.

488

fiAxiomati Foundations Ranking Systems
iteration step 3 algorithm al ulates

"

"

v:

"

# ##
1
1
1
r0 + max
r
+
max
,
r
n+1
n + 1 |V |1 p|V | P (p|V |1 ) n + 1 |V |
p1 P (v)


ri = r(|P (pi )|)

p0 = v .



value equal

max

max

p1 P (v) p2 P (p1 )

max

p|V | P (p|V | 1)
|V |+1

=
=


Pm (v)

max

(p1 ,...,p|V |+1 )P|V | (v)

max

pP|V |+1 (v)



vpr (v),

X
i=1

|V |

i=0

ri
=
(n + 1)i+1

ri
=
(n + 1)i

set reverse paths length



|V |
X

(10)





v,

simple otherwise.As

verti es, two verti es dier value assigned value

fun tion (1) must also dier value (10) al ulated algorithm
dire tion.
shall prove time omplexity algorithm, tra ing ea h step. Steps 1

O(|V |) time. iteration step 3 repeated |V | times, every vertex
O(|P (v)|) al ulations, ea h iteration takes O(|E|) time thus total


time O(|V | |E|). Step 4 takes O(|V | log |V |) O(|V | log |E|) O(|V | |E|). Finally,
output step 5 takes O(|V |) time. every step takes O(|V | |E|) time,
2 take



V

performs

entire algorithm.

8. Axiomatization Approval Voting
Se tions 5 6 seen mostly negative results whi h arise trying ommodate (weak) transitivity RIIA. shown although ea h axioms
satised separately, exists general ranking system satises axioms.
Tennenholtz (2004) previously shown non-trivial ranking system satises
(weak) transitivity, previous se tion seen su h system RIIA. However, provided representation theorem new system.
se tion provide representation theorem ranking system satises
RIIA weak transitivity approval voting ranking system (see Denition 2.5).
axiomatization provide se tion shows power RIIA, shows
exists one (interesting) ranking system satises without introdu ing transitive
ee ts.
Fishburn (1978) axiomatized Approval Voting ranking system ontext
ial hoi e, output algorithm ranking, rather set winners.
two distin settings similar, thus Fishburn's axiomatization approval
voting great relevan e work. shall ompare two axiomatizations later
se tion.
order spe ify axiomatization, following lassi al denitions
theory ial hoi e:

489

fiAltman & Tennenholtz
positive response axiom (sometimes referred

positive responsiveness ) essentially

means agent eives additional votes, rank must improve:

Denition 8.1.

F satises positive response
G = (V, E) (v1 , v2 ) (V V ) \ E , v1 6= v2 , v3 V :
(V, E (v1 , v2 )). v3 FG v2 , v3 FG v2 .
Let

F

ranking system.

graphs
Let

G =

anonymity neutrality axioms mean names voters alternatives
respe tively matter ranking:

Denition 8.2.

ranking system

: V 7 V ,
v2 v1 F(V,E ) v2 .

permutations

v1 F(V,E)



F satises anonymity G = (V, E),
v1 , v2 V : Let E = {((v1 ), v2 )|(v1 , v2 ) E}. Then,

Denition 8.3.

ranking system F satises neutrality G = (V, E), per : V 7 V , v1 , v2 V : Let E = {(v1 , (v2 ))|(v1 , v2 ) E}. Then,
v2 (v1 ) F(V,E ) (v2 ).

mutations

v1 F(V,E)

Arrow's lassi al Independen e Irrelevant Alternatives axiom requires relative
rank two agents dependant set agents preferred one other.

Denition 8.4.

F
G = (V, E),
PG (v1 ) \ PG (v2 ) = PG (v1 ) \ PG (v2 )
v1 FG v2 v1 FG v2 .

natives (AIIA)

ranking system



satises

Arrow's Independen e Irrelevant Alter-

G = (V, E ), v1 , v2 V : Let
PG (v2 ) \ PG (v1 ) = PG (v2 ) \ PG (v1 ). Then,




representation theorem states together positive response RIIA,
one three independen e onditions (anonymity, neutrality, AIIA) essential su ient ranking system

AV .

addition, show

lassi al ial hoi e setting onsidering two-level preferen es, positive response,
anonymity, neutrality, AIIA essential su ient representation approval
voting. result extends well known axiomatization majority rule due May
(1952):

Proposition 8.5. (May's Theorem) ial welfare fun tional two alternatives
majority ial welfare fun tional satises anonymity, neutrality, positive
response.
formally state theorem:

Theorem 8.6. Let

equivalent:

F general ranking system. Then, following statements

1. F approval voting ranking system (F = AV )
2. F satises positive response, anonymity, neutrality, AIIA
3. F satises positive response, RIIA, either one anonymity, neutrality, AIIA
490

fiAxiomati Foundations Ranking Systems

v

x

u
Figure 9: Example graph

Proof.

easy see

AV

G

prole

h(1, 3, 3), (2, 4)i

satises positive response, RIIA, anonymity, neutrality,

AIIA. remains show (2) (3) entail (1) above.

F satises positive response, anonymity, neutrality,
G = (V, E) graph let v1 , v2 V agents. AIIA,
ranking v1 v2 depends sets PG (v1 ) \ PG (v2 ) PG (v2 ) \

prove (2) entails (1), assume
AIIA. Let
relative

PG (v1 ).

narrowed onsideration set agents preferen es

two alternatives, apply Proposition 8.5 omplete proof.
prove (3) entails (1), assume
anonymity neutrality AIIA.
parison proles. Let

f : P 7 {0, 1}

F

F

satises positive response, RIIA either

satises RIIA limit dis ussion om-

fun tion denition RIIA.

a. positive response
(1, 1, . . . , 1) (1, 1, . . . , 1) n m. Let P = h(a1 , . . . , ), (b1 , . . . , bm )i
| {z }
| {z }

denition RIIA, easy see
also easy see

n



G = (V, E)
h(1, 3, 3), (2, 4)i Figure 9):

omparison prole. Let
prole

V

following graph (an example su h graph

= {x1 , . . . , xmax{an ,bm } }
{v1 , . . . , vn , v 1 , . . . , vn , v}
{u1 , . . . , um , u1 , . . . , um , u}

E = {(xi , vj )|i aj } {(xi , uj )|i bj }
{(vi , v)|i = 1, . . . , n} {(ui , u)|i = 1, . . . , m}.

491

fiAltman & Tennenholtz
easy see graph
permutation:

G, v



u


vi




vi
(x) =
ui


ui


x



F

v



following

Otherwise .

F

satises:

E = {((x), y)|(x, y) E}. Note graph (V, E )
h(1, 1, . . . , 1), (1, 1, . . . , 1)i, thus v F(V,E ) u n m.
| {z } | {z }

u

satisfy prole

n

arbitrary


F

v



satises neutrality, let

u



u F(V,E) v u F(V,E ) v , thus proving
omparison prole P , thus F = AV .

satisfy prole

f (P ) = 1 n



E = {(x, (y))|(x, y) E}. Note graph (V, E )
h(1, 1, . . . , 1), (1, 1, . . . , 1)i, thus v F(V,E ) u n m.
| {z } | {z }
n

arbitrary





u F(V,E) v u F(V,E ) v , showing
omparison prole P , thus F = AV .

neutrality,





satises anonymity, let

anonymity,



Let

x = vi
x = vi
x = ui
x = ui

remainder proof depends whi h additional axiom



P.

satisfy prole



f (P ) = 1 n



F satises AIIA, let E = {(x, (y))|(x, y) E} before. So, also v F(V,E )
u n m. Note PG (v) = P(V,E ) (v) PG (u) = P(V,E ) (u), AIIA,
u F(V,E) v u F(V,E ) v , thus before, F = AV .



axiomatization approval voting, spe ally one (2) related
previous axiomatization Fishburn (1978). axiomatizations share requirements Anonymity

1

Neutrality, dier additional assumptions: Fishburn's

requirements refer relations results dierent voter sets, whi h annot
easily used ranking systems setting, voters also alternatives,
requirements relate hanges preferen es single agent ability (positive
response) inability (AIIA) inuen e nal result. requirements may mapped
Fishburn's setting would probably lead distin axiomatization approval voting
setting.

9. Con luding Remarks
Reasoning preferen es preferen e aggregation fundamental task reasoning
multi-agent systems (see e.g. Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004;
Conitzer & Sandholm, 2002; LaMura & Shoham, 1998).

typi al instan e preferen e

aggregation setting ranking systems. Ranking systems fundamental ingredients
famous tools/te hniques Internet (e.g. Google's PageRank
eBay's reputation systems, among many others).

1. Fishburn onsider Anonymity axiom, rather denes ial hoi e model allow
anonymous fun tions.

492

fiAxiomati Foundations Ranking Systems
Moreover, task building su essful ee tive on-line trading environments
ome entral hallenge AI ommunity (Boutilier, Shoham, & Wellman, 1997;
Monderer, Tennenholtz, & Varian, 2000; Sandholm, 2003). Ranking systems believed
fundamental establishment su h environments.

Although reputation

always major issue e onomi (see e.g. Kreps & Wilson, 1982; Milgrom & Roberts,
1982), reputation systems ome entral ently due fa
inuential powerful Internet sites ompanies put reputation systems
ore business.
aim paper treat ranking systems axiomati perspe tive.
lassi al theory ial hoi e lay foundations large part rigorous
work multi-agent systems. Indeed, lassi al results theory hanism
design, su h Gibbard-Satterthwaite Theorem (Gibbard, 1973; Satterthwaite, 1975)
appli ations theory ial hoi e. Moreover, previous work AI employed
theory ial hoi e obtaining foundations reasoning tasks (Doyle & Wellman, 1989)
multi-agent oordination (Kr-Dahav & Tennenholtz, 1996). however interesting
note ranking systems suggest novel new type theory ial hoi e.
see point espe ially attra tive, main reason entrating study
axiomati foundations ranking systems.
paper identied two fundamental axioms ranking systems, ondu ted
basi axiomati study su h systems. parti ular, presented surprising impossibility
results, omplemented new ranking algorithm, representation theorem
well-known approval voting heme.

knowledgements
work partially supported grant Israeli ien e Foundations
(ISF).

Referen es
Altman, A., & Tennenholtz, M. (2005). Ranking systems: PageRank axioms.

Pro eedings 6th ACM onferen e Ele troni ommer e,

EC '05:

pp. 18, New York,

NY, USA. ACM Press.
Altman, A., & Tennenholtz, M. (2006).
systems..

Pro . AAAI-06.

Quantifying entive ompatibility ranking

Altman, A., & Tennenholtz, M. (2007a). axiomati approa h personalized ranking
systems.

Pro . 20th International Joint Conferen e Arti ial Intelligen e.

Altman, A., & Tennenholtz, M. (2007b). entive ompatible ranking systems.

AAMAS-07.

Arrow, K. (1963).

ial Choi e Individual Values (2nd Ed.).

Bogomolnaia, A., Moulin, H., & Stong, R. (2005).
mous preferen es.

Yale University Press.

Colle tive hoi e di hoto-

Journal E onomi Theory, 122 (2),

165184.

http://ideas.repe .org/a/eee/jetheo/v122y2005i2p165-184.html.

493

Pro .

available

fiAltman & Tennenholtz
Boutilier, C., Shoham, Y., & Wellman, M. (1997). Spe ial issue e onomi prin iples
multi-agent systems.

Arti ial Intelligen e, 94.

Boutilier, C., Brafman, R. I., Domshlak, C., Hoos, H. H., & Poole, D. (2004). Cp-nets: tool
representing reasoning onditional eteris paribus preferen e statements..

J. Artif. Intell. Res. (JAIR), 21,

135191.

Brin, S., & Page, L. (1998). anatomy large-s ale hypertextual Web sear h engine.

Computer Networks ISDN Systems, 30 (17),

107117.

Conitzer, V., & Sandholm, T. (2002). Complexity hanism design.

18th onferen e un ertainity Arti ial Intelligen e (UAI-02),

Pro eedings
pp. 103110.

Doyle, J., & Wellman, M. (1989). Impediments Universal Preferen e-Based Default The-

Pro eedings 1st onferen e prin iples knowledge representation
reasoning.

ories.

Journal E onomi Theory, 19 (1), 180185. available http://ideas.repe .org/a/eee/jetheo/v19y1978i1p180-

Fishburn, P. C. (1978). Axioms approval voting: Dire proof.
185.html.
Gibbard, A. (1973). Manipulation voting hemes.

E onometri a, 41, 587601.

Pro eedings
6th onferen e theoreti al aspe ts rationality knowledge (TARK).

Kr-Dahav, N. E., & Tennenholtz, M. (1996). Multi-Agent Belief Revision.

Journal E onomi

Kreps, D., & Wilson, R. (1982). Reputation imperfe information.

Theory, 27, 253279.

LaMura, P., & Shoham, Y. (1998). Conditional, Hierar hi al Multi-Agent Preferen es.

Pro eedings Theoreti al Aspe ts Rationality Knowledge,

May, K. O. (1952).

pp. 215224.

set independent, ne essary su ient onditions simple

majority de ision.

E onometri a, 20 (4),

68084.

Milgrom, P., & Roberts, J. (1982). Predation, reputation entry deterren e.

E onomi Theory, 27, 280312.

Journal

Monderer, D., Tennenholtz, M., & Varian, H. (2000). Game theory arti ial intelligen e.
Spe ial issue Games E onomi behavior.
Moulin, H. (1991).

Axioms Cooperative De ision Making.

Cambridge University Press.

Page, L., Brin, S., Motwani, R., & Winograd, T. (1998). PageRank itation ranking:
Bringing order web. Te hni al Report, Stanford University.
Pala ios-Huerta, I., & Volij, O. (2004). measurement intelle tual inuen e.

metri a, 73 (3).

E ono-

Resni k, P., & Ze khauser, R. (2001). Trust among strangers internet transa tions: Empiri al analysis ebay's reputation system. Working Paper NBER workshop
empiri al studies ele troni ommer e.
Resni k, P., Ze khauser, R., Friedman, R., & Kuwabara, E. (2000).

Communi ations ACM, 43 (12),

4548.

494

Reputation systems.

fiAxiomati Foundations Ranking Systems
Sandholm, T. (2003). Making markets demo ra work: story entives om-

Pro eedings International Joint Conferen e Arti ial Intelligen e
(IJCAI-03), pp. 16491671.

puting.

Satterthwaite, M. (1975). Strategy proofness arrow's onditions: Existen e orresponden e theorems voting pro edures ial welfare fun tions..

E onomi Theory, 10, 187217.

Stokey, N. L., & Lu as, R. E. (1989).

Journal

ursive Methods E onomi Dynami s.

Harvard

University Press.
Tennenholtz, M. (2004). Reputation systems: axiomati approa h.

20th onferen e un ertainity Arti ial Intelligen e (UAI-04).

495

Pro eedings

fiJournal Artificial Intelligence Research 31 (2008) 591-656

Submitted 11/07; published 3/08

Multiagent Approach
Autonomous Intersection Management
Kurt Dresner
Peter Stone

kdresner@cs.utexas.edu
pstone@cs.utexas.edu

Department Computer Sciences, University Texas Austin
1 University Station [C0500], Austin, TX 78712 USA

Abstract
Artificial intelligence research ushering new era sophisticated, mass-market
transportation technology. computers already fly passenger jet better
trained human pilot, people still faced dangerous yet tedious task driving automobiles. Intelligent Transportation Systems (ITS) field focuses integrating
information technology vehicles transportation infrastructure make transportation safer, cheaper, efficient. Recent advances point future
vehicles handle vast majority driving task. autonomous vehicles
become popular, autonomous interactions amongst multiple vehicles possible. Current methods vehicle coordination, designed work human drivers,
outdated. bottleneck roadway efficiency longer drivers,
rather mechanism drivers actions coordinated. open-road
driving well-studied more-or-less-solved problem, urban traffic scenarios, especially
intersections, much challenging.
believe current methods controlling traffic, specifically intersections,
able take advantage increased sensitivity precision autonomous vehicles
compared human drivers. article, suggest alternative mechanism
coordinating movement autonomous vehicles intersections. Drivers
intersections mechanism treated autonomous agents multiagent system.
multiagent system, intersections use new reservation-based approach built around
detailed communication protocol, also present. demonstrate simulation
new mechanism potential significantly outperform current intersection
control technologytraffic lights stop signs. mechanism emulate
traffic light stop sign, subsumes popular current methods intersection
control. article also presents two extensions mechanism. first extension
allows system control human-driven vehicles addition autonomous vehicles.
second gives priority emergency vehicles without significant cost civilian vehicles.
mechanism, including extensions, implemented tested simulation,
present experimental results strongly attest efficacy approach.

1. Introduction
concepts, any, embody goals aspirations artificial intelligence well
fully autonomous robots. Countless films stories made focus
future filled humanoid agents which, violently overthrowing human
masters, run errands, complete menial tasks, perform duties would difficult
dangerous humans. However, machines sense, think about, take actions
real world around us longer stuff science fiction fantasy. Research
c
2008
AI Access Foundation. rights reserved.

fiDresner & Stone

initiatives like Robocup (Noda, Jacoff, Bredenfeld, & Takahashi, 2006) DARPA
Grand Challenge (DARPA, 2007) shown current AI produce autonomous,
embodied, competent agents complex tasks like playing soccer navigating Mojave
Desert, respectively. certainly small feat, traversing barren desert devoid
pedestrians, narrow lanes, multitudes fast-moving vehicles typical
daily task humans. Gary Bradski, researcher Intel Corp. said following
successful completion 2005 Grand Challenge Stanley, modified Volkswagen
Touareg, need teach drive traffic (Johnson, 2005). Since then,
competitors 2007 DARPA Urban Challenge took significant strides towards next
milestone, though competition cars need sense traffic signs signals
traffic relatively sparsemore characteristic suburban dense urban settings.
modern urban settings, automobile traffic collisions lead endless frustration
well significant loss life, property, productivity. 2004 study 85 U.S.
cities researchers Texas A&M University estimated annual time spent waiting
traffic 46 hours per capita, 16 hours 1982 (Texas Transportation Institute,
2004). Americans burn approximately 5.6 billion gallons fuel year simply idling
engines. told, annual financial cost traffic congestion swollen $14 billion
$63 billion (in 2002 US dollars) period. cost wasted
time fuel due congestion pales comparison costs associated automobile
collisions. 2002 report, National Highway Traffic Safety Administration (NHTSA)
put annual societal cost automobile collisions U.S. $230 billion (National
Highway Traffic Safety Administration, 2002).
Fully autonomous vehicles may able spare us much, nearly costs.
autonomous driver agent much accurately judge distances velocities,
attentively monitor surroundings, react instantly situations would leave
(relatively) sluggish human driver helpless. Furthermore, autonomous driver agent
get sleepy, impatient, angry, drunk. Alcohol, speeding, running red lights
top three causes automobile collision fatalities. Autonomous driver agentsproperly
programmedwould eliminate three.
fully autonomous vehicle drive traffic everything
obeying speed limit staying lane detecting tracking pedestrians
choosing best route mall. certainly complex task, advances artificial intelligence, specifically, Intelligent Transportation Systems (ITS), suggest
may soon reality (Bishop, 2005). Cars already equipped features
autonomy adaptive cruise control, GPS-based route planning (Rogers, Flechter,
& Langley, 1999; Schonberg, Ojala, Suomela, Torpo, & Halme, 1995), autonomous
steering (Pomerleau, 1993; Reynolds, 1999). current production vehicles even sport
features. DaimlerBenzs Mercedes-Benz S-Class adaptive cruise control system
maintain safe following distance car front it, apply extra
braking power determines driver braking hard enough. Toyota
BMW currently selling vehicles parallel park completely autonomously, even
finding space park without driver input. 2008, General Motors (GM)
plans release nearly autonomous vehicle European Opel brand. 2008
Opel Vectra able drive speeds 60 miles per hour, even heavy
traffic. Using video camera, lasers, lot processing power, car able
592

fiA Multiagent Approach Autonomous Intersection Management

identify traffic signs, curves street, lane markings, well vehicles.
end decade, GM hopes incorporate system many models.
Autonomous vehicles coming. article, present well-defined multiagent
framework manage large numbers autonomous vehicles intersections.
still exist many technical hurdles rigorous safety tests, show simulation
framework may someday dramatically improve safety efficiency roadways.
1.1 Multiagent Systems
autonomous vehicles become prevalent, possibility autonomous
interactions among multiple vehicles becomes interesting. Multiagent Systems (MAS)
subfield AI aims provide principles construction complex systems involving multiple agents mechanisms coordination independent agents
behaviors (Stone & Veloso, 2000). Automobile traffic vast multiagent system involving millions heterogeneous agents: commuters, truck drivers, pedestrians, cyclists,
even traffic-directing police officers. mechanism coordinates behavior
agents complex conglomeration laws, signs, signaling systems vary slightly
state state widely country country. mechanism designed
work closely agentsthe humansthat populate multiagent system. Traffic
lights leave time green lights allow slower perhaps impatient drivers
clear intersections. Street signs colored brightly make easier see use
simple designs make easy understand. Drivers must maintain sufficient following distance make slow reaction times. Speed limits ensure humans
enough time process necessary information position velocities
vehicles. Safety buffers myriad sorts built almost every part system
compensate limitations humans.
first generation autonomous vehicles undoubtedly need work within
system. Processing-intensive vision algorithms identify extract semantic information signs signals, special subroutines ensure vehicles exceed
speed limit, middle night, another moving vehicle blocks,
autonomous vehicle come stop red light. However, vehicles
autonomous limitations eliminated, make sense use mechanism
designed control fundamentally different agentsit inefficient, terms
processing power getting vehicles destinations quickly.
Replacing soon-to-be-outdated mechanism inherently multiagent challenge
several reasons. First, viable single-agent solutions; one computer cannot
handle vehicles world. Second, vehicles constantly entering leaving
countries, states, cities, towns, solution flexible distributed.
Third, different agents separate, sometimes conflicting objectives.
human-driven vehicles, autonomous vehicles act self-interest, attempting
minimize travel time, distance, fuel use. types agents may aim maximize
social welfare, minimizing quantities average vehicle. Finally, even single
computer could control citys worth traffic, would sensitive point failure.
593

fiDresner & Stone

1.2 Intersections
open road, automobiles less completely autonomous. Furthermore,
little need simple reactive behavior keeps vehicle
lane, maintains reasonable distance vehicles, avoids obstacles. Even lane
changing safely efficiently accomplished autonomous vehicle (Hatipo,
Redmill, & Ozguner, 1997). algorithmic AI aspects open-road driving
essentially solved. problem difficult: pedestrians cyclists
vehicles travel direction similar velocities; relative movement smooth
rare.
Intersections completely different story: vehicles constantly cross paths, many
different directions. vehicle approaching intersection quickly find situation collision unavoidable, even acted optimally. Traffic statistics
support sensitive nature intersections. Vehicle collisions intersections account
anywhere 25% 45% collisions. intersections make small
portion roadway, wildly disproportionate amount. Collisions intersections tend involve cars traveling different directions, thus frequently result
greater injury damage. modern-day intersections controlled traffic lights
stop signs, former usually reserved larger, busier intersections. busiest
intersectionsfreeway interchangeslarge, extremely expensive cloverleaf junctions
built.
vastly improved precision control sensing autonomous vehicles
offer, must efficient safe way manage intersections. Imagine
scenario autonomous vehicle stops red light middle night
vehicles nearby. least, vehicle able communicate
presence intersection, verify vehicles nearby, turn
light green stopped vehicle. ambitious implementation, intersection
could turn light green preemptively, obviating stop altogether. article, go
step further, allowing vehicles call ahead reserve space-time intersection.
remainder article organized follows. Section 2, describe problem autonomous intersection management framework attempt
solve problem. Section 3, describe implementation solution framework. Section 4 presents experiments empirical results. Section 5, conduct
failure mode analysis proposed mechanism. Related work discussed Section 6.
Section 7 briefly explores avenues future research concludes.

2. Problem Statement Solution Framework
Automobile traffic already huge multiagent system millions human driver agents,
various signaling control mechanisms, complicated protocol governing actions
driver agents, form traffic laws. However, human drivers
replaced autonomous driving agents, elements multiagent system
rethought. Traffic lights, stop signs, current traffic laws designed
human drivers mind fail take advantage increased sensitivity precision
computerized driver agents. want autonomous vehicles operate high efficiency
safety, must design new way coordinate them. section, formulate
594

fiA Multiagent Approach Autonomous Intersection Management

problem trying solve present framework within believe
problem best solved.
2.1 Desiderata
designing mechanism traffic controlled intersections, aim satisfy
following list properties.
Autonomy vehicle autonomous agent. entire mechanism
centrally controlled, would susceptible single-point failure, require massive
amounts computational power, exert unnecessary control vehicles situations
perfectly capable controlling themselves.
Low Communication Complexity keeping number messages amount
information transmitted minimum, system afford put communication
reliability measures place. Furthermore, vehicle, autonomous agent, may
privacy concerns respected. Keeping communication complexity low
also make system scalable.
Sensor Model Realism agent access sensors available
current-day technology. mechanism rely fictional sensor technology
may never materialize.
Protocol Standardization mechanism employ simple, standardized protocol communication agents. Without standardized protocol, agent
would need understand internal workings every agent interacts.
requirement would forbid introduction new agents system. open, standardized protocol would make adoption system easier simpler private vehicle
manufacturers.
Deadlock/Starvation Avoidance Deadlocks starvation occur
system. Every vehicle approaching intersection eventually cross, even
better rest agents leave vehicle stranded.
Incremental Deployability system incrementally deployable, two
senses. First, possible set selected intersections use system,
slowly expand intersections needed. Second, system function
even autonomous vehicles. stage deployment, whether
increase proportion autonomous vehicles number equipped intersections,
overall performance system improve. point net disincentive
continue deploying system exist.
Safety Excepting gross vehicle malfunction extraordinary circumstances (e.g. natural disasters), long follow protocol, vehicles never collide
intersection. Note stronger guarantee possibleas modern mechanisms,
suicidal human driver always steer vehicle oncoming traffic. Furthermore,
system safe event total communication failure. messages dropped
corrupted, safety system compromised. impossible prevent negative effects due communication failures, negative effects
595

fiDresner & Stone

isolated efficiency. message gets dropped, make someone arrive 10 seconds
later destination, cause collision. rare unpreventable
case gross vehicle malfunction, system react attempt minimize damage
casualties.
Efficiency Vehicles get across intersection way little time
possible. quantify efficiency, introduce delay, defined amount additional
travel time incurred vehicle result passing intersection.
2.2 Reservation Idea
desiderata, modern-day traffic lights stop signs completely satisfy
last one. many accidents take place intersections governed traffic lights,
accidents rarely, ever, fault traffic light system itself, rather
human drivers. However, show, traffic lights stop signs terribly inefficient.
vehicles traversing intersections equipped mechanisms experience
large delays, intersections manage somewhat limited amount
traffic. stretch open road accommodate certain level traffic given
velocity. capacity intersection involving road trivially bounded
capacity road. also show, capacity traffic lights stop signs
much less roads feed them. aim research create
intersection control mechanism exceeds efficiency traffic lights stop signs,
maintaining desiderata.
desiderata mind, developed multiagent approach direct vehicles
intersections efficiently. approach, computer programs called driver
agents control vehicles, arbiter agent called intersection manager placed
intersection. driver agents call ahead attempt reserve block
space-time intersection. intersection manager decides whether grant reject requested reservations according intersection control policy. Figure 1 shows one
interaction driver agent intersection manager. system functions
analogously human attempting make reservation hotelthe potential guest
specifies arriving, much space required, long stay
be; human reservation agent determines whether grant reservation,
according hotels reservation policy. guest need understand
hotels decision process, driver agents require knowledge
intersection control policy used intersection manager.
vehicle approaches intersection, vehicles driver agent transmits reservation request, includes parameters time arrival, velocity arrival, well
vehicle characteristics like size acceleration/deceleration capabilities, intersection manager. intersection manager passes information policy,
determines whether safe vehicle cross intersection. policy
deems safe, intersection manager responds driver agent message
indicating reservation accepted including supplemental restrictions
driver must observe order guarantee safety traversal. Otherwise,
intersection manager sends message indicating reservation request rejected, possibly including grounds rejection. addition confirming rejecting
596

fiA Multiagent Approach Autonomous Intersection Management

REQUEST

Driver
Agent

REJECT

Postprocess

Preprocess

No, Reason

Yes,
Restrictions

Intersection
Control Policy

CONFIRM

Intersection Manager
Figure 1: One driver agents attempts make reservation. intersection manager responds based decision intersection control policy.

request, intersection manager may respond counter-offer. driver agent
may pilot vehicle intersection without reservation. Even reservation, driver agent may proceed intersection according parameters
restrictions associated reservation. sake brevity, may refer
vehicle obtaining reservation, rather specifically stating driver
agent vehicle obtains reservation.

3. Building System
section describes realization reservation idea implemented algorithm.
process involved developing simulator run algorithm, well
creating behaviors agents protocol communicate.
3.1 Custom Simulator
order empirically evaluate reservation idea, built custom time-based simulator.
simulator models area 250 250 m. intersection located
center area, size determined number lanes traveling
direction, variable. assume throughout vehicles drive right side
road, however assumption required system work properly.
Figure 2 shows screenshot simulators graphical display. time step,
simulator:
1.
2.
3.
4.
5.

Probabilistically spawns new vehicles
Provides sensor input vehicles
Allows driver agents act
Updates position vehicles according physical model
Removes vehicles outside simulated area completed journey

3.1.1 Vehicles
Vehicles simulator following properties:
597

fiDresner & Stone

Figure 2: screenshot simulator action.

Vehicle Identification Number (VIN)
Length
Width
Distance front vehicle front axle
Distance front vehicle rear axle
Maximum velocity
Maximum acceleration
Minimum acceleration
Maximum steering angle
Sensor range
following state variables:
Position
Velocity
Heading
Acceleration
Steering angle
driver agent assigned pilot vehicle may access quantities,
without noise, depending configuration simulator. driver agent may
also access several simulated external sensors: list vehicles within sensor range,
simplified laser range finder. detailed description simplified laser range finder
found Appendix A.
steering angle angle front wheels respect vehicle.
angle changed driver agent, simulator limits rate
changed. limitation simulates fact even computerized driver cannot
move steering wheel infinitely fast. introducing limitation, accurately
approximate vehicle turning, including dangerous aspects. driver
598

fiA Multiagent Approach Autonomous Intersection Management

cannot turn wheels instantaneously, must ensure drive around corners
high velocityit may able straighten quickly enough wind
veering road instead.
constants representing distance front vehicle front
rear axles allow accurate simulation vehicle turning. Specifically, allow
simulator treat different styles vehicle differently. distance front
rear axles known wheelbase. Vehicles shorter wheelbases turn
sharply longer wheelbasesif simulator accurately model turning,
needs access important parameters. Furthermore, vehicle long hood
turn differently vehicle whose front wheels located nearer front
vehicle.
3.1.2 Lanes
Lanes system consist directed line segment, width, left right borders
vehicles may may permitted cross, references lanes, any, border
right left side. real-life implementation, would software construct
vehicles driver agents would use perform lane following changing. vehicle
wants change lanes left right, must first establish vehicle allowed
cross border lanes, feed lane-following algorithm
reference desired lane.
3.1.3 Physical Model
time step, simulator must update position every vehicle.
model planar vehicle kinematics dynamics, must make assumptions.
First, assume vehicles skid road. Second, assume vehicles
move according following differential equations non-holonomic motion:
x
= v cos()


= v sin()


tan
=v

L
equations, x, y, describe vehicles position orientation, v represents vehicles velocity, describes vehicles steering angle, L vehicles
wheelbase. solve equations holding v constant time step.
3.1.4 Measuring Delay
Section 2.1, introduced delaythe increase travel time vehicle due
presence intersection. simulation, measured first assuming
open road, vehicle maintain velocity speed limit. vehicle
timestamped enters simulation keeps track far traveled.
vehicle removed simulation, total delay calculated difference
long actually took travel far long would take
599

fiDresner & Stone

vehicle travel speed limit entire journey. measure, zero delay
possible vehicle turning, needs slow order safely make
turn. practice, compare delays vehicles delays using policy
allows vehicles intersection unhindered, also non-zero
vehicles turn road congested. way, quantify effect
intersection vehicle, directly (not able go intersection
requests rejected) indirectly (having decelerate another vehicle
cannot get through).
3.2 Communication Protocol
section presents detailed communication protocol vehicles intersections
coordinate behavior. protocol presented offers three major benefits:
information agents goes one monitorable channel,
makes reasoning communication straightforward.
limiting interactions agents message types, ensure
agent unrealistic amount control another.
agents way communicate identical intersection management policy driver agent policy. Thus, vehicle cross intersection without
idea policy intersection manager usingit simply sends
receives messages obeys rules.
protocol consists several message types kind agent, well
rules governing messages sent sorts guarantees accompany
them. Driver agents send Request, Change-Request, Cancel, Done messages. Request Change-Request used driver agent wants make
reservation change existing reservation, respectively. types request message
include relevant properties vehicle. Driver agents send Cancel message
want cancel existing reservation. vehicle successfully crossed
intersection, driver agent sends Done message intersection manager.
Cancel Done messages include VIN vehicle, well identifier
reservation cancelled reported complete.
Intersection managers send Confirm, Reject, Acknowledge messages,
well special Emergency-Stop message, used intersection
manager detects major problem intersection (see Section 5). Confirm sent
intersection manager approves Request Change-Request message.
includes information describing reservationa unique identifier reservation,
start time, start lane, departure lane (which identical start lane unless
vehicle turning), list constraints vehicles acceleration
intersection. Reject message used reject either Request Change-Request
message. intersection sends Acknowledge message response Cancel
Done messages sent vehicles. detailed specification protocol including
full syntax semantics found Appendix B.
600

fiA Multiagent Approach Autonomous Intersection Management

3.2.1 Message Corruption Loss
assume messages digitally signed, possibility undetected
message corruption acceptably small. protocol designed specifically robust
message loss. message sent receivedor deemed corruptedthe worst
thing happen additional delay. collisions occur due lost messages.
vehicle makes reservation request, assume space reserved
receives confirmation intersection manager. Request message dropped,
Confirm message follow. Confirm Reject message dropped, vehicle
simply try againit wont assume valid reservation.
3.2.2 Enabling Policy Switching
protocol hides implementation policy driver agents
idea intersection manager making decisions, guaranteed
follow them, safe. Thus, stipulations policy must
remain fixed. intersection manager could use one policy one moment switch
appropriate policy later, provided still guarantee vehicles following
protocol make safely across intersection.
3.2.3 Intersection Manager
intersection manager acts stable communication interface driver agents
intersection control policy therefore contain lot functionality.
However, regardless policy makes decision, intersection manager must
present interface driver agents. general intersection manager algorithm
shown Algorithm 1. it, Cancel messages Done messages treated almost
identicallywhen Done message received, intersection manager knows
policy erase information related reservation. However, Done message
also may contain information useful intersection manager policy.
example, vehicle sends Done message, could include delay experienced
crossing intersection, providing intersection manager sort reward signal,
judge performance.
3.3 Driver Agent
vast majority research focuses make better intersection manager
control policy. parts designed work driver agent follows
protocol. However, testing purposes, driver agent implementation required. Despite
fact lot work went driver agent (it probably intricate
part system), focus article. refer interested reader
Appendix C, explains driver agent detail. brief, driver agent estimates
time velocity reach intersection, requests appropriate
reservation. granted reservation, attempts arrive schedule. determines
unable keep reservation, cancels reservation. believes
substantially early, attempts change earlier reservation. unable get
reservation, decelerates (down minimum velocity) requests again.
601

fiDresner & Stone

Algorithm 1 intersection manager algorithm. Vehicle V sends message
intersection manager, responds according policy P .
1: loop
2:
receive message V
3:
message type Request
4:
process request new reservation P
5:
P accepts request
6:
send Confirm message V containing reservation returned P
7:
else
8:
send Reject message V
9:
else message type Change-Request
10:
process request change reservation P
11:
P accepts request
12:
send Confirm message V containing reservation returned P
13:
else
14:
send Reject message V
15:
else message type Cancel
16:
process cancel P
17:
send Acknowledge message V
18:
else message type Done
19:
record statistics supplied message
20:
process cancel P
21:
send Acknowledge message V

enter intersection without reservation. open road, driver agent employs
simple lane-following algorithm, maintains following distance one second
vehicle vehicle front it.
3.4 FCFS Policy
point, weve described substrate infrastructure enables research.
remainder Section 3 introduces core contribution article main payoff
creating infrastructure, namely intersection control policy enables fine-grained
coordination vehicles intersections, subsequent dramatic decrease delays.
intersection manager communicates directly driver agents, intersection control policy brains behind operation. describe intersection
control policy created reservation idea discussed Section 2.2.
First Come, First Served nature policy, name policy FCFS. main part
policythe request processingis shown Algorithm 2.
Recall FCFS enables car reserve advance space-time needs cross
intersection. Planning ahead allows vehicles coming directions traverse
intersection simultaneously minimal delay. policy works follows:
intersection divided n n grid reservation tiles, n
granularity policy.
602

fiA Multiagent Approach Autonomous Intersection Management

Upon receiving reservation parameters approaching driver agent, policy runs internal simulation trajectory vehicle across intersection
using parameters.
time step internal simulation, policy determines reservation
tiles occupied vehicle
time simulation requesting vehicle occupies reservation tile
already reserved another vehicle, policy rejects request. Otherwise,
policy accepts reservation reserves appropriate tiles times
required.
Figure 3 shows graphical depiction concept behind FCFS policy.

(a) Successful

(b) Rejected

Figure 3: internal simulation granularity-8 FCFS policy. black rectangles represent vehicles, shaded tiles tiles currently reserved. 3(a),
vehicles request accepted, intersection reserves set tiles time
t. 3(b), second vehicles request rejected simulation
trajectory, policy determines requires tile (darkly shaded) already
reserved first vehicle time t.

concept behind FCFS sound, requires modifications
work reliably, safely, efficientlyeven simulation. remainder section,
present modifications, created response early experimental
results documented Section 4.
3.4.1 Determining Outbound Lane
first implementation reservation system, vehicles capable traveling
straight lines. allowed vehicles turn, became apparent
driver agents determine lane use exit intersection. Instead,
intersection manager, information intersection, makes
decision. Driver agents indicate request message way intend turn,
complicated intersections, direction intend go. intersection
control policy decides outbound lane place vehicle. experiments
documented article, FCFS policy chooses natural lane: left
603

fiDresner & Stone

right turns, chooses nearest lane, whereas vehicles going turn,
chooses lane planning arrive intersection. However, policy
could behave differently configured so. example, policy create priority
list outbound lanes based inbound lane, run internal simulations using
lanes found acceptable configuration. turning vehicles, list
would set outbound lanes correct direction, sorted nearest farthest.
vehicles turning, would spiral lane arrivefirst
arrival lane, lane left, lane right, two lanes left,
forth. manner, vehicle might otherwise request rejected
obtain reservation different path intersection.
3.4.2 Acceleration Intersection
Given set reservation parameters, infinite number possible trajectories
vehicle take, allowed accelerate intersection.
time step, driver agent could set vehicles acceleration value within
limits vehicles capabilities. Depending trajectory, intersection manager
may may able grant reservationone set accelerations may cause
collide another vehicle, second set might let vehicle safely.
reason, acceleration intersection must constrained intersection control
policy. Allowing driver agents decide acceleration within intersection would
require policy much conservative estimating vehicle trajectories, thereby
reducing efficiency substantially. Instead, responsibility intersection control
policy choose safe efficient acceleration schedule include Confirm
message, driver agents request accepted.
Choosing best acceleration schedule requesting vehicle, even
basic level, finding schedule intersection manager grant reservation, difficult challenge intersection control policy. initial solution
allow acceleration within intersection; driver agents required maintain
velocity throughout entire trajectory. approach several major flaws,
severe causing deadlock scenario vehicles traversed intersection
slowly, unable recover slightest decelerations. scenario
described much detail Section 4.2.
FCFS policy, implemented still takes fairly straightforward approach
problem determining acceleration schedules reservation requests. first attempts trajectory requesting vehicle accelerates quickly possible
maximum velocity soon enters intersection. cannot grant reservation
based trajectory, tries one requesting vehicle maintains constant
velocity throughout intersection. neither work, rejects request. Furthermore,
request indicates vehicle arrive sufficiently slow velocityin case
10 m/sit grant fixed-velocity reservation. grant arbitrarily slow
reservations, vehicle could use excessively large amount space-time intersection, causing vehicles undue delay. enforcing minimum velocity fixed-velocity
reservations, policy ensures vehicle spend long intersection.
complex solutions exist, solution good several reasons. First, compu604

fiA Multiagent Approach Autonomous Intersection Management




Figure 4: Several vehicles waiting intersection. reservation distance
d, front (white) vehicle incapable obtaining reservation
vehicles behind (shaded) hold conflicting reservations. white vehicles
request rejected, reservation distance decreased d0 . shaded
vehicles cancel reservations, white vehicle obtain reservation uncontested.

tationally tractable: policy runs two internal simulations per request. Second,
allows vehicles stopped moving slowly intersection clear intersection timely manner get reservation. Third, eliminates deadlock
scenario presented Section 4.2 allowing vehicles recover decelerating
cannot obtain reservation; even vehicle comes full stop intersection
accelerate back reasonable velocity crosses intersection.
3.4.3 Reservation Distance
Allowing accelerations intersection helps eliminate deadlocks, problems
arose prototype implementation significantly impaired performance
system. Frequently, lane traffic would become congested many vehicles
spawned lane. Even simulator stopped spawning vehicles lane,
lane would remain congested. problem FCFS, first described, nothing
control vehicles lane alloted reservations. best, frontmost
vehicle get reservation make intersection unhindered. However,
often case. Sometimes vehicle front cannot obtain reservation (due
congestion), must decelerate. shown Figure 4, driver agents vehicles
back may expect accelerate soon successfully reserve space-time intersection
frontmost vehicle needs. vehicles eventually make (a vehicle
might get reservation immediately vehicles behind cancel), process repeat
many times frontmost vehicle gets reservation. worst scenarios, single
vehicle continue quite time obtain reservations prevent front car
crossing intersection.
could maintain invariant vehicles get reservations unless cars
front (in lane) reservations, scenario could avoided entirely.
605

fiDresner & Stone

simple way enforce would insist vehicle get reservation unless
vehicle front already one. Unfortunately, way strictly enforce this:
vehicles communicate positions (and even did, could untruthful).
However, vehicles communicate time plan arrive
intersection, well velocity get (quantities
vehicles incentive misrepresent), possible approximate vehicles
distance intersection, given reservation request vehicle. approximate
distance, call reservation distance, va (ta t), va proposed
arrival velocity vehicle (at intersection), ta proposed arrival time
vehicle, current time. approximation assumes vehicle maintaining
constant velocity.
policy uses approximation follows. lane i, policy variable
di , initialized . reservation request r lane i, policy computes
reservation distance, d(r). d(r) > di , r rejected. If, hand, d(r) di , r
processed normal. r rejected processed normal, min(di , d(r)).
Otherwise, di .
guarantee vehicles get reservations vehicles front
already reservations, makes much likely. Two properties make
approximation particularly well-suited problem. First, vehicle stopped
intersection, reservation distance approximated zero. means
vehicle behind granted reservation isno smaller reservation distance
possible. Furthermore, reservation distance product arrival
velocity time vehicle arrives, vehicles approach intersection
slow down, reservation distance gets smaller accurate. Thus, vehicles
susceptible problem described Figure 4 likely protected
it. second property estimate uses arrival velocity vehicle,
overestimates distance vehicles expecting accelerate significantly reaching
intersection. expectation causes driver agents reserve space-time
needed vehicles front them. Note also heuristic works within
single laneeach lane keeps track reservation distance.
example Figure 4, white vehicles rejected reservation request would shorten
maximum allowed reservation distance lane. This, turn, would cause future
requests shaded vehicles immediately rejected, giving white vehicle exclusive
access (within lane) reservation mechanism. white vehicle secured
reservation, maximum allowed reservation distance would reset maximum,
vehicles would equal priority.
3.4.4 Timeouts
driver agents reservation request rejected, driver agent may immediately
make new request. Unless new request significantly different, likely
rejected well. exception request made immediately first
rejected request, driver agents estimate arrival intersection likely
change much instant consecutive requests. Eventually, vehicle
decelerated enough driver agents conflicting reservations canceled, ve606

fiA Multiagent Approach Autonomous Intersection Management

hicle obtain reservation make intersection. standpoint
intersection manager, requests successful one wasted effort.
policy runs two internal simulations per request, simulations may
computationally expensive, especially FCFS policy high granularity. Furthermore, rejected vehicle makes request every possible instant, work
add quickly.
order keep required amount computation discourage driver agents
overloading intersection manager requests, policy employs system
timeouts. driver agents request rejected, subsequent requests considered period time (determined reservation parameters) elapsed.
rejecting request, policy includes rejection message time
consider requests driver agent. implementation, time
equal + min( 12 , (ta2t) ), current time ta time arrival
request message. process serves two purposes. First, dramatically reduces
amount computation policy needs do, intersection manager receives
fewer requests. Vehicles may obtain reservations earliest moment possible,
computational savings worth it. Second, gives preference vehicles
enter intersection sooner. vehicle stopped intersection, send
requests quickly wishes, giving best chance getting reservation approved.
vehicle farther away, however, may wait full half-second attempting
make another reservation. vehicle approaches intersection, unable
procure reservation, frequency opportunities send reservation requests increases.
practice, timeouts significantly improve performance system, allowing
handle much higher traffic loads avoiding backups.
3.4.5 Buffers: Static vs. Time
system involving physical robots, noise sensor readings errors actuators
inevitably manifest themselves. Even simulation, artifacts resulting discretization time enough weaken reservation tiles guarantees exclusivity.
intersection, vehicles move high speeds different directions, potential
sources calamity cannot ignored. example, happens driver agent
realizes make reservation exactly time, close enough intersection
possible stop entering intersection? sort safety buffer
required. Two types buffers natural: static buffers time buffers.
Static buffersbuffers whose size constantcertainly suffice safety purposes.
intersection manager assumes vehicle ten times large dimension, certainly
vehicle even get close another vehicle. However, defeats point
intersection manager, leverage increased precision autonomous vehicles.
Furthermore, static buffer take account direction motion vehicle.
Two vehicles whose paths would never intersect may begin interfere one anothers
reservation process large static buffer used, Figure 5(a).
Time buffers, hand, take account motion vehicles.
intersection manager instead assumes vehicle might early late, actual
area restricted buffer shrink grow vehicles velocity,
607

fiDresner & Stone





























(a) Static buffer

(b) Time buffer, low velocity

























(c) Time buffer, high velocity

(d) Hybrid buffer

Figure 5: Various styles buffers designed cope sensor noise actuator errors.
hatched areas show buffers would cause reservation conflicts: one
pair conflicting vehicles would granted reservation.

direction movement. Figures 5(b) 5(c) show buffer scales speed
vehicle. Thus, two vehicles traveling along parallel lines, time buffers
vehicles interfere unless vehicles could potentially collide (they
lane lanes close together vehicles width). Alone, time buffers
sufficient guarantee safety small error lateral positioning (orthogonal
direction motion) may still cause collision. Figure 5(d) shows best solution:
hybrid buffer. hybrid buffer time buffer scales velocity, well
small static buffer protects lateral positioning errors serves minimum
buffer slow-moving vehicles.
3.4.6 Edge Tiles
driving open road, vehicles must maintain reasonable following interval
(usually measured amount time) one another. vehicle decelerates suddenly, puts vehicle behind dangerous situationif rear vehicle doesnt react
quickly enough, may collide front vehicle. intersection, following intervals
practical, vehicles traveling many different directions. Vehicles
intersection cannot react normally sensor readings, intersection
manager may orchestrate close calls would look like potential collision
vehicle operating open road mode. Instead, vehicles trust constraints given
intersection manager. pose problem intersection,
vehicle exits intersection, may enounter vehicle also left
intersection, much slower velocity. shown Figures 6(a) 6(b), may
lead unavoidable collision, later vehicle unable stop quickly enough.
Even autonomous vehicles, react almost instantaneously, amount
following interval required vehicles leaving intersection.
608

fiA Multiagent Approach Autonomous Intersection Management

B



B





B

(a) turns right front
B.

(b) B cannot stop time.

(c) B must slow preemptively.

Figure 6: Edge tiles prevent collisions vehicles leave intersection. 6(a), vehicle
turns front vehicle B, traveling slowly making right turn.
6(b), vehicle B gets intersection without incident, finds
leaves intersection, cannot stop colliding vehicle A.
extra buffers edge tiles, shown 6(c), prevent vehicle B obtaining
reservation would cause exit intersection close vehicle A.
shaded tiles edge tiles, darkly shaded tiles specific tiles
would prevent collision 6(a) 6(b).

first-cut solution problem simply increase time buffers reservation tiles desired following interval. Thus, vehicles require following interval
one second exiting intersection, vehicle able reserve tile
within one second another vehicle. ensures vehicles leaving intersection
lane exit within one second other, gap
least one second vehicles. Unfortunately, wreaks havoc FCFSs
ability conduct vehicles efficiently intersection. close calls
system gets efficiency advantages longer possible.
Instead, divide reservation tiles two groups. Internal tiles tiles
surrounded sides reservation tiles. Edge tiles, shown shaded
Figure 6(c), tiles abut intersection. sufficiently high granularities, edge
tiles relatively small fraction total number tiles. tiles
increase time buffer desired following interval. (at sufficiently
high granularities) vehicles leaving lane require edge tiles,
modification enforces desired following intervals without otherwise preventing
intersection exploiting ability interleave vehicles closely.
3.5 Policies
layer abstraction provided protocol, intersection manager
work emulation mode, imitating modern-day control mechanisms, stop
sign traffic light. briefly explain implementation two intersection control
policies designed mimic mechanisms.
609

fiDresner & Stone

Algorithm 2 FCFSs request processing algorithm. FCFS persistent state variables:
tiles, map tiles times vehicles, reservations, map vehicles sets
tiles, timeouts, map vehicles times.
1: tc current time
2: timeouts[vehicle id] < tc
3:
reject request
4: ta proposed arrival time
5: timeouts[vehicle id] tc + min(0.5, (ta tc )/2)
6: acceleration {true, false}
7:
tile times {}
8:
ta
9:
V temporary vehicle initialized according reservation parameters
10:
V intersection
11:
tiles occupied V V static buffer time
12:
tile times tile times {(t, S)}
13:

14:
edge tile
15:
buf edge tile buffer
16:
else
17:
buf internal tile buffer
18:
= buf buf
19:
tiles[s, + i] reserved another vehicle
20:
acceleration
21:
goto line 29
22:
else
23:
reject request
24:
+ time step
25:
move V according physical model
26:
acceleration
27:
increase V velocity V maximum acceleration
28:
break
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:
39:

request change
old tile times reservations[vehicle id]
(ti , Si ) old tile times
Si
clear reserved status tiles[s, ti ]
(ti , Si ) tile times
Si
tiles[s, ti ] vehicle id
reservations[vehicle id] tile times
accept request, return reservation constraints (incl. accelerations)

610

fiA Multiagent Approach Autonomous Intersection Management

Stop-Sign Stop signs traditionally used intersections light traffic.
much cost-effective reliable, cannot provide throughput
efficiency traffic light. Thus, would never reason system
emulate stop sign, however include description completeness.
Stop-Sign exactly like FCFS, except accepts reservations vehicles
stopped intersection. reservation requests rejected
message indicating vehicle must stop intersection. intersection
determines whether vehicle stopped intersection examining difference
current time arrival time request message.
Traffic-Light Traffic-Light policy receives reservation request message,
calculates next time proposed arrival time light sending
vehicles lane green. responds confirmation message reflects
information. confirmation messages maximum tolerable errors associated them, intersection manager uses errors encode beginning
end green light period.
3.6 Compatibility Human Drivers
intersection control mechanism autonomous vehicles someday
useful, always people enjoy driving. Additionally, fairly
long transitional period current situation (all human drivers) one
human drivers rarity. Even switching system comprised solely autonomous
vehicles possible, pedestrians cyclists must also able traverse intersections
controlled safe manner. reason, necessary create intersection control
policies aware able accommodate humans, whether bicycle,
walking corner store, driving classic car entertainment purposes.
section explain extended FCFS policy reservation framework
incorporate human drivers. order accommodate human drivers, control policy
must able direct human autonomous vehicles, coordinating them,
despite much less control information regarding human
drivers be. main concept behind extension assumption
human-driven vehicle anywhere one could be. may less efficient
approach attempts precisely model human behavior, guaranteed
safe, one desiderata unwilling compromise. Adding pedestrians
cyclists follows naturally, give brief descriptions would differ
extensions human drivers.
Compatibility human drivers offers ability handle occasional
human driver levels human drivers everyday traffic reaches steady state.
also help facilitate transition current standardall human-driven vehicles
steady state, human drivers scarce. Section 2.1, emphasized
need incremental deployability. show experimentally, human compatibility
adds significantly incremental deployability reservation system. also
show specifics implementation offer benefits: incentives
communities private individuals adopt autonomous vehicle technology.
611

fiDresner & Stone

3.6.1 Using Existing Infrastructure
reliable method communicating human drivers prerequisite including
system. simplest best solution use something human drivers
already know understand traffic lights. Traffic light infrastructure already present
many intersections engineering manufacturing traffic light systems well
developed. pedestrians cyclists, standard push-button crossing signals
used give enough time person traverse intersection. also serve
alert intersection presence.
3.6.2 Light Models
real traffic lights used communicate human drivers, must controlled
understood intersection manager. Thus, add new component
intersection control policy, called light model. light model controls physical lights
well providing information policy make decisions.
complicated scenarios, light model modified control policy, example,
order adapt changing traffic conditions. lights semantics
modern-day lights: red (do enter), yellow (if possible, enter; light soon
red), green (enter). control policy requires light model human users
know do. instance, light model FCFS keeps lights red
times, indicating humans never safe enter. Traffic-Light policys light
model, hand, corresponds exactly light system policy emulating.
Here, describe light models used experiments.
All-Lanes model, similar current traffic light systems,
direction succession gets green lights lanes. Thus, northbound traffic (turning
going straight) green lights eastbound, westbound, southbound traffic
red lights. green lights cycle directions. similar
current traffic lights, light model particularly well-suited controlling
distributions vehicles significant contingents human drivers. demonstrate
fact experimentally Section 4.5. Figure 7 shows graphical depiction light model.

Figure 7: All-Lanes light model. direction gets green lights cycle: north,
east, south, west. phase, available paths autonomous
vehicles red lights right turns.

Single-Lane Single-Lane light model, green light rotates lanes
one time instead direction. example, left turn lane northbound traffic
612

fiA Multiagent Approach Autonomous Intersection Management

would green light, lanes would red light. Next, straight
lane northbound traffic would green light, right turn. Next, green
light would go lane eastbound traffic, forth. graphical description
models cycle seen Figure 8. light model work well
vehicles human-driven, show, useful intersections
control mostly autonomous vehicles need also handle occasional human
driver.

Figure 8: Single-Lane light model. individual lane gets green light (left turn,
straight, right turn), process repeated direction. Note
smaller part intersection used human vehicles given
time. rest intersection available autonomous vehicles.

3.6.3 FCFS-Light Policy
order obtain benefits FCFS policy still accommodating human
drivers, policy needs two things:
1. light green, ensure safe vehicle (autonomous human-driven)
drive intersection lane light regulates.
2. Grant reservations driver agents whenever possible. Autonomous vehicles thus
move red lights (whereas humans cannot), provided reservation
similar right red, extended much safe situations.
policy FCFS-Light, these, described follows:
FCFS, intersection divided grid n n tiles.
Upon receiving request message, policy uses parameters message
establish vehicle arrive intersection.
light controlling lane vehicle arrive intersection
green time, reservation confirmed.
light controlling lane yellow, reservation rejected.
light controlling lane red, journey vehicle simulated
FCFS (Section 3.4).
throughout simulation, required tile reserved another vehicle use
lane green yellow light, policy reserves tiles confirms
reservation. Otherwise, request rejected.
613

fiDresner & Stone

REQUEST
Preprocess

Yes,
Restrictions

Red

Light Model

No, Reason

FCFS

REJECT

Postprocess

Yellow

Driver
Agent

Green

CONFIRM

Intersection Manager

Figure 9: FCFS-Light combination FCFS light model. request
received, FCFS-Light first checks see color light be.
green, grants request. yellow, rejects. red, defers
FCFS.

Off-Limits Tiles Unfortunately, simply deferring FCFS guarantee safety
vehicle. vehicle granted reservation conflicts vehicle following
physical lights, collision could easily ensue. determine tiles use
light system given time, associate set off-limits tiles light.
example, light northbound left turn lane green (or yellow), tiles
could used vehicle turning left lane considered reserved
purposes FCFS. length yellow light adjusted vehicles entering
intersection enough time clear intersection tiles longer
limits.
FCFS-Light Subsumes FCFS Using traffic lightlike light model (for example AllLanes), FCFS-Light behave exactly like Traffic-Light drivers human.
light model keeps lights constantly red, FCFS-Light behaves exactly like
FCFS. case, human drivers present fail spectacularly, leaving
humans stuck intersection indefinitely. However, absence human drivers,
perform exceptionally well. FCFS special case FCFS-Light. thus
alter FCFS-Lights behavior vary strictly superior Traffic-Light exactly
FCFS.
3.7 Emergency Vehicles
current traffic laws special procedures involving emergency vehicles
ambulances, fire trucks, police cars. Vehicles required pull side
road come complete stop emergency vehicle passed.
emergency vehicle may traveling quickly, posing danger
vehicles, emergency vehicle must arrive destination quickly
possiblelives may stake. Hopefully, system implemented,
automobile accidentsa major reason emergency vehicles dispatchedwill
eradicated. Nonetheless, emergency vehicles still required time time fires,
heart attacks, emergencies still exist. previously proposed
methods giving priority emergency vehicles (Dresner & Stone, 2006),
present new, simpler method, fully implemented tested.
614

fiA Multiagent Approach Autonomous Intersection Management

3.7.1 Augmenting Protocol
order accommodate emergency vehicles, intersection manager must first able
detect presence. easiest way accomplish add new field
request messages. implementation, field simply flag indicates
intersection manager requesting vehicle emergency vehicle emergency
situation (lights flashing siren blaring). practice, however, safeguards would need
incorporated prevent normal vehicles abusing feature order obtain
preferential treatment. could accomplished using sort secret key instead
simply boolean value, even sort public/private key challenge/response mechanism. details implementation, however, beyond scope project
already well-studied area cryptography computer security.
3.7.2 FCFS-Emerg Policy
intersection control policy detect emergency vehicles, process reservation requests giving priority emergency vehicles. first-cut solution simply
deny reservations vehicles emergency vehicles. However, solution
satisfactory, traffic comes stop due rejected reservation
requests, emergency vehicles may get stuck resulting congestion. Instead,
FCFS-Emerg policy keeps track lanes currently contain approaching emergency
vehicles. long least one emergency vehicle approaching intersection,
policy grants reservations vehicles lanes. ensures vehicles front
emergency vehicles also receive priority. Due increase priority, lanes
emergency vehicles tend empty rapidly, allowing emergency vehicles proceed
relatively unhindered.
3.8 Summary
section, explained created reservation-based intersection control
mechanism simulation. described construction simulator itself, well
communication protocol, intersection manager, driver agent, several
intersection control policies. first policy, FCFS fully autonomous vehicles.
FCFS-Light extends FCFS allow human interoperability using existing traffic light
infrastructure. last policy, FCFS-Emerg, extends FCFS give priority emergency
vehicles without significant increasing delays vehicles.

4. Experimental Results
section, fully test features introduced Section 3 demonstrate
reservation system reduce delay two orders magnitude. experiments evaluate performance reservation system using different intersection control policies,
amounts traffic, granularities, levels human drivers, presence emergency
vehicles. first compare system using FCFS traffic lights varying cycle periods
using prototype simulator. show results full version, including stop
sign control policy implemented protocol, comparing results
traffic light experiments. Next, experiment allowing vehicles turn
615

fiDresner & Stone

lanesomething would extremely dangerous without reservation-based mechanism. Finally, evaluate two extensions FCFS: FCFS-Light FCFS-Emerg.
Videos simulator action, including many scenarios section, well
supplementary materials found http://www.cs.utexas.edu/~kdresner/aim/.
4.1 Low-Granularity FCFS vs. Traffic Light
simplest implementation FCFS granularity 1the entire intersection single reservation tile. one vehicle may intersection time,
vehicle traveling sufficiently fast, total amount time occupy
intersection small. increase granularity 2, intersection longer entirely exclusive. example, non-turning vehicles traveling north longer compete
reservation tiles non-turning vehicles traveling south (similarly, eastbound
westbound non-turning vehicles longer compete). present initial results
comparing two instances reservation mechanism several incarnations
traffic light.
4.1.1 Experimental Setup
experiments carried using prototype version simulator, fully
described earlier publication (Dresner & Stone, 2004). version simulator,
vehicles allowed turn accelerate intersection. restrictions
detract core challenge problem, results relevant even
restrictions relaxed. simulation contains one lane traveling direction,
speed limits 25 meters per second. Traffic spawning probability varies
0.0001 0.02 increments 0.0001, configuration runs 500,000 steps
simulator, corresponds approximately 2.5 hours simulated time.
4.1.2 Results
Figure 10(a) shows delay times traffic light systems varying periods, ranging
extremely short (10 seconds) fairly long (50 seconds). expected real-life experience, short-period traffic lights control light traffic well, traffic lights longer
periods work better heavy-traffic scenarios. traffic sparse, short period allows
vehicles wait shorter time getting green light. many cities, traffic light
periods shortened early hours morning take advantage fact.
scenarios densely packed vehicles, per-vehicle costs slowing stop
accelerating back full speed, well intervals needed clear intersection
(the time yellow light, lights red), tend dominate.
makes longer-period lights better situations. Figure 10(a),
certain traffic level, traffic light systems reaches appears maximum
delay level. artifact simulatorwhen traffic level gets high enough,
vehicles back far simulator cannot keep track (it cannot spawn
new vehicles, lack place put them). point, vehicles arriving
intersection faster traffic lights safely coordinate passage. Thus,
point delay spikes upwards indicates maximum throughput traffic
configuration.
616

fi100
90
80
70
60
50
40
30
20
10
0

0.7
Period 10

0.6

Period 30

Average Delay (s)

Average Delay (s)

Multiagent Approach Autonomous Intersection Management

Period 50

Granularity 1

0.5
0.4
0.3
Granularity 1
0.2
Granularity 2

0.1
0
0.2

0.4

0.6

0.8

1

0.2

Traffic Level (vehicles/s)

0.4

0.6

0.8

1

Traffic Level (vehicles/s)

(a) Reservation vs. Traffic Light

(b) Increasing Granularity

Figure 10: 10(a) shows average delays traffic light systems period 10, 30, 50 seconds plotted varying traffic levels along 1-tiled reservation-based
system. 10(b) shows average delays granularity 1 2 FCFS policies
varying traffic levels. Spawning probability varied increments 0.0001,
configuration run 1,000,000 steps simulation (approximately
5.5 hours simulated time). direction 1 lane.

Also Figure 10(a) delays granularity-1 2 FCFS policies.
car spawning probability 0.013, granularity-1 policys delay visually
indistinguishable x-axis, true granularity-2 reservation system
whole graph. Figure 10(b) shows bottom 0.7% graph, enlarged show
results detail. vehicle spawning rate 0.02, traffic light
systems already beyond maximum capacity, granularity-2 system allowing
vehicles without even adding tenth second average vehicles travel
time.
4.2 Choosing Granularity
note Figure 10(a) spike delay granularity-1 FCFS policy. system looks though behaving chaoticallyin Figure 10(b), delay slowly steadily
increases traffic level, spiking graph probability spawning
vehicle time step reaches 0.013.
granularity-1 system, vehicles traveling parallel one another compete
tiles. also happens vehicles lanes closest middle
road whenever granularity small, odd number, Figure 11(b). Recall
prototype simulator, acceleration intersection forbidden. Thus, vehicle
slows cannot obtain reservation, finally get reservation
moving slowly entirety reservation occupy reservation
tiles longer period time. next car approach intersection therefore
likely slow well. process feeds vehicles slow
more. small average amounts traffic, delays increase, system
recovers probabilistically generated periods light traffic. However, heavy
617

fiDresner & Stone

traffic, intersection eventually reach deadlocked state. traffic generated
stochastically, could happen early late experiment. happens early,
large effect average delay, whereas happens late, effect smaller.
Deadlocking difficult measure quantitatively, progresses, driver agents
make reservations long periods timeso long, fact, overflow
memory computer running simulator. effect seen rough line
Figure 10(a). explore effects granularity, ran several experiments,
varying granularity well number lanes.

(a) Granularity 8

(b) Granularity 9

Figure 11: Increasing granularity always improve performance. 11(a),
granularity 8 suffices. 11(b), increasing granularity 9 actually hurts
performancevehicles traveling parallel (but opposite directions) competing middle row tiles.

4.2.1 Experimental Setup
experiments also used prototype simulator described Section 4.1.1.
data point represents 500,000 steps simulation (approximately 2.5 hours simulated
time). traffic level fixed 0.2 vehicles per second.
4.2.2 Results
shown Figure 12, 2 lanes direction, 2 2 grid performs better
3 3 grid. Increasing 4 4 grid better 2 2, increasing 5 5
worse. increase granularity correspond decrease delay. However,
small granularities, incrementing granularity small even number small
odd number actually increases delay. case maximum delay, even granularity-2
618

fiA Multiagent Approach Autonomous Intersection Management

0.14

3

0.12

2.5

0.1

Maximum Delay (s)

Average Delay (s)

system performs better granularity-5 system; ill effects odd granularities
shown Figure 11 tend slow unfortunate vehicles.

0.08
0.06
0.04

2

1.5

1

0.5

0.02
0

0
2

3

4

5

2

Granularity

3

4

5

Granularity

(a) Average delay

(b) Maximum delay

Figure 12: Simulation statistics FCFS policies varying granularity. 2
lanes direction traffic level 0.2 vehicles per second.
experiment run 500,000 simulation steps. Note increasing granularity always improve performance.
experiment suggests FCFS always run granularity high enough
vehicles never cross paths never compete reservation tiles.
Figure 13 shows, lanes require higher granularity (though even low granularity,
system out-performs traffic light). However, computational complexity
system increases proportional square granularity, granularity
increased indiscriminately.
4.3 Full Power FCFS
earlier experiments used prototype simulator, experiments use full power
FCFSturning, acceleration, modifications Section 3.4. vehicles
turn, thus always travel within line reservation tiles, increasing granularity
beyond twice number lanes improve performance even more. addition
FCFS, evaluate stop sign policy presented Section 3.2.
Technically, optimal delay individual vehicle delay all. However,
although vehicle could experience delay low 0 seconds, turning vehicles may need
slow avoid losing control. order create worthwhile benchmark
compare reservation system, empirically measure optimal average delay
intersection manager. this, use special control policy accepts requests.
also deactivate vehicles ability detect vehicles, eliminating interactions
them. results presented optimal control policy,
optimal terms delay, provides safety guarantees.
Small intersections slow-moving traffic tend amenable control traffic
lights. light traffic usually regulate fairly effectively. example, consider
intersection stop signall vehicles must come stop, afterwards may proceed
619

fiDresner & Stone

6 Lanes
3 Lanes
2 Lanes
1 Lane

0.14

Average Delay (s)

0.12
0.1
0.08
0.06
0.04
0.02
0
1

2

6
Granularity

12

48

Figure 13: Average delays FCFS policy independently varying numbers lanes
granularity. Increasing granularity beyond twice number lanes
results marginal improvements. simulations run least
500,000 steps. 6 lanes 1 tile deadlocks overflows system memory
500,000 steps complete.

intersection clear. situations, stop sign often much efficient
traffic light, vehicles never stuck waiting light change
cross-traffic. protocol enables us define control policy, compare
experimentally policies. Note policy much efficient
actual stop sign, vehicle stopped intersection, driver agent
intersection determine car may safely proceed much precisely
much less conservatively human driver.
4.3.1 Experimental Setup
simulator simulates 3 lanes 4 cardinal directions. speed limit
lanes 25 meters per second. Every configuration shown run least 100,000 steps
simulator, corresponds approximately half hour simulated time. Vehicles
spawned turn probability 0.1, turning vehicles turn left right
equal probability. Vehicles turning right spawned right lane, whereas vehicles
turning left spawned left lane. Vehicles turning distributed
probabilistically amongst lanes traffic lane equal possible.
FCFS stop sign (implemented extension FCFSsee Section 3.5)
granularity 24.
4.3.2 Results
results experiments shown Figure 14. expected, average delay
optimal system positive nonzero, small.
620

fiA Multiagent Approach Autonomous Intersection Management

FCFS performs well, nearly matching performance optimal policy.
higher levels traffic, average delay vehicle gets high 0.35 seconds,
never 1 second optimal. none tested conditions FCFS
even approach delay traffic light system previous experiment, shown
Figure 10(a).
stop sign perform well FCFS, low amounts traffic,
still performs fairly well, average delay 3 seconds greater optimal.
However, traffic level increases, performance degrades. difficult imagine
scenario implementation stop sign would actually usedit requires
technology reservation system, advantages
FCFS.
5
4.5

Stop Sign

4
3.5
Delay (s)

Traffic Light Minimum
3
2.5
2
1.5
1

Optimal

0.5

FCFS

0
0

0.5

1

1.5

2

2.5

Traffic Level (vehicles/s)

Figure 14: Delays varying amounts traffic FCFS, stop sign, optimal
system.

4.4 Allowing Turns Lane
traditional traffic systems, especially traffic lights, vehicles wishing turn
onto cross street must specially designated turning lanes. helps prevent
cars want turn holding non-turning traffic. However, system like
reservation system, restriction longer necessary. nothing inherent
reservation system demands vehicles turn specific lane. Investigating
effects allowing turning lane produced surprising results. seen
Figure 15, relaxing restriction actually hurts FCFSs performance slightly. one
621

fiDresner & Stone

might think allows vehicles flexibility, average increases resources
used one turning vehicle. making left turns left lane right turns
right lane, vehicles travel shorter distance reserve reservation tiles
less heavily used. However, experiments may misleading. Vehicles changing
lanes get designated turn lane could potentially delay vehicles behind
process. currently model lane changing intersection,
able experimentally verify conjecture.
1
Fixed Lane
Lane

Delay (s)

0.8

0.6

0.4

0.2

0
0

0.5

1
1.5
Traffic Level (vehicles/s)

2

2.5

Figure 15: Comparison FCFS policy traditional turns one allowing turning
lane. Allowing turns lane decreases performance slightly,
producing longer delays.

4.5 Effects Human Interoperability
Section 4.3, showed vehicles autonomous, intersection-associated
delays reduced dramatically. following experiments suggest stronger result:
using two light models presented Section 3.6.2, delays reduced stage
adoption. Furthermore, additional incentives exist stage drivers switch
autonomous vehicles.
4.5.1 Experimental Setup
experiments, simulator models 3 lanes 4 cardinal directions.
speed limit lanes 25 meters per second. intersection control policy
reservation tiles, granularity 24. simulator spawns vehicles turning left
left lane, vehicles turning right right lane, vehicles traveling straight
622

fiA Multiagent Approach Autonomous Intersection Management

center lane1 . Unless otherwise specified, data point represents 180000 time steps,
one hour simulated time. simulated human-driven vehicles use two-second
following distance, use lane-following algorithm autonomous drivers.
also employ point-of-no-return mechanism reacting lightsif vehicle
stop yellow red light, does, otherwise proceeds.
4.5.2 Results
present experimental results human-compatible policies two parts.
first focuses policies facilitate smooth transition all-autonomous
mostly-autonomous vehicle system. second focuses incentives throughout
process, global individual, continue deployment system. Combined,
results suggest incremental deployment (one desiderata) technically
possible desirable.
Transition Full Deployment purpose hybrid intersection control policy
confer benefits autonomy passengers driver-agent controlled vehicles
still allowing human users participate system. Figure 16 shows smooth
monotonically improving transition modern-day traffic lights (represented
Traffic-Light policy) completely mostly autonomous vehicle mechanism (FCFSLight Single-Lane light model). early stages (100%-10% human), AllLanes light model used. Later (less 10% human), Single-Lane light model
introduced. change (both driver populations light models), delays
decreased. Notice rather drastic drop delay FCFS-Light All-Lanes
light model FCFS-Light Single-Lane light model. Although none
results quite close minimum pure FCFS, Single-Lane light model
allows greater use intersection FCFS portion FCFS-Light policy,
translates higher efficiency lower delay.
systems significant proportion human drivers, All-Lanes light model
works wellhuman drivers experience would Traffic-Light
policy, autonomous driver agents extra opportunities make
intersection. small amount benefit passed human drivers, may
find closer front lane waiting red light turn green.
explore much average vehicle would benefit, ran simulator FCFSLight policy, All-Lanes light model, 100%, 50%, 10% rate human drivers.
means vehicle spawned, receives human driver (instead driver
agent) probability 1, .5, .1 respectively. seen Figure 17, proportion
human drivers decreases, delay experienced average driver also decreases.
decreases large brought Single-Lane light model,
least possible significant numbers human drivers.
Incentives Individuals Even without sort autonomous intersection control
mechanism, incentives humans switch autonomous vehicles.
1. constraint likely relax future. included work give Single-Lane
light model flexibility fair comparison FCFS policy, performs even better
absence.

623

fiDresner & Stone

60

50

5% Human

Delay (s)

40

30
100% Human
20
1% Human

10% Human
10

Fully Autonomous
0
0

0.5

1
1.5
Traffic Level (vehicles/s)

2

2.5

Figure 16: Average delays vehicles function traffic level FCFS-Light
two different light models: All-Lanes light model, well-suited
high percentages human-driven vehicles, Single-Lane light model,
works well relatively human-driven vehicles. adoption
autonomous vehicles increases, average delays decrease.

20

Delay (s)

15

10

5
TRAFFIC-LIGHT
FCFS-LIGHT 50% Human
FCFS-LIGHT 10% Human
0
0

0.25

0.5
Traffic Level (vehicles/s)

0.75

1

Figure 17: Average delays vehicles function traffic level FCFS-Light
All-Lanes light model. Shown results 100%, 50%,
10% human-driven vehicles. 100% case equivalent Traffic-Light
policy. Note average delay decreases percentage human-driven
vehicles decreases.

driving, well myriad safety benefits strong incentives promote
autonomous vehicles marketplace. experimental results suggest additional incentives. Using reservation system, autonomous vehicles experience lower average delays
human-driven vehicles difference increases autonomous vehicles become
prevalent.
Figure 18 shows average delays human drivers compared autonomous driver
agents FCFS-Light policy using All-Lanes light model. experiment,
half drivers human. Humans experience slightly longer delays autonomous
vehicles, worse Traffic-Light policy. Thus, putting
624

fiA Multiagent Approach Autonomous Intersection Management

autonomous vehicles road, drivers experience equal smaller delays compared
current situation. expected autonomous driver everything
human driver more.
35
Humans
Autonomous
30

Delay (s)

25
20
15
10
5
0
0

0.25

0.5
Traffic Level (vehicles/s)

0.75

1

Figure 18: Average delays human-driven vehicles vehicles function traffic
level FCFS-Light All-Lanes light model. experiment,
50% vehicles human driven. Autonomous vehicles experience slightly
lower delays across board, human drivers experience delays worse
Traffic-Light policy.

reservation system widespread use autonomous vehicles make
vast majority road, door opened even efficient light
model FCFS-Light policy. low concentration human drivers,
Single-Lane light model drastically reduce delays, even levels overall traffic
Traffic-Light policy handle. Using light model, autonomous drivers
pass red lights even frequently fewer tiles off-limits given
time. Figure 19 compare delays experienced autonomous drivers
human drivers 5% drivers human thus Single-Lane light model
used. improvements using All-Lanes light model benefit drivers
extent, Single-Lane light models sharp decrease average delays (Figure 16)
comes high price human drivers.
shown Figure 19, human drivers experience much higher delays average.
lower traffic levels, delays even higher associated TrafficLight policy. Figure 16 shows despite this, high levels traffic, human drivers
benefit relative Traffic-Light. Additionally, intersections using FCFS-Light still
able handle far traffic using Traffic-Light.
SingleLane light model effectively gives humans high, fairly constant
delay. green light one lane comes around lane
green light, human-driven vehicle may find sitting red light time
light changes. However, since light model would put operation
human drivers fairly scarce, huge benefit 95% 99% vehicles
far outweighs cost. light model detects reacts presence human
625

fiDresner & Stone

60
Humans
Autonomous
50

Delay (s)

40

30

20

10

0
0

0.25

0.5
Traffic Level (vehicles/s)

0.75

1

Figure 19: Average delays human-driven vehicles vehicles function traffic
level FCFS-Light Single-Lane light model. Humans experience
worse delay Traffic-Light, average delay vehicles much
lower. experiment, 5% vehicles human-driven.

drivers might able achieve even better overall performance, without causing human
drivers wait long.
data suggest incentive early adopters (persons
purchasing vehicles capable interacting reservation system) cities
towns. properly equipped vehicles get going faster (not
mention safely). Cities towns equip intersections utilize
reservation paradigm experience fewer traffic jams efficient use roadways
(along fewer collisions less wasted gasoline). penalty
human drivers (which would presumably majority point), would
reason party involved oppose introduction system. Later,
drivers made transition autonomous vehicles, Single-Lane light
model introduced, incentive move new technology increasedboth
cities individuals. time, autonomous vehicle owners far outnumber human
drivers, still benefit traffic worst.
4.6 Emergency Vehicle Experiments
already shown FCFS significantly reduce average delays
vehicles, FCFS-Emerg helps reduce delays emergency vehicles even further.
4.6.1 Experimental Setup
demonstrate improvement, ran simulator varying amounts traffic,
keeping proportion emergency vehicles fixed 0.1% (that is, spawned vehicle
made emergency vehicle probability 0.001). small number
emergency vehicles created realistically low proportions, ran configuration
(data point) 100 hours simulated timemuch longer experiments.
626

fiA Multiagent Approach Autonomous Intersection Management

4.6.2 Results
shown Figure 20, emergency vehicles average experience lower delays
normal vehicles. amount emergency vehicles outperform normal
vehicles increases traffic increases, suggesting designed, FCFS-Emerg helps
traffic contending space-time intersection.
10

Delay (s)

8

6

4

2
Vehicles
Emergency Vehicles
0
0

1

2
3
Traffic Level (vehicles/s)

4

5

Figure 20: Average delays vehicles emergency vehicles function traffic
level FCFS-Emerg policy. One thousand vehicles (on average)
emergency vehicle. Delays emergency vehicles lower data
points.

5. Performance Failure Modes
Fully autonomous vehicles promise enormous gains safety, efficiency, economy
transportation. However, gains realized, plethora safety reliability concerns must addressed. previous sections, assumed
vehicles perform without gross malfunctions. section, relax assumption
demonstrate reservation-based mechanism reacts scenarios malfunctions occur. Additionally, intentionally disable elements system order
investigate necessity efficacy.
5.1 Causes Accidents
collision purely autonomous traffic number causes, including software
errors driver agent, physical malfunction vehicle, even meteorological
phenomena. modern-day traffic, factors largely ignored two reasons. First,
exclusively human-populated system, generous margins error,
sensitive small moderate aberrations. Second, none significant
respect driver error causes accidents (Wierwille, Hanowski, Hankey, Kieliszewski,
Lee, Medina, Keisler, & Dingus, 2002). However, future infallible autonomous
driver agents, exactly issues prevalent causes automobile
collisions. safety allowances explained Sections 3.4.5 3.4.6 adjustablegiven
maximum allowable error vehicle positioning, buffers extended handle
627

fiDresner & Stone

errorbut reasonable adjustment account gross mechanical malfunction like
blowout failed brakes. types issues infrequent, believe safety
intersection control mechanism acceptable even individual occurrences
slightly worse accidents today.
5.2 Adding Safety Net
One easily imagine badly accident efficient system could without
reactive safety measures place. Here, explain system deals
rare, dangerous events. show Section 5.3, disabling safety measures
leaves system prone spectacular failure modes, sometimes involving dozens vehicles.
Intact, measures make events much manageable.
5.2.1 Assumptions
Section 5.3, show reactive safety measures reduce average number
vehicles involved crash dozens one two. However, order employ
safety measures fully, must make additional assumptions.
Detecting Problem First, assume intersection manager able detect
something gone wrong. certainly non-trivial assumption, without
it, substantial mitigation possible. Simply put, intersection manager cannot react
something cannot detect. two basic ways intersection manager
could detect vehicle encountered sort problem: vehicle inform
intersection manager, intersection manager detect vehicle directly.
instance, event collision, device similar triggers airbag
send signal intersection manager. Devices like already exist aircraft emit
distress signals locator beacons event crash. intersection manager
might notice less severe problem, vehicle supposed be,
using cameras sensors intersection. However, method detection likely
much slower react problem. advantages disadvantages,
combination two would likely safest. specifics implementation
beyond scope analysis. important whenever vehicle violates
reservation way, intersection manager become aware soon possible.
simulations deal collisions, assume colliding vehicle sends
signal intersection manager becomes aware situation immediately.
described Appendix B, protocol includes Done message vehicles transmit complete reservations. One way reliably sense vehicle
distress would notice missing Done message. approach two drawbacks.
First, Done message optional, mainly incentive driver
agent transmit it. Second, intersection manager may able notice missing message time incident occurred. intend investigate
alternative future work.
Informing Vehicles also assume exists way intersection
manager broadcast fact something wrong vehicles. Since intersection manager already communicate vehicles, big assumption.
628

fiA Multiagent Approach Autonomous Intersection Management

However, mode communication bit different employed rest
communication protocol (see Appendix B). normal operating conditions, individual
messages containing multiple pieces information transmitted agents.
cannot verify receipt messages without response, semantics
protocol ensure whenever message sent, sending agent makes conservative assumptionin case Request message, received;
case Confirm message, was. event collision, however, intersection
manager needs communicate one bit information many vehicles possible:
something wrong. important vehicles receive message,
transmitted repeatedly, vehicles, exclusion messages.
would like assume vehicles receive message, show Section 5.3
even significant number vehicles not, safety measures place still protect
many vehicles would otherwise wind crashing.
5.2.2 Incident Mitigation
vehicle deviates significantly planned course intersection resulting physical harm vehicle presumed occupants, refer situation
incident. incident occurred, first priority ensure safety
persons vehicles nearby. expect incidents infrequent
occurrences, re-establishing normal operation intersection lower priority
optimization process left future work.
Intersection Manager Response soon intersection manager detects
notified incident, immediately stops granting reservations. subsequent received
requests rejected without consideration. Due nature protocol, intersection manager cannot revoke reservations, driver agents would incentive
acknowledge receipt. However, intersection manager send message
vehicles incident occurred. message special Emergency-Stop message, intersection manager may send emergency situation,
(as rest protocol) must assume received.
Emergency-Stop message lets vehicles know event taken place
intersection that:
reservations accepted
vehicles able come stop entering intersection
vehicles intersection longer assume near misses result
collisions
human-compatible policies, FCFS-Light, intersection manager also
turns lights red. real-world implementation, conspicuous visual cue could
provided, semantically important intersection informs human
drivers may enter.
Vehicle Response Emergency-Stop message useful way, driver
agents must react it. explain specific actions implementation driver
agent takes receives message. Normally, approaching intersection,
driver agent ignores vehicles sensed intersection. might
otherwise appear imminent collision open road almost certainly precisely
629

fiDresner & Stone

coordinated near-miss intersection. However, driver agent receives
Emergency-Stop message intersection manager, disables behavior.
vehicle intersection, driver agent blindly drive another vehicle
help it. vehicle intersection stop time, enter,
even reservation.
first inclination make driver agent immediately decelerate
stop, quickly realized safest behavior. vehicles receive
message come stop, vehicles would otherwise cleared intersection without
colliding may find stuck intersectionanother object vehicles
run into. especially true vehicle caused incident edge
intersection unlikely hit. Trying stop vehicles
intersection makes situation worse.
driver agent detect impending collision, take evasive actions
apply brakes. Since true multiagent system self-interested agents,
cannot prevent driver agents so, even detrimental vehicles overall.
Thus, driver agent brakes believes collision imminent.
5.3 Experiments
order evaluate effects reactive safety measures, performed several experiments various components intentionally disabled. various configurations
separated three classes. oblivious intersection manager takes action
upon detecting incident. intersection manager utilizing passive safety measures stops
accepting reservations, send Emergency-Stop messages nearby driver
agents. Finally, active configuration intersection managerwhich corresponds
full version protocol specified Appendix Bhas safety features
place. addition considering three incarnations intersection manager,
also study effects unreliable communication active case. Note
vehicles receive Emergency-Stop message, active passive configurations
identical.
5.3.1 Experimental Setup
great efficiency reservation-based system comes extreme sensitivity
error. buffering might protect minute discrepancies, cannot hope cover
gross mechanical malfunctions. determine much effect malfunction
would have, created simulation individual vehicles could crashed, causing
immediately stop remain stopped. Whenever vehicle crashed
comes contact one is, becomes crashed well. model
specifics individual impacts, allow us estimate malfunction might
lead collisions.
order ensure included malfunctions different parts intersection,
triggered incident choosing random (x, y) coordinate pair inside intersection, crashing first vehicle cross either x coordinate. akin
creating two infinitesimally thin walls, one horizontal vertical, intersect
(x, y). Figure 21 provides visual depiction process.
630

fiA Multiagent Approach Autonomous Intersection Management

Figure 21: Triggering incident intersection simulator. dark vehicle turning
left crashed crossed randomly chosen x coordinate. different vehicle crossed x coordinate randomly chosen coordinate
earlier, would crashed instead.

initiating incident, ran simulator additional 60 seconds, observing
subsequent collisions recording occurred. Using information,
constructed crash log, essentially histogram crashed vehicles. step
remaining simulation, crash log indicates many vehicles crashed
step. averaging many crash logs configuration, able
construct average crash log, gives picture typical incident would
produce.
system compatible humans, included experiments humancompatible intersection control policy. demonstrated Section 4.5, significant
number human drivers present, FCFS-Light cannot offer much performance
benefit traditional traffic light systems. such, limited experimentation
scenarios 5% vehicles controlled simulated human drivers, used
Single-Lane light model (see Section 3.6.2). 5% human drivers, FCFSLight policy still create lot precarious situations focus
investigation.
experiments, ran simulator scenarios 3, 4, 5, 6 lanes
four cardinal directions, although discuss results 3-
6-lane cases (other results similar) sake brevity. earlier experiments,
vehicles spawned equally likely directions, generated via Poisson process
controlled probability vehicle generated step. Vehicles
generated set destination15% vehicles turn left, 15% turn right,
remaining 70% go straight. before, leftmost lane always left turn lane,
right lane always right turn lane. Turning vehicles always spawned correct
lane, non-turning vehicles spawned turn lanes. scenarios involving
autonomous vehicles, set traffic level average 1.667 vehicles per second per
lane direction. equates 5 total vehicles per second 3 lanes, 10 total
631

fiDresner & Stone

vehicles per second 6 lanes. Scenarios human-driven vehicles one third
traffic fully autonomous scenariosthe intersection cannot nearly efficient
human drivers present. chose amounts traffic toward high end
spectrum manageable traffic respective variants intersection manager.
wanted traffic flowing smoothly, also wanted intersection full
vehicles test situations likely lead destructive possible collisions.
5.3.2 Bad It?
suspected, average crash log oblivious intersection manager quite grisly.
explained Section 5.2.2, driver agents must ignore sensors intersection, many close calls would appear impending collisions. Without
way react situation going awry, vehicles careen intersection, piling
entire intersection filled crashed vehicles protrude incoming lanes.
Figure 22 shows 6-lane casesfully autonomous 5% human driversthe
rate collisions abate 70 vehicles crashed. Even full 60 seconds
incident begins, vehicles still colliding. 3-lane case, intersection
much smaller thus fills much rapidly; 50 seconds, number collided
vehicles levels off.
100

60
6 Lanes
3 Lanes

90

6 Lanes
3 Lanes
50

80

Cars Crashed

Cars Crashed

70
60
50
40

40

30

20

30
20

10

10
0

10

20

30

40

50

60

0

Time (s)

10

20

30

40

50

60

Time (s)

(a) autonomous

(b) 5% humans

Figure 22: Average crash logs (with 95% confidence interval) 3- 6-lane oblivious
intersections. 22(a), intersection manages autonomous vehicles,
22(b) includes 5% human drivers.

scenarios human drivers, shown Figure 22(b), number vehicles involved average incident noticeably smaller. outcome likely result
two factors. First foremost, FCFS-Light policy must make broad allowances
accommodate human drivers, thus overall inherently less dangerous.
characteristic close calls standard FCFS policy less common. Second,
simulated human driver agents drive blindly intersectiontrusting
intersection managerthe way autonomous vehicles do. Also note Figure 22(b)
visible periodicity light model portion policy. paths open
632

fiA Multiagent Approach Autonomous Intersection Management

autonomous vehicles due changes lights, drive unwittingly growing
mass crashed cars.
5.3.3 Reducing Number Collisions
two main components safety mechanism introduced Section 5.2. First,
intersection manager stops accepting reservations. Second, intersection manager
sends messages informing driver agents incident taken place.
possibility second part might always work perfectly; vehicles might
receive message. investigate effects potential communication failures,
intentionally disabled vehicles ability receive Emergency-Stop
message. parameter simulator controls fraction vehicles created
property, varying parameter, could observe subsequent effect
average number vehicles involved incidents.
compared oblivious intersection manager, number vehicles involved
average incident active intersection manager decreases dramatically. Table 1
shows numerical results 3- 6-lane intersections, along 95%
confidence interval. average crash logs runs shown Figure 22,
would indistinguishable one another scale. Instead, present
Figure 23.

Oblivious
Passive
Active
20% receiving
40% receiving
60% receiving
80% receiving
100% receiving

Fully Autonomous
3 Lanes
6 Lanes
27.9 1.3 90.9 4.9
2.63 .13 3.23 .16

5% Human
3 Lanes
6 Lanes
19.3 1.1 49.3 2.7
2.23 .10 2.35 .13

2.44 .13
2.28 .12
1.89 .10
1.71 .08
1.36 .06

2.07 .10
1.91 .10
1.72 .09
1.46 .07
1.22 .05

3.15 .17
2.90 .16
2.69 .15
2.30 .13
1.77 .10

2.29 .13
2.07 .12
1.98 .11
1.65 .09
1.50 .09

Table 1: Average number simulated vehicles involved incidents 3- 6-lane intersections. Even passive safety measures, number crashed
vehicles dramatically decreased oblivious intersection manager.
active configuration, vehicles receive emergency signal, number
crashed vehicles decreases further.
Figure 23 shows effects reactive safety measures intersections 6 lanes,
proportion receiving vehicles varying 0% (passive) 100% increments
20%. Even passive configuration, overall number vehicles involved
average incident decreases factor almost 30 fully autonomous scenario,
factor 20 scenario 5% human drivers, compared oblivious
intersection manager. expected active configuration, vehicles receive
emergency signal, fewer wind crashing. graphs Figure 23 show first
633

fiDresner & Stone

15 seconds incident, case collision occur 15 seconds
incident started.
3.5

2.4
Passive
20% receiving
40% receiving
60% receiving
80% receiving
100% receiving

2
Cars Crashed

Cars Crashed

3

Passive
20% receiving
40% receiving
60% receiving
80% receiving
100% receiving

2.2

2.5

2

1.8
1.6
1.4

1.5
1.2
1

1
0

2

4

6

8

10

12

14

0

Time (s)

2

4

6

8

10

12

14

Time (s)

(a) autonomous

(b) 5% humans

Figure 23: first 15 seconds average crash logs 6-lane passive active intersections. vehicles react signal, safety improves.

5.3.4 Reducing Severity Collisions
reassuring know number vehicles involved average incident
kept fairly low, data give entire picture. example, compare
incident 30 vehicles lose hubcap one two vehicles completely
destroyed occupants killed. currently plans model
intricate physics individual collision high fidelity, simulations allow us
observe velocity collisions occur. previous example, might
notice 30 vehicles bumped one another low velocities, two
vehicles traveling full speed. quantify information, record
collision happens, velocity happens. collision, amount
damage done approximately proportional amount kinetic energy lost.
kinetic energy proportional square velocity, use running total
squares crash velocities create rough estimate amount damage
caused incident. Figure 24 shows average damage log 6-lane intersection
autonomous vehicles. Qualitatively similar results found intersection
types.
Figure 24(a) shows, effect safety measures metric quite
dramatic well. passive case total accumulated squared velocity decreases
factor 25. active case, vehicles receiving signal, decreases
another factor 2. particular note zoomed-in graph Figure 24(b).
passive configuration, total squared velocity accumulates intersection manager
oblivious, first vehicles stop short intersection around 3 seconds;
without reservation, may enter. active scenario, vehicles
receive message, improvement almost immediate.
634

fiA Multiagent Approach Autonomous Intersection Management

1800
Oblivious
Passive
Active

2 2

25000

Accumulated Squared Velocity (m /s )

2 2

Accumulated Squared Velocity (m /s )

30000

20000

15000

10000

5000

0

Oblivious
Passive
Active

1600
1400
1200
1000
800
600
400
200

0

10

20

30

40

50

60

0

1

2

3

Time (s)

4

5

6

7

Time (s)

(a) average incident

(b) zoomed

Figure 24: Average total squared velocity crashed vehicles 6-lane intersection
autonomous vehicles. Sending emergency message vehicles
causes fewer collisions, also makes collisions happen less dangerous.

5.3.5 Delayed Incident Detection
Implicit results assumption intersection managers become aware
incidents instantaneously. could case many collisionsvehicles
communicate collidedif vehicles communications faulty,
vehicle realize collided, intersection may discover problem
seconds, another vehicle sensor detect problem. assess effects
delayed incident detection, artificially delayed intersection managers response
simulations. Figure 25 shows results experiments.
4

5
Number crashed vehicles

Number crashed vehicles

5s delay
3.5
3

3s delay

2.5
1s delay

2

delay

1.5
1

4.5
g

vin

ei

4
%

3.5

,0

ay

3

5s

l
de

c


g

2s delay, 50% receivin

2.5
2
1.5
1

0

2

4

6

8

10

0

Time (s)

2

4

6

8

10

Time (s)

(a) delaying detection

(b) delays faulty communication

Figure 25: Crash logs showing effects delayed incident detection.
Figure 25(a), intersection managers reaction delayed 0, 1, 3, 5 seconds.
Note total number crashed vehicles delay 5 seconds par
number experiment intersection manager reacts immediately, none
635

fiDresner & Stone

vehicles receive message, shown Figure 23(a). Figure 25(b) shows happens
delayed detection faulty communication. graph, along earlier
results, suggests small values, second delay approximately equivalent
20% vehicles receiving Emergency-Stop message, combined,
delayed detection faulty communication additive effect. larger delays,
number vehicles involved approximated using data shown Figure 22(a),
cases, number vehicles crash intersection much
smaller number crash reacts.
5.4 Safety Discussion
results section suggest may possible improve efficiency also
improving safety. course deployment real world, extensive testing
real vehicles would needed order verify suggested efficiency benefits,
well safety properties system. People often hesitant put wellbeing (physical otherwise) hands computer unless convinced
receive significant safety benefit exchange surrendering precious control.
Humans often suffer overconfidence effect, erroneously believing
skillful others. 1981 survey Swedish drivers, respondents asked rate
driving ability relation others. full 80% asked placed
top 30% drivers (Svenson, 1981). effect creates high standard
computerized systems held. insufficient systems marginally
safer, safer average user; must paragon safety.
experiments, showed number vehicles involved individual incidents drastically reduced utilizing fairly straightforward reactive safety mechanism. fact, active configuration 3 lanes, 75% incidents involved
one vehicle: one intentionally crashed (60% 6 lanes). Even passive case
6 lanes traffic, average 3.23 vehicles involved.
compare current systems? conservatively assume accidents traffic today
involve one vehicle, represents 223% increase per occurrence. Thus,
things equal, frequency accidents reduced 70%, experiments
suggest autonomous intersection management system safer overall. 2002
report U.S. Federal Highway Administration blamed 95% accidents
driver error (Wierwille et al., 2002). remaining accidents divided equally
vehicle failures problems roads. important note numbers
driving, intersection driving. Accidents intersections even likely
caused driver error, sometimes even drivers willfully disobeying law: running
red lights stop signs making illegal U-turns.
Even make overly conservative assumptionsthat driving dangerous intersection driving, driver error accountable intersection crashes
types drivingour data suggest automobile traffic autonomous
driver agents intersection control mechanism like reduce collisions intersections 80%. believe reality, improvement much greater.
safety measures presented section constitute one approach mitigating
systems failure modes. sophisticated methods involving explicit cooperation
636

fiA Multiagent Approach Autonomous Intersection Management

amongst vehicles may create even safer system. shown (or attempted
show) particular solution best possible. Rather demonstrated
even simple straightforward response accidents, overall safety
system maintained, without sacrificing benefits vastly improved efficiency.

6. Related Work
Traffic control vast area research computer scientists engineers alike. field
Intelligent Transportation Systems (ITS) concerned applying information, computing, sensor technologies solve problems traffic road management (Bishop,
2005). includes intelligent vehicles (IV) well infrastructure, intersections. Unfortunately, aspects heavily studied, relatively little current
research considers intelligent autonomous vehicles infrastructure work together improve efficiency safety overall traffic system. Berkeley
PATH project produced lot interesting work, including work fully-automated
highway (Alvarez & Horowitz, 1997).
section, describe work related own, directly tangentially. work specifically concerned intersection control, takes
multiagent approach aspects traffic management, represents work
technologies necessary bring fully autonomous vehicles mainstream.
6.1 Requisite Technology
autonomous vehicles take roads, need able interact
aspects roadways, including pedestrians, vehicles, lanes. early
1991, driver agent system named Ulysses developed simulation (Reece
& Shafer, 1991). systems currently development implementation
real vehicles geared toward assisting human drivers, many technologies created
efforts applicable creation completely autonomous driver
agent. successful driver agent needs three main things: detect entities
road, keep vehicle lane, maintain safe distances vehicles.
Fortunately, three subtasks currently attracts extensive amount research.
6.1.1 Object Detection Tracking
fully autonomous vehicle must able reliably detect, classify, track various objects
may roadway. pedestrians bicycles cars trucks, autonomous
vehicles require robust sensors monitor world around manner
lighting conditions weather. Without abilities, amount higher reasoning
driver agent irrelevant. Fortunately, researchers attacking problem
many techniques.
2004, Honda introduced intelligent night vision system Japanese market
capable detecting pedestrians (Liu & Fujimura, 2003). system uses two far-IR
(FIR) cameras front vehicle detect heat-emitting objects beyond range
illuminated vehicles headlights. two cameras allow system obtain distance
information detected pedestrians warn driver. DaimlerChrysler
637

fiDresner & Stone

developing similar system also extrapolates trajectories classified objects
order predict possible outcomes sooner (Gavrila, Giebel, & Munder, 2004). Mahlisch
et al. (2005) developed sensor fusion technique glean information
pedestrians reliably even low-resolution images.
Ford Motor Company investigating track vehicles using color
shape information (She, Bebis, Gu, & Miller, 2004). Gepperth et al. (2005)
demonstrated gray-valued videos (no color), two-stage (initial detection
confirmation) mechanism using simple neural network confirmation reliably
quickly classify vehicles.
Vehicle pedestrian classification tracking well-studied area IV research
progressing quickly. glance IV-related conference symposium reveal
plethora articles aimed using lidar, FIR, normal video, combination
sensors algorithms like Kalman filters, particle filters, neural networks track
classify objects road.
6.1.2 Lane Following
pedestrian vehicle detection tracking, lane following heavily studied area IV research. Varying passive lane- road-departure warning systems
(LDWS/RDWS) active lane keeping assistance (LKA), many systems already showing production vehicles.
far RDWS go, Kohl et al. (2006) used neuroevolution create warning
system warn drivers road departure impending crashes
vehicles. system tested simulation robotic vehicle. work
sponsored Toyota, also currently LDWS market Japan.
system unique uses rear-facing camera predict warn impending lane
departures. LDWS RDWS promise extensive benefits drivers, warn
imminent road lane departures, provide information specific
action taken. Autonomous vehicles need ensure reach point
lane road departure imminent.
Lane keeping, hand, provides executes actions. example,
Hands Across America project 1995 drove vehicle 2,849 miles Pittsburgh Los
Angeles. 98.2% journey, vehicle steered (Pomerleau, 1993). recent
projects concentrated making systems robust varying speed, inclement
weather poor lighting conditions beneath overpasses tunnels. Wu et
al. (2005) proposed tested vision-based lane-keeping system operate
varying speed providing smooth human-like steering. Watanabe Nishida (2005),
working Toyota, developed lane detection algorithm specifically designed
steering assistance systems extremely robust varying road conditions lighting.
several LKA systems market Japan, systems intended
allow autonomous driving. Rather, attempt reduce driver fatigue make
turning stable (Bishop, 2005). Production systems allow autonomous steering
almost invariably based specially painted lines limited special vehicles
closed courses.
638

fiA Multiagent Approach Autonomous Intersection Management

Even without benefit explicitly designated lanes, autonomous vehicles keep
roadway. 2005 DARPA Grand Challenge (DARPA, 2007),
winning vehicle, Stanley, used technique fusing short-range laser range finders
long-range video cameras follow rough dirt path. First, vehicle found smooth areas
front using laser range finders. mapped information onto video
images forward-facing cameras. determining color area image
corresponding smooth areas found laser range finder, Stanley able
extrapolate using flood-fill-type algorithm find areas video image
dirt path (NOVA, 2006). Ramstrom Christensen (2005) achieved similar goal
using strategy based probabilistic generative model.
6.1.3 Adaptive Cruise Control
lane-keeping systems represent main lateral component autonomous vehicles
driver agent, adaptive cruise control (ACC) main longitudinal component. ACC
allows vehicle maintain safe following distance react quicker human
driver case sudden deceleration vehicle front. ACC systems already
available marketDaimlerChryslers Mercedes-Benz S-class, example, comes
system automatically apply brake detects driver slowing
sufficiently fast. Jaguar, Honda, BMW offer similar systems. Nissan Toyota
recently begun offering low-speed following systems, follow vehicles
slower, denser, urban traffic scenarios (Bishop, 2005). ACC relies robust sensing
uses radar, lidar, traditional machine vision algorithms. combining various flavors
ACC low speed, high speed, etc.an agent could control longitudinal motion
vehicle situations. Recently, notion cooperative adaptive cruise control (CACC)
emerged (Laumonier, Desjardins, & Chaib-draa, 2006). concept goes much
toward realizing goal fully autonomous vehicles. allowing vehicles collaborate
take advantage precision autonomous driver agents, vehicles use
existing road space much efficiently.
6.2 Intersection Collision Avoidance
date, much work relating intersections focused Intersection Collision
Avoidance (ICA). work seeks warn driver vehicle may entering
intersection unsafely. aid high-precision digital maps GPS equipment,
vehicle detects classifies state traditional signaling systems placed
intersection (Lindner, Kressel, & Kaelberer, 2004). ICA systems typically take
action behalf driver, simply provide visual auditory warning.
Rasche Naumann (1997, 1998, 1997) worked extensively decentralized solutions intersection collision avoidance problems, including involving autonomous
vehicles. work similar uses potential points collision
restrict access intersection. one vehicle may occupy potential point
collision time. Vehicles attempt obtain token (similar token-ring computer networking) point needed cross intersection. vehicle
necessary tokens, may cross. Rasche Naumanns system also includes priority
model allows emergency vehicles cross quickly prevents deadlocks amongst
639

fiDresner & Stone

normal vehicles. However, system fails satisfy several desiderata.
make guarantees, authors provide results regarding efficiency
system compared traditional system. Furthermore, distributed algorithm
shown resilient unreliable communication. authors also provide
insight system could adapted work mixed human/autonomous
vehicle population. striking difference, however, mechanism
seem notion planning ahead. Tokens potential points collision
either taken takena vehicle seek obtain token point
future, thus allowing proceed toward intersection without slowing
vehicles tokens.
context video games animation, Reynolds (1999) developed autonomous steering algorithms attempt avoid collisions intersections
signaling mechanisms. system would enormous advantage
requiring special infrastructure agent intersectionvehicles equipped
algorithms could operate intersection. Unfortunately, two main drawbacks
system make unsuitable use real-life traffic. First, algorithm
let agent choose path take intersection; vehicle may even find
exiting intersection way came in, due efforts avoid colliding
vehicles. Second, algorithm attempts avoid collisionsit make
guarantees safety.
Cooperative intersection collision avoidance form cooperative vehicle-highway system (CVHS) intersection allowed participate ICA problem. ICA
systems contained entirely individual vehicles cannot account gaps sensor views
sources incomplete information. Thus, CVHS approach required.
many technologies, production systems still assume human driver attempt
warn violation occur, cases, punish
fact, cameras detect vehicle run red light automatically
issues driver citation. U.S. Department Transportation sponsoring several ICA projects including infrastructure-only cooperative approaches (USDOT,
2003). intention first deploy infrastructure-only systems,
market penetration ICA-equipped vehicles increases, roll cooperative systems.
Significant work ICA also underway Japan (Bishop, 2005).
systems large step toward enabling autonomous vehicles take
roads, none designed work specifically autonomous vehicles. exception
algorithm designed games, assumes human driver traditional
signaling systemsa clumsy, inefficient interface find obsolete due
autonomous vehicle technology.
6.3 Optimizing Traffic Signal Timing
vast majority deployed technology intersection control involves calibrating
timing traditional traffic lights order create wave green vehicles
reach one green light, continue subsequent intersections without
stop. Unfortunately, practice, waves tend sporadic short-lived due
640

fiA Multiagent Approach Autonomous Intersection Management

rapidly changing traffic patterns. However, offer substantial benefits compared
systems without coordination.
TRANSYT, Traffic Network Study Tool, off-line system that, given average
traffic flows, determine optimum fixed-time coordinated traffic signal timings (Robertson, 1969). TRANSYT requires extensive data gathering analysis, used
heavily world. Unfortunately, system brittle
ability react unusual changes traffic flow. example, end
major sporting event, thousands vehicles may attempting cross intersection
direction normal circumstances rarely used. light timings
set reflect normal circumstances, length time departing
vehicles get green light may significantly less cross traffic, may
little.
SCOOT, Split, Cycle, Offset Optimisation Technique, represents advancement TRANSYT (Hunt, Robertson, Bretherton, & Winton, 1981). SCOOT
on-line adaptive traffic control system react changes traffic levels, give priority vehicles buses, even estimate vehicle emissions. SCOOT
shown reduce traffic delays average 20% systems like TRANSYT, still
relies traditional signaling systems vehicles. Furthermore, SCOOT requires reliable
traffic data order adapt, thus may slow react changes traffic flow.
6.4 MAS Traffic
Automobile traffic great example multiagent system, surprising
lot research modelling studying traffic using multiagent techniques.
Many approaches consider systems consisting traffic-light-controlling agents
driver agents, opposed heterogeneous multiagent system many kinds
agents. Nevertheless, many ideas involved could potentially adapted work
within framework reservation system.
6.4.1 Cooperative Traffic Signals
Much MAS traffic research focuses improving current technology (systems traffic
lights). example, Roozemond (1999) allows intersections act autonomously
sharing data gather. intersections use information make
short- long-term predictions traffic adjust accordingly. strategy
attempts overcome one weaknesses SCOOT: need large amounts
reliable traffic data. multiple intersections share data, intersection get
accurate picture current traffic situation.
Bazzan (2005) used decentralized approach combining MAS evolutionary
game theory. approach models intersection individually-motivated agent
must focus local goals (getting vehicles intersection),
also global goals (reducing travel times vehicles). Bazzan Roozemonds
techniques still assume traditional signaling mechanisms human drivers.
641

fiDresner & Stone

6.4.2 Platoons
addition multi-intersection systems, multi-vehicle systems focus lot
research. Much research centers creating platoons vehicles order minimize
effects stop-and-go driving. Consider line cars stopped red light.
light turns green, first car begins move. Eventually, car behind notices
enough space accelerate well. time later, vehicle back
line begin move, may late actually get intersection
current green phase light. If, hand, vehicles
simultaneously uniformly accelerate, vehicles could make green
phase, vehicles would efficiently use space-time available
cross intersection.
Clement (2002) proposed model called Simple Platoon Advancement (SPA),
addresses exact problem. SPA boasts ability get nearly twice many
vehicles green light (increasing lights throughput) compared normal
human drivers, addition safety delay benefits associated automated control. One vehicles intersection dispersed safe following distances,
control returned human driver.
Halle Chaib-draa (2005) used platoon approach facilitate collaborative
driving general. allow vehicles, controlled separate agents, form
platoons, varying degrees autonomy. Vehicles merge split platoons
using carefully crafted maneuvers, vehicle platoon specific
responsibility. present centralized version, master vehicle gives orders
rest platoon, decentralized version, social laws dictate
agents role, platoons leader acts representative platoons.
platooning systems assume automated control vehicles, use ordinary traffic
lights intersection control. using platoons, methods attempt solve problem
inherent traffic lights themselvesthey designed humans use,
well suited automated vehicle control. work presented article attempts
free autonomous vehicles control traffic lights instead design new system
specifically utilizes capabilities fully autonomous vehicles.
6.4.3 History-Based Traffic Control
Taking different approach intersection control, Balan Luke (2006) use historybased method maximize fairness (all vehicles experience similar delays) opposed
efficiency (the average vehicle experiences short delays). paradigm, vehicles
historically (previously journey) experienced long delays
likely experience shorter delays subsequent intersections. addition
multi-intersection approach, method uses marketplace model involving system
credits given taken exchange shorter longer delays, respectively.
Coordination individual intersections still done traditional traffic lights, timings part mechanism. Interestingly, fairness approach actually yields
results also reasonably efficient.
642

fiA Multiagent Approach Autonomous Intersection Management

6.5 Machine Learning Traffic
Abdulhai et al. (2003) used Q-learning, simple, yet powerful form reinforcement
learning, on-line adaptive signal control. work, authors explore
isolated intersection well linear chain intersections. demonstrate Qlearning significantly reduce delays vehicles quickly adapt changing traffic
patterns. Bull et al. (2004) shown Learning Classifier Systems (LCS) also make
traditional traffic signals efficient. Wiering (2000) demonstrated multiagent,
model-based reinforcement learning also used optimize signal timings
complex networks intersections.
focusing intersections, Moriarty Langley (1998) shown
reinforcement learningspecifically neuro-evolutioncan train efficient driver agents
lane, speed, route selection freeway driving, critical components
fully autonomous vehicle. Additionally, many object tracking detection
examples mentioned previously use neural networks classify objects.
6.6 Physical Robots
real autonomous vehicles, Kolodko Vlacic (2003) created small-scale system
intersection control similar granularity-1 FCFS policy. authors
developed mechanism small Cooperative Autonomous Mobile Robots (CAMRs),
30 cm diameter top speed 10 cm/s. CAMRs
programmed follow Australian traffic laws, communicate several different types
messages. demonstrated CAMRs, mechanism scaled use Imara
vehicles, much larger (capable carrying two human passengers) faster (top
speed 30 km/h). system completely distributed require extensive
infrastructure intersection. However, assume vehicles cooperate
one another.
6.7 Safety Analysis
Section 5 includes failure-mode analysis proposed intersection control mechanism. best knowledge, first study impact
autonomous intersection protocol driver safety. However, enormous
body work regarding safety properties traditional intersections. includes
generalcorrelating traffic level accident frequency (Sayed & Zein, 1999) analyses
particular types intersections (Bonneson & McCoy, 1993; Harwood, Bauer, Potts, Torbic, Richard, Rabbani, Hauer, Elefteriadou, & Griffith, 2003; Persaud, Retting, Gardner,
& Lord, 2001)as well plenty esoteric work, characterizing role
Alzheimers Disease intersection collisions (Rizzo, McGehee, Dawson, & Anderson,
2001). However, concerns human-operated vehicles, none work
particularly applicable setting concerned with.

7. Conclusion Future Work
reservation system presented large step toward easing traffic woes, terms
wasted time injury loss life. However, substantial work must still done
643

fiDresner & Stone

system ready deploy. work represents possible future directions
line research. example, detailed studies safety properties
systemhow reacts various failures whether effects failures
mitigatedare required. Another area ripe improvement intersection manager.
manager switch among several different policies, learning reservation histories policy best suited particular traffic conditions, could significantly improve
performance. Furthermore, light model could react traffic conditions,
also presence individual vehicles, might better able exploit abilities
autonomous vehicles, without adversely affecting human drivers. Framing intersection
marketplace space-time commodity could allow system handle vehicle priorities intelligently allow driver agents exchange long wait one day
quick passage later, important day. Finally, driver agent may
able benefit machine learning techniques, perhaps learning make
accurate reservations thus needing cancel less frequently.

article makes three main contributions. First, defines problem autonomous
intersection management, including set desiderata potential solutions
evaluated. Second, presents framework meet desiderata,
algorithm (FCFS) shows advantages framework current intersection
control methods. Third, demonstrates framework extended allow
human-driven (not autonomous) vehicles use system, still exploiting abilities
autonomous vehicles increase throughput subsequently decrease delays.

Getting today future humans longer burdened
mundane yet dangerous task piloting automobiles involve vast amount
work many different disciplines. extensively address engineering
societal challenges involved building deploying system, article suggests
algorithmically feasible worthwhile (in terms decreasing delay)
so.

Acknowledgments

research supported NSF CAREER award IIS-0237699, experiments
carried machines provided NSF grant EIA-0303609.
644

fiA Multiagent Approach Autonomous Intersection Management

Appendix A. Simplified Laser Range Finder
appendix describes implementation detail driver agents sensor model. Recall
Section 3.1.1 driver access set simulated external sensors.
set simplified laser range finder, intended give agent type
information actual laser range finder, without expensive computation required
fully simulate sensor. Instead, simplified laser range finder sensor examines
vehicle within sensor range determines closest front sensing
vehicle. Then, records point vehicle closest sensing vehicle
provides distance angle point.
Modern laser range finders distance sensors provide large amount distance
angle data mobile agent. real life setting, information would definitely
prove useful fine-tuning driver agent. However, simple simulation, must process
sensor information vehicles simultaneously, accurately simulating full laserrange finder feasible. Thus, use simple, yet pertinent sensor reading
driver agent use control actions respect vehicles. purely
straight-ahead sensor suffices vehicles traveling straight lines. However,
vehicle turns, must also take account going direction
turning. complicate matters, vehicle turning must still take
account going directly front point might straighten
wheels continue current heading. sensor points direction
wheels sufficient vehicles coming turns may run
vehicles ahead them. Instead, sensors scope widens direction turn,
narrowing slightly side. Figure 26 shows scenario demonstrates
concept. testament sensors usefulness, vehicles equipped sensor
(i.e. intersection manager present) able avoid many collisions intersection,
even moderate amounts traffic.

Appendix B. Communication Protocol
Section 3.2 gave brief introduction communication protocol used agents
reservation system. appendix, specify protocol much greater detail.
protocol consists several message types kind agent, well rules
governing messages sent sorts guarantees accompany them.
section present aspects essential understanding remainder
article.
B.1 Message Types
vehicles intersection manager restricted types messages
must coordinate.
B.1.1 Vehicle Intersection
four types messages sent vehicles intersection.
645

fiDresner & Stone

Figure 26: depiction sensor model driver agents. sensor focused
gray lines provide information outside them.
black line represents reading provided driver agent.

1. Request message vehicle sends reservation
wishes make one. contains properties vehicle (ID number, performance, size, etc.) well properties proposed reservation (arrival time,
arrival velocity, type turn, arrival lane, etc.). message also communicates
vehicles status emergency vehicle (in emergency situation). practice,
would implemented using secure method normal vehicles could
impersonate emergency vehicles. methods well understood details
implementation beyond scope research.
message 15 fields:
vehicle id unique identifier vehicle.
arrival time absolute time vehicle agrees arrive intersection.
arrival lane unique identifier lane vehicle
arrives intersection.
turn way vehicle turn reaches intersection.
arrival velocity velocity vehicle agrees traveling
arrives intersection.
maximum velocity maximum velocity vehicle travel.
maximum acceleration maximum rate vehicle accelerate.
646

fiA Multiagent Approach Autonomous Intersection Management

minimum acceleration minimum rate vehicle accelerate (i.e.
negative number representing maximum deceleration).
vehicle length length vehicle.
vehicle width width vehicle.
front wheel displacement distance front vehicle
front axle.
rear wheel displacement distance front vehicle
rear axle.
max steering angle maximum angle front wheels turned
purposes steering.
max turn per second rate vehicle turn wheels.
emergency whether emergency vehicle emergency situation.
2. Change-Request message vehicle sends reservation,
would like switch different set parameters. new parameters
acceptable intersection, vehicle may keep old reservation.
identical request message, except includes unique reservation ID
reservation vehicle currently has.
message identical Request message, except one added field:
reservation id identifier reservation changed.
3. Cancel message vehicle sends longer desires current
reservation.
2 fields:
vehicle id unique identifier vehicle.
reservation id identifier reservation cancelled.
4. Done message sent vehicle completed traversal
intersection. communicates information Cancel message,
may behavior tied Cancel message occur vehicle successfully completes trip across intersection. Additionally, message
could extended order communicate statistics vehicle, could
recorded order analyze performance intersection manager.
message used collect statistics vehicle, recorded
order analyze improve performance intersection manager.
2 fields:
vehicle id unique identifier vehicle.
reservation id identifier reservation completed.
647

fiDresner & Stone

B.1.2 Intersection Vehicle
four types messages sent intersection individual
vehicles.
1. Confirm message response vehicles Request (or Change-Request)
message. always mean parameters transmitted vehicle
acceptable. could, example, contain counter-offer intersection.
reservation parameters message implicitly accepted vehicle,
must explicitly cancelled driver agent vehicle approve. Note
safe even faulty communicationthe worst happen
intersection reserves space get used. Included message
acceleration constraints determined intersection. list rates
durations. list created depends intersection manager. However,
vehicles safety must guaranteed adheres list.
message 7 fields:
reservation id unique identifier reservation created.
arrival time absolute time vehicle expected arrive.
early error tolerable error (early) arrival time vehicle.
late error tolerable error (late) arrival time vehicle. Note
intersection manager must assume car could arrive traverse
intersection time within resulting bounds
arrival lane unique identifier lane vehicle
arrives intersection.
arrival velocity velocity vehicle expected traveling
arrives intersection. negative number signifies velocity
acceptable.
accelerations run-length encoded description expected acceleration
vehicle travels intersection. Here, run-length encoded description sequence order pairs acceleration durationstarting
instant vehicle enters intersection, maintain
acceleration duration paired. sequence empty,
accelerations acceptable.
2. Reject sending message, intersection inform vehicle
parameters sent latest Request (or Change-Request) acceptable,
intersection either could want make counter-offer.
message also indicates whether rejection reservation
manager requires vehicle stop intersection entering. lets
driver agent know attempt reservations reaches
intersection.
message 1 field:
648

fiA Multiagent Approach Autonomous Intersection Management

stop required boolean value indicating whether vehicle must first come
full stop entering intersection.
3. Acknowledge message acknowledges receipt Cancel Done
message.
1 field:
reservation id unique identifier reservation cancelled completed.
4. Emergency-Stop message sent intersection manager
determined collision similar problem occurred intersection.
message informs receiving driver agent reservation requests
granted, possible, vehicle attempt stop instead entering
intersection, even reservation. specifics message used
discussed Section 5.2.2. message fields, communicates
single bit information.
B.1.3 Vehicle Vehicle
currently protocol communication vehicles.
B.2 Protocol Actions
addition message types, agents involved (the vehicles intersection) must
obey set rules. entirely unlike rules human drivers follow
driving.
B.2.1 Vehicle Actions
rules vehicles expected follow order allow intersection
function efficiently.
1. vehicle may enter intersection without reservation.
2. vehicle going cross intersection, must everything reasonable within
power cross accordance parameters included recent
Confirm message received intersection.
3. vehicle sends another message intersection manager sent response,
intersection manager may choose ignore it. Thus, vehicle send
message received response previous message.
4. vehicle yet entered intersection reservation,
may send Request message. yet entered intersection
reservation, may send either Change-Request Cancel message.
sends messages allowed to, intersection may choose
ignore them.
5. vehicle reservation successfully crossed intersection, may send
Done message.
649

fiDresner & Stone

6. vehicle receives Confirm message, considered reservation.
B.2.2 Intersection Actions
rules representing obligations intersection manager expected
fulfill.
1. intersection receives Request message, must respond either
Confirm Reject message. responds Confirm message, guaranteeing cross-traffic interfere vehicle crosses intersection
accordance parameters message.
2. intersection receives Change-Request message, must respond
either Confirm Reject message. responds Confirm message,
guaranteeing cross-traffic interfere vehicle crosses intersection accordance parameters message. previous guarantees
nullified.
3. intersection receives Cancel message, must respond Acknowledge message. guarantee made sending vehicle nullified.

Appendix C. Driver Agent
stated Section 3.3, main focus work improving framework
algorithms intersection control. However, order require sort
driver agent. Furthermore, efficiency reservation framework depends driver
agents reasonably intelligent, non-trivial. appendix describes driver
agent implementation used experiments.
Containing behaviors control turning vehicles well optimizations increase
peformance system overall, driver agent represents single intricate component reservation mechanism. Algorithm 3 gives high-level pseudocode description
driver agent.
C.1 Lane Following
Given model lanes, driver must able drive vehicles lanes.
accomplish means lane following behavior acts modifying
steering angle vehicle. behavior entirely independent rest agents
behavior, controls vehicles acceleration communicates intersection
manager. behavior active timesthe vehicle always attempting stay
current lane. lane-following behavior designed robust sudden lane
reassignment, turning lane changing implemented: driver
agent simply changes lane current lane, lane-following behavior steers
vehicle correct lane. process entirely smooth, provided vehicle
traveling reasonable velocitya condition enforced parts driver agents
behavior.
lanes modeled directed line segments, lane-following behavior attempts keep vehicle evenly straddling lane. line segment represents
650

fiA Multiagent Approach Autonomous Intersection Management

middle lane, thus condition equivalent keeping vehicle centered
lane. driver agent accomplishes turning front wheels toward point
segment. point, call aim point farther along segment
vehicle. aim point computed first projecting point front center
vehicle onto line segment, displacing point direction line
segment amount call lead distance. part, lead distance
proportional velocity vehicle. proportion smaller inside intersection
vehicles pull strongly new lane turningthey must
entirely correct lane leave intersection collide
vehicles outside intersection. proportional lead distance necessary
otherwise high velocities, required steering angle may change faster
driver agent steer, resulting either wildly erratic steering vehicle driving
circles. lead distance also minimum value 1 meter. lead distance gets
small, effect velocity largeby ensuring aim point
least meter farther lane, ensure vehicle end
stable configuration traveling proper direction. Figure 27 depicts driver
agent determines lead distance (and subsequent aim point) different velocities.



b

c

Figure 27: vehicle attempting follow lane. so, first calculates point
represents projection position onto directed line segment
running center lane (a). Then, depending velocity,
displaces resulting point direction travel small large amount
obtain point aim front wheels. low velocities,
point displaced muchonly enough ensure vehicle moves
correct direction (b). higher velocities, aim point must farther
along lane, vehicles steering gradual thus
stable (c).

method lane following one possible method, selected
sufficient purposes. Furthermore, reservation systems functionality
depend driver agent using particular method, work method,
provided driver agent turns within mutually understood constraints.
651

fiDresner & Stone

C.2 Optimistic Pessimistic Driver Agents
nave driver agent perform poorly when, example, makes reservation
stuck behind slower-moving vehicle. vehicle front eventually accelerates, would
ideally accelerate well (possibly switching earlier reservation).
account situations like this, introduce notion optimistic pessimistic
driver agent. optimistic agent makes reservation assuming arrive intersection minimum possible time. agent finds longer stuck behind
slower vehicle become optimistic attempt make new, earlier reservation.
pessimistic agent assumes stuck current velocity reaches intersection. agent cancel reservation way arrive
time, becomes pessimistic. Due relatively infrequent smooth transitions
moods, driver agent take advantage improving circumstances
without causing send excessive numbers messages things change.
shown Figure 2, addition optimism pessimism driver agent
reduced average number reservations made well average number
messages transmitted. expected, effect less pronounced lower amounts
traffic.


Without

Messages
560.85
5.97

Reservations
165.89
1.02

Table 2: moderate amount traffic, average number messages sent reservations made driver agents without optimism/pessimism heuristic.

C.3 Estimating Time Intersection
driver agents estimate long take get intersection must
precise vehicles arrive time reservations. If, point,
vehicle certain whether arrive schedule, cannot safely continue.
pessimistic driver agent simply divides distance intersection current
velocityit assumes able accelerate. optimistic driver first determines
velocity arrives. turning, example, velocity may
lower speed limit. Otherwise, may limited amount vehicle
accelerate reaching intersection. computes minimum possible time
reach intersection velocity, is, assumes accelerate much
possible decelerating arrival velocity.
652

fiA Multiagent Approach Autonomous Intersection Management

Algorithm 3 driver agent behavior. driver agents initialized optimistic.
1: determine aim point attempt point wheels
2: current time
3: Velocity speed limit
4:
Accelerate
5: intersection
6:
Optimistic
7:
ti optimisitic estimate time intersection
8:
else
9:
ti pessimistic estimate time intersection
10:
reservation
11:
+ ti scheduled arrival
12:
Cancel reservation
13:
become pessimistic
14:
else + ti significantly scheduled arrival
15:
Become optimistic
16:
Attempt change reservation earlier time
17:
else
18:
Try make reservation according ti
19:
Reservation request rejected
20:
Decelerate
21: else intersection
22:
Set acceleration according parameters reservation
23: intersection less 1 second behind car front
24:
Decelerate

References
Abdulhai, B., Pringle, R., & Karakoulas, G. J. (2003). Reinforcement learning true
adaptive traffic signal control. Journal Transportation Engineering, 129 (3), 278
285.
Alvarez, L., & Horowitz, R. (1997). Traffic flow control automated highway systems.
Tech. rep. UCB-ITS-PRR-97-47, University California, Berkeley, Berkeley, California, USA.
Balan, G., & Luke, S. (2006). History-based traffic control. Proceedings Fifth
International Joint Conference Autonomous Agents Multiagent Systems, pp.
616621, Hakodate, Japan.
Bazzan, A. L. C. (2005). distributed approach coordination traffic signal agents.
Autonomous Agents Multi-Agent Systems, 10(2), 131164.
Bishop, R. (2005). Intelligent Vehicle Technology Trends. Artech House.
Bonneson, J. A., & McCoy, P. T. (1993). Estimation safety two-way stopcontrolled
intersections rural highways. Transportation Research Record, 1401, 8389.
653

fiDresner & Stone

Bull, L., ShaAban, J., Tomlinson, A., Addison, J. D., & Heydecker, B. G. (2004). Towards
distributed adaptive control road traffic junction signals using learning classifier
systems. Bull, L. (Ed.), Applications Learning Classifier Systems, pp. 276299.
Springer.
Clement, S. (2002). SPA model smooth acceleration. 24th Conference
Australian Institutes Transport Research (CAITR-2002), Sydney, Australia.
DARPA (2007). DARPA urban challenge.. http://www.darpa.mil/grandchallenge.
Dresner, K., & Stone, P. (2004). Multiagent traffic management: reservation-based intersection control mechanism. Third International Joint Conference Autonomous Agents Multiagent Systems, pp. 530537, New York, NY, USA.
Dresner, K., & Stone, P. (2006). Multiagent traffic management: Opportunities multiagent learning. K. Tuyls et al. (Ed.), LAMAS 2005, Vol. 3898 Lecture Notes
Artificial Intelligence, pp. 129138. Springer Verlag, Berlin.
Gavrila, D. M., Giebel, J., & Munder, S. (2004). Vision-based pedestrian detection:
PROTECTOR+ system. Proceedings IEEE Intelligent Vehicles Symposium
(IV2004), Parma, Italy.
Gepperth, A., Edelbrunner, J., & Bucher, T. (2005). Real-time detection classification
cars video sequences. Proceedings IEEE Intelligent Vehicle Symposium
(IV2005), pp. 625631, Las Vegas, NV, USA.
Halle, S., & Chaib-draa, B. (2005). collaborative driving system based multiagent
modelling simulations. Journal Transportation Research Part C (TRC-C):
Emergent Technologies, 13, 320345.
Harwood, D. W., Bauer, K. M., Potts, I. B., Torbic, D. J., Richard, K. R., Rabbani, E.
R. K., Hauer, E., Elefteriadou, L., & Griffith, M. S. (2003). Safety effectiveness
intersection left- right-turn lanes. Transportation Research Record, 1840, 131139.
Hatipo, C., Redmill, K., & Ozguner, U. (1997). Steering lane change: working system.
IEEE Conference Intelligent Transportation Systems, pp. 272277.
Hunt, P. B., Robertson, D. I., Bretherton, R. D., & Winton, R. I. (1981). SCOOT - traffic
responsive method co-ordinating signals. Tech. rep. TRRL-LR-1014, Transport
Road Research Laboratory.
Johnson, R. C. (2005). Steady pace takes DARPA race. EE Times. Accessed http:
//www.eetimes.com.
Kohl, N., Stanley, K., Miikkulainen, R., Samples, M., & Sherony, R. (2006). Evolving
real-world vehicle warning system. Proceedings Genetic Evolutionary
Computation Conference 2006, Seattle, WA, USA.
Kolodko, J., & Vlacic, L. (2003). Cooperative autonomous driving intelligent control
systems laboratory. IEEE Intelligent Systems, 18 (4), 811.
Laumonier, J., Desjardins, C., & Chaib-draa, B. (2006). Cooperative adaptive cruise control:
reinforcement learning approach. Fourth Workshop Agents Traffic
Transportation, Hakodate, Hokkaido, Japan.
654

fiA Multiagent Approach Autonomous Intersection Management

Lindner, F., Kressel, U., & Kaelberer, S. (2004). Robust recognition traffic signals.
Proceedings IEEE Intelligent Vehicles Symposium (IV2004), Parma, Italy.
Liu, X., & Fujimura, K. (2003). Pedestrian detection using stereo night vision. IEEE
International Conference Intelligent Transportation Systems, Shanghai, China.
Mahlisch, M., Oberlander, M., Lohlein, O., Gavrila, D., & Ritter, W. (2005). multiple
detector approach low-resolution FIR pedestrian recognition. Proceedings
IEEE Intelligent Vehicles Symposium (IV2005), Las Vegas, NV, USA.
Moriarty, D., & Langley, P. (1998). Learning cooperative lane selection strategies highways. Proceedings Fifteenth National Conference Artificial Intelligence,
pp. 684691, Madison, WI. AAAI Press.
National Highway Traffic Safety Administration (2002). Economic impact U.S. motor
vehicle crashes reaches $230.6 billion, new NHTSA study shows. NHTSA Press Release
38-02. http://www.nhtsa.dot.gov.
Naumann, R., & Rasche, R. (1997). Intersection collision avoidance means decentralized security communication management autonomous vehicles. Proceedings
30th ISATA - ATT/IST Conference.
Naumann, R., Rasche, R., & Tacken, J. (1998). Managing autonomous vehicles intersections. IEEE Intelligent Systems, 13 (3), 8286.
Noda, I., Jacoff, A., Bredenfeld, A., & Takahashi, Y. (Eds.). (2006). RoboCup-2005: Robot
Soccer World Cup IX. Springer Verlag, Berlin.
NOVA (2006). great robot race.. Originally aired 28 March 2006 PBS, available
online http://www.pbs.org/wgbh/nova/darpa.
Persaud, B. N., Retting, R. A., Gardner, P. E., & Lord, D. (2001). Safety effect roundabout conversions united states: Empirical bayes observational before-after
study. Transportation Research Record, 1751, 18.
Pomerleau, D. A. (1993). Neural Network Perception Mobile Robot Guidance. Kluwer
Academic Publishers.
Ramstrom, O., & Christensen, H. (2005). method following umarked roads.
Proceedings IEEE Intelligent Vehicle Symposium (IV2005), pp. 650655, Las
Vegas, NV, USA.
Rasche, R., Naumann, R., Tacken, J., & Tahedl, C. (1997). Validation simulation
decentralized intersection collision avoidance algorithm. Proceedings IEEE
Conference Intelligent Transportation Systems (ITSC 97).
Reece, D. A., & Shafer, S. (1991). computational model driving autonomous
vehicles. Tech. rep. CMU-CS-91-122, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA.
Reynolds, C. W. (1999). Steering behaviors autonomous characters. Proceedings
Game Developers Conference, pp. 763782.
Rizzo, M., McGehee, D. V., Dawson, J. D., & Anderson, S. N. (2001). Simulated car crashes
intersections drivers Alzheimer disease. Alzheimer Disease Associated
Disorders, 15 (1), 1020.
655

fiDresner & Stone

Robertson, D. I. (1969). TRANSYT traffic network study tool. Tech. rep. TRRL-LR253, Transport Road Research Laboratory.
Rogers, S., Flechter, C.-N., & Langley, P. (1999). adaptive interactive agent route
advice. Etzioni, O., Muller, J. P., & Bradshaw, J. M. (Eds.), Proceedings Third
International Conference Autonomous Agents (Agents99), pp. 198205, Seattle,
WA, USA. ACM Press.
Roozemond, D. A. (1999). Using intelligent agents urban traffic control systems. Proceedings International Conference Artificial Intelligence Transportation
Systems Science, pp. 6979.
Sayed, T., & Zein, S. (1999). Traffic conflict standards intersections. Transportation
Planning Technology, 22 (4), 309323.
Schonberg, T., Ojala, M., Suomela, J., Torpo, A., & Halme, A. (1995). Positioning
autonomous off-road vehicle using fused DGPS inertial navigation. 2nd
IFAC Conference Intelligent Autonomous Vehicles, pp. 226231.
She, K., Bebis, G., Gu, H., & Miller, R. (2004). Vehicle tracking using on-line fusion
color shape features. Proceedings IEEE International Conference
Intelligent Transportation Systems, Washington, DC, USA.
Stone, P., & Veloso, M. (2000). Multiagent systems: survey machine learning
perspective. Autonomous Robots, 8 (3), 345383.
Svenson, O. (1981). less risky skillful fellow drivers?. Acta
Psychologica, 47 (2), 143148.
Texas Transportation Institute (2004). 2004 urban mobility report.. Accessed http:
//mobility.tamu.edu/ums December 2004.
USDOT (2003). Inside USDOTs intelligent intersection test facility. Newsletter
Cooperative Deployment Network. Accessed online 17 May 2006 http:
//www.ntoctalks.com/icdn/intell_intersection.php.
Watanabe, A., & Nishida, M. (2005). Lane detection steering assistance system.
Proceedings IEEE Intelligent Vehicle Symposium (IV2005), pp. 159164, Las
Vegas, NV, USA.
Wiering, M. A. (2000). Multi-agent reinforcement learning traffic light control.
Langley, P. (Ed.), Proceedings Seventeenth International Conference Machine
Learning (ICML2000), pp. 11511158.
Wierwille, W. W., Hanowski, R. J., Hankey, J. M., Kieliszewski, C. A., Lee, S. E., Medina,
A., Keisler, A. S., & Dingus, T. A. (2002). Identification evaluation driver
errors: Overview recommendations. Tech. rep. FHWA-RD-02-003, Virginia Tech
Transportation Institute, Blacksburg, Virginia, USA. Sponsored Federal Highway Administration.
Wu, S.-J., Chiang, H.-H., Perng, J.-W., Lee, T.-T., & Chen, C.-J. (2005). automated
lane-keeping design intelligent vehicle. Proceedings IEEE Intelligent
Vehicle Symposium (IV2005), pp. 508513, Las Vegas, NV, USA.

656

fiJournal Artificial Intelligence Research 31 (2008) 157-204

Submitted 06/07; published 01/08

Conjunctive Query Answering Description Logic
SHIQ
Birte Glimm
Ian Horrocks

birte.glimm@comlab.ox.ac.uk
ian.horrocks@comlab.ox.ac.uk

Oxford University Computing Laboratory, UK

Carsten Lutz

clu@tcs.inf.tu-dresden.de

Dresden University Technology, Germany

Ulrike Sattler

sattler@cs.man.ac.uk

University Manchester, UK

Abstract
Conjunctive queries play important role expressive query language Description Logics (DLs). Although modern DLs usually provide transitive roles, conjunctive
query answering DL knowledge bases poorly understood transitive roles
admitted query. paper, consider unions conjunctive queries knowledge bases formulated prominent DL SHIQ allow transitive roles
query knowledge base. show decidability query answering setting
establish two tight complexity bounds: regarding combined complexity, prove
deterministic algorithm query answering needs time single exponential
size KB double exponential size query, optimal.
Regarding data complexity, prove containment co-NP.

1. Introduction
Description Logics (DLs) family logic based knowledge representation formalisms
(Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003). DLs fragments
First-Order Logic restricted unary binary predicates, called concepts
roles DLs. constructors building complex expressions usually chosen
key inference problems, concept satisfiability, decidable preferably low
computational complexity. DL knowledge base (KB) consists TBox, contains
intensional knowledge concept definitions general background knowledge,
ABox, contains extensional knowledge used describe individuals. Using
database metaphor, TBox corresponds schema, ABox corresponds
data. contrast databases, however, DL knowledge bases adopt open world
semantics, i.e., represent information domain incomplete way.
Standard DL reasoning services include testing concepts satisfiability retrieving
certain instances given concept. latter retrieves, knowledge base consisting
ABox TBox , (ABox) individuals instances given (possibly
complex) concept expression C, i.e., individuals entail
instance C. underlying reasoning problems well-understood, known
combined complexity reasoning problems, i.e., complexity measured
size TBox, ABox, query, ExpTime-complete SHIQ (Tobies,
c
2008
AI Access Foundation. rights reserved.

fiGlimm, Horrocks, Lutz, & Sattler

2001). data complexity reasoning problem measured size ABox
only. Whenever TBox query small compared ABox, often
case practice, data complexity gives useful performance estimate. SHIQ,
instance retrieval known data complete co-NP (Hustadt, Motik, & Sattler,
2005).
Despite high worst case complexity standard reasoning problems
expressive DLs SHIQ, highly optimized implementations available, e.g.,
FaCT++ (Tsarkov & Horrocks, 2006), KAON21 , Pellet (Sirin, Parsia, Cuenca Grau, Kalyanpur, & Katz, 2006), RacerPro2 . systems used wide range applications,
e.g., configuration (McGuinness & Wright, 1998), bio informatics (Wolstencroft, Brass,
Horrocks, Lord, Sattler, Turi, & Stevens, 2005), information integration (Calvanese,
De Giacomo, Lenzerini, Nardi, & Rosati, 1998b). prominently, DLs known
use logical underpinning ontology languages, e.g., OIL, DAML+OIL,
OWL (Horrocks, Patel-Schneider, & van Harmelen, 2003), W3C recommendation (Bechhofer, van Harmelen, Hendler, Horrocks, McGuinness, Patel-Schneider, & Stein,
2004).
data-intensive applications, querying KBs plays central role. Instance retrieval
is, aspects, rather weak form querying: although possibly complex concept
expressions used queries, query tree-like relational structures, i.e.,
DL concept cannot express arbitrary cyclic structures. property known
tree model property considered important reason decidability
Modal Description Logics (Gradel, 2001; Vardi, 1997). Conjunctive queries (CQs)
well known database community constitute expressive query language
capabilities go well beyond standard instance retrieval. example, consider
knowledge base contains ABox assertion (hasSon.(hasDaughter.>))(Mary),
informally states individual (or constant FOL terms) Mary son
daughter; hence, Mary grandmother. Additionally, assume
roles hasSon hasDaughter transitive super-role hasDescendant. implies Mary related via role hasDescendant (anonymous) grandchild.
knowledge base, Mary clearly answer conjunctive query hasSon(x, y)
hasDaughter(y, z) hasDescendant(x, z), assume x distinguished variable
(also called answer free variable) y, z non-distinguished (existentially quantified)
variables.
variables query non-distinguished, query answer true false
query called Boolean query. Given knowledge base K Boolean CQ q,
query entailment problem deciding whether q true false w.r.t. K. CQ contains
distinguished variables, answers query tuples individual names
knowledge base entails query obtained replacing free variables
individual names answer tuple. problem finding answer tuples
known query answering. Since query entailment decision problem thus better
suited complexity analysis query answering, concentrate query entailment.
restriction since query answering easily reduced query entailment
illustrate detail Section 2.2.
1. http://kaon2.semanticweb.org
2. http://www.racer-systems.com

158

fiConjunctive Query Answering DL SHIQ

Devising decision procedure conjunctive query entailment expressive DLs
SHIQ challenging problem, particular transitive roles admitted query
(Glimm, Horrocks, & Sattler, 2006). conference version paper, presented
first decision procedure conjunctive query entailment SHIQ. paper,
generalize result unions conjunctive queries (UCQs) SHIQ knowledge bases.
achieve rewriting conjunctive query set conjunctive queries
resulting query either tree-shaped (i.e., expressed concept) grounded
(i.e., contains constants/individual names variables). entailment
types queries reduced standard reasoning problems (Horrocks & Tessaris, 2000;
Calvanese, De Giacomo, & Lenzerini, 1998a).
paper organized follows: Section 2, give necessary definitions, followed
discussion related work Section 3. Section 4, motivate query rewriting
steps means example. Section 5, give formal definitions rewriting
procedure show Boolean query indeed entailed knowledge base K iff
disjunction rewritten queries entailed K. Section 6, present deterministic
algorithm UCQ entailment SHIQ runs time single exponential size
knowledge base double exponential size query. Since combined
complexity conjunctive query entailment already 2ExpTime-hard DL ALCI
(Lutz, 2007), follows problem 2ExpTime-complete SHIQ. shows
conjunctive query entailment SHIQ strictly harder instance checking,
also case simpler DLs EL (Rosati, 2007b). show
(the decision problem corresponding to) conjunctive query answering SHIQ co-NPcomplete regarding data complexity, thus harder instance retrieval.
presented decision procedure gives insight query answering; also
immediate consequence field extending DL knowledge bases rules.
work Rosati (2006a, Thm. 11), consistency SHIQ knowledge base extended
(weakly-safe) Datalog rules decidable iff entailment unions conjunctive
queries SHIQ decidable. Hence, close open problem well.
paper extended version conference paper: Conjunctive Query Answering Description Logic SHIQ. Proceedings Twentieth International Joint
Conference Artificial Intelligence (IJCAI07), Jan 06 - 12, 2007.

2. Preliminaries
introduce basic terms notations used throughout paper. particular,
introduce DL SHIQ (Horrocks, Sattler, & Tobies, 2000) (unions of) conjunctive
queries.
2.1 Syntax Semantics SHIQ
Let NC , NR , NI countably infinite sets concept names, role names, individual
names. assume set role names contains subset NtR NR transitive role
names. role element NR {r | r NR }, roles form r called
inverse roles. role inclusion form r v r, roles. role hierarchy R
finite set role inclusions.
159

fiGlimm, Horrocks, Lutz, & Sattler

interpretation = (I ,I ) consists non-empty set , domain I,
function , maps every concept name subset AI , every role name r NR
binary relation rI , every role name r NtR transitive binary relation
rI , every individual name element aI . interpretation
satisfies role inclusion r v rI sI role hierarchy R satisfies role
inclusions R.
use following standard notation:
1. define function Inv roles Inv(r) := r r NR Inv(r) :=
r = role name s.
2. role hierarchy R, define v* R reflexive transitive closure v
R {Inv(r) v Inv(s) | r v R}. use r R abbreviation r v
* R
sv
* R r.
3. role hierarchy R role s, define set TransR transitive roles
{s | role r r R r NtR Inv(r) NtR }.
4. role r called simple w.r.t. role hierarchy R if, role v* R r,

/ TransR .
subscript R v* R TransR dropped clear context. set SHIQconcepts (or concepts short) smallest set built inductively NC using
following grammar, NC , n IN, r role simple role:
C ::= > | | | C | C1 u C2 | C1 C2 | r.C | r.C |6 n s.C |> n s.C.
Given interpretation I, semantics SHIQ-concepts defined follows:
>I

(r.C)I
(r.C)I
(6 n s.C)I
(> n s.C)I

=
=
=
=
=
=


(C u D)I = C DI
(C)I = \ C

(C D)I = C DI
{d | (d, d0 ) rI , d0 C }
{d | (d, d0 ) rI d0 C }
{d | ](sI (d, C)) n}
{d | ](sI (d, C)) n}

](M ) denotes cardinality set sI (d, C) defined
{d0 | (d, d0 ) sI d0 C }.
general concept inclusion (GCI) expression C v D, C
concepts. finite set GCIs called TBox. interpretation satisfies GCI C v
C DI , TBox satisfies GCI .
.
(ABox) assertion expression form C(a), r(a, b), r(a, b), =
6 b,
C concept, r role, a, b NI . ABox finite set assertions. use Inds(A)
denote set individual names occurring A. interpretation satisfies assertion
.
C(a) aI C , r(a, b) (aI , bI ) rI , r(a, b) (aI , bI )
/ rI , =
6 b aI 6= bI .
160

fiConjunctive Query Answering DL SHIQ

interpretation satisfies ABox satisfies assertion A, denote
|= A.
knowledge base (KB) triple (T , R, A) TBox, R role hierarchy,
ABox. Let K = (T , R, A) KB = (I ,I ) interpretation. say
satisfies K satisfies , R, A. case, say model K write
|= K. say K consistent K model.
2.1.1 Extending SHIQ SHIQu
following section, show reduce conjunctive query set ground
tree-shaped conjunctive queries. reduction, may introduce concepts
contain intersection roles existential quantification. define, therefore,
extension SHIQ role conjunction/intersection, denoted SHIQu and,
appendix, show decide consistency SHIQu knowledge bases.
addition constructors introduced SHIQ, SHIQu allows concepts
form
C ::= R.C | R.C |6 n S.C |> n S.C,
R := r1 u . . . u rn , := s1 u . . . u sn , r1 , . . . , rn roles, s1 , . . . , sn simple
roles. interpretation function extended (r1 u . . . u rn )I = r1 . . . rn .
2.2 Conjunctive Queries Unions Conjunctive Queries
introduce Boolean conjunctive queries since basic form queries
concerned with. later also define non-Boolean queries show
reduced Boolean queries. Finally, unions conjunctive queries disjunction
conjunctive queries.
simplicity, write conjunctive query set instead conjunction atoms.
example, write introductory example Section 1
{hasSon(x, y), hasDaughter(y, z), hasDescendant(x, z)}.
non-Boolean queries, i.e., consider problem query answering,
answer variables often given head query, e.g.,
(x1 , x2 , x3 ) {hasSon(x1 , x2 ), hasDaughter(x2 , x3 ), hasDescendant(x1 , x3 )}
indicates query answers tuples (a1 , a2 , a3 ) individual names that,
substituted x1 , x2 , x3 respectively, result Boolean query entailed
knowledge base. simplicity since mainly focus query entailment,
use query head even case non-Boolean query. Instead, explicitly say
variables answer variables ones existentially quantified. give
definition Boolean conjunctive queries.
Definition 1. Let NV countably infinite set variables disjoint NC , NR , NI .
term element NV NI . Let C concept, r role, t, t0 terms. atom
expression C(t), r(t, t0 ), t0 refer three different types atoms
concept atoms, role atoms, equality atoms respectively. Boolean conjunctive query
161

fiGlimm, Horrocks, Lutz, & Sattler

q non-empty set atoms. use Vars(q) denote set (existentially quantified)
variables occurring q, Inds(q) denote set individual names occurring q,
Terms(q) set terms q, Terms(q) = Vars(q) Inds(q). terms q
individual names, say q ground. sub-query q simply subset q
(including q itself). usual, use ](q) denote cardinality q, simply
number atoms q, use |q| size q, i.e., number symbols necessary
write q. SHIQ conjunctive query conjunctive query concepts C
occur concept atom C(t) SHIQ-concepts.
Since equality reflexive, symmetric transitive, define * transitive,
reflexive, symmetric closure terms q. Hence, relation *
equivalence relation terms q and, Terms(q), use [t] denote
equivalence class * .
Let = (I ,I ) interpretation. total function : Terms(q) evaluation (i) (a) = aI individual name Inds(q) (ii) (t) = (t0 ) t* t0 .
write
|= C(t) (t) C ;
|= r(t, t0 ) ((t), (t0 )) rI ;
|= t0 (t) = (t0 ).
If, evaluation , |= atoms q, write |= q. say
satisfies q write |= q exists evaluation |= q. call
match q I.
Let K SHIQ knowledge base q conjunctive query. |= K implies |= q,
say K entails q write K |= q.
4
query entailment problem defined follows: given knowledge base K
query q, decide whether K |= q.
atoms q follows:
brevity simplicity notation, define relation
0
* 0

q
C(t) q term Terms(q) C(t0 ) q, r(t1 , t2 )
0
0
0
0
0
0
* 0
* 0
terms t1 , t2 Terms(q) t1 t1 , t2 t2 , r(t1 , t2 ) q Inv(r)(t2 , t1 ) q.
clearly justified definition semantics, particular, |= r(t, t0 )
implies |= Inv(r)(t0 , t).
devising decision procedure CQ entailment, complications arise
cyclic queries (Calvanese et al., 1998a; Chekuri & Rajaraman, 1997). context,
say cyclic, mean graph structure induced query cyclic, i.e., graph
obtained q term considered node role atom induces
edge. Since, presence inverse roles, query containing role atom r(t, t0 )
equivalent query obtained replacing atom Inv(r)(t0 , t), direction
edges important say query cyclic underlying undirected
graph structure cyclic. Please note also multiple role atoms two terms
considered cycle, e.g., query {r(t, t0 ), s(t, t0 )} cyclic query. following
formal definition property.
Definition 2. query q cyclic exists sequence terms t1 , . . . , tn n > 3

162

fiConjunctive Query Answering DL SHIQ

q,
1. 1 < n, exists role atom ri (ti , ti+1 )
2. t1 = tn ,
3. ti 6= tj 1 < j < n.

4

definition, Item 3 makes sure consider queries cyclic
contain two terms t, t0 two role atoms using
here, implicitly uses relation
two terms. Please note use relation
*
abstracts directedness role atoms.
q s(t1 , t2 ), . . . , s(tn1 , tn )
following, write replace r(t, t0 )
0
= t1 = tn , mean first remove occurrences r(t, t0 ) Inv(r)(t0 , t)
* 0
t* t0
q, add atoms s(t1 , t2 ), . . . , s(tn1 , tn ) q.
W.l.o.g., assume queries connected. precisely, let q conjunctive
query. say q connected if, t, t0 Terms(q), exists sequence t1 , . . . , tn
q.
t1 = t, tn = t0 and, 1 < n, exists role r r(ti , ti+1 )
collection q1 , . . . , qn queries partitioning q q = q1 . . . qn , qi qj =
1 < j n, qi connected.
Lemma 3. Let K knowledge base, q conjunctive query, q1 , . . . , qn partitioning
q. K |= q iff K |= qi 1 n.
proof given Tessaris (2001, 7.3.2) and, lemma, clear
restriction connected queries indeed w.l.o.g. since entailment q decided
checking entailment qi time. follows, therefore assume queries
connected without notice.
Definition 4. union Boolean conjunctive queries formula q1 . . . qn ,
disjunct qi Boolean conjunctive query.
knowledge base K entails union Boolean conjunctive queries q1 . . . qn , written
K |= q1 . . . qn , if, interpretation |= K,
|= qi 1 n.
4
W.l.o.g. assume variable names disjunct different
variable names disjuncts. always achieved naming variables
apart. assume disjunct connected conjunctive query.
w.l.o.g. since UCQ contains unconnected disjuncts always transformed
conjunctive normal form; decide entailment resulting conjunct
separately conjunct union connected conjunctive queries. describe
transformation detail and, convenient notation, write
conjunctive query {at1 , . . . , atk } at1 . . . atk following proof, instead usual
set notation.
Lemma 5. Let K knowledge base, q = q1 . . . qn union conjunctive queries
that, 1 n, qi1 , . . . , qiki partitioning conjunctive query qi . K |= q iff
^
(q1i1 . . . qnin ).
K |=
(i1 ,...,in ){1,...,k1 }...{1,...,kn }

163

fiGlimm, Horrocks, Lutz, & Sattler

Again, detailed proof given Tessaris (2001, 7.3.3). Please note that, due
transformation conjunctive normal form, resulting number unions connected
conjunctive queries test entailment exponential size
original query. analysing complexity decision procedures presented
Section 6, show assumption CQ UCQ connected
increase complexity.
make connection query entailment query answering clearer.
query answering, let variables conjunctive query typed: variable either
existentially quantified (also called non-distinguished ) free (also called distinguished
answer variables). Let q query n variables (i.e., ](Vars(q)) = n), v1 , . . . , vm
(m n) answer variables. answers K = (T , R, A) q m-tuples
(a1 , . . . , ) Inds(A)m that, models K, |= q satisfies
(vi ) = ai 1 m. hard see answers K q
computed testing, (a1 , . . . , ) Inds(A)m , whether query q[v1 ,...,vm /a1 ,...,am ]
obtained q replacing occurrence vi ai 1 entailed K.
answer q set m-tuples (a1 , . . . , ) K |= q[v1 ,...,vm /a1 ,...,am ] .
Let k = ](Inds(A)) number individual names used ABox A. Since finite,
clearly k finite. Hence, deciding tuples belong set answers checked
k entailment tests. clearly efficient, optimizations
used, e.g., identify (hopefully small) set candidate tuples.
algorithm present Section 6 decides query entailment. reasons
devising decision procedure query entailment instead query answering twofold: first, query answering reduced query entailment shown above; second,
contrast query answering, query entailment decision problem studied
terms complexity theory.
remainder paper, stated otherwise, use q (possibly subscripts)
connected Boolean conjunctive query, K SHIQ knowledge base (T , R, A),
interpretation (I ,I ), evaluation.

3. Related Work
recently, automata-based decision procedure positive existential path queries
ALCQIbreg knowledge bases presented (Calvanese, Eiter, & Ortiz, 2007).
Positive existential path queries generalize unions conjunctive queries since SHIQ
knowledge base polynomially reduced ALCQIbreg knowledge base, presented algorithm decision procedure (union of) conjunctive query entailment
SHIQ well. automata-based technique considered elegant
rewriting algorithm, give NP upper bound data complexity
technique.
existing algorithms conjunctive query answering expressive DLs assume,
however, role atoms conjunctive queries use roles transitive.
consequence, example query introductory section cannot answered.
restriction, decision procedures various DLs around SHIQ known (Horrocks &
Tessaris, 2000; Ortiz, Calvanese, & Eiter, 2006b), known answering conjunctive
queries setting data complete co-NP (Ortiz et al., 2006b). Another common
164

fiConjunctive Query Answering DL SHIQ

restriction individuals named ABox considered assignments
variables. setting, semantics queries longer standard First-Order
one. restriction, answer example query introduction would
false since Mary named individual. hard see conjunctive query
answering restriction reduced standard instance retrieval replacing
variables individual names ABox testing entailment
conjunct separately. implemented DL reasoners, e.g., KAON2, Pellet,
RacerPro, provide interface conjunctive query answering setting
employ several optimizations improve performance (Sirin & Parsia, 2006; Motik,
Sattler, & Studer, 2004; Wessel & Moller, 2005). Pellet appears reasoner
also supports standard First-Order semantics SHIQ conjunctive queries
restriction queries acyclic.
best knowledge, still open problem whether conjunctive query
entailment decidable SHOIQ. Regarding undecidability results, known
conjunctive query entailment two variable fragment First-Order Logic L2 undecidable (Rosati, 2007a) Rosati identifies relatively small set constructors
causes undecidability.
Query entailment answering also studied context databases
incomplete information (Rosati, 2006b; van der Meyden, 1998; Grahne, 1991).
setting, DLs used schema languages, expressivity considered DLs
much lower expressivity SHIQ. example, constructors provided
logics DL-Lite family (Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2007)
chosen standard reasoning tasks PTime query entailment
LogSpace respect data complexity. Furthermore, TBox reasoning
done independently ABox ABox stored accessed using standard
database SQL engine. Since considered DLs considerable less expressive SHIQ,
techniques used databases incomplete information cannot applied
setting.
Regarding query language, well known extension conjunctive queries
inequalities undecidable (Calvanese et al., 1998a). Recently,
shown even DLs low expressivity, extension conjunctive queries
inequalities safe role negation leads undecidability (Rosati, 2007a).
related reasoning problem query containment. Given schema (or TBox)
two queries q q 0 , q contained q 0 w.r.t. iff every interpretation
satisfies q also satisfies q 0 . well known query containment w.r.t.
TBox reduced deciding query entailment (unions of) conjunctive queries w.r.t.
knowledge base (Calvanese et al., 1998a). Hence decision procedure (unions of)
conjunctive queries SHIQ also used deciding query containment w.r.t.
SHIQ TBox.
Entailment unions conjunctive queries also closely related problem
adding rules DL knowledge base, e.g., form Datalog rules. Augmenting
DL KB arbitrary Datalog program easily leads undecidability (Levy & Rousset,
1998). order ensure decidability, interaction Datalog rules
DL knowledge base usually restricted imposing safeness condition. DL+log
framework (Rosati, 2006a) provides least restrictive integration proposed far. Rosati
165

fiGlimm, Horrocks, Lutz, & Sattler

presents algorithm decides consistency DL+log knowledge base reducing
problem entailment unions conjunctive queries, proves decidability
UCQs SHIQ implies decidability consistency SHIQ+log knowledge bases.

4. Query Rewriting Example
section, motivate ideas behind query rewriting technique means
examples. following section, give precise definitions rewriting steps.
4.1 Forest Bases Canonical Interpretations
main idea focus models knowledge base kind
tree forest shape. well known one reason Description Modal Logics
robustly decidable enjoy form tree model property, i.e., every
satisfiable concept model tree-shaped (Vardi, 1997; Gradel, 2001). going
concept satisfiability knowledge base consistency, need replace tree model
property form forest model property, i.e., every consistent KB model
consists set trees, root corresponds named individual ABox.
roots connected via arbitrary relational structures, induced role assertions
given ABox. forest model is, therefore, forest graph theoretic sense.
Furthermore, transitive roles introduce short-cut edges elements within
tree even elements different trees. Hence talk form forest model
property.
define forest models show that, deciding query entailment,
restrict attention forest models. rewriting steps used transform cyclic
subparts query tree-shaped ones forest-shaped match
rewritten query forest models.
order make forest model property even clearer, also introduce forest bases,
interpretations interpret transitive roles unrestricted way, i.e.,
necessarily transitive way. forest base, require particular relationships elements domain inferred transitively closing role
omitted. following, assume ABox contains least one individual name,
i.e., Inds(A) non-empty. w.l.o.g. since always add assertion >(a)
ABox fresh individual name NI . readers familiar tableau algorithms,
worth noting forest bases also thought tableaux generated
complete clash-free completion tree (Horrocks et al., 2000).
Definition 6. Let denote non-negative integers set (finite) words
alphabet IN. tree non-empty, prefix-closed subset . w, w0 ,
call w0 successor w w0 = w c c IN, denotes concatenation.
call w0 neighbor w w0 successor w vice versa. empty word
called root.
forest base K interpretation J = (J ,J ) interprets transitive roles
unrestricted (i.e., necessarily transitive) way and, additionally, satisfies following
conditions:
T1 J Inds(A) that, Inds(A), set {w | (a, w) J } tree;
166

fiConjunctive Query Answering DL SHIQ

T2 ((a, w), (a0 , w0 )) rJ , either w = w0 = = a0 w0 neighbor w;
T3 Inds(A), aJ = (a, );
interpretation canonical K exists forest base J K
identical J except that, non-simple roles r,
[
(sJ )+
rI = rJ
v* R r, sTransR

case, say J forest base |= K say canonical
model K.
4
convenience, extend notion successors neighbors elements canonical models. Let canonical model (a, w), (a0 , w0 ) . call (a0 , w0 )
successor (a, w) either = a0 w0 = w c c w = w0 = . call
(a0 , w0 ) neighbor (a, w) (a0 , w0 ) successor (a, w) vice versa.
Please note definition implicitly relies unique name assumption
(UNA) (cf. T3). w.l.o.g. guess appropriate partition among individual names replace individual names partition one representative
individual name partition. Section 6, show partitioning individual names used simulate UNA, hence, decision procedure rely
UNA. also show affect complexity.
Lemma 7. Let K SHIQ knowledge base q = q1 . . . qn union conjunctive
queries. K 6|= q iff exists canonical model K 6|= q.
detailed proof given appendix. Informally, direction,
take arbitrary counter-model query, exists assumption, unravel
non-tree structures. Since, unraveling process, replace cycles
model infinite paths leave interpretation concepts unchanged, query
still satisfied unravelled canonical model. direction proof trivial.
4.2 Running Example
use following Boolean query knowledge base running example:
Example 8. Let K = (T , R, A) SHIQ knowledge base r, NtR , k
= { Ck v > k p.>,
C3 v > 3 p.>,
D2 v .> u t.>
}
R = { v ,
v r
}
= { r(a, b),
(p.Ck u p.C u r .C3 )(a),
(p.D1 u r.D2 )(b)
}
q = {r(u, x), r(x, y), t(y, y), s(z, y), r(u, z)} Inds(q) = Vars(q) = {u, x, y, z}.
167

fiGlimm, Horrocks, Lutz, & Sattler

simplicity, choose use CQ instead UCQ. case UCQ, rewriting
steps applied disjunct separately.
r

(a, )
p
(a, 1) Ck
p
(a, 11)

(a, 12)

p
...

p

r



r

(a, 2) C

p

(a, 1k)

r
r

(a, 31)

p

(a, 32)

p

D1 (b, 1)
r

(a, 3) C3

p

(b, )
r

p
(a, 33)

r

t,

(b, 2)
r D2

r,
t,
(b, 21)

t,

(b, 22)

Figure 1: representation canonical interpretation K.
Figure 1 shows representation canonical model knowledge base K
Example 8. labeled node represents element domain, e.g., individual
name represented node labeled (a, ). edges represent relationships
individuals. example, read r-labeled edge (a, ) (b, )

directions, i.e., (aI , bI ) = ((a, ), (b, )) rI (bI , aI ) = ((b, ), (a, )) r .
short-cuts due transitive roles shown dashed lines, relationship
nodes represent ABox individuals shown grey. Please note
indicate interpretations concepts figure.
Since canonical model K, elements domain pairs (a, w),
indicates individual name corresponds root tree, i.e., aI = (a, )
elements second place form tree according definition trees.
individual name ABox, can, therefore, easily define tree rooted
{w | (a, w) }.
(a, )
p
(a, 1)
p
(a, 11)

(a, 12)

p
...

p
(a, 2)

(a, 1k)

r

p
(a, 3)
p

p
(a, 31)

(b, )

r


p

(a, 32)

(b, 1)
p
(a, 33)

r
(b, 2)
r,
t,
(b, 21)

(b, 22)

Figure 2: forest base interpretation represented Figure 1.
Figure 2 shows representation forest base interpretation Figure 1
above. simplicity, interpretation concepts longer shown. two trees,
rooted (a, ) (b, ) respectively, clear.
graphical representation query q Example 8 shown Figure 3,
meaning nodes edges analogous ones given interpretations.
call query cyclic query since underlying undirected graph cyclic (cf. Definition 2).
Figure 4 shows match q and, although consider one canonical
model here, hard see query true model knowledge base,
i.e., K |= q.
168

fiConjunctive Query Answering DL SHIQ

x
r

r

u




r

z

Figure 3: graph representation query Example 8.
(a, )
r
(a, 1)

(a, 11)

(a, 12)

...

(a, 2)

(a, 1k)

(a, 31)

x
(b, )

r
r
(a, 3)

(a, 32)

u

(a, 33)

r

r

r

t,

r
(b, 1)
r

r

(b, 2)
r,
t,

z

(b, 21)

t,
(b, 22)

Figure 4: match query q Example 8 onto model Figure 1.
forest model property also exploited query rewriting process. want
rewrite q set queries q1 , . . . , qn ground tree-shaped queries K |= q
iff K |= q1 . . . qn . Since resulting queries ground tree-shaped queries,
explore known techniques deciding entailment queries. first step,
transform q set forest-shaped queries. Intuitively, forest-shaped queries consist
set tree-shaped sub-queries, roots trees might arbitrarily
interconnected (by atoms form r(t, t0 )). tree-shaped query special case
forest-shaped query. call arbitrarily interconnected terms forest-shaped
query root choice (or, short, roots). end rewriting process,
replace roots individual names Inds(A) transform tree parts
concept applying called rolling-up tuple graph technique (Tessaris, 2001;
Calvanese et al., 1998a).
proof correctness procedure, use structure forest bases
order explicate transitive short-cuts used query match. explicating
mean replace role atom mapped short-cut sequence
role atoms extended match modified query uses paths
forest base.
4.3 Rewriting Steps
rewriting process query q six stage process. end process,
rewritten query may may forest shape. show later, dont know
non-determinism compromise correctness algorithm. first stage,
derive collapsing qco q adding (possibly several) equality atoms q. Consider,
169

fiGlimm, Horrocks, Lutz, & Sattler

example, cyclic query q = {r(x, y), r(x, 0 ), s(y, z), s(y 0 , z)} (see Figure 5),
transformed tree-shaped one adding equality atom 0 .
x

x
r

r

r

y, 0


y0






z

z

Figure 5: representation cyclic query tree-shaped query obtained adding
atom 0 query depicted left hand side.
common property next three rewriting steps allow substituting
implicit short-cut edges explicit paths induce short-cut. three steps
aim different cases short-cuts occur describe goals
application detail:
second stage called split rewriting. split rewriting take care role
atoms matched transitive short-cuts connecting elements two different trees
by-passing one roots. substitute short-cuts either one
two role atoms roots included. running example, maps u (a, 3)
x (b, ). Hence |= r(u, x), used r-edge transitive short-cut connecting
tree rooted tree rooted b, by-passing (a, ). Similar arguments hold
atom r(u, z), path implies short-cut relationship goes via
two roots (a, ) (b, ). clear r must non-simple role since, forest
base J I, direct connection different trees
roots trees. Hence, ((u), (x)) rI holds role TransR
v
* R r. case example, r transitive. split rewriting eliminates
transitive short-cuts different trees canonical model adds missing
variables role atoms matching sequence edges induce short-cut.
ux

r
r

x
r


r

u



z

Figure 6: split rewriting qsr query shown Figure 3.
Figure 6 depicts split rewriting
qsr = { r(u, ux), r(ux, x), r(x, y), t(y, y), s(z, y),
r(u, ux), r(ux, x), r(x, z)}
170

fiConjunctive Query Answering DL SHIQ

q obtained q replacing (i) r(u, x) r(u, ux) r(ux, x) (ii) r(u, z)
r(u, ux), r(ux, x), r(x, z). Please note introduced new variable (ux)
re-used existing variable (x). Figure 7 shows match qsr canonical model
K two trees connected via roots. rewritten query,
also guess set roots, contains variables mapped roots
canonical model. running example, guess set roots {ux, x}.
(a, )

ux

x
(b, )

r
r

(a, 1)

r
(a, 3)

(a, 2)

(b, 1)

u

,
(b, 2)

r

s, r
(a, 11)

(a, 12)

...

(a, 1k)

(a, 31)

(a, 32)

(a, 33)

z

(b, 21)

(b, 22)

Figure 7: split match sr query qsr Figure 6 onto canonical interpretation
Figure 1.
third step, called loop rewriting, eliminate loops variables v
correspond roots replacing atoms r(v, v) two atom r(v, v 0 ) r(v 0 , v), v 0
either new existing variable q. running example, eliminate
loop t(y, y) follows:
q`r = { r(u, ux), r(ux, x), r(x, y), t(y, 0 ), t(y 0 , y), s(z, y),
r(u, ux), r(ux, x), r(x, z)}
query obtained qsr (see Figure 6) replacing t(y, y) t(y, 0 ) t(y 0 , y)
new variable 0 . Please note that, since defined transitive symmetric, t(y, y)
still implied, i.e., loop also transitive short-cut. Figure 8 shows canonical
interpretation Figure 1 match `r q`r . introduction new variable
0 needed case since variable could re-used individual
(b, 22) range match sr .
(a, )

ux

x
(b, )

r
r

(a, 1)

(a, 11)

(a, 12)

...

(a, 2)

(a, 1k)

(a, 31)

r
(a, 3)

(a, 32)

u

(a, 33)

(b, 1)

(b, 2)
r
s, r
t,
z

(b, 21)

(b, 22)
y0

Figure 8: loop rewriting q`r match canonical interpretation Figure 1.
forth rewriting step, called forest rewriting, allows replacement role
atoms sets role atoms. allows elimination cycles within single
171

fiGlimm, Horrocks, Lutz, & Sattler

tree. forest rewriting qf r example obtained q`r replacing role
atom r(x, z) r(x, y) r(y, z), resulting query
qf r = { r(u, ux), r(ux, x), r(x, y), t(y, 0 ), t(y 0 , y), s(z, y),
r(u, ux), r(ux, x), r(x, y), r(y, z)}.
Clearly, results tree-shaped sub-queries, one rooted ux one rooted x.
Hence qf r forest-shaped w.r.t. root terms ux x. Figure 9 shows canonical
interpretation Figure 1 match f r qf r .
(a, )

ux

x
(b, )

r
r

(a, 1)

(a, 2)

r
(a, 3)

u

(b, 1)

(b, 2)
r,
t,

(a, 11)

(a, 12)

...

(a, 1k)

(a, 31)

(a, 32)

(a, 33)

z

(b, 21)

(b, 22)
y0

Figure 9: forest rewriting qf r forest match f r canonical interpretation
Figure 1.
fifth step, use standard rolling-up technique (Horrocks & Tessaris, 2000;
Calvanese et al., 1998a) express tree-shaped sub-queries concepts. order
this, traverse tree bottom-up fashion replace leaf (labeled
concept C, say) incoming edge (labeled role r, say) concept r.C
added predecessor. example, tree rooted ux (i.e., role atom r(u, ux))
replaced atom (r .>)(ux). Similarly, tree rooted x (i.e., role
atoms r(x, y), r(y, z), s(z, y), t(y, 0 ), t(y 0 , y)) replaced atom
(r.(((r u Inv(s)).>) u ((t u Inv(t)).>))(x).
Please note use role conjunctions resulting query order capture
semantics multiple role atoms relating pair variables.
Recall that, split rewriting, guessed x ux correspond roots and,
therefore, correspond individual names Inds(A). sixth last rewriting step,
guess variable corresponds individual name replace variables
guessed names. possible guess running example would ux corresponds
x b. results (ground) query
{(r .>)(a), r(a, b), (r.(((r u Inv(s)).>) u ((t u Inv(t)).>)))(b)},
entailed K.
Please note focused running example reasonable rewriting.
several possible rewritings, e.g., obtain another rewriting qf r
replacing ux b x last step. UCQ, apply rewriting steps
disjuncts separately.
172

fiConjunctive Query Answering DL SHIQ

end rewriting process, have, disjunct, set ground queries
and/or queries rolled-up single concept atom. latter queries result
forest rewritings tree-shaped empty set roots. tree-shaped
rewritings match anywhere tree can, thus, grounded. Finally, check
knowledge base entails disjunction rewritten queries. show
bound number (forest-shaped) rewritings hence number
queries produced rewriting process.
Summing up, rewriting process connected conjunctive query q involves
following steps:
1. Build collapsings q.
2. Build split rewritings collapsing w.r.t. subset R roots.
3. Build loop rewritings split rewritings.
4. Build (forest-shaped) forest rewritings loop rewritings.
5. Roll tree-shaped sub-query forest-rewriting concept atom
6. replace roots R individual names ABox possible ways.
Let q1 , . . . , qn queries resulting rewriting process. next section,
define rewriting step prove K |= q iff K |= q1 qn . Checking entailment
rewritten queries easily reduced KB consistency decision procedure
SHIQu KB consistency could used order decide K |= q. present one
decision procedure Section 6.

5. Query Rewriting
previous section, used several terms, e.g., tree- forest-shaped query,
rather informally. following, give definitions terms used query
rewriting process. done, formalize query rewriting steps prove
correctness procedure, i.e., show forest-shaped queries obtained
rewriting process indeed used deciding whether knowledge base entails
original query. give detailed proofs here, rather intuitions behind
proofs. Proofs full detail given appendix.
5.1 Tree- Forest-Shaped Queries
order define tree- forest-shaped queries precisely, use mappings
queries trees forests. Instead mapping equivalence classes terms * nodes
tree, extend well-known properties functions follows:
Definition 9. mapping f : B, use dom(f ) ran(f ) denote f domain
*
range B, respectively. Given equivalence relation
dom(f ), say f
0
0
*
injective modulo
if, a, dom(f ), f (a) = f (a ) implies a* a0 say f
*
bijective modulo
f injective modulo * surjective. Let q query. tree
mapping q total function f terms q tree
173

fiGlimm, Horrocks, Lutz, & Sattler

*
1. f bijective modulo
,

q, f (t) neighbor f (t0 ), and,
2. r(t, t0 )
3. Inds(q), f (a) = .
query q tree-shaped ](Inds(q)) 1 tree mapping q.
root choice R q subset Terms(q) Inds(q) R and, R
* 0
, t0 R. R, use Reach(t) denote set terms t0 Terms(q)
exists sequence terms t1 , . . . , tn Terms(q)
1. t1 = tn = t0 ,
q, and,
2. 1 < n, role r r(ti , ti+1 )
3. 1 < n, ti R, ti* t.
call R root splitting w.r.t. q either R = if, ti , tj R, ti 6 * tj implies
Reach(ti ) Reach(tj ) = . term R induces sub-query
q | terms occur Reach(t)}\
subq(q, t) := {at
q}.
{r(t, t) | r(t, t)
query q forest-shaped w.r.t. root splitting R either R = q tree-shaped
sub-query subq(q, t) R tree-shaped.
4
term R, collect terms reachable set Reach(t).
Condition 3, make sure R * t0 Reach(t) either
R t* t0 . Since queries connected assumption, would otherwise collect terms
Reach(t) t0
/ R. root splitting, require resulting sets
mutually disjoint terms t, t0 R equivalent. guarantees
paths sub-queries go via root nodes respective trees. Intuitively,
forest-shaped query one potentially mapped onto canonical interpretation
= (I ,I ) terms root splitting R correspond roots (a, ) .
q, parts
definition subq(q, t), exclude loops form r(t, t)
query grounded later query rewriting process ground terms,
allow arbitrary relationships.
Consider, example, query qsr running example previous section
(cf. Figure 6). Let us make root choice R := {ux, x} q. sets Reach(ux)
Reach(x) w.r.t. qsr R {ux, u} {x, y, z} respectively. Since sets
disjoint, R root splitting w.r.t. qsr . choose, however, R := {x, y}, set R
root splitting w.r.t. qsr since Reach(x) = {ux, u, z} Reach(y) = {z} disjoint.
5.2 Graphs Forests
ready define query rewriting steps. Given arbitrary query, exhaustively apply rewriting steps show use resulting queries
forest-shaped deciding entailment original query. Please note following
definitions conjunctive queries unions conjunctive queries since
apply rewriting steps disjunct separately.
174

fiConjunctive Query Answering DL SHIQ

Definition 10. Let q Boolean conjunctive query. collapsing qco q obtained
adding zero equality atoms form t0 t, t0 Terms(q) q. use co(q)
denote set queries collapsing q.
Let K SHIQ knowledge base. query qsr called split rewriting q w.r.t. K
q, either:
obtained q choosing, atom r(t, t0 )
1. nothing,
2. choose role TransR v* R r replace r(t, t0 ) s(t, u), s(u, t0 ),
3. choose role TransR v* R r replace r(t, t0 ) s(t, u), s(u, u0 ),
s(u0 , t0 ),
u, u0 NV possibly fresh variables. use srK (q) denote set pairs
(qsr , R) query qco co(q) qsr split rewriting qco R
root splitting w.r.t. qsr .
query q`r called loop rewriting q w.r.t. root splitting R K obtained
q
q choosing, atoms form r(t, t)
/ R, role TransR
v* R r replacing r(t, t) two atoms s(t, t0 ) s(t0 , t) t0 NV possibly
fresh variable. use lrK (q) denote set pairs (q`r , R) tuple
(qsr , R) srK (q) q`r loop rewriting qsr w.r.t. R K.
forest rewriting, fix set V NV variables occurring q
](V ) ](Vars(q)). forest rewriting qf r w.r.t. root splitting R q K obtained
q
q choosing, role atom r(t, t0 ) either R = r(t, t0 )
subq(q, tr ) either
tr R r(t, t0 )
1. nothing,
2. choose role TransR v
* R r replace r(t, t0 ) ` ](Vars(q)) role
atoms s(t1 , t2 ), . . . , s(t` , t`+1 ), t1 = t, t`+1 = t0 , t2 , . . . , t` Vars(q) V .
use frK (q) denote set pairs (qf r , R) tuple (q`r , R) lrK (q)
qf r forest-shaped forest rewriting q`r w.r.t. R K.
4
K clear context, say q 0 split, loop, forest rewriting
q instead saying q 0 split, loop, forest rewriting q w.r.t. K. assume
srK (q), lrK (q), frK (q) contain isomorphic queries, i.e., differences (newly
introduced) variable names neglected.
next section, show build disjunction conjunctive queries
q1 q` queries frK (q) qi 1 ` either form
C(v) single variable v Vars(qi ) qi ground, i.e., qi contains constants
variables. remains show K |= q iff K |= q1 q` .
5.3 Trees Concepts
order transform tree-shaped query single concept atom forest-shaped
query ground query, define mapping f terms tree-shaped subquery tree. incrementally build concept corresponds tree-shaped
query traversing tree bottom-up fashion, i.e., leaves upwards
root.
175

fiGlimm, Horrocks, Lutz, & Sattler

Definition 11. Let q tree-shaped query one individual name.
Inds(q), let tr = otherwise let tr = v variable v Vars(q). Let f tree
mapping f (tr ) = . inductively assign, term Terms(q),
concept con(q, t) follows:

f (t) leaf ran(f ), con(q, t) := C(t) q C,
f (t) successors f (t1 ), . . . , f (tk ),
con(q, t) :=





Cu


q r .con(q, ti ).
r(t,ti )
1ik
q
C(t)

Finally, query concept q w.r.t. tr con(q, tr ).

4

Please note definition takes equality atoms account.
*
function f bijective modulo
and, case concept atoms C(t) C(t0 )
.
t* t0 , concepts conjoined query concept due use relation
Similar arguments applied role atoms.
following lemma shows query concepts indeed capture semantics q.
Lemma 12. Let q tree-shaped query tr Terms(q) defined above, Cq =
con(q, tr ), interpretation. |= q iff match element
Cq (tr ) = d.
proof given Horrocks, Sattler, Tessaris, Tobies (1999) easily transfers
DLR SHIQ. applying result lemma, transform
forest-shaped query ground query follows:
Definition 13. Let (qf r , R) frK (q) R 6= , : R Inds(A) total function
that, Inds(q), (a) = and, t, t0 R, (t) = (t0 ) iff t* t0 . call
mapping ground mapping R w.r.t. A. obtain ground query ground(qf r , R, )
qf r w.r.t. root splitting R ground mapping follows:
replace R (t), and,
ran( ), replace sub-query qa = subq(qf r , a) con(qa , a).
define set groundK (q) ground queries q w.r.t. K follows:
groundK (q) := {q 0 | exists (qf r , R) frK (q) R 6=
ground mapping w.r.t. R
q 0 = ground(qf r , R, )}
define set treesK (q) tree queries q follows:
treesK (q) := {q 0 | exists (qf r , ) frK (q)
v Vars(qf r ) q 0 = (con(qf r , v))(v)}
176

4

fiConjunctive Query Answering DL SHIQ

Going back running example, already seen (qf r , {ux, x}) belongs
set frK (q)
qf r = {r(u, ux), r(ux, x), r(x, y), t(y, 0 ), t(y 0 , y), s(z, y), r(y, z)}.
also several queries set frK (q), e.g., (q, {u, x, y, z}), q
original query root splitting R R = Terms(q), i.e., terms
root choice q. order build set groundK (q), build possible ground
mappings set Inds(A) individual names ABox root splittings
queries frK (q). tuple (qf r , {ux, x}) frK (q) contributes two ground queries
set groundK (q):
ground(qf r , {ux, x}, {ux 7 a, x 7 b}) =
{r(a, b), (Inv(r).>)(a), (r.(((r u Inv(s)).>) u ((t u Inv(t)).>)))(b)},
Inv(r).> query concept (tree-shaped) sub-query subq(qf r , ux)
r.(((r u Inv(s)).>) u ((t u Inv(t)).>) query concept subq(qf r , x)
ground(qf r , {ux, x}, {ux 7 b, x 7 a}) =
{r(b, a), (Inv(r).>)(b), (r.(((r u Inv(s)).>) u ((t u Inv(t)).>)))(a)}.
tuple (q, {u, x, y, z}) frK (q), however, contribute ground query since,
ground mapping, require (t) = (t0 ) iff t* t0 two individual
names Inds(A) compared four terms q need distinct value. Intuitively,
restriction, since first rewriting step (collapsing) produce queries
terms q identified possible ways.
example, K |= q K |= q1 q` , q1 q` queries treesK (q)
groundK (q) since model K satisfies qi = ground(qf r , {ux, x}, {ux 7 a, x 7 b}).
5.4 Query Matches
Even query true canonical model, necessarily mean query
tree- forest-shaped. However, match canonical interpretation guide
process rewriting query. Similarly definition tree- forest-shaped queries,
define shape matches query. particular, introduce three different kinds
matches: split matches, forest matches, tree matches every tree match
forest match, every forest match split match. correspondence query
shapes follows: given split match , set root nodes (a, ) range
match define root splitting query, additionally forest match,
query forest-shaped w.r.t. root splitting induced , additionally tree
match, whole query mapped single tree (i.e., query tree-shaped
forest-shaped w.r.t. empty root splitting). Given arbitrary query match
canonical model, first obtain split match tree forest match, using
structure canonical model guiding application rewriting steps.
Definition 14. Let K SHIQ knowledge base, q query, = (I ,I ) canonical
model K, : Terms(q) evaluation |= q. call split match
q, one following holds:
if, r(t, t0 )
177

fiGlimm, Horrocks, Lutz, & Sattler

1. (t) = (a, ) (t0 ) = (b, ) a, b Inds(A);
2. (t) = (a, w) (t0 ) = (a, w0 ) Inds(A) w, w0 .
call forest match if, additionally, term tr Terms(q) (tr ) = (a, )
Inds(A), total bijective mapping f {(a, w) | (a, w) ran()}
subq(q, tr ) implies f ((t)) neighbor f ((t0 )).
tree r(t, t0 )
call tree match if, additionally, Inds(A) element ran()
form (a, w).
split match canonical interpretation induces (possibly empty) root splitting
R R iff (t) = (a, ) Inds(A). call R root splitting induced
.
4
two elements (a, w) (a, w0 ) canonical model, path (a, w) (a, w0 )
sequence (a, w1 ), . . . , (a, wn ) w = w1 , w0 = wn , and, 1 < n, wi+1
successor wi . length path n. Please note that, forest match,
require w neighbor w0 vice versa. still allows map role atoms
paths canonical model length greater two, paths must
ancestors elements different branches tree. mapping f
tree also makes sure R induced root splitting, sub-query subq(q, t)
* 0
R tree-shaped. tree match, root splitting either empty

0
*
t, R, i.e., single root modulo , whole query tree-shaped.
5.5 Correctness Query Rewriting
following lemmas state correctness rewriting step step
rewriting stages. Full proofs given appendix. motivated previous
section, use given canonical model guide rewriting process
obtain forest-shaped query also match model.
Lemma 15. Let model K.
1. |= q, collapsing qco q |=co qco co injection
*
modulo
.
2. |=co qco collapsing qco q, |= q.
Given model satisfies q, simply add equality atoms pairs terms
mapped element I. hard see results
mapping injective modulo * . second part, easy see model
satisfies collapsing also satisfies original query.
Lemma 16. Let model K.
1. canonical |= q, pair (qsr , R) srK (q) split match
sr |=sr qsr , R induced root splitting sr , sr injection
*
modulo
.
2. (qsr , R) srK (q) |=sr qsr match sr , |= q.

178

fiConjunctive Query Answering DL SHIQ

first part lemma, proceed exactly illustrated example section
use canonical model match guide rewriting steps. first build
collapsing qco co(q) described proof Lemma 15 |=co qco co
*
injection modulo
. Since canonical, paths different trees occur
due non-simple roles, thus replace role atom uses short-cut
two three role atoms roots explicitly included query (cf.
query match Figure 4 obtained split rewriting split match
Figure 7). second part lemma follows immediately fact use
transitive sub-roles replacement.
Lemma 17. Let model K.
1. canonical |= q, pair (q`r , R) lrK (q) mapping `r
*
|=`r q`r , `r injection modulo
, R root splitting induced
q`r , R.
`r and, r(t, t)
2. (q`r , R) lrK (q) |=`r q`r match `r , |= q.
second part straightforward, given use transitive sub-roles
loop rewriting. first part, proceed described examples
section use canonical model match guide rewriting process.
first build split rewriting qsr root splitting R described proof Lemma 16
(qsr , R) srK (q) |=sr qsr split match sr . Since canonical
model, forest base J . forest base, non-root nodes cannot successors
themselves, loop short-cut due transitive role. element
is, say, r-related has, therefore, neighbor r- Inv(r)-successor.
Depending whether neighbor already range match, either
re-use existing variable introduce new one, making path explicit (cf.
loop rewriting depicted Figure 8 obtained split rewriting shown Figure 7).
Lemma 18. Let model K.
1. canonical |= q, pair (qf r , R) frK (q) |=f r qf r
forest match f r , R induced root splitting f r , f r injection
*
modulo
.
2. (qf r , R) frK (q) |=f r qf r match f r , |= q.
main challenge proof (1) give short idea here.
point, know Lemma 17 use query q`r
root splitting R split match `r . Since `r split match, match
sub-query restricted tree thus transform sub-query q`r induced
term root choice separately. following example meant illustrate
given bound ](Vars(q)) number new variables role atoms
introduced forest rewriting suffices. Figure 10 depicts representation tree
canonical model, use second part names elements, e.g.,
use instead (a, ). simplicity, also indicate concepts
roles label nodes edges, respectively. use black color indicate nodes
179

fiGlimm, Horrocks, Lutz, & Sattler

edges used match query dashed lines short-cuts due
transitive roles. example, grey edges also belong forest base
query match uses short-cuts.


1

11

12

111

Figure 10: part representation canonical model, black nodes
edges used match query dashed edges indicate short-cuts due
transitive roles.

forest rewriting aims making short-cuts explicit replacing
edges necessary obtain tree match. order this, need include
common ancestors forest base two nodes used match.
w, w0 , therefore define longest common prefix (LCP) w w0 longest
w w prefix w w0 . forest rewriting, determine
LCPs two nodes range match add variable LCPs
yet range match set V new variables used forest
rewriting. example Figure 10 set V contains single variable v1
node 1.
explicate short-cuts follows: edge used match, e.g.,
edge 111 example, define path sequence elements
path forest base, e.g., path edge 111 , 1, 11, 111. relevant
path obtained dropping elements path range
mapping correspond variable set V , resulting relevant path , 1, 111
example. replace role atom matched edge 111
two role atoms match uses edge 1 1 111.
appropriate transitive sub-role exists since otherwise could short-cut. Similar
arguments used replace role atom mapped edge 111 12
one mapped edge 12, resulting match represented
Figure 11. given restriction cardinality set V limitation since
number LCPs set V maximal pair nodes one
ancestor other. see nodes n leaf nodes tree least
binarily branching. Since tree n inner nodes, need n
new variables query n variables.
180

fiConjunctive Query Answering DL SHIQ



1

11

12

111

Figure 11: match forest rewriting obtained example given Figure 10.

bound number role atoms used replacement
single role atom, consider, example, cyclic query
q = {r(x1 , x2 ), r(x2 , x3 ), r(x3 , x4 ), t(x1 , x4 )},
knowledge base K = (T , R, A) = , R = {r v t} TransR
= {(r.(r.(r.>)))(a)}. hard check K |= q. Similarly running
example previous section, also single rewriting true
canonical model KB, obtained building forest rewriting
nothing rewriting steps, except choosing empty set root splitting
split rewriting step. forest rewriting, explicate short-cut used
mapping t(x1 , x4 ) replacing t(x1 , x4 ) t(x1 , x2 ), t(x2 , x3 ), t(x3 , x4 ).
using Lemmas 15 18, get following theorem, shows use
ground queries groundK (q) queries treesK (q) order check whether K
entails q, well understood problem.
Theorem 19. Let K SHIQ knowledge base, q Boolean conjunctive query,
{q1 , . . . , q` } = treesK (q) groundK (q). K |= q iff K |= q1 . . . q` .
give upper bounds size number queries treesK (q) groundK (q).
before, use ](S) denote cardinality set S. size |K| (|q|) knowledge
base K (a query q) simply number symbols needed write alphabet
constructors, concept names, role names occur K (q), numbers
encoded binary. Obviously, number atoms query bounded size, hence
](q) |q| and, simplicity, use n size cardinality q follows.
Lemma 20. Let q Boolean conjunctive query, K = (T , R, A) SHIQ knowledge base,
|q| := n |K| := m. polynomial p
1. ](co(q)) 2p(n) and, q 0 co(q), |q 0 | p(n),
2. ](srK (q)) 2p(n)log p(m) , and, q 0 srK (q), |q 0 | p(n),
3. ](lrK (q)) 2p(n)log p(m) , and, q 0 lrK (q), |q 0 | p(n),
181

fiGlimm, Horrocks, Lutz, & Sattler

4. ](frK (q)) 2p(n)log p(m) , and, q 0 frK (q), |q 0 | p(n),
5. ](treesK (q)) 2p(n)log p(m) , and, q 0 treesK (q), |q 0 | p(n),
6. ](groundK (q)) 2p(n)log p(m) , and, q 0 groundK (q), |q 0 | p(n).
consequence lemma, bound number queries
groundK (q) treesK (q) hard see two sets computed
time polynomial exponential n.
next section, present algorithm decides entailment unions conjunctive queries, queries either ground query consists single
concept atom C(x) existentially quantified variable x. Theorem 19 Lemma 20,
algorithm decision procedure arbitrary unions conjunctive queries.
5.6 Summary Discussion
section, presented main technical foundations answering (unions
of) conjunctive queries. known queries contain non-simple roles cycles
among existentially quantified variables difficult handle. applying rewriting
steps Definition 10, rewrite cyclic conjunctive queries set acyclic
and/or ground queries. types queries easier handle algorithms
types exist. point, reasoning algorithm SHIQu knowledge base consistency
used deciding query entailment. order obtain tight complexity results,
present following section decision procedure based extension
translation looping tree automata given Tobies (2001).
worth mentioning that, queries simple roles, algorithm behaves
exactly existing rewriting algorithms (i.e., rolling-up tuple graph technique)
since, case, collapsing step applicable. need identifying variables
first pointed work Horrocks et al. (1999) also required (although
mentioned) algorithm proposed Calvanese et al. (1998a).
new rewriting steps (split, loop, forest rewriting) required
applicable non-simple roles and, replacing role atom, transitive sub-roles
replaced role used. Hence number resulting queries fact determined
size whole knowledge base, number transitive sub-roles
non-simple roles query. Therefore, number resulting queries really depends
number transitive roles depth role hierarchy non-simple roles
query, can, usually, expected small.

6. Decision Procedure
devise decision procedure entailment unions Boolean conjunctive queries
uses, disjunct, queries obtained rewriting process defined
previous section. Detailed proofs lemmas theorems section
found appendix. knowledge base K union Boolean conjunctive queries
q1 . . . q` , show use queries treesK (qi ) groundK (qi ) 1 `
order build set knowledge bases K1 , . . . , Kn K |= q1 . . . q` iff
Ki inconsistent. gives rise two decision procedures: deterministic one
182

fiConjunctive Query Answering DL SHIQ

enumerate Ki , use derive tight upper bound combined
complexity; non-deterministic one guess Ki , yields tight
upper bound data complexity. Recall that, combined complexity, knowledge
base K queries qi count input, whereas data complexity
ABox counts input, parts assumed fixed.
6.1 Deterministic Decision Procedure Query Entailment SHIQ
first define deterministic version decision procedure give upper bound
combined complexity. given algorithm takes input union connected
conjunctive queries works unique name assumption (UNA). show afterwards extended algorithm make UNA takes
arbitrary UCQs input, complexity results carry over.
construct set knowledge bases extend original knowledge base K
w.r.t. TBox ABox. extended knowledge bases given KB K
entails query q iff extended KBs inconsistent. handle concepts obtained
tree-shaped queries differently ground queries: axioms add
TBox prevent matches tree-shaped queries, whereas extended ABoxes contain
assertions prevent matches ground queries.
Definition 21. Let K = (T , R, A) SHIQ knowledge base q = q1 . . . q` union
Boolean conjunctive queries. set
1. := treesK (q1 ) . . . treesK (q` ),
2. G := groundK (q1 ) . . . groundK (q` ),
3. Tq := {> v C | C(v) }.
extended knowledge base Kq w.r.t. K q tuple (T Tq , R, Aq ) Aq
contains, q 0 G, least one assertion q 0 .
4
Informally, extended TBox Tq ensures tree matches.
extended ABox Aq contains, ground query q 0 obtained rewriting process,
least one assertion q 0 spoils match q 0 . model
extended ABox can, therefore, satisfy ground queries. model
extended knowledge bases, know counter-model original
query.
use extended knowledge bases order define deterministic
version algorithm deciding entailment unions Boolean conjunctive queries
SHIQ.
Definition 22. Given SHIQ knowledge base K = (T , R, A) union connected
Boolean conjunctive queries q input, algorithm answers K entails q extended
knowledge base w.r.t. K q inconsistent answers K entail q otherwise.
4
following lemma shows described algorithm indeed correct.
183

fiGlimm, Horrocks, Lutz, & Sattler

Lemma 23. Let K SHIQ knowledge base q union connected Boolean conjunctive queries. Given K q input, algorithm Definition 22 answers K
entails q iff K |= q unique name assumption.
proof direction lemma, use canonical model
K order guide rewriting process. direction, assume
contrary shown consistent extended knowledge base,
K 6|= q. use model K 6|= q, exists assumption,
show also model extended knowledge base.
6.1.1 Combined Complexity Query Entailment SHIQ
According lemma, algorithm given Definition 22 correct.
analyse combined complexity thereby prove also terminating.
complexity analysis, assume, usual (Hustadt et al., 2005; Calvanese,
De Giacomo, Lembo, Lenzerini, & Rosati, 2006; Ortiz et al., 2006b), concepts
concept atoms ABox assertions literals, i.e., concept names negated concept
names. input query ABox contains non-literal atoms assertions, easily
transform literal ones truth preserving way: concept atom C(t)
query C non-literal concept, introduce new atomic concept AC NC ,
add axiom C v AC TBox, replace C(t) AC (t); non-literal
concept assertion C(a) ABox, introduce new atomic concept AC NC , add
axiom AC v C TBox, replace C(a) AC (a). transformation
obviously polynomial, without loss generality, safe assume ABox
query contain literal concepts. advantage size atom
ABox assertion constant.
Since algorithm involves checking consistency SHIQu knowledge base,
analyse complexity reasoning service. Tobies (2001) shows ExpTime
upper bound deciding consistency SHIQ knowledge bases (even binary
coding numbers) translating SHIQ KB equisatisfiable ALCQIb knowledge
base. b stands safe Boolean role expressions built ALCQIb roles using
operator u (role intersection), (role union), (role negation/complement) that,
transformed disjunctive normal form, every disjunct contains least one nonnegated conjunct. Given query q SHIQ knowledge base K = (T , R, A), reduce
query entailment deciding knowledge base consistency extended SHIQu knowledge
base Kq = (T Tq , R, Aq ). Recall Tq Aq parts contain
role conjunctions use role negation ABox assertions. extend
translation given SHIQ used deciding consistency SHIQu
KBs. Although translation works SHIQu KBs, assume input KB
exactly form extended knowledge bases described above.
translation unrestricted SHIQu longer polynomial, case SHIQ,
exponential size longest role conjunction universal quantifier. Since
role conjunctions occur extended ABox TBox, since size role
conjunction is, Lemma 20, polynomial size q, translation exponential
size query case extended knowledge bases.
184

fiConjunctive Query Answering DL SHIQ

assume here, usual, concepts negation normal form (NNF);
concept transformed linear time equivalent one NNF pushing negation
inwards, making use de Morgans laws duality existential universal
restrictions, atmost atleast number restrictions (6 n r.C > n r.C
respectively) (Horrocks et al., 2000). concept C, use C
denote NNF
C.
define closure cl(C, R) concept C w.r.t. role hierarchy R smallest
set satisfying following conditions:
sub-concept C, cl(C, R),
cl(C, R),
cl(C, R),
r.D cl(C, R), v* R r, TransR , s.D cl(C, R).
show extend translation SHIQ ALCQIb given
Tobies. first consider SHIQu -concepts extend translation KBs.
Definition 24. role hierarchy R roles r, r1 , . . . , rn , let
l
(r, R) =

(r1 u . . . u rn , R) =(r1 , R) u . . . u (rn , R).
r v* R

4

Please note that, since r v* R r, r occurs (r, R).
Lemma 25. Let R role hierarchy, r1 , . . . , rn roles. every interpretation
|= R, holds ((r1 u . . . u rn , R))I = (r1 u . . . u rn )I .
extended definition role conjunctions, adapt definition
(Def. 6.22) Tobies provides translating SHIQ-concepts ALCQIb-concepts.
Definition 26. Let C SHIQu -concept NNF R role hierarchy. every
concept (r1 u . . . u rn ).D cl(C, R), let Xr1 u...urn ,D NC unique concept name
occur cl(C, R). Given role hierarchy R, define function tr inductively
structure concepts setting
tr(A, R)
tr(A, R)
tr(C1 u C2 , R)
tr(C1 C2 , R)
tr(./ n(r1 u . . . u rn ).D, R)
tr((r1 u . . . u rn ).D, R)
tr((r1 u . . . u rn ).D, R)

=
=
=
=
=
=
=

NC
NC
tr(C1 , R) u tr(C2 , R)
tr(C1 , R) tr(C2 , R)
(./ n (r1 u . . . u rn , R).tr(D, R))
Xr1 u...urn ,D
(Xr1 u...urn ,D
)

./ stands 6 >. Set tc((r1 u . . . u rn ), R) := {(t1 u . . . u tn ) | ti v* R ri ti
TransR 1 n} define extended TBox TC,R
TC,R ={Xr1 u...urn ,D (r1 u . . . u rn , R).tr(D, R)| (r1 u . . . u rn ).D cl(C, R)}
4
{Xr1 u...urn ,D v (T, R).XT,D
| tc(r1 u . . . u rn , R)}
Lemma 27. Let C SHIQu -concept NNF, R role hierarchy, tr TC,R
defined Definition 26. concept C satisfiable w.r.t. R iff ALCQIb-concept
tr(C, R) satisfiable w.r.t. TC,R .

185

fiGlimm, Horrocks, Lutz, & Sattler

Given Lemma 25, proof Lemma 27 long, straightforward extension
proof given Tobies (2001, Lemma 6.23).
analyse complexity described problem. Let := |R|
r1 u . . . u rn longest role conjunction occurring C, i.e., maximal number roles
occur role conjunction C n. TBox TC,R contain exponentially
many axioms n since cardinality set tc((r1 u . . . u rn ), R) longest role
conjunction bounded mn ri one transitive
sub-role. hard check size axiom polynomial |C|. Since
deciding whether ALCQIb concept C satisfiable w.r.t. ALCQIb TBox
ExpTime-complete problem (even binary coding numbers) (Tobies, 2001, Thm.
p(n)
4.42), satisfiability SHIQu -concept C checked time 2p(m)2 .
extend translation concepts knowledge bases. Tobies assumes
role assertions ABox form r(a, b) r role name inverse
role name. Extended ABoxes contain, however, also negated roles role assertions,
require different translation. positive role assertion r(a, b) translated
standard way closing role upwards. difference using directly
additionally split conjunction ((r, R))(a, b) = (r1 u . . . u rn )(a, b) n different role
assertions r1 (a, b), . . . , rn (a, b), clearly justified semantics. negated roles
role assertion r(a, b), close role downwards instead upwards add
role atom s(a, b) sub-role r. justified semantics. Let
K = (T Tq , R, Aq ) extended knowledge base. precisely, set
tr(T Tq , R) := {tr(C, R) v tr(D, R) | C v Tq },
tr(A Aq , R) := {(tr(C, R))(a) | C(a) Aq }
{s(a, b) | r(a, b) Aq r v* R s}
{s(a, b) | r(a, b) Aq v
* R r},
use tr(K, R) denote ALCQIb knowledge base (tr(T Tq , R), tr(A Aq , R)).
complexity deciding consistency translated SHIQu knowledge base,
apply arguments concept satisfiability, gives following result:
Lemma 28. Given SHIQu knowledge base K = (T , R, A) := |K| size
longest role conjunction n, decide consistency K deterministic time
p(n)
2p(m)2
p polynomial.
ready show algorithm given Definition 22 runs deterministic
time single exponential size input KB double exponential size
input query.
Lemma 29. Let K = (T , R, A) SHIQ knowledge base = |K| q union
connected Boolean conjunctive queries n = |q|. Given K q input, algorithm given Definition 22 decides whether K |= q unique name assumption
p(n)
deterministic time 2p(m)2 .

186

fiConjunctive Query Answering DL SHIQ

proof lemma, show polynomial p
p(n)
extended knowledge bases consistency
check 2p(m)2
consistency check done time bound well.
precisely, let q = q1 . . .q` , = treesK (q1 ). . .treesK (q` ), G = groundK (q1 )
. . . groundK (q` ). Together Lemma 20, get ](T ) ](G) bounded
2p(n)log p(m) polynomial p size query G polynomial
n. 2p(n)log p(m) ground queries G contributes p(n) negated assertion
p(n)
extended ABox Aq . Hence, 2p(m)2
extended ABoxes Aq and,
p(n)
p(m)2
extended knowledge bases tested consistency.
therefore, 2
Given bounds cardinalities G fact size
query G polynomial n, hard check size extended
knowledge base Kq = (T Tq , R, Aq ) bounded 2p(n)log p(m) Kq
computed time bound well. Since extended parts contain role
conjunctions number roles role conjunction polynomial n,
polynomial p
1. |tr(T , R)| p(m),
2. |tr(Tq , R)| 2p(n)log p(m) ,
3. |tr(A, R)| p(m),
4. |tr(Aq , R)| 2p(n)log p(m) , and, hence,
5. |tr(Kq , R)| 2p(n)log p(m) .
p(n)

polynomial
Lemma 28, consistency check done time 2p(m)2
p(n)
p(m)2
p. Since check 2
extended knowledge bases consistency,
p(n)
check done time 2p(m)2 , obtain desired upper bound.
show result carries even restrict interpretations
unique name assumption.
Definition 30. Let K = (T , R, A) SHIQ knowledge base q SHIQ union
Boolean conjunctive queries. partition P Inds(A), knowledge base KP =
(T , R, AP ) query q P called A-partition w.r.t. K q AP q P
obtained q follows:
P P
1. Choose one individual name P .
2. b P , replace occurrence b q a.
4
Please note w.l.o.g. assume constants occur query occur
ABox well thus partition individual names ABox also
partitions query.
Lemma 31. Let K = (T , R, A) SHIQ knowledge base q union Boolean
conjunctive queries. K 6|= q without making unique name assumption iff
A-partition KP = (T , R, AP ) q P w.r.t. K q KP 6|= q P unique
name assumption.
187

fiGlimm, Horrocks, Lutz, & Sattler

Let K = (T , R, A) knowledge base Description Logic DL, C complexity
class deciding whether K |= q unique name assumption C, let
n = 2|A| . Since number partitions ABox exponential number
individual names occur ABox, following straightforward consequence
lemma: Boolean conjunctive DL query q, deciding whether K |= q without
making unique name assumption reduced deciding n times problem C.
order extend algorithm unions possibly unconnected Boolean conjunctive
queries, first transform input query q conjunctive normal form (CNF).
check entailment conjunct qi , union connected Boolean
conjunctive queries. algorithm returns K entails q entailment check succeeds
answers K entail q otherwise. Lemma 5 Lemma 23, algorithm
correct.
Let K knowledge base Description Logic DL, q union connected Boolean
conjunctive DL queries, C complexity class deciding whether K |= q
C. Let q 0 union possibly unconnected Boolean conjunctive queries cnf(q 0 )
CNF q 0 . Since number conjuncts cnf(q 0 ) exponential |q 0 |, deciding
0
whether K |= q 0 reduced deciding n times problem C, n = 2p(|q |) p
polynomial.
observation together results Lemma 29 gives following
general result:
Theorem 32. Let K = (T , R, A) SHIQ knowledge base = |K| q union
Boolean conjunctive queries n = |q|. Deciding whether K |= q done
p(n)
deterministic time 2p(m)2 .
corresponding lower bound follows work Lutz (2007). Hence
result tight. result improves known co-3NExpTime upper bound setting
roles query restricted simple ones (Ortiz, Calvanese, & Eiter, 2006a).
Corollary 33. Let K SHIQ knowledge base = |K| q union Boolean
conjunctive queries n = |q|. Deciding whether K |= q 2 ExpTime-complete problem.
Regarding query answering, refer back end Section 2.2, explain
deciding tuples belong set answers checked mkA
entailment tests, k number answer variables query
number individual names Inds(A). Hence, least theoretically, absorbed
combined complexity query entailment SHIQ.
6.2 Non-Deterministic Decision Procedure Query Entailment SHIQ
order study data complexity query entailment, devise non-deterministic
decision procedure provides tight bound complexity problem. Actually,
devised algorithm decides non-entailment queries: guess extended knowledge
base Kq , check whether consistent, return K entail q check succeeds
K entails q otherwise.
Definition 34. Let SHIQ TBox, R SHIQ role hierarchy, q union
Boolean conjunctive queries. Given SHIQ ABox input, algorithm guesses
188

fiConjunctive Query Answering DL SHIQ

A-partition KP = (T , R, AP ) q P w.r.t. K = (T , R, A) q. query q P
transformed CNF one resulting conjuncts, say qiP , chosen. algorithm
P
P
guesses extended knowledge base KqPi = (T Tqi , R, AP AP
qi ) w.r.t. K qi
P
returns K entail q Kqi consistent returns K entails q otherwise.
4
Compared deterministic version algorithm given Definition 22,
make UNA guess partition individual names. also non-deterministically
choose one conjuncts result transformation CNF. conjunct,
guess extended ABox check whether extended knowledge base guessed
ABox consistent and, therefore, counter-model query entailment.
(equivalent) negated form, Lemma 23 says K 6|= q iff extended
knowledge base Kq w.r.t. K q Kq consistent. Together Lemma 31
follows, therefore, algorithm Definition 34 correct.
6.2.1 Data Complexity Query Entailment SHIQ
analyze data complexity algorithm given Definition 34 show
deciding UCQ entailment SHIQ indeed co-NP data complexity.
Theorem 35. Let SHIQ TBox, R SHIQ role hierarchy, q union
Boolean conjunctive queries. Given SHIQ ABox = |A|, algorithm
Definition 34 decides non-deterministic polynomial time whether K 6|= q K =
(T , R, A).
Clearly, size ABox AP A-partition bounded . Since query
longer input, size constant transformation CNF done
constant time. non-deterministically choose one resulting conjuncts. Let
conjunct qi = q(i,1) . . . q(i,`) . established Lemma 32, maximal size
P
P
extended ABox AP
qi polynomial . Hence, |A Aqi | p(ma ) polynomial
p. Due Lemma 20 since size q, , R fixed assumption, sets
treesKP (q(i,j) ) groundKP (q(i,j) ) j 1 j ` computed time
polynomial . Lemma 29, know translation extended knowledge
base ALCQIb knowledge base polynomial close inspection
algorithm Tobies (2001) deciding consistency ALCQIb knowledge base shows
runtime also polynomial .
bound given Theorem 35 tight since data complexity conjunctive query
entailment already co-NP-hard ALE fragment SHIQ (Schaerf, 1993).
Corollary 36. Conjunctive query entailment SHIQ data complete co-NP.
Due correspondence query containment query answering (Calvanese
et al., 1998a), algorithm also used decide containment two unions
conjunctive queries SHIQ knowledge base, gives following result:
Corollary 37. Given SHIQ knowledge base K two unions conjunctive queries q
q 0 , problem whether K |= q q 0 decidable.

189

fiGlimm, Horrocks, Lutz, & Sattler

using result Rosati (2006a, Thm. 11), show consistency
SHIQ knowledge base extended (weakly-safe) Datalog rules decidable.
Corollary 38. consistency SHIQ+log-KBs (both FOL semantics
NM semantics) decidable.

7. Conclusions
decision procedure presented entailment unions conjunctive queries
SHIQ, close long standing open problem. solution immediate consequences
related areas, shows several open problems query answering,
query containment extension knowledge base weakly safe Datalog rules
SHIQ decidable well. Regarding combined complexity, present deterministic
algorithm needs time single exponential size KB double exponential
size query, gives tight upper bound problem. result
shows deciding conjunctive query entailment strictly harder instance checking
SHIQ. prove co-NP-completeness data complexity. Interestingly,
shows regarding data complexity deciding UCQ entailment (at least theoretically)
harder instance checking SHIQ, also previously open question.
part future work extend procedure SHOIQ,
DL underlying OWL DL. also attempt find implementable algorithms
query answering SHIQ. Carrying query rewriting steps goal directed
way crucial achieving this.

Acknowledgments
work supported EU funded IST-2005-7603 FET Project Thinking Ontologies (TONES). Birte Glimm supported EPSRC studentship.

190

fiConjunctive Query Answering DL SHIQ

Appendix A. Complete Proofs
Lemma (7). Let K SHIQ knowledge base q = q1 . . . qn union conjunctive
queries, K 6|= q iff exists canonical model K 6|= q.

Proof Lemma 7. direction trivial.
direction, since inconsistent knowledge base entails every query,
0
0
assume K consistent. Hence, interpretation 0 = (I , )
0 |= K 0 6|= q. 0 , construct canonical model K forest base J
0
follows: define set P (I ) paths smallest set
0

Inds(A), aI path;
d1 dn path,
d1 dn path,
0

(dn , d) rI role r,
0

Inds(A) = aI , n > 2.
path p = d1 dn , length len(p) p n. fix set Inds(A)
bijection f : P
(i) Inds(A) {} S,
(ii) Inds(A), {w | (a, w) S} tree,
0

(iii) f ((a, )) = aI ,
(iv) (a, w), (a, w0 ) w0 successor w, f ((a, w0 )) = f ((a, w))
0
.
(a, w) S, set Tail((a, w)) := dn f ((a, w)) = d1 dn . Now, define forest base
J = (J ,J ) K follows:
(a) J := S;
(b) Inds(A), aJ := (a, ) S;
(c) b NI \ Inds(A), bJ = aJ fixed Inds(A);
0

(d) C NC , (a, w) C J (a, w) Tail((a, w)) C ;
(e) roles r, ((a, w), (b, w0 )) rJ either
0

0

0

(I) w = w0 = (aI , bI ) rI
0

(II) = b, w0 neighbor w (Tail((a, w)), Tail((b, w0 ))) rI .
191

fiGlimm, Horrocks, Lutz, & Sattler

clear J forest base K due definition construction
J S.
Let = (I ,I ) interpretation identical J except that, non-simple
roles r, set
[
(sJ )+
rI = rJ
v* R r, sTransR

tedious hard verify |= K J forest base I. Hence
canonical model K.
Therefore, show 6|= q. Assume contrary |= q.
1 n |= qi . define mapping
0
0 : Terms(qi ) setting 0 (t) := Tail((t)) Terms(qi ). difficult
0
0
check 0 |= qi hence 0 |= q, contradiction.
Lemma (15). Let model K.
1. |= q, collapsing qco q |=co qco co injection
*
modulo
.
2. |=co qco collapsing qco q, |= q.
Proof Lemma 15. (1), let |= q, let qco collapsing q
obtained adding atom t0 terms t, t0 Terms(q) (t) = (t0 ).
*
definition semantics, |= qco injection modulo
.

Condition (2) trivially holds since q qco hence |= co q.
Lemma (16). Let model K.
1. canonical |= q, pair (qsr , R) srK (q) split match
sr |=sr qsr , R induced root splitting sr , sr injection
*
modulo
.
2. (qsr , R) srK (q) |=sr qsr match sr , |= q.

Proof Lemma 16. proof second claim relatively straightforward: since
(qsr , R) srK (q), collapsing qco q qsr split rewriting qco .
Since roles replaced split rewriting non-simple |= qsr assumption,
|= qco . Lemma 15 (2), |= q required.
go proof first claim detail: let qco co(q)
|=co qco match co injective modulo * . collapsing qco
match co exist due Lemma 15. co split match w.r.t. q already,
done, since split match induces root splitting R (qco , R) trivially srK (q).
qco
co split match, least two terms t, t0 r(t, t0 )
co (t) = (a, w), co (t0 ) = (a0 , w0 ), 6= a0 , w 6= w0 6= . distinguish two cases:
192

fiConjunctive Query Answering DL SHIQ

1. t0 mapped roots, i.e., w 6= w0 6= . Since |=co r(t, t0 ),
(co (t), co (t0 )) rI . Since canonical model K, must
role v* R r TransR
{(co (t), (a, )), ((a, ), (a0 , )), ((a0 , ), co (t0 ))} sI .
Terms(qco ) co (t) = (a, ), let u = t, otherwise let u
fresh variable. Similarly, t0 Terms(qco ) co (t0 ) = (a0 , ),
let u0 = t0 , otherwise let u0 fresh variable. Hence, define split
rewriting qsr qco replacing r(t, t0 ) s(t, u), s(u, u0 ), s(u0 , t0 ).
define new mapping sr agrees co terms occur qco
maps u (a, ) u0 (a0 , ).
2. Either t0 mapped root. W.l.o.g., let t, i.e., (t) = (a, ). use
arguments above: since |=co r(t, t0 ), ((t), (t0 )) rI and,
since canonical model K, must role v* R r TransR
{((t), (a0 , )), ((a0 , ), (t0 ))} sI . Terms(qco )
co (t) = (a0 , ), let u = t, otherwise let u fresh variable. define
split rewriting qsr qco replacing r(t, t0 ) s(t, u), s(u, t0 )and mapping sr
agrees co terms occur qco maps u (a0 , ).
immediately follows |=sr qsr . proceed described role
atom r(t, t0 ) (t) = (a, w) (t0 ) = (a0 , w0 ) 6= a0 w 6= w0 6=
. result split rewriting qsr split match sr |=sr qsr .
Furthermore, sr injective modulo * since introduce new variables,
variable mapped element yet range match. Since sr
split match, induces root splitting R and, hence, (qsr , R) srK (q) required.
Lemma (17). Let model K.
1. canonical |= q, pair (q`r , R) lrK (q) mapping `r
*
|=`r q`r , `r injection modulo
, R root splitting induced

`r and, r(t, t) q`r , R.
2. (q`r , R) lrK (q) |=`r q`r match `r , |= q.
Proof Lemma 17. proof (2) analogous one given Lemma 16 since,
definition loop rewritings, roles replaced loop rewriting non-simple.
(1), let (qsr , R) srK (q) |=sr qsr , sr split match, R
root splitting induced sr . split rewriting qsr match sr exist due
Lemma 16 canonicity I.
qsr
Let r(t, t)
/ R. Since R root splitting induced sr since

/ R, sr (t) = (a, w) Inds(A) w 6= . Now, let J forest base
I. show exists neighbor sr (t) role TransR v
* Rr
(sr (t), d) sI Inv(s)I . Since |=sr qsr , (sr (t), sr (t)) rI . Since J
forest base since w 6= , (sr (t), sr (t))
/ rJ . follows

sequence d1 , . . . , dn role TransR v
* R r, d1 = sr (t) = dn ,
193

fiGlimm, Horrocks, Lutz, & Sattler

(di , di+1 ) sJ 1 < n di 6= d1 1 < < n. hard
see that, {w0 | (a, w0 ) } tree w 6= , d2 = dn1 . Since
(d1 , d2 ) sJ (dn1 , dn ) sJ dn1 = d2 dn = d1 , role element
qsr
= d2 required. r(t, t)
/ R, select element dr,t role
sr,t described above. let q`r obtained qsr following
qsr
r(t, t)
/ R:
dr,t = sr (t0 ) t0 Terms(qsr ), replace r(t, t) sr,t (t, t0 ) sr,t (t0 , t);
otherwise, introduce new variable vr,t NV replace r(t, t) sr,t (t, vr,t )
sr,t (vr,t , t).
Let `r obtained sr extending `r (vr,t ) = dr,t newly introduced
variable vr,t . definition q`r `r , q`r connected, `r injective modulo * ,
|=`r q`r .
Lemma (18). Let model K.
1. canonical |= q, pair (qf r , R) frK (q) |=f r qf r
forest match f r , R induced root splitting f r , f r injection
*
modulo
.
2. (qf r , R) frK (q) |=f r qf r match f r , |= q.
Proof Lemma 18. proof (2) analogous one given Lemma 16.
(1), let (q`r , R) lrK (q) |=`r q`r , R root splitting induced `r , `r
q`r , R. loop rewriting match `r
injective modulo * and, r(t, t)
exist due Lemma 17 canonicity I. definition, R root splitting w.r.t.
q`r K.
w, w0 , longest common prefix (LCP) w, w0 longest w
w prefix w w0 . match `r define set follows:
:= ran(`r ) {(a, w) | w LCP w, w0
(a, w0 ), (a, w00 ) ran(`r )}.
Let V NV \ Vars(q`r ) that, \ ran(`r ), unique vd V .
define mapping f r `r {vd V 7 d}. definition V vd , f r
split match well. set V Vars(q`r ) set variables new query qf r .
Note ran(f r ) = D.
Fact (a) (a, w), (a, w0 ) ran(f r ), (a, w00 ) ran(f r ), w00 LCP w
w0 ;
Fact (b) ](V ) ](Vars(q`r )) (Because, worst case, (a, w) ran(`r ) incomparable thus seen leaves binarily branching tree. Now, tree
n leaves least binarily branching every non-leaf n inner
nodes, thus ](V ) ](Vars(q`r )).
194

fiConjunctive Query Answering DL SHIQ

pair individuals d, d0 , path d0 (unique) shortest sequence
elements d1 , . . . , dn d1 = d, dn = d0 , di+1 neighbor di
1 < n. length path number elements it, i.e., path d1 , . . . , dn
length n. relevant path d01 , . . . , d0` d0 sub-sequence d1 , . . . , dn
obtained dropping elements di
/ D.
subq(q`r , tr ) tr R let d01 , . . . , d0` relevant path
Claim 1. Let r(t, t0 )
= d01 = `r (t) d0 = d0` = `r (t0 ). ` > 2, role TransR
v* R r (d0i , d0i+1 ) sI 1 < `.
Proof. Let d1 , . . . , dn path d01 , . . . , d0` relevant path `r (t) `r (t0 ).
` > 2 implies n > 2. show role claim. Let J
forest base I. Since |=`r q`r , n > 2 implies (`r (t), `r (t0 )) rI \ rJ . Since
based J , follows TransR v* R r, (di , di+1 ) sJ
1 < n. construction J , follows (d0i , d0i+1 ) sI 1 < `,
finishes proof claim.
subq(q`r , tr )
let qf r obtained q`r follows: role atom r(t, t)
0
0
0
tr R, length relevant path d1 , . . . , d` = d1 = `r (t) d0 = d0` = `r (t0 )
greater 2, select role variables tj f r (tj ) = d0j
Claim 1 replace atom r(t, t0 ) s(t1 , t2 ), . . . , s(t`1 , t` ), = t1 , t0 = t` .
Please note tj chosen dont care non-deterministic way since f r
injective modulo * , i.e., f r (tj ) = dj = f r (t0j ), tj * t0j pick these.
show
(i) |=f r qf r ,
(ii) f r forest match.
q`r \ qf r let s(t1 , t2 ), . . . , s(t`1 , t` ) atoms replaced
(i), let r(t, t0 )
0

`r
r(t, ). Since |= q`r , |=`r r(t, t0 ) (`r (t), `r (t0 )) rI . Since r(t, t0 ) replaced
qf r , length relevant path `r (t) `r (t0 ) greater 2. Hence, must
case (`r (t), `r (t0 )) rI \ rJ . Let d1 , . . . , dn d1 = `r (t) dn = `r (t0 )
path `r (t) `r (t0 ) d01 , . . . , d0` relevant path `r (t) `r (t0 ).
construction J , means role TransR v* R r
(di , di+1 ) sJ 1 < n. construction I, means (d0i , d0i+1 ) sI
1 < ` required. Hence |=f r s(ti , ti+1 ) < ` definition f r .
(ii): mapping f r differs `r newly introduced variables.
Furthermore, introduced new role atoms within sub-query subq(q`r , tr ) `r
split match assumption. Hence, f r trivially split match
show f r forest match. Since f r split match, tree tree.
Inds(A), let Ta := {w | (a, w) ran(f r )}. need construct mapping
f specified Definition 14, start root tr . Ta 6= , let tr Terms(q)
unique term f r (tr ) = (a, wr ) Terms(q)
f r (t) = (a, w) w proper prefix wr . term exists since f r split match
unique due Fact (a) above. Define trace sequence w = w1 wn Ta+

w1 = wr ;
195

fiGlimm, Horrocks, Lutz, & Sattler

1 < n, wi longest proper prefix wi+1 .
Since canonical, wi Ta IN. hard see = {w | w trace}
{} tree. trace w = w1 wn , let Tail(w) = wn . Define mapping f maps
term f r (t) = (a, w) Ta unique trace wt w = Tail(wt ). Let
r(t, t0 ) qf r f r (t), f r (t0 ) Ta . construction qf r , implies
length relevant path f r (t) f r (t0 ) exactly 2. Thus, f (t) f (t0 )
neighbors and, hence, f r forest match required.
Theorem (19). Let K SHIQ knowledge base, q Boolean conjunctive query,
{q1 , . . . , q` } = treesK (q) groundK (q). K |= q iff K |= q1 . . . q` .
Proof Theorem 19. direction: let us assume K |= q1 . . . q` . Hence,
model K, query qi 1 ` |= qi . distinguish
two cases: (i) qi treesK (q) (ii) qi groundK (q).
(i): qi form C(v) C query concept query qf r w.r.t.
v Vars(qf r ) (qf r , ) frK (q). Hence |= qi match , thus |= C(v).
Let = (v) C . Lemma 12, |= qf r and,
Lemma 18, |= q required.
(ii): since qi groundK (q), pair (qf r , R) frK (q) qi =
ground(qf r , R, ). show |=f r qf r match f r . Since |= q1 ,
match |=i qi . construct match f r . R,
qi contains concept atom C( (t)) C = con(subq(qf r , t), t) query concept
subq(qf r , t) w.r.t. t. Since |=i C( (t)) Lemma 12, match
|=t subq(qf r , t). define f r union , R. Please note
f r (t) = ( (t)). Since Inds(qf r ) R that, Inds(qf r ), (a) =
qf r
(t) = (t0 ) iff t* t0 , follows |=f r atom

f
r
contains terms root choice R hence |=
qf r required.
direction show that, K |= q, K |= q1 . . . q` , let
us assume K |= q. Lemma 7 negated form K |= q iff canonical
models K |= q. Hence, restrict attention canonical
models K. Lemma 18, |= K |= q implies pair (qf r , R) frK (q)
|=f r qf r forest match f r , R induced root splitting f r , f r
*
injection modulo
. distinguish two cases:
(i) R = , i.e., root splitting empty f r tree match,
(ii) R 6= , i.e., root splitting non-empty f r forest match tree
match.
(i): since (qf r , ) frK (q), v Terms(qf r ) C = con(qf r , v)
qi = C(v). Lemma 12 and, since |= qf r , element C .
Hence |= C(v) : v 7 required.
(ii): since R root splitting induced f r , R
Inds(A) f r (t) = (at , ). define mapping : R Inds(A)
follows: R, (t) = iff f r (t) = (at , ). definition ground(qf r , R, ),
qi = ground(qf r , R, ) groundK (q). Since |=f r qf r , |= subq(qf r , t) R.
196

fiConjunctive Query Answering DL SHIQ

Since qf r forest-shaped, subq(qf r , t) tree-shaped. Then, Lemma 12, |= qi0 ,
qi0 query obtained qf r replacing sub-query subq(qf r , t) C(t)
C query concept subq(qf r , t) w.r.t. t. definition forest match
f r , clear |= ground(qf r , R, ) required.
Lemma (20). Let q Boolean conjunctive query, K = (T , R, A) SHIQ knowledge
base, |q| := n |K| := m. polynomial p
1. ](co(q)) 2p(n) and, q 0 co(q), |q 0 | p(n),
2. ](srK (q)) 2p(n)log p(m) , and, q 0 srK (q), |q 0 | p(n),
3. ](lrK (q)) 2p(n)log p(m) , and, q 0 lrK (q), |q 0 | p(n),
4. ](frK (q)) 2p(n)log p(m) , and, q 0 frK (q), |q 0 | p(n),
5. ](treesK (q)) 2p(n)log p(m) , and, q 0 treesK (q), |q 0 | p(n),
6. ](groundK (q)) 2p(n)log p(m) , and, q 0 groundK (q), |q 0 | p(n).
Proof Lemma 20.
1. set co(q) contains queries obtained q adding n equality
atoms q. number collapsings corresponds, therefore, building equivalence classes terms q * . Hence, cardinality set co(q)
exponential n. Since add one equality atom pair terms,
size query q 0 co(q) n + n2 , |q 0 | is, therefore, polynomial n.
2. n role atoms, choose nothing, replace
atom two atoms, three atoms. every replacement, choose
introduce new variable re-use one existing variables. introduce new
variable every time, new query contains 3n terms. Since K contain
non-simple roles sub-role role used role atoms q,
roles choose replacing role atom. Overall, gives us
1 + m(3n) + m(3n)(3n) choices n role atoms query
and, therefore, number split rewritings query q 0 co(q) polynomial
exponential n. combination results (1), also shows
overall number split rewritings polynomial exponential n.
Since add two new role atoms existing role atoms, size
query q 0 srK (q) linear n.
3. n role atoms form r(t, t) query q 0 srK (q) could
give rise loop rewriting, non-simple sub-roles r K
used loop rewriting, introduce one new variable
role atom r(t, t). Therefore, query srK (q), number loop rewritings
polynomial exponential n. Combined results (2),
bound also holds cardinality lrK (q).
loop rewriting, one role atom replaced two role atoms, hence, size
query q 0 lrK (q) doubles.
197

fiGlimm, Horrocks, Lutz, & Sattler

4. use similar arguments order derive bound exponential
n polynomial number forest rewritings frK (q).
Since number role atoms introduce forest rewriting polynomial n, size query q 0 frK (q) quadratic n.
5. cardinality set treesK (q) clearly also polynomial exponential
n since query frK (q) contribute one query set treesK (q).
hard see size query q 0 treesK (q) polynomial n.
6. (1)-(4) above, number terms root splitting polynomial n
individual names occurring used mapping
terms individual names. Hence number different ground mappings
polynomial exponential n. number ground queries
single tuple (qf r , R) frK (q) contribute is, therefore, also polynomial
exponential n. Together bound number forest rewritings
(4), shows cardinality groundK (q) polynomial exponential
n. hard see size query q 0 groundK (q)
polynomial n.

Lemma (23). Let K SHIQ knowledge base q union connected Boolean
conjunctive queries. algorithm Definition 22 answers K entails q iff K |= q
unique name assumption.
Proof Lemma 23. if-direction: let q = q1 . . . q` . show contrapositive assume K 6|= q. assume K consistent since inconsistent
knowledge base trivially entails every query. Let model K 6|= q.
show also model extended knowledge base Kq = (T Tq , R, Aq ).
first show model Tq . end, let > v C Tq . C(v)
C = con(qf r , v) pair (qf r , ) frK (q1 ) . . . frK (q` ) v Vars(qf r ). Let
(qf r , ) frK (qi ). C 6= implies, Lemma 12, |= qf r and,
Lemma 18, |= qi and, hence, |= q, contradicting assumption. Thus |= > v C
and, thus, |= Tq .
Next, define extended ABox Aq that, q 0 G,
C(a) q 0 aI C , C(a) Aq ;
r(a, b) q 0 (aI , bI )
/ rI , r(a, b) Aq .
assume query q 0 = ground(qf r , R, ) groundK (q1 ). . .groundK (q` )
atom q 0 Aq . trivially |= q 0 . Let
(qf r , R) frK (qi ). Theorem 19, |= qi thus |= q, contradiction. Hence
Kq extended knowledge base |= Kq required.
if-direction, assume K |= q, algorithm answers K
entail q. Hence extended knowledge base Kq = (T Tq , R, Aq )
consistent, i.e., model |= Kq . Since Kq extension K,
198

fiConjunctive Query Answering DL SHIQ

|= K. Moreover, |= Tq hence, , C
C(v) treesK (q1 ) . . . treesK (q` ). Lemma 12, 6|= q 0
q 0 treesK (q1 ) . . . treesK (q` ) and, Lemma 18, 6|= qi 1 `.
definition extended knowledge bases, Aq contains assertion least one
atom query q 0 = ground(qf r , R, ) groundK (q1 ) . . . groundK (q` ). Hence
6|= q 0 q 0 groundK (q1 ) . . . groundK (q` ). Then, Theorem 19, 6|= q,
contradicts assumption.
Lemma (25). Let R role hierarchy, r1 , . . . , rn roles. every interpretation
|= R, holds ((r1 u . . . u rn , R))I = (r1 u . . . u rn )I .
Proof Lemma 25. proof straightforward extension Lemma 6.19 Tobies
(2001). definition, (r1 u . . . u rn , R) = (r1 , R) u . . . u (rn , R) and, definition semantics role conjunctions, ((r1 , R) u . . . u (rn , R))I =
(r1 , R)I . . . (rn , R)I . v* R r, {s0 | r v* R s0 } {s0 | v* R s0 } hence
(s, R)I (r, R)I . |= R, rI sI every r v* R s. Hence, (r, R)I = rI
((r1 u . . . u rn , R))I = ((r1 , R) u . . . u (rn , R))I = (r1 , R)I . . . (rn , R)I =
r1 . . . rn = (r1 u . . . u rn )I required.
Lemma (28). Given SHIQu knowledge base K = (T , R, A) := |K| size
longest role conjunction n, decide consistency K deterministic time
p(n)
2p(m)2
p polynomial.
Proof Lemma 28. first translate K ALCQIb knowledge base tr(K, R) =
(tr(T , R), tr(A, R)). Since longest role conjunction size n, cardinality
set tc(R, R) role conjunction R bounded mn . Hence, TBox tr(T , R)
contain exponentially many axioms n. hard check size axiom
polynomial m. Since deciding whether ALCQIb KB consistent ExpTimecomplete problem (even binary coding numbers) (Tobies, 2001, Theorem 4.42),
p(n)
consistency tr(K, R) checked time 2p(m)2 .
Lemma (29). Let K = (T , R, A) SHIQ knowledge base := |K| q
union connected Boolean conjunctive queries n := |q|. algorithm given
Definition 22 decides whether K |= q unique name assumption deterministic
p(n)
time 2p(m)2 .
Proof Lemma 29. first show polynomial p
p(n)
check 2p(m)2
extended knowledge bases consistency
p(n)
p(n)
consistency check done time 2p(m)2 , gives upper bound 2p(m)2
time needed deciding whether K |= q.
Let q := q1 . . . q` . Clearly, use n bound `, i.e., ` n. Moreover,
size query qi 1 ` bounded n. Together Lemma 20, get
](T ) ](G) bounded 2p(n)log p(m) polynomial p clear
sets computed time bound well. size query q 0 G w.r.t.
ABox polynomial n and, constructing Aq , add subset (negated)
199

fiGlimm, Horrocks, Lutz, & Sattler

p(n)

atoms q 0 G Aq . Hence, 2p(m)2
extended ABoxes Aq
p(n)
p(m)2
extended knowledge bases tested consistency.
and, therefore, 2
Due Lemma 20 (5), size query q 0 polynomial n. Computing
query concept Cq0 q 0 w.r.t. variable v Vars(q 0 ) done time polynomial
n. Thus TBox Tq computed time 2p(n)log p(m) . size extended
ABox Aq maximal add, 2p(n)log p(m) ground queries G, atoms
negated form. Since, Lemma 20 (6), size queries polynomial n,
size extended ABox Aq bounded 2p(n)log p(m) clear
compute extended ABox time bound well. Hence, size extended
KB Kq = (T Tq , R, Aq ) bounded 2p(n)log p(m) . Since role conjunctions occur
Tq Aq , size concept Tq Aq polynomial n, length
longest role conjunction also polynomial n.
translating extended knowledge base ALCQIb knowledge base,
number axioms resulting concept C occurs Tq Aq exponential
n. Thus, size extended knowledge base bounded 2p(n)log p(m) .
Since deciding whether ALCQIb knowledge base consistent ExpTimecomplete problem (even binary coding numbers) (Tobies, 2001, Theorem 4.42),
p(n)
checked time 2p(m)2
K consistent not.
p(n)
Since check 2p(m)2
knowledge bases consistency,
p(n)
p(n)
p(m)2
check done time 2
, obtain desired upper bound 2p(m)2

deciding whether K |= q.
Lemma (31). Let K = (T , R, A) SHIQ knowledge base q union Boolean
conjunctive queries. K 6|= q without making unique name assumption iff
A-partition KP = (T , R, AP ) q P w.r.t. K q KP 6|= q P unique
name assumption.
Proof Lemma 31. if-direction: Since K 6|= q, model K
6|= q. Let f : Inds(A) Inds(A) total function that, set
individual names {a1 , . . . , } a1 = ai 1 n, f (ai ) = a1 . Let AP
q P obtained q replacing individual name q f (a).
P
Clearly, KP = (T , R, AP ) q P A-partition w.r.t. K q. Let P = (I , )
interpretation obtained restricting individual names Inds(AP ).
easy see P |= KP unique name assumption holds P .
0
show P 6|= q P . Assume, contrary shown, P |= q P
match 0 . define mapping : Terms(q) 0 (a) = 0 (f (a))
individual name Inds(q) (v) = 0 (v) variable v Vars(q). easy
see |= q, contradiction.
P
if-direction: Let P = (I , ) P |= KP UNA
P 6|= q P let f : Inds(A) Inds(AP ) total function f (a) individual
replaced AP q P . Let = (I ,I ) interpretation extends P
P
aI = f (a)I . show |= K 6|= q. clear |= . Let
C(a) assertion replaced aP AP . Since P |= C(aP )
P

aI = f (a)I = aP

IP

P

C , |= C(a). use similar argument (possibly
200

fiConjunctive Query Answering DL SHIQ

.
negated) role assertions. Let =
6 b assertion replaced aP
P
.
b bP AP , i.e., f (a) = aP f (b) = bP . Since P |= aP =
6 bP , aI = f (a)I =
P
IP
IP
.
aP 6= bP = f (b)I = bI |= =
6 b required. Therefore, |= K
required.
Assume |= q match . Let P : Terms(q P ) mapping
P
(v) = (v) v Vars(q P ) P (aP ) = (a) aP Inds(q P )
aP = f (a). Let C(aP ) q P C(a) q replaced aP , i.e., f (a) =
P

IP

= P (aP ) C
aP . assumption, (a) C , (a) = aI = f (a)I = aP
P
P
|= C(a ). Similar arguments used show entailment role equality
atoms, yields desired contradiction.
P

Theorem (35). Let K = (T , R, A) SHIQ knowledge base := |K| q :=
q1 . . . q` union Boolean conjunctive queries n := |q|. algorithm given
Definition 34 decides non-deterministic time p(ma ) whether K 6|= q := |A| p
polynomial.
Proof Theorem 35. Clearly, size ABox AP A-partition bounded .
established Lemma 32, maximal size extended ABox AP
q polynomial
. Hence, |AP AP
|

p(m
)


polynomial
p.
Due

Lemma
20 since

q
size q, , R fixed assumption, sets treesKP (qi ) groundKP (qi )
1 ` computed time polynomial . Lemma 29,
know translation extended knowledge base ALCQIb knowledge base
polynomial close inspection algorithm Tobies (2001) deciding
consistency ALCQIb knowledge base shows runtime also polynomial
.

References
Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2003). Description Logic Handbook. Cambridge University Press.
Bechhofer, S., van Harmelen, F., Hendler, J., Horrocks, I., McGuinness, D. L., PatelSchneider, P. F., & Stein, L. A. (2004). OWL web ontology language reference. Tech.
rep., World Wide Web Consortium. http://www.w3.org/TR/2004/REC-owl-ref-20040210/.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2006). Data
complexity query answering description logics. Doherty, P., Mylopoulos, J., &
Welty, C. A. (Eds.), Proceedings 10th International Conference Principles
Knowledge Representation Reasoning (KR 2006), pp. 260270. AAAI Press/The
MIT Press.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
reasoning efficient query answering description logics: dl-lite family. Journal Automated Reasoning, 39 (3), 385429.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (1998a). decidability query
containment constraints. Proceedings 17th ACM SIGACT-SIGMOD201

fiGlimm, Horrocks, Lutz, & Sattler

SIGART Symposium Principles Database Systems (PODS 1998), pp. 149158.
ACM Press Addison Wesley.
Calvanese, D., De Giacomo, G., Lenzerini, M., Nardi, D., & Rosati, R. (1998b). Description
logic framework information integration. Proceedings 6th International
Conference Principles Knowledge Representation Reasoning (KR 1998).
Calvanese, D., Eiter, T., & Ortiz, M. (2007). Answering regular path queries expressive description logics: automata-theoretic approach. Proceedings 22th
National Conference Artificial Intelligence (AAAI 2007).
Chekuri, C., & Rajaraman, A. (1997). Conjunctive query containment revisited. Proceedings 6th International Conference Database Theory (ICDT 1997), pp.
5670, London, UK. Springer-Verlag.
Glimm, B., Horrocks, I., & Sattler, U. (2006). Conjunctive query answering description
logics transitive roles. Proceedings 19th International Workshop
Description Logics (DL 2006). http://www.cs.man.ac.uk/~glimmbx/download/GlHS06a.pdf.
Gradel, E. (2001). modal logics robustly decidable?. Paun, G., Rozenberg,
G., & Salomaa, A. (Eds.), Current Trends Theoretical Computer Science, Entering
21th Century, Vol. 2, pp. 393408. World Scientific.
Grahne, G. (1991). Problem Incomplete Information Relational Databases. SpringerVerlag.
Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). SHIQ RDF
OWL: making web ontology language. Journal Web Semantics, 1 (1),
726.
Horrocks, I., Sattler, U., Tessaris, S., & Tobies, S. (1999). Query containment using DLR ABox. Ltcs-report LTCS-99-15, LuFG Theoretical Computer Science,
RWTH Aachen, Germany. Available online http://www-lti.informatik.rwth-aachen.
de/Forschung/Reports.html.
Horrocks, I., Sattler, U., & Tobies, S. (2000). Reasoning Individuals Description
Logic SHIQ. McAllester, D. (Ed.), Proceedings 17th International Conference Automated Deduction (CADE 2000), No. 1831 Lecture Notes Artificial
Intelligence, pp. 482496. Springer-Verlag.
Horrocks, I., & Tessaris, S. (2000). conjunctive query language description logic aboxes.
Proceedings 17th National Conference Artificial Intelligence (AAAI 2000),
pp. 399404.
Hustadt, U., Motik, B., & Sattler, U. (2005). Data complexity reasoning expressive
description logics. Proceedings International Joint Conference Artificial
Intelligence (IJCAI 2005), pp. 466471.
Levy, A. Y., & Rousset, M.-C. (1998). Combining horn rules description logics
CARIN. Artificial Intelligence, 104 (12), 165209.
Lutz, C. (2007). Inverse roles make conjunctive queries hard. Proceedings 20th
International Workshop Description Logics (DL 2007).
202

fiConjunctive Query Answering DL SHIQ

McGuinness, D. L., & Wright, J. R. (1998). industrial strength description logic-based
configuration platform. IEEE Intelligent Systems, 13 (4).
Motik, B., Sattler, U., & Studer, R. (2004). Query answering OWL-DL rules.
Proceedings 3rd International Semantic Web Conference (ISWC 2004), Hiroshima, Japan.
Ortiz, M., Calvanese, D., & Eiter, T. (2006a). Data complexity answering unions
conjunctive queries SHIQ. Proceedings 19th International Workshop
Description Logics (DL 2006).
Ortiz, M. M., Calvanese, D., & Eiter, T. (2006b). Characterizing data complexity
conjunctive query answering expressive description logics. Proceedings
21th National Conference Artificial Intelligence (AAAI 2006).
Rosati, R. (2006a). DL+log: Tight integration description logics disjunctive datalog. Proceedings Tenth International Conference Principles Knowledge
Representation Reasoning (KR 2006), pp. 6878.
Rosati, R. (2006b). ddecidability finite controllability query processing
databases incomplete information. Proceedings 25th ACM SIGACT
SIGMOD Symposium Principles Database Systems (PODS-06), pp. 356365.
ACM Press Addison Wesley.
Rosati, R. (2007a). limits querying ontologies. Proceedings Eleventh
International Conference Database Theory (ICDT 2007), Vol. 4353 Lecture Notes
Computer Science, pp. 164178. Springer-Verlag.
Rosati, R. (2007b). conjunctive query answering EL. Proceedings 2007
Description Logic Workshop (DL 2007). CEUR Workshop Proceedings.
Schaerf, A. (1993). complexity instance checking problem concept languages
existential quantification. Journal Intelligent Information Systems, 2 (3), 265
278.
Sirin, E., & Parsia, B. (2006). Optimizations answering conjunctive abox queries.
Proceedings 19th International Workshop Description Logics (DL 2006).
Sirin, E., Parsia, B., Cuenca Grau, B., Kalyanpur, A., & Katz, Y. (2006). Pellet: practical
OWL-DL reasoner. Accepted Journal Web Semantics, Available online
http://www.mindswap.org/papers/PelletJWS.pdf.
Tessaris, S. (2001). Questions answers: reasoning querying Description Logic.
PhD thesis, University Manchester.
Tobies, S. (2001). Complexity Results Practical Algorithms Logics Knowledge
Representation. PhD thesis, RWTH Aachen.
Tsarkov, D., & Horrocks, I. (2006). FaCT++ description logic reasoner: System description.
Furbach, U., & Shankar, N. (Eds.), Proceedings Third International Joint
Conference Automated Reasoning (IJCAR 2006), Vol. 4130 Lecture Notes
Computer Science, pp. 292 297. Springer-Verlag.
203

fiGlimm, Horrocks, Lutz, & Sattler

van der Meyden, R. (1998). Logical approaches incomplete information: survey.
Logics Databases Information Systems, pp. 307356. Kluwer Academic Publishers.
Vardi, M. Y. (1997). modal logic robustly decidable?. Descriptive Complexity
Finite Models: Proceedings DIMACS Workshop, Vol. 31 DIMACS: Series
Discrete Mathematics Theoretical Computer Science, pp. 149184. American
Mathematical Society.
Wessel, M., & Moller, R. (2005). high performance semantic web query answering engine.
Proceedings 18th International Workshop Description Logics.
Wolstencroft, K., Brass, A., Horrocks, I., Lord, P., Sattler, U., Turi, D., & Stevens, R.
(2005). Little Semantic Web Goes Long Way Biology. Proceedings
2005 International Semantic Web Conference (ISWC 2005).

204

fiJournal Artificial Intelligence Research 31 (2008) 353-398

Submitted 09/07; published 02/08

Gesture Salience Hidden Variable Coreference
Resolution Keyframe Extraction
Jacob Eisenstein
Regina Barzilay
Randall Davis

jacobe@csail.mit.edu
regina@csail.mit.edu
davis@csail.mit.edu

Computer Science Artificial Intelligence Laboratory
Massachusetts Institute Technology
77 Massachusetts Avenue
Cambridge, 02139 USA

Abstract
Gesture non-verbal modality contribute crucial information understanding natural language. gestures informative, non-communicative
hand motions may confuse natural language processing (NLP) impede learning. People little difficulty ignoring irrelevant hand movements focusing meaningful
gestures, suggesting automatic system could also trained perform task.
However, informativeness gesture context-dependent labeling enough data
cover cases would expensive. present conditional modality fusion, conditional
hidden-variable model learns predict gestures salient coreference resolution, task determining whether two noun phrases refer semantic
entity. Moreover, approach uses coreference annotations, annotations
gesture salience itself. show gesture features improve performance coreference
resolution, attending gestures salient, method achieves
significant gains. addition, show model gesture salience learned
context coreference accords human intuition, demonstrating gestures
judged salient model used successfully create multimedia keyframe
summaries video. summaries similar created human raters,
significantly outperform summaries produced baselines literature.1

1. Introduction
Gesture nearly ubiquitous feature face-to-face natural language communication
may used supplement speech additional information reinforce meaning
already conveyed (McNeill, 1992). either case, gesture increase robustness
natural language processing (NLP) systems inevitable disfluency spontaneous
1. article extension unification two conference publications (Eisenstein, Barzilay, & Davis,
2007; Eisenstein & Davis, 2007). extends prior published work several new, unpublished results:
stability analysis respect initialization weights (Section 4.3); analysis verbal features terms
centering theory (Section 5.1); interrater agreement analysis coreference annotations (Section 6);
evaluation coreference using global metric (Section 6.2); expanded empirical evaluation
coreference task additional fusion models (Section 6.2); analysis different types gesture features
multimodal coreference resolution (Section 6.4.1); study interaction gestural
verbal features (Section 6.4.2); interrater agreement keyframe extraction (Section 7.3). Source
code data available http : //mug.csail.mit.edu/publications/2008/Eisenstein JAIR/

c
2008
AI Access Foundation. rights reserved.

fiEisenstein, Barzilay, & Davis

top one clears area here, goes
way top...

moves up. everything moves up.

1

2

Figure 1: excerpt explanatory narrative gesture helps disambiguate
meaning.

speech. example, consider following excerpt presentation
speaker describes mechanical device:
moves up, everything moves up. top one clears area
here, goes way top.

references passage difficult disambiguate, meaning becomes clearer
set context accompanying hand gestures (Figure 1).
Despite apparent benefits offered gestural cues, obtaining concrete gains natural language understanding difficult. key problem combine gesture
linguistic features. Existing systems typically address issue directly concatenating
low-level visual information (e.g., hand position speed) traditional textual features (Eisenstein & Davis, 2006), combining posteriors separately-trained
models (Chen, Harper, & Huang, 2006). appealing alternative consider inherent linguistic quality gesture, distinguished hand movements may
meaningful desired language understanding task (Goodwin & Goodwin, 1986).2
show better results obtained focusing hand movements likely
correspond relevant gestures.
move beyond low-level representation gesture, one could attempt develop
general-purpose taxonomy gestures based relation language. taxonomies
proved useful psychological linguistic research gesture, application
corpus-based statistical language processing immediately practical. Gesture
multifaceted phenomenon, key features understanding gestures meaning may
highly context-dependent (Lascarides & Stone, 2006). example, flexion
2. hand motions even absence hand motion may meaningful sense. However,
specific language processing problem, gestures directly relevant. remainder
article, terms meaningful meaningless always assumed framed within
context specific language processing task. Hand motions meaningless coreference
resolution may indeed quite useful another problem, sentiment classification.

354

fiGesture Salience Hidden Variable

single finger might crucial component one gesture irrelevant detail another
context. possible create formal annotation scheme expressive enough capture
details, yet compact enough tractable? topic ongoing research.
even possible, annotation would time-consuming, particularly
scale necessary corpus-based NLP.
paper propose middle path: model learns attend salient gestures
without explicit gesture annotation. Instead top-down approach attempts
analyze gestures according universal taxonomy, work bottom-up specific
language understanding problem: coreference resolution. speaker produces similar,
meaningful deictic3 gestures two noun phrases, good indication noun
phrases coreferent (Eisenstein & Davis, 2006). automatically identify gestures
relevant coreference resolution, among hand motions co-occur
noun phrases. approach shown enhance contribution low-level gesture
features towards coreference.
concretely, employ conditional model hidden variable governs
whether gesture features included determination coreference pair
noun phrases. model, possible learn gesture salience jointly coreference. baseline, demonstrate even low-level concatenative approach
gesture-speech fusion4 yields small statistically significant improvement coreference resolution, compared textual features alone. importantly, show
contribution gesture features increases substantially gesture speech
combined using structured model.
model gesture salience learn relevant coreference resolution, would useful engineering perspective. interesting question
whether estimates gesture salience related humans perceive multimodal communication. answer this, examine whether model gesture salience
relevant language processing tasks. demonstrate model learned
coreference resolution applied selection keyframes generating visual
summaries instructional presentations. Without explicit training keyframe
extraction task, approach selects keyframes cohere meaningfully chosen
human annotators.
main contributions paper summarized follows.
New applications gesture: demonstrate benefits incorporating gesture
two tasks: coreference resolution video keyframe extraction. coreference
task, substantially improve previous work showed gesture similarity help predict coreference resolution (Eisenstein & Davis, 2006); application
linguistic analysis gesture video keyframe extraction novel. previous
research, gesture information shown boost performance sentence segmentation, local syntactic phenomenon. work demonstrates gestures usefulness
non-local, discourse-level tasks. end, introduce novel set features
tightly combine linguistic gestural information.
3. Deictic gestures communicate meaning spatial location (McNeill, 1992).
4. use term speech indicate dealing spoken language, note hand
transcriptions rather automatic speech recognition (ASR) used throughout experiments.
applicability techniques context noisy ASR transcripts topic future work.

355

fiEisenstein, Barzilay, & Davis

Gesture salience language: develop idea gesture information
considered language processing gesture salient. prior research (e.g., Chen, Liu, Harper, & Shriberg, 2004), model uses low-level features
extracted directly vision-based hand tracker, avoiding need manual annotation gesture features. However, relevance low-level features depends
linguistic context. modeling relationship gesture salience,
obtain significant performance gains. addition, present set features designed
capture salience gesture associated speech.
Hidden-variable modeling gesture salience: develop framework gesture salience modeled jointly coreference resolution. show gesture salience expressed hidden variable learned without explicit labels,
leveraging coreference annotations. novel framework realized within conditional model, enabling use arbitrary possibly non-independent features.
experiments demonstrate estimates gesture salience obtained
model applied extract keyframes containing salient deictic gestures.
Section 2, consider prior work relating existing models taxonomies
gesture psychology literature, well previous efforts incorporate gesture
natural language understanding. Section 3, describe dataset
conduct experiments. Section 4, present model, conditional modality fusion.
show gesture salience treated hidden variable learned without
explicit annotations. Section 5 includes description textual gestural features
use experiments. Section 6 present experimental results showing
model improves performance coreference resolution. Section 7, show
estimates gesture salience general, applied select useful keyframes
video. Finally, Section 8 discuss implications research, conclude.

2. Related Work
section, describe four general areas related work provide background
context efforts. Section 2.1 discusses models gesture language
psychology linguistics communities. Section 2.2 describes projects employed
gesture natural language processing. Section 2.3 describes general modality fusion
techniques, particularly ones used incorporate prosodic features NLP.
Finally, Section 2.4 considers models machine learning literature related
conditional modality fusion.
2.1 Models Gesture Language
Psychology research explored problem modeling gesture relation language. discuss two frequently-cited models: Kendons taxonomy (1980), focuses
kinematic structure individual gestures; McNeills (1992), identifies
ways gesture communicates meaning within discourse.
According Kendon, gestures constructed set movement phases: prepare, stroke, hold, retract. prepare retract phases initiate terminate
gesture, respectively. stroke content-carrying part gesture, hold
356

fiGesture Salience Hidden Variable

pause may occur immediately stroke. phases provide
essentially kinematic description gesture. McNeill focuses way gestures communicate meaning, identifying four major types conversational gestures: deictic, iconic,
metaphoric, beat. Deictics communicate reference spatial locations, iconics
metaphorics create imagery using form gesture, beats communicate using
timing emphasis.5
taxonomies proved useful psychological linguistic research,
substantial effort would required create corpus annotations statistical
natural language processing. circumvent problem learning model gesture
directly automatically-extracted visual features. relationship gesture
language semantics learned context specific language phenomenon, using annotations verbal language semantics. approach may capture meaningful
hand gestures, capture a) relevant coreference resolution
b) identified using feature set.
2.2 Multimodal Natural Language Processing
Early computational work relationship language gesture focused identifying examples connections discourse elements automatically-recognized
properties hand gesture (Quek et al., 2000, 2002a). Quek et al. (2002a) show examples
similar gestures used connection repetitions associated discourse
elements. exploit idea using features quantify gesture similarity predict
noun phrase coreference. Follow-up papers attempt capture contribution individual gesture features, spatial location (Quek, McNeill, Bryll, & Harper, 2002b)
symmetric motion (Xiong & Quek, 2006), using similar methodology. line
work provides helpful framework understanding relationship gesture
natural language.
engineering side, several papers report work exploiting relationship gesture language. one line research, linguistic features used improve gesture processing (Poddar, Sethi, Ozyildiz, & Sharma, 1998; Kettebekov, Yeasin,
& Sharma, 2005). papers evaluate performance human-human language
domain weather broadcasts, stated goal developing techniques gesturebased human-computer interaction. authors note domain weather broadcasts, many hand motions well-described relatively small taxonomy gestures,
identify points, contours, regions. Lexical features ASR transcripts
shown improve gesture recognition (Poddar et al., 1998), prosodic features used
identify key gestural segments (Kettebekov et al., 2005).
Similarly, linguistic analysis shown important consequences gesture generation animated agents. Nakano, Reinstein, Stocky, Cassell (2003) present
empirical study human-human interaction, showing statistical relationship
hand-coded descriptions head gestures discourse labels associated utterances (e.g., acknowledgment, answer, assertion). demonstrated
findings encoded model generate realistic conversational grounding
5. McNeill notes types thought mutually exclusive bins, features
may present varying degrees.

357

fiEisenstein, Barzilay, & Davis

behavior animated agent. addition discourse-moderating function, gestures
also shown useful supplementing semantic content verbal explanations.
Kopp, Tepper, Ferriman, Cassell (2007) describe system animated agents give
navigation directions, using hand gestures describe physical properties landmarks
along route. research describes interesting relationships gesture
language exploited generation, focus recognition multimodal
communication.
research cited uses linguistic context supplement gesture generation
recognition, work used gesture features supplement natural language processing. Much research taken place context spoken language dialogue
systems incorporating pen gestures automatically recognized speech. early example system Quickset (Cohen et al., 1997), pen gestures speech
input used plan military missions. Working domain, Johnston Bangalore (2000) describe multimodal integration algorithm parses entire utterances
resolves ambiguity speech gesture modalities. Chai Qu (2005) present
alternative take similar problem, showing speech recognition improved
increasing salience entities targeted gestures. research projects differ
assume ontology possible referents known advance.
addition, gestures performed pen rather free hand, gesture
segmentation inferred contact pen sensing surface. Finally,
dialogue cases human-computer, rather human-human, language
usage probably differs.
research similar involves using gesture features improve language
processing spontaneous human-to-human discourse. Gesture shown improve task
sentence segmentation using automatically recognized features (Chen et al., 2004),
successfully manual gesture annotations (Chen et al., 2006). hidden Markov
model (HMM) used capture relation lexical tokens sentence boundaries. train maximum entropy model using feature vector posterior
probability estimates HMM set gesture features based KendonMcNeill taxonomies described above. earlier work, show gesture features
also improve coreference resolution; describe system classifier trained
coreference, using joint feature vector gesture textual features (Eisenstein & Davis,
2006). approaches, gesture speech combined unstructured way,
even irrelevant hand movements may influence classification decisions. approach
present paper includes gesture features likely relevant, substantially improving performance previous reported results (Eisenstein
& Davis, 2006).
2.3 Model Combination Techniques NLP
large literature integrating non-verbal features NLP, much relating
prosody. example, Shriberg, Stolcke, Hakkani-Tur, Tur (2000) explore use
prosodic features sentence topic segmentation. first modality combination
technique consider trains single classifier modalities combined
single feature vector; sometimes called early fusion. also consider training

358

fiGesture Salience Hidden Variable

separate classifiers combining posteriors, either weighted addition
multiplication; sometimes called late fusion (see also Liu, 2004). Experiments
multimodal fusion prosodic features find conclusive winner among early fusion,
additive late fusion, multiplicative late fusion (Shriberg et al., 2000; Kim, Schwarm, &
Osterdorf, 2004). techniques also employed gesture-speech fusion.
prior work, employed early fusion gesture-speech combination (Eisenstein & Davis,
2006); late fusion also applied gesture-speech combination (Chen et al., 2004,
2006).
Toyama Horvitz (2000) introduce Bayesian network approach modality combination speaker identification. late fusion, modality-specific classifiers trained
independently. However, Bayesian approach also learns predict reliability
modality given instance, incorporates information Bayes net.
flexible early late fusion, training modality-specific classifiers separately
still suboptimal compared training jointly, independent training
modality-specific classifiers forces account data cannot possibly explain. example, speakers gestures relevant language processing
task, counterproductive train gesture-modality classifier features
instant; lead overfitting poor generalization.
approach combines aspects early late fusion. early fusion, classifiers modalities trained jointly. Toyama Horvitzs Bayesian late
fusion model, modalities weighted based predictive power specific instances. addition, model trained maximize conditional likelihood, rather
joint likelihood.
2.4 Related Machine Learning Approaches
machine learning perspective, research relates three general areas: domain
adaptation, co-training, hidden-variable conditional models.
domain adaptation, one small amount in-domain data relevant
target classification task, large amount out-of-domain data related,
different task (Blitzer, McDonald, & Pereira, 2006; Chelba & Acero, 2006). goal
use out-of-domain data improve performance target domain. one recent
approach, feature replicated separate weights learned general
domain-specific applications feature (Daume III, 2007). sense, model learns
features relevant generally, relevant specific domains.
task somewhat similar, interested learning apply gesture
features, simultaneously learning predict coreference. However, one key
difference domain adaptation, data partitioned separate domains
advance, model must learn identify cases gesture salient.
Co-training another technique combining multiple datasets (Blum & Mitchell,
1998). co-training, small amount labeled data supplemented large amount
unlabeled data. Given sets features sufficient predict desired
label called views separate classifiers trained predictions one
classifier provide labeled data classifier. approach shown
yield better performance using labeled data applications,

359

fiEisenstein, Barzilay, & Davis

parsing (Sarkar, 2001). large amounts unlabeled data available, co-training could
applied here, using gesture verbal features independent views.
research, acquiring data greater bottleneck creating coreference annotations.
addition, previous attempts apply co-training textual coreference resolution proved
largely unsuccessful (Muller, Rapp, & Strube, 2002), possibly views
independently sufficient predict label. investigation topic
merited, approach make use unlabeled data; instead, treat gestures
salience hidden variable within existing dataset.
methodological standpoint, work closely related literature
hidden variables conditionally trained models. Quattoni, Collins, Darrell (2004)
improve object recognition use hidden variable indicating part
object contains localized visual feature. Part-based object recognition
previously performed generative framework, conditional approach permits
use broader feature set, without concern whether features mutually
independent. Subsequent work shown conditional hidden-variable models
used gesture recognition (Wang, Quattoni, Morency, Demirdjian, & Darrell, 2006)
language processing (Koo & Collins, 2005; Sutton, McCallum, & Rohanimanesh, 2007).
Wang et al. (2006) employ model similar HMM-based gesture recognition,
hidden variable encoding different phases gesture recognized; again,
conditional approach shown improve performance. Hidden variables applied
statistical parsing Koo Collins (2005), assigning lexical items word clusters
word senses. Finally, Sutton et al. (2007) use hidden variables encode intermediate
levels linguistic structure relevant overall language-processing task.
example, one application, hidden variables encode part-of-speech tags,
used noun phrase chunking. continue line work, extending hidden-variable
conditional models novel, linguistically-motivated hidden-variable architecture
gesture-speech combination.

3. Dataset
research described paper based corpus multimodal presentations.
existing corpora include visual data, none appropriate
research. Ami corpus (Carletta et al., 2005) includes video audio meetings,
participants usually seated hands often visible video.
Vace corpus (Chen et al., 2005) also contains recordings meetings, tracking beacons
attached speakers providing accurate tracking. corpus publicly
released time writing.
corpora address seated meeting scenarios; observed gesture
frequent speakers give standing presentations, classroom lectures business
presentations. many video recordings available, typically
filmed circumstances frustrate current techniques automatic extraction
visual features, including camera movement, non-static background, poor lighting,
occlusion speaker. Rather focusing substantial challenges computer
vision, chose gather new multimodal corpus.

360

fiGesture Salience Hidden Variable

Figure 2: example pre-printed diagram used gathering corpus. diagram
schematic depiction candy dispenser.

gathering corpus, aimed capture conversations gesture frequent direct, also natural unsolicited. sought middle ground
task-oriented dialogues Trains (Allen et al., 1995) completely open-ended
discussions Switchboard (Godfrey, Holliman, & McDaniel, 1992). work,
participants given specific topics discussion (usually function mechanical
devices), permitted converse without outside interference. speakers
given pre-printed diagrams aid explanations. interpretation gestures
condition usually relatively straightforward; many, gestures involve
pointing locations diagram. Visual aids printed projected diagrams
common important application areas, including business presentations, classroom
lectures, weather reports. Thus, restriction seem overly limiting
applicability work. leave presumably challenging problem understanding gestures produced without visual aids future work.
Figure 1 shows two still frames corpus, accompanying text.
visual aid shown detail Figure 2. corpus includes sixteen short videos
nine different speakers. total 1137 noun phrases transcribed; roughly
half number found MUC6 training set, text-only dataset also used
coreference resolution (Hirschman & Chinchor, 1998). Building multimodal corpus
time-consuming task requiring substantial manpower, hope initial work
lead larger future corpora well-suited study gesture natural language
processing. Corpus statistics found Appendix C, data available on-line
at: http : //mug.csail.mit.edu/publications/2008/Eisenstein JAIR/
Finally, draw readers attention differences corpus
commonly-used textual corpora coreference resolution, MUC (Hirschman & Chinchor, 1998). Topically, corpus focuses description mechanical devices, rather
news articles. Consequently, emphasis less disambiguating entities people
organizations, resolving references physical objects. corpora also
differ genre, corpus comprised spontaneous speech, MUC corpus
361

fiEisenstein, Barzilay, & Davis

includes edited text. genre distinctions known play important role patterns reference (Strube & Muller, 2003) language use generally (Biber, 1988). Four
different mechanical devices used topics discussion: piston, candy dispenser
(Figure 2), latch box (shown Appendix B), pinball machine.
3.1 Data Gathering Protocol
Fifteen pairs participants joined study responding posters university
campus; ages ranged 18-32, participants university students staff.
subset nine pairs participants selected basis recording quality,6
speech transcribed annotated. corpus composed two videos
nine pairs; audio recording problems forced us exclude two videos, yielding
16 annotated documents, two three minutes duration.
One participant randomly selected pair speaker,
listener. speakers role explain behavior mechanical device
listener. listeners role understand speakers explanations well enough
take quiz later. Prior discussion, speaker privately viewed simulation
operation relevant device.
speaker limited two minutes view video object three minutes
explain it; majority speakers used time allotted. suggests could
obtained natural data limiting explanation time. However, found
pilot studies led problematic ordering effects, participants devoted
long time early conditions, rushed later conditions. time
constraints, total running time experiment usually around 45 minutes.
data used study part larger dataset initially described Adler, Eisenstein,
Oltmans, Guttentag, Davis (2004).
3.2 Speech Processing
Speech recorded using headset microphones. integrated system controlled synchronization microphones video cameras. Speech transcribed manually,
audio hand-segmented well-separated chunks duration longer
twenty seconds. chunks force-aligned Sphinx-II speech recognition
system (Huang, Alleva, Hwang, & Rosenfeld, 1993).
wide range possibilities exist regarding fidelity richness transcribed
speech. Choices include transcription quality, existence punctuation capitalization,
presence sentence boundaries syntactic annotations. assume perfect transcription words sentence boundaries,7 additional punctuation. similar
much NLP research Switchboard corpus, (e.g., Kahn, Lease, Charniak,
Johnson, & Ostendorf, 2005; Li & Roth, 2001), although automatic speech recognition
(ASR) transcripts also used (e.g., Shriberg et al., 2000). Using ASR may accurately replicate situation application developer. However, approach
would also introduce certain arbitrariness, results would depend heavily amount
6. Difficulties microphones prevented us getting suitable audio recordings several cases;
cases difficulties synchronizing two microphones two video cameras.
7. Sentence boundaries annotated according NIST Rich Transcription Evaluation (NIST, 2003).

362

fiGesture Salience Hidden Variable

effort spent tuning recognizer. particular, recognizer well-tuned,
approach risks overstating relative contribution gesture features, verbal
features would little value.
natural language task coreference resolution requires noun phrase boundaries
preprocessing step, provide gold-standard noun phrase annotation. goal
isolate contribution model gesture-speech combination coreference task,
thus wish deliberately introduce noise noun phrase boundaries.
Gold standard noun phrase annotations used previous research coreference
resolution, (e.g., McCallum & Wellner, 2004; Haghighi & Klein, 2007).8 addition,
automatic noun phrase chunking possible high accuracy. F-measures exceeding
.94 reported textual corpora (Kudo & Matsumoto, 2001; Sha & Pereira, 2003);
transcripts Switchboard corpus, state-of-the-art performance exceeds .91 (Li &
Roth, 2001).9
annotation noun phrases followed MUC task definition markable NPs
(Hirschman & Chinchor, 1998). Personal pronouns annotated, discourse
focused descriptions mechanical devices. pronouns could easily filtered
automatically. Annotation attempted transcribe noun phrases. total 1137
markable NPs transcribed. roughly half size MUC6 training set,
includes 2072 markable NPs 30 documents. gold standard coreference
markable annotation performed first author, using audio video
information.
additional rater performed coreference annotations help assess validity. rater
native speaker English author paper. annotated two documents, comprising total 270 noun phrases. Using interrater agreement methodology
described Passonneau (1997), score .65 obtained Krippendorfs alpha.
comparable results MUC textual corpus (Passonneau, 1997),
higher agreement reported corpus multi-party spoken dialogues (Muller,
2007).
Finally, assume gold standard sentence boundaries, additional punctuation.
3.3 Vision Processing
Video recording performed using standard digital camcorders. Participants given
two different colored gloves facilitate hand tracking. Despite use colored gloves,
post-study questionnaire indicated one thirty participants guessed
study related gesture. study deliberately designed participants
little free time think; actually conducting dialogue, speaker
busy viewing next mechanical system, participant busy tested
previous conversation. also presented consent forms immediately gloves,
may diverted attention gloves purpose.
8. cited references include noun phrase unless participate coreference relations;
include noun phrases regardless.
9. high accuracy switchboard imply good performance data, since
annotated data noun phrase boundaries. Thus overall impact noisy preprocessing
coreference performance unknown. addition, possible noisy noun phrase boundaries may
pose particular problems approach, assesses gesture features duration NP.

363

fiEisenstein, Barzilay, & Davis

articulated upper-body tracker used model position speakers torso,
arms, hands. building complete upper-body tracker, rather simply tracking
individual hands, able directly model occlusion hands arms.
frame, annealed particle filter used search space body configurations.
Essentially, system performs randomized beam search simultaneously achieve three
objectives: a) maximize overlap model pixels judged
foreground, b) match known glove color color observed hypothesized hand
positions, c) respect physiological constraints temporal continuity. system
implemented using OpenCV library.10
tracker inspired largely annealed particle filter Deutscher, Blake,
Reid (2000); main differences Deutscher et al. use color cues
gloves, use multiple cameras facilitate 3D tracking. used
single monocular camera 2.5D model (with one degree freedom depth
plane, permitting body rotation). Parameters model, body dimensions,
customized speaker. speaker provided two different explanations,
segmentation videos performed manually. additional post-processing,
calibration, cleaning tracker output performed.
inspection, lack depth information appears cause many systems
errors; bending arm joints depth dimension caused arm length appear
change ways confusing model. Nonetheless, estimate manual
examination tracking output hands tracked accurately smoothly
90% time occluded. difficult assess tracker performance
precisely, would require ground truth data actual hand positions
annotated manually time step.

4. Conditional Modality Fusion Coreference Resolution
section describe conditional modality fusion. Section 4.1 describe
hidden variables incorporated conditional models. Section 4.2, describe
various theories model combination expressed framework. Section 4.3,
give details implementation.
4.1 Hidden Variables Conditional Models
goal learn use non-verbal features make predictions helpful,
ignore not. call approach conditional modality fusion.
formally, trying predict label {1, 1}, representing single binary
coreference decision whether two noun phrases refer entity.
hidden variable h describes salience gesture features. observable
features written x, model learn set weights w. hidden variable
approach learns predict h jointly, given labeled training data y. use
conditional model, writing:
10. http://www.intel.com/technology/computing/opencv/

364

fiGesture Salience Hidden Variable

p(y|x; w) =

X

p(y, h|x; w)

h

=

P
exp((y, h, x; w))
P h
.
0
0 ,h exp((y , h, x; w))

Here, potential function representing compatibility label y,
hidden variable h, observations x; potential parameterized vector
weights, w. numerator expresses compatibility label observations x,
summed possible values hidden variable h. denominator sums
h possible labels 0 , yielding conditional probability p(y|x; w).
model trained gradient-based optimization maximize conditional
log-likelihood observations. unregularized log-likelihood gradient given
by:
l(w)

=

X

log(p(yi |xi ; w))

(1)

P
exp((yi , h, xi ; w))
log P h
0
0 ,h exp((y , h, xi ; w))

(2)



=

X


li
wj

=

X
h

p(h|yi , xi ; w)

X


(yi , h, xi ; w)
p(h, 0 |xi ; w)
(y 0 , h, xi ; w)
wj
w
j
0
,h

use hidden variables conditionally-trained model follows Quattoni et al.
(2004). However, reference gives general outline hidden-variable conditional models, form potential function depends role hidden variable.
problem-specific, novel contribution research exploration several
different potential functions, permitting different forms modality fusion.
4.2 Models Modality Fusion
form potential function intuitions role hidden
variable formalized. consider three alternative forms , capturing different
theories gesture-speech integration. models range simple concatenation
gesture-speech features structured fusion model dynamically assesses relevance
gesture features every noun phrase.
models consider influenced goal, determine whether two
noun phrases (NPs) coreferent. Gesture salience assessed NP, determine
whether gestural features influence decision whether noun phrases
corefer. set h = hh1 , h2 i, h1 {1, 1} representing gesture salience
first noun phrase (antecedent), h2 {1, 1} representing gesture salience
second noun phrase (anaphor).

365

fiEisenstein, Barzilay, & Davis

4.2.1 Same-Same model
trivial case, ignore hidden variable always include features
gesture speech. Since weight vectors modalities unaffected
hidden variable, model referred same-same model. Note
identical standard log-linear conditional model, concatenating features single
vector. model thus type early fusion, meaning verbal non-verbal
features combined prior training.

ss (y, h, x; w) y(wvT xv + wnv
xnv )

(3)

xv wv refer features weights verbal modality; xnv wnv refer
non-verbal modality.
4.2.2 Same-Zero Model
Next, consider model treats hidden variable gate governing whether
gesture features included. model called same-zero model, since verbal
features weighted identically regardless hidden variable, gesture feature
weights go zero unless h1 = h2 = 1.
(
x ) + h wT x + h wT x , h = h = 1
y(wvT xv + wnv
1
2
2 h h2
nv
1 h h1
sz (y, h, x; w)
otherwise.
ywvT xv + h1 whT xh1 + h2 whT xh2 ,

(4)

features xh weights wh contribute estimation hidden variable h.
may include features xv xnv , different features.
features assessed independently noun phrase, yielding xh1 antecedent
xh2 anaphor.
model reflects intuition gesture features (measured xnv ) relevant
gestures noun phrases salient. Thus, features contribute
towards overall potential h1 = h2 = 1.
4.2.3 Different-Zero Model
may add flexibility model permitting weights verbal features
change hidden variable. model called different-zero model, since
different set verbal weights (wv,1 wv,2 ) used depending value hidden
variable. model motivated empirical research showing language usage different used combination meaningful non-verbal communication
used unimodally (Kehler, 2000; Melinger & Levelt, 2004).
formal definition potential function is:
(
x + wT x ) + h wT x + h wT x , h = h = 1
y(wv,1
v
1 h h1
2 h h2
1
2
nv nv
dz (y, h, x; w)



ywv,2 xv + h1 wh xh1 + h2 wh xh2 ,
otherwise.

366

(5)

fiGesture Salience Hidden Variable

4.2.4 Models
presented three models increasing complexity; different-different model
one step complex, including two pairs weight vectors verbal gestural
features (see Equation 6). model, distinction verbal non-verbal
features (xv xnv ) evaporates, reason hidden variable h
actually indicate relevance non-verbal features. addition, high degree
freedom model may lead overfitting.
(
x + wT x ) + h wT x + h wT x , h = h = 1
y(wv,1
v
1 h h1
2 h h2
1
2
nv,1 nv
dd (y, h, x; w)
x + wT x ) + h wT x + h wT x , otherwise.
y(wv,2
v
1 h h1
2 h h2
nv,2 nv

(6)

models considered assume verbal features always relevant, gesture features may sometimes ignored. words,
considered whether might necessary assess salience verbal features. One
might consider alternative potential functions zero-same model,
verbal features sometimes ignored. consider models, gesture unaccompanied speech extremely rare dataset.
4.3 Implementation
objective function (Equation 1) optimized using Java implementation L-BFGS,
quasi-Newton numerical optimization technique (Liu & Nocedal, 1989). Standard L2norm regularization employed prevent overfitting, cross-validation select
regularization constant. Java source code available online:
http://rationale.csail.mit.edu/gesture
Although standard logistic regression optimizes convex objective, inclusion
hidden variable renders objective non-convex. Thus, convergence global optimum
guaranteed, results may differ depending initialization. Nonetheless, nonconvexity encountered many models natural language processing machine
learning generally, Baum-Welch training hidden Markov models (HMMs) (Rabiner, 1989) hidden-state conditional random fields (Quattoni et al., 2004; Sutton &
McCallum, 2006). Often, results shown reasonably robust initialization;
otherwise, multiple restarts used obtain greater stability. present empirical
evaluation Section 6.2 showing results overly sensitive initialization.
experiments, weights initialized zero, enabling results reproduced
deterministically.

5. Features
Coreference resolution studied thirty years AI community (Sidner, 1979; Kameyama, 1986; Brennan, Friedman, & Pollard, 1987; Lappin & Leass, 1994;
Walker, 1998; Strube & Hahn, 1999; Soon, Ng, & Lim, 2001; Ng & Cardie, 2002). Based
large body work, broad consensus core set useful verbal features. paper contributes literature study gesture features,
multimodal coreference resolution identifying salient gestures. describe
367

fiEisenstein, Barzilay, & Davis

feature
edit-distance
exact-match
str-match
nonpro-str

type
similarity
similarity
similarity
similarity

pro-str
j-substring-i
i-substring-j
overlap
np-dist
sent-dist
both-subj
same-verb
number-match

similarity
similarity
similarity
similarity
centering-based
centering-based
centering-based
centering-based
compatibility

pronoun
count
has-modifiers
indef-np
def-np
dem-np
lexical features

centering-based
centering-based
centering-based
centering-based
centering-based
centering-based
centering-based

Pairwise verbal features
description
numerical measure string similarity two NPs
true two NPs identical
true NPs identical removing articles
true antecedent anaphor j pronouns,
str-match true
true j pronouns, str-match true
true j substring
true substring j
true shared words j
number noun phrases j document
number sentences j document
true j precede first verb sentences
true first verb sentences j identical
true j number
Single-phrase verbal features
true NP pronoun
number times NP appears document
true NP adjective modifiers
true NP indefinite NP (e.g., fish)
true NP definite NP (e.g., scooter )
true NP begins this, that, these,
lexical features defined common pronouns: it, that,
this,

Table 1: set verbal features multimodal coreference resolution. table,
refers antecedent noun phrase j refers anaphor.

features Sections 5.2 5.3, begin review verbal features
selected literature.
5.1 Verbal Features
selection verbal features motivated extensive empirical literature textbased coreference resolution (Soon et al., 2001; Ng & Cardie, 2002; Strube & Muller, 2003;
Daume III & Marcu, 2005). proliferation variety features explored
consequence fact coreference complex discourse phenomenon. Moreover,
realization coreference highly dependent type discourse appears;
relevant factors include modality (e.g., speech vs. language), genre (e.g., meeting vs.
lecture) topic (e.g., politics vs. scientific subject). Although certain feature types
application-specific, three classes features centering-based, similarity, compatibility
features useful across coreference applications. classes form basis
verbal features used model. Table 1 provides brief description verbal feature
set. draw examples transcript Appendix provide detailed
explanation features motivate use application.

368

fiGesture Salience Hidden Variable

focus-distance
DTW-agreement
same-cluster*
JS-div*
dist-to-rest
jitter
speed
rest-cluster*
movement-cluster*

Pairwise gesture features
Euclidean distance pixels average hand position two
NPs
measure agreement hand-trajectories two NPs, computed
using dynamic time warping
true hand positions two NPs fall cluster
Jensen-Shannon divergence cluster assignment likelihoods
Single-phrase gesture features
distance hand rest position
sum instantaneous motion across NP
total displacement NP, divided duration
true hand usually cluster associated rest position
true hand usually cluster associated movement

Table 2: set gesture features multimodal coreference resolution. Features
used prior work gesture analysis annotated asterisk (*).

Centering-related features: set features captures relative prominence
discourse entity local discourse, likelihood act coreferent
given phrase. features inspired linguistic analysis formalized Centering Theory, links coreferential status entity discourse prominence (Grosz, Joshi, & Weinstein, 1995; Walker, Joshi, & Prince, 1998; Strube &
Hahn, 1999; Poesio, Stevenson, Eugenio, & Hitzeman, 2004; Kibble & Power, 2004).
theory hypothesizes point coherent discourse, one entity
focus characterizes local discourse terms focus transitions
adjacent sentences.
existing machine-learning based coreference systems attempt
fully implement Centering-style analysis.11 Instead, number centering-related features included. instance, identify focus-preserving transitions (i.e., CONTINUE transitions) feature both-subj introduced. According theory,
transitions common locally-coherent discourse, therefore coreference
assignments consistent principle may preferable. also characterize
transitions terms span (np-dist sent-dist). Transitions involve
short gaps preferred transitions long gaps.
Another important set Centering-related features defined level single
phrase. syntactic role phrase sentence captured features
pronoun, has-modifiers, indef-np indicates discourse prominence
therefore likelihood coreference antecedent. example, consider
utterance lines 12 13: spring active meaning going
down. Here, anaphor clearly refers antecedent spring.
11. implementation challenging several respects: one specify free parameters
system (Poesio et al., 2004) determine ways combining effects various constraints.
Additionally, implementation centering depends obtaining detailed syntactic information,
available case.

369

fiEisenstein, Barzilay, & Davis

fact antecedent demonstrative noun phrase (beginning this)12
anaphor pronoun also centering-related features suggest
coreference likely. addition syntactic status, also take account
frequency noun phrase monologue (see count). Frequency information
commonly used approximate topical salience entity text (Barzilay &
Lapata, 2005).
Similarity features: simple yet informative set coreference cues based
string-level similarity noun phrases. instance, reference
spring line 12 identical noun phrase line 5 resolved exact
match surface forms. general, researchers text-based coreference resolution
found string match feature single predictive feature
discourse entity commonly described using identical similar noun phrases (Soon
et al., 2001).
system, similarity information captured seven features quantify
degree string overlap. instance, feature (exact-match) indicates full
overlap noun phrases, feature (overlap) captures whether two
phrases share common words. context coreference resolution, noun
phrase match informative pronoun match, use distinct features
matching strings syntactic categories (e.g., nonpro-str vs. pro-str),
following (Ng & Cardie, 2002). Surface similarity may also quantified terms
edit-distance (Strube, Rapp, & Muller, 2002).
Compatibility features: important source coreference information compatibility two noun phrases. instance, utterance ball line 11
refer preceding noun phrase things, since incompatible number. Feature number-match captures information. Since topic
discourse corpus relates mechanical devices, almost noun phrases
neuter-gendered. eliminates utility features measure gender compatibility. Finally, note complex semantic compatibility features
previously explored (Harabagiu, Bunescu, & Maiorano, 2001; Strube et al., 2002;
Yang, Zhou, Su, & Tan, 2003; Ji, Westbrook, & Grishman, 2005; Daume III & Marcu,
2005; Yang, Su, & Tan, 2005).
features traditionally used coreference avoided here. Features
depend punctuation seem unlikely applicable automatic recognition
setting, least near future. addition, many systems MUC ACE
coreference corpora use gazetteers list names nations business entities,
features relevant corpus. Another possibility use features identifying
speaker, capture individual variation patterns reference (Chai, Hong, Zhou, &
Prasov, 2004; Jordan & Walker, 2005). However, wished develop approach
speaker-independent.
12. Simple string matching techniques used assess phrase types: definite noun phrases
beginning article the; indefinite noun phrases begin an; demonstrative noun
phrases begin this. Bare plurals marked indefinites, proper names appear
dataset.

370

fiGesture Salience Hidden Variable

5.2 Non-Verbal Features
non-verbal features attempt capture similarity speakers hand gestures,
similar gestures suggest semantic similarity (McNeill, 1992; Quek et al., 2002a).
example, two noun phrases may likely corefer accompanied
identical pointing gestures. section, describe features quantify various
aspects gestural similarity.
general, features computed duration noun phrase, yielding single feature vector per NP. universally true beginning
end points relevant gestures line exactly beginning end associated words, several experiments demonstrated close synchrony gesture
speech (McNeill, 1992). future work, hope explore whether sophisticated
gesture segmentation techniques improve performance.
straightforward measure similarity Euclidean distance
average hand position noun phrase call focus-distance.13 Euclidean
distance captures cases speaker performing gestural hold roughly
location (McNeill, 1992). However, Euclidean distance may correlate directly
semantic similarity. example, gesturing detailed part diagram,
small changes hand position may semantically meaningful, regions
positional similarity may defined loosely. Ideally, would compute semantic
feature capturing object speakers reference (e.g., red block),
possible general, since complete taxonomy possible objects reference usually
unknown.
Instead, perform spatio-temporal clustering hand position velocity, using
hidden Markov model (HMM). Hand position speed used observations,
assumed generated Gaussians, indexed model states. states
correspond clusters, cluster membership used discretized representation
positional similarity. Inference state membership learning model parameters
performed using traditional forward-backward Baum-Welch algorithms (Rabiner,
1989).
standard hidden Markov model may suitable, increase robustness
make better use available training data reducing models degrees-of-freedom.
Reducing number degrees-of-freedom means learning simpler models,
often general. done parameter tying: requiring subsets
model parameters take values (Bishop, 2006). employ three forms
parameter tying:
1. one state permitted expected speed greater zero. state
called move state; states hold states, speed observations
assumed generated zero-mean Gaussians. single move state
used concerned location hold gestures.
2. Transitions distinct hold states permitted. reflects commonsense idea possible transition two distinct positions without
moving.
13. general, features computed duration individual noun phrases.

371

fiEisenstein, Barzilay, & Davis

3. outgoing transition probabilities hold states assumed identical.
Intuitively, means likelihood remaining within hold state
depend hold located. possible imagine scenarios
hold, reasonable simplification dramatically reduces
number parameters estimated.
Two similarity features derived spatio-temporal clustering. samecluster feature reports whether two gestures occupy state majority
durations two noun phrases. boolean feature indicates whether
two gestures roughly area, without need explicit discretization step.
However, two nearby gestures may classified similar method,
near boundary two states, gestures move multiple states.
reason, quantify similarity state assignment probabilities using
Jensen-Shannon divergence, metric probability distributions (Lin, 1991). JS-div
real-valued feature provides nuanced view gesture similarity based
HMM clustering. same-cluster JS-div computed independently models
comprising five, ten, fifteen states.
gesture features described thus far largely capture similarity static
gestures; is, gestures hand position nearly constant. However,
features capture similarity gesture trajectories, may also used
communicate meaning. example, description two identical motions might
expressed similar gesture trajectories. measure similarity
dynamic gestures, use dynamic time warping (Huang, Acero, & Hon, 2001);
reported DTW-distance feature. Dynamic time warping used frequently
recognition predefined gestures (Darrell & Pentland, 1993).
features computed hand body pixel coordinates, obtained
automatically via computer vision, without manual post-processing kind (see Section 3.3). feature set currently supports single-hand gestures, using hand
farthest body center. verbal feature set, Wekas default supervised
binning class applied continuous-valued features (Fayyad & Irani, 1993).14
method identifies cut points minimize class-wise impurity side cut,
measured using average class entropy. greedy top-down approach used recursively
partition domain attribute value. Partitioning terminated criterion based
minimum description length.
5.3 Meta Features
Meta features observable properties speech gesture give clues
whether speaker gesturing way meaningful language language
processing task hand. hypothesize difference relevant irrelevant hand motions apparent range verbal visual features. equations 4-6,
features represented xh1 xh2 . Unlike similarity-based features described above, meta features must computable single instant time, encode
properties individual gestures cotemporal NPs.
14. specific class weka.filters.supervised.attribute.Discretize.

372

fiGesture Salience Hidden Variable

Previous research investigated types verbal utterances likely
accompanied gestural communication (Melinger & Levelt, 2004; Kehler, 2000). However,
first attempt formalize relationship context machine-learning
approach predicts gesture salience.
5.3.1 Verbal Meta Features
Meaningful gesture shown frequent associated speech
ambiguous (Melinger & Levelt, 2004). Kehler (2000) finds fully-specified noun phrases
less likely receive multimodal support. findings lead us expect gestures
likely co-occur pronouns, unlikely co-occur noun phrases
begin determiner the, particularly include adjectival modifiers. capture
intuitions, single-phrase verbal features (Table 1) included meta-features.
5.3.2 Non-verbal Meta Features
Research gesture shown semantically meaningful hand motions usually take
place away rest position, located speakers lap sides (McNeill,
1992). Effortful movements away default positions thus expected
predict gesture used communicate. identify rest position center
body x-axis, fixed, predefined location y-axis. dist-torest feature computes average Euclidean distance hand rest position,
duration NP.
Hand speed may also related gesture salience. speed feature captures
overall displacement (in pixels) divided length noun phrase. Writing x
hand position {1, 2, . . . , } time index, speed = T1 ||xT x1 ||2 .
P
jitter feature captures average instantaneous speed: jitter = T1 Tt=2 (xt xt1 )T (xt
xt1 ). feature captures periodic jittery motion, quantified
speed feature end position far original position. Also, high jitter
often indicates tracker lost hand position, would excellent
reason ignore gesture features.
noted previous section, HMM used perform spatio-temporal
clustering hand positions velocities. rest-cluster feature takes value
true iff frequently occupied state NP closest rest position.
addition, parameter tying used HMM ensure states one
static holds, remaining state represents transition movements
holds. last state permitted expected non-zero speed; hand
frequently state NP, movement-cluster feature takes
value true.

6. Evaluation Coreference Resolution
evaluation, assess whether gesture features improve coreference resolution,
compare conditional modality fusion approaches gesture-speech combination.

373

fiEisenstein, Barzilay, & Davis

6.1 Evaluation Setup
describe procedure evaluating performance approach. includes
evaluation metric (Section 6.1.1), baselines comparison (Section 6.1.2), parameter
tuning (Section 6.1.3). coreference annotations described Section 3.2.
6.1.1 Evaluation Metric
Coreference resolution often performed two phases: binary classification phase,
likelihood coreference pair noun phrases assessed; global
partitioning phase, clusters mutually-coreferring NPs formed. model
address global partitioning phase, question whether pair
noun phrases document corefer. Moving pairwise noun phrase coreference
global partitioning requires clustering step may obscure performance differences
level model operates. Moreover, results depend choice
clustering algorithm mechanism selecting number clusters (or,
alternatively, cut-off value merging clusters). parameterization particularly
challenging corpus large dedicated development set. Consequently, bulk evaluation performed binary classification phase. However, purpose comparing prior work coreference, also perform global
evaluation, measures overall results clustering.
binary evaluation, use area ROC curve (auc) error metric (Bradley, 1997). auc evaluates classifier performance without requiring specification
cutoff. metric penalizes misorderings cases classifier ranks negative
examples highly positive examples. ROC analysis increasingly popular,
used variety NLP tasks, including detection action items
emails (Bennett & Carbonell, 2007) topic segmentation (Malioutov & Barzilay, 2006).
Although binary evaluation typically used coreference resolution, believe
appropriate choice here, reasons noted above.
global evaluation uses constrained entity-alignment f-measure (ceaf) evaluation (Luo, 2005). metric avoids well-known problems earlier MUC evaluation
metric (Vilain, Burger, Aberdeen, Connolly, & Hirschman, 1995). clustering step
performed using two standard techniques literature, describe Section 6.3. future work plan explore techniques perform coreference single
joint step (e.g., Daume III & Marcu, 2005). global metric would appropriate
measure contributions model directly.
6.1.2 Baselines
Conditional modality fusion (cmf) compared traditional approaches modality
combination NLP tasks:
Early fusion. early fusion baseline includes features single vector,
ignoring modality. equivalent standard maximum-entropy classification.
Early fusion implemented conditionally-trained log-linear classifier; uses
code cmf model, always includes features.

374

fiGesture Salience Hidden Variable

Late fusion. two late fusion baselines train separate classifiers gesture
speech, combine posteriors. modality-specific classifiers conditionallytrained log-linear classifiers, use code cmf model.
simplicity, parameter sweep identifies interpolation weights maximize performance test set. Thus, likely results somewhat overestimate
performance baseline models. additive multiplicative combination considered.
fusion. baselines include features single modality,
build conditionally-trained log-linear classifier. Implementation uses
code cmf model, weights features outside target modality forced
zero.
important question results compare existing state-of-the-art coreference systems. fusion, verbal features baseline provides reasonable representation prior work coreference, applying maximum-entropy classifier set
typical textual features. direct comparison existing implemented systems would
ideal, available systems use textual features inapplicable
spoken-language dataset, punctuation, capitalization, gazetteers.
6.1.3 Parameter Tuning
small size corpus permit dedicated test training sets, results
computed using leave-one-out cross-validation, one fold sixteen
documents corpus. Parameter tuning performed using cross validation within
training fold. includes selection regularization constant, controls
trade-off fitting training data learning model simpler (and
thus, potentially general). addition, binning continuous features performed
within cross-validation fold, using method described Section 5.2. Finally,
noted above, model weights initialized zero, enabling deterministic reproducibility
experiments.
6.2 Results
Conditional modality fusion outperforms approaches statistically significant
margin (Table 4). Compared early fusion, different-zero model conditional
modality fusion offers absolute improvement 1.17% area ROC curve (auc)
compare lines 1 4 table. paired t-test shows result statistically
significant (p < .01, t(15) = 3.73). cmf obtains higher performance fourteen
sixteen cross-validation folds. additive multiplicative late fusion perform par
early fusion. p-values significance tests pairwise comparisons
shown Table 5.
Early fusion gesture features superior unimodal verbal classification
absolute improvement 1.64% auc (p < .01, t(15) = 4.45) compare lines 4 7
Table 4. additional 1.17% auc provided conditional modality fusion amounts
relative 73% increase power gesture features. results relatively robust
variations regularization constant, shown Figure 3. means
375

fiEisenstein, Barzilay, & Davis

cmf different-different (DD)

cmf different-zero (DZ)

cmf same-zero (SZ)

Early fusion (E)

Late fusion, multiplicative (LM)

Late fusion, additive (LA)

fusion (VO, GO)

Uses two different sets weights verbal gestural features, depending
hidden variable (equation 6).
Uses different weights verbal features
depending hidden variable; hidden variable indicates non-salience, gesture
weights set zero (equation 5).
Uses weights verbal features regardless gesture salience; hidden variable indicates non-salience, gesture weights
set zero (equation 4).
Standard log-linear classifier. Uses
weights verbal gestural features, regardless hidden variable (equation 3).
Trains separate log-linear classifiers gesture verbal features. Combines posteriors
multiplication.
Trains separate log-linear classifiers gesture verbal features. Combines posteriors
interpolation.
Uses one modality classification.

Table 3: Summary systems compared evaluation

model
1. cmf different-zero
2. cmf different-different
3. cmf same-zero
4. Early fusion (same-same)
5. Late fusion, multiplicative
6. Late fusion, additive
7. fusion (verbal features only)
8. fusion (gesture features only)

auc
.8226
.8174
.8084
.8109
.8103
.8068
.7945
.6732

Table 4: Coreference performance, area ROC curve (auc). systems
described Table 3

376

fiGesture Salience Hidden Variable

DD
.01

cmf different-zero (DZ)
cmf different-different (DD)
cmf same-zero (SZ)
Early fusion (E)
Late fusion, multiplicative (LM)
Late fusion, additive (LA)
Verbal features (VO)
Gesture features (GO)

SZ
.01
.05

E
.01
ns
ns

LM
.01
ns
ns
ns

LA
.01
.05
ns
ns
ns

VO
.01
.01
.05
.01
.01
.01

GO
.01
.01
.01
.01
.01
.01
.01

Table 5: P-values pairwise comparison models. ns indicates
difference model performance significant p < .05. parentheses
left column explain abbreviations top line.

Comparison models

Comparison among CMF models

0.82
area ROC curve

area ROC curve

0.82
0.81
0.8
0.79
CMF (diffzero)
early fusion
verbalonly

0.78
0.77

1

2

3
4
5
6
log regularization constant

7

0.81
0.8
0.79
differentzero
differentdifferent
samezero

0.78
0.77

8

1

2

3
4
5
6
log regularization constant

7

8

Figure 3: Results regularization constant

performance gains obtained conditional modality fusion highly dependent
finding optimal regularization constant.
noted Section 4.3, conditional modality fusion optimizes non-convex objective.
perform additional evaluation determine whether performance sensitive initialization. Randomizing weights five different iterations best-performing
system, observed standard deviation 1.09 103 area ROC curve
(auc). experiments weights initialized zero, enabling results
reproduced deterministically.
6.3 Global Metric
Coreference traditionally evaluated global error metric. research directed
specifically binary classification coreference pairs noun phrases,

377

fiEisenstein, Barzilay, & Davis

model
cmf (different-zero)
cmf (different-different)
cmf (same-zero)
Early fusion (same-same)
Late fusion, multiplicative
Late fusion, additive
fusion (verbal features only)
fusion (gesture features only)

first-antecedent
55.67
54.71
53.91
54.18
53.74
53.56
53.47
44.68

best-antecedent
56.02
56.20
55.32
55.50
54.44
55.94
55.15
44.85

Table 6: ceaf global evaluation scores, using best clustering threshold
Firstantecedent clustering

Bestantecedent clustering

0.5

0.5
CEAF

0.55

CEAF

0.55

0.45

0.45

0.4

0.4
CMF (diffzero)
late fusion, additive
verbal

0.35
0.1

0.2

0.3
0.4
0.5
clustering threshold

CMF (diffzero)
late fusion, additive
verbal
0.35
0.1

0.6

0.2

0.3
0.4
0.5
clustering threshold

0.6

Figure 4: Global coreference performance, measured using ceaf scores, plotted
threshold clustering

focused evaluating specific portion larger coreference problem. However,
purpose comparing prior research coreference, present results using
traditional global metric.
perform global evaluation, must cluster noun phrases document, using
pairwise coreference likelihoods similarity metric. experiment two clustering methods commonly used literature. first-antecedent technique
resolves NPs first antecedent whose similarity predefined threshold (Soon
et al., 2001). best-antecedent technique resolves noun phrase compatible prior noun phrase, unless none threshold (Ng & Cardie, 2002).
Figure 4 shows global scores, plotted value clustering threshold.
clarity, best performing system class shown: conditional
378

fiGesture Salience Hidden Variable

modality fusion, plot different-zero model; multimodal baselines, plot
additive late fusion model (the combination additive late fusion best-antecedent
clustering method best performing multimodal baseline); unimodal baseline,
plot verbal-features baseline. Table 6 lists performance method
optimum clustering threshold. comparison, Ng reports ceaf score 62.3 (Ng,
2007) ACE dataset, although results directly comparable due
differences corpora.
shown results, performance sensitive clustering method
clustering threshold. Conditional modality fusion generally achieves best results,
best-antecedent clustering generally outperforms first-antecedent technique. Nonetheless, advantage conditional modality fusion smaller ROC analysis.
believe ROC analysis demonstrates advantage conditional modality fusion
directly, global metric interposes clustering step obscures differences
classification techniques. Nonetheless, global metric may better overall measure quality coreference downstream applications search
summarization. future work, hope investigate whether conditional modality
fusion approach applied global models coreference require separate
classification clustering phases (e.g., Daume III & Marcu, 2005).
6.4 Feature Analysis
machine learning approach adopted permits novel analysis
compare linguistic contribution gesture features presence verbal
features. Thus investigate gesture features supply unique information
verbal features. addition, analyze types verbal features correlate
closely gesture features, independent. statistical significance results
based two-tailed, paired t-tests.
6.4.1 Gestural Similarity
Figure 5 shows contribution three classes gestural similarity features: focusdistance, DTW-agreement, two HMM-based features (same-cluster JSdiv). top dotted line graph shows performance different-zero model
complete feature set, bottom line shows performance model without
gestural similarity features.15
feature group conveys useful information, performance one feature
group always better performance without gestural similarity features (p < .01, t(15) =
3.86 DTW-agreement, weakest three feature groups). performance using focus-distance significantly better DTW-agreement
feature used (p < .05, t(15) = 2.44); comparisons significant. also find
15. Note baseline gesture features higher fusion (verbal features only)
baseline Table 4. Although feature groups identical, classifiers different.
fusion (verbal features only) baseline uses standard log-linear classifier, gesture
features uses conditional modality fusion, permitting two sets weights verbal features,
shown equation 5.

379

fiEisenstein, Barzilay, & Davis

0.83
feature absent
others absent
0.825
gesture features

feature group
gesture similarity features
focus-distance
DTW-agreement
HMM-based

AUC

0.82

0.815

0.81

+
.8226
.8200
.8149
.8183

.8054
.8200
.8234
.8198

gesture features
0.805

0.8

distance

DTW

HMMbased

Figure 5: analysis contributions set gestural similarity features.
plus column left table shows results feature set
present; minus column shows results removed. before,
metric area ROC curve (auc).

evidence redundancy feature groups, removing individual feature
group significantly impair performance two feature groups remain.
6.4.2 Verbal Gestural Overlap
Next, assess degree overlap gesture verbal information. hypothesize gesture complementary certain verbal features, redundant
others. example, string match features edit-distance exact-match
seem unlikely convey information gesture. see why, consider cases
string match likely helpful: fully-specified noun phrases red
ball, rather pronouns. Empirical research suggests majority informative
gestures occur pronouns underspecified utterances, string match
unlikely helpful (Kehler, 2000). Thus, expect low level overlap
gesture string match features.
Distributional features another source verbal information. include number intervening sentences noun phrases two candidate NPs,
number times NP appears document. features establish context
may permit resolution references ambiguous surface form alone.
example, noun phrase occurred recently, often, pronominal reference
may sufficiently clear. Since gesture may also used cases, expect
redundancy gestural similarity distributional features.
intuitions lead us specific predictions system performance. presence
gesture similarity features mitigate cost removing distributional
features, gesture features reinforce information. However,
presence gesture features effect cost removing string
match features.

380

fiGesture Salience Hidden Variable

0.01

0

Cost AUC

0.01

feature group

0.02

string match
distance, count

0.03

gesture similarity features
.7721
.8258

without
.7522
.8018

0.04

0.05
gesture
without gesture
0.06

string match

distance count

Figure 6: contribution verbal features, without gesture similarity features.
graph shows loss incurred removing verbal feature class, conditioned presence gesture similarity features. table shows overall
performance combination feature groups.

predictions supported data (Figure 6). Removing distributional
features impair performance long gesture features present,
impair performance gesture features also removed difference statistically
significant (p < .01, t(15) = 3.76). observation consistent hypothesis
feature groups convey similar information. contrast, cost removing
string match features vary significant margin, regardless whether
gesture features present. accords intuition feature groups
convey independent information.

7. Evaluation Keyframe Extraction
previous sections describe application conditional modality fusion natural
language processing: using gesture features meaningful, contribution coreference classification enhanced. section, show conditional
modality fusion also predicts gestures useful human viewers. Specifically,
use conditional modality fusion estimate gesture salience select keyframes
video. demonstrate keyframes selected method match selected
human raters better keyframes selected traditional text image-based
algorithms.
Section 7.1, explain keyframe-based summarization task. describe
basic modeling approach Section 7.2. evaluation setup presented Section 7.3.
Section 7.4 gives experimental results.
7.1 Keyframe-Based Summarization
goal produce comic book summary video, transcript augmented salient keyframes still images clarify accompanying text. Keyframe381

fiEisenstein, Barzilay, & Davis

based summaries allow viewers quickly review key points video presentation, without
requiring time hardware necessary view actual video (Boreczky, Girgensohn,
Golovchinsky, & Uchihashi, 2000). argued above, textual transcriptions alone
capture relevant information, keyframe-based summary may provide minimal visual information required understand presentation. Appendix B contains
excerpt summary produced system.
noted, gesture supplements speech unique semantic information. Thus, keyframes
showing salient gestures would valuable addition transcript text. Ideally,
would select keyframes avoid redundancy visual verbal modalities,
conveying relevant information.
Existing techniques keyframe extraction usually focused edited videos
news broadcasts (e.g., Uchihashi, Foote, Girgensohn, & Boreczky, 1999; Boreczky et al.,
2000; Zhu, Fan, Elmagarmid, & Wu, 2003). systems seek detect large-scale changes
image features identify different scenes, choose representative example
scene. approach poorly suited unedited videos, recording
classroom lecture business presentation. videos, key visual information
variation scenes camera angles, visual communication provided
gestures speaker. goal capture relevant keyframes identifying salient
gestures, using model developed previous sections paper.
7.2 Modeling Approach
One possible approach formulate gesture extraction standard supervised learning
task, using corpus salient gestures annotated. However, annotation
expensive, prefer avoid it. Instead learn salience bootstrapping
multimodal coreference resolution, using conditional modality fusion. learning predict
specific instances gesture helps, obtain model gesture salience.
example, expect pointing gesture presence anaphoric expression
would found highly salient (as Figure 1); ambiguous hand pose
presence fully-specified noun phrase would salient. approach
identify salient gestures, identify occur context selected
language understanding task. coreference resolution, gestures co-occur
noun phrases selected. noun phrases ubiquitous language, still
cover usefully broad collection gestures.
Using model coreference resolution introduced Section 4, obtain probability distribution hidden variable, controls whether gesture features
included coreference resolution. basic hypothesis instances gesture
features included high likelihood likely correspond salient gestures.
gestures rated salient method used select keyframes summary.
Models coreference resolution gesture salience learned jointly, based
same-zero model defined Equation 4. training, set weights wh obtained,
allowing estimation gesture
sum possible
P salience noun phrase.

values h2 , obtaining y,h2 (y, h, x; w) = h1 wh xh1 . find potential

382

fiGesture Salience Hidden Variable

case gesture salient setting h1 = 1, yielding whT xh1 .16 working
assumption potential reasonable proxy informativeness keyframe
displays noun phrases accompanying gesture.
potential provides ordering noun phrases document. select
keyframes midpoints top n noun phrases, n specified advance
annotator. Providing system ground truth number keyframes follows
common practice textual summarization literature summaries different lengths
difficult compare, summary duration governed partially annotators
preference brevity completeness (Mani & Maybury, 1999). keyframe given
caption includes relevant noun phrase accompanying text, noun
phrase next keyframe. Portions output system shown Figure 1
Appendix B.
7.3 Evaluation Setup
evaluation methodology similar intrinsic evaluation developed Document Understanding Conference.17 assess quality automatically extracted
keyframes comparing human-annotated ground truth.
7.3.1 Dataset
dataset consists dialogues collected using procedure described Section 3.
coreference annotations described Section 3.2 used. Additionally, nine
sixteen videos annotated keyframes. these, three used developing
system baselines, remaining six used final evaluation (these
indicated asterisks table Appendix C). explicit training
keyframe annotations, development set used evaluation system
construction.
specification ground truth annotation required keyframes capture
static visual information annotator deems crucial understanding content
video. number selected frames left discretion; average, 17.8
keyframes selected per document, average total 4296 frames per document.
Annotation performed two raters; subset two videos annotated raters,
raw interrater agreement 86%, yielding kappa .52.
One important difference dataset standard sentence extraction datasets
many frames may nearly identical, due high frame rate video.
reason, rather annotating individual frames, annotators marked regions
identical visual information. regions define equivalence classes, frame
given region would equally acceptable. single keyframe selected
every ground truth region, result would minimal set keyframes necessary
reader fully understand discourse. average, 17.8 regions selected per
document spanned 568 frames.
16. Note consider noun phrase anaphor (xh2 ) sum possible values
h1 , resulting potential identical.
17. http://duc.nist.gov

383

fiEisenstein, Barzilay, & Davis

Ground truth

1

2

System response

Score

False
Negative

False
Positive

True
Positive


scored

Figure 7: example scoring setup.

7.3.2 Training Coreference Resolution
described Section 7.2, approach keyframe extraction based model
gesture salience learned labeled data coreference resolution. training
phase performed leave-one-out cross-validation: separate set weights learned
presentation, using fifteen presentations training set. learned
weights used obtain values hidden variable indicating gesture salience,
described previous subsection.
7.3.3 Evaluation Metric
Figure 7 illustrates scoring setup. top row figure represents ground
truth; middle row represents system response, vertical lines indicating selected
keyframes; bottom row shows response scored.
systems number keyframes fixed equal number regions
ground truth annotation. system response includes keyframe
within ground truth region, false positive recorded. system response fails
include keyframe region ground truth, false negative recorded; true
positive recorded first frame selected given ground truth region,
additional frames region scored. system thus still penalized
redundant keyframe, wasted one finite number keyframes
allowed select. time, error seems less grave true substitution
error, keyframe containing relevant visual information selected. report
F-measure, harmonic mean recall precision.
7.3.4 Baselines
compare performance system three baselines, present
order increasing competitiveness.
Random-keyframe: simplest baseline selects n keyframes random
throughout document. baseline similar random sentence baselines
common textual summarization literature (Mani & Maybury, 1999).
number keyframes selected baseline equal number regions
ground truth. baseline expresses lower bound performance
reasonable system achieve task. results report average
500 independent runs.

384

fiGesture Salience Hidden Variable

NP-salience: NP-salience system based frequency-based approaches
identifying salient NPs purpose text summarization (Mani & Maybury,
1999). salience heuristic prefers common representative tokens
largest homogeneous coreference clusters.18 largest cluster one
containing noun phrases; homogeneity measured inverse
number unique surface forms. provides total ordering NPs document; select keyframes midpoint top n noun phrases, n
number keyframe regions ground truth. future work hope explore
finding best point within noun phrase keyframe selection.
Pose-clustering: final baseline based purely visual features. employs
clustering find representative subset frames minimum mutual redundancy.
Uchihashi et al. (1999), seminal paper keyframe selection, perform clustering
frames video, using similarity color histograms distance
metric. Representative images cluster used keyframes.
recent video summarization techniques advanced clustering algorithms (Liu
& Kender, 2007) similarity metric (Zhu et al., 2003), basic approach
forming clusters based visual similarity choosing exemplar keyframes
clusters still used much state-of-the-art research topic (see
Lew, Sebe, Djeraba, & Jain, 2006, survey).
dataset, single fixed camera change video except
movements speaker; thus, color histograms nearly constant
throughout. Instead, use tracked coordinates speakers hands upperbody, normalize values, use Euclidean distance metric. setting,
clusters correspond typical body poses, segments correspond holds
poses. Following Uchihashi et al. (1999), video divided segments
cluster membership constant, keyframes taken midpoints segments.
use importance metric paper ranking segments, choose top
n, n number keyframes ground truth.
7.4 Results
Table 7 compares performance method (Gesture-salience) three
baselines. Using paired t-tests, find Gesture-salience significantly outperforms
alternatives (p < .05 cases). Pose-clustering NP-salience systems
statistically equivalent; significantly better Random-keyframe baseline
(p < .05).
set baselines system compared necessarily incomplete,
many ways keyframes extraction could performed. example, prosodic features could used identify moments particular interest dialogue (Sundaram & Chang, 2003). addition, combination baselines including visual
linguistic features may also perform better individual baseline. However, developing complicated baselines somewhat beside point. evaluation demonstrates simple yet effective technique selecting meaningful keyframes obtained
18. Here, coreference clusters based manual annotations.

385

fiEisenstein, Barzilay, & Davis

Model
GESTURE-SALIENCE
Pose-clustering
NP-salience
Random-keyframe

F-Measure
.404
.290
.239
.120

Recall
.383
.290
.234
.119

Precision
.427
.290
.245
.121

Table 7: Comparison performance keyframe selection task

byproduct conditional modality fusion. suggests estimates gesture
salience given model cohere human perception.
Error analysis manual inspection system output revealed many cases
system selected noun phrase accompanied relevant gesture, specific
keyframe slightly off. method always chooses keyframe midpoint
accompanying noun phrase; often, relevant gesture brief, necessarily
overlap middle noun phrase. Thus, one promising approach improving
results would look inside noun phrase, using local gesture features attempt
identify specific frame gesture salient.
errors arise key gestures related noun phrases.
example, suppose speaker says shoots ball up, accompanies word
gesture indicating balls trajectory. gesture might important
understanding speakers meaning, since overlap noun phrase,
gesture identified system. believe results show
focusing noun phrases good start linguistically-motivated keyframe extraction,
unsupervised approach successful identifying noun phrases require
keyframes. gesture applied language tasks, hope model salience
gesture phrase types, thus increasing coverage approach keyframe
extraction.

8. Conclusions Future Work
summary, work motivated idea gestures best interpreted
individual units self-contained meaning, context gestural
linguistic information. Previous NLP research gesture largely focused building
recognizers gestures characterize specific language phenomena: example, detecting hand gestures cue sentence boundaries (Chen et al., 2006), body language
suggests topic shifts (Nakano et al., 2003). approaches treat gesture sort
visual punctuation. contrast, interested semantics gesture carries.
take recognition-based approach believe unlikely space
possible meaningful gestures could delineated training set. Instead, start
hypothesis patterns gesture correspond patterns meaning,
degree similarity two gestures predicts semantic similarity associated
speech. approach validated experimental results, show substantial
improvement noun phrase coreference resolution using gesture features. one

386

fiGesture Salience Hidden Variable

first results showing automatically-extracted visual features significantly improve
discourse analysis.
second key finding structured approach multimodal integration crucial
achieving full benefits offered gesture features language understanding. Rather
building separate verbal gesture interpretation units simply concatenating
features build model whose structure encodes role modality.
particular, gesture modality supplements speech intermittently, therefore
represent gesture salience explicitly hidden variable. approach, call
conditional modality fusion, yields 73% relative improvement contribution
gesture features towards coreference resolution. improvement attained modeling
gesture salience hidden variable ignoring gestures salient.
Conditional modality fusion induces estimate gesture salience within context
specific linguistic task. test generality salience model, transfer derived
estimates completely different task: keyframe extraction. Without labeled data
keyframe task, simple algorithm outperforms competitive unimodal alternatives.
suggests model gesture salience learned coreference coheres
human perception gesture salience.
theme generality gesture salience suggestive future research. principle, general model gesture salience could applied range discourse-related
language processing tasks. example, consider topic segmentation: text, changes
distribution lexical items strong indicator topic boundaries (Hearst, 1994).
Assuming salient gestures carry unique semantic content, changes distribution
features salient gestures could used similar way, supplementing purely textual
analysis.
Moreover, combination multiple language processing tasks single joint framework raises possibility gesture salience could modeled robustly knowledge transferred tasks. argued using explicit universal
taxonomy gesture, favoring approach focused relevance gesture specific
language processing problems. However, joint framework would generalize notion
salience bottom-up, data-driven way. research may relevant purely
linguistic standpoint: example, investigating types language phenomena share
coherent notions gesture salience, gesture salience expressed visual
linguistic features. paper argued structured models conditional
modality fusion used incorporate linguistic ideas gesture principled
way. hope future work show models also provide new tool
study linguistics gesture.
Another possibility future work investigate richer models gesture salience.
structure explored minimal binary variable indicate salience
gesture coreference resolution. see first step towards complex
structural representations gesture salience may yield greater gains performance.
example, likely gesture salience observes temporal regularity, suggesting
Markov state model. tasks may involve structured dependencies among
gestures, requiring models probabilistic context-free grammars.
Finally, note hand gesture one several modalities accompany spoken
language. Prosody attracted greatest amount attention natural language
387

fiEisenstein, Barzilay, & Davis

processing, coverbal modalities include facial expressions, body posture,
settings lectures meetings writing diagrams. relationship
modalities poorly understood; future research might explore mapping linguistic
functions modalities, whether sets modalities redundant
complementary.

Acknowledgments
authors acknowledge editor anonymous reviewers helpful comments.
also thank colleagues Aaron Adler, S. R. K. Branavan, Emma Brunskill, Sonya
Cates, Erdong Chen, C. Mario Christoudias, Michael Collins, Pawan Deshpande, Lisa Guttentag, Igor Malioutov, Max Van Kleek, Michael Oltmans, Tom Ouyang, Christina Sauper,
Tom Yeh, Luke Zettlemoyer. authors acknowledge support National
Science Foundation (Barzilay; CAREER grant IIS-0448168 grant IIS-0415865)
Microsoft Faculty Fellowship. opinions, findings, conclusions recommendations expressed authors necessarily reflect views
National Science Foundation Microsoft.

388

fiGesture Salience Hidden Variable

Appendix A. Example Transcript
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43

ok [(0) object right here].
im going attempt explain [(0) this] you.
[(1) ball right here]
[(1) ball]
[(2) spring]
[(3) long arm]
[(4) this] [(5) objects actually move].
take [(6) that] back.
[(7) this] rotates well.
[(8) things] stay fixed.
happens [(1) ball] comes [(9) here].
[(2) spring] active.
meaning [(2) its] going down.
[(4) this] come up.
jostle [(3) that].
go around.
[(3) itll] [(4) this] raises [(3) it] up.
[(10) hand] goes down.
[(10) itll] spring back up.
[(1) ball] typically goes [(11) here].
bounces [(12) here].
gets caught like [(13) groove].
[(7) this] continually moving around [(14) circle]
[(15) this] happened three times
watched [(16) video] [(15) it] happened [(17) three times]
[(1) ball] never went [(18) there] [(19) here]
[(1) it] always would get back [(20) here]
[(9) here]
sometimes [(21) thing] would hit [(1) it] harder
[(1) it] would go higher
sometimes [(1) it] would kind loop
[(1) it] came [(9) here]
idea theres [(22) anchors] [(23) here]
[(24) that] wasnt really made clear
yeah [(25) thats] pretty much [(26) it]
[(1) its] essentially [(1) bouncy ball]
[(1) it] pretty much drops like [(27) dead weight]
[(1) it] hits [(28) something]
[(26) it]
[(16) it] probably like [(16) forty five second video]
[(29) it] happened [(17) three times] [(16) video]
[(16) it] moves relatively quickly
much lodged like [(1) it] would like come [(13) here]

389

fiEisenstein, Barzilay, & Davis

44
45
46
47
48
49
50
51
52
53
54
55
56
57
58

[(7) this] moving [(7) it] would
kind like dump [(1) it] [(20) here]
[(7) its] something thats [(30) way]
actually [(31) transfer]
[(7) this] wasnt [(32) here]
[(1) it] would still fall [(20) here] get [(9) here]
thats
im actually sure [(0) this]
[(0) it] looks like [(0) it] looks like
[(0) this] [(33) computer screen]
um [(0) it] basically looks like
[(34) kind game pinball]
[(35) that] [(36) understanding it]
im sure else [(37) its] supposed
ok done guys [(37) one]

390

fiGesture Salience Hidden Variable

Appendix B. Example Keyframe Summary
excerpt keyframe summary generated automatically, described
Section 7.

spring brings thing back in.
latches here. spring right here. thing dont
know is. goes like this.

um goes level here.

1

2

goes down.

goes out.

3

4

soon passes thing.

comes back.

5

6

391

fiEisenstein, Barzilay, & Davis

Appendix C. Dataset Parameters
number
1
2
3*
4
5*
6*
7
8
9*
10*
11
12
13
14*
15
16
total

speaker
1
1
2
2
3
4
4
5
5
6
7
7
8
8
9
9

topic
pinball
candy dispenser
latch
pinball
pinball
candy dispenser
pinball
pinball
piston
pinball
latchbox
pinball
pinball
piston
pinball
candy dispenser

gender
F
F
F
F
F







F
F



duration
3:00
2:27
1:19
2:31
3:00
3:00
3:00
3:00
3:00
3:00
2:20
3:00
2:23
0:47
2:30
2:43
41:00

# words
455
428
104
283
325
404
421
362
313
315
347
221
192
48
378
358
4954

# NPs
95
101
27
65
69
100
109
89
69
71
72
51
47
8
87
77
1137

Corpus statistics dataset used experiments. Asterisks indicate videos
used keyframe evaluation.

References
Adler, A., Eisenstein, J., Oltmans, M., Guttentag, L., & Davis, R. (2004). Building
design studio future. Proceedings AAAI Workshop Making Pen-Based
Interaction Intelligent Natural, pp. 17.
Allen, J., Schubert, L., Ferguson, G., Heeman, P., Hwang, C., Kato, T., Light, M., Martin,
N., Miller, B., Poesio, M., et al. (1995). TRAINS project: case study building
conversational planning agent. Journal Experimental & Theoretical Artificial
Intelligence, 7 (1), 748.
Barzilay, R., & Lapata, M. (2005). Modeling local coherence: entity-based approach.
Proceedings ACL, pp. 141148.
Bennett, P., & Carbonell, J. (2007). Combining Probability-Based Rankers Action-Item
Detection. Proceedings HLT-NAACL, pp. 324331.
Biber, D. (1988). Variation Across Speech Language. Cambridge University Press.
Bishop, C. M. (2006). Pattern Recognition Machine Learning. Springer.
Blitzer, J., McDonald, R., & Pereira, F. (2006). Domain adaptation structural correspondence learning. Proceedings EMNLP, pp. 120128.
Blum, A., & Mitchell, T. (1998). Combining labeled unlabeled data co-training.
Proceedings COLT, pp. 92100.
392

fiGesture Salience Hidden Variable

Boreczky, J., Girgensohn, A., Golovchinsky, G., & Uchihashi, S. (2000). interactive
comic book presentation exploring video. Proceedings CHI, pp. 185192.
Bradley, A. (1997). use area ROC curve evaluation machine
learning algorithms. Pattern Recognition, 30 (7), 11451159.
Brennan, S. E., Friedman, M. W., & Pollard, C. J. (1987). centering approach pronouns.
Proceedings ACL, pp. 155162.
Carletta, J., Ashby, S., Bourban, S., Flynn, M., Guillemot, M., Hain, T., Kadlec, J.,
Karaiskos, V., Kraaij, W., Kronenthal, M., Lathoud, G., Lincoln, M., Lisowska, A.,
McCowan, I., Post, W., Reidsma, D., & Wellner, P. (2005). ami meeting corpus: pre-announcement. Proceedings Workshop Machine Learning
Multimodal Interaction, pp. 2839.
Chai, J. Y., Hong, P., Zhou, M. X., & Prasov, Z. (2004). Optimization multimodal
interpretation. Proceedings ACL, pp. 18.
Chai, J. Y., & Qu, S. (2005). salience driven approach robust input interpretation
multimodal conversational systems. Proceedings HLT-EMNLP, pp. 217224.
Chelba, C., & Acero, A. (2006). Adaptation maximum entropy capitalizer: Little data
help lot. Computer Speech & Language, 20 (4), 382399.
Chen, L., Harper, M., & Huang, Z. (2006). Using maximum entropy (ME) model incorporate gesture cues sentence segmentation. Proceedings ICMI, pp. 185192.
Chen, L., Liu, Y., Harper, M. P., & Shriberg, E. (2004). Multimodal model integration
sentence unit detection. Proceedings ICMI, pp. 121128.
Chen, L., Rose, R. T., Parrill, F., Han, X., Tu, J., Huang, Z., Harper, M., Quek, F., McNeill,
D., Tuttle, R., & Huang, T. (2005). VACE multimodal meeting corpus. Proceedings
Workshop Machine Learning Multimodal Interaction, pp. 4051.
Cohen, P. R., Johnston, M., McGee, D., Oviatt, S., Pittman, J., Smith, I., Chen, L., &
Clow, J. (1997). Quickset: Multimodal interaction distributed applications.
Proceedings ACM Multimedia, pp. 3140.
Darrell, T., & Pentland, A. (1993). Space-time gestures. Proceedings CVPR, pp.
335340.
Daume III, H. (2007). Frustratingly easy domain adaptation. Proceedings ACL,
pp. 256263.
Daume III, H., & Marcu, D. (2005). large-scale exploration effective global features
joint entity detection tracking model. Proceedings HLT-EMNLP, pp.
97104.
Deutscher, J., Blake, A., & Reid, I. (2000). Articulated body motion capture annealed
particle filtering. Proceedings CVPR, Vol. 2, pp. 126133.
Eisenstein, J., Barzilay, R., & Davis, R. (2007). Turning lectures comic books
linguistically salient gestures. Proceedings AAAI, pp. 877882.
Eisenstein, J., & Davis, R. (2006). Gesture improves coreference resolution. Proceedings
HLT-NAACL, Companion Volume: Short Papers, pp. 3740.
393

fiEisenstein, Barzilay, & Davis

Eisenstein, J., & Davis, R. (2007). Conditional modality fusion coreference resolution.
Proceedings ACL, pp. 352359.
Fayyad, U. M., & Irani, K. B. (1993). Multi-interval discretization continuous-valued
attributes classification learning. Proceedings IJCAI, Vol. 2, pp. 10221027.
Godfrey, J., Holliman, E., & McDaniel, J. (1992). Switchboard: Telephone speech corpus
research development. Proceedings IEEE Conference Acoustics, Speech,
Signal Processing (Vol. 1), pp. 517520.
Goodwin, M., & Goodwin, C. (1986). Gesture co-participation activity searching word. Semiotica, 62, 5175.
Grosz, B., Joshi, A. K., & Weinstein, S. (1995). Centering: framework modeling
local coherence discourse. Computational Linguistics, 21 (2), 203225.
Haghighi, A., & Klein, D. (2007). Unsupervised coreference resolution nonparametric
bayesian model. Proceedings ACL, pp. 848855.
Harabagiu, S. M., Bunescu, R. C., & Maiorano, S. J. (2001). Text knowledge mining
coreference resolution. Proceedings NAACL, pp. 18.
Hearst, M. A. (1994). Multi-paragraph segmentation expository text. Proceedings
ACL.
Hirschman, L., & Chinchor, N. (1998). MUC-7 coreference task definition. Proceedings
Message Understanding Conference.
Huang, X., Acero, A., & Hon, H.-W. (2001). Spoken Language Processing. Prentice Hall.
Huang, X., Alleva, F., Hwang, M.-Y., & Rosenfeld, R. (1993). overview SphinxII speech recognition system. Proceedings ARPA Human Language Technology
Workshop, pp. 8186.
Ji, H., Westbrook, D., & Grishman, R. (2005). Using semantic relations refine coreference
decisions. Proceedings HLT-EMNLP, pp. 1724.
Johnston, M., & Bangalore, S. (2000). Finite-state multimodal parsing understanding,.
Proceedings COLING-2000, pp. 369375.
Jordan, P., & Walker, M. (2005). Learning Content Selection Rules Generating Object
Descriptions Dialogue. Journal Artificial Intelligence Research, 24, 157194.
Kahn, J. G., Lease, M., Charniak, E., Johnson, M., & Ostendorf, M. (2005). Effective
use prosody parsing conversational speech. Proceedings HLT-EMNLP, pp.
233240.
Kameyama, M. (1986). property-sharing constraint Centering. Proceedings
ACL, pp. 200206.
Kehler, A. (2000). Cognitive status form reference multimodal human-computer
interaction. Proceedings AAAI, pp. 685690.
Kendon, A. (1980). Gesticulation speech: Two aspects process utterance.
Key, M. R. (Ed.), relation verbal non-verbal communication, pp.
207227. Mouton.

394

fiGesture Salience Hidden Variable

Kettebekov, S., Yeasin, M., & Sharma, R. (2005). Prosody based audiovisual coanalysis
coverbal gesture recognition. IEEE Transactions Multimedia, 7 (2), 234242.
Kibble, R., & Power, R. (2004). Optimising referential coherence text generation. Computational Linguistics, 30 (4), 401416.
Kim, J., Schwarm, S. E., & Osterdorf, M. (2004). Detecting structural metadata
decision trees transformation-based learning. Proceedings HLT-NAACL04.
Koo, T., & Collins, M. (2005). Hidden-variable models discriminative reranking.
Proceedings HLT-EMNLP, pp. 507514.
Kopp, S., Tepper, P., Ferriman, K., & Cassell, J. (2007). Trading spaces: humans
humanoids use speech gesture give directions. Nishida, T. (Ed.), Conversational Informatics: Engineering Approach. Wiley.
Kudo, T., & Matsumoto, Y. (2001). Chunking support vector machines. Proceedings
NAACL, pp. 18.
Lappin, S., & Leass, H. J. (1994). algorithm pronominal anaphora resolution. Computational Linguistics, 20 (4), 535561.
Lascarides, A., & Stone, M. (2006). Formal semantics iconic gesture. Proceedings
10th Workshop Semantics Pragmatics Dialogue (BRANDIAL), pp.
6471.
Lew, M. S., Sebe, N., Djeraba, C., & Jain, R. (2006). Content-based multimedia information retrieval: State art challenges. ACM Transactions Multimedia
Computing, Communications Applications, 2 (1), 119.
Li, X., & Roth, D. (2001). Exploring evidence shallow parsing. Proceedings CoNLL,
pp. 17.
Lin, J. (1991). Divergence measures based shannon entropy. IEEE Transactions
Information Theory, 37, 145151.
Liu, D. C., & Nocedal, J. (1989). limited memory BFGS method large scale
optimization. Mathematical Programming, 45, 503528.
Liu, T., & Kender, J. R. (2007). Computational approaches temporal sampling video
sequences. ACM Transactions Multimedia Computing, Communications Applications, 3 (2), 7.
Liu, Y. (2004). Structural Event Detection Rich Transcription Speech. Ph.D. thesis,
Purdue University.
Luo, X. (2005). coreference resolution performance metrics. Proceedings HLTEMNLP, pp. 2532.
Malioutov, I., & Barzilay, R. (2006). Minimum cut model spoken lecture segmentation.
Proceedings ACL, pp. 2532.
Mani, I., & Maybury, M. T. (Eds.). (1999). Advances Automatic Text Summarization.
MIT Press, Cambridge, MA, USA.
McCallum, A., & Wellner, B. (2004). Conditional models identity uncertainty
application noun coreference. Proceedings NIPS, pp. 905912.
395

fiEisenstein, Barzilay, & Davis

McNeill, D. (1992). Hand Mind. University Chicago Press.
Melinger, A., & Levelt, W. J. M. (2004). Gesture communicative intention
speaker. Gesture, 4 (2), 119141.
Muller, C., Rapp, S., & Strube, M. (2002). Applying Co-Training reference resolution.
Proceedings ACL, pp. 352359.
Muller, C. (2007). Resolving it, this, unrestricted multi-party dialog. Proceedings ACL, pp. 816823.
Nakano, Y., Reinstein, G., Stocky, T., & Cassell, J. (2003). Towards model face-to-face
grounding. Proceedings ACL, pp. 553561.
Ng, V. (2007). Shallow semantics coreference resolution. Proceedings IJCAI, pp.
16891694.
Ng, V., & Cardie, C. (2002). Improving machine learning approaches coreference resolution. Proceedings ACL, pp. 104111.
NIST (2003). Rich Transcription Fall 2003 (RT-03F) Evaluation plan..
Passonneau, R. J. (1997). Applying reliability metrics co-reference annotation. Tech.
rep. CUCS-017-97, Columbia University.
Poddar, I., Sethi, Y., Ozyildiz, E., & Sharma, R. (1998). Toward natural gesture/speech
HCI: case study weather narration. Proceedings Perceptual User Interfaces,
pp. 16.
Poesio, M., Stevenson, R., Eugenio, B. D., & Hitzeman, J. (2004). Centering: parametric
theory instantiations. Computational Linguistics, 30 (3), 309363.
Quattoni, A., Collins, M., & Darrell, T. (2004). Conditional random fields object recognition. Proceedings NIPS, pp. 10971104.
Quek, F., McNeill, D., Bryll, R., Duncan, S., Ma, X., Kirbas, C., McCullough, K. E., &
Ansari, R. (2002a). Multimodal human discourse: gesture speech. ACM Transactions Computer-Human Interaction, 9:3, 171193.
Quek, F., McNeill, D., Bryll, R., & Harper, M. (2002b). Gestural spatialization natural discourse segmentation. Proceedings International Conference Spoken
Language Processing, pp. 189192.
Quek, F., McNeill, D., Bryll, R., Kirbas, C., Arslan, H., McCullough, K. E., Furuyama, N.,
& Ansari, R. (2000). Gesture, speech, gaze cues discourse segmentation.
Proceedings CVPR, Vol. 2, pp. 247254.
Rabiner, L. R. (1989). tutorial hidden markov models selected applications
speech recognition. Proceedings IEEE, 77 (2), 257286.
Sarkar, A. (2001). Applying co-training methods statistical parsing. Proceedings
NAACL, pp. 18.
Sha, F., & Pereira, F. (2003). Shallow parsing conditional random fields. Proceedings
NAACL, pp. 134141.
Shriberg, E., Stolcke, A., Hakkani-Tur, D., & Tur, G. (2000). Prosody-based automatic
segmentation speech sentences topics. Speech Communication, 32.
396

fiGesture Salience Hidden Variable

Sidner, C. L. (1979). Towards computational theory definite anaphora comprehension
english discourse. Tech. rep. AITR-537, Massachusetts Institute Technology.
Soon, W. M., Ng, H. T., & Lim, D. C. Y. (2001). machine learning approach coreference
resolution noun phrases. Computational Linguistics, 27 (4), 521544.
Strube, M., & Hahn, U. (1999). Functional centering: grounding referential coherence
information structure. Computational Linguistics, 25 (3), 309344.
Strube, M., Rapp, S., & Muller, C. (2002). influence minimum edit distance
reference resolution. Proceedings EMNLP, pp. 312319.
Strube, M., & Muller, C. (2003). machine learning approach pronoun resolution
spoken dialogue. Proceedings ACL, pp. 168175.
Sundaram, H., & Chang, S.-F. (2003). Video analysis summarization structural
semantic levels. D. Feng, W. C. S., & Zhang, H. (Eds.), Multimedia Information
Retrieval Management: Technological Fundamentals Applications, pp. 7594.
Springer Verlag.
Sutton, C., & McCallum, A. (2006). introduction conditional random fields
relational learning. Getoor, L., & Taskar, B. (Eds.), Introduction Statistical
Relational Learning, pp. 95130. MIT Press.
Sutton, C., McCallum, A., & Rohanimanesh, K. (2007). Dynamic conditional random fields:
Factorized probabilistic models labeling segmenting sequence data. Journal
Machine Learning Research, 8, 693723.
Toyama, K., & Horvitz, E. (2000). Bayesian modality fusion: Probabilistic integration
multiple vision algorithms head tracking. Proceedings Asian Conference
Computer Vision (ACCV).
Uchihashi, S., Foote, J., Girgensohn, A., & Boreczky, J. (1999). Video manga: generating
semantically meaningful video summaries. Proceedings ACM MULTIMEDIA,
pp. 383392.
Vilain, M., Burger, J., Aberdeen, J., Connolly, D., & Hirschman, L. (1995). modeltheoretic coreference scoring scheme. Proceedings Message Understanding Conference, pp. 4552.
Walker, M., Joshi, A., & Prince, E. (Eds.). (1998). Centering Theory Discourse. Clarendon Press, Oxford.
Walker, M. A. (1998). Centering, anaphora resolution, discourse structure. Marilyn
A. Walker, A. K. J., & Prince, E. F. (Eds.), Centering Discourse, pp. 401435.
Oxford University Press.
Wang, S., Quattoni, A., Morency, L.-P., Demirdjian, D., & Darrell, T. (2006). Hidden
conditional random fields gesture recognition. Proceedings CVPR, Vol. 02,
pp. 15211527.
Xiong, Y., & Quek, F. (2006). Hand Motion Gesture Frequency Properties Multimodal
Discourse Analysis. International Journal Computer Vision, 69 (3), 353371.
Yang, X., Su, J., & Tan, C. L. (2005). Improving pronoun resolution using statistics-based
semantic compatibility information. Proceedings ACL, pp. 165172.
397

fiEisenstein, Barzilay, & Davis

Yang, X., Zhou, G., Su, J., & Tan, C. L. (2003). Coreference resolution using competition
learning approach. Proceedings ACL, pp. 176183.
Zhu, X., Fan, J., Elmagarmid, A., & Wu, X. (2003). Hierarchical video content description
summarization using unified semantic visual similarity. Multimedia Systems,
9 (1), 3153.

398

fiJournal Artificial Intelligence Research 31 (2008) 497-542

Submitted 08/07; published 03/08

Exploiting Subgraph Structure
Multi-Robot Path Planning
Malcolm R. K. Ryan

malcolmr@cse.unsw.edu.au

ARC Centre Excellence Autonomous Systems
University New South Wales, Australia

Abstract
Multi-robot path planning difficult due combinatorial explosion search
space every new robot added. Complete search combined state-space soon
becomes intractable. paper present novel form abstraction allows
us plan much efficiently. key abstraction partitioning
map subgraphs known structure entry exit restrictions
represent compactly. Planning becomes search much smaller space subgraph
configurations. abstract plan found, quickly resolved correct
(but possibly sub-optimal) concrete plan without need search. prove
technique sound complete demonstrate practical effectiveness
real map.
contending solution, prioritised planning, also evaluated shown similar
performance albeit cost completeness. two approaches necessarily
conflicting; demonstrate combined single algorithm outperforms either approach alone.

1. Introduction
many scenarios require large groups robots navigate around shared
environment. Examples include: delivery robots office (Hada & Takase, 2001),
warehouse (Everett, Gage, Gilbreth, Laird, & Smurlo, 1994), shipping yard (Alami, Fleury,
Herrb, Ingrand, & Robert, 1998), mine (Alarie & Gamache, 2002); even virtual
armies computer wargame (Buro & Furtak, 2004). case many robots
independent goals must traverse shared environment without colliding
one another. planning path single robot usually consider
rest world static, world represented graph called
road-map. path-planning problem amounts finding path road-map,
reasonably efficient algorithms exist. However, multi-robot scenario world
static. must avoid collisions obstacles, also robots.
Centralised methods (Barraquand & Latombe, 1991), treat robots single composite entity, scale poorly number robots increases. Decoupled methods
(LaValle & Hutchinson, 1998; Erdmann & Lozano-Perez, 1986), first plan
robot independently resolve conflicts afterwards, prove much faster incomplete many problems require robots deliberately detour optimal
path order let another robot pass. Even priority ordering used (van den Berg
& Overmars, 2005), requiring low priority robots plan avoid high-priority robots,
problems found cannot solved priority ordering.

c
2008
AI Access Foundation. rights reserved.

fiRyan

realistic maps common structures roads, corridors open spaces
produce particular topological features map constrain possible interactions robots. long narrow corridor, instance, may impossible one robot
overtake another robots must enter exit first-in/first-out order.
hand, large open space may permit many robots pass simultaneously
without collision.
characterise features particular kinds subgraphs occurring
road-map. decompose map collection simple subgraphs,
build plans hierarchically, first planning movements one subgraph another,
using special-purpose planners build paths within subgraph.
paper propose abstraction. limit considering
homogeneous group robots navigating using shared road-map. identify particular
kinds subgraphs road-map place known constraints ordering robots
pass them. use constraints make efficient planning algorithms
traversing kind subgraph, combine local planners hierarchical
planner solving arbitrary problems.
abstraction used implement centralised prioritised planners,
demonstrate paper. Unlike heuristic abstractions, method
sound complete. is, used centralised search guaranteed find
correct plan one exists. guarantee cannot made prioritised search
used, however two-stage planning process means prioritised planner
abstraction often find plans would available otherwise. Experimental
investigation shows approach effective maps sparsely connected
graph representations.

2. Problem Formulation
assume work provided road-map form graph
G = (V, E) representing connectivity free space single robot moving around
world (e.g. vertical cell decomposition visibility graph, LaValle, 2006). also
set robots R = {r1 , . . . , rk } shall consider homogeneous, single map
suffices all. shall assume starting locations goals lie road-map.
Further, shall assume map constructed collisions occur
one robot entering vertex v time another robot occupying, entering
leaving vertex. Robots occupying vertices map affect movement.
appropriate levels underlying control assumptions satisfied
real-world problems.
simple centralised approach computing plan proceeds follows: First, initialise
every robot starting position, select robot move neighbouring vertex,
checking first robot currently occupying vertex. Continue fashion, selecting moving one robots step goal. Pseudocode
process shown Algorithm 1. code presented non-deterministic algorithm, choice points indicated choose operator, backtracking required
fail command encountered. practice, search algorithm depth-first,
breadth-first A* search necessary evaluate alternative paths presents.

498

fiExploiting Subgraph Structure Multi-Robot Path Planning

Algorithm 1 simple centralised planning algorithm.
1: function Plan(G, a, b)
2:
= b
3:
return hi
4:
end
5:
choose r R
6:
select vf : a[vf ] = r
7:
choose vt {v | (vf , v) G}
8:
a[vt ] 6= 2
9:
fail
10:
else
11:
a[vf ] 2
12:
a[vt ] r
13:
return (r, vf , vt ).Plan(G, a, b)
14:
end
15: end function

. Build plan b graph G.
. Nothing do.
. Choose robot.
. Find location.
. Choose edge.
. destination occupied; backtrack.
. Move robot vf vt .
. Recurse.

algorithm complete search composite space Gk = G G G,
k = |R| robots. eliminating vertices represent collisions robots,
size composite graph given by:
fi
fi
fi
fi
fiV (Gk )fi = n Pk
=

n!
(n k)!

fi
fi
fi
k fi
fiE(G )fi = k |E(G)| (n2) P(k1)
= k |E(G)|

(n 2)!
(n k 1)!

n = |V (G)| k = |R|. running time algorithm depend
search algorithm used, expected long moderately large values
n k.

3. Subgraph Abstraction
Consider problem shown Figure 1. road-map contains 18 vertices 17 edges,
3 robots plan for. So, according formulae, composite
graph 18!/15! = 4896 vertices 3 17 16!/14! = 12240 edges. small map
expanded large search problem. human mind obvious lot
arrangements equivalent. important exact positions robots,
ordering.
Consider subgraph labeled X. recognise subgraph stack. is, robots
move subgraph last-in-first-out (LIFO) order. Robots inside
stack cannot change order without exiting re-entering stack.
goal reverse order robots X, know immediately cannot done
without moving robots stack re-enter opposite
order. robots right order, rearranging right positions
499

fiRyan

y1

x1

x2


x3

x4

x5

y2

y3

y4

y5

y6



z2

z3

z4

z5

z6

Z

x6

c

b

X

z1

Figure 1: planning problem illustrating use subgraphs.
trivial. Thus make distinction arrangement robots (in
specify exactly vertex robot occupies) configuration stack (in
interested order).
X 6 vertices, robots stack, 6 Pm =
6!/(6 m)! possible arrangements. total number arrangements is:
6

P3 + 3 6 P2 + 3 6 P1 + 6 P0 = 120 + 3 30 + 3 6 + 1
= 229

terms deciding whether robot leave stack, however, need know
order. need represent 3! + 3 2! + 3 1! + 1 = 16 different configurations
stack.
Subgraphs Z also stacks. Applying analysis three, find
represent abstract state space 60 different states, 144 possible
transitions states (moving top-most robot one stack onto another).
dramatically smaller composite map space above.
stack simple kind subgraph need larger collection canonical
subgraphs represent realistic problems. key features looking follows:
1. Computing transitions subgraph require knowledge exact arrangement robots within subgraph, abstract configuration
(in case, order).
2. two arrangements robots share configuration, transforming one
done easily without search,
3. Therefore planning need done configuration space, significantly
smaller.
Later introduce three subgraph types cliques, halls rings also
share properties readily found realistic planning problems. first
need formalise ideas subgraph planning.

500

fiExploiting Subgraph Structure Multi-Robot Path Planning

4. Definitions
section outline concepts use later paper. complete formal
definition terms provided Appendix, along proof soundness
completeness subgraph planning process.
Given map represented graph G partition set disjoint subgraphs
S1 , . . . , Sm . subgraphs induced, i.e. edge exists two vertices
subgraph also exists G.
arrangement robots G 1-to-1 partial function : V (G) R,
represents locations robots within G. robot r vertex v, write a(v) = r.
also speak arrangement robots within subgraph S. denote
arrangements lowercase roman letters a, b
configuration subgraph set equivalent arrangements robots within S.
Two arrangements equivalent exists plan move robots one
without robots leaving subgraph. denote configuration subgraph Sx
cx . configuration whole map represented tuple subgraph
configurations = (c1 , . . . , cm ).
two operators operate configurations, representing robot
entering leaving subgraph respectively. robot r moves two subgraphs Sx Sy configurations change depending identity edge (u, v)
robot traveled. write:
c0x cx (r, u),
c0y cy (r, v)
complex subgraphs possible transition result several possible configurations, operators return sets. also possible transition
impossible particular configuration, case operation returns empty
set.
abstract plan defined sequence transitions intermediate
configurations . every abstract plan two arrangements exists least one
corresponding concrete plan, vice versa. subgraph transitions concrete
plan must also exist abstract plan. equivalence arrangements configuration
guarantees existence intermediate steps. See Appendix complete
proof.

5. Subgraph Planning
construct planning algorithm searches space abstract plans (Algorithm 2). procedure much before. First compute configuration
tuple initial arrangement. extend plan one step time. step
consists selecting robot r moving subgraph currently occupies Sx
neighbouring subgraph Sy reduced graph X, along connecting edge (u, v).
transition possible plan-step (s, (u, v)) applicable. is, may
result number different configurations subgraph entered. need choose
one create configuration tuple next step. applicability test
selection subsequent configurations performed lines 10-11 AbstractPlan.
501

fiRyan

abstract plan extended step step fashion reaches configuration
tuple matches goal arrangement. resulting abstract plan resolved
concrete plan. transition abstract plan build two short concrete plans
one move robot outgoing vertex transition, one make sure
incoming vertex clear subgraph appropriately arranged create subsequent
configuration. Since two plans separate subgraphs, combined
parallel. final step rearrange robots goal arrangement. Again,
done parallel subgraphs.
AbstractPlan written non-deterministic program, including choicepoints. search algorithm breadth-first depth-first search needed examine
possible set choices ordered fashion. search complete
abstract plan guaranteed found, one exists theorem
planning algorithm sound complete. Note resolution phase
planner entirely deterministic, search needed abstract plan
found.
5.1 Subgraph Methods
efficiency algorithm relies able compute several functions without
lot search:
Exit compute c (r, u), testing possible robot exit subgraph
determining resulting configuration(s).
Enter compute c (r, v), testing possible robot enter subgraph
determining resulting configuration(s).
Terminate compute b/S c, testing possible robots subgraph
move terminating positions.
ResolveExit build plan rearranging robots subgraph allow one exit.
ResolveEnter build plan rearranging robots subgraph allow one
enter.
ResolveTerminate build plan rearranging robots subgraph
terminating positions.
key efficient subgraph planning carefully constrain allowed structure
subgraphs partition, functions simple implement
require expensive search. advantage approach functions
always computed based arrangement robots within particular
subgraph, relying positions robots elsewhere.

6. Subgraph Structures
key process therefore selection subgraph types. abstractions
need chosen that:
502

fiExploiting Subgraph Structure Multi-Robot Path Planning

v1

v2

v3

v1

vk

v2

(a) stack

v3

vk

(b) hall

v1

v2

v1

v2

v4

v3

v4

v3

(c) clique

(d) ring

Figure 2: Examples four different subgraph structures.

1. commonly occurring real road-maps,
2. easy detect extract road-map,
3. abstract large portion search space,
4. Computing legality transitions fast, sound complete,
5. Resolving abstract plan concrete sequence movements efficient.
paper present four subgraph types: stacks, halls, cliques rings, satisfy
requirements. following analysis, let n number vertices
subgraph k number robots occupying subgraph action takes
place.
6.1 Stacks
stack (Figure 2(a)) represents narrow dead-end corridor road-map.
one exit narrow robots pass one another, robots must enter leave
last-in-first-out order. one simplest subgraphs occur often
real maps, serves easy illustration subgraph methods. Formally
consists chain vertices, linked predecessor successor.
vertex one end chain, called head, connected subgraphs
entrances exits happen there.
configuration stack corresponds ordering robots reside it,
head down. Robots stack cannot pass other, ordering cannot
changed without robots exiting re-entering stack.

503

fiRyan

6.1.1 Enter
robot always enter stack long stack full. one new configuration created, adding robot front ordering. computation
done O(1) time.
6.1.2 Exit
robot exit stack top robot ordering. one new
configuration created, removing robot ordering. computation also
done O(1) time.
6.1.3 Terminate
determine whether termination possible, need check order robots
current configuration terminating arrangement. operation
takes O(k) time.
6.1.4 ResolveEnter
Rearranging robots inside stack simple since know ordering constant.
vacate top stack (the possible entrance point) move robots deeper
stack (as necessary). guaranteed room, since entering full stack
permitted. worst takes O(k) time.
6.1.5 ResolveExit
robot exits stack, abstract planner already determined
first robot stack others head vertex. simply move
stack head, out. robots need moved. worst
takes O(n) time.
6.1.6 ResolveTerminate
Finally, moving robots terminating positions done top-to-bottom order.
robot terminating position move upwards without interference.
robot terminating position, robots may need moved lower
order clear path. approach sound, since terminating positions
robots must stack (or else ordering would different). process
O(nk) total worst-case running time.
6.2 Halls
hall generalisation stack (Figure 2(b)). Like stack, narrow corridor
permit passing, hall may multiple entrances exits along
length. Formally consists single chain vertices, one joined predecessor
successor. must edges vertices hall, may
edges connecting subgraphs node hall. Halls much
commonly occurring structures, still maintain property stacks: robots

504

fiExploiting Subgraph Structure Multi-Robot Path Planning

j=0
v1

v2

v3

v4

v5


v6
B

C


j=1
v1

v2

v3

v4

v5
B



v6
C


j=2
v1


v2

v3

v4

B

v5

v6

C



Figure 3: Example entering hall subgraph, k = 3, n = 6 = 3. Robot
enter three possible sequence positions j = 0, 1 2 j = 3.

cannot reordered without exiting re-entering. Thus, stacks, configuration
hall corresponds order robots occupying it, one end hall
other.
6.2.1 Enter
robot enter hall long full. configurations generated
entrance depend three factors: 1) size hall n, 2) number robots
already hall k, 3) index vertex enters (ranging 1
n).
Figure 3 shows entering hall result several different configurations.
matter robots already hall arranged, left right
entrance, entering robot moves in. enough space hall either
side entrance vertex, new robot inserted point ordering.
space limited (as example) may possible move robots
one side another, limiting possible insertion points.
Given three variables k, n, above, compute maximum minimum
insertion points as:
j min(i 1, k)
j max(0, k (n i))

505

fiRyan

Creating new configuration matter inserting new robot
ordering appropriate point. Since list robots needs copied order
this, takes O(k) time new configuration.
6.2.2 Exit
Whether robot exit hall via given edge depends several factors: 1)
size hall n, 2) number robots hall k, 3) index vertex
exits (from 1 k), 3) index j robot ordering (from 1 k). Exit
possible if:
j n (k j)
exit possible one resulting configuration: previous ordering robot
removed. takes O(k) time compute.
6.2.3 Terminate
Checking termination halls stacks, test order
robots final arrangement matches current configuration. done
O(k) time k robots hall.
6.2.4 ResolveEnter
resolve entrance hall need know subsequent configurations
aiming generate, know proper insertion point entering robot.
robots insertion point shuffled one direction one side
entry vertex, rest side. worst take O(nk) time.
6.2.5 ResolveExit
Resolving exit involves moving robot hall exit vertex, shuffling
robots way. worst case, robots shuffle
one end hall other, takes O(nk) time.
6.2.6 ResolveTerminate
ResolveTerminate hall identical stack, described above.
6.3 Cliques
clique (Figure 2(c)) represents large open area map many exit points
(vertices) around perimeter. Robots cross directly vertex another,
long clique full, robots inside shuffled way allow
happen.
Formally clique totally connected subgraph. Cliques quite different properties
halls stacks. long least one empty vertex clique, possible
rearrange arbitrarily. configuration clique, circumstance,
set robots contains.

506

fiExploiting Subgraph Structure Multi-Robot Path Planning

However special set configurations clique locked.
occurs number robots clique equals number vertices.
impossible clique rearranged. configuration locked clique
explicitly record position robot.
6.3.1 Enter
clique always entered long full. clique one
vacant vertex, single new configuration entering robot added
set occupants. clique one space remaining, entering robot locks
clique. theory, point necessary make new configuration every
possible arrangement occupying robots (with entering robot always vertex
enters).
practice, efficient create single locked configuration
records locking robot vertex, leaves positions unspecified.
permutation robots possible, exact details configuration need
pinned next action (either Exit Terminate) requires be.
form least commitment, significantly reduce branching factor
search.
Performing test creating new configuration takes O(k) time k robots
clique.
6.3.2 Exit
clique unlocked robot exit vertex new configuration
created simply removing robot set occupants.
clique locked robot exit specific vertex occupies. resulting configuration unlocked exact locations robots
discarded.
least-commitment version, locking robot constrained exit vertex
every robot exit vertex except one occupied locking
robot.
Performing test creating new configuration takes O(k) time k robots
clique.
6.3.3 Terminate
unlocked configuration, checking termination simply consists making sure
(and only) required occupants clique. locked configuration
robots must terminating positions (as possibility rearranging
them). least-commitment version locking robot must terminating vertex. assume robots also places (thus
committing choice configuration delayed earlier).
Performing test takes O(k) time k robots clique.

507

fiRyan

6.3.4 ResolveEnter
entrance vertex occupied robot wishes enter simply move
occupant directly another vacant vertex clique, since every vertex connected
every other.
using least commitment entering robot locks clique need
look ahead plan see next action involving clique. exit transition
need move exiting robot exit vertex (before clique locked).
subsequent exit, meaning robots terminating clique,
need rearrange terminating positions point.
amortise cost rearrangements subsequent call ResolveExit
ResolveTerminate treat operation taking O(1) time.
6.3.5 ResolveExit
clique full time exit assume exiting robot already
exit vertex nothing needs done. hand, clique full
may robot exit vertex. must moved there. exit vertex
already occupied another robot, moved another unoccupied vertex.
movements done directly, clique totally connected. operation
takes O(1) time.
6.3.6 ResolveTerminate
clique locked assume robots already appropriately
arranged terminal positions work needs done. Otherwise
robots may need rearranged. simple way proceed follows:
robot place, first vacate terminating position moving occupant
another unoccupied vertex, move terminating robot vertex. robot
moved way move again, process correct
may produce longer plans necessary. upside takes O(n) time.
6.4 Rings
ring (Figure 2(d)) resembles hall ends connected. Formally, subgraph
vertices V (S) = {v1 , . . . , vn } induced edges E(S) satisfying:
(vi , vj ) E(S) iff |i j| 1 (mod n)
hall, ordering important ring. Robots ring cannot pass one another
cannot re-order themselves. can, however, rotate ordering (provided
ring full). Thus ring size 4 more, sequence hr1 , r2 , r3 equivalent
hr3 , r1 , r2 hr2 , r1 , r3 i. Equivalent sequences represent configuration.
Like cliques, rings locked full. locked ring cannot rotated,
ring size three sequences hr1 , r2 , r3 hr3 , r1 , r2 equivalent.
represent two locked configurations different properties.

508

fiExploiting Subgraph Structure Multi-Robot Path Planning

6.4.1 Enter
robot may always enter ring provided full. k robots already
occupying ring, k possible configurations result (or one k
zero), one possible insertion point.
entering robot locks ring must also record specific positions
robot ring. still produce k different configurations
robots cannot arbitrarily rearranged, unlike cliques.
also possible produce least-commitment versions Enter rings
cliques. Again, significantly reduce branching factor search,
details involved wish enter paper.
operation takes O(k) time new configuration generated.
6.4.2 Exit
ring locked robot exit recorded position, otherwise
exit vertex. robot removed sequence produce resultant
configuration. new configuration unlocked position information
discarded. done O(k) time k robots ring.
6.4.3 Terminate
check termination possible need see order robots around ring
terminal arrangement matches current configuration. configuration
locked rotations allowed, otherwise match must exact. test
done O(k) time k robots ring.
6.4.4 ResolveEnter
robot enter ring, need first rearrange entry
vertex empty nearest robots either side vertex provide correct
insertion point subsequent configuration, selected Enter above. may
require shuffling robots one way another, much fashion stack
hall. worst case take O(nk) operations k robots ring n vertices.
6.4.5 ResolveExit
ring locked robot exiting must already exit position nothing needs
done. Otherwise, unlocked ring, robots may need shuffled around
ring order move robot exit. worst case take O(nk) operations
k robots ring n vertices.
6.4.6 ResolveTerminate
ring locked robots must already terminating positions;
guaranteed abstract planner. Otherwise need rotated correct
positions. one robot moved correct vertex, rest ring
treated stack ResolveTerminate method described used,
O(nk) worst case running time k robots ring n vertices.
509

fiRyan

6.5 Summary
four subgraphs halls rings powerful. subgraphs
common structured maps man-made environments, also found often
purely random graphs (consider: shortest path unweighted graph hall).
Halls, rings cliques significant size found many realistic planning problems.
Importantly, structures well constrained enough six procedures
planning outlined implemented efficiently deterministically, without
need search. cases clique ring, resolution methods
describe sometimes sacrifice path optimality speed, could improved
using smarter resolution planners. Since resolution stage done once, probably
would major effect overall running time planner.

7. Prioritised Planning
common solution rapid growth search spaces multi-robot planning prioritised
planning (Erdmann & Lozano-Perez, 1986; van den Berg & Overmars, 2005).
approach give robots fixed priority ordering begin. Planning performed
priority ordering: first plan built robot highest priority; plan
second highest, interfere first; third,
on. new plan must constructed interfere plans
it. example implementation shown Algorithm 3. Usually backtracking
plan made. signified algorithm cut operator line 8
Plan.
cut, search longer complete. problems solutions
prioritised planner cannot find. Figure 4 example. Robots b wish
change positions. plan either robot easy; plan contains one
step. plan robots together requires move way,
right hand side map pass. prioritised planner
committed one-step plan either b cannot construct plan
robot interfere.
incompleteness mistake, however. core makes prioritised planning efficient. search space pruned significantly eliminating

x1

x2


x3

x4

b


Figure 4: simple planning problem cannot solved naive prioritised planning.
goal swap positions robots b.

510

fiExploiting Subgraph Structure Multi-Robot Path Planning

certain plans consideration. still viable solution within pruned space
(and often is) found much quickly. (hopefully few) cases
fails, always resort complete planner backup.
7.1 Prioritised Subgraph Planning
Prioritised planning strictly competitor subgraph planning. fact, prioritised
search subgraph representation orthogonal ideas, quite possible use
together. Algorithm 3, plan constructed robot consecutively,
rather building entire concrete plan, abstract version produced,
fashion Algorithm 2 earlier. compatible abstract plans produced
every robot, resolved concrete plan.
well adding advantage abstraction prioritised planning, subgraph
representation also allows planner cover space possible plans.
delaying resolution end, avoid commitment concrete choices high
priority robot hamper planning later robots.
illustrate this, lets return example Figure 4 above. partition
subgraph vertices {x1 , x2 , x3 , x4 } hall X, prioritised subgraph planner
solve problem. abstract plan highest priority robot empty;
nothing already goal subgraph. Given plan, second highest
priority robot plan move X back again. plan produce
goal configuration required. Resolving plan move highest priority robot
x4 back needed, plan built Resolve methods halls,
search.
course thing free lunch example works choose
right partition. instead treat {x1 , x2 } stack {x4 , x3 , y} separate hall
prioritised subgraph planner help us. Furthermore exist problems,
one Figure 5 solved standard prioritised planners
fail introduce wrong subgraph abstraction. difficult generate realistic

x1

x2

x3



x4
b



Figure 5: simple planning problem solved naive prioritised planning
subgraph abstraction. goal swap positions robots
b. priority ordering a, b subgraph planner choose robot
remain inside hall. Robot b trapped, blocks
exit (note edges (x1 , y) (y, x4 ) directed).

511

fiRyan

cases problem small numbers robots, see Section 9.3 below,
occur number robots large.

8. Search Complexity
Let us consider carefully advantages (if any) subgraph decomposition lie. Subgraph transitions act macro-operators one abstract state (set
configurations) another. long history planners using macros one kind
another, advantages disadvantages well known (see Section10.1).
widely recognised macros advantageous reduce depth search,
become disadvantage many macros created branching factor
search becomes large. guidelines also apply use subgraphs.
typical search algorithm proceeds follows: select plan frontier incomplete plans create expansions. Add expansions frontier recurse
complete plan found. time taken complete search determined
number nodes search tree, turn determined three factors:
1. d, depth goal state,
2. b, average branching factor tree, i.e. number nodes generated per
node expanded
3. efficiency search.
perfect search algorithm, heads directly goal, nevertheless contain O(bd)
nodes alternative nodes must still generated, even never followed.
uninformed breadth-first search, hand, generate O(bd ) nodes.
regarded sensible upper bound efficiency search (although possible
worse).
Macro-operators tend decrease expense increasing b, well
uninformed search dominates, show less advantage good heuristic exists,
b equally important. becomes important consider keep
increases branching factor minimum. case subgraph planning,
two main reasons b increases:
1. reduced graph may larger average degree original. Since
subgraph contains many vertices, tends out-going edges single
vertex. edges connect different subgraphs, branching factor
significantly larger. Sparse subgraphs (such halls) worse regard
dense subgraphs (such cliques). subgraph decomposition needs chosen
carefully avoid problem.
2. single subgraph transition may create large number possible configurations,
robot enters large hall already occupied several robots.
cases may strictly matter configuration generated
possible use least commitment avoid creating unnecessary alternatives,
possibility different configurations result different outcomes

512

fiExploiting Subgraph Structure Multi-Robot Path Planning

track, need considered. Halls particular
problem.
see experiments follow, careful choice subgraph decomposition important avoid pitfalls, appropriate partition abstraction
significantly improve informed uninformed search.

9. Experiments
empirically test advantages subgraph approach, ran several experiments
real randomly generated problems. first experiment demonstrates
algorithms scale changes size problem, terms number vertices,
edges robots, standard breadth-first search. second experiment shows
results affected using heuristic guide search. experiments
use randomly generated graphs. final experiment demonstrates algorithm
realistic problem.
first two experiments, maps generated randomly automatically partitioned subgraphs. Random generation done follows: first spanning tree
generated adding vertices one one, connecting randomly selected vertex
graph. edges required generated randomly selecting two
non-adjacent vertices creating edge them. edges undirected.1
Automated partitioning worked follows:
1. Initially mark vertices unused.
2. Select pair adjacent unused vertices.
3. Use pair basis growing hall, ring clique:
Hall: Randomly add unused vertices adjacent either end hall, provided
violate hall property. Continue growth possible.
Ring: Randomly add unused vertices adjacent either end ring loop
created. Discard vertices involved loop.
Clique: Randomly add unused vertices adjacent every vertex clique. Continue growth possible.
4. Keep biggest three generated subgraphs. Mark vertices used.
5. Go back step 2, adjacent unused pairs found.
6. remaining unused vertices singletons.
intended ideal algorithm. results far optimal fast
effective. Experience suggests partition generated approach contain
twice many subgraphs one crafted hand, makes effort minimise
degree reduced graph, even randomly generated partitions
advantages subgraph abstraction apparent.
1. noted algorithm generate uniform distribution connected graphs
given size, difficult generate sparse connected graphs uniform distribution.
bias deemed significant.

513

fiRyan

4.0
Original
Reduced

30

3.5

degree

# subgraphs

3.0
20

2.5

2.0
10
1.5

0

1.0
10

20

30

40

50

60

70

80

90

100

10

# vertices

20

30

40

50

60

70

80

90

100

# vertices

Figure 6: results automatic partitioning program Experiment 1a. left
graph shows average number subgraphs generated right graph
shows average degree reduced graph.

9.1 Experiment 1: Scaling Problem Size
9.1.1 Scaling |V |
first experiment investigate effect scaling number vertices
graph search time. Random graphs generated number vertices
ranging 10 100. Edges added average degree = |E|/|V | always
equal 3. (This value seems typical realistic maps.) One hundred graphs
generated size, one partitioned using method described above.
Figure 6 shows performance auto-partitioning. see, number
subgraphs increased roughly linearly size graph, average subgraph
size 4. small graphs (with fewer 40 vertices) reduced graph partitioning
sparser original, size increases average degree reduced graph
gets larger. results presented informative purposes only. make claims
quality partitioning algorithm, indeed reducing
size graph, small factor.
graph, three robots given randomly selected initial final locations,
plan generated. Figure 7(a) shows average run times four approaches.2 shows clear performance hierarchy. complete planners significantly
slower priority planners, cases subgraph abstraction shows significant improvement naive alternative. Nevertheless, every case combinatorial
growth runtime apparent (note graph plotted log scale). linear
relationship number vertices number subgraphs prevents subgraph
2. noted times overall rather slow. acknowledge attribute
implementation, Java heavily optimised avoid garbage collection.
currently working implementation optimised search engine, believe
results still provide valuable comparison methods.

514

fiExploiting Subgraph Structure Multi-Robot Path Planning

1000000

100000

Time (ms)

10000

1000

100
Naive complete
Naive priority
Subgraph complete
Subgraph priority

10

1
10

20

30

40

50

60

70

80

90

100

# vertices
(a) run times
8
Naive complete
Naive priority
Subgraph complete
Subgraph priority

7

Naive complete
Naive priority
Subgraph complete
Subgraph priority

30

5

path length

branching factor

6

4

20

3
10

2
1
0

0
10

20

30

40

50

60

70

80

90

100

10

# vertices

20

30

40

50

60

70

80

90

100

# vertices

(b) branching factor

(c) goal depth

Figure 7: results Experiment 1a. graph (a) boxes show first third
quartile whiskers show complete range. experiment failed
complete due time memory limits incompleteness search, run
time treated infinite. value plotted cases 50%
experiments failed. graph (c) goal depth naive complete
subgraph priority approaches identical graphs 30 60 vertices,
lines overlap. naive complete planner could solve problems
60 vertices.

515

fiRyan

Table 1: number planning failures recorded two prioritised planning approaches Experiment 1a.
# Failures
Vertices Naive Subgraph
10
2
0
20 - 70
0
0
80
1
0
90 - 100
0
0

approaches better this. better partitioning algorithm ameliorate
problem.
analyse causes variation run times, need consider search
process carefully. measure search depth average branching factor b
experiment. results plotted Figure 7(b) (c). expected,
subgraph abstraction used, goal depth decreased grows slowly,
branching factor increased. Since uninformed search, dominates
overall result improvement planning time.
incompleteness prioritised planning shows Table 1. three occasions
naive prioritised search failed find available solutions. However problem
prioritised subgraph search.
9.1.2 Scaling |E|
Next examine effect graph density. Fixing number vertices 30,
generated random graphs average degree ranging 2.0 4.0. value,
100 graphs randomly generated automatically partitioned. planning
problem move three robots selected initial goal locations.
results experiment shown Figure 8. appear much
overall change run times approaches, small improvement
naive prioritised planner graph gets denser. Figures 8(b) (c) show
expected result: increasing density graph increases branching factor
decreases depth. appears affect four approaches similarly.
interesting difference, however, shown Table 2. records percentage
experiments prioritised planners unable find solution.
sparse graphs, naive planner failed many 10% problems, improved
quickly density increased. subgraph abstraction added, planner able
solve two problems. case find problems solved
naive planner subgraph planner.
9.1.3 Scaling |R|
last scaling experiments, investigate approach performs
varying numbers robots. before, 100 random graphs generated partitioned,
30 vertices average degree 3, one partitioned using
516

fiExploiting Subgraph Structure Multi-Robot Path Planning

10000

Time (ms)

1000

100

10
Naive complete
Naive priority
Subgraph complete
Subgraph priority
1
2.0

2.2

2.4

2.6

2.8

3.0

3.2

3.4

3.6

3.8

4.0

degree
(a) run times
8

40

6

Naive complete
Naive priority
Subgraph complete
Subgraph priority

30
path length

branching factor

7

Naive complete
Naive priority
Subgraph complete
Subgraph priority

5
4

20

3
10
2
1
2.0

2.2

2.4

2.6

2.8

3.0

3.2

3.4

3.6

3.8

4.0

0
2.0

2.2

2.4

2.6

2.8

degree

3.0

3.2

degree

(b) branching factor

(c) goal depth

Figure 8: results Experiment 1b.

517

3.4

3.6

3.8

4.0

fiRyan

100000

Time (ms)

10000

1000

100

Naive complete
Naive priority
Subgraph complete
Subgraph priority

10

1
1

2

3

4

5

6

7

8

9

10

# robots
(a) run times
Naive complete
Naive priority
Subgraph complete
Subgraph priority

7

Naive complete
Naive priority
Subgraph complete
Subgraph priority

1000

5

100
path length

branching factor

6

4

3

10

2

1

1
1

2

3

4

5

6

7

8

9

10

1

2

3

4

# robots

5

6
# robots

(b) branching factor

(c) goal depth

Figure 9: results Experiment 1c.

518

7

8

9

10

fiExploiting Subgraph Structure Multi-Robot Path Planning

Table 2: number planning failures recorded two prioritised planning approaches Experiment 1b.
# Failures
Degree Naive Subgraph
2.0
10
0
2.2
8
0
2.4
5
0
2.6
1
1
2.8
0
0
3.0
2
0
3.2
1
1
3.4 - 4.0
0
0

Table 3: number planning failures recorded two prioritised planning approaches Experiment 1c.
# Failures
# Robots Naive Subgraph
1-3
0
0
4
3
0
5
4
0
6
10
0
7
7
1
8
7
1
9
26
0
10
46
1

automatic partitioning algorithm. Ten planning problems set graph
number robots varying 1 10. case initial goal locations selected
randomly.
running times four approaches plotted Figure 9(a). major
performance difference prioritised non-prioritised planners, prioritised planners able handle twice many robots. two complete-search
approaches, subgraph abstraction unnecessary overhead small problems,
shows significant advantage number robots increases.
less obvious advantage subgraph abstraction case prioritised
planning, look failure rates shown Table 3. number robots
increases incompleteness naive prioritised algorithm begins become apparent,
10 robots see 46% problems could solved planner.
advantage subgraph abstraction apparent: total 3 problems
could solved 1000 tried.

519

fiRyan

Figures 9(b) (c) plot average branching factor goal depth problems.
previous experiments, subgraph abstraction seen increase branching
factor decrease depth. complete search approaches branching factor
grows rapidly number robots, node search path contains choice
robot move. prioritised approach reverses trend, planning
ever done one robot time, later robots much heavily constrained
options available them, providing fewer alternatives search tree.
9.1.4 Discussion
summarise experiments, advantages subgraph abstraction twofold. Firstly, decreases necessary search depth planning problem compressing
many robot movements single abstract step. Like macro-based abstractions,
expense increasing branching factor gains seem outweigh
losses practice. course, dependent degree use uninformed
search, shall address below.
advantage specific prioritised planner. tightly constrained problems sparse maps and/or many robots incompleteness naive prioritised
search becomes significant issue. addition subgraph abstraction
number failures dramatically reduced, without additional search.
9.2 Experiment 2: Heuristic Search
experiments far involved uninformed breadth-first search without use
heuristic. such, runtime algorithms strongly affected changes
search depth branching factor. explained above, uninformed search
O(bd ) expected running time. However perfect heuristic reduce O(bd),
making branching factor much significant aspect. perfect heuristic is, course,
unavailable, possible efficiently compute reasonably good search heuristic
task relaxing problem. Disregarding collisions simply compute sum
shortest path lengths robots location goal. underestimate
actual path length, accurate loosely constrained problems (with robots
dense graphs).
experiment used best-first search algorithm guided heuristic.3
every node search tree, selected plan minimised value. case
subgraph planner, actual locations robots time-point specified,
subgraph occupy, heuristic calculated using maximum distances
vertex robots subgraph goal. pre-computed shortest path
distances every pair nodes running planner, time
computation counted runtime algorithm.
utility heuristic depends largely constrained problem is.
graph dense relatively robots, heuristic direct planner
quickly goal. However graph sparser, interactions robots
become important, heuristic less useful. reason,
3. A* algorithm used, desire minimise length solution, find
solution quickly possible.

520

fiExploiting Subgraph Structure Multi-Robot Path Planning

concentrate attention experiment varying density graph affects
performance different approaches.
Random maps 200 vertices generated, average degree ranging 2
3. One hundred graphs generated size partitioned using algorithm
described earlier. Figure 10 shows results. original graph gets denser,
number subgraphs decreases, mostly possible create longer halls.
good, fewer subgraphs mean shorter paths, consequential increase degree
adversely affect branching factor.
Ten robots placed randomly graph assigned random goal locations.
four planning approaches applied problems. resulting run-times
plotted Figure 11(a). first thing apparent graph
distinction different approaches greatly reduced. size graph
number robots much larger previous experiments,
corresponding effect goal depth branching factor (Figure 11(b) (c)),
run-times much smaller, clearly heuristic effective guiding search.
average ratio search nodes expanded goal depth close 1.0
experiments, slight increase constrained cases, conclude
heuristic close perfect.
compare four approaches see three distinct stages. constrained case, 200 edges, see subgraph approaches outperforming either naive
approach, small benefit prioritised search complete search. 220 edges
pattern changed. two prioritised methods significantly better two
complete approaches. number edges increases, naive methods continue
improve, prioritised subgraph search holds steady complete subgraph search
gets significantly worse (due rapid increase branching factor). 300 edges
naive approaches significantly better subgraph approaches.


Singletons
Halls
Cliques
Rings

60

4.5

Original
Reduced

4.0

50

degree

# subgraphs

3.5
40
30

3.0
2.5

20

2.0

10

1.5

0

1.0
200

210

220

230

240

250

260

270

280

290

300

200

# edges

210

220

230

240

250

260

270

280

290

# edges

(a) subgraphs

(b) degree

Figure 10: results auto-partitioner graphs Experiment 2.

521

300

fiRyan

10000

Time (ms)

1000

100
Naive complete
Naive priority
Subgraph complete
Subgraph priority
10
200

210

220

230

240

250

260

270

280

290

300

edges
(a) run times
120

Naive complete
Naive priority
Subgraph complete
Subgraph priority

400

Naive complete
Naive priority
Subgraph complete
Subgraph priority

300
80

path length

branching factor

100

60

200

40
100
20
0
200

220

240

260

280

300

0
200

220

240

edges

260
edges

(b) branching factor

(c) goal depth

Figure 11: results Experiment 2.

522

280

300

fiExploiting Subgraph Structure Multi-Robot Path Planning

Table 4: number planning failures recorded two prioritised planning approaches Experiment 2.
# Failures
# Edges Naive Subgraph
200
14
0
210
2
0
220
0
0
230
0
0
240
1
0
250 - 300
0
0

cause clearly seen Figures 11(b) (c). branching factors subgraph
approaches increase significantly faster naive approaches, corresponding
improvement goal depth sufficient outweigh cost.
benefits subgraph abstraction highly constrained cases also shown
failure cases (Table 4). 200 edges naive prioritised search unable solve 10%
problems, prioritised search subgraphs could solve all. number
failures fell quickly density graph increased.
9.2.1 Discussion
graph becomes moderately dense interactions robots become few,
total-single-robot-paths measure becomes near perfect heuristic. makes
branching factor much critical factor using uninformed search.
auto-partitioning algorithm use poor job limiting factor
subgraph approaches perform poorly.
Better results could achieved better decomposition, clear whether
could found random graph without excessive computation. Certainly partitioning graphs hand easy task. Realistic graphs, hand, generally
shaped natural constraints (e.g. rooms, doors corridors) make decomposition
much simpler, see following experiment.
9.3 Experiment 3: Indoor Map
Figure 12 shows map final two experiments, based floor-plan Level 4
K17 building University New South Wales. road-map 113 vertices 308
edges drawn (by hand) connecting offices open-plan desk locations.
imagined might used map delivery task involving team
medium-sized robots.
road-map partitioned 47 subgraphs 11 cliques, 7 halls 1 ring,
plus 28 remaining singleton nodes (subgraphs containing one vertex). average

523

fiRyan

Figure 12: map Experiment 3. Vertices coloured subgraph.

524

fiExploiting Subgraph Structure Multi-Robot Path Planning

10000
Naive complete
Naive priority
Subgraph complete
Subgraph priority

Time (ms)

1000

100

10
1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

robots

Figure 13: Comparing run times Experiment 3.
degree reduced graph 2.1, compared 2.7 original.4 Partitioning done
hand aid interactive GUI performed simple graph analysis
offered recommendations (by indicating nodes could added hall clique
user creating). road-map clearly laid partitioning mind
deciding partitioning whole difficult. Large open spaces generally
became cliques. Corridors became halls rings. foyer area (around vertex 94)
caused particular trouble finding ideal partitioning, due slightly unusual
topology.5
series experiments run world, varying number robots 1
20. experiment 100 runs performed robot placed
random office desk required make delivery another random office
desk (chosen without replacement, two robots goal). Plans built
using complete prioritised planners without subgraph abstraction.
four approaches utilised total single-robot shortest path heuristic previous
experiment. running times algorithm shown Figure 13.
see small numbers robots (1 2) naive approaches significantly better subgraph approaches. overhead subgraph search
outweighs disadvantages simple problems. number robots increases
subgraph methods take over, around 9 16 robots subgraph methods
significant better either naive approach. 17 robots combination complete
search subgraphs begins perform less well two prioritised approaches
best performers, considerable advantage subgraph approach.
4. comparison, auto-partitioner yielded partition fewer subgraphs (avg. 41.8) higher
degree (avg. 2.25).
5. curious, empty rooms centre map, near vertex 91, bathrooms.
consider robots would need make deliveries there.

525

fiRyan

Naive complete
Naive priority
Subgraph complete
Subgraph priority

1.8
1.7

expanded / path

1.6
1.5
1.4
1.3
1.2
1.1
1.0
1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20
robots

Figure 14: Assessing quality heuristic Experiment 3. value plotted
ratio number expanded nodes search tree goal depth.
perfect heuristic yields value 1.0.

Considering search complexity, let us first examine performance heuristic.
Figure 14 plots ratio average number expanded nodes search tree
goal depth. perfect heuristic, value 1.0, experiment
11 robots. 11 robots heuristic begins become inaccurate.
inaccuracy seems affect complete planners badly prioritised ones,
cases subgraph approach seriously affected naive approach.
explain difference, note heuristic using contains significantly less
information subgraph search naive search. know exactly
robot within subgraph, assume worst possible position.
means value configuration tuple based solely allocation robots
subgraphs, particular configurations subgraphs. Hall subgraphs
particular may several different configurations set robots,
assigned heuristic value despite significantly different real distances
goal.This creates plateau heuristic function broadens search. large
numbers robots permutations become significant factor search. improve
heuristic need find way distinguish value different configurations
subgraph. probably require extra method specific subgraph structure.
graphs branching factor goal depth (Figure 15) show come
expect branching factor larger complete search prioritised search
subgraph abstraction makes worse. Significantly, branching factor prioritised
526

fiExploiting Subgraph Structure Multi-Robot Path Planning

Naive complete
Naive priority
Subgraph complete
Subgraph priority

50

Naive complete
Naive priority
Subgraph complete
Subgraph priority

2000

path length

branching factor

40

30

20

1000

10

0

0
1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20

1

robots

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20
robots

(a) branching factor

(b) goal depth

Figure 15: branching factor goal depth Experiment 3.
Table 5: number planning failures recorded two prioritised planning approaches Experiment 3.
# Failures
Edges Naive Subgraph
1-9
0
0
10 - 19
0
1
20
0
2

search increase robots added, step plan
one robot moved. goal depth shows opposite pattern, complete searches
shorter prioritised searches subgraph abstraction approximately halves
search depth cases.
Failure rates recorded Table 5. story different previous
experiments. naive prioritised planner able solve problems every depth,
adding subgraph abstraction caused small number failures complex
problems. clear caused reversal. cases involved complex
elude analysis. problem warrants investigation.
9.3.1 Discussion
experiment shown realistic problem appropriately chosen set
subgraphs subgraph abstraction effective way reduce search even
good heuristic available. subgraph abstraction work well
example, compared random graphs Experiment 2? answer seems found
degree reduced graph. Automatically partitioning random graph significantly
increases degree, saw Figure 10(b). This, turn, increases branching factor
thus search time.

527

fiRyan

contrast, partition realistic map decreased degree graph
2.7 2.1 (by hand) 2.25 (automatically). branching factor subgraph
methods still larger (as one transition still create multiple configurations)
effect reduced enough overcome decrease goal depth. indication
realistic map structure exploited abstraction.
investigation warranted characterise features many possible.

10. Conclusion
demonstrated new kind abstract representation multi-robot path planning
allows much faster planning without sacrificing completeness. Decomposing
road-map subgraphs simple intuitive way providing background knowledge
planner efficiently exploited. key find subgraph structures
allow us treat many arrangements robots equivalent configurations compute
transitions configurations quickly deterministically. described
four structures paper: stacks, halls, cliques rings. structures
simple enough compute configurations easily also common enough found
many realistic maps.
shown abstract plans subgraphs resolved deterministically
concrete plans without need search. planner sound complete,
although plans produced necessarily optimal. Future work could prove
worth spending time resolution phase trim unnecessarily wasteful plans,
using, example, simulated annealing (Sanchez, Ramos, & Frausto, 1999). may
time saved abstract planning leaves us space clever resolution.
conventional solution search-space explosion multi-robot planning prioritisation. shown subgraph-based planning competitive
prioritised planning also combination two methods powerful still
cases, partly alleviates incompleteness prioritised approach.
10.1 Related Work
Abstraction hierarchical decomposition standard techniques planning
related search problems. use macro-operators dates back far Sacerdotis early
work Abstrips planning system (Sacerdoti, 1974) introduced abstraction
hierarchies, whereby problem could first solved high level abstraction
ignoring lower-level details. idea re-expressed many different ways
history planning far many review detail. present work particularly
inspired generic types Long Fox (2002) similarly detected
substructures task-planning problem solved using structure-specific planners.
Hierarchical planning applied path-planning abstractions
approximate cell decomposition (Barbehenn & Hutchinson, 1995; Conte & Zulli, 1995),
generalised Voronoi graphs (Choset & Burdick, 1995; Choset, 1996) general ad-hoc
hierarchical maps (Bakker, Zivkovic, & Krose, 2005; Zivkovic, Bakker, & Krose, 2005, 2006),
structures identified examples carry well multi-robot
scenario.

528

fiExploiting Subgraph Structure Multi-Robot Path Planning

faster solutions multi-robot problem available assume
existence garage locations robot (LaValle & Hutchinson, 1998) kinds
temporary free space (Sharma & Aloimonos, 1992; Fitch, Butler, & Rus, 2003).
method present makes assumption thus general application.
appear previous work provides complete abstraction-based
planner general multi-robot problem.
work bears similarity explicitly robot path planning,
solving Sokoban puzzle (Botea, Muller, & Schaeffer, 2003; Junghanns & Schaeffer, 2001). domain significantly constrained (the map necessarily
orthogonal grid stones move pushed man)
method employ similar. Dividing map rooms tunnels use
strongly-connected-component algorithm identify equivalent arrangements boulders subpart. Equivalent arrangements treated single abstract state
corresponding configuration formulation used state global
search. particular structures represent different, general ideas partitioning independent local subproblems identifying abstract states strongly
connected components, employed work.
10.2 Future Plans
next stage project plan examine symmetries provided subgraph
representation. Recent work symbolic task-planning (Porteous, Long, & Fox, 2004)
shown recognising exploiting symmetries almost-symmetries planning
problems eliminate large amounts search. Subgraph configurations provide natural
ground similar work problem domain expect similar improvements
possible.
also plan investigate problem automatic subgraph partitioning
maps. identified importance trading path depth branching
factor, plan make partitioning algorithm chooses subgraphs optimise
relationship. Automatically finding optimal partition could hard, creating
powerful interactive partitioning tool human operator would seem viable
compromise. One approach would adapt auto-partitioner describe
paper seed vertices selected user, allowed choose
number possible subgraphs based selection.
subgraph structures also identified, currently working
formalising properties tree-structured subgraphs. Another possibility would
generalise cliques rings new ring-with-chords structure, although characterising
structure may prove difficult.
many advances search technology may applicable
multi-robot planning problem. currently process re-expressing
entire problem constraint satisfaction problem (CSP) Gecode constraint engine
(Gecode Team, 2006). believe CSP formulation powerful way take
advantage structural knowledge subgraph decomposition represents.

Acknowledgments
529

fiRyan

Id like thank Jonathan Paxman, Brad Tonkes Maurice Pagnucco help
developing ideas paper proofreading drafts.

Appendix A. Proof Soundness Completeness
appendix set necessary formal definitions prove soundness
completeness abstract planning process. main result theorem showing
abstract plan exists given problem concrete plan also exists.
A.1 Graphs Subgraphs
induced subgraph G graph = (V (S), E(S))
V (S) V (G)

E(S) = {(u, v) | u, v V (S), (u, v) E(G)}

Intuitively describes subgraph consisting subset vertices connecting edges parent graph. Thus induced subgraph specified solely
terms vertices. shall henceforth assume subgraphs refer induced.
partition P G set {S1 , . . . , Sm } subgraphs G satisfying
[
V (G) =
V (Si )

V (Si ) V (Sj ) = , i, j : 6= j
i=1...m

Given graph G partition P construct reduced graph X G
contracting subgraph single vertex
V (X) = P
E(X) = {(Si , Sj ) | x Si , Sj : (x, y) G}
A.2 Robots Arrangements
Let us assume set robots R. arrangement robots graph G
1-to-1 partial function : V (G) R. arrangement represents locations robots
within G. a(v) = r, robot r vertex v. shall use notation a(v) = 2
indicate undefined v, i.e. vertex v unoccupied. arrangement may
necessarily include every robot R. Two arrangements b said disjoint
range(a) range(b) = . Let AG represent set arrangements R G.
subgraph G, arrangement R G define a/S,
induced arrangement R S,
a/S(v) = a(v), v V (S)
S1 S2 disjoint subgraphs G disjoint arrangements a1 S1 a2
S2 , define combined arrangement = a1 a2 arrangement S1 S2
satisfying
(
a1 (v) v S1
a(v) =
a2 (v) v S2

530

fiExploiting Subgraph Structure Multi-Robot Path Planning

Lemma 1 arrangement G partition P = {S1 , . . . , Sm } {a1 , . . . , }
set induced arrangements ai = a/Si , combined arrangement a1 =
a.
Given identity, uniquely identify arrangement G combination
induced arrangements partition P.
A.3 Concrete Plans
need define means move robots around graph. First define
two operators respectively add remove robots given arrangement.
Formally : AG R V (G) AG mapping satisfies
G

(r, v) = b
G



(
r
b(u) =
a(u)

u = v
otherwise

Similarly : AG R AG mapping satisfies
G

r =b
G



(
2
b(u) =
a(u)

a(u) = r
otherwise

omit subscript G clear context.
define plan-step R E(G) G robot/edge pair (r, u, v),
representing movement r along edge u v, u 6= v. plan-step
applicable arrangement AG iff a(u) = r a(v) = 2. case apply
produce new arrangement b = s(a)
s(a) = (a r) (r, v)
concrete plan (or plan) G AG b AG sequence plan-steps
hs1 , . . . , sl exist arrangements a0 , . . . , al AG si applicable ai1

a0 =
al = b
ai = si (ai1 ), : 0 < l
Lemma 2 subgraph G P plan P also plan G.
Lemma 3 P plan G b Q plan G b c,
concatenation P Q, written P.Q plan G c.
531

fiRyan

Lemma 4 Let P kQ denote set interleavings sequences P Q. Let S1
S2 disjoint subgraphs G, P1 plan S1 a1 b1 P2 plan S2
a2 b2 , a1 a2 disjoint. arbitrary interleaving P P1 kP2 plan
G a1 a2 b1 b2 .
A.4 Configurations
defined machinery concrete plans, introduce abstraction. key
idea configuration abstraction arrangements. robots
subgraph rearranged one arrangement another, without robots
leave subgraph rearrangement, two arrangements
treated equivalent. Configurations represent sets equivalent arrangements
subgraph. So, example, stack subgraph configuration set arrangements
ordering robots. arrangement entire partitioned graph
abstracted list configurations produces subgraphs.
Formally, define configuration relation graph G equivalence relation
G

AG b iff exists plans Pab Pba G b b
G

respectively.
configuration c G equivalence class . write c = (a) represent
G

G

equivalence class containing arrangement a. Let CG set configurations
G.
Lemma 5 b range(a) = range(b)
G

Given identity, unambiguously define range configuration c
range(c) = range(a), c
extend definitions configurations. c CG
configuration G, r R v V (G)


c (r, v) = (a (r, v)) | c, a(v) = 2
G
G
G


c (r, v) = (a r) | c, a(v) = r
G

G

G

Note map configurations sets configurations.6
Given partition P = {S1 , . . . , Sm } G corresponding set configuration
relations { , . . . , } define configuration tuple R G tuple (c1 , . . . , cm )
S1

Sm

: ci CSi ,
[

range(ci ) = R

i=1...m

range(ci ) range(cj ) = , i, j : 6= j
6. Astute readers notice c (r, v) never contains one element, although may
G

empty.

532

fiExploiting Subgraph Structure Multi-Robot Path Planning

configuration tuple represents abstract state robots entire graph,
terms configurations individual subgraphs partition. Given arrangement G construct corresponding configuration tuple (a) = (c1 , . . . , cm )
ci = (a/Si ). Conversely, a/Si ci ci , write .
Si

Lemma 6 b arrangements graph G partition {S1 , . . . , Sm }
configuration tuple G a, b , exists plan b G.
Proof
= 1 . . . m, let ai = a/Si bi = b/Si . ai ci bi ci
ai bi . Therefore definition exists plan Pi ai bi Si .
Si

Let P P1 k . . . kPm . Since Pi plans disjoint subgraphs, P plan
a1 = b1 bm = b required.

A.5 Abstract Plans
configuration tuples abstract state representation, define abstract
plans, sequences subgraph transitions plan steps move robot one
subgraph another. prove main result section, abstract
plan problem exists corresponding concrete plan exists. allow
us later prove soundness completeness subgraph planning algorithm.
rest section shall assume graph G partition P =
{S1 , . . . , Sm } corresponding configuration relations { , . . . , }.
S1

Sm

subgraph transition (or transition) plan-step = (r, u, v) u Sx ,
v Sy Sx 6= Sy . transition = (r, u, v) applicable configuration tuple
= (c1 , . . . , cm ) G
cx (r, u) 6= , u Sx ,
Sx

cy (r, v) 6= , v Sy .
Sy

is, robots Sx rearranged robot r leave via u robots
Sy rearranged v empty r enter.
transition = (r, u, v) applicable = (c1 , . . . , cm ) u Sx v Sy
apply compute set s() configuration-tuples
(c01 , . . . c0m ) s()

c0x cx (r, u),
Sx



c0y

cy (r, v),

c0z

= cz , otherwise.

Sy

Lemma 7 arrangement G partition {S1 , . . . , Sm } transition =
(r, u, v) applicable also applicable (a),
(s(a)) s((a))
533

fiRyan

Proof Let Sx , Sy disjoint subgraphs partition u Sx , v Sy . Let
ax = a/Sx ay = a/Sy . Let (a) = (c1 , . . . , cm ).
ax cx
ax (u) = r
cx (r, u) 6= .
similarly
ay cy
ay (v) = 2
cy (r, v) 6= .
Therefore applicable (a).
Further, let b = s(a) (b) = (c01 , . . . , c0m ).
c0x = (b/Sx )
Sx

= (ax r)
Sx

cx (r, u)

c0y = (b/Sy )
Sy

= (ay (r, v))
Sy

cy (r, v)

c0z = cz .
Therefore (b) s() required.



Lemma 8 = (r, u, v) u, v Sx (i.e. transition) arrangement G applicable a, (a) = (s(a)).
Proof
Let b = s(a). Let ai = a/Si bi = b/Si = 1 . . . m. Let (a) =
(c1 , . . . , cm ) (b) = (c01 , . . . , c0m ).
plan Px = hsi plan ax bx Sx , ax bx implying cx = c0x .
z 6= x, az = bz cz = c0z . Therefore (a) = (b) required.


534

fiExploiting Subgraph Structure Multi-Robot Path Planning

define abstract plan arrangement G tuple (, )
sequence configuration tuples h0 , . . . , l sequence plan steps
hs1 , . . . , sl i,
0 = (),
l = (),
si applicable i1 ,
s(i1 ).
Theorem 1 abstract plan G exists exists corresponding concrete plan P G.
Proof Case ( P ):
Let = (, ) abstract plan G , = h0 , . . . , l
= hs1 , . . . , sl i. Let = (ci0 , . . . , cim ).
shall construct concrete plan
P = P0 . hs1 .P1 . .Pl1 . hsl .Pl
Pi concrete plan ai bi , satisfying
a0 = ,
bl = ,
ai , bi ,
si+1 applicable bi ,
ai+1 = si+1 (bi ), = 0 . . . l 1.
Proposition 1 ai bi exist satisfying conditions = 1 . . . l.
Proof induction:
a0 = therefore a0 exists.
Assume ai exists:
Let si+1 = (r, u, v) u Sx v Sy . definition abstract plan,
si+1 applicable , i+1 = si+1 (i ). Therefore
ci+1
cix (r, u) 6=
x
n


ci+1

(a

(r,
u))
|
a(u)
=
r,


c
x
x 6=
cix : a(u) = r
Set bix equal a.
ci+1
= (bix (r, u))
x
bix (r, u) ci+1
x

535

fiRyan

Also
ci+1
ciy (r, v) 6=

n


ci+1

(a

(r,
v))
|
a(v)
=
2,


c

6=
ciy : a(v) = 2
Set biy equal a.
ci+1
= (biy (r, v))

biy (r, v) ci+1

Set biz = ai /Sz z
/ {x, y}
bij defined every subgraph Sj partition G. Therefore bi = bi1 bim
exists arrangement G.
ai exists bi also exists = 0 . . . l 1.
si+1 applicable bi since
bi (u) = bix (u) = r
bi (v) = biy (v) = 2
ai+1 = si+1 (bi ) exists,
ai+1 /Sx = bix r ci+1
x
ai+1 /Sy = biy (r, v) ci+1

ai+1 /Sz = biz , z
/ {x, y} ciz
ci+1
z

ai+1 i+1
induction, ai exists = 0 . . . l bi exists = 0 . . . l 1.
Furthermore bl = () = l , bi exists = 0 . . . l, required.

Proposition 2 concrete plan Pi ai bi exists, = 0, . . . , l
Proof

Since ai , bi plan Pi must exist ai bi , Lemma 6 above.



Proposition 3 P concrete plan G.
Proof
Pi plan ai bi = 0, . . . , l. Furthermore ai+1 = si+1 (bi ),
hsi+1 plan bi ai+1 . Therefore concatenation plans
P = P0 . hs1 . . hsl .Pl
plan G a0 = bl = , required.
536



fiExploiting Subgraph Structure Multi-Robot Path Planning

Case (P ):
Let P = hs1 , . . . , sL concrete plan G. wish construct
abstract plan = (, ) G.
Let = ht0 , . . . , tl increasing sequence integers t0 = 0 ti = iff st
subgraph transition. (Note: using capital L designate length concrete
plan P lowercase l designate number transitions plan,
length corresponding abstract plan .)
construct sequence arrangements = h0 , . . . , L
0 =
i+1 = si+1 (i ), = 0 . . . L 1
split subsequences A0 , . . . , Al


ff
Ai = ti , . . . , ti+1 1
Define = (ti ), = 0, . . . , l, = h0 , . . . , l = hst1 , . . . , stl i.
Proposition 4 : Ai
Proof induction:
definition,
ti (ti ) =
assume = ti + j, j < |Ai | 1. need prove t+1 .
Let st+1 = (r, u, v). Since + 1
/ must u, v Sx . using Lemma 8
(t+1 ) = (st+1 (t ))
= (t )
= .
Therefore, induction
, Ai
required.



Proposition 5 = (, ) valid abstract plan .
Proof
First check initial final configuration-tuples contain
respectively:
0 = (0 ) = ().

Al
l ,
l = ().

537

fiRyan

Now, = 0 . . . l 1 let bi = ti+1 1 (i.e. final element Ai ), let
= bi /Sz z = 1 . . . m.
Let = sti+1 = (r, u, v) u Sx v Sy . applicable bi
definition P . Therefore, Lemma 7 above, applicable

biz

i+1 = (ai+1 )
= (s(bi ))
s((bi )) = s(i ), required.

Therefore valid abstract plan.



theorem significant planning problem. tells us need
perform search concrete plans. Instead, need search abstract plan
convert concrete form. search succeed concrete
plan exists.

References
Alami, R., Fleury, S., Herrb, M., Ingrand, F., & Robert, F. (1998). Multi-robot cooperation
MARTHA project. Robotics & Automation Magazine, IEEE, 5 (1), 3647.
Alarie, S., & Gamache, M. (2002). Overview Solution Strategies Used Truck Dispatching Systems Open Pit Mines. International Journal Surface Mining, Reclamation Environment, 16 (1), 5976.
Bakker, B., Zivkovic, Z., & Krose, B. (2005). Hierarchical dynamic programming robot
path planning. Proceedings IEEE/RSJ International Conference Intelligent
Robots Systems, 27562761.
Barbehenn, M., & Hutchinson, S. (1995). Efficient search hierarchical motion planning
dynamically maintaining single-source shortest paths trees. IEEE transactions
robotics automation, 11 (2), 198214.
Barraquand, J., & Latombe, J.-C. (1991). Robot motion planning: distributed representation approach. International Journal Robotics Research, 10 (6), 628649.
Botea, A., Muller, M., & Schaeffer, J. (2003). Using abstraction planning sokoban.
Computers Games: Lecture Notes Computer Science, Vol. 2883, pp. 360375.
Springer.
Buro, M., & Furtak, T. (2004). RTS games real-time AI research. Proceedings
Behavior Representation Modeling Simulation Conference (BRIMS), Arlington
VA 2004, 5158.
Choset, H. (1996). Sensor based motion planning: hierarchical generalized voronoi
graph. Ph.D. thesis, California Institute Technology, Pasadena, California.
Choset, H., & Burdick, J. (1995). Sensor based planning. I. generalized Voronoi graph.
Proceedings International Conference Robotics utomation, 2.

538

fiExploiting Subgraph Structure Multi-Robot Path Planning

Conte, G., & Zulli, R. (1995). Hierarchical path planning multi-robot environment
simple navigation function. IEEE Transactions Systems, Man Cybernetics,
25 (4), 651654.
Erdmann, M., & Lozano-Perez, T. (1986). Multiple Moving Objects. Tech. rep. 883,
M.I.T. AI Laboratory.
Everett, H., Gage, D., Gilbreth, G., Laird, R., & Smurlo, R. (1994). Real-world issues
warehouse navigation. Proceedings SPIE Conference Mobile Robots IX,
2352.
Fitch, R., Butler, Z., & Rus, D. (2003). Reconfiguration planning heterogeneous selfreconfiguring robots. Proceedings IEEE/RSJ International Conference Intelligent Robots Systems, 3, 24602467.
Gecode Team (2006). Gecode: Generic constraint development environment,. Available
http://www.gecode.org.
Hada, Y., & Takase, K. (2001). Multiple mobile robot navigation using indoor global
positioning system (iGPS). Proceedings IEEE/RSJ International Conference
Intelligent Robots Systems, 2.
Junghanns, A., & Schaeffer, J. (2001). Sokoban: Enhancing general single-agent search
methods using domain knowledge. Artificial Intelligence, 129 (1-2), 219251.
LaValle, S. M. (2006). Planning Algorithms. Cambridge University Press.
LaValle, S. M., & Hutchinson, S. A. (1998). Optimal Motion Planning Multiple Robots
Independent Goals. IEEE Transactions Robotics Automation,
Vol. 14.
Long, D., & Fox, M. (2002). Planning Generic Types, chap. 4, pp. 103138. Morgan
Kaufmann.
Porteous, J., Long, D., & Fox, M. (2004). Identification Exploitation Almost
Symmetry Planning Problems. Brown, K. (Ed.), Proceedings 23rd UK
Planning Scheduling SIG.
Sacerdoti, E. (1974). Planning hierarchy abstraction spaces. Artificial Intelligence,
5 (2), 115135.
Sanchez, G., Ramos, F., & Frausto, J. (1999). Locally-Optimal Path Planning Using
Probabilistic Road Maps Simulatead Annealing. Proceedings IASTED International Conference Robotics Applications.
Sharma, R., & Aloimonos, Y. (1992). Coordinated motion planning: warehousemans
problem constraints free space. IEEE Transactions Systems, Man
Cybernetics, 22 (1), 130141.
van den Berg, J., & Overmars, M. (2005). Prioritized Motion Planning Multiple Robots.
Proceedings IEEE/RSJ International Conference Intelligent Robots Systems, pp. 430435.
Zivkovic, Z., Bakker, B., & Krose, B. (2005). Hierarchical map building using visual landmarks geometric constraints. Proceedings IEEE/RSJ International Conference
Intelligent Robots Systems, 24802485.
539

fiRyan

Zivkovic, Z., Bakker, B., & Krose, B. (2006). Hierarchical Map Building Planning based
Graph Partitioning. IEEE International Conference Robotics Automation.

540

fiExploiting Subgraph Structure Multi-Robot Path Planning

Algorithm 2 Planning subgraph abstraction.
1: function Plan(G, P, R, a, b)
2:
(a)
3:
(b)
4:
AbstractPlan(G, P, R, , )
5:
P Resolve(G, P, , a, b)
6:
return P
7: end function

. Build plan b G using partition P.
. Get initial configuration.
. Get final configuration.
. Build abstract plan.
. Resolve concrete plan.

1: function AbstractPlan(G, P, R, , ) . Build abstract plan G using P.
2:
=
3:
return (hi , hi)
. Done.
4:
end
5:
(c1 , . . . , cm ) =
6:
choose r R
. Choose robot.
7:
select x : r range(cx )
. Find subgraph occupies.
8:
choose Sy P : (Sx , Sy ) X
. Choose neighbouring subgraph.
9:
choose (u, v) E(G) : u Sx , v Sy
. Choose connecting edge.
. Choose resulting configurations Sx Sy .
10:
choose c0x cx (r, u)
11:
choose c0y cy (r, v)
12:
(c1 , . . . , c0x , . . . , c0y , . . . , cm )
. Construct new configuration tuple.
13:
(, ) AbstractPlan(G, P, R, , )
. Recurse.
14:
0 .
15:
0 (r, u, v).
16:
return (0 , 0 )
17: end function
1: function Resolve(G, P, , a, b)
. Resolve abstract plan concrete plan.
2:
= (, )
3:
= h0 , . . . , l
4:
= hs1 , . . . , sl
5:
P hi
6:
a0
7:
= 0 . . . (l 1)
8:
(r, u, v) = si+1
. next transition.
9:
(c01 , . . . , c0m ) = i+1
. target configurations.
10:
find Sx : u Sx
11:
find Sy : v Sy
12:
aiz ai /Sz , z = 1 . . .
13:
(Pxi , bix ) Sx .ResolveExit(aix , r, u, c0x )
. Rearrange Sx let robot r exit.
. Rearrange Sy let robot r enter.
14:
(Pyi , biy ) Sy .ResolveEnter(aiy , r, v, c0y )
15:
P P.(Pxi ||Pyi )
16:
bi = ai1 . . . bix . . . biy . . . aim
17:
ai+1 si+1 (bi )
18:
P P.. hsi+1
. Add transition.
19:
end
20:
z = 1 . . .
21:
Tz Sz .ResolveTerminate(al /Sz , b/Sz )
. Rearrange Sz final arrangement.
22:
end
23:
P P.(T1 || . . . ||Tm )
24:
return P
25: end function

541

fiRyan

Algorithm 3 simple prioritised planning algorithm.
1: function Plan(G, a, b)
2:
a0 [v] 2, v G
. a0 initial arrangement robots
0
3:
b [v] 2, v G
. b0 final arrangement robots
4:
= 1 . . . k
5:
a0 [v] = ri , v : a[v] = ri
6:
b0 [v] = ri , v : b[v] = ri
7:
(P, Pi ) PlanOne(G, ri , hP1 , . . . , Pi1 , h0, . . . , 0i , a0 , b0 )
. Build plan
8:
cut
. backtrack plan
9:
end
10:
return P
11: end function

r1 . . . ri .
r1 . . . ri .

r1 . . . ri .
found

1: function PlanOne(G, ri , hP1 , . . . , Pi1 , ht1 , . . . , ti1 , a, b)
2:
= b
3:
return (hi, hi)
. Done.
4:
end
5:
choose rj R : j
. Choose robot move.
6:
j =
7:
select vf : a[vf ] = ri
8:
choose vt {v | (vf , v) G}
. Choose new action ri
9:
else
10:
(r, vf , vt ) Pj [tj ]
. Select old action rj Pj
11:
tj tj + 1
12:
end
13:
a[vt ] 6= 2
14:
fail
. Backtrack destination occupied.
15:
end
16:
a[vf ] 2
. Move robot.
17:
a[vt ] r
18:
(P, Pi ) PlanOne(G, ri , hP1 , . . . , Pi1 , ht1 , . . . , ti1 , a, b)
. Recurse.
19:
P (rj , vf , vt ).P
. Add step global plan.
20:
j =
21:
Pi (ri , vf , vt ).Pri
. Add step ri plan.
22:
end
23:
return (P, Pi )
24: end function

542

fiJournal Artificial Intelligence Research 31 (2008) 319-351

Submitted 09/07; published 02/08

Complexity Planning Problems
Simple Causal Graphs
Omer Gimenez

omer.gimenez@upc.edu

Dept. de Llenguatges Sistemes Informatics
Universitat Politecnica de Catalunya
Jordi Girona, 1-3
08034 Barcelona, Spain

Anders Jonsson

anders.jonsson@upf.edu

Dept. Information Communication Technologies
Passeig de Circumvallacio, 8
08003 Barcelona, Spain

Abstract
present three new complexity results classes planning problems simple
causal graphs. First, describe polynomial-time algorithm uses macros generate plans class 3S planning problems binary state variables acyclic
causal graphs. implies plan generation may tractable even planning
problem exponentially long minimal solution. also prove problem
plan existence planning problems multi-valued variables chain causal graphs
NP-hard. Finally, show plan existence planning problems binary state
variables polytree causal graphs NP-complete.

1. Introduction
Planning area research artificial intelligence aims achieve autonomous
control complex systems. Formally, planning problem obtain sequence
transformations moving system initial state goal state, given description
possible transformations. Planning algorithms successfully used variety
applications, including robotics, process planning, information gathering, autonomous
agents spacecraft mission control. Research planning seen significant progress
last ten years, part due establishment International Planning
Competition.
important aspect research planning classify complexity solving
planning problems. able classify planning problem according complexity
makes possible select right tool solving it. Researchers usually distinguish
two problems: plan generation, problem generating sequence transformations achieving goal, plan existence, problem determining whether
sequence exists. original STRIPS formalism used, plan existence undecidable first-order case (Chapman, 1987) PSPACE-complete propositional
case (Bylander, 1994). Using PDDL, representation language used International
Planning Competition, plan existence EXPSPACE-complete (Erol, Nau, & Subrahmanian, 1995). However, planning problems usually exhibit structure makes much
c
2008
AI Access Foundation. rights reserved.

fiGimenez & Jonsson

easier solve. Helmert (2003) showed many benchmark problems used
International Planning Competition fact P NP.
common type structure researchers used characterize planning problems called causal graph (Knoblock, 1994). causal graph planning
problem graph captures degree independence among state variables
problem, easily constructed given description problem transformations. independence state variables exploited devise algorithms
efficiently solving planning problem. causal graph used tool
describing tractable subclasses planning problems (Brafman & Domshlak, 2003; Jonsson
& Backstrom, 1998; Williams & Nayak, 1997), decomposing planning problems
smaller problems (Brafman & Domshlak, 2006; Jonsson, 2007; Knoblock, 1994),
basis domain-independent heuristics guide search valid plan (Helmert,
2006).
present work explore computational complexity solving planning problems simple causal graphs. present new results three classes planning problems studied literature: class 3S (Jonsson & Backstrom, 1998), class Cn
(Domshlak & Dinitz, 2001), class planning problems polytree causal graphs
(Brafman & Domshlak, 2003). brief, show plan generation instances
first class solved polynomial time using macros, plan existence
solvable polynomial time remaining two classes, unless P = NP. work first
appeared conference paper (Gimenez & Jonsson, 2007); current paper provides
detail additional insights well new sections plan length CP-nets.
planning problem belongs class 3S causal graph acyclic state
variables either static, symmetrically reversible splitting (see Section 3 precise definition terms). class 3S introduced studied Jonsson
Backstrom (1998) example class plan existence easy (there exists
polynomial-time algorithm determines whether particular planning problem
class solvable) plan generation hard (there exists polynomial-time algorithm generates valid plan every planning problem class). precisely,
Jonsson Backstrom showed planning problems class 3S
every valid plan exponentially long. clearly prevents existence efficient
plan generation algorithm.
first contribution show plan generation 3S fact easy
allowed express valid plan using macros. macro simply sequence operators
macros. present polynomial-time algorithm produces valid plans
form planning problems class 3S. Namely, algorithm outputs polynomial
time system macros that, executed, produce actual valid plan planning
problem instance. algorithm sound complete, is, generates valid plan
one exists. contrast algorithm incremental algorithm proposed
Jonsson Backstrom (1998), polynomial size output.
also investigate complexity class Cn planning problems multivalued state variables chain causal graphs. words, causal graph
directed path. Domshlak Dinitz (2001) showed solvable instances
class require exponentially long plans. However, case class 3S,
could exist efficient procedure generating valid plans Cn instances using
320

fiComplexity Planning Problems

macros novel idea. show plan existence Cn NP-hard, hence
ruling efficient procedure exists, unless P = NP.
also prove plan existence planning problems whose causal graph polytree (i.e., underlying undirected graph acyclic) NP-complete, even restrict
problems binary variables. result closes complexity gap appears Brafman Domshlak (2003) regarding planning problems binary variables. authors
show plan existence NP-complete planning problems singly connected causal
graphs, plan generation polynomial planning problems polytree causal
graphs bounded indegree. use reduction prove similar problem
polytree CP-nets (Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004) NP-complete.
1.1 Related Work
Several researchers used causal graph devise algorithms solving planning
problems study complexity planning problems. Knoblock (1994) used
causal graph decompose planning problem hierarchy increasingly abstract
problems. certain conditions, solving hierarchy abstract problems easier
solving original problem. Williams Nayak (1997) introduced several restrictions
planning problems ensure tractability, one causal graph
acyclic. Jonsson Backstrom (1998) defined class 3S planning problems,
also requires causal graphs acyclic, showed plan existence polynomial
class.
Domshlak Dinitz (2001) analyzed complexity several classes planning
problems acyclic causal graphs. Brafman Domshlak (2003) designed polynomialtime algorithm solving planning problems binary state variables acyclic causal
graph bounded indegree. Brafman Domshlak (2006) identified conditions
possible factorize planning problem several subproblems solve
subproblems independently. claimed planning problem suitable
factorization causal graph bounded tree-width.
idea using macros planning almost old planning (Fikes & Nilsson,
1971). Minton (1985) developed algorithm measures utility plan fragments
stores macros deemed useful. Korf (1987) showed macros
exponentially reduce search space size planning problem chosen carefully. Vidal
(2004) used relaxed plans generated computing heuristics produce macros
contribute solution planning problems. Macro-FF (Botea, Enzenberger, Muller,
& Schaeffer, 2005), algorithm identifies caches macros, competed fourth
International Planning Competition. authors showed macros help reduce
search effort necessary generate valid plan.
Jonsson (2007) described algorithm uses macros generate plans planning
problems tree-reducible causal graphs. exist planning problems
algorithm generate exponentially long solutions polynomial time, like algorithm 3S. Unlike ours, algorithm handle multi-valued variables, enables
solve problems Towers Hanoi. However, planning problems 3S
tree-reducible causal graphs, algorithm cannot used show plan generation
3S polynomial.
321

fiGimenez & Jonsson

1.2 Hardness Plan Length
contribution paper show plan generation may polynomial even
planning problems exponential length minimal solutions, provided solutions may
expressed using concise notation macros. motivate result
discuss consequences. Previously, thought plan generation planning
problems exponential length minimal solutions harder NP, since
known whether problems NP intractable, certain cannot generate
exponential length output polynomial time.
However, planning problem exponential length minimal solution,
clear plan generation inherently hard, difficulty lies fact
plan long. Consider two functional problems
f1 (F ) = w(1, 2|F | ),
f2 (F ) = w(t(F ), 2|F | ),
F 3-CNF formula, |F | number clauses F , w(, k) word containing
k copies symbol , t(F ) 1 F satisfiable (i.e., F 3Sat), 0
not. cases, problem consists generating correct word. Observe
f1 f2 provably intractable, since output exponential size input.
Nevertheless, intuitive regard problem f1 easier problem f2 . One way
formalize intuition allow programs produce output succinct
notation. instance, allow programs write w(,k) instead string containing
k copies symbol , problem f1 becomes polynomial, problem f2
(unless P = NP).
wanted investigate following question: regarding class 3S, plan generation intractable solution plans long, like f1 , problem intrinsically hard, like f2 ? answer plan generation 3S solved polynomial
time, provided one allowed give solution terms macros, macro
simple substitution scheme: sequence operators and/or macros. back
claim, present algorithm solves plan generation 3S polynomial time.
researchers argued intractability using fact plans may exponential length. Domshlak Dinitz (2001) proved complexity results several classes
planning problems multi-valued state variables simple causal graphs. argued
class Cn planning problems chain causal graphs intractable since plans
may exponential length. Brafman Domshlak (2003) stated plan generation
STRIPS planning problems unary operators acyclic causal graphs intractable
using reasoning. new result puts question argument used prove
hardness problems. reason, analyze complexity problems
prove hard showing plan existence problem NP-hard.

2. Notation
Let V set state variables, let D(v) finite domain state variable v V .
define state function V maps state variable v V value
s(v) D(v) domain. partial state p function subset Vp V state
322

fiComplexity Planning Problems

variables maps state variable v Vp p(v) D(v). subset C V
state variables, p | C partial state obtained restricting domain p Vp C.
Sometimes use notation (v1 = x1 , . . . , vk = xk ) denote partial state p defined
Vp = {v1 , . . . , vk } p(vi ) = xi vi Vp . write p(v) = denote v
/ Vp .
Two partial states p q match, denote pq, p | Vq = q | Vp ,
i.e., v Vp Vq , p(v) = q(v). define replacement operator q
r two partial states, p = q r partial state defined Vp = Vq Vr , p(v) = r(v)
v Vr , p(v) = q(v) v Vq Vr . Note that, general, p q 6= q p.
partial state p subsumes partial state q, denote p q, pq
Vp Vq . remark p q r s, follows p r q s. difference
two partial states q r, denote q r, partial state p defined
Vp = {v Vq | q(v) 6= r(v)} p(v) = q(v) v Vp .
planning problem tuple P = hV, init, goal, Ai, V set variables,
init initial state, goal partial goal state, set operators. operator
= hpre(a); post(a)i consists partial state pre(a) called pre-condition
partial state post(a) called post-condition. Operator applicable state
spre(a), applying operator state results new state post(a).
valid plan P sequence operators sequentially applicable state init
resulting state satisfies goal.
causal graph planning problem P directed graph (V, E) state variables
nodes. edge (u, v) E u 6= v exists operator
u Vpre(a) Vpost(a) v Vpost(a) .

3. Class 3S
Jonsson Backstrom (1998) introduced class 3S planning problems study
relative complexity plan existence plan generation. section, introduce
additional notation needed describe class 3S illustrate properties
3S planning problems. begin defining class 3S:
Definition 3.1 planning problem P belongs class 3S causal graph acyclic
state variable v V binary either static, symmetrically reversible,
splitting.
Below, provide formal definitions static, symmetrically reversible splitting.
Note fact causal graph acyclic implies operators unary, i.e.,
operator A, |Vpost(a) | = 1. Without loss generality, assume 3S planning
problems normal form, mean following:
state variable v, D(v) = {0, 1} init(v) = 0.
post(a) = (v = x), x {0, 1}, implies pre(a)(v) = 1 x.
satisfy first condition, relabel values D(v) initial goal
states well pre- post-conditions operators. satisfy second condition,
operator post(a) = (v = x) pre(a)(v) 6= 1 x, either remove
323

fiGimenez & Jonsson

v

V0

u
v=0
v

v=0

w=0
w
w=1

w

V*



u
v=0
v



(a)

v=0

w=0
w
w=1

w



V0



V1

w

(b)

Figure 1: Causal graph splitting variable partitions (a) v, (b) w.
pre(a)(v) = x, let pre(a)(v) = 1 x previously undefined. resulting planning
problem normal form equivalent original one. process done
time O(|A||V |).
following definitions describe three categories state variables 3S:
Definition 3.2 state variable v V static one following holds:
1. exist post(a)(v) = 1,
2. goal(v) = 0 exist post(a)(v) = 0.
Definition 3.3 state variable v V reversible
post(a) = (v = x), exists post(a ) = (v = 1 x). addition, v
symmetrically reversible pre(a ) | (V {v}) = pre(a) | (V {v}).
definitions follows value static state variable cannot
must change, whereas value symmetrically reversible state variable change
freely, long possible satisfy pre-conditions operators change
value. third category state variables splitting. Informally, splitting state variable
v splits causal graph three disjoint subgraphs, one depends value
v = 1, one depends v = 0, one independent v. However,
precise definition involved, need additional notation.
v V , let Qv0 subset state variables, different v, whose value
changed operator v = 0 pre-condition. Formally, Qv0 = {u V {v} |
s.t. pre(a)(v) = 0 u Vpost(a) }. Define Qv1 way v = 1. Let
Gv0 = (V, E0v ) subgraph (V, E) whose edges exclude v Qv0 Qv1 .
Formally, E0v = E {(v, w) | w Qv0 w
/ Qv1 }. Finally, let V0v V subset state
variables weakly connected state variable Qv0 graph Gv0 . Define
V1v way v = 1.
Definition 3.4 state variable v V splitting V0v V1v disjoint.
Figure 1 illustrates causal graph planning problem two splitting state
variables, v w. edge label v = 0 indicates operators changing
value u v = 0 pre-condition. words, Qv0 = {u, w}, graph
Gv0 = (V, E0v ) excludes two edges labeled v = 0, V0v includes state state variables,
324

fiComplexity Planning Problems

since v weakly connected u w connects remaining state variables. set
Qv1 empty since operators changing value state variable
v v = 1 pre-condition. Consequently, V1v empty well. Figure 1(a) shows
resulting partition v.
w
w
case w, Qw
0 = {s}, G0 = (V, E0 ) excludes edge labeled w = 0,
w
V0 = {s}, since state variable connected edge w = 0 removed.
Likewise, V1w = {t}. use Vw = V V0w V1w denote set remaining state
variables belong neither V0w V1w . Figure 1(b) shows resulting partition
w.
Lemma 3.5 splitting state variable v, two sets V0v V1v non-empty,
v belongs neither V0v V1v .
Proof contradiction. Assume v belongs V0v . v weakly connected
state variable Qv0 graph Gv0 = (V, E0v ). since E0v exclude edges
v Qv1 , state variable Qv1 weakly connected state variable
Qv0 Gv0 . Consequently, state variables Qv1 belong V0v V1v , contradicts
v splitting. reasoning holds show v belong V1v .
Lemma 3.6 value splitting state variable never needs change twice
valid plan.
Proof Assume valid plan changes value splitting state variable v
least three times. show reorder operators way
value v need change twice. need address three cases: v
belongs V0v (cf. Figure 1(a)), v belongs V1v , v belongs Vv (cf. Figure 1(b)).
v belongs V0v , follows Lemma 3.5 V1v empty. Consequently,
operator plan requires v = 1 pre-condition. Thus, safely remove
operators change value v, except possibly last, needed case
goal(v) = 1. v belongs V1v , follows Lemma 3.5 V0v empty. Thus,
operator plan requires v = 0 pre-condition. first operator changes
value v necessary set v 1. that, safely remove operators
change value v, except last case goal(v) = 0. cases resulting
plan contains two operators changing value v.
v belongs Vv , edges V0v , V1v , Vv v Vv
Qv0 V0v Qv1 V1v . Let 0 , 1 , subsequences operators
affect state variables V0v , V1v , Vv , respectively. Write = h , av1 , i, av1
last operator changes value v 0 1. claim reordering
h0 , , av1 , 1 , plan still valid. Indeed, operators 0 require v = 0,
holds initial state, operators 1 require v = 1, holds
due operator av1 . Note operators changing value v safely
removed since value v = 1 never needed pre-condition change value
state variable Vv . result valid plan changes value v twice
(its value may reset 0 ).

325

fiGimenez & Jonsson

Variable
v1
v2
v3
v4
v5
v6
v7
v8

Operators
av11 = h(v1 = 0); (v1 = 1)i
av01 = h(v1 = 1); (v1 = 0)i
av12 = h(v1 = 1, v2 = 0); (v2 = 1)i
av13 = h(v1 = 0, v2 = 1, v3 = 0); (v3 = 1)i
av15
av16
av06
av17
av18

= h(v3
= h(v3
= h(v3
= h(v6
= h(v6

= 0, v4
= 1, v6
= 1, v6
= 1, v7
= 0, v7

= 0, v5 = 0); (v5 = 1)i
= 0); (v6 = 1)i
= 1); (v6 = 0)i
= 0); (v7 = 1)i
= 1, v8 = 0); (v8 = 1)i

V0vi
V

V1vi
V


{v4 , v5 }
V {v4 }

V

V
{v6 , v7 , v8 }


V




V


Table 1: Operators sets V0vi V1vi example planning problem.

v4
v1

v5
v7

v3
v6

v2

v8

Figure 2: Causal graph example planning problem.
previous lemma, holds splitting state variables general, provides
additional insight solve planning problem splitting state variable v.
First, try achieve goal state state variables V0v value v 0,
initial state. Then, set value v 1 try achieve goal state state
variables V1v . Finally, goal(v) = 0, reset value v 0.
3.1 Example
illustrate class 3S using example planning problem. set state variables
V = {v1 , . . . , v8 }. Since planning problem normal form, initial state
init(vi ) = 0 vi V . goal state defined goal = (v5 = 1, v8 = 1),
operators listed Table 1. Figure 2 shows causal graph (V, E)
planning problem. operators easy verify v4 static v1
v6 symmetrically reversible. planning problem 3S, remaining
state variables splitting. Table 1 lists two sets V0vi V1vi state
variable vi V show indeed, V0vi V1vi = state variables set
{v2 , v3 , v5 , v7 , v8 }.
326

fiComplexity Planning Problems

4. Plan Generation 3S
section, present polynomial-time algorithm plan generation 3S.
algorithm produces solution instance 3S form system macros.
idea construct unary macros change value single state variable.
macros may change values state variables execution, always reset
terminating. macros generated, goal achieved
one state variable time. show algorithm generates valid plan
one exists.
begin defining macros use paper. Next, describe
algorithm pseudo-code (Figures 3, 4, 5) prove correctness. facilitate
reading moved straightforward involving proof appendix. Following
description algorithm analyze complexity steps involved.
follows, assume 3S planning problems normal form defined previous
section.
4.1 Macros
macro-operator, macro short, ordered sequence operators viewed unit.
operator sequence respect pre-conditions operators follow
it, pre-condition operator sequence violated. Applying macro
equivalent applying operators sequence given order. Semantically,
macro equivalent standard operator pre-condition postcondition, unambiguously induced pre- post-conditions operators
sequence.
Since macros functionally operators, operator sequence associated macro
include macros, long create circular definition. Consequently,
possible create hierarchies macros operator sequences macros
one level include macros level below. solution planning problem
viewed macro sits top hierarchy.
define macros first introduce concept induced pre- post-conditions
operator sequences. = ha1 , . . . , ak operator sequence, write , 1 k,
denote subsequence ha1 , . . . , ai i.
Definition 4.1 operator sequence = ha1 , . . . , ak induces pre-condition pre() =
pre(ak ) pre(a1 ) post-condition post() = post(a1 ) post(ak ). addition,
operator sequence well-defined (pre(i1 )post(i1 ))pre(ai )
1 < k.
follows, assume P = (V, init, goal, A) planning problem
Vpost(a) Vpre(a) operator A, = ha1 , . . . , ak operator sequence.
Lemma 4.2 planning problem P type , Vpost() Vpre() .
Proof direct consequence definitions Vpre() = Vpre(a1 ) Vpre(ak ) Vpost() =
Vpost(a1 ) Vpost(ak ) .
327

fiGimenez & Jonsson

Lemma 4.3 operator sequence applicable state well-defined
spre(). state sk resulting application sk = post().
Proof induction k. result clearly holds k = 1. k > 1, note
pre() = pre(ak ) pre(k1 ), post() = post(k1 ) post(ak ), well-defined
k1 well-defined (pre(k1 ) post(k1 ))pre(ak ).
hypothesis induction state sk1 resulting application k1
sk1 = post(k1 ). follows sk = sk1 post(ak ) = post().
Assume applicable state s. means k1 applicable ak
applicable sk1 = post(k1 ). hypothesis induction, former implies
spre(k1 ) k1 well-defined, latter (s post(k1 ))pre(ak ).
last condition implies (pre(k1 ) post(k1 ))pre(ak ) use pre(k1 ) s,
consequence spre(k1 ) total state. Finally, deduce
s(pre(ak ) pre(k1 )) spre(k1 ) (s post(k1 ))pre(ak ), using
Vpost(k1 ) Vpre(k1 ) . follows well-defined spre().
Conversely, assume well-defined spre(). implies k1
well-defined spre(k1 ), hypothesis induction, k1 applicable state s.
remains show ak applicable state sk1 , is, (s post(k1 ))pre(ak ).
(pre(k1 ) post(k1 ))pre(ak ) follows post(k1 )pre(ak ). fact
s(pre(ak ) pre(k1 )) Vpost(k1 ) Vpre(k1 ) completes proof.
Since macros induced pre- post-conditions, Lemmas 4.2 4.3 trivially extend
case operator sequence includes macros. ready
introduce definition macros:
Definition 4.4 macro sequence = ha1 , . . . , ak operators macros
induces pre-condition pre(m) = pre() post-condition post(m) = post()
pre(). macro well-defined circular definitions occur
well-defined.
make macros consistent standard operators, induced post-condition
include state variables whose values indeed changed macro, achieved
computing difference post() pre(). particular, holds
3S planning problem normal form, derived macros satisfy second condition normal
form, namely post(m) = (v = x), x {0, 1}, implies pre(m)(v) = 1 x.
Definition 4.5 Let Ancv set ancestors state variable v 3S planning
problem. define partial state prev Vprev = Ancv
1. prev (u) = 1 u Ancv splitting v V1u ,
2. prev (u) = 0 otherwise.
Definition 4.6 macro 3S-macro well-defined and, x {0, 1}, post(m) =
(v = x) pre(m) prev (v = 1 x).
328

fiComplexity Planning Problems

Macro
mv11
mv01
mv12
mv13
mv15
mv16
mv06
mv17
mv18

Sequence

Pre-condition

hav11
hav01
hmv11 , av12 , mv01
hav13
hav15
hav16
hav06
hmv16 , av17 , mv06
hav18

(v1
(v1
(v1
(v1
(v3
(v3
(v3
(v3
(v3

= 0)
= 1)
= 0, v2
= 0, v2
= 0, v4
= 1, v6
= 1, v6
= 1, v6
= 1, v6

= 0)
= 1, v3
= 0, v5
= 0)
= 1)
= 0, v7
= 0, v7

Post-condition

= 0)
= 0)

= 0)
= 1, v8 = 0)

(v1
(v1
(v2
(v3
(v5
(v6
(v6
(v7
(v8

= 1)
= 0)
= 1)
= 1)
= 1)
= 1)
= 0)
= 1)
= 1)

Table 2: Macros generated algorithm example planning problem.

algorithm present generates 3S-macros. fact, generates one
macro = mvx post(m) = (v = x) state variable v value x {0, 1}.
illustrate idea 3S-macros give flavor algorithm, Table 2 lists macros
generated algorithm example 3S planning problem previous section.
claim macro 3S-macro. example, operator sequence hav16
induces pre-condition (v3 = 1, v6 = 0) post-condition (v3 = 1, v6 = 0) (v6 = 1) =
(v3 = 1, v6 = 1). Thus, macro mv16 induces pre-condition pre(mv16 ) = (v3 = 1, v6 = 0)
post-condition post(mv16 ) = (v3 = 1, v6 = 1) (v3 = 1, v6 = 0) = (v6 = 1). Since v2
v3 splitting since v6 V1v2 v6 V1v3 , follows prev6 (v6 = 0) =
(v1 = 0, v2 = 1, v3 = 1, v6 = 0), pre(mv16 ) = (v3 = 1, v6 = 0) prev6 (v6 = 0).
macros combined produce solution planning problem. idea
identify state variable v goal(v) = 1 append macro mv1
solution plan. example, results operator sequence hmv15 , mv18 i. However,
pre-condition mv18 specifies v3 = 1 v7 = 1, makes necessary insert mv13
mv17 mv18 . addition, pre-condition mv13 specifies v2 = 1, makes
necessary insert mv12 mv13 , resulting final plan hmv15 , mv12 , mv13 , mv17 , mv18 i.
Note order macros matter; mv15 requires v3 0 mv18 requires
v3 1. splitting state variable v, goal state achieved state
variables V0v value v set 1. expand solution plan
consists solely operators A. example, results operator sequence
hav15 , av11 , av12 , av01 , av13 , av16 , av17 , av06 , av18 i. case, algorithm generates optimal
plan, although true general.
4.2 Description Algorithm
proceed providing detailed description algorithm plan generation 3S.
first describe subroutine generating unary macro sets value state
variable v x. algorithm, call GenerateMacro, described Figure 3.
algorithm takes input planning problem P , state variable v, value x (either 0
329

fiGimenez & Jonsson

1
2
3
4
5
6
7
8
9
10
11
12
13
14

function GenerateMacro(P , v, x, )
post(a)(v) = x
S0 S1 hi
satisf true
U {u Vpre(a) {v} | pre(a)(u) = 1}
u U increasing topological order
u static mu1
/
satisf false
else u splitting mu0 mu1
S0 hS0 , mu0
S1 hmu1 , S1
satisf
return hS1 , a, S0
return f ail
Figure 3: Algorithm generating macro sets value v x.

1), set macros vs ancestors causal graph. Prior executing
algorithm, perform topological sort state variables. assume that,
v V x {0, 1}, contains one macro mvx post(mvx ) = (v = x).
algorithm, use notation mvx test whether contains mvx .
operator sets value v x, algorithm determines whether
possible satisfy pre-condition pre(a) starting initial state. this,
algorithm finds set U state variables pre(a) assigns 1 (the values
state variables already satisfy pre(a) initial state). algorithm constructs two
sequences operators, S0 S1 , going state variables U increasing
topological order. operator sequence, use hS, oi shorthand denote
operator sequence length |S| + 1 consisting operators followed o,
either operator macro. possible satisfy pre-condition pre(a)
operator A, algorithm returns macro hS1 , a, S0 i. Otherwise, returns f ail.
Lemma 4.7 v symmetrically reversible GenerateMacro(P , v, 1, ) successfully generates macro, GenerateMacro(P , v, 0, ).
Proof Assume GenerateMacro(P , v, 1, ) successfully returns macro hS1 , a, S0
operator post(a) = 1. definition symmetrically
reversible follows exists operator post(a ) = 0
pre(a ) | V {v} = pre(a) | V {v}. Thus, set U identical .
consequence, values S0 , S1 , satisf loop,
means GenerateMacro(P , v, 0, ) returns macro hS1 , , S0 . Note
GenerateMacro(P , v, 0, ) may return another macro goes operators different order; however, guaranteed successfully return macro.
Theorem 4.8 macros 3S-macros GenerateMacro(P , v, x, )
generates macro mvx 6= f ail, mvx 3S-macro.
330

fiComplexity Planning Problems

1
2
3
4
5
6
7
8
9
10

function Macro-3S(P )

v V increasing topological order
mv1 GenerateMacro(P , v, 1, )
mv0 GenerateMacro(P , v, 0, )
mv1 6= f ail mv0 6= f ail
{mv1 , mv0 }
else mv1 6= f ail goal(v) 6= 0
{mv1 }
return GeneratePlan(P , V , )
Figure 4: algorithm Macro-3S.
proof Theorem 4.8 appears Appendix A.

Next, describe algorithm plan generation 3S, call Macro-3S.
Figure 4 shows pseudocode Macro-3S. algorithm goes state variables
increasing topological order attempts generate two macros state variable
v, mv1 mv0 . macros successfully generated, added current
set macros . mv1 generated goal state assign 0 v,
algorithm adds mv1 . Finally, algorithm generates plan using subroutine
GeneratePlan, describe later.
Lemma 4.9 Let P 3S planning problem let v V state variable.
exists valid plan solving P sets v 1, Macro-3S(P ) adds macro mv1 .
If, addition, plan resets v 0, Macro-3S(P ) adds mv0 .
Proof First note mv1 mv0 generated, Macro-3S(P ) adds .
mv1 generated mv0 , Macro-3S(P ) adds mv1 unless goal(v) = 0. However,
goal(v) = 0 contradicts fact valid plan solving P sets v 1
without resetting 0. remains show GenerateMacro(P , v, 1, ) always
generates mv1 6= f ail GenerateMacro(P , v, 0, ) always generates mv0 6= f ail
plan resets v 0.
plan solving P sets v 1 contain operator
post(a)(v) = 1. plan also resets v 0, contain operator
post(a )(v) = 0. show GenerateMacro(P , v, 1, ) successfully generates
mv1 6= f ail operator selected line 2. Note algorithm may return another
macro selects another operator a; however, always generates macro a,
guaranteed successfully return macro mv1 6= f ail. true mv0 .
prove lemma induction state variables v. v ancestors
causal graph, set U empty default. Thus, satisf never set false
GenerateMacro(P , v, 1, ) successfully returns macro mv1 = hai a. exists,
GenerateMacro(P , v, 0, ) successfully returns mv0 = ha .
v ancestors causal graph, let U = {u Vpre(a) {v} | pre(a)(u) = 1}.
Since plan contains set u U 1. hypothesis induction,
Macro-3S(P ) adds mu1 u U . consequence, satisf never set
331

fiGimenez & Jonsson

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

function GeneratePlan(P , W , )
|W | = 0
return hi
v first variable topological order present W
v splitting
v0 Generate-Plan(P , W (V0v {v}), )
v1 Generate-Plan(P , W (V1v {v}), )
v Generate-Plan(P , W (V V0v V1v {v}), )
v0 = f ail v1 = f ail v = f ail (goal(v) = 1 mv1
/ )
return f ail
else mv1
/ return hv , v0 , v1
else goal(v) = 0 return hv , v0 , mv1 , v1 , mv0
else return hv , v0 , mv1 , v1
Generate-Plan(P , W {v}, )
= f ail (goal(v) = 1 mv1
/ ) return f ail
else goal(v) = 1 return h, mv1
else return
Figure 5: Algorithm generating final plan

false thus, GenerateMacro(P , v, 1, ) successfully returns mv1 a. exists,
let W = {w Vpre(a ) {v} | pre(a )(w) = 1}. plan contains , set
w W 1. hypothesis induction, Macro-3S(P ) adds mw
1 w W
consequently, GenerateMacro(P , v, 0, ) successfully returns mv0 .
Finally, describe subroutine GeneratePlan(P , W , ) generating final
plan given planning problem P , set state variables W set macros .
set state variables empty, GeneratePlan(P , W , ) returns empty operator
sequence. Otherwise, finds state variable v W comes first topological order.
v splitting, algorithm separates W three sets described V0v , V1v ,
Vv = V V0v V1v . algorithm recursively generates plans three sets
necessary, inserts mv1 V0v V1v final plan. case,
algorithm recursively generates plan W {v}. goal(v) = 1 mv1 , algorithm
appends mv1 end resulting plan.
Lemma 4.10 Let W plan generated GeneratePlan(P , W , ), let v
first state variable topological order present W , let V = ha , W , b final
plan generated Macro-3S(P ). mv1 follows (pre(a )post(a ))pre(mv1 ).
Proof determine content operator sequence precedes W final
plan inspection. Note call GeneratePlan(P , W , ) nested within
sequence recursive calls GeneratePlan starting GeneratePlan(P , V , ).
Let Z set state variables u Z first state variable
topological order call GeneratePlan prior GeneratePlan(P , W , ).
u Z correspond call GeneratePlan set state variables
U W U . u splitting, u contribute since
332

fiComplexity Planning Problems

possible addition macro plan line 16 places macro mu1 end
plan generated recursively.
Assume u Z splitting state variable. three cases: W V0u , W V1u ,
W Vu = V V0u V1u . W Vu , u contribute since never places
macros u . W V0u , plan u part since precedes u0 lines 11,
12, 13. W V1u , plans u u0 part since precede u1
cases. mu1 , macro mu1 also part since precedes u1 lines 12
13. macros part .
Since macros unary, plan generated GeneratePlan(P , U , )
changes values state variables U . splitting state variable u,
edges Vu {u} V0u , Vu {u} V1u , V0u V1u . follows
plan u change value state variable appears pre-condition
macro u0 . holds u respect u1 u0 respect u1 .
Thus, macro changes value splitting state variable u Ancv
mu1 case W V1u .
Recall prev defined Ancv assigns 1 u u splitting
v V1u . ancestors v, value 0 holds initial state
altered . u splitting v V1u , follows definition 3S-macros
pre(mv1 )(u) = 1 pre(mv1 )(u) =. pre(mv1 )(u) = 1, correct append mu1
mv1 satisfy pre(mv1 )(u). mu1
/ follows u
/ Vpre(mv1 ) , since pre(mv1 )(u) = 1
would caused GenerateMacro(P , v, 1, ) set satisf false line 8. Thus,
pre-condition pre(mv1 ) mv1 agrees pre(a ) post(a ) value state
variable, means two partial states match.
Lemma 4.11 GeneratePlan(P , V , ) generates well-defined plan.
Proof Note state variable v V , GeneratePlan(P , W , ) called
precisely v first state variable topological order. Lemma 4.10
follows (pre(a ) post(a ))pre(mv1 ), plan precedes W
final plan. Since v first state variable topological order W , plans v0 ,
v1 , v , , recursively generated GeneratePlan, change value
state variable pre(mv1 ). follows mv1 applicable following ha , v , v0 ha , i.
Since mv1 changes value v, mv0 applicable following ha , v , v0 , mv1 , v1 i.
Theorem 4.12 Macro-3S(P ) generates valid plan solving planning problem 3S
one exists.
Proof GeneratePlan(P , V , ) returns f ail exists state variable
v V goal(v) = 1 mv1
/ . Lemma 4.9 follows
exist valid plan solving P sets v 1. Consequently, exist
plan solving P . Otherwise, GeneratePlan(P , V , ) returns well-defined plan due
Lemma 4.11. Since plan sets 1 state variable v goal(v) = 1
resets 0 state variable v goal(v) = 0, plan valid plan solving
planning problem.
333

fiGimenez & Jonsson

v1

v2

v3

v4

v5

Figure 6: Causal graph planning problem P5 .
4.3 Examples
illustrate algorithm example introduced Jonsson Backstrom (1998)
show instances 3S exponentially sized minimal solutions. Let Pn =
hV, init, goal, Ai planning problem defined natural number n, V = {v1 , . . . , vn },
goal state defined Vgoal = V , goal(vi ) = 0 vi {v1 , . . . , vn1 },
goal(vn ) = 1. state variable vi V , two operators A:
av1i = h(v1 = 0, . . . , vi2 = 0, vi1 = 1, vi = 0); (vi = 1)i,
av0i = h(v1 = 0, . . . , vi2 = 0, vi1 = 1, vi = 1); (vi = 0)i.
words, state variable symmetrically reversible. causal graph planning problem P5 shown Figure 6. Note state variable vi {v1 , . . . , vn2 },
v
v
pre(a1i+1 )(vi ) = 1 pre(a1i+2 )(vi ) = 0, vi+1 Qv1i vi+2 Q0vi . Since
edge causal graph vi+1 vi+2 , state variable {v1 , . . . , vn2 }
v
splitting. hand, vn1 vn splitting since V0 n1 = V0vn = V1vn = .
Backstrom Nebel (1995) showed length shortest plan solving Pn 2n 1,
i.e., exponential number state variables.
state variable vi {v1 , . . . , vn1 }, algorithm generates two macros mv1i
vi
m0 . single operator, av1i , changes value vi 0 1. pre(av1i )
assigns 1 vi1 , U = {vi1 }. Since vi1 splitting, mv1i defined mv1i =
v
v
v
v
hm1i1 , av1i , m0i1 i. Similarly, mv0i defined mv0i = hm1i1 , av0i , m0i1 i. state variable
vn , U = {vn1 }, splitting, mv1n defined mv1n = hav1n i.
generate final plan, algorithm goes state variables topological order. state variables v1 vn2 , algorithm nothing, since
state variables splitting goal state 1. state variable vn1 ,
algorithm recursively generates plan vn , hmv1n since goal(vn ) = 1.
v
Since goal(vn1 ) = 0, algorithm inserts m1n1 mv1n satisfy pre-condition
v
vn1 = 1 m0n1 mv1n achieve goal state goal(vn1 ) = 0. Thus, final plan
vn1
v
vn
hm1 , m1 , m0n1 i. expand plan, end sequence 2n 1 operators. However, individual macro operator sequence length greater 3. Together,
macros recursively specify complete solution planning problem.
also demonstrate planning problems 3S polynomial length
solutions algorithm may generate exponential length solutions. this,
modify planning problem Pn letting goal(vi ) = 1 vi V . addition,
state variable vi V , add two operators A:
bv1i = h(v1 = 1, . . . , vi1 = 1, vi = 0); (vi = 1)i,
bv0i = h(v1 = 1, . . . , vi1 = 1, vi = 1); (vi = 0)i.
334

fiComplexity Planning Problems

also add operator cv1n = h(vn1 = 0, vn = 0); (vn = 1)i A. consequence, state variables {v1 , . . . , vn2 } still symmetrically reversible splitting.
vn1 also symmetrically reversible longer splitting, since pre(av1n )(vn1 ) = 1
v
v
pre(cv1n )(vn1 ) = 0 implies vn V0 n1 V1 n1 . vn still splitting since V0vn = V1vn = .
Assume GenerateMacro(P , vi , x, ) always selects bvxi first. consequence,
state variable vi V x {0, 1}, GenerateMacro(P , vi , x, ) generates
v
v
macro mvxi = hm1i1 , . . . , mv11 , bvxi , mv01 , . . . , m0i1 i.
Let Li length plan represented mvxi , x {0, 1}. definition
v

mx Li = 2(L1 + . . . + Li1 ) + 1. show induction Li = 3i1 .
length macro v1 L1 = 1 = 30 . > 1,
Li = 2(30 + . . . + 3i2 ) + 1 = 2

3i1 1
3i1 1
+1=2
+ 1 = 3i1 1 + 1 = 3i1 .
31
2

generate final plan algorithm change value state variable
0 1, total length plan L = L1 + . . . + Ln = 30 + . . . + 3n1 =
(3n 1)/2. However, exists plan length n solves planning problem,
namely hbv11 , . . . , bv1n i.
4.4 Complexity
section prove complexity algorithm polynomial.
analyze step algorithm separately. summary complexity result
step algorithm given below. Note number edges |E| causal
graph O(|A||V |), since operator may introduce O(|V |) edges. complexity result
O(|V | + |E|) = O(|A||V |) topological sort follows Cormen, Leiserson, Rivest,
Stein (1990).
Constructing causal graph G = (V, E)
Calculating V1v V0v v V
Performing topological sort G
GenerateMacro(P , v, x, )
GeneratePlan(P , V , )
Macro-3S(P )

O(|A||V |)
O(|A||V |2 )
O(|A||V |)
O(|A||V |)
O(|V |2 )
O(|A||V |2 )

Lemma 4.13
Lemma 4.14
Lemma 4.15
Lemma 4.16
Theorem 4.17

Lemma 4.13 complexity constructing causal graph G = (V, E) O(|A||V |).
Proof causal graph consists |V | nodes. operator state
variable u Vpre(a) , add edge u unique state variable v Vpost(a) .
worst case, |Vpre(a) | = O(|V |), case complexity O(|A||V |).
Lemma 4.14 complexity calculating sets V0v V1v state variable
v V O(|A||V |2 ).
Proof state variable v V , establish sets Qv0 Qv1 , requires
going operator worst case. Note interested
pre-condition v unique state variable Vpost(a) , means
335

fiGimenez & Jonsson

need go state variable Vpre(a) . Next, construct graph Gv0 .
copying causal graph G, takes time O(|A||V |), removing
edges v Qv0 Qv1 , takes time O(|V |).
Finally, construct set V0v find state variable weakly connected state variable u Qv0 graph Gv0 . state variable u Qv0 ,
performing undirected search starting u takes time O(|A||V |). performed search starting u, need search state variables Qv0
reached search. way, total complexity search exceed
O(|A||V |). case constructing V1v identical. Since perform
procedure state variable v V , total complexity step O(|A||V |2 ).
Lemma 4.15 complexity GenerateMacro(P , v, x, ) O(|A||V |).
Proof operator A, GenerateMacro(P , v, x, ) needs check whether
post(a)(v) = x. worst case, |U | = O(|V |), case complexity
algorithm O(|A||V |).
Lemma 4.16 complexity GeneratePlan(P , V , ) O(|V |2 ).
Proof Note state variable v V , GeneratePlan(P , V , ) called recursively exactly v first variable topological order. words,
GeneratePlan(P , V , ) called exactly |V | times. GeneratePlan(P , V , ) contains
constant operations except intersection difference sets lines 6-8.
Since intersection set difference done time O(|V |), total complexity
GeneratePlan(P , V , ) O(|V |2 ).
Theorem 4.17 complexity Macro-3S(P ) O(|A||V |2 ).
Proof Prior executing Macro-3S(P ), necessary construct causal graph G,
find sets V0v V1v state variable v V , perform topological sort
G. shown steps take time O(|A||V |2 ). state variable
v V , Macro-3S(P ) calls GenerateMacro(P , v, x, ) twice. Lemma 4.15
follows step takes time O(2|V ||A||V |) = O(|A||V |2 ). Finally, Macro-3S(P ) calls
GeneratePlan(P , V , ), takes time O(|V |2 ) due Lemma 4.16. follows
complexity Macro-3S(P ) O(|A||V |2 ).
conjecture possible improve complexity result Macro3S(P ) O(|A||V |). However, proof seems somewhat complex, main objective
devise algorithm efficient possible. Rather, interested
establishing algorithm polynomial, follows Theorem 4.17.
4.5 Plan Length
section study length plans generated given algorithm. begin
with, derive general bound length plans. Then, show compute
actual length particular plan without expanding macros. also present
algorithm uses computation efficiently obtain i-th action plan
336

fiComplexity Planning Problems

macro form. start introducing concept depth state variables
causal graph.
Definition 4.18 depth d(v) state variable v longest path v
state variable causal graph.
Since causal graph acyclic planning problems 3S, depth state variable
unique computed polynomial time. Also, follows least one state
variable depth 0, i.e., outgoing edges.
Definition 4.19 depth planning problem P 3S equals largest depth
state variable v P , i.e., = maxvV d(v).
characterize planning problem based depth state variables. Let
n = |V | number state variables, let ci denote number state variables
depth i. planning problem depth d, follows c0 + . . . + cd = n.
example, consider planning problem whose causal graph appears Figure 2.
planning problem, n = 8, = 5, c0 = 2, c1 = 2, c2 = 1, c3 = 1, c4 = 1, c5 = 1.
Lemma 4.20 Consider values Li {0, . . . , d} defined Ld = 1, Li =
2(ci+1 Li+1 + ci+2 Li+2 + . . . + cd Ld ) + 1 < d. values Li upper bound
length macros generated algorithm state variable v depth i.
Proof prove decreasing induction value i. Assume v depth = d.
follows Definition 4.18 v incoming edges. Thus, operator changing
value v pre-condition state variable v, Ld = 1 upper
bound, stated.
Now, assume v depth < d, Li+k k > 0 upper bounds
length corresponding macros. Let operator changes value v.
definition depth follows cannot pre-condition state variable
u depth j i; otherwise would edge u v causal graph, causing
depth u greater i. Thus, worst case, macro v change
values state variables depths larger i, change value v, reset
values state variables lower levels. follows Li = 2(ci+1 Li+1 + . . . + cd Ld ) + 1
upper bound.
Theorem 4.21 upper bounds Li Lemma 4.20 satisfy Li = dj=i+1 (1 + 2cj ).
Proof Note
Li = 2(ci+1 Li+1 + ci+2 Li+2 + . . . + cd Ld ) + 1 =
= 2ci+1 Li+1 + 2(ci+2 Li+2 + . . . + cd Ld ) + 1 =
= 2ci+1 Li+1 + Li+1 = (2ci+1 + 1)Li+1 .
result easily follows induction.
337

fiGimenez & Jonsson

obtain upper bound L total length plan. worst
case, goal state assigns different value state variable initial state,
i.e., goal(v) 6= init(v) v V . achieve goal state algorithm applies one
macro per state variable. Hence
L = c0 L0 + c1 L1 + . . . + cd Ld = c0 L0 +


L0 1
(1 + 2c0 )L0 1
1Y
1
=
=
(1 + 2cj ) .
2
2
2
2
j=0

previous bound depends distribution variables depths according
causal graph. obtain general bound depend depths
variables first find distribution maximizes upper bound L.
Q
Lemma 4.22 upper bound L = 21 dj=0 (1+2cj ) 12 planning problems n variables
depth maximized ci equal, is, ci = n/(d + 1).
Proof Note ci > 0 i, c0 + + cd = n. result follows direct
application well known AM-GM (arithmetic mean-geometric mean) inequality,
states arithmetic mean positive values xi greater equal geometric
mean, equality xi same.
implies product positive
P
factors xi = (1 + 2ci ) fixed sum = dj=0 xj = 2n + maximized
equal, is, ci = n/(d + 1).
Theorem 4.23 length plan generated algorithm planning problem
3S n state variables depth ((1 + 2n/(d + 1))d+1 1)/2.
Proof direct consequence Lemma 4.22. Since c0 , . . . , cd discrete, may
possible set c0 = . . . = cd = n/(d + 1). Nevertheless, ((1 + 2n/(d + 1))d+1 1)/2
upper bound L case.
Observe bound established Theorem 4.23 increasing function d.
implies given d, bound also applies planning problems 3S depth
smaller d. consequence, depth planning problem 3S bounded
d, algorithm generates solution plan planning problem
polynomial length O(nd+1 ). Since complexity executing plan proportional
plan length, use depth define tractable complexity classes planning
problems 3S respect plan execution.
Theorem 4.24 length plan generated algorithm planning problem
3S n state variables (3n 1)/2.
Proof worst case, depth planning problem n1. follows Theorem
4.23 length plan ((1 + 2n/n)n 1)/2 = (3n 1)/2.
Note bound established Theorem 4.24 tight; second example Section
4.3, showed algorithm generates plan whose length (3n 1)/2.
338

fiComplexity Planning Problems

1
2
3
4
5
6
7
8
9

function Operator(S, i)
first operator
length(o) <
length(o)
next operator
primitive(o)
return
else
return Operator(o, i)
Figure 7: algorithm determining i-th operator sequence

Lemma 4.25 complexity computing total length plan generated
algorithm O(|V |2 ).
Proof algorithm generates 2|V | = O(|V |) macros, 2 state variable.
operator sequence macro consists one operator 2(|V | 1) = O(|V |)
macros. use dynamic programming avoid computing length macro
once. worst case, compute length O(|V |) macros,
sum O(|V |) terms, resulting total complexity O(|V |2 ).
Lemma 4.26 Given solution plan length l integer 1 l, complexity
determining i-th operator plan O(|V |2 ).
Proof prove lemma providing algorithm determining i-th operator,
appears Figure 7. Since operator sequences consist operators macros,
variable represents either operator macro generated Macro-3S.
function primitive(o) returns true operator f alse macro. function
length(o) returns length macro, 1 otherwise. assume length
macros pre-computed, know Lemma 4.25 takes time O(|V |2 ).
algorithm simply finds operator macro i-th position sequence,
taking account length macros sequence. i-th position part
macro, algorithm recursively finds operator appropriate position
operator sequence represented macro. worst case, algorithm go
O(|V |) operators sequence call Operator recursively O(|V |) times,
resulting total complexity O(|V |2 ).
4.6 Discussion
general view plan generation output consist valid sequence
grounded operators solves planning problem. contrast, algorithm generates
solution plan form system macros. One might argue truly solve
plan generation problem, algorithm expand system macros arrive
sequence underlying operators. case, algorithm would longer polynomial,
since solution plan planning problem 3S may exponential length. fact,
objective execute solution plan once, algorithm offers marginal
benefit incremental algorithm proposed Jonsson Backstrom (1998).
339

fiGimenez & Jonsson

hand, several reasons view system macros generated
algorithm complete solution planning problem 3S. macros collectively
specify steps necessary reach goal. solution plan generated
verified polynomial time, plan stored reused using polynomial memory.
even possible compute length resulting plan determine i-th
operator plan polynomial time shown Lemmas 4.25 4.26. Thus,
practical purposes system macros represents complete solution. Even
objective execute solution plan once, algorithm faster
Jonsson Backstrom (1998). necessary execute plan generated
algorithm maintain stack currently executing macros select next operator
execute, whereas algorithm Jonsson Backstrom perform several steps
operator output.
Jonsson Backstrom (1998) proved bounded plan existence problem 3S
NP-hard. bounded plan existence problem problem determining whether
exists valid solution plan length k. consequence, optimal
plan generation problem 3S NP-hard well; otherwise, would possible
solve bounded plan existence problem generating optimal plan comparing
length resulting plan k. examples seen algorithm
generate optimal plan general. fact, algorithm bad
incremental algorithm Jonsson Backstrom, sense algorithms may
generate exponential length plans even though exists solution polynomial length.
Since algorithm makes possible compute total length valid solution
polynomial time, used generate heuristics planners. Specifically,
Katz Domshlak (2007) proposed projecting planning problems onto provably tractable
fragments use solution fragments heuristics original problem.
shown 3S tractable fragment. Unfortunately, optimal planning
3S NP-hard, hope generating admissible heuristic. However,
heuristic may still informative guiding search towards solution original
problem. addition, planning problems exponential length optimal solutions,
standard planner hope generating heuristic polynomial time, making
macro-based approach (and Jonsson, 2007) (current) viable option.

5. Class Cn
Domshlak Dinitz (2001) defined class Cn planning problems multi-valued
state variables chain causal graphs. Since chain causal graphs acyclic, follows
operators unary. Moreover, let vi i-th state variable chain. > 1,
operator Vpost(a) {vi } holds Vpre(a) = {vi1 , vi }. words,
operator changes value state variable vi may pre-conditions
vi1 vi .
authors showed instances Cn exponentially sized minimal
solutions, therefore argued class intractable. light previous section,
argument length solutions discard possibility instances
class solved polynomial time using macros. show
case, unless P = NP.
340

fiComplexity Planning Problems

v1

vk

w

Figure 8: Causal graph P (F ).
C1

C1, C1

0,1
Cn,Cn
Cn

C1

0,1
0





0,1



1

C1, C1
0,1

Cn,Cn

Cn

Figure 9: Domain transition graph vi .
define decision problem Plan-Existence-Cn follows. valid input PlanExistence-Cn planning instance P Cn . input P belongs Plan-ExistenceCn P solvable. show section problem Plan-ExistenceCn NP-hard. implies that, unless P = NP, solving instances Cn truly
intractable problem, namely, polynomial-time algorithm distinguish solvable
unsolvable instances Cn . particular, polynomial-time algorithm solve Cn
instances using macros kind output format.1
prove Plan-Existence-Cn NP-hard reduction Cnf-Sat, is,
problem determining whether CNF formula F satisfiable. Let C1 , . . . , Cn
clauses CNF formula F , let v1 , . . . , vk variables appear F .
briefly describe intuition behind reduction. planning problem create
formula F state variable variable appearing F , plans forced
commit value (either 0 1) state variables actually using them. Then,
satisfy goal problem, variables used pass messages. However,
operators defined way plan succeed
state variable values committed satisfying assignment F .
proceed describe reduction. First ,we define planning problem P (F ) =
hV, init, goal, Ai follows. set state variables V = {v1 , . . . , vk , w}, D(vi ) =
{S, 0, 1, C1 , C1 , . . . , Cn , Cn } vi D(w) = {S, 1, . . . , n}. initial state defines
init(v) = v V goal state defines goal(w) = n. Figure 8 shows
causal graph P (F ).
domain transition graph state variable vi shown Figure 9. node
represents value D(vi ), edge x means exists operator
pre(a)(vi ) = x post(a)(vi ) = y. Edge labels represent pre-condition
operators state variable vi1 , multiple labels indicate several operators
associated edge. enumerate operators acting vi using notation
= hpre(a); post(a)i (when = 1 mention vi1 understood void):
1. valid output format one enables efficient distinction output representing valid
plan output representing fact solution found.

341

fiGimenez & Jonsson



C1, C1

n1

1

Cn,Cn

n

Figure 10: Domain transition graph w.
(1) Two operators hvi1 = S, vi = S; vi = 0i hvi1 = S, vi = S; vi = 1i allow vi
move either 0 1.
(2) > 1. clause Cj X {Cj , Cj }, two operators
hvi1 = X, vi = 0; vi = Cj hvi1 = X, vi = 1; vi = Cj i. operators allow vi move Cj Cj vi1 done so.
(3) clause Cj X {0, 1}, operator hvi1 = X, vi = 0; vi = Cj v
occurs clause Cj , operator hvi1 = X, vi = 1; vi = Cj vi occurs clause
Cj . operators allow vi move Cj Cj even vi1 done so.
(4) clause Cj X = {0, 1}, two operators hvi1 = X, vi = Cj ; vi = 0i
hvi1 = X, vi = Cj ; vi = 1i. operators allow vi move back 0 1.
domain transition graph state variable w shown Figure 10. every clause
Cj two operators acting w hvk = X, w = j 1; w = ji, X {Cj , Cj }
(if j = 1, pre-condition w = j 1 replaced w = S).
Proposition 5.1 CNF formula F satisfiable planning instance P (F )
solvable.
Proof proof follows relatively straightforward interpretation variables
values planning instance P (F ). every state variable vi , must use
operator (1) commit either 0 1. Note that, choice made, variable vi
cannot set value. reason need two values Cj Cj clause
enforce commitment (Cj corresponds vi = 0, Cj corresponds vi = 1).
reach goal state variable w advance step step along values 1, . . . , n.
Clearly, every clause Cj must exist variable vi first set values Cj
Cj using operator (3). Then, message propagated along variables
vi+1 , . . . , vk using operators (2). Note existence operator (3) acting
vi implies initial choice 0 1 state variable vi , applied formula
variable vi , makes clause Cj true. Hence, plan solving P (F ), use
initial choices state variables vi define (partial) assignment satisfies
clauses F .
Conversely, assignment satisfies F , show obtain plan
solves P (F ). First, set every state variable vi value (vi ). every one
clauses Cj , choose variable vi among make Cj true using assignment .
Then, increasing order j, set state variable vi corresponding clause Cj
value Cj Cj (depending (vi )), pass message along vi+1 , . . . , vk w.
Theorem 5.2 Plan-Existence-Cn NP-hard.
342

fiComplexity Planning Problems

vx
vC

vC

vC

vC

vC

vC

1

2

3

vx

vy

vy

vz

vz

1

2

v1

v2

v3

v4

v5

3

Figure 11: Causal graph PF F = C1 C2 C3 three variables x, y, z.
Proof Producing planning instance P (F ) CNF formula F easily done
polynomial time, polynomial-time reduction Cnf-Sat p Plan-ExistenceCn .

6. Polytree Causal Graphs
section, study class planning problems binary state variables
polytree causal graphs. Brafman Domshlak (2003) presented algorithm finds
plans problems class time O(n2 ), n number variables
maximum indegree polytree causal graph. Brafman Domshlak (2006)
also showed solve time roughly O(n ) planning domains local depth
causal graphs tree-width . interesting observe algorithms fail solve
polytree planning domains polynomial time different reasons: first one fails
tree broad (unbounded indegree), second one fails tree deep
(unbounded local depth, since tree-width polytree 1).
section prove problem plan existence polytree causal graphs
binary variables NP-hard. proof reduction 3Sat class
planning problems. example reduction, Figure 11 shows causal graph
planning problem PF corresponds formula F three variables three
clauses (the precise definition PF given Proposition 6.2). Finally, end
section remark reduction solves problem expressed terms CP-nets
(Boutilier et al., 2004), namely, dominance testing polytree CP-nets binary
variables partially specified CPTs NP-complete.
Let us describe briefly idea behind reduction. planning problem PF two
, . . . , v ) depends
different parts. first part (state variables vx , vx , . . . , vC1 , vC
1
1
formula F property plan may change value v1 0 1
many times number clauses F truth assignment satisfy. However,
condition v1 cannot stated planning problem goal. overcome difficulty
introducing second part (state variables v1 , v2 , . . . , vt ) translates regular
planning problem goal.
first describe second part. Let P planning problem hV, init, goal, Ai
V set state variables {v1 , . . . , v2k1 } set 4k 2 operators
{1 , . . . , 2k1 , 1 , . . . , 2k1 }. = 1, operators defined 1 = hv1 = 1; v1 = 0i
343

fiGimenez & Jonsson

1 = hv1 = 0; v1 = 1i. > 1, operators = hvi1 = 0, vi = 1; vi = 0i
= hvi1 = 1, vi = 0; vi = 1i. initial state init(vi ) = 0 i, goal state
goal(vi ) = 0 even goal(vi ) = 1 odd.
Lemma 6.1 valid plan planning problem P changes state variable v1 0 1
least k times. valid plan achieves minimum.
Proof Let Ai Bi be, respectively, sequences operators h1 , . . . , h1 , . . . , i.
easy verify plan hB2k1 , A2k2 , B2k3 , . . . , B3 , A2 , B1 solves planning
problem P . Indeed, applying operators Ai (respectively, operators Bi ),
variables v1 , . . . , vi become 0 (respectively, 1). particular, variable vi attains goal
state (0 even, 1 odd). Subsequent operators plan modify vi ,
variable remains goal state end. operator 1 appears k times
plan (one sequence type Bi ), thus value v1 changes k times 0 1.
proceed show k minimum. Consider plan solves
planning problem P , let number operators appearing (in
words, number times value vi changes, either 0 1
1 0). Note number times operator appears equal precisely one
number occurrences . show i1 > . Since 2k1 1,
implies 1 2k 1, plan has, least, k occurrences 1 , completing
proof.
show i1 > . Let Si subsequence operators plan .
Clearly, Si starts (since initial state vi = 0), operator cannot
appear twice consecutively Si , Si = , , , , etc. Also note that, > 1,
vi1 = 1 pre-condition, vi1 = 0, hence must least one operator
i1 plan betweeen two operators . reason must
least one operator i1 two operators , one operator i1
first operator . shows i1 . hand, variables vi
vi1 different values goal state, subsequences Si Si1 must different
lengths, is, i1 6= . Together, implies i1 > , desired.
Proposition 6.2 3Sat reduces plan existence planning problems binary variables polytree causal graphs.
Proof Let F CNF formula k clauses n variables. produce planning
problem PF 2n + 4k 1 state variables 2n + 14k 3 operators. planning
problem two state variables vx vx every variable x F , two state variables vC
every clause C F , 2k 1 additional variables v , . . . , v
vC
1
2k1 . variables
0 initial state. (partial) goal state defined Vgoal = {v1 , . . . , v2k1 },
goal(vi ) = 0 even, goal(vi ) = 1 odd, like problem P Lemma
6.1. operators are:
(1) Operators hvx = 0; vx = 1i hvx = 0; vx = 1i every variable x F .
= 0; v = 1i, hv = 0, v = 0; v = 1i hv = 1, v = 1; v = 0i
(2) Operators hvC
C
C
C
C
C
C
C
every clause C F .

344

fiComplexity Planning Problems

(3) Seven operators every clause C, one partial assignment satisfies C.
Without loss generality, let x, y, z three variables appear clause C.
operator among seven, Vpre(a) = {vx , vx , vy , vy , vz , vz , vC , v1 },
Vpost(a) = {v1 }, pre(a)(vC ) = 1, pre(a)(v1 ) = 0, post(a)(v1 ) = 1. precondition state variables vx , vx , vy , vy , vz , vz depends corresponding satisfying partial assignment. example, operator corresponding partial
assignment {x = 0, = 0, z = 1} clause C = x z pre-condition
(vx = 0, vx = 1, vy = 0, vy = 1, vz = 1, vz = 0).
(4) operator h(C, vC = 0), v1 = 1; v1 = 0i.
(5) Operators = hvi1 = 0, vi = 1; vi = 0i = hvi1 = 1, vi = 0; vi = 1i
2 2k 1 (the operators problem P except 1 1 ).
note simple facts problem PF . variable x, state variables vx
vx PF start 0, applying operators (1) change 1
back 0. particular, plan cannot reach partial states hvx = 1, vx = 0i
hvx = 0, vx = 1i course execution.
Similarly, C clause F , state variable vC change 0 1 and, first
1, v change back 0. changes possible, since
changing vC
C
0.
operator brings back vC
interpret operators (3) (4), operators affect v1 .
change v1 0 1 need apply one operators (3), thus require
vC = 1 clause C. way bring back v1 0 applying operator
(4) pre-condition vC = 0. deduce every time v1 changes
value 0 1 back 0 plan , least one k state variables vC
used up, sense vC brought 0 1 back 0, cannot
used purpose.
show F 3Sat valid plan problem PF . Assume
F 3Sat, let truth assignment satisfies F . Consider following plan
. First, set vx = (x) vx = 1 (x) variables x using operators (1).
Then, clause C F , set vC = 1, apply operator (3) corresponds
restricted variables clause C (at point, v1 changes 0 1), set
= 1 v = 0, apply operator (4) (at point, v change 1
vC
1
C
0). repeating process every clause C F switching state variable v1
exactly k times 0 1. Now, following proof Lemma 6.1, easily extend
plan plan sets variables vi goal values.
show converse, namely, existence valid plan PF implies F
satisfiable. Define assignment setting (x) = 1 partial state {vx = 1, vx = 0}
appears execution , (x) = 0 otherwise. (Recall one
partial states {vx = 1, vx = 0} {vx = 0, vx = 1} appear execution
plan). Lemma 6.1, must state variable v1 changes 0 1 least k
times. implies k operators (3), corresponding different clauses,
used move v1 0 1. apply operator, values state
variables {vx , vx } must satisfy corresponding clause. Thus assignment satisfies
k clauses F .
345

fiGimenez & Jonsson

Theorem 6.3 Plan existence planning problems binary variables polytree
causal graph NP-complete.
Proof Due Proposition 6.2 need show problem NP.
Brafman Domshlak (2003) showed holds general setting planning
problems causal graphs component directed-path singly connected (that
is, one directed path pair nodes). proof exploits
non-trivial auxiliary result: solvable planning problems binary variables directedpath singly connected causal graph plans polynomial length (the true
non-binary variables, unrestricted causal graphs).
6.1 CP-nets
Boutilier et al. (2004) introduced notion CP-net graphical representation
user preferences. brief, CP-net network dependences set variables:
preferences user variable depend values others,
ceteris paribus (all else equal) assumption, is, user preferences variable
completely independent values variables mentioned. preferences
variable given parent variables network stored conditional preference
tables, CPTs.
Boutilier et al. (2004) showed dominance query problem acyclic CP-nets,
is, problem deciding one variable outcome preferable another,
expressed terms planning problem. network dependences CP-net
becomes causal graph planning problem.
However, certain conditions, perform opposite process: transform
planning problem CP-net dominance query problem, answering
query amounts solving planning problem. possible following
conditions planning problems acyclic causal graph binary variables:
1. Two operators modify variable opposing directions must nonmatching prevail conditions (the prevail condition operator partial state
pre(a) | V Vpost(a) ).
2. must allow partially specified CPTs CP-net description.
first condition guarantees obtain consistent CPTs planning instance
operators. second condition ensures reduction polynomial-size preserving,
since fully specified CPTs exponential maximum node indegree CP-net.
particular, planning instance PF reduced F satisfies first condition.
(Note true planning problem P Lemma 6.1, drop
reversing operators 1 1 constructing PF Proposition 6.2.) consequence,
claim following:
Theorem 6.4 Dominance testing polytree CP-nets binary variables partially
specified CPTs NP-complete.
346

fiComplexity Planning Problems

7. Conclusion
presented three new complexity results planning problems simple causal
graphs. First, provided polynomial-time algorithm uses macros generate solution plans class 3S. Although solutions generally suboptimal, algorithm
generate representations exponentially long plans polynomial time. several implications theoretical work planning, since generally accepted
exponentially sized minimal solutions imply plan generation intractable. work
shows always case, provided one allowed express solution
succinct notation macros. also showed plan existence class Cn
NP-hard, plan existence class planning problems binary variables
polytree causal graph NP-complete.
Jonsson Backstrom (1998) investigated whether plan generation significantly
harder plan existence. Using class 3S, demonstrated plan existence
solved polynomial time, plan generation intractable sense
solution plans may exponential length. work casts new light result: even
though solution plans exponential length, possible generate representation
solution polynomial time. Thus, appears class 3S, plan generation
inherently harder plan existence. aware work
determines relative complexity plan existence plan generation, question
whether plan generation harder plan existence remains open.
potential criticism algorithm solution form macros
standard, intractable expand system macros arrive possibly
exponentially long sequence underlying operators. Although true, shown
system macros share several characteristics proper solution. possible
generate validate solution polynomial time, solution stored
using polynomial memory. also showed possible compute total length
solution polynomial time, well determine i-th operator
underlying sequence.
Since relatively simple, class Cn class planning problems
binary state variables polytree causal graphs could seen promising candidates
proving relative complexity plan existence plan generation. However,
shown plan existence Cn NP-hard, plan existence planning problems
polytree causal graphs NP-complete. Consequently, classes cannot used
show plan generation harder plan existence, since plan existence already
difficult. work also closes complexity gaps appear literature regarding
two classes.
however possible exist subsets planning problems classes
plan existence solved polynomial time. fact, polytree causal
graphs binary variables know case, due algorithms Brafman
Domshlak (2003, 2006) mentioned Section 6. Hence plan generation problem
polynomial restrict polytree causal graphs either bounded indegree
bounded local depth . Consequently, reduction 3Sat exhibits unbounded
indegree unbounded local depth.
347

fiGimenez & Jonsson

Similarly, one may ask class Cn planning problems parameter that,
bounded, would yield tractable subclass. state variables reduction
domains whose size depends number clauses corresponding CNF formula,
domain size appears interesting candidate. Planning problems Cn
binary variables tractable due work Brafman Domshlak (2003),
ideas use extend domain sizes 2. Hence would interesting
investigate whether problem plan existence class Cn easier size
state variable domains bounded constant.

Appendix A. Proof Theorem 4.8
Assume GenerateMacro(P , v, x, ) successfully returns macro mvx = hS1 , a, S0 i.
Let U = {u Vpre(a) {v} | pre(a)(u) = 1} let W = {w1 , . . . , wk } U set
wi

state variables U wi splitting, {mw
0 , m1 } , wi comes wj topological order < j. follows u U static,
wk
w1
w1
k
S1 = hmw
1 , . . . , m1 S0 = hm0 , . . . , m0 i. Since state variable wi W
splitting, symmetrically reversible.
Lemma A.1 wi W , prewi prev .
Proof Since wi Vpre(a) v Vpost(a) , edge wi v causal graph.
Thus, ancestor wi also ancestor v, Ancwi Ancv . state variable
u Ancwi , prewi (u) = 1 u splitting wi V1u . graph Gu1 = (V, E1u )
includes edge wi v, means v V1u wi V1u . follows
prewi (u) = 1 prev (u) = 1, consequence, prewi prev .

Let = hS0 , a, S1 i. wi W {0, 1}, let w
sequence preceding
wi1
w
w
w
w
i+1
w

1

k
i.
macro , is, 1 = hm1 , . . . , m1 0 = hS0 , a, mw
0 , . . . , m0


Further, let sequence appearing a, is, = hS0 i.

wi


Lemma A.2 1 k, post-conditions sequences w
1 , , 0

post(w
1 ) = (wi+1 = 1, . . . , wk = 1),

post(a ) = (w1 = 1, . . . , wk = 1),

post(w
0 ) = (w1 = 0, . . . , wi1 = 0, wi = 1, . . . , wk = 1, v = x).

Proof direct consequence post(ha1 , . . . , ak i) = post(a1 ) post(ak ) post(mw
)=
(wi = y), post(a) = (v = x).

wi


Lemma A.3 1 k, pre-conditions sequences w
1 , , 0 ,
wi
v


satisfy pre(w
1 ) pre( ) pre(0 ) pre() pre (v = 1 x).


Proof Since pre(ha1 , . . . , ak i) = pre(ak ) pre(a1 ), follows pre(w
1 ) pre( )
wi
v
pre(0 ) pre(). prove pre() pre (v = 1 x). state variable u
pre()(u) 6=, let mu first operator hS0 , a, S1 u Vpre(mu ) ,
pre()(u) = pre(mu )(u).

348

fiComplexity Planning Problems

u
wi (w = 0) prev ,

mu = mw

1 , follows pre(m ) pre
wi
used m1 3S-macro, wi symmetrically reversible, prewi prev due
Lemma A.1. particular, pre(mu )(u) = prev (u).
Since assume planning problems normal form, u = wi implies
wi
u

u Vpre(mwi ) . follows mu 6= mw
1 i, u 6= wi i. = m0
1
pre(mu ) prewi (wi = 1), due u 6= wi , deduce pre(mu )(u) =
prewi (u) = prev (u).
Finally, consider case mu = a. u = v pre(mu )(u) = 1 x, desired.
u 6= v splitting, either v belongs V0u pre(mu )(u) = 0, v belongs V1u
pre(mu )(u) = 1. is, pre(mu )(u) = prev (u). u 6= v symmetrically reversible
follows pre(mu )(u) = 0, since case pre(mu )(u) = 1 would forced algorithm
either fail include u W . u 6= v static, pre(mu )(u) = 0, else algorithm would
failed.

Lemma A.4 Let p, p , q r partial states. p p (p q)r, (p q)r.
Proof direct consequence p q p q.
Lemma A.5 macro mvx generated algorithm well-defined.
Proof Since includes macros ancestors v causal graph, since
causal graph acyclic, cyclic definitions occur. remains show that, macro
sequence preceding , holds (pre(m ) post(m ))pre(m).
Note due Lemmas A.3 A.4 enough show
wi

(a) (prev (v = 1 x) post(w
1 ))pre(m1 ),

(b) (prev (v = 1 x) post(a ))pre(a),
wi

(c) (prev (v = 1 x) post(w
0 ))pre(m0 ).
wi

Case (a) follows easily since Vpost(wi ) Vpre(mwi ) = pre(mw
1 ) = pre (wi = 0)
1
1
w
prev . Case (c) similar, although time must use post(0 )(wi ) = 1
wi
wi (w = 1). Finally, case (b)

post(w

0 )(wj ) = 0 j < i, required pre(m0 ) = pre
holds variable u Vpre(a) either u = v, covered (v = 1 x),
splitting static, covered prev , symmetrically reversible, covered
prev (u) = 0 pre(a)(u) = 0, post(a )(u) = 1 pre(a)(u) = 1.

remains show mvx 3S-macro. follows Lemmas A.3 A.5
well-defined satisfies pre(mvx ) = pre() prev (v = 1 x). Finally, post(mvx ) =
post()pre() = (v = x) direct consequence post() = (w1 = 0, . . . , wk = 0, v = x)
Lemma A.2, pre()(wi ) = 0, pre()(v) = 1 x proof Lemma A.3.

Acknowledgments
work partially funded MEC grants TIN2006-15387-C03-03 TIN2004-07925C03-01 (GRAMMARS).
349

fiGimenez & Jonsson

References
Backstrom, C., & Nebel, B. (1995). Complexity Results SAS+ Planning. Computational
Intelligence, 11 (4), 625655.
Botea, A., Enzenberger, M., Muller, M., & Schaeffer, J. (2005). Macro-FF: Improving AI
Planning Automatically Learned Macro-Operators. Journal Artificial Intelligence Research, 24, 581621.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004). CP-nets: Tool
Representing Reasoning Conditional Ceteris Paribus Preference Statements.
Journal Artificial Intelligence Research, 21, 135191.
Brafman, R., & Domshlak, C. (2003). Structure Complexity Planning Unary
Operators. Journal Artificial Intelligence Research, 18, 315349.
Brafman, R., & Domshlak, C. (2006). Factored Planning: How, When, Not.
Proceedings 21st National Conference Artificial Intelligence.
Bylander, T. (1994). computational complexity propositional STRIPS planning.
Artificial Intelligence, 69, 165204.
Chapman, D. (1987). Planning conjunctive goals. Artificial Intelligence, 32(3), 333377.
Cormen, T., Leiserson, C., Rivest, R., & Stein, C. (1990). Introduction Algorithms. MIT
Press McGraw Hill.
Domshlak, C., & Dinitz, Y. (2001). Multi-Agent Off-line Coordination: Structure Complexity. Proceedings 6th European Conference Planning, pp. 277288.
Erol, K., Nau, D., & Subrahmanian, V. (1995). Complexity, decidability undecidability
results domain-independent planning. Artificial Intelligence, 76(1-2), 7588.
Fikes, R., & Nilsson, N. (1971). STRIPS: new approach application theorem
proving problem solving. Artificial Intelligence, 5 (2), 189208.
Gimenez, O., & Jonsson, A. (2007). Hardness Planning Problems Simple
Causal Graphs. Proceedings 17th International Conference Automated
Planning Scheduling, pp. 152159.
Helmert, M. (2003). Complexity results standard benchmark domains planning.
Artificial Intelligence, 143(2), 219262.
Helmert, M. (2006). Fast Downward Planning System. Journal Artificial Intelligence
Research, 26, 191246.
Jonsson, A. (2007). Role Macros Tractable Planning Causal Graphs.
Proceedings 20th International Joint Conference Artificial Intelligence, pp.
19361941.
Jonsson, P., & Backstrom, C. (1998). Tractable plan existence imply tractable
plan generation. Annals Mathematics Artificial Intelligence, 22(3-4), 281296.
Katz, M., & Domshlak, C. (2007). Structural Patterns Heuristics: Basic Idea Concrete
Instance. Workshop Heuristics Domain-independent Planning: Progress,
Ideas, Limitations, Challenges (ICAPS-07).
350

fiComplexity Planning Problems

Knoblock, C. (1994). Automatically generating abstractions planning. Artificial Intelligence, 68(2), 243302.
Korf, R. (1987). Planning search: quantitative approach. Artificial Intelligence, 33(1),
6588.
Minton, S. (1985). Selectively generalizing plans problem-solving. Proceedings
9th International Joint Conference Artificial Intelligence, pp. 596599.
Vidal, V. (2004). Lookahead Strategy Heuristic Search Planning. Proceedings
14th International Conference Automated Planning Scheduling, pp. 150159.
Williams, B., & Nayak, P. (1997). reactive planner model-based executive.
Proceedings 15th International Joint Conference Artificial Intelligence, pp.
11781185.

351

fiJournal Artificial Intelligence Research 31 (2008) 217-257

Submitted 09/07; published 02/08

Loosely Coupled Formulations Automated Planning:
Integer Programming Perspective
Menkes H.L. van den Briel

menkes@asu.edu

Department Industrial Engineering
Arizona State University, Tempe, AZ 85281 USA

Thomas Vossen

vossen@colorado.edu

Leeds School Business
University Colorado Boulder, Boulder CO, 80309 USA

Subbarao Kambhampati

rao@asu.edu

Department Computer Science Engineering
Arizona State University, Tempe, AZ 85281 USA

Abstract
represent planning set loosely coupled network flow problems,
network corresponds one state variables planning domain. network
nodes correspond state variable values network arcs correspond value
transitions. planning problem find path (a sequence actions) network
that, merged, constitute feasible plan. paper present number integer programming formulations model loosely coupled networks
varying degrees flexibility. Since merging may introduce exponentially many ordering
constraints implement so-called branch-and-cut algorithm, constraints
dynamically generated added formulation needed. results
promising, improve upon previous planning integer programming approaches
lay foundation integer programming approaches cost optimal planning.

1. Introduction
integer programming1 approaches automated planning able
scale well compilation approaches (i.e. satisfiability constraint satisfaction), extremely successful solution many real-world large scale
optimization problems. Given integer programming framework potential
incorporate several important aspects real-world automated planning problems (for
example, numeric quantities objective functions involving costs utilities),
significant motivation investigate effective integer programming formulations
classical planning could lay groundwork large scale optimization (in terms
cost resources) automated planning. paper, study novel decomposition based approach automated planning yields effective integer programming
formulations.
1. use term integer programming refer integer linear programming unless stated otherwise.
c
2008
AI Access Foundation. rights reserved.

fiVan den Briel, Vossen & Kambhampati

Decomposition general approach solving problems efficiently. involves
breaking problem several smaller subproblems solving subproblems separately. paper use decomposition break planning problem
several interacting (i.e. loosely coupled) components. decomposition, planning
problem involves finding solutions individual components trying merge
feasible plan. general approach, however, prompts following questions:
(1) components, (2) component solutions, (3) hard
merge individual component solutions feasible plan?
1.1 Components
let components represent state variables planning problem. Figure 1
illustrates idea using small logistics example, one truck package
needs moved location 1 location 2. total five components
example, one state variable. represent components appropriately
defined network, network nodes correspond values state variable
(for atoms = true F = false), network arcs correspond value
transitions. source node network, represented small in-arc, corresponds
initial value state variable. sink node(s), represented double circles,
correspond goal value(s) state variable. Note effects action
trigger value transitions state variables. example, loading package
location 1 makes atom pack-in-truck true pack-at-loc1 false. addition, loading
package location 1 requires atom truck-at-loc1 true.
idea components representing state variables planning problem
used state variable representation, particularly synergistic multivalued state variables. Multi-valued state variables provide compact representation
planning problem binary-valued counterparts. Therefore, making
conversion multi-valued state variables reduce number components
create better partitioning constraints. Figure 2 illustrates use multi-valued
state variables small logistics example. two multi-valued state variables
problem, one characterize location truck one characterize
location package. network representation, nodes correspond state
variable values (1 = at-loc1, 2 = at-loc2, = in-truck), arcs correspond
value transitions.
1.2 Component Solutions
let component solutions represent path value transitions state variables.
networks, nodes arcs appear layers. layer represents plan period
which, depending structure network, one value transitions
occur. networks Figures 1 2 three layers (i.e. plan periods)
structure allows values persist change exactly per period. layers used
solve planning problem incrementally. is, start one layer network
try solve planning problem. plan found, networks extended
one extra layer new attempt made solve planning problem. process
repeated plan found time limit reached. Figures 1 2, path (i.e.
218

fiLoosely Coupled Formulations Automated Planning

loc1

truck-at-loc1

loc2



Load loc1

F
truck-at-loc2





-





Drive loc1loc2

-



F





-





Unload loc2

-

F




F

-


F

Unload loc2

F




F

F

F
Load loc1

-

F
-

F

F
pack-in-truck




F

F
Load loc1

F
pack-at-loc2

Drive loc1loc2

F

F
pack-at-loc1




F

Unload loc2

F


F

Figure 1: Logistics example broken five components (binary-valued state variables)
represented network flow problems.

truck-location

1

Load loc1

2
pack-location

1

1

Drive loc1loc2

2
Load loc1

1

1

Unload loc2

2
-

1

1
2

Unload loc2

1

2

2

2

2









Figure 2: Logistics example broken two components (multi-valued state variables)
represented network flow problems.

solution) source node one sink nodes highlighted network.
Since execution action triggers value transitions state variables, path
network corresponds sequence actions. Consequently, planning problem
thought collection network flow problems problem find
path (i.e. sequence actions) networks. However, interactions
219

fiVan den Briel, Vossen & Kambhampati

networks impose side constraints network flow problems, complicate
solution process.
1.3 Merging Process
solve loosely coupled networks using integer programming formulations. One
design choice make expand networks (i.e. components) together,
cost finding solutions individual networks well merging depends
difficulty solving integer programming formulation. This, turn, typically depends
size integer programming formulation, partly determined
number layers networks. simplest idea number
layers networks equal length plan, sequential planning
plan length equals number actions plan. case, many
transitions networks actions plan, difference
sequence actions corresponding path network could contain no-op actions.
idea reduce required number layers allowing multiple actions
executed plan period. exactly done Graphplan (Blum & Furst,
1995) planners adopted Graphplan-style definition parallelism.
is, two actions executed parallel (i.e. plan period) long
non-interfering. formulations adopt general notions parallelism.
particular, relax strict relation number layers networks
length plan changing network representation state variables.
example, allowing multiple transitions network per plan period permit
interfering actions executed plan period. This, however, raises issues
solutions individual networks searched combined.
network representations state variables allow multiple transitions
network per plan period, thus become flexible, becomes harder merge
solutions feasible plan. Therefore, evaluate tradeoffs allowing flexible
representations, look variety integer programming formulations.
refer integer programming formulation uses network representation
shown Figures 1 2 one state change model, allows one
transition (i.e. state change) per plan period state variable. Note network
representation plan period mimics Graphplan-style parallelism. is, two actions
executed plan period one action delete precondition
add-effect action. flexible representation values change
persist change refer generalized one state
change model. Clearly, increase number changes allow plan
period. representations values change twice k times, refer
generalized two state change generalized k state change model respectively.
One disadvantage generalized k state change model creates one variable
way k value changes, thus introduces exponentially many variables per
plan period. Therefore, another network representation consider allows path
value transitions value visited per plan period.
way, limit number variables, may introduce cycles networks.
220

fiLoosely Coupled Formulations Automated Planning

integer programming formulation uses representation referred state
change path model.
general, allowing multiple transitions network per plan period (i.e. layer),
complex merging process becomes. particular, merging process checks
whether actions solutions individual networks linearized
feasible plan. integer programming formulations, ordering constraints ensure feasible
linearizations. may, however, exponentially many ordering constraints
generalize Graphplan-style parallelism. Rather inserting constraints
integer programming formulation front, add needed using branch-and-cut
algorithm. branch-and-cut algorithm branch-and-bound algorithm certain
constraints generated dynamically throughout branch-and-bound tree.
show performance integer programming (IP) formulations show new
potential competitive SATPLAN04 (Kautz, 2004). significant result
forms basis sophisticated IP-based planning systems capable
handling numeric constraints non-uniform action costs. particular, new potential
IP formulations led successful use solving partial satisfaction planning
problems (Do, Benton, van den Briel, & Kambhampati, 2007). Moreover, initiated
new line work integer linear programming used heuristic state-space
search automated planning (Benton, van den Briel, & Kambhampati, 2007; van den
Briel, Benton, Kambhampati, & Vossen, 2007).
remainder paper organized follows. Section 2 provide brief
background integer programming discuss approaches used integer
programming solve planning problems. Section 3 present series integer programming formulations adopt different network representation. describe
set loosely coupled networks, provide corresponding integer programming
formulation, discuss different variables constraints. Section 4 describe
branch-and-cut algorithm used solving formulations. provide
general background branch-and-cut concept show apply formulations means example. Section 5 provides experimental results determine
characteristics approach greatest impact performance. Related
work discussed Section 6 conclusions given Section 7.

2. Background
Since formulations based integer programming, briefly review technique
discuss use planning. mixed integer program represented linear objective
function set linear inequalities:
min{cx : Ax b, x1 , ..., xp 0 integer, xp+1 , ..., xn 0},
(m n) matrix, c n-dimensional row vector, b m-dimensional
column vector, x n-dimensional column vector variables. variables
continuous (p = 0) linear program, variables integer (p = n)
integer program, x1 , ..., xp {0, 1} mixed 0-1 program. set =
{x1 , ..., xp 0 integer, xp+1 , ..., xn 0 : Ax b} called feasible region,
n-dimensional column vector x called feasible solution x S. Moreover, function
221

fiVan den Briel, Vossen & Kambhampati

cx called objective function, feasible solution x called optimal solution
objective function small possible, is, cx = min{cx : x S}
Mixed integer programming provides rich modeling formalism general
propositional logic. propositional clause represented one linear inequality
0-1 variables, single linear inequality 0-1 variables may require exponentially many
clauses (Hooker, 1988).
widely used method solving (mixed) integer programs applying
branch-and-bound algorithm linear programming relaxation, much easier
solve2 . linear programming (LP) relaxation linear program obtained
original (mixed) integer program relaxing integrality constraints:
min{cx : Ax b, x1 , ..., xn 0}
Generally, LP relaxation solved every node branch-and-bound tree,
(1) LP relaxation gives integer solution, (2) LP relaxation value inferior
current best feasible solution, (3) LP relaxation infeasible, implies
corresponding (mixed) integer program infeasible.
ideal formulation integer program one solution linear programming relaxation integral. Even though every integer program ideal
formulation (Wolsey, 1998), practice hard characterize ideal formulation may require exponential number inequalities. problems ideal
formulation cannot determined, often desirable find strong formulation
integer program. Suppose feasible regions P1 = {x Rn : A1 x b1 }
P2 = {x Rn : A2 x b2 } describe linear programming relaxations two IP formulations problem. say formulation P1 stronger formulation P2
P1 P2 . is, feasible region P1 subsumed feasible region P2 .
words P1 improves quality linear relaxation P2 removing fractional extreme
points.
exist numerous powerful software packages solve mixed integer programs.
experiments make use commercial solver CPLEX 10.0 (Inc., 2002),
currently one best LP/IP solvers.
use integer programming techniques solve artificial intelligence planning problems intuitive appeal, especially given success IP solving similar types
problems. example, IP used extensively solving problems transportation, logistics, manufacturing. Examples include crew scheduling, vehicle routing,
production planning problems (Johnson, Nemhauser, & Savelsbergh, 2000). One potential
advantage IP techniques provide natural way incorporate several important
aspects real-world planning problems, numeric constraints objective functions
involving costs utilities.
Planning integer programming has, nevertheless, received limited attention. One
first approaches described Bylander (1997), proposes LP heuristic
partial order planning algorithms. LP heuristic helps reduce number
expanded nodes, evaluation rather time-consuming. general, performance
2. integer programming problem N P -complete (Garey & Johnson, 1979) linear programming problem polynomially solvable (Karmarkar, 1984).

222

fiLoosely Coupled Formulations Automated Planning

IP often depends structure problem problem formulated.
importance developing strong IP formulations discussed Vossen et al. (1999),
compare two formulations classical planning: (1) straightforward formulation
based conversion propositional representation SATPLAN yields
mediocre results, (2) less intuitive formulation based representation
state transitions leads considerable performance improvements. Several ideas
improve formulation based representation state transitions described
Dimopoulos (2001). ideas implemented IP-based planner Optiplan
(van den Briel & Kambhampati, 2005). Approaches rely domain-specific knowledge
proposed Bockmayr Dimopoulos (1998, 1999). exploiting structure
planning problem IP formulations often provide encouraging results. use
LP IP also explored non-classical planning. Dimopoulos Gerevini
(2002) describe IP formulation temporal planning Wolfman Weld (1999)
use LP formulations combination satisfiability-based planner solve resource
planning problems. Kautz Walser (1999) also solve resource planning problems,
use domain-specific IP formulations.

3. Formulations
section describes four IP formulations model planning problem collection
loosely coupled network flow problems. network represents state variable,
nodes correspond state variable values, arcs correspond value
transitions. state variables based SAS+ planning formalism (Backstrom &
Nebel, 1995), planning formalism uses multi-valued state variables instead
binary-valued atoms. action SAS+ modeled pre-, post- prevailconditions. pre- post-conditions express state variables changed
values must execution action, prevailconditions specify unchanged variables must specific value
execution action. SAS+ planning problem described tuple
= hC, A, s0 , where:
C = {c1 , ..., cn } finite set state variables, state variable c C
associated domain Vc implicitly defined extended domain Vc+ = Vc {u},
u denotes undefined value. state variable c C, s[c] denotes value
c state s. value c said defined state s[c] 6= u.
total state space = Vc1 ... Vcn partial state space + = Vc+1 ... Vc+n
implicitly defined.
finite set actions form hpre, post, previ, pre denotes preconditions, post denotes post-conditions, prev denotes prevail-conditions.
action A, pre[c], post[c] prev[c] denotes respective conditions
state variable c. following two restrictions imposed actions: (1)
value state variable defined, never become undefined. Hence,
c C, pre[c] 6= u pre[c] 6= post[c] 6= u; (2) prevail- post-condition
action never define value state variable. Hence, c C,
either post[c] = u prev[c] = u both.
223

fiVan den Briel, Vossen & Kambhampati

s0 denotes initial state + denotes goal state. SAS+
planning allows initial state goal state partial states, assume
s0 total state partial state. say state satisfied
state c C s[c] = u s[c] = t[c]. implies
[c] = u state variable c, defined value f Vc satisfies goal c.
obtain SAS+ description planning problem use translator component
Fast Downward planner (Helmert, 2006). translator stand-alone component
contains general purpose algorithm transforms propositional description
planning problem SAS+ description. algorithm provides efficient
grounding minimizes state description length based preprocessing
algorithm MIPS planner (Edelkamp & Helmert, 1999).
remainder section introduce notation describe IP formulations. formulations presented way progressively generalize
Graphplan-style parallelism incorporation flexible network representations. formulation describe underlying network, define variables
constraints. concentrate objective function much
constraints tolerate feasible plans.
3.1 Notation
formulations described paper assume following information given:
C: set state variables;
Vc : set possible values (i.e. domain) state variable c C;
Ec : set possible value transitions state variable c C;
Gc = (Vc , Ec ) : directed domain transition graph every c C;
State variables represented domain transition graph, nodes correspond
possible values, arcs correspond possible value transitions. example
domain transition graph variable given Figure 3. example depicts
complete graph, domain transition graph need complete graph.
Furthermore, assume given:
Eca Ec represents effect action c;
Vca Vc represents prevail condition action c;

E
AE
c := {a : |Ec | > 0} represents actions effect c, Ac (e)
represents actions effect e c;

AVc := {a : |Vca | > 0} represents actions prevail condition c,
AVc (f ) represents actions prevail condition f c;
V
C := {c C : AE
c Ac } represents state variables action
effect prevail condition.

224

fiLoosely Coupled Formulations Automated Planning

f

g

h
Figure 3: example domain transition graph, Vc = {f, g, h} possible
values (states) c Ec = {(f, g), (f, h), (g, f ), (g, h), (h, f ), (h, g)}
possible value transitions c.

Hence, action defined effects (i.e. pre- post-conditions) prevail
conditions. SAS+ planning, actions one effect prevail condition
state variable. words, c C, Eca Vca
empty |Eca | + |Vca | 1. example effects prevail conditions affect one
domain transition graphs given Figure 4.

f

f

f

g

g

g

h

h

h

Figure 4: example action effects prevail conditions represented
domain transition graph. Action implications three state variables C =
{c1 , c2 , c3 }. effects represented Eca1 = {(f, g)} Eca2 = {(h, f )},
prevail condition represented Vca3 = {h}.
addition, use following notation:
Vc+ (f ): denote in-arcs node f domain transition graph Gc ;
Vc (f ): denote out-arcs node f domain transition graph Gc ;
+
Pc,k
(f ): denote paths length k domain transition graph Gc end
+
node f . Note Pc,1
(f ) = Vc+ (f ).

Pc,k
(f ): denote paths length k domain transition graph Gc start

node f . Note Pc,1
(f ) = Vc (f ).

225

fiVan den Briel, Vossen & Kambhampati

(f ): denote paths length k domain transition graph G visit node
Pc,k
c
f , start end f .

3.2 One State Change (1SC) Formulation
first IP formulation incorporates network representation seen
Figures 1 2. name one state change relates number transitions
allow state variable per plan period. restriction allowing one value
transition network also restricts actions execute plan
period. happens case network representation 1SC formulation
incorporates standard notion action parallelism used Graphplan (Blum
& Furst, 1995). idea actions executed plan period long
delete precondition add-effect another action. terms value
transitions state variables, saying actions executed plan
period long change state variable (i.e. one value
change value persistence state variable).
3.2.1 State Change Network
Figure 5 shows single layer (i.e. period) network underlies 1SC formulation. set IP formulation plan periods, + 1 layers
nodes layers arcs network (the zeroth layer nodes initial
state remaining layers nodes arcs successive plan periods).
possible state transition arc state change network. horizontal
arcs correspond persistence value, diagonal arcs correspond value
changes. solution path individual network follows arcs whose transitions
supported action effect prevail conditions appear solution plan.
1SC network
f

f

g

g

h

h

Period

Figure 5: One state change (1SC) network.

3.2.2 Variables
two types variables formulation: action variables represent execution action, arc flow variables represent state transitions network.
226

fiLoosely Coupled Formulations Automated Planning

use separate variables changes state variable (the diagonal arcs 1SC
network) persistence value state variable (the horizontal arcs
1SC network). variables defined follows:
xat {0, 1}, A, 1 ; xat equal 1 action executed plan period
t, 0 otherwise.
yc,f,t {0, 1}, c C, f Vc , 1 ; yc,f,t equal 1 value f state
variable c persists period t, 0 otherwise.
yc,e,t {0, 1}, c C, e Ec , 1 ; yc,e,t equal 1 transition e Ec
state variable c executed period t, 0 otherwise.

3.2.3 Constraints
two classes constraints. constraints network flows
state variable network constraints action effects determine interactions
networks. 1SC integer programming formulation is:
State change flows c C, f Vc

X
1 f = s0 [c]
yc,e,1 + yc,f,1 =
0 otherwise.


(1)

eVc (f )

X

X

yc,e,t+1 + yc,f,t+1 =

X

yc,e,t + yc,f,t

1 1

(2)

eVc+ (f )

eVc (f )

yc,e,T + yc,f,T

= 1

f = [c]

(3)

eVc+ (f )

Action implications c C, 1
X
xat = yc,e,t e Ec

(4)

aA:eEca

xat yc,f,t

A, f Vca

(5)

Constraints (1), (2), (3) network flow constraints state variable c C.
Constraint (1) ensures path state transitions begins initial state
state variable constraint (3) ensures that, goal exists, path ends goal
state state variable. Note that, goal value state variable c undefined
(i.e. [c] = u) path state transitions may end values f Vc .
Hence, need goal constraint state variables whose goal states [c]
undefined. Constraint (2) flow conservation equation enforces continuity
constructed path.
Actions may introduce interactions state variables. instance, effects
load action logistics example affect two different state variables. Actions link
state variables interactions represented action implication
227

fiVan den Briel, Vossen & Kambhampati

constraints. transition e Ec , constraints (4) link action execution variables
e effect (i.e. e Eca ) arc flow variables. example, action
xat effect e Eca executed, path state variable c must follow arc
represented yc,e,t. Likewise, choose follow arc represented yc,e,t,
exactly one action xat e Eca must executed. summation left hand side
prevents two actions interfering other, hence one action may
cause state change e state variable c period t.
Prevail conditions action link state variables similar way action effects
do. Specifically, constraint (5) states action executed period (xat = 1),
prevail condition f Vca required state variable c period (yc,f,t = 1).
3.3 Generalized One State Change (G1SC) Formulation
second formulation incorporate network representation 1SC
formulation, adopt general interpretation value transitions, leads
unconventional notion action parallelism. G1SC formulation relax
condition parallel actions arranged order requiring weaker condition.
allow actions executed plan period long exists ordering
feasible. specifically, within plan period set actions feasible (1)
exists ordering actions preconditions satisfied, (2)
one state change state variables. generalization conditions
similar Rintanen, Heljanko Niemela (2006) refer -step semantics
semantics.
illustrate basic concept, let us examine small logistics example introduced Figure 1. solution problem load package location 1, drive
truck location 1 location 2, unload package location 2. Clearly, plan
would require three plan periods Graphplan-style parallelism three actions
interfere other. If, however, allow load loc1 drive loc1 loc2
action executed plan period, exists ordering
two actions feasible, namely load package location 1 driving
truck location 2. key idea behind example clear: may
possible find set actions linearized order, may nevertheless exist ordering actions viable. question is, course,
incorporate idea IP formulation.

truck-location

1

Load loc1
Drive loc1loc2

2
pack-location

1

1

Unload loc2

2
Load loc1

1

1
2

Unload loc2

1

2

2

2







Figure 6: Logistics example represented network flow problems generalized arcs.
228

fiLoosely Coupled Formulations Automated Planning

example illustrates looking set constraints allow sets
actions which: (1) action preconditions met, (2) exists ordering
actions plan period feasible, (3) within state variable, value
changed once. incorporation ideas requires minor modifications
1SC formulation. Specifically, need change action implication constraints
prevail conditions add new set constraints call ordering
implication constraints.
3.3.1 State Change Network
minor modifications revealed G1SC network. network
identical 1SC network, interpretation transition arcs somewhat different.
incorporate new set conditions, implicitly allow values persist (the dashed
horizontal arcs G1SC network) tail head transition arc.
interpretation implicit arcs plan period value may required
prevail condition, value may change, new value may also required
prevail condition shown Figure 7.
G1SC network

Generalized state change arc

f

f

f

f

g

g

g

g

h

h

h

h

Period

Period

Figure 7: Generalized one state change (G1SC) network.

3.3.2 Variables
Since G1SC network similar 1SC network variables used, thus,
action variables represent execution action, arc flow variables represent
flow network. difference interpretation state change arcs
dealt constraints G1SC formulation, therefore introduce
new variables. variable definitions, refer Section 3.2.2.
3.3.3 Constraints
three classes constraints, is, constraints network flows
state variable network, constraints linking flows action effects prevail
conditions, ordering constraints ensure actions plan linearized
feasible ordering.
229

fiVan den Briel, Vossen & Kambhampati

network flow constraints G1SC formulation identical 1SC
formulation given (1)-(3). Moreover, constraints link flows action
effects equal action effect constraints 1SC formulation given (4).
G1SC formulation differs 1SC formulation relaxes condition
parallel actions arranged order requiring weaker condition. weaker
condition affects constraints link flows action prevail conditions,
introduces new set ordering constraints. constraints G1SC formulation
given follows:
Action implications c C, 1
X
X
xat yc,f,t +
yc,e,t +
eVc+ (f )

Ordering implications
X

yc,e,t

A, f Vca

(6)

eVc (f )

xat |V ()| 1 cycles Gprec

(7)

aV ()

Constraint (6) incorporates new set conditions actions executed
plan period. particular, need ensure state variable c,
value f Vc holds required prevail condition action plan period t.
three possibilities: (1) value f holds c throughout period. (2) value f
holds initially c, value changed value f another action. (3)
value f hold initially c, value changed f another action.
either three cases value f holds point period prevail
condition action satisfied. words, value f may prevail implicitly long
state change includes f . before, prevail implication constraints
link action prevail conditions corresponding network arcs.
action implication constraints ensure preconditions actions
plan satisfied. This, however, guarantee actions linearized
feasible order. Figure 7 indicates implied orderings actions.
Actions require value f prevail condition must executed action
changes f g. Likewise, action changes f g must executed
actions require value g prevail condition. state change flow action
implication constraints outlined indicate ordering actions,
ordering could cyclic therefore infeasible. make sure ordering
acyclic start creating directed implied precedence graph Gprec = (V prec , E prec ).
graph nodes V prec correspond actions, is, V prec = A,
create directed arc (i.e. ordering) two nodes (a, b) E prec action
executed action b time period t, b executed a. particular,

[
[
E prec =
(a, b)
(a, b)
(a,b)AA,cC,f Vca ,eEcb :

eVc,f

230

(a,b)AA,cC,gVcb ,eEca :
+
eVc,g

fiLoosely Coupled Formulations Automated Planning

implied orderings become immediately clear Figure 8. figure left
depicts first set orderings expression E prec . says ordering
two actions b executed plan period implied action requires
value prevail action b deletes. Similarly, figure right depicts second set
orderings expression E prec . is, ordering implied action adds
prevail condition b.
f



f

f

b

f



g

g

g

h

h

h

b

g

h

Figure 8: Implied orderings G1SC formulation.
ordering implication constraints ensure actions final solution
linearized. basically involve putting n-ary mutex relation actions
involved cycle. Unfortunately, number ordering implication constraints
grows exponentially number actions. result, impossible solve
resulting formulation using standard approaches. address complication
implementing branch-and-cut approach ordering implication constraints
added dynamically formulation. approach discussed Section 4.
3.4 Generalized k State Change (GkSC) Formulation
G1SC formulation actions executed plan period (1) exists
ordering actions preconditions satisfied, (2) occurs
one value change state variables. One obvious generalization
would relax second condition allow kc value changes state
variable c, kc |Vc | 1. allowing multiple value changes state variable per
plan period we, fact, permit series value changes. Specifically, GkSC model
allows series value changes.
Obviously, tradeoff loosening networks versus amount
work takes merge individual plans. implemented GkSC
formulation, provide insight tradeoff describing evaluating GkSC
formulation kc = 2 c C refer special case generalized
two state change (G2SC) formulation. One reason restrict special case
general case k state changes would introduce exponentially many variables
formulation. IP techniques, however, deal exponentially many
variables (Desaulniers, Desrosiers, & Solomon, 2005), discuss here.
3.4.1 State Change Network
network underlies G2SC formulation equivalent G1SC, spans extra
layer nodes arcs. extra layer allows us series two transitions per plan
231

fiVan den Briel, Vossen & Kambhampati

period. transitions generalized implicitly allow values persist
G1SC network. Figure 9 displays network corresponding G2SC formulation.
G2SC network generalized one two state change arcs. example,
generalized one state change arc transition (f, g), generalized two
state changes arc transitions {(f, g), (g, h)}. Since arcs generalized, value
visited also persisted. also allow cyclic transitions, as, {(f, g), (g, f )}
f prevail condition action. allow cyclic transitions
f prevail condition action, action ordering plan period
implied anymore (i.e. prevail condition f would either occur
value transitions g, transitions back f ). Thus prevail condition
f safely allow cyclic transition {(f, g), (g, f )}.
1 state change arcs

2 state changes arcs

f

f

f

f
f

f

g

g

g

g
g

g

h

h

h

h
h

h

Period

Period

Figure 9: Generalized two state change (G2SC) network. left subnetwork
consists generalized one state change arcs no-op arcs, right subnetwork consists generalized two state change arcs. subnetwork
two state change arcs may include cyclic transitions, as, {(f, g), (g, f )}
long f prevail condition action.

3.4.2 Variables
variables representing execution action, variables representing flows one state change (diagonal arcs) persistence (horizontal arcs).
addition, variables representing paths two consecutive state changes. Hence,
variables pair state changes (f, g, h) (f, g) Ec (g, h) Ec .
restrict paths visit unique values only, is, f 6= g, g 6= h, h 6= f ,
f prevail condition action also allow paths f = h.
variables G1SC formulation also used G2SC formulation. is, however,
additional variable represent arcs allow two state changes:
yc,e1,e2 ,t {0, 1}, c C, (e1 , e2 ) Pc,2 , 1 ; yc,e1,e2 ,t equal 1
exists value f Vc transitions e1 , e2 Ec , e1 Vc+ (f )
e2 Vc (f ), state variable c executed period t, 0 otherwise.
232

fiLoosely Coupled Formulations Automated Planning

3.4.3 Constraints
three classes constraints, given follows:
State change flows c C, f Vc
X

yc,e1,e2 ,1 +


(e1 ,e2 )Pc,2
(f )

X

X

yc,e,1 + yc,f,1 =

eVc (f )

yc,e1,e2 ,t+1 +


(e1 ,e2 )Pc,2
(f )

X



1
0

f = s0 [c]
otherwise.

(8)

1 1

(9)

yc,e,t+1 + yc,f,t+1 =

eVc (f )

X

+
(e1 ,e2 )Pc,2
(f )

X

X

yc,e1,e2 ,t +

yc,e,t + yc,f,t

eVc+ (f )

yc,e1,e2 ,T +

+
(e1 ,e2 )Pc,2
(f )

X

yc,e,T + yc,f,T

= 1 {f [c]}

(10)

eVc+ (f )

Action implications c C, 1
X
X
xat = yc,e,t +
aA:eEca

yc,e1,e2 ,t

e Ec

X

yc,e1,e2 ,t +

(11)

(e1 ,e2 )Pc,2 :e1 =ee2 =e

xat yc,f,t +

X

eVc+ (f )

X

X

yc,e,t +

eVc (f )

X

yc,e1,e2 ,t +

+
(e1 ,e2 )Pc,2
(f )

Ordering implications
X

yc,e,t +

(f )
(e1 ,e2 )Pc,2

yc,e1,e2 ,t

A, f Vca

(12)


(e1 ,e2 )Pc,2
(f )

xat |V ()| 1 cycles Gprec

(13)

aV ()

Constraints (8), (9), (10) represent flow constraints G2SC network. Constraints (11) (12) link action effects prevail conditions corresponding
flows, constraint 13 ensures actions linearized feasible ordering.
3.5 State Change Path (PathSC) Formulation
several ways generalize network representation G1SC formulation
loosen interaction networks. GkSC formulation presented one
generalization allows k transitions state variable per plan period. Since
uses exponentially many variables another way generalize network representation
G1SC formulation requiring value true per plan
period. illustrate idea consider logistics example again, use
233

fiVan den Briel, Vossen & Kambhampati

truck-location

1

Load loc1
Drive loc1loc2
Unload loc2

2

pack-location

1

1
2

Load loc1
Unload loc2
-

1

2

2





Figure 10: Logistics example represented network flow problems allow path
value transitions per plan period value true once.

network representation allows path transitions per plan period depicted
Figure 10.
Recall solution logistics example consists three actions: first load
package location 1, drive truck location 1 location 2, last unload
package location 2. Clearly, solution would allowed within single plan
period Graphplan-style parallelism. Moreover, would also allowed within
single period G1SC formulation. reason number value
changes package-location state variable two. First, changes pack-at-loc1
pack-in-truck, changes pack-in-truck pack-at-loc2. before, however,
exists ordering three actions feasible. key idea behind
example show allow multiple value changes single period. limit
value changes state variable simple paths, is, one period value
visited once, still use implied precedences determine ordering
restrictions.
3.5.1 State Change Network
formulation value true plan period, hence
number value transitions plan period limited kc kc = |Vc | 1
c C. PathSC network, nodes appear layers correspond values
state variable. However, layer consists twice many nodes. set
IP encoding maximum number plan periods layers. Arcs
within layer correspond transitions value persistence, arcs layers
ensure plan periods connected other.
Figure 11 displays network corresponding state variable c domain Vc =
{f, g, h} allows multiple transitions per plan period. arcs pointing rightwards
correspond persistence value, arcs pointing leftwards correspond
value changes. one plan period needed curved arcs pointing rightwards
234

fiLoosely Coupled Formulations Automated Planning

link layers two consecutive plan periods. Note unit capacity
arcs, path network visit node once.
PathSC network
f

f

g

g

h

h

Period

Figure 11: Path state change (PathSC) network.
3.5.2 Variables
action execution variables arc flow variables (as defined previous
formulations), linking variables connect networks two consecutive
time periods. variables defined follows:
zc,f,t {0, 1}, c C, f Vc , 0 ; zc,f,t equal 1 value f state
variable c end value period t, 0 otherwise.
3.5.3 Constraints
previous formulations, state change flow constraints, action implication
constraints, ordering implication constraints. main difference underlying
network. PathSC integer programming formulation given follows:
State change flows c C, f Vc

1 f = s0 [c]
zc,f,0 =
0 otherwise.
X
yc,e,t + zc,f,t1 = yc,f,t

(14)
(15)

eVc+ (f )

yc,f,t =

X

yc,e,t + zc,f,t

1 1

(16)

eVc (f )

zc,f,T

= 1 f [c]

Action implications c C, 1
X
xat = yc,e,t

(17)

e Ec

(18)

f Vca

(19)

aA:eEca

xat yc,f,t
235

fiVan den Briel, Vossen & Kambhampati

Ordering implications
X

xat |V ()| 1 cycles Gprec



(20)

aV ()

Constraints (14)-(17) network flow constraints. node, except initial
goal state nodes, ensure balance flow (i.e. flow-in must equal flow-out).
initial state node supply one unit flow goal state node demand
one unit flow, given constraints (14) (17) respectively. interactions
actions impose upon different state variables represented action implication
constraints (18) (19), discussed earlier.



implied precedence graph formulation given Gprec = (V prec , E prec ).
extra set arcs incorporate implied precedences introduced

two actions imply state change class c C. nodes V prec

correspond actions, arc (a, b) E prec action executed
action b time period, b executed a. specifically,

E prec



[

= E prec

(a, b)

(a,b)AA,cC,f Vc ,eEca ,e Ecb :
eVc+ (f )e Vc (f )

before, ordering implication constraints (20) ensure actions solution plan linearized feasible ordering.

4. Branch-and-Cut Algorithm
IP problems usually solved LP-based branch-and-bound algorithm. basic
structure technique involves binary enumeration tree branches pruned
according bounds provided LP relaxation. root node enumeration tree
represents LP relaxation original IP problem node represents
subproblem objective function constraints root node except
additional bound constraints. IP solvers use LP-based branch-and-bound
algorithm combination various preprocessing probing techniques. last
years significant improvement performance solvers (Bixby,
2002).
LP-based branch-and-bound algorithm, LP relaxation original IP problem (the solution root node) rarely integer. integer variable x
fractional solution v branch create two new subproblems, bound
constraint x v added left-child node, x v added right-child
node. branching process carried recursively expand subproblems whose
solution remains fractional. Eventually, enough bounds placed variables,
integer solution found. value best integer solution found far, Z , referred
incumbent used pruning.
236

fiLoosely Coupled Formulations Automated Planning

minimization problem, branches emanating nodes whose solution value ZLP
greater current incumbent, Z , never give rise better integer solution
child node smaller feasible region parent. Hence, safely eliminate
nodes consideration prune them. Nodes whose feasible region
reduced empty set, many bounds placed variables,
pruned well.
solving IP problem LP-based branch-and-bound algorithm must
consider following two decisions. several integer variables fractional solution,
variable branch next, branch currently working
pruned, subproblem solve next? Basic rules include use
fractional variable rule branching variable selection best objective value rule
node selection.
formulations standard LP-based branch-and-bound algorithm approach
ineffective due large number (potentially exponentially many) ordering implication
constraints G1SC, G2SC, PathSC formulations. possible reduce
number constraints introducing additional variables (Martin, 1991), resulting
formulations would still intractable smallest problem instances. Therefore,
solve IP formulations so-called branch-and-cut algorithm, considers
ordering implication constraints implicitly. branch-and-cut algorithm branch-andbound algorithm certain constraints generated dynamically throughout
branch-and-bound tree. flowchart branch-and-cut algorithm given Figure 12.
If, solving LP relaxation, unable prune node basis
LP solution, branch-and-cut algorithm tries find violated cut, is, constraint
valid satisfied current solution. also known separation
problem. one violated cuts found, constraints added formulation
LP solved again. none found, algorithm creates branch
enumeration tree (if solution current subproblem fractional) generates
feasible solution (if solution current subproblem integral).
basic idea branch-and-cut leave constraints LP relaxation
many handle efficiently, add formulation
become binding solution current LP. Branch-and-cut algorithms
successfully applied solving hard large-scale optimization problems wide
variety applications including scheduling, routing, graph partitioning, network design,
facility location problems (Caprara & Fischetti, 1997).
branch-and-cut algorithm stop soon find first feasible solution,
implicitly enumerate nodes (through pruning) find optimal solution
given objective function. Note formulations used find bounded
length optimal plans. is, find optimal plan given plan period (i.e. bounded
length). experimental results, however, focus finding feasible solutions.
4.1 Constraint Generation
point runtime cut generator called solution
current LP problem, consists LP relaxation original IP problem plus
added bound constraints added cuts. implementation branch-and-cut
237

fiVan den Briel, Vossen & Kambhampati

START

STOP

Initialize LP

yes

LP solver

Feasible?



Prune

Nodes found?

Node selection

yes
ZLP > Z*?

yes


Cut generator
yes

Cuts found?

Integer?



Branching

yes
Optimize?

yes



Figure 12: Flowchart branch-and-cut algorithm. finding feasible solution (i.e.
optimize = no) algorithm stops soon first feasible integer solution
found. searching optimal solution (i.e. optimize = yes)
given formulation continue open nodes left.

238

fiLoosely Coupled Formulations Automated Planning

algorithm, start LP relaxation ordering implication constraints
omitted. given solution current LP relaxation, could fractional,
separation problem determine whether solution violates one omitted ordering
implication constraints. so, identify violated ordering implication constraints, add
formulation, resolve new problem.
4.1.1 Cycle Identification
G1SC, G2SC, PathSC formulations ordering implication constraint violated cycle implied precedence graph. Separation problems involving
cycles occur numerous applications. Probably best known kind traveling salesman problem subtours (i.e. cycles) identified subtour elimination
constraints added current LP. algorithm separating cycles based
one described Padberg Rinaldi (1991). interested finding shortest
cycle implied precedence graph, shortest cycle cuts fractional extreme
points. general idea behind approach follows:
1. Given solution LP relaxation, determine subgraph Gt plan period
consisting nodes xat > 0.
2. arcs (a, b) Gt , define weights wa,b := xat + xbt 1.
3. Determine shortest path distance da,b pairs ((a, b) Gt ) based arc
weights wa,b := 1 wa,b (for example, using Floyd-Warshall all-pairs shortest path
algorithm).
4. da,b wb,a < 0 arc (a, b) Gt , exists violated cycle constraint.
general principles behind branch-and-cut algorithms rather straightforward, number algorithmic implementation issues may significant impact overall performance. heart issues trade-off
computation time spent node enumeration tree number nodes
explored. One issue, example, decide generate violated cuts.
Another issue generated cuts (if any) added LP relaxation,
whether delete constraints added LP before.
implementation, addressed issues straightforward manner: cuts
generated every node enumeration tree, first cut found algorithm
added, constraints never deleted LP relaxation. However, given potential advanced strategies observed applications, believe
still may considerable room improvement.
4.1.2 Example
section show workings branch-and-cut algorithm G1SC
formulation using small hypothetical example involving two state variables c1 c2 , five
actions A1, A2, A3, A4, A5, one plan period. particular show
cycle detection procedure works ordering implication constraint generated.
239

fiVan den Briel, Vossen & Kambhampati

Figure 13 depicts solution current LP planning problem. state variable
c1 actions A1 A2 prevail condition g, A4 prevail condition
h, action A3 effect changes g h. Likewise, state variable c2
action A4 effect changes g f , action A5 changes g h,
action A1 prevail condition f . Note given solution fractional. Therefore
action variables fractional values. particular, xA1 = xA4 = 0.8,
xA5 = 0.2, xA2 = xA3 = 1. words, actions A2 A3 fully executed
actions A1, A4 A5 fractionally executed. Clearly, automated planning
fractional execution action meaning whatsoever, common
LP relaxation IP formulation gives fractional solution. simply try show
find violated cut even fractional solution. Also, note
actions A4 A5 interfering effects c2 . would generally infeasible,
actions executed fractionally, actually feasible solution LP
relaxation IP formulation.
State variable 1

State variable 2
A1

f

f

f

f
A4

A1,A2
g

g

g

h

h

g

A3
A4
h

A5

A1 = A4 = 0.8, A2 = A3 = 1

h
A1 = A4 = 0.8, A5 = 0.2

Figure 13: Solution small hypothetical planning example. solution current
LP flows indicated paths executes actions A1, A2, A3, A4,
A5.
order determine whether actions linearized feasible ordering first create implied precedence graph Gprec = (V prec , E prec ),
V prec = {A1, A2, A3, A4, A5} E prec = {(A1, A3),(A2, A3),(A3, A4),(A4, A1)}. ordering (A1, A3), example, established effects actions state variable
c1 . A1 prevail condition g c1 A3 changes g h c1 , implies
A1 must executed A3. orderings established similar way.
complete implied precedence graph example given Figure 14.
cycle detection algorithm gets implied precedence graph solution
current LP input. Weights arc (a, b) E prec determined values
action variables current solution. LP solution given Figure
13, example wA1,A3 = wA3,A4 = 0.8, wA2,A3 = 1, wA4,A1 = 0.6.
length shortest path A1 A4 using weights wa,b equal 0.4 (0.2+0.2). Hence,
dA1,A4 = 0.4 wA4,A1 = 0.6. Since dA1,A4 wA4,A1 < 0, violated cycle
(i.e. violated ordering implication) includes actions shortest path
A1 A4 (i.e. A1, A3, A4, retrieved shortest path algorithm).
240

fiLoosely Coupled Formulations Automated Planning

A3
A4
generates following ordering implication constraint xA1
1 + x1 + x1 2,
added current LP. Note ordering constraint violated current
A3
A4
LP solution, xA1
1 + x1 + x1 = 0.8 + 1 + 0.8 = 2.6. constraint added
LP, next solution select set actions violate newly added cut.
procedure continues cuts violated solution integer.

A1

Implied precedence graph
(0.6,0.4)

(0.8,0.2)

A4

(0.8,0.2)
A3

(1,0)
A2

A5

Figure 14: Implied precedence graph example, labels show (wa,b , wa,b ).

5. Experimental Results
described formulations based two key ideas. first idea decompose
planning problem several loosely coupled components represent components
appropriately defined network. second idea reduce number plan
periods adopting different notions parallelism use branch-and-cut algorithm
dynamically add constraints formulation order deal exponentially
many action ordering constraints efficient manner.
evaluate tradeoffs allowing flexible network representations compare
performance one state change (1SC) formulation, generalized one state change
formulation (G1SC), generalized two state change (G2SC) formulation, state
change path (PathSC) formulation. easy reference, overview formulations
given Figure 15.
experiments focus finding feasible solutions. Note, however,
formulations used bounded length optimal planning. is, given plan
period (i.e. bounded length), find optimal solution.
5.1 Experimental Setup
compare analyze formulations use STRIPS domains second
third international planning competitions (IPC2 IPC3 respectively). is,
Blocksworld, Logistics, Miconic, Freecell IPC2 Depots, Driverlog, Zenotravel,
Rovers, Satellite, Freecell IPC3. compare formulations
STRIPS domains IPC4 IPC5 mainly peripheral limitation
current implementation G2SC PathSC formulations. particular, G2SC
formulation cannot handle operators change state variable undefined value
defined value, PathSC formulation cannot handle operators domain
241

fiVan den Briel, Vossen & Kambhampati

1SC
state variable change
prevail value per
plan period.

G1SC
state variable change (and
prevail value
change) per plan period.

f

f

f

f

g

g

g

g

h

h

h

h

G2SC
state variable change (and
prevail value
change) twice per plan
period. Cyclic changes (f, g, f )
allowed f
prevail condition action

PathSC
state variable change
number times, value
true per plan
period.

f

f
f

f

f

f

g

g
g

g

g

g

h

h
h

h

h

h

Figure 15: Overview 1SC, G1SC, G2SC, PathSC formulations.

size state variable larger two. limitations could test
G2SC formulation Miconic, Satellite Rovers domains, could test
PathSC formulation Satellite domain.
order setup formulations translate STRIPS planning problem
multi-valued state description using translator Fast Downward planner (Helmert,
2006). formulation uses network representation starts setting
number plan periods equal one. try solve initial formulation
plan found, increased one, try solve new formulation. Hence,
IP formulation solved repeatedly first feasible plan found 30 minute
time limit (the time limit used international planning competitions)
reached. use CPLEX 10.0 (ILOG Inc., 2002), commercial LP/IP solver, solving
IP formulations 2.67GHz Linux machine 1GB memory.
set experiments follows. First, Section 5.2 provide brief overview
main results looking aggregated results IPC2 IPC3. Second, Section
5.3, give detailed analysis loosely coupled encodings planning
242

fiLoosely Coupled Formulations Automated Planning

focus tradeoffs reducing number plan periods solve planning problem
versus increased difficulty merging solutions different components. Third,
Section 5.4 briefly touch upon different state variable representations
planning problem influence performance.
5.2 Results Overview
general overview compare formulations following planning systems:
Optiplan (van den Briel & Kambhampati, 2005), SATPLAN04 (Kautz, 2004), SATPLAN06
(Kautz & Selman, 2006), Satplanner (Rintanen et al., 2006)3 .
Optiplan integer programming based planner participated optimal track
fourth international planning competition4 . Like formulations, Optiplan models
state transitions use factored representation planning domain.
particular, Optiplan represents state transitions atoms planning domain,
whereas formulations use multi-valued state variables. Apart this, Optiplan
similar 1SC formulation adopt Graphplan-style parallelism.
SATPLAN04, SATPLAN06, Satplanner satisfiability based planners. SATPLAN04 SATPLAN06 versions well known system SATPLAN (Kautz &
Selman, 1992), long track record international planning competitions.
Satplanner received much attention, among state-of-the-art planning satisfiability. Like formulations Satplanner generalizes Graphplan-style
parallelism improve planning efficiency.
main results summarized Figure 16. displays aggregate results IPC2
IPC3, number instances solved (y-axis) drawn function log time
(x-axis). must note graph IPC2 results favors PathSC formulation
planners. However, see Section 5.3, mainly reflection
exceptional performance Miconic domain rather overall performance
IPC2. Morever, graph IPC3 results include Satellite domain.
decided remove domain, could run public versions
SATPLAN04 SATPLAN06 G2SC PathSC formulations. results
Figure 16 provide rather coarse overview, sum following main findings.
Factored planning using loosely coupled formulations helps improve performance. Note
integer programming formulations use factored representations,
1SC, G1SC, G2SC, PathSC (except G2SC formulation could
3. note SATPLAN04, SATPLAN06, Optiplan, 1SC formulation step-optimal
G1SC, G2SC, PathSC formulations not. is, however, considerable controversy
planning community whether step-optimality guaranteed Graphplan-style planners
connection plan quality metrics users would interested in. refer reader
Kambhampati (2006) longer discussion issue us several prominent researchers
planning community. Given background, believe quite reasonable compare formulations step-optimal approaches, especially since main aim show IP formulations
come long way made competitive respect SAT-based encodings.
turn makes worthwhile consider exploiting features IP formulations,
amenability variety optimization objectives done recent work (van den Briel
et al., 2007).
4. list participating planners results available http://ipc04.icaps-conference.org/

243

fiVan den Briel, Vossen & Kambhampati

tested domains), able solve problem instances given amount
time Optiplan, use factored representation. Especially,
difference 1SC Optiplan remarkable adopt Graphplanstyle parallelism. Section 5.3, however, see Optiplan perform well
domains either serial design significant serial component.
Decreasing encoding size relaxing Graphplan-style parallelism helps improve
performance. surprising, Dimopoulos et al. (1997) already note
reduction number plan periods helps improve planning performance. However, always hold tradeoff reducing number
plan periods versus increased difficulty merging solutions different
components. Section 5.3 see different relaxations Graphplan-style
parallelism lead different results. example, PathSC formulation shows superior performance Miconic Driverlog, poorly Blocksworld, Freecell,
Zenotravel. Likewise, G2SC formulation well Freecell,
seem excel domain.

200

90

180

80

160

Solved intstances (IPC3)

Solved intstances (IPC2)

Planning integer programming shows new potential. conventional wisdom
planning community planning integer programming cannot compete planning satisfiability constraint satisfaction. Figure 16, however,
see 1SC, G1SC PathSC formulation compete quite well
SATPLAN04. SATPLAN04 state-of-the-art planning satisfiability anymore, show planning integer programming come long
way. fact IP competitive allows us exploit virtues
optimization (Do et al., 2007; Benton et al., 2007; van den Briel et al., 2007).

140
120
100
80
60
40
Satplanner
SAT04
G1SC
PathSC

20

SAT06
1SC
G2SC
Optiplan

10

100

60
50
40
30
20
10
0

0
1

70

1000

1

10

100

1000

Solution time (sec)

Solution time (sec)

Figure 16: Aggregate results second third international planning competitions.

5.3 Comparing Loosely Coupled Formulations Planning
section compare IP formulations try evaluate benefits allowing
flexible network representations. Specifically, interested effects reducing number plan periods required solve planning problem versus dealing
244

fiLoosely Coupled Formulations Automated Planning

merging solutions different components. Reducing number plan periods
lead smaller encodings, lead improved performance. However, also makes
merging loosely coupled components harder, could worsen performance.
order compare formulations analyze following two things. First,
examine performance formulations comparing solution times problem
instances IPC2 IPC3. comparison include results Optiplan
gives us idea differences formulation based Graphplan
formulations based loosely coupled components. Moreover, also show us
improvements IP based approaches planning. Second, examine number
plan periods formulation needs solve problem instance. Also, look
tradeoffs reducing number plan periods increased difficulty
merging solutions loosely coupled components. comparison
include results Satplanner because, like formulations, adopts generalized
notion Graphplan-style parallelism.
use following figures table. Figure 17 shows total solution time (y-axis)
needed solve problem instances (x-axis), Figure 18 shows number plan periods
(y-axis) solve problem instances (x-axis), Table 1 shows number ordering
constraints added solution process, seen indicator
merging effort. selected problem instances Table 1 represent five largest
instances could solved formulations (in domains, however,
formulations could solve least five problem instances).
label GP steps Figure 18 represents number plan steps SATPLAN06,
state-of-the-art Graphplan-based planner, would use. Satellite domain, however,
use results 1SC formulation unable run public version
SATPLAN06 domain. like point Figure 18 intended
favor one formulation other, simply shows possible generate encodings automated planning use drastically fewer plan periods Graphplan-based
encodings.
5.3.1 Results: Planning Performance
Blocksworld domain Optiplan solves problems formulations. Zenotravel Satellite, Optiplan generally outperformed respect
solution time, Rovers Freecell, Optiplan generally outperformed respect
number problems solved. IP formulations, G1SC provides
overall best performance performance PathSC formulation somewhat
irregular. example, Miconic, Driverlog Rovers PathSC formulation
well, Depots Freecell rather poorly.
Logistics domain formulations generalize Graphplan-style parallelism
(i.e. G1SC, G2SC, PathSC) scale better 1SC formulation Optiplan,
adopt Graphplan-style parallelism. Among G1SC, G2SC, PathSC formulations
clear best performer, larger Logistics problems G1SC formulation
seems slightly better. Logistics domain provides great example tradeoff
flexibility merging. allowing actions executed plan
period, generally shorter plans (in terms number plan periods) needed solve
245

fiVan den Briel, Vossen & Kambhampati

planning problem (see Figure 18), time merging solutions
individual components harder one respect ordering constraints (see
Table 1).
Optiplan versus 1SC. compare 1SC formulation Optiplan, note
Optiplan fares well domains either serial design (Blocksworld)
domains significant serial aspect (Depots). think Optiplans advantage
1SC formulation domains due following two possibilities. First,
intuition serial domains reachability relevance analysis Graphplan
stronger detecting infeasible action choices (due mutex propagation) network
flow restrictions 1SC formulation. Second, appears state variables
domains tightly coupled (i.e. actions effects, thus transitions one
state variable coupled several transitions state variables)
domains, may negatively affect performance 1SC formulation.
1SC versus G1SC. comparing 1SC formulation G1SC formulation
see domains, except Blocksworld Miconic, G1SC formulation
solves least many problems 1SC formulation. results Blocksworld
surprising attributed semantics domain. operator
Blocksworld requires one state change state variable arm (stack putdown
change status arm arm empty, unstack pickup change status
arm holding x x block lifted). Since, 1SC
G1SC formulations allow one state change state variable,
possibility G1SC formulation allow one action executed
plan period. Given this, one may think 1SC G1SC formulations
solve least number problems, case prevail constraints (5)
1SC formulation stronger prevail constraints (6) G1SC formulation.
is, right-hand side (6) subsumes (i.e. allows larger feasible region LP
relaxation) right-hand side (5). Figure 17 see slight advantage
1SC G1SC Blocksworld domain.
results Miconic domain are, hand, intuitive.
would expected G1SC formulation solve least many problems 1SC
formulation, turn case. One thing noticed
domain G1SC formulation required lot time determine plan
given number plan periods.
G1SC versus G2SC PathSC. Table 1 shows five largest problems
domain solved formulations, yet representative whole set
problems. table indicates Graphplan-style parallelism generalized,
ordering constraints needed ensure feasible plan. average, G2SC formulation
includes ordering constraints G1SC formulation, PathSC formulation
turn includes ordering constraints G2SC formulation. performance
formulations shown Figure 17 varies per planning domain. PathSC
formulation well Miconic Driverlog, G2SC formulation well Freecell,
G1SC well Zenotravel. performance differences, believe
ideal amount flexibility generalization Graphplan-style parallelism
different planning domain.
246

fiLoosely Coupled Formulations Automated Planning

10000
Optiplan
1SC
G1SC
G2SC
PathSC

1000
100

Solution time (sec.)

Solution time (sec.)

10000

10
1
0.1

Optiplan
1SC
G1SC
G2SC
PathSC

1000
100
10
1
0.1
0.01

0.01
1

2

3

4

5

6

7

8

1

9 10 11 12 13 14 15 16 17

3

5

7

9

11

13

10000

1000

1000

100
10
Optiplan
1SC
G1SC
G2SC
PathSC

1
0.1
0.01
3

5

7

9

11

13

15

17

19

21

Solution time (sec.)

Solution time (sec.)

10000

1

0.1

23

Optiplan
1SC
G1SC
G2SC
PathSC

1
0.1
0.01
8

Solution time (sec.)

Solution time (sec.)

10

7

Optiplan
1SC
G1SC
G2SC
PathSC

100
10
1
0.1
0.01

9 10 11 12 13 14 15 16 17

1

2

3

4

5

6

Depots
10000

Optiplan
1SC
G1SC
G2SC
PathSC

1000
100
10
1
0.1
0.01

8

9

10 11 12 13 14 15

Optiplan
1SC
G1SC
PathSC

1000
100
10
1
0.1
0.01

1

2

3

4

5

6

7

8

9

10 11 12 13 14 15 16

1

2

3

4

5

6

Zenotravel
10000

10000

1000

1000

100
10
1
Optiplan
1SC
G1SC

0.1
0.01
1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18
Rovers

7

8

9

10

11

Solution time (sec.)

Solution time (sec.)

7

Driverlog

Solution time (sec.)

Solution time (sec.)

10000

27

Miconic

100

6

25

1 10 19 28 37 46 55 64 73 82 91 100 109 118 127 136 145

1000

5

23

Optiplan
1SC
G1SC
G2SC
PathSC

0.01

1000

4

21

1

10000

3

19

10

10000

2

17

100

Freecell (IPC2)

1

15

Logistics

Blocksworld

100
10

Optiplan
1SC
G1SC
G2SC
PathSC

1
0.1
0.01
1

Satellite

2

3

4

5

Freecell (IPC3)

Figure 17: Solution times planning domains second third international
planning competition.

247

fiVan den Briel, Vossen & Kambhampati

60

#Plan periods

40

#Plan periods

GPsteps
Satplanner
G1SC
G2SC
PathSC

50

30
20
10
0
1

3

5

7

18
16
14
12
10
8
6
4
2
0

9 11 13 15 17 19 21 23 25 27 29 31 33 35

GPsteps
Satplanner
G1SC
G2SC
PathSC

1

3

5

7

9

11

13

Blocksworld
14

8
6
4

23

25

27

GPsteps
Satplanner
G1SC
PathSC

20
15
10

0

0
1

3

5

7

9

11

13

15

17

19

21

1

23

10 19 28 37 46 55 64 73 82 91 100 109 118 127 136 145
Miconic

Freecell (IPC2)

GPsteps
Satplanner
G1SC
G2SC
PathSC

25
20
15
10

#Plan periods

30

#Plan periods

21

5

2

5
0

18
16
14
12
10
8
6
4
2
0

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21

GPsteps
Satplanner
G1SC
G2SC
PathSC

1

2

3

4

5

6

7

8

Depots
10

16

GPsteps
Satplanner
G1SC
PathSC

14
#Plan periods

6

9 10 11 12 13 14 15 16 17 18
Driverlog

GPsteps
Satplanner
G1SC
G2SC
PathSC

8
#Plan periods

19

25
#Plan periods

#Plan periods

10

17

30

GPsteps
Satplanner
G1SC
G2SC
PathSC

12

15

Logistics

4
2

12
10
8
6
4
2

0

0
1

2

3

4

5

6

7

8

9

1

10 11 12 13 14 15 16

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20
Rovers

14

GPsteps
Satplanner
G1SC

#Plan periods

12
10
8
6
4
2
0
1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19

#Plan periods

Zenotravel
18
16
14
12
10
8
6
4
2
0

GPsteps
Satplanner
G1SC
G2SC
PathSC

1

2

3

4

5

6

Freecell (IPC3)

Satellite

Figure 18: Number plan periods required formulation solve planning
problems second third international planning competition.

248

fiLoosely Coupled Formulations Automated Planning

Problem
Blocksworld
5-1
5-2
6-0
6-1
8-2
Logistics
10-1
11-0
11-1
12-0
14-0
Freecell
2-1
2-2
2-3
2-4
5-5
Miconic
6-4
7-0
7-2
7-3
9-4

G1SC

G2SC

PathSC

0
0
0
0
0

0
0
0
0
0

16
62
5
5
62

0
0
0
0
0

7
0
0
16
7

11
10
49
9
110

0
0
0
0
0

0
0
0
2
3

0
0
84
0
*

0
0
0
0
0

-

0
0
2
4
5

Problem
Depots
1
2
10
13
17
Driverlog
7
8
9
10
11
Zenotravel
5
6
10
11
12
Rovers
14
15
16
17
18
Satellite
5
6
7
9
11
Freecell
1
2
3
4
5

G1SC

G2SC

PathSC

0
0
0
0
0

0
0
2
0
0

7
2
*
30
*

1
32
58
5
6

16
62
80
5
30

2
108
32
69
11

0
0
0
0
0

4
5
0
0
60

485
6
214
586
6259

0
12
1
1
1

-

13
11
92
4
192

2
6
8
14
0

-

-

0
0
0
*
*

0
0
1
1
0

3
2480
1989
*
*

Table 1: Number ordering constraints, cuts, added dynamically
solution process problems IPC2 (left) IPC3 (right). dash - indicates
IP formulation could tested domain star * indicates
formulation could solve problem instance within 30 minutes.

249

fiVan den Briel, Vossen & Kambhampati

5.3.2 Results: Number Plan Periods
Figure 18, see domains flexible network representation G1SC
formulation slightly general 1-linearization semantics used Satplanner. is, number plan periods required G1SC formulation always
less equal number plan periods used Satplanner. Moreover, flexible
network representation G2SC PathSC formulations general
one used G1SC formulation. One may think network representation
PathSC formulation provide general interpretation action parallelism, since G2SC network representations allows values change back
original value plan period always case.
domains Logistics, Freecell, Miconic, Driverlog, PathSC never required
two plan periods solve problem instances. Miconic domain
easy understand. Miconic elevator needs bring travelers
one floor another. state variables representation domain one state
variable elevator two traveler (one represent whether traveler
boarded elevator one represent whether traveler serviced). Clearly,
one devise plan value state variable visited twice.
elevator simply could visit floors pickup travelers, visit floors
bring destination floor.
5.4 Comparing Different State Variable Representations
interesting question find whether different state variable representations lead
different performance benefits. loosely coupled formulations components
represent multi-valued state variables. However, idea modeling value transitions
flows appropriately defined network applied binary multi-valued state
variable representation. section concentrate efficiency tradeoffs
binary multi-valued state descriptions. generally fewer multi-valued state
variables binary atoms needed describe planning problem, expect
formulations compact use multi-valued state description.
comparison concentrate G1SC formulation showed overall best
performance among formulations. recent work (van den Briel, Kambhampati, &
Vossen, 2007) analyze different state variable representations detail.
Table 2 compares encoding size G1SC formulation set problems
using either binary multi-valued state description. table clearly shows
encoding size becomes significantly smaller (both CPLEX presolve)
multi-valued state description used. encoding size presolve gives idea
impact using compact multi-valued state description, whereas encoding
size presolve shows much preprocessing done removing redundancies
substituting variables.
Figure 19 shows total solution time (y-axis) needed solve problem instances
(x-axis). Since make changes G1SC formulation, performance
differences result using different state descriptions. several domains multivalued state description shows clear advantage binary state description
using G1SC formulation, also domains multi-valued state
250

fiLoosely Coupled Formulations Automated Planning

description provide much advantage. general, however, G1SC
formulation using multi-valued state description leads better performance
using binary state description. tests, encountered one problem
instance (Rovers pfile10) binary state description notably outperformed
multi-valued state description.

6. Related Work
integer programming-based planning systems. Bylander (1997) considers
IP formulation based converting propositional representation given Satplan
(Kautz & Selman, 1992) IP formulation variables take value 1
certain proposition true, 0 otherwise. LP relaxation formulation
used heuristic partial order planning, tends rather time-consuming.
different IP formulation given Vossen et al. (1999). consider IP formulation
original propositional variables replaced state change variables. State
change variables take value 1 certain proposition added, deleted, persisted,
0 otherwise. Vossen et al. show formulation based state change variables
outperforms formulation based converting propositional representation. Van den
Briel Kambhampati (2005) extend work Vossen et al. incorporating
improvements described Dimopoulos (2001). integer programming approaches
planning rely domain-specific knowledge (Bockmayr & Dimopoulos, 1998, 1999)
explore non-classical planning problems (Dimopoulos & Gerevini, 2002; Kautz & Walser,
1999).
formulations model transitions state variable separate flow
problem, individual problems connected action constraints.
Graphplan planner introduced idea viewing planning network flow problem,
decompose domain several loosely coupled components. encodings
described related loosely-coupled modular planning architecture Srivastava, Kambhampati, (2001), well factored planning approaches Amir
Engelhardt (2003), Brafman Domshlak (2006). work Brafman
Domshlak, example, proposes setting separate CSP problem handling factor. individual factor plans combined global CSP. way,
similarities work (with individual state variable flows corresponding
encodings factor plans). Although Brafman Domshlak provide empirical
evaluation factored planning framework, provide analysis
factored planning expected best. would interesting adapt minimal
tree-width based analysis scenario.
branch-and-cut concept introduced Grotschel, Runger, Reinelt (1984)
Padberg Rinaldi (1991), successfully applied solution
many hard large-scale optimization problems (Caprara & Fischetti, 1997). planning,
approaches use dynamic constraint generation search include RealPlan (Srivastava et al., 2001) LPSAT (Wolfman & Weld, 1999).
Relaxed definitions Graphplan-style parallelism investigated several
researchers. Dimopoulos et al. (1997) first point necessary
require two actions independent order execute plan period.
251

fiVan den Briel, Vossen & Kambhampati

Problem
Blocksworld
6-2
7-0
8-0
Logistics
14-1
15-0
15-1
Miconic
6-4
7-0
7-2
Freecell(IPC2)
3-3
3-4
3-5
Depots
7
10
13
Driverlog
8
10
11
Zenotravel
12
13
14
Rovers
16
17
18
Satellite
6
7
11
Freecell(IPC3)
1
2
3

Binary
presolve
presolve
#va
#co
#va
#co

Multi
presolve presolve
#va
#co
#va
#co

7645
10166
11743

12561
16881
19657

5784
7384
9947

9564
12318
16773

5125
6806
7855

7281
9741
11305

3716
4761
6438

5409
6967
9479

16464
16465
14115

16801
16801
14401

7052
7044
4625

7386
7385
4935

10843
10844
9297

11180
11180
9583

2693
2696
1771

3007
3009
2133

2220
2842
2527

3403
4474
3977

1776
2295
1999

2843
3764
3287

1905
2473
2199

3088
4105
3649

428
503
431

1495
1972
1719

128636
129392
128636

399869
401486
399869

27928
28234
27947

79369
79577
79444

25267
23734
23342

62588
61601
61083

7123
6346
6237

15588
15101
14931

21845
30436
36006

36181
50785
59425

11572
13727
14729

23233
27570
29712

17250
24120
27900

15381
21713
25297

4122
4643
4372

5592
6731
6806

3431
4328
8457

3673
4645
9101

2245
2159
5907

2506
2333
6404

2595
3551
6997

2513
3292
6471

1146
1409
3558

1102
1171
3073

9656
13738
40332

15589
21649
70021

4294
7779
17815

7046
12449
32959

2858
4466
9282

5821
8417
24121

1051
1882
3367

2398
4174
10619

8631
25794
20895

8093
23906
20241

5424
19549
12056

5297
18384
12144

7367
22889
18351

6637
20700
17377

4394
16652
10081

4155
15257
9988

4471
5433
16758

4945
5833
21537

3584
4294
13643

3774
4267
16713

4087
5013
15578

4561
5413
20357

2288
2974
7108

2478
2925
10118

7332
28214
39638

19185
76343
105995

2965
16218
19603

7339
43427
50819

1624
4873
7029

3265
11383
16003

624
2604
3394

1339
6416
8055

Table 2: Formulation size binary multi-valued state description problem instances
IPC2 IPC3 number variables (#va), number constraints
(#co), number ordering constraints, cuts, (#cu) added dynamically solution process.

252

fiLoosely Coupled Formulations Automated Planning

10000

binary

1000

Solution time (sec.)

Solution time (sec.)

10000
multi

100
10
1
0.1
0.01

binary

1000

multi

100
10
1
0.1
0.01

1

2

3

4

5

6

7

8

9

10 11 12 13 14 15

1

3

5

7

9

11

13

Blocksworld
10000
binary

Solution time (sec.)

Solution time (sec.)

10000
1000

multi

100
10
1
0.1

17

19

21

23

25

27

binary

1000

multi

100
10
1
0.1
0.01

0.01
1 2 3 4 5

1

6 7 8 9 10 11 12 13 14 15 16 17 18 19 20

3

5

7

9 11 13 15 17 19 21 23 25 27 29 31 33
Miconic

Freecell (IPC2)

10000

10000

1000

1000

Solution time (sec.)

Solution time (sec.)

15

Logistics

100
10
1
binary

0.1

binary
multi

100
10
1
0.1

multi
0.01

0.01
1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17

1

2

3

4

5

Depots
10000

binary
Solution time (sec.)

Solution time (sec.)

10000
1000

multi

100
10
1
0.1
0.01

7

8

9

10

11

binary

1000

multi

100
10
1
0.1
0.01

1

2

3

4

5

6

7

8

9

10 11 12 13 14 15 16

1

2

3

4

Zenotravel
10000

6

7

8

9 10 11 12 13 14 15 16 17 18

10000

binary

1000

5

Rovers

Solution time (sec.)

Solution time (sec.)

6
Driverlog

multi

100
10
1
0.1

1000

binary
multi

100
10
1
0.1
0.01

0.01
1

2

3

4

5

6

7

8

9

10

1

11

2

3

Freecell (IPC3)

Satellite

Figure 19: Comparing binary state descriptions multi-valued state descriptions using
G1SC formulation.

253

fiVan den Briel, Vossen & Kambhampati

work introduce property post-serializability set actions. set
actions said post-serializable (1) union preconditions
consistent, (2) union effects consistent, (3) preconditions-effects
graph acyclic. preconditions-effects graph directed graph contains
node action , arc (a, b) a, b preconditions
inconsistent effects a. certain planning problems Dimopoulos et al. (1997)
show modifications reduce number plan periods improve performance.
Rintanen (1998) provides constraint-based implementation idea shows
improvements hold broad range planning domains.
Cayrol et al. (2001) introduce notion authorized linearizations, implies
order execution two actions. particular, action authorizes action
b implies executed b, preconditions b deleted
effects deleted b. notion authorized linearizations
similar property post-serializability. adopt ideas
network-based representations would compare G1SC network
generalized state change arcs (see Figure 7) allows values prevail after,
before, transition arcs.
recent discussion definitions parallel plans given Rintanen, Heljanko Niemela (2006). work introduces idea -step semantics, says
necessary parallel actions non-interfering long
executed least one order. -step semantics provide general interpretation
parallel plans notion authorized linearizations used LCGP (Cayrol et al.,
2001). current implementation -step semantics Satplanner is, however, somewhat restricted. semantics allow actions conflicting effects, current
implementation Satplanner not.

7. Conclusions
work makes two contributions: (1) improves state art modeling planning
integer programming, (2) develops novel decomposition methods solving bounded
length (in terms number plan periods) planning problems.
presented series IP formulations represent planning problem set
loosely coupled network flow problems, network flow problem corresponds
one state variables planning domain. incorporated different notions
action parallelism order reduce number plan periods needed find plan
improve planning efficiency. IP formulations described paper led
successful use solving partial satisfaction planning problems (Do et al., 2007).
Moreover, initiated new line work integer linear programming
used heuristic state-space search automated planning (Benton et al., 2007; van den
Briel et al., 2007). would interesting see approach context IP
formulations applies formulations based satisfiability constraint satisfaction.

254

fiLoosely Coupled Formulations Automated Planning

Acknowledgments
research supported part NSF grant IIS308139, ONR grant N000140610058,
Lockheed Martin subcontract TT0687680 Arizona State University part
DARPA integrated learning program. thank Derek Long valuable input,
especially thank anonymous reviewers whose attentive comments helpful
suggestions greatly improved paper.

References
Amir, E., & Engelhardt, B. (2003). Factored planning. Proceedings 18th International Joint Conference Artificial Intelligence (IJCAI-2003), pp. 929935.
Backstrom, C., & Nebel, B. (1995). Complexity results SAS+ planning. Computational
Intelligence, 11 (4), 625655.
Benton, J., van den Briel, M., & Kambhampati, S. (2007). hybrid linear programming
relaxed plan heuristic partial satisfaction planning problems. Proceedings
International Conference Automated Planning Scheduling (ICAPS-2007),
pp. 3441.
Bixby, R. E. (2002). Solving real-world linear programs: decade progress.
Operations Research, 50 (1), 315.
Blum, A., & Furst, M. (1995). Fast planning planning graph analysis. Proceedings
14th International Joint Conference Artificial Intelligence (IJCAI-1995), pp.
16361642.
Bockmayr, A., & Dimopoulos, Y. (1998). Mixed integer programming models planning problems. Working notes CP-98 Constraint Problem Reformulation
Workshop.
Bockmayr, A., & Dimopoulos, Y. (1999). Integer programs valid inequalities planning problems. Proceedings European Conference Planning (ECP-1999),
pp. 239251.
Brafman, R., & Domshlak, C. (2006). Factored planning: How, when, not.
Proceedings 21st National Conference Artificial Intelligence (AAAI-2006),
pp. 809814.
Bylander, T. (1997). linear programming heuristic optimal planning. Proceedings
14th National Conference Artificial Intelligence (AAAI-1997), pp. 694699.
Caprara, A., & Fischetti, M. (1997). Annotated Bibliographies Combinatorial Optimization, chap. Branch Cut Algorithms, pp. 4563. John Wiley Sons.
Cayrol, M., Regnier, P., & Vidal, V. (2001). Least commitment graphplan. Artificial
Intelligence, 130 (1), 85118.
255

fiVan den Briel, Vossen & Kambhampati

Desaulniers, G., Desrosiers, J., & Solomon, M. (Eds.). (2005). Column Generation. Springer.
Dimopoulos, Y. (2001). Improved integer programming models heuristic search AI
planning. Proceedings European Conference Planning (ECP-2001), pp.
301313.
Dimopoulos, Y., & Gerevini, A. (2002). Temporal planning mixed integer programming. Proceedings Workshop Planning Temporal Domains (AIPS2002), pp. 28.
Dimopoulos, Y., Nebel, B., & Koehler, J. (1997). Encoding planning problems nonmonotic logic programs. Proceedings 4th European Conference Planning
(ECP-1997), pp. 167181.
Do, M., Benton, J., van den Briel, M., & Kambhampati, S. (2007). Planning goal
utility dependencies. Proceedings 20th International Joint Conference
Artificial Intelligence (IJCAI-2007), pp. 18721878.
Edelkamp, S., & Helmert, M. (1999). Exhibiting knowledge planning problems minimize state encoding length. Proceedings European Conference Planning
(ECP-1999), pp. 135147.
Garey, M., & Johnson, D. (1979). Computers Intractability: Guide Theory
NP-Completeness. Freeman Company, N.Y.
Grotschel, M., Junger, M., & Reinelt, G. (1984). cutting plane algorithm linear
ordering problem. Operations Research, 32, 11951220.
Helmert, M. (2006). Fast Downward planning system. Journal Artifical Intelligence
Research, 26, 191246.
Hooker, J. (1988). quantitative approach logical inference. Decision Support Systems,
4, 4569.
ILOG Inc., Mountain View, CA (2002). ILOG CPLEX 8.0 users manual.
Johnson, E., Nemhauser, G., & Savelsbergh, M. (2000). Progress linear programming
based branch-and-bound algorithms: exposition. INFORMS Journal Computing, 12, 223.
Kambhampati, S., & commentary planning researchers (2006).

suboptimality optimal planning track ipc 2006.
http://raosruminations.blogspot.com/2006/07/on-suboptimality-of-optimal-planning.html.
Karmarkar, N. (1984). new polynomial time algorithm linear programming. Combinatorica, 4 (4), 373395.
Kautz, H. (2004). SATPLAN04: Planning satisfiability. Working Notes Fourth
International Planning Competition (IPC-2004), pp. 4445.
256

fiLoosely Coupled Formulations Automated Planning

Kautz, H., & Selman, B. (1992). Planning satisfiability. Proceedings European
Conference Artificial Intelligence (ECAI-1992).
Kautz, H., & Selman, B. (2006). SATPLAN04: Planning satisfiability. Working Notes
Fifth International Planning Competition (IPC-2006), pp. 4546.
Kautz, H., & Walser, J. (1999). State-space planning integer optimization. Proceedings
16th National Conference Artificial Intelligence (AAAI-1999), pp. 526533.
Martin, K. (1991). Using separation algorithms generate mixed integer model reformulations. Operations Research Letters, 10, 119128.
Padberg, M., & Rinaldi, G. (1991). branch-and-cut algorithm resolution largescale symmetric traveling salesman problems. SIAM Review, 33, 60100.
Rintanen, J. (1998). planning algorithm based directional search. Proceedings
Sixth International Conference Principles Knowledge Representation
Reasoning (KR-1998), pp. 617624.
Rintanen, J., Heljanko, K., & Niemela, I. (2006). Planning satisfiability: parallel plans
algorithms plan search. Artificial Intelligence, 170 (12), 10311080.
Srivastava, B., Kambhampati, S., & Do, M. (2001). Planning project management way:
Efficient planning effective integration causal resource reasoning realplan.
Artificial Intelligence, 131 (1-2), 73134.
van den Briel, M., Benton, J., Kambhampati, S., & Vossen, T. (2007). LP-based heuristic
optimal planning. Proceedings International Conference Principles
Practice Constraint Programming (CP-2007), pp. 651665.
van den Briel, M., & Kambhampati, S. (2005). Optiplan: Unifying IP-based graphbased planning. Journal Artificial Intelligence Research, 24, 623635.
van den Briel, M., Kambhampati, S., & Vossen, T. (2007). Fluent merging: general technique improve reachability heuristics factored planning. Proceedings
Workshop Heuristics Domain-Independent Planning: Progress, Ideas, Limitations,
Challenges (ICAPS-2007).
Vossen, T., Ball, M., Lotem, A., & Nau, D. (1999). use integer programming
models AI planning. Proceedings 18th International Joint Conference
Artificial Intelligence (IJCAI-1999), pp. 304309.
Wolfman, S., & Weld, D. (1999). LPSAT engine application resource planning. Proceedings 18th International Joint Conference Artificial Intelligence (IJCAI-1999), pp. 310317.
Wolsey, L. (1998). Integer Programming. John Wiley Sons.

257

fiJournal Artificial Intelligence Research 31 (2008) 431-472

Submitted 11/2007; published 3/2008

First Order Decision Diagrams Relational MDPs
Chenggang Wang
Saket Joshi
Roni Khardon

cwan@cs.tufts.edu
sjoshi01@cs.tufts.edu
roni@cs.tufts.edu

Department Computer Science, Tufts University
161 College Avenue, Medford, 02155, USA

Abstract
Markov decision processes capture sequential decision making uncertainty,
agent must choose actions optimize long term reward. paper studies efficient reasoning mechanisms Relational Markov Decision Processes (RMDP)
world states internal relational structure naturally described terms
objects relations among them. Two contributions presented. First, paper
develops First Order Decision Diagrams (FODD), new compact representation functions relational structures, together set operators combine FODDs,
novel reduction techniques keep representation small. Second, paper shows
FODDs used develop solutions RMDPs, reasoning performed
abstract level resulting optimal policy independent domain size (number
objects) instantiation. particular, variant value iteration algorithm developed using special operations FODDs, algorithm shown converge
optimal policy.

1. Introduction
Many real-world problems cast sequential decision making uncertainty.
Consider simple example logistics domain agent delivers boxes. agent
take three types actions: load box truck, unload box truck,
drive truck city. However effects actions may perfectly predictable.
example gripper may slippery load actions may succeed, navigation
module may reliable may end wrong location. uncertainty
compounds already complex problem planning course action achieve
goals maximize rewards.
Markov Decision Processes (MDP) become standard model sequential decision making uncertainty (Boutilier, Dean, & Hanks, 1999). models also provide
general framework artificial intelligence (AI) planning, agent achieve
maintain well-defined goal. MDPs model agent interacting world.
agent fully observe state world takes actions change state.
that, agent tries optimize measure long term reward obtain
using actions.
classical representation algorithms MDPs (Puterman, 1994) require enumeration state space. complex situations specify state space
terms set propositional variables called state attributes. state attributes
together determine world state. Consider simple logistics problem
c
2008
AI Access Foundation. rights reserved.

fiWang, Joshi, & Khardon

one box one truck. state attributes truck Paris (TP), box
Paris (BP), box Boston (BB), etc. let state space represented n binary
state attributes total number states would 2n . problems, however,
domain dynamics resulting solutions simple structure described
compactly using state attributes, previous work known propositionally factored approach developed suite algorithms take advantage structure
avoid state enumeration. example, one use dynamic Bayesian networks, decision trees, algebraic decision diagrams concisely represent MDP model.
line work showed substantial speedup propositionally factored domains (Boutilier,
Dearden, & Goldszmidt, 1995; Boutilier, Dean, & Goldszmidt, 2000; Hoey, St-Aubin, Hu,
& Boutilier, 1999).
logistics example presented small. realistic problem
large number objects corresponding relations among them. Consider problem
four trucks, three boxes, goal box Paris,
matter box Paris. propositionally factored approach, need
one propositional variable every possible instantiation relations domain,
e.g., box 1 Paris, box 2 Paris, box 1 truck 1, box 2 truck 1, on,
action space expands way. goal becomes ground disjunction
different instances stating box 1 Paris, box 2 Paris, box 3 Paris, box 4
Paris. Thus get large MDP time lose structure implicit
relations potential benefits structure terms computation.
main motivation behind relational first order MDPs (RMDP). 1 first
order representation MDPs describe domain objects relations among them,
use quantification specifying objectives. logistics example, introduce three predicates capture relations among domain objects, i.e., Bin(Box, City),
in(T ruck, City), On(Box, ruck) obvious meaning. three parameterized actions, i.e., load(Box, ruck), unload(Box, ruck), drive(T ruck, City).
domain dynamics, reward, solutions described compactly abstractly using
relational notation. example, define goal using existential quantification,
i.e., b, Bin(b, P aris). Using goal one identify abstract policy, optimal
every possible instance domain. Intuitively 0 steps go,
agent rewarded box Paris. one step go
box Paris yet, agent take one action help achieve goal.
box (say b1 ) truck (say t1 ) truck Paris, agent execute
action unload(b1 , t1 ), may make Bin(b1 , P aris) true, thus goal achieved.
two steps go, box truck Paris, agent
take unload action twice (to increase probability successful unloading
box), box truck Paris, agent first take action
drive followed unload. preferred plan depend success probability
different actions. goal paper develop efficient solutions problems
using relational approach, performs general reasoning solving problems
propositionalize domain. result complexity algorithms
1. Sanner Boutilier (2005) make distinction first order MDPs utilize full power
first order logic describe problem relational MDPs less expressive. follow
calling language RMDP.

432

fiFirst Order Decision Diagrams Relational MDPs

change number domain objects changes. Also solutions obtained good
domain size (even infinite ones) simultaneously. abstraction
possible within propositional approach.
Several approaches solving RMDPs developed last years. Much
work devoted developing techniques approximate RMDP solutions using
different representation languages algorithms (Guestrin, Koller, Gearhart, & Kanodia,
2003a; Fern, Yoon, & Givan, 2003; Gretton & Thiebaux, 2004; Sanner & Boutilier, 2005,
2006). example, Dzeroski, De Raedt, Driessens (2001) Driessens, Ramon,
Gartner (2006) use reinforcement learning techniques relational representations. Fern,
Yoon, Givan (2006) Gretton Thiebaux (2004) use inductive learning methods
learn value map policy solutions simulations small instances. Sanner
Boutilier (2005, 2006) develop approach approximate value iteration need
propositionalize domain. represent value functions linear combination
first order basis functions obtain weights lifting propositional approximate
linear programming techniques (Schuurmans & Patrascu, 2001; Guestrin, Koller, Par, &
Venktaraman, 2003b) handle first order case.
also work exact solutions symbolic dynamic programming
(SDP) (Boutilier, Reiter, & Price, 2001), relational Bellman algorithm (ReBel) (Kersting, Otterlo, & De Raedt, 2004), first order value iteration (FOVIA) (Gromann,
Holldobler, & Skvortsova, 2002; Hoolldobler, Karabaev, & Skvortsova, 2006).
working implementation SDP hard keep state formulas consistent
manageable size context situation calculus. Compared SDP, ReBel
FOVIA provide practical solutions. use restricted languages represent
RMDPs, reasoning formulas easier perform. paper develop
representation combines strong points approaches.
work inspired successful application Algebraic Decision Diagrams (ADD)
(Bryant, 1986; McMillan, 1993; Bahar, Frohm, Gaona, Hachtel, Macii, Pardo, & Somenzi,
1993) solving propositionally factored MDPs POMDPs (Hoey et al., 1999; St-Aubin,
Hoey, & Boutilier, 2000; Hansen & Feng, 2000; Feng & Hansen, 2002). intuition
behind idea ADD representation allows information sharing, e.g., sharing
value states belong abstract state, algorithms consider
many states together need resort state enumeration. sufficient
regularity model, ADDs compact, allowing problems represented
solved efficiently. provide generalization approach lifting ADDs
handle relational structure adapting MDP algorithms. main difficulty lifting
propositional solution, relational domains transition function specifies
set schemas conditional probabilities. propositional solution uses concrete
conditional probability calculate regression function. possible
schemas. One way around problem first ground domain problem hand
perform reasoning (see example Sanghai, Domingos, & Weld, 2005).
However allow solutions abstracting domains problems. Like
SDP, ReBel, FOVIA, constructions perform general reasoning.
First order decision trees even decision diagrams already considered
literature (Blockeel & De Raedt, 1998; Groote & Tveretina, 2003) several semantics
diagrams possible. Blockeel De Raedt (1998) lift propositional decision
433

fiWang, Joshi, & Khardon

trees handle relational structure context learning relational datasets.
Groote Tveretina (2003) provide notation first order Binary Decision Diagrams
(BDD) capture formulas Skolemized conjunctive normal form provide
theorem proving algorithm based representation. paper investigates
approaches identifies approach Groote Tveretina (2003) better suited
operations value iteration algorithm. Therefore adapt extend
approach handle RMDPs. particular, First Order Decision Diagrams (FODD)
defined modifying first order BDDs capture existential quantification well realvalued functions use aggregation different valuations diagram.
allows us capture MDP value functions using algebraic diagrams natural way.
also provide additional reduction transformations algebraic diagrams help keep
size small, allow use background knowledge reductions. develop
appropriate representations algorithms showing value iteration performed
using FODDs. core algorithm introduce novel diagram-based algorithm
goal regression where, given diagram representing current value function,
node diagram replaced small diagram capturing truth value
action. offers modular efficient form regression accounts potential
effects action simultaneously. show version abstract value iteration
correct hence converges optimal value function policy.
summarize, contributions paper follows. paper identifies
multiple path semantics (extending Groote & Tveretina, 2003) useful representation
RMDPs contrasts single path semantics Blockeel De Raedt (1998).
paper develops FODDs algorithms manipulate general
context RMDPs. paper also develops novel weak reduction operations first order
decision diagrams shows relevance solving relational MDPs. Finally paper
presents version relational value iteration algorithm using FODDs shows
correct thus converges optimal value function policy. relational
value iteration developed specified previous work (Boutilier et al., 2001),
knowledge first detailed proof correctness convergence algorithm.
section briefly summarized research background, motivation, approach. rest paper organized follows. Section 2 provides background
MDPs RMDPs. Section 3 introduces syntax semantics First Order Decision Diagrams (FODD), Section 4 develops reduction operators FODDs. Sections
5 6 present representation RMDPs using FODDs, relational value iteration
algorithm, proof correctness convergence. last two sections conclude
paper discussion results future work.

2. Relational Markov Decision Processes
assume familiarity standard notions MDPs value iteration (see example
Bellman, 1957; Puterman, 1994). following introduce notions.
also introduce relational MDPs discuss previous work solving them.
Markov Decision Processes (MDPs) provide mathematical model sequential optimization problems stochastic actions. MDP characterized state space
S, action space A, state transition function P r(sj |si , a) denoting probability
434

fiFirst Order Decision Diagrams Relational MDPs

transition state sj given state si action a, immediate reward function r(s),
specifying immediate utility state s. solution MDP optimal
policy maximizes expected discounted total reward defined Bellman equation:
V (s) = maxaA [r(s) +

X

P r(s0 |s, a)V (s0 )]

s0

V represents optimal state-value function. value iteration algorithm (VI)
uses Bellman equation iteratively refine estimate value function:
Vn+1 (s) = maxaA [r(s) +

X

P r(s0 |s, a)Vn (s0 )]

(1)

s0

Vn (s) represents current estimate value function Vn+1 (s) next
estimate. initialize process V0 reward function, Vn captures optimal
value function n steps go. discussed algorithm
known converge optimal value function.
Boutilier et al. (2001) used situation calculus formalize first order MDPs
structured form value iteration algorithm. One useful restrictions introduced
work stochastic actions specified randomized choice among deterministic alternatives. example, action unload logistics example succeed
fail. Therefore two alternatives action: unloadS (unload success)
unloadF (unload failure). formulation algorithms support number action
alternatives. randomness domain captured random choice specifying
action alternative (unloadS unloadF ) gets executed agent attempts
action (unload). choice determined state-dependent probability distribution
characterizing dynamics world. way one separate regression
effects action alternatives, deterministic, probabilistic choice
action. considerably simplifies reasoning required since need perform
probabilistic goal regression directly. work RMDPs used assumption, use assumption well. Sanner Boutilier (2007) investigate model
going beyond assumption.
Thus relational MDPs specified set predicates domain, set
probabilistic actions domain, reward function. probabilistic action,
specify deterministic action alternatives effects, probabilistic choice
among alternatives. relational MDP captures family MDPs generated
choosing instantiation state space. Thus logistics example corresponds
possible instantiations 2 boxes 3 boxes on. get concrete
MDP choosing instantiation.2 Yet algorithms attempt solve entire
MDP family simultaneously.
Boutilier et al. (2001) introduce case notation represent probabilities rewards
compactly. expression = case[1 , t1 ; ; n , tn ], logical formula,
equivalent (1 (t = t1 )) (n (t = tn )). words, equals ti
2. One could define single MDP including possible instances time, e.g. include
states 2 boxes, states 3 boxes infinite number boxes. obviously
subsets states form separate MDPs disjoint. thus prefer view RMDP
family MDPs.

435

fiWang, Joshi, & Khardon

true. general, constrained steps VI algorithm require
disjoint partition state space. case, exactly one
true state. denotes abstract state whose member states
value probability reward. example, reward function logistics
domain, discussed illustrated right side Figure 1, captured
case[b, Bin(b, P aris), 10; b, Bin(b, P aris), 0]. also following notation
operations function defined case expressions. operators defined
taking cross product partitions adding multiplying case values.
case[i , ti : n] case[j , vj : j m] = case[i j , ti + vj : n, j m]
case[i , ti : n] case[j , vj : j m] = case[i j , ti vj : n, j m].
iteration VI algorithm, value stochastic action A(~x) parameterized
free variables ~x determined following manner:
QA(~x) (s) = rCase(s) [ j (pCase(nj (~x), s) Regr(nj (~x), vCase(do(nj (~x), s))))] (2)
rCase(s) vCase(s) denote reward value functions case notation, n j (~x)
denotes possible outcomes action A(~x), pCase(nj (~x), s) choice probabilities nj (~x). Note replace sum possible next states s0 standard
value iteration (Equation 1) finite sum action alternatives j (reflected j
Equation 2), since different next states arise different action alternatives.
Regr, capturing goal regression, determines states one must action
order reach particular state action. Figure 1 illustrates regression
b, Bin(b, P aris) reward function R action alternative unloadS(b , ).
b, Bin(b, P aris) true action unloadS(b , ) true box
b truck truck Paris. Notice reward function R partitions
state space two regions abstract states, may include infinite
number complete world states (e.g., infinite number domain objects).
Also notice get another set abstract states regression step.
way first order regression ensures work abstract states never need
propositionalize domain.
regression, get parameterized Q-function accounts possible
instances action. need maximize action parameters Q-function
get maximum value could achieved using instance action.
illustrate step, consider logistics example two boxes b 1 b2 ,
b1 truck t1 , Paris (that is, On(b1 , t1 ) in(t1 , P aris)), b2
Boston (Bin(b2 , Boston)). action schema unload(b , ), instantiate b
b1 t1 respectively, help us achieve goal; instantiate b
b2 t1 respectively, effect. Therefore need perform
maximization action parameters get best instance action. Yet, must
perform maximization generically, without knowledge actual state. SDP,
done several steps. First, add existential quantifiers action parameters (which
leads non disjoint partitions). sort abstract states Q A(~x) value
decreasing order include negated conditions first n abstract states
formula (n + 1)th , ensuring mutual exclusion. Notice step leads complex
436

fiFirst Order Decision Diagrams Relational MDPs

R
b , Bin ( b , Paris )

b , Bin ( b , Paris )

10

b , Bin ( b , Paris )
0

( b *, *)
Tin ( *, Paris )

Figure 1: example illustrating regression action alternative unloadS(b , ).

description resulting state partitions SDP. process performed every
action separately. call step object maximization denote obj-max(Q A(~x) ).
Finally, get next value function maximize Q-functions different
actions. three steps provide one iteration VI algorithm repeats
update convergence.
solutions ReBel (Kersting et al., 2004) FOVIA (Gromann et al., 2002;
Hoolldobler et al., 2006) follow outline use simpler logical language representing RMDPs. abstract state ReBel captured using existentially quantified
conjunction. FOVIA (Gromann et al., 2002; Hoolldobler et al., 2006) complex
representation allowing conjunction must hold state set conjunctions
must violated. important feature ReBel use decision list (Rivest,
1987) style representations value functions policies. decision list gives us
implicit maximization operator since rules higher list evaluated first. result
object maximization step simple ReBel. state partition represented
implicitly negation rules it, explicitly conjunction rule.
hand, regression ReBel requires one enumerate possible matches
subset conjunctive goal (or state partition) action effects, reason
separately. step potentially improved.
following section introduce new representation First Order Decision Diagrams (FODD). FODDs allow sharing parts partitions, leading space time
saving. importantly value iteration algorithm based FODDs simple
regression simple object maximization.
437

fiWang, Joshi, & Khardon

3. First Order Decision Diagrams
decision diagram graphical representation functions propositional (Boolean)
variables. function represented labeled rooted directed acyclic graph
non-leaf node labeled propositional variable exactly two children.
outgoing edges marked values true false. Leaves labeled numerical
values. Given assignment truth values propositional variables, traverse
graph node follow outgoing edge corresponding truth value.
gives mapping assignment leaf diagram turn
value. leaves marked values {0, 1} interpret graph
representing Boolean function propositional variables. Equivalently, graph
seen representing logical expression satisfied 1 leaf
reached. case {0, 1} leaves known Binary Decision Diagrams (BDDs)
case numerical leaves (or general algebraic expressions) known Algebraic
Decision Diagrams (ADDs). Decision Diagrams particularly interesting impose
order propositional variables require node labels respect order
every path diagram; case known Ordered Decision Diagrams (ODD).
case every function unique canonical representation serves normal form
function. property means propositional theorem proving easy ODD
representations. example, formula contradictory fact evident
represent BDD, since normal form contradiction single leaf valued
0. property together efficient manipulation algorithms ODD representations
led successful applications, e.g., VLSI design verification (Bryant, 1992;
McMillan, 1993; Bahar et al., 1993) well MDPs (Hoey et al., 1999; St-Aubin et al.,
2000). following generalize representation relational problems.
3.1 Syntax First Order Decision Diagrams
various ways generalize ADDs capture relational structure. One could
use closed open formulas nodes, latter case must interpret
quantification variables. process developing ideas paper
considered several possibilities including explicit quantifiers lead
useful solutions. therefore focus following syntactic definition
explicit quantifiers.
representation, assume fixed set predicates constant symbols,
enumerable set variables. also allow using equality pair terms
(constants variables).
Definition 1 First Order Decision Diagram
1. First Order Decision Diagram (FODD) labeled rooted directed acyclic graph,
non-leaf node exactly two children. outgoing edges marked
values true false.
2. non-leaf node labeled with: atom P (t1 , . . . , tn ) equality t1 = t2
ti variable constant.
3. Leaves labeled numerical values.
438

fiFirst Order Decision Diagrams Relational MDPs

p (x)
q (x)
1

h (y)

0 1

0

Figure 2: simple FODD.

Figure 2 shows FODD binary leaves. Left going edges represent true branches.
simplify diagrams paper draw multiple copies leaves 0 1 (and
occasionally values small sub-diagrams) represent node
FODD.
use following notation: node n, nt denotes true branch n, nf
false branch n; na outgoing edge n, true false.
edge e, source(e) node edge e issues from, target(e) node edge e
points to. Let e1 e2 two edges, e1 = sibling(e2 ) iff source(e1 ) = source(e2 ).
following slightly abuse notation let na mean either edge
sub-FODD edge points to. also use na target(e1 ) interchangeably
n = source(e1 ) true false depending whether e1 lies
true false branch n.
3.2 Semantics First Order Decision Diagrams
use FODD represent function assigns values states relational MDP.
example, logistics domain, might want assign values different states
way box Paris, state assigned value 19;
box Paris box truck Paris raining, state
assigned value 6.3, on.3 question define semantics FODDs
order intended meaning.
semantics first order formulas given relative interpretations. interpretation domain elements, mapping constants domain elements and,
predicate, relation domain elements specifies predicate
true. MDP context, state captured interpretation. example
logistics domain, state includes objects boxes, trucks, cities, relations
among them, box 1 truck 1 (On(b1 , t1 )), box 2 Paris (Bin(b2 , P aris))
on. one way define meaning FODD B interpretation I.
following discuss two possibilities.
3.2.1 Semantics Based Single Path
semantics relational decision trees given Blockeel De Raedt (1998)
adapted FODDs. semantics define unique path followed traversing
3. result regression logistics domain cf. Figure 19(l).

439

fiWang, Joshi, & Khardon

B relative I. variables existential node evaluated relative path
leading it.
particular, reach node variables seen
path new. Consider node n label l(n) path leading
root, let C conjunction labels nodes exited true
branch path. node n evaluate ~x, C l(n), ~x includes
variables C l(n). formula satisfied follow true branch.
Otherwise follow false branch. process defines unique path root
leaf value.
example, evaluate diagram Figure 2 interpretation 1
domain {1, 2, 3} true atoms {p(1), q(2), h(3)} follow
true branch root since x, p(x) satisfied, follow false branch q(x)
since x, p(x) q(x) satisfied. Since leaf labeled 0 say B
satisfy I. attractive approach, partitions set interpretations
mutually exclusive sets used create abstract state partitions MDP
context. However, reasons discuss later, semantics leads various complications
value iteration algorithm, therefore used paper.
3.2.2 Semantics Based Multiple Paths
second alternative builds work Groote Tveretina (2003) defined semantics based multiple paths. Following work, define semantics first relative
variable valuation . Given FODD B variables ~x interpretation I, valuation
maps variable ~x domain element I. done, node predicate
evaluates either true false traverse single path leaf. value
leaf denoted MAPB (I, ).
Different valuations may give different values; recall use FODDs represent
function states, state must assigned single value. Therefore, next
define
MAPB (I) = aggregate {MAPB (I, )}
aggregation function. is, consider possible valuations ,
valuation calculate MAPB (I, ). aggregate values. special
case Groote Tveretina (2003) leaf labels {0, 1} variables universally
quantified; easily captured formulation using minimum aggregation
function. paper use maximum aggregation function. corresponds
existential quantification binary case (if valuation leading value 1,
value assigned 1) gives useful maximization value functions
general case. therefore define:
MAPB (I) = max{MAPB (I, )}.


Using definition B assigns every unique value v = MAPB (I) B defines function
interpretations real values. later refer function map B.
Consider evaluating diagram Figure 2 interpretation I1 given
true atoms {p(1), q(2), h(3)}. valuation x mapped 2
440

fiFirst Order Decision Diagrams Relational MDPs

mapped 3 denoted {x/2, y/3} leads leaf value 1 maximum 1. leaf
labels {0,1}, interpret diagram logical formula. MAP B (I) = 1,
example, say satisfies B MAPB (I) = 0 say falsifies
B.
define node formulas (NF) edge formulas (EF) recursively follows. node
n labeled l(n) incoming edges e1 , . . . , ek , node formula NF(n) = (i EF(ei )).
edge formula true outgoing edge n EF(nt ) = NF(n) l(n). edge formula
false outgoing edge n EF(nf ) = NF(n) l(n). formulas,
variables existentially quantified, capture conditions node edge
reached.
3.3 Basic Reduction FODDs
Groote Tveretina (2003) define several operators reduce diagram normal
form. total order node labels assumed. describe operators briefly
give main properties.
(R1) Neglect operator: children node p FODD lead node q
remove p link parents p q directly.
(R2) Join operator: two nodes p, q label point two
children join p q (remove q link qs parents p).
(R3) Merge operator: node child label parent point
directly grandchild.
(R4) Sort operator: node p parent q label ordering violated (l(p) >
l(q)) reorder nodes locally using two copies p q labels
nodes violate ordering.
Define FODD reduced none four operators applied.
following:
Theorem 1 (Groote & Tveretina, 2003)
(1) Let {Neglect, Join, Merge, Sort} operator O(B) result applying
FODD B, B, I, , MAPB (I, ) = MAPO(B) (I, ).
(2) B1 , B2 reduced satisfy , MAPB1 (I, ) = MAPB2 (I, ) identical.
Property (1) gives soundness, property (2) shows reducing FODD gives normal
form. However, holds maps identical every condition
stronger normal equivalence. normal form suffices Groote Tveretina
(2003) use provide theorem prover first order logic, strong
enough purposes. Figure 3 shows two pairs reduced FODDs (with respect R1R4) MAPB1 (I) = MAPB2 (I) , MAPB1 (I, ) 6= MAPB2 (I, ). case
although maps FODDs reduced form. Consider
first pair part (a) figure. interpretation p(a) false p(b)
true substitution {x/a, y/b} leads value 0 B1 B2 always evaluates
1. diagrams equivalent. interpretation, p(c) true object
441

fiWang, Joshi, & Khardon

B1

B2

p (x)

(a)

1

1

p (y)
0

1

p (x, y)
(b)

p (y, z)
1

p (x, y)

0

p (z, x)

0

1

0

0

Figure 3: Examples illustrating weakness normal form.

c MAPB1 (I) = 1 substitution {x/c}; p(c) false object c
MAPB1 (I) = 1 substitution {x/c, y/c}. Thus map always 1
B1 well. Section 4.2 show additional reduction operators
developed, B1 first pair reduced 1. Thus diagrams (a) form
reduction. However, reductions resolve second pair given part (b)
figure. Notice functions capture path two edges labeled p graph
(we change order two nodes rename variables) diagrams evaluate
1 interpretation path. Even though B1 B2 logically
equivalent, cannot reduced form using R1-R4 new operators.
identify unique minimal syntactic form one may consider possible renamings
variables sorted diagrams produce, expensive operation.
discussion normal form conjunctions uses operation given Garriga,
Khardon, De Raedt (2007).
3.4 Combining FODDs
Given two algebraic diagrams may need add corresponding functions, take
maximum use binary operation, op, values represented functions. adopt solution propositional case (Bryant, 1986) form
procedure Apply(B1 ,B2 ,op) B1 B2 algebraic diagrams. Let p q
roots B1 B2 respectively. procedure chooses new root label (the lower
among labels p, q) recursively combines corresponding sub-diagrams, according
relation two labels (, =, ). order make sure result
reduced propositional sense one use dynamic programming avoid generating
nodes either neglect join operators ((R1) (R2) above) would applicable.
Figure 4 illustrates process. example, assume predicate ordering
p1 p2 , parameter ordering x1 x2 . Non-leaf nodes annotated numbers
numerical leaves underlined identification execution trace. example,
442

fiFirst Order Decision Diagrams Relational MDPs

1
p1 (x1)
2
p2 (x1)
10



3
p2 (x2)
9

0

0

1+3
p1 (x1)
=

2+3
p2 (x1)

0+3

10+3
p2 (x2)
19

10

p2 (x2)
9

0

Figure 4: simple example adding two FODDs.

top level call adds functions corresponding nodes 1 3. Since p1 (x1 )
smaller label picked label root result. must add
left right child node 1 node 3. calls performed recursively. easy
see size result may product sizes input diagrams. However,
much pruning occur shared variables pruning made possible weak
reductions presented later.
Since interpretation fixed valuation FODD propositional,
following lemma. later refer property correctness Apply.
Lemma 1 Let C = Apply(A, B, op), , MAPA (I, ) op MAPB (I, ) =
MAPC (I, ).
Proof: First introduce terminology. Let #nodes(X) refer set nodes
FODD X. Let root nodes B Aroot Broot respectively. Let
FODDs rooted Aroott , Arootf , Broott , Brootf , Croott , Crootf Al , Ar , B l , B r ,
C l C r respectively.
proof induction n = |#nodes(A)| + |#nodes(B)|. lemma true
n = 2, case Aroot Broot single leaves operation
operation two real numbers. inductive step need
consider two cases.
Case 1: Aroot = Broot . Since root nodes equal, valuation reaches Al ,
also reach B l reaches Ar , also reach B r . Also,
definition Apply, case C l = Apply(Al , B l , op) C r = Apply(Ar , B r , op). Therefore statement lemma true MAPAl (I, ) op MAPB l (I, ) = MAPC l (I, )
MAPAr (I, ) op MAPB r (I, ) = MAPC r (I, ) I. Now, since |#nodes(Al ) +
#nodes(B l )| < n |#nodes(Ar ) + #nodes(B r )| < n, guaranteed induction
hypothesis.
Case 2: Aroot 6= Broot . Without loss generality let us assume Aroot Broot .
definition Apply, C l = Apply(Al , B, op) C r = Apply(Ar , B, op). Therefore
statement lemma true MAPAl (I, ) op MAPB (I, ) = MAPC l (I, )
MAPAr (I, ) op MAPB (I, ) = MAPC r (I, ) I. guaranteed
induction hypothesis.
2
443

fiWang, Joshi, & Khardon

3.5 Order Labels
syntax FODDs allows two types objects: constants variables.
argument predicate constant variable. assume complete ordering
predicates, constants, variables. ordering two labels given
following rules.
1. P (x1 , ..., xn ) P 0 (x01 , ..., x0m ) P P 0
2. P (x1 , ..., xn ) P (x01 , ..., x0n ) exists xj = x0j j < i,
type(xi ) type(x0i ) (where type constant variable) type(xi ) = type(x0i )
xi x0i .
predicate order set arbitrarily appears useful assign equality
predicate first predicate ordering equalities top
diagrams. reductions often encounter situations one side equality
completely removed leading substantial space savings. may also useful
order argument types constant variables. ordering may helpful
reductions. Intuitively, variable appearing lower diagram bound
value constant appears it. heuristic guidelines best
ordering may well problem dependent. later introduce forms arguments:
predicate parameters action parameters. ordering discussed Section 6.

4. Additional Reduction Operators
context, especially algebraic FODDs, may want reduce diagrams further.
distinguish strong reductions preserve MAPB (I, ) weak reductions
preserve MAPB (I). Theorem 1 shows R1-R4 given strong reductions. details relational VI algorithm directly depend reductions
used. Readers interested RMDP details skip Section 5 read
independently (except reductions illustrated examples).
reduction operators incorporate existing knowledge relationships
predicates domain. denote background knowledge B. example
Blocks World may know block block clear:
x, y, [on(x, y) clear(y)].
following define conditions reduction operators, two types
conditions: reachability condition value condition. name reachability
conditions starting P (for Path Condition) reduction operator number.
name conditions values starting V reduction operator number.
4.1 (R5) Strong Reduction Implied Branches
Consider node n whenever n reached true branch followed.
case remove n connect parents directly true branch. first
present condition, followed lemma regarding operator.
(P5) : B |= ~x, [NF(n) l(n)] ~x variables EF(nt ).
444

fiFirst Order Decision Diagrams Relational MDPs

Let R5(n) denote operator removes node n connects parents directly
true branch. Notice generalization R3. easy see
following lemma true:
Lemma 2 Let B FODD, n node condition P5 holds, B 0 result
R5(n). interpretation valuation MAP B (I, ) =
MAPB 0 (I, ).
similar reduction formulated false branch, i.e., B |= ~x, [NF(n)
l(n)] whenever node n reached false branch followed. case
remove n connect parents directly false branch.
Implied branches may simply result equalities along path. example (x =
y) p(x) p(y) may prune p(y) (x = y) p(x) known true. Implied
branches may also result background knowledge. example Blocks World
on(x, y) guaranteed true reach node labeled clear(y)
remove clear(y) connect parent clear(y)f .
4.2 (R7) Weak Reduction Removing Dominated Edges
Consider two edges e1 e2 FODD whose formulas satisfy follow
e2 using valuation also follow e1 using possibly different valuation.
e1 gives better value e2 intuitively e2 never determines value diagram
therefore redundant. formalize reduction operator R7. 4
Let p = source(e1 ), q = source(e2 ), e1 = pa , e2 = qb , b true
false. first present conditions operator follow
definition operator.
(P7.1) : B |= [~x, EF(e2 )] [~y , EF(e1 )] ~x variables EF(e2 ) ~y
variables EF(e1 ).
(P7.2) : B |= ~u, [[w,
~ EF(e2 )] [~v , EF(e1 )]] ~u variables appear
target(e1 ) target(e2 ), ~v variables appear EF(e1 ) ~u,
w
~ variables appear EF(e2 ) ~u. condition requires
every valuation 1 reaches e2 valuation 2 reaches e1 1
2 agree variables appear target(e1 ) target(e2 ).
(P7.3) : B |= ~r, [[~s, EF(e2 )] [~t, EF(e1 )]] ~r variables appear
target(e1 ) target(sibling(e2 )), ~t variables appear EF(e1 ) ~r,
~s variables appear EF(e2 ) ~r. condition requires
every valuation 1 reaches e2 valuation 2 reaches e1 1
2 agree variables appear target(e1 ) target(sibling(e2 )).
(V7.1) : min(target(e1 )) max(target(e2 )) min(target(e1 )) minimum leaf
value target(e1 ), max(target(e2 )) maximum leaf value target(e2 ). case
regardless valuation know better follow e1 e2 .
(V7.2) : min(target(e1 )) max(target(sibling(e2 ))).
(V7.3) : leaves = target(e1 ) target(e2 ) non-negative values, denoted
0. case fixed valuation better follow e1 instead e2 .
4. use R7 skip notation R6 consistency earlier versions paper. See
discussion Section 4.2.1.

445

fiWang, Joshi, & Khardon

(V7.4) : leaves G = target(e1 ) target(sibling(e2 )) non-negative values.
define operators R7-replace(b, e1 , e2 ) replacing target(e2 ) constant b
0 min(target(e1 )) (we may write R7-replace(e1 , e2 ) b = 0),
R7-drop(e1 , e2 ) dropping node q = source(e2 ) connecting parents
target(sibling(e2 )).
need one safety condition guarantee reduction correct:
(S1) : NF(source(e1 )) sub-FODD target(e1 ) remain
R7-replace R7-drop. condition says must harm value promised
target(e1 ). words, must guarantee p = source(e1 ) reachable
sub-FODD target(e1 ) modified replacing branch 0.
condition violated q sub-FODD pa , p sub-FODD qb .
holds cases, p q unrelated (one descendant
other), q sub-FODD pa , p sub-FODD qb , a, b
negations a, b.
Lemma 3 Let B FODD, e1 e2 edges conditions P7.1, V7.1, S1
hold, B 0 result R7-replace(b, e1 , e2 ), 0 b min(target(e1 )),
interpretation MAPB (I) = MAPB 0 (I).
Proof: Consider valuation 1 reaches target(e2 ). according P7.1,
another valuation reaching target(e1 ) V7.1 gives higher value. Therefore, MAPB (I) never determined target(e2 ) replace target(e2 )
constant 0 min(target(e1 )) without changing map.
2
Lemma 4 Let B FODD, e1 e2 edges conditions P7.2, V7.3, S1
hold, B 0 result R7-replace(b, e1 , e2 ), 0 b min(target(e1 )),
interpretation MAPB (I) = MAPB 0 (I).
Proof: Consider valuation 1 reaches target(e2 ). P7.2 another
valuation 2 reaching target(e1 ) 1 2 agree variables appear
target(e1 ) target(e2 ). Therefore, V7.3 achieves higher value (otherwise,
must branch = target(e1 ) target(e2 ) negative value). Therefore according
maximum aggregation value MAPB (I) never determined target(e2 ),
replace constant described above.
2
Note conditions previous two lemmas comparable since P7.2
P7.1 V7.1 V7.3. Intuitively relax conditions values, need
strengthen conditions reachability. subtraction operation = target(e 1 )
target(e2 ) propositional, test V7.3 implicitly assumes common variables operands P7.1 check this. Figure 5 illustrates
reachability condition P7.1 together V7.3, i.e., combining weaker portions conditions Lemma 3 Lemma 4, cannot guarantee replace
branch constant. Consider interpretation domain {1, 2, 3, 4} relations {h(1, 2), q(3, 4), p(2)}. addition assume domain knowledge B = [x, y, h(x, y)
z, w, q(z, w)]. P7.1 V7.3 hold e1 = [q(x, y)]t e2 = [h(z, y)t ].
MAPB1 (I) = 3 MAPB2 (I) = 0. therefore possible replace h(z, y)t 0.
446

fiFirst Order Decision Diagrams Relational MDPs

q(x,y)
p(y)

q(x,y)

h(z,y)
0 p(y) 0

5

3

3
B1

0

p(y)
5

0

0
B2

Figure 5: example illustrating subtraction condition R7.

10

B1

B2

p(x)

p(x)

q(y)
7

10

p(y)

20

h(y)

9 20

h(y)
0

0

Figure 6: example illustrating condition removing node R7.

Sometimes drop node q completely R7-drop. Intuitively,
remove node, must guarantee gain extra value. conditions
R7-replace guarantee lose value. remove node
q, valuation supposed reach e2 may reach better value e2 sibling.
would change map, illustrated Figure 6. Notice conditions P7.1
V7.1 hold e1 = [p(x)]t e2 = [p(y)]t replace [p(y)]t constant.
Consider interpretation domain {1, 2} relations {q(1), p(2), h(2)}.
MAPB1 (I) = 10 via valuation {x/2} MAPB2 (I) = 20 via valuation {x/1, y/2}. Thus
removing p(y) correct.
Therefore need additional condition guarantee gain extra value
node dropping. condition stated as: valuation 1 reaches e2
thus redirected reach value v1 sibling(e2 ) q removed,
valuation 2 reaches leaf value v2 v1 . However, condition complex
test practice. following identify two stronger conditions.
Lemma 5 Let B FODD, e1 e2 edges condition V7.2 hold addition
conditions replacing target(e2 ) constant, B 0 result R7-drop(e1 , e2 ),
interpretation MAPB (I) = MAPB 0 (I).
Proof: Consider valuation reaching target(e2 ). true value dominated
another valuation reaching target(e1 ). remove q = source(e2 ) valuation
reach target(sibling(e2 )) V7.2 value produced smaller value
target(e1 ). map preserved.
2
447

fiWang, Joshi, & Khardon

Lemma 6 Let B FODD, e1 e2 edges P7.3 V7.4 hold addition
conditions replacing target(e2 ) constant, B 0 result R7-drop(e1 , e2 ),
interpretation MAPB (I) = MAPB 0 (I).
Proof: Consider valuation 1 reaching target(e2 ). value dominated
another valuation reaching target(e1 ). remove q = source(e2 ) valuation
reach target(sibling(e2 )) conditions P7.3 V7.4, valuation 2
reach leaf greater value target(e1 )(otherwise branch G leading
negative value). maximum aggregation map changed.
2
summarize P7.1 V7.1 S1 hold P7.2 V7.3 S1 hold
replace target(e2 ) constant. replace V7.2 P7.3 V7.4 hold
drop q = source(e2 ) completely.
following provide detailed analysis applicability variants R7.
4.2.1 R6: Special Case R7
special case R7 p = q, i.e., e1 e2 siblings. context R7
considered focus single node n instead two edges. Assuming e 1 = nt
e2 = nf , rewrite conditions R7 follows.
(P7.1) : B |= [~x, NF(n)] [~x, ~y , EF(nt )]. condition requires n reachable
nt reachable.
(P7.2) : B |= ~r, [~v , NF(n)] [~v , w,
~ EF(nt )] ~r variables appear
nt nf , ~v variables appear NF(n) ~r, w
~ variables
l(n) ~r ~v .
(P7.3) : B |= ~u, [~v , NF(n)] [~v , w,
~ EF(nt )] ~u variables appear
nt (since sibling(e2 ) = e1 ), ~v variables appear NF(n) ~u, w
~
variables l(n) ~u ~v .
(V7.1) : min(nt ) max(nf ).
(V7.2) : nt constant.
(V7.3) : leaves diagram = nt nf non-negative values.
Conditions S1 V7.4 always true. previously analyzed special case
separate reduction operator named R6 (Wang, Joshi, & Khardon, 2007).
special case, may still useful check separately applying generalized
case R7, provides large reductions seems occur frequently example domains.
important special case R6 occurs l(n) equality t1 =
variable occur FODD node n. case, condition P7.1
holds since choose value y. also enforce equality subdiagram nt . Therefore V7.1 holds remove node n connecting parents
nt substituting t1 diagram nt . (Note may need make copies
nodes this.) Section 4.4 introduce elaborate reduction handle
equalities taking maximum left right children.
4.2.2 Application Order
cases several instances R7 applicable. turns order
apply important. following, first example shows order affects
448

fiFirst Order Decision Diagrams Relational MDPs

p(x1,y1)
q(x3)

p(x1,y1)
q(x3)
10

p(x2,y2)

q(x2)
6

0 5
(a)

p(x1,y1)

10

0

q(x2)
6

q(x2) 0

q(x3)
10

0

(b)

0

q(x3)

p(x2,y2)
0 q(x2)
5
(d)

(c)

p(x1,y1)

p(x1,y1)
10

0

0

0

q(x3)
10

p(x2,y2)
0

0

0

(e)

Figure 7: example illustrating effect application order R7.

number steps needed reduce diagram. second example shows
order affects final result.
Consider FODD Figure 7(a). R7 applicable edges e1 = [p(x1 , y1 )]t
e2 = [p(x2 , y2 )]t , e01 = [q(x3 )]t e02 = [q(x2 )]t . reduce top
manner, i.e., first apply R7 pair [p(x1 , y1 )]t [p(x2 , y2 )]t , get FODD
Figure 7(b), apply R7 [q(x3 )]t [q(x2 )]t , get
FODD Figure 7(c). However, apply R7 first [q(x3 )]t [q(x2 )]t thus getting
Figure 7(d), R7 cannot applied [p(x1 , y1 )]t [p(x2 , y2 )]t [p(x1 , y1 )]t
[p(x2 , y2 )]t negative leaves. case, diagram still reduced.
reduce comparing [q(x3 )]t [q(x2 )]t right part FODD. first
remove q(x2 ) get FODD shown Figure 7(e), use neglect operator
remove p(x2 , y2 ). see example applying one instance R7 may render
instances applicable may introduce possibilities reductions general
must apply reductions sequentially. Wang (2007) develops conditions
several instances R7 applied simultaneously.
One might hope repeated application R7 lead unique reduced result
true. fact, final result depends choice operators order
application. Consider Figure 8(a). R7 applicable edges e1 = [p(x)]t e2 = [p(y)]t ,
e01 = [q(x)]t e02 = [q(y)]t . reduce top manner, i.e., first apply
R7 pair [p(x)]t [p(y)]t , get FODD Figure 8(b), cannot
reduced using existing reduction operators (including operator R8 introduced below).
However, apply R7 first [q(x)]t [q(y)]t get Figure 8(c).
apply R7 e1 = [p(x)]t e2 = [p(y)]t get final result Figure 8(d),
clearly compact Figure 8(b). interesting first example seems
449

fiWang, Joshi, & Khardon

p(x)
10

p(x)
10

p(y)

10 q(x)
10

10 q(y)
1

q(y)
1

0

0

(a)

(b)

p(x)
10

q(x)

p(x)
10 q(x)

p(y)

10 0

10 q(x)
10 0

(d)

(c)

Figure 8: example illustrating final result R7 reductions order dependent.
suggest applying R7 top manner (since takes fewer steps), second
seems suggest opposite (since final result compact). research
needed develop useful heuristics guide choice reductions application
order general develop complete set reductions.
Note could also consider generalizing R7. Figure 8(b), reach [q(y)]
clearly reach [p(x)]t [q(x)]t . Since [p(x)]t [q(x)]t give better values, safely replace [q(y)]t 0, thus obtaining final result Figure 8(d). theory generalize P7.1 B |= [~x, EF(e2 )] [y~1 , EF(e11 )] [y~n , EF(e1n )]
~x variables EF(e2 ) y~i variables EF(e1i ) 1 n, generalize
corresponding value condition V7.1 [1, n], min(target(e1i )) max(target(e2 )).
generalize reachability value conditions similarly. However resulting
conditions expensive test practice.
4.2.3 Relaxation Reachability Conditions
conditions P7.2 P7.3 sufficient, necessary guarantee correct reductions. Sometimes valuations need agree smaller set variables
intersection variables. see this, consider example shown Figure 9,
B > 0 intersection {x, y, z}. However, guarantee B > 0 need
agree either {x, y} {x, z}. Intuitively agree variable x avoid
situation two paths p(x, y) q(x) p(x, y) q(x) h(z) co-exist. order
prevent co-existence two paths p(x, y) h(z) p(x, y) q(x) h(z), either
z well. change example little bit replace
450

fiFirst Order Decision Diagrams Relational MDPs

h(z) h(z, v), two minimal sets variables different size, one {x, y},
{x, z, v}. result cannot identify minimum set variables
subtraction must either choose intersection heuristically identify minimal set,
example, using greedy procedure.



B

p(x, y)

p(x, y)

q(x)
3

h(z)
2 3

q(x)
2

1 2

h(z)
3

h(z)
1

1

Figure 9: example illustrating minimal set variables subtraction
unique.
4.3 (R8) Weak Reduction Unification
Consider FODD B. Let ~v denote variables, let ~x ~y disjoint subsets ~v ,
cardinality. define operator R8(B, ~x, ~y ) replacing variables
~x corresponding variables ~y . denote resulting FODD B{~x/~y }
result variables ~v \~x. following condition correctness R8:
(V8) : leaves B{~x/~y } B non negative.
Lemma 7 Let B FODD, B 0 result R8(B, ~x, ~y ) V8 holds,
interpretation MAPB (I) = MAPB 0 (I).
Proof: Consider valuation 1 ~v B. V8, B{~x/~y } gives better value
valuation. Therefore lose value operator. also gain
extra value. Consider valuation 2 variables B 0 reaching leaf node value
v, construct valuation 3 ~v B variables ~x taking corresponding
value ~y , reach leaf node B value. Therefore map
changed unification.
2
Figure 10 illustrates cases R8 applicable R7 not. apply
R8 {x1 /x2 } get FODD shown Figure 10(b). Since (b) (a) 0, (b) becomes
result reduction. Note unify way, i.e.,{x2 /x1 }, get
Figure 10(c), isomorphic Figure 10(b), cannot reduce original FODD
result, (c) (a) 6 0. phenomenon happens since subtraction operation
(implemented Apply) used reductions propositional therefore sensitive
variable names.
4.4 (R9) Equality Reduction
Consider FODD B equality node n labeled = x. Sometimes drop n
connect parents sub-FODD result taking maximum left
451

fiWang, Joshi, & Khardon

p(x2)
p(x1)
p(x2)

0

x1 / x2

q(x2)
10

q(x2) 0
10
(a)

0

0

0
(b)

x2/ x1

p(x1)
q(x1)
10

0

0
(c)

Figure 10: example illustrating R8.

right children n. reduction applicable B satisfy following
condition.
(E9.1) : equality node n labeled = x least one x variable
appears neither nf node formula n. simplify description
reduction procedure below, assume x variable.
Additionally make following assumption domain.
(D9.1) : domain contains one object.
assumption guarantees valuations reaching right child equality
nodes exist. fact needed proving correctness Equality reduction operator.
First describe reduction procedure R9(n). Let Bn denote FODD rooted
node n FODD B. extract copy Bnt (and name Bnt -copy), copy
Bnf (Bnf -copy) B. Bnt -copy, rename variable x produce diagram
Bn0 -copy. Let Bn0 = Apply(Bn0 -copy, Bnf -copy, max). Finally drop node n B
connect parents root Bn0 obtain final result B 0 . example shown
Figure 11.
Informally, extracting parts FODD rooted node n, one x =
(and renaming x part) one x 6= t. condition E9.1
assumption D9.1 guarantee regardless value t, valuations reaching
parts. Since definition MAP, maximize valuations, case
maximize diagram structure itself. calculating function
maximum two functions corresponding two children n (using
Apply) replacing old sub-diagram rooted node n new combined diagram.
Theorem 9 proves affect map B.
One concern implementation simply replace old sub-diagram
new sub-diagram, may result diagram strong reductions applicable.
problem semantically, avoid need strong reductions
using Apply implicitly performs strong reductions R1(neglect) R2(join) follows.
452

fiFirst Order Decision Diagrams Relational MDPs

Let Ba denote FODD resulting replacing node n B 0, Bb
FODD resulting replacing node n 1 leaves node n 0,
final result B 0 = Ba Bb0 Bb0 = Bb Bn0 . correctness Apply two
forms calculating B 0 give map.

b=x
0

p(y)
q(x)

x=y
p(y)
5

q(x)

10

q(x)

q(x)
10

p(x)
0

10

(b)

0

(a)

(c)
b=x

q(x)
5

0

0
(d)

0

p(x)
q(x)

10

q(x)
5

0

(e)

Figure 11: example equality reduction. (a) FODD reduction.
node x = satisfies condition E9.1 variable y. (b) Bnt -copy (nt extracted).
(c) Bnt -copy renamed produce Bn0 -copy. (d) Bnf -copy. (e) Final result
node n replaced apply(Bn0 -copy, Bnf -copy, max)
following prove node n equality condition E9.1 holds B
perform equality reduction R9 without changing map interpretation
satisfying D9.1. start properties FODDs defined above, e.g., B , Bb , Bb0 . Let
n denote set valuations reaching node n let denote set valuations
reaching node n B. basic definition MAP following:
Claim
(a)
(b)
(c)
(d)

1 interpretation I,
, MAPBa (I, ) = MAPB (I, ).
n , MAPBa (I, ) = 0.
, MAPBb (I, ) = 0.
n , MAPBb (I, ) = 1.

Claim 1 definition MAP, have,
Claim 2 interpretation I,
(a) , MAPBb0 (I, ) = 0.
(b) n , MAPBb0 (I, ) = MAPBn0 (I, ).
Claim 1, Claim 2, definition MAP have,

453

fiWang, Joshi, & Khardon

Claim 3 interpretation I,
(a) , MAPB 0 (I, ) = MAPB (I, ).
(b) n , MAPB 0 (I, ) = MAPBn0 (I, ).
Next prove main property reduction stating valuations reaching
node n B, old sub-FODD rooted n new (combined) sub-FODD produce
map.
Lemma 8 Let n set valuations reaching node n FODD B. interpretation satisfying D9.1, maxn MAPBn (I, ) = maxn MAPBn0 (I, ).
Proof: condition E9.1, variable x appear N F (n) hence value
n constrained. therefore partition valuations n disjoint
sets, n = { | valuation variables x}, variables
x fixed value x take value domain I. Assumption
D9.1 guarantees every contains least one valuation reaching Bnt least one
valuation reaching Bnf B. Note valuation reaches Bnt = x satisfied
thus MAPBnt (I, ) = MAPBn0 -copy (I, ). Since x appear Bnf also

MAPBn0 -copy (I, ) constant . Therefore correctness
f
Apply max MAPBn (I, ) = max MAPBn0 (I, ).
Finally, definition MAP, maxn MAPBn (I, ) = max max MAPBn (I, )
= max max MAPBn0 (I, ) = maxn MAPBn (I, ).
2
Lemma 9 Let B FODD, n node condition E9.1 holds, B 0 result
R9(n), interpretation satisfying D9.1, MAP B (I) = MAPB 0 (I).
Proof: Let X = maxm MAPB 0 (I, ) = maxn MAPB 0 (I, ). definition MAP, MAPB 0 (I) = max(X, ). However, Claim 3, X = maxm MAPB (I, )
Claim 3 Lemma 8, = maxn MAPBn0 (I, ) = maxn MAPBn (I, ). Thus
2
max(X, ) = MAPB (I) = MAPB 0 (I).
Lemma 9 guarantees correctness, applying practice may important
avoid violations sorting order (which would require expensive re-sorting
diagram). x variables sometimes replace new variable
name resulting diagram sorted. However always possible.
violation unavoidable, tradeoff performing reduction sorting
diagram ignoring potential reduction.
summarize, section introduced several new reductions compress diagrams significantly. first (R5) generic strong reduction removes implied
branches diagram. three (R7, R8, R9) weak reductions alter
overall map diagram alter map specific valuations. three
reductions complementary since capture different opportunities space saving.

5. Decision Diagrams MDPs
section show FODDs used capture RMDP. therefore use
FODDs represent domain dynamics deterministic action alternatives, probabilistic choice action alternatives, reward function, value functions.
454

fiFirst Order Decision Diagrams Relational MDPs

5.1 Example Domain
first give concrete formulation logistics problem discussed introduction. example follows exactly details given Boutilier et al. (2001), used
illustrate constructions MDPs. domain includes boxes, trucks cities,
predicates Bin(Box, City), in(T ruck, City), On(Box, ruck). Following
Boutilier et al. (2001), assume On(b, t) Bin(b, c) mutually exclusive,
box truck city vice versa. is, background knowledge includes
statements b, c, t, On(b, t) Bin(b, c) b, c, t, Bin(b, c) On(b, t). reward
function, capturing planning goal, awards reward 10 formula b, Bin(b, P aris)
true, box Paris. Thus reward allowed include constants
need completely ground.
domain includes 3 actions load, unload, drive. Actions effect
preconditions met. Actions also fail probability. attempting
load, successful version loadS executed probability 0.99, unsuccessful version loadF (effectively no-operation) probability 0.01. drive action executed
deterministically. attempting unload, probabilities depend whether raining not. raining successful version unloadS executed probability
0.9, unloadF probability 0.1. raining unloadS executed probability
0.7, unloadF probability 0.3.
5.2 Domain Dynamics
follow Boutilier et al. (2001) specify stochastic actions randomized choice
among deterministic alternatives. domain dynamics defined truth value diagrams (TVDs). every action schema A(~a) predicate schema p(~x) TVD
(A(~a), p(~x)) FODD {0, 1} leaves. TVD gives truth value p(~x)
next state A(~a) performed current state. call ~a action parameters, ~x predicate parameters. variables allowed TVD;
reasoning behind restriction explained Section 6.2. restriction sometimes sidestepped introducing action parameters instead variables.
truth value TVD valid fix valuation parameters.
TVD simultaneously captures truth values instances p(~x) next state.
Notice TVDs different predicates separate. safely done even
action coordinated effects (not conditionally independent) since action alternatives
deterministic.
Since allow action parameters predicate parameters, effects action
restricted predicates action arguments TVD expressive
simple STRIPS based schemas. example, TVDs easily express universal effects
action. see note p(~x) true ~x action A(~a) TVD
(A(~a), p(~x)) captured leaf valued 1. universal conditional effects
captured similarly. hand, since explicit universal quantifiers,
TVDs cannot capture universal preconditions.
domain, TVD predicate p(~x) defined generically Figure 12.
idea predicate true true undone action
false brought action. TVDs logistics domain
455

fiWang, Joshi, & Khardon

p( x )
bring


undo
0

0

1

Figure 12: template TVD

Bin (B, C)
1

(B, T)

(B, t*)

B= b*

T= t*
0

Tin (t*, C)
1

0

B= b*

B= b*

Bin (B, C)
0

1

C= c*
1

0
(b)

Bin (B, c*)
1

Tin (T, c*)

(c)

C c*
0

(d)

T= t*
C= c*

1
(e)

1

0

1

Tin (T, C)
T= t*

B= b*
T= t*

Tin( t*, C)

0
(a)

(B, T)

0

rain
0.7

Bin (b, Paris)

0.9

10

(f)

0
(g)

Figure 13: FODDs logistics domain: TVDs, action choice, reward function. (a)(b) TVDs Bin(B, C) On(B, ) action choice
unloadS(b , ). (c)(d) TVDs Bin(B, C) On(B, ) action
choice loadS(b , , c ). Note c must action parameter (d)
valid TVD. (e) TVD in(T, C) action choice driveS(t , c ).
(f) probability FODD action choice unloadS(b , ). (g) reward
function.

456

fiFirst Order Decision Diagrams Relational MDPs

running example given Figure 13. TVDs omitted figure
trivial sense predicate affected action. order simplify
presentation give TVDs generic form sort diagrams using
order proposed Section 3.5; TVDs consistent ordering Bin =
rain. Notice TVDs capture implicit assumption usually taken
planning-based domains preconditions action satisfied
action effect.
Notice utilize multiple path semantics maximum aggregation. predicate true true according one paths specified get disjunction
conditions free. use single path semantics Blockeel De Raedt
(1998) corresponding notion TVD significantly complicated since single
path must capture possibilities predicate become true. capture that, must
test sequentially different conditions take union substitutions
different tests turn requires additional annotation FODDs appropriate
semantics. Similarly operation would require union substitutions, thus complicating representation. explain issues detail Section 6.3
introduce first order value iteration algorithm.
5.3 Probabilistic Action Choice
One consider modeling arbitrary conditions described formulas state
control natures probabilistic choice action. multiple path semantics makes
hard specify mutually exclusive conditions using existentially quantified variables
way specify distribution. therefore restrict conditions either propositional
depend directly action parameters. condition interpretation follows
exactly one path (since variables thus empty valuation) thus
aggregation function interact probabilities assigned. diagram showing
action choice unloadS logistics example given Figure 13. example,
condition propositional. condition also depend action parameters,
example, assume result also affected whether box big not,
diagram Figure 14 specifying action choice probability.
Big(b*)
rain

0.9

0.7 0.9

Figure 14: example showing choice probability depend action parameters.
Note probability usually depends current state. depend arbitrary properties state (with restriction stated above), e.g., rain big(b ),
shown Figure 14. allow arbitrary conditions depend predicates arguments restricted action parameters dependence complex. However,
allow free variables probability choice diagram. example, cannot
model probabilistic choice unloadS(b , ) depends boxes truck ,
457

fiWang, Joshi, & Khardon

e.g., b, On(b, ) b 6= b : 0.2; otherwise, 0.7. write FODD capture
condition, semantics FODD means path 0.7 selected max aggregation distribution cannot modeled way. clearly restriction,
conditions based action arguments still give substantial modeling power.
5.4 Reward Value Functions
Reward value functions represented directly using algebraic FODDs. reward
function logistics domain example given Figure 13.

6. Value Iteration FODDs
Following Boutilier et al. (2001) define first order value iteration algorithm follows:
given reward function R action model input, set V0 = R, n = 0 repeat
procedure Rel-greedy termination:
Procedure 1 Rel-greedy
1. action type A(~x), compute:
A(~
x)

QV n

= R [ j (prob(Aj (~x)) Regr(Vn , Aj (~x)))]

(3)

A(~
x)

2. QA
Vn = obj-max(QVn ).
3. Vn+1 = maxA QA
Vn .
notation steps procedure discussed Section 2 except
work FODDs instead case statements. Note since reward function
depend actions, move object maximization step forward adding
reward function. I.e., first
A(~
x)

TV n

= j (prob(Aj (~x)) Regr(Vn , Aj (~x))),

followed
A(~
x)

QA
Vn = R obj-max(TVn ).
Later see object maximization step makes reductions possible; therefore moving step forward get savings computation. compute
updated value function way comprehensive example value iteration given
later Section 6.8.
(Puterman, 1994). case
Value iteration terminates kVi+1 Vi k (1)
2
need test values achieved two diagrams within (1)
2 .
formulations goal based planning problems use absorbing state zero
additional reward goal reached. handle formulation
one non-zero leaf R. case, replace Equation 3
A(~
x)

QV n

= max(R, j (prob(Aj (~x)) Regr(Vn , Aj (~x))).

see correct, note due discounting max value always R. R
satisfied state care action (max would R) R 0
state get value discounted future reward.
458

fiFirst Order Decision Diagrams Relational MDPs

Note goal based domains, i.e., one non-zero
leaf. mean cannot disjunctive goals, means must
value goal condition equally.
6.1 Regressing Deterministic Action Alternatives
first describe calculation Regr(Vn , Aj (~x)) using simple idea call block replacement. proceed discuss obtain result efficiently.
Consider Vn nodes FODD. node take copy corresponding TVD, predicate parameters renamed correspond
nodes arguments action parameters unmodified. BR-regress(V n , A(~x)) FODD
resulting replacing node Vn corresponding TVD, outgoing edges
connected 0, 1 leaves TVD.
Recall RMDP represents family concrete MDPs generated choosing
concrete instantiation state space (typically represented number objects
types). formal properties algorithms hold concrete instantiation.
Fix concrete instantiation state space. Let denote state resulting
executing action A(~x) state s. Notice Vn BR-regress(Vn , A(~x)) exactly
variables. following lemma:
Lemma 10 Let valuation variables Vn (and thus also variables
BR-regress(Vn , A(~x))). MAPVn (s, ) = MAPBRregress(Vn ,A(~x)) (s, ).
Proof: Consider paths P, P followed valuation two diagrams.
definition TVDs, sub-paths P applied guarantee corresponding nodes
P take truth values s. P, P reach leaf value
obtained.
2
naive implementation block replacement may efficient. use block
replacement regression resulting FODD necessarily reduced moreover,
since different blocks sorted start result even sorted. Reducing
sorting results may expensive operation. Instead calculate result
follows. FODD Vn traverse BR-regress(Vn , A(~x)) using postorder traversal
terms blocks combine blocks. step combine 3 FODDs
parent block yet processed (so TVD binary leaves)
two children processed (so general FODDs). call parent
Bn , true branch child Bt false branch child Bf represent
combination [Bn Bt ] [(1 Bn ) Bf ].
Lemma 11 Let B FODD Bt Bf FODDs, Bn FODD {0, 1}
leaves. Let B result using Apply calculate diagram [Bn Bt ][(1 Bn )Bf ].
interpretation valuation MAPB (I, ) = MAPB (I, ).
Proof: true since fixing valuation effectively ground FODD
paths mutually exclusive. words FODD becomes propositional clearly
combination using propositional Apply correct.
2
high-level description algorithm calculate BR-regress(V n , A(~x)) block
combination follows:
459

fiWang, Joshi, & Khardon

Procedure 2 Block Combination BR-regress(Vn , A(~x))
1. Perform topological sort Vn nodes (see example Cormen, Leiserson, Rivest,
& Stein, 2001).
2. reverse order, non-leaf node n (its children Bt Bf already
processed), let Bn copy corresponding TVD, calculate [Bn Bt ] [(1
Bn ) Bf ].
3. Return FODD corresponding root.
Notice different blocks share variables cannot perform weak reductions
process. However, perform strong reductions intermediate steps since
change map valuation. process completed perform
combination weak strong reductions since change map
regressed value function.
Blue (b)

(B, T)
1

Big(t)
On(b,t)
0

0

B= b*

Big(t)
(b, t)

T= t*

1
(a)

Blue (b)

0

Bin (B, c)
Tin (T, c)

0

b= b*
t= t*

0

1
(b)

Bin (b, c)
Tin (t, c)
1

0
(c)

Figure 15: example illustrating variables allowed TVDs.
explain cannot variables TVDs example illustrated Figure 15. Suppose value function defined Figure 15(a), saying
blue block big truck block truck
value 1 assigned. Figure 15(b) gives TVD On(B, ) action loadS,
c variable instead action parameter. Figure 15(c) gives result
block replacement. Consider interpretation domain {b1 , t1 , c1 , c2 } relations
{Blue(b1 ), Big(t1 ), Bin(b1 , c1 ), in(t1 , c1 )}. action loadS(b1 , t1 ) reach
state = {Blue(b1 ), Big(t1 ), On(b1 , t1 ), in(t1 , c1 )}, gives us value 0. Figure 15(c) b = b1 , = t1 evaluated gives value 1 valuation {b/b1 , c/c2 , t/t1 }.
choice c/c2 makes sure precondition violated. making c action parameter, applying action must explicitly choose valuation leads correct
value function. Object maximization turns action parameters variables allows us
choose argument maximize value.
460

fiFirst Order Decision Diagrams Relational MDPs

6.2 Regressing Probabilistic Actions
regress probabilistic action must regress deterministic alternatives combine choice probability Equation 3. discussed Section 2, due
restriction RMDP model explicitly specifies finite number deterministic
action alternatives, replace potentially infinite sum Equation 1 finite
sum Equation 3. done correctly every state result Equation 3
correct. following specify done FODDs.
Recall prob(Aj (~x)) restricted include action parameters cannot include variables. therefore calculate prob(Aj (~x))Regr(Vn , Aj (~x)) step (1) directly
using Apply. However, different regression results independent functions
sum j (prob(Aj (~x)) Regr(Vn , Aj (~x))) must standardize apart different regression results adding functions (note action parameters still considered
constants stage). holds addition reward function. need
standardize apart complicates diagrams often introduces structure
reduced. performing operations first use propositional Apply procedure
follow weak strong reductions.

V0

ASucc(x*)
q (x)

p (x)
10

p (A)
5

1

A=x*

0

q (A)
1

(a)

0
(b)
q (x2)

q (x1)
p (x1) 2.5
x1= x*

q (x2)
+

p (x2) 2.5
5

q (x1)
5

0



q (x1)
p (x1)

0

x1= x*
q (x1)

0
(c)

7.5

Figure 16: example illustrating need standardize apart.
Figure 16 illustrates need standardize apart different action outcomes. Action
succeed (denoted ASucc) fail (denoted AF ail, effectively no-operation),
chosen probability 0.5. Part (a) gives value function V 0 . Part (b) gives
TVD P (A) action choice ASucc(x ). TVDs trivial. Part
(c) shows part result adding two outcomes standardizing apart
(to simplify presentation diagrams sorted). Consider interpretation
domain {1, 2} relations {q(1), p(2)}. seen (c), choosing x = 1, i.e.
461

fiWang, Joshi, & Khardon

action A(1), valuation x1 = 1, x2 = 2 gives value 7.5 action (without
considering discount factor). Obviously standardize apart (i.e x 1 = x2 ),
leaf value 7.5 get wrong value. Intuitively contribution
ASucc value comes bring portion diagram AF ails
contribution uses bindings undo portion two portions refer
different objects. Standardizing apart allows us capture simultaneously.
Lemma 10 11 discussion far have:
Lemma 12 Consider concrete instantiation RMDP. Let Vn value function
corresponding MDP, let A(~x) probabilistic action domain.
A(~
x)
QVn calculated Equation 3 correct. is, state s, MAPQA(~x) (s)
Vn

expected value executing A(~x) receiving terminal value V n .
6.3 Observations Single Path Semantics

Section 5.2 suggested single path semantics Blockeel De Raedt (1998)
support value iteration well multiple path semantics. explanation
regression, use example illustrate this. Suppose value function
defined Figure 17(a), saying red block big city value 1
assigned. Figure 17(b) gives result block replacement action unloadS(b , ).
However correct. Consider interpretation domain {b 1 , b2 , t1 , c1 }
relations {Red(b2 ), Blue(b1 ), Big(c1 ), Bin(b1 , c1 ), in(t1 , c1 ), On(b2 , t1 )}. Note use
single path semantics. follow true branch root since b, c, Bin(b, c) true
{b/b1 , c/c1 }. follow false branch Red(b) since b, c, Bin(b, c) Red(b)
satisfied. Therefore get value 0. Clearly, get value 1 instead
{b/b2 , c/c1 }, impossible achieve value Figure 17(b) single
path semantics. reason block replacement fails top node decides true
branch based one instance predicate really need true instances
predicate filter true leaf TVD.
correct problem, want capture instances true
undone instances made true one path. Figure 17(c) gives one
possible way it. means variable renaming, stands union operator,
takes union substitutions. treated edge operations. Note
coordinated operation, i.e., instead taking union substitutions
b0 b00 , c0 c00 separately need take union substitutions (b0 , c0 )
(b00 , c00 ). approach may possible clearly leads complicated diagrams.
Similar complications arise context object maximization. Finally use
representation procedures need handle edge marking unions
substitutions approach look promising.
6.4 Object Maximization
Notice since handling different probabilistic alternatives action
separately must keep action parameters fixed regression process
added step 1 algorithm. step 2 maximize choice action
parameters. mentioned get maximization free. simply rename
462

fiFirst Order Decision Diagrams Relational MDPs

Bin(b ,c )

Bin(b , c )

Bin(b, c)

b =b*

Red(b)

Bin(b ,c )

On(b , t*)

b =b*

Tin(t*,c )

Big(c)
0

1
(a)

Red(b )
Big(c )
1

Red(b )

On(b ,t*)
0

1

0

On(b ,t*)
Tin(t*,c )

Tin(t*,c ) (b,c)
(b ,c )
(b,c)
(b ,c )
(b ,c )

Big(c )
0

b =b*

(b,c)
(b ,c )

0

Red(b)
Big(c)

(b)

1

0

(c)

Figure 17: example illustrating union or.

action parameters using new variable names (to avoid repetition iterations)
consider variables. aggregation semantics provides maximization
definition selects best instance action. Since constants turned
variables additional reduction typically possible stage. combination weak
strong reductions used. discussion following lemma:
Lemma 13 Consider concrete instantiation RMDP. Let Vn value function
corresponding MDP, let A(~x) probabilistic action domain.
QA
Vn calculated object maximization step 2 algorithm correct. is,
state s, MAPQA (s) maximum expected values achievable executing
Vn
instance A(~x) receiving terminal value Vn .
potential criticism object maximization essentially adding
variables diagram thus future evaluation diagram state becomes
expensive (since substitutions need considered). However, true
diagram remains unchanged object maximization. fact, illustrated
example given below, variables may pruned diagram process
reduction. Thus long final value function compact evaluation efficient
hidden cost.
6.5 Maximizing Actions
maximization Vn+1 = maxA QA
n+1 step (3) combines independent functions. Therefore must first standardize apart different diagrams, follow
propositional Apply procedure finally follow weak strong reductions.
clearly maintains correctness concrete instantiation state space.
463

fiWang, Joshi, & Khardon

6.6 Order Argument Types
resume discussion ordering argument types extend predicate
action parameters. above, structure suggested operations
algorithm. Section 3.5 already suggested order constants variables.
Action parameters special constants object maximization become
variables object maximization. Thus position allow behave
variables. therefore also order constants action parameters.
Note predicate parameters exist inside TVDs, replaced domain
constants variables regression. Thus need decide relative
order predicate parameters action parameters. put action parameters
predicate parameters latter replaced constant get order
violation, order useful. hand, put predicate parameters
action parameters instantiations predicate parameters possible.
Notice substituting predicate parameter variable, action parameters
still need larger variable (as TVD). Therefore, also order
action parameters variables.
summarize, ordering: constants variables (predicate parameters case
TVDs) action parameters, suggested heuristic considerations orders maximize potential reductions, avoid need re-sorting diagrams.
Finally, note want maintain diagram sorted times, need
maintain variant versions TVD capturing possible ordering replacements
predicate parameters. Consider TVD Figure 18(a). rename predicate parameters
X x2 x1 respectively, x1 x2 , resulting sub-FODD
shown Figure 18(b) violates order. solve problem define another
TVD corresponding case substitution X substitution ,
shown Figure 18(c). case replacing X x2 x1 , use TVD
Figure 18(c) instead one Figure 18(a).

On(X, Y)

On(x2, x1)

On(X, Y)

p(X)

p(x2)

p(Y)

p(x1)

p(Y)
1

0
(a)

1

p(X)
0

(b)

1

0
(c)

Figure 18: example illustrating necessity maintain multiple TVDs.

6.7 Convergence Complexity
Since step Procedure 1 correct following theorem:

464

fiFirst Order Decision Diagrams Relational MDPs

Theorem 2 Consider concrete instantiation RMDP. Let Vn value function
corresponding MDP n steps go. value Vn+1 calculated
Procedure 1 correctly captures value function n + 1 steps go.
is, state s, MAPVn+1 (s) maximum expected value achievable n + 1
steps.
Note RMDPs problems require infinite number state partitions.
Thus cannot converge V finite number steps. However, since algorithm
implements VI exactly, standard results approximating optimal value functions
policies still hold. particular following standard result (Puterman, 1994) holds
algorithm, stopping criterion guarantees approximating optimal value functions
policies.
Theorem 3 Let V optimal value function let Vk value function calculated
relational VI algorithm.
(1) r(s) kVn V k n
(2) kVn+1 Vn k

(1)
2

2M
)
log( (1)

log 1

.

kVn+1 V k .

algorithm maintains compact diagrams, reduction diagrams guaranteed domains. Therefore provide trivial upper bounds terms
worst case time complexity. Notice first every time use Apply procedure
size output diagram may large product size inputs.
must also consider size FODD giving regressed value function. Block
replacement O(N ) N size current value function, sorted
sorting may require exponential time space worst case. example,
Bryant (1986) illustrates ordering may affect size diagram. function
2n arguments, function x1 x2 + x3 x4 + + x2n1 x2n requires diagram
2n + 2 nodes, function x1 xn+1 + x2 xn+2 + + xn x2n requires 2n+1 nodes.
Notice two functions differ permutation arguments.
x1 x2 + x3 x4 + + x2n1 x2n result block replacement clearly sorting
requires exponential time space. true block combination procedure
method calculating result, simply output exponential
size. case heuristics change variable ordering, propositional ADDs
(Bryant, 1992), would probably useful.
Assuming TVDs, reward function, probabilities size C, action
action alternatives, current value function Vn N nodes, worst case
space expansion regression Apply operations, overall size result
2
time complexity one iteration O(C (N +1) ). However note
worst case analysis take reductions account. method
guaranteed always work efficiently, alternative grounding MDP
unmanageable number states deal with, despite high worst case complexity
method provides potential improvement. next example illustrates, reductions
substantially decrease diagram size therefore save considerable time computation.
465

fiWang, Joshi, & Khardon

6.8 Comprehensive Example Value Iteration
Figure 19 traces steps application value iteration logistics domain.
TVDs, action choice probabilities, reward function domain given Figure 13. simplify presentation, continue using predicate ordering Bin =
rain introduced earlier.5
Given V0 = R shown Figure 19(a), Figure 19(b) gives result regression
V0 unloadS(b , ) block replacement, denoted Regr(V0 , unloadS(b , )).
Figure 19(c) gives result multiplying Regr(V0 , unloadS(b , )) choice
probability unloadS P r(unloadS(b , )).
Figure 19(d) gives result P r(unloadF (b , )) Regr(V0 , unloadF (b , )). Notice diagram simpler since unloadF change state TVDs
trivial.
Figure 19(e) gives unreduced result adding two outcomes unload(b , ), i.e.,
result adding [P r(unloadS(b , ))Regr(V0 , unloadS(b , ))] [P r(unloadF (b , ))
Regr(V0 , unloadF (b , ))]. Note first standardize apart diagrams unloadS(b , )
unloadF (b , ) respectively renaming b b1 b2 . Action parameters b
stage considered constants change them. Also note
recursive part Apply (addition ) performed reductions, i.e., removing node
rain children lead value 10.
Figure 19(e), apply R6 node Bin(b2 , P aris) left branch.
conditions
P7.1: [b1 , Bin(b1 , P aris)] [b1 , b2 , Bin(b1 , P aris) Bin(b2 , P aris)],
V7.1: min(Bin(b2 , P aris)t ) = 10 max(Bin(b2 , P aris)f ) = 9,
V7.2: Bin(b2 , P aris)t constant
hold. According Lemma 3 Lemma 5 drop node Bin(b2 , P aris) connect
parent Bin(b1 , P aris) true branch. Figure 19(f ) gives result reduction.
Next, consider true child Bin(b2 , P aris) true child root.
conditions
P7.1: [b1 , b2 , Bin(b1 , P aris) Bin(b2 , P aris)] [b1 , Bin(b1 , P aris)],
V7.1: min(Bin(b1 , P aris)t ) = 10 max(Bin(b2 , P aris)t ) = 10,
V7.2: min(Bin(b1 , P aris)t ) = 10 max(Bin(b2 , P aris)f ) = 9
hold. According Lemma 3 Lemma 5, drop node Bin(b2 , P aris)
connect parent Bin(b1 , P aris) Bin(b2 , P aris)f . Figure 19(g) gives result
unload(b ,t )
reduction get fully reduced diagram. TV0
.
next step perform object maximization maximize action parameters
b get best instance action unload. Note b
become variables, perform one reduction: drop equality
right branch R9. Figure 19(h) gives result object maximization, i.e.,
unload(b ,t )
obj-max(TV0
). Note renamed action parameters avoid
repetition iterations.
unload(b ,t )
Figure 19(i) gives reduced result multiplying Figure 19(h), obj-max(TV0
),
= 0.9, adding reward function. result Qunload
.
1
5. details change substantially use order suggested Section 3.5 (where equality
first).

466

fiFirst Order Decision Diagrams Relational MDPs

Bin (b, Paris)

V0

10

Bin (b, Paris)
10

b= b*

b= b*
(b, t*)

Tin (t*, Paris)

Tin (t*, Paris)

10

0

7

9

1

3

(d)

(c)

Bin (b1, Paris)
10

0

rain

0

rain

(b)

Bin (b2, Paris)

Bin (b, Paris)

(b, t*)

0
(a)

Bin (b, Paris)

Bin (b1, Paris)
10

Bin (b2, Paris)

Bin (b2, Paris)

rain
b1= b*
9
(b1, t*)

7

Tin (t*, Paris)
10

rain

b1= b*

b1= b*
(b1, t*)

(b1, t*)

3 1 7

(b1, t*)

Tin (t*, Paris)

Tin (t*, Paris)
0

rain

b1= b*

10

Tin (t*, Paris)

rain

9

3 1 7

9

(e)

(f)

Bin (b1, Paris)
10

Bin (b1, Paris)

b1= b*
(b1, t*)

7

Q1unload

(b1, t1)

10

0

6.3 8.1
(h)

V1

6.3 8.1
(l)

0

Bin (b, Paris)

0

19

Tin (t, Paris)

(b, t*)

1

Tin (t*, Paris)

rain

Q

(i)

b= b*

(b, t)

Tin (t, Paris)

(j)

0

(k)

Bin (b, Paris)

Bin (b, Paris)
19

0

drive
1

rain

9
(g)

19

Tin (t, Paris)

9

7

Bin (b, Paris)

(b, t)

19

rain
0

Q1load

Bin (b, Paris)

Tin (t1, Paris)

Tin (t*, Paris)
rain

0

rain

Tin (t, Paris)

t= t*
0

0

On(b, t)

Tin (t, Paris)
0

rain
6.3 8.1

0

rain
6.3 8.1



b=b*

19



0



0

1
=

Tin (t, Paris)
0

rain
6.3 8.1
(n)

(m)

Figure 19: example value iteration Logistics Domain.

467

fiWang, Joshi, & Khardon

calculate Qload
Q1drive way results shown Figure 19(j)
1
Figure 19(k) respectively. drive TVDs trivial calculation
relatively simple. load, potential loading box already Paris dropped
diagram reduction operators process object maximization.
Figure 19(l) gives V1 , result maximizing Qunload
, Qload
Qdrive
.
1
1
1
standardized apart diagrams, maximized them, reduced
result. case diagram unload dominates actions. Therefore Q unload
1
becomes V1 , value function first iteration.
start second iteration, i.e., computing V2 V1 . Figure 19(m) gives
result block replacement regression V 1 action alternative unloadS(b , ).
Note sorted TVD on(B, ) obeys ordering chosen.
However, diagram resulting block replacement sorted.
address use block combination algorithm combine blocks bottom
up. Figure 19(n) illustrates combine blocks in(t, P aris), TVD,
two children, processed general FODDs. combine
in(t, P aris) two children, On(b, t)t processed. Since On(b, t)f = 0,
combine On(b, t) two children next step block combination.
Continuing process get sorted representation Regr(V1 , unloadS(b , )).
6.9 Extracting Optimal Policies
one way represent policies FODDs. simply note
policy represented implicitly set regressed value functions. value
iteration terminates, perform one iteration compute set Q-functions
using Equation 3.
Then, given state s, compute maximizing action follows:
1. Q-function QA(~x) , compute MAPQA(~x) (s), ~x considered variables.
2. maximum map obtained, record action name action parameters (from
valuation) obtain maximizing action.
clearly implements policy represented value function. alternative
approach represents policy explicitly developed context policy
iteration algorithm (Wang & Khardon, 2007).

7. Discussion
ADDs used successfully solve propositional factored MDPs. work gives one
proposal lifting ideas RMDPs. general steps similar, technical
details significantly involved propositional case. decision diagram
representation combines strong points SDP ReBel approaches RMDP.
one hand get simple regression algorithms directly manipulating diagrams.
hand get object maximization free ReBel. also get space saving
since different state partitions share structure diagrams. possible disadvantage
compared ReBel reasoning required reduction operators might complex.
468

fiFirst Order Decision Diagrams Relational MDPs

terms expressiveness, approach easily capture probabilistic STRIPS style
formulations ReBel, allowing flexibility since use FODDs capture
rewards transitions. example, representation capture universal effects
actions. hand, limited SDP since cannot use arbitrary
formulas rewards, transitions, probabilistic choice. example cannot express
universal quantification using maximum aggregation, cannot used reward
functions action preconditions. approach also capture grid-world RL domains
state based reward (which propositional) factored form since reward
described function location.
contrasting single path semantics multiple path semantics see
interesting tension choice representation task. multiple path method
directly support state partitions, makes awkward specify distributions
policies (since values actions must specified leaves). However,
semantics simplifies many steps easily supporting disjunction maximization
valuations crucial value iteration likely lead significant saving
space time.
implementation empirical evaluation progress. precise choice
reduction operators application crucial obtain effective system, since
general tradeoff run time needed reductions size resulting
FODDs. apply complex reduction operators get maximally reduced FODDs,
takes longer perform reasoning required. optimization still open issue
theoretically empirically. Additionally, implementation easily incorporate
idea approximation combining leaves similar values control size
FODDs (St-Aubin et al., 2000). gives simple way trading efficiency
accuracy value functions.
many open issues concerning current representation. results
FODDs give first step toward complete generalization ADDs. Crucially
yet semantically appropriate normal form important simplifying reasoning.
one define normal form (cf., Garriga et al., 2007, treatment conjunctions)
clear calculated incrementally using local operations ADDs.
would interesting investigate conditions guarantee normal form useful set
reduction operators FODDs.
Another possible improvement representation modified allow
compression. example allow edges rename variables traversed
compress isomorphic sub-FODDs illustrated Figure 17(c). Another
interesting possibility copy operator evaluates several copies predicate (with
different variables) node illustrated Figure 20. constructs
usable one must modify FODD MDP algorithmic steps handle diagrams
new syntactic notation.

8. Conclusion
paper makes two main contributions. First, introduce FODDs, generalization
ADDs, relational domains may useful various applications. developed
calculus FODDs reduction operators minimize size many open
469

fiWang, Joshi, & Khardon

p (x) p (y)

p (x)
q (x)

0

q (x)

p (y) 0

f (y) 0

f (y) 0
2

0

2

1

1

Figure 20: Example illustrating copy operator.

issues regarding best choice operators reductions. second contribution
developing FODD-based value iteration algorithm RMDPs potential
significant improvement previous approaches. algorithm performs general
relational probabilistic reasoning without ever grounding domains proved
converge abstract optimal value function solution exists.

References
Bahar, R. I., Frohm, E. A., Gaona, C. M., Hachtel, G. D., Macii, E., Pardo, A., & Somenzi,
F. (1993). Algebraic decision diagrams applications. Proceedings
International Conference Computer-Aided Design, pp. 188191.
Bellman, R. E. (1957). Dynamic programming. Princeton University Press.
Blockeel, H., & De Raedt, L. (1998). Top induction first order logical decision trees.
Artificial Intelligence, 101, 285297.
Boutilier, C., Dean, T., & Goldszmidt, M. (2000). Stochastic dynamic programming
factored representations. Artificial Intelligence, 121(1), 49107.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,
11, 194.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure policy construction. Proceedings International Joint Conference Artificial Intelligence,
pp. 11041111.
Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming first-order
MDPs. Proceedings International Joint Conference Artificial Intelligence,
pp. 690700.
Bryant, R. E. (1986). Graph-based algorithms boolean function manipulation. IEEE
Transactions Computers, C-35 (8), 677691.
Bryant, R. E. (1992). Symbolic boolean manipulation ordered binary decision diagrams. ACM Computing Surveys, 24 (3), 293318.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001). Introduction Algorithms. MIT Press.
470

fiFirst Order Decision Diagrams Relational MDPs

Driessens, K., Ramon, J., & Gartner, T. (2006). Graph kernels gaussian processes
relational reinforcement learning. Machine Learning, 64 (1-3), 91119.
Dzeroski, S., De Raedt, L., & Driessens, K. (2001). Relational reinforcement learning.
Machine Learning, 43, 752.
Feng, Z., & Hansen, E. A. (2002). Symbolic heuristic search factored Markov Decision
Processes. Proceedings National Conference Artificial Intelligence, pp.
455460.
Fern, A., Yoon, S., & Givan, R. (2003). Approximate policy iteration policy language
bias. International Conference Neural Information Processing Systems.
Fern, A., Yoon, S., & Givan, R. (2006). Approximate policy iteration policy language
bias: Solving relational Markov Decision Processes. Journal Artificial Intelligence
Research, 25, 75118.
Garriga, G., Khardon, R., & De Raedt, L. (2007). mining closed sets multi-relational
data. Proceedings International Joint Conference Artificial Intelligence,
pp. 804809.
Gretton, C., & Thiebaux, S. (2004). Exploiting first-order regression inductive policy
selection. Proceedings Conference Uncertainty Artificial Intelligence,
pp. 217225.
Groote, J. F., & Tveretina, O. (2003). Binary decision diagrams first-order predicate
logic. Journal Logic Algebraic Programming, 57, 122.
Gromann, A., Holldobler, S., & Skvortsova, O. (2002). Symbolic dynamic programming
within fluent calculus. Proceedings IASTED International Conference
Artificial Computational Intelligence.
Guestrin, C., Koller, D., Gearhart, C., & Kanodia, N. (2003a). Generalizing plans new
environments relational MDPs. Proceedings International Joint Conference
Artificial Intelligence, pp. 10031010.
Guestrin, C., Koller, D., Par, R., & Venktaraman, S. (2003b). Efficient solution algorithms
factored MDPs. Journal Artificial Intelligence Research, 19, 399468.
Hansen, E. A., & Feng, Z. (2000). Dynamic programming POMDPs using factored
state representation. Proceedings International Conference Artificial
Intelligence Planning Systems, pp. 130139.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planning using decision diagrams. Proceedings Conference Uncertainty Artificial
Intelligence, pp. 279288.
Hoolldobler, S., Karabaev, E., & Skvortsova, O. (2006). FluCaP: heuristic search planner
first-order MDPs. Journal Artificial Intelligence Research, 27, 419439.
Kersting, K., Otterlo, M. V., & De Raedt, L. (2004). Bellman goes relational. Proceedings
International Conference Machine Learning.
McMillan, K. L. (1993). Symbolic model checking. Kluwer Academic Publishers.
471

fiWang, Joshi, & Khardon

Puterman, M. L. (1994). Markov decision processes: Discrete stochastic dynamic programming. Wiley.
Rivest, R. L. (1987). Learning decision lists. Machine Learning, 2 (3), 229246.
Sanghai, S., Domingos, P., & Weld, D. (2005). Relational dynamic bayesian networks.
Journal Artificial Intelligence Research, 24, 759797.
Sanner, S., & Boutilier, C. (2005). Approximate linear programming first-order MDPs.
Proceedings Conference Uncertainty Artificial Intelligence.
Sanner, S., & Boutilier, C. (2006). Practical linear value-approximation techniques firstorder MDPs. Proceedings Conference Uncertainty Artificial Intelligence.
Sanner, S., & Boutilier, C. (2007). Approximate solution techniques factored first-order
MDPs. Proceedings International Conference Automated Planning
Scheduling.
Schuurmans, D., & Patrascu, R. (2001). Direct value approximation factored MDPs.
International Conference Neural Information Processing Systems, pp. 15791586.
St-Aubin, R., Hoey, J., & Boutilier, C. (2000). APRICODD: Approximate policy construction using decision diagrams. International Conference Neural Information
Processing Systems, pp. 10891095.
Wang, C. (2007). First order Markov Decision Processes. Tech. rep. TR-2007-4, Computer
Science Department, Tufts University.
Wang, C., Joshi, S., & Khardon, R. (2007). First order decision diagrams relational
MDPs. Proceedings International Joint Conference Artificial Intelligence,
pp. 10951100.
Wang, C., & Khardon, R. (2007). Policy iteration relational MDPs. Proceedings
Conference Uncertainty Artificial Intelligence.

472

fiJournal Artificial Intelligence Research 31 (2008) 1-32

Submitted 5/07; published 01/08

INI AX AT: Efficient Weighted Max-SAT Solver
Federico Heras
Javier Larrosa
Albert Oliveras

FHERAS @ LSI . UPC . EDU
LARROSA @ LSI . UPC . EDU
OLIVERAS @ LSI . UPC . EDU

Technical University Catalonia, LSI Department
Jordi Girona 1-3, 08034, Barcelona, Spain.

Abstract
paper introduce INI AX AT, new Max-SAT solver built top incorporates best current SAT Max-SAT techniques. handle hard clauses
(clauses mandatory satisfaction SAT), soft clauses (clauses whose falsification penalized cost Max-SAT) well pseudo-boolean objective functions constraints.
main features are: learning backjumping hard clauses; resolution-based substractionbased lower bounding; lazy propagation two-watched literal scheme. empirical
evaluation comparing wide set solving alternatives broad set optimization benchmarks
indicates performance INI AX usually close best specialized alternative
and, cases, even better.
+.

1. Introduction
Max-SAT optimization version SAT goal satisfy maximum number
clauses. considered one fundamental combinatorial optimization problems many important problems naturally expressed Max-SAT. include academic problems
max cut max clique, well real problems domains like routing, bioinformatics, scheduling
electronic markets.
long tradition theoretical work structural complexity (Papadimitriou,
1994) approximability (Karloff & Zwick, 1997) Max-SAT. work restricted
simplest case clauses equally important (i.e., unweighted Max-SAT)
fixed size (mainly binary ternary). practical point view, significant progress
made last 3 years (Shen & Zhang, 2004; Larrosa & Heras, 2005; Larrosa, Heras, & de Givry,
2007; Xing & Zhang, 2005; Li, Manya, & Planes, 2005, 2006). result, handful
new solvers deal, first time, instances involving hundreds variables.
main motivation work comes study Max-SAT instances modelling realworld problems. usually encounter three features:
satisfaction clauses importance, clause needs
associated weight represents cost violation. extreme case,
often happens practice observed Cha, Iwama, Kambayashi, Miyazaki (1997),
clauses whose satisfaction mandatory. usually modelled associating
high weight them.
Literals appear randomly along clauses. contrary, easy identify
patterns, symmetries kinds structures.
c
2008
AI Access Foundation. rights reserved.

fiH ERAS , L ARROSA , & LIVERAS

problems mandatory clauses reduce dramatically number feasible
assignments, optimization part problem plays secondary role. However,
problems mandatory clauses trivially satisfiable real difficulty lays
optimization part.
look current Max-SAT solvers, find none robust three
features. instance, Li et al.s (2005, 2006) solvers restricted formulas clauses
equally important (i.e. unweighted Max-SAT), Shen Zhangs (2004) one restricted binary clauses, one described Larrosa et al. (2007) seems efficient overconstrained
problems (i.e., small fraction clauses simultaneously satisfied), one
Alsinet, Manya, Planes (2005) seems efficient slightly overconstrained problems (i.e.
almost clauses satisfied). solver described Argelich Manya (2007), developed parallel research described paper, handle mandatory clauses
one incorporates learning, seems perform well structured problems. However,
non-mandatory clauses must weight. Finally, approaches based translating
Max-SAT instance SAT instance solve SAT solver seem effective
highly structured problems almost clauses mandatory (Fu & Malik, 2006; Le Berre,
2006).
paper introduce INI AX AT, new weighted Max-SAT solver incorporates
current best SAT Max-SAT techniques. build top INI + (Een & Sorensson,
2006), borrows capability deal pseudo-boolean problems INI (Een
& Sorensson, 2003) features processing mandatory clauses learning backjumping.
extended allowing deal weighted clauses, preserving two-watched literal
lazy propagation method. main original contribution INI AX implements
novel efficient lower bounding technique. Specifically, applies unit propagation
order detect disjoint subsets mutually inconsistent clauses done Li et al. (2006).
simplifies problem following Larrosa Heras (2005), Heras Larrosa (2006), Larrosa
et al. (2007) order increment lower bound. However, works clauses
accomplish specific patterns transformed, INI AX need define
patterns.
structure paper follows: Section 2 provides preliminary definitions SAT
Section 3 presents state-of-the-art solving techniques incorporated modern SAT solver
INI AT. Then, Section 4 presents preliminary definitions Max-SAT Section 5 overviews
INI AX AT. that, Sections 6 7 focus lower bounding additional features,
respectively. Section 8 present benchmarks used empirical evaluation
report experimental results. Finally, Section 9 presents related work Section 10 concludes
points possible future work.

2. Preliminaries SAT
sequel X = {x1 , x2 , . . . , xn } set boolean variables. literal either variable xi
negation xi . variable literal l refers noted var(l). Given literal l, negation l xi
l xi xi l xi . clause C disjunction literals. size clause, noted |C|,
number literals has. set variables appear C noted var(C). Sometimes
associate subscript Greek letter clause (e.g. (xi x j ) ) order facilitate future references
clause.
2

fiM INI AX AT:



E FFICIENT W EIGHTED AX -SAT OLVER

Algorithm 1: DPLL basic structure.
Function Search() : boolean
1
InitQueue( ) ;
2
Loop
3
UP( ) ;
4
Conflict
5
AnalyzeConflict( ) ;
6
Top Conflict return f alse ;
else
7
LearnClause( ) ;
8
Backjump( ) ;
9
10
11
12

else variables assigned return true ;
else
l := SelectLiteral( ) ;
Enqueue(Q, l) ;

assignment set literals containing variable negation. Assignments
maximal size n called complete, otherwise called partial. Given assignment ,
variable x unassigned neither x x belong . Similarly, literal l unassigned var(l)
unassigned.
assignment satisfies literal iff belongs assignment, satisfies clause iff satisfies
one literals falsifies clause iff contains negation literals.
latter case say clause conflicting always happens empty clause, noted
2. boolean formula F conjunctive normal form (CNF) set clauses representing
conjunction. model F complete assignment satisfies clauses F .
F model, call satisfiable, otherwise say unsatisfiable. Moreover,
complete assignments satisfy F , say F tautology.
Clauses size one called unit clauses simply units. formula contains unit l,
simplified removing clauses containing l removing l clauses
appears. application rule quiescence called unit propagation (UP) well
recognized fundamental propagation technique current SAT solvers.
Another well-known rule resolution, which, given formula containing two clauses
form (x A), (x B) (called clashing clauses), allows one add new clause (A B) (called
resolvent).

3. Overview State-of-the-art DPLL-based SAT Solvers
section overview architecture SAT solvers based DPLL (Davis, Logemann,
& Loveland, 1962) procedure. procedure, currently regarded efficient complete
search procedure SAT, performs systematic depth-first search space assignments.
internal node associated partial assignment two successors obtained selecting
unassigned variable x extending current assignment x x, respectively.
visited node, new units derived due application unit propagation (UP). leads
3

fiH ERAS , L ARROSA , & LIVERAS

Algorithm 2: Unit Propagation.
Function UP(Q) : Conflict
(Q contains non-propagated literals)
13
l := GetFirstNonPropagatedLit(Q); MarkAsPropagated(l) ;
14
foreach clause C l becomes unit falsified
15
C l becomes unit q Enqueue(Q, q) ;
16
else C l becomes falsified return Conflict ;
return None ;

conflicting clause, procedure backtracks, performing non-chronological backtracking
clause learning, originally proposed Silva Sakallah (1996).
algorithmic description DPLL procedure appears Algorithm 1. algorithm uses
propagation queue Q contains units pending propagation also contains representation
current assignment.
First, propagation queue Q filled units contained original formula (line 1).
main loop starts line 2 iteration procedure charge propagating pending
units (line 3). conflicting clause found (line 4), conflict analyzed (line 5) result
new clause learned (i.e, inferred recorded, line 7).
Then, procedure backtracks, using propagation queue Q undo assignment
exactly one literals learned clause becomes unassigned (line 8). one
backtrack still maintaining condition, advantageous (this commonly referred backjumping non-chronological backtracking, see Silva & Sakallah, 1996).
leads conflict, new unassigned literal selected extend current partial assignment.
new literal added Q (line 10) new iteration takes place.
procedure stops complete assignment found (line 9) top level conflict
found (line 6). first case, procedure returns true indicates model
found, second case returns f alse means model exists input
formula.
performance DPLL-based SAT solvers greatly improved 2001, SAT
solver C HAFF (Moskewicz, Madigan, Zhao, Zhang, & Malik, 2001) incorporated two-watched
literal scheme efficient unit propagation, First UIP scheme (Zhang, Madigan, Moskewicz,
& Malik, 2001) clause learning cheap VSIDS branching heuristic. Currently, stateof-the-art SAT solvers, like INI (Een & Sorensson, 2003), implement small variations
three features. following describe depth.
3.1 Unit Propagation
aim unit propagation twofold: one hand, finds clauses become units
due current assignment, hand, detects whether clause become
conflicting. concrete procedure given Algorithm 2. non-propagated literals exist
Q, picks oldest one l marks propagated (line 13). clauses containing l
may become falsified units traversed (we later describe clauses
detected). one clauses becomes unit q, enqueued Q propagated later (line
4

fiM INI AX AT:



E FFICIENT W EIGHTED AX -SAT OLVER

15). procedure iterates units propagate conflicting clause
found (line 16).
two types literals Q: decision literals algorithm heuristically
selected assigned branching point (lines 11 12 Algorithm 1); consequence literals
added logical consequences previous decision literals (line 15).
INI uses non-standard queue handle units pending propagation. Unlike classical queues,
fetching element, removed, marked such. Consequently, Q formed
two sets elements: already propagated literals literals pending propagation.
advantage strategy execution point, Q also contains current assignment.
Besides, propagated literals Q divided decision levels. decision level contains
decision literal set related consequences. Furthermore, literal l associated
original clause caused propagation noted l(); clause usually referred
reason l. Note decision literal l reason represented
ld .
Example 1 Consider formula {(x1 x2 ) , (x1 x3 ) , (x4 x5 ) }. starting execution,
propagation queue empty Q = [k]. use symbol k separate propagated literals (on
left) literals pending propagation (on right). literal x1 selected, added
Q. propagation queue contains Q = [kxd1 ]. propagate x1 add two new
consequences x2 x3 . propagation queue Q = [xd1 kx2 (), x3 ()] current
assignment {x1 , x2 , x3 }. propagation x2 x3 add new literals Q, becomes
Q = [xd1 , x2 (), x3 ()k]
x4 decided, add new consequence x5 . propagation, Q =

[x1 , x2 (), x3 (), xd4 , x5 ()k]. current assignment {x1 , x2 , x3 , x4 , x5 }. Note literals
propagated complete assignment found. Note well Q contains two
decision levels: first one formed literals x1 , x2 x3 second one formed
literals x4 x5 .
3.1.1 L AZY DATA TRUCTURES .
mentioned, aim detect units conflicting clauses. Taking account
process typically takes 80% total runtime SAT solver, important
design efficient data structures.
first attempt use adjacency lists. literal, one keeps list clauses
literal appears. Then, upon addition literal l assignment, clauses
containing l traversed. main drawback refinements detect efficiently
clause become unit, keeping counters indicating number unassigned
literals clause, involved considerable amount work upon backtracking.
method used INI two-watched literal scheme introduced Moskewicz et al.
(2001). basic idea clause cannot unit conflicting (i) one satisfied literal
(ii) two unassigned literals.
algorithm keeps two special literals clause, called watched literals, initially
two unassigned literals, tries maintain invariant always one satisfied literal two
unassigned literals watched.
invariant may broken one two watched literals becomes falsified.
case, clause traversed looking another non-false literal watch order restore
5

fiH ERAS , L ARROSA , & LIVERAS

invariant. one literal cannot found, clause declared true, unit conflicting depending value watched literal. Hence, literal l added assignment,
clauses may become falsified unit (line 14 Algorithm 2) clauses
l watched.
main advantage approach work clauses done upon
backtracking. However, main drawback way know many literals
unassigned given clause traversing literals. Note information used
techniques Two-sided Jeroslow branching heuristic (See Section 3.3).
3.1.2 R ESOLUTION R EFUTATION REES .
detects conflict, unsatisfiable subset clauses F 0 determined using information provided Q. Since F 0 unsatisfiable, empty clause 2 derived F 0 via
resolution. resolution process called refutation. refutation unsatisfiable clause set
F 0 resolution refutation tree (or simply refutation tree) every clause used exactly
resolution process.
refutation tree built propagation queue Q follows: let C0 conflicting
clause. Traverse Q LIFO (Last First Out) fashion clashing clause D0 found.
resolution applied C0 D0 , obtaining resolvent C1 . Next, traversal Q continues
clause D1 clashes C1 found, giving resolvent C2 iterate process
resolvent obtain empty clause 2. importance refutation trees become
relevant Section 6.
Example 2 Consider F = {(x1 ) , (x1 x4 ) , (x1 x2 ) , (x1 x3 x4 ) , (x1 x2 x3 ) , (x1 x5 ) }.
apply unit propagation unit clause enqueued producing Q = [kx1 ()]. x1
propagated Q becomes [x1 ()kx4 (), x2 (), x5 ()]. that, literal x4 propagated causing
clause become unit Q becomes [x1 (), x4 ()kx2 (), x5 (), x3 ()]. that, literal x2
propagated clause found conflicting. Figure 1.a shows state Q
propagation.
build refutation tree. Starting tail Q first clause clashing
conflicting clause . Resolution generates resolvent x1 x2 x4 . first
clause clashing x2 , producing resolvent x1 x4 . next clause clashing x4
resolution generates x1 . Finally, resolve clause obtain 2.Figure 1.b shows
resulting refutation tree.
3.2 Learning Backjumping
Learning backjumping best illustrated example (see Silva & Sakallah, 1996; Zhang
et al., 2001, precise description):
Example 3 Consider formula {(x1 x2 ) , (x3 x4 ) , (x5 x6 ) , (x2 x5 x6 ) } partial
assignment {x1 , x2 , x3 , x4 , x5 , x6 } leads conflict clause . Suppose current
propagation queue Q = [xd1 , x2 (), xd3 , x4 (), xd5 , x6 ()k].
example easy see decision xd1 incompatible decision xd5 . incompatibility represented clause (x1 x5 ). Similarly, consequence x2 incompatible
decision xd5 represented clause (x2 x5 ).
6

fiM INI AX AT:



E FFICIENT W EIGHTED AX -SAT OLVER

F = {(x1 ) , (x1 x4 ) , (x1 x2 ) , (x1 x3 x4 ) , (x1 x2 x3 ) , (x1 x5 ) }

(x1 x2 x3 ) (x1 x3 x4 )
x3 ()

x1 x2 x4

(x1 x2 )

x5 ()
x2 ()

x1 x4

(x1 x4 )

x1

(x1 )

x4 ()
x1 ()

2
a)

b)

Figure 1: Graphical representation propagation queue Q refutation tree example
2. top, original formula F . left, propagation Q step 1. Arrows
indicate order resolving clauses selected. right, resolution tree
computed step 2.

Clause learning implements different techniques used discover implicit incompatibilities adds formula. Learned clauses accelerate subsequent search,
since increase potential future executions. However, observed
unrestricted clause learning impractical cases (recorded clauses consume memory
repeated recording may lead exhaustion). reason, current SAT solvers incorporate
different clause deletion policies order remove learned clauses.
Learned clauses also used backjump presence would allowed unit propagation earlier decision level. case, say clause asserting backjumping
proceed going back level adding unit propagated literal. Among several
automated ways generating asserting clauses, INI uses so-called First Unique Implication Point (1UIP) (Zhang et al., 2001).
3.3 Branching Heuristic
Branching occurs function SelectLiteral (Algorithm 1). literals
propagate, function chooses one variable unassigned ones assigns value.
7

fiH ERAS , L ARROSA , & LIVERAS

importance branching heuristic well known, since different branching heuristic may
produce different-sized search trees.
Early branching heuristics include Bohms Heuristic (Buro & Buning, 1993), Maximum Ocurrences Minimum sized clauses (MOM) (Freeman, 1995) Two sided-Jeroslow
Wang Heuristic (Jeroslow & Wang, 1990). heuristics try choose literal
assignment generate largest number implications satisfy clauses.
heuristics state dependent, is, use information state clauses given
current assignment. them, information number unassigned literals
clause. Hence, implemented jointly data structures based adjacency lists since
keep information. instance, Two sided-Jeroslow Wang Heuristic computes
literal l F following function:
J(l) =



2|C|

CF
s.t. lC

selects literal l maximizes function J(l).
solvers become efficient, updating metrics state-dependent heuristics dominates
execution time. Hence INI uses slight modification state-independent heuristic first
proposed Moskewicz et al. (2001). heuristic, called Variable State Independent Decaying
Sum (VSIDS), selects literal appears frequently clauses, giving priority
recently learned clauses. advantage heuristic metrics updated
clauses learned. Since occurs occasionally, computation low overhead.
VSIDS heuristic suits perfectly lazy data structures two-watched literal scheme.

4. (Weighted) Max-SAT
weighted clause pair (C, w), C clause w integer representing cost
falsification, also called weight. problem contains clauses must satisfied,
call clauses mandatory hard associate special weight >. Non-mandatory
clauses also called soft. weighted formula conjunctive normal form (WCNF) set
weighted clauses. model complete assignment satisfies mandatory clauses. cost
assignment sum weights clauses falsifies. Given WCNF formula F ,
Weighted Max-SAT problem finding model F minimum cost. cost
called optimal cost F . Note formula contains mandatory clauses, weighted
Max-SAT equivalent classical SAT. clauses weight 1, so-called
(unweighted) Max-SAT problem. following, assume weighted Max-SAT.
say weighted formula F 0 relaxation weighted formula F (noted F 0 v F )
optimal cost F 0 less equal optimal cost F (non-models considered
cost infinity). say two weighted formulas F 0 F equivalent (noted F 0 F )
F 0 v F F v F 0 .
Max-SAT simplification rules transforms formula F equivalent, presumably simpler formula F 0 . SAT simplification rules (e.g. unit propagation, tautology removal,...)
directly applied Max-SAT restricted mandatory clauses. However, several specific Max-SAT
simplification rules exist (Larrosa et al., 2007). instance, formula contains clauses (C, u)
(C, v), replaced (C, u + v). contains clause (C, 0), may removed.
contains unit (l, >), simplified removing (including soft) clauses containing l
8

fiM INI AX AT:



E FFICIENT W EIGHTED AX -SAT OLVER

removing l clauses (including soft clauses) appears. application
rule quiescence natural extension unit propagation Max-SAT.
empty clause may appear weighted formula. weight >, clear
formula model. weight w < >, cost assignment include
weight, w obvious lower bound formula optimal cost. Weighted empty clauses
interpretation terms lower bounds become relevant Section 6.
shown Larrosa et al. (2007), notion resolution extended weighted formulas
follows 1 ,


(A B, m),








(x A, u m),
(x B, w m),
{(x A, u), (x B, w)}



(x B, m),






(x B, m)

B arbitrary disjunctions literals = min{u, w}.
(x A, u) (x B, w) called prior clashing clauses, (A B, m) called resolvent,
(x A, u m) (x B, w m) called posterior clashing clauses, (x B, m)
(x B, m) called compensation clauses. effect Max-SAT resolution, classical
resolution, infer (namely, make explicit) connection B. However,
important difference classical resolution Max-SAT resolution. former yields
addition new clause, Max-RES transformation rule. Namely, requires replacement
left-hand clauses right-hand clauses. reason cost prior clashing
clauses must substracted order compensate new inferred information. Consequently,
Max-RES better understood movement knowledge formula.
resolution rule Max-SAT preserves equivalence (). last two compensation clauses
may lose clausal form, following rule (Larrosa et al., 2007) may needed recover it:

l : |B| = 0
CNF(A l B, u) =

{(A l B, u)} CNF(A B, u) : |B| > 0

Example 4 apply weighted resolution following clauses {(x1 x2 , 3), (x1 x2 x3 , 4)}
obtain {(x2 x2 x3 , 3), (x1 x2 , 3 3), (x1 x2 x3 , 4 3), (x1 x2 (x2 x3 ), 3), (x1 x2 x2
x3 , 3)}. first clause simplified. second clause omitted weight
zero. fifth clause omitted tautology. fourth element clause
simple disjunction. Hence, apply CNF rule obtain two new
clauses CNF(x1 x2 (x2 x3 ), 3) = {(x1 x2 x2 x3 , 3), (x1 x2 x3 , 3)}. Note first new
clause tautology. Therefore, obtain equivalent formula {(x2 x3 , 3), (x1 x2 x3 , 1), (x1
x2 x3 , 3)}.

5. Overview INI AX
INI AX weighted Max-SAT solver built top INI + (Een & Sorensson, 2006).
DPLL-based SAT solver could used, INI + particularly wellsuited short open-source code. Besides, deal pseudo-boolean constraints.
1. empty clause represents tautology. special weight >, relations > = >
> > = > (Larrosa et al., 2007)

9

fiH ERAS , L ARROSA , & LIVERAS

Algorithm 3: INI AX basic structure.
Function Search() : integer
17
ub := LocalSearch(); lb := 0 ;
18
InitQueue(Q) ;
19
Loop
20
Propagate() ;
21
Hard Conflict
AnalyzeConflict() ;
Top Level Hard Conflict return ub ;
else
LearnClause() ;
Backjump() ;
22

23
24
25

26

else Soft Conflict
ChronologicalBactrack() ;
End Search return ub ;
else variables assigned
ub := lb ;
ub = 0 return ub ;
ChronologicalBactrack() ;
End Search return ub ;
else
l := SelectLiteral() ;
Enqueue(Q, l) ;

Given WCNF formula (possibly containing hard soft clauses), INI AX returns
cost optimal model (or > model). achieved means branch-andbound search, usually done solve optimization problems.
Like INI AT, tree assignments traversed depth-first manner. search point,
algorithm tries simplify current formula and, ideally, detect conflict, would mean
current partial assignment cannot successfully extended. INI AX distinguishes
two types conflicts: hard soft. Hard conflicts indicate model extending
current partial assignment (namely, mandatory clauses cannot simultaneously satisfied).
Hard conflicts detected taking account hard clauses using methods INI AT.
hard conflict occurs, INI AX learns hard clause backjumps INI would
do. Soft conflicts indicate current partial assignment cannot extended optimal
assignment. order identify soft conflicts, algorithm maintains two values search:
cost best model found far, upper bound ub optimal solution.
underestimation best cost achieved extending current partial assignment model, lower bound lb current subproblem.
soft conflict detected lb ub, means current assignment cannot lead
optimal model. soft conflict detected, algorithm backtracks chronologically. Note
10

fiM INI AX AT:



E FFICIENT W EIGHTED AX -SAT OLVER

Algorithm 4: MiniMaxSat propagation.
Function MS-UP() : conflict
(Q contains non-propagated literals)
27
l := GetFirstNonPropagatedLit(Q); MarkAsPropagated(l) ;
;
28
lb := lb +V (l))
29
lb ub return Soft Conflict ;
>) becomes unit falsified
30
foreach Hard clause (C l,

31
(C l, >) becomes unit (q, >) Enqueue(Q, q) ;
>) becomes falsified return Hard Conflict ;
32
else (C l,
u) becomes unit
33
foreach Soft clause (C l,

34
(C l, u) becomes unit (q, u) V (q) := V (q) + u ;

35
36
37
38
39

return None ;
Function Propagate() : conflict
c := MS-UP( ) ;
c = Hard Soft Conflict return c ;
improveLB( ) ;
lb ub return Soft Conflict ;
return None ;

one could also backjump computing clause expressing reasons led lb ub.
However, presence lots soft clauses, approach ends creating many long
clauses affect negatively efficience solver hence decided perform
simple chronological backtracking.
also want remark soft clause (C, w) w ub must satisfied optimal
assignment. Therefore, following assume soft clauses automatically transformed hard clauses previous search. ones, soft clause promoted
hard one search.
algorithmic description INI AX presented Algorithm 3. starting
search, good initial upper bound obtained local search method (line 17) may yield
identification new hard clauses. current implementation use U BCSAT (Tompkins & Hoos, 2004) default parameters. selected local search algorithm IROTS (Iterated
Robust Tabu Search) (Smyth, Hoos, & Stutzle, 2003). Besides, lower bound initialized
zero. Next, queue Q initialized unit hard clauses resulting formula (line 18).
main loop starts line 19 iteration charge propagating pending literals
(line 20) and, conflict detected, attempting extension current partial assignment
(line 26). Pending literals Q propagated function Propagate (line 20), may return hard soft conflict. hard conflict encountered (line 21) conflict analyzed,
new hard clause learned backjumping performed. done introduced Section 3.
soft conflict encountered (line 22) chronological backtracking performed. conflict
found (line 26), literal heuristically selected added Q propagation next iteration.
However, current assignment complete (line 23), upper bound updated. Search stops
zero-cost solution found, since cannot improved (line 24). Else, chronological
backtracking performed (line 25). Note backjumping leads termination top level hard
11

fiH ERAS , L ARROSA , & LIVERAS

conflict found, chronological backtracking leads termination two values first
assigned variable tried.
Algorithm 4 describes propagation process (function Propagate). uses array V (l)
accumulates weight soft clauses become unit l; namely, original
clauses (A l, w) current assignment falsifies A. clauses exists, assume
V (l) = 0. First all, performs Max-SAT-adapted form unit propagation (MS-UP, line 35).
MS-UP iterates non-propagated literals l Q (line 27). Firstly, adding l assignment

may make set soft clauses falsified. Since cost clauses kept V (l),
add lower bound (line 28). lower bound increment identifies soft conflict,
returned (line 29). Then, hard clause becomes unit, corresponding literal added Q
future propagation (line 31). Finally, soft clause becomes unit clause (q, u) (line 33),
weight u added V (q) (line 34). process hard conflict detected, function
returns (lines 32,36). Else, algorithm attempts detect soft conflict call procedure
improveLB (line 37), returns soft conflict found (line 38). next section
detailed description improveLB found. Finally, conflict detected, function
returns None (line 39).

6. Lower Bounding INI AX
following, consider arbitrary search state INI AX call
procedure improveLB. purpose section, search state characterized
current assignment. current assignment determines current subformula
original formula conditioned current assignment: clause contains literal part
current assignment, removed. Besides, literals whose negation appear current
assignment removed clauses appear.
value lb maintained INI AX precisely aggregation costs
clauses become empty due current assignment. Similarly, recall value
V (l) aggregation costs clauses become unit l due current
assignment. Thus, current subformula contains (2, lb) (l,V (l)) every l.
INI AX computes lower bound deriving new soft empty clauses (2, w)
resolution process. clauses added already existing clause (2, lb) producing
increment lower bound.
w) (l, u m), (l,
w
first step, improveLB replaces occurrence (l, u) (l,
m), (2, m) (with = min{u, w}), amounts applying restricted version Max-SAT resolution known Unit Neighborhood Resolution (UNR) (Larrosa et al., 2007).
produces immediate increment lower bound (i.e., weight empty clause
line 43) illustrated following example,
Example 5 Consider current state {(2, 3), (x1 , 1), (x2 , 1), (x1 , 2), (x2 , 2), (x1 x2 , 3)}. UNR
would resolve clauses (x1 , 1) (x1 , 2) replacing (x1 , 1) (2, 1) (all compensation clauses removed weight zero tautologies). two empty
clauses grouped (2, 3 + 1 = 4). UNR would also resolve clauses (x2 , 1) (x2 , 2)
replacing (x2 , 1) (2, 1). two empty clauses grouped (2, 4 + 1 = 5). So,
new equivalent formula {(2, 5), (x1 , 1), (x2 , 1), (x1 x2 , 3)} higher lower bound 5.
12

fiM INI AX AT:



E FFICIENT W EIGHTED AX -SAT OLVER

Algorithm 5: Lower Bounding INI AX
Function SUP() : conflict
40
InitQueue(Q) ;
(Q contains non-propagated literals)
l := GetFirstNonPropagatedLit(Q); MarkAsPropagated(l) ;
41
foreach (Hard Soft) Clause C l becomes unit falsified
C l becomes unit q Enqueue(Q, q) ;
else C l becomes falsified return conflict ;

42
43
44
45
46
47
48

return None ;
Procedure improveLB() : lb
w) F
foreach (l, v), (l,
w m), (2, m) := min (v, w) ;
replace (l, v m), (l,
SU P() = con f lict
:= BuildTree() ;
:= minimum weight among clauses ;
Condition ApplyResolution( , ) ;
else lb := lb + m; remove weight clauses ;

second step improveLB executes simulation unit propagation (SUP, line 44)
soft clauses treated hard. First, SUP adds Q unit soft clauses (line
40). Then, new literals Q propagated. new (hard soft) clauses become unit,
inserted Q (line 41). SUP yields conflict, means subset (soft
hard) clauses cannot simultaneously satisfied. showed Section 3 Q used
identify subset build refutation tree . ImproveLB computes tree (line 45).
take account weights clauses apply Max-SAT resolution (Section 4)
dictated , one see produce new clause (2, m), minimum
weight among clauses tree (line 46). means extension current partial
assignment unassigned variables cost least m.
important remark step Max-SAT resolution process consider
minimum weight two clauses, rather minimum clauses
resolution tree. passed parameter line 47.
result resolution process replacement clauses leaves
(2, m) corresponding compensation clauses (function ApplyResolution line 47),
thus obtaining equivalent formula lower bound increment m. call procedure
resolution-based lower bounding.
Example 6 Consider formula F = {(x1 , 2) , (x1 x4 , 1) , (x1 x2 , >) , (x1 x3 x4 , 2) , (x1
x2 x3 , 3) , (x1 x5 , 1) }
Step 1. Apply SUP. Initially, unit clause enqueued producing Q = [kx1 ()].
x1 propagated Q becomes [x1 ()kx4 (), x2 (), x5 ()]. Literal x4 propagated clause
becomes unit, producing Q = [x1 (), x4 ()kx2 (), x5 (), x3 ()]. that, literal x2 propagated
clause found conflicting. Figure 2.a shows state Q propagation.
13

fiH ERAS , L ARROSA , & LIVERAS

F = {(x1 , 2) , (x1 x4 , 2) , (x1 x2 , >) , (x1 x3 x4 , 2) , (x1 x2 x3 , 3) , (x1 x5 , 1) }

(x1 x2 x3 , 3) (x1 x3 x4 , 2)

x3 ()






x5 ()



x2 ()

(x1 x2 x3 , 1)
(x1 x2 x3 x4 , 2) (x1 x2 x4 , 2) (x1 x2 , >)
(x1 x2 x3 x4 , 2)
(x1 x2 , >)

(x1 x4 , 2)

(x1 x4 , 2)



x4 ()
x1 ()

(x1 , 2)

2

(x1 , 2)

(2, 2)

a)

b)

c)

F 0 = {(x1 x2 , >), (x1 x5 , 1), (2, 2), (x1 x2 x3 , 1), (x1 x2 x3 x4 , 2), (x1 x2 x3 x4 , 2)}
F

00

= {(x1 x2 , >), (x1 x2 x3 , 1), (x1 x5 , 1), (2, 2)}

Figure 2: Graphical representation INI AX lower bounding. top, original
current formula F . left, propagation Q step 1. middle, structure
refutation tree computed simulation step 2. right,
effect actually executing Max-SAT resolution (step 3). resulting formula F 0
appears bellow. substraction-based lower bounding performed, step 3 replaced
substraction weights, producing formula F 00 .

Step 2. Build simulated refutation tree. Starting tail Q first clause clashing
conflicting clause . Resolution generates resolvent x1 x2 x4 .
first clause clashing x2 , producing resolvent x1 x4 . next clause clashing
x4 resolution generates x1 . Finally, resolve clause obtain 2.Figure 2.b
shows resulting resolution tree.
Step 3. Apply Max-SAT resolution. apply Max-SAT resolution indicated refutation
tree computed Step 2. Figure 2.c graphically shows result process. Leaf clauses
original (weighted) clauses involved resolution. internal node indicates resolution
step. resolvents appear junction edges. Beside resolvent, inside box,
compensation clauses must added formula preserve equivalence. Since
clauses used resolution must removed, resulting formula F 0 consists root
14

fiM INI AX AT:



E FFICIENT W EIGHTED AX -SAT OLVER

tree ((2, 2)),all compensation clauses clauses used refutation tree. is,
resulting formula F 0 = {(x1 x2 , >), (x1 x5 , 1), (2, 2), (x1 x2 x3 , 1), (x1 x2 x3 x4 , 2), (x1
x2 x3 x4 , 2)}. soundness Max-SAT resolution guarantees F F 0 .
Remark 1 transformations applied resolution-based lower bounding passed
descendent nodes changes preserve equivalence. Nevertheless, transformations
restored backtracking takes place.
alternative problem transformation resolution identify lower bound increment substract clauses would participated resolution
tree. procedure similar lower bound computed Li et al. (2005) call
substraction-based (line 48) lower bounding.
Example 7 Consider formula F previous example. Steps 1 2 identical. However,
substraction-based lower bounding would replace Step 3 Step 3 substracts weight 2
clauses appear refutation tree adds (2, 2) formula. result
F 00 = {(x1 x2 , >), (x1 x2 x3 , 1), (x1 x5 , 1), (2, 2)}. Note F 00 v F .
Remark 2 substractions applied substraction-based lower bounding restored moving descendent node preserve equivalence.
increment lower bound either technique, procedure SUP executed
again, may yield new lower bound increments. process repeated SUP
detect conflict.
comparing two previous approaches, observe resolution-based lower bounding
larger overhead, resolution steps need actually computed consequences
must added current formula removed upon backtracking. However, effort invested
transformation may well amortized increment obtained lower bound
becomes part current formula, discovered
descendent nodes search. hand, substraction-based lower bounding
smaller overhead resolution needs actually computed. also facilitates
context restoration upon backtracking.
INI AX incorporates two alternatives chooses apply one heuristically (lines 47,48) depending specific condition (line 47). observed resolution-based
lower bounding seems effective resolution applied low arity clauses.
consequence, identification resolution tree, INI AX applies resolution-based
lower bounding largest resolvent resolution tree arity strictly less 4. Otherwise, applies substraction-based lower bounding. See Section 8 details.

7. Additional Features INI AX
section overview important features INI AX AT, namely use twowatched literal scheme, branching heuristic, use soft probing INI AX
deals pseudo-boolean functions.
15

fiH ERAS , L ARROSA , & LIVERAS

7.1 Two-Watched Literals
INI AX uses two-watched literal scheme also soft clauses. Recall one main
advantages technique, applied pure SAT problems, backtracking takes
place, work done clauses. Unfortunately, case soft clauses restoration needs done. soft clause becomes unit literal l function MS-UP, weight
added V (l) clause eliminated (or marked eliminated) avoid reusing
lower bounding procedure. changes, well addition lb, restored
backtracking performed. However, note executions SUP (simulation unit
propagation) clauses considered hard. case two-watched literal scheme works
exactly SAT solver hard soft clauses. inconsistency detected
SUP stops literals propagate, initial state recovered.
situation restoring initial state completely overhead free.
7.2 Branching Heuristic
INI AX incorporates two alternative branching heuristics. first one VSIDS heuristic (Moskewicz et al., 2001) disregarding soft clauses (that is, INI default). heuristic
likely good structured problems learning backjumping play significant role,
well problems difficult find models (namely, satisfaction component
problem difficult optimization component). Since heuristic disregards soft
clauses, likely ineffective problems easy find models difficulty
find optimal one prove optimality. extreme case, problems contain soft clauses (every complete assignment model) VSIDS heuristic blind therefore
completely useless.
overcome limitation VSIDS, INI AX also incorporates Weighted Jeroslow
heuristic (Heras & Larrosa, 2006). extension SAT Jeroslow heuristic described
Section 3. Given weighted formula F, literal l F following function defined:
J(l) =



2|C| w

(C,w)F
s.t. lC

mandatory clauses assumed weight equal upper bound ub. heuristic
selects literal highest value J(l). main disadvantage metrics need
updated visited node. combination two-watched literal updating becomes
expensive seem pay general. Thus, current implementation
heuristic, J(l) values computed root node used throughout solving
process. found experiments heuristic good alternative problems
difficulty lies optimization part (e.g. problems many models). INI AX
automatically changes VSIDS weighted Jeroslow problem contain literal
l hard clauses l hard clauses l.
heuristics, literal l V (l) + lb ub node search
tree, l selected literal l never assigned.
16

fiM INI AX AT:



E FFICIENT W EIGHTED AX -SAT OLVER

7.3 Soft Probing
Probing well-known SAT technique allows formulation hypothetical scenarios (Lynce
& Silva, 2003). idea temporarily assume l hard unit clause execute unit
propagation. yields conflict, know model extending current assignment must
contain l. process iterated literals quiescence. Exhaustive experiments
SAT context indicate expensive probe search (Le Berre, 2001; Lynce
& Silva, 2003), normally done pre-process order reduce initial number
branching points.
easily extend idea Max-SAT. context, besides discovery unit hard
clauses, may used make explicit weighted unit clauses. call soft probing. SAT,
idea temporarily assume l unit clause simulate unit propagation (i.e., execute
SUP()). Then, build resolution tree propagation queue Q. clauses
hard, know l must added assignment. Else, reproduce applying Max m) minimum
SAT resolution weighted clauses derive unit clause (l,
weight among clauses . unit soft clauses upfront makes future executions
improveLB much effective subsequent search. Besides, derive (l, u)
w), generate via unit neighborhood resolution (see Example 5) initial non-trivial lower
(l,
bound min{u, w}. tested soft probing search preprocessing several
benchmarks. observed empirically soft probing preprocessing best option
SAT.

Example 8 Consider formula F = {(x1 x2 , 1) , (x1 x3 , 1) , (x2 x3 , 1) }. assume x1
adding Q execute SUP conflict reached. obtain Q = [xd1 , x2 (), x3 ()]
detect conflicting clause. clauses involved refutation tree , , .
Resolving clauses results {(x1 x2 , 1) , (x1 x2 , 1), (x1 x2 x3 , 1), (x1 x2 x3 , 1)}.
resolution previous resolvent produces (equivalent) formula F 0 = {(x1 , 1), (x1
x2 x3 , 1), (x1 x2 x3 , 1)}.
7.4 Pseudo-boolean Functions
pseudo-boolean optimization problem (PBO) (Barth, 1995; Sheini & Sakallah, 2006; Een &
Sorensson, 2006) form:
minimize nj=1 c j x j
subject nj=1 ai j l j bi , = 1 . . .
x j {0, 1}, l j either x j 1 x j , c j , ai j bi non-negative integers.
INI AX provided PBO instance, translates Max-SAT formula follows: pseudo boolean constraint translated set hard clauses using INI + (Een
& Sorensson, 2006) (the algorithm heuristically decides appropriate translation choosing
among adders, sorters BDDs). objective function translated set soft unit clauses.
summand c j x j becomes new soft unit clause (x j , c j ). translation INI AX
executed usual.
17

fiH ERAS , L ARROSA , & LIVERAS

8. Empirical Results
section present benchmarks solvers used empirical evaluation. Then,
report experiments performed order adjust parameters INI AX AT. Finally,
comparison solvers presented.
8.1 Benchmarks Encodings
good set problems fundamental show effectiveness new solvers.
following, present several problems explain encode Weighted Max-SAT.
8.1.1 AX - K -SAT
k-SAT CNF formula CNF formula clauses size k. generated random
unsatisfiable 2-SAT 3-SAT formulas Cnfgen generator2 solved corresponding
MAX-SAT problem. benchmarks, fixed number variables varied number
clauses, repeated.
8.1.2 AX - CUT
Given graph G = (V, E), cut defined subset vertices U V . size cut
number edges (vi , v j ) vi U v j V U . Max-cut problem consists
finding cut maximum size. encoded Max-SAT associating one variable xi
graph vertex. Value true (respectively, false) indicates vertex vi belongs U (respectively,
V U ). edge (vi , v j ), two soft clauses (xi x j , 1), (xi x j , 1). Given complete
assignment, number violated clauses |E| size cut associated
assignment. experiments considered Max-Cut instances extracted random graphs
60 nodes varying number edges.
8.1.3 AX - ONE
Given satisfiable CNF formula, max-one problem finding model maximum
number variables set true. problem encoded Max-SAT considering
clauses original formula mandatory adding weighted unary clause (xi , 1)
variable formula. Note solving problem much harder solving usual SAT
problem, search cannot stop soon model found. optimal model must
found optimality must proved. considered max-one problem two types
CNF formula: random 3-SAT instances 120 variables (generated Cnfgen), structured
satisfiable instances coming 2002 SAT Competition3 .
8.1.4 INIMUM V ERTEX C OVERING



AX -C LIQUE

Given graph G = (V, E), vertex covering set U V every edge (vi , v j ) either
vi U v j U . size vertex covering |U |. minimum vertex covering problem
consists finding covering minimal size. naturally formulated (weighted) MaxSAT. associate one variable xi graph vertex vi . Value true (respectively, false) indicates
2. A. van Gelder ftp://dimacs.rutgers.edu/pub/challenge/satisfiability/contributed/UCSC/instances
3. http://www.satcompetition.org/2002/

18

fiM INI AX AT:



E FFICIENT W EIGHTED AX -SAT OLVER

vertex vi belongs U (respectively, V U ). binary hard (xi x j , >) edge
(vi , v j ). specifies one two vertices covering
edge connecting them. unary clause (xi , 1) variable xi , order specify
preferred add vertices U . simple way transform minimum vertex
coverings max-cliques vice-versa (Fahle, 2002).
experiments, considered maximum clique instances extracted random graphs
150 nodes varying number edges. also considered 66 Max-Clique instances
DIMACS challenge4 .
8.1.5 C OMBINATORIAL AUCTIONS
combinatorial auction defined set goods G set bidders bid indivisible
subsets goods. bid defined subset requested goods Gi G amount
money offered. bid-taker, wants maximize revenue, must decide bids
accepted. Note two bids request good, cannot jointly accepted (Sandholm,
1999). Max-SAT encoding, one variable xi associated bid. unit
clauses (xi , ui ) indicating bid accepted loss profit ui . Besides,
pair i, j conflicting bids, mandatory clause (xi x j , >).
experiments, used CATS generator (K. Leyton-Brown & Shoham, 2000)
allows generate random instances inspired real-world scenarios. particular, generated
instances Regions, Paths Scheduling distributions. number goods fixed
60 increased number bids. increasing number bids, instances become
constrained (namely, conflicting pairs bids) harder solve.
8.1.6 ISCELLANEOUS
also considered following sets instances widely used literature:
unsatisfiable instances 2nd DIMACS Implementation Challenge 5 considered
de Givry, Larrosa, Meseguer, Schiex (2003) Li et al. (2005): random 3-SAT instances
(aim dubois), pigeon hole problem (hole) coloring problems (pret). Observe
instances modelled unweighted Max-SAT (i.e. clauses weight 1).
Max-CSP random instances generated using protocol specified Larrosa Schiex
(2003) de Givry, Heras, Larrosa, Zytnicki (2005). distinguish 4 different sets
problems: Dense Loose (DL), Dense Tight (DT), Sparse Loose (SL) Sparse Tight (ST).
Tight instances 20 variables loose instances 40 variables.
set contains 10 instances 3 values 10 instances 4 values per variable.
Planning (Cooper, Cussat-Blanc, de Roquemaurel, & Regnier, 2006) graph coloring 6
structured instances taken Weighted Constraint Satisfaction Problem (WCSP) repository 7 .
4.
5.
6.
7.

ftp://dimacs.rutgers.edu/pub/challenge/graph/benchmarks/clique
http://mat.gsia.cmu.edu/challenge.html
http://mat.gsia.cmu.edu/COLORING02/benchmarks
http://mulcyber.toulouse.inra.fr/plugins/scmcvs/cvsweb.php/benchs/?cvsroot=toolbar

19

fiH ERAS , L ARROSA , & LIVERAS

Problems taken 2006 pseudo-boolean evaluation 8 : logic synthesis, misc (garden),
routing, MPI (Minimum Prime Implicant), MPS (miplib). instances encoded
Max-SAT specified previous section.
Note Max-CSP, Planning graph coloring instances encoded Max-SAT using
direct encoding (Walsh, 2000).
8.2 Alternative Solvers
compare INI AX several optimizers different communities. restricted
comparison freely available solvers. considered following ones:
AXSATZ (Li et al., 2006; Li, Manya, & Planes, 2007). Unweighted Max-SAT solver.
best unweighted Max-SAT solver 2006 Max-SAT Evaluation.
AX -DPLL (Heras & Larrosa, 2006; Larrosa et al., 2007). Weighted Max-SAT solver.
part OOLBAR package. best solver weighted Max-SAT second
best solver unweighted Max-SAT 2006 Max-SAT Evaluation.
OOLBAR (Larrosa, 2002; Larrosa & Schiex, 2003; de Givry et al., 2003, 2005).
state-of-the-art Weighted CSP solver.
P UEBLO 1.5 (Sheini & Sakallah, 2006). pseudo-boolean solver. ranked first
several categories 2005 Pseudo Boolean Evaluation.
INISAT + (Een & Sorensson, 2006). pseudo-boolean solver translates problems SAT solves MiniSat. ranked first several categories 2005
Pseudo Boolean Evaluation.
instances taken pseudo-boolean evaluation given original format
P UEBLO INISAT +. instances translated Max-SAT PBO partitioning
set clauses three sets: H contains mandatory clauses (C, >), W contains nonunary weighted clauses (C, u < >) U contains unary weighted clauses (l, u).
hard clause (C j , >) H pseudo boolean constraint C0j 1, C0j obtained
C j replacing + negated variables x 1 x. non-unary weighted clause
(C j , u j ) W pseudo boolean constraint C0j + r j 1, C0j computed before,
r j new variable that, set 1, trivially satisfies constraint. Finally, objective
function minimize is,



u jr j +



u jl j

(l j ,u j )U

(C j ,u j )W

8.3 Experimental Results
divide experiments two parts. purpose first part evaluate impact
different techniques INI AX set different parameters. Since
techniques effective benchmarks useless even counterproductive others (Brglez, Li, & Stallman, 2002), aimed finding configuration INI AX
8. http://www.cril.univ-artois.fr/PB06/

20

fiM INI AX AT:



E FFICIENT W EIGHTED AX -SAT OLVER

performs reasonably well instances. purpose second part compare AX alternative solvers. Since solvers specifically designed
type problems, expect INI AX outperform them. rather want
show robustness INI AX showing usually close performance
best alternative type problems.
Results presented plots tables. Regarding tables, first column contains name
set problems. second column shows number instances. remaining columns
report performance different solvers. cell contains average cpu time
solver required solve instances. solver could solve instances set,
number inside brackets indicates number solved instances average cpu time
takes account solved instances. cell contains dash, means instance could
solved within time limit. Regarding plots, note legend goes accordance
performance solvers. time limit set 900 seconds instance.
solver, written C++, implemented top INISAT + (Een & Sorensson, 2006).
Executions made 3.2 Ghz Xeon computer Linux. experiments random
instances, samples 30 instances plots report mean cpu time seconds.
8.4 Setting Parameters INI AX
following evaluate order importance following techniques inside INI AX AT: lower bounding, soft probing, branching heuristics, learning backjumping.
Starting basic version guides search Jeroslow branching heuristic
rest techniques deactivated, analyze one one. analysis studies one technique
incorporates previously analyzed ones corresponding tuned parameters.
three first experiments consider little challenging instances generated randomly
lower bounding plays fundamental role solve them. Finally, consider structured instances
learning backjumping required solve them.
8.4.1 L OWER

BOUNDING

experiment analyze impact resolution-based lower bounding versus substractionbased lower bounding, well combined strategies. considered following combination
two techniques: SUP detects inconsistency refutation tree computed,
look resolvent maximum size. size less equal parameter K,
resolution-based lower bounding applied, otherwise substraction-based lower bounding
applied. tested K = {0, 1, 2, 3, 4, 5, }. Note K = 0 corresponds pure substraction-based
lower bounding (and therefore similar approach Li et al., 2005), K = corresponds
pure resolution-based lower bounding.
results presented Figure 3. seen, pure substraction-based lower bounding K = 0 always worst option. Better results obtained K increases. However,
improvement stops (or nearly stops) K = 3. K > 3 significant improvement noticed. plot omits K = 4 K = 5 case clarity reasons. Since higher values K may
produce new clauses higher size may cause overhead instances, set K = 3
rest experiments.
21

fiH ERAS , L ARROSA , & LIVERAS

(a) Max-2-SAT, 100 variables

15
10

(b) Max-3-SAT, 60 variables

K=0
K=1
K=2
K=3
K=inf

cpu time

cpu time

20

5
0
200 300 400 500 600 700 800

60
50
40
30
20
10
0

K=0
K=1
K=2
K=inf
K=3

300

number clauses

400

500

600

700

800

number clauses

cpu time

(c) Max-CUT, 60 nodes
14
12
10
8
6
4
2
0
200

K=0
K=1
K=2
K=3
K=inf

250

300

350

400

450

number nodes

Figure 3: Performance INI AX different mixed lower boundings (K = 0, 1, 2, 3, inf).

8.4.2 OFT

PROBING

second experiment, evaluate impact soft probing. preliminary experiments,
observed soft probing time consuming, decided limit soft probing
follows. Initially, assign propagation level 0 variable probe. Then, new literal
propagate assigned propagation level L + 1 literal produces propagation
level L. limited probing propagate literals maximum propagation level M.
finally restricted 2 since gives best results. Note propagation level
decision level.
compare three alternatives: probing node search (S), probing pre-process
search (P) probing (N). results, Figure 4, indicates probing
search worst option Max-2-SAT Max-3-SAT produces improvement
Max-CUT. Finally, probing preprocessing gives slightly improvement Max-2-SAT
best results Max-CUT. Note soft probing preprocessing Max-3-SAT effect
omitted plot (its results similar N). Given results, decided include
soft probing preprocessing.
8.4.3 J EROSLOW

BRANCHING HEURISTIC

following experiment, evaluate importance weighted Jeroslow heuristic. Figure
5 shows time difference INI AX Jeroslow heuristic previous
two experiments (Jeroslow) without heuristic (None). results indicates guiding search
Jeroslow heuristic gives important speed ups. Hence, maintain Jeroslow heuristic
INI AX AT.
22

fiM INI AX AT:



E FFICIENT W EIGHTED AX -SAT OLVER

(a) Max-2-SAT, 100 variables

cpu time

20
15

(b) Max-3-SAT, 60 variables


N
P

cpu time

25

10
5
0
200 300 400 500 600 700 800

70
60
50
40
30
20
10
0


N

300

number clauses

400

500

600

700

800

number clauses
(c) Max-CUT, 60 nodes

5

cpu time

4

N

P

3
2
1
0
300

350
400
450
number nodes

500

Figure 4: Performance INI AX without soft probing, probing preprocessing (P)
probing search (S).

8.4.4 L EARNING ,

BACKJUMPING

VSIDS

final experiment, evaluate importance learning backjumping. experiments use structured instances, since well known learning backjumping
useful type problems. Besides, also evaluate importance VSIDS heuristic
combination learning backjumping. Recall heuristic specially designed
work cooperation learning, meaningless analyze effect itself.
Table 6 reports results experiment. third column reports results without learning
backjumping lower bounding, probing Jeroslow heuristic (None).
fourth column reports results adding learning backjumping previous version (Learning).
fifth column reports results adding learning, backjumping changing Jeroslow heuristic
VSIDS heuristic (VSIDS). results show INI AX without learning backjumping (None) clearly worst option. Significant improvements obtained learning
backjumping (Learning) added. Finally, adding VSIDS heuristic (VSIDS) improve results specially routing instances. Based results, incorporated learning
backjumping INI AX AT.
Regarding branching heuristic, problems literals appear hard clauses
polarities applies VSIDS heuristic, otherwise Jeroslow heuristic computed
root search tree stated Section 7. choice done starting
search.
23

fiH ERAS , L ARROSA , & LIVERAS

(a) Max-2-SAT, 100 variables

15

(b) Max-3-SAT, 60 variables
100

None
Jeroslow

None
Jeroslow

80
cpu time

cpu time

20

10
5

60
40
20

0
200 300 400 500 600 700 800

0
300

number clauses

400

500

600

700

800

number clauses
(c) Max-CUT, 60 nodes

5
cpu time

4

None
Jeroslow

3
2
1
0
200 250 300 350 400 450 500
number nodes

Figure 5: Performance INI AX without Heuristic (None) Jeroslow heuristic
computed root node search tree (Jeroslow).

Problem
Max-One 3col
Max-One cnt
Max-One dp
Max-One ezfact32
Routing S3
Routing S4

n. inst.
40
3
6
10
5
10

None

13.57(1)
16.11(4)
654.94(2)
22.26(4)


Learning
29.06
119.53
40.03
0.70
1.02
410.61(2)

VSIDS
15.41
6.58
28.63
0.77
0.10
91.09(9)

Figure 6: Structured instances.
8.5 Comparison Boolean Optimizers
reporting results, omit solver cannot deal corresponding instances
technical reasons (e.g. cannot deal weighted clauses) performs extremely bad
comparison others.
Figure 7 contains plots results different benchmarks. Plots b reports results
random unweighted Max-SAT instances. P UEBLO INISAT + orders magnitude slower,
included graphics. Max-2-SAT (plot a), INI AX lays
AX -DPLL AXSATZ, best option. Max-3-SAT (plot b) INI AX
clearly outperforms AX -DPLL close AXSATZ, best.
Max-2-SAT Max-3-SAT AXSATZ 3 times faster INI AX AT.
24

fiM INI AX AT:



E FFICIENT W EIGHTED AX -SAT OLVER

Plot c reports results random Max-CUT instances. INI AX performs slightly better
AXSATZ, second alternative.
random Max-One (plot d) INI AX best solver far. Almost instances
solved instantly P UEBLO AX -DPLL require 10 seconds difficult instances. INISAT + performs poorly. results structured Max-One instances reported
Figure 9. INISAT + seems fastest general. INI AX close performance
P UEBLO. Note, however, p instances, INI AX system solving
instances.
Plot e reports results Random Max-Clique instances. INI AX best solver,
order magnitude faster AX -DPLL, second best option. P UEBLO INISAT +
perform poorly again. Regarding structured Dimacs instances, INI AX best
option. solves 36 instances within time limit, AX -DPLL,M INISAT + P UEBLO
solve 34, 22 18 respectively.
Plots f , g h present results Combinatorial Auctions following different distributions.
paths distribution, INI AX best solver, twice faster AX -DPLL,
ranks second. regions distribution, INI AX best solver AX -DPLL
second best solver requiring double time. paths regions distributions, P UEBLO
INISAT + perform poorly. scheduling distribution, INISAT + best solver
INI AX AX -DPLL one order magnitude slower.
Results regarding unsatisfiable DIMACS instances presented Figure 8. Note
instances optimum cost 1. Hence, soon INI AX find solution cost 1,
clauses declared hard learning backjumping applied hard conflicts
arise. results indicate AXSATZ AX -DPLL solve instance sets
(Pret150 Aim200), INI AX solves sets instances best times
them, except hole instances AXSATZ slightly faster. encode
problems advantageous way P UEBLO INISAT +, is, decision problems
rather optimization problems solve instances similar times INI AX AT.
planning instances (Fig. 10) P UEBLO best solver. INI AX second best
solver, OOLBAR third last one INISAT +. surprising since OOLBAR
perform learning hard constraints. Results regarding graph coloring instances
presented Fig. 10. observed, INI AX able solve one instance
OOLBAR, P UEBLO INISAT + solve many less instances. Max-CSP problems
(Fig. 10) OOLBAR solves instances instantly P UEBLO worst option unable
solve lot instances. INI AX clearly second best solver INI + third
best performing solver. Note solve instances.
Results regarding instances taken pseudo-boolean evaluation found Figure
11. Note first time Max-SAT solver tested pseudo-boolean instances.
Results indicate solver consistently outperforms INI AX fairly
competitive P UEBLO INISAT +.
results conclude INI AX robust Weighted MaxSAT solver. competitive pure optimization problems problems lots
hard clauses and, sometimes, best option.
final remark, note INI AX almost previous benchmarks submitted Second Max-SAT Evaluation 2007, co-located event Tenth International Conference Theory Applications Satisfiability Testing. Hence, interested reader find
25

fiH ERAS , L ARROSA , & LIVERAS

(a) Max-2-SAT, 100 variables
50
30

cpu time

Max-DPLL
MiniMaxSat
Maxsatz

40
cpu time

(b) Max-3-SAT, 60 variables

20
10
0
200 300 400 500 600 700 800 900

300
250
200
150
100
50
0

Max-DPLL
MiniMaxSat
Maxsatz

300 400 500 600 700 800 900

number clauses

number clauses

(c) Max-CUT, 60 nodes

8

cpu time

Max-DPLL
Maxsatz
MiniMaxSat

10
cpu time

(d) Max-ONE, random 3-SAT, 120 variables

6
4
2
0
300

350

400

450

500

30
Minisat+
25
Pueblo
Max-DPLL
20
MiniMaxSat
15
10
5
0
150 200 250 300 350 400 450 500 550

number edges

number hard clauses

(e) Max-Clique, 150 nodes
50

Minisat+
Pueblo
Max-DPLL
MiniMaxSat

30

Pueblo
Minisat+
Max-DPLL
MiniMaxSat

80
cpu time

40
cpu time

(f) C. Auctions PATHS, 60 Goods
100

20
10

60
40
20

0

0
0

25

50

75

100

70 80 90 100 110 120 130 140 150

connectivity (%)

number bids

(g) C. Auctions SCHEDULING, 60 Goods

cpu time

40
30

(h) C. Auctions REGIONS, 60 Goods
20

Pueblo
Max-DPLL
MiniMaxSat
Minisat+

cpu time

50

20

15
10

Minisat+
Pueblo
Max-DPLL
MiniMaxSat

5

10
0

0
100

70 80 90 100 110 120 130 140 150
number bids

120

140

160

180

200

number bids

Figure 7: Plots different benchmarks. Note order legend goes accordance
performance solvers.

exhaustive comparison, including instances solvers, Second Max-SAT Evaluation 2007 web page9 . results evaluation showed INI AX best
performing solver two four existing categories.
9. http://www.maxsat07.udl.es/

26

fiM INI AX AT:



n. inst.
13
4
4
5
8
8
8

Problem
Dubois
Pret60
Pret150
Hole
Aim50
Aim100
Aim200

E FFICIENT W EIGHTED AX -SAT OLVER

INI AX
0.02
0.07
0.01
8.68
0.00
0.00
0.00

AXSATZ
148.18(7)
10.06

8.34
0.01
9.55


AX -DPLL
174.33(6)
22.00

28.00
0.00
172.00


Figure 8: Unsatisfiable DIMACS instances.

Problem
3col80
3col100
3col120
3col140
cnt
dp
ezfact32

n. inst.
10
10
10
10
3
6
10

INI AX
0.15
2.25
20.49
38.33
6.59
28.81
0.77

P UEBLO
0.10
1.73
14.52
83.17
0.13
1.19(3)
0.34

INISAT +
0.02
0.12
0.74
1.61
0.12
1.21(4)
0.33

Figure 9: Structured Max-one instances.

Problem
Planning
Graph Coloring
Max-CSP DL
Max-CSP DT
Max-CSP SL
Max-CSP ST

n. inst.
71
22
20
20
20
20

Toolbar
4.02
49.29(16)
0.08
0.00
0.01
0.00

INI AX
3.81
4.16(17)
0.20
0.01
0.03
0.01

P UEBLO
0.16
68.50(11)
349.08(13)

123.67


INISAT +
7.40
0.57(11)
8.60
2.40
0.48
1.29

Figure 10: Results WCSP Max-CSP instances.

9. Related Work
previous work done incorporating SAT-techniques inside Max-SAT solver.
Alsinet et al. (2005) presented lazy data structure detect clauses become unit, requires static branching heuristic. Argelich Manya (2006a) test different versions branch
bound procedure. One versions uses two-watched literals, uses basic
lower bounding. conclude none previous approaches general use
two-watched literals. far know, rest Max-SAT solvers based adjacency
lists. Therefore, presumably inefficient unit propagation (Lynce & Silva, 2005), par27

fiH ERAS , L ARROSA , & LIVERAS

Problem
misc
Logic synthesis
MPI
MPS
Routing

n. inst.
7
17
148
16
15

INI AX
3.08(5)
82.55(2)
37.35(107)
22.65(5)
58.74(14)

P UEBLO
8.51(5)
36.21(5)
32.04(101)
36.90(8)
5.96

INISAT +
0.14(5)
253.93(5)
3.06(105)
8.50(8)
13.09

Figure 11: Results pseudo-boolean instances.
ticularly presence long clauses. Argelich Manya (2006b) enhance Max-SAT branch
bound procedure learning hard constraints, used combination simple lower bounding techniques. improved version presented Argelich Manya (2007)
powerful lower bound, incorporate two-watched literal scheme,
backjumping, etc. best knowledge, Max-SAT solver incorporates backjumping.
Note INI AX restricts backjumping occurrence hard conflicts. Related works
integration backjumping techniques branch bound include work Zivan
Meisels (2007) Weighted CSP, Manquinho Silva (2004) pseudo-boolean optimization,
Nieuwenhuis Oliveras (2006) SAT Modulo Theories.
Max-SAT solvers use variations call substraction-based lower bounding.
cases, search special patterns mutually inconsistent subsets clauses (Shen &
Zhang, 2004; Xing & Zhang, 2005; Alsinet et al., 2005). efficiency reasons, patterns
always restricted small sets small arity clauses (2 3 clauses arity less 3). INI AX uses natural weighted extension approach proposed Li et al. (2005).
first one able detect inconsistencies arbitrarily large sets arbitrarily large clauses.
idea call resolution-based lower bounding inspired WCSP domain
(Larrosa, 2002; Larrosa & Schiex, 2003; de Givry et al., 2003, 2005) first proposed
Max-SAT context Larrosa Heras (2005) developed Li et al. (2007), Heras
Larrosa (2006), Larrosa et al. (2007). works, special patterns fixed-size
resolution trees executed. use simulated unit propagation allows INI AX
identify arbitrarily large resolution trees. following example, present two inconsistent
subsets clauses detected INI AX transformed equivalent formula
previous solvers cannot transform since limited specific patterns:
{(x1 , w1 ), (x2 , w2 ), (x3 , w3 ), (x1 x2 x3 , w4 )}
{(x1 , w1 ), (x1 x2 , w2 ), (x1 x2 x3 , w3 ), (x1 x2 x3 x4 , w4 ), (x1 x2 x3 x4 , w5 )}
first case, INI AX replaces clauses (2, m) = min{w1 , w2 , w3 , w4 }
set compensation clauses. second case, INI AX replaces (2, m)
= min{w1 , w2 , w3 , w4 , w5 } set compensation clauses. cases, equivalence
preserved. However, solvers literature detect inconsistent subset clauses
cannot transform problem equivalent one (Li et al., 2007) simply cannot detect
(Heras & Larrosa, 2006).
probing method derive weighted unit clauses related 2 RES cycle rule
Heras Larrosa (2006) Larrosa et al. (2007), failed literals Li et al. (2006),
28

fiM INI AX AT:



E FFICIENT W EIGHTED AX -SAT OLVER

singleton consistency CSP (Debruyne & Bessiere, 1999). Again, use simulated unit
propagation allows INI AX identify arbitrarily large resolution trees.

10. Conclusions Future Work
INI AX efficient robust Max-SAT solver deal hard soft
clauses well pseudo-boolean functions. incorporates best available techniques
type problems, performance similar best specialized solver. Besides development INI AX combining, first time, known techniques different fields,
main original contribution paper novel lower bounding technique based resolution.
INI AX lower bounding combines clean elegant way approaches proposed last years, mainly based unit-propagation-based lower
bounding resolution-based problem transformation. paper use information provided propagation queue (i) determine subset inconsistent clauses (ii) determine
simple ordering resolution applied increase lower bound generate
equivalent formula. However, necessarily best ordering so. easy see
different orderings may generate resolvents compensation clauses different arities. one
selects ordering generates smallest resolvents compensation clauses resulting
formula may presumably simpler. Future work concerns study orderings, development VSIDS-like heuristics soft clauses backjumping techniques soft conflicts.

Acknowledgments
would like thank Niklas Een Niklas Sorensson making INISAT + code publicly
available. also grateful anonymous referees helpful suggestions improving
paper.
work partially supported Spanish Ministry Education Science
projects TIN2006-15387-C03-02 (Heras Larrosa) TIN2004-03382 (Oliveras).

References
Alsinet, T., Manya, F., & Planes, J. (2005). Improved Exact Solvers Weighted Max-SAT.
Proceedings SAT05, Vol. 3569 LNCS, pp. 371377. Springer.
Argelich, J., & Manya, F. (2006a). Exact Max-SAT solvers over-constrained problems. J.
Heuristics, 12(4-5), 375392.
Argelich, J., & Manya, F. (2006b). Learning Hard Constraints Max-SAT. Proceedings
CSCLP06, Vol. 4651 LNCS, pp. 112. Springer.
Argelich, J., & Manya, F. (2007). Partial Max-SAT Solvers Clause Learning. Proceedings
SAT07, Vol. 4501 LNCS, pp. 2840. Springer.
Barth, P. (1995). Davis-Putnam Based Enumeration Algorithm Linear pseudo-Boolean Optimization. Research report MPI-I-95-2-003, Max-Planck-Institut fur Informatik, Im Stadtwald, D-66123 Saarbrucken, Germany.
Brglez, F., Li, X., & Stallman, M. (2002). role skeptic agent testing benchmarking
SAT algorithms. Proceedings SAT02, pp. 354361.
29

fiH ERAS , L ARROSA , & LIVERAS

Buro, M., & Buning, H. K. (1993). Report SAT Competition. Bulletin European
Association Theoretical Computer Science, 49, 143151.
Cha, B., Iwama, K., Kambayashi, Y., & Miyazaki, S. (1997). Local search algorithms partial
MAXSAT. Proceedings AAAI97, pp. 263268. MIT Press.
Cooper, M., Cussat-Blanc, S., de Roquemaurel, M., & Regnier, P. (2006). Soft Arc Consistency
Applied Optimal Planning. Proceedings CP06, Vol. 4204 LNCS, pp. 680684.
Springer.
Davis, M., Logemann, G., & Loveland, G. (1962). machine program theorem proving. Communications ACM, 5, 394397.
de Givry, S., Heras, F., Larrosa, J., & Zytnicki, M. (2005). Existential arc consistency: getting
closer full arc consistency weighted CSPs. Proceedings 19th IJCAI, pp. 8489.
Professional Book Center.
de Givry, S., Larrosa, J., Meseguer, P., & Schiex, T. (2003). Solving Max-SAT weighted CSP.
Proceedings CP03, Vol. 2833 LNCS, pp. 363376. Springer.
Debruyne, R., & Bessiere, C. (1999). practicable filtering techniques constraint satisfaction problem. Proceedings ICJAI97, pp. 412417. Morgan Kaufmann.
Een, N., & Sorensson, N. (2003). Extensible SAT-solver. Proceedings SAT03, Vol. 2919
LNCS, pp. 502518. Springer.
Een, N., & Sorensson, N. (2006). Translating Pseudo-Boolean Constraints SAT. Journal
Satisfiability, Boolean Modeling Computation, 2, 126.
Fahle, T. (2002). Simple fast: Improving branch-and-bound algorithm maximum clique.
Proceedings ESA02, Vol. 2461 LNCS, pp. 485498. Springer.
Freeman, J. W. (1995). Improvements Propositional Satisfiability Search Algorithms. Ph.D.
thesis, University Pennsylvania.
Fu, Z., & Malik, S. (2006). Solving Partial MAX-SAT Problem. Proceedings SAT06,
Vol. 4121 LNCS, pp. 252265. Springer.
Heras, F., & Larrosa, J. (2006). New Inference Rules Efficient Max-SAT Solving. Proceedings
21th AAAI. AAAI Press.
Jeroslow, R. G., & Wang, J. (1990). Solving propositional satisfiability problems. Annals Mathematics Artificial Intelligence, 1, 167187.
K. Leyton-Brown, M. P., & Shoham, Y. (2000). Towards universal test suite combinatorial
auction algorithms. Proceedings ACM Conference Electronic Commerce00, pp.
6676.
Karloff, H. J., & Zwick, U. (1997). 7/8-Approximation Algorithm MAX 3SAT?. FOCS,
pp. 406415.
Larrosa, J., & Heras, F. (2005). Resolution Max-SAT relation local consistency
weighted CSPs. Proceedings IJCAI05, pp. 193198. Professional Book Center.
Larrosa, J., Heras, F., & de Givry, S. (2007). logical approach efficient max-sat solving.
Artificial Intelligence. appear.
30

fiM INI AX AT:



E FFICIENT W EIGHTED AX -SAT OLVER

Larrosa, J., & Schiex, T. (2003). quest best form local consistency weighted
CSP. Proceedings 18th IJCAI, pp. 239244.
Larrosa, J. (2002). Node Arc Consistency Weighted CSP. Proceedings AAAI02, pp.
4853. AAAI Press.
Le Berre, D. (2001). Exploiting real power Unit Propagation Lookahead. Proceedings
LICS Workshop Theory Applications Satisfiability Testing.
Le Berre, D. (2006). SAT4j project Max-SAT.. http://www.sat4j.org/.
Li, C., Manya, F., & Planes, J. (2005). Exploiting Unit Propagation Compute Lower Bounds
Branch Bound Max-SAT Solvers. Proceedings CP05, Vol. 3709 LNCS, pp.
403414.
Li, C., Manya, F., & Planes, J. (2007). New Inference Rules Max-SAT. Journal Artificial
Intelligence Research. appear.
Li, C.-M., Manya, F., & Planes, J. (2006). Detecting Disjoint Inconsistent Subformulas Computing Lower Bounds Max-SAT. Proceedings 21th AAAI. AAAI Press.
Lynce, I., & Silva, J. P. M. (2003). Probing-Based Preprocessing Techniques Propositional
Satisfiability. Proceedings ICTAI03, pp. 105111. IEEE Computer Society.
Lynce, I., & Silva, J. P. M. (2005). Efficient data structures backtrack search SAT solvers. Ann.
Math. Artif. Intell., 43(1), 137152.
Manquinho, V. M., & Silva, J. P. M. (2004). Satisfiability-Based Algorithms Boolean Optimization. Ann. Math. Artif. Intell., 40(3-4), 353372.
Moskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering
Efficient SAT Solver. Proceedings DAC01, pp. 530535. ACM.
Nieuwenhuis, R., & Oliveras, A. (2006). SAT Modulo Theories Optimization Problems.
Proceedings SAT06, Vol. 4121 LNCS, pp. 156169. Springer.
Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley, USA.
Sandholm, T. (1999). Algorithm Optimal Winner Determination Combinatorial Auctions.
Proceedings IJCAI99, pp. 542547. Morgan Kaufmann.
Sheini, H. M., & Sakallah, K. A. (2006). Pueblo: Hybrid Pseudo-Boolean SAT Solver. Journal
Satisfiability, Boolean Modeling Computation, 2, 165189.
Shen, H., & Zhang, H. (2004). Study lower bounds Max-2-SAT. Proceedings AAAI04,
pp. 185190. AAAI Press / MIT Press.
Silva, J. P. M., & Sakallah, K. A. (1996). GRASP - new search algorithm satisfiability.
ICCAD, pp. 220227.
Smyth, K., Hoos, H. H., & Stutzle, T. (2003). Iterated Robust Tabu Search MAX-SAT.
Proceedings AI03, Vol. 2671 LNCS, pp. 129144. Springer.
Tompkins, D. A. D., & Hoos, H. H. (2004). UBCSAT: Implementation Experimentation
Environment SLS Algorithms SAT & MAX-SAT. Proceedings SAT04, Vol.
3542 LNCS, pp. 306320. Springer.
Walsh, T. (2000). SAT v CSP. Proceedings CP00, Vol. 1894 LNCS, pp. 441456. Springer.
31

fiH ERAS , L ARROSA , & LIVERAS

Xing, Z., & Zhang, W. (2005). MaxSolver: efficient exact algorithm (weighted) maximum
satisfiability. Artificial Intelligence, 164(1-2), 4780.
Zhang, L., Madigan, C. F., Moskewicz, M. W., & Malik, S. (2001). Efficient Conflict Driven Learning Boolean Satisfiability Solver. Proceedings ICCAD01, pp. 279285.
Zivan, R., & Meisels, A. (2007). Conflict directed Backjumping MaxCSPs. Proceedings
IJCAI07, pp. 198204.

32

fiJournal Artificial Intelligence Research 31 (2008) 543-590

Submitted 08/07; published 03/08

Creating Relational Data Unstructured
Ungrammatical Data Sources
Matthew Michelson
Craig A. Knoblock

michelso@isi.edu
knoblock@isi.edu

University Southern California
Information Sciences Instistute
4676 Admiralty Way
Marina del Rey, CA 90292 USA

Abstract
order agents act behalf users, retrieve integrate
vast amounts textual data World Wide Web. However, much useful data
Web neither grammatical formally structured, making querying difficult.
Examples types data sources online classifieds like Craigslist1 auction
item listings like eBay.2 call unstructured, ungrammatical data posts.
unstructured nature posts makes query integration difficult attributes
embedded within text. Also, attributes conform standardized values,
prevents queries based common attribute value. schema unknown
values may vary dramatically making accurate search difficult. Creating relational
data easy querying requires define schema embedded attributes
extract values posts standardizing values. Traditional information
extraction (IE) inadequate perform task relies clues data,
structure natural language, neither found posts. Furthermore,
traditional information extraction incorporate data cleaning, necessary
accurately query integrate source. two-step approach described paper
creates relational data sets unstructured ungrammatical text addressing
issues. this, require set known entities called reference set. first step
aligns post member reference set. allows algorithm define
schema post include standard values attributes defined schema.
second step performs information extraction attributes, including attributes
easily represented reference sets, price. manner create relational
structure previously unstructured data, supporting deep accurate queries
data well standard values integration. experimental results show
technique matches posts reference set accurately efficiently outperforms
state-of-the-art extraction systems extraction task posts.

1. Introduction
future vision Web includes computer agents searching information, making
decisions taking actions behalf human users. instance, agent could query
number data sources find lowest price given car email user
car listing, along directions seller available appointments see car.
1. www.craigslist.org
2. www.ebay.com
c
2008
AI Access Foundation. rights reserved.

fiMichelson & Knoblock

requires agent contain two data gathering mechanisms: ability query
sources ability integrate relevant sources information.
However, data gathering mechanisms assume sources designed support relational queries, well defined schema standard values
attributes. Yet always case. many data sources
World Wide Web would useful query, textual data within unstructured designed support querying. call text data sources
posts. Examples posts include text eBay auction listings, Internet classifieds
like Craigslist, bulletin boards Bidding Travel3 , even summary text
hyperlinks returned querying Google. running example, consider three
posts used car classifieds shown Table 1.

Table 1: Three posts Honda Civics Craigslist
Craigslist Post
93 civic 5speed runs great obo (ri) $1800
93- 4dr Honda Civc LX Stick Shift $1800
94 DEL SOL Si Vtec (Glendale) $3000

current method query posts, whether agent person, keyword search.
However, keyword search inaccurate cannot support relational queries. example,
difference spelling keyword attribute within post would
limit post returned search. would case user searched
example listings Civic since second post would returned. Another factor
limits keyword accuracy exclusion redundant attributes. example,
classified posts cars include car model, make, since make
implied model. shown first third post Table 1. cases,
user keyword search using make Honda, posts returned.
Moreover, keyword search rich query framework. instance, consider
query, average price Hondas 1999 later?
keyword search requires user search Honda retrieve 1999
later. user must traverse returned set, keeping track prices
removing incorrectly returned posts.
However, schema standardized attribute values defined entities
posts, user could run example query using simple SQL statement
accurately, addressing problems created keyword search. standardized
attribute values ensure invariance issues spelling differences. Also, post
associated full schema values, even though post might contain car
make, instance, schema correct value it, returned
query car makes. Furthermore, standardized values allow integration
source outside sources. Integrating sources usually entails joining two sources
directly attributes translations attributes. Without standardized values
3. www.biddingfortravel.com

544

fiRelational Data Unstructured Data Sources

schema, would possible link ungrammatical unstructured data
sources outside sources. paper addresses problem adding schema
standardized attributes set posts, creating relational data set support
deep accurate queries.
One way create relational data set posts define schema
fill values schema elements using techniques information extraction. sometimes called semantic annotation. example, taking second
post Table 1 semantically annotating might yield 93- 4dr Honda Civc LX Stick
Shift $1800 <make>Honda< \make> <model>Civc< \model> <trim>4dr LX< \trim>
<year>1993< \year> <price>1800< \price>. However, traditional information extraction, relies grammatical structural characteristics text identify attributes
extract. Yet posts definition structured grammatical. Therefore, wrapper
extraction technologies Stalker (Muslea, Minton, & Knoblock, 2001) RoadRunner
(Crescenzi, Mecca, & Merialdo, 2001) cannot exploit structure posts.
posts grammatical enough exploit Natural Language Processing (NLP) based extraction
techniques used Whisk (Soderland, 1999) Rapier (Califf & Mooney,
1999).
Beyond difficulties extracting attributes within post using traditional extraction methods, also require values attributes standardized,
process known data cleaning. Otherwise, querying newly relational data would
inaccurate boil keyword search. instance, using annotation above,
would still need query model Civc return record. Traditional
extraction address this.
However, data cleaning algorithms assume tuple-to-tuple transformations (Lee, Ling, Lu, & Ko, 1999; Chaudhuri, Ganjam, Ganti, & Motwani, 2003).
is, function maps attributes one tuple attributes another. approach would work ungrammatical unstructured data,
attributes embedded within post, maps set attributes
reference set. Therefore need take different approach problems figuring
attributes within post cleaning them.
approach creating relational data sets unstructured ungrammatical
posts exploits reference sets. reference set consists collections known entities
associated, common attributes. reference set online (or offline) set
reference documents, CIA World Fact Book.4 also online (or
offline) database, Comics Price Guide.5 Semantic Web one envision
building reference sets numerous ontologies already exist. Using standardized
ontologies build reference sets allows consensus agreement upon reference set values,
implies higher reliability reference sets others might exist one
experts opinion. Using car example, reference set might Edmunds car buying
guide6 , defines schema cars well standard values attributes
model trim. order construct reference sets Web sources,
4. http://www.cia.gov/cia/publications/factbook/
5. www.comicspriceguide.com
6. www.edmunds.com

545

fiMichelson & Knoblock

Edmunds car buying guide, use wrapper technologies (Agent Builder7 case)
scrape data Web source, using schema source defines car.
use reference set build relational data set exploit attributes
reference set determine attributes post extracted. first step
algorithm finds best matching member reference set post.
called record linkage step. matching post member reference set
define schema elements post using schema reference set,
provide standard attributes attributes using attributes reference
set user queries posts.
Next, perform information extraction extract actual values post
match schema elements defined reference set. step information
extraction step. information extraction step, parts post extracted
best match attribute values reference set member chosen
record linkage step. step also extract attributes easily represented
reference sets, prices dates. Although already schema
standardized attributes required create relational data set posts, still
extract actual attributes embedded within post accurately
learn extract attributes represented reference set, prices dates.
attributes extracted using regular expressions, extract actual
attributes within post might able accurately. example, consider
Ford 500 car. Without actually extracting attributes within post, might
extract 500 price, actually car name. overall approach outlined
Figure 1.
Although previously describe similar approach semantically annotating posts
(Michelson & Knoblock, 2005), paper extends research combining annotation work scalable record matching (Michelson & Knoblock, 2006).
make matching step annotation scalable, also demonstrates
work efficient record matching extends unique problem matching posts,
embedded attributes, structured, relational data. paper also presents
detailed description past work, including thorough evaluation procedure previously, using larger experimental data sets including reference set
includes tens thousands records.
article organized follows. first describe algorithm aligning
posts best matching members reference set Section 2. particular,
show matching takes place, efficiently generate candidate matches
make matching procedure scalable. Section 3, demonstrate
exploit matches extract attributes embedded within post. present
experiments Section 4, validating approaches blocking, matching information
extraction unstructured ungrammatical text. follow discussion
results Section 5 present related work Section 6. finish final
thoughts conclusions Section 7.

7. product Fetch Technologies http://www.fetch.com/products.asp

546

fiRelational Data Unstructured Data Sources

Figure 1: Creating relational data unstructured sources

2. Aligning Posts Reference Set
exploit reference set attributes create relational data posts, algorithm needs first decide member reference set best matches post.
matching, known record linkage (Fellegi & Sunter, 1969), provides schema attribute values necessary query integrate unstructured ungrammatical data
source. Record linkage broken two steps: generating candidate matches, called
blocking; separating true matches candidates matching
step.
approach, blocking generates candidate matches based similarity methods
certain attributes reference set compare posts. cars
example, algorithm may determine generate candidates finding common
tokens posts make attribute reference set. step detailed
Section 2.1 crucial limiting number candidates matches later examine
matching step. generating candidates, algorithm generates large set
features post candidate matches reference set. Using
features, algorithm employs machine learning methods separate true matches
false positives generated blocking. matching detailed Section 2.2.
547

fiMichelson & Knoblock

2.1 Generating Candidates Learning Blocking Schemes Record Linkage
infeasible compare post members reference set. Therefore
preprocessing step generates candidate matches comparing records
sets using fast, approximate methods. called blocking thought
partitioning full cross product record comparisons mutually exclusive blocks
(Newcombe, 1967). is, block attribute, first sort cluster data sets
attribute. apply comparison method single member block.
blocking, candidate matches examined detail discover true matches.
two main goals blocking. First, blocking limit number candidate matches, limits number expensive, detailed comparisons needed
record linkage. Second, blocking exclude true matches set candidate matches. means trade-off finding matching records
limiting size candidate matches. So, overall goal blocking make
matching step scalable, limiting number comparisons must make,
hindering accuracy passing many true matches possible.
blocking done using multi-pass approach (Hernandez & Stolfo, 1998),
combines candidates generated independent runs. example, cars
data, might make one pass data blocking tokens car model,
another run might block using tokens make along common tokens trim
values. One view multi-pass approach rule disjunctive normal form,
conjunction rule defines run, union rules combines
candidates generated run. Using example, rule might become ({tokenmatch, model} ({token-match, year}) ({token-match, make})). effectiveness
multi-pass approach hinges upon methods attributes chosen conjunctions.
Note conjunction set {method, attribute} pairs, make
restrictions methods used. set methods could include full string
metrics cosine similarity, simple common token matching outlined above, even
state-of-the-art n-gram methods shown experiments. key methods
necessarily choosing fastest (though show account method speed
below), rather choosing methods generate smallest set candidate
matches still cover true positives, since matching step consume
time.
Therefore, blocking scheme include enough conjunctions cover many true
matches can. example, first conjunct might cover true matches
datasets compared overlap years, second conjunct
cover rest true matches. adding independent runs
multi-pass approach.
However, since blocking scheme includes many conjunctions needs,
conjunctions limit number candidates generate. example, second
conjunct going generate lot unnecessary candidates since return records
share make. adding {method, attribute} pairs conjunction,
limit number candidates generates. example, change ({token-match,
548

fiRelational Data Unstructured Data Sources

make}) ({token-match, make} {token-match, trim}) still cover new true matches,
generate fewer additional candidates.
Therefore effective blocking schemes learn conjunctions minimize false
positives, learn enough conjunctions cover many true matches possible. two goals blocking clearly defined Reduction Ratio Pairs
Completeness (Elfeky, Verykios, & Elmagarmid, 2002).
Reduction Ratio (RR) quantifies well current blocking scheme minimizes
number candidates. Let C number candidate matches N size
cross product data sets.
RR = 1 C/N
clear adding {method,attribute} pairs conjunction increases
RR, changed ({token-match, zip}) ({token-match, zip} {token-match,
first name}).
Pairs Completeness (PC) measures coverage true positives, i.e., many
true matches candidate set versus entire set. Sm number
true matches candidate set, Nm number matches entire dataset,
then:
P C = Sm /Nm
Adding disjuncts increase PC. example, added second conjunction example blocking scheme first cover matches.
blocking approach paper, Blocking Scheme Learner (BSL), learns effective
blocking schemes disjunctive normal form maximizing reduction ratio pairs
completeness. way, BSL tries maximize two goals blocking. Previously
showed BSL aided scalability record linkage (Michelson & Knoblock, 2006),
paper extends idea showing also work case matching posts
reference set records.
BSL algorithm uses modified version Sequential Covering Algorithm (SCA),
used discover disjunctive sets rules labeled training data (Mitchell, 1997).
case, SCA learn disjunctive sets conjunctions consisting {method, attribute}
pairs. Basically, call LEARN-ONE-RULE generates conjunction, BSL keeps
iterating call, covering true matches left iteration. way
SCA learns full blocking scheme. BSL algorithm shown Table 2.
two modifications classic SCA algorithm, shown bold.
First, BSL runs examples left cover, rather stopping
threshold. ensures maximize number true matches generated
candidates final blocking rule (Pairs Completeness). Note might, turn,
yield large number candidates, hurting Reduction Ratio. However, omitting true
matches directly affects accuracy record linkage, blocking preprocessing step
record linkage, important cover many true matches possible.
way BSL fulfills one blocking goals: eliminating true matches possible. Second,
learn new conjunction (in LEARN-ONE-RULE step) current blocking
scheme rule already contains newly learned rule, remove
rule containing newly learned rule. optimization allows us check rule
containment go, rather end.
549

fiMichelson & Knoblock

Table 2: Modified Sequential Covering Algorithm
SEQUENTIAL-COVERING(class, attributes, examples)
LearnedRules {}
Rule LEARN-ONE-RULE (class, attributes, examples)
examples left cover,
LearnedRules LearnedRules Rule
Examples Examples - {Examples covered Rule}
Rule LEARN-ONE-RULE (class, attributes, examples)
Rule contains previously learned rules, remove
contained rules.
Return LearnedRules

rule containment possible guarantee learn less restrictive
rules go. prove guarantee follows. proof done contradiction.
Assume two attributes B, method X. Also, assume previously
learned rules contain following conjunction, ({X, A}) currently learned rule
({X, A} {X, B}). is, assume learned rules contains rule less
specific currently learned rule. case, must least
one training example covered ({X, A} {X, B}) covered ({X, A}), since
SCA dictates remove examples covered ({X, A}) learn it. Clearly,
cannot happen, since examples covered specific ({X, A} {X, B})
would covered ({X, A}) already removed, means could
learned rule ({X, A} {X, B}). Thus, contradiction.
stated before, two main goals blocking minimize size candidate set, removing true matches set. already mentioned
BSL maximizes number true positives candidate set describe
BSL minimizes overall size candidate set, yields scalable record
linkage. minimize candidate sets size, learn restrictive conjunction
call LEARN-ONE-RULE SCA. define restrictive minimizing number candidates generated, long certain number true matches
still covered. (Without restriction, could learn conjunctions perfectly minimize
number candidates: simply return none.)
this, LEARN-ONE-RULE step performs general-to-specific beam search.
starts empty conjunction step adds {method, attribute} pair
yields smallest set candidates still cover least set number true matches.
is, learn conjunction maximizes Reduction Ratio,
time covering minimum value Pairs Completeness. use beam search allow
backtracking, since search greedy. However, since beam search goes
general-to-specific, ensure final rule restrictive possible. full
LEARN-ONE-RULE given Table 3.
constraint conjunction minimum PC ensures learned conjunction over-fit data. Without restriction, would possible
LEARN-ONE-RULE learn conjunction returns candidates, uselessly producing
optimal RR.
550

fiRelational Data Unstructured Data Sources

algorithms behavior well defined minimum PC threshold. Consider,
case algorithm learning restrictive rule minimum
coverage. case, parameter ends partitioning space cross product
example records threshold amount. is, set threshold amount 50%
examples covered, restrictive first rule covers 50% examples.
next rule covers 50% remaining, 25% examples. next
cover 12.5% examples, etc. sense, parameter well defined. set
threshold high, learn fewer, less restrictive conjunctions, possibly limiting RR,
although may increase PC slightly. set lower, cover examples,
need learn conjuncts. newer conjuncts, turn, may subsumed later
conjuncts, waste time learn. So, long parameter small
enough, affect coverage final blocking scheme, smaller
slows learning. set parameter 50% experiments8 .
analyze running time BSL show BSL take account
running time different blocking methods, need be. Assume x (method,
attribute) pairs (token, f irst name). Now, assume beam size b, since
use general-to-specific beam-search Learn-One-Rule procedure. Also, time
being, assume (method, attribute) pair generate blocking candidates O(1)
time. (We relax assumption later.) time hit Learn-One-Rule within BSL,
try rules beam (attribute, method) pairs current
beam rules. So, worst case, takes O(bx) time, since (method,
attribute) pair beam, try (method, attribute) pairs. Now,
worst case, learned disjunct would cover 1 training example, rule
disjunction pairs x. Therefore, run Learn-One-Rule x times, resulting
learning time O(bx2 ). e training examples, full training time O(ebx2 ),
BSL learn blocking scheme.
Now, assumed (method, attribute) runs O(1) time,
clearly case, since substantial amount literature blocking methods
8. Setting parameter lower 50% insignificant effect results, setting much
higher, 90%, increased PC small amount (if all), decreasing RR.

Table 3: Learning conjunction {method, attribute} pairs
LEARN-ONE-RULE (attributes, examples, min thresh, k)
Best-Conjunction {}
Candidate-conjunctions {method, attribute} pairs
Candidate-conjunctions empty,
ch Candidate-conjunctions
first iteration
ch ch {method,attribute}
Remove ch duplicates, inconsistent max. specific
REDUCTION-RATIO(ch) > REDUCTION-RATIO(Best-Conjunction)
PAIRS-COMPLETENESS(ch) min thresh
Best-Conjunction ch
Candidate-conjunctions best k members Candidate-conjunctions
return Best-conjunction

551

fiMichelson & Knoblock

blocking times vary significantly (Bilenko, Kamath, & Mooney, 2006). Let
us define function tx (e) represents long takes single (method, attribute)
pair x generate e candidates training example. Using notation,
Learn-One-Rule time becomes O(b(xtx (e))) (we run tx (e) time pair x)
full training time becomes O(eb(xtx (e))2 ). Clearly running time dominated
expensive blocking methodology. rule learned, bounded
time takes run rule (method, attribute) pairs involved, takes O(xtx (n)),
n number records classifying.
practical standpoint, easily modify BSL account time takes
certain blocking methods generate candidates. Learn-One-Rule step,
change performance metric reflect Reduction Ratio blocking time
weighted average. is, given Wrr weight Reduction Ratio Wb
weight blocking time, modify Learn-One-Rule maximize performance
disjunct based weighted average. Table 4 shows modified version LearnOne-Rule, changes shown bold.

Table 4: Learning conjunction {method, attribute} pairs using weights
LEARN-ONE-RULE (attributes, examples, min thresh, k)
Best-Conj {}
Candidate-conjunctions {method, attribute} pairs
Candidate-conjunctions empty,
ch Candidate-conjunctions
first iteration
ch ch {method,attribute}
Remove ch duplicates, inconsistent max. specific
SCORE(ch) = Wrr REDUCTION-RATIO(ch)+Wb BLOCK-TIME(ch)
SCORE(Best-Conj) = Wrr REDUCTION-RATIO(Best-conj)+Wb BLOCK-TIME(Best-conj)
SCORE(ch) > SCORE(Best-conj)
PAIRS-COMPLETENESS(ch) min thresh
Best-conj ch
Candidate-conjunctions best k members Candidate-conjunctions
return Best-conj

Note set Wb 0, using version Learn-One-Rule
used throughout paper, consider Reduction Ratio. Since
methods (token n-gram match) simple compute, requiring time build
initial index candidate generation, safely set Wb 0. Also,
making trade-off time versus reduction might always appropriate decision.
Although method may fast, sufficiently reduce reduction ratio,
time takes record linkage step might increase time would
taken run blocking using method provides larger increase reduction ratio.
Since classification often takes much longer candidate generation, goal
minimize candidates (maximize reduction ratio), turn minimizes classification
time. Further, key insight BSL choose blocking method,
importantly choose appropriate attributes block on. sense,
BSL like feature selection algorithm blocking method. show
552

fiRelational Data Unstructured Data Sources

experiments, blocking important pick right attribute combinations,
BSL does, even using simple methods, blocking using sophisticated
methods.
easily extend BSL algorithm handle case matching posts members
reference set. special case posts attributes embedded
within reference set data relational structured schema elements.
handle special case, rather matching attribute method pairs across
data sources LEARN-ONE-RULE, instead compare attribute method
pairs relational data entire post. small change, showing
algorithm works well even special case.
learn good blocking scheme, efficiently generate candidates
post set align reference set. blocking step essential mapping large
amounts unstructured ungrammatical data sources larger larger reference
sets.
2.2 Matching Step
set candidates generated blocking one find member
reference set best matches current post. is, one data sources record (the
post) must align record data source (the reference set candidates).
whole alignment procedure referred record linkage (Fellegi & Sunter,
1969), refer finding particular matches blocking matching step.

Figure 2: traditional record linkage problem
However, record linkage problem presented article differs traditional
record linkage problem well studied. Traditional record linkage matches record
one data source record another data source relating respective,
decomposed attributes. instance, using second post Table 1, assuming
decomposed attributes, make post compared make reference
553

fiMichelson & Knoblock

Figure 3: problem matching post reference set
set. also done models, trims, etc. record reference set
best matches post based similarities attributes would considered
match. represented Figure 2. Yet, attributes posts embedded
within single piece text yet identified. text compared reference
set, already decomposed attributes extraneous
tokens present post. Figure 3 depicts problem. type matching
traditional record linkage approaches apply.
Instead, matching step compares post attributes reference set
concatenated together. Since post compared whole record reference set
(in sense attributes), comparison record level
approximately reflects similar embedded attributes post
attributes candidate match. mimics idea traditional record linkage,
comparing fields determines similarity record level.
However, using record level similarity possible two candidates
generate record level similarity differing individual attributes. one
attributes discriminative other, needs way reflect
that. example, consider Figure 4. figure, two candidates share make
model. However, first candidate shares year second candidate shares
trim. Since candidates share make model, another
attribute common, possible generate record level comparison. Yet,
trim car, especially rare thing like Hatchback discriminative
sharing year, since lots cars make, model year,
differ trim. difference individual attributes needs reflected.
discriminate attributes, matching step borrows idea traditional
record linkage incorporating individual comparisons attribute
554

fiRelational Data Unstructured Data Sources

Figure 4: Two records equal record level different field level similarities

data source best way determine match. is, record level
information enough discriminate matches, field level comparisons must exploited
well. field level comparisons matching step compares post
individual attribute reference set.
record field level comparisons represented vector different similarity functions called RL scores. incorporating different similarity functions, RL scores
reflects different types similarity exist text. Hence, record level
comparison, matching step generates RL scores vector post
attributes concatenated. generate field level comparisons, matching step calculates RL scores post individual attributes reference
set. RL scores vectors stored vector called VRL . populated,
VRL represents record field level similarities post member
reference set.
example reference set Figure 3, schema 4 attributes <make, model,
trim, year >. Assuming current candidate <Honda, Civic, 4D LX, 1993>,
VRL looks like:

VRL =<RL
RL
RL
RL
RL

scores(post,
scores(post,
scores(post,
scores(post,
scores(post,

Honda),
Civic),
4D LX),
1993),
Honda Civic 4D LX 1993)>

generally:
555

fiMichelson & Knoblock

VRL =<RL scores(post,
RL scores(post,
...,
RL scores(post,
RL scores(post,

attribute1 ),
attribute2 ),
attributen ),
attribute1 attribute2 . . . attributen )>

RL scores vector meant include notions many ways exist define
similarity textual values data sources. might case
one attribute differs another misplaced, missing changed letters. sort
similarity identifies two attributes similar, misspelled, called edit
distance. Another type textual similarity looks tokens attributes
defines similarity based upon number tokens shared attributes.
token level similarity robust spelling mistakes, puts emphasis
order tokens, whereas edit distance requires order tokens match
order attributes similar. Lastly, cases one attribute may sound
like another, even spelled differently, one attribute may share common
root word another attribute, implies stemmed similarity. last two
examples neither token edit distance based similarities.
capture different similarity types, RL scores vector built three vectors reflect different similarity types discussed above. Hence, RL scores
is:
RL scores(post, attribute)=<token scores(post, attribute),
edit scores(post, attribute),
scores(post, attribute)>
vector token scores comprises three token level similarity scores. Two similarity
scores included vector based Jensen-Shannon distance, defines
similarities probability distributions tokens. One uses Dirichlet prior (Cohen,
Ravikumar, & Feinberg, 2003) smooths token probabilities using JelenikMercer mixture model (Zhai & Lafferty, 2001). last metric token scores vector
Jaccard similarity.
scores included, token scores vector takes form:
token scores(post, attribute)=<Jensen-Shannon-Dirichlet(post, attribute),
Jensen-Shannon-JM-Mixture(post, attribute),
Jaccard(post, attribute)>
vector edit scores consists edit distance scores comparisons
strings character level defined operations turn one string another.
instance, edit scores vector includes Levenshtein distance (Levenshtein, 1966),
returns minimum number operations turn string string T, SmithWaterman distance (Smith & Waterman, 1981) extension Levenshtein
distance. last score vector edit scores Jaro-Winkler similarity (Winkler
& Thibaudeau, 1991), extension Jaro metric (Jaro, 1989) used find
similar proper nouns. strict edit-distance, regard operations
transformations, Jaro-Winkler metric useful determinant string similarity.
character level metrics, edit scores vector defined as:
556

fiRelational Data Unstructured Data Sources

edit scores(post, attribute)=<Levenshtein(post, attribute),
Smith-Waterman(post, attribute),
Jaro-Winkler(post, attribute)>
similarities edit scores token scores vector defined SecondString package (Cohen et al., 2003) used experimental implementation
described Section 4.
Lastly, vector scores captures two types similarity fit
either token level edit distance similarity vector. vector includes two types
string similarities. first Soundex score post attribute.
Soundex uses phonetics token basis determining similarity.
is, misspelled words sound receive high Soundex score similarity.
similarity based upon Porter stemming algorithm (Porter, 1980),
removes suffixes strings root words compared similarity.
helps alleviate possible errors introduced prefix assumption introduced
Jaro-Winkler metric, since stems scored rather prefixes. Including
scores, scores vector becomes:
scores(post, attribute)=<Porter-Stemmer(post, attribute),
Soundex(post, attribute)>

Figure 5: full vector similarity scores used record linkage
Figure 5 shows full composition VRL , constituent similarity scores.
VRL constructed candidates, matching step performs
binary rescoring VRL help determine best match amongst candidates. rescoring helps determine best possible match post separating
557

fiMichelson & Knoblock

best candidate much possible. might candidates
similarly close values, one best match, rescoring emphasizes
best match downgrading close matches element values obvious non-matches, boosting difference score best
candidates elements.
rescore vectors candidate set C, rescoring method iterates
elements xi VRL C, VRL (s) contain maximum value xi map
xi 1, VRL (s) map xi 0. Mathematically, rescoring method is:
VRLj C, j = 0... |C|
fi
fi
fi
fi
xi VRLj , = 0... fiVRLj fi
(

f (xi , VRLj ) =

1, xi = max(xt VRLs , VRLs C, = i, = 0... |C|)
0, otherwise

example, suppose C contains 2 candidates, VRL1 VRL2 :
VRL1 = <{.999,...,1.2},...,{0.45,...,0.22}>
VRL2 = <{.888,...,0.0},...,{0.65,...,0.22}>
rescoring become:
VRL1 = <{1,...,1},...,{0,...,1}>
VRL2 = <{0,...,0},...,{1,...,1}>
rescoring, matching step passes VRL Support Vector Machine (SVM)
(Joachims, 1999) trained label matches non-matches. best match
candidate SVM classifies match, maximally positive score
decision function. one candidate share maximum score
decision function, thrown matches. enforces strict 1-1 mapping
posts members reference set. However, 1-n relationship captured
relaxing restriction. algorithm keeps either first candidate
maximal decision score, chooses one randomly set candidates
maximum decision score.
Although use SVMs paper differentiate matches non-matches,
algorithm strictly tied method. main characteristics learning
problem feature vectors sparse (because binary rescoring)
concepts dense (since many useful features may needed thus none
pruned feature selection). also tried use Nave Bayes classifier matching
task, monumentally overwhelmed number features number
training examples. Yet say methods deal sparse
feature vectors dense concepts, online logistic regression boosting, could
used place SVM.
match post found, attributes matching reference set member
added annotation post including values reference set attributes
tags reflect schema reference set. overall matching algorithm
shown Figure 6.
558

fiRelational Data Unstructured Data Sources

Figure 6: approach matching posts records reference set
addition providing standardized set values query posts, standardized values allow integration outside sources values standardized
canonical values. instance, want integrate car classifieds safety
ratings website, easily join sources across attribute values.
manner, approaching annotation record linkage problem, create relational
data unstructured ungrammatical data sources. However, aid extraction
attributes easily represented reference sets, perform information extraction
posts well.

3. Extracting Data Posts
Although record linkage step creates relational data posts,
still attributes would like extract post easily represented
reference sets, means record linkage step used attributes.
Examples attributes dates prices. Although many attributes
extracted using simple techniques, regular expressions, make
extraction annotation ever accurate using sophisticated information extraction.
motivate idea, consider Ford car model called 500. used regular
expressions, might extract 500 price car, would case.
However, try extract attributes, including model, would
extract 500 model correctly. Furthermore, might want extract actual
attributes post, are, extraction algorithm allows this.
perform extraction, algorithm infuses information extraction extra knowledge, rather relying possibly inconsistent characteristics. garner extra
559

fiMichelson & Knoblock

knowledge, approach exploits idea reference sets using attributes
matching reference set member basis identifying similar attributes post.
Then, algorithm label extracted values post schema
reference set, thus adding annotation based extracted values.
broad sense, algorithm two parts. First label token possible
attribute label junk ignored. tokens post labeled,
clean extracted labels. Figure 7 shows whole procedure graphically,
detail, using second post Table 1. steps shown figure
described detail below.

Figure 7: Extraction process attributes
begin extraction process, post broken tokens. Using first post
Table 1 example, set tokens becomes, {93, civic, 5speed,...}.
tokens scored attribute record reference set
deemed match.
score tokens, extraction process builds vector scores, VIE . Like VRL
vector matching step, VIE composed vectors represent similarities
token attributes reference set. However, composition
VIE slightly different VRL . contains comparison concatenation
attributes, vectors compose VIE different compose
VRL . Specifically, vectors form VIE called IE scores, similar
560

fiRelational Data Unstructured Data Sources

RL scores compose VRL , except contain token scores component, since
IE scores uses one token post time.
RL scores vector:
RL scores(post, attribute)=<token scores(post, attribute),
edit scores(post, attribute),
scores(post, attribute)>
becomes:
IE scores(token, attribute)=<edit scores(token, attribute),
scores(token, attribute)>
main difference VIE VRL VIE contains unique vector
contains user defined functions, regular expressions, capture attributes
easily represented reference sets, prices dates. attribute types
generally exhibit consistent characteristics allow extracted,
usually infeasible represent reference sets. makes traditional extraction methods
good choice attributes. vector called common scores types
characteristics used extract attributes common enough used
extraction.
Using first post Table 1, assume reference set match make Honda,
model Civic year 1993. means matching tuple would {Honda,
Civic, 1993}. match generates following VIE token civic post:
VIE =<common scores(civic),
IE scores(civic,Honda),
IE scores(civic,Civic),
IE scores(civic,1993)>
generally, given token, VIE looks like:
VIE =<common scores(token),
IE scores(token, attribute1 ),
IE scores(token, attribute2 )
...,
IE scores(token, attributen )>
VIE passed structured SVM (Tsochantaridis, Joachims, Hofmann,
& Altun, 2005; Tsochantaridis, Hofmann, Joachims, & Altun, 2004) trained give
attribute type label, make, model, price. Intuitively, similar attribute types
similar VIE vectors. makes generally high scores
make attribute reference set, small scores attributes. Further,
structured SVMs able infer extraction labels collectively, helps deciding
possible token labels. makes use structured SVMs ideal machine
learning method task. Note since VIE member cluster
winner takes all, binary rescoring.
Since many irrelevant tokens post annotated, SVM
learns VIE associate learned attribute type labeled
561

fiMichelson & Knoblock

junk, ignored. Without benefits reference set, recognizing junk
difficult characteristics text posts unreliable. example,
extraction relies solely capitalization token location, junk phrase Great Deal
might annotated attribute. Many traditional extraction systems work
domain ungrammatical unstructured text, addresses bibliographies,
assume token text must classified something, assumption
cannot made posts.
Nonetheless, possible junk token receive incorrect class label.
example, junk token enough matching letters, might labeled trim (since
trims may single letter two). leads noisy tokens within whole
extracted trim attribute. Therefore, labeling tokens individually gives approximation
data extracted.
extraction approach overcome problems generating noisy, labeled tokens
comparing whole extracted field analogue reference set attribute.
tokens post processed, whole attributes built compared corresponding attributes reference set. allows removal tokens introduce
noise extracted attribute.
removal noisy tokens extracted attribute starts generating two
baseline scores extracted attribute reference set attribute. One
Jaccard similarity, reflect token level similarity two attributes. However,
since many misspellings such, edit-distance based similarity metric,
Jaro-Winkler metric, also used. baselines demonstrate accurately system
extracted/classified tokens isolation.
Using first post Table 1 ongoing example, assume phrase civic (ri)
extracted model. might occur car model Civic Rx,
instance. isolation, token (ri) could Rx model. Comparing
extracted car model reference attribute Civic generates Jaccard similarity 0.5
Jaro-Winkler score 0.83. shown top Figure 8.
Next, cleaning method goes extracted attribute, removing one token
time calculating new Jaccard Jaro-Winkler similarities. new scores
higher baselines, token becomes removal candidate. tokens
processed way, removal candidate highest scores removed,
whole process repeated. scores derived using removed token become
new baseline compare against. process ends tokens
yield improved scores baselines.
Shown Iteration 1 Figure 8, cleaning method finds (ri) removal
candidate since removing token extracted car model yields Jaccard score
1.0 Jaro-Winkler score 1.0, higher baseline scores. Since
highest scores trying token iteration, removed
baseline scores update. Then, since none remaining tokens provide improved scores
(since none), process terminates, yielding accurate attribute value.
shown Iteration 2 Figure 8. Note process would keep iterating,
tokens removed improve scores baseline. pseudocode
algorithm shown Figure 9.
562

fiRelational Data Unstructured Data Sources

Figure 8: Improving extraction accuracy reference set attributes
Note, however, limit machine learning component extraction
algorithm SVMs. Instead, claim cases, reference sets aid extraction
general, test this, architecture replace SVM component
methods. example, extraction experiments replace SVM extractor
Conditional Random Field (CRF) (Lafferty, McCallum, & Pereira, 2001) extractor
uses VIE features.
Therefore, whole extraction process takes token text, creates VIE
passes machine-learning extractor generates label token.
field cleaned extracted attribute saved.

4. Results
Phoebus system built experimentally validate approach building relational
data unstructured ungrammatical data sources. Specifically, Phoebus tests
techniques accuracy record linkage extraction, incorporates
BSL algorithm learning using blocking schemes. experimental data, comes
three domains posts: hotels, comic books, cars.
data hotel domain contains attributes hotel name, hotel area, star
rating, price dates, extracted test extraction algorithm. data
comes Bidding Travel website9 forum users share successful
bids Priceline items airline tickets hotel rates. experimental data
limited postings hotel rates Sacramento, San Diego Pittsburgh,
compose data set 1125 posts, 1028 posts match reference
set. reference set comes Bidding Travel hotel guides, special
9. www.biddingfortravel.com

563

fiMichelson & Knoblock

Algorithm 3.1: CleanAttribute(E, R)
comment: Clean extracted attribute E using reference set attribute R
RemovalCandidates C null
JaroW inklerBaseline JaroWinkler(E, R)
JaccardBaseline Jaccard(E, R)
token E

X RemoveToken(t, E)




JaroW inklerXt JaroWinkler(X , R)






Xt Jaccard(X , R)

Jaccard

JaroW
inklerXt >JaroW inklerBaseline









Jaccard >Jaccard


Xt
Baseline

n



C C
(



C = null
return (E)
(
E RemoveMaxCandidate(C,E)
else
CleanAttribute(E, R)

Figure 9: Algorithm clean extracted attribute
posts listing hotels ever posted given area. special posts provide
hotel names, hotel areas star ratings, reference set attributes. Therefore,
3 attributes standardized values used, allowing us treat
posts relational data set. reference set contains 132 records.
experimental data comic domain comes posts items sale
eBay. generate data set, eBay searched keywords Incredible Hulk
Fantastic Four comic books section website. (This returned items
comics, tshirts sets comics limited searched
for, makes problem difficult.) returned records contain attributes
comic title, issue number, price, publisher, publication year description,
extracted. (Note: description word description commonly associated
comic book, 1st appearance Rhino.) total number posts data
set 776, 697 matches. comic domain reference set uses data
Comics Price Guide10 , lists Incredible Hulk Fantastic Four comics.
reference set attributes title, issue number, description, publisher contains
918 records.
cars data consists posts made Craigslist regarding cars sale. dataset
consists classifieds cars Los Angeles, San Francisco, Boston, New York, New
10. http://www.comicspriceguide.com/

564

fiRelational Data Unstructured Data Sources

Jersey Chicago. total 2,568 posts data set, post
contains make, model, year, trim price. reference set Cars domain comes
Edmunds11 car buying guide. data set extracted make, model,
year trim cars 1990 2005, resulting 20,076 records. 15,338
matches posts Craigslist cars Edmunds.
Unlike hotels comics domains, strict 1-1 relationship post
reference set enforced cars domain. described previously, Phoebus relaxed 1-1 relationship form 1-n relationship posts reference
set. Sometimes records contain enough attributes discriminate single best
reference member. instance, posts contain model year might match
couple reference set records would differ trim attribute,
make, model, year. Yet, still use make, model year accurately
extraction. So, case, mentioned previously, pick one matches. way,
exploit attributes reference set, since confidence
those.
experiments, posts domain split two folds, one training
one testing. usually called two-fold cross validation. However, many cases twofold cross validation results using 50% data training 50% testing.
believe much data label, especially data sets become large,
experiments instead focus using less training data. One set experiments uses 30%
posts training tests remaining 70%, second set experiments
uses 10% posts train, testing remaining 90%. believe training
small amounts data, 10%, important empirical procedure since real
world data sets large labeling 50% large data sets time consuming
unrealistic. fact, size Cars domain prevented us using 30% data
training, since machine learning algorithms could scale number training
tuples would generate. Cars domain run experiments training
10% data. experiments performed 10 times, average results
10 trials reported.
4.1 Record Linkage Results
subsection report record linkage results, broken separate discussions
blocking results matching results.
4.1.1 Blocking Results
order BSL algorithm learn blocking scheme, must provided methods
use compare attributes. domains experiments use two common
methods. first, call token, compares matching token
attributes. second method, ngram3, considers matching 3-grams
attributes.
important note comparison BSL blocking methods,
Canopies method (McCallum, Nigam, & Ungar, 2000) Bigram indexing (Baxter,
Christen, & Churches, 2003), slightly misaligned algorithms solve different
11. www.edmunds.com

565

fiMichelson & Knoblock

problems. Methods Bigram indexing techniques make process
blocking pass attribute efficient. goal BSL, however, select
attribute combinations used blocking whole, trying different attribute
method pairs. Nonetheless, contend important select right attribute
combinations, even using simple methods, use sophisticated methods,
without insight attributes might useful. test hypothesis, compare
BSL using token 3-gram methods Bigram indexing attributes.
equivalent forming disjunction attributes using Bigram indexing
method. chose Bigram indexing particular designed perform fuzzy
blocking seems necessary case noisy post data. stated previously (Baxter
et al., 2003), use threshold 0.3 Bigram indexing, since works best.
also compare BSL running disjunction attributes using simple token method
only. results, call blocking rule Disjunction. disjunction mirrors
idea picking simplest possible blocking method: namely using attributes
simple method.
stated previously, two goals blocking quantified Reduction Ratio
(RR) Pairs Completeness (PC). Table 5 shows values also
many candidates generated average entire test set, comparing three
different approaches. Table 5 also shows long took method learn rule
run rule. Lastly, column Time match shows long classifier needs
run given number candidates generated blocking scheme.
Table 6 shows example blocking schemes algorithm generated.
comparison attributes BSL selected attributes picked manually different
domains data structured reader pointed previous work
topic (Michelson & Knoblock, 2006).
results Table 5 validate idea important pick correct
attributes block (using simple methods) use sophisticated methods without
attention attributes. Comparing BSL rule Bigram results, combination
PC RR always better using BSL. Note although Cars domain Bigram
took significantly less time classifier due large RR,
PC 4%. case, Bigrams even covering 5% true matches.
Further, BSL results better using simplest method possible (the Disjuction), especially cases many records test upon. number
records scales up, becomes increasingly important gain good RR, maintaining
good PC value well. savings dramatically demonstrated Cars domain,
BSL outperformed Disjunction PC RR.
One surprising aspect results prevalent token method within
domains. expect ngram method would used almost exclusively since
many spelling mistakes within posts. However, case. hypothesize
learning algorithm uses token methods occur regularity
across posts common ngrams would since spelling mistakes might vary quite
differently across posts. suggests might regularity, terms
learn data, across posts initially surmised.
Another interesting result poor reduction ratio Comic domain. happens
rules contain disjunct finds common token within comic
566

fiRelational Data Unstructured Data Sources

Hotels (30%)
BSL
Disjunction
Bigrams
Hotels (10%)
BSL
Disjunction
Bigrams
Comics (30%)
BSL
Disjunction
Bigrams
Comics (10%)
BSL
Disjunction
Bigrams
Cars (10%)
BSL
Disjunction
Bigrams

RR

PC

# Cands

Time Learn (s)

Time Run (s)

Time match (s)

81.56
67.02
61.35

99.79
99.82
72.77

19,153
34,262
40,151

69.25
0
0

24.05
12.49
1.2

60.93
109.00
127.74

84.47
66.91
60.71

99.07
99.82
90.39

20,742
44,202
52,492

37.67
0
0

31.87
15.676
1.57

65.99
140.62
167.00

42.97
37.39
36.72

99.75
100.00
69.20

284,283
312,078
315,453

85.59
0
0

36.66
45.77
102.23

834.94
916.57
926.48

42.97
37.33
36.75

99.74
100.00
88.41

365,454
401,541
405,283

34.26
0
0

35.65
52.183
131.34

1,073.34
1,179.32
1,190.31

88.48
87.92
97.11

92.23
89.90
4.31

5,343,424
5,603,146
1,805,275

465.85
0
0

805.36
343.22
996.45

25,114.09
26.334.79
8,484.79

Table 5: Blocking results using BSL algorithm (amount data used training shown
parentheses).

Hotels Domain (30%)
({hotel area,token} {hotel name,token} {star rating, token}) ({hotel name, ngram3})
Hotels Domain (10%)
({hotel area,token} {hotel name,token}) ({hotel name,ngram3})
Comic Domain (30%)
({title, token})
Comic Domain (10%)
({title, token}) ({issue number,token} {publisher,token} {title,ngram3})
Cars Domain (10%)
({make,token}) ({model,ngram3}) ({year,token} {make,ngram3})
Table 6: example blocking schemes learned domains.

567

fiMichelson & Knoblock

title. rule produces poor reduction ratio value attribute
across almost reference set records. say,
unique values BSL algorithm use blocking, reduction ratio small.
domain, two values comic title attribute, Fantastic Four
Incredible Hulk. makes sense blocking done using title attribute only,
reduction half, since blocking value Fantastic Four gets rid
Incredible Hulk comics. points interesting limitation BSL algorithm.
many distinct values different attribute method pairs BSL
use learn from, lack values cripples performance reduction
ratio. Intuitively though, makes sense, since hard distinguish good candidate
matches bad candidate matches share attribute values.
Another result worth mentioning Hotels domain get lower RR
PC use less training data. happens BSL algorithm
runs examples cover, last examples introduce new
disjunct produces lot candidates, covering true positives,
would cause RR decrease, keeping PC high rate.
fact happens case. One way curb behavior would set
sort stopping threshold BSL, said, maximizing PC
important thing, choose this. want BSL cover many true positives
can, even means losing bit reduction.
fact, next test notion explicitly. set threshold SCA
95% training examples covered, algorithm stops returns learned
blocking scheme. helps avoid situation BSL learns general conjunction, solely cover last remaining training examples. happens, BSL
might end lowering RR, expense covering last training examples,
rule learned cover last examples overly general returns many
candidate matches.
Domain
Hotels Domain
Thresh (30%)
95% Thresh (30%)
Comic Domain
Thresh (30%)
95% Thresh (30%)
Cars Domain
Thresh (10%)
95% Thresh (10%)

Record Linkage
F-Measure

RR

PC

90.63
90.63

81.56
87.63

99.79
97.66

91.30
91.47

42.97
42.97

99.75
99.69

77.04
67.14

88.48
92.67

92.23
83.95

Table 7: comparison BSL covering training examples, covering 95%
training examples

568

fiRelational Data Unstructured Data Sources

Table 7 shows use threshold Hotels Cars domain see
statistically significant drop Pairs Completeness statistically significant increase
Reduction Ratio.12 expected behavior since threshold causes BSL kick
SCA cover last training examples, turn allows BSL
retain rule high RR, lower PC. However, look record linkage
results, see threshold fact large effect.13 Although
statistically significant difference F-measure record linkage Hotels domain,
difference Cars domain dramatic. use threshold, candidates
discovered rule generated using threshold effect 10% final
F-measure match results.14 Therefore, since F-measure results differ much,
conclude worthwhile maximize PC learning rules BSL, even
RR may decrease. say, even presence noise, turn may lead
overly generic blocking schemes, BSL try maximize true matches covers,
avoiding even difficult cases cover may affect matching results.
see Table 7, especially true Cars domain matching much
difficult Hotels domain.
Interestingly, Comic domain see statistically significant difference
RR PC. across trials almost always learn rule
whether use threshold not, rule covers enough training examples
threshold hit. Further, statistically significant change F-measure
record linkage results domain. expected since BSL would generate
candidate matches, whether uses threshold not, since cases almost always
learns blocking rules.
results using BSL encouraging show algorithm also works
blocking matching unstructured ungrammatical text relational data
source. means algorithm works special case too, case
traditional record linkage matching one structured source another.
means overall algorithm semantic annotation much scalable
using fewer candidate matches previous work (Michelson & Knoblock, 2005).
4.1.2 Matching Results
Since alignment approach hinges leveraging reference sets, becomes necessary
show matching step performs well. measure accuracy, experiments employ
usual record linkage statistics:
P recision =
Recall =

#CorrectM atches
#T otalM atchesM ade
#CorrectM atches
#P ossibleM atches

12. Bold means statistically significant using two-tailed t-test set 0.05
13. Please see subsection 4.1.2 description record linkage experiments results.
14. Much difference attributed non-threshold version algorithm learning final
predicate includes make attribute itself, version threshold learn.
Since make attribute value covers many records, generates many candidates results
increasing PC reducing RR.

569

fiMichelson & Knoblock

F easure =

2 P recision Recall
P recison + Recall

record linkage approach article compared WHIRL (Cohen, 2000).
WHIRL performs record linkage performing soft-joins using vector-based cosine similarities attributes. record linkage systems require decomposed attributes
matching, case posts. WHIRL serves benchmark
requirement. mirror alignment task Phoebus, experiment
supplies WHIRL two tables: test set posts (either 70% 90% posts)
reference set attributes concatenated approximate record level match.
concatenation also used matching individual attribute,
obvious combine matching attributes construct whole matching reference
set member.
perform record linkage, WHIRL soft-joins across tables, produces
list matches, ordered descending similarity score. post matches
join, reference set member(s) highest similarity score(s) called match.
Cars domain matches 1-N, means 1 match reference
set exploited later information extraction step. mirror idea, number
possible matches 1-N domain counted number posts match
reference set, rather reference set members match. Also,
means add single match total number correct matches given
post, rather correct matches, since one matters. done
WHIRL Phoebus, accurately reflects well algorithm would perform
processing step information extraction step.
record linkage results Phoebus WHIRL shown Table 8. Note
amount training data domain shown parentheses. results
statistically significant using two-tailed paired t-test =0.05, except
precision WHIRL Phoebus Cars domain, precision
Phoebus trained 10% 30% training data Comic domain.
Phoebus outperforms WHIRL uses many similarity types distinguish
matches. Also, since Phoebus uses record level attribute level similarities,
able distinguish records differ discriminative attributes.
especially apparent Cars domain. First, results indicate difficulty
matching car posts large reference set. largest experimental domain yet
used problem, encouraging well approach outperforms baseline. also interesting results suggest techniques equally accurate
terms precision (in fact, statistically significant difference
sense) Phoebus able retrieve many relevant matches. means
Phoebus capture rich features predict matches WHIRLs cosine similarity alone. expect behavior Phoebus notion field token
level similarity, using many different similarity measures. justifies use many
similarity types field record level information, since goal find many
matches can.
also encouraging using 10% data labeling, Phoebus able
perform almost well using 30% data training. Since amount data
Web vast, label 10% data get comparative results preferable
570

fiRelational Data Unstructured Data Sources

Hotel
Phoebus (30%)
Phoebus (10%)
WHIRL
Comic
Phoebus (30%)
Phoebus (10%)
WHIRL
Cars
Phoebus (10%)
WHIRL

Precision

Recall

F-measure

87.70
87.85
83.53

93.78
92.46
83.61

90.63
90.09
83.13

87.49
85.35
73.89

95.46
93.18
81.63

91.30
89.09
77.57

69.98
70.43

85.68
63.36

77.04
66.71

Table 8: Record linkage results

cost labeling data great. Especially since clean annotation, hence
relational data, comes correctly matching posts reference set,
label much data important want technique widely applicable.
fact, faced practical issue Cars domain unable
use 30% training since machine learning method would scale number
candidates generated much training data. So, fact report good
results 10% training data allows us extend work much larger Cars
domain.
method performs well outperforms WHIRL, results above,
clear whether use many string metrics, inclusion attributes
concatenation SVM provides advantage. test advantages
piece, ran several experiments isolating ideas.
First, ran Phoebus matching concatenation attributes
reference set, rather concatenation attributes individually. Earlier,
stated use concatenation mirror idea record level similarity also
use attribute mirror field level similarity. hypothesis cases,
post match different reference set records record level score (using
concatenation), matching different attributes. removing
individual attributes leaving concatenation matching, test
concatenation influences matching isolation. Table 9 shows results
different domains.
Cars Comic domains see improvement F-measure, indicating
using attributes concatenation much better matching using
concatenation alone. supports notion also need method capture
significance matching individual attributes since attributes better indicators
matching others. also interesting note domains, WHIRL
better job machine learning using concatenation, even though WHIRL
571

fiMichelson & Knoblock

Hotels
Phoebus (30%)
Concatenation
WHIRL
Comic
Phoebus (30%)
Concatenation
WHIRL
Cars
Phoebus (10%)
Concatenation
WHIRL

Precision

Recall

F-Measure

87.70
88.49
83.61

93.78
93.19
83.53

90.63
90.78
83.13

87.49
61.81
73.89

95.46
46.55
81.63

91.30
51.31
77.57

69.98
47.94
70.43

85.68
58.73
63.36

77.04
52.79
66.71

Table 9: Matching using concatenation

also uses concatenation attributes. WHIRL uses informationretrieval-style matching find best match, machine learning technique tries
learn characteristics best match. Clearly, difficult learn
characteristics are.
Hotels domain, find statistically significant difference F-measure
using concatenation alone. means concatenation sufficient determine
matches, need individual fields play role. specifically,
hotel name area seem important attributes matching
including part concatenation, concatenation still distinguishable enough
records determine matches. Since two three domains see
huge improvement, never lose F-measure, using concatenation
individual attributes valid matching. Also, since two domains concatenation
alone worse WHIRL, conclude part reason Phoebus outperform
WHIRL use individual attributes matching.
next experiment tests important include string metrics
feature vector matching. test idea, compare using metrics using
one, Jensen-Shannon distance. choose Jensen-Shannon distance
outperformed TF/IDF even soft TF/IDF (one accounts fuzzy token
matches) task selecting right reference sets given set posts (Michelson
& Knoblock, 2007). results shown Table 10.
Table 10 shows, using metrics yielded statistically significant, large improvement F-measure Comic Cars domains. means
string metrics, edit distances, capturing similarities JensenShannon distance alone not. Interestingly, domains, using Phoebus
Jensen-Shannon distance dominate WHIRLs performance. Therefore,
results Table 10 Table 9 demonstrate Phoebus benefits combination
572

fiRelational Data Unstructured Data Sources

Hotels
Phoebus (30%)
Jensen-Shannon
WHIRL
Comic
Phoebus (30%)
Jensen-Shannon
WHIRL
Cars
Phoebus (10%)
Jensen-Shannon
WHIRL

Precision

Recall

F-Measure

87.70
89.65
83.61

93.78
92.28
83.53

90.63
90.94
83.13

87.49
65.36
73.89

95.46
69.96
81.63

91.30
67.58
77.57

69.98
72.87
70.43

85.68
59.43
63.36

77.04
67.94
66.71

Table 10: Using string metrics versus using Jensen-Shannon distance

many, varied similarity metrics along use individual attributes field level
similarities, aspects contribute Phoebus outperforming WHIRL.
case Hotels data, statistically significant difference
matching results, case metrics provide relevant information
matching. Therefore, matches missed Jensen-Shannon method also
missed include metrics. Hence, either missed matches
difficult discover, string metric method yet capture
similarity. example, post token DT reference set record
match hotel area Downtown, abbreviation metric could capture
relationship. However, Phoebus include abbreviation similarity measure.
Since none techniques isolation consistently outperforms WHIRL, conclude
Phoebus outperforms WHIRL combines multiple string metrics, uses
individual attributes concatenation, and, stated Section 2.2, SVM classifier
well suited record linkage task. results also justify inclusion many
metrics individual attributes, along use SVM classifier.
last matching experiment justifies binary rescoring mechanism. Table 11 shows
results performing binary rescoring record linkage versus performing
binary recoring. hypothesize earlier paper binary rescoring allow
classifier accurately make match decisions rescoring separates
best candidate much possible. Table 11 shows case, across
domains perform binary rescoring gain statistically significant amount
F-measure. shows record linkage easily able identify
true matches possible candidates difference record linkage
algorithm use binary rescoring.
573

fiMichelson & Knoblock

Hotels
Phoebus (30%)
Binary Rescoring
Phoebus (10%)
Binary Rescoring
Comic
Phoebus (30%)
Binary Rescoring
Phoebus (10%)
Binary Rescoring
Cars
Phoebus (10%)
Binary Rescoring

Precision

Recall

F-Measure

87.70
75.44
87.85
73.49

93.78
81.82
92.46
78.40

90.63
78.50
90.09
75.86

87.49
84.87
85.35
81.52

95.46
89.91
93.18
88.26

91.30
87.31
89.09
84.75

69.98
39.78

85.68
48.77

77.04
43.82

Table 11: Record linkage results without binary rescoring

4.2 Extraction Results
section presents results experimentally validate approach extracting
actual attributes embedded within post. also compare approach two
information extraction methods rely structure grammar posts.
First, experiments compare Phoebus baseline Conditional Random Field
(CRF) (Lafferty et al., 2001) extractor. Conditional Random Field probabilistic
model label segment data. labeling tasks, Part-of-Speech tagging, CRFs outperform Hidden Markov Models Maximum-Entropy Markov Models.
Therefore, representing state-of-the-art probabilistic graphical model, present
strong comparison approach extraction. CRFs also used effectively
information extraction. instance, CRFs used combine information extraction coreference resolution good results (Wellner, McCallum, Peng, & Hay, 2004).
experiments use Simple Tagger implementation CRFs MALLET
(McCallum, 2002) suite text processing tools.
Further, stated Section 3 Extraction, also created version Phoebus
uses CRFs, call PhoebusCRF. PhoebusCRF uses extraction features
(VIE ) Phoebus using SVM, common score regular expressions
string similarity metrics. include PhoebusCRF show extraction general
benefit reference set matching.
Second, experiments compare Phoebus Natural Language Processing (NLP) based
extraction techniques. Since posts ungrammatical unreliable lexical characteristics, NLP based systems expected well type data.
Amilcare system (Ciravegna, 2001), uses shallow NLP extraction,
shown outperform symbolic systems extraction tasks, use Amilcare
system compare against. Since Amilcare exploit gazetteers extra
574

fiRelational Data Unstructured Data Sources

information, experiments Amilcare receives reference data gazetteer aid
extraction. Simple Tagger Amilcare used default settings.
Lastly, compare Phoebus trained using 30% data training Phoebus
trained using 10% data. (We PhoebusCRF well.) experimental
results, amount training data put parentheses.
One component extraction vector VIE vector common scores, includes
user defined functions, regular expressions. Since domain specific
functions used algorithm, common scores domain must specified.
Hotels domain, common scores includes functions matchPriceRegex
matchDateRegex. functions gives positive score token matches price
date regular expression, 0 otherwise. Comic domain, common scores contains
functions matchPriceRegex matchYearRegex, also give positive scores
token matches regular expression. Cars domain, common scores uses function
matchPriceRegex (since year attribute reference set, use common
score capture form).
cars data set, posts labeled training testing
extraction. domain, labeled 702 posts extraction, use
training testing extraction algorithm. Note, however, Phoebus perform
extraction posts, able report results those. fact,
running demo Phoebus, Cars domain live.15
extraction results presented using Precision, Recall F-Measure. Note
extraction results field level results. means extraction counted
correct tokens compromise field post correctly labeled.
Although much stricter rubric correctness, accurately models useful
extraction system would be. Tables 12, 13 14 show results correctly labeling
tokens within posts correct attribute label Hotel, Comic Cars
domains, respectively. Attributes italics attributes exist reference set.
column Freq shows average number fields test set associated
label. Also, observe * means results highest Phoebus score (Phoebus
PhoebusCRF) highest baseline (Amilcare Simple Tagger CRF) F-Measure
statistically significant using two-tailed paired t-test =0.05.
Phoebus PhoebusCRF outperform systems almost attributes (13
16), shown Table 15. fact, one attribute baseline system
best: using Amilcare extract Date attribute Hotels domain.
attribute, Phoebus PhoebusCRF use common-score regular-expression
main identifying feature. Since regular expression user supplied, propose
better regular expression could make Phoebus/PhoebusCRF extract dates even
accurately, overcoming baseline. Since systems perform well using reference
set data aid extraction, results show using reference sets greatly aid
extraction. especially evident compare PhoebusCRF Simple Tagger
CRF, since difference two extraction methods reference set attribute
similarity scores common scores.
15. http://www.isi.edu/integration/Phoebus/demos.html demo uses extraction model trained
702 labeled extraction examples, running live months writing
article.

575

fiMichelson & Knoblock

Area

Date

Name

Price

Star

Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF(30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)

Hotel
Recall
83.73
77.80
85.13
80.71
78.62
64.78
85.41
82.13
87.20
84.39
63.60
86.18
77.27
75.59
85.70
81.46
74.43
58.96
93.06
93.12
92.56
90.34
71.68
88.04
97.39
96.94
96.83
96.17
97.16
95.58

Precision
84.76
83.58
86.93
83.38
79.38
71.59
87.02
83.06
87.11
84.48
63.25
94.10
75.18
74.25
85.07
81.69
84.86
67.44
98.38
98.46
94.90
92.60
73.45
91.10
97.01
96.90
98.06
96.74
96.55
97.35

F-Measure
84.23
80.52
86.02
82.01
79.00
68.01
86.21
82.59
87.15
84.43
63.42
89.97
76.21
74.92
85.38
81.57
79.29
62.91
95.65
95.72
93.71
91.46
72.55
89.54
97.20
96.92
97.44
96.45
96.85
96.46

Frequency
~580

~700

~750

~720

~730

Table 12: Field level extraction results: Hotels domain

576

fiRelational Data Unstructured Data Sources

Descript.

Issue

Price

Publisher

Title

Year

Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)

Comic
Recall
32.43
30.16
26.02
15.45
32.30
8.00
83.39
80.90
87.77
83.01
78.31
77.66
68.09
39.84
51.06
29.09
44.24
41.21
100.00
99.85
77.91
53.22
78.13
63.75
89.34
89.37
92.93
90.64
93.57
89.88
78.44
77.50
76.24
54.63
39.93
77.05

Precision
30.71
27.15
33.03
26.83
34.75
52.55
83.65
82.17
88.70
84.68
77.81
89.11
90.00
60.00
85.34
55.40
84.44
66.67
85.38
83.89
88.30
87.29
88.52
90.48
89.34
89.37
93.70
92.13
92.79
95.65
97.69
97.35
93.46
85.07
72.89
85.67

F-Measure
31.51
28.52
28.95
18.54
33.43*
13.78
83.52
81.52
88.23
83.84
78.05
82.98
77.39*
46.91
61.16
35.71
55.77
50.93
92.09
91.18
82.50
64.26
82.72
74.75
89.34
89.37
93.31*
91.37
93.18
92.65
86.99
86.28
83.80
66.14
51.54
81.04

Frequency
~90

~510

~15

~60

~540

~100

Table 13: Field level extraction results: Comic domain.

577

fiMichelson & Knoblock

Make

Model

Price

Trim

Year

Phoebus (10%)
PhoebusCRF (10%)
Simple Tagger CRF (10%)
Amilcare (10%)
Phoebus (10%)
PhoebusCRF (10%)
Simple Tagger CRF (10%)
Amilcare (10%)
Phoebus (10%)
PhoebusCRF (10%)
Simple Tagger CRF (10%)
Amilcare (10%)
Phoebus (10%)
PhoebusCRF (10%)
Simple Tagger CRF (10%)
Amilcare (10%)
Phoebus (10%)
PhoebusCRF (10%)
Simple Tagger CRF (10%)
Amilcare (10%)

Cars
Recall
98.21
90.73
85.68
97.58
92.61
84.58
78.76
78.44
97.17
93.59
83.66
90.06
63.11
55.61
55.94
27.21
88.48
85.54
91.12
86.32

Precision
99.93
96.71
95.69
91.76
96.67
94.10
91.21
84.31
95.91
92.59
98.16
91.27
70.15
64.95
66.49
53.99
98.23
96.44
76.78
91.92

F-Measure
99.06
93.36
90.39
94.57
94.59
88.79
84.52
81.24
96.53
93.09
90.33
90.28
66.43
59.28
60.57
35.94
93.08
90.59
83.31
88.97

Frequency
~580

~620

~580

~375

~600

Table 14: Field level extraction results: Cars domain.

Domain
Hotel
Comic
Cars


Phoebus
1
2
5
8

Num. Max. F-Measures
PhoebusCRF Amilcare Simple Tagger
3
1
0
1
0
0
0
0
0
4
1
0

Total Attributes
5
6
5
16

Table 15: Summary results extraction showing number times system
statistically significant highest F-Measure attribute.

578

fiRelational Data Unstructured Data Sources

Phoebus performs especially well Cars domain, best system
attributes. One interesting thing note result record
linkage results spectacular Cars domain, good enough yield
high extraction results. times system picking best
match reference set, still picking one close enough
reference set attributes useful extraction. trim extraction
results lowest, often attribute determines match
non-match. record linkage step likely selects car close, differs trim,
match incorrect trim likely extracted correctly,
rest attributes extracted using reference set member.
couple interesting notes come results. One intriguing
aspects results allow us estimate level structure different
attributes within domain. Since CRFs rely structure tokens within
post structured SVM method, hypothesize domains
structure, PhoebusCRF perform best domains least structure,
Phoebus perform best. Table 15 shows case. PhoebusCRF dominates
Hotels domain, where, example, many posts structure star rating
comes hotel name. using structure allow extractor get
hotel name accurately using information. Therefore see
overall structure within Hotels domain PhoebusCRF method
performs best, Phoebus. Contrast Cars domain, highly
unstructured, Phoebus performs best across attributes. domain
many missing tokens order attributes varied. Comic domain
varied attributes exhibit structure not, Table
15 shows, cases Phoebus PhoebusCRF dominates. However, although
Hotels data exhibits structure, important aspect research using
Phoebus allows one perform extraction without assuming structure data.
Also, result worth noting price attribute Comic domain bit
misleading. fact, none systems statistically significant respect
prices extract F-Measures
systems.
Another aspect came light statistical significance generalization
algorithm. Hotels Comic domains, able use 30% 10%
data training, many cases statistically significant difference
F-Measures extracted attributes using Phoebus. Hotels domain
name, area date statistically significant F-Measures training 30%
10% data, Comic domain difference F-Measure
issue description attributes significant (though description borderline).
means 11 attributes domains, roughly half insignificant.
Therefore little difference extraction whether use 10% data training
30%, extraction algorithm generalizes well. important since labeling
data extraction time consuming expensive.
One interesting result note except comic price (which insignificant
systems) hotel date (which close), Phoebus, using either 10% 30%
training data, outperformed systems attributes included
579

fiMichelson & Knoblock

reference set. lends credibility claim earlier section training
system extract attributes, even reference set,
accurately extract attributes reference set training system
identify something not.
overall performance Phoebus validates approach semantic annotation.
infusing information extraction outside knowledge reference sets, Phoebus
able perform well across three different domains, representative different type
source posts: auction sites, Internet classifieds forum/bulletin boards.

5. Discussion
goal research produce relational data unstructured ungrammatical
data sources accurately queried integrated sources.
representing attributes embedded within post standardized values
reference set, support structural queries integration. instance,
perform aggregate queries treat data source relational database
now. Furthermore, standardized values performing joins across data sources,
key integration multiple sources. standardized values also aid cases
post actually contain attribute. instance, Table 1, two
listings include make Honda. However, matched reference set,
contain standardized value attribute used querying
integrating posts. especially powerful since posts never explicitly stated
attribute values. reference set attributes also provide solution cases
extraction extremely difficult. example, none systems extracted
description attribute Comic domain well. However, one instead considers
description attribute reference set, quantified record linkage
results Comic domain, yields improvement 50% F-Measure
identifying description post.
may seem using reference set attributes annotation enough since
values already cleaned, extraction unnecessary. However, case.
one thing, one may want see actual values entered different attributes.
instance, user might want discover common spelling mistake abbreviation
attribute. Also, cases extraction results outperform record
linkage results. happens even post matched incorrect member
reference set, incorrect member likely close correct match,
used correctly extract much information. strong example this,
consider Cars domain. F-measure record linkage results good
extraction results domain. means matches chosen
probably incorrect differ correct match something small.
example, true match could trim 2 Door incorrectly chosen
match might trim 4 Door, would still enough information,
rest trim tokens, year, make model correctly extract
different attributes post itself. performing extraction values
post itself, overcome mistakes record linkage step still
exploit information incorrectly chosen reference set member.
580

fiRelational Data Unstructured Data Sources

Extraction attributes also helps system classify (and ignore) junk
tokens. Labeling something junk much descriptive labeled junk
many possible class labels could share lexical characteristics. helps improve
extraction results items reference set, prices dates.
topic reference sets, important note algorithm tied
single reference set. algorithm extends include multiple reference sets iterating
process reference set used.
Consider following two cases. First, suppose user wants extract conference
names cities individual lists each. approach confined using
one reference set, would require constructing reference set contains power set
cities crossed conference names. approach would scale many attributes
distinct sources. However, lists used two reference sets, one
attribute, algorithm run conference name data,
reference set cities. iterative exploitation reference sets allows n reference
set attributes added without combinatorial explosion.
next interesting case post contains one attribute.
example, user needs extract two cities post. one reference set used,
includes cross product cities. However, using single reference set city
names done slightly modifying algorithm. new algorithm makes first
pass city reference set. pass, record linkage match either
one cities matches best, tie them. case tie, choose
first match. Using reference city, system extract city post,
remove post. system simply runs process again,
catch second city, using same, single reference set. could repeated many
times needed.
One issue arises reference sets discrepancy users knowledge
domain experts generally create reference sets. Cars domain,
instance, users interchangeably use attribute values hatchback, liftback,
wagon. reference set never includes term liftback suggests synonym
hatchback used common speech, Edmunds automobile jargon. term
wagon used Edmunds, used cars users describe
hatchbacks. implies slight difference meaning two, according
reference set authors.
Two issues arise discrepancies. first users interchanging words
cause problems extraction record linkage,
overcome incorporating sort thesaurus algorithm. record linkage,
thesaurus could expand certain attribute values used matching, example including
hatchback liftback reference set attribute includes term wagon.
However, subtle issues here. mostly case hatchback
called wagon happen wagon called hatchback. frequency
replacement must taken consideration errant matches created.
automate line future research. issue arises trusting
correctness Edmunds source. assume Edmunds right define one car
wagon different meaning classifying hatchback. fact,
581

fiMichelson & Knoblock

Edmunds classifies Mazda Protege5 wagon, Kelly Blue Book16 classifies
hatchback. seems invalidate idea wagon different meaning
hatchback. appear simple synonyms, would remain unknown
without outside knowledge Kelly Blue Book. generally, one assumes
reference set correct set standardized values, absolute truth.
meaningful reference sets constructed agreed-upon
ontologies Semantic Web. instance, reference set derived ontology
cars created biggest automotive businesses alleviate many
issues meaning, thesaurus scheme could work discrepancies introduced
users, rather reference sets.

6. Related Work
research driven principal cost annotating documents
Semantic Web free, is, automatic invisible users (Hendler, 2001).
Many researchers followed path, attempting automatically mark documents
Semantic Web, proposed (Cimiano, Handschuh, & Staab, 2004; Dingli,
Ciravegna, & Wilks, 2003; Handschuh, Staab, & Ciravegna, 2002; Vargas-Vera, Motta,
Domingue, Lanzoni, Stutt, & Ciravegna, 2002). However, systems rely lexical
information, part-of-speech tagging shallow Natural Language Processing
extraction/annotation (e.g., Amilcare, Ciravegna, 2001). option
data ungrammatical, like post data. similar vein, systems
ADEL (Lerman, Gazen, Minton, & Knoblock, 2004) rely structure identify
annotate records Web pages. Again, failure posts exhibit structure
makes approach inappropriate. So, fair amount work automatic
labeling, little emphasis techniques could label text unstructured
ungrammatical.
Although idea record linkage new (Fellegi & Sunter, 1969) well studied
even (Bilenko & Mooney, 2003) current research focuses matching one set
records another set records based decomposed attributes. little work
matching data sets one record single string composed data sets
attributes match on, case posts reference sets. WHIRL system
(Cohen, 2000) allows record linkage without decomposed attributes, shown
Section 4.1 Phoebus outperforms WHIRL, since WHIRL relies solely vector-based
cosine similarity attributes, Phoebus exploits larger set features
represent field record level similarity. note interest EROCS system
(Chakaravarthy, Gupta, Roy, & Mohania, 2006) authors tackle problem
linking full text documents relational databases. technique involves filtering
non-nouns text, finding matches database.
intriguing approach; interesting future work would involve performing similar filtering
larger documents applying Phoebus algorithm match remaining nouns
reference sets.
Using reference sets attributes normalized values similar idea data
cleaning. However, data cleaning algorithms assume tuple-to-tuple transformations
16. www.kbb.com

582

fiRelational Data Unstructured Data Sources

(Lee et al., 1999; Chaudhuri et al., 2003). is, function maps attributes
one tuple attributes another. approach would work ungrammatical
unstructured data, attributes embedded within post, maps
set attributes reference set.
Although work describes technique information extraction, many methods,
Conditional Random Fields (CRF), assume least structure extracted
attributes extraction. extraction experiments show, Phoebus outperforms methods, Simple Tagger implementation Conditional Random
Fields (McCallum, 2002). IE approaches, Datamold (Borkar, Deshmukh, &
Sarawagi, 2001) CRAM (Agichtein & Ganti, 2004), segment whole records (like bibliographies) attributes, little structural assumption. fact, CRAM even uses
reference sets aid extraction. However, systems require every token
record receive label, possible posts filled irrelevant, junk
tokens. Along lines CRAM Datamold, work Bellare McCallum (2007)
uses reference set train CRF extract data, similar PhoebusCRF
implementation. However, two differences PhoebusCRF work
(Bellare & McCallum, 2007). First, work Bellare McCallum (2007) mentions
reference set records matched using simple heuristics, unclear
done. work, matching done explicitly accurately record linkage. Second, work uses records reference set label tokens training
extraction module, PhoebusCRF uses actual values matching reference
set record produce useful features extraction annotation.
Another IE approach similar performs named entity recognition using SemiCRFs dictionary component (Cohen & Sarawagi, 2004), functions like
reference set. However, work dictionaries defined lists single attribute
entities, finding entity dictionary look-up task. reference sets
relational data, finding match becomes record linkage task. Further, work
Semi-CRFs (Cohen & Sarawagi, 2004) focuses task labeling segments tokens
uniform label, especially useful named entity recognition. case
posts, however, Phoebus needs relax restriction cases
segments interrupted, case hotel name area middle
hotel name segment. So, unlike work, Phoebus makes assumptions
structure posts. Recently, Semi-CRFs extended use database records
task integrating unstructured data relational databases (Mansuri & Sarawagi, 2006).
work similar links unstructured data, paper citations,
relational databases, reference sets authors venues. difference
view record linkage task, namely finding right reference set tuple match.
paper, even though use matches database aid extraction, view
linkage task extraction procedure followed matching task. Lastly,
first consider structured SVMs information extraction. Previous work used
structured SVMs perform Named Entity Recognition (Tsochantaridis et al., 2005)
extraction task use reference sets.
method aiding information extraction outside information (in form
reference sets) similar work ontology-based information extraction (Embley,
Campbell, Jiang, Liddle, Ng, Quass, & Smith, 1999). Later versions work even talk
583

fiMichelson & Knoblock

using ontology-based information extraction means semantically annotate unstructured data car classifieds (Ding, Embley, & Liddle, 2006). However, contrast
work, information extraction performed keyword-lookup ontology
along structural contextual rules aid labeling. ontology contains
keyword misspellings abbreviations, look-up performed presence noisy data. believe ontology-based extraction approach less scalable
record linkage type matching task creating maintaining ontology requires
extensive data engineering order encompass possible common spelling mistakes
abbreviations. Further, new data added ontology, additional data engineering
must performed. work, simply add new tuples reference set. Lastly,
contrast work, ontology based work assumes contextual structural rules
apply, making assumption data extract from. work, make
assumptions structure text extracting from.
Yet another interesting approach information extraction using ontologies Textpresso system extracts data biological text (Muller & Sternberg, 2004).
system uses regular expression based keyword look-up label tokens text based
ontology. tokens labeled, Textpresso perform fact extraction
extracting sequences labeled tokens fit particular pattern, gene-allele
reference associations. Although system uses reference set extraction,
differs keyword look-up lexicon.
recent work learning efficient blocking schemes Bilenko et al., (2006) developed
system learning disjunctive normal form blocking schemes. However, learn
schemes using graphical set covering algorithm, use version Sequential
Covering Algorithm (SCA). also similarities BSL algorithm work
mining association rules transaction data (Agrawal, Imielinski, & Swami, 1993).
algorithms discover propositional rules. Further, algorithms use multiple passes
data set discover rules. However despite similarities, techniques
really solve different problems. BSL generates set candidate matches minimal
number false positives. this, BSL learns conjunctions maximally specific
(eliminating many false positives) unions together single disjunctive rule (to
cover different true positives). Since conjunctions maximally specific, BSL uses
SCA underneath, learns rules depth-first, general specific manner (Mitchell,
1997). hand, work mining association rules (Agrawal et al., 1993) looks
actual patterns data represent internal relationships. may
many relationships data could discovered, approach covers
data breadth-first fashion, selecting set rules iteration extending
appending new possible item.

7. Conclusion
article presents algorithm semantically annotating text ungrammatical
unstructured. Unstructured, ungrammatical sources contain much information,
cannot support structured queries. technique allows informative use
sources. Using approach, eBay agents could monitor auctions looking best
deals, user could find average price four-star hotel San Diego. semantic
584

fiRelational Data Unstructured Data Sources

annotation necessary society transitions Semantic Web, information
requires annotation useful agents, users unwilling extra work
provide required annotation.
future, technique could link mediator framework (Thakkar, Ambite, &
Knoblock, 2004) automatically acquiring reference sets. similar automatically
incorporating secondary sources record linkage (Michalowski, Thakkar, & Knoblock,
2005). automatic formulation queries retrieve correct domain reference set
direction future research. mediator framework place, Phoebus could
incorporate many reference sets needed full coverage possible attribute values
attribute types.
Unsupervised approaches record linkage extraction also topics future research. including unsupervised record linkage extraction mediator component, approach would entirely self-contained, making semantic annotation posts
automatic process. Also, current implementation gives one class label per
token. Ideally Phoebus would give token possible labels, remove extraneous tokens systems cleans attributes, described Section 3.
disambiguation lead much higher accuracy extraction.
Future work could investigate inclusion thesauri terms attributes,
frequency replacement terms taken consideration. Also, exploring technologies automatically construct reference sets (and eventually thesauri)
numerous ontologies Semantic Web intriguing research path.
long term goal annotation extraction unstructured, ungrammatical
sources involves automating entire process. record linkage extraction methods
could become unsupervised, approach could automatically generate incorporate reference sets, apply automatically annotate data source.
would ideal approach making Semantic Web useful user
involvement.

Acknowledgments
research based upon work supported part National Science Foundation award number IIS-0324955, part Air Force Office Scientific Research
grant number FA9550-07-1-0416, part Defense Advanced Research Projects
Agency (DARPA), Department Interior, NBC, Acquisition Services Division, Contract No. NBCHD030010.
U.S.Government authorized reproduce distribute reports Governmental purposes notwithstanding copyright annotation thereon. views conclusions
contained herein authors interpreted necessarily representing official policies endorsements, either expressed implied,
organizations person connected them.
585

fiMichelson & Knoblock

References
Agichtein, E., & Ganti, V. (2004). Mining reference tables automatic text segmentation.
Proceedings 10th ACM Conference Knowledge Discovery Data
Mining, pp. 20 29. ACM Press.
Agrawal, R., Imielinski, T., & Swami, A. (1993). Mining association rules sets
items large databases. Proceedings ACM SIGMOD International Conference Management Data, pp. 207216. ACM Press.
Baxter, R., Christen, P., & Churches, T. (2003). comparison fast blocking methods
record linkage. Proceedings 9th ACM SIGKDD Workshop Data Cleaning,
Record Linkage, Object Identification, pp. 2527.
Bellare, K., & McCallum, A. (2007). Learning extractors unlabeled text using relevant
databases. Proceedings AAAI Workshop Information Integration
Web, pp. 1016.
Bilenko, M., Kamath, B., & Mooney, R. J. (2006). Adaptive blocking: Learning scale
record linkage clustering. Proceedings 6th IEEE International Conference
Data Mining, pp. 8796.
Bilenko, M., & Mooney, R. J. (2003). Adaptive duplicate detection using learnable string
similarity measures. Proceedings 9th ACM International Conference
Knowledge Discovery Data Mining, pp. 3948. ACM Press.
Borkar, V., Deshmukh, K., & Sarawagi, S. (2001). Automatic segmentation text
structured records. Proceedings ACM SIGMOD International Conference
Management Data, pp. 175186. ACM Press.
Califf, M. E., & Mooney, R. J. (1999). Relational learning pattern-match rules
information extraction. Proceedings 16th National Conference Artificial
Intelligence, pp. 328334.
Chakaravarthy, V. T., Gupta, H., Roy, P., & Mohania, M. (2006). Efficiently linking text
documents relevant structured information. Proceedings International
Conference Large Data Bases, pp. 667678. VLDB Endowment.
Chaudhuri, S., Ganjam, K., Ganti, V., & Motwani, R. (2003). Robust efficient fuzzy
match online data cleaning. Proceedings ACM SIGMOD International Conference Management Data, pp. 313324. ACM Press.
Cimiano, P., Handschuh, S., & Staab, S. (2004). Towards self-annotating web.
Proceedings 13th International Conference World Wide Web, pp. 462471.
ACM Press.
Ciravegna, F. (2001). Adaptive information extraction text rule induction
generalisation.. Proceedings 17th International Joint Conference Artificial
Intelligence, pp. 12511256.
586

fiRelational Data Unstructured Data Sources

Cohen, W., & Sarawagi, S. (2004). Exploiting dictionaries named entity extraction: combining semi-markov extraction processes data integration methods. Proceedings
10th ACM International Conference Knowledge Discovery Data Mining,
pp. 8998, Seattle, Washington. ACM Press.
Cohen, W. W. (2000). Data integration using similarity joins word-based information
representation language. ACM Transactions Information Systems, 18 (3), 288321.
Cohen, W. W., Ravikumar, P., & Feinberg, S. E. (2003). comparison string metrics
matching names records. Proceedings ACM SIGKDD Workshop
Data Cleaning, Record Linkage, Object Consoliation, pp. 1318.
Crescenzi, V., Mecca, G., & Merialdo, P. (2001). Roadrunner: Towards automatic data
extraction large web sites. Proceedings 27th International Conference
Large Data Bases, pp. 109118. VLDB Endowment.
Ding, Y., Embley, D. W., & Liddle, S. W. (2006). Automatic creation simplified querying semantic web content: approach based information-extraction ontologies.
Proceedings Asian Semantic Web Conference, pp. 400414.
Dingli, A., Ciravegna, F., & Wilks, Y. (2003). Automatic semantic annotation using unsupervised information extraction integration. Proceedings K-CAP Workshop Knowledge Markup Semantic Annotation.
Elfeky, M. G., Verykios, V. S., & Elmagarmid, A. K. (2002). TAILOR: record linkage
toolbox. Proceedings 18th International Conference Data Engineering, pp.
1728.
Embley, D. W., Campbell, D. M., Jiang, Y. S., Liddle, S. W., Ng, Y.-K., Quass, D., &
Smith, R. D. (1999). Conceptual-model-based data extraction multiple-record
web pages. Data Knowledge Engineering, 31 (3), 227251.
Fellegi, I. P., & Sunter, A. B. (1969). theory record linkage. Journal American
Statistical Association, 64, 11831210.
Handschuh, S., Staab, S., & Ciravegna, F. (2002). S-cream - semi-automatic creation
metadata. Proceedings 13th International Conference Knowledge Engineering Knowledge Management, pp. 165184. Springer Verlag.
Hendler, J. (2001). Agents semantic web. IEEE Intelligent Systems, 16 (2), 3037.
Hernandez, M. A., & Stolfo, S. J. (1998). Real-world data dirty: Data cleansing
merge/purge problem. Data Mining Knowledge Discovery, 2 (1), 937.
Jaro, M. A. (1989). Advances record-linkage methodology applied matching
1985 census tampa, florida. Journal American Statistical Association, 89,
414420.
Joachims, T. (1999). Advances Kernel Methods - Support Vector Learning, chap. 11:
Making large-Scale SVM Learning Practical. MIT-Press.
587

fiMichelson & Knoblock

Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random fields: Probabilistic models segmenting labeling sequence data. Proceedings 18th
International Conference Machine Learning, pp. 282289. Morgan Kaufmann.
Lee, M.-L., Ling, T. W., Lu, H., & Ko, Y. T. (1999). Cleansing data mining
warehousing. Proceedings 10th International Conference Database
Expert Systems Applications, pp. 751760. Springer-Verlag.
Lerman, K., Gazen, C., Minton, S., & Knoblock, C. A. (2004). Populating semantic web.
Proceedings AAAI Workshop Advances Text Extraction Mining.
Levenshtein, V. I. (1966). Binary codes capable correcting deletions, insertions,
reversals. English translation Soviet Physics Doklady, 10 (8), 707710.
Mansuri, I. R., & Sarawagi, S. (2006). Integrating unstructured data relational
databases. Proceedings International Conference Data Engineering, p. 29.
IEEE Computer Society.
McCallum, A. (2002).
Mallet:
http://mallet.cs.umass.edu.



machine

learning



language

toolkit.

McCallum, A., Nigam, K., & Ungar, L. H. (2000). Efficient clustering high-dimensional
data sets application reference matching. Proceedings 6th ACM
SIGKDD, pp. 169178.
Michalowski, M., Thakkar, S., & Knoblock, C. A. (2005). Automatically utilizing secondary
sources align information across sources. AI Magazine, Special Issue Semantic
Integration, Vol. 26, pp. 3345.
Michelson, M., & Knoblock, C. A. (2005). Semantic annotation unstructured ungrammatical text. Proceedings 19th International Joint Conference Artificial
Intelligence, pp. 10911098.
Michelson, M., & Knoblock, C. A. (2006). Learning blocking schemes record linkage.
Proceedings 21st National Conference Artificial Intelligence.
Michelson, M., & Knoblock, C. A. (2007). Unsupervised information extraction unstructured, ungrammatical data sources world wide web. International Journal
Document Analysis Recognition (IJDAR), Special Issue Noisy Text Analytics.
Mitchell, T. M. (1997). Machine Learning. McGraw-Hill, New York.
Muller, H.-M., & Sternberg, E. E. K. P. W. (2004). Textpresso: ontology-based information retrieval extraction system biological literature. PLoS Biology, 2 (11).
Muslea, I., Minton, S., & Knoblock, C. A. (2001). Hierarchical wrapper induction
semistructured information sources. Autonomous Agents Multi-Agent Systems,
4 (1/2), 93114.
588

fiRelational Data Unstructured Data Sources

Newcombe, H. B. (1967). Record linkage: design efficient systems linking records
individual family histories. American Journal Human Genetics, 19 (3),
335359.
Porter, M. F. (1980). algorithm suffix stripping. Program, 14 (3), 130137.
Smith, T. F., & Waterman, M. S. (1981). Identification common molecular subsequences.
Journal Molecular Biology, 147, 195197.
Soderland, S. (1999). Learning information extraction rules semi-structured free
text. Machine Learning, 34 (1-3), 233272.
Thakkar, S., Ambite, J. L., & Knoblock, C. A. (2004). data integration approach
automatically composing optimizing web services. Proceedings ICAPS
Workshop Planning Scheduling Web Grid Services.
Tsochantaridis, I., Hofmann, T., Joachims, T., & Altun, Y. (2004). Support vector machine
learning interdependent structured output spaces. Proceedings 21st
International Conference Machine Learning, p. 104. ACM Press.
Tsochantaridis, I., Joachims, T., Hofmann, T., & Altun, Y. (2005). Large margin methods
structured interdependent output variables. Journal Machine Learning
Research, 6, 14531484.
Vargas-Vera, M., Motta, E., Domingue, J., Lanzoni, M., Stutt, A., & Ciravegna, F. (2002).
MnM: Ontology driven semi-automatic automatic support semantic markup.
Proceedings 13th International Conference Knowledge Engineering
Management, pp. 213221.
Wellner, B., McCallum, A., Peng, F., & Hay, M. (2004). integrated, conditional model
information extraction coreference application citation matching.
Proceedings 20th Conference Uncertainty Artificial Intelligence, pp. 593
601.
Winkler, W. E., & Thibaudeau, Y. (1991). application fellegi-sunter model
record linkage 1990 U.S. Decennial Census. Tech. rep., Statistical Research
Report Series RR91/09 U.S. Bureau Census.
Zhai, C., & Lafferty, J. (2001). study smoothing methods language models applied
ad hoc information retrieval. Proceedings 24th ACM SIGIR Conference
Research Development Information Retrieval, pp. 334342. ACM Press.

589

fiJournal Artificial Intelligence Research 31 (2008) 273-318

Submitted 07/07; published 02/08

Modular Reuse Ontologies: Theory Practice
Bernardo Cuenca Grau
Ian Horrocks
Yevgeny Kazakov

berg@comlab.ox.ac.uk
ian.horrocks@comlab.ox.ac.uk
yevgeny.kazakov@comlab.ox.ac.uk

Oxford University Computing Laboratory
Oxford, OX1 3QD, UK

Ulrike Sattler

sattler@cs.man.ac.uk

School Computer Science
University Manchester
Manchester, M13 9PL, UK

Abstract
paper, propose set tasks relevant modular reuse ontologies. order formalize tasks reasoning problems, introduce notions
conservative extension, safety module general class logic-based ontology
languages. investigate general properties relationships notions
study relationships relevant reasoning problems previously
identified. study computability problems, consider, particular, Description Logics (DLs), provide formal underpinning W3C Web Ontology
Language (OWL), show problems consider undecidable algorithmically unsolvable description logic underlying OWL DL. order achieve
practical solution, identify conditions sufficient ontology reuse set symbols safelythat is, without changing meaning. provide notion safety
class, characterizes sufficient condition safety, identify family safety
classescalled localitywhich enjoys collection desirable properties. use notion
safety class extract modules ontologies, provide various modularization algorithms appropriate properties particular safety class use.
Finally, show practical benefits safety checking module extraction algorithms.

1. Motivation
Ontologiesconceptualizations domain shared community usersplay major role Semantic Web, increasingly used knowledge management
systems, e-Science, bio-informatics, Grid applications (Staab & Studer, 2004).
design, maintenance, reuse, integration ontologies complex tasks. Like
software engineers, ontology engineers need supported tools methodologies
help minimize introduction errors, i.e., ensure ontologies
consistent unexpected consequences. order develop support, important notions software engineering, module, black-box behavior, controlled
interaction, need adapted.
Recently, growing interest topic modularity ontology engineering (Seidenberg & Rector, 2006; Noy, 2004a; Lutz, Walther, & Wolter, 2007; Cuenca
Grau, Parsia, Sirin, & Kalyanpur, 2006b; Cuenca Grau, Horrocks, Kazakov, & Sattler,
c
2008
AI Access Foundation. rights reserved.

fiCuenca Grau, Horrocks, Kazakov, & Sattler

2007), motivated above-mentioned application needs. paper,
focus use modularity support partial reuse ontologies. particular,
consider scenario developing ontology P want reuse set
symbols foreign ontology Q without changing meaning.
example, suppose ontology engineer building ontology research
projects, specifies different types projects according research topic
focus on. ontology engineer charge projects ontology P may use terms
Cystic Fibrosis Genetic Disorder descriptions medical research projects.
ontology engineer expert research projects; may unfamiliar, however,
topics projects cover and, particular, terms Cystic Fibrosis
Genetic Disorder. order complete projects ontology suitable definitions
medical terms, decides reuse knowledge subjects wellestablished medical ontology Q.
straightforward way reuse concepts construct logical union
P Q axioms P Q. reasonable assume additional knowledge
medical terms used P Q implications meaning
projects defined P; indeed, additional knowledge reused terms provides new
information medical research projects defined using medical terms.
Less intuitive fact importing Q may also result new entailments concerning
reused symbols, namely Cystic Fibrosis Genetic Disorder. Since ontology engineer
projects ontology expert medicine relies designers Q,
expected meaning reused symbols completely specified Q;
is, fact symbols used projects ontology P imply
original meaning Q changes. P change meaning symbols
Q, say P Q conservative extension Q
realistic application scenarios, often unreasonable assume foreign ontology
Q fixed; is, Q may evolve beyond control modelers P. ontology
engineers charge P may authorized access information Q or,
importantly, may decide later time reuse symbols Cystic Fibrosis
Genetic Disorder medical ontology Q. application scenarios
external ontology Q may change, reasonable abstract particular Q
consideration. particular, given set external symbols, fact
axioms P change meaning symbol independent
particular meaning ascribed symbols Q. case, say P safe
S.
Moreover, even P safely reuses set symbols ontology Q, may still
case Q large ontology. particular, example, foreign medical
ontology may huge, importing whole ontology would make consequences
additional information costly compute difficult ontology engineers
charge projects ontology (who medical experts) understand. practice,
therefore, one may need extract module Q1 Q includes relevant information. Ideally, module small possible still guarantee capture
meaning terms used; is, answering queries research projects
ontology, importing module Q1 would give exactly answers whole
medical ontology Q imported. case, importing module
274

fiModular Reuse Ontologies: Theory Practice

observable effect projects ontology importing entire ontology. Furthermore, fact Q1 module Q independent particular P
consideration.
contributions paper follows:
1. propose set tasks relevant ontology reuse formalize
reasoning problems. end, introduce notions conservative extension,
safety module general class logic-based ontology languages.
2. investigate general properties relationships notions
conservative extension, safety, module use properties study relationships relevant reasoning problems previously identified.
3. consider Description Logics (DLs), provide formal underpinning
W3C Web Ontology Language (OWL), study computability tasks.
show tasks consider undecidable algorithmically unsolvable
description logic underlying OWL DLthe expressive dialect OWL
direct correspondence description logics.
4. consider problem deciding safety ontology signature. Given
problem undecidable OWL DL, identify sufficient conditions safety,
decidable OWL DLthat is, ontology satisfies conditions
safe; converse, however, necessarily hold. propose
notion safety class, characterizes sufficiency condition safety,
identify family safety classescalled localitywhich enjoys collection desirable
properties.
5. next apply notion safety class task extracting modules
ontologies; provide various modularization algorithms appropriate
properties particular safety class use.
6. present empirical evidence practical benefits techniques safety
checking module extraction.
paper extends results previous work (Cuenca Grau, Horrocks, Kutz, &
Sattler, 2006; Cuenca Grau et al., 2007; Cuenca Grau, Horrocks, Kazakov, & Sattler, 2007).

2. Preliminaries
section introduce description logics (DLs) (Baader, Calvanese, McGuinness,
Nardi, & Patel-Schneider, 2003), family knowledge representation formalisms
underlie modern ontology languages, OWL DL (Patel-Schneider, Hayes, & Horrocks, 2004). hierarchy commonly-used description logics summarized Table 1.
syntax description logic L given signature set constructors.
signature (or vocabulary) Sg DL (disjoint) union countably infinite sets AC
atomic concepts (A, B, . . . ) representing sets elements, AR atomic roles (r, s, . . . )
representing binary relations elements, Ind individuals (a, b, c, . . . ) representing constants. assume signature fixed every DL.
275

fiCuenca Grau, Horrocks, Kazakov, & Sattler

DLs

Constructors
Con
>, A, C1 u C2 , R.C
pp, C
pp

Axioms [ Ax ]
TBox
ABox
C, C1 v C2 : C, r(a, b)
pp
pp
pp
pp

Rol
RBox
EL
r
ALC pp
pp
Trans(r)
+
r
+ H
R 1 v R2
+ F
Funct(R)
+ N
(> n S)
+ Q
(> n S.C)
+
{i}
r AR, AC, a, b Ind, R(i) Rol, C(i) Con, n 1 Rol simple
role (see (Horrocks & Sattler, 2005)).
Table 1: hierarchy standard description logics

Every DL provides constructors defining set Rol (general) roles (R, S, . . . ),
set Con (general) concepts (C, D, . . . ), set Ax axioms (, , . . . )
union role axioms (RBox), terminological axioms (TBox) assertions (ABox).
EL (Baader, Brandt, & Lutz, 2005) simple description logic allows one
construct complex concepts using conjunction C1 u C2 existential restriction R.C
starting atomic concepts A, roles R top concept >. EL provides role
constructors role axioms; thus, every role R EL atomic. TBox axioms
EL either concept definitions C general concept inclusion axioms (GCIs)
C1 v C2 . EL assertions either concept assertions : C role assertions r(a, b).
paper assume concept definition C abbreviation two GCIs v C
C v A.
basic description logic ALC (Schmidt-Schau & Smolka, 1991) obtained EL
adding concept negation constructor C. introduce additional constructors
abbreviations: bottom concept shortcut >, concept disjunction C1 C2
stands (C1 u C2 ), value restriction R.C stands (R.C). contrast
EL, ALC express contradiction axioms like > v . logic extension
ALC where, additionally, atomic roles declared transitive using role
axiom Trans(r).
extensions description logics add features inverse roles r (indicated
appending letter name logic), role inclusion axioms (RIs) R1 v R2 (+H),
functional roles Funct(R) (+F), number restrictions (> n S), n 1, (+N ), qualified
number restrictions (> n S.C), n 1, (+Q)1 , nominals {a} (+O). Nominals make
possible construct concept representing singleton set {a} (a nominal concept)
individual a. extensions used different combinations; example ALCO
extension ALC nominals; SHIQ extension role hierarchies,
1. dual constructors (6 n S) (6 n S.C) abbreviations (> n + 1 S) (> n + 1 S.C),
respectively

276

fiModular Reuse Ontologies: Theory Practice

inverse roles qualified number restrictions; SHOIQ DL uses
constructors axiom types presented.
Modern ontology languages, OWL, based description logics and, certain extent, syntactic variants thereof. particular, OWL DL corresponds SHOIN
(Horrocks, Patel-Schneider, & van Harmelen, 2003). paper, assume ontology
based description logic L finite set axioms L. signature
ontology (of axiom ) set Sig(O) (Sig()) atomic concepts, atomic roles
individuals occur (respectively ).
main reasoning task ontologies entailment: given ontology axiom
, check implies . logical entailment |= defined using usual Tarski-style
set-theoretic semantics description logics follows. interpretation pair =
(I , ), non-empty set, called domain interpretation,
interpretation function assigns: every AC subset AI , every r AR
binary relation rI , every Ind element aI . Note
sets AC, AR Ind defined interpretation assumed fixed
ontology language (DL).
interpretation function extended complex roles concepts via DLconstructors follows:
(>)I
(C u D)I
(R.C)I
(C)I
(r )I

=
=
=
=
=


C DI
{x | y.hx, yi RI C }
\ C
{hx, yi | hy, xi rI }

(> n R)I = { x | ]{y | hx, yi RI } n }
(> n R.C)I = { x | ]{y | hx, yi RI C } n }
{a}I = {aI }
satisfaction relation |= interpretation DL axiom (read
satisfies , model ) defined follows:
|= C1 v C2 iff C1I C2I ;
|= R1 v R2 iff R1I R2I ;

|= : C iff aI C ;
|= r(a, b) iff haI , bI rI ;

|= Trans(r) iff x, y, z [ hx, yi rI hy, zi rI hx, zi rI ];
|= Funct(R) iff x, y, z [ hx, yi RI hx, zi RI = z ];
interpretation model ontology satisfies axioms O.
ontology implies axiom (written |= ) |= every model O. Given
set interpretations, say axiom (an ontology O) valid every
interpretation model (respectively O). axiom tautology valid
set interpretations (or, equivalently, implied empty ontology).
say two interpretations = (I , ) J = (J , J ) coincide subset
signature (notation: I|S = J |S ) = J X = X J every X S.
say two sets interpretations J equal modulo (notation: I|S = J|S )
every exits J J J |S = I|S every J J exists
I|S = J |S .
277

fiCuenca Grau, Horrocks, Kazakov, & Sattler

Ontology medical research projects P:
P1

Genetic Disorder Project Project u Focus.Genetic Disorder

P2

Cystic Fibrosis EUProject EUProject u Focus.Cystic Fibrosis

P3

EUProject v Project

P4

Focus.> v Project

E1

Project u (Genetic Disorder ::
u Cystic Fibrosis) v

E2

Focus.Cystic Fibrosis v Focus.Genetic Disorder

::

Ontology medical terms Q:
M1 Cystic Fibrosis Fibrosis u located In.Pancreas u Origin.Genetic Origin
M2 Genetic Fibrosis Fibrosis u Origin.Genetic Origin
M3 Fibrosis u located In.Pancreas v Genetic Fibrosis
M4 Genetic Fibrosis v Genetic Disorder
M5 DEFBI Gene v Immuno Protein Gene u associated With.Cystic Fibrosis
Figure 1: Reusing medical terminology ontology research projects

3. Ontology Integration Knowledge Reuse
section, elaborate ontology reuse scenario sketched Section 1. Based
application scenario, motivate define reasoning tasks investigated
remainder paper. particular, tasks based notions conservative
extension (Section 3.2), safety (Sections 3.2 3.3) module (Section 3.4). notions
defined relative language L. Within section, assume L ontology
language based description logic; Section 3.6, define formally class
ontology languages given definitions conservative extensions, safety
modules apply. convenience reader, tasks consider paper
summarized Table 2.
3.1 Motivating Example
Suppose ontology engineer charge SHOIQ ontology research projects,
specifies different types projects according research topic concerned
with. Assume ontology engineer defines two conceptsGenetic Disorder Project
Cystic Fibrosis EUProjectin ontology P. first one describes projects genetic
disorders; second one describes European projects cystic fibrosis, given
axioms P1 P2 Figure 1. ontology engineer expert research projects:
knows, example, every instance EUProject must instance Project (the
concept-inclusion axiom P3) role Focus applied instances
Project (the domain axiom P4). may unfamiliar, however, topics
projects cover and, particular, terms Cystic Fibrosis Genetic Disorder
mentioned P1 P2. order complete projects ontology suitable definitions
278

fiModular Reuse Ontologies: Theory Practice

medical terms, decides reuse knowledge subjects wellestablished widely-used medical ontology.
Suppose Cystic Fibrosis Genetic Disorder described ontology Q containing axioms M1-M5 Figure 1. straightforward way reuse concepts
import P ontology Qthat is, add axioms Q axioms P
work extended ontology P Q. Importing additional axioms ontology
may result new logical consequences. example, easy see axioms M1M4
Q imply every instance Cystic Fibrosis instance Genetic Disorder:
Q |= := (Cystic Fibrosis v Genetic Disorder)

(1)

Indeed, concept inclusion 1 := (Cystic Fibrosis v Genetic Fibrosis) follows axioms
M1 M2 well axioms M1 M3; follows axioms 1 M4.
Using inclusion (1) axioms P1P3 ontology P prove
every instance Cystic Fibrosis EUProject also instance Genetic Disorder Project:
P Q |= := (Cystic Fibrosis EUProject v Genetic Disorder Project)

(2)

inclusion , however, follow P alonethat is, P 6|= . ontology
engineer might aware Entailment (2), even though concerns terms primary
scope projects ontology P.
natural expect entailments like (1) imported ontology Q result
new logical consequences, like , (2), terms defined main ontology P.
One would expect, however, meaning terms defined Q changes
consequence import since terms supposed completely specified within
Q. side effect would highly undesirable modeling ontology P since
ontology engineer P might expert subject Q supposed
alter meaning terms defined Q even implicitly.
meaning reused terms, however, might change import, perhaps due
modeling errors. order illustrate situation, suppose ontology engineer
learned concepts Genetic Disorder Cystic Fibrosis ontology Q
(including inclusion (1)) decided introduce additional axioms formalizing
following statements:
Every instance Project different every instance Genetic Disorder

every instance Cystic Fibrosis.
:::

(3)

::::::
Every::::::::
project Focus Cystic Fibrosis, also Focus Genetic Disorder

(4)

Note statements (3) (4) thought adding new information
projects and, intuitively, change constrain meaning medical
terms.
Suppose ontology engineer formalized statements (3) (4) ontology
P using axioms E1 E2 respectively. point, ontology engineer introduced modeling errors and, consequence, axioms E1 E2 correspond (3)
(4): E1 actually formalizes following statement: Every instance Project different every common instance Genetic Disorder Cystic Fibrosis, E2 expresses
279

fiCuenca Grau, Horrocks, Kazakov, & Sattler

Every object either Focus nothing, Focus Cystic Fibrosis,
also Focus Genetic Disorder. kinds modeling errors difficult detect,
especially cause inconsistencies ontology.
Note that, although axiom E1 correspond fact (3), still consequence
(3) means constrain meaning medical terms.
hand, E2 consequence (4) and, fact, constrains meaning medical
terms. Indeed, axioms E1 E2 together axioms P1-P4 P imply new axioms
concepts Cystic Fibrosis Genetic Disorder, namely disjointness:
P |= := (Genetic Disorder u Cystic Fibrosis v )

(5)

entailment (5) proved using axiom E2 equivalent to:
> v Focus.(Genetic Disorder Cystic Fibrosis)

(6)

inclusion (6) P4 imply every element domain must projectthat
is, P |= (> v Project). Now, together axiom E1, implies (5).
axioms E1 E2 imply new statements medical terms, also
cause inconsistencies used together imported axioms Q. Indeed,
(1) (5) obtain P Q |= := (Cystic Fibrosis v ), expresses inconsistency
concept Cystic Fibrosis.
summarize, seen importing external ontology lead undesirable
side effects knowledge reuse scenario, like entailment new axioms even inconsistencies involving reused vocabulary. next section discuss formalize
effects consider undesirable.
3.2 Conservative Extensions Safety Ontology
argued previous section, important requirement reuse ontology Q
within ontology P P Q produces exactly logical consequences
vocabulary Q Q alone does. requirement naturally formulated
using well-known notion conservative extension, recently investigated context ontologies (Ghilardi, Lutz, & Wolter, 2006; Lutz et al., 2007).
Definition 1 (Deductive Conservative Extension). Let O1 two Lontologies, signature L. say deductive S-conservative extension
O1 w.r.t. L, every axiom L Sig() S, |= iff O1 |= .
say deductive conservative extension O1 w.r.t. L deductive
S-conservative extension O1 w.r.t. L = Sig(O1 ).

words, ontology deductive S-conservative extension O1
signature language L every logical consequence constructed
using language L symbols S, already logical consequence O1 ;
is, additional axioms \ O1 result new logical consequences
vocabulary S. Note deductive S-conservative extension O1 w.r.t. L,
deductive S1 -conservative extension O1 w.r.t. L every S1 S.
notion deductive conservative extension directly applied ontology
reuse scenario.
280

fiModular Reuse Ontologies: Theory Practice

Definition 2 (Safety Ontology). Given L-ontologies O0 , say
safe O0 (or imports O0 safe way) w.r.t. L O0 deductive conservative
extension O0 w.r.t. L.

Hence, first reasoning task relevant ontology reuse scenario formulated
follows:
T1.

given L-ontologies O0 , determine safe O0 w.r.t. L.

shown Section 3.1 that, given P consisting axioms P1P4, E1, E2, Q
consisting axioms M1M5 Figure 1, exists axiom = (Cystic Fibrosis v )
uses symbols Sig(Q) Q 6|= P Q |= . According Definition 1,
means P Q deductive conservative extension Q w.r.t. language
L expressed (e.g. L = ALC). possible, however, show
axiom E2 removed P resulting ontology P1 = P \ {E2}, P1 Q
deductive conservative extension Q. following notion useful proving deductive
conservative extensions:
Definition 3 (Model Conservative Extension, Lutz et al., 2007).
Let O1 two L-ontologies signature L. say model
S-conservative extension O1 , every model O1 , exists model J
I|S = J |S . say model conservative extension O1 model
S-conservative extension O1 = Sig(O1 ).

notion model conservative extension Definition 3 seen semantic
counterpart notion deductive conservative extension Definition 1: latter
defined terms logical entailment, whereas former defined terms models.
Intuitively, ontology model S-conservative extension O1 every model
O1 one find model domain interprets symbols
way. notion model conservative extension, however, provide
complete characterization deductive conservative extensions, given Definition 1;
is, notion used proving ontology deductive conservative
extension another, vice versa:
Proposition 4 (Model vs. Deductive Conservative Extensions, Lutz et al., 2007)
1. every two L-ontologies O, O1 O, signature L, model
S-conservative extension O1 deductive S-conservative extension O1
w.r.t. L;
2. exist two ALC ontologies O1 deductive conservative
extension O1 w.r.t. ALC, model conservative extension O1 .
Example 5 Consider ontology P1 consisting axioms P1P4, E1 ontology
Q consisting axioms M1M5 Figure 1. demonstrate P1 Q deductive
conservative extension Q. According proposition 4, sufficient show P1 Q
model conservative extension Q; is, every model Q exists model
J P1 Q I|S = J |S = Sig(Q).
281

fiCuenca Grau, Horrocks, Kazakov, & Sattler

required model J constructed follows: take J identical except interpretations atomic concepts Genetic Disorder Project,
Cystic Fibrosis EUProject, Project, EUProject atomic role Focus,
interpret J empty set. easy check axioms P1P4, E1
satisfied J hence J |= P1 . Moreover, since interpretation symbols Q
remains unchanged, I|Sig(Q) = J |Sig(Q) J |= Q. Hence, P1 Q model
conservative extension Q.
example Statement 2 Proposition, refer interested reader
literature (Lutz et al., 2007, p. 455).

3.3 Safety Ontology Signature
far, ontology reuse scenario assumed reused ontology Q fixed
axioms Q copied P import. practice, however,
often convenient keep Q separate P make axioms available demand
via reference. makes possible continue developing P Q independently.
example, ontology engineers project ontology P may willing depend
particular version Q, may even decide later time reuse medical terms
(Cystic Fibrosis Genetic Disorder) another medical ontology instead. Therefore,
many application scenarios important develop stronger safety condition P
depends little possible particular ontology Q reused. order
formulate condition, abstract particular ontology Q imported
focus instead symbols Q reused:
Definition 6 (Safety Signature). Let L-ontology signature L.
say safe w.r.t. L, every L-ontology O0 Sig(O) Sig(O0 ) S,
safe O0 w.r.t. L; is, O0 deductive conservative extension
O0 w.r.t. L.

Intuitively, knowledge reuse scenario, ontology safe signature w.r.t.
language L imports safe way ontology O0 written L shares
symbols O. associated reasoning problem formulated
following way:
T2.

given L-ontology signature L,
determine safe w.r.t. L.

seen Section 3.2, ontology P consisting axioms P1P4, E1, E2 Figure 1,
import Q consisting axioms M1M5 Figure 1 safe way L = ALC.
According Definition 6, since Sig(P) Sig(Q) = {Cystic Fibrosis, Genetic Disorder},
ontology P safe w.r.t. L.
fact, possible show stronger result, namely, ontology containing
axiom E2 safe = {Cystic Fibrosis, Genetic Disorder} w.r.t. L = ALC. Consider
ontology O0 = {1 , 2 }, 1 = (> v Cystic Fibrosis) 2 = (Genetic Disorder v ).
Since E2 equivalent axiom (6), easy see O0 inconsistent. Indeed E2,
1 2 imply contradiction = (> v ), entailed O0 . Hence, O0
deductive conservative extension O0 . Definition 6, since Sig(O) Sig(O0 ) S,
means safe S.
282

fiModular Reuse Ontologies: Theory Practice

clear one could prove ontology safe signature S: simply find
ontology O0 Sig(O) Sig(O0 ) S, O0 deductive conservative
extension O0 . clear, however, one could prove safe S.
turns notion model conservative extensions also used purpose.
following lemma introduces property relates notion model conservative
extension notion safety signature. Intuitively, says notion
model conservative extension stable expansion new axioms provided
share symbols original ontologies.
Lemma 7 Let O, O1 O, O0 L-ontologies signature L
model S-conservative extension O1 Sig(O) Sig(O0 ) S. O0
model S0 -conservative extension O1 O0 S0 = Sig(O0 ).
Proof. order show OO0 model S0 -conservative extension O1 O0 according
Definition 3, let model O1 O0 . construct model J O0
I|S0 = J |S0 .
(])
Since model S-conservative extension O1 , model O1 , Definition 3
exists model J1 I|S = J1 |S . Let J interpretation
J |Sig(O) = J1 |Sig(O) J |S0 = I|S0 . Since Sig(O) S0 = Sig(O) (S Sig(O0 ))
(Sig(O) Sig(O0 )) I|S = J1 |S , interpretation J always exists. Since
J |Sig(O) = J1 |Sig(O) J1 |= O, J |= O; since J |S0 = I|S0 , Sig(O0 ) S0 ,
|= O0 , J |= O0 . Hence J |= O0 I|S0 = J |S0 , proves (]).
Lemma 7 allows us identify condition sufficient ensure safety ontology
signature:
Proposition 8 (Safety Signature vs. Model Conservative Extensions)
Let L-ontology signature L model S-conservative
extension empty ontology O1 = ; is, every interpretation exists
model J J |S = I|S . safe w.r.t. L.
Proof. order prove safe w.r.t. L according Definition 6, take
SHOIQ ontology O0 Sig(O) Sig(O0 ) S. need demonstrate O0
deductive conservative extension O0 w.r.t. L.
(])
Indeed, Lemma 7, since model S-conservative extension O1 = ,
Sig(O)Sig(O0 ) S, OO0 model S0 -conservative extension O1 O0 = O0
S0 = Sig(O0 ). particular, since Sig(O0 ) S0 , O0 deductive
conservative extension O0 , required prove (]).
Example 9 Let P1 ontology consisting axioms P1P4, E1 Figure 1.
show P1 safe = {Cystic Fibrosis, Genetic Disorder} w.r.t. L = SHOIQ.
Proposition 8, order prove safety P1 sufficient demonstrate P1
model S-conservative extension empty ontology, is, every S-interpretation
exists model J P1 I|S = J |S .
Consider model J obtained Example 5. shown Example 5, J
model P1 I|Sig(Q) = J |Sig(Q) Q consists axioms M1M5 Figure 1.
particular, since Sig(Q), I|S = J |S .

283

fiCuenca Grau, Horrocks, Kazakov, & Sattler

3.4 Extraction Modules Ontologies
example Figure 1 medical ontology Q small. Well established medical
ontologies, however, large may describe subject matters designer P interested. example, medical ontology Q could contain information
genes, anatomy, surgical techniques, etc.
Even P imports Q without changing meaning reused symbols, processing
is, browsing, reasoning over, etcthe resulting ontology P Q may considerably
harder processing P alone. Ideally, one would like extract (hopefully small) fragment Q1 external medical ontologya modulethat describes concepts
reused P.
Intuitively, answering arbitrary query signature P, importing
module Q1 give exactly answers whole ontology Q
imported.
Definition 10 (Module Ontology). Let O, O0 O10 O0 L-ontologies.
say O10 module O0 w.r.t. L, O0 deductive S-conservative
extension O10 = Sig(O) w.r.t. L.

task extracting modules imported ontologies ontology reuse scenario
thus formulated follows:
T3.

given L-ontologies O, O0 ,
compute module O10 O0 w.r.t. L.

Example 11 Consider ontology P1 consisting axioms P1P4, E1 ontology Q
consisting axioms M1M5 Figure 1. Recall axiom (1) consequence axioms M1, M2 M4 well axioms M1, M3 M4 ontology Q.
fact, sets axioms actually minimal subsets Q imply . particular,
subset Q0 Q consisting axioms M2, M3, M4 M5, Q0 6|= .
demonstrated P1 Q0 deductive conservative extension Q0 . particular
P1 Q0 6|= . then, according Definition 10, Q0 module P1 Q w.r.t.
L = ALC extensions, since P1 Q deductive S-conservative extension
P1 Q0 w.r.t. L = ALC = Sig(P1 ). Indeed, P1 Q |= , Sig() P1 Q0 6|= .
Similarly, one show subset Q imply module Q
w.r.t. P1 .
hand, possible show subsets Q1 = {M1, M2, M4}
Q2 = {M1, M3, M4} Q modules P1 Q w.r.t. L = SHOIQ. so, Definition 10, need demonstrate P1 Q deductive S-conservative extension
P1 Q1 P1 Q2 = Sig(P1 ) w.r.t. L. usual, demonstrate stronger
fact, P1 Q model S-conservative extension P1 Q1 P1 Q2
sufficient Claim 1 Proposition 4.
order show P1 Q model S-conservative extension P1 Q1 =
Sig(P1 ), consider model P1 Q1 . need construct model J P1 Q
I|S = J |S . Let J defined exactly except interpretations
atomic concepts Fibrosis, Pancreas, Genetic Fibrosis, Genetic Origin defined
interpretation Cystic Fibrosis I, interpretations atomic roles located
284

fiModular Reuse Ontologies: Theory Practice

Origin defined identity relation. easy see axioms M1M3 M5
satisfied J . Since modify interpretation symbols P1 , J also
satisfies axioms P1 . Moreover, J model M4, Genetic Fibrosis
Genetic Disorder interpreted J like Cystic Fibrosis Genetic Disorder I,
model Q1 , implies concept inclusion = (Cystic Fibrosis v Genetic Disorder).
Hence constructed model J P1 Q I|S = J |S , thus P1 Q
model S-conservative extension P1 Q1 .
fact, construction works replace Q1 subset Q implies
. particular, P1 Q also model S-conservative extension P1 Q2 . way,
demonstrated modules P Q exactly subsets Q imply
.

algorithm implementing task T3 used extracting module
ontology Q imported P prior performing reasoning terms P. However,
ontology P modified, module extracted since module Q1
P Q might necessarily module modified ontology. Since extraction
modules potentially expensive operation, would convenient extract
module reuse version ontology P reuses specified
symbols Q. idea motivates following definition:
Definition 12 (Module Signature). Let O0 O10 O0 L-ontologies
signature L. say O10 module O0 (or S-module O0 ) w.r.t.
L, every L-ontology Sig(O) Sig(O0 ) S, O10 module
O0 w.r.t. L.

Intuitively, notion module signature uniform analog notion
module ontology, similar way notion safety signature uniform
analog safety ontology. reasoning task corresponding Definition 12
formulated follows:
T4.

given L-ontology O0 signature L,
compute module O10 O0 .

Continuing Example 11, possible demonstrate subset Q implies
axiom , fact module = {Cystic Fibrosis, Genetic Disorder} Q, is,
imported instead Q every ontology shares Q symbols S.
order prove this, use following sufficient condition based notion model
conservative extension:
Proposition 13 (Modules Signature vs. Model Conservative Extensions)
Let O0 O10 O0 L-ontologies signature L O0 model
S-conservative extension O10 . O10 module O0 w.r.t. L.
Proof. order prove O10 module O0 w.r.t. L according Definition 12,
take SHOIQ ontology Sig(O) Sig(O0 ) S. need demonstrate
O10 module O0 w.r.t. L, is, according Definition 10, O0
deductive S0 -conservative extension O10 w.r.t. L S0 = Sig(O).
(])
285

fiCuenca Grau, Horrocks, Kazakov, & Sattler

Indeed, Lemma 7, since O0 model S-conservative extension O10 , Sig(O0 )
Sig(O) S, O0 model S00 -conservative extension O10
S00 = Sig(O). particular, since S0 = Sig(O) S00 , O0 deductive
S0 -conservative extension O10 w.r.t. L, required prove (]).
Example 14 Let Q Example 11. demonstrate subset Q1 Q
implies module = {Cystic Fibrosis, Genetic Disorder} Q. According
Proposition 13, sufficient demonstrate Q model S-conservative extension
Q1 , is, every model Q1 exists model J Q I|S = J |S .
easy see model J constructed Example 11, required
property holds.

Note module signature Q necessarily contain axioms
contain symbols S. example, module Q1 consisting axiom M1, M2
M4 Q contain axiom M5 mentions atomic concept Cystic Fibrosis
S. Also note even minimal module like Q1 might still axioms
like M2 mention symbols all.
3.5 Minimal Modules Essential Axioms
One usually interested extracting arbitrary modules reused ontology,
extracting modules easy process afterwards. Ideally, extracted modules
small possible. Hence reasonable consider problem extracting
minimal modules; is, modules contain module subset.
Examples 11 14 demonstrate minimal module ontology signature
necessarily unique: ontology Q consisting axioms M1M5 two minimal modules Q1 = {M1, M2, M4}, Q2 = {M1, M3, M4}, ontology P1 = {P1, P2, P3, E1}
well signature = {Cystic Fibrosis, Genetic Disorder}, since minimal sets axioms imply axiom = (Cystic Fibrosis v Genetic Disorder). Depending
application scenario, one consider several variations tasks T3 T4 computing minimal modules. applications might necessary extract minimal
modules, whereas others minimal module suffices.
Axioms occur minimal module Q essential P
always removed every module Q, thus never need imported
P. true axioms occur minimal modules Q. might
necessary import axioms P order lose essential information Q.
arguments motivate following notion:
Definition 15 (Essential Axiom). Let O0 L-ontologies, signature
axiom L. say essential O0 w.r.t. L contained
minimal module O0 w.r.t. L. say essential axiom O0 w.r.t.
L (or S-essential O0 ) contained minimal module O0 w.r.t. L.
example axioms M1M4 Q essential ontology P1
signature = {Cystic Fibrosis, Genetic Disorder}, axiom M5 essential.
certain situations one might interested computing set essential axioms
ontology, done computing union minimal modules. Note
286

fiModular Reuse Ontologies: Theory Practice

Notation

Input

Task

Checking Safety:
T1

O, O0 , L

Check safe O0 w.r.t. L

T2

O, S, L

Check safe w.r.t. L

Extracting [all / / union of] [minimal] module(s):
T3[a,s,u][m]

O, O0 , L

Extract modules O0 w.r.t. L

T4[a,s,u][m]

O0 , S, L

Extract modules O0 w.r.t. L

O, O0 ontologies signature L
Table 2: Summary reasoning tasks relevant ontology integration reuse
computing union minimal modules might easier computing minimal
modules since one need identify axiom belongs minimal module.
Table 2 summarized reasoning tasks found potentially relevant
ontology reuse scenarios included variants T3am, T3sm, T3um
task T3 T4am, T4sm, T4um task T4 computation minimal modules
discussed section.
variants tasks T3 T4 could considered relevant ontology reuse.
example, instead computing minimal modules, one might interested computing
modules smallest number axioms, modules smallest size measured
number symbols, complexity measure ontology. theoretical
results present paper easily extended many reasoning
tasks.
3.6 Safety Modules General Ontology Languages
notions introduced Section 3 defined respect ontology language.
far, however, implicitly assumed ontology languages description
logics defined Section 2that is, fragments DL SHOIQ. notions considered Section 3 applied, however, much broader class ontology languages.
definitions apply ontology language notion entailment axioms
ontologies, mechanism identifying signatures.
Definition 16. ontology language tuple L = (Sg, Ax, Sig, |=), Sg set
signature elements (or vocabulary) L, Ax set axioms L, Sig function
assigns every axiom Ax finite set Sig() Sg called signature ,
|= entailment relation sets axioms Ax axioms Ax, written
|= . ontology L finiteSset axioms Ax. extend function Sig
ontologies follows: Sig(O) := Sig().

Definition 16 provides general notion ontology language. language L
given set symbols (a signature), set formulae (axioms) constructed
symbols, function assigns formula signature, entailment
relation sets axioms. ontology language OWL DL well description
287

fiCuenca Grau, Horrocks, Kazakov, & Sattler

logics defined Section 2 examples ontology languages accordance Definition 16.
examples ontology languages First Order Logic, Second Order Logic, Logic
Programs.
easy see notions deductive conservative extension (Definition 1), safety
(Definitions 2 6) modules (Definitions 10 12), well reasoning tasks
Table 2, well-defined every ontology language L given Definition 16.
definition model conservative extension (Definition 3) propositions involving
model conservative extensions (Propositions 4, 8, 13) also extended
languages standard Tarski model-theoretic semantics, Higher Order Logic.
simplify presentation, however, formulate general requirements
semantics ontology languages, assume deal sublanguages SHOIQ
whenever semantics taken account.
remainder section, establish relationships different
notions safety modules arbitrary ontology languages.
Proposition 17 (Safety vs. Modules Ontology) Let L ontology language,
let O, O0 , O10 O0 ontologies L. Then:
1. O0 safe w.r.t. L iff empty ontology module O0 w.r.t. L.
2. O0 \ O10 safe O10 O10 module O0 w.r.t. L.
Proof. 1. Definition 2, O0 safe w.r.t. L iff (a) O0 deductive conservative
extension w.r.t. L. Definition 10, empty ontology O00 = module
O0 w.r.t. L iff (b) O0 deductive S-conservative extension O00 = w.r.t. L
= Sig(O). easy see (a) (b).
2. Definition 2, O0 \ O10 safe O10 w.r.t. L iff (c) O10 (O0 \ O10 ) = O0
deductive conservative extension O10 w.r.t. L. particular, O0 deductive
S-conservative extension O10 w.r.t. L = Sig(O), implies, Definition 10,
O10 module O0 w.r.t. L.
also provide analog Proposition 17 notions safety modules
signature:
Proposition 18 (Safety vs. Modules Signature) Let L ontology language,
O0 O10 O0 , ontologies L, subset signature L. Then:
1. O0 safe w.r.t. L iff empty ontology O00 = S-module O0 w.r.t. L.
2. O0 \ O10 safe Sig(O10 ) w.r.t. L, O10 S-module O0 w.r.t. L.
Proof. 1. Definition 6, O0 safe w.r.t. L iff (a) every Sig(O0 ) Sig(O)
S, case O0 safe w.r.t. L. Definition 12, O00 = S-module
O0 w.r.t. L iff (b) every Sig(O0 ) Sig(O) S, case O00 =
module O0 w.r.t. L. Claim 1 Proposition 17, easy see (a)
equivalent (b).
2. Definition 6, O0 \ O10 safe Sig(O10 ) w.r.t. L iff (c) every O,
Sig(O0 \ O10 ) Sig(O) Sig(O10 ), O0 \ O10 safe w.r.t. L.
288

fiModular Reuse Ontologies: Theory Practice

Definition 12, O10 S-module O0 w.r.t. L iff (d) every Sig(O0 )Sig(O) S,
O10 module O0 w.r.t. L.
order prove (c) implies (d), let Sig(O0 ) Sig(O) S. need
demonstrate O10 module O0 w.r.t. L.
(?)
0
0
0
0
0
0
Let := O1 . Note Sig(O \ O1 ) Sig(O) = Sig(O \ O1 ) (Sig(O) Sig(O1 ))
(Sig(O0 \ O10 ) Sig(O)) Sig(O10 ) Sig(O10 ). Hence, (c) O0 \ O10 safe
= O10 w.r.t. L implies Claim 2 Proposition 17 O10 module
O0 w.r.t. L (?).

4. Undecidability Complexity Results
section study computational properties tasks Table 2 ontology
languages correspond fragments description logic SHOIQ. demonstrate
reasoning tasks algorithmically unsolvable even relatively inexpressive DLs, computationally hard simple DLs.
Since notions modules safety defined Section 3 using notion
deductive conservative extension, reasonable identify (un)decidability
complexity results conservative extensions applicable reasoning tasks Table 2. computational properties conservative extensions recently studied
context description logics. Given O1 language L, problem
deciding whether deductive conservative extension O1 w.r.t. L 2-EXPTIMEcomplete L = ALC (Ghilardi et al., 2006). result extended Lutz et al.
(2007), showed problem 2-EXPTIME-complete L = ALCIQ undecidable L = ALCIQO. Recently, problem also studied simple DLs;
shown deciding deductive conservative extensions EXPTIME-complete
L = EL(Lutz & Wolter, 2007). results immediately applied notions
safety modules ontology:
Proposition 19 Given ontologies O0 L, problem determining whether
safe O0 w.r.t. L EXPTIME-complete L = EL, 2-EXPTIME-complete
L = ALC L = ALCIQ, undecidable L = ALCIQO. Given ontologies O, O0 ,
O10 O0 L, problem determining whether O10 module O0
EXPTIME-complete L = EL, 2-EXPTIME complete L = ALC L = ALCIQ,
undecidable L = ALCIQO.
Proof. Definition 2, ontology safe O0 w.r.t. L iff O0 deductive
conservative extension O0 w.r.t. L. Definition 2, ontology O10 module
O0 w.r.t. L O0 deductive S-conservative extension O10 = Sig(O) w.r.t.
L. Hence, algorithm checking deductive conservative extensions reused
checking safety modules.
Conversely, demonstrate algorithm checking safety modules
used checking deductive conservative extensions. Indeed, deductive conservative
extension O1 w.r.t. L iff O\O1 safe O1 w.r.t. L iff, Claim 1 Proposition 17,
O0 = module O1 \ O1 w.r.t. L .

289

fiCuenca Grau, Horrocks, Kazakov, & Sattler

Corollary 20 exist algorithms performing tasks T1, T3[a,s,u]m Table 2
L = EL L = ALCIQ run EXPTIME 2-EXPTIME respectively.
algorithm performing tasks T1, T3[a,s,u]m Table 2 L = ALCIQO.
Proof. task T1 corresponds directly problem checking safety ontology,
given Definition 2.
Suppose 2-EXPTIME algorithm that, given ontologies O, O0 O10
0
, determines whether O10 module O0 w.r.t. L = ALCIQ. demonstrate
algorithm used solve reasoning tasks T3am, T3sm T3um
L = ALCIQ 2-EXPTIME. Indeed, given ontologies O0 , one enumerate
subsets O0 check 2-EXPTIME subsets modules O0
w.r.t. L. determine modules minimal return them,
one them, union depending reasoning task.
Finally, prove solving reasoning tasks T3am, T3sm T3um
easier checking safety ontology. Indeed, Claim 1 Proposition 17,
ontology safe O0 w.r.t. L iff O0 = module O0 w.r.t. L. Note
empty ontology O0 = module O0 w.r.t. L iff O0 = minimal
module O0 w.r.t. L.
demonstrated reasoning tasks T1 T3[a,s,u]m computationally
unsolvable DLs expressive ALCQIO, 2-EXPTIME-hard ALC.
remainder section, focus computational properties reasoning
tasks T2 T4[a,s,u]m related notions safety modules signature.
demonstrate reasoning tasks undecidable DLs expressive
ALCO.
Theorem 21 (Undecidability Safety Signature) problem checking
whether ontology consisting single ALC-axiom safe signature undecidable w.r.t. L = ALCO.
Proof. proof variation construction undecidability deductive conservative extensions ALCQIO (Lutz et al., 2007), based reduction domino tiling
problem.
domino system triple = (T, H, V ) = {1, . . . , k} finite set tiles
H, V horizontal vertical matching relations. solution domino
system mapping ti,j assigns every pair integers i, j 1 element ,
hti,j , ti,j+1 H hti,j , ti+1,j V . periodic solution domino system
solution ti,j exist integers 1 , n 1 called periods
ti+m,j = ti,j ti,j+n = ti,j every i, j 1.
Let set domino systems, Ds subset admit solution
Dps subset Ds admit periodic solution. well-known (Borger, Gradel,
& Gurevich, 1997, Theorem 3.1.7) sets \ Ds Dps recursively inseparable,
is, recursive (i.e. decidable) subset D0 domino systems
Dps D0 Ds .
use property reduction. domino system D, construct
signature = S(D), ontology = O(D) consists single ALC-axiom
that:
290

fiModular Reuse Ontologies: Theory Practice

(q1 )

> v A1 Ak

= {1, . . . , k}

(q2 )

u At0 v
F
v rH .( ht,t0 iH At0 )
F
v rV .( ht,t0 iV At0 )

1 < t0 k

(q3 )
(q4 )

1tk
1tk

Figure 2: ontology Otile = Otile (D) expressing tiling conditions domino system
(a) solution = O(D) safe = S(D) w.r.t. L = ALCO,

(b) periodic solution = O(D) safe = S(D) w.r.t. L = ALCO.
words, set D0 consisting domino systems = O(D)
safe = S(D) w.r.t. L = ALCO, Dps D0 Ds . Since \ Ds Dps
recursively inseparable, implies undecidability D0 hence problem
checking S-safe w.r.t. L = ALCO, otherwise one use problem
deciding membership D0 .
signature = S(D), ontology = O(D) constructed follows. Given
domino system = (T, H, V ), let consist fresh atomic concepts every
two atomic roles rH rV . Consider ontology Otile Figure 2 constructed D. Note
Sig(Otile ) = S. axioms Otile express tiling conditions domino system
D, namely (q1 ) (q2 ) express every domain element assigned unique tile
; (q3 ) (q4 ) express every domain element horizontal vertical matching
successors.
let atomic role B atomic concept s, B
/ S. Let := {}
where:

hF
:= > v s.
(Ci vDi )Otile (Ci u Di ) (rH .rV .B u rV .rH .B)
say rH rV commute interpretation = (I , ) domain
elements a, b, c, d1 d2 ha, bi rH , hb, d1 rV , ha, ci rV ,
hc, d2 rH , d1 = d2 . following claims easily proved:
Claim 1.

Otile (D) model rH rV commute, solution.

Indeed model = (, ) Otile (D) used guide construction solution
ti,j follows. every i, j 1, construct ti,j inductively together elements
ai,j hai,j , ai,j+1 rV hai,j , ai+1,j rH . set a1,1 element
.
suppose ai,j i, j 1 constructed. Since model axioms (q1 )
(q2 ) Figure 2, unique 1 k ai,j . set
ti,j := t. Since model axioms (q3 ) (q4 ) ai,j exist b, c
t0 , t00 hai,j , bi rH , hai,j , ci rV , ht, t0 H, ht, t00 V , b At0 ,
c At00 . case assign ai,j+1 := b, ai+1,j := c, ti,j+1 := t0 , ti+1,j := t00 .
Note values ai,j ti,j assigned two times: ai+1,j+1 ti+1,j+1
constructed ai,j+1 ai,j+1 . However, since rV rH commute I, value
291

fiCuenca Grau, Horrocks, Kazakov, & Sattler

ai+1,j+1 unique, (q2 ), value ti+1,j+1 unique. easy see
ti,j solution D.
Claim 2.

model Otile O, rH rV commute I.

Indeed, easy see Otile |= (> v s.[rH .rV .B u rV .rH .B]). Hence,
= (I , ) model Otile O, exist a, b, c, d1 d2 hx, ai sI
every x , ha, bi rH , hb, d1 rV , d1 B , ha, ci rV , hc, d2 rH ,
d2 (B)I . implies d1 6= d2 , so, rh rV commute I.
Finally, demonstrate = O(D) satisfies properties (a) (b).
order prove property (a) use sufficient condition safety given Proposition 8 demonstrate solution every interpretation
exists model J J |S = I|S . Proposition 8, imply safe
w.r.t. L.
Let arbitrary interpretation. Since solution, contra-positive
Claim 1 either (1) model Otile , (2) rH rV commute I.
demonstrate cases construct required model J
J |S = I|S .
Case (1). = (I , ) model Otile exists axiom (Ci v Di )
Otile 6|= (Ci v Di ). is, exists domain element
CiI 6 DiI . Let us define J identical except interpretation
atomic role define J sJ = {hx, ai | x }. Since interpretations
symbols remained unchanged, CiJ , DiJ , J |= (> v
s.[Ci u Dj ]). implies J |= , so, constructed model J
J |S = I|S .
Case (2). Suppose rH rV commute = (I , ). means
exist domain elements a, b, c, d1 d2 ha, bi rH , hb, d1 rV ,
ha, ci rV , hc, d2 rH , d1 6= d2 . Let us define J identical except
interpretation atomic role atomic concept B. interpret J
sJ = {hx, ai | x }. interpret B J B J = {d1 }. Note (rH .rV .B)J
(rV .rH .B)J since d1 6= d2 . So, J |= (> v s.[rH .rV .B u rV .rH .B])
implies J |= , thus, constructed model J
J |S = I|S .
order prove property (b), assume periodic solution ti,j
periods m, n 1. demonstrate S-safe w.r.t. L. purpose
construct ALCO-ontology O0 Sig(O) Sig(O0 ) O0 |= (> v ),
O0 6|= (> v ). imply safe O0 w.r.t. L = ALCO, and, hence,
safe w.r.t. L = ALCO.
define O0 every model O0 finite encoding periodic solution ti,j
periods n. every pair (i, j) 1 1 j n introduce
fresh individual ai,j define O0 extension Otile following axioms:
(p1 ) {ai1 ,j } v rV .{ai2 ,j }

(p2 ) {ai1 ,j } v rV .{ai2 ,j },

i2 = i1 + 1

mod

(p3 ) {ai,j1 } v rH .{ai,j2 }

(p4 ) {ai,j1 } v rH .{ai,j2 },
j2 = j1 + 1
F
(p5 ) > v 1im, 1jn {ai,j }

mod n

292

fiModular Reuse Ontologies: Theory Practice

purpose axioms (p1 )(p5 ) ensure rH rV commute every model
O0 . easy see O0 model corresponding every periodic solution
periods n. Hence O0 6|= (> v ). hand, Claim 2, since O0 contains
Otile , every model O0 O, rH rV commute. possible
O0 models, O0 |= (> v ).
direct consequence Theorem 21 Proposition 18 undecidability
problem checking whether subset ontology module signature:
Corollary 22 Given signature ALC-ontologies O0 O10 O0 , problem
determining whether O10 S-module O0 w.r.t. L = ALCO undecidable.
Proof. Claim 1 Proposition 18, S-safe w.r.t. L O0 = S-module
w.r.t. L. Hence algorithm recognizing modules signature L used
checking ontology safe signature L.
Corollary 23 algorithm perform tasks T2, T4[a,s,u]m L =
ALCO.
Proof. Theorem 21 directly implies algorithm task T2, since task
corresponds problem checking safety signature.
Solving reasoning tasks T4am, T4sm, T4um L least hard
checking safety ontology, since, Claim 1 Proposition 18, ontology
S-safe w.r.t. L iff O0 = (the minimal) S-module w.r.t. L.

5. Sufficient Conditions Safety
Theorem 21 establishes undecidability checking whether ontology expressed
OWL DL safe w.r.t. signature. undecidability result discouraging leaves
us two alternatives: First, could focus simple DLs problem
decidable. Alternatively, could look sufficient conditions notion safety
is, ontology satisfies conditions, guarantee safe;
converse, however, necessarily hold.
remainder paper focuses latter approach. go further,
however, worth noting Theorem 21 still leaves room investigating former
approach. Indeed, safety may still decidable weaker description logics, EL,
even expressive logics SHIQ. case SHIQ, however, existing
results (Lutz et al., 2007) strongly indicate checking safety likely exponentially
harder reasoning practical algorithms may hard design. said,
follows focus defining sufficient conditions safety use practice
restrict OWL DLthat is, SHOIQontologies.
5.1 Safety Classes
general, sufficient condition safety defined giving, signature
S, set ontologies language satisfy condition signature.
ontologies guaranteed safe signature consideration.
intuitions lead notion safety class.
293

fiCuenca Grau, Horrocks, Kazakov, & Sattler

Definition 24 (Class Ontologies, Safety Class). class ontologies language
L function O() assigns every subset signature L, subset O(S)
ontologies L. class O() anti-monotonic S1 S2 implies O(S2 ) O(S1 );
compact O(S Sig(O)) O(S); subset-closed O1 O2
O2 O(S) implies O1 O(S); union-closed O1 O(S) O2 O(S) implies
(O1 O2 ) O(S).
safety class (also called sufficient condition safety) ontology language L
class ontologies O() L case (i) O(S),
(ii) ontology O(S) S-safe L.

Intuitively, class ontologies collection sets ontologies parameterized
signature. safety class represents sufficient condition safety: ontology O(S)
safe S. Also, w.l.o.g., assume empty ontology belongs
every safety class every signature. follows, whenever ontology belongs
safety class given signature safety class clear context,
sometimes say passes safety test S.
Safety classes may admit many natural properties, given Definition 24. Antimonotonicity intuitively means ontology proved safe w.r.t.
using sufficient condition, proved safe w.r.t. every subset S.
Compactness means sufficient consider common elements Sig(O)
checking safety. Subset-closure (union closure) means (O1 O2 ) satisfy
sufficient condition safety, every subset (the union O1 O2 ) also satisfies
condition.
5.2 Locality
section introduce family safety classes L = SHOIQ based
semantic properties underlying notion model conservative extensions. Section 3,
seen that, according Proposition 8, one way prove S-safe show
model S-conservative extension empty ontology.
following definition formalizes classes ontologies, called local ontologies,
safety proved using Proposition 8.
Definition 25 (Class Interpretations, Locality). Given SHOIQ signature S,
say set interpretations local w.r.t. every SHOIQ-interpretation
exists interpretation J I|S = J |S .
class interpretations function I() given SHOIQ signature returns set
interpretations I(S); local I(S) local w.r.t. every S; monotonic S1 S2
implies I(S1 ) I(S2 ); compact every S1 , S2 (S1 S2 ) =
I(S1 )|S = I(S2 )|S , S1 S2 symmetric difference sets S1
S2 defined S1 S2 := S1 \ S2 S2 \ S1 .
Given class interpretations I(), say O() class ontologies O() based
I() every S, O(S) set ontologies valid I(S); I() local
say O() class local ontologies, every O(S) every
O, say local (based I()).


294

fiModular Reuse Ontologies: Theory Practice

r
Example 26 Let IA
() class SHOIQ interpretations defined follows. Given
r
signature S, set IA (S) consists interpretations J rJ = every atomic
r
role r
/ AJ = every atomic concept
/ S. easy show IA
(S)


local every S, since every interpretation = ( , ) interpretation
J = (J , J ) defined J := , rJ = r
/ S, AJ =
/ S, X J := X
r
r
r
remaining symbols X, J IA (S) I|S = J |S . Since IA
(S1 ) IA
(S2 )
r
r
every S1 S2 , case IA () monotonic; IA () also compact, since
r
r
(S2 ) defined differently
(S1 ) IA
every S1 S2 sets interpretations IA
elements S1 S2 .
r
r
Given signature S, set AxA
(S) axioms local w.r.t. based IA
(S)
r
consists axioms every J IA (S), case J |= .
r
r
r
(S).
(S) iff AxA
() defined OA
class local ontologies based IA


Proposition 27 (Locality Implies Safety) Let O() class ontologies SHOIQ
based local class interpretations I(). O() subset-closed union-closed
safety class L = SHOIQ. Additionally, I() monotonic, O() anti-monotonic,
I() compact O() also compact.
Proof. Assume O() class ontologies based I(). Definition 25
every SHOIQ signature S, O(S) iff valid I(S) iff J |= every
interpretation J I(S). Since I() local class interpretations,
every SHOIQ-interpretation exists J I(S) J |S = I|S . Hence
every I(S) every SHOIQ interpretation model J I(S)
J |S = I|S , implies Proposition 8 safe w.r.t. L = SHOIQ. Thus
O() safety class.
fact O() subset-closed union-closed follows directly Definition 25
since (O1 O2 ) O(S) iff (O1 O2 ) valid I(S) iff O1 O2 valid I(S) iff
O1 O(S) O2 O(S). I() monotonic I(S1 ) I(S2 ) every S1 S2 ,
O(S2 ) implies valid I(S2 ) implies valid I(S1 )
implies O(S1 ). Hence O() anti-monotonic.
I() compact every S1 , S2 (S1 S2 ) =
I(S1 )|S = I(S2 )|S , hence every Sig(O) valid
I(S1 ) iff valid I(S2 ), so, O(S1 ) iff O(S2 ). particular, O(S) iff
O(S Sig(O)) since (S (S Sig(O))) Sig(O) = (S \ Sig(O)) Sig(O) = . Hence,
O() compact.
r
Corollary 28 class ontologies OA
(S) defined Example 26 anti-monotonic
compact subset-closed union-closed safety class.

Example 29 Recall Example 5 Section 3, demonstrated ontology
P1 Q given Figure 1 P1 = {P1, . . . , P4, E1} deductive conservative extension
Q = {Cystic Fibrosis, Genetic Disorder}. done showing every
S-interpretation expanded model J axioms P1P4, E1 interpreting
symbols Sig(P1 ) \ empty set. terms Example 26 means
295

fiCuenca Grau, Horrocks, Kazakov, & Sattler

r
r
P1 OA
(S). Since OA
() class local ontologies, Proposition 27, ontology
P1 safe w.r.t. L = SHOIQ.


Proposition 27 Example 29 suggest particular way proving safety ontologies. Given SHOIQ ontology signature sufficient check whether
r
(S); is, whether every axiom satisfied every interpretation
OA
r
IA (S). property holds, must safe according Proposition 27.
turns notion provides powerful sufficiency test safety works
surprisingly well many real-world ontologies, shown Section 8. next
section discuss perform test practice.
5.3 Testing Locality
r
section, focus detail safety class OA
(), introduced Example 26. ambiguity arise, refer safety class simply locality.2
r
definition AxA
(S) given Example 26 easy see axiom
r
local w.r.t. (based IA (S)) satisfied every interpretation fixes interpretation atomic roles concepts outside empty set. Note defining
locality fix interpretation individuals outside S, principle,
could done. reason elegant way describe interpretations.
Namely, every individual needs interpreted element domain,
canonical element every domain choose.
order test locality w.r.t. S, sufficient interpret every atomic concept
atomic role empty set check satisfied interpretations
remaining symbols. observation suggests following test locality:

Proposition 30 (Testing Locality) Given SHOIQ signature S, concept C, axiom
ontology let (C, S), (, S) (O, S) defined recursively follows:
(C, S) ::=

(>, S)
| (A, S)
| ({a}, S)
| (C1 u C2 , S)
| (C1 , S)
| (R.C1 , S)
| (> n R.C 1 , S)

= >;
=
/ otherwise = A;
= {a};
= (C1 , S) u (C2 , S);
= (C1 , S);
= Sig(R) * otherwise = R. (C1 , S);
= Sig(R) * otherwise = (> n R. (C1 , S)).

(C1 v C2 , S) = ( (C1 , S) v (C2 , S));
| (R1 v R2 , S) = ( v ) Sig(R1 ) * S, otherwise
= R1 .> v Sig(R2 ) * S, otherwise = (R1 v R2 );
| (a : C, S)
= : (C, S);
| (r(a, b), S)
= > v r
/ otherwise = r(a, b);
| (Trans(r), S) = v r
/ otherwise = Trans(r);
| (Funct(R), S) = v Sig(R) * otherwise = Funct(R).

(O, S) ::=
(, S)
(, S) ::=

2. notion locality exactly one used previous work (Cuenca Grau et al., 2007).

296

(a)
(b)
(c)
(d)
(e)
(f )
(g)
(h)
(i)
(j)
(k)
(l)
(m)
(n)

fiModular Reuse Ontologies: Theory Practice

r
Then, OA
(S) iff every axiom (O, S) tautology.

Proof. easy check every atomic concept atomic role r (C, S),
r S, words, atomic concepts roles
eliminated transformation.3 also easy show induction every
r
interpretation IA
(S), C = ( (C, S))I |= iff |= (, S). Hence
r
axiom local w.r.t. iff |= every interpretation IA
(S) iff |= (, S)
r
every Sig()-interpretation IA (S) iff (, S) tautology.
Example 31 Let = {} ontology consisting axiom = M2 Figure 1.
demonstrate using Proposition 30 local w.r.t. S1 = {Fibrosis, Genetic Origin},
local w.r.t. S2 = {Genetic Fibrosis, Origin}.
Indeed, according Proposition 30, order check whether local w.r.t. S1
sufficient perform following replacements (the symbols S1 underlined):

M2

[by (f)]
[by (b)]
}|
{
z
}|
{
z
Genetic Fibrosis Fibrosis u Origin.Genetic Origin

(7)

Similarly, order check whether local w.r.t. S2 , sufficient perform
following replacements (the symbols S2 underlined):
[by (b)]
[by (b)]
}|
{
z }| {
z
M2 Genetic Fibrosis Fibrosis u Origin.Genetic Origin

(8)

first case obtain (M2, S1 ) = ( Fibrosis u ) SHOIQ-tautology.
Hence local w.r.t. S1 hence Proposition 8 S1 -safe w.r.t. SHOIQ.
second case (M2, S2 ) = (Genetic Fibrosis u Origin.) SHOIQ
tautology, hence local w.r.t. S2 .

5.4 Tractable Approximation Locality
One important conclusions Proposition 30 one use standard capabilities available DL-reasoners FaCT++ (Tsarkov & Horrocks, 2006), RACER
(Moller & Haarslev, 2003), Pellet (Sirin & Parsia, 2004) KAON2 (Motik, 2006) testing
locality since reasoners, among things, allow testing DL-tautologies. Checking tautologies description logics is, theoretically, difficult problem (e.g.
DL SHOIQ known NEXPTIME-complete, Tobies, 2000). are, however,
several reasons believe locality test would perform well practice. primary
reason sizes axioms need tested tautologies usually
relatively small compared sizes ontologies. Secondly, modern DL reasoners
highly optimized standard reasoning tasks behave well realistic ontologies.
case reasoning costly, possible formulate tractable approximation
locality conditions SHOIQ:
3. Recall constructors , C1 C2 , R.C, (6 n R.C) assumed expressed using >,
C1 u C2 , R.C (> n R.C), hence, particular, every role R Sig(R ) * occurs either
R .C, (> n R .C), R v R, R v R , Trans(R ), Funct(R ), hence eliminated. atomic
concepts
/ eliminated likewise. Note necessarily case Sig( (, S)) S,
since (, S) may still contain individuals occur S.

297

fiCuenca Grau, Horrocks, Kazakov, & Sattler

Definition 32 (Syntactic Locality SHOIQ). Let signature. following
grammar recursively defines two sets concepts Con(S) Con(S) signature S:
Con(S) ::= | C | C u C | C u C | R .C | R.C | (> n R .C) | (> n R.C ) .
Con(S) ::= > | C | C1 u C2 .

/ atomic concept, R (possibly inverse of) atomic role r
/ S, C
Con(S), {1, 2}.
concept, R role, C Con(S), C(i)
axiom syntactically local w.r.t. one following forms: (1) R v R,
(2) Trans(r ), (3) Funct(R ), (4) C v C, (5) C v C , (6) : C . denote
r
AxA
(S) set SHOIQ-axioms syntactically local w.r.t. S.
r
SHOIQ-ontology syntactically local w.r.t. AxA
(S). denote
r
OA (S) set SHOIQ ontologies syntactically local w.r.t. S.

Intuitively, syntactic locality provides simple syntactic test ensure axiom
r
satisfied every interpretation IA
(S). easy see inductive definitions


Con (S) Con (S) Definition 32 every interpretation = (I , )
r
IA
(S) case (C )I = (C )I = every C Con(S)

C Con(S). Hence, every syntactically local axiom satisfied every interpretation
r
IA
(S), obtain following conclusion:
r
r
Proposition 33 AxA
(S) AxA
(S).

Further, shown safety class SHOIQ based syntactic locality
enjoys properties Definition 24:
r
Proposition 34 class syntactically local ontologies OA
() given Definition 32
anti-monotonic, compact, subset-closed, union-closed safety class.
r
r
() safety class Proposition 33. Anti-monotonicity OA
()
Proof. OA



shown induction, proving Con (S2 ) Con (S1 ), Con (S2 ) Con(S1 )
r
r
AxA
(S2 ) AxA
(S1 ) S1 S2 . Also one show induction
r
r
r
r


AxA (S) iff AxA (S Sig()), OA
() compact. Since OA
(S)
r
r

iff AxA (S), OA () subset-closed union-closed.

Example 35 (Example 31 continued) easy see axiom M2 Figure 1
syntactically local w.r.t. S1 = {Fibrosis, Genetic Origin}. indicate sub-concepts
Con(S1 ):

M2

Con(S1 ) [matches ]
Con(S1 ) [matches R .C]
z
}|
{
z
}|
{
Genetic Fibrosis Fibrosis u Origin.Genetic Origin
(9)
|
{z
}
Con(S1 ) [matches C u C ]

easy show similar way axioms P1 P4, E1 Figure 1 syntactically local w.r.t. = {Cystic Fibrosis, Genetic Disorder}. Hence ontology P1 =
{P1, . . . , P4, E1} considered Example 29 syntactically local w.r.t. S.

298

fiModular Reuse Ontologies: Theory Practice

r (S)
IA

r, 6 :

r
IA
(S)
r
IA
(S)
rid (S)
IA

rJ

AJ

r (S)
IA





r
IA
(S)



r
IA
(S)



rid (S)
IA

J J
{hx, xi | x J }

r, 6 :

rJ

AJ



J

J J
{hx, xi | x J }

J
J

Table 3: Examples Different Local Classes Interpretations
converse Proposition 33 hold general since semantically
local axioms syntactically local. example, axiom = (A v B)
local w.r.t. every since tautology (and hence true every interpretation).
hand, easy see syntactically local w.r.t. = {A, B} according
Definition 32 since involves symbols only. Another example, tautology,
GCI = (r.A v r.B). axiom semantically local w.r.t. = {r}, since
(, S) = (r. v r.) tautology, syntactically local. examples show
limitation syntactic notion locality inability compare different
occurrences concepts given signature S. result, syntactic locality
detect tautological axioms. reasonable assume, however, tautological axioms
occur often realistic ontologies. Furthermore, syntactic locality checking
performed polynomial time matching axiom according Definition 32.
Proposition 36 exists algorithm given SHOIQ ontology sigr
nature S, determines whether OA
(S), whose running time polynomial
|O| + |S|, |O| |S| number symbols occurring respectively.4
5.5 Locality Classes
r
locality condition given Example 26 based class local interpretations IA
()
particular example locality used testing safety. classes
local interpretations constructed similar way fixing interpretations
elements outside different values. Table 3 listed several classes
local interpretations fix interpretation atomic roles outside either
empty set , universal relation , identity relation id ,
interpretation atomic concepts outside either empty set set
elements.
local class interpretations Table 3 defines corresponding class local
ontologies analogously Example 26. Table 4 listed classes together
examples typical types axioms used ontologies. axioms assumed
extension project ontology Figure 1. indicate axioms
local w.r.t. locality conditions assuming, usual, symbols
underlined.
seen Table 4 different types locality conditions appropriate
r
different types axioms. locality condition based IA
(S) captures domain
axiom P4, definition P5, disjointness axiom P6, functionality axiom P7

4. assume numbers number restrictions written using binary coding.

299

fiCuenca Grau, Horrocks, Kazakov, & Sattler

r


r


rid


r


r


rid


3

7

7

3

3

3

3

3

3

7

7

7

P6 Project u Bio Medicine v

3

3

3

7

7

7

P7 Funct(has Focus)

3

7

3

3

7

3

P8 Human Genome : Project

7

7

7

3

3

3

P9 Focus(Human Genome, Gene)

7

3

7

7

3

7

7

7

7

7

7

7



? Ax

Axiom

P4 Focus.> v Project
P5

E2

BioMedical Project Project u
u Focus.Bio Medicine

Focus.Cystic Fibrosis v
v Focus.Cystic Fibrosis

Table 4: Comparison Different Types Locality Conditions

neither assertions P8 P9, since individuals Human Genome Gene prevent
us interpreting atomic role Focus atomic concept Project empty
r
(S), atomic roles concepts outside
set. locality condition based IA
interpreted largest possible sets, capture assertions generally
poor types axioms. example, functionality axiom P7 captured
locality condition since atomic role Focus interpreted universal
relation , necessarily functional. order capture functionality
rid (S) rid (S), every atomic role outside
axioms, one use locality based IA

interpreted identity relation id interpretation domain. Note
modeling error E2 local given locality conditions. Note also
possible come locality condition captures axioms P4P9, since
P6 P8 together imply axiom = ({Human Genome} u Bio Medicine v )
uses symbols only. Hence, every subset P containing P6 P8 safe
w.r.t. S, cannot local w.r.t. S.
might possible come algorithms testing locality conditions
classes interpretation Table 3 similar ones presented Proposition 30.
r
example, locality based class IA
(S) tested Proposition 30,
case (a) definition (C, S) replaced following:
(A, S)

= >
/ otherwise =

(a0 )

r
rid (S), checking
remaining classes interpretations, IA
(S) IA
locality, however, straightforward, since clear eliminate
universal roles identity roles axioms preserve validity respective
classes interpretations.
Still, easy come tractable syntactic approximations locality
conditions considered section similar manner done Section 5.4. idea used Definition 32, namely define two sets
Con(S) Con(S) concepts signature interpreted empty

300

fiModular Reuse Ontologies: Theory Practice

Con(S) ::= C | C u C | C u C

Con(S) ::= > | C | C1 u C2

| R.C | > n R.C

r ():
IA

|

r ():
IA

|

r
IA
():

| R .C | (> n R .C )

r
IA
():

| R .C | > n R .C

rid ():
IA

| Rid .C | (> 1 Rid .C ) .

rid ():
IA

| (> Rid .C), 2 .

r (S) ::= C v C | C v C | : C
AxA
r
IA
():

| R v R | Trans(r ) | Funct(R )

r
IA
():

| R v R | Trans(r ) | r (a, b)

rid ():
IA

| Trans(rid ) | Funct(Rid )

Where:
, , r , r , rid 6 S;
Sig(R ), Sig(R ), Sig(Rid ) * S;
Con(S);
C Con(S), C(i)
C concept, R role

Figure 3: Syntactic Locality Conditions Classes Interpretations Table 3
set and, respectively, every interpretation class see situations DL-constructors produce elements sets. Figure 3 gave recursive
r (S) correspond classes r (S)
definitions syntactically local axioms AxA

interpretations Table 3, cases recursive definitions present
indicated classes interpretations.
5.6 Combining Extending Safety Classes
previous section gave examples several safety classes based different local
classes interpretations demonstrated different classes suitable different
types axioms. order check safety ontologies practice, one may try apply
different sufficient tests check succeeds. Obviously, gives
powerful sufficient condition safety, seen union safety classes
used tests.
Formally, given two classes ontologies O1 () O2 (), union (O1 O2 )()
class ontologies defined (O1 O2 )(S) = O1 (S)O2 (S). easy see Definition 24
O1 () O2 () safety classes union (O1 O2 )() safety
class. Moreover, safety classes also anti-monotonic subset-closed,
union anti-monotonic, respectively, subset-closed well. Unfortunately unionclosure property safety classes preserved unions, demonstrated
following example:
r
r
Example 37 Consider union (OA
OA
)() two classes local ontologies
r
r
OA () OA () defined Section 5.5. safety class union-closed since,
example, ontology O1 consisting axioms P4P7 Table 4 satisfies first
locality condition, ontology O2 consisting axioms P8P9 satisfies second locality
condition, union O1 O2 satisfies neither first second locality condition
and, fact, even safe shown Section 5.5.


shown Proposition 33, every locality condition gives union-closed safety class;
however, seen Example 37, union safety classes might longer unionclosed. One may wonder locality classes already provide powerful sufficient
301

fiCuenca Grau, Horrocks, Kazakov, & Sattler

conditions safety satisfy desirable properties Definition 24. Surprisingly
case certain extent locality classes considered Section 5.5.
Definition 38 (Maximal Union-Closed Safety Class). safety class O2 () extends
safety class O1 () O1 (S) O2 (S) every S. safety class O1 () maximal unionclosed language L O1 () union-closed every union-closed safety class O2 ()
extends O1 () every L O2 (S) implies O1 (S).

r
r
Proposition 39 classes local ontologies OA
() OA
() defined Section 5.5
maximal union-closed safety classes L = SHIQ.

Proof. According Definition 38, safety class O() maximal union-closed
language L, exists signature ontology L, (i)
/
O(S), (ii) safe w.r.t. L, (iii) every P O(S), case P
safe w.r.t. L; is, every ontology Q L Sig(Q) Sig(O P)
case P Q deductive conservative extension Q. demonstrate
r
r
possible O() = OA
() O() = OA
()
r
first consider case O() = OA () show modify proof
r
case O() = OA
().
Let ontology L satisfies conditions (i)(iii) above. define ontologies
P Q follows. Take P consist axioms v r.> v every atomic
r
concept atomic role r Sig(O) \ S. easy see P OA
(S). Take Q
consist tautologies form v v r.> every A, r S. Note
Sig(O P) Sig(Q) S. claim P Q deductive Sig(Q)-conservative
extension Q.
(])
r
Intuitively, ontology P chosen way P OA
(S) P Q
r
models IA
(S). Q ontology implies nothing tautologies
uses atomic concepts roles S.
r
r
Since
/ OA
(S), exists axiom
/ AxA
(S). Let
:= (, S) (, ) defined Proposition 30. shown proof
r
proposition, |= iff |= every IA
(S). Now, since |= P Q
r
models IA (S), case P Q |= . Proposition 30, since
r
contain individuals, Sig() = Sig(Q) and, since
/ AxA
(S),
tautology, thus Q 6|= . Hence, Definition 1, P Q deductive
Sig(Q)-conservative extension Q (]).
r
O() = OA
() proof repeated taking P consist axioms > v
r.> v A, r Sig(O) \ S, modifying (, ) discussed
Section 5.5.
difficulties extending proof Proposition 39 locality
classes considered Section 5.5. First, clear force interpretations roles
universal identity relation using SHOIQ axioms. Second, clear
define function (, ) cases (see related discussion Section 5.5). Note
also proof Proposition 39 work presence nominals, since
guarantee = (, S) contains symbols (see Footnote 3
r
r
p. 297). Hence probably room extend locality classes OA
() OA
()
L = SHOIQ preserving union-closure.
302

fiModular Reuse Ontologies: Theory Practice

6. Extracting Modules Using Safety Classes
section revisit problem extracting modules ontologies. shown
Corollary 23 Section 4, exists general procedure recognize extract
(minimal) modules signature ontology finite time.
techniques described Section 5, however, reused extracting particular
families modules satisfy certain sufficient conditions. Proposition 18 establishes
relationship notions safety module; precisely, subset O1
S-module provided \ O1 safe Sig(O1 ). Therefore, safety class
O() provide sufficient condition testing modulesthat is, order prove
O1 S-module O, sufficient show \ O1 O(S Sig(O1 )). notion
modules based property defined follows.
Definition 40 (Modules Based Safety Class).
Let L ontology language O() safety class L. Given ontology
signature L, say Om O()-based S-module \ Om
O(S Sig(Om )).

Remark 41 Note every safety class O(), ontology signature S, exists
least one O()-based S-module O, namely itself; indeed, Definition 24, empty
ontology = \ also belongs O(S) every O() S.
Note also follows Definition 40 Om O()-based S-module iff
Om O()-based S0 -module every S0 S0 (S Sig(Om )).

clear that, according Definition 40, procedure checking membership
safety class O() used directly checking whether Om module based O().
order extract O()-module, sufficient enumerate possible subsets
ontology check subsets module based O().
practice, however, possible avoid checking possible subsets
input ontology. Figure 4 presents optimized version module-extraction algorithm.
procedure manipulates configurations form Om | Ou | Os , represent
partitioning ontology three disjoint subsets Om , Ou Os . set Om
accumulates axioms extracted module; set Os intended safe w.r.t.
Sig(Om ). set Ou , initialized O, contains unprocessed axioms.
axioms distributed among Om Os according rules R1 R2. Given axiom
Ou , rule R1 moves Os provided Os remains safe w.r.t. Sig(Om ) according
safety class O(). Otherwise, rule R2 moves Om moves axioms
Os back Ou , since Sig(Om ) might expand axioms Os might become longer
safe w.r.t. Sig(Om ). end process, axioms left Ou ,
set Om O()-based module O.
rewrite rules R1 R2 preserve invariants I1I3 given Figure 4. Invariant I1
states three sets Om , Ou Os form partitioning O; I2 states set Os
satisfies safety test Sig(Om ) w.r.t. O(); finally, I3 establishes rewrite
rules either add elements Om , add elements Os without changing Om ;
words, pair (|Om |, |Os |) consisting sizes sets increases lexicographical
order.
303

fiCuenca Grau, Horrocks, Kazakov, & Sattler

Input:

ontology O, signature S, safety class O()

Output:

module Om based O()
unprocessed


Configuration: Om | Ou | Os ;




module

safe

Initial Configuration =

|O|

Termination Condition: Ou =

Rewrite rules:
R1. Om | Ou {} | Os = Om | Ou | Os {}

(Os {}) O(S Sig(Om ))

R2. Om | Ou {} | Os = Om {} | Ou Os |

(Os {}) 6 O(S Sig(Om ))

Invariants Om | Ou | Os :
I1. = Om ] Ou ] Os

0 | O0 | O0 :
Invariant Om | Ou | Os = Om
u

0 |, |O 0 |)
I3. (|Om |, |Os |) <lex (|Om


I2. Os O(S Sig(Om ))
Figure 4: Procedure Computing Modules Based Safety Class O()
Proposition 42 (Correctness Procedure Figure 4) Let O() safety
class ontology language L, ontology L, signature L. Then:
(1) procedure Figure 4 input O, S, O() terminates returns O()based S-module Om O;
(2) If, additionally, O() anti-monotonic, subset-closed union-closed,
unique minimal O()-based S-module O, procedure returns precisely
minimal module.
Proof. (1) procedure based rewrite rules Figure 4 always terminates
following reasons: (i) every configuration derived rewrite rules, sets Om ,
Ou Os form partitioning (see invariant I1 Figure 4), therefore size
every set bounded; (ii) rewrite step, (|Om |, |Os |) increases lexicographical
order (see invariant I3 Figure 4). Additionally, Ou 6= always possible
apply one rewrite rules R1 R2, hence procedure always terminates
Ou = . Upon termination, invariant I1 Figure 4, partitioned Om Os
invariant I2, Os O(S Sig(Om )), implies, Definition 40, Om
O()-based S-module O.
(2) Now, suppose that, addition, O() anti-monotonic, subset-closed, union0 O()-based S-module O. demonstrate
closed safety class, suppose Om
induction every configuration Om | Ou | Os derivable | | rewrite
0 . prove module computed
rules R1 R2, case Om Om
procedure subset every O()-based S-module O, hence, smallest
O()-based S-module O.
0 . rewrite rule R1 change
Indeed, base case Om = Om
set Om . rewrite rule R2 have: Om | Ou {} | Os = Om {} | Ou Os |
(Os {}) 6 O(S Sig(Om )).
(])
304

fiModular Reuse Ontologies: Theory Practice

Input: ontology O, signature S, safety class O()
Output: module Om based O()
Initial Configuration =

|O|

Termination Condition: Ou =

Rewrite rules:
R1.

Om | Ou {} | Os = Om | Ou | Os {}

{} O(S Sig(Om ))

R2. Om | Ou {} | Os Os = Om {} | Ou Os | Os {} 6 O(S Sig(Om )),
Sig(Os ) Sig() Sig(Om )
Figure 5: Optimized Procedure Computing Modules Based Compact SubsetClosed Union-Closed Safety Class O()
0 {} * 0 . 0 := \ 0 .
Suppose, contrary, Om Om




0 O()-based S-module O, 0 O(S Sig(O 0 )). Since O()
Since Om


0 )). Since
0
subset-closed, {} O(S Sig(Om
Om O() anti-monotonic,
{} O(S Sig(Om )). Since invariant I2 Figure 4, Os O(S Sig(Om ))
O() union-closed, Os {} O(S Sig(Om )), contradicts (]). contradiction
0 .
implies rule R2 also preserves property Om Om

Claim (1) Proposition 42 establishes procedure Figure 4 terminates
every input produces module based given safety class. Moreover, possible
show procedure runs polynomial time assuming safety test
also performed polynomial time.
safety class O() satisfies additional desirable properties, like based classes
local interpretations described Section 5.2, procedure, fact, produces smallest
possible module based safety class, stated claim (2) Proposition 42.
case, possible optimize procedure shown Figure 4. O() union closed,
then, instead checking whether (Os {}) O(S Sig(Om )) conditions rules
R1 R2, sufficient check {} O(S Sig(Om )) since already known
Os O(S Sig(Om )). O() compact subset closed, instead moving
axioms Os Ou rule R2, sufficient move axioms Os contain
least one symbol occur Om before, since set remaining
axioms stay O(S Sig(Om )). Figure 5 present optimized version
algorithm Figure 4 locality classes.
Example 43 Table 5 present trace algorithm Figure 5 ontology
consisting axioms M1M5 Figure 1, signature = {Cystic Fibrosis, Genetic Disorder}
r
safety class O() = OA
() defined Example 26. first column table lists
configurations obtained initial configuration | | applying rewrite
rules R1 R2 Figure 5; row, underlined axiom one
tested safety. second column table shows elements Sig(Om )
appeared current configuration present preceding
configurations. last column indicate whether first conditions rules R1
r
R2 fulfilled selected axiom Ou is, whether local IA
().
rewrite rule corresponding result test applied configuration.
305

fiCuenca Grau, Horrocks, Kazakov, & Sattler

Om | Ou , | Os

New elements Sig(Om )

1 | M1, M2, M3, M4, M5 | Cystic Fibrosis, Genetic Disorder

{} O(S Sig(Om ))?
Yes



R1

2

| M1, M3, M4, M5 | M2



Yes



R1

3

| M1, M4, M5 | M2, M3







R2

4

M1 | M2, M3, M4, M5 |

Fibrosis, located In, Pancreas,
Origin, Genetic Origin





R2

5

M1, M3 | M2, M4, M5 |

Genetic Fibrosis





R2

6

M1, M3, M4 | M2, M5 |



Yes



R1

7

M1, M3, M4 | M2 | M5







R2

8

M1, M2, M3, M4 | | M5



Table 5: trace Procedure Figure 5 input Q = {M1, . . . , M5} Figure 1
= {Cystic Fibrosis, Genetic Disorder}

Note axioms tested safety several times different configurations,
set Ou may increase applications rule R2; example, axiom
= M2 tested safety configurations 1 7, = M3 configurations
2 4. Note also different results locality tests obtained cases:
M2 M3 local w.r.t. Sig(Om ) Om = , became non-local
new axioms added Om . also easy see that, case, syntactic locality
produces results tests.
example, rewrite procedure produces module Om consisting axioms M1
M4. Note possible apply rewrite rules different choices axiom
Ou , results different computation. words, procedure
Figure 5 implicit non-determinism. According Claim (2) Proposition 42
r
computations produce module Om , smallest OA
()-based
S-module O; is, implicit non-determinism procedure Figure 5
impact result procedure. However, alternative choices
may result shorter computations: example could selected axiom M1
first configuration instead M2 would led shorter trace consisting
configurations 1, 48 only.

worth examining connection S-modules ontology based
particular safety class O() actual minimal S-modules O. turns
O()-based module Om guaranteed cover set minimal modules, provided
O() anti-monotonic subset-closed. words, given S, Om contains
S-essential axioms O. following Lemma provides main technical argument
underlying result.
Lemma 44 Let O() anti-monotonic subset-closed safety class ontology language L, ontology, signature L. Let O1 S-module w.r.t. L
Om O()-based S-module O. O2 := O1 Om S-module w.r.t. L.
306

fiModular Reuse Ontologies: Theory Practice

Proof. Definition 40, since Om O()-based S-module O, \ Om
O(S Sig(Om )). Since O1 \ O2 = O1 \ Om \ Om O() subset-closed, case
O1 \ O2 O(S Sig(Om )). Since O() anti-monotonic, O2 Om ,
O1 \ O2 O(S Sig(O2 )), hence, O2 O()-based S-module O1 . particular O2
S-module O1 w.r.t. L. Since O1 S-module w.r.t. L, O2 S-module
w.r.t. L.
Corollary 45 Let O() anti-monotonic, subset-closed safety class L Om
O()-based S-module O. Om contains S-essential axioms w.r.t. L.
Proof. Let O1 minimal S-module w.r.t. L. demonstrate O1 Om . Indeed,
otherwise, Lemma 44, O1 Om S-module w.r.t. L strictly contained
O1 . Hence Om superset every minimal S-module hence, contains
S-essential axioms w.r.t. L.
shown Section 3.4, axioms M1M4 essential ontology
signature considered Example 43. seen example locality-based
S-module extracted contains axioms, accordance Corollary 45.
case, extracted module contains essential axioms; general, however, localitybased modules might contain non-essential axioms.
interesting application modules pruning irrelevant axioms checking
axiom implied ontology O. Indeed, order check whether |=
suffices retrieve module Sig() verify implication holds w.r.t.
module. cases, sufficient extract module subset signature
which, general, leads smaller modules. particular, order test subsumption
pair atomic concepts, safety class used enjoys nice properties
suffices extract module one them, given following proposition:
Proposition 46 Let O() compact union-closed safety class ontology language
L, ontology A, B atomic concepts. Let OA O()-based module
= {A} O. Then:
1 |= := (A v B) {B v } O() OA |= ;
2 |= := (B v A) {> v B} O() OA |= ;
Proof. 1. Consider two cases: (a) B Sig(OA ) (b) B
/ Sig(OA ).
(a) Remark 41 case OA O()-based module = {A, B}.
Since Sig() S, Definition 12 Definition 1, case |= implies
OA |= .
(b) Consider O0 = {B v }. Since Sig(B v ) = {B} B
/ Sig(OA ), {B v }
O() O() compact, Definition 24 case {B v } O(S Sig(OA )).
Since OA O()-based S-module O, Definition 40, \ OA O(S Sig(OA )).
Since O() union-closed, case (O \ OA ) {B v } O(S Sig(OA )). Note
B v 6 OA , since B 6 Sig(OA ), hence (O \ OA ) {B v } = O0 \ OA , and,
Definition 40, OA O()-based S-module O0 . Now, since |= (A v B),
case O0 |= (A v ), hence, since OA module O0 = {A},
OA |= (A v ) implies OA |= (A v B).
307

fiCuenca Grau, Horrocks, Kazakov, & Sattler

2. proof case analogous Case 1: Case (a) applicable without
changes; Case (b) show OA O()-based module = {A} O0 =
{> v B}, and, hence, since O0 |= (> v A), case OB |= (> v A),
implies O0 |= (B v A).
r ()
Corollary 47 Let SHOIQ ontology A, B atomic concepts. Let OA
r () locality classes based local classes interpretations form r ()
OA

r
r ()
IA (), respectively, Table 3. Let OA module = {A} based OA
r (). |= (A v B) iff |= (A v B) iff
OB module = {B} based OA

OB |= (A v B).
r () {> v A} r ().
Proof. easy see {B v } OA


Proposition 46 implies module based safety class single atomic concept
used capturing either super-concepts (Case 1), sub-concepts (Case
2) B A, provided safety class captures, applied empty signature,
axioms form B v (Case 1) (> v B) (Case 2). is, B super-concept
sub-concept ontology module. property
used, example, optimize classification ontologies. order check
subsumption v B holds ontology O, sufficient extract module
= {A} using modularization algorithm based safety class
ontology {B v } local w.r.t. empty signature, check whether subsumption
holds w.r.t. module. purpose, convenient use syntactically tractable
approximation safety class use; example, one could use syntactic locality
conditions given Figure 3 instead semantic counterparts.
possible combine modularization procedures obtain modules smaller
ones obtained using procedures individually. example, order check
r ()-based module
subsumption |= (A v B) one could first extract OA
1
= {A} O; Corollary 47 module complete super-concepts
O, including Bthat is, atomic concept super-concept O, also
r ()-based module = {B}
super-concept M1 . One could extract OA
2
M1 which, Corollary 47, complete sub-concepts B M1 , including A.
Indeed, M2 S-module M1 = {A, B} M1 S-module original
ontology O. Proposition 46, therefore, case M2 also S-module O.

7. Related Work
seen Section 3 notion conservative extension valuable formalization ontology reuse tasks. problem deciding conservative extensions
recently investigated context ontologies (Ghilardi et al., 2006; Lutz et al., 2007;
Lutz & Wolter, 2007). problem deciding whether P Q deductive S-conservative
extension Q EXPTIME-complete EL (Lutz & Wolter, 2007), 2-EXPTIME-complete
w.r.t. ALCIQ (Lutz et al., 2007) (roughly OWL-Lite), undecidable w.r.t. ALCIQO
(roughly OWL DL). Furthermore, checking model conservative extensions already undecidable EL (Lutz & Wolter, 2007), ALC even semi-decidable (Lutz
et al., 2007).
308

fiModular Reuse Ontologies: Theory Practice

last years, rapidly growing body work developed
headings Ontology Mapping Alignment, Ontology Merging, Ontology Integration,
Ontology Segmentation (Kalfoglou & Schorlemmer, 2003; Noy, 2004a, 2004b). field
rather diverse roots several communities.
particular, numerous techniques extracting fragments ontologies purposes knowledge reuse proposed. techniques rely syntactically
traversing axioms ontology employing various heuristics determine
axioms relevant not.
example procedure algorithm implemented Prompt-Factor
tool (Noy & Musen, 2003). Given signature ontology Q, algorithm retrieves fragment Q1 Q follows: first, axioms Q mention
symbols added Q1 ; second, expanded symbols Sig(Q1 ).
steps repeated fixpoint reached. example Section 3, =
{Cystic Fibrosis, Genetic Disorder}, Q consists axioms M1M5 Figure 1, algorithm first retrieves axioms M1, M4, M5 containing terms, expands
symbols mentioned axioms, contains symbols Q.
step, remaining axioms Q retrieved. Hence, fragment extracted
Prompt-Factor algorithm consists axioms M1-M5. case, PromptFactor algorithm extracts module (though minimal one). general, however,
extracted fragment guaranteed module. example, consider ontology
Q = {A A, B v C} = (C v B). ontology Q inconsistent due
axiom A: axiom (and particular) thus logical consequence Q. Given
= {B, C}, Prompt-Factor algorithm extracts Q2 = {B v C}; however, Q2 6|= ,
Q2 module Q. general, Prompt-Factor algorithm may fail even Q
consistent. example, consider ontology Q = {> v {a}, v B}, = (A v r.A),
= {A}. easy see Q consistent, admits single element models,
satisfied every model; is, Q |= . case, Prompt-Factor
algorithm extracts Q1 = {A v B}, imply .
Another example Seidenbergs segmentation algorithm (Seidenberg & Rector, 2006),
used segmentation medical ontology GALEN (Rector & Rogers, 1999).
Currently, full version GALEN cannot processed reasoners, authors
investigate possibility splitting GALEN small segments processed
reasoners separately. authors describe segmentation procedure which, given set
atomic concepts S, computes segment ontology. description
procedure high-level. authors discuss concepts roles
included segment not. particular, segment contain
super- sub- concepts input concepts, concepts linked
input concepts (via existential restrictions) super-concepts, subconcepts; included concepts, also restrictions, intersection, union,
equivalent concepts considered including roles concepts contain,
together super-concepts super-roles sub-concepts
sub-roles. description procedure entirely clear whether works
classified ontology (which unlikely case GALEN since full version
GALEN classified existing reasoner), or, otherwise, super-
sub- concepts computed. also clear axioms included
309

fiCuenca Grau, Horrocks, Kazakov, & Sattler

segment end, since procedure talks inclusion concepts
roles.
different approach module extraction proposed literature (Stuckenschmidt
& Klein, 2004) consists partitioning concepts ontology facilitate visualization
navigation ontology. algorithm uses set heuristics measuring
degree dependency concepts ontology outputs graphical
representation dependencies. algorithm intended visualization technique,
establish correspondence nodes graph sets axioms
ontology.
common modularization procedures mentioned lack
formal treatment notion module. papers describing modularization
procedures attempt formally specify intended outputs procedures,
rather argue modules based intuitive notions.
particular, take semantics ontology languages account. might
possible formalize algorithms identify ontologies intuitionbased modularization procedures work correctly. studies beyond scope
paper.
Module extraction ontologies also investigated formal point view
(Cuenca Grau et al., 2006b). Cuenca Grau et al. (2006) define notion module QA
ontology Q atomic concept A. One requirements module Q
conservative extension QA (in paper QA called logical module Q).
paper imposes additional requirement modules, namely module QA
entail subsumptions original ontology atomic concepts involving
atomic concepts QA . authors present algorithm partitioning ontology
disjoint modules proved algorithm correct provided certain safety
requirements input ontology hold: ontology consistent,
contain unsatisfiable atomic concepts, safe axioms (which
terms means local empty signature). contrast, algorithm
present works ontology, including containing non-safe axioms.
growing interest notion modularity ontologies recently
reflected workshop modular ontologies5 held conjunction International
Semantic Web Conference (ISWC-2006). Concerning problem ontology reuse,
various proposals safely combining modules; proposals,
E-connections (Cuenca Grau, Parsia, & Sirin, 2006a), Distributed Description Logics
(Borgida & Serafini, 2003) Package-based Description Logics (Bao, Caragea, & Honavar,
2006) propose specialized semantics controlling interaction importing
imported modules avoid side-effects. contrast works, assume
reuse performed simply building logical union axioms modules
standard semantics, establish collection reasoning services,
safety testing, check side-effects. interested reader find literature
detailed comparison different approaches combining ontologies (Cuenca
Grau & Kutz, 2007).
5. information see homepage workshop http://www.cild.iastate.edu/events/womo.html

310

fiModular Reuse Ontologies: Theory Practice

8. Implementation Proof Concept
section, provide empirical evidence appropriateness locality safety
testing module extraction. purpose, implemented syntactic locar
r
lity checker locality classes OA
() OA
() well algorithm
extracting modules given Figure 5 Section 6.
r
First, show locality class OA
() provides powerful sufficiency test
r
safety works many real-world ontologies. Second, show OA
()-based
modules typically small compared size ontology modules
extracted using techniques. Third, report implementation ontology
editor Swoop (Kalyanpur, Parsia, Sirin, Cuenca Grau, & Hendler, 2006) illustrate
r
r
combination modularization procedures based classes OA
() OA
().
8.1 Locality Testing Safety
r
run syntactic locality checker class OA
() ontologies
library 300 ontologies various sizes complexity import
(Gardiner, Tsarkov, & Horrocks, 2006).6 ontologies P import ontology Q,
check P belongs locality class = Sig(P) Sig(Q).
turned 96 ontologies library import ontologies,
11 syntactically local (and hence also semantically local S). 11
non-local ontologies, 7 written OWL-Full species OWL (Patel-Schneider et al.,
2004) framework yet apply. remaining 4 non-localities due
presence so-called mapping axioms form B 0 ,
/ B 0 S.
Note axioms simply indicate atomic concepts A, B 0 two ontologies
consideration synonyms. Indeed, able easily repair non-localities
follows: replace every occurrence P B 0 remove axiom
ontology. transformation, 4 non-local ontologies turned local.

8.2 Extraction Modules
section, compare three modularization7 algorithms implemented
using Manchesters OWL API:8
A1: Prompt-Factor algorithm (Noy & Musen, 2003);
A2: segmentation algorithm proposed Cuenca Grau et al. (2006);
r
A3: modularisation algorithm (Algorithm 5), based locality class OA
().

aim experiments described section provide throughout comparison quality existing modularization algorithms since algorithm extracts
modules according requirements, rather give idea typical size
modules extracted real ontologies algorithms.
6. library available http://www.cs.man.ac.uk/~horrocks/testing/
7. section module understand result considered modularization procedures
may necessarily module according Definition 10 12
8. http://sourceforge.net/projects/owlapi

311

fiCuenca Grau, Horrocks, Kazakov, & Sattler

(a) Modularization
NCI
(a)
Modularization
NCI

(b) Modularization
Modularization
GALEN-Small
(b)
GALEN-Small

(c) Modularization
Modularization SNOMED
(c)
SNOMED

(d) Modularization
Modularization GALEN-Full
(d)
GALEN-Full

(e) Small
Small modules

GALEN-Full
(e)
modules
GALEN-Full

(f) Large
Large modules

GALEN-Full
(f)
modules
GALEN-Full

Figure 6: Distribution sizes syntactic locality-based modules: X-Axis gives
number concepts modules Y-Axis number modules extracted
size range.

312

fiModular Reuse Ontologies: Theory Practice

A2: Segmentation

A3: Loc.-based mod.

] Atomic

A1: Prompt-Factor

Concepts

Max.(%)

Avg.(%)

Max.(%)

Avg.(%)

Max.(%)

Avg.(%)

NCI

27772

87.6

75.84

55

30.8

0.8

0.08

SNOMED

255318

100

100

100

100

0.5

0.05

GO

Ontology

22357

1

0.1

1

0.1

0.4

0.05

SUMO

869

100

100

100

100

2

0.09

GALEN-Small

2749

100

100

100

100

10

1.7

GALEN-Full

24089

100

100

100

100

29.8

3.5

SWEET

1816

96.4

88.7

83.3

51.5

1.9

0.1

DOLCE-Lite

499

100

100

100

100

37.3

24.6

Table 6: Comparison Different Modularization Algorithms

test suite, collected set well-known ontologies available Web,
divided two groups:
Simple. group, included National Cancer Institute (NCI) Ontology,9
SUMO Upper Ontology,10 Gene Ontology (GO),11 SNOMED Ontology12 .
ontologies expressed simple ontology language simple structure;
particular, contain GCIs, definitions.
Complex. group contains well-known GALEN ontology (GALEN-Full),13
DOLCE upper ontology (DOLCE-Lite),14 NASAs Semantic Web Earth Environmental Terminology (SWEET)15 . ontologies complex since use many
constructors OWL DL and/or include significant number GCIs. case
GALEN, also considered version GALEN-Small commonly used
benchmark OWL reasoners. ontology almost 10 times smaller original
GALEN-Full ontology, yet similar structure.
Since benchmark ontology modularization use cases
available, systematic way evaluating modularization procedures. Therefore
designed simple experiment setup which, even may necessarily reflect
actual ontology reuse scenario, give idea typical module sizes.
ontology, took set atomic concepts extracted modules every atomic
concept. compare maximal average sizes extracted modules.
worth emphasizing algorithm A3 extract module
input atomic concept: extracted fragment also module whole signature,
typically includes fair amount concepts roles.
9.
10.
11.
12.
13.
14.
15.

http://www.mindswap.org/2003/CancerOntology/nciOncology.owl
http://ontology.teknowledge.com/
http://www.geneontology.org
http://www.snomed.org
http://www.openclinical.org/prj_galen.html
http://www.loa-cnr.it/DOLCE.html
http://sweet.jpl.nasa.gov/ontology/
313

fiCuenca Grau, Horrocks, Kazakov, & Sattler

r
r
(a) Concepts DNA Sequence (b) OA
(S)-based module (c) OA
(S)-based module
Microanatomy NCI
DNA Sequence NCI
Micro Anatomy fragment 7b

Figure 7: Module Extraction Functionality Swoop
results obtained summarized Table 6. table provides size
largest module average size modules obtained using
algorithms. table, clearly see locality-based modules significantly
smaller ones obtained using methods; particular, case SUMO,
DOLCE, GALEN SNOMED, algorithms A1 A2 retrieve whole ontology
module input signature. contrast, modules obtain using algorithm
significantly smaller size input ontology.
NCI, SNOMED, GO SUMO, obtained small locality-based modules. explained fact ontologies, even large, simple
structure logical expressivity. example, SNOMED, largest locality-based
module obtained approximately 0.5% size ontology, average size
modules 1/10 size largest module. fact, modules
obtained ontologies contain less 40 atomic concepts.
GALEN, SWEET DOLCE, locality-based modules larger. Indeed,
largest module GALEN-Small 1/10 size ontology, opposed 1/200
case SNOMED. DOLCE, modules even bigger1/3 size
ontologywhich indicates dependencies different concepts
ontology strong complicated. SWEET ontology exception: even
though ontology uses constructors available OWL, ontology heavily
underspecified, yields small modules.
Figure 6, detailed analysis modules NCI, SNOMED,
GALEN-Small GALEN-Full. Here, X-axis represents size ranges ob314

fiModular Reuse Ontologies: Theory Practice

tained modules Y-axis number modules whose size within given range.
plots thus give idea distribution sizes different modules.
SNOMED, NCI GALEN-Small, observe size modules
follows smooth distribution. contrast, GALEN-Full, obtained large number
small modules significant number big ones, medium-sized modules
in-between. abrupt distribution indicates presence big cycle dependencies
ontology. presence cycle spotted clearly Figure 6(f); figure
shows large number modules size 6515 6535 concepts.
cycle occur simplified version GALEN thus obtain smooth
distribution case. contrast, Figure 6(e) see distribution
small modules GALEN-Full smooth much similar one
simplified version GALEN.
considerable differences size modules extracted algorithms A1
A3 due fact algorithms extract modules according different requirements. Algorithm A1 produces fragment ontology contains input atomic
concept syntactically separated rest axiomsthat is, fragment
rest ontology disjoint signatures. Algorithm A2 extracts fragment
ontology module input atomic concept additionally semantically
separated rest ontology: entailment atomic concept
module atomic concept module hold original ontology. Since
algorithm based weaker requirements, expected extracts smaller
modules. surprising difference size modules significant.
order explore use results ontology design analysis,
integrated algorithm extracting modules ontology editor Swoop (Kalyanpur
et al., 2006). user interface Swoop allows selection input signature
retrieval corresponding module.16
Figure 7a shows classification concepts DNA Sequence Microanatomy
r
NCI ontology. Figure 7b shows minimal OA
()-based module DNA Sequence,
r
obtained Swoop. Recall that, according Corollary 47, OA
()-based module
atomic concept contains necessary axioms for, least, (entailed) super-concepts
O. Thus module seen upper ontology O. fact, Figure 7
shows module contains concepts path DNA Sequence
top level concept Anatomy Kind. suggests knowledge NCI
particular concept DNA Sequence shallow sense NCI knows
DNA Sequence macromolecular structure, which, end, anatomy kind. one
wants refine module including information ontology necessary
r
entail path DNA Sequence Micro Anatomy, one could extract OA
()based module Micro Anatomy fragment 7b. Corollary 47, module contains
sub-concepts Micro Anatomy previously extracted module. resulting
module shown Figure 7b.

16. tool downloaded http://code.google.com/p/swoop/

315

fiCuenca Grau, Horrocks, Kazakov, & Sattler

9. Conclusion
paper, proposed set reasoning problems relevant ontology
reuse. established relationships problems studied computability. Using existing results (Lutz et al., 2007) results obtained Section 4,
shown problems undecidable algorithmically unsolvable logic
underlying OWL DL. dealt problems defining sufficient conditions
solution exist, computed practice. introduced studied
notion safety class, characterizes sufficiency condition safety
ontology w.r.t. signature. addition, used safety classes extract modules
ontologies.
future work, would like study approximations produce small
modules complex ontologies like GALEN, exploit modules optimize ontology
reasoning.

References
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. IJCAI-05, Proceedings Nineteenth International Joint Conference Artificial Intelligence,
Edinburgh, Scotland, UK, July 30-August 5, 2005, pp. 364370. Professional Book
Center.
Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2003). Description Logic Handbook: Theory, Implementation, Applications.
Cambridge University Press.
Bao, J., Caragea, D., & Honavar, V. (2006). semantics linking importing
modular ontologies. Proceedings 5th International Semantic Web Conference
(ISWC-2006), Athens, GA, USA, November 5-9, 2006, Vol. 4273 Lecture Notes
Computer Science, pp. 7286.
Borger, E., Gradel, E., & Gurevich, Y. (1997). Classical Decision Problem. Perspectives
Mathematical Logic. Springer-Verlag. Second printing (Universitext) 2001.
Borgida, A., & Serafini, L. (2003). Distributed description logics: Assimilating information
peer sources. J. Data Semantics, 1, 153184.
Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2007). logical framework
modularity ontologies. IJCAI-07, Proceedings Twentieth International
Joint Conference Artificial Intelligence, Hyderabad, India, January 2007, pp. 298
304. AAAI.
Cuenca Grau, B., & Kutz, O. (2007). Modular ontology languages revisited. Proceedings
Workshop Semantic Web Collaborative Knowledge Acquisition, Hyderabad, India, January 5, 2007.
Cuenca Grau, B., Parsia, B., & Sirin, E. (2006a). Combining OWL ontologies using Econnections. J. Web Sem., 4 (1), 4059.
Cuenca Grau, B., Parsia, B., Sirin, E., & Kalyanpur, A. (2006b). Modularity web ontologies. Proceedings Tenth International Conference Principles Knowledge
316

fiModular Reuse Ontologies: Theory Practice

Representation Reasoning (KR-2006), Lake District United Kingdom, June
2-5, 2006, pp. 198209. AAAI Press.
Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2007). right amount:
extracting modules ontologies. Proceedings 16th International Conference World Wide Web (WWW-2007), Banff, Alberta, Canada, May 8-12, 2007,
pp. 717726. ACM.
Cuenca Grau, B., Horrocks, I., Kutz, O., & Sattler, U. (2006). ontologies fit
together?. Proceedings 2006 International Workshop Description Logics
(DL-2006), Windermere, Lake District, UK, May 30 - June 1, 2006, Vol. 189 CEUR
Workshop Proceedings. CEUR-WS.org.
Gardiner, T., Tsarkov, D., & Horrocks, I. (2006). Framework automated comparison description logic reasoners. Proceedings 5th International Semantic
Web Conference (ISWC-2006), Athens, GA, USA, November 5-9, 2006, Vol. 4273
Lecture Notes Computer Science, pp. 654667. Springer.
Ghilardi, S., Lutz, C., & Wolter, F. (2006). damage ontology? case conservative extensions description logics. Proceedings Tenth International
Conference Principles Knowledge Representation Reasoning (KR-2006),
Lake District United Kingdom, June 2-5, 2006, pp. 187197. AAAI Press.
Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). SHIQ RDF
OWL: making web ontology language. J. Web Sem., 1 (1), 726.
Horrocks, I., & Sattler, U. (2005). tableaux decision procedure SHOIQ. Proceedings
Nineteenth International Joint Conference Artificial Intelligence (IJCAI-05),
Edinburgh, Scotland, UK, July 30-August 5, 2005, pp. 448453. Professional Book
Center.
Kalfoglou, Y., & Schorlemmer, M. (2003). Ontology mapping: state art.
Knowledge Engineering Review, 18, 131.
Kalyanpur, A., Parsia, B., Sirin, E., Cuenca Grau, B., & Hendler, J. A. (2006). Swoop:
web ontology editing browser. J. Web Sem., 4 (2), 144153.
Lutz, C., Walther, D., & Wolter, F. (2007). Conservative extensions expressive description
logics. Proceedings Twentieth International Joint Conference Artificial
Intelligence (IJCAI-07), Hyderabad, India, January 2007, pp. 453459. AAAI.
Lutz, C., & Wolter, F. (2007). Conservative extensions lightweight description logic
EL. Proceedings 21st International Conference Automated Deduction
(CADE-21), Bremen, Germany, July 17-20, 2007, Vol. 4603 Lecture Notes Computer Science, pp. 8499. Springer.
Moller, R., & Haarslev, V. (2003). Description logic systems. Description Logic
Handbook, chap. 8, pp. 282305. Cambridge University Press.
Motik, B. (2006). Reasoning Description Logics using Resolution Deductive
Databases. Ph.D. thesis, Univesitat Karlsruhe (TH), Karlsruhe, Germany.
Noy, N. F. (2004a). Semantic integration: survey ontology-based approaches. SIGMOD
Record, 33 (4), 6570.
317

fiCuenca Grau, Horrocks, Kazakov, & Sattler

Noy, N. F. (2004b). Tools mapping merging ontologies. Staab, & Studer (Staab
& Studer, 2004), pp. 365384.
Noy, N., & Musen, M. (2003). PROMPT suite: Interactive tools ontology mapping
merging. Int. Journal Human-Computer Studies, Elsevier, 6 (59).
Patel-Schneider, P., Hayes, P., & Horrocks, I. (2004). Web ontology language OWL Abstract
Syntax Semantics. W3C Recommendation.
Rector, A., & Rogers, J. (1999). Ontological issues using description logic represent
medical concepts: Experience GALEN. IMIA WG6 Workshop, Proceedings.
Schmidt-Schau, M., & Smolka, G. (1991). Attributive concept descriptions complements. Artificial Intelligence, Elsevier, 48 (1), 126.
Seidenberg, J., & Rector, A. L. (2006). Web ontology segmentation: analysis, classification
use. Proceedings 15th international conference World Wide Web
(WWW-2006), Edinburgh, Scotland, UK, May 23-26, 2006, pp. 1322. ACM.
Sirin, E., & Parsia, B. (2004). Pellet system description. Proceedings 2004 International Workshop Description Logics (DL2004), Whistler, British Columbia,
Canada, June 6-8, 2004, Vol. 104 CEUR Workshop Proceedings. CEUR-WS.org.
Staab, S., & Studer, R. (Eds.). (2004). Handbook Ontologies. International Handbooks
Information Systems. Springer.
Stuckenschmidt, H., & Klein, M. (2004). Structure-based partitioning large class hierarchies. Proceedings Third International Semantic Web Conference (ISWC2004), Hiroshima, Japan, November 7-11, 2004, Vol. 3298 Lecture Notes Computer Science, pp. 289303. Springer.
Tobies, S. (2000). complexity reasoning cardinality restrictions nominals
expressive description logics. J. Artif. Intell. Res. (JAIR), 12, 199217.
Tsarkov, D., & Horrocks, I. (2006). FaCT++ description logic reasoner: System description.
Proceedings Third International Joint Conference Automated Reasoning
(IJCAR 2006), Seattle, WA, USA, August 17-20, 2006, Vol. 4130 Lecture Notes
Computer Science, pp. 292297. Springer.

318

fiJournal Artificial Intelligence Research 31 (2008) 83-112

Submitted 06/07; published 01/08

CUI Networks: Graphical Representation Conditional
Utility Independence
Yagil Engel
Michael P. Wellman

yagil@umich.edu
wellman@umich.edu

University Michigan, Computer Science & Engineering
2260 Hayward St, Ann Arbor, MI 48109-2121, USA

Abstract
introduce CUI networks, compact graphical representation utility functions
multiple attributes. CUI networks model multiattribute utility functions using
well-studied widely applicable utility independence concept. show conditional
utility independence leads effective functional decomposition exhibited
graphically, local, compact data graph nodes used calculate
joint utility. discuss aspects elicitation, network construction, optimization,
contrast new representation previous graphical preference modeling.

1. Introduction
Modern AI decision making based notion expected utility, probability distributions used weigh utility values possible outcomes.
representation probability distribution functions Markov Bayesian networks
(Pearl, 1988)exploiting conditional independence achieve compactness computational efficiencyhas led plethora new techniques applications. Despite
equal importance decision making, preferences utilities generally received
level attention AI researchers devoted beliefs probabilities.
(increasing) efforts develop representations inference methods utility achieved
degree success comparable impact graphical models probabilistic reasoning.
Recognizing utility functions multidimensional domains may also amenable
factoring based independence (Keeney & Raiffa, 1976), several aimed develop
models analogous benefits (Bacchus & Grove, 1995; Boutilier, Bacchus, & Brafman,
2001; La Mura & Shoham, 1999; Wellman & Doyle, 1992). goal well,
compare approach methods Related Work section (2.2).
development compact representations multiattribute utility begins
notion preferential independence (PI), separability subdomains outcome space.
subdomain outcomes separable PI sense preference order subdomain depend rest domain. subsets attributes induce
separable subdomains, ordinal utility (value) function decomposes additively
variables (Debreu, 1959; Fishburn, 1965; Gorman, 1968). cardinal utility function represents preferences outcomes also notion strength preferences,
notably represent preferences actions uncertain outcomes, lotteries. Direct
adaptation PI concept cardinal utility requires generalization notion:
set attributes utility independent (UI) preference order lotteries

c
2008
AI Access Foundation. rights reserved.

fiEngel & Wellman

induced subdomain depend values rest attributes. stronger
judgement assert preference order joint domain depends
margins attribute subsets. latter leads powerful additive decompositions,
either fully additive (when subsets attributes disjoint), generalized,
additive decomposition overlapping subsets (Fishburn, 1967; Bacchus & Grove, 1995).
Utility independence leads less convenient decompositions, multilinear (Keeney
& Raiffa, 1976) hierarchical (Von Stengel, 1988; Wellman & Doyle, 1992). previous
efforts AI community adapt modern graphical modeling utility functions employ generalized additive decomposition (Bacchus & Grove, 1995; Boutilier et al., 2001;
Gonzales & Perny, 2004). contrast, work continues thread, based
weaker utility independence assumption. elaborate difference types
independence following presentation formal definitions.

2. Background
utility-theoretic terminology follows definitive text Keeney Raiffa (1976).
multiattribute utility framework, outcome represented vector values
n variables, called attributes. decision makers preferences represented
total pre-order, , set outcomes. common applications decision makers
ability choose certain outcome, rather action results
probability distribution outcomes, also called lottery. decision maker hence
set possible lotteries. Given standard set
assumed preference order

axioms, represented real-valued utility function outcomes, U (),
numeric ranking probabilistic outcomes expected utility respects ordering
utility function unique positive affine transformations. positive linear
.
transform U () represents preferences, thus strategically equivalent.
ability represent utility probability distributions function outcomes
provides structure, multiattribute settings outcome space n-dimensional.
Unless n quite small, therefore, explicit (e.g., tabular) representation U ()
generally practical. Much research multiattribute utility theory aims
identify structural patterns enable compact representations. particular,
subsets attributes respect various independence relationships, utility function may
decomposed combinations modular subutility functions smaller dimension.
Let = {x1 , . . . , xn } set attributes. following definitions (and rest
work) capital letters denote subsets attributes, small letters (with without numeric
subscripts) denote specific attributes, X denotes complement X respect
S. denote (joint) domain X D(X), indicate specific attribute assignments
prime signs superscripts. represent instantiation subsets X
time use sequence instantiation symbols, X 0 0 .
order meaningfully discuss preferences subsets attributes, need notion
preferences subset given fixed values rest attributes.
0

0

Definition 1. Outcome 0 conditionally preferred outcome 00 given , 0
0
0
00 . denote conditional preference order given 0 .

84

fiCUI networks

0
Similarly define conditional preference order lotteries. preference order

0
lotteries represented conditional utility function, U (Y, ).
Definition 2. Preferential Independent (PI) 0 depend value
0
chosen .
Preferential independence useful qualitative preference assessment. Firstorder preferential independence (i.e., independence single attribute rest)
natural assumption many domains. example, typical purchase decisions greater
quantity higher quality desirable regardless values attributes. Preferential independence higher order, however, requires invariance tradeoffs among
attributes respect variation others, stringentthough still often
satisfiableindependence condition. standard PI condition applies subset respect full complement remaining attributes. conditional version PI specifies
independence respect subset complement, holding remaining attributes
fixed.
Definition 3. Conditionally Preferential Independent (CPI) X given Z (Z = XY ),
Z 0 , X 0 Z 0 depend value chosen X 0 . denote relationship
CPI(Y, X | Z).
counterpart preferential independence considers probability distributions
outcomes called utility independence.
Definition 4. Utility Independent (UI) , conditional preference order
0 , depend value chosen 0 .
lotteries ,

notations, apply UI conditions defined sets attributes
specific attributes.
Given UI(Y, X), taking X = , conditional utility function given X 0
invariant positive affine transformations, fixed value X 0 . fact
expressed decomposition
U (X, ) = f (X) + g(X)U (X 0 , ),

g() > 0.

Note functions f () g() may different particular choice X 0 . Since
U (X 0 , ) function , sometimes use notation UX 0 (Y ).
Utility independence conditional version well.
Definition 5. Conditionally Utility Independent (CUI) X given Z (Z = XY )
X 0 Z 0 depend value chosen X 0 . denote relationship
Z 0 ,
CUI(Y, X | Z).
CUI also supports functional decomposition. Z 0 , conditional utility function
given X 0 Z 0 strategically equivalent function given different instantiation
X. However, transformation depends X, also Z 0 . Hence
write:
U (X, Y, Z) = f (X, Z) + g(X, Z)U (X 0 , Y, Z), g() > 0.
(1)
85

fiEngel & Wellman

is, fix X arbitrary level X 0 use two transformation functions f
g get value U () levels X. stronger, symmetric form independence
leads additive decomposition utility function called additive independence.
provide definition conditional version.
Definition 6. X Conditionally Additive Independent given Z, CAI(X, | Z),
Z 0 depends marginal conditional probability disif instantiation Z 0 ,
0
tributions XZ Z 0 .
means value Z 0 , two probability distributions p, q
p(X, , Z 0 ) q(X, , Z 0 ), p(, Y, Z 0 ) q(, Y, Z 0 ), decision maker indifferent
p q. necessary (but always sufficient) condition hold
utility differences U (X 0 , Y, Z 0 ) U (X 00 , Y, Z 0 ) (for X 0 , X 00 ) depend
value .
CAI leads following decomposition (Keeney & Raiffa, 1976):
U (X, Y, Z) = f (X, Z) + g(Y, Z).
variations utility independence considered theoretical literature,
leading various decomposition results (Fishburn, 1975; Krantz, Luce, Suppes, & Tversky,
1971; Fuhrken & Richter, 1991).
2.1 Motivation
obvious benefit model based (conditional) utility independence
generality admitted weaker independence condition, comparison additive independence. Whereas additivity practically excludes interaction utility one
attribute subset (X Definition 6) value another (Y ), utility independence
allows substitutivity complementarity relationships, long risk attitude towards one variable affected value another. One could also argue UI
particularly intuitive, based invariance condition preference order.
contrast, (conditional) additive independence requires judgment effects joint
versus marginal probability distributions. Moreover, additive independence symmetric,
whereas condition U I(X, ) allow preference order depend X.
Bacchus Grove exemplify difference additive utility independence
simple state space two boolean attributes: Health Wealth. example,
shown Table 1, attributes additive independent (it immediately seen
using preference differences), H W complements: worth
sum one without other. would considered
two attributes substitutes if, example, U (W, H) = 4 U (W, H) = 3. cases
H W nonetheless preferential independent, since always prefer richer (all
else equal) healthier (all else equal). boolean variables, preferential
utility independence equivalent (we always prefer lotteries give higher probability
preferred level) therefore Health Wealth also UI other.
(Conditional) additive independence resulting additive decomposition
generalized multiple subsets necessarily disjoint. condition called

86

fiCUI networks

W
W

H
5
2

H
1
0

Table 1: Utility values Health Wealth example (Bacchus & Grove, 1995).
generalized additive independence (GAI). GAI holds, U () decomposes sum independent functions fi () GAI subsets Xi . shown Bacchus Grove, CAI
conditions accumulated global GAI decomposition (see Section 2.2). latter
may also exist without CAI conditions leading it, GAI condition hard
identify: whereas CAI condition corresponds independence two attributes
two subsets, global GAI condition intuitive interpretation.
next example, cardinal independence condition exists, except non symmetric CUI. example also shows difference PI UI, hence requires
domains H W include least three values each. also add third attribute outcome space, location (L), indicating whether live city
countryside (Table 2). order show U I(H, {W, L}) hold enough
find violated one pair lotteries. Given partial outcome Wr , Lci
prefer equal chance lottery < Hf , Hs >, whose expected utility 12+5
2 , sure
outcome Hg (value 8), whereas given Wp , Lci indifferent (expected utility 2
lotteries). Intuitively, may case additional value get fitness
(over good health) higher also rich, making significant value
Hg adds Hs . Similarly, U I(W, {H, L}) hold, comparing even-chance
gamble < Wr , Wp > sure outcome Wm , first given Hf , Lci given
Hs , Lci .
W H therefore utility independent, preferential independent.
L, however, not: rich would rather live city, way
round poor, except case poor sick prefer
city.

Wr
Wm
Wp

Hf
12
6
3

Lci
Hg
8
4
2

Hs
5
3
1

Hf
10
6
4

Lco
Hg
6
3
1.5

Hs
4
2
0

Table 2: Utility values Health, Wealth Location example. Wr means rich, Wm
medium income, Wp means poor. Hf healthy top fitness, Hg means good health,
Hs means sick. Lci stands city location, Lco means countryside location.
Therefore, symmetric independence condition exists here, rules additive multiplicative independence, conditional not, subsets attributes.
Also, since single variable unconditionally UI, subset unconditionally
UI. Further, fact preferences L depend combination H W rules
GAI decomposition form {W, L}, {W, H}, {H, L}.
87

fiEngel & Wellman

can, however, achieve decomposition using CUI. case CUI(W, L|H),
since column left matrix (Lci ) affine transformation counterpart
right side (Lco ). example, transform first column (Hf ), multiply 23
add 2.
example illustrates subtlety utility independence. particular, whereas
preferences L depend W , W may still (conditionally) UI L. CAI assumption attributes must inevitably ignore reversal preferences L
different values W , hence decision maker queried preferences
assumption may able provide meaningful answers.
interaction system requires preference representation normally requires
identification structure, population utility values required
compact representation. therefore important two aspects
simplified possible, whereas functional form handled system may
sophisticated. exactly tradeoff made CUI nets, compared GAI-based
representation: GAI condition based CAI, CUI nets achieve lower dimensionality
(Section 7), therefore easier elicitation. GAI condition based collection
CAI conditions, hard identify. CUI nets simplify bottleneck aspects,
driving complexity algorithms functional form handled
behind scenes.
2.2 Related Work
Perhaps earliest effort exploit separable preferences graphical model extension influence diagrams Tatman Shachter (1990) decompose value functions
sums products multiple value nodes. structure provided computational
advantages, enabling use dynamic programming techniques exploiting value separability.
Bacchus Grove (1995) first develop graphical model based conditional
independence structure. particular, establish CAI condition perfect
map (Pearl & Paz, 1989); is, graph attribute nodes node separation
reflects exactly set CAI conditions S. specifically, two sets nodes
X, S, CAI(X, |XY ) holds direct edge node X
node . use term CAI map referring graph reflects
perfect map CAI conditions, context preference order D(S). Bacchus
Grove go show utility function GAI decomposition set
maximal cliques CAI map. show Section 7, CUI network representation
developed achieves weakly better dimensionality CAI maps due greater
generality independence assumption.
Initiating another important line work, Boutilier et al. (1999) introduced CP networks, efficient representation ordinal preferences multiple attributes.
CP network, variable conditionally PI rest given parents. Ordinal multiattribute preference representation schemes (for decision making certainty),
especially CP networks, dramatically simplify preference elicitation process, based
intuitive relative preference statements avoid magnitude considerations.
However, limited expressive power CP networks may suffice complex decision

88

fiCUI networks

problems, tradeoff resolution may hinge complicated way attribute settings
rich domains. problem particularly acute continuous almost continuous
attributes involved, money time.
Boutilier et al. (2001) subsequently extended approach numeric, cardinal utility
UCP networks, graphical model utilizes GAI decomposition combined
CP-net topology. requires dominance relations parents children,
somewhat limiting applicability representation. GAI structure also
applied graphical models Gonzales Perny (2004), employ clique graph
CAI map (the GAI network ) elicitation purposes.
earlier work, La Mura Shoham (1999) redefine utility independence symmetric multiplicative condition, taking closer probability analog, supporting
Bayes-net like representation. Although multiplicative independence different additive independence, necessarily weaker. Recent work Abbas (2005) defines
subclass utility functions multiplicative notion UI obeys analog
Bayess rule.
graphical decomposition suggested past utility functions based
original, non-symmetric notion utility independence utility tree (Von Stengel,
1988, see also Wellman Doyle, 1992, discussion AI context). utility tree
decomposes utility function using multilinear multiplicative decomposition (Keeney
& Raiffa, 1976), tries decompose subset similarly. Using
hierarchical steps utility function becomes nested expression functions
smallest separable subsets complements.
2.3 Graphical Models CUI
concluding remarks, Bacchus Grove (1995) suggest investigating graphical
models independence concepts, particular utility independence. Founding
graphical model UI difficult, however, utility independence decompose
effectively additive independence. particular, condition U I(Y, X) ensures
subutility function, since X one harder carry
decomposition X. Hence case X large dimensionality
representation may remain high. approach therefore employs CUI conditions
large subsets , case decomposition driven decomposing
conditional utility function using CUI conditions.
sequel show serial application CUI leads functional decomposition.
corresponding graphical model, CUI network, provides lower-dimension representation utility function function vertex depends node
parents. demonstrate use CUI networks constructing example
relatively complex domain. Next elaborate technical semantic properties model knowledge required construct it. Subsequent technical sections
present optimization algorithms techniques reducing complexity
representation.

89

fiEngel & Wellman

3. CUI Networks
begin constructing DAG representing set CUI conditions, followed derivation functional decomposition nodes DAG.
3.1 CUI DAG
Suppose obtain set CUI conditions variable set = {x1 , . . . , xn },
x S, contains condition form
CUI(S \ ({x} P (x)) , x | P (x)).
words, exists set P (x) separates rest variables x.
P (x) always exists, P (x) = \ {x} condition trivially holds. set
represented graphically following procedure, name procedure C.
1. Define order set (for convenience assume ordering x1 , . . . , xn ).
2. Define set parents x1 P a(x1 ) = P (x1 ).
3. = 2, . . . , n
), set nodes
Define set intermediate descendants xi , Dn(x
x1 , . . . , xi1 turned descendants xi , xi
) smallest
parent another descendant xi parent. Formally, Dn(x
set satisfies following condition:
) P a(xj )
j {1, . . . , 1}, [xi P a(xj ), k {1, . . . , 1}.xk Dn(x
).] (2)
xj Dn(x
Define parents xi nodes P (xi ) already descendants xi ,
).
P a(xi ) = P (xi ) \ Dn(x
procedure defines DAG. denote Dn(x) final set descendants x.
set defined Equation (2), replacing {1, . . . , 1} {1, . . . , n}).

definitions, Dn(x) Dn(x),
hence

P a(x) Dn(x) P a(x) Dn(x)
= P (x).

(3)

Proposition 1. Consider DAG defined procedure C set attributes S.
x S,
CUI(S \ ({x} P a(x) Dn(x)) , x | P a(x) Dn(x)).
(4)

Proof. definitions P a(x) P (x), (4) holds replacing Dn(x) Dn(x).
definition CUI, straightforward
CUI(S \ (Y W ) , | W ) CUI(S \ (Y W Z) , | W Z),
invariance preference order \ (Y W ) implies invariance preference
order subset \ (Y W Z), difference set Z fixed. Given (3),


taking W = P a(x) Dn(x)
Z = Dn(x) \ Dn(x),
get (4).
90

fiCUI networks

example, show construction structure small set variables
= {x1 , x2 , x3 , x4 , x5 , x6 }, given following set CUI conditions:
= {CUI({x4 , x5 , x6 }, x1 | {x2 , x3 }), CUI({x4 , x3 , x6 }, x2 | {x1 , x5 }),
CUI({x2 , x4 , x6 }, x3 | {x1 , x5 }), CUI({x1 , x3 , x5 }, x4 | {x2 , x6 }),
CUI(x6 , x5 | {x1 , x2 , x3 , x4 }), CUI({x1 , x2 , x3 , x5 }, x6 | x4 )}.
Construction network using order implied indices results CUI
DAG illustrated Figure 1. minimal separating set x1 {x2 , x3 }. x2 , get
2 ) = {x1 }, non-descendant variable required separate
Dn(x
rest x5 , therefore parent. rest graph constructed
similar way. x4 placed, find P (x4 ) = {x2 , x6 }. Therefore, x4 becomes
2 ) {x4 } = {x1 , x4 }.
descendant x2 x2 placed, words Dn(x2 ) = Dn(x
ix6

ix5
Z

Z
=x

~ ix
Z

2



?
=x


4

3

Z

Z

~i
Z
=x

1

Figure 1: CUI DAG given order x1 ,. . . ,x6 .

Definition 7. Let U (S) utility function representing cardinal preferences D(S).
CUI DAG U () DAG, x S, (4) holds.
Procedure C yields CUI DAG Proposition 1. direction, given CUI
DAG G (in parents descendants denoted P aG (), DnG (), respectively)
constructed using C, follows. Define P (x) = P aG (x) DnG (x) variable
ordering according reverse topological order G, complete execution C.
straightforward show set parents selected xi exactly P aG (xi ),
hence result DAG identical G.
3.2 CUI Decomposition
show CUI conditions, guaranteed Proposition 1, applied iteratively decompose U () lower dimensional functions. first pick variable ordering
agrees reverse topological order CUI DAG. simplify presentation,
rename variables ordering x1 , . . . , xn . CUI condition (4) x1
implies following decomposition, according (1):
U (S) = f1 (x1 , P a(x1 ), Dn(x1 )) + g1 (x1 , P a(x1 ), Dn(x1 ))Ux01 (S \ {x1 }).

(5)

Note Dn(x1 ) = .
assume specified reference point 0 , arbitrary value chosen
attribute x S, denoted x0 . Ux01 () right hand side conditional
91

fiEngel & Wellman

utility function given x1 fixed reference point x01 . convenience omit
attributes whose values fixed list arguments.
applying decomposition based CUI condition x2 conditional
utility function Ux01 (), get
Ux01 (S \ {x1 }) = f2 (x, P a(x2 ), Dn(x2 )) + g2 (x2 , P a(x2 ), Dn(x2 ))Ux01 ,x02 (S \ {x1 , x2 }). (6)
Note Dn(x2 ) {x1 }, x1 fixed x01 , hence f2 g2 effectively depend
x2 P a(x2 ). point exploited below.
Substituting Ux01 () (5) according (6) yields:
U (S) = f1 + g1 (f2 + g2 Ux01 ,x02 (S \ {x1 , x2 })) = f1 + g1 f2 + g1 g2 Ux01 ,x02 (S \ {x1 , x2 }).
list arguments functions fj , gj always (xj , P a(xj ), Dn(xj )), omit
readability.
continue fashion get
U (S) =

i1
X

k1


(fk

j=1

k=1

gj ) +




gj Ux01 ,...,x0 (xi , . . . , xn ),
i1

j=1

apply CUI condition xi ,
Ux01 ,...,x0 (xi , xi+1 , . . . xn ) =
i1

fi (xi , P a(xi ), Dn(xi )) + gi (xi , P a(xi ), Dn(xi ))Ux01 ,...,x0 (xi+1 , . . . , xn ). (7)


convenience, define constant function fn+1 Ux01 ,...,x0n (). Ultimately obtain
U (S) =

n+1
X

i1


i=1

j=1

(fi (xi , P a(xi ), Dn(xi ))

gj (xj , P a(xj )), Dn(xj )).

(8)

variable ordering restricted agree reverse topological order graph,
hence (7), Dn(xi ) {x1 , . . . , xi1 }. Therefore, variables Dn(xi ) righthand side (7) fixed reference points, fi gi depend xi
P a(xi ). Formally, let y1 , . . . , yk variables Dn(xi ). abuse notation,
define:
fi (xi , P a(xi )) = fi (xi , P a(xi ), y10 , . . . , yk0 ),
gi (xi , P a(xi )) = gi (xi , P a(xi ), y10 , . . . , yk0 ).

(9)

(8) becomes
U (S) =

n+1
X

i1


i=1

j=1

(fi (xi , P a(xi ))

gj (xj , P a(xj ))).

(10)

term decomposition multiattribute utility function lower dimensional
functions, whose dimensions depend number variables P a(x). result,
92

fiCUI networks

dimensionality representation reduced (as Bayesian networks) maximal
number parents node plus one.
illustrate utility function decomposed example Figure 1.
pick ordering x4 , x1 , x6 , x3 , x2 , x5 agrees reverse topological order
graph (note renaming variables here). simplify notation
denote conditional utility function xi fixed reference point adding
subscript U ().
U (S) = f4 (x4 x2 x6 ) + g4 (x4 x2 x6 )U4 (S \ {x4 })
U4 (S \ {x4 }) = f1 (x1 x2 x3 ) + g1 (x1 x2 x3 )U1,4 (S \ {x4 x1 })
U1,4 (S \ {x4 x1 }) = f6 (x6 ) + g6 (x6 )U1,4,6 (x2 x3 x5 )
U1,4,6 (x2 x3 x5 ) = f3 (x3 x5 ) + g3 (x3 x5 )U1,3,4,6 (x2 x5 )
U1,3,4,6 (x2 x5 ) = f2 (x2 x5 ) + g2 (x2 x5 )U1,2,3,4,6 (x5 )
U1,2,3,4,6 (x5 ) = f5 (x5 ) + g5 (x5 )U1,2,3,4,5,6 ()
Note fi gi depends xi parents. Merging equations,
using definition f7 U1,2,3,4,5,6 () produces
U (S) = f4 + g4 f1 + g4 g1 f6 + g4 g1 g6 f3 + g4 g1 g6 g3 f2 + g4 g1 g6 g3 g2 f5 + g4 g1 g6 g3 g2 g5 f7 . (11)
established U (S) represented using set functions F, includes,
x S, functions (fx , gx ) resulting decomposition (1) based CUI
condition (4). means fully specify U (S) sufficient obtain data
functions F (this aspect discussed Section 5).
Definition 8. Let U (S) utility function representing cardinal preferences D(S).
CUI network U () triplet (G, F, 0 ). G = (S, E) CUI DAG U (S), 0
reference point, F set functions {fi (xi , P a(xi )), gi (xi , P a(xi )) | = 1 . . . , n}
defined above.
utility value assignment calculated CUI network
according (10), using variable ordering agrees reverse topological order
DAG. example, choose different variable ordering one used
above, x1 , x3 , x4 , x2 , x5 , x6 , leading following expression.
U (S) = f1 + g1 f3 + g1 g3 f4 + g1 g3 g4 f2 + g1 g3 g4 g2 f5 + g1 g3 g4 g2 g5 f6 + g1 g3 g4 g2 g5 g6 f7 .
sum product different one (11). However, based CUI
decompositions therefore functions (fi , gi ).
3.3 Properties CUI Networks
Based Procedure C decomposition following it, conclude following.
Proposition 2. Let set attributes, set CUI conditions S.
includes condition form CUI(S \ (x Zx ), x | Zx ) x S,
represented CUI network whose dimensionality exceed maxx (|Zx | + 1).
93

fiEngel & Wellman

Note Zx denotes minimal set attributes (variables) renders rest
CUI x. bound dimensionality obtained regardless variable
ordering. expect maximal dimension lower network constructed
using good variable ordering. good heuristic determining ordering would
use attributes smaller dependent sets first, attributes dependents
would descendants. Based ordering would expect
less important attributes lower topology, crucial attributes
would either present higher larger number parents.
point usually omit third argument referring CUI condition,
CUI(X, ), taken equivalent CUI(X, | \ (X )).
order achieve low dimensional CUI networks, required detect CUI
conditions large sets. may difficult task, address
example Section 4. task made somewhat easier fact set
CUI single variable; note condition CU I(Y, x) weaker condition
CU I(Y, X) x X. Furthermore, Section 7 shows dimensionality
reduced initial CUI decomposition sufficiently effective.
Based properties CUI, read additional independence conditions
graph. First, observe CUI composition property second argument.
Lemma 3. Let CUI(X, ) [X, S], CUI(A, B) [A, B S].
CUI(A X , B).
property leads following claim, allows us derive additional CUI
conditions graph constructed.
Proposition
4. Consider
CUI network set attributes S. Define P a(X) =

P
a(x)

Dn(X)
=
xX Dn(x). X S,
xX
CUI(S \ (X P a(X) Dn(X)) , X).
Proof. recursion X, using Lemma 3 Proposition 1.
also consider direction, defining set nodes renders set CUI
rest. dual perspective becomes particularly useful optimization (Section 6),
optimization based preference order attribute meaningful
holding enough attributes fixed make CPI CUI rest. Let Ch(X)
denote union children nodes X, let An(X) denote ancestors
nodes X, cases excluding nodes X.
Proposition 5. Consider CUI network set attributes S. CUI(X, \(X An(X)
Ch(X))) X S.
Proof. Let
/ X Ch(X) An(X). clearly x X, x
/ P a(y) Dn(y). Hence
Proposition 1, CUI(X, y). apply Lemma 3 iteratively
/ X Ch(X) An(X)
(note first argument X CUI condition, X result well),
get desired result.
conclude section relating CUI networks CAI maps.
94

fiCUI networks

Proposition 6. Let G = (X, E) CAI map, x1 , . . . , xn ordering nodes
X. Let G0 = (X, E 0 ) DAG directed arc (xi , xj ) E 0 iff < j
(xi , xj ) E. G0 CUI network.
note, however, CAI maps decompose utility function maximal
cliques, whereas CUI networks decompose nodes parents. Section 7 bridges
gap. addition, result used Section 6.3.

4. CUI Modeling Example
demonstrate potential representational advantage CUI networks require domain difficult simplify otherwise. example use choice software
package enterprise wishes automate sourcing (strategic procurement) process. focus softwares facilities running auction RFQ (request quotes)
events, tools select winning suppliers either manually automatically.
identified nine key features kinds software packages. choice scenario,
buyer evaluates package nine features, graded discrete scale (e.g.,
one five). features are, brief:
Interactive Negotiations (IN ) allows separate bargaining procedure supplier.
Multi-Stage (MS ) allows procurement event comprised separate stages different types.
Cost Formula (CF ) buyers formulate total cost business
supplier.
Supplier Tracking (ST ) allows long-term tracking supplier performance.
MultiAttribute (MA) bidding multiattribute items, potentially using scoring function.1
Event Monitoring (EM ) provides interface running events real-time graphical
views.
Bundle Bidding (BB ) bidding bundles goods.
Grid Bidding (GB ) adds bidding dimension corresponding aspect time
region.
Decision Support (DS ) tools optimization aiding choice best
supplier(s).
observe first additive independence widely apply domain.
example, Multi-Stage makes several features useful important: Interactive
Negotiations (often useful last stage), Decision Support (to choose suppliers
1. hope fact software may include facilities multiattribute decision making
cause undue confusion. Naturally, consider important feature.

95

fiEngel & Wellman

proceed next stage), Event Monitoring (helps keep track useful
stage reducing costs). Conversely, circumstances Multi-Stage substitute
functionality features: MultiAttribute (by bidding different attributes different stages), Bundle Bidding (bidding separate items different stages), Grid Bidding
(bidding different time/regions different stages) Supplier Tracking (by extracting
supplier information Request Information stage). potential dependencies
attribute shown Table 3.
Attr
EM

CF
ST

MS
DS
GB
BB

Complements
CF ST MS
ST MS
EM MS DS GB BB
EM MS DS
DS CF
DS EM ST
CF GB ST BB MS
CF DS
CF DS

Substitutes

DS

MS BB ST GB
GB BB CF
MS BB
MS GB

CUI set
IN,DS,MA,GB,BB
EM,CF,ST,DS,GB,BB
MA,GB,BB
IN,CF,GB,BB
GB,BB
MA,GB,BB
IN,EM
MA,BB
MA,GB

Table 3: Dependent independent sets attribute.
presence complement substitute relation precludes additive independence.
fact identify set six attributes must mutually (additive) dependent: {BB , GB , DS , MA, MS , CF }. consequence, best-case dimensionality achieved
CAI map (and CAI-based representations, see Section 2.2), domain would
six, size largest maximal clique.
order construct CUI network first identify, attribute x, set
CUI it. first guess set according complement/substitute information
Table 3; typically, set attributes neither complements substitutes would
CUI. approach taken attributes EM DS . However, attributes
complements substitutes may still CUI other, therefore attempt
detect verify potentially larger CUI sets. Keeney Raiffa (1976) provide several
useful results help detection UI, results generalized CUI.
particular show first detect conditional preferential independence
(CPI) condition one element also CUI. Based result, order verify
example
CUI({BB , GB , MA}, CF | \ {BB , GB , MA, CF }),

(12)

following two conditions sufficient:
CPI({BB , GB , MA}, CF | \ {BB , GB , MA, CF }),

(13)

CUI(BB , {GB , MA, CF } | \ {BB , GB , MA, CF }).

(14)

Detection verification conditions also discussed Keeney Raiffa (1976).
example, observe features BB , GB , add qualitative
96

fiCUI networks

element bidding. bidding element best exploited cost formulation
available, complements CF . complementarity similar feature, thus
implying (13). Moreover, BB crucial feature therefore risk attitude towards
expected vary level CF , MA, GB , implies (14), together
leading (12).
similar fashion, observe nature substitutivity three
mechanisms BB , GB , MS similar: simulated using multiple stages.
means tradeoffs among three depend MS , meaning
CPI({BB , GB , MA}, MS ) holds. Next, dependency among triplet {BB , GB , MA}
also result option substitute one another. result, pair CPI
third. Finally, find complementarity ST marginal
affect tradeoffs attributes. therefore verify following conditions:
CUI({BB , GB , MA}, MS ), CUI({BB , GB }, MA), CUI({ST , EM , CF , DS , GB , BB }, ),
CUI({GB , BB , CF , }, ST ). resulting maximal CUI sets attribute
shown Table 3.
construct network start variable largest CUI set, ,
needs MS parents, EM gets CF , MS , ST
parents. Next, consider ST needs four attributes conditional set, EM
descendant, therefore DS , MS , needed parents. next variable
choose MS , needs CF DS parents since dependant variables
descendants. chosen CF MS would needed four parents: , MS ,
ST , DS (note although CUI CF set {BB , GB , MA},
case union {BB , GB , MA, }). choose CF MS
MS , ST , descendants therefore DS parent. complete variable
ordering , EM , ST , MS , CF , DS , MA, GB , BB , resulting CUI network
depicted Figure 2. maximal dimension four.
structure obtained utility function example based largely
objective domain knowledge, may common various sourcing departments.
demonstrates important aspect graphical modeling captured CUI networks:
encoding qualitative information domain, thus making process extracting
numeric information easier. structure cases differs among decision makers,
cases (as above) makes sense extract data domain experts
reuse structure across decision makers.

5. Representation Elicitation
section, derive expression local node data terms conditional utility functions, discuss elicit utility information judgments relative
preference differences.
5.1 Node Data Representation
Representing U CUI network requires determine f g functions
CUI condition. node functions f, g represent affine transformation
conditional utility function U (x0 , Y, Z) (here Z = P a(x)) strategically equivalent utility
functions values x. Like transformation functions UI (Keeney & Raiffa,
97

fiEngel & Wellman

Figure 2: CUI network example. maximal number parents 3, leading
dimension 4.

1976), transformation functions CUI represented terms conditional
utility functions U (x, 1 , Z) U (x, 2 , Z) suitable values 1 2 (see below).
determine f g solving system two equations below, based
applying (1) specific values :
U (x, 1 , Z) = f (x, Z) + g(x, Z)U (x0 , 1 , Z),
U (x, 2 , Z) = f (x, Z) + g(x, Z)U (x0 , 2 , Z),
yielding
U (x, 2 , Z) U (x, 1 , Z)
,
U (x0 , 2 , Z) U (x0 , 1 , Z)
f (x, Z) = U (x, 1 , Z) g(x, Z)U (x0 , 1 , Z).
g(x, Z) =

(15)
(16)

restriction choice 1 , 2 decision maker must
indifferent given x0 current assignment Z. example, 1 , 2
may differ single attribute strictly essential.
5.2 Elicitation Measurable Value Functions
utility function used choosing action leads known probability
distribution outcomes, obtained elicitation preferences
lotteries, example using even-chance gambles certainty equivalents (Keeney &
Raiffa, 1976). Based preceding discussion, fully specify U () via CUI network,
need obtain numeric values conditional utility functions U (x, 1 , P a(x))
U (x, 2 , P a(x)) node x. significantly easier obtaining full
n-dimensional function, general done using methods described preference
98

fiCUI networks

elicitation literature (Keeney & Raiffa, 1976). section show elicitation
conducted cases choice assumed done certain outcomes,
cardinal representation nevertheless useful.
particular applications point specific attributes used
measurement others. common example preferences quasi-linear
special attribute money time. kind preferences represented
measurable value function, MVF (Krantz et al., 1971; Dyer & Sarin, 1979). MVF
cardinal utility function defined certainty represents preference differences.
shown (Dyer & Sarin, 1979) UI analogous interpretation MVF
similar resulting decomposition. extension CUI straightforward.
case monetary scaling, preference difference pair outcomes
represents difference willingness pay (wtp) each. potential way elicit
MVF asking decision maker provide wtp improve one outcome
another, particularly outcomes differ single attribute.
interpretation, first observe (15) g(x, Z) elicited terms
preference differences, outcomes possibly differ single attribute.
result convey qualitative preference information. Assume 2 1 x.x0 x.
g(x, Z) ratio preference difference 1 2 given x
difference given x0 (Z fixed outcomes). Hence, x complements
g(x, Z) > 1 increasing x. x substitutes, g(x, Z) < 1 decreasing
x. holds regardless choice 1 , 2 , since CUI(Y, x | Z) attributes
maintain complementarity substitutivity relationship x. Note also
g(x, Z) = 1 iff CAI(Y, x | Z). Another important observation though x
may depend Z, practice expect level dependency x
depend particular value Z. case g becomes single-dimensional function,
independent Z.
f (x, Z), intuitively speaking, measurement wtp improve x0 x.
value U (x0 , 1 , Z) multiplied g(x, Z) compensate interaction
x, allowing f () independent . perform elicitation obeying
topological order graph, function U (x0 , 1 , Z) readily calculated
new node data stored predecessors. Choose 1 = 0 , let Z = {z1 , . . . , zk },
ordered children precede parents. Since Y, x fixed reference point,
k
i1
X

U (x , , Z) =
(fzi
gzj )fn+1 ().
0

0

i=1

j=1

obtain f (x, Z) follows: first elicit preference difference function
e(x, Z) = U (x, 1 , Z) U (x0 , 1 , Z). Then, assuming g(x, Z) already obtained, calculate:
f (x, Z) = e(x, Z) (g(x, Z) 1)U (x0 , 1 , Z).

6. Optimization
One primary uses utility functions support optimal choices, selecting
outcome action. complexity choice depends specific properties
99

fiEngel & Wellman

environment. choice among limited set definite outcomes, recover
utility outcome using compact representation choose one
highest value. instance, software example Section 4 would normally choose
among enumerated set vendors packages. procurement scenario assume
utility MVF, usually choose outcome yields highest utility
net price. case decision uncertainty, choice among actions
lead probability distributions outcomes, optimal choice selected computing
expected utility action. action involves reasonably bounded number
outcomes non-zero probability, done exhaustive computation.
Nevertheless, often useful directly identify maximal utility outcome given
quantitative representation utility. case direct choice constrained outcome
space, optimization algorithm serves subroutine systematic optimization procedures, adapted probabilistic reasoning literature (Nilsson, 1998).
algorithm may also useful heuristic aid optimization expected utility
net utility mentioned above, set possible outcomes large explicit,
exhaustive choice.
section, develop optimization algorithms discrete domains, show
many cases CUI networks provide leverage optimization CAI maps.
typical graphical models, optimization algorithm particularly efficient
graph restricted tree.
6.1 Optimization CUI Trees
Definition 9. CUI tree CUI network node one child.
Note type graph corresponds upside-down version standard directed tree (or forest).
Let CUI tree. assume WLOG connected (a forest turned
tree adding arcs). upside-down sort tree, number roots,
single leaf. denote root nodes ai {a1 , . . . , ak }, child ai bi ,
on. root node ai , define function
hai (bi ) = arg 0 max U (bi , a0i ),
ai D(ai )

denoting selection optimal value ai corresponding given value child.
Proposition 5, hai depend reference values chosen \ {ai , bi }.
function hai (), call optimal value function (OVF) ai , stored node ai
since used descendants described below.
Next, bi children single child ci , number parents. simplicity exposition present case bi two parents, ai aj . maximization
function bi defined
hbi (ci ) = arg 0 max U (ci , b0i , hai (b0i ), haj (b0i )).
bi D(bi )

words, pick optimal value bi assignment child parents.
since already know optimum parents value bi , need
consider optimum evaluation domain bi .
100

fiCUI networks

(a)

(b)

Figure 3: CUI networks optimization examples: (a) Tree (b) Non-tree
external child set {ai , aj , bi } ci , external ancestors,
hence {ai , aj , bi } CUI rest given ci , therefore maximization
depend reference values rest attributes. Similarly, computing
hci (di ) child ci bi , value ci fixes bi (and parents ci ),
fixes ai aj (and ancestors ci ). last computation, leaf x, evaluates
value x. value x0 causes cascade fixed values ancestors,
meaning finally get optimal choice comparing |D(x)| complete assignments.
illustrate execution algorithm CUI tree Figure 3a. compute
ha (c) optimal value value c, similarly hb (c). Next,
compute hc (e), value e0 e compare outcomes (e0 , c0 , ha (c0 ), hb (c0 )), c0 D(c).
node compute hd (f ), independent nodes. node e compute
(f ) = arg maxe0 U (f, e0 , hc (e0 ), hb (hc (e0 )), ha (hc (e0 )) (node ignored here)
node f
hf () = arg max U (f 0 , (f 0 ), hd (f 0 ), hc (he (f 0 )), hb (hc (he (f 0 ))), ha (hc (he (f 0 ))).
f 0 D(f )

Note candidate value f causes cascade optimal values
ancestors. solution hf () resulting values ancestors.
optimization algorithm iterates nodes topological order, xi
calculates OVF hxi (xj ), xj child xi . calculation uses values
OVF stored parents, therefore involves comparison |D(xi )||D(xj )|
outcomes. case numeric data nodes available, factoring time takes
recover utility value outcome (which O(n)), algorithm runs time
O(n2 maxi |D(xi )|2 ).
6.2 Optimization General DAGs
common way graphical models apply tree algorithms non-trees using
junction graph. However, common notion junction graph DAG polytree,

101

fiEngel & Wellman

whereas algorithm specialized (unit) tree. Instead, optimize CUI
network directly generalizing tree algorithm.
tree case, fixing value child node x sufficient order separate
x rest graph, excluding ancestors. consider value child
time, also determines values ancestors. general DAG longer
sufficient OVF depend children, provide sufficient
information determine values An(x). Hence generalize notion
scope x (Sc(x), defined below), set nodes OVF x must
depend, order iterative computation OVF sound.
generalization, DAG algorithm similar tree algorithm. Let G
CUI network, x1 , . . . , xn variable ordering agrees topological order
G (parents precede children). xi (according ordering), compute hxi (Sc(xi ))
instantiation Sc(xi ). optimal instantiation selected backwards
hxn (), since node xi reached values Sc(xi ) already selected.
Sc(xi ) computed follows: scan variables xi+1 , . . . , xn order. scanning
xj , add xj Sc(xi ) following conditions hold:
1. undirected path xj xi .
2. path blocked node already Sc(xi ).
conditions, Sc(xi ) includes children xi , non xi ancestor since
precede xi ordering. addition, Sc(xi ) includes nodes needed
block paths reach xi ancestors. example, xk , xj children
ancestor xa xi , k < < j, xj must Sc(xi ), path
xa . children xj blocked xj , unless another path xi
Sc(xi ). children xk , ordered later xi , Sc(xi ) (but
children not), on.
Figure 3b example CUI network tree. consider scopes
variable ordering a, b, . . . , j. scope roots always equals set children
(because path reaching them), meaning Sc(a) = {d, e}, Sc(b) = {d, e, f },
Sc(c) = {e, f, h}, Sc(i) = {j}. scope must include child g siblings e
f . paths h, j, blocked g, e, f therefore Sc(d) = {g, e, f }. e,
must include child g, younger sibling f . h blocked path e
f Sc(e), also non-blocked one c
/ Sc(e), therefore Sc(e) = {g, f, h}.
Similarly, g h scope f due paths b c respectively, hence
Sc(f ) = {g, h, j}. g, addition child h add j whose path g f, b, e
blocked (Sc(g) = {h, j}) finally Sc(h) = Sc(i) = {j} Sc(j) = {}.
next step, computing OVF, requires compare set outcomes
differ xi Co(xi ), Co(xi ) set nodes whose OVF determined
xi Sc(xi ) (hence covered xi ). maximization valid, condition
CUI(xi Co(xi ), \ (xi Co(xi ) Sc(xi ))) must hold. formally define Co(xi ),
establish result proved appendix.
Definition 10. Co(xi ) smallest set nodes satisfied following condition
j < i, Sc(xj ) ({xi } Sc(xi ) Co(xi )) xj Co(xi ).
102

(17)

fiCUI networks

Intuitively, xj covered xi node xk 6= xi scope, either scope xi
determined (according scope) covered xi . Figure 3b, f Co(g)
Sc(f ) = {g} Sc(g). e Co(g) Sc(e) {g} Sc(g) {f }. Moreover,
Sc(d) = {g, e, f } hence Co(g) well, similarly find a, b Co(g).
example nodes preceding g ordering covered, necessarily
always case.
Lemma 7. assignment xi Sc(xi ) sufficient determine hxj () xj
Co(xi ).
Lemma 8. node xi , CUI({xi } Co(xi ), \ ({xi } Co(xi ) Sc(xi ))). Meaning
xi nodes covers CUI rest given Sc(xi ).
algorithm reaches node xi , every choice assignment Sc(xi ) {xi } determines optimal values Co(xi ) (Lemma 7). compare |D(xi )| assignments
differ values xi Co(xi ), select optimal one value hxi (Sc(xi )).
optimum depend nodes \({xi }Co(xi )Sc(xi ))) due Lemma 8.
illustrate, examine happens algorithm reaches node g Figure 3b.
point hx (Sc(x)) known x precedes g. showed, nodes
Co(g). Indeed, assignment Sc(g) = {g, h, j} directly determines value
hf (), together hf () determines value (), cascades
rest nodes. CUI network shows CUI({a, b, c, d, e, f, g}, {i}) (given
{h, j}) therefore maximization operation (over choice value g) valid
regardless value i.
performance optimization algorithm exponential size largest
scope (plus one). Note would seriously affected choice variable ordering. Also note, case tree algorithm specializes tree optimization
above, since node path ancestor xi , except ancestors xi must precede xi ordering. Therefore always case
Sc(xi ) = Ch(xi ), meaning hxi () function single child. Based that,
expect algorithm perform better similar CUI network tree.
6.3 CUI Tree Optimization CAI Maps
optimization procedure CUI trees particularly attractive due relatively
low amount preference information requires. cases comparison
done directly, without even data comprises utility function. Aside
direct benefit CUI networks, interested applying structure
optimization CAI maps. domains CAI map simple effective way
decompose utility function. However, optimization CAI maps exponential
size tree width, requires full data terms utility functions
maximal cliques. CAI map happens simple structure, tree,
CP condition, faster optimization algorithms used. However, could case
CAI map tree, subtle CUI conditions might exist cannot
captured CAI conditions. enough conditions could detected turn CAI
map CUI tree (or close enough tree), could take advantage simple
optimization procedure.
103

fiEngel & Wellman

(a)

(b)

(c)

Figure 4: (a) CAI map containing cycle. (b) Enhanced CAI map, expressing CUI
{a, d, f } b. (c) equivalent CUI tree.
Definition 11. Let G = (V, E) CAI map. enhanced CAI map directed graph
G0 = (V, A), pair arcs (u, v), (v, u) implies dependency
edge (u, v) E, addition node x, CUI(S \ ({x} In(x)), x) (In(x) denoting
set nodes (y, x) A). call pair arcs (u, v), (v, u) hard
link arc (u, v) s.t. (v, u)
/ weak link.
CAI map, enhanced CAI map generated replacing edge (u, v)
arcs (u, v) (v, u). require additional CUI conditions
entailed CAI map. However, additional CUI conditions
detected, might able remove one (or both) directions. Figure 4a shows
CAI map contains cycle. could detect CUI({a, d, f }, b), could remove
direction (a, b) get enhanced CAI map Figure 4b. set CUI conditions
implied enhanced CAI map expressed CUI tree, Figure 4c.
Proposition 9. Consider enhanced CAI map G. Let ordering nodes
G, G0 DAG result removing arcs (u, v) whose direction
agree . removed arc, v ancestor u G0 , G0 CUI
network.
hard links, removal (u, v) leaves v parent u, condition trivially
holds. obtain CUI tree, key therefore find variable ordering
enough weak links removed turn graph tree, maintaining condition
Proposition 9. large number variables, exhaustive search variable orderings
may feasible. However many cases effectively constrained, restricting
number orderings need consider. example, order break cycle
Figure 4b clear weak link (b, a) must implied ordering,
could ancestor b. way happen (given existing hard links),
c parent b, parent c, parent d.
Proposition 10. Let c = (y1 , . . . , yk ) cycle enhanced CAI map G. Assume
c contains exactly one weak link: (yi , yi+1 ) < k, (yk , y1 ). Let variable
104

fiCUI networks

ordering agree order path p = (yi+1 , yi+2 , . . . , yk , y1 , . . . , yi ).
CUI network constructed G (by Proposition 9), tree.
Therefore cycle contains one weak link leads constraint variable
ordering. Cycles one weak link also lead constraints. c
another weak link (yj , yj+1 ), one two links must removed, ordering must
agree either path p path p0 = (yj+1 , yj+2 , . . . , yk , y1 , . . . , yj ). Assuming
WLOG j > i, paths (yi+1 , . . . , yj ) (yj+1 , . . . , yi ) required p
p0 , therefore used constraints. Similarly find intersection
paths implied number weak links cycle.
Sometimes constraint set lead immediate contradiction, case
search redundant. not, significantly reduce search space. However,
major bottleneck preference handling usually elicitation, rather computation.
Therefore, given good variable ordering may lead reduction optimization
problem simpler, qualitative task, eliminating need full utility elicitation,
would worthwhile invest required computation time.

7. Nested Representation
Section 5.1 conclude node data represented conditional utility functions depending node parents. may best dimensionality
achieved network. Perhaps set Z = P a(x) internal structure,
sense subgraph induced Z maximal dimension lower |Z|.
case could recursively apply CUI decomposition conditional utility functions
subgraph. approach somewhat resembles hierarchical decomposition done
utility trees (Keeney & Raiffa, 1976; Von Stengel, 1988). example, represent f1
network Figure 1, require conditional utility function U (x1 , x14 , x15 , x16 , x2 , x3 ).
However network see CUI(x3 , x2 | x1 , x4 , x5 , x6 ). Hence decompose conditional utility:
U (x1 , x14 , x15 , x16 , x2 , x3 ) = f 0 (x1 , x2 ) + g 0 (x1 , x2 )U (x1 , x3 , x02 , x14 , x15 , x16 ).
use notation f 0 g 0 since f g functions
top level decomposition.
nested representation generated systematically (Algorithm 1), decomposing
local function node x (xs utility factors) whose argument set Z P a(x)
form clique. performing complete CUI decomposition subgraph
induced Z (keeping mind resulting factors depend also x).
Proposition 11. Let G CUI network utility function U (S). U (S)
represented set conditional utility functions, depending set attributes
corresponding (undirected) cliques G.
7.1 Discussion
result reduces maximal dimensionality representation size
largest maximal clique CUI network. instance, applying example
105

fiEngel & Wellman

Data: CUI Utility factors U (x, P a(x), ), U (x, P a(x), ) node x
/* note: , D(Y ) */
Determine order x1 , . . . , xn ;
j = 1, . . . , n /* initialization */
Kj1 = {xj } P a(xj ) /* scope utility factors */ ;
Yj1 = \ Kj1 /* rest variables */ ;
Q1j = P a(xj );
A1j = , dj = 1;
end
j = 1, . . . , n
= 1, . . . , dj /* loop factors node j*/
Qij 6= Kji clique
Let Gij subgraph induced Qij ;
Decompose Uji (Kji ) according CUI network Gij ;
foreach xr Qij
Let dr = dr + 1 (current num. factors xr ) denote = dr ;
Adr = Aij {xj }, Qdr = P a(xr ) Qij ;
Krd = Adr {xr } Qdr , Yrd = \ Krd ;
Store new CUI factors xr : U (Krd , Yrd ), U (Krd , Yrd );
/*Yrd , Yrd fixed assignments Yrd */
end
Remove factors U (Kji , Yji ), (Kji , Yji );
end
end
end
Algorithm 1: Recursive CUI decomposition. Process node reverse topological
order (outermost loop). Decompose factor stored current node, whose parents
form clique. parent xr (innermost loop) store resulting new
factors. defined xr , xr parents also P a(xi ) (this
Qdr ), clique Adr original factor depends. time factor
decomposed set Q shrinks. empty, K clique.

106

fiCUI networks

Section 4 reduces dimensionality four three. important implication
somewhat relax requirement find large CUI sets. variables end
many parents, reduce dimensionality using technique. example
illustrates, technique aggregates lower order CUI conditions effective
decomposition.
procedure may generate complex functional form, decomposing function multiple
times factors become restricted clique. ultimate number factors
required represent U (S) exponential number nesting levels. However,
decomposition based CUI network subgraph, therefore typically
reduces number entries maintained.
expect typical application technique composition rather
decomposition. execute Algorithm 1 without actual data, resulting list factors
per node (that conditional utility functions cliques graph). means
elicitation purposes restrict attention conditional utility functions
maximal cliques. obtained, sufficient data factors.
recover original, convenient CUI-network representation function
store (more example below). Therefore, effective dimensionality
elicitation maximal cliques. storage efficient usage requires
potentially higher dimension original CUI network, typically less
concern.
result Proposition 6, CUI networks shown always achieve weakly
better dimensionality CAI maps, since representations reduce dimensionality
size maximal clique.
7.2 Example
illustrate result using simple example. Consider domain four attributes
(a, b, c, d), following CUI conditions:
CUI(b, c), CUI(c, b), CUI(d, a)
CUI network corresponding variable ordering a, b, c, depicted Figure 5.
Since CUI sets small (a single variable each), variable ordering must
node two parents, meaning dimensionality three. nesting operation
combines lower order conditions reduce dimensionality two.
Initially, utility function represented using conditional utility functions listed
according corresponding nodes column Level 0 Table 4. remove
three-dimensional factors, need decompose functions node according
CUI network {b, c}, contains arcs. proceeds follows:
U (a, b, c, d1 ) =
fb1 (a, c) + gb1 (a, c)U (a, b, c0 , d1 ) = fb1 (a, c) + gb1 (a, c)(fc1 (a, b) + gc1 (a, b)U (a, b0 , c1 , d1 ))
U (a, b, c, d2 ) =
fb2 (a, c) + gb2 (a, c)U (abc0 d2 ) = fb2 (a, c) + gb2 (a, c)(fc2 (a, b) + gc2 (a, b)U (a, b0 , c0 , d2 ))
107

fiEngel & Wellman

Figure 5: Nesting example

resulting functions fbi (a, c), gbi (a, c), fci (a, b), gci (a, b), = 1, 2. functions
gbi (a, c) represented using conditional utility functions U (a, b1 , c, di )
U (a, b2 , c, di ), similarly two functions. delete factors a,
U (a, b, c, di ), add new lower dimensional factors second column parents
b c. Though multiply number factors store four, new
factors conditional utility functions subdomains (deleted) higher dimensional
factors. algorithm continues node b, loops six factors. factors
defined set parents b clique decomposes store
new factors next table column. case factor column level 1 could
decomposed, would add level 2 column store result. simple example
decomposition possible.
fbi (a, c)

Attr

b
c


Level 0 (CUI net)
U (a, b, c, d1 ), U (a, b, c, d1 )
U (a0 , b, c1 , d),
U (a0 , b, c2 , d)
U (a0 , b1 , c, d),
U (a0 , b2 , c, d)
U (a0 , b0 , c0 , d)

Level 1
U (a, b, c1 , d1 ),
U (a, b, c1 , d2 ),
U (a, b1 , c, d1 ),
U (a, b1 , c, d2 ),

U (a, b, c2 , d1 )
U (a, b, c2 , d2 )
U (a, b2 , c, d1 )
U (a, b2 , c, d2 )

Table 4: Nested CUI decomposition
reverse direction mentioned done follows: run Algorithm 1 without
data, resulting table Table 4 (without actual utility values).
elicit data non deleted factors (all limited maximal cliques). Next,
recover convenient level 0 CUI representation using table, computing
deleted factor (going rightmost columns left) function factors
stored parents.

8. Conclusions
present graphical representation multiattribute utility functions, based conditional utility independence. CUI networks provide potentially compact representation
multiattribute utility function, via functional decomposition lower-dimensional functions depend node parents. CUI weaker independence condition
108

fiCUI networks

previously employed basis graphical utility representations, allowing common
patterns complementarity substitutivity relations disallowed additive models.
proposed techniques obtain verify structural information, use construct network elicit numeric data. addition, developed optimization
algorithm performs particularly well special case CUI trees. cases
also leveraged efficient optimization CAI maps. Finally, show functions
decomposed set maximal cliques CUI network.
technique, CUI networks achieve dimensionality graphical models based
CAI GAI decompositions, yet broadly applicable independence conditions.

Acknowledgments
preliminary version paper published proceedings AAAI-06.
work supported part NSF grant IIS-0205435, STIET program NSF
IGERT grant 0114368. grateful thorough work anonymous reviewers,
whose suggestions provided valued help finalizing paper.

Appendix A. Proofs
A.1 Lemma 3
Proof. Let Z = \ (X ) C = \ (A B). simply apply two independence
conditions consequentially, define f, g that:
U (S) = U (XY Z) = f (Y Z) + g(Y Z)UY (S \ ) = f (Y Z) + g(Y Z)(f 0 ((BC) \ )
+ g 0 ((BC) \ )UY B (S \ (Y B))) = f(ZBY C) + g(ZBY C)U (S \ (Y B)).
Since Z B C = \(AX), last decomposition equivalent decomposition
(1) condition CUI(A X , B).
A.2 Proposition 6
Proof. CAI condition stronger CUI condition, CAI(x, y) CUI(x, y)
CUI(y, x). CUI network, node xi must case nodes
CUI given parents descendants. obvious since xi CAI
nodes given parents children.
A.3 Lemma 7
Proof. determine hxj (), Sc(xj ) needs determined. {xi } Sc(xi )
done, scope covered therefore recursively determined
assignment {xi } Sc(xi ).
A.4 Lemma 8
first introduce two additional lemmas.
Lemma 12. Ch({xi } Co(xi )) ({xi } Sc(xi ) Co(xi ))
109

fiEngel & Wellman

Proof. Let xj {xi } Co(xi ), Ch(xj ). xj = xi proof immediate
Ch(xi ) Sc(xi ). Assume xj Co(xi ). know Definition 10 Ch(xj )
Sc(xj ) ({xi } Sc(xi ) Co(xi )), proves lemma.
Lemma 13. An({xi } Co(xi )) Co(xi )
Proof. Let xj An(xi ) (clearly j < i, therefore xj
/ Sc(xi )). Let xj1 Sc(xj ).
j1 > j undirected path xj1 xj , blocked Sc(xj ). j1 i,
xj1 Sc(xi ){xi } unblocked path xj (and xi ). Otherwise,
let xj2 Sc(xj1 ), apply argument xj2 . continue xjk
xy Sc(xjk ), > point xy Sc(xi ) {xi } path xy , xjk , . . . , xj1 , xj , xi
recursion halts (note includes empty scopes), proving xj Co(xi ).
left prove An(Co(xi )) Co(xi ). Let xj Co(xi ), An(xj ). Applying
first part proof xj , get Co(xj ). Definition Co(xj ), get
< j w Sc(y), either w = xj , w Sc(xj ) w Co(xj ). show Co(xi ),
need prove cases w {xi } Sc(xi ) Co(xi ).
1. w = xj immediately w Co(xi ).
2. w Sc(xj ), xj Co(xi ) get either w = xi , w Sc(xi ) w Co(xi ).
3. w Co(xj ), repeat argument recursively z Sc(w). Note z precedes
w therefore recursion halt point.

Lemma 8. Let X = {xi } Co(xi ). Lemma 13, X external ancestors.
Lemma 12, external children X Sc(xi ). Therefore \ (X An(X) Ch(X)) =
\ (X Sc(xi )) result immediate Proposition 5.
A.5 Proposition 9
Proof. Let x node G0 . Let = \ (x In(x)) G = \ (x P a(x) Dn(x))
G0 . definition G, know CAI(Y, x), also CUI(Y, x). Let
/ Y, 6= x (so
0
In(x) G). ,
/ P a(x) = In(x) G . arc (y, x) removed,
meaning Dn(x). therefore must case
/ . Therefore hence
CUI(Y , x).
A.6 Proposition 10
Proof. G become CUI tree, cycle least one weak link must removed.
Since (yi , yi+1 ) weak link c, must removed. Proposition 9,
variable ordering must ensure yi+1 ancestor yi . done
path according order p, might another path yi+1 yi . Let p1
path. combination p1 p another cycle c1 , therefore must
broken. Since p comprises strong links, must least one weak link (u, v)
p1 . (u, v) removed, v must ancestor u. done
path cycle c1 , path includes p, another path exists,
repeat argument. stage get larger cycle ci , larger path
110

fiCUI networks

pi pi1 . Therefore point one path pi must guaranteed
variable ordering, path includes p.
A.7 Proposition 11
Proof. show Algorithm 1 leads functional decomposition cliques.
outer loop algorithm maintains following iteration properties:
1. Aij , Qij P a(a)
2. Uij defined Kji
3. Aij xj clique
properties hold trivially initialization. Assume valid factors
stored network outer iteration j inner iteration i, next show
remain valid factor Urd created iteration j, i:
1. definition Adr = Aij xj . previous iteration definition Qdr ,
Aij , Qdr Qij P a(a). definitions Qij Qdr get P a(xj ) Qij Qdr ,
together yields result.
2. Urd factor CUI decomposition Uji (Kji ) Gij . scope contains: (i)
nodes affected last CUI decomposition, i.e. Kji \ Qij =
Aij xj = Adr , (ii) node xr , (iii) parents P a(xr ) fixed
Uji (i.e. P a(xr Kji )). know Kji = Aij xj Qij , xj
/ P a(xr ) (because

xr P a(xj )), also P a(xr ) Aj = (using similar argument property 1).
Therefore (P a(xr ) Kji ) Qij , (i),(ii),(iii) get Krd = Adr xr Qdr .
3. Adr clique definition property previous iteration. xr Qij ,
therefore property 1 previous iteration xr P a(a) Aij . Also xr
Qij P a(xj ) (the last containment immediate definition Qij ). Therefore xr
parent members Adr , result Adr xr clique.
iteration properties, either Krd clique, Qdr non empty decomposition
applied reach node r outer loop. end process factors
defined cliques removed. factors remained defined
cliques. U (S) still represented new set factors since applied
valid decompositions factors.

References
Abbas, A. (2005). Attribute dominance utility. Decision Analysis, 2, 185206.
Bacchus, F., & Grove, A. (1995). Graphical models preference utility. Eleventh
Conference Uncertainty Artificial Intelligence, pp. 310, Montreal.
Boutilier, C., Bacchus, F., & Brafman, R. I. (2001). UCP-networks: directed graphical
representation conditional utilities. Seventeenth Conference Uncertainty
Artificial Intelligence, pp. 5664, Seattle.
111

fiEngel & Wellman

Boutilier, C., Brafman, R. I., Hoos, H. H., & Poole, D. (1999). Reasoning conditional ceteris paribus preference statements. Fifteenth Conference Uncertainty
Artificial Intelligence, pp. 7180, Stockholm.
Debreu, G. (1959). Topological methods cardinal utility theory. Arrow, K., Karlin, S.,
& Suppes, P. (Eds.), Mathematical Methods Social Sciences. Stanford University
Press.
Dyer, J. S., & Sarin, R. K. (1979). Measurable multiattribute value functions. Operations
Research, 27, 810822.
Fishburn, P. C. (1965). Independence utility theory whole product sets. Operations
Research, 13, 2845.
Fishburn, P. C. (1967). Interdependence additivity multivariate, unidimensional
expected utility theory. International Economic Review, 8, 335342.
Fishburn, P. C. (1975). Nondecomposable conjoint measurement bisymmetric structures.
Journal Mathematical Psychology, 12, 7589.
Fuhrken, G., & Richter, M. K. (1991). Polynomial utility. Economic Theory, 1 (3), 231249.
Gonzales, C., & Perny, P. (2004). GAI networks utility elicitation. Ninth International
Conference Principles Knowledge Representation Reasoning, pp. 224234,
Whistler, BC, Canada.
Gorman, W. M. (1968). structure utility functions. Review Economic Studies,
35, 367390.
Keeney, R. L., & Raiffa, H. (1976). Decisions Multiple Objectives: Preferences
Value Tradeoffs. Wiley.
Krantz, D. H., Luce, R. D., Suppes, P., & Tversky, A. (1971). Foundations Measurement,
Vol. 1. Academic Press, New York.
La Mura, P., & Shoham, Y. (1999). Expected utility networks. Fifteenth Conference
Uncertainty Artificial Intelligence, pp. 366373, Stockholm.
Nilsson, D. (1998). efficient algorithm finding probable configurations
probabilistic expert systems. Statistics Computing, 8 (2), 159173.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.
Pearl, J., & Paz, A. (1989). Graphoids: graph based logic reasoning relevance
relations. Du Boulay, B. (Ed.), Advances Artificial Intelligence II. North-Holland,
New York.
Tatman, J. A., & Shachter, R. D. (1990). Dynamic programming influence diagrams..
20, 365379.
Von Stengel, B. (1988). Decomposition multiattribute expected utility functions. Annals
Operations Research, 16, 161184.
Wellman, M. P., & Doyle, J. (1992). Modular utility representation decision-theoretic
planning. First International Conference Artificial Intelligence Planning Systems, pp. 236242, College Park, MD.

112

fiJournal Artificial Intelligence Research 31 (2008) 205216

Submitted 10/07; published 01/08

Sound Complete Inference Rules SE-Consequence
Ka-Shu Wong

KSWONG @ CSE . UNSW. EDU . AU

University New South Wales National ICT Australia
Sydney, NSW 2052, Australia

Abstract
notion strong equivalence logic programs answer set semantics gives rise consequence relation logic program rules, called SE-consequence. present sound complete
set inference rules SE-consequence disjunctive logic programs.

1. Introduction
recent years much research various notions equivalence two logic
programs. particular, notion strong equivalence logic programs answer set semantics (Lifschitz, Pearce, & Valverde, 2001; Turner, 2001, 2003; Cabalar, 2002; Lin, 2002)
received much attention. say two logic programs P Q strongly equivalent iff
set rules R, P R Q R answer sets.
Recent work area (Eiter, Fink, Tompits, & Woltran, 2004; Turner, 2003; Osorio, Navarro,
& Arrazola, 2001) focused simplification logic programs strong equivalence.
resulted number logic program transformation rules preserve strong equivalence. transformations, logic program rules identified removed
maintaining strong equivalence original program.
paper look different related aspect strong equivalence. notion
strong equivalence logic programs gives rise consequence relation |=s logic program
rules, called SE-consequence (Eiter et al., 2004), defined saying rule r
consequence logic program P iff P P r strongly equivalent1 . consequence
relation useful testing strong equivalence, well identifying redundant rules logic
program simplification.
paper, present set inference rules logic program rules show
sound complete SE-consequence. set inference rules consists adaptations
several well-known logic program simplification rules, together new rule call SHYP. main contribution paper new inference rule S-HYP completeness result.
completeness proof makes use construction used reduction strong equivalence
testing classical logic Lin (2002), applies restricted form resolution called lock
resolution Boyer (1971).

1. Eiter et al. (2004) uses different definition SE-consequence based Turners SE-models (Turner, 2001, 2003).
equivalence two definitions proved Section 3.
c
2008
AI Access Foundation. rights reserved.

fiW ONG

2. Definitions
deal propositional disjunctive logic programs negation-as-failure, rule
form
a1 ; a2 ; ; ak b1 , b2 , , bm , c1 , c2 , , cn .
a1 , , ak , b1 , , bm c1 , , cn set atoms, assume set
atoms fixed. Given rule r form, denote H(r) = {a1 , , ak } (head r),
B + (r) = {b1 , , bm } (positive part r), B (r) = {c1 , , cn } (negative part r).
ignore order atoms within rule; therefore, rule considered triple atom
sets. abbreviation, include atom sets rule, means atoms set
corresponding part rule. particular, X = {x1 , , xk }, X body
rule abbreviation x1 , , xk . Applied rule r above, = H(r),
B = B + (r) C = B (r), rule abbreviated
B, C.
set X atoms logic program P , use notation X |= P mean X model
P classical sense: r P , B + (r) X B (r) X = , H(r) X
non-empty. say X minimal model P X minimal set inclusion among
models P , i.e. X |= P X 0 X 0 X X 0 |= P .
Gelfond-Lifschitz reduct (1988) P X program P respect set atoms X
defined P X = {H(r) B + (r) | r P X B (r) = }. say X answer set
P X minimal model P X .

3. Strong Equivalence
notion strong equivalence (Lifschitz et al., 2001) describes property two programs
remain equivalent regardless additional rules added, defined follows:
Definition 1. Logic programs P Q strongly equivalent, iff sets R rules,
programs P R Q R answer sets.
Lifschitz et al. (2001) showed strong equivalence reduced equivalence logic
here-and-there. Based result, Turner (2003) gave following definition SE-models,
characterises strong equivalence sense two programs strongly equivalent iff
SE-models:
Definition 2. Let P logic program, let X, sets atoms. say pair (X, )
SE-model P , written (X, ) |= P , X , |= P X |= P . set
SE-models, write |= P mean (X, ) |= P (X, ) . Let Ms (P ) denote set
SE-models P .
SE-models property pair (X, ) SE-model P iff SE-model every rule
r P . implies Ms (P Q) = Ms (P ) Ms (Q).
notion strong equivalence gives rise consequence relation logic program rules,
called SE-consequence denoted |=s . SE-consequence defined by:
206

fiS OUND C OMPLETE NFERENCE RULES SE-C ONSEQUENCE

Definition 3. Let P, Q logic programs, r logic program rule. say P |=s r iff
Ms (P ) |= r, i.e. every (X, ) Ms (P ) SE-model r. Furthermore, write P |=s Q iff
P |=s r every r Q.
equivalent definition SE-consequence make use SE-models:
Proposition 1. Let P logic program r logic program rule. P |=s r iff P
P {r} strongly equivalent.
Proof. P P {r} strongly equivalent iff Ms (P ) = Ms (P {r}) (= Ms (P ) Ms (r)).
holds iff Ms (P ) Ms (r), i.e. every (X, ) Ms (P ) SE-model r.
relation |=s properties consequence relation:
Proposition 2. Let P, Q logic programs, r logic program rule.
r P , P |=s r
P Q P |=s r Q |=s r
P |=s r Q |=s P Q |=s r
proofs follow easily fact P |=s Q iff Ms (P ) Ms (Q).
exist binary resolution-style calculi logic here-and-there (also known Godels
3-valued logic). However, Example 1 suggests cannot applied SE-consequence, seemingly takes one outside logic fragment corresponding disjunctive logic programs.
supported fact construction Section 6.1 may produce clauses
correspond logic program rules. currently investigated.

4. Inference Rules Strong Equivalence
consequence relation `s defined following rules inference:
(TAUT)

x x.

(NONMIN)

(WGPPE)

(CONTRA)

x, x.

B, C.
A; X B, Y, C, Z.

A1 B1 , x, C1 .
A2 ; x B2 , C2 .
A1 ; A2 B1 , B2 , C1 , C2 .
A1 B1 , x1 , C1 .
..
.

Bn , xn , Cn .
x1 , , xn , C.
(S-HYP)
A1 ; ; B1 , , Bn , C1 , , Cn , A, C.
207

fiW ONG

Many rules well-known: tautological rules (TAUT), contradiction (CONTRA), nonminimal rules (NONMIN), weak partial evaluation (WGPPE) (also called partial deduction,
Sakama & Seki, 1997). shown strong equivalence preserving (Brass & Dix,
1999; Osorio et al., 2001; Eiter et al., 2004). new rule S-HYP thought form
hyper-resolution. knowledge considered literature.
Instead S-HYP, one might expect general rule S-HYP+ allows additional positive atoms B final rule:
A1 B1 , x1 , C1 .
..
.
Bn , xn , Cn .
x1 , , xn , B, C.
(S-HYP+)
A1 ; ; B, B1 , , Bn , C1 , , Cn , A, C.
However, shown replacing S-HYP S-HYP+ change consequence
relation:
Proposition 3. Let ` consequence relation satisfying CONTRA. S-HYP S-HYP+
interchangeable.
Proof. Since S-HYP+ general form S-HYP, suffices show S-HYP+
simulated using S-HYP CONTRA. Suppose
A1 B1 , x1 , C1 .
..
.
Bn , xn , Cn .
x1 , , xn , b1 , , bk , C.
bi , CONTRA gives us bi , bi . using S-HYP rules, plus above,
get
A1 ; ; B1 , , Bn , b1 , , bk , C1 , , Cn , A, C.
result applying S-HYP+ initial set rules.
Example 1. consider possibility using binary inference rules logic program P :
r1 : x.
r2 : y.
r3 :
x, y.
following rule SE-consequence P , derived using S-HYP r1 , r2 r3 :
s:

a.

suppose restrict binary inference rules replacing S-HYP binary
variant S-HYP+. Applying S-HYP+ r1 r3 gives:
r4 :

y.
208

fiS OUND C OMPLETE NFERENCE RULES SE-C ONSEQUENCE

using S-HYP+ r2 r4 gives:
t:

a.

observe weaker s. suggests may possible derive using
binary inference rules. However, still open question whether n-ary rules indeed
required.
Observe rule CONTRA replaced s-implication (S-IMP) (Wang & Zhou, 2005):
Proposition 4. Let ` consequence relation satisfying TAUT WGPPE. CONTRA
replaced following inference rule:
(S-IMP)

A; X B, C.
B, C, X.

Proof. (S-IMP CONTRA) rule x, x. formed applying TAUT followed
S-IMP.
(CONTRA S-IMP) Suppose rule
A; x B, C.
applying CONTRA get x, x. followed WGPPE two rules, get
B, C, x.
repeating steps derive new rules moving set atoms head
negative part rule.
note S-IMP special case solution 1-1-0 problem Lin Chen (2007).
addition, WGPPE well binary variant S-HYP+ contained solution Lin
Chens 2-1-0 problem.
shown inference rules presented sound complete SEconsequence:
Theorem 1. P |=s r iff P `s r.
Proof (Soundness). prove soundness inference rule S-HYP, soundness
rules already known.
proof proceeds contradiction. Assume rule sound. program P
A1 B1 , x1 , C1 .
..
.
Bn , xn , Cn .
x1 , , xn , C.
rule r
A1 ; ; B1 , , Bn , C1 , , Cn , A, C.
209

fiW ONG

P `s r P 6|=s r. means Ms (P ) 6 Ms (r), P SE-model (X, )
SE-model r.
(X, ) SE-model r means either 6|= r X 6|= rY . exclude first
case since clear r classical consequence P . Therefore assume X 6|= rY .
1 n Ai X = , Bi X, Ci = . Furthermore AY = CY = .
(X, ) SE-model P , hence X |= P . 1 n, following
rule P :
Ai Bi , xi , Ci .
Since Ai X = , body must hold, rule must eliminated reduct.
know Bi X Ci = . Therefore must xi rule eliminated
reduct.
= C = x1 , , xn , classical model
x1 , , xn , C.
hence 6|= P , contradicts (X, ) SE-model P .

5. Background Completeness Proof
section introduce two results used completeness proof.
5.1 Lins Construction
Lin (2002) presented method reducing strong equivalence equivalence classical logic.
Given logic program, construction produces set clauses two logic programs
strongly equivalent iff two sets clauses equivalent classical logic.
Let {x1 , , xn } set atoms. construction, atom xi represented two
propositional letters xi x0i . logic program P consists set rules following form:
a1 ; a2 ; ; ak b1 , b2 , , bm , c1 , c2 , , cn .
rule r, construct two clauses (r) 0 (r):
(r) := a1 ak b1 bm c01 c0n
0 (r) := a01 a0k b01 b0m c01 c0n
Let (P ) := {(r) | r P } 0 (P ) := { 0 (r) | r P }. Furthermore atom xi add
clause xi x0i . Let denote set clauses. Lins result showed P Q
strongly equivalent iff
^
^
((P ) 0 (P ) ) ((Q) 0 (Q) )
immediate corollary result is:
Proposition 5. P |=s r iff
^

((P ) 0 (P ) ) |= (r) 0 (r)

Observe given clause , find corresponding rule r (r) =
literals form x0 .
210

fiS OUND C OMPLETE NFERENCE RULES SE-C ONSEQUENCE

5.2 Boyers Lock Resolution
Resolution following inference rule: given two clauses x C1 x C2 , derive clause
C1 C2 . say x x literals resolved on, clause C1 C2 resolvent.
well-known resolution refutation-complete, is, set clauses unsatisfiable,
empty clause derived using resolution.
Definition 4. deduction clause C set clauses sequence clauses C1 , , Cn
Cn = C Ci either clause resolvent clauses preceding Ci .
deduction exists, say C derived S.
Lock resolution restricted form resolution introduced Boyer (1971). numeric label
given literal clause. Resolution permitted literals lowest valued
label clause. Note clause one literal label:
many literals lowest valued label, resolution allowed.
Literals resolvent inherit labels parent clauses. literal resolvent
two possible labels, lower value used. deduction follows restrictions called
lock deduction.
Example 2. Consider following clauses:
C1 : a(1) b(2)

C2 : a(2) b(3)

C3 : b(1) c(2)

resolve C1 C2 a(1) a(2) form following clause:
b(2)
Note b labelled 2 C1 3 C2 , lower value used. However cannot resolve
C2 C3 b(3) b(1) since 3 minimum label C2 .
Boyer showed lock resolution refutation-complete.

6. Completeness Proof
Proof Theorem 1 (Completeness). prove completeness, need show P |=s r implies
P `s r. showing existence lock deduction (r0 ) r0 subset
r (in sense H(r0 ) H(r), B + (r0 ) B + (r), B (r0 ) B (r))
construct deduction r P using inference rules.
6.1 Lock Deduction (r0 ) (P ) 0 (P )
V
Proposition 5 know (r) logical consequence ((P ) 0 (P ) ). Ideally,
want lock deduction (r) (P ) 0 (P ) . However, may possible
resolution refutation-complete. fact show lock deduction (r0 )
exist r0 , provided r contain atom head positive body.
first obtaining lock deduction D0 empty clause (P ) 0 (P )
negation (r). modify form deduction (r0 ) (P ) 0 (P ) ,
way restrictions lock deduction preserved.
211

fiW ONG

label literals (P ) 0 (P ) either 1 0: literal form x0 ,
give label 0, otherwise give label 1. example, (a b, c) becomes
a(1) b(1) c0(1)
0 (a b, c) becomes
a0(1) b0(0) c0(1)
Let r rule
a1 ; a2 ; ; ak b1 , b2 , , bm , c1 , c2 , , cn .
(r)
a1 ak b1 bm c01 c0n
Negating (r) gives us following set clauses, call N (r):
a1 , , ak , b1 , , bm , c01 , , c0n
label clauses way above. Note (r) contain literals label
1, N (r) set ofV
single-literal clauses, may also label 0. Since
(r) consequence ((P ) 0 (P ) ), adding negation (r) makes unsatisfiable.
Therefore lock deduction D0 empty clause (P ) 0 (P ) plus N (r).
next step, construct new lock deduction contain clauses
N (r). Let C10 , , Cn0 clauses D0 . construct inductively clauses C1 , , Cn :
Ci0 resolvent. case set Ci := Ci0 . Note Ci0 N (r),
removed next step construction.
Ci0 resolvent Cj0 Ck0 literals x x, neither Cj0 Ck0 N (r).
set Ci resolvent Cj Ck x x.
Ci0 resolvent Cj0 Ck0 , one N (r). Without loss generality, assume
Cj0 N (r). case set Ci := Ck .
complete construction, remove every Ci N (r) resolvent.
remaining clauses form deduction D. Note possible Ci0 resolvent
two clauses N (r). r contain atom head
positive body, hence N (r) cannot contain pair complementary literals. Note also
final clause Cn never removed, since Cn0 empty clause, N (r).
Example 3. Suppose P program consisting single rule
b.
r rule
b, a.
(P ) 0 (P ) consists clauses
a(1) b(1)

a0(1) b0(0)

a(1) a0(1)
212

b(1) b0(1)

fiS OUND C OMPLETE NFERENCE RULES SE-C ONSEQUENCE

(r) b(1) a0(1) (after labelling). negation (r), denoted N (r), consists clauses
b(1)

a0(0)

example lock deduction D0 empty clause (P ) 0 (P ) N (r):
(1)
(2)
(3)
(4)
(5)
(6)
(7)

b(1)
b(1) b0(1)
b0(1)
a0(1) b0(0)
a0(1)
a0(0)


N (r)

resolvent (1), (2)
0 (P )
resolvent (3), (4)
N(r)
resolvent (5), (6)

construction produces following sequence clauses:
(1)
(2)
(3)
(4)
(5)
(6)
(7)

b(1)
b(1) b0(1)
b(1) b0(1)
a0(1) b0(0)
a0(1) b(1)
a0(0)
a0(1) b(1)


copy (2)
0 (P )
resolvent (3), (4)
resolvent (3), (4)copy (5)

clauses marked removed, resulting deduction formed remaining
clauses.
construction, deduction. need show satisfies restrictions lock deduction. addition, show clause Ci consists Ci0 zero literals
(r) added, i.e. Ci = Ci0 disjunction zero literals (r).
proof induction.
Ci0 resolvent. case Ci Ci0 .
Ci0 resolvent clauses Cj0 Ck0 literals x x, neither N (r).
Ci resolvent Cj Ck literals x x. induction, Cj = Cj0 j
Ck = Ck0 k . x occur j k , Ci = Ci0 j k . Otherwise
Ci = Ci0 contains literals j k except possibly x x.
cases, Ci consists Ci0 plus zero literals (r).
resolution step satisfies restriction lock deduction added literals
labelled 1, highest label value use. Thus x lowest label value
Cj0 x Ck0 , must hold x Cj x Ck .
Ci0 resolvent clauses Cj0 Ck0 , Cj0 N (r). Since clause N (r)
negation literal found (r) (note might carry different labels), must
literal (r) Cj0 = Ck0 = Ci0 . construction Ci gives us
Ci = Ck , induction Ck = Ck0 k . Therefore Ci = Ci0 k .
213

fiW ONG

shown lock deduction, clause Ci consists Ci0 zero
literals (r) added. Recall final clause D0 empty clause. Therefore
final clause clause contains subset literals (r), hence
lock deduction (r0 ) r0 subset r.
6.2 Existence Deduction r P
Suppose lock deduction (r) (P ) 0 (P ) labelling described
above. prove induction P `s r.
BASE C ASE
(r) either (P ) 0 (P ) .
(r) (P ). r P , therefore P `s r.
(r) 0 (P ). means (r) = 0 (s) P . B + (s) must empty since
literals form x0 (r). Hence write
a1 ; ; ak c1 , cn .
r
a1 , , ak , c1 , cn .
Since `s ai , ai , move atoms head negative part using WGPPE.
Therefore P `s r.
(r) . case r
x, x.
atom x, `s r applying CONTRA.
NDUCTION TEP
(r) resolvent clauses . literal resolved either a0 a0
atom a. Assume without loss generality positive literal negative
literal .
literal resolved on. Since literal form x0
resolvent, cannot literal form . Therefore find logic
program rules = (s) = (t). inference rule WGPPE gives us
s, `s r
P `s s, induction. Therefore P `s r.
a0 a0 literal resolved on. a0 , logic program
rule = (t). However, find logic program rule = (s),
lock resolution property: a0 labelled 1 literal form
x0 , labelled 0, .
214

fiS OUND C OMPLETE NFERENCE RULES SE-C ONSEQUENCE

resolvent, literal resolved must form x0 x0 , a0
parent clause contains x0 . Again, lock resolution
property: presence a0 clause prevents resolution literal
labelled 0. Let 2 parent clause containing x0 , let 2 parent clause.
2 contains literals labelled 1, find logic program rule s2
2 = (s2 ).
repeating this, form chain resolvents 1 (= ), 2 , , n 1 (= ), 2 , , n .
resolvent i+1 i+1 literal a0i i+1 a0i i+1 .
logic program rule si corresponding (si ) = .
extend chain far possible, n resolvent. n contains literals
form x0 , come 0 (P ). Therefore P 0 (t) = n .
Observe rule
X a1 , , , Y.
sets atoms X, . cannot additional positive atoms body,
correspond literals form x0 n , literals remaining
result chain resolution steps, (r).
inference rule S-HYP gives us
s1 , , sn , `s r
P `s s1 , , sn induction, P `s P . Therefore P `s r.
6.3 Final Step
shown P |=s r, find r0 subset r P `s r0 , apart
special cases. final step proof given applying NONMIN, allows
us deduction r0 `s r r0 subset r.
special case r contains atom head positive body handled
observing r produced using rules TAUT followed NONMIN, shows `s r.
Therefore P |=s r implies P `s r.

7. Conclusion
paper presented sound complete set inference rules SE-consequence
disjunctive logic programs, consisting number well-known logic program transformation
rules, TAUT, CONTRA, NONMIN, WGPPE, plus new rule call S-HYP. proved
set rules complete SE-consequence using reduction logic programs
propositional clauses apply restricted form resolution. result leads
syntactic definition closure operator logic programs strong equivalence. Future work
involves applying construct logic program update operators respect strong equivalence,
well finding similar results notions equivalence logic programs.

Acknowledgments
thank anonymous reviewers many helpful suggestions used revising
paper. work partially supported scholarship National ICT Australia. NICTA
215

fiW ONG

funded Australian Governments Backing Australias Ability initiative, part
Australian Research Council.

References
Boyer, R. (1971). Locking: Restriction Resolution. Ph.D. thesis, University Texas, Austin.
Brass, S., & Dix, J. (1999). Semantics (disjunctive) logic programs based partial evaluation.
Journal Logic Programming, 38(3), 167213.
Cabalar, P. (2002). three-valued characterization strong equivalence logic programs.
Proceedings 18th National Conference Artificial Intelligence (AAAI-2002), pp. 106
111.
Eiter, T., Fink, M., Tompits, H., & Woltran, S. (2004). Simplifying logic programs uniform
strong equivalence. Proceedings 7th International Conference Logic Programming Nonmonotonic Reasoning, pp. 8799.
Gelfond, M., & Lifschitz, V. (1988). stable model semantics logic programming. Proceedings 5th International Conference Logic Programming, pp. 10701080.
Lifschitz, V., Pearce, D., & Valverde, A. (2001). Strongly equivalent logic programs. Computational
Logic, 2(4), 526541.
Lin, F. (2002). Reducing strong equivalence logic programs entailment classical propositional logic. Proceedings 8th International Conference Principles Knowledge
Representation Reasoning, pp. 170176.
Lin, F., & Chen, Y. (2007). Discovering classes strongly equivalent logic programs. Journal
Artificial Intelligence Research, 28, 431451.
Osorio, M., Navarro, J. A., & Arrazola, J. (2001). Equivalence answer set programming. Proceedings 11th International Workshop Logic Based Program Synthesis Transformation, pp. 5775.
Sakama, C., & Seki, H. (1997). Partial deduction disjunctive logic programming. Journal
Logic Programming, 32(3), 229245.
Turner, H. (2001). Strong equivalence logic programs default theories (made easy).
Proceedings 6th International Conference Logic Programming Nonmonotonic
Reasoning, pp. 8192.
Turner, H. (2003). Strong equivalence made easy: nested expressions weight constraints. Theory
Practice Logic Programming, 3(4-5), 609622.
Wang, K., & Zhou, L. (2005). Comparisons computation well-founded semantics disjunctive logic programs. ACM Transactions Computational Logic, 6(2), 295327.

216

fiJournal Artificial Intelligence Research 31 (2008) 399-429

Submitted 09/07; published 03/08

Global Inference Sentence Compression
Integer Linear Programming Approach
James Clarke

jclarke@ed.ac.uk

Mirella Lapata

mlap@inf.ed.ac.uk

School Informatics
University Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK

Abstract
Sentence compression holds promise many applications ranging summarization
subtitle generation. work views sentence compression optimization problem
uses integer linear programming (ILP) infer globally optimal compressions
presence linguistically motivated constraints. show previous formulations
sentence compression recast ILPs extend models novel global
constraints. Experimental results written spoken texts demonstrate improvements
state-of-the-art models.

1. Introduction
computational treatment sentence compression recently attracted much attention
literature. task viewed producing summary single sentence
retains important information remains grammatical (Jing, 2000). sentence
compression mechanism would greatly benefit wide range applications. example,
summarization, could improve conciseness generated summaries (Jing, 2000;
Lin, 2003; Zajic, Door, Lin, & Schwartz, 2007). examples include compressing text
displayed small screens mobile phones PDAs (Corston-Oliver, 2001),
subtitle generation spoken transcripts (Vandeghinste & Pan, 2004), producing
audio scanning devices blind (Grefenstette, 1998).
Sentence compression commonly expressed word deletion problem: given input source sentence words x = x1 , x2 , . . . , xn , aim produce target compression
removing subset words (Knight & Marcu, 2002). compression problem extensively studied across different modeling paradigms, supervised
unsupervised. Supervised models typically trained parallel corpus source sentences target compressions come many flavors. Generative models aim model
probability target compression given source sentence either directly (Galley
& McKeown, 2007) indirectly using noisy-channel model (Knight & Marcu, 2002;
Turner & Charniak, 2005), whereas discriminative formulations attempt minimize error
rate training set. include decision-tree learning (Knight & Marcu, 2002), maximum entropy (Riezler, King, Crouch, & Zaenen, 2003), support vector machines (Nguyen,
Shimazu, Horiguchi, Ho, & Fukushi, 2004), large-margin learning (McDonald, 2006).
c
2008
AI Access Foundation. rights reserved.

fiClarke & Lapata

Unsupervised methods dispense parallel corpus generate compressions either
using rules (Turner & Charniak, 2005) language model (Hori & Furui, 2004).
Despite differences formulation, approaches model compression process
using local information. instance, order decide words drop, exploit
information adjacent words constituents. Local models good job
producing grammatical compressions, however somewhat limited scope since
cannot incorporate global constraints compression output. constraints
consider sentence whole instead isolated linguistic units (words constituents).
give concrete example may want ensure target compression verb,
provided source one first place. verbal arguments present
compression. pronouns retained. constraints fairly intuitive
used instill linguistic also task specific information model.
instance, application compresses text displayed small screens would
presumably higher compression rate system generating subtitles spoken
text. global constraint could force former system generate compressions
fixed rate fixed number words.
Existing approaches model global properties compression problem
good reason. Finding best compression source sentence given space
possible compressions1 (this search process often referred decoding inference)
become intractable many constraints overly long sentences. Typically,
decoding problem solved efficiently using dynamic programming often conjunction
heuristics reduce search space (e.g., Turner & Charniak, 2005). Dynamic
programming guarantees find global optimum provided principle optimality holds. principle states given current state, optimal decision
remaining stages depend previously reached stages previously made
decisions (Winston & Venkataramanan, 2003). However, know false
case sentence compression. example, included modifiers left
noun compression probably include noun include verb
also include arguments. dynamic programming approach cannot
easily guarantee constraints hold.
paper propose novel framework sentence compression incorporates
constraints compression output allows us find optimal solution.
formulation uses integer linear programming (ILP), general-purpose exact framework
NP-hard problems. Specifically, show previously proposed models recast
integer linear programs. extend models constraints express
linear inequalities. Decoding framework amounts finding best solution given
linear (scoring) function set linear constraints either global local.
Although ILP previously used sequence labeling tasks (Roth & Yih, 2004;
Punyakanok, Roth, Yih, & Zimak, 2004), application natural language generation
less widespread. present three compression models within ILP framework,
representative unsupervised (Knight & Marcu, 2002), semi-supervised (Hori & Furui,
2004), fully supervised modeling approach (McDonald, 2006). propose small
number constraints ensuring compressions structurally semantically
1. 2n possible compressions n number words sentence.

400

fiGlobal Inference Sentence Compression

valid experimentally evaluate impact compression task. cases,
show added constraints yield performance improvements.
remainder paper organized follows. Section 2 provides overview
related work. Section 3 present ILP framework compression models
employ experiments. constraints introduced Section 3.5. Section 4.3
discusses experimental set-up Section 5 presents results. Discussion future
work concludes paper.

2. Related Work
paper develop several ILP-based compression models. presenting
models, briefly summarize previous work addressing sentence compression emphasis data-driven approaches. Next, describe ILP techniques used
past solve inference problems natural language processing (NLP).
2.1 Sentence Compression
Jing (2000) perhaps first tackle sentence compression problem. approach
uses multiple knowledge sources determine phrases sentence remove. Central
system grammar checking module specifies sentential constituents
grammatically obligatory therefore present compression.
achieved using simple rules large-scale lexicon. knowledge sources include
WordNet corpus evidence gathered parallel corpus source-target sentence
pairs. phrase removed grammatically obligatory, focus
local context reasonable deletion probability (estimated parallel corpus).
contrast Jing (2000), bulk research sentence compression relies exclusively corpus data modeling compression process without recourse extensive knowledge sources (e.g., WordNet). large number approaches based
noisy-channel model (Knight & Marcu, 2002). approaches consist language
model P (y) (whose role guarantee compression output grammatical), channel
model P (x|y) (capturing probability source sentence x expansion
target compression y), decoder (which searches compression maximizes
P (y)P (x|y)). channel model acquired parsed version parallel corpus;
essentially stochastic synchronous context-free grammar (Aho & Ullman, 1969) whose
rule probabilities estimated using maximum likelihood. Modifications model
presented Turner Charniak (2005) Galley McKeown (2007) improved
results.
discriminative models (Knight & Marcu, 2002; Riezler et al., 2003; McDonald, 2006;
Nguyen et al., 2004) sentences represented rich feature space (also induced
parse trees) goal learn words word spans deleted given
context. instance, Knight Marcus (2002) decision-tree model, compression
performed deterministically tree rewriting process inspired shift-reduce
parsing paradigm. Nguyen et al. (2004) render model probabilistic use
support vector machines. McDonald (2006) formalizes sentence compression largemargin learning framework without making reference shift-reduce parsing. model
compression classification task: pairs words source sentence classified
401

fiClarke & Lapata

adjacent target compression. large number features defined
words, parts-of-speech, phrase structure trees dependencies. features
gathered adjacent words compression words in-between
dropped (see Section 3.4.3 detailed account).
compression models developed written text mind, Hori
Furui (2004) propose model automatically transcribed spoken text. model
generates compressions word deletion without using parallel data syntactic information way. Assuming fixed compression rate, searches compression
highest score using dynamic programming algorithm. scoring function consists language model responsible producing grammatical output, significance score
indicating whether word topical not, score representing speech recognizers
confidence transcribing given word correctly.
2.2 Integer Linear Programming NLP
ILPs constrained optimization problems objective function
constraints linear equations integer variables (see Section 3.1 details). ILP
techniques recently applied several NLP tasks, including relation extraction
(Roth & Yih, 2004), semantic role labeling (Punyakanok et al., 2004), generation
route directions (Marciniak & Strube, 2005), temporal link analysis (Bramsen, Deshpande,
Lee, & Barzilay, 2006), set partitioning (Barzilay & Lapata, 2006), syntactic parsing (Riedel
& Clarke, 2006), coreference resolution (Denis & Baldridge, 2007).
approaches combine local classifier inference procedure based
ILP. classifier proposes possible answers assessed presence global
constraints. ILP used make final decision consistent constraints
likely according classifier. example, semantic role labeling task involves
identifying verb-argument structure given sentence. Punyakanok et al. (2004) first
use SNOW, multi-class classifier2 (Roth, 1998), identify label candidate arguments.
observe labels assigned arguments sentence often contradict other.
resolve conflicts propose global constraints (e.g., argument
instantiated given verb, every verb least one argument) use
ILP reclassify output SNOW.
Dras (1999) develops document paraphrasing model using ILP. key premise
work cases one may want rewrite document conform
global constraints length, readability, style. proposed model three
ingredients: set sentence-level paraphrases rewriting text, set global constraints, objective function quantifies effect incurred paraphrases.
formulation, ILP used select paraphrases apply
global constraints satisfied. Paraphrase generation falls outside scope ILP
model sentence rewrite operations mainly syntactic provided module based
synchronous tree adjoining grammar (S-TAG, Shieber & Schabes, 1990). Unfortunately,
proof-of-concept presented; implementation evaluation module left
future work.
2. SNOWs learning algorithm variation Winnow update rule.

402

fiGlobal Inference Sentence Compression

work models sentence compression optimization problem. show previously proposed models reformulated context integer linear programming
allows us easily incorporate constraints decoding process. constraints linguistically semantically motivated designed bring less local
syntactic knowledge model help preserve meaning source sentence.
Previous work identified several important features compression task (Knight
& Marcu, 2002; McDonald, 2006); however, use global constraints novel
knowledge. Although sentence compression explicitly formulated terms
optimization, previous approaches rely optimization procedure generating
best compression. decoding process noisy-channel model searches best
compression given source channel models. However, compression found usually sub-optimal heuristics used reduce search space locally optimal
due search method employed. example, work Turner Charniak
(2005) decoder first searches best combination rules apply. traverses
list compression rules, removes sentences outside 100 best compressions (according channel model). list eventually truncated 25 compressions.
models (Hori & Furui, 2004; McDonald, 2006) compression score maximized
using dynamic programming however yield suboptimal results (see discussion
Section 1).
Contrary NLP work using ILP (a notable exception Roth & Yih, 2005),
view compression generation two stage process learning inference
carried sequentially (i.e., first local classifier hypothesizes list possible answers best answer selected using global constraints). models integrate
learning inference unified framework decoding takes place presence
available constraints, local global. Moreover, investigate influence
constraint set across models learning paradigms. Previous work typically formulates constraints single model (e.g., SNOW classifier) learning paradigm
(e.g., supervised). therefore assess constraint-based framework advocated
article influences performance expressive models (which require large amounts
parallel data) non-expressive ones (which use little parallel data none all).
words, able pose answer following question: kinds models
benefit constraint-based inference?
work close spirit rather different content Dras (1999). concentrate
compression, specific paraphrase type, apply models sentence-level.
constraints thus affect document whole individual sentences. Furthermore, compression generation integral part ILP models, whereas Dras assumes
paraphrases generated separate process.

3. Framework
section present details proposed framework sentence compression.
mentioned earlier, work models sentence compression directly optimization
problem. 2n possible compressions source sentence many
unreasonable, unlikely one compression satisfactory (Knight & Marcu, 2002). Ideally, require function captures operations
403

fiClarke & Lapata

(or rules) performed sentence create compression
time factoring desirable operation makes resulting compression.
perform search possible compressions select best one, determined
desirable is. wide range models expressed framework.
prerequisites implementing fairly low, require decoding process expressed linear function set linear constraints. practice, many
models rely Markov assumption factorization usually solved dynamic programming-based decoding process. algorithms formulated integer
linear programs little effort.
first give brief introduction integer linear programming, extension linear
programming readers unfamiliar mathematical programming. compression
models next described Section 3.4 constraints Section 3.5.
3.1 Linear Programming
Linear programming (LP) problems optimization problems constraints.
consist three parts:
Decision variables. variables control wish assign
optimal values to.
linear function (the objective function). function wish minimize
maximize. function influences values assigned decision variables.
Constraints. problems allow decision variables take certain
values. restrictions constraints.
terms best demonstrated simple example taken Winston
Venkataramanan (2003). Imagine manufacturer tables chairs shall call
Telfa Corporation. produce table, 1 hour labor 9 square board feet wood
required. Chairs require 1 hour labor 5 square board feet wood. Telfa
6 hours labor 45 square board feet wood available. profit made
table 8 GBP 5 GBP chairs. wish determine number tables
chairs manufactured maximize Telfas profit.
First, must determine decision variables. case define:
x1 = number tables manufactured
x2 = number chairs manufactured
objective function value wish maximize, namely profit.
Profit = 8x1 + 5x2
two constraints problem: must exceed 6 hours labor
45 square board feet wood must used. Also, cannot create negative
amount chairs tables:
404

fiGlobal Inference Sentence Compression

Labor constraint
x 1 + x2
Wood constraint
9x1 + 5x2
Variable constraints
x1
x2

6
45
0
0

decision variables, objective function constraints determined
express LP model:
max z = 8x1 + 5x2 (Objective function)
subject (s.t.)
x1 + x2
9x1 + 5x2
x1
x2

6 (Labor constraint)
45 (Wood constraint)
0
0

Two basic concepts involved solving LP problems feasibility region
optimal solution. optimal solution one constraints satisfied
objective function minimized maximized. specification value
decision variable referred point. feasibility region LP region
consisting set points satisfy LPs constraints. optimal solution
lies within feasibility region, point minimum maximum objective
function value.
set points satisfying single linear inequality half-space. feasibility region
defined intersection half-spaces (for linear inequalities) forms
polyhedron. Telfa example forms polyhedral set (a polyhedral convex set)
intersection four constraints. Figure 1a shows feasible region Telfa
example. find optimal solution graph line (or hyperplane) points
objective function value. maximization problems called isoprofit
line minimization problems isocost line. One isoprofit line represented
dashed black line Figure 1a. one isoprofit line find isoprofit
lines moving parallel original isoprofit line.
extreme points polyhedral set defined intersections lines
form boundaries polyhedral set (points B C Figure 1a).
shown LP optimal solution, extreme point globally
optimal. reduces search space optimization problem finding extreme
point highest lowest value. simplex algorithm (Dantzig, 1963) solves LPs
exploring extreme points polyhedral set. Specifically, moves one extreme
point adjacent extreme point (extreme points lie line segment)
optimal extreme point found. Although simplex algorithm exponential
worst-case complexity, practice algorithm efficient.
15
9
optimal solution Telfa example z = 165
4 , x1 = 4 , x2 = 4 . Thus,
achieve maximum profit 41.25 GBP must build 3.75 tables 2.25 chairs.
obviously impossible would expect people buy fractions tables chairs.
Here, want able constrain problem decision variables
take integer values. done Integer Linear Programming.
405

fiClarke & Lapata

a.

b.

10

10

9

9

= LPs feasible region

9x1 + 5x2 = 45

9x1+ 5x2 = 45

8

8

7

7

6 B

6

x2 5

x2 5

4

4

= IP feasible point
= IP relaxations feasible region

3

3

Optimal LP solution

Optimal LP solution
2

C

2

x 1 + x2 = 6

1
0



0

1

2

3

x1

4


5

6

x 1 + x2 = 6

11

7

0

0

1

2

3

x1

4

5

6

7

Figure 1: Feasible region Telfa example using linear (graph (a)) integer linear
(graph (b)) programming

3.2 Integer Linear Programming
Integer linear programming (ILP) problems LP problems
variables required non-negative integers. formulated similar manner
LP problems added constraint decision variables must take non-negative
integer values.
formulate Telfa problem ILP model merely add constraints x1
x2 must integer. gives:
max z = 8x1 + 5x2 (Objective function)
subject (s.t.)
x1 + x2
9x1 + 5x2
x1
x2


6 (Labor constraint)

45 (Wood constraint)
0; x1 integer
0; x2 integer

LP models, proved optimal solution lies extreme point
feasible region. case integer linear programs, wish consider points
integer values. illustrated Figure 1b Telfa problem. contrast
linear programming, solved efficiently worst case, integer programming
problems many practical situations NP-hard (Cormen, Leiserson, & Rivest, 1992).
406

fiGlobal Inference Sentence Compression

Fortunately, ILPs well studied optimization problem number techniques
developed find optimal solution. Two techniques cutting planes
method (Gomory, 1960) branch-and-bound method (Land & Doig, 1960).
briefly discuss methods here. detailed treatment refer interested
reader Winston Venkataramanan (2003) Nemhauser Wolsey (1988).
cutting planes method adds extra constraints slice parts feasible region
contains integer extreme points. However, process difficult
impossible (Nemhauser & Wolsey, 1988). branch-and-bound method enumerates
points ILPs feasible region prunes sections region known
sub-optimal. relaxing integer constraints solving resulting
LP problem (known LP relaxation). solution LP relaxation integral,
optimal solution. Otherwise, resulting solution provides upper bound
solution ILP. algorithm proceeds creating two new sub-problems based
non-integer solution one variable time. solved process
repeats optimal integer solution found.
Using branch-and-bound method, find optimal solution Telfa
problem z = 40, x1 = 5, x2 = 0; thus, achieve maximum profit 40 GBP, Telfa
must manufacture 5 tables 0 chairs. relatively simple problem, could
solved merely inspection. ILP problems involve many variables constraints
resulting feasible region large number integer points. branch-and-bound
procedure efficiently solve ILPs matter seconds forms part many
commercial ILP solvers. experiments use lp solve 3 , free optimization package
relies simplex algorithm brand-and-bound methods solving ILPs.
Note special circumstances solving methods may applicable.
example, implicit enumeration used solve ILPs variables binary
(also known pure 01 problems). Implicit enumeration similar branch-andbound method, systematically evaluates possible solutions, without however explicitly
solving (potentially) large number LPs derived relaxation. removes
much computational complexity involved determining sub-problem infeasible. Furthermore, class ILP problems known minimum cost network flow
problems (MCNFP), LP relaxation always yields integral solution. problems
therefore treated LP problems.
general, model yield optimal solution variables integers
constraint matrix property known total unimodularity. matrix totally
unimodular every square sub-matrix determinant equal 0, +1 1.
case constraint matrix looks totally unimodular, easier
problem solve branch-and-bound methods. practice good
formulate ILPs many variables possible coefficients 0, +1 1
constraints (Winston & Venkataramanan, 2003).
3.3 Constraints Logical Conditions
Although integer variables ILP problems may take arbitrary values, frequently
restricted 0 1. Binary variables (01 variables) particularly useful rep3. software available http://lpsolve.sourceforge.net/.

407

fiClarke & Lapata

Condition
Implication
Iff

Xor



Statement
b
b
b c
xor b xor c
b


Constraint
ba0
ab=0
a+b+c1
a+b+c=1
= 1; b = 1
1a=1

Table 1: represent logical conditions using binary variables constraints ILP.

resenting variety logical conditions within ILP framework use constraints. Table 1 lists several logical conditions equivalent constraints.
also express transitivity, i.e., c b. Although often thought transitivity expressed polynomial expression binary
variables (i.e., ab = c), possible replace latter following linear inequalities (Williams, 1999):

(1 c) + 1
(1 c) + b 1
c + (1 a) + (1 b) 1
easily extended model indicator variables representing whether set binary
variables take certain values.
3.4 Compression Models
section describe three compression models reformulate integer linear
programs. first model simple language model used baseline
previous research (Knight & Marcu, 2002). second model based work Hori
Furui (2004); combines language model corpus-based significance scoring
function (we omit confidence score derived speech recognizer since
models applied text only). model requires small amount parallel data
learn weights language model significance score.
third model fully supervised, uses discriminative large-margin framework
(McDonald, 2006), trained trained larger parallel corpus. chose model
instead popular noisy-channel decision-tree models, two reasons, practical one theoretical one. First, McDonalds (2006) model delivers performance superior
decision-tree model (which turn performs comparably noisy-channel). Second, noisy channel entirely appropriate model sentence compression.
uses language model trained uncompressed sentences even though represents
probability compressed sentences. result, model consider compressed sentences less likely uncompressed ones (a discussion provided Turner &
Charniak, 2005).
408

fiGlobal Inference Sentence Compression

3.4.1 Language Model
language model perhaps simplest model springs mind. require
parallel corpus (although relatively large monolingual corpus necessary training),
naturally prefer short sentences longer ones. Furthermore, language model
used drop words either infrequent unseen training corpus. Knight
Marcu (2002) use bigram language model baseline noisy-channel
decision-tree models.
Let x = x1 , x2 , . . . , xn denote source sentence wish generate target
compression. introduce decision variable word source constrain
binary; value 0 represents word dropped, whereas value 1 includes
word target compression. Let:
=

(

1 xi compression
[1 . . . n]
0 otherwise

using unigram language model, objective function would maximize
overall sum decision variables (i.e., words) multiplied unigram probabilities
(all probabilities throughout paper log-transformed):
max

n
X

P (xi )

(1)

i=1

Thus, word selected, corresponding given value 1, probability
P (xi ) according language model counted total score.
unigram language model probably generate many ungrammatical compressions.
therefore use context-aware model objective function, namely trigram
model. Dynamic programming would typically used decode language model
traversing sentence left-to-right manner. algorithm efficient provides
context required conventional language model. However, difficult
impossible incorporate global constraints model decisions word
inclusion cannot extend beyond three word window. formulating decoding process
trigram language model integer linear program able take account
constraints affect compressed sentence globally. process much
involved task unigram case context, instead must
make decisions based word sequences rather isolated words. first create
additional decision variables:
=

(

ij =



1

ijk =

1 xi starts compression
[1 . . . n]
0 otherwise

sequence xi , xj ends
compression
[0 . . . n 1]

0 otherwise
j [i + 1 . . . n]



1

sequence xi , xj , xk [0 . . . n 2]
compression j [i + 1 . . . n 1]

0 otherwise
k [j + 1 . . . n]
409

fiClarke & Lapata

objective function given Equation (2). sum possible trigrams
occur compressions source sentence x0 represents start
token xi ith word sentence x. Equation (3) constrains decision variables
binary.
max z =

n
X

P (xi |start)
i=1
n2
n
X n1
X X

+

ijk P (xk |xi , xj )

i=1 j=i+1 k=j+1

+

n1
X

n
X

ij P (end|xi , xj )

(2)

i=0 j=i+1

subject to:

, , ij , ijk = 0 1

(3)

objective function (2) allows combination trigrams selected.
means invalid trigram sequences (e.g., two trigrams containing end token)
could appear target compression. avoid situation introducing sequential
constraints (on decision variables , ijk , , ij ) restrict set allowable
trigram combinations.
Constraint 1

Exactly one word begin sentence.
n
X

= 1

(4)

i=1

Constraint 2 word included sentence must either start sentence
preceded two words one word start token x0 .
k k

k2
X k1
X

ijk = 0

(5)

i=0 j=1

k : k [1 . . . n]
Constraint 3 word included sentence must either preceded one
word followed another must preceded one word end sentence.
j

j1
X

n
X

ijk

i=0 k=j+1

j1
X

ij = 0

(6)

i=0

j : j [1 . . . n]

Constraint 4 word sentence must followed two words followed
one word end sentence must preceded one word end
sentence.


n1
X

n
X

j=i+1 k=j+1

ijk

n
X

j=i+1

410

ij

i1
X

hi = 0

h=0

: [1 . . . n]

(7)

fiGlobal Inference Sentence Compression

Constraint 5

Exactly one word pair end sentence.
n1
X

n
X

ij = 1

(8)

i=0 j=i+1

sequential constraints described ensure second order factorization (for
trigrams) holds different compression-specific constraints presented Section 3.5.
Unless normalized sentence length, language model naturally prefer one-word
output. normalization however non-linear cannot incorporated ILP
formulation. Instead, impose constraint length compressed sentence.
Equation (9) forces compression contain least b tokens.
n
X

b

(9)

i=1

Alternatively, could force compression exactly b tokens (by substituting
inequality equality (9)) less b tokens (by replacing ).4
constraint (9) language model-specific used elsewhere.
3.4.2 Significance Model
language model described notion content words include
compression thus prefers words seen before. words constituents
different relative importance different documents even sentences.
Inspired Hori Furui (2004), add objective function (see Equation (2))
significance score designed highlight important content words. Hori Furuis
original formulation word weighted score similar un-normalized tf idf .
significance score applied indiscriminately words sentence solely
topic-related words, namely nouns verbs. score differs one respect. combines
document-level sentence-level significance. addition tf idf , word
weighted level embedding syntactic tree.
Intuitively, sentence multiply nested clauses, deeply embedded clauses
tend carry semantic content. illustrated Figure 2 depicts
clause embedding sentence Mr Field said resign reselected,
move could divide party nationally. Here, important information
conveyed clauses S3 (he resign) S4 (if reselected) embedded.
Accordingly, give weight words found clauses main
clause (S1 Figure 2). simple way enforce give clauses weight proportional
level embedding. modified significance score becomes:
I(xi ) =

Fa
l
fi log
N
Fi

(10)

xi topic word, fi Fi frequency xi document corpus
respectively, Fa sum topic words corpus, l number clause
4. Compression rate also limited range including two inequality constraints.

411

fiClarke & Lapata

S1
S2
Mr Field said
S3
resign
S4
reselected
, move
SBAR
could divide party nationally

Figure 2: clause embedding sentence Mr Field said resign
reselected, move could divide party nationally; nested boxes
correspond nested clauses.

constituents xi , N deepest level clause embedding. Fa Fi
estimated large document collection, fi document-specific, whereas Nl sentencespecific. So, Figure 2 term Nl 1.0 (4/4) clause S4 , 0.75 (3/4) clause S3 ,
on. Individual words inherit weight clauses.
modified objective function significance score given below:
max z =

n
X

I(xi ) +

i=1
n2
X n1
X

+

n
X

P (xi |start)

i=1

n
X

ijk P (xk |xi , xj )

i=1 j=i+1 k=j+1

+

n1
X

n
X

ij P (end|xi , xj )

(11)

i=0 j=i+1

also add weighting factor () objective, order counterbalance importance language model significance score. weight tuned small
parallel corpus. sequential constraints Equations (4)(8) used ensure
trigrams combined valid way.
3.4.3 Discriminative Model
fully supervised model, used discriminative model presented McDonald
(2006). model uses large-margin learning framework coupled feature set
defined compression bigrams syntactic structure.
Let x = x1 , . . . , xn denote source sentence target compression = y1 , . . . , ym
yj occurs x. function L(yi ) {1 . . . n} maps word yi target com412

fiGlobal Inference Sentence Compression

pression index word source sentence, x. also include constraint
L(yi ) < L(yi+1 ) forces word x occur compression
y. Let score compression sentence x be:
(12)

s(x, y)

score factored using first-order Markov assumption words target
compression give:
s(x, y) =

|y|
X

s(x, L(yj1 ), L(yj ))

(13)

j=2

score function defined dot product high dimensional feature
representation corresponding weight vector:
s(x, y) =

|y|
X

w f (x, L(yj1 ), L(yj ))

(14)

j=2

Decoding model amounts finding combination bigrams maximizes
scoring function (14). McDonald (2006) uses dynamic programming approach
maximum score found left-to-right manner. algorithm extension
Viterbi case scores factor dynamic sub-strings (Sarawagi & Cohen,
2004; McDonald, Crammer, & Pereira, 2005a). allows back-pointers used
reconstruct highest scoring compression well k-best compressions.
similar trigram language model decoding process (see Section 3.4.1),
except bigram model used. Consequently, ILP formulation slightly
simpler trigram language model. Let:
=

(

1 xi compression
(1 n)
0 otherwise

introduce decision variables:
=
=
ij =

(

(

(

1 xi starts compression
[1 . . . n]
0 otherwise

1 word xi ends compression
0 otherwise
[1 . . . n]

1 sequence xi , xj compression [1 . . . n 1]
0 otherwise
j [i + 1 . . . n]

discriminative model expressed as:
max z =

n
X


i=1
n1
X

s(x, 0, i)

+

s(x, i, n + 1)

+

n
X

i=1 j=i+1
n
X
i=1

413

ij s(x, i, j)
(15)

fiClarke & Lapata

Constraint 1

Exactly one word begin sentence.
n
X

= 1

(16)

i=1

Constraint 2 word included sentence must either start compression
follow another word.

j j

j
X

ij = 0

(17)

i=1

j : j [1 . . . n]
Constraint 3 word included sentence must either followed another
word end sentence.



n
X

ij = 0

(18)

j=i+1

: [1 . . . n]

Constraint 4

Exactly one word end sentence.
n
X

= 1

(19)

i=1

Again, sequential constraints Equations (16)(19) necessary ensure
resulting combination bigrams valid.
current formulation provides single optimal compression given model. However, McDonalds (2006) dynamic programming algorithm capable returning k-best
compressions; useful learning algorithm described later. order produce
k-best compressions, must rerun ILP extra constraints forbid previous
solutions. words, first formulate ILP above, solve it, add solution
k-best list, create set constraints forbid configuration decision
variables form current solution. procedure repeated k compressions
found.
computation compression score crucially relies dot product
high dimensional feature representation corresponding weight vector (see Equation (14)). McDonald (2006) employs rich feature set defined adjacent words
individual parts-of-speech, dropped words phrases source sentence, dependency structures (also source sentence). features designed mimic
information presented previous noisy-channel decision-tree models Knight
Marcu (2002). Features adjacent words used proxy language model
noisy channel. Unlike models, treat parses gold standard, McDonald
uses dependency information another form evidence. Faced parses
noisy learning algorithm reduce weighting given features prove
414

fiGlobal Inference Sentence Compression

poor discriminators training data. Thus, model much robust
portable across different domains training corpora.
weight vector, w learned using Margin Infused Relaxed Algorithm (MIRA,
Crammer & Singer, 2003) discriminative large-margin online learning technique (McDonald, Crammer, & Pereira, 2005b). algorithm learns compressing sentence
comparing result gold standard. weights updated score
correct compression (the gold standard) greater score compressions margin proportional loss. loss function number words falsely
retained dropped incorrect compression relative gold standard. source
sentence exponentially many compressions thus exponentially many margin
constraints. render learning computationally tractable, McDonald et al. (2005b) create
constraints k compressions currently highest score, bestk (x; w).
3.5 Constraints
ready describe compression-specific constraints. models presented
previous sections contain sequential constraints thus equivalent
original formulation. constraints linguistically semantically motivated
similar fashion grammar checking component Jing (2000). However,
rely additional knowledge sources (such grammar lexicon WordNet)
beyond parse grammatical relations source sentence. obtain
RASP (Briscoe & Carroll, 2002), domain-independent, robust parsing system English.
However, parser broadly similar output (e.g., Lin, 2001) could also serve
purposes. constraints revolve around modification, argument structure, discourse
related factors.
Modifier Constraints Modifier constraints ensure relationships head words
modifiers remain grammatical compression:
j 0

(20)

i, j : xj xi ncmods
j 0

(21)

i, j : xj xi detmods
Equation (20) guarantees include non-clausal modifier5 (ncmod) compression (such adjective noun) head modifier must also included;
repeated determiners (detmod) (21). Table 2 illustrate constraints disallow deletion certain words (starred sentences denote compressions
would possible given constraints). example, modifier word Pasok
sentence (1a) compression, head Party also included (see (1b)).
also want ensure meaning source sentence preserved
compression, particularly face negation. Equation (22) implements forcing
compression head included (see sentence (2b) Table 2). similar
constraint added possessive modifiers (e.g., his, our), including genitives (e.g., Johns
5. Clausal modifiers (cmod) adjuncts modifying entire clauses. example ate cake
hungry, because-clause modifier sentence ate cake.

415

fiClarke & Lapata

1a.
1b.
2a.
2b.
2c.
3a.
3b.
3c.
3d.
3e.
3f.

became power player Greek Politics 1974, founded
socialist Pasok Party.
*He became power player Greek Politics 1974, founded
Pasok.
took troubled youth dont fathers, brought
room Dads dont children.
*We took troubled youth fathers, brought
room Dads children.
*We took troubled youth dont fathers, brought
room Dads dont children.
chain stretched Uganda Grenada Nicaragua, since 1970s.
*Stretched Uganda Grenada Nicaragua, since 1970s.
*The chain Uganda Grenada Nicaragua, since 1970s.
*The chain stretched Uganda Grenada Nicaragua, since 1970s.
*The chain stretched Grenada Nicaragua, since 1970s.
*The chain stretched Uganda Grenada Nicaragua, since 1970s.
Table 2: Examples compressions disallowed set constraints.

gift), shown Equation (23). example possessive constraint given
sentence (2c) Table 2.
j = 0

(22)

i, j : xj xi ncmods xj =
j = 0

(23)

i, j : xj xi possessive mods
Argument Structure Constraints also define intuitive constraints take
overall sentence structure account. first constraint (Equation (24)) ensures
verb present compression arguments,
arguments included compression verb must also included. thus
force program make decision verb, subject, object (see
sentence (3b) Table 2).
j = 0

(24)

i, j : xj subject/object verb xi
second constraint forces compression contain least one verb provided
source sentence contains one well:
X

1

(25)

i:xi verbs

constraint entails possible drop main verb stretched sentence (3a) (see also sentence (3c) Table 2).
416

fiGlobal Inference Sentence Compression

sentential constraints include Equations (26) (27) apply prepositional phrases subordinate clauses. constraints force introducing term
(i.e., preposition, subordinator) included compression word
within syntactic constituent also included. subordinator mean wh-words
(e.g., who, which, how, where), word that, subordinating conjunctions (e.g., after,
although, because). reverse also true, i.e., introducing term included,
least one word syntactic constituent also included.
j 0

(26)

i, j : xj PP/SUB
xi starts PP/SUB
X

j 0

(27)

i:xi PP/SUB

j : xj starts PP/SUB
example consider sentence (3d) Table 2. Here, cannot drop preposition
Uganda compression. Conversely, must include Uganda
compression (see sentence (3e)).
also wish handle coordination. two head words conjoined source
sentence, included compression coordinating conjunction must
also included:
(1 ) + j 1

(28)

(1 ) + k 1

(29)

+ (1 j ) + (1 k ) 1

(30)

i, j, k : xj xk conjoined xi
Consider sentence (3f) Table 2. Uganda Nicaragua present
compression, must include conjunction and.
Finally, Equation (31) disallows anything within brackets source sentence
included compression. somewhat superficial attempt excluding
parenthetical potentially unimportant material compression.
= 0

(31)

: xi bracketed words (inc parentheses)
Discourse Constraints discourse constraint concerns personal pronouns. Specifically, Equation (32) forces personal pronouns included compression.
constraint admittedly important generating coherent documents (as opposed
individual sentences). nevertheless impact sentence-level compressions,
particular verbal arguments missed parser. pronominal,
constraint (32) result grammatical output since argument structure
source sentence preserved compression.
= 1
: xi personal pronouns
417

(32)

fiClarke & Lapata

note constraints described would captured
models learn synchronous deletion rules corpus. example, noisy-channel
model Knight Marcu (2002) learns drop head latter modified
adjective noun, since transformations DT NN DT AJD NN ADJ
almost never seen data. Similarly, coordination constraint (Equations (28)(30))
would enforced using Turner Charniaks (2005) special rules enhance
parallel grammar rules modeling structurally complicated deletions
attested corpus. designing constraints aimed capturing appropriate
deletions many possible models, including rely training corpus
explicit notion parallel grammar (e.g., McDonald, 2006).
modification constraints would presumably redundant noisy-channel model,
could otherwise benefit specialized constraints, e.g., targeting sparse rules
noisy parse trees, however leave future work.
Another feature modeling framework presented deletions (or nondeletions) treated unconditional decisions. example, require drop
noun adjective-noun sequences adjective deleted well. also require
always include verb compression source sentence one. hardwired decisions could cases prevent valid compressions considered. instance,
possible compress sentence appropriate behavior
appropriate orBob loves Mary John loves Susan Bob loves Mary John
Susan. Admittedly lose expressive power, yet ensure compressions
broadly grammatically, even unsupervised semi-supervised models. Furthermore, practice find models consistently outperform non-constraint-based
alternatives, without extensive constraint engineering.
3.6 Solving ILP
mentioned earlier (Section 3.1), solving ILPs NP-hard. cases coefficient matrix unimodular, shown optimal solution linear
program integral. Although coefficient matrix problems unimodular,
obtained integral solutions sentences experimented (approximately 3,000,
see Section 4.1 details). conjecture due fact variables 0, +1 1 coefficients constraints therefore constraint matrix
shares many properties unimodular matrix. generate solve ILP every
sentence wish compress. Solve times less second per sentence (including
input-output overheads) models presented here.

4. Experimental Set-up
evaluation experiments motivated three questions: (1) constraintbased compression models deliver performance gains non-constraint-based ones?
expect better compressions model variants incorporate compression-specific
constraints. (2) differences among constraint-based models? Here, would like
investigate much modeling power gained addition constraints.
example, may case state-of-the-art model like McDonalds (2006)
benefit much addition constraints. effect much bigger less
418

fiGlobal Inference Sentence Compression

sophisticated models. (3) models reported paper port across domains?
particular, interested assessing whether models proposed constraints
general robust enough produce good compressions written spoken
texts.
next describe data sets models trained tested (Section 4.1),
explain model parameters estimated (Section 4.2) present evaluation setup
(Section 4.3). discuss results Section 5.
4.1 Corpora
intent assess performance models described written spoken
text. appeal written text understandable since summarization work today
focuses domain. Speech data provides natural test-bed compression
applications (e.g., subtitle generation) also poses additional challenges. Spoken utterances ungrammatical, incomplete, often contain artefacts false starts,
interjections, hesitations, disfluencies. Rather focusing spontaneous speech
abundant artefacts, conduct study less ambitious domain
broadcast news transcripts. lies in-between extremes written text spontaneous speech scripted beforehand usually read autocue.
Previous work sentence compression almost exclusively used Ziff-Davis corpus
training testing purposes. corpus originates collection news articles
computer products. created automatically matching sentences occur
article sentences occur abstract (Knight & Marcu, 2002). abstract
sentences contain subset source sentences words word order
remain same. earlier work (Clarke & Lapata, 2006) argued
Ziff-Davis corpus ideal studying compression several reasons. First, showed
human-authored compressions differ substantially Ziff-Davis tends
aggressively compressed. Second, humans likely drop individual words
lengthy constituents. Third, test portion Ziff-Davis contains solely 32 sentences. extremely small data set reveal statistically significant differences
among systems. fact, previous studies relied almost exclusively human judgments
assessing well-formedness compressed output, significance tests reported
by-subjects analyses only.
thus focused present study manually created corpora. Specifically,
asked annotators perform sentence compression removing tokens sentence-bysentence basis. Annotators free remove words deemed superfluous provided
deletions: (a) preserved important information source sentence,
(b) ensured compressed sentence remained grammatical. wished, could leave
sentence uncompressed marking inappropriate compression.
allowed delete whole sentences even believed contained information content
respect story would blur task abstracting. Following
guidelines, annotators produced compressions 82 newspaper articles (1,433 sentences)
British National Corpus (BNC) American News Text corpus (henceforth
written corpus) 50 stories (1,370 sentences) HUB-4 1996 English Broadcast
News corpus (henceforth spoken corpus). written corpus contains articles LA
419

fiClarke & Lapata

Times, Washington Post, Independent, Guardian Daily Telegraph. spoken
corpus contains broadcast news variety networks (CNN, ABC, CSPAN NPR)
manually transcribed segmented story sentence level.
corpora split training, development testing sets6 randomly article
boundaries (with set containing full stories) publicly available http:
//homepages.inf.ed.ac.uk/s0460084/data/.
4.2 Parameter Estimation
work present three compression models ranging unsupervised semisupervised, fully supervised. unsupervised model simply relies trigram language model driving compression (see Section 3.4.1). estimated 25 million tokens North American corpus using CMU-Cambridge Language Modeling
Toolkit (Clarkson & Rosenfeld, 1997) vocabulary size 50,000 tokens GoodTuring discounting. discourage one-word output force ILP generate compressions whose length less 40% source sentence (see constraint (9)).
semi-supervised model weighted combination word-based significance score
language model (see Section 3.4.2). significance score calculated using
25 million tokens American News Text corpus. optimized weight (see
Equation (11)) small subset training data (three documents case) using Powells method (Press, Teukolsky, Vetterling, & Flannery, 1992) loss function
based F-score grammatical relations found gold standard compression
systems best compression (see Section 4.3 details). optimal weight
approximately 1.8 written corpus 2.2 spoken corpus.
McDonalds (2006) supervised model trained written spoken training
sets. implementation used feature sets McDonald, difference
phrase structure dependency features extracted output
Roarks (2001) parser. McDonald uses Charniaks (2000) parser performs comparably.
model learnt using k-best compressions. development data, found
k = 10 provided best performance.
4.3 Evaluation
Previous studies relied almost exclusively human judgments assessing wellformedness automatically derived compressions. typically rated naive subjects two dimensions, grammaticality importance (Knight & Marcu, 2002). Although
automatic evaluation measures proposed (Riezler et al., 2003; Bangalore, Rambow, & Whittaker, 2000) use less widespread, suspect due small size
test portion Ziff-Davis corpus commonly used compression work.
evaluate output models two ways. First, present results using
automatic evaluation measure put forward Riezler et al. (2003). compare
grammatical relations found system compressions found gold
standard. allows us measure semantic aspects summarization quality terms
grammatical-functional information quantified using F-score. Furthermore,
6. splits 908/63/462 sentences written corpus 882/78/410 sentences spoken
corpus.

420

fiGlobal Inference Sentence Compression

Clarke Lapata (2006) show relations-based F-score correlates reliably
human judgments compression output. Since test corpora larger ZiffDavis (by factor ten), differences among systems highlighted using
significance testing.
implementation F-score measure used grammatical relations annotations
provided RASP (Briscoe & Carroll, 2002). parser particularly appropriate
compression task since provides parses full sentences sentence fragments
generally robust enough analyze semi-grammatical sentences. calculated F-score
relations provided RASP (e.g., subject, direct/indirect object, modifier; 15
total).
line previous work also evaluate models eliciting human judgments.
Following work Knight Marcu (2002), conducted two separate experiments.
first experiment participants presented source sentence target
compression asked rate well compression preserved important
information source sentence. second experiment, asked rate
grammaticality compressed outputs. cases used five point rating
scale high number indicates better performance. randomly selected 21 sentences
test portion corpus. sentences compressed automatically
three models presented paper without constraints. also included
gold standard compressions. materials thus consisted 294 (21 2 7) sourcetarget sentences. Latin square design ensured subjects see two different
compressions sentence. collected ratings 42 unpaid volunteers, self
reported native English speakers. studies conducted Internet using
custom build web interface. Examples experimental items given Table 3.

5. Results
Let us first discuss results compression output evaluated terms F-score.
Tables 4 5 illustrate performance models written spoken corpora,
respectively. also present compression rate7 system. cases
constraint-based models (+Constr) yield better F-scores non-constrained ones.
difference starker semi-supervised model (Sig). constraints bring
improvement 17.2% written corpus 18.3% spoken corpus.
examined whether performance differences among models statistically significant, using
Wilcoxon test. written corpus constraint models significantly outperform
models without constraints. tendency observed spoken corpus except
model McDonald (2006) performs comparably without constraints.
also wanted establish best constraint model. corpora
find language model performs worst, whereas significance model McDonald
perform comparably (i.e., F-score differences statistically significant). get
feeling difficulty task, calculated much annotators agreed
compression output. inter-annotator agreement (F-score) written corpus
65.8% spoken corpus 73.4%. agreement higher spoken texts since
consists many short utterances (e.g., Okay, Thats now, Good night)
7. term refers percentage words retained source sentence compression.

421

fiClarke & Lapata

Source

aim give councils control future growth second
homes.
Gold
aim give councils control growth homes.
LM
aim future.
LM+Constr aim give councils control.
Sig
aim give councils control future growth homes.
Sig+Constr aim give councils control future growth homes.
McD
aim give councils.
McD+Constr aim give councils control growth homes.
Source
Clinton administration recently unveiled new means encourage
brownfields redevelopment form tax incentive proposal.
Gold
Clinton administration unveiled new means encourage brownfields redevelopment tax incentive proposal.
LM
Clinton administration form tax.
LM+Constr Clinton administration unveiled means encourage redevelopment form.
Sig
Clinton administration unveiled encourage brownfields redevelopment form tax proposal.
Sig+Constr Clinton administration unveiled means encourage brownfields
redevelopment form tax proposal.
McD
Clinton unveiled means encourage brownfields redevelopment
tax incentive proposal.
McD+Constr Clinton administration unveiled means encourage brownfields
redevelopment form incentive proposal.
Table 3: Example compressions produced systems (Source: source sentence, Gold:
gold-standard compression, LM: language model compression, LM+Constr: language model compression constraints, Sig: significance model, Sig+Constr:
significance model constraints, McD: McDonalds (2006) compression model,
McD+Constr: McDonalds (2006) compression model constraints).

compressed little all. Note marked difference
automatic human compressions. best performing systems inferior human
output 20 F-score percentage points.
Differences automatic systems human output also observed
respect compression rate. seen language model compresses
aggressively, whereas significance model McDonald tend conservative
closer gold standard. Interestingly, constraints necessarily increase
compression rate. latter increases significance model decreases
language model remains relatively constant McDonald. straightforward
impose compression rate constraint-based models (e.g., forcing model
P
retain b tokens ni=1 = b). However, refrained since wanted
422

fiGlobal Inference Sentence Compression

Models
LM
Sig
McD
LM+Constr
Sig+Constr
McD+Constr
Gold

CompR
46.2
60.6
60.1
41.2
72.0
63.7
70.3

F-score
18.4
23.3
36.0
28.2
40.5
40.8


Table 4: Results written corpus; compression rate (CompR) grammatical relation F-score (F-score); : +Constr model significantly different model
without constraints; : significantly different LM+Constr.
Models
LM
Sig
McD
LM+Constr
Sig+Constr
McD+Constr
Gold

CompR
52.0
60.9
68.6
49.5
78.4
68.5
76.1

F-score
25.4
30.4
47.6
34.8
48.7
50.1


Table 5: Results spoken corpus; compression rate (CompR) grammatical relation F-score (F-score); : +Constr model significantly different without
constraints; : significantly different LM+Constr.

models regulate compression rate sentence individually according
specific information content structure.
next consider results human study assesses detail quality
generated compressions two dimensions, namely grammaticality information
content. F-score conflates two dimensions therefore theory could unduly reward
system produces perfectly grammatical output without information loss. Tables 6
7 show mean ratings8 system (and gold standard) written
spoken corpora, respectively. first performed Analysis Variance (Anova)
examine effect different system compressions. Anova revealed reliable effect
grammaticality importance corpus (the effect significant
subjects items (p < 0.01)).
next examine impact constraints (+Constr tables). cases
observe increase ratings grammaticality importance model
supplemented constraints. Post-hoc Tukey tests reveal grammaticality
importance ratings language model significance model significantly improve
8. statistical tests reported subsequently done using mean ratings.

423

fiClarke & Lapata

Models

Grammar
2.25$

Importance

LM
Sig
McD

3.05

1.82$
2.99$
2.84

LM+Constr
Sig+Constr
McD+Constr
Gold

3.47
3.76
3.50
4.25

2.37$
3.53
3.17
3.98

2.26$

Table 6: Results written text corpus; average grammaticality score (Grammar)
average importance score (Importance) human judgments; : +Constr model
significantly different model without constraints; : significantly different
gold standard; $ : significantly different McD+Constr.

Models

Grammar
2.20$

Importance

LM
Sig
McD

2.29$
3.33

1.56
2.64
3.32

LM+Constr
Sig+Constr
McD+Constr
Gold

3.18
3.80
3.60
4.45

2.49$
3.69
3.31
4.25

Table 7: Results spoken text corpus; average grammaticality score (Grammar)
average importance score (Importance) human judgments; : +Constr model
significantly different model without constraints; : significantly different
gold standard; $ : significantly different McD+Constr.

constraints ( < 0.01). contrast, McDonalds system sees numerical improvement
additional constraints, difference statistically significant.
tendencies observed spoken written corpus.
Upon closer inspection, see constraints influence considerably
grammaticality unsupervised semi-supervised systems. Tukey tests reveal
LM+Constr Sig+Constr grammatical McD+Constr. terms importance,
Sig+Constr McD+Constr significantly better LM+Constr ( < 0.01).
surprising given LM+Constr simple model without mechanism
highlighting important words sentence. Interestingly, Sig+Constr performs well
McD+Constr retaining important words, despite fact requires
minimal supervision. Although constraint-based models overall perform better models without constraints, receive lower ratings (for grammaticality importance)
comparison gold standard. differences significant cases.
424

fiGlobal Inference Sentence Compression

summary, observe constraints boost performance. pronounced compression models either unsupervised use small amounts
parallel data. example, simple model like Sig yields performance comparable
McDonald (2006) constraints taken account. encouraging result
suggesting ILP used create good compression models relatively little
effort (i.e., without extensive feature engineering elaborate knowledge sources). Performance gains also obtained competitive models like McDonalds fully
supervised. gains smaller, presumably initial model contains
rich feature representation consisting syntactic information generally good job
producing grammatical output. Finally, improvements consistent across corpora
evaluation paradigms.

6. Conclusions
paper presented novel method automatic sentence compression. key
aspect approach use integer linear programming inferring globally optimal
compressions presence linguistically motivated constraints. shown
previous formulations sentence compression recast ILPs extended
models local global constraints ensuring compressed output structurally
semantic well-formed. Contrary previous work employed ILP solely
decoding, models integrate learning inference unified framework.
experiments demonstrated advantages approach. Constraint-based
models consistently bring performance gains models without constraints. improvements impressive models require little supervision. case
point significance model discussed above. no-constraints incarnation
model performs poorly considerably worse McDonalds (2006) state-of-the-art
model. addition constraints improves output model performance indistinguishable McDonald. Note significance model requires
small amount training data (50 parallel sentences), whereas McDonald trained hundreds sentences. also presupposes little feature engineering, whereas McDonald utilizes
thousands features. effort associated framing constraints, however
created applied across models corpora. also observed small
performance gains McDonalds system latter supplemented constraints.
Larger improvements possible sophisticated constraints, however intent
devise set general constraints tuned mistakes specific
system particular.
Future improvements many varied. obvious extension concerns constraint set. Currently constraints mostly syntactic consider sentence
isolation. incorporating discourse constraints could highlight words important document-level. Presumably words topical document retained
compression. constraints could manipulate compression rate. example,
could encourage higher compression rate longer sentences. Another interesting
direction includes development better objective functions compression task.
objective functions presented far rely first second-order Markov assumptions.
Alternative objectives could take account structural similarity source
425

fiClarke & Lapata

sentence target compression; whether share content could
operationalized terms entropy.
Beyond task systems presented paper, believe approach holds
promise generation applications using decoding algorithms searching space
possible outcomes. Examples include sentence-level paraphrasing, headline generation,
summarization.

Acknowledgments
grateful annotators Vasilis Karaiskos, Beata Kouchnir, Sarah Luger.
Thanks Jean Carletta, Frank Keller, Steve Renals, Sebastian Riedel helpful
comments suggestions anonymous referees whose feedback helped substantially improve present paper. Lapata acknowledges support EPSRC (grant
GR/T04540/01). preliminary version work published proceedings
ACL 2006.

References
Aho, A. V., & Ullman, J. D. (1969). Syntax directed translations pushdown assembler. Journal Computer System Sciences, 3, 3756.
Bangalore, S., Rambow, O., & Whittaker, S. (2000). Evaluation metrics generation.
Proceedings first International Conference Natural Language Generation,
pp. 18, Mitzpe Ramon, Israel.
Barzilay, R., & Lapata, M. (2006). Aggregation via set partitioning natural language
generation. Proceedings Human Language Technology Conference
North American Chapter Association Computational Linguistics, pp. 359
366, New York, NY, USA.
Bramsen, P., Deshpande, P., Lee, Y. K., & Barzilay, R. (2006). Inducing temporal graphs.
Proceedings 2006 Conference Empirical Methods Natural Language
Processing, pp. 189198, Sydney, Australia.
Briscoe, E. J., & Carroll, J. (2002). Robust accurate statistical annotation general text.
Proceedings Third International Conference Language Resources Evaluation, pp. 14991504, Las Palmas, Gran Canaria.
Charniak, E. (2000). maximum-entropy-inspired parser. Proceedings 1st North
American Annual Meeting Association Computational Linguistics, pp. 132
139, Seattle, WA, USA.
Clarke, J., & Lapata, M. (2006). Models sentence compression: comparison across
domains, training requirements evaluation measures. Proceedings 21st
International Conference Computational Linguistics 44th Annual Meeting
Association Computational Linguistics, pp. 377384, Sydney, Australia.
Clarkson, P., & Rosenfeld, R. (1997). Statistical language modeling using CMU
Cambridge toolkit. Proceedings Eurospeech97, pp. 27072710, Rhodes, Greece.
426

fiGlobal Inference Sentence Compression

Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1992). Intoduction Algorithms.
MIT Press.
Corston-Oliver, S. (2001). Text Compaction Display Small Screens. Proceedings Workshop Automatic Summarization 2nd Meeting North
American Chapter Association Computational Linguistics, pp. 8998, Pittsburgh, PA, USA.
Crammer, K., & Singer, Y. (2003). Ultraconservative online algorithms multiclass problems. Journal Machine Learning Research, 3, 951991.
Dantzig, G. B. (1963). Linear Programming Extensions. Princeton University Press,
Princeton, NJ, USA.
Denis, P., & Baldridge, J. (2007). Joint determination anaphoricity coreference
resolution using integer programming. Human Language Technologies 2007:
Conference North American Chapter Association Computational Linguistics; Proceedings Main Conference, pp. 236243, Rochester, NY.
Dras, M. (1999). Tree Adjoining Grammar Reluctant Paraphrasing Text. Ph.D.
thesis, Macquarie University.
Galley, M., & McKeown, K. (2007). Lexicalized markov grammars sentence compression.
Proceedings North American Chapter Association Computational
Linguistics, pp. 180187, Rochester, NY, USA.
Gomory, R. E. (1960). Solving linear programming problems integers. Bellman,
R., & Hall, M. (Eds.), Combinatorial analysis, Proceedings Symposia Applied
Mathematics, Vol. 10, Providence, RI, USA.
Grefenstette, G. (1998). Producing Intelligent Telegraphic Text Reduction Provide
Audio Scanning Service Blind. Hovy, E., & Radev, D. R. (Eds.), Proceedings
AAAI Symposium Intelligent Text Summarization, pp. 111117, Stanford,
CA, USA.
Hori, C., & Furui, S. (2004). Speech summarization: approach word extraction
method evaluation. IEICE Transactions Information Systems, E87D (1), 1525.
Jing, H. (2000). Sentence reduction automatic text summarization. Proceedings
6th Applied Natural Language Processing Conference, pp. 310315, Seattle,WA,
USA.
Knight, K., & Marcu, D. (2002). Summarization beyond sentence extraction: probabilistic
approach sentence compression. Artificial Intelligence, 139 (1), 91107.
Land, A. H., & Doig, A. G. (1960). automatic method solving discrete programming
problems. Econometrica, 28, 497520.
Lin, C.-Y. (2003). Improving summarization performance sentence compression pilot
study. Proceedings 6th International Workshop Information Retrieval
Asian Languages, pp. 18, Sapporo, Japan.
Lin, D. (2001). LaTaT: Language text analysis tools. Proceedings first Human
Language Technology Conference, pp. 222227, San Francisco, CA, USA.
427

fiClarke & Lapata

Marciniak, T., & Strube, M. (2005). Beyond pipeline: Discrete optimization NLP.
Proceedings Ninth Conference Computational Natural Language Learning,
pp. 136143, Ann Arbor, MI, USA.
McDonald, R. (2006). Discriminative sentence compression soft syntactic constraints.
Proceedings 11th Conference European Chapter Association
Computational Linguistics, Trento, Italy.
McDonald, R., Crammer, K., & Pereira, F. (2005a). Flexible text segmentation structured multilabel classification. Proceedings Human Language Technology Conference Conference Empirical Methods Natural Language Processing, pp.
987994, Vancouver, BC, Canada.
McDonald, R., Crammer, K., & Pereira, F. (2005b). Online large-margin training dependency parsers. 43rd Annual Meeting Association Computational
Linguistics, pp. 9198, Ann Arbor, MI, USA.
Nemhauser, G. L., & Wolsey, L. A. (1988). Integer Combinatorial Optimization. WileyInterscience series discrete mathematicals opitmization. Wiley, New York, NY,
USA.
Nguyen, M. L., Shimazu, A., Horiguchi, S., Ho, T. B., & Fukushi, M. (2004). Probabilistic
sentence reduction using support vector machines. Proceedings 20th international conference Computational Linguistics, pp. 743749, Geneva, Switzerland.
Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1992). Numerical
Recipes C: Art Scientific Computing. Cambridge University Press, New
York, NY, USA.
Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). Semantic role labeling via integer linear programming inference. Proceedings International Conference
Computational Linguistics, pp. 13461352, Geneva, Switzerland.
Riedel, S., & Clarke, J. (2006). Incremental integer linear programming non-projective
dependency parsing. Proceedings 2006 Conference Empirical Methods
Natural Language Processing, pp. 129137, Sydney, Australia.
Riezler, S., King, T. H., Crouch, R., & Zaenen, A. (2003). Statistical sentence condensation
using ambiguity packing stochastic disambiguation methods lexical-functional
grammar. Human Language Technology Conference 3rd Meeting
North American Chapter Association Computational Linguistics, pp. 118
125, Edmonton, Canada.
Roark, B. (2001). Probabilistic top-down parsing language modeling. Computational
Linguistics, 27 (2), 249276.
Roth, D. (1998). Learning resolve natural language ambiguities: unified approach.
Proceedings 15th American Association Artificial Intelligence, pp.
806813, Madison, WI, USA.
Roth, D., & Yih, W. (2004). linear programming formulation global inference
natural language tasks. Proceedings Annual Conference Computational
Natural Language Learning, pp. 18, Boston, MA, USA.
428

fiGlobal Inference Sentence Compression

Roth, D., & Yih, W. (2005). Integer linear programming inference conditional random
fields. Proceedings International Conference Machine Learning, pp. 737
744, Bonn.
Sarawagi, S., & Cohen, W. W. (2004). Semi-markov conditional random fields information extraction. Advances Neural Information Processing Systems, Vancouver,
BC, Canada.
Shieber, S., & Schabes, Y. (1990). Synchronous tree-adjoining grammars. Proceedings 13th International Conference Computational Linguistics, pp. 253258,
Helsinki, Finland.
Turner, J., & Charniak, E. (2005). Supervised unsupervised learning sentence
compression. Proceedings 43rd Annual Meeting Association Computational Linguistics, pp. 290297, Ann Arbor, MI, USA.
Vandeghinste, V., & Pan, Y. (2004). Sentence compression automated subtitling:
hybrid approach. Marie-Francine Moens, S. S. (Ed.), Text Summarization Branches
Out: Proceedings ACL-04 Workshop, pp. 8995, Barcelona, Spain.
Williams, H. P. (1999). Model Building Mathematical Programming (4th edition). Wiley.
Winston, W. L., & Venkataramanan, M. (2003). Introduction Mathematical Programming: Applications Algorithms (4th edition). Duxbury.
Zajic, D., Door, B. J., Lin, J., & Schwartz, R. (2007). Multi-candidate reduction: Sentence
compression tool document summarization tasks. Information Processing
Management Special Issue Summarization, 43 (6), 15491570.

429

fiJournal Artificial Intelligence Research 31 (2008) 259-272

Submitted 08/07; published 02/08

Expressiveness Levesques Normal Form
Yongmei Liu

ymliu@mail.sysu.edu.cn

Department Computer Science
Sun Yat-sen University
Guangzhou 510275, China

Gerhard Lakemeyer

gerhard@cs.rwth-aachen.de

Department Computer Science
RWTH Aachen
52056 Aachen, Germany

Abstract
Levesque proposed generalization database called proper knowledge base (KB),
equivalent possibly infinite consistent set ground literals. contrast
databases, proper KBs make closed-world assumption hence entailment
problem becomes undecidable. Levesque proposed limited efficient inference
method V proper KBs, sound and, query certain normal
form, also logically complete. conjectured every first-order query
equivalent one normal form. note, show conjecture false. fact,
show class formulas V complete must strictly less expressive
full first-order logic. Moreover, propositional case unlikely
formula always polynomial-size normal form.

1. Introduction
argued Levesque (1998), one deductive technique efficient enough
feasible knowledge bases (KBs) size seemingly required common-sense reasoning: deduction underlying classical database query evaluation. yet, databases
restricted serve representational scheme common-sense reasoning, since require, among things, complete knowledge domain. Levesque
proposed generalization database called proper knowledge base, equivalent
possibly infinite consistent set ground literals. illustrate meant
proper KB consider following example:
Ann likes Bob, Dan likes Fred.
Likes(ann, bob)
Likes(dan, fred)
Ann like Dan.
Likes(ann, dan)
Carol likes everyone.
x. Likes(carol, x)
Eve like anyone Ann herself.
x. x 6= ann x 6= eve Likes(eve, x)
c
2008
AI Access Foundation. rights reserved.

fiLiu & Lakemeyer

contrast databases, proper KBs make closed-world assumption.
example, Likes(eve, fred) follows KB (if assume unique names),
neither Likes(ann, eve) Likes(ann, eve) does. Sadly, even restricted form
incompleteness renders entailment problem undecidable, entailment empty
KB reduces validity classical logic.
Nevertheless, given KBs like many queries seem easy answer. example,
consider formula
Likes(eve, carol) Likes(carol, eve),
follows KB simply Likes(carol, eve) does. work Levesque
devised limited efficient inference mechanism V gets examples like right
expense incomplete others, is, V sometimes answers dont know
even though query logically entailed.
give flavor V works, consider sentence conjunctive normal form
(CNF), = c1 c2 . . . cn , ci disjunction ground literals.
order see whether follows KB, V simply checks whether ci contains
literal instance one sentences KB. evaluation-based
scheme clearly sound also easily seen incomplete. example, V would return
dont know given query Likes(ann, eve) Likes(ann, eve), neither literal
contained KB.
paper, Levesque introduced certain normal form (NF) sentences proved
V logically complete queries NF. propositional case, examples
sentences NF CNF contain tautological clauses
otherwise closed resolution. Since every propositional sentence equivalent
sentence form, follows immediately every propositional sentence
converted equivalent one NF.
Levesque conjectured every sentence first-order logic also
equivalent one NF . note, show conjecture Levesque false.
fact, show class formulas V complete must strictly less
expressive full first-order logic. Moreover, propositional case unlikely
formula always polynomial-size normal form.
Note Levesques conjecture weaker statement exists algorithm converts every first-order sentence equivalent one NF. latter
statement easily refuted since whether first-order sentence entailed proper
KB undecidable, whether NF sentence entailed proper KB decidable (V
decision procedure).
next section, briefly review Levesques evaluation-based inference method
proper KBs. Section 3 contains main result, is, show every sentence
equivalent normal form. Section 4 considers size NF formulas propositional
case.

2. Levesques Evaluation-Based Reasoning Procedure V
underlying language L standard first-order dialect equality. countably infinitely many first-order variables predicate symbols every arity (including
binary equality predicate). addition countably infinite set C = {d1 , d2 , . . .}
260

fiOn Expressiveness Levesques Normal Form

constants (but function symbols). logical connectives , , .
atomic formulas L predicate symbols variables constants arguments.
set formulas L least set contains atomic formulas,
set x variable, , x. set.
sometimes refer propositional subset language, consists
ground atoms L equality closed negation conjunction.
Notation: usual, equality written = using infix notation.
freely use connectives , , , , understood usual abbreviations.
Formulas without free variables called sentences. Variables written x, y, z
sub- superscripts. ewffs mean quantifier-free formulas whose predicate
equality. example, (x = z 6= d1 ) ewff. use denote universal
closure . example, (x = x 6= z P (x, y, z)) stands xyz.x = x 6= z
P (x, y, z). write xd denote free occurrences x replaced constant d.
clause disjunction literals, identify set literals contained it.
let meta-variable c range clauses write c denote set {l | l c},
V
l complement literal l. finite set formulas, write
denote conjunction elements (and true, empty). use H()
denote set constants appearing set formulas H + () denote
set constants appearing plus extra one occurring . set returned
H + () made unique assuming constants ordered letting new
constant least constant appearing . singleton {} simply
write H() H + ().
Levesque considered special class Tarskian interpretations called standard interpretations, equality interpreted identity set constants isomorphic
domain discourse. shown following definition theorem, restriction
standard interpretations captured precisely set axioms, provided limit
logically implied finite sets sentences.
Definition 1 Let set E consist following axioms:
1. x.x = x;
2. (xi = (P (x1 , . . . , xi , . . . , xn ) P (x1 , . . . , y, . . . , xn )));
3. {(di 6= dj ) | 6= j}.
(1) (2) version axioms equality. P ranges predicate symbols
including equality. (3) asserts unique names assumption constants.
Theorem 2 (Levesque) Let finite set sentences. E |= iff every
standard interpretation also model .
write |=E E |= .
database viewed maximally consistent set ground literals. Levesque
proposed generalization database called proper KB, equivalent (possibly
infinite) consistent set ground literals.
following use e range ewffs range atoms (excluding
equality) whose arguments distinct variables.
261

fiLiu & Lakemeyer

Definition 3 (Levesque) set sentences proper E consistent, finite
every sentence form (e ) (e ).
propositional case, definition simplifies finite consistent set
ground literals excluding equality. (As equality plays role case, E ignored.)
general, proper KB represents set ground literals lits() = {l | (e
l) E |= e}, consistent, since E consistent. special case,
database represented proper KB: relation R = {d~1 , . . . , d~m } represented
(e R(~x)) (e R(~x)), e ~x = d~1 . . .~x = d~m . importantly,
proper KB represent incomplete set literals, specifying positive instances
negative instances leaving status rest open.
example introduction formulated proper KB. rephrasing
needed predicates right hand sides implications may mention
distinct variables.
Ann likes Bob, Dan likes Fred.
xy. x = ann = bob x = dan = fred Likes(x, y)
Ann like Dan.
xy. x = ann = dan Likes(x, y)
Carol likes everyone.
xy. x = carol Likes(x, y)
Eve like anyone Ann herself.
xy. x = eve 6= ann 6= eve Likes(x, y)
Again, note information cannot expressed traditional database where,
example, cannot leave open whether Ann likes Eve.
Levesques evaluation-based inference procedure V proper KBs defined follows.
use range substitutions variables constants, write mean
result applying formula . restrict attention Boolean queries,
is, sentences L. Given proper KB sentence L, V returns one three
values 0 (known false), 1 (known true), 12 (unknown). precisely,
1. V [, ] =



1

0




1
2

(e ) V [, e] = 1
(e ) V [, e] = 1
otherwise

2. V [, = ] = 1 identical , 0 otherwise;
3. V [, ] = 1 V [, ];
4. V [, ] = min{V [, ], V [, ]};
5. V [, x] = mindH + ({}) V [, xd ].
262

fiOn Expressiveness Levesques Normal Form

hard show procedure logically sound, is, whenever returns
1 0, either query negation follows knowledge base. V also obviously
decidable quantification handled finite number variable substitutions. shown
Liu Levesque (2003), also efficient sense database retrieval.
see V incomplete, let set sentences example KB
= (Likes(ann, eve) Likes(ann, eve)). obviously follows , yet V [, ] = 12
V returns 12 Likes(ann, eve) Likes(ann, eve). problem is, roughly,
V requires one disjuncts derivable order whole disjunction
derivable.
slightly complex example, let = (p q)(q r), p = Likes(ann, bob),
q = Likes(ann, eve), r = Likes(ann, dan). again, V [, ] = 12 , correct
answer 0 since |= |= p |= r. However, notice
clauses (p q) (q r) entail clause (p r). conjoin (p r) ,
logical equivalence would preserved V would return correct answer 0 since
V [, p] = 1 V [, r] = 0.
Despite limitation, Levesque showed queries certain normal form called
NF, V actually complete. first state result, followed definition NF .
Theorem 4 (Levesque) Let proper.
every NF, V [, ] = 1 iff |=E ; V [, ] = 0 iff |=E .
definition NF based logical separability:
Definition 5 (Levesque) set sentences logically separable iff every consistent
set ground literals L, L standard interpretation, L {} inconsistent
.
intuition behind logical separability consistent set literals entails disjunction, one disjuncts must entailed. see this, consider propositional
case = {p, q} L consistent set propositional literals. Suppose L
standard interpretation, propositional case L inconsistent
or, equivalently, L |= (p q). L must contain either p q. Hence either L {p}
L {q} inconsistent, is, either L |= p L |= q. cases, proves
logically separable.
set {p, p}, hand, logically separable. {p, p}
already inconsistent let L empty set. case, L {p}
L {p} consistent.
Definition 6 (Levesque) NF least set
1. ground atom ewff, NF;
2. NF, NF ;
3. NF, logically separable, finite,

V

NF ;

4. NF, logically separable, , = {xd | C}, x NF.
263

fiLiu & Lakemeyer

(1) (2) say, roughly, NF contains ground atoms closed negation.
(3) (4) say closure conjunction universal generalization restricted
formulas logically separable.
idea behind definition formula NF , sense, contain
logical puzzles. case, example, non-tautologous clause. see
why, consider consistent set literals. Similar earlier example {p, q},
shown set logically separable. Hence conjunction literals
negation, clause, NF. hand, tautology like (p p)
NF. shown above, set {p, p} logically separable. Hence neither p p
negation, is, (p p) NF.
Levesque (1998) showed propositional case, CNF formula NF
clauses non-tautologous closed resolution, is, resolvent two
clauses also belongs clauses. Consider = (p q) (q r)
earlier example. NF since clauses closed resolution. However,
(p q) (q r) (p r) NF.
first-order case, two examples formulas NF x(P (x) Q(x))
x(P (x) Q(x)). see let constant. {p, q} logically separable,
{P (d), Q(d)}. Thus (P (d) Q(d)) NF . consider = {(P (d) Q(d)) | C}.
Let L consistent set literals. L standard model, L must contain
P (d) Q(d) d, hence L {(P (d) Q(d))} inconsistent d. Thus
logically separable, x(P (x) Q(x)) NF. Similarly, x(P (x) Q(x))
NF. Therefore, x(P (x) Q(x)), is, x(P (x) Q(x)), NF.
Note use expression normal form somewhat non-standard. Unlike
traditional normal forms like CNF, even clear whether membership NF
decidable. Levesque pointed number classes first-order formulas
membership NF decided syntactically, unlikely general. example,
showed x.R(a, x) R(x, b) NF.
turns Levesques original definition logical separability (Def. 5) little
strong rules certain sentences NF definitely would like
in. problem definition mixes use standard regular
Tarskian interpretations. peculiar effect formulas like x(x = P (x)),
make proper KBs, NF.1 see why, definition
NF, x(x = P (x)), is, x(x = P (x)), NF, must
(d = P (d)) NF every constant d, requires {d = a, P (d)} logically
separable. However, let b constant distinct a, {b = a, P (b)} logically
separable. reason {b = a, P (b)} standard model built-in
unique names assumption, {b = a} {P (b)} consistent classical logic.
turns anomaly easy fix using following, slightly weaker
definition logical separability, talk standard interpretations.
Definition 7 set sentences logically separable iff every consistent set ground
literals L, L standard interpretation, L{} standard interpretation
.
1. anomaly first observed Thomas Eiter (personal communication).

264

fiOn Expressiveness Levesques Normal Form

new definition, {b = a, P (b)} logically separable, since {b = a}
standard model. result, show x(x = P (x)) NF. Note
NF using new definition logical separability strictly bigger original NF,
use new version on. main result, says
sentences equivalent sentence NF, trivially extends Levesques original
definition.
turn that, let us briefly recall NF good point related
work. user poses query NF proper KB, need V obtain
correct (sound complete) answer respect logical entailment. Moreover,
mentioned earlier, V proven efficient database retrieval (Liu & Levesque,
2003), standard database technology brought bear implementation.
regard, also interesting connection recent work evaluating certain queries description logics (Baader, Calvanese, McGuiness, Nardi, & Patel-Schneider,
2003). description-logic KB consists two parts, TBOX terminological definitions
like mother female person least one child ABOX, set
atomic formulas. ABOX special case proper KB. importantly,
ABOX make closed world assumption, proper KBs. Calvanese, de
Giacomo, Lembo, Lenzerini, Rosati (2006) showed conjunctive queries,
consist conjunctions atoms existentially quantified variables, query answering
reduced database retrieval well. interesting note queries consider NF. consider small fragment NF, go beyond proper
KBs also perform terminological reasoning (using TBOX). remark
extensions proper KBs explicitly allowing disjunctions (Lakemeyer
& Levesque, 2002; Liu, Lakemeyer, & Levesque, 2004), reasoning goes beyond
database retrieval.

3. First-Order NF Expressive
Levesque showed propositional case, every formula transformed
equivalent one NF. transformation this. Convert formula CNF, run
resolution repeatedly set clauses, deleting tautologous subsumed ones
new clauses generated. resulting set clauses set prime implicates
. conjunction clauses NF equivalent . However,
transformation cannot extended first-order case. see why, consider =
xyz[R(x, y) R(y, z) R(x, z)], says R transitive. run first-order
version Levesques transformation , would end infinite set clauses,
format R(x1 , x2 ) . . . R(xn , xn+1 ) R(x1 , xn+1 ), n 2.
section, prove first-order case, every formula equivalent
one NF . purpose, need first-order version prime implicates.
propositional case, prime implicates defined follows:
Definition 8 Let theory. implicate non-tautologous clause c
|= c. prime implicate implicate c proper subset c c
|= c .
265

fiLiu & Lakemeyer

Since consider standard interpretations, easily generalize prime implicates first-order case:
Definition 9 Let L. implicate non-tautologous ground clause c
|=E c. prime implicate implicate c proper subset c c
|=E c . use PI() denote set prime implicates .
|=E , is, E |= , E {} E {} entail
sentences and, particular, PI() = PI().
following basic property prime implicates:
Proposition 10 Let c non-tautologous ground clause. |=E c iff exists
c PI() c c.
note, length formula, mean number predicate symbols,
variables, constants logical connectives contained formula. length
clause, mean length corresponding disjunctive formula. key property
note defined follows:
Definition 11 say PI() bounded exists number n length
every member PI() n. PI() bounded, use B() denote
maximum length member PI().
show every NF, PI() bounded.
equivalent NF, PI() bounded too. However, exist formulas PI()
bounded. Thus every formula transformed equivalent one NF.
proof prime implicates formulas NF bounded proceed
induction. following lemma useful establish induction conjunctions
negations.
Lemma 12 Let = {1 , . . . , n }.
1.

V

2. PI(

NF, PI( )
V

V

) {



ci

PI(i ).



| ci PI(i ), = 1, . . . , n}.

Proof:
1. Let c PI( ). |=E c. Theorem 2, c standard interpretation.
V
Since NF, logically separable. Since c non-tautologous, c consistent.
Thus {i } c standard interpretation i, |=E c. Let c c
V
V
|=E c . |=E c . Since c PI( ), c = c. Thus c PI(i ).
V

V

2. Let c PI( ). |=E c. Thus |=E c i. Proposition 10,
i, exists ci PI(i ) (and hence |=E ci ) ci c.
V


V

|=E ci ci c. Since c PI( ), c = ci .
2
V

V

266

fiOn Expressiveness Levesques Normal Form

Note that, prime implicates bounded, follows easily lemma
V
V
prime implicates bounded well. obtain similar result
quantified formulas complicated. obvious generalization Lemma 12,
V
replacing {xd1 , xd2 , . . .} x work, would lead
infinite union sets first part infinite union clauses second part.
get around observing similarity PI(xb ) PI(xd ), b
constants appearing , shown Proposition 14 below.
begin property useful proof. Let mapping C
C. use denote every constant replaced . use denote
{ | }.
Proposition 13 (Levesque) Let bijection C C. |=E , |=E .
Proposition 14 Let formula single free variable x. Let b, constants C
appearing . Let bijection swaps b leaves constants
unchanged. PI(xd ) = {c | c PI(xb )}.
Proof: Let c PI(xb ). xb |=E c. Proposition 13, (xb ) |=E c , is, xd |=E c .
Let c c xd |=E c . xb |=E c . Since c PI(xb ) c c, c = c,
hence c = c . Thus c PI(xd ). Similarly, c PI(xd ), c PI(xb ). Therefore,
PI(xd ) = {c | c PI(xb )}.
2
Basically, proposition says prime implicates xb xd
modulo constant renaming.
Lemma 15 Let formula single free variable x.
1. x NF , constants C, PI(xd ) bounded, PI(x) also
bounded.
2. constants C, PI(xd ) bounded, PI(x) also bounded.
Proof:
1. Since C, PI(xd ) bounded, let n = max{B(xd ) | H + ()}.
Proposition 14, C, PI(xd ) relabeling PI(xb ) b H + ().
Thus C, PI(xd ) bounded n. show PI(x) also bounded
n, showing every element PI(x) also element PI(xd ),
C.
suppose c PI(x). x |=E c. Thus {x} c standard
interpretation. {xd | C} c standard interpretation. Since x NF,
{xd | C} logically separable. Thus exists C {xd } c
standard interpretation. xd |=E c. Let c c xd |=E c . x |=E c .
Since c PI(x), c = c. Thus c PI(xd ).
267

fiLiu & Lakemeyer

2. Part 1, let n = max{B(xd ) | H + ()}. C, PI(xd )
bounded n. show PI(x) bounded (m + n + 1) n,
length . done showing c PI(x), exists
set (m + n + 1) constants D, exists

cd PI(xd ) c = dD cd .

suppose c PI(x). x |=E c, i.e., x |=E c. Let
arbitrary constant C. xd |=E c xd |=E x. Proposition 10,
exists cd PI(xd ) cd c. let b constant appears
neither c cb PI(xb ) cb c. b appear cb ,
length cb n. Let = H() H(cb ) {b}.

(m + n + 1) elements. show c = dD cd . so, let
arbitrary constant D. Let bijection swaps b
leaves constants unchanged. Since cb PI(xb ) xb |=E cb ,
and, Proposition 13, (xb ) |=E (cb ) . Since neither b appears cb ,
(xb ) = xa (cb ) = cb . Hence xa |=E cb . D,

xd |=E cd ; 6 D, xa |=E cb . Thus x |=E dD cd , subset

2
c. Since c PI(x), c = dD cd .

pieces hand prove main theorem:
Theorem 16 Let NF . PI() bounded.
Proof: technical reasons, easier prove slightly general statement, namely
PI() PI() bounded provided NF. proof induction
.
1. ground atom ewff. ground atom, PI() = {} PI() =
{}, hence bounded. ground ewff true,
entail ground clause, hence PI() empty set; ground ewff
false, entails empty clause, hence PI() set consisting
empty clause. Therefore, ground ewff, PI() PI() bounded.
2. . induction, PI() PI() bounded. Since PI() = PI(),
PI() PI() bounded.
3. . induction, , PI() PI() bounded. Lemma 12,
V
V
PI( ) bounded max{B() | }, PI( ) sum B()
.
V

4. x. induction, constant d, PI(xd ) PI(xd ) bounded.
Lemma 15, PI(x) PI(x) bounded.
2
easy corollary, have:
Corollary 17 every sentence equivalent one NF.
268

fiOn Expressiveness Levesques Normal Form

Proof: Let = xyz[R(x, y) R(y, z) R(x, z)], says R transitive.
n, following PI():
R(d1 , d2 ) . . . R(dn , dn+1 ) R(d1 , dn+1 ).
Thus PI() bounded. Suppose exists NF
equivalent. PI() = PI( ). Theorem 16, PI( ) bounded, contradiction.
2
Moreover, easy generalize inexpressiveness result:
Theorem 18 exist class F sentences properties:
1. every formula equivalent one F;
2. V logically complete F (i.e., every proper KB every F, |=E
V [, ] = 1, |=E V [, ] = 0);
3. F F;
constants d.

V

F, F; x F, xd F

Note theorem require logical separability, V complete
F. call set formulas satisfies third requirement downward saturated,
which, besides desirable property normal form, needed technical reasons.
Proof: Suppose, contrary, exists class F sentences satisfies
three properties stated above. case NF , show PI() bounded
every F use sentence proof Corollary 17 obtain
contradiction.
boundedness proof almost identical argument NF ,
repeat here. Instead note necessary changes. fact,
changes needed proofs Item 1 Lemma 12 15, appeal logical
separability show |=E c respectively xd |=E c d. show
conclusions drawn using assumption V complete F.
First, note following: Let F, let c non-tautologous ground clause.
c essentially proper KB. Thus |=E c, c |=E , hence V [c, ] = 0,
completeness V F.
Change Lemma 12, Item 1:
V
V
V
V
Let c PI( ). |=E c. Since F, V [c, ] = 0. definition
V , V [c, ] = 0 . soundness V , c |=E . Thus |=E c.
Change Lemma 15, Item 1:
Let c PI(x). x |=E c. Since x F, V [c, x] = 0. definition
V , V [c, xd ] = 0 constant d. soundness V , c |=E xd . Thus xd |=E c.
small changes proofs two lemmas go F instead NF.
Finally, proof Theorem 16 carries without change, since induction works
downward-saturated set.
2

269

fiLiu & Lakemeyer

4. Propositional NF Succinct
propositional case, Levesques transformation NF, is, taking conjunction
prime implicates formula, may cause exponential blowup size
formula. number prime implicates formula n propositions
exponential n worst case (Chandra & Markowsky, 1978). section, show
propositional case, certain complexity assumption, every formula
polynomial-size equivalent one NF. done relating NF existing
result knowledge compilation.
Knowledge compilation (Selman & Kautz, 1996; Darwiche & Marquis, 2002)
proposed one main techniques deal computational intractability
general propositional reasoning. approach, tractable language, usually
means language whether clause entailed formula language
decided polynomial time, identified target compilation language.
propositional theory first compiled off-line target language, result
used on-line answer multiple queries. main motivation shift
computational cost off-line phase, amortized on-line queries.
shown following, NF serve knowledge compilation language. reason
propositional case, answering arbitrary query proper KB
equivalent answering clausal query arbitrary KB. mentioned Section 2,
propositional case, proper KB simply consistent set literals.
Proposition 19 Clausal entailment NF decided polynomial time.
Proof: Let NF , let c non-tautologous clause. c proper KB. Thus
|= c iff c |= iff V [c, ] = 0, soundness completeness V NF. Clearly,
propositional case, V runs polynomial time.
2
following well-known result knowledge compilation:
Theorem 20 (Selman & Kautz, 1996) Unless NP P/poly, exist class
F formulas every propositional formula polynomial-size equivalent one
F, clausal entailment F decided polynomial time.
complexity class P/poly, also known non-uniform P , originated circuit complexity (Boppana & Sipser, 1990). Roughly, problem P/poly every integer n
exists circuit size polynomial n solves instances size n. Without going
details, NP P/poly implies collapse polynomial hierarchy
second level, considered unlikely.
easy corollary proposition theorem, have:
Corollary 21 Unless NP P/poly, every propositional formula polynomial-size
equivalent one NF.
words, unlikely obtain compact NF representations
arbitrary propositional formulas.
270

fiOn Expressiveness Levesques Normal Form

5. Conclusion
Levesque remarked paper envision use NF sense query
optimization taking arbitrary query converting NF handing
V . main argument high computational cost, usually cannot afforded
on-line. Besides, except special cases even clear convert formula
NF, one exists. Instead suggested NF could guideline users formulate
good queries evaluated efficiently.
contribution technical note point limits even
use NF . propositional case result says likely queries
cannot NF compactly representable. sense,
bad news, since practice queries tend small compared knowledge base.
first-order case result serious showed queries
normal form all. words, matter ingenious user might be,
always queries easy-to-answer form, least insist
form independent knowledge base NF. Indeed, may still possible
find another notion normal form depends way knowledge base,
example, constants contains. future work.

Acknowledgments
thank Hector Levesque many helpful discussions topic paper
reading earlier version paper. also thank anonymous reviewers
detailed comments improving presentation paper.

References
Baader, F., Calvanese, D., McGuiness, D., Nardi, D., & Patel-Schneider, P. (2003).
Description Logic Handbook: Theory, Implementation Applications. Cambridge
University Press.
Boppana, R. B., & Sipser, M. (1990). complexity finite functions. van Leeuwen,
J. (Ed.), Handbook Theoretical Computer Science, Vol. A, pp. 757804. Elsevier.
Calvanese, D., de Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2006). Data
complexity query answering description logics. Proc. Tenth International
Conference Principles Knowledge Representation Reasoning (KR-06), pp.
260270.
Chandra, A. K., & Markowsky, G. (1978). number prime implicants. Discrete
Mathematics, 24, 711.
Darwiche, A., & Marquis, P. (2002). knowledge compilation map. Journal Artificial
Intelligence Research, 17, 229264.
Lakemeyer, G., & Levesque, H. J. (2002). Evaluation-based reasoning disjunctive information first-order knowledge bases. Proc. Eighth International Conference
Principles Knowledge Representation Reasoning (KR-02), pp. 7381.
271

fiLiu & Lakemeyer

Levesque, H. J. (1998). completeness result reasoning incomplete first-order
knowledge bases. Proc. Sixth International Conference Principles
Knowledge Representation Reasoning (KR-98), pp. 1423.
Liu, Y., Lakemeyer, G., & Levesque, H. J. (2004). logic limited belief reasoning
disjunctive information. Proc. Ninth International Conference Principles
Knowledge Representation Reasoning (KR-04), pp. 587597.
Liu, Y., & Levesque, H. J. (2003). tractability result reasoning incomplete firstorder knowledge bases. Proc. Eighteenth International Joint Conference
Artificial Intelligence (IJCAI-03), pp. 8388.
Selman, B., & Kautz, H. (1996). Knowledge compilation theory approximation. Journal
ACM, 43 (2), 193224.

272

fiJournal Artificial Intelligence Research 31 (2008) 113-155

Submitted 08/07; published 01/08

CTL Model Update System Modifications
Yan Zhang

yan@scm.uws.edu.au

Intelligent Systems Laboratory
School Computing Mathematics
University Western Sydney, Australia

Yulin Ding

yulin@cs.adelaide.edu.au

Department Computer Science
University Adelaide, Australia

Abstract
Model checking promising technology, applied verification
many hardware software systems. paper, introduce concept model update towards development automatic system modification tool extends model
checking functions. define primitive update operations models Computation
Tree Logic (CTL) formalize principle minimal change CTL model update.
primitive update operations, together underlying minimal change principle, serve foundation CTL model update. Essential semantic computational
characterizations provided CTL model update approach. describe
formal algorithm implements approach. also illustrate two case studies CTL
model updates well-known microwave oven example Andrew File System 1,
propose method optimize update results complex system
modifications.

1. Introduction
Model checking one effective technologies automatic system verifications.
model checking approach, system behaviours modeled Kripke structure,
specification properties require system meet expressed formulas
propositional temporal logic, e.g., CTL. model checker, e.g., SMV, takes
Kripke model formula input, verifies whether formula satisfied
Kripke model. formula satisfied Kripke model, system report
errors, possibly provides useful information (e.g., counterexamples).
past decade, model checking technology considerably developed,
many effective model checking tools demonstrated provision thorough automatic error diagnosis complex designs e.g., (Amla, Du, Kuehlmann, Kurshan,
& McMillan, 2005; Berard, Bidoit, Finkel, Laroussinie, Petit, Petrucci, & Schnoebelen,
2001; Boyer & Sighireanu, 2003; Chauhan, Clarke, Kukula, Sapra, Veith, & Wang, 2002;
Wing & Vaziri-Farahani, 1995). current state-of-the-art model checkers,
SMV (Clarke, Grumberg, & Peled, 1999), NuSMV (Cimatti, Clarke, Giunchiglia, & Roveri,
1999) Cadence SMV (McMillan & Amla, 2002), employ SMV specification language
Computational Tree Logic (CTL) Linear Temporal Logic (LTL) variants (Clarke
et al., 1999; Huth & Ryan, 2004). model checkers, SPIN (Holzmann, 2003),
use Promela specification language on-the-fly LTL model checking. Additionally,
c
2008
AI Access Foundation. rights reserved.

fiZhang & Ding

MCK (Gammie & van der Meyden, 2004) model checker developed integrating
knowledge operator CTL model checking verify knowledge-related properties
security protocols.
Although model checking approaches used verification problems large
complex systems, one major limitation approaches verify
correctness system specification. words, errors identified system
specification model checking, task correcting system completely left
system designers. is, model checking generally used verify correctness
system, modify it. Although idea repair indeed proposed modelbased diagnosis, repairing system possible specific cases (Dennis, Monroy, &
Nogueira, 2006; Stumptner & Wotawa, 1996).
1.1 Motivation
Since model checking handle complex system verification problems may
implemented via fast algorithms, quite natural consider whether develop
associated algorithms handle system modification well. idea
integrating model checking automatic modification investigated recent
years. Buccafurri, Eiter, Gottlob, Leone (1999) proposed approach whereby
AI techniques combined model checking enhanced algorithm
identify errors concurrent system, also provide possible modifications
system.
approach, system described Kripke structure , modification
set state transitions may added removed . CTL
formula satisfied i.e., system contains errors respect property ,
repaired adding new state transitions removing existing ones specified
. result, new Kripke structure 0 satisfy formula . approach
Buccafurri et al. (1999) integrates model checking abductive theory revision
perform system repairs. also demonstrate approach applied
repair concurrent programs.
observed type system repair quite restricted, relation
elements (i.e., state transitions) Kripke model changed 1 . implies errors
fixed changing system behaviors. fact, show paper,
allowing change states relation elements Kripke structure significantly
enhances system repair process situations. Also, since providing admissible
modifications (i.e., set ) pre-condition repair, approach Buccafurri
et al. lacks flexibility. Indeed, stated authors themselves, approach may
general enough system modifications.
hand, knowledge-base update subject extensive study
AI community since late 1980s. Winsletts Possible Model Approach (PMA)
viewed pioneering work towards model-based minimal change approach knowledgebase update (Winslett, 1988). Many researchers since proposed different approaches
knowledge system update (e.g., see references Eiter & Gottlob, 1992; Herzig &
1. NB: state changes occur specified system repairs (see Definitions 3.2 3.3 Buccafurri
et al., 1999).

114

fiCTL Model Update System Modifications

Rifi, 1999). works, Harris Ryan (2002, 2003) considered using update
approach system modification, designed update operations tackle feature
integration, performing theory change belief revision. However, study focused
mainly theoretical properties system update, practical implementation
approach system modification remains unclear.
Baral Zhang (2005) recently developed formal approach knowledge update
based single-agent S5 Kripke structures observing system modification closely
related knowledge update. knowledge dynamics perspective, view
finite transition system, represents real time complex system, model
knowledge set (i.e., Kripke model). Thus problem system modification reduced
problem updating model new updated model satisfies knowledge
formula.
observation motivated initial development general approach updating
Kripke models, integrated model checking technology, towards
general automatic system modification. Ding Zhangs work (2005) may viewed
first attempt apply idea LTL model update. LTL model update modifies
existing LTL model abstracted system automatically correct errors occurring
within model.
Based investigation described above, intend integrate knowledge update
CTL model checking develop practical model updater, represents general
method automatic system repairs.
1.2 Contributions Paper
overall aim work design model updater improves model checking
function adding error repair (see schematic Figure 1). outcome updater
corrected Kripke model. model updaters function automatically correct
errors reported (possibly counterexamples) model checking compiler. Eventually,
model updater intended universal compiler used certain common
situations model error detection correction.

CTL
Kripke Model

System
Design

Model checking
& Updating

Corrected
Kripke Model

Figure 1: CTL model update.

main contributions paper described follows:
115

fiZhang & Ding

1. propose formal framework CTL model update. Firstly, define primitive CTL model update operations and, based operations, specify minimal
change principle CTL model update. study relationship
proposed CTL model update traditional propositional belief update. Interestingly, prove CTL model update obeys Katsuno Mendelzon update
postulates (U1) - (U8). provide important characterizations special
CTL model update formulas EX, AG EG. characterizations
play important role optimization update procedure. Finally, study
computational properties CTL model update show that, general, model
checking problem CTL model update co-NP-complete. also classify useful
subclass CTL model update problems performed polynomial time.
2. develop formal algorithm CTL model update. principle, algorithm
perform update given CTL Kripke model arbitrary satisfiable
CTL formula generate model satisfies input formula minimal
change respect original model. model viewed possible
correction original system specification. Based algorithm, implement
system prototype CTL model updater C code Linux.
3. demonstrate important applications CTL model update approach two
case studies well-known microwave oven example (Clarke et al., 1999)
Andrew File System 1 (Wing & Vaziri-Farahani, 1995). case
studies, propose new update principle minimal change maximal
reachable states, significantly improve update results complex system
modification scenarios.
summary, work presented paper initial step towards formal study
automatic system modification. approach may integrated existing model
checkers may develop unified methodology system model checking
model correction. sense, work enhance current model checking
technology. results presented paper published ECAI 2006 (Ding &
Zhang, 2006).
rest paper organized follows. overview CTL syntax semantics provided Section 2.1. Primitive update operations CTL models defined
Section 3, minimal change principle CTL model update developed.
Section 4 consists study relationship CTL model update Katsuno
Mendelzons update postulates (U1) - (U8), various characterizations special CTL model updates. Section 5, general computational complexity result CTL
model update proved, useful tractable subclass CTL model update problems
identified. formal algorithm proposed CTL model update approach described
Section 6. Section 7, two update case studies illustrated demonstrate applications CTL model update approach. Section 8 proposes improved CTL model
update approach significantly optimize update results complex system
modification scenarios. Finally, paper concludes future work discussions
Section 9.
116

fiCTL Model Update System Modifications

2. Preliminaries
section, briefly review syntax semantics Computation Tree Logic
basic concepts belief update, foundation CTL model update.
2.1 CTL Syntax Semantics
begin with, briefly review CTL syntax semantics (refer Clarke et al., 1999
Huth & Ryan, 2004 details).
Definition 1 Let AP set atomic propositions. Kripke model AP
triple = (S, R, L) where:
1. finite set states;
2. R binary relation representing state transitions;
3. L : 2AP labeling function assigns state set atomic
propositions.
example finite Kripke model represented graph Figure 2,
node represents state S, attached set propositional atoms
assigned labeling function, edge represents state transition - relation
element R describing system transition one state another.
S0
p, q

S2
q, r

r
S1

Figure 2: Transition state graph.
Computation Tree Logic (CTL) temporal logic allowing us refer future.
also branching-time logic, meaning model time tree-like structure
future determined consists different paths, one might
actual path eventually realized (Huth & Ryan, 2004).
Definition 2 CTL following syntax given Backus-Naur form:
::= > || p | () | (1 2 ) | (1 2 ) | | AX | EX
| AG | EG | AF | EF | A[1 U2 ] | E[1 U2 ]
p propositional atom.
117

fiZhang & Ding

CTL formula evaluated Kripke model. path Kripke model state
a(n) (infinite) sequence states. Note given path, state may occur
infinite number times path (i.e., path contains loop). simplify
following discussions, may identify states path different position subscripts,
although states occurring different positions path may same. way,
say one state precedes another path without much confusion.
present useful notions formal way. Let = (S, R, L) Kripke model S.
path starting denoted = [s 0 , s1 , , si1 , si , si+1 , ], s0 =
(si , si+1 ) R holds 0. write si si state occurring path .
path = [s0 , s1 , , si , , sj , ] < j, also denote si < sj . Furthermore
given path , use notion si denote state state < si .
simplicity, may use succ(s) denote state 0 relation element (s, s0 ) R.
Definition 3 Let = (S, R, L) Kripke model CTL. Given S, define
whether CTL formula holds state s. denote (M, s) |= .
satisfaction relation |= defined structural induction CTL formulas:
1. (M, s) |= > (M, s) 6|= S.
2. (M, s) |= p iff p L(s).
3. (M, s) |= iff (M, s) 6|= .
4. (M, s) |= 1 2 iff (M, s) |= 1 (M, s) |= 2 .
5. (M, s) |= 1 2 iff (M, s) |= 1 (M, s) |= 2 .
6. (M, s) |= 1 2 iff (M, s) |= 1 , (M, s) |= 2 .
7. (M, s) |= AX iff s1 (s, s1 ) R, (M, s1 ) |= .
8. (M, s) |= EX iff s1 (s, s1 ) R, (M, s1 ) |= .
9. (M, s) |= AG iff paths = [s0 , s1 , s2 , ] s0 = si , si ,
(M, si ) |= .
10. (M, s) |= EG iff path = [s 0 , s1 , s2 , ] s0 = si , si ,
(M, si ) |= .
11. (M, s) |= AF iff paths = [s 0 , s1 , s2 , ] s0 = si , si ,
(M, si ) |= .
12. (M, s) |= EF iff path = [s 0 , s1 , s2 , ] s0 = si , si ,
(M, si ) |= .
13. (M, s) |= A[1 U2 ] iff paths = [s0 , s1 , s2 , ] s0 = s, si , (M, si ) |=
2 j < i, (M, sj ) |= 1 .
14. (M, s) |= E[1 U2 ] iff path = [s0 , s1 , s2 , ] s0 = s, si ,
(M, si ) |= 2 j < i, (M, sj ) |= 1 .
118

fiCTL Model Update System Modifications

definition, see intuitive meaning A, E, X, G
quite clear: means paths, E means exists path, X refers next
state G means states globally. semantics CTL formula easy
capture follows.
first six clauses, truth value formula state depends truth
value 1 2 state. example, truth value state
depends truth value state. contrasts clauses 7 8
AX EX. instance, truth value AX state determined
truth value s, truth values states 0 (s, s0 ) R; (s, s) R,
value also depends truth value s.
next four clauses (9 - 12) also exhibit phenomenon. example, truth value
AG involves looking truth value immediately related states,
indirectly related states well. case AG, must examine truth value
every state related number forward links (paths) current state s.
clauses 13 14, symbol U may explained until: path = [s 0 , s1 , s2 , ] satisfies
1 U2 state si < si , (M, s) |= 1 (M, si ) |= 2 .
Clauses 9 - 14 refer computation paths models. is, therefore, useful
visualize possible computation paths given state unwinding transition
system obtain infinite computation tree. greatly facilitates deciding whether
state satisfies CTL formula. unwound tree graph Figure 2 depicted
Figure 3 (note assume s0 initial state Kripke model).

S0

p, q

S2

S1

q,r

r

S2
p,q

S0

r

S2
r

q,r

S1

r

S2
r

S2

Figure 3: Unwinding transition state graph infinite tree.

Figure 3, = r, AXr true; = q, EXq true. figure,
= r, AFr true states paths satisfy r time
future. = q, EFq true states paths satisfy q time
future. clauses AG EG explained Figure 4. tree,
states satisfy r. Thus, AGr true Kripke model. one path states
satisfy = q. Thus, EGq true Kripke model.
119

fiZhang & Ding










p,q,r








AG = r;
EG

= q.










































p,q


, r





















q,r





q,r




S0

S2

S1

r

S2
S0

r

S2
r

S1

r

S2
r

S2

Figure 4: AG EG unwound tree.

following De Morgan rules equivalences (Huth & Ryan, 2004) useful
CTL model update algorithm implementation:
AF EG;
EF AG;
AX EX;
AF A[>U];
EF E[>U];
A[1 U2 ] (E[2 U(1 2 )] EG2 ).
rest paper, without explicit declaration, assume CTL
formulas occurring context satisfiable. instance, consider updating
Kripke model satisfy CTL formula , already assume satisfiable.
Definition 3, see given CTL Kripke model = (S, R, L),
(M, s) |= propositional formula, truth value solely depends
labeling function Ls assignment state s. case may simply write L(s) |=
confusion context.
2.2 Belief Update
Belief change primary research topic AI community almost two decades
e.g., (Gardenfors, 1988; Winslett, 1990). Basically, studies problem agent
change beliefs wants bring new beliefs belief set. two
types belief changes, namely belief revision belief update. Intuitively, belief revision
used modify belief set order accept new information static world,
120

fiCTL Model Update System Modifications

belief update bring belief set date world described
changes.
Katsuno Mendelzon (1991) discovered original AGM revision postulates cannot precisely characterize feature belief update. proposed following
alternative update postulates, argued propositional belief update operators
satisfy postulates. following (U1) - (U8) postulates, occurrences
, , , etc. propositional formulas.
|= .
|= .
satisfiable also satisfiable.
T1 T2 1 2 1 T2 2 .
(T ) |= ( ).
1 |= 2 2 |= 1 1 2 .
complete (i.e., unique model)
(T 1 ) (T 2 ) |= (1 2 ).
(U8) (T1 T2 ) (T1 ) (T2 ).

(U1)
(U2)
(U3)
(U4)
(U5)
(U6)
(U7)

shown Katsuno Mendelzon (1991), postulates (U1) - (U8) precisely capture
minimal change criterion update defined based certain partial ordering
models. typical model based belief update approach, briefly introduce
Winsletts Possible Models Approach (PMA) (Winslett, 1990). consider propositional language L. Let I1 I2 two Herband interpretations L. symmetric
difference I1 I2 defined dif f (I1 , I2 ) = (I1 I2 ) (I2 I1 ).
given interpretation I, define partial ordering follows: I1 I2
dif f (I, I1 ) dif f (I, I2 ). Let collection interpretations, denote in(I, )
set minimal models respect ordering , model
fixed. let two propositional formulas, update using
PMA, denoted pma , defined follows:
od( pma ) =



od()

in(M od(), ),

od() denotes set models formula . proved PMA
update operator pma satisfies postulates (U1) - (U8).
work CTL model update close connection idea belief update.
shown paper, approach, view CTL Kripke model description
world interested in, i.e., description system dynamic behaviours,
update Kripke model occurs setting system dynamic
behaviours change accommodate desired properties. Although
significant difference classical propositional belief update CTL model
update, show Katsuno Mendelzons update postulates (U1) - (U8) also
suitable characterize minimal change principle CTL model update.

3. Minimal Change CTL Model Update
would like extend idea minimal change belief update CTL model
update. principle, need update CTL Kripke model satisfy CTL formula,
121

fiZhang & Ding

expect updated model retain much information possible represented
original model. words, prefer change model minimal way achieve
goal. section, propose formal metrics minimal change CTL model
update.
3.1 Primitive Update Operations
Given CTL Kripke model (satisfiable) CTL formula, consider model
updated order satisfy given formula. discussion previous
section, try incorporate minimal change principle update approach.
first step towards aim, way measure difference two
CTL Kripke models relation given model. first illustrate initial consideration
aspect example.
Example 1 Consider simple CTL model = ({s 0 , s1 , s2 }, {(s0 , s0 ), (s0 , s1 ), (s0 , s2 ),
(s1 , s1 ), (s2 , s2 ), (s2 , s1 )}, L), L(s0 ) = {p, q}, L(s1 ) = {q, r} L(s2 ) = {r}.
described Figure 5.

p,q

s1

s0

r

q,r

s2

Figure 5: Model .

consider formula AGp. Clearly, (M, 0 ) 6|= AGp. One way update satisfy
AGp update states s1 s2 updated states satisfy p 2 . Therefore,
obtain new CTL model 0 = ({s0 , s1 , s2 }, {(s0 , s0 ), (s0 , s1 ), (s0 , s2 ), (s1 , s1 ), (s2 , s2 ),
(s2 , s1 )}, L0 ), L0 (s0 ) = L(s0 ) = {p, q}, L0 (s1 ) = {p, q, r} L0 (s2 ) = {p, r}.
update, see labeling function changed associate different truth
assignments states s1 s2 . Another way update satisfy formula AGp
simply remove relation elements (s 0 , s1 ) (s0 , s2 ) , gives (M 00 , s0 ) |= AGp,
00 = ({s0 , s1 , s2 }, {(s0 , s0 ), (s1 , s1 ), (s2 , s2 ), (s2 , s1 )}, L). closely resembles
approach Buccafurri et al. (Buccafurri et al., 1999), state changes occur.
interesting note first updated models retains structure
original, significantly changed second. two possible results
described Figure 6. 2
2. Precisely, update labeling function L changes truth assignments s1 s2 .

122

fiCTL Model Update System Modifications

p,q

p,q,r
s1

p,q

s0

p,r

q,r

s2

s1

s0

r
s2

Figure 6: Two possible results updating AGp.

example shows order update CTL model satisfy formula,
may apply different kinds operations change model. possible operations
applicable CTL model, consider five basic ones changes CTL model
achieved.
PU1: Adding one relation element
Given = (S, R, L), updated model 0 = (S 0 , R0 , L0 ) obtained adding
one new relation element. is, 0 = S, L0 = L, R0 = R {(si , sj )},
(si , sj ) 6 R two states si , sj S.
PU2: Removing one relation element
Given = (S, R, L), updated model 0 = (S 0 , R0 , L0 ) obtained removing
one existing relation element. is, 0 = S, L0 = L, R0 = R {(si , sj )},
(si , sj ) R two states si , sj S.
PU3: Changing labeling function one state
Given = (S, R, L), updated model 0 = (S 0 , R0 , L0 ) obtained changing
labeling function particular state. is, 0 = S, R0 = R, (S {s }), S,
L0 (s) = L(s), L0 (s ) set true variable assigned state L0 (s ) 6= L(s ).
PU4: Adding one state
Given = (S, R, L), updated model 0 = (S 0 , R0 , L0 ) obtained adding
one new state. is, 0 = {s }, 6 S, R0 = R, S, L0 (s) = L(s)
L0 (s ) set true variables assigned .
PU5: Removing one isolated state
Given = (S, R, L), updated model 0 = (S 0 , R0 , L0 ) obtained removing
one isolated state: 0 = {s }, 6= , neither
(s, ) (s , s) R, R0 = R, 0 , L0 (s) = L(s).
call five operations primitive since express kinds changes
CTL model. Figure 7 illustrates examples applying operations model.
five operations, PU1, PU2, PU4 PU5 represent basic operations graph. Generally, using four operations, perform changes
CTL model. instance, want substitute state CTL model,
123

fiZhang & Ding

following: (1) remove relation elements associated state, (2) remove isolated
states, (3) add state want replace original one, (4) add relevant
relation elements associated new state.
Although four operations sufficient enough represent changes CTL
model, sometimes complicate measure changes CTL models. Consider
case state substitution. Given CTL model , one CTL model 0 exactly
graphical structure except 0 one particular state different
, tend think 0 obtained single change state
replacement, instead sequence operations PU1, PU2, PU4 PU5.
motivates us operation PU3. PU3 effect state substitution,
fundamentally different combination PU1, PU2, PU4 PU5, PU3
change state name relation elements original model, assigns
different set propositional atoms state original model. sense,
combination PU1, PU2, PU4 PU5 cannot replace operation PU3. Using PU3
represent state substitution significantly simplifies measure model difference
illustrated Definition 4. rest paper, assume state
substitutions CTL model achieved PU3 unique way
measure differences CTL model changes relation states substitutions.
also note operation PU3 way substitute state CTL
model, PU5 becomes unnecessary, actually need remove isolated
state model. need remove relevant relation element(s) model,
state becomes unreachable initial state. Nevertheless, remain
discussions coherent primitive operations described above, following
definition CTL minimal change, still consider measure changes caused
applying PU5 CTL model update.
S0

S3

S0

S3

S1



S1

S2

M1

S2

PU2 applied M.

S0

S3

M2

S1

S2

PU2, PU2, PU5, PU4,
PU1 PU1 applied M.

Figure 7: Illustration primitive updates.

3.2 Defining Minimal Change
Following traditional belief update principle, order make CTL model satisfy
property, would expect given CTL model changed little possible.
using primitive update operations, CTL Kripke model may updated different ways:
124

fiCTL Model Update System Modifications

adding removing state transitions, adding new states, changing labeling function
state(s) model. Therefore, first need method measure
changes CTL models, develop minimal change criterion CTL
model update.
Given two CTL models = (S, R, L) 0 = (S 0 , R0 , L0 ), operation P U
(i = 1, , 5), Diff P U (M, 0 ) denotes differences two models 0
updated model , makes clear several operations type P U
occurred. Since PU1 PU2 change relation elements, define Diff P U 1 (M, 0 ) =
R0 R (adding relation elements only) Diff P U 2 (M, 0 ) = R R0 (removing relation elements only). operation PU3, since labeling function changed, difference measure 0 PU3 defined Diff P U 3 (M, 0 ) = {s |
0 L(s) 6= L0 (s)}. operations PU4 PU5, hand, define
Diff P U 4 (M, 0 ) = 0 (adding states) Diff P U 5 (M, 0 ) = 0 (removing states).
Let = (M, s) M0 = (M 0 , s0 ), convenience, also denote Diff (M, 0 ) =
(Diff P U 1 (M, 0 ), Diff P U 2 (M, 0 ), Diff P U 3 (M, 0 ), Diff P U 4 (M, 0 ), Diff P U 5 (M, 0 )).
worth mentioning given two CTL Kripke models 0 ,
ambiguity compute Diff P U (M, 0 ) (i = 1, , 5), primitive operation
cause one type changes (states, relation elements, labeling function) models
matter many times applied. precisely define ordering
CTL models.
Definition 4 (Closeness ordering) Let , 1 M2 three CTL Kripke models.
say M1 least close M2 , denoted M1 M2 ,
set PU1-PU5 operations transform 2 , exists set PU1-PU5
operations transform M1 following conditions hold:
(1) (i = 1, , 5), Diff P U (M, M1 ) Diff P U (M, M2 ),
(2) Diff P U 3 (M, M1 ) = Diff P U 3 (M, M2 ), Diff P U 3 (M, M1 ),
dif f (L(s), L1 (s)) dif f (L(s), L2 (s)).
denote M1 <M M2 M1 M2 M2 6M M1 .
Definition 4 presents measure difference two models respect
given model. Intuitively, say model 1 closer relative model M2 ,
(1) M1 obtained applying primitive update operations cause fewer
changes applied obtain model 2 ; (2) set states M1 affected
applying PU3 2 , take closer look difference
set propositional atoms associated relevant states. ordering
specified Definition 4, define CTL model update formally.
Definition 5 (Admissible update) Given CTL Kripke model = (S, R, L), =
(M, s0 ) s0 S, CTL formula , CTL Kripke model U pdate(M, ) called
admissible model (or admissible updated model) following conditions hold: (1)
U pdate(M, ) = (M 0 , s00 ), (M 0 , s00 ) |= , 0 = (S 0 , R0 , L0 ) s00 0 ; and, (2)
exist another updated model 00 = (S 00 , R00 , L00 ) s000 00 (M 00 , s000 ) |=
00 <M 0 . use Poss(U pdate(M, )) denote set possible admissible
models updating satisfy .
125

fiZhang & Ding

Example 2 Figure 8, model updated two different ways. Model 1 result
updating applying PU1. Model 2 another update resulting applying
PU1, PU2 PU5. Diff P U 1 (M, M1 ) = {(s0 , s2 )}, Diff P U 1 (M, M2 ) =
{(s1 , s0 ), (s0 , s2 )}, results Diff P U 1 (M, M1 ) Diff P U 1 (M, M2 ). Also, easy
see Diff P U 2 (M, M1 ) = Diff P U 2 (M, M2 ) = {(s3 , s0 ), (s2 , s3 )}, Diff P U 2 (M, M1 )
Diff P U 2 (M, M2 ). Similarly, see Diff P U 3 (M, M1 ) = Diff P U 3 (M, M2 ) = ,
Diff P U 4 (M, M1 ) = Diff P U 4 (M, M2 ) = . Finally, Diff P U 5 (M, M1 ) =
Diff P U 5 (M, M2 ) = {s3 }. According Definition 4, 1 <M M2 . 2

s0

s3



s1

s2

s0

s3

s0

s1

M1

s2

s1

s2

M2

Figure 8: Illustration minimal change rules.
note CTL model update, simply replace initial state
another existing state model satisfy formula, model actually
changed, unique admissible model according Definition 5.
case, updates ruled Definition 5. example, consider CTL
model described Figure 9: want update (M, 0 ) AXp, see

S0

S1

p

S2

Figure 9: special model update scenario.
(M, s1 ) becomes admissible updated model according definition: simply
replace initial state s0 s1 . Nevertheless, would expect update
126

fiCTL Model Update System Modifications

may also equally reasonable. instance, may change labeling function
make L0 (s1 ) = {p}. updates, changed something , change
caused first update represented minimal change definition.
overcome difficulty creating dummy state ] CTL Kripke model ,
initial state , add relation element (], s) . way, change
initial state s0 imply removal relation element (], s) addition
new relation element (], s0 ). changes measured minimal change
definition. treatment, updated models described admissible.
rest paper, without explicit declaration, assume CTL Kripke
model contains dummy state ] special state transitions ] initial states.

4. Semantic Properties
section, first explore relationship CTL model update traditional belief update, provide useful semantic characterizations typical
CTL model update cases.
4.1 Relationship Propositional Belief Update
First show following result ordering defined Definition 4.
Proposition 1 partial ordering.
Proof: Definition 4, easy see reflexive antisymmetric.
show also transitive. Suppose M1 M2 M2 M3 . According Definition 4, Dif fP U (M, M1 ) Dif fP U i(M, M2 ), Dif fP U i(M, M2 )
Dif fP U i(M, M3 ) (i = 1, , 5). Consequently, Dif f P U (M, M1 ) Dif fP U i(M, M3 )
(i = 1, , 5). Condition 1 Definition 4 holds. consider Condition 2 definition. case need consider Dif f P U 3 (M, M1 ) = Dif fP U 3(M, M2 )
Dif fP U 3 (M, M2 ) = Dif fP U 3 (M, M3 ) (note cases directly imply
Dif fP U 3 (M, M1 ) Dif fP U 3 (M, M3 ) Dif fP U 3 (M, M1 ) 6= Dif fP U 3 (M, M3 )).
case, obvious Dif f P U 3 (M, M1 ) = Dif fP U 3(M, M3 ), dif f (L(s), L1 (s))
dif f (L(s), L3 (s)). M1 M3 . 2
also interesting consider special case CTL model update update
formula classical propositional formula. following proposition indicates
propositional formula considered CTL model update, admissible model
obtained traditional model based belief update approach (Winslett, 1988).
Proposition 2 Let = (S, R, L) CTL model 0 S. Suppose
satisfiable propositional formula (M, 0 ) 6|= , admissible model updating
(M, s0 ) satisfy (M 0 , s0 ), 0 = (S, R, L0 ), (S {s0 }), L0 (s) = L(s),
L0 (s0 ) |= , exist another 00 = (S, R, L00 ) L00 (s0 ) |=
dif f (L(s0 ), L00 (s0 )) dif f (L(s0 ), L0 (s0 )).
Proof: Since propositional formula, update (M, 0 ) satisfy affect
relation elements states except 0 . Since L(s0 ) 6|= , obvious
127

fiZhang & Ding

applying PU3, change labeling function L L 0 assigns s0 new set
propositional atoms satisfy . Definition 5, see model specified
proposition indeed minimally changed CTL model respect ordering . 2
see problem addressed CTL model update essentially different
problem concerned traditional propositional belief update. Nevertheless,
idea model based minimal change CTL model update closely related belief update.
Therefore, worth investigating relationship CTL model update
traditional propositional belief update postulates (U1) - (U8). order make
comparison possible, lift update operator occurring postulates (U1) - (U8)
beyond propositional logic case.
purpose, first introduce notions. Given CTL formula Kripke
model = (S, R, L), let Init(S) set initial states . (M, s) called
model iff (M, s) |= , Init(S). use od() denote set
models . specify update operator c impose CTL formulas follows:
given two CTL formulas , define c CTL formula whose models
defined as:
od( c ) =



(M,s)M od()

Poss(U pdate((M, s), )).

Theorem 1 Operator c satisfies Katsuno Mendelzon update postulates (U1) (U8).
Proof: Definitions 4 5, easy verify c satisfies (U1)-(U4). prove
c satisfies (U5). prove (c ) |= c (), sufficient prove
model (M, s) od(), Poss(U pdate((M, s), ))M od() Poss(U pdate((M, s), )).
particular, need show (M 0 , s0 ) Poss(U pdate((M, s), )) od(),
(M 0 , s0 ) Poss(U pdate((M, s), )). Suppose (M 0 , s0 ) 6 Poss(U pdate((M, s), )).
(1) (M 0 , s0 ) 6|= ; (2) exists different admissible model (M 00 , s00 )
od() 00 <M 0 . case (1), (M 0 , s0 ) 6 Poss(U pdate((M, s), ))
od(). result holds. case (2), also implies (M 00 , s00 ) |=
00 <M 0 . means, (M 0 , s0 ) 6 Poss(U pdate((M, s), )). result still holds.
prove c satisfies (U6). prove result, sufficient prove
(M, s) od(), Poss(U pdate((M, s), 1 )) od(2 ) Poss(U pdate((M, s), 2 ))
od(1 ), Poss(U pdate((M, s), 1 )) = Poss(U pdate((M, s), 2 )). first prove
Poss(U pdate((M, s), 1 )) Poss(U pdate((M, s), 2 )). Let (M 0 , s0 ) Poss(U pdate((M, s),
1 )). (M 0 , s0 ) |= 2 . Suppose (M 0 , s0 ) 6 Poss(U pdate((M, s), 2 )). exists
different admissible model (M 00 , s00 ) Poss(U pdate((M, s), 2 )) 00 <M 0 . Also
note (M 00 , s00 ) |= 1 . contradicts fact (M 0 , s0 ) Poss(U pdate((M, s), 1 )).
Poss(U pdate((M, s), 1 )) Poss(U pdate((M, s), 2 )). Similarly, prove
Poss(U pdate((M, s), 2 )) Poss(U pdate((M, s), 1 )).
prove c satisfies (U7), sufficient prove Poss(U pdate((M, s), 1 ))
Poss(U pdate((M, s), 1 )) Poss(U pdate((M, s), 1 2 )), (M, s) unique model
(note complete). Let (M 0 , s0 ) Poss(U pdate((M, s), 1 ))Poss(U pdate((M, s),
1 )). Suppose (M 0 , s0 ) 6 Poss(U pdate((M, s), 1 2 )). exists admissible model (M 00 , s00 ) Poss(U pdate((M, s), 1 2 )) 00 <M 0 . Note
128

fiCTL Model Update System Modifications

(M 00 , s00 ) |= 1 2 . (M 00 , s00 ) |= 1 , implies (M 0 , s0 ) 6 Poss(U pdate((M, s), 1 )).
(M 00 , s00 ) |= 2 , implies (M 0 , s0 ) 6 Poss(U pdate((M, s), 2 )). cases,
(M 0 , s0 ) 6 Poss(U pdate((M, s), 1 )) Poss(U pdate((M, s), 1 )). proves result.
Finally, show c satisfies (U8). Definition 5, od(( 1 2 )c


) = (M,s)M od(1 2 ) Poss(U pdate((M, s), )) = (M,s)M od(1 ) Poss(U pdate((M, s), ))

(M,s)M od(2 ) Poss(U pdate((M, s), )) = od(1 c ) od(2 c ). completes
proof. 2
Theorem 1, evident Katsuno Mendelzons update postulates (U1) (U8) characterize wide range update formulations beyond propositional logic case,
model based minimal change principle employed. sense, view
Katsuno Mendelzons update postulates (U1) - (U8) essential requirements
model based update approaches.
4.2 Characterizing Special CTL Model Updates
previous description, observe that, given CTL Kripke model formula
, may many admissible models satisfying , simpler others.
section, provide various results present possible solutions achieve admissible updates certain conditions. general, order achieve admissible update
results, may combine various primitive operations update process.
Nevertheless, shown below, single type primitive operation enough
achieve admissible updated model many situations. characterizations also play
essential role simplifying CTL model update implementation.
Firstly, following proposition simply shows CTL update reachable
states taken account sense unreachable state never removed
newly introduced.
Proposition 3 Let = (S, R, L) CTL Kripke model, 0 initial state ,
satisfiable CTL formula (M, s0 ) 6|= . Suppose (M 0 , s00 ) admissible model
updating (M, s0 ) , 0 = (S 0 , R0 , L0 ). following properties hold:
1. state (i.e. S) reachable 0 (i.e. exist
path = [s0 , ] ), must also state 0 (i.e.
0 );
2. s0 state 0 reachable s00 , s0 must also state .
Proof: give proof result 1 since proof result 2 similar. Suppose
0 . is, removed generation (M 0 , s00 ).
Definitions 4 5, know way remove apply operation
PU5 (and possibly associated operations PU2 - removing transition relations,
connected states).
construct new CTL Kripke model 00 way 00 exactly
0 except also 00 . is, 00 = (S 00 , R00 , L00 ), 00 = 0 {s},
R00 = R0 , 0 , L00 (s ) = L0 (s ), L00 (s) = L(s). Note 00 , state
129

fiZhang & Ding

isolated state, connecting states. Since , Definition 4
see 00 <M 0 . show (M 00 , s00 ) |= . prove showing
bit general result:
Result: satisfiable CTL formula state 0 , (M 00 , ) |=
iff (M 0 , ) |= .
showed induction structure . (a) Suppose propositional
formula. case, (M 00 , ) |= iff L00 (s ) |= . Since L00 (s ) = L0 (s ), (M 0 , ) |=
iff L0 (s ) |= , (M 00 , ) |= iff (M 00 , ) |= . (b) Assume result holds
formula . (c) consider variours cases formulas constructed . (c.1) Suppose
form AG. (M 0 , ) |= AG iff every path 0 = [s , , ],
every state s0 0 , (M 0 , s0 ) |= . construction 00 , obvious every
path 0 must also path 00 , vice versa. Also induction
assumption, (M 0 , s0 ) |= iff (M 00 , s0 ) |= . follows (M 0 , ) |= AG iff
(M 00 , ) |= AG. Proofs cases AF, EG, etc. similar.
Thus, find another model 00 (M 00 , s00 ) |= 00 <M 0 .
contradicts fact (M 0 , s00 ) admissible model update (M, 0 )
. 2

Theorem 2 Let = (S, R, L) Kripke model = (M, 0 ) 6|= EX, s0
propositional formula. Let 0 = Update(M, EX) model obtained
update EX following 1 2, 0 admissible model.
1. PU3 applied one succ(s0 ) make L0 (succ(s0 )) |=
diff (L(succ(s0 )), L0 (succ(s0 ))) minimal, or, PU4 PU1 applied successively add new state L0 (s ) |= new relation element (s0 , );
2. exists si L(si ) |= si 6= succ(s0 ), PU1 applied
add new relation element (s 0 , si ).
Proof: Consider case 1 first. PU3 applied change assignment succ(s 0 ),
PU4 PU1 applied add new state relation element (s0 , ), new
model 0 contains succ(s0 ) L0 (succ(s0 )) |= . Thus, M0 = (M 0 , s0 ) |= EX.
PU3 applied once, Diff (M, M0 ) = (, , {succ(s0 )}, , ); PU4 PU1 applied successively, Diff (M, M0 ) = ({(s0 , )}, , , {, }, ). Thus, updates single
application PU3 applications PU4 PU1 successively compatible
other. PU3, update applied combination, Diff (M, 00 )
either compatible Diff (M, 0 ) contain Diff (M, M0 ) (e.g., another PU3
together predecessor). similar situation occurs applications PU4
PU1. Thus, applying either PU3 PU4 PU1 successively represents
minimal change. case 2, PU1 applied connect 0 L(si ) |= , new
model 0 successor satisfies . Thus, 0 = (M 0 , s0 ) |= EX. PU1 applied,
Diff (M, M0 ) = ({(s0 , si )}, , , , ). Note case remains minimal change
relation element original model compatible case 1. Hence, case 2
130

fiCTL Model Update System Modifications

also represents minimal change. 2
Theorem 2 provides two cases admissible CTL model update results
achieved formula EX. important note restrict propositional formula. first case says either select one successor states
s0 change assignment minimally satisfy (i.e., apply PU3 once), simply add
new state new relation element satisfies successor 0 (i.e., apply PU4
PU1 successively). second case indicates state already
satisfies , enough simply add new relation element (s 0 , si ) make
successor s0 . Clearly, cases yield new CTL models satisfy EX.
Theorem 3 Let = (S, R, L) Kripke model = (M, 0 ) 6|= AG, s0 S,
propositional formula s0 |= . Let M0 = Update(M, AG) model obtained
update AG following way, 0 admissible model.
path starting s0 : = [s0 , , si , ]:
1. < si , L(s) |= L(si ) 6|= , PU2 applied remove relation
element (si1 , si );
2. PU3 applied states satisfying change assignments
L0 (s) |= diff (L(s), L0 (s)) minimal.
Proof: Case 1 simply cut path first state satisfy . Clearly,
one minimal way cut : remove relation element (s i1 , s) (i.e., apply PU2
once). Case 2 minimally change assignments states belonging
satisfy . Since changes imposed case 1 case 2 compatible
other, generate admissible update results. 2
Theorem 3, case 1 considers special form path first states
starting s0 already satisfy formula . condition, simply cut
path disconnect states satisfying . Case 2 straightforward: minimally
modify assignments states belonging satisfy formula .
Theorem 4 Let = (S, R, L) Kripke model, = (M, 0 ) 6|= EG, s0
propositional formula. Let 0 = Update(M, EG) model obtained
update EG following way, 0 admissible model: Select
path = [s0 , s1 , , si , , sj , ] contains minimal number different
states satisfying 3 ,
1. s0 L(s0 ) 6|= , exist si , sj satisfying si < s0 < sj
si sj , L(s) |= , PU1 applied add relation element (s , sj ),
PU4 PU1 applied add state L0 (s ) |= new relation
elements (si , ) (s , sj );
2. si si , L(s) |= , sk 00 , 00 = [s0 , , sk , ]
sk L(s) |= , PU1 applied connect sk ;
3. Note although path may infinite, contain finite number different states.

131

fiZhang & Ding

3. si (i > 1) s0 < si , L(s0 ) |= , L(si ) 6|= , then,
a. PU1 applied connect si1 s0 form new transition (si1 , s0 );
b. si successor si1 , PU2 applied remove relation element
(si1 , si );
4. s0 , L(s0 ) 6|= , PU3 applied change assignments
states s0 L0 (s0 ) |= diff (L(s), L0 (s0 )) minimal.
Proof: case 1, without loss generality, assume selected path ,
exist states s0 satisfy , states satisfy . also assume
s0 middle path . Therefore, two states , sj
si < s0 < sj . is, = [s0 , , si1 , si , , s0 , , sj , sj+1 , ]. first
consider applying PU1. clear applying PU1 add new relation element
(si , sj ), new path formed: 0 = [s0 , , si1 , si , sj , sj+1 , ]. Note state
0 also path s0 6 0 . Accordingly, know EG holds new
model 0 = (S, R {(si , sj )}, L) state s0 . Consider = (M, s0 ) M0 = (M 0 , s00 ).
Clearly, Diff (M, M0 ) = ({(si , sj )}, , , , ), implies (M 0 , s0 ) must minimally changed model respect satisfies EG.
consider applying PU4 PU1. case, new model
0 = (S {s }, R {(si , ), (s , sj )}, L0 ) L0 extension L new state
satisfies . see 0 = [s0 , , si , , sj , ] path 0 shares
states path except state 0 states si+1 sj1
including s0 . also (M 0 , s0 ) |= EG. Furthermore, Diff (M, 0 ) =
({(si , ), (s , sj )}, , , {s }, ). Obviously, (M 0 , s0 ) minimally changed model
respect satisfies EG.
worth mentioning case 1, model obtained applying PU1
comparable model obtained applying PU4 PU1, set inclusion
relation holds changes relation elements caused two different ways.
case 2, consider two different paths = [s 0 , , si , ] 0 = [s0 , , sk , ]
states state si path satisfy , states state k path 0
satisfy , PU1 applied form new transition (s , sk ). transition therefore
connects states s0 si path states sk path 0 . Hence states
new path [s0 , , si , sk ] satisfy . Thus, M0 |= EG. change also minimal,
PU1 applied, Diff (M, 0 ) = ({(si , sk )}, , , , ) minimum (M 0 , s0 )
minimally changed model respect satisfies EG.
case 3, two situations. (a) PU1 applied form new transition (si1 , s0 ), new path containing [s0 , , s0 , , si1 , s0 , , si1 , s0 , ] consists
Strongly Connected Components states satisfy ,
Diff (M, M0 ) = ({(si1 , s0 )}, , , , ) minimum. Thus, (M 0 , s0 ) minimally changed
model respect satisfies EG.
(b) PU2 applied, then, new path 0 containing [s0 , , s0 , , si1 ] derived
states satisfy Dif f (M, 0 ) = (, {(si1 , si )}, , , ) minimal. Obviously,
(M 0 , s0 ) minimally changed model respect satisfies EG.
case 4, suppose n states selected path satisfy .
PU3 applied states, Diff (M, 0 ) = (, , {s01 , s02 , , s0n }, , ),
s0 {s01 , , s0n }, dif f (L(s0 ), L0 (s0 )) minimal. Diff (M, M0 ) case
132

fiCTL Model Update System Modifications

compatible cases 1, 2 3. Thus, (M 0 , s0 ) minimally changed model
respect satisfies EG. 2
Theorem 4 characterizes four typical situations update formula EG
propositional formula. Basically, theorem says order make formula
EG true, first select path, either make new path based path
states new path satisfy (i.e., case 1, case 2 case 3(a)), trim path
state previous states satisfy (i.e., case 3(b)), previous state
state successor; simply change assignments states satisfying
path (i.e., case 4). proof shows models obtained operations
admissible.
possible provide semantic characterizations updates special
CTL formulas EF, AX, E[U]. fact, prototype implementation,
characterizations used simplify update process whenever certain
conditions hold.
also indicate characterization theorems presented section
provide sufficient conditions compute admissible models. admissible
models captured theorems.

5. Computational Properties
section, study computational properties CTL model update approach
detail. first present general complexity result, identify useful
subclass CTL model updates always achieved polynomial time.
5.1 General Complexity Result
Theorem 5 Given two CTL Kripke models = (S, R, L) 0 = (S 0 , R0 , L0 ),
s0 s00 0 , CTL formula , co-NP-complete decide whether (M 0 , s00 )
admissible model update (M, 0 ) satisfy . hardness holds even
form EX propositional formula.
Proof: Membership proof: Firstly, know Clarke et al. (1999) checking
whether (M 0 , s00 ) satisfies performed time O(|| (|S| + |R|)). order
check whether (M 0 , s00 ) admissible update result, need check whether 0
minimally updated model respect ordering . purpose, consider
complement problem checking whether 0 minimally updated model.
Therefore, two things: (1) guess another updated model : 00 = (S 00 , R00 , L00 )
satisfying s00 00 ; and, (2) test whether 00 <M 0 . Step (1) done
polynomial time. check 00 <M 0 , first compute dif f (S, 0 ), dif f (S, 00 ),
dif f (R, R0 ) dif f (R, R00 ). computed polynomial time. Then, according sets, identify Dif f P U (M, 0 ) Dif fP U i(M, 00 ) (i = 1, , 5)
terms PU1 PU5. Again, steps also completed polynomial time. Finally,
checking Dif fP U i(M, 00 ) Dif fP U i(M, 0 ) (i = 1, , 5), dif f (L(s), L0 (s))
dif f (L(s), L00 (s)) Dif fP U 3 (M, 00 ) (if Dif fP U 3 (M, 00 ) = Dif fP U 3 (M, 0 )),
133

fiZhang & Ding

decide whether 00 <M 0 . Thus, steps (1) (2) achieved
polynomial time non-deterministic Turing machine.
Hardness proof: well known validity problem propositional formula
co-NP-complete. Given propositional formula , construct transformation
problem deciding validity CTL model update polynomial time. Let X
set variables occurring , a, b two new variables occur X.
V
denote X = xi X xi . Then, specify CTL Kripke model based variable set
X {a, b}: = ({s0 , s1 }, {(s0 , s1 ), (s1 , s1 )}, L), L(s0 ) = (i.e., variables assigned false), L(s1 ) = X (i.e., variables X assigned true, a, b assigned false).
define new formula = EX((( a)(X b))(a)). Clearly, formula ((
a)(X b))(a) satisfiable 1 6|= (( a)(X b))(a). (M, 0 ) 6|= .
Consider update U pdate((M, s0 ), ). define 0 = ({s0 , s1 }, {(s0 , s1 ), (s1 , s1 )}, L0 ),
L0 (s0 ) = L(s0 ) L0 (s1 ) = {a, b}. Next, show valid iff (M 0 , s0 )
admissible update result U pdate((M, 0 ), ).
Case 1: show valid, (M 0 , s0 ) admissible update result
U pdate((M, s0 ), ). Since valid, X |= . Thus, L 0 (s1 ) |= ( a) (X b)).
leads (M 0 , s0 ) |= . Also note 0 obtained applying PU3 change L(s 1 )
L0 (s1 ). dif f (L(s1 ), L0 (s1 )) = X {a, b}, presents minimal change L(s 1 )
order satisfy ( a) (X b).
Case 2: Suppose valid. Then, X 1 X exists X1 |= . construct 00 = ({s0 , s1 }, {(s0 , s1 ), (s1 , s1 )}, L00 ), L00 (s0 ) = L(s0 ) L00 (s1 ) = X1 {a}.
seen L00 (s1 ) |= ( a), hence (M 00 , s0 ) |= . show (M 0 , s0 ) |=
implies 00 <M 0 . Suppose (M 0 , s0 ) |= . Clearly, 0 00 obtained applying PU3 change assignment 1 . However,
dif f (L(s1 ), L00 (s1 )) = (X X1 ) {a} X {a, b} = dif f (L(s), L0 (s1 )). Thus, conclude
(M 0 , s0 ) admissible updated model. 2
Theorem 5 implies probably feasible develop polynomial time algorithm
implement CTL model update. Indeed, algorithm described next section,
generally runs exponential time.
5.2 Tractable Subclass CTL Model Updates
light complexity result Theorem 5, expect identify useful cases
CTL model updates performed efficiently. First, following
observation.
Observation: Let = (S, R, L) CTL Kripke model, CTL formula (M, 0 ) 6|=
s0 S. admissible model U pdate((M, 0 ), ) obtained applying
operations PU1 PU2 , result computed polynomial time.
Intuitively, admissible updated model obtained using PU1 PU2,
implies need visit states relation elements ,
operation involving PU1 PU2 completed adding removing relation
elements, obviously done linear time.
134

fiCTL Model Update System Modifications

observation tells us certain conditions, operations PU1 PU2 may
efficiently applied compute admissible model. quite obvious PU3
PU4 involved finding models propositional formulas, applying
PU3 usually needs find minimal change assignment state,
operations may cost exponential time size input updating formula
. However, observation tell us kinds CTL model updates
really achieved polynomial time. following, provide sufficient condition
class CTL model updates always solved polynomial time.
first specify subclass CTL formulas AEClass: (1) formulas AX, AG, AF,
A[1 U2 ], EX, EG, EF E[1 U2 ] AEClass, , 1 2 propositional formulas; (2) 1 2 AEClass, 1 2 1 2
AEClass; (3) formulas specified (1) (2) AEClass.
also call formulas forms specified (1) atomic AEClass formulas.
Note AEClass class CTL formulas without nested temporal operators.
Although somewhat restricted, show next, updates kind CTL
formulas may much simpler cases. define valid states paths
AEClass formulas respect given model.
Definition 6 (Valid state path AEClass) Let = (S, R, L) CTL Kripke
model, AEClass, (M, s0 ) 6|= , s0 S. define valid state valid
path (M, s0 ) follows.
1. form AX, state valid state (M, 0 ) (s0 , s) R
L(s) |= ;
2. form (a) AG, (b) AF (c) A[ 1 U2 ], path = [s0 , ]
valid path (M, s0 ) , L(s) |= (case (a)); > 0 ,
L(s) |= (case (b)); , |= 2 s0 < L(s0 ) |= 1 (case (c)) respectively;
3. form EX, state valid state (M, 0 ) L(s) |= ;
4. form (a) EG, (b) EF (c) E[ 1 U2 ], path = [s00 , ]
(s00 6= s0 ) valid path (M, s0 ) , L(s) |= L(s0 ) |= (case
(a)); > s00 , L(s) |= (case (b)); L(s0 ) |= 1 , L(s) |= 2
s0 < L(s0 ) |= 1 (case (c)) respectively.
arbitrary AEClass, say valid witness (M, 0 ) every atomic
AEClass formula occurring valid state path (M, 0 ).
Intuitively, formulas AX, AG, AF A[ 1 U2 ], valid state path
CTL model represents local structure partially satisfies underlying formula.
formulas EX, EG, EF E[1 U2 ], hand, valid state path also
represents local structure satisfy underlying formula relation element
added connect local structure initial state.
Example 3 Consider CTL Kripke model Figure 10 formula EX(p q).
Clearly, (M, s0 ) 6|= EX(p q). Since p, q L(s3 ), s3 valid state EX(p q).
135

fiZhang & Ding

S0

S1
p

S2

q

r

S3

S4

p,q

r,p

Figure 10: simple CTL model update.

simply add one relation element (s 0 , s3 ) form new model 0
(M 0 , s0 ) |= EX(p q). Obviously, (M 0 , s0 ) admissible updated model.
2
example, observe update CTL model AEClass
formula formula valid witness model, possible compute
admissible model adding removing relation elements (i.e. operations PU1
PU2). following results confirm CTL model update AEClass formula
may achieved polynomial time formula valid witness model.
Theorem 6 Let = (S, R, L) CTL Kripke model, AEClass, (M, 0 ) 6|=
. Deciding whether valid witness (M, 0 ) solved polynomial time.
Furthermore, valid witness (M, 0 ), valid states paths atomic
AEClass formulas occurring computed (M, 0 ) time O(||(|S|+|R|)2 ).

Proof: prove theorem, show using CTL model checking algorithm SAT
(Huth & Ryan, 2004), takes CTL Kripke model AEClass formula inputs,
generate valid states paths atomic AEClass formulas occurring (if
any). know complexity algorithm SAT O(|| (|S| + |R|)). consider
case atomic AEClass formulas.
AX. use SAT check whether (M, 0 ) |= EX. (M, s0 ) 6|= EX,
AE valid state (M, 0 ). Otherwise, SAT return state
L(s) |= (s0 , s) R. remove relation element (s 0 , s) , continue
checking formula EX model. end process, obtain valid states
(M, s0 ) formula AX. Altogether, |S| SAT calls.
AG. use SAT check whether (M, 0 ) |= EG. (M, s0 ) |= EG,
obtain path SAT = [s0 , s1 , ] , L(s) |= . Clearly,
valid path AG. exist state 6
(s, s) R , i.e. state connects state leading different path,
136

fiCTL Model Update System Modifications

process stops, valid path AG. Otherwise, remove
one relation element (s, s0 ) (i.e. s, s0 ) states s00
s0 < s00 , relation element (s00 , ) leading different path (i.e. 6 ).
way, actually disable path satisfy formula EG without affecting paths.
continue checking formula EG newly obtained model. end
process, obtain paths make EG true, paths valid paths
AG. Since generated valid path, need remove one relation element
path generate next valid path, |R| valid paths
generated. together, |R| SAT calls find valid paths
AG.
cases AF A[U2 ], valid paths formulas generated
similar way described formula AG. different point
case A[U2 ], valid path generated, need find last state
2 becomes true, connects state 6 leading different
path, disable removing relation element (s, succ(s)) . continue
procedure generate next valid path A[U 2 ]. exists ,
process stops.
EX. case, valid state found checking whether L(s) |= .
need visit |S| states checking.
EG. Similarly, find valid path selecting state (s 6= 0 ),
(M, s) |= EG. most, need visit |S| states, |S| SAT calls check
(M, s) |= EG.
Finally, valid paths EF E[ 1 2 ] found similar way. 2

Theorem 7 Let = (S, R, L) CTL Kripke model, AEClass, (M, 0 ) 6|= .
admissible model U pdate((M, s0 ), ) computed polynomial time valid
witness (M, s0 ).
Proof: proof Theorem 6, obtain valid states paths atomic
AEClass formulas time O(|| (|S| + |R|) 2 ). consider case atomic
AEClass formulas , cases conjunctive disjunctive AEClass formulas
easy justify.
AX. Let = {s1 , , sk } valid states AX. remove relation
elements (s0 , s) 6 . way, obtain new model 0 = (S, R0 , L),
R0 = R {(s0 , s) | 6 }. Obviously, (M 0 , s0 ) |= AX. also easy see
change 0 minimal order satisfy AX. (M 0 , s0 ) also
admissible model.
AG. Let set states valid paths AG.
state s0 L(s0 ) 6|= , check whether s0 reachable s0 . reachable,
remove relation element (s 1 , s2 ) s0 becomes unreachable
s0 (s1 , s2 ) relation element valid path AG. Clearly, model (M 0 , s0 )
satisfy AG. Also, checking whether state reachable 0 done
polynomial time computing spanning tree rooted 0 (Pettie & Ramachandran,
2002).
137

fiZhang & Ding

AF. case, need cut paths starting 0
valid paths AF (M, s0 ). this, sufficient disconnect states
reachable s0 occur AFs valid paths (M, 0 ). Let
set states, R set relation elements directly connected
states, i.e. (s1 , s2 ) R iff s1 s2 . remove minimal subset
R removing disconnect states s0 . set
identified polynomial time computing spanning tree rooted 0 ,
minimal subset R disconnects states s0 found time
O(|R |2 ). entire process completed polynomial time.
case A[1 U2 ] handled similar way described AF.
consider EX. case, need select one valid state
EX, add relation element (s0 , s) . model (M 0 , s0 ) satisfies EX.
case EG, also select valid path = [s, ] EG, add relation
element (s0 , s), (M 0 , s0 ) |= EG. two cases EF E[ 1 U2 ]
handled similar way. 2
emphasize although results characterize useful subclass
CTL model update scenarios admissible updated models computed
simple operations adding removing relation elements, mean
admissible models represent intuitive modifications practical viewpoint.
Sometimes, update problem, using operations PU3 PU4
probably preferred order generate sensible system modification.
illustrated Section 7.

6. CTL Model Update Algorithm
implemented prototype CTL model update. implementation,
CTL model update algorithm designed line CTL model checking algorithm
used SAT (Huth & Ryan, 2004), updated formula parsed according
structure recursive calls appropriate functions used. recursive call usage
allows checked property range nested modalities atomic propositional
formulas. section, focus discussions key ideas handling CTL
model update provide high level pseudo code major functions algorithm.
6.1 Main Functions
Handling propositional formulas
Since satisfaction propositional formula involve relation elements
CTL Kripke model, implement update propositional formula directly
operation PU3 minimal change labeling function truth assignment relevant state. procedure outlined follows.
function Updateprop ((M, s0 ), )
input: (M, s0 ) , = (S, R, L) s0 S;
output: (M 0 , s00 ), 0 = (S 0 , R0 , L0 ), s00 0 L0 (s00 ) |= ;
138

fiCTL Model Update System Modifications

01 begin
02
apply PU3 change labeling function L state 0 form new model 0 =
0
0
(S , R , L0 ):
03
0 = S; R0 = R; 6= s0 , L0 (s) = L(s);
04
L0 (s0 ) defined L0 (s0 ) |= , dif f (L0 (s0 ), L(s0 )) minimal;
05
return (M 0 , s0 );
06 end
easy observe procedure implemented PMA belief update
(Winslett, 1988). used lowest level CTL model update prototype.
Handling modal formulas AF, EX E[ 1 2 ]
De Morgan rules equivalences displayed Section 2.1, know
CTL formulas modal operators expressed terms three typical CTL
modal formulas. Hence sufficient give update functions three types
formulas without considering types CTL modal formulas.
function UpdateAF ((M, s0 ), AF)
input: (M, s0 ) AF, = (S, R, L), s0 S, (M, s0 ) 6|= AF;
output: (M 0 , s00 ), 0 = (S 0 , R0 , L0 ), s00 0 (M 0 , s00 ) |= AF;
01 begin
02
S, (M, s) 6|= ,
03
select state reachable 0 , (M 0 , ) = CTLUpdate((M, s), )4 ;
04
else select path starting 0 , (M, s) 6|= , (a) (b):
05
(a) select state , (M 0 , s0 ) = CTLUpdate((M, s), );
06
(b) apply PU2 disable path form new model:
07
remove relation element affect paths;
08
form new model 0 = (S 0 , R0 , L0 ):
09
0 = S, R0 = R {(si , si+1 )} (note (si , si+1 ) ),
10
0 , L0 (s) = L(s);
0
11
(M , s00 ) |= AF, return (M 0 , s00 )5 ;
12
else UpdateAF ((M 0 , s00 ), AF);
13 end
Function UpdateAF handles update formula AF follows: state
model satisfies formula , UpdateAF first update model one state satisfy ;
otherwise, path model fails satisfy AF, Update AF either disables
path minimal way, updates path make valid AF.
function UpdateEX ((M, s0 ), EX)
input: (M, s0 ) EX, = (S, R, L), s0 S, (M, s0 ) 6|= EX;
output: (M 0 , s00 ), 0 = (S 0 , R0 , L0 ), s00 0 (M 0 , s00 ) |= EX;
01 begin
02
one (a), (b) (c):
4. CTLUpdate((M, s), ) main update function describe later.
5. s00 corresponding state s0 updated model 0 , functions
described next.

139

fiZhang & Ding

03
04
05
06
07
08
09
10
11
12
13

(a) apply PU1 form new model:
select state (M, s) |= ;
add relation element (s0 , s) form new model 0 = (S 0 , R0 , L0 ):
0 = S; R0 = R {(s0 , s)}; S, L0 (s) = L(s);
(b) select state = succ(s0 ), (M 0 , ) = CTLUpdate((M, s), );
(c) apply PU4 PU1 form new model 0 = (S 0 , R0 , L0 ):
0 = {s }; R0 = R {(s0 , }; S, L0 (s) = L(s),
L0 (s ) defined (M 0 , ) |= ;
0
(M , s00 ) |= EX, return (M 0 , s00 );
else UpdateEX ((M 0 , s00 ), EX);
end

Function UpdateEX may viewed implementation algorithm characterization EX Theorem 2 Section 4. However, worth mentioning
algorithm illustrates difference update functions update
characterizations demonstrates wider application algorithm compared
corresponding characterizations. usage recursive calls algorithm allows
arbitrary CTL formula rather propositional formula demonstrated
characterizations. major difference characterizations
algorithmic implementation.
function UpdateEU ((M, s0 ), E[1 U2 ])
input: (M, s0 ) E[1 U2 ], = (S, R, L), s0 S, (M, s0 ) 6|= E[1 U2 ];
output: (M 0 , s00 ), 0 = (S 0 , R0 , L0 ), s00 0 (M 0 , s00 ) |= E[1 U2 ];
01 begin
02
(M, s0 ) 6|= 1 , (M 0 , s00 ) = CTLUpdate((M, s0 ), 1 );
03
else (a) (b):
04
(a) (M, s0 ) |= 1 , path = [s , ] (s0 6= )
05
(M, ) |= E[1 U2 ],
06
apply PU1 form new model 0 = (S 0 , R0 , L0 ):
07
0 = S; R0 = R {(s0 , }; L0 (s) = L(s);
08
(b) select path = [s0 , , si , , sj , ];
09
s0 < < si , (M, s) |= 1 , (M, sj ) |= 2 ,
10
s0 si+1 < s0 < sj1 , (M, s0 ) 6|= 1 2
11
apply PU1 form new model 0 = (S 0 , R0 , L0 ):
12
0 = S; R0 = R {(si , sj )}; S, L0 (s) = L(s);
13
< si , (M, s) |= 1 , s0 s0 > si+1 , (M, s0 ) 6|= 1 2 ,
14
apply PU4 form new model 0 = (S 0 , R0 , L0 ):
15
0 = {s }; R0 = R {(si1 , ), (s , si )};
16
S, L0 (s) = L(s), L(s ) defined (M 0 , ) |= 2 ;
0
0
17
(M , s0 ) |= E[1 U2 ], return (M 0 , s00 );
18
else UpdateEU ((M 0 , s00 ), E[1 U2 ]);
19 end
update (M, s0 ) satisfy formula E[1 U2 ], function UpdateEU first checks whether
satisfies 1 initial state s0 . not, UpdateEU update
140

fiCTL Model Update System Modifications

initial state model satisfies 1 initial state. make later
update possible. condition (M, 0 ) satisfies 1 , UpdateEU considers
two cases: valid path formula E[ 1 U2 ], simply links
initial state s0 path forms new path satisfies E[ 1 U2 ] (i.e. case (a));
UpdateEU directly selects path make satisfy formula E[ 1 U2 ] (i.e. case (b)).
Handling logical connectives ,
De Morgan rules equivalences CTL modal formulas, update
formula handled quite easily. fact need consider primary forms
negative formulas algorithm implementation. Update disjunctive formula
1 2 , hand, simply implemented calling CTLUpdate((M, 0 ), 1 )
CTLUpdate((M, s0 ), 2 ) nondeterministic way. Hence describe
function updating conjunctive formula 1 2 .
function Update ((M, s0 ), 1 2 )
input: (M, s0 ) 1 2 , = (S, R, L), s0 S, (M, s0 ) 6|= 1 2 ;
output: (M 0 , s00 ), 0 = (S 0 , R0 , L0 ), s00 0 (M 0 , s00 ) |= 1 2 ;
01 begin
02
1 2 propositional formula, (M 0 , s00 ) = Updateprop ((M, s0 ), 1 2 );
03
else (M , s0 ) = CTLUpdate((M, s0 ), 1 );
04
(M 0 , s00 ) = CTLUpdate((M , s0 ), 2 ) constraint 1 ;
05
return (M 0 , s00 );
06 end
Function Update handles update conjunctive formula obvious way. Line
04 indicates conduct update 2 , view 1 constraint
update obey. Without condition, result updating 2 may violate
satisfaction 1 achieved previous update. address point
details next subsection.
Finally, describe CTL model update algorithm follows.
algorithm CTLUpdate((M, s0 ), )
input: (M, s0 ) , = (S, R, L), s0 S, (M, s0 ) 6|= ;
output: (M 0 , s00 ), 0 = (S 0 , R0 , L0 ), s00 0 (M 0 , s00 ) |= ;
01 begin
02
case
03
propositional formula: return Update prop ((M, s0 ), );
04
1 2 : return Update ((M, s0 ), 1 2 );
05
1 2 : return Update ((M, s0 ), 1 2 );
06
1 : return Update ((M, s0 ), 1 );
07
AX1 : return CTLUpdate((M, s0 ), EX1 );
08
EX1 : return UpdateEX ((M, s0 ), EX1 );
09
A[1 U2 ]: return CTLUpdate((M, s0 ), (E[2 U(1 2 )] EG2 ));
10
E[1 U2 ]: return UpdateEU ((M, s0 ), E[1 U2 ]);
11
EF1 ; return CTLUpdate((M, s0 ), E[>U1 ]);
12
EG1 : return CTLUpdate((M, s0 ), AF1 );
141

fiZhang & Ding

13
14
15
16

AF1 : return UpdateAF ((M, s0 ), AF1 );
AG1 : return CTLUpdate((M, s0 ), E[>U1 ]);
end case;
end

Theorem 8 Given CTL Kripke model = (S, R, L) satisfiable CTL formula
, (M, s0 ) 6|= s0 S. Algorithm CTLUpdate((M, s0 ), ) terminates
generates admissible model satisfy . worst case, CTLUpdate runs time
O(2|| ||2 (|S| + |R|)2 ).
Proof: Since assumed satisfiable, descriptions,
difficult see CTLUpdate call functions finite times, call
functions (recursively) generate result satisfies underlying updated
formula, return main algorithm CTLUpdate. CTLUpdate((M, 0 ), )
terminate, output model (M 0 , s00 ) satisfies .
show output model (M 0 , s00 ) admissible induction structure
. proof quite tedious - involves detailed examinations running
update function. sufficient observe update function, time
input model updated minimal way, e.g., adds one state relation element, removes
minimal set relation elements disconnect state, updates state minimally.
iterated updates sub-formulas , minimal changes original input model
retained.
consider complexity CTLUpdate. first analyze functions complexity without considering embedded recursions. Function Update prop update
state propositional formula, worst time complexity O(2 || ). Function UpdateAF contains following major computations: (1) finding reachable state
(M, s0 ); (2) selecting path state satisfy ; (3) checking
(M 0 , s00 ) |= AF. Task (1) achieved computing spanning tree rooted
s0 , done time O(|R| log|S|) (Pettie & Ramachandran, 2002). Task
(2) reduced find valid path formula AG. Theorem 6,
done time O(|| (|S| + |R|)2 ). Task (3) complexity task (2). So,
overall, function UpdateAF complexity O(|| (|S| + |R|) 2 ). Similarly,
show functions UpdateEX UpdateEU complexity O(|| (|S| + |R|)2 )
O(|| (|S| + |R|)2 + 2|| ) respectively. functions complexity obvious either
implementations based De Morgan rules equivalences, calls
functions (i.e. Update ) main algorithm (i.e. Update Update ).
algorithm CTLUpdate || calls functions. Therefore, worst time,
CTLUpdate runs time O(2|| ||2 (|S| + |R|)2 ). 2

6.2 Discussions
worth mentioning except functions Update prop , Update Update ,
functions used algorithm CTLUpdate involved nondeterministic choices.
implies algorithm CTLUpdate syntax independent. words, given
142

fiCTL Model Update System Modifications

CTL model two logical equivalent formulas, updating model one formula
may generate different admissible models.
description function Update , briefly mentioned issue constraints
CTL model update. general, perform CTL model update, usually
protect properties violated update procedure.
properties usually called domain constraints. difficult modify algorithm
CTLUpdate cope requirement. particular, suppose C set domain
constraints system specification = (S, R, L), need update (M, 0 )
formula , s0 S, C {} satisfiable. function CTLUpdate,
simply add model checking condition candidate model 0 = (S 0 , R0 , L0 ): (M 0 , s00 ) |=
C (s00 0 ). result (M 0 , s00 ) returned function satisfies C. Otherwise,
function look another candidate model. Since model checking (M 0 , s00 ) |= C
done time O(|C| (|S 0 | + |R0 |)), modified algorithm significantly increase
overall complexity. implemented system prototype, integrated generic
constraint checking component option added update functions
domain constraints may taken account necessary.
addition implementation algorithm CTLUpdate, implemented
separate update functions typical CTL formulas EX, AG, EG, AF, EF,
etc., propositional formula, based characterizations provided Section
4.2. functions simplify update procedure input formula contain
nested CTL temporal operators converted simplified formula.

7. Two Case Studies
section, show two case studies applications CTL model update approach
system modifications. two cases implemented CTL model updater
prototype, simplified compiler. prototype, input complete CTL
Kripke model CTL formula, output updated CTL Kripke model
satisfies input formula.
indicate prototype contains three major components: parsing, model
checking model update functions. prototype first parses input formula
breaks atomic subformulas. model checking function checks
whether input formula satisfied underlying model. formula satisfied
model, model checking function generate relevant states violate
input formula. Consequently, information directly used model update
function update model.
7.1 Microwave Oven Example
consider well-known microwave oven scenario presented Clarke et al. (1999),
used illustrate CTL model checking algorithm model describing
behaviour microwave oven. Kripke model shown Figure 11 viewed
hardware design microwave oven. Kripke model, state labeled
propositional atoms true state negations propositional
atoms false state. labels arcs present actions cause
state transitions Kripke model. Note actions part Kripke model.
143

fiZhang & Ding

initial state state 1. given Kripke model describes behaviour
microwave oven.

s1

start oven

s2
Start
~Close
~Heat
Error

open door

s5

close door

Start
Close
~Heat
Error

~Start
~Close
~Heat
~Error

open door

close door

s3

s4

~Start
Close
~Heat
~Error

reset

s6

done

~Start
Close
Heat
~Error

start cooking

start oven

Start
Close
~Heat
~Error

cook

open door

s7
warm

Start
Close
Heat
~Error

Figure 11: CTL Kripke model microwave oven.

observed model satisfy desired property = EF(Start
EGHeat): microwave oven started, stuff inside eventually heated
(Clarke et al., 1999)6 . is, (M, s1 ) 6|= . would like apply
CTL model update prototype modify Kripke model satisfy property .
mentioned earlier, since prototype combines formula parsing, model checking model
update together, update procedure case study exactly follow generic
CTL model update algorithm illustrated Section 6.

s1

~Start
~Close
~Heat
~Error

open door

s2
Start
~Close
~Heat
Error

open door

s5

close door

Start
Close
~Heat
Error

close door

s3

s4

~Start
Close
~Heat
~Error

reset

s6

done

s7
warm

~Start
Close
Heat
~Error

start cooking

start oven

Start
Close
~Heat
~Error

cook

open door

Start
Close
Heat
~Error

Figure 12: Updated microwave oven model using PU2.

6. formula equivalent AG(Start AFheat).

144

fiCTL Model Update System Modifications

First, parse AG((StartEGHeat)) remove front . translation
performed function Update , called CTLUpdate((M, 1 ), ). check
whether state satisfies (StartEGHeat). First, select EGHeat checked
using model checking function EG. model checking, path every
state Heat identified. find paths [s 1 , s2 , s5 , s3 , s1 , ] [s1 , s3 , s1 , ]
strongly connected component loops (Clarke et al., 1999) containing states
Heat. Thus model satisfies EGHeat. Consequently, identify states
Start: {s2 , s5 , s6 , s7 }. select states Start Heat:
{s2 , s5 }. Since formula AG((StartEGHeat)) requires model
states Start Heat, perform model update related
states s2 s5 . Now, using Theorem 3 Section 4.2, proper update performed.
Eventually, obtain two possible minimal updates: (1) applying PU2 remove relation
element (s1 , s2 ); (2) applying PU3 change truth assignments 2 s5 .
update, model satisfies formula minimal change original
model . instance, choosing update (1) above, obtain new Kripke model
(as shown Figure 12), simply states state transition 1 s2 allowed,
whereas choosing update (2), obtain new Kripke model (as shown Figure 13),
says allowing transition state 1 state s2 cause error microwave
oven could start s2 , error message carry next state 5 .
s1

start oven
s2

~Start
~Close
~Heat
Error

open door

s5 ~Start

close door

Close
~Heat
Error

~Start
~Close
~Heat
~Error

open door

close door

s3

s4

~Start
Close
~Heat
~Error

reset

s6

open door

done

~Start
Close
Heat
~Error

start cooking

start oven

Start
Close
~Heat
~Error

cook

s7
warmup

Start
Close
Heat
~Error

Figure 13: Updated microwave oven model using PU3.

7.2 Updating Andrew File System 1 Protocol
Andrew File System 1 (AFS1) (Wing & Vaziri-Farahani, 1995) cache coherence
protocol distributed file system. AFS1 applies validation-based technique
client-server protocol, described Wing Vaziri-Farahani (1995). protocol,
client two initial states: either files one files beliefs
validity. protocol starts client suspect files, client
may request file validation server. file invalid, client requests
new copy run terminates. file valid, protocol simply terminates.
145

fiZhang & Ding

AFS1 abstracted model one client, one server one file. state transition
diagrams single client server modules presented Figure 14. nodes
arcs labelled value state variable, belief , and, name received
message causes state transition, respectively. protocol run begins initial
state (one leftmost nodes) ends final state (one rightmost nodes).
Client

val

nofile
valid

val
suspect
inval

val

fetch

Server
none

invalid

valid

validate & valid-file

validate
& !valid-file

invalid

fetch

Figure 14: State transition diagrams AFS1.
clients belief file 4 possible values {nof ile, valid, invalid, suspect},
nofile means client cache empty; valid, client believes cached
file valid; invalid believes caches file valid; suspect, belief
validity file (it could valid invalid). servers belief file
cached client ranges {valid, invalid, none}, valid, server believes
file cached client valid; invalid, server believes valid; none,
server belief existence file clients cache validity.
set messages client may send server {f etch, validate}.
message f etch stands request file, validate message used client
determine validity file cache. set messages server may send
client {val, inval}. server sends val (inval) message indicate
client cached file valid (invalid). valid-f ile used client suspect
file cache requests validation server. update client
occurred server reflects fact nondeterministically setting value
valid-f ile 0; otherwise, 1 (the file cached client still valid). specification
property AFS1 is:
AG((Server.belief = valid) (Client.belief = valid)).

(1)

file system design, client belief leads server belief. specification
property deliberately chosen fail AFS1 (Wing & Vaziri-Farahani, 1995).
Thus, model updating, need pay much attention rationality
updated models. model updater update AFS1 model derive admissible
146

fiCTL Model Update System Modifications

models satisfy specification property (1). case study, focus
update procedure according functionality prototype.
Extracting Kripke model AFS1 NuSMV
noted that, CTL model update algorithm described Section 6,
complete Kripke model describing system behaviours one two input parameters (i.e.,
(M, s0 ) ), original AFS1 model checking process demonstrated (Wing &
Vaziri-Farahani, 1995) contain Kripke model. fact, provides SMV
model definitions (e.g., AFS1.smv) input SMV model checker. requires initial
extraction complete AFS1 Kripke model performing update it.
purpose, NuSMV (Cimatti et al., 1999) used derive Kripke model
loaded model (AFS1). output Kripke model shown Figure 15. method
also used extracting Kripke model.

#1: Client.out={0,fetch,validate} ;
#2: Client.belief={valid,invalid,suspect,nofile} ;
#3: Server.out={0,val,inval} ;
#4: Server.belief={none,valid,invalid} ;
#5: Server.validfile={true,false} ;
11

17

19

0,n,
0,n,


0,n,
0,n,
f

f,n,
0,n,
f

f,n,
0,n,


f,n,
v,v,


f,n,
v,v,
f

25

13

v,s,

12
1

22 f,i,

23

f,i,
v,v,


26

v,s,
i, i,
f

4

v,i,
0,i,


20

v,s,
0,n,


f

v,s,
i, i,


0,i,
f

f,v,
0,v,


0,s,
0,n,
f
14

6 0,n,

3

18

0,s,
0,n,


2

v,i,
0,i,
f

21

f,i,
0,i,


24

7

8 v,s,
v,v,

v,s,
v,v,


9

f

v,v,
0,v,


v,v,

10 0,v,
f

f,i,
v,v,
f

f,v,
0,v,
f

#1,#2,
#3,#4, shows order variables state;
15
#5
Initials values variables shown states.

5

0,v,
0,v,


0,v,
0,v,
f

16

Initial states: {11, 12, 13, 14}
False states: {19, 20, 23, 24, 7, 8}

Figure 15: CTL Kripke model AFS1.

AFS1 Kripke model (see Figure 15), 26 reachable states (out total 216
states) 52 transitions them. model contains 4 initial states {11, 12, 13, 14}
5 variables individual variable 2, 3 4 possible values. variables are: Client.out, (range {0, f etch, validate}); Client.belief (range {valid, invalid,
147

fiZhang & Ding

suspect, nof ile}); Server.out (range {0, val, inval}); Server.belief (range {none, valid,
invalid}); Server.valid-file (range {true, f alse}).
Update procedure
Model checking: CTL model update prototype, first check whether formula (1)
satisfied AFS1 model. is, need check whether reachable state contains
either Server.belief = valid Client.belief = valid. model updater identifies
set reachable states satisfy conditions {19, 20, 23, 24, 7, 8}.
call states false states.
Model update: Figure 15 reveals false state AFS1 different path.
Theorem 3 Section 4 UpdateAG Section 6, know update model
satisfy property, operations PU2 PU3 may applied false states
certain combinations. result, one admissible model depicted Figure 16.
model results update false state false path updated using
PU2. observe update, states 25, 26, 15 16 longer reachable
initial states 11 12, states 9 10 become unreachable initial states 13
14.

#1: Client.out={0,fetch,validate} ;
#2: Client.belief={valid,invalid,suspect,nofile} ;
#3: Server.out={0,val,inval} ;
#4: Server.belief={none,valid,invalid} ;
#5: Server.validfile={true,false} ;
0,n,
0,n,
f

0,n,
0,n,


11

f,n,
0,n,
f

f,n,
0,n,


17

19

f,n,
v,v,


f,n,
v,v,
f

25

f,v,
0,v,


13

3
12
1
18

v,s,
i, i,


4 v,s,

i, i,
f

22 f,i,

0,i,
f

23

f,i,
v,v,


26

0,s,
0,n,
f

v,s,
6 0,n,
f

v,i,
0,i,


20

0,s,
0,n,


2

v,i,
0,i,
f

21

f,i,
0,i,


24

v,s,
0,n,


7

9

0,v,
0,v,


5

8 v,s,
v,v,

v,s,
v,v,


f

v,v,

v,v,
0,v,


10 0,v,
f

f,i,
v,v,
f

f,v,
0,v,
f

#1,#2,
#3,#4, shows order variables state;
15
#5
Initials values variables shown states.

14

0,v,
0,v,
f

16

Figure 16: One admissible models AFS1 model update.

148

fiCTL Model Update System Modifications

know Figure 16 presents one possible updated model update AFS1 model. fact many possible admissible models. instance,
instead using PU2 operation, could also use PU2 PU3 different combinations produce many admissible models. total number admissible
models 64.

8. Optimizing Update Results
Section 7.2, observe often, CTL model update approach may derive
many possible admissible models really need. practice, would expect
solution CTL model update provides concrete information correct
underlying system specification. motivates us improve CTL model update
approach eliminate unnecessary admissible models narrow
update results.
Consider AFS1 update case again. model described Figure 16 satisfies
required property admissible, it, however, retain similar structure
original AFS1 model. implies update, significant change
system behaviour. admissible model may represent desirable correction
original system. One way reduce possibility impose notion maximal
reachable states minimal change principle, possible updated model
also retain many reachable states possible original model.
Given Kripke model = (S, R, L) 0 S, and, let = (M, s0 ), say
s0 reachable state M, path = (S, R, L) form = [s 0 , s1 , ]
s0 . RS(M) = RS(M, s0 ) used denote set reachable states
M. Now, propose refined CTL model update principle significantly reduce
number updated models. Let = (S, R, L) CTL Kripke model 0 S.
Suppose 0 = (S 0 , R0 , L0 ) (M 0 , s00 ) updated model obtained update
(M, s0 ) satisfy CTL formula. specify
RS(M) RS(M0 ) = {s | RS(M) RS(M0 ) L(s) = L0 (s)}.
States RS(M) RS(M0 ) common reachable states 0 , called unchanged reachable states. Note state name may reachable two
different models different truth assignments defined L L 0 respectively.
case, state common reachable state 0 .
Definition 7 (Minimal change maximal reachable states) Given CTL Kripke
model = (S, R, L), = (M, s0 ), s0 S, CTL formula , model
U pdate(M, ) called committed respect update satisfy ,
following conditions hold: (1) U pdate(M, ) = 0 = (M 0 , s00 ) admissible; and, (2)
model M00 = (M 00 , s000 ) M00 |= RS(M) RS(M0 )
RS(M) RS(M00 ).
Condition (2) Definition 7 ensures maximal set unchanged reachable states
retained updated model. prove next, amended CTL model update
approach based Definition 7 significantly increase overall computational
cost.
149

fiZhang & Ding

Lemma 1 Given CTL Kripke model = (S, R, L), = (M, 0 ), s0 S, CTL
formula , two models M0 = (M 0 , s00 ) M00 = (M 00 , s000 ) update (M, s0 )
satisfy , checking whether RS(M) RS(M0 ) RS(M) RS(M00 ) achieved
polynomial time.
Proof: given = (S, R, L), view directed graph G(M ) = (S, R),
set vertices R represents edges graph. Obviously, problem finding reachable states 0 finding reachable
vertices vertex s0 graph G(M ), obtained computing spanning tree
root s0 G(M ). well known spanning tree computed polynomial
time (Pettie & Ramachandran, 2002). Therefore, sets RS(M), RS(M 0 ), RS(M00 )
obtained polynomial time. Also, RS(M) RS(M0 ) RS(M) RS(M00 )
checked polynomial time. 2
Theorem 9 Given two CTL Kripke models = (S, R, L) 0 = (S 0 , R0 , L0 ),
s0 s00 0 , CTL formula , co-NP-complete decide whether (M 0 , s00 )
committed result update (M, 0 ) satisfy .
Proof: Since every committed result also admissible one, Theorem 5, hardness holds. membership, need check (1) whether (M 0 , s00 ) admissible; and,
(2) updated model 00 exist (M 00 , s000 ) |= RS(M) RS(M0 )
RS(M) RS(M00 ). Theorem 5, checking whether (M 0 , s00 ) co-NP. (2),
consider complement: updated model 00 exits (M 00 , s000 ) |=
RS(M) RS(M0 ) RS(M) RS(M00 ). Lemma 1, conclude
problem NP. Consequently, original problem checking (2) co-NP. 2
Section 4, many commonly used CTL formulas, also provide useful
semantic characterizations simplify process computing committed model
update. Here, present one result formula AF, propositional
formula. Given CTL model = (S, R, L) (M, 0 ) 6|= AF (s0 S). recall
= [s0 , ] (M, s0 ) valid path AF exists state > 0
L(s) |= ; otherwise, called false path AF.
Theorem 10 Let = (S, R, L) Kripke model, = (M, 0 ) 6|= AF,
s0 propositional formula. Let 0 = U pdate(M, AF) model obtained
following 1 2, M0 committed model. false path = [s 0 , s1 , ]:
1. false path 0 sharing common state , PU3 applied
state (s > s0 ) change ss truth assignment L 0 (s) |=
Dif f (L(s), L0 (s)) minimal; otherwise, operation applied shared
state sj (j > 0) maximum number false paths;
2. PU2 applied remove relation element (s 0 , s1 ), s1 also occurs another valid
path 0 , 0 = [s0 , s01 , , s0k , s1 , s0k+1 , . . .] exists s0i (1 k)
L(s0i ) |= .
150

fiCTL Model Update System Modifications

Proof: first prove Result 1. Consider false path = [s 0 , , si , si+1 , ]. Since
state satisfy , need (minimally) change one state ss truth assignment
along path L0 (s) satisfies (i.e., apply PU3 once). false
path shares states , apply PU3 state path .
case, one reachable state original model respect path changed
satisfy . Thus, updated model retains maximal set unchanged states.
Suppose false paths sharing common state . Without loss
generality, let 0 = [s0 , , s0i1 , si , s0i+1 , ] false path sharing common state
. applying PU3 state rather necessarily retain
maximal set unchanged reachable states, change state
si could made path 0 order make 0 valid. Since si sharing state
two paths 0 , implies updating two states PU3 retain maximal
set unchanged reachable states comparing change one state makes
0 valid.
consider general case. order retain maximal set unchanged
reachable states original model, consider states also
false paths. case, need apply PU3 operation one state j
shared maximal number false paths. way, changing j satisfy
also minimally change false paths valid time. Consequently,
retain maximal set unchanged reachable states original model.
prove Result 2. Let = [s0 , s1 , s2 , ] false path. According
condition, valid path 0 form 0 = [s0 , s01 , , s0k , s1 , s0k+1 , ],
s0i 0 (1 k), s0i |= . Note third path, formed 0 ,
00 = [s0 , s01 , , s0k , s1 , s2 , ] also valid. Applying PU2 relation element (s 0 , s1 )
simply eliminate false path model. condition, easy see
operation actually affect state reachability original model
valid path 00 connect s1 states path still reachable 0
path 00 . described Figure 17 follows. 2
optimization function UpdateAF described Section 6.1, Theorem 10 proposes
efficient way update CTL model satisfy formula AF guarantee update
model retains maximal set reachable states original model. Compared
(a) function UpdateAF , updates state path, case 1 Theorem 10
updates state shared maximum number false paths minimize changes
update protect unchanged reachable states. Compared (b) function UpdateAF,
could disconnect false path make disconnected part unreachable, case 2
Theorem 10 disconnects false path accompanied alternate path ensure
disconnected path still reachable via alternate path. theorem illustrates
principle optimization characterizations CTL formulas.
general, committed models computed revising previous CTL model
update algorithms particular emphasis identifying maximal reachable states.
example, using improved approach, obtain committed model AFS1 model
update (as illustrated Figure 18), rules model presented Figure 16.
shown using improved approach AFS1 model update, number
total possible updated models reduced 64 36.
151

fiZhang & Ding

s0

sk

s1
sk+1

Figure 17: s1 occurs another valid path {s0 , s01 , , s0k , s1 , s0k+1 , ].

#1: Client.out={0,fetch,validate} ;
#2: Client.belief={valid,invalid,suspect,nofile} ;
#3: Server.out={0,val,inval} ;
#4: Server.belief={none,valid,invalid} ;
#5: Server.validfile={true,false} ;
11

17

19

0,n,
0,n,


0,n,
0,n,
f

f,n,
0,n,


f,n,
0,n,
f

f,n,
v,v,


f,v,
v,v,
f

25

f,v,
0,v,


13

0,s,
0,n,
f

v,s,

6 0,n,

3
12
1
18

0,s,
0,n,


4 v,s,

i, i,
f

v,i,
0,i,


22 f,i,

0,i,
f

20

23

f,i,
v,v,


26

v,s,
0,n,


f

v,s,
i, i,


2

v,i,
0,i,
f

21

f,i,
0,i,


24

7

9

0,v,
0,v,


f

v,v,
0,v,


v,v,

10 0,v,
f

f,i,
v,v,
f

0,v,
0,v,
f

Figure 18: One committed models AFS1.

152

5

8 v,s,
v,n,

v,s,
v,v,


f,v,
0,v,
f

#1,#2,
#3,#4, shows order variables state;
15
#5
Initials values variables shown states.

14

16

fiCTL Model Update System Modifications

9. Concluding Remarks
paper, present formal approach update CTL models. specifying
primitive operations CTL Kripke models, defined minimal change criteria
CTL model update. semantic computational properties approach also
investigated detail. Based formalization, developed CTL model
update algorithm implemented system prototype perform CTL model update.
Two case studies used demonstrate important applications work.
number issues merit investigations. current research
focuses following two tasks:

- Partial CTL model update: current approach, model update performed
complete Kripke model. practice, may feasible system complex
large number states transition relations. One possible method handle
problem employ model checker extract partial useful information
use model update input. could counterexample partial Kripke
model containing components repaired (Buccafurri, Eiter, Gottlob, &
Leone, 2001; Clarke, Jha, Lu, & Veith, 2002; Groce & Visser, 2003; Rustan, Leino,
Millstein, & Saxe, 2005). way, update directly performed
counterexample partial model generate possible corrections. possible
develop unified prototype integrating model checking (e.g., SMV) model update.
- Combining maximal structure similarity minimal change: demonstrated
Section 8, principle minimal change maximal reachable states may significantly reduce number updated models. However, evident
maximal reachable states principle applied minimal change (see Definition
7). may improve principle defining unified analogue integrates
minimal change maximal structural similarity level. may
restrict number final updated models. unified principle may defined
based notion bisimulation Kripke models (Clarke, Grumberg, Jha, Liu,
& Veith, 2003). instance, two states preserved update
path two states original model, new definition
preserve path updated model well, updated model retains
maximal structural similarity respect original. Consider committed
model described Figure 18: since path state 21 state 26
original model (i.e., Figure 15), would require retention path 21
26 updated model. Accordingly, model displayed Figure 18
ruled final updated model.

Acknowledgments
research supported part Australian Research Council Discovery Grant
(DP0559592). authors thank three anonymous reviewers many valuable comments earlier version paper.
153

fiZhang & Ding

References
Amla, N., Du, X., Kuehlmann, A., Kurshan, R., & McMillan, K. (2005). analysis
sat-based model checking techniques industrial environment. Proceedings
Correct Hardware Design Verification Methods - 13th IFIP WG 10.5 Advanced
Research Working Conference (CHARME 2005), pp. 254268.
Baral, C., & Zhang, Y. (2005). Knowledge updates: semantics complexity issues. Artificial Intelligence, 164, 209243.
Berard, B., Bidoit, M., Finkel, A., Laroussinie, F., Petit, A., Petrucci, L., & Schnoebelen,
P. (2001). System Software Verification: Model-Checking Techniques Tools.
Springer-Verlag Berlin Heidelberg.
Boyer, M., & Sighireanu, M. (2003). Synthesis verification constraints pgm
protocol. Proceedings 12th International Symposium Formal Methods
Europe (FME03), pp. 264281. Springer Verlag.
Buccafurri, F., Eiter, T., Gottlob, G., & Leone, N. (1999). Enhancing model checking
verification ai techniques. Artificial Intelligence, 112, 57104.
Buccafurri, F., Eiter, T., Gottlob, G., & Leone, N. (2001). actl formulas linear
counterexamples. Journal Computer System Sciences, 62, 463515.
Chauhan, P., Clarke, E., Kukula, J., Sapra, S., Veith, H., & Wang, D. (2002). Automated
abstraction refinement model checking large state spaces using sat based conflict
analysis. Proceedings Formal Methods Computer Aided Design (FMCAD02),
pp. 3351.
Cimatti, A., Clarke, E., Giunchiglia, F., & Roveri, M. (1999). Nusmv: new symbolic
model verifier. Proceedings 11th International Conference Computer
Aided Verification, pp. 495499.
Clarke, E., Grumberg, O., Jha, S., Liu, Y., & Veith, H. (2003). Counterexample-guided
abstraction refinement symbolic model checking. Journal ACM, 50, 752794.
Clarke, E., Grumberg, O., & Peled, D. (1999). Model Checking. MIT Press.
Clarke, E., Jha, S., Lu, Y., & Veith, H. (2002). Tree-like counterexamples model checking.
Proceedings 17th Annual IEEE Symposium Logic Computer Science
(LICS02), pp. 1929.
Dennis, L., Monroy, R., & Nogueira, P. (2006). Proof-directed debugging repair.
Proceedings 7th Symposium Trends Functional Programming, pp. 131140.
Ding, Y., & Zhang, Y. (2005). logic approach ltl system modification. Proceedings
15th International Symposium Methodologies Intelligent Systems (ISMIS
2005), pp. 436444. Springer.
Ding, Y., & Zhang, Y. (2006). Ctl model update: Semantics, computations implementation. Proceedings 17th European Conference Artificial Intelligence (ECAI
2006), pp. 362366. IOS Press.
Eiter, T., & Gottlob, G. (1992). complexity propositional knowledge base revision,
updates, counterfactuals. Artificial Intelligence, 57, 227270.
154

fiCTL Model Update System Modifications

Gammie, P., & van der Meyden, R. (2004). Mck-model checking logic knowledge.
Proceedings 16th International Conference Computer Aided Verification
(CAV-2004), pp. 479483.
Gardenfors, P. (1988). Knowledge Flux: Modeling Dynamics Epistemic States.
MIT Press.
Groce, A., & Visser, W. (2003). went wrong: Explaining counterexamples. Proceedings SPIN Workshop Model Checking Software, pp. 121135.
Harris, H., & Ryan, M. (2002). Feature integration operation theory change.
Proceedings 15th European Conference Artificial Intelligence (ECAI-2002),
pp. 546550.
Harris, H., & Ryan, M. (2003). Theoretical foundations updating systems. Proceedings
18th IEEE International Conference Automated Software Engineering, pp.
291298.
Herzig, A., & Rifi, O. (1999). Propositional belief base update minimal change. Artificial
Intelligence, 115, 107138.
Holzmann, C. (2003). SPIN Model Checker: Primer Reference Manual. AddisonWesley Professional.
Huth, M., & Ryan, M. (2004). Logic Computer Science: Modelling Reasoning
Systems. 2nd edition, Cambridge University Press.
Katsuno, H., & Mendelzon, A. (1991). difference updating knowledge
base revising it. Proceedings International Conference Principles
Knowledge Representation Reasoning (KR91), pp. 387394.
McMillan, K., & Amla, N. (2002). logic implicit explicit belief. Automatic Abstraction without Counterexamples. Cadence Berkeley Labs, Cadence Design Systems.
Pettie, S., & Ramachandran, V. (2002). optimal minimum spanning tree algorithm.
Journal ACM, 49, 1634.
Rustan, K., Leino, M., Millstein, T., & Saxe, J. (2005). Generating error traces
verification-condition counterexamples. Science Computer Programming, 55, 209
226.
Stumptner, M., & Wotawa, F. (1996). model-based approach software debugging.
Proceedings 7th International Workshop Principles Diagnosis.
Wing, J., & Vaziri-Farahani, M. (1995). case study model checking software.
Proceedings 3rd ACM SIGSOFT Symposium Foundations Software
Engineering.
Winslett, M. (1988). Reasoning action using possible models approach. Proceedings AAAI-88, pp. 8993.
Winslett, M. (1990). Updating Logical Databases. Cambridge University Press.

155

fiJournal Artificial Intelligence Research 31 (2008) 33-82

Submitted 02/07; published 01/08

Planning Durative Actions Stochastic Domains
Mausam
Daniel S. Weld

MAUSAM @ CS . WASHINGTON . EDU
WELD @ CS . WASHINGTON . EDU

Dept Computer Science Engineering
Box 352350, University Washington
Seattle, WA 98195 USA

Abstract
Probabilistic planning problems typically modeled Markov Decision Process (MDP).
MDPs, otherwise expressive model, allow sequential, non-durative actions.
poses severe restrictions modeling solving real world planning problem. extend
MDP model incorporate 1) simultaneous action execution, 2) durative actions, 3) stochastic durations. develop several algorithms combat computational explosion introduced
features. key theoretical ideas used building algorithms modeling complex problem MDP extended state/action space, pruning irrelevant actions, sampling
relevant actions, using informed heuristics guide search, hybridizing different planners
achieve benefits both, approximating problem replanning. empirical evaluation
illuminates different merits using various algorithms, viz., optimality, empirical closeness
optimality, theoretical error bounds, speed.

1. Introduction
Recent progress achieved planning researchers yielded new algorithms relax, individually, many classical assumptions. example, successful temporal planners like SGPlan,
SAPA, etc. (Chen, Wah, & Hsu, 2006; & Kambhampati, 2003) able model actions take
time, probabilistic planners like GPT, LAO*, SPUDD, etc. (Bonet & Geffner, 2005; Hansen &
Zilberstein, 2001; Hoey, St-Aubin, Hu, & Boutilier, 1999) deal actions probabilistic
outcomes, etc. However, order apply automated planning many real-world domains must
eliminate larger groups assumptions concert. example, NASA researchers note
optimal control NASA Mars rover requires reasoning uncertain, concurrent, durative
actions mixture discrete metric fluents (Bresina, Dearden, Meuleau, Smith, & Washington, 2002). todays planners handle large problems deterministic concurrent
durative actions, MDPs provide clear framework non-concurrent durative actions
face uncertainty, researchers considered concurrent, uncertain, durative actions
focus paper.
example consider NASA Mars rovers, Spirit Oppurtunity. goal
gathering data different locations various instruments (color infrared cameras, microscopic imager, Mossbauer spectrometers etc.) transmitting data back Earth. Concurrent
actions essential since instruments turned on, warmed calibrated, rover
moving, using instruments transmitting data. Similarly, uncertainty must explicitly
confronted rovers movement, arm control actions cannot accurately predicted.
Furthermore, actions, e.g., moving locations setting experiments, take
time. fact, temporal durations uncertain rover might lose way
c
2008
AI Access Foundation. rights reserved.

fiM AUSAM & W ELD

take long time reach another location, etc. able solve planning problems encountered rover, planning framework needs explicitly model domain constructs
concurrency, actions uncertain outcomes uncertain durations.
paper present unified formalism models domain features together.
Concurrent Markov Decision Processes (CoMDPs) extend MDPs allowing multiple actions per
decision epoch. use CoMDPs base model planning problems involving concurrency.
Problems durative actions, concurrent probabilistic temporal planning (CPTP), formulated
CoMDPs extended state space. formulation also able incorporate uncertainty
durations form probabilistic distributions.
Solving planning problems poses several computational challenges: concurrency, extended durations, uncertainty durations lead explosive growth state space,
action space branching factor. develop two techniques, Pruned RTDP Sampled RTDP
address blowup concurrency. also develop DUR family algorithms handle
stochastic durations. algorithms explore different points running time vs. solutionquality tradeoff. different algorithms propose several speedup mechanisms 1) pruning provably sub-optimal actions Bellman backup, 2) intelligent sampling action
space, 3) admissible inadmissible heuristics computed solving non-concurrent problems, 4)
hybridizing two planners obtain hybridized planner finds good quality solution intermediate running times, 5) approximating stochastic durations mean values replanning, 6)
exploiting structure multi-modal duration distributions achieve higher quality approximations.
rest paper organized follows: section 2 discuss fundamentals
MDPs real-time dynamic programming (RTDP) solution method. Section 3 describe
model Concurrent MDPs. Section 4 investigates theoretical properties temporal
problems. Section 5 explains formulation CPTP problem deterministic durations.
algorithms extended case stochastic durations Section 6. section supported
empirical evaluation techniques presented section. Section 7 survey
related work area. conclude future directions research Sections 8 9.

2. Background
Planning problems probabilistic uncertainty often modeled using Markov Decision Processes (MDPs). Different research communities looked slightly different formulations
MDPs. versions typically differ objective functions (maximizing reward vs. minimizing
cost), horizons (finite, infinite, indefinite) action representations (DBN vs. parametrized action
schemata). formulations similar nature, algorithms solve
them. Though, methods proposed paper applicable variants models,
clarity explanation assume particular formulation, known stochastic shortest path
problem (Bertsekas, 1995).
define Markov decision process (M) tuple hS, A, Ap, Pr, C, G, s0
finite set discrete states. use factored MDPs, i.e., compactly represented
terms set state variables.
finite set actions.
34

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

State variables : x1 , x2 , x3 , x4 , p12
Action
Precondition
Effect
toggle-x1
p12
x1 x1
toggle-x2
p12
x2 x2
toggle-x3
true
x3 x3
change
toggle-x4
true
x4 x4
change
toggle-p12
true
p12 p12
Goal : x1 = 1, x2 = 1, x3 = 1, x4 = 1

Probability
1
1
0.9
0.1
0.9
0.1
1

Figure 1: Probabilistic STRIPS definition simple MDP potential parallelism
Ap defines applicability function. Ap : P(A), denotes set actions
applied given state (P represents power set).
Pr : [0, 1] transition function. write Pr(s0 |s, a) denote
probability arriving state s0 executing action state s.
C : <+ cost model. write C(s, a, s0 ) denote cost incurred
state s0 reached executing action state s.
G set absorbing goal states, i.e., process ends one states
reached.
s0 start state.
assume full observability, i.e., execution system complete access new state
action performed. seek find optimal, stationary policy i.e., function
: minimizes expected cost (over indefinite horizon) incurred reach goal
state. Note cost function, J: <, mapping states expected cost reaching goal
state defines policy follows:
J (s) = argmin

X

Pr(s0 |s, a) C(s, a, s0 ) + J(s0 )




(1)

aAp(s) s0

optimal policy derives optimal cost function, J , satisfies following pair
Bellman equations.
J (s) = 0, G else
J (s) = min

aAp(s)

X

Pr(s0 |s, a) C(s, a, s0 ) + J (s0 )




(2)

s0

example, Figure 1 defines simple MDP four state variables (x1 , . . . , x4 ) need
set using toggle actions. actions, e.g., toggle-x3 probabilistic.
Various algorithms developed solve MDPs. Value iteration dynamic programming approach optimal cost function (the solution equations 2) calculated
limit series approximations, considering increasingly long action sequences. Jn (s)
35

fiM AUSAM & W ELD

cost state iteration n, cost state next iteration calculated
process called Bellman backup follows:
Jn+1 (s) = min

aAp(s)

X

Pr(s0 |s, a) C(s, a, s0 ) + Jn (s0 )




(3)

s0

Value iteration terminates S, |Jn (s) Jn1 (s)| , termination guaranteed > 0. Furthermore, limit, sequence {Ji } guaranteed converge
optimal cost function, J , regardless initial values long goal reached every reachable state non-zero probability. Unfortunately, value iteration tends quite slow,
since explicitly updates every state, |S| exponential number domain features. One
optimization restricts search part state space reachable initial state s0 . Two algorithms exploiting reachability analysis LAO* (Hansen & Zilberstein, 2001) focus:
RTDP (Barto, Bradtke, & Singh, 1995).
RTDP, conceptually, lazy version value iteration states get updated proportion frequency visited repeated executions greedy policy.
RTDP trial path starting s0 , following greedy policy updating costs
states visited using Bellman backups; trial ends goal reached number
updates exceeds threshold. RTDP repeats trials convergence. Note common states
updated frequently, RTDP wastes time states unreachable, given current
policy. RTDPs strength ability quickly produce relatively good policy; however, complete
convergence (at every relevant state) slow less likely (but potentially important) states get
updated infrequently. Furthermore, RTDP guaranteed terminate. Labeled RTDP (LRTDP)
fixes problems clever labeling scheme focuses attention states value
function yet converged (Bonet & Geffner, 2003). Labeled RTDP guaranteed terminate,
guaranteed converge -approximation optimal cost function (for states reachable using optimal policy) initial cost function admissible, costs (C) positive
goal reachable reachable states non-zero probability.
MDPs powerful framework model stochastic planning domains. However, MDPs make
two unrealistic assumptions 1) actions need executed sequentially, 2) actions
instantaneous. Unfortunately, many real-world domains assumptions
unrealistic. example, concurrent actions essential Mars rover, since instruments
turned on, warmed calibrated rover moving, using instruments
transmitting data. Moreover, action durations non-zero stochastic rover might
lose way navigating may take long time reach destination; may make multiple
attempts finding accurate arm placement. paper successively relax two
assumptions build models algorithms scale spite additional complexities
imposed general models.

3. Concurrent Markov Decision Processes
define new model, Concurrent MDP (CoMDP), allows multiple actions executed
parallel. model different semi-MDPs generalized state semi-MDPs (Younes
& Simmons, 2004b) incorporate action durations explicitly. CoMDPs focus
adding concurrency MDP framework. input CoMDP slightly different
MDP hS, A, Apk , Prk , Ck , G, s0 i. new applicability function, probability model cost
36

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

(Apk , Prk Ck respectively) encode distinction allowing sequential executions
single actions versus simultaneous executions sets actions.
3.1 Model
set states (S), set actions (A), goals (G) start state (s0 ) follow input MDP.
difference lies fact instead executing one action time, may execute
multiple them. Let us define action combination, A, set one actions
executed parallel. action combination new unit operator available agent,
CoMDP takes following new inputs
Apk defines new applicability function. Apk : P(P(A)), denotes set action
combinations applied given state.
Prk : P(A) [0, 1] transition function. write Prk (s0 |s, A) denote
probability arriving state s0 executing action combination state s.
Ck : P(A) <+ cost model. write Ck (s, A, s0 ) denote cost incurred
state s0 reached executing action combination state s.
essence, CoMDP takes action combination unit operator instead single action.
approach convert CoMDP equivalent MDP (Mk ) specified
tuple hS, P(A), Apk , Prk , Ck , G, s0 solve using known MDP algorithms.
3.2 Case Study: CoMDP Probabilistic STRIPS
general CoMDP could require exponentially larger input MDP, since transition model, cost model applicability function defined terms action combinations
opposed actions. compact input representation general CoMDP interesting, open
research question future. work, consider special class compact CoMDP
one defined naturally via domain description similar probabilistic STRIPS
representation MDPs (Boutilier, Dean, & Hanks, 1999).
Given domain encoded probabilistic STRIPS compute safe set co-executable
actions. safe semantics, probabilistic dynamics gets defined consistent way
describe below.
3.2.1 PPLICABILITY F UNCTION
first discuss compute sets actions executed parallel since
actions may conflict other. adopt classical planning notion mutual exclusion (Blum & Furst, 1997) apply factored action representation probabilistic STRIPS.
Two distinct actions mutex (may executed concurrently) state one following occurs:
1. inconsistent preconditions
2. outcome one action conflicts outcome
3. precondition one action conflicts (possibly probabilistic) effect other.
37

fiM AUSAM & W ELD

4. effect one action possibly modifies feature upon another actions transition
function conditioned upon.
Additionally, action never mutex itself. essence, non-mutex actions interact effects executing sequence a1 ; a2 equals a2 ; a1 semantics
parallel executions clear.
Example: Continuing Figure 1, toggle-x1 , toggle-x3 toggle-x4 execute parallel
toggle-x1 toggle-x2 mutex conflicting preconditions. Similarly, toggle-x1
toggle-p12 mutex effect toggle-p12 interferes precondition toggle-x1 .
toggle-x4 outcomes depended toggle-x1 would mutex too, due point 4 above.
example, toggle-x4 toggle-x1 mutex effect toggle-x4 follows: togglex1 probability x4 x4 0.9 else 0.1. 2
applicability function defined set action-combinations, A, action
independently applicable actions pairwise non-mutex other.
Note pairwise concurrency sufficient ensure problem-free concurrency multiple
actions A. Formally Apk defined terms original definition Ap follows:
Apk (s) = {A A|a, a0 A, a, a0 Ap(s) mutex(a, a0 )}

(4)

3.2.2 RANSITION F UNCTION
Let = {a1 , a2 , . . . , ak } action combination applicable s. Since none actions
mutex, transition function may calculated choosing arbitrary order apply
follows:
Prk (s0 |s, A) =

X

...

X

Pr(s1 |s, a1 )Pr(s2 |s1 , a2 ) . . . Pr(s0 |sk1 , ak )

(5)

s1 ,s2 ,...sk

define applicability function transition function allowing consistent set actions executable concurrently, alternative definitions possible.
instance, one might willing allow executing two actions together probability
conflict small. conflict may defined two actions asserting contradictory effects
one negating precondition other. case, new state called failure could created system transitions state case conflict. transition may
computed reflect low probability transition failure state.
Although impose model conflict-free, techniques dont actually depend assumption explicitly extend general CoMDPs.
3.2.3 C OST MODEL
make small change probabilistic STRIPS representation. Instead defining single
cost (C) action, define additively sum resource time components follows:
Let durative cost, i.e., cost due time taken complete action.
Let r resource cost, i.e., cost resources used action.
38

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Assuming additivity think cost action C(s, a, s0 ) = t(s, a, s0 ) + r(s, a, s0 ),
sum time resource usage. Hence, cost model combination actions terms
components may defined as:
Ck (s, {a1 , a2 , ..., ak }, s0 ) =

k
X

r(s, ai , s0 ) + max {t(s, ai , s0 )}
i=1..k

i=1

(6)

example, Mars rover might incur lower cost preheats instrument changing
locations executes actions sequentially, total time reduced
energy consumed change.
3.3 Solving CoMDP MDP Algorithms
taken concurrent MDP allowed concurrency actions formulated equivalent MDP, Mk , extended action space. rest paper use term CoMDP
also refer equivalent MDP Mk .
3.3.1 B ELLMAN EQUATIONS
extend Equations 2 set equations representing solution CoMDP:
Jk (s) = 0, G else
Jk (s) = min

AApk (s)

X

n



Prk (s0 |s, A) Ck (s, A, s0 ) + Jk (s0 )

(7)

s0

equations traditional MDP, except instead considering single
actions backup state, need consider applicable action combinations. Thus,
small change must made traditional algorithms (e.g., value iteration, LAO*, Labeled RTDP).
However, since number action combinations worst-case exponential |A|, efficiently
solving CoMDP requires new techniques. Unfortunately, structure exploit easily,
since optimal action state classical MDP solution may even appear optimal
action combination associated concurrent MDP.
Theorem 1 actions optimal combination CoMDP (Mk ) may individually suboptimal MDP M.
Proof: domain Figure 1 let us additional action toggle-x34 toggles x3
x4 probability 0.5 toggles exactly one x3 x4 probability 0.25 each. Let
actions take one time unit each, therefore cost action combination one well.
Let start state x1 = 1, x2 = 1, x3 = 0, x4 = 0 p12 = 1. MDP optimal
action start state toggle-x34 . However, CoMDP Mk optimal combination
{toggle-x3 , toggle-x4 }. 2
3.4 Pruned Bellman Backups
Recall trial, Labeled RTDP performs Bellman backups order calculate costs
applicable actions (or case, action combinations) chooses best action (combination); describe two pruning techniques reduce number backups computed.
39

fiM AUSAM & W ELD

Let Qk (s, A) expected cost incurred executing action combination state
following greedy policy, i.e.
Qkn (s, A) =

X

n



Prk (s0 |s, A) Ck (s, A, s0 ) + Jkn1 (s0 )

(8)

s0

Bellman update thus rewritten as:
Jkn (s) =

min

AApk (s)

Qkn (s, A)

(9)

3.4.1 C OMBO -S KIPPING
Since number applicable action combinations exponential, would like prune
suboptimal combinations. following theorem imposes lower bound Qk (s, A) terms
costs Qk -values single actions. theorem costs actions may depend
action starting ending state, i.e., states s, s0 C(s, a, s0 ) = C(a).
Theorem 2 Let = {a1 , a2 , . . . , ak } action combination applicable state s.
CoMDP probabilistic STRIPS, costs dependent actions Qkn values
monotonically non-decreasing
Qk (s, A) max Qk (s, {ai }) + Ck (A)
i=1..k

k
X

!

Ck ({ai })

i=1

Proof:
Qkn (s, A) = Ck (A) +

X

Prk (s0 |s, A)Jkn1 (s0 )

(using Eqn. 8)

s0



X

Prk (s0 |s, A)Jkn1 (s0 ) = Qkn (s, A) Ck (A)

(10)

s0

Qkn (s, {a1 }) = Ck ({a1 }) +

X

Pr(s00 |s, a1 )Jkn1 (s00 )

s00

"

Ck ({a1 }) +

X

#

00

Pr(s |s, a1 ) Ck ({a2 }) +

s00

X

000

00

000

Pr(s |s , a2 )Jkn2 (s )

s000

(using Eqns. 8 9)
= Ck ({a1 }) + Ck ({a2 }) +

X

000

000

Prk (s |s, {a1 , a2 })Jkn2 (s )

s000


=

k
X
i=1
k
X

Ck ({ai }) +

X

Prk (s0 |s, A)Jknk (s0 )

(repeating actions A)

s0

Ck ({ai }) + [Qknk+1 (s, A) Ck (A)]

i=1

Replacing n n + k 1
40

(using Eqn. 10)

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Qkn (s, A) Qkn+k1 (s, {a1 }) + Ck (A)

k
X

!

Ck ({ai })

i=1

Qkn (s, {a1 }) + Ck (A)

k
X

!

Ck ({ai })

(monotonicity Qkn )

i=1



max Qkn (s, {ai }) + Ck (A)

i=1..k

k
X

!

Ck ({ai })

i=1

2

proof assumes equation 5 probabilistic STRIPS. following corollary
used prune suboptimal action combinations:
Corollary 3 Let dJkn (s)e upper bound Jkn (s).
dJkn (s)e < max Qkn (s, {ai }) + Ck (A)
i=1..k

k
X

!

Ck ({ai })

i=1

cannot optimal state iteration.
Proof: Let = {a1 , a2 , . . . , ak } optimal combination state iteration n. Then,
dJkn (s)e Jkn (s)
Jkn (s) = Qkn (s, )
Combining Theorem 2
dJkn (s)e maxi=1..k Qkn (s, {ai }) +

Ck (An )



k
X

!

Ck ({ai }) 2

i=1

Corollary 3 justifies pruning rule, combo-skipping, preserves optimality iteration
algorithm maintains cost function monotonicity. powerful Bellman-backup
based algorithms preserve monotonicity started admissible cost function. apply
combo-skipping, one must compute Qk (s, {a}) values single actions applicable
s. calculate dJkn (s)e one may use optimal combination state previous iteration
(Aopt ) compute Qkn (s, Aopt ). value gives upper bound value Jkn (s).
Example: Consider Figure 1. Let single action incur unit cost, let cost action combination be: Ck (A) = 0.5 + 0.5|A|. Let state = (1,1,0,0,1) represent ordered values x1 = 1, x2 =
1, x3 = 0, x4 = 0, p12 = 1. Suppose, nth iteration, cost function assigns values:
Jkn (s) = 1, Jkn (s1 =(1,0,0,0,1)) = 2, Jkn (s2 =(1,1,1,0,1)) = 1, Jkn (s3 =(1,1,0,1,1)) = 1. Let Aopt
state {toggle-x3 , toggle-x4 }. Now, Qkn+1 (s, {toggle-x2 }) = Ck ({toggle-x2 }) + Jkn (s1 ) = 3
Qkn+1 (s, Aopt ) = Ck (Aopt ) + 0.810 + 0.09Jkn (s2 ) + 0.09Jkn (s3 ) + 0.01Jkn (s) = 1.69.
apply Corollary 3 skip combination {toggle-x2 , toggle-x3 } iteration, since
using toggle-x2 a1 , dJkn+1 (s)e = Qkn+1 (s, Aopt ) = 1.69 3 + 1.5 - 2 = 2.5. 2
Experiments show combo-skipping yields considerable savings. Unfortunately, comboskipping weakness prunes combination single iteration. contrast,
second rule, combo-elimination, prunes irrelevant combinations altogether.
41

fiM AUSAM & W ELD

3.4.2 C OMBO -E LIMINATION
adapt action elimination theorem traditional MDPs (Bertsekas, 1995) prove similar
theorem CoMDPs.
Theorem 4 Let action combination applicable state s. Let bQk (s, A)c denote
lower bound Qk (s, A). bQk (s, A)c > dJk (s)e never optimal combination
state s.
Proof: CoMDP MDP new action space, original proof MDPs (Bertsekas,
1995) holds replacing action action combination. 2
order apply theorem pruning, one must able evaluate upper lower
bounds. using admissible cost function starting RTDP search (or value iteration,
LAO* etc.), current cost Jkn (s) guaranteed lower bound optimal cost; thus,
Qkn (s, A) also lower bound Qk (s, A). Thus, easy compute left hand side
inequality. calculate upper bound optimal Jk (s), one may solve MDP M,
i.e., traditional MDP forbids concurrency. much faster solving CoMDP,
yields upper bound cost, forbidding concurrency restricts policy use
strict subset legal action combinations. Notice combo-elimination used general
MDPs restricted CoMDPs probabilistic STRIPS.
Example: Continuing previous example, let A={toggle-x2 } Qkn+1 (s, A) = Ck (A) +
Jkn (s1 ) = 3 dJk (s)e = 2.222 (from solving MDP M). 3 > 2.222, eliminated
state remaining iterations. 2
Used fashion, combo-elimination requires additional overhead optimally solving
single-action MDP M. Since algorithms like RTDP exploit state-space reachability limit
computation relevant states, computation incrementally, new states visited
algorithm.
Combo-elimination also requires computation current value Qk (s, A) (for lower
bound Qk (s, A)); differs combo-skipping avoids computation. However,
combo-elimination prunes combination, never needs reconsidered. Thus,
tradeoff: one perform expensive computation, hoping long-term pruning, try
cheaper pruning rule fewer benefits? Since Q-value computation costly step, adopt
following heuristic: First, try combo-skipping; fails prune combination, attempt
combo-elimination; succeeds, never consider again. also tried implementing
heuristics, as: 1) combination skipped repeatedly, try prune altogether combo-elimination. 2) every state, try combo-elimination probability p. Neither
alternative performed significantly better, kept original (lower overhead) heuristic.
Since combo-skipping change step labeled RTDP combo-elimination removes provably sub-optimal combinations, pruned labeled RTDP maintains convergence, termination, optimality efficiency, used admissible heuristic.
3.5 Sampled Bellman Backups
Since fundamental challenge posed CoMDPs explosion action combinations, sampling promising method reduce number Bellman backups required per state.
describe variant RTDP, called sampled RTDP, performs backups random set
42

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

action combinations1 , choosing distribution favors combinations likely
optimal. generate distribution by:
1. using combinations previously discovered low Qk -values (recorded memoizing best combinations per state, iteration)
2. calculating Qk -values applicable single actions (using current cost function)
biasing sampling combinations choose ones contain actions low
Qk -values.

Algorithm 1 Sampled Bellman Backup(state, m)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

Function 2 SampleComb(state, i, l)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

//returns best combination found

list l = //a list applicable actions values
action
compute Qk (state, {action})
insert ha, 1/Qk (state, {action})i l
[1..m]
newcomb = SampleComb(state, i, l);
compute Qk (state, newcomb)
clear memoizedlist[state]
compute Qmin minimum Qk values computed line 7
store combinations Qk (state, A) = Qmin memoizedlist[state]
return first entry memoizedlist[state]

//returns ith combination sampled backup

size(memoizedlist[state])
return ith entry memoizedlist[state] //return combination memoized previous iteration
newcomb =
repeat
randomly sample action l proportional value
insert newcomb
remove actions mutex l
l empty
done = true
else |newcomb| == 1
done = false //sample least 2 actions per combination
else
|newcomb|
done = true prob. |newcomb|+1
done
return newcomb

approach exposes exploration / exploitation trade-off. Exploration, here, refers testing wide range action combinations improve understanding relative merit. Exploitation, hand, advocates performing backups combinations previously
shown best. manage tradeoff carefully maintaining distribution
combinations. First, memoize best combinations per state; always backed-up
1. similar action sampling approach also used context space shuttle scheduling reduce number
actions considered value function computation (Zhang & Dietterich, 1995).

43

fiM AUSAM & W ELD

Bellman update. combinations constructed incremental probabilistic process,
builds combination first randomly choosing initial action (weighted individual Qk -value), deciding whether add non-mutex action stop growing combination.
many implementations possible high level idea. tried several
found results similar them. Algorithm 1 describes implementation used
experiments. algorithm takes state total number combinations input
returns best combination obtained far. also memoizes best combinations
state memoizedlist. Function 2 helper function returns ith combination either
one best combinations memoized previous iteration new sampled combination.
Also notice line 10 Function 2. forces sampled combinations least size 2, since
individual actions already backed (line 3 Algo 1).
3.5.1 ERMINATION PTIMALITY
Since system consider every possible action combination, sampled RTDP guaranteed choose best combination execute state. result, even started
admissible heuristic, algorithm may assign Jkn (s) cost greater optimal Jk (s)
i.e., Jkn (s) values longer admissible. better combination chosen subsequent
iteration, Jkn+1 (s) might set lower value Jkn (s), thus sampled RTDP monotonic.
unfortunate, since admissibility monotonicity important properties required termination2 optimality labeled RTDP; indeed, sampled RTDP loses important theoretical
properties. good news extremely useful practice. experiments, sampled
RTDP usually terminates quickly, returns costs extremely close optimal.
3.5.2 MPROVING OLUTION Q UALITY
investigated several heuristics order improve quality solutions found
sampled RTDP. heuristics compensate errors due partial search lack admissibility.
Heuristic 1: Whenever sampled RTDP asserts convergence state, immediately
label converged (which would preclude exploration (Bonet & Geffner, 2003));
instead first run complete backup phase, using admissible combinations, rule
easy-to-detect inconsistencies.
Heuristic 2: Run sampled RTDP completion, use cost function produces, J (),
initial heuristic estimate, J0 (), subsequent run pruned RTDP. Usually,
heuristic, though inadmissible, highly informative. Hence, pruned RTDP terminates quite
quickly.
Heuristic 3: Run sampled RTDP pruned RTDP, Heuristic 2, except instead
using J () cost function directly initial estimate, scale linearly downward i.e.,
use J0 () := cJ () constant c (0, 1). guarantees hope
lies admissible side optimal. experience often case
c = 0.9, run pruned RTDP yields optimal policy quickly.
2. ensure termination implemented policy: number trials exceeds threshold, force monotonicity
cost function. achieve termination reduce quality solution.

44

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Experiments showed Heuristic 1 returns cost function close optimal. Adding
Heuristic 2 improves value moderately, combination Heuristics 1 3 returns
optimal solution experiments.
3.6 Experiments: Concurrent MDP
Concurrent MDP fundamental formulation, modeling concurrent actions general planning
domain. first compare various techniques solve CoMDPs, viz., pruned sampled RTDP.
following sections use techniques model problems durative actions.
tested algorithms problems three domains. first domain probabilistic
variant NASA Rover domain 2002 AIPS Planning Competition (Long & Fox, 2003),
multiple objects photographed various rocks tested resulting
data communicated back base station. Cameras need focused, arms need
positioned usage. Since rover multiple arms multiple cameras, domain
highly parallel. cost function includes resource time components, executing multiple actions parallel cheaper executing sequentially. generated problems
20-30 state variables 81,000 reachable states average number applicable
combinations per state, Avg(Ap(s)), measures amount concurrency problem,
2735.
also tested probabilistic version machineshop domain multiple subtasks (e.g.,
roll, shape, paint, polish etc.), need performed different objects using different
machines. Machines perform parallel, capable every task. tested
problems 26-28 state variables around 32,000 reachable states. Avg(Ap(s)) ranged
170 2640 various problems.
Finally, tested artificial domain similar one shown Figure 1 much
complex. domain, Boolean variables need toggled; however, toggling probabilistic nature. Moreover, certain pairs actions conflicting preconditions thus,
varying number mutex actions may control domains degree parallelism.
problems domain 19 state variables 32,000 reachable states, Avg(Ap(s))
1024 12287.
used Labeled RTDP, implemented GPT (Bonet & Geffner, 2005), base MDP
solver. implemented C++. implemented3 various algorithms, unpruned RTDP (U RTDP), pruned RTDP using combo skipping (Ps -RTDP), pruned RTDP using combo
skipping combo elimination (Pse -RTDP), sampled RTDP using Heuristic 1 (S-RTDP) sampled RTDP using Heuristics 1 3, value functions scaled 0.9 (S3 -RTDP). tested
algorithms number problem instantiations three domains, generated
varying number objects, degrees parallelism, distances goal. experiments
performed 2.8 GHz Pentium processor 2 GB RAM.
observe (Figure 2(a,b)) pruning significantly speeds algorithm. comparison Pse -RTDP S-RTDP S3 -RTDP (Figure 3(a,b)) shows sampling dramatic
speedup respect pruned versions. fact, pure sampling, S-RTDP, converges extremely
quickly, S3 -RTDP slightly slower. However, S3 -RTDP still much faster Pse -RTDP.
comparison qualities solutions produced S-RTDP S3 -RTDP w.r.t. optimal shown
Table 1. observe solutions produced S-RTDP always nearly optimal. Since
3. code may downloaded http://www.cs.washington.edu/ai/comdp/comdp.tgz

45

fiM AUSAM & W ELD

Comparison Pruned Unpruned RTDP Rover domain

Comparison Pruned Unpruned RTDP Factory domain
12000

y=x
Ps-RTDP
Pse-RTDP

25000

Times Pruned RTDP (in sec)

Times Pruned RTDP (in sec)

30000

20000
15000
10000
5000

y=x
Ps-RTDP
Pse-RTDP

10000
8000
6000
4000
2000

0

0
0

5000
10000 15000 20000 25000
Times Unpruned RTDP (in sec)

30000

0

2000
4000
6000
8000
10000
Times Unpruned RTDP (in sec)

12000

Figure 2: (a,b): Pruned vs. Unpruned RTDP Rover MachineShop domains respectively. Pruning
non-optimal combinations achieves significant speedups larger problems.

Comparison Pruned Sampled RTDP Rover domain

Comparison Pruned Sampled RTDP Factory domain
8000

y=x
S-RTDP
S3-RTDP

8000

Times Sampled RTDP (in sec)

Times Sampled RTDP (in sec)

10000

6000
4000
2000
0

y=x
S-RTDP
S3-RTDP

7000
6000
5000
4000
3000
2000
1000
0

0

2000 4000 6000 8000 1000012000140001600018000
Times Pruned RTDP (Pse-RTDP) - (in sec)

0

1000 2000 3000 4000 5000 6000 7000 8000
Times Pruned RTDP (Pse-RTDP) - (in sec)

Figure 3: (a,b): Sampled vs Pruned RTDP Rover MachineShop domains respectively. Random
sampling action combinations yields dramatic improvements running times.

46

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Comparison algorithms size problem Rover domain
30000

S-RTDP
S3-RTDP
Pse-RTDP
U-RTDP

25000

S-RTDP
S3-RTDP
Pse-RTDP
U-RTDP

15000

20000

Times (in sec)

Times (in sec)

Comparison different algorithms Artificial domain
20000

15000
10000

10000

5000
5000
0

0
0

5e+07

1e+08
1.5e+08
Reach(|S|)*Avg(Ap(s))

2e+08

2.5e+08

0

2000

4000

6000 8000
Avg(Ap(s))

10000 12000 14000

Figure 4: (a,b): Comparison different algorithms size problems Rover Artificial domains. problem size increases, gap sampled pruned approaches widens
considerably.

Results varying Number samples Rover Problem#4

300
250
200
150
100
50
0

Running times
Values start state

300

100

200

300 400 500 600 700
Concurrency : Avg(Ap(s))/|A|

800

900

12.8

250

12.79

200

12.78

150

12.77

100

12.76

50

J*(s0)

0
0

12.81

Value start state

350

S-RTDP/Pse-RTDP

Times Sampled RTDP (in sec)

Speedup : Sampled RTDP/Pruned RTDP

Speedup vs. Concurrency Artificial domain
350

10

20

30

40 50 60 70 80
Number samples

12.74
90 100

Figure 5: (a): Relative Speed vs. Concurrency Artificial domain. (b) : Variation quality solution
efficiency algorithm (with 95% confidence intervals) number samples Sampled RTDP one particular problem Rover domain. number samples increase,
quality solution approaches optimal time still remains better Pse -RTDP (which
takes 259 sec. problem).

47

fiM AUSAM & W ELD

Problem
Rover1
Rover2
Rover3
Rover4
Rover5
Rover6
Rover7
Artificial1
Artificial2
Artificial3
MachineShop1
MachineShop2
MachineShop3
MachineShop4
MachineShop5

J(s0 ) (S-RTDP)
10.7538
10.7535
11.0016
12.7490
7.3163
10.5063
12.9343
4.5137
6.3847
6.5583
15.0859
14.1414
16.3771
15.8588
9.0314

J (s0 ) (Optimal)
10.7535
10.7535
11.0016
12.7461
7.3163
10.5063
12.9246
4.5137
6.3847
6.5583
15.0338
14.0329
16.3412
15.8588
8.9844

Error
<0.01%
0
0
0.02%
0
0
0.08%
0
0
0
0.35%
0.77%
0.22%
0
0.56%

Table 1: Quality solutions produced Sampled RTDP
error S-RTDP small, scaling 0.9 makes admissible initial cost function pruned
RTDP; indeed, experiments, S3 -RTDP produced optimal solution.
Figure 4(a,b) demonstrates running times vary problem size. use product
number reachable states average number applicable action combinations per state
estimate size problem (the number reachable states artificial domains
same, hence x-axis Figure 4(b) Avg(Ap(s))). figures, verify
number applicable combinations plays major role running times concurrent MDP
algorithms. Figure 5(a), fix factors vary degree parallelism. observe
speedups obtained S-RTDP increase concurrency increases. encouraging result,
expect S-RTDP perform well large problems inolving high concurrency, even
approaches fail.
Figure 5(b), present another experiment vary number action combinations sampled backup. solution quality inferior sampling
combinations, quickly approaches optimal increasing number samples.
experiments sample 40 combinations per state.

4. Challenges Temporal Planning
CoMDP model powerful enough model concurrency actions, still assumes
action instantaneous. incorporate actual action durations modeling problem.
essential increase scope current models real world domains.
present model algorithms discuss several new theoretical challenges
imposed explicit action durations. Note results section apply wide range
planning problems:
regardless whether durations uncertain fixed
regardless whether effects stochastic deterministic.
48

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Actions uncertain duration modeled associating distribution (possibly conditioned
outcome stochastic effects) execution times. focus problems whose objective
achieve goal state minimizing total expected time (make-span), results extend
cost functions combine make-span resource usage. raises question
goal counts achieved. require that:
Assumption 1 executing actions terminate goal considered achieved.
Assumption 2 action, started, cannot terminated prematurely.
start asking question restricted set time points optimality
preserved even actions started points?
Definition 1 time point new action allowed start execution called decision
epoch. time point pivot either 0 time new effect might occur (e.g.,
end actions execution) new precondition may needed existing precondition may
longer needed. happening either 0 time effect actually occurs new
precondition definitely needed existing precondition longer needed.
Intuitively, happening point change world state action constraints actually
happens (e.g., new effect new precondition). execution crosses pivot (a possible
happening), information gained agents execution system (e.g., didnt effect
occur) may change direction future action choices. Clearly, action durations
deterministic, set pivots set happenings.
Example: Consider action whose durations follow uniform integer duration 1
10. started time 0 timepoints 0, 1, 2,. . ., 10 pivots. certain execution
finishes time 4 4 (and 0) happening (for execution). 2
Definition 2 action PDDL2.1 action (Fox & Long, 2003) following hold:
effects realized instantaneously either (at start) (at end), i.e., beginning
completion action (respectively).
preconditions may need hold instaneously start (at start), end (at
end) complete execution action (over all).

(:durative-action
:duration (= ?duration 4)
:condition (and (over P ) (at end Q))
:effect (at end Goal))
(:durative-action b
:duration (= ?duration 2)
:effect (and (at start Q) (at end (not P ))))

Figure 6: domain illustrate expressive action model may require arbitrary decision epochs
solution. example, b needs start 3 units execution reach Goal.

49

fiM AUSAM & W ELD

Theorem 5 PDDL2.1 domain restricting decision epochs pivots causes incompleteness
(i.e., problem may incorrectly deemed unsolvable).
Proof: Consider deterministic temporal planning domain Figure 6 uses PDDL2.1 notation
(Fox & Long, 2003). initial state P =true Q=false, way reach Goal
start time (e.g., 0), b timepoint open interval (t + 2, + 4). Clearly,
new information gained time points interval none pivot. Still,
required solving problem. 2
Intuitively, instantaneous start end effects two PDDL2.1 actions may require certain
relative alignment within achieve goal. alignment may force one action start
somewhere (possibly non-pivot point) midst others execution, thus requiring
intermediate decision epochs considered.
Temporal planners may classified one two architectures: constraint-posting
approaches times action execution gradually constrained planning (e.g.,
Zeno LPG, see Penberthy Weld, 1994; Gerevini Serina, 2002) extended statespace methods (e.g., TP4 SAPA, see Haslum Geffner, 2001; Kambhampati, 2001).
Theorem 5 holds architectures strong computational implications state-space
planners limiting attention subset decision epochs speed planners.
theorem also shows planners like SAPA Prottle (Little, Aberdeen, & Thiebaux, 2005)
incomplete. Fortunately, assumption restricts set decision epochs considerably.
Definition 3 action TGP-style action4 following hold:
effects realized unknown point action execution, thus used
action completed.
preconditions must hold beginning action.
preconditions (and features transition function conditioned) must
changed actions execution, except effect action itself.
Thus, two TGP-style actions may execute concurrently clobber others preconditions effects. case TGP-style actions set happenings nothing set
time points action terminates. TGP pivots set points action might
terminate. (Of course sets additionally include zero).
Theorem 6 actions TGP-style, set decision epochs may restricted pivots
without sacrificing completeness optimality.
Proof Sketch: contradiction. Suppose optimal policy satisfies theorem;
must exist path optimal policy one must start action, a, time even
though action could terminated t. Since planner hasnt gained
information t, case analysis (which requires actions TGP-style) shows one could
started earlier execution path without increasing make-span. detailed proof
discussed Appendix. 2
case deterministic durations, set happenings set pivots; hence
following corollary holds:
4. original TGP (Smith & Weld, 1999) considered deterministic actions fixed duration, use
phrase TGP-style general way, without restrictions.

50

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Probabillity: 0.5
a0

s0

a2

G

a1
Makespan: 3
Probability 0.5
a0

s0
a1

G

b0
Makespan: 9

0

2

4

6

8

Time

Figure 7: Pivot decision epochs necessary optimal planning face nonmonotonic continuation. domain, Goal achieved h{a0 , a1 }; a2 hb0 i; a0 duration 2
9; b0 mutex a1 . optimal policy starts a0 then, a0 finish
time 2, starts b0 (otherwise starts a1 ).

Corollary 7 actions TGP-style deterministic durations, set decision
epochs may restricted happenings without sacrificing completeness optimality.
planning uncertain durations may huge number pivots; useful
constrain range decision epochs.
Definition 4 action independent duration correlation probabilistic
effects duration.
Definition 5 action monotonic continuation expected time action termination
nonincreasing execution.
Actions without probabilistic effects, nature, independent duration. Actions monotonic continuations common, e.g. uniform, exponential, Gaussian, many duration distributions. However, actions bimodal multi-modal distributions dont monotonic continuations. example consider action uniform distribution [1,3].
action doesnt terminate 2, expected time completion calculated 2, 1.5,
1 times 0, 1, 2 respectively, monotonically decreasing. example
non-monotonic continuation see Figure 18.
Conjecture 8 actions TGP-style, independent duration monotonic continuation,
set decision epochs may restricted happenings without sacrificing completeness
optimality.
actions continuation nonmonotonic failure terminate increase expected
time remaining cause another sub-plan preferred (see Figure 7). Similarly, actions
duration isnt independent failure terminate changes probability eventual effects
may prompt new actions started.
exploiting theorems conjecture may significantly speed planning since
able limit number decision epochs needed decision-making. use theoretical
understanding models. First, simplicity, consider case TGP-style actions
deterministic durations. Section 6, relax restriction allowing stochastic durations,
unimodal well multimodal.
51

fiM AUSAM & W ELD

togglep12
p12 (effect)
conflict
p12 (Precondition)
togglex1
0

2

4

6

8

10

Figure 8: sample execution demonstrating conflict due interfering preconditions effects. (The
actions shaded disambiguate preconditions effects)

5. Temporal Planning Deterministic Durations
use abbreviation CPTP (short Concurrent Probabilistic Temporal Planning) refer
probabilistic planning problem durative actions. CPTP problem input model
similar CoMDPs except action costs, C(s, a, s0 ), replaced deterministic
durations, (a), i.e., input form hS, A, Pr, , G, s0 i. study objective minimizing expected time (make-span) reaching goal. rest paper make
following assumptions:
Assumption 3 action durations integer-valued.
assumption negligible effect expressiveness one convert problem
rational durations one satisfies Assumption 3 scaling durations g.c.d.
denominators. case irrational durations, one always find arbitrarily close approximation original problem approximating irrational durations rational numbers.
reasons discussed previous section adopt TGP temporal action model Smith
Weld (1999), rather complex PDDL2.1 (Fox & Long, 2003). Specifically:
Assumption 4 actions follow TGP model.
restrictions consistent previous definition concurrency. Specifically,
mutex definitions (of CoMDPs probabilistic STRIPS) hold required assumptions. illustration, consider Figure 8. describes situation two actions
interfering preconditions effects executed concurrently. see not, suppose
initially p12 false two actions toggle-x1 toggle-p12 started time 2 4, respectively. p12 precondition toggle-x1 , whose duration 5, needs remain false
time 7. toggle-p12 may produce effects anytime 4 9, may conflict
preconditions executing action. Hence, forbid concurrent execution
toggle-x1 toggle-p12 ensure completely predictable outcome distribution.
definition concurrency, dynamics model remains consistent
Equation 5. Thus techniques developed CoMDPs derived probabilistic STRIPS actions
may used.
52

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Aligned Epoch policy execution
(takes 9 units)
togglex1
t3

f

f

f

f

t3 t3 t3 t3

0

5



10
time

togglex1
f

f

f

f

t3 t3 t3 t3 t3



Interwoven Epoch policy execution
(takes 5 units)

Figure 9: Comparison times taken sample execution interwoven-epoch policy alignedepoch policy. trajectories toggle-x3 (t3) action fails four times succeeding.
aligned policy must wait actions complete starting more, takes
time interwoven policy, start actions middle.

5.1 Formulation CoMDP
model CPTP problem CoMDP, thus MDP, one way. list
two prominent formulations below. first formulation, aligned epoch CoMDP models
problem approximately solves quickly. second formulation, interleaved epochs models
problem exactly results larger state space hence takes longer solve using existing
techniques. subsequent subsections explore ways speed policy construction
interleaved epoch formulation.
5.1.1 LIGNED E POCH EARCH PACE
simple way formulate CPTP model standard CoMDP probabilistic STRIPS,
action costs set durations cost combination maximum
duration constituent actions (as Equation 6). formulation introduces substantial
approximation CPTP problem. true deterministic domains too, illustrate
using example involving stochastic effects. Figure 9 compares trajectories
toggle-x3 (t3) actions fails four consecutive times succeeding. figure, f
denote failure success uncertain actions, respectively. vertical dashed lines represent
time-points action started.
Consider actual executions resulting policies. aligned-epoch case (Figure 9
top), combination actions started state, next decision taken
effects actions observed (hence name aligned-epochs). contrast, Figure 9
bottom shows decision epoch optimal execution CPTP problem, many actions
may midway execution. explicitly take account actions
remaining execution times making subsequent decision. Thus, actual state space
CPTP decision making substantially different simple aligned-epoch model.
Note due Corollary 7 sufficient consider new decision epoch happening,
i.e., time-point one actions complete. Thus, using Assumption 3 infer
decision epochs discrete (integer). course, optimal policies property.
53

fiM AUSAM & W ELD

State variables : x1 , x2 , x3 , x4 , p12
Action
(a) Precondition
toggle-x1
5
p12
toggle-x2
5
p12
toggle-x3
1
true

Effect
x1 x1
x2 x2
x3 x3
change
toggle-x4
1
true
x4 x4
change
toggle-p12
5
true
p12 p12
Goal : x1 = 1, x2 = 1, x3 = 1, x4 = 1

Probability
1
1
0.9
0.1
0.9
0.1
1

Figure 10: domain Example 1 extended action durations.
easy see exists least one optimal policy action begins
happening. Hence search space reduces considerably.
5.1.2 NTERWOVEN E POCH EARCH PACE
adapt search space representation Haslum Geffner (2001), similar
research (Bacchus & Ady, 2001; & Kambhampati, 2001). original state space
Section 2 augmented including set actions currently executing times passed
since started. Formally, let new interwoven state5 - ordered pair hX,
where:
XS
= {(a, )|a A, 0 < (a)}
X represents values state variables (i.e. X state original state space)
denotes set ongoing actions times
passed since start . Thus
N
overall interwoven-epoch search space - = aA {a} Z(a) , Z(a) represents
N
set {0, 1, . . . , (a) 1}
denotes Cartesian product multiple sets.
Also define set actions already execution. words, projection
ignoring execution times progress:
= {a|(a, ) = hX, i}
Example: Continuing example domain Figure 10, suppose state s1 state
variables false, suppose action toggle-x1 started 3 units ago current time.
state would represented hX1 , Y1 X1 =(F, F, F, F, F ) Y1 ={(toggle-x1 ,3)} (the five
state variables listed order: x1 , x2 , x3 , x4 p12 ). set As1 would {toggle-x1 }.
allow possibility simply waiting action complete execution, is, deciding decision epoch start additional action, augment set no-op action,
applicable states = hX, 6= (i.e. states action still
executed). state s, no-op action mutex non-executing actions, i.e.,
\ . words, decision epoch either no-op started combination
5. use subscript - denote interwoven state space (S - ), value function (J - ), etc..

54

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

involving no-op. define no-op variable duration6 equal time another
already executing action completes (next (s, A) defined below).
interwoven applicability set defined as:
(

Ap - (s) =

Apk (X) = else
{noop}{A|AAs Apk (X) AAs = }

Transition Function: also need define probability transition function, Pr - ,
interwoven state space. decision epoch let agent state = (X, ). Suppose
agent decides execute action combination A. Define Ynew set similar
consisting actions starting; formally Ynew = {(a, (a))|a A}. system,
next decision epoch next time executing action terminates. Let us call time
next (s, A). Notice next (s, A) depends executing newly started actions. Formally,
next (s, A) =

min

(a,)Y Ynew

(a)

Moreover, multiple actions may complete simultaneously. Define Anext (s, A)
set actions complete exactly next (s, A) timesteps. -component state
decision epoch next (s, A) time
Ynext (s, A) = {(a, + next (s, A))|(a, ) Ynew , (a) > next (s, A)}
Let s=hX, let s0 =hX 0 , 0 i. transition function CPTP defined as:
Prk (X 0 |X, Anext (s, A)) 0= Ynext (s, A)
0
otherwise

(
0

Pr - (s |s, A)=

words, executing action combination state = hX, takes agent
decision epoch next (s, A) ahead time, specifically first time combination
Anext (s, A) completes. lets us calculate Ynext (s, A): new set actions still executing
times elapsed. Also, TGP-style actions, probability distribution different
state variables modified independently. Thus probability transition function due CoMDP
probabilistic STRIPS used decide new distribution state variables,
combination Anext (s, A) taken state X.
Example: Continuing previous example, let agent state s1 execute action combination = {toggle-x4 }. next (s1 , A) = 1, since toggle-x4 finish first. Thus,
Anext (s1 , A)= {toggle-x4 }. Ynext (s1 , A) = {(toggle-x1 ,4)}. Hence, probability distribution
states executing combination state s1
((F, F, F, T, F ), Ynext (s1 , A)) probability = 0.9
((F, F, F, F, F ), Ynext (s1 , A)) probability = 0.1
6. precise definition model create multiple no-opt actions different constant durations no-opt
applicable interwoven state one = next (s, A).

55

fiM AUSAM & W ELD

Start Goal States: interwoven space, start state hs0 , new set goal
states G - = {hX, i|X G}.
redefining start goal states, applicability function, probability transition
function, finished modeling CPTP problem CoMDP interwoven state space.
use techniques CoMDPs (and MDPs well) solve problem. particular,
use Bellman equations described below.
Bellman Equations: set equations solution CPTP problem written as:
J - (s) = 0, G - else



(11)








next (s, A) + Pr - (s0 |s, A)J - (s0 )
J - (s) = min



AAp - (s)


s0



X

use DURsamp refer sampled RTDP algorithm search space. main
bottleneck naively inheriting algorithms like DURsamp huge size interwoven state
space. worst case (when actions executed concurrently) size state space
Q
|S| ( aA (a)). get bound observing action a, (a) number
possibilities: either executing remaining times 1, 2, . . . , (a) 1.
Thus need reduce abstract/aggregate state space order make problem
tractable. present several heuristics used speed search.
5.2 Heuristics
present admissible inadmissible heuristics used initial cost
function DURsamp algorithm. first heuristic (maximum concurrency) solves underlying MDP thus quite efficient compute. second heuristic (average concurrency)
inadmissible, tends informed maximum concurrency heuristic.
5.2.1 AXIMUM C ONCURRENCY H EURISTIC
prove optimal expected cost traditional (serial) MDP divided maximum
number actions executed parallel lower bound expected make-span
reaching goal CPTP problem. Let J(X) denote value state X traditional
MDP costs action equal duration. Let Q(X, A) denote expected cost reach
goal initially actions combination executed greedy serial policy followed
P
thereafter. Formally, Q(X, A) = X 0 Prk (X 0 |X, A)J(X 0 ). Let J - (s) value equivalent
CPTP problem interwoven-epoch state space. Let concurrency state
maximum number actions could executed state concurrently. define maximum
concurrency domain (c) maximum number actions concurrently executed
world state domain. following theorem used provide admissible
heuristic CPTP problems.
Theorem 9 Let = hX, i,
J - (s)

J - (s)


J (X)
=
c
Q (X, )
6=
c
56

(12)

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Proof Sketch: Consider trajectory make-span L (from state = hX, goal state)
CPTP problem using optimal policy. make concurrent actions sequential executing
chronological order started. concurrent actions non-interacting,
outcomes stage similar probabilities. maximum make-span sequential
trajectory cL (assuming c actions executing points semi-MDP trajectory). Hence
J(X) using (possibly non-stationary) policy would cJ - (s). Thus J (X) cJ - (s).


second inequality proven similar way. 2
cases bounds tight. example, consider deterministic planning
problem optimal plan concurrently executing c actions unit duration (makespan = 1). sequential version, actions would taken sequentially (make-span =
c).
Following theorem, maximum concurrency (MC) heuristic state = hX,
defined follows:
Q (X, )
J (X)
else HM C (s) =
= HM C (s) =
c
c
maximum concurrency c calculated static analysis domain onetime expense. complete heuristic function evaluated solving MDP states.
However, many states may never visited. implementation, calculation
demand, states visited, starting MDP current state. RTDP run
seeded previous value function, thus computation thrown away
relevant part state space explored. refer DURsamp initiated MC heuristic
DURMC
samp .
5.2.2 AVERAGE C ONCURRENCY H EURISTIC
Instead using maximum concurrency c heuristic use average concurrency
domain (ca ) get average concurrency (AC) heuristic. call resulting algorithm
DURAC
samp . AC heuristic admissible, experiments typically informed
heuristic. Moreover, case actions duration, AC heuristic equals
MC heuristic.
5.3 Hybridized Algorithm
present approximate method solve CPTP problems. many kinds
possible approximation methods, technique exploits intuition best focus computation probable branches current policys reachable space. danger
approach chance that, execution, agent might end unlikely branch,
poorly explored; indeed might blunder dead-end case. undesirable, apparently attractive policy might true expected make-span infinity.
Since, wish avoid dead-ends, explore desirable notion propriety.
Definition 6 Propriety: policy proper state guaranteed lead, eventually, goal
state (i.e., avoids dead-ends cycles) (Barto et al., 1995). define planning algorithm
proper always produces proper policy (when one exists) initial state.
describe anytime approximation algorithm, quickly generates proper policy
uses additional available computation time improve policy, focusing
likely trajectories.
57

fiM AUSAM & W ELD

5.3.1 H YBRIDIZED P LANNER
algorithm, DURhyb , created hybridizing two policy creation algorithms. Indeed,
novel notion hybridization general powerful, applying many MDP-like problems; however, paper focus use hybridization CPTP. Hybridization uses
anytime algorithm like RTDP create policy frequently visited states, uses faster (and
presumably suboptimal) algorithm infrequent states.
case CPTP, algorithm hybridizes RTDP algorithms interwoven-epoch
aligned-epoch models. aligned-epochs, RTDP converges relatively quickly, state
space smaller, resulting policy suboptimal CPTP problem, policy
waits currently executing actions terminate starting new actions. contrast,
RTDP interwoven-epochs generates optimal policy, takes much longer converge.
insight run RTDP interwoven space long enough generate policy
good common states, stop well converges every state. Then, ensure
rarely explored states proper policy, substitute aligned policy, returning hybridized
policy.
Algorithm 3 Hybridized Algorithm DURhyb (r, k, m)
1: -

2:
initialize J - (s) admissible heuristic
3: repeat
4:
perform RTDP trials
5:
compute hybridized policy (hyb ) using interwoven-epoch policy k-familiar states aligned-

epoch policy otherwise
clean hyb removing dead-ends cycles
J - hs0 , evaluation hyb start state


J - (hs0 ,i)J - (hs0 ,i)


8:
<r
J - (hs0 ,i)

9: return hybridized policy hyb

6:
7:

Thus key question decide states well explored not.
define familiarity state number times visited previous RTDP
trials. reachable state whose familiarity less constant, k, aligned policy created
it. Furthermore, dead-end state reached using greedy interwoven policy, create
aligned policy immediate precursors state. cycle detected7 , compute
aligned policy states part cycle.
yet said hybridized algorithm terminates. Use RTDP helps us defining
simple termination condition parameter varied achieve desired
closeness optimality well. intuition simple. Consider first, optimal labeled RTDP.
starts admissible heuristic guarantees value start state, J - (hs0 , i),
remains admissible (thus less equal optimal). contrast, hybridized policys makespan always longer equal optimal. Thus time progresses, values approach
optimal make-span opposite sides. Whenever two values within optimality ratio (r),
know algorithm found solution, close optimal.
7. implementation cycles detected using simulation.

58

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Finally, evaluation hybridized policy done using simulation, perform
fixed number RTDP trials. Algorithm 3 summarizes details algorithm. One see
combined policy proper two reasons: 1) policy state aligned
policy, proper RTDP aligned-epoch model run convergence,
2) rest states explicitly ensured cycles dead-ends.
5.4 Experiments: Planning Deterministic Durations
Continuing Section 3.6, set experiments evaluate various techniques
solving problems involving explicit deterministic durations. compare computation time
solution quality five methods: interwoven Sampled RTDP heuristic (DURsamp ),
AC
maximum concurrency (DURMC
samp ), average concurrency (DURsamp ) heuristics, hybridized
algorithm (DURhyb ) Sampled RTDP aligned-epoch model (DURAE ). test
Rover, MachineShop Aritificial domains. also use Artificial domain see relative
performance techniques varies amount concurrency domain.
5.4.1 E XPERIMENTAL ETUP
modify domains used Section 3.6 additionally including action durations. NASA
Rover MachineShop domains, generate problems 17-26 state variables 12-18 actions, whose duration range 1 20. problems 15,000-700,000 reachable states interwoven-epoch state space, - .
use Artificial domain control experiments study effect degree parallelism.
problems domain 14 state variables 17,000-40,000 reachable states
durations actions 1 3.
use implementation Sampled RTDP8 implement heuristics: maximum concurrency (HM C ), average concurrency (HAC ), initialization value function. calculate
heuristics demand states visited, instead computing complete heuristic
whole state space once. also implement hybridized algorithm initial value
function set HM C heuristic. parameters r, k, kept 0.05, 100 500,
respectively. test algorithms number problem instances three
domains, generate varying number objects, degrees parallelism, durations
actions distances goal.
5.4.2 C OMPARISON RUNNING IMES
Figures 11(a, b) 12(a) show variations running times algorithms different
problems Rover, Machineshop Artificial domains, respectively. first three bars represent
base Sampled RTDP without heuristic, HM C , HAC , respectively. fourth
bar represents hybridized algorithm (using HM C heuristic) fifth bar computation
aligned-epoch Sampled RTDP costs set maximum action duration. white
region fourth bar represents time taken aligned-epoch RTDP computations
hybridized algorithm. error bars represent 95% confidence intervals running times. Note
plots log scale.
8. Note policies returned DURsamp guaranteed optimal. Thus implemented algorithms
approximate. replace DURsamp pruned RTDP (DURprun ) optimality desired.

59

fiM AUSAM & W ELD

Rover16

Mach11

10^3

10^2

10^1

Mach12

Mach13

Mach14

Mach15

Mach16

0

AC
H
AE

Rover15

0

AC
H
AE

Rover14

Time sec (on log scale)

10^3

10^2

10^1

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

10^0

10^0

0

AC
H
AE

Time sec (on log scale)

Rover13

0

AC
H
AE

Rover12

Rover11

0

AC
H
AE

10^4

10^4

Figure 11: (a,b): Running times (on log scale) Rover Machineshop domain, respectively.
problem five bars represent times taken algorithms: DURsamp (0), DURMC
samp
(AE), DURAC
(AC),
DUR
(H),

DUR
(AE),
respectively.

white
bar

DUR
hyb
AE
hyb
samp
denotes portion time taken aligned-epoch RTDP.

Algos
DURMC
samp
DURAC
samp
DURhyb
DURAE

Speedup compared DURsamp
Rover
Machineshop Artificial Average
3.016764
1.545418
1.071645 1.877942
3.585993
2.173809
1.950643 2.570148
10.53418
2.154863
16.53159 9.74021
135.2841
16.42708
241.8623 131.1911

Table 2: ratio time taken - S-RTDP heuristics algorithm.
heuristics produce 2-3 times speedups. hybridized algo produces 10x speedup. Aligned
epoch search produces 100x speedup, sacrifices solution quality.

notice DURAE solves problems extremely quickly; natural since alignedepoch space much smaller. Use HM C HAC always speeds search - model.
Comparing heuristics amongst themselves, find average concurrency heuristic mostly
performs faster maximum concurrency presumably HAC informed heuristic practice, although cost inadmissible. find couple cases HAC
doesnt perform better; could focusing search incorrect region, given
inadmissible nature.
Rover domain, hybridized algorithm performs fastest. fact, speedups
dramatic compared methods. domains, results comparable small
problems. However, large problems two domains, hybridized outperforms others
huge margin. fact largest problem Artificial domain, none heuristics able
converge (within day) DURhyb DURAE converge solution.
60

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

10^4

Art12

Art13

Art14

Art15

0

AC
H
AE

Art11

0

AC
H
AE

1.6
Art11(68) Art12(77) Art13(81) Art14(107) Art15(224) Art16(383) Art17(1023)

Art16

Art17

Ratio make-span optimal

Time sec (on log scale)

1.5
10^3

10^2

10^1

1.4
1.3
1.2
1.1
1
0.9

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0.8

10^0

Figure 12: (a,b): Comparison different algorithms (running times solution quality respectively)
Artificial domain. degree parallelism increases problems become harder;
largest problem solved DURhyb DURAE .

Table 2 shows speedups obtained various algorithms compared basic DURsamp .
Rover Artificial domains speedups obtained DURhyb DURAE much
prominent Machineshop domain. Averaging domains, H produces 10x speedup
AE produces 100x speedup.
5.4.3 C OMPARISON OLUTION Q UALITY
Figures 13(a, b) 12(b) show quality policies obtained five methods
domains. measure quality simulating generated policy across multiple trials,
reporting average time taken reach goal. plot ratio so-measured expected
make-span optimal expected make-span9 . Table 3 presents solution qualities method,
averaged problems domain. note aligned-epoch policies usually yield
significantly longer make-spans (e.g., 25% longer); thus one must make quality sacrifice
speedy policy construction. contrast, hybridized algorithm extorts small sacrifice
quality exchange speed.
5.4.4 VARIATION C ONCURRENCY
Figure 12(a) represents attempt see relative performance algorithms changed
increasing concurrency. Along top figure, problem names, numbers brackets;
list average number applicable combinations MDP state, AvgsS - |Ap(s)|,

range 68 1023 concurrent actions. Note difficult problems lot parallelism, DURsamp slows dramatically, regardless heuristic. contrast, DURhyb still able
quickly produce policy, almost loss quality (Figure 12(b)).
9. large problems optimal algorithm converge. those, take optimal, best policy found
runs.

61

fiM AUSAM & W ELD

Rover16

1.7 Mach11
Ratio make-span optimal

1.4
1.3
1.2
1.1
1

Mach16

1.2
1.1
1

0

AC
H
AE

0

AC
H
AE

Mach15

1.3

0.8
0

AC
H
AE

Mach14

1.4

0.8
0

AC
H
AE

Mach13

1.5

0.9

0

AC
H
AE

Mach12

1.6

0.9

0

AC
H
AE

Ratio make-span optimal

1.5

0

AC
H
AE

Rover15

0

AC
H
AE

Rover14

0

AC
H
AE

Rover13

0

AC
H
AE

Rover12

0

AC
H
AE

Rover11

0

AC
H
AE

1.8

1.6

Figure 13: (a,b): Comparison make-spans solution found optimal(plotted 1 yaxes) Rover Machineshop domains, respectively. algorithms except DURAE produce
solutions quite close optimal.

Algos
DURsamp
DURMC
samp
DURAC
samp
DURhyb
DURAE

Rover
1.059625
1.018405
1.017141
1.059349
1.257205

Average Quality
Machineshop Artificial
1.065078
1.042561
1.062564
1.013465
1.046391
1.020523
1.075534
1.059201
1.244862
1.254407

Average
1.055704
1.031478
1.028019
1.064691
1.252158

Table 3: Overall solution quality produced algorithms. Note algorithms except DURAE produce policies whose quality quite close optimal. average DURAE produces make-spans
125% optimal.

6. Optimal Planning Uncertain Durations
extend techniques previous section case action durations deterministic. before, consider TGP-style actions discrete temporal model. assume
independent durations, monotonic continuations, Section 6.3 relaxes latter, extending
algorithms handle multimodal duration distributions. aim minimize
expected time required reach goal.
6.1 Formulating CoMDP
formulate planning problem CoMDP similar Section 5.1.
parameters CoMDP used directly work deterministic durations, need
recompute transition function.
62

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

State Space: aligned epoch state space well interwoven epoch space, defined
Section 5.1 adequate model planning problem. determine size interwoven
space, replace duration action max duration. Let (a) denote maximum
time within action complete. overall interwoven-epoch search space - =




{a} ZM (a) , ZM (a) represents set {0, 1, . . . , (a) 1}
denotes
Cartesian product multiple sets.
Action Space: state may apply combination actions applicability function
reflecting fact combination actions safe w.r.t (and w.r.t. already executing
actions case interwoven space) previous sections. previous state space
action space work well problem, transition function definition needs change, since
need take account uncertainty durations.
Transition Function: Uncertain durations require significant changes probability transition
function (Pr - ) interwoven space definitions Section 5.1.2. Since assumptions justify Conjecture 8, need consider happenings choosing decision epochs.
N

N

aA

Algorithm 4 ComputeTransitionFunc(s=hX, i,A)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

{(a, 0)}
mintime min(a,)Y minimum remaining time
maxtime min(a,)Y maximum remaining time
integer [mintime, maxtime]
set actions could possibly terminate
non-empty subsets Asubt
pc (prob. exactly Asubt terminates (see Equation 13).
W {(Xt , pw ) | Xt world state; pw probability Asubt terminates yielding Xt }.
(Xt , pw ) W
Yt {(a, + t) | (a, ) Y,
/ Asubt }
insert (hXt , Yt i, pw pc ) output
return output

computation transition function described Algorithm 4. Although next decision
epoch determined happening, still need consider pivots next state calculations
potential happenings. mintime minimum time executing action could
terminate, maxtime minimum time guaranteed least one action
terminate. times mintime maxtime compute possible combinations
could terminate resulting next interwoven state. probability, pc , (line 7) may
computed using following formula:
pc =



(prob. terminates + t|a hasn0 terminated till )

(a,a )Y,aAsubt



(prob. b doesn0 terminate b + t|b hasn0 terminated till b )

(13)

(b,b )Y,bAsub
/


Considering pivots makes algorithm computationally intensive may
many pivots many action combinations could end one, many outcomes each.
implementation, cache transition function recompute
information state.
63

fiM AUSAM & W ELD

Start Goal States: start state goal set developed deterministic durations
work unchanged durations stochastic. So, start state hs0 , goal set
G - = {hX, i|X G}.
Thus modeled problem CoMDP interwoven state space. redefined start goal states, probability transition function. use techniques
CoMDPs solve problem. particular, use Bellman equations below.
Bellman Equations Interwoven-Epoch Space: Define el (s, A, s0 ) time elapsed two interwoven states s0 combination executed s. set equations
solution problem written as:

J - (s) = 0, G - else

n

X
Pr - (s0 |s, A) el (s, A, s0 ) + J - (s0 )
J - (s) = min


AAp - (s) 0
-

(14)

Compare equations Equation 11. one difference besides new transition
function time elapsed within summation sign. time elapsed depends
also next interwoven state.
modeled problem CoMDP use algorithms Section 5. use
DUR denote family algorithms CPTP problems involving stochastic durations.
main bottleneck solving problem, besides size interwoven state space,
high branching factor.
6.1.1 P OLICY C ONSTRUCTION : RTDP & H YBRIDIZED P LANNING
Since modeled problem CoMDP new interwoven space, may use pruned
RTDP (DURprun ) sampled RTDP (DURsamp ) policy construction. Since cost function problem (el ) depends also current next state, combo-skipping
apply problem. Thus DURprun refers RTDP combo-elimination.
Furthermore, small adaptations necessary incrementally compute (admissible)
maximum concurrency (M C) (more informed, inadmissible) average concurrency (AC)
heuristics. example, serial MDP (in RHS Equation 12) need compute
average duration action use actions cost.
Likewise, speed planning hybridizing (DURhyb ) RTDP algorithms interwoven aligned-epoch CoMDPs produce near-optimal policy significantly less time.
dynamics aligned epoch space Section 5 one exception. cost
combination, case deterministic durations, simply max duration constituent
actions. novel twist stems fact uncertain durations require computation cost
action combination expected time last action combination terminate.
example, suppose two actions, uniform duration distributions [1,3], started
concurrently. probabilities actions finished times 1, 2 3 (and earlier) 1/9, 3/9, 5/9 respectively. Thus expected duration completion combination
(let us call AE ) 11/9 + 23/9 + 35/9 = 2.44.
64

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

6.2 Expected-Duration Planner
modeled CoMDP full-blown interwoven space, stochastic durations cause
exlposive growth branching factor. general, n actions started possible
durations r probabilistic effects, (m 1)[(r + 1)n rn 1] + rn
potential successors. number may computed follows: duration 1
1 subset actions could complete action could result r outcomes. Hence, total
P
number successors per duration i[1..n] n Ci ri = (r + 1)n rn 1. Moreover, none
actions finish time 1 last step actions terminate leading rn outcomes.
So, total number successors (m 1)[(r + 1)n rn 1] + rn . Thus, branching factor
multiplicative duration uncertainty exponential concurrency.
manage extravagant computation must curb branching factor. One method
ignore duration distributions. assign action constant duration equal mean
distribution, apply deterministic-duration planner DURsamp . However,
executing deterministic-duration policy setting durations actually stochastic,
action likely terminate time different mean, expected duration. DURexp
planner addresses problem augmenting deterministic-duration policy created account
unexpected outcomes.
6.2.1 NLINE V ERSION
procedure easiest understand online version (Algorithm 5): wait unexpected
happens, pause execution, re-plan. original estimate actions duration implausible,
compute revised deterministic estimate terms Ea (min) expected value
duration given terminated time min. Thus, Ea (0) compute expected
duration a.
Algorithm 5 Online DURexp
1: build deterministic-duration policy start state s0
2: repeat
3:
execute action combination specified policy
4:
wait interrupt
5:
case: action terminated expected {//do nothing}
6:
case: action terminates early
7:
extend policy current state
8:
case: action didnt terminate expected
9:
extend policy current state revising

duration follows:
10:
time elapsed since started executing
11:
nextexp dEa (0)e
12:
nextexp <
13:
nextexp dEa (nextexp)e
14:
endwhile
15:
revised duration nextexp
16:
endwait
17: goal reached

65

fiM AUSAM & W ELD

Example: Let duration action follow uniform distribution 1 15.
expected value gets assigned first run algorithm (dEa (0)e) 8. running
algorithm, suppose action didnt terminate 8 reach state running
for, say, 9 time units. case, revised expected duration would (dEa (8)e) = 12.
Similarly, doesnt terminate 12 either next expected duration would 14,
finally 15. words states executing times 0 8, expected
terminate 8. times 8 12 expected completion 12, 12 14 14
doesnt terminate 14 15. 2
6.2.2 FFLINE V ERSION
algorithm also offline version re-planning contingencies done ahead
time fairness used version experiments. Although offline algorithm plans
possible action durations, still much faster algorithms. reason
planning problems solved significantly smaller (less branching factor, smaller
reachable state space), previous computation succinctly stored form
hinterwoven state, valuei pairs thus reused. Algorithm 6 describes offline planner
subsequent example illustrates savings.
Algorithm 6 Offline DURexp
1: build deterministic-duration policy start state s0 ; get current J - - values


2: insert s0 queue open
3: repeat
4:
state = open.pop()
5:
currstate s.t. Pr - (currstate|state, - (state)) > 0



currstate goal currstate set visited
visited.insert(currstate)
J - (currstate) converged
required, change expected durations actions currently executing
currstate.
10:
solve deterministic-duration planning problem start state currstate
11:
insert currstate queue open
12: open empty
6:
7:
8:
9:

Line 9 Algorithm 6 assigns new expected duration actions currently running
current state completd time previous termination point.
reassignment follows similar case online version (line 13).
Example: Consider domain two state-variables, x1 x2 , two actions set-x1
set-x2 . task set variables (initially false). Assume set-x2 always
succeeds whereas set-x1 succeeds 0.5 probability. Moreover, let actions
uniform duration distribution 1, 2, 3. case complete interwoven epoch search
could touch 36 interwoven states (each state variable could true false, action could
running, running 1 unit, running 2 units). Instead, build deterministic
duration policy actions deterministic duration 2, total number states
touched 16 interwoven states (each action could running
running 1 unit).
66

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Problem
A2

B2

G

C2



G

Optimal Solution (Trajectory 1, pr =0.5, makespan 9)
A2

B2

G

C2

Optimal Solution (Trajectory 2, pr =0.5, makespan 5)
A2
C2



G

DUR exp Solution (makespan 8)
A2

Time

0

B2
4

G
8

12

Figure 14: example domain DURexp algorithm compute optimal solution.
Now, suppose deterministic planner decides execute actions start state.
committed combination, easy see certain states never reached.
example, state h(x1 , x2 ), {(setx1 , 2)}i never visited, since set-x2 completes
guaranteed x2 set. fact, example, 3 new states initiate offline
replanning (line 10 Algo 6), viz., h(x1 , x2 ), {(setx2 , 2)}i, h(x1 , x2 ), {(setx2 , 2)}i,
h(x1 , x2 ), {(setx1 , 2)}i 2
6.2.3 P ROPERTIES
Unfortunately, DURexp algorithm guaranteed produce optimal policy. bad
policies generated expected-duration planner? experiments show DURexp
typically generates policies extremely close optimal. Even worst-case pathological
domain able construct leads expected make-span 50% longer
optimal (in limit). example illustrated below.
Example: consider domain actions A2:n , B2:n , C2:n D. Ai Bi
takes time 2i . Ci probabilistic duration: probability 0.5, Ci takes 1 unit time,
remaining probability, takes 2i+1 + 1 time. Thus, expected duration Ci
2i + 1. takes 4 units. sub-problem SPi , goal may reached executing Ai followed
Bi . Alternatively, goal may reached first executing Ci recursively solving
sub-problem SPi1 . domain, DURexp algorithm always compute hAi ; Bi
best solution. However, optimal policy starts {Ai , Ci }. Ci terminates 1,
policy executes solution SPi1 ; otherwise, waits Ai terminates executes Bi .
Figure 14 illustrates sub-problem SP2 optimal policy expected make-span
7 (vs. DURexp make-span 8). general, expected make-span optimal policy
3
SPn 13 [2n+2 + 24n ] + 22n + 2. Thus, limn exp
opt = 2 .2
6.3 Multi-Modal Duration Distributions
planners previous two sections benefited considering small set happenings
instead pivots, approach licensed Conjecture 8. Unfortunately, simplification
67

fiM AUSAM & W ELD

warranted case actions multi-modal duration distributions, common
complex domains factors cant modeled explicitly. example, amount
time Mars rover transmit data might bimodal distribution normally would
take little time, dust storm progress (unmodeled) could take much longer.
handle cases model durations mixture Gaussians parameterized triple
hamplitude, mean, variancei.
6.3.1 C MDP F ORMULATION
Although cannot restrict decision epochs happenings, need consider pivots;
required actions multi-modal distributions. fact, suffices consider pivots
regions distribution expected-time-to-completion increases. cases
need consider happenings.
Two changes required transition function Algorithm 4. line 3, maxtime
computation involves time next pivot increasing remaining time region
actions multi-modal distributions (thus forcing us take decision points, even
action terminates). Another change (in line 6) allows non-empty subset Asub =
maxtime. is, next state computed even without action termination. making
changes transition function reformulate problem CoMDP interwoven space
thus solve, using previous methods pruned/sampled RTDP, hybrid algorithm expectedduration algorithm.
6.3.2 RCHETYPAL -D URATION P LANNER
also develop multi-modal variation expected-duration planner, called DURarch . Instead assigning action single deterministic duration equal expected value, planner
assigns probabilistic duration various outcomes means different modes
distribution probabilities probability mass mode. enhancement
reflects intuitive understanding multi-modal distributions experiments confirm
DURarch produces solutions shorter make-spans DURexp .
6.4 Experiments: Planning Stochastic Durations
evaluate techniques solving planning problems involving stochastic durations.
compare computation time solution quality (make-span) five planners domains
without multi-modal duration distributions. also re-evaluate effectiveness
maximum- (MC) average-concurrency (AC) heuristics domains.
6.4.1 E XPERIMENTAL ETUP
modify Rover, MachineShop, Artificial domains additionally including uncertainty
action durations. set experiments, largest problem 4 million world states
65536 reachable. algorithms explored 1,000,000 distinct states
interwoven state space planning. domains contained many 18 actions,
actions many 13 possible durations. details domains please refer
longer version (Mausam, 2007).
68

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Planning Time (in sec)

6000
5000
4000

Rover

Machine-Shop

3000
Pruned
DURprun
DURsamp
Sampled
DURhyb
Hybrid

2000
1000

DURexp
Exp-Dur

0
21

22

23

24

25

26

27

28

29

30 Problems

Figure 15: Planning time comparisons Rover MachineShop domains: Variation along algorithms
initialized average concurrency (AC) heuristic; DURexp performs best.

Algos
DURsamp
DURhyb
DURexp

Average Quality Make-Span
Rover MachineShop Artificial
1.001
1.000
1.001
1.022
1.011
1.019
1.008
1.015
1.046

Table 4: three planners produce near-optimal policies shown table ratios
optimal make-span.11

6.4.2 C OMPARING RUNNING IMES
compare algorithms without heuristics reaffirm heuristics significantly
speed computation problems; indeed, problems large solved without
heuristics. Comparing amongst find AC beats C regardless
planning algorithm; isnt surprising since AC sacrifices admissibility.
Figure 15 reports running times various algorithms (initialized AC heuristic)
Rover Machine-Shop domains durations unimodal. DURexp out-performs
planners substantial margins. algorithm solving comparatively simpler
problem, fewer states expanded thus approximation scales better others solving,
example, two Machine-Shop problems, large planners.
cases hybridization speeds planning significant amounts, performs better DURexp
artificial domain.
6.4.3 C OMPARING OLUTION Q UALITY
measure quality simulating generated policy across multiple trials. report ratio
average expected make-span optimal expected make-span domains unimodal
distributions Table 4. find make-spans inadmissible heuristic AC par
11. optimal algorithm doesnt converge, use best solution found across runs optimal.

69

fiM AUSAM & W ELD

28
26
24

1000
DURprun
Pruned
DURsamp
Sampled

100

J*(s0)

Planning time (log scale)

10000

DURhyb
Hybrid
DURarch
Arch-Dur
DURexp
Exp-Dur

10

22
DURprun
DUR-prun
DURsamp
DUR-samp

20
18

DURhyb
DUR-hyb
DURarch
DUR-arch

16

DURexp
DUR-exp

14

31

32

33

34

35

36 Problems

31

32

33

34

35

36 Problems

Figure 16: Comparisons Machine-Shop domain multi-modal distributions. (a) Computation
Time comparisons: DURexp DURarch perform much better algos. (b) Makespans returned different algos: Solutions returned DURsamp almost optimal. Overall
DURarch finds good balance running time solution quality.

admissible heuristic C. hybridized planner approximate userdefined bound. experiments, set bound 5% find make-spans returned
algorithm quite close optimal always differ 5%. DURexp
quality guarantees, still solutions returned problems tested upon nearly good
algorithms. Thus, believe approximation quite useful scaling larger
problems without losing solution quality.
6.4.4 ULTIMODAL OMAINS
develop multi-modal variants domains; e.g., Machine-Shop domain, time fetching paint bimodal (if stock, paint fetched fast, else needs ordered).
alternative costly paint action doesnt require fetching paint. Solutions produced
DURsamp made use pivots decision epochs starting costly paint action case
fetch action didnt terminate within first mode bimodal distribution (i.e. paint
stock).
running time comparisons shown Figure 16(a) log-scale. find DURexp
terminates extremely quickly DURarch far behind. However, make-span comparisons Figure 16(b) clearly illustrate approximations made methods order achieve
planning time. DURarch exhibits good balance planning time solution quality.

7. Related Work
paper extends prior work, originally reported several conference publications (Mausam
& Weld, 2004, 2005, 2006a, 2006b).
Temporal planners may classified using constraint-posting extended state-space methods (discussed earlier Section 4). constraint approach promising, (if any) probabilistic planners implemented using architecture; one exception Buridan (Kush70

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

stochastic

deterministic

concurrent
durative
non-durative
DUR, Tempastic,
Concurrent MDP,
GSMDP, Prottle,
Factorial MDP,
FPG, Aberdeen et al.
Paragraph
Temporal Planning
Step-optimal planning
(TP4, Sapa, MIPS
(GraphPlan, SATPlan)
TLPlan, etc.)

non-concurrent
durative
non-durative
Time Dependent MDP,
MDP
IxTeT, CIRCA,
(RTDP, LAO*, etc.)
Foss & Onder
Planning
Classical Planning
Numerical Resources
(HSP, FF, etc.)
(Sapa, Metric-FF, CPT)

Figure 17: table listing various planners implement different subsets concurrent, stochastic, durative actions.

merick, Hanks, & Weld, 1995), performed poorly. contrast, MDP community
proven state-space approach successful. Since powerful deterministic temporal planners,
various planning competitions, also use state-space approach, adopt
algorithms combine temporal planning MDPs. may interesting incorporate
constraint-based approaches probabilistic paradigm compare techniques
paper.
7.1 Comparison Semi-MDPs
Semi-Markov Decision Process extension MDPs allows durative actions take variable time. discrete time semi-MDP solved solving set equations direct
extension Equations 2. techniques solving discrete time semi-MDPs natural generalizations MDPs. main distinction semi-MDP formulation
concurrent probabilistic temporal planning stochastic durations concerns presence concurrently executing actions model. semi-MDP allow concurrent actions
assumes one executing action time. allowing concurrency actions intermediate decision epochs, algorithms need deal large state action spaces, encountered
semi-MDPs.
Furthermore, Younes Simmons shown general case, semi-MDPs incapable modeling concurrency. problem concurrent actions stochastic continuous
durations needs another model known Generalized Semi-Markov Decision Process (GSMDP)
precise mathematical formulation (Younes & Simmons, 2004b).
7.2 Concurrency Stochastic, Durative Actions
Tempastic (Younes & Simmons, 2004a) uses rich formalism (e.g. continuous time, exogenous
events, expressive goal language) generate concurrent plans stochastic durative actions. Tempastic uses completely non-probabilistic planner generate plan treated
candidate policy repaired failure points identified. method guarantee
completeness proximity optimal. Moreover, attention paid towards heuristics
search control making implementation impractical.
GSMDPs (Younes & Simmons, 2004b) extend continuous-time MDPs semi-Markov MDPs,
modeling asynchronous events processes. Younes Simmonss approaches handle
71

fiM AUSAM & W ELD

strictly expressive model due modeling continuous time. solve
GSMDPs approximation standard MDP using phase-type distributions. approach
elegant, scalability realistic problems yet demonstrated. particular, approximate, discrete MDP model require many states yet still behave differently
continuous original.
Prottle (Little et al., 2005) also solves problems action language expressive
ours: effects occur middle action execution dependent durations supported.
Prottle uses RTDP-type search guided heuristics computed probabilistic planning
graph; however, plans finite horizon thus acyclic state space. difficult
compare Prottle approach Prottle optimizes different objective function (probability reaching goal), outputs finite-length conditional plan opposed cyclic plan
policy, guaranteed reach goal.
FPG (Aberdeen & Buffet, 2007) learns separate neural network action individually
based current state. execution phase decision, i.e., whether action needs
executed not, taken independently decisions regarding actions. way FPG able
effectively sidestep blowup caused exponential combinations actions. practice
able quickly compute high quality solutions.
Rohanimanesh Mahadevan (2001) investigate concurrency hierarchical reinforcement
learning framework, abstract actions represented Markov options. propose
algorithm based value-iteration, focus calculating joint termination conditions rewards received, rather speeding policy construction. Hence, consider possible Markov
option combinations backup.
Aberdeen et al. (2004) plan concurrent, durative actions deterministic durations
specific military operations domain. apply various domain-dependent heuristics speed
search extended state space.
7.3 Concurrency Stochastic, Non-durative Actions
Meuleau et al. Singh & Cohn deal special type MDP (called factorial MDP)
represented set smaller weakly coupled MDPs separate MDPs completely
independent except common resource constraints, reward cost models
purely additive (Meuleau, Hauskrecht, Kim, Peshkin, Kaelbling, Dean, & Boutilier, 1998; Singh
& Cohn, 1998). describe solutions sub-MDPs independently solved
sub-policies merged create global policy. Thus, concurrency actions different
sub-MDPs by-product work. Singh & Cohn present optimal algorithm (similar
combo-elimination used DURprun ), whereas domain specific heuristics Meuleau et al.
guarantees. work Factorial MDPs assumes weak coupling exists
identified, factoring MDP hard problem itself.
Paragraph (Little & Thiebaux, 2006) formulates planning concurrency regression
search probabilistic planning graph. uses techniques like nogood learning mutex
reasoning speed policy construction.
Guestrin et al. solve multi-agent MDP problem using linear programming (LP) formulation expressing value function linear combination basis functions. assuming
basis functions depend agents, able reduce size LP
(Guestrin, Koller, & Parr, 2001).
72

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

7.4 Stochastic, Non-concurrent, Durative Actions
Many researchers studied planning stochastic, durative actions absence concurrency.
example, Foss Onder (2005) use simple temporal networks generate plans
objective function time component. Simple Temporal Networks allow effective temporal
constraint reasoning methods generate temporally contingent plans.
Boyan Littman (2000) propose Time-dependent MDPs model problems actions
concurrent time-dependent, stochastic durations; solution generates piecewise linear value functions.
NASA researchers developed techniques generating non-concurrent plans uncertain continuous durations using greedy algorithm incrementally adds branches straightline plan (Bresina et al., 2002; Dearden, Meuleau, Ramakrishnan, Smith, & Washington, 2003).
handle continuous variables uncertain continuous effects, solution heuristic
quality policies unknown. Also, since consider limited contingencies,
solutions guaranteed reach goal.
IxTeT temporal planner uses constraint based reasoning within partial order planning
(Laborie & Ghallab, 1995). embeds temporal properties actions constraints
optimize make-span. CIRCA example system plans uncertain durations
action associated unweighted set durations (Musliner, Murphy, & Shin, 1991).
7.5 Deterministic, Concurrent, Durative Actions
Planning deterministic actions comparitively simpler problem much work
planning uncertainty based previous, deterministic planning research. instance,
interwoven state representation transition function extensions extended state representations TP4, SAPA, TLPlan (Haslum & Geffner, 2001; & Kambhampati, 2003;
Bacchus & Ady, 2001).
planners, like MIPS AltAltp , also investigated fast generation parallel plans
deterministic settings (Edelkamp, 2003; Nigenda & Kambhampati, 2003) Jensen Veloso
(2000) extend problems disjunctive uncertainty.

8. Future Work
presented comprehensive set techniques handle probabilistic outcomes, concurrent
durative actions single formalism, direct attention towards different relaxations
extensions proposed model. particular, explore objective functions, infinite
horizon problems, continuous-valued duration distributions, temporally expressive action models,
degrees goal satisfaction interruptibility actions.
8.1 Extension Cost Functions
planning problems durative actions (sections 4 beyond) focused make-span
minimization problems. However, techniques quite general applicable (directly
minor variations) variety cost metrics. illustration, consider mixed cost
optimization problem addition duration action, also given
amount resource consumed per action, wish minimize sum make-span
total resource usage. Assuming resource consumption unaffected concurrent
73

fiM AUSAM & W ELD

execution, easily compute new max-concurrency heuristic. mixed-cost counterpart
Equations 12 is:
Jt (X)
+ Jr (X)
=
c
Qt (X, )
+ Qr (X, ) 6=
c

J - (s)

J - (s)


(15)

Here, Jt single-action MDP assignng costs durations Jr single
action MDP assigning costs resource consumptions. informed average concurrency
heuristic similarly computed replacing maximum concurrency average concurrency.
hybridized algorithm follows fashion, fast algorithm CoMDP
solved using techniques Section 3.
lines, objective function minimize make-span given certain maximum
resource usage, total amount resource remaining included state-space
CoMDPs underlying single-action MDPs etc. techniques may used.
8.2 Infinite Horizon Problems
paper defined techniques case indefinite horizon problems,
absorbing state defined reachable. problems alternative formulation
preferred allows infinite execution discounts future costs multiplying
discount factor step. Again, techniques suitably extended scenario.
example, Theorem 2 gets modified following:
Qk (s, A)

1k

Qk (s, {a1 }) + Ck (A)

k
X

!



ik

Ck ({ai })

i=1

Recall theorem provides us pruning rule, combo-skipping. Thus, use
Pruned RTDP new pruning rule.
8.3 Extensions Continuous Duration Distributions
confined actions discrete durations (refer Assumption 3).
investigate effects dealing directly continuous uncertainty duration distributions. Let fiT (t)dt probability action ai completing times + + + dt,
conditioned action ai finishing time . Similarly, define FiT (t) probability
action finishing time + .
Let us consider extended state hX, {(a1 , )}i, denotes action a1 started units
ago world state X. Let a2 applicable action started extended state. Define
= min(M (a1 )T, (a2 )), denotes maximum possible duration execution
action. Intuitively, time least one action complete.

Q - n+1 (hX, {(a1 , )}i, a2 ) =

Z
0

Z
0

h



f1T (t)F20 (t) + J - n (hX1 , {a2 , t}i) dt +
h



F1T (t)f20 (t) + J - n (hX2 , {a1 , + }i) dt
74

(16)

fi0

2

4

Time

6

8

10

10

10

Expected time reach goal

Expected Remaining Time action a0

Duration Distribution a0

P LANNING URATIVE ACTIONS TOCHASTIC OMAINS

8
6
4
2

0

2

4

Time

6

8

10

8
6
4
2

0

2

4

6

8

10

Time

Figure 18: durations continuous (real-valued) rather discrete, may infinite number
potentially important decision epochs. domain, crucial decision epoch could required
time (0, 1] depending length possible alternate plans.

X1 X2 world states obtained applying deterministic actions a1 a2
respectively X. Recall J - n+1 (s) = mina Q - n+1 (s, a). fixed point computation

form, desire Jn+1 Jn functional form12 . Going equation
seems difficult achieve, except perhaps specific action distributions
special planning problems. example, distributions constant
concurrency domain, equations easily solvable. interesting cases,
solving equations challenging open question.
Furthermore, dealing continuous multi-modal distributions worsens decision epochs
explosion. illustrate help example.
Example: Consider domain Figure 7 except let action a0 bimodal distribution,
two modes uniform 0-1 9-10 respectively shown Figure 18(a). Also
let a1 small duration. Figure 18(b) shows expected remaining termination times
a0 terminates time 10. Notice due bimodality, expected remaining execution time
increases 0 1. expected time reach goal using plan h{a0 , a1 }; a2 shown
third graph. suppose, started {a0 , a1 }, need choose next decision
epoch. easy see optimal decision epoch could point 0 1
would depend alternative routes goal. example, duration b0 7.75,
optimal time-point start alternative route 0.5 (right expected time reach goal
using first plan exceeds 7.75).
Thus, choice decision epochs depends expected durations alternative routes.
values known advance, fact ones calculated planning
phase. Therefore, choosing decision epochs ahead time seem possible. makes
optimal continuous multi-modal distribution planning problem mostly intractable reasonable
sized problem.
8.4 Generalizing TGP Action Model
assumption TGP style actions enables us compute optimal policies, since prune
number decision epochs. case complex action models like PDDL2.1 (Fox & Long, 2003),
old, deterministic state-space planners incomplete. reasons, algorithms
12. idea exploited order plan continuous resources (Feng, Dearden, Meuleau, & Washington,
2004).

75

fiM AUSAM & W ELD

incomplete problems PPDDL2.1 . Recently, Cushing et al. introduced Tempo, statespace planner, uses lifting time achieve completeness (Cushing, Kambhampati,
Mausam, & Weld, 2007). pursuit finding complete, state-space, probabilistic planner
complex action models, natural step consider Tempo-like representation probabilistic
setting. working details seems relatively straightforward, important research
challenge find right heuristics streamline search algorithm scale.
8.5 Extensions
several extensions basic framework suggested. different
construct introduces additional structure need exploit knowledge order design
fast algorithms. Many times, basic algorithms proposed paper may easily adapted
situations, sometimes may not. list two important extensions below.
Notion Goal Satisfaction: Different problems may require slightly different notions
goal reached. example, assumed thus far goal officially
achieved executed actions terminated. Alternatively, one might consider goal
achieved satisfactory world state reached, even though actions may
midst execution. intermediate possibilities goal requires specific
actions necessarily end. changing definition goal set, problems
modeled CoMDP. hybridized algorithm heuristics easily adapted
case.
Interruptible Actions: assumed that, started, action cannot terminated.
However, richer model may allow preemptions, well continuation interrupted
action. problems, actions could interrupted will, significantly
different flavor. Interrupting action new kind decision requires full study
might action termination useful. large extent, planning similar
finding different concurrent paths goal starting together, since one
always interrupt executing paths soon goal reached. instance, example
Figure 7 longer holds since b0 started time 1, later terminated needed
shorten make-span.
8.6 Effect Large Durations
weakness extended-state space approaches, deterministic well probabilistic
settings, dependence absolute durations (or accurate, greatest common
divisor action durations). instance, domain action large duration,
say 100 another concurrently executable action duration 1, world states
explored tuples (a, 1), (a, 2), . . ., (a, 98), (a, 99). general, many states
behave similarly certain decision boundaries important. Start b
executing 50 units c otherwise one example decision boundary.
Instead representing flat discrete states individually, planning aggregate space
state represents several extended states help alleviate inefficiency.
However, obvious achieve aggregation automatically, since adapting
well-known methods aggregation hold case. instance, SPUDD (Hoey
76

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

et al., 1999) uses algebraic decision diagrams represent abstract states Jvalue. Aggregating valued states may enough us, since expected time
completion depends linearly amount time left longest executing action. So,
states differ amount time action executing able
aggregate together. similar way, Feng et al. (2004) use piecewise constant piecewise
linear representations adaptively discretize continuous variables. case, |A|
variables. executing active given time, modeling
sparse high-dimensional value function easy either. able exploit structure due
action durations essential future direction order scale algorithms complex real
world domains.

9. Conclusions
Although concurrent durative actions stochastic effects characterize many real-world domains, planners handle challenges concert. paper proposes unified statespace based framework model solve problems. State space formulations popular
deterministic temporal planning well probabilistic planning. However,
features bring additional complexities formulation afford new solution techniques.
develop DUR family algorithms alleviates complexities. evaluate techniques running times qualities solutions produced. Moreover, study theoretical
properties domains also identify key conditions fast, optimal algorithms
possible. make following contributions:
1. define Concurrent MDPs (CoMDP) extension MDP model formulate
stochastic planning problem concurrent actions. CoMDP cast back new
MDP extended action space. action space possibly exponential
number actions, solving new MDP naively may take huge performance hit.
develop general notions pruning sampling speed algorithms. Pruning
refers pruning provably sub-optimal action-combinations state, thus performing less computation still guaranteeing optimal solutions. Sampling-based solutions
rely intelligent sampling action-combinations avoid dealing exponential
number. method converges orders magnitude faster methods produces
near-optimal solutions.
2. formulate planning concurrent, durative actions CoMDP two modified
state spaces aligned epoch, interwoven epoch. aligned epoch based solutions
run fast, interwoven epoch algorithms yield much higher quality solutions. also define two heuristic functions maximum concurrency (MC), average concurrency (AC)
guide search. MC admissible heuristic, whereas AC, inadmissible, typically more-informed leading better computational gains. call algorithms DUR
family algorithms. subscripts samp prun refer sampling pruning respectively,
optional superscripts AC MC refer heuristic employed, optional ""
DUR notifies problem stochastic durations. example, Labeled RTDP
deterministic duration problem employing sampling started AC heuristic
abbreviated DURAC
samp .
77

fiM AUSAM & W ELD

3. also develop general technique hybridizing two planners. Hybridizing interwovenepoch aligned-epoch CoMDPs yields much efficient algorithm, DURhyb .
algorithm parameter, varied trade-off speed optimality.
experiments, DURhyb quickly produces near-optimal solutions. larger problems,
speedups algorithms quite significant. hybridized algorithm also
used anytime fashion thus producing good-quality proper policies (policies
guaranteed reach goal) within desired time. Moreover, idea hybridizing two
planners general notion; recently applied solving general stochastic planning
problems (Mausam, Bertoli, & Weld, 2007).
4. Uncertainty durations leads complexities addition state action
spaces, also blowup branching factor number decision epochs.
bound space decision epochs terms pivots (times actions may potentially terminate) conjecture restrictions, thus making problem tractable.
also propose two algorithms, expected duration planner (DURexp ) archetypal
duration planner (DURarch ), successively solve small planning problems
limited duration uncertainty, respectively. DURarch also able make use
additional structure offered multi-modal duration distributions. algorithms perform
much faster techniques. Moreover, DURarch offers good balance
planning time vs. solution quality tradeoff.
5. Besides focus stochastic actions, expose important theoretical issues related
durative actions repercussions deterministic temporal planners well.
particular, prove common state-space temporal planners incomplete face
expressive action models, e.g., PDDL2.1 , result may strong impact
future temporal planning research (Cushing et al., 2007).
Overall, paper proposes large set techniques useful modeling solving
planning problems employing stochastic effects, concurrent executions durative actions
duration uncertainties. algorithms range fast suboptimal solutions, relatively slow
optimal. Various algorithms explore different intermediate points spectrum also
presented. hope techniques useful scaling planning techniques real
world problems future.

Acknowledgments
thank Blai Bonet providing source code GPT well comments course
work. thankful Sumit Sanghai theorem proving skills advice various
stages research. grateful Derek Long anonymous reviewers paper
gave several thoughtful suggestions generalizing theory improving clarity
text. also thank Subbarao Kambhampati, Daniel Lowd, Parag, David Smith others
provided useful comments drafts parts research. work performed University Washington 2003 2007 supported generous grants National
Aeronautics Space Administration (Award NAG 2-1538), National Science Foundation (Award
IIS-0307906), Office Naval Research (Awards N00014-02-1-0932, N00014-06-1-0147)
WRF / TJ Cable Professorship.
78

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

References
Aberdeen, D., Thiebaux, S., & Zhang, L. (2004). Decision-theoretic military operations planning.
ICAPS04.
Aberdeen, D., & Buffet, O. (2007).
gradients. ICAPS07.

Concurrent probabilistic temporal planning policy-

Bacchus, F., & Ady, M. (2001). Planning resources concurrency: forward chaining
approach. IJCAI01, pp. 417424.
Barto, A., Bradtke, S., & Singh, S. (1995). Learning act using real-time dynamic programming.
Artificial Intelligence, 72, 81138.
Bertsekas, D. (1995). Dynamic Programming Optimal Control. Athena Scientific.
Blum, A., & Furst, M. (1997). Fast planning planning graph analysis. Artificial Intelligence,
90(12), 281300.
Bonet, B., & Geffner, H. (2003). Labeled RTDP: Improving convergence real-time dynamic
programming. ICAPS03, pp. 1221.
Bonet, B., & Geffner, H. (2005). mGPT: probabilistic planner based heuristic search. JAIR,
24, 933.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptions
computational leverage. J. Artificial Intelligence Research, 11, 194.
Boyan, J. A., & Littman, M. L. (2000). Exact solutions time-dependent MDPs. NIPS00, p.
1026.
Bresina, J., Dearden, R., Meuleau, N., Smith, D., & Washington, R. (2002). Planning continuous time resource uncertainty : challenge AI. UAI02.
Chen, Y., Wah, B. W., & Hsu, C. (2006). Temporal planning using subgoal partitioning resolution sgplan. JAIR, 26, 323.
Cushing, W., Kambhampati, S., Mausam, & Weld, D. S. (2007). temporal planning really
temporal?. IJCAI07.
Dearden, R., Meuleau, N., Ramakrishnan, S., Smith, D. E., & Washington, R. (2003). Incremental
Contingency Planning. ICAPS03 Workshop Planning Uncertainty Incomplete Information.
Do, M. B., & Kambhampati, S. (2001). Sapa: domain-independent heuristic metric temporal
planner. ECP01.
Do, M. B., & Kambhampati, S. (2003). Sapa: scalable multi-objective metric temporal planner.
JAIR, 20, 155194.
Edelkamp, S. (2003). Taming numbers duration model checking integrated planning
system. Journal Artificial Intelligence Research, 20, 195238.
79

fiM AUSAM & W ELD

Feng, Z., Dearden, R., Meuleau, N., & Washington, R. (2004). Dynamic programming structured continuous Markov decision processes. UAI04, p. 154.
Foss, J., & Onder, N. (2005). Generating temporally contingent plans. IJCAI05 Workshop
Planning Learning Apriori Unknown Dynamic Domains.
Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal planning
domains.. JAIR Special Issue 3rd International Planning Competition, 20, 61124.
Gerevini, A., & Serina, I. (2002). LPG: planner based local search planning graphs
action graphs. AIPS02, p. 281.
Guestrin, C., Koller, D., & Parr, R. (2001). Max-norm projections factored MDPs. IJCAI01,
pp. 673682.
Hansen, E., & Zilberstein, S. (2001). LAO*: heuristic search algorithm finds solutions
loops. Artificial Intelligence, 129, 3562.
Haslum, P., & Geffner, H. (2001). Heuristic planning time resources. ECP01.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planning using decision
diagrams. UAI99, pp. 279288.
Jensen, R. M., & Veloso, M. (2000). OBDD=based universal planning synchronized agents
non-deterministic domains. Journal Artificial Intelligence Research, 13, 189.
Kushmerick, N., Hanks, S., & Weld, D. (1995). algorithm probabilistic planning. Artificial
Intelligence, 76(1-2), 239286.
Laborie, P., & Ghallab, M. (1995). Planning sharable resource constraints. IJCAI95, p.
1643.
Little, I., Aberdeen, D., & Thiebaux, S. (2005). Prottle: probabilistic temporal planner.
AAAI05.
Little, I., & Thiebaux, S. (2006). Concurrent probabilistic planning graphplan framework.
ICAPS06.
Long, D., & Fox, M. (2003). 3rd international planning competition: Results analysis.
JAIR, 20, 159.
Mausam (2007). Stochastic planning concurrent, durative actions. Ph.d. dissertation, University Washington.
Mausam, Bertoli, P., & Weld, D. (2007). hybridized planner stochastic domains. IJCAI07.
Mausam, & Weld, D. (2004). Solving concurrent Markov decision processes. AAAI04.
Mausam, & Weld, D. (2005). Concurrent probabilistic temporal planning. ICAPS05, pp. 120
129.
80

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Mausam, & Weld, D. (2006a). Challenges temporal planning uncertain durations.
ICAPS06.
Mausam, & Weld, D. (2006b). Probabilistic temporal planning uncertain durations.
AAAI06.
Meuleau, N., Hauskrecht, M., Kim, K.-E., Peshkin, L., Kaelbling, L., Dean, T., & Boutilier, C.
(1998). Solving large weakly coupled Markov Decision Processes. AAAI98, pp.
165172.
Musliner, D., Murphy, D., & Shin, K. (1991). World modeling dynamic construction
real-time control plans. Artificial Intelligence, 74, 83127.
Nigenda, R. S., & Kambhampati, S. (2003). Altalt-p: Online parallelization plans heuristic
state search. Journal Artificial Intelligence Research, 19, 631657.
Penberthy, J., & Weld, D. (1994). Temporal planning continuous change. AAAI94, p. 1010.
Rohanimanesh, K., & Mahadevan, S. (2001). Decision-Theoretic planning concurrent temporally extended actions. UAI01, pp. 472479.
Singh, S., & Cohn, D. (1998). dynamically merge markov decision processes. NIPS98.
MIT Press.
Smith, D., & Weld, D. (1999). Temporal graphplan mutual exclusion reasoning. IJCAI99,
pp. 326333 Stockholm, Sweden. San Francisco, CA: Morgan Kaufmann.
Vidal, V., & Geffner, H. (2006). Branching pruning: optimal temporal pocl planner based
constraint programming. AIJ, 170(3), 298335.
Younes, H. L. S., & Simmons, R. G. (2004a). Policy generation continuous-time stochastic
domains concurrency. ICAPS04, p. 325.
Younes, H. L. S., & Simmons, R. G. (2004b). Solving generalized semi-markov decision processes
using continuous phase-type distributions. AAAI04, p. 742.
Zhang, W., & Dietterich, T. G. (1995). reinforcement learning approach job-shop scheduling.
IJCAI95, pp. 11141120.

Appendix
Proof Theorem 6
prove statement Theorem 6, i.e., actions TGP-style set pivots
suffices optimal planning. proof make use fact actions TGP-style
consistent execution concurrent plan requires two executing actions non-mutex
(refer Section 5 explanation that). particular, none effects conflict
precondition one conflict effects another.
prove theorem contradition. Let us assume problem optimal solution
requires least one action start non-pivot. Let us consider one optimal plans,
81

fiM AUSAM & W ELD

first non-pivot point action needs start non-pivot minimized. Let
us name time point let action starts point a. prove case
analysis may, well, start time 1 without changing nature plan. 1
also non-pivot contradict hypothesis minimum first non-pivot point.
1 pivot hypothesis contradicted "need to" start non-pivot.
prove left-shifted 1 unit, take one trajectory time (recall
actions could several durations) consider actions playing role 1, t, + (a) 1,
+ (a), (a) refers duration trajectory. Considering points
suffice, since system state change points trajectory. prove
execution none actions affected left shift. following twelve
cases:
1. actions b start 1: b cant end (t non-pivot). Thus b execute
concurrently t, implies b non-mutex. Thus b may well start together.
2. actions b continue execution 1: Use argument similar case 1 above.
3. actions b end 1: b TGP-style, effects realized open interval
ending 1. Therefore, start conflict end b.
4. actions b start t: b start together hence dependent
preconditions. Also, non-mutex, starting times shifted
direction.
5. actions b continue execution t: b started 1 refer case 1 above. not,
1 similar points b.
6. actions b end t: Case possible due assumption non-pivot.
7. actions b start + (a) 1: Since continued execution point, b
non-mutex. Thus effects clobber bs preconditions. Hence, b still executed
realizing effects.
8. actions b continue execution + (a) 1: b non-mutex, may end
earlier without effect b.
9. actions b end + (a) 1: b executing concurrently. Thus
non-mutex. may end together.
10. actions b start + (a): b may still start + (a), since state + (a)
doesnt change.
11. actions b continue execution + (a): b started + (a) 1 refer case
7 above, else state change + (a) cause effect b.
12. actions b end + (a): b non-mutex executing concurrently. Thus, effects dont clobber bs preconditions. Hence, may end earlier.
Since left shifted trajectories, therefore left-shift legal. Also,
multiple actions start may shifted one one using argument.
Hence Proved. 2
82

fi
